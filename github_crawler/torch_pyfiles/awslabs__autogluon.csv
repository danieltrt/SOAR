file_path,api_count,code
setup.py,0,"b'#!/usr/bin/env python\nimport os\nimport shutil\nimport subprocess\n\nfrom setuptools import setup, find_packages\n\ncwd = os.path.dirname(os.path.abspath(__file__))\n\nversion = \'0.0.11\'\n""""""\nTo release a new stable version on PyPi, simply tag the release on github, and the Github CI will automatically publish \na new stable version to PyPi using the configurations in .github/workflows/pypi_release.yml . \nYou need to increase the version number after stable release, so that the nightly pypi can work properly.\n""""""\ntry:\n    if not os.getenv(\'RELEASE\'):\n        from datetime import date\n        today = date.today()\n        day = today.strftime(""b%Y%m%d"")\n        version += day\nexcept Exception:\n    pass\n\ndef create_version_file():\n    global version, cwd\n    print(\'-- Building version \' + version)\n    version_path = os.path.join(cwd, \'autogluon\', \'version.py\')\n    with open(version_path, \'w\') as f:\n        f.write(\'""""""This is autogluon version file.""""""\\n\')\n        f.write(""__version__ = \'{}\'\\n"".format(version))\n\nlong_description = open(\'README.md\').read()\n\nMIN_PYTHON_VERSION = \'>=3.6.*\'\n\nrequirements = [\n    \'Pillow<=6.2.1\',\n    \'numpy>=1.16.0\',\n    \'scipy>=1.3.3\',\n    \'cython\',\n    \'tornado>=5.0.1\',\n    \'requests\',\n    \'matplotlib\',\n    \'tqdm>=4.38.0\',\n    \'paramiko>=2.4\',\n    \'dask>=2.6.0\',\n    \'cryptography>=2.8\',\n    \'distributed>=2.6.0\',\n    \'ConfigSpace<=0.4.10\',\n    \'gluoncv>=0.5.0,<1.0\',\n    \'gluonnlp==0.8.1\',\n    \'graphviz\',\n    \'scikit-optimize\',\n    \'catboost<0.24\',\n    \'boto3\',\n    \'lightgbm>=2.3.0,<3.0\',\n    \'pandas>=0.24.0,<1.0\',\n    \'psutil>=5.0.0\',\n    \'scikit-learn>=0.20.0,<0.23\',\n    \'networkx>=2.3,<3.0\'\n]\n\ntest_requirements = [\n    \'pytest\',\n]\n\nif __name__ == \'__main__\':\n    create_version_file()\n    setup(\n        # Metadata\n        name=\'autogluon\',\n        version=version,\n        author=\'AutoGluon Community\',\n        url=\'https://github.com/awslabs/autogluon\',\n        description=\'AutoML Toolkit with MXNet Gluon\',\n        long_description=long_description,\n        long_description_content_type=\'text/markdown\',\n        license=\'Apache\',\n\n        # Package info\n        packages=find_packages(exclude=(\'docs\', \'tests\', \'scripts\')),\n        zip_safe=True,\n        include_package_data=True,\n        install_requires=requirements + test_requirements,\n        python_requires=MIN_PYTHON_VERSION,\n        package_data={\'autogluon\': [\n            \'LICENSE\',\n        ]},\n        entry_points={\n            \'console_scripts\': [\n                \'agremote = autogluon.scheduler.remote.cli:main\',\n            ]\n        },\n    )\n'"
autogluon/__init__.py,0,"b'# coding: utf-8\n# pylint: disable=wrong-import-position\n"""""" AutoGluon: AutoML Toolkit for Deep Learning """"""\nfrom __future__ import absolute_import\nfrom .version import __version__\n\nimport logging\nlogger = logging.getLogger(""distributed.utils_perf"")\nlogger.setLevel(logging.ERROR)\n\nlogger = logging.getLogger(""distributed.logging.distributed"")\nlogger.setLevel(logging.ERROR)\n\nlogger = logging.getLogger(""distributed.worker"")\nlogger.setLevel(logging.ERROR)\n\nfrom .utils.try_import import *\ntry_import_mxnet()\n\nfrom . import scheduler, searcher, utils\nfrom .scheduler import get_cpu_count, get_gpu_count\n\nfrom .utils import *\nfrom .core import *\nfrom .task import *\n'"
examples/cifar_autogluon.py,0,"b'import os\nimport numpy as np\n\nimport argparse, time, logging\nimport mxnet as mx\nfrom mxnet import gluon\nfrom mxnet.gluon.data.vision import transforms\n\nfrom gluoncv.model_zoo import get_model\nfrom gluoncv.utils import makedirs, LRScheduler\nfrom gluoncv.data import transforms as gcv_transforms\n\nimport autogluon as ag\n\nimport ConfigSpace as CS\nimport ConfigSpace.hyperparameters as CSH\n\n# CLI\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\'Train a model for image classification.\')\n    parser.add_argument(\'--num-gpus\', type=int, default=1,\n                        help=\'number of gpus to use.\')\n    parser.add_argument(\'--num-trials\', default=10, type=int,\n                        help=\'number of trail tasks\')\n    parser.add_argument(\'--epochs\', default=20, type=int,\n                        help=\'number of epochs\')\n    parser.add_argument(\'--scheduler\', type=str, default=\'fifo\',\n                        help=\'scheduler name (default: fifo)\')\n    parser.add_argument(\'--checkpoint\', type=str, default=\'checkpoint/cifar1.ag\',\n                        help=\'checkpoint path (default: None)\')\n    parser.add_argument(\'--debug\', action=\'store_true\', default= False,\n                        help=\'debug if needed\')\n    args = parser.parse_args()\n    return args\n\n\n@ag.args(\n    batch_size=64,\n    num_workers=2,\n    num_gpus=1,\n    model=\'cifar_resnet20_v1\',\n    j=4,\n    lr=ag.space.Real(1e-2, 1e-1, log=True),\n    momentum=0.9,\n    wd=ag.space.Real(1e-5, 1e-3, log=True),\n    epochs=20,\n)\ndef train_cifar(args, reporter):\n    print(\'args\', args)\n    batch_size = args.batch_size\n\n    num_gpus = args.num_gpus\n    batch_size *= max(1, num_gpus)\n    context = [mx.gpu(i) for i in range(num_gpus)] if num_gpus > 0 else [mx.cpu()]\n    num_workers = args.num_workers\n\n    model_name = args.model\n    net = get_model(model_name, classes=10)\n\n    transform_train = transforms.Compose([\n        gcv_transforms.RandomCrop(32, pad=4),\n        transforms.RandomFlipLeftRight(),\n        transforms.ToTensor(),\n        transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])\n    ])\n\n    transform_test = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])\n    ])\n\n    def test(ctx, val_data):\n        metric = mx.metric.Accuracy()\n        for i, batch in enumerate(val_data):\n            data = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0)\n            label = gluon.utils.split_and_load(batch[1], ctx_list=ctx, batch_axis=0)\n            outputs = [net(X) for X in data]\n            metric.update(label, outputs)\n        return metric.get()\n\n    def train(epochs, ctx):\n        if isinstance(ctx, mx.Context):\n            ctx = [ctx]\n        net.initialize(mx.init.Xavier(), ctx=ctx)\n\n        train_data = gluon.data.DataLoader(\n            gluon.data.vision.CIFAR10(train=True).transform_first(transform_train),\n            batch_size=batch_size, shuffle=True, last_batch=\'discard\', num_workers=num_workers)\n\n        val_data = gluon.data.DataLoader(\n            gluon.data.vision.CIFAR10(train=False).transform_first(transform_test),\n            batch_size=batch_size, shuffle=False, num_workers=num_workers)\n\n        lr_scheduler = LRScheduler(mode=\'cosine\', base_lr=args.lr,\n                                   nepochs=args.epochs,\n                                   iters_per_epoch=len(train_data))\n        trainer = gluon.Trainer(net.collect_params(), \'sgd\',\n                                {\'lr_scheduler\': lr_scheduler, \'wd\': args.wd, \'momentum\': args.momentum})\n        metric = mx.metric.Accuracy()\n        train_metric = mx.metric.Accuracy()\n        loss_fn = gluon.loss.SoftmaxCrossEntropyLoss()\n\n        iteration = 0\n        best_val_score = 0\n\n        start_epoch = 0\n\n        for epoch in range(start_epoch, epochs):\n            tic = time.time()\n            train_metric.reset()\n            metric.reset()\n            train_loss = 0\n            num_batch = len(train_data)\n            alpha = 1\n\n            for i, batch in enumerate(train_data):\n                data = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0)\n                label = gluon.utils.split_and_load(batch[1], ctx_list=ctx, batch_axis=0)\n\n                with mx.autograd.record():\n                    output = [net(X) for X in data]\n                    loss = [loss_fn(yhat, y) for yhat, y in zip(output, label)]\n                for l in loss:\n                    l.backward()\n                trainer.step(batch_size)\n                train_loss += sum([l.sum().asscalar() for l in loss])\n\n                train_metric.update(label, output)\n                name, acc = train_metric.get()\n                iteration += 1\n\n            train_loss /= batch_size * num_batch\n            name, acc = train_metric.get()\n            name, val_acc = test(ctx, val_data)\n            reporter(epoch=epoch+1, accuracy=val_acc)\n\n    train(args.epochs, context)\n\ndef cifar_evaluate(net, args):\n    batch_size = args.batch_size\n    batch_size *= max(1, args.num_gpus)\n\n    ctx = [mx.gpu(i) for i in range(args.num_gpus)] if args.num_gpus > 0 else [mx.cpu()]\n    net.collect_params().reset_ctx(ctx)\n    metric = mx.metric.Accuracy()\n    transform_test = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])\n    ])\n    val_data = gluon.data.DataLoader(\n        gluon.data.vision.CIFAR10(train=False).transform_first(transform_test),\n        batch_size=batch_size, shuffle=False, num_workers=args.num_workers)\n\n    for i, batch in enumerate(val_data):\n        data = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0)\n        label = gluon.utils.split_and_load(batch[1], ctx_list=ctx, batch_axis=0)\n        outputs = [net(X) for X in data]\n        metric.update(label, outputs)\n    return metric.get()[1]\n\nif __name__ == \'__main__\':\n    args = parse_args()\n    if args.debug:\n        logging.basicConfig(level=logging.DEBUG)\n    else:\n        logging.basicConfig(level=logging.INFO)\n\n    train_cifar.update(epochs=args.epochs)\n    # create searcher and scheduler\n    extra_node_ips = []\n    if args.scheduler == \'hyperband\':\n        myscheduler = ag.scheduler.HyperbandScheduler(train_cifar,\n                                                      resource={\'num_cpus\': 2, \'num_gpus\': args.num_gpus},\n                                                      num_trials=args.num_trials,\n                                                      checkpoint=args.checkpoint,\n                                                      time_attr=\'epoch\', reward_attr=""accuracy"",\n                                                      max_t=args.epochs, grace_period=args.epochs//4,\n                                                      dist_ip_addrs=extra_node_ips)\n    elif args.scheduler == \'fifo\':\n        myscheduler = ag.scheduler.FIFOScheduler(train_cifar,\n                                                 resource={\'num_cpus\': 2, \'num_gpus\': args.num_gpus},\n                                                 num_trials=args.num_trials,\n                                                 checkpoint=args.checkpoint,\n                                                 reward_attr=""accuracy"",\n                                                 dist_ip_addrs=extra_node_ips)\n    else:\n        raise RuntimeError(\'Unsuported Scheduler!\')\n\n    print(myscheduler)\n    myscheduler.run()\n    myscheduler.join_tasks()\n    myscheduler.get_training_curves(\'{}.png\'.format(os.path.splitext(args.checkpoint)[0]))\n    print(\'The Best Configuration and Accuracy are: {}, {}\'.format(myscheduler.get_best_config(),\n                                                                   myscheduler.get_best_reward()))\n'"
tests/conftest.py,0,"b'import pytest\n\n\ndef pytest_addoption(parser):\n    parser.addoption(\n        ""--runslow"", action=""store_true"", default=False, help=""run slow tests""\n    )\n\n\ndef pytest_configure(config):\n    config.addinivalue_line(""markers"", ""slow: mark test as slow to run"")\n\n\ndef pytest_collection_modifyitems(config, items):\n    if config.getoption(""--runslow""):\n        # --runslow given in cli: do not skip slow tests\n        return\n    skip_slow = pytest.mark.skip(reason=""need --runslow option to run"")\n    for item in items:\n        if ""slow"" in item.keywords:\n            item.add_marker(skip_slow)\n'"
tests/test_check_style.py,0,"b'from subprocess import Popen, PIPE\nimport logging\n\n\ndef test_check_style():\n    logging.getLogger().setLevel(logging.INFO)\n    logging.info(""PEP8 Style check"")\n    flake8_proc = Popen([\'flake8\', \'--count\'], stdout=PIPE)\n    flake8_out = flake8_proc.communicate()[0]\n    lines = flake8_out.splitlines()\n    count = int(lines[-1].decode())\n    if count > 0:\n        logging.warn(""%d PEP8 warnings remaining"", count)\n    if count > 3438:\n        logging.warn(""Additional PEP8 warnings were introducing, style check fails"")\n        return 1\n    logging.info(""Passed"")\n    return 0\n\n'"
autogluon/contrib/__init__.py,0,b'from . import enas\n'
autogluon/core/__init__.py,0,b'from .space import *\nfrom .task import *\nfrom .decorator import *\n\nfrom . import optimizer\n'
autogluon/core/decorator.py,0,"b'import copy\nimport logging\nimport argparse\nimport functools\nfrom collections import OrderedDict\nimport numpy as np\nimport multiprocessing as mp\nimport ConfigSpace as CS\n\nfrom .space import *\nfrom .space import _add_hp, _add_cs, _rm_hp, _strip_config_space\nfrom ..utils import EasyDict as ezdict\nfrom ..utils.deprecate import make_deprecate\n\n__all__ = [\'args\', \'obj\', \'func\', \'sample_config\',\n           \'autogluon_register_args\', \'autogluon_object\', \'autogluon_function\',\n           \'autogluon_register_dict\']\n\nlogger = logging.getLogger(__name__)\n\ndef sample_config(args, config):\n    args = copy.deepcopy(args)\n    striped_keys = [k.split(\'.\')[0] for k in config.keys()]\n    if isinstance(args, (argparse.Namespace, argparse.ArgumentParser)):\n        args_dict = vars(args)\n    else:\n        args_dict = args\n    for k, v in args_dict.items():\n        # handle different type of configurations\n        if k in striped_keys:\n            if isinstance(v, NestedSpace):\n                sub_config = _strip_config_space(config, prefix=k)\n                args_dict[k] = v.sample(**sub_config)\n            else:\n                if \'.\' in k: continue\n                args_dict[k] = config[k]\n        elif isinstance(v, AutoGluonObject):\n            args_dict[k] = v.init()\n    return args\n\nclass _autogluon_method(object):\n    SEED = mp.Value(\'i\', 0)\n    LOCK = mp.Lock()\n    def __init__(self, f):\n        self.f = f\n        self.args = ezdict()\n        functools.update_wrapper(self, f)\n\n    def __call__(self, args, config={}, **kwargs):\n        new_config = copy.deepcopy(config)\n        self._rand_seed()\n        args = sample_config(args, new_config)\n        from ..scheduler.reporter import FakeReporter\n        if \'reporter\' not in kwargs:\n            logger.debug(\'Creating FakeReporter for test purpose.\')\n            kwargs[\'reporter\'] = FakeReporter()\n\n        output = self.f(args, **kwargs)\n        logger.debug(\'Reporter Done!\')\n        kwargs[\'reporter\'](done=True)\n        return output\n \n    def register_args(self, default={}, **kwvars):\n        if isinstance(default, (argparse.Namespace, argparse.ArgumentParser)):\n            default = vars(default)\n        self.kwvars = {}\n        self.args = ezdict()\n        self.args.update(default)\n        self.update(**kwvars)\n\n    def update(self, **kwargs):\n        """"""For searcher support ConfigSpace\n        """"""\n        self.kwvars.update(kwargs)\n        for k, v in self.kwvars.items():\n            if isinstance(v, (NestedSpace)):\n                self.args.update({k: v})\n            elif isinstance(v, Space):\n                hp = v.get_hp(name=k)\n                self.args.update({k: hp.default_value})\n            else:\n                self.args.update({k: v})\n\n    @property\n    def cs(self):\n        cs = CS.ConfigurationSpace()\n        for k, v in self.kwvars.items():\n            if isinstance(v, NestedSpace):\n                _add_cs(cs, v.cs, k)\n            elif isinstance(v, Space):\n                hp = v.get_hp(name=k)\n                _add_hp(cs, hp)\n            else:\n                _rm_hp(cs, k)\n        return cs\n\n    @property\n    def kwspaces(self):\n        """"""For RL searcher/controller\n        """"""\n        kw_spaces = OrderedDict()\n        for k, v in self.kwvars.items():\n            if isinstance(v, NestedSpace):\n                if isinstance(v, Categorical):\n                    kw_spaces[\'{}.choice\'.format(k)] = v\n                for sub_k, sub_v in v.kwspaces.items():\n                    new_k = \'{}.{}\'.format(k, sub_k)\n                    kw_spaces[new_k] = sub_v\n            elif isinstance(v, Space):\n                kw_spaces[k] = v\n        return kw_spaces\n\n    def _rand_seed(self):\n        _autogluon_method.SEED.value += 1\n        np.random.seed(_autogluon_method.SEED.value)\n\n    def __repr__(self):\n        return repr(self.f)\n\n\ndef args(default={}, **kwvars):\n    """"""Decorator for a Python training script that registers its arguments as hyperparameters. \n       Each hyperparameter takes fixed value or is a searchable space, and the arguments may either be:\n       built-in Python objects (e.g. floats, strings, lists, etc.), AutoGluon objects (see :func:`autogluon.obj`), \n       or AutoGluon search spaces (see :class:`autogluon.space.Int`, :class:`autogluon.space.Real`, etc.).\n\n    Examples\n    --------\n    >>> import autogluon as ag\n    >>> @ag.args(batch_size=10, lr=ag.Real(0.01, 0.1))\n    >>> def train_func(args):\n    ...     print(\'Batch size is {}, LR is {}\'.format(args.batch_size, arg.lr))\n    """"""\n    kwvars[\'_default_config\'] = default\n    def registered_func(func):\n        @_autogluon_method\n        @functools.wraps(func)\n        def wrapper_call(*args, **kwargs):\n            return func(*args, **kwargs)\n\n        default = kwvars[\'_default_config\']\n        wrapper_call.register_args(default=default, **kwvars)\n        return wrapper_call\n\n    return registered_func\n\n\ndef func(**kwvars):\n    """"""Decorator for a function that registers its arguments as hyperparameters. \n       Each hyperparameter may take a fixed value or be a searchable space (autogluon.space).\n\n    Returns\n    -------\n    Instance of :class:`autogluon.space.AutoGluonObject`:\n        A lazily initialized object, which allows for distributed training.\n\n    Examples\n    --------\n    >>> import autogluon as ag\n    >>> from gluoncv.model_zoo import get_model\n    >>> \n    >>> @ag.func(pretrained=ag.space.Categorical(True, False))\n    >>> def cifar_resnet(pretrained):\n    ...     return get_model(\'cifar_resnet20_v1\', pretrained=pretrained)\n    """"""\n    def _autogluon_kwargs_func(**kwvars):\n        def registered_func(func):\n            kwspaces = OrderedDict()\n            @functools.wraps(func)\n            def wrapper_call(*args, **kwargs):\n                _kwvars = copy.deepcopy(kwvars)\n                _kwvars.update(kwargs)\n                for k, v in _kwvars.items():\n                    if isinstance(v, NestedSpace):\n                        kwspaces[k] = v\n                        kwargs[k] = v\n                    elif isinstance(v, Space):\n                        kwspaces[k] = v\n                        hp = v.get_hp(name=k)\n                        kwargs[k] = hp.default_value\n                    else:\n                        kwargs[k] = v\n                return func(*args, **kwargs)\n            wrapper_call.kwspaces = kwspaces\n            return wrapper_call\n        return registered_func\n\n    def registered_func(func):\n        class autogluonobject(AutoGluonObject):\n            @_autogluon_kwargs_func(**kwvars)\n            def __init__(self, *args, **kwargs):\n                self.func = func\n                self.args = args\n                self.kwargs = kwargs\n                self._inited = False\n\n            def sample(self, **config):\n                kwargs = copy.deepcopy(self.kwargs)\n                kwspaces = copy.deepcopy(autogluonobject.kwspaces)\n                for k, v in kwargs.items():\n                    if k in kwspaces and isinstance(kwspaces[k], NestedSpace):\n                        sub_config = _strip_config_space(config, prefix=k)\n                        kwargs[k] = kwspaces[k].sample(**sub_config)\n                    elif k in config:\n                        kwargs[k] = config[k]\n                        \n                return self.func(*self.args, **kwargs)\n\n        @functools.wraps(func)\n        def wrapper_call(*args, **kwargs):\n            _kwvars = copy.deepcopy(kwvars)\n            _kwvars.update(kwargs)\n            agobj = autogluonobject(*args, **kwargs)\n            agobj.kwvars = _kwvars\n            return agobj\n        return wrapper_call\n    return registered_func\n\ndef obj(**kwvars):\n    """"""Decorator for a Python class that registers its arguments as hyperparameters. \n       Each hyperparameter may take a fixed value or be a searchable space (autogluon.space).\n\n    Returns\n    -------\n    Instance of :class:`autogluon.space.AutoGluonObject`:\n        A lazily initialized object, which allows distributed training.\n\n    Examples\n    --------\n    >>> import autogluon as ag\n    >>> from mxnet import optimizer as optim\n    >>> @ag.obj(\n    >>>     learning_rate=ag.space.Real(1e-4, 1e-1, log=True),\n    >>>     wd=ag.space.Real(1e-4, 1e-1),\n    >>> )\n    >>> class Adam(optim.Adam):\n    >>>     pass\n    """"""\n    def _autogluon_kwargs_obj(**kwvars):\n        def registered_func(func):\n            kwspaces = OrderedDict()\n            @functools.wraps(func)\n            def wrapper_call(*args, **kwargs):\n                kwvars.update(kwargs)\n                for k, v in kwvars.items():\n                    if isinstance(v, NestedSpace):\n                        kwspaces[k] = v\n                        kwargs[k] = v\n                    elif isinstance(v, Space):\n                        kwspaces[k] = v\n                        hp = v.get_hp(name=k)\n                        kwargs[k] = hp.default_value\n                    else:\n                        kwargs[k] = v\n                return func(*args, **kwargs)\n            wrapper_call.kwspaces = kwspaces\n            wrapper_call.kwvars = kwvars\n            return wrapper_call\n        return registered_func\n\n    def registered_class(Cls):\n        class autogluonobject(AutoGluonObject):\n            @_autogluon_kwargs_obj(**kwvars)\n            def __init__(self, *args, **kwargs):\n                self.args = args\n                self.kwargs = kwargs\n                self._inited = False\n\n            def sample(self, **config):\n                kwargs = copy.deepcopy(self.kwargs)\n                kwspaces = copy.deepcopy(autogluonobject.kwspaces)\n                for k, v in kwargs.items():\n                    if k in kwspaces and isinstance(kwspaces[k], NestedSpace):\n                        sub_config = _strip_config_space(config, prefix=k)\n                        kwargs[k] = kwspaces[k].sample(**sub_config)\n                    elif k in config:\n                        kwargs[k] = config[k]\n\n                args = self.args\n                return Cls(*args, **kwargs)\n\n            def __repr__(self):\n                return \'AutoGluonObject -- \' + Cls.__name__\n\n        autogluonobject.kwvars = autogluonobject.__init__.kwvars\n        autogluonobject.__doc__ = Cls.__doc__\n        autogluonobject.__name__ = Cls.__name__\n        return autogluonobject\n\n    return registered_class\n\n\n\nautogluon_register_args = make_deprecate(args, \'autogluon_register_args\')\nautogluon_register_dict = make_deprecate(args, \'autogluon_register_dict\')\nautogluon_function = make_deprecate(func, \'autogluon_function\')\nautogluon_object = make_deprecate(obj, \'autogluon_object\')\n'"
autogluon/core/loss.py,0,"b""from mxnet.gluon import loss\nfrom ..core import obj\n\n__all__ = ['SoftmaxCrossEntropyLoss']\n\n@obj()\nclass SoftmaxCrossEntropyLoss(loss.SoftmaxCrossEntropyLoss):\n    pass\n"""
autogluon/core/optimizer.py,0,"b""from mxnet import optimizer as optim\nfrom ..core import obj\n\n__all__ = ['Adam', 'NAG', 'SGD']\n\n@obj()\nclass Adam(optim.Adam):\n    pass\n\n@obj()\nclass NAG(optim.NAG):\n    pass\n\n@obj()\nclass SGD(optim.SGD):\n    pass\n"""
autogluon/core/space.py,0,"b'import copy\nfrom collections import OrderedDict\nimport ConfigSpace as CS\nimport ConfigSpace.hyperparameters as CSH\nfrom ..utils import DeprecationHelper, EasyDict, classproperty\n\n__all__ = [\'Space\', \'NestedSpace\', \'AutoGluonObject\', \'List\', \'Dict\',\n           \'Categorical\', \'Choice\', \'Real\', \'Int\', \'Bool\']\n\nclass Space(object):\n    """"""Basic search space describing set of possible values for hyperparameter.\n    """"""\n    pass\n\nclass SimpleSpace(Space):\n    """"""Non-nested search space (i.e. corresponds to a single simple hyperparameter).\n    """"""\n    def __repr__(self):\n        reprstr = self.__class__.__name__\n        if hasattr(self, \'lower\') and hasattr(self, \'upper\'):\n            reprstr += \': lower={}, upper={}\'.format(self.lower, self.upper)\n        if hasattr(self, \'value\'):\n            reprstr += \': value={}\'.format(self.value)\n        return reprstr\n\n    def get_hp(self, name):\n        """"""Fetch particular hyperparameter based on its name.\n        """"""\n        raise NotImplementedError\n\n    @property\n    def hp(self):\n        """""" Return hyperparameter corresponding to this search space.\n        """"""\n        return self.get_hp(name=\'\')\n\n    @property\n    def default(self):\n        """"""Return default value of hyperparameter corresponding to this search space.\n        """"""\n        default = self._default if self._default else self.hp.default_value\n        return default\n\n    @default.setter\n    def default(self, value):\n        """"""Set default value for hyperparameter corresponding to this search space.\n        """"""\n        self._default = value\n\n    @property\n    def rand(self):\n        """"""Return randomly sampled (but valid) value from this search space.\n        """"""\n        cs = CS.ConfigurationSpace()\n        cs.add_hyperparameter(self.hp)\n        return cs.sample_configuration().get_dictionary()[\'\']\n\nclass NestedSpace(Space):\n    """"""Nested hyperparameter search space, which is a search space that itself contains multiple search spaces.\n    """"""\n    def sample(self, **config):\n        """"""Sample a configuration from this search space.\n        """"""\n        pass\n\n    @property\n    def cs(self):\n        """""" ConfigSpace representation of this search space.\n        """"""\n        raise NotImplementedError\n\n    @property\n    def kwspaces(self):\n        """""" OrderedDict representation of this search space.\n        """"""\n        raise NotImplementedError\n\n    @property\n    def default(self):\n        """"""Return default value for hyperparameter corresponding to this search space.\n        """"""\n        config = self.cs.get_default_configuration().get_dictionary()\n        return self.sample(**config)\n\n    @property\n    def rand(self):\n        """"""Randomly sample configuration from this nested search space.\n        """"""\n        config = self.cs.sample_configuration().get_dictionary()\n        return self.sample(**config)\n\nclass AutoGluonObject(NestedSpace):\n    r""""""Searchable objects, \n    created by decorating a custom Python class or function using the \n    :func:`autogluon.obj` or :func:`autogluon.func` decorators.\n    """"""\n    def __call__(self, *args, **kwargs):\n        """"""Convenience method for interacting with AutoGluonObject.\n        """"""\n        if not self._inited:\n            self._inited = True\n            self._instance = self.init()\n        return self._instance.__call__(*args, **kwargs)\n\n    def init(self):\n        """"""Instantiate an actual instance of this `AutoGluonObject`. \n            In order to interact with such an `object`, you must always first call: `object.init()`.\n        """"""\n        config = self.cs.get_default_configuration().get_dictionary()\n        return self.sample(**config)\n\n    @property\n    def cs(self):\n        """""" ConfigSpace representation of this search space.\n        """"""\n        cs = CS.ConfigurationSpace()\n        for k, v in self.kwvars.items():\n            if isinstance(v, NestedSpace):\n                _add_cs(cs, v.cs, k)\n            elif isinstance(v, Space):\n                hp = v.get_hp(name=k)\n                _add_hp(cs, hp)\n            else:\n                _rm_hp(cs, k)\n        return cs\n\n    @classproperty\n    def kwspaces(cls):\n        """""" OrderedDict representation of this search space.\n        """"""\n        return cls.__init__.kwspaces\n\n    def sample(self):\n        """"""Sample a configuration from this search space.\n        """"""\n        raise NotImplementedError\n\n    def __repr__(self):\n        return \'AutoGluonObject\'\n\nclass List(NestedSpace):\n    r""""""Nested search space corresponding to an ordered list of hyperparameters.\n\n    Parameters\n    ----------\n\n    args : list\n        a list of search spaces.\n\n    Examples\n    --------\n    >>> sequence = ag.List(\n    >>>     ag.space.Categorical(\'conv3x3\', \'conv5x5\', \'conv7x7\'),\n    >>>     ag.space.Categorical(\'BatchNorm\', \'InstanceNorm\'),\n    >>>     ag.space.Categorical(\'relu\', \'sigmoid\'),\n    >>> )\n    """"""\n    def __init__(self, *args):\n        self.data = [*args]\n\n    def __iter__(self):\n        for elem in self.data:\n            yield elem\n\n    def __getitem__(self, index):\n        return self.data[index]\n\n    def __setitem__(self, index, data):\n        self.data[index] = data\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getstate__(self):\n        return self.data\n\n    def __setstate__(self, d):\n        self.data = d\n\n    def __getattribute__(self, s):\n        try:    \n            x = super(List, self).__getattribute__(s)\n        except AttributeError:      \n            pass\n        else:\n            return x\n        x = self.data.__getattribute__(s)\n        return x\n\n    def sample(self, **config):\n        """"""Sample a configuration from this search space.\n        """"""\n        ret = []\n        kwspaces = self.kwspaces\n        striped_keys = [k.split(\'.\')[0] for k in config.keys()]\n        for idx, obj in enumerate(self.data):\n            if isinstance(obj, NestedSpace):\n                sub_config = _strip_config_space(config, prefix=str(idx))\n                ret.append(obj.sample(**sub_config))\n            elif isinstance(obj, SimpleSpace):\n                ret.append(config[str(idx)])\n            else:\n                ret.append(obj)\n        return ret\n\n    @property\n    def cs(self):\n        """""" ConfigSpace representation of this search space.\n        """"""\n        cs = CS.ConfigurationSpace()\n        for k, v in enumerate(self.data):\n            if isinstance(v, NestedSpace):\n                _add_cs(cs, v.cs, str(k))\n            elif isinstance(v, Space):\n                hp = v.get_hp(name=str(k))\n                _add_hp(cs, hp)\n        return cs\n\n    @property\n    def kwspaces(self):\n        """""" OrderedDict representation of this search space.\n        """"""\n        kw_spaces = OrderedDict()\n        for idx, obj in enumerate(self.data):\n            k = str(idx)\n            if isinstance(obj, NestedSpace):\n                kw_spaces[k] = obj\n                for sub_k, sub_v in obj.kwspaces.items():\n                    new_k = \'{}.{}\'.format(k, sub_k)\n                    kw_spaces[new_k] = sub_v\n            elif isinstance(obj, Space):\n                kw_spaces[k] = obj\n        return kw_spaces\n\n    def __repr__(self):\n        reprstr = self.__class__.__name__ + str(self.data)\n        return reprstr\n\nclass Dict(NestedSpace):\n    """"""Nested search space\xc2\xa0for dictionary containing multiple hyperparameters.\n\n    Examples\n    --------\n    >>> g = ag.space.Dict(\n    >>>         hyperparam1 = ag.space.Categorical(\'alpha\', \'beta\'),\n    >>>         hyperparam2 = ag.space.Int(0, 3)\n    >>>     )\n    >>> print(g)\n    """"""\n    def __init__(self, **kwargs):\n        self.data = EasyDict(kwargs)\n\n    def __getattribute__(self, s):\n        try:    \n            x = super(Dict, self).__getattribute__(s)\n        except AttributeError:      \n            pass\n        else:\n            return x\n        x = self.data.__getattribute__(s)\n        return x\n\n    def __getitem__(self, key):\n        return self.data[key]\n\n    def __setitem__(self, key, data):\n        self.data[key] = data\n\n    def __getstate__(self):\n        return self.data\n\n    def __setstate__(self, d):\n        self.data = d\n\n    @property\n    def cs(self):\n        """""" ConfigSpace representation of this search space.\n        """"""\n        cs = CS.ConfigurationSpace()\n        for k, v in self.data.items():\n            if isinstance(v, NestedSpace):\n                _add_cs(cs, v.cs, k)\n            elif isinstance(v, Space):\n                hp = v.get_hp(name=k)\n                _add_hp(cs, hp)\n        return cs\n\n    @property\n    def kwspaces(self):\n        """""" OrderedDict representation of this search space.\n        """"""\n        kw_spaces = OrderedDict()\n        for k, obj in self.data.items():\n            if isinstance(obj, NestedSpace):\n                kw_spaces[k] = obj\n                for sub_k, sub_v in obj.kwspaces.items():\n                    new_k = \'{}.{}\'.format(k, sub_k)\n                    kw_spaces[new_k] = sub_v\n                    kw_spaces[new_k] = sub_v\n            elif isinstance(obj, Space):\n                kw_spaces[k] = obj\n        return kw_spaces\n\n    def sample(self, **config):\n        """"""Sample a configuration from this search space.\n        """"""\n        ret = {}\n        ret.update(self.data)\n        kwspaces = self.kwspaces\n        kwspaces.update(config)\n        striped_keys = [k.split(\'.\')[0] for k in config.keys()]\n        for k, v in kwspaces.items():\n            if k in striped_keys:\n                if isinstance(v, NestedSpace):\n                    sub_config = _strip_config_space(config, prefix=k)\n                    ret[k] = v.sample(**sub_config)\n                else:\n                    ret[k] = v\n        return ret\n\n    def __repr__(self):\n        reprstr = self.__class__.__name__ + str(self.data)\n        return reprstr\n\nclass Categorical(NestedSpace):\n    """"""Nested search space for hyperparameters which are categorical. Such a hyperparameter takes one value out of the discrete set of provided options.\n\n    Parameters\n    ----------\n    data : Space or python built-in objects\n        the choice candidates\n\n    Examples\n    --------\n    a = ag.space.Categorical(\'a\', \'b\', \'c\', \'d\')\n    b = ag.space.Categorical(\'resnet50\', autogluon_obj())\n    """"""\n    def __init__(self, *data):\n        self.data = [*data]\n\n    def __iter__(self):\n        for elem in self.data:\n            yield elem\n\n    def __getitem__(self, index):\n        return self.data[index]\n\n    def __setitem__(self, index, data):\n        self.data[index] = data\n\n    def __len__(self):\n        return len(self.data)\n\n    @property\n    def cs(self):\n        """""" ConfigSpace representation of this search space.\n        """"""\n        cs = CS.ConfigurationSpace()\n        if len(self.data) == 0: \n            return CS.ConfigurationSpace()\n        hp = CSH.CategoricalHyperparameter(name=\'choice\', choices=range(len(self.data)))\n        _add_hp(cs, hp)\n        for i, v in enumerate(self.data):\n            if isinstance(v, NestedSpace):\n                _add_cs(cs, v.cs, str(i))\n        return cs\n\n    def sample(self, **config):\n        """"""Sample a configuration from this search space.\n        """"""\n        choice = config.pop(\'choice\')\n        if isinstance(self.data[choice], NestedSpace):\n            # nested space: Categorical of AutoGluonobjects\n            min_config = _strip_config_space(config, prefix=str(choice))\n            return self.data[choice].sample(**min_config)\n        else:\n            return self.data[choice]\n\n    @property\n    def kwspaces(self):\n        """"""OrderedDict representation of this search space.\n        """"""\n        kw_spaces = OrderedDict()\n        for idx, obj in enumerate(self.data):\n            if isinstance(obj, NestedSpace):\n                for sub_k, sub_v in obj.kwspaces.items():\n                    new_k = \'{}.{}\'.format(idx, sub_k)\n                    kw_spaces[new_k] = sub_v\n        return kw_spaces\n\n    def __repr__(self):\n        reprstr = self.__class__.__name__ + str(self.data)\n        return reprstr\n\nChoice = DeprecationHelper(Categorical, \'Choice\')\n\nclass Real(SimpleSpace):\n    """"""Search space for numeric hyperparameter that takes continuous values.\n\n    Parameters\n    ----------\n    lower : float\n        the lower bound of the search space\n    upper : float\n        the upper bound of the search space\n    default : float (optional)\n        default value\n    log : (True/False)\n        Whether to search the values on a logarithmic rather than linear scale. \n        This is useful for numeric hyperparameters (such as learning rates) whose search space spans many orders of magnitude.\n\n    Examples\n    --------\n    >>> learning_rate = ag.Real(0.01, 0.1, log=True)\n    """"""\n    def __init__(self, lower, upper, default=None, log=False):\n        self.lower = lower\n        self.upper = upper\n        self.log = log\n        self._default = default\n\n    def get_hp(self, name):\n        \n        return CSH.UniformFloatHyperparameter(name=name, lower=self.lower, upper=self.upper,\n                                              default_value=self._default, log=self.log)\n\nclass Int(SimpleSpace):\n    """"""Search space for numeric hyperparameter that takes integer values.\n\n    Parameters\n    ----------\n    lower : int\n        The lower bound of the search space\n    upper : int\n        The upper bound of the search space\n    default : int (optional)\n        Default value\n\n\n    Examples\n    --------\n    >>> range = ag.space.Int(0, 100)\n    """"""\n    def __init__(self, lower, upper, default=None):\n        self.lower = lower\n        self.upper = upper\n        self._default = default\n\n    def get_hp(self, name):\n        return CSH.UniformIntegerHyperparameter(name=name, lower=self.lower, upper=self.upper,\n                                                default_value=self._default)\n\nclass Bool(Int):\n    """"""Search space for hyperparameter that is either True or False. \n       `ag.Bool()` serves as shorthand for: `ag.space.Categorical(True, False)`\n\n    Examples\n    --------\n    pretrained = ag.space.Bool()\n    """"""\n    def __init__(self):\n        super(Bool, self).__init__(0, 1)\n\ndef _strip_config_space(config, prefix):\n    # filter out the config with the corresponding prefix\n    new_config = {}\n    for k, v in config.items():\n        if k.startswith(prefix):\n            new_config[k[len(prefix)+1:]] = v\n    return new_config\n\ndef _add_hp(cs, hp):\n    if hp.name in cs._hyperparameters:\n        cs._hyperparameters[hp.name] = hp\n    else:\n        cs.add_hyperparameter(hp)\n\ndef _add_cs(master_cs, sub_cs, prefix, delimiter=\'.\', parent_hp=None):\n    new_parameters = []\n    for hp in sub_cs.get_hyperparameters():\n        new_parameter = copy.deepcopy(hp)\n        # Allow for an empty top-level parameter\n        if new_parameter.name == \'\':\n            new_parameter.name = prefix\n        elif not prefix == \'\':\n            new_parameter.name = ""%s%s%s"" % (prefix, \'.\', new_parameter.name)\n        new_parameters.append(new_parameter)\n    for hp in new_parameters:\n        _add_hp(master_cs, hp)\n\ndef _rm_hp(cs, k):\n    if k in cs._hyperparameters:\n        cs._hyperparameters.pop(k)\n    for hp in cs.get_hyperparameters():\n        if  hp.name.startswith(""%s.""%(k)):\n            cs._hyperparameters.pop(hp.name)\n'"
autogluon/core/task.py,0,"b'import copy\nimport logging\nimport argparse\nimport multiprocessing as mp\n\nlogger = logging.getLogger(__name__)\n\n__all__ = [\'Task\']\n\nclass Task(object):\n    """"""Individual training task, containing the lauch function, default arguments and\n    required resources.\n\n    Args:\n        fn (callable): Lauch function for the training task.\n        args (argparse.ArgumentParser): Default function arguments.\n        resources (autogluon.scheduler.Resources): Required resources for lauching the task.\n\n    Example:\n        >>> def my_task():\n        >>>     pass\n        >>> resource = Resources(num_cpus=2, num_gpus=0)\n        >>> task = Task(my_task, {}, resource)\n    """"""\n    TASK_ID = mp.Value(\'i\', 0)\n    LOCK = mp.Lock()\n    def __init__(self, fn, args, resources):\n        self.fn = fn\n        self.args = copy.deepcopy(args)\n        self.resources = resources\n        with Task.LOCK:\n            self.task_id = Task.TASK_ID.value\n            if \'args\' in self.args:\n                if isinstance(self.args[\'args\'], (argparse.Namespace, argparse.ArgumentParser)):\n                    args_dict = vars(self.args[\'args\'])\n                else:\n                    args_dict = self.args[\'args\']\n                args_dict.update({\'task_id\': self.task_id})\n            Task.TASK_ID.value += 1\n\n    @classmethod\n    def set_id(cls, taskid):\n        logger.info(\'Setting TASK ID: {}\'.format(taskid))\n        cls.TASK_ID.value = taskid\n\n    def __repr__(self):\n        reprstr = self.__class__.__name__ +  \\\n            \' (\' + \'task_id: \' + str(self.task_id) + \\\n            \',\\n\\tfn: \' + str(self.fn) + \\\n            \',\\n\\targs: {\'\n        for k, v in self.args.items():\n            data = str(v)\n            info = (data[:100] + \'..\') if len(data) > 100 else data\n            reprstr +=  \'{}\'.format(k) + \': \' + info + \', \'\n        reprstr +=  \'},\\n\\tresource: \' + str(self.resources) + \')\\n\'\n        return reprstr\n'"
autogluon/model_zoo/__init__.py,0,b'from .model_zoo import get_model\nfrom .models import *\n'
autogluon/model_zoo/model_store.py,0,"b'""""""Model store which provides pretrained models.""""""\nfrom __future__ import print_function\n\n__all__ = [\'get_model_file\', \'purge\']\n\nimport os\nimport zipfile\n\nfrom ..utils import download, check_sha1\n\n\n_model_sha1 = {name: checksum for checksum, name in [\n    (\'dd74519f956bc608912e413316a7eaa7fac4b365\', \'efficientnet_b0\'),\n    (\'d920bff669b8c0cadd3107b984d8a52748d341a4\', \'efficientnet_b1\'),\n    (\'3a552326d08c80cecbe1abf6f209223dfcaf7b30\', \'efficientnet_b2\'),\n    (\'8ed810a38e73ec71659b110919e59bcb4fbebd8f\', \'efficientnet_b3\'),\n    (\'20319a29bb6cd3c0bb0c4d4dce9f0d5a279f7b7e\', \'efficientnet_b4\'),\n    (\'e0da163506c2aa3ad48b92829af75162e09e0b7b\', \'efficientnet_b5\'),\n    (\'eb97a9dab456c9931673e469e0a09a0d3fda2a11\', \'efficientnet_b6\'),\n    (\'3cb5cc71e074ddb8eb1b334d1099fc1ecaa78562\', \'efficientnet_b7\'),\n    (\'96443327d8113ae5b4346db1cd29b96b361eed72\', \'standford_dog_resnet152_v1\'),\n    (\'5dbc5d6789b0f6fd8c27974e7bb68db540c2e2e5\', \'standford_dog_resnext101_64x4d\'),\n    ]}\n\nautogluon_repo_url = \'https://autogluon.s3.amazonaws.com/\'\n_url_format = \'{repo_url}models/{file_name}.zip\'\n\ndef short_hash(name):\n    if name not in _model_sha1:\n        raise ValueError(\'Pretrained model for {name} is not available.\'.format(name=name))\n    return _model_sha1[name][:8]\n\ndef get_model_file(name, root=os.path.join(\'~\', \'.autogluon\', \'models\')):\n    r""""""Return location for the pretrained on local file system.\n    This function will download from online model zoo when model cannot be found or has mismatch.\n    The root directory will be created if it doesn\'t exist.\n    Parameters\n    ----------\n    name : str\n        Name of the model.\n    root : str, default \'~/.encoding/models\'\n        Location for keeping the model parameters.\n    Returns\n    -------\n    file_path\n        Path to the requested pretrained model file.\n    """"""\n    file_name = \'{name}-{short_hash}\'.format(name=name, short_hash=short_hash(name))\n    root = os.path.expanduser(root)\n    file_path = os.path.join(root, file_name+\'.params\')\n    sha1_hash = _model_sha1[name]\n    if os.path.exists(file_path):\n        if check_sha1(file_path, sha1_hash):\n            return file_path\n        else:\n            print(\'Mismatch in the content of model file {} detected.\' +\n                  \' Downloading again.\'.format(file_path))\n    else:\n        print(\'Model file {} is not found. Downloading.\'.format(file_path))\n\n    if not os.path.exists(root):\n        os.makedirs(root)\n\n    zip_file_path = os.path.join(root, file_name+\'.zip\')\n    repo_url = os.environ.get(\'ENCODING_REPO\', autogluon_repo_url)\n    if repo_url[-1] != \'/\':\n        repo_url = repo_url + \'/\'\n    download(_url_format.format(repo_url=repo_url, file_name=file_name),\n             path=zip_file_path,\n             overwrite=True)\n    with zipfile.ZipFile(zip_file_path) as zf:\n        zf.extractall(root)\n    os.remove(zip_file_path)\n\n    if check_sha1(file_path, sha1_hash):\n        return file_path\n    else:\n        raise ValueError(\'Downloaded file has different hash. Please try again.\')\n\ndef purge(root=os.path.join(\'~\', \'.encoding\', \'models\')):\n    r""""""Purge all pretrained model files in local file store.\n    Parameters\n    ----------\n    root : str, default \'~/.encoding/models\'\n        Location for keeping the model parameters.\n    """"""\n    root = os.path.expanduser(root)\n    files = os.listdir(root)\n    for f in files:\n        if f.endswith("".params""):\n            os.remove(os.path.join(root, f))\n\ndef pretrained_model_list():\n    return list(_model_sha1.keys())\n'"
autogluon/model_zoo/model_zoo.py,0,"b'import gluoncv as gcv\nfrom .models.efficientnet import *\nfrom .models.standford_dog_models import *\n\n_all__ = [\'get_model\', \'get_model_list\']\n\nmodels = {\n    \'efficientnet_b0\': get_efficientnet_b0,\n    \'efficientnet_b1\': get_efficientnet_b1,\n    \'efficientnet_b2\': get_efficientnet_b2,\n    \'efficientnet_b3\': get_efficientnet_b3,\n    \'efficientnet_b4\': get_efficientnet_b4,\n    \'efficientnet_b5\': get_efficientnet_b5,\n    \'efficientnet_b6\': get_efficientnet_b6,\n    \'efficientnet_b7\': get_efficientnet_b7,\n    \'standford_dog_resnet152_v1\': standford_dog_resnet152_v1,\n    \'standford_dog_resnext101_64x4d\': standford_dog_resnext101_64x4d,\n    }\n\n\ndef get_model(name, **kwargs):\n    """"""Returns a pre-defined model by name\n    Parameters\n    ----------\n    name : str\n        Name of the model.\n    pretrained : bool\n        Whether to load the pretrained weights for model.\n    root : str, default \'~/.encoding/models\'\n        Location for keeping the model parameters.\n    Returns\n    -------\n    Module:\n        The model.\n    """"""\n\n    name = name.lower()\n    if name in models:\n        net = models[name](**kwargs)\n    elif name in gcv.model_zoo.get_model_list():\n        net = gcv.model_zoo.get_model(name, **kwargs)\n    else:\n        raise ValueError(\'%s\\n\\t%s\' % (str(name), \'\\n\\t\'.join(sorted(models.keys()))))\n    return net\n\n\ndef get_model_list():\n    """"""Get the entire list of model names in model_zoo.\n    Returns\n    -------\n    list of str\n        Entire list of model names in model_zoo.\n    """"""\n    return list(models.keys()) + list(gcv.model_zoo.get_model_list())\n'"
autogluon/scheduler/__init__.py,0,"b'from .import remote, resource\nfrom .resource import get_cpu_count, get_gpu_count\n\n# schedulers\nfrom .scheduler import *\nfrom .fifo import *\nfrom .hyperband import *\nfrom .rl_scheduler import *\n'"
autogluon/scheduler/fifo.py,0,"b'import json\nimport logging\nimport multiprocessing as mp\nimport os\nimport pickle\nimport threading\nimport time\nfrom collections import OrderedDict\nimport numpy as np\nimport copy\n\nfrom tqdm.auto import tqdm\n\nfrom .reporter import DistStatusReporter, FakeReporter\nfrom .resource import DistributedResource\nfrom .scheduler import TaskScheduler\nfrom ..core import Task\nfrom ..core.decorator import _autogluon_method\nfrom ..searcher import BaseSearcher\nfrom ..searcher.searcher_factory import searcher_factory\nfrom ..utils import save, load, mkdir, try_import_mxboard\n\n__all__ = [\'FIFOScheduler\']\n\nlogger = logging.getLogger(__name__)\n\n\nclass FIFOScheduler(TaskScheduler):\n    r""""""Simple scheduler that just runs trials in submission order.\n\n    Parameters\n    ----------\n    train_fn : callable\n        A task launch function for training.\n    args : object (optional)\n        Default arguments for launching train_fn.\n    resource : dict\n        Computation resources. For example, `{\'num_cpus\':2, \'num_gpus\':1}`\n    searcher : str or BaseSearcher\n        Searcher (get_config decisions). If str, this is passed to\n        searcher_factory along with search_options.\n    search_options : dict\n        If searcher is str, these arguments are passed to searcher_factory.\n    checkpoint : str\n        If filename given here, a checkpoint of scheduler (and searcher) state\n        is written to file every time a job finishes.\n        Note: May not be fully supported by all searchers.\n    resume : bool\n        If True, scheduler state is loaded from checkpoint, and experiment\n        starts from there.\n        Note: May not be fully supported by all searchers.\n    num_trials : int\n        Maximum number of jobs run in experiment. One of `num_trials`,\n        `time_out` must be given.\n    time_out : float\n        If given, jobs are started only until this time_out (wall clock time).\n        One of `num_trials`, `time_out` must be given.\n    reward_attr : str\n        Name of reward (i.e., metric to maximize) attribute in data obtained\n        from reporter\n    time_attr : str\n        Name of resource (or time) attribute in data obtained from reporter.\n        This attribute is optional for FIFO scheduling, but becomes mandatory\n        in multi-fidelity scheduling (e.g., Hyperband).\n        Note: The type of resource must be int.\n    dist_ip_addrs : list of str\n        IP addresses of remote machines.\n    training_history_callback : callable\n        Callback function func called every time a job finishes, if at least\n        training_history_callback_delta_secs seconds passed since the last\n        recent call. The call has the form:\n            func(self.training_history, self._start_time)\n        Here, self._start_time is time stamp for when experiment started.\n        Use this callback to serialize self.training_history after regular\n        intervals.\n    training_history_callback_delta_secs : float\n        See training_history_callback.\n    delay_get_config : bool\n        If True, the call to searcher.get_config is delayed until a worker\n        resource for evaluation is available. Otherwise, get_config is called\n        just after a job has been started.\n        For searchers which adapt to past data, True should be preferred.\n        Otherwise, it does not matter.\n\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> import autogluon as ag\n    >>> @ag.args(\n    ...     lr=ag.space.Real(1e-3, 1e-2, log=True),\n    ...     wd=ag.space.Real(1e-3, 1e-2))\n    >>> def train_fn(args, reporter):\n    ...     print(\'lr: {}, wd: {}\'.format(args.lr, args.wd))\n    ...     for e in range(10):\n    ...         dummy_accuracy = 1 - np.power(1.8, -np.random.uniform(e, 2*e))\n    ...         reporter(epoch=e+1, accuracy=dummy_accuracy, lr=args.lr, wd=args.wd)\n    >>> scheduler = ag.scheduler.FIFOScheduler(train_fn,\n    ...                                        resource={\'num_cpus\': 2, \'num_gpus\': 0},\n    ...                                        num_trials=20,\n    ...                                        reward_attr=\'accuracy\',\n    ...                                        time_attr=\'epoch\')\n    >>> scheduler.run()\n    >>> scheduler.join_jobs()\n    >>> scheduler.get_training_curves(plot=True)\n    """"""\n\n    def __init__(self, train_fn, args=None, resource=None,\n                 searcher=None, search_options=None,\n                 checkpoint=None,\n                 resume=False, num_trials=None,\n                 time_out=None, max_reward=None, reward_attr=\'accuracy\',\n                 time_attr=\'epoch\',\n                 visualizer=\'none\', dist_ip_addrs=None,\n                 training_history_callback=None,\n                 training_history_callback_delta_secs=60,\n                 delay_get_config=True):\n        super().__init__(dist_ip_addrs)\n        if resource is None:\n            resource = {\'num_cpus\': 1, \'num_gpus\': 0}\n        self.resource = resource\n\n        if searcher is None:\n            searcher = \'random\'  # RandomSearcher\n        if isinstance(searcher, str):\n            if search_options is None:\n                search_options = dict()\n            _search_options = search_options.copy()\n            _search_options[\'configspace\'] = train_fn.cs\n            _search_options[\'reward_attribute\'] = reward_attr\n            # Adjoin scheduler info to search_options, if not already done by\n            # subclass\n            if \'scheduler\' not in _search_options:\n                _search_options[\'scheduler\'] = \'fifo\'\n            self.searcher: BaseSearcher = searcher_factory(\n                searcher, **_search_options)\n        else:\n            assert isinstance(searcher, BaseSearcher)\n            self.searcher: BaseSearcher = searcher\n\n        assert isinstance(train_fn, _autogluon_method)\n        self.train_fn = train_fn\n        self.args = args if args else train_fn.args\n        if num_trials is None:\n            assert time_out is not None, \\\n                ""Need stopping criterion: Either num_trials or time_out""\n        self.num_trials = num_trials\n        self.time_out = time_out\n        self.max_reward = max_reward\n        # meta data\n        self.metadata = {\n            \'search_space\': train_fn.kwspaces,\n            \'search_strategy\': searcher,\n            \'stop_criterion\': {\n                \'time_limits\': time_out, \'max_reward\': max_reward},\n            \'resources_per_trial\': resource}\n\n        self._checkpoint = checkpoint\n        self._reward_attr = reward_attr\n        self._time_attr = time_attr\n        self.visualizer = visualizer.lower()\n        if self.visualizer == \'tensorboard\' or self.visualizer == \'mxboard\':\n            assert checkpoint is not None, \\\n                ""Need checkpoint to be set""\n            try_import_mxboard()\n            from mxboard import SummaryWriter\n            self.mxboard = SummaryWriter(\n                logdir=os.path.join(os.path.splitext(checkpoint)[0], \'logs\'),\n                flush_secs=3,\n                verbose=False\n            )\n\n        self._fifo_lock = mp.Lock()\n        # training_history maintains the complete history of the experiment,\n        # in terms of all results obtained from the reporter. Keys are\n        # str(task.task_id)\n        self.training_history = OrderedDict()\n        self.config_history = OrderedDict()\n        # Needed for training_history callback mechanism, which is used to\n        # serialize training_history after each\n        # training_history_call_delta_secs seconds\n        self._start_time = None\n        self._training_history_callback_last_block = None\n        self._training_history_callback_last_len = None\n        self.training_history_callback = training_history_callback\n        self.training_history_callback_delta_secs = \\\n            training_history_callback_delta_secs\n        self._delay_get_config = delay_get_config\n        # Resume experiment from checkpoint?\n        if resume:\n            assert checkpoint is not None, \\\n                ""Need checkpoint to be set if resume = True""\n            if os.path.isfile(checkpoint):\n                self.load_state_dict(load(checkpoint))\n            else:\n                msg = f\'checkpoint path {checkpoint} is not available for resume.\'\n                logger.exception(msg)\n                raise FileExistsError(msg)\n\n    def run(self, **kwargs):\n        """"""Run multiple number of trials\n        """"""\n        # Make sure that this scheduler is configured at the searcher\n        self.searcher.configure_scheduler(self)\n        start_time = time.time()\n        self._start_time = start_time\n        num_trials = kwargs.get(\'num_trials\', self.num_trials)\n        time_out = kwargs.get(\'time_out\', self.time_out)\n        # For training_history callback mechanism:\n        self._training_history_callback_last_block = -1\n        self._training_history_callback_last_len = len(self.training_history)\n\n        logger.info(\'Starting Experiments\')\n        logger.info(f\'Num of Finished Tasks is {self.num_finished_tasks}\')\n        if num_trials is not None:\n            logger.info(f\'Num of Pending Tasks is {num_trials - self.num_finished_tasks}\')\n            tbar = tqdm(range(self.num_finished_tasks, num_trials))\n        else:\n            # In this case, only stopping by time_out is used. We do not display\n            # a progress bar then\n            tbar = range(self.num_finished_tasks, 100000)\n        if time_out is not None:\n            logger.info(f\'Time out (secs) is {time_out}\')\n        for _ in tbar:\n            if (time_out and time.time() - start_time >= time_out) or \\\n                    (self.max_reward and self.get_best_reward() >= self.max_reward):\n                break\n            self.schedule_next()\n\n    def save(self, checkpoint=None):\n        """"""Save Checkpoint\n        """"""\n        if checkpoint is None:\n            checkpoint = self._checkpoint\n        if checkpoint is not None:\n            mkdir(os.path.dirname(checkpoint))\n            save(self.state_dict(), checkpoint)\n\n    def _create_new_task(self, config, resources=None):\n        if resources is None:\n            resources = DistributedResource(**self.resource)\n        return Task(\n            self.train_fn, {\'args\': self.args, \'config\': config},\n            resources=resources)\n\n    def schedule_next(self):\n        """"""Schedule next searcher suggested task\n        """"""\n        resources = DistributedResource(**self.resource)\n        if self._delay_get_config:\n            # Wait for available resource here, instead of in add_job. This\n            # delays the get_config call until a resource is available\n            FIFOScheduler.resource_manager._request(resources)\n\n        # Allow for the promotion of a previously chosen config. Also,\n        # extra_kwargs contains extra info passed to both add_job and to\n        # get_config (if no config is promoted)\n        config, extra_kwargs = self._promote_config()\n        # Time stamp to be used in get_config, and maybe in add_job\n        extra_kwargs[\'elapsed_time\'] = self._elapsed_time()\n        if config is None:\n            # No config to promote: Query next config to evaluate from searcher\n            config = self.searcher.get_config(**extra_kwargs)\n            extra_kwargs[\'new_config\'] = True\n        else:\n            # This is not a new config, but a paused one which is now promoted\n            extra_kwargs[\'new_config\'] = False\n        task = self._create_new_task(config, resources=resources)\n        self.add_job(task, **extra_kwargs)\n\n    def run_with_config(self, config):\n        """"""Run with config for final fit.\n        It launches a single training trial under any fixed values of the hyperparameters.\n        For example, after HPO has identified the best hyperparameter values based on a hold-out dataset,\n        one can use this function to retrain a model with the same hyperparameters on all the available labeled data\n        (including the hold out set). It can also returns other objects or states.\n        """"""\n        task = self._create_new_task(config)\n        reporter = FakeReporter()\n        task.args[\'reporter\'] = reporter\n        return self.run_job(task)\n\n    def _dict_from_task(self, task):\n        if isinstance(task, Task):\n            return {\'TASK_ID\': task.task_id, \'Config\': task.args[\'config\']}\n        else:\n            assert isinstance(task, dict)\n            return {\'TASK_ID\': task[\'TASK_ID\'], \'Config\': task[\'Config\']}\n\n    def add_job(self, task, **kwargs):\n        """"""Adding a training task to the scheduler.\n\n        Args:\n            task (:class:`autogluon.scheduler.Task`): a new training task\n\n        Relevant entries in kwargs:\n            - bracket: HB bracket to be used. Has been sampled in _promote_config\n            - new_config: If True, task starts new config eval, otherwise it promotes\n              a config (only if type == \'promotion\')\n\n        Only if new_config == False:\n            - config_key: Internal key for config\n            - resume_from: config promoted from this milestone\n            - milestone: config promoted to this milestone (next from resume_from)\n        """"""\n        cls = FIFOScheduler\n        if not self._delay_get_config:\n            # Wait for resource to become available here, as this has not happened\n            # in schedule_next before\n            cls.resource_manager._request(task.resources)\n        # reporter\n        reporter = DistStatusReporter(remote=task.resources.node)\n        task.args[\'reporter\'] = reporter\n        # Register pending evaluation\n        self.searcher.register_pending(task.args[\'config\'])\n        # main process\n        job = cls._start_distributed_job(task, cls.resource_manager)\n        # reporter thread\n        rp = threading.Thread(\n            target=self._run_reporter,\n            args=(task, job, reporter),\n            daemon=False)\n        rp.start()\n        task_dict = self._dict_from_task(task)\n        task_dict.update({\'Task\': task, \'Job\': job, \'ReporterThread\': rp})\n        # Checkpoint thread. This is also used for training_history\n        # callback\n        if self._checkpoint is not None or \\\n                self.training_history_callback is not None:\n            self._add_checkpointing_to_job(job)\n        with self.LOCK:\n            self.scheduled_tasks.append(task_dict)\n\n    def _clean_task_internal(self, task_dict):\n        task_dict[\'ReporterThread\'].join()\n\n    def _add_checkpointing_to_job(self, job):\n        def _save_checkpoint_callback(fut):\n            self._cleaning_tasks()\n            self.save()\n            # training_history callback\n            with self._fifo_lock:\n                if self._trigger_training_history_callback():\n                    logger.debug(""Execute training_history callback"")\n                    self.training_history_callback(\n                        self.training_history, self._start_time)\n\n        job.add_done_callback(_save_checkpoint_callback)\n\n    def _trigger_training_history_callback(self):\n        if self.training_history_callback is None:\n            return False\n        assert self._training_history_callback_last_block is not None\n        current_block = int(np.floor(\n            self._elapsed_time() / self.training_history_callback_delta_secs))\n        current_len = len(self.training_history)\n        ret_val = (current_block >\n                   self._training_history_callback_last_block) and \\\n            current_len > self._training_history_callback_last_len\n        if ret_val:\n            self._training_history_callback_last_block = current_block\n            self._training_history_callback_last_len = current_len\n        return ret_val\n\n    def _run_reporter(self, task, task_job, reporter):\n        last_result = None\n        while not task_job.done():\n            reported_result = reporter.fetch()\n            if \'traceback\' in reported_result:\n                # Evaluation has failed\n                logger.exception(reported_result[\'traceback\'])\n                self.searcher.evaluation_failed(\n                    config=task.args[\'config\'], **reported_result)\n                reporter.move_on()\n                break\n            if reported_result.get(\'done\', False):\n                reporter.move_on()\n                break\n            if len(reported_result) == 0:\n                # An empty dict should just be skipped\n                logger.warning(""Skipping empty dict received from reporter"")\n                continue\n            # Time since start of experiment\n            elapsed_time = self._elapsed_time()\n            reported_result[\'time_since_start\'] = elapsed_time\n\n            # Extra information from searcher (optional)\n            dataset_size = self.searcher.dataset_size()\n            if dataset_size > 0:\n                reported_result[\'searcher_data_size\'] = dataset_size\n            for k, v in self.searcher.cumulative_profile_record().items():\n                reported_result[\'searcher_profile_\' + k] = v\n            for k, v in self.searcher.model_parameters().items():\n                reported_result[\'searcher_params_\' + k] = v\n            self._add_training_result(\n                task.task_id, reported_result, config=task.args[\'config\'])\n            reporter.move_on()\n            last_result = reported_result\n        # Pass all of last_result to searcher\n        if last_result is not None:\n            self.searcher.update(config=task.args[\'config\'], **last_result)\n\n    def _promote_config(self):\n        """"""\n        Provides a hook in schedule_next, which allows to promote a config\n        which has been selected and partially evaluated previously.\n\n        :return: config, extra_args\n        """"""\n        config = None\n        extra_args = dict()\n        return config, extra_args\n\n    def _elapsed_time(self):\n        """"""\n        :return: Time elapsed since start of experiment (see \'run\')\n        """"""\n        assert self._start_time is not None, \\\n            ""Experiment has not been started yet""\n        return time.time() - self._start_time\n\n    def get_best_config(self):\n        """"""Get the best configuration from the finished jobs.\n        """"""\n        return self.searcher.get_best_config()\n\n    def get_best_reward(self):\n        """"""Get the best reward from the finished jobs.\n        """"""\n        return self.searcher.get_best_reward()\n\n    def _add_training_result(self, task_id, reported_result, config=None):\n        if self.visualizer == \'mxboard\' or self.visualizer == \'tensorboard\':\n            if \'loss\' in reported_result:\n                self.mxboard.add_scalar(\n                    tag=\'loss\',\n                    value=(\n                        f\'task {task_id} valid_loss\',\n                        reported_result[\'loss\']\n                    ),\n                    global_step=reported_result[self._reward_attr]\n                )\n            self.mxboard.add_scalar(\n                tag=self._reward_attr,\n                value=(\n                    f\'task {task_id} {self._reward_attr}\',\n                    reported_result[self._reward_attr]\n                ),\n                global_step=reported_result[self._reward_attr]\n            )\n        with self._fifo_lock:\n            # Note: We store all of reported_result in training_history[task_id],\n            # not just the reward value.\n            task_key = str(task_id)\n            new_entry = copy.copy(reported_result)\n            if task_key in self.training_history:\n                self.training_history[task_key].append(new_entry)\n            else:\n                self.training_history[task_key] = [new_entry]\n                if config:\n                    self.config_history[task_key] = config\n\n    def get_training_curves(self, filename=None, plot=False, use_legend=True):\n        """"""Get Training Curves\n\n        Parameters\n        ----------\n            filename : str\n            plot : bool\n            use_legend : bool\n\n        Examples\n        --------\n        >>> scheduler.run()\n        >>> scheduler.join_jobs()\n        >>> scheduler.get_training_curves(plot=True)\n\n            .. image:: https://github.com/zhanghang1989/AutoGluonWebdata/blob/master/doc/api/autogluon.1.png?raw=true\n        """"""\n        if filename is None and not plot:\n            logger.warning(\'Please either provide filename or allow plot in get_training_curves\')\n        import matplotlib.pyplot as plt\n        plt.ylabel(self._reward_attr)\n        plt.xlabel(self._time_attr)\n        plt.title(""Performance vs Training-Time in each HPO Trial"")\n        with self._fifo_lock:\n            for task_id, task_res in self.training_history.items():\n                rewards = [x[self._reward_attr] for x in task_res]\n                x = list(range(len(task_res)))\n                plt.plot(x, rewards, label=f\'task {task_id}\')\n        if use_legend:\n            plt.legend(loc=\'best\')\n        if filename:\n            logger.info(f\'Saving Training Curve in {filename}\')\n            plt.savefig(filename)\n        if plot:\n            plt.show()\n\n    def state_dict(self, destination=None):\n        """"""\n        Returns a dictionary containing a whole state of the Scheduler. This is\n        used for checkpointing.\n\n        Note that the checkpoint only contains information which has been\n        registered at scheduler and searcher. It does not contain information\n        about currently running jobs, except what they reported before the\n        checkpoint.\n        Therefore, resuming an experiment from a checkpoint is slightly\n        different from continuing the experiment past the checkpoint. The\n        former behaves as if all currently running jobs are terminated at\n        the checkpoint, and new jobs are scheduled from there, starting from\n        scheduler and searcher state according to all information recorded\n        until the checkpoint.\n\n        Examples\n        --------\n        >>> ag.save(scheduler.state_dict(), \'checkpoint.ag\')\n        """"""\n        destination = super().state_dict(destination)\n        with self._fifo_lock:\n            # The result of searcher.get_state can always be pickled\n            destination[\'searcher\'] = pickle.dumps(self.searcher.get_state())\n            destination[\'training_history\'] = json.dumps(self.training_history)\n            destination[\'config_history\'] = json.dumps(self.config_history)\n        if self.visualizer == \'mxboard\' or self.visualizer == \'tensorboard\':\n            destination[\'visualizer\'] = json.dumps(self.mxboard._scalar_dict)\n        return destination\n\n    def load_state_dict(self, state_dict):\n        """"""\n        Load from the saved state dict. This can be used to resume an\n        experiment from a checkpoint (see \'state_dict\' for caveats).\n\n        This method must only be called as part of scheduler construction.\n        Calling it in the middle of an experiment can lead to an undefined\n        inner state of scheduler or searcher.\n\n        Examples\n        --------\n        >>> scheduler.load_state_dict(ag.load(\'checkpoint.ag\'))\n        """"""\n        super().load_state_dict(state_dict)\n        with self._fifo_lock:\n            self.searcher = self.searcher.clone_from_state(\n                pickle.loads(state_dict[\'searcher\']))\n            self.training_history = json.loads(state_dict[\'training_history\'])\n            self.config_history = json.loads(state_dict[\'config_history\'])\n        if self.visualizer == \'mxboard\' or self.visualizer == \'tensorboard\':\n            self.mxboard._scalar_dict = json.loads(state_dict[\'visualizer\'])\n        logger.debug(f\'Loading Searcher State {self.searcher}\')\n'"
autogluon/scheduler/hyperband.py,0,"b'import pickle\nimport logging\nimport threading\nimport numpy as np\nimport multiprocessing as mp\nimport os\n\nfrom .fifo import FIFOScheduler\nfrom .hyperband_stopping import HyperbandStopping_Manager\nfrom .hyperband_promotion import HyperbandPromotion_Manager\nfrom .reporter import DistStatusReporter\nfrom ..utils import load\n\n__all__ = [\'HyperbandScheduler\']\n\nlogger = logging.getLogger(__name__)\n\n\nclass HyperbandScheduler(FIFOScheduler):\n    r""""""Implements different variants of asynchronous Hyperband\n\n    See \'type\' for the different variants. One implementation detail is when\n    using multiple brackets, task allocation to bracket is done randomly \n    based on a softmax probability.\n\n    Note: This scheduler requires both reward and resource (time) to be\n    returned by the reporter. Here, resource (time) values must be positive\n    int. If time_attr == \'epoch\', this should be the number of epochs done,\n    starting from 1 (not the epoch number, starting from 0).\n\n    Parameters\n    ----------\n    train_fn : callable\n        A task launch function for training.\n    args : object, optional\n        Default arguments for launching train_fn.\n    resource : dict\n        Computation resources.  For example, `{\'num_cpus\':2, \'num_gpus\':1}`\n    searcher : str or BaseSearcher\n        Searcher (get_config decisions). If str, this is passed to\n        searcher_factory along with search_options.\n    search_options : dict\n        If searcher is str, these arguments are passed to searcher_factory.\n    checkpoint : str\n        If filename given here, a checkpoint of scheduler (and searcher) state\n        is written to file every time a job finishes.\n        Note: May not be fully supported by all searchers.\n    resume : bool\n        If True, scheduler state is loaded from checkpoint, and experiment\n        starts from there.\n        Note: May not be fully supported by all searchers.\n    num_trials : int\n        Maximum number of jobs run in experiment. One of `num_trials`,\n        `time_out` must be given.\n    time_out : float\n        If given, jobs are started only until this time_out (wall clock time).\n        One of `num_trials`, `time_out` must be given.\n    reward_attr : str\n        Name of reward (i.e., metric to maximize) attribute in data obtained\n        from reporter\n    time_attr : str\n        Name of resource (or time) attribute in data obtained from reporter.\n        Note: The type of resource must be positive int.\n    max_t : int\n        Maximum resource (see time_attr) to be used for a job. Together with\n        grace_period and reduction_factor, this is used to determine rung\n        levels in Hyperband brackets.\n    grace_period : int\n        Minimum resource (see time_attr) to be used for a job.\n    reduction_factor : int (>= 2)\n        Parameter to determine rung levels in successive halving (Hyperband).\n    brackets : int\n        Number of brackets to be used in Hyperband. Each bracket has a different\n        grace period, all share max_t and reduction_factor.\n        If brackets == 1, we just run successive halving.\n    training_history_callback : callable\n        Callback function func called every time a job finishes, if at least\n        training_history_callback_delta_secs seconds passed since the last\n        recent call. The call has the form:\n            func(self.training_history, self._start_time)\n        Here, self._start_time is time stamp for when experiment started.\n        Use this callback to serialize self.training_history after regular\n        intervals.\n    training_history_callback_delta_secs : float\n        See training_history_callback.\n    delay_get_config : bool\n        If True, the call to searcher.get_config is delayed until a worker\n        resource for evaluation is available. Otherwise, get_config is called\n        just after a job has been started.\n        For searchers which adapt to past data, True should be preferred.\n        Otherwise, it does not matter.\n    type : str\n        Type of Hyperband scheduler:\n            stopping:\n                See :class:`HyperbandStopping_Manager`. Tasks and config evals are\n                tightly coupled. A task is stopped at a milestone if worse than\n                most others, otherwise it continues. As implemented in Ray/Tune:\n                https://ray.readthedocs.io/en/latest/tune-schedulers.html#asynchronous-hyperband\n            promotion:\n                See :class:`HyperbandPromotion_Manager`. A config eval may be\n                associated with multiple tasks over its lifetime. It is never\n                terminated, but may be paused. Whenever a task becomes available,\n                it may promote a config to the next milestone, if better than most\n                others. If no config can be promoted, a new one is chosen. This\n                variant may benefit from pause&resume, which is not directly\n                supported here. As proposed in this paper (termed ASHA):\n                https://arxiv.org/abs/1810.05934\n    dist_ip_addrs : list of str\n        IP addresses of remote machines.\n    keep_size_ratios : bool\n        Implemented for type \'promotion\' only. If True,\n        promotions are done only if the (current estimate of the) size ratio\n        between rung and next rung are 1 / reduction_factor or better. This\n        avoids higher rungs to get more populated than they would be in\n        synchronous Hyperband. A drawback is that promotions to higher rungs\n        take longer.\n    maxt_pending : bool\n        Relevant only if a model-based searcher is used.\n        If True, register pending config at level max_t\n        whenever a new evaluation is started. This has a direct effect on\n        the acquisition function (for model-based variant), which operates\n        at level max_t. On the other hand, it decreases the variance of the\n        latent process there.\n    searcher_data : str\n        Relevant only if a model-based searcher is used, and if train_fn is such\n        that we receive results (from the reporter) at each successive resource\n        level, not just at the rung levels.\n        Example: For NN tuning and time_attr == \'epoch\', we receive a result for\n        each epoch, but not all epoch values are also rung levels.\n        searcher_data determines which of these results are passed to the\n        searcher. As a rule, the more data the searcher receives, the better its\n        fit, but also the more expensive get_config may become. Choices:\n        - \'rungs\' (default): Only results at rung levels. Cheapest\n        - \'all\': All results. Most expensive\n        - \'rungs_and_last\': Results at rung levels, plus the most recent result.\n            This means that in between rung levels, only the most recent result\n            is used by the searcher. This is in between\n\n    See Also\n    --------\n    HyperbandStopping_Manager\n    HyperbandPromotion_Manager\n\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> import autogluon as ag\n    >>> @ag.args(\n    ...     lr=ag.space.Real(1e-3, 1e-2, log=True),\n    ...     wd=ag.space.Real(1e-3, 1e-2))\n    >>> def train_fn(args, reporter):\n    ...     print(\'lr: {}, wd: {}\'.format(args.lr, args.wd))\n    ...     for e in range(10):\n    ...         dummy_accuracy = 1 - np.power(1.8, -np.random.uniform(e, 2*e))\n    ...         reporter(epoch=e+1, accuracy=dummy_accuracy, lr=args.lr, wd=args.wd)\n    >>> scheduler = ag.scheduler.HyperbandScheduler(train_fn,\n    ...                                             resource={\'num_cpus\': 2, \'num_gpus\': 0},\n    ...                                             num_trials=20,\n    ...                                             reward_attr=\'accuracy\',\n    ...                                             time_attr=\'epoch\',\n    ...                                             grace_period=1)\n    >>> scheduler.run()\n    >>> scheduler.join_jobs()\n    >>> scheduler.get_training_curves(plot=True)\n    """"""\n    def __init__(self, train_fn, args=None, resource=None,\n                 searcher=None, search_options=None,\n                 checkpoint=None,\n                 resume=False, num_trials=None,\n                 time_out=None, max_reward=None,\n                 reward_attr=""accuracy"",\n                 time_attr=""epoch"",\n                 max_t=50, grace_period=1,\n                 reduction_factor=3, brackets=1,\n                 visualizer=\'none\',\n                 training_history_callback=None,\n                 training_history_callback_delta_secs=60,\n                 delay_get_config=True,\n                 type=\'stopping\',\n                 dist_ip_addrs=None,\n                 keep_size_ratios=False,\n                 maxt_pending=False,\n                 searcher_data=\'rungs\',\n                 do_snapshots=False):\n        # Adjoin information about scheduler to search_options\n        if search_options is None:\n            _search_options = dict()\n        else:\n            _search_options = search_options.copy()\n        _search_options[\'scheduler\'] = \'hyperband_{}\'.format(type)\n        _search_options[\'resource_attribute\'] = time_attr\n        _search_options[\'min_epochs\'] = grace_period\n        _search_options[\'max_epochs\'] = max_t\n        # Pass resume=False here. Resume needs members of this object to be\n        # created\n        super().__init__(\n            train_fn=train_fn, args=args, resource=resource, searcher=searcher,\n            search_options=_search_options, checkpoint=checkpoint, resume=False,\n            num_trials=num_trials, time_out=time_out, max_reward=max_reward,\n            reward_attr=reward_attr, time_attr=time_attr,\n            visualizer=visualizer, dist_ip_addrs=dist_ip_addrs,\n            training_history_callback=training_history_callback,\n            training_history_callback_delta_secs=training_history_callback_delta_secs,\n            delay_get_config=delay_get_config)\n        self.max_t = max_t\n        self.reduction_factor = reduction_factor\n        self.type = type\n        self.maxt_pending = maxt_pending\n        if type == \'stopping\':\n            self.terminator = HyperbandStopping_Manager(\n                time_attr, reward_attr, max_t, grace_period, reduction_factor,\n                brackets)\n        elif type == \'promotion\':\n            assert not do_snapshots, \\\n                ""Snapshots are supported only for type = \'stopping\'""\n            self.terminator = HyperbandPromotion_Manager(\n                time_attr, reward_attr, max_t, grace_period, reduction_factor,\n                brackets, keep_size_ratios=keep_size_ratios)\n        else:\n            raise AssertionError(\n                ""type \'{}\' not supported, must be \'stopping\' or \'promotion\'"".format(\n                    type))\n        self.do_snapshots = do_snapshots\n        sd_values = {\'rungs\', \'all\', \'rungs_and_last\'}\n        assert searcher_data in sd_values, \\\n            ""searcher_data = \'{}\', must be in {}"".format(\n                searcher_data, sd_values)\n        self.searcher_data = searcher_data\n        # Maintains a snapshot of currently running tasks, needed by several\n        # features (for example, searcher_data == \'rungs_and_last\', or for\n        # providing a snapshot to the searcher).\n        # Maps str(task_id) to dict, with fields:\n        # - config\n        # - time_stamp: Time when task was started, or when last recent\n        #       result was reported\n        # - reported_result: Last recent reported result, or None (task was\n        #       started, but did not report anything yet.\n        #       Note: Only contains attributes self._reward_attr and\n        #       self._time_attr).\n        # - bracket: Bracket number\n        # - keep_case: See _run_reporter\n        self._running_tasks = dict()\n        # This lock protects both _running_tasks and terminator, the latter\n        # does not define its own lock\n        self._hyperband_lock = mp.Lock()\n        if resume:\n            assert checkpoint is not None, \\\n                ""Need checkpoint to be set if resume = True""\n            if os.path.isfile(checkpoint):\n                self.load_state_dict(load(checkpoint))\n            else:\n                msg = f\'checkpoint path {checkpoint} is not available for resume.\'\n                logger.exception(msg)\n                raise FileExistsError(msg)\n\n    def add_job(self, task, **kwargs):\n        """"""Adding a training task to the scheduler.\n\n        Args:\n            task (:class:`autogluon.scheduler.Task`): a new training task\n\n        Relevant entries in kwargs:\n        - bracket: HB bracket to be used. Has been sampled in _promote_config\n        - new_config: If True, task starts new config eval, otherwise it promotes\n          a config (only if type == \'promotion\')\n        Only if new_config == False:\n        - config_key: Internal key for config\n        - resume_from: config promoted from this milestone\n        - milestone: config promoted to this milestone (next from resume_from)\n        """"""\n        cls = HyperbandScheduler\n        if not self._delay_get_config:\n            # Wait for resource to become available here, as this has not happened\n            # in schedule_next before\n            cls.resource_manager._request(task.resources)\n        # reporter and terminator\n        reporter = DistStatusReporter(remote=task.resources.node)\n        task.args[\'reporter\'] = reporter\n\n        # Register task\n        task_key = str(task.task_id)\n        with self._hyperband_lock:\n            assert task_key not in self._running_tasks, \\\n                ""Task {} is already registered as running"".format(task_key)\n            self._running_tasks[task_key] = {\n                \'config\': task.args[\'config\'],\n                \'time_stamp\': kwargs[\'elapsed_time\'],\n                \'bracket\': kwargs[\'bracket\'],\n                \'reported_result\': None,\n                \'keep_case\': False}\n            milestones = self.terminator.on_task_add(task, **kwargs)\n        if kwargs.get(\'new_config\', True):\n            first_milestone = milestones[-1]\n            logger.debug(""Adding new task (first milestone = {}):\\n{}"".format(\n                first_milestone, task))\n            self.searcher.register_pending(\n                task.args[\'config\'], milestone=first_milestone)\n            if self.maxt_pending:\n                # Also introduce pending evaluation for resource max_t\n                final_milestone = milestones[0]\n                if final_milestone != first_milestone:\n                    self.searcher.register_pending(\n                        task.args[\'config\'], milestone=final_milestone)\n        else:\n            # Promotion of config\n            # This is a signal towards train_fn, in case it supports\n            # pause and resume:\n            task.args[\'resume_from\'] = kwargs[\'resume_from\']\n            next_milestone = kwargs[\'milestone\']\n            logger.debug(""Promotion task (next milestone = {}):\\n{}"".format(\n                next_milestone, task))\n            self.searcher.register_pending(\n                task.args[\'config\'], milestone=next_milestone)\n\n        # main process\n        job = cls._start_distributed_job(task, cls.resource_manager)\n        # reporter thread\n        rp = threading.Thread(\n            target=self._run_reporter,\n            args=(task, job, reporter),\n            daemon=False)\n        rp.start()\n        task_dict = self._dict_from_task(task)\n        task_dict.update({\'Task\': task, \'Job\': job, \'ReporterThread\': rp})\n        # Checkpoint thread. This is also used for training_history\n        # callback\n        if self._checkpoint is not None or \\\n                self.training_history_callback is not None:\n            self._add_checkpointing_to_job(job)\n        with self.LOCK:\n            self.scheduled_tasks.append(task_dict)\n\n    def _update_searcher(self, task, result):\n        config = task.args[\'config\']\n        if self.searcher_data == \'rungs_and_last\':\n            with self._hyperband_lock:\n                task_info = self._running_tasks[str(task.task_id)]\n                if task_info[\'reported_result\'] is not None:\n                    # Remove last recently added result for this task,\n                    # unless it fell on a rung level\n                    if not task_info[\'keep_case\']:\n                        rem_result = task_info[\'reported_result\']\n                        self.searcher.remove_case(config, **rem_result)\n        self.searcher.update(config, **result)\n\n    def _run_reporter(self, task, task_job, reporter):\n        last_result = None\n        last_updated = None\n        task_key = str(task.task_id)\n        while not task_job.done():\n            reported_result = reporter.fetch()\n            if \'traceback\' in reported_result:\n                # Evaluation has failed\n                logger.exception(reported_result[\'traceback\'])\n                self.searcher.evaluation_failed(\n                    config=task.args[\'config\'], **reported_result)\n                reporter.move_on()\n                with self._hyperband_lock:\n                    self.terminator.on_task_remove(task)\n                break\n            if reported_result.get(\'done\', False):\n                reporter.move_on()\n                with self._hyperband_lock:\n                    if last_result is not None:\n                        self.terminator.on_task_complete(task, last_result)\n                    # Cleanup\n                    del self._running_tasks[task_key]\n                break\n            if len(reported_result) == 0:\n                # An empty dict should just be skipped\n                if self.searcher.debug_log is not None:\n                    logger.info(""Skipping empty dict received from reporter"")\n                continue\n            # Time since start of experiment\n            elapsed_time = self._elapsed_time()\n            reported_result[\'time_since_start\'] = elapsed_time\n\n            # Call before _add_training_results, since we may be able to report\n            # extra information from the bracket\n            with self._hyperband_lock:\n                task_info = self.terminator.on_task_report(\n                    task, reported_result)\n            task_continues = task_info[\'task_continues\']\n            update_searcher = task_info[\'update_searcher\']\n            # Append extra information to reported_result\n            reported_result[\'bracket\'] = task_info[\'bracket_id\']\n            if \'rung_counts\' in task_info:\n                for k, v in task_info[\'rung_counts\'].items():\n                    key = \'count_at_{}\'.format(k)\n                    reported_result[key] = v\n            dataset_size = self.searcher.dataset_size()\n            if dataset_size > 0:\n                reported_result[\'searcher_data_size\'] = dataset_size\n            for k, v in self.searcher.cumulative_profile_record().items():\n                reported_result[\'searcher_profile_\' + k] = v\n            for k, v in self.searcher.model_parameters().items():\n                reported_result[\'searcher_params_\' + k] = v\n            self._add_training_result(\n                task.task_id, reported_result, config=task.args[\'config\'])\n\n            if self.searcher_data == \'rungs\':\n                # Only results on rung levels are reported to the searcher\n                if task_continues and update_searcher:\n                    # Update searcher with intermediate result\n                    # Note: If task_continues is False here, we also call\n                    # searcher.update, but outside the loop.\n                    self._update_searcher(task, reported_result)\n                    last_updated = reported_result\n                    next_milestone = task_info.get(\'next_milestone\')\n                    if next_milestone is not None:\n                        self.searcher.register_pending(\n                            task.args[\'config\'], milestone=next_milestone)\n            elif not task_info.get(\'ignore_data\', False):\n                # All results are reported to the searcher\n                # When is task_info[\'ignore_data\'] True? See header comment of\n                # HyperbandPromotion_Manager.\n                self._update_searcher(task, reported_result)\n                last_updated = reported_result\n                # Since all results are reported, the next report for this task\n                # will be for resource + 1.\n                # NOTE: This assumes that results are reported for all successive\n                # resource levels (int). If any resource level is skipped,\n                # there may be left-over pending candidates, who will be\n                # removed once the task finishes.\n                if task_continues:\n                    self.searcher.register_pending(\n                        task.args[\'config\'],\n                        milestone=int(reported_result[self._time_attr]) + 1)\n\n            # Change snapshot entry for task\n            # Note: This must not be done above, because what _update_searcher\n            # is doing, depends on the entry *before* its update here.\n            # If searcher_data == \'rungs_and_last\', the result is kept in\n            # the dataset iff update_searcher == True (i.e., we are at a\n            # rung level).\n            with self._hyperband_lock:\n                # Note: reported_result may contain all sorts of extra info.\n                # All we need to maintain in the snapshot are reward and\n                # resource level\n                self._running_tasks[task_key].update({\n                    \'time_stamp\': elapsed_time,\n                    \'reported_result\': {\n                        self._reward_attr: reported_result[self._reward_attr],\n                        self._time_attr: reported_result[self._time_attr]},\n                    \'keep_case\': update_searcher})\n\n            last_result = reported_result\n            if task_continues:\n                debug_log = self.searcher.debug_log\n                if update_searcher and debug_log is not None:\n                    # Debug log output\n                    config_id = debug_log.config_id(task.args[\'config\'])\n                    milestone = int(reported_result[self._time_attr])\n                    next_milestone = task_info[\'next_milestone\']\n                    msg = ""config_id {}: Reaches {}, continues"".format(\n                        config_id, milestone)\n                    if next_milestone is not None:\n                        msg += "" to {}"".format(next_milestone)\n                    logger.info(msg)\n                reporter.move_on()\n            else:\n                # Note: The \'terminated\' signal is sent even in the promotion\n                # variant. It means that the *task* terminates, while the\n                # evaluation of the config is just paused\n                last_result[\'terminated\'] = True\n                debug_log = self.searcher.debug_log\n                if debug_log is not None:\n                    # Debug log output\n                    config_id = debug_log.config_id(task.args[\'config\'])\n                    resource = int(reported_result[self._time_attr])\n                    if self.type == \'stopping\' or resource >= self.max_t:\n                        act_str = \'Terminating\'\n                    else:\n                        act_str = \'Pausing\'\n                    msg = ""config_id {}: {} evaluation at {}"".format(\n                        config_id, act_str, resource)\n                    logger.info(msg)\n                with self._hyperband_lock:\n                    self.terminator.on_task_remove(task)\n                    # Cleanup\n                    del self._running_tasks[task_key]\n                reporter.terminate()\n                break\n\n        # Pass all of last_result to searcher (unless this has already been\n        # done)\n        if last_result is not last_updated:\n            self._update_searcher(task, last_result)\n\n    def state_dict(self, destination=None):\n        """"""Returns a dictionary containing a whole state of the Scheduler\n\n        Examples\n        --------\n        >>> ag.save(scheduler.state_dict(), \'checkpoint.ag\')\n        """"""\n        destination = super().state_dict(destination)\n        # Note: _running_tasks is not part of the state to be checkpointed.\n        # The assumption is that if an experiment is resumed from a\n        # checkpoint, tasks which did not finish at the checkpoint, are not\n        # restarted\n        with self._hyperband_lock:\n            destination[\'terminator\'] = pickle.dumps(self.terminator)\n        return destination\n\n    def load_state_dict(self, state_dict):\n        """"""Load from the saved state dict.\n\n        Examples\n        --------\n        >>> scheduler.load_state_dict(ag.load(\'checkpoint.ag\'))\n        """"""\n        with self._hyperband_lock:\n            assert len(self._running_tasks) == 0, \\\n                ""load_state_dict must only be called as part of scheduler construction""\n            super().load_state_dict(state_dict)\n            # Note: _running_tasks is empty from __init__, it is not recreated,\n            # since running tasks are not part of the checkpoint\n            self.terminator = pickle.loads(state_dict[\'terminator\'])\n            logger.info(\'Loading Terminator State {}\'.format(self.terminator))\n\n    def _snapshot_tasks(self, bracket_id):\n        return {\n            k: {\'config\': v[\'config\'],\n                \'time\': v[\'time_stamp\'],\n                \'level\': 0 if v[\'reported_result\'] is None\n                else v[\'reported_result\'][self._time_attr]}\n            for k, v in self._running_tasks.items()\n            if v[\'bracket\'] == bracket_id}\n\n    # Snapshot (in extra_kwargs[\'snapshot\']):\n    # - max_resource\n    # - reduction_factor\n    # - tasks: Info about running tasks in bracket bracket_id:\n    #   dict(task_id) -> dict:\n    #   - config: config as dict\n    #   - time: Time when task was started, or when last recent result was\n    #     reported\n    #   - level: Level of last recent result report, or 0 if no reports yet\n    # - rungs: Metric values at rung levels in bracket bracket_id:\n    #   List of (rung_level, metric_dict), where metric_dict has entries\n    #   task_id: metric_value.\n    def _promote_config(self):\n        with self._hyperband_lock:\n            config, extra_kwargs = self.terminator.on_task_schedule()\n            if self.do_snapshots:\n                # Append snapshot\n                bracket_id = extra_kwargs[\'bracket\']\n                extra_kwargs[\'snapshot\'] = {\n                    \'tasks\': self._snapshot_tasks(bracket_id),\n                    \'rungs\': self.terminator.snapshot_rungs(bracket_id),\n                    \'max_resource\': self.max_t,\n                    \'reduction_factor\': self.reduction_factor}\n            debug_log = self.searcher.debug_log\n            if (debug_log is not None) and (config is not None):\n                # Debug log output\n                config_id = debug_log.config_id(config)\n                msg = ""config_id {}: Promotion from {} to {}"".format(\n                    config_id, extra_kwargs[\'resume_from\'],\n                    extra_kwargs[\'milestone\'])\n                logger.info(msg)\n            return config, extra_kwargs\n\n    def map_resource_to_index(self):\n        def fun(resource):\n            assert 0.0 <= resource <= 1.0, \\\n                ""resource must be in [0, 1]""\n            return self.terminator.resource_to_index(resource)\n\n        return fun\n\n    def __repr__(self):\n        reprstr = self.__class__.__name__ + \'(\' +  \\\n            \'terminator: \' + str(self.terminator)\n        return reprstr\n'"
autogluon/scheduler/hyperband_promotion.py,0,"b'import logging\nimport numpy as np\nimport heapq\nimport copy\n\nfrom .hyperband_stopping import map_resource_to_index, _sample_bracket\n\nlogger = logging.getLogger(__name__)\n\n\nclass HyperbandPromotion_Manager(object):\n    """"""Hyperband Manager\n    \n    Implements both the promotion and stopping logic for an asynchronous\n    variant of Hyperband, known as ASHA:\n    https://arxiv.org/abs/1810.05934\n\n    In ASHA, configs sit paused at milestones (rung levels) in their\n    bracket, until they get promoted, which means that a free task picks\n    up their evaluation until the next milestone.\n\n    We do not directly support pause & resume here, so that in general,\n    the evaluation for a promoted config is started from scratch. However,\n    see Hyperband_Scheduler.add_task, task.args[\'resume_from\']: the\n    evaluation function receives info about the promotion, so pause &\n    resume can be implemented there.\n\n    Note: If the evaluation function does not implement pause & resume, it\n    needs to start training from scratch, in which case metrics are reported\n    for every epoch, also those < task.args[\'resume_from\']. At least for some\n    modes of fitting the searcher model to data, this would lead to duplicate\n    target values for the same extended config (x, r), which we want to avoid.\n    The solution is to maintain task.args[\'resume_from\'] in the data for the\n    terminator (see PromotionBracket._running). Given this, we can report\n    in on_task_report that the current metric data should not be used for the\n    searcher model (ignore_data = True), namely as long as the evaluation has\n    not yet gone beyond level task.args[\'resume_from\'].\n\n    Args:\n        time_attr : str\n            See HyperbandScheduler.\n        reward_attr : str\n            See HyperbandScheduler.\n        max_t : int\n            See HyperbandScheduler.\n        grace_period : int\n            See HyperbandScheduler.\n        reduction_factor : int\n            See HyperbandScheduler.\n        brackets : int\n            See HyperbandScheduler.\n        keep_size_ratios : bool\n            See HyperbandScheduler.\n    """"""\n    def __init__(\n            self, time_attr, reward_attr, max_t, grace_period,\n            reduction_factor, brackets, keep_size_ratios):\n        self._reward_attr = reward_attr\n        self._time_attr = time_attr\n        self._reduction_factor = reduction_factor\n        self._max_t = max_t\n        self._min_t = grace_period\n        # Maps str(task_id) -> bracket_id\n        self._task_info = dict()\n        self._brackets = []\n        for s in range(brackets):\n            bracket = PromotionBracket(\n                grace_period, max_t, reduction_factor, s, keep_size_ratios)\n            if not bracket._rungs:\n                break\n            self._brackets.append(bracket)\n\n    def on_task_add(self, task, **kwargs):\n        assert \'bracket\' in kwargs\n        bracket_id = kwargs[\'bracket\']\n        bracket = self._brackets[bracket_id]\n        bracket.on_task_add(task, **kwargs)\n        self._task_info[str(task.task_id)] = bracket_id\n        levels = [x[0] for x in bracket._rungs]\n        if levels[0] < self._max_t:\n            levels.insert(0, self._max_t)\n        return levels\n\n    def _get_bracket(self, task_id):\n        bracket_id = self._task_info[str(task_id)]\n        return self._brackets[bracket_id], bracket_id\n\n    def on_task_report(self, task, result):\n        """"""\n        See docstring of HyperbandStopping_Manager. Additional entries:\n        - rung_counts: Occupancy counts per rung level\n        - ignore_data: If True, the metric value should not be added to the\n          dataset of the searcher\n\n        ignore_data = True iff the task is running a config which has been\n        promoted, and the resource level is <= the rung from where the config\n        was promoted. This happens if the evaluation function does not support\n        pause&resume and has to be started from scratch.\n        """"""\n        action = False\n        update_searcher = True\n        next_milestone = None\n        ignore_data = False\n        bracket, bracket_id = self._get_bracket(task.task_id)\n        rung_counts = bracket.get_rung_counts()\n        if result[self._time_attr] < self._max_t:\n            action, update_searcher, next_milestone, ignore_data = \\\n                bracket.on_result(task, result[self._time_attr],\n                                  result[self._reward_attr])\n        return {\n            \'task_continues\': action,\n            \'update_searcher\': update_searcher,\n            \'next_milestone\': next_milestone,\n            \'bracket_id\': bracket_id,\n            \'rung_counts\': rung_counts,\n            \'ignore_data\': ignore_data}\n\n    def on_task_complete(self, task, result):\n        bracket, _ = self._get_bracket(task.task_id)\n        bracket.on_result(\n            task, result[self._time_attr], result[self._reward_attr])\n        self.on_task_remove(task)\n\n    def on_task_remove(self, task):\n        task_id = task.task_id\n        bracket, _ = self._get_bracket(task_id)\n        bracket.on_task_remove(task)\n        del self._task_info[str(task_id)]\n\n    def _sample_bracket(self):\n        return _sample_bracket(\n            num_brackets=len(self._brackets),\n            max_num_rungs=len(self._brackets[0]._rungs),\n            rf=self._reduction_factor)\n\n    def on_task_schedule(self):\n        # Sample bracket for task to be scheduled\n        bracket_id = self._sample_bracket()\n        extra_kwargs = {\'bracket\': bracket_id}\n        bracket = self._brackets[bracket_id]\n        # Check whether config can be promoted in that bracket\n        config, config_key, milestone, next_milestone = \\\n            bracket.on_task_schedule()\n        if config is not None:\n            extra_kwargs[\'milestone\'] = next_milestone\n            extra_kwargs[\'config_key\'] = config_key\n            extra_kwargs[\'resume_from\'] = milestone\n        else:\n            # First milestone the new config will get to\n            extra_kwargs[\'milestone\'] = bracket.get_first_milestone()\n        return config, extra_kwargs\n\n    def resource_to_index(self, resource):\n        return map_resource_to_index(\n            resource, self._reduction_factor, self._min_t, self._max_t)\n\n    def snapshot_rungs(self, bracket_id):\n        return self._brackets[bracket_id].snapshot_rungs()\n\n    def __repr__(self):\n        reprstr = self.__class__.__name__ + \'(\' + \\\n                  \'reward_attr: \' + self._reward_attr + \\\n                  \', time_attr: \' + self._time_attr + \\\n                  \', reduction_factor: \' + str(self._reduction_factor) + \\\n                  \', max_t: \' + str(self._max_t) + \\\n                  \', brackets: \' + str(self._brackets) + \\\n                  \')\'\n        return reprstr\n\n\nclass PromotionBracket(object):\n    """"""\n    Different to StoppingBracket in hyperband_stopping, reward data at rungs is\n    associated with configs, not with tasks. To avoid having to use config\n    as key, we maintain unique integers as config keys.\n\n    The stopping rule is simple: Per task_id, we record the config key and\n    the milestone the task should be stopped at (it may still continue there,\n    if it directly passes the promotion test).\n    """"""\n    def __init__(\n            self, min_t, max_t, reduction_factor, s, keep_size_ratios):\n        self.rf = reduction_factor\n        self.max_t = max_t\n        self.keep_size_ratios = keep_size_ratios\n        MAX_RUNGS = int(np.log(max_t / min_t) / np.log(self.rf) - s + 1)\n        # The second entry in each tuple in _rungs is a dict mapping\n        # config_key to (reward_value, was_promoted)\n        self._rungs = [(min_t * self.rf ** (k + s), dict())\n                       for k in reversed(range(MAX_RUNGS))]\n        # Note: config_key are positions into _config, cast to str\n        self._config = list()\n        # _running maps str(task_id) to tuples\n        #   (config_key, milestone, resume_from),\n        # which means task task_id runs evaluation of config_key until\n        # time_attr reaches milestone. The resume_from field can be None. If\n        # not, the task is running a config which has been promoted from\n        # rung level resume_from. This info is required for on_result to\n        # properly report ignore_data.\n        self._running = dict()\n        # _count_tasks[m] counts the number of tasks started with target\n        # milestone m. Here, m includes max_t. Used to implement the\n        # keep_size_ratios rule\n        self._count_tasks = dict()\n        for milestone, _ in self._rungs:\n            self._count_tasks[str(milestone)] = 0\n        if self._rungs and max_t > self._rungs[0][0]:\n            self._count_tasks[str(max_t)] = 0\n\n    def _find_promotable_config(self, recorded, config_key=None):\n        """"""\n        Scans the top (1 / self.rf) fraction of recorded (sorted w.r.t. reward\n        value) for config not yet promoted. If config_key is given, the key\n        must also be equal to config_key.\n\n        Note: It would be more efficient to keep the dictionary as a heap,\n        instead of rebuilding it each time.\n\n        :param recorded: Dict to scan\n        :param config_key: See above\n        :return: Key of config if found, otherwise None\n        """"""\n        num_recorded = len(recorded)\n        ret_key = None\n        if num_recorded >= self.rf:\n            # Search for not yet promoted config in the top\n            # 1 / self.rf fraction\n            def filter_pred(k, v):\n                return (not v[1]) and (config_key is None or k == config_key)\n\n            num_top = int(num_recorded / self.rf)\n            top_list = heapq.nlargest(\n                num_top, recorded.items(), key=lambda x: x[1][0])\n            try:\n                ret_key = next(k for k, v in top_list if filter_pred(k, v))\n            except StopIteration:\n                ret_key = None\n        return ret_key\n\n    def _do_skip_promotion(self, milestone, next_milestone):\n        skip_promotion = False\n        if self.keep_size_ratios:\n            count_this = self._count_tasks[str(milestone)]\n            count_next = self._count_tasks[str(next_milestone)]\n            # We skip the promotion if the ideal size ratio is currently\n            # violated. This is a little more permissive than we could be.\n            # Since a promotion increases count_next, we could also use\n            #    (count_next + 1) * self.rf > count_this\n            skip_promotion = (count_next * self.rf > count_this)\n        return skip_promotion\n\n    def on_task_schedule(self):\n        """"""\n        Used to implement _promote_config of scheduler. Searches through rungs\n        to find a config which can be promoted. If one is found, we return the\n        config and other info (config_key, current milestone, milestone to be\n        promoted to).\n        We also mark the config as being promoted at the rung level it sits\n        right now.\n        """"""\n        config_key = None\n        next_milestone = self.max_t\n        milestone = None\n        recorded = None\n        for _milestone, _recorded in self._rungs:\n            config_key = None\n            if _milestone < self.max_t:\n                skip_promotion = self._do_skip_promotion(\n                    _milestone, next_milestone)\n                config_key = self._find_promotable_config(_recorded) \\\n                    if not skip_promotion else None\n            if config_key is not None:\n                recorded = _recorded\n                milestone = _milestone\n                break\n            next_milestone = _milestone\n\n        if config_key is None:\n            # No promotable config in any rung\n            return None, None, None, None\n        else:\n            # Mark config as promoted\n            reward = recorded[config_key][0]\n            assert not recorded[config_key][1]\n            recorded[config_key] = (reward, True)\n            return self._config[int(config_key)], config_key, milestone, \\\n                   next_milestone\n\n    def on_task_add(self, task, **kwargs):\n        """"""\n        Called when new task is started. Depending on kwargs[\'new_config\'],\n        this could start an evaluation (True) or promote an existing config\n        to the next milestone (False). In the latter case, kwargs contains\n        additional information about the promotion.\n        """"""\n        new_config = kwargs.get(\'new_config\', True)\n        if new_config:\n            # New config\n            config_key = str(len(self._config))\n            self._config.append(copy.copy(task.args[\'config\']))\n            # First milestone:\n            milestone = self._rungs[-1][0]\n            resume_from = None\n        else:\n            # Existing config is promoted\n            # Note that self._rungs has already been updated in\n            # on_task_schedule\n            assert \'milestone\' in kwargs\n            assert \'config_key\' in kwargs\n            config_key = kwargs[\'config_key\']\n            assert self._config[int(config_key)] == task.args[\'config\']\n            milestone = kwargs[\'milestone\']\n            resume_from = kwargs.get(\'resume_from\')\n        self._running[str(task.task_id)] = (config_key, milestone, resume_from)\n        self._count_tasks[str(milestone)] += 1\n\n    def on_result(self, task, cur_iter, cur_rew):\n        """"""\n        Decision on whether task may continue (action = True), or should be\n        stopped (action = False).\n        milestone_reached is a flag whether cur_iter coincides with a milestone.\n        If True, next_milestone is the next milestone after cur_iter, or None\n        if there is none.\n\n        :param task: Only need task.task_id\n        :param cur_iter: Current time_attr value of task\n        :param cur_rew: Current reward_attr value of task\n        :return: action, milestone_reached, next_milestone, ignore_data\n        """"""\n        assert cur_rew is not None, \\\n            ""Reward attribute must be a numerical value, not None""\n        task_key = str(task.task_id)\n        action = True\n        milestone_reached = False\n        next_milestone = None\n        milestone = self._running[task_key][1]\n        if cur_iter >= milestone:\n            assert cur_iter == milestone, \\\n                ""cur_iter = {} > {} = milestone. Make sure to report time attributes covering all milestones"".format(\n                    cur_iter, milestone)\n            action = False\n            milestone_reached = True\n            config_key = self._running[task_key][0]\n            assert self._config[int(config_key)] == task.args[\'config\']\n            try:\n                rung_pos = next(i for i, v in enumerate(self._rungs)\n                                if v[0] == milestone)\n                # Register reward at rung level (as not promoted)\n                recorded = self._rungs[rung_pos][1]\n                recorded[config_key] = (cur_rew, False)\n                next_milestone = self._rungs[rung_pos - 1][0] \\\n                    if rung_pos > 0 else self.max_t\n                # Check whether config can be promoted immediately. If so,\n                # we do not have to stop the task\n                if milestone < self.max_t:\n                    skip_promotion = self._do_skip_promotion(\n                        milestone, next_milestone)\n                    if (not skip_promotion) and (self._find_promotable_config(\n                            recorded, config_key=config_key) is not None):\n                        action = True\n                        recorded[config_key] = (cur_rew, True)\n                        self._running[task_key] = (\n                            config_key, next_milestone, None)\n                        self._count_tasks[str(next_milestone)] += 1\n            except StopIteration:\n                # milestone not a rung level. This can happen, in particular\n                # if milestone == self.max_t\n                pass\n        resume_from = self._running[task_key][2]\n        ignore_data = (resume_from is not None) and (cur_iter <= resume_from)\n\n        return action, milestone_reached, next_milestone, ignore_data\n\n    def get_rung_counts(self):\n        return self._count_tasks\n\n    def on_task_remove(self, task):\n        del self._running[str(task.task_id)]\n\n    def get_first_milestone(self):\n        return self._rungs[-1][0]\n\n    def snapshot_rungs(self):\n        return [(x[0], copy.copy(x[1])) for x in self._rungs]\n\n    def _num_promotable_config(self, recorded):\n        num_recorded = len(recorded)\n        num_top = 0\n        num_promotable = 0\n        if num_recorded >= self.rf:\n            # Search for not yet promoted config in the top\n            # 1 / self.rf fraction\n            num_top = int(num_recorded / self.rf)\n            top_list = heapq.nlargest(\n                num_top, recorded.values(), key=lambda x: x[0])\n            num_promotable = sum((not x) for _, x in top_list)\n        return num_promotable, num_top\n\n    def __repr__(self):\n        iters = "" | "".join([\n            ""Iter {:.3f}: {} of {}"".format(\n                milestone, *self._num_promotable_config(recorded))\n            for milestone, recorded in self._rungs\n        ])\n        return ""Bracket: "" + iters\n'"
autogluon/scheduler/hyperband_stopping.py,0,"b'import logging\nimport numpy as np\nimport copy\n\nlogger = logging.getLogger(__name__)\n\n\ndef map_resource_to_index(resource, rf, min_t, max_t):\n    max_rungs = int(np.log(max_t / min_t) / np.log(rf) + 1)\n    index = int(np.round(np.log(resource * max_t / min_t) / np.log(rf)))\n    index = max(min(index, max_rungs - 1), 0)\n    return index\n\n\ndef _sample_bracket(num_brackets, max_num_rungs, rf):\n    # Brackets are sampled in proportion to the number of configs started\n    # in synchronous Hyperband in each bracket\n    if num_brackets > 1:\n        smax_plus1 = max_num_rungs\n        probs = np.array([\n            (smax_plus1 / (smax_plus1 - s)) * (rf ** (smax_plus1 - s - 1))\n            for s in range(num_brackets)])\n        normalized = probs / probs.sum()\n        return np.random.choice(num_brackets, p=normalized)\n    else:\n        return 0\n\n\nclass HyperbandStopping_Manager(object):\n    """"""Hyperband Manager\n\n    Implements stopping rule which uses the brackets and rung levels defined\n    in Hyperband. The overall algorithm is NOT what is published as ASHA\n    (see HyperbandPromotion_Manager), but rather something resembling the\n    median rule.\n\n    Args:\n        time_attr : str\n            See HyperbandScheduler.\n        reward_attr : str\n            See HyperbandScheduler.\n        max_t : int\n            See HyperbandScheduler.\n        grace_period : int\n            See HyperbandScheduler.\n        reduction_factor : int\n            See HyperbandScheduler.\n        brackets : int\n            See HyperbandScheduler.\n\n    """"""\n    def __init__(\n            self, time_attr, reward_attr, max_t, grace_period,\n            reduction_factor, brackets):\n        self._reward_attr = reward_attr\n        self._time_attr = time_attr\n        self._reduction_factor = reduction_factor\n        self._max_t = max_t\n        self._min_t = grace_period\n        # Maps str(task_id) -> bracket_id\n        self._task_info = dict()\n        self._num_stopped = 0\n        self._brackets = []\n        for s in range(brackets):\n            bracket = StoppingBracket(\n                grace_period, max_t, reduction_factor, s)\n            if not bracket._rungs:\n                break\n            self._brackets.append(bracket)\n\n    def on_task_add(self, task, **kwargs):\n        """"""\n        Since the bracket has already been sampled in on_task_schedule,\n        not much is done here.\n        We return the list of milestones for this bracket in reverse\n        (decreasing) order. The first entry is max_t, even if it is\n        not a milestone in the bracket. This list contains the resource\n        levels the task would reach if it ran to max_t without being stopped.\n\n        :param task: Only task.task_id is used\n        :return: See above\n        """"""\n        assert \'bracket\' in kwargs\n        bracket_id = kwargs[\'bracket\']\n        bracket = self._brackets[bracket_id]\n        self._task_info[str(task.task_id)] = bracket_id\n        levels = [x[0] for x in bracket._rungs]\n        if levels[0] < self._max_t:\n            levels.insert(0, self._max_t)\n        return levels\n\n    def _get_bracket(self, task_id):\n        bracket_id = self._task_info[str(task_id)]\n        return self._brackets[bracket_id], bracket_id\n\n    def on_task_report(self, task, result):\n        """"""\n        This method is called by the reporter thread whenever a new metric\n        value is received. It returns a dictionary with all the information\n        needed for making decisions (e.g., stop / continue task, update\n        model, etc)\n        - task_continues: Should task continue or stop/pause?\n        - update_searcher: True if rung level (or max_t) is hit, at which point\n          the searcher should be updated\n        - next_milestone: If hit rung level < max_t, this is the subsequent\n          rung level (otherwise: None). Used for pending candidates\n        - bracket_id: Bracket in which the task is running\n\n        :param task: Only task.task_id is used\n        :param result: Current reported results from task\n        :return: See above\n        """"""\n        action = False\n        update_searcher = True\n        next_milestone = None\n        bracket, bracket_id = self._get_bracket(task.task_id)\n        if result[self._time_attr] < self._max_t:\n            action, update_searcher, next_milestone = bracket.on_result(\n                task, result[self._time_attr], result[self._reward_attr])\n            # Special case: If config just reached the last milestone in\n            # the bracket and survived, next_milestone is equal to max_t\n            if action and update_searcher and (next_milestone is None):\n                next_milestone = self._max_t\n        if action == False:\n            self._num_stopped += 1\n        return {\n            \'task_continues\': action,\n            \'update_searcher\': update_searcher,\n            \'next_milestone\': next_milestone,\n            \'bracket_id\': bracket_id}\n\n    def on_task_complete(self, task, result):\n        bracket, _ = self._get_bracket(task.task_id)\n        bracket.on_result(\n            task, result[self._time_attr], result[self._reward_attr])\n        self.on_task_remove(task)\n\n    def on_task_remove(self, task):\n        del self._task_info[str(task.task_id)]\n\n    def _sample_bracket(self):\n        return _sample_bracket(\n            num_brackets=len(self._brackets),\n            max_num_rungs=len(self._brackets[0]._rungs),\n            rf=self._reduction_factor)\n\n    def on_task_schedule(self):\n        # Sample bracket for task to be scheduled\n        bracket_id = self._sample_bracket()\n        # \'milestone\' is first milestone the new config will get to\n        extra_kwargs = {\n            \'bracket\': bracket_id,\n            \'milestone\': self._brackets[bracket_id].get_first_milestone()}\n        return None, extra_kwargs\n\n    def snapshot_rungs(self, bracket_id):\n        return self._brackets[bracket_id].snapshot_rungs()\n\n    def resource_to_index(self, resource):\n        return map_resource_to_index(\n            resource, self._reduction_factor, self._min_t, self._max_t)\n\n    def __repr__(self):\n        reprstr = self.__class__.__name__ + \'(\' + \\\n                  \'reward_attr: \' + self._reward_attr + \\\n                  \', time_attr: \' + self._time_attr + \\\n                  \', reduction_factor: \' + str(self._reduction_factor) + \\\n                  \', max_t: \' + str(self._max_t) + \\\n                  \', brackets: \' + str(self._brackets) + \\\n                  \')\'\n        return reprstr\n\n\nclass StoppingBracket(object):\n    """"""Bookkeeping system to track the cutoffs.\n    Rungs are created in reversed order so that we can more easily find\n    the correct rung corresponding to the current iteration of the result.\n    """"""\n\n    def __init__(self, min_t, max_t, reduction_factor, s):\n        self.rf = reduction_factor\n        MAX_RUNGS = int(np.log(max_t / min_t) / np.log(self.rf) - s + 1)\n        self._rungs = [(min_t * self.rf ** (k + s), dict())\n                       for k in reversed(range(MAX_RUNGS))]\n\n    def cutoff(self, recorded):\n        if not recorded:\n            return None\n        return np.percentile(list(recorded.values()), (1 - 1 / self.rf) * 100)\n\n    def on_result(self, task, cur_iter, cur_rew):\n        """"""\n        Decision on whether task may continue (action = True), or should be\n        stoppped (action = False).\n        milestone_reached is a flag whether cur_iter coincides with a milestone.\n        If True, next_milestone is the next milestone after cur_iter, or None\n        if there is none.\n\n        :param task: Only need task.task_id\n        :param cur_iter: Current time_attr value of task\n        :param cur_rew: Current reward_attr value of task\n        :return: action, milestone_reached, next_milestone\n        """"""\n        assert cur_rew is not None, \\\n            ""Reward attribute must be a numerical value, not None""\n        action = True\n        milestone_reached = False\n        next_milestone = None\n        task_key = str(task.task_id)\n        for milestone, recorded in self._rungs:\n            if not (cur_iter < milestone or task_key in recorded):\n                # Note: It is important for model-based searchers that\n                # milestones are reached exactly, not jumped over. In\n                # particular, if a future milestone is reported via\n                # register_pending, its reward value has to be passed\n                # later on via update.\n                assert cur_iter == milestone, \\\n                    ""cur_iter = {} > {} = milestone. Make sure to report time attributes covering all milestones"".format(\n                        cur_iter, milestone)\n                milestone_reached = True\n                cutoff = self.cutoff(recorded)\n                if cutoff is not None and cur_rew < cutoff:\n                    action = False\n                recorded[task_key] = cur_rew\n                break\n            next_milestone = milestone\n        return action, milestone_reached, next_milestone\n\n    def get_first_milestone(self):\n        return self._rungs[-1][0]\n\n    def snapshot_rungs(self):\n        return [(x[0], copy.copy(x[1])) for x in self._rungs]\n\n    def __repr__(self):\n        iters = "" | "".join([\n            ""Iter {:.3f}: {}"".format(milestone, self.cutoff(recorded))\n            for milestone, recorded in self._rungs\n        ])\n        return ""Bracket: "" + iters\n'"
autogluon/scheduler/reporter.py,0,"b'import os\nimport sys\nimport time\nimport json\nimport logging\nimport threading\nimport multiprocessing as mp\nfrom ..utils import save, load, AutoGluonEarlyStop\nimport distributed\nfrom distributed import Queue, Variable\nfrom distributed.comm.core import CommClosedError\n\nlogger = logging.getLogger(__name__)\n\n__all__ = [\'DistStatusReporter\',\n           \'FakeReporter\',\n           \'DistSemaphore\',\n           \'Communicator\',\n           \'LocalStatusReporter\']\n\n\nclass FakeReporter(object):\n    """"""FakeReporter for internal use in final fit\n    """"""\n    def __call__(self, **kwargs):\n        pass\n\n\nclass DistStatusReporter(object):\n    """"""Report status through the training scheduler.\n\n    Example\n    -------\n    >>> @autogluon_method\n    >>> def train_func(config, reporter):\n    ...     reporter(accuracy=0.1)\n    """"""\n\n    def __init__(self, remote=None):\n        self._queue = Queue(client=remote)\n        self._stop = Variable(client=remote)\n        self._stop.set(False)\n        self._continue_semaphore = DistSemaphore(0, remote)\n        self._last_report_time = time.time()\n\n    def __call__(self, **kwargs):\n        """"""Report updated training status.\n        Pass in `done=True` when the training job is completed.\n\n        Args:\n            kwargs: Latest training result status.\n\n        Example\n        _______\n        >>> reporter(accuracy=1, training_iters=4)\n        """"""\n        report_time = time.time()\n        if \'time_this_iter\' not in kwargs:\n            kwargs[\'time_this_iter\'] = report_time - self._last_report_time\n        self._last_report_time = report_time\n\n        logger.debug(\'Reporting {}\'.format(json.dumps(kwargs)))\n        try:\n            self._queue.put(kwargs.copy())\n        except RuntimeError:\n            return\n        self._continue_semaphore.acquire()\n        if self._stop.get():\n            raise AutoGluonEarlyStop(\'Stopping!\')\n\n    def fetch(self, block=True):\n        try:\n            kwargs = self._queue.get()\n        except CommClosedError:\n            return {}\n        return kwargs\n\n    def terminate(self):\n        self._stop.set(True)\n        self._continue_semaphore.release()\n\n    def move_on(self):\n        self._continue_semaphore.release()\n\n    def _start(self):\n        """"""Adjust the real starting time\n        """"""\n        self._last_report_time = time.time()\n\n    def save_dict(self, **state_dict):\n        raise NotImplementedError\n\n    def get_dict(self):\n        raise NotImplementedError\n\n    def __repr__(self):\n        reprstr = self.__class__.__name__\n        return reprstr\n\n\nclass LocalStatusReporter(object):\n    """"""Local status reporter (automatically created by communicator)\n    Example\n    -------\n    >>> def train_func(config, reporter):\n    ...     assert isinstance(reporter, StatusReporter)\n    ...     reporter(timesteps_this_iter=1)\n    """"""\n\n    def __init__(self, dict_path=None):#, result_queue, continue_semaphore):\n        self._queue = mp.Queue(1)\n        self._stop = mp.Value(\'i\', 0)\n        self._last_report_time = None\n        self._continue_semaphore = mp.Semaphore(0)\n        self._last_report_time = time.time()\n        self._save_dict = False\n        self.dict_path = dict_path\n\n    def __call__(self, **kwargs):\n        """"""Report updated training status.\n        Pass in `done=True` when the training job is completed.\n\n        Args:\n            kwargs: Latest training result status.\n        Example\n        -------\n        >>> reporter(accuracy=1, training_iters=4)\n        """"""\n        report_time = time.time()\n        if \'time_this_iter\' not in kwargs:\n            kwargs[\'time_this_iter\'] = report_time - self._last_report_time\n        self._last_report_time = report_time\n\n        self._queue.put(kwargs.copy(), block=True)\n        logger.debug(\'StatusReporter reporting: {}\'.format(json.dumps(kwargs)))\n\n        self._continue_semaphore.acquire()\n        if self._stop.value:\n            raise AutoGluonEarlyStop\n\n    def fetch(self, block=True):\n        kwargs = self._queue.get(block=block)\n        return kwargs\n\n    def move_on(self):\n        self._continue_semaphore.release()\n\n    def terminate(self):\n        self._stop.value = 1\n        self._continue_semaphore.release()\n\n    def _start(self):\n        """"""Adjust the real starting time\n        """"""\n        self._last_report_time = time.time()\n\n    def save_dict(self, **state_dict):\n        """"""Save the serializable state_dict\n        """"""\n        logger.debug(\'Saving the task dict to {}\'.format(self.dict_path))\n        save(state_dict, self.dict_path)\n\n    def has_dict(self):\n        logger.debug(\'has_dict {}\'.format(os.path.isfile(self.dict_path)))\n        return os.path.isfile(self.dict_path)\n\n    def get_dict(self):\n        return load(self.dict_path)\n\n    def __repr__(self):\n        reprstr = self.__class__.__name__\n        return reprstr\n\n\nclass Communicator(threading.Thread):\n    def __init__(self, process, local_reporter, dist_reporter):\n        super().__init__()\n        self.process = process\n        self.local_reporter = local_reporter\n        self.dist_reporter = dist_reporter\n        self._stop_event = threading.Event()\n\n    def run(self):\n        while self.process.is_alive():\n\n            # breaking communication if process raises exception\n            if self.process.exception is not None:\n                error, traceback = self.process.exception\n                self.local_reporter.terminate()\n                self.dist_reporter(done=True, traceback=traceback)\n                self.process.join()\n                break\n\n            try:\n                # waiting until process reports results or raises exception\n                if self.local_reporter._queue.empty():\n                    continue\n                reported_result = self.local_reporter.fetch()\n            except BrokenPipeError:\n                break\n\n            try:\n                self.dist_reporter(**reported_result)\n                self.local_reporter.move_on()\n            except AutoGluonEarlyStop:\n                self.local_reporter.terminate()\n\n            if reported_result.get(\'done\', False):\n                self.process.join()\n                break\n\n    def stop(self):\n        self._stop_event.set()\n\n    def stopped(self):\n        return self._stop_event.is_set()\n\n    @classmethod\n    def Create(cls, process, local_reporter, dist_reporter):\n        communicator = cls(process, local_reporter, dist_reporter)\n        communicator.start()\n        return communicator\n\n    def __repr__(self):\n        reprstr = self.__class__.__name__\n        return reprstr\n\n\nclass DistSemaphore(object):\n    def __init__(self, value, remote=None):\n        self._queue = Queue(client=remote)\n        for i in range(value):\n            self._queue.put(1)\n\n    def acquire(self):\n        try:\n            _ = self._queue.get()\n        except distributed.comm.core.CommClosedError:\n            pass\n\n    def release(self):\n        self._queue.put(1)\n\n    def __repr__(self):\n        reprstr = self.__class__.__name__\n        return reprstr\n'"
autogluon/scheduler/rl_scheduler.py,0,"b'import os\nimport json\nimport time\nimport pickle\nimport logging\nimport threading\nimport multiprocessing as mp\nfrom collections import OrderedDict\n\nimport mxnet as mx\n\nfrom .resource import DistributedResource\nfrom ..utils import (save, load, mkdir, try_import_mxboard, tqdm)\nfrom ..core import Task\nfrom ..core.decorator import _autogluon_method\nfrom ..searcher import RLSearcher\nfrom .fifo import FIFOScheduler\nfrom .reporter import DistStatusReporter\n\n__all__ = [\'RLScheduler\']\n\nlogger = logging.getLogger(__name__)\n\n\nclass RLScheduler(FIFOScheduler):\n    r""""""Scheduler that uses Reinforcement Learning with a LSTM controller created based on the provided search spaces\n\n    Parameters\n    ----------\n    train_fn : callable\n        A task launch function for training. Note: please add the `@ag.args` decorater to the original function.\n    args : object (optional)\n        Default arguments for launching train_fn.\n    resource : dict\n        Computation resources.  For example, `{\'num_cpus\':2, \'num_gpus\':1}`\n    searcher : object (optional)\n        Autogluon searcher.  For example, autogluon.searcher.RandomSearcher\n    time_attr : str\n        A training result attr to use for comparing time.\n        Note that you can pass in something non-temporal such as\n        `training_epoch` as a measure of progress, the only requirement\n        is that the attribute should increase monotonically.\n    reward_attr : str\n        The training result objective value attribute. As with `time_attr`, this may refer to any objective value.\n        Stopping procedures will use this attribute.\n    controller_resource : int\n        Batch size for training controllers.\n    dist_ip_addrs : list of str\n        IP addresses of remote machines.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> import autogluon as ag\n    >>>\n    >>> @ag.args(\n    ...     lr=ag.space.Real(1e-3, 1e-2, log=True),\n    ...     wd=ag.space.Real(1e-3, 1e-2))\n    >>> def train_fn(args, reporter):\n    ...     print(\'lr: {}, wd: {}\'.format(args.lr, args.wd))\n    ...     for e in range(10):\n    ...         dummy_accuracy = 1 - np.power(1.8, -np.random.uniform(e, 2*e))\n    ...         reporter(epoch=e+1, accuracy=dummy_accuracy, lr=args.lr, wd=args.wd)\n    ...\n    >>> scheduler = ag.scheduler.RLScheduler(train_fn,\n    ...                                      resource={\'num_cpus\': 2, \'num_gpus\': 0},\n    ...                                      num_trials=20,\n    ...                                      reward_attr=\'accuracy\',\n    ...                                      time_attr=\'epoch\')\n    >>> scheduler.run()\n    >>> scheduler.join_jobs()\n    >>> scheduler.get_training_curves(plot=True)\n    """"""\n    def __init__(self, train_fn, args=None, resource=None, searcher=None, checkpoint=\'./exp/checkpoint.ag\',\n                 resume=False, num_trials=None, time_attr=\'epoch\', reward_attr=\'accuracy\',\n                 visualizer=\'none\', controller_lr=1e-3, ema_baseline_decay=0.95,\n                 controller_resource={\'num_cpus\': 0, \'num_gpus\': 0},\n                 controller_batch_size=1,\n                 dist_ip_addrs=[], sync=True, **kwargs):\n        assert isinstance(train_fn, _autogluon_method), \'Please use @ag.args \' + \\\n                \'to decorate your training script.\'\n        self.ema_baseline_decay = ema_baseline_decay\n        self.sync = sync\n        # create RL searcher/controller\n        if not isinstance(searcher, RLSearcher):\n            searcher = RLSearcher(\n                train_fn.kwspaces, reward_attribute=reward_attr)\n        super(RLScheduler,self).__init__(\n                train_fn, train_fn.args, resource, searcher,\n                checkpoint=checkpoint, resume=False, num_trials=num_trials,\n                time_attr=time_attr, reward_attr=reward_attr,\n                visualizer=visualizer, dist_ip_addrs=dist_ip_addrs, **kwargs)\n        # reserve controller computation resource on master node\n        master_node = self.remote_manager.get_master_node()\n        self.controller_resource = DistributedResource(**controller_resource)\n        assert self.resource_manager.reserve_resource(\n                master_node, self.controller_resource), \'Not Enough Resource on Master Node\' + \\\n                    \' for Training Controller\'\n        self.controller_ctx = [mx.gpu(i) for i in self.controller_resource.gpu_ids] if \\\n                controller_resource[\'num_gpus\'] > 0 else [mx.cpu()]\n        # controller setup\n        self.controller = searcher.controller\n        self.controller.collect_params().reset_ctx(self.controller_ctx)\n        self.controller_optimizer = mx.gluon.Trainer(\n                self.controller.collect_params(), \'adam\',\n                optimizer_params={\'learning_rate\': controller_lr*controller_batch_size})\n        self.controller_batch_size = controller_batch_size\n        self.baseline = None\n        self.lock = mp.Lock()\n        # async buffers\n        if not sync:\n            self.mp_count = mp.Value(\'i\', 0)\n            self.mp_seed = mp.Value(\'i\', 0)\n            self.mp_fail = mp.Value(\'i\', 0)\n\n        if resume:\n            if os.path.isfile(checkpoint):\n                self.load_state_dict(load(checkpoint))\n            else:\n                msg = \'checkpoint path {} is not available for resume.\'.format(checkpoint)\n                logger.exception(msg)\n\n    def run(self, **kwargs):\n        """"""Run multiple number of trials\n        """"""\n        self.num_trials = kwargs.get(\'num_trials\', self.num_trials)\n        logger.info(\'Starting Experiments\')\n        logger.info(\'Num of Finished Tasks is {}\'.format(self.num_finished_tasks))\n        logger.info(\'Num of Pending Tasks is {}\'.format(self.num_trials - self.num_finished_tasks))\n        if self.sync:\n            self._run_sync()\n        else:\n            self._run_async()\n\n    def _run_sync(self):\n        decay = self.ema_baseline_decay\n        for i in tqdm(range(self.num_trials // self.controller_batch_size + 1)):\n            with mx.autograd.record():\n                # sample controller_batch_size number of configurations\n                batch_size = self.num_trials % self.num_trials \\\n                    if i == self.num_trials // self.controller_batch_size \\\n                    else self.controller_batch_size\n                if batch_size == 0: continue\n                configs, log_probs, entropies = self.controller.sample(\n                    batch_size, with_details=True)\n                # schedule the training tasks and gather the reward\n                rewards = self.sync_schedule_tasks(configs)\n                # substract baseline\n                if self.baseline is None:\n                    self.baseline = rewards[0]\n                avg_rewards = mx.nd.array([reward - self.baseline for reward in rewards],\n                                          ctx=self.controller.context)\n                # EMA baseline\n                for reward in rewards:\n                    self.baseline = decay * self.baseline + (1 - decay) * reward\n                # negative policy gradient\n                log_probs = log_probs.sum(axis=1)\n                loss = - log_probs * avg_rewards#.reshape(-1, 1)\n                loss = loss.sum()  # or loss.mean()\n\n            # update\n            loss.backward()\n            self.controller_optimizer.step(batch_size)\n            logger.debug(\'controller loss: {}\'.format(loss.asscalar()))\n\n    def _run_async(self):\n        def _async_run_trial():\n            self.mp_count.value += 1\n            self.mp_seed.value += 1\n            seed = self.mp_seed.value\n            mx.random.seed(seed)\n            with mx.autograd.record():\n                # sample one configuration\n                with self.lock:\n                    config, log_prob, entropy = self.controller.sample(with_details=True)\n                config = config[0]\n                task = Task(self.train_fn, {\'args\': self.args, \'config\': config},\n                            DistributedResource(**self.resource))\n                # start training task\n                reporter = DistStatusReporter(remote=task.resources.node)\n                task.args[\'reporter\'] = reporter\n                task_thread = self.add_job(task)\n\n                # run reporter\n                last_result = None\n                config = task.args[\'config\']\n                while task_thread.is_alive():\n                    reported_result = reporter.fetch()\n                    if reported_result.get(\'done\', False):\n                        reporter.move_on()\n                        task_thread.join()\n                        break\n                    self._add_training_result(task.task_id, reported_result, task.args[\'config\'])\n                    reporter.move_on()\n                    last_result = reported_result\n                self.searcher.update(config, **last_result)\n                reward = last_result[self._reward_attr]\n                with self.lock:\n                    if self.baseline is None:\n                        self.baseline = reward\n                avg_reward = mx.nd.array([reward - self.baseline], ctx=self.controller.context)\n                # negative policy gradient\n                with self.lock:\n                    loss = -log_prob * avg_reward.reshape(-1, 1)\n                    loss = loss.sum()\n\n            # update\n            print(\'loss\', loss)\n            with self.lock:\n                try:\n                    loss.backward()\n                    self.controller_optimizer.step(1)\n                except Exception:\n                    self.mp_fail.value += 1\n                    logger.warning(\'Exception during backward {}.\'.format(self.mp_fail.value))\n\n            self.mp_count.value -= 1\n            # ema\n            with self.lock:\n                decay = self.ema_baseline_decay\n                self.baseline = decay * self.baseline + (1 - decay) * reward\n\n        reporter_threads = []\n        for i in range(self.num_trials):\n            while self.mp_count.value >= self.controller_batch_size:\n                time.sleep(0.2)\n            #_async_run_trial()\n            reporter_thread = threading.Thread(target=_async_run_trial)\n            reporter_thread.start()\n            reporter_threads.append(reporter_thread)\n\n        for p in reporter_threads:\n            p.join()\n\n    def sync_schedule_tasks(self, configs):\n        rewards = []\n        results = {}\n        def _run_reporter(task, task_job, reporter):\n            last_result = None\n            config = task.args[\'config\']\n            while not task_job.done():\n                reported_result = reporter.fetch()\n                if \'traceback\' in reported_result:\n                    logger.exception(reported_result[\'traceback\'])\n                    reporter.move_on()\n                    break\n\n                if reported_result.get(\'done\', False):\n                    reporter.move_on()\n                    break\n                self._add_training_result(task.task_id, reported_result, task.args[\'config\'])\n                reporter.move_on()\n                last_result = reported_result\n            if last_result is not None:\n                self.searcher.update(config, **last_result)\n                with self.lock:\n                    results[pickle.dumps(config)] = \\\n                        last_result[self._reward_attr]\n\n        # launch the tasks\n        tasks = []\n        task_jobs = []\n        reporter_threads = []\n        for config in configs:\n            logger.debug(\'scheduling config: {}\'.format(config))\n            # create task\n            task = Task(self.train_fn, {\'args\': self.args, \'config\': config},\n                        DistributedResource(**self.resource))\n            reporter = DistStatusReporter()\n            task.args[\'reporter\'] = reporter\n            task_job = self.add_job(task)\n            # run reporter\n            reporter_thread = threading.Thread(target=_run_reporter, args=(task, task_job, reporter))\n            reporter_thread.start()\n            tasks.append(task)\n            task_jobs.append(task_job)\n            reporter_threads.append(reporter_thread)\n\n        for p1, p2 in zip(task_jobs, reporter_threads):\n            p1.result()\n            p2.join()\n        with self.LOCK:\n            for task in tasks:\n                self.finished_tasks.append({\'TASK_ID\': task.task_id,\n                                           \'Config\': task.args[\'config\']})\n        if self._checkpoint is not None:\n            logger.debug(\'Saving Checkerpoint\')\n            self.save()\n\n        for config in configs:\n            rewards.append(results[pickle.dumps(config)])\n\n        return rewards\n\n    def add_job(self, task, **kwargs):\n        """"""Adding a training task to the scheduler.\n\n        Args:\n            task (:class:`autogluon.scheduler.Task`): a new training task\n        """"""\n        cls = RLScheduler\n        cls.resource_manager._request(task.resources)\n        # main process\n        job = cls._start_distributed_job(task, cls.resource_manager)\n        return job\n\n    def join_tasks(self):\n        pass\n\n    def state_dict(self, destination=None):\n        """"""Returns a dictionary containing a whole state of the Scheduler\n\n        Examples\n        --------\n        >>> ag.save(scheduler.state_dict(), \'checkpoint.ag\')\n        """"""\n        if destination is None:\n            destination = OrderedDict()\n            destination._metadata = OrderedDict()\n        logger.debug(\'\\nState_Dict self.finished_tasks: {}\'.format(self.finished_tasks))\n        destination[\'finished_tasks\'] = pickle.dumps(self.finished_tasks)\n        destination[\'baseline\'] = pickle.dumps(self.baseline)\n        destination[\'TASK_ID\'] = Task.TASK_ID.value\n        destination[\'searcher\'] = self.searcher.state_dict()\n        destination[\'training_history\'] = json.dumps(self.training_history)\n        if self.visualizer == \'mxboard\' or self.visualizer == \'tensorboard\':\n            destination[\'visualizer\'] = json.dumps(self.mxboard._scalar_dict)\n        return destination\n\n    def load_state_dict(self, state_dict):\n        """"""Load from the saved state dict.\n\n        Examples\n        --------\n        >>> scheduler.load_state_dict(ag.load(\'checkpoint.ag\'))\n        """"""\n        self.finished_tasks = pickle.loads(state_dict[\'finished_tasks\'])\n        #self.baseline = pickle.loads(state_dict[\'baseline\'])\n        Task.set_id(state_dict[\'TASK_ID\'])\n        self.searcher.load_state_dict(state_dict[\'searcher\'])\n        self.training_history = json.loads(state_dict[\'training_history\'])\n        if self.visualizer == \'mxboard\' or self.visualizer == \'tensorboard\':\n            self.mxboard._scalar_dict = json.loads(state_dict[\'visualizer\'])\n        logger.debug(\'Loading Searcher State {}\'.format(self.searcher))\n'"
autogluon/scheduler/scheduler.py,0,"b'""""""Distributed Task Scheduler""""""\nimport os\nimport pickle\nimport logging\nfrom warnings import warn\nimport multiprocessing as mp\nfrom collections import OrderedDict\n\nfrom .remote import RemoteManager\nfrom .resource import DistributedResourceManager\nfrom ..core import Task\nfrom .reporter import *\nfrom ..utils import AutoGluonWarning, AutoGluonEarlyStop, CustomProcess\n\nlogger = logging.getLogger(__name__)\n\n__all__ = [\'TaskScheduler\']\n\n\nclass ClassProperty(object):\n\n    def __init__(self, fget):\n        self.fget = fget\n\n    def __get__(self, owner_self, owner_cls):\n        return self.fget(owner_cls)\n\n\nclass TaskScheduler(object):\n    """"""Base Distributed Task Scheduler\n    """"""\n    LOCK = mp.Lock()\n    _resource_manager = None\n    _remote_manager = None\n\n    @ClassProperty\n    def resource_manager(cls):\n        if cls._resource_manager is None:\n            cls._resource_manager = DistributedResourceManager()\n        return cls._resource_manager\n\n    @ClassProperty\n    def remote_manager(cls):\n        if cls._remote_manager is None:\n            cls._remote_manager = RemoteManager()\n        return cls._remote_manager\n\n    def __init__(self, dist_ip_addrs=None):\n        if dist_ip_addrs is None:\n            dist_ip_addrs=[]\n        cls = TaskScheduler\n        remotes = cls.remote_manager.add_remote_nodes(dist_ip_addrs)\n        cls.resource_manager.add_remote(cls.remote_manager.get_remotes())\n        self.scheduled_tasks = []\n        self.finished_tasks = []\n\n    def add_remote(self, ip_addrs):\n        """"""Add remote nodes to the scheduler computation resource.\n        """"""\n        ip_addrs = [ip_addrs] if isinstance(ip_addrs, str) else ip_addrs\n        with self.LOCK:\n            remotes = TaskScheduler.remote_manager.add_remote_nodes(ip_addrs)\n            TaskScheduler.resource_manager.add_remote(remotes)\n\n    @classmethod\n    def upload_files(cls, files, **kwargs):\n        """"""Upload files to remote machines, so that they are accessible by import or load.\n        """"""\n        cls.remote_manager.upload_files(files, **kwargs)\n\n    def _dict_from_task(self, task):\n        if isinstance(task, Task):\n            return {\'TASK_ID\': task.task_id, \'Args\': task.args}\n        else:\n            assert isinstance(task, dict)\n            return {\'TASK_ID\': task[\'TASK_ID\'], \'Args\': task[\'Args\']}\n\n    def add_task(self, task, **kwargs):\n        """"""add_task() is now deprecated in favor of add_job().\n        """"""\n        warn(""scheduler.add_task() is now deprecated in favor of scheduler.add_job()."",\n             AutoGluonWarning)\n        self.add_job(task, **kwargs)\n\n    def add_job(self, task, **kwargs):\n        """"""Adding a training task to the scheduler.\n\n        Args:\n            task (:class:`autogluon.scheduler.Task`): a new training task\n\n        Relevant entries in kwargs:\n        - bracket: HB bracket to be used. Has been sampled in _promote_config\n        - new_config: If True, task starts new config eval, otherwise it promotes\n          a config (only if type == \'promotion\')\n        Only if new_config == False:\n        - config_key: Internal key for config\n        - resume_from: config promoted from this milestone\n        - milestone: config promoted to this milestone (next from resume_from)\n        """"""\n        # adding the task\n        cls = TaskScheduler\n        if not task.resources.is_ready:\n            cls.resource_manager._request(task.resources)\n        job = cls._start_distributed_job(task, cls.resource_manager)\n        new_dict = self._dict_from_task(task)\n        new_dict[\'Job\'] = job\n        with self.LOCK:\n            self.scheduled_tasks.append(new_dict)\n\n    def run_job(self, task):\n        """"""Run a training task to the scheduler (Sync).\n        """"""\n        cls = TaskScheduler\n        cls.resource_manager._request(task.resources)\n        job = cls._start_distributed_job(task, cls.resource_manager)\n        return job.result()\n\n    @staticmethod\n    def _start_distributed_job(task, resource_manager):\n        """"""Async Execute the job in remote and release the resources\n        """"""\n        logger.debug(\'\\nScheduling {}\'.format(task))\n        job = task.resources.node.submit(TaskScheduler._run_dist_job,\n                                         task.fn, task.args, task.resources.gpu_ids)\n        def _release_resource_callback(fut):\n            logger.debug(\'Start Releasing Resource\')\n            resource_manager._release(task.resources)\n        job.add_done_callback(_release_resource_callback)\n        return job\n\n    @staticmethod\n    def _run_dist_job(fn, args, gpu_ids):\n        """"""Remote function Executing the task\n        """"""\n        if \'_default_config\' in args[\'args\']:\n            args[\'args\'].pop(\'_default_config\')\n\n        if \'reporter\' in args:\n            local_reporter = LocalStatusReporter()\n            dist_reporter = args[\'reporter\']\n            args[\'reporter\'] = local_reporter\n\n        manager = mp.Manager()\n        return_list = manager.list()\n        def _worker(return_list, gpu_ids, args):\n            """"""Worker function in thec client\n            """"""\n            if len(gpu_ids) > 0:\n                # handle GPU devices\n                os.environ[\'CUDA_VISIBLE_DEVICES\'] = "","".join(map(str, gpu_ids))\n                os.environ[\'MXNET_CUDNN_AUTOTUNE_DEFAULT\'] = ""0""\n\n            # running\n            try:\n                ret = fn(**args)\n            except AutoGluonEarlyStop:\n                ret = None\n            return_list.append(ret)\n\n        try:\n            # start local progress\n            p = CustomProcess(target=_worker, args=(return_list, gpu_ids, args))\n            p.start()\n            if \'reporter\' in args:\n                cp = Communicator.Create(p, local_reporter, dist_reporter)\n            p.join()\n        except Exception as e:\n            logger.error(\'Exception in worker process: {}\'.format(e))\n        ret = return_list[0] if len(return_list) > 0 else None\n        return ret\n\n    def _clean_task_internal(self, task_dict):\n        pass\n\n    def _cleaning_tasks(self):\n        with self.LOCK:\n            new_scheduled_tasks = []\n            for task_dict in self.scheduled_tasks:\n                if task_dict[\'Job\'].done():\n                    self._clean_task_internal(task_dict)\n                    self.finished_tasks.append(self._dict_from_task(task_dict))\n                else:\n                    new_scheduled_tasks.append(task_dict)\n            if len(new_scheduled_tasks) < len(self.scheduled_tasks):\n                self.scheduled_tasks = new_scheduled_tasks\n\n    def join_tasks(self):\n        warn(""scheduler.join_tasks() is now deprecated in favor of scheduler.join_jobs()."",\n             AutoGluonWarning)\n        self.join_jobs()\n\n    def join_jobs(self, timeout=None):\n        """"""Wait all scheduled jobs to finish\n        """"""\n        self._cleaning_tasks()\n        for task_dict in self.scheduled_tasks:\n            try:\n                task_dict[\'Job\'].result(timeout=timeout)\n            except TimeoutError as e:\n                logger.error(str(e))\n            self._clean_task_internal(task_dict)\n        self._cleaning_tasks()\n\n    def shutdown(self):\n        """"""shutdown() is now deprecated in favor of :func:`autogluon.done`.\n        """"""\n        warn(""scheduler.shutdown() is now deprecated in favor of autogluon.done()."",\n             AutoGluonWarning)\n        self.join_jobs()\n        self.remote_manager.shutdown()\n\n    def state_dict(self, destination=None):\n        """"""Returns a dictionary containing a whole state of the Scheduler\n\n        Examples\n        --------\n        >>> ag.save(scheduler.state_dict(), \'checkpoint.ag\')\n        """"""\n        if destination is None:\n            destination = OrderedDict()\n            destination._metadata = OrderedDict()\n        destination[\'finished_tasks\'] = pickle.dumps(self.finished_tasks)\n        destination[\'TASK_ID\'] = Task.TASK_ID.value\n        return destination\n\n    def load_state_dict(self, state_dict):\n        """"""Load from the saved state dict.\n\n        Examples\n        --------\n        >>> scheduler.load_state_dict(ag.load(\'checkpoint.ag\'))\n        """"""\n        self.finished_tasks = pickle.loads(state_dict[\'finished_tasks\'])\n        Task.set_id(state_dict[\'TASK_ID\'])\n        logger.debug(\'\\nLoading finished_tasks: {} \'.format(self.finished_tasks))\n\n    @property\n    def num_finished_tasks(self):\n        return len(self.finished_tasks)\n\n    def __repr__(self):\n        reprstr = self.__class__.__name__ + \'(\\n\' + \\\n            str(self.resource_manager) +\')\\n\'\n        return reprstr\n'"
autogluon/searcher/__init__.py,0,b'from .searcher import *\nfrom .skopt_searcher import *\nfrom .rl_controller import *\nfrom .grid_searcher import *\nfrom .gp_searcher import *\nfrom .searcher_factory import *\n'
autogluon/searcher/default_arguments.py,0,"b'from typing import Set, Tuple, Dict\nimport logging\nimport numbers\n\nlogger = logging.getLogger(__name__)\n\n\nclass CheckType(object):\n    def assert_valid(self, key: str, value):\n        pass\n\n\nclass Float(CheckType):\n    def __init__(self, lower: float = None, upper: float = None):\n        if lower and upper:\n            assert lower < upper\n        self.lower = lower\n        self.upper = upper\n\n    def assert_valid(self, key: str, value):\n        assert isinstance(value, numbers.Real), \\\n            ""{}: Value = {} must be of type float"".format(key, value)\n        assert (not self.lower) or value >= self.lower, \\\n            ""{}: Value = {} must be >= {}"".format(key, value, self.lower)\n        assert (not self.upper) or value <= self.upper, \\\n            ""{}: Value = {} must be <= {}"".format(key, value, self.upper)\n\n\nclass Integer(CheckType):\n    def __init__(self, lower: int = None, upper: int = None):\n        if lower and upper:\n            assert lower < upper\n        self.lower = lower\n        self.upper = upper\n\n    def assert_valid(self, key: str, value):\n        assert isinstance(value, numbers.Integral), \\\n            ""{}: Value = {} must be of type int"".format(key, value)\n        assert (not self.lower) or value >= self.lower, \\\n            ""{}: Value = {} must be >= {}"".format(key, value, self.lower)\n        assert (not self.upper) or value <= self.upper, \\\n            ""{}: Value = {} must be <= {}"".format(key, value, self.upper)\n\n\nclass Categorical(CheckType):\n    def __init__(self, choices: Tuple[str, ...]):\n        self.choices = set(choices)\n\n    def assert_valid(self, key: str, value):\n        assert isinstance(value, str) and value in self.choices, \\\n            ""{}: Value = {} must be in {}"".format(key, value, self.choices)\n\n\nclass Boolean(CheckType):\n    def assert_valid(self, key: str, value):\n        assert isinstance(value, bool), \\\n            ""{}: Value = {} must be boolean"".format(key, value)\n\n\ndef check_and_merge_defaults(\n        options: dict, mandatory: Set[str], default_options: dict,\n        constraints: Dict[str, CheckType]=None, dict_name=None) -> dict:\n    """"""\n    First, check that all keys in mandatory appear in options. Second, create\n    result_options by merging options and default_options, where entries in\n    options have precedence. Finally, if constraints is given, this is used to\n    check validity of values.\n\n    :param options:\n    :param mandatory:\n    :param default_options:\n    :param constraints:\n    :return: result_options\n    """"""\n    prefix = """" if dict_name is None else ""{}: "".format(dict_name)\n    for key in mandatory:\n        assert key in options, \\\n            prefix + ""Key \'{}\' is missing (but is mandatory)"".format(key)\n    log_msg = """"\n    result_options = options.copy()\n    for key, value in default_options.items():\n        if key not in options:\n            log_msg += (prefix + ""Key \'{}\': Imputing default value {}\\n"".format(\n                key, value))\n            result_options[key] = value\n    if log_msg:\n        logger.info(log_msg)\n    # Check constraints\n    if constraints:\n        for key, value in result_options.items():\n            check = constraints.get(key)\n            if check:\n                check.assert_valid(prefix + ""Key \'{}\'"".format(key), value)\n\n    return result_options\n'"
autogluon/searcher/gp_searcher.py,0,"b'import ConfigSpace as CS\nimport multiprocessing as mp\n\nfrom autogluon.searcher.default_arguments import check_and_merge_defaults\nfrom autogluon.searcher.bayesopt.autogluon.searcher_factory import \\\n    gp_fifo_searcher_factory, gp_multifidelity_searcher_factory, \\\n    gp_fifo_searcher_defaults, gp_multifidelity_searcher_defaults\nfrom .searcher import BaseSearcher\n\n__all__ = [\'GPFIFOSearcher\',\n           \'GPMultiFidelitySearcher\']\n\n\ndef _to_config_cs(config_space: CS.ConfigurationSpace, config: dict) \\\n        -> CS.Configuration:\n    return CS.Configuration(config_space, values=config)\n\n\nclass GPFIFOSearcher(BaseSearcher):\n    """"""Gaussian process Bayesian optimization for FIFO scheduler\n\n    This searcher must be used with `FIFOScheduler`. It provides Bayesian\n    optimization, based on a Gaussian process surrogate model. It is created\n    along with the scheduler, using `searcher=\'bayesopt\'`:\n\n    Pending configurations (for which evaluation tasks are currently running)\n    are dealt with by fantasizing (i.e., target values are drawn from the\n    current posterior, and acquisition functions are averaged over this\n    sample, see `num_fantasy_samples`).\n    The GP surrogate model uses a Matern 5/2 covariance function with automatic\n    relevance determination (ARD) of input attributes, and a constant mean\n    function. The acquisition function is expected improvement (EI). All\n    hyperparameters of the surrogate model are estimated by empirical Bayes\n    (maximizing the marginal likelihood). In general, this hyperparameter\n    fitting is the most expensive part of a `get_config` call.\n\n    The following happens in `get_config`. For the first `num_init_random` calls,\n    a config is drawn at random (the very first call results in the default\n    config of the space). Afterwards, Bayesian optimization is used, unless\n    there are no finished evaluations yet.\n    First, model hyperparameter are refit. This step can be skipped (see\n    `opt_skip*` parameters). Next, `num_init_candidates` configs are sampled at\n    random, and ranked by a scoring function (`initial_scoring`). BFGS local\n    optimization is then run starting from the top scoring config, where EI\n    is minimized.\n\n    Parameters\n    ----------\n    configspace : ConfigSpace.ConfigurationSpace\n        Config space of `train_fn`, equal to `train_fn.cs`\n    reward_attribute : str\n        Name of reward attribute reported by `train_fn`, equal to `reward_attr`\n        of `scheduler\n    debug_log : bool (default: False)\n        If True, both searcher and scheduler output an informative log, from\n        which the configs chosen and decisions being made can be traced.\n    first_is_default : bool (default: True)\n        If True, the first config to be evaluated is the default one of the\n        config space. Otherwise, this first config is drawn at random.\n    random_seed : int\n        Seed for pseudo-random number generator used.\n    num_init_random : int\n        Number of initial `get_config` calls for which randomly sampled configs\n        are returned. Afterwards, Bayesian optimization is used\n    num_init_candidates : int\n        Number of initial candidates sampled at random in order to seed the\n        search for `get_config`\n    num_fantasy_samples : int\n        Number of samples drawn for fantasizing (latent target values for\n        pending candidates)\n    initial_scoring : str\n        Scoring function to rank initial candidates (local optimization of EI\n        is started from top scorer). Values are \'thompson_indep\' (independent\n        Thompson sampling; randomized score, which can increase exploration),\n        \'acq_func\' (score is the same (EI) acquisition function which is afterwards\n        locally optimized).\n    opt_nstarts : int\n        Parameter for hyperparameter fitting. Number of random restarts\n    opt_maxiter : int\n        Parameter for hyperparameter fitting. Maximum number of iterations\n        per restart\n    opt_warmstart : bool\n        Parameter for hyperparameter fitting. If True, each fitting is started\n        from the previous optimum. Not recommended in general\n    opt_verbose : bool\n        Parameter for hyperparameter fitting. If True, lots of output\n    opt_skip_init_length : int\n        Parameter for hyperparameter fitting, skip predicate. Fitting is never\n        skipped as long as number of observations below this threshold\n    opt_skip_period : int\n        Parameter for hyperparameter fitting, skip predicate. If >1, and number\n        of observations above `opt_skip_init_length`, fitting is done only\n        K-th call, and skipped otherwise\n    map_reward : str or MapReward (default: \'1_minus_x\')\n        AutoGluon is maximizing reward, while internally, Bayesian optimization\n        is minimizing the criterion. States how reward is mapped to criterion.\n        This must a strictly decreasing function. Values are \'1_minus_x\'\n        (criterion = 1 - reward), \'minus_x\' (criterion = -reward).\n        From a technical standpoint, it does not matter what is chosen here,\n        because criterion is only used internally. Also note that criterion\n        data is always normalized to mean 0, variance 1 before fitted with a\n        GP.\n\n    Examples\n    --------\n    >>> import autogluon as ag\n    >>> @ag.args(\n    ...     lr=ag.space.Real(1e-3, 1e-2, log=True))\n    >>> def train_fn(args, reporter):\n    ...     reporter(accuracy = args.lr ** 2)\n    >>> searcher_options = {\n    ...     \'map_reward\': \'minus_x\',\n    ...     \'opt_skip_period\': 2}\n    >>> scheduler = ag.scheduler.FIFOScheduler(\n    ...     train_fn, searcher=\'bayesopt\', searcher_options=searcher_options,\n    ...     num_trials=10, reward_attr=\'accuracy\')\n    """"""\n    def __init__(self, **kwargs):\n        _gp_searcher = kwargs.get(\'_gp_searcher\')\n        if _gp_searcher is None:\n            _kwargs = check_and_merge_defaults(\n                kwargs, *gp_fifo_searcher_defaults(),\n                dict_name=\'search_options\')\n            _gp_searcher = gp_fifo_searcher_factory(**_kwargs)\n        super().__init__(\n            _gp_searcher.hp_ranges.config_space,\n            reward_attribute=kwargs.get(\'reward_attribute\'))\n        self.gp_searcher = _gp_searcher\n        # This lock protects gp_searcher. We are not using self.LOCK, this\n        # can lead to deadlocks when superclass methods are called\n        self._gp_lock = mp.Lock()\n\n    def configure_scheduler(self, scheduler):\n        from ..scheduler.fifo import FIFOScheduler\n\n        assert isinstance(scheduler, FIFOScheduler), \\\n            ""This searcher requires FIFOScheduler scheduler""\n        super().configure_scheduler(scheduler)\n\n    def get_config(self, **kwargs):\n        with self._gp_lock:\n            config_cs = self.gp_searcher.get_config()\n        return config_cs.get_dictionary()\n\n    def update(self, config, **kwargs):\n        super().update(config, **kwargs)\n        with self._gp_lock:\n            config_cs = self._to_config_cs(config)\n            self.gp_searcher.update(\n                config_cs, reward=kwargs[self._reward_attribute])\n\n    def register_pending(self, config, milestone=None):\n        with self._gp_lock:\n            config_cs = self._to_config_cs(config)\n            self.gp_searcher.register_pending(config_cs)\n\n    def evaluation_failed(self, config, **kwargs):\n        with self._gp_lock:\n            config_cs = self._to_config_cs(config)\n            self.gp_searcher.evaluation_failed(config_cs)\n\n    def dataset_size(self):\n        with self._gp_lock:\n            return self.gp_searcher.dataset_size()\n\n    def cumulative_profile_record(self):\n        with self._gp_lock:\n            return self.gp_searcher.cumulative_profile_record()\n\n    def model_parameters(self):\n        with self._gp_lock:\n            return self.gp_searcher.get_params()\n\n    def get_state(self):\n        with self._gp_lock:\n            return self.gp_searcher.get_state()\n\n    def clone_from_state(self, state):\n        with self._gp_lock:\n            _gp_searcher = self.gp_searcher.clone_from_state(state)\n        # Use copy constructor\n        return GPFIFOSearcher(\n            reward_attribute=self._reward_attribute,\n            _gp_searcher=_gp_searcher)\n\n    @property\n    def debug_log(self):\n        with self._gp_lock:\n            return self.gp_searcher.debug_log\n\n    def _to_config_cs(self, config):\n        return _to_config_cs(self.gp_searcher.hp_ranges.config_space, config)\n\n\nclass GPMultiFidelitySearcher(BaseSearcher):\n    """"""Gaussian process Bayesian optimization for Hyperband scheduler\n\n    This searcher must be used with `HyperbandScheduler`. It provides a novel\n    combination of Bayesian optimization, based on a Gaussian process surrogate\n    model, with Hyperband scheduling. In particular, observations across\n    resource levels are modelled jointly. It is created along with the\n    scheduler, using `searcher=\'bayesopt\'`:\n\n    Most of `GPFIFOSearcher` comments apply here as well.\n    In multi-fidelity HPO, we optimize a function f(x, r), x the configuration,\n    r the resource (or time) attribute. The latter must be a positive integer.\n    In most applications, `time_attr` == \'epoch\', and the resource is the number\n    of epochs already trained.\n\n    We model the function f(x, r) jointly over all resource levels r at which\n    it is observed (but see `searcher_data` in `HyperbandScheduler`). The kernel\n    and mean function of our surrogate model are over (x, r). The surrogate\n    model is selected by `gp_resource_kernel`. More details about the supported\n    kernels is in:\n\n        Tiao, Klein, Archambeau, Seeger (2020)\n        Model-based Asynchronous Hyperparameter Optimization\n        https://arxiv.org/abs/2003.10865\n\n    The acquisition function (EI) which is optimized in `get_config`, is obtained\n    by fixing the resource level r to a value which is determined depending on\n    the current state. If `resource_acq` == \'bohb\', r is the largest value\n    <= max_t, where we have seen >= dimension(x) metric values. If\n    `resource_acq` == \'first\', r is the first milestone which config x would\n    reach when started.\n\n    Parameters\n    ----------\n    configspace : ConfigSpace.ConfigurationSpace\n        Config space of `train_fn`, equal to `train_fn.cs`\n    reward_attribute : str\n        Name of reward attribute reported by `train_fn`, equal to `reward_attr`\n        of scheduler\n    resource_attribute : str\n        Name of resource (or time) attribute reported by `train_fn`, equal to\n        `time_attr` of scheduler\n    debug_log : bool (default: False)\n        If True, both searcher and scheduler output an informative log, from\n        which the configs chosen and decisions being made can be traced.\n    first_is_default : bool (default: True)\n        If True, the first config to be evaluated is the default one of the\n        config space. Otherwise, this first config is drawn at random.\n    random_seed : int\n        Seed for pseudo-random number generator used.\n    num_init_random : int\n        See `GPFIFOSearcher`\n    num_init_candidates : int\n        See `GPFIFOSearcher`\n    num_fantasy_samples : int\n        See `GPFIFOSearcher`\n    initial_scoring : str\n        See `GPFIFOSearcher`\n    opt_nstarts : int\n        See `GPFIFOSearcher`\n    opt_maxiter : int\n        See `GPFIFOSearcher`\n    opt_warmstart : bool\n        See `GPFIFOSearcher`\n    opt_verbose : bool\n        See `GPFIFOSearcher`\n    opt_skip_init_length : int\n        See `GPFIFOSearcher`\n    opt_skip_period : int\n        See `GPFIFOSearcher`\n    map_reward : str or MapReward (default: \'1_minus_x\')\n        See `GPFIFOSearcher`\n    gp_resource_kernel : str\n        Surrogate model over criterion function f(x, r), x the config, r the\n        resource. Note that x is encoded to be a vector with entries in [0, 1],\n        and r is linearly mapped to [0, 1], while the criterion data is\n        normalized to mean 0, variance 1. The reference above provides details\n        on the models supported here. For the exponential decay kernel, the\n        base kernel over x is Matern 5/2 ARD.\n        Values are \'matern52\' (Matern 5/2 ARD kernel over [x, r]),\n        \'matern52-res-warp\' (Matern 5/2 ARD kernel over [x, r], with additional\n        warping on r),\n        \'exp-decay-sum\' (exponential decay kernel, with delta=0. This is the\n        additive kernel from Freeze-Thaw Bayesian Optimization),\n        \'exp-decay-delta1\' (exponential decay kernel, with delta=1),\n        \'exp-decay-combined\' (exponential decay kernel, with delta in [0, 1]\n        a hyperparameter).\n    resource_acq : str\n        Determines how the EI acquisition function is used (see above).\n        Values: \'bohb\', \'first\'\n    opt_skip_num_max_resource : bool\n        Parameter for hyperparameter fitting, skip predicate. If True, and\n        number of observations above `opt_skip_init_length`, fitting is done\n        only when there is a new datapoint at r = max_t, and skipped otherwise.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> import autogluon as ag\n    >>>\n    >>> @ag.args(\n    ...     lr=ag.space.Real(1e-3, 1e-2, log=True),\n    ...     wd=ag.space.Real(1e-3, 1e-2))\n    >>> def train_fn(args, reporter):\n    ...     print(\'lr: {}, wd: {}\'.format(args.lr, args.wd))\n    ...     for e in range(9):\n    ...         dummy_accuracy = 1 - np.power(1.8, -np.random.uniform(e, 2*e))\n    ...         reporter(epoch=e+1, accuracy=dummy_accuracy, lr=args.lr,\n    ...         wd=args.wd)\n    >>> searcher_options = {\n    ...     \'gp_resource_kernel\': \'matern52-res-warp\',\n    ...     \'opt_skip_num_max_resource\': True}\n    >>> scheduler = ag.scheduler.HyperbandScheduler(\n    ...     train_fn, searcher=\'bayesopt\', searcher_options=searcher_options,\n    ...     num_trials=10, reward_attr=\'accuracy\', time_attr=\'epoch\',\n    ...     max_t=10, grace_period=1, reduction_factor=3)\n\n    See Also\n    --------\n    GPFIFOSearcher\n    """"""\n    def __init__(self, **kwargs):\n        _gp_searcher = kwargs.get(\'_gp_searcher\')\n        if _gp_searcher is None:\n            _kwargs = check_and_merge_defaults(\n                kwargs, *gp_multifidelity_searcher_defaults(),\n                dict_name=\'search_options\')\n            _gp_searcher = gp_multifidelity_searcher_factory(**_kwargs)\n        super().__init__(\n            _gp_searcher.hp_ranges.config_space,\n            reward_attribute=kwargs.get(\'reward_attribute\'))\n        self.gp_searcher = _gp_searcher\n        self._resource_attribute = kwargs.get(\'resource_attribute\')\n        # This lock protects gp_searcher. We are not using self.LOCK, this\n        # can lead to deadlocks when superclass methods are called\n        self._gp_lock = mp.Lock()\n\n    def configure_scheduler(self, scheduler):\n        from ..scheduler.hyperband import HyperbandScheduler\n\n        assert isinstance(scheduler, HyperbandScheduler), \\\n            ""This searcher requires HyperbandScheduler scheduler""\n        super().configure_scheduler(scheduler)\n        with self._gp_lock:\n            self.gp_searcher.set_map_resource_to_index(\n                scheduler.map_resource_to_index())\n        self._resource_attribute = scheduler._time_attr\n\n    def get_config(self, **kwargs):\n        with self._gp_lock:\n            config_cs = self.gp_searcher.get_config(**kwargs)\n        return config_cs.get_dictionary()\n\n    def update(self, config, **kwargs):\n        super().update(config, **kwargs)\n        with self._gp_lock:\n            config_cs = self._to_config_cs(config)\n            self.gp_searcher.update(\n                config_cs, reward=kwargs[self._reward_attribute],\n                resource=int(kwargs[self._resource_attribute]))\n            # If evaluation task has terminated, cleanup pending evaluations\n            # which may have been overlooked\n            if kwargs.get(\'terminated\', False):\n                self.gp_searcher.cleanup_pending(config_cs)\n\n    def register_pending(self, config, milestone=None):\n        assert milestone is not None, \\\n            ""This searcher works with a multi-fidelity scheduler only""\n        with self._gp_lock:\n            config_cs = self._to_config_cs(config)\n            self.gp_searcher.register_pending(config_cs, milestone)\n\n    def remove_case(self, config, **kwargs):\n        with self._gp_lock:\n            config_cs = self._to_config_cs(config)\n            self.gp_searcher.remove_case(\n                config_cs, resource=int(kwargs[self._resource_attribute]))\n\n    def evaluation_failed(self, config, **kwargs):\n        with self._gp_lock:\n            config_cs = self._to_config_cs(config)\n            self.gp_searcher.evaluation_failed(config_cs)\n\n    def dataset_size(self):\n        with self._gp_lock:\n            return self.gp_searcher.dataset_size()\n\n    def cumulative_profile_record(self):\n        with self._gp_lock:\n            return self.gp_searcher.cumulative_profile_record()\n\n    def model_parameters(self):\n        with self._gp_lock:\n            return self.gp_searcher.get_params()\n\n    def get_state(self):\n        with self._gp_lock:\n            return self.gp_searcher.get_state()\n\n    def clone_from_state(self, state):\n        with self._gp_lock:\n            _gp_searcher = self.gp_searcher.clone_from_state(state)\n        # Use copy constructor\n        return GPMultiFidelitySearcher(\n            reward_attribute=self._reward_attribute,\n            resource_attribute=self._resource_attribute,\n            _gp_searcher=_gp_searcher)\n\n    @property\n    def debug_log(self):\n        with self._gp_lock:\n            return self.gp_searcher.debug_log\n\n    def _to_config_cs(self, config):\n        return _to_config_cs(self.gp_searcher.hp_ranges.config_space, config)\n'"
autogluon/searcher/grid_searcher.py,0,"b'from .searcher import BaseSearcher\nfrom sklearn.model_selection import ParameterGrid\n\n__all__ = [\'GridSearcher\']\n\n\nclass GridSearcher(BaseSearcher):\n    """"""Grid Searcher that exhaustively tries all possible configurations. \n       This Searcher can only be used for discrete search spaces of type :class:`autogluon.space.Categorical`\n\n    Examples\n    --------\n    >>> import autogluon as ag\n    >>> @ag.args(\n    ...     x=ag.space.Categorical(0, 1, 2),\n    ...     y=ag.space.Categorical(\'a\', \'b\', \'c\'))\n    >>> def train_fn(args, reporter):\n    ...     pass\n    >>> searcher = ag.searcher.GridSearcher(train_fn.cs)\n    >>> searcher.get_config()\n    Number of configurations for grid search is 9\n    {\'x.choice\': 2, \'y.choice\': 2}\n    """"""\n    def __init__(self, configspace, **kwargs):\n        super().__init__(\n            configspace, reward_attribute=kwargs.get(\'reward_attribute\'))\n        param_grid = {}\n        hp_ordering = configspace.get_hyperparameter_names()\n        for hp in hp_ordering:\n            hp_obj = configspace.get_hyperparameter(hp)\n            hp_type = str(type(hp_obj)).lower()\n            assert \'categorical\' in hp_type, \\\n                \'Only Categorical is supported, but {} is {}\'.format(hp, hp_type)\n            param_grid[hp] = hp_obj.choices\n\n        self._configs = list(ParameterGrid(param_grid))\n        print(\'Number of configurations for grid search is {}\'.format(len(self._configs)))\n\n    def __len__(self):\n        return len(self._configs)\n\n    def get_config(self):\n        """""" Return new hyperparameter configuration to try next.\n        """"""\n        return self._configs.pop()\n'"
autogluon/searcher/rl_controller.py,0,"b'import pickle\nimport mxnet as mx\nimport mxnet.gluon.nn as nn\nimport mxnet.ndarray as F\nimport multiprocessing\nfrom multiprocessing.pool import ThreadPool\n\nfrom ..core.space import *\nfrom .searcher import BaseSearcher\nfrom ..utils import keydefaultdict, update_params\nfrom collections import OrderedDict\n\n__all__ = [\'RLSearcher\', \'LSTMController\']\n\n\nclass RLSearcher(BaseSearcher):\n    """"""Reinforcement Learning Searcher for ConfigSpace\n\n    Parameters\n    ----------\n    kwspaces : keyword search spaces\n        The keyword spaces automatically generated by :func:`autogluon.args`\n\n    Examples\n    --------\n    >>> import autogluon as ag\n    >>> @ag.args(\n    ...     lr=ag.space.Real(1e-3, 1e-2, log=True),\n    ...     wd=ag.space.Real(1e-3, 1e-2))\n    >>> def train_fn(args, reporter):\n    ...     pass\n    >>> searcher = RLSearcher(train_fn.kwspaces)\n    >>> searcher.get_config()\n    """"""\n    def __init__(self, kwspaces, ctx=mx.cpu(), controller_type=\'lstm\',\n                 **kwargs):\n        super().__init__(\n            configspace=None, reward_attribute=kwargs.get(\'reward_attribute\'))\n        self._best_state_path = None\n        if controller_type == \'lstm\':\n            self.controller = LSTMController(kwspaces, ctx=ctx, **kwargs)\n        elif controller_type == \'alpha\':\n            self.controller = AlphaController(kwspaces, ctx=ctx, **kwargs)\n        elif controller_type == \'atten\':\n            self.controller = AttenController(kwspaces, ctx=ctx, **kwargs)\n        else:\n            raise NotImplementedError\n        self.controller.initialize(ctx=mx.cpu())\n        for _ in range(self.controller._nprefetch):\n            self.controller._prefetch()\n\n    def __repr__(self):\n        reprstr = self.__class__.__name__ + \'(\' +  \\\n            \'Number of Trials: {}.\'.format(len(self._results)) + \\\n            \'Best Config: {}\'.format(self.get_best_config()) + \\\n            \'Best Reward: {}\'.format(self.get_best_reward()) + \\\n            \')\'\n        return reprstr\n\n    def get_config(self, **kwargs):\n        return self.controller.sample()[0]\n\n    def get_best_config(self):\n        return self.controller.inference()\n\n    def state_dict(self, destination=None):\n        if destination is None:\n            destination = OrderedDict()\n            destination._metadata = OrderedDict()\n        destination[\'results\'] = pickle.dumps(self._results)\n        destination[\'controller_params\'] = pickle.dumps(self.controller.collect_params())\n        return destination\n\n    def load_state_dict(self, state_dict):\n        self._results=pickle.loads(state_dict[\'results\'])\n        update_params(self.controller, pickle.loads(state_dict[\'controller_params\']))\n\n\nclass BaseController(mx.gluon.Block):\n    def __init__(self, prefetch=4, num_workers=4, timeout=20, **kwargs):\n        super().__init__()\n        #manager = multiprocessing.Manager()\n        self._data_buffer = {}#manager.dict()\n        self._rcvd_idx = 0\n        self._sent_idx = 0\n        self._num_workers = num_workers\n        self._worker_pool = ThreadPool(self._num_workers)\n        self._timeout = timeout\n        self._nprefetch = prefetch\n\n    def sample(self, *args, **kwargs):\n        raise NotImplementedError\n\n    def _prefetch(self):\n        async_ret = self._worker_pool.apply_async(self.sample, ())\n        self._data_buffer[self._sent_idx] = async_ret\n        self._sent_idx += 1\n\n    def initialize(self, ctx=mx.cpu(), *args, **kwargs):\n        self.context = ctx[0] if isinstance(ctx, (list, tuple)) else ctx\n        super().initialize(ctx=ctx, *args, **kwargs)\n\n    def pre_sample(self):\n        if self._rcvd_idx == self._sent_idx:\n            self._prefetch()\n        self._prefetch()\n        assert self._rcvd_idx < self._sent_idx, ""rcvd_idx must be smaller than sent_idx""\n        try:\n            ret = self._data_buffer.pop(self._rcvd_idx)\n            self._rcvd_idx += 1\n            return  ret.get(timeout=self._timeout)\n        except multiprocessing.context.TimeoutError:\n            msg = \'\'\'Worker timed out after {} seconds. This might be caused by \\n\n            - Slow transform. Please increase timeout to allow slower data loading in each worker.\n            \'\'\'.format(self._timeout)\n            print(msg)\n            raise\n        except Exception:\n            self._worker_pool.terminate()\n            raise\n\n\n# Reference: https://github.com/carpedm20/ENAS-pytorch/\nclass LSTMController(BaseController):\n    def __init__(self, kwspaces, softmax_temperature=1.0, hidden_size=100,\n                 ctx=mx.cpu(), **kwargs):\n        super().__init__(**kwargs)\n        self.softmax_temperature = softmax_temperature\n        self.spaces = list(kwspaces.items())\n        self.hidden_size = hidden_size\n        self.context = ctx\n\n        # only support Categorical space for now\n        self.num_tokens = []\n        for _, space in self.spaces:\n            assert isinstance(space, Categorical)\n            self.num_tokens.append(len(space))\n        num_total_tokens = sum(self.num_tokens)\n\n        # controller lstm\n        self.encoder = nn.Embedding(num_total_tokens, hidden_size)\n        self.lstm = mx.gluon.rnn.LSTMCell(input_size=hidden_size, hidden_size=hidden_size)\n        self.decoders = nn.Sequential()\n        for idx, size in enumerate(self.num_tokens):\n            decoder = nn.Dense(in_units=hidden_size, units=size)\n            self.decoders.add(decoder)\n\n        def _init_hidden(batch_size):\n            zeros = mx.nd.zeros((batch_size, hidden_size), ctx=self.context)\n            return zeros, zeros.copy()\n\n        def _get_default_hidden(key):\n            return mx.nd.zeros((key, hidden_size), ctx=self.context)\n        \n        self.static_init_hidden = keydefaultdict(_init_hidden)\n        self.static_inputs = keydefaultdict(_get_default_hidden)\n\n    def forward(self, inputs, hidden, block_idx, is_embed):\n        if not is_embed:\n            embed = self.encoder(inputs)\n        else:\n            embed = inputs\n        _, (hx, cx) = self.lstm(embed, hidden)\n\n        logits = self.decoders[block_idx](hx)\n        logits = logits / self.softmax_temperature\n\n        return logits, (hx, cx)\n\n    def inference(self):\n        inputs = self.static_inputs[1]\n        hidden = self.static_init_hidden[1]\n        actions = []\n        for block_idx in range(len(self.num_tokens)):\n            logits, hidden = self.forward(inputs, hidden,\n                                          block_idx, is_embed=(block_idx==0))\n            probs = F.softmax(logits, axis=-1)\n            action = mx.nd.argmax(probs, 1)\n            actions.append(action)\n            inputs = action + sum(self.num_tokens[:block_idx])\n            inputs.detach()\n\n        config = {}\n        for i, action in enumerate(actions):\n            choice = action.asscalar()\n            k, space = self.spaces[i]\n            config[k] = int(choice)\n\n        return config\n\n    def sample(self, batch_size=1, with_details=False, with_entropy=False):\n        """"""\n        Returns\n        -------\n        configs : list of dict\n            list of configurations\n        """"""\n        inputs = self.static_inputs[batch_size]\n        hidden = self.static_init_hidden[batch_size]\n\n        actions = []\n        entropies = []\n        log_probs = []\n\n        for idx in range(len(self.num_tokens)):\n            logits, hidden = self.forward(inputs, hidden,\n                                          idx, is_embed=(idx==0))\n\n            probs = F.softmax(logits, axis=-1)\n            log_prob = F.log_softmax(logits, axis=-1)\n            entropy = -(log_prob * probs).sum(1, keepdims=False) if with_entropy else None\n\n            action = mx.random.multinomial(probs, 1)\n            ind = mx.nd.stack(mx.nd.arange(probs.shape[0], ctx=action.context),\n                              action.astype(\'float32\'))\n            selected_log_prob = F.gather_nd(log_prob, ind)\n\n            actions.append(action[:, 0])\n            entropies.append(entropy)\n            log_probs.append(selected_log_prob)\n\n            inputs = action[:, 0] + sum(self.num_tokens[:idx])\n            inputs.detach()\n\n        configs = []\n        for idx in range(batch_size):\n            config = {}\n            for i, action in enumerate(actions):\n                choice = action[idx].asscalar()\n                k, space = self.spaces[i]\n                config[k] = int(choice)\n            configs.append(config)\n\n        if with_details:\n            entropies = F.stack(*entropies, axis=1) if with_entropy else entropies\n            return configs, F.stack(*log_probs, axis=1), entropies\n        else:\n            return configs\n\n\nclass Alpha(mx.gluon.Block):\n    def __init__(self, shape):\n        super().__init__()\n        self.weight = self.params.get(\'weight\', shape=shape)\n\n    def forward(self, batch_size):\n        return self.weight.data().expand_dims(0).repeat(batch_size, axis=0)\n\n\nclass AttenController(BaseController):\n    def __init__(self, kwspaces, softmax_temperature=1.0, hidden_size=100,\n                 ctx=mx.cpu(), **kwargs):\n        super().__init__(**kwargs)\n        self.softmax_temperature = softmax_temperature\n        self.spaces = list(kwspaces.items())\n        self.context = ctx\n\n        # only support Categorical space for now\n        self.num_tokens = []\n        for _, space in self.spaces:\n            assert isinstance(space, Categorical)\n            self.num_tokens.append(len(space))\n        self.num_total_tokens = sum(self.num_tokens)\n        self.hidden_size = hidden_size\n\n        self.embedding = Alpha((self.num_total_tokens, hidden_size))\n        self.querry = mx.gluon.nn.Dense(hidden_size, in_units=hidden_size)\n        self.key = mx.gluon.nn.Dense(hidden_size, in_units=hidden_size)\n        self.value = mx.gluon.nn.Dense(1, in_units=hidden_size)\n\n    def inference(self):\n        # self-attention\n        x = self.embedding(1).reshape(-3, 0)#.squeeze() # b x action x h\n        kshape = (1, self.num_total_tokens, self.hidden_size)\n        vshape = (1, self.num_total_tokens, 1)\n        querry = self.querry(x).reshape(*kshape) # b x actions x h\n        key = self.key(x).reshape(*kshape) #b x actions x h\n        value = self.value(x).reshape(*vshape) # b x actions x 1\n        atten = mx.nd.linalg_gemm2(querry, key, transpose_b=True).softmax(axis=1)\n        alphas = mx.nd.linalg_gemm2(atten, value).squeeze(axis=-1)\n\n        actions = []\n        for idx in range(len(self.num_tokens)):\n            i0 = sum(self.num_tokens[:idx])\n            i1 = sum(self.num_tokens[:idx+1])\n            logits = alphas[:, i0: i1]\n            probs = F.softmax(logits, axis=-1)\n            action = mx.nd.argmax(probs, 1)\n            actions.append(action)\n\n        config = {}\n        for i, action in enumerate(actions):\n            choice = action.asscalar()\n            k, space = self.spaces[i]\n            config[k] = int(choice)\n\n        return config\n\n    def sample(self, batch_size=1, with_details=False, with_entropy=False):\n        # self-attention\n        x = self.embedding(batch_size).reshape(-3, 0)#.squeeze() # b x action x h\n        kshape = (batch_size, self.num_total_tokens, self.hidden_size)\n        vshape = (batch_size, self.num_total_tokens, 1)\n        querry = self.querry(x).reshape(*kshape) # b x actions x h\n        key = self.key(x).reshape(*kshape) #b x actions x h\n        value = self.value(x).reshape(*vshape) # b x actions x 1\n        atten = mx.nd.linalg_gemm2(querry, key, transpose_b=True).softmax(axis=1)\n        alphas = mx.nd.linalg_gemm2(atten, value).squeeze(axis=-1)\n\n        actions = []\n        entropies = []\n        log_probs = []\n        for idx in range(len(self.num_tokens)):\n            i0 = sum(self.num_tokens[:idx])\n            i1 = sum(self.num_tokens[:idx+1])\n            logits = alphas[:, i0: i1]\n\n            probs = F.softmax(logits, axis=-1)\n            log_prob = F.log_softmax(logits, axis=-1)\n\n            entropy = -(log_prob * probs).sum(1, keepdims=False) if with_entropy else None\n\n            action = mx.random.multinomial(probs, 1)\n            ind = mx.nd.stack(mx.nd.arange(probs.shape[0], ctx=action.context),\n                              action.astype(\'float32\'))\n            selected_log_prob = F.gather_nd(log_prob, ind)\n\n            actions.append(action[:, 0])\n            entropies.append(entropy)\n            log_probs.append(selected_log_prob)\n\n        configs = []\n        for idx in range(batch_size):\n            config = {}\n            for i, action in enumerate(actions):\n                choice = action[idx].asscalar()\n                k, space = self.spaces[i]\n                config[k] = int(choice)\n            configs.append(config)\n\n        if with_details:\n            entropies = F.stack(*entropies, axis=1) if with_entropy else entropies\n            return configs, F.stack(*log_probs, axis=1), entropies\n        else:\n            return configs\n\n\nclass AlphaController(BaseController):\n    def __init__(self, kwspaces, softmax_temperature=1.0, ctx=mx.cpu(), **kwargs):\n        super().__init__(**kwargs)\n        self.softmax_temperature = softmax_temperature\n        self.spaces = list(kwspaces.items())\n        self.context = ctx\n\n        # only support Categorical space for now\n        self.num_tokens = []\n        for _, space in self.spaces:\n            assert isinstance(space, Categorical)\n            self.num_tokens.append(len(space))\n\n        # controller lstm\n        self.decoders = nn.Sequential()\n        for idx, size in enumerate(self.num_tokens):\n            self.decoders.add(Alpha((size,)))\n\n    def inference(self):\n        actions = []\n\n        for idx in range(len(self.num_tokens)):\n            logits = self.decoders[idx](1)\n            probs = F.softmax(logits, axis=-1)\n            action = mx.nd.argmax(probs, 1)\n            actions.append(action)\n\n        config = {}\n        for i, action in enumerate(actions):\n            choice = action.asscalar()\n            k, space = self.spaces[i]\n            config[k] = int(choice)\n\n        return config\n\n    def sample(self, batch_size=1, with_details=False, with_entropy=False):\n        actions = []\n        entropies = []\n        log_probs = []\n\n        for idx in range(len(self.num_tokens)):\n            logits = self.decoders[idx](batch_size)\n\n            probs = F.softmax(logits, axis=-1)\n            log_prob = F.log_softmax(logits, axis=-1)\n\n            entropy = -(log_prob * probs).sum(1, keepdims=False) if with_entropy else None\n\n            action = mx.random.multinomial(probs, 1)\n            ind = mx.nd.stack(mx.nd.arange(probs.shape[0], ctx=action.context),\n                              action.astype(\'float32\'))\n            selected_log_prob = F.gather_nd(log_prob, ind)\n\n            actions.append(action[:, 0])\n            entropies.append(entropy)\n            log_probs.append(selected_log_prob)\n\n        configs = []\n        for idx in range(batch_size):\n            config = {}\n            for i, action in enumerate(actions):\n                choice = action[idx].asscalar()\n                k, space = self.spaces[i]\n                config[k] = int(choice)\n            configs.append(config)\n\n        if with_details:\n            entropies = F.stack(*entropies, axis=1) if with_entropy else entropies\n            return configs, F.stack(*log_probs, axis=1), entropies\n        else:\n            return configs\n'"
autogluon/searcher/searcher.py,0,"b'import logging\nimport multiprocessing as mp\nimport pickle\nfrom collections import OrderedDict\nimport numpy as np\n\nfrom ..utils import DeprecationHelper\nfrom autogluon.searcher.bayesopt.autogluon.debug_log import DebugLogPrinter\n\n__all__ = [\'BaseSearcher\',\n           \'RandomSearcher\',\n           \'RandomSampling\']\n\nlogger = logging.getLogger(__name__)\n\n\nclass BaseSearcher(object):\n    """"""Base Searcher (virtual class to inherit from if you are creating a custom Searcher).\n\n    Parameters\n    ----------\n    configspace: ConfigSpace.ConfigurationSpace\n        The configuration space to sample from. It contains the full\n        specification of the Hyperparameters with their priors\n    """"""\n    LOCK = mp.Lock()\n\n    def __init__(self, configspace, reward_attribute=None):\n        """"""\n        :param configspace: Configuration space to sample from or search in\n        :param reward_attribute: Reward attribute passed to update.\n            Default: \'accuracy\'\n\n        """"""\n        self.configspace = configspace\n        self._results = OrderedDict()\n        if reward_attribute is None:\n            reward_attribute = \'accuracy\'\n        self._reward_attribute = reward_attribute\n\n    def configure_scheduler(self, scheduler):\n        """"""\n        Some searchers need to obtain information from the scheduler they are\n        used with, in order to configure themselves.\n        This method has to be called before the searcher can be used.\n\n        The implementation here sets _reward_attribute for schedulers which\n        specify it.\n\n        Args:\n            scheduler: TaskScheduler\n                Scheduler the searcher is used with.\n\n        """"""\n        from autogluon.scheduler.fifo import FIFOScheduler\n\n        if isinstance(scheduler, FIFOScheduler):\n            self._reward_attribute = scheduler._reward_attr\n\n    @staticmethod\n    def _reward_while_pending():\n        """"""Defines the reward value which is assigned to config, while it is pending.""""""\n        return float(""-inf"")\n\n    def get_config(self, **kwargs):\n        """"""Function to sample a new configuration\n\n        This function is called inside TaskScheduler to query a new configuration\n\n        Args:\n        kwargs:\n            Extra information may be passed from scheduler to searcher\n        returns: (config, info_dict)\n            must return a valid configuration and a (possibly empty) info dict\n        """"""\n        raise NotImplementedError(f\'This function needs to be overwritten in {self.__class__.__name__}.\')\n\n    def update(self, config, **kwargs):\n        """"""Update the searcher with the newest metric report\n\n        kwargs must include the reward (key == reward_attribute). For\n        multi-fidelity schedulers (e.g., Hyperband), intermediate results are\n        also reported. In this case, kwargs must also include the resource\n        (key == resource_attribute).\n        We can also assume that if `register_pending(config, ...)` is received,\n        then later on, the searcher receives `update(config, ...)` with\n        milestone as resource.\n\n        Note that for Hyperband scheduling, update is also called for\n        intermediate results. _results is updated in any case, if the new\n        reward value is larger than the previously recorded one. This implies\n        that the best value for a config (in _results) could be obtained for\n        an intermediate resource, not the final one (virtue of early stopping).\n        Full details can be reconstruction from training_history of the\n        scheduler.\n\n        """"""\n        reward = kwargs.get(self._reward_attribute)\n        assert reward is not None, \\\n            ""Missing reward attribute \'{}\'"".format(self._reward_attribute)\n        with self.LOCK:\n            # _results is updated if reward is larger than the previous entry.\n            # This is the correct behaviour for multi-fidelity schedulers,\n            # where update is called multiple times for a config, with\n            # different resource levels.\n            config_pkl = pickle.dumps(config)\n            old_reward = self._results.get(config_pkl, reward)\n            self._results[config_pkl] = max(reward, old_reward)\n\n    def register_pending(self, config, milestone=None):\n        """"""\n        Signals to searcher that evaluation for config has started, but not\n        yet finished, which allows model-based searchers to register this\n        evaluation as pending.\n        For multi-fidelity schedulers, milestone is the next milestone the\n        evaluation will attend, so that model registers (config, milestone)\n        as pending.\n        In general, the searcher may assume that update is called with that\n        config at a later time.\n        """"""\n        pass\n\n    def remove_case(self, config, **kwargs):\n        """"""Remove data case previously appended by update\n\n        For searchers which maintain the dataset of all cases (reports) passed\n        to update, this method allows to remove one case from the dataset.\n        """"""\n        pass\n\n    def evaluation_failed(self, config, **kwargs):\n        """"""\n        Called by scheduler if an evaluation job for config failed. The\n        searcher should react appropriately (e.g., remove pending evaluations\n        for this config, and blacklist config).\n        """"""\n        pass\n\n    def dataset_size(self):\n        """"""\n        :return: Size of dataset a model is fitted to, or 0 if no model is\n            fitted to data\n        """"""\n        return 0\n\n    def cumulative_profile_record(self):\n        """"""\n        If profiling is supported and active, the searcher accumulates\n        profiling information over get_config calls, the corresponding dict\n        is returned here.\n        """"""\n        return dict()\n\n    def model_parameters(self):\n        """"""\n        :return: Dictionary with current model (hyper)parameter values if\n            this is supported; otherwise empty\n        """"""\n        return dict()\n\n    def get_best_reward(self):\n        """"""Calculates the reward (i.e. validation performance) produced by training under the best configuration identified so far.\n           Assumes higher reward values indicate better performance.\n        """"""\n        with self.LOCK:\n            if self._results:\n                return max(self._results.values())\n        return self._reward_while_pending()\n\n    def get_reward(self, config):\n        """"""Calculates the reward (i.e. validation performance) produced by training with the given configuration.\n        """"""\n        k = pickle.dumps(config)\n        with self.LOCK:\n            assert k in self._results\n            return self._results[k]\n\n    def get_best_config(self):\n        """"""Returns the best configuration found so far.\n        """"""\n        with self.LOCK:\n            if self._results:\n                config_pkl = max(self._results, key=self._results.get)\n                return pickle.loads(config_pkl)\n            else:\n                return dict()\n\n    def get_best_config_reward(self):\n        """"""Returns the best configuration found so far, as well as the reward associated with this best config.\n        """"""\n        with self.LOCK:\n            if self._results:\n                config_pkl = max(self._results, key=self._results.get)\n                return pickle.loads(config_pkl), self._results[config_pkl]\n            else:\n                return dict(), self._reward_while_pending()\n\n    def get_state(self):\n        """"""\n        Together with clone_from_state, this is needed in order to store and\n        re-create the mutable state of the searcher.\n\n        The state returned here must be pickle-able. If the searcher object is\n        pickle-able, the default is returning self.\n\n        :return: Pickle-able mutable state of searcher\n        """"""\n        return self\n\n    def clone_from_state(self, state):\n        """"""\n        Together with get_state, this is needed in order to store and\n        re-create the mutable state of the searcher.\n\n        Given state as returned by get_state, this method combines the\n        non-pickle-able part of the immutable state from self with state\n        and returns the corresponding searcher clone. Afterwards, self is\n        not used anymore.\n\n        If the searcher object as such is already pickle-able, then state is\n        already the new searcher object, and the default is just returning it.\n        In this default, self is ignored.\n\n        :param state: See above\n        :return: New searcher object\n        """"""\n        return state\n\n    @property\n    def debug_log(self):\n        """"""\n        Some BaseSearcher subclasses support writing a debug log, using\n        DebugLogPrinter. See RandomSearcher for an example.\n\n        :return: DebugLogPrinter; or None (not supported)\n        """"""\n        return None\n\n    def __repr__(self):\n        config, reward = self.get_best_config_reward()\n        reprstr = (\n                f\'{self.__class__.__name__}(\' +\n                f\'\\nConfigSpace: {self.configspace}.\' +\n                f\'\\nNumber of Trials: {len(self._results)}.\' +\n                f\'\\nBest Config: {config}\' +\n                f\'\\nBest Reward: {reward}\' +\n                f\')\'\n        )\n        return reprstr\n\n\nclass RandomSearcher(BaseSearcher):\n    """"""Searcher which randomly samples configurations to try next.\n\n    Parameters\n    ----------\n    configspace: ConfigSpace.ConfigurationSpace\n        The configuration space to sample from. It contains the full\n        specification of the set of hyperparameter values (with optional prior distributions over these values).\n\n    Examples\n    --------\n    By default, the searcher is created along with the scheduler. For example:\n\n    >>> import autogluon as ag\n    >>> @ag.args(\n    ...     lr=ag.space.Real(1e-3, 1e-2, log=True))\n    >>> def train_fn(args, reporter):\n    ...     reporter(accuracy = args.lr ** 2)\n    >>> scheduler = ag.scheduler.FIFOScheduler(\n    ...     train_fn, searcher=\'random\', num_trials=10,\n    ...     reward_attr=\'accuracy\')\n\n    This would result in a BaseSearcher with cs = train_fn.cs. You can also\n    create a RandomSearcher by hand:\n\n    >>> import ConfigSpace as CS\n    >>> import ConfigSpace.hyperparameters as CSH\n    >>> # create configuration space\n    >>> cs = CS.ConfigurationSpace()\n    >>> lr = CSH.UniformFloatHyperparameter(\'lr\', lower=1e-4, upper=1e-1, log=True)\n    >>> cs.add_hyperparameter(lr)\n    >>> # create searcher\n    >>> searcher = RandomSearcher(cs)\n    >>> searcher.get_config()\n    """"""\n    MAX_RETRIES = 100\n\n    def __init__(self, configspace, **kwargs):\n        super().__init__(\n            configspace, reward_attribute=kwargs.get(\'reward_attribute\'))\n        self._first_is_default = kwargs.get(\'first_is_default\', True)\n        # We use an explicit random_state here, in order to better support\n        # checkpoint and resume\n        self.random_state = np.random.RandomState(\n            kwargs.get(\'random_seed\', 31415927))\n        # Debug log printing (optional)\n        self._debug_log = kwargs.get(\'debug_log\')\n        if self._debug_log is not None:\n            if isinstance(self._debug_log, bool):\n                if self._debug_log:\n                    self._debug_log = DebugLogPrinter()\n                else:\n                    self._debug_log = None\n            else:\n                assert isinstance(self._debug_log, DebugLogPrinter)\n\n    def configure_scheduler(self, scheduler):\n        """"""\n        Some searchers need to obtain information from the scheduler they are\n        used with, in order to configure themselves.\n        This method has to be called before the searcher can be used.\n\n        The implementation here sets _reward_attribute for schedulers which\n        specify it.\n\n        Args:\n            scheduler: TaskScheduler\n                Scheduler the searcher is used with.\n\n        """"""\n        from autogluon.scheduler.hyperband import HyperbandScheduler\n\n        super().configure_scheduler(scheduler)\n        self._resource_attribute = None\n        # If the scheduler is Hyperband, we want to know the resource\n        # attribute, this is used for debug_log\n        if isinstance(scheduler, HyperbandScheduler):\n            self._resource_attribute = scheduler._time_attr\n\n    def get_config(self, **kwargs):\n        """"""Sample a new configuration at random\n\n        Returns\n        -------\n        A new configuration that is valid.\n        """"""\n        self.configspace.random = self.random_state\n        if self._debug_log is not None:\n            self._debug_log.start_get_config(\'random\')\n        if self._first_is_default and (not self._results):\n            # Try default config first\n            new_config = self.configspace.get_default_configuration().get_dictionary()\n        else:\n            new_config = self.configspace.sample_configuration().get_dictionary()\n        with self.LOCK:\n            num_tries = 1\n            while pickle.dumps(new_config) in self._results:\n                assert num_tries <= self.MAX_RETRIES, \\\n                    f""Cannot find new config in BaseSearcher, even after {self.MAX_RETRIES} trials""\n                new_config = self.configspace.sample_configuration().get_dictionary()\n                num_tries += 1\n            self._results[pickle.dumps(new_config)] = self._reward_while_pending()\n        if self._debug_log is not None:\n            self._debug_log.set_final_config(new_config)\n            # All get_config debug log info is only written here\n            self._debug_log.write_block()\n        return new_config\n\n    def update(self, config, **kwargs):\n        super().update(config, **kwargs)\n        if self._debug_log is not None:\n            config_id = self._debug_log.config_id(config)\n            reward = kwargs[self._reward_attribute]\n            if self._resource_attribute is not None:\n                # For HyperbandScheduler, also add the resource attribute\n                resource = int(kwargs[self._resource_attribute])\n                config_id = config_id + \':{}\'.format(resource)\n            msg = ""Update for config_id {}: reward = {}"".format(\n                config_id, reward)\n            logger.info(msg)\n\n    def get_state(self):\n        state = {\n            \'random_state\': self.random_state,\n            \'results\': self._results}\n        if self._debug_log is not None:\n            state[\'debug_log\'] = self._debug_log.get_mutable_state()\n        return state\n\n    def clone_from_state(self, state):\n        new_searcher = RandomSearcher(\n            self.configspace, reward_attribute=self._reward_attribute,\n            first_is_default=self._first_is_default,\n            debug_log=self._debug_log)\n        new_searcher.random_state = state[\'random_state\']\n        new_searcher._results = state[\'results\']\n        if self._debug_log and \'debug_log\' in state:\n            new_searcher._debug_log.set_mutable_state(state[\'debug_log\'])\n        return new_searcher\n\n    @property\n    def debug_log(self):\n        return self._debug_log\n\n\nRandomSampling = DeprecationHelper(RandomSearcher, \'RandomSampling\')\n'"
autogluon/searcher/searcher_factory.py,0,"b'from .searcher import RandomSearcher\nfrom .skopt_searcher import SKoptSearcher\nfrom .grid_searcher import GridSearcher\nfrom .gp_searcher import GPFIFOSearcher, GPMultiFidelitySearcher\n\n__all__ = [\'searcher_factory\']\n\n\ndef searcher_factory(name, **kwargs):\n    """"""Factory for searcher objects\n\n    This function creates searcher objects from string argument name and\n    additional kwargs. It is typically called in the constructor of a\n    scheduler (see FIFOScheduler), which provides most of the required kwargs.\n\n    Note: RLSearcher is not supported here, because its **kwargs are different\n    from other searchers (no \'configspace\').\n\n    The \'bayesopt\' searchers depend on which scheduler is used (GPFIFOSearcher\n    for FIFOScheduler, GPMultiFidelitySearcher for HyperbandScheduler). They\n    have many additional parameters in kwargs (see docstrings for\n    GPFIFOSearcher, GPMultiFidelitySearcher).\n\n    Parameters\n    ----------\n    name : str\n        Searcher type. Supported are \'random\' (RandomSearcher), \'skopt\'\n        (SKoptSearcher), \'grid\' (GridSearcher), \'bayesopt\' (GPFIFOSearcher,\n        GPMultiFidelitySearcher)\n    configspace : ConfigSpace.ConfigurationSpace\n        Config space of train_fn, equal to train_fn.cs\n    scheduler : str\n        Scheduler type the searcher is used in. Supported are \'fifo\'\n        (FIFOScheduler), \'hyperband_stopping\', \'hyperband_promotion\'\n        (HyperbandScheduler: type = \'stopping\' or \'promotion\')\n    reward_attribute : str\n        Name of reward attribute reported by train_fn, equal to\n        reward_attr\n    resource_attribute : str [only for HyperbandScheduler]\n        Name of resource (or time) attribute reported by train_fn,\n        equal to time_attr\n    min_epochs : int [only for HyperbandScheduler]\n        Minimum value of resource attribute, equal to grace_period\n    max_epochs : int [only for HyperbandScheduler]\n        Maximum value of resource attribute, equal to max_t\n    debug_log : bool (default: False)\n        Supported by \'random\', \'bayesopt\'. If True, both searcher and\n        scheduler output an informative log, from which the configs chosen\n        and decisions being made can be traced.\n    first_is_default : bool (default: True)\n        Supported by \'random\', \'skopt\', \'bayesopt\'. If True, the first config\n        to be evaluated is the default one of the config space. Otherwise, this\n        first config is drawn at random.\n    random_seed : int\n        Seed for pseudo-random number generator used.\n\n    See Also\n    --------\n    GPFIFOSearcher\n    GPMultiFidelitySearcher\n    """"""\n    if name == \'random\':\n        return RandomSearcher(**kwargs)\n    elif name == \'skopt\':\n        _check_supported_scheduler(\n            name, kwargs.get(\'scheduler\'), {\'fifo\'})\n        return SKoptSearcher(**kwargs)\n    elif name == \'grid\':\n        return GridSearcher(**kwargs)\n    elif name == \'bayesopt\':\n        # Gaussian process based Bayesian optimization\n        # The searchers and their kwargs differ depending on the scheduler\n        # type (fifo, hyperband_*)\n        scheduler = _check_supported_scheduler(\n            name, kwargs.get(\'scheduler\'),\n            {\'fifo\', \'hyperband_stopping\', \'hyperband_promotion\'})\n        if scheduler == \'fifo\':\n            return GPFIFOSearcher(**kwargs)\n        else:\n            return GPMultiFidelitySearcher(**kwargs)\n    else:\n        raise AssertionError(""name = \'{}\' not supported"".format(name))\n\n\ndef _check_supported_scheduler(name, scheduler, supp_schedulers):\n    assert scheduler is not None, \\\n        ""Scheduler must set search_options[\'scheduler\']""\n    assert scheduler in supp_schedulers, \\\n        ""Searcher \'{}\' only works with schedulers {} (not with \'{}\')"".format(\n            name, supp_schedulers, scheduler)\n    return scheduler\n'"
autogluon/searcher/skopt_searcher.py,0,"b'import pickle\nimport logging\n\nfrom ..utils import warning_filter\nwith warning_filter():\n    from skopt import Optimizer\n    from skopt.space import Integer, Real, Categorical\n\nfrom .searcher import BaseSearcher\n\n__all__ = [\'SKoptSearcher\']\n\nlogger = logging.getLogger(__name__)\n\n\nclass SKoptSearcher(BaseSearcher):\n    """"""SKopt Searcher that uses Bayesian optimization to suggest new hyperparameter configurations. \n        Requires that \'scikit-optimize\' package is installed.\n    \n    Parameters\n    ----------\n    configspace: ConfigSpace.ConfigurationSpace\n        The configuration space to sample from. It contains the full\n        specification of the Hyperparameters with their priors\n    kwargs: Optional arguments passed to skopt.optimizer.Optimizer class. \n        Please see documentation at this link: `skopt.optimizer.Optimizer <http://scikit-optimize.github.io/optimizer/index.html#skopt.optimizer.Optimizer>`_\n        These kwargs be used to specify which surrogate model Bayesian optimization should rely on,\n        which acquisition function to use, how to optimize the acquisition function, etc.\n        The skopt library provides comprehensive Bayesian optimization functionality,\n        where popular non-default kwargs options here might include: \n        \n        - base_estimator = \'GP\' or \'RF\' or \'ET\' or \'GBRT\' (to specify different surrogate models like Gaussian Processes, Random Forests, etc)\n        \n        - acq_func = \'LCB\' or \'EI\' or \'PI\' or \'gp_hedge\' (to specify different acquisition functions like Lower Confidence Bound, Expected Improvement, etc)\n        \n        For example, we can tell our Searcher to perform Bayesian optimization with a Random Forest surrogate model\n        and use the Expected Improvement acquisition function by invoking: \n        `SKoptSearcher(cs, base_estimator=\'RF\', acq_func=\'EI\')`\n    \n    Examples\n    --------\n    By default, the searcher is created along with the scheduler. For example:\n\n    >>> import autogluon as ag\n    >>> @ag.args(\n    ...     lr=ag.space.Real(1e-3, 1e-2, log=True))\n    >>> def train_fn(args, reporter):\n    ...     reporter(accuracy = args.lr ** 2)\n    >>> search_options = {\'base_estimator\': \'RF\', \'acq_func\': \'EI\'}\n    >>> scheduler = ag.scheduler.FIFOScheduler(\n    ...     train_fn, searcher=\'skopt\', search_options=search_options,\n    ...     num_trials=10, reward_attr=\'accuracy\')\n\n    This would result in a SKoptSearcher with cs = train_fn.cs. You can also\n    create a SKoptSearcher by hand:\n\n    >>> import autogluon as ag\n    >>> @ag.args(\n    ...     lr=ag.space.Real(1e-3, 1e-2, log=True),\n    ...     wd=ag.space.Real(1e-3, 1e-2))\n    >>> def train_fn(args, reporter):\n    ...     pass\n    >>> searcher = ag.searcher.SKoptSearcher(train_fn.cs)\n    >>> searcher.get_config()\n    {\'lr\': 0.0031622777, \'wd\': 0.0055}\n    >>> searcher = SKoptSearcher(\n    ...     train_fn.cs, reward_attribute=\'accuracy\', base_estimator=\'RF\',\n    ...     acq_func=\'EI\')\n    >>> next_config = searcher.get_config()\n    >>> searcher.update(next_config, accuracy=10.0)  # made-up value\n    \n    .. note::\n        \n        - get_config() cannot ensure valid configurations for conditional spaces since skopt \n        does not contain this functionality as it is not integrated with ConfigSpace. \n        If invalid config is produced, `SKoptSearcher.get_config()` will catch these Exceptions and revert to `random_config()` instead.\n        \n        - get_config(max_tries) uses skopt\'s batch BayesOpt functionality to query at most \n        max_tries number of configs to try out.\n        If all of these have configs have already been scheduled to try (might happen in asynchronous setting), \n        then get_config simply reverts to random search via random_config().\n    """"""\n    errors_tohandle = (ValueError, TypeError, RuntimeError)\n\n    def __init__(self, configspace, **kwargs):\n        super().__init__(\n            configspace, reward_attribute=kwargs.get(\'reward_attribute\'))\n        self.hp_ordering = configspace.get_hyperparameter_names() # fix order of hyperparams in configspace.\n        skopt_hpspace = []\n        for hp in self.hp_ordering:\n            hp_obj = configspace.get_hyperparameter(hp)\n            hp_type = str(type(hp_obj)).lower() # type of hyperparam\n            if \'integer\' in hp_type:\n                hp_dimension = Integer(low=int(hp_obj.lower), high=int(hp_obj.upper),name=hp)\n            elif \'float\' in hp_type:\n                if hp_obj.log: # log10-scale hyperparmeter\n                    hp_dimension = Real(low=float(hp_obj.lower), high=float(hp_obj.upper), prior=\'log-uniform\', name=hp)\n                else:\n                    hp_dimension = Real(low=float(hp_obj.lower), high=float(hp_obj.upper), name=hp)\n            elif \'categorical\' in hp_type:\n                hp_dimension = Categorical(hp_obj.choices, name=hp)\n            elif \'ordinal\' in hp_type:\n                hp_dimension = Categorical(hp_obj.sequence, name=hp)\n            else:\n                raise ValueError(""unknown hyperparameter type: %s"" % hp)\n            skopt_hpspace.append(hp_dimension)\n        skopt_keys = {\n            \'base_estimator\', \'n_random_starts\', \'n_initial_points\',\n            \'acq_func\', \'acq_optimizer\', \'random_state\',  \'model_queue_size\',\n            \'acq_func_kwargs\', \'acq_optimizer_kwargs\'}\n        skopt_kwargs = self._filter_skopt_kwargs(kwargs, skopt_keys)\n        self.bayes_optimizer = Optimizer(\n            dimensions=skopt_hpspace, **skopt_kwargs)\n\n    @staticmethod\n    def _filter_skopt_kwargs(kwargs, keys):\n        return {k: v for k, v in kwargs.items() if k in keys}\n\n    def configure_scheduler(self, scheduler):\n        from ..scheduler.fifo import FIFOScheduler\n\n        assert isinstance(scheduler, FIFOScheduler), \\\n            ""This searcher requires FIFOScheduler scheduler""\n        super().configure_scheduler(scheduler)\n\n    def get_config(self, **kwargs):\n        """"""Function to sample a new configuration\n        This function is called to query a new configuration that has not yet been tried.\n        Asks for one point at a time from skopt, up to max_tries. \n        If an invalid hyperparameter configuration is proposed by skopt, then reverts to random search\n        (since skopt configurations cannot handle conditional spaces like ConfigSpace can).\n        TODO: may loop indefinitely due to no termination condition (like RandomSearcher.get_config() ) \n        \n        Parameters\n        ----------\n        max_tries: int, default = 1e2\n            The maximum number of tries to ask for a unique config from skopt before reverting to random search.\n        """"""\n        max_tries = kwargs.get(\'max_tries\', 1e2)\n        if len(self._results) == 0: # no hyperparams have been tried yet, first try default config\n            return self.default_config()\n        try:\n            new_points = self.bayes_optimizer.ask(n_points=1) # initially ask for one new config\n            new_config_cs = self.skopt2config(new_points[0]) # hyperparameter-config to evaluate\n            try:\n                new_config_cs.is_valid_configuration()\n                new_config = new_config_cs.get_dictionary()\n                if pickle.dumps(new_config) not in self._results.keys(): # have not encountered this config\n                    self._results[pickle.dumps(new_config)] = self._reward_while_pending()\n                    return new_config\n            except self.errors_tohandle:\n                pass\n            new_points = self.bayes_optimizer.ask(n_points=max_tries) # ask skopt for many configs since first one was not new\n            i = 1 # which new point to return as new_config, we already tried the first point above\n            while i < max_tries:\n                new_config_cs = self.skopt2config(new_points[i]) # hyperparameter-config to evaluate\n                try:\n                    new_config_cs.is_valid_configuration()\n                    new_config = new_config_cs.get_dictionary()\n                    if (pickle.dumps(new_config) not in self._results.keys()): # have not encountered this config\n                        self._results[pickle.dumps(new_config)] = self._reward_while_pending()\n                        return new_config\n                except self.errors_tohandle:\n                    pass\n                i += 1\n        except self.errors_tohandle:\n            pass\n        logger.info(""used random search instead of skopt to produce new hyperparameter configuration in this trial"")\n        return self.random_config()\n    \n    def default_config(self):\n        """""" Function to return the default configuration that should be tried first.\n        \n        Returns\n        -------\n        returns: config\n        """"""\n        new_config_cs = self.configspace.get_default_configuration()\n        new_config = new_config_cs.get_dictionary()\n        self._results[pickle.dumps(new_config)] = self._reward_while_pending()\n        return new_config\n        \n    def random_config(self):\n        """"""Function to randomly sample a new configuration (which is ensured to be valid in the case of conditional hyperparameter spaces).\n        """"""\n        # TODO: may loop indefinitely due to no termination condition (like RandomSearcher.get_config() ) \n        new_config = self.configspace.sample_configuration().get_dictionary()\n        while pickle.dumps(new_config) in self._results.keys():\n            new_config = self.configspace.sample_configuration().get_dictionary()\n        self._results[pickle.dumps(new_config)] = self._reward_while_pending()\n        return new_config\n\n    def update(self, config, **kwargs):\n        """"""Update the searcher with the newest metric report.\n        """"""\n        super().update(config, **kwargs)\n        reward = kwargs[self._reward_attribute]\n        try:\n            self.bayes_optimizer.tell(self.config2skopt(config),\n                                      -reward)  # provide negative reward since skopt performs minimization\n        except self.errors_tohandle:\n            logger.info(""surrogate model not updated this trial"")\n\n    def config2skopt(self, config):\n        """""" Converts autogluon config (dict object) to skopt format (list object).\n\n        Returns\n        -------\n        Object of same type as: `skOpt.Optimizer.ask()`\n        """"""\n        point = []\n        for hp in self.hp_ordering:\n            point.append(config[hp])\n        return point\n    \n    def skopt2config(self, point):\n        """""" Converts skopt point (list object) to autogluon config format (dict object. \n        \n        Returns\n        -------\n        Object of same type as: `RandomSampling.configspace.sample_configuration().get_dictionary()`\n        """"""\n        config = self.configspace.sample_configuration()\n        for i in range(len(point)):\n            hp = self.hp_ordering[i]\n            config[hp] = point[i]\n        return config\n'"
autogluon/task/__init__.py,0,"b""import logging\nlogging.basicConfig(format='%(message)s') # just print message in logs\n\nfrom .base import BaseTask\nfrom .image_classification import ImageClassification\nfrom .object_detection import ObjectDetection, Detector\nfrom .text_classification import TextClassification\nfrom .tabular_prediction import TabularPrediction\nfrom . import image_classification, object_detection, text_classification, tabular_prediction\n"""
autogluon/utils/__init__.py,0,b'from .files import *\nfrom .miscs import *\nfrom .plots import *\nfrom .tqdm import tqdm\nfrom .dataset import *\nfrom .mxutils import *\nfrom .deprecate import *\nfrom .try_import import *\nfrom .learning_rate import *\nfrom .edict import EasyDict\nfrom .serialization import *\nfrom .dataloader import DataLoader\nfrom .sync_remote import *\nfrom .custom_queue import Queue\nfrom .file_helper import *\nfrom .plot_network import plot_network\nfrom .defaultdict import keydefaultdict\nfrom .util_decorator import classproperty\nfrom .custom_process import CustomProcess\n'
autogluon/utils/augment.py,0,"b'# code adapted from:\n# https://github.com/kakaobrain/fast-autoaugment\n# https://github.com/rpmcruz/autoaugment\nimport math\nimport random\n\nimport numpy as np\nfrom collections import defaultdict\nimport PIL, PIL.ImageOps, PIL.ImageEnhance, PIL.ImageDraw\nfrom .pil_transforms import ToPIL, ToNDArray\n\nfrom mxnet.gluon.block import Block\n\nrandom_mirror = True\n\nRESAMPLE_MODE=PIL.Image.BICUBIC\n\ndef ShearX(img, v):  # [-0.3, 0.3]\n    assert -0.3 <= v <= 0.3\n    if random_mirror and random.random() > 0.5:\n        v = -v\n    return img.transform(img.size, PIL.Image.AFFINE, (1, v, 0, 0, 1, 0),\n                         RESAMPLE_MODE)\n\n\ndef ShearY(img, v):  # [-0.3, 0.3]\n    assert -0.3 <= v <= 0.3\n    if random_mirror and random.random() > 0.5:\n        v = -v\n    return img.transform(img.size, PIL.Image.AFFINE, (1, 0, 0, v, 1, 0),\n                         RESAMPLE_MODE)\n\n\ndef TranslateX(img, v):  # [-150, 150] => percentage: [-0.45, 0.45]\n    assert -0.45 <= v <= 0.45\n    if random_mirror and random.random() > 0.5:\n        v = -v\n    v = v * img.size[0]\n    return img.transform(img.size, PIL.Image.AFFINE, (1, 0, v, 0, 1, 0),\n                         RESAMPLE_MODE)\n\n\ndef TranslateY(img, v):  # [-150, 150] => percentage: [-0.45, 0.45]\n    assert -0.45 <= v <= 0.45\n    if random_mirror and random.random() > 0.5:\n        v = -v\n    v = v * img.size[1]\n    return img.transform(img.size, PIL.Image.AFFINE, (1, 0, 0, 0, 1, v),\n                         RESAMPLE_MODE)\n\n\ndef TranslateXabs(img, v):  # [-150, 150] => percentage: [-0.45, 0.45]\n    assert 0 <= v\n    if random.random() > 0.5:\n        v = -v\n    return img.transform(img.size, PIL.Image.AFFINE, (1, 0, v, 0, 1, 0),\n                         RESAMPLE_MODE)\n\n\ndef TranslateYabs(img, v):  # [-150, 150] => percentage: [-0.45, 0.45]\n    assert 0 <= v\n    if random.random() > 0.5:\n        v = -v\n    return img.transform(img.size, PIL.Image.AFFINE, (1, 0, 0, 0, 1, v),\n                         RESAMPLE_MODE)\n\n\ndef Rotate(img, v):  # [-30, 30]\n    assert -30 <= v <= 30\n    if random_mirror and random.random() > 0.5:\n        v = -v\n    return img.rotate(v)\n\n\ndef AutoContrast(img, _):\n    return PIL.ImageOps.autocontrast(img)\n\n\ndef Invert(img, _):\n    return PIL.ImageOps.invert(img)\n\n\ndef Equalize(img, _):\n    return PIL.ImageOps.equalize(img)\n\n\ndef Flip(img, _):  # not from the paper\n    return PIL.ImageOps.mirror(img)\n\n\ndef Solarize(img, v):  # [0, 256]\n    assert 0 <= v <= 256\n    return PIL.ImageOps.solarize(img, v)\n\n\ndef SolarizeAdd(img, addition=0, threshold=128):\n    img_np = np.array(img).astype(np.int)\n    img_np = img_np + addition\n    img_np = np.clip(img_np, 0, 255)\n    img_np = img_np.astype(np.uint8)\n    img = PIL.Image.fromarray(img_np)\n    return PIL.ImageOps.solarize(img, threshold)\n\n\ndef Posterize(img, v):  # [4, 8]\n    #assert 4 <= v <= 8\n    v = int(v)\n    return PIL.ImageOps.posterize(img, v)\n\ndef Contrast(img, v):  # [0.1,1.9]\n    assert 0.1 <= v <= 1.9\n    return PIL.ImageEnhance.Contrast(img).enhance(v)\n\n\ndef Color(img, v):  # [0.1,1.9]\n    assert 0.1 <= v <= 1.9\n    return PIL.ImageEnhance.Color(img).enhance(v)\n\n\ndef Brightness(img, v):  # [0.1,1.9]\n    assert 0.1 <= v <= 1.9\n    return PIL.ImageEnhance.Brightness(img).enhance(v)\n\n\ndef Sharpness(img, v):  # [0.1,1.9]\n    assert 0.1 <= v <= 1.9\n    return PIL.ImageEnhance.Sharpness(img).enhance(v)\n\n\ndef CutoutAbs(img, v):  # [0, 60] => percentage: [0, 0.2]\n    # assert 0 <= v <= 20\n    if v < 0:\n        return img\n    w, h = img.size\n    x0 = np.random.uniform(w)\n    y0 = np.random.uniform(h)\n\n    x0 = int(max(0, x0 - v / 2.))\n    y0 = int(max(0, y0 - v / 2.))\n    x1 = min(w, x0 + v)\n    y1 = min(h, y0 + v)\n\n    xy = (x0, y0, x1, y1)\n    color = (125, 123, 114)\n    # color = (0, 0, 0)\n    img = img.copy()\n    PIL.ImageDraw.Draw(img).rectangle(xy, color)\n    return img\n\n\ndef Cutout(img, v):  # [0, 60] => percentage: [0, 0.2]\n    assert 0.0 <= v <= 0.2\n    if v <= 0.:\n        return img\n\n    v = v * img.size[0]\n    return CutoutAbs(img, v)\n\n\n\ndef TranslateYAbs(img, v):  # [-150, 150] => percentage: [-0.45, 0.45]\n    assert 0 <= v <= 10\n    if random.random() > 0.5:\n        v = -v\n    return img.transform(img.size, PIL.Image.AFFINE, (1, 0, 0, 0, 1, v),\n                         resample=RESAMPLE_MODE)\n\n\ndef TranslateXAbs(img, v):  # [-150, 150] => percentage: [-0.45, 0.45]\n    assert 0 <= v <= 10\n    if random.random() > 0.5:\n        v = -v\n    return img.transform(img.size, PIL.Image.AFFINE, (1, 0, v, 0, 1, 0),\n                         resample=RESAMPLE_MODE)\n\n\ndef Posterize2(img, v):  # [0, 4]\n    assert 0 <= v <= 4\n    v = int(v)\n    return PIL.ImageOps.posterize(img, v)\n\n\n\ndef SamplePairing(imgs):  # [0, 0.4]\n    def f(img1, v):\n        i = np.random.choice(len(imgs))\n        img2 = PIL.Image.fromarray(imgs[i])\n        return PIL.Image.blend(img1, img2, v)\n\n    return f\n\n\ndef augment_list(for_autoaug=True):  # 16 oeprations and their ranges\n    l = [\n        (ShearX, -0.3, 0.3),  # 0\n        (ShearY, -0.3, 0.3),  # 1\n        (TranslateX, -0.45, 0.45),  # 2\n        (TranslateY, -0.45, 0.45),  # 3\n        (Rotate, -30, 30),  # 4\n        (AutoContrast, 0, 1),  # 5\n        (Invert, 0, 1),  # 6\n        (Equalize, 0, 1),  # 7\n        (Solarize, 0, 256),  # 8\n        (Posterize, 4, 8),  # 9\n        (Contrast, 0.1, 1.9),  # 10\n        (Color, 0.1, 1.9),  # 11\n        (Brightness, 0.1, 1.9),  # 12\n        (Sharpness, 0.1, 1.9),  # 13\n        (Cutout, 0, 0.2),  # 14\n        # (SamplePairing(imgs), 0, 0.4),  # 15\n    ]\n    if for_autoaug:\n        l += [\n            (CutoutAbs, 0, 20),  # compatible with auto-augment\n            (Posterize2, 0, 4),  # 9\n            (TranslateXAbs, 0, 10),  # 9\n            (TranslateYAbs, 0, 10),  # 9\n        ]\n    return l\n\n\n\naugment_dict = {fn.__name__: (fn, v1, v2) for fn, v1, v2 in augment_list()}\n\nPARAMETER_MAX = 10\n\n\ndef float_parameter(level, maxval):\n    return float(level) * maxval / PARAMETER_MAX\n\n\ndef int_parameter(level, maxval):\n    return int(float_parameter(level, maxval))\n\n\ndef autoaug2fastaa(f):\n    def autoaug():\n        mapper = defaultdict(lambda: lambda x: x)\n        mapper.update({\n            \'ShearX\': lambda x: float_parameter(x, 0.3),\n            \'ShearY\': lambda x: float_parameter(x, 0.3),\n            \'TranslateX\': lambda x: int_parameter(x, 10),\n            \'TranslateY\': lambda x: int_parameter(x, 10),\n            \'Rotate\': lambda x: int_parameter(x, 30),\n            \'Solarize\': lambda x: 256 - int_parameter(x, 256),\n            \'Posterize2\': lambda x: 4 - int_parameter(x, 4),\n            \'Contrast\': lambda x: float_parameter(x, 1.8) + .1,\n            \'Color\': lambda x: float_parameter(x, 1.8) + .1,\n            \'Brightness\': lambda x: float_parameter(x, 1.8) + .1,\n            \'Sharpness\': lambda x: float_parameter(x, 1.8) + .1,\n            \'CutoutAbs\': lambda x: int_parameter(x, 20)\n        })\n\n        def low_high(name, prev_value):\n            _, low, high = get_augment(name)\n            return float(prev_value - low) / (high - low)\n\n        policies = f()\n        new_policies = []\n        for policy in policies:\n            new_policies.append([(name, pr, low_high(name, mapper[name](level))) for name, pr, level in policy])\n        return new_policies\n\n    return autoaug\n\n\ndef rand_augment_list():  # 16 oeprations and their ranges\n    l = [\n        (AutoContrast, 0, 1),\n        (Equalize, 0, 1),\n        (Invert, 0, 1),\n        (Rotate, 0, 30),\n        (Posterize, 0, 4),\n        (Solarize, 0, 256),\n        (SolarizeAdd, 0, 110),\n        (Color, 0.1, 1.9),\n        (Contrast, 0.1, 1.9),\n        (Brightness, 0.1, 1.9),\n        (Sharpness, 0.1, 1.9),\n        (ShearX, 0., 0.3),\n        (ShearY, 0., 0.3),\n        (CutoutAbs, 0, 40),\n        (TranslateXabs, 0., 100),\n        (TranslateYabs, 0., 100),\n    ]\n\n    return l\n\n\n@autoaug2fastaa\ndef autoaug_cifar10_policies():\n    return [\n        [(\'Invert\', 0.1, 7), (\'Contrast\', 0.2, 6)],\n        [(\'Rotate\', 0.7, 2), (\'TranslateXAbs\', 0.3, 9)],\n        [(\'Sharpness\', 0.8, 1), (\'Sharpness\', 0.9, 3)],\n        [(\'ShearY\', 0.5, 8), (\'TranslateYAbs\', 0.7, 9)],\n        [(\'AutoContrast\', 0.5, 8), (\'Equalize\', 0.9, 2)],\n        [(\'ShearY\', 0.2, 7), (\'Posterize2\', 0.3, 7)],\n        [(\'Color\', 0.4, 3), (\'Brightness\', 0.6, 7)],\n        [(\'Sharpness\', 0.3, 9), (\'Brightness\', 0.7, 9)],\n        [(\'Equalize\', 0.6, 5), (\'Equalize\', 0.5, 1)],\n        [(\'Contrast\', 0.6, 7), (\'Sharpness\', 0.6, 5)],\n        [(\'Color\', 0.7, 7), (\'TranslateXAbs\', 0.5, 8)],\n        [(\'Equalize\', 0.3, 7), (\'AutoContrast\', 0.4, 8)],\n        [(\'TranslateYAbs\', 0.4, 3), (\'Sharpness\', 0.2, 6)],\n        [(\'Brightness\', 0.9, 6), (\'Color\', 0.2, 6)],\n        [(\'Solarize\', 0.5, 2), (\'Invert\', 0.0, 3)],\n        [(\'Equalize\', 0.2, 0), (\'AutoContrast\', 0.6, 0)],\n        [(\'Equalize\', 0.2, 8), (\'Equalize\', 0.6, 4)],\n        [(\'Color\', 0.9, 9), (\'Equalize\', 0.6, 6)],\n        [(\'AutoContrast\', 0.8, 4), (\'Solarize\', 0.2, 8)],\n        [(\'Brightness\', 0.1, 3), (\'Color\', 0.7, 0)],\n        [(\'Solarize\', 0.4, 5), (\'AutoContrast\', 0.9, 3)],\n        [(\'TranslateYAbs\', 0.9, 9), (\'TranslateYAbs\', 0.7, 9)],\n        [(\'AutoContrast\', 0.9, 2), (\'Solarize\', 0.8, 3)],\n        [(\'Equalize\', 0.8, 8), (\'Invert\', 0.1, 3)],\n        [(\'TranslateYAbs\', 0.7, 9), (\'AutoContrast\', 0.9, 1)],\n    ]\n\n@autoaug2fastaa\ndef autoaug_imagenet_policies():\n    return [\n        [(\'Posterize2\', 0.4, 8), (\'Rotate\', 0.6, 9)],\n        [(\'Solarize\', 0.6, 5), (\'AutoContrast\', 0.6, 5)],\n        [(\'Equalize\', 0.8, 8), (\'Equalize\', 0.6, 3)],\n        [(\'Posterize2\', 0.6, 7), (\'Posterize2\', 0.6, 6)],\n        [(\'Equalize\', 0.4, 7), (\'Solarize\', 0.2, 4)],\n        [(\'Equalize\', 0.4, 4), (\'Rotate\', 0.8, 8)],\n        [(\'Solarize\', 0.6, 3), (\'Equalize\', 0.6, 7)],\n        [(\'Posterize2\', 0.8, 5), (\'Equalize\', 1.0, 2)],\n        [(\'Rotate\', 0.2, 3), (\'Solarize\', 0.6, 8)],\n        [(\'Equalize\', 0.6, 8), (\'Posterize2\', 0.4, 6)],\n        [(\'Rotate\', 0.8, 8), (\'Color\', 0.4, 0)],\n        [(\'Rotate\', 0.4, 9), (\'Equalize\', 0.6, 2)],\n        [(\'Equalize\', 0.0, 7), (\'Equalize\', 0.8, 8)],\n        [(\'Invert\', 0.6, 4), (\'Equalize\', 1.0, 8)],\n        [(\'Color\', 0.6, 4), (\'Contrast\', 1.0, 8)],\n        [(\'Rotate\', 0.8, 8), (\'Color\', 1.0, 0)],\n        [(\'Color\', 0.8, 8), (\'Solarize\', 0.8, 7)],\n        [(\'Sharpness\', 0.4, 7), (\'Invert\', 0.6, 8)],\n        [(\'ShearX\', 0.6, 5), (\'Equalize\', 1.0, 9)],\n        [(\'Color\', 0.4, 0), (\'Equalize\', 0.6, 3)],\n        [(\'Equalize\', 0.4, 7), (\'Solarize\', 0.2, 4)],\n        [(\'Solarize\', 0.6, 5), (\'AutoContrast\', 0.6, 5)],\n        [(\'Invert\', 0.6, 4), (\'Equalize\', 1.0, 8)],\n        [(\'Color\', 0.6, 4), (\'Contrast\', 1.0, 8)],\n        [(\'Equalize\', 0.8, 8), (\'Equalize\', 0.6, 3)],\n    ]\n\n\ndef get_augment(name):\n    return augment_dict[name]\n\n\ndef apply_augment(img, name, level):\n    augment_fn, low, high = get_augment(name)\n    return augment_fn(img.copy(), level * (high - low) + low)\n\nclass Augmentation(object):\n    def __init__(self, policies):\n        """"""\n        plicies : list of (name, pr, level)\n        """"""\n        self.policies = policies\n        self.topil = ToPIL()\n\n    def __call__(self, img):\n        img = self.topil(img)\n        for _ in range(1):\n            policy = random.choice(self.policies)\n            for name, pr, level in policy:\n                if random.random() > pr:\n                    continue\n                img = apply_augment(img, name, level)\n        return img\n\nclass AugmentationBlock(Block):\n    r""""""\n    AutoAugment Block\n\n    Example\n    -------\n    >>> from autogluon.utils.augment import AugmentationBlock, autoaug_imagenet_policies\n    >>> aa_transform = AugmentationBlock(autoaug_imagenet_policies())\n    """"""\n    def __init__(self, policies):\n        """"""\n        plicies : list of (name, pr, level)\n        """"""\n        super().__init__()\n        self.policies = policies\n        self.topil = ToPIL()\n        self.tond = ToNDArray()\n\n    def forward(self, img):\n        img = self.topil(img)\n        for _ in range(1):\n            policy = random.choice(self.policies)\n            for name, pr, level in policy:\n                if random.random() > pr:\n                    continue\n                img = apply_augment(img, name, level)\n        img = self.tond(img)\n        return img\n\nclass RandAugment(object):\n    def __init__(self, n, m):\n        self.n = n\n        self.m = m\n        self.augment_list = rand_augment_list()\n        self.topil = ToPIL()\n\n    def __call__(self, img):\n        img = self.topil(img)\n        ops = random.choices(self.augment_list, k=self.n)\n        for op, minval, maxval in ops:\n            if random.random() > random.uniform(0.2, 0.8):\n                continue\n            val = (float(self.m) / 30) * float(maxval - minval) + minval\n            img = op(img, val)\n        return img\n\n'"
autogluon/utils/custom_process.py,0,"b'# Adapted from: https://stackoverflow.com/a/33599967/8199034\nimport multiprocessing as mp\nimport traceback\n\n\nclass CustomProcess(mp.Process):\n    """"""Custom Process implementation which checks has Exception property.""""""\n\n    def __init__(self, *args, **kwargs):\n        mp.Process.__init__(self, *args, **kwargs)\n        self._pconn, self._cconn = mp.Pipe()\n        self._exception = None\n\n    def run(self):\n        try:\n            mp.Process.run(self)\n            self._cconn.send(None)\n        except Exception as e:\n            tb = traceback.format_exc()\n            self._cconn.send((e, tb))\n            # raise e  # You can still rise this exception if you need to\n\n    @property\n    def exception(self):\n        if self._pconn.poll():\n            self._exception = self._pconn.recv()\n        return self._exception\n'"
autogluon/utils/custom_queue.py,0,"b'# Adapted from: http://eli.thegreenplace.net/2012/01/04/shared-counter-with-pythons-multiprocessing/\nimport multiprocessing\nimport multiprocessing.queues\n\nclass SharedCounter(object):\n    """""" A synchronized shared counter.\n    """"""\n\n    def __init__(self, n = 0):\n        self.count = multiprocessing.Value(\'i\', n)\n\n    def increment(self, n = 1):\n        """""" Increment the counter by n (default = 1) """"""\n        with self.count.get_lock():\n            self.count.value += n\n\n    @property\n    def value(self):\n        """""" Return the value of the counter """"""\n        return self.count.value\n\n\nclass Queue(multiprocessing.queues.Queue):\n    """""" A portable implementation of multiprocessing.Queue.\n\n    Because of multithreading / multiprocessing semantics, Queue.qsize() may\n    raise the NotImplementedError exception on Unix platforms like Mac OS X\n    where sem_getvalue() is not implemented. This subclass addresses this\n    problem by using a synchronized shared counter (initialized to zero) and\n    increasing / decreasing its value every time the put() and get() methods\n    are called, respectively. This not only prevents NotImplementedError from\n    being raised, but also allows us to implement a reliable version of both\n    qsize() and empty().\n    """"""\n    def __init__(self, *args, **kwargs):\n        super(Queue, self).__init__(*args, ctx=multiprocessing.get_context(), **kwargs)\n        self.size = SharedCounter(0)\n\n    def put(self, *args, **kwargs):\n        self.size.increment(1)\n        super(Queue, self).put(*args, **kwargs)\n\n    def get(self, *args, **kwargs):\n        self.size.increment(-1)\n        return super(Queue, self).get(*args, **kwargs)\n\n    def qsize(self):\n        """""" Reliable implementation of multiprocessing.Queue.qsize() """"""\n        return self.size.value\n\n    def empty(self):\n        """""" Reliable implementation of multiprocessing.Queue.empty() """"""\n        return not self.qsize()\n'"
autogluon/utils/dataloader.py,0,"b'from __future__ import absolute_import\n__all__ = [\'DataLoader\']\n\nimport pickle\nimport io\nimport sys\nimport signal\nimport multiprocessing\nimport multiprocessing.queues\nfrom multiprocessing.reduction import ForkingPickler\nfrom multiprocessing.pool import ThreadPool\nimport threading\nimport numpy as np\n\ntry:\n    import multiprocessing.resource_sharer\nexcept ImportError:\n    pass\n\nfrom mxnet.gluon.data import sampler as _sampler\nfrom mxnet import nd, context\n#from mxnet.util import is_np_shape, is_np_array, set_np\nfrom mxnet.gluon.data.dataloader import default_mp_batchify_fn, default_batchify_fn\n\n_worker_dataset = None\ndef _worker_initializer(dataset, active_shape, active_array):\n    """"""Initialier for processing pool.""""""\n    # global dataset is per-process based and only available in worker processes\n    # this is only necessary to handle MXIndexedRecordIO because otherwise dataset\n    # can be passed as argument\n    global _worker_dataset\n    _worker_dataset = dataset\n    #set_np(shape=active_shape, array=active_array)\n\ndef _worker_fn(samples, batchify_fn, dataset=None):\n    """"""Function for processing data in worker process.""""""\n    # pylint: disable=unused-argument\n    # it is required that each worker process has to fork a new MXIndexedRecordIO handle\n    # preserving dataset as global variable can save tons of overhead and is safe in new process\n    global _worker_dataset\n    batch = batchify_fn([_worker_dataset[i] for i in samples])\n    buf = io.BytesIO()\n    ForkingPickler(buf, pickle.HIGHEST_PROTOCOL).dump(batch)\n    return buf.getvalue()\n\ndef _thread_worker_initializer(active_shape, active_array):\n    """"""Initializer for ThreadPool.""""""\n    set_np(shape=active_shape, array=active_array)\n\ndef _thread_worker_fn(samples, batchify_fn, dataset):\n    """"""Threadpool worker function for processing data.""""""\n    return batchify_fn([dataset[i] for i in samples])\n\nclass _MultiWorkerIter(object):\n    """"""Internal multi-worker iterator for DataLoader.""""""\n    def __init__(self, worker_pool, batchify_fn, batch_sampler, pin_memory=False,\n                 pin_device_id=0, worker_fn=_worker_fn, prefetch=0, dataset=None,\n                 data_loader=None, timeout=120, sample_times=None):\n        self._worker_pool = worker_pool\n        self._batchify_fn = batchify_fn\n        self._batch_sampler = batch_sampler\n        self._data_buffer = {}\n        self._rcvd_idx = 0\n        self._sent_idx = 0\n        self._iter = iter(self._batch_sampler)\n        self._worker_fn = worker_fn\n        self._pin_memory = pin_memory\n        self._pin_device_id = pin_device_id\n        self._dataset = dataset\n        self._data_loader = data_loader\n        self._timeout = timeout\n        self._sample_times = sample_times\n        self._iters = 0\n        self._push_next()\n        # pre-fetch\n        if prefetch > 1:\n            for _ in range(prefetch-1):\n                self._push_next()\n\n    def reset_sample_times(self):\n        self._sample_times = None\n\n    def __len__(self):\n        return len(self._batch_sampler)\n\n    def _push_next(self):\n        """"""Assign next batch workload to workers.""""""\n        r = next(self._iter, None)\n        if r is None:\n            return\n        async_ret = self._worker_pool.apply_async(\n            self._worker_fn, (r, self._batchify_fn, self._dataset))\n        self._data_buffer[self._sent_idx] = async_ret\n        self._sent_idx += 1\n\n    def __next__(self):\n        self._iters += 1\n        if not self._sample_times or self._iters < self._sample_times:\n            self._push_next()\n        if self._rcvd_idx == self._sent_idx:\n            assert not self._data_buffer, ""Data buffer should be empty at this moment""\n            raise StopIteration\n\n        assert self._rcvd_idx < self._sent_idx, ""rcvd_idx must be smaller than sent_idx""\n        assert self._rcvd_idx in self._data_buffer, ""fatal error with _push_next, rcvd_idx missing""\n        ret = self._data_buffer.pop(self._rcvd_idx)\n        try:\n            if self._dataset is None:\n                batch = pickle.loads(ret.get(self._timeout))\n            else:\n                batch = ret.get(self._timeout)\n            if self._pin_memory:\n                batch = _as_in_context(batch, context.cpu_pinned(self._pin_device_id))\n            batch = batch[0] if len(batch) == 1 else batch\n            self._rcvd_idx += 1\n            return batch\n        except multiprocessing.context.TimeoutError:\n            msg = \'\'\'Worker timed out after {} seconds. This might be caused by \\n\n            - Slow transform. Please increase timeout to allow slower data loading in each worker.\n            \'\'\'.format(self._timeout)\n            if not isinstance(self._worker_pool, multiprocessing.pool.ThreadPool):\n                msg += \'\'\'- Insufficient shared_memory if `timeout` is large enough.\n            Please consider reduce `num_workers` or increase shared_memory in system.\n            \'\'\'\n            print(msg)\n            raise\n        except Exception:\n            self._worker_pool.terminate()\n            raise\n\n    def next(self):\n        return self.__next__()\n\n    def __iter__(self):\n        return self\n\n\nclass DataLoader(object):\n    """"""Loads data from a dataset and returns mini-batches of data.\n    Parameters\n    ----------\n    dataset : Dataset\n        Source dataset. Note that numpy and mxnet arrays can be directly used\n        as a Dataset.\n    batch_size : int\n        Size of mini-batch.\n    shuffle : bool\n        Whether to shuffle the samples.\n    sampler : Sampler\n        The sampler to use. Either specify sampler or shuffle, not both.\n    last_batch : {\'keep\', \'discard\', \'rollover\'}\n        How to handle the last batch if batch_size does not evenly divide\n        `len(dataset)`.\n        keep - A batch with less samples than previous batches is returned.\n        discard - The last batch is discarded if its incomplete.\n        rollover - The remaining samples are rolled over to the next epoch.\n    batch_sampler : Sampler\n        A sampler that returns mini-batches. Do not specify batch_size,\n        shuffle, sampler, and last_batch if batch_sampler is specified.\n    batchify_fn : callable\n        Callback function to allow users to specify how to merge samples\n        into a batch. Defaults to `default_batchify_fn`::\n            def default_batchify_fn(data):\n                if isinstance(data[0], nd.NDArray):\n                    return nd.stack(*data)\n                elif isinstance(data[0], tuple):\n                    data = zip(*data)\n                    return [default_batchify_fn(i) for i in data]\n                else:\n                    data = np.asarray(data)\n                    return nd.array(data, dtype=data.dtype)\n    num_workers : int, default 0\n        The number of multiprocessing workers to use for data preprocessing.\n    pin_memory : boolean, default False\n        If ``True``, the dataloader will copy NDArrays into pinned memory\n        before returning them. Copying from CPU pinned memory to GPU is faster\n        than from normal CPU memory.\n    pin_device_id : int, default 0\n        The device id to use for allocating pinned memory if pin_memory is ``True``\n    prefetch : int, default is `num_workers * 2`\n        The number of prefetching batches only works if `num_workers` > 0.\n        If `prefetch` > 0, it allow worker process to prefetch certain batches before\n        acquiring data from iterators.\n        Note that using large prefetching batch will provide smoother bootstrapping performance,\n        but will consume more shared_memory. Using smaller number may forfeit the purpose of using\n        multiple worker processes, try reduce `num_workers` in this case.\n        By default it defaults to `num_workers * 2`.\n    thread_pool : bool, default False\n        If ``True``, use threading pool instead of multiprocessing pool. Using threadpool\n        can avoid shared memory usage. If `DataLoader` is more IO bounded or GIL is not a killing\n        problem, threadpool version may achieve better performance than multiprocessing.\n    timeout : int, default is 120\n        The timeout in seconds for each worker to fetch a batch data. Only modify this number\n        unless you are experiencing timeout and you know it\'s due to slow data loading.\n        Sometimes full `shared_memory` will cause all workers to hang and causes timeout. In these\n        cases please reduce `num_workers` or increase system `shared_memory` size instead.\n    """"""\n    def __init__(self, dataset, batch_size=None, shuffle=False, sampler=None,\n                 last_batch=None, batch_sampler=None, batchify_fn=None,\n                 num_workers=0, pin_memory=False, pin_device_id=0,\n                 prefetch=None, thread_pool=False, timeout=120,\n                 sample_times=None):\n        self._dataset = dataset\n        self._pin_memory = pin_memory\n        self._pin_device_id = pin_device_id\n        self._thread_pool = thread_pool\n        self._timeout = timeout\n        self._sample_times = sample_times\n        assert timeout > 0, ""timeout must be positive, given {}"".format(timeout)\n\n        if batch_sampler is None:\n            if batch_size is None:\n                raise ValueError(""batch_size must be specified unless "" \\\n                                 ""batch_sampler is specified"")\n            if sampler is None:\n                if shuffle:\n                    sampler = _sampler.RandomSampler(len(dataset))\n                else:\n                    sampler = _sampler.SequentialSampler(len(dataset))\n            elif shuffle:\n                raise ValueError(""shuffle must not be specified if sampler is specified"")\n\n            batch_sampler = _sampler.BatchSampler(\n                sampler, batch_size, last_batch if last_batch else \'keep\')\n        elif batch_size is not None or shuffle or sampler is not None or \\\n                last_batch is not None:\n            raise ValueError(""batch_size, shuffle, sampler and last_batch must "" \\\n                             ""not be specified if batch_sampler is specified."")\n\n        self._batch_sampler = batch_sampler\n        self._num_workers = num_workers if num_workers >= 0 else 0\n        self._worker_pool = None\n        self._prefetch = max(0, int(prefetch) if prefetch is not None else 2 * self._num_workers)\n        if self._num_workers > 0:\n            if self._thread_pool:\n                self._worker_pool = ThreadPool(self._num_workers,\n                                               initializer=_thread_worker_initializer,\n                                               initargs=(False, False))\n            else:\n                # set ignore keyboard interupt signal before forking processes\n                original_sigint_handler = signal.signal(signal.SIGINT, signal.SIG_IGN)\n                self._worker_pool = multiprocessing.Pool(\n                    self._num_workers, initializer=_worker_initializer,\n                    initargs=[self._dataset, False, False])\n                # resume keyboard interupt signal in main process\n                signal.signal(signal.SIGINT, original_sigint_handler)\n        if batchify_fn is None:\n            if num_workers > 0:\n                self._batchify_fn = default_mp_batchify_fn\n            else:\n                self._batchify_fn = default_batchify_fn\n        else:\n            self._batchify_fn = batchify_fn\n\n    def __iter__(self):\n        if self._num_workers == 0:\n            def same_process_iter():\n                for batch in self._batch_sampler:\n                    ret = self._batchify_fn([self._dataset[idx] for idx in batch])\n                    if self._pin_memory:\n                        ret = _as_in_context(ret, context.cpu_pinned(self._pin_device_id))\n                    yield ret\n            return same_process_iter()\n\n        # multi-worker\n        return _MultiWorkerIter(self._worker_pool, self._batchify_fn, self._batch_sampler,\n                                pin_memory=self._pin_memory, pin_device_id=self._pin_device_id,\n                                worker_fn=_thread_worker_fn if self._thread_pool else _worker_fn,\n                                prefetch=self._prefetch,\n                                dataset=self._dataset if self._thread_pool else None,\n                                data_loader=self, timeout=self._timeout,\n                                sample_times=self._sample_times)\n\n    def __len__(self):\n        return len(self._batch_sampler)\n\n    def __del__(self):\n        if self._worker_pool:\n            assert isinstance(self._worker_pool, multiprocessing.pool.Pool)\n            self._worker_pool.terminate()\n'"
autogluon/utils/dataset.py,0,"b'import time\nimport numpy as np\nimport mxnet as mx\n\n__all__ = [\'SplitSampler\', \'SampledDataset\', \'get_split_samplers\']\n\nSPLIT_SEED = int(time.time())\n\ndef get_split_samplers(train_dataset, split_ratio=0.8):\n    num_samples = len(train_dataset)\n    split_idx = int(num_samples * split_ratio)\n    # numpy seed for consistency\n    indices = list(range(num_samples))\n    np.random.seed(SPLIT_SEED)\n    np.random.shuffle(indices)\n    train_sampler = SplitSampler(indices[0: split_idx])\n    val_sampler = SplitSampler(indices[split_idx:num_samples])\n    return train_sampler, val_sampler\n\nclass SplitSampler(object):\n    """"""Samples elements from [start, start+length) randomly without replacement.\n\n    Parameters\n    ----------\n    length : int\n        Length of the sequence.\n    """"""\n    def __init__(self, indices):\n        self.indices = indices\n\n    def __iter__(self):\n        indices = self.indices\n        np.random.shuffle(indices)\n        return iter(indices)\n\n    def __len__(self):\n        return len(self.indices)\n\n\nclass SampledDataset(mx.gluon.data.Dataset):\n    """"""Dataset with elements chosen by a sampler""""""\n    def __init__(self, dataset, sampler):\n        self._dataset = dataset\n        self._sampler = sampler\n        self._indices = list(iter(sampler))\n\n    def __len__(self):\n        return len(self._sampler)\n\n    def __getitem__(self, idx):\n        return self._dataset[self._indices[idx]]\n'"
autogluon/utils/defaultdict.py,0,"b'from collections import defaultdict\n\nclass keydefaultdict(defaultdict):\n    def __missing__(self, key):\n        if self.default_factory is None:\n            raise KeyError(key)\n        else:\n            ret = self[key] = self.default_factory(key)\n            return ret\n'"
autogluon/utils/deprecate.py,0,"b'import warnings\nfrom warnings import warn\n\n__all__ = [\'AutoGluonEarlyStop\', \'AutoGluonWarning\', \'make_deprecate\',\n           \'DeprecationHelper\']\n\nclass AutoGluonEarlyStop(ValueError):\n    pass\n\nclass AutoGluonWarning(DeprecationWarning):\n    pass\n\nwarnings.simplefilter(\'once\', AutoGluonWarning)\n\ndef make_deprecate(meth, old_name):\n    """"""TODO Add Docs\n    """"""\n    new_name = meth.__name__\n    def deprecated_init(*args, **kwargs):\n        warn(""autogluon.{} is now deprecated in favor of autogluon.{}.""\n             .format(old_name, new_name), AutoGluonWarning)\n        return meth(*args, **kwargs)\n\n    deprecated_init.__doc__ = r""""""\n    {old_name}(...)\n    .. warning::\n        This method is now deprecated in favor of :func:`autogluon.{new_name}`. \\\n    See :func:`~autogluon.{new_name}` for details."""""".format(\n        old_name=old_name, new_name=new_name)\n    deprecated_init.__name__ = old_name\n    return deprecated_init\n\n\nclass DeprecationHelper(object):\n    """"""TODO Add Docs\n    """"""\n    def __init__(self, new_class, new_name):\n        self.new_class = new_class\n        self.new_name = new_class.__name__\n        self.old_name = new_name\n\n    def _warn(self):\n        warn(""autogluon.{} is now deprecated in favor of autogluon.{}."" \\\n             .format(self.old_name, self.new_name), AutoGluonWarning)\n\n    def __call__(self, *args, **kwargs):\n        self._warn()\n        return self.new_class(*args, **kwargs)\n\n    def __getattr__(self, attr):\n        return getattr(self.new_class, attr)\n'"
autogluon/utils/edict.py,0,"b""# adapted from https://github.com/makinacorpus/easydict\nclass EasyDict(dict):\n    def __init__(self, d=None, **kwargs):\n        if d is None:\n            d = {}\n        if kwargs:\n            d.update(**kwargs)\n        for k, v in d.items():\n            setattr(self, k, v)\n        # Class attributes\n        for k in self.__class__.__dict__.keys():\n            if not (k.startswith('__') and k.endswith('__')) and not k in ('update', 'pop'):\n                setattr(self, k, getattr(self, k))\n\n    def __setattr__(self, name, value):\n        if isinstance(value, (list, tuple)):\n            value = [self.__class__(x)\n                     if isinstance(x, dict) else x for x in value]\n        elif isinstance(value, dict) and not isinstance(value, self.__class__):\n            value = self.__class__(value)\n        super(EasyDict, self).__setattr__(name, value)\n        super(EasyDict, self).__setitem__(name, value)\n\n    __setitem__ = __setattr__\n\n    def update(self, e=None, **f):\n        d = e or dict()\n        d.update(f)\n        for k in d:\n            setattr(self, k, d[k])\n\n    def pop(self, k, d=None):\n        delattr(self, k)\n        return super(EasyDict, self).pop(k, d)\n\n"""
autogluon/utils/file_helper.py,0,"b'import os, csv\nimport pandas as pd\nimport numpy as np\nfrom glob import glob\n\n__all__ = [\'generate_csv\', \'generate_csv_submission\', \'generate_prob_csv\']\n\ndef generate_csv_submission(dataset_path, dataset, local_path, inds, preds, class_name, custom):\n    """"""\n    Generate_csv for submission with different formats.\n    :param dataset: dataset name.\n    :param dataset_path: dataset path.\n    :param local_path: save log and plot performance_vs_trials figure.\n    :param inds: the category id.\n    :param preds: the category probability.\n    :param class_name: the category name.\n    :param custom: save the name of csv.\n    """"""\n    if dataset == \'dogs-vs-cats-redux-kernels-edition\':\n        csv_config = {\'fullname\': False,\n                      \'need_sample\': False,\n                      \'content\': \'int\',\n                      \'image_column_name\': \'id\',\n                      \'class_column_name\': \'label\',\n                      \'value\': \'probability_1\',\n                      \'special\': 0}\n    elif dataset == \'aerial-cactus-identification\':\n        csv_config = {\'fullname\': True,\n                      \'need_sample\': False,\n                      \'image_column_name\': \'id\',\n                      \'class_column_name\': \'has_cactus\',\n                      \'content\': \'empty\',\n                      \'value\': \'probability_1\',\n                      \'special\': 0}\n    elif dataset == \'plant-seedlings-classification\':\n        csv_config = {\'fullname\': True,\n                      \'need_sample\': True,\n                      \'content\': \'origin\',\n                      \'value\': \'category\',\n                      \'image_column_name\': \'file\',\n                      \'class_column_name\': \'species\',\n                      \'special\': \'\'\n                      }\n    elif dataset == \'fisheries_Monitoring\':\n        csv_config = {\'fullname\': True,\n                      \'need_sample\': True,\n                      \'content\': \'str\',\n                      \'value\': \'multi_prob\',\n                      \'image_column_name\': \'image\',\n                      \'class_column_name\': \'\',\n                      \'special\': \'rename\'\n                      }\n    elif dataset == \'dog-breed-identification\':\n        csv_config = {\'fullname\': False,\n                      \'need_sample\': True,\n                      \'content\': \'str\',\n                      \'value\': \'multi_prob\',\n                      \'image_column_name\': \'id\',\n                      \'class_column_name\': \'\',\n                      \'special\': \'\'\n                      }\n    elif dataset == \'shopee-iet-machine-learning-competition\':\n        csv_config = {\'fullname\': False,\n                      \'need_sample\': False,\n                      \'image_column_name\': \'id\',\n                      \'class_column_name\': \'category\',\n                      \'value\': \'class_id\',\n                      \'content\': \'special\',\n                      \'special\': 5}\n\n    elif dataset == \'shopee-iet\':# dogs\n        csv_config = {\'fullname\': False,\n                      \'need_sample\': False,\n                      \'content\': \'int\',\n                      \'image_column_name\': \'id\',\n                      \'class_column_name\': \'label\',\n                      \'value\': \'probability_1\',\n                      \'special\': 0}\n\n    test_path = os.path.join(dataset_path, \'test\')\n    csv_path = os.path.join(dataset_path, \'sample_submission.csv\')\n    ids = sorted(os.listdir(test_path))\n    save_csv_name = custom + \'.csv\'\n    save_csv_path = os.path.join(local_path, dataset, save_csv_name)\n    if csv_config[\'need_sample\']:  # plant\\fish\\dog\n        df = pd.read_csv(csv_path)\n        if not csv_config[\'fullname\']:\n            imagename_list = [name_id[:-4] for name_id in ids]\n        else:\n            imagename_list = ids\n        row_index_group = []\n        for i in imagename_list:\n            if csv_config[\'content\'] == \'str\':\n                row_index = df[df[csv_config[\'image_column_name\']] == str(i)].index.tolist()\n            elif csv_config[\'content\'] == \'origin\':\n                row_index = df[df[csv_config[\'image_column_name\']] == i].index.tolist()\n            elif csv_config[\'content\'] == \'int\':\n                row_index = df[df[csv_config[\'image_column_name\']] == int(i)].index.tolist()\n            elif csv_config[\'content\'] == \'special\':\n                row_index = df[df[csv_config[\'image_column_name\']] == int(i[5:])].index.tolist()\n            row_index_group.append(row_index[0])\n        if csv_config[\'value\'] == \'category\':\n            df.loc[row_index_group, csv_config[\'class_column_name\']] = class_name\n        elif csv_config[\'value\'] == \'multi_prob\':\n            df.loc[row_index_group, 1:] = preds\n        if csv_config[\'special\'] == \'rename\':\n            def get_name(name):\n                if name.startswith(\'image\'):\n                    name = \'test_stg2/\' + name\n                return name\n            df[\'image\'] = df[\'image\'].apply(get_name)\n        df.to_csv(save_csv_path, index=False)\n        print(\'generate_csv A is done\')\n    else:  # dogs/aerial/shopee\n        with open(save_csv_path, \'w\') as f:\n            row = [csv_config[\'image_column_name\'], csv_config[\'class_column_name\']]\n            writer = csv.writer(f)\n            writer.writerow(row)\n            if csv_config[\'value\'] == \'class_id\':\n                for i, ind in zip(ids, inds):\n                    row = [i, ind]\n                    writer = csv.writer(f)\n                    writer.writerow(row)\n            if csv_config[\'value\'] == \'probability_1\':\n                for i, prob in zip(ids, preds):\n                    row = [i, prob[1]]\n                    print(row)\n                    writer = csv.writer(f)\n                    writer.writerow(row)\n            if csv_config[\'value\'] == \'probability_0\':\n                for i, prob in zip(ids, preds):\n                    row = [i, prob[0]]\n                    writer = csv.writer(f)\n                    writer.writerow(row)\n        f.close()\n        if not csv_config[\'fullname\']:\n            df = pd.read_csv(save_csv_path)\n            df[\'id\'] = df[\'id\'].apply(lambda x: int(x[csv_config[\'special\']:-4]))\n            df.sort_values(""id"", inplace=True)\n            df.to_csv(save_csv_path, index=False)\n        print(\'generate_csv B is done\')\n\ndef filter_value(prob, Threshold):\n    if prob > Threshold:\n        prob = prob\n    else:\n        prob = 0\n    return prob\n\ndef generate_prob_csv(test_dataset, preds, set_prob_thresh=0, ensemble_list=\'\', custom=\'./submission.csv\', scale_min_max=True):\n    if isinstance(test_dataset.rand, list):\n        ids = sorted([x for x, _ in test_dataset.rand[0].items])\n        csv_path = test_dataset.rand[0]._root.replace(\'test\', \'sample_submission.csv\')\n    else:\n        ids = sorted([x for x, _ in test_dataset.rand.items])\n        csv_path = test_dataset.rand._root.replace(\'test\', \'sample_submission.csv\')\n    df = pd.read_csv(csv_path)\n    imagename_list = [name_id[:-4] for name_id in ids]\n    row_index_group = []\n    for i in imagename_list:\n        row_index = df[df[\'id\'] == str(i)].index.tolist()\n        if not len(row_index) == 0:\n            row_index_group.append(row_index[0])\n    df.loc[row_index_group, 1:] = preds\n    df.to_csv(custom, index=False)\n\n    def ensemble_csv(glob_files):\n        file_list = []\n        for i, glob_file in enumerate(glob(glob_files)):\n            file_list.append(pd.read_csv(glob_file, index_col=0))\n        w = sum([*file_list])/len(file_list)\n        if scale_min_max:\n            w = w.apply(lambda x: np.round((x - min(x)) / (1.0 * (max(x) - min(x))), 2), axis=1)\n        for i in w.columns.values:\n            w[i] = w[i].apply(filter_value, Threshold=set_prob_thresh)\n        w.to_csv(custom)\n    if not ensemble_list.strip() == \'\':\n        ensemble_csv(ensemble_list)\n    print(\'dog_generate_csv is done\')\n\ndef generate_csv(inds, path):\n    with open(path, \'w\') as csvFile:\n        row = [\'id\', \'category\']\n        writer = csv.writer(csvFile)\n        writer.writerow(row)\n        id = 1\n        for ind in inds:\n            row = [id, ind]\n            writer = csv.writer(csvFile)\n            writer.writerow(row)\n            id += 1\n    csvFile.close()\n'"
autogluon/utils/files.py,0,"b'import os\nfrom pathlib import Path\nimport requests\nimport errno\nimport shutil\nimport hashlib\nimport zipfile\nimport logging\nfrom .tqdm import tqdm\n\nlogger = logging.getLogger(__name__)\n\n__all__ = [\'unzip\', \'download\', \'mkdir\', \'check_sha1\', \'raise_num_file\']\n\ndef unzip(zip_file_path, root=os.path.expanduser(\'./\')):\n    """"""Unzips files located at `zip_file_path` into parent directory specified by `root`.\n    """"""\n    folders = []\n    with zipfile.ZipFile(zip_file_path) as zf:\n        zf.extractall(root)\n        for name in zf.namelist():\n            folder = Path(name).parts[0]\n            if folder not in folders:\n                folders.append(folder)\n    folders = folders[0] if len(folders) == 1 else tuple(folders)\n    return folders\n\ndef download(url, path=None, overwrite=False, sha1_hash=None):\n    """"""Download files from a given URL.\n\n    Parameters\n    ----------\n    url : str\n        URL where file is located\n    path : str, optional\n        Destination path to store downloaded file. By default stores to the\n        current directory with same name as in url.\n    overwrite : bool, optional\n        Whether to overwrite destination file if one already exists at this location.\n    sha1_hash : str, optional\n        Expected sha1 hash in hexadecimal digits (will ignore existing file when hash is specified\n        but doesn\'t match).\n\n    Returns\n    -------\n    str\n        The file path of the downloaded file.\n    """"""\n    if path is None:\n        fname = url.split(\'/\')[-1]\n    else:\n        path = os.path.expanduser(path)\n        if os.path.isdir(path):\n            fname = os.path.join(path, url.split(\'/\')[-1])\n        else:\n            fname = path\n\n    if overwrite or not os.path.exists(fname) or (sha1_hash and not check_sha1(fname, sha1_hash)):\n        dirname = os.path.dirname(os.path.abspath(os.path.expanduser(fname)))\n        if not os.path.exists(dirname):\n            os.makedirs(dirname)\n\n        logger.info(\'Downloading %s from %s...\'%(fname, url))\n        r = requests.get(url, stream=True)\n        if r.status_code != 200:\n            raise RuntimeError(""Failed downloading url %s""%url)\n        total_length = r.headers.get(\'content-length\')\n        with open(fname, \'wb\') as f:\n            if total_length is None: # no content length header\n                for chunk in r.iter_content(chunk_size=1024):\n                    if chunk: # filter out keep-alive new chunks\n                        f.write(chunk)\n            else:\n                total_length = int(total_length)\n                for chunk in tqdm(r.iter_content(chunk_size=1024),\n                                  total=int(total_length / 1024. + 0.5),\n                                  unit=\'KB\', unit_scale=False, dynamic_ncols=True):\n                    f.write(chunk)\n\n        if sha1_hash and not check_sha1(fname, sha1_hash):\n            raise UserWarning(\'File {} is downloaded but the content hash does not match. \' \\\n                              \'The repo may be outdated or download may be incomplete. \' \\\n                              \'If the ""repo_url"" is overridden, consider switching to \' \\\n                              \'the default repo.\'.format(fname))\n\n    return fname\n\n\ndef check_sha1(filename, sha1_hash):\n    """"""Check whether the sha1 hash of the file content matches the expected hash.\n\n    Parameters\n    ----------\n    filename : str\n        Path to the file.\n    sha1_hash : str\n        Expected sha1 hash in hexadecimal digits.\n\n    Returns\n    -------\n    bool\n        Whether the file content matches the expected hash.\n    """"""\n    sha1 = hashlib.sha1()\n    with open(filename, \'rb\') as f:\n        while True:\n            data = f.read(1048576)\n            if not data:\n                break\n            sha1.update(data)\n\n    return sha1.hexdigest() == sha1_hash\n\n\ndef mkdir(path):\n    """"""Make directory at the specified local path with special error handling.\n    """"""\n    try:\n        os.makedirs(path)\n    except OSError as exc:  # Python >2.5\n        if exc.errno == errno.EEXIST and os.path.isdir(path):\n            pass\n        else:\n            raise\n\ndef raise_num_file(nofile_atleast=4096):\n    try:\n        import resource as res\n    except ImportError: #Windows\n        res = None\n    if res is None:\n        return (None,)*2\n    # what is current ulimit -n setting?\n    soft,ohard = res.getrlimit(res.RLIMIT_NOFILE)\n    hard = ohard\n    # increase limit (soft and even hard) if needed\n    if soft < nofile_atleast:\n        soft = nofile_atleast\n\n        if hard<soft:\n            hard = soft\n\n        #logger.warning(\'setting soft & hard ulimit -n {} {}\'.format(soft,hard))\n        try:\n            res.setrlimit(res.RLIMIT_NOFILE,(soft,hard))\n        except (ValueError,res.error):\n            try:\n               hard = soft\n               logger.warning(\'trouble with max limit, retrying with soft,hard {},{}\'.format(soft,hard))\n               res.setrlimit(res.RLIMIT_NOFILE,(soft,hard))\n            except Exception:\n               logger.warning(\'failed to set ulimit\')\n               soft,hard = res.getrlimit(res.RLIMIT_NOFILE)\n\n    return soft,hard\n\nraise_num_file()\n'"
autogluon/utils/learning_rate.py,0,"b""from gluoncv.utils import LRSequential, LRScheduler\n\nclass LR_params:\n    def __init__(self, *args):\n        lr, lr_mode, num_epochs, num_batches, lr_decay_epoch, \\\n        lr_decay, lr_decay_period, warmup_epochs, warmup_lr= args\n        self._num_batches = num_batches\n        self._lr_decay = lr_decay\n        self._lr_decay_period = lr_decay_period\n        self._warmup_epochs = warmup_epochs\n        self._warmup_lr= warmup_lr\n        self._num_epochs = num_epochs\n        self._lr = lr\n        self._lr_mode = lr_mode\n        if lr_decay_period > 0:\n            lr_decay_epoch = list(range(lr_decay_period, num_epochs, lr_decay_period))\n        else:\n            lr_decay_epoch = [int(i) for i in lr_decay_epoch.split(',')]\n        self._lr_decay_epoch = [e - warmup_epochs for e in lr_decay_epoch]\n\n        self._lr_scheduler = LRSequential([\n            LRScheduler('linear', base_lr=self._warmup_lr, target_lr=lr,\n                        nepochs=warmup_epochs, iters_per_epoch=self._num_batches),\n            LRScheduler(lr_mode, base_lr=lr, target_lr=0,\n                        nepochs=num_epochs - warmup_epochs,\n                        iters_per_epoch=self._num_batches,\n                        step_epoch=self._lr_decay_epoch,\n                        step_factor=lr_decay, power=2)])\n\n    @property\n    def get_lr_decay_epoch(self):\n        return self._lr_decay_epoch\n\n    @property\n    def get_lr_scheduler(self):\n        return self._lr_scheduler"""
autogluon/utils/miscs.py,0,"b'import os\nimport warnings\n\n__all__ = [\'in_ipynb\', \'warning_filter\', \'verbosity2loglevel\']\n\ndef in_ipynb():\n    if \'AG_DOCS\' in os.environ and os.environ[\'AG_DOCS\']:\n        return False\n    try:\n        cfg = get_ipython().config \n        if \'IPKernelApp\' in cfg:\n            return True\n        else:\n            return False\n    except NameError:\n        return False\n\nclass warning_filter(object):\n    def __enter__(self):\n        warnings.filterwarnings(""ignore"", category=UserWarning)\n        warnings.filterwarnings(""ignore"", category=FutureWarning)\n        warnings.filterwarnings(""ignore"", category=DeprecationWarning)\n        return self\n\n    def __exit__(self, *args):\n        warnings.filterwarnings(""default"", category=UserWarning)\n        warnings.filterwarnings(""default"", category=FutureWarning)\n        warnings.filterwarnings(""default"", category=DeprecationWarning)\n\ndef verbosity2loglevel(verbosity):\n    """""" Translates verbosity to logging level. Suppresses warnings if verbosity = 0. """"""\n    if verbosity <= 0: # only errors\n        warnings.filterwarnings(""ignore"")\n        # print(""Caution: all warnings suppressed"")\n        log_level = 40\n    elif verbosity == 1: # only warnings and critical print statements\n        log_level = 25\n    elif verbosity == 2: # key print statements which should be shown by default\n        log_level = 20 \n    elif verbosity == 3: # more-detailed printing\n        log_level = 15\n    else:\n        log_level = 10 # print everything (ie. debug mode)\n    \n    return log_level'"
autogluon/utils/mxutils.py,0,"b'""""""MXNet Utils Functions""""""\n\nimport os\nimport math\nimport mxnet as mx\n\nmx_ver = mx.__version__[:3]\n\n__all__ = [\'update_params\', \'collect_params\', \'get_data_rec\', \'read_remote_ips\']\n\n\ndef update_params(net, params, multi_precision=False, ctx=mx.cpu(0)):\n    param_dict = net._collect_params_with_prefix()\n    kwargs = {\'ctx\': None} if mx_ver == \'1.4\' else {\'cast_dtype\': multi_precision, \'ctx\': None}\n    for k, v in param_dict.items():\n        param_dict[k]._load_init(params[k], **kwargs)\n\ndef collect_params(net):\n    params = net._collect_params_with_prefix()\n    param_dict = {key : val._reduce() for key, val in params.items()}\n    return param_dict\n\ndef get_data_rec(input_size, crop_ratio,\n                 rec_file, rec_file_idx,\n                 batch_size, num_workers, train=True,\n                 shuffle=True,\n                 jitter_param=0.4, max_rotate_angle=0):\n    import mxnet as mx\n    from mxnet import gluon\n    rec_file = os.path.expanduser(rec_file)\n    rec_file_idx = os.path.expanduser(rec_file_idx)\n    lighting_param = 0.1\n    input_size = input_size\n    crop_ratio = crop_ratio if crop_ratio > 0 else 0.875\n    resize = int(math.ceil(input_size / crop_ratio))\n    mean_rgb = [123.68, 116.779, 103.939]\n    std_rgb = [58.393, 57.12, 57.375]\n\n    if train:\n        data_loader = mx.io.ImageRecordIter(\n            path_imgrec         = rec_file,\n            path_imgidx         = rec_file_idx,\n            preprocess_threads  = num_workers,\n            shuffle             = True,\n            batch_size          = batch_size,\n\n            data_shape          = (3, input_size, input_size),\n            mean_r              = mean_rgb[0],\n            mean_g              = mean_rgb[1],\n            mean_b              = mean_rgb[2],\n            std_r               = std_rgb[0],\n            std_g               = std_rgb[1],\n            std_b               = std_rgb[2],\n            rand_mirror         = True,\n            random_resized_crop = True,\n            max_aspect_ratio    = 4. / 3.,\n            min_aspect_ratio    = 3. / 4.,\n            max_random_area     = 1,\n            min_random_area     = 0.08,\n            max_rotate_angle    = max_rotate_angle,\n            brightness          = jitter_param,\n            saturation          = jitter_param,\n            contrast            = jitter_param,\n            pca_noise           = lighting_param,\n        )\n    else:\n        data_loader = mx.io.ImageRecordIter(\n            path_imgrec         = rec_file,\n            path_imgidx         = rec_file_idx,\n            preprocess_threads  = num_workers,\n            shuffle             = False,\n            batch_size          = batch_size,\n\n            resize              = resize,\n            data_shape          = (3, input_size, input_size),\n            mean_r              = mean_rgb[0],\n            mean_g              = mean_rgb[1],\n            mean_b              = mean_rgb[2],\n            std_r               = std_rgb[0],\n            std_g               = std_rgb[1],\n            std_b               = std_rgb[2],\n        )\n    return data_loader\n\ndef read_remote_ips(filename):\n    ip_addrs = []\n    if filename is None:\n        return ip_addrs\n    with open(""remote_ips.txt"", ""r"") as myfile:\n        line = myfile.readline()\n        while line != \'\':\n            ip_addrs.append(line.rstrip())\n            line = myfile.readline()\n    return ip_addrs\n\n'"
autogluon/utils/pil_transforms.py,0,"b'import math\nimport random\nimport numbers\nimport warnings\nimport numpy as np\nimport mxnet as mx\nfrom PIL import Image, ImageEnhance\nimport collections\n\n__all__ = [\'Compose\', \'RandomResizedCrop\', \'RandomHorizontalFlip\', \'ColorJitter\',\n           \'Resize\', \'CenterCrop\', \'ToTensor\', \'RandomCrop\', \'ToNDArray\', \'ToPIL\']\n\nclass Compose(object):\n    """"""Composes several transforms together.\n    Args:\n        transforms (list of ``Transform`` objects): list of transforms to compose.\n    Example:\n        >>> transforms.Compose([\n        >>>     transforms.CenterCrop(10),\n        >>>     transforms.ToTensor(),\n        >>> ])\n    """"""\n\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, img):\n        for t in self.transforms:\n            img = t(img)\n        return img\n\n    def __repr__(self):\n        format_string = self.__class__.__name__ + \'(\'\n        for t in self.transforms:\n            format_string += \'\\n\'\n            format_string += \'    {0}\'.format(t)\n        format_string += \'\\n)\'\n        return format_string\n\nclass ToPIL(object):\n    """"""Convert image from ndarray format to PIL\n    """"""\n    def __call__(self, img):\n        x = Image.fromarray(img.asnumpy())\n        return x\n\nclass ToNDArray(object):\n    def __call__(self, img):\n        x = mx.nd.array(np.array(img), mx.cpu(0))\n        return x\n\nclass ToTensor(object):\n    def __call__(self, img):\n        x = mx.nd.array(np.array(img), mx.cpu(0))\n        return x.swapaxes(0, 2).swapaxes(1, 2)\n\nclass RandomResizedCrop(object):\n    """"""Crop the given PIL Image to random size and aspect ratio.\n    A crop of random size (default: of 0.08 to 1.0) of the original size and a random\n    aspect ratio (default: of 3/4 to 4/3) of the original aspect ratio is made. This crop\n    is finally resized to given size.\n    This is popularly used to train the Inception networks.\n    Args:\n        size: expected output size of each edge\n        scale: range of size of the origin size cropped\n        ratio: range of aspect ratio of the origin aspect ratio cropped\n        interpolation: Default: PIL.Image.BILINEAR\n    """"""\n\n    def __init__(self, size, scale=(0.08, 1.0), ratio=(3. / 4., 4. / 3.), interpolation=Image.BILINEAR):\n        if isinstance(size, tuple):\n            self.size = size\n        else:\n            self.size = (size, size)\n        if (scale[0] > scale[1]) or (ratio[0] > ratio[1]):\n            warnings.warn(""range should be of kind (min, max)"")\n\n        self.interpolation = interpolation\n        self.scale = scale\n        self.ratio = ratio\n\n    @staticmethod\n    def get_params(img, scale, ratio):\n        width, height = img.size\n        area = height * width\n\n        for attempt in range(10):\n            target_area = random.uniform(*scale) * area\n            log_ratio = (math.log(ratio[0]), math.log(ratio[1]))\n            aspect_ratio = math.exp(random.uniform(*log_ratio))\n\n            w = int(round(math.sqrt(target_area * aspect_ratio)))\n            h = int(round(math.sqrt(target_area / aspect_ratio)))\n\n            if 0 < w <= width and 0 < h <= height:\n                i = random.randint(0, height - h)\n                j = random.randint(0, width - w)\n                return i, j, h, w\n\n        # Fallback to central crop\n        in_ratio = float(width) / float(height)\n        if (in_ratio < min(ratio)):\n            w = width\n            h = int(round(w / min(ratio)))\n        elif (in_ratio > max(ratio)):\n            h = height\n            w = int(round(h * max(ratio)))\n        else:  # whole image\n            w = width\n            h = height\n        i = (height - h) // 2\n        j = (width - w) // 2\n        return i, j, h, w\n\n    def __call__(self, img):\n        """"""\n        Args:\n            img (PIL Image): Image to be cropped and resized.\n        Returns:\n            PIL Image: Randomly cropped and resized image.\n        """"""\n        i, j, h, w = self.get_params(img, self.scale, self.ratio)\n        return self.resized_crop(img, i, j, h, w, self.size, self.interpolation)\n\n    def __repr__(self):\n        interpolate_str = _pil_interpolation_to_str[self.interpolation]\n        format_string = self.__class__.__name__ + \'(size={0}\'.format(self.size)\n        format_string += \', scale={0}\'.format(tuple(round(s, 4) for s in self.scale))\n        format_string += \', ratio={0}\'.format(tuple(round(r, 4) for r in self.ratio))\n        format_string += \', interpolation={0})\'.format(interpolate_str)\n        return format_string\n\n    @staticmethod\n    def resized_crop(img, top, left, height, width, size, interpolation=Image.BILINEAR):\n        img = crop(img, top, left, height, width)\n        img = resize(img, size, interpolation)\n        return img\n    \nclass RandomCrop(object):\n    """"""Crop the given PIL Image at a random location.\n\n    Parameters\n    ----------\n        size (sequence or int): Desired output size of the crop. If size is an\n            int instead of sequence like (h, w), a square crop (size, size) is\n            made.\n        padding (int or sequence, optional): Optional padding on each border\n            of the image. Default is None, i.e no padding. If a sequence of length\n            4 is provided, it is used to pad left, top, right, bottom borders\n            respectively. If a sequence of length 2 is provided, it is used to\n            pad left/right, top/bottom borders, respectively.\n        pad_if_needed (boolean): It will pad the image if smaller than the\n            desired size to avoid raising an exception. Since cropping is done\n            after padding, the padding seems to be done at a random offset.\n        fill: Pixel fill value for constant fill. Default is 0. If a tuple of\n            length 3, it is used to fill R, G, B channels respectively.\n            This value is only used when the padding_mode is constant\n        padding_mode: Type of padding. Should be: constant, edge, reflect or symmetric. Default is constant.\n\n             - constant: pads with a constant value, this value is specified with fill\n\n             - edge: pads with the last value on the edge of the image\n\n             - reflect: pads with reflection of image (without repeating the last value on the edge)\n\n                padding [1, 2, 3, 4] with 2 elements on both sides in reflect mode\n                will result in [3, 2, 1, 2, 3, 4, 3, 2]\n\n             - symmetric: pads with reflection of image (repeating the last value on the edge)\n\n                padding [1, 2, 3, 4] with 2 elements on both sides in symmetric mode\n                will result in [2, 1, 1, 2, 3, 4, 4, 3]\n\n    """"""\n    def __init__(self, size, padding=None, pad_if_needed=False, fill=0, padding_mode=\'constant\'):\n        if isinstance(size, numbers.Number):\n            self.size = (int(size), int(size))\n        else:\n            self.size = size\n        self.padding = padding\n        self.pad_if_needed = pad_if_needed\n        self.fill = fill\n        self.padding_mode = padding_mode\n\n    @staticmethod\n    def get_params(img, output_size):\n        """"""Get parameters for ``crop`` for a random crop.\n\n        Parameters\n        ----------\n            img (PIL Image): Image to be cropped.\n            output_size (tuple): Expected output size of the crop.\n\n        Returns\n        -------\n            tuple: params (i, j, h, w) to be passed to ``crop`` for random crop.\n        """"""\n        w, h = _get_image_size(img)\n        th, tw = output_size\n        if w == tw and h == th:\n            return 0, 0, h, w\n\n        i = random.randint(0, h - th)\n        j = random.randint(0, w - tw)\n        return i, j, th, tw\n\n    def __call__(self, img):\n        """"""\n        Args:\n            img (PIL Image): Image to be cropped.\n\n        Returns:\n            PIL Image: Cropped image.\n        """"""\n        if self.padding is not None:\n            img = pad(img, self.padding, self.fill, self.padding_mode)\n\n        # pad the width if needed\n        if self.pad_if_needed and img.size[0] < self.size[1]:\n            img = pad(img, (self.size[1] - img.size[0], 0), self.fill, self.padding_mode)\n        # pad the height if needed\n        if self.pad_if_needed and img.size[1] < self.size[0]:\n            img = pad(img, (0, self.size[0] - img.size[1]), self.fill, self.padding_mode)\n\n        i, j, h, w = self.get_params(img, self.size)\n\n        return crop(img, i, j, h, w)\n\n    def __repr__(self):\n        return self.__class__.__name__ + \'(size={0}, padding={1})\'.format(self.size, self.padding)\n\n\nclass RandomHorizontalFlip(object):\n    """"""Horizontally flip the given PIL Image randomly with a given probability.\n    Args:\n        p (float): probability of the image being flipped. Default value is 0.5\n    """"""\n\n    def __init__(self, p=0.5):\n        self.p = p\n\n    def __call__(self, img):\n        """"""\n        Args:\n            img (PIL Image): Image to be flipped.\n        Returns:\n            PIL Image: Randomly flipped image.\n        """"""\n        if random.random() < self.p:\n            return img.transpose(Image.FLIP_LEFT_RIGHT)\n        return img\n\n    def __repr__(self):\n        return self.__class__.__name__ + \'(p={})\'.format(self.p)\n\nclass Lambda(object):\n    """"""Apply a user-defined lambda as a transform.\n    Args:\n        lambd (function): Lambda/function to be used for transform.\n    """"""\n\n    def __init__(self, lambd):\n        assert callable(lambd), repr(type(lambd).__name__) + "" object is not callable""\n        self.lambd = lambd\n\n    def __call__(self, img):\n        return self.lambd(img)\n\n    def __repr__(self):\n        return self.__class__.__name__ + \'()\'\n\nclass ColorJitter(object):\n    """"""Randomly change the brightness, contrast and saturation of an image.\n    Args:\n        brightness (float or tuple of float (min, max)): How much to jitter brightness.\n            brightness_factor is chosen uniformly from [max(0, 1 - brightness), 1 + brightness]\n            or the given [min, max]. Should be non negative numbers.\n        contrast (float or tuple of float (min, max)): How much to jitter contrast.\n            contrast_factor is chosen uniformly from [max(0, 1 - contrast), 1 + contrast]\n            or the given [min, max]. Should be non negative numbers.\n        saturation (float or tuple of float (min, max)): How much to jitter saturation.\n            saturation_factor is chosen uniformly from [max(0, 1 - saturation), 1 + saturation]\n            or the given [min, max]. Should be non negative numbers.\n        hue (float or tuple of float (min, max)): How much to jitter hue.\n            hue_factor is chosen uniformly from [-hue, hue] or the given [min, max].\n            Should have 0<= hue <= 0.5 or -0.5 <= min <= max <= 0.5.\n    """"""\n    def __init__(self, brightness=0, contrast=0, saturation=0, hue=0):\n        self.brightness = self._check_input(brightness, \'brightness\')\n        self.contrast = self._check_input(contrast, \'contrast\')\n        self.saturation = self._check_input(saturation, \'saturation\')\n        self.hue = self._check_input(hue, \'hue\', center=0, bound=(-0.5, 0.5),\n                                     clip_first_on_zero=False)\n\n    def _check_input(self, value, name, center=1, bound=(0, float(\'inf\')), clip_first_on_zero=True):\n        if isinstance(value, numbers.Number):\n            if value < 0:\n                raise ValueError(""If {} is a single number, it must be non negative."".format(name))\n            value = [center - value, center + value]\n            if clip_first_on_zero:\n                value[0] = max(value[0], 0)\n        elif isinstance(value, (tuple, list)) and len(value) == 2:\n            if not bound[0] <= value[0] <= value[1] <= bound[1]:\n                raise ValueError(""{} values should be between {}"".format(name, bound))\n        else:\n            raise TypeError(""{} should be a single number or a list/tuple with lenght 2."".format(name))\n\n        # if value is 0 or (1., 1.) for brightness/contrast/saturation\n        # or (0., 0.) for hue, do nothing\n        if value[0] == value[1] == center:\n            value = None\n        return value\n\n    @staticmethod\n    def get_params(brightness, contrast, saturation, hue):\n        """"""Get a randomized transform to be applied on image.\n        Arguments are same as that of __init__.\n        Returns:\n            Transform which randomly adjusts brightness, contrast and\n            saturation in a random order.\n        """"""\n        transforms = []\n\n        if brightness is not None:\n            brightness_factor = random.uniform(brightness[0], brightness[1])\n            transforms.append(Lambda(lambda img: adjust_brightness(img, brightness_factor)))\n\n        if contrast is not None:\n            contrast_factor = random.uniform(contrast[0], contrast[1])\n            transforms.append(Lambda(lambda img: adjust_contrast(img, contrast_factor)))\n\n        if saturation is not None:\n            saturation_factor = random.uniform(saturation[0], saturation[1])\n            transforms.append(Lambda(lambda img: adjust_saturation(img, saturation_factor)))\n\n        if hue is not None:\n            hue_factor = random.uniform(hue[0], hue[1])\n            transforms.append(Lambda(lambda img: adjust_hue(img, hue_factor)))\n\n        random.shuffle(transforms)\n        transform = Compose(transforms)\n\n        return transform\n\n    def __call__(self, img):\n        """"""\n        Args:\n            img (PIL Image): Input image.\n        Returns:\n            PIL Image: Color jittered image.\n        """"""\n        transform = self.get_params(self.brightness, self.contrast,\n                                    self.saturation, self.hue)\n        return transform(img)\n\n    def __repr__(self):\n        format_string = self.__class__.__name__ + \'(\'\n        format_string += \'brightness={0}\'.format(self.brightness)\n        format_string += \', contrast={0}\'.format(self.contrast)\n        format_string += \', saturation={0}\'.format(self.saturation)\n        format_string += \', hue={0})\'.format(self.hue)\n        return format_string\n\nclass Resize(object):\n    """"""Resize the input PIL Image to the given size.\n    Args:\n        size (sequence or int): Desired output size. If size is a sequence like\n            (h, w), output size will be matched to this. If size is an int,\n            smaller edge of the image will be matched to this number.\n            i.e, if height > width, then image will be rescaled to\n            (size * height / width, size)\n        interpolation (int, optional): Desired interpolation. Default is\n            ``PIL.Image.BILINEAR``\n    """"""\n\n    def __init__(self, size, interpolation=Image.BILINEAR):\n        assert isinstance(size, int) or (isinstance(size, collections.Iterable) and len(size) == 2)\n        self.size = size\n        self.interpolation = interpolation\n\n    def __call__(self, img):\n        """"""\n        Args:\n            img (PIL Image): Image to be scaled.\n        Returns:\n            PIL Image: Rescaled image.\n        """"""\n        return resize(img, self.size, self.interpolation)\n\n    def __repr__(self):\n        interpolate_str = _pil_interpolation_to_str[self.interpolation]\n        return self.__class__.__name__ + \'(size={0}, interpolation={1})\'.format(self.size, interpolate_str)\n\n\nclass CenterCrop(object):\n    """"""Crops the given PIL Image at the center.\n    Args:\n        size (sequence or int): Desired output size of the crop. If size is an\n            int instead of sequence like (h, w), a square crop (size, size) is\n            made.\n    """"""\n\n    def __init__(self, size):\n        if isinstance(size, numbers.Number):\n            self.size = (int(size), int(size))\n        else:\n            self.size = size\n\n    def __call__(self, img):\n        """"""\n        Args:\n            img (PIL Image): Image to be cropped.\n        Returns:\n            PIL Image: Cropped image.\n        """"""\n        return self.center_crop(img, self.size)\n\n    def __repr__(self):\n        return self.__class__.__name__ + \'(size={0})\'.format(self.size)\n\n    @staticmethod\n    def center_crop(img, output_size):\n        if isinstance(output_size, numbers.Number):\n            output_size = (int(output_size), int(output_size))\n        image_width, image_height = img.size\n        crop_height, crop_width = output_size\n        crop_top = int(round((image_height - crop_height) / 2.))\n        crop_left = int(round((image_width - crop_width) / 2.))\n        return crop(img, crop_top, crop_left, crop_height, crop_width)\n\ndef crop(img, top, left, height, width):\n    return img.crop((left, top, left + width, top + height))\n\ndef resize(img, size, interpolation=Image.BILINEAR):\n    if not (isinstance(size, int) or (isinstance(size, collections.Iterable) and len(size) == 2)):\n        raise TypeError(\'Got inappropriate size arg: {}\'.format(size))\n\n    if isinstance(size, int):\n        w, h = img.size\n        if (w <= h and w == size) or (h <= w and h == size):\n            return img\n        if w < h:\n            ow = size\n            oh = int(size * h / w)\n            return img.resize((ow, oh), interpolation)\n        else:\n            oh = size\n            ow = int(size * w / h)\n            return img.resize((ow, oh), interpolation)\n    else:\n        return img.resize(size[::-1], interpolation)\n\ndef adjust_brightness(img, brightness_factor):\n    enhancer = ImageEnhance.Brightness(img)\n    img = enhancer.enhance(brightness_factor)\n    return img\n\ndef adjust_contrast(img, contrast_factor):\n    enhancer = ImageEnhance.Contrast(img)\n    img = enhancer.enhance(contrast_factor)\n    return img\n\ndef adjust_saturation(img, saturation_factor):\n    enhancer = ImageEnhance.Color(img)\n    img = enhancer.enhance(saturation_factor)\n    return img\n\n\ndef adjust_hue(img, hue_factor):\n    if not(-0.5 <= hue_factor <= 0.5):\n        raise ValueError(\'hue_factor is not in [-0.5, 0.5].\'.format(hue_factor))\n\n    input_mode = img.mode\n    if input_mode in {\'L\', \'1\', \'I\', \'F\'}:\n        return img\n    h, s, v = img.convert(\'HSV\').split()\n    np_h = np.array(h, dtype=np.uint8)\n    # uint8 addition take cares of rotation across boundaries\n    with np.errstate(over=\'ignore\'):\n        np_h += np.uint8(hue_factor * 255)\n    h = Image.fromarray(np_h, \'L\')\n\n    img = Image.merge(\'HSV\', (h, s, v)).convert(input_mode)\n    return img\n\ndef pad(img, padding, fill=0, padding_mode=\'constant\'):\n    if not isinstance(padding, (numbers.Number, tuple)):\n        raise TypeError(\'Got inappropriate padding arg\')\n    if not isinstance(fill, (numbers.Number, str, tuple)):\n        raise TypeError(\'Got inappropriate fill arg\')\n    if not isinstance(padding_mode, str):\n        raise TypeError(\'Got inappropriate padding_mode arg\')\n\n    if isinstance(padding, Sequence) and len(padding) not in [2, 4]:\n        raise ValueError(""Padding must be an int or a 2, or 4 element tuple, not a "" +\n                         ""{} element tuple"".format(len(padding)))\n\n    assert padding_mode in [\'constant\', \'edge\', \'reflect\', \'symmetric\'], \\\n        \'Padding mode should be either constant, edge, reflect or symmetric\'\n\n    if padding_mode == \'constant\':\n        if img.mode == \'P\':\n            palette = img.getpalette()\n            image = ImageOps.expand(img, border=padding, fill=fill)\n            image.putpalette(palette)\n            return image\n        return ImageOps.expand(img, border=padding, fill=fill)\n    else:\n        if isinstance(padding, int):\n            pad_left = pad_right = pad_top = pad_bottom = padding\n        if isinstance(padding, Sequence) and len(padding) == 2:\n            pad_left = pad_right = padding[0]\n            pad_top = pad_bottom = padding[1]\n        if isinstance(padding, Sequence) and len(padding) == 4:\n            pad_left = padding[0]\n            pad_top = padding[1]\n            pad_right = padding[2]\n            pad_bottom = padding[3]\n        if img.mode == \'P\':\n            palette = img.getpalette()\n            img = np.asarray(img)\n            img = np.pad(img, ((pad_top, pad_bottom), (pad_left, pad_right)), padding_mode)\n            img = Image.fromarray(img)\n            img.putpalette(palette)\n            return img\n\n        img = np.asarray(img)\n        # RGB image\n        if len(img.shape) == 3:\n            img = np.pad(img, ((pad_top, pad_bottom), (pad_left, pad_right), (0, 0)), padding_mode)\n        # Grayscale image\n        if len(img.shape) == 2:\n            img = np.pad(img, ((pad_top, pad_bottom), (pad_left, pad_right)), padding_mode)\n\n        return Image.fromarray(img)\n'"
autogluon/utils/plot_network.py,0,"b'import mxnet as mx\nimport numpy as np\nfrom matplotlib import pyplot\ntry:\n    import graphviz\nexcept ImportError:\n    graphviz = None\n\n__all__ = [\'plot_network\']\n\ndef plot_network(block, shape=(1, 3, 224, 224), savefile=False):\n    """"""Plot network to visualize internal structures.\n\n    Parameters\n    ----------\n    block (mxnet.gluon.HybridBlock): mxnet.gluon.HybridBlock\n        A hybridizable network to be visualized.\n    shape (tuple of int):\n        Desired input shape, default is (1, 3, 224, 224).\n    save_prefix (str or None):\n        If not `None`, will save rendered pdf to disk with prefix.\n\n    """"""\n    if graphviz is None:\n        raise RuntimeError(""Cannot import graphviz."")\n    if not isinstance(block, mx.gluon.HybridBlock):\n        raise ValueError(""block must be HybridBlock, given {}"".format(type(block)))\n    data = mx.sym.var(\'data\')\n    sym = block(data)\n    if isinstance(sym, tuple):\n        sym = mx.sym.Group(sym)\n\n    a = mx.viz.plot_network(sym, shape={\'data\':shape},\n                            node_attrs={\'shape\':\'rect\', \'fixedsize\':\'false\'})\n    if savefile:\n        a.view(savefile)\n    return a\n'"
autogluon/utils/plots.py,0,"b'"""""" Methods to create various plots used throughout AutoGluon.\n    If matplotlib or bokeh are not installed, simply will print warning message that plots cannot be shown.\n""""""\n\nimport warnings, os\nimport numpy as np\nfrom collections import OrderedDict\n\nfrom .miscs import warning_filter\n\n__all__ = [\'plot_performance_vs_trials\', \'plot_summary_of_models\', \'plot_tabular_models\', \'mousover_plot\']\n\n\ndef plot_performance_vs_trials(results, output_directory, save_file=""PerformanceVsTrials.png"", plot_title=""""):\n    try:\n        import matplotlib.pyplot as plt\n        matplotlib_imported = True\n    except ImportError:\n        matplotlib_imported = False\n    \n    if not matplotlib_imported:\n        warnings.warn(\'AutoGluon summary plots cannot be created because matplotlib is not installed. Please do: ""pip install matplotlib""\')\n        return None\n    \n    ordered_trials = sorted(list(results[\'trial_info\'].keys()))\n    ordered_val_perfs = [results[\'trial_info\'][trial_id][results[\'reward_attr\']] for trial_id in ordered_trials]\n    x = range(1, len(ordered_trials)+1)\n    y = []\n    for i in x:\n        y.append(max([ordered_val_perfs[j] for j in range(i)])) # best validation performance in trials up until ith one (assuming higher = better)\n    fig, ax = plt.subplots()\n    ax.plot(x, y)\n    ax.set(xlabel=\'Completed Trials\', ylabel=\'Best Performance\', title = plot_title)\n    if output_directory is not None:\n        outputfile = os.path.join(output_directory, save_file)\n        fig.savefig(outputfile)\n        print(""Plot of HPO performance saved to file: %s"" % outputfile)\n    plt.show()\n\ndef plot_summary_of_models(results, output_directory, save_file=\'SummaryOfModels.html\', plot_title=""Models produced during fit()""):\n    """""" Plot dynamic scatterplot summary of each model encountered during fit(), based on the returned Results object. \n    """"""\n    num_trials = len(results[\'trial_info\'])\n    attr_color = None\n    attr_size = None\n    datadict = {\'trial_id\': sorted(results[\'trial_info\'].keys())}\n    datadict[\'performance\'] = [results[\'trial_info\'][trial_id][results[\'reward_attr\']] for trial_id in datadict[\'trial_id\']]\n    datadict[\'hyperparameters\'] = [_formatDict(results[\'trial_info\'][trial_id][\'config\']) for trial_id in datadict[\'trial_id\']]\n    hidden_keys = []\n    # Determine x-axis attribute:\n    if \'latency\' in results[\'metadata\']:\n        datadict[\'latency\'] = [results[\'trial_info\'][trial_id][\'metadata\'][\'latency\'] for trial_id in datadict[\'trial_id\']]\n        attr_x = \'latency\'\n    else:\n        attr_x = list(results[\'best_config\'].keys())[0]\n        datadict[attr_x] = [results[\'trial_info\'][trial_id][\'config\'][attr_x] for trial_id in datadict[\'trial_id\']]\n        hidden_keys.append(attr_x)\n    # Determine size attribute:\n    if \'memory\' in results[\'metadata\']:\n        datadict[\'memory\'] = [results[\'trial_info\'][trial_id][\'metadata\'][\'memory\'] for trial_id in datadict[\'trial_id\']]\n        attr_size = \'memory\'\n    \n    # Determine color attribute:\n    if \'training_loss\' in results:\n        datadict[\'training_loss\'] = [results[\'trial_info\'][trial_id][\'training_loss\'] for trial_id in datadict[\'trial_id\']]\n        attr_color = \'training_loss\'\n\n    save_path = os.path.join(output_directory, save_file) if output_directory else None\n    mousover_plot(datadict, attr_x=attr_x, attr_y=\'performance\', attr_color=attr_color, \n        attr_size=attr_size, save_file=save_path, plot_title=plot_title, hidden_keys=hidden_keys)\n    if save_path is not None:\n        print(""Plot summary of models saved to file: %s"" % save_file)\n\ndef plot_tabular_models(results, output_directory=None, save_file=""SummaryOfModels.html"", plot_title=""Models produced during fit()""):\n    """""" Plot dynamic scatterplot of every single model trained during tabular_prediction.fit()\n        Args:\n            results: \n                Dict created during TabularPredictor.fit_summary().\n                Must at least contain key: \'model_performance\'.\n    """"""\n    save_path = output_directory + save_file if output_directory else None\n    hidden_keys = []\n    model_performancedict = results[\'model_performance\']\n    model_names = list(model_performancedict.keys())\n    val_perfs = [model_performancedict[key] for key in model_names]\n    model_types = [results[\'model_types\'][key] for key in model_names]\n    hidden_keys.append(model_types)\n    model_hyperparams = [_formatDict(results[\'model_hyperparams\'][key]) for key in model_names]\n    datadict = {\'performance\': val_perfs, \'model\': model_names, \'model_type\': model_types, \'hyperparameters\': model_hyperparams}\n    hpo_used = results[\'hyperparameter_tune\']\n    if not hpo_used:  # currently, times are only stored without HPO\n        leaderboard = results[\'leaderboard\'].copy()\n        leaderboard[\'fit_time\'] = leaderboard[\'fit_time\'].fillna(0)\n        leaderboard[\'pred_time_val\'] = leaderboard[\'pred_time_val\'].fillna(0)\n\n        datadict[\'inference_latency\'] = [leaderboard[\'pred_time_val\'][leaderboard[\'model\'] == m].values[0] for m in model_names]\n        datadict[\'training_time\'] = [leaderboard[\'fit_time\'][leaderboard[\'model\'] == m].values[0] for m in model_names]\n        mousover_plot(datadict, attr_x=\'inference_latency\', attr_y=\'performance\', attr_color=\'model_type\', \n                      save_file=save_path, plot_title=plot_title, hidden_keys=hidden_keys)\n    else:\n        mousover_plot(datadict, attr_x=\'model_type\', attr_y=\'performance\',\n                      save_file=save_path, plot_title=plot_title, hidden_keys=hidden_keys)\n\ndef _formatDict(d):\n    """""" Returns dict as string with HTML new-line tags <br> between key-value pairs. """"""\n    s = \'\'\n    for key in d:\n        new_s = str(key) + "": "" + str(d[key]) + ""<br>""\n        s += new_s\n    return s[:-4]\n\ndef mousover_plot(datadict, attr_x, attr_y, attr_color=None, attr_size=None, save_file=None, plot_title="""",\n                  point_transparency = 0.5, point_size=20, default_color=""#2222aa"", hidden_keys = []):\n    """""" Produces dynamic scatter plot that can be interacted with by mousing over each point to see its label\n        Args:\n            datadict (dict): keys contain attributes, values of lists of data from each attribute to plot (each list index corresponds to datapoint).\n                             The values of all extra keys in this dict are considered (string) labels to assign to datapoints when they are moused over.\n                             Apply _formatDict() to any entries in datadict which are themselves dicts.\n            attr_x (str): name of column in dataframe whose values are shown on x-axis (eg. \'latency\'). Can be categorical or numeric values\n            attr_y (str): name of column in dataframe whose values are shown on y-axis (eg. \'validation performance\'). Must be numeric values.\n            attr_size (str): name of column in dataframe whose values determine size of dots (eg. \'memory consumption\'). Must be numeric values.\n            attr_color (str): name of column in dataframe whose values determine color of dots  (eg. one of the hyperparameters). Can be categorical or numeric values\n            point_labels (list): list of strings describing the label for each dot (must be in same order as rows of dataframe)\n            save_file (str): where to save plot to (html) file (if None, plot is not saved)\n            plot_title (str): Title of plot and html file\n            point_transparency (float): alpha value of points, lower = more transparent\n            point_size (int): size of points, higher = larger\n            hidden keys (list[str]): which keys of datadict NOT to show labels for.\n    """"""\n    try:\n        with warning_filter():\n            import bokeh\n            from bokeh.plotting import output_file, ColumnDataSource, show, figure\n            from bokeh.models import HoverTool, CategoricalColorMapper, LinearColorMapper, Legend, LegendItem, ColorBar\n            from bokeh.palettes import Category20\n        bokeh_imported = True\n    except ImportError:\n        bokeh_imported = False\n    \n    if not bokeh_imported:\n        warnings.warn(\'AutoGluon summary plots cannot be created because bokeh is not installed. To see plots, please do: ""pip install bokeh==2.0.1""\')\n        return None\n    \n    n = len(datadict[attr_x])\n    for key in datadict.keys(): # Check lengths are all the same\n        if len(datadict[key]) != n:\n            raise ValueError(""Key %s in datadict has different length than %s"" % (key, attr_x))\n    \n    attr_x_is_string = any([type(val)==str for val in datadict[attr_x]])\n    if attr_x_is_string:\n        attr_x_levels = list(set(datadict[attr_x])) # use this to translate between int-indices and x-values\n        og_x_vals = datadict[attr_x][:]\n        attr_x2 = attr_x + ""___"" # this key must not already be in datadict.\n        hidden_keys.append(attr_x2)\n        datadict[attr_x2] = [attr_x_levels.index(category) for category in og_x_vals] # convert to ints\n    \n    legend = None\n    if attr_color is not None:\n        attr_color_is_string = any([type(val)==str for val in datadict[attr_color]])\n        color_datavals = datadict[attr_color]\n        if attr_color_is_string:\n            attr_color_levels = list(set(color_datavals))\n            colorpalette = Category20[20]\n            color_mapper = CategoricalColorMapper(factors=attr_color_levels, palette=[colorpalette[2*i % len(colorpalette)] for i in range(len(attr_color_levels))])\n            legend = attr_color\n        else:\n            color_mapper = LinearColorMapper(palette=\'Magma256\', low=min(datadict[attr_color]), high=max(datadict[attr_color])*1.25)\n        default_color = {\'field\': attr_color, \'transform\': color_mapper}\n    \n    if attr_size is not None: # different size for each point, ensure mean-size == point_size\n        attr_size2 = attr_size + ""____""\n        hidden_keys.append(attr_size2)\n        og_sizevals = np.array(datadict[attr_size])\n        sizevals = point_size + (og_sizevals - np.mean(og_sizevals))/np.std(og_sizevals) * (point_size/2)\n        if np.min(sizevals) < 0:\n            sizevals = -np.min(sizevals) + sizevals + 1.0\n        datadict[attr_size2] = list(sizevals)\n        point_size = attr_size2\n    \n    if save_file is not None:\n        output_file(save_file, title=plot_title)\n        print(""Plot summary of models saved to file: %s"" % save_file)\n    \n    source = ColumnDataSource(datadict)\n    TOOLS=""crosshair,pan,wheel_zoom,box_zoom,reset,hover,save""\n    p = figure(title=plot_title, tools=TOOLS)\n    if attr_x_is_string:\n        circ = p.circle(attr_x2, attr_y, line_color=default_color, line_alpha = point_transparency,\n                fill_color = default_color, fill_alpha=point_transparency, size=point_size, source=source)\n    else:\n        circ = p.circle(attr_x, attr_y, line_color=default_color, line_alpha = point_transparency,\n                fill_color = default_color, fill_alpha=point_transparency, size=point_size, source=source)\n    hover = p.select(dict(type=HoverTool))\n    hover.tooltips = OrderedDict([(key,\'@\'+key+\'{safe}\') for key in datadict.keys() if key not in hidden_keys])\n    # Format axes:\n    p.xaxis.axis_label = attr_x\n    p.yaxis.axis_label = attr_y\n    if attr_x_is_string: # add x-ticks:\n        p.xaxis.ticker = list(range(len(attr_x_levels)))\n        p.xaxis.major_label_overrides = {i: attr_x_levels[i] for i in range(len(attr_x_levels))}\n    \n    # Legend additions:\n    if attr_color is not None and attr_color_is_string:\n        legend_it = []\n        for i in range(len(attr_color_levels)):\n            legend_it.append(LegendItem(label=attr_color_levels[i], renderers = [circ], index=datadict[attr_color].index(attr_color_levels[i])))\n        legend = Legend(items=legend_it, location=(0, 0))\n        p.add_layout(legend, \'right\')\n    \n    if attr_color is not None and not attr_color_is_string: \n        color_bar = ColorBar(color_mapper=color_mapper, title = attr_color, \n                             label_standoff=12, border_line_color=None, location=(0,0))\n        p.add_layout(color_bar, \'right\')\n    \n    if attr_size is not None:\n        p.add_layout(Legend(items=[LegendItem(label=\'Size of points based on ""\'+attr_size + \'""\')]), \'below\')\n    \n    show(p)\n    \n'"
autogluon/utils/serialization.py,0,"b'""""""Serilization and checkpoint""""""\nimport logging\nimport difflib\nimport inspect\nimport os\nimport io\nimport struct\nimport sys\nimport tarfile\nimport zipfile\nimport tempfile\nimport warnings\nfrom contextlib import closing, contextmanager\n\nif sys.version_info[0] == 2:\n    import cPickle as pickle\n    _string_classes = basestring\nelse:\n    import pickle\n    import pathlib\n    _string_classes = (str, bytes)\n\n__all__ = [\'save\', \'load\']\n\nDEFAULT_PROTOCOL = 2\n\nLONG_SIZE = struct.Struct(\'=l\').size\nINT_SIZE = struct.Struct(\'=i\').size\nSHORT_SIZE = struct.Struct(\'=h\').size\n\nlogger = logging.getLogger(__name__)\nMAGIC_NUMBER = 0x7df059597099bb7dcf25\nPROTOCOL_VERSION = 1001\nSTORAGE_KEY_SEPARATOR = \',\'\n\n_package_registry = []\n\ndef save(obj, f, pickle_module=pickle, pickle_protocol=DEFAULT_PROTOCOL):\n    """"""Saves an object to local file.\n\n    Parameters\n    ----------\n    obj : object\n        Python object to save.\n    f : string or file object\n        A file-like object (has to implement write and flush), or a string containing a file name\n    pickle_module : pickle\n        Module used for pickling metadata and objects\n    pickle_protocol : protocol (optional)\n        Protocol can be specified to override the default pickle protocol.\n\n    Examples\n    --------\n    >>> save(scheduler.state_dict(), checkname)\n    """"""\n    return _with_file_like(f, ""wb"", lambda f: _save(obj, f, pickle_module, pickle_protocol))\n\ndef load(f, map_location=None, pickle_module=pickle, **pickle_load_args):\n    """"""Loads an object saved with :func:`save` from file.\n\n    Parameters\n    ----------\n        f (string or file object): a file-like object (has to implement write and flush)\n            or a string containing a file name\n\n    Examples\n    --------\n    >>> scheduler.load_state_dict(load(checkpoint))\n    """"""\n    new_fd = False\n    if isinstance(f, str) or \\\n            (sys.version_info[0] == 2 and isinstance(f, unicode)):\n        new_fd = True\n        f = open(f, \'rb\')\n    elif (sys.version_info[0] == 3 and isinstance(f, pathlib.Path)):\n        new_fd = True\n        f = f.open(\'rb\')\n    try:\n        return _load(f, map_location, pickle_module, **pickle_load_args)\n    finally:\n        if new_fd:\n            f.close()\n\ndef _with_file_like(f, mode, body):\n    """"""\n    Executes a body function with a file object for f, opening\n    it in \'mode\' if it is a string filename.\n    """"""\n    new_fd = False\n    if isinstance(f, str) or \\\n            (sys.version_info[0] == 2 and isinstance(f, unicode)) or \\\n            (sys.version_info[0] == 3 and isinstance(f, pathlib.Path)):\n        new_fd = True\n        f = open(f, mode)\n    try:\n        return body(f)\n    finally:\n        if new_fd:\n            f.close()\n\n\ndef register_package(priority, tagger, deserializer):\n    queue_elem = (priority, tagger, deserializer)\n    _package_registry.append(queue_elem)\n    _package_registry.sort()\n\ndef _cpu_deserialize(obj, location):\n    if location == \'cpu\':\n        return obj\n\ndef _cpu_tag(obj):\n    if type(obj).__module__ == \'mxnet\':\n        return \'cpu\'\n\nregister_package(10, _cpu_tag, _cpu_deserialize)\n\ndef default_restore_location(storage, location):\n    for _, _, fn in _package_registry:\n        result = fn(storage, location)\n        if result is not None:\n            return result\n    raise RuntimeError(""don\'t know how to restore data location"")\n\ndef _is_compressed_file(f):\n    compress_modules = [\'gzip\']\n    try:\n        return f.__module__ in compress_modules\n    except AttributeError:\n        return False\n\ndef _should_read_directly(f):\n    """"""\n    Checks if f is a file that should be read directly. It should be read\n    directly if it is backed by a real file (has a fileno) and is not a\n    a compressed file (e.g. gzip)\n    """"""\n    if _is_compressed_file(f):\n        return False\n    try:\n        return f.fileno() >= 0\n    except io.UnsupportedOperation:\n        return False\n    except AttributeError:\n        return False\n\ndef _check_seekable(f):\n    def raise_err_msg(patterns, e):\n        for p in patterns:\n            if p in str(e):\n                msg = (str(e) + "". You can only load from a file that is seekable."" +\n                                "" Please pre-load the data into a buffer like io.BytesIO and"" +\n                                "" try to load from it instead."")\n                raise type(e)(msg)\n        raise e\n\n    try:\n        f.seek(f.tell())\n        return True\n    except (io.UnsupportedOperation, AttributeError) as e:\n        raise_err_msg([""seek"", ""tell""], e)\n\n\ndef _save(obj, f, pickle_module, pickle_protocol):\n    if sys.version_info[0] == 2:\n        import StringIO\n        if isinstance(f, StringIO.StringIO):\n            msg = (\'save received unsupported StringIO.StringIO file object, whose \'\n                   \'write method does not return the number of bytes written. \'\n                   \'Please use something like io.BytesIO for save instead.\')\n            logger.error(msg)\n            raise RuntimeError(msg)\n\n    serialized_container_types = {}\n    serialized_storages = {}\n\n    sys_info = dict(\n        protocol_version=PROTOCOL_VERSION,\n        little_endian=sys.byteorder == \'little\',\n        type_sizes=dict(\n            short=SHORT_SIZE,\n            int=INT_SIZE,\n            long=LONG_SIZE,\n        ),\n    )\n\n    pickle_module.dump(MAGIC_NUMBER, f, protocol=pickle_protocol)\n    pickle_module.dump(PROTOCOL_VERSION, f, protocol=pickle_protocol)\n    pickle_module.dump(sys_info, f, protocol=pickle_protocol)\n    pickler = pickle_module.Pickler(f, protocol=pickle_protocol)\n    pickler.dump(obj)\n\n    serialized_storage_keys = sorted(serialized_storages.keys())\n    pickle_module.dump(serialized_storage_keys, f, protocol=pickle_protocol)\n    f.flush()\n    for key in serialized_storage_keys:\n        serialized_storages[key]._write_file(f, _should_read_directly(f))\n\n\ndef _load(f, map_location, pickle_module, **pickle_load_args):\n    deserialized_objects = {}\n\n    if map_location is None:\n        restore_location = default_restore_location\n    elif isinstance(map_location, dict):\n        def restore_location(storage, location):\n            location = map_location.get(location, location)\n            return default_restore_location(storage, location)\n    elif isinstance(map_location, _string_classes):\n        def restore_location(storage, location):\n            return default_restore_location(storage, map_location)\n    else:\n        def restore_location(storage, location):\n            result = map_location(storage, location)\n            if result is None:\n                result = default_restore_location(storage, location)\n            return result\n\n    def _check_container_source(container_type, source_file, original_source):\n        try:\n            current_source = inspect.getsource(container_type)\n        except Exception:  # saving the source is optional, so we can ignore any errors\n            warnings.warn(""Couldn\'t retrieve source code for container of ""\n                          ""type "" + container_type.__name__ + "". It won\'t be checked ""\n                          ""for correctness upon loading."")\n            return\n        if original_source != current_source:\n            if container_type.dump_patches:\n                file_name = container_type.__name__ + \'.patch\'\n                diff = difflib.unified_diff(current_source.split(\'\\n\'),\n                                            original_source.split(\'\\n\'),\n                                            source_file,\n                                            source_file, lineterm="""")\n                lines = \'\\n\'.join(diff)\n                try:\n                    with open(file_name, \'a+\') as f:\n                        file_size = f.seek(0, 2)\n                        f.seek(0)\n                        if file_size == 0:\n                            f.write(lines)\n                        elif file_size != len(lines) or f.read() != lines:\n                            raise IOError\n                    msg = (""Saved a reverse patch to "" + file_name + "". ""\n                           ""Run `patch -p0 < "" + file_name + ""` to revert your ""\n                           ""changes."")\n                except IOError:\n                    msg = (""Tried to save a patch, but couldn\'t create a ""\n                           ""writable file "" + file_name + "". Make sure it ""\n                           ""doesn\'t exist and your working directory is ""\n                           ""writable."")\n            else:\n                msg = (""you can retrieve the original source code by ""\n                       ""accessing the object\'s source attribute"")\n            msg = (""source code of class has changed. {}""\n                   .format(msg))\n            warnings.warn(msg, SourceChangeWarning)\n\n    deserialized_objects = {}\n\n    def maybe_decode_ascii(bytes_str):\n        # When using encoding=\'bytes\' in Py3, some **internal** keys stored as\n        # strings in Py2 are loaded as bytes. This function decodes them with\n        # ascii encoding, one that Py3 uses by default.\n        #\n        # NOTE: This should only be used on internal keys (e.g., `typename` and\n        #       `location` in `persistent_load` below!\n        if isinstance(bytes_str, bytes):\n            return bytes_str.decode(\'ascii\')\n        return bytes_str\n\n    def persistent_load(saved_id):\n        assert isinstance(saved_id, tuple)\n        typename = maybe_decode_ascii(saved_id[0])\n        data = saved_id[1:]\n\n        if typename == \'module\':\n            # Ignore containers that don\'t have any sources saved\n            if all(data[1:]):\n                _check_container_source(*data)\n            return data[0]\n        elif typename == \'storage\':\n            data_type, root_key, location, size, view_metadata = data\n            location = maybe_decode_ascii(location)\n            if root_key not in deserialized_objects:\n                obj = data_type(size)\n                deserialized_objects[root_key] = restore_location(obj, location)\n            storage = deserialized_objects[root_key]\n            if view_metadata is not None:\n                view_key, offset, view_size = view_metadata\n                if view_key not in deserialized_objects:\n                    deserialized_objects[view_key] = storage[offset:offset + view_size]\n                return deserialized_objects[view_key]\n            else:\n                return storage\n        else:\n            raise RuntimeError(""Unknown saved id type: %s"" % saved_id[0])\n\n    def legacy_load(f):\n        deserialized_objects = {}\n\n        def persistent_load(saved_id):\n            if isinstance(saved_id, tuple):\n                # Ignore containers that don\'t have any sources saved\n                if all(saved_id[1:]):\n                    _check_container_source(*saved_id)\n                return saved_id[0]\n            return deserialized_objects[int(saved_id)]\n\n        with closing(tarfile.open(fileobj=f, mode=\'r:\', format=tarfile.PAX_FORMAT)) as tar, \\\n                mkdtemp() as tmpdir:\n\n            tar.extract(\'storages\', path=tmpdir)\n            with open(os.path.join(tmpdir, \'storages\'), \'rb\', 0) as f:\n                num_storages = pickle_module.load(f, **pickle_load_args)\n                for i in range(num_storages):\n                    args = pickle_module.load(f, **pickle_load_args)\n                    key, location, storage_type = args\n                    obj = storage_type._new_with_file(f)\n                    obj = restore_location(obj, location)\n                    deserialized_objects[key] = obj\n\n                storage_views = pickle_module.load(f, **pickle_load_args)\n                for target_cdata, root_cdata, offset, size in storage_views:\n                    root = deserialized_objects[root_cdata]\n                    deserialized_objects[target_cdata] = root[offset:offset + size]\n\n            tar.extract(\'tensors\', path=tmpdir)\n            with open(os.path.join(tmpdir, \'tensors\'), \'rb\', 0) as f:\n                num_tensors = pickle_module.load(f, **pickle_load_args)\n                for _ in range(num_tensors):\n                    args = pickle_module.load(f, **pickle_load_args)\n                    key, storage_id, original_tensor_type = args\n                    storage = deserialized_objects[storage_id]\n                    tensor_type = storage_to_tensor_type(storage)\n                    ndim, = struct.unpack(\'<i\', f.read(4))\n                    # skip next 4 bytes; legacy encoding treated ndim as 8 bytes\n                    f.read(4)\n                    size = struct.unpack(\'<{}q\'.format(ndim), f.read(8 * ndim))\n                    stride = struct.unpack(\'<{}q\'.format(ndim), f.read(8 * ndim))\n                    storage_offset, = struct.unpack(\'<q\', f.read(8))\n                    tensor = tensor_type().set_(storage, storage_offset, size, stride)\n                    deserialized_objects[key] = tensor\n\n            pickle_file = tar.extractfile(\'pickle\')\n            unpickler = pickle_module.Unpickler(pickle_file, **pickle_load_args)\n            unpickler.persistent_load = persistent_load\n            result = unpickler.load()\n            return result\n\n    _check_seekable(f)\n    f_should_read_directly = _should_read_directly(f)\n\n    if f_should_read_directly and f.tell() == 0:\n        try:\n            return legacy_load(f)\n        except tarfile.TarError:\n            if zipfile.is_zipfile(f):\n                raise RuntimeError(""Please uncompress the file."")\n            # if not a tarfile, reset file offset and proceed\n            f.seek(0)\n\n    magic_number = pickle_module.load(f, **pickle_load_args)\n    if magic_number != MAGIC_NUMBER:\n        raise RuntimeError(""Invalid magic number; corrupt file?"")\n    protocol_version = pickle_module.load(f, **pickle_load_args)\n    if protocol_version != PROTOCOL_VERSION:\n        raise RuntimeError(""Invalid protocol version: %s"" % protocol_version)\n\n    _sys_info = pickle_module.load(f, **pickle_load_args)\n    unpickler = pickle_module.Unpickler(f, **pickle_load_args)\n    unpickler.persistent_load = persistent_load\n    result = unpickler.load()\n\n    deserialized_storage_keys = pickle_module.load(f, **pickle_load_args)\n\n    offset = f.tell() if f_should_read_directly else None\n    for key in deserialized_storage_keys:\n        assert key in deserialized_objects\n        deserialized_objects[key]._set_from_file(f, offset, f_should_read_directly)\n        offset = None\n\n    return result\n'"
autogluon/utils/sync_remote.py,0,"b'import os\nimport sys\nimport time\nimport json\nimport socket\nimport signal\nimport subprocess\nfrom contextlib import contextmanager\n\n__all__ = [\'sagemaker_setup\']\n\ndef sagemaker_setup():\n    # Read info that SageMaker provides\n    current_host = os.environ[\'SM_CURRENT_HOST\']\n    hosts = json.loads(os.environ[\'SM_HOSTS\'])\n    # Enable SSH connections between containers\n    subprocess.Popen([""/usr/sbin/sshd"", ""-D""])\n    if current_host == sorted(hosts)[0]:\n        _wait_for_worker_nodes_to_start_sshd(hosts)\n    else:\n        sync_training_processes(\'dask-scheduler\', current_host)\n\ndef _wait_for_worker_nodes_to_start_sshd(hosts, interval=1, timeout_in_seconds=180):\n    with timeout(seconds=timeout_in_seconds):\n        while hosts:\n            print(""hosts that aren\'t SSHable yet: %s"", str(hosts))\n            for host in hosts:\n                ssh_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n                if _can_connect(host, 22, ssh_socket):\n                    hosts.remove(host)\n            time.sleep(interval)\n\ndef _can_connect(host, port, s):\n    try:\n        print(""testing connection to host %s"", host)\n        s.connect((host, port))\n        s.close()\n        print(""can connect to host %s"", host)\n        return True\n    except socket.error:\n        print(""can\'t connect to host %s"", host)\n        return False\n\nclass TimeoutError(Exception):\n    pass\n\ndef sync_training_processes(proccess_id_string, worker_id, sync_frequency=300):\n    training_process_started = False\n    while True:\n        time.sleep(sync_frequency)\n        training_process_ps = subprocess.check_output(f\'ps -elf | grep ""{proccess_id_string}""\', encoding=\'utf-8\', shell=True)\n        print(training_process_ps)\n        training_process_count = subprocess.check_output(f\'ps -elf | grep ""{proccess_id_string}"" | wc -l\', encoding=\'utf-8\', shell=True)\n        training_process_count_str = training_process_count.replace(""\\n"", """").strip()\n        training_process_count = int(training_process_count_str) - 2\n        training_process_running = training_process_count > 0\n        if training_process_started:\n            print(f\'training processes running: {training_process_count}\')\n            if not training_process_running:\n                print(f\'Worker {worker_id} training completed.\')\n                time.sleep(5)\n                return\n        if not training_process_started:\n            if training_process_running:\n                training_process_started = True\n            else:\n                print(f\'Worker {worker_id} exiting: training not started in 300 seconds.\')\n                return\n\n@contextmanager\ndef timeout(seconds=0, minutes=0, hours=0):\n    """"""\n    Add a signal-based timeout to any block of code.\n    If multiple time units are specified, they will be added together to determine time limit.\n    Usage:\n    with timeout(seconds=5):\n        my_slow_function(...)\n    Args:\n        - seconds: The time limit, in seconds.\n        - minutes: The time limit, in minutes.\n        - hours: The time limit, in hours.\n    """"""\n\n    limit = seconds + 60 * minutes + 3600 * hours\n\n    def handler(signum, frame):\n        raise TimeoutError(\'timed out after {} seconds\'.format(limit))\n\n    try:\n        signal.signal(signal.SIGALRM, handler)\n        signal.setitimer(signal.ITIMER_REAL, limit)\n        yield\n    finally:\n        signal.alarm(0)\n'"
autogluon/utils/tqdm.py,0,"b'import sys\nfrom .miscs import in_ipynb\n\nfrom tqdm import tqdm as base\n\n__all__ = [\'tqdm\']\n\n\nif True:  # pragma: no cover\n    # import IPython/Jupyter base widget and display utilities\n    IPY = 0\n    IPYW = 0\n    try:  # IPython 4.x\n        import ipywidgets\n        IPY = 4\n        try:\n            IPYW = int(ipywidgets.__version__.split(\'.\')[0])\n        except AttributeError:  # __version__ may not exist in old versions\n            pass\n    except ImportError:  # IPython 3.x / 2.x\n        IPY = 32\n        import warnings\n        with warnings.catch_warnings():\n            ipy_deprecation_msg = ""The `IPython.html` package"" \\\n                                  "" has been deprecated""\n            warnings.filterwarnings(\'error\',\n                                    message="".*"" + ipy_deprecation_msg + "".*"")\n            try:\n                import IPython.html.widgets as ipywidgets\n            except Warning as e:\n                if ipy_deprecation_msg not in str(e):\n                    raise\n                warnings.simplefilter(\'ignore\')\n                try:\n                    import IPython.html.widgets as ipywidgets  # NOQA\n                except ImportError:\n                    pass\n            except ImportError:\n                pass\n\n    try:  # IPython 4.x / 3.x\n        if IPY == 32:\n            from IPython.html.widgets import IntProgress, HBox, HTML, VBox\n            IPY = 3\n        else:\n            from ipywidgets import IntProgress, HBox, HTML, VBox\n    except ImportError:\n        try:  # IPython 2.x\n            from IPython.html.widgets import IntProgressWidget as IntProgress\n            from IPython.html.widgets import ContainerWidget as HBox\n            from IPython.html.widgets import HTML\n            IPY = 2\n        except ImportError:\n            IPY = 0\n\n    try:\n        from IPython.display import display  # , clear_output\n    except ImportError:\n        pass\n\n    # HTML encoding\n    try:  # Py3\n        from html import escape\n    except ImportError:  # Py2\n        from cgi import escape\n\n\nclass mytqdm(base):\n    @staticmethod\n    def status_printer(_, total=None, desc=None, ncols=None, img=None):\n        """"""\n        Manage the printing of an IPython/Jupyter Notebook progress bar widget.\n        """"""\n        # Fallback to text bar if there\'s no total\n        # DEPRECATED: replaced with an \'info\' style bar\n        # if not total:\n        #    return super(mytqdm, mytqdm).status_printer(file)\n\n        # fp = file\n\n        # Prepare IPython progress bar\n        try:\n            if total:\n                pbar = IntProgress(min=0, max=total)\n            else:  # No total? Show info style bar with no progress tqdm status\n                pbar = IntProgress(min=0, max=1)\n                pbar.value = 1\n                pbar.bar_style = \'info\'\n        except NameError:\n            # #187 #451 #558\n            raise ImportError(\n                ""IntProgress not found. Please update jupyter and ipywidgets.""\n                "" See https://ipywidgets.readthedocs.io/en/stable""\n                ""/user_install.html"")\n\n        if desc:\n            pbar.description = desc\n            if IPYW >= 7:\n                pbar.style.description_width = \'initial\'\n        # Prepare status text\n        ptext = HTML()\n        timg = HTML()\n        if img:\n            timg.value = ""<br>%s<br>"" % img\n        # Only way to place text to the right of the bar is to use a container\n        container = VBox([HBox(children=[pbar, ptext]), timg])\n        # Prepare layout\n        if ncols is not None:  # use default style of ipywidgets\n            # ncols could be 100, ""100px"", ""100%""\n            ncols = str(ncols)  # ipywidgets only accepts string\n            try:\n                if int(ncols) > 0:  # isnumeric and positive\n                    ncols += \'px\'\n            except ValueError:\n                pass\n            pbar.layout.flex = \'2\'\n            container.layout.width = ncols\n            container.layout.display = \'inline-flex\'\n            container.layout.flex_flow = \'row wrap\'\n        display(container)\n\n        return container\n\n    def display(self, msg=None, pos=None,\n                # additional signals\n                close=False, bar_style=None):\n        # Note: contrary to native tqdm, msg=\'\' does NOT clear bar\n        # goal is to keep all infos if error happens so user knows\n        # at which iteration the loop failed.\n\n        # Clear previous output (really necessary?)\n        # clear_output(wait=1)\n\n        if not msg and not close:\n            msg = self.__repr__()\n\n        tbar, timg = self.container.children\n        pbar, ptext = tbar.children\n        pbar.value = self.n\n\n        if self.img:\n            timg.value = ""<br>%s<br>"" % self.img\n\n        if msg:\n            # html escape special characters (like \'&\')\n            if \'<bar/>\' in msg:\n                left, right = map(escape, msg.split(\'<bar/>\', 1))\n            else:\n                left, right = \'\', escape(msg)\n\n            # remove inesthetical pipes\n            if left and left[-1] == \'|\':\n                left = left[:-1]\n            if right and right[0] == \'|\':\n                right = right[1:]\n\n            # Update description\n            pbar.description = left\n            if IPYW >= 7:\n                pbar.style.description_width = \'initial\'\n\n            # never clear the bar (signal: msg=\'\')\n            if right:\n                ptext.value = right\n\n        # Change bar style\n        if bar_style:\n            # Hack-ish way to avoid the danger bar_style being overridden by\n            # success because the bar gets closed after the error...\n            if not (pbar.bar_style == \'danger\' and bar_style == \'success\'):\n                pbar.bar_style = bar_style\n\n        # Special signal to close the bar\n        if close and pbar.bar_style != \'danger\':  # hide only if no error\n            try:\n                self.container.close()\n            except AttributeError:\n                self.container.visible = False\n\n    def set_svg(self, img):\n        self.img = img\n\n    def __init__(self, *args, **kwargs):\n        # Setup default output\n        file_kwarg = kwargs.get(\'file\', sys.stderr)\n        if file_kwarg is sys.stderr or file_kwarg is None:\n            kwargs[\'file\'] = sys.stdout  # avoid the red block in IPython\n\n        # Initialize parent class + avoid printing by using gui=True\n        kwargs[\'gui\'] = True\n        kwargs.setdefault(\'bar_format\', \'{l_bar}{bar}{r_bar}\')\n        kwargs[\'bar_format\'] = kwargs[\'bar_format\'].replace(\'{bar}\', \'<bar/>\')\n        super(mytqdm, self).__init__(*args, **kwargs)\n        if self.disable or not kwargs[\'gui\']:\n            return\n\n        # Get bar width\n        self.ncols = \'100%\' if self.dynamic_ncols else kwargs.get(""ncols"", None)\n\n        # Replace with IPython progress bar display (with correct total)\n        unit_scale = 1 if self.unit_scale is True else self.unit_scale or 1\n        total = self.total * unit_scale if self.total else self.total\n        self.img = None\n        self.container = self.status_printer(\n            self.fp, total, self.desc, self.ncols, self.img)\n        self.sp = self.display\n\n        # Print initial bar state\n        if not self.disable:\n            self.display()\n\n    def __iter__(self, *args, **kwargs):\n        try:\n            for obj in super(mytqdm, self).__iter__(*args, **kwargs):\n                # return super(tqdm...) will not catch exception\n                yield obj\n        # NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\n        except:  # NOQA\n            self.sp(bar_style=\'danger\')\n            raise\n\n    def update(self, *args, **kwargs):\n        try:\n            super(mytqdm, self).update(*args, **kwargs)\n        except Exception as exc:\n            # cannot catch KeyboardInterrupt when using manual tqdm\n            # as the interrupt will most likely happen on another statement\n            self.sp(bar_style=\'danger\')\n            raise exc\n\n    def close(self, *args, **kwargs):\n        super(mytqdm, self).close(*args, **kwargs)\n        # If it was not run in a notebook, sp is not assigned, check for it\n        if hasattr(self, \'sp\'):\n            # Try to detect if there was an error or KeyboardInterrupt\n            # in manual mode: if n < total, things probably got wrong\n            if self.total and self.n < self.total:\n                self.sp(bar_style=\'danger\')\n            else:\n                if self.leave:\n                    self.sp(bar_style=\'success\')\n                else:\n                    self.sp(close=True)\n\n    def moveto(self, *args, **kwargs):\n        # void -> avoid extraneous `\\n` in IPython output cell\n        return\n\n\ntqdm = mytqdm if in_ipynb() else base\n'"
autogluon/utils/try_import.py,0,"b'__all__ = [\'try_import_catboost\', \'try_import_lightgbm\', \'try_import_mxboard\', \'try_import_mxnet\',\n           \'try_import_cv2\']\n\ndef try_import_catboost():\n    try:\n        import catboost\n    except ValueError as e:\n        raise ImportError(""Import catboost failed. Numpy version may be outdated, ""\n                          ""Please ensure numpy version >=1.16.0. If it is not, please try \'pip uninstall numpy; pip install numpy>=1.17.0\' Detailed info: {}"".format(str(e)))\n\ndef try_import_lightgbm():\n    try:\n        import lightgbm\n    except OSError as e:\n        raise ImportError(""Import lightgbm failed. If you are using Mac OSX, ""\n                          ""Please try \'brew install libomp\'. Detailed info: {}"".format(str(e)))\n\ndef try_import_mxboard():\n    try:\n        import mxboard\n    except ImportError:\n        raise ImportError(\n            ""Unable to import dependency mxboard. ""\n            ""A quick tip is to install via `pip install mxboard`. "")\n\ndef try_import_mxnet():\n    mx_version = \'1.4.1\'\n    try:\n        import mxnet as mx\n        from distutils.version import LooseVersion\n\n        if LooseVersion(mx.__version__) < LooseVersion(mx_version):\n            msg = (\n                ""Legacy mxnet-mkl=={} detected, some new modules may not work properly. ""\n                ""mxnet-mkl>={} is required. You can use pip to upgrade mxnet ""\n                ""`pip install mxnet-mkl --pre --upgrade` ""\n                ""or `pip install mxnet-cu90mkl --pre --upgrade`"").format(mx.__version__, mx_version)\n            raise ImportError(msg)\n    except ImportError:\n        raise ImportError(\n            ""Unable to import dependency mxnet. ""\n            ""A quick tip is to install via `pip install mxnet-mkl/mxnet-cu90mkl --pre`. "")\n\ndef try_import_cv2():\n    try:\n        import cv2\n    except ImportError:\n        raise ImportError(\n            ""Unable to import dependency cv2. ""\n            ""A quick tip is to install via `pip install opencv-python`. "")\n'"
autogluon/utils/util_decorator.py,0,"b'import weakref\n\nclass classproperty(object):\n    def __init__(self, fget):\n        self.fget = fget\n    def __get__(self, owner_self, owner_cls):\n        return self.fget(owner_cls)\n'"
docs/test/__init__.py,0,b''
examples/datasets/prepare_imagenet.py,0,"b'""""""Prepare the ImageNet dataset""""""\nimport os\nimport argparse\nimport tarfile\nimport pickle\nimport gzip\nimport subprocess\nfrom tqdm import tqdm\nfrom autogluon.utils import check_sha1, download, mkdir\n\n_TARGET_DIR = os.path.expanduser(\'~/.autogluon/datasets/imagenet\')\n_TRAIN_TAR = \'ILSVRC2012_img_train.tar\'\n_TRAIN_TAR_SHA1 = \'43eda4fe35c1705d6606a6a7a633bc965d194284\'\n_VAL_TAR = \'ILSVRC2012_img_val.tar\'\n_VAL_TAR_SHA1 = \'5f3f73da3395154b60528b2b2a2caf2374f5f178\'\n\ndef parse_args():\n    parser = argparse.ArgumentParser(\n        description=\'Setup the ImageNet dataset.\',\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    parser.add_argument(\'--download-dir\', required=True,\n                        help=""The directory that contains downloaded tar files"")\n    parser.add_argument(\'--target-dir\', default=_TARGET_DIR,\n                        help=""The directory to store extracted images"")\n    parser.add_argument(\'--checksum\', action=\'store_true\',\n                        help=""If check integrity before extracting."")\n    parser.add_argument(\'--with-rec\', action=\'store_true\',\n                        help=""If build image record files."")\n    parser.add_argument(\'--num-thread\', type=int, default=1,\n                        help=""Number of threads to use when building image record file."")\n    args = parser.parse_args()\n    return args\n\ndef check_file(filename, checksum, sha1):\n    if not os.path.exists(filename):\n        raise ValueError(\'File not found: \'+filename)\n    if checksum and not check_sha1(filename, sha1):\n        raise ValueError(\'Corrupted file: \'+filename)\n\ndef build_rec_process(img_dir, train=False, num_thread=1):\n    rec_dir = os.path.abspath(os.path.join(img_dir, \'../rec\'))\n    mkdir(rec_dir)\n    prefix = \'train\' if train else \'val\'\n    print(\'Building ImageRecord file for \' + prefix + \' ...\')\n    to_path = rec_dir\n\n    # download lst file and im2rec script\n    script_path = os.path.join(rec_dir, \'im2rec.py\')\n    script_url = \'https://raw.githubusercontent.com/apache/incubator-mxnet/master/tools/im2rec.py\'\n    download(script_url, script_path)\n\n    # execution\n    import sys\n    cmd = [\n        sys.executable,\n        script_path,\n        rec_dir,\n        img_dir,\n        \'--recursive\',\n        \'--pass-through\',\n        \'--pack-label\',\n        \'--num-thread\',\n        str(num_thread)\n    ]\n    subprocess.call(cmd)\n    os.remove(script_path)\n    print(\'ImageRecord file for \' + prefix + \' has been built!\')\n\ndef extract_train(tar_fname, target_dir, with_rec=False, num_thread=1):\n    mkdir(target_dir)\n    with tarfile.open(tar_fname) as tar:\n        print(""Extracting ""+tar_fname+""..."")\n        # extract each class one-by-one\n        pbar = tqdm(total=len(tar.getnames()))\n        for class_tar in tar:\n            pbar.set_description(\'Extract \'+class_tar.name)\n            tar.extract(class_tar, target_dir)\n            class_fname = os.path.join(target_dir, class_tar.name)\n            class_dir = os.path.splitext(class_fname)[0]\n            os.mkdir(class_dir)\n            with tarfile.open(class_fname) as f:\n                f.extractall(class_dir)\n            os.remove(class_fname)\n            pbar.update(1)\n        pbar.close()\n    if with_rec:\n        build_rec_process(target_dir, True, num_thread)\n\ndef extract_val(tar_fname, target_dir, with_rec=False, num_thread=1):\n    mkdir(target_dir)\n    print(\'Extracting \' + tar_fname)\n    with tarfile.open(tar_fname) as tar:\n        tar.extractall(target_dir)\n    # build rec file before images are moved into subfolders\n    if with_rec:\n        build_rec_process(target_dir, False, num_thread)\n    # move images to proper subfolders\n    val_maps_file = os.path.join(os.path.dirname(__file__), \'imagenet_val_maps.pklz\')\n    download(\'https://gluon-cv.mxnet.io/_downloads/f5c3f5262b5968d15a687bf7bd73db68/imagenet_val_maps.pklz\', val_maps_file)\n    with gzip.open(val_maps_file, \'rb\') as f:\n        dirs, mappings = pickle.load(f)\n    for d in dirs:\n        os.makedirs(os.path.join(target_dir, d))\n    for m in mappings:\n        os.rename(os.path.join(target_dir, m[0]), os.path.join(target_dir, m[1], m[0]))\n\ndef main():\n    args = parse_args()\n\n    target_dir = os.path.expanduser(args.target_dir)\n    if os.path.exists(target_dir):\n        raise ValueError(\'Target dir [\'+target_dir+\'] exists. Remove it first\')\n\n    download_dir = os.path.expanduser(args.download_dir)\n    train_tar_fname = os.path.join(download_dir, _TRAIN_TAR)\n    check_file(train_tar_fname, args.checksum, _TRAIN_TAR_SHA1)\n    val_tar_fname = os.path.join(download_dir, _VAL_TAR)\n    check_file(val_tar_fname, args.checksum, _VAL_TAR_SHA1)\n\n    build_rec = args.with_rec\n    if build_rec:\n        os.makedirs(os.path.join(target_dir, \'rec\'))\n    extract_train(train_tar_fname, os.path.join(target_dir, \'train\'), build_rec, args.num_thread)\n    extract_val(val_tar_fname, os.path.join(target_dir, \'val\'), build_rec, args.num_thread)\n\nif __name__ == \'__main__\':\n    main()\n'"
examples/image_classification/benchmark.py,0,"b""import os\nimport argparse\nimport logging\nimport autogluon as ag\nfrom autogluon import ImageClassification as task\nfrom kaggle_configuration import config_choice\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description='Train a model for different kaggle competitions.')\n    parser.add_argument('--data-dir', type=str, default='data/',\n                        help='training and validation pictures to use.')\n    parser.add_argument('--dataset', type=str, default='shopee-iet',\n                        help='the kaggle competition.')\n    parser.add_argument('--custom', type=str, default='predict',\n                        help='the name of the submission file you set.')\n    parser.add_argument('--num-trials', type=int, default=30,\n                        help='number of trials')\n    parser.add_argument('--num-epochs', type=int, default=5,\n                        help='number of training epochs.')\n    parser.add_argument('--batch-size', type=int, default=32,\n                        help='training batch size per device (CPU/GPU).')\n    parser.add_argument('--ngpus-per-trial', type=int, default=1,\n                        help='number of gpus to use.')\n    parser.add_argument('--resume', action='store_true',\n                        help='whether to load last hyperparameters to retrain.')\n    parser.add_argument('--submission', action='store_true',\n                        help='whether to submit test predictions to Leaderboard (Optional).')\n    opt = parser.parse_args()\n    return opt\n\ndef predict_details(test_dataset, classifier, load_dataset):\n    inds, probs, probs_all = classifier.predict(test_dataset)\n    value = []\n    target_dataset = load_dataset.init()\n    for i in inds:\n        value.append(target_dataset.classes[i])\n    return inds, probs, probs_all, value\n\ndef main():\n    opt = parse_args()\n    if not os.path.exists(opt.dataset):\n        os.mkdir(opt.dataset)\n    dataset_path = os.path.join(opt.data_dir, opt.dataset)\n\n    local_path = os.path.dirname(__file__)\n    output_directory = os.path.join(opt.dataset, 'checkpoint/')\n    filehandler = logging.FileHandler(os.path.join(opt.dataset, 'summary.log'))\n    streamhandler = logging.StreamHandler()\n    logger = logging.getLogger('')\n    logger.setLevel(logging.INFO)\n    logger.addHandler(filehandler)\n    logger.addHandler(streamhandler)\n    logging.info(opt)\n\n    target = config_choice(opt.data_dir, opt.dataset)\n    load_dataset = task.Dataset(target['dataset'])\n    classifier = task.fit(dataset=load_dataset,\n                          output_directory=output_directory,\n                          net=target['net'],\n                          optimizer=target['optimizer'],\n                          tricks=target['tricks'],\n                          lr_config=target['lr_config'],\n                          resume=opt.resume,\n                          epochs=opt.num_epochs,\n                          ngpus_per_trial=opt.ngpus_per_trial,\n                          num_trials=opt.num_trials,\n                          batch_size=opt.batch_size,\n                          verbose=True,\n                          plot_results=True)\n\n    summary = classifier.fit_summary(output_directory=opt.dataset, verbosity=4)\n    logging.info('Top-1 val acc: %.3f' % classifier.results['best_reward'])\n    logger.info(summary)\n\n    if opt.submission:\n        test_dataset = task.Dataset(os.path.join(opt.data_dir, opt.dataset, 'test'), train=False)\n        inds, probs, probs_all, value = predict_details(test_dataset, classifier, load_dataset)\n        ag.utils.generate_csv_submission(dataset_path, opt.dataset, local_path, inds, probs_all, value, opt.custom)\n\nif __name__ == '__main__':\n    main()\n\n"""
examples/image_classification/blog.py,0,"b""import os\nimport autogluon as ag\nfrom autogluon import ImageClassification as task\nfrom mxnet import optimizer as optim\n\ndef task_dog_breed_identification(data_path, dataset):\n    images_path = os.path.join(data_path, dataset, 'images_all')\n    label_path =  os.path.join(data_path, dataset, 'labels.csv')\n    test_path = os.path.join(data_path, dataset, 'test')\n    load_dataset = task.Dataset(images_path, label_file=label_path)\n\n    @ag.obj(\n        learning_rate=ag.space.Real(0.3, 0.5),\n        momentum=ag.space.Real(0.90, 0.95),\n        wd=ag.space.Real(1e-6, 1e-4, log=True),\n        multi_precision=False\n    )\n    class NAG(optim.NAG):\n        pass\n\n    classifier = task.fit(dataset=load_dataset,\n                          net=ag.Categorical('standford_dog_resnext101_64x4d', 'standford_dog_resnet152_v1'),\n                          optimizer=NAG(),\n                          epochs=20,\n                          final_fit_epochs=180,\n                          num_trials=40,\n                          ngpus_per_trial=8,\n                          batch_size=48,\n                          verbose=False,\n                          ensemble=1)\n\n    test_dataset = task.Dataset(test_path, train=False, crop_ratio=0.65)\n    inds, probs, probs_all = classifier.predict(test_dataset, set_prob_thresh=0.001)\n    ag.utils.generate_prob_csv(test_dataset, probs_all, custom='./submission.csv')\n\nif __name__ == '__main__':\n    data_path = '/home/ubuntu/workspace/dataset'\n    dataset = 'dog-breed-identification'\n    task_dog_breed_identification(data_path, dataset)"""
examples/image_classification/data_processing.py,0,"b'import csv\nimport os\nimport pandas as pd\nimport shutil\nimport string\nfrom gluoncv.utils import makedirs\nimport argparse\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\'Train a model for different kaggle competitions.\')\n    parser.add_argument(\'--data-dir\', type=str, default=\'/home/ubuntu/workspace/autogluon_kaggle/examples/image_classification/data/\',\n                        help=\'training and validation pictures to use.\')\n    parser.add_argument(\'--dataset\', type=str, default=\'dog\',\n                        help=\'the kaggle competition\')\n    opt = parser.parse_args()\n    return opt\nopt = parse_args()\n\nif opt.dataset == \'dog\':\n\n    csvfile = ""labels.csv""\n    pic_path = ""images_all/""\n    train_path = ""images/""\n    csvfile = os.path.join(opt.data_dir,\'dog-breed-identification\',csvfile)\n    pic_path = os.path.join(opt.data_dir,\'dog-breed-identification\',pic_path)\n    train_path = os.path.join(opt.data_dir,\'dog-breed-identification\',train_path)\n\n    csvfile = open(csvfile, \'r\')\n    data = []\n    for line in csvfile:\n        data.append(list(line.strip().split(\',\')))\n    for i in range(len(data)):\n        if i == 0:\n            continue\n        if i >= 1:\n            cl = data[i][1]\n            name = data[i][0]\n            path = pic_path + str(name) + \'.jpg\'\n            isExists = os.path.exists(path)\n            if (isExists):\n                if not os.path.exists(train_path + cl):\n                    os.makedirs(train_path + cl)\n                newpath = train_path + cl + \'/\' + str(name) + \'.jpg\'\n                shutil.copyfile(path, newpath)\n                print(str(name) + \',success\')\n            else:\n                print(str(name) + "",not here"")\nelif opt.dataset == \'aerial\':\n    csvfile = ""train.csv""\n    pic_path = ""images_all/""\n    train_path = ""images/""\n\n    csvfile = os.path.join(opt.data_dir,\'aerial-cactus-identification\',csvfile)\n    pic_path = os.path.join(opt.data_dir,\'aerial-cactus-identification\',pic_path)\n    train_path = os.path.join(opt.data_dir,\'aerial-cactus-identification\',train_path)\n\n    csvfile = open(csvfile, \'r\')\n    data = []\n    for line in csvfile:\n        data.append(list(line.strip().split(\',\')))\n    for i in range(len(data)):\n        if i == 0:\n            continue\n        if i >= 1:\n            cl = data[i][1]\n            name = data[i][0]\n            path = pic_path + str(name)\n            isExists = os.path.exists(path)\n            if (isExists):\n                if not os.path.exists(train_path + cl):\n                    os.makedirs(train_path + cl)\n                newpath = train_path + cl + \'/\' + str(name)\n                shutil.copyfile(path, newpath)\n                print(str(name) + \',success\')\n            else:\n                print(str(name) + "",not here"")\n\n##\nelif opt.dataset == \'fisheries_Monitoring\':\n    csvfile = os.path.join(opt.data_dir, opt.dataset, \'auto_5_30_fish.csv\')\n    df = pd.read_csv(csvfile)\n    def get_name(name):\n        if name.startswith(\'image\'):\n            name = \'test_stg2/\' + name\n        return name\n    df[\'image\'] = df[\'image\'].apply(get_name)\n    df.to_csv(csvfile.replace(\'auto_5_30_fish\', \'auto_5_30_fish_add\'), index=False)\n\n\n\n'"
examples/image_classification/kaggle_configuration.py,0,"b""import os\nimport autogluon as ag\nfrom mxnet import optimizer as optim\n\ndef download_shopee(dataset, data_path):\n    if not os.path.exists(os.path.join(data_path, dataset + '.zip')):\n        filename = ag.download('https://autogluon.s3.amazonaws.com/datasets/shopee-iet.zip',\n                               path='data/')\n        ag.mkdir(filename[:-4])\n        ag.unzip(filename, root=filename[:-4])\n    else:\n        print(dataset + '.zip already exists.\\n')\n\ndef config_choice(data_path, dataset):\n    global kaggle_choice\n    dataset_path = os.path.join(data_path, dataset, 'images')\n    if dataset == 'dogs-vs-cats-redux-kernels-edition':\n        net_cat = ag.space.Categorical('resnet34_v1b') #resnet34_v1\n        @ag.obj(\n            learning_rate=ag.space.Real(0.3, 0.5),\n            momentum=ag.space.Real(0.86, 0.99),\n            wd=ag.space.Real(1e-5, 1e-3, log=True)\n        )\n        class NAG(optim.NAG):\n            pass\n        optimizer = NAG()\n\n        lr_config = ag.space.Dict(\n                    lr_mode='step',\n                    lr_decay=0.1,\n                    lr_decay_period=0,\n                    lr_decay_epoch='40,80',\n                    warmup_lr=0.0,\n                    warmup_epochs=5)\n        tricks = ag.space.Dict(\n                    last_gamma=True,\n                    use_pretrained=True,\n                    use_se=False,\n                    mixup=False,\n                    mixup_alpha=0.2,\n                    mixup_off_epoch=0,\n                    label_smoothing=True,\n                    no_wd=True,\n                    teacher_name=None,\n                    temperature=20.0,\n                    hard_weight=0.5,\n                    batch_norm=False,\n                    use_gn=False)\n        kaggle_choice = {'classes': 2, 'net': net_cat, 'optimizer': optimizer,\n                         'dataset': dataset_path,\n                         'batch_size': 320,#512\n                         'epochs': 180,\n                         'ngpus_per_trial': 4,\n                         'lr_config': lr_config,\n                         'tricks': tricks,\n                         'num_trials': 16}\n    elif dataset == 'aerial-cactus-identification':\n        net_aeri = ag.space.Categorical('resnet34_v1b')\n        @ag.obj(\n            learning_rate=ag.space.Real(0.3, 0.5),\n            momentum=ag.space.Real(0.88, 0.95),\n            wd=ag.space.Real(1e-5, 1e-3, log=True)\n        )\n        class NAG(optim.NAG):\n            pass\n        optimizer = NAG()\n        lr_config = ag.space.Dict(\n                    lr_mode='step',\n                    lr_decay=0.1,\n                    lr_decay_period=0,\n                    lr_decay_epoch='60,120',\n                    warmup_lr=0.0,\n                    warmup_epochs=5)\n        tricks = ag.space.Dict(\n                    last_gamma=True,\n                    use_pretrained=True,\n                    use_se=False,\n                    mixup=False,\n                    mixup_alpha=0.2,\n                    mixup_off_epoch=0,\n                    label_smoothing=True,\n                    no_wd=True,\n                    teacher_name=None,\n                    temperature=20.0,\n                    hard_weight=0.5,\n                    batch_norm=False,\n                    use_gn=False)\n        kaggle_choice = {'classes': 2, 'net': net_aeri, 'optimizer': optimizer,\n                         'dataset': dataset_path,\n                         'batch_size': 320,#256\n                         'epochs': 180,\n                         'ngpus_per_trial': 4,\n                         'lr_config': lr_config,\n                         'tricks': tricks,\n                         'num_trials': 30}\n    elif dataset == 'plant-seedlings-classification':\n        net_plant = ag.space.Categorical('resnet50_v1')\n        @ag.obj(\n            learning_rate=ag.space.Real(0.3, 0.5),\n            momentum=ag.space.Real(0.85, 0.95),\n            wd=ag.space.Real(1e-6, 1e-4, log=True)\n        )\n        class NAG(optim.NAG):\n            pass\n        optimizer = NAG()\n        lr_config = ag.space.Dict(\n                    lr_mode='cosine',\n                    lr_decay=0.1,\n                    lr_decay_period=0,\n                    lr_decay_epoch='40,80',\n                    warmup_lr=0.0,\n                    warmup_epochs=5)\n        tricks = ag.space.Dict(\n                    last_gamma=True,\n                    use_pretrained=True,\n                    use_se=False,\n                    mixup=False,\n                    mixup_alpha=0.2,\n                    mixup_off_epoch=0,\n                    label_smoothing=True,\n                    no_wd=True,\n                    teacher_name=None,\n                    temperature=20.0,\n                    hard_weight=0.5,\n                    batch_norm=False,\n                    use_gn=False)\n        kaggle_choice = {'classes': 12, 'net': net_plant, 'optimizer': optimizer,\n                         'dataset': dataset_path,\n                         'batch_size': 128,\n                         'epochs': 120,\n                         'ngpus_per_trial': 2,\n                         'lr_config': lr_config,\n                         'tricks': tricks,\n                         'num_trials': 30}\n    elif dataset == 'fisheries_Monitoring':\n        net_fish = ag.space.Categorical('resnet50_v1')\n        @ag.obj(\n            learning_rate=ag.space.Real(0.3, 0.5),\n            momentum=ag.space.Real(0.85, 0.95),\n            wd=ag.space.Real(1e-6, 1e-4, log=True)\n        )\n        class NAG(optim.NAG):\n            pass\n        optimizer = NAG()\n        lr_config = ag.space.Dict(\n                    lr_mode='cosine',\n                    lr_decay=0.1,\n                    lr_decay_period=0,\n                    lr_decay_epoch='40,80',\n                    warmup_lr=0.0,\n                    warmup_epochs=5)\n        tricks = ag.space.Dict(\n                    last_gamma=True,\n                    use_pretrained=True,\n                    use_se=False,\n                    mixup=False,\n                    mixup_alpha=0.2,\n                    mixup_off_epoch=0,\n                    label_smoothing=True,\n                    no_wd=True,\n                    teacher_name=None,\n                    temperature=20.0,\n                    hard_weight=0.5,\n                    batch_norm=False,\n                    use_gn=False)\n        kaggle_choice = {'classes': 8, 'net': net_fish, 'optimizer': optimizer,\n                         'dataset': dataset_path,\n                         'batch_size': 128,\n                         'epochs': 120,\n                         'ngpus_per_trial': 2,\n                         'lr_config': lr_config,\n                         'tricks': tricks,\n                         'num_trials': 30}\n    elif dataset == 'dog-breed-identification':\n        net_dog = ag.space.Categorical('resnext101_64x4d')\n        @ag.obj(\n            learning_rate=ag.space.Real(0.3, 0.5),\n            momentum=ag.space.Real(0.85, 0.95),\n            wd=ag.space.Real(1e-6, 1e-4, log=True)\n        )\n        class NAG(optim.NAG):\n            pass\n        optimizer = NAG()\n        lr_config = ag.space.Dict(\n                    lr_mode='cosine',\n                    lr_decay=0.1,\n                    lr_decay_period=0,\n                    lr_decay_epoch='60,120',\n                    warmup_lr=0.0,\n                    warmup_epochs=5)\n        tricks = ag.space.Dict(\n                    last_gamma=True,\n                    use_pretrained=True,\n                    use_se=False,\n                    mixup=False,\n                    mixup_alpha=0.2,\n                    mixup_off_epoch=0,\n                    label_smoothing=True,\n                    no_wd=True,\n                    teacher_name=None,\n                    temperature=20.0,\n                    hard_weight=0.5,\n                    batch_norm=False,\n                    use_gn=False)\n        kaggle_choice = {'classes': 120, 'net': net_dog, 'optimizer': optimizer,\n                         'dataset': dataset_path,\n                         'batch_size': 48,\n                         'epochs': 180,\n                         'ngpus_per_trial': 4,\n                         'lr_config': lr_config,\n                         'tricks': tricks,\n                         'num_trials': 30}\n    elif dataset == 'shopee-iet-machine-learning-competition':\n        net_shopee = ag.space.Categorical('resnet152_v1d')\n        @ag.obj(\n            learning_rate=ag.space.Real(1e-2, 1e-1, log=True),\n            momentum=ag.space.Real(0.85, 0.95),\n            wd=ag.space.Real(1e-6, 1e-4, log=True)\n        )\n        class NAG(optim.NAG):\n            pass\n        optimizer = NAG()\n        lr_config = ag.space.Dict(\n                    lr_mode='cosine',\n                    lr_decay=0.1,\n                    lr_decay_period=0,\n                    lr_decay_epoch='60,120',\n                    warmup_lr=0.0,\n                    warmup_epochs=5)\n\n        tricks = ag.space.Dict(\n                    last_gamma=True,\n                    use_pretrained=True,\n                    use_se=False,\n                    mixup=False,\n                    mixup_alpha=0.2,\n                    mixup_off_epoch=0,\n                    label_smoothing=True,\n                    no_wd=True,\n                    teacher_name=None,\n                    temperature=20.0,\n                    hard_weight=0.5,\n                    batch_norm=False,\n                    use_gn=False)\n\n        kaggle_choice = {'classes': 18, 'net': net_shopee, 'optimizer': optimizer,\n                         'dataset': dataset_path,\n                         'batch_size': 48,\n                         'epochs': 180,\n                         'ngpus_per_trial': 4,\n                         'lr_config': lr_config,\n                         'tricks': tricks,\n                         'num_trials': 30}\n    elif dataset == 'shopee-iet':\n        download_shopee(dataset, data_path)\n        dataset_path = os.path.join(data_path, dataset, 'data', 'train')\n        net_shopee = ag.space.Categorical('resnet18_v1')\n        @ag.obj(\n            learning_rate=ag.space.Real(1e-4, 1e-2, log=True),\n            momentum=ag.space.Real(0.85, 0.95),\n            wd=ag.space.Real(1e-6, 1e-2, log=True),\n            multi_precision=True\n        )\n        class NAG(optim.NAG):\n            pass\n        optimizer = NAG()\n\n        lr_config = ag.space.Dict(\n                    lr_mode='cosine',\n                    lr_decay=0.1,\n                    lr_decay_period=0,\n                    lr_decay_epoch='40,80',\n                    warmup_lr=0.0,\n                    warmup_epochs=5)\n\n        tricks = ag.space.Dict(\n                    last_gamma=True,\n                    use_pretrained=True,\n                    use_se=False,\n                    mixup=False,\n                    mixup_alpha=0.2,\n                    mixup_off_epoch=0,\n                    label_smoothing=True,\n                    no_wd=True,\n                    teacher_name=None,\n                    temperature=20.0,\n                    hard_weight=0.5,\n                    batch_norm=False,\n                    use_gn=False)\n\n        kaggle_choice = {'classes': 4, 'net': net_shopee, 'optimizer': optimizer,\n                         'dataset': dataset_path,\n                         'batch_size': 2,\n                         'epochs': 1,\n                         'ngpus_per_trial': 1,\n                         'lr_config': lr_config,\n                         'tricks': tricks,\n                         'num_trials': 1}\n    return kaggle_choice\n\n"""
examples/nas/search_efficientnet.py,0,"b""import math\nimport autogluon as ag\nfrom autogluon import ImageClassification as task\n\n@ag.obj(\n    width_coefficient=ag.space.Categorical(1.1, 1.2),\n    depth_coefficient=ag.space.Categorical(1.1, 1.2),\n)\nclass EfficientNetB1(ag.nas.EfficientNet):\n    def __init__(self, width_coefficient, depth_coefficient):\n        input_factor = math.sqrt(2.0 / (width_coefficient ** 2) / depth_coefficient)\n        input_size = math.ceil((224 * input_factor) / 32) * 32\n        super().__init__(width_coefficient=width_coefficient,\n                         depth_coefficient=depth_coefficient,\n                         input_size=input_size)\n\nresults = task.fit('imagenet', net=EfficientNetB1(), search_strategy='grid',\n                   optimizer=ag.optimizer.SGD(learning_rate=1e-1,momentum=0.9,wd=1e-4),\n                   batch_size=32)\n\nprint(results)\n"""
examples/nas/train_enas_imagenet.py,0,"b""import mxnet as mx\nfrom mxnet import gluon\n\nimport autogluon as ag\nfrom autogluon.contrib.enas import *\n\nnet = ENAS_MBNet()\n\nnet.initialize(ctx=mx.gpu(0))\nx = mx.nd.random.uniform(shape=(8, 32, 112, 112), ctx=mx.gpu(0))\nnet.evaluate_latency(x)\n\nprint('average latency is ', net.avg_latency)\n\nscheduler = ENAS_Scheduler(net, train_set='imagenet', num_gpus=8, num_cpu=16, controller_type='atten',\n                           checkname='./enas_imagenet_nov1/checkpoint.ag')\nscheduler.run()\n\n"""
examples/object_detection/benchmark.py,0,"b'import argparse\nimport logging\nimport os\n\nimport mxnet as mx\n\nimport autogluon as ag\nfrom autogluon import ObjectDetection as task\nfrom autogluon.scheduler.resource import get_gpu_count\n\n# meta info for each dataset. { name: (url, index_file_name_trainval, index_file_name_test), ...}\ndataset_dict = {\n    \'clipart\': (\n        \'http://www.hal.t.u-tokyo.ac.jp/~inoue/projects/cross_domain_detection/datasets/clipart.zip\',\n        \'train\', \'test\', None),\n    \'watercolor\': (\n        \'http://www.hal.t.u-tokyo.ac.jp/~inoue/projects/cross_domain_detection/datasets/watercolor.zip\',\n        \'instance_level_annotated\', \'test\', None),\n    \'comic\': (\n        \'http://www.hal.t.u-tokyo.ac.jp/~inoue/projects/cross_domain_detection/datasets/comic.zip\',\n        \'instance_level_annotated\', \'test\', None),\n    \'tiny_motorbike\': (\'https://autogluon.s3.amazonaws.com/datasets/tiny_motorbike.zip\',\n                       \'trainval\', \'test\', (\'motorbike\',))\n}\n\n\ndef get_dataset(args):\n    # built-in dataset (voc)\n    if \'voc\' in args.dataset_name:\n        logging.info(\'Please follow this instruction to download dataset: \\\n            https://gluon-cv.mxnet.io/build/examples_datasets/pascal_voc.html#sphx-glr-build-examples-datasets-pascal-voc-py \')\n        train_dataset = task.Dataset(name=args.dataset_name)\n        test_dataset = task.Dataset(name=args.dataset_name, Train=False)\n        return (train_dataset, test_dataset)\n\n        # custom datset.\n    if args.dataset_name in dataset_dict:\n        url, index_file_name_trainval, index_file_name_test, classes, \\\n            = dataset_dict[args.dataset_name]\n\n        data_root = os.path.join(args.dataset_root, args.dataset_name)\n        if not args.no_redownload:\n            root = args.dataset_root\n            filename_zip = ag.download(url, path=root)\n            filename = ag.unzip(filename_zip, root=root)\n            data_root = os.path.join(root, filename)\n    else:\n        logging.info(\n            ""This dataset is not in dataset_dict. It should be downloaded before running this script."")\n        index_file_name_trainval = args.index_file_name_trainval\n        index_file_name_test = args.index_file_name_test\n        classes = args.classes\n        data_root = args.data_root\n\n    train_dataset = task.Dataset(data_root, index_file_name=index_file_name_trainval,\n                                 classes=classes)\n    test_dataset = task.Dataset(data_root, index_file_name=index_file_name_test, classes=classes,\n                                Train=False)\n\n    return (train_dataset, test_dataset)\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(description=\'benchmark for object detection\')\n    parser.add_argument(\'--dataset-name\', type=str, default=\'voc\', help=""dataset name"")\n    parser.add_argument(\'--dataset-root\', type=str, default=\'./\',\n                        help=""root path to the downloaded dataset, only for custom datastet"")\n    parser.add_argument(\'--dataset-format\', type=str, default=\'voc\', help=""dataset format"")\n    parser.add_argument(\'--index-file-name-trainval\', type=str, default=\'\',\n                        help=""name of txt file which contains images for training and validation "")\n    parser.add_argument(\'--index-file-name-test\', type=str, default=\'\',\n                        help=""name of txt file which contains images for testing"")\n    parser.add_argument(\'--classes\', type=tuple, default=None, help=""classes for custom classes"")\n    parser.add_argument(\'--no-redownload\', action=\'store_true\',\n                        help=""whether need to re-download dataset"")\n    parser.add_argument(\'--meta-arch\', type=str, default=\'yolo3\', choices=[\'yolo3\', \'faster_rcnn\'],\n                        help=""Meta architecture of the model"")\n\n    args = parser.parse_args()\n    logging.info(\'args: {}\'.format(args))\n\n    dataset_train, dataset_test = get_dataset(args)\n\n    time_limits = 5 * 24 * 60 * 60  # 5 days\n    epochs = 20\n    # use coco pre-trained model for custom datasets\n    transfer = None if (\'voc\' in args.dataset_name) or (\'coco\' in args.dataset_name) else \'coco\'\n    if args.meta_arch == \'yolo3\':\n        kwargs = {\'num_trials\': 30, \'epochs\': epochs,\n                  \'net\': ag.Categorical(\'darknet53\', \'mobilenet1.0\'), \'meta_arch\': args.meta_arch,\n                  \'lr\': ag.Categorical(1e-2, 5e-3, 1e-3, 5e-4, 1e-4, 5e-5), \'transfer\': transfer,\n                  \'data_shape\': ag.Categorical(320, 416), \'nthreads_per_trial\': 16,\n                  \'ngpus_per_trial\': 8, \'batch_size\': 64,\n                  \'lr_decay_epoch\': ag.Categorical(\'80,90\', \'85,95\'),\n                  \'warmup_epochs\': ag.Int(1, 10), \'warmup_iters\': ag.Int(250, 1000),\n                  \'wd\': ag.Categorical(1e-4, 5e-4, 2.5e-4), \'syncbn\': ag.Bool(),\n                  \'label_smooth\': ag.Bool(), \'time_limits\': time_limits, \'dist_ip_addrs\': []}\n    elif args.meta_arch == \'faster_rcnn\':\n        kwargs = {\'num_trials\': 30, \'epochs\': ag.Categorical(30, 40, 50, 60),\n                  \'net\': ag.Categorical(\'resnest101\', \'resnest50\'),\n                  \'meta_arch\': args.meta_arch,\n                  \'lr\': ag.Categorical(0.02, 0.01, 0.005, 0.002, 2e-4, 5e-4), \'transfer\': transfer,\n                  \'data_shape\': (640, 800), \'nthreads_per_trial\': 16,\n                  \'ngpus_per_trial\': 8, \'batch_size\': 16,\n                  \'lr_decay_epoch\': ag.Categorical(\'24,28\', \'35\', \'50,55\', \'40\', \'45\', \'55\',\n                                                   \'30, 35\', \'20\'),\n                  \'warmup_iters\': ag.Int(5, 500),\n                  \'wd\': ag.Categorical(1e-4, 5e-4, 2.5e-4), \'syncbn\': True,\n                  \'label_smooth\': False, \'time_limits\': time_limits, \'dist_ip_addrs\': []}\n    else:\n        raise NotImplementedError(\'%s is not implemented.\', args.meta_arch)\n    detector = task.fit(dataset_train, **kwargs)\n    ctx = [mx.gpu(i) for i in range(get_gpu_count())]\n    if not ctx:\n        ctx = [mx.cpu()]\n    test_map = detector.evaluate(dataset_test, ctx=ctx)\n    print(""mAP on test dataset: {}"".format(test_map[-1][-1]))\n    print(test_map)\n    detector.save(\'final_model.model\')\n'"
examples/object_detection/demo.py,0,"b'import autogluon as ag\nfrom autogluon import ObjectDetection as task\nimport os\n\nroot = \'./\'\nfilename_zip = ag.download(\'https://autogluon.s3.amazonaws.com/datasets/tiny_motorbike.zip\',\n                        path=root)\nfilename = ag.unzip(filename_zip, root=root)\n\n\ndata_root = os.path.join(root, filename)\ndataset_train = task.Dataset(data_root, classes=(\'motorbike\',))\n\ntime_limits = 5*60*60 # 5 hours\nepochs = 30\ndetector = task.fit(dataset_train,\n                    num_trials=2,\n                    epochs=epochs,\n                    lr=ag.Categorical(5e-4, 1e-4),\n                    ngpus_per_trial=1,\n                    time_limits=time_limits)\n\n# Evaluation on test dataset\ndataset_test = task.Dataset(data_root, index_file_name=\'test\', classes=(\'motorbike\',))\ntest_map = detector.evaluate(dataset_test)\nprint(""mAP on test dataset: {}"".format(test_map[1][1]))\n\n# visualization \nimage = \'000467.jpg\'\nimage_path = os.path.join(data_root, \'JPEGImages\', image)\nprint(image_path)\n\nind, prob, loc = detector.predict(image_path)\n'"
examples/tabular/example_advanced_tabular.py,0,"b'"""""" Example script for predicting columns of tables, demonstrating more advanced usage of fit(). \n    Note that all settings demonstrated here are just chosen for demonstration purposes (to minimize runtime),\n    and do not represent wise choices to use in practice.\n""""""\n\nimport autogluon as ag\nfrom autogluon import TabularPrediction as task\n\n# Training time:\ntrain_data = task.Dataset(file_path=\'https://autogluon.s3.amazonaws.com/datasets/Inc/train.csv\') # can be local CSV file as well, returns Pandas DataFrame\ntrain_data = train_data.head(100) # subsample for faster demo\nprint(train_data.head())\nlabel_column = \'class\' # specifies which column do we want to predict\nsavedir = \'ag_hpo_models/\' # where to save trained models\n\nhyperparams = {\'NN\': {\'num_epochs\': 10, \'activation\': \'relu\', \'dropout_prob\': ag.Real(0.0,0.5)}, \n               \'GBM\': {\'num_boost_round\': 1000, \'learning_rate\': ag.Real(0.01,0.1,log=True)} }\n\npredictor = task.fit(train_data=train_data, label=label_column, output_directory=savedir, \n                     hyperparameter_tune=True, hyperparameters=hyperparams, \n                     num_trials=5, time_limits=1*60, num_bagging_folds=0, stack_ensemble_levels=0) # since tuning_data = None, automatically determines train/validation split\n\nresults = predictor.fit_summary() # display detailed summary of fit() process\n\n# Inference time:\ntest_data = task.Dataset(file_path=\'https://autogluon.s3.amazonaws.com/datasets/Inc/test.csv\') # another Pandas DataFrame\nprint(test_data.head())\n\nperf = predictor.evaluate(test_data) # shorthand way to evaluate our predictor if test-labels available\n\n# Otherwise we make predictions and can evaluate them later:\ny_test = test_data[label_column]\ntest_data = test_data.drop(labels=[label_column],axis=1) # Delete labels from test data since we wouldn\'t have them in practice\ny_pred = predictor.predict(test_data)\nperf = predictor.evaluate_predictions(y_true=y_test, y_pred=y_pred, auxiliary_metrics=True)\n'"
examples/tabular/example_simple_tabular.py,0,"b'"""""" Example script for predicting columns of tables, demonstrating simple use-case """"""\n\nfrom autogluon import TabularPrediction as task\n\n# Training time:\ntrain_data = task.Dataset(file_path=\'https://autogluon.s3.amazonaws.com/datasets/Inc/train.csv\') # can be local CSV file as well, returns Pandas DataFrame\ntrain_data = train_data.head(500) # subsample for faster demo\nprint(train_data.head())\nlabel_column = \'class\' # specifies which column do we want to predict\nsavedir = \'ag_models/\' # where to save trained models\n\npredictor = task.fit(train_data=train_data, label=label_column, output_directory=savedir) # since tuning_data = None, automatically determines train/validation split\nresults = predictor.fit_summary() # display summary of models trained during fit()\n\n# Inference time:\ntest_data = task.Dataset(file_path=\'https://autogluon.s3.amazonaws.com/datasets/Inc/test.csv\') # another Pandas DataFrame\ny_test = test_data[label_column]\ntest_data = test_data.drop(labels=[label_column],axis=1) # delete labels from test data since we wouldn\'t have them in practice\nprint(test_data.head())\n\npredictor = task.load(savedir) # Unnecessary, we reload predictor just to demonstrate how to load previously-trained predictor from file\ny_pred = predictor.predict(test_data)\nperf = predictor.evaluate_predictions(y_true=y_test, y_pred=y_pred, auxiliary_metrics=True)\n'"
examples/text_classification/example_custom_dataset.py,0,"b""# Step 1: run the dataset downloading scripts\n# sh download_dataset.sh\n\n# Step 2: run the following example script\n\nfrom autogluon import TextClassification as task\n\ndataset = task.Dataset(filepath='./data/nlp-getting-started/train.csv', usecols=['text', 'target'])\npredictor = task.fit(dataset, epochs=1)\nprint('Top-1 val acc: %.3f' % predictor.results['best_reward'])\ntest_acc = predictor.evaluate(dataset)\nprint('Top-1 test acc: %.3f' % test_acc)\n"""
examples/text_classification/example_glue_dataset.py,0,"b""import autogluon as ag\nfrom autogluon import TextClassification as task\n\ndataset = task.Dataset(name='SST')\npredictor = task.fit(dataset, epochs=5, warmup_ratio=0.1, log_interval=400, seed=2, num_trials=100, dev_batch_size=8,\n                     accumulate=None)\nprint('Top-1 val acc: %.3f' % predictor.results['best_reward'])\ntest_acc = predictor.evaluate(dataset)\nprint('Top-1 test acc: %.3f' % test_acc)\nsentence = 'I feel this is awesome!'\nind = predictor.predict(sentence)\nprint('The input sentence sentiment is classified as [%d].' % ind.asscalar())\nprint('The best configuration is:')\nprint(predictor.results['best_config'])"""
examples/torch/train_enas_imagenet.py,1,"b""import torch\nfrom torch import nn\n\nimport autogluon as ag\nfrom autogluon.contrib.torch import *\nfrom autogluon import Dict, Categorical\nfrom autogluon.model_zoo.models.utils import _update_input_size\n\n@enas_unit()\nclass ENAS_MBConv(MBConvBlock):\n    pass\n\nblocks_args = [\n    Dict(kernel=3, num_repeat=1, output_filters=16, expand_ratio=1, stride=1, se_ratio=0.25, input_filters=32),\n    Dict(kernel=3, num_repeat=1, output_filters=16, expand_ratio=1, stride=1, se_ratio=0.25, input_filters=16, with_zero=True),\n    Dict(kernel=Categorical(3, 5, 7), num_repeat=1, output_filters=24, expand_ratio=Categorical(3, 6), stride=2, se_ratio=0.25, input_filters=16),\n    Dict(kernel=Categorical(3, 5, 7), num_repeat=3, output_filters=24, expand_ratio=Categorical(3, 6), stride=1, se_ratio=0.25, input_filters=24, with_zero=True),\n    Dict(kernel=Categorical(3, 5, 7), num_repeat=1, output_filters=40, expand_ratio=Categorical(3, 6), stride=2, se_ratio=0.25, input_filters=24),\n    Dict(kernel=Categorical(3, 5, 7), num_repeat=3, output_filters=40, expand_ratio=Categorical(3, 6), stride=1, se_ratio=0.25, input_filters=40, with_zero=True),\n    Dict(kernel=Categorical(3, 5, 7), num_repeat=1, output_filters=80, expand_ratio=Categorical(3, 6), stride=2, se_ratio=0.25, input_filters=40),\n    Dict(kernel=Categorical(3, 5, 7), num_repeat=4, output_filters=80, expand_ratio=Categorical(3, 6), stride=1, se_ratio=0.25, input_filters=80, with_zero=True),\n    Dict(kernel=Categorical(3, 5, 7), num_repeat=1, output_filters=112, expand_ratio=Categorical(3, 6), stride=1, se_ratio=0.25, input_filters=80),\n    Dict(kernel=Categorical(3, 5, 7), num_repeat=4, output_filters=112, expand_ratio=Categorical(3, 6), stride=1, se_ratio=0.25, input_filters=112, with_zero=True),\n    Dict(kernel=Categorical(3, 5, 7), num_repeat=1, output_filters=192, expand_ratio=Categorical(3, 6), stride=2, se_ratio=0.25, input_filters=112),\n    Dict(kernel=Categorical(3, 5, 7), num_repeat=5, output_filters=192, expand_ratio=Categorical(3, 6), stride=1, se_ratio=0.25, input_filters=192, with_zero=True),\n    Dict(kernel=3, num_repeat=1, output_filters=320, expand_ratio=6, stride=1, se_ratio=0.25, input_filters=192),\n]\n\ninput_size = 224\nConv2D = get_same_padding_conv2d(input_size)\nfeatures = nn.Sequential(\n        Conv2D(3, 32, kernel_size=3, stride=2),\n        nn.BatchNorm2d(32),\n        nn.ReLU(True),\n    )\n\n_blocks = []\ninput_size = 112\nout_filters = 32\nfor block_arg in blocks_args:\n    block_arg.update(input_filters=out_filters, input_size=input_size)\n    out_filters=block_arg.output_filters\n    _blocks.append(ENAS_MBConv(**block_arg))\n    input_size = _update_input_size(input_size, block_arg.stride)\n    if block_arg.num_repeat > 1:\n        block_arg.update(input_filters=out_filters, stride=1,\n                         input_size=input_size)\n\n    for _ in range(block_arg.num_repeat - 1):\n        _blocks.append(ENAS_MBConv(**block_arg))\n\n@enas_net(\n    features = features,\n    blocks = ENAS_Sequential(_blocks),\n)\nclass ENAS_MBNet(nn.Module):\n    def __init__(self, features, blocks, dropout_rate=0.2, num_classes=1000, input_size=224):\n        super().__init__()\n        self.features = features\n        # blocks\n        self.blocks = blocks\n        # head\n        Conv2D = get_same_padding_conv2d(input_size//32)\n        self.conv_head = nn.Sequential(\n            Conv2D(320, 1280, kernel_size=3, stride=2),\n            nn.BatchNorm2d(1280),\n            nn.ReLU(True),\n        )\n        # pool + fc\n        self.pool = nn.AdaptiveAvgPool2d(1)\n        self.flatten = nn.Flatten()\n        self._dropout = nn.Dropout(dropout_rate) if dropout_rate > 0 else None\n        self.fc = nn.Linear(1280, num_classes)\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.blocks(x)\n        x = self.conv_head(x)\n        x = self.pool(x)\n        x = self.flatten(x)\n        if self._dropout:\n            x = self._dropout(x)\n        x = self.fc(x)\n        return x\n\nmbnet = ENAS_MBNet()\nmbnet.cuda()\nx = torch.rand(8, 3, 224, 224).cuda()\nmbnet.evaluate_latency(x)\n\nprint('average latency is ', mbnet.avg_latency)\n\nreward_fn = lambda metric, net: metric * ((net.avg_latency / net.latency) ** 0.1)\n\nscheduler = Torch_ENAS_Scheduler(mbnet, train_set='imagenet', num_cpus=32, num_gpus=8,\n                                 reward_fn=reward_fn, \n                                 warmup_epochs=5, epochs=120, controller_lr=1e-3,\n                                 plot_frequency=0, update_arch_frequency=5)\n\nscheduler.run()\n"""
tests/unittests/test_checkpoint_resume.py,0,"b'import autogluon as ag\nimport logging\nimport numpy as np\nimport time\n\nfrom autogluon.searcher.bayesopt.gpmxnet.comparison_gpy import Branin\nfrom autogluon.searcher.gp_searcher import _to_config_cs\nfrom autogluon.searcher.bayesopt.autogluon.hp_ranges import \\\n    HyperparameterRanges_CS\nfrom autogluon.core import Task\n\nlogger = logging.getLogger(__name__)\n\n\n@ag.args(\n    x1=ag.space.Real(-5.0, 10.0),\n    x2=ag.space.Real(0.0, 15.0))\ndef branin_fn(args, reporter, **kwargs):\n    func = Branin()\n    accuracy = -func.evaluate(args.x1, args.x2)\n    reporter(accuracy=accuracy)\n\n\n# The dependence on epoch is totally made up here, not suitable to benchmark\n# Hyperband.\n# We also sample a sleep time per epoch in 0.1 * [0.5, 1.5]. The random\n# seed for this is generated from the input in a way that if two inputs are\n# very close, they map to the same seed.\n@ag.args(\n    x1=ag.space.Real(-5.0, 10.0),\n    x2=ag.space.Real(0.0, 15.0),\n    epochs=9)\ndef branin_epochs_fn(args, reporter, **kwargs):\n    func = Branin()\n    resume_from = kwargs.get(\'resume_from\', 1)\n    # Sample sleep time per epoch\n    x1 = (args.x1 + 5) / 15\n    x2 = args.x2 / 15\n    local_seed = int(np.floor(x1 * x2 * 10000))\n    random_state = np.random.RandomState(local_seed)\n    time_per_epoch = random_state.uniform(0.05, 0.15)\n    #print(""[{}, {}]: seed={}, time={}"".format(\n    #    args.x1, args.x2, local_seed, time_per_epoch))\n    for epoch in range(resume_from, args.epochs + 1):\n        factor = epoch / args.epochs\n        x1 = (args.x1 - 2.5) * factor + 2.5\n        x2 = (args.x2 - 7.5) * factor + 7.5\n        accuracy = -func.evaluate(x1, x2)\n        time.sleep(time_per_epoch)\n        reporter(accuracy=accuracy, epoch=epoch)\n\n\ndef to_input(config_dict: dict,\n             hp_ranges: HyperparameterRanges_CS) -> np.ndarray:\n    config = _to_config_cs(hp_ranges.config_space, config_dict)\n    return hp_ranges.to_ndarray(config)\n\n\ndef _to_tuple(config):\n    keys = sorted(config.keys())\n    return tuple(config[k] for k in keys)\n\n\n# Pretty silly, slow code to do this:\ndef remove_duplicate_configs(config_history, task_id):\n    unique_configs = []\n    configs_so_far = set()\n    for i in range(len(config_history)):\n        key = str(i + task_id)\n        config = config_history[key]\n        _config = _to_tuple(config)\n        if _config not in configs_so_far:\n            unique_configs.append(config)\n            configs_so_far.add(_config)\n    return unique_configs\n\n\ndef assert_same_config_history(\n        results1, results2, num_trials, hp_ranges, decimal=5):\n    config_history1 = results1[\'config_history\']\n    config_history2 = results2[\'config_history\']\n    assert len(config_history1) == num_trials\n    assert len(config_history2) == num_trials\n    config_history1 = remove_duplicate_configs(\n        config_history1, results1[\'task_id\'])\n    config_history2 = remove_duplicate_configs(\n        config_history2, results2[\'task_id\'])\n    for i in range(min(len(config_history1), len(config_history2))):\n        input1 = to_input(config_history1[i], hp_ranges)\n        input2 = to_input(config_history2[i], hp_ranges)\n        np.testing.assert_almost_equal(\n            input1, input2, decimal=decimal,\n            err_msg=\'Configs different for i = {}\'.format(i))\n\n\ndef print_config_history(config_history, task_id):\n    for i in range(len(config_history)):\n        key = str(i + task_id)\n        print(""{}: {}"".format(i, config_history[key]))\n\n\ndef test_resume_fifo_random():\n    random_seed = 623478423\n    num_trials1 = 10\n    num_trials2 = 20\n    exper_type = \'fifo_random\'\n    checkpoint_fname = \'tests/unittests/checkpoint_{}.ag\'.format(exper_type)\n    search_options = {\'random_seed\': random_seed}\n\n    # First experiment: Two phases, with resume\n    task_id = Task.TASK_ID.value\n    scheduler1 = ag.scheduler.FIFOScheduler(\n        branin_fn,\n        searcher=\'random\',\n        search_options=search_options,\n        checkpoint=checkpoint_fname,\n        num_trials=num_trials1,\n        reward_attr=\'accuracy\')\n    logger.info(""Running [{} - two phases]: num_trials={} and checkpointing"".format(\n        exper_type, num_trials1))\n    scheduler1.run()\n    scheduler1.join_jobs()\n    scheduler2 = ag.scheduler.FIFOScheduler(\n        branin_fn,\n        searcher=\'random\',\n        search_options=search_options,\n        checkpoint=checkpoint_fname,\n        num_trials=num_trials2,\n        reward_attr=\'accuracy\',\n        resume=True)\n    logger.info(""Running [{} - two phases]: Resume from checkpoint, num_trials={}"".format(\n        exper_type, num_trials2))\n    scheduler2.run()\n    scheduler2.join_jobs()\n    searcher = scheduler2.searcher\n    results1 = {\n        \'task_id\': task_id,\n        \'config_history\': scheduler2.config_history,\n        \'best_reward\': searcher.get_best_reward(),\n        \'best_config\': searcher.get_best_config()}\n\n    # Second experiment: Just one phase\n    task_id = Task.TASK_ID.value\n    scheduler3 = ag.scheduler.FIFOScheduler(\n        branin_fn,\n        searcher=\'random\',\n        search_options=search_options,\n        checkpoint=None,\n        num_trials=num_trials2,\n        reward_attr=\'accuracy\')\n    logger.info(""Running [{} - one phase]: num_trials={}"".format(\n        exper_type, num_trials2))\n    scheduler3.run()\n    scheduler3.join_jobs()\n    searcher = scheduler3.searcher\n    results2 = {\n        \'task_id\': task_id,\n        \'config_history\': scheduler3.config_history,\n        \'best_reward\': searcher.get_best_reward(),\n        \'best_config\': searcher.get_best_config()}\n\n    hp_ranges = HyperparameterRanges_CS(branin_fn.cs)\n    assert_same_config_history(\n        results1, results2, num_trials2, hp_ranges)\n\n\ndef test_resume_hyperband_random():\n    random_seed = 623478423\n    num_trials1 = 20\n    num_trials2 = 40\n    search_options = {\'random_seed\': random_seed}\n    scheduler_options = {\n        \'reward_attr\': \'accuracy\',\n        \'time_attr\': \'epoch\',\n        \'max_t\': 9,\n        \'grace_period\': 1,\n        \'reduction_factor\': 3,\n        \'brackets\': 1}\n\n    # Note: The difficulty with HB promotion is that when configs are\n    # promoted, they appear more than once in config_history (which is\n    # indexed by task_id). This can lead to differences between the two\n    # and one phase runs. What matters is that the same configs are\n    # proposed.\n    for hp_type in [\'stopping\', \'promotion\']:\n        exper_type = \'hyperband_{}_random\'.format(hp_type)\n        checkpoint_fname = \'tests/unittests/checkpoint_{}.ag\'.format(exper_type)\n        # First experiment: Two phases, with resume\n        task_id = Task.TASK_ID.value\n        scheduler1 = ag.scheduler.HyperbandScheduler(\n            branin_epochs_fn,\n            searcher=\'random\',\n            search_options=search_options,\n            checkpoint=checkpoint_fname,\n            num_trials=num_trials1,\n            type=hp_type,\n            **scheduler_options)\n        logger.info(""Running [{} - two phases]: num_trials={} and checkpointing"".format(\n            exper_type, num_trials1))\n        scheduler1.run()\n        scheduler1.join_jobs()\n        scheduler2 = ag.scheduler.HyperbandScheduler(\n            branin_epochs_fn,\n            searcher=\'random\',\n            search_options=search_options,\n            checkpoint=checkpoint_fname,\n            num_trials=num_trials2,\n            type=hp_type,\n            resume=True,\n            **scheduler_options)\n        logger.info(""Running [{} - two phases]: Resume from checkpoint, num_trials={}"".format(\n            exper_type, num_trials2))\n        scheduler2.run()\n        scheduler2.join_jobs()\n        searcher = scheduler2.searcher\n        results1 = {\n            \'task_id\': task_id,\n            \'config_history\': scheduler2.config_history,\n            \'best_reward\': searcher.get_best_reward(),\n            \'best_config\': searcher.get_best_config()}\n        # DEBUG\n        #print_config_history(results1[\'config_history\'], task_id)\n\n        # Second experiment: Just one phase\n        task_id = Task.TASK_ID.value\n        scheduler3 = ag.scheduler.HyperbandScheduler(\n            branin_epochs_fn,\n            searcher=\'random\',\n            search_options=search_options,\n            checkpoint=None,\n            num_trials=num_trials2,\n            type=hp_type,\n            **scheduler_options)\n        logger.info(""Running [{} - one phase]: num_trials={}"".format(\n            exper_type, num_trials2))\n        scheduler3.run()\n        scheduler3.join_jobs()\n        searcher = scheduler3.searcher\n        results2 = {\n            \'task_id\': task_id,\n            \'config_history\': scheduler3.config_history,\n            \'best_reward\': searcher.get_best_reward(),\n            \'best_config\': searcher.get_best_config()}\n        # DEBUG\n        #print_config_history(results2[\'config_history\'], task_id)\n\n        hp_ranges = HyperparameterRanges_CS(branin_epochs_fn.cs)\n        assert_same_config_history(\n            results1, results2, num_trials2, hp_ranges)\n\n\nif __name__ == ""__main__"":\n    test_resume_fifo_random()\n    test_resume_hyperband_random()\n'"
tests/unittests/test_classification_tricks.py,0,"b'import os\n\nimport pytest\n\nimport autogluon as ag\nfrom autogluon import ImageClassification as task\nfrom mxnet import optimizer as optim\n\ntricks_combination = [\n    ag.space.Dict(\n        last_gamma=True,\n        use_pretrained=True,\n        use_se=False,\n        mixup=False,\n        mixup_alpha=0.2,\n        mixup_off_epoch=0,\n        label_smoothing=True,\n        no_wd=True,\n        teacher_name=None,\n        temperature=20.0,\n        hard_weight=0.5,\n        batch_norm=False,\n        use_gn=False),\n    ag.space.Dict(\n        last_gamma=False,\n        use_pretrained=False,\n        use_se=False,\n        mixup=True,\n        mixup_alpha=0.2,\n        mixup_off_epoch=0,\n        label_smoothing=False,\n        no_wd=False,\n        teacher_name=\'resnet50_v1\',\n        temperature=20.0,\n        hard_weight=0.5,\n        batch_norm=True,\n        use_gn=False)]\n\n\ndef download_shopee(data_dir, dataset):\n    if not os.path.exists(os.path.join(data_dir, dataset + \'.zip\')):\n        filename = ag.download(\'https://autogluon.s3.amazonaws.com/datasets/shopee-iet.zip\', path=data_dir)\n        ag.mkdir(filename[:-4])\n        ag.unzip(filename, root=filename[:-4])\n    else:\n        print(dataset + \'.zip already exists.\\n\')\n\n\ndef config_choice(dataset, data_path, tricks):\n    dataset_path = os.path.join(data_path, dataset, \'data\', \'train\')\n    net_shopee = ag.space.Categorical(\'resnet50_v1\')\n    @ag.obj(\n        learning_rate=ag.space.Real(1e-4, 1e-2, log=True),\n        momentum=ag.space.Real(0.85, 0.95),\n        wd=ag.space.Real(1e-6, 1e-2, log=True),\n        multi_precision=False\n    )\n    class NAG(optim.NAG):\n        pass\n    optimizer = NAG()\n\n    lr_config = ag.space.Dict(\n        lr_mode=ag.space.Categorical(\'step\', \'cosine\'),\n        lr_decay=ag.space.Real(0.1, 0.2),\n        lr_decay_period=ag.space.Int(0,1),\n        lr_decay_epoch=ag.space.Categorical(\'40,80\'),\n        warmup_lr=ag.space.Real(0.0,0.5),\n        warmup_epochs=ag.space.Int(0,5)\n    )\n\n    kaggle_choice = {\'classes\': 4, \'net\': net_shopee, \'optimizer\': optimizer,\n                     \'dataset\': dataset_path,\n                     \'batch_size\': 4,\n                     \'epochs\': 1,\n                     \'ngpus_per_trial\': 1,\n                     \'lr_config\': lr_config,\n                     \'tricks\': tricks,\n                     \'num_trials\': 1}\n    return kaggle_choice\n\n\n@pytest.mark.slow\n@pytest.mark.parametrize(""combination"", tricks_combination)\ndef test_tricks(combination):\n    dataset = \'shopee-iet\'\n    data_dir = \'./\'\n    download_shopee(data_dir, dataset)\n\n    target = config_choice(dataset, data_dir, combination)\n    classifier = task.fit(dataset = task.Dataset(target[\'dataset\']),\n                          net = target[\'net\'],\n                          optimizer = target[\'optimizer\'],\n                          epochs = target[\'epochs\'],\n                          ngpus_per_trial = target[\'ngpus_per_trial\'],\n                          num_trials = target[\'num_trials\'],\n                          batch_size = target[\'batch_size\'],\n                          verbose = True,\n                          search_strategy=\'random\',\n                          tricks = target[\'tricks\'],\n                          lr_config = target[\'lr_config\'],\n                          plot_results = True)\n\n    test_dataset = task.Dataset(target[\'dataset\'].replace(\'train\', \'test/BabyPants\'), train=False,\n                                scale_ratio_choice=[0.7, 0.8, 0.875])\n    inds, probs, probs_all = classifier.predict(test_dataset, set_prob_thresh=0.001)\n    print(inds[0],probs[0],probs_all[0])\n\n    print(\'Top-1 val acc: %.3f\' % classifier.results[\'best_reward\'])\n    # summary = classifier.fit_summary(output_directory=dataset, verbosity=3)\n    # print(summary)\n\n\n\n\n'"
tests/unittests/test_image_classification.py,0,"b""import autogluon as ag\nfrom autogluon import ImageClassification as task\n\n\ndef test_ensemble():\n    dataset = task.Dataset(name='FashionMNIST')\n    test_dataset = task.Dataset(name='FashionMNIST', train=False)\n    classifier = task.fit(dataset,\n                          epochs=1,\n                          ngpus_per_trial=1,\n                          verbose=False,\n                          ensemble=2)\n    test_acc = classifier.evaluate(test_dataset)\n\n\ndef test_classifier_save_load():\n    dataset = task.Dataset(name='FashionMNIST')\n    test_dataset = task.Dataset(name='FashionMNIST', train=False)\n    classifier = task.fit(dataset,\n                          epochs=1,\n                          ngpus_per_trial=1,\n                          verbose=False)\n    classifier.save('classifier.ag')\n    classifier2 = task.Classifier.load('classifier.ag')\n    test_acc = classifier2.evaluate(test_dataset)\n"""
tests/unittests/test_model_zoo.py,0,"b'import mxnet as mx\nimport pytest\n\nfrom autogluon.model_zoo import get_model\n\nx = mx.nd.random.uniform(shape=(1, 3, 224, 224))\n\n\n@pytest.mark.parametrize(""model_name"", [\n    \'standford_dog_resnet152_v1\', \'standford_dog_resnext101_64x4d\',\n    \'efficientnet_b0\', \'efficientnet_b1\', \'efficientnet_b2\',\n    \'efficientnet_b3\', \'efficientnet_b4\', \'efficientnet_b5\',\n    \'efficientnet_b6\', \'efficientnet_b7\'\n])\ndef test_image_classification_models(model_name):\n    # get the model\n    net = get_model(model_name, pretrained=True)\n    # test inference\n    _ = net(x)\n'"
tests/unittests/test_scheduler.py,0,"b""import numpy as np\nimport autogluon as ag\n\n@ag.args(\n    lr=ag.space.Real(1e-3, 1e-2, log=True),\n    wd=ag.space.Real(1e-3, 1e-2))\ndef train_fn(args, reporter):\n    for e in range(10):\n        dummy_accuracy = 1 - np.power(1.8, -np.random.uniform(e, 2*e))\n        reporter(epoch=e+1, accuracy=dummy_accuracy, lr=args.lr, wd=args.wd)\n\n@ag.args(\n    lr=ag.space.Categorical(1e-3, 1e-2),\n    wd=ag.space.Categorical(1e-3, 1e-2))\ndef rl_train_fn(args, reporter):\n    for e in range(10):\n        dummy_accuracy = 1 - np.power(1.8, -np.random.uniform(e, 2*e))\n        reporter(epoch=e+1, accuracy=dummy_accuracy, lr=args.lr, wd=args.wd)\n\n\ndef test_fifo_scheduler():\n    scheduler = ag.scheduler.FIFOScheduler(train_fn,\n                                           resource={'num_cpus': 4, 'num_gpus': 0},\n                                           num_trials=10,\n                                           reward_attr='accuracy',\n                                           time_attr='epoch',\n                                           checkpoint=None)\n    scheduler.run()\n    scheduler.join_jobs()\n\ndef test_hyperband_scheduler():\n    scheduler = ag.scheduler.HyperbandScheduler(train_fn,\n                                                resource={'num_cpus': 4, 'num_gpus': 0},\n                                                num_trials=10,\n                                                reward_attr='accuracy',\n                                                time_attr='epoch',\n                                                grace_period=1,\n                                                checkpoint=None)\n    scheduler.run()\n    scheduler.join_jobs()\n\ndef test_rl_scheduler():\n    scheduler = ag.scheduler.RLScheduler(rl_train_fn,\n                                         resource={'num_cpus': 4, 'num_gpus': 0},\n                                         num_trials=10,\n                                         reward_attr='accuracy',\n                                         time_attr='epoch',\n                                         checkpoint=None)\n    scheduler.run()\n    scheduler.join_jobs()\n\n"""
tests/unittests/test_search_space.py,0,"b""import autogluon as ag\n\n@ag.obj(\n    name=ag.space.Categorical('auto', 'gluon'),\n)\nclass myobj:\n    def __init__(self, name):\n        self.name = name\n\n@ag.func(\n    framework=ag.space.Categorical('mxnet', 'pytorch'),\n)\ndef myfunc(framework):\n    return framework\n\n@ag.args(\n    a=ag.space.Real(1e-3, 1e-2, log=True),\n    b=ag.space.Real(1e-3, 1e-2),\n    c=ag.space.Int(1, 10),\n    d=ag.space.Categorical('a', 'b', 'c', 'd'),\n    e=ag.space.Bool(),\n    f=ag.space.List(\n            ag.space.Int(1, 2),\n            ag.space.Categorical(4, 5),\n        ),\n    g=ag.space.Dict(\n            a=ag.Real(0, 10),\n            obj=myobj(),\n        ),\n    h=ag.space.Categorical('test', myobj()),\n    i = myfunc(),\n    )\ndef train_fn(args, reporter):\n    a, b, c, d, e, f, g, h, i = args.a, args.b, args.c, args.d, args.e, \\\n            args.f, args.g, args.h, args.i\n    assert a <= 1e-2 and a >= 1e-3\n    assert b <= 1e-2 and b >= 1e-3\n    assert c <= 10 and c >= 1\n    assert d in ['a', 'b', 'c', 'd']\n    assert e in [True, False]\n    assert f[0] in [1, 2]\n    assert f[1] in [4, 5]\n    assert g['a'] <= 10 and g['a'] >= 0\n    assert g.obj.name in ['auto', 'gluon']\n    assert hasattr(h, 'name') or h == 'test'\n    assert i in ['mxnet', 'pytorch']\n    reporter(epoch=1, accuracy=0)\n\n\ndef test_search_space():\n    scheduler = ag.scheduler.FIFOScheduler(train_fn,\n                                           resource={'num_cpus': 4, 'num_gpus': 0},\n                                           num_trials=10,\n                                           reward_attr='accuracy',\n                                           time_attr='epoch',\n                                           checkpoint=None)\n    scheduler.run()\n    scheduler.join_jobs()\n\n"""
tests/unittests/test_skoptsearcher.py,0,"b'import logging\nimport random\nimport ConfigSpace as CS\nimport ConfigSpace.hyperparameters as CSH\nfrom autogluon.searcher import RandomSearcher\nfrom autogluon.searcher import SKoptSearcher\n\n# Suppress known UserWarnings:\nimport warnings\nwarnings.filterwarnings(""ignore"", message="".*objective has been evaluated at this point before.*"")\nwarnings.filterwarnings(""ignore"", message="".*skopt failed to produce new config, using random search instead.*"")\n\nlogger = logging.getLogger(__name__)\n\n\ndef test_skoptsearcher():\n    logger.debug(\'Start testing SKoptSearcher\')\n    random.seed(1)\n    reward_attribute = \'accuracy\'\n    # Create configuration space:\n    cs = CS.ConfigurationSpace()\n    a = CSH.UniformFloatHyperparameter(\'a\', lower=1e-4, upper=1e-1, log=True) # log-scale float\n    b = CSH.UniformFloatHyperparameter(\'b\', lower=-2, upper=0) # float with uniform prior\n    c = CSH.UniformIntegerHyperparameter(\'c\', lower=0, upper=1000) # integer\n    d = CSH.CategoricalHyperparameter(\'d\', choices=[\'good\',\'neutral\',\'bad\']) # categorical\n    cs.add_hyperparameters([a,b,c,d])\n    # Determine reward of optimal config:\n    optimal_config = cs.sample_configuration()\n    optimal_config[\'a\'] = 1e-1\n    optimal_config[\'b\'] = 0\n    optimal_config[\'c\'] = 1000\n    optimal_config[\'d\'] = \'good\' \n    optimal_reward = toy_reward(optimal_config) # should ~= 7025.58\n    # Compare skopt searchers VS random sampling searcher:\n    num_configs_totry = 15\n    skopt_searcher = SKoptSearcher(\n        cs, reward_attribute=reward_attribute)\n    skopt_config_list = [None]*num_configs_totry\n    skopt_reward_list = [0.0]*num_configs_totry # stores rewards scaled between 0-1\n    # Also try skopt searcher which uses various kwargs (random forest surrgoate model, expected improvement acquisition):\n    skrf_searcher = SKoptSearcher(\n        cs, reward_attribute=reward_attribute, base_estimator=\'RF\',\n        acq_func=\'EI\')\n    skrf_config_list = [None]*num_configs_totry \n    skrf_reward_list = [0.0]*num_configs_totry # stores rewards scaled between 0-1\n    # Benchmark against random searcher:\n    rs_searcher = RandomSearcher(cs, reward_attribute=reward_attribute)\n    random_config_list = [None]*num_configs_totry\n    random_reward_list = [0.0]*num_configs_totry\n    # Run search:\n    reported_result = {reward_attribute: 0.0}\n    for i in range(num_configs_totry):\n        skopt_config = skopt_searcher.get_config()\n        skopt_reward = toy_reward(skopt_config) / optimal_reward\n        reported_result[reward_attribute] = skopt_reward\n        skopt_searcher.update(skopt_config, **reported_result)\n        skopt_config_list[i] = skopt_config\n        skopt_reward_list[i] = skopt_reward\n        skrf_config = skrf_searcher.get_config()\n        skrf_reward = toy_reward(skrf_config) / optimal_reward\n        reported_result[reward_attribute] = skrf_reward\n        skrf_searcher.update(skrf_config, **reported_result)\n        skrf_config_list[i] = skrf_config\n        skrf_reward_list[i] = skrf_reward\n        rs_config = rs_searcher.get_config()\n        rs_reward = toy_reward(rs_config) / optimal_reward\n        reported_result[reward_attribute] = rs_reward\n        rs_searcher.update(rs_config, **reported_result)\n        random_config_list[i] = rs_config\n        random_reward_list[i] = rs_reward\n        # print(""Round %d: skopt best reward=%f"" % (i,max(skopt_reward_list)))\n    # Summarize results:\n    logger.debug(""best reward from SKopt: %f,  best reward from SKopt w/ RF: %f,  best reward from Random search: %f"" % \n          (max(skopt_reward_list), max(skrf_reward_list), max(random_reward_list)))\n    # Ensure skopt outperformed random search:\n    assert (max(skopt_reward_list) >= max(random_reward_list)),""SKopt did worse than Random Search""\n    # Ensure skopt found reasonably good config within num_configs_totry:\n    assert (max(skopt_reward_list) >= 0.6),""SKopt performed poorly""\n    logger.debug(\'Test Finished.\')\n\n\ndef toy_reward(config):\n    """""" The reward function to maximize (ie. returns performance from a fake training trial).\n    \n        Args:\n            config: dict() object defined in unit-test, not ConfigSpace object.\n    """"""\n    reward = 10*config[\'b\'] + config[\'c\']\n    reward *= 30**config[\'a\']\n    if config[\'d\'] == \'good\':\n        reward *= 5\n    elif config[\'d\'] == \'neutral\':\n        reward *= 2\n    return reward\n\n'"
tests/unittests/test_tabular.py,0,"b'"""""" Runs autogluon.tabular on multiple benchmark datasets. \n    Run this benchmark with fast_benchmark=False to assess whether major chances make autogluon better or worse overall.\n    Lower performance-values = better, normalized to [0,1] for each dataset to enable cross-dataset comparisons.\n    Classification performance = error-rate, Regression performance = 1 - R^2\n    \n    # TODO: assess that Autogluon correctly inferred the type of each feature (continuous vs categorical vs text)\n    \n    # TODO: may want to take allowed run-time of AutoGluon into account? Eg. can produce performance vs training time curves for each dataset.\n    \n    # TODO: We\'d like to add extra benchmark datasets with the following properties:\n    - parquet file format\n    - poker hand data: https://archive.ics.uci.edu/ml/datasets/Poker+Hand \n    - test dataset with just one data point\n    - test dataset where order of columns different than in training data (same column names)\n    - extreme-multiclass classification (500+ classes)\n    - high-dimensional features + low-sample size\n    - high levels of missingness in test data only, no missingness in train data\n    - classification w severe class imbalance\n    - regression with severely skewed Y-values (eg. predicting count data)\n    - text features in dataset\n""""""\nimport warnings, shutil, os\nimport numpy as np\nimport mxnet as mx\nfrom random import seed\n\nimport pytest\n\nimport autogluon as ag\nfrom autogluon import TabularPrediction as task\nfrom autogluon.utils.tabular.ml.constants import BINARY, MULTICLASS, REGRESSION\n\n\ndef test_tabular():\n    ############ Benchmark options you can set: ########################\n    perf_threshold = 1.1 # How much worse can performance on each dataset be vs previous performance without warning\n    seed_val = 0 # random seed\n    subsample_size = None\n    hyperparameter_tune = False\n    verbosity = 2 # how much output to print\n    hyperparameters = None\n    time_limits = None\n    fast_benchmark = True # False\n    # If True, run a faster benchmark (subsample training sets, less epochs, etc),\n    # otherwise we run full benchmark with default AutoGluon settings.\n    # performance_value warnings are disabled when fast_benchmark = True.\n\n    #### If fast_benchmark = True, can control model training time here. Only used if fast_benchmark=True ####\n    if fast_benchmark:\n        subsample_size = 100\n        time_limits = 60\n\n    fit_args = {\n        \'hyperparameter_tune\': hyperparameter_tune,\n        \'verbosity\': verbosity,\n    }\n    if hyperparameters is not None:\n        fit_args[\'hyperparameters\'] = hyperparameters\n    if time_limits is not None:\n        fit_args[\'time_limits\'] = time_limits\n    ###################################################################\n    run_tabular_benchmarks(fast_benchmark=fast_benchmark, subsample_size=subsample_size, perf_threshold=perf_threshold, seed_val=seed_val, fit_args=fit_args)\n    run_tabular_benchmark_toy(fit_args=fit_args)\n\n\ndef test_advanced_functionality():\n    fast_benchmark = True\n    dataset = {\'url\': \'https://autogluon.s3.amazonaws.com/datasets/AdultIncomeBinaryClassification.zip\',\n                      \'name\': \'AdultIncomeBinaryClassification\',\n                      \'problem_type\': BINARY}\n    label = \'class\'\n    directory_prefix = \'./datasets/\'\n    train_file = \'train_data.csv\'\n    test_file = \'test_data.csv\'\n    train_data, test_data = load_data(directory_prefix=directory_prefix, train_file=train_file, test_file=test_file, name=dataset[\'name\'], url=dataset[\'url\'])\n    if fast_benchmark:  # subsample for fast_benchmark\n        subsample_size = 100\n        train_data = train_data.head(subsample_size)\n        test_data = test_data.head(subsample_size)\n    print(f""Evaluating Advanced Functionality on Benchmark Dataset {dataset[\'name\']}"")\n    directory = directory_prefix + \'advanced/\' + dataset[\'name\'] + ""/""\n    savedir = directory + \'AutogluonOutput/\'\n    shutil.rmtree(savedir, ignore_errors=True)  # Delete AutoGluon output directory to ensure previous runs\' information has been removed.\n    predictor = task.fit(train_data=train_data, label=label, output_directory=savedir)\n    leaderboard = predictor.leaderboard(dataset=test_data)\n    assert(set(predictor.model_names) == set(leaderboard[\'model\']))\n    num_models = len(predictor.model_names)\n    feature_importances = predictor.feature_importance(dataset=test_data)\n    original_features = set(train_data.columns)\n    original_features.remove(label)\n    assert(set(feature_importances.keys()) == original_features)\n    predictor.transform_features()\n    predictor.transform_features(dataset=test_data)\n    predictor.info()\n    assert(predictor.get_model_full_dict() == dict())\n    predictor.refit_full()\n    assert(len(predictor.get_model_full_dict()) == num_models)\n    assert(len(predictor.model_names) == num_models * 2)\n    for model in predictor.model_names:\n        predictor.predict(dataset=test_data, model=model)\n    predictor.refit_full()  # Confirm that refit_models aren\'t further refit.\n    assert(len(predictor.get_model_full_dict()) == num_models)\n    assert(len(predictor.model_names) == num_models * 2)\n    predictor.delete_models(models_to_keep=[])  # Test that dry-run doesn\'t delete models\n    assert(len(predictor.model_names) == num_models * 2)\n    predictor.predict(dataset=test_data)\n    predictor.delete_models(models_to_keep=[], dry_run=False)  # Test that dry-run deletes models\n    assert(len(predictor.model_names) == 0)\n    assert(len(predictor.leaderboard()) == 0)\n    try:\n        predictor.predict(dataset=test_data)\n    except:\n        pass\n    else:\n        raise AssertionError(\'predictor.predict should raise exception after all models are deleted\')\n    print(\'Tabular Advanced Functionality Test Succeeded.\')\n\n\ndef load_data(directory_prefix, train_file, test_file, name, url=None):\n    if not os.path.exists(directory_prefix):\n        os.mkdir(directory_prefix)\n    directory = directory_prefix + name + ""/""\n    train_file_path = directory + train_file\n    test_file_path = directory + test_file\n    if (not os.path.exists(train_file_path)) or (not os.path.exists(test_file_path)):\n        # fetch files from s3:\n        print(""%s data not found locally, so fetching from %s"" % (name, url))\n        zip_name = ag.download(url, directory_prefix)\n        ag.unzip(zip_name, directory_prefix)\n        os.remove(zip_name)\n\n    train_data = task.Dataset(file_path=train_file_path)\n    test_data = task.Dataset(file_path=test_file_path)\n    return train_data, test_data\n\n\ndef run_tabular_benchmark_toy(fit_args):\n    dataset = {\'url\': \'https://autogluon.s3.amazonaws.com/datasets/toyClassification.zip\',\n                          \'name\': \'toyClassification\',\n                          \'problem_type\': MULTICLASS,\n                          \'label_column\': \'y\',\n                          \'performance_val\': 0.436}\n    # 2-D toy noisy, imbalanced 4-class classification task with: feature missingness, out-of-vocabulary feature categories in test data, out-of-vocabulary labels in test data, training column missing from test data, extra distraction columns in test data\n    # toyclassif_dataset should produce 1 warning and 1 error during inference:\n    # Warning: Ignoring 181 (out of 1000) training examples for which the label value in column \'y\' is missing\n    # ValueError: Required columns are missing from the provided dataset. Missing columns: [\'lostcolumn\']\n\n    # Additional warning that would have occurred if ValueError was not triggered:\n    # UserWarning: These columns from this dataset were not present in the training dataset (AutoGluon will ignore them):  [\'distractioncolumn1\', \'distractioncolumn2\']\n\n    directory_prefix = \'./datasets/\'\n    train_file = \'train_data.csv\'\n    test_file = \'test_data.csv\'\n    train_data, test_data = load_data(directory_prefix=directory_prefix, train_file=train_file, test_file=test_file, name=dataset[\'name\'], url=dataset[\'url\'])\n    print(f""Evaluating Benchmark Dataset {dataset[\'name\']}"")\n    directory = directory_prefix + dataset[\'name\'] + ""/""\n    savedir = directory + \'AutogluonOutput/\'\n    shutil.rmtree(savedir, ignore_errors=True)  # Delete AutoGluon output directory to ensure previous runs\' information has been removed.\n    predictor = task.fit(train_data=train_data, label=dataset[\'label_column\'], output_directory=savedir, **fit_args)\n    try:\n        predictor.predict(test_data)\n    except ValueError:  # ValueError should be raised because test_data has missing column \'lostcolumn\'\n        pass\n    else:\n        raise AssertionError(f\'{dataset[""name""]} should raise an exception.\')\n\n\ndef run_tabular_benchmarks(fast_benchmark, subsample_size, perf_threshold, seed_val, fit_args, dataset_indices=None):\n    print(""Running fit with args:"")\n    print(fit_args)\n    # Each train/test dataset must be located in single directory with the given names.\n    train_file = \'train_data.csv\'\n    test_file = \'test_data.csv\'\n    EPS = 1e-10\n\n    # Information about each dataset in benchmark is stored in dict.\n    # performance_val = expected performance on this dataset (lower = better),should update based on previously run benchmarks\n    binary_dataset = {\'url\': \'https://autogluon.s3.amazonaws.com/datasets/AdultIncomeBinaryClassification.zip\',\n                      \'name\': \'AdultIncomeBinaryClassification\',\n                      \'problem_type\': BINARY,\n                      \'label_column\': \'class\',\n                      \'performance_val\': 0.129} # Mixed types of features.\n\n    multi_dataset = {\'url\': \'https://autogluon.s3.amazonaws.com/datasets/CoverTypeMulticlassClassification.zip\',\n                      \'name\': \'CoverTypeMulticlassClassification\',\n                      \'problem_type\': MULTICLASS,\n                      \'label_column\': \'Cover_Type\',\n                      \'performance_val\': 0.032} # big dataset with 7 classes, all features are numeric. Runs SLOW.\n\n    regression_dataset = {\'url\': \'https://autogluon.s3.amazonaws.com/datasets/AmesHousingPriceRegression.zip\',\n                       \'name\': \'AmesHousingPriceRegression\',\n                      \'problem_type\': REGRESSION,\n                      \'label_column\': \'SalePrice\',\n                      \'performance_val\': 0.076} # Regression with mixed feature-types, skewed Y-values.\n\n    toyregres_dataset = {\'url\': \'https://autogluon.s3.amazonaws.com/datasets/toyRegression.zip\',\n                         \'name\': \'toyRegression\',\n                         \'problem_type\': REGRESSION, \n                        \'label_column\': \'y\', \n                        \'performance_val\': 0.183}\n    # 1-D toy deterministic regression task with: heavy label+feature missingness, extra distraction column in test data\n\n    # List containing dicts for each dataset to include in benchmark (try to order based on runtimes)\n    datasets = [toyregres_dataset, binary_dataset, regression_dataset, multi_dataset]\n    if dataset_indices is not None: # only run some datasets\n        datasets = [datasets[i] for i in dataset_indices]\n\n    # Aggregate performance summaries obtained in previous benchmark run:\n    prev_perf_vals = [dataset[\'performance_val\'] for dataset in datasets]\n    previous_avg_performance = np.mean(prev_perf_vals)\n    previous_median_performance = np.median(prev_perf_vals)\n    previous_worst_performance = np.max(prev_perf_vals)\n\n    # Run benchmark:\n    performance_vals = [0.0] * len(datasets) # performance obtained in this run\n    directory_prefix = \'./datasets/\'\n    with warnings.catch_warnings(record=True) as caught_warnings:\n        for idx in range(len(datasets)):\n            dataset = datasets[idx]\n            train_data, test_data = load_data(directory_prefix=directory_prefix, train_file=train_file, test_file=test_file, name=dataset[\'name\'], url=dataset[\'url\'])\n            if seed_val is not None:\n                seed(seed_val)\n                np.random.seed(seed_val)\n                mx.random.seed(seed_val)\n            print(""Evaluating Benchmark Dataset %s (%d of %d)"" % (dataset[\'name\'], idx+1, len(datasets)))\n            directory = directory_prefix + dataset[\'name\'] + ""/""\n            savedir = directory + \'AutogluonOutput/\'\n            shutil.rmtree(savedir, ignore_errors=True) # Delete AutoGluon output directory to ensure previous runs\' information has been removed.\n            label_column = dataset[\'label_column\']\n            y_test = test_data[label_column]\n            test_data = test_data.drop(labels=[label_column], axis=1)\n            if fast_benchmark:\n                if subsample_size is None:\n                    raise ValueError(""fast_benchmark specified without subsample_size"")\n                train_data = train_data.head(subsample_size) # subsample for fast_benchmark\n            predictor = task.fit(train_data=train_data, label=label_column, output_directory=savedir, **fit_args)\n            results = predictor.fit_summary(verbosity=0)\n            if predictor.problem_type != dataset[\'problem_type\']:\n                warnings.warn(""For dataset %s: Autogluon inferred problem_type = %s, but should = %s"" % (dataset[\'name\'], predictor.problem_type, dataset[\'problem_type\']))\n            predictor = task.load(savedir)  # Test loading previously-trained predictor from file\n            y_pred = predictor.predict(test_data)\n            perf_dict = predictor.evaluate_predictions(y_true=y_test, y_pred=y_pred, auxiliary_metrics=True)\n            if dataset[\'problem_type\'] != REGRESSION:\n                perf = 1.0 - perf_dict[\'accuracy_score\'] # convert accuracy to error-rate\n            else:\n                perf = 1.0 - perf_dict[\'r2_score\'] # unexplained variance score.\n            performance_vals[idx] = perf\n            print(""Performance on dataset %s: %s   (previous perf=%s)"" % (dataset[\'name\'], performance_vals[idx], dataset[\'performance_val\']))\n            if (not fast_benchmark) and (performance_vals[idx] > dataset[\'performance_val\'] * perf_threshold):\n                warnings.warn(""Performance on dataset %s is %s times worse than previous performance."" % \n                              (dataset[\'name\'], performance_vals[idx]/(EPS+dataset[\'performance_val\'])))\n\n    # Summarize:\n    avg_perf = np.mean(performance_vals)\n    median_perf = np.median(performance_vals)\n    worst_perf = np.max(performance_vals)\n    for idx in range(len(datasets)):\n        print(""Performance on dataset %s: %s   (previous perf=%s)"" % (datasets[idx][\'name\'], performance_vals[idx], datasets[idx][\'performance_val\']))\n\n    print(""Average performance: %s"" % avg_perf)\n    print(""Median performance: %s"" % median_perf)\n    print(""Worst performance: %s"" % worst_perf)\n\n    if not fast_benchmark:\n        if avg_perf > previous_avg_performance * perf_threshold:\n            warnings.warn(""Average Performance is %s times worse than previously."" % (avg_perf/(EPS+previous_avg_performance)))\n        if median_perf > previous_median_performance * perf_threshold:\n            warnings.warn(""Median Performance is %s times worse than previously."" % (median_perf/(EPS+previous_median_performance)))\n        if worst_perf > previous_worst_performance * perf_threshold:\n            warnings.warn(""Worst Performance is %s times worse than previously."" % (worst_perf/(EPS+previous_worst_performance)))\n\n    print(""Ran fit with args:"")\n    print(fit_args)\n    # List all warnings again to make sure they are seen:\n    print(""\\n\\n WARNINGS:"")\n    for w in caught_warnings:\n        warnings.warn(w.message)\n\n\n@pytest.mark.slow\ndef test_tabularHPObagstack():\n    ############ Benchmark options you can set: ########################\n    perf_threshold = 1.1 # How much worse can performance on each dataset be vs previous performance without warning\n    seed_val = 10000 # random seed\n    subsample_size = None\n    hyperparameter_tune = True\n    stack_ensemble_levels = 2\n    num_bagging_folds = 2\n    verbosity = 2 # how much output to print\n    hyperparameters = None\n    time_limits = None\n    num_trials = None\n    fast_benchmark = True # False\n    # If True, run a faster benchmark (subsample training sets, less epochs, etc),\n    # otherwise we run full benchmark with default AutoGluon settings.\n    # performance_value warnings are disabled when fast_benchmark = True.\n\n    #### If fast_benchmark = True, can control model training time here. Only used if fast_benchmark=True ####\n    if fast_benchmark:\n        subsample_size = 100\n        nn_options = {\'num_epochs\': 2, \'learning_rate\': ag.Real(0.001,0.01), \'lr_scheduler\': ag.Categorical(None, \'cosine\',\'step\')}\n        gbm_options = {\'num_boost_round\': 20, \'learning_rate\': ag.Real(0.01,0.1)}\n        hyperparameters = {\'GBM\': gbm_options, \'NN\': nn_options}\n        time_limits = 150\n        num_trials = 3\n\n    fit_args = {\n        \'num_bagging_folds\': num_bagging_folds,\n        \'stack_ensemble_levels\': stack_ensemble_levels,\n        \'hyperparameter_tune\': hyperparameter_tune,\n        \'verbosity\': verbosity,\n    }\n    if hyperparameters is not None:\n        fit_args[\'hyperparameters\'] = hyperparameters\n    if time_limits is not None:\n        fit_args[\'time_limits\'] = time_limits\n        fit_args[\'num_bagging_sets\'] = 2\n    if num_trials is not None:\n        fit_args[\'num_trials\'] = num_trials\n    ###################################################################\n    run_tabular_benchmarks(fast_benchmark=fast_benchmark, subsample_size=subsample_size, perf_threshold=perf_threshold,\n                           seed_val=seed_val, fit_args=fit_args)\n\n\ndef test_tabularHPO():\n    ############ Benchmark options you can set: ########################\n    perf_threshold = 1.1 # How much worse can performance on each dataset be vs previous performance without warning\n    seed_val = 99 # random seed\n    subsample_size = None\n    hyperparameter_tune = True\n    verbosity = 2 # how much output to print\n    hyperparameters = None\n    time_limits = None\n    num_trials = None\n    fast_benchmark = True # False\n    # If True, run a faster benchmark (subsample training sets, less epochs, etc),\n    # otherwise we run full benchmark with default AutoGluon settings.\n    # performance_value warnings are disabled when fast_benchmark = True.\n\n    #### If fast_benchmark = True, can control model training time here. Only used if fast_benchmark=True ####\n    if fast_benchmark:\n        subsample_size = 100\n        nn_options = {\'num_epochs\': 2}\n        gbm_options = {\'num_boost_round\': 20}\n        hyperparameters = {\'GBM\': gbm_options, \'NN\': nn_options}\n        time_limits = 60\n        num_trials = 5\n\n    fit_args = {\n        \'hyperparameter_tune\': hyperparameter_tune,\n        \'verbosity\': verbosity,\n    }\n    if hyperparameters is not None:\n        fit_args[\'hyperparameters\'] = hyperparameters\n    if time_limits is not None:\n        fit_args[\'time_limits\'] = time_limits\n    if num_trials is not None:\n        fit_args[\'num_trials\'] = num_trials\n    ###################################################################\n    run_tabular_benchmarks(fast_benchmark=fast_benchmark, subsample_size=subsample_size, perf_threshold=perf_threshold,\n                           seed_val=seed_val, fit_args=fit_args)\n\n\ndef test_tabular_bag():\n    ############ Benchmark options you can set: ########################\n    num_bagging_folds = 3\n    stack_ensemble_levels = 0\n    perf_threshold = 1.1 # How much worse can performance on each dataset be vs previous performance without warning\n    seed_val = 123 # random seed\n    subsample_size = None\n    hyperparameter_tune = False\n    verbosity = 2 # how much output to print\n    hyperparameters = None\n    time_limits = None\n    fast_benchmark = True # False\n    # If True, run a faster benchmark (subsample training sets, less epochs, etc),\n    # otherwise we run full benchmark with default AutoGluon settings.\n    # performance_value warnings are disabled when fast_benchmark = True.\n\n    #### If fast_benchmark = True, can control model training time here. Only used if fast_benchmark=True ####\n    if fast_benchmark:\n        subsample_size = 120\n        nn_options = {\'num_epochs\': 1}\n        gbm_options = {\'num_boost_round\': 30}\n        hyperparameters = {\'GBM\': gbm_options, \'NN\': nn_options}\n        time_limits = 60\n\n    fit_args = {\n        \'num_bagging_folds\': num_bagging_folds,\n        \'stack_ensemble_levels\': stack_ensemble_levels,\n        \'hyperparameter_tune\': hyperparameter_tune,\n        \'verbosity\': verbosity,\n    }\n    if hyperparameters is not None:\n        fit_args[\'hyperparameters\'] = hyperparameters\n    if time_limits is not None:\n        fit_args[\'time_limits\'] = time_limits\n        fit_args[\'num_bagging_sets\'] = 2\n    ###################################################################\n    run_tabular_benchmarks(fast_benchmark=fast_benchmark, subsample_size=subsample_size, perf_threshold=perf_threshold,\n                           seed_val=seed_val, fit_args=fit_args)\n\n\n@pytest.mark.skip(reason=""Ignored for now, since stacking is disabled without bagging."")\ndef test_tabular_stack1():\n    ############ Benchmark options you can set: ########################\n    stack_ensemble_levels = 1\n    num_bagging_folds = 0\n    perf_threshold = 1.1 # How much worse can performance on each dataset be vs previous performance without warning\n    seed_val = 32 # random seed\n    subsample_size = None\n    hyperparameter_tune = False\n    verbosity = 2 # how much output to print\n    hyperparameters = None\n    time_limits = None\n    fast_benchmark = True # False\n    # If True, run a faster benchmark (subsample training sets, less epochs, etc),\n    # otherwise we run full benchmark with default AutoGluon settings.\n    # performance_value warnings are disabled when fast_benchmark = True.\n\n    #### If fast_benchmark = True, can control model training time here. Only used if fast_benchmark=True ####\n    if fast_benchmark:\n        subsample_size = 100\n        nn_options = {\'num_epochs\': 3}\n        gbm_options = {\'num_boost_round\': 30}\n        hyperparameters = {\'GBM\': gbm_options, \'NN\': nn_options}\n        time_limits = 60\n\n    fit_args = {\n        \'num_bagging_folds\': num_bagging_folds,\n        \'stack_ensemble_levels\': stack_ensemble_levels,\n        \'hyperparameter_tune\': hyperparameter_tune,\n        \'verbosity\': verbosity,\n    }\n    if hyperparameters is not None:\n        fit_args[\'hyperparameters\'] = hyperparameters\n    if time_limits is not None:\n        fit_args[\'time_limits\'] = time_limits\n    ###################################################################\n    run_tabular_benchmarks(fast_benchmark=fast_benchmark, subsample_size=subsample_size, perf_threshold=perf_threshold,\n                           seed_val = seed_val, fit_args=fit_args)\n\n\n@pytest.mark.skip(reason=""Ignored for now, since stacking is disabled without bagging."")\ndef test_tabular_stack2():\n    ############ Benchmark options you can set: ########################\n    stack_ensemble_levels = 2\n    num_bagging_folds = 0\n    perf_threshold = 1.1 # How much worse can performance on each dataset be vs previous performance without warning\n    seed_val = 66 # random seed\n    subsample_size = None\n    hyperparameter_tune = False\n    verbosity = 2 # how much output to print\n    hyperparameters = None\n    time_limits = None\n    fast_benchmark = True # False\n    # If True, run a faster benchmark (subsample training sets, less epochs, etc),\n    # otherwise we run full benchmark with default AutoGluon settings.\n    # performance_value warnings are disabled when fast_benchmark = True.\n\n    #### If fast_benchmark = True, can control model training time here. Only used if fast_benchmark=True ####\n    if fast_benchmark:\n        subsample_size = 100\n        nn_options = {\'num_epochs\': 3}\n        gbm_options = {\'num_boost_round\': 30}\n        hyperparameters = {\'GBM\': gbm_options, \'NN\': nn_options}\n        time_limits = 60\n\n    fit_args = {\n        \'num_bagging_folds\': num_bagging_folds,\n        \'stack_ensemble_levels\': stack_ensemble_levels,\n        \'hyperparameter_tune\': hyperparameter_tune,\n        \'verbosity\': verbosity,\n    }\n    if hyperparameters is not None:\n        fit_args[\'hyperparameters\'] = hyperparameters\n    if time_limits is not None:\n        fit_args[\'time_limits\'] = time_limits\n    ###################################################################\n    run_tabular_benchmarks(fast_benchmark=fast_benchmark, subsample_size=subsample_size, perf_threshold=perf_threshold,\n                           seed_val=seed_val, fit_args=fit_args)\n\n\n@pytest.mark.slow\ndef test_tabular_bagstack():\n    ############ Benchmark options you can set: ########################\n    stack_ensemble_levels = 2\n    num_bagging_folds = 3\n    perf_threshold = 1.1 # How much worse can performance on each dataset be vs previous performance without warning\n    seed_val = 53 # random seed\n    subsample_size = None\n    hyperparameter_tune = False\n    verbosity = 2 # how much output to print\n    hyperparameters = None\n    time_limits = None\n    fast_benchmark = True # False\n    # If True, run a faster benchmark (subsample training sets, less epochs, etc),\n    # otherwise we run full benchmark with default AutoGluon settings.\n    # performance_value warnings are disabled when fast_benchmark = True.\n\n    #### If fast_benchmark = True, can control model training time here. Only used if fast_benchmark=True ####\n    if fast_benchmark:\n        subsample_size = 105\n        nn_options = {\'num_epochs\': 2}\n        gbm_options = {\'num_boost_round\': 40}\n        hyperparameters = {\'GBM\': gbm_options, \'NN\': nn_options, \'custom\': [\'GBM\']}\n        time_limits = 60\n\n    fit_args = {\n        \'num_bagging_folds\': num_bagging_folds,\n        \'stack_ensemble_levels\': stack_ensemble_levels,\n        \'hyperparameter_tune\': hyperparameter_tune,\n        \'verbosity\': verbosity,\n    }\n    if hyperparameters is not None:\n        fit_args[\'hyperparameters\'] = hyperparameters\n    if time_limits is not None:\n        fit_args[\'time_limits\'] = time_limits\n        fit_args[\'num_bagging_sets\'] = 2\n    ###################################################################\n    run_tabular_benchmarks(fast_benchmark=fast_benchmark, subsample_size=subsample_size, perf_threshold=perf_threshold,\n                           seed_val=seed_val, fit_args=fit_args)\n\n'"
tests/unittests/test_text_classification.py,0,"b""import os\nimport autogluon as ag\nfrom autogluon import TextClassification as task\n\ndef test_fit():\n    dataset = task.Dataset(name='ToySST')\n    predictor = task.fit(dataset,\n                         net=ag.Categorical('bert_12_768_12'),\n                         pretrained_dataset=ag.Categorical('book_corpus_wiki_en_uncased'),\n                         epochs=1,\n                         num_trials=1,\n                         batch_size=4,\n                         dev_batch_size=4,\n                         max_len=16,\n                         ngpus_per_trial=0,\n                         seed=2)\n    test_acc = predictor.evaluate(dataset)\n    print('accuracy is %.2f' % test_acc)\n    print('finished')\n\ndef test_custom_dataset_fit():\n    os.system('wget https://autogluon-hackathon.s3.amazonaws.com/demodata.zip')\n    os.system('unzip -o demodata.zip')\n    dataset = task.Dataset(filepath='./demodata/train.csv', usecols=['text', 'target'])\n    predictor = task.fit(dataset,\n                         net=ag.Categorical('bert_12_768_12'),\n                         pretrained_dataset=ag.Categorical('book_corpus_wiki_en_uncased'),\n                         epochs=1,\n                         num_trials=1,\n                         batch_size=4,\n                         dev_batch_size=4,\n                         max_len=16,\n                         ngpus_per_trial=0,\n                         seed=2)\n    test_acc = predictor.evaluate(dataset)\n    print('accuracy is %.2f' % test_acc)\n    print('finished')\n"""
autogluon/contrib/enas/__init__.py,0,b'from .enas import *\nfrom .enas_net import *\nfrom .enas_scheduler import *\n\n'
autogluon/contrib/enas/enas.py,0,"b'import json\nimport collections\nimport mxnet as mx\nfrom mxnet import gluon\nfrom ...core.space import Categorical, Space, _strip_config_space\n\nimport warnings\nwarnings.filterwarnings(""ignore"", category=UserWarning)\n\n__all__ = [\'enas_unit\', \'enas_net\',\n           \'Zero_Unit\', \'ENAS_Unit\', \'ENAS_Sequential\']\n\ndef enas_unit(**kwvars):\n    def registered_class(Cls):\n        class enas_unit(ENAS_Unit):\n            def __init__(self, *args, **kwargs):\n                kwvars.update(kwargs)\n                with_zero=False\n                if \'with_zero\' in kwvars:\n                    with_zero = kwvars.pop(\'with_zero\')\n                blocks = []\n                self._args = []\n                for arg in self.get_config_grid(kwvars):\n                    blocks.append(Cls(*args, **arg))\n                    self._args.append(json.dumps(arg))\n                if with_zero:\n                    self._args.append(None)\n                super().__init__(*blocks, with_zero=with_zero)\n\n            @property\n            def node(self):\n                arg = self._args[self.index]\n                if arg is None: return arg\n                summary = {}\n                name = self.module_list[self.index].__class__.__name__ + \'(\'\n                for k, v in json.loads(arg).items():\n                    if \'kernel\' in k.lower():\n                        cm = (""#8dd3c7"", ""#fb8072"", ""#ffffb3"", ""#bebada"", ""#80b1d3"",\n                              ""#fdb462"", ""#b3de69"", ""#fccde5"")\n                        summary[\'fillcolor\'] = cm[v]\n                    k = k[:1].upper() if len(k) > 4 else k\n                    name += \'{}{}.\'.format(k, v)\n                name += \')\'\n                summary[\'label\'] = name\n                return summary\n\n            @staticmethod\n            def get_config_grid(dict_space):\n                param_grid = {}\n                constants = {}\n                for k, v in dict_space.items():\n                    if isinstance(v, Categorical):\n                        param_grid[k] = v.data\n                    elif isinstance(v, Space):\n                        raise NotImplementedError\n                    else:\n                        constants[k] = v\n                from sklearn.model_selection import ParameterGrid\n                configs = list(ParameterGrid(param_grid))\n                for config in configs:\n                    config.update(constants)\n                return configs\n\n        return enas_unit\n    return registered_class\n\ndef enas_net(**kwvars):\n    def registered_class(Cls):\n        class ENAS_Net(Cls):\n            def __init__(self, *args, **kwargs):\n                kwvars.update(kwargs)\n                super().__init__(*args, **kwvars)\n                # \n                self._modules = {}\n                self._kwspaces = collections.OrderedDict()\n                for k, module in kwvars.items():\n                    if isinstance(module, (ENAS_Unit, ENAS_Sequential)):\n                        self._modules[k] = module\n                        if isinstance(module, ENAS_Unit):\n                            self._kwspaces[k] = module.kwspaces\n                        else:\n                            assert isinstance(module, ENAS_Sequential)\n                            for key, v in module.kwspaces.items():\n                                new_key = \'{}.{}\'.format(k, key)\n                                self._kwspaces[new_key] = v\n                self.latency_evaluated = False\n                self._avg_latency = 1\n\n            @property\n            def nparams(self):\n                nparams = 0\n                for k, op in self._modules.items():\n                    if isinstance(op, (ENAS_Unit, ENAS_Sequential)):\n                        nparams += op.nparams\n                    else:\n                        # standard block\n                        for _, v in op.collect_params().items():\n                            nparams += v.data().size\n                return nparams\n\n            @property\n            def nodeend(self):\n                return list(self._modules.keys())[-1]\n\n            @property\n            def nodehead(self):\n                return list(self._modules.keys())[0]\n\n            @property\n            def graph(self):\n                from graphviz import Digraph\n                e = Digraph(node_attr={\'color\': \'lightblue2\', \'style\': \'filled\', \'shape\': \'box\'})\n                pre_node = \'input\'\n                e.node(pre_node)\n                for k, op in self._modules.items():\n                    if hasattr(op, \'graph\'):\n                        e.subgraph(op.graph)\n                        e.edge(pre_node, op.nodehead)\n                        pre_node = op.nodeend\n                    else:\n                        if hasattr(op, \'node\'):\n                            if op.node is None: continue\n                            node_info = op.node\n                        else:\n                            node_info = {\'label\': op.__class__.__name__}\n                        e.node(k, **node_info)\n                        e.edge(pre_node, k)\n                        pre_node = k\n                return e\n\n\n            @property\n            def kwspaces(self):\n                return self._kwspaces\n\n            def sample(self, **configs):\n                striped_keys = [k.split(\'.\')[0] for k in configs.keys()]\n                for k in striped_keys:\n                    if isinstance(self._modules[k], ENAS_Unit):\n                        self._modules[k].sample(configs[k])\n                    else:\n                        sub_configs = _strip_config_space(configs, prefix=k)\n                        self._modules[k].sample(**sub_configs)\n\n            @property\n            def latency(self):\n                if not self.latency_evaluated:\n                    raise Exception(\'Latency is not evaluated yet.\')\n                return self._avg_latency\n\n            @property\n            def avg_latency(self):\n                if not self.latency_evaluated:\n                    raise Exception(\'Latency is not evaluated yet.\')\n                return self._avg_latency\n\n            def evaluate_latency(self, x):\n                import time\n                # evaluate submodule latency\n                for k, op in self._modules.items():\n                    if hasattr(op, \'evaluate_latency\'):\n                        x = op.evaluate_latency(x)\n                # calc avg_latency\n                avg_latency = 0.0\n                for k, op in self._modules.items():\n                    if hasattr(op, \'avg_latency\'):\n                        avg_latency += op.avg_latency\n                self._avg_latency = avg_latency\n                self.latency_evaluated = True\n\n        return ENAS_Net\n    return registered_class\n\nclass ENAS_Sequential(gluon.HybridBlock):\n    def __init__(self, *modules_list):\n        """"""\n        Args:\n            modules_list(list of ENAS_Unit)\n        """"""\n        super().__init__()\n        if len(modules_list) == 1 and isinstance(modules_list, (list, tuple)):\n            modules_list = modules_list[0]\n        self._modules = {}\n        self._blocks = gluon.nn.HybridSequential()\n        self._kwspaces = collections.OrderedDict()\n        for i, op in enumerate(modules_list):\n            self._modules[str(i)] = op\n            with self._blocks.name_scope():\n                self._blocks.add(op)\n            if hasattr(op, \'kwspaces\'):\n                self._kwspaces[str(i)] = op.kwspaces\n        self.latency_evaluated = False\n        self._avg_latency = 1\n\n    def __getitem__(self, index):\n        return self._blocks[index]\n\n    def hybrid_forward(self, F, x):\n        for k, op in self._modules.items():\n            x = op(x)\n        return x\n\n    def prune(self):\n        _modules_keys = self._modules.keys()\n        module_list = []\n        for k in _modules_keys:\n            if isinstance(self._modules[k], ENAS_Unit):\n                index = self._modules[k].index\n                op = self._modules[k].module_list[index]\n                module_list.append(op)\n            else:\n                module_list.append(self._modules[k])\n        return module_list\n\n    @property\n    def nodeend(self):\n        return list(self._modules.keys())[-1]\n\n    @property\n    def nodehead(self):\n        return list(self._modules.keys())[0]\n\n    @property\n    def graph(self):\n        from graphviz import Digraph\n        e = Digraph(node_attr={\'color\': \'lightblue2\', \'style\': \'filled\', \'shape\': \'box\'})\n        pre_node = None\n        for i, op in self._modules.items():\n            if hasattr(op, \'graph\'):\n                e.subgraph(op.graph)\n                if pre_node:\n                    e.edge(pre_node, op.nodehead)\n                pre_node = op.nodeend\n            else:\n                if hasattr(op, \'node\'):\n                    if op.node is None: continue\n                    node_info = op.node\n                else:\n                    node_info = {\'label\': op.__class__.__name__}\n                e.node(i, **node_info)\n                if pre_node:\n                    e.edge(pre_node, i)\n                pre_node = i\n        return e\n \n    @property\n    def kwspaces(self):\n        return self._kwspaces\n\n    @property\n    def nparams(self):\n        nparams = 0\n        for k, op in self._modules.items():\n            if isinstance(op, ENAS_Unit):\n                nparams += op.nparams\n            else:\n                # standard block\n                for _, v in op.collect_params().items():\n                    nparams += v.data().size\n        return nparams\n\n    @property\n    def latency(self):\n        if not self.latency_evaluated:\n            raise Exception(\'Latency is not evaluated yet.\')\n        latency = 0.0\n        for k, op in self._modules.items():\n            if hasattr(op, \'latency\'):\n                latency += op.latency\n        return latency\n\n    @property\n    def avg_latency(self):\n        if not self.latency_evaluated:\n            raise Exception(\'Latency is not evaluated yet.\')\n        return self._avg_latency\n\n    def evaluate_latency(self, x):\n        import time\n        # evaluate submodule latency\n        for k, op in self._modules.items():\n            if hasattr(op, \'evaluate_latency\'):\n                x = op.evaluate_latency(x)\n            else:\n                x = op(x)\n        # calc avg_latency\n        avg_latency = 0.0\n        for k, op in self._modules.items():\n            if hasattr(op, \'avg_latency\'):\n                avg_latency += op.avg_latency\n        self._avg_latency = avg_latency\n        self.latency_evaluated = True\n        return x\n\n    def sample(self, **configs):\n        for k, v in configs.items():\n            self._modules[k].sample(v)\n\n    def __repr__(self):\n        reprstr = self.__class__.__name__ + \'(\'\n        for i, op in self._modules.items():\n            reprstr += \'\\n\\t{}: {}\'.format(i, op)\n        reprstr += \')\\n\'\n        return reprstr\n\nclass Zero_Unit(gluon.HybridBlock):\n    def hybrid_forward(self, F, x):\n        return x\n    def __repr__(self):\n        return self.__class__.__name__\n\nclass ENAS_Unit(gluon.HybridBlock):\n    def __init__(self, *ops, with_zero=False):\n        super().__init__()\n        self.module_list = gluon.nn.HybridSequential()\n        self._latency = []\n        for op in ops:\n            self.module_list.add(op)\n            self._latency.append(1)\n        if with_zero:\n            self.module_list.add(Zero_Unit())\n            self._latency.append(1)\n        self.index = 0\n        self._latency_benchmark_times = 10\n        self._latency_warmup_times = 5\n        self.latency_evaluated = False\n\n    def hybrid_forward(self, F, x):\n        return self.module_list[self.index](x)\n\n    @property\n    def kwspaces(self):\n        return Categorical(*list(range(len(self.module_list))))\n\n    @property\n    def nparams(self):\n        nparams = 0\n        for _, v in self.module_list[self.index].collect_params().items():\n            nparams += v.data().size\n        return nparams\n\n    @property\n    def latency(self):\n        if not self.latency_evaluated:\n            raise Exception(\'Latency is not evaluated yet.\')\n        return self._latency[self.index]\n\n    @property\n    def avg_latency(self):\n        if not self.latency_evaluated:\n            raise Exception(\'Latency is not evaluated yet.\')\n        return sum(self._latency) / len(self._latency)\n\n    def evaluate_latency(self, x):\n        import time\n        for i, op in enumerate(self.module_list):\n            latency_i = 0\n            for j in range(self._latency_benchmark_times + self._latency_warmup_times):\n                start_time = time.time() * 1000 # ms\n                #print(\'op {}, shape x {}\'.format(op, x.shape))\n                y = op(x)\n                mx.nd.waitall()\n                end_time = time.time() * 1000 # ms\n                if j > self._latency_warmup_times:\n                    latency_i += end_time - start_time\n            self._latency[i] = latency_i / self._latency_benchmark_times\n        self.latency_evaluated = True\n        return y\n\n    def sample(self, ind):\n        self.index = ind\n\n    def __len__(self):\n        return len(self.module_list)\n\n    def __repr__(self):\n        reprstr = self.__class__.__name__ + \'(num of choices: {}), current architecture:\\n\\t {}\' \\\n            .format(len(self.module_list), self.module_list[self.index])\n        return reprstr\n'"
autogluon/contrib/enas/enas_net.py,0,"b'import mxnet as mx\nfrom mxnet import gluon\nfrom ...model_zoo.models.utils import *\nfrom ...model_zoo.models.mbconv import MBConvBlock\nfrom ...core.space import *\nfrom .enas import enas_unit, ENAS_Sequential\n\n__all__ = [\'ENAS_MbBlock\', \'ENAS_MBNet\']\n\n@enas_unit()\nclass ENAS_MbBlock(MBConvBlock):\n    pass\n\nclass ENAS_MBNet(gluon.HybridBlock):\n    def __init__(self, blocks_args=[], dropout_rate=0.2, num_classes=1000, input_size=224,\n                 activation=\'swish\', blocks=None, **kwargs):\n        r""""""ENAS model with MobileNet search space\n\n        Args:\n            blocks_args (list of autogluon.Dict)\n        """"""\n        super(ENAS_MBNet, self).__init__(**kwargs)\n        if len(blocks_args)==0 and blocks is None:\n            blocks_args = get_enas_blockargs()\n        #assert isinstance(blocks_args, (tuple, list)), \'blocks_args should be a list\'\n        self.input_size = input_size\n        with self.name_scope():\n            self._features = gluon.nn.HybridSequential()\n            with self._features.name_scope():\n                # stem conv\n                out_channels = 32\n                _add_conv(self._features, out_channels, kernel=3,\n                    stride=2, activation=activation, batchnorm=True,\n                    input_size=input_size, in_channels=3)\n\n            input_size = _update_input_size(input_size, 2)\n            _blocks = []\n            for block_arg in blocks_args:\n                block_arg.update(in_channels=out_channels, input_size=input_size, activation=activation)\n                out_channels=block_arg.channels\n                _blocks.append(ENAS_MbBlock(**block_arg))\n                input_size = _update_input_size(input_size, block_arg.stride)\n                if block_arg.num_repeat > 1:\n                    block_arg.update(in_channels=out_channels, stride=1,\n                                     input_size=input_size)\n\n                for _ in range(block_arg.num_repeat - 1):\n                    _blocks.append(ENAS_MbBlock(**block_arg))\n\n            if blocks is not None:\n                self._blocks = ENAS_Sequential(blocks)\n            else:\n                self._blocks = ENAS_Sequential(_blocks)\n            # Head\n            self._conv_head = gluon.nn.HybridSequential()\n            out_channels = 320\n            hidden_channels = 1280\n            with self._conv_head.name_scope():\n                _add_conv(self._conv_head, hidden_channels, activation=activation,\n                          batchnorm=True, input_size=input_size,\n                          in_channels=out_channels)\n            out_channels = hidden_channels\n            # Final linear layer\n            self._dropout = dropout_rate\n            self._fc = gluon.nn.Dense(num_classes, use_bias=True, in_units=out_channels)\n\n    def sample(self, **config):\n        self._blocks.sample(**config)\n\n    @property\n    def kwspaces(self):\n        return self._blocks.kwspaces\n\n    @property\n    def latency(self):\n        return self._blocks.latency\n\n    @property\n    def avg_latency(self):\n        return self._blocks.avg_latency\n\n    @property\n    def nparams(self):\n        return self._blocks.nparams\n\n    def evaluate_latency(self, x):\n        self._blocks.evaluate_latency(x)\n\n    def hybrid_forward(self, F, x):\n        x = self._features(x)\n        x = self._blocks(x)\n        x = self._conv_head(x)\n        x = F.contrib.AdaptiveAvgPooling2D(x, 1)\n        if self._dropout:\n            x = F.Dropout(x, self._dropout)\n        x = self._fc(x)\n        return x\n\ndef get_enas_blockargs():\n    """""" Creates a predefined efficientnet model, which searched by original paper. """"""\n    blocks_args = [\n        Dict(kernel=3, num_repeat=1, channels=16, expand_ratio=1, stride=1, se_ratio=0.25, in_channels=32),\n        Dict(kernel=3, num_repeat=1, channels=16, expand_ratio=1, stride=1, se_ratio=0.25, in_channels=16, with_zero=True),\n        Dict(kernel=Categorical(3, 5, 7), num_repeat=1, channels=24, expand_ratio=Categorical(3, 6), stride=2, se_ratio=0.25, in_channels=16),\n        Dict(kernel=Categorical(3, 5, 7), num_repeat=3, channels=24, expand_ratio=Categorical(3, 6), stride=1, se_ratio=0.25, in_channels=24, with_zero=True),\n        Dict(kernel=Categorical(3, 5, 7), num_repeat=1, channels=40, expand_ratio=Categorical(3, 6), stride=2, se_ratio=0.25, in_channels=24),\n        Dict(kernel=Categorical(3, 5, 7), num_repeat=3, channels=40, expand_ratio=Categorical(3, 6), stride=1, se_ratio=0.25, in_channels=40, with_zero=True),\n        Dict(kernel=Categorical(3, 5, 7), num_repeat=1, channels=80, expand_ratio=Categorical(3, 6), stride=2, se_ratio=0.25, in_channels=40),\n        Dict(kernel=Categorical(3, 5, 7), num_repeat=4, channels=80, expand_ratio=Categorical(3, 6), stride=1, se_ratio=0.25, in_channels=80, with_zero=True),\n        Dict(kernel=Categorical(3, 5, 7), num_repeat=1, channels=112, expand_ratio=Categorical(3, 6), stride=1, se_ratio=0.25, in_channels=80),\n        Dict(kernel=Categorical(3, 5, 7), num_repeat=4, channels=112, expand_ratio=Categorical(3, 6), stride=1, se_ratio=0.25, in_channels=112, with_zero=True),\n        Dict(kernel=Categorical(3, 5, 7), num_repeat=1, channels=192, expand_ratio=Categorical(3, 6), stride=2, se_ratio=0.25, in_channels=112),\n        Dict(kernel=Categorical(3, 5, 7), num_repeat=5, channels=192, expand_ratio=Categorical(3, 6), stride=1, se_ratio=0.25, in_channels=192, with_zero=True),\n        Dict(kernel=3, num_repeat=1, channels=320, expand_ratio=6, stride=1, se_ratio=0.25, in_channels=192),\n    ]\n    return blocks_args\n'"
autogluon/contrib/enas/enas_scheduler.py,0,"b'import os\nimport pickle\nimport logging\nfrom collections import OrderedDict\nfrom multiprocessing.pool import ThreadPool\n\nimport mxnet as mx\n\nfrom ...searcher import RLSearcher\nfrom ...scheduler.resource import get_gpu_count, get_cpu_count\nfrom ...task.image_classification.dataset import get_built_in_dataset\nfrom ...task.image_classification.utils import *\nfrom ...utils import (mkdir, save, load, update_params, collect_params, DataLoader, tqdm, in_ipynb)\nfrom .enas_utils import *\n\n__all__ = [\'ENAS_Scheduler\']\n\nlogger = logging.getLogger(__name__)\n\nIMAGENET_TRAINING_SAMPLES = 1281167\n\nclass ENAS_Scheduler(object):\n    """"""ENAS Scheduler, which automatically creates LSTM controller based on the search spaces.\n    """"""\n    def __init__(self, supernet, train_set=\'imagenet\', val_set=None,\n                 train_fn=default_train_fn, eval_fn=default_val_fn,\n                 train_args={}, val_args={}, reward_fn= default_reward_fn,\n                 num_gpus=0, num_cpus=4,\n                 batch_size=256, epochs=120, warmup_epochs=5,\n                 controller_lr=1e-3, controller_type=\'lstm\',\n                 controller_batch_size=10, ema_baseline_decay=0.95,\n                 update_arch_frequency=20, checkname=\'./enas/checkpoint.ag\',\n                 plot_frequency=0, **kwargs):\n        num_cpus = get_cpu_count() if num_cpus > get_cpu_count() else num_cpus\n        num_gpus = get_gpu_count() if num_gpus > get_gpu_count() else num_gpus\n        self.supernet = supernet\n        self.train_fn = train_fn\n        self.eval_fn = eval_fn\n        self.reward_fn = reward_fn\n        self.checkname = checkname\n        self.plot_frequency = plot_frequency\n        self.epochs = epochs\n        self.warmup_epochs = warmup_epochs\n        self.controller_batch_size = controller_batch_size\n        kwspaces = self.supernet.kwspaces\n\n        self.initialize_miscs(train_set, val_set, batch_size, num_cpus, num_gpus,\n                              train_args, val_args)\n\n        # create RL searcher/controller\n        self.baseline = None\n        self.ema_decay = ema_baseline_decay\n        self.searcher = RLSearcher(\n            kwspaces, controller_type=controller_type, prefetch=4,\n            num_workers=4)\n        # controller setup\n        self.controller = self.searcher.controller\n        self.controller_optimizer = mx.gluon.Trainer(\n                self.controller.collect_params(), \'adam\',\n                optimizer_params={\'learning_rate\': controller_lr})\n        self.update_arch_frequency = update_arch_frequency\n        self.val_acc = 0\n        # async controller sample\n        self._worker_pool = ThreadPool(2)\n        self._data_buffer = {}\n        self._rcvd_idx = 0\n        self._sent_idx = 0\n        self._timeout = 20\n        # logging history\n        self.training_history = []\n        self._prefetch_controller()\n\n    def initialize_miscs(self, train_set, val_set, batch_size, num_cpus, num_gpus,\n                         train_args, val_args):\n        """"""Initialize framework related miscs, such as train/val data and train/val\n        function arguments.\n        """"""\n        ctx = [mx.gpu(i) for i in range(num_gpus)] if num_gpus > 0 else [mx.cpu(0)]\n        self.supernet.collect_params().reset_ctx(ctx)\n        self.supernet.hybridize()\n        dataset_name = train_set\n\n        if isinstance(train_set, str):\n            train_set = get_built_in_dataset(dataset_name, train=True, batch_size=batch_size,\n                                             num_workers=num_cpus, shuffle=True)\n            val_set = get_built_in_dataset(dataset_name, train=False, batch_size=batch_size,\n                                           num_workers=num_cpus, shuffle=True)\n        if isinstance(train_set, gluon.data.Dataset):\n            self.train_data = DataLoader(\n                    train_set, batch_size=batch_size, shuffle=True,\n                    last_batch=""discard"", num_workers=num_cpus)\n            # very important, make shuffle for training contoller\n            self.val_data = DataLoader(\n                    val_set, batch_size=batch_size, shuffle=True,\n                    num_workers=num_cpus, prefetch=0, sample_times=self.controller_batch_size)\n        else:\n            self.train_data = train_set\n            self.val_data = val_set\n        iters_per_epoch = len(self.train_data) if hasattr(self.train_data, \'__len__\') else \\\n                IMAGENET_TRAINING_SAMPLES // batch_size\n        self.train_args = init_default_train_args(batch_size, self.supernet, self.epochs, iters_per_epoch) \\\n                if len(train_args) == 0 else train_args\n        self.val_args = val_args\n        self.val_args[\'ctx\'] = ctx\n        self.val_args[\'batch_fn\'] = imagenet_batch_fn if dataset_name == \'imagenet\' else default_batch_fn\n        self.train_args[\'ctx\'] = ctx\n        self.train_args[\'batch_fn\'] = imagenet_batch_fn if dataset_name == \'imagenet\' else default_batch_fn\n        self.ctx = ctx\n\n    def run(self):\n        tq = tqdm(range(self.epochs))\n        for epoch in tq:\n            # for recordio data\n            if hasattr(self.train_data, \'reset\'): self.train_data.reset()\n            tbar = tqdm(self.train_data)\n            idx = 0\n            for batch in tbar:\n                # sample network configuration\n                config = self.controller.pre_sample()[0]\n                self.supernet.sample(**config)\n                # self.train_fn(self.supernet, batch, **self.train_args)\n                self.train_fn(epoch, self.epochs, self.supernet, batch, **self.train_args)\n                mx.nd.waitall()\n                if epoch >= self.warmup_epochs and (idx % self.update_arch_frequency) == 0:\n                    self.train_controller()\n                if self.plot_frequency > 0 and idx % self.plot_frequency == 0 and in_ipynb():\n                    graph = self.supernet.graph\n                    graph.attr(rankdir=\'LR\', size=\'8,3\')\n                    tbar.set_svg(graph._repr_svg_())\n                if self.baseline:\n                    tbar.set_description(\'avg reward: {:.2f}\'.format(self.baseline))\n                idx += 1\n            self.validation()\n            self.save()\n            msg = \'epoch {}, val_acc: {:.2f}\'.format(epoch, self.val_acc)\n            if self.baseline:\n                msg += \', avg reward: {:.2f}\'.format(self.baseline)\n            tq.set_description(msg)\n\n    def validation(self):\n        if hasattr(self.val_data, \'reset\'): self.val_data.reset()\n        # data iter, avoid memory leak\n        it = iter(self.val_data)\n        if hasattr(it, \'reset_sample_times\'): it.reset_sample_times()\n        tbar = tqdm(it)\n        # update network arc\n        config = self.controller.inference()\n        self.supernet.sample(**config)\n        metric = mx.metric.Accuracy()\n        for batch in tbar:\n            self.eval_fn(self.supernet, batch, metric=metric, **self.val_args)\n            reward = metric.get()[1]\n            tbar.set_description(\'Val Acc: {}\'.format(reward))\n\n        self.val_acc = reward\n        self.training_history.append(reward)\n\n    def _sample_controller(self):\n        assert self._rcvd_idx < self._sent_idx, ""rcvd_idx must be smaller than sent_idx""\n        try:\n            ret = self._data_buffer.pop(self._rcvd_idx)\n            self._rcvd_idx += 1\n            return  ret.get(timeout=self._timeout)\n        except Exception:\n            self._worker_pool.terminate()\n            raise\n\n    def _prefetch_controller(self):\n        async_ret = self._worker_pool.apply_async(self._async_sample, ())\n        self._data_buffer[self._sent_idx] = async_ret\n        self._sent_idx += 1\n\n    def _async_sample(self):\n        with mx.autograd.record():\n            # sample controller_batch_size number of configurations\n            configs, log_probs, entropies = self.controller.sample(batch_size=self.controller_batch_size,\n                                                                   with_details=True)\n        return configs, log_probs, entropies\n\n    def train_controller(self):\n        """"""Run multiple number of trials\n        """"""\n        decay = self.ema_decay\n        if hasattr(self.val_data, \'reset\'): self.val_data.reset()\n        # update \n        metric = mx.metric.Accuracy()\n        with mx.autograd.record():\n            # sample controller_batch_size number of configurations\n            configs, log_probs, entropies = self._sample_controller()\n            for i, batch in enumerate(self.val_data):\n                if i >= self.controller_batch_size: break\n                self.supernet.sample(**configs[i])\n                # schedule the training tasks and gather the reward\n                metric.reset()\n                self.eval_fn(self.supernet, batch, metric=metric, **self.val_args)\n                reward = metric.get()[1]\n                reward = self.reward_fn(reward, self.supernet)\n                self.baseline = reward if not self.baseline else self.baseline\n                # substract baseline\n                avg_rewards = mx.nd.array([reward - self.baseline],\n                                          ctx=self.controller.context)\n                # EMA baseline\n                self.baseline = decay * self.baseline + (1 - decay) * reward\n                # negative policy gradient\n                log_prob = log_probs[i]\n                log_prob = log_prob.sum()\n                loss = - log_prob * avg_rewards\n                loss = loss.sum()\n\n        # update\n        loss.backward()\n        self.controller_optimizer.step(self.controller_batch_size)\n        self._prefetch_controller()\n\n    def load(self, checkname=None):\n        checkname = checkname if checkname else self.checkname\n        state_dict = load(checkname)\n        self.load_state_dict(state_dict)\n\n    def save(self, checkname=None):\n        checkname = checkname if checkname else self.checkname\n        mkdir(os.path.dirname(checkname))\n        save(self.state_dict(), checkname)\n\n    def state_dict(self, destination=None):\n        if destination is None:\n            destination = OrderedDict()\n            destination._metadata = OrderedDict()\n        destination[\'supernet_params\'] = collect_params(self.supernet)\n        destination[\'controller_params\'] = collect_params(self.controller)\n        destination[\'training_history\'] = self.training_history\n        return destination\n\n    def load_state_dict(self, state_dict):\n        update_params(self.supernet, state_dict[\'supernet_params\'], ctx=self.ctx)\n        update_params(self.controller, state_dict[\'controller_params\'], ctx=self.controller.context)\n        self.training_history = state_dict[\'training_history\']\n'"
autogluon/contrib/enas/enas_utils.py,0,"b""import mxnet as mx\nfrom mxnet import gluon\nimport gluoncv as gcv\n\nfrom ...scheduler.resource import get_gpu_count\n\ndef default_reward_fn(metric, net):\n    reward = metric * ((net.avg_latency / net.latency) ** 0.07)\n    return reward\n\ndef init_default_train_args(batch_size, net, epochs, iters_per_epoch):\n    train_args = {}\n    base_lr = 0.1 * batch_size / 256\n    lr_scheduler = gcv.utils.LRScheduler('cosine', base_lr=base_lr, target_lr=0.0001,\n                                         nepochs=epochs, iters_per_epoch=iters_per_epoch)\n    optimizer_params = {'wd': 1e-4, 'momentum': 0.9, 'lr_scheduler': lr_scheduler}\n    train_args['trainer'] = gluon.Trainer(net.collect_params(), 'sgd', optimizer_params)\n    train_args['batch_size'] = batch_size\n    train_args['criterion'] = gluon.loss.SoftmaxCrossEntropyLoss()\n    return train_args\n"""
autogluon/model_zoo/models/__init__.py,0,b'from .mbconv import *\nfrom .efficientnet import *\n'
autogluon/model_zoo/models/efficientnet.py,0,"b'import os\nimport mxnet as mx\nfrom mxnet.gluon import nn\nfrom mxnet.gluon.block import HybridBlock\n\nfrom ...utils import EasyDict\nfrom .mbconv import *\nfrom .utils import *\n\n__all__ = [\'EfficientNet\', \'get_efficientnet_blockargs\', \'get_efficientnet\',\n           \'get_efficientnet_b0\', \'get_efficientnet_b1\', \'get_efficientnet_b2\',\n           \'get_efficientnet_b3\', \'get_efficientnet_b4\', \'get_efficientnet_b5\',\n           \'get_efficientnet_b6\', \'get_efficientnet_b7\']\n\nclass EfficientNet(HybridBlock):\n    def __init__(self, blocks_args=None, dropout_rate=0.2, num_classes=1000, width_coefficient=1.0,\n                 depth_coefficient=1.0, depth_divisor=8, min_depth=None, drop_connect_rate=0.2,\n                 input_size=224, **kwargs):\n        r""""""EfficientNet model from the\n            `""EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks""\n            <https://arxiv.org/abs/1905.11946>`_ paper.\n\n            Parameters\n            ----------\n            blocks_args: nametuple, it concludes the hyperparameters of the MBConvBlock block.\n            dropout_rate: float, rate of hidden units to drop.\n            num_classes: int, number of output classes.\n            width_coefficient:float, coefficient of the filters used for\n            expanding or reducing the channels.\n            depth_coefficient:float, it is used for repeat the EfficientNet Blocks.\n            depth_divisor:int , it is used for reducing the number of filters.\n            min_depth: int, used for deciding the minimum depth of the filters.\n            drop_connect_rate: used for dropout.\n        """"""\n        super(EfficientNet, self).__init__(**kwargs)\n        if blocks_args is None:\n            blocks_args = get_efficientnet_blockargs()\n        else:\n            assert isinstance(blocks_args, list), \'blocks_args should be a list\'\n            assert len(blocks_args) > 0, \'block args must be greater than 0\'\n        self.input_size = input_size\n        with self.name_scope():\n            self.features = nn.HybridSequential()\n            with self.features.name_scope():\n                # stem conv\n                out_channels = round_filters(32, width_coefficient,\n                                             depth_divisor, min_depth)\n                _add_conv(self.features, out_channels, kernel=3,\n                    stride=2, activation=\'swish\', batchnorm=True, input_size=input_size)\n\n            input_size = _update_input_size(input_size, 2)\n\n            self._blocks = nn.HybridSequential()\n            with self._blocks.name_scope():\n                for block_arg in blocks_args:\n                    block_arg.update(\n                        in_channels=out_channels,\n                        channels=round_filters(block_arg.channels, width_coefficient,\n                                               depth_divisor, min_depth),\n                        num_repeat=round_repeats(\n                            block_arg.num_repeat, depth_coefficient),\n                            input_size=input_size)\n                    self._blocks.add(MBConvBlock(**block_arg))\n\n                    out_channels=block_arg.channels\n                    input_size = _update_input_size(input_size, block_arg.stride)\n\n                    if block_arg.num_repeat > 1:\n                        block_arg.update(\n                            in_channels=out_channels, stride=1,\n                            input_size=input_size)\n\n                    for _ in range(block_arg.num_repeat - 1):\n                        self._blocks.add(MBConvBlock(**block_arg))\n\n            # Head\n            out_channels = round_filters(1280, width_coefficient,\n                                         depth_divisor, min_depth)\n            self._conv_head = nn.HybridSequential()\n            with self._conv_head.name_scope():\n                _add_conv(self._conv_head, out_channels,\n                          activation=\'swish\', batchnorm=True, input_size=input_size)\n            # Final linear layer\n            self._dropout = dropout_rate\n            self._fc = nn.Dense(num_classes, use_bias=True)\n\n    def hybrid_forward(self, F, x):\n        x = self.features(x)\n        x = self._blocks(x)\n        x = self._conv_head(x)\n        x = F.contrib.AdaptiveAvgPooling2D(x, 1)\n        if self._dropout:\n            x = F.Dropout(x, self._dropout)\n        x = self._fc(x)\n        return x\n\n\ndef get_efficientnet_blockargs():\n    """""" Creates a predefined efficientnet model, which searched by original paper. """"""\n    blocks_args = [\n        EasyDict(kernel=3, num_repeat=1, channels=16, expand_ratio=1, stride=1, se_ratio=0.25, in_channels=32),\n        EasyDict(kernel=3, num_repeat=2, channels=24, expand_ratio=6, stride=2, se_ratio=0.25, in_channels=16),\n        EasyDict(kernel=5, num_repeat=2, channels=40, expand_ratio=6, stride=2, se_ratio=0.25, in_channels=24),\n        EasyDict(kernel=3, num_repeat=3, channels=80, expand_ratio=6, stride=2, se_ratio=0.25, in_channels=40),\n        EasyDict(kernel=5, num_repeat=3, channels=112, expand_ratio=6, stride=1, se_ratio=0.25, in_channels=80),\n        EasyDict(kernel=5, num_repeat=4, channels=192, expand_ratio=6, stride=2, se_ratio=0.25, in_channels=112),\n        EasyDict(kernel=3, num_repeat=1, channels=320, expand_ratio=6, stride=1, se_ratio=0.25, in_channels=192),\n    ]\n    return blocks_args\n\ndef get_efficientnet(dropout_rate=None, num_classes=None, width_coefficient=None, depth_coefficient=None,\n                     depth_divisor=None, min_depth=None, drop_connect_rate=None, input_size=224, **kwargs):\n\n    blocks_args = get_efficientnet_blockargs()\n    model = EfficientNet(blocks_args, dropout_rate, num_classes, width_coefficient,\n                         depth_coefficient, depth_divisor, min_depth, drop_connect_rate,\n                         input_size, **kwargs)\n    return model\n\ndef get_efficientnet_b0(pretrained=False, dropout_rate=0.2, classes=1000, width_coefficient=1.0, depth_coefficient=1.0,\n                        depth_divisor=8, min_depth=None, drop_connect_rate=0.2, input_size=224, ctx=mx.cpu(),\n                        root=os.path.join(\'~\', \'.autogluon\', \'models\')):\n    model = get_efficientnet(dropout_rate, classes, width_coefficient, depth_coefficient,\n                             depth_divisor, min_depth, drop_connect_rate,\n                             input_size=input_size)\n    if pretrained:\n        from ..model_store import get_model_file\n        model.load_parameters(get_model_file(\'efficientnet_b0\', root=root), ctx=ctx)\n    else:\n        model.collect_params().initialize(ctx=ctx)\n    return model\n\ndef get_efficientnet_b1(pretrained=False, dropout_rate=0.2, classes=1000, width_coefficient=1.0, depth_coefficient=1.1,\n                        depth_divisor=8, min_depth=None, drop_connect_rate=0.2, input_size=240, ctx=mx.cpu(),\n                        root=os.path.join(\'~\', \'.autogluon\', \'models\')):\n    model = get_efficientnet(dropout_rate, classes, width_coefficient, depth_coefficient,\n                             depth_divisor, min_depth, drop_connect_rate,\n                             input_size=input_size)\n    if pretrained:\n        from ..model_store import get_model_file\n        model.load_parameters(get_model_file(\'efficientnet_b1\', root=root), ctx=ctx)\n    else:\n        model.collect_params().initialize(ctx=ctx)\n    return model\n\ndef get_efficientnet_b2(pretrained=False, dropout_rate=0.2, classes=1000, width_coefficient=1.1, depth_coefficient=1.2,\n                        depth_divisor=8, min_depth=None, drop_connect_rate=0.2, input_size=260, ctx=mx.cpu(),\n                        root=os.path.join(\'~\', \'.autogluon\', \'models\')):\n    model = get_efficientnet(dropout_rate, classes, width_coefficient, depth_coefficient,\n                             depth_divisor, min_depth, drop_connect_rate,\n                             input_size=input_size)\n    if pretrained:\n        from ..model_store import get_model_file\n        model.load_parameters(get_model_file(\'efficientnet_b2\', root=root), ctx=ctx)\n    else:\n        model.collect_params().initialize(ctx=ctx)\n    return model\n\ndef get_efficientnet_b3(pretrained=False, dropout_rate=0.2, classes=1000, width_coefficient=1.2, depth_coefficient=1.4,\n                        depth_divisor=8, min_depth=None, drop_connect_rate=0.2, input_size=300, ctx=mx.cpu(),\n                        root=os.path.join(\'~\', \'.autogluon\', \'models\')):\n    model = get_efficientnet(dropout_rate, classes, width_coefficient, depth_coefficient,\n                             depth_divisor, min_depth, drop_connect_rate,\n                             input_size=input_size)\n    if pretrained:\n        from ..model_store import get_model_file\n        model.load_parameters(get_model_file(\'efficientnet_b3\', root=root), ctx=ctx)\n    else:\n        model.collect_params().initialize(ctx=ctx)\n    return model\n\ndef get_efficientnet_b4(pretrained=False, dropout_rate=0.2, classes=1000, width_coefficient=1.4, depth_coefficient=1.8,\n                        depth_divisor=8, min_depth=None, drop_connect_rate=0.2, input_size=380, ctx=mx.cpu(),\n                        root=os.path.join(\'~\', \'.autogluon\', \'models\')):\n    model = get_efficientnet(dropout_rate, classes, width_coefficient, depth_coefficient,\n                             depth_divisor, min_depth, drop_connect_rate,\n                             input_size=input_size)\n    if pretrained:\n        from ..model_store import get_model_file\n        model.load_parameters(get_model_file(\'efficientnet_b4\', root=root), ctx=ctx)\n    else:\n        model.collect_params().initialize(ctx=ctx)\n    return model\n\ndef get_efficientnet_b5(pretrained=False, dropout_rate=0.2, classes=1000, width_coefficient=1.6, depth_coefficient=2.2,\n                        depth_divisor=8, min_depth=None, drop_connect_rate=0.2, input_size=456, ctx=mx.cpu(),\n                        root=os.path.join(\'~\', \'.autogluon\', \'models\')):\n    model = get_efficientnet(dropout_rate, classes, width_coefficient, depth_coefficient,\n                             depth_divisor, min_depth, drop_connect_rate,\n                             input_size=input_size)\n    if pretrained:\n        from ..model_store import get_model_file\n        model.load_parameters(get_model_file(\'efficientnet_b5\', root=root), ctx=ctx)\n    else:\n        model.collect_params().initialize(ctx=ctx)\n    return model\n\ndef get_efficientnet_b6(pretrained=False, dropout_rate=0.2, classes=1000, width_coefficient=1.8, depth_coefficient=2.6,\n                        depth_divisor=8, min_depth=None, drop_connect_rate=0.2, input_size=528, ctx=mx.cpu(),\n                        root=os.path.join(\'~\', \'.autogluon\', \'models\')):\n    model = get_efficientnet(dropout_rate, classes, width_coefficient, depth_coefficient,\n                             depth_divisor, min_depth, drop_connect_rate,\n                             input_size=input_size)\n    if pretrained:\n        from ..model_store import get_model_file\n        model.load_parameters(get_model_file(\'efficientnet_b6\', root=root), ctx=ctx)\n    else:\n        model.collect_params().initialize(ctx=ctx)\n    return model\n\ndef get_efficientnet_b7(pretrained=False, dropout_rate=0.2, classes=1000, width_coefficient=2.0, depth_coefficient=3.1,\n                        depth_divisor=8, min_depth=None, drop_connect_rate=0.2, input_size=600, ctx=mx.cpu(),\n                        root=os.path.join(\'~\', \'.autogluon\', \'models\')):\n    model = get_efficientnet(dropout_rate, classes, width_coefficient, depth_coefficient,\n                             depth_divisor, min_depth, drop_connect_rate,\n                             input_size=input_size)\n    if pretrained:\n        from ..model_store import get_model_file\n        model.load_parameters(get_model_file(\'efficientnet_b7\', root=root), ctx=ctx)\n    else:\n        model.collect_params().initialize(ctx=ctx)\n    return model\n'"
autogluon/model_zoo/models/mbconv.py,0,"b'from mxnet.gluon.block import HybridBlock\nfrom mxnet.gluon import nn\n\nfrom .utils import *\n\n__all__ = [\'MBConvBlock\']\n\nclass MBConvBlock(HybridBlock):\n    def __init__(self, in_channels, channels, expand_ratio, kernel, stride=1, se_ratio=0.25,\n                 drop_connect_rate=0, input_size=None, num_repeat=None, activation=\'swish\', **kwargs):\n        r""""""MobileNet V3 Block\n            Parameters\n            ----------\n            int_channels: int, input channels.\n            channels: int, output channels.\n            t: int, the expand ratio used for increasing channels.\n            kernel: int, filter size.\n            stride: int, stride of the convolution.\n            se_ratio: float, ratio of the squeeze layer and excitation layer.\n            drop_connect_rate: float, drop rate of drop out.\n        """"""\n        super(MBConvBlock, self).__init__(**kwargs)\n        assert input_size\n        if isinstance(input_size, int):\n            input_size = (input_size,) * 2\n        self.use_shortcut = stride == 1 and in_channels == channels\n        self.se_ratio = se_ratio\n        self.drop_connect_rate = drop_connect_rate\n\n        self.depth_conv = nn.HybridSequential(prefix=\'depth_conv_\')\n        if expand_ratio != 1:\n            _add_conv(self.depth_conv, in_channels*expand_ratio,\n                      activation=activation, batchnorm=True, input_size=input_size,\n                      in_channels=in_channels)\n        _add_conv(self.depth_conv, in_channels*expand_ratio,\n            kernel=kernel, stride=stride,\n            num_group=in_channels*expand_ratio,\n            activation=activation, batchnorm=True, input_size=input_size,\n            in_channels=in_channels*expand_ratio)\n        input_size = _update_input_size(input_size, stride)\n\n        if se_ratio:\n            num_squeezed_channels = max(1, int(in_channels*se_ratio))\n            self.se_module = nn.HybridSequential(prefix=\'se_module_\')\n            self.se_module.add(nn.Conv2D(num_squeezed_channels, 1, 1, 0, use_bias=True,\n                                         in_channels=in_channels*expand_ratio))\n            if activation == \'relu\':\n                self.se_module.add(nn.Activation(\'relu\'))\n            elif activation == \'swish\':\n                self.se_module.add(Swish())\n            self.se_module.add(nn.Conv2D(in_channels*expand_ratio, 1, 1, 0, use_bias=True,\n                                         in_channels=num_squeezed_channels))\n        self.project_conv = nn.HybridSequential(prefix=\'preject_conv_\')\n        _add_conv(self.project_conv, channels,\n                  activation=None, batchnorm=True, input_size=input_size,\n                  in_channels=in_channels*expand_ratio)\n        if drop_connect_rate:\n            self.drop_out = nn.Dropout(drop_connect_rate)\n\n    def hybrid_forward(self, F, inputs):\n        x = inputs\n        x = self.depth_conv(x)\n        out = x\n        if self.se_ratio:\n            out = F.contrib.AdaptiveAvgPooling2D(x, 1)\n            out = self.se_module(out)\n            out = F.broadcast_mul(F.sigmoid(out), x)\n        out = self.project_conv(out)\n        if self.use_shortcut:\n            if self.drop_connect_rate:\n                out = self.drop_out(out)\n            out = F.elemwise_add(out, inputs)\n        return out\n\n'"
autogluon/model_zoo/models/standford_dog_models.py,0,"b'""""""Pretrained models for Standford Dogs dataset""""""\nimport os\nimport mxnet as mx\nimport gluoncv as gcv\nfrom ..model_store import get_model_file\n\n__all__ = [\'standford_dog_resnet152_v1\', \'standford_dog_resnext101_64x4d\']\n\ndef standford_dog_resnet152_v1(pretrained=False, root=os.path.join(\'~\', \'.autogluon\', \'models\'),\n                               ctx=mx.cpu(0), **kwargs):\n    net = gcv.model_zoo.resnet152_v1(classes=120, **kwargs)\n    if pretrained:\n        net.load_parameters(get_model_file(\'standford_dog_resnet152_v1\',\n                                           root=root), ctx=ctx)\n    return net\n\ndef standford_dog_resnext101_64x4d(pretrained=False, root=os.path.join(\'~\', \'.autogluon\', \'models\'),\n                                   ctx=mx.cpu(0), **kwargs):\n    net = gcv.model_zoo.resnext.resnext101_64x4d(classes=120, **kwargs)\n    if pretrained:\n        net.load_parameters(get_model_file(\'standford_dog_resnext101_64x4d\',\n                                           root=root), ctx=ctx)\n    return net\n'"
autogluon/model_zoo/models/utils.py,0,"b'import math\nimport mxnet as mx\nfrom mxnet.gluon import nn\nfrom mxnet.gluon.block import HybridBlock\n\n__all__ = [\'round_repeats\', \'round_filters\', \'SamePadding\', \'Swish\',\n           \'_add_conv\', \'_update_input_size\']\n\ndef round_repeats(repeats, depth_coefficient=None):\n    """""" Round number of filters based on depth multiplier. """"""\n    multiplier = depth_coefficient\n    if not multiplier:\n        return repeats\n    return int(math.ceil(multiplier * repeats))\n\ndef round_filters(filters, width_coefficient=None, depth_divisor=None, min_depth=None):\n    """""" Calculate and round number of filters based on depth multiplier. """"""\n    multiplier = width_coefficient\n    if not multiplier:\n        return filters\n    divisor = depth_divisor\n    min_depth = min_depth\n    filters *= multiplier\n    min_depth = min_depth or divisor\n    new_filters = max(\n        min_depth, int(\n            filters + divisor / 2) // divisor * divisor)\n    if new_filters < 0.9 * filters:  # prevent rounding by more than 10%\n        new_filters += divisor\n    return int(new_filters)\n\nclass SamePadding(HybridBlock):\n    def __init__(self, kernel_size, stride, dilation, input_size, **kwargs):\n        super(SamePadding, self).__init__(**kwargs)\n        if isinstance(kernel_size, int):\n            kernel_size = (kernel_size,) * 2\n        if isinstance(stride, int):\n            stride = (stride,) * 2\n        if isinstance(input_size, int):\n            input_size = (input_size,) * 2\n\n        self.input_size = input_size\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.dilation = dilation\n\n        ih, iw = self.input_size\n        kh, kw = self.kernel_size\n        sh, sw = self.stride\n        oh, ow = math.ceil(ih / sh), math.ceil(iw / sw)\n\n        self.pad_h = max((oh - 1) * self.stride[0] +\n                    (kh - 1) * self.dilation[0] + 1 - ih, 0)\n        self.pad_w = max((ow - 1) * self.stride[1] +\n                    (kw - 1) * self.dilation[1] + 1 - iw, 0)\n\n    def hybrid_forward(self, F, x):\n        if self.pad_h > 0 or self.pad_w > 0:\n            x = F.pad(x, mode=\'constant\', pad_width=(0, 0, 0, 0, self.pad_h//2, self.pad_h - self.pad_h//2,\n                                                     self.pad_w//2, self.pad_w -self.pad_w//2))\n            return x\n        return x\n\n    def __repr__(self):\n        s = \'{}({}, {}, {}, {})\'\n        return s.format(self.__class__.__name__,\n                        self.pad_h//2, self.pad_h - self.pad_h//2,\n                        self.pad_w//2, self.pad_w -self.pad_w//2)\n    \n\n#class swish(mx.autograd.Function):\n#    def __init__(self, beta):\n#        super().__init__()\n#        self.beta = beta\n#\n#    def forward(self, x):\n#        y = x * self.beta\n#        y1 = y.sigmoid()\n#        y = x * y1\n#        self.save_for_backward(x, y1)\n#        return y\n#\n#    def backward(self, dy):\n#        x, y1 = self.saved_tensors\n#        return dy * (- self.beta * x * y1 * (1 - y1) + y1)\n#\n#    def __repr__(self):\n#        return \'{} (beta={})\'.format(self.__class__.__name__, self.beta)\n#\n\nclass Swish(HybridBlock):\n    def __init__(self, beta=1.0, **kwargs):\n        super(Swish, self).__init__(**kwargs)\n        self._beta = beta\n\n    def hybrid_forward(self, F, x):\n        return x * F.sigmoid(self._beta * x)\n        #return swish(self._beta)(x)\n\n    def __repr__(self):\n        return \'{} (beta={})\'.format(self.__class__.__name__, self._beta)\n\ndef _add_conv(out, channels=1, kernel=1, stride=1, pad=0, num_group=1,\n              activation=\'swish\', batchnorm=True, input_size=None,\n              in_channels=0):\n    out.add(SamePadding(kernel, stride, dilation=(1, 1), input_size=input_size))\n    out.add(nn.Conv2D(channels, kernel, stride, pad, groups=num_group,\n                      use_bias=False, in_channels=in_channels))\n    if batchnorm:\n        out.add(nn.BatchNorm(in_channels=channels, scale=True,\n                             momentum=0.99, epsilon=1e-3))\n    if activation == \'relu\':\n        out.add(nn.Activation(\'relu\'))\n    elif activation == \'swish\':\n        out.add(Swish())\n\ndef _update_input_size(input_size, stride):\n    sh, sw = (stride, stride) if isinstance(stride, int) else stride\n    ih, iw = (input_size, input_size) if isinstance(input_size, int) else input_size\n    oh, ow = math.ceil(ih / sh), math.ceil(iw / sw)\n    input_size = (oh, ow)\n    return input_size\n\n'"
autogluon/scheduler/remote/__init__.py,0,b'# remotes\nfrom .remote import *\nfrom .ssh_helper import *\nfrom .remote_manager import *\n'
autogluon/scheduler/remote/cli.py,0,"b'import click\n\nfrom autogluon.scheduler.remote.remote import DaskRemoteService\n\n@click.command(\n    help=""""""Launch an AutoGluon Remote from terminal, example:\n    agremote --address 172.31.14.110 --port 8781\n    """"""\n)\n@click.option(\n    ""--address"",\n    default=None,\n    type=str,\n    help=""Specify the ip address."",\n)\n@click.option(\n    ""--port"",\n    default=8786,\n    show_default=True,\n    type=int,\n    help=""Specify the port number."",\n)\ndef main(address, port):\n    service = DaskRemoteService(address, port)\n'"
autogluon/scheduler/remote/dask_scheduler.py,0,"b'import atexit\nimport logging\nimport gc\nimport os\nimport re\nimport shutil\nimport sys\nimport tempfile\nimport warnings\n\nimport click\n\nfrom tornado.ioloop import IOLoop\nfrom tornado.util import TimeoutError\n\nfrom distributed import Scheduler\nfrom distributed.preloading import validate_preload_argv\nfrom distributed.security import Security\nfrom distributed.cli.utils import check_python_3, install_signal_handlers\nfrom distributed.proctitle import (\n    enable_proctitle_on_children,\n    enable_proctitle_on_current,\n)\n\nlogger = logging.getLogger(""distributed.scheduler"")\n\n\npem_file_option_type = click.Path(exists=True, resolve_path=True)\n\n\n@click.command(context_settings=dict(ignore_unknown_options=True))\n@click.option(""--host"", type=str, default="""", help=""URI, IP or hostname of this server"")\n@click.option(""--port"", type=int, default=None, help=""Serving port"")\n@click.option(\n    ""--interface"",\n    type=str,\n    default=None,\n    help=""Preferred network interface like \'eth0\' or \'ib0\'"",\n)\n@click.option(\n    ""--protocol"", type=str, default=None, help=""Protocol like tcp, tls, or ucx""\n)\n@click.option(\n    ""--tls-ca-file"",\n    type=pem_file_option_type,\n    default=None,\n    help=""CA cert(s) file for TLS (in PEM format)"",\n)\n@click.option(\n    ""--tls-cert"",\n    type=pem_file_option_type,\n    default=None,\n    help=""certificate file for TLS (in PEM format)"",\n)\n@click.option(\n    ""--tls-key"",\n    type=pem_file_option_type,\n    default=None,\n    help=""private key file for TLS (in PEM format)"",\n)\n# XXX default port (or URI) values should be centralized somewhere\n@click.option(\n    ""--bokeh-port"", type=int, default=None, help=""Deprecated.  See --dashboard-address""\n)\n@click.option(\n    ""--dashboard-address"",\n    type=str,\n    default="":8787"",\n    show_default=True,\n    help=""Address on which to listen for diagnostics dashboard"",\n)\n@click.option(\n    ""--dashboard/--no-dashboard"",\n    ""dashboard"",\n    default=True,\n    required=False,\n    help=""Launch the Dashboard [default: --dashboard]"",\n)\n@click.option(\n    ""--bokeh/--no-bokeh"",\n    ""bokeh"",\n    default=None,\n    required=False,\n    help=""Deprecated.  See --dashboard/--no-dashboard."",\n)\n@click.option(""--show/--no-show"", default=False, help=""Show web UI [default: --show]"")\n@click.option(\n    ""--dashboard-prefix"", type=str, default=None, help=""Prefix for the dashboard app""\n)\n@click.option(\n    ""--use-xheaders"",\n    type=bool,\n    default=False,\n    show_default=True,\n    help=""User xheaders in dashboard app for ssl termination in header"",\n)\n@click.option(""--pid-file"", type=str, default="""", help=""File to write the process PID"")\n@click.option(\n    ""--scheduler-file"",\n    type=str,\n    default="""",\n    help=""File to write connection information. ""\n    ""This may be a good way to share connection information if your ""\n    ""cluster is on a shared network file system."",\n)\n@click.option(\n    ""--local-directory"", default="""", type=str, help=""Directory to place scheduler files""\n)\n@click.option(\n    ""--preload"",\n    type=str,\n    multiple=True,\n    is_eager=True,\n    default="""",\n    help=""Module that should be loaded by the scheduler process  ""\n    \'like ""foo.bar"" or ""/path/to/foo.py"".\',\n)\n@click.argument(\n    ""preload_argv"", nargs=-1, type=click.UNPROCESSED, callback=validate_preload_argv\n)\n@click.option(\n    ""--idle-timeout"",\n    default=None,\n    type=str,\n    help=""Time of inactivity after which to kill the scheduler"",\n)\n@click.version_option()\ndef main(\n    host,\n    port,\n    bokeh_port,\n    show,\n    dashboard,\n    bokeh,\n    dashboard_prefix,\n    use_xheaders,\n    pid_file,\n    local_directory,\n    tls_ca_file,\n    tls_cert,\n    tls_key,\n    dashboard_address,\n    **kwargs\n):\n    g0, g1, g2 = gc.get_threshold()  # https://github.com/dask/distributed/issues/1653\n    gc.set_threshold(g0 * 3, g1 * 3, g2 * 3)\n\n    enable_proctitle_on_current()\n    enable_proctitle_on_children()\n\n    if bokeh_port is not None:\n        warnings.warn(\n            ""The --bokeh-port flag has been renamed to --dashboard-address. ""\n            ""Consider adding ``--dashboard-address :%d`` "" % bokeh_port\n        )\n        dashboard_address = bokeh_port\n    if bokeh is not None:\n        warnings.warn(\n            ""The --bokeh/--no-bokeh flag has been renamed to --dashboard/--no-dashboard. ""\n        )\n        dashboard = bokeh\n\n    if port is None and (not host or not re.search(r"":\\d"", host)):\n        port = 8786\n\n    sec = Security(\n        **{\n            k: v\n            for k, v in [\n                (""tls_ca_file"", tls_ca_file),\n                (""tls_scheduler_cert"", tls_cert),\n                (""tls_scheduler_key"", tls_key),\n            ]\n            if v is not None\n        }\n    )\n\n    if not host and (tls_ca_file or tls_cert or tls_key):\n        host = ""tls://""\n\n    if pid_file:\n        with open(pid_file, ""w"") as f:\n            f.write(str(os.getpid()))\n\n        def del_pid_file():\n            if os.path.exists(pid_file):\n                os.remove(pid_file)\n\n        atexit.register(del_pid_file)\n\n    local_directory_created = False\n    if local_directory:\n        if not os.path.exists(local_directory):\n            os.mkdir(local_directory)\n            local_directory_created = True\n    else:\n        local_directory = tempfile.mkdtemp(prefix=""scheduler-"")\n        local_directory_created = True\n    if local_directory not in sys.path:\n        sys.path.insert(0, local_directory)\n\n    if sys.platform.startswith(""linux""):\n        import resource  # module fails importing on Windows\n\n        soft, hard = resource.getrlimit(resource.RLIMIT_NOFILE)\n        limit = max(soft, hard // 2)\n        resource.setrlimit(resource.RLIMIT_NOFILE, (limit, hard))\n\n    loop = IOLoop.current()\n    logger.info(""-"" * 47)\n\n    scheduler = Scheduler(\n        loop=loop,\n        security=sec,\n        host=host,\n        port=port,\n        dashboard_address=dashboard_address if dashboard else None,\n        service_kwargs={""dashboard"": {""prefix"": dashboard_prefix}},\n        **kwargs\n    )\n    logger.info(""Local Directory: %26s"", local_directory)\n    logger.info(""-"" * 47)\n\n    install_signal_handlers(loop)\n\n    async def run():\n        await scheduler\n        await scheduler.finished()\n\n    try:\n        loop.run_sync(run)\n    except TimeoutError:\n        pass\n    finally:\n        scheduler.stop()\n        if local_directory_created:\n            shutil.rmtree(local_directory)\n\n        logger.info(""End scheduler at %r"", scheduler.address)\n\n\ndef go():\n    check_python_3()\n    main()\n\n\nif __name__ == ""__main__"":\n    go()\n'"
autogluon/scheduler/remote/remote.py,0,"b'import os\nimport time\nimport signal\nimport atexit\nimport weakref\nimport logging\nimport subprocess\nimport concurrent\nfrom threading import Thread \nimport multiprocessing as mp\nfrom distributed import Client\n\nfrom .ssh_helper import start_scheduler, start_worker\n\n__all__ = [\'Remote\']\n\nlogger = logging.getLogger(__name__)\n\n_global_remote_services = weakref.WeakValueDictionary()\n_global_service_index = [0]\n\ndef _get_global_remote_service():\n    L = sorted(list(_global_remote_services), reverse=True)\n    for k in L:\n        c = _global_remote_services[k]\n        if c.status != ""closed"":\n            return c\n        else:\n            del _global_remote_services[k]\n    del L\n    return None\n\ndef _set_global_remote_service(c):\n    if c is not None:\n        _global_remote_services[_global_service_index[0]] = c\n        _global_service_index[0] += 1\n\ndef _close_global_remote_services():\n    """"""\n    Force close of global client.  This cleans up when a client\n    wasn\'t close explicitly, e.g. interactive sessions.\n    """"""\n    c = _get_global_remote_service()\n    if c is not None:\n        c.shutdown()\n\nclass Service(object):\n    def __init__(self, proc):\n        self.proc = proc\n        self.status = \'live\'\n\n    def shutdown(self):\n        os.killpg(os.getpgid(self.proc.pid), signal.SIGTERM)\n\ndef start_service(remote_ip, port):\n    cmd = [\'agremote\', \'--address\', remote_ip, \'--port\', str(port)]\n    proc = subprocess.Popen(cmd)\n    return Service(proc)\n\nclass Remote(Client):\n    LOCK = mp.Lock()\n    REMOTE_ID = mp.Value(\'i\', 0)\n    def __init__(self, remote_ip=None, port=None, local=False, ssh_username=None,\n            ssh_port=22, ssh_private_key=None, remote_python=None):\n        self.service = None\n        if local:\n            super().__init__(processes=False)\n        else:\n            remote_addr = (remote_ip + \':{}\'.format(port))\n            self.service = start_service(remote_ip, port)\n            _set_global_remote_service(self.service)\n            import time\n            time.sleep(10)\n            super().__init__(remote_addr)\n        with Remote.LOCK:\n            self.remote_id = Remote.REMOTE_ID.value\n            Remote.REMOTE_ID.value += 1\n\n    def close(self, timeout=2):\n        if self.service:\n            self.service.shutdown()\n        super().close(timeout)\n        self.shutdown()\n\n    def upload_files(self, files, **kwargs):\n        for filename in files:\n            self.upload_file(filename, **kwargs)\n\n    def __repr__(self):\n        reprstr = self.__class__.__name__ + \' REMOTE_ID: {}, \\n\\t\'.format(self.remote_id) + \\\n            super().__repr__()\n        return reprstr\n\nclass DaskRemoteService(object):\n    def __init__(self, remote_addr, scheduler_port, ssh_username=None,\n        ssh_port=22, ssh_private_key=None, remote_python=None):\n\n        self.scheduler_addr = remote_addr\n        self.scheduler_port = scheduler_port\n\n        self.ssh_username = ssh_username\n        self.ssh_port = ssh_port\n        self.ssh_private_key = ssh_private_key\n        self.remote_python = remote_python\n        self.monitor_thread = Thread()\n\n        # Start the scheduler node\n        self.scheduler = start_scheduler(\n            remote_addr,\n            scheduler_port,\n            ssh_username,\n            ssh_port,\n            ssh_private_key,\n            remote_python,\n        )\n        # Start worker nodes\n        self.worker = start_worker(\n            self.scheduler_addr,\n            self.scheduler_port,\n            remote_addr,\n            self.ssh_username,\n            self.ssh_port,\n            self.ssh_private_key,\n            self.remote_python,\n        )\n        self.start_monitoring()\n        self.status = ""live""\n\n    def start_monitoring(self):\n        if self.monitor_thread.is_alive():\n            return\n        self.monitor_thread = Thread(target=self.monitor_remote_processes)\n        #self.monitor_thread.daemon = True\n        self.monitor_thread.start()\n\n    def monitor_remote_processes(self):\n        all_processes = [self.scheduler, self.worker]\n        try:\n            while True:\n                for process in all_processes:\n                    while not process[""output_queue""].empty():\n                        try:\n                            msg = process[""output_queue""].get()\n                            if \'distributed.\' in msg:\n                                msg = msg.replace(\'distributed\', \'autogluon\')\n\n                            print(msg)\n                        except Exception as e:\n                            print(f\'Exception happend {e}, terminating the remote.\')\n                            break\n                # Kill some time and free up CPU\n                time.sleep(0.1)\n\n        except KeyboardInterrupt:\n            pass\n\n    def shutdown(self):\n        all_processes = [self.worker, self.scheduler]\n\n        for process in all_processes:\n            process[""input_queue""].put(""shutdown"")\n            process[""thread""].join()\n        self.status = ""closed""\n\natexit.register(_close_global_remote_services)\n'"
autogluon/scheduler/remote/remote_manager.py,0,"b""import os\nimport socket\nimport logging\nimport multiprocessing as mp\n\nfrom .remote import Remote\nfrom ...utils import warning_filter\n\n__all__ = ['RemoteManager']\n\nlogger = logging.getLogger(__name__)\n\n\ndef get_ip():\n    s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n    try:\n        # doesn't even have to be reachable\n        s.connect(('10.255.255.255', 1))\n        IP = s.getsockname()[0]\n    except:\n        IP = '127.0.0.1'\n    finally:\n        s.close()\n    return IP\n\n\nclass RemoteManager(object):\n    NODES = {}\n    LOCK = mp.Lock()\n    PORT_ID = mp.Value('i', 8700)\n    MASTER_IP = None\n    __instance = None\n    def __new__(cls):\n        # Singleton\n        if cls.__instance is None:\n            cls.__instance = object.__new__(cls)\n            cls.MASTER_IP = get_ip()\n            cls.start_local_node()\n        return cls.__instance\n\n    @classmethod\n    def get_master_node(cls):\n        return cls.NODES[cls.MASTER_IP]\n\n    @classmethod\n    def start_local_node(cls):\n        port = cls.get_port_id()\n        with warning_filter():\n            remote = Remote(cls.MASTER_IP, port, local=True)\n        with cls.LOCK:\n            cls.NODES[cls.MASTER_IP] = remote\n\n    @classmethod\n    def launch_each(cls, launch_fn, *args, **kwargs):\n        for node in cls.NODES.values():\n            node.submit(launch_fn, *args, **kwargs)\n\n    @classmethod\n    def get_remotes(cls):\n        return list(cls.NODES.values())\n\n    @classmethod\n    def upload_files(cls, files, **kwargs):\n        if isinstance(files, str):\n            files = [files]\n        for node in cls.NODES.values():\n            node.upload_files(files, **kwargs)\n\n    @classmethod\n    def add_remote_nodes(cls, ip_addrs):\n        ip_addrs = [ip_addrs] if isinstance(ip_addrs, str) else ip_addrs\n        remotes = []\n        for node_ip in ip_addrs:\n            if node_ip in cls.NODES.keys():\n                logger.warning('Already added remote {}'.format(node_ip))\n                continue\n            port = cls.get_port_id()\n            remote = Remote(node_ip, port)\n            with cls.LOCK:\n                cls.NODES[node_ip] = remote\n            remotes.append(remote)\n        return remotes\n    \n    @classmethod\n    def shutdown(cls):\n        for node in cls.NODES.values():\n            node.shutdown()\n        cls.NODES = {}\n        cls.__instance = None\n\n    @classmethod\n    def get_port_id(cls):\n        with cls.LOCK:\n            cls.PORT_ID.value += 1\n            return cls.PORT_ID.value\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, *args):\n        for node in self.NODES.values():\n            node.shutdown()\n\n    def __repr__(self):\n        reprstr = self.__class__.__name__ + '(\\n'\n        for node in self.NODES.values():\n           reprstr += '{}, \\n'.format(node)\n        reprstr += ')\\n'\n        return reprstr\n"""
autogluon/scheduler/remote/ssh_helper.py,0,"b'""""""ssh helper for starting remote dask scheduler""""""\nfrom __future__ import print_function, division, absolute_import\n\nimport os\nimport socket\nimport sys\nimport time\nimport traceback\n\ntry:\n    from queue import Queue\nexcept ImportError:  # Python 2.7 fix\n    from Queue import Queue\nimport logging\n\nfrom threading import Thread\nfrom toolz import merge\n\nlogger = logging.getLogger(__name__)\n\nclass bcolors:\n    HEADER = ""\\033[95m""\n    OKBLUE = ""\\033[94m""\n    OKGREEN = ""\\033[92m""\n    WARNING = ""\\033[93m""\n    FAIL = ""\\033[91m""\n    ENDC = ""\\033[0m""\n    BOLD = ""\\033[1m""\n    UNDERLINE = ""\\033[4m""\n\ndef async_ssh(cmd_dict):\n    import paramiko\n    from paramiko.buffered_pipe import PipeTimeout\n    from paramiko.ssh_exception import SSHException, PasswordRequiredException\n\n    ssh = paramiko.SSHClient()\n    ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n\n    retries = 0\n    while True:  # Be robust to transient SSH failures.\n        try:\n            # Set paramiko logging to WARN or higher to squelch INFO messages.\n            logging.getLogger(""paramiko"").setLevel(logging.WARN)\n\n            ssh.connect(\n                hostname=cmd_dict[""address""],\n                username=cmd_dict[""ssh_username""],\n                port=cmd_dict[""ssh_port""],\n                key_filename=cmd_dict[""ssh_private_key""],\n                compress=True,\n                timeout=20,\n                banner_timeout=20,\n            )  # Helps prevent timeouts when many concurrent ssh connections are opened.\n            # Connection successful, break out of while loop\n            break\n\n        except (SSHException, PasswordRequiredException) as e:\n\n            print(\n                ""[ dask-ssh ] : ""\n                + bcolors.FAIL\n                + ""SSH connection error when connecting to {addr}:{port}""\n                ""to run \'{cmd}\'"".format(\n                    addr=cmd_dict[""address""],\n                    port=cmd_dict[""ssh_port""],\n                    cmd=cmd_dict[""cmd""],\n                )\n                + bcolors.ENDC\n            )\n\n            print(\n                bcolors.FAIL\n                + ""               SSH reported this exception: ""\n                + str(e)\n                + bcolors.ENDC\n            )\n\n            # Print an exception traceback\n            traceback.print_exc()\n\n            # Transient SSH errors can occur when many SSH connections are\n            # simultaneously opened to the same server. This makes a few\n            # attempts to retry.\n            retries += 1\n            if retries >= 3:\n                print(\n                    ""[ dask-ssh ] : ""\n                    + bcolors.FAIL\n                    + ""SSH connection failed after 3 retries. Exiting.""\n                    + bcolors.ENDC\n                )\n\n                # Connection failed after multiple attempts.  Terminate this thread.\n                os._exit(1)\n\n            # Wait a moment before retrying\n            print(\n                ""               ""\n                + bcolors.FAIL\n                + ""Retrying... (attempt {n}/{total})"".format(n=retries, total=3)\n                + bcolors.ENDC\n            )\n\n            time.sleep(1)\n\n    # Execute the command, and grab file handles for stdout and stderr. Note\n    # that we run the command using the user\'s default shell, but force it to\n    # run in an interactive login shell, which hopefully ensures that all of the\n    # user\'s normal environment variables (via the dot files) have been loaded\n    # before the command is run. This should help to ensure that important\n    # aspects of the environment like PATH and PYTHONPATH are configured.\n\n    print(""[ {label} ] : {cmd}"".format(label=cmd_dict[""label""], cmd=cmd_dict[""cmd""]))\n    stdin, stdout, stderr = ssh.exec_command(\n        ""$SHELL -i -c \'"" + cmd_dict[""cmd""] + ""\'"", get_pty=True\n    )\n\n    # Set up channel timeout (which we rely on below to make readline() non-blocking)\n    channel = stdout.channel\n    channel.settimeout(0.1)\n\n    def read_from_stdout():\n        """"""\n        Read stdout stream, time out if necessary.\n        """"""\n        try:\n            line = stdout.readline()\n            while len(line) > 0:  # Loops until a timeout exception occurs\n                line = line.rstrip()\n                logger.debug(""stdout from ssh channel: %s"", line)\n                cmd_dict[""output_queue""].put(\n                    ""[ {label} ] : {output}"".format(\n                        label=cmd_dict[""label""], output=line\n                    )\n                )\n                line = stdout.readline()\n        except (PipeTimeout, socket.timeout):\n            pass\n\n    def read_from_stderr():\n        """"""\n        Read stderr stream, time out if necessary.\n        """"""\n        try:\n            line = stderr.readline()\n            while len(line) > 0:\n                line = line.rstrip()\n                logger.debug(""stderr from ssh channel: %s"", line)\n                cmd_dict[""output_queue""].put(\n                    ""[ {label} ] : "".format(label=cmd_dict[""label""])\n                    + bcolors.FAIL\n                    + ""{output}"".format(output=line)\n                    + bcolors.ENDC\n                )\n                line = stderr.readline()\n        except (PipeTimeout, socket.timeout):\n            pass\n\n    def communicate():\n        """"""\n        Communicate a little bit, without blocking too long.\n        Return True if the command ended.\n        """"""\n        read_from_stdout()\n        read_from_stderr()\n\n        # Check to see if the process has exited. If it has, we let this thread\n        # terminate.\n        if channel.exit_status_ready():\n            exit_status = channel.recv_exit_status()\n            cmd_dict[""output_queue""].put(\n                ""[ {label} ] : "".format(label=cmd_dict[""label""])\n                + bcolors.FAIL\n                + ""remote process exited with exit status ""\n                + str(exit_status)\n                + bcolors.ENDC\n            )\n            return True\n\n    # Get transport to current SSH client\n    transport = ssh.get_transport()\n\n    # Wait for a message on the input_queue. Any message received signals this\n    # thread to shut itself down.\n    while cmd_dict[""input_queue""].empty():\n        # Kill some time so that this thread does not hog the CPU.\n        time.sleep(1.0)\n        # Send noise down the pipe to keep connection active\n        transport.send_ignore()\n        if communicate():\n            break\n\n    # Ctrl-C the executing command and wait a bit for command to end cleanly\n    start = time.time()\n    while time.time() < start + 5.0:\n        try:\n            channel.send(b""\\x03"")  # Ctrl-C\n        except Exception:\n            break\n        if communicate():\n            break\n        time.sleep(1.0)\n\n    # Shutdown the channel, and close the SSH connection\n    channel.close()\n    ssh.close()\n\n\ndef start_scheduler(addr, port, ssh_username, ssh_port,\n                    ssh_private_key, remote_python=None):\n    cmd = ""dask-scheduler --port {port}"".format(\n        #python=remote_python or sys.executable,\n        port=port\n    )\n\n    # Format output labels we can prepend to each line of output, and create\n    # a \'status\' key to keep track of jobs that terminate prematurely.\n    label = (\n        bcolors.BOLD\n        + ""scheduler {addr}:{port}"".format(addr=addr, port=port)\n        + bcolors.ENDC\n    )\n\n    # Create a command dictionary, which contains everything we need to run and\n    # interact with this command.\n    input_queue = Queue()\n    output_queue = Queue()\n    cmd_dict = {\n        ""cmd"": cmd,\n        ""label"": label,\n        ""address"": addr,\n        ""port"": port,\n        ""input_queue"": input_queue,\n        ""output_queue"": output_queue,\n        ""ssh_username"": ssh_username,\n        ""ssh_port"": ssh_port,\n        ""ssh_private_key"": ssh_private_key,\n    }\n\n    # Start the thread\n    thread = Thread(target=async_ssh, args=[cmd_dict])\n    thread.daemon = True\n    thread.start()\n\n    return merge(cmd_dict, {""thread"": thread})\n\ndef start_worker(scheduler_addr, scheduler_port, worker_addr,\n    ssh_username, ssh_port, ssh_private_key,\n    remote_python=None):\n\n    cmd = (\n        ""dask-worker ""\n        ""{scheduler_addr}:{scheduler_port} ""\n        ""--no-nanny ""\n    )\n\n    #if not nohost:\n    cmd += "" --host {worker_addr}""\n\n    cmd = cmd.format(\n        #python=remote_python or sys.executable,\n        scheduler_addr=scheduler_addr,\n        scheduler_port=scheduler_port,\n        worker_addr=worker_addr,\n    )\n\n    label = ""worker {addr}"".format(addr=worker_addr)\n\n    # Create a command dictionary, which contains everything we need to run and\n    # interact with this command.\n    input_queue = Queue()\n    output_queue = Queue()\n    cmd_dict = {\n        ""cmd"": cmd,\n        ""label"": label,\n        ""address"": worker_addr,\n        ""input_queue"": input_queue,\n        ""output_queue"": output_queue,\n        ""ssh_username"": ssh_username,\n        ""ssh_port"": ssh_port,\n        ""ssh_private_key"": ssh_private_key,\n    }\n\n    # Start the thread\n    thread = Thread(target=async_ssh, args=[cmd_dict])\n    thread.daemon = True\n    thread.start()\n\n    return merge(cmd_dict, {""thread"": thread})\n'"
autogluon/scheduler/resource/__init__.py,0,b'from .resource import *\nfrom .dist_manager import *\n'
autogluon/scheduler/resource/dist_manager.py,0,"b'import logging\nimport multiprocessing as mp\nfrom .resource import *\nfrom ...utils import Queue\n\n__all__ = [\'DistributedResourceManager\', \'NodeResourceManager\']\n\nlogger = logging.getLogger(__name__)\n\nclass DistributedResourceManager(object):\n    LOCK = mp.Lock()\n    REQUESTING_STACK = []\n    MAX_CPU_COUNT = 0\n    MAX_GPU_COUNT = 0\n    NODE_RESOURCE_MANAGER = {}\n    __instance = None\n    def __new__(cls):\n        # Singleton\n        if cls.__instance is None:\n            cls.__instance = object.__new__(cls)\n        return cls.__instance\n\n    @classmethod\n    def add_remote(cls, remotes):\n        """"""Enables dynamically removing nodes\n        """"""\n        remotes = remotes if isinstance(remotes, list) else [remotes]\n        for remote in remotes:\n            cls.NODE_RESOURCE_MANAGER[remote] = NodeResourceManager(remote)\n        cls._refresh_resource()\n\n    @classmethod\n    def reserve_resource(cls, remote, resource):\n        node_manager = cls.NODE_RESOURCE_MANAGER[remote]\n        with cls.LOCK:\n            if not node_manager.check_availability(resource):\n                return False\n            node_manager._request(remote, resource)\n        logger.info(\'Reserved {} in {}\'.format(resource, remote))\n        return True\n\n    @classmethod\n    def release_reserved_resource(cls, remote, resource):\n        node_manager = cls.NODE_RESOURCE_MANAGER[remote]\n        node_manager._release(resource)\n        cls._evoke_request()\n\n    @classmethod\n    def _refresh_resource(cls):\n        cls.MAX_CPU_COUNT = max([x.get_all_resources()[0] for x in cls.NODE_RESOURCE_MANAGER.values()])\n        cls.MAX_GPU_COUNT = max([x.get_all_resources()[1] for x in cls.NODE_RESOURCE_MANAGER.values()])\n\n    @classmethod\n    def _request(cls, resource):\n        """"""ResourceManager, we recommand using scheduler instead of creating your own\n        resource manager.\n        """"""\n        assert cls.check_possible(resource), \\\n            \'Requested num_cpu={} and num_gpu={} should be less than or equal to\' + \\\n            \'largest node availability CPUs={}, GPUs={}\'. \\\n            format(resource.num_cpus, resource.num_gpus, cls.MAX_GPU_COUNT, cls.MAX_CPU_COUNT)\n       \n        with cls.LOCK:\n            node = cls.check_availability(resource)\n            if node is not None:\n                cls.NODE_RESOURCE_MANAGER[node]._request(node, resource)\n                return\n\n        logger.debug(\'Appending {} to Request Stack\'.format(resource))\n        request_semaphore = mp.Semaphore(0)\n        with cls.LOCK:\n            cls.REQUESTING_STACK.append((resource, request_semaphore))\n        request_semaphore.acquire()\n        return\n\n    @classmethod\n    def _release(cls, resource):\n        logger.debug(\'\\nReleasing resource {}\'.format(resource))\n        cls.NODE_RESOURCE_MANAGER[resource.node]._release(resource)\n        cls._evoke_request()\n\n    @classmethod\n    def _evoke_request(cls):\n        succeed = False\n        with cls.LOCK:\n            if len(cls.REQUESTING_STACK) > 0:\n                resource, request_semaphore = cls.REQUESTING_STACK.pop()\n                node = cls.check_availability(resource)\n                if node is not None:\n                    cls.NODE_RESOURCE_MANAGER[node]._request(node, resource)\n                    logger.debug(\'\\nEvoking requesting resource {}\'.format(resource))\n                    request_semaphore.release()\n                    succeed = True\n                else:\n                    cls.REQUESTING_STACK.append((resource, request_semaphore))\n                    return\n        if succeed:\n            cls._evoke_request()\n\n    @classmethod\n    def check_availability(cls, resource):\n        """"""Unsafe check\n        """"""\n        candidate_nodes = cls._get_possible_nodes(resource)\n        for node in candidate_nodes:\n            if cls.NODE_RESOURCE_MANAGER[node].check_availability(resource):\n                #logger.debug(\'\\nSuccessfully find node {}\'.format(node))\n                return node\n        return None\n\n    @classmethod\n    def check_possible(cls, resource):\n        assert isinstance(resource, DistributedResource), \\\n            \'Only support autogluon.resource.DistributedResource\'\n        if resource.num_cpus > cls.MAX_CPU_COUNT or resource.num_gpus > cls.MAX_GPU_COUNT:\n            return False\n        return True\n\n    @classmethod\n    def remove_remote(cls, remotes):\n        #TODO \n        """"""Enables dynamically removing nodes\n        """"""\n        cls._refresh_resource()\n        pass\n\n    @classmethod\n    def _get_possible_nodes(cls, resource):\n        candidates = []\n        for remote, manager in cls.NODE_RESOURCE_MANAGER.items():\n            if manager.check_possible(resource):\n                candidates.append(remote)\n        return candidates\n\n    def __repr__(self):\n        reprstr = self.__class__.__name__ + \'{\\n\'\n        for remote, manager in self.NODE_RESOURCE_MANAGER.items():\n            reprstr += \'(Remote: {}, Resource: {})\\n\'.format(remote, manager)\n        reprstr += \'}\'\n        return reprstr\n\n\nclass NodeResourceManager(object):\n    """"""Remote Resource Manager to keep track of the cpu and gpu usage\n    """"""\n    def __init__(self, remote):\n        self.LOCK = mp.Lock()\n        self.MAX_CPU_COUNT = get_remote_cpu_count(remote)\n        self.MAX_GPU_COUNT = get_remote_gpu_count(remote)\n        self.CPU_QUEUE = Queue()\n        self.GPU_QUEUE = Queue()\n        for cid in range(self.MAX_CPU_COUNT):\n            self.CPU_QUEUE.put(cid)\n        for gid in range(self.MAX_GPU_COUNT):\n            self.GPU_QUEUE.put(gid)\n\n    def _request(self, remote, resource):\n        """"""ResourceManager, we recommand using scheduler instead of creating your own\n        resource manager.\n        """"""\n        assert self.check_possible(resource), \\\n            \'Requested num_cpu={} and num_gpu={} should be less than or equal to\' + \\\n            \'system availability CPUs={}, GPUs={}\'. \\\n            format(resource.num_cpus, resource.num_gpus, self.MAX_GPU_COUNT, self.MAX_CPU_COUNT)\n\n        with self.LOCK:\n            cpu_ids = [self.CPU_QUEUE.get() for i in range(resource.num_cpus)]\n            gpu_ids = [self.GPU_QUEUE.get() for i in range(resource.num_gpus)]\n            resource._ready(remote, cpu_ids, gpu_ids)\n            #logger.debug(""\\nReqeust succeed {}"".format(resource))\n            return\n \n    def _release(self, resource):\n        cpu_ids = resource.cpu_ids\n        gpu_ids = resource.gpu_ids\n        resource._release()\n        if len(cpu_ids) > 0:\n            for cid in cpu_ids:\n                self.CPU_QUEUE.put(cid)\n        if len(gpu_ids) > 0:\n            for gid in gpu_ids:\n                self.GPU_QUEUE.put(gid)\n\n    def get_all_resources(self):\n        return self.MAX_CPU_COUNT, self.MAX_GPU_COUNT\n\n    def check_availability(self, resource):\n        """"""Unsafe check\n        """"""\n        if resource.num_cpus > self.CPU_QUEUE.qsize() or resource.num_gpus > self.GPU_QUEUE.qsize():\n            return False\n        return True\n\n    def check_possible(self, resource):\n        assert isinstance(resource, DistributedResource), \'Only support autogluon.resource.Resources\'\n        if resource.num_cpus > self.MAX_CPU_COUNT or resource.num_gpus > self.MAX_GPU_COUNT:\n            return False\n        return True\n\n    def __repr__(self):\n        reprstr = self.__class__.__name__ + \'(\' + \\\n            \'{} CPUs, \'.format(self.MAX_CPU_COUNT) + \\\n            \'{} GPUs)\'.format(self.MAX_GPU_COUNT)\n        return reprstr\n'"
autogluon/scheduler/resource/nvutil.py,0,"b'from ctypes import *\nfrom ctypes.util import find_library\nimport sys\nimport os\nimport threading\nimport string\n\n__all__ = [\'cudaInit\', \'cudaDeviceGetCount\', \'cudaSystemGetNVMLVersion\'\n           \'cudaShutdown\']\n\nNVML_SUCCESS                                = 0\nNVML_ERROR_UNINITIALIZED                    = 1\nNVML_ERROR_LIBRARY_NOT_FOUND                = 12\nNVML_ERROR_FUNCTION_NOT_FOUND               = 13\nNVML_SYSTEM_NVML_VERSION_BUFFER_SIZE        = 80\n\ncudaLib = None\nlibLoadLock = threading.Lock()\n_cudaLib_refcount = 0 # Incremented on each cudaInit and decremented on cudaShutdown\n\n## C function wrappers ##\ndef cudaInit():\n    if not _LoadNvmlLibrary():\n        return False\n    \n    #\n    # Initialize the library\n    #\n    fn = _cudaGetFunctionPointer(""nvmlInit_v2"")\n    ret = fn()\n    try:\n        _cudaCheckReturn(ret)\n    except NVMLError:\n        return False\n   \n    # Atomically update refcount\n    global _cudaLib_refcount\n    libLoadLock.acquire()\n    _cudaLib_refcount += 1\n    libLoadLock.release()\n    return True\n\n## Device get functions\ndef cudaDeviceGetCount():\n    c_count = c_uint()\n    fn = _cudaGetFunctionPointer(""nvmlDeviceGetCount_v2"")\n    ret = fn(byref(c_count))\n    _cudaCheckReturn(ret)\n    return c_count.value\n\ndef _LoadNvmlLibrary():\n    \'\'\'\n    Load the library if it isn\'t loaded already\n    \'\'\'\n    global cudaLib\n    \n    ret = True\n    if (cudaLib == None):\n        # lock to ensure only one caller loads the library\n        libLoadLock.acquire()\n        try:\n            # ensure the library still isn\'t loaded\n            if (cudaLib == None):\n                try:\n                    if (sys.platform[:3] == ""win""):\n                        # cdecl calling convention\n                        # load cuda.dll from %ProgramFiles%/NVIDIA Corporation/NVSMI/cuda.dll\n                        cudaLib = CDLL(os.path.join(os.getenv(""ProgramFiles"", ""C:/Program Files""), ""NVIDIA Corporation/NVSMI/cuda.dll""))\n                    else:\n                        # assume linux\n                        cudaLib = CDLL(""libnvidia-ml.so.1"")\n                except OSError as ose:\n                    pass\n\n                if (cudaLib == None):\n                    ret = False\n        finally:\n            # lock is always freed\n            libLoadLock.release()\n\n    return ret\n\ndef cudaSystemGetNVMLVersion():\n    c_version = create_string_buffer(NVML_SYSTEM_NVML_VERSION_BUFFER_SIZE)\n    fn = _cudaGetFunctionPointer(""nvmlSystemGetNVMLVersion"")\n    ret = fn(c_version, c_uint(NVML_SYSTEM_NVML_VERSION_BUFFER_SIZE))\n    _cudaCheckReturn(ret)\n    return c_version.value.decode(\'UTF-8\')\n\n## Function access ##\n_cudaGetFunctionPointer_cache = dict() # function pointers are cached to prevent unnecessary libLoadLock locking\ndef _cudaGetFunctionPointer(name):\n    global cudaLib\n\n    if name in _cudaGetFunctionPointer_cache:\n        return _cudaGetFunctionPointer_cache[name]\n    \n    libLoadLock.acquire()\n    try:\n        # ensure library was loaded\n        if (cudaLib == None):\n            raise NVMLError(NVML_ERROR_UNINITIALIZED)\n        try:\n            _cudaGetFunctionPointer_cache[name] = getattr(cudaLib, name)\n            return _cudaGetFunctionPointer_cache[name]\n        except AttributeError:\n            raise NVMLError(NVML_ERROR_FUNCTION_NOT_FOUND)\n    finally:\n        # lock is always freed\n        libLoadLock.release()\n\ndef _cudaCheckReturn(ret):\n    if (ret != NVML_SUCCESS):\n        raise NVMLError(ret)\n    return ret\n\nclass NVMLError(Exception):\n    _valClassMapping = dict()\n    # List of currently known error codes\n    _errcode_to_string = {\n        NVML_ERROR_UNINITIALIZED:       ""Uninitialized"",\n        NVML_ERROR_LIBRARY_NOT_FOUND:   ""NVML Shared Library Not Found"",\n        }\n    def __new__(typ, value):\n        \'\'\'\n        Maps value to a proper subclass of NVMLError.\n        See _extractNVMLErrorsAsClasses function for more details\n        \'\'\'\n        if typ == NVMLError:\n            typ = NVMLError._valClassMapping.get(value, typ)\n        obj = Exception.__new__(typ)\n        obj.value = value\n        return obj\n\n    def __str__(self):\n        try:\n            if self.value not in NVMLError._errcode_to_string:\n                NVMLError._errcode_to_string[self.value] = str(cudaErrorString(self.value))\n            return NVMLError._errcode_to_string[self.value]\n        except Exception:\n            return ""NVML Error with code %d"" % self.value\n\n    def __eq__(self, other):\n        return self.value == other.value\n\ndef cudaShutdown():\n    #\n    # Leave the library loaded, but shutdown the interface\n    #\n    fn = _cudaGetFunctionPointer(""nvmlShutdown"")\n    ret = fn()\n    _cudaCheckReturn(ret)\n    \n    # Atomically update refcount\n    global _cudaLib_refcount\n    libLoadLock.acquire()\n    if (0 < _cudaLib_refcount):\n        _cudaLib_refcount -= 1\n    libLoadLock.release()\n    return None\n'"
autogluon/scheduler/resource/resource.py,0,"b'import logging\nfrom multiprocessing import cpu_count\n\n__all__ = [\'Resources\', \'DistributedResource\',\n           \'get_cpu_count\', \'get_gpu_count\', \n           \'get_remote_cpu_count\', \'get_remote_gpu_count\']\n\nlogger = logging.getLogger(__name__)\n\nclass Resources(object):\n    """"""Resource for AutoGluon Scheduler :class:`autogluon.scheduler.TaskScheduler`\n\n    Args:\n        num_cpus (int): number of cpu cores required for the training task.\n        num_gpus (int): number of gpu required for the training task.\n\n    Example:\n        >>> def my_task():\n        >>>     pass\n        >>> resource = Resources(num_cpus=2, num_gpus=0)\n        >>> task = Task(my_task, {}, resource)\n    """"""\n    def __init__(self, num_cpus=1, num_gpus=0):\n        self.num_cpus = num_cpus\n        self.num_gpus = num_gpus\n        self.cpu_ids = []\n        self.gpu_ids = []\n        self.ready = False\n\n    def _release(self):\n        self.ready = False\n        self.cpu_ids = []\n        self.gpu_ids = []\n\n    def _ready(self, cids, gids):\n        self.cpu_ids = cids\n        self.gpu_ids = gids\n        self.ready = True\n\n    def __repr__(self):\n        reprstr = self.__class__.__name__ + \'(\' \\\n            + \'nCPUs = \' + str(self.num_cpus)\n        if len(self.cpu_ids) > 0:\n            reprstr += \', CPU_IDs = {\' + str(self.cpu_ids) + \'}\'\n        if self.num_gpus > 0:\n            reprstr += \', nGPUs = \' + str(self.num_gpus)\n        if len(self.gpu_ids) > 0:\n            reprstr += \', GPU_IDs = {\' + str(self.gpu_ids) + \'}\'\n        reprstr += \')\'\n        return reprstr\n\nclass DistributedResource(Resources):\n    """"""Resource for AutoGluon Distributed Scheduler :class:`autogluon.distributed.DistributedTaskScheduler`\n\n    Args:\n        num_cpus (int): number of cpu cores required for the training task.\n        num_gpus (int): number of gpu required for the training task.\n\n    Example:\n        >>> def my_task():\n        >>>     pass\n        >>> resource = DistributedResource(num_cpus=2, num_gpus=1)\n        >>> task = Task(my_task, {}, resource)\n    """"""\n    def __init__(self, num_cpus=1, num_gpus=0):\n        super(DistributedResource, self).__init__(num_cpus, num_gpus)\n        self.node = None\n        self.is_ready = False\n\n    def _ready(self, remote, cids, gids):\n        super(DistributedResource, self)._ready(cids, gids)\n        self.node = remote\n        self.is_ready = True\n \n    def _release(self):\n        super(DistributedResource, self)._release()\n        self.node = None\n\n    def __repr__(self):\n        reprstr = self.__class__.__name__ + \'(\\n\\t\'\n        if self.node: reprstr  += \'Node = \' + str(self.node)\n        reprstr  += \'\\n\\tnCPUs = \' + str(self.num_cpus)\n        if len(self.cpu_ids) > 0:\n            reprstr += \', CPU_IDs = {\' + str(self.cpu_ids) + \'}\'\n        if self.num_gpus > 0:\n            reprstr += \', nGPUs = \' + str(self.num_gpus)\n        if len(self.gpu_ids) > 0:\n            reprstr += \', GPU_IDs = {\' + str(self.gpu_ids) + \'}\'\n        reprstr += \')\'\n        return reprstr\n\ndef get_cpu_count():\n    return cpu_count()\n\ndef get_gpu_count():\n    from .nvutil import cudaInit, cudaDeviceGetCount, cudaShutdown\n    if not cudaInit(): return 0\n    gpu_count = cudaDeviceGetCount()\n    cudaShutdown()\n    return gpu_count\n\ndef get_remote_cpu_count(node):\n    ret = node.submit(get_cpu_count)\n    return ret.result()\n\ndef get_remote_gpu_count(node):\n    ret = node.submit(get_gpu_count)\n    return ret.result()\n'"
autogluon/task/base/__init__.py,0,b'from .base_task import *\nfrom .base_predictor import *\n'
autogluon/task/base/base_predictor.py,0,"b'"""""" TODOs needed to get predictor object working for any task:\n    - Base Task should implement @classmethod load(output_directory) method which restores a Predictor object from file (previously saved using Predictor.save(output_directory)). task.load(output_directory) can simply: return Predictor.load(output_directory)\n\n    - Base Task.fit() needs to create Results object and assign it to predictor.results before returning predictor. The only thing task.fit should return is a single Predictor object.\n\n    - Right before task.fit() returns predictor, it should call: predictor.save(outputdir) so that training progress is not accidentally lost.\n\n    - task.fit() needs to have an output_directory argument where to store all outputs\n\n    - Delete line ""Results = collections.namedtuple(\'Results\', \'model reward config time metadata\')"" from task.fit(), and store all info in self.results dict object defined below instead.\n\n    - This code assumes trial_ids are sortable such that lower trial_id indicates trial was scheduled earlier than trial with higher trial_id\n\n    - task object should have get_labels(Dataset) method\n""""""\n\nimport json\nimport logging\nimport pickle\nfrom abc import ABC, abstractmethod\n\nfrom ...utils import plot_performance_vs_trials, plot_summary_of_models\n\nlogger = logging.getLogger(__name__)\n\n__all__ = [\'BasePredictor\']\n\nPREDICTOR_FILENAME = ""predictor.pkl""  # Filename in which predictor object is stored. Should be hardcoded so that user only needs to specify directory where to store all training-related output files.\nRESULTS_FILENAME = ""results.json""  # Filename in which FitResults object is stored. Should be hardcoded so that user only needs to specify directory where to store all training-related output files.\n\n\nclass BasePredictor(ABC):\n    """"""\n    Base object returned by task.fit() for each task implemented in AutoGluon.\n    Example user workflow for say image classification applications:\n        # Training time:\n        >>> from autogluon import image_classification as task\n        >>> train_data = task.Dataset(traindata_filepath)\n        >>> output_directory = \'~/temp/\' # any directory name specifying where to store all results\n        >>> predictor = task.fit(train_data=train_data, output_directory=output_directory)\n        >>> # To instead specify train/val split, do: predictor = task.fit(train_data=train_data, val_data=task.Dataset(valdata_filepath), output_directory=output_directory)\n        >>> results = predictor.fit_summary() # will also print out summary and create plots\n        # Inference time (may be a new Python session):\n        >>> test_data = task.Dataset(testdata_filepath)\n        >>> test_labels = task.get_labels(test_data)\n        >>> predictor = None  # We delete predictor here just to demonstrate how to load previously-trained predictor from file\n        >>> predictor = task.load(output_directory)\n        >>> batch_predictions = predictor.predict(test_data)\n        >>> performance = predictor.evaluate_predictions(y_true=test_labels, y_pred=batch_predictions)\n        # or can instead just use equivalent shorthand: performance = predictor.evaluate(test_data)\n        # Can also do inference on just a single test example: x_i = single datapoint, eg. x_i = test_data[i]\n        >>> single_prediction = predictor.predict(x_i)\n        >>> print((x_i, single_prediction))\n    """"""\n\n    def __init__(self, loss_func, eval_func, model=None, results=None, **kwargs):\n        self.model = model  # MXnet model or list of multiple models / ensemble. Each model should have its own loading/saving functionality.\n        self.loss_func = loss_func  # Loss function (or string name) minimized during training\n        self.eval_func = eval_func  # Evaluation function / metric applied on validation/test data to gauge predictive performance.\n        # Note: we may save a lot of headache if higher values of this eval_func metric = better, consistently across all tasks.\n        # self.results = self._createResults() # dict object to store all information during task.fit().\n        self.results = results\n\n    @classmethod\n    @abstractmethod\n    def load(cls, output_directory):\n        """""" Load Predictor object from given directory.\n            Make sure to also load any models from files that exist in output_directory and set them = predictor.model.\n        """"""\n        filepath = output_directory + PREDICTOR_FILENAME\n        results_file = output_directory + RESULTS_FILENAME\n        predictor = pickle.load(open(filepath, ""rb""))\n        predictor.results = json.load(open(results_file, \'r\'))\n        pass  # Need to load models and set them = predictor.model\n\n    @abstractmethod\n    def save(self, output_directory):\n        """""" Saves this object to file. Don\'t forget to save the models and the Results objects if they exist.\n            Before returning a Predictor, task.fit() should call predictor.save()\n        """"""\n        filepath = output_directory + PREDICTOR_FILENAME\n        self._save_model(output_directory)\n        self._save_results(output_directory)\n        self.model = None  # Save model separately from Predictor object\n        self.results = None  # Save results separately from Predictor object\n        pickle.dump(self, open(filepath, \'wb\'))\n        logger.info(""Predictor saved to file: %s "" % filepath)\n\n    def _save_results(self, output_directory):\n        """""" Internal helper function: Save results in human-readable file JSON format """"""\n        results_file = output_directory + RESULTS_FILENAME\n        json.dump(self.results, open(results_file, \'w\'))\n\n    def _save_model(self, output_directory):\n        """""" Internal helper function: Save self.model object to file located in output_directory.\n            For example, if self.model is MXNet model, can simply call self.model.save(output_directory+filename)\n        """"""\n        raise NotImplementedError\n\n    @abstractmethod\n    def predict(self, X):\n        """""" This method should be able to produce predictions regardless if:\n            X = single data example (e.g. single image, single document),\n            X = batch of many examples, X = task.Dataset object\n        """"""\n        pass\n\n    @abstractmethod\n    def predict_proba(self, X):\n        """""" Produces predicted class probabilities if we are dealing with a classification task.\n            In this case, predict() should just be a wrapper around this method to convert predicted probabilties to predicted class labels.\n        """"""\n        pass\n\n    @abstractmethod\n    def evaluate_predictions(self, y_true, y_pred):\n        """""" Evaluate the provided list of predictions against list of ground truth labels according to the task-specific evaluation metric (self.eval_func). """"""\n        pass\n\n    @abstractmethod\n    def evaluate(self, dataset):\n        """""" Use self.model to produce predictions from the given Dataset object, and then compute task-specific evaluation metric (self.eval_func)\n            comparing these predictions against ground truth labels stored in the Dataset object.\n        """"""\n        pass\n\n    def fit_summary(self, output_directory=None, verbosity=2):\n        """"""\n            Returns a summary of the fit process.\n            Args:\n                verbosity (int): how much output to print:\n                <= 0 for no output printing, 1 for just high-level summary, 2 for summary and plot, >= 3 for all information contained in results object.\n        """"""\n        if verbosity > 0:\n            summary = {}\n            for k in self.results.keys():\n                if k not in [\'metadata\', \'trial_info\']:\n                    summary[k] = self.results[k]\n            print(""Summary of Fit Process:  "")\n            print(summary)\n            if len(self.results[\'metadata\']) > 0:\n                print(self.results[\'metadata\'])\n\n        if len(self.results[\'trial_info\']) > 0 and verbosity > 1:\n            ordered_trials = sorted(self.results[\'trial_info\'].keys())\n            if verbosity > 2:\n                for trial_id in ordered_trials:\n                    print(""Information about each trial:  "")\n                    print(""Trial ID: %s"" % trial_id)\n                    print(self.results[\'trial_info\'][trial_id])\n            if verbosity > 3:\n                # Create plot summaries:\n                plot_summary_of_models(self.results, output_directory)\n                plot_performance_vs_trials(self.results, output_directory)\n        return self.results\n\n    def _createResults(self):\n        """""" Internal helper function: Dict object to store all relevant information produced during task.fit(). \n            Empty for now, but should be updated during task.fit().\n            All tasks should adhere to this same template for consistency.\n        """"""\n        results = {}\n        results[\'time\'] = None  # run-time of task.fit()\n        results[\'reward_attr\'] = \'none\'  # (str), the reward attribute used to measure the performance\n        results[results[\'reward_attr\']] = None  # performance of the best trials\n        results[\'num_trials_completed\'] = None  # number of trials completed during task.fit()\n        results[\'best_hyperparameters\'] = None  # hyperparameter values corresponding to the chosen model in self.model\n        results[\'search_space\'] = None  # hyperparameter search space considered in task.fit()\n        results[\'search_strategy\'] = None  # HPO algorithm used (ie. Hyperband, random, BayesOpt). If the HPO algorithm used kwargs, then this should be tuple (HPO_algorithm_string, HPO_kwargs)\n\n        results[\'metadata\'] = {}  # dict containing other optional metadata with keys. For example:\n        # latency = inference-time of self.model (time for feedforward pass)\n        # memory = amount of memory required by self.model\n\n        results[\'trial_info\'] = {}  # dict with keys = trial_IDs, values = dict of information about each individual trial (length = results[\'num_trials_completed\'])\n        """""" Example of what one element of this dict must look like: \n\n        results[\'trial_info\'][trial_id] =  {\n            \'config\' : hyperparameter configuration tried in this trial\n            \'training_loss\' : training loss value achieved by the model from this trial (on the training data)\n            \'metadata\' : dict of various optional metadata with keys such as: latency, memory, time, early_stopped, etc.\n        }\n\n        """"""\n        return results\n\n    @staticmethod\n    def _format_results(results):\n        """""" Formats miscellaneous records captured by scheduler into user-viewable Results object. """"""\n\n        def _merge_scheduler_history(training_history, config_history, reward_attr):\n            trial_info = {}\n            for tid, config in config_history.items():\n                trial_info[tid] = {}\n                trial_info[tid][\'config\'] = config\n                if tid in training_history:\n                    trial_info[tid][\'history\'] = training_history[tid]\n                    trial_info[tid][\'metadata\'] = {}\n\n                    if len(training_history[tid]) > 0 and reward_attr in training_history[tid][-1]:\n                        last_history = training_history[tid][-1]\n                        trial_info[tid][reward_attr] = last_history.pop(reward_attr)\n                        trial_info[tid][\'metadata\'].update(last_history)\n            return trial_info\n\n        training_history = results.pop(\'training_history\')\n        config_history = results.pop(\'config_history\')\n        results[\'trial_info\'] = _merge_scheduler_history(training_history, config_history,\n                                                         results[\'reward_attr\'])\n        results[results[\'reward_attr\']] = results[\'best_reward\']\n        results[\'search_space\'] = results[\'metadata\'].pop(\'search_space\')\n        results[\'search_strategy\'] = results[\'metadata\'].pop(\'search_strategy\')\n        return results\n'"
autogluon/task/base/base_task.py,0,"b'import collections\nimport copy\nimport time\nfrom abc import abstractmethod\n\nimport mxnet as mx\n\nfrom ...scheduler import *\nfrom ...utils import in_ipynb\n\n__all__ = [\n    \'BaseDataset\',\n    \'BaseTask\',\n    \'compile_scheduler_options\',\n    \'create_scheduler\']\n\nResults = collections.namedtuple(\'Results\', \'model reward config time metadata\')\n\nschedulers = {\n    \'grid\': FIFOScheduler,\n    \'random\': FIFOScheduler,\n    \'skopt\': FIFOScheduler,\n    \'hyperband\': HyperbandScheduler,\n    \'rl\': RLScheduler,\n    \'bayesopt\': FIFOScheduler,\n    \'bayesopt_hyperband\': HyperbandScheduler}\n\n\ndef create_scheduler(train_fn, search_strategy, scheduler_options):\n    if isinstance(search_strategy, str):\n        scheduler_cls = schedulers[search_strategy.lower()]\n    else:\n        assert callable(search_strategy)\n        scheduler_cls = search_strategy\n        scheduler_options = copy.copy(scheduler_options)\n        scheduler_options[\'searcher\'] = \'random\'\n    return scheduler_cls(train_fn, **scheduler_options)\n\n\nclass BaseDataset(mx.gluon.data.Dataset):\n    # put any sharable dataset methods here\n    pass\n\n\nclass BaseTask(object):\n    """"""BaseTask for AutoGluon applications\n    """"""\n    Dataset = BaseDataset\n\n    @classmethod\n    def run_fit(cls, train_fn, search_strategy, scheduler_options,\n                plot_results=False):\n        start_time = time.time()\n        # create scheduler and schedule tasks\n        scheduler = create_scheduler(\n            train_fn, search_strategy, scheduler_options)\n        print(\'scheduler:\', scheduler)\n        scheduler.run()\n        scheduler.join_jobs()\n        # gather the best configuration\n        best_reward = scheduler.get_best_reward()\n        best_config = scheduler.get_best_config()\n        args = train_fn.args\n        args.final_fit = True\n        if hasattr(args, \'epochs\') and hasattr(args, \'final_fit_epochs\'):\n            args.epochs = args.final_fit_epochs\n        results = scheduler.run_with_config(best_config)\n        total_time = time.time() - start_time\n        if plot_results or in_ipynb():\n            plot_training_curves = scheduler_options[\'checkpoint\'].replace(\'exp1.ag\', \'plot_training_curves.png\')\n            scheduler.get_training_curves(filename=plot_training_curves, plot=True, use_legend=False)\n        record_args = copy.deepcopy(args)\n        results.update(best_reward=best_reward,\n                       best_config=best_config,\n                       total_time=total_time,\n                       metadata=scheduler.metadata,\n                       training_history=scheduler.training_history,\n                       config_history=scheduler.config_history,\n                       reward_attr=scheduler._reward_attr,\n                       args=record_args)\n        return results\n\n    @classmethod\n    @abstractmethod\n    def fit(cls, *args, **kwargs):\n        pass\n\n\n# These search_strategies use HyperbandScheduler, along with certain\n# searchers.\nsearcher_for_hyperband_strategy = {\n    \'hyperband\': \'random\',\n    \'bayesopt_hyperband\': \'bayesopt\'}\n\n\ndef compile_scheduler_options(\n        scheduler_options, search_strategy, search_options, nthreads_per_trial,\n        ngpus_per_trial, checkpoint, num_trials, time_out, resume, visualizer,\n        time_attr, reward_attr, dist_ip_addrs, epochs=None):\n    """"""\n    Updates a copy of scheduler_options (scheduler-specific options, can be\n    empty) with general options. The result can be passed to __init__ of the\n    scheduler.\n\n    Special role of epochs for HyperbandScheduler: If the search_strategy\n    involves HyperbandScheduler and epochs is given, then this value is\n    copied to scheduler_options[\'max_t\']. Pass epochs for applications\n    where the time_attr is epoch, and epochs is the maximum number of\n    epochs.\n\n    :param scheduler_options:\n    :param search_strategy:\n    :param search_options:\n    :param nthreads_per_trial:\n    :param ngpus_per_trial:\n    :param checkpoint:\n    :param num_trials:\n    :param time_out:\n    :param resume:\n    :param visualizer:\n    :param time_attr:\n    :param reward_attr:\n    :param dist_ip_addrs:\n    :param kwargs:\n    :param epochs: See above. Optional\n    :return: Copy of scheduler_options with updates\n\n    """"""\n    if scheduler_options is None:\n        scheduler_options = dict()\n    else:\n        assert isinstance(scheduler_options, dict)\n    assert isinstance(search_strategy, str)\n    if search_options is None:\n        search_options = dict()\n    if visualizer is None:\n        visualizer = \'none\'\n    if time_attr is None:\n        time_attr = \'epoch\'\n    if reward_attr is None:\n        reward_attr = \'accuracy\'\n    scheduler_options = copy.copy(scheduler_options)\n    scheduler_options.update({\n        \'resource\': {\n            \'num_cpus\': nthreads_per_trial, \'num_gpus\': ngpus_per_trial},\n        \'searcher\': search_strategy,\n        \'search_options\': search_options,\n        \'checkpoint\': checkpoint,\n        \'resume\': resume,\n        \'num_trials\': num_trials,\n        \'time_out\': time_out,\n        \'reward_attr\': reward_attr,\n        \'time_attr\': time_attr,\n        \'visualizer\': visualizer,\n        \'dist_ip_addrs\': dist_ip_addrs})\n    searcher = searcher_for_hyperband_strategy.get(search_strategy)\n    if searcher is not None:\n        scheduler_options[\'searcher\'] = searcher\n        if epochs is not None:\n            scheduler_options[\'max_t\'] = epochs\n    return scheduler_options\n'"
autogluon/task/image_classification/__init__.py,0,b'from .image_classification import *\nfrom .nets import get_built_in_network\nfrom .dataset import *\nfrom .classifier import Classifier\n'
autogluon/task/image_classification/classifier.py,0,"b'import copy\nimport math\nimport os\nfrom collections import OrderedDict, defaultdict\n\nimport cloudpickle as pkl\nimport matplotlib.pyplot as plt\nimport mxnet as mx\nimport numpy as np\nfrom PIL import Image\nfrom mxnet.gluon.data.vision import transforms\n\nfrom .metrics import get_metric_instance\nfrom .nets import get_network\nfrom .utils import *\nfrom ..base.base_predictor import BasePredictor\nfrom ...core import AutoGluonObject\nfrom ...utils import save, load, tqdm, collect_params, update_params\n\n__all__ = [\'Classifier\']\n\n\nclass Classifier(BasePredictor):\n    """"""Trained Image Classifier returned by fit() that can be used to make predictions on new images.\n\n    Attributes\n    ----------\n\n    Examples\n    --------\n    >>> from autogluon import ImageClassification as task\n    >>> dataset = task.Dataset(train_path=\'data/train\',\n    >>>                        test_path=\'data/test\')\n    >>> classifier = task.fit(dataset,\n    >>>                       nets=ag.space.Categorical[\'resnet18_v1\', \'resnet34_v1\'],\n    >>>                       time_limits=time_limits,\n    >>>                       ngpus_per_trial=1,\n    >>>                       num_trials = 4)\n    >>> image = \'data/test/BabyShirt/BabyShirt_323.jpg\'\n    >>> ind, prob = classifier.predict(image)\n    """"""\n\n    def __init__(self, model, results, eval_func, scheduler_checkpoint,\n                 args, ensemble=0, format_results=True, **kwargs):\n        self.model = model\n        self.eval_func = eval_func\n        self.results = self._format_results(results) if format_results else results\n        self.scheduler_checkpoint = scheduler_checkpoint\n        self.args = args\n        self.ensemble = ensemble\n\n    @classmethod\n    def load(cls, checkpoint):\n        """"""Load trained Image Classifier from directory specified by `checkpoint`.\n        """"""\n        state_dict = load(checkpoint)\n        args = state_dict[\'args\']\n        results = pkl.loads(state_dict[\'results\'])\n        eval_func = state_dict[\'eval_func\']\n        scheduler_checkpoint = state_dict[\'scheduler_checkpoint\']\n        model_params = state_dict[\'model_params\']\n        ensemble = state_dict[\'ensemble\']\n\n        if ensemble <= 1:\n            model_args = copy.deepcopy(args)\n            model_args.update(results[\'best_config\'])\n            model = get_network(args.net, num_classes=results[\'num_classes\'], ctx=mx.cpu(0))\n            update_params(model, model_params)\n        else:\n            raise NotImplemented\n        return cls(model, results, eval_func, scheduler_checkpoint, args,\n                   ensemble, format_results=False)\n\n    def state_dict(self, destination=None):\n        if destination is None:\n            destination = OrderedDict()\n            destination._metadata = OrderedDict()\n        model_params = collect_params(self.model)\n        destination[\'model_params\'] = model_params\n        destination[\'eval_func\'] = self.eval_func\n        destination[\'results\'] = pkl.dumps(self.results)\n        destination[\'scheduler_checkpoint\'] = self.scheduler_checkpoint\n        destination[\'args\'] = self.args\n        destination[\'ensemble\'] = self.ensemble\n        return destination\n\n    def save(self, checkpoint):\n        """""" Save image classifier to folder specified by `checkpoint`.\n        """"""\n        state_dict = self.state_dict()\n        save(state_dict, checkpoint)\n\n    def predict(self, X, input_size=224, crop_ratio=0.875, set_prob_thresh=0.001, plot=False):\n        """"""Predict class-index and associated class probability for each image in a given dataset (or just a single image). \n        \n        Parameters\n        ----------\n        X : str or :class:`autogluon.task.ImageClassification.Dataset` or list of `autogluon.task.ImageClassification.Dataset`\n            If str, should be path to the input image (when we just want to predict on single image).\n            If class:`autogluon.task.ImageClassification.Dataset`, should be dataset of multiple images in same format as training dataset.\n            If list of `autogluon.task.ImageClassification.Dataset`, should be a set of test dataset with different scales of origin images.\n        input_size : int\n            Size of the images (pixels).\n        plot : bool\n            Whether to plot the image being classified.\n        set_prob_thresh: float\n            Results with probability below threshold are set to 0 by default.\n        Examples\n        --------\n        >>> from autogluon import ImageClassification as task\n        >>> train_data = task.Dataset(train_path=\'~/data/train\')\n        >>> classifier = task.fit(train_data,\n        >>>                       nets=ag.space.Categorical[\'resnet18_v1\', \'resnet34_v1\'],\n        >>>                       time_limits=600, ngpus_per_trial=1, num_trials=4)\n        >>> test_data = task.Dataset(\'~/data/test\', train=False)\n        >>> class_index, class_probability = classifier.predict(\'example.jpg\')\n        """"""\n\n        input_size = self.model.input_size if hasattr(self.model, \'input_size\') else input_size\n        resize = int(math.ceil(input_size / crop_ratio))\n\n        transform_size = transforms.Compose([\n            transforms.Resize(resize),\n            transforms.CenterCrop(input_size),\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n        ])\n\n        def predict_img(img, ensemble=False):\n            proba = self.predict_proba(img)\n            if ensemble:\n                return proba\n            else:\n                ind = mx.nd.argmax(proba, axis=1).astype(\'int\')\n                idx = mx.nd.stack(mx.nd.arange(proba.shape[0], ctx=proba.context), ind.astype(\'float32\'))\n                probai = mx.nd.gather_nd(proba, idx)\n                return ind, probai, proba\n\n        def avg_prediction(different_dataset, threshold=0.001):\n            result = defaultdict(list)\n            inds, probas, probals_all = [], [], []\n            for i in range(len(different_dataset)):\n                for j in range(len(different_dataset[0])):\n                    result[j].append(different_dataset[i][j])\n\n            for c in result.keys():\n                proba_all = sum([*result[c]]) / len(different_dataset)\n                proba_all = (proba_all >= threshold) * proba_all\n                ind = mx.nd.argmax(proba_all, axis=1).astype(\'int\')\n                idx = mx.nd.stack(mx.nd.arange(proba_all.shape[0], ctx=proba_all.context), ind.astype(\'float32\'))\n                proba = mx.nd.gather_nd(proba_all, idx)\n                inds.append(ind.asscalar())\n                probas.append(proba.asnumpy())\n                probals_all.append(proba_all.asnumpy().flatten())\n            return inds, probas, probals_all\n\n        def predict_imgs(X):\n            if isinstance(X, list):\n                different_dataset = []\n                for i, x in enumerate(X):\n                    proba_all_one_dataset = []\n                    tbar = tqdm(range(len(x.items)))\n                    for j, x_item in enumerate(x):\n                        tbar.update(1)\n                        proba_all = predict_img(x_item[0], ensemble=True)\n                        tbar.set_description(\'ratio:[%d],The input picture [%d]\' % (i, j))\n                        proba_all_one_dataset.append(proba_all)\n                    different_dataset.append(proba_all_one_dataset)\n                inds, probas, probals_all = avg_prediction(different_dataset, threshold=set_prob_thresh)\n            else:\n                inds, probas, probals_all = [], [], []\n                tbar = tqdm(range(len(X.items)))\n                for i, x in enumerate(X):\n                    tbar.update(1)\n                    ind, proba, proba_all = predict_img(x[0])\n                    tbar.set_description(\n                        \'The input picture [%d] is classified as [%d], with probability %.2f \' %\n                        (i, ind.asscalar(), proba.asscalar())\n                    )\n                    inds.append(ind.asscalar())\n                    probas.append(proba.asnumpy())\n                    probals_all.append(proba_all.asnumpy().flatten())\n            return inds, probas, probals_all\n\n        if isinstance(X, str) and os.path.isfile(X):\n            img = mx.image.imread(filename=X)\n            if plot:\n                plt.imshow(img.asnumpy())\n                plt.show()\n\n            img = transform_size(img)\n            return predict_img(img)\n\n        if isinstance(X, AutoGluonObject):\n            X = X.init()\n            return predict_imgs(X)\n\n        if isinstance(X, list) and len(X) > 1:\n            X_group = []\n            for X_item in X:\n                X_item = X_item.init()\n                X_group.append(X_item)\n            return predict_imgs(X_group)\n\n    @staticmethod\n    def loader(path):\n        with open(path, \'rb\') as f:\n            img = Image.open(f)\n            return img.convert(\'RGB\')\n\n    def predict_proba(self, X):\n        """"""Produces predicted class probabilities for a given image.\n        """"""\n        pred = self.model(X.expand_dims(0))\n        return mx.nd.softmax(pred)\n\n    def evaluate(self, dataset, input_size=224, ctx=[mx.cpu()]):\n        """"""Evaluate predictive performance of trained image classifier using given test data.\n        \n        Parameters\n        ----------\n        dataset : :class:`autogluon.task.ImageClassification.Dataset`\n            The dataset containing test images (must be in same format as the training dataset).\n        input_size : int\n            Size of the images (pixels).\n        ctx : List of mxnet.context elements.\n            Determines whether to use CPU or GPU(s), options include: `[mx.cpu()]` or `[mx.gpu()]`.\n        \n        Examples\n        --------\n        >>> from autogluon import ImageClassification as task\n        >>> train_data = task.Dataset(train_path=\'~/data/train\')\n        >>> classifier = task.fit(train_data,\n        >>>                       nets=ag.space.Categorical[\'resnet18_v1\', \'resnet34_v1\'],\n        >>>                       time_limits=600, ngpus_per_trial=1, num_trials = 4)\n        >>> test_data = task.Dataset(\'~/data/test\', train=False)\n        >>> test_acc = classifier.evaluate(test_data)\n        """"""\n        args = self.args\n        net = self.model\n        batch_size = args.batch_size * max(len(ctx), 1)\n        metric = get_metric_instance(args.metric)\n        input_size = net.input_size if hasattr(net, \'input_size\') else input_size\n\n        test_data, _, batch_fn, _ = get_data_loader(dataset, input_size, batch_size, args.num_workers, True, None)\n        tbar = tqdm(test_data)\n        for batch in tbar:\n            self.eval_func(net, batch, batch_fn, metric, ctx)\n            _, test_reward = metric.get()\n            tbar.set_description(\'{}: {}\'.format(args.metric, test_reward))\n        _, test_reward = metric.get()\n        return test_reward\n\n    def evaluate_predictions(self, y_true, y_pred):\n        raise NotImplementedError  # TODO\n'"
autogluon/task/image_classification/dataset.py,0,"b'import logging\nimport math\nimport os\nimport platform\nimport sys\nimport warnings\n\nimport numpy as np\nfrom PIL import Image\nfrom mxnet import gluon, nd\nfrom mxnet import recordio\nfrom mxnet.gluon.data import RecordFileDataset\nfrom mxnet.gluon.data.vision import ImageFolderDataset as MXImageFolderDataset\nfrom mxnet.gluon.data.vision import ImageRecordDataset, transforms\n\nfrom ...core import *\nfrom ...utils import get_data_rec\nfrom ...utils.pil_transforms import *\n\n_is_osx = platform.system() == ""Darwin""\n\n__all__ = [\n    \'get_dataset\', \'get_built_in_dataset\',\n    \'ImageFolderDataset\', \'RecordDataset\', \'NativeImageFolderDataset\'\n]\n\nlogger = logging.getLogger(__name__)\n\nbuilt_in_datasets = [\n    \'mnist\',\n    \'cifar\',\n    \'cifar10\',\n    \'cifar100\',\n    \'imagenet\',\n    \'fashionmnist\',\n]\n\n\nclass _TransformFirstClosure(object):\n    """"""Use callable object instead of nested function, it can be pickled.""""""\n\n    def __init__(self, fn):\n        self._fn = fn\n\n    def __call__(self, x, *args):\n        if args:\n            return (self._fn(x),) + args\n        return self._fn(x)\n\n\ndef generate_transform(train, resize, _is_osx, input_size, jitter_param):\n    if _is_osx:\n        # using PIL to load image (slow)\n        if train:\n            transform = Compose(\n                [\n                    RandomResizedCrop(input_size),\n                    RandomHorizontalFlip(),\n                    ColorJitter(0.4, 0.4, 0.4),\n                    ToTensor(),\n                    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n                ]\n            )\n        else:\n            transform = Compose(\n                [\n                    Resize(resize),\n                    CenterCrop(input_size),\n                    ToTensor(),\n                    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n                ]\n            )\n    else:\n        if train:\n            transform = transforms.Compose(\n                [\n                    transforms.RandomResizedCrop(input_size),\n                    transforms.RandomFlipLeftRight(),\n                    transforms.RandomColorJitter(\n                        brightness=jitter_param,\n                        contrast=jitter_param,\n                        saturation=jitter_param\n                    ),\n                    transforms.RandomLighting(0.1),\n                    transforms.ToTensor(),\n                    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n                ]\n            )\n        else:\n            transform = transforms.Compose(\n                [\n                    transforms.Resize(resize),\n                    transforms.CenterCrop(input_size),\n                    transforms.ToTensor(),\n                    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n                ]\n            )\n    return transform\n\n\n@func()\ndef get_dataset(path=None, train=True, name=None,\n                input_size=224, crop_ratio=0.875, jitter_param=0.4, scale_ratio_choice=[],\n                *args, **kwargs):\n    """""" Method to produce image classification dataset for AutoGluon, can either be a\n    :class:`ImageFolderDataset`, :class:`RecordDataset`, or a\n    popular dataset already built into AutoGluon (\'mnist\', \'cifar10\', \'cifar100\', \'imagenet\').\n\n    Parameters\n    ----------\n    name : str, optional\n        Which built-in dataset\xc2\xa0to use, will override all other options if specified.\n        The options are (\'mnist\', \'cifar\', \'cifar10\', \'cifar100\', \'imagenet\')\n    train : bool, default = True\n        Whether this dataset should be used for training or validation.\n    path : str\n        The training data location. If using :class:`ImageFolderDataset`,\n        image folder`path/to/the/folder` should be provided.\n        If using :class:`RecordDataset`, the `path/to/*.rec` should be provided.\n    input_size : int\n        The input image size.\n    crop_ratio : float\n        Center crop ratio (for evaluation only)\n    scale_ratio_choice: list\n        List of crop_ratio, only in the test dataset, the set of scaling ratios obtained is scaled to the original image, and then cut a fixed size (input_size) and get a set of predictions for averaging.\n\n    Returns\n    -------\n    Dataset object that can be passed to `task.fit()`, which is actually an :class:`autogluon.space.AutoGluonObject`.\n    To interact with such an object yourself, you must first call `Dataset.init()` to instantiate the object in Python.\n    """"""\n\n    resize = int(math.ceil(input_size / crop_ratio))\n    transform = generate_transform(train, resize, _is_osx, input_size, jitter_param)\n\n    if isinstance(name, str) and name.lower() in built_in_datasets:\n        return get_built_in_dataset(name, train=train, input_size=input_size, *args, **kwargs)\n\n    if \'.rec\' in path:\n        dataset = RecordDataset(\n            path,\n            *args,\n            transform=_TransformFirstClosure(transform),\n            **kwargs\n        )\n    elif _is_osx:\n        dataset = ImageFolderDataset(path, transform=transform, *args, **kwargs)\n    elif not train:\n        if not scale_ratio_choice:\n            dataset = TestImageFolderDataset(\n                path,\n                *args,\n                transform=_TransformFirstClosure(transform),\n                **kwargs\n            )\n        else:\n            dataset = []\n            for i in scale_ratio_choice:\n                resize = int(math.ceil(input_size / i))\n                dataset_item = TestImageFolderDataset(\n                    path,\n                    *args,\n                    transform=_TransformFirstClosure(\n                        generate_transform(train, resize, _is_osx, input_size, jitter_param)\n                    ),\n                    **kwargs\n                )\n                dataset.append(dataset_item.init())\n\n    elif \'label_file\' in kwargs:\n        dataset = IndexImageDataset(\n            path,\n            transform=_TransformFirstClosure(transform),\n            *args,\n            **kwargs\n        )\n    else:\n        dataset = NativeImageFolderDataset(\n            path,\n            *args,\n            transform=_TransformFirstClosure(transform),\n            **kwargs\n        )\n\n    if not scale_ratio_choice:\n        dataset = dataset.init()\n    return dataset\n\n\n@obj()\nclass IndexImageDataset(MXImageFolderDataset):\n    """"""A image classification dataset with a CVS label file\n       Each sample is an image and its corresponding label.\n\n    Parameters\n    ----------\n    root : str\n        Path to the image folder.\n    indexfile : str\n        Local path to the csv index file. The CSV should have two collums\n        1. image name (e.g. xxxx or xxxx.jpg)\n        2. label name or index (e.g. aaa or 1)\n    gray_scale : False\n        If True, always convert images to greyscale.\n        If False, always convert images to colored (RGB).\n    transform : function, default None\n        A user defined callback that transforms each sample.\n    """"""\n\n    def __init__(self, root, label_file, gray_scale=False, transform=None,\n                 extension=\'.jpg\'):\n        self._root = os.path.expanduser(root)\n        self.items, self.synsets = self.read_csv(label_file, root, extension)\n        self._flag = 0 if gray_scale else 1\n        self._transform = transform\n\n    @staticmethod\n    def read_csv(filename, root, extension):\n        """"""The CSV should have two collums\n        1. image name (e.g. xxxx or xxxx.jpg)\n        2. label name or index (e.g. aaa or 1)\n        """"""\n\n        def label_to_index(label_list, name):\n            return label_list.index(name)\n\n        import csv\n        label_dict = {}\n        with open(filename) as f:\n            reader = csv.reader(f)\n            for row in reader:\n                assert len(row) == 2\n                label_dict[row[0]] = row[1]\n\n        if \'id\' in label_dict:\n            label_dict.pop(\'id\')\n\n        labels = list(set(label_dict.values()))\n        samples = [\n            (os.path.join(root, f""{k}{extension}""), label_to_index(labels, v))\n            for k, v in label_dict.items()\n        ]\n        return samples, labels\n\n    @property\n    def num_classes(self):\n        return len(self.synsets)\n\n    @property\n    def classes(self):\n        return self.synsets\n\n    @property\n    def num_classes(self):\n        return len(self.synsets)\n\n    @property\n    def classes(self):\n        return self.synsets\n\n\n@obj()\nclass RecordDataset:\n    """"""A dataset wrapping over a RecordIO file containing images.\n       Each sample is an image and its corresponding label.\n\n    Parameters\n    ----------\n    filename : str\n        Local path to the .rec file.\n    gray_scale : False\n        If True, always convert images to greyscale.\n        If False, always convert images to colored (RGB).\n    transform : function, default None\n        A user defined callback that transforms each sample.\n    classes : iterable of str, default is None\n        User provided class names. If `None` is provide, will use\n        a list of increasing natural number [\'0\', \'1\', ..., \'N\'] by default.\n    """"""\n\n    def __init__(self, filename, gray_scale=False, transform=None, classes=None):\n        flag = 0 if gray_scale else 1\n        # retrieve number of classes without decoding images\n        td = RecordFileDataset(filename)\n        s = set([recordio.unpack(td.__getitem__(i))[0].label[0] for i in range(len(td))])\n        self._num_classes = len(s)\n        if not classes:\n            self._classes = [str(i) for i in range(self._num_classes)]\n        else:\n            if len(self._num_classes) != len(classes):\n                warnings.warn(\'Provided class names do not match data, expected ""num_class"" is {} \'\n                              \'vs. provided: {}\'.format(self._num_classes, len(classes)))\n                self._classes = list(classes) + \\\n                    [str(i) for i in range(len(classes), self._num_classes)]\n        self._dataset = ImageRecordDataset(filename, flag=flag)\n        if transform:\n            self._dataset = self._dataset.transform_first(transform)\n\n    @property\n    def num_classes(self):\n        return self._num_classes\n\n    @property\n    def classes(self):\n        return self._classes\n\n    def __len__(self):\n        return len(self._dataset)\n\n    def __getitem__(self, idx):\n        return self._dataset[idx]\n\n\n@obj()\nclass NativeImageFolderDataset(MXImageFolderDataset):\n    def __init__(self, root, gray_scale=False, transform=None):\n        flag = 0 if gray_scale else 1\n        super().__init__(root, flag=flag, transform=transform)\n\n    @property\n    def num_classes(self):\n        return len(self.synsets)\n\n    @property\n    def classes(self):\n        return self.synsets\n\n\n@obj()\nclass TestImageFolderDataset(MXImageFolderDataset):\n    def __init__(self, root, gray_scale=False, transform=None):\n        flag = 0 if gray_scale else 1\n        super().__init__(root, flag=flag, transform=transform)\n\n    def _list_images(self, root):\n        self.synsets = []\n        self.items = []\n        path = os.path.expanduser(root)\n        if not os.path.isdir(path):\n            raise ValueError(\'Ignoring %s, which is not a directory.\' % path, stacklevel=3)\n        for filename in sorted(os.listdir(path)):\n            filename = os.path.join(path, filename)\n            if os.path.isfile(filename):  # add\n                label = len(self.synsets)\n                ext = os.path.splitext(filename)[1]\n                if ext.lower() not in self._exts:\n                    warnings.warn(\n                        f\'Ignoring {filename} of type {ext}.\'\n                        f\' Only support {"", "".join(self._exts)}\'\n                    )\n                    continue\n                self.items.append((filename, label))\n            else:\n                folder = filename\n                if not os.path.isdir(folder):\n                    raise ValueError(f\'Ignoring {path}, which is not a directory.\', stacklevel=3)\n                label = len(self.synsets)\n                for sub_filename in sorted(os.listdir(folder)):\n                    sub_filename = os.path.join(folder, sub_filename)\n                    ext = os.path.splitext(sub_filename)[1]\n                    if ext.lower() not in self._exts:\n                        warnings.warn(\n                            f\'Ignoring {sub_filename} of type {ext}.\'\n                            f\' Only support {"", "".join(self._exts)}\'\n                        )\n                        continue\n                    self.items.append((sub_filename, label))\n                self.synsets.append(label)\n\n    @property\n    def num_classes(self):\n        return len(self.synsets)\n\n    @property\n    def classes(self):\n        return self.synsets\n\n\n@obj()\nclass ImageFolderDataset(object):\n    """"""A generic data loader where the images are arranged in this way on your local filesystem: ::\n\n        root/dog/a.png\n        root/dog/b.png\n        root/dog/c.png\n\n        root/cat/x.png\n        root/cat/y.png\n        root/cat/z.png\n\n    Here, folder-names `dog` and `cat` are the class labels and the images with file-names \'a\', `b`, `c` belong to the `dog` class while the others are `cat` images.\n\n    Parameters\n    ----------\n    root : string\n        Root directory path to the folder containing all of the data.\n    transform : callable (optional)\n        A function/transform that  takes in an PIL image\n        and returns a transformed version. E.g, ``transforms.RandomCrop``\n    is_valid_file : callable (optional)\n        A function that takes path of an Image file\n        and check if the file is a valid file (used to check of corrupt files)\n\n    Attributes\n    ----------\n    classes : list\n        List of the class names.\n    class_to_idx : dict\n        Dict with items (class_name, class_index).\n    imgs : list\n        List of (image path, class_index) tuples\n    """"""\n    _repr_indent = 4\n    IMG_EXTENSIONS = (\'.jpg\', \'.jpeg\', \'.png\', \'.ppm\', \'.bmp\', \'.pgm\', \'.tif\', \'.tiff\', \'.webp\')\n\n    def __init__(self, root, extensions=None, transform=None, is_valid_file=None):\n        root = os.path.expanduser(root)\n        self.root = root\n        extensions = extensions if extensions else self.IMG_EXTENSIONS\n\n        self._transform = transform\n        classes, class_to_idx = self._find_classes(self.root)\n        samples = self.make_dataset(self.root, class_to_idx, extensions, is_valid_file)\n        if len(samples) == 0:\n            raise RuntimeError(\n                f""Found 0 files in subfolders of:  {self.root} ""\n                f""\\nSupported extensions are:  {\',\'.join(extensions)}""\n            )\n\n        self.extensions = extensions\n\n        self.classes = classes\n        self.class_to_idx = class_to_idx\n        self.samples = samples\n        self.targets = [s[1] for s in samples]\n        self.imgs = self.samples\n\n    @staticmethod\n    def make_dataset(dir, class_to_idx, extensions=None, is_valid_file=None):\n        images = []\n        dir = os.path.expanduser(dir)\n        if not ((extensions is None) ^ (is_valid_file is None)):\n            raise ValueError(""Both extensions and is_valid_file cannot be None or not None at the same time"")\n\n        if extensions is not None:\n\n            def is_valid_file(x):\n                if not x.lower().endswith(extensions):\n                    return False\n                valid = True\n                try:\n                    with open(x, \'rb\') as f:\n                        Image.open(f)\n                except OSError:\n                    valid = False\n                return valid\n\n        for target in sorted(class_to_idx.keys()):\n            d = os.path.join(dir, target)\n            if not os.path.isdir(d):\n                continue\n            for root, _, fnames in sorted(os.walk(d)):\n                for fname in sorted(fnames):\n                    path = os.path.abspath(os.path.join(root, fname))\n                    if is_valid_file(path):\n                        item = (path, class_to_idx[target])\n                        images.append(item)\n        if not class_to_idx:\n            for root, _, fnames in sorted(os.walk(dir)):\n                for fname in sorted(fnames):\n                    path = os.path.abspath(os.path.join(root, fname))\n                    if is_valid_file(path):\n                        item = (path, 0)\n                        images.append(item)\n        return images\n\n    @staticmethod\n    def loader(path):\n        with open(path, \'rb\') as f:\n            img = Image.open(f)\n            return img.convert(\'RGB\')\n\n    @staticmethod\n    def _find_classes(dir):\n        """"""Finds the class folders in a dataset.\n\n        Parameters\n        ----------\n        dir : string\n            Root directory path.\n\n        Returns\n        -------\n        tuple: (classes, class_to_idx)\n            where classes are relative to (dir), and class_to_idx is a dictionary.\n        """"""\n        if sys.version_info >= (3, 5):\n            # Faster and available in Python 3.5 and above\n            classes = [d.name for d in os.scandir(dir) if d.is_dir()]\n        else:\n            classes = [d for d in os.listdir(dir) if os.path.isdir(os.path.join(dir, d))]\n        classes.sort()\n        class_to_idx = {classes[i]: i for i in range(len(classes))}\n        return classes, class_to_idx\n\n    @property\n    def num_classes(self):\n        return len(self.classes)\n\n    def __getitem__(self, index):\n        """"""\n        Parameters\n        ----------\n        index : int\n            Index\n\n        Returns\n        ----------\n        tuple : (sample, target)\n            where target is class_index of the target class.\n        """"""\n        path, target = self.samples[index]\n        sample = self.loader(path)\n        if self._transform is not None:\n            sample = self._transform(sample)\n\n        return sample, target\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __repr__(self):\n        head = ""Dataset "" + self.__class__.__name__\n        body = [f""Number of datapoints: {self.__len__()}""]\n        if self.root is not None:\n            body.append(f""Root location: {self.root}"")\n        lines = [head] + ["" "" * self._repr_indent + line for line in body]\n        return \'\\n\'.join(lines)\n\n\ndef get_built_in_dataset(name, train=True, input_size=224, batch_size=256, num_workers=32,\n                         shuffle=True, **kwargs):\n    """"""Returns built-in popular image classification dataset based on provided string name (\'cifar10\', \'cifar100\',\'mnist\',\'imagenet\').\n    """"""\n    logger.info(f\'get_built_in_dataset {name}\')\n    name = name.lower()\n    if name in (\'cifar10\', \'cifar\'):\n        import gluoncv.data.transforms as gcv_transforms\n        if train:\n            transform_split = transforms.Compose(\n                [\n                    gcv_transforms.RandomCrop(32, pad=4),\n                    transforms.RandomFlipLeftRight(),\n                    transforms.ToTensor(),\n                    transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])\n                ]\n            )\n        else:\n            transform_split = transforms.Compose(\n                [\n                    transforms.ToTensor(),\n                    transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])\n                ]\n            )\n        return gluon.data.vision.CIFAR10(train=train).transform_first(transform_split)\n    elif name == \'cifar100\':\n        import gluoncv.data.transforms as gcv_transforms\n        if train:\n            transform_split = transforms.Compose(\n                [\n                    gcv_transforms.RandomCrop(32, pad=4),\n                    transforms.RandomFlipLeftRight(),\n                    transforms.ToTensor(),\n                    transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])\n                ]\n            )\n        else:\n            transform_split = transforms.Compose(\n                [\n                    transforms.ToTensor(),\n                    transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])\n                ]\n            )\n        return gluon.data.vision.CIFAR100(train=train).transform_first(transform_split)\n    elif name == \'mnist\':\n        def transform(data, label):\n            return nd.transpose(data.astype(np.float32), (2, 0, 1)) / 255, label.astype(np.float32)\n\n        return gluon.data.vision.MNIST(train=train, transform=transform)\n    elif name == \'fashionmnist\':\n        def transform(data, label):\n            return nd.transpose(data.astype(np.float32), (2, 0, 1)) / 255, label.astype(np.float32)\n\n        return gluon.data.vision.FashionMNIST(train=train, transform=transform)\n    elif name == \'imagenet\':\n        # Please setup the ImageNet dataset following the tutorial from GluonCV\n        if train:\n            rec_file = \'/media/ramdisk/rec/train.rec\'\n            rec_file_idx = \'/media/ramdisk/rec/train.idx\'\n        else:\n            rec_file = \'/media/ramdisk/rec/val.rec\'\n            rec_file_idx = \'/media/ramdisk/rec/val.idx\'\n        data_loader = get_data_rec(input_size, 0.875, rec_file, rec_file_idx,\n                                   batch_size, num_workers, train, shuffle=shuffle,\n                                   **kwargs)\n        return data_loader\n    else:\n        raise NotImplementedError\n'"
autogluon/task/image_classification/image_classification.py,0,"b'import logging\nimport os\nimport copy\nimport mxnet as mx\n\nfrom .classifier import Classifier\nfrom .dataset import get_dataset\nfrom .nets import *\nfrom .pipeline import train_image_classification\nfrom .utils import *\nfrom ..base import BaseTask, compile_scheduler_options, create_scheduler\nfrom ...core import *\nfrom ...core.loss import *\nfrom ...core.optimizer import *\nfrom ...scheduler.resource import get_cpu_count, get_gpu_count\nfrom ...utils import update_params\n\n__all__ = [\'ImageClassification\']\n\nlogger = logging.getLogger(__name__)\n\n\nclass ImageClassification(BaseTask):\n    """"""AutoGluon Task for classifying images based on their content\n    """"""\n    Classifier = Classifier\n\n    @staticmethod\n    def Dataset(path=None, name=None, train=True, input_size=224, crop_ratio=0.875, *args, **kwargs):\n        """"""Dataset for AutoGluon image classification tasks.\n           May either be a :class:`autogluon.task.image_classification.ImageFolderDataset`, :class:`autogluon.task.image_classification.RecordDataset`,\n           or a popular dataset already built into AutoGluon (\'mnist\', \'fashionmnist\', \'cifar10\', \'cifar100\', \'imagenet\').\n\n        Parameters\n        ----------\n        path : str, optional\n            The data location. If using :class:`ImageFolderDataset`,\n            image folder`path/to/the/folder` should be provided.\n            If using :class:`RecordDataset`, the `path/to/*.rec` should be provided.\n        name : str, optional\n            Which built-in dataset\xc2\xa0to use, will override all other options if specified.\n            The options are: \'mnist\', \'fashionmnist\', \'cifar\', \'cifar10\', \'cifar100\', \'imagenet\'\n        train : bool, optional, default = True\n            Whether this dataset should be used for training or validation.\n        input_size : int\n            The input image size.\n        crop_ratio : float\n            Center crop ratio (for evaluation only).\n\n        Returns\n        -------\n        Dataset object that can be passed to `task.fit()`, which is actually an :class:`autogluon.space.AutoGluonObject`.\n        To interact with such an object yourself, you must first call `Dataset.init()` to instantiate the object in Python.\n        """"""\n        if name is None:\n            if path is None:\n                raise ValueError(""Either `path` or `name` must be present in Dataset(). ""\n                                 ""If `name` is provided, it will override the rest of the arguments."")\n        return get_dataset(path=path, train=train, name=name,\n                           input_size=input_size, crop_ratio=crop_ratio,\n                           *args, **kwargs)\n\n    @staticmethod\n    def fit(dataset,\n            net=Categorical(\'ResNet50_v1b\', \'ResNet18_v1b\'),\n            optimizer=NAG(\n                learning_rate=Real(1e-3, 1e-2, log=True),\n                wd=Real(1e-4, 1e-3, log=True),\n                multi_precision=False\n            ),\n            loss=SoftmaxCrossEntropyLoss(),\n            split_ratio=0.8,\n            batch_size=64,\n            input_size=224,\n            epochs=20,\n            final_fit_epochs=None,\n            ensemble=1,\n            metric=\'accuracy\',\n            nthreads_per_trial=60,\n            ngpus_per_trial=1,\n            hybridize=True,\n            scheduler_options=None,\n            search_strategy=\'random\',\n            search_options=None,\n            plot_results=False,\n            verbose=False,\n            num_trials=None,\n            time_limits=None,\n            resume=False,\n            output_directory=\'checkpoint/\',\n            visualizer=\'none\',\n            dist_ip_addrs=None,\n            auto_search=True,\n            lr_config=Dict(\n                lr_mode=\'cosine\',\n                lr_decay=0.1,\n                lr_decay_period=0,\n                lr_decay_epoch=\'40,80\',\n                warmup_lr=0.0,\n                warmup_epochs=0\n            ),\n            tricks=Dict(\n                last_gamma=False,\n                use_pretrained=True,\n                use_se=False,\n                mixup=False,\n                mixup_alpha=0.2,\n                mixup_off_epoch=0,\n                label_smoothing=False,\n                no_wd=False,\n                teacher_name=None,\n                temperature=20.0,\n                hard_weight=0.5,\n                batch_norm=False,\n                use_gn=False),\n            **kwargs):\n        # TODO: ensemble and hybridize are not in docstring\n        """"""\n        Fit image classification models to a given dataset.\n\n        Parameters\n        ----------\n        dataset : str or :meth:`autogluon.task.ImageClassification.Dataset`\n            Training dataset containing images and their associated class labels.\n            Popular image datasets built into AutoGluon can be used by specifying their name as a string (options: \xe2\x80\x98mnist\xe2\x80\x99, \xe2\x80\x98fashionmnist\xe2\x80\x99, \xe2\x80\x98cifar\xe2\x80\x99, \xe2\x80\x98cifar10\xe2\x80\x99, \xe2\x80\x98cifar100\xe2\x80\x99, \xe2\x80\x98imagenet\xe2\x80\x99).\n        input_size : int\n            Size of images in the dataset (pixels).\n        net : str or :class:`autogluon.space.Categorical`\n            Which existing neural network models to consider as candidates.\n        optimizer : str or :class:`autogluon.space.AutoGluonObject`\n            Which optimizers to consider as candidates for learning the neural network weights.\n        batch_size : int\n            How many images to group in each mini-batch during gradient computations in training.\n        epochs: int\n            How many epochs to train the neural networks for at most.\n        final_fit_epochs: int, default None\n            Final fit epochs, the same number of epochs will be used as during the HPO if not specified.\n        metric : str or callable object\n            Evaluation metric by which predictions will be ulitmately evaluated on test data.\n        loss : `mxnet.gluon.loss`\n            Loss function used during training of the neural network weights.\n        num_trials : int\n            Maximal number of hyperparameter configurations to try out.\n        time_limits : int\n            Approximately how long `fit()` should run for (wallclock time in seconds).\n            `fit()` will stop training new models after this amount of time has elapsed (but models which have already started training will continue to completion).\n        split_ratio : float, default = 0.8\n            Fraction of dataset to use for training (rest of data is held-out for tuning hyperparameters).\n            The final returned model may be fit to all of the data (after hyperparameters have been selected).\n        nthreads_per_trial : int\n            How many CPUs to use in each trial (ie. single training run of a model).\n        ngpus_per_trial : int\n            How many GPUs to use in each trial (ie. single training run of a model).\n        output_directory : str\n            Checkpoints of the search state are written to\n            os.path.join(output_directory, \'exp1.ag\')\n        scheduler_options : dict\n            Extra arguments passed to __init__ of scheduler, to configure the\n            orchestration of training jobs during hyperparameter-tuning.\n        search_strategy : str\n            Which hyperparameter search algorithm to use.\n            Options include: \'random\' (random search), \'skopt\' (SKopt Bayesian\n            optimization), \'grid\' (grid search), \'hyperband\' (Hyperband random),\n            \'rl\' (reinforcement learner).\n        search_options : dict\n            Auxiliary keyword arguments to pass to the searcher that performs\n            hyperparameter optimization.\n        resume : bool\n            If True, the hyperparameter search is started from state loaded from\n            os.path.join(output_directory, \'exp1.ag\')\n        dist_ip_addrs : list\n            List of IP addresses corresponding to remote workers, in order to leverage distributed computation.\n        verbose : bool\n            Whether or not to print out intermediate information during training.\n        plot_results : bool\n            Whether or not to generate plots summarizing training process.\n        visualizer : str\n            Describes method to visualize training progress during `fit()`. Options: [\'mxboard\', \'tensorboard\', \'none\'].\n        auto_search : bool\n            If True, enables automatic suggestion of network types and hyper-parameter ranges adaptively based on provided dataset.\n\n        Returns\n        -------\n            :class:`autogluon.task.image_classification.Classifier` object which can make predictions on new data and summarize what happened during `fit()`.\n\n        Examples\n        --------\n        >>> from autogluon import ImageClassification as task\n        >>> dataset = task.Dataset(train_path=\'data/train\',\n        >>>                        test_path=\'data/test\')\n        >>> classifier = task.fit(dataset,\n        >>>                       nets=ag.space.Categorical[\'resnet18_v1\', \'resnet34_v1\'],\n        >>>                       time_limits=time_limits,\n        >>>                       ngpus_per_trial=1,\n        >>>                       num_trials = 4)\n        >>> test_data = task.Dataset(\'~/data/test\', train=False)\n        >>> test_acc = classifier.evaluate(test_data)\n\n\n        Bag of tricks are used on image classification dataset\n\n        lr_config\n        ----------\n        lr-mode : type=str, default=\'step\'.\n            describes how learning rate should be adjusted over the course of training. Options include: \'cosine\', \'poly\'.\n        lr-decay : type=float, default=0.1.\n            decay rate of learning rate. default is 0.1.\n        lr-decay-period : type=int, default=0.\n            interval for periodic learning rate decays. default is 0 to disable.\n        lr-decay-epoch : type=str, default=\'10,20,30\'.\n            epochs at which learning rate decays. epochs=40, default is 10, 20, 30.\n        warmup-lr : type=float, default=0.0.\n            starting warmup learning rate. default is 0.0.\n        warmup-epochs : type=int, default=0.\n            number of warmup epochs.\n\n        tricks\n        ----------\n        last-gamma\', default= True.\n            whether to init gamma of the last BN layer in each bottleneck to 0.\n        use-pretrained\', default= True.\n            enable using pretrained model from gluon.\n        use_se\', default= False.\n            use SE layers or not in resnext. default is false.\n        mixup\', default= False.\n            whether train the model with mix-up. default is false.\n        mixup-alpha\', type=float, default=0.2.\n            beta distribution parameter for mixup sampling, default is 0.2.\n        mixup-off-epoch\', type=int, default=0.\n            how many last epochs to train without mixup, default is 0.\n        label-smoothing\', default= True.\n            use label smoothing or not in training. default is false.\n        no-wd\', default= True.\n            whether to remove weight decay on bias, and beta/gamma for batchnorm layers.\n        teacher\', type=str, default=None.\n            teacher model for distillation training\n        temperature\', type=float, default=20.\n            temperature parameter for distillation teacher model\n        hard-weight\', type=float, default=0.5.\n            weight for the loss of one-hot label for distillation training\n        batch-norm\', default= True.\n            enable batch normalization or not in vgg. default is false.\n        use-gn\', default= False.\n            whether to use group norm.\n        """"""\n        assert search_strategy not in {\'bayesopt\', \'bayesopt_hyperband\'}, \\\n            ""search_strategy == \'bayesopt\' or \'bayesopt_hyperband\' not yet supported""\n        checkpoint = os.path.join(output_directory, \'exp1.ag\')\n        if auto_search:\n            # The strategies can be injected here, for example: automatic suggest some hps\n            # based on the dataset statistics\n            net = auto_suggest_network(dataset, net)\n\n        nthreads_per_trial = get_cpu_count() if nthreads_per_trial > get_cpu_count() else nthreads_per_trial\n        ngpus_per_trial = get_gpu_count() if ngpus_per_trial > get_gpu_count() else ngpus_per_trial\n\n        # If only time_limits is given, the scheduler starts trials until the\n        # time limit is reached\n        if num_trials is None and time_limits is None:\n            num_trials = 2\n\n        final_fit_epochs = final_fit_epochs if final_fit_epochs else epochs\n        train_image_classification.register_args(\n            dataset=dataset,\n            net=net,\n            optimizer=optimizer,\n            loss=loss,\n            metric=metric,\n            num_gpus=ngpus_per_trial,\n            split_ratio=split_ratio,\n            batch_size=batch_size,\n            input_size=input_size,\n            epochs=epochs,\n            final_fit_epochs=final_fit_epochs,\n            verbose=verbose,\n            num_workers=nthreads_per_trial,\n            hybridize=hybridize,\n            final_fit=False,\n            tricks=tricks,\n            lr_config=lr_config\n        )\n\n        # Backward compatibility:\n        grace_period = kwargs.get(\'grace_period\')\n        if grace_period is not None:\n            if scheduler_options is None:\n                scheduler_options = {\'grace_period\': grace_period}\n            else:\n                assert \'grace_period\' not in scheduler_options, \\\n                    ""grace_period appears both in scheduler_options and as direct argument""\n                scheduler_options = copy.copy(scheduler_options)\n                scheduler_options[\'grace_period\'] = grace_period\n            logger.warning(\n                ""grace_period is deprecated, use ""\n                ""scheduler_options={\'grace_period\': ...} instead"")\n        scheduler_options = compile_scheduler_options(\n            scheduler_options=scheduler_options,\n            search_strategy=search_strategy,\n            search_options=search_options,\n            nthreads_per_trial=nthreads_per_trial,\n            ngpus_per_trial=ngpus_per_trial,\n            checkpoint=checkpoint,\n            num_trials=num_trials,\n            time_out=time_limits,\n            resume=resume,\n            visualizer=visualizer,\n            time_attr=\'epoch\',\n            reward_attr=\'classification_reward\',\n            dist_ip_addrs=dist_ip_addrs,\n            epochs=epochs)\n        results = BaseTask.run_fit(\n            train_image_classification, search_strategy, scheduler_options,\n            plot_results=plot_results)\n        args = sample_config(train_image_classification.args, results[\'best_config\'])\n\n        kwargs = {\'num_classes\': results[\'num_classes\'], \'ctx\': mx.cpu(0)}\n        model = get_network(args.net, **kwargs)\n        multi_precision = optimizer.kwvars[\'multi_precision\'] if \'multi_precision\' in optimizer.kwvars else False\n        update_params(model, results.pop(\'model_params\'), multi_precision)\n\n        if ensemble > 1:\n            models = [model]\n            scheduler = create_scheduler(\n                train_image_classification, search_strategy, scheduler_options)\n            for i in range(1, ensemble):\n                resultsi = scheduler.run_with_config(results[\'best_config\'])\n                kwargs = {\n                    \'num_classes\': resultsi[\'num_classes\'], \'ctx\': mx.cpu(0)}\n                model = get_network(args.net, **kwargs)\n                update_params(\n                    model, resultsi.pop(\'model_params\'), multi_precision)\n                models.append(model)\n            model = Ensemble(models)\n\n        results.pop(\'args\')\n        args.pop(\'optimizer\')\n        args.pop(\'dataset\')\n        args.pop(\'loss\')\n        return Classifier(model, results, default_val_fn, checkpoint, args)\n'"
autogluon/task/image_classification/losses.py,0,"b'from mxnet import gluon\nimport gluoncv\n\n__all__ = [\'get_loss_instance\']\n\n#TODO: abstract general loss shared across tasks\nlosses = {\'SoftmaxCrossEntropyLoss\': gluon.loss.SoftmaxCrossEntropyLoss,\n          \'L2Loss\': gluon.loss.L2Loss,\n          \'L1Loss\': gluon.loss.L1Loss,\n          \'SigmoidBinaryCrossEntropyLoss\': gluon.loss.SigmoidBinaryCrossEntropyLoss,\n          \'KLDivLoss\': gluon.loss.KLDivLoss,\n          \'HuberLoss\': gluon.loss.HuberLoss,\n          \'HingeLoss\': gluon.loss.HingeLoss,\n          \'SquaredHingeLoss\': gluon.loss.SquaredHingeLoss,\n          \'LogisticLoss\': gluon.loss.LogisticLoss,\n          \'TripletLoss\': gluon.loss.TripletLoss,\n          \'CTCLoss\': gluon.loss.CTCLoss,\n          \'CosineEmbeddingLoss\': gluon.loss.CosineEmbeddingLoss,\n          \'PoissonNLLLoss\': gluon.loss.SoftmaxCrossEntropyLoss,\n          \'DistillationSoftmaxCrossEntropyLoss\': gluoncv.loss.SoftmaxCrossEntropyLoss,\n          \'MixSoftmaxCrossEntropyLoss\': gluoncv.loss.SoftmaxCrossEntropyLoss}\n\ndef get_loss_instance(name, **kwargs):\n    """"""Returns a loss instance by name\n\n    Args:\n        name : str\n            Name of the loss.\n    """"""\n    if name not in losses and name.lower() not in losses:\n        err_str = \'""%s"" is not among the following loss list:\\n\\t\' % (name)\n        err_str += \'%s\' % (\'\\n\\t\'.join(sorted(losses.keys())))\n        raise ValueError(err_str)\n    loss = losses[name]()\n    return loss\n\n'"
autogluon/task/image_classification/metrics.py,0,"b'import mxnet\nimport gluoncv\n\n#from ...metric import autogluon_metrics, Metric\n\n__all__ = [\'get_metric_instance\']\n\nmetrics = {\'accuracy\': mxnet.metric.Accuracy,\n           \'topkaccuracy\': mxnet.metric.TopKAccuracy,\n           \'mae\': mxnet.metric.MAE,\n           \'mse\': mxnet.metric.MSE,\n           \'rmse\': mxnet.metric.RMSE,\n           \'crossentropy\': mxnet.metric.CrossEntropy}\n\ndef get_metric_instance(name, **kwargs):\n    """"""Returns a metric instance by name\n\n    Args:\n        name : str\n            Name of the metric.\n    """"""\n    name = name.lower()\n    if name not in metrics:\n        err_str = \'""%s"" is not among the following metric list:\\n\\t\' % (name)\n        err_str += \'%s\' % (\'\\n\\t\'.join(sorted(metrics.keys())))\n        raise ValueError(err_str)\n    metric = metrics[name]()\n    return metric\n'"
autogluon/task/image_classification/nets.py,0,"b""import logging\nimport mxnet as mx\nfrom mxnet import gluon, init\nfrom mxnet.gluon import nn\n\nfrom ...model_zoo.model_zoo import get_model\nfrom ...core import *\n\nlogger = logging.getLogger(__name__)\n\n__all__ = ['Ensemble', 'get_network', 'auto_suggest_network']\n\nclass Ensemble(object):\n    def __init__(self, model_list):\n        self.model_list = model_list\n\n    def __call__(self, *inputs):\n        outputs = [model(*inputs) for model in self.model_list]\n        output = outputs[0].exp()\n        for i in range(1, len(outputs)):\n            output += outputs[i].exp()\n\n        output /= len(outputs)\n        return output.log()\n\nclass Identity(mx.gluon.HybridBlock):\n    def hybrid_forward(self, F, x):\n        return x\n\nclass ConvBNReLU(mx.gluon.HybridBlock):\n    def __init__(self, in_channels, channels, kernel, stride):\n        super().__init__()\n        padding = (kernel - 1) // 2\n        self.conv = nn.Conv2D(channels, kernel, stride, padding, in_channels=in_channels)\n        self.bn = nn.BatchNorm(in_channels=channels)\n        self.relu = nn.Activation('relu')\n    def hybrid_forward(self, F, x):\n        return self.relu(self.bn(self.conv(x)))\n\nclass ResUnit(mx.gluon.HybridBlock):\n    def __init__(self, in_channels, channels, hidden_channels, kernel, stride):\n        super().__init__()\n        self.conv1 = ConvBNReLU(in_channels, hidden_channels, kernel, stride)\n        self.conv2 = ConvBNReLU(hidden_channels, channels, kernel, 1)\n        if in_channels == channels and stride == 1:\n            self.shortcut = Identity()\n        else:\n            self.shortcut = nn.Conv2D(channels, 1, stride, in_channels=in_channels)\n    def hybrid_forward(self, F, x):\n        return self.conv2(self.conv1(x)) + self.shortcut(x)\n\n@func()\ndef mnist_net():\n    mnist_net = gluon.nn.Sequential()\n    mnist_net.add(ResUnit(1, 8, hidden_channels=8, kernel=3, stride=2))\n    mnist_net.add(ResUnit(8, 8, hidden_channels=8, kernel=5, stride=2))\n    mnist_net.add(ResUnit(8, 16, hidden_channels=8, kernel=3, stride=2))\n    mnist_net.add(nn.GlobalAvgPool2D())\n    mnist_net.add(nn.Flatten())\n    mnist_net.add(nn.Activation('relu'))\n    mnist_net.add(nn.Dense(10, in_units=16))\n    return mnist_net\n\ndef auto_suggest_network(dataset, net):\n    if isinstance(dataset, str):\n        dataset_name = dataset\n    elif isinstance(dataset, AutoGluonObject):\n        if 'name' in dataset.kwargs and dataset.kwargs['name'] is not None:\n            dataset_name = dataset.kwargs['name']\n        else:\n            return net\n    else:\n        return net\n    dataset_name = dataset_name.lower()\n    if 'mnist' in dataset_name:\n        if isinstance(net, str) or isinstance(net, Categorical):\n            net = mnist_net()\n            logger.info('Auto suggesting network net for dataset {}'.format(net, dataset_name))\n            return net\n    elif 'cifar' in dataset_name:\n        if isinstance(net, str):\n            if 'cifar' not in net:\n                net = 'cifar_resnet20_v1'\n        elif isinstance(net, Categorical):\n            newdata = []\n            for x in net.data:\n                if 'cifar' in x:\n                    newdata.append(x)\n            net.data = newdata if len(newdata) > 0 else ['cifar_resnet20_v1', 'cifar_resnet56_v1']\n        logger.info('Auto suggesting network net for dataset {}'.format(net, dataset_name))\n        return net\n\ndef get_network(net, **kwargs):\n    if type(net) == str:\n        net = get_built_in_network(net, **kwargs)\n    else:\n        net.initialize(ctx=kwargs['ctx'])\n    return net\n\ndef get_built_in_network(name, *args, **kwargs):\n    def _get_finetune_network(model_name, num_classes, ctx, **kwargs):\n        kwargs['pretrained'] = True\n        finetune_net = get_model(model_name, **kwargs)\n        # change the last fully connected layer to match the number of classes\n        with finetune_net.name_scope():\n            if hasattr(finetune_net, 'output'):\n                finetune_net.output = gluon.nn.Dense(num_classes)\n                finetune_net.output.initialize(init.Xavier(), ctx=ctx)\n            elif hasattr(finetune_net, '_fc'):\n                finetune_net._fc = gluon.nn.Dense(num_classes)\n                finetune_net._fc.initialize(init.Xavier(), ctx=ctx)\n            else:\n                assert hasattr(finetune_net, 'fc')\n                finetune_net.fc = gluon.nn.Dense(num_classes)\n                finetune_net.fc.initialize(init.Xavier(), ctx=ctx)\n        # initialize and context\n        finetune_net.collect_params().reset_ctx(ctx)\n        # finetune_net.load_parameters(opt.resume_params, ctx=context, cast_dtype=True)\n        finetune_net.hybridize()\n        return finetune_net\n\n    def _get_cifar_network(name, num_classes, ctx=mx.cpu(), *args, **kwargs):\n        name = name.lower()\n        assert 'cifar' in name\n        net = get_model(name, *args, **kwargs)\n        net.initialize(ctx=ctx)\n        return net\n\n    name = name.lower()\n    if 'cifar' in name:\n        return _get_cifar_network(name, *args, **kwargs)\n    else:\n        return _get_finetune_network(name, *args, **kwargs)\n"""
autogluon/task/image_classification/pipeline.py,0,"b""import logging\n\nimport mxnet as mx\nfrom gluoncv.loss import DistillationSoftmaxCrossEntropyLoss\nfrom mxnet import gluon, nd\n\nfrom .metrics import get_metric_instance\nfrom .processing_params import Sample_params, Getmodel_kwargs\nfrom .utils import *\nfrom ...core import *\nfrom ...utils import tqdm\nfrom ...utils.learning_rate import LR_params\nfrom ...utils.mxutils import collect_params\n\n__all__ = ['train_image_classification']\n\n\n@args()\ndef train_image_classification(args, reporter):\n    logging.basicConfig()\n    logger = logging.getLogger(__name__)\n    if args.verbose:\n        logger.setLevel(logging.INFO)\n        logger.info(args)\n\n    target_params = Sample_params(args.batch_size, args.num_gpus, args.num_workers)\n    batch_size = target_params.get_batchsize\n    ctx = target_params.get_context\n    classes = args.dataset.num_classes if hasattr(args.dataset, 'num_classes') else None\n    target_kwargs = Getmodel_kwargs(ctx,\n                                    classes,\n                                    args.net,\n                                    args.tricks.teacher_name,\n                                    args.tricks.hard_weight,\n                                    args.hybridize,\n                                    args.optimizer.multi_precision,\n                                    args.tricks.use_pretrained,\n                                    args.tricks.use_gn,\n                                    args.tricks.last_gamma,\n                                    args.tricks.batch_norm,\n                                    args.tricks.use_se)\n    distillation = target_kwargs.distillation\n    net = target_kwargs.get_net\n    input_size = net.input_size if hasattr(net, 'input_size') else args.input_size\n\n    if args.tricks.no_wd:\n        for k, v in net.collect_params('.*beta|.*gamma|.*bias').items():\n            v.wd_mult = 0.0\n\n    if args.tricks.label_smoothing or args.tricks.mixup:\n        sparse_label_loss = False\n    else:\n        sparse_label_loss = True\n\n    if distillation:\n        teacher = target_kwargs.get_teacher\n\n        def teacher_prob(data):\n            return [\n                nd.softmax(teacher(X.astype(target_kwargs.dtype, copy=False)) / args.tricks.temperature)\n                for X in data\n            ]\n\n        L = DistillationSoftmaxCrossEntropyLoss(temperature=args.tricks.temperature,\n                                                hard_weight=args.tricks.hard_weight,\n                                                sparse_label=sparse_label_loss)\n    else:\n        L = gluon.loss.SoftmaxCrossEntropyLoss(sparse_label=sparse_label_loss)\n        teacher_prob = None\n    if args.tricks.mixup:\n        metric = get_metric_instance('rmse')\n    else:\n        metric = get_metric_instance(args.metric)\n\n    train_data, val_data, batch_fn, num_batches = get_data_loader(\n        args.dataset, input_size, batch_size, args.num_workers, args.final_fit, args.split_ratio\n    )\n\n    if isinstance(args.lr_config.lr_mode, str):  # fix\n        target_lr = LR_params(args.optimizer.lr, args.lr_config.lr_mode, args.epochs, num_batches,\n                              args.lr_config.lr_decay_epoch,\n                              args.lr_config.lr_decay,\n                              args.lr_config.lr_decay_period,\n                              args.lr_config.warmup_epochs,\n                              args.lr_config.warmup_lr)\n        lr_scheduler = target_lr.get_lr_scheduler\n    else:\n        lr_scheduler = args.lr_config.lr_mode\n    args.optimizer.lr_scheduler = lr_scheduler\n\n    trainer = gluon.Trainer(net.collect_params(), args.optimizer)\n\n    def train(epoch, num_epochs, metric):\n        for i, batch in enumerate(train_data):\n            metric = default_train_fn(epoch, num_epochs, net, batch, batch_size, L, trainer,\n                                      batch_fn, ctx, args.tricks.mixup, args.tricks.label_smoothing,\n                                      distillation, args.tricks.mixup_alpha, args.tricks.mixup_off_epoch,\n                                      classes, target_kwargs.dtype, metric, teacher_prob)\n            mx.nd.waitall()\n        return metric\n\n    def test(epoch):\n        metric.reset()\n        for i, batch in enumerate(val_data):\n            default_val_fn(net, batch, batch_fn, metric, ctx, target_kwargs.dtype)\n        _, reward = metric.get()\n        reporter(epoch=epoch, classification_reward=reward)\n        return reward\n\n    # Note: epoch must start with 1, not 0\n    tbar = tqdm(range(1, args.epochs + 1))\n    for epoch in tbar:\n        metric = train(epoch, args.epochs, metric)\n        train_metric_name, train_metric_score = metric.get()\n        tbar.set_description(f'[Epoch {epoch}] training: {train_metric_name}={train_metric_score :.3f}')\n        if not args.final_fit:\n            reward = test(epoch)\n            tbar.set_description(f'[Epoch {epoch}] Validation: {reward :.3f}')\n\n    if args.final_fit:\n        return {'model_params': collect_params(net), 'num_classes': classes}\n"""
autogluon/task/image_classification/processing_params.py,0,"b'import mxnet as mx\n\nfrom .nets import get_network\n\n\nclass Sample_params(object):\n    propose = ""Sample params""\n\n    def __init__(self, *args):\n        batch_size, num_gpus, num_workers = args\n        self._batch_size = batch_size * max(1, num_gpus)\n        self._num_gpus = num_gpus\n        self._context = [mx.gpu(i) for i in range(num_gpus)] if num_gpus > 0 else [mx.cpu()]\n\n    @classmethod\n    def tell_info(cls):\n        print(""propose:"", cls.propose)\n\n    @property\n    def get_batchsize(self):\n        return self._batch_size\n\n    @property\n    def get_context(self):\n        return self._context\n\n\nclass Getmodel_kwargs:\n    def __init__(self,\n                 context,\n                 classes,\n                 model_name,\n                 model_teacher,\n                 hard_weight,\n                 hybridize,\n                 multi_precision=False,\n                 use_pretrained=True,\n                 use_gn=False,\n                 last_gamma=False,\n                 batch_norm=False,\n                 use_se=False):\n        self._kwargs = {\'ctx\': context, \'pretrained\': use_pretrained, \'num_classes\': classes}\n        self._model_name = model_name\n        self._model_teacher = model_teacher\n        self._hybridize = hybridize\n        self._hard_weight = hard_weight\n\n        if multi_precision:\n            self._dtype = \'float16\'\n        else:\n            self._dtype = \'float32\'\n\n        if use_gn:\n            from gluoncv.nn import GroupNorm\n            self._kwargs[\'norm_layer\'] = GroupNorm\n\n        if isinstance(model_name, str):\n            if model_name.startswith(\'vgg\'):\n                self._kwargs[\'batch_norm\'] = batch_norm\n            elif model_name.startswith(\'resnext\'):\n                self._kwargs[\'use_se\'] = use_se\n\n        if last_gamma:\n            self._kwargs[\'last_gamma\'] = True\n\n        if self._model_teacher is not None and self._hard_weight < 1.0:\n            self.distillation = True\n        else:\n            self.distillation = False\n\n    @property\n    def dtype(self):\n        return self._dtype\n\n    @property\n    def get_teacher(self):\n        net = get_network(self._model_teacher, **self._kwargs)\n        net.cast(self._dtype)\n        if self._hybridize:\n            net.hybridize(static_alloc=True, static_shape=True)\n        return net\n\n    @property\n    def get_net(self):\n        net = get_network(self._model_name, **self._kwargs)\n        net.cast(self._dtype)\n        if self._hybridize:\n            net.hybridize(static_alloc=True, static_shape=True)\n        return net\n'"
autogluon/task/image_classification/utils.py,0,"b'import mxnet as mx\nimport numpy as np\nfrom mxnet import gluon, nd\n\nfrom .dataset import *\nfrom ...core import AutoGluonObject\nfrom ...utils import get_split_samplers, SampledDataset, DataLoader\n\n__all__ = [\'get_data_loader\', \'imagenet_batch_fn\',\n           \'default_batch_fn\', \'default_val_fn\', \'default_train_fn\']\n\n\ndef get_data_loader(dataset, input_size, batch_size, num_workers, final_fit, split_ratio):\n    if isinstance(dataset, AutoGluonObject):\n        dataset = dataset.init()\n    elif isinstance(dataset, str):\n        dataset = get_built_in_dataset(dataset,\n                                       train=True,\n                                       input_size=input_size,\n                                       batch_size=batch_size,\n                                       num_workers=num_workers)\n    if final_fit:\n        train_dataset, val_dataset = dataset, None\n    else:\n        train_dataset, val_dataset = _train_val_split(dataset, split_ratio)\n\n    if isinstance(dataset, str) and dataset.lower() == \'imagenet\':\n        train_data = train_dataset\n        val_data = val_dataset\n        batch_fn = imagenet_batch_fn\n        imagenet_samples = 1281167\n        num_batches = imagenet_samples // batch_size\n    else:\n        num_workers = 0  # ?\n        train_data = DataLoader(\n            train_dataset, batch_size=batch_size, shuffle=True,\n            last_batch=""discard"", num_workers=num_workers\n        )\n        val_data = None\n        if not final_fit:\n            val_data = DataLoader(\n                val_dataset, batch_size=batch_size,\n                shuffle=False, num_workers=num_workers\n            )\n        batch_fn = default_batch_fn\n        num_batches = len(train_data)\n    return train_data, val_data, batch_fn, num_batches\n\n\ndef imagenet_batch_fn(batch, ctx):\n    data = gluon.utils.split_and_load(batch.data[0], ctx_list=ctx, batch_axis=0)\n    label = gluon.utils.split_and_load(batch.label[0], ctx_list=ctx, batch_axis=0)\n    return data, label\n\n\ndef default_batch_fn(batch, ctx):\n    data = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0, even_split=False)\n    label = gluon.utils.split_and_load(batch[1], ctx_list=ctx, batch_axis=0, even_split=False)\n    return data, label\n\n\ndef default_val_fn(net, batch, batch_fn, metric, ctx, dtype=\'float32\'):\n    with mx.autograd.pause(train_mode=False):\n        data, label = batch_fn(batch, ctx)\n        outputs = [net(X.astype(dtype, copy=False)) for X in data]\n    metric.update(label, outputs)\n\n\ndef mixup_transform(label, classes, lam=1, eta=0.0):\n    if isinstance(label, nd.NDArray):\n        label = [label]\n    res = []\n    for l in label:\n        y1 = l.one_hot(classes, on_value=1 - eta + eta / classes, off_value=eta / classes)\n        y2 = l[::-1].one_hot(classes, on_value=1 - eta + eta / classes, off_value=eta / classes)\n        res.append(lam * y1 + (1 - lam) * y2)\n    return res\n\n\ndef smooth(label, classes, eta=0.1):\n    if isinstance(label, nd.NDArray):\n        label = [label]\n    smoothed = [\n        l.one_hot(classes, on_value=1 - eta + eta / classes, off_value=eta / classes)\n        for l in label\n    ]\n    return smoothed\n\n\ndef default_train_fn(epoch, num_epochs, net, batch, batch_size, criterion, trainer, batch_fn, ctx,\n                     mixup=False, label_smoothing=False, distillation=False,\n                     mixup_alpha=0.2, mixup_off_epoch=0, classes=1000,\n                     dtype=\'float32\', metric=None, teacher_prob=None):\n    data, label = batch_fn(batch, ctx)\n    if mixup:\n        lam = np.random.beta(mixup_alpha, mixup_alpha)\n        if epoch >= num_epochs - mixup_off_epoch:\n            lam = 1\n        data = [lam * X + (1 - lam) * X[::-1] for X in data]\n        if label_smoothing:\n            eta = 0.1\n        else:\n            eta = 0.0\n        label = mixup_transform(label, classes, lam, eta)\n    elif label_smoothing:\n        hard_label = label\n        label = smooth(label, classes)\n\n    with mx.autograd.record():\n        outputs = [net(X.astype(dtype, copy=False)) for X in data]\n        if distillation:\n            loss = [\n                criterion(\n                    yhat.astype(\'float\', copy=False),\n                    y.astype(\'float\', copy=False),\n                    p.astype(\'float\', copy=False)\n                )\n                for yhat, y, p in zip(outputs, label, teacher_prob(data))\n            ]\n        else:\n            loss = [criterion(yhat, y.astype(dtype, copy=False)) for yhat, y in zip(outputs, label)]\n\n    for l in loss:\n        l.backward()\n    trainer.step(batch_size, ignore_stale_grad=True)\n\n    if metric:\n        if mixup:\n            output_softmax = [\n                nd.SoftmaxActivation(out.astype(\'float32\', copy=False))\n                for out in outputs\n            ]\n            metric.update(label, output_softmax)\n        else:\n            if label_smoothing:\n                metric.update(hard_label, outputs)\n            else:\n                metric.update(label, outputs)\n        return metric\n    else:\n        return\n\n\ndef _train_val_split(train_dataset, split_ratio=0.2):\n    train_sampler, val_sampler = get_split_samplers(train_dataset, split_ratio)\n    return SampledDataset(train_dataset, train_sampler), SampledDataset(train_dataset, val_sampler)\n'"
autogluon/task/object_detection/__init__.py,0,b'from .object_detection import *\nfrom .detector import *'
autogluon/task/object_detection/data_parallel.py,0,"b'from gluoncv.utils.parallel import Parallelizable\nfrom mxnet import autograd\n\n\nclass ForwardBackwardTask(Parallelizable):\n    def __init__(self, net, optimizer, rpn_cls_loss, rpn_box_loss, rcnn_cls_loss, rcnn_box_loss,\n                 mix_ratio, enable_amp):\n        super(ForwardBackwardTask, self).__init__()\n        self.net = net\n        self._optimizer = optimizer\n        self.rpn_cls_loss = rpn_cls_loss\n        self.rpn_box_loss = rpn_box_loss\n        self.rcnn_cls_loss = rcnn_cls_loss\n        self.rcnn_box_loss = rcnn_box_loss\n        self.mix_ratio = mix_ratio\n        self.amp = enable_amp\n\n    def forward_backward(self, x):\n        data, label, rpn_cls_targets, rpn_box_targets, rpn_box_masks = x\n        with autograd.record():\n            gt_label = label[:, :, 4:5]\n            gt_box = label[:, :, :4]\n            cls_pred, box_pred, roi, samples, matches, rpn_score, rpn_box, anchors, cls_targets, \\\n                box_targets, box_masks, _ = self.net(data, gt_box, gt_label)\n            # losses of rpn\n            rpn_score = rpn_score.squeeze(axis=-1)\n            num_rpn_pos = (rpn_cls_targets >= 0).sum()\n            rpn_loss1 = self.rpn_cls_loss(rpn_score, rpn_cls_targets,\n                                          rpn_cls_targets >= 0) * rpn_cls_targets.size / num_rpn_pos\n            rpn_loss2 = self.rpn_box_loss(rpn_box, rpn_box_targets,\n                                          rpn_box_masks) * rpn_box.size / num_rpn_pos\n            # rpn overall loss, use sum rather than average\n            rpn_loss = rpn_loss1 + rpn_loss2\n            # losses of rcnn\n            num_rcnn_pos = (cls_targets >= 0).sum()\n            rcnn_loss1 = self.rcnn_cls_loss(cls_pred, cls_targets,\n                                            cls_targets.expand_dims(-1) >= 0) * cls_targets.size / \\\n                num_rcnn_pos\n            rcnn_loss2 = self.rcnn_box_loss(box_pred, box_targets, box_masks) * box_pred.size / \\\n                num_rcnn_pos\n            rcnn_loss = rcnn_loss1 + rcnn_loss2\n            # overall losses\n            total_loss = rpn_loss.sum() * self.mix_ratio + rcnn_loss.sum() * self.mix_ratio\n\n            rpn_loss1_metric = rpn_loss1.mean() * self.mix_ratio\n            rpn_loss2_metric = rpn_loss2.mean() * self.mix_ratio\n            rcnn_loss1_metric = rcnn_loss1.mean() * self.mix_ratio\n            rcnn_loss2_metric = rcnn_loss2.mean() * self.mix_ratio\n            rpn_acc_metric = [[rpn_cls_targets, rpn_cls_targets >= 0], [rpn_score]]\n            rpn_l1_loss_metric = [[rpn_box_targets, rpn_box_masks], [rpn_box]]\n            rcnn_acc_metric = [[cls_targets], [cls_pred]]\n            rcnn_l1_loss_metric = [[box_targets, box_masks], [box_pred]]\n\n            if self.amp:\n                from mxnet.contrib import amp\n                with amp.scale_loss(total_loss, self._optimizer) as scaled_losses:\n                    autograd.backward(scaled_losses)\n            else:\n                total_loss.backward()\n\n        return rpn_loss1_metric, rpn_loss2_metric, rcnn_loss1_metric, rcnn_loss2_metric, \\\n            rpn_acc_metric, rpn_l1_loss_metric, rcnn_acc_metric, rcnn_l1_loss_metric\n'"
autogluon/task/object_detection/detector.py,0,"b'from collections import OrderedDict\n\nimport cloudpickle as pkl\nimport gluoncv as gcv\nimport matplotlib.pyplot as plt\nimport mxnet as mx\nfrom gluoncv.data.batchify import Tuple, Stack, Pad, Append\nfrom gluoncv.data.transforms import presets\nfrom gluoncv.data.transforms.presets.rcnn import FasterRCNNDefaultValTransform\nfrom gluoncv.data.transforms.presets.yolo import YOLO3DefaultValTransform\nfrom mxnet import gluon\n\nfrom .utils import rcnn_split_and_load, get_network\nfrom ..base.base_predictor import BasePredictor\nfrom ...core import AutoGluonObject\nfrom ...utils import save, load, collect_params, update_params\n\n__all__ = [\'Detector\']\n\n\nclass Detector(BasePredictor):\n    """"""\n    Trained Object Detector returned by `task.fit()`\n    """"""\n\n    def __init__(self, model, results, scheduler_checkpoint, args, format_results=True, **kwargs):\n        self.model = model\n        self.results = self._format_results(results) if format_results else results\n        self.scheduler_checkpoint = scheduler_checkpoint\n        self.args = args\n\n    def evaluate(self, dataset, ctx=[mx.cpu()]):\n        """"""Evaluate performance of this object detector\'s predictions on test data.\n         \n         Parameters\n         ----------\n        dataset: `Dataset`\n            Test dataset (must be in the same format as training data previously provided to fit).\n        ctx : List of `mxnet.context` elements.\n            Determines whether to use CPU or GPU(s), options include: `[mx.cpu()]` or `[mx.gpu()]`.\n        """"""\n        args = self.args\n        net = self.model\n        net.collect_params().reset_ctx(ctx)\n\n        def _get_dataloader(net, test_dataset, data_shape, batch_size, num_workers, num_devices,\n                            args):\n            """"""Get dataloader.""""""\n            if args.meta_arch == \'yolo3\':\n                width, height = data_shape, data_shape\n                val_batchify_fn = Tuple(Stack(), Pad(pad_val=-1))\n                test_loader = gluon.data.DataLoader(\n                    test_dataset.transform(YOLO3DefaultValTransform(width, height)),\n                    batch_size,\n                    False,\n                    batchify_fn=val_batchify_fn,\n                    last_batch=\'keep\',\n                    num_workers=num_workers\n                )\n                return test_loader\n            elif args.meta_arch == \'faster_rcnn\':\n                """"""Get faster rcnn dataloader.""""""\n                test_bfn = Tuple(*[Append() for _ in range(3)])\n                short = net.short[-1] if isinstance(net.short, (tuple, list)) else net.short\n                # validation use 1 sample per device\n                test_loader = gluon.data.DataLoader(\n                    test_dataset.transform(FasterRCNNDefaultValTransform(short, net.max_size)),\n                    num_devices,\n                    False,\n                    batchify_fn=test_bfn,\n                    last_batch=\'keep\',\n                    num_workers=args.num_workers\n                )\n                return test_loader\n            else:\n                raise NotImplementedError(\'%s not implemented.\' % args.meta_arch)\n\n        def _validate(net, val_data, ctx, eval_metric):\n            """"""Test on validation dataset.""""""\n            eval_metric.reset()\n            # set nms threshold and topk constraint\n            if args.meta_arch == \'yolo3\':\n                net.set_nms(nms_thresh=0.45, nms_topk=400)\n            mx.nd.waitall()\n            net.hybridize()\n            for batch in val_data:\n                if args.meta_arch == \'yolo3\':\n                    data = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0,\n                                                      even_split=False)\n                    label = gluon.utils.split_and_load(batch[1], ctx_list=ctx, batch_axis=0,\n                                                       even_split=False)\n                    split_batch = data, label\n                elif args.meta_arch == \'faster_rcnn\':\n                    split_batch = rcnn_split_and_load(batch, ctx_list=ctx)\n                    clipper = gcv.nn.bbox.BBoxClipToImage()\n                else:\n                    raise NotImplementedError(\'%s not implemented.\' % args.meta_arch)\n                det_bboxes = []\n                det_ids = []\n                det_scores = []\n                gt_bboxes = []\n                gt_ids = []\n                gt_difficults = []\n                for data in zip(*split_batch):\n                    if args.meta_arch == \'yolo3\':\n                        x, y = data\n                    elif args.meta_arch == \'faster_rcnn\':\n                        x, y, im_scale = data\n                    else:\n                        raise NotImplementedError(\'%s not implemented.\' % args.meta_arch)\n                    # get prediction results\n                    ids, scores, bboxes = net(x)\n                    det_ids.append(ids)\n                    det_scores.append(scores)\n                    # clip to image size\n                    if args.meta_arch == \'yolo3\':\n                        det_bboxes.append(bboxes.clip(0, batch[0].shape[2]))\n                    elif args.meta_arch == \'faster_rcnn\':\n                        det_bboxes.append(clipper(bboxes, x))\n                        # rescale to original resolution\n                        im_scale = im_scale.reshape((-1)).asscalar()\n                        det_bboxes[-1] *= im_scale\n                    else:\n                        raise NotImplementedError(\'%s not implemented.\' % args.meta_arch)\n                    # split ground truths\n                    gt_ids.append(y.slice_axis(axis=-1, begin=4, end=5))\n                    gt_bboxes.append(y.slice_axis(axis=-1, begin=0, end=4))\n                    if args.meta_arch == \'faster_rcnn\':\n                        gt_bboxes[-1] *= im_scale\n                    gt_difficults.append(\n                        y.slice_axis(axis=-1, begin=5, end=6) if y.shape[-1] > 5 else None)\n                # update metric\n                if args.meta_arch == \'yolo3\':\n                    eval_metric.update(det_bboxes, det_ids, det_scores, gt_bboxes, gt_ids,\n                                       gt_difficults)\n                elif args.meta_arch == \'faster_rcnn\':\n                    for det_bbox, det_id, det_score, gt_bbox, gt_id, gt_diff in \\\n                            zip(det_bboxes, det_ids, det_scores, gt_bboxes, gt_ids, gt_difficults):\n                        eval_metric.update(det_bbox, det_id, det_score, gt_bbox, gt_id, gt_diff)\n                else:\n                    raise NotImplementedError(\'%s not implemented.\' % args.meta_arch)\n            return eval_metric.get()\n\n        if isinstance(dataset, AutoGluonObject):\n            dataset = dataset.init()\n        test_dataset, eval_metric = dataset.get_dataset_and_metric()\n        test_data = _get_dataloader(net, test_dataset, args.data_shape, args.batch_size,\n                                    args.num_workers, len(ctx), args)\n        return _validate(net, test_data, ctx, eval_metric)\n\n    def predict(self, X, input_size=224, thresh=0.15, plot=True):\n        """""" Use this object detector to make predictions on test data.\n        \n        Parameters\n        ----------\n        X : Test data with image(s) to make predictions for.\n        input_size : int\n            Size of images in test data (pixels).\n        thresh : float\n            Confidence Threshold above which detector outputs bounding box for object.\n        plot : bool\n            Whether or not to plot the bounding box of detected objects on top of the original images.\n        \n        Returns\n        -------\n        Tuple containing the class-IDs of detected objects, the confidence-scores associated with \n        these detectiions, and the corresponding predicted bounding box locations.\n        """"""\n        net = self.model\n        net.set_nms(0.45, 200)\n        net.collect_params().reset_ctx(ctx=mx.cpu())\n\n        x, img = presets.yolo.load_test(X, short=512)\n        ids, scores, bboxes = [xx[0].asnumpy() for xx in net(x)]\n\n        if plot:\n            gcv.utils.viz.plot_bbox(img, bboxes, scores, ids,\n                                    thresh=thresh,\n                                    class_names=net.classes,\n                                    ax=None)\n        plt.show()\n        return ids, scores, bboxes\n\n    @classmethod\n    def load(cls, checkpoint):\n        """""" load trained object detector from the file specified by \'checkpoint\'\n        """"""\n        state_dict = load(checkpoint)\n        args = state_dict[\'args\']\n        results = pkl.loads(state_dict[\'results\'])\n        scheduler_checkpoint = state_dict[\'scheduler_checkpoint\']\n        model_params = state_dict[\'model_params\']\n        classes = state_dict[\'classes\']\n\n        model = get_network(args.meta_arch, args.net, classes, ctx=mx.cpu(0), syncbn=args.syncbn)\n        update_params(model, model_params)\n\n        return cls(model, results, scheduler_checkpoint, args, format_results=False)\n\n    def state_dict(self, destination=None):\n        if destination is None:\n            destination = OrderedDict()\n            destination._metadata = OrderedDict()\n        model_params = collect_params(self.model)\n        destination[\'model_params\'] = model_params\n        destination[\'results\'] = pkl.dumps(self.results)\n        destination[\'scheduler_checkpoint\'] = self.scheduler_checkpoint\n        destination[\'args\'] = self.args\n        destination[\'classes\'] = destination[\'args\'].pop(\'dataset\').get_classes()\n        return destination\n\n    def save(self, checkpoint):\n        """"""save object detector to the file specified by \'checkpoint\'\n        """"""\n        state_dict = self.state_dict()\n        save(state_dict, checkpoint)\n\n    def evaluate_predictions(self, y_true, y_pred):\n        raise NotImplementedError\n\n    def predict_proba(self, X):\n        raise NotImplementedError\n'"
autogluon/task/object_detection/nets.py,0,"b""import gluoncv as gcv\nimport mxnet as mx\nfrom mxnet import gluon\n\n\ndef get_built_in_network(meta_arch, name, *args, **kwargs):\n    def _get_network(meta_arch, name, transfer_classes, transfer, ctx=mx.cpu(), syncbn=False):\n        name = name.lower()\n        if meta_arch == 'yolo3':\n            net_name = '_'.join((meta_arch, name, 'custom'))\n            net = gcv.model_zoo.get_model(net_name,\n                                          classes=transfer_classes,\n                                          pretrained_base=False,\n                                          transfer=None)\n            net.initialize(ctx=ctx, force_reinit=True)\n        elif meta_arch == 'faster_rcnn':\n            net_name = '_'.join(('custom', meta_arch, 'fpn'))\n            kwargs = {'base_network_name': name, 'short': 600, 'max_size': 1000,\n                      'nms_thresh': 0.5, 'nms_topk': -1, 'min_stage': 2, 'max_stage': 6,\n                      'post_nms': -1, 'roi_mode': 'align', 'roi_size': (7, 7),\n                      'strides': (4, 8, 16, 32, 64), 'clip': 4.14, 'rpn_channel': 256,\n                      'base_size': 16, 'scales': (2, 4, 8, 16, 32), 'ratios': (0.5, 1, 2),\n                      'alloc_size': (384, 384), 'rpn_nms_thresh': 0.7, 'rpn_train_pre_nms': 12000,\n                      'rpn_train_post_nms': 2000, 'rpn_test_pre_nms': 6000,\n                      'rpn_test_post_nms': 1000, 'rpn_min_size': 1, 'per_device_batch_size': 1,\n                      'num_sample': 512, 'pos_iou_thresh': 0.5, 'pos_ratio': 0.25,\n                      'max_num_gt': 100}\n            if syncbn and len(ctx) > 1:\n                net = gcv.model_zoo.get_model(net_name,\n                                              classes=transfer_classes,\n                                              pretrained_base=True, transfer=transfer,\n                                              norm_layer=gluon.contrib.nn.SyncBatchNorm,\n                                              norm_kwargs={'num_devices': len(ctx)}, **kwargs)\n            else:\n                net = gcv.model_zoo.get_model(net_name, classes=transfer_classes,\n                                              pretrained_base=False,\n                                              transfer=transfer, **kwargs)\n            net.initialize(ctx=ctx, force_reinit=True)\n        else:\n            raise NotImplementedError('%s not implemented.' % meta_arch)\n        return net\n\n    name = name.lower()\n    return _get_network(meta_arch, name, *args, **kwargs)\n"""
autogluon/task/object_detection/object_detection.py,0,"b'import logging\n\nimport mxnet as mx\nfrom mxnet import gluon\nimport copy\n\nfrom .dataset import get_dataset\nfrom .detector import Detector\nfrom .pipeline import train_object_detection\nfrom .utils import get_network\nfrom ..base import BaseTask, compile_scheduler_options\nfrom ...core.decorator import sample_config\nfrom ...core.space import Categorical\nfrom ...scheduler.resource import get_cpu_count, get_gpu_count\nfrom ...utils import update_params\n\n__all__ = [\'ObjectDetection\']\n\nlogger = logging.getLogger(__name__)\n\n\nclass ObjectDetection(BaseTask):\n    """"""AutoGluon Task for detecting and locating objects in images\n    """"""\n\n    @staticmethod\n    def Dataset(*args, **kwargs):\n        """""" Dataset of images in which to detect objects.\n        """"""\n        return get_dataset(*args, **kwargs)\n\n    @staticmethod\n    def fit(dataset=\'voc\',\n            net=Categorical(\'mobilenet1.0\'),\n            meta_arch=\'yolo3\',\n            lr=Categorical(5e-4, 1e-4),\n            loss=gluon.loss.SoftmaxCrossEntropyLoss(),\n            split_ratio=0.8,\n            batch_size=16,\n            epochs=50,\n            num_trials=None,\n            time_limits=None,\n            nthreads_per_trial=12,\n            num_workers=32,\n            ngpus_per_trial=1,\n            hybridize=True,\n            scheduler_options=None,\n            search_strategy=\'random\',\n            search_options=None,\n            verbose=False,\n            transfer=\'coco\',\n            resume=\'\',\n            checkpoint=\'checkpoint/exp1.ag\',\n            visualizer=\'none\',\n            dist_ip_addrs=None,\n            auto_search=True,\n            seed=223,\n            data_shape=416,\n            start_epoch=0,\n            lr_mode=\'step\',\n            lr_decay=0.1,\n            lr_decay_period=0,\n            lr_decay_epoch=\'160,180\',\n            warmup_lr=0.0,\n            warmup_epochs=2,\n            warmup_iters=1000,\n            warmup_factor=1. / 3.,\n            momentum=0.9,\n            wd=0.0005,\n            log_interval=100,\n            save_prefix=\'\',\n            save_interval=10,\n            val_interval=1,\n            num_samples=-1,\n            no_random_shape=False,\n            no_wd=False,\n            mixup=False,\n            no_mixup_epochs=20,\n            label_smooth=False,\n            syncbn=False,\n            reuse_pred_weights=True,\n            **kwargs):\n\n        """"""\n        Fit object detection models.\n\n        Parameters\n        ----------\n        dataset : str or :class:`autogluon.task.ObjectDectection.Dataset`\n            Training dataset containing images and corresponding object bounding boxes.\n        net : str, :class:`autogluon.space.AutoGluonObject`\n            Which existing neural network base models to consider as candidates.\n        meta_arch : str\n            Meta architecture of the model. Currently support YoloV3 (Default) and FasterRCNN.\n            YoloV3 is faster, while FasterRCNN is more accurate.\n        lr : float or :class:`autogluon.space`\n            The learning rate to use in each update of the neural network weights during training.\n        loss : mxnet.gluon.loss\n            Loss function used during training of the neural network weights.\n        split_ratio : float\n            Fraction of dataset to hold-out during training in order to tune hyperparameters (i.e. validation data).\n            The final returned model may be fit to all of the data (after hyperparameters have been selected).\n        batch_size : int\n            How many images to group in each mini-batch during gradient computations in training.\n        epochs : int\n            How many epochs to train the neural networks for at most.\n        num_trials : int\n            Maximal number of hyperparameter configurations to try out.\n        time_limits : int\n            Approximately how long should `fit()` should run for (wallclock time in seconds).\n            `fit()` will stop training new models after this amount of time has elapsed (but models which have already started training will continue to completion).\n        nthreads_per_trial : int\n            How many CPUs to use in each trial (ie. single training run of a model).\n        num_workers : int\n            How many CPUs to use for data loading during training of a model.\n        ngpus_per_trial : int\n            How many GPUs to use in each trial (ie. single training run of a model). \n        hybridize : bool\n            Whether or not the MXNet neural network should be hybridized (for increased efficiency).\n        scheduler_options : dict\n            Extra arguments passed to __init__ of scheduler, to configure the\n            orchestration of training jobs during hyperparameter-tuning.\n        search_strategy : str\n            Which hyperparameter search algorithm to use.\n            Options include: \'random\' (random search), \'skopt\' (SKopt Bayesian\n            optimization), \'grid\' (grid search), \'hyperband\' (Hyperband random),\n            \'rl\' (reinforcement learner).\n        search_options : dict\n            Auxiliary keyword arguments to pass to the searcher that performs\n            hyperparameter optimization.\n        verbose : bool\n            Whether or not to print out intermediate information during training.\n        resume : str\n            Path to checkpoint file of existing model, from which model training\n            should resume.\n            If not empty, we also start the hyperparameter search from the state\n            loaded from checkpoint.\n        checkpoint : str or None\n            State of hyperparameter search is stored to this local file\n        visualizer : str\n            Describes method to visualize training progress during `fit()`. Options: [\'mxboard\', \'tensorboard\', \'none\']. \n        dist_ip_addrs : list\n            List of IP addresses corresponding to remote workers, in order to leverage distributed computation.\n        auto_search : bool\n            If True, enables automatic suggestion of network types and hyper-parameter ranges adaptively based on provided dataset.\n        seed : int\n            Random seed to set for reproducibility.\n        data_shape : int\n            Shape of the image data.\n        start_epoch : int\n            Which epoch we begin training from \n            (eg. if we resume training of an existing model, then this\n            argument may be set to the number of epochs the model has already been trained for previously).\n        lr_mode : str\n            What sort of learning rate schedule should be followed during training.\n        lr_decay : float\n            How much learning rate should be decayed during training.\n        lr_decay_period : int\n            How often learning rate should be decayed during training.\n        warmup_lr : float\n            Learning rate to use during warm up period at the start of training.\n        warmup_epochs : int\n            How many initial epochs constitute the ""warm up"" period of model training.\n        warmup_iters : int\n            How many initial iterations constitute the ""warm up"" period of model training.\n            This is used by R-CNNs\n        warmup_factor : float\n            warmup factor of target lr. initial lr starts from target lr * warmup_factor\n        momentum : float or  :class:`autogluon.space`\n            Momentum to use in optimization of neural network weights during training.\n        wd : float or :class:`autogluon.space`\n            Weight decay to use in optimization of neural network weights during training.\n        log_interval : int\n            Log results every so many epochs during training.\n        save_prefix : str\n            Prefix to append to file name for saved model.\n        save_interval : int\n            Save a copy of model every so many epochs during training.\n        val_interval : int\n            Evaluate performance on held-out validation data every so many epochs during training.\n        no_random_shape : bool\n            Whether random shapes should not be used.\n        no_wd : bool\n            Whether weight decay should be turned off.\n        mixup : bool\n            Whether or not to utilize mixup data augmentation strategy.\n        no_mixup_epochs : int\n            If using mixup, we first train model for this many epochs without mixup data augmentation.\n        label_smooth : bool\n            Whether or not to utilize label smoothing.\n        syncbn : bool\n            Whether or not to utilize synchronized batch normalization.\n        \n        Returns\n        -------\n            :class:`autogluon.task.object_detection.Detector` object which can make predictions on new data and summarize what happened during `fit()`.\n        \n        Examples\n        --------\n        >>> from autogluon import ObjectDetection as task\n        >>> detector = task.fit(dataset = \'voc\', net = \'mobilenet1.0\',\n        >>>                     time_limits = 600, ngpus_per_trial = 1, num_trials = 1)\n        """"""\n        assert search_strategy not in {\'bayesopt\', \'bayesopt_hyperband\'}, \\\n            ""search_strategy == \'bayesopt\' or \'bayesopt_hyperband\' not yet supported""\n        if auto_search:\n            # The strategies can be injected here, for example: automatic suggest some hps\n            # based on the dataset statistics\n            pass\n\n        nthreads_per_trial = get_cpu_count() if nthreads_per_trial > get_cpu_count() else nthreads_per_trial\n        if ngpus_per_trial > get_gpu_count():\n            logger.warning(\n                ""The number of requested GPUs is greater than the number of available GPUs."")\n        ngpus_per_trial = get_gpu_count() if ngpus_per_trial > get_gpu_count() else ngpus_per_trial\n\n        # If only time_limits is given, the scheduler starts trials until the\n        # time limit is reached\n        if num_trials is None and time_limits is None:\n            num_trials = 2\n\n        train_object_detection.register_args(\n            meta_arch=meta_arch,\n            dataset=dataset,\n            net=net,\n            lr=lr,\n            loss=loss,\n            num_gpus=ngpus_per_trial,\n            batch_size=batch_size,\n            split_ratio=split_ratio,\n            epochs=epochs,\n            num_workers=nthreads_per_trial,\n            hybridize=hybridize,\n            verbose=verbose,\n            final_fit=False,\n            seed=seed,\n            data_shape=data_shape,\n            start_epoch=0,\n            transfer=transfer,\n            lr_mode=lr_mode,\n            lr_decay=lr_decay,\n            lr_decay_period=lr_decay_period,\n            lr_decay_epoch=lr_decay_epoch,\n            warmup_lr=warmup_lr,\n            warmup_epochs=warmup_epochs,\n            warmup_iters=warmup_iters,\n            warmup_factor=warmup_factor,\n            momentum=momentum,\n            wd=wd,\n            log_interval=log_interval,\n            save_prefix=save_prefix,\n            save_interval=save_interval,\n            val_interval=val_interval,\n            num_samples=num_samples,\n            no_random_shape=no_random_shape,\n            no_wd=no_wd,\n            mixup=mixup,\n            no_mixup_epochs=no_mixup_epochs,\n            label_smooth=label_smooth,\n            resume=resume,\n            syncbn=syncbn,\n            reuse_pred_weights=reuse_pred_weights)\n\n        # Backward compatibility:\n        grace_period = kwargs.get(\'grace_period\')\n        if grace_period is not None:\n            if scheduler_options is None:\n                scheduler_options = {\'grace_period\': grace_period}\n            else:\n                assert \'grace_period\' not in scheduler_options, \\\n                    ""grace_period appears both in scheduler_options and as direct argument""\n                scheduler_options = copy.copy(scheduler_options)\n                scheduler_options[\'grace_period\'] = grace_period\n            logger.warning(\n                ""grace_period is deprecated, use ""\n                ""scheduler_options={\'grace_period\': ...} instead"")\n        scheduler_options = compile_scheduler_options(\n            scheduler_options=scheduler_options,\n            search_strategy=search_strategy,\n            search_options=search_options,\n            nthreads_per_trial=nthreads_per_trial,\n            ngpus_per_trial=ngpus_per_trial,\n            checkpoint=checkpoint,\n            num_trials=num_trials,\n            time_out=time_limits,\n            resume=(len(resume) > 0),\n            visualizer=visualizer,\n            time_attr=\'epoch\',\n            reward_attr=\'map_reward\',\n            dist_ip_addrs=dist_ip_addrs,\n            epochs=epochs)\n        results = BaseTask.run_fit(\n            train_object_detection, search_strategy, scheduler_options)\n        logger.info("">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> finish model fitting"")\n        args = sample_config(train_object_detection.args, results[\'best_config\'])\n        logger.info(\'The best config: {}\'.format(results[\'best_config\']))\n\n        ctx = [mx.gpu(i) for i in range(get_gpu_count())]\n        model = get_network(args.meta_arch, args.net, dataset.init().get_classes(), transfer, ctx,\n                            syncbn=args.syncbn)\n        update_params(model, results.pop(\'model_params\'))\n        return Detector(model, results, checkpoint, args)\n'"
autogluon/task/object_detection/pipeline.py,0,"b'import logging\nimport os\nimport time\nimport warnings\n\nimport gluoncv as gcv\nimport mxnet as mx\nimport numpy as np\nfrom gluoncv import utils as gutils\nfrom gluoncv.data.batchify import FasterRCNNTrainBatchify, Tuple, Append\nfrom gluoncv.data.batchify import Stack, Pad\nfrom gluoncv.data.dataloader import RandomTransformDataLoader\nfrom gluoncv.data.transforms.presets.rcnn import FasterRCNNDefaultTrainTransform, \\\n    FasterRCNNDefaultValTransform\nfrom gluoncv.data.transforms.presets.yolo import YOLO3DefaultTrainTransform, \\\n    YOLO3DefaultValTransform\nfrom gluoncv.utils.parallel import Parallel\nfrom mxnet import gluon, autograd\n\nfrom .data_parallel import ForwardBackwardTask\nfrom .utils import get_lr_scheduler, get_yolo3_metrics, get_faster_rcnn_metrics, get_rcnn_losses, \\\n    rcnn_split_and_load\n# TODO: move it to general util.py\nfrom ..image_classification.utils import _train_val_split\nfrom ...core import args\nfrom ...utils.mxutils import collect_params\n\n\ndef get_dataloader(net, train_dataset, val_dataset, data_shape, batch_size, num_workers, args):\n    """"""Get dataloader.""""""\n    # when it is not in final_fit stage and val_dataset is not provided, we randomly\n    # sample (1 - args.split_ratio) data as our val_dataset\n    if (not args.final_fit) and (not val_dataset):\n        train_dataset, val_dataset = _train_val_split(train_dataset, args.split_ratio)\n\n    width, height = data_shape, data_shape\n    batchify_fn = Tuple(*([Stack() for _ in range(6)] + [Pad(axis=0, pad_val=-1) for _ in range(\n        1)]))  # stack image, all targets generated\n    if args.no_random_shape:\n        train_loader = gluon.data.DataLoader(\n            train_dataset.transform(\n                YOLO3DefaultTrainTransform(width, height, net, mixup=args.mixup)),\n            batch_size, True, batchify_fn=batchify_fn, last_batch=\'rollover\',\n            num_workers=num_workers)\n    else:\n        transform_fns = [YOLO3DefaultTrainTransform(x * 32, x * 32, net, mixup=args.mixup) for x in\n                         range(10, 20)]\n        train_loader = RandomTransformDataLoader(\n            transform_fns, train_dataset, batch_size=batch_size, interval=10, last_batch=\'rollover\',\n            shuffle=True, batchify_fn=batchify_fn, num_workers=num_workers)\n\n    val_batchify_fn = Tuple(Stack(), Pad(pad_val=-1))\n    val_loader = None\n    if val_dataset:\n        val_loader = gluon.data.DataLoader(\n            val_dataset.transform(YOLO3DefaultValTransform(width, height)),\n            batch_size, False, batchify_fn=val_batchify_fn, last_batch=\'keep\',\n            num_workers=num_workers)\n    return train_loader, val_loader\n\n\ndef get_faster_rcnn_dataloader(net, train_dataset, val_dataset, train_transform, val_transform,\n                               batch_size, num_shards, args):\n    """"""Get faster rcnn dataloader.""""""\n    if (not args.final_fit) and (not val_dataset):\n        train_dataset, val_dataset = _train_val_split(train_dataset, args.split_ratio)\n\n    train_bfn = FasterRCNNTrainBatchify(net, num_shards)\n    if hasattr(train_dataset, \'get_im_aspect_ratio\'):\n        im_aspect_ratio = train_dataset.get_im_aspect_ratio()\n    else:\n        im_aspect_ratio = [1.] * len(train_dataset)\n    train_sampler = \\\n        gcv.nn.sampler.SplitSortedBucketSampler(im_aspect_ratio, batch_size, num_parts=1,\n                                                part_index=0, shuffle=True)\n    train_loader = gluon.data.DataLoader(train_dataset.transform(\n        train_transform(net.short, net.max_size, net, ashape=net.ashape, multi_stage=True)),\n        batch_sampler=train_sampler, batchify_fn=train_bfn, num_workers=args.num_workers)\n    val_bfn = Tuple(*[Append() for _ in range(3)])\n    short = net.short[-1] if isinstance(net.short, (tuple, list)) else net.short\n    # validation use 1 sample per device\n    val_loader = None\n    if val_dataset:\n        val_loader = gluon.data.DataLoader(\n            val_dataset.transform(val_transform(short, net.max_size)), num_shards, False,\n            batchify_fn=val_bfn, last_batch=\'keep\', num_workers=args.num_workers)\n    args.num_samples = len(train_dataset)\n    return train_loader, val_loader\n\n\ndef validate(net, val_data, ctx, eval_metric, args):\n    """"""Test on validation dataset.""""""\n    eval_metric.reset()\n    # set nms threshold and topk constraint\n    if args.meta_arch == \'yolo3\':\n        net.set_nms(nms_thresh=0.45, nms_topk=400)\n    mx.nd.waitall()\n    net.hybridize()\n    for batch in val_data:\n        if args.meta_arch == \'yolo3\':\n            data = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0,\n                                              even_split=False)\n            label = gluon.utils.split_and_load(batch[1], ctx_list=ctx, batch_axis=0,\n                                               even_split=False)\n            split_batch = data, label\n        elif args.meta_arch == \'faster_rcnn\':\n            split_batch = rcnn_split_and_load(batch, ctx_list=ctx)\n            clipper = gcv.nn.bbox.BBoxClipToImage()\n        else:\n            raise NotImplementedError(\'%s not implemented.\' % args.meta_arch)\n        det_bboxes = []\n        det_ids = []\n        det_scores = []\n        gt_bboxes = []\n        gt_ids = []\n        gt_difficults = []\n        for data in zip(*split_batch):\n            if args.meta_arch == \'yolo3\':\n                x, y = data\n            elif args.meta_arch == \'faster_rcnn\':\n                x, y, im_scale = data\n            else:\n                raise NotImplementedError(\'%s not implemented.\' % args.meta_arch)\n            # get prediction results\n            ids, scores, bboxes = net(x)\n            det_ids.append(ids)\n            det_scores.append(scores)\n            # clip to image size\n            if args.meta_arch == \'yolo3\':\n                det_bboxes.append(bboxes.clip(0, batch[0].shape[2]))\n            elif args.meta_arch == \'faster_rcnn\':\n                det_bboxes.append(clipper(bboxes, x))\n                # rescale to original resolution\n                im_scale = im_scale.reshape((-1)).asscalar()\n                det_bboxes[-1] *= im_scale\n            else:\n                raise NotImplementedError(\'%s not implemented.\' % args.meta_arch)\n            # split ground truths\n            gt_ids.append(y.slice_axis(axis=-1, begin=4, end=5))\n            gt_bboxes.append(y.slice_axis(axis=-1, begin=0, end=4))\n            if args.meta_arch == \'faster_rcnn\':\n                gt_bboxes[-1] *= im_scale\n            gt_difficults.append(y.slice_axis(axis=-1, begin=5, end=6) if y.shape[-1] > 5 else None)\n\n        # update metric\n        if args.meta_arch == \'yolo3\':\n            eval_metric.update(det_bboxes, det_ids, det_scores, gt_bboxes, gt_ids, gt_difficults)\n        elif args.meta_arch == \'faster_rcnn\':\n            for det_bbox, det_id, det_score, gt_bbox, gt_id, gt_diff in zip(det_bboxes, det_ids,\n                                                                            det_scores, gt_bboxes,\n                                                                            gt_ids, gt_difficults):\n                eval_metric.update(det_bbox, det_id, det_score, gt_bbox, gt_id, gt_diff)\n        else:\n            raise NotImplementedError(\'%s not implemented.\' % args.meta_arch)\n    return eval_metric.get()\n\n\ndef train(net, train_data, val_data, eval_metric, ctx, args, reporter, final_fit):\n    """"""Training pipeline""""""\n    net.collect_params().reset_ctx(ctx)\n    if args.no_wd:\n        for k, v in net.collect_params(\'.*beta|.*gamma|.*bias\').items():\n            v.wd_mult = 0.0\n    if args.meta_arch == \'faster_rcnn\':\n        net.collect_params().setattr(\'grad_req\', \'null\')\n        net.collect_train_params().setattr(\'grad_req\', \'write\')\n\n    if args.label_smooth:\n        net._target_generator._label_smooth = True\n\n    lr_scheduler = get_lr_scheduler(args)\n\n    trainer = gluon.Trainer(\n        net.collect_train_params() if args.meta_arch == \'faster_rcnn\' else net.collect_params(),\n        \'sgd\', {\'wd\': args.wd, \'momentum\': args.momentum, \'lr_scheduler\': lr_scheduler},\n        kvstore=\'nccl\')\n\n    # metrics & losses\n    if args.meta_arch == \'yolo3\':\n        raw_metrics, metrics = get_yolo3_metrics()\n    elif args.meta_arch == \'faster_rcnn\':\n        raw_metrics, metrics = get_faster_rcnn_metrics()\n        rpn_cls_loss, rpn_box_loss, rcnn_cls_loss, rcnn_box_loss = get_rcnn_losses(args)\n    else:\n        raise NotImplementedError(\'%s not implemented.\' % args.meta_arch)\n\n    # set up logger\n    logging.basicConfig()\n    logger = logging.getLogger()\n    logger.setLevel(logging.INFO)\n    log_file_path = args.save_prefix + \'_train.log\'\n    log_dir = os.path.dirname(log_file_path)\n    if log_dir and not os.path.exists(log_dir):\n        os.makedirs(log_dir)\n    fh = logging.FileHandler(log_file_path)\n    logger.addHandler(fh)\n    logger.info(args)\n    # logger.info(\'Start training from [Epoch {}]\'.format(args.start_epoch))\n    best_map = [0]\n\n    if args.meta_arch == \'faster_rcnn\':\n        rcnn_task = ForwardBackwardTask(net, trainer, rpn_cls_loss, rpn_box_loss, rcnn_cls_loss,\n                                        rcnn_box_loss, mix_ratio=1.0, enable_amp=False)\n        executor = Parallel(args.num_gpus // 2, rcnn_task)\n\n    pre_current_map = 0\n    for epoch in range(args.start_epoch, args.epochs):\n        mix_ratio = 1.0\n        net.hybridize()\n        if args.mixup:\n            # TODO(zhreshold): more elegant way to control mixup during runtime\n            try:\n                train_data._dataset.set_mixup(np.random.beta, 1.5, 1.5)\n            except AttributeError:\n                train_data._dataset._data.set_mixup(np.random.beta, 1.5, 1.5)\n            if epoch >= args.epochs - args.no_mixup_epochs:\n                try:\n                    train_data._dataset.set_mixup(None)\n                except AttributeError:\n                    train_data._dataset._data.set_mixup(None)\n        for metric in raw_metrics:\n            metric.reset()\n        tic = time.time()\n        btic = time.time()\n        mx.nd.waitall()\n        for i, batch in enumerate(train_data):\n            batch_size = args.batch_size\n            raw_metrics_values = [[] for _ in raw_metrics]\n            metrics_values = [[] for _ in metrics]\n            if args.meta_arch == \'yolo3\':\n                data = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0)\n                # objectness, center_targets, scale_targets, weights, class_targets\n                fixed_targets = [gluon.utils.split_and_load(batch[it], ctx_list=ctx, batch_axis=0)\n                                 for it in range(1, 6)]\n                gt_boxes = gluon.utils.split_and_load(batch[6], ctx_list=ctx, batch_axis=0)\n                sum_losses = []\n                with autograd.record():\n                    for ix, x in enumerate(data):\n                        obj_loss, center_loss, scale_loss, cls_loss = net(x, gt_boxes[ix],\n                                                                          *[ft[ix] for ft in\n                                                                            fixed_targets])\n                        sum_losses.append(obj_loss + center_loss + scale_loss + cls_loss)\n                        result = obj_loss, center_loss, scale_loss, cls_loss\n                        for k in range(len(raw_metrics_values)):\n                            raw_metrics_values[k].append(result[k])\n                    autograd.backward(sum_losses)\n            elif args.meta_arch == \'faster_rcnn\':\n                batch = rcnn_split_and_load(batch, ctx_list=ctx)\n                if executor is not None:\n                    for data in zip(*batch):\n                        executor.put(data)\n                for j in range(len(ctx)):\n                    if executor is not None:\n                        result = executor.get()\n                    else:\n                        result = rcnn_task.forward_backward(list(zip(*batch))[0])\n                    for k in range(len(raw_metrics_values)):\n                        raw_metrics_values[k].append(result[k])\n                    for k in range(len(metrics_values)):\n                        metrics_values[k].append(result[len(metrics_values) + k])\n            else:\n                raise NotImplementedError(\'%s not implemented.\' % args.meta_arch)\n            trainer.step(batch_size)\n\n            for metric, record in zip(raw_metrics, raw_metrics_values):\n                metric.update(0, record)\n            for metric, records in zip(metrics, metrics_values):\n                for pred in records:\n                    metric.update(pred[0], pred[1])\n            if args.log_interval and not (i + 1) % args.log_interval:\n                msg = \',\'.join(\n                    [\'{}={:.3f}\'.format(*metric.get()) for metric in raw_metrics + metrics])\n                logger.info(\n                    \'[Epoch {}][Batch {}], LR: {:.2E}, Speed: {:.3f} samples/sec, {}\'.format(\n                        epoch, i, trainer.learning_rate, batch_size / (time.time() - btic), msg))\n            btic = time.time()\n\n        msg = \',\'.join([\'{}={:.3f}\'.format(*metric.get()) for metric in raw_metrics])\n        logger.info(\'[Epoch {}] Training cost: {:.3f}, {}\'.format(\n            epoch, (time.time() - tic), msg))\n        if (not (epoch + 1) % args.val_interval) and not final_fit:\n            # consider reduce the frequency of validation to save time\n            map_name, mean_ap = validate(net, val_data, ctx, eval_metric, args)\n            val_msg = \' \'.join([\'{}={}\'.format(k, v) for k, v in zip(map_name, mean_ap)])\n            # $tbar.set_description(\'[Epoch {}] Validation: {}\'.format(epoch, val_msg))\n            logger.info(\'[Epoch {}] Validation: {}\'.format(epoch, val_msg))\n            current_map = float(mean_ap[-1])\n            pre_current_map = current_map\n        else:\n            current_map = pre_current_map\n        # Note: epoch reported back must start with 1, not 0\n        reporter(epoch=epoch+1, map_reward=current_map)\n\n\n@args()\ndef train_object_detection(args, reporter):\n    # fix seed for mxnet, numpy and python builtin random generator.\n    gutils.random.seed(args.seed)\n\n    # training contexts\n    ctx = [mx.gpu(i) for i in range(args.num_gpus)] if args.num_gpus > 0 else [mx.cpu()]\n    if args.meta_arch == \'yolo3\':\n        net_name = \'_\'.join((args.meta_arch, args.net, \'custom\'))\n        kwargs = {}\n    elif args.meta_arch == \'faster_rcnn\':\n        net_name = \'_\'.join((\'custom\', args.meta_arch, \'fpn\'))\n        kwargs = {\'base_network_name\': args.net, \'short\': args.data_shape, \'max_size\': 1000,\n                  \'nms_thresh\': 0.5, \'nms_topk\': -1, \'min_stage\': 2, \'max_stage\': 6, \'post_nms\': -1,\n                  \'roi_mode\': \'align\', \'roi_size\': (7, 7), \'strides\': (4, 8, 16, 32, 64),\n                  \'clip\': 4.14, \'rpn_channel\': 256, \'base_size\': 16, \'scales\': (2, 4, 8, 16, 32),\n                  \'ratios\': (0.5, 1, 2), \'alloc_size\': (384, 384), \'rpn_nms_thresh\': 0.7,\n                  \'rpn_train_pre_nms\': 12000, \'rpn_train_post_nms\': 2000, \'rpn_test_pre_nms\': 6000,\n                  \'rpn_test_post_nms\': 1000, \'rpn_min_size\': 1,\n                  \'per_device_batch_size\': args.batch_size // args.num_gpus, \'num_sample\': 512,\n                  \'pos_iou_thresh\': 0.5, \'pos_ratio\': 0.25, \'max_num_gt\': 100}\n    else:\n        raise NotImplementedError(args.meta_arch, \'is not implemented.\')\n    args.save_prefix += net_name\n\n    # use sync bn if specified\n    if args.syncbn and len(ctx) > 1:\n        net = gcv.model_zoo.get_model(net_name,\n                                      classes=args.dataset.get_classes(),\n                                      pretrained_base=True,\n                                      transfer=args.transfer,\n                                      norm_layer=gluon.contrib.nn.SyncBatchNorm,\n                                      norm_kwargs={\'num_devices\': len(ctx)}, **kwargs)\n        if not args.reuse_pred_weights:\n            net.reset_class(args.dataset.get_classes(), reuse_weights=None)\n        if args.meta_arch == \'yolo3\':\n            async_net = gcv.model_zoo.get_model(net_name,\n                                                classes=args.dataset.get_classes(),\n                                                pretrained_base=True,\n                                                transfer=args.transfer, **kwargs)\n            if not args.reuse_pred_weights:\n                async_net.reset_class(args.dataset.get_classes(), reuse_weights=None)\n    else:\n        net = gcv.model_zoo.get_model(net_name,\n                                      classes=args.dataset.get_classes(),\n                                      pretrained_base=True,\n                                      transfer=args.transfer, **kwargs)\n        if not args.reuse_pred_weights:\n            net.reset_class(args.dataset.get_classes(), reuse_weights=None)\n        async_net = net\n\n    if args.resume.strip():\n        net.load_parameters(args.resume.strip())\n        if args.meta_arch == \'yolo3\':\n            async_net.load_parameters(args.resume.strip())\n    else:\n        with warnings.catch_warnings(record=True) as w:\n            warnings.simplefilter(""always"")\n            net.initialize()\n            if args.meta_arch == \'yolo3\':\n                async_net.initialize()\n\n    # training data\n    train_dataset, eval_metric = args.dataset.get_dataset_and_metric()\n    if args.meta_arch == \'yolo3\':\n        train_data, val_data = get_dataloader(\n            async_net, train_dataset, None, args.data_shape, args.batch_size, args.num_workers,\n            args)\n    elif args.meta_arch == \'faster_rcnn\':\n        train_data, val_data = get_faster_rcnn_dataloader(\n            net, train_dataset, None, FasterRCNNDefaultTrainTransform,\n            FasterRCNNDefaultValTransform, args.batch_size, args.num_gpus, args)\n\n    # training\n    train(net, train_data, val_data, eval_metric, ctx, args, reporter, args.final_fit)\n\n    if args.final_fit:\n        return {\'model_params\': collect_params(net)}\n'"
autogluon/task/object_detection/utils.py,0,"b'import mxnet as mx\nfrom gluoncv.utils import LRScheduler, LRSequential\nfrom gluoncv.utils.metrics.rcnn import RPNAccMetric, RPNL1LossMetric, RCNNAccMetric, \\\n    RCNNL1LossMetric\n\nfrom .nets import get_built_in_network\n\n\ndef get_network(meta_arch, net, transfer_classes, transfer=None, ctx=mx.cpu(), syncbn=False):\n    if type(net) == str:\n        net = get_built_in_network(meta_arch, net, transfer_classes, transfer, ctx=ctx,\n                                   syncbn=syncbn)\n    else:\n        net.initialize(ctx=ctx)\n    return net\n\n\ndef get_lr_scheduler(args):\n    if args.lr_decay_period > 0:\n        lr_decay_epoch = list(range(args.lr_decay_period, args.epochs, args.lr_decay_period))\n    else:\n        lr_decay_epoch = [int(i) for i in args.lr_decay_epoch.split(\',\')]\n    num_batches = args.num_samples // args.batch_size\n    if args.meta_arch == \'yolo3\':\n        return LRSequential([\n            LRScheduler(\'linear\', base_lr=0, target_lr=args.lr,\n                        nepochs=args.warmup_epochs, iters_per_epoch=num_batches),\n            LRScheduler(args.lr_mode, base_lr=args.lr, nepochs=args.epochs - args.warmup_epochs,\n                        iters_per_epoch=num_batches, step_epoch=lr_decay_epoch,\n                        step_factor=args.lr_decay, power=2)])\n    elif args.meta_arch == \'faster_rcnn\':\n        return LRSequential([\n            LRScheduler(\'linear\', base_lr=args.lr * args.warmup_factor, target_lr=args.lr,\n                        niters=args.warmup_iters, iters_per_epoch=num_batches),\n            LRScheduler(args.lr_mode, base_lr=args.lr, nepochs=args.epochs,\n                        iters_per_epoch=num_batches, step_epoch=lr_decay_epoch,\n                        step_factor=args.lr_decay, power=2)])\n\n\ndef get_faster_rcnn_metrics():\n    return [mx.metric.Loss(\'RPN_Conf\'), mx.metric.Loss(\'RPN_SmoothL1\'),\n            mx.metric.Loss(\'RCNN_CrossEntropy\'), mx.metric.Loss(\'RCNN_SmoothL1\')], \\\n           [RPNAccMetric(), RPNL1LossMetric(), RCNNAccMetric(), RCNNL1LossMetric()]\n\n\ndef get_yolo3_metrics():\n    return [mx.metric.Loss(\'ObjLoss\'), mx.metric.Loss(\'BoxCenterLoss\'),\n            mx.metric.Loss(\'BoxScaleLoss\'), mx.metric.Loss(\'ClassLoss\')], \\\n           []\n\n\ndef get_rcnn_losses(args):\n    return mx.gluon.loss.SigmoidBinaryCrossEntropyLoss(from_sigmoid=False), \\\n           mx.gluon.loss.HuberLoss(rho=0.001), \\\n           mx.gluon.loss.SoftmaxCrossEntropyLoss(), \\\n           mx.gluon.loss.HuberLoss(rho=0.01)\n\n\ndef rcnn_split_and_load(batch, ctx_list):\n    """"""Split data to 1 batch each device.""""""\n    new_batch = []\n    for i, data in enumerate(batch):\n        if isinstance(data, (list, tuple)):\n            new_data = [x.as_in_context(ctx) for x, ctx in zip(data, ctx_list)]\n        else:\n            new_data = [data.as_in_context(ctx_list[0])]\n        new_batch.append(new_data)\n    return new_batch\n'"
autogluon/task/tabular_prediction/__init__.py,0,b'from .tabular_prediction import *\nfrom .dataset import *\nfrom .predictor import * \n'
autogluon/task/tabular_prediction/dataset.py,0,"b'import warnings\n\nimport pandas as pd\n\nfrom ...utils import warning_filter\n\nwith warning_filter():\n    from ...utils.tabular.utils.loaders import load_pd\n\n__all__ = [\'TabularDataset\']\n\n\nclass TabularDataset(pd.DataFrame):\n    """"""\n    A dataset in tabular format (with rows = samples, columns = features/variables). \n    This object is essentially a pandas DataFrame (with some extra attributes) and all existing pandas methods can be applied to it. \n    For full list of methods/attributes, see pandas Dataframe documentation: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html\n\n    Parameters\n    ----------\n    file_path : str (optional)\n        Path to the data file (may be on local filesystem or URL to cloud s3 bucket). \n        At least one of `file_path` and `df` arguments must be specified when constructing a new `TabularDataset`.\n    df : `pandas.DataFrame` (optional)\n        If you already have your data in a pandas Dataframe, you can directly provide it by specifying `df`. \n        At least one of `file_path` and `df` arguments must be specified when constructing new `TabularDataset`.\n    feature_types : dict (optional)\n        Mapping from column_names to string describing data type of each column. \n        If not specified, AutoGluon\'s fit() will automatically infer what type of data each feature contains.\n    subsample : int (optional)\n        If specified = k, we only keep first k rows of the provided dataset.\n    name : str (optional)\n         Optional name to assign to dataset (has no effect beyond being accessible via `TabularDataset.name`).\n\n    Attributes\n    ----------\n    name: (str)\n        An optional name assigned to this `TabularDataset`.\n    file_path: (str)\n        Path to data file from which this `TabularDataset` was created.\n    feature_types: (dict) \n        Maps column-names to string describing the data type of each column in this `TabularDataset`.\n    subsample: (int) \n        Describes size of subsample retained in this `TabularDataset` (None if this is original dataset).\n    \n    Note: In addition to these attributes, `TabularDataset` also shares all the same attributes and methods of a pandas Dataframe. \n    For detailed list, see:  https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html\n\n    Examples\n    --------\n    >>> from autogluon import TabularPrediction as task  # Note: TabularPrediction.Dataset == TabularDataset.\n    >>> train_data = task.Dataset(file_path=\'https://autogluon.s3.amazonaws.com/datasets/Inc/train.csv\')\n    >>> test_data = task.Dataset(file_path=\'https://autogluon.s3.amazonaws.com/datasets/Inc/test.csv\')\n    >>> train_data.head(30)\n    >>> train_data.columns\n    """"""\n\n    _metadata = [\'name\', \'file_path\', \'feature_types\', \'subsample\']  # preserved properties that will be copied to a new instance of TabularDataset\n\n    @property\n    def _constructor(self):\n        return TabularDataset\n\n    @property\n    def _constructor_sliced(self):\n        return pd.Series\n\n    def __init__(self, *args, **kwargs):\n        file_path = kwargs.get(\'file_path\', None)\n        name = kwargs.get(\'name\', None)\n        feature_types = kwargs.get(\'feature_types\', None)\n        df = kwargs.get(\'df\', None)\n        subsample = kwargs.get(\'subsample\', None)\n        construct_from_df = False  # whether or not we are constructing new dataset object from scratch based on provided DataFrame.\n        # if df is None and file_path is None: # Cannot be used currently!\n        #     raise ValueError(""Must specify either named argument \'file_path\' or \'df\' in order to construct tabular Dataset"")\n        if df is not None:  # Create Dataset from existing Python DataFrame:\n            construct_from_df = True\n            if not isinstance(df, pd.DataFrame):\n                raise ValueError(""\'df\' must be existing pandas DataFrame. To read dataset from file instead, use \'file_path\' string argument."")\n            if file_path is not None:\n                warnings.warn(""Both \'df\' and \'file_path\' supplied. Creating dataset based on DataFrame \'df\' rather than reading from file_path."")\n            df = df.copy(deep=True)\n        elif file_path is not None:  # Read from file to create dataset\n            construct_from_df = True\n            df = load_pd.load(file_path)\n        if construct_from_df:  # Construct new Dataset object based off of DataFrame\n            if subsample is not None:\n                if not isinstance(subsample, int) or subsample <= 1:\n                    raise ValueError(""\'subsample\' must be of type int and > 1"")\n                df = df.head(subsample)\n            super().__init__(df)\n            self.file_path = file_path\n            self.name = name\n            self.feature_types = feature_types\n            self.subsample = subsample\n        else:\n            super().__init__(*args, **kwargs)\n'"
autogluon/task/tabular_prediction/hyperparameter_configs.py,0,"b""\nimport copy\n\n\n# Dictionary of preset hyperparameter configurations.\nhyperparameter_config_dict = dict(\n    # Default AutoGluon hyperparameters intended to maximize accuracy without significant regard to inference time or disk usage.\n    default={\n        'NN': {},\n        'GBM': {},\n        'CAT': {},\n        'RF': [\n            {'criterion': 'gini', '_ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}},\n            {'criterion': 'entropy', '_ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}},\n            {'criterion': 'mse', '_ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression']}},\n        ],\n        'XT': [\n            {'criterion': 'gini', '_ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}},\n            {'criterion': 'entropy', '_ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}},\n            {'criterion': 'mse', '_ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression']}},\n        ],\n        'KNN': [\n            {'weights': 'uniform', '_ag_args': {'name_suffix': 'Unif'}},\n            {'weights': 'distance', '_ag_args': {'name_suffix': 'Dist'}},\n        ],\n        'custom': ['GBM'],\n    },\n    # Results in smaller models. Generally will make inference speed much faster and disk usage much lower, but with worse accuracy.\n    light={\n        'NN': {},\n        'GBM': {},\n        'CAT': {},\n        'RF': [\n            {'criterion': 'gini', 'max_depth': 15, '_ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}},\n            {'criterion': 'entropy', 'max_depth': 15, '_ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}},\n            {'criterion': 'mse', 'max_depth': 15, '_ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression']}},\n        ],\n        'XT': [\n            {'criterion': 'gini', 'max_depth': 15, '_ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}},\n            {'criterion': 'entropy', 'max_depth': 15, '_ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}},\n            {'criterion': 'mse', 'max_depth': 15, '_ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression']}},\n        ],\n        'custom': ['GBM'],\n    },\n    # Results in much smaller models. Behaves similarly to 'light', but in many cases with over 10x less disk usage and a further reduction in accuracy.\n    very_light={\n        'NN': {},\n        'GBM': {},\n        'CAT': {},\n    },\n    # Results in extremely quick to train models. Only use this when prototyping, as the model accuracy will be severely reduced.\n    toy={\n        'NN': {'num_epochs': 10},\n        'GBM': {'num_boost_round': 10},\n        'CAT': {'iterations': 10},\n    }\n)\n\n\ndef get_hyperparameter_config_options():\n    return list(hyperparameter_config_dict.keys())\n\n\ndef get_hyperparameter_config(config_name):\n    config_options = get_hyperparameter_config_options()\n    if config_name not in config_options:\n        raise ValueError(f'Valid hyperparameter config names are: {config_options}, but \\'{config_name}\\' was given instead.')\n    return copy.deepcopy(hyperparameter_config_dict[config_name])\n"""
autogluon/task/tabular_prediction/predictor.py,0,"b'import copy\nimport logging\n\nimport pandas as pd\n\nfrom .dataset import TabularDataset\nfrom ..base.base_predictor import BasePredictor\nfrom ...utils import plot_performance_vs_trials, plot_summary_of_models, plot_tabular_models, verbosity2loglevel\nfrom ...utils.tabular.ml.constants import REGRESSION\nfrom ...utils.tabular.ml.learner.abstract_learner import AbstractLearner as Learner  # TODO: Keep track of true type of learner for loading\nfrom ...utils.tabular.ml.trainer.abstract_trainer import AbstractTrainer  # TODO: Keep track of true type of trainer for loading\nfrom ...utils.tabular.ml.utils import setup_outputdir\n\n__all__ = [\'TabularPredictor\']\n\nlogger = logging.getLogger()  # return root logger\n\n\nclass TabularPredictor(BasePredictor):\n    """""" Object returned by `fit()` in Tabular Prediction tasks.\n        Use for making predictions on new data and viewing information about models trained during `fit()`.\n\n        Attributes\n        ----------\n        output_directory : str\n            Path to directory where all models used by this Predictor are stored.\n        problem_type : str\n            What type of prediction problem this Predictor has been trained for.\n        eval_metric : function or str\n            What metric is used to evaluate predictive performance.\n        label_column : str\n            Name of table column that contains data from the variable to predict (often referred to as: labels, response variable, target variable, dependent variable, Y, etc).\n        feature_types : dict\n            Inferred data type of each predictive variable (i.e. column of training data table used to predict `label_column`).\n        model_names : list\n            List of model names trained during `fit()`.\n        model_performance : dict\n            Maps names of trained models to their predictive performance values attained on the validation dataset during `fit()`.\n        class_labels : list\n            For multiclass problems, this list contains the class labels in sorted order of `predict_proba()` output.\n            For binary problems, this list contains the class labels in sorted order of `predict_proba(as_multiclass=True)` output.\n                `class_labels[0]` corresponds to internal label = 0 (negative class), `class_labels[1]` corresponds to internal label = 1 (positive class).\n                This is relevant for certain metrics such as F1 where True and False labels impact the metric score differently.\n            For other problem types, will equal None.\n            For example if `pred = predict_proba(x, as_multiclass=True)`, then ith index of `pred` provides predicted probability that `x` belongs to class given by `class_labels[i]`.\n        class_labels_internal : list\n            For multiclass problems, this list contains the internal class labels in sorted order of internal `predict_proba()` output.\n            For binary problems, this list contains the internal class labels in sorted order of internal `predict_proba(as_multiclass=True)` output.\n                The value will always be `class_labels_internal=[0, 1]` for binary problems, with 0 as the negative class, and 1 as the positive class.\n            For other problem types, will equal None.\n        class_labels_internal_map : dict\n            For binary and multiclass classification problems, this dictionary contains the mapping of the original labels to the internal labels.\n            For example, in binary classification, label values of \'True\' and \'False\' will be mapped to the internal representation `1` and `0`.\n                Therefore, class_labels_internal_map would equal {\'True\': 1, \'False\': 0}\n            For other problem types, will equal None.\n            For multiclass, it is possible for not all of the label values to have a mapping.\n                This indicates that the internal models will never predict those missing labels, and training rows associated with the missing labels were dropped.\n\n        Examples\n        --------\n        >>> from autogluon import TabularPrediction as task\n        >>> train_data = task.Dataset(file_path=\'https://autogluon.s3.amazonaws.com/datasets/Inc/train.csv\')\n        >>> predictor = task.fit(train_data=train_data, label=\'class\')\n        >>> results = predictor.fit_summary()\n        >>> test_data = task.Dataset(file_path=\'https://autogluon.s3.amazonaws.com/datasets/Inc/test.csv\')\n        >>> perf = predictor.evaluate(test_data)\n\n    """"""\n\n    def __init__(self, learner):\n        """""" Creates TabularPredictor object.\n            You should not construct a TabularPredictor yourself, it is only intended to be produced during fit().\n\n            Parameters\n            ----------\n            learner : `AbstractLearner` object\n                Object that implements the `AbstractLearner` APIs.\n\n            To access any learner method `func()` from this Predictor, use: `predictor._learner.func()`.\n            To access any trainer method `func()` from this `Predictor`, use: `predictor._trainer.func()`.\n        """"""\n        self._learner: Learner = learner  # Learner object\n        self._learner.persist_trainer(low_memory=True)\n        self._trainer: AbstractTrainer = self._learner.load_trainer()  # Trainer object\n        self.output_directory = self._learner.path\n        self.problem_type = self._learner.problem_type\n        self.eval_metric = self._learner.objective_func\n        self.label_column = self._learner.label\n        self.feature_types = self._trainer.feature_types_metadata\n        self.model_names = self._trainer.get_model_names_all()  # TODO: Consider making this a function instead of a variable: This should never be de-synced with the output of self._trainer.get_model_names_all()\n        self.model_performance = self._trainer.model_performance  # TODO: Remove this in future, do not use this.\n        self.class_labels = self._learner.class_labels\n        self.class_labels_internal = self._learner.label_cleaner.ordered_class_labels_transformed\n        self.class_labels_internal_map = self._learner.label_cleaner.inv_map\n\n    def predict(self, dataset, model=None, as_pandas=False, use_pred_cache=False, add_to_pred_cache=False):\n        """""" Use trained models to produce predicted labels (in classification) or response values (in regression).\n\n            Parameters\n            ----------\n            dataset : str or :class:`TabularDataset` or `pandas.DataFrame`\n                The dataset to make predictions for. Should contain same column names as training Dataset and follow same format\n                (may contain extra columns that won\'t be used by Predictor, including the label-column itself).\n                If str is passed, `dataset` will be loaded using the str value as the file path.\n            model : str (optional)\n                The name of the model to get predictions from. Defaults to None, which uses the highest scoring model on the validation set.\n                Valid models are listed in this `predictor` by calling `predictor.model_names`\n            as_pandas : bool (optional)\n                Whether to return the output as a pandas Series (True) or numpy array (False)\n            use_pred_cache : bool (optional)\n                Whether to used previously-cached predictions for table rows we have already predicted on before\n                (can speedup repeated runs of `predict()` on multiple datasets\xc2\xa0with overlapping rows between them).\n            add_to_pred_cache : bool (optional)\n                Whether these predictions should be cached for reuse in future `predict()` calls on the same table rows\n                (can speedup repeated runs of `predict()` on multiple datasets\xc2\xa0with overlapping rows between them).\n\n            Returns\n            -------\n            Array of predictions, one corresponding to each row in given dataset. Either numpy Ndarray or pandas Series depending on `as_pandas` argument.\n\n        """"""\n        dataset = self.__get_dataset(dataset)\n        return self._learner.predict(X_test=dataset, model=model, as_pandas=as_pandas, use_pred_cache=use_pred_cache, add_to_pred_cache=add_to_pred_cache)\n\n    def predict_proba(self, dataset, model=None, as_pandas=False, as_multiclass=False):\n        """""" Use trained models to produce predicted class probabilities rather than class-labels (if task is classification).\n\n            Parameters\n            ----------\n            dataset : str or :class:`TabularDataset` or `pandas.DataFrame`\n                The dataset to make predictions for. Should contain same column names as training Dataset and follow same format\n                (may contain extra columns that won\'t be used by Predictor, including the label-column itself).\n                If str is passed, `dataset` will be loaded using the str value as the file path.\n            model : str (optional)\n                The name of the model to get prediction probabilities from. Defaults to None, which uses the highest scoring model on the validation set.\n                Valid models are listed in this `predictor` by calling `predictor.model_names`\n            as_pandas : bool (optional)\n                Whether to return the output as a pandas object (True) or numpy array (False).\n                Pandas object is a DataFrame if this is a multiclass problem or `as_multiclass=True`, otherwise it is a Series.\n                If the output is a DataFrame, the column order will be equivalent to `predictor.class_labels`.\n            as_multiclass : bool (optional)\n                Whether to return binary classification probabilities as if they were for multiclass classification.\n                    Output will contain two columns, and if `as_pandas=True`, the column names will correspond to the binary class labels.\n                    The columns will be the same order as `predictor.class_labels`.\n                Only impacts output for binary classification problems.\n\n            Returns\n            -------\n            Array of predicted class-probabilities, corresponding to each row in the given dataset.\n            May be a numpy ndarray or pandas Series/DataFrame depending on `as_pandas` and `as_multiclass` arguments and the type of prediction problem.\n            For binary classification problems, the output contains for each datapoint only the predicted probability of the positive class, unless you specify `as_multiclass=True`.\n        """"""\n        dataset = self.__get_dataset(dataset)\n        return self._learner.predict_proba(X_test=dataset, model=model, as_pandas=as_pandas, as_multiclass=as_multiclass)\n\n    def evaluate(self, dataset, silent=False):\n        """""" Report the predictive performance evaluated for a given Dataset.\n            This is basically a shortcut for: `pred = predict(dataset); evaluate_predictions(dataset[label_column], preds, auxiliary_metrics=False)`\n            that automatically uses `predict_proba()` instead of `predict()` when appropriate.\n\n            Parameters\n            ----------\n            dataset : str or :class:`TabularDataset` or `pandas.DataFrame`\n                This Dataset must also contain the label-column with the same column-name as specified during `fit()`.\n                If str is passed, `dataset` will be loaded using the str value as the file path.\n\n            silent : bool (optional)\n                Should performance results be printed?\n\n            Returns\n            -------\n            Predictive performance value on the given dataset, based on the `eval_metric` used by this Predictor.\n        """"""\n        dataset = self.__get_dataset(dataset)\n        perf = self._learner.score(dataset)\n        sign = self._learner.objective_func._sign\n        perf = perf * sign  # flip negative once again back to positive (so higher is no longer necessarily better)\n        if not silent:\n            print(""Predictive performance on given dataset: %s = %s"" % (self.eval_metric, perf))\n        return perf\n\n    def evaluate_predictions(self, y_true, y_pred, silent=False, auxiliary_metrics=False, detailed_report=True):\n        """""" Evaluate the provided predictions against ground truth labels.\n            Evaluation is based on the `eval_metric` previously specifed to `fit()`, or default metrics if none was specified.\n\n            Parameters\n            ----------\n            y_true : list or `numpy.array`\n                The ordered collection of ground-truth labels.\n            y_pred : list or `numpy.array`\n                The ordered collection of predictions.\n                Caution: For certain types of `eval_metric` (such as \'roc_auc\'), `y_pred` must be predicted-probabilities rather than predicted labels.\n            silent : bool (optional)\n                Should performance results be printed?\n            auxiliary_metrics: bool (optional)\n                Should we compute other (`problem_type` specific) metrics in addition to the default metric?\n            detailed_report : bool (optional)\n                Should we computed more detailed versions of the `auxiliary_metrics`? (requires `auxiliary_metrics = True`)\n\n            Returns\n            -------\n            Scalar performance value if `auxiliary_metrics = False`.\n            If `auxiliary_metrics = True`, returns dict where keys = metrics, values = performance along each metric.\n        """"""\n        return self._learner.evaluate(y_true=y_true, y_pred=y_pred, silent=silent,\n                                      auxiliary_metrics=auxiliary_metrics, detailed_report=detailed_report)\n\n    def leaderboard(self, dataset=None, only_pareto_frontier=False, silent=False):\n        """"""\n            Output summary of information about models produced during fit() as a pandas DataFrame.\n            Includes information on test and validation scores for all models, model training times, inference times, and stack levels.\n            Output DataFrame columns include:\n                \'model\': The name of the model.\n                \'score_val\': The validation score of the model on the \'eval_metric\'.\n                \'pred_time_val\': The inference time required to compute predictions on the validation data end-to-end.\n                    Equivalent to the sum of all \'pred_time_val_marginal\' values for the model and all of its base models.\n                \'fit_time\': The fit time required to train the model end-to-end (Including base models if the model is a stack ensemble).\n                    Equivalent to the sum of all \'fit_time_marginal\' values for the model and all of its base models.\n                \'pred_time_val_marginal\': The inference time required to compute predictions on the validation data (Ignoring inference times for base models).\n                    Note that this ignores the time required to load the model into memory when bagging is disabled.\n                \'fit_time_marginal\': The fit time required to train the model (Ignoring base models).\n                \'stack_level\': The stack level of the model.\n                    A model with stack level N can take any set of models with stack level less than N as input, with stack level 0 models having no model inputs.\n\n            Parameters\n            ----------\n            dataset : str or :class:`TabularDataset` or `pandas.DataFrame` (optional)\n                This Dataset must also contain the label-column with the same column-name as specified during fit().\n                If specified, then the leaderboard returned will contain additional columns \'score_test\', \'pred_time_test\', and \'pred_time_test_marginal\'.\n                    \'score_test\': The score of the model on the \'eval_metric\' for the dataset provided.\n                    \'pred_time_test\': The true end-to-end wall-clock inference time of the model for the dataset provided.\n                        Equivalent to the sum of all \'pred_time_test_marginal\' values for the model and all of its base models.\n                    \'pred_time_test_marginal\': The inference time of the model for the dataset provided, minus the inference time for the model\'s base models, if it has any.\n                        Note that this ignores the time required to load the model into memory when bagging is disabled.\n                If str is passed, `dataset` will be loaded using the str value as the file path.\n            only_pareto_frontier : bool (optional)\n                If `True`, only return model information of models in the Pareto frontier of the accuracy/latency trade-off (models which achieve the highest score within their end-to-end inference time).\n                At minimum this will include the model with the highest score and the model with the lowest inference time.\n                This is useful when deciding which model to use during inference if inference time is a consideration.\n                Models filtered out by this process would never be optimal choices for a user that only cares about model inference time and score.\n            silent : bool (optional)\n                Should leaderboard DataFrame be printed?\n\n            Returns\n            -------\n            Pandas `pandas.DataFrame` of model performance summary information.\n        """"""\n        dataset = self.__get_dataset(dataset) if dataset is not None else dataset\n        return self._learner.leaderboard(X=dataset, only_pareto_frontier=only_pareto_frontier, silent=silent)\n\n    def fit_summary(self, verbosity=3):\n        """"""\n            Output summary of information about models produced during `fit()`.\n            May create various generated summary plots and store them in folder: `Predictor.output_directory`.\n\n            Parameters\n            ----------\n            verbosity : int,\xc2\xa0default = 3\n                Controls how detailed of a summary to ouput.\n                Set <= 0 for no output printing, 1 to print just high-level summary,\n                2 to print summary and create plots, >= 3 to print all information produced during fit().\n\n            Returns\n            -------\n            Dict containing various detailed information. We do not recommend directly printing this dict as it may be very large.\n        """"""\n        hpo_used = len(self._trainer.hpo_results) > 0\n        model_typenames = {key: self._trainer.model_types[key].__name__ for key in self._trainer.model_types}\n        model_innertypenames = {key: self._trainer.model_types_inner[key].__name__ for key in self._trainer.model_types if key in self._trainer.model_types_inner}\n        MODEL_STR = \'Model\'\n        ENSEMBLE_STR = \'Ensemble\'\n        for model in model_typenames:\n            if (model in model_innertypenames) and (ENSEMBLE_STR not in model_innertypenames[model]) and (ENSEMBLE_STR in model_typenames[model]):\n                new_model_typename = model_typenames[model] + ""_"" + model_innertypenames[model]\n                if new_model_typename.endswith(MODEL_STR):\n                    new_model_typename = new_model_typename[:-len(MODEL_STR)]\n                model_typenames[model] = new_model_typename\n\n        unique_model_types = set(model_typenames.values())  # no more class info\n        # all fit() information that is returned:\n        results = {\n            \'model_types\': model_typenames,  # dict with key = model-name, value = type of model (class-name)\n            \'model_performance\': self._trainer.get_model_attributes_dict(\'val_score\'),  # dict with key = model-name, value = validation performance\n            \'model_best\': self._trainer.model_best,  # the name of the best model (on validation data)\n            \'model_paths\': self._trainer.model_paths,  # dict with key = model-name, value = path to model file\n            \'model_fit_times\': self._trainer.get_model_attributes_dict(\'fit_time\'),\n            \'model_pred_times\': self._trainer.get_model_attributes_dict(\'predict_time\'),\n            \'num_bagging_folds\': self._trainer.kfolds,\n            \'stack_ensemble_levels\': self._trainer.stack_ensemble_levels,\n            \'feature_prune\': self._trainer.feature_prune,\n            \'hyperparameter_tune\': hpo_used,\n            \'hyperparameters_userspecified\': self._trainer.hyperparameters,\n        }\n        if self.problem_type != REGRESSION:\n            results[\'num_classes\'] = self._trainer.num_classes\n        if hpo_used:\n            results[\'hpo_results\'] = self._trainer.hpo_results\n        # get dict mapping model name to final hyperparameter values for each model:\n        model_hyperparams = {}\n        for model_name in self._trainer.get_model_names_all():\n            model_obj = self._trainer.load_model(model_name)\n            model_hyperparams[model_name] = model_obj.params\n        results[\'model_hyperparams\'] = model_hyperparams\n\n        if verbosity > 0:  # print stuff\n            print(""*** Summary of fit() ***"")\n            print(""Estimated performance of each model:"")\n            results[\'leaderboard\'] = self._learner.leaderboard(silent=False)\n            # self._summarize(\'model_performance\', \'Validation performance of individual models\', results)\n            #  self._summarize(\'model_best\', \'Best model (based on validation performance)\', results)\n            # self._summarize(\'hyperparameter_tune\', \'Hyperparameter-tuning used\', results)\n            print(""Number of models trained: %s"" % len(results[\'model_performance\']))\n            print(""Types of models trained:"")\n            print(unique_model_types)\n            num_fold_str = """"\n            bagging_used = results[\'num_bagging_folds\'] > 0\n            if bagging_used:\n                num_fold_str = f"" (with {results[\'num_bagging_folds\']} folds)""\n            print(""Bagging used: %s %s"" % (bagging_used, num_fold_str))\n            num_stack_str = """"\n            stacking_used = results[\'stack_ensemble_levels\'] > 0\n            if stacking_used:\n                num_stack_str = f"" (with {results[\'stack_ensemble_levels\']} levels)""\n            print(""Stack-ensembling used: %s %s"" % (stacking_used, num_stack_str))\n            hpo_str = """"\n            if hpo_used and verbosity <= 2:\n                hpo_str = "" (call fit_summary() with verbosity >= 3 to see detailed HPO info)""\n            print(""Hyperparameter-tuning used: %s %s"" % (hpo_used, hpo_str))\n            # TODO: uncomment once feature_prune is functional:  self._summarize(\'feature_prune\', \'feature-selection used\', results)\n            print(""User-specified hyperparameters:"")\n            print(results[\'hyperparameters_userspecified\'])\n        if verbosity > 1:  # create plots\n            plot_tabular_models(results, output_directory=self.output_directory,\n                                save_file=""SummaryOfModels.html"",\n                                plot_title=""Models produced during fit()"")\n            if hpo_used:\n                for model_type in results[\'hpo_results\']:\n                    plot_summary_of_models(\n                        results[\'hpo_results\'][model_type],\n                        output_directory=self.output_directory, save_file=model_type + ""_HPOmodelsummary.html"",\n                        plot_title=f""Models produced during {model_type} HPO"")\n                    plot_performance_vs_trials(\n                        results[\'hpo_results\'][model_type],\n                        output_directory=self.output_directory, save_file=model_type + ""_HPOperformanceVStrials.png"",\n                        plot_title=f""HPO trials for {model_type} models"")\n        if verbosity > 2:  # print detailed information\n            if hpo_used:\n                hpo_results = results[\'hpo_results\']\n                print(""*** Details of Hyperparameter optimization ***"")\n                for model_type in hpo_results:\n                    hpo_model = hpo_results[model_type]\n                    print(""HPO for %s model:  Num. configurations tried = %s, Time spent = %s, Search strategy = %s""\n                          % (model_type, len(hpo_model[\'trial_info\']), hpo_model[\'total_time\'], hpo_model[\'search_strategy\']))\n                    print(""Best hyperparameter-configuration (validation-performance: %s = %s):""\n                          % (self.eval_metric, hpo_model[\'validation_performance\']))\n                    print(hpo_model[\'best_config\'])\n            """"""\n            if bagging_used:\n                pass # TODO: print detailed bagging info\n            if stacking_used:\n                pass # TODO: print detailed stacking info, like how much it improves validation performance\n            if results[\'feature_prune\']:\n                pass # TODO: print detailed feature-selection info once feature-selection is functional.\n            """"""\n        if verbosity > 0:\n            print(""*** End of fit() summary ***"")\n        return results\n\n    def transform_features(self, dataset=None, model=None, base_models=None):\n        """"""\n        Transforms dataset features through the AutoGluon feature generator.\n        This is useful to gain an understanding of how AutoGluon interprets the dataset features.\n        The output of this function can be used to train further models, even outside of AutoGluon.\n        This can be useful for training your own models on the same data representation as AutoGluon.\n        Individual AutoGluon models like the neural network may apply additional feature transformations that are not reflected in this method.\n        This method only applies universal transforms employed by all AutoGluon models.\n        When `dataset=None`, `base_models=[{best_model}], and bagging was enabled during fit():\n            This returns the out-of-fold predictions of the best model, which can be used as training input to a custom user stacker model.\n\n        Parameters\n        ----------\n        dataset : str or :class:`TabularDataset` or `pandas.DataFrame` (optional)\n            The dataset to apply feature transformation to.\n            This dataset does not require the label column.\n            If str is passed, `dataset` will be loaded using the str value as the file path.\n            If not specified, the original dataset used during fit() will be used if fit() was previously called with `cache_data=True`. Otherwise, an exception will be raised.\n                For non-bagged mode predictors:\n                    The dataset used when not specified is the validation set.\n                    This can either be an automatically generated validation set or the user-defined `tuning_data` if passed during fit().\n                    If all parameters are unspecified, then the output is equivalent to `predictor.load_data_internal(dataset=\'val\', return_X=True, return_y=False)[0]`.\n                    To get the label values of the output, call `predictor.load_data_internal(dataset=\'val\', return_X=False, return_y=True)[1]`.\n                    If the original training set is desired, it can be passed in through `dataset`.\n                        Warning: Do not pass the original training set if `model` or `base_models` are set. This will result in overfit feature transformation.\n                For bagged mode predictors:\n                    The dataset used when not specified is the full training set.\n                    If all parameters are unspecified, then the output is equivalent to `predictor.load_data_internal(dataset=\'train\', return_X=True, return_y=False)[0]`.\n                    To get the label values of the output, call `predictor.load_data_internal(dataset=\'train\', return_X=False, return_y=True)[1]`.\n                    `base_model` features generated in this instance will be from out-of-fold predictions.\n                    Note that the training set may differ from the training set originally passed during fit(), as AutoGluon may choose to drop or duplicate rows during training.\n                    Warning: Do not pass the original training set through `dataset` if `model` or `base_models` are set. This will result in overfit feature transformation. Instead set `dataset=None`.\n        model : str, default = None\n            Model to generate input features for.\n            The output data will be equivalent to the input data that would be sent into `model.predict_proba(data)`.\n                Note: This only applies to cases where `dataset` is not the training data.\n            If `None`, then only return generically preprocessed features prior to any model fitting.\n            Valid models are listed in this `predictor` by calling `predictor.model_names`.\n            Specifying a `refit_full` model will cause an exception if `dataset=None`.\n            `base_models=None` is a requirement when specifying `model`.\n        base_models : list, default = None\n            List of model names to use as base_models for a hypothetical stacker model when generating input features.\n            If `None`, then only return generically preprocessed features prior to any model fitting.\n            Valid models are listed in this `predictor` by calling `predictor.model_names`.\n            If a stacker model S exists with `base_models=M`, then setting `base_models=M` is equivalent to setting `model=S`.\n            `model=None` is a requirement when specifying `base_models`.\n\n        Returns\n        -------\n        Pandas `pandas.DataFrame` of the provided `dataset` after feature transformation has been applied.\n        This output does not include the label column, and will remove it if present in the supplied `dataset`.\n        If a transformed label column is desired, use `predictor.transform_labels`.\n\n        Examples\n        --------\n        >>> from autogluon import TabularPrediction as task\n        >>> train_data = task.Dataset(\'train.csv\')\n        >>> predictor = task.fit(train_data=train_data, label=\'class\', auto_stack=True, cache_data=True)  # predictor is in bagged mode and `cache_data=True`.\n        >>> model = \'weighted_ensemble_k0_l1\'\n        >>> test_data = task.Dataset(\'test.csv\')\n        >>> train_data_transformed = predictor.transform_features(model=model)  # Internal training DataFrame used as input to `model.fit()` during `predictor = task.fit(train_data=train_data, ...)`\n        >>> test_data_transformed = predictor.transform_features(dataset=test_data, model=model)  # Internal test DataFrame used as input to `model.predict_proba()` during `predictor.predict_proba(test_data, model=model)`\n\n        """"""\n        dataset = self.__get_dataset(dataset) if dataset is not None else dataset\n        # TODO: Make this index fix inside of learner/trainer once the defect is identified. For now, this resolves the issue.\n        if dataset is not None:\n            original_indices = copy.deepcopy(dataset.index)\n            dataset = dataset.reset_index(drop=True)\n        else:\n            original_indices = None\n\n        dataset_transformed = self._learner.get_inputs_to_stacker(dataset=dataset, model=model, base_models=base_models)\n        if original_indices is not None:\n            dataset_transformed.index = original_indices\n        return dataset_transformed\n\n    # TODO: Add support for DataFrame input and support for DataFrame output\n    #  Add support for retaining DataFrame/Series indices in output\n    #  Add as_pandas parameter\n    def transform_labels(self, labels, inverse=False, proba=False):\n        """"""\n        Transforms dataset labels to the internal label representation.\n        This can be useful for training your own models on the same data label representation as AutoGluon.\n        Regression problems do not differ between original and internal representation, and thus this method will return the provided labels.\n        Warning: When `inverse=False`, it is possible for the output to contain NaN label values in multiclass problems if the provided label was dropped during training.\n\n        Parameters\n        ----------\n        labels : `numpy.ndarray` or `pandas.Series`\n            Labels to transform.\n            If `proba=False`, an example input would be the output of `predictor.predict(test_data)`.\n            If `proba=True`, an example input would be the output of `predictor.predict_proba(test_data)`.\n        inverse : boolean, default = False\n            When `True`, the input labels are treated as being in the internal representation and the original representation is outputted.\n        proba : boolean, default = False\n            When `True`, the input labels are treated as probabilities and the output will be the internal representation of probabilities.\n                In this case, it is expected that `labels` be a `numpy.ndarray`.\n                If the `problem_type` is multiclass:\n                    The input column order must be equal to `predictor.class_labels`.\n                    The output column order will be equal to `predictor.class_labels_internal`.\n                    if `inverse=True`, the same logic applies, but with input and output columns interchanged.\n            When `False`, the input labels are treated as actual labels and the output will be the internal representation of the labels.\n                In this case, it is expected that `labels` be a `pandas.Series` or `numpy.ndarray`.\n\n        Returns\n        -------\n        Pandas `pandas.Series` of labels if `proba=False` or Numpy `numpy.ndarray` of label probabilities if `proba=True`\n\n        """"""\n        if inverse:\n            if proba:\n                labels_transformed = self._learner.label_cleaner.inverse_transform_proba(y=labels)\n            else:\n                labels_transformed = self._learner.label_cleaner.inverse_transform(y=labels)\n        else:\n            if proba:\n                labels_transformed = self._learner.label_cleaner.transform_proba(y=labels)\n            else:\n                labels_transformed = self._learner.label_cleaner.transform(y=labels)\n        return labels_transformed\n\n    # TODO: Consider adding time_limit option to early stop the feature importance process\n    def feature_importance(self, dataset=None, model=None, features=None, feature_stage=\'original\', subsample_size=1000, silent=False, **kwargs):\n        """"""\n        Calculates feature importance scores for the given model.\n        A feature\'s importance score represents the performance drop that results when the model makes predictions on a perturbed copy of the dataset where this feature\'s values have been randomly shuffled across rows.\n        A feature score of 0.01 would indicate that the predictive performance dropped by 0.01 when the feature was randomly shuffled.\n        The higher the score a feature has, the more important it is to the model\'s performance.\n        If a feature has a negative score, this means that the feature is likely harmful to the final model, and a model trained with the feature removed would be expected to achieve a better predictive performance.\n        Note that calculating feature importance can be a very computationally expensive process, particularly if the model uses hundreds or thousands of features. In many cases, this can take longer than the original model training.\n        To estimate how long `feature_importance(model, dataset, features)` will take, it is roughly the time taken by `predict_proba(dataset, model)` multiplied by the number of features.\n\n        Parameters\n        ----------\n        dataset : str or :class:`TabularDataset` or `pandas.DataFrame` (optional)\n            This dataset must also contain the label-column with the same column-name as specified during `fit()`.\n            If specified, then the dataset is used to calculate the feature importance scores.\n            If str is passed, `dataset` will be loaded using the str value as the file path.\n            If not specified, the original dataset used during `fit()` will be used if `cache_data=True`. Otherwise, an exception will be raised.\n            Do not pass the training data through this argument, as the feature importance scores calculated will be biased due to overfitting.\n                More accurate feature importances will be obtained from new data that was held-out during `fit()`.\n        model : str, default = None\n            Model to get feature importances for, if None the best model is chosen.\n            Valid models are listed in this `predictor` by calling `predictor.model_names`\n        features : list, default = None\n            List of str feature names that feature importances are calculated for and returned, specify None to get all feature importances.\n            If you only want to compute feature importances for some of the features, you can pass their names in as a list of str.\n            Valid feature names change depending on the `feature_stage`.\n                To get the list of feature names for `feature_stage=\'transformed\'`, call `list(predictor.transform_features().columns)`.\n                To get the list of feature names for `feature_stage=`transformed_model`, call `list(predictor.transform_features(model={model_name}).columns)`.\n        feature_stage : str, default = \'original\'\n            What stage of feature-processing should importances be computed for.\n            Options:\n                \'original\':\n                    Compute importances of the original features.\n                    Warning: `dataset` must be specified with this option, otherwise an exception will be raised.\n                \'transformed\':\n                    Compute importances of the post-internal-transformation features (after automated feature engineering). These features may be missing some original features, or add new features entirely.\n                    An example of new features would be ngram features generated from a text column.\n                    Warning: For bagged models, feature importance calculation is not yet supported with this option when `dataset=None`. Doing so will raise an exception.\n                \'transformed_model\':\n                    Compute importances of the post-model-transformation features. These features are the internal features used by the requested model. They may differ greatly from the original features.\n                    If the model is a stack ensemble, this will include stack ensemble features such as the prediction probability features of the stack ensemble\'s base (ancestor) models.\n        subsample_size : int, default = 1000\n            The number of rows to sample from `dataset` when computing feature importance.\n            If `subsample_size=None` or `dataset` contains fewer than `subsample_size` rows, all rows will be used during computation.\n            Larger values increase the accuracy of the feature importance scores.\n            Runtime linearly scales with `subsample_size`.\n        silent : bool, default = False\n            Whether to suppress logging output\n\n        Returns\n        -------\n        Pandas `pandas.Series` of feature importance scores.\n\n        """"""\n        allowed_kwarg_names = {\'raw\'}\n        for kwarg_name in kwargs.keys():\n            if kwarg_name not in allowed_kwarg_names:\n                raise ValueError(""Unknown keyword argument specified: %s"" % kwarg_name)\n        if \'raw\' in kwargs.keys():\n            logger.log(30, \'Warning: `raw` is a deprecated parameter. Use `feature_stage` instead. Starting from AutoGluon 0.1.0, specifying `raw` as a parameter will cause an exception.\')\n            logger.log(30, \'Overriding `feature_stage` value with `raw` value.\')\n            raw = kwargs[\'raw\']\n            if raw is True:\n                feature_stage = \'transformed\'\n            elif raw is False:\n                feature_stage = \'transformed_model\'\n            else:\n                raise ValueError(f\'`raw` must be one of [True, False], but was {raw}.\')\n\n        dataset = self.__get_dataset(dataset) if dataset is not None else dataset\n        if (dataset is None) and (not self._trainer.is_data_saved):\n            raise AssertionError(\'No dataset was provided and there is no cached data to load for feature importance calculation. `cache_data=True` must be set in the `TabularPrediction.fit()` call to enable this functionality when dataset is not specified.\')\n\n        return self._learner.get_feature_importance(model=model, X=dataset, features=features, feature_stage=feature_stage, subsample_size=subsample_size, silent=silent)\n\n    def refit_full(self, model=\'all\'):\n        """"""\n        Retrain model on all of the data (training + validation).\n        For bagged models:\n            Optimizes a model\'s inference time by collapsing bagged ensembles into a single model fit on all of the training data.\n            This process will typically result in a slight accuracy reduction and a large inference speedup.\n            The inference speedup will generally be between 10-200x faster than the original bagged ensemble model.\n                The inference speedup factor is equivalent to (k * n), where k is the number of folds (`num_bagging_folds`) and n is the number of finished repeats (`num_bagging_sets`) in the bagged ensemble.\n            The runtime is generally 10% or less of the original fit runtime.\n                The runtime can be roughly estimated as 1 / (k * n) of the original fit runtime, with k and n defined above.\n        For non-bagged models:\n            Optimizes a model\'s accuracy by retraining on 100% of the data without using a validation set.\n            Will typically result in a slight accuracy increase and no change to inference time.\n            The runtime will be approximately equal to the original fit runtime.\n        This process does not alter the original models, but instead adds additional models.\n        If stacker models are refit by this process, they will use the refit_full versions of the ancestor models during inference.\n        Models produced by this process will not have validation scores, as they use all of the data for training.\n            Therefore, it is up to the user to determine if the models are of sufficient quality by including test data in `predictor.leaderboard(dataset=test_data)`.\n            If the user does not have additional test data, they should reference the original model\'s score for an estimate of the performance of the refit_full model.\n                Warning: Be aware that utilizing refit_full models without separately verifying on test data means that the model is untested, and has no guarantee of being consistent with the original model.\n        `cache_data` must have been set to `True` during the original training to enable this functionality.\n\n        Parameters\n        ----------\n        model : str, default = \'all\'\n            Model name of model to refit.\n                If \'all\' then all models are refitted.\n                If \'best\' then the model with the highest validation score is refit.\n            All ancestor models will also be refit in the case that the selected model is a weighted or stacker ensemble.\n            Valid models are listed in this `predictor` by calling `predictor.model_names`.\n\n        Returns\n        -------\n        Dictionary of original model names -> refit_full model names.\n        """"""\n        refit_full_dict = self._learner.refit_ensemble_full(model=model)\n        self.model_names = self._trainer.get_model_names_all()\n        return refit_full_dict\n\n    def get_model_full_dict(self):\n        """"""\n        Returns a dictionary of original model name -> refit full model name.\n        Empty unless `refit_full=True` was set during fit or `predictor.refit_full()` was called.\n        This can be useful when determining the best model based off of `predictor.leaderboard()`, then getting the _FULL version of the model by passing its name as the key to this dictionary.\n\n        Returns\n        -------\n        Dictionary of original model name -> refit full model name.\n        """"""\n        return copy.deepcopy(self._trainer.model_full_dict)\n\n    def info(self):\n        """"""\n        [EXPERIMENTAL] Returns a dictionary of `predictor` metadata.\n        Warning: This functionality is currently in preview mode.\n            The metadata information returned may change in structure in future versions without warning.\n            The definitions of various metadata values are not yet documented.\n            The output of this function should not be used for programmatic decisions.\n        Contains information such as row count, column count, model training time, validation scores, hyperparameters, and much more.\n\n        Returns\n        -------\n        Dictionary of `predictor` metadata.\n        """"""\n        return self._learner.get_info(include_model_info=True)\n\n    @classmethod\n    def load(cls, output_directory, verbosity=2):\n        """"""\n        Load a predictor object previously produced by `fit()` from file and returns this object.\n        Is functionally equivalent to :meth:`autogluon.task.tabular_prediction.TabularPrediction.load`.\n\n        Parameters\n        ----------\n        output_directory : str\n            Path to directory where trained models are stored (i.e. the `output_directory` specified in previous call to `fit()`).\n        verbosity : int, default = 2\n            Verbosity levels range from 0 to 4 and control how much information is generally printed by this Predictor.\n            Higher levels correspond to more detailed print statements (you can set verbosity = 0 to suppress warnings).\n            If using logging, you can alternatively control amount of information printed via `logger.setLevel(L)`,\n            where `L` ranges from 0 to 50 (Note: higher values `L` correspond to fewer print statements, opposite of verbosity levels)\n\n        Returns\n        -------\n        :class:`TabularPredictor` object\n        """"""\n        logger.setLevel(verbosity2loglevel(verbosity))  # Reset logging after load (may be in new Python session)\n        if output_directory is None:\n            raise ValueError(""output_directory cannot be None in load()"")\n\n        output_directory = setup_outputdir(output_directory)  # replace ~ with absolute path if it exists\n        learner = Learner.load(output_directory)\n        return cls(learner=learner)\n\n    def save(self):\n        """""" Save this predictor to file in directory specified by this Predictor\'s `output_directory`.\n            Note that `fit()` already saves the predictor object automatically\n            (we do not recommend modifying the Predictor object yourself as it tracks many trained models).\n        """"""\n        self._learner.save()\n        logger.log(20, ""TabularPredictor saved. To load, use: TabularPredictor.load(\\""%s\\"")"" % self.output_directory)\n\n    def load_data_internal(self, dataset=\'train\', return_X=True, return_y=True):\n        """"""\n        Loads the internal data representation used during model training.\n        Individual AutoGluon models like the neural network may apply additional feature transformations that are not reflected in this method.\n        This method only applies universal transforms employed by all AutoGluon models.\n        This will raise an exception if `cache_data=False` was set in `task.fit()`.\n        Warning, the internal representation may:\n            Have different features compared to the original data.\n            Have different row counts compared to the original data.\n            Have indices which do not align with the original data.\n            Have label values which differ from those in the original data.\n        Internal data representations should NOT be combined with the original data, in most cases this is not possible.\n\n        Parameters\n        ----------\n        dataset : str, default = \'train\'\n            The dataset to load.\n            Valid values are:\n                \'train\':\n                    Load the training data used during model training.\n                    This is a transformed and augmented version of the `train_data` passed in `task.fit()`.\n                \'val\':\n                    Load the validation data used during model training.\n                    This is a transformed and augmented version of the `tuning_data` passed in `task.fit()`.\n                    If `tuning_data=None` was set in `task.fit()`, then `tuning_data` is an automatically generated validation set created by splitting `train_data`.\n                    Warning: Will raise an exception if called by a bagged predictor, as bagged predictors have no validation data.\n        return_X : bool, default = True\n            Whether to return the internal data features\n            If set to `False`, then the first element in the returned tuple will be None.\n        return_y : bool, default = True\n            Whether to return the internal data labels\n            If set to `False`, then the second element in the returned tuple will be None.\n\n        Returns\n        -------\n        Tuple of (`pandas.DataFrame`, `pandas.Series`) corresponding to the internal data features and internal data labels, respectively.\n\n        """"""\n        if dataset == \'train\':\n            load_X = self._trainer.load_X_train\n            load_y = self._trainer.load_y_train\n        elif dataset == \'val\':\n            load_X = self._trainer.load_X_val\n            load_y = self._trainer.load_y_val\n        else:\n            raise ValueError(f\'dataset must be one of: [\\\'train\\\', \\\'val\\\'], but was \\\'{dataset}\\\'.\')\n        X = load_X() if return_X else None\n        y = load_y() if return_y else None\n        return X, y\n\n    def save_space(self, remove_data=True, remove_fit_stack=True, requires_save=True, reduce_children=False):\n        """"""\n        Reduces the memory and disk size of predictor by deleting auxiliary model files that aren\'t needed for prediction on new data.\n        This function has NO impact on inference accuracy.\n        It is recommended to invoke this method if the only goal is to use the trained model for prediction.\n        However, certain advanced functionality may no longer be available after `save_space()` has been called.\n\n        Parameters\n        ----------\n        remove_data : bool, default = True\n            Whether to remove cached files of the original training and validation data.\n            Only reduces disk usage, it has no impact on memory usage.\n            This is especially useful when the original data was large.\n            This is equivalent to setting `cache_data=False` during the original `fit()`.\n                Will disable all advanced functionality that requires `cache_data=True`.\n        remove_fit_stack : bool, default = True\n            Whether to remove information required to fit new stacking models and continue fitting bagged models with new folds.\n            Only reduces disk usage, it has no impact on memory usage.\n            This includes:\n                out-of-fold (OOF) predictions\n            This is useful for multiclass problems with many classes, as OOF predictions can become very large on disk. (1 GB per model in extreme cases)\n            This disables `predictor.refit_full()` for stacker models.\n        requires_save : bool, default = True\n            Whether to remove information that requires the model to be saved again to disk.\n            Typically this only includes flag variables that don\'t have significant impact on memory or disk usage, but should technically be updated due to the removal of more important information.\n                An example is the `is_data_saved` boolean variable in `trainer`, which should be updated to `False` if `remove_data=True` was set.\n        reduce_children : bool, default = False\n            Whether to apply the reduction rules to bagged ensemble children models. These are the models trained for each fold of the bagged ensemble.\n            This should generally be kept as `False` since the most important memory and disk reduction techniques are automatically applied to these models during the original `fit()` call.\n\n        """"""\n        self._trainer.reduce_memory_size(remove_data=remove_data, remove_fit_stack=remove_fit_stack, remove_fit=True, remove_info=False, requires_save=requires_save, reduce_children=reduce_children)\n\n    def delete_models(self, models_to_keep=None, models_to_delete=None, allow_delete_cascade=False, delete_from_disk=True, dry_run=True):\n        """"""\n        Deletes models from `predictor`.\n        This can be helpful to minimize memory usage and disk usage, particularly for model deployment.\n        This will remove all references to the models in `predictor`.\n            For example, removed models will not appear in `predictor.leaderboard()`.\n        WARNING: If `delete_from_disk=True`, this will DELETE ALL FILES in the deleted model directories, regardless if they were created by AutoGluon or not.\n            DO NOT STORE FILES INSIDE OF THE MODEL DIRECTORY THAT ARE UNRELATED TO AUTOGLUON.\n\n        Parameters\n        ----------\n        models_to_keep : str or list, default = None\n            Name of model or models to not delete.\n            All models that are not specified and are also not required as a dependency of any model in `models_to_keep` will be deleted.\n            Specify `models_to_keep=\'best\'` to keep only the best model and its model dependencies.\n            `models_to_delete` must be None if `models_to_keep` is set.\n            To see the list of possible model names, use: `predictor.model_names` or `predictor.leaderboard()`.\n        models_to_delete : str or list, default = None\n            Name of model or models to delete.\n            All models that are not specified but depend on a model in `models_to_delete` will also be deleted.\n            `models_to_keep` must be None if `models_to_delete` is set.\n        allow_delete_cascade : bool, default = False\n            If `False`, if unspecified dependent models of models in `models_to_delete` exist an exception will be raised instead of deletion occurring.\n                An example of a dependent model is m1 if m2 is a stacker model and takes predictions from m1 as inputs. In this case, m1 would be a dependent model of m2.\n            If `True`, all dependent models of models in `models_to_delete` will be deleted.\n            Has no effect if `models_to_delete=None`.\n        delete_from_disk : bool, default = True\n            If `True`, deletes the models from disk if they were persisted.\n            WARNING: This deletes the entire directory for the deleted models, and ALL FILES located there.\n                It is highly recommended to first run with `dry_run=True` to understand which directories will be deleted.\n        dry_run : bool, default = True\n            If `True`, then deletions don\'t occur, and logging statements are printed describing what would have occurred.\n            Set `dry_run=False` to perform the deletions.\n\n        """"""\n        if models_to_keep == \'best\':\n            models_to_keep = self._trainer.model_best\n            if models_to_keep is None:\n                models_to_keep = self._trainer.get_model_best()\n        self._trainer.delete_models(models_to_keep=models_to_keep, models_to_delete=models_to_delete, allow_delete_cascade=allow_delete_cascade, delete_from_disk=delete_from_disk, dry_run=dry_run)\n        self.model_names = self._trainer.get_model_names_all()\n\n    @staticmethod\n    def _summarize(key, msg, results):\n        if key in results:\n            print(msg + "": "" + str(results[key]))\n\n    @staticmethod\n    def __get_dataset(dataset):\n        if isinstance(dataset, TabularDataset):\n            return dataset\n        elif isinstance(dataset, pd.DataFrame):\n            return TabularDataset(df=dataset)\n        elif isinstance(dataset, str):\n            return TabularDataset(file_path=dataset)\n        elif isinstance(dataset, pd.Series):\n            raise TypeError(""dataset must be TabularDataset or pandas.DataFrame, not pandas.Series. \\\n                   To predict on just single example (ith row of table), use dataset.iloc[[i]] rather than dataset.iloc[i]"")\n        else:\n            raise TypeError(""dataset must be TabularDataset or pandas.DataFrame or str file path to dataset"")\n'"
autogluon/task/tabular_prediction/presets_configs.py,0,"b""\nimport functools\n\n\ndef unpack(g):\n    def _unpack_inner(f):\n        @functools.wraps(f)\n        def _call(**kwargs):\n            return f(**g(**kwargs))\n        return _call\n    return _unpack_inner\n\n\n# Dictionary of preset fit() parameter configurations.\npreset_dict = dict(\n    # Best predictive accuracy with little consideration to inference time or disk usage. Achieve even better results by specifying a large time_limits value.\n    # Recommended for applications that benefit from the best possible model accuracy.\n    best_quality={'auto_stack': True},\n\n    # Identical to best_quality but additionally trains refit_full models that have slightly lower predictive accuracy but are over 10x faster during inference and require 10x less disk space.\n    best_quality_with_high_quality_refit={'auto_stack': True, 'refit_full': True},\n\n    # High predictive accuracy with fast inference. ~10x-200x faster inference and ~10x-200x lower disk usage than `best_quality`.\n    # Recommended for applications that require reasonable inference speed and/or model size.\n    high_quality_fast_inference_only_refit={'auto_stack': True, 'refit_full': True, 'set_best_to_refit_full': True, 'save_bagged_folds': False},\n\n    # Good predictive accuracy with very fast inference. ~4x faster inference and ~4x lower disk usage than `high_quality_fast_inference_only_refit`.\n    # Recommended for applications that require fast inference speed.\n    good_quality_faster_inference_only_refit={'auto_stack': True, 'refit_full': True, 'set_best_to_refit_full': True, 'save_bagged_folds': False, 'hyperparameters': 'light'},\n\n    # Medium predictive accuracy with very fast inference and very fast training time. ~20x faster training than `good_quality_faster_inference_only_refit`.\n    # This is the default preset in AutoGluon, but should generally only be used for quick prototyping, as `good_quality_faster_inference_only_refit` results in significantly better predictive accuracy and faster inference time.\n    medium_quality_faster_train={'auto_stack': False},\n\n    # Optimizes result immediately for deployment by deleting unused models and removing training artifacts.\n    # Often can reduce disk usage by ~2-4x with no negatives to model accuracy or inference speed.\n    # This will disable numerous advanced functionality, but has no impact on inference.\n    # Recommended for applications where the inner details of AutoGluon's training is not important and there is no intention of manually choosing between the final models.\n    # This preset pairs well with the other presets such as `good_quality_faster_inference_only_refit` to make a very compact final model.\n    # Identical to calling `predictor.delete_models(models_to_keep='best', dry_run=False)` and `predictor.save_space()` directly after `fit()`.\n    optimize_for_deployment={'keep_only_best': True, 'save_space': True},\n\n    # Disables automated feature generation when text features are detected.\n    # This is useful to determine how beneficial text features are to the end result, as well as to ensure features are not mistaken for text when they are not.\n    ignore_text={'feature_generator_kwargs': {'enable_nlp_vectorizer_features': False, 'enable_nlp_ratio_features': False}},\n\n    # TODO: Consider HPO-enabled configs if training time doesn't matter but inference latency does.\n)\n\n\ndef set_presets(**kwargs):\n    if 'presets' in kwargs:\n        presets = kwargs['presets']\n        if presets is None:\n            return kwargs\n        if not isinstance(presets, list):\n            presets = [presets]\n        preset_kwargs = {}\n        for preset in presets:\n            if isinstance(preset, str):\n                preset_orig = preset\n                preset = preset_dict.get(preset, None)\n                if preset is None:\n                    raise ValueError(f'Preset \\'{preset_orig}\\' was not found. Valid presets: {list(preset_dict.keys())}')\n            if isinstance(preset, dict):\n                for key in preset:\n                    preset_kwargs[key] = preset[key]\n            else:\n                raise TypeError(f'Preset of type {type(preset)} was given, but only presets of type [dict, str] are valid.')\n        for key in preset_kwargs:\n            if key not in kwargs:\n                kwargs[key] = preset_kwargs[key]\n    return kwargs\n"""
autogluon/task/tabular_prediction/tabular_prediction.py,0,"b'import copy\nimport logging\nimport math\n\nimport numpy as np\n\nfrom .dataset import TabularDataset\nfrom .hyperparameter_configs import get_hyperparameter_config\nfrom .predictor import TabularPredictor\nfrom .presets_configs import set_presets, unpack\nfrom ..base import BaseTask, compile_scheduler_options\nfrom ..base.base_task import schedulers\nfrom ...utils import verbosity2loglevel\nfrom ...utils.tabular.features.auto_ml_feature_generator import AutoMLFeatureGenerator\nfrom ...utils.tabular.metrics import get_metric\nfrom ...utils.tabular.ml.learner.default_learner import DefaultLearner as Learner\nfrom ...utils.tabular.ml.trainer.auto_trainer import AutoTrainer\nfrom ...utils.tabular.ml.utils import setup_outputdir, setup_compute, setup_trial_limits\n\n__all__ = [\'TabularPrediction\']\n\nlogger = logging.getLogger()  # return root logger\n\n\nclass TabularPrediction(BaseTask):\n    """"""\n    AutoGluon Task for predicting values in column of tabular dataset (classification or regression)\n    """"""\n\n    Dataset = TabularDataset\n    Predictor = TabularPredictor\n\n    @staticmethod\n    def load(output_directory, verbosity=2):\n        """"""\n        Load a predictor object previously produced by `fit()` from file and returns this object.\n\n        Parameters\n        ----------\n        output_directory : str\n            Path to directory where trained models are stored (i.e. the output_directory specified in previous call to `fit`).\n        verbosity : int, default = 2\n            Verbosity levels range from 0 to 4 and control how much information will be printed by the loaded `Predictor`.\n            Higher levels correspond to more detailed print statements (you can set verbosity = 0 to suppress warnings).\n            If using logging, you can alternatively control amount of information printed via `logger.setLevel(L)`,\n            where L ranges from 0 to 50 (Note: higher values L correspond to fewer print statements, opposite of verbosity levels)\n\n        Returns\n        -------\n        :class:`autogluon.task.tabular_prediction.TabularPredictor` object that can be used to make predictions.\n        """"""\n        logger.setLevel(verbosity2loglevel(verbosity)) # Reset logging after load (since we may be in new Python session)\n        if output_directory is None:\n            raise ValueError(""output_directory cannot be None in load()"")\n\n        output_directory = setup_outputdir(output_directory) # replace ~ with absolute path if it exists\n        learner = Learner.load(output_directory)\n        return TabularPredictor(learner=learner)\n\n    @staticmethod\n    @unpack(set_presets)\n    def fit(train_data,\n            label,\n            tuning_data=None,\n            time_limits=None,\n            output_directory=None,\n            presets=None,\n            problem_type=None,\n            eval_metric=None,\n            stopping_metric=None,\n            auto_stack=False,\n            hyperparameter_tune=False,\n            feature_prune=False,\n            holdout_frac=None,\n            num_bagging_folds=0,\n            num_bagging_sets=None,\n            stack_ensemble_levels=0,\n            hyperparameters=None,\n            num_trials=None,\n            scheduler_options=None,\n            search_strategy=\'random\',\n            search_options=None,\n            nthreads_per_trial=None,\n            ngpus_per_trial=None,\n            dist_ip_addrs=None,\n            visualizer=\'none\',\n            verbosity=2,\n            **kwargs):\n        """"""\n        Fit models to predict a column of data table based on the other columns.\n\n        Parameters\n        ----------\n        train_data : str or :class:`autogluon.task.tabular_prediction.TabularDataset` or `pandas.DataFrame`\n            Table of the training data, which is similar to pandas DataFrame.\n            If str is passed, `train_data` will be loaded using the str value as the file path.\n        label : str\n            Name of the column that contains the target variable to predict.\n        tuning_data : str or :class:`autogluon.task.tabular_prediction.TabularDataset` or `pandas.DataFrame`, default = None\n            Another dataset containing validation data reserved for hyperparameter tuning (in same format as training data).\n            If str is passed, `tuning_data` will be loaded using the str value as the file path.\n            Note: final model returned may be fit on this tuning_data as well as train_data. Do not provide your evaluation test data here!\n            In particular, when `num_bagging_folds` > 0 or `stack_ensemble_levels` > 0, models will be trained on both `tuning_data` and `train_data`.\n            If `tuning_data = None`, `fit()` will automatically hold out some random validation examples from `train_data`.\n        time_limits : int\n            Approximately how long `fit()` should run for (wallclock time in seconds).\n            If not specified, `fit()` will run until all models have completed training, but will not repeatedly bag models unless `num_bagging_sets` or `auto_stack` is specified.\n        output_directory : str, default = None\n            Path to directory where models and intermediate outputs should be saved.\n            If unspecified, a time-stamped folder called ""AutogluonModels/ag-[TIMESTAMP]"" will be created in the working directory to store all models.\n            Note: To call `fit()` twice and save all results of each fit, you must specify different `output_directory` locations.\n            Otherwise files from first `fit()` will be overwritten by second `fit()`.\n        presets : list or str or dict, default = \'medium_quality_faster_train\'\n            List of preset configurations for various arguments in `fit()`. Can significantly impact predictive accuracy, memory-footprint, and inference latency of trained models, and various other properties of the returned `predictor`.\n            It is recommended to specify presets and avoid specifying most other `fit()` arguments or model hyperparameters prior to becoming familiar with AutoGluon.\n            As an example, to get the most accurate overall predictor (regardless of its efficiency), set `presets=\'best_quality\'`.\n            To get good quality with minimal disk usage, set `presets=[\'good_quality_faster_inference_only_refit\', \'optimize_for_deployment\']`\n            Any user-specified arguments in `fit()` will override the values used by presets.\n            If specifying a list of presets, later presets will override earlier presets if they alter the same argument.\n            For precise definitions of the provided presets, see file: `autogluon/tasks/tabular_prediction/presets_configs.py`.\n            Users can specify custom presets by passing in a dictionary of argument values as an element to the list.\n\n            Available Presets: [\'best_quality\', \'best_quality_with_high_quality_refit\', \'high_quality_fast_inference_only_refit\', \'good_quality_faster_inference_only_refit\', \'medium_quality_faster_train\', \'optimize_for_deployment\', \'ignore_text\']\n            It is recommended to only use one `quality` based preset in a given call to `fit()` as they alter many of the same arguments and are not compatible with each-other.\n\n            In-depth Preset Info:\n                best_quality={\'auto_stack\': True}\n                    Best predictive accuracy with little consideration to inference time or disk usage. Achieve even better results by specifying a large time_limits value.\n                    Recommended for applications that benefit from the best possible model accuracy.\n\n                best_quality_with_high_quality_refit={\'auto_stack\': True, \'refit_full\': True}\n                    Identical to best_quality but additionally trains refit_full models that have slightly lower predictive accuracy but are over 10x faster during inference and require 10x less disk space.\n\n                high_quality_fast_inference_only_refit={\'auto_stack\': True, \'refit_full\': True, \'set_best_to_refit_full\': True, \'save_bagged_folds\': False}\n                    High predictive accuracy with fast inference. ~10x-200x faster inference and ~10x-200x lower disk usage than `best_quality`.\n                    Recommended for applications that require reasonable inference speed and/or model size.\n\n                good_quality_faster_inference_only_refit={\'auto_stack\': True, \'refit_full\': True, \'set_best_to_refit_full\': True, \'save_bagged_folds\': False, \'hyperparameters\': \'light\'}\n                    Good predictive accuracy with very fast inference. ~4x faster inference and ~4x lower disk usage than `high_quality_fast_inference_only_refit`.\n                    Recommended for applications that require fast inference speed.\n\n                medium_quality_faster_train={\'auto_stack\': False}\n                    Medium predictive accuracy with very fast inference and very fast training time. ~20x faster training than `good_quality_faster_inference_only_refit`.\n                    This is the default preset in AutoGluon, but should generally only be used for quick prototyping, as `good_quality_faster_inference_only_refit` results in significantly better predictive accuracy and faster inference time.\n\n                optimize_for_deployment={\'keep_only_best\': True, \'save_space\': True}\n                    Optimizes result immediately for deployment by deleting unused models and removing training artifacts.\n                    Often can reduce disk usage by ~2-4x with no negatives to model accuracy or inference speed.\n                    This will disable numerous advanced functionality, but has no impact on inference.\n                    This will make certain functionality less informative, such as `predictor.leaderboard()` and `predictor.fit_summary()`.\n                        Because unused models will be deleted under this preset, methods like `predictor.leaderboard()` and `predictor.fit_summary()` will no longer show the full set of models that were trained during `fit()`.\n                    Recommended for applications where the inner details of AutoGluon\'s training is not important and there is no intention of manually choosing between the final models.\n                    This preset pairs well with the other presets such as `good_quality_faster_inference_only_refit` to make a very compact final model.\n                    Identical to calling `predictor.delete_models(models_to_keep=\'best\', dry_run=False)` and `predictor.save_space()` directly after `fit()`.\n\n                ignore_text={\'feature_generator_kwargs\': {\'enable_nlp_vectorizer_features\': False, \'enable_nlp_ratio_features\': False}}\n                    Disables automated feature generation when text features are detected.\n                    This is useful to determine how beneficial text features are to the end result, as well as to ensure features are not mistaken for text when they are not.\n\n        problem_type : str, default = None\n            Type of prediction problem, i.e. is this a binary/multiclass classification or regression problem (options: \'binary\', \'multiclass\', \'regression\').\n            If `problem_type = None`, the prediction problem type is inferred based on the label-values in provided dataset.\n        eval_metric : function or str, default = None\n            Metric by which predictions will be ultimately evaluated on test data.\n            AutoGluon tunes factors such as hyperparameters, early-stopping, ensemble-weights, etc. in order to improve this metric on validation data.\n\n            If `eval_metric = None`, it is automatically chosen based on `problem_type`.\n            Defaults to \'accuracy\' for binary and multiclass classification and \'root_mean_squared_error\' for regression.\n            Otherwise, options for classification:\n                [\'accuracy\', \'balanced_accuracy\', \'f1\', \'f1_macro\', \'f1_micro\', \'f1_weighted\',\n                \'roc_auc\', \'average_precision\', \'precision\', \'precision_macro\', \'precision_micro\', \'precision_weighted\',\n                \'recall\', \'recall_macro\', \'recall_micro\', \'recall_weighted\', \'log_loss\', \'pac_score\']\n            Options for regression:\n                [\'root_mean_squared_error\', \'mean_squared_error\', \'mean_absolute_error\', \'median_absolute_error\', \'r2\']\n            For more information on these options, see `sklearn.metrics`: https://scikit-learn.org/stable/modules/classes.html#sklearn-metrics-metrics\n\n            You can also pass your own evaluation function here as long as it follows formatting of the functions defined in `autogluon/utils/tabular/metrics/`.\n        stopping_metric : function or str, default = None\n            Metric which iteratively-trained models use to early stop to avoid overfitting.\n            `stopping_metric` is not used by weighted ensembles, instead weighted ensembles maximize `eval_metric`.\n            Defaults to `eval_metric` value except when `eval_metric=\'roc_auc\'`, where it defaults to `log_loss`.\n            Options are identical to options for `eval_metric`.\n        auto_stack : bool, default = False\n            Whether AutoGluon should automatically utilize bagging and multi-layer stack ensembling to boost predictive accuracy.\n            Set this = True if you are willing to tolerate longer training times in order to maximize predictive accuracy!\n            Note: This overrides `num_bagging_folds` and `stack_ensemble_levels` arguments (selects optimal values for these parameters based on dataset properties).\n            Note: This can increase training time (and inference time) by up to 20x, but can greatly improve predictive performance.\n        hyperparameter_tune : bool, default = False\n            Whether to tune hyperparameters or just use fixed hyperparameter values for each model. Setting as True will increase `fit()` runtimes.\n            It is currently not recommended to use `hyperparameter_tune` with `auto_stack` due to potential overfitting.\n            Use `auto_stack` to maximize predictive accuracy; use `hyperparameter_tune` if you prefer to deploy just a single model rather than an ensemble.\n        feature_prune : bool, default = False\n            Whether or not to perform feature selection.\n        hyperparameters : str or dict, default = \'default\'\n            Determines the hyperparameters used by the models.\n            If `str` is passed, will use a preset hyperparameter configuration.\n                Valid `str` options: [\'default\', \'light\', \'very_light\', \'toy\']\n                    \'default\': Default AutoGluon hyperparameters intended to maximize accuracy without significant regard to inference time or disk usage.\n                    \'light\': Results in smaller models. Generally will make inference speed much faster and disk usage much lower, but with worse accuracy.\n                    \'very_light\': Results in much smaller models. Behaves similarly to \'light\', but in many cases with over 10x less disk usage and a further reduction in accuracy.\n                    \'toy\': Results in extremely small models. Only use this when prototyping, as the model quality will be severely reduced.\n                Reference `autogluon/task/tabular_prediction/hyperparameter_configs.py` for information on the hyperparameters associated with each preset.\n            Keys are strings that indicate which model types to train.\n                Options include: \'NN\' (neural network), \'GBM\' (lightGBM boosted trees), \'CAT\' (CatBoost boosted trees), \'RF\' (random forest), \'XT\' (extremely randomized trees), \'KNN\' (k-nearest neighbors)\n                If certain key is missing from hyperparameters, then `fit()` will not train any models of that type.\n                For example, set `hyperparameters = { \'NN\':{...} }` if say you only want to train neural networks and no other types of models.\n            Values = dict of hyperparameter settings for each model type, or list of dicts.\n                Each hyperparameter can either be a single fixed value or a search space containing many possible values.\n                Unspecified hyperparameters will be set to default values (or default search spaces if `hyperparameter_tune = True`).\n                Caution: Any provided search spaces will be overridden by fixed defaults if `hyperparameter_tune = False`.\n                To train multiple models of a given type, set the value to a list of hyperparameter dictionaries.\n                    For example, `hyperparameters = {\'RF\': [{\'criterion\': \'gini\'}, {\'criterion\': \'entropy\'}]}` will result in 2 random forest models being trained with separate hyperparameters.\n            Advanced functionality: Custom models\n                `hyperparameters` can also take a special key \'custom\', which maps to a list of model names (currently supported options = \'GBM\').\n                    If `hyperparameter_tune = False`, then these additional models will also be trained using custom pre-specified hyperparameter settings that are known to work well.\n            Advanced functionality: Custom stack levels\n                By default, AutoGluon re-uses the same models and model hyperparameters at each level during stack ensembling.\n                To customize this behaviour, create a hyperparameters dictionary separately for each stack level, and then add them as values to a new dictionary, with keys equal to the stack level.\n                    Example: `hyperparameters = {0: {\'RF\': rf_params1}, 1: {\'CAT\': [cat_params1, cat_params2], \'NN\': {}}}\n                    This will result in a stack ensemble that has one custom random forest in level 0 followed by two CatBoost models with custom hyperparameters and a default neural network in level 1, for a total of 4 models.\n                If a level is not specified in `hyperparameters`, it will default to using the highest specified level to train models. This can also be explicitly controlled by adding a \'default\' key.\n\n            Default:\n                hyperparameters = {\n                    \'NN\': {},\n                    \'GBM\': {},\n                    \'CAT\': {},\n                    \'RF\': [\n                        {\'criterion\': \'gini\', \'_ag_args\': {\'name_suffix\': \'Gini\', \'problem_types\': [\'binary\', \'multiclass\']}},\n                        {\'criterion\': \'entropy\', \'_ag_args\': {\'name_suffix\': \'Entr\', \'problem_types\': [\'binary\', \'multiclass\']}},\n                        {\'criterion\': \'mse\', \'_ag_args\': {\'name_suffix\': \'MSE\', \'problem_types\': [\'regression\']}},\n                    ],\n                    \'XT\': [\n                        {\'criterion\': \'gini\', \'_ag_args\': {\'name_suffix\': \'Gini\', \'problem_types\': [\'binary\', \'multiclass\']}},\n                        {\'criterion\': \'entropy\', \'_ag_args\': {\'name_suffix\': \'Entr\', \'problem_types\': [\'binary\', \'multiclass\']}},\n                        {\'criterion\': \'mse\', \'_ag_args\': {\'name_suffix\': \'MSE\', \'problem_types\': [\'regression\']}},\n                    ],\n                    \'KNN\': [\n                        {\'weights\': \'uniform\', \'_ag_args\': {\'name_suffix\': \'Unif\'}},\n                        {\'weights\': \'distance\', \'_ag_args\': {\'name_suffix\': \'Dist\'}},\n                    ],\n                    \'custom\': [\'GBM\']\n                }\n\n            Details regarding the hyperparameters you can specify for each model are provided in the following files:\n                NN: `autogluon/utils/tabular/ml/models/tabular_nn/hyperparameters/parameters.py`\n                    Note: certain hyperparameter settings may cause these neural networks to train much slower.\n                GBM: `autogluon/utils/tabular/ml/models/lgb/hyperparameters/parameters.py`\n                     See also the lightGBM docs: https://lightgbm.readthedocs.io/en/latest/Parameters.html\n                CAT: `autogluon/utils/tabular/ml/models/catboost/hyperparameters/parameters.py`\n                     See also the CatBoost docs: https://catboost.ai/docs/concepts/parameter-tuning.html\n                RF: See sklearn documentation: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n                    Note: Hyperparameter tuning is disabled for this model.\n                    Note: \'criterion\' parameter will be overridden. Both \'gini\' and \'entropy\' are used automatically, training two models.\n                XT: See sklearn documentation: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html\n                    Note: Hyperparameter tuning is disabled for this model.\n                    Note: \'criterion\' parameter will be overridden. Both \'gini\' and \'entropy\' are used automatically, training two models.\n                KNN: See sklearn documentation: https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n                    Note: Hyperparameter tuning is disabled for this model.\n                    Note: \'weights\' parameter will be overridden. Both \'distance\' and \'uniform\' are used automatically, training two models.\n                LR: `autogluon/utils/tabular/ml/models/lr/hyperparameters/parameters.py`\n                    Note: a list of hyper-parameters dicts can be passed; each set will create different version of the model.\n                    Note: Hyperparameter tuning is disabled for this model.\n                    Note: \'penalty\' parameter can be used for regression to specify regularization method: \'L1\' and \'L2\' values are supported.\n                Advanced functionality: Custom AutoGluon model arguments\n                    These arguments are optional and can be specified in any model\'s hyperparameters.\n                        Example: `hyperparameters = {\'RF\': {..., \'_ag_args\': {\'name_suffix\': \'CustomModelSuffix\', \'disable_in_hpo\': True}}`\n                    _ag_args: Dictionary of customization options related to AutoGluon.\n                        Valid keys:\n                            name: (str) The name of the model. This overrides AutoGluon\'s naming logic and all other name arguments if present.\n                            name_main: (str) The main name of the model. In \'RandomForestClassifier\', this is \'RandomForest\'.\n                            name_prefix: (str) Add a custom prefix to the model name. Unused by default.\n                            name_type_suffix: (str) Override the type suffix of the model name. In \'RandomForestClassifier\', this is \'Classifier\'. This comes before \'name_suffix\'.\n                            name_suffix: (str) Add a custom suffix to the model name. Unused by default.\n                            priority: (int) Determines the order in which the model is trained. Larger values result in the model being trained earlier. Default values range from 100 (RF) to 0 (custom), dictated by model type. If you want this model to be trained first, set priority = 999.\n                            problem_types: (list) List of valid problem types for the model. `problem_types=[\'binary\']` will result in the model only being trained if `problem_type` is \'binary\'.\n                            disable_in_hpo: (bool) If True, the model will only be trained if `hyperparameter_tune=False`.\n                        Reference the default hyperparameters for example usage of these options.\n\n        holdout_frac : float\n            Fraction of train_data to holdout as tuning data for optimizing hyperparameters (ignored unless `tuning_data = None`, ignored if `num_bagging_folds != 0`).\n            Default value is selected based on the number of rows in the training data. Default values range from 0.2 at 2,500 rows to 0.01 at 250,000 rows.\n            Default value is doubled if `hyperparameter_tune = True`, up to a maximum of 0.2.\n            Disabled if `num_bagging_folds >= 2`.\n        num_bagging_folds : int, default = 0\n            Number of folds used for bagging of models. When `num_bagging_folds = k`, training time is roughly increased by a factor of `k` (set = 0 to disable bagging).\n            Disabled by default, but we recommend values between 5-10 to maximize predictive performance.\n            Increasing num_bagging_folds will result in models with lower bias but that are more prone to overfitting.\n            Values > 10 may produce diminishing returns, and can even harm overall results due to overfitting.\n            To further improve predictions, avoid increasing num_bagging_folds much beyond 10 and instead increase num_bagging_sets.\n        num_bagging_sets : int\n            Number of repeats of kfold bagging to perform (values must be >= 1). Total number of models trained during bagging = num_bagging_folds * num_bagging_sets.\n            Defaults to 1 if time_limits is not specified, otherwise 20 (always disabled if num_bagging_folds is not specified).\n            Values greater than 1 will result in superior predictive performance, especially on smaller problems and with stacking enabled (reduces overall variance).\n        stack_ensemble_levels : int, default = 0\n            Number of stacking levels to use in stack ensemble. Roughly increases model training time by factor of `stack_ensemble_levels+1` (set = 0 to disable stack ensembling).\n            Disabled by default, but we recommend values between 1-3 to maximize predictive performance.\n            To prevent overfitting, this argument is ignored unless you have\xc2\xa0also set `num_bagging_folds >= 2`.\n        num_trials : int\n            Maximal number of different hyperparameter settings of each model type to evaluate during HPO (only matters if `hyperparameter_tune = True`).\n            If both `time_limits` and `num_trials` are specified, `time_limits` takes precedent.\n        scheduler_options : dict\n            Extra arguments passed to __init__ of scheduler, to configure the\n            orchestration of training jobs during hyperparameter-tuning. This\n            is ignored if hyperparameter_tune=False.\n        search_strategy : str\n            Which hyperparameter search algorithm to use (only matters if\n            `hyperparameter_tune = True`).\n            Options include: \'random\' (random search), \'bayesopt\' (Gaussian process\n            Bayesian optimization), \'skopt\' (SKopt Bayesian optimization), \'grid\'\n            (grid search), \'hyperband\' (Hyperband random), rl\' (reinforcement\n            learner).\n        search_options : dict\n            Auxiliary keyword arguments to pass to the searcher that performs\n            hyperparameter optimization.\n        nthreads_per_trial : int\n            How many CPUs to use in each training run of an individual model.\n            This is automatically determined by AutoGluon when left as None (based on available compute).\n        ngpus_per_trial : int\n            How many GPUs to use in each trial (ie. single training run of a model).\n            This is automatically determined by AutoGluon when left as None.\n        dist_ip_addrs : list\n            List of IP addresses corresponding to remote workers, in order to leverage distributed computation.\n        visualizer : str\n            How to visualize the neural network training progress during `fit()`. Options: [\'mxboard\', \'tensorboard\', \'none\'].\n        verbosity : int, default = 2\n            Verbosity levels range from 0 to 4 and control how much information is printed during fit().\n            Higher levels correspond to more detailed print statements (you can set verbosity = 0 to suppress warnings).\n            If using logging, you can alternatively control amount of information printed via `logger.setLevel(L)`,\n            where `L` ranges from 0 to 50 (Note: higher values of `L` correspond to fewer print statements, opposite of verbosity levels)\n\n        Kwargs can include additional arguments for advanced users:\n            feature_generator_type : `FeatureGenerator` class, default=`AutoMLFeatureGenerator`\n                A `FeatureGenerator` class specifying which feature engineering protocol to follow\n                (see autogluon.utils.tabular.features.abstract_feature_generator.AbstractFeatureGenerator).\n                Note: The file containing your `FeatureGenerator` class must be imported into current Python session in order to use a custom class.\n            feature_generator_kwargs : dict, default={}\n                Keyword arguments to pass into the `FeatureGenerator` constructor.\n            trainer_type : `Trainer` class, default=`AutoTrainer`\n                A class inheritng from `autogluon.utils.tabular.ml.trainer.abstract_trainer.AbstractTrainer` that controls training/ensembling of many models.\n                Note: In order to use a custom `Trainer` class, you must import the class file that defines it into the current Python session.\n            label_count_threshold : int, default = 10\n                For multi-class classification problems, this is the minimum number of times a label must appear in dataset in order to be considered an output class.\n                AutoGluon will ignore any classes whose labels do not appear at least this many times in the dataset (i.e. will never predict them).\n            id_columns : list, default = []\n                Banned subset of column names that model may not use as predictive features (e.g. contains label, user-ID, etc).\n                These columns are ignored during `fit()`, but DataFrame of just these columns with appended predictions may be produced, for example to submit in a ML competition.\n            set_best_to_refit_full : bool, default = False\n                If True, will set Trainer.best_model = Trainer.full_model_dict[Trainer.best_model]\n                This will change the default model that Predictor uses for prediction when model is not specified to the refit_full version of the model that previously exhibited the highest validation score.\n                Only valid if `refit_full` is set.\n            save_bagged_folds : bool, default = True\n                If True, bagged models will save their fold models (the models from each individual fold of bagging). This is required to use bagged models for prediction after `fit()`.\n                If False, bagged models will not save their fold models. This means that bagged models will not be valid models during inference.\n                    This should only be set to False when planning to call `predictor.refit_full()` or when `refit_full` is set and `set_best_to_refit_full=True`.\n                    Particularly useful if disk usage is a concern. By not saving the fold models, bagged models will use only very small amounts of disk space during training.\n                    In many training runs, this will reduce peak disk usage by >10x.\n                This parameter has no effect if bagging is disabled.\n            keep_only_best : bool, default = False\n                If True, only the best model and its ancestor models are saved in the outputted `predictor`. All other models are deleted.\n                    If you only care about deploying the most accurate predictor with the smallest file-size and no longer need any of the other trained models or functionality beyond prediction on new data, then set: `keep_only_best=True`, `save_space=True`.\n                    This is equivalent to calling `predictor.delete_models(models_to_keep=\'best\', dry_run=False)` directly after `fit()`.\n                If used with `refit_full` and `set_best_to_refit_full`, the best model will be the refit_full model, and the original bagged best model will be deleted.\n                    `refit_full` will be automatically set to \'best\' in this case to avoid training models which will be later deleted.\n            save_space : bool, default = False\n                If True, reduces the memory and disk size of predictor by deleting auxiliary model files that aren\'t needed for prediction on new data.\n                    This is equivalent to calling `predictor.save_space()` directly after `fit()`.\n                This has NO impact on inference accuracy.\n                It is recommended if the only goal is to use the trained model for prediction.\n                Certain advanced functionality may no longer be available if `save_space=True`. Refer to `predictor.save_space()` documentation for more details.\n            cache_data : bool, default = True\n                When enabled, the training and validation data are saved to disk for future reuse.\n                Enables advanced functionality in the resulting Predictor object such as feature importance calculation on the original data.\n            refit_full : bool or str, default = False\n                Whether to retrain all models on all of the data (training + validation) after the normal training procedure.\n                This is equivalent to calling `predictor.refit_full(model=refit_full)` after training.\n                If `refit_full=True`, it will be treated as `refit_full=\'all\'`.\n                If `refit_full=False`, refitting will not occur.\n                Valid str values:\n                    `all`: refits all models.\n                    `best`: refits only the best model (and its ancestors if it is a stacker model).\n                    `{model_name}`: refits only the specified model (and its ancestors if it is a stacker model).\n                For bagged models:\n                    Reduces a model\'s inference time by collapsing bagged ensembles into a single model fit on all of the training data.\n                    This process will typically result in a slight accuracy reduction and a large inference speedup.\n                    The inference speedup will generally be between 10-200x faster than the original bagged ensemble model.\n                        The inference speedup factor is equivalent to (k * n), where k is the number of folds (`num_bagging_folds`) and n is the number of finished repeats (`num_bagging_sets`) in the bagged ensemble.\n                    The runtime is generally 10% or less of the original fit runtime.\n                        The runtime can be roughly estimated as 1 / (k * n) of the original fit runtime, with k and n defined above.\n                For non-bagged models:\n                    Optimizes a model\'s accuracy by retraining on 100% of the data without using a validation set.\n                    Will typically result in a slight accuracy increase and no change to inference time.\n                    The runtime will be approximately equal to the original fit runtime.\n                This process does not alter the original models, but instead adds additional models.\n                If stacker models are refit by this process, they will use the refit_full versions of the ancestor models during inference.\n                Models produced by this process will not have validation scores, as they use all of the data for training.\n                    Therefore, it is up to the user to determine if the models are of sufficient quality by including test data in `predictor.leaderboard(dataset=test_data)`.\n                    If the user does not have additional test data, they should reference the original model\'s score for an estimate of the performance of the refit_full model.\n                        Warning: Be aware that utilizing refit_full models without separately verifying on test data means that the model is untested, and has no guarantee of being consistent with the original model.\n                The time taken by this process is not enforced by `time_limits`.\n                `cache_data` must be set to `True` to enable this functionality.\n            random_seed : int, default = 0\n                Seed to use when generating data split indices such as kfold splits and train/validation splits.\n                Caution: This seed only enables reproducible data splits (and the ability to randomize splits in each run by changing seed values).\n                This seed is NOT used in the training of individual models, for that you need to explicitly set the corresponding seed hyperparameter (usually called \'seed_value\') of each individual model.\n                If stacking is enabled:\n                    The seed used for stack level L is equal to `seed+L`.\n                    This means `random_seed=1` will have the same split indices at L=0 as `random_seed=0` will have at L=1.\n                If `random_seed=None`, a random integer is used.\n\n        Returns\n        -------\n        :class:`autogluon.task.tabular_prediction.TabularPredictor` object which can make predictions on new data and summarize what happened during `fit()`.\n\n        Examples\n        --------\n        >>> from autogluon import TabularPrediction as task\n        >>> train_data = task.Dataset(file_path=\'https://autogluon.s3.amazonaws.com/datasets/Inc/train.csv\')\n        >>> label_column = \'class\'\n        >>> predictor = task.fit(train_data=train_data, label=label_column)\n        >>> test_data = task.Dataset(file_path=\'https://autogluon.s3.amazonaws.com/datasets/Inc/test.csv\')\n        >>> y_test = test_data[label_column]\n        >>> test_data = test_data.drop(labels=[label_column], axis=1)\n        >>> y_pred = predictor.predict(test_data)\n        >>> perf = predictor.evaluate_predictions(y_true=y_test, y_pred=y_pred)\n        >>> results = predictor.fit_summary()\n\n        To maximize predictive performance, use the following:\n\n        >>> eval_metric = \'roc_auc\'  # set this to the metric you ultimately care about\n        >>> time_limits = 360  # set as long as you are willing to wait (in sec)\n        >>> predictor = task.fit(train_data=train_data, label=label_column, eval_metric=eval_metric, auto_stack=True, time_limits=time_limits)\n        """"""\n        assert search_strategy != \'bayesopt_hyperband\', \\\n            ""search_strategy == \'bayesopt_hyperband\' not yet supported""\n        if verbosity < 0:\n            verbosity = 0\n        elif verbosity > 4:\n            verbosity = 4\n\n        logger.setLevel(verbosity2loglevel(verbosity))\n        allowed_kwarg_names = {\n            \'feature_generator_type\',\n            \'feature_generator_kwargs\',\n            \'trainer_type\',\n            \'label_count_threshold\',\n            \'id_columns\',\n            \'set_best_to_refit_full\',\n            \'save_bagged_folds\',\n            \'keep_only_best\',\n            \'save_space\',\n            \'cache_data\',\n            \'refit_full\',\n            \'random_seed\',\n            \'enable_fit_continuation\'  # TODO: Remove on 0.1.0 release\n        }\n        for kwarg_name in kwargs.keys():\n            if kwarg_name not in allowed_kwarg_names:\n                raise ValueError(""Unknown keyword argument specified: %s"" % kwarg_name)\n\n        if isinstance(train_data,  str):\n            train_data = TabularDataset(file_path=train_data)\n        if tuning_data is not None and isinstance(tuning_data, str):\n            tuning_data = TabularDataset(file_path=tuning_data)\n\n        if len(set(train_data.columns)) < len(train_data.columns):\n            raise ValueError(""Column names are not unique, please change duplicated column names (in pandas: train_data.rename(columns={\'current_name\':\'new_name\'})"")\n        if tuning_data is not None and np.any(train_data.columns != tuning_data.columns):\n            raise ValueError(""Column names must match between training and tuning data"")\n\n        if feature_prune:\n            feature_prune = False  # TODO: Fix feature pruning to add back as an option\n            # Currently disabled, needs to be updated to align with new model class functionality\n            logger.log(30, \'Warning: feature_prune does not currently work, setting to False.\')\n\n        cache_data = kwargs.get(\'cache_data\', True)\n        refit_full = kwargs.get(\'refit_full\', False)\n        # TODO: Remove on 0.1.0 release\n        if \'enable_fit_continuation\' in kwargs.keys():\n            logger.log(30, \'Warning: `enable_fit_continuation` is a deprecated parameter. It has been renamed to `cache_data`. Starting from AutoGluon 0.1.0, specifying `enable_fit_continuation` as a parameter will cause an exception.\')\n            logger.log(30, \'Setting `cache_data` value equal to `enable_fit_continuation` value.\')\n            cache_data = kwargs[\'enable_fit_continuation\']\n        if not cache_data:\n            logger.log(30, \'Warning: `cache_data=False` will disable or limit advanced functionality after training such as feature importance calculations. It is recommended to set `cache_data=True` unless you explicitly wish to not have the data saved to disk.\')\n            if refit_full:\n                raise ValueError(\'`refit_full=True` is only available when `cache_data=True`. Set `cache_data=True` to utilize `refit_full`.\')\n\n        set_best_to_refit_full = kwargs.get(\'set_best_to_refit_full\', False)\n        if set_best_to_refit_full and not refit_full:\n            raise ValueError(\'`set_best_to_refit_full=True` is only available when `refit_full=True`. Set `refit_full=True` to utilize `set_best_to_refit_full`.\')\n\n        save_bagged_folds = kwargs.get(\'save_bagged_folds\', True)\n\n        if hyperparameter_tune:\n            logger.log(30, \'Warning: `hyperparameter_tune=True` is currently experimental and may cause the process to hang. Setting `auto_stack=True` instead is recommended to achieve maximum quality models.\')\n\n        if dist_ip_addrs is None:\n            dist_ip_addrs = []\n\n        if search_options is None:\n            search_options = dict()\n\n        if hyperparameters is None:\n            hyperparameters = \'default\'\n        if isinstance(hyperparameters, str):\n            hyperparameters = get_hyperparameter_config(hyperparameters)\n\n        # Process kwargs to create feature generator, trainer, schedulers, searchers for each model:\n        output_directory = setup_outputdir(output_directory)  # Format directory name\n        feature_generator_type = kwargs.get(\'feature_generator_type\', AutoMLFeatureGenerator)\n        feature_generator_kwargs = kwargs.get(\'feature_generator_kwargs\', {})\n        feature_generator = feature_generator_type(**feature_generator_kwargs) # instantiate FeatureGenerator object\n        id_columns = kwargs.get(\'id_columns\', [])\n        trainer_type = kwargs.get(\'trainer_type\', AutoTrainer)\n        random_seed = kwargs.get(\'random_seed\', 0)\n        nthreads_per_trial, ngpus_per_trial = setup_compute(nthreads_per_trial, ngpus_per_trial)\n        num_train_rows = len(train_data)\n        if auto_stack:\n            # TODO: What about datasets that are 100k+? At a certain point should we not bag?\n            # TODO: What about time_limits? Metalearning can tell us expected runtime of each model, then we can select optimal folds + stack levels to fit time constraint\n            num_bagging_folds = min(10, max(5, math.floor(num_train_rows / 100)))\n            stack_ensemble_levels = min(1, max(0, math.floor(num_train_rows / 750)))\n\n        if num_bagging_sets is None:\n            if num_bagging_folds >= 2:\n                if time_limits is not None:\n                    num_bagging_sets = 20\n                else:\n                    num_bagging_sets = 1\n            else:\n                num_bagging_sets = 1\n\n        label_count_threshold = kwargs.get(\'label_count_threshold\', 10)\n        if num_bagging_folds is not None:  # Ensure there exist sufficient labels for stratified splits across all bags\n            label_count_threshold = max(label_count_threshold, num_bagging_folds)\n\n        time_limits_orig = copy.deepcopy(time_limits)\n        time_limits_hpo = copy.deepcopy(time_limits)\n\n        if num_bagging_folds >= 2 and (time_limits_hpo is not None):\n            time_limits_hpo = time_limits_hpo / (1 + num_bagging_folds * (1 + stack_ensemble_levels))\n        time_limits_hpo, num_trials = setup_trial_limits(time_limits_hpo, num_trials, hyperparameters)  # TODO: Move HPO time allocation to Trainer\n\n        if (num_trials is not None) and hyperparameter_tune and (num_trials == 1):\n            hyperparameter_tune = False\n            logger.log(30, \'Warning: Specified num_trials == 1 or time_limits is too small for hyperparameter_tune, setting to False.\')\n\n        if holdout_frac is None:\n            # Between row count 5,000 and 25,000 keep 0.1 holdout_frac, as we want to grow validation set to a stable 2500 examples\n            if num_train_rows < 5000:\n                holdout_frac = max(0.1, min(0.2, 500.0 / num_train_rows))\n            else:\n                holdout_frac = max(0.01, min(0.1, 2500.0 / num_train_rows))\n\n            if hyperparameter_tune:\n                holdout_frac = min(0.2, holdout_frac * 2)  # We want to allocate more validation data for HPO to avoid overfitting\n\n        # Add visualizer to NN hyperparameters:\n        if (visualizer is not None) and (visualizer != \'none\') and (\'NN\' in hyperparameters):\n            hyperparameters[\'NN\'][\'visualizer\'] = visualizer\n\n        eval_metric = get_metric(eval_metric, problem_type, \'eval_metric\')\n        stopping_metric = get_metric(stopping_metric, problem_type, \'stopping_metric\')\n\n        # All models use the same scheduler:\n        scheduler_options = compile_scheduler_options(\n            scheduler_options=scheduler_options,\n            search_strategy=search_strategy,\n            search_options=search_options,\n            nthreads_per_trial=nthreads_per_trial,\n            ngpus_per_trial=ngpus_per_trial,\n            checkpoint=None,\n            num_trials=num_trials,\n            time_out=time_limits_hpo,\n            resume=False,\n            visualizer=visualizer,\n            time_attr=\'epoch\',\n            reward_attr=\'validation_performance\',\n            dist_ip_addrs=dist_ip_addrs)\n        scheduler_cls = schedulers[search_strategy.lower()]\n        scheduler_options = (scheduler_cls, scheduler_options)  # wrap into tuple\n        learner = Learner(path_context=output_directory, label=label, problem_type=problem_type, objective_func=eval_metric, stopping_metric=stopping_metric,\n                          id_columns=id_columns, feature_generator=feature_generator, trainer_type=trainer_type,\n                          label_count_threshold=label_count_threshold, random_seed=random_seed)\n        learner.fit(X=train_data, X_test=tuning_data, scheduler_options=scheduler_options,\n                    hyperparameter_tune=hyperparameter_tune, feature_prune=feature_prune,\n                    holdout_frac=holdout_frac, num_bagging_folds=num_bagging_folds, num_bagging_sets=num_bagging_sets, stack_ensemble_levels=stack_ensemble_levels,\n                    hyperparameters=hyperparameters, time_limit=time_limits_orig, save_data=cache_data, save_bagged_folds=save_bagged_folds, verbosity=verbosity)\n\n        predictor = TabularPredictor(learner=learner)\n\n        keep_only_best = kwargs.get(\'keep_only_best\', False)\n        if refit_full is True:\n            if keep_only_best is True:\n                if set_best_to_refit_full is True:\n                    refit_full = \'best\'\n                else:\n                    logger.warning(f\'refit_full was set to {refit_full}, but keep_only_best=True and set_best_to_refit_full=False. Disabling refit_full to avoid training models which would be automatically deleted.\')\n                    refit_full = False\n            else:\n                refit_full = \'all\'\n\n        if refit_full is not False:\n            trainer = predictor._trainer\n            trainer_model_best = trainer.get_model_best()\n            predictor.refit_full(model=refit_full)\n            if set_best_to_refit_full:\n                if trainer_model_best in trainer.model_full_dict.keys():\n                    trainer.model_best = trainer.model_full_dict[trainer_model_best]\n                    # Note: model_best will be overwritten if additional training is done with new models, since model_best will have validation score of None and any new model will have a better validation score.\n                    # This has the side-effect of having the possibility of model_best being overwritten by a worse model than the original model_best.\n                    trainer.save()\n                else:\n                    logger.warning(f\'Best model ({trainer_model_best}) is not present in refit_full dictionary. Training may have failed on the refit model. AutoGluon will default to using {trainer_model_best} for predictions.\')\n\n        if keep_only_best:\n            predictor.delete_models(models_to_keep=\'best\', dry_run=False)\n\n        save_space = kwargs.get(\'save_space\', False)\n        if save_space:\n            predictor.save_space()\n\n        return predictor\n'"
autogluon/task/text_classification/__init__.py,0,b'from .predictor import *\nfrom .text_classification import *\nfrom .dataset import get_dataset'
autogluon/task/text_classification/dataset.py,0,"b'import multiprocessing as mp\nimport warnings\nfrom typing import AnyStr\nimport pandas as pd\n\nfrom mxnet import gluon\nfrom mxnet.metric import Accuracy, F1, MCC, PearsonCorrelation, CompositeEvalMetric\nimport gluonnlp as nlp\n\nfrom gluonnlp.data import GlueCoLA, GlueSST2, GlueSTSB, GlueMRPC\nfrom gluonnlp.data import GlueQQP, GlueRTE, GlueMNLI, GlueQNLI, GlueWNLI\nfrom ...core import *\nfrom ...utils.dataset import get_split_samplers, SampledDataset\n\n__all__ = [\'MRPCTask\', \'QQPTask\', \'QNLITask\', \'RTETask\', \'STSBTask\', \'CoLATask\', \'MNLITask\',\n           \'WNLITask\', \'SSTTask\', \'AbstractGlueTask\', \'get_dataset\', \'AbstractCustomTask\']\n\n@func()\ndef get_dataset(name=None, *args, **kwargs):\n    """"""Load a text classification dataset to train AutoGluon models on.\n        \n        Parameters\n        ----------\n        name : str\n            Name describing which built-in popular text dataset to use (mostly from the GLUE NLP benchmark).\n            Options include: \'mrpc\', \'qqp\', \'qnli\', \'rte\', \'sts-b\', \'cola\', \'mnli\', \'wnli\', \'sst\', \'toysst\'. \n            Detailed descriptions can be found in the file: `autogluon/task/text_classification/dataset.py`\n        **kwargs : some keyword arguments when using custom dataset as below.\n        kwargs[\'label\']: str or int Default: last column\n            Index or column name of the label/target column.\n    """"""\n    path = kwargs.get(\'filepath\', None)\n    if path is not None:\n        if path.endswith(\'.tsv\') or path.endswith(\'.csv\'):\n            return CustomTSVClassificationTask(*args, **kwargs)\n        else:\n            raise ValueError(\'tsv or csv format is supported now, please use the according files ending with\'\n                             \'`.tsv` or `.csv`.\')\n    if name is not None and name.lower() in built_in_tasks:\n        return built_in_tasks[name.lower()](*args, **kwargs)\n    else:\n        raise ValueError(\'mrpc, qqp, qnli, rte, sts-b, cola, mnli, wnli, sst, toysst are supported now.\')\n\nclass AbstractGlueTask:\n    """"""Abstract task class\xc2\xa0for datasets with GLUE format.\n\n    Parameters\n    ----------\n    class_labels : list of str, or None\n        Classification labels of the task.\n        Set to None for regression tasks with continuous real values.\n    metrics : list of EValMetric\n        Evaluation metrics of the task.\n    is_pair : bool\n        Whether the task deals with sentence pairs or single sentences.\n    label_alias : dict\n        label alias dict, some different labels in dataset actually means\n        the same. e.g.: {\'contradictory\':\'contradiction\'} means contradictory\n        and contradiction label means the same in dataset, they will get\n        the same class id.\n    """"""\n    def __init__(self, class_labels, metrics, is_pair, label_alias=None):\n        self.class_labels = class_labels\n        self.metrics = metrics\n        self.is_pair = is_pair\n        self.label_alias = label_alias\n\n    def get_dataset(self, segment=\'train\'):\n        """"""Get the corresponding dataset for the task.\n\n        Parameters\n        ----------\n        segment : str, default \'train\'\n            Dataset segments.\n\n        Returns\n        -------\n        TSVDataset : the dataset of target segment.\n        """"""\n        raise NotImplementedError\n\n    def dataset_train(self):\n        """"""Get the training segment of the dataset for the task.\n\n        Returns\n        -------\n        tuple of str, TSVDataset : the segment name, and the dataset.\n        """"""\n        return \'train\', self.get_dataset(segment=\'train\')\n\n    def dataset_dev(self):\n        """"""Get the development (i.e. validation) segment of the dataset for this task.\n\n        Returns\n        -------\n        tuple of (str, TSVDataset), or list of tuple : the segment name, and the dataset.\n        """"""\n        return \'dev\', self.get_dataset(segment=\'dev\')\n\n    def dataset_test(self):\n        """"""Get the test segment of the dataset for the task.\n\n        Returns\n        -------\n        tuple of (str, TSVDataset), or list of tuple : the segment name, and the dataset.\n        """"""\n        return \'test\', self.get_dataset(segment=\'test\')\n\nclass ToySSTTask(AbstractGlueTask):\n    """"""The Stanford Sentiment Treebank task on GLUE benchmark.""""""\n    def __init__(self):\n        is_pair = False\n        class_labels = [\'0\', \'1\']\n        self.metric = Accuracy()\n        super(ToySSTTask, self).__init__(class_labels, self.metric, is_pair)\n\n    def get_dataset(self, segment=\'train\'):\n        """"""Get the corresponding dataset for SST\n\n        Parameters\n        ----------\n        segment : str, default \'train\'\n            Dataset segments. Options are \'train\', \'dev\', \'test\'.\n        """"""\n        dataset = GlueSST2(segment=segment)\n        sampler, _ = get_split_samplers(dataset, split_ratio=0.01)\n        return SampledDataset(dataset, sampler)\n\nclass AbstractCustomTask:\n    """"""Abstract task class\xc2\xa0for custom datasets to be used with BERT-family models.\n\n    Parameters\n    ----------\n    class_labels : list of str, or None\n        Classification labels of the task.\n        Set to None for regression tasks with continuous real values (TODO).\n    metrics : list of EValMetric\n        Evaluation metrics of the task.\n    is_pair : bool\n        Whether the task deals with sentence pairs or single sentences.\n    label_alias : dict\n        label alias dict, some different labels in dataset actually means\n        the same. e.g.: {\'contradictory\':\'contradiction\'} means contradictory\n        and contradiction label means the same in dataset, they will get\n        the same class id.\n    """"""\n    def __init__(self, class_labels, metrics, is_pair, label_alias=None):\n        self.class_labels = class_labels\n        self.metrics = metrics\n        self.is_pair = is_pair\n        self.label_alias = label_alias\n\n    def get_dataset(self, segment=\'train\'):\n        """"""Get the corresponding dataset for the task.\n\n        Parameters\n        ----------\n        segment : str, default \'train\'\n            Dataset segments.\n\n        Returns\n        -------\n        Dataset : the dataset of target segment.\n        """"""\n        raise ValueError(\'This is an abstract class. For custom dataset support, \'\n                         \'please use CustomTSVClassificationTask instead.\')\n\n    def dataset_train(self):\n        """"""Get the training segment of the dataset for the task.\n\n        Returns\n        -------\n        tuple of str, Dataset : the segment name, and the dataset.\n        """"""\n        return \'train\', self.get_dataset(segment=\'train\')\n\n    def dataset_dev(self):\n        """"""Get the development (i.e. validation) segment of the dataset for this task.\n\n        Returns\n        -------\n        tuple of str, Dataset : the segment name, and the dataset.\n        """"""\n        return \'dev\', self.get_dataset(segment=\'dev\')\n\n    def dataset_test(self):\n        """"""Get the test segment of the dataset for the task.\n\n        Returns\n        -------\n        tuple of str, Dataset : the segment name, and the dataset.\n        """"""\n        return \'test\', self.get_dataset(segment=\'test\')\n\n\nclass CustomTSVClassificationTask(AbstractCustomTask):\n    """"""\n\n    Parameters\n    ----------\n    filepath: str, path of the file\n        Any valid string path is acceptable.\n    sep : str, default \xe2\x80\x98,\xe2\x80\x99\n        Delimiter to use.\n    usecols : list-like or callable\n        Return a subset of the columns.\n    header : int, list of int, default \'infer\', optional\n        Row number(s) to use as the column names, and the start of the\n        data. Default behavior is to infer the column names: if no names are passed the behavior is identical\n        to header=0 and column names are inferred from the first line of the file,\n        if column names are passed explicitly then the behavior is identical to header=None.\n    names : array-like, optional\n        List of column names to use.\n    split_ratio: float, default 0.8, optional\n        Split ratio of training data and HPO validation data. split_ratio of the data goes to training data,\n        (1-split_ratio) of th data goes to HPO validation data.\n    For other keyword arguments, please see\n        <https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html#pandas.read_csv`_ .\n    """"""\n    def __init__(self, *args, **kwargs):\n        self._read(**kwargs)\n        self.is_pair = False\n        self._label_column_id = self._index_label_column(**kwargs)\n        self.class_labels = list(set([sample[self._label_column_id] for sample in self.dataset]))\n        self.metric = Accuracy()\n        super(CustomTSVClassificationTask, self).__init__(self.class_labels, self.metric, self.is_pair)\n\n    def get_dataset(self, segment=\'train\'):\n        """"""Get the corresponding dataset for the task.\n\n        Parameters\n        ----------\n        segment : str, default \'train\'\n            Dataset segments.\n\n        Returns\n        -------\n        The dataset of target segment.\n        """"""\n        if segment == \'train\':\n            return SampledDataset(self.dataset, self.train_sampler)\n        elif segment == \'dev\':\n            return SampledDataset(self.dataset, self.dev_sampler)\n        else:\n            raise ValueError(\'Please specify either train or dev dataset.\')\n\n    def _read(self, **kwargs):\n        low_memory = kwargs.get(\'low_memory\', None)\n        if low_memory is None:\n            kwargs[\'low_memory\'] = False\n        split_ratio = kwargs.pop(\'split_ratio\', 0.8)\n        path = kwargs.pop(\'filepath\', None)\n        kwargs[\'filepath_or_buffer\'] = path\n        dataset_df = kwargs.pop(\'df\', None)\n        if path is not None:\n            if \'usecols\' not in kwargs:\n                raise ValueError(\'`usecols` must be provided to support the \'\n                                 \'identification of text and target/label columns.\')\n            dataset_df = pd.read_csv(**kwargs)\n        elif dataset_df is not None:\n            dataset_df = dataset_df.copy(deep=True)\n        else:\n            raise ValueError(\'A dataset constructed from either an external csv/tsv file by setting filepath=x,\'\n                             \'or an in-memory python DataFrame by setting df=x is supported now.\')\n        dataset_df_lst = dataset_df.values.tolist()\n        self.dataset = gluon.data.SimpleDataset(dataset_df_lst)\n        self.train_sampler, self.dev_sampler = get_split_samplers(self.dataset, split_ratio=split_ratio)\n\n    def _index_label_column(self, **kwargs):\n        label_column_id = kwargs.get(\'label\', None)\n        if label_column_id is None:\n            warnings.warn(\'By default we are using the last column as the label column, \'\n                          \'if you wish to specify the label column by yourself or the column is not label column,\'\n                          \'please specify label column using either the column name or index by using label=x.\')\n            return len(self.dataset[0]) - 1\n        if isinstance(label_column_id, str):\n            usecols = kwargs.get(\'usecols\', None)\n            if usecols is None:\n                raise ValueError(\'Please specify the `usecols` when specifying the label column.\')\n            else:\n                for i, col in enumerate(usecols):\n                    if col == label_column_id:\n                        return i\n        elif isinstance(label_column_id, int):\n            return label_column_id\n        else:\n            raise ValueError(\'Please specify label column using either the column name (str) or index (int).\')\n\n    def dataset_train(self):\n        """"""Get the training segment of the dataset for the task.\n\n        Returns\n        -------\n        tuple of str, Dataset : the segment name, and the dataset.\n        """"""\n        return \'train\', self.get_dataset(\'train\')\n\n    def dataset_dev(self):\n        """"""Get the dev segment of the dataset for the task.\n\n        Returns\n        -------\n        tuple of str, Dataset : the segment name, and the dataset.\n        """"""\n        return \'dev\', self.get_dataset(\'dev\')\n\nclass MRPCTask(AbstractGlueTask):\n    """"""The MRPC task on GLUE benchmark.""""""\n    def __init__(self):\n        is_pair = True\n        class_labels = [\'0\', \'1\']\n        metric = CompositeEvalMetric()\n        metric.add(F1())\n        metric.add(Accuracy())\n        super(MRPCTask, self).__init__(class_labels, metric, is_pair)\n\n    def get_dataset(self, segment=\'train\'):\n        """"""Get the corresponding dataset for MRPC.\n\n        Parameters\n        ----------\n        segment : str, default \'train\'\n            Dataset segments. Options are \'train\', \'dev\', \'test\'.\n        """"""\n        return GlueMRPC(segment=segment)\n\nclass QQPTask(AbstractGlueTask):\n    """"""The Quora Question Pairs task on GLUE benchmark.""""""\n    def __init__(self):\n        is_pair = True\n        class_labels = [\'0\', \'1\']\n        metric = CompositeEvalMetric()\n        metric.add(F1())\n        metric.add(Accuracy())\n        super(QQPTask, self).__init__(class_labels, metric, is_pair)\n\n    def get_dataset(self, segment=\'train\'):\n        """"""Get the corresponding dataset for QQP.\n\n        Parameters\n        ----------\n        segment : str, default \'train\'\n            Dataset segments. Options are \'train\', \'dev\', \'test\'.\n        """"""\n        return GlueQQP(segment=segment)\n\n\nclass RTETask(AbstractGlueTask):\n    """"""The Recognizing Textual Entailment task on GLUE benchmark.""""""\n    def __init__(self):\n        is_pair = True\n        class_labels = [\'not_entailment\', \'entailment\']\n        metric = Accuracy()\n        super(RTETask, self).__init__(class_labels, metric, is_pair)\n\n    def get_dataset(self, segment=\'train\'):\n        """"""Get the corresponding dataset for RTE.\n\n        Parameters\n        ----------\n        segment : str, default \'train\'\n            Dataset segments. Options are \'train\', \'dev\', \'test\'.\n        """"""\n        return GlueRTE(segment=segment)\n\nclass QNLITask(AbstractGlueTask):\n    """"""The SQuAD NLI task on GLUE benchmark.""""""\n    def __init__(self):\n        is_pair = True\n        class_labels = [\'not_entailment\', \'entailment\']\n        metric = Accuracy()\n        super(QNLITask, self).__init__(class_labels, metric, is_pair)\n\n    def get_dataset(self, segment=\'train\'):\n        """"""Get the corresponding dataset for QNLI.\n\n        Parameters\n        ----------\n        segment : str, default \'train\'\n            Dataset segments. Options are \'train\', \'dev\', \'test\'.\n        """"""\n        return GlueQNLI(segment=segment)\n\nclass STSBTask(AbstractGlueTask):\n    """"""The Sentence Textual Similarity Benchmark task on GLUE benchmark.""""""\n    def __init__(self):\n        is_pair = True\n        class_labels = None\n        metric = PearsonCorrelation()\n        super(STSBTask, self).__init__(class_labels, metric, is_pair)\n\n    def get_dataset(self, segment=\'train\'):\n        """"""Get the corresponding dataset for STSB\n\n        Parameters\n        ----------\n        segment : str, default \'train\'\n            Dataset segments. Options are \'train\', \'dev\', \'test\'.\n        """"""\n        return GlueSTSB(segment=segment)\n\nclass CoLATask(AbstractGlueTask):\n    """"""The Warstdadt acceptability task on GLUE benchmark.""""""\n    def __init__(self):\n        is_pair = False\n        class_labels = [\'0\', \'1\']\n        metric = MCC(average=\'micro\')\n        super(CoLATask, self).__init__(class_labels, metric, is_pair)\n\n    def get_dataset(self, segment=\'train\'):\n        """"""Get the corresponding dataset for CoLA\n\n        Parameters\n        ----------\n        segment : str, default \'train\'\n            Dataset segments. Options are \'train\', \'dev\', \'test\'.\n        """"""\n        return GlueCoLA(segment=segment)\n\nclass SSTTask(AbstractGlueTask):\n    """"""The Stanford Sentiment Treebank task on GLUE benchmark.""""""\n    def __init__(self):\n        is_pair = False\n        class_labels = [\'0\', \'1\']\n        self.metric = Accuracy()\n        super(SSTTask, self).__init__(class_labels, self.metric, is_pair)\n\n    def get_dataset(self, segment=\'train\'):\n        """"""Get the corresponding dataset for SST\n\n        Parameters\n        ----------\n        segment : str, default \'train\'\n            Dataset segments. Options are \'train\', \'dev\', \'test\'.\n        """"""\n        return GlueSST2(segment=segment)\n\nclass WNLITask(AbstractGlueTask):\n    """"""The Winograd NLI task on GLUE benchmark.""""""\n    def __init__(self):\n        is_pair = True\n        class_labels = [\'0\', \'1\']\n        metric = Accuracy()\n        super(WNLITask, self).__init__(class_labels, metric, is_pair)\n\n    def get_dataset(self, segment=\'train\'):\n        """"""Get the corresponding dataset for WNLI\n\n        Parameters\n        ----------\n        segment : str, default \'train\'\n            Dataset segments. Options are \'dev\', \'test\', \'train\'\n        """"""\n        return GlueWNLI(segment=segment)\n\nclass MNLITask(AbstractGlueTask):\n    """"""The Multi-Genre Natural Language Inference task on GLUE benchmark.""""""\n    def __init__(self):\n        is_pair = True\n        class_labels = [\'neutral\', \'entailment\', \'contradiction\']\n        metric = Accuracy()\n        super(MNLITask, self).__init__(class_labels, metric, is_pair)\n\n    def get_dataset(self, segment=\'train\'):\n        """"""Get the corresponding dataset for MNLI\n\n        Parameters\n        ----------\n        segment : str, default \'train\'\n            Dataset segments. Options are \'dev_matched\', \'dev_mismatched\', \'test_matched\',\n            \'test_mismatched\', \'train\'\n        """"""\n        return GlueMNLI(segment=segment)\n\n    def dataset_dev(self):\n        """"""Get the dev segment of the dataset for the task.\n\n        Returns\n        -------\n        list of TSVDataset : the dataset of the dev segment.\n        """"""\n        return [(\'dev_matched\', self.get_dataset(segment=\'dev_matched\')),\n                (\'dev_mismatched\', self.get_dataset(segment=\'dev_mismatched\'))]\n\n    def dataset_test(self):\n        """"""Get the test segment of the dataset for the task.\n\n        Returns\n        -------\n        list of TSVDataset : the dataset of the test segment.\n        """"""\n        return [(\'test_matched\', self.get_dataset(segment=\'test_matched\')),\n                (\'test_mismatched\', self.get_dataset(segment=\'test_mismatched\'))]\n\nbuilt_in_tasks = {\n    \'mrpc\': MRPCTask,\n    \'qqp\': QQPTask,\n    \'qnli\': QNLITask,\n    \'rte\': RTETask,\n    \'sts-b\': STSBTask,\n    \'cola\': CoLATask,\n    \'mnli\': MNLITask,\n    \'wnli\': WNLITask,\n    \'sst\': SSTTask,\n    \'toysst\': ToySSTTask,\n}\n'"
autogluon/task/text_classification/network.py,0,"b'from typing import AnyStr\n\nimport gluonnlp as nlp\nimport mxnet as mx\nfrom mxnet import gluon\nfrom mxnet.gluon import Block, HybridBlock, nn\nimport gluonnlp as nlp\n\nfrom .dataset import *\n\n\n__all__ = [\'get_network\', \'LMClassifier\', \'BERTClassifier\', \'RoBERTaClassifier\']\n\ndef get_network(bert, class_labels, use_roberta=False):\n    do_regression = not class_labels\n    num_classes = 1 if do_regression else len(class_labels)\n    # reuse the BERTClassifier class with num_classes=1 for regression\n    if use_roberta:\n        model = RoBERTaClassifier(bert, dropout=0.0, num_classes=num_classes)\n    else:\n        model = BERTClassifier(bert, dropout=0.1, num_classes=num_classes)\n    return model\n\n\nclass LMClassifier(gluon.Block):\n    """"""\n    Network for Text Classification which uses a pre-trained language model.\n    This works with  standard_lstm_lm_200, standard_lstm_lm_650, standard_lstm_lm_1500, awd_lstm_lm_1150, awd_lstm_lm_600\n    """"""\n\n    def __init__(self, prefix=None, params=None, embedding=None):\n        super(LMClassifier, self).__init__(prefix=prefix, params=params)\n        with self.name_scope():\n            self.embedding = embedding\n            self.encoder = None\n            self.classifier = None\n            self.pool_out = None\n\n    def forward(self, data, valid_length):  # pylint: disable=arguments-differ\n        encoded = self.encoder(self.embedding(data))\n        # Add mean pooling to the output of the LSTM Layers\n        masked_encoded = mx.ndarray.SequenceMask(encoded, sequence_length=valid_length, use_sequence_length=True)\n        self.pool_out = mx.ndarray.broadcast_div(mx.ndarray.sum(masked_encoded, axis=0),\n                                             mx.ndarray.expand_dims(valid_length, axis=1))\n        out = self.classifier(self.pool_out)\n        return out\n\n\nclass BERTClassifier(gluon.Block):\n    """"""\n    Network for Text Classification which uses a BERT pre-trained model.\n    This works with  bert_12_768_12, bert_24_1024_16.\n    Adapted from https://github.com/dmlc/gluon-nlp/blob/master/scripts/bert/model/classification.py#L76\n    """"""\n\n    def __init__(self, bert, num_classes=2, dropout=0.0, prefix=None, params=None):\n        super(BERTClassifier, self).__init__(prefix=prefix, params=params)\n        self.bert = bert\n        self.pool_out = None\n        with self.name_scope():\n            self.classifier = gluon.nn.HybridSequential(prefix=prefix)\n            if dropout:\n                self.classifier.add(gluon.nn.Dropout(rate=dropout))\n            self.classifier.add(gluon.nn.Dense(units=num_classes))\n\n    def forward(self, inputs, token_types, valid_length=None):  # pylint: disable=arguments-differ\n        _, self.pooler_out = self.bert(inputs, token_types, valid_length)\n        return self.classifier(self.pooler_out)\n\nclass RoBERTaClassifier(HybridBlock):\n    """"""Model for sentence (pair) classification task with BERT.\n    The model feeds token ids and token type ids into BERT to get the\n    pooled BERT sequence representation, then apply a Dense layer for\n    classification.\n    Parameters\n    ----------\n    bert: RoBERTaModel\n        The RoBERTa model.\n    num_classes : int, default is 2\n        The number of target classes.\n    dropout : float or None, default 0.0.\n        Dropout probability for the bert output.\n    prefix : str or None\n        See document of `mx.gluon.Block`.\n    params : ParameterDict or None\n        See document of `mx.gluon.Block`.\n    Inputs:\n        - **inputs**: input sequence tensor, shape (batch_size, seq_length)\n        - **valid_length**: optional tensor of input sequence valid lengths.\n            Shape (batch_size, num_classes).\n    Outputs:\n        - **output**: Regression output, shape (batch_size, num_classes)\n    """"""\n\n    def __init__(self, roberta, num_classes=2, dropout=0.0,\n                 prefix=None, params=None):\n        super(RoBERTaClassifier, self).__init__(prefix=prefix, params=params)\n        self.roberta = roberta\n        self._units = roberta._units\n        with self.name_scope():\n            self.classifier = nn.HybridSequential(prefix=prefix)\n            if dropout:\n                self.classifier.add(nn.Dropout(rate=dropout))\n            self.classifier.add(nn.Dense(units=self._units, activation=\'tanh\'))\n            if dropout:\n                self.classifier.add(nn.Dropout(rate=dropout))\n            self.classifier.add(nn.Dense(units=num_classes))\n\n    def __call__(self, inputs, valid_length=None):\n        # pylint: disable=dangerous-default-value, arguments-differ\n        """"""Generate the unnormalized score for the given the input sequences.\n        Parameters\n        ----------\n        inputs : NDArray or Symbol, shape (batch_size, seq_length)\n            Input words for the sequences.\n        valid_length : NDArray or Symbol, or None, shape (batch_size)\n            Valid length of the sequence. This is used to mask the padded tokens.\n        Returns\n        -------\n        outputs : NDArray or Symbol\n            Shape (batch_size, num_classes)\n        """"""\n        return super(RoBERTaClassifier, self).__call__(inputs, valid_length)\n\n    def hybrid_forward(self, F, inputs, valid_length=None):\n        # pylint: disable=arguments-differ\n        """"""Generate the unnormalized score for the given the input sequences.\n        Parameters\n        ----------\n        inputs : NDArray or Symbol, shape (batch_size, seq_length)\n            Input words for the sequences.\n        valid_length : NDArray or Symbol, or None, shape (batch_size)\n            Valid length of the sequence. This is used to mask the padded tokens.\n        Returns\n        -------\n        outputs : NDArray or Symbol\n            Shape (batch_size, num_classes)\n        """"""\n        seq_out = self.roberta(inputs, valid_length)\n        assert not isinstance(seq_out, (tuple, list)), \'Expected one output from RoBERTaModel\'\n        outputs = seq_out.slice(begin=(0, 0, 0), end=(None, 1, None))\n        outputs = outputs.reshape(shape=(-1, self._units))\n        return self.classifier(outputs)\n'"
autogluon/task/text_classification/pipeline.py,0,"b'import os\nimport io\nimport time\nimport logging\nimport multiprocessing\nimport random\nimport warnings\nimport numpy as np\n\nimport mxnet as mx\nfrom mxnet import gluon, init, autograd, nd\nfrom mxnet.gluon import nn\nimport gluonnlp as nlp\nfrom gluonnlp.data import BERTTokenizer\nfrom .network import get_network#BERTClassifier, RoBERTaClassifier, LMClassifier\nfrom .dataset import *\nfrom .transforms import BERTDatasetTransform\nfrom ...core import *\nfrom ...utils import tqdm\nfrom ...utils.mxutils import collect_params\n\n__all__ = [\'train_text_classification\', \'preprocess_data\']\n\n\ndef preprocess_data(tokenizer, task, batch_size, dev_batch_size, max_len, vocab, pad=False, num_workers=1):\n    """"""Train/eval Data preparation function.""""""\n    pool = multiprocessing.Pool()\n\n    # transformation for data train and dev\n    label_dtype = \'float32\' if not task.class_labels else \'int32\'\n    trans = BERTDatasetTransform(tokenizer, max_len,\n                                 vocab=vocab,\n                                 class_labels=task.class_labels,\n                                 label_alias=task.label_alias,\n                                 pad=pad, pair=task.is_pair,\n                                 has_label=True)\n\n    # data train\n    # task.dataset_train returns (segment_name, dataset)\n    train_tsv = task.dataset_train()[1]\n    data_train = mx.gluon.data.SimpleDataset(pool.map(trans, train_tsv))\n    data_train_len = data_train.transform(\n        lambda input_id, length, segment_id, label_id: length, lazy=False)\n    # bucket sampler for training\n    pad_val = vocab[vocab.padding_token]\n    batchify_fn = nlp.data.batchify.Tuple(\n        nlp.data.batchify.Pad(axis=0, pad_val=pad_val),  # input\n        nlp.data.batchify.Stack(),  # length\n        nlp.data.batchify.Pad(axis=0, pad_val=0),  # segment\n        nlp.data.batchify.Stack(label_dtype))  # label\n    batch_sampler = nlp.data.sampler.FixedBucketSampler(\n        data_train_len,\n        batch_size=batch_size,\n        num_buckets=10,\n        ratio=0,\n        shuffle=True)\n    # data loader for training\n    loader_train = gluon.data.DataLoader(\n        dataset=data_train,\n        num_workers=num_workers,\n        batch_sampler=batch_sampler,\n        batchify_fn=batchify_fn)\n\n    # data dev. For MNLI, more than one dev set is available\n    dev_tsv = task.dataset_dev()\n    dev_tsv_list = dev_tsv if isinstance(dev_tsv, list) else [dev_tsv]\n    loader_dev_list = []\n    for segment, data in dev_tsv_list:\n        data_dev = mx.gluon.data.SimpleDataset(pool.map(trans, data))\n        loader_dev = mx.gluon.data.DataLoader(\n            data_dev,\n            batch_size=dev_batch_size,\n            num_workers=num_workers,\n            shuffle=False,\n            batchify_fn=batchify_fn)\n        loader_dev_list.append((segment, loader_dev))\n\n    # batchify for data test\n    test_batchify_fn = nlp.data.batchify.Tuple(\n        nlp.data.batchify.Pad(axis=0, pad_val=pad_val), nlp.data.batchify.Stack(),\n        nlp.data.batchify.Pad(axis=0, pad_val=0))\n    # transform for data test\n    test_trans = BERTDatasetTransform(tokenizer, max_len,\n                                      vocab=vocab,\n                                      class_labels=None,\n                                      pad=pad, pair=task.is_pair,\n                                      has_label=False)\n\n    # data test. For MNLI, more than one test set is available\n    #test_tsv = task.dataset_test()\n    #test_tsv_list = test_tsv if isinstance(test_tsv, list) else [test_tsv]\n    #loader_test_list = []\n    #for segment, data in test_tsv_list:\n    #    data_test = mx.gluon.data.SimpleDataset(pool.map(test_trans, data))\n    #    loader_test = mx.gluon.data.DataLoader(\n    #        data_test,\n    #        batch_size=dev_batch_size,\n    #        num_workers=num_workers,\n    #        shuffle=False,\n    #        batchify_fn=test_batchify_fn)\n    #    loader_test_list.append((segment, loader_test))\n    return loader_train, loader_dev_list, len(data_train), trans, test_trans\n\n@args()\ndef train_text_classification(args, reporter=None):\n    # Step 1: add scripts every function and python objects in the original training script except for the training function\n    # at the beginning of the decorated function\n    logger = logging.getLogger(__name__)\n    if args.verbose:\n        logger.setLevel(logging.INFO)\n        logger.info(args)\n    batch_size = args.batch_size\n    dev_batch_size = args.dev_batch_size\n    lr = args.lr\n    epsilon = args.epsilon\n    accumulate = args.accumulate\n    log_interval = args.log_interval * accumulate if accumulate else args.log_interval\n    if accumulate:\n        logger.info(\'Using gradient accumulation. Effective batch size = \' \\\n                     \'batch_size * accumulate = %d\', accumulate * batch_size)\n\n    # random seed\n    np.random.seed(args.seed)\n    random.seed(args.seed)\n    mx.random.seed(args.seed)\n\n    # TODO support for multi-GPU\n    ctx = [mx.gpu(i) for i in range(args.num_gpus)][0] if args.num_gpus > 0 else [mx.cpu()][0]\n\n    task = args.dataset\n    # data type with mixed precision training\n    if args.dtype == \'float16\':\n        try:\n            from mxnet.contrib import amp  # pylint: disable=ungrouped-imports\n            # monkey patch amp list since topk does not support fp16\n            amp.lists.symbol.FP32_FUNCS.append(\'topk\')\n            amp.lists.symbol.FP16_FP32_FUNCS.remove(\'topk\')\n            amp.init()\n        except ValueError:\n            # topk is already in the FP32_FUNCS list\n            amp.init()\n        except ImportError:\n            # amp is not available\n            logger.info(\'Mixed precision training with float16 requires MXNet >= \'\n                         \'1.5.0b20190627. Please consider upgrading your MXNet version.\')\n            exit()\n\n    # model and loss\n    model_name = args.net\n    dataset = args.pretrained_dataset\n\n    use_roberta = \'roberta\' in model_name\n    get_model_params = {\n        \'name\': model_name,\n        \'dataset_name\': dataset,\n        \'pretrained\': True,\n        \'ctx\': ctx,\n        \'use_decoder\': False,\n        \'use_classifier\': False,\n    }\n    # RoBERTa does not contain parameters for sentence pair classification\n    if not use_roberta:\n        get_model_params[\'use_pooler\'] = True\n\n    bert, vocabulary = nlp.model.get_model(**get_model_params)\n    model = get_network(bert, task.class_labels, use_roberta)\n    #do_regression = not task.class_labels\n    #if do_regression:\n    #    num_classes = 1\n    #    loss_function = gluon.loss.L2Loss()\n    #else:\n    #    num_classes = len(task.class_labels)\n    #    loss_function = gluon.loss.SoftmaxCELoss()\n    ## reuse the BERTClassifier class with num_classes=1 for regression\n    #if use_roberta:\n    #    model = RoBERTaClassifier(bert, dropout=0.0, num_classes=num_classes)\n    #else:\n    #    model = BERTClassifier(bert, dropout=0.1, num_classes=num_classes)\n    # initialize classifier\n    loss_function = gluon.loss.SoftmaxCELoss() if task.class_labels else gluon.loss.L2Loss()\n    initializer = mx.init.Normal(0.02)\n    model.classifier.initialize(init=initializer, ctx=ctx)\n\n    model.hybridize(static_alloc=True)\n    loss_function.hybridize(static_alloc=True)\n\n    # data processing\n    do_lower_case = \'uncased\' in dataset\n    if use_roberta:\n        bert_tokenizer = nlp.data.GPT2BPETokenizer()\n    else:\n        bert_tokenizer = BERTTokenizer(vocabulary, lower=do_lower_case)\n\n\n    # Get the loader.\n    train_data, dev_data_list, num_train_examples, trans, test_trans = preprocess_data(\n        bert_tokenizer, task, batch_size, dev_batch_size, args.max_len, vocabulary,\n        True, args.num_workers)\n\n    def log_train(batch_id, batch_num, metric, step_loss, log_interval, epoch_id, learning_rate, tbar):\n        """"""Generate and print out the log message for training. """"""\n        metric_nm, metric_val = metric.get()\n        if not isinstance(metric_nm, list):\n            metric_nm, metric_val = [metric_nm], [metric_val]\n\n        train_str = \'[Epoch %d] loss=%.4f, lr=%.7f, metrics:\' + \\\n                    \',\'.join([i + \':%.4f\' for i in metric_nm])\n        tbar.set_description(train_str % (epoch_id, step_loss / log_interval, learning_rate, *metric_val))\n\n    def log_eval(batch_id, batch_num, metric, step_loss, log_interval, tbar):\n        """"""Generate and print out the log message for inference. """"""\n        metric_nm, metric_val = metric.get()\n        if not isinstance(metric_nm, list):\n            metric_nm, metric_val = [metric_nm], [metric_val]\n\n        eval_str = \'loss=%.4f, metrics:\' + \\\n                   \',\'.join([i + \':%.4f\' for i in metric_nm])\n        tbar.set_description(eval_str % (step_loss / log_interval, *metric_val))\n\n    def evaluate(loader_dev, metric, segment):\n        """"""Evaluate the model on validation dataset.""""""\n        metric.reset()\n        step_loss = 0\n        tbar = tqdm(loader_dev)\n        for batch_id, seqs in enumerate(tbar):\n            input_ids, valid_length, segment_ids, label = seqs\n            input_ids = input_ids.as_in_context(ctx)\n            valid_length = valid_length.as_in_context(ctx).astype(\'float32\')\n            label = label.as_in_context(ctx)\n            if use_roberta:\n                out = model(input_ids, valid_length)\n            else:\n                out = model(input_ids, segment_ids.as_in_context(ctx), valid_length)\n            ls = loss_function(out, label).mean()\n\n            step_loss += ls.asscalar()\n            metric.update([label], [out])\n\n            if (batch_id + 1) % (args.log_interval) == 0:\n                log_eval(batch_id, len(loader_dev), metric, step_loss, args.log_interval, tbar)\n                step_loss = 0\n\n        metric_nm, metric_val = metric.get()\n        if not isinstance(metric_nm, list):\n            metric_nm, metric_val = [metric_nm], [metric_val]\n        metric_str = \'validation metrics:\' + \',\'.join([i + \':%.4f\' for i in metric_nm])\n        logger.info(metric_str, *metric_val)\n\n        mx.nd.waitall()\n        return metric_nm, metric_val\n\n    # Step 2: the training function in the original training script is added in the decorated function in autogluon for training.\n    """"""Training function.""""""\n\n    all_model_params = model.collect_params()\n    optimizer_params = {\'learning_rate\': lr, \'epsilon\': epsilon, \'wd\': 0.01}\n    trainer = gluon.Trainer(all_model_params, \'bertadam\',\n                            optimizer_params, update_on_kvstore=False)\n    if args.dtype == \'float16\':\n        amp.init_trainer(trainer)\n\n    step_size = batch_size * accumulate if accumulate else batch_size\n    num_train_steps = int(num_train_examples / step_size * args.epochs)\n    warmup_ratio = args.warmup_ratio\n    num_warmup_steps = int(num_train_steps * warmup_ratio)\n    step_num = 0\n\n    # Do not apply weight decay on LayerNorm and bias terms\n    for _, v in model.collect_params(\'.*beta|.*gamma|.*bias\').items():\n        v.wd_mult = 0.0\n    # Collect differentiable parameters\n    params = [p for p in all_model_params.values() if p.grad_req != \'null\']\n\n    # Set grad_req if gradient accumulation is required\n    if accumulate and accumulate > 1:\n        for p in params:\n            p.grad_req = \'add\'\n    # track best eval score\n    metric_history = []\n    best_metric = None\n    patience = args.early_stop\n\n    tic = time.time()\n    for epoch_id in range(args.epochs):\n        if args.early_stop and patience == 0:\n            logger.info(\'Early stopping at epoch %d\', epoch_id)\n            break\n        task.metric.reset()\n        step_loss = 0\n        tic = time.time()\n        all_model_params.zero_grad()\n\n        tbar = tqdm(train_data)\n        for batch_id, seqs in enumerate(tbar):\n            # learning rate schedule\n            if step_num < num_warmup_steps:\n                new_lr = lr * step_num / num_warmup_steps\n            else:\n                non_warmup_steps = step_num - num_warmup_steps\n                offset = non_warmup_steps / (num_train_steps - num_warmup_steps)\n                new_lr = lr - offset * lr\n            trainer.set_learning_rate(new_lr)\n\n            # forward and backward\n            with mx.autograd.record():\n                input_ids, valid_length, segment_ids, label = seqs\n                input_ids = input_ids.as_in_context(ctx)\n                valid_length = valid_length.as_in_context(ctx).astype(\'float32\')\n                label = label.as_in_context(ctx)\n                if use_roberta:\n                    out = model(input_ids, valid_length)\n                else:\n                    out = model(input_ids, segment_ids.as_in_context(ctx), valid_length)\n                ls = loss_function(out, label).mean()\n                if args.dtype == \'float16\':\n                    with amp.scale_loss(ls, trainer) as scaled_loss:\n                        mx.autograd.backward(scaled_loss)\n                else:\n                    ls.backward()\n\n            # update\n            if not accumulate or (batch_id + 1) % accumulate == 0:\n                trainer.allreduce_grads()\n                nlp.utils.clip_grad_global_norm(params, 1)\n                trainer.update(accumulate if accumulate else 1)\n                step_num += 1\n                if accumulate and accumulate > 1:\n                    # set grad to zero for gradient accumulation\n                    all_model_params.zero_grad()\n\n            step_loss += ls.asscalar()\n            task.metric.update([label], [out])\n            if (batch_id + 1) % (args.log_interval) == 0:\n                log_train(batch_id, len(train_data), task.metric, step_loss, args.log_interval,\n                          epoch_id, trainer.learning_rate, tbar)\n                step_loss = 0\n        mx.nd.waitall()\n\n        # inference on dev data\n        for segment, dev_data in dev_data_list:\n            metric_nm, metric_val = evaluate(dev_data, task.metric, segment)\n            if best_metric is None or metric_val >= best_metric:\n                best_metric = metric_val\n                patience = args.early_stop\n            else:\n                if args.early_stop is not None:\n                    patience -= 1\n            metric_history.append((epoch_id, metric_nm, metric_val))\n\n        if reporter is not None:\n            # Note: epoch reported back must start with 1, not with 0\n            reporter(epoch=epoch_id+1, accuracy=metric_val[0])\n\n    if args.final_fit:\n        get_model_params.pop(\'ctx\')\n        return {\'model_params\': collect_params(model),\n                \'get_model_args\': get_model_params,\n                \'class_labels\': task.class_labels,\n                \'transform\': trans,\n                \'test_transform\': test_trans}\n'"
autogluon/task/text_classification/predictor.py,0,"b'import os\nimport math\nimport pickle\nimport copy\nimport numpy as np\nfrom collections import OrderedDict\nimport mxnet as mx\nimport matplotlib.pyplot as plt\nimport gluonnlp as nlp\n\nfrom ...utils import *\nfrom .network import *\nfrom .pipeline import *\nfrom .dataset import *\nfrom ..image_classification.classifier import Classifier\nfrom ...core import AutoGluonObject\n\n__all__ = [\'TextClassificationPredictor\']\n\nclass TextClassificationPredictor(Classifier):\n    """"""Trained Text Classifier returned by `fit()` that can be used to make predictions on new text data.\n    """"""\n    def __init__(self, model, transform, test_transform,\n                 results, scheduler_checkpoint, args):\n        self.model = model\n        self.use_roberta = \'roberta\' in args.net\n        self.transform = transform\n        self.test_transform = test_transform\n        self.results = self._format_results(results)\n        self.scheduler_checkpoint = scheduler_checkpoint\n        self.args = args\n\n    def predict(self, X):\n        """"""Predict class-index of a given sentence / text-snippet.\n        \n        Parameters\n        ----------\n        X : str\n            The input sentence we should classify.\n    \n        Examples\n        --------\n        >>> class_index = predictor.predict(\'this is cool\')\n    \n        Returns\n        -------\n        Int corresponding to index of the predicted class.\n        """"""\n        proba = self.predict_proba(X)\n        ind = mx.nd.argmax(proba, axis=1).astype(\'int\')\n        return ind\n\n    def predict_proba(self, X):\n        """"""Predict class-probabilities of a given sentence / text-snippet.\n        \n        Parameters\n        ----------\n        X : str\n            The input sentence we should classify.\n        \n        Examples\n        --------\n        >>> class_probs = predictor.predict_proba(\'this is cool\')\n        \n        Returns\n        -------\n        `mxnet.NDArray` containing predicted probabilities of each class.\n        """"""\n        inputs = self.test_transform(X)\n        X, valid_length, segment_id = [mx.nd.array(np.expand_dims(x, 0)) for x in inputs]\n        if self.use_roberta:\n            pred = self.model(X, valid_length)\n        else:\n            pred = self.model(X, segment_id, valid_length)\n        return mx.nd.softmax(pred)\n\n    def evaluate(self, dataset, ctx=[mx.cpu()]):\n        """"""Evaluate predictive performance of trained text classifier using given test data.\n        \n        Parameters\n        ----------\n        dataset : :class:`autogluon.task.TextClassification.Dataset`\n            The dataset containing test sentences (must be in same format as the training dataset provided to fit).\n        ctx : List of `mxnet.context` elements.\n            Determines whether to use CPU or GPU(s), options include: `[mx.cpu()]` or `[mx.gpu()]`.\n        \n         Examples\n         --------\n        >>> from autogluon import TextClassification as task\n        >>> dataset = task.Dataset(test_path=\'~/data/test\')\n        >>> test_performance = predictor.evaluate(dataset)\n        """"""\n        args = self.args\n        net = self.model\n        if isinstance(dataset, AutoGluonObject):\n            dataset = dataset.init()\n        if isinstance(dataset, AbstractGlueTask) or isinstance(dataset, AbstractCustomTask):\n            dataset = dataset.get_dataset(\'dev\')\n        if isinstance(ctx, list):\n            ctx = ctx[0]\n\n        metric = mx.metric.Accuracy()\n        dataset = dataset.transform(self.transform)\n        vocab = self.transform.vocab\n        pad_val = vocab[vocab.padding_token]\n        batchify_fn = nlp.data.batchify.Tuple(\n            nlp.data.batchify.Pad(axis=0, pad_val=pad_val),  # input\n            nlp.data.batchify.Stack(),  # length\n            nlp.data.batchify.Pad(axis=0, pad_val=0),  # segment\n            nlp.data.batchify.Stack(\'int32\'))  # label\n        loader_dev = mx.gluon.data.DataLoader(\n            dataset,\n            batch_size=args.dev_batch_size,\n            num_workers=args.num_workers,\n            shuffle=False,\n            batchify_fn=batchify_fn)\n\n        eval_func(net, loader_dev, metric, ctx, self.use_roberta)\n        _, test_reward = metric.get()\n        return test_reward\n\ndef eval_func(model, loader_dev, metric, ctx, use_roberta):\n    """"""Evaluate the model on validation dataset.""""""\n    metric.reset()\n    for batch_id, seqs in enumerate(loader_dev):\n        input_ids, valid_length, segment_ids, label = seqs\n        input_ids = input_ids.as_in_context(ctx)\n        valid_length = valid_length.as_in_context(ctx).astype(\'float32\')\n        label = label.as_in_context(ctx)\n        if use_roberta:\n            out = model(input_ids, valid_length)\n        else:\n            out = model(input_ids, segment_ids.as_in_context(ctx), valid_length)\n        metric.update([label], [out])\n\n    metric_nm, metric_val = metric.get()\n    if not isinstance(metric_nm, list):\n        metric_nm, metric_val = [metric_nm], [metric_val]\n    mx.nd.waitall()\n    return metric_nm, metric_val\n'"
autogluon/task/text_classification/text_classification.py,0,"b'import logging\n\nimport mxnet as mx\nimport gluonnlp as nlp\nimport copy\n\nfrom ...core import *\nfrom ...scheduler.resource import get_cpu_count, get_gpu_count\nfrom ..base import BaseTask, compile_scheduler_options\nfrom ...utils import update_params\n\nfrom .network import get_network\nfrom .dataset import get_dataset\nfrom .pipeline import *\nfrom .predictor import TextClassificationPredictor\n\n__all__ = [\'TextClassification\']\n\nlogger = logging.getLogger(__name__)\n\nclass TextClassification(BaseTask):\n    """"""AutoGluon Task for classifying text snippets based on their content\n    """"""\n    @staticmethod\n    def Dataset(*args, **kwargs):\n        """"""Dataset of text examples to make predictions for. \n           See :meth:`autogluon.task.TextClassification.get_dataset`\n        """"""\n        return get_dataset(*args, **kwargs)\n\n    @staticmethod\n    def fit(dataset=\'SST\',\n            net=Categorical(\'bert_12_768_12\'),\n            pretrained_dataset=Categorical(\'book_corpus_wiki_en_uncased\',\n                                           \'openwebtext_book_corpus_wiki_en_uncased\'),\n            lr=Real(2e-05, 2e-04, log=True),\n            warmup_ratio=0.01,\n            lr_scheduler=\'cosine\',\n            log_interval=100,\n            seed=0,\n            batch_size=32,\n            dev_batch_size=32,\n            max_len=128,\n            dtype=\'float32\',\n            epochs=3,\n            epsilon=1e-6,\n            accumulate=1,\n            early_stop=False,\n            nthreads_per_trial=4,\n            ngpus_per_trial=1,\n            hybridize=True,\n            scheduler_options=None,\n            search_strategy=\'random\',\n            search_options=None,\n            num_trials=None,\n            time_limits=None,\n            resume=False,\n            checkpoint=\'checkpoint/exp1.ag\',\n            visualizer=\'none\',\n            dist_ip_addrs=None,\n            auto_search=True,\n            verbose=False,\n            **kwargs):\n\n        """"""Fit neural networks on text dataset.\n\n        Parameters\n        ----------\n        dataset : str or :class:`autogluon.task.TextClassification.Dataset`\n            The Training dataset. You can specify a string to use a popular built-in text dataset.\n        net : str or :class:`autogluon.space.Categorical`\n            Which existing neural network models to consider as candidates.\n        pretrained_dataset : str, :class:`autogluon.space.Categorical`\n            Which existing datasets to consider as candidates for transfer learning from.\n        lr : float or :class:`autogluon.space`\n            The learning rate to use in each update of the neural network weights during training.\n        warmup_ratio : float\n            Ratio of overall training period considered as ""warm up"".\n        lr_scheduler : str\n            Describes how learning rate should be adjusted over the course of training. Options include: \'cosine\', \'poly\'.\n        log_interval : int\n            Log results every so many epochs during training.\n        seed : int\n            Random seed to set for reproducibility.\n        batch_size : int\n            How many examples to group in each mini-batch during gradient computations in training.\n        dev_batch_size : int\n            How many examples to group in each mini-batch during performance evalatuion over validation dataset.\n        max_len : int\n            Maximum number of words in a single training example (i.e. one text snippet).\n        dtype : str\n            Dtype used to represent data fed to neural networks.\n        epochs: int\n            How many epochs to train the neural networks for at most.\n        epsilon : float\n            Small number.\n        accumulate : int\n            How often to accumulate losses.\n        early_stop : bool\n            Whether to utilize early stopping during training to avoid overfitting.\n        num_trials : int\n            Maximal number of hyperparameter configurations to try out.\n        time_limits : int\n            Approximately how long should `fit()` should run for (wallclock time in seconds).\n            `fit()` will stop training new models after this amount of time has elapsed (but models which have already started training will continue to completion).\n        nthreads_per_trial : int\n            How many CPUs to use in each trial (ie. single training run of a model).\n        ngpus_per_trial : int\n            How many GPUs to use in each trial (ie. single training run of a model). \n        hybridize : bool\n            Whether or not the MXNet neural network should be hybridized (for increased efficiency).\n        scheduler_options : dict\n            Extra arguments passed to __init__ of scheduler, to configure the\n            orchestration of training jobs during hyperparameter-tuning.\n        search_strategy : str\n            Which hyperparameter search algorithm to use.\n            Options include: \'random\' (random search), \'skopt\' (SKopt Bayesian\n            optimization), \'grid\' (grid search), \'hyperband\' (Hyperband random),\n            \'rl\' (reinforcement learner).\n        search_options : dict\n            Auxiliary keyword arguments to pass to the searcher that performs\n            hyperparameter optimization.\n        verbose : bool\n            Whether or not to print out intermediate information during training.\n        checkpoint : str or None\n            State of hyperparameter search is stored to this local file\n        resume : bool\n            If True, the hyperparameter search is started from state loaded\n            from checkpoint\n        visualizer : str\n            Describes method to visualize training progress during `fit()`. Options: [\'mxboard\', \'tensorboard\', \'none\']. \n        dist_ip_addrs : list\n            List of IP addresses corresponding to remote workers, in order to leverage distributed computation.\n        auto_search : bool\n            If True, enables automatic suggestion of network types and hyper-parameter ranges adaptively based on provided dataset.\n        \n         Returns\n        -------\n        :class:`autogluon.task.text_classification.TextClassificationPredictor` object which can make predictions on new data and summarize what happened during `fit()`.\n        \n        Examples\n        --------\n        >>> from autogluon import TextClassification as task\n        >>> dataset = task.Dataset(name=\'ToySST\')\n        >>> predictor = task.fit(dataset)\n        """"""\n        assert search_strategy not in {\'bayesopt\', \'bayesopt_hyperband\'}, \\\n            ""search_strategy == \'bayesopt\' or \'bayesopt_hyperband\' not yet supported""\n\n        logger.warning(\'`TextClassification` is in preview mode.\'\n                       \'Please feel free to request new features in issues \'\n                       \'if it is not covered in the current implementation. \'\n                       \'If your dataset is in tabular format, you could also try out our `TabularPrediction` module.\')\n\n        if auto_search:\n            # The strategies can be injected here, for example: automatic suggest some hps\n            # based on the dataset statistics\n            pass\n\n        nthreads_per_trial = get_cpu_count() if nthreads_per_trial > get_cpu_count() else nthreads_per_trial\n        ngpus_per_trial = get_gpu_count() if ngpus_per_trial > get_gpu_count() else ngpus_per_trial\n\n        # If only time_limits is given, the scheduler starts trials until the\n        # time limit is reached\n        if num_trials is None and time_limits is None:\n            num_trials = 2\n\n        train_text_classification.register_args(\n            dataset=dataset,\n            pretrained_dataset=pretrained_dataset,\n            net=net,\n            lr=lr,\n            warmup_ratio=warmup_ratio,\n            early_stop=early_stop,\n            dtype=dtype,\n            max_len=max_len,\n            log_interval=log_interval,\n            epsilon=epsilon,\n            accumulate=accumulate,\n            seed=seed,\n            lr_scheduler=lr_scheduler,\n            num_gpus=ngpus_per_trial,\n            batch_size=batch_size,\n            dev_batch_size=dev_batch_size,\n            epochs=epochs,\n            num_workers=nthreads_per_trial,\n            hybridize=hybridize,\n            verbose=verbose,\n            final_fit=False,\n            **kwargs)\n\n        # Backward compatibility:\n        grace_period = kwargs.get(\'grace_period\')\n        if grace_period is not None:\n            if scheduler_options is None:\n                scheduler_options = {\'grace_period\': grace_period}\n            else:\n                assert \'grace_period\' not in scheduler_options, \\\n                    ""grace_period appears both in scheduler_options and as direct argument""\n                scheduler_options = copy.copy(scheduler_options)\n                scheduler_options[\'grace_period\'] = grace_period\n            logger.warning(\n                ""grace_period is deprecated, use ""\n                ""scheduler_options={\'grace_period\': ...} instead"")\n        scheduler_options = compile_scheduler_options(\n            scheduler_options=scheduler_options,\n            search_strategy=search_strategy,\n            search_options=search_options,\n            nthreads_per_trial=nthreads_per_trial,\n            ngpus_per_trial=ngpus_per_trial,\n            checkpoint=checkpoint,\n            num_trials=num_trials,\n            time_out=time_limits,\n            resume=resume,\n            visualizer=visualizer,\n            time_attr=\'epoch\',\n            reward_attr=\'accuracy\',\n            dist_ip_addrs=dist_ip_addrs,\n            epochs=epochs)\n        results = BaseTask.run_fit(\n            train_text_classification, search_strategy, scheduler_options)\n        args = sample_config(train_text_classification.args, results[\'best_config\'])\n        get_model_params = results.pop(\'get_model_args\')\n        get_model_params[\'ctx\'] = mx.cpu(0)\n        bert, _ = nlp.model.get_model(**get_model_params)\n        model = get_network(bert, results.pop(\'class_labels\'), \'roberta\' in args.net)\n        update_params(model, results.pop(\'model_params\'))\n        transform = results.pop(\'transform\')\n        test_transform = results.pop(\'test_transform\')\n        return TextClassificationPredictor(model, transform, test_transform, results, checkpoint, args)\n'"
autogluon/task/text_classification/transforms.py,0,"b'# Copyright 2018 The Google AI Language Team Authors and DMLC.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""BERT dataset transform.""""""\n\n\n__all__ = [\'BERTDatasetTransform\']\n\nimport numpy as np\nfrom gluonnlp.data import BERTSentenceTransform\n\nclass BERTDatasetTransform:\n    """"""Dataset transformation for BERT-style sentence classification or regression.\n\n    Parameters\n    ----------\n    tokenizer : BERTTokenizer.\n        Tokenizer for the sentences.\n    max_seq_length : int.\n        Maximum sequence length of the sentences.\n    vocab : Vocab or BERTVocab\n        The vocabulary.\n    labels : list of int , float or None. defaults None\n        List of all label ids for the classification task and regressing task.\n        If labels is None, the default task is regression\n    pad : bool, default True\n        Whether to pad the sentences to maximum length.\n    pair : bool, default True\n        Whether to transform sentences or sentence pairs.\n    label_dtype: int32 or float32, default float32\n        label_dtype = int32 for classification task\n        label_dtype = float32 for regression task\n    """"""\n\n    def __init__(self,\n                 tokenizer,\n                 max_seq_length,\n                 vocab=None,\n                 class_labels=None,\n                 label_alias=None,\n                 pad=True,\n                 pair=True,\n                 has_label=True):\n        self.class_labels = class_labels\n        self.has_label = has_label\n        self._label_dtype = \'int32\' if class_labels else \'float32\'\n        self.vocab = vocab\n        if has_label and class_labels:\n            self._label_map = {}\n            for (i, label) in enumerate(class_labels):\n                self._label_map[label] = i\n            if label_alias:\n                for key in label_alias:\n                    self._label_map[key] = self._label_map[label_alias[key]]\n        self._bert_xform = BERTSentenceTransform(\n            tokenizer, max_seq_length, pad=pad, pair=pair)\n\n    def __call__(self, line):\n        """"""Perform transformation for sequence pairs or single sequences.\n\n        The transformation is processed in the following steps:\n        - tokenize the input sequences\n        - insert [CLS], [SEP] as necessary\n        - generate type ids to indicate whether a token belongs to the first\n          sequence or the second sequence.\n        - generate valid length\n\n        For sequence pairs, the input is a tuple of 3 strings:\n        text_a, text_b and label.\n\n        Inputs:\n            text_a: \'is this jacksonville ?\'\n            text_b: \'no it is not\'\n            label: \'0\'\n        Tokenization:\n            text_a: \'is this jack ##son ##ville ?\'\n            text_b: \'no it is not .\'\n        Processed:\n            tokens:  \'[CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\'\n            type_ids: 0     0  0    0    0     0       0 0     1  1  1  1   1 1\n            valid_length: 14\n            label: 0\n\n        For single sequences, the input is a tuple of 2 strings: text_a and label.\n        Inputs:\n            text_a: \'the dog is hairy .\'\n            label: \'1\'\n        Tokenization:\n            text_a: \'the dog is hairy .\'\n        Processed:\n            text_a:  \'[CLS] the dog is hairy . [SEP]\'\n            type_ids: 0     0   0   0  0     0 0\n            valid_length: 7\n            label: 1\n\n        Parameters\n        ----------\n        line: tuple of str\n            Input strings. For sequence pairs, the input is a tuple of 3 strings:\n            (text_a, text_b, label). For single sequences, the input is a tuple\n            of 2 strings: (text_a, label).\n\n        Returns\n        -------\n        np.array: input token ids in \'int32\', shape (batch_size, seq_length)\n        np.array: valid length in \'int32\', shape (batch_size,)\n        np.array: input token type ids in \'int32\', shape (batch_size, seq_length)\n        np.array: classification task: label id in \'int32\', shape (batch_size, 1),\n            regression task: label in \'float32\', shape (batch_size, 1)\n        """"""\n        if self.has_label:\n            input_ids, valid_length, segment_ids = self._bert_xform(line[:-1])\n            label = line[-1]\n            # map to int if class labels are available\n            if self.class_labels:\n                label = self._label_map[label]\n            label = np.array([label], dtype=self._label_dtype)\n            return input_ids, valid_length, segment_ids, label\n        else:\n            return self._bert_xform(line)\n'"
autogluon/utils/tabular/__init__.py,0,"b""import logging\n\nDEFAULT_LOGGING_LEVEL = 20\n\nlogging.basicConfig(format='%(message)s') # just print message in logs\nlogger = logging.getLogger() # root logger\nlogger.setLevel(DEFAULT_LOGGING_LEVEL)"""
tests/unittests/bayesopt/test_autogluon_hp_ranges.py,0,"b""# TODO: This code is comparing HyperparameterRanges_CS with HyperparameterRanges.\n# If the latter code is removed, this test can go as well.\n\nimport numpy as np\nimport ConfigSpace as CS\nimport ConfigSpace.hyperparameters as CSH\nfrom numpy.testing import assert_allclose\n\nfrom autogluon.searcher.bayesopt.autogluon.hp_ranges import \\\n    HyperparameterRanges_CS\nfrom autogluon.searcher.bayesopt.datatypes.hp_ranges import \\\n    HyperparameterRanges_Impl, HyperparameterRangeCategorical, \\\n    HyperparameterRangeContinuous, HyperparameterRangeInteger\nfrom autogluon.searcher.bayesopt.datatypes.scaling import LinearScaling, \\\n    LogScaling\n\n\ndef test_to_ndarray():\n    np.random.seed(123456)\n    random_state = np.random.RandomState(123456)\n    prob_categ = 0.3\n\n    for iter in range(20):\n        # Create ConfigurationSpace\n        num_hps = np.random.randint(low=1, high=20)\n        if iter == 0:\n            _prob_categ = 0.\n        elif iter == 1:\n            _prob_categ = 1.\n        else:\n            _prob_categ = prob_categ\n        config_space = CS.ConfigurationSpace()\n        ndarray_size = 0\n        _hp_ranges = dict()\n        for hp_it in range(num_hps):\n            name = str(hp_it)\n            if np.random.random() < _prob_categ:\n                num_choices = np.random.randint(low=2, high=11)\n                choices = tuple([str(i) for i in range(num_choices)])\n                hp = CSH.CategoricalHyperparameter(name, choices=choices)\n                hp2 = HyperparameterRangeCategorical(name, choices)\n                ndarray_size += num_choices\n            else:\n                ndarray_size += 1\n                rand_coin = np.random.random()\n                if rand_coin < 0.5:\n                    log_scaling = (rand_coin < 0.25)\n                    hp = CSH.UniformFloatHyperparameter(\n                        name=name, lower=0.5, upper=5., log=log_scaling)\n                    hp2 = HyperparameterRangeContinuous(\n                        name, lower_bound=0.5, upper_bound=5.,\n                        scaling=LogScaling() if log_scaling else LinearScaling())\n                else:\n                    log_scaling = (rand_coin < 0.75)\n                    hp = CSH.UniformIntegerHyperparameter(\n                        name=name, lower=2, upper=10, log=log_scaling)\n                    hp2 = HyperparameterRangeInteger(\n                        name=name, lower_bound=2, upper_bound=10,\n                        scaling=LogScaling() if log_scaling else LinearScaling())\n            config_space.add_hyperparameter(hp)\n            _hp_ranges[name] = hp2\n        hp_ranges_cs = HyperparameterRanges_CS(config_space)\n        hp_ranges = HyperparameterRanges_Impl(\n            *[_hp_ranges[x] for x in config_space.get_hyperparameter_names()])\n        # Compare ndarrays created by both codes\n        for cmp_it in range(5):\n            config_cs = hp_ranges_cs.random_candidate(random_state)\n            _config = config_cs.get_dictionary()\n            config = (_config[name]\n                      for name in config_space.get_hyperparameter_names())\n            ndarr_cs = hp_ranges_cs.to_ndarray(config_cs)\n            ndarr = hp_ranges.to_ndarray(config)\n            assert_allclose(ndarr_cs, ndarr, rtol=1e-4)\n\n\ndef test_to_ndarray_name_last_pos():\n    np.random.seed(123456)\n    random_state = np.random.RandomState(123456)\n\n    config_space = CS.ConfigurationSpace()\n    config_space.add_hyperparameters([\n        CSH.UniformFloatHyperparameter('a', lower=0., upper=1.),\n        CSH.UniformIntegerHyperparameter('b', lower=2, upper=3),\n        CSH.CategoricalHyperparameter('c', choices=('1', '2', '3')),\n        CSH.UniformIntegerHyperparameter('d', lower=2, upper=3),\n        CSH.CategoricalHyperparameter('e', choices=('1', '2'))])\n    hp_a = HyperparameterRangeContinuous(\n        'a', lower_bound=0., upper_bound=1., scaling=LinearScaling())\n    hp_b = HyperparameterRangeInteger(\n        'b', lower_bound=2, upper_bound=3, scaling=LinearScaling())\n    hp_c = HyperparameterRangeCategorical('c', choices=('1', '2', '3'))\n    hp_d = HyperparameterRangeInteger(\n        'd', lower_bound=2, upper_bound=3, scaling=LinearScaling())\n    hp_e = HyperparameterRangeCategorical('e', choices=('1', '2'))\n\n    for name_last_pos in ['a', 'c', 'd', 'e']:\n        hp_ranges_cs = HyperparameterRanges_CS(\n            config_space, name_last_pos=name_last_pos)\n        if name_last_pos == 'a':\n            lst = [hp_b, hp_c, hp_d, hp_e, hp_a]\n        elif name_last_pos == 'c':\n            lst = [hp_a, hp_b, hp_d, hp_e, hp_c]\n        elif name_last_pos == 'd':\n            lst = [hp_a, hp_b, hp_c, hp_e, hp_d]\n        else:\n            lst = [hp_a, hp_b, hp_c, hp_d, hp_e]\n        hp_ranges = HyperparameterRanges_Impl(*lst)\n        names = [hp.name for hp in hp_ranges.hp_ranges]\n        config_cs = hp_ranges_cs.random_candidate(random_state)\n        _config = config_cs.get_dictionary()\n        config = (_config[name] for name in names)\n        ndarr_cs = hp_ranges_cs.to_ndarray(config_cs)\n        ndarr = hp_ranges.to_ndarray(config)\n        assert_allclose(ndarr_cs, ndarr, rtol=1e-4)\n"""
tests/unittests/bayesopt/test_bo_algorithm_components.py,0,"b""import pytest\n\nfrom autogluon.searcher.bayesopt.datatypes.hp_ranges import \\\n    HyperparameterRangeContinuous, HyperparameterRangeInteger, \\\n    HyperparameterRangeCategorical, HyperparameterRanges_Impl\nfrom autogluon.searcher.bayesopt.datatypes.scaling import LinearScaling\nfrom autogluon.searcher.bayesopt.datatypes.tuning_job_state import \\\n    TuningJobState\nfrom autogluon.searcher.bayesopt.datatypes.common import CandidateEvaluation\n\n\n@pytest.fixture(scope='function')\ndef tuning_job_state():\n    return {'algo-1': TuningJobState(\n        hp_ranges=HyperparameterRanges_Impl(\n            HyperparameterRangeContinuous('a1_hp_1', -5.0, 5.0, LinearScaling()),\n            HyperparameterRangeCategorical('a1_hp_2', ('a', 'b', 'c'))),\n        candidate_evaluations=[CandidateEvaluation(candidate=(-3.0, 'a'), value=1.0),\n                               CandidateEvaluation(candidate=(-1.9, 'c'), value=2.0),\n                               CandidateEvaluation(candidate=(-3.5, 'a'), value=0.3)],\n        failed_candidates=[],\n        pending_evaluations=[]\n    ),\n        'algo-2': TuningJobState(\n            hp_ranges=HyperparameterRanges_Impl(\n                HyperparameterRangeContinuous('a2_hp_1', -5.0, 5.0, LinearScaling()),\n                HyperparameterRangeInteger('a2_hp_2', -5, 5, LinearScaling(), -5, 5)),\n            candidate_evaluations=[CandidateEvaluation(candidate=(-1.9, -1), value=0.0),\n                                   CandidateEvaluation(candidate=(-3.5, 3), value=2.0)],\n            failed_candidates=[],\n            pending_evaluations=[]\n        )\n    }\n\n\n@pytest.fixture(scope='function')\ndef tuning_job_sub_state():\n    return TuningJobState(\n        hp_ranges=HyperparameterRanges_Impl(),\n        candidate_evaluations=[],\n        failed_candidates=[],\n        pending_evaluations=[])\n"""
tests/unittests/bayesopt/test_bo_algorithm_functions.py,0,"b""from typing import Iterable, List, Optional\nimport pytest\n\nfrom autogluon.searcher.bayesopt.datatypes.hp_ranges import \\\n    HyperparameterRanges_Impl, HyperparameterRangeContinuous\nfrom autogluon.searcher.bayesopt.datatypes.scaling import LinearScaling\nfrom autogluon.searcher.bayesopt.tuning_algorithms.base_classes import \\\n    ScoringFunction, SurrogateModel\nfrom autogluon.searcher.bayesopt.tuning_algorithms.bo_algorithm_components import \\\n    NoOptimization\nfrom autogluon.searcher.bayesopt.tuning_algorithms.bo_algorithm import \\\n    _pick_from_locally_optimized, _lazily_locally_optimize, _order_candidates\nfrom autogluon.searcher.bayesopt.datatypes.common import StateIdAndCandidate\nfrom autogluon.searcher.bayesopt.utils.duplicate_detector import \\\n    DuplicateDetectorIdentical, DuplicateDetectorEpsilon\n\n\ndef test_pick_from_locally_optimized():\n    duplicate_detector1 = DuplicateDetectorIdentical()\n    duplicate_detector2 = DuplicateDetectorEpsilon(\n        hp_ranges=HyperparameterRanges_Impl(\n            HyperparameterRangeContinuous('hp1', -10.0, 10.0, scaling=LinearScaling()),\n            HyperparameterRangeContinuous('hp2', -10.0, 10.0, scaling=LinearScaling()),\n        )\n    )\n    for duplicate_detector in (duplicate_detector1, duplicate_detector2):\n\n        got = _pick_from_locally_optimized(\n            candidates_with_optimization=[\n                # original,   optimized\n                ((0.1, 1.0), (0.1, 1.0)),\n                ((0.1, 1.0), (0.6, 1.0)),  # not a duplicate\n                ((0.2, 1.0), (0.1, 1.0)),  # duplicate optimized; Resolved by the original\n                ((0.1, 1.0), (0.1, 1.0)),  # complete duplicate\n                ((0.3, 1.0), (0.1, 1.0)),  # blacklisted original\n                ((0.4, 3.0), (0.3, 1.0)),  # blacklisted all\n                ((1.0, 2.0), (1.0, 1.0)),  # final candidate to be selected into a batch\n                ((0.0, 2.0), (1.0, 0.0)),  # skipped\n                ((0.0, 2.0), (1.0, 0.0)),  # skipped\n            ],\n            blacklisted_candidates={\n                (0.3, 1.0),\n                (0.4, 3.0),\n                (0.0, 0.0),  # blacklisted candidate, not present in candidates\n            },\n            num_candidates=4,\n            duplicate_detector=duplicate_detector,\n        )\n\n        expected = [\n            (0.1, 1.0),\n            (0.6, 1.0),\n            (0.2, 1.0),\n            (1.0, 1.0)\n        ]\n\n        # order of the candidates should be preserved\n        assert len(expected) == len(got)\n        assert all(a == b for a, b in zip(got, expected))\n\n\ndef test_lazily_locally_optimize():\n    original_candidates = [\n        (1.0, 'a', 3, 'b'),\n        (2.0, 'c', 2, 'a'),\n        (0.0, 'd', 0, 'd')\n    ]\n\n    # NoOptimization class is used to check the interfaces only in here\n    i = 0\n    for candidate in  _lazily_locally_optimize(\n            original_candidates, NoOptimization(None, None, None), model=None):\n        # no optimization is applied ot the candidates\n        assert candidate[0] == original_candidates[i]\n        assert candidate[1] == original_candidates[i]\n        i += 1\n\n    assert i == len(original_candidates)\n    assert len(list(_lazily_locally_optimize(\n        [], NoOptimization(None, None, None), model=None))) == 0\n\n\n@pytest.mark.parametrize('example,expected', [\n    (\n        {\n            (1.0, 'a'),\n            (2.0, 'a'),\n            (1.0, 'b'),\n            (3.0, 'a'),\n            (2.0, 'b'),\n            (3.0, 'b')\n        },\n        (1.0, 1.0, 2.0, 2.0, 3.0, 3.0)\n    ),\n    (\n        {\n            (1.0, 'a', 'b'),\n            (1.0, 'c', 'b'),\n            (3.0, 'a', 'd'),\n            (0.0, 'e', 'b')\n        },\n        (0.0, 1.0, 1.0, 3.0)\n    ),\n    (\n        {\n            ('ba',),\n            ('b',),\n            ('a',),\n        },\n        ('a', 'b', 'ba')\n    ),\n])\ndef test_order_candidates(example, expected):\n    # Scorer orders by the first dimension\n    class MyScorer(ScoringFunction):\n        def score(self, candidates: Iterable[StateIdAndCandidate],\n                  model: Optional[SurrogateModel] = None) -> List[float]:\n            return [candidate[0] for candidate in candidates]\n\n    # very important condition: ordering is deterministic.\n    # if not, it breaks fixing random seed in a very sinister way.\n    # fixes a bug with random seed, introduced by arbitrary order of set().\n    example = list(example)\n    A = tuple(_order_candidates(example, MyScorer(), None))\n    B = tuple(_order_candidates(example, MyScorer(), None))\n    C = tuple(_order_candidates(example, MyScorer(), None))\n\n    assert A == B\n    assert A == C\n    assert expected == tuple(r[0] for r in A)\n"""
tests/unittests/bayesopt/test_checkpointing.py,0,"b'import numpy as np\nimport pickle\n\nfrom autogluon.searcher.bayesopt.autogluon.searcher_factory import \\\n    gp_fifo_searcher_factory, gp_fifo_searcher_defaults\nfrom autogluon.searcher.bayesopt.gpmxnet.comparison_gpy import Ackley, \\\n    sample_data, assert_equal_candidates, assert_equal_randomstate\nfrom autogluon.searcher.bayesopt.tuning_algorithms.default_algorithm import \\\n    DEFAULT_METRIC\n\n\ndef test_pickle_gp_fifo_searcher():\n    random_seed = 894623209\n    # This data is used below\n    _, searcher_options, _ = gp_fifo_searcher_defaults()\n    num_data = searcher_options[\'num_init_random\'] + 2\n    num_pending = 2\n    data = sample_data(Ackley, num_train=num_data + num_pending, num_grid=5)\n    # Create searcher1 using default arguments\n    searcher_options[\'configspace\'] = data[\'state\'].hp_ranges.config_space\n    searcher_options[\'scheduler\'] = \'fifo\'\n    searcher_options[\'random_seed\'] = random_seed\n    searcher1 = gp_fifo_searcher_factory(**searcher_options)\n    # Feed searcher1 with some data\n    for eval in data[\'state\'].candidate_evaluations[:num_data]:\n        reward = searcher1.map_reward.reverse(eval.metrics[DEFAULT_METRIC])\n        searcher1.update(eval.candidate, reward)\n    # Calling next_config is forcing a GP hyperparameter update\n    next_config = searcher1.get_config()\n    # Register some pending evaluations\n    for eval in data[\'state\'].candidate_evaluations[-num_pending:]:\n        searcher1.register_pending(eval.candidate)\n    # Pickle mutable state of searcher1\n    pkl_state = pickle.dumps(searcher1.get_state())\n    # Clone searcher2 from mutable state\n    searcher2 = gp_fifo_searcher_factory(**searcher_options)\n    searcher2 = searcher2.clone_from_state(pickle.loads(pkl_state))\n    # At this point, searcher1 and searcher2 should be essentially the same\n    # Compare model parameters\n    params1 = searcher1.get_params()\n    params2 = searcher2.get_params()\n    for k, v1 in params1.items():\n        v2 = params2[k]\n        np.testing.assert_almost_equal(\n            np.array([v1]), np.array([v2]), decimal=4)\n    # Compare states\n    state1 = searcher1.state_transformer.state\n    state2 = searcher2.state_transformer.state\n    hp_ranges = state1.hp_ranges\n    assert_equal_candidates(\n        [x.candidate for x in state1.candidate_evaluations],\n        [x.candidate for x in state2.candidate_evaluations], hp_ranges,\n        decimal=5)\n    eval_targets1 = np.array([\n        x.metrics[DEFAULT_METRIC] for x in state1.candidate_evaluations])\n    eval_targets2 = np.array([\n        x.metrics[DEFAULT_METRIC] for x in state1.candidate_evaluations])\n    np.testing.assert_almost_equal(eval_targets1, eval_targets2, decimal=5)\n    assert_equal_candidates(\n        state1.pending_candidates, state2.pending_candidates, hp_ranges,\n        decimal=5)\n    # Compare random_state, random_generator state\n    assert_equal_randomstate(searcher1.random_state, searcher2.random_state)\n\n\nif __name__ == ""__main__"":\n    test_pickle_gp_fifo_searcher()\n'"
tests/unittests/bayesopt/test_common.py,0,"b""from typing import List, Set\nimport pytest\n\nfrom autogluon.searcher.bayesopt.datatypes.common import Candidate, \\\n    CandidateEvaluation, PendingEvaluation\nfrom autogluon.searcher.bayesopt.datatypes.hp_ranges import \\\n    HyperparameterRanges_Impl, HyperparameterRangeInteger, \\\n    HyperparameterRangeCategorical, HyperparameterRangeContinuous\nfrom autogluon.searcher.bayesopt.datatypes.scaling import LinearScaling\nfrom autogluon.searcher.bayesopt.datatypes.tuning_job_state import \\\n    TuningJobState\nfrom autogluon.searcher.bayesopt.tuning_algorithms.common import \\\n    compute_blacklisted_candidates, generate_unique_candidates, \\\n    RandomCandidateGenerator\nfrom autogluon.searcher.bayesopt.tuning_algorithms.default_algorithm import \\\n    dictionarize_objective\nfrom autogluon.searcher.bayesopt.utils.test_objects import \\\n    RepeatedCandidateGenerator\n\n\n@pytest.fixture(scope='function')\ndef hp_ranges():\n    return HyperparameterRanges_Impl(\n        HyperparameterRangeInteger('hp1', 0, 200, LinearScaling()),\n        HyperparameterRangeCategorical('hp2', ('a', 'b', 'c')))\n\n\n@pytest.fixture(scope='function')\ndef multi_algo_state():\n    def _candidate_evaluations(num):\n        return [\n            CandidateEvaluation(\n                candidate=(i,),\n                metrics=dictionarize_objective(float(i)))\n            for i in range(num)]\n\n    return {\n        '0': TuningJobState(\n            hp_ranges=HyperparameterRanges_Impl(\n                HyperparameterRangeContinuous('a1_hp_1', -5.0, 5.0, LinearScaling(), -5.0, 5.0)),\n            candidate_evaluations=_candidate_evaluations(2),\n            failed_candidates=[(i,) for i in range(3)],\n            pending_evaluations=[PendingEvaluation((i,)) for i in range(100)]\n        ),\n        '1': TuningJobState(\n            hp_ranges=HyperparameterRanges_Impl(),\n            candidate_evaluations=_candidate_evaluations(5),\n            failed_candidates=[],\n            pending_evaluations=[]\n        ),\n        '2': TuningJobState(\n            hp_ranges=HyperparameterRanges_Impl(),\n            candidate_evaluations=_candidate_evaluations(3),\n            failed_candidates=[(i,) for i in range(10)],\n            pending_evaluations=[PendingEvaluation((i,)) for i in range(1)]\n        ),\n        '3': TuningJobState(\n            hp_ranges=HyperparameterRanges_Impl(),\n            candidate_evaluations=_candidate_evaluations(6),\n            failed_candidates=[],\n            pending_evaluations=[]\n        ),\n        '4': TuningJobState(\n            hp_ranges=HyperparameterRanges_Impl(),\n            candidate_evaluations=_candidate_evaluations(120),\n            failed_candidates=[],\n            pending_evaluations=[]\n        ),\n    }\n\n\n@pytest.mark.parametrize('candidate_evaluations,failed_candidates,pending_evaluations,expected', [\n    ([], [], [], set()),\n    ([CandidateEvaluation((123, 'a'), {'': 9.87})], [], [], {(123, 'a')}),\n    ([], [(123, 'a')], [], {(123, 'a')}),\n    ([], [], [PendingEvaluation((123, 'a'))], {(123, 'a')}),\n    ([CandidateEvaluation((1, 'a'), {'': 9.87})], [(2, 'b')],\n     [PendingEvaluation((3, 'c'))], {(1, 'a'), (2, 'b'), (3, 'c')})\n])\ndef test_compute_blacklisted_candidates(\n        hp_ranges: HyperparameterRanges_Impl,\n        candidate_evaluations: List[CandidateEvaluation],\n        failed_candidates: List[Candidate],\n        pending_evaluations: List[PendingEvaluation],\n        expected: Set[Candidate]):\n    state = TuningJobState(\n        hp_ranges, candidate_evaluations,\n        failed_candidates, pending_evaluations\n    )\n    actual = compute_blacklisted_candidates(state)\n    assert set(expected) == set(actual)\n\n\n@pytest.mark.parametrize('num_unique_candidates,num_requested_candidates', [\n    (5, 10),\n    (15, 10)\n])\ndef test_generate_unique_candidates(num_unique_candidates, num_requested_candidates):\n    candidates = generate_unique_candidates(RepeatedCandidateGenerator(num_unique_candidates),\n                                            num_requested_candidates, set())\n    assert len(candidates) == min(num_unique_candidates, num_requested_candidates)\n    assert len(candidates) == len(set(candidates)) # make sure they are unique\n\n    # introduce excluded candidates, simply take a few already unique\n    size_excluded = len(candidates) // 2\n    excluded = list(candidates)[:size_excluded]\n    excluded = set(excluded)\n\n    candidates = generate_unique_candidates(RepeatedCandidateGenerator(num_unique_candidates),\n                                            num_requested_candidates, excluded)\n\n    # total unique candidates are adjusted by the number of excluded candidates which are unique too due to set()\n    assert len(candidates) == min(num_unique_candidates - len(excluded), num_requested_candidates)\n    assert len(candidates) == len(set(candidates)) # make sure they are unique\n\n\ndef test_generate_unique_candidates_fixed_choice(multi_algo_state):\n    random_generator = RandomCandidateGenerator(\n        multi_algo_state['0'].hp_ranges, 0)\n    candidates = []\n\n    for candidate in random_generator.generate_candidates():\n        candidates.append(candidate)\n        if len(candidates) > 9:\n            break\n\n    random_generator = RandomCandidateGenerator(\n        multi_algo_state['0'].hp_ranges, 0)\n    candidates_2 = generate_unique_candidates(random_generator, 10, set())\n    assert candidates == candidates_2\n"""
tests/unittests/bayesopt/test_duplicate_detector.py,0,"b""import pytest\n\nfrom autogluon.searcher.bayesopt.datatypes.hp_ranges import \\\n    HyperparameterRanges_Impl, HyperparameterRangeInteger, \\\n    HyperparameterRangeContinuous, HyperparameterRangeCategorical\nfrom autogluon.searcher.bayesopt.datatypes.scaling import LinearScaling\nfrom autogluon.searcher.bayesopt.utils.duplicate_detector import \\\n    DuplicateDetectorEpsilon, DuplicateDetectorIdentical, \\\n    DuplicateDetectorNoDetection\n\n\nhp_ranges = HyperparameterRanges_Impl(\n    HyperparameterRangeInteger('hp1', 0, 1000000000, scaling=LinearScaling()),\n    HyperparameterRangeContinuous('hp2', -10.0, 10.0, scaling=LinearScaling()),\n    HyperparameterRangeCategorical('hp3', ('a', 'b', 'c')),\n)\n\nduplicate_detector_epsilon = DuplicateDetectorEpsilon(hp_ranges)\n\n\n@pytest.mark.parametrize('existing, new, contained', [\n    ({(10, 1.0, 'a'), (20, 2.0, 'b')}, (10000, 3.0, 'c'), False),\n    ({(10, 1.0, 'a'), (20, 2.0, 'b')}, (10, 1.000001, 'a'), False),\n    ({(10, 1.0, 'a'), (20, 2.0, 'b')}, (20, 2.000001, 'b'), False),\n    ({(10, 1.0, 'a'), (20, 2.0, 'b')}, (25, 1.0, 'a'), False),\n    ({(10, 1.0, 'a'), (20, 2.0, 'b')}, (10, 1.0, 'a'), True),\n    ({(10, 1.0, 'a'), (20, 2.0, 'b')}, (20, 2.0, 'b'), True),\n    ({(10, 1.0, 'a'), (20, 2.0, 'b')}, (19, 1.0, 'a'), True),\n    ({(10, 1.0, 'a'), (20, 2.0, 'b')}, (10, 1.0000001, 'a'), True),\n    ({(10, 1.0, 'a'), (20, 2.0, 'b')}, (10, 1.0, 'c'), False),\n    ({(10, 1.0, 'a'), (20, 2.0, 'b')}, (10, 1.0, 'b'), False),\n    ({(10, 1.0, 'a'), (20, 2.0, 'b')}, (20, 1.0, 'b'), False),\n])\ndef test_contains_epsilon(existing, new, contained):\n    assert duplicate_detector_epsilon.contains(existing, new) == contained\n\n\n@pytest.mark.parametrize('existing, new, contained', [\n    ({(10, 1.0, 'a'), (20, 2.0, 'b')}, (10000, 3.0, 'c'), False),\n    ({(10, 1.0, 'a'), (20, 2.0, 'b')}, (10, 1.000001, 'a'), False),\n    ({(10, 1.0, 'a'), (20, 2.0, 'b')}, (20, 2.000001, 'b'), False),\n    ({(10, 1.0, 'a'), (20, 2.0, 'b')}, (25, 1.0, 'a'), False),\n    ({(10, 1.0, 'a'), (20, 2.0, 'b')}, (10, 1.0, 'a'), True),\n    ({(10, 1.0, 'a'), (20, 2.0, 'b')}, (20, 2.0, 'b'), True),\n    ({(10, 1.0, 'a'), (20, 2.0, 'b')}, (19, 1.0, 'a'), False),\n    ({(10, 1.0, 'a'), (20, 2.0, 'b')}, (10, 1.0000001, 'a'), False),\n    ({(10, 1.0, 'a'), (20, 2.0, 'b')}, (10, 1.0, 'c'), False),\n    ({(10, 1.0, 'a'), (20, 2.0, 'b')}, (10, 1.0, 'b'), False),\n    ({(10, 1.0, 'a'), (20, 2.0, 'b')}, (20, 1.0, 'b'), False),\n])\ndef test_contains_identical(existing, new, contained):\n    assert DuplicateDetectorIdentical().contains(existing, new) == contained\n\n\n@pytest.mark.parametrize('existing, new', [\n    ({(10, 1.0, 'a'), (20, 2.0, 'b')}, (10000, 3.0, 'c')),\n    ({(10, 1.0, 'a'), (20, 2.0, 'b')}, (10, 1.000001, 'a')),\n    ({(10, 1.0, 'a'), (20, 2.0, 'b')}, (20, 2.000001, 'b')),\n    ({(10, 1.0, 'a'), (20, 2.0, 'b')}, (25, 1.0, 'a')),\n    ({(10, 1.0, 'a'), (20, 2.0, 'b')}, (10, 1.0, 'a')),\n    ({(10, 1.0, 'a'), (20, 2.0, 'b')}, (20, 2.0, 'b')),\n    ({(10, 1.0, 'a'), (20, 2.0, 'b')}, (19, 1.0, 'a')),\n    ({(10, 1.0, 'a'), (20, 2.0, 'b')}, (10, 1.0000001, 'a')),\n    ({(10, 1.0, 'a'), (20, 2.0, 'b')}, (10, 1.0, 'c')),\n    ({(10, 1.0, 'a'), (20, 2.0, 'b')}, (10, 1.0, 'b')),\n    ({(10, 1.0, 'a'), (20, 2.0, 'b')}, (20, 1.0, 'b')),\n])\ndef test_contains_no_detection(existing, new):\n    assert not DuplicateDetectorNoDetection().contains(existing, new)\n"""
tests/unittests/bayesopt/test_ei_mxnet.py,0,"b'from typing import List\nimport numpy as np\n\nfrom autogluon.searcher.bayesopt.datatypes.common import CandidateEvaluation\nfrom autogluon.searcher.bayesopt.datatypes.hp_ranges import \\\n    HyperparameterRanges_Impl, HyperparameterRangeContinuous\nfrom autogluon.searcher.bayesopt.datatypes.scaling import LinearScaling\nfrom autogluon.searcher.bayesopt.datatypes.tuning_job_state import \\\n    TuningJobState\nfrom autogluon.searcher.bayesopt.models.nphead_acqfunc import \\\n    EIAcquisitionFunction\nfrom autogluon.searcher.bayesopt.models.gpmxnet import GPMXNetModel, \\\n    default_gpmodel, default_gpmodel_mcmc\nfrom autogluon.searcher.bayesopt.tuning_algorithms.bo_algorithm_components import \\\n    LBFGSOptimizeAcquisition\nfrom autogluon.searcher.bayesopt.tuning_algorithms.default_algorithm import \\\n    dictionarize_objective, DEFAULT_METRIC\nfrom autogluon.searcher.bayesopt.gpmxnet.constants import DEFAULT_MCMC_CONFIG, \\\n    DEFAULT_OPTIMIZATION_CONFIG\n\n\n# This setup makes little sense for good testing.\n#\n# When default model for no MCMC is plotted:\n# - Plot on [0, 1]^2:\n#   - Mean essentially constant at 10, stddev essentially constant at 2.5\n#   - EI essentially constant at -0.12717145\n# - Plot on [0, 0.1]^2:\n#   - Mean = 10, except dropping in corner, stddev = 2.5\n#   - EI essentially constant, dropping in corner\n# - Plot on [0, 0.01]^2:\n#   - Mean growing 0 -> 8, sttdev = 2.5, but drops to 0 in corner\n#   - EI from -0.66 to -0.12, -> 0 only very close to origin\n# - Plot on [0, 0.001]^2:\n#   - Mean growing 0 -> 1.5, stddev growing 0 -> 1.8\n#   - EI about -0.6, but -> 0 close to origin\n# EI is minimized (value -0.66817) very close to origin (order 0.001). Grows to\n# 0 at origin, increases to constant -0.12717145 very rapidly away from origin.\n#\n# In fact, if EI is optimized starting at a point outside [0, 0.1]^2, the optimizer\n# returns with the starting point, and test_optimization_improves fails.\ndef default_models(do_mcmc=True) -> List[GPMXNetModel]:\n    X = [\n        (0.0, 0.0),\n        (1.0, 0.0),\n        (0.0, 1.0),\n        (1.0, 1.0),\n    ]\n    Y = [dictionarize_objective(np.sum(x) * 10.0) for x in X]\n\n    state = TuningJobState(\n        HyperparameterRanges_Impl(\n            HyperparameterRangeContinuous(\'x\', 0.0, 1.0, LinearScaling()),\n            HyperparameterRangeContinuous(\'y\', 0.0, 1.0, LinearScaling()),\n        ),\n        [\n            CandidateEvaluation(x, y) for x, y in zip(X, Y)\n        ],\n        [], []\n    )\n    random_seed = 0\n\n    gpmodel = default_gpmodel(\n        state, random_seed=random_seed,\n        optimization_config=DEFAULT_OPTIMIZATION_CONFIG)\n    result = [GPMXNetModel(state, DEFAULT_METRIC, random_seed, gpmodel,\n                           fit_parameters=True, num_fantasy_samples=20)]\n    if do_mcmc:\n        gpmodel_mcmc = default_gpmodel_mcmc(\n            state, random_seed=random_seed,\n            mcmc_config=DEFAULT_MCMC_CONFIG)\n        result.append(\n            GPMXNetModel(state, DEFAULT_METRIC, random_seed, gpmodel_mcmc,\n                         fit_parameters=True,num_fantasy_samples=20))\n    return result\n\n\ndef plot_ei_mean_std(model, ei, max_grid=1.0):\n    import matplotlib.pyplot as plt\n\n    grid = np.linspace(0, max_grid, 400)\n    Xgrid, Ygrid = np.meshgrid(grid, grid)\n    inputs = np.hstack([Xgrid.reshape(-1, 1), Ygrid.reshape(-1, 1)])\n    Z_ei = ei.compute_acq(inputs)[0]\n    Z_means, Z_vars = model.predict(inputs)[0]\n    Z_std = np.sqrt(Z_vars)\n    titles = [\'EI\', \'mean\', \'std\']\n    for i, (Z, title) in enumerate(zip([Z_ei, Z_means, Z_std], titles)):\n        plt.subplot(1, 3, i + 1)\n        plt.imshow(\n            Z.reshape(Xgrid.shape), extent=[0, max_grid, 0, max_grid],\n            origin=\'lower\')\n        plt.colorbar()\n        plt.title(title)\n    plt.show()\n\n\n# Note: This test fails when run with GP MCMC model. There, acq[5] > acq[7], and acq[8] > acq[5]\n# ==> Need to look into GP MCMC model\ndef test_sanity_check():\n    # - test that values are negative as we should be returning *minus* expected improvement\n    # - test that values that are further from evaluated candidates have higher expected improvement\n    #   given similar mean\n    # - test that points closer to better points have higher expected improvement\n    for model in default_models(do_mcmc=False):\n        ei = EIAcquisitionFunction(model)\n        X = np.array([\n            (0.0, 0.0),  # 0\n            (1.0, 0.0),  # 1\n            (0.0, 1.0),  # 2\n            (1.0, 1.0),  # 3\n            (0.2, 0.0),  # 4\n            (0.0, 0.2),  # 5\n            (0.1, 0.0),  # 6\n            (0.0, 0.1),  # 7\n            (0.1, 0.1),  # 8\n            (0.9, 0.9),  # 9\n        ])\n        _acq = ei.compute_acq(X).flatten()\n        #print(\'Negative EI values:\')\n        #print(_acq)\n        acq = list(_acq)\n\n        assert all(a <= 0 for a in acq), acq\n\n        # lower evaluations should correspond to better acquisition\n        # second inequality is less equal because last two values are likely zero\n        assert acq[0] < acq[1] <= acq[3], acq\n        # Note: The improvement here is tiny, just 0.01%:\n        assert acq[8] < acq[9], acq\n\n        # further from an evaluated point should correspond to better acquisition\n        assert acq[6] < acq[4] < acq[1], acq\n        assert acq[7] < acq[5] < acq[2], acq\n\n\ndef test_best_value():\n    # test that the best value affects expected improvement\n    for model in default_models():\n        ei = EIAcquisitionFunction(model)\n\n        random = np.random.RandomState(42)\n        test_X = random.uniform(low=0.0, high=1.0, size=(10, 2))\n\n        acq_best0 = list(ei.compute_acq(test_X).flatten())\n        acq0_best0 = ei.compute_acq(np.array((0, 0)))\n\n        # override current best\n        def new_current_best():\n            return np.array([10])\n        model.current_best = new_current_best\n\n        acq_best2 = list(ei.compute_acq(test_X).flatten())\n        acq0_best2 = ei.compute_acq(np.array((0, 0)))\n\n        # if the best is only 2 the acquisition function should be better (lower value)\n        assert all(a2 < a0 for a2, a0 in zip(acq_best2, acq_best0))\n\n        # there should be a considerable gap at the point of the best evaluation\n        assert acq0_best2 < acq0_best0 - 1.0\n\n\n# The original version of this test is failing. See comments above.\n# In fact, if EI is optimized from a starting point outside [0, 0.1]^2,\n# the gradient is tiny there, so the optimizer returns with the starting\n# point, and no improvement is made.\n#\n# If the starting point is sampled in [0, 0.1]^2, the test works. The optimum\n# of EI is very close to the origin.\ndef test_optimization_improves():\n    debug_output = False\n    # Pick a random point, optimize and the expected improvement should be better:\n    # But only if the starting point is not too far from the origin\n    random = np.random.RandomState(42)\n    for model in default_models():\n        ei = EIAcquisitionFunction(model)\n        opt = LBFGSOptimizeAcquisition(\n            model.state, model, EIAcquisitionFunction)\n        if debug_output:\n            print(\'\\n\\nGP MCMC\' if model.does_mcmc() else \'GP Opt\')\n            fzero = ei.compute_acq(np.zeros((2,)))[0]\n            print(\'f(0) = {}\'.format(fzero))\n        if debug_output and not model.does_mcmc():\n            print(\'Hyperpars: {}\'.format(model.get_params()))\n            # Plot the thing!\n            plot_ei_mean_std(model, ei, max_grid=0.001)\n            plot_ei_mean_std(model, ei, max_grid=0.01)\n            plot_ei_mean_std(model, ei, max_grid=0.1)\n            plot_ei_mean_std(model, ei, max_grid=1.0)\n\n        non_zero_acq_at_least_once = False\n        for iter in range(10):\n            #initial_point = random.uniform(low=0.0, high=1.0, size=(2,))\n            initial_point = random.uniform(low=0.0, high=0.1, size=(2,))\n            acq0, df0 = ei.compute_acq_with_gradients(initial_point)\n            if debug_output:\n                print(\'\\nInitial point: f(x0) = {}, x0 = {}\'.format(\n                    acq0, initial_point))\n                print(\'grad0 = {}\'.format(df0[0]))\n            if acq0 != 0:\n                non_zero_acq_at_least_once = True\n                optimized = np.array(opt.optimize(tuple(initial_point)))\n                acq_opt = ei.compute_acq(optimized)\n                if debug_output:\n                    print(\'Final point: f(x1) = {}, x1 = {}\'.format(\n                        acq_opt, optimized))\n                assert acq_opt < 0\n                assert acq_opt < acq0\n\n        assert non_zero_acq_at_least_once\n\n# Changes from original version: Half of the time, we sample x in [0, 0.02]^2, where\n# the shape of EI is more interesting\ndef test_numerical_gradient():\n    debug_output = False\n    random = np.random.RandomState(42)\n    eps = 1e-6\n\n    for model in default_models():\n        ei = EIAcquisitionFunction(model)\n\n        for iter in range(10):\n            high = 1.0 if iter < 5 else 0.02\n            x = random.uniform(low=0.0, high=high, size=(2,))\n            f0, analytical_gradient = ei.compute_acq_with_gradients(x)\n            analytical_gradient = analytical_gradient.flatten()\n            f0 = f0[0]\n            if debug_output:\n                print(\'x0 = {}, f(x_0) = {}, grad(x_0) = {}\'.format(\n                    x, f0, analytical_gradient))\n\n            for i in range(2):\n                h = np.zeros_like(x)\n                h[i] = eps\n                fpeps = ei.compute_acq(x+h)\n                fmeps = ei.compute_acq(x-h)\n                numerical_derivative = (fpeps - fmeps) / (2 * eps)\n                if debug_output:\n                    print(\'f(x0+eps) = {}, f(x0-eps) = {}, findiff = {}, deriv = {}\'.format(\n                        fpeps[0], fmeps[0], numerical_derivative[0],\n                        analytical_gradient[i]))\n                np.testing.assert_almost_equal(\n                    numerical_derivative.item(), analytical_gradient[i],\n                    decimal=4)\n\n\ndef test_value_same_as_with_gradient():\n    # test that compute_acq and compute_acq_with_gradients return the same acquisition values\n    for model in default_models():\n        ei = EIAcquisitionFunction(model)\n\n        random = np.random.RandomState(42)\n        X = random.uniform(low=0.0, high=1.0, size=(10, 2))\n\n        # assert same as computation with gradients\n        np.testing.assert_almost_equal(ei.compute_acq(X).flatten(), ei.compute_acq_with_gradients(X)[0].flatten())\n\n\ndef test_value_same_in_batch():\n\n    for model in default_models():\n        ei = EIAcquisitionFunction(model)\n\n        random = np.random.RandomState(42)\n        X = random.uniform(low=0.0, high=1.0, size=(10, 2))\n\n        # assert same as computation with gradients\n        batch_result, batch_gradient = ei.compute_acq_with_gradients(X)\n\n        for i, xi in enumerate(X):\n            assert xi.shape == (2,)\n            assert ei.compute_acq(xi) == batch_result[i]\n            acq, dacq = ei.compute_acq_with_gradients(xi)\n            assert acq <= 0\n            assert acq == batch_result[i]\n            np.testing.assert_array_equal(dacq.flatten(), batch_gradient[i, :])\n\n\nif __name__ == ""__main__"":\n    test_optimization_improves()\n    test_numerical_gradient()\n'"
tests/unittests/bayesopt/test_get_set_params.py,0,"b'import numpy as np\n\nfrom autogluon.searcher.bayesopt.autogluon.searcher_factory import \\\n    gp_multifidelity_searcher_factory, gp_multifidelity_searcher_defaults\nfrom autogluon.searcher.bayesopt.gpmxnet.comparison_gpy import Ackley, \\\n    sample_data\n\n\ndef test_params_gp_multifidelity():\n    # Create GP multifidelity searcher, including a GP surrogate model\n    _, searcher_options, _ = gp_multifidelity_searcher_defaults()\n    searcher_options[\'gp_resource_kernel\'] = \'exp-decay-combined\'\n    # Note: We are lazy here, we just need the config_space\n    data = sample_data(Ackley, num_train=5, num_grid=5)\n    searcher_options[\'configspace\'] = data[\'state\'].hp_ranges.config_space\n    searcher_options[\'scheduler\'] = \'hyperband_stopping\'\n    searcher_options[\'min_reward\'] = 0.\n    searcher_options[\'min_epochs\'] = 1\n    searcher_options[\'max_epochs\'] = 27\n    searcher_options[\'reward_attribute\'] = \'accuracy\'\n    searcher_options[\'resource_attribute\'] = \'epoch\'\n    searcher = gp_multifidelity_searcher_factory(**searcher_options)\n    # Set parameters\n    params = {\n        \'noise_variance\': 0.01,\n        \'kernel_alpha\': 9.0,\n        \'kernel_mean_lam\': 0.25,\n        \'kernel_gamma\': 0.75,\n        \'kernel_delta\': 0.125,\n        \'kernel_kernelx_inv_bw0\': 0.11,\n        \'kernel_kernelx_inv_bw1\': 11.0,\n        \'kernel_kernelx_covariance_scale\': 5.5,\n        \'kernel_meanx_mean_value\': 1e-5}\n    searcher.set_params(params)\n    # Get parameters: Must be the same\n    params2 = searcher.get_params()\n    assert len(params) == len(params2), (params, params2)\n    for k, v in params.items():\n        assert k in params2, (k, params, params2)\n        v2 = params2[k]\n        np.testing.assert_almost_equal(\n            [v], [v2], decimal=6, err_msg=\'key={}\'.format(k))\n\n\nif __name__ == ""__main__"":\n    test_params_gp_multifidelity()\n'"
tests/unittests/bayesopt/test_gpmxnet.py,0,"b'from typing import List\nimport numpy as np\nimport mxnet\nimport pytest\n\nfrom autogluon.searcher.bayesopt.datatypes.scaling import LinearScaling\nfrom autogluon.searcher.bayesopt.datatypes.common import CandidateEvaluation, \\\n    PendingEvaluation\nfrom autogluon.searcher.bayesopt.datatypes.hp_ranges import \\\n    HyperparameterRangeContinuous, HyperparameterRanges_Impl\nfrom autogluon.searcher.bayesopt.datatypes.tuning_job_state import \\\n    TuningJobState\nfrom autogluon.searcher.bayesopt.models.gpmxnet import default_gpmodel, \\\n    default_gpmodel_mcmc, GPMXNetModel\nfrom autogluon.searcher.bayesopt.models.nphead_acqfunc import \\\n    EIAcquisitionFunction\nfrom autogluon.searcher.bayesopt.tuning_algorithms.default_algorithm import \\\n    DEFAULT_METRIC, dictionarize_objective\nfrom autogluon.searcher.bayesopt.gpmxnet.constants import DEFAULT_MCMC_CONFIG, \\\n    DEFAULT_OPTIMIZATION_CONFIG\n\n\n@pytest.fixture(scope=\'function\')\ndef tuning_job_state() -> TuningJobState:\n    X = [\n        (0.0, 0.0),\n        (1.0, 0.0),\n        (0.0, 1.0),\n        (1.0, 1.0),\n    ]\n    Y = [dictionarize_objective(np.sum(x) * 10.0) for x in X]\n\n    return TuningJobState(\n        HyperparameterRanges_Impl(\n            HyperparameterRangeContinuous(\'x\', 0.0, 1.0, LinearScaling()),\n            HyperparameterRangeContinuous(\'y\', 0.0, 1.0, LinearScaling()),\n        ),\n        [\n            CandidateEvaluation(x, y) for x, y in zip(X, Y)\n        ],\n        [], []\n    )\n\n\ndef _set_seeds(seed=0):\n    mxnet.random.seed(seed)\n    np.random.seed(seed)\n\n\ndef test_gp_fit(tuning_job_state):\n    _set_seeds(0)\n    X = [\n        (0.0, 0.0),\n        (1.0, 0.0),\n        (0.0, 1.0),\n        (1.0, 1.0),\n    ]\n    Y = [np.sum(x) * 10.0 for x in X]\n\n    # checks if fitting is running\n    random_seed = 0\n    gpmodel = default_gpmodel(tuning_job_state, random_seed,\n        optimization_config=DEFAULT_OPTIMIZATION_CONFIG)\n    model = GPMXNetModel(\n        tuning_job_state, DEFAULT_METRIC, random_seed, gpmodel,\n        fit_parameters=True, num_fantasy_samples=20)\n\n    X = [tuning_job_state.hp_ranges.to_ndarray(x) for x in X]\n    Y_mean, Y_std = model.predict(np.array(X))[0]\n\n    assert np.all(np.abs(Y_mean - Y) < 1e-1), \\\n        ""in a noiseless setting, mean of GP should coincide closely with outputs at training points""\n\n    X_test = [\n        (0.2, 0.2),\n        (0.4, 0.2),\n        (0.1, 0.9),\n        (0.5, 0.5),\n    ]\n    X_test = [tuning_job_state.hp_ranges.to_ndarray(x) for x in X_test]\n\n    Y_mean_test, Y_std_test = model.predict(np.array(X_test))[0]\n    assert np.min(Y_std) < np.min(Y_std_test), \\\n        ""Standard deviation on un-observed points should be greater than at observed ones""\n\n\ndef test_gp_mcmc_fit(tuning_job_state):\n\n    def tuning_job_state_mcmc(X, Y) -> TuningJobState:\n        Y = [dictionarize_objective(y) for y in Y]\n\n        return TuningJobState(\n            HyperparameterRanges_Impl(HyperparameterRangeContinuous(\'x\', -4., 4., LinearScaling())),\n            [CandidateEvaluation(x, y) for x, y in zip(X, Y)],\n            [], []\n        )\n\n    _set_seeds(0)\n\n    def f(x):\n        return 0.1 * np.power(x, 3)\n\n    X = np.concatenate((np.random.uniform(-4., -1., 10), np.random.uniform(1., 4., 10)))\n    Y = f(X)\n    X_test = np.sort(np.random.uniform(-1., 1., 10))\n\n    X = [(x,)for x in X]\n    X_test = [(x,) for x in X_test]\n\n    tuning_job_state = tuning_job_state_mcmc(X, Y)\n    # checks if fitting is running\n    random_seed = 0\n    gpmodel = default_gpmodel_mcmc(tuning_job_state, random_seed, mcmc_config=DEFAULT_MCMC_CONFIG)\n    model = GPMXNetModel(\n        tuning_job_state, DEFAULT_METRIC, random_seed, gpmodel,\n        fit_parameters=True, num_fantasy_samples=20)\n\n    X = [tuning_job_state.hp_ranges.to_ndarray(x) for x in X]\n    predictions = model.predict(np.array(X))\n\n    Y_std_list = [stds for means, stds in predictions]\n    Y_mean_list = [means for means, stds in predictions]\n    Y_mean = np.mean(Y_mean_list, axis=0)\n    Y_std = np.mean(Y_std_list, axis=0)\n\n    assert np.all(np.abs(Y_mean - Y) < 1e-1), \\\n        ""in a noiseless setting, mean of GP should coincide closely with outputs at training points""\n\n    X_test = [tuning_job_state.hp_ranges.to_ndarray(x) for x in X_test]\n\n    predictions_test = model.predict(np.array(X_test))\n    Y_std_test_list = [stds for means, stds in predictions_test]\n    Y_std_test = np.mean(Y_std_test_list, axis=0)\n    assert np.max(Y_std) < np.min(Y_std_test), \\\n        ""Standard deviation on un-observed points should be greater than at observed ones""\n\n\ndef test_gp_fantasizing():\n    """"""\n    Compare whether acquisition function evaluations (values, gradients) with\n    fantasizing are the same as averaging them by hand.\n    """"""\n    random_seed = 4567\n    _set_seeds(random_seed)\n    num_fantasy_samples = 10\n    num_pending = 5\n\n    hp_ranges = HyperparameterRanges_Impl(\n        HyperparameterRangeContinuous(\'x\', 0.0, 1.0, LinearScaling()),\n        HyperparameterRangeContinuous(\'y\', 0.0, 1.0, LinearScaling()))\n    X = [\n        (0.0, 0.0),\n        (1.0, 0.0),\n        (0.0, 1.0),\n        (1.0, 1.0),\n    ]\n    num_data = len(X)\n    Y = [dictionarize_objective(np.random.randn(1, 1)) for _ in range(num_data)]\n    # Draw fantasies. This is done for a number of fixed pending candidates\n    # The model parameters are fit in the first iteration, when there are\n    # no pending candidates\n\n    # Note: It is important to not normalize targets, because this would be\n    # done on the observed targets only, not the fantasized ones, so it\n    # would be hard to compare below.\n    pending_evaluations = []\n    for _ in range(num_pending):\n        pending_cand = tuple(np.random.rand(2,))\n        pending_evaluations.append(PendingEvaluation(pending_cand))\n    state = TuningJobState(\n        hp_ranges,\n        [CandidateEvaluation(x, y) for x, y in zip(X, Y)],\n        failed_candidates=[],\n        pending_evaluations=pending_evaluations)\n    gpmodel = default_gpmodel(\n        state, random_seed, optimization_config=DEFAULT_OPTIMIZATION_CONFIG)\n    model = GPMXNetModel(\n        state, DEFAULT_METRIC, random_seed, gpmodel, fit_parameters=True,\n        num_fantasy_samples=num_fantasy_samples, normalize_targets=False)\n    fantasy_samples = model.fantasy_samples\n    # Evaluate acquisition function and gradients with fantasizing\n    num_test = 50\n    X_test = np.vstack([hp_ranges.to_ndarray(\n        tuple(np.random.rand(2,))) for _ in range(num_test)])\n    acq_func = EIAcquisitionFunction(model)\n    fvals, grads = acq_func.compute_acq_with_gradients(X_test)\n    # Do the same computation by averaging by hand\n    fvals_cmp = np.empty((num_fantasy_samples,) + fvals.shape)\n    grads_cmp = np.empty((num_fantasy_samples,) + grads.shape)\n    X_full = X + state.pending_candidates\n    for it in range(num_fantasy_samples):\n        Y_full = Y + [dictionarize_objective(eval.fantasies[DEFAULT_METRIC][:, it])\n                      for eval in fantasy_samples]\n        state2 = TuningJobState(\n            hp_ranges,\n            [CandidateEvaluation(x, y) for x, y in zip(X_full, Y_full)],\n            failed_candidates=[],\n            pending_evaluations=[])\n        # We have to skip parameter optimization here\n        model2 = GPMXNetModel(\n            state2, DEFAULT_METRIC, random_seed, gpmodel,\n            fit_parameters=False, num_fantasy_samples=num_fantasy_samples,\n            normalize_targets=False)\n        acq_func2 = EIAcquisitionFunction(model2)\n        fvals_, grads_ = acq_func2.compute_acq_with_gradients(X_test)\n        fvals_cmp[it, :] = fvals_\n        grads_cmp[it, :] = grads_\n    # Comparison\n    fvals2 = np.mean(fvals_cmp, axis=0)\n    grads2 = np.mean(grads_cmp, axis=0)\n    assert np.allclose(fvals, fvals2)\n    assert np.allclose(grads, grads2)\n\n\ndef default_models() -> List[GPMXNetModel]:\n    X = [\n        (0.0, 0.0),\n        (1.0, 0.0),\n        (0.0, 1.0),\n        (1.0, 1.0),\n        (0.0, 0.0),  # same evals are added multiple times to force GP to unlearn prior\n        (1.0, 0.0),\n        (0.0, 1.0),\n        (1.0, 1.0),\n        (0.0, 0.0),\n        (1.0, 0.0),\n        (0.0, 1.0),\n        (1.0, 1.0),\n    ]\n    Y = [dictionarize_objective(np.sum(x) * 10.0) for x in X]\n\n    state = TuningJobState(\n        HyperparameterRanges_Impl(\n            HyperparameterRangeContinuous(\'x\', 0.0, 1.0, LinearScaling()),\n            HyperparameterRangeContinuous(\'y\', 0.0, 1.0, LinearScaling()),\n        ),\n        [\n            CandidateEvaluation(x, y) for x, y in zip(X, Y)\n        ],\n        [], [],\n    )\n    random_seed = 0\n\n    gpmodel = default_gpmodel(\n        state, random_seed=random_seed,\n        optimization_config=DEFAULT_OPTIMIZATION_CONFIG\n    )\n\n    gpmodel_mcmc = default_gpmodel_mcmc(\n        state, random_seed=random_seed,\n        mcmc_config=DEFAULT_MCMC_CONFIG\n    )\n\n    return [\n        GPMXNetModel(state, DEFAULT_METRIC, random_seed, gpmodel,\n                     fit_parameters=True, num_fantasy_samples=20),\n        GPMXNetModel(state, DEFAULT_METRIC, random_seed, gpmodel_mcmc,\n                     fit_parameters=True, num_fantasy_samples=20)]\n\n\ndef test_current_best():\n    for model in default_models():\n        current_best = model.current_best().item()\n        print(current_best)\n        assert -0.1 < current_best < 0.1\n'"
tests/unittests/bayesopt/test_gpmxnet_components.py,0,"b'import numpy as np\nimport mxnet as mx\n\nfrom autogluon.searcher.bayesopt.gpmxnet.kernel import Matern52\nfrom autogluon.searcher.bayesopt.gpmxnet.warping import WarpedKernel\nfrom autogluon.searcher.bayesopt.models.gpmxnet import \\\n    get_internal_candidate_evaluations, build_kernel, \\\n    dimensionality_and_warping_ranges\nfrom autogluon.searcher.bayesopt.datatypes.tuning_job_state import \\\n    TuningJobState, CandidateEvaluation\nfrom autogluon.searcher.bayesopt.datatypes.hp_ranges import \\\n    HyperparameterRangeCategorical, HyperparameterRangeInteger, \\\n    HyperparameterRangeContinuous, HyperparameterRanges_Impl\nfrom autogluon.searcher.bayesopt.datatypes.scaling import LinearScaling, \\\n    LogScaling\nfrom autogluon.searcher.bayesopt.tuning_algorithms.default_algorithm import \\\n    dictionarize_objective, DEFAULT_METRIC\nfrom autogluon.searcher.bayesopt.models.mxnet_base import \\\n    _compute_mean_across_samples\nfrom autogluon.searcher.bayesopt.models.nphead_acqfunc import \\\n    _reshape_predictions\n\n\ndef test_get_internal_candidate_evaluations():\n    """"""we do not test the case with no evaluations, as it is assumed\n    that there will be always some evaluations generated in the beginning\n    of the BO loop.""""""\n\n    candidates = [\n        CandidateEvaluation((2, 3.3, \'X\'), dictionarize_objective(5.3)),\n        CandidateEvaluation((1, 9.9, \'Y\'), dictionarize_objective(10.9)),\n        CandidateEvaluation((7, 6.1, \'X\'), dictionarize_objective(13.1)),\n    ]\n\n    state = TuningJobState(\n        hp_ranges=HyperparameterRanges_Impl(\n            HyperparameterRangeInteger(\'integer\', 0, 10, LinearScaling()),\n            HyperparameterRangeContinuous(\'real\', 0, 10, LinearScaling()),\n            HyperparameterRangeCategorical(\'categorical\', (\'X\', \'Y\')),\n        ),\n        candidate_evaluations=candidates,\n        failed_candidates=[candidates[0].candidate],  # these should be ignored by the model\n        pending_evaluations=[]\n    )\n\n    result = get_internal_candidate_evaluations(\n        state, DEFAULT_METRIC, normalize_targets=True,\n        num_fantasize_samples=20)\n\n    assert len(result.X.shape) == 2, ""Input should be a matrix""\n    assert len(result.y.shape) == 2, ""Output should be a matrix""\n\n    assert result.X.shape[0] == len(candidates)\n    assert result.y.shape[-1] == 1, ""Only single output value per row is suppored""\n\n    assert np.abs(np.mean(result.y)) < 1e-8, ""Mean of the normalized outputs is not 0.0""\n    assert np.abs(np.std(result.y) - 1.0) < 1e-8, ""Std. of the normalized outputs is not 1.0""\n\n    np.testing.assert_almost_equal(result.mean, 9.766666666666666)\n    np.testing.assert_almost_equal(result.std, 3.283629428273267)\n\n\ndef test_compute_mean_across_samples():\n    # Assume we predict on 2 test data points\n    # GP without MCMC, no fantasizing, means and stds are all of shape (2,)\n    means = mx.nd.array([1., 0.5])\n    stds = mx.nd.array([0.5, 1.])\n    expected_means = np.array([1., 0.5])\n    prediction_list = [(means, stds)]\n    pred_means = _compute_mean_across_samples(prediction_list)\n    np.testing.assert_equal(expected_means, pred_means.asnumpy())\n\n    # GP with 2 MCMC samples, no fantasizing, means and stds are all of shape (2,)\n    means_2 = mx.nd.array([0.5, 1.])\n    expected_means = np.array([0.75, 0.75])\n    prediction_list = [(means, stds), (means_2, stds)]\n    pred_means = _compute_mean_across_samples(prediction_list)\n    np.testing.assert_equal(expected_means, pred_means.asnumpy())\n\n    # GP with 2 MCMC samples, fantasizing with 3 samples, means has the shape (2, 3)\n    means = mx.nd.array([[1., 0.8, 1.2], [0.5, 0.3, 0.7]])\n    stds = mx.nd.array([0.5, 1.])\n    means_2 = mx.nd.array([[0, 0.6, 0.8], [0.3, 0.5, 0.5]])\n    expected_means = np.array([[0.5, 0.7, 1.0], [0.4, 0.4, 0.6]])\n    prediction_list = [(means, stds), (means_2, stds)]\n    pred_means = _compute_mean_across_samples(prediction_list)\n    np.testing.assert_array_almost_equal(expected_means, pred_means.asnumpy())\n\n\ndef test_reshape_predictions():\n    # without MCMC samples, without fantasizing\n    predictions = [(mx.nd.array([1,]), mx.nd.array([0.1,]))]\n    expected = (mx.nd.array([1,]), mx.nd.array([0.1,]))\n    result = _reshape_predictions(predictions)\n    np.testing.assert_equal(expected[0].asnumpy(), result[0].asnumpy())\n    np.testing.assert_equal(expected[1].asnumpy(), result[1].asnumpy())\n\n    # 3 MCMC samples, without fantasizing\n    predictions = [(mx.nd.array([1,]), mx.nd.array([0.1,])),\n                   (mx.nd.array([2,]), mx.nd.array([0.2,])),\n                   (mx.nd.array([3,]), mx.nd.array([0.3,]))]\n    expected = (mx.nd.array([1,2,3]), mx.nd.array([0.1,0.2,0.3]))\n    result = _reshape_predictions(predictions)\n    np.testing.assert_equal(expected[0].asnumpy(), result[0].asnumpy())\n    np.testing.assert_equal(expected[1].asnumpy(), result[1].asnumpy())\n\n    # 3 MCMC samples, with fantasizing of 2 samples\n    predictions = [(mx.nd.array([1,2]), mx.nd.array([0.1,])),\n                   (mx.nd.array([2,3]), mx.nd.array([0.2,])),\n                   (mx.nd.array([3,4]), mx.nd.array([0.3,]))]\n    expected = (mx.nd.array([1,2,2,3,3,4]), mx.nd.array([0.1,0.1,0.2,0.2,0.3,0.3]))\n    result = _reshape_predictions(predictions)\n    np.testing.assert_equal(expected[0].asnumpy(), result[0].asnumpy())\n    np.testing.assert_equal(expected[1].asnumpy(), result[1].asnumpy())\n\n\ndef test_dimensionality_and_warping_ranges():\n    hp_ranges = HyperparameterRanges_Impl(\n        HyperparameterRangeCategorical(\'categorical1\', (\'X\', \'Y\')),\n        HyperparameterRangeContinuous(\'integer\', 0.1, 10.0, LogScaling()),\n        HyperparameterRangeCategorical(\'categorical2\', (\'a\', \'b\', \'c\')),\n        HyperparameterRangeContinuous(\'real\', 0.0, 10.0, LinearScaling(), 2.5, 5.0),\n        HyperparameterRangeCategorical(\'categorical3\', (\'X\', \'Y\')),\n    )\n\n    dim, warping_ranges = dimensionality_and_warping_ranges(hp_ranges)\n    assert dim == 9\n    assert warping_ranges == {\n        2: (0.0, 1.0),\n        6: (0.0, 1.0)\n    }\n'"
tests/unittests/bayesopt/test_hp_ranges.py,0,"b""# TODO: This code tests HyperparameterRanges and XYZScaling.\n# If the latter code is removed, this test can go as well.\n\nfrom collections import Counter\nimport numpy as np\nimport pytest\nfrom numpy.testing import assert_allclose, assert_almost_equal\nfrom pytest import approx\n\nfrom autogluon.searcher.bayesopt.datatypes.hp_ranges import \\\n    HyperparameterRangeContinuous, HyperparameterRangeInteger, \\\n    HyperparameterRangeCategorical, HyperparameterRanges_Impl\nfrom autogluon.searcher.bayesopt.datatypes.scaling import LinearScaling, \\\n    LogScaling, ReverseLogScaling\n\n\n@pytest.mark.parametrize('lower,upper,external_hp,internal_ndarray,scaling', [\n    (0.0, 8.0, 0.0, 0.0, LinearScaling()),\n    (0.0, 8.0, 8.0, 1.0, LinearScaling()),\n    (0.0, 8.0, 2.0, 0.25, LinearScaling()),\n    (100.2, 100.6, 100.4, 0.5, LinearScaling()),\n    (-2.0, 8.0, 0.0, 0.2, LinearScaling()),\n    (-11.0, -1.0, -10.0, 0.1, LinearScaling()),\n    (1.0, 8.0, 1.0, 0.0, LogScaling()),\n    (1.0, 8.0, 8.0, 1.0, LogScaling()),\n    (1.0, 10000.0, 10.0, 0.25, LogScaling()),\n    (1.0, 10000.0, 100.0, 0.5, LogScaling()),\n    (1.0, 10000.0, 1000.0, 0.75, LogScaling()),\n    (0.001, 0.1, 0.01, 0.5, LogScaling()),\n    (0.1, 100, 1.0, 1.0/3, LogScaling()),\n])\ndef test_continuous_to_and_from_ndarray(lower, upper, external_hp, internal_ndarray, scaling):\n    hp_range = HyperparameterRangeContinuous('hp', lower, upper, scaling)\n    assert_allclose(\n        hp_range.to_ndarray(external_hp), np.array([internal_ndarray])\n    )\n    assert_allclose(\n        hp_range.from_ndarray(np.array([internal_ndarray])), external_hp\n    )\n\n\n@pytest.mark.parametrize('lower,upper,external_hp,internal_ndarray,scaling', [\n    (1, 10001, 5001, 0.5, LinearScaling()),\n    (-10, 10, 0, 0.5, LinearScaling()),\n])\ndef test_integer_to_and_from_ndarray(lower, upper, external_hp, internal_ndarray, scaling):\n    hp_range = HyperparameterRangeInteger('hp', lower, upper, scaling)\n    assert_allclose(\n        hp_range.to_ndarray(external_hp), np.array([internal_ndarray])\n    )\n    assert_allclose(\n        hp_range.from_ndarray(np.array([internal_ndarray])), external_hp\n    )\n\n\n@pytest.mark.parametrize('choices,external_hp,internal_ndarray', [\n    (('a', 'b'), 'a', [1.0, 0.0]),\n    (('a', 'b'), 'b', [0.0, 1.0]),\n    (('a', 'b', 'c', 'd'), 'c', [0.0, 0.0, 1.0, 0.0]),\n])\ndef test_categorical_to_and_from_ndarray(choices, external_hp, internal_ndarray):\n    hp_range = HyperparameterRangeCategorical('hp', choices)\n    assert_allclose(\n        hp_range.to_ndarray(external_hp), np.array(internal_ndarray)\n    )\n    assert hp_range.from_ndarray(np.array(internal_ndarray)) == external_hp\n\n\n# Going to internal representation and back should give back the original value\n@pytest.mark.parametrize('lower,upper,scaling', [\n    (0.0, 8.0, LinearScaling()),\n    (0.01, 0.1, LinearScaling()),\n    (-10.0, -5.1, LinearScaling()),\n    (-1000000000000000.0, 100000000000000000.0, LinearScaling()),\n    (10.0, 10000000000.0, LogScaling()),\n    (-1000.0, 100.0, LinearScaling()),\n    (1.0, 1000.0, LogScaling()),\n    (10.0, 15.0, LogScaling()),\n    (0.1, 20.0, LogScaling()),\n])\ndef test_continuous_to_ndarray_and_back(lower, upper, scaling):\n    # checks the lower bound upper bound and 5 random values\n    _test_continuous_to_ndarray_and_back(lower, upper, lower, scaling)\n    _test_continuous_to_ndarray_and_back(lower, upper, upper, scaling)\n    rnd = np.random.RandomState(0)\n    for random_hp in rnd.uniform(lower, upper, size=10):\n        _test_continuous_to_ndarray_and_back(lower, upper, random_hp, scaling)\n    _test_continuous_to_ndarray_and_back(lower, upper, lower, scaling)\n    _test_continuous_to_ndarray_and_back(lower, upper, upper, scaling)\n\n\n# helper for the previous test\ndef _test_continuous_to_ndarray_and_back(lower, upper, external_hp, scaling):\n    hp_range = HyperparameterRangeContinuous('hp', lower, upper, scaling)\n    assert hp_range.from_ndarray(hp_range.to_ndarray(external_hp)) == approx(external_hp)\n\n\n@pytest.mark.parametrize('lower,upper,scaling', [\n    (0, 8, LinearScaling()),\n    (1, 20, LinearScaling()),\n    (-10, -5, LinearScaling()),\n    (-1000000000000000, 100000000000000000, LinearScaling()),\n    (10, 10000000000, LogScaling()),\n    (-1000, 100, LinearScaling()),\n    (1, 1000, LogScaling()),\n    (10, 15, LogScaling()),\n])\ndef test_integer_to_ndarray_and_back(lower, upper, scaling):\n    # checks the lower bound upper bound and 5 random values\n    _test_integer_to_ndarray_and_back(lower, upper, lower, scaling)\n    _test_integer_to_ndarray_and_back(lower, upper, upper, scaling)\n    rnd = np.random.RandomState(0)\n    for random_hp in rnd.randint(lower+1, upper, size=15):\n        _test_integer_to_ndarray_and_back(lower, upper, int(random_hp), scaling)\n    _test_integer_to_ndarray_and_back(lower, upper, lower, scaling)\n    _test_integer_to_ndarray_and_back(lower, upper, upper, scaling)\n\n\n# helper for the previous test\ndef _test_integer_to_ndarray_and_back(lower, upper, external_hp, scaling):\n    hp_range = HyperparameterRangeInteger('hp', lower, upper, scaling)\n    assert hp_range.from_ndarray(hp_range.to_ndarray(external_hp)) == approx(external_hp)\n\n\n# this is more of a functional test testing of HP conversion and scaling\n# it generates random candidates and checks the distribution is correct\n# and also that they can be transformed to internal representation and back while still obtaining\n# the same value\ndef test_distribution_of_random_candidates():\n    random_state = np.random.RandomState(0)\n    hp_ranges = HyperparameterRanges_Impl(\n        HyperparameterRangeContinuous('0', 1.0, 1000.0, scaling=LinearScaling()),\n        HyperparameterRangeContinuous('1', 1.0, 1000.0, scaling=LogScaling()),\n        HyperparameterRangeContinuous('2', 0.9, 0.9999, scaling=ReverseLogScaling()),\n        HyperparameterRangeInteger('3', 1, 1000, scaling=LinearScaling()),\n        HyperparameterRangeInteger('4', 1, 1000, scaling=LogScaling()),\n        HyperparameterRangeCategorical('5', ('a', 'b', 'c')),\n    )\n    num_random_candidates = 600\n    random_candidates = [hp_ranges.random_candidate(random_state) for _ in range(num_random_candidates)]\n\n    # check converting back gets to the same candidate\n    for cand in random_candidates[2:]:\n        ndarray_candidate = hp_ranges.to_ndarray(cand)\n        converted_back = hp_ranges.from_ndarray(ndarray_candidate)\n        for hp, hp_converted_back in zip(cand, converted_back):\n            if isinstance(hp, str):\n                assert hp == hp_converted_back\n            else:\n                assert_almost_equal(hp, hp_converted_back)\n\n    hps0, hps1, hps2, hps3, hps4, hps5 = zip(*random_candidates)\n    assert 200 < np.percentile(hps0, 25) < 300\n    assert 450 < np.percentile(hps0, 50) < 550\n    assert 700 < np.percentile(hps0, 75) < 800\n\n    # same bounds as the previous but log scaling\n    assert 3 < np.percentile(hps1, 25) < 10\n    assert 20 < np.percentile(hps1, 50) < 40\n    assert 100 < np.percentile(hps1, 75) < 200\n\n    # reverse log\n    assert 0.9 < np.percentile(hps2, 25) < 0.99\n    assert 0.99 < np.percentile(hps2, 50) < 0.999\n    assert 0.999 < np.percentile(hps2, 75) < 0.9999\n\n    # integer\n    assert 200 < np.percentile(hps3, 25) < 300\n    assert 450 < np.percentile(hps3, 50) < 550\n    assert 700 < np.percentile(hps3, 75) < 800\n\n    # same bounds as the previous but log scaling\n    assert 3 < np.percentile(hps4, 25) < 10\n    assert 20 < np.percentile(hps4, 50) < 40\n    assert 100 < np.percentile(hps4, 75) < 200\n\n    counter = Counter(hps5)\n    assert len(counter) == 3\n\n    assert 150 < counter['a'] < 250  # should be about 200\n    assert 150 < counter['b'] < 250  # should be about 200\n    assert 150 < counter['c'] < 250  # should be about 200\n\n\n@pytest.mark.parametrize('lower,upper,scaling,constructor', [\n    (0, 8, LinearScaling(), HyperparameterRangeInteger),\n    (1, 20, LinearScaling(), HyperparameterRangeInteger),\n    (-10, -5, LinearScaling(), HyperparameterRangeInteger),\n    (-1000000000000000, 100000000000000000, LinearScaling(), HyperparameterRangeInteger),\n    (10, 10000000000, LogScaling(), HyperparameterRangeInteger),\n    (-1000, 100, LinearScaling(), HyperparameterRangeInteger),\n    (1, 1000, LogScaling(), HyperparameterRangeInteger),\n    (10, 15, LogScaling(), HyperparameterRangeInteger),\n    \n    (0.0, 8.0, LinearScaling(), HyperparameterRangeContinuous),\n    (0.01, 0.1, LinearScaling(), HyperparameterRangeContinuous),\n    (-10.0, -5.1, LinearScaling(), HyperparameterRangeContinuous),\n    (-1000000000000000.0, 100000000000000000.0, LinearScaling(), HyperparameterRangeContinuous),\n    (-1000.0, 100.0, LinearScaling(), HyperparameterRangeContinuous),\n    (10.0, 10000000000.0, LogScaling(), HyperparameterRangeContinuous),\n    (1.0, 1000.0, LogScaling(), HyperparameterRangeContinuous),\n    (10.0, 15.0, LogScaling(), HyperparameterRangeContinuous),\n    (0.1, 20.0, LogScaling(), HyperparameterRangeContinuous),\n])\ndef test_from_zero_one_extremes(lower, upper, scaling, constructor):\n    hp_range = constructor('hp', lower, upper, scaling)\n    assert hp_range.from_zero_one(0.0) == approx(lower)\n    assert hp_range.from_zero_one(1.0) == approx(upper)\n"""
tests/unittests/bayesopt/test_scaling.py,0,"b""# TODO: This code tests XYZScaling, which is only needed for HyperparameterRanges.\n# If the latter code is removed, this test can go as well.\n\nimport pytest\nfrom numpy.testing import assert_almost_equal\n\nfrom autogluon.searcher.bayesopt.datatypes.scaling import LinearScaling, \\\n    LogScaling, ReverseLogScaling\n\n\n@pytest.mark.parametrize('value, expected, scaling', [\n    (0.0, 0.0, LinearScaling()),\n    (0.5, 0.5, LinearScaling()),\n    (5.0, 5.0, LinearScaling()),\n    (-5.0, -5.0, LinearScaling()),\n    (0.5, -0.69314718055994529, LogScaling()),\n    (5.0, 1.6094379124341003, LogScaling()),\n    (0.0, 0.0, ReverseLogScaling()),\n    (0.5, 0.69314718055994529, ReverseLogScaling())\n])\ndef test_to_internal(value, expected, scaling):\n    assert_almost_equal(expected, scaling.to_internal(value))\n\n\n@pytest.mark.parametrize('value, expected, scaling', [\n    (0.0001, -9.210340371976182, LogScaling()),\n    (0.000001, -13.815510557964274, LogScaling()),\n    (0.0001, 0.00010000500033334732, ReverseLogScaling()),\n    (0.000001, 1.000000500029089e-06, ReverseLogScaling()),\n    (0.9999, 9.210340371976294, ReverseLogScaling()),\n    (0.999999, 13.815510557935518, ReverseLogScaling())\n])\ndef test_close_to_bounds_values(value, expected, scaling):\n    assert_almost_equal(expected, scaling.to_internal(value))\n\n\n@pytest.mark.parametrize('value, scaling', [\n    (-5.0, LogScaling()),\n    (0.0, LogScaling()),\n    (5.0, ReverseLogScaling()),\n    (-5.0, ReverseLogScaling())\n])\ndef test_invalid_values(value, scaling):\n    with pytest.raises(AssertionError):\n        scaling.to_internal(value)\n\n\n@pytest.mark.parametrize('value, expected, scaling', [\n    (0.0, 0.0, LinearScaling()),\n    (0.5, 0.5, LinearScaling()),\n    (5.0, 5.0, LinearScaling()),\n    (-5.0, -5.0, LinearScaling()),\n    (0.0, 1.0, LogScaling()),\n    (0.5, 1.6487212707001282, LogScaling()),\n    (5.0, 148.4131591025766, LogScaling()),\n    (-5.0, 0.006737946999085467, LogScaling()),\n    (0.0, 0.0, ReverseLogScaling()),\n    (0.5, 0.39346934028736658, ReverseLogScaling()),\n    (5.0, 0.99326205300091452, ReverseLogScaling()),\n    (-5.0, -147.4131591025766, ReverseLogScaling())\n])\ndef test_from_internal(value, expected, scaling):\n    assert_almost_equal(expected, scaling.from_internal(value))\n"""
autogluon/searcher/bayesopt/autogluon/__init__.py,0,b''
autogluon/searcher/bayesopt/autogluon/config_ext.py,0,"b'from typing import Tuple, Union\nimport ConfigSpace as CS\nimport ConfigSpace.hyperparameters as CSH\nimport copy\n\nfrom autogluon.searcher.bayesopt.autogluon.hp_ranges import \\\n    HyperparameterRanges_CS\n\n\nRESOURCE_ATTR_PREFIX = \'RESOURCE_ATTR_\'\n\n\nclass ExtendedConfiguration(object):\n    """"""\n    This class facilitates handling extended configs, which consist of a normal\n    config and a resource attribute.\n\n    The config space hp_ranges is extended by an additional resource\n    attribute. Note that this is not a hyperparameter we optimize over,\n    but it is under the control of the scheduler.\n    Its allowed range is [1, resource_attr_range[1]], which can be larger than\n    [resource_attr_range[0], resource_attr_range[1]]. This is because extended\n    configs with resource values outside of resource_attr_range may arise (for\n    example, in the early stopping context, we may receive data from\n    epoch < resource_attr_range[0]).\n\n    """"""\n    def __init__(\n            self, hp_ranges: HyperparameterRanges_CS, resource_attr_key: str,\n            resource_attr_range: Tuple[int, int]):\n        assert resource_attr_range[0] >= 1\n        assert resource_attr_range[1] >= resource_attr_range[0]\n        self.hp_ranges = hp_ranges\n        self.resource_attr_key = resource_attr_key\n        self.resource_attr_range = resource_attr_range\n        # Extended configuration space including resource attribute\n        config_space_ext = copy.deepcopy(hp_ranges.config_space)\n        self.resource_attr_name = RESOURCE_ATTR_PREFIX + resource_attr_key\n        # Allowed range: [1, resource_attr_range[1]]\n        config_space_ext.add_hyperparameter(CSH.UniformIntegerHyperparameter(\n            name=self.resource_attr_name, lower=1,\n            upper=resource_attr_range[1]))\n        self.hp_ranges_ext = HyperparameterRanges_CS(\n            config_space_ext, name_last_pos=self.resource_attr_name)\n\n    def get(self, config: CS.Configuration, resource: int) -> CS.Configuration:\n        """"""\n        Create extended config with resource added.\n\n        :param config:\n        :param resource:\n        :return: Extended config\n        """"""\n        values = copy.deepcopy(config.get_dictionary())\n        values[self.resource_attr_name] = resource\n        return CS.Configuration(self.hp_ranges_ext.config_space, values=values)\n\n    def remap_resource(\n            self, config_ext: CS.Configuration, resource: int,\n            as_dict: bool=False) -> Union[CS.Configuration, dict]:\n        x_dct = copy.copy(config_ext.get_dictionary())\n        x_dct[self.resource_attr_name] = resource\n        if as_dict:\n            return x_dct\n        else:\n            return CS.Configuration(\n                self.hp_ranges_ext.config_space, values=x_dct)\n\n    def remove_resource(\n            self, config_ext: CS.Configuration,\n            as_dict: bool=False) -> Union[CS.Configuration, dict]:\n        x_dct = copy.copy(config_ext.get_dictionary())\n        del x_dct[self.resource_attr_name]\n        if as_dict:\n            return x_dct\n        else:\n            return CS.Configuration(self.hp_ranges.config_space, values=x_dct)\n\n    def from_dict(self, config_dct: dict) -> CS.Configuration:\n        # Note: Here, the key for resource is resource_attr_key, not\n        # resource_attr_name\n        return CS.Configuration(\n            self.hp_ranges_ext if self.resource_attr_key in config_dct \\\n                else self.hp_ranges, values=config_dct)\n'"
autogluon/searcher/bayesopt/autogluon/debug_log.py,0,"b'from typing import Optional, Union\nimport logging\nimport numpy as np\nimport ConfigSpace as CS\nimport json\nfrom collections import OrderedDict\n\nfrom autogluon.searcher.bayesopt.datatypes.tuning_job_state import \\\n    TuningJobState\nfrom autogluon.searcher.bayesopt.autogluon.config_ext import \\\n    ExtendedConfiguration\n\nlogger = logging.getLogger(__name__)\n\n\ndef _to_key(\n        config: Union[CS.Configuration, dict],\n        configspace_ext: Optional[ExtendedConfiguration]) -> \\\n        (str, dict, Optional[int]):\n    if isinstance(config, CS.Configuration):\n        config_dict = config.get_dictionary()\n    else:\n        assert isinstance(config, dict)\n        config_dict = config\n    if configspace_ext is None:\n        resource = None\n    else:\n        resource = config_dict.get(configspace_ext.resource_attr_name)\n        if resource is not None:\n            config_dict = configspace_ext.remove_resource(config, as_dict=True)\n    return json.dumps(OrderedDict(config_dict)), config_dict, resource\n\n\nclass DebugLogPrinter(object):\n    """"""\n    Supports a concise debug log, currently specialized to GPFIFOSearcher (it\n    could be extended to GPMultiFidelitySearcher as well).\n\n    The log is made concise and readable by a few properties:\n    - configs are mapped to config IDs 0, 1, 2, ... as they get returned by\n        get_config. For multi-fidelity schedulers, extended config IDs are\n        of the form ""<k>:<r>"", k the ID of the config, <r> the resource\n        parameter. Note that even in this case, configs coming out of\n        get_config are not extended\n    - Information about get_config is displayed in a single block. For that,\n      different parts are first collected until the end of get_config\n\n    """"""\n    def __init__(\n            self, configspace_ext: Optional[ExtendedConfiguration] = None):\n        self.config_counter = 0\n        self._config_id = dict()\n        self.block_info = dict()\n        self.get_config_type = None\n        self.configspace_ext = configspace_ext\n\n    def config_id(self, config: Union[CS.Configuration, dict]) -> str:\n        config_key, _, resource = _to_key(config, self.configspace_ext)\n        _id = str(self._config_id[config_key])\n        if resource is None:\n            return _id\n        else:\n            return _id + \':{}\'.format(resource)\n\n    def start_get_config(self, gc_type):\n        assert gc_type in {\'random\', \'BO\'}\n        assert self.get_config_type is None, \\\n            ""Block for get_config of type \'{}\' is currently open"".format(\n                self.get_config_type)\n        self.get_config_type = gc_type\n        logger.info(""Starting get_config[{}] for config_id {}"".format(\n            gc_type, self.config_counter))\n\n    def set_final_config(self, config: Union[CS.Configuration, dict]):\n        assert self.get_config_type is not None, ""No block open right now""\n        config_key, config, resource = _to_key(config, self.configspace_ext)\n        assert resource is None, \\\n            ""set_final_config: config must not be extended""\n        _id = self._config_id.get(config_key)\n        assert _id is None, \\\n            ""Config {} already has been assigned a config ID = {}"".format(\n                config, _id)\n        # Counter is advanced in write_block\n        self._config_id[config_key] = self.config_counter\n        entries = [\'{}: {}\'.format(k, v) for k, v in config.items()]\n        msg = \'\\n\'.join(entries)\n        self.block_info[\'final_config\'] = msg\n\n    def set_state(self, state: TuningJobState):\n        assert self.get_config_type == \'BO\', ""Need to be in \'BO\' block""\n        labeled_configs = [\n            x.candidate for x in state.candidate_evaluations]\n        labeled_str = \', \'.join(\n            [self.config_id(x) for x in labeled_configs])\n        pending_str = \', \'.join(\n            [self.config_id(x) for x in state.pending_candidates])\n        msg = \'Labeled: \' + labeled_str + \'. Pending: \' + pending_str\n        self.block_info[\'state\'] = msg\n\n    def set_targets(self, targets: np.ndarray):\n        assert self.get_config_type == \'BO\', ""Need to be in \'BO\' block""\n        msg = \'Targets: \' + str(targets.reshape((-1,)))\n        self.block_info[\'targets\'] = msg\n\n    def set_gp_params(self, params: dict):\n        assert self.get_config_type == \'BO\', ""Need to be in \'BO\' block""\n        msg = \'GP params:\' + str(params)\n        self.block_info[\'params\'] = msg\n\n    def set_fantasies(self, fantasies: np.ndarray):\n        assert self.get_config_type == \'BO\', ""Need to be in \'BO\' block""\n        msg = \'Fantasized targets:\\n\' + str(fantasies)\n        self.block_info[\'fantasies\'] = msg\n\n    def set_init_config(self, config: Union[CS.Configuration, dict],\n                        top_scores: np.ndarray = None):\n        assert self.get_config_type == \'BO\', ""Need to be in \'BO\' block""\n        _, config, _ = _to_key(config, self.configspace_ext)\n        entries = [\'{}: {}\'.format(k, v) for k, v in config.items()]\n        msg = ""Started BO from (top scorer):\\n"" + \'\\n\'.join(entries)\n        if top_scores is not None:\n            msg += (""\\nTop score values: "" + str(top_scores.reshape((-1,))))\n        self.block_info[\'start_config\'] = msg\n\n    def set_num_evaluations(self, num_evals: int):\n        assert self.get_config_type == \'BO\', ""Need to be in \'BO\' block""\n        self.block_info[\'num_evals\'] = num_evals\n\n    def write_block(self):\n        assert self.get_config_type is not None, ""No block open right now""\n        info = self.block_info\n        if \'num_evals\' in info:\n            parts = [\'[{}: {}] ({} evaluations)\'.format(\n                self.config_counter, self.get_config_type, info[\'num_evals\'])]\n        else:\n            parts = [\'[{}: {}]\'.format(\n                self.config_counter, self.get_config_type)]\n        parts.append(info[\'final_config\'])\n        if self.get_config_type == \'BO\':\n            parts.extend(\n                [info[\'start_config\'], info[\'state\'], info[\'targets\'],\n                 info[\'params\']])\n            if \'fantasies\' in info:\n                parts.append(info[\'fantasies\'])\n        msg = \'\\n\'.join(parts)\n        logger.info(msg)\n        # Advance counter\n        self.config_counter += 1\n        self.get_config_type = None\n        self.block_info = dict()\n\n    def get_mutable_state(self) -> dict:\n        return {\n            \'config_counter\': self.config_counter,\n            \'config_id\': self._config_id}\n\n    def set_mutable_state(self, state: dict):\n        assert self.get_config_type is None, \\\n            ""Block for get_config of type \'{}\' is currently open"".format(\n                self.get_config_type)\n        self.config_counter = state[\'config_counter\']\n        self._config_id = state[\'config_id\']\n        self.block_info = dict()\n'"
autogluon/searcher/bayesopt/autogluon/gp_fifo_searcher.py,0,"b'import numpy as np\nfrom typing import Callable, Type, NamedTuple, Optional\nimport copy\nimport logging\nimport ConfigSpace as CS\n\nfrom autogluon.searcher.bayesopt.datatypes.tuning_job_state import \\\n    TuningJobState\nfrom autogluon.searcher.bayesopt.datatypes.common import CandidateEvaluation, \\\n    Candidate, candidate_for_print, PendingEvaluation\nfrom autogluon.searcher.bayesopt.datatypes.hp_ranges import HyperparameterRanges\nfrom autogluon.searcher.bayesopt.models.gpmxnet import GPModel, GPMXNetModel\nfrom autogluon.searcher.bayesopt.models.gpmxnet_transformers import \\\n    GPMXNetPendingCandidateStateTransformer, GPMXNetModelArgs\nfrom autogluon.searcher.bayesopt.models.gpmxnet_skipopt import \\\n    SkipOptimizationPredicate\nfrom autogluon.searcher.bayesopt.tuning_algorithms.common import \\\n    RandomStatefulCandidateGenerator, compute_blacklisted_candidates\nfrom autogluon.searcher.bayesopt.tuning_algorithms.bo_algorithm_components import \\\n    IndependentThompsonSampling\nfrom autogluon.searcher.bayesopt.tuning_algorithms.bo_algorithm import \\\n    BayesianOptimizationAlgorithm\nfrom autogluon.searcher.bayesopt.tuning_algorithms.default_algorithm import \\\n    dictionarize_objective, DEFAULT_METRIC, \\\n    DEFAULT_LOCAL_OPTIMIZER_CLASS, DEFAULT_NUM_INITIAL_CANDIDATES, \\\n    DEFAULT_NUM_INITIAL_RANDOM_EVALUATIONS\nfrom autogluon.searcher.bayesopt.tuning_algorithms.base_classes import \\\n    LocalOptimizer, AcquisitionFunction, ScoringFunction\nfrom autogluon.searcher.bayesopt.utils.duplicate_detector import \\\n    DuplicateDetectorIdentical\nfrom autogluon.searcher.bayesopt.autogluon.hp_ranges import \\\n    HyperparameterRanges_CS\nfrom autogluon.searcher.bayesopt.autogluon.gp_profiling import \\\n    GPMXNetSimpleProfiler\nfrom autogluon.searcher.bayesopt.autogluon.debug_log import DebugLogPrinter\n\nlogger = logging.getLogger(__name__)\n\n\nGET_CONFIG_RANDOM_RETRIES = 50\n\n\nclass MapReward(NamedTuple):\n    forward: Callable[[float], float]\n    reverse: Callable[[float], float]\n\n    def __call__(self, x: float) -> float:\n        return self.forward(x)\n\n\ndef accumulate_profiling_record(\n        cum_records: dict, profiler: GPMXNetSimpleProfiler, pick_random: bool):\n    # Pull out profiling data from this block\n    block_id = profiler.id_counter - 1\n    curr_record = {\n        r.tag: r.duration for r in profiler.records\n        if r.id == block_id}\n    if pick_random:\n        curr_record[\'num_random\'] = 1\n        curr_record[\'total_all\'] = curr_record[\'random\']\n    else:\n        curr_record[\'num_model\'] = 1\n    # Sum up entries\n    for k, v in curr_record.items():\n        if k in cum_records:\n            cum_records[k] += v\n        else:\n            cum_records[k] = v\n\n\nSUPPORTED_INITIAL_SCORING = {\n    \'thompson_indep\',\n    \'acq_func\'}\n\n\nDEFAULT_INITIAL_SCORING = \'thompson_indep\'\n\n\ndef create_initial_candidates_scorer(\n        initial_scoring: str, model: GPMXNetModel,\n        acquisition_class: Type[AcquisitionFunction],\n        random_state: np.random.RandomState) -> ScoringFunction:\n    if initial_scoring == \'thompson_indep\':\n        return IndependentThompsonSampling(model, random_state=random_state)\n    else:\n        return acquisition_class(model)\n\n\ndef check_initial_candidates_scorer(initial_scoring: str) -> str:\n    if initial_scoring is None:\n        return DEFAULT_INITIAL_SCORING\n    else:\n        assert initial_scoring in SUPPORTED_INITIAL_SCORING, \\\n            ""initial_scoring = \'{}\' is not supported"".format(\n                initial_scoring)\n        return initial_scoring\n\n\nclass GPFIFOSearcher(object):\n    """"""\n    Supports standard GP-based hyperparameter optimization, when used with a\n    FIFO scheduler.\n\n    """"""\n    def __init__(\n            self, hp_ranges: HyperparameterRanges, random_seed: int,\n            gpmodel: GPModel, model_args: GPMXNetModelArgs,\n            map_reward: MapReward,\n            acquisition_class: Type[AcquisitionFunction],\n            init_state: TuningJobState = None,\n            local_minimizer_class: Type[LocalOptimizer] = DEFAULT_LOCAL_OPTIMIZER_CLASS,\n            skip_optimization: SkipOptimizationPredicate = None,\n            num_initial_candidates: int = DEFAULT_NUM_INITIAL_CANDIDATES,\n            num_initial_random_choices: int = DEFAULT_NUM_INITIAL_RANDOM_EVALUATIONS,\n            initial_scoring: Optional[str] = None,\n            profiler: Optional[GPMXNetSimpleProfiler] = None,\n            first_is_default: bool = True,\n            debug_log: Optional[DebugLogPrinter] = None):\n        """"""\n        Note that the GPMXNetModel is created on demand (by the state\n        transformer) in get_config, along with components needed for the BO\n        algorithm.\n\n        The searcher is supposed to maximize reward, while internally, the\n        criterion is minimized. map_reward maps reward to internal criterion, it\n        must be strictly decreasing.\n\n        :param hp_ranges: Configuration space without resource attribute\n        :param random_seed:\n        :param gpmodel: GP regression model\n        :param model_args: Arguments for GPMXNet model creation\n        :param map_reward: Function mapping reward to criterion to be minimized\n        :param acquisition_class: Type for acquisition function\n        :param init_state: TuningJobState to start from (default is empty)\n        :param local_minimizer_class: Type for local minimizer\n        :param skip_optimization: Predicate, see\n            GPMXNetPendingCandidateStateTransformer\n        :param num_initial_candidates: See BayesianOptimizationAlgorithm\n        :param num_initial_random_choices: Configs are sampled at random until\n            this many candidates received label feedback\n        :param initial_scoring: Scoring function to rank initial candidates.\n            Default: thompson_indep (independent Thompson sampling)\n        :param profiler: If given, HPO computations are profiled\n        :param first_is_default: If true, the first config to be evaluated is\n            the default of the search space. Otherwise, the first is sampled\n            at random\n        :param debug_log: DebugLogPrinter for debug logging (optional)\n\n        """"""\n        self.hp_ranges = hp_ranges\n        self.random_seed = random_seed\n        self.num_initial_candidates = num_initial_candidates\n        self.num_initial_random_choices = num_initial_random_choices\n        self.map_reward = map_reward\n        self.local_minimizer_class = local_minimizer_class\n        self.acquisition_class = acquisition_class\n        self.debug_log = debug_log\n        self.initial_scoring = check_initial_candidates_scorer(initial_scoring)\n        # Create state transformer\n        # Initial state is empty (note that the state is mutable)\n        if init_state is None:\n            init_state = TuningJobState(\n                hp_ranges=hp_ranges,\n                candidate_evaluations=[],\n                failed_candidates=[],\n                pending_evaluations=[])\n        else:\n            assert hp_ranges is init_state.hp_ranges, \\\n                ""hp_ranges and init_state.hp_ranges must be same object""\n        self.state_transformer = GPMXNetPendingCandidateStateTransformer(\n            gpmodel=gpmodel,\n            init_state=init_state,\n            model_args=model_args,\n            skip_optimization=skip_optimization,\n            profiler=profiler,\n            debug_log=debug_log)\n        self.random_state = np.random.RandomState(random_seed)\n        self.random_generator = RandomStatefulCandidateGenerator(\n            hp_ranges, random_state=self.random_state)\n        self.profiler = profiler\n        self.do_profile = (profiler is not None)\n        self.first_is_default = first_is_default\n        if first_is_default:\n            assert isinstance(hp_ranges, HyperparameterRanges_CS), \\\n                ""If first_is_default, must have hp_ranges of HyperparameterRanges_CS type""\n        if debug_log is not None:\n            assert isinstance(hp_ranges, HyperparameterRanges_CS), \\\n                ""If debug_log is given, must have hp_ranges of HyperparameterRanges_CS type""\n        # Sums up profiling records across all get_config calls\n        self._profile_record = dict()\n        if debug_log is not None:\n            deb_msg = ""[GPFIFOSearcher.__init__]\\n""\n            deb_msg += (""- acquisition_class = {}\\n"".format(acquisition_class))\n            deb_msg += (""- local_minimizer_class = {}\\n"".format(local_minimizer_class))\n            deb_msg += (""- num_initial_candidates = {}\\n"".format(num_initial_candidates))\n            deb_msg += (""- num_initial_random_choices = {}\\n"".format(num_initial_random_choices))\n            deb_msg += (""- initial_scoring = {}\\n"".format(self.initial_scoring))\n            deb_msg += (""- first_is_default = {}"".format(first_is_default))\n            logger.info(deb_msg)\n\n    def update(self, config: Candidate, reward: float):\n        """"""\n        Registers new datapoint at config, with reward reward.\n        Note that in general, config should previously have been registered as\n        pending (register_pending). If so, it is switched from pending\n        to labeled. If not, it is considered directly labeled.\n\n        :param config:\n        :param reward:\n        """"""\n        crit_val = self.map_reward(reward)\n        self.state_transformer.label_candidate(CandidateEvaluation(\n            candidate=copy.deepcopy(config),\n            metrics=dictionarize_objective(crit_val)))\n        if self.debug_log is not None:\n            config_id = self.debug_log.config_id(config)\n            msg = ""Update for config_id {}: reward = {}, crit_val = {}"".format(\n                config_id, reward, crit_val)\n            logger.info(msg)\n\n    def register_pending(self, config: Candidate):\n        """"""\n        Registers config as pending. This means the corresponding evaluation\n        task is running. Once it finishes, update is called for config.\n\n        """"""\n        # It is OK for the candidate already to be registered as pending, in\n        # which case we do nothing\n        state = self.state_transformer.state\n        if config not in state.pending_candidates:\n            if config in (x.candidate for x in state.candidate_evaluations):\n                evals = state.candidate_evaluations\n                num_labeled = len(evals)\n                pos_cand = next(\n                    i for i, x in enumerate(evals) if x.candidate == config)\n                error_msg = """"""\n                This configuration is already registered as labeled:\n                   Position of labeled candidate: {} of {}\n                   Label value: {}\n                """""".format(\n                    pos_cand, num_labeled,\n                    evals[pos_cand].metrics[DEFAULT_METRIC])\n                assert False, error_msg\n            self.state_transformer.append_candidate(config)\n\n    def get_config(self) -> Candidate:\n        """"""\n        Runs Bayesian optimization in order to suggest the next config to evaluate.\n\n        :return: Next config to evaluate at\n        """"""\n        state = self.state_transformer.state\n        if self.do_profile:\n            fit_hyperparams = not self.state_transformer.skip_optimization(\n                state)\n            self.profiler.set_state(state, fit_hyperparams)\n        blacklisted_candidates = compute_blacklisted_candidates(state)\n        pick_random = (len(blacklisted_candidates) < self.num_initial_random_choices) or \\\n            (not state.candidate_evaluations)\n        if self.debug_log is not None:\n            self.debug_log.start_get_config(\'random\' if pick_random else \'BO\')\n        if pick_random:\n            config = None\n            if self.first_is_default and (not blacklisted_candidates):\n                # Use default configuration if there is one specified\n                default_config = self.hp_ranges.config_space.get_default_configuration()\n                if default_config and len(default_config.get_dictionary()) > 0:\n                    config = default_config\n                    if self.debug_log is not None:\n                        logger.info(""Start with default config:\\n{}"".format(\n                            candidate_for_print(config)))\n            if config is None:\n                if self.do_profile:\n                    self.profiler.start(\'random\')\n                for _ in range(GET_CONFIG_RANDOM_RETRIES):\n                    _config = self.hp_ranges.random_candidate(self.random_state)\n                    if _config not in blacklisted_candidates:\n                        config = _config\n                        break\n                if config is None:\n                    raise AssertionError(\n                        ""Failed to sample a configuration not already chosen ""\n                        ""before. Maybe there are no free configurations left? ""\n                        ""The blacklist size is {}"".format(len(blacklisted_candidates)))\n                if self.do_profile:\n                    self.profiler.stop(\'random\')\n        else:\n            # Obtain current GPMXNetModel from state transformer. Based on\n            # this, the BO algorithm components can be constructed\n            state = self.state_transformer.state\n            if self.do_profile:\n                self.profiler.start(\'total_all\')\n                self.profiler.start(\'total_update\')\n            # Note: Asking for the model triggers the posterior computation\n            model = self.state_transformer.model()\n            if self.do_profile:\n                self.profiler.stop(\'total_update\')\n            # Create BO algorithm\n            initial_candidates_scorer = create_initial_candidates_scorer(\n                self.initial_scoring, model, self.acquisition_class,\n                self.random_state)\n            local_optimizer = self.local_minimizer_class(\n                state, model, self.acquisition_class)\n            # Make sure not to use the same random seed for each call:\n            #random_seed = compute_random_seed({\'0\': state}, self.random_seed)\n            bo_algorithm = BayesianOptimizationAlgorithm(\n                initial_candidates_generator=self.random_generator,\n                initial_candidates_scorer=initial_candidates_scorer,\n                num_initial_candidates=self.num_initial_candidates,\n                local_optimizer=local_optimizer,\n                pending_candidate_state_transformer=None,\n                blacklisted_candidates=blacklisted_candidates,\n                num_requested_candidates=1,\n                greedy_batch_selection=False,\n                duplicate_detector=DuplicateDetectorIdentical(),\n                profiler=self.profiler,\n                sample_unique_candidates=False,\n                debug_log=self.debug_log)\n            # Next candidate decision\n            if self.do_profile:\n                self.profiler.start(\'total_nextcand\')\n            _config = bo_algorithm.next_candidates()\n            if len(_config) == 0:\n                raise AssertionError(\n                    ""Failed to find a configuration not already chosen ""\n                    ""before. Maybe there are no free configurations left? ""\n                    ""The blacklist size is {}"".format(len(blacklisted_candidates)))\n            config = _config[0]\n            if self.do_profile:\n                self.profiler.stop(\'total_nextcand\')\n            if self.do_profile:\n                self.profiler.stop(\'total_all\')\n\n        if self.debug_log is not None:\n            self.debug_log.set_final_config(config)\n            # All get_config debug log info is only written here\n            self.debug_log.write_block()\n        if self.do_profile:\n            self.profiler.clear()\n            # Pull out profiling data from this block, add to _profile_record\n            accumulate_profiling_record(\n                self._profile_record, self.profiler, pick_random)\n\n        return config\n\n    def evaluation_failed(self, config: Candidate):\n        # Remove pending candidate\n        self.state_transformer.drop_candidate(config)\n        # Mark config as failed (which means it will be blacklisted in\n        # future get_config calls)\n        self.state_transformer.mark_candidate_failed(config)\n\n    def dataset_size(self):\n        return len(self.state_transformer.state.candidate_evaluations)\n\n    def cumulative_profile_record(self):\n        """"""\n        If profiling is activated, we sum up the profiling blocks for each\n        call of get_config and return it as dict. See get_config for what\n        is recorded:\n        - num_random: Number of get_config calls with random selection\n        - num_model: Number of get_config calls with model-based selection\n        - total_all: Sum of total times for all get_config calls\n        """"""\n        return self._profile_record\n\n    def get_params(self):\n        """"""\n        Note: Once MCMC is supported, this method will have to be refactored.\n\n        :return: Dictionary with current hyperparameter values\n        """"""\n        return self.state_transformer.get_params()\n\n    def set_params(self, param_dict):\n        self.state_transformer.set_params(param_dict)\n\n    def get_state(self):\n        """"""\n        The mutable state consists of the GP model parameters, the\n        TuningJobState, and the skip_optimization predicate (which can have a\n        mutable state).\n        We assume that skip_optimization can be pickled.\n\n        """"""\n        state = {\n            \'model_params\': self.get_params(),\n            \'state\': encode_state(self.state_transformer.state),\n            \'skip_optimization\': self.state_transformer.skip_optimization,\n            \'random_state\': self.random_state}\n        if self.debug_log is not None:\n            state[\'debug_log\'] = self.debug_log.get_mutable_state()\n        return state\n\n    def clone_from_state(self, state):\n        # Create clone with mutable state taken from \'state\'\n        model_args = self.state_transformer._model_args\n        init_state = decode_state(state[\'state\'], self.hp_ranges)\n        skip_optimization = state[\'skip_optimization\']\n        new_searcher = GPFIFOSearcher(\n            hp_ranges=self.hp_ranges,\n            random_seed=self.random_seed,\n            gpmodel=self.state_transformer._gpmodel,\n            model_args=model_args,\n            map_reward=self.map_reward,\n            acquisition_class=self.acquisition_class,\n            init_state=init_state,\n            local_minimizer_class=self.local_minimizer_class,\n            skip_optimization=skip_optimization,\n            num_initial_candidates=self.num_initial_candidates,\n            num_initial_random_choices=self.num_initial_random_choices,\n            initial_scoring=self.initial_scoring,\n            profiler=self.profiler,\n            first_is_default=self.first_is_default,\n            debug_log=self.debug_log)\n        new_searcher.state_transformer.set_params(state[\'model_params\'])\n        new_searcher.random_state = state[\'random_state\']\n        new_searcher.random_generator.random_state = \\\n            state[\'random_state\']\n        if self.debug_log and \'debug_log\' in state:\n            new_searcher.debug_log.set_mutable_state(state[\'debug_log\'])\n        # Invalidate self (must not be used afterwards)\n        self.state_transformer = None\n        return new_searcher\n\n\ndef encode_state(state: TuningJobState) -> dict:\n    assert isinstance(state.hp_ranges, HyperparameterRanges_CS), \\\n        ""Must have hp_ranges of HyperparameterRanges_CS type""\n    candidate_evaluations = [\n        {\'candidate\': eval.candidate.get_dictionary(),\n         \'metrics\': eval.metrics}\n        for eval in state.candidate_evaluations]\n    failed_candidates = [x.get_dictionary() for x in state.failed_candidates]\n    pending_evaluations = [\n        eval.candidate.get_dictionary() for eval in state.pending_evaluations]\n    return {\n        \'candidate_evaluations\': candidate_evaluations,\n        \'failed_candidates\': failed_candidates,\n        \'pending_evaluations\': pending_evaluations}\n\n\ndef decode_state(enc_state: dict, hp_ranges: HyperparameterRanges_CS) \\\n        -> TuningJobState:\n    assert isinstance(hp_ranges, HyperparameterRanges_CS), \\\n        ""Must have hp_ranges of HyperparameterRanges_CS type""\n    config_space = hp_ranges.config_space\n\n    def to_cs(x):\n        return CS.Configuration(config_space, values=x)\n\n    candidate_evaluations = [\n        CandidateEvaluation(to_cs(x[\'candidate\']), x[\'metrics\'])\n        for x in enc_state[\'candidate_evaluations\']]\n    failed_candidates = [to_cs(x) for x in enc_state[\'failed_candidates\']]\n    pending_evaluations = [\n        PendingEvaluation(to_cs(x)) for x in enc_state[\'pending_evaluations\']]\n    return TuningJobState(\n        hp_ranges=hp_ranges,\n        candidate_evaluations=candidate_evaluations,\n        failed_candidates=failed_candidates,\n        pending_evaluations=pending_evaluations)\n\n\ndef map_reward(const=1.0) -> MapReward:\n    """"""\n    Factory for map_reward argument in GPMultiFidelitySearcher.\n    """"""\n    def const_minus_x(x):\n        return const - x\n\n    return MapReward(forward=const_minus_x, reverse=const_minus_x)\n'"
autogluon/searcher/bayesopt/autogluon/gp_multifidelity_searcher.py,0,"b'import numpy as np\nfrom typing import Callable, Type, Tuple, Set, Iterable, Optional\nimport ConfigSpace as CS\nfrom collections import Counter\nimport logging\n\nfrom autogluon.searcher.bayesopt.datatypes.tuning_job_state import \\\n    TuningJobState\nfrom autogluon.searcher.bayesopt.datatypes.common import \\\n    CandidateEvaluation, PendingEvaluation, candidate_for_print\nfrom autogluon.searcher.bayesopt.models.gpmxnet import GPModel\nfrom autogluon.searcher.bayesopt.models.gpmxnet_transformers import \\\n    GPMXNetPendingCandidateStateTransformer, GPMXNetModelArgs\nfrom autogluon.searcher.bayesopt.models.gpmxnet_skipopt import \\\n    SkipOptimizationPredicate\nfrom autogluon.searcher.bayesopt.tuning_algorithms.common import \\\n    RandomStatefulCandidateGenerator, compute_blacklisted_candidates\nfrom autogluon.searcher.bayesopt.tuning_algorithms.bo_algorithm import \\\n    BayesianOptimizationAlgorithm\nfrom autogluon.searcher.bayesopt.tuning_algorithms.default_algorithm import \\\n    dictionarize_objective, DEFAULT_METRIC, \\\n    DEFAULT_LOCAL_OPTIMIZER_CLASS, DEFAULT_NUM_INITIAL_CANDIDATES, \\\n    DEFAULT_NUM_INITIAL_RANDOM_EVALUATIONS\nfrom autogluon.searcher.bayesopt.tuning_algorithms.base_classes import \\\n    LocalOptimizer, AcquisitionFunction\nfrom autogluon.searcher.bayesopt.utils.duplicate_detector import \\\n    DuplicateDetectorIdentical\nfrom autogluon.searcher.bayesopt.autogluon.hp_ranges import \\\n    HyperparameterRanges_CS\nfrom autogluon.searcher.bayesopt.autogluon.gp_profiling import \\\n    GPMXNetSimpleProfiler\nfrom autogluon.searcher.bayesopt.autogluon.gp_fifo_searcher import \\\n    GET_CONFIG_RANDOM_RETRIES, accumulate_profiling_record, MapReward, \\\n    check_initial_candidates_scorer, create_initial_candidates_scorer, \\\n    encode_state, decode_state\nfrom autogluon.searcher.bayesopt.autogluon.config_ext import \\\n    ExtendedConfiguration\nfrom autogluon.searcher.bayesopt.autogluon.debug_log import DebugLogPrinter\n\nlogger = logging.getLogger(__name__)\n\n\nclass GPMultiFidelitySearcher(object):\n    """"""\n    Supports asynchronous multi-fidelity hyperparameter optimization, in the\n    style of Hyperband or BOHB. Here, a joint GP surrogate model is fit to\n    observations made at all resource levels.\n\n    """"""\n    def __init__(\n            self, hp_ranges: HyperparameterRanges_CS,\n            resource_attr_key: str,\n            resource_attr_range: Tuple[int, int],\n            random_seed: int,\n            gpmodel: GPModel, model_args: GPMXNetModelArgs,\n            map_reward: MapReward,\n            acquisition_class: Type[AcquisitionFunction],\n            resource_for_acquisition: Callable[..., int],\n            init_state: TuningJobState = None,\n            local_minimizer_class: Type[LocalOptimizer] = DEFAULT_LOCAL_OPTIMIZER_CLASS,\n            skip_optimization: SkipOptimizationPredicate = None,\n            num_initial_candidates: int = DEFAULT_NUM_INITIAL_CANDIDATES,\n            num_initial_random_choices: int = DEFAULT_NUM_INITIAL_RANDOM_EVALUATIONS,\n            initial_scoring: Optional[str] = None,\n            profiler: Optional[GPMXNetSimpleProfiler] = None,\n            first_is_default: bool = True,\n            debug_log: Optional[DebugLogPrinter] = None):\n        """"""\n        Note that the GPMXNetModel is created on demand (by the state\n        transformer) in get_config, along with components needed for the BO\n        algorithm.\n\n        The configuration space is hp_ranges. This does not include the resource\n        attribute, which is passed as result component instead, with key\n        resource_attr_key. The GP model is over configuration and resource\n        attribute, its configuration space is maintained in configspace_ext.\n\n        The search for a next candidate in get_config is fixing the resource\n        level, meaning that extended configs for which the acquisition function\n        is evaluated, all have the same resource level. This level may depend on\n        the current state, the function can be passed as\n        resource_for_acquisition. Its signature is\n            resource_for_acquisition(state, resource_attr_name, **kwargs) -> int,\n        where state is TuningJobState.\n        Example: resource_for_acquisition may count the number of labeled data\n        at each resource level, and choose the largest level supported by\n        enough data. Or information about bracket and first milestone may be\n        passed from the scheduler via **kwargs.\n\n        The searcher is supposed to maximize reward, while internally, the\n        criterion is minimized. map_reward maps reward to internal criterion, it\n        must be strictly decreasing.\n\n        :param hp_ranges: Configuration space without resource attribute\n        :param resource_attr_key: Key for resource attribute.\n            Note: The resource attribute must be int valued\n        :param resource_attr_range: Range (lower, upper) for resource\n            attribute( (must be int valued)\n        :param random_seed:\n        :param gpmodel: GP regression model\n        :param model_args: Arguments for GPMXNet model creation\n        :param map_reward: Function mapping reward to criterion to be minimized\n        :param acquisition_class: Type for acquisition function\n        :param resource_for_acquisition: See above\n        :param init_state: TuningJobState to start from (default is empty).\n            Here, init_state.hp_ranges must be equal to\n            self.configspace_ext.hp_ranges_ext here (this is not checked)\n        :param local_minimizer_class: Type for local minimizer\n        :param skip_optimization: Predicate, see\n            GPMXNetPendingCandidateStateTransformer\n        :param num_initial_candidates: See BayesianOptimizationAlgorithm\n        :param num_initial_random_choices: Configs are sampled at random until\n            this many candidates received label feedback\n        :param initial_scoring: Scoring function to rank initial candidates.\n            Default: thompson_indep (independent Thompson sampling)\n        :param profiler: If given, HPO computations are profiled\n        :param first_is_default: If true, the first result of get_config is the\n            default config of hp_ranges\n        :param debug_log: DebugLogPrinter for debug logging (optional)\n\n        """"""\n        self.hp_ranges = hp_ranges\n        self.random_seed = random_seed\n        self.num_initial_candidates = num_initial_candidates\n        self.num_initial_random_choices = num_initial_random_choices\n        self.map_reward = map_reward\n        self.resource_for_acquisition = resource_for_acquisition\n        self.local_minimizer_class = local_minimizer_class\n        self.acquisition_class = acquisition_class\n        self._gpmodel = gpmodel\n        self.initial_scoring = check_initial_candidates_scorer(initial_scoring)\n        # Extended configuration space including resource attribute\n        self.configspace_ext = ExtendedConfiguration(\n            hp_ranges, resource_attr_key, resource_attr_range)\n        if debug_log is not None:\n            # Configure DebugLogPrinter\n            debug_log.configspace_ext = self.configspace_ext\n        self.debug_log = debug_log\n        # Create state transformer\n        # Initial state is empty (note that the state is mutable)\n        if init_state is None:\n            init_state = TuningJobState(\n                hp_ranges=self.configspace_ext.hp_ranges_ext,\n                candidate_evaluations=[],\n                failed_candidates=[],\n                pending_evaluations=[])\n        self.state_transformer = GPMXNetPendingCandidateStateTransformer(\n            gpmodel=gpmodel,\n            init_state=init_state,\n            model_args=model_args,\n            skip_optimization=skip_optimization,\n            profiler=profiler,\n            debug_log=debug_log)\n        self.random_state = np.random.RandomState(random_seed)\n        self.random_generator = RandomStatefulCandidateGenerator(\n            self.configspace_ext.hp_ranges_ext, random_state=self.random_state)\n        self.profiler = profiler\n        self.do_profile = (profiler is not None)\n        self.first_is_default = first_is_default\n        # Sums up profiling records across all get_config calls\n        self._profile_record = dict()\n        if debug_log is not None:\n            deb_msg = ""[GPMultiFidelitySearcher.__init__]\\n""\n            deb_msg += (""- acquisition_class = {}\\n"".format(acquisition_class))\n            deb_msg += (""- local_minimizer_class = {}\\n"".format(local_minimizer_class))\n            deb_msg += (""- num_initial_candidates = {}\\n"".format(num_initial_candidates))\n            deb_msg += (""- num_initial_random_choices = {}\\n"".format(num_initial_random_choices))\n            deb_msg += (""- initial_scoring = {}\\n"".format(self.initial_scoring))\n            deb_msg += (""- first_is_default = {}"".format(first_is_default))\n            logger.info(deb_msg)\n\n    def update(self, config: CS.Configuration, reward: float, resource: int):\n        """"""\n        Registers new datapoint at config, with reward and resource.\n        Note that in general, config should previously have been registered as\n        pending (register_pending). If so, it is switched from pending\n        to labeled. If not, it is considered directly labeled.\n\n        :param config:\n        :param reward:\n        :param resource:\n        """"""\n        config_ext = self.configspace_ext.get(config, resource)\n        crit_val = self.map_reward(reward)\n        self.state_transformer.label_candidate(CandidateEvaluation(\n            candidate=config_ext, metrics=dictionarize_objective(crit_val)))\n        if self.debug_log is not None:\n            config_id = self.debug_log.config_id(config_ext)\n            msg = ""Update for config_id {}: reward = {}, crit_val = {}"".format(\n                config_id, reward, crit_val)\n            logger.info(msg)\n\n    def register_pending(self, config: CS.Configuration, milestone: int):\n        """"""\n        Registers config as pending for resource level milestone. This means\n        the corresponding evaluation task is running and should reach that\n        level later, when update is called for it.\n\n        :param config:\n        :param milestone:\n        """"""\n        # It is OK for the candidate already to be registered as pending, in\n        # which case we do nothing\n        state = self.state_transformer.state\n        config_ext = self.configspace_ext.get(config, milestone)\n        if config_ext not in state.pending_candidates:\n            if config_ext in (x.candidate for x in state.candidate_evaluations):\n                evals = state.candidate_evaluations\n                num_labeled = len(evals)\n                pos_cand = next(\n                    i for i, x in enumerate(evals) if x.candidate == config_ext)\n                error_msg = """"""\n                This configuration is already registered as labeled:\n                   Position of labeled candidate: {} of {}\n                   Label value: {}\n                   Resource level: {}\n                """""".format(\n                    pos_cand, num_labeled,\n                    evals[pos_cand].metrics[DEFAULT_METRIC], milestone)\n                assert False, error_msg\n            self.state_transformer.append_candidate(config_ext)\n\n    def _get_unique_candidates(\n            self, candidate_list: Iterable[CS.Configuration],\n            target_resource: int) -> Set[CS.Configuration]:\n\n        remap_resource = lambda x: self.configspace_ext.remap_resource(\n            x, target_resource)\n        return set(map(remap_resource, candidate_list))\n\n    def _get_blacklisted_candidates(self, target_resource: int) -> Set[CS.Configuration]:\n        """"""\n        We want to blacklist all configurations which are labeled or pending at\n        any resource level. As the search affected by the blacklist happens at\n        resource level target_resource, the candidates have to be modified\n        accordingly\n\n        """"""\n        return self._get_unique_candidates(\n            compute_blacklisted_candidates(self.state_transformer.state),\n            target_resource)\n\n    def _fix_resource_attribute(self, resource_attr_value: int):\n        self.configspace_ext.hp_ranges_ext.value_for_last_pos = \\\n            resource_attr_value\n\n    def get_config(self, **kwargs) -> CS.Configuration:\n        """"""\n        Runs Bayesian optimization in order to suggest the next config to evaluate.\n\n        :return: Next config to evaluate at\n\n        """"""\n        state = self.state_transformer.state\n        if self.do_profile:\n            fit_hyperparams = not self.state_transformer.skip_optimization(\n                state)\n            self.profiler.set_state(state, fit_hyperparams)\n        # Fix resource attribute during the search to the value obtained from\n        # self.resource_for_acquisition. Compute blacklisted_candidates.\n        if state.candidate_evaluations:\n            target_resource = self.resource_for_acquisition(\n                state, self.configspace_ext.resource_attr_name, **kwargs)\n        else:\n            # Any valid value works here:\n            target_resource = self.configspace_ext.resource_attr_range[0]\n        blacklisted_candidates = self._get_blacklisted_candidates(target_resource)\n        pick_random = (len(blacklisted_candidates) < self.num_initial_random_choices) or \\\n            (not state.candidate_evaluations)\n        if self.debug_log is not None:\n            self.debug_log.start_get_config(\'random\' if pick_random else \'BO\')\n        if pick_random:\n            config = None\n            if self.first_is_default and (not blacklisted_candidates):\n                # Use default configuration if there is one specified\n                default_config = self.hp_ranges.config_space.get_default_configuration()\n                if default_config and len(default_config.get_dictionary()) > 0:\n                    config = default_config\n                    if self.debug_log is not None:\n                        logger.info(""Start with default config:\\n{}"".format(\n                            candidate_for_print(config)))\n            if config is None:\n                if self.do_profile:\n                    self.profiler.start(\'random\')\n                config, _ = draw_random_candidate(\n                    blacklisted_candidates, self.configspace_ext,\n                    self.random_state, target_resource)\n                if self.do_profile:\n                    self.profiler.stop(\'random\')\n        else:\n            # Obtain current GPMXNetModel from state transformer. Based on\n            # this, the BO algorithm components can be constructed\n            state = self.state_transformer.state\n            if self.do_profile:\n                self.profiler.start(\'total_all\')\n                self.profiler.start(\'total_update\')\n            # Note: Asking for the model triggers the posterior computation\n            model = self.state_transformer.model()\n            if self.do_profile:\n                self.profiler.stop(\'total_update\')\n            # BO should only search over configs at resource level\n            # target_resource\n            self._fix_resource_attribute(target_resource)\n            # Create BO algorithm\n            initial_candidates_scorer = create_initial_candidates_scorer(\n                self.initial_scoring, model, self.acquisition_class,\n                self.random_state)\n            local_optimizer = self.local_minimizer_class(\n                state, model, self.acquisition_class)\n            # Make sure not to use the same random seed for each call:\n            #random_seed = compute_random_seed({\'0\': state}, self.random_seed)\n            bo_algorithm = BayesianOptimizationAlgorithm(\n                initial_candidates_generator=self.random_generator,\n                initial_candidates_scorer=initial_candidates_scorer,\n                num_initial_candidates=self.num_initial_candidates,\n                local_optimizer=local_optimizer,\n                pending_candidate_state_transformer=None,\n                blacklisted_candidates=blacklisted_candidates,\n                num_requested_candidates=1,\n                greedy_batch_selection=False,\n                duplicate_detector=DuplicateDetectorIdentical(),\n                profiler=self.profiler,\n                sample_unique_candidates=False,\n                debug_log=self.debug_log)\n            # Next candidate decision\n            if self.do_profile:\n                self.profiler.start(\'total_nextcand\')\n            _config = bo_algorithm.next_candidates()\n            assert len(_config) > 0, \\\n                (""Failed to find a configuration not already chosen ""\n                 ""before. Maybe there are no free configurations left? ""\n                 ""The blacklist size is {}"".format(len(blacklisted_candidates)))\n            next_config = _config[0]\n            if self.do_profile:\n                self.profiler.stop(\'total_nextcand\')\n            # Remove resource attribute\n            config = self.configspace_ext.remove_resource(next_config)\n            if self.do_profile:\n                self.profiler.stop(\'total_all\')\n\n        if self.debug_log is not None:\n            self.debug_log.set_final_config(config)\n            # All get_config debug log info is only written here\n            self.debug_log.write_block()\n        if self.do_profile:\n            self.profiler.clear()\n            # Pull out profiling data from this block, add to _profile_record\n            accumulate_profiling_record(\n                self._profile_record, self.profiler, pick_random)\n\n        return config\n\n    def evaluation_failed(self, config: CS.Configuration):\n        # Remove all pending evaluations for config\n        self.cleanup_pending(config)\n        # Mark config as failed (which means it will be blacklisted in\n        # future get_config calls)\n        # We need to create an extended config by appending a resource\n        # attribute. Its value does not matter, because of how the blacklist\n        # is created\n        lowest_attr_value = self.configspace_ext.resource_attr_range[0]\n        config_ext = self.configspace_ext.get(config, lowest_attr_value)\n        self.state_transformer.mark_candidate_failed(config_ext)\n\n    def cleanup_pending(self, config: CS.Configuration):\n        """"""\n        Removes all pending candidates whose configuration (i.e., lacking the\n        resource attribute) is equal to config.\n        This should be called after an evaluation terminates. For various\n        reasons (e.g., termination due to convergence), pending candidates\n        for this evaluation may still be present.\n        It is also called for a failed evaluation.\n\n        :param config: See above\n        """"""\n        config_dct = config.get_dictionary()\n\n        def filter_pred(x: PendingEvaluation) -> bool:\n            x_dct = self.configspace_ext.remove_resource(\n                x.candidate, as_dict=True)\n            return (x_dct != config_dct)\n\n        self.state_transformer.filter_pending_evaluations(filter_pred)\n\n    def remove_case(self, config: CS.Configuration, resource: int):\n        config_ext = self.configspace_ext.get(config, resource)\n        self.state_transformer.drop_candidate(config_ext)\n\n    def set_map_resource_to_index(self, map_fun: Callable[[float], int]):\n        """"""\n        map_fun is a function mapping the normalized resource level\n        r / max_t in (0, 1] to an index for the rung level. The function is\n        increasing.\n\n        If self.gp_model uses a kernel function that needs map_fun, configure\n        the kernel function here.\n\n        :param map_fun: See above\n        """"""\n        pass\n\n    def dataset_size(self):\n        return len(self.state_transformer.state.candidate_evaluations)\n\n    def cumulative_profile_record(self):\n        """"""\n        If profiling is activated, we sum up the profiling blocks for each\n        call of get_config and return it as dict. See get_config for what\n        is recorded:\n        - num_random: Number of get_config calls with random selection\n        - num_model: Number of get_config calls with model-based selection\n        - total_all: Sum of total times for all get_config calls\n        """"""\n        return self._profile_record\n\n    def get_params(self):\n        """"""\n        Note: Once MCMC is supported, this method will have to be refactored.\n\n        :return: Dictionary with current hyperparameter values\n        """"""\n        return self.state_transformer.get_params()\n\n    def set_params(self, param_dict):\n        self.state_transformer.set_params(param_dict)\n\n    def get_state(self):\n        """"""\n        The mutable state consists of the GP model parameters, the\n        TuningJobState, and the skip_optimization predicate (which can have a\n        mutable state).\n        We assume that skip_optimization can be pickled.\n\n        """"""\n        state = {\n            \'model_params\': self.get_params(),\n            \'state\': encode_state(self.state_transformer.state),\n            \'skip_optimization\': self.state_transformer.skip_optimization,\n            \'random_state\': self.random_state}\n        if self.debug_log is not None:\n            state[\'debug_log\'] = self.debug_log.get_mutable_state()\n        return state\n\n    def clone_from_state(self, state):\n        # Create clone with mutable state taken from \'state\'\n        model_args = self.state_transformer._model_args\n        init_state = decode_state(\n            state[\'state\'], self.configspace_ext.hp_ranges_ext)\n        skip_optimization = state[\'skip_optimization\']\n        new_searcher = GPMultiFidelitySearcher(\n            hp_ranges=self.hp_ranges,\n            resource_attr_key=self.configspace_ext.resource_attr_key,\n            resource_attr_range=self.configspace_ext.resource_attr_range,\n            random_seed=self.random_seed,\n            gpmodel=self.state_transformer._gpmodel,\n            model_args=model_args,\n            map_reward=self.map_reward,\n            acquisition_class=self.acquisition_class,\n            resource_for_acquisition=self.resource_for_acquisition,\n            init_state=init_state,\n            local_minimizer_class=self.local_minimizer_class,\n            skip_optimization=skip_optimization,\n            num_initial_candidates=self.num_initial_candidates,\n            num_initial_random_choices=self.num_initial_random_choices,\n            initial_scoring=self.initial_scoring,\n            profiler=self.profiler,\n            first_is_default=self.first_is_default,\n            debug_log=self.debug_log)\n        new_searcher.state_transformer.set_params(state[\'model_params\'])\n        new_searcher.random_state = state[\'random_state\']\n        new_searcher.random_generator.random_state = \\\n            state[\'random_state\']\n        if self.debug_log and \'debug_log\' in state:\n            new_searcher.debug_log.set_mutable_state(state[\'debug_log\'])\n        # Invalidate self (must not be used afterwards)\n        self.state_transformer = None\n        return new_searcher\n\n\ndef draw_random_candidate(\n        blacklisted_candidates: Set[CS.Configuration],\n        configspace_ext: ExtendedConfiguration,\n        random_state: np.random.RandomState,\n        target_resource: int) -> (CS.Configuration, CS.Configuration):\n    config, config_ext = None, None\n    for _ in range(GET_CONFIG_RANDOM_RETRIES):\n        _config = configspace_ext.hp_ranges.random_candidate(random_state)\n        # Test whether this config has already been considered\n        _config_ext = configspace_ext.remap_resource(\n            _config, target_resource)\n        if _config_ext not in blacklisted_candidates:\n            config = _config\n            config_ext = _config_ext\n            break\n    if config is None:\n        raise AssertionError(\n            ""Failed to sample a configuration not already chosen ""\n            ""before. Maybe there are no free configurations left? ""\n            ""The blacklist size is {}"".format(len(blacklisted_candidates)))\n    return config, config_ext\n\n\ndef _max_at_least_k(k, counter=None):\n    """"""\n    Get largest key of `counter` whose value is at least `k`.\n\n    :param counter: dict with keys that support comparison operators\n    :param k: lower bound\n    :return: largest key of `counter`\n    """"""\n    assert counter, ""counter must be non-empty and not `None`.""\n    return max(filter(lambda r: counter[r] >= k, counter.keys()),\n               default=min(counter.keys()))\n\n\ndef resource_for_acquisition_bohb(threshold: int) -> Callable[..., int]:\n    """"""\n    Factory for resource_for_acquisition argument in GPMultiFidelitySearcher.\n    Mirrors what is done in the BOHB algorithm. An integer threshold is given\n    at construction.\n    We return the largest resource level whose number of labeled candidates\n    is >= the threshold. If none of the levels passes the threshold, the\n    lowest level is returned.\n\n    """"""\n    def rfa_map(state: TuningJobState, resource_attr_name: str, **kwargs) -> int:\n        assert state.candidate_evaluations, ""state must not be empty""\n        histogram = Counter([int(x.candidate.get_dictionary()[resource_attr_name])\n                             for x in state.candidate_evaluations])\n        return _max_at_least_k(threshold, histogram)\n\n    return rfa_map\n\n\ndef resource_for_acquisition_first_milestone(\n        state: TuningJobState, resource_attr_name: str, **kwargs) -> int:\n    """"""\n    Implementation for resource_for_acquisition argument in\n    GPMultiFidelitySearcher. We assume that the scheduler passes the first\n    milestone to be attained by the new config as kwargs[\'milestone\']. This is\n    returned as resource level.\n\n    """"""\n    assert \'milestone\' in kwargs, \\\n        ""Need the first milestone to be attained by the new config passed as ""\\\n        ""kwargs[\'milestone\']. Use a scheduler which does that (in particular, ""\\\n        ""Hyperband_Scheduler)""\n    return kwargs[\'milestone\']\n'"
autogluon/searcher/bayesopt/autogluon/gp_profiling.py,0,"b'from typing import NamedTuple\nimport time\nimport logging\n\nfrom autogluon.searcher.bayesopt.datatypes.tuning_job_state import \\\n    TuningJobState\n\nlogger = logging.getLogger(__name__)\n\n\nclass ProfilingData(NamedTuple):\n    id: int\n    tag: str\n    duration: float\n    num_labeled: int\n    num_pending: int\n    fit_hyperparams: bool\n\n    def to_tuple(self):\n        return (self.id,\n                self.tag,\n                self.duration,\n                self.num_labeled,\n                self.num_pending,\n                self.fit_hyperparams)\n\n    @staticmethod\n    def field_names():\n        return (\'id\',\n                \'tag\',\n                \'duration\',\n                \'num_labeled\',\n                \'num_pending\',\n                \'fit_hyperparams\')\n\n\nclass GPMXNetSimpleProfiler(object):\n    def __init__(self):\n        self.records = list()\n        self.block = None\n        self.start_time = dict()\n        self.id_counter = 0\n\n    def set_state(self, state: TuningJobState,\n                  fit_hyperparams: bool):\n        assert not self.start_time, \\\n            ""Timers for these tags still running:\\n{}"".format(\n                self.start_time.keys())\n        self.block = ProfilingData(\n            id=self.id_counter, tag=\'\', duration=0.,\n            num_labeled=len(state.candidate_evaluations),\n            num_pending=len(state.pending_evaluations),\n            fit_hyperparams=fit_hyperparams\n        )\n        self.id_counter += 1\n\n    def start(self, tag: str):\n        assert self.block is not None\n        assert tag not in self.start_time, \\\n            ""Timer for \'{}\' already running"".format(tag)\n        self.start_time[tag] = time.process_time()\n\n    def stop(self, tag: str):\n        assert tag in self.start_time, \\\n            ""Timer for \'{}\' does not exist"".format(tag)\n        duration = time.process_time() - self.start_time[tag]\n        self.records.append(\n            self.block._replace(duration=duration, tag=tag))\n        del self.start_time[tag]\n\n    def clear(self):\n        remaining_tags = list(self.start_time.keys())\n        if remaining_tags:\n            logger.warning(""Timers for these tags not stopped (will be removed):\\n{}"".format(\n                remaining_tags))\n        self.start_time = dict()\n'"
autogluon/searcher/bayesopt/autogluon/hp_ranges.py,0,"b'import numpy as np\nimport ConfigSpace as CS\nfrom typing import List, Tuple\n\nfrom autogluon.searcher.bayesopt.datatypes.common import Candidate\nfrom autogluon.searcher.bayesopt.datatypes.hp_ranges import HyperparameterRanges\n\n\nclass HyperparameterRanges_CS(HyperparameterRanges):\n    def __init__(self, config_space: CS.ConfigurationSpace,\n                 name_last_pos: str = None,\n                 value_for_last_pos = None):\n        """"""\n        If name_last_pos is given, the hyperparameter of that name is assigned\n        the final position in the vector returned by to_ndarray. This can be\n        used to single out the (time) resource for a GP model, where that\n        component has to come last.\n\n        If in this case (name_last_pos given), value_for_last_pos is also given,\n        some methods are modified:\n        - random_candidate samples a config as normal, but then overwrites the\n          name_last_pos component by value_for_last_pos\n        - get_ndarray_bounds works as normal, but returns bound (a, a) for\n          name_last_pos component, where a is the internal value corresponding\n          to value_for_last_pos\n        The use case is HPO with a resource attribute. This attribute should be\n        fixed when optimizing the acquisition function, but can take different\n        values in the evaluation data (coming from all previous searches).\n\n        :param config_space: ConfigurationSpace\n        :param name_last_pos: See above. Default: None\n        :param value_for_last_pos: See above. Default: None\n        """"""\n        self.config_space = config_space\n        self.name_last_pos = name_last_pos\n        self.value_for_last_pos = value_for_last_pos\n        # Supports conversion to ndarray\n        numer_src = []\n        numer_trg = []\n        categ_src = []\n        categ_trg = []\n        categ_card = []\n        trg_pos = 0\n        append_at_end = None\n        for src_pos, hp in enumerate(config_space.get_hyperparameters()):\n            if isinstance(hp, CS.CategoricalHyperparameter):\n                card = hp.num_choices\n                if hp.name == name_last_pos:\n                    assert append_at_end is None\n                    append_at_end = (src_pos, card, True)\n                else:\n                    categ_src.append(src_pos)\n                    categ_trg.append(trg_pos)\n                    categ_card.append(card)\n                    trg_pos += card\n            elif isinstance(hp, CS.UniformIntegerHyperparameter) or \\\n                    isinstance(hp, CS.UniformFloatHyperparameter):\n                if hp.name == name_last_pos:\n                    assert append_at_end is None\n                    append_at_end = (src_pos, 1, False)\n                else:\n                    numer_src.append(src_pos)\n                    numer_trg.append(trg_pos)\n                    trg_pos += 1\n            else:\n                raise NotImplementedError(\n                    ""We only support hyperparameters of type ""\n                    ""CategoricalHyperparameter, UniformIntegerHyperparameter, ""\n                    ""UniformFloatHyperparameter"")\n        if append_at_end is not None:\n            if append_at_end[2]:\n                categ_src.append(append_at_end[0])\n                categ_trg.append(trg_pos)\n                categ_card.append(append_at_end[1])\n            else:\n                numer_src.append(append_at_end[0])\n                numer_trg.append(trg_pos)\n            trg_pos += append_at_end[1]\n        self.numer_src = np.array(numer_src, dtype=np.int64)\n        self.numer_trg = np.array(numer_trg, dtype=np.int64)\n        self.categ_src = np.array(categ_src, dtype=np.int64)\n        self.categ_trg = np.array(categ_trg, dtype=np.int64)\n        self.categ_card = np.array(categ_card, dtype=np.int64)\n        self._ndarray_size = trg_pos\n\n    def to_ndarray(self, cand_tuple: Candidate) -> np.ndarray:\n        assert isinstance(cand_tuple, CS.Configuration)\n        trgvec = np.zeros((self._ndarray_size,))\n        srcvec = cand_tuple.get_array()\n        # https://wesmckinney.com/blog/numpy-indexing-peculiarities/\n        # take, put much faster than []\n        trgvec.put(\n            self.numer_trg, srcvec.take(self.numer_src, mode=\'clip\'),\n            mode=\'clip\')\n        relpos = srcvec.take(self.categ_src, mode=\'clip\').astype(np.int64)\n        trgvec.put(self.categ_trg + relpos, [1.], mode=\'clip\')\n        return trgvec\n\n    def ndarray_size(self) -> int:\n        return self._ndarray_size\n\n    def from_ndarray(self, cand_ndarray: np.ndarray) -> Candidate:\n        assert cand_ndarray.size == self._ndarray_size, \\\n            ""Internal vector [{}] must have size {}"".format(\n                cand_ndarray, self._ndarray_size)\n        cand_ndarray = cand_ndarray.reshape((-1,))\n        assert cand_ndarray.min() >= 0. and cand_ndarray.max() <= 1., \\\n            ""Internal vector [{}] must have entries in [0, 1]"".format(\n                cand_ndarray)\n        # Deal with categoricals by using argmax\n        srcvec = np.zeros(self.__len__(), dtype=cand_ndarray.dtype)\n        srcvec.put(\n            self.numer_src, cand_ndarray.take(self.numer_trg, mode=\'clip\'),\n            mode=\'clip\')\n        for srcpos, trgpos, card in zip(\n                self.categ_src, self.categ_trg, self.categ_card):\n            maxpos = cand_ndarray[trgpos:(trgpos + card)].argmax()\n            srcvec[srcpos] = maxpos\n        # Rest is dealt with by CS.Configuration\n        return CS.Configuration(self.config_space, vector=srcvec)\n\n    def is_attribute_fixed(self):\n        return (self.name_last_pos is not None) and \\\n               (self.value_for_last_pos is not None)\n\n    def _fix_attribute_value(self, name):\n        return self.is_attribute_fixed() and name == self.name_last_pos\n\n    def get_ndarray_bounds(self) -> List[Tuple[float, float]]:\n        bounds = []\n        final_bound = None\n        for hp in self.config_space.get_hyperparameters():\n            if isinstance(hp, CS.CategoricalHyperparameter):\n                if not self._fix_attribute_value(hp.name):\n                    bound = [(0., 1.)] * len(hp.choices)\n                else:\n                    bound = [(0., 0.)] * len(hp.choices)\n                    bound[int(self.value_for_last_pos)] = (1., 1.)\n            else:\n                if not self._fix_attribute_value(hp.name):\n                    bound = [(0., 1.)]\n                else:\n                    val_int = float(hp._inverse_transform(\n                        np.array([self.value_for_last_pos])).item())\n                    bound = [(val_int, val_int)]\n            if hp.name == self.name_last_pos:\n                final_bound = bound\n            else:\n                bounds.extend(bound)\n        if final_bound is not None:\n            bounds.extend(final_bound)\n        return bounds\n\n    # NOTE: Assumes that config argument not used afterwards...\n    def _transform_config(self, config: CS.Configuration) -> CS.Configuration:\n        values = config.get_dictionary()  # No copy is done here\n        values[self.name_last_pos] = self.value_for_last_pos\n        return CS.Configuration(self.config_space, values=values)\n\n    def random_candidate(self, random_state) -> Candidate:\n        self.config_space.random = random_state  # Not great...\n        rnd_config = self.config_space.sample_configuration()\n        if self.is_attribute_fixed():\n            rnd_config = self._transform_config(rnd_config)\n        return rnd_config\n\n    def random_candidates(\n            self, random_state, num_configs: int) -> List[Candidate]:\n        self.config_space.random = random_state  # Not great...\n        rnd_configs = self.config_space.sample_configuration(num_configs)\n        if self.is_attribute_fixed():\n            rnd_configs = [self._transform_config(x) for x in rnd_configs]\n        return rnd_configs\n\n    def __repr__(self) -> str:\n        return ""{}{}"".format(\n            self.__class__.__name__, repr(self.config_space))\n\n    def __eq__(self, other: object) -> bool:\n        if isinstance(other, HyperparameterRanges_CS):\n            return self.config_space == other.config_space\n        return False\n\n    def __len__(self) -> int:\n        return len(self.config_space.get_hyperparameters())\n\n    # This is just for convenience, roughly shadows what is done in\n    # HyperparameterRanges_Impl\n    @property\n    def hp_ranges(self) -> List[CS.hyperparameters.Hyperparameter]:\n        return self.config_space.get_hyperparameters()\n\n    def filter_for_last_pos_value(\n            self, candidates: List[Candidate]) -> List[Candidate]:\n        """"""\n        If is_attribute_fixed, the candidates list is filtered by removing\n        entries whose name_last_pos attribute value is different from\n        value_for_last_pos. Otherwise, candidates is returned unchanged.\n\n        """"""\n        if self.is_attribute_fixed():\n            def filter_pred(x: CS.Configuration) -> bool:\n                x_dct = x.get_dictionary()\n                return (x_dct[self.name_last_pos] == self.value_for_last_pos)\n\n            candidates = list(filter(filter_pred, candidates))\n        return candidates\n'"
autogluon/searcher/bayesopt/autogluon/model_factories.py,0,"b'from mxnet import gluon\n\nfrom autogluon.searcher.bayesopt.gpmxnet.kernel import KernelFunction, \\\n    Matern52, ExponentialDecayResourcesKernelFunction, \\\n    ExponentialDecayResourcesMeanFunction\nfrom autogluon.searcher.bayesopt.gpmxnet.warping import WarpedKernel, Warping\n\n\ndef resource_kernel_factory(\n        name: str, kernel_x: KernelFunction, mean_x: gluon.HybridBlock,\n        max_metric_value: float) -> (KernelFunction, gluon.HybridBlock):\n    """"""\n    Given kernel function kernel_x and mean function mean_x over config x,\n    create kernel and mean functions over (x, r), where r is the resource\n    attribute (nonnegative scalar, usually in [0, 1]).\n\n    :param name: Selects resource kernel type\n    :param kernel_x: Kernel function over configs x\n    :param mean_x: Mean function over configs x\n    :return: res_kernel, res_mean, both over (x, r)\n\n    """"""\n    if name == \'matern52\':\n        res_kernel = Matern52(dimension=kernel_x.dimension + 1, ARD=True)\n        res_mean = mean_x\n    elif name == \'matern52-res-warp\':\n        # Warping on resource dimension (last one)\n        dim_x = kernel_x.dimension\n        res_warping = Warping(\n            dimension=dim_x + 1, index_to_range={dim_x: (0., 1.)})\n        res_kernel = WarpedKernel(\n            kernel=Matern52(dimension=dim_x + 1, ARD=True),\n            warping=res_warping)\n        res_mean = mean_x\n    else:\n        if name == \'exp-decay-sum\':\n            delta_fixed_value = 0.0\n        elif name == \'exp-decay-combined\':\n            delta_fixed_value = None\n        elif name == \'exp-decay-delta1\':\n            delta_fixed_value = 1.0\n        else:\n            raise AssertionError(""name = \'{}\' not supported"".format(name))\n        res_kernel = ExponentialDecayResourcesKernelFunction(\n            kernel_x, mean_x, gamma_init=0.5 * max_metric_value,\n            delta_fixed_value=delta_fixed_value,\n            max_metric_value=max_metric_value)\n        res_mean = ExponentialDecayResourcesMeanFunction(\n            kernel=res_kernel)\n\n    return res_kernel, res_mean\n'"
autogluon/searcher/bayesopt/autogluon/searcher_factory.py,0,"b'from typing import Set\n\nfrom autogluon.searcher.bayesopt.autogluon.model_factories import \\\n    resource_kernel_factory\nfrom autogluon.searcher.bayesopt.autogluon.gp_fifo_searcher import \\\n    GPFIFOSearcher, map_reward, MapReward, DEFAULT_INITIAL_SCORING, \\\n    SUPPORTED_INITIAL_SCORING\nfrom autogluon.searcher.bayesopt.autogluon.gp_multifidelity_searcher import \\\n    GPMultiFidelitySearcher, resource_for_acquisition_bohb, \\\n    resource_for_acquisition_first_milestone\nfrom autogluon.searcher.bayesopt.autogluon.hp_ranges import \\\n    HyperparameterRanges_CS\nfrom autogluon.searcher.bayesopt.autogluon.gp_profiling import GPMXNetSimpleProfiler\nfrom autogluon.searcher.bayesopt.models.gpmxnet_skipopt import \\\n    SkipNoMaxResourcePredicate, SkipPeriodicallyPredicate\nfrom autogluon.searcher.bayesopt.gpmxnet.kernel import Matern52\nfrom autogluon.searcher.bayesopt.gpmxnet.mean import ScalarMeanFunction\nfrom autogluon.searcher.bayesopt.gpmxnet.constants import OptimizationConfig, \\\n    DEFAULT_OPTIMIZATION_CONFIG\nfrom autogluon.searcher.bayesopt.gpmxnet.gp_regression import \\\n    GaussianProcessRegression\nfrom autogluon.searcher.bayesopt.models.gpmxnet_transformers import \\\n    GPMXNetModelArgs\nfrom autogluon.searcher.bayesopt.models.nphead_acqfunc import \\\n    EIAcquisitionFunction\nfrom autogluon.searcher.default_arguments import Integer, Categorical, Boolean\nfrom autogluon.searcher.bayesopt.tuning_algorithms.default_algorithm import \\\n    DEFAULT_NUM_INITIAL_CANDIDATES, DEFAULT_NUM_INITIAL_RANDOM_EVALUATIONS\nfrom autogluon.searcher.bayesopt.datatypes.hp_ranges import HyperparameterRanges  # DEBUG!\nfrom autogluon.searcher.bayesopt.tuning_algorithms.default_algorithm import \\\n    DEFAULT_METRIC\nfrom autogluon.searcher.bayesopt.autogluon.debug_log import DebugLogPrinter\nfrom autogluon.searcher.bayesopt.gpmxnet.debug_gp_regression import \\\n    DebugGPRegression\n\n__all__ = [\'gp_fifo_searcher_factory\',\n           \'gp_multifidelity_searcher_factory\',\n           \'from_argparse\',\n           \'gp_fifo_searcher_defaults\',\n           \'gp_multifidelity_searcher_defaults\']\n\n\ndef _create_common_objects(**kwargs):\n    # TODO: Validity checks on kwargs arguments\n    scheduler = kwargs[\'scheduler\']\n    config_space = kwargs[\'configspace\']\n    is_hyperband = scheduler.startswith(\'hyperband\')\n    if kwargs.get(\'debug_use_hyperparameter_ranges\', False):\n        assert isinstance(config_space, HyperparameterRanges)\n        assert not is_hyperband, \\\n            ""Cannot use debug_use_hyperparameter_ranges with Hyperband scheduling""\n        hp_ranges_cs = config_space\n    else:\n        import ConfigSpace as CS\n        assert isinstance(config_space, CS.ConfigurationSpace)\n        hp_ranges_cs = HyperparameterRanges_CS(config_space)\n    # Note: This base random seed is used to create different random seeds for\n    # each BO get_config call internally\n    random_seed = kwargs.get(\'random_seed\', 31415927)\n    # Skip optimization predicate for GP surrogate model\n    if kwargs.get(\'opt_skip_num_max_resource\', False) and is_hyperband:\n        skip_optimization = SkipNoMaxResourcePredicate(\n            init_length=kwargs[\'opt_skip_init_length\'],\n            resource_attr_name=kwargs[\'resource_attribute\'],\n            max_resource=kwargs[\'max_epochs\'])\n    elif kwargs.get(\'opt_skip_period\', 1) > 1:\n        skip_optimization = SkipPeriodicallyPredicate(\n            init_length=kwargs[\'opt_skip_init_length\'],\n            period=kwargs[\'opt_skip_period\'])\n    else:\n        skip_optimization = None\n    # Profiler\n    if kwargs.get(\'profiler\', False):\n        profiler = GPMXNetSimpleProfiler()\n    else:\n        profiler = None\n    # Conversion from reward to metric (strictly decreasing) and back\n    _map_reward = kwargs.get(\'map_reward\', \'1_minus_x\')\n    if isinstance(_map_reward, str):\n        _map_reward_name = _map_reward\n        supp_map_reward = {\'1_minus_x\', \'minus_x\'}\n        assert _map_reward_name in supp_map_reward, \\\n            ""This factory needs map_reward in {}"".format(supp_map_reward)\n        _map_reward: MapReward = map_reward(\n            const=1.0 if _map_reward_name == \'1_minus_x\' else 0.0)\n    else:\n        assert isinstance(_map_reward, MapReward), \\\n            ""map_reward must either be string or of MapReward type""\n    if is_hyperband:\n        # Note: \'min_reward\' is needed only to support the exp-decay\n        # surrogate model. If not given, it is assumed to be 0.\n        min_reward = kwargs.get(\'min_reward\', 0)\n        max_metric_value = _map_reward(min_reward)\n    else:\n        max_metric_value = None\n    opt_warmstart = kwargs.get(\'opt_warmstart\', False)\n\n    # Underlying GP regression model\n    kernel = Matern52(dimension=hp_ranges_cs.ndarray_size(), ARD=True)\n    mean = ScalarMeanFunction()\n    if is_hyperband:\n        kernel, mean = resource_kernel_factory(\n            kwargs[\'gp_resource_kernel\'],\n            kernel_x=kernel, mean_x=mean,\n            max_metric_value=max_metric_value)\n    optimization_config = OptimizationConfig(\n        lbfgs_tol=DEFAULT_OPTIMIZATION_CONFIG.lbfgs_tol,\n        lbfgs_maxiter=kwargs[\'opt_maxiter\'],\n        verbose=kwargs[\'opt_verbose\'],\n        n_starts=kwargs[\'opt_nstarts\'])\n    debug_writer = None\n    if kwargs.get(\'opt_debug_writer\', False):\n        fname_msk = kwargs.get(\'opt_debug_writer_fmask\', \'debug_gpr_{}\')\n        debug_writer = DebugGPRegression(\n            fname_msk=fname_msk, rolling_size=5)\n    gpmodel = GaussianProcessRegression(\n        kernel=kernel, mean=mean,\n        optimization_config=optimization_config,\n        fit_reset_params=not opt_warmstart,\n        debug_writer=debug_writer)\n    model_args = GPMXNetModelArgs(\n        num_fantasy_samples=kwargs[\'num_fantasy_samples\'],\n        random_seed=random_seed,\n        active_metric=DEFAULT_METRIC,\n        normalize_targets=True)\n    debug_log = DebugLogPrinter() if kwargs.get(\'debug_log\', False) else None\n\n    return hp_ranges_cs, random_seed, gpmodel, model_args, profiler, \\\n           _map_reward, skip_optimization, debug_log\n\n\ndef gp_fifo_searcher_factory(**kwargs) -> GPFIFOSearcher:\n    """"""\n    Creates GPFIFOSearcher object, based on kwargs equal to search_options\n    passed to and extended by scheduler (see FIFOScheduler).\n\n    Extensions of kwargs by the scheduler:\n    - scheduler: Name of scheduler (\'fifo\', \'hyperband_*\')\n    - configspace: CS.ConfigurationSpace (or HyperparameterRanges if\n      debug_use_hyperparameter_ranges is true)\n    Only Hyperband schedulers:\n    - resource_attribute: Name of resource (or time) attribute\n    - min_epochs: Smallest resource value being rung level\n    - max_epochs: Maximum resource value\n\n    :param kwargs: search_options coming from scheduler\n    :return: GPFIFOSearcher object\n\n    """"""\n    assert kwargs[\'scheduler\'] == \'fifo\', \\\n        ""This factory needs scheduler = \'fifo\' (instead of \'{}\')"".format(\n            kwargs[\'scheduler\'])\n    # Common objects\n    hp_ranges_cs, random_seed, gpmodel, model_args, profiler, _map_reward, \\\n    skip_optimization, debug_log = \\\n        _create_common_objects(**kwargs)\n\n    gp_searcher = GPFIFOSearcher(\n        hp_ranges=hp_ranges_cs,\n        random_seed=random_seed,\n        gpmodel=gpmodel,\n        model_args=model_args,\n        map_reward=_map_reward,\n        acquisition_class=EIAcquisitionFunction,\n        skip_optimization=skip_optimization,\n        num_initial_candidates=kwargs[\'num_init_candidates\'],\n        num_initial_random_choices=kwargs[\'num_init_random\'],\n        initial_scoring=kwargs[\'initial_scoring\'],\n        profiler=profiler,\n        first_is_default=kwargs[\'first_is_default\'],\n        debug_log=debug_log)\n    return gp_searcher\n\n\ndef gp_multifidelity_searcher_factory(**kwargs) -> GPMultiFidelitySearcher:\n    """"""\n    Creates GPMultiFidelitySearcher object, based on kwargs equal to search_options\n    passed to and extended by scheduler (see HyperbandScheduler).\n\n    :param kwargs: search_options coming from scheduler\n    :return: GPMultiFidelitySearcher object\n\n    """"""\n    supp_schedulers = {\'hyperband_stopping\', \'hyperband_promotion\'}\n    assert kwargs[\'scheduler\'] in supp_schedulers, \\\n        ""This factory needs scheduler in {} (instead of \'{}\')"".format(\n            supp_schedulers, kwargs[\'scheduler\'])\n    # Common objects\n    hp_ranges_cs, random_seed, gpmodel, model_args, profiler, _map_reward,\\\n    skip_optimization, debug_log = \\\n        _create_common_objects(**kwargs)\n\n    _resource_acq = kwargs.get(\'resource_acq\', \'bohb\')\n    if _resource_acq == \'bohb\':\n        resource_for_acquisition = resource_for_acquisition_bohb(\n            threshold=hp_ranges_cs.ndarray_size())\n    else:\n        assert _resource_acq == \'first\', \\\n            ""resource_acq must be \'bohb\' or \'first\'""\n        resource_for_acquisition = resource_for_acquisition_first_milestone\n    epoch_range = (kwargs[\'min_epochs\'], kwargs[\'max_epochs\'])\n    gp_searcher = GPMultiFidelitySearcher(\n        hp_ranges=hp_ranges_cs,\n        resource_attr_key=kwargs[\'resource_attribute\'],\n        resource_attr_range=epoch_range,\n        random_seed=random_seed,\n        gpmodel=gpmodel,\n        model_args=model_args,\n        map_reward=_map_reward,\n        acquisition_class=EIAcquisitionFunction,\n        resource_for_acquisition=resource_for_acquisition,\n        skip_optimization=skip_optimization,\n        num_initial_candidates=kwargs[\'num_init_candidates\'],\n        num_initial_random_choices=kwargs[\'num_init_random\'],\n        initial_scoring=kwargs[\'initial_scoring\'],\n        profiler=profiler,\n        first_is_default=kwargs[\'first_is_default\'],\n        debug_log=debug_log)\n    return gp_searcher\n\n\ndef from_argparse(args) -> (dict, dict):\n    """"""\n    Given result from ArgumentParser.parse_args() from run_benchmarks script,\n    create both search_options (kwargs in XYZ_searcher_factory above) and\n    scheduler_options.\n\n    :param args: See above\n    :return: search_options, scheduler_options\n\n    """"""\n    # Options for searcher\n    search_options = dict()\n    search_options[\'random_seed\'] = args.run_id  # TODO: Change this\n    search_options[\'opt_skip_num_max_resource\'] = args.opt_skip_num_max_resource\n    search_options[\'opt_skip_init_length\'] = args.opt_skip_init_length\n    search_options[\'opt_skip_period\'] = args.opt_skip_period\n    search_options[\'profiler\'] = args.profiler\n    search_options[\'gp_resource_kernel\'] = args.gp_searcher_resource_kernel\n    search_options[\'opt_maxiter\'] = args.opt_maxiter\n    search_options[\'opt_nstarts\'] = args.opt_nstarts\n    search_options[\'opt_warmstart\'] = args.opt_warmstart\n    search_options[\'opt_verbose\'] = args.opt_verbose\n    search_options[\'opt_debug_writer\'] = args.opt_debug_writer\n    if args.opt_debug_writer:\n        pref = \'debug_gpr_{}\'.format(args.run_id)\n        search_options[\'opt_debug_writer_fmask\'] = pref + \'_{}\'\n    search_options[\'num_fantasy_samples\'] = args.gp_searcher_num_fantasy_samples\n    search_options[\'num_init_random\'] = args.gp_searcher_num_init_random\n    search_options[\'num_init_candidates\'] = args.gp_searcher_num_init_candidates\n    search_options[\'resource_acq\'] = args.gp_searcher_resource_acq\n    search_options[\'first_is_default\'] = not args.first_is_not_default\n    search_options[\'debug_log\'] = args.debug_log\n    search_options[\'initial_scoring\'] = args.gp_searcher_initial_scoring\n\n    # Options for scheduler\n    scheduler_options = dict()\n    scheduler_options[\'num_trials\'] = args.num_trials\n    scheduler_options[\'time_out\'] = args.scheduler_timeout\n    scheduler_options[\'checkpoint\'] = args.checkpoint\n    scheduler_options[\'resume\'] = args.resume\n    scheduler_options[\'scheduler\'] = args.scheduler\n    if args.scheduler == \'hyperband_stopping\':\n        scheduler_options[\'type\'] = \'stopping\'\n    elif args.scheduler == \'hyperband_promotion\':\n        scheduler_options[\'type\'] = \'promotion\'\n    scheduler_options[\'reduction_factor\'] = args.reduction_factor\n    scheduler_options[\'max_t\'] = args.epochs\n    scheduler_options[\'grace_period\'] = args.grace_period\n    scheduler_options[\'brackets\'] = args.brackets\n    scheduler_options[\'maxt_pending\'] = args.maxt_pending\n    scheduler_options[\'keep_size_ratios\'] = args.scheduler_keep_size_ratios\n    scheduler_options[\'searcher_data\'] = args.gp_searcher_data\n    scheduler_options[\'store_results_period\'] = args.store_results_period\n    scheduler_options[\'delay_get_config\'] = not args.no_delay_get_config\n\n    return search_options, scheduler_options\n\n\ndef _common_defaults(is_hyperband: bool) -> (Set[str], dict, dict):\n    mandatory = set()\n\n    default_options = {\n        \'random_seed\': 31415927,\n        \'opt_skip_init_length\': 150,\n        \'opt_skip_period\': 1,\n        \'profiler\': False,\n        \'opt_maxiter\': 50,\n        \'opt_nstarts\': 2,\n        \'opt_warmstart\': False,\n        \'opt_verbose\': False,\n        \'opt_debug_writer\': False,\n        \'num_fantasy_samples\': 20,\n        \'num_init_random\': DEFAULT_NUM_INITIAL_RANDOM_EVALUATIONS,\n        \'num_init_candidates\': DEFAULT_NUM_INITIAL_CANDIDATES,\n        \'initial_scoring\': DEFAULT_INITIAL_SCORING,\n        \'first_is_default\': True,\n        \'debug_log\': False}\n    if is_hyperband:\n        default_options[\'opt_skip_num_max_resource\'] = False\n        default_options[\'gp_resource_kernel\'] = \'matern52\'\n        default_options[\'resource_acq\'] = \'bohb\'\n        default_options[\'num_init_random\'] = 10\n\n    constraints = {\n        \'random_seed\': Integer(),\n        \'opt_skip_init_length\': Integer(0, None),\n        \'opt_skip_period\': Integer(1, None),\n        \'profiler\': Boolean(),\n        \'opt_maxiter\': Integer(1, None),\n        \'opt_nstarts\': Integer(1, None),\n        \'opt_warmstart\': Boolean(),\n        \'opt_verbose\': Boolean(),\n        \'opt_debug_writer\': Boolean(),\n        \'num_fantasy_samples\': Integer(1, None),\n        \'num_init_random\': Integer(1, None),\n        \'num_init_candidates\': Integer(5, None),\n        \'initial_scoring\': Categorical(\n            choices=tuple(SUPPORTED_INITIAL_SCORING)),\n        \'first_is_default\': Boolean(),\n        \'debug_log\': Boolean()}\n    if is_hyperband:\n        constraints[\'opt_skip_num_max_resource\'] = Boolean()\n        constraints[\'gp_resource_kernel\'] = Categorical(choices=(\n            \'exp-decay-sum\', \'exp-decay-combined\', \'exp-decay-delta1\',\n            \'matern52\', \'matern52-res-warp\'))\n        constraints[\'resource_acq\'] = Categorical(\n            choices=(\'bohb\', \'first\'))\n\n    return mandatory, default_options, constraints\n\n\ndef gp_fifo_searcher_defaults() -> (Set[str], dict, dict):\n    """"""\n    Returns mandatory, default_options, config_space for\n    check_and_merge_defaults to be applied to search_options for\n    GPFIFOSearcher.\n\n    :return: (mandatory, default_options, config_space)\n\n    """"""\n    return _common_defaults(is_hyperband=False)\n\n\ndef gp_multifidelity_searcher_defaults() -> (Set[str], dict, dict):\n    """"""\n    Returns mandatory, default_options, config_space for\n    check_and_merge_defaults to be applied to search_options for\n    GPMultiFidelitySearcher.\n\n    :return: (mandatory, default_options, config_space)\n\n    """"""\n    return _common_defaults(is_hyperband=True)\n'"
autogluon/searcher/bayesopt/datatypes/__init__.py,0,b''
autogluon/searcher/bayesopt/datatypes/common.py,0,"b'from typing import Union, Tuple, NamedTuple, Dict\nimport numpy as np\nimport ConfigSpace as CS\n\n\n\n# Allows underlying BO code to be used with different basic types for a\n# candidate\nHyperparameter = Union[str, int, float]\n\nCandidate = Union[Tuple[Hyperparameter, ...], CS.Configuration]\n\n\ndef candidate_for_print(candidate: Candidate):\n    if isinstance(candidate, CS.Configuration):\n        return candidate.get_dictionary()\n    else:\n        return candidate\n\n\nclass StateIdAndCandidate(NamedTuple):\n    """"""\n    Just used in utils/test_objects.py, could probably be removed\n    """"""\n    state_id: str\n    candidate: Candidate\n\n\nclass CandidateEvaluation(NamedTuple):\n    candidate: Candidate\n    metrics: Dict[str, float]\n\n\nclass PendingEvaluation(object):\n    """"""\n    Maintains information for pending candidates (i.e. candidates which have\n    been queried for labeling, but target feedback has not yet been obtained.\n\n    The minimum information is the candidate which has been queried.\n    """"""\n    def __init__(self, candidate: Candidate):\n        super(PendingEvaluation, self).__init__()\n        self._candidate = candidate\n\n    @property\n    def candidate(self):\n        return self._candidate\n\n\nclass FantasizedPendingEvaluation(PendingEvaluation):\n    """"""\n    Here, latent target values are integrated out by Monte Carlo samples,\n    also called ""fantasies"".\n\n    """"""\n    def __init__(self, candidate: Candidate, fantasies: Dict[str, np.ndarray]):\n        super(FantasizedPendingEvaluation, self).__init__(candidate)\n        fantasy_sizes = [\n            fantasy_values.size for fantasy_values in fantasies.values()]\n        assert all(fantasy_size > 0 for fantasy_size in fantasy_sizes), \\\n            ""fantasies must be non-empty""\n        assert len(set(fantasy_sizes)) == 1, \\\n            ""fantasies must all have the same length""\n        self._fantasies = fantasies.copy()\n\n    @property\n    def fantasies(self):\n        return self._fantasies\n'"
autogluon/searcher/bayesopt/datatypes/hp_ranges.py,0,"b'from abc import ABC, abstractmethod\nfrom math import floor\nfrom typing import Tuple, List, Iterable, Dict\nimport numpy as np\n\nfrom autogluon.searcher.bayesopt.datatypes.common import Hyperparameter, \\\n    Candidate\nfrom autogluon.searcher.bayesopt.datatypes.scaling import Scaling\n\n\n# Epsilon margin to account for numerical errors\nEPS = 1e-8\n\n\nclass HyperparameterRange(ABC):\n    """"""\n    One can argue that this class does a bit too much: both definition of\n    ranges and things like normalization, for now we do not plan to make the\n    latter configurable, so it should be okay having everything in one place to\n    keeps things fairly simple\n    """"""\n    def __init__(self, name: str):\n        self._name = name\n\n    @property\n    def name(self) -> str:\n        return self._name\n\n    @abstractmethod\n    def to_ndarray(self, hp: Hyperparameter) -> np.ndarray:\n        pass\n\n    @abstractmethod\n    def from_ndarray(self, cand_ndarray: np.ndarray) -> Hyperparameter:\n        pass\n\n    def ndarray_size(self) -> int:\n        return 1\n\n    @abstractmethod\n    def from_zero_one(self, normalized_value: float) -> Hyperparameter:\n        """"""\n        Used to generate a random hp, takes as input a random value between 0.0\n        and 1.0, which should be generated uniformly at random. Returns a valid\n        active hyperparameter.\n\n        Normally is the same as from_ndarray, but in the warm start case it is\n        not. Will be limited to active ranges.\n        """"""\n        # by default this is the same as from_ndarray\n        pass\n\n    def random_hp(self, random_state) -> Hyperparameter:\n        return self.from_zero_one(random_state.uniform(0.0, 1.0))\n\n\ndef scale_from_zero_one(\n        value, lower_bound: float, upper_bound: float, scaling: Scaling):\n    assert 0.0 <= value <= 1.0, value\n    if lower_bound == upper_bound:\n        return lower_bound\n    else:\n        lower = scaling.to_internal(lower_bound)\n        upper = scaling.to_internal(upper_bound)\n        range = upper - lower\n        assert range > 0, (lower, upper)\n        internal_value = value * range + lower\n        hp = scaling.from_internal(internal_value)\n        # set value in case it is off due to numerical rounding\n        if hp < lower_bound:\n            hp = lower_bound\n        if hp > upper_bound:\n            hp = upper_bound\n        return hp\n\n\nclass HyperparameterRangeContinuous(HyperparameterRange):\n    def __init__(\n            self, name: str, lower_bound: float, upper_bound: float,\n            scaling: Scaling, active_lower_bound: float = None,\n            active_upper_bound: float = None):\n        """"""\n        Real valued hyperparameter.\n\n        :param name: unique name of the hyperparameter.\n        :param lower_bound: inclusive lower bound on all the values that\n            parameter can take.\n        :param upper_bound: inclusive upper bound on all the values that\n            parameter can take.\n        :param scaling: determines how the values of the parameter are enumerated internally.\n            The parameter value is expressed as parameter = scaling(internal), where internal\n            is internal representation of parameter, whcih is a real value, normally in range\n            [0, 1]. To optimize the parameter, the internal is varied, and the parameter to be\n            tested is calculated from such internal representation. This allows to speed up\n            on many machine learning problems, where the parameter being optimized adjusts\n            modelling capacity of the model. In such situation, often holds that\n            capacity = log(parameter). See e.g. VC dimension for finite set-family:\n            https://en.wikipedia.org/wiki/Vapnik\xe2\x80\x93Chervonenkis_dimension#Bounds\n        :param active_lower_bound: changes lower bound of the range of the parameter; Is used\n            within warmstart functionality.\n        :param active_upper_bound: changes upper bound on the values of parameter. Used for\n            warmstart.\n        """"""\n        super().__init__(name)\n        self.lower_bound = lower_bound\n        self.upper_bound = upper_bound\n        self.active_lower_bound = lower_bound if active_lower_bound is None else active_lower_bound\n        self.active_upper_bound = upper_bound if active_upper_bound is None else active_upper_bound\n\n        assert self.lower_bound <= self.active_upper_bound <= self.upper_bound\n        assert self.lower_bound <= self.active_lower_bound <= self.upper_bound\n\n        self.scaling = scaling\n\n    def to_ndarray(self, hp: float) -> np.ndarray:\n        assert isinstance(hp, float), (hp, self)\n        assert self.lower_bound <= hp <= self.upper_bound, (hp, self)\n        # convert everything to internal scaling, and then normalize between zero and one\n        lower = self.scaling.to_internal(self.lower_bound)\n        upper = self.scaling.to_internal(self.upper_bound)\n        if upper == lower:\n            result = 0.0  # if the bounds are fixed for a dimension\n        else:\n            result = (self.scaling.to_internal(hp) - lower) / (upper - lower)\n        assert 0.0 <= result <= 1.0, (result, self)\n        return np.array([result])\n\n    def from_ndarray(self, ndarray: np.ndarray) -> float:\n        scalar = ndarray.item()\n        return scale_from_zero_one(scalar, self.lower_bound,\n                                   self.upper_bound, self.scaling)\n\n    def __repr__(self) -> str:\n        return ""{}({}, {}, {}, {}, {}, {})"".format(\n            self.__class__.__name__, repr(self.name),\n            repr(self.active_lower_bound), repr(self.active_upper_bound),\n            repr(self.scaling), repr(self.lower_bound), repr(self.upper_bound)\n        )\n\n    def __eq__(self, other: object) -> bool:\n        if isinstance(other, HyperparameterRangeContinuous):\n            return self.name == other.name \\\n                   and self.lower_bound == other.lower_bound \\\n                   and self.upper_bound == other.upper_bound \\\n                   and self.scaling == other.scaling\n        return False\n\n    def from_zero_one(self, normalized_value: float) -> float:\n        return scale_from_zero_one(normalized_value, self.active_lower_bound,\n                                   self.active_upper_bound, self.scaling)\n\n\nclass HyperparameterRangeInteger(HyperparameterRange):\n    def __init__(self, name: str, lower_bound: int, upper_bound: int, scaling: Scaling,\n                 active_lower_bound: int = None, active_upper_bound: int = None):\n        """"""\n        Both bounds are INCLUDED in the valid values. Under the hood generates a continuous\n        range from lower_bound - 0.5 to upper_bound + 0.5.\n        See docs for continuous hyperparameter for more information.\n        """"""\n        super().__init__(name)\n        # reduce the range by epsilon at both ends, to avoid corner cases where numerical rounding\n        # would cause a value that would end up out of range by one\n        active_lower_bound = lower_bound if active_lower_bound is None \\\n            else active_lower_bound\n        active_upper_bound = upper_bound if active_upper_bound is None \\\n            else active_upper_bound\n\n        self.lower_bound = lower_bound\n        self.upper_bound = upper_bound\n        self.active_lower_bound = active_lower_bound\n        self.active_upper_bound = active_upper_bound\n        self.scaling = scaling\n\n        self._continuous_range = HyperparameterRangeContinuous(\n            name, lower_bound - 0.5 + EPS, upper_bound + 0.5 - EPS, scaling,\n            active_lower_bound - 0.5 + EPS, active_upper_bound + 0.5 - EPS,\n        )\n\n    def to_ndarray(self, hp: int) -> np.ndarray:\n        assert isinstance(hp, int), (hp, type(hp), self)\n        return self._continuous_range.to_ndarray(float(hp))\n\n    def from_ndarray(self, ndarray: np.ndarray) -> int:\n        continuous = self._continuous_range.from_ndarray(ndarray)\n        result = int(round(continuous))\n        # just to be sure\n        assert result >= self.lower_bound, (result, self)\n        assert result <= self.upper_bound, (result, self)\n        return result\n\n    def __repr__(self) -> str:\n        return ""{}({}, {}, {}, {}, {}, {})"".format(\n            self.__class__.__name__, repr(self.name),\n            repr(self.active_lower_bound), repr(self.active_upper_bound),\n            repr(self.scaling), repr(self.lower_bound), repr(self.upper_bound)\n        )\n\n    def __eq__(self, other):\n        if isinstance(other, HyperparameterRangeInteger):\n            return self.name == other.name \\\n                   and self.lower_bound == other.lower_bound \\\n                   and self.upper_bound == other.upper_bound \\\n                   and self.active_lower_bound == other.active_lower_bound \\\n                   and self.active_upper_bound == other.active_upper_bound \\\n                   and self.scaling == other.scaling\n        return False\n\n    def from_zero_one(self, normalized_value: float) -> Hyperparameter:\n        scaling_result = scale_from_zero_one(\n            normalized_value, self._continuous_range.active_lower_bound,\n            self._continuous_range.active_upper_bound, self.scaling)\n        result = int(round(scaling_result))\n        return result\n\n\nclass HyperparameterRangeCategorical(HyperparameterRange):\n    def __init__(self, name: str, choices: Tuple[str, ...],\n                 active_choices: Tuple[str, ...] = None):\n        """"""\n        Can take on discrete set of values.\n        :param name: name of dimension.\n        :param choices: possible values of the hyperparameter.\n        :param active_choices: a subset of choices, restricts choices to some subset of values.\n        """"""\n        super().__init__(name)\n        # sort the choices, so that we are sure the order is the same when we do one hot encoding\n        choices = sorted(choices)\n        self.choices = choices\n        self.active_choices = choices if active_choices is None else active_choices\n        self.active_choices = sorted(self.active_choices)\n\n        assert set(self.choices).issuperset(self.active_choices)\n\n    def to_ndarray(self, hp: str) -> np.ndarray:\n        assert hp in self.choices, ""{} not in {}"".format(hp, self)\n        idx = self.choices.index(hp)\n        result = np.zeros(shape=(len(self.choices),))\n        result[idx] = 1.0\n        return result\n\n    def from_ndarray(self, cand_ndarray: np.ndarray) -> str:\n        assert len(cand_ndarray) == len(self.choices), (cand_ndarray, self)\n        return self.choices[int(np.argmax(cand_ndarray))]\n\n    def ndarray_size(self) -> int:\n        return len(self.choices)\n\n    def from_zero_one(self, normalized_value: float) -> str:\n        if normalized_value == 1.0:\n            return self.active_choices[-1]\n        else:\n            return self.active_choices[int(floor(normalized_value * len(self.active_choices)))]\n\n    def __repr__(self) -> str:\n        return ""{}({}, {})"".format(\n            self.__class__.__name__, repr(self.name), repr(self.choices)\n        )\n\n    def __eq__(self, other) -> bool:\n        if isinstance(other, HyperparameterRangeCategorical):\n            return self.name == other.name \\\n                   and self.choices == other.choices\n        return False\n\n\nclass HyperparameterRanges(ABC):\n    @abstractmethod\n    def to_ndarray(self, cand_tuple: Candidate) -> np.ndarray:\n        pass\n\n    def to_ndarray_matrix(self, candidates: Iterable[Candidate]) -> np.ndarray:\n        return np.vstack([self.to_ndarray(cand) for cand in candidates])\n\n    @abstractmethod\n    def ndarray_size(self) -> int:\n        pass\n\n    @abstractmethod\n    def from_ndarray(self, cand_ndarray: np.ndarray) -> Candidate:\n        """"""\n        Converts a candidate from internal ndarray representation (fed to the\n        GP) into an external Candidate. This typically involves rounding.\n\n        For numerical HPs it assumes values scaled between 0.0 and 1.0, for\n        categorical HPs it assumes one scalar per category, which will convert\n        to the category with the highest value (think of it as a soft one hot\n        encoding).\n        """"""\n        pass\n\n    @abstractmethod\n    def random_candidate(self, random_state) -> Candidate:\n        pass\n\n    @abstractmethod\n    def random_candidates(self, random_state, num_configs: int) -> \\\n            List[Candidate]:\n        pass\n\n    @abstractmethod\n    def get_ndarray_bounds(self) -> List[Tuple[float, float]]:\n        """"""\n        Returns (lower, upper) bounds for each dimension in ndarray vector\n        representation.\n\n        :return: [(lower0, upper0), (lower1, upper1), ...]\n        """"""\n        pass\n\n    @abstractmethod\n    def __repr__(self) -> str:\n        pass\n\n    @abstractmethod\n    def __eq__(self, other: object) -> bool:\n        pass\n\n    @abstractmethod\n    def __len__(self) -> int:\n        pass\n\n    # Needed because some code iterates over HyperparameterRange entries\n    @property\n    def hp_ranges(self) -> Tuple[HyperparameterRange]:\n        raise NotImplementedError(\n            ""Only some HyperparameterRanges subclasses implement hp_ranges"")\n\n\nclass HyperparameterRanges_Impl(HyperparameterRanges):\n    """"""\n    Alternative to HyperparameterRanges_CS, without depending on ConfigSpace.\n\n    """"""\n    def __init__(self, *args: HyperparameterRange):\n        """"""\n        Usage:\n\n        hp_ranges = HyperparameterRanges_Impl(hp1_range, hp2_range, hp3_range)\n        """"""\n        self._hp_ranges = tuple(args)\n\n        names = [hp_range.name for hp_range in args]\n        if len(set(names)) != len(names):\n            raise ValueError(""duplicate names in {}"".format(names))\n\n    def to_ndarray(self, cand_tuple: Candidate) -> np.ndarray:\n        pieces = [hp_range.to_ndarray(hp) for hp_range, hp in zip(self._hp_ranges, cand_tuple)]\n        return np.hstack(pieces)\n\n    def ndarray_size(self) -> int:\n        return sum(d.ndarray_size() for d in self._hp_ranges)\n\n    def from_ndarray(self, cand_ndarray: np.ndarray) -> Candidate:\n        """"""\n        Converts a candidate from internal ndarray representation (fed to the GP) into an\n        external Candidate\n\n        For numerical HPs it assumes values scaled between 0.0 and 1.0, for categorical HPs it\n        assumes one scalar per category, which will convert to the category with the highest value\n        (think of it as a soft one hot encoding).\n\n        all such values should be concatenated, see to_ndarray for the opposite conversion\n        """"""\n\n        # check size\n        expected_size = self.ndarray_size()\n        assert cand_ndarray.shape == (expected_size,), (cand_ndarray.shape, expected_size)\n\n        hps = []\n        start = 0\n        for hp_range in self._hp_ranges:\n            end = start + hp_range.ndarray_size()\n            hps.append(hp_range.from_ndarray(cand_ndarray[start:end]))\n            start = end\n\n        return tuple(hps)\n\n    def get_ndarray_bounds(self) -> List[Tuple[float, float]]:\n        # Context values are fixed by setting min = max in bounds of lbfgs minimizer:\n        # https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.Bounds.html#scipy.optimize.Bounds\n        bounds = []\n        for hp_range in self._hp_ranges:\n            if isinstance(hp_range, HyperparameterRangeCategorical):\n                for category in hp_range.choices:\n                    if category in hp_range.active_choices:\n                        if len(hp_range.active_choices) == 1:\n                            bounds.append((1.0, 1.0))  # constrain choice to the only one which is active\n                        else:\n                            bounds.append((0.0, 1.0))\n                    else:\n                        bounds.append((0.0, 0.0))\n            else:\n                # conversion below includes log / reverse log / etc scaling\n                # due to warm start the below could be different from global\n                # bounds\n                low = hp_range.to_ndarray(hp_range.from_zero_one(0.0)).item()\n                high = hp_range.to_ndarray(hp_range.from_zero_one(1.0)).item()\n                bounds.append((low, high))\n        return bounds\n\n    def refine_ndarray_bounds(\n            self, bounds: List[Tuple[float, float]], candidate: Candidate,\n            margin: float) -> List[Tuple[float, float]]:\n        new_bounds = []\n        bound_it = iter(bounds)\n        for i, hp_range in enumerate(self._hp_ranges):\n            if isinstance(hp_range, HyperparameterRangeCategorical):\n                for _ in range(len(hp_range.choices)):\n                    new_bounds.append(next(bound_it))\n            else:\n                low, high = next(bound_it)\n                x = hp_range.to_ndarray(candidate[i]).item()\n                low = max(low, x - margin)\n                high = min(high, x + margin)\n                new_bounds.append((low, high))\n        assert len(bounds) == len(new_bounds)  # Sanity check\n        return new_bounds\n\n    def to_kwargs(self, cand_tuple: Candidate) -> Dict[str, Hyperparameter]:\n        return {hp_range.name: hp for hp_range, hp in zip(self._hp_ranges, cand_tuple)}\n\n    def random_candidate(self, random_state) -> Candidate:\n        return tuple(hp_range.random_hp(random_state) for hp_range in self._hp_ranges)\n\n    def random_candidates(self, random_state, num_configs: int) -> \\\n            List[Candidate]:\n        return [self.random_candidate(random_state)\n                for _ in range(num_configs)]\n\n    def __repr__(self) -> str:\n        return ""{}{}"".format(\n            self.__class__.__name__, repr(self._hp_ranges)\n        )\n\n    def __eq__(self, other: object) -> bool:\n        if isinstance(other, HyperparameterRanges_Impl):\n            return self._hp_ranges == other.hp_ranges\n        return False\n\n    def __len__(self) -> int:\n        return len(self._hp_ranges)\n\n    @property\n    def hp_ranges(self) -> Tuple[HyperparameterRange]:\n        return self._hp_ranges\n'"
autogluon/searcher/bayesopt/datatypes/scaling.py,0,"b'from abc import ABC, abstractmethod\nimport numpy as np\n\n\nclass Scaling(ABC):\n    @abstractmethod\n    def to_internal(self, value: float) -> float:\n        pass\n\n    @abstractmethod\n    def from_internal(self, value: float) -> float:\n        pass\n\n    def __repr__(self):\n        return ""{}()"".format(self.__class__.__name__)\n\n    def __eq__(self, other):\n        # For usage in tests. Make sure to edit if parameters are added.\n        return self.__class__ == other.__class__\n\n\nclass LinearScaling(Scaling):\n    def to_internal(self, value: float) -> float:\n        return value\n\n    def from_internal(self, value: float) -> float:\n        return value\n\n\nclass LogScaling(Scaling):\n    def to_internal(self, value: float) -> float:\n        assert value > 0, ""Value must be strictly positive to be log-scaled.""\n        return np.log(value)\n\n    def from_internal(self, value: float) -> float:\n        return np.exp(value)\n\n\nclass ReverseLogScaling(Scaling):\n    def to_internal(self, value: float) -> float:\n        assert 0 <= value < 1, \\\n            ""Value must be between 0 (inclusive) and 1 (exclusive) to be reverse-log-scaled.""\n        return -np.log(1.0 - value)\n\n    def from_internal(self, value: float) -> float:\n        return 1.0 - np.exp(-value)\n'"
autogluon/searcher/bayesopt/datatypes/tuning_job_state.py,0,"b'from typing import NamedTuple, List\n\nfrom autogluon.searcher.bayesopt.datatypes.common import Candidate, \\\n    CandidateEvaluation, PendingEvaluation\nfrom autogluon.searcher.bayesopt.datatypes.hp_ranges import \\\n    HyperparameterRanges\n\n\nclass TuningJobState(NamedTuple):\n    """"""\n    Tuning job state (non disjoint: so for a single algorithm when tuning\n    multiple algorithms)\n    """"""\n    hp_ranges: HyperparameterRanges\n    candidate_evaluations: List[CandidateEvaluation]\n    failed_candidates: List[Candidate]\n    pending_evaluations: List[PendingEvaluation]\n\n    @property\n    def pending_candidates(self):\n        return [x.candidate for x in self.pending_evaluations]\n'"
autogluon/searcher/bayesopt/gpmxnet/__init__.py,0,"b'from autogluon.searcher.bayesopt.gpmxnet.custom_op import AddJitterOp, \\\n    AddJitterOpProp\n\n\nclass SliceException(Exception):\n    pass\n'"
autogluon/searcher/bayesopt/gpmxnet/comparison_gpy.py,0,"b'from typing import Optional\nimport numpy as np\nfrom scipy.linalg import solve_triangular\nimport ConfigSpace as CS\nimport ConfigSpace.hyperparameters as CSH\n\nfrom autogluon.searcher.bayesopt.models.gpmxnet import \\\n    get_internal_candidate_evaluations\nfrom autogluon.searcher.bayesopt.datatypes.tuning_job_state import \\\n    TuningJobState\nfrom autogluon.searcher.bayesopt.autogluon.hp_ranges import \\\n    HyperparameterRanges_CS\nfrom autogluon.searcher.bayesopt.datatypes.common import CandidateEvaluation\nfrom autogluon.searcher.bayesopt.tuning_algorithms.default_algorithm import \\\n    dictionarize_objective, DEFAULT_METRIC\n\n\nclass ThreeHumpCamel(object):\n    @property\n    def search_space(self):\n        return [{\'min\': -5.0, \'max\': 5.0},\n                {\'min\': -5.0, \'max\': 5.0}]\n\n    def evaluate(self, x1, x2):\n        return 2 * x1 ** 2 - 1.05 * x1 ** 4 + x1 ** 6 / 6 + x1 * x2 + x2 ** 2\n\n\ndef branin_function(x1, x2):\n    return (x2 - (5.1 / (4 * np.pi ** 2)) * x1 ** 2 + (5 / np.pi) * x1 - 6) ** 2 + \\\n           10 * (1 - 1 / (8 * np.pi)) * np.cos(x1) + 10\n\n\nclass Branin(object):\n    @property\n    def search_space(self):\n        return [{\'min\': -5.0, \'max\': 10.0},\n                {\'min\': 0.0, \'max\': 15.0}]\n\n    def evaluate(self, x1, x2):\n        return branin_function(x1, x2)\n\n\nclass Ackley(object):\n    @property\n    def search_space(self):\n        const = 32.768\n        return [{\'min\': -const, \'max\': const},\n                {\'min\': -const, \'max\': const}]\n\n    def evaluate(self, x1, x2):\n        a = 20\n        b = 0.2\n        c = 2 * np.pi\n        ssq = (x1 ** 2) + (x2 ** 2)\n        scos = np.cos(c * x1) + np.cos(c * x2)\n        return -a * np.exp(-b * np.sqrt(0.5 * ssq)) - np.exp(0.5 * scos) + \\\n               (a + np.exp(1))\n\n\ndef _decode_input(x, lim):\n    mn, mx = lim[\'min\'], lim[\'max\']\n    return x * (mx - mn) + mn\n\n\ndef evaluate_blackbox(bb_func, inputs: np.ndarray) -> np.ndarray:\n    num_dims = inputs.shape[1]\n    input_list = []\n    for x, lim in zip(np.split(inputs, num_dims, axis=1), bb_func.search_space):\n        input_list.append(_decode_input(x, lim))\n    return bb_func.evaluate(*input_list)\n\n\n# NOTE: Inputs will always be in [0, 1] (so come in encoded form). They are\n# only scaled to their native ranges (linearly) when evaluations of the\n# blackbox are done. This avoids silly errors.\ndef sample_data(\n        bb_cls, num_train: int, num_grid: int) -> dict:\n    bb_func = bb_cls()\n    ss_limits = bb_func.search_space\n    num_dims = len(ss_limits)\n    # Sample training inputs\n    train_inputs = np.random.uniform(\n        low=0.0, high=1.0, size=(num_train, num_dims))\n    # Training targets (currently, no noise is added)\n    train_targets = evaluate_blackbox(bb_func, train_inputs).reshape((-1,))\n    # Inputs for prediction (regular grid)\n    grids = [np.linspace(0.0, 1.0, num_grid)] * num_dims\n    grids2 = tuple(np.meshgrid(*grids))\n    test_inputs = np.hstack([x.reshape(-1, 1) for x in grids2])\n    # Also evaluate true function on grid\n    true_targets = evaluate_blackbox(bb_func, test_inputs).reshape((-1,))\n    data = {\n        \'ss_limits\': ss_limits,\n        \'train_inputs\': train_inputs,\n        \'train_targets\': train_targets,\n        \'test_inputs\': test_inputs,\n        \'grid_shape\': grids2[0].shape,\n        \'true_targets\': true_targets}\n    # Make sure that ours and GPy below receive exactly the same inputs\n    state = data_to_state(data)\n    data_internal = get_internal_candidate_evaluations(\n        state, active_metric=DEFAULT_METRIC, normalize_targets=True,\n        num_fantasize_samples=20)\n    data[\'state\'] = state\n    data[\'train_inputs\'] = data_internal.X\n    data[\'train_targets_normalized\'] = data_internal.y\n    return data\n\n\n# Recall that inputs in data are encoded, so we have to decode them to their\n# native ranges for candidate_evaluations\ndef data_to_state(data: dict) -> TuningJobState:\n    cs = CS.ConfigurationSpace()\n    cs_names = [\'x{}\'.format(i) for i in range(len(data[\'ss_limits\']))]\n    cs.add_hyperparameters([\n        CSH.UniformFloatHyperparameter(\n            name=name, lower=lims[\'min\'], upper=lims[\'max\'])\n        for name, lims in zip(cs_names, data[\'ss_limits\'])])\n    _evaluations = []\n    x_mult = []\n    x_add = []\n    for lim in data[\'ss_limits\']:\n        mn, mx = lim[\'min\'], lim[\'max\']\n        x_mult.append(mx - mn)\n        x_add.append(mn)\n    x_mult = np.array(x_mult)\n    x_add = np.array(x_add)\n    for x, y in zip(data[\'train_inputs\'], data[\'train_targets\']):\n        x_decoded = x * x_mult + x_add\n        config_dct = dict(zip(cs_names, x_decoded))\n        config = CS.Configuration(cs, values=config_dct)\n        _evaluations.append(CandidateEvaluation(\n            config, dictionarize_objective(y)))\n    return TuningJobState(\n        hp_ranges=HyperparameterRanges_CS(cs),\n        candidate_evaluations=_evaluations,\n        failed_candidates=[],\n        pending_evaluations=[])\n\n\ndef assert_equal_candidates(candidates1, candidates2, hp_ranges, decimal=5):\n    inputs1 = hp_ranges.to_ndarray_matrix(candidates1)\n    inputs2 = hp_ranges.to_ndarray_matrix(candidates2)\n    np.testing.assert_almost_equal(inputs1, inputs2, decimal=decimal)\n\n\ndef assert_equal_randomstate(randomstate1, randomstate2):\n    assert str(randomstate1.get_state()) == str(randomstate2.get_state())\n\n\ndef compare_gpy_predict_posterior_marginals(\n        test_intermediates: dict, noise_variance_gpy: Optional[float] = None):\n    """"""\n    Compares all intermediates of cholesky_computations and\n    predict_posterior_marginals to using GPy and NumPy.\n\n    Currently, this is restricted:\n    - Kernel must be Matern52 with ARD\n    - Mean function must be constant 0\n\n    :param test_intermediates: Intermediates computed using our code\n    :param noise_variance_gpy: Overrides noise_variance in test_intermediates.\n        Use this if jitter was added during the posterior state computation.\n\n    """"""\n    import GPy\n    # Create GPy kernel and model\n    num_data = test_intermediates[\'features\'].shape[0]\n    num_dims = test_intermediates[\'features\'].shape[1]\n    lengthscales = [\n        1.0 / test_intermediates[\'inv_bw{}\'.format(i)]\n        for i in range(num_dims)]\n    kernel = GPy.kern.Matern52(\n        num_dims,\n        variance=test_intermediates[\'covariance_scale\'],\n        lengthscale=lengthscales,\n        ARD=True)\n    if noise_variance_gpy is None:\n        noise_variance_gpy = test_intermediates[\'noise_variance\']\n    model = GPy.models.GPRegression(\n        test_intermediates[\'features\'],\n        test_intermediates[\'targets\'].reshape((-1, 1)),\n        kernel=kernel, noise_var=noise_variance_gpy)\n    # Compare intermediates step by step (cholesky_computations)\n    kernel_mat_gpy = kernel.K(test_intermediates[\'features\'], X2=None)\n    np.testing.assert_almost_equal(\n        test_intermediates[\'kernel_mat\'], kernel_mat_gpy, decimal=5)\n    sys_mat_gpy = kernel_mat_gpy + np.diag(np.ones(num_data)) * \\\n                  noise_variance_gpy\n    np.testing.assert_almost_equal(\n        test_intermediates[\'sys_mat\'], sys_mat_gpy, decimal=5)\n    chol_fact_gpy = np.linalg.cholesky(sys_mat_gpy)\n    # Use test_intermediates[\'sys_mat\'] instead:\n    #chol_fact_gpy = np.linalg.cholesky(test_intermediates[\'sys_mat\'])\n    np.testing.assert_almost_equal(\n        test_intermediates[\'chol_fact\'], chol_fact_gpy, decimal=4)\n    # Mean function must be constant 0\n    centered_y = test_intermediates[\'targets\'].reshape((-1, 1))\n    np.testing.assert_almost_equal(\n        test_intermediates[\'centered_y\'], centered_y, decimal=9)\n    pred_mat_gpy = solve_triangular(chol_fact_gpy, centered_y, lower=True)\n    np.testing.assert_almost_equal(\n        test_intermediates[\'pred_mat\'], pred_mat_gpy, decimal=3)\n    # Compare intermediates step by step (predict_posterior_marginals)\n    k_tr_te_gpy = kernel.K(test_intermediates[\'features\'],\n                           X2=test_intermediates[\'test_features\'])\n    np.testing.assert_almost_equal(\n        test_intermediates[\'k_tr_te\'], k_tr_te_gpy, decimal=5)\n    linv_k_tr_te_gpy = solve_triangular(chol_fact_gpy, k_tr_te_gpy, lower=True)\n    np.testing.assert_almost_equal(\n        test_intermediates[\'linv_k_tr_te\'], linv_k_tr_te_gpy, decimal=4)\n    pred_means_gpy = np.dot(linv_k_tr_te_gpy.T, pred_mat_gpy)\n    np.testing.assert_almost_equal(\n        test_intermediates[\'pred_means\'], pred_means_gpy, decimal=4)\n    k_tr_diag_gpy = kernel.Kdiag(\n        test_intermediates[\'test_features\']).reshape((-1,))\n    tvec_gpy = np.sum(np.square(linv_k_tr_te_gpy), axis=0).reshape((-1,))\n    pred_vars_gpy = k_tr_diag_gpy - tvec_gpy\n    np.testing.assert_almost_equal(\n        test_intermediates[\'pred_vars\'], pred_vars_gpy, decimal=4)\n    # Also test against GPy predict\n    pred_means_gpy2, pred_vars_gpy2 = model.predict(\n        test_intermediates[\'test_features\'], include_likelihood=False)\n    pred_vars_gpy2 = pred_vars_gpy2.reshape((-1,))\n    np.testing.assert_almost_equal(pred_means_gpy, pred_means_gpy2, decimal=3)\n    np.testing.assert_almost_equal(pred_vars_gpy, pred_vars_gpy2, decimal=3)\n'"
autogluon/searcher/bayesopt/gpmxnet/constants.py,0,"b'# This file contains various constants required for the definition of the model\n# or to set up the optimization\n\nfrom typing import NamedTuple\nimport numpy as np\n\n\nDEFAULT_ENCODING = ""logarithm""  # the other choices is positive\n\nNUMERICAL_JITTER = 1e-9\n\nINITIAL_NOISE_VARIANCE = 1e-3\nINITIAL_MEAN_VALUE = 0.0\nINITIAL_COVARIANCE_SCALE = 1.0\nINITIAL_INVERSE_BANDWIDTHS = 1.0\nINITIAL_WARPING = 1.0\n\nINVERSE_BANDWIDTHS_LOWER_BOUND = 1e-4\nINVERSE_BANDWIDTHS_UPPER_BOUND = 100\n\nCOVARIANCE_SCALE_LOWER_BOUND = 1e-3\nCOVARIANCE_SCALE_UPPER_BOUND = 1e3\n\nNOISE_VARIANCE_LOWER_BOUND = 1e-9\nNOISE_VARIANCE_UPPER_BOUND = 1e6\n\nWARPING_LOWER_BOUND = 0.25\nWARPING_UPPER_BOUND = 4.\n\nMIN_POSTERIOR_VARIANCE = 1e-12\n\nMIN_CHOLESKY_DIAGONAL_VALUE = 1e-10\n\nDATA_TYPE = np.float64\n\n\nclass OptimizationConfig(NamedTuple):\n    lbfgs_tol: float\n    lbfgs_maxiter : int\n    verbose : bool\n    n_starts : int\n\n\nclass MCMCConfig(NamedTuple):\n    n_samples : int\n    n_burnin : int\n    n_thinning : int\n\n\nDEFAULT_OPTIMIZATION_CONFIG = OptimizationConfig(\n    lbfgs_tol=1e-6,\n    lbfgs_maxiter=500,\n    verbose=False,\n    n_starts=5)\n\nDEFAULT_MCMC_CONFIG = MCMCConfig(\n    n_samples=300,\n    n_burnin=250,\n    n_thinning=5)\n'"
autogluon/searcher/bayesopt/gpmxnet/custom_op.py,0,"b'import mxnet as mx\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n__all__ = [\'AddJitterOp\', \'AddJitterOpProp\']\n\n\nINITIAL_JITTER_FACTOR = 1e-9\nJITTER_GROWTH = 10.\nJITTER_UPPERBOUND_FACTOR = 1e3\n\n\nclass AddJitterOp(mx.operator.CustomOp):\n    """"""\n    Finds smaller jitter to add to diagonal of square matrix to render the\n    matrix positive definite (in that linalg.potrf works).\n\n    Given input x (positive semi-definite matrix) and sigsq_init (nonneg\n    scalar), find sigsq_final (nonneg scalar), so that:\n        sigsq_final = sigsq_init + jitter, jitter >= 0,\n        x + sigsq_final * Id positive definite (so that potrf call works)\n    We return the matrix x + sigsq_final * Id, for which potrf has not failed.\n\n    For the gradient, the dependence of jitter on the inputs is ignored.\n\n    The values tried for sigsq_final are:\n        sigsq_init, sigsq_init + initial_jitter * (jitter_growth ** k),\n        k = 0, 1, 2, ...,\n        initial_jitter = initial_jitter_factor * mean(diag(x))\n\n    Note: The scaling of initial_jitter with mean(diag(x)) is taken from GPy.\n    The rationale is that the largest eigenvalue of x is >= mean(diag(x)), and\n    likely of this magnitude.\n\n    There is no guarantee that the Cholesky factor returned is well-conditioned\n    enough for subsequent computations to be reliable. A better solution\n    would be to estimate the condition number of the Cholesky factor, and to add\n    jitter until this is bounded below a threshold we tolerate. See\n\n        Higham, N.\n        A Survey of Condition Number Estimation for Triangular Matrices\n        MIMS EPrint: 2007.10\n\n    Algorithm 4.1 could work for us.\n    """"""\n\n    def __init__(\n            self, initial_jitter_factor, jitter_growth, debug_log, **kwargs):\n        super(AddJitterOp, self).__init__(**kwargs)\n\n        assert initial_jitter_factor > 0. and jitter_growth > 1.\n        self._initial_jitter_factor = initial_jitter_factor\n        self._jitter_growth = jitter_growth\n        self._debug_log = debug_log\n\n    def _get_constant_identity(self, x, constant):\n        n, _ = x.shape\n        return mx.nd.diag(\n            mx.nd.ones(shape=(n,), ctx=x.context, dtype=x.dtype) * constant)\n\n    def _get_jitter_upperbound(self, x):\n        # To define a safeguard in the while-loop of the forward,\n        # we define an upperbound on the jitter we can reasonably add\n        # the bound is quite generous, and is dependent on the scale of the input x\n        # (the scale is captured via the trace of x)\n        # the primary goal is avoid any infinite while-loop.\n        return JITTER_UPPERBOUND_FACTOR * max(\n            1., mx.nd.mean(mx.nd.diag(x)).asscalar())\n\n    def forward(self, is_train, req, in_data, out_data, aux):\n        x = in_data[0]\n        sigsq_init = in_data[1]\n        jitter = 0.\n        jitter_upperbound = self._get_jitter_upperbound(x)\n        must_increase_jitter = True\n        x_plus_constant = None\n        while must_increase_jitter and jitter <= jitter_upperbound:\n            try:\n                x_plus_constant = x + self._get_constant_identity(\n                    x, sigsq_init + jitter)\n                L = mx.nd.linalg.potrf(x_plus_constant)\n                # because of the implicit asynchronous processing in MXNet,\n                # we need to enforce the computation of L to happen right here\n                L.wait_to_read()\n                must_increase_jitter = False\n            except mx.base.MXNetError:\n                if self._debug_log == \'true\':\n                    logger.info(""sigsq = {} does not work"".format(\n                        sigsq_init.asscalar() + jitter))\n                if jitter == 0.0:\n                    jitter = self._initial_jitter_factor * mx.nd.mean(\n                        mx.nd.diag(x)).asscalar()\n                else:\n                    jitter *= self._jitter_growth\n\n        assert not must_increase_jitter,\\\n            ""The jitter ({}) has reached its upperbound ({}) while the Cholesky of the input matrix still cannot be computed."".format(jitter, jitter_upperbound)\n        if self._debug_log == \'true\':\n            _sigsq_init = sigsq_init.asscalar()\n            logger.info(""sigsq_final = {}"".format(_sigsq_init + jitter))\n        self.assign(out_data[0], req[0], x_plus_constant)\n\n    def backward(self, req, out_grad, in_data, out_data, in_grad, aux):\n        self.assign(in_grad[0], req[0], out_grad[0])\n        trace_out_grad = mx.nd.sum(mx.nd.diag(out_grad[0]))\n        self.assign(in_grad[1], req[1], trace_out_grad)\n\n\n@mx.operator.register(""add_jitter"")\nclass AddJitterOpProp(mx.operator.CustomOpProp):\n    def __init__(\n            self, initial_jitter_factor=INITIAL_JITTER_FACTOR,\n            jitter_growth=JITTER_GROWTH, debug_log=\'false\'):\n        super(AddJitterOpProp, self).__init__(need_top_grad=True)\n        # We need to cast the arguments\n        # see detailed example https://github.com/Xilinx/mxnet/blob/master/docs/tutorials/gluon/customop.md\n        self._initial_jitter_factor = float(initial_jitter_factor)\n        self._jitter_growth = float(jitter_growth)\n        self._debug_log = debug_log\n\n    def list_arguments(self):\n        return [\'x\', \'sigsq_init\']\n\n    def list_outputs(self):\n        return [\'x_plus_sigsq_final\']\n\n    def infer_shape(self, in_shape):\n        x_shape = in_shape[0]\n        assert len(x_shape) == 2 and x_shape[0] == x_shape[1], \\\n            ""x must be square matrix, shape (n, n)""\n        ssq_shape = in_shape[1]\n        assert len(ssq_shape) == 1 and ssq_shape[0] == 1, \\\n            ""sigsq_init must be scalar, shape (1,)""\n        return in_shape, [x_shape], []\n\n    def create_operator(self, ctx, shapes, dtypes, **kwargs):\n        return AddJitterOp(\n            initial_jitter_factor=self._initial_jitter_factor,\n            jitter_growth=self._jitter_growth,\n            debug_log=self._debug_log, **kwargs)\n'"
autogluon/searcher/bayesopt/gpmxnet/debug_gp_regression.py,0,"b'import mxnet as mx\nimport logging\nimport time\n\nfrom autogluon.searcher.bayesopt.gpmxnet.utils import param_to_pretty_string\n\nlogger = logging.getLogger(__name__)\n\n\nclass DebugGPRegression(object):\n    """"""\n    Supports finding errors in GaussianProcessRegression.fit. For each\n    criterion evaluation, we store the input arguments before and the\n    criterion value afterwards. Storage is done to a rolling sequence\n    of local files.\n    """"""\n    def __init__(self, fname_msk=\'debug_gpr_{}\', rolling_size=3):\n        self.fname_msk = fname_msk\n        self.rolling_size = rolling_size\n        self.global_counter = 0\n        self.optim_counter = -1\n        self.local_counter = 0\n\n    def start_optimization(self):\n        self.optim_counter += 1\n        self.local_counter = 0\n\n    def store_args(self, params, X, Y, param_encoding_pairs):\n        arg_dict = {\n            \'features\': X,\n            \'targets\': Y}\n        for param in params:\n            arg_dict[param.name] = param.data(ctx=mx.cpu())\n        fname = self._filename()\n        mx.nd.save(fname + \'_args.nd\', arg_dict)\n        with open(fname + \'_args.txt\', \'w\') as f:\n            self._write_meta(f)\n            for param, encoding in param_encoding_pairs:\n                f.write(param_to_pretty_string(param, encoding) + \'\\n\')\n\n    def store_value(self, value):\n        fname = self._filename()\n        with open(fname + \'_value.txt\', \'w\') as f:\n            f.write(\'value = {}\\n\'.format(value))\n            self._write_meta(f)\n        # Advance counters\n        self.global_counter += 1\n        self.local_counter += 1\n\n    def _filename(self):\n        return self.fname_msk.format(self.global_counter % self.rolling_size)\n\n    def _write_meta(self, f):\n        f.write(\'optim_counter = {}\\n\'.format(self.optim_counter))\n        f.write(\'local_counter = {}\\n\'.format(self.local_counter))\n        f.write(\'global_counter = {}\\n\'.format(self.global_counter))\n        f.write(\'time = {}\\n\'.format(time.time()))\n'"
autogluon/searcher/bayesopt/gpmxnet/distribution.py,0,"b'from abc import ABC, abstractmethod\nimport numpy as np\nfrom scipy.special import gammaln\nimport numbers\n\nfrom autogluon.searcher.bayesopt.gpmxnet.constants import MIN_POSTERIOR_VARIANCE\n\n__all__ = [\'Distribution\',\n           \'Gamma\',\n           \'Uniform\',\n           \'Normal\',\n           \'LogNormal\',\n           \'Horseshoe\']\n\n\nclass Distribution(ABC):\n    @abstractmethod\n    def negative_log_density(self, F, x):\n        """"""\n        Negative log density, computed in MXNet. lower and upper limits are\n        ignored. If x is not a scalar, the distribution is i.i.d. over all\n        entries.\n        """"""\n        pass\n\n\nclass Gamma(Distribution):\n    """"""\n    Gamma(mean, alpha):\n\n        p(x) = C(alpha, beta) x^{alpha - 1} exp( -beta x), beta = alpha / mean,\n        C(alpha, beta) = beta^alpha / Gamma(alpha)\n\n    """"""\n    def __init__(self, mean, alpha):\n        self._assert_positive_number(mean, \'mean\')\n        self._assert_positive_number(alpha, \'alpha\')\n        self.mean = np.maximum(mean, MIN_POSTERIOR_VARIANCE)\n        self.alpha = np.maximum(alpha, MIN_POSTERIOR_VARIANCE)\n        self.beta = self.alpha / self.mean\n        self.log_const = gammaln(self.alpha) - self.alpha * np.log(self.beta)\n        self.__call__ = self.negative_log_density\n\n    @staticmethod\n    def _assert_positive_number(x, name):\n        assert isinstance(x, numbers.Real) and x > 0.0, \\\n            ""{} = {}, must be positive number"".format(name, x)\n\n    def negative_log_density(self, F, x):\n        x_safe = F.maximum(x, MIN_POSTERIOR_VARIANCE)\n        return F.sum(\n            (1.0 - self.alpha) * F.log(x_safe) + self.beta * x_safe +\n            self.log_const)\n\n    def __call__(self, F, x):\n        return self.negative_log_density(F, x)\n\n\nclass Uniform(Distribution):\n    def __init__(self, lower: float, upper: float):\n        self.log_const = np.log(upper - lower)\n        self.__call__ = self.negative_log_density\n\n    def negative_log_density(self, F, x):\n        return F.sum(F.ones_like(x)) * self.log_const\n\n    def __call__(self, F, x):\n        return self.negative_log_density(F, x)\n\n\nclass Normal(Distribution):\n    def __init__(self, mean: float, sigma: float):\n        self.mean = mean\n        self.sigma = sigma\n        self.__call__ = self.negative_log_density\n\n    def negative_log_density(self, F, x):\n        return F.sum(F.square(x - self.mean)) * (0.5 / np.square(self.sigma))\n\n    def __call__(self, F, x):\n        return self.negative_log_density(F, x)\n\n\nclass LogNormal(Distribution):\n    def __init__(self, mean: float, sigma: float):\n        self.mean = mean\n        self.sigma = sigma\n        self.__call__ = self.negative_log_density\n\n    def negative_log_density(self, F, x):\n        x_safe = F.maximum(x, MIN_POSTERIOR_VARIANCE)\n        return F.sum(\n            F.log(x_safe * self.sigma) +\n            F.square(F.log(x_safe) - self.mean) * (0.5 / np.square(self.sigma)))\n\n    def __call__(self, F, x):\n        return self.negative_log_density(F, x)\n\n\nclass Horseshoe(Distribution):\n    def __init__(self, s: float):\n        assert s > 0.0\n        self.s = max(s, MIN_POSTERIOR_VARIANCE)\n        self.__call__ = self.negative_log_density\n\n    def negative_log_density(self, F, x):\n        arg = F.maximum(3.0 * F.square(self.s / x), MIN_POSTERIOR_VARIANCE)\n        return -F.sum(F.log(F.log1p(arg)))\n\n    def __call__(self, F, x):\n        return self.negative_log_density(F, x)\n'"
autogluon/searcher/bayesopt/gpmxnet/gluon_blocks_helpers.py,0,"b'import numpy as np\nimport mxnet as mx\nfrom mxnet import gluon\nimport numbers\n\nfrom autogluon.searcher.bayesopt.gpmxnet.posterior_utils import \\\n    mxnet_is_ndarray\nfrom autogluon.searcher.bayesopt.gpmxnet.constants import DATA_TYPE\n\n__all__ = [\'ConstantPositiveVector\',\n           \'PositiveScalarEncoding\',\n           \'IdentityScalarEncoding\',\n           \'LogarithmScalarEncoding\']\n\n\nclass ConstantPositiveVector(gluon.HybridBlock):\n    """"""\n    ConstantPositiveVector\n    ======================\n\n    Represents constant vector, with positive entry value represented as Gluon\n    parameter, to be used in the context of wrapper classes in\n    gluon_blocks.py. Shape, dtype, and context are determined from the\n    features argument:\n\n    - If features.shape = (n, d):\n       shape = (d, 1) if size_cols = True (number cols of features)\n       shape = (n, 1) if size_cols = False (number rows of features)\n    - dtype = features.dtype, ctx = features.ctx\n\n    Encoding and internal Gluon parameter:\n    The positive scalar parameter is encoded via encoding (see\n    ScalarEncodingBase). The internal Gluon parameter (before encoding) has the\n    name param_name + \'_internal\'.\n    """"""\n\n    def __init__(self, param_name, encoding, size_cols, **kwargs):\n        super(ConstantPositiveVector, self).__init__(**kwargs)\n        assert isinstance(encoding, ScalarEncodingBase)\n        self.param_name = param_name\n        self.encoding = encoding\n        self.size_cols = size_cols\n        with self.name_scope():\n            init_val_int = encoding.init_val_int\n            # Note: The initialization values are bogus!\n            self.param_internal = self.params.get(\n                param_name + \'_internal\',\n                init=mx.init.Constant(init_val_int),\n                shape=(1,), dtype=DATA_TYPE)\n\n    def hybrid_forward(self, F, features, param_internal):\n        """"""Returns constant positive vector\n\n        If features.shape = (n, d), the shape of the vector returned is\n        (d, 1) if size_cols = True, (n, 1) otherwise.\n\n        :param F: mx.sym or mx.nd\n        :param features: Matrix for shape, dtype, ctx\n        :param param_internal: Unwrapped parameter\n        :return: Constant positive vector\n        """"""\n        # Shape, dtype, ctx is determined by extracting column or row from\n        # features, then use ones_like\n        axis = 0 if self.size_cols else 1\n        ones_vec = F.ones_like(F.reshape(F.slice_axis(\n            F.BlockGrad(features), axis=axis, begin=0, end=1), shape=(-1, 1)))\n        param = F.reshape(self.encoding.get(F, param_internal), shape=(1, 1))\n        return F.broadcast_mul(ones_vec, param)\n\n    def set(self, val):\n        self.encoding.set(self.param_internal, val)\n\n    def get(self):\n        param_internal = unwrap_parameter(\n            mx.nd, self.param_internal, None)\n        return self.encoding.get(\n            mx.nd, param_internal).asscalar()\n\n    def get_box_constraints_internal(self):\n        return self.encoding.box_constraints_internal(\n            self.param_internal)\n\n    def log_parameters(self):\n        return \'{} = {}\'.format(self.param_name, self.get())\n\n    def get_parameters(self):\n        return {self.param_name: self.get()}\n\n    def switch_updating(self, flag):\n        """"""Is the underlying parameter updated during learning?\n\n        By default, the parameter takes part in learning (its grad_req\n        attribute is \'write\'). For flag == False, the attribute is\n        flipped to \'null\', and the parameter remains constant during\n        learning.\n\n        :param flag: Update parameter during learning?\n        """"""\n        grad_req = \'write\' if flag else \'null\'\n        self.param_internal.grad_req = grad_req\n\n    def has_regularizer(self):\n        return (self.encoding.regularizer is not None)\n\n    def eval_regularizer(self, F, features):\n        if self.has_regularizer():\n            param_internal = unwrap_parameter(\n                F, self.param_internal, features)\n            param = self.encoding.get(F, param_internal)\n            return self.encoding.regularizer(F, param)\n        else:\n            return 0.0\n\n\nclass ScalarEncodingBase(object):\n    """"""\n    ScalarEncodingBase\n    ==================\n\n    Base class for encoding and box constraints for Gluon parameter,\n    represented as mx.gluon.Parameter. The parameter is with shape (dimension,)\n    where dimension is 1 by default.\n\n    An encoding is given as\n\n        param = enc(param_internal), param_internal = dec(param)\n\n    The Gluon parameter represents param_internal, while param is what is\n    visible to the outside.\n\n    Here, enc and dec are inverses of each other. enc is used in \'get\', dec\n    is used in \'set\'. Use \'IdentityScalarEncoding\' for no encoding (identity).\n    NOTE: enc (and dec) must be strictly increasing.\n\n    Box constraints are given by constr_lower_int < constr_upper_int. Here,\n    None means no constraint. The constraints apply to param_internal. If both\n    are None, param_internal is unconstrained (default).\n    NOTE: Box constraints are just maintained here, they have to be enforced\n    by an optimizer!\n\n    If regularizer is given, it specifies a regularization term for the\n    (encoded) parameter which can be added to a criterion function. It is\n    evaluated as regularizer(F, param), where F is mx.sym or mx.nd.\n\n    Typical use cases:\n    - Unconstrained optimizer, positive scalar > lower:\n      Use PositiveScalarEncoding(lower), box constraints = [None, None]\n    - Optimizer supports box constaints [constr_lower, constr_upper]:\n      Use IdentityScalarEncoding(constr_lower, constr_upper)\n    """"""\n\n    def __init__(\n            self, init_val, constr_lower=None, constr_upper=None,\n            regularizer=None, dimension=1):\n        if constr_lower is not None and constr_upper is not None:\n            assert constr_lower < constr_upper\n        init_val = self._check_or_set_init_val(\n            init_val, constr_lower, constr_upper)\n        init_val_int = self.decode(init_val, \'init_val\')\n        if constr_lower is not None:\n            assert init_val >= constr_lower\n            constr_lower_int = self.decode(constr_lower, \'constr_lower\')\n        else:\n            constr_lower_int = None\n        if constr_upper is not None:\n            assert init_val <= constr_upper\n            constr_upper_int = self.decode(constr_upper, \'constr_upper\')\n        else:\n            constr_upper_int = None\n        self.constraints = (constr_lower, constr_upper)\n        self.constraints_internal = (constr_lower_int, constr_upper_int)\n        self.init_val_int = init_val_int\n        self.regularizer = regularizer\n        self.dimension = dimension\n\n    def get(self, F, param_internal):\n        raise NotImplementedError(""get must be implemented"")\n\n    def set(self, param_internal, param_val):\n        assert isinstance(param_internal, mx.gluon.Parameter)\n        assert param_internal.shape == (self.dimension,)\n\n        if isinstance(param_val, (list, np.ndarray)):\n            assert len(param_val) == self.dimension\n            assert np.array(param_val).ndim == 1\n            val_int_list = [self.decode(val, \'param_val\') for val in param_val]\n        else:\n            assert np.isscalar(param_val) is True\n            val_int_list = [self.decode(param_val, \'param_val\')] * self.dimension\n        param_internal.set_data(mx.nd.array(val_int_list))\n\n    def decode(self, val, name):\n        raise NotImplementedError(""decode to be implemented in subclass"")\n\n    def box_constraints_internal(self, param_internal):\n        assert isinstance(param_internal, mx.gluon.Parameter)\n        assert param_internal.shape == (self.dimension,)\n        return {param_internal.name: self.constraints_internal}\n\n    def box_constraints(self):\n        return self.constraints\n\n    @staticmethod\n    def _check_or_set_init_val(init_val, constr_lower, constr_upper):\n        if init_val is not None:\n            if constr_upper is not None:\n                assert init_val <= constr_upper\n            if constr_lower is not None:\n                assert constr_lower <= init_val\n        else:\n            # Just pick some value which is feasible\n            if constr_lower is not None:\n                if constr_upper is not None:\n                    init_val = 0.5 * (constr_upper + constr_lower)\n                else:\n                    init_val = 1.1 * constr_lower\n            else:\n                if constr_upper is not None:\n                    init_val = 0.9 * constr_upper\n                else:\n                    init_val = 1.0\n        return init_val\n\n\nclass PositiveScalarEncoding(ScalarEncodingBase):\n    """"""\n    PositiveScalarEncoding\n    ======================\n\n    Provides encoding for positive scalar and vector: param > lower.\n    Here, param is represented as mx.gluon.Parameter. The param\n    is with shape (dimension,) where dimension is 1 by default.\n\n    The encoding is given as:\n\n        param = softrelu(param_internal) + lower,\n        softrelu(x) = log(1 + exp(x))\n\n    If constr_upper is used, the constraint\n\n        param_internal < dec(constr_upper)\n\n    can be enforced by an optimizer. Since dec is increasing, this translates\n    to param < constr_upper.\n    NOTE: While lower is enforced by the encoding, the upper bound is not, has\n    to be enforced by an optimizer.\n    """"""\n\n    def __init__(\n            self, lower, constr_upper=None, init_val=None, regularizer=None, dimension=1):\n        assert isinstance(lower, numbers.Real) and lower >= 0.0\n        self.lower = lower\n        super(PositiveScalarEncoding, self).__init__(\n            init_val, constr_lower=None, constr_upper=constr_upper,\n            regularizer=regularizer, dimension=dimension)\n\n    def get(self, F, param_internal):\n        return F.Activation(\n            param_internal, act_type=\'softrelu\') + self.lower\n\n    def decode(self, val, name):\n        assert val > self.lower, \\\n            \'{} = {} must be > self.lower = {}\'.format(\n                name, val, self.lower)\n        # Inverse of encoding: Careful with numerics:\n        # val_int = log(exp(arg) - 1) = arg + log(1 - exp(-arg))\n        #         = arg + log1p(-exp(-arg))\n        arg = val - self.lower\n        return arg + np.log1p(-np.exp(-arg))\n\n\nclass IdentityScalarEncoding(ScalarEncodingBase):\n    """"""\n    IdentityScalarEncoding\n    ======================\n\n    Identity encoding for scalar and vector:\n\n        param = param_internal\n\n    This does not ensure that param is positive! Use this only if positivity\n    is otherwise guaranteed.\n    """"""\n\n    def __init__(\n            self, constr_lower=None, constr_upper=None, init_val=None,\n            regularizer=None, dimension=1):\n        super(IdentityScalarEncoding, self).__init__(\n            init_val, constr_lower=constr_lower, constr_upper=constr_upper,\n            regularizer=regularizer, dimension=dimension)\n\n    def get(self, F, param_internal):\n        return param_internal\n\n    def decode(self, val, name):\n        return val\n\n\nclass LogarithmScalarEncoding(ScalarEncodingBase):\n    """"""\n    LogarithmScalarEncoding\n    =======================\n\n    Logarithmic encoding for scalar and vector:\n\n        param = exp(param_internal)\n    """"""\n\n    def __init__(\n            self, constr_lower=None, constr_upper=None, init_val=None,\n            regularizer=None, dimension=1):\n        super(LogarithmScalarEncoding, self).__init__(\n            init_val, constr_lower=constr_lower, constr_upper=constr_upper,\n            regularizer=regularizer, dimension=dimension)\n\n    def get(self, F, param_internal):\n        return F.exp(param_internal)\n\n    def decode(self, val, name):\n        assert val > 0.0, \\\n            \'{} = {} must be positive\'.format(name, val)\n        return np.log(np.maximum(val, 1e-15))\n\n\n# === Internal ===\n\n\ndef unwrap_parameter(F, param_internal, some_arg=None):\n    assert isinstance(param_internal, mx.gluon.Parameter)\n    if mxnet_is_ndarray(F):\n        ctx = some_arg.context if some_arg is not None else mx.cpu()\n        val = param_internal.data(ctx=ctx)\n    else:\n        val = param_internal.var()\n    return val\n\n\ndef encode_unwrap_parameter(F, param_internal, encoding, some_arg=None):\n    return encoding.get(F, unwrap_parameter(F, param_internal, some_arg))\n'"
autogluon/searcher/bayesopt/gpmxnet/gp_model.py,0,"b'from abc import ABC, abstractmethod\nfrom typing import List, Optional\nimport mxnet as mx\n\nfrom autogluon.searcher.bayesopt.gpmxnet.constants import DATA_TYPE\nfrom autogluon.searcher.bayesopt.gpmxnet.posterior_state import \\\n    GaussProcPosteriorState\n\n\nclass GaussianProcessModel(ABC):\n    @property\n    @abstractmethod\n    def states(self) -> Optional[List[GaussProcPosteriorState]]:\n        pass\n\n    @property\n    @abstractmethod\n    def ctx(self):\n        pass\n\n    @abstractmethod\n    def fit(self, X: mx.nd.NDArray, Y: mx.nd.NDArray):\n        """"""Train GP on the data and set a list of posterior states to be used by predict function""""""\n        pass\n\n    @abstractmethod\n    def recompute_states(self, X: mx.nd.NDArray, Y: mx.nd.NDArray):\n        """"""Fixing GP hyperparameters and recompute the list of posterior states based on X and Y""""""\n        pass\n\n    def _check_and_format_input(self, u):\n        """"""\n        Check and massage the input to conform with the numerical type and context\n\n        :param u: some mx.nd\n        """"""\n        assert isinstance(u, mx.nd.NDArray)\n        if u.ndim == 1:\n            u = u.reshape(-1, 1)\n        if u.dtype != DATA_TYPE or u.context != self.ctx:\n            return mx.nd.array(u, dtype=DATA_TYPE, ctx=self.ctx)\n        else:\n            return u\n\n    def predict(self, X_test):\n        """"""\n        Compute the posterior mean(s) and variance(s) for the points in X_test.\n        If the posterior state is based on m target vectors, a (n, m) matrix is\n        returned for posterior means.\n\n        :param X_test: Data matrix X_test of size (n, d) (type mx.nd) for which n\n            predictions are made\n        :return: posterior_means, posterior_variances\n        """"""\n\n        X_test = self._assert_check_xtest(X_test)\n        predictions = []\n        for state in self.states:\n            post_means, post_vars = state.predict(X_test)\n            # Just to make sure the return shapes are the same as before:\n            if post_means.shape[1] == 1:\n                post_means = post_means.reshape((-1,))\n            predictions.append((post_means, post_vars))\n        return predictions\n\n    def _assert_check_xtest(self, X_test):\n        assert self.states is not None, \\\n            ""Posterior state does not exist (run \'fit\')""\n        X_test = self._check_and_format_input(X_test)\n        assert X_test.shape[1] == self.states[0].num_features, \\\n            ""X_test and X_train should have the same number of columns (received {}, expected {})"".format(\n                X_test.shape[1], self.states[0].num_features)\n        return X_test\n\n    def multiple_targets(self):\n        """"""\n        :return: Posterior state based on multiple (fantasized) target vectors?\n        """"""\n        assert self.states is not None, \\\n            ""Posterior state does not exist (run \'fit\')""\n        return self.states[0].num_fantasies > 1\n\n    def sample_marginals(self, X_test, num_samples=1):\n        """"""\n        Draws marginal samples from predictive distribution at n test points.\n        Notice we concat the samples for each state. Let n_states = len(self._states)\n\n        If the posterior state is based on m > 1 target vectors, a\n        (n, m, num_samples * n_states) tensor is returned, for m == 1 we return a\n        (n, num_samples * n_states) matrix.\n\n        :param X_test: Test input points, shape (n, d)\n        :param num_samples: Number of samples\n        :return: Samples with shape (n, num_samples * n_states) or (n, m, num_samples * n_states) if m > 1\n        """"""\n\n        X_test = self._assert_check_xtest(X_test)\n        samples_list = [state.sample_marginals(X_test, num_samples)\n                        for state in self.states]\n        return _concatenate_samples(samples_list)\n\n    def sample_joint(self, X_test, num_samples=1):\n        """"""\n        Draws joint samples from predictive distribution at n test points.\n        This scales cubically with n.\n        the posterior state must be based on a single target vector\n        (m > 1 is not supported).\n\n        :param X_test: Test input points, shape (n, d)\n        :param num_samples: Number of samples\n        :return: Samples, shape (n, num_samples)\n        """"""\n\n        X_test = self._assert_check_xtest(X_test)\n        samples_list = [state.sample_joint(X_test, num_samples)\n                        for state in self.states]\n        return _concatenate_samples(samples_list)\n\n\ndef _concatenate_samples(samples_list: List[mx.nd.NDArray]) -> mx.nd.NDArray:\n    return mx.nd.concat(*samples_list, dim=-1)\n'"
autogluon/searcher/bayesopt/gpmxnet/gp_regression.py,0,"b'import mxnet as mx\nfrom mxnet import autograd\nfrom typing import Optional, List\nimport logging\n\nfrom autogluon.searcher.bayesopt.gpmxnet.gp_model import GaussianProcessModel\nfrom autogluon.searcher.bayesopt.gpmxnet.utils import param_to_pretty_string\nfrom autogluon.searcher.bayesopt.gpmxnet.optimization_utils import \\\n    apply_lbfgs_with_multiple_starts\nfrom autogluon.searcher.bayesopt.gpmxnet.likelihood import MarginalLikelihood\nfrom autogluon.searcher.bayesopt.gpmxnet.posterior_state import \\\n    GaussProcPosteriorState\nfrom autogluon.searcher.bayesopt.gpmxnet.mean import ScalarMeanFunction, \\\n    MeanFunction\nfrom autogluon.searcher.bayesopt.gpmxnet.kernel import KernelFunction\nfrom autogluon.searcher.bayesopt.gpmxnet.constants import OptimizationConfig, \\\n    DEFAULT_OPTIMIZATION_CONFIG\nfrom autogluon.searcher.bayesopt.gpmxnet.gluon_blocks_helpers import \\\n    encode_unwrap_parameter\nfrom autogluon.searcher.bayesopt.autogluon.gp_profiling import \\\n    GPMXNetSimpleProfiler\nfrom autogluon.searcher.bayesopt.gpmxnet.debug_gp_regression import \\\n    DebugGPRegression\n\nlogger = logging.getLogger(__name__)\n\n\ndef negative_log_posterior(\n        likelihood: MarginalLikelihood, X: mx.nd.NDArray, Y: mx.nd.NDArray):\n    objective_nd = likelihood(X, Y)\n    # Add neg log hyperpriors, whenever some are defined\n    for param_int, encoding in likelihood.param_encoding_pairs():\n        if encoding.regularizer is not None:\n            param = encode_unwrap_parameter(\n                mx.nd, param_int, encoding, X)\n            objective_nd = objective_nd + encoding.regularizer(\n                mx.nd, param)\n    return objective_nd\n\n\nclass GaussianProcessRegression(GaussianProcessModel):\n    """"""\n    Gaussian Process Regression\n\n    Takes as input a mean function (which depends on X only) and a kernel\n    function.\n\n    :param kernel: Kernel function (for instance, a Matern52---note we cannot\n        provide Matern52() as default argument since we need to provide with\n        the dimension of points in X)\n    :param mean: Mean function (which depends on X only)\n    :param initial_noise_variance: Initial value for noise variance parameter\n    :param optimization_config: Configuration that specifies the behavior of\n        the optimization of the marginal likelihood.\n    :param random_seed: Random seed to be used (optional)\n    :param ctx: MXNet execution context (CPU or GPU)\n    :param fit_reset_params: Reset parameters to initial values before running\n        \'fit\'? If False, \'fit\' starts from the current values\n\n    """"""\n    def __init__(\n            self, kernel: KernelFunction, mean: MeanFunction = None,\n            initial_noise_variance: float = None,\n            optimization_config: OptimizationConfig = None,\n            random_seed=None, ctx=None,\n            fit_reset_params: bool = True,\n            test_intermediates: Optional[dict] = None,\n            debug_writer: Optional[DebugGPRegression] = None):\n        if mean is None:\n            mean = ScalarMeanFunction()\n        if optimization_config is None:\n            optimization_config = DEFAULT_OPTIMIZATION_CONFIG\n        if ctx is None:\n            ctx = mx.cpu()\n        if random_seed is not None:\n            mx.random.seed(random_seed)\n\n        self._states = None\n        self._ctx = ctx\n        self.fit_reset_params = fit_reset_params\n        self.optimization_config = optimization_config\n        self._test_intermediates = test_intermediates\n        self._debug_writer = debug_writer\n        self.likelihood = MarginalLikelihood(\n            kernel=kernel, mean=mean,\n            initial_noise_variance=initial_noise_variance)\n        self.reset_params()\n\n    @property\n    def states(self) -> Optional[List[GaussProcPosteriorState]]:\n        return self._states\n\n    @property\n    def ctx(self):\n        return self._ctx\n\n    def _create_lbfgs_arguments(self, X, Y):\n        """"""\n        Prepare the objective, parameters and the gradient arguments for\n        L-BFGS-B\n\n        :param X: data matrix X of size (n, d) (type mx.nd)\n        :param Y: vector of targets of size (n, 1) (type mx.nd)\n        """"""\n\n        def executor():\n            if self._debug_writer is not None:\n                self._debug_writer.store_args(\n                    self.likelihood.collect_params().values(), X, Y,\n                    self.likelihood.param_encoding_pairs())\n            with autograd.record():\n                objective_nd = negative_log_posterior(self.likelihood, X, Y)\n            objective_np = objective_nd.asscalar()\n            if self._debug_writer is not None:\n                self._debug_writer.store_value(objective_np)\n            objective_nd.backward()\n            if self.optimization_config.verbose:\n                msg_lst = [""[criterion = {}]"".format(objective_np)]\n                for param, encoding in self.likelihood.param_encoding_pairs():\n                    msg_lst.append(param_to_pretty_string(param, encoding))\n                logger.info(\'\\n\'.join(msg_lst))\n            return objective_np\n\n        arg_dict = {}\n        grad_dict = {}\n        params = self.likelihood.collect_params().values()\n        for param in params:\n            arg_dict[param.name] = param.data(ctx=self._ctx)\n            grad_dict[param.name] = param.grad(ctx=self._ctx)\n\n        return executor, arg_dict, grad_dict\n\n    def fit(self, X, Y, profiler: GPMXNetSimpleProfiler = None):\n        """"""\n        Fit the parameters of the GP by optimizing the marginal likelihood,\n        and set posterior states.\n\n        We catch exceptions during the optimization restarts. If any restarts\n        fail, log messages are written. If all restarts fail, the current\n        parameters are not changed.\n\n        :param X: data matrix X of size (n, d) (type mx.nd)\n        :param Y: matrix of targets of size (n, m) (type mx.nd)\n        """"""\n\n        X = self._check_and_format_input(X)\n        Y = self._check_and_format_input(Y)\n        assert X.shape[0] == Y.shape[0], \\\n            ""X and Y should have the same number of points (received {} and {})"".format(\n                X.shape[0], Y.shape[0])\n        assert Y.shape[1] == 1, \\\n            ""Y cannot be a matrix if parameters are to be fit""\n\n        if self.fit_reset_params:\n            self.reset_params()\n        if self._debug_writer is not None:\n            self._debug_writer.start_optimization()\n        mean_function = self.likelihood.mean\n        if isinstance(mean_function, ScalarMeanFunction):\n            mean_function.set_mean_value(mx.nd.mean(Y).asscalar())\n        if profiler is not None:\n            profiler.start(\'fit_hyperpars\')\n        n_starts = self.optimization_config.n_starts\n        ret_infos = apply_lbfgs_with_multiple_starts(\n            *self._create_lbfgs_arguments(X, Y),\n            bounds=self.likelihood.box_constraints_internal(),\n            n_starts=n_starts,\n            tol=self.optimization_config.lbfgs_tol,\n            maxiter=self.optimization_config.lbfgs_maxiter)\n        if profiler is not None:\n            profiler.stop(\'fit_hyperpars\')\n        # Logging in response to failures of optimization runs\n        n_succeeded = sum(x is None for x in ret_infos)\n        if n_succeeded < n_starts:\n            log_msg = ""[GaussianProcessRegression.fit]\\n""\n            log_msg += (""{} of the {} restarts failed with the following exceptions:\\n"".format(\n                n_starts - n_succeeded, n_starts))\n            for i, ret_info in enumerate(ret_infos):\n                if ret_info is not None:\n                    log_msg += (""- Restart {}: Exception {}\\n"".format(\n                        i, ret_info[\'type\']))\n                    log_msg += (""  Message: {}\\n"".format(ret_info[\'msg\']))\n                    log_msg += (""  Args: {}\\n"".format(ret_info[\'args\']))\n                    logger.info(log_msg)\n            if n_succeeded == 0:\n                logger.info(""All restarts failed: Skipping hyperparameter fitting for now"")\n        # Recompute posterior state for new hyperparameters\n        self._recompute_states(X, Y, profiler=profiler)\n\n    def recompute_states(self, X, Y, profiler: GPMXNetSimpleProfiler = None):\n        """"""\n        We allow Y to be a matrix with m>1 columns, which is useful to support\n        batch decisions by fantasizing.\n        """"""\n        X = self._check_and_format_input(X)\n        Y = self._check_and_format_input(Y)\n        assert X.shape[0] == Y.shape[0], \\\n            ""X and Y should have the same number of points (received {} and {})"".format(\n                X.shape[0], Y.shape[0])\n        assert self._states is not None, self._states\n        self._recompute_states(X, Y, profiler=profiler)\n\n    def _recompute_states(self, X, Y, profiler: GPMXNetSimpleProfiler = None):\n        if profiler is not None:\n            profiler.start(\'comp_posterstate\')\n        self._states = [GaussProcPosteriorState(\n            X, Y, self.likelihood.mean, self.likelihood.kernel,\n            self.likelihood.get_noise_variance(as_ndarray=True),\n            debug_log=(self._test_intermediates is not None),\n            test_intermediates=self._test_intermediates)]\n        if profiler is not None:\n            profiler.stop(\'comp_posterstate\')\n\n    def get_params(self):\n        return self.likelihood.get_params()\n\n    def set_params(self, param_dict):\n        self.likelihood.set_params(param_dict)\n\n    def reset_params(self):\n        """"""\n        Reset hyperparameters to their initial values (or resample them).\n\n        """"""\n        self.likelihood.initialize(ctx=self._ctx, force_reinit=True)\n        self.likelihood.hybridize()\n'"
autogluon/searcher/bayesopt/gpmxnet/gpr_mcmc.py,0,"b'from typing import Callable, Optional, List\nimport mxnet as mx\nimport numpy as np\nfrom mxnet import Context\n\nfrom autogluon.searcher.bayesopt.gpmxnet.constants import DEFAULT_MCMC_CONFIG, \\\n    MCMCConfig\nfrom autogluon.searcher.bayesopt.gpmxnet.gp_model import GaussianProcessModel\nfrom autogluon.searcher.bayesopt.gpmxnet.kernel import KernelFunction\nfrom autogluon.searcher.bayesopt.gpmxnet.likelihood import MarginalLikelihood\nfrom autogluon.searcher.bayesopt.gpmxnet.mean import ScalarMeanFunction\nfrom autogluon.searcher.bayesopt.gpmxnet.posterior_state import \\\n    GaussProcPosteriorState\nfrom autogluon.searcher.bayesopt.gpmxnet.slice import SliceSampler\nfrom autogluon.searcher.bayesopt.gpmxnet.gp_regression import \\\n    negative_log_posterior\nfrom autogluon.searcher.bayesopt.gpmxnet.gluon_blocks_helpers import \\\n    encode_unwrap_parameter\n\n\nclass GPRegressionMCMC(GaussianProcessModel):\n\n    def __init__(self,\n                 build_kernel: Callable[[], KernelFunction],\n                 mcmc_config: MCMCConfig = DEFAULT_MCMC_CONFIG,\n                 random_seed: int = None,\n                 ctx: Context = mx.cpu()):\n\n        self.mcmc_config = mcmc_config\n\n        if random_seed is not None:\n            mx.random.seed(random_seed)\n            self.random_seed = random_seed\n\n        self.likelihood = _create_likelihood(build_kernel, ctx)\n        self._states = None\n        self.samples = None\n        self.build_kernel = build_kernel\n        self._ctx = ctx\n\n    @property\n    def states(self) -> Optional[List[GaussProcPosteriorState]]:\n        return self._states\n\n    @property\n    def ctx(self):\n        return self._ctx\n\n    def fit(self, X, Y, **kwargs):\n        F = mx.nd\n        X = self._check_and_format_input(X)\n        Y = self._check_and_format_input(Y)\n\n        mean_function = self.likelihood.mean\n        if isinstance(mean_function, ScalarMeanFunction):\n            mean_function.set_mean_value(F.mean(Y).asscalar())\n\n        def _log_posterior_density(hp_values: np.ndarray) -> float:\n            # We check box constraints before converting hp_values to\n            # internal\n            if not self._is_feasible(hp_values):\n                return -float(\'inf\')\n            # Decode and write into Gluon parameters\n            _set_gp_hps(hp_values, self.likelihood)\n            neg_log = negative_log_posterior(self.likelihood, X, Y)\n            return -neg_log.asscalar()\n\n        slice_sampler = SliceSampler(\n            _log_posterior_density, 1.0, self.random_seed)\n        init_hp_values = _get_gp_hps(self.likelihood)\n\n        self.samples = slice_sampler.sample(\n            init_hp_values, self.mcmc_config.n_samples,\n            self.mcmc_config.n_burnin, self.mcmc_config.n_thinning)\n        self._states = self._create_posterior_states(self.samples, X, Y)\n\n    def recompute_states(self, X, Y, **kwargs):\n        X = self._check_and_format_input(X)\n        Y = self._check_and_format_input(Y)\n        assert len(self.samples) > 0\n        self._states = self._create_posterior_states(self.samples, X, Y)\n\n    def _is_feasible(self, hp_values: np.ndarray) -> bool:\n        pos = 0\n        for _, encoding in self.likelihood.param_encoding_pairs():\n            lower, upper = encoding.box_constraints()\n            dim = encoding.dimension\n            if lower is not None or upper is not None:\n                values = hp_values[pos:(pos + dim)]\n                if (lower is not None) and any(values < lower):\n                    return False\n                if (upper is not None) and any(values > upper):\n                    return False\n            pos += dim\n        return True\n\n    def _create_posterior_states(self, samples, X, Y):\n        states = []\n        for sample in samples:\n            likelihood = _create_likelihood(self.build_kernel, self._ctx)\n            _set_gp_hps(sample, likelihood)\n            state = GaussProcPosteriorState(\n                X, Y, likelihood.mean, likelihood.kernel,\n                likelihood.get_noise_variance(as_ndarray=True))\n            states.append(state)\n        return states\n\n\ndef _get_gp_hps(likelihood: MarginalLikelihood) -> np.ndarray:\n    """"""Get GP hyper-parameters as numpy array for a given likelihood object.""""""\n    hp_values = []\n    for param_int, encoding in likelihood.param_encoding_pairs():\n        hp_values.append(\n            encode_unwrap_parameter(mx.nd, param_int, encoding).asnumpy())\n    return np.concatenate(hp_values)\n\n\ndef _set_gp_hps(params_numpy: np.ndarray, likelihood: MarginalLikelihood):\n    """"""Set GP hyper-parameters from numpy array for a given likelihood object.""""""\n    pos = 0\n    for param, encoding in likelihood.param_encoding_pairs():\n        dim = encoding.dimension\n        values = params_numpy[pos:(pos + dim)]\n        if dim == 1:\n            internal_values = encoding.decode(values, param.name)\n        else:\n            internal_values = np.array(\n                [encoding.decode(v, param.name) for v in values])\n        param.set_data(internal_values)\n        pos += dim\n\n\ndef _create_likelihood(build_kernel, ctx) -> MarginalLikelihood:\n    """"""Create a MarginalLikelihood object with default initial GP hyperparameters.""""""\n\n    likelihood = MarginalLikelihood(\n        kernel=build_kernel(),\n        mean=ScalarMeanFunction(),\n        initial_noise_variance=None\n    )\n    likelihood.initialize(ctx=ctx, force_reinit=True)\n    likelihood.hybridize()\n\n    return likelihood\n'"
autogluon/searcher/bayesopt/gpmxnet/likelihood.py,0,"b'import mxnet as mx\nfrom mxnet import gluon\n\nfrom autogluon.searcher.bayesopt.gpmxnet.kernel import KernelFunction\nfrom autogluon.searcher.bayesopt.gpmxnet.mean import ScalarMeanFunction, \\\n    MeanFunction\nfrom autogluon.searcher.bayesopt.gpmxnet.constants import DATA_TYPE, \\\n    INITIAL_NOISE_VARIANCE, NOISE_VARIANCE_LOWER_BOUND, \\\n    NOISE_VARIANCE_UPPER_BOUND, DEFAULT_ENCODING\nfrom autogluon.searcher.bayesopt.gpmxnet.utils import create_encoding, \\\n    register_parameter\nfrom autogluon.searcher.bayesopt.gpmxnet.distribution import Gamma\nfrom autogluon.searcher.bayesopt.gpmxnet.posterior_state import \\\n    GaussProcPosteriorState\nfrom autogluon.searcher.bayesopt.gpmxnet.gluon_blocks_helpers import \\\n    encode_unwrap_parameter\n\n\nclass MarginalLikelihood(gluon.HybridBlock):\n    """"""\n    Marginal likelihood of Gaussian process with Gaussian likelihood\n\n    :param kernel: Kernel function (for instance, a Matern52---note we cannot\n        provide Matern52() as default argument since we need to provide with\n        the dimension of points in X)\n    :param mean: Mean function which depends on the input X only (by default,\n        a scalar fitted while optimizing the likelihood)\n    :param initial_noise_variance: A scalar to initialize the value of the\n        residual noise variance\n    """"""\n\n    def __init__(\n            self, kernel: KernelFunction, mean: MeanFunction = None,\n            initial_noise_variance=None, encoding_type=None, **kwargs):\n        super(MarginalLikelihood, self).__init__(**kwargs)\n        if mean is None:\n            mean = ScalarMeanFunction()\n        if initial_noise_variance is None:\n            initial_noise_variance = INITIAL_NOISE_VARIANCE\n        if encoding_type is None:\n            encoding_type=DEFAULT_ENCODING\n        self.encoding = create_encoding(\n             encoding_type, initial_noise_variance, NOISE_VARIANCE_LOWER_BOUND,\n             NOISE_VARIANCE_UPPER_BOUND, 1, Gamma(mean=0.1, alpha=0.1))\n        self.mean = mean\n        self.kernel = kernel\n        with self.name_scope():\n            self.noise_variance_internal = register_parameter(\n                self.params, \'noise_variance\', self.encoding)\n\n    def hybrid_forward(self, F, X, y, noise_variance_internal):\n        """"""\n        Actual computation of the marginal likelihood\n        See http://www.gaussianprocess.org/gpml/chapters/RW.pdf, equation (2.30)\n\n        :param F: mx.sym or mx.nd\n        :param X: input data of size (n, d)\n        :param y: targets corresponding to X, of size (n, 1)\n        :param noise_variance_internal: self.noise_variance_internal passed as\n            sym or nd depending on F (natively handled by gluon.HybridBlock)\n        """"""\n\n        noise_variance = self.encoding.get(F, noise_variance_internal)\n        state = GaussProcPosteriorState(\n            X, y, self.mean, self.kernel, noise_variance)\n        return state.neg_log_likelihood()\n\n    def param_encoding_pairs(self):\n        """"""\n        Return a list of tuples with the Gluon parameters of the likelihood and\n        their respective encodings\n        """"""\n        own_param_encoding_pairs = [\n            (self.noise_variance_internal, self.encoding)]\n        return own_param_encoding_pairs + \\\n               self.mean.param_encoding_pairs() + \\\n               self.kernel.param_encoding_pairs()\n\n    def box_constraints_internal(self):\n        """"""\n        Collect the box constraints for all the underlying parameters\n        """"""\n\n        all_box_constraints = {}\n        for param, encoding in self.param_encoding_pairs():\n            assert encoding is not None, \\\n                ""encoding of param {} should not be None"".format(param.name)\n            all_box_constraints.update(encoding.box_constraints_internal(param))\n        return all_box_constraints\n\n    def get_noise_variance(self, as_ndarray=False):\n        noise_variance = encode_unwrap_parameter(\n            mx.nd, self.noise_variance_internal, self.encoding)\n        return noise_variance if as_ndarray else noise_variance.asscalar()\n\n    def set_noise_variance(self, val):\n        self.encoding.set(self.noise_variance_internal, val)\n\n    def get_params(self):\n        result = {\'noise_variance\': self.get_noise_variance()}\n        for pref, func in [(\'kernel_\', self.kernel), (\'mean_\', self.mean)]:\n            result.update({\n                (pref + k): v for k, v in func.get_params().items()})\n        return result\n\n    def set_params(self, param_dict):\n        for pref, func in [(\'kernel_\', self.kernel), (\'mean_\', self.mean)]:\n            len_pref = len(pref)\n            stripped_dict = {\n                k[len_pref:]: v for k, v in param_dict.items()\n                if k.startswith(pref)}\n            func.set_params(stripped_dict)\n        self.set_noise_variance(param_dict[\'noise_variance\'])\n'"
autogluon/searcher/bayesopt/gpmxnet/mean.py,0,"b'from abc import ABC, abstractmethod\nimport mxnet as mx\nfrom mxnet import gluon\n\nfrom autogluon.searcher.bayesopt.gpmxnet.distribution import Normal\nfrom autogluon.searcher.bayesopt.gpmxnet.constants import INITIAL_MEAN_VALUE\nfrom autogluon.searcher.bayesopt.gpmxnet.gluon_blocks_helpers import \\\n    IdentityScalarEncoding, encode_unwrap_parameter\nfrom autogluon.searcher.bayesopt.gpmxnet.utils import register_parameter\n\n__all__ = [\n    \'MeanFunction\',\n    \'ScalarMeanFunction\',\n    \'ZeroMeanFunction\'\n]\n\n\nclass MeanFunction(gluon.HybridBlock, ABC):\n    """"""\n    Mean function, parameterizing a surrogate model together with a kernel function.\n\n    Note: KernelFunction also inherits from this interface.\n\n    """"""\n    def __init__(self, **kwargs):\n        gluon.HybridBlock.__init__(self, **kwargs)\n\n    @abstractmethod\n    def param_encoding_pairs(self):\n        """"""\n        Returns list of tuples\n            (param_internal, encoding)\n        over all Gluon parameters maintained here.\n\n        :return: List [(param_internal, encoding)]\n        """"""\n        pass\n\n    @abstractmethod\n    def get_params(self):\n        """"""\n        :return: Dictionary with hyperparameter values\n        """"""\n        pass\n\n    @abstractmethod\n    def set_params(self, param_dict):\n        """"""\n\n        :param param_dict: Dictionary with new hyperparameter values\n        :return:\n        """"""\n        pass\n\n\nclass ScalarMeanFunction(MeanFunction):\n    """"""\n    Mean function defined as a scalar (fitted while optimizing the marginal\n    likelihood).\n\n    :param initial_mean_value: A scalar to initialize the value of the mean\n\n    """"""\n    def __init__(self, initial_mean_value = INITIAL_MEAN_VALUE, **kwargs):\n        super(ScalarMeanFunction, self).__init__(**kwargs)\n\n        # Even though we do not apply specific transformation to the mean value\n        # we use an encoding to handle in a consistent way the box constraints\n        # of Gluon parameters (like bandwidths or residual noise variance)\n        self.encoding = IdentityScalarEncoding(\n            init_val=initial_mean_value, regularizer=Normal(0.0, 1.0))\n        with self.name_scope():\n            self.mean_value_internal = register_parameter(\n                self.params, \'mean_value\', self.encoding)\n\n    def hybrid_forward(self, F, X, mean_value_internal):\n        """"""\n        Actual computation of the scalar mean function\n        We compute mean_value * vector_of_ones, whose dimensions are given by\n        the the first column of X\n\n        :param F: mx.sym or mx.nd\n        :param X: input data of size (n,d) for which we want to compute the\n            mean (here, only useful to extract the right dimension)\n\n        """"""\n        mean_value = self.encoding.get(F, mean_value_internal)\n        return F.broadcast_mul(F.ones_like(F.slice_axis(\n            F.BlockGrad(X), axis=1, begin=0, end=1)), mean_value)\n\n    def param_encoding_pairs(self):\n        return [(self.mean_value_internal, self.encoding)]\n\n    def get_mean_value(self):\n        return encode_unwrap_parameter(\n            mx.nd, self.mean_value_internal, self.encoding).asscalar()\n\n    def set_mean_value(self, mean_value):\n        self.encoding.set(self.mean_value_internal, mean_value)\n\n    def get_params(self):\n        return {\'mean_value\': self.get_mean_value()}\n\n    def set_params(self, param_dict):\n        self.set_mean_value(param_dict[\'mean_value\'])\n\n\nclass ZeroMeanFunction(MeanFunction):\n    def __init__(self, **kwargs):\n        super(ZeroMeanFunction, self).__init__(**kwargs)\n\n    def hybrid_forward(self, F, X):\n        return F.zeros_like(F.slice_axis(\n            F.BlockGrad(X), axis=1, begin=0, end=1))\n\n    def param_encoding_pairs(self):\n        return []\n\n    def get_params(self):\n        return dict()\n\n    def set_params(self, param_dict):\n        pass\n'"
autogluon/searcher/bayesopt/gpmxnet/optimization_utils.py,0,"b'""""""\noptimization_utils\n==================\n\nWrapper of SciPy L-BFGS-B optimizer minimizing objectives given by MXNet\nexecutors. The main issue is to map between parameter dictionaries of\nmx.ndarray (executor side) and a single flat np.ndarray (optimizer side).\n""""""\n\nimport mxnet as mx\nimport numpy as np\nfrom scipy import optimize\nimport ctypes\n\n__all__ = [\'apply_lbfgs\',\n           \'from_executor\',\n           \'apply_lbfgs_with_multiple_starts\']\n\n\n# NOTE: _dtype is the dtype used on the NumPy side (argument, value, and\n# gradient of objective.\n_dtype = np.float64\ndefault_LBFGS_tol, default_LBFGS_maxiter = 1e-5, 500\n\nN_STARTS = 5\nSTARTING_POINT_RANDOMIZATION_STD = 1.\n\n\n# Utility functions for (un-)grouping NDArrays into Numpy arrays.\n# Useful e.g. to use optimizers from scipy.optimize to optimize\n# functions expressed in MXNet.\n\ndef _get_name_to_index(nd_arrays, names):\n    name_to_index = {}\n    global_position = 0\n    for name in names:\n        a = nd_arrays[name]\n        name_to_index[name] = np.arange(global_position, global_position+a.size)\n        global_position += a.size\n    return name_to_index\n\n\ndef _zeros_like_nd_list(l, dtype):\n    """"""\n    Create a Numpy array with size equal to the\n    sum of the sizes of all the NDArrays in the list\n    of NDArrays l.\n    """"""\n    total_size = np.sum([x.size for x in l])\n    return np.zeros(total_size, dtype)\n\n\ndef _copy_into(ndarray, nparray):\n    """"""\n    Copy the values from the given ndarray into the given (preallocated) numpy array.\n    This can be used to avoid extra memory allocation that ndarray.asnumpy()\n    performs.\n    """"""\n    assert nparray.size == ndarray.size\n    assert nparray.flags.f_contiguous and nparray.flags.behaved\n    # NOTE: The copy=False variant of NDArray.astype does not seem to work\n    if ndarray.dtype != nparray.dtype:\n        ndarray = ndarray.astype(nparray.dtype)\n    mx.base.check_call(mx.base._LIB.MXNDArraySyncCopyToCPU(\n        ndarray.handle,\n        nparray.ctypes.data_as(ctypes.c_void_p),\n        ctypes.c_size_t(ndarray.size)))\n\n\ndef _copy_to_numpy_array(l, a):\n    """"""\n    Copy values from each NDArray in the list l to the numpy array a (in\n    order).\n    """"""\n    total_size = np.sum([x.size for x in l])\n    assert total_size == a.size\n    j = 0\n    for x in l:\n        # a[j:j+x.size] = x.asnumpy().reshape((x.size,))\n        _copy_into(x, a[j:j + x.size])\n        j += x.size\n\n\ndef _copy_from_numpy_array(a, l):\n    """"""\n    Copy values from sub-arrays of the numpy array a to\n    the NDArrays in the list l. The sizes of the sub arrays\n    correspond to the sizes of the NDArrays, so that this\n    performs a copy in the reverse direction of\n    copy_to_numpy_array().\n\n    Entries of l can have different dtype than a.\n    """"""\n    total_size = np.sum([x.size for x in l])\n    assert total_size == a.size\n    j = 0\n    for x in l:\n        x[:] = a[j:j + x.size].reshape(x.shape)\n        j += x.size\n\n\ndef _make_objective(\n        exec_func, arg_dict, grad_dict, param_names):\n    # list of argument and grad NDArrays corresponding to param_names\n    param_nd_arrays = [arg_dict[name] for name in param_names]\n    grad_nd_arrays = [grad_dict[name] for name in param_names]\n    # Numpy arrays for holding parameters and gradients\n    # grad output array for direct copy\n    grad_numpy_array = _zeros_like_nd_list(grad_nd_arrays, _dtype)\n\n    # Objective and its gradients\n    def objective(a):\n        # copy parameter values to arg_dict (NDArrays)\n        _copy_from_numpy_array(a, param_nd_arrays)\n        # compute objective and gradients\n        obj_val = exec_func()\n        # Copy gradients from grad_dict (NDArray) into numpy array\n        _copy_to_numpy_array(grad_nd_arrays, grad_numpy_array)\n        return obj_val, grad_numpy_array\n\n    return objective\n\n\ndef _apply_lbfgs_internal(\n        exec_func, arg_dict, grad_dict, param_names, param_numpy_array,\n        name_to_index, bounds, **kwargs):\n    # Define bounds for L-BFGS, None by default\n    param_bounds = np.array([(None, None)] * len(param_numpy_array))\n    for name, bound in bounds.items():\n        if name in param_names:\n            param_bounds[name_to_index[name]] = bound\n    # Objective from executor\n    mx_objective = _make_objective(\n        exec_func, arg_dict, grad_dict, param_names)\n    # Run L-BFGS-B\n    LBFGS_tol = kwargs.get(""tol"", default_LBFGS_tol)\n    LBFGS_maxiter = kwargs.get(""maxiter"", default_LBFGS_maxiter)\n    LBFGS_callback = kwargs.get(""callback"", None)\n    ret_info = None\n    try:\n        output = optimize.minimize(mx_objective,\n                                   param_numpy_array,\n                                   jac=True,\n                                   method=""L-BFGS-B"",\n                                   bounds=param_bounds,\n                                   tol=LBFGS_tol,\n                                   options={""maxiter"": LBFGS_maxiter},\n                                   callback=LBFGS_callback)\n        # NOTE: Be aware that the stopping condition based on tol can terminate\n        # with a gradient size which is not small.\n        # To control L-BFGS convergence conditions for real, have to instead use\n        # something like this:\n        #                           tol=None,\n        #                           options={\n        #                               ""maxiter"": LBFGS_maxiter,\n        #                               ""ftol"": 1e-6,\n        #                               ""gtol"": 1e-1},\n        #                           callback=LBFGS_callback)\n\n        # Write result evaluation point back to arg_dict\n        optimized_param_numpy_array = output.x\n        param_nd_arrays = [arg_dict[name] for name in param_names]\n        _copy_from_numpy_array(optimized_param_numpy_array, param_nd_arrays)\n    except Exception as inst:\n        ret_info = {\n            \'type\': type(inst),\n            \'args\': inst.args,\n            \'msg\': str(inst)}\n    return ret_info\n\n\n# Utility functions for multiple restarts\n\n\nclass ExecutorDecorator:\n    """"""\n    This class is a lightweight decorator around the executor passed to L-BFGS\n    It adds the functionality of keeping track of the best objective function\n    """"""\n\n    def __init__(self, exec_func):\n        self.best_objective = np.inf\n        self._exec_func = exec_func\n\n    def exec_func(self):\n        objective = self._exec_func()\n        self.best_objective = min(self.best_objective, objective)\n        return objective\n\n\ndef _deep_copy_arg_dict(input_arg_dict):\n    """"""\n    Make a deep copy of the input arg_dict (dict param_name to mx.nd)\n    :param input_arg_dict:\n    :return: deep copy of input_arg_dict\n    """"""\n    output_arg_dict = {}\n    for name, param in input_arg_dict.items():\n        output_arg_dict[name] = param.copy()\n    return output_arg_dict\n\n\ndef _inplace_arg_dict_randomization(arg_dict, mean_arg_dict, bounds, std=STARTING_POINT_RANDOMIZATION_STD):\n    """"""\n    In order to initialize L-BFGS from multiple starting points, this function makes it possible to\n    randomize, inplace, an arg_dict (as used by executors to communicate parameters to L-BFGS).\n    The randomization is centered around mean_arg_dict, with standard deviation std.\n\n    :param arg_dict: dict param_name to mx.nd (as used in executors). This argument is modified inplace\n    :param mean_arg_dict: arg_dict around which the random perturbations occur (dict param_name to mx.nd, as used in executors))\n    :param bounds: dict param_name to (lower, upper) bounds, as used in L-BFGS\n    :param std: standard deviation according to which the (Gaussian) random perturbations happen\n    """"""\n\n    # We check that arg_dict and mean_arg_dict are compatible\n    assert arg_dict.keys() == mean_arg_dict.keys()\n    for name, param in arg_dict.items():\n        assert param.shape == mean_arg_dict[name].shape\n        assert param.dtype == mean_arg_dict[name].dtype\n        assert param.context == mean_arg_dict[name].context\n\n    # We apply a sort to make the for loop deterministic (especially with the internal calls to mx.random)\n    for name, param in sorted(arg_dict.items()):\n\n        arg_dict[name][:] = mean_arg_dict[name] + mx.random.normal(0.0, std, shape=param.shape, dtype=param.dtype, ctx=param.context)\n\n        lower, upper = bounds[name]\n        lower = lower if lower is not None else -np.inf\n        upper = upper if upper is not None else np.inf\n\n        # We project back arg_dict[name] within its specified lower and upper bounds\n        # (in case of we would have perturbed beyond those bounds)\n        arg_dict[name][:] = mx.nd.maximum(lower, mx.nd.minimum(upper, arg_dict[name]))\n\n\n# === Exported functions ===\n\n\ndef apply_lbfgs(exec_func, arg_dict, grad_dict, bounds, **kwargs):\n    """"""Run SciPy L-BFGS-B on criterion given by MXNet code\n\n    Run SciPy L-BFGS-B in order to minimize criterion given by MXNet code.\n    Criterion and gradient are computed by:\n\n        crit_val = exec_func()\n\n    Here, arguments are taken from arg_dict, and gradients are written to\n    grad_dict (both are dictionaries with NDArray values). crit_val is a\n    Python scalar, not an NDArray.\n    Think of arg_dict and grad_dict as args and args_grad arguments of an\n    MXNet executor.\n\n    The variables which L-BFGS-B is optimizing over are all those in\n    grad_dict whose values are not None. Both arg_dict and grad_dict must\n    contain values for these keys, and they must have the same shapes.\n    Both arg_dict and grad_dict can have additional entries, but these are\n    not modified.\n\n    Initial values are taken from arg_dict, and final values are written\n    back there.\n\n    L-BFGS-B allows box constraints [a, b] for any coordinate. Here, None\n    stands for -infinity (a) or +infinity (b). The default is (None, None),\n    so no constraints. In bounds, box constraints can be specified per\n    argument (the constraint applies to all coordinates of the argument).\n    Pass {} for no constraints.\n\n    If the criterion function is given by an MXNet executor mx_executor,\n    you can call\n\n        apply_bfgs(*from_executor(mx_executor), bounds, ...)\n\n    See from_executor comments for details.\n\n    :param exec_func: Function to compute criterion\n    :param arg_dict: See above\n    :param grad_dict: See above\n    :param bounds: See above\n    :return: None, or dict with info about exception caught\n    """"""\n\n    param_names = sorted(\n        [name for name, value in grad_dict.items() \\\n         if value is not None])\n    name_to_index = _get_name_to_index(arg_dict, param_names)\n    # Construct initial evaluation point (NumPy)\n    param_nd_arrays = [arg_dict[name] for name in param_names]\n    param_numpy_array = _zeros_like_nd_list(param_nd_arrays, _dtype)\n    _copy_to_numpy_array(param_nd_arrays, param_numpy_array)\n\n    return _apply_lbfgs_internal(\n        exec_func, arg_dict, grad_dict, param_names, param_numpy_array,\n        name_to_index, bounds, return_results=False, **kwargs)\n\n\ndef from_executor(executor):\n    """"""Maps MXNet executor to apply_lbfgs arguments\n\n    apply_lbfgs allows to pass exec_func, arg_dict, grad_dict to specify the\n    criterion function, its argument and gradient dictionaries. If your\n    criterion is given by an MXNet executor mx_executor, you can call\n\n        apply_bfgs(*from_executor(mx_executor), bounds, ...)\n\n    Here, arg_dict = mx_executor.arg_dict, grad_dict = mx_executor.grad_dict.\n    This requires that mx_executors represents a loss function (use\n    mx.sym.MakeLoss to be safe).\n\n    :param executor: MXNet executor representing a loss function\n    :return: exec_func, arg_dict, grad_dict arguments for apply_lbfgs\n    """"""\n\n    def exec_func():\n        executor.forward(is_train=True)\n        obj_val = executor.outputs[0].asscalar()\n        executor.backward()\n        return obj_val\n\n    return exec_func, executor.arg_dict, executor.grad_dict\n\n\ndef apply_lbfgs_with_multiple_starts(\n        exec_func, arg_dict, grad_dict, bounds, n_starts=N_STARTS, **kwargs):\n    """"""\n    When dealing with non-convex problems (e.g., optimization the marginal\n    likelihood), we typically need to start from various starting points. This\n    function applies this logic around apply_lbfgs, randomizing the starting\n    points around the initial values provided in arg_dict (see below\n    ""copy_of_initial_arg_dict"").\n\n    The first optimization happens exactly at arg_dict, so that the case\n    n_starts=1 exactly coincides with the previously used apply_lbfgs.\n    Importantly, the communication with the L-BFGS solver happens via arg_dict,\n    hence all the operations with respect to arg_dict are inplace.\n\n    We catch exceptions and return ret_infos about these. If none of the\n    restarts worked, arg_dict is not modified.\n\n    :param exec_func: see above\n    :param arg_dict: see above\n    :param grad_dict: see above\n    :param bounds: see above\n    :param n_starts: Number of times we start an optimization with L-BFGS\n        (must be >= 1)\n    :return: List ret_infos of length n_starts. Entry is None if optimization\n        worked, or otherwise has dict with info about exception caught\n    """"""\n\n    assert n_starts >= 1\n\n    copy_of_initial_arg_dict = _deep_copy_arg_dict(arg_dict)\n    best_objective_over_restarts = None\n    best_arg_dict_over_restarts = copy_of_initial_arg_dict\n\n    # Loop over restarts\n    ret_infos = []\n    for iter in range(n_starts):\n        if iter > 0:\n            _inplace_arg_dict_randomization(\n                arg_dict, copy_of_initial_arg_dict, bounds)\n        decorator = ExecutorDecorator(exec_func)\n        ret_info = apply_lbfgs(\n            decorator.exec_func, arg_dict, grad_dict, bounds, **kwargs)\n        ret_infos.append(ret_info)\n        if ret_info is None and (\n                best_objective_over_restarts is None or\n                decorator.best_objective < best_objective_over_restarts):\n            best_objective_over_restarts = decorator.best_objective\n            best_arg_dict_over_restarts = _deep_copy_arg_dict(arg_dict)\n\n    # We copy back the values of the best parameters into arg_dict (again,\n    # inplace, as required by the executor)\n    for name in arg_dict.keys():\n        arg_dict[name][:] = best_arg_dict_over_restarts[name]\n    return ret_infos\n'"
autogluon/searcher/bayesopt/gpmxnet/poster_state_incremental.py,0,"b'import mxnet as mx\nfrom mxnet.ndarray import NDArray\n\nfrom autogluon.searcher.bayesopt.gpmxnet.posterior_utils import \\\n    sample_and_cholesky_update\nfrom autogluon.searcher.bayesopt.gpmxnet.posterior_state import \\\n    GaussProcPosteriorState\nfrom autogluon.searcher.bayesopt.gpmxnet.kernel import KernelFunction\nfrom autogluon.searcher.bayesopt.gpmxnet.mean import MeanFunction\n\n\nclass GPPosteriorStateIncrementalUpdater(object):\n    """"""\n    This class supports incremental updating of Gaussian process posterior\n    states, as in IncrementalUpdateGPPosteriorState.\n\n    Compared to IncrementalUpdateGPPosteriorState, repeated computations of\n    updates in sequence of ever-larger datasets is sped up here:\n\n    - Drawing a fantasy sample and updating the state is done together, which\n      saves compute\n    - Whenever a certain size n (inputs) is encountered for the first time,\n      an executor is bound and maintained here. When called the next time for\n      this size, the executor is reused. This speeds up repeated execution,\n      since for example all memory is already allocated\n\n    NOTE: The same updater must not be used by several simulation threads in\n    parallel. This is because the input and output arguments of the executors\n    cached here are used as members of the posterior states. This works fine,\n    as long as an updater is used sequentially along a single thread, but\n    leads to errors when threads run in parallel.\n\n    For a weak check, the updater maintains the size n of the last recently\n    used executor in last_recent_exec_n. The current n in sample_and_update\n    must be > last_recent_exec_n.\n    This means that once a thread ends, reset has to be called for the updater,\n    which resets last_recent_exec_n, so the next recent thread can use it\n    afterwards.\n\n    """"""\n    def __init__(self, mean: MeanFunction, kernel: KernelFunction):\n        self.mean = mean\n        self.kernel = kernel\n        self._executors = dict()\n        self.last_recent_exec_n = None\n\n    def sample_and_update(\n            self, features: NDArray, chol_fact: NDArray, pred_mat: NDArray,\n            noise_variance: NDArray,\n            feature: NDArray) -> (NDArray, GaussProcPosteriorState):\n        """"""\n        Samples target for feature from predictive distribution (without\n        noise_variance), then updates the posterior state accordingly.\n\n        :param features: Part of current poster_state\n        :param chol_fact: Part of current poster_state\n        :param pred_mat: Part of current poster_state\n        :param noise_variance: Part of current poster_state\n        :param feature: New feature, shape (1, d)\n        :return: (target, poster_state_new)\n\n        """"""\n        # Get executor for this size (bind if not exists)\n        args = {\n            \'features\': features,\n            \'chol_fact\': chol_fact,\n            \'pred_mat\': pred_mat,\n            \'noise_variance\': noise_variance,\n            \'feature\': feature}\n        executor = self._get_executor(args)\n        chol_fact_new = executor.outputs[0]\n        pred_mat_new = executor.outputs[1]\n        features_new = executor.outputs[2]\n        target = executor.outputs[3]\n        # Run computation\n        for k, v in args.items():\n            executor.arg_dict[k][:] = v\n        executor.forward(is_train=False)\n        poster_state_new = GaussProcPosteriorState(\n            features = features_new,\n            targets=None,\n            mean=self.mean,\n            kernel=self.kernel,\n            noise_variance=noise_variance,\n            chol_fact=chol_fact_new,\n            pred_mat=pred_mat_new)\n        return target, poster_state_new\n\n    def reset(self):\n        """"""\n        Call this method at the end of a simulation thread.\n        """"""\n        self.last_recent_exec_n = None\n\n    def _get_executor(self, args):\n        n = args[\'features\'].shape[0]\n        # Sanity check\n        assert self.last_recent_exec_n is None or n > self.last_recent_exec_n, \\\n            ""Updater is not used in sequence in a single simulation thread [n = {}, last_recent_exec_n = {}]"".format(\n                n, self.last_recent_exec_n)\n        if n not in self._executors:\n            # Bind executor to sample_and_cholesky_update\n            ex_args = {\n                k: mx.nd.zeros_like(v) for k, v in args.items()}\n            executor = self._proc_s().bind(\n                ctx=args[\'features\'].context, grad_req=\'null\', args=ex_args)\n            self._executors[n] = executor\n        else:\n            executor = self._executors[n]\n        self.last_recent_exec_n = n\n        return executor\n\n    def _proc_s(self):\n        return mx.sym.Group(list(sample_and_cholesky_update(\n            mx.sym, mx.sym.Variable(\'features\'),\n            mx.sym.Variable(\'chol_fact\'), mx.sym.Variable(\'pred_mat\'),\n            self.mean, self.kernel, mx.sym.Variable(\'noise_variance\'),\n            mx.sym.Variable(\'feature\'))))\n\n'"
autogluon/searcher/bayesopt/gpmxnet/posterior_state.py,0,"b'from typing import Tuple, Optional\nimport mxnet as mx\n\nfrom autogluon.searcher.bayesopt.gpmxnet.kernel import KernelFunction\nfrom autogluon.searcher.bayesopt.gpmxnet.mean import MeanFunction\nfrom autogluon.searcher.bayesopt.gpmxnet.posterior_utils import Tensor, \\\n    mxnet_F, cholesky_computations, predict_posterior_marginals, \\\n    sample_posterior_marginals, sample_posterior_joint, cholesky_update, \\\n    negative_log_marginal_likelihood, mxnet_is_ndarray\n\n\nclass GaussProcPosteriorState(object):\n    """"""\n    Represent posterior state for Gaussian process regression model.\n    Note that members are immutable. If the posterior state is to be\n    updated, a new object is created and returned.\n\n    """"""\n    def __init__(\n            self, features: Tensor, targets: Optional[Tensor],\n            mean: MeanFunction, kernel: KernelFunction,\n            noise_variance: Tensor, debug_log: bool = False,\n            test_intermediates: Optional[dict] = None,\n            **kwargs):\n        """"""\n        If targets has m > 1 columns, they correspond to fantasy samples.\n\n        If targets is None, this is an internal (copy) constructor, where\n        kwargs contains chol_fact, pred_mat.\n\n        :param features: Input points X, shape (n, d)\n        :param targets: Targets Y, shape (n, m)\n        :param mean: Mean function m(X)\n        :param kernel: Kernel function k(X, X\')\n        :param noise_variance: Noise variance sigsq, shape (1,)\n        :param test_intermediates: See cholesky_computations\n        """"""\n        F = mxnet_F(features)\n        self.F = F\n        self.mean = mean\n        self.kernel = kernel\n        if targets is not None:\n            targets = F.reshape(targets, shape=(0, -1))\n            chol_fact, pred_mat = cholesky_computations(\n                F, features, targets, mean, kernel, noise_variance,\n                debug_log=debug_log, test_intermediates=test_intermediates)\n            if self.is_ndarray():\n                # Make sure the computations are done, and not deferred until\n                # later. This is important in order to get profiling right\n                # (and there is no advantage in deferring at this point)\n                chol_fact.wait_to_read()\n                pred_mat.wait_to_read()\n            # Make copy, just to be safe:\n            self.features = features if F == mx.sym else features.copy()\n            self.chol_fact = chol_fact\n            self.pred_mat = pred_mat\n            self._test_intermediates = test_intermediates\n        else:\n            # Internal (copy) constructor\n            self.features = features\n            self.chol_fact = kwargs[\'chol_fact\']\n            self.pred_mat = kwargs[\'pred_mat\']\n\n    @property\n    def num_data(self):\n        self._check_is_ndarray()\n        return self.features.shape[0]\n\n    @property\n    def num_features(self):\n        self._check_is_ndarray()\n        return self.features.shape[1]\n\n    @property\n    def num_fantasies(self):\n        self._check_is_ndarray()\n        return self.pred_mat.shape[1]\n\n    def _state_args(self):\n        return [self.F, self.features, self.mean, self.kernel, self.chol_fact,\n            self.pred_mat]\n\n    def predict(self, test_features: Tensor) -> Tuple[Tensor, Tensor]:\n        return predict_posterior_marginals(\n            *self._state_args(), test_features,\n            test_intermediates=self._test_intermediates)\n\n    def sample_joint(self, test_features: Tensor, num_samples: int=1) -> \\\n            Tensor:\n        return sample_posterior_joint(\n            *self._state_args(), test_features, num_samples)\n\n    def sample_marginals(self, test_features: Tensor, num_samples: int=1) -> \\\n            Tensor:\n        return sample_posterior_marginals(\n            *self._state_args(), test_features, num_samples)\n\n    def neg_log_likelihood(self) -> Tensor:\n        """"""\n        Works only if fantasy samples are not used (single targets vector).\n\n        :return: Negative log (marginal) likelihood\n        """"""\n        return negative_log_marginal_likelihood(\n            self.F, self.chol_fact, self.pred_mat)\n\n    def is_ndarray(self):\n        return mxnet_is_ndarray(self.F)\n\n    def _check_is_ndarray(self):\n        assert self.is_ndarray(), \\\n            ""Implemented only for mxnet.ndarray, not for mxnet.symbol""\n\n\nclass IncrementalUpdateGPPosteriorState(GaussProcPosteriorState):\n    """"""\n    Extension of GaussProcPosteriorState which allows for incremental\n    updating, given that a single data case is appended to the training\n    set.\n\n    In order to not mutate members, the update method returns a new\n    object.\n\n    """"""\n    def __init__(\n            self, features: Tensor, targets: Optional[Tensor],\n            mean: MeanFunction, kernel: KernelFunction,\n            noise_variance: Tensor, **kwargs):\n        super(IncrementalUpdateGPPosteriorState, self).__init__(\n            features, targets, mean, kernel, noise_variance, **kwargs)\n        # Noise variance is needed here for updates (make copy, to be safe)\n        self.noise_variance = noise_variance if self.F == mx.sym \\\n            else noise_variance.copy()\n\n    def update(self, feature: Tensor, target: Tensor) -> \\\n            \'IncrementalUpdateGPPosteriorState\':\n        """"""\n        :param feature: Additional input xstar, shape (1, d)\n        :param target: Additional target ystar, shape (1, m)\n        :return: Posterior state for increased data set\n        """"""\n        # Ensure feature, target have one row\n        F = self.F\n        feature = F.reshape(feature, shape=(1, -1))\n        target = F.reshape(target, shape=(1, -1))\n        if self.is_ndarray():\n            assert feature.shape[1] == self.features.shape[1], \\\n                ""feature.shape[1] = {} != {} = self.features.shape[1]"".format(\n                    feature.shape[1], self.features.shape[1])\n            assert target.shape[1] == self.pred_mat.shape[1], \\\n                ""target.shape[1] = {} != {} = self.pred_mat.shape[1]"".format(\n                    target.shape[1], self.pred_mat.shape[1])\n        chol_fact_new, pred_mat_new = cholesky_update(\n            F, self.features, self.chol_fact, self.pred_mat, self.mean,\n            self.kernel, self.noise_variance, feature, target)\n        features_new = F.concat(self.features, feature, dim=0)\n        # Create object by calling internal constructor\n        state_new = IncrementalUpdateGPPosteriorState(\n            features=features_new,\n            targets=None,\n            mean=self.mean,\n            kernel=self.kernel,\n            noise_variance=self.noise_variance,\n            chol_fact=chol_fact_new,\n            pred_mat=pred_mat_new)\n        return state_new\n\n    def expand_fantasies(self, num_fantasies: int) -> \\\n            \'IncrementalUpdateGPPosteriorState\':\n        """"""\n        If this posterior has been created with a single targets vector,\n        shape (n, 1), use this to duplicate this vector m = num_fantasies\n        times. Call this method before fantasy targets are appended by\n        update.\n\n        :param num_fantasies: Number m of fantasy samples\n        :return: New state with targets duplicated m times\n        """"""\n        assert num_fantasies > 1\n        F = self.F\n        if self.is_ndarray():\n            assert self.pred_mat.shape[1] == 1, \\\n                ""Method requires posterior without fantasy samples""\n        pred_mat_new = F.concat(*([self.pred_mat] * num_fantasies), dim=1)\n        state_new = IncrementalUpdateGPPosteriorState(\n            features=self.features,\n            targets=None,\n            mean=self.mean,\n            kernel=self.kernel,\n            noise_variance=self.noise_variance,\n            chol_fact=self.chol_fact,\n            pred_mat=pred_mat_new)\n        return state_new\n'"
autogluon/searcher/bayesopt/gpmxnet/posterior_utils.py,0,"b'from typing import Union\nimport mxnet as mx\nimport numpy as np\n\nfrom autogluon.searcher.bayesopt.gpmxnet.constants import \\\n    NOISE_VARIANCE_LOWER_BOUND, MIN_POSTERIOR_VARIANCE, \\\n    MIN_CHOLESKY_DIAGONAL_VALUE\nfrom autogluon.searcher.bayesopt.gpmxnet.custom_op import AddJitterOp, \\\n    AddJitterOpProp\n\n\nTensor = Union[mx.nd.NDArray, mx.sym.Symbol]\n\n\ndef mxnet_F(x: Tensor):\n    if isinstance(x, mx.nd.NDArray):\n        return mx.nd\n    else:\n        return mx.sym\n\n\ndef mxnet_is_ndarray(F):\n    return F.__name__ == \'mxnet.ndarray\'\n\n\ndef cholesky_computations(\n        F, features, targets, mean, kernel, noise_variance, debug_log=False,\n        test_intermediates=None):\n    """"""\n    Given input matrix X (features), target matrix Y (targets), mean and kernel\n    function, compute posterior state {L, P}, where L is the Cholesky factor\n    of\n        k(X, X) + sigsq_final * I\n    and\n        L P = Y - mean(X)\n    Here, sigsq_final >= noise_variance is minimal such that the Cholesky\n    factorization does not fail.\n\n    :param F: mx.nd or mx.sym\n    :param features: Input matrix X\n    :param targets: Target matrix Y\n    :param mean: Mean function\n    :param kernel: Kernel function\n    :param noise_variance: Noise variance (may be increased)\n    :param debug_log: Debug output during add_jitter CustomOp?\n    :param test_intermediates: If given, all intermediates are written into\n        this dict (only if F = mx.nd)\n    :return: L, P\n\n    """"""\n    kernel_mat = kernel(features, features)\n    # Add jitter to noise_variance (if needed) in order to guarantee that\n    # Cholesky factorization works\n    sys_mat = F.Custom(\n        kernel_mat, noise_variance, name=""add_jitter"", op_type=\'add_jitter\',\n        initial_jitter_factor=NOISE_VARIANCE_LOWER_BOUND,\n        debug_log=\'true\' if debug_log else \'false\')\n    chol_fact = F.linalg.potrf(sys_mat)\n    centered_y = F.broadcast_sub(\n        targets, F.reshape(mean(features), shape=(-1, 1)))\n    pred_mat = F.linalg.trsm(chol_fact, centered_y)\n    # For testing:\n    if test_intermediates is not None and mxnet_is_ndarray(F):\n        assert isinstance(test_intermediates, dict)\n        test_intermediates.update({\n            \'features\': features.asnumpy(),\n            \'targets\': targets.asnumpy(),\n            \'noise_variance\': noise_variance.asscalar(),\n            \'kernel_mat\': kernel_mat.asnumpy(),\n            \'sys_mat\': sys_mat.asnumpy(),\n            \'chol_fact\': chol_fact.asnumpy(),\n            \'pred_mat\': pred_mat.asnumpy(),\n            \'centered_y\': centered_y.asnumpy()})\n        test_intermediates.update(kernel.get_params())\n        test_intermediates.update(mean.get_params())\n    return chol_fact, pred_mat\n\n\ndef predict_posterior_marginals(\n        F, features, mean, kernel, chol_fact, pred_mat, test_features,\n        test_intermediates=None):\n    """"""\n    Computes posterior means and variances for test_features.\n    If pred_mat is a matrix, so will be posterior_means, but not\n    posterior_variances. Reflects the fact that for GP regression and fixed\n    hyperparameters, the posterior mean depends on the targets y, but the\n    posterior covariance does not.\n\n    :param F: mx.sym or mx.nd\n    :param features: Training inputs\n    :param mean: Mean function\n    :param kernel: Kernel function\n    :param chol_fact: Part L of posterior state\n    :param pred_mat: Part P of posterior state\n    :param test_features: Test inputs\n    :return: posterior_means, posterior_variances\n\n    """"""\n    k_tr_te = kernel(features, test_features)\n    linv_k_tr_te = F.linalg.trsm(chol_fact, k_tr_te)\n    posterior_means = F.broadcast_add(\n        F.linalg.gemm2(\n            linv_k_tr_te, pred_mat, transpose_a=True, transpose_b=False),\n        F.reshape(mean(test_features), shape=(-1, 1)))\n    posterior_variances = kernel.diagonal(F, test_features) - F.sum(\n        F.square(linv_k_tr_te), axis=0)\n    # For testing:\n    if test_intermediates is not None and mxnet_is_ndarray(F):\n        assert isinstance(test_intermediates, dict)\n        test_intermediates.update({\n            \'k_tr_te\': k_tr_te.asnumpy(),\n            \'linv_k_tr_te\': linv_k_tr_te.asnumpy(),\n            \'test_features\': test_features.asnumpy(),\n            \'pred_means\': posterior_means.asnumpy(),\n            \'pred_vars\': F.reshape(F.maximum(\n                posterior_variances, MIN_POSTERIOR_VARIANCE),\n                shape=(-1,)).asnumpy()})\n    return posterior_means, F.reshape(F.maximum(\n        posterior_variances, MIN_POSTERIOR_VARIANCE), shape=(-1,))\n\n\ndef sample_posterior_marginals(\n        F, features, mean, kernel, chol_fact, pred_mat, test_features,\n        num_samples=1):\n    """"""\n    Draws num_sample samples from the product of marginals of the posterior\n    over input points test_features. If pred_mat is a matrix with m columns,\n    the samples returned have shape (n_test, m, num_samples).\n\n    Note: If F = mx.sym, the samples always have shape\n    (n_test, m, num_samples), even if m = 1.\n\n    :param F: mx.sym or mx.nd\n    :param features: Training inputs\n    :param mean: Mean function\n    :param kernel: Kernel function\n    :param chol_fact: Part L of posterior state\n    :param pred_mat: Part P of posterior state\n    :param test_features: Test inputs\n    :param num_samples: Number of samples to draw\n    :return: Samples, shape (n_test, num_samples) or (n_test, m, num_samples)\n\n    """"""\n    # Shape of post_means is (n_test, m)\n    post_means, post_vars = predict_posterior_marginals(\n        F, features, mean, kernel, chol_fact, pred_mat, test_features)\n    post_means = F.expand_dims(post_means, axis=-1)  # (n_test, m, 1)\n    post_stds = F.sqrt(F.reshape(post_vars, shape=(-1, 1, 1)))  # (n_test, 1, 1)\n    n01_vecs = [F.random.normal_like(post_means) for _ in range(num_samples)]\n    n01_mat = F.concat(*n01_vecs, dim=-1)  # (n_test, m, num_samples)\n    samples = F.broadcast_add(F.broadcast_mul(n01_mat, post_stds), post_means)\n    # Remove m == 1 dimension. Can be done only for mx.nd\n    if mxnet_is_ndarray(F) and samples.shape[1] == 1:\n        samples = F.reshape(samples, shape=(0, -1))  # (n_test, num_samples)\n    return samples\n\n\ndef sample_posterior_joint(\n        F, features, mean, kernel, chol_fact, pred_mat, test_features,\n        num_samples=1):\n    """"""\n    Draws num_sample samples from joint posterior distribution over inputs\n    test_features. This is done by computing mean and covariance matrix of\n    this posterior, and using the Cholesky decomposition of the latter. If\n    pred_mat is a matrix with m columns, the samples returned have shape\n    (n_test, m, num_samples).\n\n    Note: If F = mx.sym, the samples always have shape\n    (n_test, m, num_samples), even if m = 1.\n\n    :param F: mx.sym or mx.nd\n    :param features: Training inputs\n    :param mean: Mean function\n    :param kernel: Kernel function\n    :param chol_fact: Part L of posterior state\n    :param pred_mat: Part P of posterior state\n    :param test_features: Test inputs\n    :param num_samples: Number of samples to draw\n    :return: Samples, shape (n_test, num_samples) or (n_test, m, num_samples)\n\n    """"""\n    k_tr_te = kernel(features, test_features)\n    linv_k_tr_te = F.linalg.trsm(chol_fact, k_tr_te)\n    posterior_mean = F.broadcast_add(\n        F.linalg.gemm2(\n            linv_k_tr_te, pred_mat, transpose_a=True, transpose_b=False),\n        F.reshape(mean(test_features), shape=(-1, 1)))\n    posterior_cov = kernel(test_features, test_features) - F.linalg.syrk(\n        linv_k_tr_te, transpose=True)\n    # Compute the Cholesky decomposition of posterior_cov (expensive!)\n    # Add some jitter proactively (>= 1e-5)\n    ones_like_test = F.ones_like(F.reshape(F.slice_axis(\n        F.BlockGrad(test_features), axis=1, begin=0, end=1), shape=(-1,)))\n    jitter_init = F.ones_like(\n        F.slice_axis(ones_like_test, axis=0, begin=0, end=1)) * (1e-5)\n    sys_mat = F.Custom(\n        posterior_cov, jitter_init, name=""add_jitter"",\n        op_type=\'add_jitter\', initial_jitter_factor=NOISE_VARIANCE_LOWER_BOUND)\n    lfact = F.linalg.potrf(sys_mat)\n    # Draw samples\n    # posterior_mean.shape = (n_test, m), where m is number of cols of pred_mat\n    # Reshape to (n_test, m, 1)\n    posterior_mean = F.expand_dims(posterior_mean, axis=-1)\n    n01_vecs = [\n        F.random.normal_like(posterior_mean) for _ in range(num_samples)]\n    # n01_mat.shape = (n_test, m, num_samples) -> (n_test, m * num_samples)\n    n01_mat = F.reshape(F.concat(*n01_vecs, dim=-1), shape=(0, -1))\n    # Reshape samples back to (n_test, m, num_samples) after trmm\n    samples = F.reshape(\n        F.linalg.trmm(lfact, n01_mat), shape=(0, -1, num_samples))\n    samples = F.broadcast_add(samples, posterior_mean)\n    # Remove m == 1 dimension. Can be done only for mx.nd\n    if mxnet_is_ndarray(F) and samples.shape[1] == 1:\n        samples = F.reshape(samples, shape=(0, -1))  # (n_test, num_samples)\n    return samples\n\n\ndef _compute_lvec(F, features, chol_fact, kernel, feature):\n    kvec = F.reshape(kernel(features, feature), shape=(-1, 1))\n    return F.reshape(F.linalg.trsm(chol_fact, kvec), shape=(1, -1))\n\n\ndef cholesky_update(\n        F, features, chol_fact, pred_mat, mean, kernel, noise_variance,\n        feature, target, lvec=None):\n    """"""\n    Incremental update of posterior state (Cholesky factor, prediction\n    matrix), given one datapoint (feature, target).\n\n    Note: noise_variance is the initial value, before any jitter may have\n    been added to compute chol_fact. Here, we add the minimum amount of\n    jitter such that the new diagonal entry of the Cholesky factor is\n    >= MIN_CHOLESKY_DIAGONAL_VALUE. This means that if cholesky_update is\n    used several times, we in fact add a diagonal (but not spherical)\n    jitter matrix.\n\n    :param F:\n    :param features: Shape (n, d)\n    :param chol_fact: Shape (n, n)\n    :param pred_mat: Shape (n, m)\n    :param mean:\n    :param kernel:\n    :param noise_variance:\n    :param feature: Shape (1, d)\n    :param target: Shape (1, m)\n    :param lvec: If given, this is the new column of the Cholesky factor\n        except the diagonal entry. If not, this is computed here\n    :return: chol_fact_new (n+1, n+1), pred_mat_new (n+1, m)\n\n    """"""\n    if lvec is None:\n        lvec = _compute_lvec(F, features, chol_fact, kernel, feature)\n    kscal = F.reshape(kernel.diagonal(F, feature), shape=(1,))\n    noise_variance = F.reshape(noise_variance, shape=(1,))\n    lsqscal = F.maximum(kscal + noise_variance - F.sum(F.square(lvec)),\n                        MIN_CHOLESKY_DIAGONAL_VALUE ** 2)\n    lscal = F.reshape(F.sqrt(lsqscal), shape=(1, 1))\n    mscal = F.reshape(mean(feature), shape=(1, 1))\n    pvec = F.broadcast_sub(target, mscal)\n    pvec = F.broadcast_div(pvec - F.linalg.gemm2(lvec, pred_mat), lscal)\n    pred_mat_new = F.concat(pred_mat, pvec, dim=0)\n    tmpmat = F.concat(chol_fact, lvec, dim=0)\n    zerovec = F.zeros_like(F.reshape(lvec, shape=(-1, 1)))\n    chol_fact_new = F.concat(\n        tmpmat, F.concat(zerovec, lscal, dim=0), dim=1)\n    return chol_fact_new, pred_mat_new\n\n\n# Specialized routine, used in GPPosteriorStateIncrementalUpdater.\n# The idea is to share the computation of lvec between sampling a new target\n# value and incremental Cholesky update\ndef sample_and_cholesky_update(\n        F, features, chol_fact, pred_mat, mean, kernel, noise_variance,\n        feature):\n    # Draw sample target. Also, lvec is reused below\n    lvec = _compute_lvec(F, features, chol_fact, kernel, feature)\n    pred_mean = F.broadcast_add(\n        F.dot(lvec, pred_mat), F.reshape(mean(feature), shape=(1, 1)))\n    # Note: We do not add noise_variance to the predictive variance\n    pred_std = F.reshape(F.sqrt(F.maximum(\n        kernel.diagonal(F, feature) - F.sum(F.square(lvec)),\n        MIN_POSTERIOR_VARIANCE)), shape=(1, 1))\n    n01mat = F.random.normal_like(pred_mean)\n    target = pred_mean + F.broadcast_mul(n01mat, pred_std)\n    # Incremental posterior update\n    chol_fact_new, pred_mat_new = cholesky_update(\n        F, features, chol_fact, pred_mat, mean, kernel, noise_variance,\n        feature, target, lvec=lvec)\n    features_new = F.concat(features, feature, dim=0)\n    return chol_fact_new, pred_mat_new, features_new, target\n\n\ndef negative_log_marginal_likelihood(F, chol_fact, pred_mat):\n    """"""\n    The marginal likelihood is only computed if pred_mat has a single column\n    (not for fantasy sample case).\n    """"""\n\n    if mxnet_is_ndarray(F):\n        assert pred_mat.ndim == 1 or pred_mat.shape[1] == 1, \\\n            ""Multiple target vectors are not supported""\n    sqnorm_predmat = F.sum(F.square(pred_mat))\n    logdet_cholfact = 2.0 * F.sum(F.log(F.abs(F.diag(chol_fact))))\n    # pred_mat is a vector of dimension the number of samples\n    n_samples = F.sum(F.ones_like(F.BlockGrad(pred_mat)))\n    return 0.5 * (sqnorm_predmat + n_samples * np.log(2 * np.pi) + logdet_cholfact)\n'"
autogluon/searcher/bayesopt/gpmxnet/slice.py,0,"b'from typing import Callable, Tuple, List\nimport numpy as np\n\nfrom autogluon.searcher.bayesopt.gpmxnet import SliceException\n\n\nMAX_STEP_OUT = 200\nMAX_STEP_LOOP = 200\n\n\nclass SliceSampler(object):\n\n    def __init__(self, log_density: Callable[[np.ndarray], float], scale: float, random_seed: int):\n        self.log_density = log_density\n        self.scale = scale  # default in scala core is 1.0\n        self.random_state = np.random.RandomState(random_seed)\n\n    def _gen_next_sample(self, x0: np.ndarray) -> np.ndarray:\n        random_direction = gen_random_direction(len(x0), self.random_state)\n\n        def sliced_log_density(_movement: float) -> float:\n            return self.log_density(x0 + random_direction * _movement)\n\n        # a quantity used to determine the bounds and accept movement along random_direction\n        log_pivot = sliced_log_density(0.0) + np.log(self.random_state.rand())\n\n        lower_bound, upper_bound = slice_sampler_step_out(log_pivot, self.scale, sliced_log_density, self.random_state)\n        movement = slice_sampler_step_in(lower_bound, upper_bound, log_pivot, sliced_log_density, self.random_state)\n        return x0 + random_direction * movement\n\n    def sample(self, init_sample: np.ndarray, num_samples: int, burn: int, thin: int) -> List[np.ndarray]:\n        samples = []\n        next_sample = init_sample\n        for _ in range(num_samples):\n            next_sample = self._gen_next_sample(next_sample)\n            samples.append(next_sample)\n        return samples[burn::thin]\n\n\ndef gen_random_direction(dimension: int, random_state: np.random.RandomState) -> np.ndarray:\n    random_direction = random_state.randn(dimension)\n    random_direction *= 1.0 / np.linalg.norm(random_direction)\n    return random_direction\n\n\ndef slice_sampler_step_out(log_pivot: float, scale: float,\n        sliced_log_density: Callable[[float], float], random_state: np.random.RandomState) -> Tuple[float, float]:\n\n    r = random_state.rand()\n    lower_bound = -r * scale\n    upper_bound = lower_bound + scale\n\n    def bound_step_out(bound, direction):\n        """"""direction -1 for lower bound, +1 for upper bound""""""\n        for _ in range(MAX_STEP_OUT):\n            if sliced_log_density(bound) <= log_pivot:\n                return bound\n            else:\n                bound += direction * scale\n        raise SliceException(""Reach maximum iteration ({}) while stepping out for bound ({})"".format(MAX_STEP_OUT, direction))\n\n    lower_bound = bound_step_out(lower_bound, -1.)\n    upper_bound = bound_step_out(upper_bound, 1.)\n    return lower_bound, upper_bound\n\n\ndef slice_sampler_step_in(lower_bound: float, upper_bound: float, log_pivot: float,\n        sliced_log_density: Callable[[float], float], random_state: np.random.RandomState) -> float:\n    """"""Find the right amount of movement along with a random_direction""""""\n    for _ in range(MAX_STEP_LOOP):\n        movement = (upper_bound - lower_bound) * random_state.rand() + lower_bound\n        if movement == 0.0:\n            raise SliceException(""The interval for slice sampling has reduced to zero in step in"")\n        if sliced_log_density(movement) > log_pivot:\n            return movement\n        else:\n            lower_bound = movement if movement < 0.0 else lower_bound\n            upper_bound = movement if movement > 0.0 else upper_bound\n    raise SliceException(""Reach maximum iteration ({}) while stepping in"".format(MAX_STEP_LOOP))\n'"
autogluon/searcher/bayesopt/gpmxnet/utils.py,0,"b'import mxnet as mx\nfrom mxnet import gluon\n\nfrom autogluon.searcher.bayesopt.gpmxnet.gluon_blocks_helpers import \\\n    LogarithmScalarEncoding, PositiveScalarEncoding, encode_unwrap_parameter\nfrom autogluon.searcher.bayesopt.gpmxnet.constants import DATA_TYPE\n\n\ndef param_to_pretty_string(gluon_param, encoding):\n    """"""\n    Take a gluon parameter and transform it to a string amenable to plotting\n    If need be, the gluon parameter is appropriately encoded (e.g., log-exp transform).\n\n    :param gluon_param: gluon parameter\n    :param encoding: object in charge of encoding/decoding the gluon_param\n    """"""\n\n    assert isinstance(gluon_param, gluon.Parameter)\n    assert encoding is not None, \\\n        ""encoding of param {} should not be None"".format(gluon_param.name)\n\n    param_as_numpy = encode_unwrap_parameter(\n        mx.nd, gluon_param, encoding).asnumpy()\n    return ""{}: {}"".format(\n        gluon_param.name, "";"".join(\n            ""{:.6f}"".format(value) for value in param_as_numpy))\n\n\ndef create_encoding(\n        encoding_name, init_val, constr_lower, constr_upper, dimension, prior):\n    assert encoding_name in [\'logarithm\', \'positive\'], \\\n        ""encoding name can only be \'logarithm\' or \'positive\'""\n\n    if encoding_name == \'logarithm\':\n        return LogarithmScalarEncoding(init_val=init_val,\n                                       constr_lower=constr_lower,\n                                       constr_upper=constr_upper,\n                                       dimension=dimension,\n                                       regularizer=prior)\n    else:\n        return PositiveScalarEncoding(lower=constr_lower,\n                                      init_val = init_val,\n                                      constr_upper=constr_upper,\n                                      dimension=dimension,\n                                      regularizer=prior)\n\n\nPARAMETER_POSTFIX = \'_internal\'\n\n\ndef get_name_internal(name):\n    return name + PARAMETER_POSTFIX\n\n\ndef register_parameter(\n        params, name, encoding, shape=(1,), dtype=DATA_TYPE):\n    return params.get(\n        get_name_internal(name), shape=shape,\n        init=mx.init.Constant(encoding.init_val_int), dtype=dtype)\n'"
autogluon/searcher/bayesopt/gpmxnet/warping.py,0,"b'import mxnet as mx\nfrom mxnet import gluon\n\nfrom autogluon.searcher.bayesopt.gpmxnet.constants import DEFAULT_ENCODING, \\\n    INITIAL_WARPING, WARPING_LOWER_BOUND, WARPING_UPPER_BOUND, NUMERICAL_JITTER\nfrom autogluon.searcher.bayesopt.gpmxnet.utils import create_encoding, \\\n    register_parameter\nfrom autogluon.searcher.bayesopt.gpmxnet.distribution import LogNormal\nfrom autogluon.searcher.bayesopt.gpmxnet.kernel import KernelFunction\nfrom autogluon.searcher.bayesopt.gpmxnet.gluon_blocks_helpers import \\\n    encode_unwrap_parameter\n\n\nclass OneDimensionalWarping(gluon.HybridBlock):\n    """"""\n    HybridBlock that is responsible for the warping of a single, column\n    feature x. Typically, the full data X = [x1, x2,..., xd] is in (n, d) and\n    each xi is a column feature in (n, 1).\n\n    Consider column feature x and assume that the entries of x are contained in\n    the range input_range. Each entry of x is transformed by\n        warping(u) = 1. - (1. - R(u)^a)^b,\n    with a,b two non negative parameters learned by empirical Bayes, and R(.)\n    is a linear transformation that, based on input_range, rescales the entry\n    of x into [eps, 1-eps] for some small eps > 0.\n\n    :param input_range: tuple that contains the lower and upper bounds of the\n        entries of x.\n    """"""\n\n    def __init__(self, input_range, encoding_type=DEFAULT_ENCODING, **kwargs):\n        super(OneDimensionalWarping, self).__init__(**kwargs)\n\n        self.input_range = input_range\n        self.encoding = create_encoding(\n            encoding_type, INITIAL_WARPING, WARPING_LOWER_BOUND,\n            WARPING_UPPER_BOUND, 2, LogNormal(0.0, 0.75))\n        with self.name_scope():\n            self.warping_internal = register_parameter(\n                self.params, \'warping\', self.encoding, shape=(2,))\n\n    def _rescale(self, x):\n        """"""\n        We linearly rescale the entries of x into [NUMERICAL_JITTER, 1-NUMERICAL_JITTER]\n        In this way, we avoid the differentiability problems at 0\n        :param x: mx.sym or mx.nd array to be rescaled\n        """"""\n\n        lower, upper = self.input_range\n\n        P = (1. - 2 * NUMERICAL_JITTER)/(upper - lower)\n        Q = (NUMERICAL_JITTER * (upper + lower) - lower)/(upper - lower)\n\n        return P * x + Q\n\n    def hybrid_forward(self, F, x, warping_internal):\n        """"""\n        Actual computation of the warping transformation (see details above)\n\n        :param F: mx.sym or mx.nd\n        :param x: input data of size (n,1)\n        """"""\n\n        warping = self.encoding.get(F, warping_internal)\n        # We extract the first and second entries of warping (the shape of\n        # warping_internal is fixed to 2)\n        warping_a = F.take(warping, F.zeros(shape=(1,)))\n        warping_b = F.take(warping, F.ones(shape=(1,)))\n\n        return 1. - F.broadcast_power(1. - F.broadcast_power(\n            self._rescale(x), warping_a), warping_b)\n\n    def param_encoding_pairs(self):\n        """"""\n        Return a list of tuples with the Gluon parameters of the 1-D warping\n        and their respective encodings\n        """"""\n\n        return [(self.warping_internal, self.encoding)]\n\n    def get_params(self):\n        warping = encode_unwrap_parameter(\n            mx.nd, self.warping_internal,\n            self.encoding).asnumpy().reshape((-1,))\n        return {\n            \'warping_a\': warping[0],\n            \'warping_b\': warping[1]}\n\n    def set_params(self, param_dict):\n        warping = [param_dict[\'warping_a\'], param_dict[\'warping_b\']]\n        self.encoding.set(self.warping_internal, warping)\n\n\nclass Warping(gluon.HybridBlock):\n    """"""\n    HybridBlock that computes warping over all the columns of some input data X.\n    If X is of size (n,dimension), where dimension has to be specified, a 1-D warping\n    transformation is applied to each column X[:,j] with j a key in index_to_range.\n    More precisely, index_to_range is a dictionary of the form\n        {\n            j : (lower_bound_column_j, upper_bound_column_j),\n            k : (lower_bound_column_k, upper_bound_column_k),\n            ....\n        }\n    that maps column indexes to their corresponding ranges.\n    """"""\n\n    def __init__(self, dimension, index_to_range, encoding_type=DEFAULT_ENCODING,\n                 **kwargs):\n        super(Warping, self).__init__(**kwargs)\n\n        assert isinstance(index_to_range, dict)\n        assert all(isinstance(r, tuple) for r in index_to_range.values())\n        assert all(r[0] < r[1] for r in index_to_range.values())\n\n        self.transformations = []\n        self._params_encoding_pairs = []\n        self.dimension = dimension\n        self.index_to_range = index_to_range\n\n        some_are_warped = False\n        for col_index in range(dimension):\n            if col_index in index_to_range:\n                transformation = OneDimensionalWarping(\n                    index_to_range[col_index], encoding_type=encoding_type)\n                # To make sure that OneDimensionalWarping will get initialized\n                # and managed by Warping, we register it as a child.\n                self.register_child(transformation, name=transformation.name)\n                self._params_encoding_pairs += \\\n                    transformation.param_encoding_pairs()\n                some_are_warped = True\n            else:\n                # if a column is not warped, we do not apply any transformation\n                transformation = lambda x: x\n            self.transformations.append(transformation)\n        assert some_are_warped, \\\n            ""At least one of the dimensions must be warped""\n\n    def hybrid_forward(self, F, X):\n        """"""\n        Actual computation of warping applied to each column of X\n\n        :param F: mx.sym or mx.nd\n        :param X: input data of size (n,dimension)\n        """"""\n        warped_X = []\n        for col_index, transformation in enumerate(self.transformations):\n            x = F.slice_axis(X, axis=1, begin=col_index, end=col_index+1)\n            warped_X.append(transformation(x))\n\n        return F.concat(*warped_X, dim=1)\n\n    def param_encoding_pairs(self):\n        """"""\n        Return a list of tuples with the Gluon parameters of the warping and\n        their respective encodings\n        """"""\n\n        return self._params_encoding_pairs\n\n    def get_params(self):\n        """"""\n        Keys are warping_a, warping_b if there is one dimension, and\n        warping_a<k>, warping_b<k> otherwise.\n\n        """"""\n        if len(self.transformations) == 1:\n            result = self.transformations[0].get_params()\n        else:\n            result = dict()\n            for i, warping in enumerate(self.transformations):\n                if isinstance(warping, OneDimensionalWarping):\n                    istr = str(i)\n                    for k, v in warping.get_params().items():\n                        result[k + istr] = v\n        return result\n\n    def set_params(self, param_dict):\n        if len(self.transformations) == 1:\n            self.transformations[0].set_params(param_dict)\n        else:\n            transf_keys = None\n            for i, warping in enumerate(self.transformations):\n                if isinstance(warping, OneDimensionalWarping):\n                    if transf_keys is None:\n                        transf_keys = warping.get_params().keys()\n                    istr = str(i)\n                    stripped_dict = dict()\n                    for k in transf_keys:\n                        stripped_dict[k] = param_dict[k + istr]\n                    warping.set_params(stripped_dict)\n\n\nclass WarpedKernel(KernelFunction):\n    """"""\n    HybridBlock that composes warping with an arbitrary kernel\n    """"""\n\n    def __init__(self, kernel: KernelFunction, warping: Warping, **kwargs):\n        super(WarpedKernel, self).__init__(kernel.dimension, **kwargs)\n        self.kernel = kernel\n        self.warping = warping\n\n    def hybrid_forward(self, F, X1, X2):\n        """"""\n        Actual computation of the composition of warping with an arbitrary\n        kernel K. If we have input data X1 and X2, of respective dimensions\n        (n1, d) and (n2, d), we compute the matrix\n\n            K(warping(X1), warping(X2)) of size (n1,n2)\n            whose (i,j) entry is given by K(warping(X1[i,:]), warping(X2[j,:]))\n\n        :param F: mx.sym or mx.nd\n        :param X1: input data of size (n1, d)\n        :param X2: input data of size (n2, d)\n        """"""\n\n        warped_X1 = self.warping(X1)\n        if X1 is X2:\n            warped_X2 = warped_X1\n        else:\n            warped_X2 = self.warping(X2)\n        return self.kernel(warped_X1, warped_X2)\n\n    def diagonal(self, F, X):\n        # If kernel.diagonal does not depend on content of X (but just its\n        # size), can pass X instead of self.warping(X)\n        warped_X = self.warping(X) if self.kernel.diagonal_depends_on_X() \\\n            else X\n        return self.kernel.diagonal(F, warped_X)\n\n    def diagonal_depends_on_X(self):\n        return self.kernel.diagonal_depends_on_X()\n\n    def param_encoding_pairs(self):\n        return self.kernel.param_encoding_pairs() + \\\n               self.warping.param_encoding_pairs()\n\n    def get_params(self):\n        # We use the union of get_params for kernel and warping, without\n        # prefixes.\n        result = self.kernel.get_params()\n        result.update(self.warping.get_params())\n        return result\n\n    def set_params(self, param_dict):\n        self.kernel.set_params(param_dict)\n        self.warping.set_params(param_dict)\n'"
autogluon/searcher/bayesopt/models/__init__.py,0,b''
autogluon/searcher/bayesopt/models/gpmxnet.py,0,"b'from typing import Tuple, NamedTuple, Dict, Union, List, Optional\nimport mxnet as mx\nimport numpy as np\nimport logging\n\nfrom autogluon.searcher.bayesopt.gpmxnet.constants import MCMCConfig, \\\n    OptimizationConfig\nfrom autogluon.searcher.bayesopt.gpmxnet.gp_regression import \\\n    GaussianProcessRegression\nfrom autogluon.searcher.bayesopt.gpmxnet.gpr_mcmc import GPRegressionMCMC\nfrom autogluon.searcher.bayesopt.gpmxnet.kernel import Matern52, KernelFunction\nfrom autogluon.searcher.bayesopt.gpmxnet.warping import WarpedKernel, Warping\nfrom autogluon.searcher.bayesopt.gpmxnet.posterior_state import \\\n    GaussProcPosteriorState\nfrom autogluon.searcher.bayesopt.datatypes.hp_ranges import \\\n    HyperparameterRanges, HyperparameterRangeCategorical, \\\n    HyperparameterRangeContinuous, HyperparameterRangeInteger\nfrom autogluon.searcher.bayesopt.datatypes.tuning_job_state import \\\n    TuningJobState\nfrom autogluon.searcher.bayesopt.datatypes.common import \\\n    FantasizedPendingEvaluation, Candidate, candidate_for_print\nfrom autogluon.searcher.bayesopt.autogluon.hp_ranges import \\\n    HyperparameterRanges_CS\nfrom autogluon.searcher.bayesopt.autogluon.gp_profiling import \\\n    GPMXNetSimpleProfiler\nfrom autogluon.searcher.bayesopt.models.mxnet_base import SurrogateModelMXNet\nfrom autogluon.searcher.bayesopt.autogluon.debug_log import DebugLogPrinter\n\nlogger = logging.getLogger(__name__)\n\n\nGPModel = Union[GaussianProcessRegression, GPRegressionMCMC]\n\n\nclass InternalCandidateEvaluations(NamedTuple):\n    X: np.ndarray\n    y: np.ndarray\n    mean: float\n    std: float\n\n\n# Note: If state.pending_evaluations is not empty, it must contain entries\n# of type FantasizedPendingEvaluation, which contain the fantasy samples. This\n# is the case only for internal states, the member GPMXNetModel.state has\n# PendingEvaluation entries without the fantasy samples.\ndef get_internal_candidate_evaluations(\n        state: TuningJobState, active_metric: str, normalize_targets: bool,\n        num_fantasize_samples: int) -> InternalCandidateEvaluations:\n    candidates_ndarray = []\n    evaluation_values = []\n    for candidate_evaluation in state.candidate_evaluations:\n        candidates_ndarray.append(\n            state.hp_ranges.to_ndarray(candidate_evaluation.candidate))\n        evaluation_values.append(candidate_evaluation.metrics[active_metric])\n    X = np.vstack(candidates_ndarray)\n    # Normalize\n    # Note: The fantasy values in state.pending_evaluations are sampled\n    # from the model fit to normalized targets, so they are already\n    # normalized\n    y = np.vstack(evaluation_values).reshape((-1, 1))\n    mean = 0.0\n    std = 1.0\n    if normalize_targets:\n        std = max(np.std(y).item(), 1e-15)\n        mean = np.mean(y).item()\n        y = (y - mean) / std\n    if state.pending_evaluations:\n        # In this case, y becomes a matrix, where the observed values are\n        # broadcasted\n        fanta_lst = []\n        cand_lst = []\n        for pending_eval in state.pending_evaluations:\n            assert isinstance(pending_eval, FantasizedPendingEvaluation), \\\n                ""state.pending_evaluations has to contain FantasizedPendingEvaluation""\n            fantasies = pending_eval.fantasies[active_metric]\n            assert fantasies.size == num_fantasize_samples, \\\n                ""All state.pending_evaluations entries must have length {}"".format(\n                    num_fantasize_samples)\n            fanta_lst.append(fantasies.reshape((1, -1)))\n            cand_lst.append(state.hp_ranges.to_ndarray(pending_eval.candidate))\n        y = np.vstack([y * np.ones((1, num_fantasize_samples))] + fanta_lst)\n        X = np.vstack([X] + cand_lst)\n    return InternalCandidateEvaluations(X, y, mean, std)\n\n\ndef build_kernel(state: TuningJobState,\n                 do_warping: bool = False) -> KernelFunction:\n    dims, warping_ranges = dimensionality_and_warping_ranges(state.hp_ranges)\n    kernel = Matern52(dims, ARD=True)\n    if do_warping:\n        return WarpedKernel(\n            kernel=kernel, warping=Warping(dims, warping_ranges))\n    else:\n        return kernel\n\n\ndef default_gpmodel(\n        state: TuningJobState, random_seed: int,\n        optimization_config: OptimizationConfig) -> GaussianProcessRegression:\n    return GaussianProcessRegression(\n        kernel=build_kernel(state),\n        optimization_config=optimization_config,\n        random_seed=random_seed\n    )\n\n\ndef default_gpmodel_mcmc(\n        state: TuningJobState, random_seed: int,\n        mcmc_config: MCMCConfig) -> GPRegressionMCMC:\n    return GPRegressionMCMC(\n        build_kernel=lambda: build_kernel(state),\n        mcmc_config=mcmc_config,\n        random_seed=random_seed\n    )\n\n\ndef dimensionality_and_warping_ranges(hp_ranges: HyperparameterRanges) -> \\\n        Tuple[int, Dict[int, Tuple[float, float]]]:\n    dims = 0\n    warping_ranges = dict()\n    # NOTE: This explicit loop over hp_ranges will fail if\n    # HyperparameterRanges.hp_ranges is not implemented! Needs to be fixed if\n    # it becomes an issue, either by moving the functionality here into\n    # HyperparameterRanges, or by converting hp_ranges to\n    # HyperparameterRanges_Impl, which supports the hp_ranges property.\n    for hp_range in hp_ranges.hp_ranges:\n        if not isinstance(hp_range, HyperparameterRangeCategorical):\n            if isinstance(hp_range, HyperparameterRangeInteger):\n                lower = int(round(hp_range.lower_bound))\n                upper = int(round(hp_range.upper_bound))\n            else:\n                assert isinstance(hp_range, HyperparameterRangeContinuous)\n                lower = float(hp_range.lower_bound)\n                upper = float(hp_range.upper_bound)\n            lower_internal = hp_range.to_ndarray(lower).item()\n            upper_internal = hp_range.to_ndarray(upper).item()\n            if upper_internal > lower_internal:  # exclude cases where max equal to min\n                warping_ranges[dims] = (lower_internal, upper_internal)\n            else:\n                assert upper_internal == lower_internal\n        dims += hp_range.ndarray_size()\n    return dims, warping_ranges\n\n\nclass GPMXNetModel(SurrogateModelMXNet):\n    def __init__(\n            self, state: TuningJobState, active_metric: str, random_seed: int,\n            gpmodel: GPModel, fit_parameters: bool, num_fantasy_samples: int,\n            ctx_nd: mx.Context = None, dtype_nd = None,\n            normalize_targets: bool = True,\n            profiler: GPMXNetSimpleProfiler = None,\n            debug_log: Optional[DebugLogPrinter] = None):\n        """"""\n        Given a TuningJobState state, the corresponding posterior state is\n        computed here, based on which predictions are supported.\n        Note: state is immutable. It must contain labeled examples.\n\n        Parameters of the GP model in gpmodel are optimized iff fit_parameters\n        is true. This requires state to contain labeled examples.\n\n        We support pending evaluations via fantasizing. Note that state does\n        not contain the fantasy values, but just the pending configs. Fantasy\n        values are sampled here.\n\n        :param state: TuningJobSubState\n        :param active_metric: name of the metric to optimize.\n        :param random_seed: Used only if GP model is created here\n        :param gpmodel: GaussianProcessRegression model or GPRegressionMCMC model\n        :param fit_parameters: Optimize parameters of gpmodel? Otherwise, these\n            parameters are not changed\n        :param num_fantasy_samples: See above\n        :param normalize_targets: Normalize target values in\n            state.candidate_evaluations?\n\n        """"""\n        super(GPMXNetModel, self).__init__(\n            state, active_metric, random_seed, ctx_nd, dtype_nd, debug_log)\n        assert num_fantasy_samples > 0\n        self._gpmodel = gpmodel\n        self.num_fantasy_samples = num_fantasy_samples\n        self.normalize_targets = normalize_targets\n        self.active_metric = active_metric\n        # Compute posterior (including fitting (optional) and dealing with\n        # pending evaluations)\n        # If state.pending_evaluations is not empty, fantasy samples are drawn\n        # here, but they are not maintained in the state (which is immutable),\n        # and not in the posterior state either.\n        # Instead, fantasy samples can be accessed via self.fantasy_samples.\n        self.fantasy_samples = None\n        self._compute_posterior(fit_parameters, profiler)\n\n    def predict_nd(self, x_nd: mx.nd.NDArray) -> List[Tuple[mx.nd.NDArray, mx.nd.NDArray]]:\n        """"""\n        Note: Different to GPyOpt, means and stddevs are de-normalized here.\n        """"""\n        predictions_list_denormalized = []\n        for posterior_mean, posterior_variance in self._gpmodel.predict(x_nd):\n            assert posterior_mean.shape[0] == x_nd.shape[0], \\\n                (posterior_mean.shape, x_nd.shape)\n            assert posterior_variance.shape == (x_nd.shape[0],), \\\n                (posterior_variance.shape, x_nd.shape)\n            if self.state.pending_evaluations:\n                # If there are pending candidates with fantasy values,\n                # posterior_mean must be a matrix\n                assert posterior_mean.ndim == 2 and \\\n                    posterior_mean.shape[1] == self.num_fantasy_samples, \\\n                    (posterior_mean.shape, self.num_fantasy_samples)\n            predictions_list_denormalized.append(\n                (posterior_mean * self.std + self.mean,\n                mx.nd.sqrt(posterior_variance) * self.std))\n        return predictions_list_denormalized\n\n    @property\n    def gpmodel(self) -> GPModel:\n        return self._gpmodel\n\n    def does_mcmc(self):\n        return isinstance(self._gpmodel, GPRegressionMCMC)\n\n    def posterior_states(self) -> Optional[List[GaussProcPosteriorState]]:\n        return self._gpmodel.states\n\n    def get_params(self):\n        """"""\n        Note: Once MCMC is supported, this method will have to be refactored.\n        Note: If self.state still has no labeled data, the parameters returned\n        are the initial ones, where an update would start from.\n\n        :return: Hyperparameter dictionary\n        """"""\n        if not self.does_mcmc():\n            return self._gpmodel.get_params()\n        else:\n            return dict()\n\n    def set_params(self, param_dict):\n        self._gpmodel.set_params(param_dict)\n\n    def _compute_posterior(\n            self, fit_parameters: bool, profiler: GPMXNetSimpleProfiler):\n        """"""\n        Completes __init__, by computing the posterior. If fit_parameters, this\n        includes optimizing the surrogate model parameters.\n\n        If self.state.pending_evaluations is not empty, we proceed as follows:\n        - Compute posterior for state without pending evals\n        - Draw fantasy values for pending evals\n        - Recompute posterior (without fitting)\n\n        """"""\n        if self._debug_log is not None:\n            self._debug_log.set_state(self.state)\n        # Compute posterior for state without pending evals\n        no_pending_state = self.state\n        if self.state.pending_evaluations:\n            no_pending_state = TuningJobState(\n                hp_ranges=self.state.hp_ranges,\n                candidate_evaluations=self.state.candidate_evaluations,\n                failed_candidates=self.state.failed_candidates,\n                pending_evaluations=[])\n        self._posterior_for_state(no_pending_state, fit_parameters, profiler)\n        if self.state.pending_evaluations:\n            # Sample fantasy values for pending evals\n            pending_configs = [\n                x.candidate for x in self.state.pending_evaluations]\n            new_pending = self._draw_fantasy_values(pending_configs)\n            # Compute posterior for state with pending evals\n            # Note: profiler is not passed here, this would overwrite the\n            # results from the first call\n            with_pending_state = TuningJobState(\n                hp_ranges=self.state.hp_ranges,\n                candidate_evaluations=self.state.candidate_evaluations,\n                failed_candidates=self.state.failed_candidates,\n                pending_evaluations=new_pending)\n            self._posterior_for_state(\n                with_pending_state, fit_parameters=False, profiler=None)\n            # Note: At this point, the fantasy values are dropped, they are not\n            # needed anymore. They\'ve just been sampled for the posterior\n            # computation. We still maintain them in self.fantasy_samples,\n            # which is mainly used for testing\n            self.fantasy_samples = new_pending\n\n    def _posterior_for_state(\n            self, state: TuningJobState, fit_parameters: bool,\n            profiler: Optional[GPMXNetSimpleProfiler]):\n        """"""\n        Computes posterior for state.\n        If fit_parameters and state.pending_evaluations is empty, we first\n        optimize the model parameters.\n        If state.pending_evaluations are given, these must be\n        FantasizedPendingEvaluations, i.e. the fantasy values must have been\n        sampled.\n\n        """"""\n        assert state.candidate_evaluations, \\\n            ""Cannot compute posterior: state has no labeled datapoints""\n        internal_candidate_evaluations = get_internal_candidate_evaluations(\n            state, self.active_metric, self.normalize_targets,\n            self.num_fantasy_samples)\n        X_all = self.convert_np_to_nd(internal_candidate_evaluations.X)\n        Y_all = self.convert_np_to_nd(internal_candidate_evaluations.y)\n        assert X_all.shape[0] == Y_all.shape[0]\n        self.mean = internal_candidate_evaluations.mean\n        self.std = internal_candidate_evaluations.std\n\n        fit_parameters = fit_parameters and (not state.pending_evaluations)\n        if not fit_parameters:\n            logger.info(""Recomputing GP state"")\n            self._gpmodel.recompute_states(X_all, Y_all, profiler=profiler)\n        else:\n            logger.info(""Fitting GP model"")\n            self._gpmodel.fit(X_all, Y_all, profiler=profiler)\n        if self._debug_log is not None:\n            self._debug_log.set_gp_params(self.get_params())\n            if not state.pending_evaluations:\n                deb_msg = ""[GPMXNetModel._posterior_for_state]\\n""\n                deb_msg += (""- self.mean = {}\\n"".format(self.mean))\n                deb_msg += (""- self.std = {}"".format(self.std))\n                logger.info(deb_msg)\n                self._debug_log.set_targets(internal_candidate_evaluations.y)\n            else:\n                num_pending = len(state.pending_evaluations)\n                fantasies = internal_candidate_evaluations.y[-num_pending:, :]\n                self._debug_log.set_fantasies(fantasies)\n\n    def _draw_fantasy_values(self, candidates: List[Candidate]) \\\n            -> List[FantasizedPendingEvaluation]:\n        """"""\n        Note: The fantasy values need not be de-normalized, because they are\n        only used internally here (e.g., get_internal_candidate_evaluations).\n\n        Note: A complication is that if the sampling methods of _gpmodel\n        are called when there are no pending candidates (with fantasies) yet,\n        they do return a single sample (instead of num_fantasy_samples). This\n        is because GaussianProcessRegression knows about num_fantasy_samples\n        only due to the form of the posterior state (bad design!).\n        In this case, we draw num_fantasy_samples i.i.d.\n\n        """"""\n        if candidates:\n            logger.debug(""Fantasizing target values for candidates:\\n{}""\n                         .format(candidates))\n            X_new = self.state.hp_ranges.to_ndarray_matrix(candidates)\n            X_new_mx = self.convert_np_to_nd(X_new)\n            # Special case (see header comment): If the current posterior state\n            # does not contain pending candidates (no fantasies), we sample\n            # num_fantasy_samples times i.i.d.\n            num_samples = 1 if self._gpmodel.multiple_targets() \\\n                else self.num_fantasy_samples\n            # We need joint sampling for >1 new candidates\n            num_candidates = len(candidates)\n            sample_func = self._gpmodel.sample_joint if num_candidates > 1 else \\\n                self._gpmodel.sample_marginals\n            Y_new_mx = sample_func(X_new_mx, num_samples=num_samples)\n            Y_new = Y_new_mx.asnumpy().astype(X_new.dtype, copy=False).reshape(\n                (num_candidates, -1))\n            return [\n                FantasizedPendingEvaluation(\n                    candidate, {self.active_metric: y_new.reshape((1, -1))})\n                for candidate, y_new in zip(candidates, Y_new)\n            ]\n        else:\n            return []\n\n    def _current_best_filter_candidates(self, candidates):\n        hp_ranges = self.state.hp_ranges\n        if isinstance(hp_ranges, HyperparameterRanges_CS):\n            candidates = hp_ranges.filter_for_last_pos_value(candidates)\n            assert candidates, \\\n                ""state.hp_ranges does not contain any candidates "" + \\\n                ""(labeled or pending) with resource attribute "" + \\\n                ""\'{}\' = {}"".format(\n                    hp_ranges.name_last_pos, hp_ranges.value_for_last_pos)\n        return candidates\n'"
autogluon/searcher/bayesopt/models/gpmxnet_skipopt.py,0,"b'from abc import ABC, abstractmethod\nimport ConfigSpace as CS\n\nfrom autogluon.searcher.bayesopt.datatypes.tuning_job_state import \\\n    TuningJobState\nfrom autogluon.searcher.bayesopt.datatypes.common import Candidate\n\n\nclass SkipOptimizationPredicate(ABC):\n    """"""\n    Interface for skip_optimization predicate in GPMXNetModel\n\n    """"""\n    def reset(self):\n        """"""\n        If there is an internal state, reset it to its initial value\n        """"""\n        pass\n\n    @abstractmethod\n    def __call__(self, state: TuningJobState) -> bool:\n        """"""\n        :param state: Current TuningJobState\n        :return: Skip hyperparameter optimization in GPMXNetModel.update?\n        """"""\n        pass\n\n\nclass NeverSkipPredicate(SkipOptimizationPredicate):\n    """"""\n    Hyperparameter optimization is never skipped.\n\n    """"""\n    def __call__(self, state: TuningJobState) -> bool:\n        return False\n\n\nclass AlwaysSkipPredicate(SkipOptimizationPredicate):\n    """"""\n    Hyperparameter optimization is always skipped.\n\n    """"""\n    def __call__(self, state: TuningJobState) -> bool:\n        return True\n\n\nclass SkipPeriodicallyPredicate(SkipOptimizationPredicate):\n    """"""\n    Let N = len(state.candidate_evaluations) be the number of labeled\n    points. Optimizations are not skipped if N < init_length, and\n    afterwards optimizations are done every period times.\n\n    NOTE: This predicate cannot be a state-less function, because\n    __call__ may not be called for every value of N in sequence!\n\n    """"""\n    def __init__(self, init_length: int, period: int):\n        assert init_length >= 0\n        assert period > 1\n        self.init_length = init_length\n        self.period = period\n        self.reset()\n\n    def reset(self):\n        self.current_bound = self.init_length\n        self.lastrec_key = None\n\n    def __call__(self, state: TuningJobState) -> bool:\n        num_labeled = len(state.candidate_evaluations)\n        assert self.lastrec_key is None or \\\n            num_labeled >= self.lastrec_key, \\\n            ""num_labeled = {} < {} = lastrec_key"".format(\n                num_labeled, self.lastrec_key)\n        # self.lastrec_XYZ is needed in order to allow __call__\n        # to be called several times with the same num_labeled\n        if num_labeled == self.lastrec_key:\n            return self.lastrec_value\n        if num_labeled < self.current_bound:\n            ret_value = True\n        else:\n            # At this point, we passed current_bound, so should do the\n            # optimization\n            # Smallest init_length + k*period > num_labeled:\n            self.current_bound = num_labeled + self.period -\\\n                ((num_labeled - self.init_length) % self.period)\n            ret_value = False\n        self.lastrec_key = num_labeled\n        self.lastrec_value = ret_value\n        return ret_value\n\n\nclass SkipNoMaxResourcePredicate(SkipOptimizationPredicate):\n    """"""\n    This predicate works for multi-fidelity HPO, see for example\n    GPMultiFidelitySearcher.\n\n    We track the number of labeled datapoints at resource level max_resource.\n    HP optimization is skipped if the total number of labeled cases is >=\n    init_length, and if the number of max_resource cases has not increased\n    since the last recent optimization.\n\n    This means that as long as the dataset only grows w.r.t. cases at lower\n    resources than max_resource, this does not trigger HP optimization.\n\n    """"""\n    def __init__(self, init_length: int, resource_attr_name: str,\n                 max_resource: int):\n        assert init_length >= 0\n        self.init_length = init_length\n        self.resource_attr_name = resource_attr_name\n        self.max_resource = max_resource\n        self.reset()\n\n    def reset(self):\n        self.lastrec_max_resource_cases = None\n\n    def _num_max_resource_cases(self, state: TuningJobState):\n        def is_max_resource(config: Candidate) -> int:\n            if isinstance(config, CS.Configuration) and \\\n                    (config.get_dictionary()[self.resource_attr_name] ==\n                     self.max_resource):\n                return 1\n            else:\n                return 0\n\n        return sum(is_max_resource(x.candidate)\n                   for x in state.candidate_evaluations)\n\n    def __call__(self, state: TuningJobState) -> bool:\n        if len(state.candidate_evaluations) < self.init_length:\n            return False\n        num_max_resource_cases = self._num_max_resource_cases(state)\n        if self.lastrec_max_resource_cases is None or \\\n                num_max_resource_cases > self.lastrec_max_resource_cases:\n            self.lastrec_max_resource_cases = num_max_resource_cases\n            return False\n        else:\n            return True\n'"
autogluon/searcher/bayesopt/models/gpmxnet_transformers.py,0,"b'from typing import List, Callable, NamedTuple, Optional\nimport logging\nimport copy\n\nfrom autogluon.searcher.bayesopt.datatypes.tuning_job_state import \\\n    TuningJobState\nfrom autogluon.searcher.bayesopt.datatypes.common import \\\n    Candidate, PendingEvaluation, CandidateEvaluation\nfrom autogluon.searcher.bayesopt.tuning_algorithms.base_classes import \\\n    PendingCandidateStateTransformer\nfrom autogluon.searcher.bayesopt.tuning_algorithms.default_algorithm import \\\n    DEFAULT_METRIC\nfrom autogluon.searcher.bayesopt.models.gpmxnet import GPMXNetModel, GPModel\nfrom autogluon.searcher.bayesopt.models.gpmxnet_skipopt import \\\n    SkipOptimizationPredicate, NeverSkipPredicate\nfrom autogluon.searcher.bayesopt.autogluon.gp_profiling import \\\n    GPMXNetSimpleProfiler\nfrom autogluon.searcher.bayesopt.autogluon.debug_log import DebugLogPrinter\n\nlogger = logging.getLogger(__name__)\n\n\nclass GPMXNetModelArgs(NamedTuple):\n    num_fantasy_samples: int\n    random_seed: int\n    active_metric: str = DEFAULT_METRIC\n    normalize_targets: bool = True\n\n\nclass GPMXNetPendingCandidateStateTransformer(PendingCandidateStateTransformer):\n    """"""\n    This class maintains the TuningJobState along an asynchronous GP-based\n    HPO experiment, and manages the reaction to changes of this state.\n    In particular, it provides a GPMXNetModel on demand, which encapsulates\n    the GP posterior.\n\n    Note: The GPMXNetModel can be accessed only once the state has at least\n    one labeled case, since otherwise no posterior can be computed.\n\n    skip_optimization is a predicate depending on TuningJobState, determining\n    what is done at the next recent GPMXNetModel computation. If False, the\n    GP hyperparameters are optimized. Otherwise, the current ones are not\n    changed.\n\n    Safeguard against multiple GP hyperparameter optimization while labeled\n    data does not change:\n    The posterior has to be recomputed every time the state changes, even if\n    this only concerns pending evaluations. The expensive part of this is\n    refitting the GP hyperparameters, which makes sense only when the labeled\n    data in the state changes. We put a safeguard in place to avoid refitting\n    when the labeled data is unchanged.\n\n    """"""\n    def __init__(\n            self, gpmodel: GPModel, init_state: TuningJobState,\n            model_args: GPMXNetModelArgs,\n            skip_optimization: SkipOptimizationPredicate = None,\n            profiler: GPMXNetSimpleProfiler = None,\n            debug_log: Optional[DebugLogPrinter] = None):\n        self._gpmodel = gpmodel\n        self._state = copy.copy(init_state)\n        self._model_args = model_args\n        if skip_optimization is None:\n            self.skip_optimization = NeverSkipPredicate()\n        else:\n            self.skip_optimization = skip_optimization\n        self._profiler = profiler\n        self._debug_log = debug_log\n        # GPMXNetModel computed on demand\n        self._model: GPMXNetModel = None\n        self._candidate_evaluations = None\n        # _model_params is returned by get_params. Careful: This is not just\n        # self._gpmodel.get_params(), since the current GPMXNetModel may append\n        # additional parameters\n        self._model_params = gpmodel.get_params()\n\n    @property\n    def state(self) -> TuningJobState:\n        return self._state\n\n    def model(self, **kwargs) -> GPMXNetModel:\n        """"""\n        If skip_optimization is given, it overrides the self.skip_optimization\n        predicate.\n\n        :return: GPMXNetModel for current state\n\n        """"""\n        if self._model is None:\n            skip_optimization = kwargs.get(\'skip_optimization\')\n            self._compute_model(skip_optimization=skip_optimization)\n        return self._model\n\n    def get_params(self):\n        return self._model_params\n\n    def set_params(self, param_dict):\n        self._gpmodel.set_params(param_dict)\n        self._model_params = self._gpmodel.get_params()\n\n    def append_candidate(self, candidate: Candidate):\n        """"""\n        Appends new pending candidate to the state.\n\n        :param candidate: New pending candidate\n\n        """"""\n        self._model = None  # Invalidate\n        self._state.pending_evaluations.append(PendingEvaluation(candidate))\n\n    @staticmethod\n    def _find_candidate(candidate: Candidate, lst: List):\n        try:\n            pos = next(\n                i for i, x in enumerate(lst)\n                if x.candidate == candidate)\n        except StopIteration:\n            pos = -1\n        return pos\n\n    def drop_candidate(self, candidate: Candidate):\n        """"""\n        Drop candidate (labeled or pending) from state.\n\n        :param candidate: Candidate to be dropped\n\n        """"""\n        # Candidate may be labeled or pending. First, try labeled\n        pos = self._find_candidate(\n            candidate, self._state.candidate_evaluations)\n        if pos != -1:\n            self._model = None  # Invalidate\n            self._state.candidate_evaluations.pop(pos)\n            if self._debug_log is not None:\n                deb_msg = ""[GPMXNetAsyncPendingCandidateStateTransformer.drop_candidate]\\n""\n                deb_msg += (""- len(candidate_evaluations) afterwards = {}"".format(\n                    len(self.state.candidate_evaluations)))\n                logger.info(deb_msg)\n        else:\n            # Try pending\n            pos = self._find_candidate(\n                candidate, self._state.pending_evaluations)\n            assert pos != -1, \\\n                ""Candidate {} not registered (neither labeled, nor pending)"".format(\n                    candidate)\n            self._model = None  # Invalidate\n            self._state.pending_evaluations.pop(pos)\n            if self._debug_log is not None:\n                deb_msg = ""[GPMXNetAsyncPendingCandidateStateTransformer.drop_candidate]\\n""\n                deb_msg += (""- len(pending_evaluations) afterwards = {}\\n"".format(\n                    len(self.state.pending_evaluations)))\n                logger.info(deb_msg)\n\n    def label_candidate(self, data: CandidateEvaluation):\n        """"""\n        Adds a labeled candidate. If it was pending before, it is removed as\n        pending candidate.\n\n        :param data: New labeled candidate\n\n        """"""\n        pos = self._find_candidate(\n            data.candidate, self._state.pending_evaluations)\n        if pos != -1:\n            self._state.pending_evaluations.pop(pos)\n        self._state.candidate_evaluations.append(data)\n        self._model = None  # Invalidate\n\n    def filter_pending_evaluations(\n            self, filter_pred: Callable[[PendingEvaluation], bool]):\n        """"""\n        Filters state.pending_evaluations with filter_pred.\n\n        :param filter_pred Filtering predicate\n\n        """"""\n        new_pending_evaluations = list(filter(\n            filter_pred, self._state.pending_evaluations))\n        if len(new_pending_evaluations) != len(self._state.pending_evaluations):\n            if self._debug_log is not None:\n                deb_msg = ""[GPMXNetAsyncPendingCandidateStateTransformer.filter_pending_evaluations]\\n""\n                deb_msg += (""- from len {} to {}"".format(\n                    len(self.state.pending_evaluations), len(new_pending_evaluations)))\n                logger.info(deb_msg)\n            self._model = None  # Invalidate\n            del self._state.pending_evaluations[:]\n            self._state.pending_evaluations.extend(new_pending_evaluations)\n\n    def mark_candidate_failed(self, candidate: Candidate):\n        self._state.failed_candidates.append(candidate)\n\n    def _compute_model(self, skip_optimization: bool = None):\n        args = self._model_args\n        if skip_optimization is None:\n            skip_optimization = self.skip_optimization(self._state)\n        fit_parameters = not skip_optimization\n        if fit_parameters and self._candidate_evaluations:\n            # Did the labeled data really change since the last recent refit?\n            # If not, skip the refitting\n            if self._state.candidate_evaluations == self._candidate_evaluations:\n                fit_parameters = False\n                logger.warning(\n                    ""Skipping the refitting of GP hyperparameters, since the ""\n                    ""labeled data did not change since the last recent fit"")\n        self._model = GPMXNetModel(\n            state=self._state,\n            active_metric=args.active_metric,\n            random_seed=args.random_seed,\n            gpmodel=self._gpmodel,\n            fit_parameters=fit_parameters,\n            num_fantasy_samples=args.num_fantasy_samples,\n            normalize_targets=args.normalize_targets,\n            profiler=self._profiler,\n            debug_log=self._debug_log)\n        # Note: This may be different than self._gpmodel.get_params(), since\n        # the GPMXNetModel may append additional info\n        self._model_params = self._model.get_params()\n        if fit_parameters:\n            # Keep copy of labeled data in order to avoid unnecessary\n            # refitting\n            self._candidate_evaluations = copy.copy(\n                self._state.candidate_evaluations)\n'"
autogluon/searcher/bayesopt/models/mxhead_acqfunc.py,0,"b'import numpy as np\nimport mxnet as mx\nfrom mxnet import autograd\nfrom mxnet.ndarray import NDArray\nfrom abc import ABC, abstractmethod\nfrom typing import Tuple, Optional\n\nfrom autogluon.searcher.bayesopt.tuning_algorithms.base_classes import \\\n    SurrogateModel, AcquisitionFunction\nfrom autogluon.searcher.bayesopt.models.nphead_acqfunc import \\\n    _reshape_predictions\n\n\nclass MXNetHeadAcquisitionFunction(AcquisitionFunction, ABC):\n    """"""\n    Base class for acquisition functions whose head are implemented in MXNet.\n    Here:\n\n        f(x, model) = h(mean, std, model.current_best())\n\n    where h(.) is the head, and mean, std are predictive mean and stddev.\n\n    """"""\n    def __init__(self, model: SurrogateModel):\n        super(MXNetHeadAcquisitionFunction, self).__init__(model)\n\n    def compute_acq(self, x: np.ndarray,\n                    model: Optional[SurrogateModel] = None) -> np.ndarray:\n        if model is None:\n            model = self.model\n        predictions_list = model.predict_nd(model.convert_np_to_nd(x))\n        fvals_list = []\n        for mean, std in predictions_list:\n            if self._head_needs_current_best():\n                current_best = model.convert_np_to_nd(\n                    model.current_best()).reshape((1, -1))\n            else:\n                current_best = None\n            # Are we in batch mode? If so, mean is a matrix, whose number of columns\n            # is identical to the number of samples used for fantasizing. In this\n            # case, we compute the criterion values separate for every sample, and\n            # average them\n            do_avg = (mean.ndim == 2 and mean.shape[1] > 1)\n            if do_avg:\n                assert mean.shape[1] == current_best.size, \\\n                    ""mean.shape[1] = {}, current_best.size = {} (must be the same)"".format(\n                        mean.shape[1], current_best.size)\n                std = std.reshape((-1, 1))\n            fvals = self._compute_head(mean, std, current_best)\n            if do_avg:\n                fvals = mx.nd.mean(fvals, axis=1)\n            fvals_list.append(fvals.asnumpy().astype(x.dtype, copy=False))\n        return np.mean(fvals_list, axis=0)\n\n    def compute_acq_with_gradients(\n            self, x: np.ndarray,\n            model: Optional[SurrogateModel] = None) -> \\\n            Tuple[np.ndarray, np.ndarray]:\n        if model is None:\n            model = self.model\n        dtype_np = x.dtype\n        if x.ndim == 1:\n            x = x[None, :]\n        num_data = x.shape[0]\n\n        # Loop over cases (rows of x), we need the gradients for each case\n        # separately\n        f_acqu = np.empty((num_data, 1), dtype=dtype_np)\n        df_acqu = np.empty_like(x)\n        # The current best\n        if self._head_needs_current_best():\n            current_best_nd = model.convert_np_to_nd(\n                model.current_best()).reshape((-1,))\n        else:\n            current_best_nd = None\n\n        for row in range(num_data):\n            x_nd = model.convert_np_to_nd(x[row, None])\n            # Record for gradient computation\n            x_nd.attach_grad()\n            with autograd.record():\n                m_nd, s_nd = _reshape_predictions(model.predict_nd(x_nd))\n                fval = mx.nd.mean(self._compute_head(\n                    m_nd, s_nd, current_best_nd))\n            f_acqu[row] = fval.asscalar()\n            fval.backward()\n            df_acqu[row] = x_nd.grad.asnumpy().astype(\n                dtype_np, copy=False)\n        return f_acqu, df_acqu\n\n    @abstractmethod\n    def _head_needs_current_best(self) -> bool:\n        """"""\n        :return: Is the current_best argument in _compute_head needed?\n        """"""\n        pass\n\n    @abstractmethod\n    def _compute_head(self, mean: NDArray, std: NDArray,\n                      current_best: NDArray) -> NDArray:\n        """"""\n        All input and return arguments are of type mx.nd.NDArray.\n\n        :param mean: Predictive means\n        :param std: Predictive stddevs\n        :param current_best: Encumbent\n        :return: hvals\n        """"""\n        pass\n\n\nclass LCBAcquisitionFunction(MXNetHeadAcquisitionFunction):\n    """"""\n    Lower confidence bound (LCB) acquisition function:\n\n        h(mean, std) = mean - kappa * std\n\n    """"""\n    def __init__(self, model: SurrogateModel, kappa: float):\n        super(LCBAcquisitionFunction, self).__init__(model)\n        assert kappa > 0, \'kappa must be positive\'\n        self.kappa = kappa\n\n    def _head_needs_current_best(self) -> bool:\n        return False\n\n    def _compute_head(self, mean: NDArray, std: NDArray,\n                      current_best: NDArray) -> NDArray:\n        return mean - self.kappa * std\n'"
autogluon/searcher/bayesopt/models/mxnet_base.py,0,"b'from typing import Tuple, List, Optional\nimport mxnet as mx\nfrom mxnet.ndarray import NDArray\nimport numpy as np\nimport logging\n\nfrom autogluon.searcher.bayesopt.datatypes.tuning_job_state import \\\n    TuningJobState\nfrom autogluon.searcher.bayesopt.tuning_algorithms.base_classes import \\\n    SurrogateModel\nfrom autogluon.searcher.bayesopt.gpmxnet.constants import DATA_TYPE\nfrom autogluon.searcher.bayesopt.autogluon.debug_log import DebugLogPrinter\n\nlogger = logging.getLogger(__name__)\n\n\nclass SurrogateModelMXNet(SurrogateModel):\n    """"""\n    MXNet based model\n\n    """"""\n    def __init__(\n            self, state: TuningJobState, active_metric: str, random_seed: int,\n            ctx_nd=None, dtype_nd=None,\n            debug_log: Optional[DebugLogPrinter] = None):\n        if ctx_nd is None:\n            ctx_nd = mx.cpu()\n        if dtype_nd is None:\n            dtype_nd = DATA_TYPE\n        super(SurrogateModelMXNet, self).__init__(\n            state, active_metric, random_seed)\n        assert dtype_nd == np.float32 or dtype_nd == np.float64\n        self.ctx_nd = ctx_nd\n        self.dtype_nd = dtype_nd\n        self._current_best = None\n        self._debug_log = debug_log\n\n    ######################################\n    # shared functions for all sub-classes\n\n    def predict(self, X: np.ndarray) -> List[Tuple[np.ndarray, np.ndarray]]:\n        """"""return prediction of X using predict_nd""""""\n        dtype_np, _ = self._get_dtypes(X)\n        if X.ndim == 1:\n            X = X[None, :]\n\n        X_nd = self.convert_np_to_nd(X)\n        predictions_nd = self.predict_nd(X_nd)\n\n        predictions_list = [\n            (m_nd.asnumpy().astype(dtype_np, copy=False),\n             s_nd.asnumpy().astype(dtype_np, copy=False))\n            for m_nd, s_nd in predictions_nd\n        ]\n        return predictions_list\n\n    def context_for_nd(self) -> mx.Context:\n        return self.ctx_nd\n\n    def dtype_for_nd(self):\n        return self.dtype_nd\n\n    def _get_dtypes(self, x: np.ndarray):\n        dtype_np = x.dtype\n        if dtype_np == np.int32 or dtype_np == np.int64:\n            dtype_np = np.float64\n        assert dtype_np == np.float32 or dtype_np == np.float64\n        dtype_nd = self.dtype_nd\n        return dtype_np, dtype_nd\n\n    def _current_best_filter_candidates(self, candidates):\n        """"""\n        Can be used by subclasses to specialize current_best.\n\n        """"""\n        return candidates\n\n    def current_best(self) -> np.ndarray:\n        if self._current_best is None:\n            def convert(c):\n                x = self.state.hp_ranges.to_ndarray(c)\n                return self.convert_np_to_nd(x).reshape((1, -1))\n\n            candidates = [\n                x.candidate for x in self.state.candidate_evaluations] + \\\n                         self.state.pending_candidates\n            if self._debug_log is not None:\n                deb_msg = ""[GPMXNetModel.current_best -- RECOMPUTING]\\n""\n                deb_msg += (""- len(candidates) = {}"".format(len(candidates)))\n                logger.info(deb_msg)\n\n            if len(candidates) == 0:\n                # this can happen if no evaluation is present for the current task\n                # in some sense the best so far is plus infinity, any new result would be an improvement\n                # In this special setting optimizing the EI is equivalent to optimizing the mean\n                # and any current_best high enough will approximately give the same results\n                assert len(self.state.candidate_evaluations) > 0  # other tasks will have evaluations\n                # take the worst and add a few times standard dev\n                values = [x.metrics[self.active_metric] for x in self.state.candidate_evaluations]\n                current_worst = max(values)\n                std = np.std(values)\n                self._current_best = np.array([current_worst + 10 * std])\n            else:\n                candidates = self._current_best_filter_candidates(candidates)\n                X_all = mx.nd.concat(*map(convert, candidates), dim=0)\n                pred_mean = _compute_mean_across_samples(self.predict_nd(X_all))\n                self._current_best = mx.nd.min(pred_mean, axis=0).asnumpy()\n\n            logger.info(f""Current best is {self._current_best}"")\n        return self._current_best\n\n\ndef _compute_mean_across_samples(predictions_list: List[Tuple[NDArray, NDArray]]) -> NDArray:\n    pred_means_list = [pred_mean for pred_mean, _ in predictions_list]\n    pred_means = mx.nd.stack(*pred_means_list, axis=0)\n    return mx.nd.mean(pred_means, axis=0)\n'"
autogluon/searcher/bayesopt/models/nphead_acqfunc.py,0,"b'import numpy as np\nimport mxnet as mx\nfrom mxnet import autograd\nfrom mxnet.ndarray import NDArray\nfrom abc import ABC, abstractmethod\nfrom typing import Tuple, List, NamedTuple, Optional\n\nfrom autogluon.searcher.bayesopt.tuning_algorithms.base_classes import \\\n    SurrogateModel, AcquisitionFunction\nfrom autogluon.searcher.bayesopt.utils.density import get_quantiles\n\n\nclass NumPyHeadResult(NamedTuple):\n    hvals: np.array\n    dh_dmean: np.array\n    dh_dstd: np.array\n\n\nclass NumPyHeadAcquisitionFunction(AcquisitionFunction, ABC):\n    """"""\n    Base class for acquisition functions whose head are implemented in NumPy\n    (and not in MXNet). Here:\n\n        f(x, model) = h(mean, std, model.current_best())\n\n    where h(.) is the head, mean, std predictive mean and stddev.\n    If h(.) can be implemented in MXNet, it is more efficient to do that, but\n    often MXNet lacks functionality.\n\n    NOTE that acquisition functions will always be *minimized*!\n\n    """"""\n    def __init__(self, model: SurrogateModel):\n        super(NumPyHeadAcquisitionFunction, self).__init__(model)\n\n    def compute_acq(self, x: np.ndarray,\n                    model: Optional[SurrogateModel] = None) -> np.ndarray:\n        if model is None:\n            model = self.model\n        predictions_list = model.predict(x)\n        fvals_list = []\n        for mean, std in predictions_list:\n            if self._head_needs_current_best():\n                current_best = model.current_best().reshape((1, -1))\n            else:\n                current_best = None\n            # Are we in batch mode? If so, mean is a matrix, whose number of columns\n            # is identical to the number of samples used for fantasizing. In this\n            # case, we compute the criterion values separate for every sample, and\n            # average them\n            do_avg = (mean.ndim == 2 and mean.shape[1] > 1)\n            if do_avg:\n                assert mean.shape[1] == current_best.size, \\\n                    ""mean.shape[1] = {}, current_best.size = {} (must be the same)"".format(\n                        mean.shape[1], current_best.size)\n                std = std.reshape((-1, 1))\n            fvals = self._compute_head(mean, std, current_best).hvals\n            if do_avg:\n                fvals = np.mean(fvals, axis=1)\n            fvals_list.append(fvals)\n        return np.mean(fvals_list, axis=0)\n\n    def compute_acq_with_gradients(\n            self, x: np.ndarray,\n            model: Optional[SurrogateModel] = None) -> \\\n            Tuple[np.ndarray, np.ndarray]:\n        if model is None:\n            model = self.model\n        dtype_nd = model.dtype_for_nd()\n        dtype_np = x.dtype\n        ctx = model.context_for_nd()\n        if x.ndim == 1:\n            x = x[None, :]\n        num_data = x.shape[0]\n\n        # Loop over cases (rows of x), we need the gradients for each case\n        # separately\n        f_acqu = np.empty((num_data, 1), dtype=dtype_np)\n        df_acqu = np.empty_like(x)\n        # The current best\n        if self._head_needs_current_best():\n            current_best = model.current_best().reshape((-1,))\n        else:\n            current_best = None\n\n        dfdm_nd, dfds_nd, num_samples = None, None, None\n        for row in range(num_data):\n            x_nd = model.convert_np_to_nd(x[row, None])\n            # Compute heads m_nd, s_nd while recording\n            x_nd.attach_grad()\n            with autograd.record():\n                m_nd, s_nd = _reshape_predictions(model.predict_nd(x_nd))\n                if dtype_np != dtype_nd:\n                    m_nd = m_nd.astype(dtype_np)\n                    s_nd = s_nd.astype(dtype_np)\n\n            # Compute head gradients in NumPy\n            head_result = self._compute_head(\n                m_nd.asnumpy(), s_nd.asnumpy(), current_best)\n            f_acqu[row] = np.mean(head_result.hvals)\n            if row == 0:\n                num_samples = m_nd.size\n                dfdm_nd = mx.nd.array(head_result.dh_dmean, ctx=ctx, dtype=dtype_np)\n                dfds_nd = mx.nd.array(head_result.dh_dstd, ctx=ctx, dtype=dtype_np)\n            else:\n                dfdm_nd[:] = head_result.dh_dmean\n                dfds_nd[:] = head_result.dh_dstd\n\n            # Backward with specific head gradients\n            autograd.backward([m_nd, s_nd], [dfdm_nd, dfds_nd])\n            df_acqu[row] = x_nd.grad.asnumpy().astype(\n                dtype_np, copy=False) / num_samples\n        return f_acqu, df_acqu\n\n    @abstractmethod\n    def _head_needs_current_best(self) -> bool:\n        """"""\n        :return: Is the current_best argument in _compute_head needed?\n        """"""\n        pass\n\n    @abstractmethod\n    def _compute_head(\n            self, mean: np.ndarray, std: np.ndarray,\n            current_best: Optional[np.ndarray]) -> NumPyHeadResult:\n        """"""\n        Note that the head value will always be *minimized*, that is why we for example return\n        minus the expected improvement\n\n        :param mean: Predictive means\n        :param std: Predictive stddevs\n        :param current_best: Encumbent\n        :return: NumPyHeadResult containing hvals, dh_dmean, dh_dstd\n        """"""\n        pass\n\n\ndef _reshape_predictions(predictions: List[Tuple[NDArray, NDArray]]) \\\n        -> Tuple[NDArray, NDArray]:\n    """"""\n\n    :param predictions: prediction of one sample. It\'s list of tuples, which are predicted mean\n    and variance. The predicted means can have more than 1 column in the case of fantasizing\n\n    predicted mean can be mx.nd.array([1,]) or mx.nd.array([1,2,3]) (fantasizing)\n    predicted variance is always mx.nd.array([1,])\n\n    An example, 3 MCMC samples with 2 fantasized output\n    predictions = [(mx.nd.array([1,2]), mx.nd.array([0.1,])),\n                   (mx.nd.array([2,3]), mx.nd.array([0.2,])),\n                   (mx.nd.array([3,4]), mx.nd.array([0.3,])]\n\n    The method returns (mx.nd.array([1,2,2,3,3,4]), mx.nd.array([0.1,0.1,0.2,0.2,0.3,0.3]))\n\n    :return: a tuple of two mx.nd.NDArray, which are flattened mean and variances\n    """"""\n    m_nd_list, s_nd_list = zip(*predictions)\n    n_samples = len(predictions)\n    n_fantasizing = m_nd_list[0].size\n\n    m_nd = mx.nd.concat(*m_nd_list, dim=0)\n    s_nd = mx.nd.concat(*s_nd_list, dim=0)\n    m_nd = m_nd.reshape(n_samples, n_fantasizing)\n    s_nd = s_nd.reshape(n_samples, 1)\n\n    s_nd = mx.nd.broadcast_axis(s_nd, axis=1, size=n_fantasizing)\n    return m_nd.reshape((-1)), s_nd.reshape((-1))\n\n\nclass EIAcquisitionFunction(NumPyHeadAcquisitionFunction):\n    """"""\n    Minus expected improvement acquisition function\n    (minus because the convention is to always minimize acquisition functions)\n\n    """"""\n    def __init__(self, model: SurrogateModel, jitter: float = 0.01):\n        super().__init__(model)\n        self.jitter = jitter\n\n    def _head_needs_current_best(self) -> bool:\n        return True\n\n    def _compute_head(self, mean: np.array, std: np.array, current_best: Optional[np.array]):\n        """"""\n        Returns -1 times the expected improvement and gradients with respect to mean\n        and standard deviation\n        """"""\n        assert current_best is not None\n\n        # phi, Phi is PDF and CDF of Gaussian\n        phi, Phi, u = get_quantiles(self.jitter, current_best, mean, std)\n        f_acqu = std * (u * Phi + phi)\n        return NumPyHeadResult(\n            hvals=-f_acqu,\n            dh_dmean=Phi,\n            dh_dstd=-phi)\n'"
autogluon/searcher/bayesopt/tuning_algorithms/__init__.py,0,b''
autogluon/searcher/bayesopt/tuning_algorithms/base_classes.py,0,"b'from abc import ABC, abstractmethod\nfrom typing import List, Iterator, Iterable, Tuple, Type, Optional\nimport mxnet as mx\nimport numpy as np\nfrom mxnet.ndarray import NDArray\n\nfrom autogluon.searcher.bayesopt.datatypes.common import Candidate\nfrom autogluon.searcher.bayesopt.datatypes.tuning_job_state import \\\n    TuningJobState\n\n\nclass NextCandidatesAlgorithm:\n    def next_candidates(self) -> List[Candidate]:\n        # Not using ABC otherwise it will be difficult to create a subclass that also is a\n        # NamedTuple(as both define their own metaclass)\n        raise NotImplemented(""Abstract method"")\n\n\nclass CandidateGenerator(ABC):\n    """"""\n    Class to generate candidates from which to start the local minimization, typically random candidate\n    or some form of more uniformly spaced variation, such as latin hypercube or sobol sequence\n    """"""\n    @abstractmethod\n    def generate_candidates(self) -> Iterator[Candidate]:\n        pass\n\n    def generate_candidates_en_bulk(self, num_cands: int) -> List[Candidate]:\n        raise NotImplementedError()\n\n\nclass SurrogateModel(ABC):\n    def __init__(self, state: TuningJobState, active_metric: str,\n                 random_seed: int):\n        self.state = state\n        self.random_seed = random_seed\n        self.active_metric = active_metric\n\n    @abstractmethod\n    def predict_nd(self, x_nd: NDArray) -> List[Tuple[NDArray, NDArray]]:\n        """"""\n        Given a (n, d) matrix x_nd of test input points, return predictive means\n        and predictive stddevs, as (n,) vectors.\n\n        Note: Different to state passed at construction, the input points in\n        x_nd must be encoded already. See also \'predict_candidates\'.\n\n        If the model supports fantasizing (see FantasizingSurrogateModel), and\n        the state passed at construction contains pending evaluations with\n        fantasized target values, then pred_mean will be a matrix of shape\n        (n, num_fantasy_samples), one column per fantasy sample, while pred_std\n        remains a vector (the same for each sample).\n\n        When using GP with marginal likelihood estimation, the returned list\n        will be of length 1. When using GP with MCMC, the returned list will have\n        one entry per MCMC sample.\n\n        :param x_nd: Test input points\n        :return: A list of (pred_mean, pred_std)\n        """"""\n        pass\n\n    @abstractmethod\n    def predict(self, X: np.ndarray) -> List[Tuple[np.ndarray, np.ndarray]]:\n        """"""\n        Wrapper around predict_nd, where inputs and returns are np.ndarray.\n\n        Note: Different to state passed at construction, the input points in\n        X must be encoded already. See also \'predict_candidates\'.\n        """"""\n        pass\n\n    def predict_candidates(self, candidates: Iterable[Candidate]) -> \\\n            List[Tuple[np.ndarray, np.ndarray]]:\n        """"""\n        Convenience function to get a list of means and standard deviations\n        of candidates.\n        """"""\n        return self.predict(self.state.hp_ranges.to_ndarray_matrix(candidates))\n\n    @abstractmethod\n    def current_best(self) -> np.ndarray:\n        """"""\n        Returns the so-called incumbent, to be used in acquisition functions\n        such as expected improvement. This is the minimum of predictive means\n        at all current candidate locations (both state.candidate_evaluations\n        and state.pending_evaluations).\n        Normally, a scalar is returned, but if the model supports fantasizing\n        and the state contains pending evaluations, there is one incumbent\n        per fantasy sample, so a vector is returned.\n\n        NOTE: When using MCMC, we should really maintain one incumbent per MCMC\n        sample (for the same reason as we do that for fantasies). This is\n        currently not done.\n\n        :return: Incumbent\n        """"""\n        pass\n\n    @abstractmethod\n    def context_for_nd(self) -> mx.Context:\n        """"""\n        :return: Context for mx.nd input/output arguments\n        """"""\n        pass\n\n    @abstractmethod\n    def dtype_for_nd(self):\n        """"""\n        :return: Datatype for mx.nd input/output arguments\n        """"""\n        pass\n\n    def convert_np_to_nd(self, x: np.ndarray) -> NDArray:\n        return mx.nd.array(\n            x, ctx=self.context_for_nd(), dtype=self.dtype_for_nd())\n\n\nclass ScoringFunction(ABC):\n    """"""\n    Class to score candidates, typically combine an acquisition function with\n    potentially Thompson sampling\n\n    NOTE: it will be minimized, i.e. lower is better\n    """"""\n    @abstractmethod\n    def score(self, candidates: Iterable[Candidate],\n              model: Optional[SurrogateModel] = None) -> List[float]:\n        """"""\n        Requires multiple candidates, is this can be much quicker: we can use matrix operations\n\n        lower is better\n        """"""\n        pass\n\n\nclass AcquisitionFunction(ScoringFunction):\n    @abstractmethod\n    def __init__(self, model: SurrogateModel):\n        self.model = model\n\n    @abstractmethod\n    def compute_acq(self, x: np.ndarray,\n                    model: Optional[SurrogateModel] = None) -> np.ndarray:\n        pass\n\n    @abstractmethod\n    def compute_acq_with_gradients(\n            self, x: np.ndarray,\n            model: Optional[SurrogateModel] = None) -> \\\n            Tuple[np.ndarray, np.ndarray]:\n        pass\n\n    def score(self, candidates: Iterable[Candidate],\n              model: Optional[SurrogateModel] = None) -> List[float]:\n        if model is None:\n            model = self.model\n        x = model.state.hp_ranges.to_ndarray_matrix(candidates)\n        return list(self.compute_acq(x, model=model))\n\n\nclass LocalOptimizer(ABC):\n    """"""\n    Class that tries to find a local candidate with a better score, typically using a local\n    optimization method such as lbfgs. It would normally encapsulate an acquisition function and model\n\n    """"""\n    def __init__(self, state: TuningJobState, model: SurrogateModel,\n                 acquisition_function_class: Type[AcquisitionFunction]):\n        self.state = state\n        self.model = model\n        self.acquisition_function_class = acquisition_function_class\n\n    @abstractmethod\n    def optimize(self, candidate: Candidate,\n                 model: Optional[SurrogateModel] = None) -> Candidate:\n        """"""\n        Run local optimization, starting from candidate.\n        If model is given, it overrides self.model.\n\n        :param candidate: Starting point\n        :param model: See above\n        :return: Candidate found by local optimization\n        """"""\n        pass\n\n\nclass PendingCandidateStateTransformer(ABC):\n    """"""\n    This concept is needed if HPO can deal with pending candidates, which\n    remain unlabeled during further decision-making (e.g., batch decisions\n    or asynchronous HPO). In this case, information for the pending candidate\n    has to be added to the state.\n\n    """"""\n    @abstractmethod\n    def append_candidate(self, candidate: Candidate):\n        """"""\n        Determines PendingEvaluation information for candidate and append to\n        state.pending_evaluations.\n\n        :param candidate: Novel pending candidate\n        """"""\n        pass\n\n    @property\n    @abstractmethod\n    def state(self) -> TuningJobState:\n        """"""\n        :return: Current TuningJobState\n        """"""\n        pass\n\n    @abstractmethod\n    def model(self, **kwargs) -> SurrogateModel:\n        """"""\n        In general, the model is computed here on demand, based on the current\n        state, unless the state has not changed since the last \'model\' call.\n\n        :return: Surrogate model for current state\n        """"""\n        pass\n'"
autogluon/searcher/bayesopt/tuning_algorithms/bo_algorithm.py,0,"b'from typing import List, NamedTuple, Set, Tuple, Iterator, Optional\nimport logging\nimport numpy as np\nimport itertools\n\nfrom autogluon.searcher.bayesopt.datatypes.common import Candidate, \\\n    candidate_for_print\nfrom autogluon.searcher.bayesopt.autogluon.gp_profiling import \\\n    GPMXNetSimpleProfiler\nfrom autogluon.searcher.bayesopt.utils.duplicate_detector import \\\n    DuplicateDetector\nfrom autogluon.searcher.bayesopt.tuning_algorithms.base_classes import \\\n    NextCandidatesAlgorithm, CandidateGenerator, ScoringFunction, \\\n    LocalOptimizer, PendingCandidateStateTransformer, SurrogateModel\nfrom autogluon.searcher.bayesopt.tuning_algorithms.common import \\\n    generate_unique_candidates\nfrom autogluon.searcher.bayesopt.tuning_algorithms.bo_algorithm_components import \\\n    LBFGSOptimizeAcquisition    \nfrom autogluon.searcher.bayesopt.autogluon.debug_log import DebugLogPrinter\n\nlogger = logging.getLogger(__name__)\n\n\nclass BayesianOptimizationAlgorithm(NamedTuple, NextCandidatesAlgorithm):\n    """"""\n    Core logic of the Bayesian optimization algorithm\n    :param initial_candidates_generator: generator of candidates\n    :param initial_scoring_function: scoring function used to rank the initial\n        candidates.\n        Note: If a batch is selected in one go (num_requested_candidates > 1,\n        greedy_batch_selection = False), this function should encourage\n        diversity among its top scorers. In general, greedy batch selection\n        is recommended.\n    :param num_initial_candidates: how many initial candidates to generate, if\n        possible\n    :param local_optimizer: local optimizer which starts from score minimizer.\n        If a batch is selected in one go (not greedily), then local\n        optimizations are started from the top num_requested_candidates ranked\n        candidates (after scoring)\n    :param pending_candidate_state_transformer: Once a candidate is selected, it\n        becomes pending, and the state is transformed by appending information.\n        This is done by the transformer.\n        This is object is needed only if next_candidates goes through > 1 outer\n        iterations (i.e., if greedy_batch_selection is True and\n        num_requested_candidates > 1. Otherwise, None can be passed here.\n        Note: Model updates (by the state transformer) for batch candidates beyond\n        the first do not involve fitting hyperparameters, so they are usually\n        cheap.\n    :param blacklisted_candidates: set of tuples, candidates that should not be\n        returned, because they are already labeled, currently pending, or have\n        failed\n    :param num_requested_candidates: number of candidates to return\n    :param greedy_batch_selection: If True and num_requested_candidates > 1, we\n        generate, order, and locally optimize for each single candidate to be\n        selected. Otherwise (False), this is done just once, and\n        num_requested_candidates are extracted in one go.\n        Note: If this is True, pending_candidate_state_transformer is needed.\n    :param duplicate_detector: used to make sure no candidates equal to already\n        evaluated ones is returned\n    :param profiler: If given, this is used for profiling parts in the code\n    :param sample_unique_candidates: If True, we check that initial candidates\n        sampled at random are unique and disjoint from the blacklist. See below.\n    :param debug_log: If a DebugLogPrinter is passed here, it is used to write\n        log messages\n\n    Filtering out configs in blacklisted_candidates:\n    If Candidate = ConfigSpace.Configuration, it turns out to be very expensive\n    to ensure uniqueness of initial candidates, which is why\n    sample_unique_candidates = False by default. In this case, make sure to\n    use DuplicateDetectorIdentical or DuplicateDetectorEpsilon for\n    duplicate_detector, which makes sure blacklisted_candidates are filtered\n    out at the end. If you use DuplicateDetectorNoDetection, it is possible\n    that the config returned is in blacklisted_candidates.\n\n    """"""\n\n    initial_candidates_generator: CandidateGenerator\n    initial_candidates_scorer: ScoringFunction\n    num_initial_candidates: int\n    local_optimizer: LocalOptimizer\n    pending_candidate_state_transformer: Optional[PendingCandidateStateTransformer]\n    blacklisted_candidates: Set[Candidate]\n    num_requested_candidates: int\n    greedy_batch_selection: bool\n    duplicate_detector: DuplicateDetector\n    profiler: GPMXNetSimpleProfiler = None\n    sample_unique_candidates: bool = False\n    debug_log: Optional[DebugLogPrinter] = None\n\n    # Note: For greedy batch selection (num_outer_iterations > 1), the\n    # underlying GPMXNetModel changes with each new pending candidate. The\n    # model changes are managed by pending_candidate_state_transformer. The\n    # model has to be passed to both initial_candidates_scorer and\n    # local_optimizer.\n    def next_candidates(self) -> List[Candidate]:\n        if self.greedy_batch_selection:\n            # Select batch greedily, one candidate at a time, updating the\n            # model in between\n            num_outer_iterations = self.num_requested_candidates\n            num_inner_candidates = 1\n        else:\n            # Select batch in one go\n            num_outer_iterations = 1\n            num_inner_candidates = self.num_requested_candidates\n        assert num_outer_iterations == 1 or self.pending_candidate_state_transformer, \\\n            ""Need pending_candidate_state_transformer for greedy batch selection""\n        candidates = []\n        model = None  # GPMXNetModel, if num_outer_iterations > 1\n        for outer_iter in range(num_outer_iterations):\n            inner_candidates = self._get_next_candidates(\n                num_inner_candidates, model=model)\n            candidates.extend(inner_candidates)\n            if outer_iter < num_outer_iterations - 1:\n                # This is not the last outer iteration\n                self.blacklisted_candidates.update(inner_candidates)\n                # State transformer is used to produce new model\n                # Note: We suppress fit_hyperpars for models obtained during\n                # batch selection\n                for candidate in inner_candidates:\n                    self.pending_candidate_state_transformer.append_candidate(\n                        candidate)\n                model = self.pending_candidate_state_transformer.model(\n                    skip_optimization=True)\n        return candidates\n\n    def _get_next_candidates(self, num_candidates: int,\n                             model: Optional[SurrogateModel]):\n        # generate a random candidates among which to pick the ones to be\n        # locally optimized\n        logger.info(""BO Algorithm: Generating initial candidates."")\n        if self.profiler is not None:\n            self.profiler.start(\'nextcand_genrandom\')\n        if self.sample_unique_candidates:\n            # This can be expensive, depending on what type Candidate is\n            initial_candidates = generate_unique_candidates(\n                self.initial_candidates_generator,\n                self.num_initial_candidates, self.blacklisted_candidates)\n        else:\n            initial_candidates = \\\n                self.initial_candidates_generator.generate_candidates_en_bulk(\n                    self.num_initial_candidates)\n        if self.profiler is not None:\n            self.profiler.stop(\'nextcand_genrandom\')\n            self.profiler.start(\'nextcand_scoring\')\n        logger.info(""BO Algorithm: Scoring (and reordering) candidates."")\n        if self.debug_log is not None:\n            candidates_and_scores = _order_candidates(\n                initial_candidates, self.initial_candidates_scorer,\n                model=model, with_scores=True)\n            initial_candidates = [cand for score, cand in candidates_and_scores]\n            config = initial_candidates[0]\n            top_scores = np.array([x for x, _ in candidates_and_scores[:5]])\n            self.debug_log.set_init_config(config, top_scores)\n        else:\n            initial_candidates = _order_candidates(\n                initial_candidates, self.initial_candidates_scorer,\n                model=model)\n        if self.profiler is not None:\n            self.profiler.stop(\'nextcand_scoring\')\n            self.profiler.start(\'nextcand_localsearch\')\n        candidates_with_optimization = _lazily_locally_optimize(\n            initial_candidates, self.local_optimizer, model=model)\n        logger.info(""BO Algorithm: Selecting final set of candidates."")\n        if self.debug_log is not None and \\\n                isinstance(self.local_optimizer, LBFGSOptimizeAcquisition):\n            # We would like to get num_evaluations from the first run (usually\n            # the only one). This requires peeking at the first entry of the\n            # iterator\n            peek = candidates_with_optimization.__next__()\n            self.debug_log.set_num_evaluations(\n                self.local_optimizer.num_evaluations)\n            candidates_with_optimization = itertools.chain(\n                [peek], candidates_with_optimization)\n        candidates = _pick_from_locally_optimized(\n            candidates_with_optimization, self.blacklisted_candidates,\n            num_candidates, self.duplicate_detector)\n        if self.profiler is not None:\n            self.profiler.stop(\'nextcand_localsearch\')\n        return candidates\n\n\ndef _order_candidates(\n        candidates: List[Candidate],\n        scoring_function: ScoringFunction,\n        model: Optional[SurrogateModel],\n        with_scores: bool = False) -> List[Candidate]:\n    if len(candidates) == 0:\n        return []\n    # scored in batch as this can be more efficient\n    scores = scoring_function.score(candidates, model=model)\n    sorted_list = sorted(zip(scores, candidates), key=lambda x: x[0])\n    if with_scores:\n        return sorted_list\n    else:\n        return [cand for score, cand in sorted_list]\n\n\ndef _lazily_locally_optimize(\n        candidates: List[Candidate],\n        local_optimizer: LocalOptimizer,\n        model: Optional[SurrogateModel]) -> Iterator[Tuple[Candidate, Candidate]]:\n    """"""\n    Due to local deduplication we do not know in advance how many candidates\n    we have to locally optimize, hence this helper to create a lazy generator\n    of locally optimized candidates\n    """"""\n    for cand in candidates:\n        yield cand, local_optimizer.optimize(cand, model=model)\n\n\n# Note: If duplicate_detector is at least DuplicateDetectorIdentical, it will\n# filter out candidates in blacklisted_candidates here. Such can in principle\n# arise if sample_unique_candidates == False.\n# This does not work if duplicate_detector is DuplicateDetectorNoDetection.\ndef _pick_from_locally_optimized(\n        candidates_with_optimization: Iterator[Tuple[Candidate, Candidate]],\n        blacklisted_candidates: Set[Candidate],\n        num_candidates: int,\n        duplicate_detector: DuplicateDetector) -> List[Candidate]:\n    updated_blacklist = set(blacklisted_candidates)  # make a copy\n    result = []\n    for original_candidate, optimized_candidate in candidates_with_optimization:\n        insert_candidate = None\n        optimized_is_duplicate = duplicate_detector.contains(\n            updated_blacklist, optimized_candidate)\n        if optimized_is_duplicate:\n            # in the unlikely case that the optimized candidate ended at a\n            # place that caused a duplicate we try to return the original instead\n            original_also_duplicate = duplicate_detector.contains(\n                updated_blacklist, original_candidate)\n            if not original_also_duplicate:\n                insert_candidate = original_candidate\n        else:\n            insert_candidate = optimized_candidate\n        if insert_candidate is not None:\n            result.append(insert_candidate)\n            updated_blacklist.add(insert_candidate)\n        if len(result) == num_candidates:\n            break\n\n    return result\n'"
autogluon/searcher/bayesopt/tuning_algorithms/bo_algorithm_components.py,0,"b'from typing import Iterable, List, Type, Optional\nimport numpy as np\nfrom scipy.optimize import fmin_l_bfgs_b\nimport logging\n\nfrom autogluon.searcher.bayesopt.datatypes.common import Candidate\nfrom autogluon.searcher.bayesopt.datatypes.tuning_job_state import \\\n    TuningJobState\nfrom autogluon.searcher.bayesopt.tuning_algorithms.base_classes import \\\n    SurrogateModel, AcquisitionFunction, ScoringFunction, LocalOptimizer\n\nlogger = logging.getLogger(__name__)\n\n\nclass IndependentThompsonSampling(ScoringFunction):\n    """"""\n    Note: This is not Thompson sampling, but rather a variant called\n    ""independent Thompson sampling"", where means and variances are drawn\n    from the marginal rather than the joint distribution. This is cheap,\n    but incorrect.\n\n    """"""\n    def __init__(\n            self, model: SurrogateModel,\n            random_state: Optional[np.random.RandomState] = None):\n        self.model = model\n        if random_state is None:\n            random_state = np.random.RandomState(31415629)\n        self.random_state = random_state\n\n    def score(self, candidates: Iterable[Candidate],\n              model: Optional[SurrogateModel] = None) -> List[float]:\n        if model is None:\n            model = self.model\n        predictions_list = model.predict_candidates(candidates)\n        scores = []\n        # If the model supports fantasizing, posterior_means is a matrix. In\n        # that case, samples are drawn for every column, then averaged (why\n        # we need np.mean)\n        for posterior_means, posterior_stds in predictions_list:\n            new_score = [\n                np.mean(self.random_state.normal(m, s))\n                for m, s in zip(posterior_means, posterior_stds)]\n            scores.append(new_score)\n        return list(np.mean(np.array(scores), axis=0))\n\n\nclass LBFGSOptimizeAcquisition(LocalOptimizer):\n    def __init__(self, state: TuningJobState, model: SurrogateModel,\n                 acquisition_function_class: Type[AcquisitionFunction]):\n        super().__init__(state, model, acquisition_function_class)\n        # Number criterion evaluations in last recent optimize call\n        self.num_evaluations = None\n\n    def optimize(self, candidate: Candidate,\n                 model: Optional[SurrogateModel] = None) -> Candidate:\n        # Before local minimization, the model for this state_id should have been fitted.\n        if model is None:\n            model = self.model\n        state = self.state\n        acquisition_function = self.acquisition_function_class(model)\n\n        x0 = state.hp_ranges.to_ndarray(candidate)\n        bounds = state.hp_ranges.get_ndarray_bounds()\n        n_evaluations = [0]  # wrapped in list to allow access from function\n\n        # unwrap 2d arrays\n        def f_df(x):\n            n_evaluations[0] += 1\n            f, df = acquisition_function.compute_acq_with_gradients(x)\n            assert len(f) == 1\n            assert len(df) == 1\n            return f[0], df[0]\n\n        res = fmin_l_bfgs_b(f_df, x0=x0, bounds=bounds, maxiter=1000)\n        self.num_evaluations = n_evaluations[0]\n        if res[2][\'task\'] == b\'ABNORMAL_TERMINATION_IN_LNSRCH\':\n            # this condition was copied from the old GPyOpt code\n            # this condition was silently ignored in the old code\n            logger.warning(\n                f""ABNORMAL_TERMINATION_IN_LNSRCH in lbfgs after {n_evaluations[0]} evaluations, ""\n                ""returning original candidate""\n            )\n            return candidate  # returning original candidate\n        else:\n            # Clip to avoid situation where result is small epsilon out of bounds\n            a_min, a_max = zip(*bounds)\n            optimized_x = np.clip(res[0], a_min, a_max)\n            # Make sure the above clipping does really just fix numerical rounding issues in LBFGS\n            # if any bigger change was made there is a bug and we want to throw an exception\n            assert np.linalg.norm(res[0] - optimized_x) < 1e-6, (res[0], optimized_x, bounds)\n            result = state.hp_ranges.from_ndarray(optimized_x.flatten())\n            return result\n\n\nclass NoOptimization(LocalOptimizer):\n    def optimize(self, candidate: Candidate,\n                 model: Optional[SurrogateModel]=None) -> Candidate:\n        return candidate\n'"
autogluon/searcher/bayesopt/tuning_algorithms/common.py,0,"b'from typing import Iterator, Set, List\nimport numpy as np\nimport logging\n\nfrom autogluon.searcher.bayesopt.datatypes.common import Candidate\nfrom autogluon.searcher.bayesopt.datatypes.tuning_job_state import \\\n    TuningJobState\nfrom autogluon.searcher.bayesopt.tuning_algorithms.base_classes import \\\n    CandidateGenerator\nfrom autogluon.searcher.bayesopt.datatypes.hp_ranges import \\\n    HyperparameterRanges\n\nlogger = logging.getLogger(__name__)\n\n\nclass RandomCandidateGenerator(CandidateGenerator):\n    def __init__(self, hp_ranges: HyperparameterRanges, random_seed: int):\n        self.hp_ranges = hp_ranges\n        self.random_seed = random_seed\n\n    def generate_candidates(self) -> Iterator[Candidate]:\n        random_state = np.random.RandomState(self.random_seed)\n        while True:\n            yield self.hp_ranges.random_candidate(random_state)\n\n    def generate_candidates_en_bulk(self, num_cands: int) -> List[Candidate]:\n        random_state = np.random.RandomState(self.random_seed)\n        return self.hp_ranges.random_candidates(random_state, num_cands)\n\n\nclass RandomStatefulCandidateGenerator(CandidateGenerator):\n    """"""\n    As opposed to RandomCandidateGenerator, this generator maintains a\n    random state, so if generate_candidates is called several times, different\n    sequences are returned.\n\n    """"""\n    def __init__(self, hp_ranges: HyperparameterRanges,\n                 random_state: np.random.RandomState):\n        self.hp_ranges = hp_ranges\n        self.random_state = random_state\n\n    def generate_candidates(self) -> Iterator[Candidate]:\n        while True:\n            yield self.hp_ranges.random_candidate(self.random_state)\n\n    def generate_candidates_en_bulk(self, num_cands: int) -> List[Candidate]:\n        return self.hp_ranges.random_candidates(self.random_state, num_cands)\n\n\ndef compute_blacklisted_candidates(state: TuningJobState) -> Set[Candidate]:\n    return set([x.candidate for x in state.candidate_evaluations] + \\\n               state.pending_candidates + state.failed_candidates)\n\n\nMAX_RETRIES_ON_DUPLICATES = 10000\n\n\n# ATTENTION: If this is used with Candidate = CS.Configuration, the overhead\n# for filtering out duplicates and blacklisted configs becomes large\ndef generate_unique_candidates(\n        candidates_generator: CandidateGenerator, num_candidates: int,\n        blacklisted_candidates: Set[Candidate]) -> List[Candidate]:\n    blacklisted = set(blacklisted_candidates)  # copy\n    result = []\n    num_results = 0\n    retries = 0\n    for i, cand in enumerate(candidates_generator.generate_candidates()):\n        if cand not in blacklisted:\n            result.append(cand)\n            num_results += 1\n            blacklisted.add(cand)\n            retries = 0\n        else:\n            # found a duplicate; retry\n            retries += 1\n\n        # End loop if enough candidates where generated, or after too many retries\n        # (this latter can happen when most of them are duplicates, and must be done\n        # to avoid infinite loops in the purely discrete case)\n        if num_results == num_candidates or retries > MAX_RETRIES_ON_DUPLICATES:\n            if retries > MAX_RETRIES_ON_DUPLICATES:\n                logger.warning(\n                    f""Reached limit of {MAX_RETRIES_ON_DUPLICATES} retries with i={i}. ""\n                    f""Returning {len(result)} candidates instead of the requested {num_candidates}""\n                )\n            break\n\n    return result\n'"
autogluon/searcher/bayesopt/tuning_algorithms/default_algorithm.py,0,"b""from autogluon.searcher.bayesopt.models.nphead_acqfunc import \\\n    EIAcquisitionFunction\nfrom autogluon.searcher.bayesopt.tuning_algorithms.bo_algorithm_components import \\\n    LBFGSOptimizeAcquisition\n\n\nDEFAULT_ACQUISITION_FUNCTION = EIAcquisitionFunction\nDEFAULT_LOCAL_OPTIMIZER_CLASS = LBFGSOptimizeAcquisition\nDEFAULT_NUM_INITIAL_CANDIDATES = 250\nDEFAULT_NUM_INITIAL_RANDOM_EVALUATIONS = 5\nDEFAULT_METRIC = 'active_metric'\n\n\ndef dictionarize_objective(x):\n    return {DEFAULT_METRIC: x}\n"""
autogluon/searcher/bayesopt/utils/__init__.py,0,b''
autogluon/searcher/bayesopt/utils/density.py,0,"b""from math import erfc\nimport numpy as np\nfrom scipy.special import erfc\n\n\ndef get_quantiles(acquisition_par, fmin, m, s):\n    '''\n    Quantiles of the Gaussian distribution useful to determine the acquisition function values\n    :param acquisition_par: parameter of the acquisition function\n    :param fmin: current minimum.\n    :param m: vector of means.\n    :param s: vector of standard deviations.\n    '''\n    if isinstance(s, np.ndarray):\n        s[s<1e-10] = 1e-10\n    elif s< 1e-10:\n        s = 1e-10\n    u = (fmin - m - acquisition_par)/s\n\n    phi = np.exp(-0.5 * u**2) / np.sqrt(2*np.pi)\n    # vectorized version of erfc to not depend on scipy\n    Phi = 0.5 * erfc(-u / np.sqrt(2))\n    return (phi, Phi, u)\n"""
autogluon/searcher/bayesopt/utils/duplicate_detector.py,0,"b'from abc import ABC, abstractmethod\nfrom typing import Set\n\nfrom autogluon.searcher.bayesopt.datatypes.common import Candidate\nfrom autogluon.searcher.bayesopt.datatypes.hp_ranges import \\\n    HyperparameterRanges\n\n\nclass DuplicateDetector(ABC):\n    @abstractmethod\n    def contains(self, existing_candidates: Set[Candidate], new_candidate: Candidate) -> bool:\n        pass\n\n\nclass DuplicateDetectorNoDetection(DuplicateDetector):\n    def contains(self, existing_candidates: Set[Candidate], new_candidate: Candidate) -> bool:\n        return False  # no duplicate detection at all\n\n\nclass DuplicateDetectorIdentical(DuplicateDetector):\n    def contains(self, existing_candidates: Set[Candidate], new_candidate: Candidate) -> bool:\n        return new_candidate in existing_candidates\n\n\nDUPLICATE_DETECTION_EPSILON = 1e-8\n\n\nclass DuplicateDetectorEpsilon(DuplicateDetector):\n    def __init__(self, hp_ranges: HyperparameterRanges):\n        self.hp_ranges = hp_ranges\n\n    def contains(self, existing_candidates: Set[Candidate], new_candidate: Candidate) -> bool:\n        return any(self._almost_equal(c, new_candidate) for c in existing_candidates)\n\n    def _almost_equal(self, candidate1, candidate2):\n        assert len(candidate1) == len(candidate2), (candidate1, candidate2)\n        np_cand1 = self.hp_ranges.to_ndarray(candidate1)\n        np_cand2 = self.hp_ranges.to_ndarray(candidate2)\n        assert np_cand1.shape == np_cand2.shape, (np_cand1, np_cand2)\n        return all(abs(hp1 - hp2) < DUPLICATE_DETECTION_EPSILON for hp1, hp2 in zip(np_cand1, np_cand2))\n'"
autogluon/searcher/bayesopt/utils/test_objects.py,0,"b'# Could eventually remove this code: Is this needed in unit tests?\n\n""""""\nObject definitions that are used for testing.\n""""""\n\nfrom typing import Iterator\nimport numpy as np\n\nfrom autogluon.searcher.bayesopt.tuning_algorithms.base_classes import \\\n    CandidateGenerator\nfrom autogluon.searcher.bayesopt.datatypes.common import StateIdAndCandidate\nfrom autogluon.searcher.bayesopt.datatypes.hp_ranges import \\\n    HyperparameterRanges_Impl, HyperparameterRangeContinuous, \\\n    HyperparameterRangeInteger, HyperparameterRangeCategorical\nfrom autogluon.searcher.bayesopt.datatypes.scaling import LogScaling, \\\n    LinearScaling\nfrom autogluon.searcher.bayesopt.tuning_algorithms.default_algorithm import \\\n    dictionarize_objective\n\n\nclass RepeatedCandidateGenerator(CandidateGenerator):\n    """"""Generates candidates from a fixed set. Used to test the deduplication logic.""""""\n    def __init__(self, n_unique_candidates: int):\n        self.all_unique_candidates = [\n            (1.0*j, j, ""value_"" + str(j))\n            for j in range(n_unique_candidates)\n        ]\n\n    def generate_candidates(self) -> Iterator[StateIdAndCandidate]:\n        i = 0\n        while True:\n            i += 1\n            yield self.all_unique_candidates[i % len(self.all_unique_candidates)]\n\n\n# Example black box function, with adjustable location of global minimum.\n# Potentially could catch issues with optimizer, e.g. if the optimizer\n# ignoring somehow candidates on the edge of search space.\n# A simple quadratic function is used.\nclass Quadratic3d:\n    def __init__(self, local_minima, active_metric, metric_names):\n        # local_minima: point where local_minima is located\n        self.local_minima = np.array(local_minima).astype(\'float\')\n        self.local_minima[0] = np.log10(self.local_minima[0])\n        self.active_metric = active_metric\n        self.metric_names = metric_names\n\n    @property\n    def search_space(self):\n        return HyperparameterRanges_Impl(\n            HyperparameterRangeContinuous(\'x\', 1.0, 100.0, scaling=LogScaling()),\n            HyperparameterRangeInteger(\'y\', 0, 2, scaling=LinearScaling()),\n            HyperparameterRangeCategorical(\'z\', (\'0.0\', \'1.0\', \'2.0\'))\n        )\n\n    @property\n    def f_min(self):\n        return 0.0\n\n    def __call__(self, candidate):\n        p = np.array([float(hp) for hp in candidate])\n        p[0] = np.log10(p[0])\n        return dictionarize_objective(np.sum((self.local_minima - p) ** 2))\n'"
autogluon/task/object_detection/dataset/__init__.py,0,"b'from .base import *\nfrom .voc import *\nfrom .coco import *\n\nimport logging\nlogger = logging.getLogger(__name__)\n\ndef get_dataset(root=\'~/.mxnet/datasets/voc\', index_file_name=\'trainval\', name=None, \\\n                classes=None, format=\'voc\', Train=True, **kwargs):\n    """""" Load dataset to use for object detection, which must be in either VOC or COCO format.\n        \n    Parameters\n    ----------\n    root : str\n        Path to folder storing the dataset.\n    index_file_name : str\n        Name of file containing the training/validation indices of each text example. The name of the .txt file which constains images for training or testing. \n        this is only for custom dataset.\n    name: str\n        name for built-in dataset, (\'voc\', \'voc2007\' or \'voc2012\')\n        when use built-in dataset, the index_file_name should be None.\n    classes : tuple of classes, default = None\n        users can specify classes for custom dataset ex. classes = (\'bike\', \'bird\', \'cat\', ...)\n        We reuse the neural network weights if the corresponding class appears in the pretrained model. \n        Otherwise, we randomly initialize the neural network weights for new classes.\n    format : str\n        Format of the object detection dataset, either: \'voc\' or \'coco\'.\n        For details, see: `autogluon/task/object_detection/dataset/voc.py`, `autogluon/task/object_detection/dataset/coco.py`\n    Train : bool, default = True\n        pecify Train/Test mode. It is only valid when name is not None.\n    kwargs : keyword arguments\n        Passed to either: :meth:`autogluon.task.object_detection.dataset.CustomVOCDetection` or :meth:`autogluon.task.object_detection.dataset.COCO`.\n    \n    Returns\n    -------\n    Dataset object that can be passed to `task.fit()`, which is actually an :class:`autogluon.space.AutoGluonObject`. \n    To interact with such an object yourself, you must first call `Dataset.init()` to instantiate the object in Python.\n    """"""\n    if format==\'voc\':\n        logger.info("">>> create dataset(VOC format) "")\n\n        # built-in dataset\n        if name:\n            if Train:\n                if name==\'voc\':\n                    splits = [(\'VOC2007\', \'trainval\'), (\'VOC2012\', \'trainval\')]\n                elif name==\'voc2007\':\n                    splits = [(\'VOC2007\', \'trainval\')]\n            else:\n                splits= [(\'VOC2007\', \'test\')] \n        else:  # custom dataset\n            splits = [(\'\', index_file_name)]\n        return CustomVOCDetection(root, splits, name, classes, **kwargs)\n\n    elif format==\'coco\':\n        logger.info("">>> create dataset(COCO format)"")\n        return COCO(*args, **kwargs)\n    else:\n        raise NotImplementedError(\'Other data formats are not implemented.\')\n\n'"
autogluon/task/object_detection/dataset/base.py,0,"b'from ....core import *\nfrom abc import ABC, abstractmethod\n\nclass DatasetBase(ABC):\n    @abstractmethod\n    def __init__(self):\n        pass\n    \n    @abstractmethod\n    def get_dataset_and_metric(self):\n        pass\n\n    @abstractmethod\n    def get_dataset_name(self):\n        pass\n\n    \n\n\n\n\n    \n'"
autogluon/task/object_detection/dataset/coco.py,0,"b'from .base import DatasetBase\nfrom ....core import *\n\nfrom gluoncv import data as gdata\n\n@obj()\nclass COCO(DatasetBase):\n    """"""Built-in class to work with the well-known COCO dataset for object detection. \n    \n    Returns\n    -------\n    Dataset object that can be passed to `task.fit()`, which is actually an :class:`autogluon.space.AutoGluonObject`. \n    To interact with such an object yourself, you must first call `Dataset.init()` to instantiate the object in Python.\n    """"""\n    def __init__(self):\n        super(COCO. self).__init__()\n        self.train_dataset = gdata.COCODetection(splits=\'instances_train2017\')\n        self.val_dataset = gdata.COCODetection(splits=\'instances_val2017\', skip_empty=False)\n        self.val_metric = COCODetectionMetric(\n                                self.val_dataset, args.save_prefix + \'_eval\', cleanup=True,\n                                data_shape=(args.data_shape, args.data_shape))\n        \n        #TODO: whether to use the code below\n        """"""\n        # coco validation is slow, consider increase the validation interval\n        if args.val_interval == 1:\n            args.val_interval = 10\n        """"""\n    \n    def get_train_val_metric(self):\n        return (self.train_dataset, self.val_dataset, self.val_metric)\n    \n    def get_dataset_name(self):\n        return \'coco\'\n\n\n\n\n\n\n'"
autogluon/task/object_detection/dataset/voc.py,0,"b'""""""Pascal VOC object detection dataset.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nimport os\nimport warnings\nimport numpy as np\nimport glob\ntry:\n    import xml.etree.cElementTree as ET\nexcept ImportError:\n    import xml.etree.ElementTree as ET\nimport mxnet as mx\nfrom gluoncv.data.base import VisionDataset\n\nfrom ....core import *\nfrom .base import DatasetBase\nimport autogluon as ag\n\nfrom gluoncv import data as gdata\nfrom gluoncv.utils.metrics.voc_detection import VOC07MApMetric\n\nclass CustomVOCDetectionBase(gdata.VOCDetection):\n    """"""Base class for custom Dataset which follows protocol/formatting of the well-known VOC object detection dataset.\n    \n    Parameters\n    ----------\n    class: tuple of classes, default = None\n        We reuse the neural network weights if the corresponding class appears in the pretrained model. \n        Otherwise, we randomly initialize the neural network weights for new classes.\n    root : str, default \'~/mxnet/datasets/voc\'\n        Path to folder storing the dataset.\n    splits : list of tuples, default ((2007, \'trainval\'), (2012, \'trainval\'))\n        List of combinations of (year, name)\n        For years, candidates can be: 2007, 2012.\n        For names, candidates can be: \'train\', \'val\', \'trainval\', \'test\'.\n    transform : callable, default = None\n        A function that takes data and label and transforms them. Refer to\n        :doc:`./transforms` for examples.\n        A transform function for object detection should take label into consideration,\n        because any geometric modification will require label to be modified.\n    index_map : dict, default = None\n        By default, the 20 classes are mapped into indices from 0 to 19. We can\n        customize it by providing a str to int dict specifying how to map class\n        names to indices. This is only for advanced users, when you want to swap the orders\n        of class labels.\n    preload_label : bool, default = True\n        If True, then parse and load all labels into memory during\n        initialization. It often accelerate speed but require more memory\n        usage. Typical preloaded labels took tens of MB. You only need to disable it\n        when your dataset is extremely large.\n    """"""\n\n    def __init__(self, classes=None, root=os.path.join(\'~\', \'.mxnet\', \'datasets\', \'voc\'),\n                 splits=((2007, \'trainval\'), (2012, \'trainval\')),\n                 transform=None, index_map=None, preload_label=True):\n\n        # update classes \n        if classes:\n            self._set_class(classes)  \n        super(CustomVOCDetectionBase, self).__init__(root=root,\n                                               splits=splits,\n                                               transform=transform,\n                                               index_map=index_map,\n                                               preload_label=False),\n        self._items_new = [self._items[each_id] for each_id in range(len(self._items)) if self._check_valid(each_id) ]\n        self._items = self._items_new\n        self._label_cache = self._preload_labels() if preload_label else None\n    \n    @classmethod\n    def _set_class(cls, classes):\n        cls.CLASSES = classes\n    \n    def _load_items(self, splits):\n        """"""Load individual image indices from splits.""""""\n        ids = []\n        for subfolder, name in splits:\n            root = os.path.join(self._root, subfolder) if subfolder else self._root\n            lf = os.path.join(root, \'ImageSets\', \'Main\', name + \'.txt\')\n            with open(lf, \'r\') as f:\n                ids += [(root, line.strip()) for line in f.readlines()]\n        return ids\n\n    def _check_valid(self, idx):\n        """"""Parse xml file and return labels.""""""\n        img_id = self._items[idx]\n        anno_path = self._anno_path.format(*img_id)\n        root = ET.parse(anno_path).getroot()\n        size = root.find(\'size\')\n        width = float(size.find(\'width\').text)\n        height = float(size.find(\'height\').text)\n        if idx not in self._im_shapes:\n            # store the shapes for later usage\n            self._im_shapes[idx] = (width, height)\n        label = []\n        for obj in root.iter(\'object\'):\n            try:\n                difficult = int(obj.find(\'difficult\').text)\n            except ValueError:\n                difficult = 0\n            cls_name = obj.find(\'name\').text.strip().lower()\n            if cls_name not in self.classes:\n                continue\n            cls_id = self.index_map[cls_name]\n            xml_box = obj.find(\'bndbox\')\n            xmin = (float(xml_box.find(\'xmin\').text) - 1)\n            ymin = (float(xml_box.find(\'ymin\').text) - 1)\n            xmax = (float(xml_box.find(\'xmax\').text) - 1)\n            ymax = (float(xml_box.find(\'ymax\').text) - 1)\n\n            if not ((0 <= xmin < width) and (0 <= ymin < height) \\\n                    and (xmin < xmax <= width) and (ymin < ymax <= height)):\n                return False\n\n        return True\n\n\n@obj()\nclass CustomVOCDetection():\n    """"""Custom Dataset which follows protocol/formatting of the well-known VOC object detection dataset.\n    \n    Parameters\n    ----------\n    root : str, default \'~/mxnet/datasets/voc\'\n        Path to folder storing the dataset.\n    splits : list of tuples        \n        List of combinations of (year, name) to indicate how to split data into training, validation, and test sets.\n        For the original VOC dataset, the year candidates can be: 2007, 2012.\n        For the original VOC dataset, the name candidates can be: \'train\', \'val\', \'trainval\', \'test\'.\n        For the original VOC dataset, one might use for example: ((2007, \'trainval\'), (2012, \'trainval\'))\n    classes: tuple of classes\n        We reuse the neural network weights if the corresponding class appears in the pretrained model. \n        Otherwise, we randomly initialize the neural network weights for new classes.\n    \n    Returns\n    -------\n    Dataset object that can be passed to `task.fit()`, which is actually an :class:`autogluon.space.AutoGluonObject`. \n    To interact with such an object yourself, you must first call `Dataset.init()` to instantiate the object in Python.\n    """"""\n    def __init__(self, root, splits, name, classes, **kwargs):\n        super().__init__()\n        self.root = root\n        \n        # search classes from gt files for custom dataset\n        if not (classes or name):\n            classes = self.generate_gt() \n        \n        self.dataset = CustomVOCDetectionBase(classes=classes,\n                                              root=root,\n                                              splits=splits)\n\n        self.metric = VOC07MApMetric(iou_thresh=0.5, class_names=self.dataset.classes)\n\n    def get_dataset_and_metric(self):\n        return (self.dataset, self.metric)\n\n    def get_classes(self):\n        return self.dataset.classes \n\n    def generate_gt(self):\n        classes = []\n        all_xml = glob.glob( os.path.join(self.root, \'Annotations\', \'*.xml\') )\n        for each_xml_file in all_xml:\n            tree = ET.parse(each_xml_file)\n            root = tree.getroot()\n            for child in root:\n                if child.tag==\'object\':\n                    for item in child:\n                        if item.tag==\'name\':\n                            object_name = item.text\n                            if object_name not in classes:\n                                classes.append(object_name)\n\n        classes = sorted(classes)\n        return classes\n\n\n'"
autogluon/utils/tabular/data/__init__.py,0,b''
autogluon/utils/tabular/data/cleaner.py,0,"b""import logging\nfrom pandas import DataFrame\n\nfrom ..ml.constants import BINARY, MULTICLASS, REGRESSION\n\nlogger = logging.getLogger(__name__)\n\n\n# Cleaner cleans data prior to entering feature generation\nclass Cleaner:\n    @staticmethod\n    def construct(problem_type: str, label: str, threshold: int):\n        if problem_type == BINARY:\n            return CleanerDummy()\n        elif problem_type == MULTICLASS:\n            return CleanerMulticlass(label=label, threshold=threshold)\n        elif problem_type == REGRESSION:\n            return CleanerDummy()\n        else:\n            raise NotImplementedError\n\n    def fit(self, X: DataFrame) -> DataFrame:\n        raise NotImplementedError\n\n    def fit_transform(self, X: DataFrame) -> DataFrame:\n        self.fit(X)\n        return self.transform(X)\n\n    def transform(self, X: DataFrame) -> DataFrame:\n        raise NotImplementedError\n\n\nclass CleanerDummy(Cleaner):\n    def __init__(self):\n        pass\n\n    def fit(self, X: DataFrame) -> DataFrame:\n        pass\n\n    def transform(self, X: DataFrame) -> DataFrame:\n        return X\n\n\nclass CleanerMulticlass(Cleaner):\n    def __init__(self, label: str, threshold: int):\n        self.label = label\n        self.threshold = threshold\n        self.valid_classes = None\n\n    def fit(self, X: DataFrame):\n        self.valid_classes = self.get_valid_classes(X=X, label=self.label, threshold=self.threshold)\n\n    def transform(self, X: DataFrame) -> DataFrame:\n        return self.remove_classes(X=X, label=self.label, valid_classes=self.valid_classes)\n\n    @staticmethod\n    def get_valid_classes(X, label, threshold):\n        class_counts = X[label].value_counts()\n        class_counts_valid = class_counts[class_counts >= threshold]\n        valid_classes = list(class_counts_valid.index)\n        sum_prior = sum(class_counts)\n        sum_after = sum(class_counts_valid)\n        percent = sum_after / sum_prior\n        if len(valid_classes) < len(class_counts):\n            logger.log(25, 'Warning: Some classes in the training set have fewer than %s examples. AutoGluon will only keep %s out of %s classes for training and will not try to predict the rare classes. '\n                           'To keep more classes, increase the number of datapoints from these rare classes in the training data or reduce label_count_threshold.' % (threshold, len(valid_classes), len(class_counts)))\n        if percent < 1.0:\n            logger.log(25, 'Fraction of data from classes with at least %s examples that will be kept for training models: %s' % (threshold, percent))\n        return valid_classes\n\n    @staticmethod\n    def remove_classes(X, label, valid_classes):\n        X = X[X[label].isin(valid_classes)]\n        return X\n"""
autogluon/utils/tabular/data/label_cleaner.py,0,"b""import logging\n\nimport numpy as np\nfrom pandas import Series\n\nfrom ..ml.constants import BINARY, MULTICLASS, REGRESSION\n\nlogger = logging.getLogger(__name__)\n\n\n# LabelCleaner cleans labels prior to entering feature generation\nclass LabelCleaner:\n    num_classes = None\n    inv_map = None\n    ordered_class_labels = None\n    ordered_class_labels_transformed = None\n\n    @staticmethod\n    def construct(problem_type: str, y: Series, y_uncleaned: Series):\n        if problem_type == BINARY:\n            return LabelCleanerBinary(y)\n        elif problem_type == MULTICLASS:\n            if len(y.unique()) == 2:\n                return LabelCleanerMulticlassToBinary(y, y_uncleaned)\n            else:\n                return LabelCleanerMulticlass(y, y_uncleaned)\n        elif problem_type == REGRESSION:\n            return LabelCleanerDummy()\n        else:\n            raise NotImplementedError\n\n    def transform(self, y: Series) -> Series:\n        raise NotImplementedError\n\n    def inverse_transform(self, y: Series) -> Series:\n        raise NotImplementedError\n\n    def transform_proba(self, y):\n        return y\n\n    def inverse_transform_proba(self, y):\n        return y\n\n\nclass LabelCleanerMulticlass(LabelCleaner):\n    def __init__(self, y: Series, y_uncleaned: Series):\n        self.cat_mappings_dependent_var: dict = self._generate_categorical_mapping(y)\n        self.inv_map: dict = {v: k for k, v in self.cat_mappings_dependent_var.items()}\n\n        self.cat_mappings_dependent_var_uncleaned: dict = self._generate_categorical_mapping(y_uncleaned)\n        self.inv_map_uncleaned: dict = {v: k for k, v in self.cat_mappings_dependent_var_uncleaned.items()}\n\n        self.num_classes = len(self.cat_mappings_dependent_var.keys())\n        self.ordered_class_labels = list(y_uncleaned.astype('category').cat.categories)\n        self.valid_ordered_class_labels = list(y.astype('category').cat.categories)\n        self.ordered_class_labels_transformed = list(range(len(self.valid_ordered_class_labels)))\n        self.invalid_class_count = len(self.ordered_class_labels) - len(self.valid_ordered_class_labels)\n        self.labels_to_zero_fill = [1 if label not in self.valid_ordered_class_labels else 0 for label in self.ordered_class_labels]\n        self.label_index_to_keep = [i for i, label in enumerate(self.labels_to_zero_fill) if label == 0]\n        self.label_index_to_remove = [i for i, label in enumerate(self.labels_to_zero_fill) if label == 1]\n\n    def transform(self, y: Series) -> Series:\n        if isinstance(y, np.ndarray):\n            y = Series(y)\n        y = y.map(self.inv_map)\n        return y\n\n    def inverse_transform(self, y: Series) -> Series:\n        y = y.map(self.cat_mappings_dependent_var)\n        return y\n\n    # TODO: Unused?\n    def transform_proba(self, y):\n        if self.invalid_class_count > 0:\n            # this assumes y has only 0's for any columns it is about to remove, if it does not, weird things may start to happen since rows will not sum to 1\n            return np.delete(y, self.label_index_to_remove, axis=1)\n        else:\n            return y\n\n    def inverse_transform_proba(self, y):\n        if self.invalid_class_count > 0:\n            y_transformed = np.zeros([len(y), len(self.ordered_class_labels)])\n            y_transformed[:, self.label_index_to_keep] = y\n            return y_transformed\n        else:\n            return y\n\n    @staticmethod\n    def _generate_categorical_mapping(y: Series) -> dict:\n        categories = y.astype('category')\n        cat_mappings_dependent_var = dict(enumerate(categories.cat.categories))\n        return cat_mappings_dependent_var\n\n\n# TODO: Expand print statement to multiclass as well\nclass LabelCleanerBinary(LabelCleaner):\n    def __init__(self, y: Series):\n        self.num_classes = 2\n        self.unique_values = list(y.unique())\n        if len(self.unique_values) != 2:\n            raise AssertionError('y does not contain exactly 2 unique values:', self.unique_values)\n        # TODO: Clean this code, for loop\n        if (1 in self.unique_values) and (2 in self.unique_values):\n            self.inv_map: dict = {1: 0, 2: 1}\n        elif ('1' in self.unique_values) and ('2' in self.unique_values):\n            self.inv_map: dict = {'1': 0, '2': 1}\n        elif ((str(False) in [str(val) for val in self.unique_values]) and\n              (str(True) in [str(val) for val in self.unique_values])):\n            false_val = [val for val in self.unique_values if str(val) == str(False)][0]  # may be str or bool\n            true_val = [val for val in self.unique_values if str(val) == str(True)][0]  # may be str or bool\n            self.inv_map: dict = {false_val: 0, true_val: 1}\n        elif (0 in self.unique_values) and (1 in self.unique_values):\n            self.inv_map: dict = {0: 0, 1: 1}\n        elif ('0' in self.unique_values) and ('1' in self.unique_values):\n            self.inv_map: dict = {'0': 0, '1': 1}\n        elif ('No' in self.unique_values) and ('Yes' in self.unique_values):\n            self.inv_map: dict = {'No': 0, 'Yes': 1}\n        elif ('N' in self.unique_values) and ('Y' in self.unique_values):\n            self.inv_map: dict = {'N': 0, 'Y': 1}\n        elif ('n' in self.unique_values) and ('y' in self.unique_values):\n            self.inv_map: dict = {'n': 0, 'y': 1}\n        elif ('F' in self.unique_values) and ('T' in self.unique_values):\n            self.inv_map: dict = {'F': 0, 'T': 1}\n        elif ('f' in self.unique_values) and ('t' in self.unique_values):\n            self.inv_map: dict = {'f': 0, 't': 1}\n        else:\n            self.inv_map: dict = {self.unique_values[0]: 0, self.unique_values[1]: 1}\n            logger.log(15, 'Note: For your binary classification, AutoGluon arbitrarily selects which label-value represents positive vs negative class')\n        poslabel = [lbl for lbl in self.inv_map.keys() if self.inv_map[lbl] == 1][0]\n        neglabel = [lbl for lbl in self.inv_map.keys() if self.inv_map[lbl] == 0][0]\n        logger.log(20, 'Selected class <--> label mapping:  class 1 = %s, class 0 = %s' % (poslabel, neglabel))\n        self.cat_mappings_dependent_var: dict = {v: k for k, v in self.inv_map.items()}\n        self.ordered_class_labels_transformed = [0, 1]\n        self.ordered_class_labels = [self.cat_mappings_dependent_var[label_transformed] for label_transformed in self.ordered_class_labels_transformed]\n\n    def transform(self, y: Series) -> Series:\n        if isinstance(y, np.ndarray):\n            y = Series(y)\n        y = y.map(self.inv_map)\n        return y\n\n    def inverse_transform(self, y: Series) -> Series:\n        return y.map(self.cat_mappings_dependent_var)\n\n\nclass LabelCleanerMulticlassToBinary(LabelCleanerMulticlass):\n    def __init__(self, y: Series, y_uncleaned: Series):\n        super().__init__(y=y, y_uncleaned=y_uncleaned)\n        self.label_cleaner_binary = LabelCleanerBinary(y=y.map(self.inv_map))\n\n    def transform(self, y: Series) -> Series:\n        y = super().transform(y)\n        y = self.label_cleaner_binary.transform(y)\n        return y\n\n    def inverse_transform_proba(self, y):\n        y = self.convert_binary_proba_to_multiclass_proba(y=y)\n        return super().inverse_transform_proba(y)\n\n    @staticmethod\n    def convert_binary_proba_to_multiclass_proba(y):\n        y_transformed = np.zeros([len(y), 2])\n        y_transformed[:, 0] = 1 - y\n        y_transformed[:, 1] = y\n        return y_transformed\n\n\nclass LabelCleanerDummy(LabelCleaner):\n    def transform(self, y: Series) -> Series:\n        if isinstance(y, np.ndarray):\n            y = Series(y)\n        return y\n\n    def inverse_transform(self, y: Series) -> Series:\n        return y\n"""
autogluon/utils/tabular/features/__init__.py,0,b''
autogluon/utils/tabular/features/abstract_feature_generator.py,0,"b'import copy\nimport logging\nimport re\nimport warnings\nfrom collections import defaultdict\n\nimport numpy as np\nimport pandas as pd\nfrom pandas import DataFrame, Series\nfrom pandas.api.types import CategoricalDtype\n\nfrom ..utils.decorators import calculate_time\nfrom ..utils.savers import save_pkl\n\nlogger = logging.getLogger(__name__)\n\n\n# TODO: Add optimization to make Vectorizer smaller in size by deleting key dictionary\n# TODO: Add feature of # of observation counts to high cardinality categorical features\n# TODO: Use code from problem type detection for column types! Ints/Floats could be Categorical through this method! Maybe try both?\nclass AbstractFeatureGenerator:\n    def __init__(self):\n        self.features_init = []\n        self.features_init_to_keep = []\n        self.features_to_remove = []\n        self.features_to_remove_post = []\n        self.features_to_keep_raw = []\n        self.features_object = []\n        self.features_init_types = dict()\n        self.feature_types = defaultdict(list)\n        self.feature_type_family = defaultdict(list)\n        self.feature_type_family_generated = defaultdict(list)\n        self.features_bool = []\n        self.features_nlp = []\n        self.features_nlp_ratio = []\n        self.features_datetime = []\n        self.features_categorical = []\n        self.features_categorical_final = []\n        self.features_categorical_final_mapping = defaultdict()\n        self.features_binned = []\n        self.features_binned_mapping = defaultdict()\n        self.features_vectorizers = []\n        self.features = []\n        self.banned_features = []\n        self.fit = False\n\n    @property\n    def feature_types_metadata(self):\n        feature_types_metadata = copy.deepcopy(\n            {\n                \'nlp\': self.features_nlp,\n                \'vectorizers\': self.features_vectorizers,\n                **self.feature_type_family\n            }\n        )\n        for key, val in self.feature_type_family_generated.items():\n            if key in feature_types_metadata:\n                feature_types_metadata[key] += val\n            else:\n                feature_types_metadata[key] = val\n        return feature_types_metadata\n\n    @property\n    def feature_types_metadata_generated(self):\n        feature_types_metadata_generated = copy.deepcopy(\n            {**self.feature_type_family_generated}\n        )\n        if \'int\' in feature_types_metadata_generated:  # TODO: Clean this, feature_vectorizers should already be handled\n            feature_types_metadata_generated[\'int\'] += self.features_vectorizers\n        elif len(self.features_vectorizers) > 0:\n            feature_types_metadata_generated[\'int\'] = self.features_vectorizers\n        return feature_types_metadata_generated\n\n    @property\n    def feature_types_metadata_full(self):\n        feature_types_metadata_full = copy.deepcopy(\n            {**self.feature_type_family}\n        )\n        for key, val in self.feature_type_family_generated.items():\n            if key in feature_types_metadata_full:\n                feature_types_metadata_full[key] += val\n            else:\n                feature_types_metadata_full[key] = val\n        if \'int\' in feature_types_metadata_full:  # TODO: Clean this, feature_vectorizers should already be handled\n            feature_types_metadata_full[\'int\'] += self.features_vectorizers\n        elif len(self.features_vectorizers) > 0:\n            feature_types_metadata_full[\'int\'] = self.features_vectorizers\n        return feature_types_metadata_full\n\n    @staticmethod\n    def train_vectorizer(text_list, vectorizer):\n        logger.log(15, \'Fitting vectorizer...\')\n        transform_matrix = vectorizer.fit_transform(text_list)  # TODO: Consider upgrading to pandas 0.25.0 to benefit from sparse attribute improvements / bug fixes! https://pandas.pydata.org/pandas-docs/stable/whatsnew/v0.25.0.html\n        vectorizer.stop_words_ = None  # Reduces object size by 100x+ on large datasets, no effect on usability\n        logger.log(15, f\'Vectorizer fit with vocabulary size = {len(vectorizer.vocabulary_)}\')\n        return vectorizer, transform_matrix\n\n    def preprocess(self, X: DataFrame):\n        return X\n\n    @calculate_time\n    def fit_transform(self, X: DataFrame, y=None, banned_features=None, fix_categoricals=False, drop_duplicates=True):\n        self.fit = False\n        X_len = len(X)\n        if banned_features:\n            self.banned_features = banned_features\n            self.features_to_remove += self.banned_features\n        X_index = copy.deepcopy(X.index)\n        self.get_feature_types(X)\n        X = X.drop(self.features_to_remove, axis=1, errors=\'ignore\')\n        self.features_init_to_keep = copy.deepcopy(list(X.columns))\n        self.features_init_types = X.dtypes.to_dict()\n        X.reset_index(drop=True, inplace=True)\n        X_features = self.generate_features(X)\n        for column in X_features:\n            unique_value_count = len(X_features[column].unique())\n            if unique_value_count == 1:\n                self.features_to_remove_post.append(column)\n            elif column in self.feature_type_family[\'object\'] and (unique_value_count / X_len > 0.99):\n                self.features_to_remove_post.append(column)\n\n        self.features_binned = set(self.features_binned) - set(self.features_to_remove_post)\n        self.features_binned_mapping = self.generate_bins(X_features, self.features_binned)\n        for column in self.features_binned:  # TODO: Should binned columns be continuous or categorical if they were initially continuous? (Currently categorical)\n            X_features[column] = self.bin_column(series=X_features[column], mapping=self.features_binned_mapping[column])\n            # print(X_features[column].value_counts().sort_index())\n        X_features = X_features.drop(self.features_to_remove_post, axis=1)\n        if drop_duplicates:\n            X_features = self.drop_duplicate_features(X_features)\n        self.features_categorical_final = list(X_features.select_dtypes(include=\'category\').columns.values)\n        if fix_categoricals:  # if X_test is not used in fit_transform and the model used is from SKLearn\n            X_features = self.fix_categoricals_for_sklearn(X_features=X_features)\n        for column in self.features_categorical_final:\n            self.features_categorical_final_mapping[column] = X_features[column].cat.categories  # dict(enumerate(X_features[column].cat.categories))\n        X_features.index = X_index\n        self.features = list(X_features.columns)\n        self.feature_type_family_generated[\'int\'] += self.features_binned\n        self.fit = True\n\n        logger.log(20, \'Feature Generator processed %s data points with %s features\' % (X_len, len(self.features)))\n        logger.log(20, \'Original Features:\')\n        for key, val in self.feature_type_family.items():\n            logger.log(20, \'\\t%s features: %s\' % (key, len(val)))\n        logger.log(20, \'Generated Features:\')\n        for key, val in self.feature_types_metadata_generated.items():\n            logger.log(20, \'\\t%s features: %s\' % (key, len(val)))\n        logger.log(20, \'All Features:\')\n        for key, val in self.feature_types_metadata_full.items():\n            logger.log(20, \'\\t%s features: %s\' % (key, len(val)))\n\n        return X_features\n\n    @calculate_time\n    def transform(self, X: DataFrame):\n        if not self.fit:\n            raise AssertionError(\'FeatureGenerator has not\xc2\xa0yet been fit.\')\n        if self.features is None:\n            raise AssertionError(\'FeatureGenerator.features is None, have you called fit() yet?\')\n        X_index = copy.deepcopy(X.index)\n        X = X.drop(self.features_to_remove, axis=1, errors=\'ignore\')\n        X_columns = X.columns.tolist()\n        # Create any columns present in the training dataset that are now missing from this dataframe:\n        missing_cols = []\n        for col in self.features_init_to_keep:\n            if col not in X_columns:\n                missing_cols.append(col)\n        if len(missing_cols) > 0:\n            raise ValueError(f\'Required columns are missing from the provided dataset. Missing columns: {missing_cols}\')\n\n        X = X.astype(self.features_init_types)\n        X.reset_index(drop=True, inplace=True)\n        X_features = self.generate_features(X)\n        for column in self.features_binned:\n            X_features[column] = self.bin_column(series=X_features[column], mapping=self.features_binned_mapping[column])\n        X_features = X_features[self.features]\n        for column in self.features_categorical_final:\n            X_features[column].cat.set_categories(self.features_categorical_final_mapping[column], inplace=True)\n        X_features.index = X_index\n        return X_features\n\n    @staticmethod\n    def bin_column(series, mapping):\n        mapping_dict = {k: v for v, k in enumerate(list(mapping))}\n        series_out = pd.cut(series, mapping)\n        # series_out.cat.categories = [str(g) for g in series_out.cat.categories]  # LightGBM crashes at end of training without this\n        series_out_int = [mapping_dict[val] for val in series_out]\n        return series_out_int\n\n    # TODO: Rewrite with normalized value counts as binning technique, will be more performant and optimal\n    @staticmethod\n    def generate_bins(X_features: DataFrame, features_to_bin):\n        X_len = len(X_features)\n        ideal_cats = 10\n        starting_cats = 1000\n        bin_index_starting = [np.floor(X_len * (num + 1) / starting_cats) for num in range(starting_cats - 1)]\n        bin_epsilon = 0.000000001\n        bin_mapping = defaultdict()\n        max_iterations = 20\n        for column in features_to_bin:\n            num_cats_initial = starting_cats\n            bins_value_counts = X_features[column].value_counts(ascending=False, normalize=True)\n            max_bins = len(bins_value_counts)\n\n            if max_bins <= ideal_cats:\n                bins = pd.Series(data=sorted(X_features[column].unique()))\n                num_cats_initial = max_bins\n                cur_len = max_bins\n                bin_index = range(num_cats_initial)\n                interval_index = AbstractFeatureGenerator.get_bins(bins=bins, bin_index=bin_index, bin_epsilon=bin_epsilon)\n            else:\n                cur_len = X_len\n                bins = X_features[column].sort_values(ascending=True)\n                interval_index = AbstractFeatureGenerator.get_bins(bins=bins, bin_index=bin_index_starting, bin_epsilon=bin_epsilon)\n\n            max_desired_bins = min(ideal_cats, max_bins)\n            min_desired_bins = min(ideal_cats, max_bins)\n\n            if (len(interval_index) >= min_desired_bins) and (len(interval_index) <= max_desired_bins):\n                is_satisfied = True\n            else:\n                is_satisfied = False\n\n            num_cats_current = num_cats_initial\n            # print(column, min_desired_bins, max_desired_bins)\n            cur_iteration = 0\n            while not is_satisfied:\n                if len(interval_index) > max_desired_bins:\n                    pass\n                elif len(interval_index) < min_desired_bins:\n                    pass\n                ratio_reduction = max_desired_bins / len(interval_index)\n                num_cats_current = int(np.floor(num_cats_current * ratio_reduction))\n                bin_index = [np.floor(cur_len * (num + 1) / num_cats_current) for num in range(num_cats_current - 1)]\n                interval_index = AbstractFeatureGenerator.get_bins(bins=bins, bin_index=bin_index, bin_epsilon=bin_epsilon)\n\n                if (len(interval_index) >= min_desired_bins) and (len(interval_index) <= max_desired_bins):\n                    is_satisfied = True\n                    # print(\'satisfied\', column, len(interval_index))\n                cur_iteration += 1\n                if cur_iteration >= max_iterations:\n                    is_satisfied = True\n                    # print(\'max_iterations met, stopping prior to satisfaction!\', column, len(interval_index))\n\n            bin_mapping[column] = interval_index\n        return bin_mapping\n\n    # TODO: Clean code\n    # bins is a sorted int/float series, ascending=True\n    @staticmethod\n    def get_bins(bins: Series, bin_index, bin_epsilon):\n        max_val = bins.max()\n        bins_2 = bins.iloc[bin_index]\n        bins_3 = list(bins_2.values)\n        bins_unique = sorted(list(set(bins_3)))\n        bins_with_epsilon_max = set([i for i in bins_unique] + [i - bin_epsilon for i in bins_unique if i == max_val])\n        removal_bins = set([bins_unique[index - 1] for index, i in enumerate(bins_unique[1:], start=1) if i == max_val])\n        bins_4 = sorted(list(bins_with_epsilon_max - removal_bins))\n        bins_5 = [np.inf if (x == max_val) else x for x in bins_4]\n        bins_6 = sorted(list(set([-np.inf] + bins_5 + [np.inf])))\n        bins_7 = [(bins_6[i], bins_6[i + 1]) for i in range(len(bins_6) - 1)]\n        interval_index = pd.IntervalIndex.from_tuples(bins_7)\n        return interval_index\n\n    def get_feature_types(self, X: DataFrame):\n        self.features_init = list(X.columns)\n        self.features_init = [feature for feature in self.features_init if feature not in self.features_to_remove]\n        for column in self.features_init:\n            mark_for_removal = False\n            col_val = X[column]\n            dtype = col_val.dtype\n            num_unique = len(col_val.unique())\n            unique_counts = col_val.value_counts()\n\n            # num_unique_duplicates = len(unique_counts[unique_counts > 100])\n            # num_rows = len(col_val)\n            # unique_ratio = num_unique / float(num_rows)\n            # print(column)\n            # print(num_unique)\n            # # print(num_rows)\n            # # print(unique_ratio)\n            # print(dtype)\n\n            type_family = self.get_type_family(dtype)\n            # print(num_unique, \'\\t\', num_unique_duplicates, \'\\t\', unique_ratio, \'\\t\', type_family, \'\\t\', column,)\n\n            if num_unique == 1:\n                mark_for_removal = True\n\n            # if num_unique == num_rows:\n            #     print(\'fully unique!\')\n            # if unique_ratio > 0.5:\n            #     print(\'fairly unique!\')\n            # print(col_val.value_counts())\n\n            if self.check_if_datetime_feature(col_val):\n                type_family = \'datetime\'  # TODO: Verify\n                dtype = \'datetime\'\n                self.features_datetime.append(column)\n                logger.debug(f\'date: {column}\')\n                logger.debug(unique_counts.head(5))\n            elif self.check_if_nlp_feature(col_val):\n                self.features_nlp.append(column)\n                self.features_nlp_ratio.append(column)\n                logger.debug(f\'nlp: {column}\')\n                logger.debug(unique_counts.head(5))\n            # print(is_nlp, \'\\t\', column)\n\n            if mark_for_removal:\n                self.features_to_remove.append(column)\n            else:\n                self.feature_type_family[type_family].append(column)\n                if type_family == \'object\':\n                    self.features_categorical.append(column)\n                elif type_family != \'datetime\':\n                    self.features_to_keep_raw.append(column)\n                self.feature_types[dtype].append(column)\n\n        pass\n\n    def generate_features(self, X: DataFrame):\n        raise NotImplementedError()\n\n    # TODO: Expand to int64 -> date features (milli from epoch etc)\n    def check_if_datetime_feature(self, X: Series):\n        type_family = self.get_type_family(X.dtype)\n        # TODO: Check if low numeric numbers, could be categorical encoding!\n        # TODO: If low numeric, potentially it is just numeric instead of date\n        if X.isnull().all():\n            return False\n        if type_family == \'datetime\':\n            return True\n        if type_family != \'object\':  # TODO: seconds from epoch support\n            return False\n        try:\n            X.apply(pd.to_datetime)\n            return True\n        except:\n            return False\n\n    def check_if_nlp_feature(self, X: Series):\n        type_family = self.get_type_family(X.dtype)\n        if type_family != \'object\':\n            return False\n        X_unique = X.unique()\n        num_unique = len(X_unique)\n        num_rows = len(X)\n        unique_ratio = num_unique / num_rows\n        # print(X_unique)\n        if unique_ratio <= 0.01:\n            return False\n        avg_words = np.mean([len(re.sub(\' +\', \' \', value).split(\' \')) if isinstance(value, str) else 0 for value in X_unique])\n        # print(avg_words)\n        if avg_words < 3:\n            return False\n\n        return True\n\n    def generate_text_features(self, X: Series, feature: str) -> DataFrame:\n        X: DataFrame = X.to_frame(name=feature)\n        X[feature + \'.char_count\'] = [self.char_count(value) for value in X[feature]]\n        X[feature + \'.word_count\'] = [self.word_count(value) for value in X[feature]]\n        X[feature + \'.capital_ratio\'] = [self.capital_ratio(value) for value in X[feature]]\n        X[feature + \'.lower_ratio\'] = [self.lower_ratio(value) for value in X[feature]]\n        X[feature + \'.digit_ratio\'] = [self.digit_ratio(value) for value in X[feature]]\n        X[feature + \'.special_ratio\'] = [self.special_ratio(value) for value in X[feature]]\n\n        symbols = [\'!\', \'?\', \'@\', \'%\', \'$\', \'*\', \'&\', \'#\', \'^\', \'.\', \':\', \' \', \'/\', \';\', \'-\', \'=\']\n        for symbol in symbols:\n            X[feature + \'.symbol_count.\' + symbol] = [self.symbol_in_string_count(value, symbol) for value in X[feature]]\n            X[feature + \'.symbol_ratio.\' + symbol] = X[feature + \'.symbol_count.\' + symbol] / X[feature + \'.char_count\']\n            X[feature + \'.symbol_ratio.\' + symbol].fillna(0, inplace=True)\n\n        X = X.drop(feature, axis=1)\n\n        return X\n\n    def fix_categoricals_for_sklearn(self, X_features):\n        for column in self.features_categorical_final:\n            rank = X_features[column].value_counts().sort_values(ascending=True)\n            rank = rank[rank >= 3]\n            rank = rank.reset_index()\n            val_list = list(rank[\'index\'].values)\n            if len(val_list) <= 1:\n                self.features_to_remove_post.append(column)\n                self.features_categorical_final = [feature for feature in self.features_categorical_final if feature != column]\n                logger.debug(f\'Dropping {column}\')\n            else:\n                X_features[column] = X_features[column].astype(CategoricalDtype(categories=val_list))\n        return X_features\n\n    # TODO: add option for user to specify dtypes on load\n    @staticmethod\n    def get_type_family(type):\n        try:\n            if \'datetime\' in type.name:\n                return \'datetime\'\n            elif np.issubdtype(type, np.integer):\n                return \'int\'\n            elif np.issubdtype(type, np.floating):\n                return \'float\'\n        except Exception as err:\n            logger.exception(\'Warning: dtype %s is not recognized as a valid dtype by numpy! AutoGluon may incorrectly handle this feature...\')\n            logger.exception(err)\n\n        if type.name in [\'bool\', \'bool_\']:\n            return \'bool\'\n        elif type.name in [\'str\', \'string\', \'object\']:\n            return \'object\'\n        else:\n            return type.name\n\n    @staticmethod\n    def word_count(string):\n        return len(string.split())\n\n    @staticmethod\n    def char_count(string):\n        return len(string)\n\n    @staticmethod\n    def special_ratio(string):\n        string = string.replace(\' \', \'\')\n        if not string:\n            return 0\n        new_str = re.sub(r\'[\\w]+\', \'\', string)\n        return len(new_str) / len(string)\n\n    @staticmethod\n    def digit_ratio(string):\n        string = string.replace(\' \', \'\')\n        if not string:\n            return 0\n        return sum(c.isdigit() for c in string) / len(string)\n\n    @staticmethod\n    def lower_ratio(string):\n        string = string.replace(\' \', \'\')\n        if not string:\n            return 0\n        return sum(c.islower() for c in string) / len(string)\n\n    @staticmethod\n    def capital_ratio(string):\n        string = string.replace(\' \', \'\')\n        if not string:\n            return 0\n        return sum(1 for c in string if c.isupper()) / len(string)\n\n    @staticmethod\n    def symbol_in_string_count(string, character):\n        if not string:\n            return 0\n        return sum(1 for c in string if c == character)\n\n    # TODO: optimize by not considering columns with unique sums/means\n    # TODO: Multithread?\n    @staticmethod\n    def drop_duplicate_features(X):\n        X_without_dups = X.T.drop_duplicates().T\n        logger.debug(f""X_without_dups.shape: {X_without_dups.shape}"")\n\n        columns_orig = X.columns.values\n        columns_new = X_without_dups.columns.values\n        columns_removed = [column for column in columns_orig if column not in columns_new]\n\n        del X_without_dups\n\n        logger.log(15, \'Warning: duplicate columns removed \')\n        logger.log(15, columns_removed)\n        logger.log(15, f\'Removed {len(columns_removed)} duplicate columns before training models\')\n\n        return X[columns_new]\n\n    def save_self(self, path):\n        save_pkl.save(path=path, object=self)\n'"
autogluon/utils/tabular/features/add_datepart_helper.py,0,"b'import re\n\nimport numpy as np\nimport pandas as pd\nfrom pandas import DataFrame\n\n\ndef make_date(df: DataFrame, date_field: str):\n    """"""Make sure `df[field_name]` is of the right date type.""""""\n    field_dtype = df[date_field].dtype\n    if isinstance(field_dtype, pd.core.dtypes.dtypes.DatetimeTZDtype):\n        field_dtype = np.datetime64\n    if not np.issubdtype(field_dtype, np.datetime64):\n        df[date_field] = pd.to_datetime(df[date_field], infer_datetime_format=True)\n\n\ndef add_datepart(df: DataFrame, field_name: str, prefix: str = None, drop: bool = True, time: bool = False):\n    """"""Helper function that adds columns relevant to a date in the column `field_name` of `df`.""""""\n    make_date(df, field_name)\n    field = df[field_name]\n    prefix = ifnone(prefix, re.sub(\'[Dd]ate$\', \'\', field_name))\n    attr = [\n        \'Year\', \'Month\', \'Week\', \'Day\', \'Dayofweek\', \'Dayofyear\', \'Is_month_end\', \'Is_month_start\',\n        \'Is_quarter_end\', \'Is_quarter_start\', \'Is_year_end\', \'Is_year_start\'\n    ]\n    if time:\n        attr = attr + [\'Hour\', \'Minute\', \'Second\']\n    for n in attr:\n        df[prefix + n] = getattr(field.dt, n.lower())\n    df[prefix + \'Elapsed\'] = field.astype(np.int64) // 10 ** 9\n    if drop:\n        df.drop(field_name, axis=1, inplace=True)\n    return df\n\n\ndef ifnone(a, b):\n    """"""`a` if `a` is not None, otherwise `b`.""""""\n    return b if a is None else a\n'"
autogluon/utils/tabular/features/auto_ml_feature_generator.py,0,"b""import copy\nimport logging\nimport traceback\n\nimport numpy as np\nimport pandas as pd\nimport psutil\nfrom pandas import DataFrame\n\nfrom .abstract_feature_generator import AbstractFeatureGenerator\nfrom .vectorizers import get_ngram_freq, downscale_vectorizer\nfrom .vectorizers import vectorizer_auto_ml_default\n\nlogger = logging.getLogger(__name__)\n\n\nclass AutoMLFeatureGenerator(AbstractFeatureGenerator):\n    def __init__(self, enable_nlp_vectorizer_features=True, enable_nlp_ratio_features=True,\n                 enable_categorical_features=True, enable_raw_features=True, enable_datetime_features=True,\n                 vectorizer=None):\n        super().__init__()\n        self.enable_nlp_features = enable_nlp_vectorizer_features\n        self.enable_nlp_ratio_features = enable_nlp_ratio_features\n        self.enable_categorical_features = enable_categorical_features\n        self.enable_raw_features = enable_raw_features\n        self.enable_datetime_features = enable_datetime_features\n        if vectorizer is None:\n            self.vectorizer_default_raw = vectorizer_auto_ml_default()\n        else:\n            self.vectorizer_default_raw = vectorizer\n        self.vectorizers = []\n\n    # TODO: Parallelize with decorator!\n    def generate_features(self, X: DataFrame):\n        X_features = pd.DataFrame(index=X.index)\n        for column in X.columns:\n            if X[column].dtype.name == 'object':\n                X[column].fillna('', inplace=True)\n            else:\n                X[column].fillna(np.nan, inplace=True)\n\n        X_text_features_combined = []\n        if self.enable_nlp_ratio_features and self.features_nlp_ratio:\n            for nlp_feature in self.features_nlp_ratio:\n                X_text_features = self.generate_text_features(X[nlp_feature], nlp_feature)\n                if not self.fit:\n                    self.features_binned += list(X_text_features.columns)\n                X_text_features_combined.append(X_text_features)\n            X_text_features_combined = pd.concat(X_text_features_combined, axis=1)\n\n        X = self.preprocess(X)\n\n        if self.enable_raw_features and self.features_to_keep_raw:\n            X_features = X_features.join(X[self.features_to_keep_raw])\n\n        if self.enable_categorical_features and self.features_categorical:\n            X_categoricals = X[self.features_categorical]\n            # TODO: Add stateful categorical generator, merge rare cases to an unknown value\n            # TODO: What happens when training set has no unknown/rare values but test set does? What models can handle this?\n            X_categoricals = X_categoricals.astype('category')\n            X_features = X_features.join(X_categoricals)\n\n        if self.enable_nlp_ratio_features and self.features_nlp_ratio:\n            X_features = X_features.join(X_text_features_combined)\n\n        if self.enable_datetime_features and self.features_datetime:\n            for datetime_feature in self.features_datetime:\n                X_features[datetime_feature] = pd.to_datetime(X[datetime_feature])\n                X_features[datetime_feature] = pd.to_numeric(X_features[datetime_feature])  # TODO: Use actual date info\n                # TODO: Add fastai date features\n\n        if self.enable_nlp_features and self.features_nlp:\n            # Combine Text Fields\n            txt = ['. '.join(row) for row in X[self.features_nlp].values]\n\n            X['__nlp__'] = txt  # Could potentially find faster methods if this ends up being slow\n\n            features_nlp_current = ['__nlp__']\n\n            if not self.fit:\n                features_nlp_to_remove = []\n                logger.log(15, 'Fitting vectorizer for nlp features: ' + str(self.features_nlp))\n                for nlp_feature in features_nlp_current:\n                    # TODO: Preprocess text?\n                    # print('fitting vectorizer for', nlp_feature, '...')\n                    text_list = list(X[nlp_feature].drop_duplicates().values)\n                    vectorizer_raw = copy.deepcopy(self.vectorizer_default_raw)\n                    try:\n                        vectorizer_fit, _ = self.train_vectorizer(text_list, vectorizer_raw)\n                        self.vectorizers.append(vectorizer_fit)\n                    except ValueError:\n                        logger.debug('Removing nlp features due to error')\n                        features_nlp_to_remove = self.features_nlp\n\n                self.features_nlp = [feature for feature in self.features_nlp if feature not in features_nlp_to_remove]\n\n            X_features_cols_prior_to_nlp = list(X_features.columns)\n            downsample_ratio = None\n            nlp_failure_count = 0\n            keep_trying_nlp = True\n            while keep_trying_nlp:\n                try:\n                    X_nlp_features_combined = self.generate_nlp_ngrams(X=X, features_nlp_current=features_nlp_current, downsample_ratio=downsample_ratio)\n\n                    if self.features_nlp:\n                        X_features = X_features.join(X_nlp_features_combined)\n\n                    if not self.fit:\n                        self.features_vectorizers = self.features_vectorizers + list(X_nlp_features_combined.columns)\n                    keep_trying_nlp = False\n                except Exception as err:\n                    nlp_failure_count += 1\n                    if self.fit:\n                        logger.exception('Error: OOM error during NLP feature transform, unrecoverable. Increase memory allocation or reduce data size to avoid this error.')\n                        raise\n                    traceback.print_tb(err.__traceback__)\n\n                    X_features = X_features[X_features_cols_prior_to_nlp]\n                    skip_nlp = False\n                    for vectorizer in self.vectorizers:\n                        vocab_size = len(vectorizer.vocabulary_)\n                        if vocab_size <= 50:\n                            skip_nlp = True\n                            break\n                    else:\n                        if nlp_failure_count >= 3:\n                            skip_nlp = True\n\n                    if skip_nlp:\n                        logger.log(15, 'Warning: ngrams generation resulted in OOM error, removing ngrams features. If you want to use ngrams for this problem, increase memory allocation for AutoGluon.')\n                        logger.debug(str(err))\n                        self.vectorizers = []\n                        self.features_nlp = []\n                        self.features_vectorizers = []\n                        self.enable_nlp_features = False\n                        keep_trying_nlp = False\n                    else:\n                        logger.log(15, 'Warning: ngrams generation resulted in OOM error, attempting to reduce ngram feature count. If you want to optimally use ngrams for this problem, increase memory allocation for AutoGluon.')\n                        logger.debug(str(err))\n                        downsample_ratio = 0.25\n\n        return X_features\n\n    def generate_nlp_ngrams(self, X, features_nlp_current, downsample_ratio: int = None):\n        X_nlp_features_combined = []\n        for i, nlp_feature in enumerate(features_nlp_current):\n            vectorizer_fit = self.vectorizers[i]\n\n            transform_matrix = vectorizer_fit.transform(X[nlp_feature].values)\n\n            predicted_ngrams_memory_usage_bytes = len(X) * 8 * (transform_matrix.shape[1] + 1) + 80\n            mem_avail = psutil.virtual_memory().available\n            mem_rss = psutil.Process().memory_info().rss\n            # TODO: 0.25 causes OOM error with 72 GB ram on nyc-wendykan-lending-club-loan-data, fails on NN or Catboost, distributed.worker spams logs with memory warnings\n            # TODO: 0.20 causes OOM error with 64 GB ram on NN with several datasets. LightGBM and CatBoost succeed\n            max_memory_percentage = 0.15  # TODO: Finetune this, or find a better metric\n            predicted_rss = mem_rss + predicted_ngrams_memory_usage_bytes\n            predicted_percentage = predicted_rss / mem_avail\n            if not self.fit:\n                if downsample_ratio is None:\n                    if predicted_percentage > max_memory_percentage:\n                        downsample_ratio = max_memory_percentage / predicted_percentage\n                        logger.log(15, 'Warning: Due to memory constraints, ngram feature count is being reduced. Allocate more memory to maximize model quality.')\n\n                if downsample_ratio is not None:\n                    if (downsample_ratio >= 1) or (downsample_ratio <= 0):\n                        raise ValueError(f'downsample_ratio must be >0 and <1, but downsample_ratio is {downsample_ratio}')\n                    vocab_size = len(vectorizer_fit.vocabulary_)\n                    downsampled_vocab_size = int(np.floor(vocab_size * downsample_ratio))\n                    logger.debug(f'Reducing Vectorizer vocab size from {vocab_size} to {downsampled_vocab_size} to avoid OOM error')\n                    ngram_freq = get_ngram_freq(vectorizer=vectorizer_fit, transform_matrix=transform_matrix)\n                    downscale_vectorizer(vectorizer=vectorizer_fit, ngram_freq=ngram_freq, vocab_size=downsampled_vocab_size)\n                    # TODO: This doesn't have to be done twice, can update transform matrix based on new vocab instead of calling .transform\n                    #  If we have this functionality, simply update transform_matrix each time OOM occurs instead of re-calling .transform\n                    transform_matrix = vectorizer_fit.transform(X[nlp_feature].values)\n\n            nlp_features_names = vectorizer_fit.get_feature_names()\n\n            X_nlp_features = pd.DataFrame(transform_matrix.toarray())  # FIXME\n            X_nlp_features.columns = [f'{nlp_feature}.{x}' for x in nlp_features_names]\n            X_nlp_features[nlp_feature + '._total_'] = X_nlp_features.gt(0).sum(axis=1)\n\n            X_nlp_features_combined.append(X_nlp_features)\n\n        if self.features_nlp:\n            X_nlp_features_combined = pd.concat(X_nlp_features_combined, axis=1)\n\n        return X_nlp_features_combined\n"""
autogluon/utils/tabular/features/vectorizers.py,0,"b'from collections import Counter\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n\ndef vectorizer_auto_ml_default():\n    return CountVectorizer(min_df=30, ngram_range=(1, 3), max_features=30000)\n\n\ndef get_ngram_freq(vectorizer, transform_matrix):\n    names = vectorizer.get_feature_names()\n    frequencies = transform_matrix.sum(axis=0).tolist()[0]\n    ngram_freq = {ngram: freq for ngram, freq in zip(names, frequencies)}\n    return ngram_freq\n\n\n# Reduces vectorizer vocabulary size to vocab_size, keeping highest frequency ngrams\ndef downscale_vectorizer(vectorizer, ngram_freq, vocab_size):\n    counter = Counter(ngram_freq)\n    top_n = counter.most_common(vocab_size)\n    top_n_names = sorted([name for name, _ in top_n])\n    new_vocab = {name: i for i, name in enumerate(top_n_names)}\n    vectorizer.vocabulary_ = new_vocab\n'"
autogluon/utils/tabular/metrics/__init__.py,0,"b'import copy\nfrom abc import ABCMeta, abstractmethod\nfrom functools import partial\n\nimport numpy as np\nimport sklearn.metrics\nfrom sklearn.utils.multiclass import type_of_target\n\nfrom . import classification_metrics, softclass_metrics\nfrom .util import sanitize_array\nfrom ..ml.constants import PROBLEM_TYPES, PROBLEM_TYPES_REGRESSION, PROBLEM_TYPES_CLASSIFICATION\nfrom ...miscs import warning_filter\n\n\nclass Scorer(object, metaclass=ABCMeta):\n    def __init__(self, name, score_func, optimum, sign, kwargs):\n        self.name = name\n        self._kwargs = kwargs\n        self._score_func = score_func\n        self._optimum = optimum\n        self._sign = sign\n\n    @abstractmethod\n    def __call__(self, y_true, y_pred, sample_weight=None):\n        pass\n\n    def __repr__(self):\n        return self.name\n\n    def sklearn_scorer(self):\n        if isinstance(self, _ProbaScorer):\n            needs_proba = True\n            needs_threshold = False\n        elif isinstance(self, _ThresholdScorer):\n            needs_proba = False\n            needs_threshold = True\n        else:\n            needs_proba = False\n            needs_threshold = False\n\n        with warning_filter():\n            ret = sklearn.metrics.scorer.make_scorer(score_func=self, greater_is_better=True, needs_proba=needs_proba, needs_threshold=needs_threshold)\n        return ret\n\n\nclass _PredictScorer(Scorer):\n    def __call__(self, y_true, y_pred, sample_weight=None):\n        """"""Evaluate predicted target values for X relative to y_true.\n\n        Parameters\n        ----------\n        y_true : array-like\n            Gold standard target values for X.\n\n        y_pred : array-like, [n_samples x n_classes]\n            Model predictions\n\n        sample_weight : array-like, optional (default=None)\n            Sample weights.\n\n        Returns\n        -------\n        score : float\n            Score function applied to prediction of estimator on X.\n        """"""\n\n        if isinstance(y_true, list):\n            y_true = np.array(y_true)\n        if isinstance(y_pred, list):\n            y_pred = np.array(y_pred)\n        type_true = type_of_target(y_true)\n\n        if len(y_pred.shape) == 1 or y_pred.shape[1] == 1 or type_true == \'continuous\':\n            pass  # must be regression, all other task types would return at least two probabilities\n        elif type_true in [\'binary\', \'multiclass\']:\n            y_pred = np.argmax(y_pred, axis=1)\n        elif type_true == \'multilabel-indicator\':\n            y_pred[y_pred > 0.5] = 1.0\n            y_pred[y_pred <= 0.5] = 0.0\n        else:\n            raise ValueError(type_true)\n\n        if sample_weight is not None:\n            return self._sign * self._score_func(y_true, y_pred,\n                                                 sample_weight=sample_weight,\n                                                 **self._kwargs)\n        else:\n            return self._sign * self._score_func(y_true, y_pred,\n                                                 **self._kwargs)\n\n\nclass _ProbaScorer(Scorer):\n    def __call__(self, y_true, y_pred, sample_weight=None):\n        """"""Evaluate predicted probabilities for X relative to y_true.\n        Parameters\n        ----------\n        y_true : array-like\n            Gold standard target values for X. These must be class labels,\n            not probabilities.\n\n        y_pred : array-like, [n_samples x n_classes]\n            Model predictions\n\n        sample_weight : array-like, optional (default=None)\n            Sample weights.\n\n        Returns\n        -------\n        score : float\n            Score function applied to prediction of estimator on X.\n        """"""\n        if sample_weight is not None:\n            return self._sign * self._score_func(y_true, y_pred,\n                                                 sample_weight=sample_weight,\n                                                 **self._kwargs)\n        else:\n            return self._sign * self._score_func(y_true, y_pred, **self._kwargs)\n\n\nclass _ThresholdScorer(Scorer):\n    def __call__(self, y_true, y_pred, sample_weight=None):\n        """"""Evaluate decision function output for X relative to y_true.\n        Parameters\n        ----------\n        y_true : array-like\n            Gold standard target values for X. These must be class labels,\n            not probabilities.\n\n        y_pred : array-like, [n_samples x n_classes]\n            Model predictions\n\n        sample_weight : array-like, optional (default=None)\n            Sample weights.\n\n        Returns\n        -------\n        score : float\n            Score function applied to prediction of estimator on X.\n        """"""\n        if isinstance(y_true, list):\n            y_true = np.array(y_true)\n        if isinstance(y_pred, list):\n            y_pred = np.array(y_pred)\n        y_type = type_of_target(y_true)\n        if y_type not in (""binary"", ""multilabel-indicator""):\n            raise ValueError(""{0} format is not supported"".format(y_type))\n\n        if y_type == ""binary"":\n            pass\n            # y_pred = y_pred[:, 1]\n        elif isinstance(y_pred, list):\n            y_pred = np.vstack([p[:, -1] for p in y_pred]).T\n\n        if sample_weight is not None:\n            return self._sign * self._score_func(y_true, y_pred,\n                                                 sample_weight=sample_weight,\n                                                 **self._kwargs)\n        else:\n            return self._sign * self._score_func(y_true, y_pred, **self._kwargs)\n\n\ndef scorer_expects_y_pred(scorer: Scorer):\n    if isinstance(scorer, _ProbaScorer):\n        return False\n    elif isinstance(scorer, _ThresholdScorer):\n        return False\n    else:\n        return True\n\n\ndef make_scorer(name, score_func, optimum=1, greater_is_better=True,\n                needs_proba=False, needs_threshold=False, **kwargs):\n    """"""Make a scorer from a performance metric or loss function.\n\n    Factory inspired by scikit-learn which wraps scikit-learn scoring functions\n    to be used in auto-sklearn.\n\n    Parameters\n    ----------\n    score_func : callable\n        Score function (or loss function) with signature\n        ``score_func(y, y_pred, **kwargs)``.\n\n    optimum : int or float, default=1\n        The best score achievable by the score function, i.e. maximum in case of\n        scorer function and minimum in case of loss function.\n\n    greater_is_better : boolean, default=True\n        Whether score_func is a score function (default), meaning high is good,\n        or a loss function, meaning low is good. In the latter case, the\n        scorer object will sign-flip the outcome of the score_func.\n\n    needs_proba : boolean, default=False\n        Whether score_func requires predict_proba to get probability estimates\n        out of a classifier.\n\n    needs_threshold : boolean, default=False\n        Whether score_func takes a continuous decision certainty.\n        This only works for binary classification.\n\n    **kwargs : additional arguments\n        Additional parameters to be passed to score_func.\n\n    Returns\n    -------\n    scorer : callable\n        Callable object that returns a scalar score; greater is better.\n    """"""\n    sign = 1 if greater_is_better else -1\n    if needs_proba:\n        cls = _ProbaScorer\n    elif needs_threshold:\n        cls = _ThresholdScorer\n    else:\n        cls = _PredictScorer\n    return cls(name, score_func, optimum, sign, kwargs)\n\n\n# Standard regression scores\nr2 = make_scorer(\'r2\',\n                 sklearn.metrics.r2_score)\nmean_squared_error = make_scorer(\'mean_squared_error\',\n                                 sklearn.metrics.mean_squared_error,\n                                 optimum=0,\n                                 greater_is_better=False)\nmean_absolute_error = make_scorer(\'mean_absolute_error\',\n                                  sklearn.metrics.mean_absolute_error,\n                                  optimum=0,\n                                  greater_is_better=False)\nmedian_absolute_error = make_scorer(\'median_absolute_error\',\n                                    sklearn.metrics.median_absolute_error,\n                                    optimum=0,\n                                    greater_is_better=False)\n\n\ndef rmse_func(predictions, targets):\n    return np.sqrt(((predictions - targets) ** 2).mean())\n\n\nroot_mean_squared_error = make_scorer(\'root_mean_squared_error\',\n                                      rmse_func,\n                                      optimum=0,\n                                      greater_is_better=False)\n\n# Standard Classification Scores\naccuracy = make_scorer(\'accuracy\',\n                       sklearn.metrics.accuracy_score)\nbalanced_accuracy = make_scorer(\'balanced_accuracy\',\n                                classification_metrics.balanced_accuracy)\nf1 = make_scorer(\'f1\',\n                 sklearn.metrics.f1_score)\n\n# Score functions that need decision values\nroc_auc = make_scorer(\'roc_auc\',\n                      sklearn.metrics.roc_auc_score,\n                      greater_is_better=True,\n                      needs_threshold=True)\naverage_precision = make_scorer(\'average_precision\',\n                                sklearn.metrics.average_precision_score,\n                                needs_threshold=True)\nprecision = make_scorer(\'precision\',\n                        sklearn.metrics.precision_score)\nrecall = make_scorer(\'recall\',\n                     sklearn.metrics.recall_score)\n\n# Score function for probabilistic classification\nlog_loss = make_scorer(\'log_loss\',\n                       sklearn.metrics.log_loss,\n                       optimum=0,\n                       greater_is_better=False,\n                       needs_proba=True)\npac_score = make_scorer(\'pac_score\',\n                        classification_metrics.pac_score,\n                        greater_is_better=True,\n                        needs_proba=True)\n\n# Score for soft-classisification (with soft, probalistic labels):\nsoft_log_loss = make_scorer(\'soft_log_loss\', softclass_metrics.soft_log_loss,\n                            greater_is_better=False, needs_proba=True)\n\n# TODO what about mathews correlation coefficient etc?\n\n\nREGRESSION_METRICS = {\n    scorer.name: scorer\n    for scorer in [r2, mean_squared_error, root_mean_squared_error, mean_absolute_error, median_absolute_error]\n}\n\nCLASSIFICATION_METRICS = {\n    scorer.name: scorer\n    for scorer in [accuracy, balanced_accuracy, roc_auc, average_precision, log_loss, pac_score]\n}\n\nfor name, metric in [(\'precision\', sklearn.metrics.precision_score),\n                     (\'recall\', sklearn.metrics.recall_score),\n                     (\'f1\', sklearn.metrics.f1_score)]:\n    globals()[name] = make_scorer(name, metric)\n    CLASSIFICATION_METRICS[name] = globals()[name]\n    for average in [\'macro\', \'micro\', \'samples\', \'weighted\']:\n        qualified_name = \'{0}_{1}\'.format(name, average)\n        globals()[qualified_name] = make_scorer(qualified_name,\n                                                partial(metric, pos_label=None, average=average))\n        CLASSIFICATION_METRICS[qualified_name] = globals()[qualified_name]\n\n\ndef calculate_score(solution, prediction, task_type, metric,\n                    all_scoring_functions=False):\n    if task_type not in PROBLEM_TYPES:\n        raise NotImplementedError(task_type)\n\n    if all_scoring_functions:\n        score = dict()\n        if task_type in PROBLEM_TYPES_REGRESSION:\n            # TODO put this into the regression metric itself\n            cprediction = sanitize_array(prediction)\n            metric_dict = copy.copy(REGRESSION_METRICS)\n            metric_dict[metric.name] = metric\n            for metric_ in REGRESSION_METRICS:\n                func = REGRESSION_METRICS[metric_]\n                score[func.name] = func(solution, cprediction)\n\n        else:\n            metric_dict = copy.copy(CLASSIFICATION_METRICS)\n            metric_dict[metric.name] = metric\n            for metric_ in metric_dict:\n                func = CLASSIFICATION_METRICS[metric_]\n\n                # TODO maybe annotate metrics to define which cases they can\n                # handle?\n\n                try:\n                    score[func.name] = func(solution, prediction)\n                except ValueError as e:\n                    if e.args[0] == \'multiclass format is not supported\':\n                        continue\n                    elif e.args[0] == ""Samplewise metrics are not available ""\\\n                            ""outside of multilabel classification."":\n                        continue\n                    elif e.args[0] == ""Target is multiclass but ""\\\n                            ""average=\'binary\'. Please choose another average ""\\\n                            ""setting, one of [None, \'micro\', \'macro\', \'weighted\']."":\n                        continue\n                    else:\n                        raise e\n\n    else:\n        if task_type in PROBLEM_TYPES_REGRESSION:\n            # TODO put this into the regression metric itself\n            cprediction = sanitize_array(prediction)\n            score = metric(solution, cprediction)\n        else:\n            score = metric(solution, prediction)\n\n    return score\n\n\ndef get_metric(metric, problem_type, metric_type):\n    """"""Returns metric function by using its name if the metric is str.\n    Performs basic check for metric compatibility with given problem type.""""""\n    if metric is not None and isinstance(metric, str):\n        if metric in CLASSIFICATION_METRICS:\n            if problem_type is not None and problem_type not in PROBLEM_TYPES_CLASSIFICATION:\n                raise ValueError(f""{metric_type}={metric} can only be used for classification problems"")\n            return CLASSIFICATION_METRICS[metric]\n        elif metric in REGRESSION_METRICS:\n            if problem_type is not None and problem_type not in PROBLEM_TYPES_REGRESSION:\n                raise ValueError(f""{metric_type}={metric} can only be used for regression problems"")\n            return REGRESSION_METRICS[metric]\n        elif metric == \'soft_log_loss\':\n            return soft_log_loss\n        else:\n            raise ValueError(\n                f""{metric} is an unknown metric, see utils/tabular/metrics/ for available options ""\n                f""or how to define your own {metric_type} function""\n            )\n    else:\n        return metric\n'"
autogluon/utils/tabular/metrics/classification_metrics.py,0,"b'import logging\n\nimport numpy as np\nimport pandas as pd\ntry:\n    from sklearn.metrics._classification import _check_targets, type_of_target\nexcept:\n    from sklearn.metrics.classification import _check_targets, type_of_target\n\nlogger = logging.getLogger(__name__)\n\n\ndef balanced_accuracy(solution, prediction):\n    y_type, solution, prediction = _check_targets(solution, prediction)\n\n    if y_type not in [""binary"", ""multiclass"", \'multilabel-indicator\']:\n        raise ValueError(f""{y_type} is not supported"")\n\n    if y_type == \'binary\':\n        # Do not transform into any multiclass representation\n        pass\n\n    elif y_type == \'multiclass\':\n        n = len(solution)\n        unique_sol, encoded_sol = np.unique(solution, return_inverse=True)\n        unique_pred, encoded_pred = np.unique(prediction, return_inverse=True)\n        classes = np.unique(np.concatenate((unique_sol, unique_pred)))\n        map_sol = np.array([np.where(classes==c)[0][0] for c in unique_sol])\n        map_pred = np.array([np.where(classes==c)[0][0] for c in unique_pred])\n        # one hot encoding\n        sol_ohe = np.zeros((n, len(classes)))\n        pred_ohe = np.zeros((n, len(classes)))\n        sol_ohe[np.arange(n), map_sol[encoded_sol]] = 1\n        pred_ohe[np.arange(n), map_pred[encoded_pred]] = 1\n        solution = sol_ohe\n        prediction = pred_ohe\n\n    elif y_type == \'multilabel-indicator\':\n        solution = solution.toarray()\n        prediction = prediction.toarray()\n    else:\n        raise NotImplementedError(f\'bac_metric does not support task type {y_type}\')\n\n    fn = np.sum(np.multiply(solution, (1 - prediction)), axis=0, dtype=float)\n    tp = np.sum(np.multiply(solution, prediction), axis=0, dtype=float)\n    # Bounding to avoid division by 0\n    eps = 1e-15\n    tp = np.maximum(eps, tp)\n    pos_num = np.maximum(eps, tp + fn)\n    tpr = tp / pos_num  # true positive rate (sensitivity)\n\n    if y_type in (\'binary\', \'multilabel-indicator\'):\n        tn = np.sum(\n            np.multiply((1 - solution), (1 - prediction)),\n            axis=0, dtype=float\n        )\n        fp = np.sum(\n            np.multiply((1 - solution), prediction),\n            axis=0, dtype=float\n        )\n        tn = np.maximum(eps, tn)\n        neg_num = np.maximum(eps, tn + fp)\n        tnr = tn / neg_num  # true negative rate (specificity)\n        bac = 0.5 * (tpr + tnr)\n    elif y_type == \'multiclass\':\n        bac = tpr\n    else:\n        raise ValueError(y_type)\n\n    return np.mean(bac)  # average over all classes\n\n\ndef pac_score(solution, prediction):\n    """"""\n    Probabilistic Accuracy based on log_loss metric.\n    We assume the solution is in {0, 1} and prediction in [0, 1].\n    Otherwise, run normalize_array.\n    :param solution:\n    :param prediction:\n    :param task:\n    :return:\n    """"""\n\n    def normalize_array(solution, prediction):\n        """"""\n        Use min and max of solution as scaling factors to normalize prediction,\n        then threshold it to [0, 1].\n        Binarize solution to {0, 1}. This allows applying classification\n        scores to all cases. In principle, this should not do anything to\n        properly formatted classification inputs and outputs.\n        :param solution:\n        :param prediction:\n        :return:\n        """"""\n        # Binarize solution\n        sol = np.ravel(solution)  # convert to 1-d array\n        maxi = np.nanmax(sol[np.isfinite(sol)])\n        mini = np.nanmin(sol[np.isfinite(sol)])\n        if maxi == mini:\n            logger.debug(\'Warning: cannot normalize array\')\n            return [solution, prediction]\n        diff = maxi - mini\n        mid = (maxi + mini) / 2.\n\n        solution[solution >= mid] = 1\n        solution[solution < mid] = 0\n        # Normalize and threshold predictions (takes effect only if solution not\n        # in {0, 1})\n\n        prediction -= float(mini)\n        prediction /= float(diff)\n\n        # and if predictions exceed the bounds [0, 1]\n        prediction[prediction > 1] = 1\n        prediction[prediction < 0] = 0\n        # Make probabilities smoother\n        # new_prediction = np.power(new_prediction, (1./10))\n\n        return [solution, prediction]\n\n    def log_loss(solution, prediction, task):\n        """"""Log loss for binary and multiclass.""""""\n        [sample_num, label_num] = solution.shape\n        # Lower gives problems with float32!\n        eps = 0.00000003\n\n        if (task == \'multiclass\') and (label_num > 1):\n            # Make sure the lines add up to one for multi-class classification\n            norma = np.sum(prediction, axis=1)\n            for k in range(sample_num):\n                prediction[k, :] /= np.maximum(norma[k], eps)\n\n            sample_num = solution.shape[0]\n            for i in range(sample_num):\n                j = np.argmax(solution[i, :])\n                solution[i, :] = 0\n                solution[i, j] = 1\n\n            solution = solution.astype(np.int32, copy=False)\n            # For the base prediction, this solution is ridiculous in the\n            # multi-label case\n\n            # Bounding of predictions to avoid log(0),1/0,...\n        prediction = np.minimum(1 - eps, np.maximum(eps, prediction))\n        # Compute the log loss\n        pos_class_log_loss = -np.mean(solution * np.log(prediction), axis=0)\n        if (task != \'multiclass\') or (label_num == 1):\n            # The multi-label case is a bunch of binary problems.\n            # The second class is the negative class for each column.\n            neg_class_log_loss = -np.mean(\n                (1 - solution) * np.log(1 - prediction),\n                axis=0\n            )\n            log_loss = pos_class_log_loss + neg_class_log_loss\n            # Each column is an independent problem, so we average.\n            # The probabilities in one line do not add up to one.\n            # log_loss = mvmean(log_loss)\n            # print(\'binary {}\'.format(log_loss))\n            # In the multilabel case, the right thing i to AVERAGE not sum\n            # We return all the scores so we can normalize correctly later on\n        else:\n            # For the multiclass case the probabilities in one line add up one.\n            log_loss = pos_class_log_loss\n            # We sum the contributions of the columns.\n            log_loss = np.sum(log_loss)\n            # print(\'multiclass {}\'.format(log_loss))\n        return log_loss\n\n    def prior_log_loss(frac_pos, task):\n        """"""Baseline log loss.\n        For multiple classes ot labels return the values for each column\n        """"""\n        eps = 1e-15\n        frac_pos_ = np.maximum(eps, frac_pos)\n        if task != \'multiclass\':  # binary case\n            frac_neg = 1 - frac_pos\n            frac_neg_ = np.maximum(eps, frac_neg)\n            pos_class_log_loss_ = -frac_pos * np.log(frac_pos_)\n            neg_class_log_loss_ = -frac_neg * np.log(frac_neg_)\n            base_log_loss = pos_class_log_loss_ + neg_class_log_loss_\n            # base_log_loss = mvmean(base_log_loss)\n            # print(\'binary {}\'.format(base_log_loss))\n            # In the multilabel case, the right thing i to AVERAGE not sum\n            # We return all the scores so we can normalize correctly later on\n        else:  # multiclass case\n            fp = frac_pos_ / sum(frac_pos_)  # Need to renormalize the lines in multiclass case\n            # Only ONE label is 1 in the multiclass case active for each line\n            pos_class_log_loss_ = -frac_pos * np.log(fp)\n            base_log_loss = np.sum(pos_class_log_loss_)\n        return base_log_loss\n\n    y_type = type_of_target(solution)\n\n    if isinstance(solution, pd.Series):\n        solution = solution.values\n    if isinstance(prediction, pd.Series):\n        prediction = prediction.values\n\n    if y_type == \'binary\':\n        if len(solution.shape) == 1:\n            solution = solution.reshape((-1, 1))\n        if len(prediction.shape) == 1:\n            prediction = prediction.reshape((-1, 1))\n        if len(prediction.shape) == 2:\n            if prediction.shape[1] > 2:\n                raise ValueError(f\'A prediction array with probability values \'\n                                 f\'for {prediction.shape[1]} classes is not a binary \'\n                                 f\'classification problem\')\n            # Prediction will be copied into a new binary array - no copy\n            prediction = prediction.reshape((-1, 1))\n        else:\n            raise ValueError(f\'Invalid prediction shape {prediction.shape}\')\n\n    elif y_type == \'multiclass\':\n        if len(solution.shape) == 2:\n            if solution.shape[1] > 1:\n                raise ValueError(f\'Solution array must only contain one class \'\n                                 f\'label, but contains {solution.shape[1]}\')\n        elif len(solution.shape) == 1:\n            pass\n        else:\n            raise ValueError(\'Solution.shape %s\' % solution.shape)\n\n        # Need to create a multiclass solution and a multiclass predictions\n        max_class = prediction.shape[1] - 1\n        solution_binary = np.zeros((len(solution), max_class + 1))\n        for i in range(len(solution)):\n            solution_binary[i, int(solution[i])] = 1\n        solution = solution_binary\n\n    elif y_type == \'multilabel-indicator\':\n        solution = solution.copy()\n\n    else:\n        raise NotImplementedError(f\'pac_score does not support task {y_type}\')\n\n    solution, prediction = normalize_array(solution, prediction.copy())\n\n    sample_num, _ = solution.shape\n\n    eps = 1e-7\n    # Compute the base log loss (using the prior probabilities)\n    pos_num = 1. * np.sum(solution, axis=0, dtype=float)  # float conversion!\n    frac_pos = pos_num / sample_num  # prior proba of positive class\n    the_base_log_loss = prior_log_loss(frac_pos, y_type)\n    the_log_loss = log_loss(solution, prediction, y_type)\n\n    # Exponentiate to turn into an accuracy-like score.\n    # In the multi-label case, we need to average AFTER taking the exp\n    # because it is an NL operation\n    pac = np.mean(np.exp(-the_log_loss))\n    base_pac = np.mean(np.exp(-the_base_log_loss))\n    # Normalize: 0 for random, 1 for perfect\n    score = (pac - base_pac) / np.maximum(eps, (1 - base_pac))\n\n    return score\n'"
autogluon/utils/tabular/metrics/softclass_metrics.py,0,"b'"""""" Metrics for classification with soft (probabilistic) labels """"""\n\nimport logging\n\nimport mxnet as mx\nimport numpy as np\n\nlogger = logging.getLogger(__name__)\n\nEPS = 1e-10  # clipping threshold to prevent NaN\n\n# assumes predictions are already log-probabilities.\nsoftloss = mx.gluon.loss.SoftmaxCrossEntropyLoss(sparse_label=False, from_logits=True)\n\n\ndef soft_log_loss(true_probs, predicted_probs):\n    """""" Both args must be 2D pandas/numpy arrays """"""\n    true_probs = np.array(true_probs)\n    predicted_probs = np.array(predicted_probs)\n    if len(true_probs.shape) != 2 or len(predicted_probs.shape) != 2:\n        raise ValueError(""both truth and prediction must be 2D numpy arrays"")\n    if true_probs.shape != predicted_probs.shape:\n        raise ValueError(""truth and prediction must be 2D numpy arrays with the same shape"")\n\n    # true_probs = np.clip(true_probs, a_min=EPS, a_max=None)\n    predicted_probs = np.clip(predicted_probs, a_min=EPS, a_max=None)  # clip 0s to avoid NaN\n    true_probs = true_probs / true_probs.sum(axis=1, keepdims=1)  # renormalize\n    predicted_probs = predicted_probs / predicted_probs.sum(axis=1, keepdims=1)\n    losses = softloss(mx.nd.log(mx.nd.array(predicted_probs)), mx.nd.array(true_probs))\n    return mx.nd.mean(losses).asscalar()\n'"
autogluon/utils/tabular/metrics/util.py,0,"b'import numpy as np\n\n\ndef sanitize_array(array):\n    """"""\n    Replace NaN and Inf (there should not be any!)\n    :param array:\n    :return:\n    """"""\n    a = np.ravel(array)\n    maxi = np.nanmax(a[np.isfinite(a)])\n    mini = np.nanmin(a[np.isfinite(a)])\n    array[array == float(\'inf\')] = maxi\n    array[array == float(\'-inf\')] = mini\n    mid = (maxi + mini) / 2\n    array[np.isnan(array)] = mid\n    return array\n'"
autogluon/utils/tabular/ml/__init__.py,0,b''
autogluon/utils/tabular/ml/constants.py,0,"b'# Do not change these!\nBINARY = \'binary\'\nMULTICLASS = \'multiclass\'\nREGRESSION = \'regression\'\nSOFTCLASS = \'softclass\' # classification with soft-target (rather than classes, labels are probabilities of each class).\n\nPROBLEM_TYPES_CLASSIFICATION = [BINARY, MULTICLASS]\nPROBLEM_TYPES_REGRESSION = [REGRESSION]\nPROBLEM_TYPES = PROBLEM_TYPES_CLASSIFICATION + PROBLEM_TYPES_REGRESSION + [SOFTCLASS]\n\nREFIT_FULL_NAME = \'refit_single_full\'  # stack-name used for refit_single_full (aka ""compressed"") models\nREFIT_FULL_SUFFIX = ""_FULL""  # suffix appended to model name for refit_single_full (aka ""compressed"") models\n\n# AG_ARGS variables are key names in model hyperparameters to dictionaries of custom AutoGluon arguments.\nAG_ARGS = \'_ag_args\'  # Contains arguments to control model name, model priority, and the valid configurations which it can be used in.\nAG_ARGS_FIT = \'_ag_args_fit\'  # Contains arguments that impact model training, such as early stopping rounds, #cores, #gpus, max time limit, max memory usage  # TODO\n'"
autogluon/utils/tabular/ml/utils.py,0,"b'import logging\nimport multiprocessing\nimport os\nfrom collections import defaultdict\nfrom datetime import datetime\n\nimport numpy as np\nimport pandas as pd\nfrom pandas import DataFrame, Series\nfrom sklearn.model_selection import KFold, StratifiedKFold, RepeatedKFold, RepeatedStratifiedKFold, train_test_split\n\nfrom .constants import BINARY, REGRESSION, SOFTCLASS\n\nlogger = logging.getLogger(__name__)\n\n\ndef get_pred_from_proba(y_pred_proba, problem_type=BINARY):\n    if problem_type == BINARY:\n        y_pred = [1 if pred >= 0.5 else 0 for pred in y_pred_proba]\n    elif problem_type == REGRESSION:\n        y_pred = y_pred_proba\n    else:\n        y_pred = np.argmax(y_pred_proba, axis=1)\n    return y_pred\n\n\ndef generate_kfold(X, y=None, n_splits=5, random_state=0, stratified=False, n_repeats=1):\n    if stratified and (y is not None):\n        if n_repeats > 1:\n            kf = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=random_state)\n        else:\n            kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n\n        kf.get_n_splits(X, y)\n        return [[train_index, test_index] for train_index, test_index in kf.split(X, y)]\n    else:\n        if n_repeats > 1:\n            kf = RepeatedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=random_state)\n        else:\n            kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n\n        kf.get_n_splits(X)\n        return [[train_index, test_index] for train_index, test_index in kf.split(X)]\n\n\ndef generate_train_test_split(X: DataFrame, y: Series, problem_type: str, test_size: float = 0.1, random_state=0) -> (DataFrame, DataFrame, Series, Series):\n    if (test_size <= 0.0) or (test_size >= 1.0):\n        raise ValueError(""fraction of data to hold-out must be specified between 0 and 1"")\n\n    if problem_type in [REGRESSION, SOFTCLASS]:\n        stratify = None\n    else:\n        stratify = y\n\n    # TODO: Enable stratified split when y class would result in 0 samples in test.\n    #  One approach: extract low frequency classes from X/y, add back (1-test_size)% to X_train, y_train, rest to X_test\n    #  Essentially stratify the high frequency classes, random the low frequency (While ensuring at least 1 example stays for each low frequency in train!)\n    #  Alternatively, don\'t test low frequency at all, trust it to work in train set. Risky, but highest quality for predictions.\n    X_train, X_test, y_train, y_test = train_test_split(X, y.values, test_size=test_size, shuffle=True, random_state=random_state, stratify=stratify)\n    if problem_type != SOFTCLASS:\n        y_train = pd.Series(y_train, index=X_train.index)\n        y_test = pd.Series(y_test, index=X_test.index)\n    else:\n        y_train = pd.DataFrame(y_train, index=X_train.index)\n        y_test = pd.DataFrame(y_test, index=X_test.index)\n    return X_train, X_test, y_train, y_test\n\n\ndef convert_categorical_to_int(X):\n    X = X.copy()\n    cat_columns = X.select_dtypes(include=[\'category\']).columns\n    X[cat_columns] = X[cat_columns].apply(lambda x: x.cat.codes)\n    return X\n\n\ndef setup_outputdir(output_directory):\n    if output_directory is None:\n        utcnow = datetime.utcnow()\n        timestamp = utcnow.strftime(""%Y%m%d_%H%M%S"")\n        output_directory = f""AutogluonModels/ag-{timestamp}{os.path.sep}""\n        os.makedirs(output_directory)\n        logger.log(25, f""No output_directory specified. Models will be saved in: {output_directory}"")\n    output_directory = os.path.expanduser(output_directory)  # replace ~ with absolute path if it exists\n    if output_directory[-1] != os.path.sep:\n        output_directory = output_directory + os.path.sep\n    return output_directory\n\n\ndef setup_compute(nthreads_per_trial, ngpus_per_trial):\n    if nthreads_per_trial is None:\n        nthreads_per_trial = multiprocessing.cpu_count()  # Use all of processing power / trial by default. To use just half: # int(np.floor(multiprocessing.cpu_count()/2))\n\n    if ngpus_per_trial is None:\n        ngpus_per_trial = 0  # do not use GPU by default\n    elif ngpus_per_trial > 1:\n        ngpus_per_trial = 1\n        logger.debug(""tabular_prediction currently doesn\'t use >1 GPU per training run. ngpus_per_trial set = 1"")\n    return nthreads_per_trial, ngpus_per_trial\n\n\ndef setup_trial_limits(time_limits, num_trials, hyperparameters={\'NN\': None}):\n    """""" Adjust default time limits / num_trials """"""\n    if num_trials is None:\n        if time_limits is None:\n            time_limits = 10 * 60  # run for 10min by default\n        time_limits /= float(len(hyperparameters))  # each model type gets half the available time\n        num_trials = 1000  # run up to 1000 trials (or as you can within the given time_limits)\n    elif time_limits is None:\n        time_limits = int(1e6)  # user only specified num_trials, so run all of them regardless of time-limits\n    else:\n        time_limits /= float(len(hyperparameters))  # each model type gets half the available time\n\n    if time_limits <= 10:  # threshold = 10sec, ie. too little time to run >1 trial.\n        num_trials = 1\n    time_limits *= 0.9  # reduce slightly to account for extra time overhead\n    return time_limits, num_trials\n\n\ndef dd_list():\n    return defaultdict(list)\n\n\ndef get_leaderboard_pareto_frontier(leaderboard: DataFrame, score_col=\'score_val\', inference_time_col=\'pred_time_val_full\') -> DataFrame:\n    """"""\n    Given a set of models, returns in ranked order from best score to worst score models which satisfy the criteria:\n    1. No other model in the set has both a lower inference time and a better or equal score.\n\n    :param leaderboard: Leaderboard DataFrame of model info containing score_col and inference_time_col\n    :param score_col: Column name in leaderboard of model score values\n    :param inference_time_col: Column name in leaderboard of model inference times\n    :return: Subset of the original leaderboard DataFrame containing only models that are a valid optimal choice at different valuations of score and inference time.\n    """"""\n    leaderboard = leaderboard.sort_values(by=[score_col, inference_time_col], ascending=[False, True]).reset_index(drop=True)\n    leaderboard_unique = leaderboard.drop_duplicates(subset=[score_col])\n\n    pareto_frontier = []\n    inference_time_min = None\n    for index, row in leaderboard_unique.iterrows():\n        if row[inference_time_col] is None or row[score_col] is None:\n            pass\n        elif (inference_time_min is None) or (row[inference_time_col] < inference_time_min):\n            inference_time_min = row[inference_time_col]\n            pareto_frontier.append(index)\n    leaderboard_pareto_frontier = leaderboard_unique.loc[pareto_frontier].reset_index(drop=True)\n    return leaderboard_pareto_frontier\n\n\ndef combine_pred_and_true(y_predprob, y_true, upweight_factor=0.25):\n    """""" Used in distillation, combines true (integer) classes with 2D array of predicted probabilities.\n        Returns new\xc2\xa02D array of predicted probabilities where true classes are upweighted by upweight_factor (and then probabilities are renormalized)\n    """"""\n    if len(y_predprob) != len(y_true):\n        raise ValueError(""y_predprob and y_true cannot have different lengths for distillation. Perhaps some classes\' data was deleted during label cleaning."")\n\n    y_trueprob = np.zeros((y_true.size, y_true.max() + 1))\n    y_trueprob[np.arange(y_true.size), y_true] = upweight_factor\n    y_predprob = y_predprob + y_trueprob\n    y_predprob = y_predprob / y_predprob.sum(axis=1, keepdims=1)  # renormalize\n    return y_predprob\n\n\ndef shuffle_df_rows(X: DataFrame, seed=0, reset_index=True):\n    """"""Returns DataFrame with rows shuffled based on seed value.""""""\n    row_count = X.shape[0]\n    np.random.seed(seed)\n    rand_shuffle = np.random.randint(0, row_count, size=row_count)\n    X_shuffled = X.iloc[rand_shuffle]\n    if reset_index:\n        X_shuffled.reset_index(inplace=True, drop=True)\n    return X_shuffled\n'"
autogluon/utils/tabular/utils/__init__.py,0,b''
autogluon/utils/tabular/utils/decorators.py,0,"b'import time, logging\n\nlogger = logging.getLogger(__name__)\n\n# decorator to calculate duration taken by any function. Logs times at debug level (logging level 10).\ndef calculate_time(func):\n    # added arguments inside the inner1,\n    # if function takes any arguments,\n    # can be added like this.\n    def inner1(*args, **kwargs):\n        # storing time before function execution\n        begin = time.time()\n\n        output = func(*args, **kwargs)\n\n        # storing time after function execution\n        end = time.time()\n        logger.debug(""Total time taken in "" + str(func.__name__)+"": ""+str(end - begin))\n\n        return output\n\n    return inner1\n'"
autogluon/utils/tabular/utils/exceptions.py,0,b'\n\nclass TimeLimitExceeded(Exception):\n    pass\n\n\nclass NotEnoughMemoryError(Exception):\n    pass\n'
autogluon/utils/tabular/utils/multiprocessing_utils.py,0,"b""import multiprocessing, logging\nimport pandas as pd\nimport numpy as np\n\nlogger = logging.getLogger(__name__)\n\ndef dataframe_transform_parallel(\n        df, transformer\n                   ):\n    cpu_count = multiprocessing.cpu_count()\n    workers_count = int(round(cpu_count))\n    logger.log(15, 'Dataframe_transform_parallel running pool with '+str(workers_count)+' workers')\n    df_chunks = np.array_split(df, workers_count)\n    df_list = execute_multiprocessing(workers_count=workers_count, transformer=transformer, chunks=df_chunks)\n    df_combined = pd.concat(df_list, axis=0, ignore_index=True)\n    return df_combined\n\n\n# If multiprocessing_method is 'fork', initialization time scales linearly with current allocated memory, dramatically slowing down runs. forkserver makes this time constant\ndef execute_multiprocessing(workers_count, transformer, chunks, multiprocessing_method='forkserver'):\n    logger.log(15, 'Execute_multiprocessing starting worker pool...')\n    ctx = multiprocessing.get_context(multiprocessing_method)\n    with ctx.Pool(workers_count) as pool:\n        out = pool.map(transformer, chunks)\n    return out\n"""
autogluon/utils/tabular/utils/s3_utils.py,0,"b""import boto3\n\n\ndef is_s3_url(path):\n    if (path[:2] == 's3') and ('://' in path[:6]):\n        return True\n    return False\n\n\ndef s3_path_to_bucket_prefix(s3_path):\n    s3_path_cleaned = s3_path.split('://', 1)[1]\n    bucket, prefix = s3_path_cleaned.split('/', 1)\n\n    # print('extracted bucket:', bucket, 'and prefix:', prefix, 'from s3_path:', s3_path)\n    return bucket, prefix\n\n\ndef s3_bucket_prefix_to_path(bucket, prefix, version='s3'):\n    return version + '://' + bucket + '/' + prefix\n\n\ndef delete_s3_prefix(bucket, prefix):\n    s3 = boto3.resource('s3')\n    objects_to_delete = s3.meta.client.list_objects(Bucket=bucket, Prefix=prefix)\n\n    delete_keys = {'Objects': []}\n    delete_keys['Objects'] = [{'Key': k} for k in [obj['Key'] for obj in objects_to_delete.get('Contents', [])]]\n\n    # print(delete_keys)\n\n    if len(delete_keys['Objects']) != 0:\n      s3.meta.client.delete_objects(Bucket=bucket, Delete=delete_keys)\n"""
tests/unittests/bayesopt/gpmxnet/test_gp.py,0,"b'import numpy as np\nimport mxnet as mx\n\nfrom autogluon.searcher.bayesopt.gpmxnet.mean import ScalarMeanFunction\nfrom autogluon.searcher.bayesopt.gpmxnet.kernel import Matern52\nfrom autogluon.searcher.bayesopt.gpmxnet.likelihood import MarginalLikelihood\nfrom autogluon.searcher.bayesopt.gpmxnet.gp_regression import \\\n    GaussianProcessRegression\nfrom autogluon.searcher.bayesopt.gpmxnet.constants import \\\n    NOISE_VARIANCE_LOWER_BOUND, INVERSE_BANDWIDTHS_LOWER_BOUND\nfrom autogluon.searcher.bayesopt.gpmxnet.gluon_blocks_helpers import \\\n    LogarithmScalarEncoding, PositiveScalarEncoding\n\n\ndef test_likelihood_encoding():\n    mean = ScalarMeanFunction()\n    kernel = Matern52(dimension=1)\n    likelihood = MarginalLikelihood(mean=mean, kernel=kernel)\n    assert isinstance(likelihood.encoding, LogarithmScalarEncoding)\n    likelihood = MarginalLikelihood(mean=mean, kernel=kernel, encoding_type=""positive"")\n    assert isinstance(likelihood.encoding, PositiveScalarEncoding)\n\n\ndef test_gp_regression_no_noise():\n\n    def f(x):\n        return np.sin(x)/x\n\n    x_train = np.arange(-5, 5, 0.2)# [-5,-4.8,-4.6,...,4.8]\n    x_test = np.arange(-4.9, 5, 0.2)# [-4.9, -4.7, -4.5,...,4.9], note that train and test points do not overlap\n    y_train = f(x_train)\n    y_test = f(x_test)\n\n    # to mx.nd\n    y_train_mx_nd = mx.nd.array(y_train)\n    x_train_mx_nd = mx.nd.array(x_train)\n    x_test_mx_nd = mx.nd.array(x_test)\n\n    model = GaussianProcessRegression(kernel=Matern52(dimension=1))\n    model.fit(x_train_mx_nd, y_train_mx_nd)\n\n    # Check that the value of the residual noise variance learned by empirical Bayes is in the same order\n    # as the smallest allowed value (since there is no noise)\n    noise_variance = model.likelihood.get_noise_variance()\n    np.testing.assert_almost_equal(noise_variance, NOISE_VARIANCE_LOWER_BOUND)\n\n    mu_train, var_train = model.predict(x_train_mx_nd)[0]\n    mu_test, var_test = model.predict(x_test_mx_nd)[0]\n\n    # back to np.array\n    mu_train = mu_train.asnumpy()\n    mu_test = mu_test.asnumpy()\n    var_train = var_train.asnumpy()\n    var_test = var_test.asnumpy()\n\n    np.testing.assert_almost_equal(mu_train, y_train, decimal=4)\n    np.testing.assert_almost_equal(var_train, [0.0] * len(var_train), decimal=4)\n    # Fewer decimals imposed for the test points\n    np.testing.assert_almost_equal(mu_test, y_test, decimal=3)\n\n    # If we wish plot\n    # import matplotlib.pyplot as plt\n    # plt.plot(x_train, y_train, ""r"")\n    # plt.errorbar(x=x_train,\n    #              y=mu_train,\n    #              yerr=var_train)\n    # plt.plot(x_test, y_test, ""b"")\n    # plt.errorbar(x=x_test,\n    #              y=mu_test,\n    #              yerr=var_test)\n    # plt.show()\n\n\ndef test_gp_regression_with_noise():\n\n    def f(x):\n        return np.sin(x)/x\n\n    np.random.seed(7)\n\n    x_train = np.arange(-5, 5, 0.2)# [-5, -4.8, -4.6,..., 4.8]\n    x_test = np.arange(-4.9, 5, 0.2)# [-4.9, -4.7, -4.5,..., 4.9], note that train and test points do not overlap\n    y_train = f(x_train)\n    y_test = f(x_test)\n\n    std_noise = 0.01\n    noise_train = np.random.normal(0.0, std_noise,size=y_train.shape)\n\n    # to mx.nd\n    y_train_mx_nd = mx.nd.array(y_train)\n    noise_train_mx_nd = mx.nd.array(noise_train)\n    x_train_mx_nd = mx.nd.array(x_train)\n    x_test_mx_nd = mx.nd.array(x_test)\n\n    model = GaussianProcessRegression(kernel=Matern52(dimension=1))\n    model.fit(x_train_mx_nd, y_train_mx_nd + noise_train_mx_nd)\n\n    # Check that the value of the residual noise variance learned by empirical Bayes is in the same order as std_noise^2\n    noise_variance = model.likelihood.get_noise_variance()\n    np.testing.assert_almost_equal(noise_variance, std_noise**2, decimal=4)\n\n    mu_train, _ = model.predict(x_train_mx_nd)[0]\n    mu_test, _ = model.predict(x_test_mx_nd)[0]\n\n    # back to np.array\n    mu_train = mu_train.asnumpy()\n    mu_test = mu_test.asnumpy()\n\n    np.testing.assert_almost_equal(mu_train, y_train, decimal=2)\n    np.testing.assert_almost_equal(mu_test, y_test, decimal=2)\n\n\ndef test_gp_regression_2d_with_ard():\n\n    def f(x):\n        # Only dependent on the first column of x\n        return np.sin(x[:,0])/x[:,0]\n\n    np.random.seed(7)\n\n    dimension = 3\n\n    # 30 train and test points in R^3\n    x_train = np.random.uniform(-5, 5, size=(30,dimension))\n    x_test = np.random.uniform(-5, 5, size=(30,dimension))\n    y_train = f(x_train)\n    y_test = f(x_test)\n\n    # to mx.nd\n    y_train_mx_nd = mx.nd.array(y_train)\n    x_train_mx_nd = mx.nd.array(x_train)\n    x_test_mx_nd = mx.nd.array(x_test)\n\n    model = GaussianProcessRegression(kernel=Matern52(dimension=dimension, ARD=True))\n    model.fit(x_train_mx_nd, y_train_mx_nd)\n\n    # Check that the value of the residual noise variance learned by empirical Bayes is in the same order as the smallest allowed value (since there is no noise)\n    noise_variance = model.likelihood.get_noise_variance()\n    np.testing.assert_almost_equal(noise_variance, NOISE_VARIANCE_LOWER_BOUND)\n\n    # Check that the bandwidths learned by empirical Bayes reflect the fact that only the first column is useful\n    # In particular, for the useless dimensions indexed by {1,2}, the inverse bandwidths should be close to INVERSE_BANDWIDTHS_LOWER_BOUND\n    # (or conversely, bandwidths should be close to their highest allowed values)\n    sqd = model.likelihood.kernel.squared_distance\n    inverse_bandwidths = sqd.encoding.get(mx.nd, sqd.inverse_bandwidths_internal.data()).asnumpy()\n\n    assert inverse_bandwidths[0] > inverse_bandwidths[1] and inverse_bandwidths[0] > inverse_bandwidths[2]\n    np.testing.assert_almost_equal(inverse_bandwidths[1], INVERSE_BANDWIDTHS_LOWER_BOUND)\n    np.testing.assert_almost_equal(inverse_bandwidths[2], INVERSE_BANDWIDTHS_LOWER_BOUND)\n\n    mu_train, _ = model.predict(x_train_mx_nd)[0]\n    mu_test, _ = model.predict(x_test_mx_nd)[0]\n\n    # back to np.array\n    mu_train = mu_train.asnumpy()\n    mu_test = mu_test.asnumpy()\n\n    np.testing.assert_almost_equal(mu_train, y_train, decimal=2)\n    # Fewer decimals imposed for the test points\n    np.testing.assert_almost_equal(mu_test, y_test, decimal=1)\n'"
tests/unittests/bayesopt/gpmxnet/test_kernel.py,0,"b'import numpy as np\nimport mxnet as mx\nimport pytest\n\nfrom autogluon.searcher.bayesopt.gpmxnet.kernel import Matern52, \\\n    FabolasKernelFunction, ProductKernelFunction\nfrom autogluon.searcher.bayesopt.gpmxnet.kernel.base import SquaredDistance\nfrom autogluon.searcher.bayesopt.gpmxnet.constants import DATA_TYPE\nfrom autogluon.searcher.bayesopt.gpmxnet.gluon_blocks_helpers import \\\n    LogarithmScalarEncoding, PositiveScalarEncoding\n\n\ndef test_square_distance_no_ard_unit_bandwidth():\n    X = mx.nd.array([[1, 0], [0, 1]], dtype=DATA_TYPE)\n    # test default ard=False\n    sqd = SquaredDistance(dimension=2)\n    assert sqd.ARD == False\n    sqd.collect_params().initialize()\n    D = sqd(X, X).asnumpy()\n    expected_D = np.array([[0.0, 2.0], [2.0, 0.0]])\n    np.testing.assert_almost_equal(expected_D, D)\n\n\ndef test_square_distance_no_ard_non_unit_bandwidth():\n    X = mx.nd.array([[1, 0], [0, 1]], dtype=DATA_TYPE)\n    sqd = SquaredDistance(dimension=2)\n    assert sqd.ARD == False\n    sqd.collect_params().initialize()\n    sqd.encoding.set(sqd.inverse_bandwidths_internal, 1./np.sqrt(2.))\n    D = sqd(X, X).asnumpy()\n    expected_D = np.array([[0.0, 1.0], [1.0, 0.0]])\n    np.testing.assert_almost_equal(expected_D, D)\n\n\ndef test_square_distance_with_ard():\n    X = mx.nd.array([[2., 1.], [1., 2.], [0., 1.]], dtype=DATA_TYPE)\n    sqd = SquaredDistance(dimension=2, ARD=True)\n    assert sqd.ARD == True\n    sqd.collect_params().initialize()\n    sqd.encoding.set(sqd.inverse_bandwidths_internal, [1. / np.sqrt(2.), 1.])\n    D = sqd(X, X).asnumpy()\n    expected_D = np.array([[0., 3./2., 2.], [3./2., 0., 3./2.], [2.0, 3./2., 0.]])\n    np.testing.assert_almost_equal(expected_D, D)\n\n\nmater52 = lambda squared_dist: (1. + np.sqrt(5. * squared_dist) + 5. / 3. * squared_dist) * np.exp(-np.sqrt(5. * squared_dist))\nfreeze_thaw = lambda u, alpha, beta: beta**alpha / (u + beta)**alpha\n\n\ndef test_matern52_unit_scale():\n    X = mx.nd.array([[1, 0], [0, 1]], dtype=DATA_TYPE)\n    kernel = Matern52(dimension=2)\n    assert kernel.ARD == False\n    kernel.collect_params().initialize()\n    K = kernel(X,X).asnumpy()\n    expected_K = np.array([[mater52(0.0), mater52(2.0)], [mater52(2.0), mater52(0.0)]])\n    np.testing.assert_almost_equal(expected_K, K)\n\n\ndef test_matern52_non_unit_scale():\n    X = mx.nd.array([[1, 0], [0, 1]], dtype=DATA_TYPE)\n    kernel = Matern52(dimension=2)\n    assert kernel.ARD == False\n    kernel.collect_params().initialize()\n    kernel.encoding.set(kernel.covariance_scale_internal, 0.5)\n    K = kernel(X,X).asnumpy()\n    expected_K = 0.5 * np.array([[mater52(0.0), mater52(2.0)], [mater52(2.0), mater52(0.0)]])\n    np.testing.assert_almost_equal(expected_K, K)\n\n\ndef test_matern52_ard():\n    X = mx.nd.array([[2., 1.], [1., 2.], [0., 1.]], dtype=DATA_TYPE)\n    kernel = Matern52(dimension=2, ARD=True)\n    kernel.collect_params().initialize()\n    sqd = kernel.squared_distance\n    assert kernel.ARD == True\n    assert sqd.ARD == True\n    sqd.encoding.set(sqd.inverse_bandwidths_internal, [1. / np.sqrt(2.), 1.])\n    K = kernel(X,X).asnumpy()\n    # expected_D is taken from previous test about squared distances\n    expected_D = np.array([[0., 3. / 2., 2.], [3. / 2., 0., 3. / 2.], [2.0, 3. / 2., 0.]])\n    expected_K = mater52(expected_D)\n    np.testing.assert_almost_equal(expected_K, K)\n\n\ndef test_matern52_encoding():\n    kernel = Matern52(dimension=2, ARD=True)\n    assert isinstance(kernel.encoding, LogarithmScalarEncoding)\n    assert isinstance(kernel.squared_distance.encoding, LogarithmScalarEncoding)\n    assert kernel.encoding.dimension == 1\n    assert kernel.squared_distance.encoding.dimension == 2\n    kernel = Matern52(dimension=2, ARD=True, encoding_type=""positive"")\n    assert isinstance(kernel.encoding, PositiveScalarEncoding)\n    assert isinstance(kernel.squared_distance.encoding, PositiveScalarEncoding)\n    assert kernel.encoding.dimension == 1\n    assert kernel.squared_distance.encoding.dimension == 2\n\n\ndef test_fabolas_encoding():\n    kernel = FabolasKernelFunction()\n    assert isinstance(kernel.encoding_u12, LogarithmScalarEncoding)\n    assert kernel.encoding_u12.dimension == 1\n\n    kernel = FabolasKernelFunction(encoding_type=""positive"")\n    assert isinstance(kernel.encoding_u12, PositiveScalarEncoding)\n    assert kernel.encoding_u12.dimension == 1\n\n\ndef test_matern52_wrongshape():\n    kernel = Matern52(dimension=3)\n    kernel.collect_params().initialize()\n    X1 = mx.nd.random_normal(0.0, 1.0, shape=(5, 2))\n    with pytest.raises(mx.base.MXNetError):\n        kmat = kernel(X1, X1)\n    with pytest.raises(mx.base.MXNetError):\n        kdiag = kernel.diagonal(mx.nd, X1)\n    X2 = mx.nd.random_normal(0.0, 1.0, shape=(3, 3))\n    with pytest.raises(mx.base.MXNetError):\n        kmat = kernel(X2, X1)\n\n\ndef test_product_wrongshape():\n    kernel1 = Matern52(dimension=2)\n    kernel1.collect_params().initialize()\n\n    #  A better way to do this is using the `pytest.mark.parametrize`\n    #  decorator. Keeping it simple for now.\n    kernels = [Matern52(dimension=1),\n               FabolasKernelFunction()]\n\n    for kernel2 in kernels:\n\n        kernel2.collect_params().initialize()\n        kernel = ProductKernelFunction(kernel1, kernel2)\n        X1 = mx.nd.random_normal(0.0, 1.0, shape=(5, 4))\n        with pytest.raises(mx.base.MXNetError):\n            kmat = kernel(X1, X1)\n        with pytest.raises(mx.base.MXNetError):\n            kdiag = kernel.diagonal(mx.nd, X1)\n        X2 = mx.nd.random_normal(0.0, 1.0, shape=(3, 3))\n        with pytest.raises(mx.base.MXNetError):\n            kmat = kernel(X2, X1)\n        X1 = mx.nd.random_normal(0.0, 1.0, shape=(5, 2))\n        with pytest.raises(mx.base.MXNetError):\n            kmat = kernel(X1, X1)\n'"
tests/unittests/bayesopt/gpmxnet/test_mcmc.py,0,"b'import numpy as np\nimport mxnet as mx\nimport pytest\n\nfrom autogluon.searcher.bayesopt.gpmxnet import SliceException\nfrom autogluon.searcher.bayesopt.gpmxnet.slice import SliceSampler, \\\n    slice_sampler_step_out, slice_sampler_step_in\nfrom autogluon.searcher.bayesopt.gpmxnet.distribution import Normal, \\\n    LogNormal, Horseshoe, Uniform\nfrom autogluon.searcher.bayesopt.gpmxnet.warping import Warping, WarpedKernel\nfrom autogluon.searcher.bayesopt.gpmxnet.kernel import Matern52\nfrom autogluon.searcher.bayesopt.gpmxnet.mean import ScalarMeanFunction\nfrom autogluon.searcher.bayesopt.gpmxnet.likelihood import MarginalLikelihood\nfrom autogluon.searcher.bayesopt.gpmxnet.gpr_mcmc import GPRegressionMCMC, \\\n    _get_gp_hps, _set_gp_hps, _create_likelihood\n\n\n# This is just to make the tests work. In GPRegressionMCMC, lower and upper\n# bounds are dealt with through the encoding\ndef old_log_likelihood(\n        x, distribution, lower=-float(\'inf\'), upper=float(\'inf\')):\n    if any(x < lower) or any(x > upper):\n        return -float(""inf"")\n    return -distribution(mx.nd, mx.nd.array(x, dtype=np.float64)).asnumpy()\n\n\ndef test_uniform():\n    uniform = Uniform(0.0, 1.0)\n    lower, upper = 0.0, 1.0\n    assert old_log_likelihood(np.array([0.2]), uniform, lower, upper) == \\\n           old_log_likelihood(np.array([0.3]), uniform, lower, upper)\n    assert old_log_likelihood(np.array([2.0]), uniform, lower, upper) == -float(""inf"")\n    assert old_log_likelihood(np.array([-1.0]), uniform, lower, upper) == -float(""inf"")\n\n\ndef test_normal():\n    normal = Normal(0, 1)\n    lower, upper =  -1e3, 1e3\n    assert old_log_likelihood(np.array([0.0]), normal, lower, upper) > \\\n           old_log_likelihood(np.array([0.1]), normal, lower, upper)\n    assert old_log_likelihood(np.array([0.0]), normal, lower, upper) > \\\n           old_log_likelihood(np.array([-0.1]), normal, lower, upper)\n    assert old_log_likelihood(np.array([1e4]), normal, lower, upper) == -float(""inf"")\n    assert old_log_likelihood(np.array([-1e4]), normal, lower, upper) == -float(""inf"")\n\n\ndef test_log_normal():\n    log_normal = LogNormal(0.0, 1.0)\n    lower, upper =  1e-6, 1e9\n    assert old_log_likelihood(np.array([1.0]), log_normal, lower, upper) > \\\n           old_log_likelihood(np.array([1.1]), log_normal, lower, upper)\n    assert old_log_likelihood(np.array([1.0]), log_normal, lower, upper) < \\\n           old_log_likelihood(np.array([0.9]), log_normal, lower, upper)\n    assert old_log_likelihood(np.array([1e10]), log_normal, lower, upper) == -float(""inf"")\n    assert old_log_likelihood(np.array([1e-8]), log_normal, lower, upper) == -float(""inf"")\n\n\ndef test_horse_shoe():\n    horse_shoe = Horseshoe(0.1)\n    lower, upper = 1e-6, 1e6\n    assert old_log_likelihood(np.array([0.01]), horse_shoe, lower, upper) > \\\n           old_log_likelihood(np.array([0.1]), horse_shoe, lower, upper)\n    assert old_log_likelihood(np.array([1e-7]), horse_shoe, lower, upper) == -float(""inf"")\n    assert old_log_likelihood(np.array([1e7]), horse_shoe, lower, upper) == -float(""inf"")\n\n\ndef test_slice_normal():\n    normal = Normal(0, 1)\n    slice = SliceSampler(lambda x: old_log_likelihood(x, normal), 1.0, 0)\n    samples = slice.sample(np.array([0.0]), 5000, 1, 1)\n    np.testing.assert_almost_equal(np.mean(samples), 0.0, decimal=2)\n    np.testing.assert_almost_equal(np.std(samples),  1.0, decimal=2)\n\n\ndef test_slice_step_out():\n    normal = Normal(0, 1)\n\n    def sliced_log_density(x):\n        return old_log_likelihood(np.array([x]), normal)\n    # the lower and upper bound should has log density smaller than this log_pivot\n    log_pivot = sliced_log_density(1.0)\n    random_state = np.random.RandomState(0)\n    lower, upper = slice_sampler_step_out(\n        log_pivot, 0.1, sliced_log_density, random_state)\n    assert lower < -1.0 and upper > 1.0\n\n    log_pivot = sliced_log_density(100)\n    with pytest.raises(SliceException):  # the log_pivot is too small so need > 200 steps\n        slice_sampler_step_out(log_pivot, 0.1, sliced_log_density, random_state)\n\n\ndef test_slice_step_in():\n    normal = Normal(0., 1.)\n\n    def sliced_log_density(x):\n        return old_log_likelihood(np.array([x]), normal)\n    log_pivot = sliced_log_density(1.)  # the movement should between [-1., 1.] after step in\n    random_state = np.random.RandomState(0)\n    movement = slice_sampler_step_in(-20.0, 20.0, log_pivot, sliced_log_density, random_state)\n    assert -1.0 < movement < 1.0\n\n    with pytest.raises(SliceException):  # when bound is off, should get SliceException\n        slice_sampler_step_in(2.0, 10.0, log_pivot, sliced_log_density, random_state)\n\n\ndef test_get_gp_hps():\n    mean = ScalarMeanFunction()\n    kernel = Matern52(dimension=1)\n    warping = Warping(dimension=1, index_to_range={0: (-4., 4.)})\n    warped_kernel = WarpedKernel(kernel=kernel, warping=warping)\n    likelihood = MarginalLikelihood(kernel=warped_kernel, mean=mean, initial_noise_variance=1e-6)\n    likelihood.initialize(ctx=mx.cpu(), force_reinit=True)\n    likelihood.hybridize()\n    hp_values = _get_gp_hps(likelihood)\n    # the oder of hps are noise, mean, covariance scale, bandwidth, warping a, warping b\n    np.testing.assert_array_almost_equal(hp_values, np.array([1e-6, 0.0, 1.0, 1.0, 1.0, 1.0]))\n\n\ndef test_set_gp_hps():\n    mean = ScalarMeanFunction()\n    kernel = Matern52(dimension=1)\n    warping = Warping(dimension=1, index_to_range={0: (-4., 4.)})\n    warped_kernel = WarpedKernel(kernel=kernel, warping=warping)\n    likelihood = MarginalLikelihood(kernel=warped_kernel, mean=mean, initial_noise_variance=1e-6)\n    likelihood.initialize(ctx=mx.cpu(), force_reinit=True)\n    likelihood.hybridize()\n    hp_values = np.array([1e-2, 1.0, 0.5, 0.3, 0.2, 1.1])\n    _set_gp_hps(hp_values, likelihood)\n    np.testing.assert_array_almost_equal(hp_values, _get_gp_hps(likelihood))\n\n\ndef test_create_likelihood():\n    def build_kernel():\n        kernel = Matern52(dimension=1)\n        warping = Warping(dimension=1, index_to_range={0: (-4., 4.)})\n        return WarpedKernel(kernel=kernel, warping=warping)\n    likelihood1 = _create_likelihood(build_kernel, ctx=mx.cpu())\n    likelihood2 = _create_likelihood(build_kernel, ctx=mx.cpu())\n    np.testing.assert_array_almost_equal(_get_gp_hps(likelihood1), _get_gp_hps(likelihood2))\n\n\n@pytest.mark.skip(reason=""Need manual inspection on the plots"")\ndef test_mcmc():\n    np.random.seed(7)\n\n    def f_n(x):\n        noise = np.random.normal(0.0, 0.25, x.shape[0])\n        return 0.1 * np.power(x, 3) + noise\n\n    def f(x):\n        return 0.1 * np.power(x, 3)\n\n    x_train = np.concatenate((np.random.uniform(-4., -1., 40), np.random.uniform(1., 4., 40)))\n    y_train = f_n(x_train)\n    x_test = np.sort(np.random.uniform(-4., 4., 200))\n\n    y_train_mx_nd = mx.nd.array(y_train, dtype=np.float64)\n    x_train_mx_nd = mx.nd.array(x_train, dtype=np.float64)\n    x_test_mx_nd = mx.nd.array(x_test, dtype=np.float64)\n\n    def build_kernel():\n        return WarpedKernel(\n            kernel=Matern52(dimension=1),\n            warping=Warping(dimension=1, index_to_range={0: (-4., 4.)})\n        )\n\n    model_mcmc = GPRegressionMCMC(build_kernel=build_kernel, random_seed=1)\n    model_mcmc.fit(x_train_mx_nd, y_train_mx_nd)\n    mcmc_predictions = model_mcmc.predict(x_test_mx_nd)\n\n    import matplotlib.pyplot as plt\n    for mcmc_mean, mcmc_var in mcmc_predictions:\n        mcmc_mean, mcmc_std = mcmc_mean.asnumpy(), np.sqrt(mcmc_var.asnumpy())\n        plt.figure()\n        plt.scatter(x_train, y_train, color=""red"", label=""observations"")\n        plt.plot(x_test, f(x_test), color=""black"", label=""ground truth"")\n        plt.plot(x_test, mcmc_mean, color=""blue"", label=""mcmc prediction"")\n        plt.fill_between(x_test, mcmc_mean - 1.96*mcmc_std, mcmc_mean + 1.96*mcmc_std, alpha=.5)\n        plt.legend()\n    plt.show()\n'"
tests/unittests/bayesopt/gpmxnet/test_optimization_utils.py,0,"b""import mxnet as mx\nimport numpy as np\nfrom mxnet import autograd\n\nfrom autogluon.searcher.bayesopt.gpmxnet.optimization_utils import \\\n    apply_lbfgs_with_multiple_starts, apply_lbfgs\n\n\ndef _create_toy_lbfgs_problem():\n\n    x = -0.5*mx.nd.ones((1,), ctx=mx.cpu(), dtype='float64')\n    x.attach_grad()\n\n    ###############\n    def executor():\n\n        with autograd.record():\n            # Over [-4,4]^3, the function has two minima, one local at around x = -2.\n            # and global at around x = +2.91318053.\n            # Moreover, it has a maximum around x = 0. (plot it in Google, simply typing y=(x+2)*sin(x+2))\n            # If the starting point is about x = -0.5 (note that this is the default value for x),\n            # L-BFGS should end-up at the local minima while if the starting is about x = 0.5,\n            # L-BFGS should end-up at the global minima\n            non_convex_function = (x+2.)*mx.nd.sin(x+2.)\n\n        objective_value = non_convex_function.asscalar()\n        non_convex_function.backward()\n\n        return objective_value\n    ###############\n\n    arg_dict = {'x':x}\n    grad_dict = {'x':x.grad}\n\n    return executor, arg_dict, grad_dict\n\n\ndef test_apply_lbfgs_with_single_restart():\n\n    bounds = {'x' : (-4.,4.)}\n    executor, arg_dict_with_single_start, grad_dict = _create_toy_lbfgs_problem()\n    apply_lbfgs_with_multiple_starts(executor, arg_dict_with_single_start, grad_dict, bounds, n_starts=1)\n\n    # See explanations above to understand the target value of -2.\n    np.testing.assert_almost_equal(arg_dict_with_single_start['x'].asscalar(), -2., decimal=5)\n\n    executor, arg_dict, grad_dict = _create_toy_lbfgs_problem()\n    apply_lbfgs(executor, arg_dict, grad_dict, bounds)\n\n    # With a single starting point, apply_lbfgs and apply_lbfgs_with_restarts must coincide\n    np.testing.assert_almost_equal(arg_dict['x'].asscalar(), arg_dict_with_single_start['x'].asscalar())\n\n\ndef test_apply_lbfgs_with_multiple_restart():\n\n    bounds = {'x' : (-4.,4.)}\n    executor, arg_dict_with_multiple_starts, grad_dict = _create_toy_lbfgs_problem()\n    # If this is left at -0.5, then the test may fail even with 10 repetitions\n    # In fact, the logic in apply_lbfgs_with_multiple_starts makes little sense.\n    # At least, the randomization would have to be with stddev of size\n    # related to the norm of the mean.\n    arg_dict_with_multiple_starts['x'][:] = 0.1\n    apply_lbfgs_with_multiple_starts(executor, arg_dict_with_multiple_starts, grad_dict, bounds, n_starts=10)\n\n    # See explanations above to understand the target value of 2.91318053\n    np.testing.assert_almost_equal(arg_dict_with_multiple_starts['x'].asscalar(), 2.91318053, decimal=4)"""
tests/unittests/bayesopt/gpmxnet/test_posterior_state.py,0,"b'import numpy as np\nimport mxnet as mx\n\nfrom autogluon.searcher.bayesopt.gpmxnet.posterior_state import \\\n    IncrementalUpdateGPPosteriorState, GaussProcPosteriorState\nfrom autogluon.searcher.bayesopt.gpmxnet.gp_regression import \\\n    GaussianProcessRegression\nfrom autogluon.searcher.bayesopt.gpmxnet.kernel import Matern52\n\n\ndef to_nd(x, dtype=np.float64):\n    return mx.nd.array(x, dtype=dtype)\n\n\ndef test_incremental_update():\n    def f(x):\n        return np.sin(x) / x\n\n    np.random.seed(298424)\n    std_noise = 0.01\n\n    for rep in range(10):\n        model = GaussianProcessRegression(kernel=Matern52(dimension=1))\n        # Sample data\n        num_train = np.random.randint(low=5, high=15)\n        num_incr = np.random.randint(low=1, high=7)\n        sizes = [num_train, num_incr]\n        features = []\n        targets = []\n        for sz in sizes:\n            feats = np.random.uniform(low=-1.0, high=1.0, size=sz).reshape((-1, 1))\n            features.append(feats)\n            targs = f(feats)\n            targs += np.random.normal(0.0, std_noise, size=targs.shape)\n            targets.append(targs)\n        # Posterior state by incremental updating\n        train_features = to_nd(features[0])\n        train_targets = to_nd(targets[0])\n        model.fit(train_features, train_targets)\n        noise_variance_1 = model.likelihood.get_noise_variance()\n        state_incr = IncrementalUpdateGPPosteriorState(\n            features=train_features, targets=train_targets,\n            mean=model.likelihood.mean, kernel=model.likelihood.kernel,\n            noise_variance=model.likelihood.get_noise_variance(as_ndarray=True))\n        for i in range(num_incr):\n            state_incr = state_incr.update(\n                to_nd(features[1][i].reshape((1, -1))),\n                to_nd(targets[1][i].reshape((1, -1))))\n        noise_variance_2 = state_incr.noise_variance.asscalar()\n        # Posterior state by direct computation\n        state_comp = GaussProcPosteriorState(\n            features=to_nd(np.concatenate(features, axis=0)),\n            targets=to_nd(np.concatenate(targets, axis=0)),\n            mean=model.likelihood.mean, kernel=model.likelihood.kernel,\n            noise_variance=state_incr.noise_variance)\n        # Compare them\n        assert noise_variance_1 == noise_variance_2, \\\n            ""noise_variance_1 = {} != {} = noise_variance_2"".format(\n                noise_variance_1, noise_variance_2)\n        chol_fact_incr = state_incr.chol_fact.asnumpy()\n        chol_fact_comp = state_comp.chol_fact.asnumpy()\n        np.testing.assert_almost_equal(chol_fact_incr, chol_fact_comp, decimal=2)\n        pred_mat_incr = state_incr.pred_mat.asnumpy()\n        pred_mat_comp = state_comp.pred_mat.asnumpy()\n        np.testing.assert_almost_equal(pred_mat_incr, pred_mat_comp, decimal=2)\n\n\nif __name__ == ""__main__"":\n    test_incremental_update()\n'"
tests/unittests/bayesopt/gpmxnet/test_warping.py,0,"b'import numpy as np\nimport mxnet as mx\n\nfrom autogluon.searcher.bayesopt.gpmxnet.warping import OneDimensionalWarping, \\\n    Warping, WarpedKernel\nfrom autogluon.searcher.bayesopt.gpmxnet.constants import DATA_TYPE, \\\n    NUMERICAL_JITTER\nfrom autogluon.searcher.bayesopt.gpmxnet.kernel import Matern52\nfrom autogluon.searcher.bayesopt.gpmxnet.gp_regression import \\\n    GaussianProcessRegression\nfrom autogluon.searcher.bayesopt.gpmxnet.gluon_blocks_helpers import \\\n    LogarithmScalarEncoding, PositiveScalarEncoding\n\n\ndef test_warping_encoding():\n    input_range = (0., 2.)\n    warping = OneDimensionalWarping(input_range)\n    assert isinstance(warping.encoding, LogarithmScalarEncoding)\n    assert warping.encoding.dimension == 2\n    warping = OneDimensionalWarping(input_range, encoding_type=""positive"")\n    assert isinstance(warping.encoding, PositiveScalarEncoding)\n\n\ndef test_warping_default_parameters():\n    x = mx.nd.array([0., 1., 2.], dtype=DATA_TYPE)\n    input_range = (0., 2.)\n    warping = OneDimensionalWarping(input_range)\n    warping.collect_params().initialize()\n\n    warping_parameters = warping.encoding.get(mx.nd, warping.warping_internal.data())\n\n    np.testing.assert_almost_equal(warping_parameters.asnumpy(), np.ones(2))\n    np.testing.assert_almost_equal(warping(x).asnumpy(), np.array([NUMERICAL_JITTER, 0.5, 1.-NUMERICAL_JITTER]))\n\n\ndef test_warping_with_arbitrary_parameters():\n    x = mx.nd.array([0., 1., 2.], dtype=DATA_TYPE)\n    input_range = (0., 2.)\n\n    warping = OneDimensionalWarping(input_range)\n    warping.collect_params().initialize()\n\n    warping.encoding.set(warping.warping_internal, [2., 0.5])\n    warping_parameters = warping.encoding.get(mx.nd, warping.warping_internal.data())\n\n    np.testing.assert_almost_equal(warping_parameters.asnumpy(), [2., 0.5])\n\n    # In that case (with parameters [2., 0.5]), the warping is given by x => 1. - sqrt(1. - x^2)\n    def expected_warping(x):\n        return 1. - np.sqrt(1. - x*x)\n\n    np.testing.assert_almost_equal(warping(x).asnumpy(), expected_warping(np.array([NUMERICAL_JITTER, 0.5, 1.-NUMERICAL_JITTER])))\n\n\ndef test_warping_with_multidimension_and_arbitrary_parameters():\n    X = mx.nd.array([[0., 1., 0.], [1.,2.,1.], [2., 0., 2.]], dtype=DATA_TYPE)\n\n    dimension=3\n\n    # We transform only the columns {0,2} of the 3-dimensional data X\n    input_range = (0., 2.)\n    warping = Warping(index_to_range={0:input_range, 2:input_range}, dimension=dimension)\n\n    assert len(warping.transformations) == dimension\n\n    warping.collect_params().initialize()\n\n    # We change the warping parameters of the first dimension only\n    w0 = warping.transformations[0]\n    w0.encoding.set(w0.warping_internal, [2., 0.5])\n\n    w2 = warping.transformations[2]\n    w2_parameters = w2.encoding.get(mx.nd, w2.warping_internal.data())\n\n    # The parameters of w2 should be the default ones (as there was no set operations)\n    np.testing.assert_almost_equal(w2_parameters.asnumpy(), np.ones(2))\n\n    # print(warping(X).asnumpy())\n    # for name, p  in warping.collect_params().items():\n    #     print(name, p.data().asnumpy())\n\n    # With parameters [2., 0.5], the warping is given by x => 1. - sqrt(1. - x^2)\n    def expected_warping(x):\n        return 1. - np.sqrt(1. - x*x)\n\n    expected_column0 = expected_warping(np.array([NUMERICAL_JITTER, 0.5, 1.-NUMERICAL_JITTER])).reshape((-1,1))\n    expected_column1 = np.array([1., 2., 0.]).reshape((-1,1))\n    expected_column2 = np.array([NUMERICAL_JITTER, 0.5, 1.-NUMERICAL_JITTER]).reshape((-1,1))\n\n    np.testing.assert_almost_equal(warping(X).asnumpy(), np.hstack([expected_column0, expected_column1, expected_column2]))\n\n\ndef test_gp_regression_with_warping():\n\n    def f(x):\n        return np.sin(3*np.log(x))\n\n    np.random.seed(7)\n\n    L, U = -5., 12.\n    input_range = (2.**L, 2.**U)\n\n    x_train = np.sort(2.**np.random.uniform(L, U, 250))\n    x_test = np.sort(2.**np.random.uniform(L, U, 500))\n    y_train = f(x_train)\n    y_test = f(x_test)\n\n    # to mx.nd\n    y_train_mx_nd = mx.nd.array(y_train)\n    x_train_mx_nd = mx.nd.array(x_train)\n    x_test_mx_nd = mx.nd.array(x_test)\n\n    kernels = [\n        Matern52(dimension=1),\n        WarpedKernel(\n            kernel=Matern52(dimension=1),\n            warping=Warping(dimension=1, index_to_range={0: input_range})\n        )\n    ]\n\n    models = [GaussianProcessRegression(kernel=k, random_seed=0) for k in kernels]\n    train_errors, test_errors = [], []\n\n    for model in models:\n\n        model.fit(x_train_mx_nd, y_train_mx_nd)\n\n        mu_train, var_train = model.predict(x_train_mx_nd)[0]\n        mu_test, var_test = model.predict(x_test_mx_nd)[0]\n\n        # back to np.array\n        mu_train = mu_train.asnumpy()\n        mu_test = mu_test.asnumpy()\n        # var_train = var_train.asnumpy()\n        # var_test = var_test.asnumpy()\n\n        train_errors.append(np.mean(np.abs((mu_train - y_train))))\n        test_errors.append(np.mean(np.abs((mu_test - y_test))))\n\n    # The two models have similar performance on training points\n    np.testing.assert_almost_equal(train_errors[0], train_errors[1], decimal=4)\n\n    # As expected, the model with warping largely outperforms the model without\n    assert test_errors[1] < 0.1 * test_errors[0]\n\n    # If we wish to plot things\n    # import matplotlib.pyplot as plt\n    # plt.plot(x_train, y_train, ""r-"")\n    # plt.plot(x_train, mu_train, ""b--"")\n    #\n    # plt.plot(x_test, y_test, ""y-"")\n    # plt.plot(x_test, mu_test, ""m--"")\n\n    # plt.fill_between(x_train,\n    #                  mu_train - np.sqrt(var_train),\n    #                  mu_train + np.sqrt(var_train),\n    #                  alpha=0.5, edgecolor=\'#3F7F4C\', facecolor=\'#7EFF99\', linewidth=0)\n    #\n    # plt.fill_between(x_test,\n    #                  mu_test - np.sqrt(var_test),\n    #                  mu_test + np.sqrt(var_test),\n    #                  alpha=0.5, edgecolor=\'#3F7F4C\', facecolor=\'#7EFF99\', linewidth=0)\n    #\n    # plt.show()\n'"
autogluon/searcher/bayesopt/gpmxnet/kernel/__init__.py,0,b'from .base import *\nfrom .coregionalization import *\nfrom .exponential_decay import *\nfrom .fabolas import *\nfrom .product_kernel import *\n'
autogluon/searcher/bayesopt/gpmxnet/kernel/base.py,0,"b'import mxnet as mx\nfrom mxnet import gluon\nfrom abc import abstractmethod\n\nfrom autogluon.searcher.bayesopt.gpmxnet.gluon_blocks_helpers import \\\n    encode_unwrap_parameter\nfrom autogluon.searcher.bayesopt.gpmxnet.utils import create_encoding, \\\n    register_parameter\nfrom autogluon.searcher.bayesopt.gpmxnet.distribution import Uniform, LogNormal\nfrom autogluon.searcher.bayesopt.gpmxnet.constants import \\\n    INITIAL_COVARIANCE_SCALE, INITIAL_INVERSE_BANDWIDTHS, DEFAULT_ENCODING, \\\n    INVERSE_BANDWIDTHS_LOWER_BOUND, INVERSE_BANDWIDTHS_UPPER_BOUND,\\\n    COVARIANCE_SCALE_LOWER_BOUND, COVARIANCE_SCALE_UPPER_BOUND, NUMERICAL_JITTER\nfrom autogluon.searcher.bayesopt.gpmxnet.mean import MeanFunction\n\n__all__ = [\'KernelFunction\', \'Matern52\']\n\n\nclass KernelFunction(MeanFunction):\n    """"""\n    Base class of kernel (or covariance) functions\n\n    """"""\n    def __init__(self, dimension: int, **kwargs):\n        """"""\n        :param dimension: Dimensionality of input points after encoding into\n            ndarray\n        """"""\n        super(KernelFunction, self).__init__(**kwargs)\n        self._dimension = dimension\n\n    @property\n    def dimension(self):\n        """"""\n        :return: Dimension d of input points\n        """"""\n        return self._dimension\n\n    @abstractmethod\n    def diagonal(self, F, X):\n        """"""\n        :param F: mx.sym or mx.nd\n        :param X: Input data, shape (n, d)\n        :return: Diagonal of K(X, X), shape (n,)\n        """"""\n        pass\n\n    @abstractmethod\n    def diagonal_depends_on_X(self):\n        """"""\n        For stationary kernels, diagonal does not depend on X\n\n        :return: Does diagonal(F, X) depend on X?\n        """"""\n        pass\n\n    def _check_input_shape(self, F, X):\n        # This fails if the shape is not (*, dimension)\n        return F.reshape(X, shape=(0, self._dimension))\n\n\nclass SquaredDistance(gluon.HybridBlock):\n    """"""\n    HybridBlock that is responsible for the computation of matrices of squared\n    distances. The distances can possibly be weighted (e.g., ARD\n    parametrization). For instance:\n        X1 with size (n1,d)\n        X2 with size (n2,d)\n        inverse_bandwidths with size (1,d)\n        results in a matrix of size (n1,n2) with i,j entry equal to\n            sum_{k=1}^d (X1[i,k] - X2[j,k])^2 * inverse_bandwidths[k]^2\n\n    if ARD == False, inverse_bandwidths is equal to a scalar broadcast to the\n    d components (with d=dimension, i.e., the number of features in X)\n    otherwise, inverse_bandwidths is (1,d)\n    """"""\n\n    def __init__(self, dimension, ARD=False, encoding_type=DEFAULT_ENCODING, **kwargs):\n        super(SquaredDistance, self).__init__(**kwargs)\n\n        self.ARD = ARD\n        inverse_bandwidths_dimension = 1 if not ARD else dimension\n        self.encoding = create_encoding(\n            encoding_type, INITIAL_INVERSE_BANDWIDTHS,\n            INVERSE_BANDWIDTHS_LOWER_BOUND, INVERSE_BANDWIDTHS_UPPER_BOUND,\n            inverse_bandwidths_dimension,\n            Uniform(INVERSE_BANDWIDTHS_LOWER_BOUND,\n                    INVERSE_BANDWIDTHS_UPPER_BOUND))\n        with self.name_scope():\n            self.inverse_bandwidths_internal = register_parameter(\n                self.params, \'inverse_bandwidths\', self.encoding,\n                shape=(inverse_bandwidths_dimension,))\n\n    def hybrid_forward(self, F, X1, X2, inverse_bandwidths_internal):\n        """"""\n        Actual computation of the matrix of squared distances (see details above)\n\n        :param F: mx.sym or mx.nd\n        :param X1: input data of size (n1,d)\n        :param X2: input data of size (n2,d)\n        :param inverse_bandwidths_internal: self.inverse_bandwidths_internal\n            passed as sym or nd depending on F (natively handled by\n            gluon.HybridBlock)\n        """"""\n\n        inverse_bandwidths = self.encoding.get(F, inverse_bandwidths_internal)\n        # in case inverse_bandwidths if of size (1,dimension), dimension>1,\n        # ARD is handled by broadcasting\n        inverse_bandwidths = F.reshape(inverse_bandwidths, shape=(1, -1))\n\n        if X2 is X1:\n            X1_scaled = F.broadcast_mul(X1, inverse_bandwidths)\n            D = -2.0 * F.linalg.syrk(X1_scaled)\n            X1_squared_norm = F.sum(F.square(X1_scaled), axis=1)\n            D = F.broadcast_add(D, F.reshape(X1_squared_norm, shape=(1, -1)))\n            D = F.broadcast_add(D, F.reshape(X1_squared_norm, shape=(-1, 1)))\n        else:\n            X1_scaled = F.broadcast_mul(X1, inverse_bandwidths)\n            X2_scaled = F.broadcast_mul(X2, inverse_bandwidths)\n            X1_squared_norm = F.sum(F.square(X1_scaled), axis=1)\n            X2_squared_norm = F.sum(F.square(X2_scaled), axis=1)\n            D = -2.0 * F.linalg.gemm2(\n                X1_scaled, X2_scaled, transpose_a=False, transpose_b=True)\n            D = F.broadcast_add(D, F.reshape(X1_squared_norm, shape=(-1, 1)))\n            D = F.broadcast_add(D, F.reshape(X2_squared_norm, shape=(1, -1)))\n        return F.abs(D)\n\n    def get_params(self):\n        """"""\n        Parameter keys are inv_bw<k> if dimension > 1, and inv_bw if\n        dimension == 1.\n\n        """"""\n        inverse_bandwidths = encode_unwrap_parameter(\n            mx.nd, self.inverse_bandwidths_internal,\n            self.encoding).asnumpy().reshape((-1,))\n        if inverse_bandwidths.size == 1:\n            return {\'inv_bw\': inverse_bandwidths[0]}\n        else:\n            return {\n                \'inv_bw{}\'.format(k): inverse_bandwidths[k]\n                for k in range(inverse_bandwidths.size)}\n\n    def set_params(self, param_dict):\n        dimension = self.encoding.dimension\n        if dimension == 1:\n            inverse_bandwidths = [param_dict[\'inv_bw\']]\n        else:\n            keys = [\'inv_bw{}\'.format(k) for k in range(dimension)]\n            for k in keys:\n                assert k in param_dict, \\\n                    ""\'{}\' not in param_dict = {}"".format(k, param_dict)\n            inverse_bandwidths = [param_dict[k] for k in keys]\n        self.encoding.set(self.inverse_bandwidths_internal, inverse_bandwidths)\n\n\nclass Matern52(KernelFunction):\n    """"""\n    HybridBlock that is responsible for the computation of Matern52 kernel\n    matrices. For instance:\n        X1 with size (n1,d)\n        X2 with size (n2,d)\n    results in a matrix of size (n1,n2) with i,j entry equal to the\n    Matern52 kernel at (X1[i,:], X2[j,:]).\n\n    If ARD == False, inverse_bandwidths is equal to a scalar broadcast to the\n    d components (with d=dimension, i.e., the number of features in X)\n    otherwise (ARD == True), inverse_bandwidths is (1,d)\n\n    """"""\n    def __init__(self, dimension, ARD=False, encoding_type=DEFAULT_ENCODING,\n                 **kwargs):\n        super(Matern52, self).__init__(dimension, **kwargs)\n        self.encoding = create_encoding(\n            encoding_type, INITIAL_COVARIANCE_SCALE,\n            COVARIANCE_SCALE_LOWER_BOUND, COVARIANCE_SCALE_UPPER_BOUND, 1,\n            LogNormal(0.0, 1.0))\n        self.ARD = ARD\n        self.squared_distance = SquaredDistance(\n            dimension=dimension, ARD=ARD, encoding_type=encoding_type)\n\n        with self.name_scope():\n            self.covariance_scale_internal = register_parameter(\n                self.params, \'covariance_scale\', self.encoding)\n\n    def hybrid_forward(self, F, X1, X2, covariance_scale_internal):\n        """"""\n        Actual computation of the Matern52 kernel matrix (see details above)\n        See http://www.gaussianprocess.org/gpml/chapters/RW.pdf,\n        equation (4.17)\n\n        :param F: mx.sym or mx.nd\n        :param X1: input data of size (n1, d)\n        :param X2: input data of size (n2, d)\n        """"""\n\n        covariance_scale = self.encoding.get(F, covariance_scale_internal)\n        X1 = self._check_input_shape(F, X1)\n        if X2 is not X1:\n            X2 = self._check_input_shape(F, X2)\n        D = self.squared_distance(X1, X2)\n        # Using the plain F.sqrt is numerically unstable for D ~ 0\n        # (non-differentiability)\n        # that\'s why we add NUMERICAL_JITTER\n        B = F.sqrt(5.0 * D + NUMERICAL_JITTER)\n        K = F.broadcast_mul(\n            (1.0 + B + 5.0 / 3.0 * D) * F.exp(-B), covariance_scale)\n\n        return K\n\n    def _covariance_scale(self, F, X):\n        return encode_unwrap_parameter(\n            F, self.covariance_scale_internal, self.encoding, X)\n\n    def diagonal(self, F, X):\n        X = self._check_input_shape(F, X)\n        covariance_scale = self._covariance_scale(F, X)\n        covariance_scale_times_ones = F.broadcast_mul(\n            F.ones_like(F.slice_axis(X, axis=1, begin=0, end=1)),\n            covariance_scale\n        )\n        return covariance_scale_times_ones.reshape((-1,))\n\n    def diagonal_depends_on_X(self):\n        return False\n\n    def param_encoding_pairs(self):\n        return [\n            (self.covariance_scale_internal, self.encoding),\n            (self.squared_distance.inverse_bandwidths_internal,\n             self.squared_distance.encoding)\n        ]\n\n    def get_covariance_scale(self):\n        return self._covariance_scale(mx.nd, None).asscalar()\n\n    def set_covariance_scale(self, covariance_scale):\n        self.encoding.set(self.covariance_scale_internal, covariance_scale)\n\n    def get_params(self):\n        result = self.squared_distance.get_params()\n        result[\'covariance_scale\'] = self.get_covariance_scale()\n        return result\n\n    def set_params(self, param_dict):\n        self.squared_distance.set_params(param_dict)\n        self.set_covariance_scale(param_dict[\'covariance_scale\'])\n'"
autogluon/searcher/bayesopt/gpmxnet/kernel/coregionalization.py,0,"b'import mxnet as mx\n\nfrom autogluon.searcher.bayesopt.gpmxnet.kernel.base import KernelFunction\nfrom autogluon.searcher.bayesopt.gpmxnet.utils import create_encoding\nfrom autogluon.searcher.bayesopt.gpmxnet.constants import \\\n    INITIAL_NOISE_VARIANCE, NOISE_VARIANCE_LOWER_BOUND, \\\n    NOISE_VARIANCE_UPPER_BOUND, DATA_TYPE, DEFAULT_ENCODING\nfrom autogluon.searcher.bayesopt.gpmxnet.gluon_blocks_helpers import \\\n    unwrap_parameter, IdentityScalarEncoding\n\n__all__ = [\'Coregionalization\']\n\n\nclass Coregionalization(KernelFunction):\n    """"""\n    k(i, j) = K_{ij}, where K = W W^T + diag(rho).\n    """"""\n    def __init__(self, num_outputs, num_factors=16,\n                 rho_init=INITIAL_NOISE_VARIANCE,\n                 encoding_type=DEFAULT_ENCODING, **kwargs):\n\n        super(Coregionalization, self).__init__(dimension=1, **kwargs)\n\n        self.encoding_W_flat = IdentityScalarEncoding(\n            dimension=num_outputs * num_factors)\n        self.encoding_rho = create_encoding(encoding_type, rho_init,\n                                            NOISE_VARIANCE_LOWER_BOUND,\n                                            NOISE_VARIANCE_UPPER_BOUND,\n                                            dimension=1)\n\n        self.num_outputs = num_outputs\n        self.num_factors = num_factors\n\n        with self.name_scope():\n            self.W_flat_internal = self.params.get(\n                ""W_internal"", shape=(num_outputs * num_factors,),\n                init=mx.init.Normal(),  # TODO: Use Xavier initialization here\n                dtype=DATA_TYPE)\n            self.rho_internal = self.params.get(\n                ""rho_internal"", shape=(1,),\n                init=mx.init.Constant(self.encoding_rho.init_val_int),\n                dtype=DATA_TYPE)\n\n    @staticmethod\n    def _meshgrid(F, a, b):\n        """"""\n        Return coordinate matrices from coordinate vectors.\n\n        Like https://docs.scipy.org/doc/numpy/reference/generated/numpy.meshgrid.html\n        (with Cartesian indexing), but only supports two coordinate vectors as input.\n\n        :param a: 1-D array representing the coordinates of a grid (length n) \n        :param b: 1-D array representing the coordinates of a grid (length m) \n        :return: coordinate matrix. 3-D array of shape (2, m, n).\n        """"""\n        aa = F.broadcast_mul(F.ones_like(F.expand_dims(a, axis=-1)), b)\n        bb = F.broadcast_mul(F.ones_like(F.expand_dims(b, axis=-1)), a)\n        return F.stack(bb, F.transpose(aa), axis=0)\n\n    def _compute_gram_matrix(self, F, W_flat, rho):\n        W = F.reshape(W_flat, shape=(self.num_outputs, self.num_factors))\n        rho_vec = F.broadcast_mul(rho, F.ones(self.num_outputs, dtype=DATA_TYPE))\n        return F.linalg.syrk(W) + F.diag(rho_vec)\n\n    def hybrid_forward(self, F, ind1, ind2, W_flat_internal, rho_internal):\n        W_flat = self.encoding_W_flat.get(F, W_flat_internal)\n        rho = self.encoding_rho.get(F, rho_internal)\n        K = self._compute_gram_matrix(F, W_flat, rho)\n        ind1 = self._check_input_shape(F, ind1)\n        if ind2 is not ind1:\n            ind2 = self._check_input_shape(F, ind2)\n        ind = self._meshgrid(F, ind1, ind2)\n        return F.transpose(F.squeeze(F.gather_nd(K, ind)))\n\n    def diagonal(self, F, ind):\n        ind = self._check_input_shape(F, ind)\n        W_flat = self.encoding_W_flat.get(F, unwrap_parameter(F, self.W_flat_internal, ind))\n        rho = self.encoding_rho.get(F, unwrap_parameter(F, self.rho_internal, ind))\n        K = self._compute_gram_matrix(F, W_flat, rho)\n        K_diag = F.diag(K)\n        return F.take(K_diag, ind)\n\n    def diagonal_depends_on_X(self):\n        return True\n\n    def param_encoding_pairs(self):\n        return [\n            (self.W_flat_internal, self.encoding_W_flat),\n            (self.rho_internal, self.encoding_rho),\n        ]\n'"
autogluon/searcher/bayesopt/gpmxnet/kernel/exponential_decay.py,0,"b'import mxnet as mx\n\nfrom autogluon.searcher.bayesopt.gpmxnet.kernel.base import KernelFunction\nfrom autogluon.searcher.bayesopt.gpmxnet.mean import MeanFunction\nfrom autogluon.searcher.bayesopt.gpmxnet.utils import create_encoding, \\\n    register_parameter, get_name_internal\nfrom autogluon.searcher.bayesopt.gpmxnet.constants import DEFAULT_ENCODING\nfrom autogluon.searcher.bayesopt.gpmxnet.gluon_blocks_helpers import \\\n    unwrap_parameter, IdentityScalarEncoding\n\n__all__ = [\'ExponentialDecayResourcesKernelFunction\',\n           \'ExponentialDecayResourcesMeanFunction\']\n\n\nclass ExponentialDecayResourcesKernelFunction(KernelFunction):\n    """"""\n    Variant of the kernel function for modeling exponentially decaying\n    learning curves, proposed in:\n\n        Swersky, K., Snoek, J., & Adams, R. P. (2014).\n        Freeze-Thaw Bayesian Optimization.\n        ArXiv:1406.3896 [Cs, Stat).\n        Retrieved from http://arxiv.org/abs/1406.3896\n\n    The argument in that paper actually justifies using a non-zero mean\n    function (see ExponentialDecayResourcesMeanFunction) and centralizing\n    the kernel proposed there. This is done here. Details in:\n\n        Tiao, Klein, Archambeau, Seeger (2020)\n        Model-based Asynchronous Hyperparameter Optimization\n        https://arxiv.org/abs/2003.10865\n\n    We implement a new family of kernel functions, for which the additive\n    Freeze-Thaw kernel is one instance (delta = 0).\n    The kernel has parameters alpha, mean_lam, gamma > 0, and delta in [0, 1].\n    Note that beta = alpha / mean_lam is used in the Freeze-Thaw paper (the\n    Gamma distribution over lambda is parameterized differently).\n    The additive Freeze-Thaw kernel is obtained for delta = 0 (use\n    delta_fixed_value = 0).\n\n    In fact, this class is configured with a kernel and a mean function over\n    inputs x (dimension d) and represents a kernel (and mean function) over\n    inputs (x, r) (dimension d + 1), where the resource attribute r >= 0 is\n    last.\n\n    """"""\n    def __init__(\n            self, kernel_x: KernelFunction, mean_x: MeanFunction,\n            encoding_type=DEFAULT_ENCODING, alpha_init=1.0, mean_lam_init=0.5,\n            gamma_init=0.5, delta_fixed_value=None, delta_init=0.5,\n            max_metric_value=1.0, **kwargs):\n        """"""\n        :param kernel_x: Kernel k_x(x, x\') over configs\n        :param mean_x: Mean function mu_x(x) over configs\n        :param encoding_type: Encoding used for alpha, mean_lam, gamma (positive\n            values)\n        :param alpha_init: Initial value alpha\n        :param mean_lam_init: Initial value mean_lam\n        :param gamma_init: Initial value gamma\n        :param delta_fixed_value: If not None, delta is fixed to this value, and\n            does not become a free parameter\n        :param delta_init: Initial value delta (if delta_fixed_value is None)\n        :param max_metric_value: Maximum value which metric can attend. This is\n            used as upper bound on gamma\n        """"""\n\n        super(ExponentialDecayResourcesKernelFunction, self).__init__(\n            dimension=kernel_x.dimension + 1, **kwargs)\n        self.kernel_x = kernel_x\n        self.mean_x = mean_x\n        # alpha, mean_lam are parameters of a Gamma distribution, where alpha is\n        # a scale parameter, and\n        #   E[lambda] = mean_lam, Var[lambda] = mean_lam ** 2 / alpha\n        alpha_lower, alpha_upper = 1e-6, 250.0\n        alpha_init = self._wrap_initvals(alpha_init, alpha_lower, alpha_upper)\n        self.encoding_alpha = create_encoding(\n            encoding_type, alpha_init, alpha_lower, alpha_upper, 1, None)\n        mean_lam_lower, mean_lam_upper = 1e-4, 50.0\n        mean_lam_init = self._wrap_initvals(\n            mean_lam_init, mean_lam_lower, mean_lam_upper)\n        self.encoding_mean_lam = create_encoding(\n            encoding_type, mean_lam_init, mean_lam_lower, mean_lam_upper, 1,\n            None)\n        # If f(x, 0) is the metric value at r -> 0, f(x) at r -> infty,\n        # then f(x, 0) = gamma (for delta = 1), or f(x, 0) = gamma + f(x) for\n        # delta = 0. gamma should not be largest than the maximum metric\n        # value.\n        gamma_lower= max_metric_value * 0.0001\n        gamma_upper = max_metric_value\n        gamma_init = self._wrap_initvals(gamma_init, gamma_lower, gamma_upper)\n        self.encoding_gamma = create_encoding(\n            encoding_type, gamma_init, gamma_lower, gamma_upper, 1, None)\n        if delta_fixed_value is None:\n            delta_init = self._wrap_initvals(delta_init, 0.0, 1.0)\n            self.encoding_delta = IdentityScalarEncoding(\n                constr_lower=0.0, constr_upper=1.0, init_val=delta_init)\n        else:\n            assert 0.0 <= delta_fixed_value <= 1.0, \\\n                ""delta_fixed_value = {}, must lie in [0, 1]"".format(\n                    delta_fixed_value)\n            self.encoding_delta = None\n            self.delta_fixed_value = delta_fixed_value\n\n        with self.name_scope():\n            self.alpha_internal = register_parameter(\n                self.params, ""alpha"", self.encoding_alpha)\n            self.mean_lam_internal = register_parameter(\n                self.params, ""mean_lam"", self.encoding_mean_lam)\n            self.gamma_internal = register_parameter(\n                self.params, ""gamma"", self.encoding_gamma)\n            if delta_fixed_value is None:\n                self.delta_internal = register_parameter(\n                    self.params, ""delta"", self.encoding_delta)\n\n    @staticmethod\n    def _wrap_initvals(init, lower, upper):\n        return max(min(init, upper * 0.999), lower * 1.001)\n\n    @staticmethod\n    def _compute_kappa(F, x, alpha, mean_lam):\n        beta = alpha / mean_lam\n        return F.broadcast_power(F.broadcast_div(\n            beta, F.broadcast_add(x, beta)), alpha)\n\n    def _compute_terms(self, F, X, alpha, mean_lam, gamma, delta, ret_mean=False):\n        dim = self.kernel_x.dimension\n        cfg = F.slice_axis(X, axis=1, begin=0, end=dim)\n        res = F.slice_axis(X, axis=1, begin=dim, end=None)\n        kappa = self._compute_kappa(F, res, alpha, mean_lam)\n        kr_pref = F.reshape(gamma, shape=(1, 1))\n        if ret_mean or (self.encoding_delta is not None) or delta > 0.0:\n            mean = self.mean_x(cfg)\n        else:\n            mean = None\n        if self.encoding_delta is not None:\n            kr_pref = F.broadcast_sub(kr_pref, F.broadcast_mul(delta, mean))\n        elif delta > 0.0:\n            kr_pref = F.broadcast_sub(kr_pref, mean * delta)\n        return cfg, res, kappa, kr_pref, mean\n\n    @staticmethod\n    def _unwrap(F, X, kwargs, key, enc, var_internal):\n        return enc.get(\n            F, kwargs.get(get_name_internal(key),\n                          unwrap_parameter(F, var_internal, X)))\n\n    def _get_params(self, F, X, **kwargs):\n        alpha = self._unwrap(\n            F, X, kwargs, \'alpha\', self.encoding_alpha, self.alpha_internal)\n        mean_lam = self._unwrap(\n            F, X, kwargs, \'mean_lam\', self.encoding_mean_lam,\n            self.mean_lam_internal)\n        gamma = self._unwrap(\n            F, X, kwargs, \'gamma\', self.encoding_gamma, self.gamma_internal)\n        if self.encoding_delta is not None:\n            delta = F.reshape(self._unwrap(\n                F, X, kwargs, \'delta\', self.encoding_delta,\n                self.delta_internal), shape=(1, 1))\n        else:\n            delta = self.delta_fixed_value\n        return (alpha, mean_lam, gamma, delta)\n\n    def hybrid_forward(self, F, X1, X2, **kwargs):\n        alpha, mean_lam, gamma, delta = self._get_params(F, X1, **kwargs)\n\n        cfg1, res1, kappa1, kr_pref1, _ = self._compute_terms(\n            F, X1, alpha, mean_lam, gamma, delta)\n        if X2 is not X1:\n            cfg2, res2, kappa2, kr_pref2, _ = self._compute_terms(\n                F, X2, alpha, mean_lam, gamma, delta)\n        else:\n            cfg2, res2, kappa2, kr_pref2 = cfg1, res1, kappa1, kr_pref1\n        res2 = F.reshape(res2, shape=(1, -1))\n        kappa2 = F.reshape(kappa2, shape=(1, -1))\n        kr_pref2 = F.reshape(kr_pref2, shape=(1, -1))\n        kappa12 = self._compute_kappa(\n            F, F.broadcast_add(res1, res2), alpha, mean_lam)\n\n        kmat_res = F.broadcast_sub(kappa12, F.broadcast_mul(kappa1, kappa2))\n        kmat_res = F.broadcast_mul(kr_pref1, F.broadcast_mul(\n            kr_pref2, kmat_res))\n        kmat_x = self.kernel_x(cfg1, cfg2)\n        if self.encoding_delta is None:\n            if delta > 0.0:\n                tmpmat = F.broadcast_add(kappa1, F.broadcast_sub(\n                    kappa2, kappa12 * delta))\n                tmpmat = tmpmat * (-delta) + 1.0\n            else:\n                tmpmat = 1.0\n        else:\n            tmpmat = F.broadcast_add(kappa1, F.broadcast_sub(\n                kappa2, F.broadcast_mul(kappa12, delta)))\n            tmpmat = F.broadcast_mul(tmpmat, -delta) + 1.0\n\n        return kmat_x * tmpmat + kmat_res\n\n    def diagonal(self, F, X):\n        alpha, mean_lam, gamma, delta = self._get_params(F, X)\n\n        cfg, res, kappa, kr_pref, _ = self._compute_terms(\n            F, X, alpha, mean_lam, gamma, delta)\n        kappa2 = self._compute_kappa(F, res * 2, alpha, mean_lam)\n\n        kdiag_res = F.broadcast_sub(kappa2, F.square(kappa))\n        kdiag_res = F.reshape(\n            F.broadcast_mul(kdiag_res, F.square(kr_pref)), shape=(-1,))\n        kdiag_x = self.kernel_x.diagonal(F, cfg)\n        if self.encoding_delta is None:\n            if delta > 0.0:\n                tmpvec = F.broadcast_sub(kappa * 2, kappa2 * delta)\n                tmpvec = F.reshape(tmpvec * (-delta) + 1.0, shape=(-1,))\n            else:\n                tmpvec = 1.0\n        else:\n            tmpvec = F.broadcast_sub(kappa * 2, F.broadcast_mul(kappa2, delta))\n            tmpvec = F.reshape(\n                F.broadcast_mul(tmpvec, -delta) + 1.0, shape=(-1,))\n\n        return kdiag_x * tmpvec + kdiag_res\n\n    def diagonal_depends_on_X(self):\n        return True\n\n    def param_encoding_pairs(self):\n        enc_list = [\n            (self.alpha_internal, self.encoding_alpha),\n            (self.mean_lam_internal, self.encoding_mean_lam),\n            (self.gamma_internal, self.encoding_gamma)]\n        if self.encoding_delta is not None:\n            enc_list.append((self.delta_internal, self.encoding_delta))\n        enc_list.extend(self.kernel_x.param_encoding_pairs())\n        enc_list.extend(self.mean_x.param_encoding_pairs())\n        return enc_list\n\n    def mean_function(self, F, X):\n        alpha, mean_lam, gamma, delta = self._get_params(F, X)\n        cfg, res, kappa, kr_pref, mean = self._compute_terms(\n            F, X, alpha, mean_lam, gamma, delta, ret_mean=True)\n\n        return F.broadcast_add(mean, F.broadcast_mul(kappa, kr_pref))\n\n    def get_params(self):\n        """"""\n        Parameter keys are alpha, mean_lam, gamma, delta (only if not fixed\n        to delta_fixed_value), as well as those of self.kernel_x (prefix\n        \'kernelx_\') and of self.mean_x (prefix \'meanx_\').\n\n        """"""\n        values = list(self._get_params(mx.nd, None))\n        keys = [\'alpha\', \'mean_lam\', \'gamma\', \'delta\']\n        if self.encoding_delta is None:\n            values.pop()\n            keys.pop()\n        result = {k: v.reshape((1,)).asscalar() for k, v in zip(keys, values)}\n        for pref, func in [(\'kernelx_\', self.kernel_x), (\'meanx_\', self.mean_x)]:\n            result.update({\n                (pref + k): v for k, v in func.get_params().items()})\n        return result\n\n    def set_params(self, param_dict):\n        for pref, func in [(\'kernelx_\', self.kernel_x), (\'meanx_\', self.mean_x)]:\n            len_pref = len(pref)\n            stripped_dict = {\n                k[len_pref:]: v for k, v in param_dict.items()\n                if k.startswith(pref)}\n            func.set_params(stripped_dict)\n        self.encoding_alpha.set(self.alpha_internal, param_dict[\'alpha\'])\n        self.encoding_mean_lam.set(\n            self.mean_lam_internal, param_dict[\'mean_lam\'])\n        self.encoding_gamma.set(self.gamma_internal, param_dict[\'gamma\'])\n        if self.encoding_delta is not None:\n            self.encoding_delta.set(self.delta_internal, param_dict[\'delta\'])\n\n\nclass ExponentialDecayResourcesMeanFunction(MeanFunction):\n    def __init__(self, kernel: ExponentialDecayResourcesKernelFunction,\n                 **kwargs):\n        super(ExponentialDecayResourcesMeanFunction, self).__init__(**kwargs)\n        assert isinstance(kernel, ExponentialDecayResourcesKernelFunction)\n        self.kernel = kernel\n\n    def hybrid_forward(self, F, X):\n        return self.kernel.mean_function(F, X)\n\n    def param_encoding_pairs(self):\n        return []\n\n    def get_params(self):\n        return dict()\n\n    def set_params(self, param_dict):\n        pass\n'"
autogluon/searcher/bayesopt/gpmxnet/kernel/fabolas.py,0,"b'import mxnet as mx\n\nfrom autogluon.searcher.bayesopt.gpmxnet.kernel.base import KernelFunction\nfrom autogluon.searcher.bayesopt.gpmxnet.utils import create_encoding, \\\n    register_parameter\nfrom autogluon.searcher.bayesopt.gpmxnet.constants import \\\n    COVARIANCE_SCALE_LOWER_BOUND, COVARIANCE_SCALE_UPPER_BOUND, DEFAULT_ENCODING\nfrom autogluon.searcher.bayesopt.gpmxnet.gluon_blocks_helpers import \\\n    encode_unwrap_parameter, IdentityScalarEncoding\n\n__all__ = [\'FabolasKernelFunction\']\n\n\nclass FabolasKernelFunction(KernelFunction):\n    """"""\n    The kernel function proposed in:\n\n        Klein, A., Falkner, S., Bartels, S., Hennig, P., & Hutter, F. (2016).\n        Fast Bayesian Optimization of Machine Learning Hyperparameters\n        on Large Datasets, in AISTATS 2017.\n        ArXiv:1605.07079 [Cs, Stat]. Retrieved from http://arxiv.org/abs/1605.07079\n\n    Please note this is only one of the components of the factorized kernel\n    proposed in the paper. This is the finite-rank (""degenerate"") kernel for\n    modelling data subset fraction sizes. Defined as:\n\n        k(x, y) = (U phi(x))^T (U phi(y)),  x, y in [0, 1],\n        phi(x) = [1, (1 - x)^2]^T,  U = [[u1, u3], [0, u2]] upper triangular,\n        u1, u2 > 0.\n    """"""\n\n    def __init__(self, encoding_type=DEFAULT_ENCODING,\n                 u1_init=1.0, u3_init=0.0, **kwargs):\n\n        super(FabolasKernelFunction, self).__init__(dimension=1, **kwargs)\n\n        self.encoding_u12 = create_encoding(\n            encoding_type, u1_init, COVARIANCE_SCALE_LOWER_BOUND,\n            COVARIANCE_SCALE_UPPER_BOUND, 1, None)\n        # This is not really needed, but param_encoding_pairs needs an encoding\n        # for each parameter\n        self.encoding_u3 = IdentityScalarEncoding(init_val=u3_init)\n        with self.name_scope():\n            self.u1_internal = register_parameter(\n                self.params, \'u1\', self.encoding_u12)\n            self.u2_internal = register_parameter(\n                self.params, \'u2\', self.encoding_u12)\n            self.u3_internal = register_parameter(\n                self.params, \'u3\', self.encoding_u3)\n\n    @staticmethod\n    def _compute_factor(F, x, u1, u2, u3):\n        tvec = (1.0 - x) ** 2\n        return F.concat(\n            F.broadcast_add(F.broadcast_mul(tvec, u3), u1),\n            F.broadcast_mul(tvec, u2), dim=1)\n\n    def hybrid_forward(self, F, X1, X2, u1_internal, u2_internal, u3_internal):\n        X1 = self._check_input_shape(F, X1)\n\n        u1 = self.encoding_u12.get(F, u1_internal)\n        u2 = self.encoding_u12.get(F, u2_internal)\n        u3 = self.encoding_u3.get(F, u3_internal)\n\n        mat1 = self._compute_factor(F, X1, u1, u2, u3)\n        if X2 is X1:\n            return F.linalg.syrk(mat1, transpose=False)\n        else:\n            X2 = self._check_input_shape(F, X2)\n            mat2 = self._compute_factor(F, X2, u1, u2, u3)\n            return F.dot(mat1, mat2, transpose_a=False, transpose_b=True)\n\n    def _get_pars(self, F, X):\n        u1 = encode_unwrap_parameter(F, self.u1_internal, self.encoding_u12, X)\n        u2 = encode_unwrap_parameter(F, self.u2_internal, self.encoding_u12, X)\n        u3 = encode_unwrap_parameter(F, self.u3_internal, self.encoding_u3, X)\n        return (u1, u2, u3)\n\n    def diagonal(self, F, X):\n        X = self._check_input_shape(F, X)\n        u1, u2, u3 = self._get_pars(F, X)\n        mat = self._compute_factor(F, X, u1, u2, u3)\n        return F.sum(mat ** 2, axis=1)\n\n    def diagonal_depends_on_X(self):\n        return True\n\n    def param_encoding_pairs(self):\n        return [\n            (self.u1_internal, self.encoding_u12),\n            (self.u2_internal, self.encoding_u12),\n            (self.u3_internal, self.encoding_u3)\n        ]\n\n    def get_params(self):\n        values = list(self._get_pars(mx.nd, None))\n        keys = [\'u1\', \'u2\', \'u3\']\n        return {k: v.reshape((1,)).asscalar() for k, v in zip(keys, values)}\n\n    def set_params(self, param_dict):\n        self.encoding_u12.set(self.u1_internal, param_dict[\'u1\'])\n        self.encoding_u12.set(self.u2_internal, param_dict[\'u2\'])\n        self.encoding_u3.set(self.u3_internal, param_dict[\'u3\'])\n'"
autogluon/searcher/bayesopt/gpmxnet/kernel/product_kernel.py,0,"b'from autogluon.searcher.bayesopt.gpmxnet.kernel.base import KernelFunction\n\n__all__ = [\'ProductKernelFunction\']\n\n\nclass ProductKernelFunction(KernelFunction):\n    """"""\n    Given two kernel functions K1, K2, this class represents the product kernel\n    function given by\n\n        ((x1, x2), (y1, y2)) -> K(x1, y1) * K(x2, y2)\n\n    We assume that parameters of K1 and K2 are disjoint.\n\n    """"""\n    def __init__(self, kernel1: KernelFunction, kernel2: KernelFunction,\n                 name_prefixes=None, **kwargs):\n        """"""\n        :param kernel1: Kernel function K1\n        :param kernel2: Kernel function K2\n        :param name_prefixes: Name prefixes for K1, K2 used in get_params\n\n        """"""\n        super(ProductKernelFunction, self).__init__(\n            kernel1.dimension + kernel2.dimension, **kwargs)\n        self.kernel1 = kernel1\n        self.kernel2 = kernel2\n        if name_prefixes is None:\n            self.name_prefixes = [\'kernel1\', \'kernel2\']\n        else:\n            assert len(name_prefixes) == 2\n            self.name_prefixes = name_prefixes\n\n    def hybrid_forward(self, F, X1, X2):\n        # Note: In the symbolic case, we cannot check whether the shape of the\n        # inputs is correct. By sending the remaining dimensions to\n        # self.kernel2, evaluations may fail there (instead of silently doing\n        # the wrong thing)\n        d1 = self.kernel1.dimension\n        X1_1 = F.slice_axis(X1, axis=1, begin=0, end=d1)\n        X1_2 = F.slice_axis(X1, axis=1, begin=d1, end=None)\n        X2_1 = F.slice_axis(X2, axis=1, begin=0, end=d1)\n        X2_2 = F.slice_axis(X2, axis=1, begin=d1, end=None)\n        kmat1 = self.kernel1(X1_1, X2_1)\n        kmat2 = self.kernel2(X1_2, X2_2)\n        return kmat1 * kmat2\n\n    def diagonal(self, F, X):\n        # Note: In the symbolic case, we cannot check whether the shape of the\n        # inputs is correct. By sending the remaining dimensions to\n        # self.kernel2, evaluations may fail there (instead of silently doing\n        # the wrong thing)\n        d1 = self.kernel1.dimension\n        X1 = F.slice_axis(X, axis=1, begin=0, end=d1)\n        X2 = F.slice_axis(X, axis=1, begin=d1, end=None)\n        diag1 = self.kernel1.diagonal(F, X1)\n        diag2 = self.kernel2.diagonal(F, X2)\n        return diag1 * diag2\n\n    def diagonal_depends_on_X(self):\n        return (self.kernel1.diagonal_depends_on_X() or\n                self.kernel2.diagonal_depends_on_X())\n\n    def param_encoding_pairs(self):\n        """"""\n        Note: We assume that K1 and K2 have disjoint parameters, otherwise\n        there will be a redundancy here.\n        """"""\n        return self.kernel1.param_encoding_pairs() + \\\n               self.kernel2.param_encoding_pairs()\n\n    def get_params(self):\n        result = dict()\n        prefs = [k + \'_\' for k in self.name_prefixes]\n        for pref, kernel in zip(prefs, [self.kernel1, self.kernel2]):\n            result.update({\n                (pref + k): v for k, v in kernel.get_params().items()})\n        return result\n\n    def set_params(self, param_dict):\n        prefs = [k + \'_\' for k in self.name_prefixes]\n        for pref, kernel in zip(prefs, [self.kernel1, self.kernel2]):\n            len_pref = len(pref)\n            stripped_dict = {\n                k[len_pref:]: v for k, v in param_dict.items()\n                if k.startswith(pref)}\n            kernel.set_params(stripped_dict)\n'"
autogluon/utils/tabular/ml/learner/__init__.py,0,b''
autogluon/utils/tabular/ml/learner/abstract_learner.py,0,"b'import datetime\nimport json\nimport logging\nimport os\nimport random\nimport time\nimport warnings\nfrom collections import OrderedDict\n\nimport numpy as np\nimport pandas as pd\nfrom numpy import corrcoef\nfrom pandas import DataFrame, Series\nfrom sklearn.metrics import accuracy_score, balanced_accuracy_score, matthews_corrcoef, f1_score, classification_report  # , roc_curve, auc\nfrom sklearn.metrics import mean_absolute_error, explained_variance_score, r2_score, mean_squared_error, median_absolute_error  # , max_error\n\nfrom ..constants import BINARY, MULTICLASS, REGRESSION\nfrom ..trainer.abstract_trainer import AbstractTrainer\nfrom ..tuning.ensemble_selection import EnsembleSelection\nfrom ..utils import get_pred_from_proba, get_leaderboard_pareto_frontier\nfrom ...data.label_cleaner import LabelCleaner, LabelCleanerMulticlassToBinary\nfrom ...utils.loaders import load_pkl, load_pd\nfrom ...utils.savers import save_pkl, save_pd, save_json\n\nlogger = logging.getLogger(__name__)\n\n\n# TODO: - Semi-supervised learning\n# TODO: - Minimize memory usage of DataFrames (convert int64 -> uint8 when possible etc.)\n# Learner encompasses full problem, loading initial data, feature generation, model training, model prediction\n# TODO: Loading learner from S3 on Windows may cause issues due to os.path.sep\nclass AbstractLearner:\n    learner_file_name = \'learner.pkl\'\n    learner_info_name = \'info.pkl\'\n    learner_info_json_name = \'info.json\'\n\n    def __init__(self, path_context: str, label: str, id_columns: list, feature_generator, label_count_threshold=10,\n                 problem_type=None, objective_func=None, stopping_metric=None, is_trainer_present=False, random_seed=0):\n        self.path, self.model_context, self.latest_model_checkpoint, self.eval_result_path, self.pred_cache_path, self.save_path = self.create_contexts(path_context)\n        self.label = label\n        self.submission_columns = id_columns\n        self.threshold = label_count_threshold\n        self.problem_type = problem_type\n        self.trainer_problem_type = None\n        self.objective_func = objective_func\n        self.stopping_metric = stopping_metric\n        self.is_trainer_present = is_trainer_present\n        if random_seed is None:\n            random_seed = random.randint(0, 1000000)\n        self.random_seed = random_seed\n        self.cleaner = None\n        self.label_cleaner: LabelCleaner = None\n        self.feature_generator = feature_generator\n        self.feature_generators = [self.feature_generator]\n\n        self.trainer: AbstractTrainer = None\n        self.trainer_type = None\n        self.trainer_path = None\n        self.reset_paths = False\n\n        self.time_fit_total = None\n        self.time_fit_preprocessing = None\n        self.time_fit_training = None\n        self.time_limit = None\n\n    @property\n    def class_labels(self):\n        return self.label_cleaner.ordered_class_labels\n\n    def set_contexts(self, path_context):\n        self.path, self.model_context, self.latest_model_checkpoint, self.eval_result_path, self.pred_cache_path, self.save_path = self.create_contexts(path_context)\n\n    def create_contexts(self, path_context):\n        model_context = path_context + \'models\' + os.path.sep\n        latest_model_checkpoint = model_context + \'model_checkpoint_latest.pointer\'\n        eval_result_path = model_context + \'eval_result.pkl\'\n        predictions_path = path_context + \'predictions.csv\'\n        save_path = path_context + self.learner_file_name\n        return path_context, model_context, latest_model_checkpoint, eval_result_path, predictions_path, save_path\n\n    def fit(self, X: DataFrame, X_test: DataFrame = None, scheduler_options=None, hyperparameter_tune=True,\n            feature_prune=False, holdout_frac=0.1, hyperparameters=None, verbosity=2):\n        raise NotImplementedError\n\n    # TODO: Add pred_proba_cache functionality as in predict()\n    def predict_proba(self, X_test: DataFrame, model=None, as_pandas=False, as_multiclass=False, inverse_transform=True):\n        X_test = self.transform_features(X_test)\n        y_pred_proba = self.load_trainer().predict_proba(X_test, model=model)\n        if inverse_transform:\n            y_pred_proba = self.label_cleaner.inverse_transform_proba(y_pred_proba)\n        if as_multiclass and (self.problem_type == BINARY):\n            y_pred_proba = LabelCleanerMulticlassToBinary.convert_binary_proba_to_multiclass_proba(y_pred_proba)\n        if as_pandas:\n            if self.problem_type == MULTICLASS or (as_multiclass and self.problem_type == BINARY):\n                y_pred_proba = pd.DataFrame(data=y_pred_proba, columns=self.class_labels)\n            else:\n                y_pred_proba = pd.Series(data=y_pred_proba, name=self.label)\n        return y_pred_proba\n\n    # TODO: Add decorators for cache functionality, return core code to previous state\n    # use_pred_cache to check for a cached prediction of rows, can dramatically speedup repeated runs\n    # add_to_pred_cache will update pred_cache with new predictions\n    def predict(self, X_test: DataFrame, model=None, as_pandas=False, use_pred_cache=False, add_to_pred_cache=False):\n        pred_cache = None\n        if use_pred_cache or add_to_pred_cache:\n            try:\n                pred_cache = load_pd.load(path=self.pred_cache_path, dtype=X_test[self.submission_columns].dtypes.to_dict())\n            except Exception:\n                pass\n\n        if use_pred_cache and (pred_cache is not None):\n            X_id = X_test[self.submission_columns]\n            X_in_cache_with_pred = pd.merge(left=X_id.reset_index(), right=pred_cache, on=self.submission_columns).set_index(\'index\')  # Will break if \'index\' == self.label or \'index\' in self.submission_columns\n            X_test_cache_miss = X_test[~X_test.index.isin(X_in_cache_with_pred.index)]\n            logger.log(20, f\'Using cached predictions for {len(X_in_cache_with_pred)} out of {len(X_test)} rows, \'\n                           f\'which have already been predicted previously. To make new predictions, set use_pred_cache=False\')\n        else:\n            X_in_cache_with_pred = pd.DataFrame(data=None, columns=self.submission_columns + [self.label])\n            X_test_cache_miss = X_test\n\n        if len(X_test_cache_miss) > 0:\n            y_pred_proba = self.predict_proba(X_test=X_test_cache_miss, model=model, inverse_transform=False)\n            problem_type = self.trainer_problem_type or self.problem_type\n            y_pred = get_pred_from_proba(y_pred_proba=y_pred_proba, problem_type=problem_type)\n            y_pred = self.label_cleaner.inverse_transform(pd.Series(y_pred))\n            y_pred.index = X_test_cache_miss.index\n        else:\n            logger.debug(\'All X_test rows found in cache, no need to load model\')\n            y_pred = X_in_cache_with_pred[self.label].values\n            if as_pandas:\n                y_pred = pd.Series(data=y_pred, name=self.label)\n            return y_pred\n\n        if add_to_pred_cache:\n            X_id_with_y_pred = X_test_cache_miss[self.submission_columns].copy()\n            X_id_with_y_pred[self.label] = y_pred\n            if pred_cache is None:\n                pred_cache = X_id_with_y_pred.drop_duplicates(subset=self.submission_columns).reset_index(drop=True)\n            else:\n                pred_cache = pd.concat([X_id_with_y_pred, pred_cache]).drop_duplicates(subset=self.submission_columns).reset_index(drop=True)\n            save_pd.save(path=self.pred_cache_path, df=pred_cache)\n\n        if len(X_in_cache_with_pred) > 0:\n            y_pred = pd.concat([y_pred, X_in_cache_with_pred[self.label]]).reindex(X_test.index)\n\n        y_pred = y_pred.values\n        if as_pandas:\n            y_pred = pd.Series(data=y_pred, name=self.label)\n        return y_pred\n\n    def get_inputs_to_stacker(self, dataset=None, model=None, base_models: list = None):\n        if model is not None or base_models is not None:\n            if model is not None and base_models is not None:\n                raise AssertionError(\'Only one of `model`, `base_models` is allowed to be set.\')\n\n        trainer = self.load_trainer()\n        if dataset is None:\n            if trainer.bagged_mode:\n                dataset_preprocessed = trainer.load_X_train()\n                fit = True\n            else:\n                dataset_preprocessed = trainer.load_X_val()\n                fit = False\n        else:\n            dataset_preprocessed = self.transform_features(dataset)\n            fit = False\n        if base_models is not None:\n            dataset_preprocessed = trainer.get_inputs_to_stacker_v2(X=dataset_preprocessed, base_models=base_models, fit=fit)\n        elif model is not None:\n            base_models = list(trainer.model_graph.predecessors(model))\n            dataset_preprocessed = trainer.get_inputs_to_stacker_v2(X=dataset_preprocessed, base_models=base_models, fit=fit)\n            # Note: Below doesn\'t quite work here because weighted_ensemble has unique input format returned that isn\'t a DataFrame.\n            # dataset_preprocessed = trainer.get_inputs_to_model(model=model_to_get_inputs_for, X=dataset_preprocessed, fit=fit)\n\n        return dataset_preprocessed\n\n    # TODO: Experimental, not integrated with core code, highly subject to change\n    # TODO: Add X, y parameters -> Requires proper preprocessing of train data\n    # X should be X_train from original fit call, if None then load saved X_train in trainer (if save_data=True)\n    # y should be y_train from original fit call, if None then load saved y_train in trainer (if save_data=True)\n    # Compresses bagged ensembles to a single model fit on 100% of the data.\n    # Results in worse model quality (-), but much faster inference times (+++), reduced memory usage (+++), and reduced space usage (+++).\n    # You must have previously called fit() with cache_data=True.\n    def refit_single_full(self, models=None):\n        X = None\n        y = None\n        if X is not None:\n            if y is None:\n                X, y = self.extract_label(X)\n            X = self.transform_features(X)\n            y = self.label_cleaner.transform(y)\n        else:\n            y = None\n        trainer = self.load_trainer()\n        return trainer.refit_single_full(X=X, y=y, models=models)\n\n    # Fits _FULL models and links them in the stack so _FULL models only use other _FULL models as input during stacking\n    # If model is specified, will fit all _FULL models that are ancestors of the provided model, automatically linking them.\n    # If no model is specified, all models are refit and linked appropriately.\n    def refit_ensemble_full(self, model=\'all\'):\n        trainer = self.load_trainer()\n        return trainer.refit_ensemble_full(model=model)\n\n    def fit_transform_features(self, X, y=None):\n        for feature_generator in self.feature_generators:\n            X = feature_generator.fit_transform(X, y)\n        return X\n\n    def transform_features(self, X):\n        for feature_generator in self.feature_generators:\n            X = feature_generator.transform(X)\n        return X\n\n    def score(self, X: DataFrame, y=None, model=None):\n        if y is None:\n            X, y = self.extract_label(X)\n        X = self.transform_features(X)\n        y = self.label_cleaner.transform(y)\n        trainer = self.load_trainer()\n        if self.problem_type == MULTICLASS:\n            y = y.fillna(-1)\n            if (not trainer.objective_func_expects_y_pred) and (-1 in y.unique()):\n                # log_loss / pac_score\n                raise ValueError(f\'Multiclass scoring with eval_metric=\\\'{self.objective_func.name}\\\' does not support unknown classes.\')\n        return trainer.score(X=X, y=y, model=model)\n\n    # Scores both learner and all individual models, along with computing the optimal ensemble score + weights (oracle)\n    def score_debug(self, X: DataFrame, y=None, compute_oracle=False, silent=False):\n        if y is None:\n            X, y = self.extract_label(X)\n        X = self.transform_features(X)\n        y = self.label_cleaner.transform(y)\n        trainer = self.load_trainer()\n        if self.problem_type == MULTICLASS:\n            y = y.fillna(-1)\n            if (not trainer.objective_func_expects_y_pred) and (-1 in y.unique()):\n                # log_loss / pac_score\n                raise ValueError(f\'Multiclass scoring with eval_metric=\\\'{self.objective_func.name}\\\' does not support unknown classes.\')\n\n        scores = {}\n        all_trained_models = trainer.get_model_names_all()\n        all_trained_models_can_infer = trainer.get_model_names_all(can_infer=True)\n        all_trained_models_original = all_trained_models.copy()\n        model_pred_proba_dict, pred_time_test_marginal = trainer.get_model_pred_proba_dict(X=X, models=all_trained_models_can_infer, fit=False, record_pred_time=True)\n\n        if compute_oracle:\n            pred_probas = list(model_pred_proba_dict.values())\n            ensemble_selection = EnsembleSelection(ensemble_size=100, problem_type=trainer.problem_type, metric=self.objective_func)\n            ensemble_selection.fit(predictions=pred_probas, labels=y, identifiers=None)\n            oracle_weights = ensemble_selection.weights_\n            oracle_pred_time_start = time.time()\n            oracle_pred_proba_norm = [pred * weight for pred, weight in zip(pred_probas, oracle_weights)]\n            oracle_pred_proba_ensemble = np.sum(oracle_pred_proba_norm, axis=0)\n            oracle_pred_time = time.time() - oracle_pred_time_start\n            model_pred_proba_dict[\'oracle_ensemble\'] = oracle_pred_proba_ensemble\n            pred_time_test_marginal[\'oracle_ensemble\'] = oracle_pred_time\n            all_trained_models.append(\'oracle_ensemble\')\n\n        for model_name, pred_proba in model_pred_proba_dict.items():\n            if (trainer.problem_type == BINARY) and (self.problem_type == MULTICLASS):\n                pred_proba = self.label_cleaner.inverse_transform_proba(pred_proba)\n            if trainer.objective_func_expects_y_pred:\n                pred = get_pred_from_proba(y_pred_proba=pred_proba, problem_type=self.problem_type)\n                scores[model_name] = self.objective_func(y, pred)\n            else:\n                scores[model_name] = self.objective_func(y, pred_proba)\n\n        pred_time_test = {}\n        # TODO: Add support for calculating pred_time_test_full for oracle_ensemble, need to copy graph from trainer and add oracle_ensemble to it with proper edges.\n        for model in model_pred_proba_dict.keys():\n            if model in all_trained_models_original:\n                base_model_set = trainer.get_minimum_model_set(model)\n                if len(base_model_set) == 1:\n                    pred_time_test[model] = pred_time_test_marginal[base_model_set[0]]\n                else:\n                    pred_time_test_full_num = 0\n                    for base_model in base_model_set:\n                        pred_time_test_full_num += pred_time_test_marginal[base_model]\n                    pred_time_test[model] = pred_time_test_full_num\n            else:\n                pred_time_test[model] = None\n\n        scored_models = list(scores.keys())\n        for model in all_trained_models:\n            if model not in scored_models:\n                scores[model] = None\n                pred_time_test[model] = None\n                pred_time_test_marginal[model] = None\n\n        logger.debug(\'Model scores:\')\n        logger.debug(str(scores))\n        model_names_final = list(scores.keys())\n        df = pd.DataFrame(\n            data={\n                \'model\': model_names_final,\n                \'score_test\': list(scores.values()),\n                \'pred_time_test\': [pred_time_test[model] for model in model_names_final],\n                \'pred_time_test_marginal\': [pred_time_test_marginal[model] for model in model_names_final],\n            }\n        )\n\n        df = df.sort_values(by=[\'score_test\', \'pred_time_test\'], ascending=[False, True]).reset_index(drop=True)\n\n        leaderboard_df = self.leaderboard(silent=silent)\n\n        df_merged = pd.merge(df, leaderboard_df, on=\'model\', how=\'left\')\n        df_columns_lst = df_merged.columns.tolist()\n        explicit_order = [\n            \'model\',\n            \'score_test\',\n            \'score_val\',\n            \'pred_time_test\',\n            \'pred_time_val\',\n            \'fit_time\',\n            \'pred_time_test_marginal\',\n            \'pred_time_val_marginal\',\n            \'fit_time_marginal\',\n            \'stack_level\',\n            \'can_infer\',\n        ]\n        df_columns_other = [column for column in df_columns_lst if column not in explicit_order]\n        df_columns_new = explicit_order + df_columns_other\n        df_merged = df_merged[df_columns_new]\n\n        return df_merged\n\n    def get_pred_probas_models_and_time(self, X, trainer, model_names):\n        pred_probas_lst = []\n        pred_probas_time_lst = []\n        for model_name in model_names:\n            model = trainer.load_model(model_name)\n            time_start = time.time()\n            pred_probas = trainer.pred_proba_predictions(models=[model], X_test=X)\n            if (self.problem_type == MULTICLASS) and (not trainer.objective_func_expects_y_pred):\n                # Handles case where we need to add empty columns to represent classes that were not used for training\n                pred_probas = [self.label_cleaner.inverse_transform_proba(pred_proba) for pred_proba in pred_probas]\n            time_diff = time.time() - time_start\n            pred_probas_lst += pred_probas\n            pred_probas_time_lst.append(time_diff)\n        return pred_probas_lst, pred_probas_time_lst\n\n    def _remove_missing_labels(self, y_true, y_pred):\n        """"""Removes missing labels and produces warning if any are found.""""""\n        if self.problem_type == REGRESSION:\n            non_missing_boolean_mask = [(y is not None and not np.isnan(y)) for y in y_true]\n        else:\n            non_missing_boolean_mask = [(y is not None and y != \'\') for y in y_true]\n\n        n_missing = len([x for x in non_missing_boolean_mask if not x])\n        if n_missing > 0:\n            y_true = y_true[non_missing_boolean_mask]\n            y_pred = y_pred[non_missing_boolean_mask]\n            warnings.warn(f""There are {n_missing} (out of {len(y_true)}) evaluation datapoints for which the label is missing. ""\n                          f""AutoGluon removed these points from the evaluation, which thus may not be entirely representative. ""\n                          f""You should carefully study why there are missing labels in your evaluation data."")\n        return y_true, y_pred\n\n    def evaluate(self, y_true, y_pred, silent=False, auxiliary_metrics=False, detailed_report=True, high_always_good=False):\n        """""" Evaluate predictions.\n            Args:\n                silent (bool): Should we print which metric is being used as well as performance.\n                auxiliary_metrics (bool): Should we compute other (problem_type specific) metrics in addition to the default metric?\n                detailed_report (bool): Should we computed more-detailed versions of the auxiliary_metrics? (requires auxiliary_metrics=True).\n                high_always_good (bool): If True, this means higher values of returned metric are ALWAYS superior (so metrics like MSE should be returned negated)\n\n            Returns single performance-value if auxiliary_metrics=False.\n            Otherwise returns dict where keys = metrics, values = performance along each metric.\n        """"""\n        assert isinstance(y_true, (np.ndarray, pd.Series))\n        assert isinstance(y_pred, (np.ndarray, pd.Series))\n\n        # TODO: Consider removing _remove_missing_labels, this creates an inconsistency between how .score, .score_debug, and .evaluate compute scores.\n        y_true, y_pred = self._remove_missing_labels(y_true, y_pred)\n        trainer = self.load_trainer()\n        if trainer.objective_func_expects_y_pred:\n            y_pred_cleaned = self.label_cleaner.transform(y_pred)\n            y_true_cleaned = self.label_cleaner.transform(y_true)\n            if self.problem_type == MULTICLASS:\n                y_true_cleaned = y_true_cleaned.fillna(-1)  # map unknown classes to -1\n            performance = self.objective_func(y_true_cleaned, y_pred_cleaned)\n        else:\n            if self.problem_type == MULTICLASS:\n                y_true_cleaned = self.label_cleaner.transform(y_true)\n                y_true_cleaned = y_true_cleaned.fillna(-1)\n                if (not trainer.objective_func_expects_y_pred) and (-1 in y_true_cleaned.unique()):\n                    # log_loss / pac_score\n                    raise ValueError(f\'Multiclass scoring with eval_metric=\\\'{self.objective_func.name}\\\' does not support unknown classes.\')\n                performance = self.objective_func(y_true_cleaned, y_pred)\n            else:\n                performance = self.objective_func(y_true, y_pred)\n\n        metric = self.objective_func.name\n\n        if not high_always_good:\n            sign = self.objective_func._sign\n            performance = performance * sign  # flip negative once again back to positive (so higher is no longer necessarily better)\n\n        if not silent:\n            logger.log(20, f""Evaluation: {metric} on test data: {performance}"")\n\n        if not auxiliary_metrics:\n            return performance\n\n        # Otherwise compute auxiliary metrics:\n        auxiliary_metrics = []\n        if self.problem_type == REGRESSION:  # Adding regression metrics\n            pearson_corr = lambda x, y: corrcoef(x, y)[0][1]\n            pearson_corr.__name__ = \'pearson_correlation\'\n            auxiliary_metrics += [\n                mean_absolute_error, explained_variance_score, r2_score, pearson_corr, mean_squared_error, median_absolute_error,\n                # max_error\n            ]\n        else:  # Adding classification metrics\n            auxiliary_metrics += [accuracy_score, balanced_accuracy_score, matthews_corrcoef]\n            if self.problem_type == BINARY:  # binary-specific metrics\n                # def auc_score(y_true, y_pred): # TODO: this requires y_pred to be probability-scores\n                #     fpr, tpr, _ = roc_curve(y_true, y_pred, pos_label)\n                #   return auc(fpr, tpr)\n                f1micro_score = lambda y_true, y_pred: f1_score(y_true, y_pred, average=\'micro\')\n                f1micro_score.__name__ = f1_score.__name__\n                auxiliary_metrics += [f1micro_score]  # TODO: add auc?\n            elif self.problem_type == MULTICLASS:  # multiclass metrics\n                auxiliary_metrics += []  # TODO: No multi-class specific metrics for now. Include top-5, top-10 accuracy here.\n\n        performance_dict = OrderedDict({metric: performance})\n        for metric_function in auxiliary_metrics:\n            metric_name = metric_function.__name__\n            if metric_name not in performance_dict:\n                try: # only compute auxiliary metrics which do not error (y_pred = class-probabilities may cause some metrics to error)\n                    performance_dict[metric_name] = metric_function(y_true, y_pred)\n                except ValueError:\n                    pass\n\n        if not silent:\n            logger.log(20, ""Evaluations on test data:"")\n            logger.log(20, json.dumps(performance_dict, indent=4))\n\n        if detailed_report and (self.problem_type != REGRESSION):\n            # One final set of metrics to report\n            cl_metric = lambda y_true, y_pred: classification_report(y_true, y_pred, output_dict=True)\n            metric_name = \'classification_report\'\n            if metric_name not in performance_dict:\n                try: # only compute auxiliary metrics which do not error (y_pred = class-probabilities may cause some metrics to error)\n                    performance_dict[metric_name] = cl_metric(y_true, y_pred)\n                except ValueError:\n                    pass\n                if not silent and metric_name in performance_dict:\n                    logger.log(20, ""Detailed (per-class) classification report:"")\n                    logger.log(20, json.dumps(performance_dict[metric_name], indent=4))\n        return performance_dict\n\n    def extract_label(self, X):\n        if self.label not in list(X.columns):\n            raise ValueError(f""Provided DataFrame does not contain label column: {self.label}"")\n        y = X[self.label].copy()\n        X = X.drop(self.label, axis=1)\n        return X, y\n\n    def submit_from_preds(self, X_test: DataFrame, y_pred_proba, save=True, save_proba=False):\n        submission = X_test[self.submission_columns].copy()\n        y_pred = get_pred_from_proba(y_pred_proba=y_pred_proba, problem_type=self.problem_type)\n\n        submission[self.label] = y_pred\n        submission[self.label] = self.label_cleaner.inverse_transform(submission[self.label])\n\n        if save:\n            utcnow = datetime.datetime.utcnow()\n            timestamp_str_now = utcnow.strftime(""%Y%m%d_%H%M%S"")\n            path_submission = self.model_context + \'submissions\' + os.path.sep + \'submission_\' + timestamp_str_now + \'.csv\'\n            path_submission_proba = self.model_context + \'submissions\' + os.path.sep + \'submission_proba_\' + timestamp_str_now + \'.csv\'\n            save_pd.save(path=path_submission, df=submission)\n            if save_proba:\n                submission_proba = pd.DataFrame(y_pred_proba)  # TODO: Fix for multiclass\n                save_pd.save(path=path_submission_proba, df=submission_proba)\n\n        return submission\n\n    def predict_and_submit(self, X_test: DataFrame, save=True, save_proba=False):\n        y_pred_proba = self.predict_proba(X_test=X_test, inverse_transform=False)\n        return self.submit_from_preds(X_test=X_test, y_pred_proba=y_pred_proba, save=save, save_proba=save_proba)\n\n    def leaderboard(self, X=None, y=None, only_pareto_frontier=False, silent=False):\n        if X is not None:\n            leaderboard = self.score_debug(X=X, y=y, silent=True)\n        else:\n            trainer = self.load_trainer()\n            leaderboard = trainer.leaderboard()\n        if only_pareto_frontier:\n            if \'score_test\' in leaderboard.columns and \'pred_time_test\' in leaderboard.columns:\n                score_col = \'score_test\'\n                inference_time_col = \'pred_time_test\'\n            else:\n                score_col = \'score_val\'\n                inference_time_col = \'pred_time_val\'\n            leaderboard = get_leaderboard_pareto_frontier(leaderboard=leaderboard, score_col=score_col, inference_time_col=inference_time_col)\n        if not silent:\n            with pd.option_context(\'display.max_rows\', None, \'display.max_columns\', None, \'display.width\', 1000):\n                print(leaderboard)\n        return leaderboard\n\n    # TODO: cache_data must be set to True to be able to pass X and y as None in this function, otherwise it will error.\n    # Warning: This can take a very, very long time to compute if the data is large and the model is complex.\n    # A value of 0.01 means that the objective metric error would be expected to increase by 0.01 if the feature were removed.\n    # Negative values mean the feature is likely harmful.\n    # model: model (str) to get feature importances for, if None will choose best model.\n    # features: list of feature names that feature importances are calculated for and returned, specify None to get all feature importances.\n    # feature_stage: Whether to compute feature importance on raw original features (\'original\'), transformed features (\'transformed\') or on the features used by the particular model (\'transformed_model\').\n    def get_feature_importance(self, model=None, X=None, y=None, features: list = None, feature_stage=\'original\', subsample_size=1000, silent=False) -> Series:\n        valid_feature_stages = [\'original\', \'transformed\', \'transformed_model\']\n        if feature_stage not in valid_feature_stages:\n            raise ValueError(f\'feature_stage must be one of: {valid_feature_stages}, but was {feature_stage}.\')\n        trainer = self.load_trainer()\n        if X is not None:\n            if y is None:\n                X, y = self.extract_label(X)\n            y = self.label_cleaner.transform(y)\n            X, y = self._remove_nan_label_rows(X, y)\n\n            if feature_stage == \'original\':\n                return trainer._get_feature_importance_raw(model=model, X=X, y=y, features_to_use=features, subsample_size=subsample_size, transform_func=self.transform_features, silent=silent)\n            X = self.transform_features(X)\n        else:\n            if feature_stage == \'original\':\n                raise AssertionError(\'Feature importance `dataset` cannot be None if `feature_stage==\\\'original\\\'`. A test dataset must be specified.\')\n            y = None\n        raw = feature_stage == \'transformed\'\n        return trainer.get_feature_importance(X=X, y=y, model=model, features=features, raw=raw, subsample_size=subsample_size, silent=silent)\n\n    @staticmethod\n    def _remove_nan_label_rows(X, y):\n        if y.isnull().any():\n            y = y.dropna()\n            X = X.loc[y.index]\n        return X, y\n\n    # TODO: Add data info gathering at beginning of .fit() that is used by all learners to add to get_info output\n    # TODO: Add feature inference / feature engineering info to get_info output\n    def get_info(self, include_model_info=False):\n        trainer = self.load_trainer()\n        trainer_info = trainer.get_info(include_model_info=include_model_info)\n        learner_info = {\n            \'path\': self.path,\n            \'label\': self.label,\n            \'time_fit_preprocessing\': self.time_fit_preprocessing,\n            \'time_fit_training\': self.time_fit_training,\n            \'time_fit_total\': self.time_fit_total,\n            \'time_limit\': self.time_limit,\n            \'random_seed\': self.random_seed,\n        }\n\n        learner_info.update(trainer_info)\n        return learner_info\n\n    @staticmethod\n    def get_problem_type(y: Series):\n        """""" Identifies which type of prediction problem we are interested in (if user has not specified).\n            Ie. binary classification, multi-class classification, or regression.\n        """"""\n        if len(y) == 0:\n            raise ValueError(""provided labels cannot have length = 0"")\n        y = y.dropna()  # Remove missing values from y (there should not be any though as they were removed in Learner.general_data_processing())\n        num_rows = len(y)\n\n        unique_values = y.unique()\n        unique_count = len(unique_values)\n        if unique_count > 10:\n            logger.log(20, f\'Here are the first 10 unique label values in your data:  {list(unique_values[:10])}\')\n        else:\n            logger.log(20, f\'Here are the {unique_count} unique label values in your data:  {list(unique_values)}\')\n\n        MULTICLASS_LIMIT = 1000  # if numeric and class count would be above this amount, assume it is regression\n        if num_rows > 1000:\n            REGRESS_THRESHOLD = 0.05  # if the unique-ratio is less than this, we assume multiclass classification, even when labels are integers\n        else:\n            REGRESS_THRESHOLD = 0.1\n\n        if unique_count == 2:\n            problem_type = BINARY\n            reason = ""only two unique label-values observed""\n        elif unique_values.dtype == \'object\':\n            problem_type = MULTICLASS\n            reason = ""dtype of label-column == object""\n        elif np.issubdtype(unique_values.dtype, np.floating):\n            unique_ratio = unique_count / float(num_rows)\n            if (unique_ratio <= REGRESS_THRESHOLD) and (unique_count <= MULTICLASS_LIMIT):\n                try:\n                    can_convert_to_int = np.array_equal(y, y.astype(int))\n                    if can_convert_to_int:\n                        problem_type = MULTICLASS\n                        reason = ""dtype of label-column == float, but few unique label-values observed and label-values can be converted to int""\n                    else:\n                        problem_type = REGRESSION\n                        reason = ""dtype of label-column == float and label-values can\'t be converted to int""\n                except:\n                    problem_type = REGRESSION\n                    reason = ""dtype of label-column == float and label-values can\'t be converted to int""\n            else:\n                problem_type = REGRESSION\n                reason = ""dtype of label-column == float and many unique label-values observed""\n        elif np.issubdtype(unique_values.dtype, np.integer):\n            unique_ratio = unique_count / float(num_rows)\n            if (unique_ratio <= REGRESS_THRESHOLD) and (unique_count <= MULTICLASS_LIMIT):\n                problem_type = MULTICLASS  # TODO: Check if integers are from 0 to n-1 for n unique values, if they have a wide spread, it could still be regression\n                reason = ""dtype of label-column == int, but few unique label-values observed""\n            else:\n                problem_type = REGRESSION\n                reason = ""dtype of label-column == int and many unique label-values observed""\n        else:\n            raise NotImplementedError(\'label dtype\', unique_values.dtype, \'not supported!\')\n        logger.log(25, f""AutoGluon infers your prediction problem is: {problem_type}  (because {reason})."")\n        logger.log(25, f""If this is wrong, please specify `problem_type` argument in fit() instead ""\n                       f""(You may specify problem_type as one of: {[BINARY, MULTICLASS, REGRESSION]})\\n"")\n        return problem_type\n\n    def save(self):\n        save_pkl.save(path=self.save_path, object=self)\n\n    # reset_paths=True if the learner files have changed location since fitting.\n    # TODO: Potentially set reset_paths=False inside load function if it is the same path to avoid re-computing paths on all models\n    # TODO: path_context -> path\n    @classmethod\n    def load(cls, path_context, reset_paths=True):\n        load_path = path_context + cls.learner_file_name\n        obj = load_pkl.load(path=load_path)\n        if reset_paths:\n            obj.set_contexts(path_context)\n            obj.trainer_path = obj.model_context\n            obj.reset_paths = reset_paths\n            # TODO: Still have to change paths of models in trainer + trainer object path variables\n            return obj\n        else:\n            obj.set_contexts(obj.path_context)\n            return obj\n\n    def save_trainer(self, trainer):\n        if self.is_trainer_present:\n            self.trainer = trainer\n            self.save()\n        else:\n            self.trainer_path = trainer.path\n            trainer.save()\n\n    def load_trainer(self) -> AbstractTrainer:\n        if self.is_trainer_present:\n            return self.trainer\n        else:\n            return self.trainer_type.load(path=self.trainer_path, reset_paths=self.reset_paths)\n\n    # TODO: Add to predictor\n    # TODO: Make this safe in large ensemble situations that would result in OOM\n    # Loads all models in memory so that they don\'t have to loaded during predictions\n    def persist_trainer(self, low_memory=False):\n        self.trainer = self.load_trainer()\n        self.is_trainer_present = True\n        if not low_memory:\n            self.trainer.load_models_into_memory()\n            # Warning: After calling this, it is not necessarily safe to save learner or trainer anymore\n            #  If neural network is persisted and then trainer or learner is saved, there will be an exception thrown\n\n    @classmethod\n    def load_info(cls, path, reset_paths=True, load_model_if_required=True):\n        load_path = path + cls.learner_info_name\n        try:\n            return load_pkl.load(path=load_path)\n        except Exception as e:\n            if load_model_if_required:\n                learner = cls.load(path_context=path, reset_paths=reset_paths)\n                return learner.get_info()\n            else:\n                raise e\n\n    def save_info(self, include_model_info=False):\n        info = self.get_info(include_model_info=include_model_info)\n\n        save_pkl.save(path=self.path + self.learner_info_name, object=info)\n        save_json.save(path=self.path + self.learner_info_json_name, obj=info)\n        return info\n'"
autogluon/utils/tabular/ml/learner/default_learner.py,0,"b'import copy\nimport logging\nimport math\nimport time\n\nimport numpy as np\nimport pandas as pd\nfrom pandas import DataFrame\n\nfrom .abstract_learner import AbstractLearner\nfrom ..constants import BINARY, MULTICLASS, REGRESSION\nfrom ..trainer.auto_trainer import AutoTrainer\nfrom ...data.cleaner import Cleaner\nfrom ...data.label_cleaner import LabelCleaner\n\nlogger = logging.getLogger(__name__)\n\n\n# TODO: Add functionality for advanced feature generators such as gl_code_matrix_generator (inter-row dependencies, apply to train differently than test, etc., can only run after train/test split, rerun for each cv fold)\n# TODO: - Differentiate between advanced generators that require fit (stateful, gl_code_matrix) and those that do not (bucket label averaging in SCOT GC 2019)\n# TODO: - Those that do not could be added to preprocessing function of model, but would then have to be recomputed on each model.\n# TODO: Add cv / OOF generator option, so that AutoGluon can be used as a base model in an ensemble stacker\n# Learner encompasses full problem, loading initial data, feature generation, model training, model prediction\nclass DefaultLearner(AbstractLearner):\n    def __init__(self, path_context: str, label: str, id_columns: list, feature_generator, label_count_threshold=10,\n                 problem_type=None, objective_func=None, stopping_metric=None, is_trainer_present=False, random_seed=0, trainer_type=AutoTrainer):\n        super().__init__(path_context=path_context, label=label, id_columns=id_columns, feature_generator=feature_generator, label_count_threshold=label_count_threshold,\n                         problem_type=problem_type, objective_func=objective_func, stopping_metric=stopping_metric, is_trainer_present=is_trainer_present, random_seed=random_seed)\n        self.trainer_type = trainer_type\n\n    # TODO: Add trainer_kwargs to simplify parameter count and extensibility\n    def fit(self, X: DataFrame, X_test: DataFrame = None, scheduler_options=None, hyperparameter_tune=True,\n            feature_prune=False, holdout_frac=0.1, num_bagging_folds=0, num_bagging_sets=1, stack_ensemble_levels=0,\n            hyperparameters=None, time_limit=None, save_data=False, save_bagged_folds=True, verbosity=2):\n        """""" Arguments:\n                X (DataFrame): training data\n                X_test (DataFrame): data used for hyperparameter tuning. Note: final model may be trained using this data as well as training data\n                hyperparameter_tune (bool): whether to tune hyperparameters or simply use default values\n                feature_prune (bool): whether to perform feature selection\n                scheduler_options (tuple: (search_strategy, dict): Options for scheduler\n                holdout_frac (float): Fraction of data to hold out for evaluating validation performance (ignored if X_test != None, ignored if kfolds != 0)\n                num_bagging_folds (int): kfolds used for bagging of models, roughly increases model training time by a factor of k (0: disabled)\n                num_bagging_sets (int): number of repeats of kfold bagging to perform (values must be >= 1),\n                    total number of models trained during bagging = num_bagging_folds * num_bagging_sets\n                stack_ensemble_levels : (int) Number of stacking levels to use in ensemble stacking. Roughly increases model training time by factor of stack_levels+1 (0: disabled)\n                    Default is 0 (disabled). Use values between 1-3 to improve model quality.\n                    Ignored unless kfolds is also set >= 2\n                hyperparameters (dict): keys = hyperparameters + search-spaces for each type of model we should train.\n        """"""\n        if hyperparameters is None:\n            hyperparameters = {\'NN\': {}, \'GBM\': {}}\n        # TODO: if provided, feature_types in X, X_test are ignored right now, need to pass to Learner/trainer and update this documentation.\n        if time_limit:\n            self.time_limit = time_limit\n            logger.log(20, f\'Beginning AutoGluon training ... Time limit = {time_limit}s\')\n        else:\n            self.time_limit = 1e7\n            logger.log(20, \'Beginning AutoGluon training ...\')\n        logger.log(20, f\'AutoGluon will save models to {self.path}\')\n        logger.log(20, f\'Train Data Rows:    {len(X)}\')\n        logger.log(20, f\'Train Data Columns: {len(X.columns)}\')\n        if X_test is not None:\n            logger.log(20, f\'Tuning Data Rows:    {len(X_test)}\')\n            logger.log(20, f\'Tuning Data Columns: {len(X_test.columns)}\')\n        time_preprocessing_start = time.time()\n        logger.log(20, \'Preprocessing data ...\')\n        X, y, X_test, y_test, holdout_frac, num_bagging_folds = self.general_data_processing(X, X_test, holdout_frac, num_bagging_folds)\n        time_preprocessing_end = time.time()\n        self.time_fit_preprocessing = time_preprocessing_end - time_preprocessing_start\n        logger.log(20, f\'\\tData preprocessing and feature engineering runtime = {round(self.time_fit_preprocessing, 2)}s ...\')\n        if time_limit:\n            time_limit_trainer = time_limit - self.time_fit_preprocessing\n        else:\n            time_limit_trainer = None\n\n        trainer = self.trainer_type(\n            path=self.model_context,\n            problem_type=self.trainer_problem_type,\n            objective_func=self.objective_func,\n            stopping_metric=self.stopping_metric,\n            num_classes=self.label_cleaner.num_classes,\n            feature_types_metadata=self.feature_generator.feature_types_metadata,\n            low_memory=True,\n            kfolds=num_bagging_folds,\n            n_repeats=num_bagging_sets,\n            stack_ensemble_levels=stack_ensemble_levels,\n            scheduler_options=scheduler_options,\n            time_limit=time_limit_trainer,\n            save_data=save_data,\n            save_bagged_folds=save_bagged_folds,\n            random_seed=self.random_seed,\n            verbosity=verbosity\n        )\n\n        self.trainer_path = trainer.path\n        if self.objective_func is None:\n            self.objective_func = trainer.objective_func\n        if self.stopping_metric is None:\n            self.stopping_metric = trainer.stopping_metric\n\n        self.save()\n        trainer.train(X, y, X_test, y_test, hyperparameter_tune=hyperparameter_tune, feature_prune=feature_prune, holdout_frac=holdout_frac,\n                      hyperparameters=hyperparameters)\n        self.save_trainer(trainer=trainer)\n        time_end = time.time()\n        self.time_fit_training = time_end - time_preprocessing_end\n        self.time_fit_total = time_end - time_preprocessing_start\n        logger.log(20, f\'AutoGluon training complete, total runtime = {round(self.time_fit_total, 2)}s ...\')\n\n    def general_data_processing(self, X: DataFrame, X_test: DataFrame, holdout_frac: float, num_bagging_folds: int):\n        """""" General data processing steps used for all models. """"""\n        X = copy.deepcopy(X)\n        # TODO: We should probably uncomment the below lines, NaN label should be treated as just another value in multiclass classification -> We will have to remove missing, compute problem type, and add back missing if multiclass\n        # if self.problem_type == MULTICLASS:\n        #     X[self.label] = X[self.label].fillna(\'\')\n\n        # Remove all examples with missing labels from this dataset:\n        missinglabel_inds = [index for index, x in X[self.label].isna().iteritems() if x]\n        if len(missinglabel_inds) > 0:\n            logger.warning(f""Warning: Ignoring {len(missinglabel_inds)} (out of {len(X)}) training examples for which the label value in column \'{self.label}\' is missing"")\n            X = X.drop(missinglabel_inds, axis=0)\n\n        if self.problem_type is None:\n            self.problem_type = self.get_problem_type(X[self.label])\n\n        if X_test is not None and self.label in X_test.columns:\n            # TODO: This is not an ideal solution, instead check if bagging and X_test exists with label, then merge them prior to entering general data processing.\n            #  This solution should handle virtually all cases correctly, only downside is it might cut more classes than it needs to.\n            self.threshold, holdout_frac, num_bagging_folds = self.adjust_threshold_if_necessary(X[self.label], threshold=self.threshold, holdout_frac=1, num_bagging_folds=num_bagging_folds)\n        else:\n            self.threshold, holdout_frac, num_bagging_folds = self.adjust_threshold_if_necessary(X[self.label], threshold=self.threshold, holdout_frac=holdout_frac, num_bagging_folds=num_bagging_folds)\n\n        if (self.objective_func is not None) and (self.objective_func.name in [\'log_loss\', \'pac_score\']) and (self.problem_type == MULTICLASS):\n            X = self.augment_rare_classes(X)\n\n        # Gets labels prior to removal of infrequent classes\n        y_uncleaned = X[self.label].copy()  # .astype(\'category\').cat.categories\n\n        self.cleaner = Cleaner.construct(problem_type=self.problem_type, label=self.label, threshold=self.threshold)\n        # TODO: What if all classes in X are low frequency in multiclass? Currently we would crash. Not certain how many problems actually have this property\n        X = self.cleaner.fit_transform(X)  # TODO: Consider merging cleaner into label_cleaner\n        self.label_cleaner = LabelCleaner.construct(problem_type=self.problem_type, y=X[self.label], y_uncleaned=y_uncleaned)\n        if (self.label_cleaner.num_classes is not None) and (self.label_cleaner.num_classes == 2):\n            self.trainer_problem_type = BINARY\n        else:\n            self.trainer_problem_type = self.problem_type\n\n        X, y = self.extract_label(X)\n        y = self.label_cleaner.transform(y)\n\n        if X_test is not None and self.label in X_test.columns:\n            X_test = self.cleaner.transform(X_test)\n            if len(X_test) == 0:\n                logger.debug(\'All X_test data contained low frequency classes, ignoring X_test and generating from subset of X\')\n                X_test = None\n                y_test = None\n            else:\n                X_test, y_test = self.extract_label(X_test)\n                y_test = self.label_cleaner.transform(y_test)\n        else:\n            y_test = None\n\n        # TODO: Move this up to top of data before removing data, this way our feature generator is better\n        if X_test is not None:\n            # Do this if working with SKLearn models, otherwise categorical features may perform very badly on the test set\n            logger.log(15, \'Performing general data preprocessing with merged train & validation data, so validation performance may not accurately reflect performance on new test data\')\n            X_super = pd.concat([X, X_test], ignore_index=True)\n            X_super = self.feature_generator.fit_transform(X_super, banned_features=self.submission_columns, drop_duplicates=False)\n            X = X_super.head(len(X)).set_index(X.index)\n            X_test = X_super.tail(len(X_test)).set_index(X_test.index)\n            del X_super\n        else:\n            X = self.feature_generator.fit_transform(X, banned_features=self.submission_columns, drop_duplicates=False)\n\n        return X, y, X_test, y_test, holdout_frac, num_bagging_folds\n\n    def adjust_threshold_if_necessary(self, y, threshold, holdout_frac, num_bagging_folds):\n        new_threshold, new_holdout_frac, new_num_bagging_folds = self._adjust_threshold_if_necessary(y, threshold, holdout_frac, num_bagging_folds)\n        if new_threshold != threshold:\n            if new_threshold < threshold:\n                logger.warning(f\'Warning: Updated label_count_threshold from {threshold} to {new_threshold} to avoid cutting too many classes.\')\n        if new_holdout_frac != holdout_frac:\n            if new_holdout_frac > holdout_frac:\n                logger.warning(f\'Warning: Updated holdout_frac from {holdout_frac} to {new_holdout_frac} to avoid cutting too many classes.\')\n        if new_num_bagging_folds != num_bagging_folds:\n            logger.warning(f\'Warning: Updated num_bagging_folds from {num_bagging_folds} to {new_num_bagging_folds} to avoid cutting too many classes.\')\n        return new_threshold, new_holdout_frac, new_num_bagging_folds\n\n    def _adjust_threshold_if_necessary(self, y, threshold, holdout_frac, num_bagging_folds):\n        new_threshold = threshold\n        if self.problem_type == REGRESSION:\n            num_rows = len(y)\n            holdout_frac = max(holdout_frac, 1 / num_rows + 0.001)\n            num_bagging_folds = min(num_bagging_folds, num_rows)\n            return new_threshold, holdout_frac, num_bagging_folds\n\n        if num_bagging_folds < 2:\n            minimum_safe_threshold = math.ceil(1 / holdout_frac)\n        else:\n            minimum_safe_threshold = num_bagging_folds\n\n        if minimum_safe_threshold > new_threshold:\n            new_threshold = minimum_safe_threshold\n\n        class_counts = y.value_counts()\n        total_rows = class_counts.sum()\n        minimum_percent_to_keep = 0.975\n        minimum_rows_to_keep = math.ceil(total_rows * minimum_percent_to_keep)\n        minimum_class_to_keep = 2\n\n        num_classes = len(class_counts)\n        class_counts_valid = class_counts[class_counts >= new_threshold]\n        num_rows_valid = class_counts_valid.sum()\n        num_classes_valid = len(class_counts_valid)\n\n        if (num_rows_valid >= minimum_rows_to_keep) and (num_classes_valid >= minimum_class_to_keep):\n            return new_threshold, holdout_frac, num_bagging_folds\n\n        num_classes_valid = 0\n        num_rows_valid = 0\n        new_threshold = None\n        for i in range(num_classes):\n            num_classes_valid += 1\n            num_rows_valid += class_counts.iloc[i]\n            new_threshold = class_counts.iloc[i]\n            if (num_rows_valid >= minimum_rows_to_keep) and (num_classes_valid >= minimum_class_to_keep):\n                break\n\n        if new_threshold == 1:\n            new_threshold = 2  # threshold=1 is invalid, can\'t perform any train/val split in this case.\n        self.threshold = new_threshold\n\n        if new_threshold < minimum_safe_threshold:\n            if num_bagging_folds >= 2:\n                if num_bagging_folds > new_threshold:\n                    num_bagging_folds = new_threshold\n            elif math.ceil(1 / holdout_frac) > new_threshold:\n                holdout_frac = 1 / new_threshold + 0.001\n\n        return new_threshold, holdout_frac, num_bagging_folds\n\n    def augment_rare_classes(self, X):\n        """""" Use this method when using certain eval_metrics like log_loss,\n            for which no classes may be filtered out.\n            This method will augment dataset with additional examples of rare classes.\n        """"""\n        class_counts = X[self.label].value_counts()\n        class_counts_invalid = class_counts[class_counts < self.threshold]\n        if len(class_counts_invalid) == 0:\n            logger.debug(""augment_rare_classes did not need to duplicate any data from rare classes"")\n            return X\n\n        aug_df = None\n        for clss, n_clss in class_counts_invalid.iteritems():\n            n_toadd = self.threshold - n_clss\n            clss_df = X.loc[X[self.label] == clss]\n            if aug_df is None:\n                aug_df = clss_df[:0].copy()\n            duplicate_times = int(np.floor(n_toadd / n_clss))\n            remainder = n_toadd % n_clss\n            new_df = clss_df.copy()\n            new_df = new_df[:remainder]\n            while duplicate_times > 0:\n                logger.debug(f""Duplicating data from rare class: {clss}"")\n                duplicate_times -= 1\n                new_df = new_df.append(clss_df.copy())\n            aug_df = aug_df.append(new_df.copy())\n\n        X = X.append(aug_df)\n        class_counts = X[self.label].value_counts()\n        class_counts_invalid = class_counts[class_counts < self.threshold]\n        if len(class_counts_invalid) > 0:\n            raise RuntimeError(""augment_rare_classes failed to produce enough data from rare classes"")\n        logger.log(15, ""Replicated some data from rare classes in training set because eval_metric requires all classes"")\n        return X\n'"
autogluon/utils/tabular/ml/models/__init__.py,0,b''
autogluon/utils/tabular/ml/trainer/__init__.py,0,b''
autogluon/utils/tabular/ml/trainer/abstract_trainer.py,0,"b'import copy, time, traceback, logging\nimport os\nfrom typing import List\nimport networkx as nx\nimport numpy as np\nimport pandas as pd\nfrom pandas import DataFrame, Series\nfrom collections import defaultdict\n\nfrom ..constants import AG_ARGS, AG_ARGS_FIT, BINARY, MULTICLASS, REGRESSION, SOFTCLASS, REFIT_FULL_NAME, REFIT_FULL_SUFFIX\nfrom ...utils.loaders import load_pkl\nfrom ...utils.savers import save_pkl, save_json\nfrom ...utils.exceptions import TimeLimitExceeded, NotEnoughMemoryError\nfrom ..utils import get_pred_from_proba, dd_list, generate_train_test_split, combine_pred_and_true, shuffle_df_rows\nfrom ..models.abstract.abstract_model import AbstractModel\nfrom ...metrics import accuracy, log_loss, root_mean_squared_error, scorer_expects_y_pred\nfrom ..models.ensemble.bagged_ensemble_model import BaggedEnsembleModel\nfrom ..trainer.model_presets.presets import get_preset_stacker_model\nfrom ..trainer.model_presets.presets_custom import get_preset_custom\nfrom ..models.ensemble.stacker_ensemble_model import StackerEnsembleModel\nfrom ..models.ensemble.weighted_ensemble_model import WeightedEnsembleModel\n\nlogger = logging.getLogger(__name__)\n\n\n# FIXME: Below is major defect!\n#  Weird interaction for metrics like AUC during bagging.\n#  If kfold = 5, scores are 0.9, 0.85, 0.8, 0.75, and 0.7, the score is not 0.8! It is much lower because probs are combined together and AUC is recalculated\n#  Do we want this to happen? Should we calculate score by 5 separate scores and then averaging instead?\n\n# TODO: Dynamic model loading for ensemble models during prediction, only load more models if prediction is uncertain. This dynamically reduces inference time.\n# TODO: Try midstack Semi-Supervised. Just take final models and re-train them, use bagged preds for SS rows. This would be very cheap and easy to try.\nclass AbstractTrainer:\n    trainer_file_name = \'trainer.pkl\'\n    trainer_info_name = \'info.pkl\'\n    trainer_info_json_name = \'info.json\'\n\n    def __init__(self, path: str, problem_type: str, scheduler_options=None, objective_func=None, stopping_metric=None,\n                 num_classes=None, low_memory=False, feature_types_metadata=None, kfolds=0, n_repeats=1,\n                 stack_ensemble_levels=0, time_limit=None, save_data=False, save_bagged_folds=True, random_seed=0, verbosity=2):\n        self.path = path\n        self.problem_type = problem_type\n        if feature_types_metadata is None:\n            feature_types_metadata = {}\n        self.feature_types_metadata = feature_types_metadata\n        self.save_data = save_data\n        self.random_seed = random_seed  # Integer value added to the stack level to get the random_seed for kfold splits or the train/val split if bagging is disabled\n        self.verbosity = verbosity\n        if objective_func is not None:\n            self.objective_func = objective_func\n        elif self.problem_type == BINARY:\n            self.objective_func = accuracy\n        elif self.problem_type == MULTICLASS:\n            self.objective_func = accuracy\n        else:\n            self.objective_func = root_mean_squared_error\n\n        # stopping_metric is used to early stop all models except for aux models.\n        if stopping_metric is not None:\n            self.stopping_metric = stopping_metric\n        elif self.objective_func.name == \'roc_auc\':\n            self.stopping_metric = log_loss\n        else:\n            self.stopping_metric = self.objective_func\n\n        self.objective_func_expects_y_pred = scorer_expects_y_pred(scorer=self.objective_func)\n        logger.log(25, ""AutoGluon will gauge predictive performance using evaluation metric: %s"" % self.objective_func.name)\n        if not self.objective_func_expects_y_pred:\n            logger.log(25, ""This metric expects predicted probabilities rather than predicted class labels, so you\'ll need to use predict_proba() instead of predict()"")\n\n        logger.log(20, ""To change this, specify the eval_metric argument of fit()"")\n        logger.log(25, ""AutoGluon will early stop models using evaluation metric: %s"" % self.stopping_metric.name)\n        self.num_classes = num_classes\n        self.feature_prune = False # will be set to True if feature-pruning is turned on.\n        self.low_memory = low_memory\n        self.bagged_mode = True if kfolds >= 2 else False\n        if self.bagged_mode:\n            self.kfolds = kfolds  # int number of folds to do model bagging, < 2 means disabled\n            self.stack_ensemble_levels = stack_ensemble_levels\n            self.stack_mode = True if self.stack_ensemble_levels >= 1 else False\n            self.n_repeats = n_repeats\n        else:\n            self.kfolds = 0\n            self.stack_ensemble_levels = 0\n            self.stack_mode = False\n            self.n_repeats = 1\n        self.save_bagged_folds = save_bagged_folds\n\n        self.hyperparameters = {}  # TODO: This is currently required for fetching stacking layer models. Consider incorporating more elegantly\n\n        # self.models_level_all[\'core\'][0] # Includes base models\n        # self.models_level_all[\'core\'][1] # Stacker level 1\n        # self.models_level_all[\'aux1\'][1] # Stacker level 1 aux models, such as weighted_ensemble\n        # self.models_level_all[\'core\'][2] # Stacker level 2\n        self.models_level = defaultdict(dd_list)\n\n        self.model_best = None\n\n        self.model_performance = {}  # TODO: Remove in future, use networkx.\n        self.model_paths = {}\n        self.model_types = {}  # Outer type, can be BaggedEnsemble, StackEnsemble (Type that is able to load the model)\n        self.model_types_inner = {}  # Inner type, if Ensemble then it is the type of the inner model (May not be able to load with this type)\n        self.models = {}\n        self.model_graph = nx.DiGraph()\n        self.model_full_dict = {}  # Dict of normal Model -> FULL Model\n        self.reset_paths = False\n\n        self.hpo_results = {}  # Stores summary of HPO process\n        # Scheduler attributes:\n        if scheduler_options is not None:\n            self.scheduler_func = scheduler_options[0]  # unpack tuple\n            self.scheduler_options = scheduler_options[1]\n        else:\n            self.scheduler_func = None\n            self.scheduler_options = None\n\n        self.time_limit = time_limit\n        if self.time_limit is None:\n            self.time_limit = 1e7\n            self.ignore_time_limit = True\n        else:\n            self.ignore_time_limit = False\n        self.time_train_start = None\n        self.time_train_level_start = None\n        self.time_limit_per_level = self.time_limit / (self.stack_ensemble_levels + 1)\n\n        self.num_rows_train = None\n        self.num_cols_train = None\n\n        self.is_data_saved = False\n        self.normalize_predprobs = False # whether or not probabilistic predictions may need to be renormalized (eg. distillation with BINARY -> REGRESSION)\n        # TODO: ensure each model always outputs appropriately normalized predictions so this final safety check then becomes unnecessary\n\n    # path_root is the directory containing learner.pkl\n    @property\n    def path_root(self):\n        return self.path.rsplit(os.path.sep, maxsplit=2)[0] + os.path.sep\n\n    @property\n    def path_utils(self):\n        return self.path_root + \'utils\' + os.path.sep\n\n    @property\n    def path_data(self):\n        return self.path_utils + \'data\' + os.path.sep\n\n    def load_X_train(self):\n        path = self.path_data + \'X_train.pkl\'\n        return load_pkl.load(path=path)\n\n    def load_X_val(self):\n        path = self.path_data + \'X_val.pkl\'\n        return load_pkl.load(path=path)\n\n    def load_y_train(self):\n        path = self.path_data + \'y_train.pkl\'\n        return load_pkl.load(path=path)\n\n    def load_y_val(self):\n        path = self.path_data + \'y_val.pkl\'\n        return load_pkl.load(path=path)\n\n    def save_X_train(self, X, verbose=True):\n        path = self.path_data + \'X_train.pkl\'\n        save_pkl.save(path=path, object=X, verbose=verbose)\n\n    def save_X_val(self, X, verbose=True):\n        path = self.path_data + \'X_val.pkl\'\n        save_pkl.save(path=path, object=X, verbose=verbose)\n\n    def save_y_train(self, y, verbose=True):\n        path = self.path_data + \'y_train.pkl\'\n        save_pkl.save(path=path, object=y, verbose=verbose)\n\n    def save_y_val(self, y, verbose=True):\n        path = self.path_data + \'y_val.pkl\'\n        save_pkl.save(path=path, object=y, verbose=verbose)\n\n    def get_model_names_all(self, can_infer=None):\n        model_names_all = list(self.model_graph.nodes)\n        # TODO: can_infer is technically more complicated, if an ancestor can\'t infer then the model can\'t infer.\n        if can_infer is not None:\n            node_attributes = nx.get_node_attributes(self.model_graph, \'can_infer\')\n            model_names_all = [model for model in model_names_all if node_attributes[model] == can_infer]\n        return model_names_all\n\n    def get_model_names(self, stack_name):\n        model_names = []\n        levels = np.sort(list(self.models_level[stack_name].keys()))\n        for level in levels:\n            model_names += self.models_level[stack_name][level]\n        return model_names\n\n    def get_max_level(self, stack_name: str):\n        try:\n            return sorted(list(self.models_level[stack_name].keys()))[-1]\n        except IndexError:\n            return -1\n\n    def get_max_level_all(self):\n        max_level = 0\n        for stack_name in self.models_level.keys():\n            max_level = max(max_level, self.get_max_level(stack_name))\n        return max_level\n\n    def get_models(self, hyperparameters, hyperparameter_tune=False, **kwargs):\n        raise NotImplementedError\n\n    def get_model_level(self, model_name):\n        for stack_name in self.models_level.keys():\n            for level in self.models_level[stack_name].keys():\n                if model_name in self.models_level[stack_name][level]:\n                    return level\n        raise ValueError(\'Model\' + str(model_name) + \'does not exist in trainer.\')\n\n    def set_contexts(self, path_context):\n        self.path, self.model_paths = self.create_contexts(path_context)\n\n    def create_contexts(self, path_context):\n        path = path_context\n        model_paths = copy.deepcopy(self.model_paths)\n        for model in self.model_paths:\n            prev_path = self.model_paths[model]\n            model_local_path = prev_path.split(self.path, 1)[1]\n            new_path = path + model_local_path\n            model_paths[model] = new_path\n\n        return path, model_paths\n\n    def train(self, X_train, y_train, X_test=None, y_test=None, hyperparameter_tune=True, feature_prune=False, holdout_frac=0.1, hyperparameters=None):\n        raise NotImplementedError\n\n    def train_single(self, X_train, y_train, X_test, y_test, model, kfolds=None, k_fold_start=0, k_fold_end=None, n_repeats=None, n_repeat_start=0, level=0, time_limit=None):\n        if kfolds is None:\n            kfolds = self.kfolds\n        if n_repeats is None:\n            n_repeats = self.n_repeats\n        if model.feature_types_metadata is None:\n            model.feature_types_metadata = self.feature_types_metadata  # TODO: move this into model creation process?\n        model_fit_kwargs = {}\n        if self.scheduler_options is not None:\n            model_fit_kwargs = {\'verbosity\': self.verbosity,\n                                \'num_cpus\': self.scheduler_options[\'resource\'][\'num_cpus\'],\n                                \'num_gpus\': self.scheduler_options[\'resource\'][\'num_gpus\']}  # Additional configurations for model.fit\n        if self.bagged_mode or isinstance(model, WeightedEnsembleModel):\n            model.fit(X=X_train, y=y_train, k_fold=kfolds, k_fold_start=k_fold_start, k_fold_end=k_fold_end, n_repeats=n_repeats, n_repeat_start=n_repeat_start, compute_base_preds=False, time_limit=time_limit, **model_fit_kwargs)\n        else:\n            model.fit(X_train=X_train, Y_train=y_train, X_test=X_test, Y_test=y_test, time_limit=time_limit, **model_fit_kwargs)\n        return model\n\n    def train_and_save(self, X_train, y_train, X_test, y_test, model: AbstractModel, stack_name=\'core\', kfolds=None, k_fold_start=0, k_fold_end=None, n_repeats=None, n_repeat_start=0, level=0, time_limit=None):\n        fit_start_time = time.time()\n        model_names_trained = []\n        try:\n            if time_limit is not None:\n                if time_limit <= 0:\n                    logging.log(15, \'Skipping \' + str(model.name) + \' due to lack of time remaining.\')\n                    return model_names_trained\n                time_left_total = self.time_limit - (fit_start_time - self.time_train_start)\n                logging.log(20, \'Fitting model: \' + str(model.name) + \' ...\' + \' Training model for up to \' + str(round(time_limit, 2)) + \'s of the \' + str(round(time_left_total, 2)) + \'s of remaining time.\')\n            else:\n                logging.log(20, \'Fitting model: \' + str(model.name) + \' ...\')\n            model = self.train_single(X_train, y_train, X_test, y_test, model, kfolds=kfolds, k_fold_start=k_fold_start, k_fold_end=k_fold_end, n_repeats=n_repeats, n_repeat_start=n_repeat_start, level=level, time_limit=time_limit)\n            fit_end_time = time.time()\n            if isinstance(model, BaggedEnsembleModel):\n                if model.bagged_mode or isinstance(model, WeightedEnsembleModel):\n                    score = model.score_with_oof(y=y_train)\n                else:\n                    score = None\n            else:\n                if X_test is not None and y_test is not None:\n                    score = model.score(X=X_test, y=y_test)\n                else:\n                    score = None\n            pred_end_time = time.time()\n            if model.fit_time is None:\n                model.fit_time = fit_end_time - fit_start_time\n            if model.predict_time is None:\n                if score is None:\n                    model.predict_time = None\n                else:\n                    model.predict_time = pred_end_time - fit_end_time\n            model.val_score = score\n            # TODO: Add recursive=True to avoid repeatedly loading models each time this is called for bagged ensembles (especially during repeated bagging)\n            self.save_model(model=model)\n        except TimeLimitExceeded:\n            logger.log(20, \'\\tTime limit exceeded... Skipping %s.\' % model.name)\n            # logger.log(20, \'\\tTime wasted: \' + str(time.time() - fit_start_time))\n            del model\n        except NotEnoughMemoryError:\n            logger.warning(\'\\tNot enough memory to train %s... Skipping this model.\' % model.name)\n            del model\n        except Exception as err:\n            if self.verbosity >= 1:\n                traceback.print_tb(err.__traceback__)\n            logger.exception(\'Warning: Exception caused %s to fail during training... Skipping this model.\' % model.name)\n            logger.log(20, err)\n            del model\n        else:\n            self.add_model(model=model, stack_name=stack_name, level=level)\n            model_names_trained.append(model.name)\n            if self.low_memory:\n                del model\n        return model_names_trained\n\n    def add_model(self, model: AbstractModel, stack_name: str, level: int):\n        stack_loc = self.models_level[stack_name]  # TODO: Consider removing, have train_multi handle this\n        self.model_performance[model.name] = model.val_score\n        self.model_paths[model.name] = model.path\n        self.model_types[model.name] = type(model)\n        if isinstance(model, BaggedEnsembleModel):\n            self.model_types_inner[model.name] = model._child_type\n        else:\n            self.model_types_inner[model.name] = type(model)\n        if model.val_score is not None:\n            logger.log(20, \'\\t\' + str(round(model.val_score, 4)) + \'\\t = Validation \' + self.objective_func.name + \' score\')\n        if model.fit_time is not None:\n            logger.log(20, \'\\t\' + str(round(model.fit_time, 2)) + \'s\' + \'\\t = Training runtime\')\n        if model.predict_time is not None:\n            logger.log(20, \'\\t\' + str(round(model.predict_time, 2)) + \'s\' + \'\\t = Validation runtime\')\n        # TODO: Add to HPO\n        if model.is_valid():\n            self.model_graph.add_node(model.name, fit_time=model.fit_time, predict_time=model.predict_time, val_score=model.val_score, can_infer=model.can_infer())\n            if isinstance(model, StackerEnsembleModel):\n                for stack_column_prefix in model.stack_column_prefix_lst:\n                    base_model_name = model.stack_column_prefix_to_model_map[stack_column_prefix]\n                    self.model_graph.add_edge(base_model_name, model.name)\n            if model.name not in stack_loc[level]:\n                stack_loc[level].append(model.name)\n        if self.low_memory:\n            del model\n\n    def train_single_full(self, X_train, y_train, X_test, y_test, model: AbstractModel, feature_prune=False,\n                          hyperparameter_tune=True, stack_name=\'core\', kfolds=None, k_fold_start=0, k_fold_end=None, n_repeats=None, n_repeat_start=0, level=0, time_limit=None):\n        if (n_repeat_start == 0) and (k_fold_start == 0):\n            model.feature_types_metadata = self.feature_types_metadata  # TODO: Don\'t set feature_types_metadata here\n        if feature_prune:\n            if n_repeat_start != 0:\n                raise ValueError(\'n_repeat_start must be 0 to feature_prune, value = \' + str(n_repeat_start))\n            elif k_fold_start != 0:\n                raise ValueError(\'k_fold_start must be 0 to feature_prune, value = \' + str(k_fold_start))\n            self.autotune(X_train=X_train, X_holdout=X_test, y_train=y_train, y_holdout=y_test, model_base=model)  # TODO: Update to use CV instead of holdout\n        if hyperparameter_tune:\n            if self.scheduler_func is None or self.scheduler_options is None:\n                raise ValueError(""scheduler_options cannot be None when hyperparameter_tune = True"")\n            if n_repeat_start != 0:\n                raise ValueError(\'n_repeat_start must be 0 to hyperparameter_tune, value = \' + str(n_repeat_start))\n            elif k_fold_start != 0:\n                raise ValueError(\'k_fold_start must be 0 to hyperparameter_tune, value = \' + str(k_fold_start))\n            # hpo_models (dict): keys = model_names, values = model_paths\n            try:\n                if isinstance(model, BaggedEnsembleModel):\n                    hpo_models, hpo_model_performances, hpo_results = model.hyperparameter_tune(X=X_train, y=y_train, k_fold=kfolds, scheduler_options=(self.scheduler_func, self.scheduler_options), verbosity=self.verbosity)\n                else:\n                    hpo_models, hpo_model_performances, hpo_results = model.hyperparameter_tune(X_train=X_train, X_test=X_test, Y_train=y_train, Y_test=y_test, scheduler_options=(self.scheduler_func, self.scheduler_options), verbosity=self.verbosity)\n            except Exception as err:\n                if self.verbosity >= 1:\n                    traceback.print_tb(err.__traceback__)\n                logger.exception(\'Warning: Exception caused \' + model.name + \' to fail during hyperparameter tuning... Skipping this model.\')\n                logger.debug(err)\n                del model\n                model_names_trained = []\n            else:\n                self.hpo_results[model.name] = hpo_results\n                model_names_trained = []\n                for model_hpo_name, model_path in hpo_models.items():\n                    model_hpo = self.load_model(model_hpo_name, path=model_path, model_type=type(model))\n                    self.add_model(model=model_hpo, stack_name=stack_name, level=level)\n                    model_names_trained.append(model_hpo.name)\n        else:\n            model_names_trained = self.train_and_save(X_train, y_train, X_test, y_test, model, stack_name=stack_name, kfolds=kfolds, k_fold_start=k_fold_start, k_fold_end=k_fold_end, n_repeats=n_repeats, n_repeat_start=n_repeat_start, level=level, time_limit=time_limit)\n        self.save()\n        return model_names_trained\n\n    # TODO: How to deal with models that fail during this? They have trained valid models before, but should we still use those models or remove the entire model? Currently we still use models.\n    # TODO: Time allowance can be made better by only using time taken during final model training and not during HPO and feature pruning.\n    # TODO: Time allowance not accurate if running from fit_continue\n    # Takes trained bagged ensemble models and fits additional k-fold bags.\n    def train_multi_repeats(self, X_train, y_train, X_test, y_test, models, kfolds, n_repeats, n_repeat_start=1, stack_name=\'core\', level=0, time_limit=None):\n        models_valid = models\n        models_valid_next = []\n        repeats_completed = 0\n        time_start = time.time()\n        for n in range(n_repeat_start, n_repeats):\n            if time_limit is not None:\n                time_start_repeat = time.time()\n                time_left = time_limit - (time_start_repeat - time_start)\n                if n == n_repeat_start:\n                    time_required = self.time_limit_per_level * 0.575  # Require slightly over 50% to be safe\n                else:\n                    time_required = (time_start_repeat - time_start) / repeats_completed * (0.575/0.425)\n                if time_left < time_required:\n                    logger.log(15, \'Not enough time left to finish repeated k-fold bagging, stopping early ...\')\n                    break\n            logger.log(20, \'Repeating k-fold bagging: \' + str(n+1) + \'/\' + str(n_repeats))\n            for i, model in enumerate(models_valid):\n                if isinstance(model, str):\n                    model = self.load_model(model)\n                if time_limit is None:\n                    time_left = None\n                else:\n                    time_start_model = time.time()\n                    time_left = time_limit - (time_start_model - time_start)\n                models_valid_next += self.train_single_full(X_train, y_train, X_test, y_test, model, hyperparameter_tune=False, feature_prune=False, stack_name=stack_name, kfolds=kfolds, k_fold_start=0, k_fold_end=None, n_repeats=n+1, n_repeat_start=n, level=level, time_limit=time_left)\n            models_valid = copy.deepcopy(models_valid_next)\n            models_valid_next = []\n            repeats_completed += 1\n        logger.log(20, \'Completed \' + str(n_repeat_start + repeats_completed) + \'/\' + str(n_repeats) + \' k-fold bagging repeats ...\')\n        return models_valid\n\n    def train_multi_initial(self, X_train, y_train, X_test, y_test, models: List[AbstractModel], kfolds, n_repeats, hyperparameter_tune=True, feature_prune=False, stack_name=\'core\', level=0, time_limit=None):\n        stack_loc = self.models_level[stack_name]\n\n        models_valid = models\n        if kfolds == 0:\n            models_valid = self.train_multi_fold(X_train, y_train, X_test, y_test, models_valid, hyperparameter_tune=hyperparameter_tune, feature_prune=feature_prune, stack_name=stack_name,\n                                                          kfolds=kfolds, level=level, time_limit=time_limit)\n        else:\n            k_fold_start = 0\n            if hyperparameter_tune or feature_prune:\n                time_start = time.time()\n                models_valid = self.train_multi_fold(X_train, y_train, X_test, y_test, models_valid, hyperparameter_tune=hyperparameter_tune, feature_prune=feature_prune, stack_name=stack_name,\n                                                     kfolds=kfolds, k_fold_start=0, k_fold_end=1, n_repeats=n_repeats, n_repeat_start=0, level=level, time_limit=time_limit)\n                k_fold_start = 1\n                if time_limit is not None:\n                    time_limit = time_limit - (time.time() - time_start)\n\n            models_valid = self.train_multi_fold(X_train, y_train, X_test, y_test, models_valid, hyperparameter_tune=False, feature_prune=False, stack_name=stack_name,\n                                                 kfolds=kfolds, k_fold_start=k_fold_start, k_fold_end=kfolds, n_repeats=n_repeats, n_repeat_start=0, level=level, time_limit=time_limit)\n\n        model_names_trained = models_valid\n        unique_names = []\n        for item in stack_loc[level]:\n            if item not in unique_names: unique_names.append(item)\n        stack_loc[level] = unique_names  # make unique and preserve order\n        return model_names_trained\n\n    # TODO: Ban KNN from being a Stacker model outside of aux. Will need to ensemble select on all stack layers ensemble selector to make it work\n    # TODO: Robert dataset, LightGBM is super good but RF and KNN take all the time away from it on 1h despite being much worse\n    # TODO: Add time_limit_per_model\n    def train_multi_fold(self, X_train, y_train, X_test, y_test, models: List[AbstractModel], hyperparameter_tune=True, feature_prune=False, stack_name=\'core\', kfolds=None, k_fold_start=0, k_fold_end=None, n_repeats=None, n_repeat_start=0, level=0, time_limit=None):\n        models_valid = []\n        time_start = time.time()\n        for i, model in enumerate(models):\n            if isinstance(model, str):\n                model = self.load_model(model)\n            elif self.low_memory:\n                model = copy.deepcopy(model)\n            # TODO: Only update scores when finished, only update model as part of final models if finished!\n            if time_limit is None:\n                time_left = None\n            else:\n                time_start_model = time.time()\n                time_left = time_limit - (time_start_model - time_start)\n            model_name_trained_lst = self.train_single_full(X_train, y_train, X_test, y_test, model, hyperparameter_tune=hyperparameter_tune, feature_prune=feature_prune, stack_name=stack_name,\n                                                            kfolds=kfolds, k_fold_start=k_fold_start, k_fold_end=k_fold_end,\n                                                            n_repeats=n_repeats, n_repeat_start=n_repeat_start, level=level, time_limit=time_left)\n\n            if self.low_memory:\n                del model\n            models_valid += model_name_trained_lst\n\n        return models_valid\n\n    def train_multi(self, X_train, y_train, X_test, y_test, models: List[AbstractModel], hyperparameter_tune=True, feature_prune=False, stack_name=\'core\', kfolds=None, n_repeats=None, n_repeat_start=0, level=0, time_limit=None):\n        if kfolds is None:\n            kfolds = self.kfolds\n        if n_repeats is None:\n            n_repeats = self.n_repeats\n        if (kfolds == 0) and (n_repeats != 1):\n            raise ValueError(\'n_repeats must be 1 when kfolds is 0, values: (%s, %s)\' % (n_repeats, kfolds))\n        if time_limit is None:\n            n_repeats_initial = n_repeats\n        else:\n            n_repeats_initial = 1\n        if n_repeat_start == 0:\n            time_start = time.time()\n            model_names_trained = self.train_multi_initial(X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test, models=models, kfolds=kfolds, n_repeats=n_repeats_initial, hyperparameter_tune=hyperparameter_tune, feature_prune=feature_prune,\n                                                           stack_name=stack_name, level=level, time_limit=time_limit)\n            n_repeat_start = n_repeats_initial\n            if time_limit is not None:\n                time_limit = time_limit - (time.time() - time_start)\n        else:\n            model_names_trained = models\n        if (n_repeats > 1) and self.bagged_mode and (n_repeat_start < n_repeats):\n            model_names_trained = self.train_multi_repeats(X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test, models=model_names_trained,\n                                                           kfolds=kfolds, n_repeats=n_repeats, n_repeat_start=n_repeat_start, stack_name=stack_name, level=level, time_limit=time_limit)\n        return model_names_trained\n\n    def train_multi_and_ensemble(self, X_train, y_train, X_test, y_test, models: List[AbstractModel], hyperparameter_tune=True, feature_prune=False):\n        if self.save_data and not self.is_data_saved:\n            self.save_X_train(X_train)\n            self.save_y_train(y_train)\n            if X_test is not None:\n                self.save_X_val(X_test)\n                if y_test is not None:\n                    self.save_y_val(y_test)\n            self.is_data_saved = True\n\n        self.num_rows_train = len(X_train)\n        if X_test is not None:\n            self.num_rows_train += len(X_test)\n        self.num_cols_train = len(list(X_train.columns))\n        self.time_train_start = time.time()\n        self.train_multi_levels(X_train, y_train, X_test, y_test, models=models, hyperparameter_tune=hyperparameter_tune, feature_prune=feature_prune, level_start=0, level_end=self.stack_ensemble_levels)\n        if len(self.get_model_names_all()) == 0:\n            raise ValueError(\'AutoGluon did not successfully train any models\')\n\n    def train_multi_levels(self, X_train, y_train, X_test, y_test, models: List[AbstractModel], hyperparameter_tune=True, feature_prune=False, level_start=0, level_end=0):\n        for level in range(max(0, level_start), level_end + 1):\n            self.time_train_level_start = time.time()\n            self.time_limit_per_level = (self.time_limit - (self.time_train_level_start - self.time_train_start)) / (level_end + 1 - level)\n            if self.ignore_time_limit:\n                time_limit_core = None\n                time_limit_aux = None\n            else:\n                time_limit_core = self.time_limit_per_level\n                time_limit_aux = max(self.time_limit_per_level * 0.1, min(self.time_limit, 360))  # Allows aux to go over time_limit, but only by a small amount\n            if level == 0:\n                self.stack_new_level(X=X_train, y=y_train, X_test=X_test, y_test=y_test, models=models, level=level, hyperparameter_tune=hyperparameter_tune, feature_prune=feature_prune, time_limit_core=time_limit_core, time_limit_aux=time_limit_aux)\n            else:\n                self.stack_new_level(X=X_train, y=y_train, X_test=X_test, y_test=y_test, level=level, time_limit_core=time_limit_core, time_limit_aux=time_limit_aux)\n\n        self.save()\n\n    def stack_new_level(self, X, y, X_test=None, y_test=None, level=0, models=None, hyperparameter_tune=False, feature_prune=False, time_limit_core=None, time_limit_aux=None):\n        core_models = self.stack_new_level_core(X=X, y=y, X_test=X_test, y_test=y_test, models=models, level=level, hyperparameter_tune=hyperparameter_tune, feature_prune=feature_prune, time_limit=time_limit_core)\n        if self.bagged_mode:\n            aux_models = self.stack_new_level_aux(X=X, y=y, level=level+1, time_limit=time_limit_aux)\n        else:\n            aux_models = self.stack_new_level_aux(X=X_test, y=y_test, fit=False, level=level+1, time_limit=time_limit_aux)\n        return core_models + aux_models\n\n    def stack_new_level_core(self, X, y, X_test=None, y_test=None, models=None, level=1, stack_name=\'core\', kfolds=None, n_repeats=None, hyperparameter_tune=False, feature_prune=False, time_limit=None, save_bagged_folds=None, stacker_type=StackerEnsembleModel, extra_ag_args_fit=None):\n        use_orig_features = True\n        if models is None:\n            models = self.get_models(self.hyperparameters, level=level, extra_ag_args_fit=extra_ag_args_fit)\n        if kfolds is None:\n            kfolds = self.kfolds\n        if n_repeats is None:\n            n_repeats = self.n_repeats\n        if save_bagged_folds is None:\n            save_bagged_folds = self.save_bagged_folds\n\n        if self.bagged_mode:\n            if level == 0:\n                (base_model_names, base_model_paths, base_model_types) = ([], {}, {})\n            elif level > 0:\n                base_model_names, base_model_paths, base_model_types = self.get_models_load_info(model_names=self.models_level[\'core\'][level - 1])\n                if len(base_model_names) == 0:\n                    logger.log(20, \'No base models to train on, skipping stack level...\')\n                    return\n            else:\n                raise AssertionError(\'Stack level cannot be negative! level = %s\' % level)\n            models = [\n                stacker_type(path=self.path, name=model.name + \'_STACKER_l\' + str(level), model_base=model, base_model_names=base_model_names,\n                                     base_model_paths_dict=base_model_paths, base_model_types_dict=base_model_types, use_orig_features=use_orig_features,\n                                     num_classes=self.num_classes, save_bagged_folds=save_bagged_folds, random_state=level+self.random_seed)\n                for model in models]\n        X_train_init = self.get_inputs_to_stacker(X, level_start=0, level_end=level, fit=True)\n        if X_test is not None:\n            X_test = self.get_inputs_to_stacker(X_test, level_start=0, level_end=level, fit=False)\n\n        return self.train_multi(X_train=X_train_init, y_train=y, X_test=X_test, y_test=y_test, models=models, hyperparameter_tune=hyperparameter_tune, feature_prune=feature_prune, level=level, stack_name=stack_name, kfolds=kfolds, n_repeats=n_repeats, time_limit=time_limit)\n\n    def stack_new_level_aux(self, X, y, level, fit=True, time_limit=None):\n        stack_name = \'aux1\'\n        X_train_stack_preds = self.get_inputs_to_stacker(X, level_start=0, level_end=level, fit=fit)\n        return self.generate_weighted_ensemble(X=X_train_stack_preds, y=y, level=level, kfolds=0, n_repeats=1, stack_name=stack_name, time_limit=time_limit)\n\n    def generate_weighted_ensemble(self, X, y, level, kfolds=0, n_repeats=1, stack_name=None, hyperparameters=None, time_limit=None, base_model_names=None, name_suffix=\'\', save_bagged_folds=None, check_if_best=True, child_hyperparameters=None):\n        if save_bagged_folds is None:\n            save_bagged_folds = self.save_bagged_folds\n        if base_model_names is None:\n            base_model_names = self.models_level[\'core\'][level - 1]\n        if len(base_model_names) == 0:\n            logger.log(20, \'No base models to train on, skipping weighted ensemble...\')\n            return []\n\n        # TODO: Remove extra_params, currently a hack\n        if child_hyperparameters is not None:\n            extra_params = {\'_tmp_greedy_hyperparameters\': child_hyperparameters}\n        else:\n            extra_params = {}\n        weighted_ensemble_model = WeightedEnsembleModel(path=self.path, name=\'weighted_ensemble\' + name_suffix + \'_k\' + str(kfolds) + \'_l\' + str(level), base_model_names=base_model_names,\n                                                        base_model_paths_dict=self.model_paths, base_model_types_dict=self.model_types, base_model_types_inner_dict=self.model_types_inner, base_model_performances_dict=self.model_performance, hyperparameters=hyperparameters,\n                                                        objective_func=self.objective_func, num_classes=self.num_classes, save_bagged_folds=save_bagged_folds, random_state=level+self.random_seed, **extra_params)\n\n        self.train_multi(X_train=X, y_train=y, X_test=None, y_test=None, models=[weighted_ensemble_model], kfolds=kfolds, n_repeats=n_repeats, hyperparameter_tune=False, feature_prune=False, stack_name=stack_name, level=level, time_limit=time_limit)\n        if check_if_best and weighted_ensemble_model.name in self.get_model_names_all():\n            if self.model_best is None:\n                self.model_best = weighted_ensemble_model.name\n            else:\n                best_score = self.model_performance[self.model_best]\n                cur_score = self.model_performance[weighted_ensemble_model.name]\n                if cur_score > best_score:\n                    # new best model\n                    self.model_best = weighted_ensemble_model.name\n        return [weighted_ensemble_model.name]\n\n    def generate_stack_log_reg(self, X, y, level, kfolds=0, stack_name=None):\n        base_model_names, base_model_paths, base_model_types = self.get_models_load_info(model_names=self.models_level[\'core\'][level - 1])\n        stacker_model_lr = get_preset_stacker_model(path=self.path, problem_type=self.problem_type, objective_func=self.objective_func, num_classes=self.num_classes)\n        name_new = stacker_model_lr.name + \'_STACKER_k\' + str(kfolds) + \'_l\' + str(level)\n\n        stacker_model_lr = StackerEnsembleModel(path=self.path, name=name_new, model_base=stacker_model_lr, base_model_names=base_model_names, base_model_paths_dict=base_model_paths, base_model_types_dict=base_model_types,\n                                                use_orig_features=False,\n                                                num_classes=self.num_classes, random_state=level+self.random_seed)\n\n        return self.train_multi(X_train=X, y_train=y, X_test=None, y_test=None, models=[stacker_model_lr], hyperparameter_tune=False, feature_prune=False, stack_name=stack_name, kfolds=kfolds, level=level)\n\n    def predict(self, X, model=None):\n        if model is not None:\n            return self.predict_model(X, model)\n        elif self.model_best is not None:\n            return self.predict_model(X, self.model_best)\n        else:\n            model = self.get_model_best()\n            return self.predict_model(X, model)\n\n    def predict_proba(self, X, model=None):\n        if model is not None:\n            return self.predict_proba_model(X, model)\n        elif self.model_best is not None:\n            return self.predict_proba_model(X, self.model_best)\n        else:\n            model = self.get_model_best()\n            return self.predict_proba_model(X, model)\n\n    def predict_model(self, X, model, model_pred_proba_dict=None):\n        if isinstance(model, str):\n            model = self.load_model(model)\n        X = self.get_inputs_to_model(model=model, X=X, model_pred_proba_dict=model_pred_proba_dict, fit=False)\n        return model.predict(X=X, preprocess=False)\n\n    def predict_proba_model(self, X, model, model_pred_proba_dict=None):\n        if isinstance(model, str):\n            model = self.load_model(model)\n        X = self.get_inputs_to_model(model=model, X=X, model_pred_proba_dict=model_pred_proba_dict, fit=False)\n        EPS = 1e-10 # predicted probabilities can be at most this confident if we normalize predicted probabilities\n        # TODO: ensure each model always outputs appropriately normalized predictions so this final safety check then becomes unnecessary\n        if not self.normalize_predprobs:\n            return model.predict_proba(X=X, preprocess=False)\n        elif self.problem_type == MULTICLASS:\n           y_predproba = model.predict_proba(X=X, preprocess=False)\n           most_negative_rowvals = np.clip(np.min(y_predproba, axis=1), a_min=None, a_max=0)\n           y_predproba = y_predproba - most_negative_rowvals[:,None] # ensure nonnegative rows\n           y_predproba = np.clip(y_predproba, a_min = EPS, a_max = None) # ensure no zeros\n           return y_predproba / y_predproba.sum(axis=1, keepdims=1) # renormalize\n        elif self.problem_type == BINARY:\n            y_predproba = model.predict_proba(X=X, preprocess=False)\n            min_y = np.min(y_predproba)\n            max_y = np.max(y_predproba)\n            if min_y < EPS or max_y > 1-EPS: # remap predicted probs to line that goes through: (min_y, EPS), (max_y, 1-EPS)\n                y_predproba =  EPS + ((1-2*EPS)/(max_y-min_y)) * (y_predproba - min_y)\n            return y_predproba\n        return model.predict_proba(X=X, preprocess=False)\n\n    # Note: model_pred_proba_dict is mutated in this function to minimize memory usage\n    def get_inputs_to_model(self, model, X, model_pred_proba_dict=None, fit=False, preprocess=True):\n        if isinstance(model, str):\n            model = self.load_model(model)\n        model_level = self.get_model_level(model.name)\n        if model_level >= 1 and isinstance(model, StackerEnsembleModel):\n            if fit:\n                X = model.preprocess(X=X, preprocess=preprocess, fit=fit, model_pred_proba_dict=None)\n            else:\n                model_set = self.get_minimum_model_set(model)\n                model_set = [m for m in model_set if m != model.name]  # TODO: Can probably be faster, get this result from graph\n                model_pred_proba_dict = self.get_model_pred_proba_dict(X=X, models=model_set, model_pred_proba_dict=model_pred_proba_dict, fit=fit)\n                X = model.preprocess(X=X, preprocess=preprocess, fit=fit, model_pred_proba_dict=model_pred_proba_dict)\n        elif preprocess:\n            X = model.preprocess(X)\n        return X\n\n    def score(self, X, y, model=None):\n        if self.objective_func_expects_y_pred:\n            y_pred_ensemble = self.predict(X=X, model=model)\n            return self.objective_func(y, y_pred_ensemble)\n        else:\n            y_pred_proba_ensemble = self.predict_proba(X=X, model=model)\n            return self.objective_func(y, y_pred_proba_ensemble)\n\n    def score_with_y_pred_proba(self, y, y_pred_proba):\n        if self.objective_func_expects_y_pred:\n            y_pred = get_pred_from_proba(y_pred_proba=y_pred_proba, problem_type=self.problem_type)\n            return self.objective_func(y, y_pred)\n        else:\n            return self.objective_func(y, y_pred_proba)\n\n    def autotune(self, X_train, X_holdout, y_train, y_holdout, model_base: AbstractModel):\n        model_base.feature_prune(X_train, X_holdout, y_train, y_holdout)\n\n    def pred_proba_predictions(self, models, X_test):\n        preds = []\n        for model in models:\n            if isinstance(model, str):\n                model = self.load_model(model)\n            model_pred = model.predict_proba(X_test)\n            preds.append(model_pred)\n        return preds\n\n    # TODO: Consider adding persist to disk functionality for pred_proba dictionary to lessen memory burden on large multiclass problems.\n    #  For datasets with 100+ classes, this function could potentially run the system OOM due to each pred_proba numpy array taking significant amounts of space.\n    #  This issue already existed in the previous level-based version but only had the minimum required predictions in memory at a time, whereas this has all model predictions in memory.\n    # TODO: Add memory optimal topological ordering -> Minimize amount of pred_probas in memory at a time, delete pred probas that are no longer required\n    # Optimally computes pred_probas for each model in `models`. Will compute each necessary model only once and store its predictions in a dictionary.\n    # Note: Mutates model_pred_proba_dict and model_pred_time_dict input if present to minimize memory usage\n    # fit = get oof pred proba\n    # if record_pred_time is `True`, outputs tuple of dicts (model_pred_proba_dict, model_pred_time_dict), else output only model_pred_proba_dict\n    def get_model_pred_proba_dict(self, X, models, model_pred_proba_dict=None, model_pred_time_dict=None, fit=False, record_pred_time=False):\n        if model_pred_proba_dict is None:\n            model_pred_proba_dict = {}\n        if model_pred_time_dict is None:\n            model_pred_time_dict = {}\n\n        if fit:\n            model_pred_order = [model for model in models if model not in model_pred_proba_dict.keys()]\n        else:\n            model_set = set()\n            for model in models:\n                if model in model_set:\n                    continue\n                min_model_set = set(self.get_minimum_model_set(model))\n                model_set = model_set.union(min_model_set)\n            model_set = model_set.difference(set(model_pred_proba_dict.keys()))\n            models_to_load = list(model_set)\n            subgraph = nx.subgraph(self.model_graph, models_to_load)\n\n            # For model in model_pred_proba_dict, remove model node from graph and all ancestors that have no remaining descendants and are not in `models`\n            models_to_ignore = [model for model in models_to_load if (model not in models) and (not list(subgraph.successors(model)))]\n            while models_to_ignore:\n                model = models_to_ignore[0]\n                predecessors = list(subgraph.predecessors(model))\n                subgraph.remove_node(model)\n                models_to_ignore = models_to_ignore[1:]\n                for predecessor in predecessors:\n                    if (predecessor not in models) and (not list(subgraph.successors(predecessor))) and (predecessor not in models_to_ignore):\n                        models_to_ignore.append(predecessor)\n\n            # Get model prediction order\n            model_pred_order = list(nx.lexicographical_topological_sort(subgraph))\n\n        # Compute model predictions in topological order\n        for model_name in model_pred_order:\n            if record_pred_time:\n                time_start = time.time()\n\n            if fit:\n                model_type = self.model_types[model_name]\n                if issubclass(model_type, BaggedEnsembleModel):\n                    model_path = self.model_paths[model_name]\n                    model_pred_proba_dict[model_name] = model_type.load_oof(path=model_path)\n                else:\n                    raise AssertionError(f\'Model {model.name} must be a BaggedEnsembleModel to return oof_pred_proba\')\n            else:\n                model = self.load_model(model_name=model_name)\n                if isinstance(model, StackerEnsembleModel):\n                    X_input = model.preprocess(X=X, preprocess=True, infer=False, model_pred_proba_dict=model_pred_proba_dict)\n                    model_pred_proba_dict[model_name] = model.predict_proba(X_input, preprocess=False)\n                else:\n                    model_pred_proba_dict[model_name] = model.predict_proba(X)\n\n            if record_pred_time:\n                time_end = time.time()\n                model_pred_time_dict[model_name] = time_end - time_start\n\n        if record_pred_time:\n            return model_pred_proba_dict, model_pred_time_dict\n        else:\n            return model_pred_proba_dict\n\n    # TODO: Remove get_inputs_to_stacker eventually, move logic internally into this function instead\n    def get_inputs_to_stacker_v2(self, X, base_models, model_pred_proba_dict=None, fit=False):\n        if not fit:\n            model_pred_proba_dict = self.get_model_pred_proba_dict(X=X, models=base_models, model_pred_proba_dict=model_pred_proba_dict)\n            model_pred_proba_list = [model_pred_proba_dict[model] for model in base_models]\n        else:\n            # TODO: After get_inputs_to_stacker is removed, this if/else is not necessary, instead pass fit param to get_model_pred_proba_dict()\n            model_pred_proba_list = None\n\n        X_stacker_input = self.get_inputs_to_stacker(X=X, level_start=0, level_end=1, model_levels={0: base_models}, y_pred_probas=model_pred_proba_list, fit=fit)\n        return X_stacker_input\n\n    # TODO: Legacy code, still used during training because it is technically slightly faster and more memory efficient than get_model_pred_proba_dict()\n    #  Remove in future as it limits flexibility in stacker inputs during training\n    def get_inputs_to_stacker(self, X, level_start, level_end, model_levels=None, y_pred_probas=None, fit=False):\n        if level_start > level_end:\n            raise AssertionError(\'level_start cannot be greater than level end:\' + str(level_start) + \', \' + str(level_end))\n        if (level_start == 0) and (level_end == 0):\n            return X\n        if fit:\n            if level_start >= 1:\n                dummy_stacker_start = self._get_dummy_stacker(level=level_start, model_levels=model_levels, use_orig_features=True)\n                cols_to_drop = dummy_stacker_start.stack_columns\n                X = X.drop(cols_to_drop, axis=1)\n            dummy_stacker = self._get_dummy_stacker(level=level_end, model_levels=model_levels, use_orig_features=True)\n            X = dummy_stacker.preprocess(X=X, preprocess=False, fit=True, compute_base_preds=True)\n        elif y_pred_probas is not None:\n            dummy_stacker = self._get_dummy_stacker(level=level_end, model_levels=model_levels, use_orig_features=True)\n            X_stacker = dummy_stacker.pred_probas_to_df(pred_proba=y_pred_probas)\n            if dummy_stacker.use_orig_features:\n                if level_start >= 1:\n                    dummy_stacker_start = self._get_dummy_stacker(level=level_start, model_levels=model_levels, use_orig_features=True)\n                    cols_to_drop = dummy_stacker_start.stack_columns\n                    X = X.drop(cols_to_drop, axis=1)\n                X = pd.concat([X_stacker, X], axis=1)\n            else:\n                X = X_stacker\n        else:\n            dummy_stackers = {}\n            for level in range(level_start, level_end+1):\n                if level >= 1:\n                    dummy_stackers[level] = self._get_dummy_stacker(level=level, model_levels=model_levels, use_orig_features=True)\n            for level in range(level_start, level_end):\n                if level >= 1:\n                    cols_to_drop = dummy_stackers[level].stack_columns\n                else:\n                    cols_to_drop = []\n                X = dummy_stackers[level+1].preprocess(X=X, preprocess=False, fit=False, compute_base_preds=True)\n                if len(cols_to_drop) > 0:\n                    X = X.drop(cols_to_drop, axis=1)\n        return X\n\n    # You must have previously called fit() with cache_data=True\n    # Fits _FULL versions of specified models, but does NOT link them (_FULL stackers will still use normal models as input)\n    def refit_single_full(self, X=None, y=None, X_val=None, y_val=None, models=None):\n        if X is None:\n            X = self.load_X_train()\n            if X_val is None and not self.bagged_mode:\n                X_val = self.load_X_val()\n        if y is None:\n            y = self.load_y_train()\n            if y_val is None and not self.bagged_mode:\n                y_val = self.load_y_val()\n\n        if X_val is not None and y_val is not None:\n            X_full = pd.concat([X, X_val])\n            y_full = pd.concat([y, y_val])\n        else:\n            X_full = X\n            y_full = y\n\n        if models is None:\n            models = self.get_model_names_all()\n\n        model_levels = defaultdict(dd_list)\n        ignore_models = []\n        ignore_stack_names = [REFIT_FULL_NAME]\n        for stack_name in ignore_stack_names:\n            ignore_models += self.get_model_names(stack_name)  # get_model_names returns [] if stack_name does not exist\n        for model_name in models:\n            if model_name in ignore_models:\n                continue\n            model_level = self.get_model_level(model_name)\n            model_levels[REFIT_FULL_NAME][model_level] += [model_name]\n\n        levels = sorted(model_levels[REFIT_FULL_NAME].keys())\n        models_trained_full = []\n        model_full_dict = {}\n        for level in levels:\n            models_level = model_levels[REFIT_FULL_NAME][level]\n            for model in models_level:\n                model = self.load_model(model)\n                model_name = model.name\n                model_full = model.convert_to_refitfull_template()\n                # Mitigates situation where bagged models barely had enough memory and refit requires more. Worst case results in OOM, but this lowers chance of failure.\n                model_full.params_aux[\'max_memory_usage_ratio\'] = model_full.params_aux[\'max_memory_usage_ratio\'] * 1.15\n                # TODO: Do it for all models in the level at once to avoid repeated processing of data?\n                stacker_type = type(model)\n                if issubclass(stacker_type, WeightedEnsembleModel):\n                    # TODO: Technically we don\'t need to re-train the weighted ensemble, we could just copy the original and re-use the weights.\n                    if self.bagged_mode:\n                        X_train_stack_preds = self.get_inputs_to_stacker(X, level_start=0, level_end=level, fit=True)\n                        y_input = y\n                    else:\n                        X_train_stack_preds = self.get_inputs_to_stacker(X_val, level_start=0, level_end=level, fit=False)  # TODO: May want to cache this during original fit, as we do with OOF preds\n                        y_input = y_val\n\n                    # TODO: Remove child_hyperparameters, make this cleaner\n                    #  This fixes the following: Use the original weighted ensemble\'s iterations: Currently Dionis spends over 1hr training the refit weighted ensemble because it isn\'t time limited and goes to 100 iterations.\n                    child_hyperparameters = copy.deepcopy(model_full.params)\n                    child_hyperparameters[AG_ARGS_FIT] = copy.deepcopy(model_full.params_aux)\n                    # TODO: stack_name=REFIT_FULL_NAME_AUX?\n                    models_trained = self.generate_weighted_ensemble(X=X_train_stack_preds, y=y_input, level=level, stack_name=REFIT_FULL_NAME, kfolds=0, n_repeats=1, base_model_names=list(model.stack_column_prefix_to_model_map.values()), name_suffix=REFIT_FULL_SUFFIX, save_bagged_folds=True, check_if_best=False, child_hyperparameters=child_hyperparameters)\n                    # TODO: Do the below more elegantly, ideally as a parameter to the trainer train function to disable recording scores/pred time.\n                    for model_weighted_ensemble in models_trained:\n                        model_loaded = self.load_model(model_weighted_ensemble)\n                        model_loaded.val_score = None\n                        model_loaded.predict_time = None\n                        self.model_performance[model_weighted_ensemble] = None\n                        self.save_model(model_loaded)\n                else:\n                    models_trained = self.stack_new_level_core(X=X_full, y=y_full, models=[model_full], level=level, stack_name=REFIT_FULL_NAME, hyperparameter_tune=False, feature_prune=False, kfolds=0, n_repeats=1, save_bagged_folds=True, stacker_type=stacker_type)\n                if len(models_trained) == 1:\n                    model_full_dict[model_name] = models_trained[0]\n                models_trained_full += models_trained\n\n        keys_to_del = []\n        for model in model_full_dict.keys():\n            if model_full_dict[model] not in models_trained_full:\n                keys_to_del.append(model)\n        for key in keys_to_del:\n            del model_full_dict[key]\n        self.model_full_dict.update(model_full_dict)\n        self.save()  # TODO: This could be more efficient by passing in arg to not save if called by refit_ensemble_full since it saves anyways later.\n        return models_trained_full\n\n    # Fits _FULL models and links them in the stack so _FULL models only use other _FULL models as input during stacking\n    # If model is specified, will fit all _FULL models that are ancestors of the provided model, automatically linking them.\n    # If no model is specified, all models are refit and linked appropriately.\n    def refit_ensemble_full(self, model=\'all\'):\n        if model is \'all\':\n            ensemble_set = self.get_model_names_all()\n        else:\n            if model is \'best\':\n                model = self.get_model_best()\n            ensemble_set = self.get_minimum_model_set(model)\n        models_trained_full = self.refit_single_full(models=ensemble_set)\n\n        self.model_graph.remove_nodes_from(models_trained_full)\n        for model_full in models_trained_full:\n            # TODO: Consider moving base model info to a separate pkl file so that it can be edited without having to load/save the model again\n            #  Downside: Slower inference speed when models are not persisted in memory prior.\n            model_loaded = self.load_model(model_full)\n            if isinstance(model_loaded, StackerEnsembleModel):\n                for stack_column_prefix in model_loaded.stack_column_prefix_lst:\n                    base_model = model_loaded.stack_column_prefix_to_model_map[stack_column_prefix]\n                    new_base_model = self.model_full_dict[base_model]\n                    new_base_model_type = self.model_types[new_base_model]\n                    new_base_model_path = self.model_paths[new_base_model]\n\n                    model_loaded.base_model_paths_dict[new_base_model] = new_base_model_path\n                    model_loaded.base_model_types_dict[new_base_model] = new_base_model_type\n                    model_loaded.base_model_names.append(new_base_model)\n                    model_loaded.stack_column_prefix_to_model_map[stack_column_prefix] = new_base_model\n\n            model_loaded.save()  # TODO: Avoid this!\n\n            # TODO: Consider moving into internal function in model to update graph with node + links?\n            self.model_graph.add_node(model_loaded.name, fit_time=model_loaded.fit_time, predict_time=model_loaded.predict_time, val_score=model_loaded.val_score, can_infer=model_loaded.can_infer())\n            if isinstance(model_loaded, StackerEnsembleModel):\n                for stack_column_prefix in model_loaded.stack_column_prefix_lst:\n                    base_model_name = model_loaded.stack_column_prefix_to_model_map[stack_column_prefix]\n                    self.model_graph.add_edge(base_model_name, model_loaded.name)\n\n        self.save()\n        return copy.deepcopy(self.model_full_dict)\n\n    # TODO: Take best performance model with lowest inference\n    def best_single_model(self, stack_name, stack_level):\n        """""" Returns name of best single model in this trainer object, at a particular stack_level with particular stack_name.\n\n            Examples:\n                To get get best single (refit_single_full) model:\n                    trainer.best_single_model(\'refit_single_full\', 0)  # TODO: does not work because FULL models have no validation score.\n                To get best single (distilled) model:\n                    trainer.best_single_model(\'distill\', 0)\n        """"""\n        models = self.models_level[stack_name][stack_level]\n        perfs = [(m, self.model_performance[m]) for m in models if self.model_performance[m] is not None]\n        if not perfs:\n            raise AssertionError(\'No fit models exist with a validation score to choose the best model.\')\n        return max(perfs, key=lambda i: i[1])[0]\n\n    # TODO: Take best performance model with lowest inference\n    def get_model_best(self, can_infer=None, allow_full=True):\n        models = self.get_model_names_all(can_infer=can_infer)\n        if not models:\n            raise AssertionError(\'Trainer has no fit models that can infer.\')\n        perfs = [(m, self.model_performance[m]) for m in models if self.model_performance[m] is not None]\n        if not perfs:\n            model_full_dict_inverse = {full: orig for orig, full in self.model_full_dict.items()}\n            models = [m for m in models if m in model_full_dict_inverse]\n            perfs = [(m, self.model_performance[model_full_dict_inverse[m]]) for m in models if self.model_performance[model_full_dict_inverse[m]] is not None]\n            if not perfs:\n                raise AssertionError(\'No fit models that can infer exist with a validation score to choose the best model.\')\n            elif not allow_full:\n                raise AssertionError(\'No fit models that can infer exist with a validation score to choose the best model, but refit_full models exist. Set `allow_full=True` to get the best refit_full model.\')\n        return max(perfs, key=lambda i: i[1])[0]\n\n    def save_model(self, model, reduce_memory=True):\n        # TODO: In future perhaps give option for the reduce_memory_size arguments, perhaps trainer level variables specified by user?\n        if reduce_memory:\n            model.reduce_memory_size(remove_fit=True, remove_info=False, requires_save=True)\n        if self.low_memory:\n            model.save()\n        else:\n            self.models[model.name] = model\n\n    def save(self):\n        save_pkl.save(path=self.path + self.trainer_file_name, object=self)\n\n    def load_models_into_memory(self, model_names=None):\n        if model_names is None:\n            model_names = self.get_model_names_all()\n        models = []\n        for model_name in model_names:\n            model = self.load_model(model_name)\n            self.models[model.name] = model\n            models.append(model)\n\n        for model in models:\n            if isinstance(model, StackerEnsembleModel):\n                for base_model_name in model.base_model_names:\n                    if base_model_name not in model.base_models_dict.keys():\n                        if base_model_name in self.models.keys():\n                            model.base_models_dict[base_model_name] = self.models[base_model_name]\n            if isinstance(model, BaggedEnsembleModel):\n                for fold, fold_model in enumerate(model.models):\n                    if isinstance(fold_model, str):\n                        model.models[fold] = model.load_child(fold_model)\n\n    # TODO: model_name change to model in params\n    def load_model(self, model_name: str, path: str = None, model_type=None) -> AbstractModel:\n        if isinstance(model_name, AbstractModel):\n            return model_name\n        if model_name in self.models.keys():\n            return self.models[model_name]\n        else:\n            if path is None:\n                path = self.model_paths[model_name]\n            if model_type is None:\n                model_type = self.model_types[model_name]\n            return model_type.load(path=path, reset_paths=self.reset_paths)\n\n    def _get_dummy_stacker(self, level, model_levels=None, use_orig_features=True):\n        if model_levels is None:\n            model_levels = self.models_level[\'core\']\n        model_names = model_levels[level - 1]\n        base_models_dict = {}\n        for model_name in model_names:\n            if model_name in self.models.keys():\n                base_models_dict[model_name] = self.models[model_name]\n        dummy_stacker = StackerEnsembleModel(\n            path=\'\', name=\'\',\n            model_base=AbstractModel(path=\'\', name=\'\', problem_type=self.problem_type, objective_func=self.objective_func),\n            base_model_names=model_names, base_models_dict=base_models_dict, base_model_paths_dict=self.model_paths,\n            base_model_types_dict=self.model_types, use_orig_features=use_orig_features, num_classes=self.num_classes, random_state=level+self.random_seed\n        )\n        return dummy_stacker\n\n    # TODO: Enable raw=True for bagged models when X=None\n    #  This is non-trivial to implement for multi-layer stacking ensembles on the OOF data.\n    # TODO: Consider limiting X to 10k rows here instead of inside the model call\n    def get_feature_importance(self, model=None, X=None, y=None, features=None, raw=True, subsample_size=1000, silent=False):\n        if model is None:\n            model = self.model_best\n        model: AbstractModel = self.load_model(model)\n        if X is None and model.val_score is None:\n            raise AssertionError(f\'Model {model.name} is not valid for generating feature importances on original training data because no validation data was used during training, please specify new test data to compute feature importances.\')\n\n        if X is None:\n            if isinstance(model, WeightedEnsembleModel):\n                if self.bagged_mode:\n                    if raw:\n                        raise AssertionError(\'`feature_stage=\\\'transformed\\\'` feature importance on the original training data is not yet supported when bagging is enabled, please specify new test data to compute feature importances.\')\n                    X = None\n                    is_oof = True\n                else:\n                    if raw:\n                        X = self.load_X_val()\n                    else:\n                        X = None\n                    is_oof = False\n            elif isinstance(model, BaggedEnsembleModel):\n                if raw:\n                    raise AssertionError(\'`feature_stage=\\\'transformed\\\'` feature importance on the original training data is not yet supported when bagging is enabled, please specify new test data to compute feature importances.\')\n                X = self.load_X_train()\n                X = self.get_inputs_to_model(model=model, X=X, fit=True)\n                is_oof = True\n            else:\n                X = self.load_X_val()\n                if not raw:\n                    X = self.get_inputs_to_model(model=model, X=X, fit=False)\n                is_oof = False\n        else:\n            is_oof = False\n            if not raw:\n                X = self.get_inputs_to_model(model=model, X=X, fit=False)\n\n        if y is None and X is not None:\n            if is_oof:\n                y = self.load_y_train()\n            else:\n                y = self.load_y_val()\n\n        if raw:\n            feature_importance = self._get_feature_importance_raw(model=model, X=X, y=y, features_to_use=features, subsample_size=subsample_size, silent=silent)\n        else:\n            feature_importance = model.compute_feature_importance(X=X, y=y, features_to_use=features, preprocess=False, subsample_size=subsample_size, is_oof=is_oof, silent=silent)\n        return feature_importance\n\n    # TODO: Can get feature importances of all children of model at no extra cost, requires scoring the values after predict_proba on each model\n    #  Could solve by adding a self.score_all() function which takes model as input and also returns scores of all children models.\n    #  This would be best solved after adding graph representation, it lives most naturally in AbstractModel\n    # TODO: Can skip features which were pruned on all models that model depends on (Complex to implement, requires graph representation)\n    # TODO: Note that raw importance will not equal non-raw importance for bagged models, even if raw features are identical to the model features.\n    #  This is because for non-raw, we do an optimization where each fold model calls .compute_feature_importance(), and then the feature importances are averaged across the folds.\n    #  This is different from raw, where the predictions of the folds are averaged and then feature importance is computed.\n    #  Consider aligning these methods so they produce the same result.\n    # The output of this function is identical to non-raw when model is level 0 and non-bagged\n    def _get_feature_importance_raw(self, model, X, y, features_to_use=None, subsample_size=1000, transform_func=None, silent=False):\n        time_start = time.time()\n        if model is None:\n            model = self.model_best\n        model: AbstractModel = self.load_model(model)\n        if features_to_use is None:\n            features_to_use = list(X.columns)\n        feature_count = len(features_to_use)\n\n        if not silent:\n            logger.log(20, f\'Computing raw permutation importance for {feature_count} features on {model.name} ...\')\n\n        if (subsample_size is not None) and (len(X) > subsample_size):\n            X = X.sample(subsample_size, random_state=0)\n            y = y.loc[X.index]\n\n        time_start_score = time.time()\n        if transform_func is None:\n            score_baseline = self.score(X=X, y=y, model=model)\n        else:\n            X_transformed = transform_func(X)\n            score_baseline = self.score(X=X_transformed, y=y, model=model)\n        time_score = time.time() - time_start_score\n\n        if not silent:\n            time_estimated = (feature_count + 1) * time_score + time_start_score - time_start\n            logger.log(20, f\'\\t{round(time_estimated, 2)}s\\t= Expected runtime\')\n\n        X_shuffled = shuffle_df_rows(X=X, seed=0)\n\n        # Assuming X_test or X_val\n        # TODO: Can check multiple features at a time only if non-OOF\n        permutation_importance_dict = dict()\n        X_to_check = X.copy()\n        last_processed = None\n        for feature in features_to_use:\n            if last_processed is not None:  # resetting original values\n                X_to_check[last_processed] = X[last_processed].values\n            X_to_check[feature] = X_shuffled[feature].values\n            if transform_func is None:\n                score_feature = self.score(X=X_to_check, y=y, model=model)\n            else:\n                X_to_check_transformed = transform_func(X_to_check)\n                score_feature = self.score(X=X_to_check_transformed, y=y, model=model)\n            score_diff = score_baseline - score_feature\n            permutation_importance_dict[feature] = score_diff\n            last_processed = feature\n        feature_importances = pd.Series(permutation_importance_dict).sort_values(ascending=False)\n\n        if not silent:\n            logger.log(20, f\'\\t{round(time.time() - time_start, 2)}s\\t= Actual runtime\')\n\n        return feature_importances\n\n    def get_models_load_info(self, model_names):\n        model_names = copy.deepcopy(model_names)\n        model_paths = {model_name: self.model_paths[model_name] for model_name in model_names}\n        model_types = {model_name: self.model_types[model_name] for model_name in model_names}\n        return model_names, model_paths, model_types\n\n    # Sums the attribute value across all models that the provided model depends on, including itself.\n    # For instance, this function can return the expected total predict_time of a model.\n    # attribute is the name of the desired attribute to be summed.\n    def get_model_attribute_full(self, model, attribute):\n        base_model_set = self.get_minimum_model_set(model)\n        if len(base_model_set) == 1:\n            return self.model_graph.nodes[base_model_set[0]][attribute]\n        attribute_full = 0\n        for base_model in base_model_set:\n            if self.model_graph.nodes[base_model][attribute] is None:\n                return None\n            attribute_full += self.model_graph.nodes[base_model][attribute]\n        return attribute_full\n\n    # Returns dictionary of model name -> attribute value for the provided attribute\n    def get_model_attributes_dict(self, attribute):\n        return nx.get_node_attributes(self.model_graph, attribute)\n\n    # Gets the minimum set of models that the provided model depends on, including itself\n    # Returns a list of model names\n    def get_minimum_model_set(self, model):\n        if not isinstance(model, str):\n            model = model.name\n        return list(nx.bfs_tree(self.model_graph, model, reverse=True))\n\n    def leaderboard(self):\n        model_names = self.get_model_names_all()\n        score_val = []\n        fit_time_marginal = []\n        pred_time_val_marginal = []\n        stack_level = []\n        fit_time = []\n        pred_time_val = []\n        can_infer = []\n        score_val_dict = self.get_model_attributes_dict(\'val_score\')\n        fit_time_marginal_dict = self.get_model_attributes_dict(\'fit_time\')\n        predict_time_marginal_dict = self.get_model_attributes_dict(\'predict_time\')\n        for model_name in model_names:\n            score_val.append(score_val_dict[model_name])\n            fit_time_marginal.append(fit_time_marginal_dict[model_name])\n            fit_time.append(self.get_model_attribute_full(model=model_name, attribute=\'fit_time\'))\n            pred_time_val_marginal.append(predict_time_marginal_dict[model_name])\n            pred_time_val.append(self.get_model_attribute_full(model=model_name, attribute=\'predict_time\'))\n            stack_level.append(self.get_model_level(model_name))\n            can_infer.append(self.model_graph.nodes[model_name][\'can_infer\'])\n        df = pd.DataFrame(data={\n            \'model\': model_names,\n            \'score_val\': score_val,\n            \'pred_time_val\': pred_time_val,\n            \'fit_time\': fit_time,\n            \'pred_time_val_marginal\': pred_time_val_marginal,\n            \'fit_time_marginal\': fit_time_marginal,\n            \'stack_level\': stack_level,\n            \'can_infer\': can_infer,\n        })\n        df_sorted = df.sort_values(by=[\'score_val\', \'pred_time_val\', \'model\'], ascending=[False, True, False]).reset_index(drop=True)\n        return df_sorted\n\n    def get_info(self, include_model_info=False):\n        num_models_trained = len(self.get_model_names_all())\n        if self.model_best is not None:\n            best_model = self.model_best\n        else:\n            best_model = self.get_model_best()\n        best_model_score_val = self.model_performance.get(best_model)\n        # fit_time = None\n        num_bagging_folds = self.kfolds\n        max_core_stack_level = self.get_max_level(\'core\')\n        max_stack_level = self.get_max_level_all()\n        best_model_stack_level = self.get_model_level(best_model)\n        problem_type = self.problem_type\n        objective_func = self.objective_func.name\n        stopping_metric = self.stopping_metric.name\n        time_train_start = self.time_train_start\n        num_rows_train = self.num_rows_train\n        num_cols_train = self.num_cols_train\n        num_classes = self.num_classes\n        # TODO:\n        #  Disk size of models\n        #  Raw feature count\n        #  HPO time\n        #  Bag time\n        #  Feature prune time\n        #  Exception count / models failed count\n        #  True model count (models * kfold)\n        #  AutoGluon version fit on\n        #  Max memory usage\n        #  CPU count used / GPU count used\n\n        info = {\n            \'time_train_start\': time_train_start,\n            \'num_rows_train\': num_rows_train,\n            \'num_cols_train\': num_cols_train,\n            \'num_classes\': num_classes,\n            \'problem_type\': problem_type,\n            \'eval_metric\': objective_func,\n            \'stopping_metric\': stopping_metric,\n            \'best_model\': best_model,\n            \'best_model_score_val\': best_model_score_val,\n            \'best_model_stack_level\': best_model_stack_level,\n            \'num_models_trained\': num_models_trained,\n            \'num_bagging_folds\': num_bagging_folds,\n            \'max_stack_level\': max_stack_level,\n            \'max_core_stack_level\': max_core_stack_level,\n            \'model_stack_info\': self.models_level.copy(),\n        }\n\n        if include_model_info:\n            info[\'model_info\'] = self.get_models_info()\n\n        return info\n\n    def get_models_info(self, models=None):\n        if models is None:\n            models = self.get_model_names_all()\n        model_info_dict = dict()\n        for model in models:\n            if isinstance(model, str):\n                if model in self.models.keys():\n                    model = self.models[model]\n            if isinstance(model, str):\n                model_type = self.model_types[model]\n                model_path = self.model_paths[model]\n                model_info_dict[model] = model_type.load_info(path=model_path)\n            else:\n                model_info_dict[model.name] = model.get_info()\n        return model_info_dict\n\n    def reduce_memory_size(self, remove_data=True, remove_fit_stack=False, remove_fit=True, remove_info=False, requires_save=True, reduce_children=False, **kwargs):\n        if remove_data and self.is_data_saved:\n            data_files = [\n                self.path_data + \'X_train.pkl\',\n                self.path_data + \'X_val.pkl\',\n                self.path_data + \'y_train.pkl\',\n                self.path_data + \'y_val.pkl\',\n            ]\n            for data_file in data_files:\n                try:\n                    os.remove(data_file)\n                except FileNotFoundError:\n                    pass\n            if requires_save:\n                self.is_data_saved = False\n            try:\n                os.rmdir(self.path_data)\n            except OSError:\n                pass\n            try:\n                os.rmdir(self.path_utils)\n            except OSError:\n                pass\n        models = self.get_model_names_all()\n        for model in models:\n            model = self.load_model(model)\n            model.reduce_memory_size(remove_fit_stack=remove_fit_stack, remove_fit=remove_fit, remove_info=remove_info, requires_save=requires_save, reduce_children=reduce_children, **kwargs)\n            if requires_save:\n                self.save_model(model, reduce_memory=False)\n        if requires_save:\n            self.save()\n\n    # TODO: Also enable deletion of models which didn\'t succeed in training (files may still be persisted)\n    #  This includes the original HPO fold for stacking\n    # Deletes specified models from trainer and from disk (if delete_from_disk=True).\n    def delete_models(self, models_to_keep=None, models_to_delete=None, allow_delete_cascade=False, delete_from_disk=True, dry_run=True):\n        if models_to_keep is not None and models_to_delete is not None:\n            raise ValueError(\'Exactly one of [models_to_keep, models_to_delete] must be set.\')\n        if models_to_keep is not None:\n            if not isinstance(models_to_keep, list):\n                models_to_keep = [models_to_keep]\n            minimum_model_set = set()\n            for model in models_to_keep:\n                minimum_model_set.update(self.get_minimum_model_set(model))\n            minimum_model_set = list(minimum_model_set)\n            models_to_remove = [model for model in self.get_model_names_all() if model not in minimum_model_set]\n        elif models_to_delete is not None:\n            if not isinstance(models_to_delete, list):\n                models_to_delete = [models_to_delete]\n            minimum_model_set = set(models_to_delete)\n            minimum_model_set_orig = copy.deepcopy(minimum_model_set)\n            for model in models_to_delete:\n                minimum_model_set.update(nx.algorithms.dag.descendants(self.model_graph, model))\n            if not allow_delete_cascade:\n                if minimum_model_set != minimum_model_set_orig:\n                    raise AssertionError(\'models_to_delete contains models which cause a delete cascade due to other models being dependent on them. Set allow_delete_cascade=True to enable the deletion.\')\n            minimum_model_set = list(minimum_model_set)\n            models_to_remove = [model for model in self.get_model_names_all() if model in minimum_model_set]\n        else:\n            raise ValueError(\'Exactly one of [models_to_keep, models_to_delete] must be set.\')\n\n        if dry_run:\n            logger.log(30, f\'Dry run enabled, AutoGluon would have deleted the following models: {models_to_remove}\')\n            if delete_from_disk:\n                for model in models_to_remove:\n                    model = self.load_model(model)\n                    logger.log(30, f\'\\tDirectory {model.path} would have been deleted.\')\n            logger.log(30, f\'To perform the deletion, set dry_run=False\')\n            return\n\n        self.model_graph.remove_nodes_from(models_to_remove)\n        for model in models_to_remove:\n            if model in self.models:\n                self.models.pop(model)\n\n        models_kept = self.get_model_names_all()\n        # TODO: Refactor this part, link models_level to model_graph\n        for key in self.models_level:\n            for level in self.models_level[key]:\n                self.models_level[key][level] = [model for model in self.models_level[key][level] if model in models_kept]\n\n        if self.model_best is not None and self.model_best not in models_kept:\n            try:\n                self.model_best = self.get_model_best()\n            except AssertionError:\n                self.model_best = None\n\n        # TODO: Delete from all the other model dicts\n        self.save()\n        if delete_from_disk:\n            for model in models_to_remove:\n                model = self.load_model(model)\n                model.delete_from_disk()\n\n    @classmethod\n    def load(cls, path, reset_paths=False):\n        load_path = path + cls.trainer_file_name\n        if not reset_paths:\n            return load_pkl.load(path=load_path)\n        else:\n            obj = load_pkl.load(path=load_path)\n            obj.set_contexts(path)\n            obj.reset_paths = reset_paths\n            return obj\n\n    @classmethod\n    def load_info(cls, path, reset_paths=False, load_model_if_required=True):\n        load_path = path + cls.trainer_info_name\n        try:\n            return load_pkl.load(path=load_path)\n        except:\n            if load_model_if_required:\n                trainer = cls.load(path=path, reset_paths=reset_paths)\n                return trainer.get_info()\n            else:\n                raise\n\n    def save_info(self, include_model_info=False):\n        info = self.get_info(include_model_info=include_model_info)\n\n        save_pkl.save(path=self.path + self.trainer_info_name, object=info)\n        save_json.save(path=self.path + self.trainer_info_json_name, obj=info)\n        return info\n\n    def _process_hyperparameters(self, hyperparameters):\n        hyperparameters = copy.deepcopy(hyperparameters)\n\n        has_levels = False\n        top_level_keys = hyperparameters.keys()\n        for key in top_level_keys:\n            if isinstance(key, int) or key == \'default\':\n                has_levels = True\n        if not has_levels:\n            hyperparameters = {\'default\': hyperparameters}\n        top_level_keys = hyperparameters.keys()\n        for key in top_level_keys:\n            for subkey in hyperparameters[key].keys():\n                if not isinstance(hyperparameters[key][subkey], list):\n                    hyperparameters[key][subkey] = [hyperparameters[key][subkey]]\n                models_expanded = []\n                for i, model in enumerate(hyperparameters[key][subkey]):\n                    if isinstance(model, str):\n                        candidate_models = get_preset_custom(name=model, problem_type=self.problem_type, num_classes=self.num_classes)\n                    else:\n                        candidate_models = [model]\n                    valid_models = []\n                    for candidate in candidate_models:\n                        is_valid = True\n                        if AG_ARGS in candidate:\n                            model_valid_problem_types = candidate[AG_ARGS].get(\'problem_types\', None)\n                            if model_valid_problem_types is not None:\n                                if self.problem_type not in model_valid_problem_types:\n                                    is_valid = False\n                        if is_valid:\n                            valid_models.append(candidate)\n                    models_expanded += valid_models\n\n                hyperparameters[key][subkey] = models_expanded\n        if \'default\' not in hyperparameters.keys():\n            level_keys = [key for key in hyperparameters.keys() if isinstance(key, int)]\n            max_level_key = max(level_keys)\n            hyperparameters[\'default\'] = copy.deepcopy(hyperparameters[max_level_key])\n        return hyperparameters\n'"
autogluon/utils/tabular/ml/trainer/auto_trainer.py,0,"b""import logging\nimport pandas as pd\n\nfrom .abstract_trainer import AbstractTrainer\nfrom .model_presets.presets import get_preset_models\nfrom ..utils import generate_train_test_split\n\nlogger = logging.getLogger(__name__)\n\n\n# This Trainer handles model training details\nclass AutoTrainer(AbstractTrainer):\n    def get_models(self, hyperparameters, hyperparameter_tune=False, level='default', extra_ag_args_fit=None, **kwargs):\n        return get_preset_models(path=self.path, problem_type=self.problem_type, objective_func=self.objective_func, stopping_metric=self.stopping_metric,\n                                 num_classes=self.num_classes, hyperparameters=hyperparameters, hyperparameter_tune=hyperparameter_tune, level=level, extra_ag_args_fit=extra_ag_args_fit)\n\n    def train(self, X_train, y_train, X_test=None, y_test=None, hyperparameter_tune=True, feature_prune=False, holdout_frac=0.1, hyperparameters=None):\n        if hyperparameters is None:\n            hyperparameters = {}\n        self.hyperparameters = self._process_hyperparameters(hyperparameters=hyperparameters)\n        models = self.get_models(self.hyperparameters, hyperparameter_tune=hyperparameter_tune, level=0)\n        if self.bagged_mode:\n            if (y_test is not None) and (X_test is not None):\n                # TODO: User could be intending to blend instead. Perhaps switch from OOF preds to X_test preds while still bagging? Doubt a user would want this.\n                logger.warning('Warning: Training AutoGluon in Bagged Mode but X_test is specified, concatenating X_train and X_test for cross-validation')\n                X_train = pd.concat([X_train, X_test], ignore_index=True)\n                y_train = pd.concat([y_train, y_test], ignore_index=True)\n            X_test = None\n            y_test = None\n        else:\n            if (y_test is None) or (X_test is None):\n                X_train, X_test, y_train, y_test = generate_train_test_split(X_train, y_train, problem_type=self.problem_type, test_size=holdout_frac, random_state=self.random_seed)\n        self.train_multi_and_ensemble(X_train, y_train, X_test, y_test, models, hyperparameter_tune=hyperparameter_tune, feature_prune=feature_prune)\n"""
autogluon/utils/tabular/ml/tuning/__init__.py,0,b''
autogluon/utils/tabular/ml/tuning/ensemble_selection.py,0,"b'import logging, time\nimport numpy as np\nfrom collections import Counter\n\nfrom ..constants import PROBLEM_TYPES\nfrom ...metrics import calculate_score, _ProbaScorer, _ThresholdScorer\nfrom ..utils import get_pred_from_proba\n\nlogger = logging.getLogger(__name__)\n\n\nclass EnsembleSelection:\n    def __init__(\n            self,\n            ensemble_size: int,\n            problem_type: str,\n            metric,\n            sorted_initialization: bool = False,\n            bagging: bool = False,\n            random_state: np.random.RandomState = None,\n    ):\n        self.ensemble_size = ensemble_size\n        self.problem_type = problem_type\n        self.metric = metric\n        self.sorted_initialization = sorted_initialization\n        self.bagging = bagging\n        self.use_best = True\n        if random_state is not None:\n            self.random_state = random_state\n        else:\n            self.random_state = np.random.RandomState(seed=0)\n        if isinstance(metric, _ProbaScorer):\n            self.objective_func_expects_y_pred = False\n        elif isinstance(metric, _ThresholdScorer):\n            self.objective_func_expects_y_pred = False\n        else:\n            self.objective_func_expects_y_pred = True\n\n    def fit(self, predictions, labels, time_limit=None, identifiers=None):\n        self.ensemble_size = int(self.ensemble_size)\n        if self.ensemble_size < 1:\n            raise ValueError(\'Ensemble size cannot be less than one!\')\n        if not self.problem_type in PROBLEM_TYPES:\n            raise ValueError(\'Unknown problem type %s.\' % self.problem_type)\n        # if not isinstance(self.metric, Scorer):\n        #     raise ValueError(\'Metric must be of type scorer\')\n\n        self._fit(predictions=predictions, labels=labels, time_limit=time_limit)\n        self._calculate_weights()\n        logger.log(15, \'Ensemble weights: \')\n        logger.log(15, self.weights_)\n        return self\n\n    # TODO: Consider having a removal stage, remove each model and see if score is affected, if improves or not effected, remove it.\n    def _fit(self, predictions, labels, time_limit=None):\n        ensemble_size = self.ensemble_size\n        self.num_input_models_ = len(predictions)\n        ensemble = []\n        trajectory = []\n        order = []\n\n        # if self.sorted_initialization:\n        #     n_best = 20\n        #     indices = self._sorted_initialization(predictions, labels, n_best)\n        #     for idx in indices:\n        #         ensemble.append(predictions[idx])\n        #         order.append(idx)\n        #         ensemble_ = np.array(ensemble).mean(axis=0)\n        #         ensemble_performance = calculate_score(\n        #             labels, ensemble_, self.task_type, self.metric,\n        #             ensemble_.shape[1])\n        #         trajectory.append(ensemble_performance)\n        #     ensemble_size -= n_best\n\n        time_start = time.time()\n        for i in range(ensemble_size):\n            scores = np.zeros((len(predictions)))\n            s = len(ensemble)\n            if s == 0:\n                weighted_ensemble_prediction = np.zeros(predictions[0].shape)\n            else:\n                # Memory-efficient averaging!\n                ensemble_prediction = np.zeros(ensemble[0].shape)\n                for pred in ensemble:\n                    ensemble_prediction += pred\n                ensemble_prediction /= s\n\n                weighted_ensemble_prediction = (s / float(s + 1)) * \\\n                                               ensemble_prediction\n            fant_ensemble_prediction = np.zeros(weighted_ensemble_prediction.shape)\n            for j, pred in enumerate(predictions):\n                fant_ensemble_prediction[:] = weighted_ensemble_prediction + (1. / float(s + 1)) * pred\n                if self.objective_func_expects_y_pred:\n                    preds = get_pred_from_proba(y_pred_proba=fant_ensemble_prediction, problem_type=self.problem_type)\n                else:\n                    preds = fant_ensemble_prediction\n\n                scores[j] = self.metric._optimum - calculate_score(\n                    solution=labels,\n                    prediction=preds,\n                    task_type=self.problem_type,\n                    metric=self.metric,\n                    all_scoring_functions=False)\n\n                # scores[j] = -self.metric(y_true=labels, y_pred=fant_ensemble_prediction)\n            # print(\'scores:\', scores)\n            all_best = np.argwhere(scores == np.nanmin(scores)).flatten()\n            best = self.random_state.choice(all_best)\n\n            # TODO: Instead of selecting random, compute additional metric which can be a tie-breaker!\n\n            ensemble.append(predictions[best])\n            trajectory.append(scores[best])\n            order.append(best)\n\n            # Handle special case\n            if len(predictions) == 1:\n                break\n\n            if time_limit is not None:\n                time_elapsed = time.time() - time_start\n                time_left = time_limit - time_elapsed\n                if time_left <= 0:\n                    logger.warning(\'Warning: Ensemble Selection ran out of time, early stopping at iteration %s. This may mean that the time_limit specified is very small for this problem.\' % (i+1))\n                    break\n\n        min_score = np.min(trajectory)\n        first_index_of_best = trajectory.index(min_score)\n\n        self.indices_ = order\n        self.trajectory_ = trajectory\n        self.train_score_ = trajectory[-1]  # TODO: Select best iteration or select final iteration? Earlier iteration could have a better score!\n\n        if self.use_best:\n            self.indices_ = order[:first_index_of_best+1]\n            self.trajectory_ = trajectory[:first_index_of_best+1]\n            self.train_score_ = trajectory[first_index_of_best]  # TODO: Select best iteration or select final iteration? Earlier iteration could have a better score!\n            self.ensemble_size = first_index_of_best + 1\n            logger.log(15, \'Ensemble size: %s\' % self.ensemble_size)\n\n        logger.debug(""Ensemble indices: ""+str(self.indices_))\n\n    def _calculate_weights(self):\n        ensemble_members = Counter(self.indices_).most_common()\n        weights = np.zeros((self.num_input_models_,), dtype=float)\n        for ensemble_member in ensemble_members:\n            weight = float(ensemble_member[1]) / self.ensemble_size\n            weights[ensemble_member[0]] = weight\n\n        if np.sum(weights) < 1:\n            weights = weights / np.sum(weights)\n\n        self.weights_ = weights\n\n    def predict(self, X):\n        y_pred_proba = self.predict_proba(X)\n        return get_pred_from_proba(y_pred_proba=y_pred_proba, problem_type=self.problem_type)\n\n    def predict_proba(self, X):\n        return self.weight_pred_probas(X, weights=self.weights_)\n\n    @staticmethod\n    def weight_pred_probas(pred_probas, weights):\n        preds_norm = [pred * weight for pred, weight in zip(pred_probas, weights)]\n        preds_ensemble = np.sum(preds_norm, axis=0)\n        return preds_ensemble\n'"
autogluon/utils/tabular/ml/tuning/feature_pruner.py,0,"b'# import warnings\n# warnings.filterwarnings(\'ignore\')\nimport copy, logging\nimport pandas as pd\n\nlogger = logging.getLogger(__name__)\n\n\n# TODO: currently is buggy\nclass FeaturePruner:\n    def __init__(self, model_base, threshold_baseline=0.004, is_fit=False):\n        self.model_base = model_base\n        self.threshold_baseline = threshold_baseline\n        self.is_fit = is_fit\n\n        self.best_score = 0\n        self.best_iteration = 0\n        self.features_in_iter = []\n        self.score_in_iter = []\n        self.valid_feature_counts = []\n        self.gain_dfs = []\n        self.banned_features_in_iter = []\n        self.thresholds = []\n        self.threshold = self.threshold_baseline\n        self.cur_iteration = 0\n        self.tuned = False\n        self.early_stopping_rounds = 2\n\n    def evaluate(self):\n        untuned_score = self.score_in_iter[0]\n        logger.debug(\'untuned_score: %s\' % untuned_score)\n        logger.debug(\'best_score: %s\' % self.best_score)\n        logger.debug(\'best_iteration: %s\' % self.best_iteration)\n\n        data_dict = {\n            \'score\': self.score_in_iter,\n            \'feature_count\': self.valid_feature_counts,\n            \'threshold\': self.thresholds,\n        }\n        logger.debug(self.gain_dfs[self.best_iteration])\n        z = pd.DataFrame(data=data_dict)\n        # logger.debug(z)\n        z_sorted = z.sort_values(by=[\'score\'], ascending=False)\n        # logger.debug(z_sorted)\n\n    # TODO: CV5 instead of holdout? Should be better\n    # TODO: Add holdout here, it is overfitting with Logistic Regression\n    def tune(self, X_train, y_train, X_test, y_test, X_holdout, y_holdout, total_runs=999):\n        objective_goal_is_negative = False  # Fixed to false if using sklearn scorers # self.model_base.problem_type == REGRESSION  # TODO: if objective function goal = lower (logloss, MAE, etc.)\n        logger.log(15, \'Feature-pruning \'+str(self.model_base.name)+\' for \'+str(total_runs)+\' runs...\')\n\n        if len(self.features_in_iter) == 0:\n            valid_features = X_train.columns.values\n        else:\n            valid_features = self.features_in_iter[-1]\n\n        iter_since_best = 0\n        iter_start = self.cur_iteration\n        for iteration in range(self.cur_iteration, total_runs):\n            self.cur_iteration = iteration\n            logger.debug(\'iteration: %s \' % iteration)\n            X_train_subset = X_train[valid_features].copy()\n            X_test_subset = X_test[valid_features].copy()\n            self.thresholds.append(self.threshold)\n            self.valid_feature_counts.append(len(valid_features))\n            self.features_in_iter.append(valid_features)\n\n            if self.is_fit and (iteration == iter_start):\n                model_iter = self.model_base\n            else:\n                model_iter = copy.deepcopy(self.model_base)\n                model_iter.fit(X_train=X_train_subset, Y_train=y_train, X_test=X_test_subset, Y_test=y_test)\n\n            banned_features = []\n\n            feature_importance = None\n            if hasattr(model_iter.model, \'feature_importances_\'):\n                feature_importance = model_iter.model.feature_importances_\n            elif hasattr(model_iter.model, \'feature_importance\'):\n                feature_importance = model_iter.model.feature_importance()\n\n            if feature_importance is not None:\n                a = pd.Series(data=feature_importance, index=X_train_subset.columns)\n                unused_features = a[a == 0]\n\n                logger.log(15, \'Unused features after pruning: \'+ str(list(unused_features.index)))\n                features_to_use = [feature for feature in valid_features if feature not in unused_features.index]\n                banned_features += list(unused_features.index)\n            else:\n                features_to_use = list(valid_features)\n\n            cur_score_val = model_iter.score(X=X_test_subset, y=y_test)\n            cur_score = model_iter.score(X=X_holdout[valid_features], y=y_holdout)\n\n            logger.log(15, \'Iter \'+str(iteration)+\'  Score: \'+str(cur_score))\n            logger.log(15, \'Iter \'+str(iteration)+ \'  Score Val: \'+str(cur_score_val))\n            if objective_goal_is_negative:\n                cur_score = -cur_score\n\n            if self.best_score == 0 or cur_score > self.best_score:\n                logger.log(15, ""New best score found!"")\n                logger.log(15, str(cur_score)+ \' > \'+str(self.best_score))\n                self.best_score = cur_score\n                self.best_iteration = iteration\n                iter_since_best = 0\n            else:\n                iter_since_best += 1\n\n            self.score_in_iter.append(cur_score)\n\n            if iter_since_best >= self.early_stopping_rounds:\n                logger.log(15, ""Early stopping on iter %s"" % iteration)\n                logger.log(15, ""Best Iteration: %s"" % self.best_iteration)\n                self.tuned = True\n                break\n\n            gain_df = model_iter.compute_feature_importance(X=X_test_subset, y=y_test, features_to_use=features_to_use)\n            if not objective_goal_is_negative:\n                gain_df = -gain_df\n\n            self.gain_dfs.append(gain_df)\n\n            # TODO: Save gain_df, banned_features\n\n            self.threshold = self.adjust_threshold(gain_df, self.threshold)\n\n            banned_df = gain_df.loc[gain_df >= self.threshold].index\n            banned_features += list(banned_df)\n\n            logger.log(15, ""Banned features: ""+str(banned_features))\n            self.banned_features_in_iter.append(banned_features)\n            valid_features = [feature for feature in valid_features if feature not in banned_features]\n\n            if len(valid_features) == 0:\n                logger.log(15, \'No more features to remove, Feature pruning complete\')\n                logger.log(15, ""score_in_iter: ""+str(self.score_in_iter))\n                self.tuned = True\n                break\n        self.tuned = True\n\n    @staticmethod\n    def adjust_threshold(gain_df, threshold):\n        if gain_df.shape[0] == 0:\n            raise BaseException(\'ran out of features to prune!\')\n        banned_df = gain_df.loc[gain_df >= threshold].index\n        banned_features = list(banned_df)\n\n        if len(banned_features) == 0:\n            if threshold < -100000000:  # FIXME: Hacked for regression\n                raise BaseException(\'threshold already max!\')\n            elif threshold > 0.000001:\n                threshold_new = threshold / 2\n            elif threshold > 0:\n                threshold_new = 0\n            elif threshold == 0:\n                threshold_new = -0.00000001\n            elif (threshold < 0) and (threshold > -0.0001):\n                threshold_new = threshold * 2\n            else:\n                threshold_new = gain_df.max()\n\n            logger.log(10, \'Adjusting threshold to %s\' % threshold_new)\n            return FeaturePruner.adjust_threshold(gain_df=gain_df, threshold=threshold_new)\n        else:\n            return threshold\n'"
autogluon/utils/tabular/utils/loaders/__init__.py,0,b''
autogluon/utils/tabular/utils/loaders/load_pd.py,0,"b'import multiprocessing, logging\nimport pandas as pd\nfrom os import listdir\nfrom os.path import isfile, join\nfrom pandas import DataFrame\n\nfrom . import load_pointer\nfrom ..savers import save_pointer\nfrom .. import s3_utils, multiprocessing_utils\nfrom .load_s3 import list_bucket_prefix_suffix_s3\n\nlogger = logging.getLogger(__name__)\n\n\ndef load(path, delimiter=None, encoding=\'utf-8\', columns_to_keep=None, dtype=None, error_bad_lines=True, header=0,\n         names=None, format=None, nrows=None, skiprows=None, usecols=None, low_memory=False, converters=None, \n         filters=None, sample_count=None, worker_count=None, multiprocessing_method=\'forkserver\') -> DataFrame:\n    if isinstance(path, list):\n        return load_multipart(\n            paths=path, delimiter=delimiter, encoding=encoding, columns_to_keep=columns_to_keep,\n            dtype=dtype, error_bad_lines=error_bad_lines, header=header, names=names, format=format,\n            nrows=nrows, skiprows=skiprows, usecols=usecols, low_memory=low_memory, converters=converters,\n            filters=filters,\n            worker_count=worker_count,\n            multiprocessing_method=multiprocessing_method\n        )\n    if format is not None:\n        pass\n    elif path.endswith(save_pointer.POINTER_SUFFIX):\n        format = \'pointer\'\n    elif path[-1] == \'/\' and s3_utils.is_s3_url(path):  # and path[:2] == \'s3\'\n        format = \'multipart_s3\'\n    elif path[-1] == \'/\' and not s3_utils.is_s3_url(path):  # and path[:2] != \'s3\'\n        format = \'multipart_local\'\n    elif \'.parquet\' in path or path[-1] == \'/\':\n        format = \'parquet\'\n    else:\n        format = \'csv\'\n        if delimiter is None:\n            if path.endswith(\'.tsv\'):\n                delimiter = \'\\t\'\n                logger.debug(f\'File delimiter for {path} inferred as \\\'\\\\t\\\' (tab). If this is incorrect, please manually load the data as a pandas DataFrame.\')\n            else:\n                delimiter = \',\'\n                logger.debug(f\'File delimiter for {path} inferred as \\\',\\\' (comma). If this is incorrect, please manually load the data as a pandas DataFrame.\')\n\n    if format == \'pointer\':\n        content_path = load_pointer.get_pointer_content(path)\n        return load(path=content_path, delimiter=delimiter, encoding=encoding, columns_to_keep=columns_to_keep, dtype=dtype, \n                    error_bad_lines=error_bad_lines, header=header, names=names, format=None, nrows=nrows, skiprows=skiprows, \n                    usecols=usecols, low_memory=low_memory, converters=converters, filters=filters, sample_count=sample_count, \n                    worker_count=worker_count, multiprocessing_method=multiprocessing_method)\n    elif format == \'multipart_s3\':\n        bucket, prefix = s3_utils.s3_path_to_bucket_prefix(path)\n        return load_multipart_s3(bucket=bucket, prefix=prefix, columns_to_keep=columns_to_keep, dtype=dtype, filters=filters, \n                                 sample_count=sample_count, worker_count=worker_count, multiprocessing_method=multiprocessing_method)  # TODO: Add arguments!\n    elif format == \'multipart_local\':\n        paths = [join(path, f) for f in listdir(path) if (isfile(join(path, f))) & (f.startswith(\'part-\'))]\n        return load_multipart(\n            paths=paths, delimiter=delimiter, encoding=encoding, columns_to_keep=columns_to_keep,\n            dtype=dtype, error_bad_lines=error_bad_lines, header=header, names=names, format=None,\n            nrows=nrows, skiprows=skiprows, usecols=usecols, low_memory=low_memory, converters=converters,\n            filters=filters,\n            worker_count=worker_count,\n            multiprocessing_method=multiprocessing_method,\n        )\n    elif format == \'parquet\':\n        try:\n            df = pd.read_parquet(path, columns=columns_to_keep, engine=\'fastparquet\')  # TODO: Deal with extremely strange issue resulting from torch being present in package, will cause read_parquet to either freeze or Segmentation Fault when performing multiprocessing\n        except:\n            df = pd.read_parquet(path, columns=columns_to_keep, engine=\'pyarrow\')\n        column_count_full = len(df.columns)\n    elif format == \'csv\':\n        df = pd.read_csv(path, converters=converters, delimiter=delimiter, encoding=encoding, header=header, names=names, dtype=dtype, \n                         error_bad_lines=error_bad_lines, low_memory=low_memory, nrows=nrows, skiprows=skiprows, usecols=usecols)\n        column_count_full = len(list(df.columns.values))\n        if columns_to_keep is not None:\n            df = df[columns_to_keep]\n    else:\n        raise Exception(\'file format \' + format + \' not supported!\')\n\n    row_count = df.shape[0]\n\n    column_count_trimmed = len(list(df.columns.values))\n\n    if filters is not None:\n        if isinstance(filters, list):\n            for filter in filters:\n                df = filter(df)\n        else:\n            df = filters(df)\n\n    logger.log(20, ""Loaded data from: "" +str(path)+"" | Columns = ""+str(column_count_trimmed)+"" / ""+\n               str(column_count_full)+ "" | Rows = ""+str(row_count)+"" -> ""+ str(len(df)))\n    return df\n\n\ndef load_multipart_child(chunk):\n    path, delimiter, encoding, columns_to_keep, dtype, error_bad_lines, header, names, format, nrows, skiprows, usecols, low_memory, converters, filters = chunk\n    df = load(path=path, delimiter=delimiter, encoding=encoding, columns_to_keep=columns_to_keep,\n            dtype=dtype, error_bad_lines=error_bad_lines, header=header, names=names, format=format,\n            nrows=nrows, skiprows=skiprows, usecols=usecols, low_memory=low_memory, converters=converters,\n            filters=filters)\n    return df\n\n\ndef load_multipart(paths, delimiter=\',\', encoding=\'utf-8\', columns_to_keep=None, dtype=None, error_bad_lines=True, header=0,\n                   names=None, format=None, nrows=None, skiprows=None, usecols=None, low_memory=False, converters=None,\n                   filters=None, worker_count=None, multiprocessing_method=\'forkserver\'):\n    cpu_count = multiprocessing.cpu_count()\n    workers = int(round(cpu_count))\n    if worker_count is not None:\n        if worker_count <= workers:\n            workers = worker_count\n\n    logger.log(15, \'Load multipart running pool with \'+str(workers)+\' workers...\')\n\n    full_chunks = [[\n        path, delimiter, encoding, columns_to_keep, dtype, error_bad_lines, header, names,\n        format, nrows, skiprows, usecols, low_memory, converters, filters\n    ] for path in paths]\n\n    df_list = multiprocessing_utils.execute_multiprocessing(workers_count=workers, transformer=load_multipart_child, \n                                                    chunks=full_chunks, multiprocessing_method=multiprocessing_method)\n\n    df_combined = pd.concat(df_list, axis=0, ignore_index=True)\n\n    column_count = len(list(df_combined.columns.values))\n    row_count = df_combined.shape[0]\n\n    logger.log(20, ""Loaded data from multipart file | Columns = ""+str(column_count)+"" | Rows = ""+str(row_count))\n    return df_combined\n\n\n# Loads multiple files and concatenates row-wise (adding columns together)\ndef load_multi(path_list, delimiter=\',\', encoding=\'utf-8\', columns_to_keep_list=None, dtype_list=None):\n    num_files = len(path_list)\n\n    df_list = []\n    for i in range(num_files):\n        columns_to_keep = None\n        dtype = None\n        if dtype_list:\n            dtype = dtype_list[i]\n        if columns_to_keep_list:\n            columns_to_keep = columns_to_keep_list[i]\n        df = load(path_list[i], delimiter=delimiter, encoding=encoding, columns_to_keep=columns_to_keep, dtype=dtype)\n        df_list.append(df)\n\n    df_multi = pd.concat(df_list, axis=1, sort=False)\n\n    column_count = len(list(df_multi.columns.values))\n    row_count = df_multi.shape[0]\n    logger.log(20, ""Loaded data from ""+str(num_files)+"" files | Columns = ""+str(column_count)+"" | Rows = ""+str(row_count))\n    return df_multi\n\n\ndef load_multipart_s3(bucket, prefix, columns_to_keep=None, dtype=None, sample_count=None, filters=None, worker_count=None, multiprocessing_method=\'forkserver\'):\n    if prefix[-1] == \'/\':\n        prefix = prefix[:-1]\n    files = list_bucket_prefix_suffix_s3(bucket=bucket, prefix=prefix, suffix=\'/part-\')\n    files_cleaned = [file for file in files if prefix + \'/part-\' in file]\n    paths_full = [s3_utils.s3_bucket_prefix_to_path(bucket=bucket, prefix=file, version=\'s3\') for file in files_cleaned]\n    if sample_count is not None:\n        logger.log(15, \'Load multipart s3 taking sample of \'+str(sample_count)+\' out of \'+str(len(paths_full))+\' files to load\')\n        paths_full = paths_full[:sample_count]\n\n    df = load(path=paths_full, columns_to_keep=columns_to_keep, dtype=dtype, filters=filters, \n              worker_count=worker_count, multiprocessing_method=multiprocessing_method)\n    return df\n'"
autogluon/utils/tabular/utils/loaders/load_pkl.py,0,"b""import io, logging, pickle, boto3\n\nfrom . import load_pointer\nfrom .. import s3_utils\n\nlogger = logging.getLogger(__name__)\n\ndef load(path, format=None, verbose=True):\n    if path.endswith('.pointer'):\n        format = 'pointer'\n    elif s3_utils.is_s3_url(path):\n        format = 's3'\n    if format == 'pointer':\n        content_path = load_pointer.get_pointer_content(path)\n        if content_path == path:\n            raise RecursionError('content_path == path! : ' + str(path))\n        return load(path=content_path)\n    elif format == 's3':\n        if verbose: logger.log(15, 'Loading: %s' % path)\n        s3_bucket, s3_prefix = s3_utils.s3_path_to_bucket_prefix(s3_path=path)\n        s3 = boto3.resource('s3')\n        return pickle.loads(s3.Bucket(s3_bucket).Object(s3_prefix).get()['Body'].read())\n\n    if verbose: logger.log(15, 'Loading: %s' % path)\n    with open(path, 'rb') as fin:\n        object = pickle.load(fin)\n    return object\n\n\ndef load_with_fn(path, pickle_fn, format=None, verbose=True):\n    if path.endswith('.pointer'):\n        format = 'pointer'\n    elif s3_utils.is_s3_url(path):\n        format = 's3'\n    if format == 'pointer':\n        content_path = load_pointer.get_pointer_content(path)\n        if content_path == path:\n            raise RecursionError('content_path == path! : ' + str(path))\n        return load_with_fn(content_path, pickle_fn)\n    elif format == 's3':\n        if verbose: logger.log(15, 'Loading: %s' % path)\n        s3_bucket, s3_prefix = s3_utils.s3_path_to_bucket_prefix(s3_path=path)\n        s3 = boto3.resource('s3')\n        # Has to be wrapped in IO buffer since s3 stream does not implement seek()\n        buff = io.BytesIO(s3.Bucket(s3_bucket).Object(s3_prefix).get()['Body'].read())\n        return pickle_fn(buff)\n\n    if verbose: logger.log(15, 'Loading: %s' % path)\n    with open(path, 'rb') as fin:\n        object = pickle_fn(fin)\n    return object\n"""
autogluon/utils/tabular/utils/loaders/load_pointer.py,0,"b'import os, boto3, logging\n\nfrom .. import s3_utils\n\nlogger = logging.getLogger(__name__)\n\ndef get_pointer_content(path, verbose=True):\n    if s3_utils.is_s3_url(path):\n        bucket, key = s3_utils.s3_path_to_bucket_prefix(path)\n        s3 = boto3.resource(\'s3\')\n        obj = s3.Object(bucket, key)\n        content_path = obj.get()[\'Body\'].read().decode(\'utf-8\')\n    else:\n        os.makedirs(os.path.dirname(path), exist_ok=True)\n        f = open(path, ""r"")\n        content_path = f.read()\n        f.close()\n    if verbose:\n        logger.log(15, \'Loaded pointer file \'+str(path)+\' pointing to \'+str(content_path))\n\n    return content_path\n'"
autogluon/utils/tabular/utils/loaders/load_s3.py,0,"b""import boto3, os, pathlib, logging\n\nfrom . import load_pd\nfrom .. import s3_utils\n\nlogger = logging.getLogger(__name__)\n\ndef list_bucket_s3(bucket):\n    logger.log(15, 'Listing s3 bucket: '+str(bucket))\n\n    s3bucket = boto3.resource('s3')\n    my_bucket = s3bucket.Bucket(bucket)\n    files = []\n    for object in my_bucket.objects.all():\n        files.append(object.key)\n        logger.log(15, str(object.key))\n    return files\n\n\ndef download(input_bucket, input_prefix, local_path):\n    directory = os.path.dirname(local_path)\n    pathlib.Path(directory).mkdir(parents=True, exist_ok=True)\n\n    s3 = boto3.resource('s3')\n    s3.Bucket(input_bucket).download_file(input_prefix, local_path)\n\n\ndef list_bucket_prefix_s3(bucket, prefix):\n    return list_bucket_prefix_suffix_s3(bucket=bucket, prefix=prefix)\n\n\ndef list_bucket_prefix_suffix_s3(bucket, prefix, suffix=None, banned_suffixes=None):\n    if banned_suffixes is None:\n        banned_suffixes = []\n    s3 = boto3.resource('s3')\n    my_bucket = s3.Bucket(bucket)\n    prefix = prefix\n\n    files = []\n    for object_summary in my_bucket.objects.filter(Prefix=prefix):\n        suffix_full = object_summary.key.split(prefix, 1)[1]\n        is_banned = False\n        for banned_suffix in banned_suffixes:\n            if banned_suffix in suffix_full:\n                is_banned = True\n        if (not is_banned) and ((suffix is None) or (suffix in suffix_full)):\n            files.append(object_summary.key)\n\n    return files\n\n\ndef list_bucket_prefix_suffix_contains_s3(bucket, prefix, suffix=None, banned_suffixes=None, contains=None):\n    if banned_suffixes is None:\n        banned_suffixes = []\n    s3 = boto3.resource('s3')\n    my_bucket = s3.Bucket(bucket)\n    prefix = prefix\n\n    files = []\n    for object_summary in my_bucket.objects.filter(Prefix=prefix):\n        suffix_full = object_summary.key.split(prefix, 1)[1]\n        is_banned = False\n        for banned_suffix in banned_suffixes:\n            if banned_suffix in suffix_full:\n                is_banned = True\n        if (not is_banned) and ((suffix is None) or (suffix in suffix_full)) and contains in suffix_full:\n            files.append(object_summary.key)\n\n    return files\n\n\ndef load_multipart_s3(bucket, prefix, columns_to_keep=None, dtype=None, sample_count=None):\n    files = list_bucket_prefix_s3(bucket, prefix)\n    files_cleaned = [file for file in files if prefix + '/part-' in file]\n    paths_full = [s3_utils.s3_bucket_prefix_to_path(bucket=bucket, prefix=file, version='s3') for file in files_cleaned]\n    if sample_count is not None:\n        logger.log(15, 'Taking sample of '+str(sample_count)+' of '+str(len(paths_full))+' s3 files to load')\n        paths_full = paths_full[:sample_count]\n\n    df = load_pd.load(path=paths_full, columns_to_keep=columns_to_keep, dtype=dtype)\n    return df\n"""
autogluon/utils/tabular/utils/savers/__init__.py,0,b''
autogluon/utils/tabular/utils/savers/save_json.py,0,"b""# TODO: Standardize / unify this code with ag.save()\nimport json\nimport os\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\n# TODO: Support S3 paths\ndef save(path, obj, sanitize=True):\n    if sanitize:\n        obj = sanitize_object_to_primitives(obj=obj)\n    dirname = os.path.dirname(path)\n    if dirname:\n        os.makedirs(dirname, exist_ok=True)\n    with open(path, 'w') as fp:\n        json.dump(obj, fp, indent=2)\n\n\ndef sanitize_object_to_primitives(obj):\n    if isinstance(obj, dict):\n        obj_sanitized = dict()\n        for key, val in obj.items():\n            obj_sanitized[key] = sanitize_object_to_primitives(val)\n    else:\n        try:\n            json.dumps(obj)\n            obj_sanitized = obj\n        except (TypeError, OverflowError):\n            json.dumps(type(obj).__name__)\n            obj_sanitized = type(obj).__name__\n    return obj_sanitized\n"""
autogluon/utils/tabular/utils/savers/save_pd.py,0,"b'import multiprocessing, os, boto3, json, logging\nfrom io import StringIO\nimport numpy as np\n\nfrom .. import s3_utils, multiprocessing_utils\n\nlogger = logging.getLogger(__name__)\n\n\n# TODO: Update so verbose prints at level 20, and adjust calls to save accordingly\n# gzip compression produces random deflate issues on linux machines - use with caution\ndef save(path, df, index=False, verbose=True, type=None, sep=\',\', compression=\'gzip\', header=True, json_dump_columns=None):\n    if json_dump_columns is not None:\n        df = df.copy()\n        for column in json_dump_columns:\n            if column in df.columns.values:\n                df[column] = [json.dumps(x[0]) for x in zip(df[column])]\n    if type is None:\n        if path[-1] == \'/\' and s3_utils.is_s3_url(path):  # and path[:2] == \'s3\'\n            type = \'multipart_s3\'\n        elif path[-1] == \'/\' and not s3_utils.is_s3_url(path):  # and path[:2] != \'s3\'\n            type = \'multipart_local\'\n        elif \'.csv\' in path:\n            type = \'csv\'\n        elif \'.parquet\' in path:\n            type = \'parquet\'\n        else:\n            type = \'csv\'\n    if \'s3\' not in path[:2]:\n        is_local = True\n    else:\n        is_local = False\n    column_count = len(list(df.columns.values))\n    row_count = df.shape[0]\n    if is_local:\n        os.makedirs(os.path.dirname(path), exist_ok=True)\n    if type == \'csv\':\n        if is_local:\n            df.to_csv(path, index=index, sep=sep, header=header)\n        else:\n            buffer = StringIO()\n            df.to_csv(buffer, index=index, sep=sep, header=header)\n            bucket, prefix = s3_utils.s3_path_to_bucket_prefix(s3_path=path)\n            s3_resource = boto3.resource(\'s3\')\n            s3_resource.Object(bucket, prefix).put(Body=buffer.getvalue(), ACL=\'bucket-owner-full-control\')\n        if verbose:\n            logger.log(15, ""Saved "" +str(path)+"" | Columns = ""+str(column_count)+"" | Rows = ""+str(row_count))\n    elif type == \'parquet\':\n        try:\n            df.to_parquet(path, compression=compression, engine=\'fastparquet\')  # TODO: Might be slower than pyarrow in multiprocessing\n        except:\n            df.to_parquet(path, compression=compression, engine=\'pyarrow\')\n        if verbose:\n            logger.log(15, ""Saved ""+str(path)+"" | Columns = ""+str(column_count)+"" | Rows = ""+str(row_count))\n    elif type == \'multipart_s3\':\n        bucket, prefix = s3_utils.s3_path_to_bucket_prefix(s3_path=path)\n        s3_utils.delete_s3_prefix(bucket=bucket, prefix=prefix)  # TODO: Might only delete the first 1000!\n        save_multipart(path=path, df=df, index=index, verbose=verbose, type=\'parquet\', sep=sep, compression=compression, header=header, json_dump_columns=None)\n    elif type == \'multipart_local\':\n        if os.path.isdir(path):\n            for file in os.listdir(path):\n                file_path = os.path.join(path, file)\n                try:\n                    if os.path.isfile(file_path):\n                        os.unlink(file_path)\n                except Exception as e:\n                    logger.exception(e)\n        save_multipart(path=path, df=df, index=index, verbose=verbose, type=\'parquet\', sep=sep, compression=compression, header=header, json_dump_columns=None)\n    else:\n        raise Exception(\'Unknown save type: \' + type)\n\n\ndef save_multipart_child(chunk):\n    path, df, index, verbose, type, sep, compression, header, json_dump_columns = chunk\n    save(path=path, df=df, index=index, verbose=verbose, type=type, sep=sep, compression=compression, header=header, json_dump_columns=json_dump_columns)\n\n\ndef save_multipart(path, df, index=False, verbose=True, type=None, sep=\',\', compression=\'snappy\', header=True, json_dump_columns=None):\n    cpu_count = multiprocessing.cpu_count()\n    workers_count = int(round(cpu_count))\n    parts = workers_count\n\n    logger.log(15, \'Save_multipart running pool with \'+str(workers_count)+\' workers\')\n\n    paths = [path + \'part-\' + \'0\' * (5 - min(5, len(str(i)))) + str(i) + \'.parquet\' for i in range(parts)]\n    df_parts = np.array_split(df, parts)\n\n    full_chunks = [[\n        path, df_part, index, verbose, type, sep, compression, header, json_dump_columns,\n    ] for path, df_part in zip(paths, df_parts)]\n\n    multiprocessing_utils.execute_multiprocessing(workers_count=workers_count, transformer=save_multipart_child, chunks=full_chunks)\n\n    logger.log(15, ""Saved multipart file to ""+str(path))\n'"
autogluon/utils/tabular/utils/savers/save_pkl.py,0,"b""# TODO: Standardize / unify this code with ag.save()\nimport os, pickle, tempfile, logging, boto3\n\nfrom .. import s3_utils\n\nlogger = logging.getLogger(__name__)\n\n\ndef save(path, object, format=None, verbose=True):\n    pickle_fn = lambda o, buffer: pickle.dump(o, buffer, protocol=4)\n    save_with_fn(path, object, pickle_fn, format=format, verbose=verbose)\n\n\ndef save_with_fn(path, object, pickle_fn, format=None, verbose=True):\n    if verbose:\n        logger.log(15, 'Saving '+str(path))\n    if s3_utils.is_s3_url(path):\n        format = 's3'\n    if format == 's3':\n        save_s3(path, object, pickle_fn, verbose=verbose)\n    else:\n        os.makedirs(os.path.dirname(path), exist_ok=True)\n        with open(path, 'wb') as fout:\n            pickle_fn(object, fout)\n\n\ndef save_s3(path: str, obj, pickle_fn, verbose=True):\n    if verbose:\n        logger.info(f'save object to {path}')\n    with tempfile.TemporaryFile() as f:\n        pickle_fn(obj, f)\n        f.flush()\n        f.seek(0)\n\n        bucket, key = s3_utils.s3_path_to_bucket_prefix(path)\n        s3_client = boto3.client('s3')\n        try:\n            config = boto3.s3.transfer.TransferConfig()   # enable multipart uploading for files larger than 8MB\n            response = s3_client.upload_fileobj(f, bucket, key, Config=config)\n        except:\n            logger.exception('Failed to save object to s3')\n            raise\n"""
autogluon/utils/tabular/utils/savers/save_pointer.py,0,"b'import os, logging\n\nPOINTER_SUFFIX = \'.pointer\'\n\nlogger = logging.getLogger(__name__)\n\n# TODO: Add S3 support\ndef save(path, content_path, verbose=True):\n    os.makedirs(os.path.dirname(path), exist_ok=True)\n\n    f = open(path, ""w"")\n    f.write(content_path)\n    f.close()\n\n    if verbose:\n        logger.log(15, \'Saved pointer file to \'+str(path)+\' pointing to \'+str(content_path))\n'"
autogluon/utils/tabular/ml/models/abstract/__init__.py,0,b''
autogluon/utils/tabular/ml/models/abstract/abstract_model.py,0,"b'import copy\nimport logging\nimport math\nimport os\nimport pickle\nimport sys\nimport time\n\nimport pandas as pd\nimport psutil\n\nfrom .model_trial import model_trial\nfrom ...constants import AG_ARGS_FIT, BINARY, REGRESSION, REFIT_FULL_SUFFIX\nfrom ...tuning.feature_pruner import FeaturePruner\nfrom ...utils import get_pred_from_proba, generate_train_test_split, shuffle_df_rows, convert_categorical_to_int\nfrom .... import metrics\nfrom ....utils.loaders import load_pkl\nfrom ....utils.savers import save_pkl, save_json\nfrom ......core import Space, Categorical, List, NestedSpace\nfrom ......scheduler.fifo import FIFOScheduler\nfrom ......task.base import BasePredictor\n\nlogger = logging.getLogger(__name__)\n\n\n# Methods useful for all models:\ndef fixedvals_from_searchspaces(params):\n    """""" Converts any search space hyperparams in params dict into fixed default values. """"""\n    if any(isinstance(params[hyperparam], Space) for hyperparam in params):\n        logger.warning(""Attempting to fit model without HPO, but search space is provided. fit() will only consider default hyperparameter values from search space."")\n        bad_keys = [hyperparam for hyperparam in params if isinstance(params[hyperparam], Space)][:]  # delete all keys which are of type autogluon Space\n        params = params.copy()\n        for hyperparam in bad_keys:\n            params[hyperparam] = hp_default_value(params[hyperparam])\n        return params\n    else:\n        return params\n\n\ndef hp_default_value(hp_value):\n    """""" Extracts default fixed value from hyperparameter search space hp_value to use a fixed value instead of a search space.\n    """"""\n    if not isinstance(hp_value, Space):\n        return hp_value\n    if isinstance(hp_value, Categorical):\n        return hp_value[0]\n    elif isinstance(hp_value, List):\n        return [z[0] for z in hp_value]\n    elif isinstance(hp_value, NestedSpace):\n        raise ValueError(""Cannot extract default value from NestedSpace. Please specify fixed value instead of: %s"" % str(hp_value))\n    else:\n        return hp_value.get_hp(\'dummy_name\').default_value\n\n\nclass AbstractModel:\n    model_file_name = \'model.pkl\'\n    model_info_name = \'info.pkl\'\n    model_info_json_name = \'info.json\'\n\n    def __init__(self, path: str, name: str, problem_type: str, objective_func, num_classes=None, stopping_metric=None, model=None, hyperparameters=None, features=None, feature_types_metadata=None, debug=0, **kwargs):\n        """""" Creates a new model.\n            Args:\n                path (str): directory where to store all outputs\n                name (str): name of subdirectory inside path where model will be saved\n                hyperparameters (dict): various hyperparameters that will be used by model (can be search spaces instead of fixed values)\n        """"""\n        self.name = name\n        self.path_root = path\n        self.path_suffix = self.name + os.path.sep  # TODO: Make into function to avoid having to reassign on load?\n        self.path = self.create_contexts(self.path_root + self.path_suffix)  # TODO: Make this path a function for consistency.\n        self.num_classes = num_classes\n        self.model = model\n        self.problem_type = problem_type\n        self.objective_func = objective_func  # Note: we require higher values = better performance\n\n        if stopping_metric is None:\n            self.stopping_metric = self.objective_func\n        else:\n            self.stopping_metric = stopping_metric\n\n        if isinstance(self.objective_func, metrics._ProbaScorer):\n            self.metric_needs_y_pred = False\n        elif isinstance(self.objective_func, metrics._ThresholdScorer):\n            self.metric_needs_y_pred = False\n        else:\n            self.metric_needs_y_pred = True\n\n        if isinstance(self.stopping_metric, metrics._ProbaScorer):\n            self.stopping_metric_needs_y_pred = False\n        elif isinstance(self.stopping_metric, metrics._ThresholdScorer):\n            self.stopping_metric_needs_y_pred = False\n        else:\n            self.stopping_metric_needs_y_pred = True\n\n        self.feature_types_metadata = feature_types_metadata  # TODO: Should this be passed to a model on creation? Should it live in a Dataset object and passed during fit? Currently it is being updated prior to fit by trainer\n        self.features = features\n        self.debug = debug\n\n        self.fit_time = None  # Time taken to fit in seconds (Training data)\n        self.predict_time = None  # Time taken to predict in seconds (Validation data)\n        self.val_score = None  # Score with eval_metric (Validation data)\n\n        self.params = {}\n        self.params_aux = {}\n\n        self._set_default_auxiliary_params()\n        if hyperparameters is not None:\n            hyperparameters = hyperparameters.copy()\n            if AG_ARGS_FIT in hyperparameters:\n                ag_args_fit = hyperparameters.pop(AG_ARGS_FIT)\n                self.params_aux.update(ag_args_fit)\n        self._set_default_params()\n        self.nondefault_params = []\n        if hyperparameters is not None:\n            self.params.update(hyperparameters)\n            self.nondefault_params = list(hyperparameters.keys())[:]  # These are hyperparameters that user has specified.\n        self.params_trained = dict()\n\n    # Checks if model is capable of inference on new data (if normal model) or has produced out-of-fold predictions (if bagged model)\n    def is_valid(self):\n        return self.is_fit()\n\n    # Checks if model is capable of inference on new data\n    def can_infer(self):\n        return self.is_valid()\n\n    # Checks if a model has been fit\n    def is_fit(self):\n        return self.model is not None\n\n    def _set_default_params(self):\n        pass\n\n    def _set_default_auxiliary_params(self):\n        # TODO: Consider adding to get_info() output\n        default_auxiliary_params = dict(\n            max_memory_usage_ratio=1.0,  # Ratio of memory usage allowed by the model. Values > 1.0 have an increased risk of causing OOM errors.\n            # TODO: Add more params\n            # max_memory_usage=None,\n            # max_disk_usage=None,\n            # max_time_limit_ratio=1.0,\n            # max_time_limit=None,\n            # num_cpu=None,\n            # num_gpu=None,\n            # ignore_hpo=False,\n            # max_early_stopping_rounds=None,\n        )\n        for key, value in default_auxiliary_params.items():\n            self._set_default_param_value(key, value, params=self.params_aux)\n\n    def _set_default_param_value(self, param_name, param_value, params=None):\n        if params is None:\n            params = self.params\n        if param_name not in params:\n            params[param_name] = param_value\n\n    def _get_default_searchspace(self) -> dict:\n        return NotImplementedError\n\n    def _set_default_searchspace(self):\n        """""" Sets up default search space for HPO. Each hyperparameter which user did not specify is converted from\n            default fixed value to default search space.\n        """"""\n        def_search_space = self._get_default_searchspace().copy()\n        # Note: when subclassing AbstractModel, you must define or import get_default_searchspace() from the appropriate location.\n        for key in self.nondefault_params:  # delete all user-specified hyperparams from the default search space\n            def_search_space.pop(key, None)\n        if self.params is not None:\n            self.params.update(def_search_space)\n\n    def set_contexts(self, path_context):\n        self.path = self.create_contexts(path_context)\n        self.path_suffix = self.name + os.path.sep\n        # TODO: This should be added in future once naming conventions have been standardized for WeightedEnsembleModel\n        # if self.path_suffix not in self.path:\n        #     raise ValueError(\'Expected path_suffix not in given path! Values: (%s, %s)\' % (self.path_suffix, self.path))\n        self.path_root = self.path.rsplit(self.path_suffix, 1)[0]\n\n    @staticmethod\n    def create_contexts(path_context):\n        path = path_context\n        return path\n\n    def rename(self, name):\n        self.path = self.path[:-len(self.name) - 1] + name + os.path.sep\n        self.name = name\n\n    # Extensions of preprocess must act identical in bagged situations, otherwise test-time predictions will be incorrect\n    # This means preprocess cannot be used for normalization\n    # TODO: Add preprocess_stateful() to enable stateful preprocessing for models such as KNN\n    def preprocess(self, X):\n        if self.features is not None:\n            # TODO: In online-inference this becomes expensive, add option to remove it (only safe in controlled environment where it is already known features are present\n            if list(X.columns) != self.features:\n                return X[self.features]\n        else:\n            self.features = list(X.columns)  # TODO: add fit and transform versions of preprocess instead of doing this\n        return X\n\n    def fit(self, X_train, Y_train, **kwargs):\n        # kwargs may contain: num_cpus, num_gpus\n        X_train = self.preprocess(X_train)\n        self.model = self.model.fit(X_train, Y_train)\n\n    def predict(self, X, preprocess=True):\n        y_pred_proba = self.predict_proba(X, preprocess=preprocess)\n        y_pred = get_pred_from_proba(y_pred_proba=y_pred_proba, problem_type=self.problem_type)\n        return y_pred\n\n    def predict_proba(self, X, preprocess=True):\n        if preprocess:\n            X = self.preprocess(X)\n\n        if self.problem_type == REGRESSION:\n            return self.model.predict(X)\n\n        y_pred_proba = self.model.predict_proba(X)\n\n        if self.problem_type == BINARY:\n            if len(y_pred_proba.shape) == 1:\n                return y_pred_proba\n            elif y_pred_proba.shape[1] > 1:\n                return y_pred_proba[:, 1]\n            else:\n                return y_pred_proba\n        elif y_pred_proba.shape[1] > 2:\n            return y_pred_proba\n        else:\n            return y_pred_proba[:, 1]\n\n    def score(self, X, y, eval_metric=None, metric_needs_y_pred=None, preprocess=True):\n        if eval_metric is None:\n            eval_metric = self.objective_func\n        if metric_needs_y_pred is None:\n            metric_needs_y_pred = self.metric_needs_y_pred\n        if metric_needs_y_pred:\n            y_pred = self.predict(X=X, preprocess=preprocess)\n            return eval_metric(y, y_pred)\n        else:\n            y_pred_proba = self.predict_proba(X=X, preprocess=preprocess)\n            return eval_metric(y, y_pred_proba)\n\n    def score_with_y_pred_proba(self, y, y_pred_proba, eval_metric=None, metric_needs_y_pred=None):\n        if eval_metric is None:\n            eval_metric = self.objective_func\n        if metric_needs_y_pred is None:\n            metric_needs_y_pred = self.metric_needs_y_pred\n        if metric_needs_y_pred:\n            y_pred = get_pred_from_proba(y_pred_proba=y_pred_proba, problem_type=self.problem_type)\n            return eval_metric(y, y_pred)\n        else:\n            return eval_metric(y, y_pred_proba)\n\n    def save(self, file_prefix="""", directory=None, return_filename=False, verbose=True):\n        if directory is None:\n            directory = self.path\n        file_name = directory + file_prefix + self.model_file_name\n        save_pkl.save(path=file_name, object=self, verbose=verbose)\n        if return_filename:\n            return file_name\n\n    @classmethod\n    def load(cls, path, file_prefix="""", reset_paths=False, verbose=True):\n        load_path = path + file_prefix + cls.model_file_name\n        if not reset_paths:\n            return load_pkl.load(path=load_path, verbose=verbose)\n        else:\n            obj = load_pkl.load(path=load_path, verbose=verbose)\n            obj.set_contexts(path)\n            return obj\n\n    # TODO: Consider disabling feature pruning when num_features is high (>1000 for example), or using a faster feature importance calculation method\n    def compute_feature_importance(self, X, y, features_to_use=None, preprocess=True, subsample_size=10000, silent=False, **kwargs):\n        if (subsample_size is not None) and (len(X) > subsample_size):\n            X = X.sample(subsample_size, random_state=0)\n            y = y.loc[X.index]\n        else:\n            X = X.copy()\n            y = y.copy()\n\n        if preprocess:\n            X = self.preprocess(X)\n\n        if not features_to_use:\n            features = list(X.columns.values)\n        else:\n            features = list(features_to_use)\n\n        feature_importance_quick_dict = self.get_model_feature_importance()\n        # TODO: Also consider banning features with close to 0 importance\n        # TODO: Consider adding \'golden\' features if the importance is high enough to avoid unnecessary computation when doing feature selection\n        banned_features = [feature for feature, importance in feature_importance_quick_dict.items() if importance == 0 and feature in features]\n        features = [feature for feature in features if feature not in banned_features]\n\n        permutation_importance_dict = self.compute_permutation_importance(X=X, y=y, features=features, preprocess=False, silent=silent)\n\n        feature_importances = pd.Series(permutation_importance_dict)\n        results_banned = pd.Series(data=[0 for _ in range(len(banned_features))], index=banned_features)\n        feature_importances = pd.concat([feature_importances, results_banned])\n        feature_importances = feature_importances.sort_values(ascending=False)\n\n        return feature_importances\n\n    # TODO: Consider repeating with different random seeds and averaging to increase confidence\n    # TODO: Optimize this\n    # Compute feature importance via permutation importance\n    # Note: Expensive to compute\n    #  Time to compute is O(predict_time*num_features)\n    def compute_permutation_importance(self, X, y, features: list, preprocess=True, silent=False) -> dict:\n        time_start = time.time()\n\n        feature_count = len(features)\n        if not silent:\n            logger.log(20, f\'Computing permutation importance for {feature_count} features on {self.name} ...\')\n        if preprocess:\n            X = self.preprocess(X)\n\n        time_start_score = time.time()\n        model_score_base = self.score(X=X, y=y, preprocess=False)\n        time_score = time.time() - time_start_score\n\n        if not silent:\n            time_estimated = (feature_count + 1) * time_score + time_start_score - time_start\n            logger.log(20, f\'\\t{round(time_estimated, 2)}s\\t= Expected runtime\')\n\n        X_test_shuffled = shuffle_df_rows(X=X, seed=0)\n        row_count = X.shape[0]\n\n        # calculating maximum number of features, which is safe to process parallel\n        X_memory_ratio_max = 0.2\n        compute_count_max = 200\n\n        X_size_bytes = sys.getsizeof(pickle.dumps(X, protocol=4))\n        available_mem = psutil.virtual_memory().available\n        X_memory_ratio = X_size_bytes / available_mem\n\n        compute_count_safe = math.floor(X_memory_ratio_max / X_memory_ratio)\n        compute_count = max(1, min(compute_count_max, compute_count_safe))\n        compute_count = min(compute_count, feature_count)\n\n        # creating copy of original data N=compute_count times for parallel processing\n        X_test_raw = pd.concat([X.copy() for _ in range(compute_count)], ignore_index=True, sort=False).reset_index(drop=True)\n\n        #  TODO: Make this faster by multi-threading?\n        permutation_importance_dict = {}\n        for i in range(0, feature_count, compute_count):\n            parallel_computed_features = features[i:i + compute_count]\n\n            # if final iteration, leaving only necessary part of X_test_raw\n            num_features_processing = len(parallel_computed_features)\n            final_iteration = i + num_features_processing == feature_count\n            if (num_features_processing < compute_count) and final_iteration:\n                X_test_raw = X_test_raw.loc[:row_count * num_features_processing - 1]\n\n            row_index = 0\n            for feature in parallel_computed_features:\n                row_index_end = row_index + row_count\n                X_test_raw.loc[row_index:row_index_end - 1, feature] = X_test_shuffled[feature].values\n                row_index = row_index_end\n\n            if self.metric_needs_y_pred:\n                Y_pred = self.predict(X_test_raw, preprocess=False)\n            else:\n                Y_pred = self.predict_proba(X_test_raw, preprocess=False)\n\n            row_index = 0\n            for feature in parallel_computed_features:\n                # calculating importance score for given feature\n                row_index_end = row_index + row_count\n                Y_pred_cur = Y_pred[row_index:row_index_end]\n                score = self.objective_func(y, Y_pred_cur)\n                permutation_importance_dict[feature] = model_score_base - score\n\n                if not final_iteration:\n                    # resetting to original values for processed feature\n                    X_test_raw.loc[row_index:row_index_end - 1, feature] = X[feature].values\n\n                row_index = row_index_end\n\n        if not silent:\n            logger.log(20, f\'\\t{round(time.time() - time_start, 2)}s\\t= Actual runtime\')\n\n        return permutation_importance_dict\n\n    # Custom feature importance values for a model (such as those calculated from training)\n    def get_model_feature_importance(self) -> dict:\n        return dict()\n\n    # Hyperparameters of trained model\n    def get_trained_params(self):\n        trained_params = self.params.copy()\n        trained_params.update(self.params_trained)\n        return trained_params\n\n    # After calling this function, returned model should be able to be fit as if it was new, as well as deep-copied.\n    def convert_to_template(self):\n        model = self.model\n        self.model = None\n        template = copy.deepcopy(self)\n        template.reset_metrics()\n        self.model = model\n        return template\n\n    # After calling this function, model should be able to be fit without test data using the iterations trained by the original model\n    def convert_to_refitfull_template(self):\n        params_trained = self.params_trained.copy()\n        template = self.convert_to_template()\n        template.params.update(params_trained)\n        template.name = template.name + REFIT_FULL_SUFFIX\n        template.set_contexts(self.path_root + template.name + os.path.sep)\n        return template\n\n    def hyperparameter_tune(self, X_train, X_test, Y_train, Y_test, scheduler_options, **kwargs):\n        # verbosity = kwargs.get(\'verbosity\', 2)\n        time_start = time.time()\n        logger.log(15, ""Starting generic AbstractModel hyperparameter tuning for %s model..."" % self.name)\n        self._set_default_searchspace()\n        params_copy = self.params.copy()\n        directory = self.path  # also create model directory if it doesn\'t exist\n        # TODO: This will break on S3. Use tabular/utils/savers for datasets, add new function\n        scheduler_func, scheduler_options = scheduler_options  # Unpack tuple\n        if scheduler_func is None or scheduler_options is None:\n            raise ValueError(""scheduler_func and scheduler_options cannot be None for hyperparameter tuning"")\n        params_copy[\'num_threads\'] = scheduler_options[\'resource\'].get(\'num_cpus\', None)\n        params_copy[\'num_gpus\'] = scheduler_options[\'resource\'].get(\'num_gpus\', None)\n        dataset_train_filename = \'dataset_train.p\'\n        train_path = directory + dataset_train_filename\n        save_pkl.save(path=train_path, object=(X_train, Y_train))\n\n        dataset_val_filename = \'dataset_val.p\'\n        val_path = directory + dataset_val_filename\n        save_pkl.save(path=val_path, object=(X_test, Y_test))\n\n        if not any(isinstance(params_copy[hyperparam], Space) for hyperparam in params_copy):\n            logger.warning(""Attempting to do hyperparameter optimization without any search space (all hyperparameters are already fixed values)"")\n        else:\n            logger.log(15, ""Hyperparameter search space for %s model: "" % self.name)\n            for hyperparam in params_copy:\n                if isinstance(params_copy[hyperparam], Space):\n                    logger.log(15, f""{hyperparam}:   {params_copy[hyperparam]}"")\n\n        util_args = dict(\n            dataset_train_filename=dataset_train_filename,\n            dataset_val_filename=dataset_val_filename,\n            directory=directory,\n            model=self,\n            time_start=time_start,\n            time_limit=scheduler_options[\'time_out\'],\n        )\n\n        model_trial.register_args(util_args=util_args, **params_copy)\n        scheduler: FIFOScheduler = scheduler_func(model_trial, **scheduler_options)\n        if (\'dist_ip_addrs\' in scheduler_options) and (len(scheduler_options[\'dist_ip_addrs\']) > 0):\n            # This is multi-machine setting, so need to copy dataset to workers:\n            logger.log(15, ""Uploading data to remote workers..."")\n            scheduler.upload_files([train_path, val_path])  # TODO: currently does not work.\n            directory = self.path  # TODO: need to change to path to working directory used on every remote machine\n            model_trial.update(directory=directory)\n            logger.log(15, ""uploaded"")\n\n        scheduler.run()\n        scheduler.join_jobs()\n\n        return self._get_hpo_results(scheduler=scheduler, scheduler_options=scheduler_options, time_start=time_start)\n\n    def _get_hpo_results(self, scheduler, scheduler_options, time_start):\n        # Store results / models from this HPO run:\n        best_hp = scheduler.get_best_config()  # best_hp only contains searchable stuff\n        hpo_results = {\n            \'best_reward\': scheduler.get_best_reward(),\n            \'best_config\': best_hp,\n            \'total_time\': time.time() - time_start,\n            \'metadata\': scheduler.metadata,\n            \'training_history\': scheduler.training_history,\n            \'config_history\': scheduler.config_history,\n            \'reward_attr\': scheduler._reward_attr,\n            \'args\': model_trial.args\n        }\n\n        hpo_results = BasePredictor._format_results(hpo_results)  # results summarizing HPO for this model\n        if (\'dist_ip_addrs\' in scheduler_options) and (len(scheduler_options[\'dist_ip_addrs\']) > 0):\n            raise NotImplementedError(""need to fetch model files from remote Workers"")\n            # TODO: need to handle locations carefully: fetch these files and put them into self.path directory:\n            # 1) hpo_results[\'trial_info\'][trial][\'metadata\'][\'trial_model_file\']\n\n        hpo_models = {}  # stores all the model names and file paths to model objects created during this HPO run.\n        hpo_model_performances = {}\n        for trial in sorted(hpo_results[\'trial_info\'].keys()):\n            # TODO: ignore models which were killed early by scheduler (eg. in Hyperband). How to ID these?\n            file_id = ""trial_"" + str(trial)  # unique identifier to files from this trial\n            trial_model_name = self.name + os.path.sep + file_id\n            trial_model_path = self.path_root + trial_model_name + os.path.sep\n            hpo_models[trial_model_name] = trial_model_path\n            hpo_model_performances[trial_model_name] = hpo_results[\'trial_info\'][trial][scheduler._reward_attr]\n\n        logger.log(15, ""Time for %s model HPO: %s"" % (self.name, str(hpo_results[\'total_time\'])))\n        logger.log(15, ""Best hyperparameter configuration for %s model: "" % self.name)\n        logger.log(15, str(best_hp))\n        return hpo_models, hpo_model_performances, hpo_results\n\n    def feature_prune(self, X_train, X_holdout, Y_train, Y_holdout):\n        feature_pruner = FeaturePruner(model_base=self)\n        X_train, X_test, y_train, y_test = generate_train_test_split(X_train, Y_train, problem_type=self.problem_type, test_size=0.2)\n        feature_pruner.tune(X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test, X_holdout=X_holdout, y_holdout=Y_holdout)\n        features_to_keep = feature_pruner.features_in_iter[feature_pruner.best_iteration]\n        logger.debug(str(features_to_keep))\n        self.features = features_to_keep\n\n    # Resets metrics for the model\n    def reset_metrics(self):\n        self.fit_time = None\n        self.predict_time = None\n        self.val_score = None\n        self.params_trained = dict()\n\n    # TODO: Experimental, currently unused\n    #  Has not been tested on Windows\n    #  Does not work if model is located in S3\n    #  Does not work if called before model was saved to disk (Will output 0)\n    def get_disk_size(self):\n        # Taken from https://stackoverflow.com/a/1392549\n        from pathlib import Path\n        model_path = Path(self.path)\n        model_disk_size = sum(f.stat().st_size for f in model_path.glob(\'**/*\') if f.is_file())\n        return model_disk_size\n\n    # TODO: This results in a doubling of memory usage of the model to calculate its size.\n    #  If the model takes ~40%+ of memory, this may result in an OOM error.\n    #  This is generally not an issue because the model already needed to do this when being saved to disk, so the error would have been triggered earlier.\n    #  Consider using Pympler package for memory efficiency: https://pympler.readthedocs.io/en/latest/asizeof.html#asizeof\n    def get_memory_size(self):\n        return sys.getsizeof(pickle.dumps(self, protocol=4))\n\n    # Removes non-essential objects from the model to reduce memory and disk footprint.\n    # If `remove_fit=True`, enables the removal of variables which are required for fitting the model. If the model is already fully trained, then it is safe to remove these.\n    # If `remove_info=True`, enables the removal of variables which are used during model.get_info(). The values will be None when calling model.get_info().\n    # If `requires_save=True`, enables the removal of variables which are part of the model.pkl object, requiring an overwrite of the model to disk if it was previously persisted.\n    def reduce_memory_size(self, remove_fit=True, remove_info=False, requires_save=True, **kwargs):\n        pass\n\n    # Deletes the model from disk.\n    # WARNING: This will DELETE ALL FILES in the self.path directory, regardless if they were created by AutoGluon or not.\n    #  DO NOT STORE FILES INSIDE OF THE MODEL DIRECTORY THAT ARE UNRELATED TO AUTOGLUON.\n    def delete_from_disk(self):\n        logger.log(30, f\'Deleting model {self.name}. All files under {self.path} will be removed.\')\n        from pathlib import Path\n        import shutil\n        model_path = Path(self.path)\n        # TODO: Report errors?\n        shutil.rmtree(path=model_path, ignore_errors=True)\n\n    def get_info(self):\n        info = dict(\n            name=self.name,\n            model_type=type(self).__name__,\n            problem_type=self.problem_type,\n            eval_metric=self.objective_func.name,\n            stopping_metric=self.stopping_metric.name,\n            fit_time=self.fit_time,\n            predict_time=self.predict_time,\n            val_score=self.val_score,\n            hyperparameters=self.params,\n            hyperparameters_fit=self.params_trained,  # TODO: Explain in docs that this is for hyperparameters that differ in final model from original hyperparameters, such as epochs (from early stopping)\n            hyperparameters_nondefault=self.nondefault_params,\n            # disk_size=self.get_disk_size(),\n            memory_size=self.get_memory_size(),  # Memory usage of model in bytes\n        )\n        return info\n\n    @classmethod\n    def load_info(cls, path, load_model_if_required=True):\n        load_path = path + cls.model_info_name\n        try:\n            return load_pkl.load(path=load_path)\n        except:\n            if load_model_if_required:\n                model = cls.load(path=path, reset_paths=True)\n                return model.get_info()\n            else:\n                raise\n\n    def save_info(self):\n        info = self.get_info()\n\n        save_pkl.save(path=self.path + self.model_info_name, object=info)\n        json_path = self.path + self.model_info_json_name\n        save_json.save(path=json_path, obj=info)\n        return info\n\n\nclass SKLearnModel(AbstractModel):\n    """"""Abstract model for all Sklearn models.""""""\n\n    def preprocess(self, X):\n        X = convert_categorical_to_int(X)\n        return super().preprocess(X)\n'"
autogluon/utils/tabular/ml/models/abstract/model_trial.py,0,"b'import os\nimport time\nimport logging\n\nfrom ....utils.loaders import load_pkl\nfrom ....utils.exceptions import TimeLimitExceeded\nfrom ......core import args\nfrom ......scheduler.reporter import LocalStatusReporter\n\nlogger = logging.getLogger(__name__)\n\n\n@args()\ndef model_trial(args, reporter: LocalStatusReporter):\n    """""" Training script for hyperparameter evaluation of an arbitrary model that subclasses AbstractModel.\n        \n        Notes:\n            - Model object itself must be passed as kwarg: model\n            - All model hyperparameters must be stored in model.params dict that may contain special keys such as:\n                \'seed_value\' to ensure reproducibility\n                \'num_threads\', \'num_gpus\' to set specific resources in model.fit()\n            - model.save() must have return_filename, file_prefix, directory options\n    """"""\n    try:\n        model, args, util_args = prepare_inputs(args=args)\n\n        X_train, y_train = load_pkl.load(util_args.directory + util_args.dataset_train_filename)\n        X_val, y_val = load_pkl.load(util_args.directory + util_args.dataset_val_filename)\n\n        fit_model_args = dict(X_train=X_train, Y_train=y_train, X_test=X_val, Y_test=y_val)\n        predict_proba_args = dict(X=X_val)\n        model = fit_and_save_model(model=model, params=args, fit_args=fit_model_args, predict_proba_args=predict_proba_args, y_test=y_val,\n                                   time_start=util_args.time_start, time_limit=util_args.get(\'time_limit\', None), reporter=None)\n    except Exception as e:\n        if not isinstance(e, TimeLimitExceeded):\n            logger.exception(e, exc_info=True)\n        reporter.terminate()\n    else:\n        reporter(epoch=1, validation_performance=model.val_score)\n\n\ndef prepare_inputs(args):\n    task_id = args.pop(\'task_id\')\n    util_args = args.pop(\'util_args\')\n\n    file_prefix = f""trial_{task_id}""  # append to all file names created during this trial. Do NOT change!\n    model = util_args.model  # the model object must be passed into model_trial() here\n    model.name = model.name + os.path.sep + file_prefix\n    model.set_contexts(path_context=model.path_root + model.name + os.path.sep)\n    return model, args, util_args\n\n\ndef fit_and_save_model(model, params, fit_args, predict_proba_args, y_test, time_start, time_limit=None, reporter=None):\n    time_current = time.time()\n    time_elapsed = time_current - time_start\n    if time_limit is not None:\n        time_left = time_limit - time_elapsed\n        if time_left <= 0:\n            raise TimeLimitExceeded\n    else:\n        time_left = None\n\n    model.params.update(params)\n    time_fit_start = time.time()\n    model.fit(**fit_args, time_limit=time_left, reporter=reporter)\n    time_fit_end = time.time()\n    y_pred_proba = model.predict_proba(**predict_proba_args)\n    time_pred_end = time.time()\n    model.val_score = model.score_with_y_pred_proba(y=y_test, y_pred_proba=y_pred_proba)\n    model.fit_time = time_fit_end - time_fit_start\n    model.predict_time = time_pred_end - time_fit_end\n    model.save()\n    return model\n'"
autogluon/utils/tabular/ml/models/catboost/__init__.py,0,b''
autogluon/utils/tabular/ml/models/catboost/catboost_model.py,0,"b'import logging\nimport math\nimport os\nimport pickle\nimport sys\nimport time\n\nimport psutil\n\nfrom .catboost_utils import construct_custom_catboost_metric\nfrom .hyperparameters.parameters import get_param_baseline\nfrom .hyperparameters.searchspaces import get_default_searchspace\nfrom ..abstract.abstract_model import AbstractModel\nfrom ...constants import PROBLEM_TYPES_CLASSIFICATION, MULTICLASS\nfrom ....utils.exceptions import NotEnoughMemoryError, TimeLimitExceeded\nfrom .....try_import import try_import_catboost\n\nlogger = logging.getLogger(__name__)\n\n\n# TODO: Catboost crashes on multiclass problems where only two classes have significant member count.\n#  Question: Do we turn these into binary classification and then convert to multiclass output in Learner? This would make the most sense.\n# TODO: Consider having Catboost variant that converts all categoricals to numerical as done in RFModel, was showing improved results in some problems.\nclass CatboostModel(AbstractModel):\n    def __init__(self, path: str, name: str, problem_type: str, objective_func, stopping_metric=None, num_classes=None, hyperparameters=None, features=None, debug=0, **kwargs):\n        super().__init__(path=path, name=name, problem_type=problem_type, objective_func=objective_func, stopping_metric=stopping_metric, num_classes=num_classes, hyperparameters=hyperparameters, features=features, debug=debug, **kwargs)\n        try_import_catboost()\n        from catboost import CatBoostClassifier, CatBoostRegressor\n        self.model_type = CatBoostClassifier if problem_type in PROBLEM_TYPES_CLASSIFICATION else CatBoostRegressor\n        if isinstance(self.params[\'eval_metric\'], str):\n            self.metric_name = self.params[\'eval_metric\']\n        else:\n            self.metric_name = type(self.params[\'eval_metric\']).__name__\n\n    def _set_default_params(self):\n        default_params = get_param_baseline(problem_type=self.problem_type)\n        for param, val in default_params.items():\n            self._set_default_param_value(param, val)\n        self._set_default_param_value(\'random_seed\', 0)  # Remove randomness for reproducibility\n        self._set_default_param_value(\'eval_metric\', construct_custom_catboost_metric(self.stopping_metric, True, not self.stopping_metric_needs_y_pred, self.problem_type))\n        # Set \'allow_writing_files\' to True in order to keep log files created by catboost during training (these will be saved in the directory where AutoGluon stores this model)\n        self._set_default_param_value(\'allow_writing_files\', False)  # Disables creation of catboost logging files during training by default\n\n    def _get_default_searchspace(self):\n        return get_default_searchspace(self.problem_type, num_classes=self.num_classes)\n\n    def preprocess(self, X):\n        X = super().preprocess(X)\n        categoricals = list(X.select_dtypes(include=\'category\').columns)\n        if categoricals:\n            X = X.copy()\n            for category in categoricals:\n                current_categories = X[category].cat.categories\n                if \'__NaN__\' in current_categories:\n                    X[category] = X[category].fillna(\'__NaN__\')\n                else:\n                    X[category] = X[category].cat.add_categories(\'__NaN__\').fillna(\'__NaN__\')\n        return X\n\n    # TODO: Use Pool in preprocess, optimize bagging to do Pool.split() to avoid re-computing pool for each fold! Requires stateful + y\n    #  Pool is much more memory efficient, avoids copying data twice in memory\n    def fit(self, X_train, Y_train, X_test=None, Y_test=None, time_limit=None, **kwargs):\n        from catboost import Pool\n        num_rows_train = len(X_train)\n        num_cols_train = len(X_train.columns)\n        if self.problem_type == MULTICLASS:\n            if self.num_classes is not None:\n                num_classes = self.num_classes\n            else:\n                num_classes = 10  # Guess if not given, can do better by looking at y_train\n        else:\n            num_classes = 1\n\n        # TODO: Add ignore_memory_limits param to disable NotEnoughMemoryError Exceptions\n        max_memory_usage_ratio = self.params_aux[\'max_memory_usage_ratio\']\n        approx_mem_size_req = num_rows_train * num_cols_train * num_classes / 2  # TODO: Extremely crude approximation, can be vastly improved\n        if approx_mem_size_req > 1e9:  # > 1 GB\n            available_mem = psutil.virtual_memory().available\n            ratio = approx_mem_size_req / available_mem\n            if ratio > (1 * max_memory_usage_ratio):\n                logger.warning(\'\\tWarning: Not enough memory to safely train CatBoost model, roughly requires: %s GB, but only %s GB is available...\' % (round(approx_mem_size_req / 1e9, 3), round(available_mem / 1e9, 3)))\n                raise NotEnoughMemoryError\n            elif ratio > (0.2 * max_memory_usage_ratio):\n                logger.warning(\'\\tWarning: Potentially not enough memory to safely train CatBoost model, roughly requires: %s GB, but only %s GB is available...\' % (round(approx_mem_size_req / 1e9, 3), round(available_mem / 1e9, 3)))\n\n        start_time = time.time()\n        X_train = self.preprocess(X_train)\n        cat_features = list(X_train.select_dtypes(include=\'category\').columns)\n        X_train = Pool(data=X_train, label=Y_train, cat_features=cat_features)\n\n        if X_test is not None:\n            X_test = self.preprocess(X_test)\n            X_test = Pool(data=X_test, label=Y_test, cat_features=cat_features)\n            eval_set = X_test\n            if num_rows_train <= 10000:\n                modifier = 1\n            else:\n                modifier = 10000/num_rows_train\n            early_stopping_rounds = max(round(modifier*150), 10)\n            num_sample_iter_max = max(round(modifier*50), 2)\n        else:\n            eval_set = None\n            early_stopping_rounds = None\n            num_sample_iter_max = 50\n\n        invalid_params = [\'num_threads\', \'num_gpus\']\n        for invalid in invalid_params:\n            if invalid in self.params:\n                self.params.pop(invalid)\n        train_dir = None\n        if \'allow_writing_files\' in self.params and self.params[\'allow_writing_files\']:\n            if \'train_dir\' not in self.params:\n                try:\n                    # TODO: What if path is in S3?\n                    os.makedirs(os.path.dirname(self.path), exist_ok=True)\n                except:\n                    pass\n                else:\n                    train_dir = self.path + \'catboost_info\'\n        logger.log(15, f\'\\tCatboost model hyperparameters: {self.params}\')\n\n        # TODO: Add more control over these params (specifically early_stopping_rounds)\n        verbosity = kwargs.get(\'verbosity\', 2)\n        if verbosity <= 1:\n            verbose = False\n        elif verbosity == 2:\n            verbose = False\n        elif verbosity == 3:\n            verbose = 20\n        else:\n            verbose = True\n\n        init_model = None\n        init_model_tree_count = None\n        init_model_best_iteration = None\n        init_model_best_score = None\n\n        params = self.params.copy()\n        num_features = len(self.features)\n        if self.problem_type == MULTICLASS and \'rsm\' not in params and \'colsample_bylevel\' not in params and num_features > 1000:\n            if time_limit:\n                # Reduce sample iterations to avoid taking unreasonable amounts of time\n                num_sample_iter_max = max(round(num_sample_iter_max/2), 2)\n            # Subsample columns to speed up training\n            params[\'colsample_bylevel\'] = max(min(1.0, 1000 / num_features), 0.05)\n            logger.log(30, f\'\\tMany features detected ({num_features}), dynamically setting \\\'colsample_bylevel\\\' to {params[""colsample_bylevel""]} to speed up training (Default = 1).\')\n            logger.log(30, f\'\\tTo disable this functionality, explicitly specify \\\'colsample_bylevel\\\' in the model hyperparameters.\')\n\n        if time_limit:\n            time_left_start = time_limit - (time.time() - start_time)\n            if time_left_start <= time_limit * 0.4:  # if 60% of time was spent preprocessing, likely not enough time to train model\n                raise TimeLimitExceeded\n            params_init = params.copy()\n            num_sample_iter = min(num_sample_iter_max, params_init[\'iterations\'])\n            params_init[\'iterations\'] = num_sample_iter\n            if train_dir is not None:\n                params_init[\'train_dir\'] = train_dir\n            self.model = self.model_type(\n                **params_init,\n            )\n            self.model.fit(\n                X_train,\n                eval_set=eval_set,\n                use_best_model=True,\n                verbose=verbose,\n                # early_stopping_rounds=early_stopping_rounds,\n            )\n\n            init_model_tree_count = self.model.tree_count_\n            init_model_best_iteration = self.model.get_best_iteration()\n            init_model_best_score = self.model.get_best_score()[\'validation\'][self.metric_name]\n\n            time_left_end = time_limit - (time.time() - start_time)\n            time_taken_per_iter = (time_left_start - time_left_end) / num_sample_iter\n            estimated_iters_in_time = round(time_left_end / time_taken_per_iter)\n            init_model = self.model\n\n            params_final = params.copy()\n\n            # TODO: This only handles memory with time_limits specified, but not with time_limits=None, handle when time_limits=None\n            available_mem = psutil.virtual_memory().available\n            model_size_bytes = sys.getsizeof(pickle.dumps(self.model))\n\n            max_memory_proportion = 0.3 * max_memory_usage_ratio\n            mem_usage_per_iter = model_size_bytes / num_sample_iter\n            max_memory_iters = math.floor(available_mem * max_memory_proportion / mem_usage_per_iter)\n\n            params_final[\'iterations\'] = min(params[\'iterations\'] - num_sample_iter, estimated_iters_in_time)\n            if params_final[\'iterations\'] > max_memory_iters - num_sample_iter:\n                if max_memory_iters - num_sample_iter <= 500:\n                    logger.warning(\'\\tWarning: CatBoost will be early stopped due to lack of memory, increase memory to enable full quality models, max training iterations changed to %s from %s\' % (max_memory_iters - num_sample_iter, params_final[\'iterations\']))\n                params_final[\'iterations\'] = max_memory_iters - num_sample_iter\n        else:\n            params_final = params.copy()\n\n        if train_dir is not None:\n            params_final[\'train_dir\'] = train_dir\n        if params_final[\'iterations\'] > 0:\n            self.model = self.model_type(\n                **params_final,\n            )\n\n            # TODO: Strangely, this performs different if clone init_model is sent in than if trained for same total number of iterations. May be able to optimize catboost models further with this\n            self.model.fit(\n                X_train,\n                eval_set=eval_set,\n                verbose=verbose,\n                early_stopping_rounds=early_stopping_rounds,\n                # use_best_model=True,\n                init_model=init_model,\n            )\n\n            if init_model is not None:\n                final_model_best_score = self.model.get_best_score()[\'validation\'][self.metric_name]\n                if self.stopping_metric._optimum > final_model_best_score:\n                    if final_model_best_score > init_model_best_score:\n                        best_iteration = init_model_tree_count + self.model.get_best_iteration()\n                    else:\n                        best_iteration = init_model_best_iteration\n                else:\n                    if final_model_best_score < init_model_best_score:\n                        best_iteration = init_model_tree_count + self.model.get_best_iteration()\n                    else:\n                        best_iteration = init_model_best_iteration\n\n                self.model.shrink(ntree_start=0, ntree_end=best_iteration+1)\n\n        self.params_trained[\'iterations\'] = self.model.tree_count_\n\n    def get_model_feature_importance(self):\n        importance_df = self.model.get_feature_importance(prettified=True)\n        importance_df[\'Importances\'] = importance_df[\'Importances\'] / 100\n        importance_series = importance_df.set_index(\'Feature Id\')[\'Importances\']\n        importance_dict = importance_series.to_dict()\n        return importance_dict\n'"
autogluon/utils/tabular/ml/models/catboost/catboost_utils.py,0,"b""import numpy as np\n\nfrom ...constants import BINARY, MULTICLASS, REGRESSION\n\n\n# TODO: Add weight support?\n# TODO: Can these be optimized? What computational cost do they have compared to the default catboost versions?\nclass CustomMetric:\n    def __init__(self, metric, is_higher_better, needs_pred_proba):\n        self.metric = metric\n        self.is_higher_better = is_higher_better\n        self.needs_pred_proba = needs_pred_proba\n\n    @staticmethod\n    def get_final_error(error, weight):\n        return error\n\n    def is_max_optimal(self):\n        return self.is_higher_better\n\n    def evaluate(self, approxes, target, weight):\n        raise NotImplementedError\n\n\nclass BinaryCustomMetric(CustomMetric):\n    @staticmethod\n    def _get_y_pred_proba(approxes):\n        return np.array(approxes[0])\n\n    @staticmethod\n    def _get_y_pred(y_pred_proba):\n        return np.round(y_pred_proba)\n\n    def evaluate(self, approxes, target, weight):\n        y_pred_proba = self._get_y_pred_proba(approxes=approxes)\n\n        # TODO: Binary log_loss doesn't work for some reason\n        if self.needs_pred_proba:\n            score = self.metric(np.array(target), y_pred_proba)\n        else:\n            raise NotImplementedError('Custom Catboost Binary prob metrics are not supported by AutoGluon.')\n            # y_pred = self._get_y_pred(y_pred_proba=y_pred_proba)  # This doesn't work at the moment because catboost returns some strange valeus in approxes which are not the probabilities\n            # score = self.metric(np.array(target), y_pred)\n\n        return score, 1\n\n\nclass MulticlassCustomMetric(CustomMetric):\n    @staticmethod\n    def _get_y_pred_proba(approxes):\n        return np.array(approxes)\n\n    @staticmethod\n    def _get_y_pred(y_pred_proba):\n        return y_pred_proba.argmax(axis=0)\n\n    def evaluate(self, approxes, target, weight):\n        y_pred_proba = self._get_y_pred_proba(approxes=approxes)\n        if self.needs_pred_proba:\n            raise NotImplementedError('Custom Catboost Multiclass proba metrics are not supported by AutoGluon.')\n            # y_pred_proba = y_pred_proba.reshape(len(np.unique(np.array(target))), -1).T\n            # score = self.metric(np.array(target), y_pred_proba)  # This doesn't work at the moment because catboost returns some strange valeus in approxes which are not the probabilities\n        else:\n            y_pred = self._get_y_pred(y_pred_proba=y_pred_proba)\n            score = self.metric(np.array(target), y_pred)\n\n        return score, 1\n\n\nclass RegressionCustomMetric(CustomMetric):\n    @staticmethod\n    def _get_y_pred(approxes):\n        return np.array(approxes[0])\n\n    def evaluate(self, approxes, target, weight):\n        y_pred = self._get_y_pred(approxes=approxes)\n        score = self.metric(np.array(target), y_pred)\n\n        return score, 1\n\n\nmetric_classes_dict = {\n    BINARY: BinaryCustomMetric,\n    MULTICLASS: MulticlassCustomMetric,\n    REGRESSION: RegressionCustomMetric,\n}\n\n\ndef construct_custom_catboost_metric(metric, is_higher_better, needs_pred_proba, problem_type):\n    if (metric.name == 'log_loss') and (problem_type == MULTICLASS) and needs_pred_proba:\n        return 'MultiClass'\n    if metric.name == 'accuracy':\n        return 'Accuracy'\n    if (metric.name == 'log_loss') and (problem_type == BINARY) and needs_pred_proba:\n        return 'Logloss'\n    if (metric.name == 'f1') and (problem_type == BINARY) and not needs_pred_proba:\n        return 'F1'\n    if (metric.name == 'balanced_accuracy') and (problem_type == BINARY) and not needs_pred_proba:\n        return 'BalancedAccuracy'\n    if (metric.name == 'recall') and (problem_type == BINARY) and not needs_pred_proba:\n        return 'Recall'\n    if (metric.name == 'precision') and (problem_type == BINARY) and not needs_pred_proba:\n        return 'Precision'\n    metric_class = metric_classes_dict[problem_type]\n    return metric_class(metric=metric, is_higher_better=is_higher_better, needs_pred_proba=needs_pred_proba)\n"""
autogluon/utils/tabular/ml/models/ensemble/__init__.py,0,b''
autogluon/utils/tabular/ml/models/ensemble/bagged_ensemble_model.py,0,"b'import copy\nimport logging\nimport os\nimport time\nfrom collections import Counter\nfrom statistics import mean\n\nimport numpy as np\nimport pandas as pd\n\nfrom ..abstract.abstract_model import AbstractModel\nfrom ...constants import MULTICLASS, REGRESSION, SOFTCLASS, REFIT_FULL_SUFFIX\nfrom ...utils import generate_kfold\nfrom ....utils.exceptions import TimeLimitExceeded\nfrom ....utils.loaders import load_pkl\nfrom ....utils.savers import save_pkl\n\nlogger = logging.getLogger(__name__)\n\n\n# TODO: Add metadata object with info like score on each model, train time on each model, etc.\nclass BaggedEnsembleModel(AbstractModel):\n    _oof_filename = \'oof.pkl\'\n    def __init__(self, path: str, name: str, model_base: AbstractModel, hyperparameters=None, objective_func=None, stopping_metric=None, num_classes=None, save_bagged_folds=True, random_state=0, debug=0, **kwargs):\n        self.model_base = model_base\n        self._child_type = type(self.model_base)\n        self.models = []\n        self._oof_pred_proba = None\n        self._oof_pred_model_repeats = None\n        self._n_repeats = 0  # Number of n_repeats with at least 1 model fit, if kfold=5 and 8 models have been fit, _n_repeats is 2\n        self._n_repeats_finished = 0  # Number of n_repeats finished, if kfold=5 and 8 models have been fit, _n_repeats_finished is 1\n        self._k_fold_end = 0  # Number of models fit in current n_repeat (0 if completed), if kfold=5 and 8 models have been fit, _k_fold_end is 3\n        self._k = None  # k models per n_repeat, equivalent to kfold value\n        self._k_per_n_repeat = []  # k-fold used for each n_repeat. == [5, 10, 3] if first kfold was 5, second was 10, and third was 3\n        self._random_state = random_state\n        self.low_memory = True\n        self.bagged_mode = None\n        self.save_bagged_folds = save_bagged_folds\n\n        try:\n            feature_types_metadata = self.model_base.feature_types_metadata\n        except:\n            feature_types_metadata = None\n\n        if objective_func is None:\n            objective_func = self.model_base.objective_func\n        if stopping_metric is None:\n            stopping_metric = self.model_base.stopping_metric\n\n        super().__init__(path=path, name=name, problem_type=self.model_base.problem_type, objective_func=objective_func, stopping_metric=stopping_metric, num_classes=num_classes, feature_types_metadata=feature_types_metadata, hyperparameters=hyperparameters, debug=debug, **kwargs)\n\n    def is_valid(self):\n        return self.is_fit() and (self._n_repeats == self._n_repeats_finished)\n\n    def can_infer(self):\n        return self.is_fit() and self.save_bagged_folds\n\n    def is_stratified(self):\n        if self.problem_type == REGRESSION or self.problem_type == SOFTCLASS:\n            return False\n        else:\n            return True\n\n    def is_fit(self):\n        return len(self.models) != 0\n\n    # TODO: This assumes bagged ensemble has a complete k_fold and no partial k_fold models, this is likely fine but will act incorrectly if called when only a partial k_fold has been completed\n    #  Solving this is memory intensive, requires all oof_pred_probas from all n_repeats, so its probably not worth it.\n    @property\n    def oof_pred_proba(self):\n        # TODO: Require is_valid == True (add option param to ignore is_valid)\n        return self._oof_pred_proba_func(self._oof_pred_proba, self._oof_pred_model_repeats)\n\n    @staticmethod\n    def _oof_pred_proba_func(oof_pred_proba, oof_pred_model_repeats):\n        oof_pred_model_repeats_without_0 = np.where(oof_pred_model_repeats == 0, 1, oof_pred_model_repeats)\n        if oof_pred_proba.ndim == 2:\n            oof_pred_model_repeats_without_0 = oof_pred_model_repeats_without_0[:, None]\n        return oof_pred_proba / oof_pred_model_repeats_without_0\n\n    def preprocess(self, X, model=None):\n        if model is None:\n            if not self.models:\n                return X\n            model = self.models[0]\n        model = self.load_child(model)\n        return model.preprocess(X)\n\n    def fit(self, X, y, k_fold=5, k_fold_start=0, k_fold_end=None, n_repeats=1, n_repeat_start=0, time_limit=None, **kwargs):\n        if k_fold < 1:\n            k_fold = 1\n        if k_fold_end is None:\n            k_fold_end = k_fold\n\n        if self._oof_pred_proba is None and (k_fold_start != 0 or n_repeat_start != 0):\n            self._load_oof()\n        if n_repeat_start != self._n_repeats_finished:\n            raise ValueError(f\'n_repeat_start must equal self._n_repeats_finished, values: ({n_repeat_start}, {self._n_repeats_finished})\')\n        if n_repeats <= n_repeat_start:\n            raise ValueError(f\'n_repeats must be greater than n_repeat_start, values: ({n_repeats}, {n_repeat_start})\')\n        if k_fold_start != self._k_fold_end:\n            raise ValueError(f\'k_fold_start must equal previous k_fold_end, values: ({k_fold_start}, {self._k_fold_end})\')\n        if k_fold_start >= k_fold_end:\n            # TODO: Remove this limitation if n_repeats > 1\n            raise ValueError(f\'k_fold_end must be greater than k_fold_start, values: ({k_fold_end}, {k_fold_start})\')\n        if (n_repeats - n_repeat_start) > 1 and k_fold_end != k_fold:\n            # TODO: Remove this limitation\n            raise ValueError(f\'k_fold_end must equal k_fold when (n_repeats - n_repeat_start) > 1, values: ({k_fold_end}, {k_fold})\')\n        if self._k is not None and self._k != k_fold:\n            raise ValueError(f\'k_fold must equal previously fit k_fold value for the current n_repeat, values: (({k_fold}, {self._k})\')\n        fold_start = n_repeat_start * k_fold + k_fold_start\n        fold_end = (n_repeats - 1) * k_fold + k_fold_end\n        time_start = time.time()\n\n        model_base = self._get_model_base()\n        if self.features is not None:\n            model_base.features = self.features\n        model_base.feature_types_metadata = self.feature_types_metadata  # TODO: Don\'t pass this here\n\n        if self.model_base is not None:\n            self.save_model_base(self.model_base)\n            self.model_base = None\n\n        if k_fold == 1:\n            if self._n_repeats != 0:\n                raise ValueError(f\'n_repeats must equal 0 when fitting a single model with k_fold < 2, values: ({self._n_repeats}, {k_fold})\')\n            model_base.set_contexts(path_context=self.path + model_base.name + os.path.sep)\n            time_start_fit = time.time()\n            model_base.fit(X_train=X, Y_train=y, time_limit=time_limit, **kwargs)\n            model_base.fit_time = time.time() - time_start_fit\n            model_base.predict_time = None\n            self._oof_pred_proba = model_base.predict_proba(X=X)  # TODO: Cheater value, will be overfit to valid set\n            self._oof_pred_model_repeats = np.ones(shape=len(X))\n            self._n_repeats = 1\n            self._n_repeats_finished = 1\n            self._k_per_n_repeat = [1]\n            self.bagged_mode = False\n            model_base.reduce_memory_size(remove_fit=True, remove_info=False, requires_save=True)\n            if not self.save_bagged_folds:\n                model_base.model = None\n            if self.low_memory:\n                self.save_child(model_base, verbose=False)\n                self.models = [model_base.name]\n            else:\n                self.models = [model_base]\n            self._add_child_times_to_bag(model=model_base)\n            return\n\n        # TODO: Preprocess data here instead of repeatedly\n        kfolds = generate_kfold(X=X, y=y, n_splits=k_fold, stratified=self.is_stratified(), random_state=self._random_state, n_repeats=n_repeats)\n\n        if self.problem_type == MULTICLASS:\n            oof_pred_proba = np.zeros(shape=(len(X), len(y.unique())))\n        elif self.problem_type == SOFTCLASS:\n            oof_pred_proba = np.zeros(shape=y.shape)\n        else:\n            oof_pred_proba = np.zeros(shape=len(X))\n        oof_pred_model_repeats = np.zeros(shape=len(X))\n\n        models = []\n        folds_to_fit = fold_end - fold_start\n        for j in range(n_repeat_start, n_repeats):  # For each n_repeat\n            cur_repeat_count = j - n_repeat_start\n            fold_start_n_repeat = fold_start + cur_repeat_count * k_fold\n            fold_end_n_repeat = min(fold_start_n_repeat + k_fold, fold_end)\n            # TODO: Consider moving model fit inner for loop to a function to simply this code\n            for i in range(fold_start_n_repeat, fold_end_n_repeat):  # For each fold\n                folds_finished = i - fold_start\n                folds_left = fold_end - i\n                fold = kfolds[i]\n                time_elapsed = time.time() - time_start\n                if time_limit is not None:\n                    time_left = time_limit - time_elapsed\n                    required_time_per_fold = time_left / folds_left\n                    time_limit_fold = required_time_per_fold * 0.8\n                    if folds_finished > 0:\n                        expected_time_required = time_elapsed * folds_to_fit / folds_finished\n                        expected_remaining_time_required = expected_time_required * folds_left / folds_to_fit\n                        if expected_remaining_time_required > time_left:\n                            raise TimeLimitExceeded\n                    if time_left <= 0:\n                        raise TimeLimitExceeded\n                else:\n                    time_limit_fold = None\n\n                time_start_fold = time.time()\n                train_index, test_index = fold\n                X_train, X_test = X.iloc[train_index, :], X.iloc[test_index, :]\n                y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n                fold_model = copy.deepcopy(model_base)\n                fold_model.name = f\'{fold_model.name}_fold_{i}\'\n                fold_model.set_contexts(self.path + fold_model.name + os.path.sep)\n                fold_model.fit(X_train=X_train, Y_train=y_train, X_test=X_test, Y_test=y_test, time_limit=time_limit_fold, **kwargs)\n                time_train_end_fold = time.time()\n                if time_limit is not None:  # Check to avoid unnecessarily predicting and saving a model when an Exception is going to be raised later\n                    if i != (fold_end - 1):\n                        time_elapsed = time.time() - time_start\n                        time_left = time_limit - time_elapsed\n                        expected_time_required = time_elapsed * folds_to_fit / (folds_finished + 1)\n                        expected_remaining_time_required = expected_time_required * (folds_left - 1) / folds_to_fit\n                        if expected_remaining_time_required > time_left:\n                            raise TimeLimitExceeded\n                pred_proba = fold_model.predict_proba(X_test)\n                time_predict_end_fold = time.time()\n                fold_model.fit_time = time_train_end_fold - time_start_fold\n                fold_model.predict_time = time_predict_end_fold - time_train_end_fold\n                fold_model.val_score = fold_model.score_with_y_pred_proba(y=y_test, y_pred_proba=pred_proba)\n                fold_model.reduce_memory_size(remove_fit=True, remove_info=False, requires_save=True)\n                if not self.save_bagged_folds:\n                    fold_model.model = None\n                if self.low_memory:\n                    self.save_child(fold_model, verbose=False)\n                    models.append(fold_model.name)\n                else:\n                    models.append(fold_model)\n                oof_pred_proba[test_index] += pred_proba\n                oof_pred_model_repeats[test_index] += 1\n                self._add_child_times_to_bag(model=fold_model)\n            if (fold_end_n_repeat != fold_end) or (k_fold == k_fold_end):\n                self._k_per_n_repeat.append(k_fold)\n        self.models += models\n\n        self.bagged_mode = True\n\n        if self._oof_pred_proba is None:\n            self._oof_pred_proba = oof_pred_proba\n            self._oof_pred_model_repeats = oof_pred_model_repeats\n        else:\n            self._oof_pred_proba += oof_pred_proba\n            self._oof_pred_model_repeats += oof_pred_model_repeats\n\n        self._n_repeats = n_repeats\n        if k_fold == k_fold_end:\n            self._k = None\n            self._k_fold_end = 0\n            self._n_repeats_finished = self._n_repeats\n        else:\n            self._k = k_fold\n            self._k_fold_end = k_fold_end\n            self._n_repeats_finished = self._n_repeats - 1\n\n    # FIXME: Defective if model does not apply same preprocessing in all bags!\n    #  No model currently violates this rule, but in future it could happen\n    def predict_proba(self, X, preprocess=True):\n        model = self.load_child(self.models[0])\n        if preprocess:\n            X = self.preprocess(X, model=model)\n\n        pred_proba = model.predict_proba(X=X, preprocess=False)\n        for model in self.models[1:]:\n            model = self.load_child(model)\n            pred_proba += model.predict_proba(X=X, preprocess=False)\n        pred_proba = pred_proba / len(self.models)\n\n        return pred_proba\n\n    def score_with_oof(self, y):\n        self._load_oof()\n        valid_indices = self._oof_pred_model_repeats > 0\n        y = y[valid_indices]\n        y_pred_proba = self.oof_pred_proba[valid_indices]\n\n        return self.score_with_y_pred_proba(y=y, y_pred_proba=y_pred_proba)\n\n    # TODO: Augment to generate OOF after shuffling each column in X (Batching), this is the fastest way.\n    # Generates OOF predictions from pre-trained bagged models, assuming X and y are in the same row order as used in .fit(X, y)\n    def compute_feature_importance(self, X, y, features_to_use=None, preprocess=True, is_oof=True, silent=False, **kwargs):\n        feature_importance_fold_list = []\n        fold_weights = []\n        # TODO: Preprocess data here instead of repeatedly\n        model_index = 0\n        for n_repeat, k in enumerate(self._k_per_n_repeat):\n            if is_oof:\n                if not self.bagged_mode:\n                    raise AssertionError(\'Model trained with no validation data cannot get feature importances on training data, please specify new test data to compute feature importances (model=%s)\' % self.name)\n                kfolds = generate_kfold(X=X, y=y, n_splits=k, stratified=self.is_stratified(), random_state=self._random_state, n_repeats=n_repeat + 1)\n                cur_kfolds = kfolds[n_repeat * k:(n_repeat+1) * k]\n            else:\n                cur_kfolds = [(None, list(range(len(X))))]*k\n            for i, fold in enumerate(cur_kfolds):\n                _, test_index = fold\n                model = self.load_child(self.models[model_index + i])\n                feature_importance_fold = model.compute_feature_importance(X=X.iloc[test_index, :], y=y.iloc[test_index], features_to_use=features_to_use, preprocess=preprocess, silent=silent)\n                feature_importance_fold_list.append(feature_importance_fold)\n                fold_weights.append(len(test_index))\n            model_index += k\n\n        weight_total = sum(fold_weights)\n        fold_weights = [weight/weight_total for weight in fold_weights]\n\n        for i, result in enumerate(feature_importance_fold_list):\n            feature_importance_fold_list[i] = feature_importance_fold_list[i] * fold_weights[i]\n\n        feature_importance = pd.concat(feature_importance_fold_list, axis=1, sort=True).sum(1).sort_values(ascending=False)\n\n        # TODO: Consider utilizing z scores and stddev to make threshold decisions\n        # stddev = pd.concat(feature_importance_fold_list, axis=1, sort=True).std(1).sort_values(ascending=False)\n        # feature_importance_df = pd.DataFrame(index=feature_importance.index)\n        # feature_importance_df[\'importance\'] = feature_importance\n        # feature_importance_df[\'stddev\'] = stddev\n        # feature_importance_df[\'z\'] = feature_importance_df[\'importance\'] / feature_importance_df[\'stddev\']\n\n        return feature_importance\n\n    def load_child(self, model, verbose=False) -> AbstractModel:\n        if isinstance(model, str):\n            child_path = self.create_contexts(self.path + model + os.path.sep)\n            return self._child_type.load(path=child_path, verbose=verbose)\n        else:\n            return model\n\n    def save_child(self, model, verbose=False):\n        child = self.load_child(model)\n        child.set_contexts(self.path + child.name + os.path.sep)\n        child.save(verbose=verbose)\n\n    # TODO: Multiply epochs/n_iterations by some value (such as 1.1) to account for having more training data than bagged models\n    def convert_to_refitfull_template(self):\n        compressed_params = self._get_compressed_params()\n        model_compressed = copy.deepcopy(self._get_model_base())\n        model_compressed.feature_types_metadata = self.feature_types_metadata  # TODO: Don\'t pass this here\n        model_compressed.params = compressed_params\n        model_compressed.name = model_compressed.name + REFIT_FULL_SUFFIX\n        model_compressed.set_contexts(self.path_root + model_compressed.name + os.path.sep)\n        return model_compressed\n\n    def _get_compressed_params(self):\n        model_params_list = [\n            self.load_child(child).get_trained_params()\n            for child in self.models\n        ]\n\n        model_params_compressed = dict()\n        for param in model_params_list[0].keys():\n            model_param_vals = [model_params[param] for model_params in model_params_list]\n            if all(isinstance(val, bool) for val in model_param_vals):\n                counter = Counter(model_param_vals)\n                compressed_val = counter.most_common(1)[0][0]\n            elif all(isinstance(val, int) for val in model_param_vals):\n                compressed_val = round(mean(model_param_vals))\n            elif all(isinstance(val, float) for val in model_param_vals):\n                compressed_val = mean(model_param_vals)\n            else:\n                try:\n                    counter = Counter(model_param_vals)\n                    compressed_val = counter.most_common(1)[0][0]\n                except TypeError:\n                    compressed_val = model_param_vals[0]\n            model_params_compressed[param] = compressed_val\n        return model_params_compressed\n\n    def _get_model_base(self):\n        if self.model_base is None:\n            return self.load_model_base()\n        else:\n            return self.model_base\n\n    def _add_child_times_to_bag(self, model):\n        if self.fit_time is None:\n            self.fit_time = model.fit_time\n        else:\n            self.fit_time += model.fit_time\n\n        if self.predict_time is None:\n            self.predict_time = model.predict_time\n        else:\n            self.predict_time += model.predict_time\n\n    @classmethod\n    def load(cls, path, file_prefix="""", reset_paths=True, low_memory=True, load_oof=False, verbose=True):\n        path = path + file_prefix\n        load_path = path + cls.model_file_name\n        obj = load_pkl.load(path=load_path, verbose=verbose)\n        if reset_paths:\n            obj.set_contexts(path)\n        if not low_memory:\n            obj.persist_child_models(reset_paths=reset_paths)\n        if load_oof:\n            obj._load_oof()\n        return obj\n\n    @classmethod\n    def load_oof(cls, path, verbose=True):\n        try:\n            oof = load_pkl.load(path=path + \'utils\' + os.path.sep + cls._oof_filename, verbose=verbose)\n            oof_pred_proba = oof[\'_oof_pred_proba\']\n            oof_pred_model_repeats = oof[\'_oof_pred_model_repeats\']\n        except FileNotFoundError:\n            model = cls.load(path=path, reset_paths=True, verbose=verbose)\n            model._load_oof()\n            oof_pred_proba = model._oof_pred_proba\n            oof_pred_model_repeats = model._oof_pred_model_repeats\n        return cls._oof_pred_proba_func(oof_pred_proba=oof_pred_proba, oof_pred_model_repeats=oof_pred_model_repeats)\n\n    def _load_oof(self):\n        if self._oof_pred_proba is not None:\n            pass\n        else:\n            oof = load_pkl.load(path=self.path + \'utils\' + os.path.sep + self._oof_filename)\n            self._oof_pred_proba = oof[\'_oof_pred_proba\']\n            self._oof_pred_model_repeats = oof[\'_oof_pred_model_repeats\']\n\n    def persist_child_models(self, reset_paths=True):\n        for i, model_name in enumerate(self.models):\n            if isinstance(model_name, str):\n                child_path = self.create_contexts(self.path + model_name + os.path.sep)\n                child_model = self._child_type.load(path=child_path, reset_paths=reset_paths, verbose=True)\n                self.models[i] = child_model\n\n    def load_model_base(self):\n        return load_pkl.load(path=self.path + \'utils\' + os.path.sep + \'model_template.pkl\')\n\n    def save_model_base(self, model_base):\n        save_pkl.save(path=self.path + \'utils\' + os.path.sep + \'model_template.pkl\', object=model_base)\n\n    def save(self, file_prefix="""", directory=None, return_filename=False, save_oof=True, verbose=True, save_children=False):\n        if directory is None:\n            directory = self.path\n        directory = directory + file_prefix\n\n        if save_children:\n            model_names = []\n            for child in self.models:\n                child = self.load_child(child)\n                child.set_contexts(self.path + child.name + os.path.sep)\n                child.save(verbose=False)\n                model_names.append(child.name)\n            self.models = model_names\n\n        file_name = directory + self.model_file_name\n\n        if save_oof and self._oof_pred_proba is not None:\n            save_pkl.save(path=self.path + \'utils\' + os.path.sep + self._oof_filename, object={\n                    \'_oof_pred_proba\': self._oof_pred_proba,\n                    \'_oof_pred_model_repeats\': self._oof_pred_model_repeats,\n            })\n            self._oof_pred_proba = None\n            self._oof_pred_model_repeats = None\n        save_pkl.save(path=file_name, object=self, verbose=verbose)\n        if return_filename:\n            return file_name\n\n    # If `remove_fit_stack=True`, variables will be removed that are required to fit more folds and to fit new stacker models which use this model as a base model.\n    #  This includes OOF variables.\n    def reduce_memory_size(self, remove_fit_stack=False, remove_fit=True, remove_info=False, requires_save=True, reduce_children=False, **kwargs):\n        super().reduce_memory_size(remove_fit=remove_fit, remove_info=remove_info, requires_save=requires_save, **kwargs)\n        if remove_fit_stack:\n            try:\n                os.remove(self.path + \'utils\' + os.path.sep + self._oof_filename)\n            except FileNotFoundError:\n                pass\n            if requires_save:\n                self._oof_pred_proba = None\n                self._oof_pred_model_repeats = None\n            try:\n                os.remove(self.path + \'utils\' + os.path.sep + \'model_template.pkl\')\n            except FileNotFoundError:\n                pass\n            if requires_save:\n                self.model_base = None\n            try:\n                os.rmdir(self.path + \'utils\')\n            except OSError:\n                pass\n        if reduce_children:\n            for model in self.models:\n                model = self.load_child(model)\n                model.reduce_memory_size(remove_fit=remove_fit, remove_info=remove_info, requires_save=requires_save, **kwargs)\n                if requires_save and self.low_memory:\n                    self.save_child(model=model)\n\n    def _get_model_names(self):\n        model_names = []\n        for model in self.models:\n            if isinstance(model, str):\n                model_names.append(model)\n            else:\n                model_names.append(model.name)\n        return model_names\n\n    def get_info(self):\n        info = super().get_info()\n        children_info = self._get_child_info()\n        child_memory_sizes = [child[\'memory_size\'] for child in children_info.values()]\n        sum_memory_size_child = sum(child_memory_sizes)\n        if child_memory_sizes:\n            max_memory_size_child = max(child_memory_sizes)\n        else:\n            max_memory_size_child = 0\n        if self.low_memory:\n            max_memory_size = info[\'memory_size\'] + sum_memory_size_child\n            min_memory_size = info[\'memory_size\'] + max_memory_size_child\n        else:\n            max_memory_size = info[\'memory_size\']\n            min_memory_size = info[\'memory_size\'] - sum_memory_size_child + max_memory_size_child\n\n        bagged_info = dict(\n            child_type=self._child_type.__name__,\n            num_child_models=len(self.models),\n            child_model_names=self._get_model_names(),\n            _n_repeats=self._n_repeats,\n            # _n_repeats_finished=self._n_repeats_finished,  # commented out because these are too technical\n            # _k_fold_end=self._k_fold_end,\n            # _k=self._k,\n            _k_per_n_repeat=self._k_per_n_repeat,\n            _random_state=self._random_state,\n            low_memory=self.low_memory,  # If True, then model will attempt to use at most min_memory_size memory by having at most one child in memory. If False, model will use max_memory_size memory.\n            bagged_mode=self.bagged_mode,\n            max_memory_size=max_memory_size,  # Memory used when all children are loaded into memory at once.\n            min_memory_size=min_memory_size,  # Memory used when only the largest child is loaded into memory.\n        )\n        info[\'bagged_info\'] = bagged_info\n        info[\'children_info\'] = children_info\n\n        return info\n\n    def _get_child_info(self):\n        child_info_dict = dict()\n        for model in self.models:\n            if isinstance(model, str):\n                child_path = self.create_contexts(self.path + model + os.path.sep)\n                child_info_dict[model] = self._child_type.load_info(child_path)\n            else:\n                child_info_dict[model.name] = model.get_info()\n        return child_info_dict\n'"
autogluon/utils/tabular/ml/models/ensemble/greedy_weighted_ensemble_model.py,0,"b""import logging\n\nfrom ..abstract.abstract_model import AbstractModel\nfrom ...constants import MULTICLASS\nfrom ...tuning.ensemble_selection import EnsembleSelection\n\nlogger = logging.getLogger(__name__)\n\n\nclass GreedyWeightedEnsembleModel(AbstractModel):\n    def __init__(self, path: str, name: str, problem_type: str, objective_func, num_classes, base_model_names, model_base=EnsembleSelection, stopping_metric=None, hyperparameters=None, features=None, feature_types_metadata=None, debug=0, **kwargs):\n        super().__init__(path, name, problem_type=problem_type, objective_func=objective_func, stopping_metric=stopping_metric, num_classes=num_classes, hyperparameters=hyperparameters, features=features, feature_types_metadata=feature_types_metadata, debug=debug, **kwargs)\n        self.model_base = model_base\n        self.base_model_names = base_model_names\n        self.weights_ = None\n        self.features, self.num_pred_cols_per_model = self.set_stack_columns(base_model_names=self.base_model_names)\n\n    def _get_default_searchspace(self):\n        spaces = {}\n        return spaces\n\n    def _set_default_params(self):\n        default_params = {'ensemble_size': 100}\n        for param, val in default_params.items():\n            self._set_default_param_value(param, val)\n\n    # TODO: Consider moving convert_pred_probas_df_to_list into inner model to ensure X remains a dataframe after preprocess is called\n    def preprocess(self, X):\n        X = self.convert_pred_probas_df_to_list(X)\n        return X\n\n    # TODO: Check memory after loading best model predictions, only load top X model predictions that fit in memory\n    def fit(self, X_train, Y_train, X_test=None, Y_test=None, time_limit=None, **kwargs):\n        X_train = self.preprocess(X_train)\n\n        self.model = self.model_base(ensemble_size=self.params['ensemble_size'], problem_type=self.problem_type, metric=self.stopping_metric)\n        self.model = self.model.fit(X_train, Y_train, time_limit=time_limit)\n        self.base_model_names, self.model.weights_ = self.remove_zero_weight_models(self.base_model_names, self.model.weights_)\n        self.weights_ = self.model.weights_\n        self.features, self.num_pred_cols_per_model = self.set_stack_columns(base_model_names=self.base_model_names)\n        self.params_trained['ensemble_size'] = self.model.ensemble_size\n\n    def convert_pred_probas_df_to_list(self, pred_probas_df) -> list:\n        pred_probas = []\n        for i, model in enumerate(self.base_model_names):\n            index_start = i * self.num_pred_cols_per_model\n            index_end = (i + 1) * self.num_pred_cols_per_model\n            model_cols = self.features[index_start:index_end]\n            pred_proba = pred_probas_df[model_cols].values\n            if self.num_pred_cols_per_model == 1:\n                pred_proba = pred_proba.flatten()\n            pred_probas.append(pred_proba)\n        return pred_probas\n\n    @staticmethod\n    def remove_zero_weight_models(base_model_names, base_model_weights):\n        base_models_to_keep = []\n        base_model_weights_to_keep = []\n        for i, weight in enumerate(base_model_weights):\n            if weight != 0:\n                base_models_to_keep.append(base_model_names[i])\n                base_model_weights_to_keep.append(weight)\n        return base_models_to_keep, base_model_weights_to_keep\n\n    def set_stack_columns(self, base_model_names):\n        if self.problem_type == MULTICLASS:\n            stack_columns = [model_name + '_' + str(cls) for model_name in base_model_names for cls in range(self.num_classes)]\n            num_pred_cols_per_model = self.num_classes\n        else:\n            stack_columns = base_model_names\n            num_pred_cols_per_model = 1\n        return stack_columns, num_pred_cols_per_model\n\n    def _get_model_weights(self):\n        num_models = len(self.base_model_names)\n        model_weight_dict = {self.base_model_names[i]: self.weights_[i] for i in range(num_models)}\n        return model_weight_dict\n\n    def get_info(self):\n        info = super().get_info()\n        info['model_weights'] = self._get_model_weights()\n        return info\n"""
autogluon/utils/tabular/ml/models/ensemble/stacker_ensemble_model.py,0,"b""import copy, logging, time\nimport os\nfrom typing import Dict\nimport numpy as np\nimport pandas as pd\nfrom collections import defaultdict\n\nfrom ...utils import generate_kfold\nfrom ..abstract.abstract_model import AbstractModel\nfrom .bagged_ensemble_model import BaggedEnsembleModel\nfrom ...constants import MULTICLASS\n\nlogger = logging.getLogger(__name__)\n\n\n# TODO: Currently, if this is a stacker above level 1, it will be very slow taking raw input due to each stacker needing to repeat computation on the base models.\n#  To solve this, this model must know full context of stacker, and only get preds once for each required model\n#  This is already done in trainer, but could be moved internally.\nclass StackerEnsembleModel(BaggedEnsembleModel):\n    def __init__(self, path: str, name: str, model_base: AbstractModel, base_model_names=None, base_models_dict=None, base_model_paths_dict=None, base_model_types_dict=None, base_model_types_inner_dict=None, base_model_performances_dict=None, use_orig_features=True, num_classes=None, hyperparameters=None, objective_func=None, stopping_metric=None, save_bagged_folds=True, random_state=0, debug=0, **kwargs):\n        super().__init__(path=path, name=name, model_base=model_base, hyperparameters=hyperparameters, objective_func=objective_func, stopping_metric=stopping_metric, num_classes=num_classes, save_bagged_folds=save_bagged_folds, random_state=random_state, debug=debug, **kwargs)\n        if base_model_names is None:\n            base_model_names = []\n        if base_models_dict is None:\n            base_models_dict = {}\n        if base_model_paths_dict is None:\n            base_model_paths_dict = {}\n        if base_model_types_dict is None:\n            base_model_types_dict = {}\n        self.base_model_names = base_model_names\n        self.base_models_dict: Dict[str, AbstractModel] = base_models_dict  # String name -> Model objects\n        self.base_model_paths_dict = base_model_paths_dict\n        self.base_model_types_dict = base_model_types_dict\n        self.use_orig_features = use_orig_features\n\n        if (base_model_performances_dict is not None) and (base_model_types_inner_dict is not None):\n            if self.params['max_models_per_type'] > 0:\n                self.base_model_names = self.limit_models_per_type(models=self.base_model_names, model_types=base_model_types_inner_dict, model_scores=base_model_performances_dict, max_models_per_type=self.params['max_models_per_type'])\n            if self.params['max_models'] > 0:\n                self.base_model_names = self.limit_models(models=self.base_model_names, model_scores=base_model_performances_dict, max_models=self.params['max_models'])\n\n        for model_name, model in self.base_models_dict.items():\n            if model_name not in self.base_model_names:\n                self.base_models_dict.pop(model_name)\n\n        self.stack_column_prefix_lst = copy.deepcopy(self.base_model_names)\n        self.stack_columns, self.num_pred_cols_per_model = self.set_stack_columns(stack_column_prefix_lst=self.stack_column_prefix_lst)\n        self.stack_column_prefix_to_model_map = {stack_column_prefix: self.base_model_names[i] for i, stack_column_prefix in enumerate(self.stack_column_prefix_lst)}\n\n    @staticmethod\n    def limit_models_per_type(models, model_types, model_scores, max_models_per_type):\n        model_type_groups = defaultdict(list)\n        for model in models:\n            model_type = model_types[model]\n            model_type_groups[model_type].append((model, model_scores[model]))\n        for key in model_type_groups:\n            model_type_groups[key] = sorted(model_type_groups[key], key=lambda x: x[1], reverse=True)\n        for key in model_type_groups:\n            model_type_groups[key] = model_type_groups[key][:max_models_per_type]\n        models_remain = []\n        for key in model_type_groups:\n            models_remain += model_type_groups[key]\n        models_valid = [model for model, score in models_remain]\n        return models_valid\n\n    def limit_models(self, models, model_scores, max_models):\n        model_types = {model: '' for model in models}\n        return self.limit_models_per_type(models=models, model_types=model_types, model_scores=model_scores, max_models_per_type=max_models)\n\n    def _set_default_params(self):\n        default_params = {'max_models': 25, 'max_models_per_type': 5}\n        for param, val in default_params.items():\n            self._set_default_param_value(param, val)\n\n    def preprocess(self, X, preprocess=True, fit=False, compute_base_preds=True, infer=True, model=None, model_pred_proba_dict=None):\n        if self.stack_column_prefix_lst:\n            if infer:\n                if set(self.stack_columns).issubset(set(list(X.columns))):\n                    compute_base_preds = False  # TODO: Consider removing, this can be dangerous but the code to make this work otherwise is complex (must rewrite predict_proba)\n            if compute_base_preds:\n                X_stacker = []\n                for stack_column_prefix in self.stack_column_prefix_lst:\n                    base_model_name = self.stack_column_prefix_to_model_map[stack_column_prefix]\n                    if fit:\n                        base_model_type = self.base_model_types_dict[base_model_name]\n                        base_model_path = self.base_model_paths_dict[base_model_name]\n                        y_pred_proba = base_model_type.load_oof(path=base_model_path)\n                    elif model_pred_proba_dict and base_model_name in model_pred_proba_dict:\n                        y_pred_proba = model_pred_proba_dict[base_model_name]\n                    else:\n                        base_model = self.load_base_model(base_model_name)\n                        y_pred_proba = base_model.predict_proba(X)\n                    X_stacker.append(y_pred_proba)  # TODO: This could get very large on a high class count problem. Consider capping to top N most frequent classes and merging least frequent\n                X_stacker = self.pred_probas_to_df(X_stacker)\n                X_stacker.index = X.index\n                if self.use_orig_features:\n                    X = pd.concat([X_stacker, X], axis=1)\n                else:\n                    X = X_stacker\n            elif not self.use_orig_features:\n                X = X[self.stack_columns]\n        if preprocess:\n            X = super().preprocess(X, model=model)\n        return X\n\n    def pred_probas_to_df(self, pred_proba: list) -> pd.DataFrame:\n        if self.problem_type == MULTICLASS:\n            pred_proba = np.concatenate(pred_proba, axis=1)\n            pred_proba = pd.DataFrame(pred_proba, columns=self.stack_columns)\n        else:\n            pred_proba = pd.DataFrame(data=np.asarray(pred_proba).T, columns=self.stack_columns)\n        return pred_proba\n\n    def fit(self, X, y, k_fold=5, k_fold_start=0, k_fold_end=None, n_repeats=1, n_repeat_start=0, compute_base_preds=True, time_limit=None, **kwargs):\n        start_time = time.time()\n        X = self.preprocess(X=X, preprocess=False, fit=True, compute_base_preds=compute_base_preds)\n        if time_limit is not None:\n            time_limit = time_limit - (time.time() - start_time)\n        if len(self.models) == 0:\n            if self.feature_types_metadata is None:  # TODO: This is probably not the best way to do this\n                self.feature_types_metadata = {'float': self.stack_columns}\n            else:\n                self.feature_types_metadata = copy.deepcopy(self.feature_types_metadata)\n                if 'float' in self.feature_types_metadata.keys():\n                    self.feature_types_metadata['float'] += self.stack_columns\n                else:\n                    self.feature_types_metadata['float'] = self.stack_columns\n        super().fit(X=X, y=y, k_fold=k_fold, k_fold_start=k_fold_start, k_fold_end=k_fold_end, n_repeats=n_repeats, n_repeat_start=n_repeat_start, time_limit=time_limit, **kwargs)\n\n    def set_contexts(self, path_context):\n        path_root_orig = self.path_root\n        super().set_contexts(path_context=path_context)\n        for model, model_path in self.base_model_paths_dict.items():\n            model_local_path = model_path.split(path_root_orig, 1)[1]\n            self.base_model_paths_dict[model] = self.path_root + model_local_path\n\n    def set_stack_columns(self, stack_column_prefix_lst):\n        if self.problem_type == MULTICLASS:\n            stack_columns = [stack_column_prefix + '_' + str(cls) for stack_column_prefix in stack_column_prefix_lst for cls in range(self.num_classes)]\n            num_pred_cols_per_model = self.num_classes\n        else:\n            stack_columns = stack_column_prefix_lst\n            num_pred_cols_per_model = 1\n        return stack_columns, num_pred_cols_per_model\n\n    # TODO: Currently double disk usage, saving model in HPO and also saving model in stacker\n    def hyperparameter_tune(self, X, y, k_fold, scheduler_options=None, compute_base_preds=True, **kwargs):\n        if len(self.models) != 0:\n            raise ValueError('self.models must be empty to call hyperparameter_tune, value: %s' % self.models)\n\n        if len(self.models) == 0:\n            if self.feature_types_metadata is None:  # TODO: This is probably not the best way to do this\n                self.feature_types_metadata = {'float': self.stack_columns}\n            else:\n                self.feature_types_metadata = copy.deepcopy(self.feature_types_metadata)\n                if 'float' in self.feature_types_metadata.keys():\n                    self.feature_types_metadata['float'] += self.stack_columns\n                else:\n                    self.feature_types_metadata['float'] = self.stack_columns\n        self.model_base.feature_types_metadata = self.feature_types_metadata  # TODO: Move this\n\n        # TODO: Preprocess data here instead of repeatedly\n        X = self.preprocess(X=X, preprocess=False, fit=True, compute_base_preds=compute_base_preds)\n        kfolds = generate_kfold(X=X, y=y, n_splits=k_fold, stratified=self.is_stratified(), random_state=self._random_state, n_repeats=1)\n\n        train_index, test_index = kfolds[0]\n        X_train, X_test = X.iloc[train_index, :], X.iloc[test_index, :]\n        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n        orig_time = scheduler_options[1]['time_out']\n        scheduler_options[1]['time_out'] = orig_time * 0.8  # TODO: Scheduler doesn't early stop on final model, this is a safety net. Scheduler should be updated to early stop\n        hpo_models, hpo_model_performances, hpo_results = self.model_base.hyperparameter_tune(X_train=X_train, X_test=X_test, Y_train=y_train, Y_test=y_test, scheduler_options=scheduler_options, **kwargs)\n        scheduler_options[1]['time_out'] = orig_time\n\n        stackers = {}\n        stackers_performance = {}\n        for i, (model_name, model_path) in enumerate(hpo_models.items()):\n            child: AbstractModel = self._child_type.load(path=model_path)\n            y_pred_proba = child.predict_proba(X_test)\n\n            # TODO: Create new StackerEnsemble Here\n            stacker = copy.deepcopy(self)\n            stacker.name = stacker.name + os.path.sep + str(i)\n            stacker.set_contexts(self.path_root + stacker.name + os.path.sep)\n\n            if self.problem_type == MULTICLASS:\n                oof_pred_proba = np.zeros(shape=(len(X), len(y.unique())))\n            else:\n                oof_pred_proba = np.zeros(shape=len(X))\n            oof_pred_model_repeats = np.zeros(shape=len(X))\n            oof_pred_proba[test_index] += y_pred_proba\n            oof_pred_model_repeats[test_index] += 1\n\n            stacker.model_base = None\n            child.set_contexts(stacker.path + child.name + os.path.sep)\n            stacker.save_model_base(child.convert_to_template())\n\n            stacker._k = k_fold\n            stacker._k_fold_end = 1\n            stacker._n_repeats = 1\n            stacker._oof_pred_proba = oof_pred_proba\n            stacker._oof_pred_model_repeats = oof_pred_model_repeats\n            child.name = child.name + '_fold_0'\n            child.set_contexts(stacker.path + child.name + os.path.sep)\n            if not self.save_bagged_folds:\n                child.model = None\n            if stacker.low_memory:\n                stacker.save_child(child, verbose=False)\n                stacker.models.append(child.name)\n            else:\n                stacker.models.append(child)\n            stacker.val_score = child.val_score\n            stacker._add_child_times_to_bag(model=child)\n\n            stacker.save()\n            stackers[stacker.name] = stacker.path\n            stackers_performance[stacker.name] = stacker.val_score\n\n        # TODO: hpo_results likely not correct because no renames\n        return stackers, stackers_performance, hpo_results\n\n    def load_base_model(self, model_name):\n        if model_name in self.base_models_dict.keys():\n            model = self.base_models_dict[model_name]\n        else:\n            model_type = self.base_model_types_dict[model_name]\n            model_path = self.base_model_paths_dict[model_name]\n            model = model_type.load(model_path)\n        return model\n\n    def get_info(self):\n        info = super().get_info()\n        stacker_info = dict(\n            num_base_models=len(self.base_model_names),\n            base_model_names=self.base_model_names,\n            use_orig_features=self.use_orig_features,\n        )\n        children_info = info.pop('children_info')\n        info['stacker_info'] = stacker_info\n        info['children_info'] = children_info  # Ensure children_info is last in order\n        return info\n"""
autogluon/utils/tabular/ml/models/ensemble/weighted_ensemble_model.py,0,"b""import logging\nfrom collections import defaultdict\n\nimport pandas as pd\n\nfrom .stacker_ensemble_model import StackerEnsembleModel\nfrom .greedy_weighted_ensemble_model import GreedyWeightedEnsembleModel\n\nlogger = logging.getLogger(__name__)\n\n\n# TODO: Optimize predict speed when fit on kfold, can simply sum weights\nclass WeightedEnsembleModel(StackerEnsembleModel):\n    def __init__(self, path: str, name: str, base_model_names, base_model_paths_dict, base_model_types_dict, base_model_types_inner_dict=None, base_model_performances_dict=None, num_classes=None, hyperparameters=None, objective_func=None, stopping_metric=None, save_bagged_folds=True, random_state=0, debug=0, **kwargs):\n        model_0 = base_model_types_dict[base_model_names[0]].load(path=base_model_paths_dict[base_model_names[0]], verbose=False)\n        super().__init__(path=path, name=name, model_base=model_0, base_model_names=base_model_names, base_model_paths_dict=base_model_paths_dict, base_model_types_dict=base_model_types_dict, base_model_types_inner_dict=base_model_types_inner_dict, base_model_performances_dict=base_model_performances_dict, use_orig_features=False, num_classes=num_classes, hyperparameters=hyperparameters, objective_func=objective_func, stopping_metric=stopping_metric, save_bagged_folds=save_bagged_folds, random_state=random_state, debug=debug, **kwargs)\n        child_hyperparameters = kwargs.get('_tmp_greedy_hyperparameters', None)  # TODO: Rework to avoid this hack\n        self.model_base = GreedyWeightedEnsembleModel(path='', name='greedy_ensemble', num_classes=self.num_classes, base_model_names=self.stack_column_prefix_lst, problem_type=self.problem_type, objective_func=self.objective_func, stopping_metric=self.stopping_metric, hyperparameters=child_hyperparameters)\n        self._child_type = type(self.model_base)\n        self.low_memory = False\n\n    def fit(self, X, y, k_fold=5, k_fold_start=0, k_fold_end=None, n_repeats=1, n_repeat_start=0, compute_base_preds=True, time_limit=None, **kwargs):\n        super().fit(X, y, k_fold=k_fold, k_fold_start=k_fold_start, k_fold_end=k_fold_end, n_repeats=n_repeats, n_repeat_start=n_repeat_start, compute_base_preds=compute_base_preds, time_limit=time_limit, **kwargs)\n        stack_columns = []\n        for model in self.models:\n            model = self.load_child(model, verbose=False)\n            stack_columns = stack_columns + [stack_column for stack_column in model.base_model_names if stack_column not in stack_columns]\n        self.stack_column_prefix_lst = [stack_column for stack_column in self.stack_column_prefix_lst if stack_column in stack_columns]\n        self.stack_columns, self.num_pred_cols_per_model = self.set_stack_columns(stack_column_prefix_lst=self.stack_column_prefix_lst)\n        min_stack_column_prefix_to_model_map = {k: v for k, v in self.stack_column_prefix_to_model_map.items() if k in self.stack_column_prefix_lst}\n        self.base_model_names = [base_model_name for base_model_name in self.base_model_names if base_model_name in min_stack_column_prefix_to_model_map.values()]\n        self.stack_column_prefix_to_model_map = min_stack_column_prefix_to_model_map\n\n    def _get_model_weights(self):\n        weights_dict = defaultdict(int)\n        num_models = len(self.models)\n        for model in self.models:\n            model: GreedyWeightedEnsembleModel = self.load_child(model, verbose=False)\n            model_weight_dict = model._get_model_weights()\n            for key in model_weight_dict.keys():\n                weights_dict[key] += model_weight_dict[key]\n        for key in weights_dict:\n            weights_dict[key] = weights_dict[key] / num_models\n        return weights_dict\n\n    def compute_feature_importance(self, X, y, features_to_use=None, preprocess=True, is_oof=True, **kwargs):\n        logger.warning('Warning: non-raw feature importance calculation is not valid for weighted ensemble since it does not have features, returning ensemble weights instead...')\n        if is_oof:\n            feature_importance = pd.Series(self._get_model_weights()).sort_values(ascending=False)\n        else:\n            logger.warning('Warning: Feature importance calculation is not yet implemented for WeightedEnsembleModel on unseen data, returning generic feature importance...')\n            feature_importance = pd.Series(self._get_model_weights()).sort_values(ascending=False)\n            # TODO: Rewrite preprocess() in greedy_weighted_ensemble_model to enable\n            # feature_importance = super().compute_feature_importance(X=X, y=y, features_to_use=features_to_use, preprocess=preprocess, is_oof=is_oof, **kwargs)\n        return feature_importance\n"""
autogluon/utils/tabular/ml/models/knn/__init__.py,0,b''
autogluon/utils/tabular/ml/models/knn/knn_model.py,0,"b""import logging\nimport pickle\nimport sys\nimport time\n\nimport psutil\nfrom sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n\nfrom ..abstract import model_trial\nfrom ..abstract.abstract_model import SKLearnModel\nfrom ...constants import REGRESSION\nfrom ....utils.exceptions import NotEnoughMemoryError\n\nlogger = logging.getLogger(__name__)\n\n\n# TODO: Normalize data!\nclass KNNModel(SKLearnModel):\n    def __init__(self, path: str, name: str, problem_type: str, objective_func, hyperparameters=None, features=None, feature_types_metadata=None, debug=0, **kwargs):\n        super().__init__(path=path, name=name, problem_type=problem_type, objective_func=objective_func, hyperparameters=hyperparameters, features=features, feature_types_metadata=feature_types_metadata, debug=debug, **kwargs)\n        if self.problem_type == REGRESSION:\n            self._model_type = KNeighborsRegressor\n        else:\n            self._model_type = KNeighborsClassifier\n\n    def preprocess(self, X):\n        cat_columns = X.select_dtypes(['category']).columns\n        X = X.drop(cat_columns, axis=1)  # TODO: Test if crash when all columns are categorical\n        X = super().preprocess(X).fillna(0)\n        return X\n\n    def _set_default_params(self):\n        default_params = {\n            'weights': 'uniform',\n            'n_jobs': -1,\n        }\n        for param, val in default_params.items():\n            self._set_default_param_value(param, val)\n\n    # TODO: Enable HPO for KNN\n    def _get_default_searchspace(self):\n        spaces = {}\n        return spaces\n\n    def fit(self, X_train, Y_train, **kwargs):\n        X_train = self.preprocess(X_train)\n        max_memory_usage_ratio = self.params_aux['max_memory_usage_ratio']\n        model_size_bytes = sys.getsizeof(pickle.dumps(X_train, protocol=4))\n        expected_final_model_size_bytes = model_size_bytes * 2.1  # Roughly what can be expected of the final KNN model in memory size\n        if expected_final_model_size_bytes > 10000000:  # Only worth checking if expected model size is >10MB\n            available_mem = psutil.virtual_memory().available\n            model_memory_ratio = expected_final_model_size_bytes / available_mem\n            if model_memory_ratio > (0.35 * max_memory_usage_ratio):\n                logger.warning(f'\\tWarning: Model is expected to require {model_memory_ratio * 100} percent of available memory...')\n            if model_memory_ratio > (0.45 * max_memory_usage_ratio):\n                raise NotEnoughMemoryError  # don't train full model to avoid OOM error\n\n        model = self._model_type(**self.params)\n        self.model = model.fit(X_train, Y_train)\n\n    def hyperparameter_tune(self, X_train, X_test, Y_train, Y_test, scheduler_options=None, **kwargs):\n        fit_model_args = dict(X_train=X_train, Y_train=Y_train, **kwargs)\n        predict_proba_args = dict(X=X_test)\n        model_trial.fit_and_save_model(model=self, params=dict(), fit_args=fit_model_args, predict_proba_args=predict_proba_args, y_test=Y_test, time_start=time.time(), time_limit=None)\n        hpo_results = {'total_time': self.fit_time}\n        hpo_model_performances = {self.name: self.val_score}\n        hpo_models = {self.name: self.path}\n        return hpo_models, hpo_model_performances, hpo_results\n"""
autogluon/utils/tabular/ml/models/lgb/__init__.py,0,b''
autogluon/utils/tabular/ml/models/lgb/callbacks.py,0,"b'import collections, warnings, time, os, psutil, logging\nfrom operator import gt, lt\n\nfrom ....utils.savers import save_pkl, save_pointer\nfrom .....try_import import try_import_lightgbm\n\nlogger = logging.getLogger(__name__)\n\n\ndef save_model_callback(path, latest_model_checkpoint, interval, offset):\n    def _callback(env):\n        if ((env.iteration - offset) % interval == 0) & (env.iteration != 0):\n            save_pkl.save(path=path, object=env.model)\n            save_pointer.save(path=latest_model_checkpoint, content_path=path)\n    _callback.before_iteration = True\n    _callback.order = 0\n    return _callback\n\n\n# TODO: dart might alter previous iterations, check if this is occurring, if so then save snapshot of model when best_iteration to preserve quality\ndef record_evaluation_custom(path, eval_result, interval, offset=0, early_stopping_rounds=None):\n    """"""Create a callback that records the evaluation history into ``eval_result``.\n\n    Parameters\n    ----------\n    eval_result : dict\n       A dictionary to store the evaluation results.\n\n    Returns\n    -------\n    callback : function\n        The callback that records the evaluation history into the passed dictionary.\n    """"""\n    if not isinstance(eval_result, dict):\n        raise TypeError(\'Eval_result should be a dictionary\')\n    eval_result.clear()\n\n    def _init(env):\n        for data_name, _, _, _ in env.evaluation_result_list:\n            eval_result.setdefault(data_name, collections.defaultdict(list))\n\n    def _callback(env):\n        if not eval_result:\n            _init(env)\n        for data_name, eval_name, result, _ in env.evaluation_result_list:\n            eval_result[data_name][eval_name].append(result)\n        if (interval > 0) and ((env.iteration - offset) % interval == 0) and (env.iteration != 0):\n            # min_error = min(eval_result[\'valid_set\'][\'multi_error\'])\n            # print(\'iter:\', env.iteration, \'min_error:\', min_error)\n            save_pkl.save(path=path, object=eval_result)\n    _callback.order = 20\n    return _callback\n\n\n# TODO: Add option to stop if current run\'s metric value is X% lower, such as min 30%, current 40% -> Stop\ndef early_stopping_custom(stopping_rounds, first_metric_only=False, metrics_to_use=None, start_time=None, time_limit=None, verbose=True, max_diff=None, ignore_dart_warning=False, manual_stop_file=None, train_loss_name=None, reporter=None):\n    """"""Create a callback that activates early stopping.\n\n    Note\n    ----\n    Activates early stopping.\n    The model will train until the validation score stops improving.\n    Validation score needs to improve at least every ``early_stopping_rounds`` round(s)\n    to continue training.\n    Requires at least one validation data and one metric.\n    If there\'s more than one, will check all of them. But the training data is ignored anyway.\n    To check only the first metric set ``first_metric_only`` to True.\n\n    Parameters\n    ----------\n    stopping_rounds : int\n       The possible number of rounds without the trend occurrence.\n    first_metric_only : bool, optional (default=False)\n       Whether to use only the first metric for early stopping.\n    verbose : bool, optional (default=True)\n        Whether to print message with early stopping information.\n    train_loss_name : str, optional (default=None):\n        Name of metric that contains training loss value.\n    reporter : optional (default=None):\n        reporter object from AutoGluon scheduler.\n\n    Returns\n    -------\n    callback : function\n        The callback that activates early stopping.\n    """"""\n    best_score = []\n    best_iter = []\n    best_score_list = []\n    best_trainloss = []  # stores training losses at corresponding best_iter\n    cmp_op = []\n    enabled = [True]\n    indices_to_check = []\n    timex = [time.time()]\n    mem_status = psutil.Process()\n    init_mem_rss = []\n    init_mem_avail = []\n\n    def _init(env):\n        if not ignore_dart_warning:\n            enabled[0] = not any((boost_alias in env.params\n                                  and env.params[boost_alias] == \'dart\') for boost_alias in (\'boosting\',\n                                                                                             \'boosting_type\',\n                                                                                             \'boost\'))\n        if not enabled[0]:\n            warnings.warn(\'Early stopping is not available in dart mode\')\n            return\n        if not env.evaluation_result_list:\n            raise ValueError(\'For early stopping, \'\n                             \'at least one dataset and eval metric is required for evaluation\')\n\n        if verbose:\n            msg = ""Training until validation scores don\'t improve for {} rounds.""\n            logger.debug(msg.format(stopping_rounds))\n            if manual_stop_file:\n                logger.debug(\'Manually stop training by creating file at location: \', manual_stop_file)\n\n        for eval_ret in env.evaluation_result_list:\n            best_iter.append(0)\n            best_score_list.append(None)\n            best_trainloss.append(None)\n            if eval_ret[3]:\n                best_score.append(float(\'-inf\'))\n                cmp_op.append(gt)\n            else:\n                best_score.append(float(\'inf\'))\n                cmp_op.append(lt)\n\n        if metrics_to_use is None:\n            for i in range(len(env.evaluation_result_list)):\n                indices_to_check.append(i)\n                if first_metric_only:\n                    break\n        else:\n            for i, eval in enumerate(env.evaluation_result_list):\n                if (eval[0], eval[1]) in metrics_to_use:\n                    indices_to_check.append(i)\n                    if first_metric_only:\n                        break\n\n        init_mem_rss.append(mem_status.memory_info().rss)\n        init_mem_avail.append(psutil.virtual_memory().available)\n\n    def _callback(env):\n        try_import_lightgbm()\n        from lightgbm.callback import _format_eval_result, EarlyStopException\n        if not cmp_op:\n            _init(env)\n        if not enabled[0]:\n            return\n        if train_loss_name is not None:\n            train_loss_evals = [eval for eval in env.evaluation_result_list if eval[0] == \'train_set\' and eval[1] == train_loss_name]\n            train_loss_val = train_loss_evals[0][2]\n        else:\n            train_loss_val = 0.0\n        for i in indices_to_check:\n            score = env.evaluation_result_list[i][2]\n            if best_score_list[i] is None or cmp_op[i](score, best_score[i]):\n                best_score[i] = score\n                best_iter[i] = env.iteration\n                best_score_list[i] = env.evaluation_result_list\n                best_trainloss[i] = train_loss_val\n            if reporter is not None:  # Report current best scores for iteration, used in HPO\n                if i == indices_to_check[0]:  # TODO: documentation needs to note that we assume 0th index is the \'official\' validation performance metric.\n                    if cmp_op[i] == gt:\n                        validation_perf = score\n                    else:\n                        validation_perf = -score\n                    reporter(epoch=env.iteration + 1,\n                             validation_performance=validation_perf,\n                             train_loss=best_trainloss[i],\n                             best_iter_sofar=best_iter[i] + 1,\n                             best_valperf_sofar=best_score[i])\n            if env.iteration - best_iter[i] >= stopping_rounds:\n                if verbose:\n                    logger.log(15, \'Early stopping, best iteration is:\\n[%d]\\t%s\' % (\n                        best_iter[i] + 1, \'\\t\'.join([_format_eval_result(x) for x in best_score_list[i]])))\n                raise EarlyStopException(best_iter[i], best_score_list[i])\n            elif (max_diff is not None) and (abs(score - best_score[i]) > max_diff):\n                if verbose:\n                    logger.debug(\'max_diff breached!\')\n                    logger.debug(abs(score - best_score[i]))\n                    logger.log(15, \'Early stopping, best iteration is:\\n[%d]\\t%s\' % (\n                        best_iter[i] + 1, \'\\t\'.join([_format_eval_result(x) for x in best_score_list[i]])))\n                raise EarlyStopException(best_iter[i], best_score_list[i])\n            if env.iteration == env.end_iteration - 1:\n                if verbose:\n                    logger.log(15, \'Did not meet early stopping criterion. Best iteration is:\\n[%d]\\t%s\' % (\n                        best_iter[i] + 1, \'\\t\'.join([_format_eval_result(x) for x in best_score_list[i]])))\n                raise EarlyStopException(best_iter[i], best_score_list[i])\n            if verbose:\n                logger.debug((env.iteration - best_iter[i], env.evaluation_result_list[i]))\n        if manual_stop_file:\n            if os.path.exists(manual_stop_file):\n                i = indices_to_check[0]\n                logger.log(20, \'Found manual stop file, early stopping. Best iteration is:\\n[%d]\\t%s\' % (\n                    best_iter[i] + 1, \'\\t\'.join([_format_eval_result(x) for x in best_score_list[i]])))\n                raise EarlyStopException(best_iter[i], best_score_list[i])\n        if time_limit:\n            time_elapsed = time.time() - start_time\n            time_left = time_limit - time_elapsed\n            if time_left <= 0:\n                i = indices_to_check[0]\n                logger.log(20, \'\\tRan out of time, early stopping on iteration \' + str(env.iteration+1) + \'. Best iteration is:\\n\\t[%d]\\t%s\' % (\n                    best_iter[i] + 1, \'\\t\'.join([_format_eval_result(x) for x in best_score_list[i]])))\n                raise EarlyStopException(best_iter[i], best_score_list[i])\n\n        # TODO: Add toggle parameter to early_stopping to disable this\n        # TODO: Identify optimal threshold values for early_stopping based on lack of memory\n        if env.iteration % 10 == 0:\n            available = psutil.virtual_memory().available\n            cur_rss = mem_status.memory_info().rss\n\n            if cur_rss < init_mem_rss[0]:\n                init_mem_rss[0] = cur_rss\n            estimated_model_size_mb = (cur_rss - init_mem_rss[0]) >> 20\n            available_mb = available >> 20\n\n            model_size_memory_ratio = estimated_model_size_mb / available_mb\n            if verbose or (model_size_memory_ratio > 0.25):\n                logging.debug(\'Available Memory: \'+str(available_mb)+\' MB\')\n                logging.debug(\'Estimated Model Size: \'+str(estimated_model_size_mb)+\' MB\')\n\n            early_stop = False\n            if model_size_memory_ratio > 1.0:\n                logger.warning(\'Warning: Large GBM model size may cause OOM error if training continues\')\n                logger.warning(\'Available Memory: \'+str(available_mb)+\' MB\')\n                logger.warning(\'Estimated GBM model size: \'+str(estimated_model_size_mb)+\' MB\')\n                early_stop = True\n\n            # TODO: We will want to track size of model as well, even if we early stop before OOM, we will still crash when saving if the model is large enough\n            if available_mb < 512:  # Less than 500 MB\n                logger.warning(\'Warning: Low available memory may cause OOM error if training continues\')\n                logger.warning(\'Available Memory: \'+str(available_mb)+\' MB\')\n                logger.warning(\'Estimated GBM model size: \'+str(estimated_model_size_mb)+\' MB\')\n                early_stop = True\n\n            if early_stop:\n                logger.warning(\'Warning: Early stopped GBM model prior to optimal result to avoid OOM error. Please increase available memory to avoid subpar model quality.\')\n                logger.log(15, \'Early stopping, best iteration is:\\n[%d]\\t%s\' % (\n                        best_iter[0] + 1, \'\\t\'.join([_format_eval_result(x) for x in best_score_list[0]])))\n                raise EarlyStopException(best_iter[0], best_score_list[0])\n\n    _callback.order = 30\n    return _callback\n'"
autogluon/utils/tabular/ml/models/lgb/lgb_model.py,0,"b'import gc\nimport logging\nimport os\nimport random\nimport re\nimport time\nimport warnings\n\nimport numpy as np\nimport pandas as pd\nfrom pandas import DataFrame, Series\n\nfrom . import lgb_utils\nfrom .callbacks import early_stopping_custom\nfrom .hyperparameters.lgb_trial import lgb_trial\nfrom .hyperparameters.parameters import get_param_baseline\nfrom .hyperparameters.searchspaces import get_default_searchspace\nfrom .lgb_utils import construct_dataset\nfrom ..abstract.abstract_model import AbstractModel, fixedvals_from_searchspaces\nfrom ...constants import BINARY, MULTICLASS, REGRESSION\nfrom ....utils.savers import save_pkl\nfrom .....try_import import try_import_lightgbm\nfrom ......core import Int, Space\n\nwarnings.filterwarnings(""ignore"", category=UserWarning, message=""Starting from version"")  # lightGBM brew libomp warning\nlogger = logging.getLogger(__name__)\n\n\n# TODO: Save dataset to binary and reload for HPO. This will avoid the memory spike overhead when training each model and instead it will only occur once upon saving the dataset.\nclass LGBModel(AbstractModel):\n    def __init__(self, path: str, name: str, problem_type: str, objective_func, stopping_metric=None, num_classes=None, hyperparameters=None, features=None, debug=0, **kwargs):\n        super().__init__(path=path, name=name, problem_type=problem_type, objective_func=objective_func, stopping_metric=stopping_metric, num_classes=num_classes, hyperparameters=hyperparameters, features=features, debug=debug, **kwargs)\n\n        self.eval_metric_name = self.stopping_metric.name\n        self.is_higher_better = True\n        self._internal_feature_map = None\n\n    def _set_default_params(self):\n        default_params = get_param_baseline(problem_type=self.problem_type, num_classes=self.num_classes)\n        for param, val in default_params.items():\n            self._set_default_param_value(param, val)\n\n    def _get_default_searchspace(self):\n        return get_default_searchspace(problem_type=self.problem_type, num_classes=self.num_classes)\n\n    def get_eval_metric(self):\n        return lgb_utils.func_generator(metric=self.stopping_metric, is_higher_better=True, needs_pred_proba=not self.stopping_metric_needs_y_pred, problem_type=self.problem_type)\n\n    def fit(self, X_train=None, Y_train=None, X_test=None, Y_test=None, dataset_train=None, dataset_val=None, time_limit=None, **kwargs):\n        start_time = time.time()\n        params = self.params.copy()\n\n        # TODO: kwargs can have num_cpu, num_gpu. Currently these are ignored.\n        verbosity = kwargs.get(\'verbosity\', 2)\n        params = fixedvals_from_searchspaces(params)\n\n        if verbosity <= 1:\n            verbose_eval = False\n        elif verbosity == 2:\n            verbose_eval = 1000\n        elif verbosity == 3:\n            verbose_eval = 50\n        else:\n            verbose_eval = 1\n\n        eval_metric = self.get_eval_metric()\n        dataset_train, dataset_val = self.generate_datasets(X_train=X_train, Y_train=Y_train, params=params, X_test=X_test, Y_test=Y_test, dataset_train=dataset_train, dataset_val=dataset_val)\n        gc.collect()\n\n        num_boost_round = params.pop(\'num_boost_round\', 1000)\n        logger.log(15, f\'Training Gradient Boosting Model for {num_boost_round} rounds...\')\n        logger.log(15, ""with the following hyperparameter settings:"")\n        logger.log(15, params)\n\n        num_rows_train = len(dataset_train.data)\n        if \'min_data_in_leaf\' in params:\n            if params[\'min_data_in_leaf\'] > num_rows_train:  # TODO: may not be necessary\n                params[\'min_data_in_leaf\'] = max(1, int(num_rows_train / 5.0))\n\n        # TODO: Better solution: Track trend to early stop when score is far worse than best score, or score is trending worse over time\n        if (dataset_val is not None) and (dataset_train is not None):\n            modifier = 1 if num_rows_train <= 10000 else 10000 / num_rows_train\n            early_stopping_rounds = max(round(modifier * 150), 10)\n        else:\n            early_stopping_rounds = 150\n\n        callbacks = []\n        valid_names = [\'train_set\']\n        valid_sets = [dataset_train]\n        if dataset_val is not None:\n            reporter = kwargs.get(\'reporter\', None)\n            train_loss_name = self._get_train_loss_name() if reporter is not None else None\n            callbacks += [\n                # Note: Don\'t use self.params_aux[\'max_memory_usage_ratio\'] here as LightGBM handles memory per iteration optimally.  # TODO: Consider using when ratio < 1.\n                early_stopping_custom(early_stopping_rounds, metrics_to_use=[(\'valid_set\', self.eval_metric_name)], max_diff=None, start_time=start_time, time_limit=time_limit,\n                                      ignore_dart_warning=True, verbose=False, manual_stop_file=False, reporter=reporter, train_loss_name=train_loss_name),\n            ]\n            valid_names = [\'valid_set\'] + valid_names\n            valid_sets = [dataset_val] + valid_sets\n\n        seed_val = params.pop(\'seed_value\', 0)\n        train_params = {\n            \'params\': params,\n            \'train_set\': dataset_train,\n            \'num_boost_round\': num_boost_round,\n            \'valid_sets\': valid_sets,\n            \'valid_names\': valid_names,\n            \'callbacks\': callbacks,\n            \'verbose_eval\': verbose_eval,\n        }\n        if not isinstance(eval_metric, str):\n            train_params[\'feval\'] = eval_metric\n        if seed_val is not None:\n            train_params[\'params\'][\'seed\'] = seed_val\n            random.seed(seed_val)\n            np.random.seed(seed_val)\n\n        # Train LightGBM model:\n        try_import_lightgbm()\n        import lightgbm as lgb\n        self.model = lgb.train(**train_params)\n        self.params_trained[\'num_boost_round\'] = self.model.best_iteration\n\n    def predict_proba(self, X, preprocess=True):\n        if preprocess:\n            X = self.preprocess(X)\n        if self.problem_type == REGRESSION:\n            return self.model.predict(X)\n\n        y_pred_proba = self.model.predict(X)\n        if self.problem_type == BINARY:\n            if len(y_pred_proba.shape) == 1:\n                return y_pred_proba\n            elif y_pred_proba.shape[1] > 1:\n                return y_pred_proba[:, 1]\n            else:\n                return y_pred_proba\n        elif self.problem_type == MULTICLASS:\n            return y_pred_proba\n        else:\n            if len(y_pred_proba.shape) == 1:\n                return y_pred_proba\n            elif y_pred_proba.shape[1] > 2:  # Should this ever happen?\n                return y_pred_proba\n            else:  # Should this ever happen?\n                return y_pred_proba[:, 1]\n\n    def preprocess(self, X, is_train=False):\n        X = super().preprocess(X=X)\n\n        if is_train:\n            for column in X.columns:\n                new_column = re.sub(r\'["",:{}[\\]]\', \'\', column)\n                if new_column != column:\n                    self._internal_feature_map = {feature: i for i, feature in enumerate(list(X.columns))}\n                    break\n\n        if self._internal_feature_map:\n            new_columns = [self._internal_feature_map[column] for column in list(X.columns)]\n            X_new = X.copy(deep=False)\n            X_new.columns = new_columns\n            return X_new\n        else:\n            return X\n\n    def generate_datasets(self, X_train: DataFrame, Y_train: Series, params, X_test=None, Y_test=None, dataset_train=None, dataset_val=None, save=False):\n        lgb_dataset_params_keys = [\'objective\', \'two_round\', \'num_threads\', \'num_classes\', \'verbose\']  # Keys that are specific to lightGBM Dataset object construction.\n        data_params = {key: params[key] for key in lgb_dataset_params_keys if key in params}.copy()\n\n        W_train = None  # TODO: Add weight support\n        W_test = None  # TODO: Add weight support\n        if X_train is not None:\n            X_train = self.preprocess(X_train, is_train=True)\n        if X_test is not None:\n            X_test = self.preprocess(X_test)\n        # TODO: Try creating multiple Datasets for subsets of features, then combining with Dataset.add_features_from(), this might avoid memory spike\n        if not dataset_train:\n            # X_train, W_train = self.convert_to_weight(X=X_train)\n            dataset_train = construct_dataset(x=X_train, y=Y_train, location=f\'{self.path}datasets{os.path.sep}train\', params=data_params, save=save, weight=W_train)\n            # dataset_train = construct_dataset_lowest_memory(X=X_train, y=Y_train, location=self.path + \'datasets/train\', params=data_params)\n        if (not dataset_val) and (X_test is not None) and (Y_test is not None):\n            # X_test, W_test = self.convert_to_weight(X=X_test)\n            dataset_val = construct_dataset(x=X_test, y=Y_test, location=f\'{self.path}datasets{os.path.sep}val\', reference=dataset_train, params=data_params, save=save, weight=W_test)\n            # dataset_val = construct_dataset_lowest_memory(X=X_test, y=Y_test, location=self.path + \'datasets/val\', reference=dataset_train, params=data_params)\n        return dataset_train, dataset_val\n\n    def debug_features_to_use(self, X_test_in):\n        feature_splits = self.model.feature_importance()\n        total_splits = feature_splits.sum()\n        feature_names = list(X_test_in.columns.values)\n        feature_count = len(feature_names)\n        feature_importances = pd.DataFrame(data=feature_names, columns=[\'feature\'])\n        feature_importances[\'splits\'] = feature_splits\n        feature_importances_unused = feature_importances[feature_importances[\'splits\'] == 0]\n        feature_importances_used = feature_importances[feature_importances[\'splits\'] >= (total_splits / feature_count)]\n        logger.debug(feature_importances_unused)\n        logger.debug(feature_importances_used)\n        logger.debug(f\'feature_importances_unused: {len(feature_importances_unused)}\')\n        logger.debug(f\'feature_importances_used: {len(feature_importances_used)}\')\n        features_to_use = list(feature_importances_used[\'feature\'].values)\n        logger.debug(str(features_to_use))\n        return features_to_use\n\n    # FIXME: Requires major refactor + refactor lgb_trial.py\n    #  model names are not aligned with what is communicated to trainer!\n    # FIXME: Likely tabular_nn_trial.py and abstract trial also need to be refactored heavily + hyperparameter functions\n    def hyperparameter_tune(self, X_train, X_test, Y_train, Y_test, scheduler_options, **kwargs):\n        time_start = time.time()\n        logger.log(15, ""Beginning hyperparameter tuning for Gradient Boosting Model..."")\n        self._set_default_searchspace()\n        params_copy = self.params.copy()\n        if isinstance(params_copy[\'min_data_in_leaf\'], Int):\n            upper_minleaf = params_copy[\'min_data_in_leaf\'].upper\n            if upper_minleaf > X_train.shape[0]:  # TODO: this min_data_in_leaf adjustment based on sample size may not be necessary\n                upper_minleaf = max(1, int(X_train.shape[0] / 5.0))\n                lower_minleaf = params_copy[\'min_data_in_leaf\'].lower\n                if lower_minleaf > upper_minleaf:\n                    lower_minleaf = max(1, int(upper_minleaf / 3.0))\n                params_copy[\'min_data_in_leaf\'] = Int(lower=lower_minleaf, upper=upper_minleaf)\n\n        directory = self.path  # also create model directory if it doesn\'t exist\n        # TODO: This will break on S3! Use tabular/utils/savers for datasets, add new function\n        os.makedirs(directory, exist_ok=True)\n        scheduler_func, scheduler_options = scheduler_options  # Unpack tuple\n        if scheduler_func is None or scheduler_options is None:\n            raise ValueError(""scheduler_func and scheduler_options cannot be None for hyperparameter tuning"")\n        num_threads = scheduler_options[\'resource\'].get(\'num_cpus\', -1)\n        params_copy[\'num_threads\'] = num_threads\n        # num_gpus = scheduler_options[\'resource\'][\'num_gpus\'] # TODO: unused\n\n        dataset_train, dataset_val = self.generate_datasets(X_train=X_train, Y_train=Y_train, params=params_copy, X_test=X_test, Y_test=Y_test)\n        dataset_train_filename = ""dataset_train.bin""\n        train_file = self.path + dataset_train_filename\n        if os.path.exists(train_file):  # clean up old files first\n            os.remove(train_file)\n        dataset_train.save_binary(train_file)\n        dataset_val_filename = ""dataset_val.bin""  # names without directory info\n        val_file = self.path + dataset_val_filename\n        if os.path.exists(val_file):  # clean up old files first\n            os.remove(val_file)\n        dataset_val.save_binary(val_file)\n        dataset_val_pkl_filename = \'dataset_val.pkl\'\n        val_pkl_path = directory + dataset_val_pkl_filename\n        save_pkl.save(path=val_pkl_path, object=(X_test, Y_test))\n\n        if not np.any([isinstance(params_copy[hyperparam], Space) for hyperparam in params_copy]):\n            logger.warning(""Attempting to do hyperparameter optimization without any search space (all hyperparameters are already fixed values)"")\n        else:\n            logger.log(15, ""Hyperparameter search space for Gradient Boosting Model: "")\n            for hyperparam in params_copy:\n                if isinstance(params_copy[hyperparam], Space):\n                    logger.log(15, f\'{hyperparam}:   {params_copy[hyperparam]}\')\n\n        util_args = dict(\n            dataset_train_filename=dataset_train_filename,\n            dataset_val_filename=dataset_val_filename,\n            dataset_val_pkl_filename=dataset_val_pkl_filename,\n            directory=directory,\n            model=self,\n            time_start=time_start,\n            time_limit=scheduler_options[\'time_out\']\n        )\n        lgb_trial.register_args(util_args=util_args, **params_copy)\n        scheduler = scheduler_func(lgb_trial, **scheduler_options)\n        if (\'dist_ip_addrs\' in scheduler_options) and (len(scheduler_options[\'dist_ip_addrs\']) > 0):\n            # This is multi-machine setting, so need to copy dataset to workers:\n            logger.log(15, ""Uploading data to remote workers..."")\n            scheduler.upload_files([train_file, val_file, val_pkl_path])  # TODO: currently does not work.\n            directory = self.path  # TODO: need to change to path to working directory used on every remote machine\n            lgb_trial.update(directory=directory)\n            logger.log(15, ""uploaded"")\n\n        scheduler.run()\n        scheduler.join_jobs()\n\n        return self._get_hpo_results(scheduler=scheduler, scheduler_options=scheduler_options, time_start=time_start)\n\n    # TODO: Consider adding _internal_feature_map functionality to abstract_model\n    def compute_feature_importance(self, **kwargs):\n        permutation_importance = super().compute_feature_importance(**kwargs)\n        if self._internal_feature_map is not None:\n            inverse_internal_feature_map = {i: feature for feature, i in self._internal_feature_map.items()}\n            permutation_importance = {inverse_internal_feature_map[i]: importance for i, importance in permutation_importance.items()}\n        return permutation_importance\n\n    def _get_train_loss_name(self):\n        if self.problem_type == BINARY:\n            train_loss_name = \'binary_logloss\'\n        elif self.problem_type == MULTICLASS:\n            train_loss_name = \'multi_logloss\'\n        elif self.problem_type == REGRESSION:\n            train_loss_name = \'l2\'\n        else:\n            raise ValueError(f""unknown problem_type for LGBModel: {self.problem_type}"")\n        return train_loss_name\n\n    def get_model_feature_importance(self, use_original_feature_names=False):\n        feature_names = self.model.feature_name()\n        importances = self.model.feature_importance()\n        importance_dict = {feature_name: importance for (feature_name, importance) in zip(feature_names, importances)}\n        if use_original_feature_names and (self._internal_feature_map is not None):\n            inverse_internal_feature_map = {i: feature for feature, i in self._internal_feature_map.items()}\n            importance_dict = {inverse_internal_feature_map[i]: importance for i, importance in importance_dict.items()}\n        return importance_dict\n'"
autogluon/utils/tabular/ml/models/lgb/lgb_utils.py,0,"b""import os\n\nimport numpy as np\nfrom pandas import DataFrame, Series\n\nfrom autogluon import try_import_lightgbm\nfrom ...constants import MULTICLASS\n\n\ndef func_generator(metric, is_higher_better, needs_pred_proba, problem_type):\n    if needs_pred_proba:\n        if problem_type == MULTICLASS:\n            def function_template(y_hat, data):\n                y_true = data.get_label()\n                y_hat = y_hat.reshape(len(np.unique(y_true)), -1).T\n                return metric.name, metric(y_true, y_hat), is_higher_better\n        else:\n            def function_template(y_hat, data):\n                y_true = data.get_label()\n                return metric.name, metric(y_true, y_hat), is_higher_better\n    else:\n        if problem_type == MULTICLASS:\n            def function_template(y_hat, data):\n                y_true = data.get_label()\n                y_hat = y_hat.reshape(len(np.unique(y_true)), -1)\n                y_hat = y_hat.argmax(axis=0)\n                return metric.name, metric(y_true, y_hat), is_higher_better\n        else:\n            def function_template(y_hat, data):\n                y_true = data.get_label()\n                y_hat = np.round(y_hat)\n                return metric.name, metric(y_true, y_hat), is_higher_better\n    return function_template\n\n\ndef construct_dataset(x: DataFrame, y: Series, location=None, reference=None, params=None, save=False, weight=None):\n    try_import_lightgbm()\n    import lightgbm as lgb\n\n    dataset = lgb.Dataset(data=x, label=y, reference=reference, free_raw_data=True, params=params, weight=weight)\n\n    if save:\n        assert location is not None\n        saving_path = f'{location}.bin'\n        if os.path.exists(saving_path):\n            os.remove(saving_path)\n\n        os.makedirs(os.path.dirname(saving_path), exist_ok=True)\n        dataset.save_binary(saving_path)\n        # dataset_binary = lgb.Dataset(location + '.bin', reference=reference, free_raw_data=False)# .construct()\n\n    return dataset\n"""
autogluon/utils/tabular/ml/models/lr/__init__.py,0,b''
autogluon/utils/tabular/ml/models/lr/lr_model.py,0,"b'import logging\nimport re\n\nimport numpy as np\nfrom pandas import DataFrame\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import FeatureUnion, Pipeline\nfrom sklearn.preprocessing import StandardScaler, QuantileTransformer\n\nfrom .hyperparameters.parameters import get_param_baseline, get_model_params, get_default_params, INCLUDE, IGNORE, ONLY\nfrom .hyperparameters.searchspaces import get_default_searchspace\nfrom .lr_preprocessing_utils import NlpDataPreprocessor, OheFeaturesGenerator, NumericDataPreprocessor\nfrom ...constants import BINARY, REGRESSION\nfrom ....ml.models.abstract.abstract_model import AbstractModel\n\nlogger = logging.getLogger(__name__)\n\n\nclass LinearModel(AbstractModel):\n\n    def __init__(self, path: str, name: str, problem_type: str, objective_func, hyperparameters=None, features=None, feature_types_metadata=None, debug=0, **kwargs):\n        self.model_class, self.penalty, self.handle_text = get_model_params(problem_type, hyperparameters)\n        super().__init__(path=path, name=name, problem_type=problem_type, objective_func=objective_func, hyperparameters=hyperparameters, features=features, feature_types_metadata=feature_types_metadata, debug=debug, **kwargs)\n\n        self.types_of_features = None\n        self.pipeline = None\n\n        self.model_params, default_params = get_default_params(self.problem_type, self.penalty)\n        for param, val in default_params.items():\n            self._set_default_param_value(param, val)\n\n    def tokenize(self, s):\n        return re.split(\'[ ]+\', s)\n\n    def _get_types_of_features(self, df):\n        """""" Returns dict with keys: : \'continuous\', \'skewed\', \'onehot\', \'embed\', \'language\', values = ordered list of feature-names falling into each category.\n            Each value is a list of feature-names corresponding to columns in original dataframe.\n            TODO: ensure features with zero variance have already been removed before this function is called.\n        """"""\n        if self.types_of_features is not None:\n            logger.warning(""Attempting to _get_types_of_features for LRModel, but previously already did this."")\n        categorical_featnames = self.__get_feature_type_if_present(\'object\') + self.__get_feature_type_if_present(\'bool\')\n        continuous_featnames = self.__get_feature_type_if_present(\'float\') + self.__get_feature_type_if_present(\'int\') + self.__get_feature_type_if_present(\n            \'datetime\')\n        language_featnames = self.feature_types_metadata[\'nlp\']\n        valid_features = categorical_featnames + continuous_featnames + language_featnames\n        if len(categorical_featnames) + len(continuous_featnames) + len(language_featnames) != df.shape[1]:\n            unknown_features = [feature for feature in df.columns if feature not in valid_features]\n            df = df.drop(columns=unknown_features)\n        self.features = list(df.columns)\n\n        types_of_features = {\'continuous\': [], \'skewed\': [], \'onehot\': [], \'language\': []}\n        return self._select_features(df, types_of_features, categorical_featnames, language_featnames, continuous_featnames)\n\n    def _select_features(self, df, types_of_features, categorical_featnames, language_featnames, continuous_featnames):\n        features_seclector = {\n            INCLUDE: self._select_features_handle_text_include,\n            ONLY: self._select_features_handle_text_only,\n            IGNORE: self._select_features_handle_text_ignore,\n        }.get(self.handle_text, self._select_features_handle_text_ignore)\n        return features_seclector(df, types_of_features, categorical_featnames, language_featnames, continuous_featnames)\n\n    def __get_feature_type_if_present(self, feature_type):\n        """""" Returns crude categorization of feature types """"""\n        return self.feature_types_metadata[feature_type] if feature_type in self.feature_types_metadata else []\n\n    # TODO: handle collinear features - they will impact results quality\n    def preprocess(self, X: DataFrame, is_train=False, vect_max_features=1000, model_specific_preprocessing=False):\n        if model_specific_preprocessing:  # This is hack to work-around pre-processing caching in bagging/stacker models\n            X = X.copy()\n            if is_train:\n                feature_types = self._get_types_of_features(X)\n                self.preprocess_train(X, feature_types, vect_max_features)\n            X = self.pipeline.transform(X)\n\n        return X\n\n    def preprocess_train(self, X, feature_types, vect_max_features):\n        transformer_list = []\n        if len(feature_types[\'language\']) > 0:\n            pipeline = Pipeline(steps=[\n                (""preparator"", NlpDataPreprocessor(nlp_cols=feature_types[\'language\'])),\n                (""vectorizer"",\n                 TfidfVectorizer(ngram_range=self.params[\'proc.ngram_range\'], sublinear_tf=True, max_features=vect_max_features, tokenizer=self.tokenize))\n            ])\n            transformer_list.append((\'vect\', pipeline))\n        if len(feature_types[\'onehot\']) > 0:\n            pipeline = Pipeline(steps=[\n                (\'generator\', OheFeaturesGenerator(cats_cols=feature_types[\'onehot\'])),\n            ])\n            transformer_list.append((\'cats\', pipeline))\n        if len(feature_types[\'continuous\']) > 0:\n            pipeline = Pipeline(steps=[\n                (\'generator\', NumericDataPreprocessor(cont_cols=feature_types[\'continuous\'])),\n                (\'imputer\', SimpleImputer(strategy=self.params[\'proc.impute_strategy\'])),\n                (\'scaler\', StandardScaler())\n            ])\n            transformer_list.append((\'cont\', pipeline))\n        if len(feature_types[\'skewed\']) > 0:\n            pipeline = Pipeline(steps=[\n                (\'generator\', NumericDataPreprocessor(cont_cols=feature_types[\'skewed\'])),\n                (\'imputer\', SimpleImputer(strategy=self.params[\'proc.impute_strategy\'])),\n                (\'quantile\', QuantileTransformer(output_distribution=\'normal\')),  # Or output_distribution = \'uniform\'\n            ])\n            transformer_list.append((\'skew\', pipeline))\n        self.pipeline = FeatureUnion(transformer_list=transformer_list)\n        self.pipeline.fit(X)\n\n    def _set_default_params(self):\n        for param, val in get_param_baseline().items():\n            self._set_default_param_value(param, val)\n\n    def _get_default_searchspace(self, problem_type):\n        return get_default_searchspace(problem_type)\n\n    # TODO: It could be possible to adaptively set max_iter [1] to approximately respect time_limit based on sample-size, feature-dimensionality, and the solver used.\n    #  [1] https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#examples-using-sklearn-linear-model-logisticregression\n    def fit(self, X_train, Y_train, X_test=None, Y_test=None, time_limit=None, **kwargs):\n        hyperparams = self.params.copy()\n\n        if self.problem_type == BINARY:\n            Y_train = Y_train.astype(int).values\n\n        X_train = self.preprocess(X_train, is_train=True, vect_max_features=hyperparams[\'vectorizer_dict_size\'], model_specific_preprocessing=True)\n\n        params = {k: v for k, v in self.params.items() if k in self.model_params}\n\n        # Ridge/Lasso are using alpha instead of C, which is C^-1\n        # https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge\n        if self.problem_type == REGRESSION:\n            # For numerical reasons, using alpha = 0 with the Lasso object is not advised, so we add epsilon\n            params[\'alpha\'] = 1 / (params[\'C\'] if params[\'C\'] != 0 else 1e-8)\n            params.pop(\'C\', None)\n\n        model = self.model_class(**params)\n\n        logger.log(15, f\'Training Model with the following hyperparameter settings:\')\n        logger.log(15, model)\n\n        self.model = model.fit(X_train, Y_train)\n\n    def predict_proba(self, X, preprocess=True):\n        X = self.preprocess(X, is_train=False, model_specific_preprocessing=True)\n        return super().predict_proba(X, preprocess=False)\n\n    def hyperparameter_tune(self, X_train, X_test, Y_train, Y_test, scheduler_options=None, **kwargs):\n        self.fit(X_train=X_train, X_test=X_test, Y_train=Y_train, Y_test=Y_test, **kwargs)\n        hpo_model_performances = {self.name: self.score(X_test, Y_test)}\n        hpo_results = {}\n        self.save()\n        hpo_models = {self.name: self.path}\n\n        return hpo_models, hpo_model_performances, hpo_results\n\n    def get_info(self):\n        # TODO: All AG-Tabular models now offer a get_info method:\n        # https://github.com/awslabs/autogluon/blob/master/autogluon/utils/tabular/ml/models/abstract/abstract_model.py#L474\n        # dict of weights?\n        return super().get_info()\n\n    def _select_features_handle_text_include(self, df, types_of_features, categorical_featnames, language_featnames, continuous_featnames):\n        # continuous = numeric features to rescale\n        # skewed = features to which we will apply power (ie. log / box-cox) transform before normalization\n        # onehot = features to one-hot encode (unknown categories for these features encountered at test-time are encoded as all zeros). We one-hot encode any features encountered that only have two unique values.\n        one_hot_threshold = 10000  # FIXME research memory constraints\n        for feature in self.features:\n            feature_data = df[feature]\n            num_unique_vals = len(feature_data.unique())\n            if feature in language_featnames:\n                types_of_features[\'language\'].append(feature)\n            elif feature in continuous_featnames:\n                if np.abs(feature_data.skew()) > self.params[\'proc.skew_threshold\']:\n                    types_of_features[\'skewed\'].append(feature)\n                else:\n                    types_of_features[\'continuous\'].append(feature)\n            elif (feature in categorical_featnames) and (num_unique_vals <= one_hot_threshold):\n                types_of_features[\'onehot\'].append(feature)\n        return types_of_features\n\n    def _select_features_handle_text_only(self, df, types_of_features, categorical_featnames, language_featnames, continuous_featnames):\n        for feature in self.features:\n            if feature in language_featnames:\n                types_of_features[\'language\'].append(feature)\n        return types_of_features\n\n    def _select_features_handle_text_ignore(self, df, types_of_features, categorical_featnames, language_featnames, continuous_featnames):\n        # continuous = numeric features to rescale\n        # skewed = features to which we will apply power (ie. log / box-cox) transform before normalization\n        # onehot = features to one-hot encode (unknown categories for these features encountered at test-time are encoded as all zeros). We one-hot encode any features encountered that only have two unique values.\n        one_hot_threshold = 10000  # FIXME research memory constraints\n        for feature in self.features:\n            feature_data = df[feature]\n            num_unique_vals = len(feature_data.unique())\n            if feature in continuous_featnames:\n                if \'__nlp__\' in feature:\n                    continue\n                if np.abs(feature_data.skew()) > self.params[\'proc.skew_threshold\']:\n                    types_of_features[\'skewed\'].append(feature)\n                else:\n                    types_of_features[\'continuous\'].append(feature)\n            elif (feature in categorical_featnames) and (num_unique_vals <= one_hot_threshold):\n                types_of_features[\'onehot\'].append(feature)\n        return types_of_features\n'"
autogluon/utils/tabular/ml/models/lr/lr_preprocessing_utils.py,0,"b""from scipy.sparse import hstack\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.preprocessing import OneHotEncoder\n\n\nclass OheFeaturesGenerator(BaseEstimator, TransformerMixin):\n    missing_category_str = '!missing!'\n\n    def __init__(self, cats_cols):\n        self._feature_names = []\n        self.cats = cats_cols\n        self.ohe_encs = None\n        self.labels = None\n\n    def fit(self, X, y=None):\n        self.ohe_encs = {f: OneHotEncoder(handle_unknown='ignore') for f in self.cats}\n        self.labels = {}\n\n        for c in self.cats:\n            self.ohe_encs[c].fit(self._normalize(X[c]))\n            self.labels[c] = self.ohe_encs[c].categories_\n        return self\n\n    def transform(self, X, y=None):\n        Xs = [self.ohe_encs[c].transform(self._normalize(X[c])) for c in self.cats]\n\n        # Update feature names\n        self._feature_names = []\n        for k, v in self.labels.items():\n            for f in k + '_' + v[0]:\n                self._feature_names.append(f)\n\n        return hstack(Xs)\n\n    def _normalize(self, col):\n        return col.astype(str).fillna(self.missing_category_str).values.reshape(-1, 1)\n\n    def get_feature_names(self):\n        return self._feature_names\n\n\nclass NlpDataPreprocessor(BaseEstimator, TransformerMixin):\n\n    def __init__(self, nlp_cols):\n        self.nlp_cols = nlp_cols\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X, y=None):\n        X = X[self.nlp_cols].copy()\n        for c in self.nlp_cols:\n            X[c] = X[c].astype(str).fillna(' ')\n        X = X.apply(' '.join, axis=1).str.replace('[ ]+', ' ', regex=True)\n        return X.values.tolist()\n\n\nclass NumericDataPreprocessor(BaseEstimator, TransformerMixin):\n\n    def __init__(self, cont_cols):\n        self.cont_cols = cont_cols\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X, y=None):\n        X = X[self.cont_cols].copy()\n        return X.values.tolist()\n"""
autogluon/utils/tabular/ml/models/rf/__init__.py,0,b''
autogluon/utils/tabular/ml/models/rf/rf_model.py,0,"b""import logging\nimport math\nimport pickle\nimport sys\nimport time\n\nimport psutil\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, ExtraTreesClassifier, ExtraTreesRegressor\n\nfrom ..abstract import model_trial\nfrom ..abstract.abstract_model import SKLearnModel\nfrom ...constants import MULTICLASS, REGRESSION\nfrom ....utils.exceptions import NotEnoughMemoryError, TimeLimitExceeded\n\nlogger = logging.getLogger(__name__)\n\n\nclass RFModel(SKLearnModel):\n    def __init__(self, path: str, name: str, problem_type: str, objective_func, num_classes=None, hyperparameters=None, features=None, feature_types_metadata=None, debug=0, **kwargs):\n        super().__init__(path=path, name=name, problem_type=problem_type, objective_func=objective_func, num_classes=num_classes, hyperparameters=hyperparameters, features=features, feature_types_metadata=feature_types_metadata, debug=debug, **kwargs)\n        self._model_type = self._get_model_type()\n\n    def _get_model_type(self):\n        if self.problem_type == REGRESSION:\n            return RandomForestRegressor\n        else:\n            return RandomForestClassifier\n\n    # TODO: X.fillna -inf? Add extra is_missing column?\n    def preprocess(self, X):\n        X = super().preprocess(X).fillna(0)\n        return X\n\n    def _set_default_params(self):\n        default_params = {\n            'n_estimators': 300,\n            'n_jobs': -1,\n        }\n        for param, val in default_params.items():\n            self._set_default_param_value(param, val)\n\n    # TODO: Add in documentation that Categorical default is the first index\n    # TODO: enable HPO for RF models\n    def _get_default_searchspace(self):\n        spaces = {\n            # 'n_estimators': Int(lower=10, upper=1000, default=300),\n            # 'max_features': Categorical(['auto', 0.5, 0.25]),\n            # 'criterion': Categorical(['gini', 'entropy']),\n        }\n        return spaces\n\n    def fit(self, X_train, Y_train, time_limit=None, **kwargs):\n        time_start = time.time()\n        max_memory_usage_ratio = self.params_aux['max_memory_usage_ratio']\n        hyperparams = self.params.copy()\n        n_estimators_final = hyperparams['n_estimators']\n\n        n_estimators_minimum = min(40, n_estimators_final)\n        if n_estimators_minimum < 40:\n            n_estimators_test = max(1, math.floor(n_estimators_minimum/5))\n        else:\n            n_estimators_test = 8\n\n        X_train = self.preprocess(X_train)\n        n_estimator_increments = [n_estimators_final]\n\n        # Very rough guess to size of a single tree before training\n        if self.problem_type == MULTICLASS:\n            if self.num_classes is None:\n                num_trees_per_estimator = 10  # Guess since it wasn't passed in, could also check y_train for a better value\n            else:\n                num_trees_per_estimator = self.num_classes\n        else:\n            num_trees_per_estimator = 1\n        bytes_per_estimator = num_trees_per_estimator * len(X_train) / 60000 * 1e6  # Underestimates by 3x on ExtraTrees\n        available_mem = psutil.virtual_memory().available\n        expected_memory_usage = bytes_per_estimator * n_estimators_final / available_mem\n        expected_min_memory_usage = bytes_per_estimator * n_estimators_minimum / available_mem\n        if expected_min_memory_usage > (0.5 * max_memory_usage_ratio):  # if minimum estimated size is greater than 50% memory\n            logger.warning(f'\\tWarning: Model is expected to require {expected_min_memory_usage * 100} percent of available memory (Estimated before training)...')\n            raise NotEnoughMemoryError\n\n        if n_estimators_final > n_estimators_test * 2:\n            if self.problem_type == MULTICLASS:\n                n_estimator_increments = [n_estimators_test, n_estimators_final]\n                hyperparams['warm_start'] = True\n            else:\n                if expected_memory_usage > (0.05 * max_memory_usage_ratio):  # Somewhat arbitrary, consider finding a better value, should it scale by cores?\n                    # Causes ~10% training slowdown, so try to avoid if memory is not an issue\n                    n_estimator_increments = [n_estimators_test, n_estimators_final]\n                    hyperparams['warm_start'] = True\n\n        hyperparams['n_estimators'] = n_estimator_increments[0]\n        self.model = self._model_type(**hyperparams)\n\n        time_train_start = time.time()\n        for i, n_estimators in enumerate(n_estimator_increments):\n            if i != 0:\n                self.model.n_estimators = n_estimators\n            self.model = self.model.fit(X_train, Y_train)\n            if (i == 0) and (len(n_estimator_increments) > 1):\n                time_elapsed = time.time() - time_train_start\n\n                model_size_bytes = sys.getsizeof(pickle.dumps(self.model))\n                expected_final_model_size_bytes = model_size_bytes * (n_estimators_final / self.model.n_estimators)\n                available_mem = psutil.virtual_memory().available\n                model_memory_ratio = expected_final_model_size_bytes / available_mem\n\n                ideal_memory_ratio = 0.25 * max_memory_usage_ratio\n                n_estimators_ideal = min(n_estimators_final, math.floor(ideal_memory_ratio / model_memory_ratio * n_estimators_final))\n\n                if n_estimators_final > n_estimators_ideal:\n                    if n_estimators_ideal < n_estimators_minimum:\n                        logger.warning(f'\\tWarning: Model is expected to require {round(model_memory_ratio*100, 1)}% of available memory...')\n                        raise NotEnoughMemoryError  # don't train full model to avoid OOM error\n                    logger.warning(f'\\tWarning: Reducing model \\'n_estimators\\' from {n_estimators_final} -> {n_estimators_ideal} due to low memory. Expected memory usage reduced from {round(model_memory_ratio*100, 1)}% -> {round(ideal_memory_ratio*100, 1)}% of available memory...')\n\n                if time_limit is not None:\n                    time_expected = time_train_start - time_start + (time_elapsed * n_estimators_ideal / n_estimators)\n                    n_estimators_time = math.floor((time_limit - time_train_start + time_start) * n_estimators / time_elapsed)\n                    if n_estimators_time < n_estimators_ideal:\n                        if n_estimators_time < n_estimators_minimum:\n                            logger.warning(f'\\tWarning: Model is expected to require {round(time_expected, 1)}s to train, which exceeds the maximum time limit of {round(time_limit, 1)}s, skipping model...')\n                            raise TimeLimitExceeded\n                        logger.warning(f'\\tWarning: Reducing model \\'n_estimators\\' from {n_estimators_ideal} -> {n_estimators_time} due to low time. Expected time usage reduced from {round(time_expected, 1)}s -> {round(time_limit, 1)}s...')\n                        n_estimators_ideal = n_estimators_time\n\n                for j in range(len(n_estimator_increments)):\n                    if n_estimator_increments[j] > n_estimators_ideal:\n                        n_estimator_increments[j] = n_estimators_ideal\n\n        self.params_trained['n_estimators'] = self.model.n_estimators\n\n    def hyperparameter_tune(self, X_train, X_test, Y_train, Y_test, scheduler_options=None, **kwargs):\n        fit_model_args = dict(X_train=X_train, Y_train=Y_train, **kwargs)\n        predict_proba_args = dict(X=X_test)\n        model_trial.fit_and_save_model(model=self, params=dict(), fit_args=fit_model_args, predict_proba_args=predict_proba_args, y_test=Y_test, time_start=time.time(), time_limit=None)\n        hpo_results = {'total_time': self.fit_time}\n        hpo_model_performances = {self.name: self.val_score}\n        hpo_models = {self.name: self.path}\n        return hpo_models, hpo_model_performances, hpo_results\n\n    def get_model_feature_importance(self):\n        if self.features is None:\n            # TODO: Consider making this raise an exception\n            logger.warning('Warning: get_model_feature_importance called when self.features is None!')\n            return dict()\n        return dict(zip(self.features, self.model.feature_importances_))\n"""
autogluon/utils/tabular/ml/models/tabular_nn/__init__.py,0,b''
autogluon/utils/tabular/ml/models/tabular_nn/categorical_encoders.py,0,"b'"""""" \nVariant of the sklearn OneHotEncoder and OrdinalEncoder that can handle unknown classes at test-time \nas well as binning of infrequent categories to limit the overall number of categories considered.\nUnknown categories are returned as None in inverse transforms. Always converts input list X to list of the same type elements first (string typically)\n""""""\nfrom numbers import Integral\n\nimport numpy as np\nfrom scipy import sparse\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.utils import check_array\nfrom sklearn.utils.fixes import _argmax\nfrom sklearn.utils.validation import check_is_fitted\n\n\n__all__ = [\n    \'OneHotMergeRaresHandleUnknownEncoder\',\n    \'OrdinalMergeRaresHandleUnknownEncoder\'\n]\n\n\ndef _encode_numpy(values, uniques=None, encode=False, check_unknown=True):\n    # only used in _encode below, see docstring there for details\n    if uniques is None:\n        if encode:\n            uniques, encoded = np.unique(values, return_inverse=True)\n            return uniques, encoded\n        else:\n            # unique sorts\n            return np.unique(values)\n    if encode:\n        if check_unknown:\n            diff = _encode_check_unknown(values, uniques)\n            if diff:\n                raise ValueError(""y contains previously unseen labels: %s""\n                                 % str(diff))\n        encoded = np.searchsorted(uniques, values)\n        return uniques, encoded\n    else:\n        return uniques\n\n\ndef _encode_python(values, uniques=None, encode=False):\n    # only used in _encode below, see docstring there for details\n    if uniques is None:\n        uniques = sorted(set(values))\n        uniques = np.array(uniques, dtype=values.dtype)\n    if encode:\n        table = {val: i for i, val in enumerate(uniques)}\n        try:\n            encoded = np.array([table[v] for v in values])\n        except KeyError as e:\n            raise ValueError(""y contains previously unseen labels: %s""\n                             % str(e))\n        return uniques, encoded\n    else:\n        return uniques\n\n\ndef _encode(values, uniques=None, encode=False, check_unknown=True):\n    """"""Helper function to factorize (find uniques) and encode values.\n    Uses pure python method for object dtype, and numpy method for\n    all other dtypes.\n    The numpy method has the limitation that the `uniques` need to\n    be sorted. Importantly, this is not checked but assumed to already be\n    the case. The calling method needs to ensure this for all non-object\n    values.\n    Parameters\n    ----------\n    values : array\n        Values to factorize or encode.\n    uniques : array, optional\n        If passed, uniques are not determined from passed values (this\n        can be because the user specified categories, or because they\n        already have been determined in fit).\n    encode : bool, default False\n        If True, also encode the values into integer codes based on `uniques`.\n    check_unknown : bool, default True\n        If True, check for values in ``values`` that are not in ``unique``\n        and raise an error. This is ignored for object dtype, and treated as\n        True in this case. This parameter is useful for\n        _BaseEncoder._transform() to avoid calling _encode_check_unknown()\n        twice.\n    Returns\n    -------\n    uniques\n        If ``encode=False``. The unique values are sorted if the `uniques`\n        parameter was None (and thus inferred from the data).\n    (uniques, encoded)\n        If ``encode=True``.\n    """"""\n    if values.dtype == object:\n        try:\n            res = _encode_python(values, uniques, encode)\n        except TypeError:\n            raise TypeError(""argument must be a string or number"")\n        return res\n    else:\n        return _encode_numpy(values, uniques, encode,\n                             check_unknown=check_unknown)\n\n\ndef _encode_check_unknown(values, uniques, return_mask=False):\n    """"""\n    Helper function to check for unknowns in values to be encoded.\n    Uses pure python method for object dtype, and numpy method for\n    all other dtypes.\n    Parameters\n    ----------\n    values : array\n        Values to check for unknowns.\n    uniques : array\n        Allowed uniques values.\n    return_mask : bool, default False\n        If True, return a mask of the same shape as `values` indicating\n        the valid values.\n    Returns\n    -------\n    diff : list\n        The unique values present in `values` and not in `uniques` (the\n        unknown values).\n    valid_mask : boolean array\n        Additionally returned if ``return_mask=True``.\n    """"""\n    if values.dtype == object:\n        uniques_set = set(uniques)\n        diff = list(set(values) - uniques_set)\n        if return_mask:\n            if diff:\n                valid_mask = np.array([val in uniques_set for val in values])\n            else:\n                valid_mask = np.ones(len(values), dtype=bool)\n            return diff, valid_mask\n        else:\n            return diff\n    else:\n        unique_values = np.unique(values)\n        diff = list(np.setdiff1d(unique_values, uniques, assume_unique=True))\n        if return_mask:\n            if diff:\n                valid_mask = np.in1d(values, uniques)\n            else:\n                valid_mask = np.ones(len(values), dtype=bool)\n            return diff, valid_mask\n        else:\n            return diff\n\n\nclass _BaseEncoder(BaseEstimator, TransformerMixin):\n    """"""\n    Base class for encoders that includes the code to categorize and\n    transform the input features.\n    """"""\n    \n    def _check_X(self, X):\n        """"""\n        Perform custom check_array:\n        - convert list of strings to object dtype\n        - check for missing values for object dtype data (check_array does\n          not do that)\n        - return list of features (arrays): this list of features is\n          constructed feature by feature to preserve the data types\n          of pandas DataFrame columns, as otherwise information is lost\n          and cannot be used, eg for the `categories_` attribute.\n        \n        """"""\n        if not (hasattr(X, \'iloc\') and getattr(X, \'ndim\', 0) == 2):\n            # if not a dataframe, do normal check_array validation\n            X_temp = check_array(X, dtype=None)\n            if (not hasattr(X, \'dtype\')\n                    and np.issubdtype(X_temp.dtype, np.str_)):\n                X = check_array(X, dtype=np.object)\n            else:\n                X = X_temp\n            needs_validation = False\n        else:\n            # pandas dataframe, do validation later column by column, in order\n            # to keep the dtype information to be used in the encoder.\n            needs_validation = True\n        \n        n_samples, n_features = X.shape\n        X_columns = []\n        \n        for i in range(n_features):\n            Xi = self._get_feature(X, feature_idx=i)\n            Xi = check_array(Xi, ensure_2d=False, dtype=None,\n                             force_all_finite=needs_validation)\n            X_columns.append(Xi)\n        \n        return X_columns, n_samples, n_features\n    \n    def _get_feature(self, X, feature_idx):\n        if hasattr(X, \'iloc\'):\n            # pandas dataframes\n            return X.iloc[:, feature_idx]\n        # numpy arrays, sparse arrays\n        return X[:, feature_idx]\n    \n    def _fit(self, X, handle_unknown=\'error\'):\n        X_list, n_samples, n_features = self._check_X(X)\n        \n        if self.categories != \'auto\':\n            if len(self.categories) != n_features:\n                raise ValueError(""Shape mismatch: if categories is an array,""\n                                 "" it has to be of shape (n_features,)."")\n        \n        if self.max_levels is not None:\n            if (not isinstance(self.max_levels, Integral) or\n                    self.max_levels <= 0):\n                raise ValueError(""max_levels must be None or a strictly ""\n                                 ""positive int, got {}."".format(\n                                     self.max_levels))\n        \n        self.categories_ = []\n        self.infrequent_indices_ = []\n        \n        for i in range(n_features):\n            Xi = X_list[i]\n            if self.categories == \'auto\':\n                cats = _encode(Xi)\n            else:\n                cats = np.array(self.categories[i], dtype=Xi.dtype)\n                if Xi.dtype != object:\n                    if not np.all(np.sort(cats) == cats):\n                        raise ValueError(""Unsorted categories are not ""\n                                         ""supported for numerical categories"")\n                if handle_unknown == \'error\':\n                    diff = _encode_check_unknown(Xi, cats)\n                    if diff:\n                        msg = (""Found unknown categories {0} in column {1}""\n                               "" during fit"".format(diff, i))\n                        raise ValueError(msg)\n            self.categories_.append(cats)\n            \n            if self.max_levels is not None:\n                infrequent_indices = self._find_infrequent_category_indices(Xi)\n            else:\n                infrequent_indices = np.array([])\n            self.infrequent_indices_.append(infrequent_indices)\n    \n    def _find_infrequent_category_indices(self, Xi):\n        # TODO: this is using unique on X again. Ideally we should integrate\n        # this into _encode()\n        _, counts = np.unique(Xi, return_counts=True)\n        return np.argsort(counts)[:-self.max_levels]\n    \n    def _transform(self, X, handle_unknown=\'error\'):\n        X_list, n_samples, n_features = self._check_X(X)\n        \n        X_int = np.zeros((n_samples, n_features), dtype=np.int)\n        X_mask = np.ones((n_samples, n_features), dtype=np.bool)\n        \n        if n_features != len(self.categories_):\n            raise ValueError(\n                ""The number of features in X is different to the number of ""\n                ""features of the fitted data. The fitted data had {} features ""\n                ""and the X has {} features.""\n                .format(len(self.categories_,), n_features)\n            )\n        \n        for i in range(n_features):\n            Xi = X_list[i]\n            diff, valid_mask = _encode_check_unknown(Xi, self.categories_[i],\n                                                     return_mask=True)\n            \n            if not np.all(valid_mask):\n                if handle_unknown == \'error\':\n                    msg = (""Found unknown categories {0} in column {1}""\n                           "" during transform"".format(diff, i))\n                    raise ValueError(msg)\n                else:\n                    # Set the problematic rows to an acceptable value and\n                    # continue `The rows are marked `X_mask` and will be\n                    # removed later.\n                    X_mask[:, i] = valid_mask\n                    # cast Xi into the largest string type necessary\n                    # to handle different lengths of numpy strings\n                    if (self.categories_[i].dtype.kind in (\'U\', \'S\')\n                            and self.categories_[i].itemsize > Xi.itemsize):\n                        Xi = Xi.astype(self.categories_[i].dtype)\n                    else:\n                        Xi = Xi.copy()\n                    \n                    Xi[~valid_mask] = self.categories_[i][0]\n            # We use check_unknown=False, since _encode_check_unknown was\n            # already called above.\n            _, encoded = _encode(Xi, self.categories_[i], encode=True,\n                                 check_unknown=False)\n            X_int[:, i] = encoded\n        \n        # We need to take care of infrequent categories here. We want all the\n        # infrequent categories to end up in a specific column, after all the\n        # frequent ones. Let\'s say we have 4 categories with 2 infrequent\n        # categories (and 2 frequent categories): we want the value in X_int\n        # for the infrequent categories to be 2 (third and last column), and\n        # the values for the frequent ones to be 0 and 1. The piece of code\n        # below performs this mapping.\n        # TODO: maybe integrate this part with the one above\n        self._infrequent_mappings = {}\n        huge_int = np.iinfo(X_int.dtype).max\n        for feature_idx in range(n_features):\n            if self.infrequent_indices_[feature_idx].size > 0:\n                mapping = np.arange(len(self.categories_[feature_idx]))\n                # Trick: set the infrequent cats columns to a very big int and\n                # encode again.\n                for ordinal_cat in self.infrequent_indices_[feature_idx]:\n                    mapping[ordinal_cat] = huge_int\n                _, mapping = _encode_numpy(mapping, encode=True)\n                \n                # update X_int and save mapping for later (for dropping logic)\n                X_int[:, feature_idx] = mapping[X_int[:, feature_idx]]\n                self._infrequent_mappings[feature_idx] = mapping\n        \n        return X_int, X_mask\n    \n    def _more_tags(self):\n        return {\'X_types\': [\'categorical\']}\n\n\nclass OneHotMergeRaresHandleUnknownEncoder(_BaseEncoder):\n    """"""Encode categorical integer features as a one-hot numeric array.\n    \n    The input to this transformer should be an array-like of integers or\n    strings, denoting the values taken on by categorical (discrete) features.\n    The features are encoded using a one-hot (aka \'one-of-K\' or \'dummy\')\n    encoding scheme. This creates a binary column for each category and\n    returns a sparse matrix or dense array (depending on the ``sparse``\n    parameter)\n    \n    By default, the encoder derives the categories based on the unique values\n    in each feature. Alternatively, you can also specify the `categories`\n    manually.\n    \n    Always uses handle_unknown=\'ignore\' which maps unknown test-time categories to all zeros vector.\n\n    Parameters\n    ----------\n    categories : \'auto\' or a list of lists/arrays of values, default=\'auto\'.\n        Categories (unique values) per feature:\n\n        - \'auto\' : Determine categories automatically from the training data.\n        - list : ``categories[i]`` holds the categories expected in the ith\n          column. The passed categories should not mix strings and numeric\n          values within a single feature, and should be sorted in case of\n          numeric values.\n\n        The used categories can be found in the ``categories_`` attribute.\n\n    drop : \'first\' or a list/array of shape (n_features,), default=None.\n        Specifies a methodology to use to drop one of the categories per\n        feature. This is useful in situations where perfectly collinear\n        features cause problems, such as when feeding the resulting data\n        into a neural network or an unregularized regression.\n\n        - None : retain all features (the default).\n        - \'first\' : drop the first category in each feature. If only one\n          category is present, the feature will be dropped entirely.\n        - array : ``drop[i]`` is the category in feature ``X[:, i]`` that\n          should be dropped. If ``drop[i]`` is an infrequent category, an\n          error is raised: it is only possible to drop all of the infrequent\n          categories, not just one of them.\n        - \'infrequent\' : drop the infrequent categories column (see\n          ``max_levels`` parameter).\n\n    sparse : boolean, default=True\n        Will return sparse matrix if set True else will return an array.\n\n    dtype : number type, default=np.float\n        Desired dtype of output.\n        \n    max_levels : int, default=None\n        One less than the maximum number of categories to keep (max_levels = 2 means we keep 3 distinct categories). \n        Infrequent categories are grouped together and mapped into a single column, which counts as extra category.\n        Unknown categories encountered at test time are mapped to all zeros vector.\n    \n    Attributes\n    ----------\n    categories_ : list of arrays\n        The categories of each feature determined during fitting\n        (in order of the features in X and corresponding with the output\n        of ``transform``). This includes the category specified in ``drop``\n        (if any).\n\n    drop_idx_ : array of shape (n_features,)\n        ``drop_idx_[i]`` is\xc2\xa0the index in ``categories_[i]`` of the category to\n        be dropped for each feature. None if all the transformed features will\n        be retained.\n\n    infrequent_indices_: list of arrays of shape(n_infrequent_categories)\n        ``infrequent_indices_[i]`` contains a list of indices in\n        ``categories_[i]`` corresponding to the infrequent categories.\n\n    """"""\n    \n    def __init__(self, categories=\'auto\', drop=None, sparse=True,\n                 dtype=np.float64, max_levels=None):\n        self.categories = categories\n        self.sparse = sparse\n        self.dtype = dtype\n        self.handle_unknown = \'ignore\'\n        self.drop = drop\n        self.max_levels = max_levels\n    \n    def _validate_keywords(self):\n        if self.handle_unknown not in (\'error\', \'ignore\'):\n            msg = (""handle_unknown should be either \'error\' or \'ignore\', ""\n                   ""got {0}."".format(self.handle_unknown))\n            raise ValueError(msg)\n        # If we have both dropped columns and ignored unknown\n        # values, there will be ambiguous cells. This creates difficulties\n        # in interpreting the model.\n        if self.drop is not None and self.handle_unknown != \'error\':\n            raise ValueError(\n                ""`handle_unknown` must be \'error\' when the drop parameter is ""\n                ""specified, as both would create categories that are all ""\n                ""zero."")\n    \n    def _compute_drop_idx(self):\n        if self.drop is None:\n            return None\n        elif (isinstance(self.drop, str) and\n                self.drop in (\'first\', \'infrequent\')):\n            return np.zeros(len(self.categories_), dtype=np.int_)\n        elif not isinstance(self.drop, str):\n            try:\n                self.drop = np.asarray(self.drop, dtype=object)\n                droplen = len(self.drop)\n            except (ValueError, TypeError):\n                msg = (""Wrong input for parameter `drop`. Expected ""\n                       ""\'first\', None or array of objects, got {}"")\n                raise ValueError(msg.format(type(self.drop)))\n            if droplen != len(self.categories_):\n                msg = (""`drop` should have length equal to the number ""\n                       ""of features ({}), got {}"")\n                raise ValueError(msg.format(len(self.categories_),\n                                            len(self.drop)))\n            missing_drops = [(i, val) for i, val in enumerate(self.drop)\n                             if val not in self.categories_[i]]\n            if any(missing_drops):\n                msg = (""The following categories were supposed to be ""\n                       ""dropped, but were not found in the training ""\n                       ""data.\\n{}"".format(\n                           ""\\n"".join(\n                                [""Category: {}, Feature: {}"".format(c, v)\n                                    for c, v in missing_drops])))\n                raise ValueError(msg)\n            return np.array([np.where(cat_list == val)[0][0]\n                             for (val, cat_list) in\n                             zip(self.drop, self.categories_)], dtype=np.int_)\n        else:\n            msg = (""Wrong input for parameter `drop`. Expected ""\n                   ""\'first\', None or array of objects, got {}"")\n            raise ValueError(msg.format(type(self.drop)))\n    \n    def fit(self, X, y=None):\n        """"""Fit OneHotEncoder to X.\n    \n        Parameters\n        ----------\n        X : array-like, shape [n_samples, n_features]\n            The data to determine the categories of each feature.\n    \n        Returns\n        -------\n        self\n        """"""\n        X = np.array(X).tolist() # converts all elements in X to the same type (i.e. cannot mix floats, ints, and str)\n        self._validate_keywords()\n        self._fit(X, handle_unknown=self.handle_unknown)\n        self.drop_idx_ = self._compute_drop_idx()\n        # check if user wants to manually drop a feature that is\n        # infrequent: this is not allowed\n        if self.drop is not None and not isinstance(self.drop, str):\n            for feature_idx, (infrequent_indices, drop_idx) in enumerate(\n                    zip(self.infrequent_indices_, self.drop_idx_)):\n                if drop_idx in infrequent_indices:\n                    raise ValueError(\n                        ""Category {} of feature {} is infrequent and thus ""\n                        ""cannot be dropped. Use drop=\'infrequent\' ""\n                        ""instead."".format(\n                            self.categories_[feature_idx][drop_idx],\n                            feature_idx\n                        )\n                    )\n        return self\n    \n    def fit_transform(self, X, y=None):\n        """"""Fit OneHotEncoder to X, then transform X.\n\n        Equivalent to fit(X).transform(X) but more convenient.\n\n        Parameters\n        ----------\n        X : array-like, shape [n_samples, n_features]\n            The data to encode.\n\n        Returns\n        -------\n        X_out : sparse matrix if sparse=True else a 2-d array\n            Transformed input.\n        """"""\n        X = np.array(X).tolist() # converts all elements in X to the same type (i.e. cannot mix floats, ints, and str)\n        self._validate_keywords()\n        return super().fit_transform(X, y)\n    \n    def transform(self, X):\n        """"""Transform X using one-hot encoding.\n        \n        Parameters\n        ----------\n        X : array-like, shape [n_samples, n_features]\n            The data to encode.\n        \n        Returns\n        -------\n        X_out : sparse matrix if sparse=True else a 2-d array\n            Transformed input.\n        """"""\n        X = np.array(X).tolist() # converts all elements in X to the same type (i.e. cannot mix floats, ints, and str)\n        check_is_fitted(self, \'categories_\')\n        # validation of X happens in _check_X called by _transform\n        X_int, X_mask = self._transform(X, handle_unknown=self.handle_unknown)\n        n_samples, n_features = X_int.shape\n        \n        # n_columns indicates, for each feature, how many columns are used in\n        # X_trans. By default this corresponds to the number of categories, but\n        # will differ if we drop some of them, or if there are infrequent\n        # categories (all mapped to the same column)\n        n_columns = [len(cats) for cats in self.categories_]\n        for feature_idx in range(n_features):\n            n_infrequent = self.infrequent_indices_[feature_idx].size\n            if n_infrequent > 0:\n                # still add 1 for the infrequent column\n                n_columns[feature_idx] += 1 - n_infrequent\n            if self.drop is not None:\n                # if drop is not None we always drop one column in general,\n                # except when drop is \'infrequent\' and there is no infrequent\n                # category.\n                n_columns[feature_idx] -= 1\n                if (isinstance(self.drop, str) and self.drop == \'infrequent\'\n                        and n_infrequent == 0):\n                    n_columns[feature_idx] += 1  # revert decrement from above\n        \n        if self.drop is not None:\n            to_drop = self.drop_idx_.copy()\n            if isinstance(self.drop, str):\n                if self.drop == \'infrequent\':\n                    for feature_idx in range(n_features):\n                        if self.infrequent_indices_[feature_idx].size > 0:\n                            # drop the infrequent column (i.e. the last one)\n                            to_drop[feature_idx] = n_columns[feature_idx]\n                        else:\n                            # no infrequent category, use special marker -1\n                            # so that no dropping happens for this feature\n                            to_drop[feature_idx] = -1\n            else:\n                # self.drop is an array of categories. we need to remap the\n                # dropped indexes if some of the categories are infrequent.\n                # see _transform() for details about the mapping.\n                for feature_idx in range(n_features):\n                    if self.infrequent_indices_[feature_idx].size > 0:\n                        mapping = self._infrequent_mappings[feature_idx]\n                        to_drop[feature_idx] = mapping[to_drop[feature_idx]]\n            \n            # We remove all the dropped categories from mask, and decrement\n            # all categories that occur after them to avoid an empty column.\n            to_drop = to_drop.reshape(1, -1)\n            keep_cells = (X_int != to_drop) | (to_drop == -1)\n            X_mask &= keep_cells\n            X_int[(X_int > to_drop) & (to_drop != -1)] -= 1\n        \n        mask = X_mask.ravel()\n        n_values = np.array([0] + n_columns)\n        feature_indices = np.cumsum(n_values)\n        indices = (X_int + feature_indices[:-1]).ravel()[mask]\n        indptr = X_mask.sum(axis=1).cumsum()\n        indptr = np.insert(indptr, 0, 0)\n        data = np.ones(n_samples * n_features)[mask]\n        \n        out = sparse.csr_matrix((data, indices, indptr),\n                                shape=(n_samples, feature_indices[-1]),\n                                dtype=self.dtype)\n        if not self.sparse:\n            return out.toarray()\n        else:\n            return out\n    \n    def inverse_transform(self, X):\n        """"""Convert the back data to the original representation.\n        \n        In case unknown categories are encountered (all zeros in the\n        one-hot encoding), ``None`` is used to represent this category.\n        \n        Parameters\n        ----------\n        X : array-like or sparse matrix, shape [n_samples, n_encoded_features]\n            The transformed data.\n        \n        Returns\n        -------\n        X_tr : array-like, shape [n_samples, n_features]\n            Inverse transformed array.\n        \n        """"""\n        check_is_fitted(self, \'categories_\')\n        X = check_array(X, accept_sparse=\'csr\')\n        \n        n_samples, _ = X.shape\n        n_features = len(self.categories_)\n        if self.drop is None:\n            n_transformed_features = sum(len(cats)\n                                         for cats in self.categories_)\n        else:\n            n_transformed_features = sum(len(cats) - 1\n                                         for cats in self.categories_)\n        \n        # validate shape of passed X\n        msg = (""Shape of the passed X data is not correct. Expected {0} ""\n               ""columns, got {1}."")\n        if X.shape[1] != n_transformed_features:\n            raise ValueError(msg.format(n_transformed_features, X.shape[1]))\n        \n        # create resulting array of appropriate dtype\n        dt = np.find_common_type([cat.dtype for cat in self.categories_], [])\n        X_tr = np.empty((n_samples, n_features), dtype=dt)\n        j = 0\n        found_unknown = {}\n        \n        for i in range(n_features):\n            if self.drop is None:\n                cats = self.categories_[i]\n            else:\n                cats = np.delete(self.categories_[i], self.drop_idx_[i])\n            n_categories = len(cats)\n        \n            # Only happens if there was a column with a unique\n            # category. In this case we just fill the column with this\n            # unique category value.\n            if n_categories == 0:\n                X_tr[:, i] = self.categories_[i][self.drop_idx_[i]]\n                j += n_categories\n                continue\n            sub = X[:, j:j + n_categories]\n            # for sparse X argmax returns 2D matrix, ensure 1D array\n            labels = np.asarray(_argmax(sub, axis=1)).flatten()\n            X_tr[:, i] = cats[labels]\n            if self.handle_unknown == \'ignore\':\n                unknown = np.asarray(sub.sum(axis=1) == 0).flatten()\n                # ignored unknown categories: we have a row of all zero\n                if unknown.any():\n                    found_unknown[i] = unknown\n            # drop will either be None or handle_unknown will be error. If\n            # self.drop is not None, then we can safely assume that all of\n            # the nulls in each column are the dropped value\n            elif self.drop is not None:\n                dropped = np.asarray(sub.sum(axis=1) == 0).flatten()\n                if dropped.any():\n                    X_tr[dropped, i] = self.categories_[i][self.drop_idx_[i]]\n            \n            j += n_categories\n        \n        # if ignored are found: potentially need to upcast result to\n        # insert None values\n        if found_unknown:\n            if X_tr.dtype != object:\n                X_tr = X_tr.astype(object)\n        \n            for idx, mask in found_unknown.items():\n                X_tr[mask, idx] = None\n        \n        return X_tr\n    \n    def get_feature_names(self, input_features=None):\n        """"""Return feature names for output features.\n    \n        Parameters\n        ----------\n        input_features : list of string, length n_features, optional\n            String names for input features if available. By default,\n            ""x0"", ""x1"", ... ""xn_features"" is used.\n        \n        Returns\n        -------\n        output_feature_names : array of string, length n_output_features\n        \n        """"""\n        check_is_fitted(self, \'categories_\')\n        cats = self.categories_\n        if input_features is None:\n            input_features = [\'x%d\' % i for i in range(len(cats))]\n        elif len(input_features) != len(self.categories_):\n            raise ValueError(\n                ""input_features should have length equal to number of ""\n                ""features ({}), got {}"".format(len(self.categories_),\n                                               len(input_features)))\n        \n        feature_names = []\n        for i in range(len(cats)):\n            names = [\n                input_features[i] + \'_\' + str(t) for t in cats[i]]\n            if self.drop is not None:\n                names.pop(self.drop_idx_[i])\n            feature_names.extend(names)\n        \n        return np.array(feature_names, dtype=object)\n\n\nclass OrdinalMergeRaresHandleUnknownEncoder(_BaseEncoder):\n    """"""Encode categorical features as an integer array.\n\n    The input to this transformer should be an array-like of integers or\n    strings, denoting the values taken on by categorical (discrete) features.\n    The features are converted to ordinal integers. This results in\n    a single column of integers (0 to n_categories - 1) per feature.\n\n    Read more in the :ref:`User Guide <preprocessing_categorical_features>`.\n\n    Parameters\n    ----------\n    categories : \'auto\' or a list of lists/arrays of values.\n        Categories (unique values) per feature:\n\n        - \'auto\' : Determine categories automatically from the training data.\n        - list : ``categories[i]`` holds the categories expected in the ith\n          column. The passed categories should not mix strings and numeric\n          values, and should be sorted in case of numeric values.\n\n        The used categories can be found in the ``categories_`` attribute.\n\n    dtype : number type, default np.float64\n        Desired dtype of output.\n\n    max_levels : int, default=None\n        One less than the maximum number of categories to keep (max_levels = 2 means we keep 3 distinct categories). \n        Infrequent categories are grouped together and mapped to the highest int\n        Unknown categories encountered at test time are mapped to another extra category. Embedding layers should be able to take in max_levels + 1 categories!\n\n    Attributes\n    ----------\n    categories_ : list of arrays\n        The categories of each feature determined during fitting\n        (in order of the features in X and corresponding with the output\n        of ``transform``).\n\n    infrequent_indices_: list of arrays of shape(n_infrequent_categories)\n        ``infrequent_indices_[i]`` contains a list of indices in\n        ``categories_[i]`` corresponsing to the infrequent categories.\n    \n    """"""\n    \n    def __init__(self, categories=\'auto\', dtype=np.float64, max_levels=None):\n        self.categories = categories\n        self.dtype = dtype\n        self.max_levels = max_levels\n    \n    def fit(self, X, y=None):\n        """"""Fit the OrdinalEncoder to X.\n        \n        Parameters\n        ----------\n        X : array-like, shape [n_samples, n_features]\n            The data to determine the categories of each feature.\n        \n        Returns\n        -------\n        self\n        \n        """"""\n        X = np.array(X).tolist() # converts all elements in X to the same type (i.e. cannot mix floats, ints, and str)\n        self._fit(X, handle_unknown=\'ignore\')\n        return self\n    \n    def transform(self, X):\n        """"""Transform X to ordinal codes.\n        \n        Parameters\n        ----------\n        X : array-like, shape [n_samples, n_features]\n            The data to encode.\n        \n        Returns\n        -------\n        X_out : sparse matrix or a 2-d array\n            Transformed input.\n        \n        """"""\n        X = np.array(X).tolist() # converts all elements in X to the same type (i.e. cannot mix floats, ints, and str)\n        X_int, _ = self._transform(X, handle_unknown=\'ignore\')  # will contain zeros for 0th category as well as unknown values.\n        X_og_array = np.array(X) # original X array before transform\n        for i in range(X_int.shape[1]):\n            feature_i_categories = self.categories_[i]\n            feature_i_numlevels = min(len(feature_i_categories), self.max_levels)\n            unknown_level_i = feature_i_numlevels  # new level introduced to account for unknown categories, alway= 1 + total number of categories seen during training\n            unknown_elements = np.logical_not(np.isin(X_og_array[:,i], feature_i_categories))\n            X_int[unknown_elements,i] = unknown_level_i # replace entries with unknown categories with feature_i_numlevels + 1 value. Do NOT modify self.categories_\n        return X_int.astype(self.dtype, copy=False)\n    \n    def inverse_transform(self, X):\n        """"""Convert the data back to the original representation.\n            In case unknown categories are encountered (all zeros in the one-hot encoding), ``None`` is used to represent this category.\n        \n        Parameters\n        ----------\n        X : array-like or sparse matrix, shape [n_samples, n_encoded_features]\n            The transformed data.\n        \n        Returns\n        -------\n        X_tr : array-like, shape [n_samples, n_features]\n            Inverse transformed array.\n        \n        """"""\n        check_is_fitted(self, \'categories_\')\n        X = check_array(X, accept_sparse=\'csr\')\n        \n        n_samples, _ = X.shape\n        n_features = len(self.categories_)\n        \n        # validate shape of passed X\n        msg = (""Shape of the passed X data is not correct. Expected {0} ""\n               ""columns, got {1}."")\n        if X.shape[1] != n_features:\n            raise ValueError(msg.format(n_features, X.shape[1]))\n        \n        # create resulting array of appropriate dtype\n        dt = np.find_common_type([cat.dtype for cat in self.categories_], [])\n        X_tr = np.empty((n_samples, n_features), dtype=dt)\n        \n        for i in range(n_features):\n            possible_categories = np.append(self.categories_[i], None)\n            labels = X[:, i].astype(\'int64\', copy=False)\n            X_tr[:, i] = self.categories_[i][labels]\n        \n        return X_tr\n\n\n'"
autogluon/utils/tabular/ml/models/tabular_nn/embednet.py,0,"b'import numpy as np\nimport mxnet as mx\nfrom mxnet import nd, gluon\n\n\nclass NumericBlock(gluon.HybridBlock):\n    """""" Single Dense layer that jointly embeds all numeric and one-hot features """"""\n    def __init__(self, params, **kwargs):\n        super(NumericBlock, self).__init__(**kwargs)\n        with self.name_scope():\n            self.body = gluon.nn.Dense(params[\'numeric_embed_dim\'], activation=params[\'activation\'])\n            \n    def hybrid_forward(self, F, x):\n        return self.body(x)\n\n\nclass EmbedBlock(gluon.HybridBlock):\n    """""" Used to embed a single embedding feature. """"""\n    def __init__(self, embed_dim, num_categories, **kwargs):\n        super(EmbedBlock, self).__init__(**kwargs)\n        with self.name_scope():\n            self.body = gluon.nn.Embedding(input_dim=num_categories, output_dim=embed_dim,\n                                           weight_initializer=mx.init.Orthogonal(scale=0.1, rand_type=\'uniform\')) # for Xavier-style: scale = np.sqrt(3/float(embed_dim))\n    \n    def hybrid_forward(self, F, x):\n        return self.body(x)\n\n\nclass FeedforwardBlock(gluon.HybridBlock):\n    """""" Standard Feedforward layers """"""\n    def __init__(self, params, num_net_outputs, **kwargs):\n        super(FeedforwardBlock, self).__init__(**kwargs)\n        layers = params[\'layers\']\n        with self.name_scope():\n            self.body = gluon.nn.HybridSequential()\n            if params[\'use_batchnorm\']:\n                 self.body.add(gluon.nn.BatchNorm())\n            if params[\'dropout_prob\'] > 0:\n                self.body.add(gluon.nn.Dropout(params[\'dropout_prob\']))\n            for i in range(len(layers)):\n                layer_width = layers[i]\n                if layer_width < 1 or int(layer_width) != layer_width:\n                    raise ValueError(""layers must be ints >= 1"")\n                self.body.add(gluon.nn.Dense(layer_width, activation=params[\'activation\']))\n                if params[\'use_batchnorm\']:\n                    self.body.add(gluon.nn.BatchNorm())\n                if params[\'dropout_prob\'] > 0:\n                    self.body.add(gluon.nn.Dropout(params[\'dropout_prob\']))\n            self.body.add(gluon.nn.Dense(num_net_outputs, activation=None))\n    \n    def hybrid_forward(self, F, x):\n        return self.body(x)\n\n\nclass WideAndDeepBlock(gluon.HybridBlock):\n    """""" Standard feedforward layers with a single skip connection from output directly to input (ie. deep and wide network).\n    """"""\n    def __init__(self, params, num_net_outputs, **kwargs):\n        super(WideAndDeepBlock, self).__init__(**kwargs)\n        self.deep = FeedforwardBlock(params, num_net_outputs, **kwargs)\n        with self.name_scope(): # Skip connection, ie. wide network branch\n            self.wide = gluon.nn.Dense(num_net_outputs, activation=None)\n    \n    def hybrid_forward(self, F, x):\n        return self.deep(x) + self.wide(x)\n\n\nclass EmbedNet(gluon.Block): # TODO: hybridize?\n    """""" Gluon net with input layers to handle numerical data & categorical embeddings\n        which are concatenated together after input layer and then passed into feedforward network.\n        If architecture_desc != None, then we assume EmbedNet has already been previously created,\n        and we create a new EmbedNet based on the provided architecture description \n        (thus ignoring train_dataset, params, num_net_outputs). \n    """"""\n    def __init__(self, train_dataset=None, params=None, num_net_outputs=None, architecture_desc=None, ctx=None, **kwargs):\n        if (architecture_desc is None) and (train_dataset is None or params is None or num_net_outputs is None):\n            raise ValueError(""train_dataset, params, num_net_outputs cannot = None if architecture_desc=None"")\n        super(EmbedNet, self).__init__(**kwargs)\n        if architecture_desc is None: # Adpatively specify network architecture based on training dataset\n            self.from_logits = False\n            self.has_vector_features = train_dataset.has_vector_features()\n            self.has_embed_features = train_dataset.num_embed_features() > 0\n            self.has_language_features = train_dataset.num_language_features() > 0\n            if self.has_embed_features:\n                num_categs_per_feature = train_dataset.getNumCategoriesEmbeddings()\n                embed_dims = getEmbedSizes(train_dataset, params, num_categs_per_feature)\n        else: # Ignore train_dataset, params, etc. Recreate architecture based on description:\n            self.architecture_desc = architecture_desc\n            self.has_vector_features = architecture_desc[\'has_vector_features\']\n            self.has_embed_features = architecture_desc[\'has_embed_features\']\n            self.has_language_features = architecture_desc[\'has_language_features\']\n            self.from_logits = architecture_desc[\'from_logits\']\n            num_net_outputs = architecture_desc[\'num_net_outputs\']\n            params = architecture_desc[\'params\']\n            if self.has_embed_features:\n                num_categs_per_feature = architecture_desc[\'num_categs_per_feature\']\n                embed_dims = architecture_desc[\'embed_dims\']\n        \n        # Define neural net parameters:\n        if self.has_vector_features:\n            self.numeric_block = NumericBlock(params)\n        if self.has_embed_features:\n            self.embed_blocks = gluon.nn.HybridSequential()\n            for i in range(len(num_categs_per_feature)):\n                self.embed_blocks.add(EmbedBlock(embed_dims[i], num_categs_per_feature[i]))\n        if self.has_language_features:\n            self.text_block = None\n            raise NotImplementedError(""text data cannot be handled"")\n        if params[\'network_type\'] == \'feedforward\':\n            self.output_block = FeedforwardBlock(params, num_net_outputs)\n        elif params[\'network_type\'] == \'widedeep\':\n            self.output_block = WideAndDeepBlock(params, num_net_outputs)\n        else:\n            raise ValueError(""unknown network_type specified: %s"" % params[\'network_type\'])\n        \n        y_range = params[\'y_range\'] # Used specifically for regression. = None for classification.\n        self.y_constraint = None # determines if Y-predictions should be constrained\n        if y_range is not None:\n            if y_range[0] == -np.inf and y_range[1] == np.inf:\n                self.y_constraint = None # do not worry about Y-range in this case\n            elif y_range[0] >= 0 and y_range[1] == np.inf:\n                self.y_constraint = \'nonnegative\'\n            elif y_range[0] == -np.inf and y_range[1] <= 0:\n                self.y_constraint = \'nonpositive\'\n            else:\n                self.y_constraint = \'bounded\'\n            self.y_lower = nd.array(params[\'y_range\'][0]).reshape(1,)\n            self.y_upper = nd.array(params[\'y_range\'][1]).reshape(1,)\n            if ctx is not None:\n                self.y_lower.as_in_context(ctx)\n                self.y_upper.as_in_context(ctx)\n            self.y_span = self.y_upper - self.y_lower\n        \n        if architecture_desc is None: # Save Architecture description\n            self.architecture_desc = {\'has_vector_features\': self.has_vector_features, \n                                  \'has_embed_features\': self.has_embed_features,\n                                  \'has_language_features\': self.has_language_features,\n                                  \'params\': params, \'num_net_outputs\': num_net_outputs,\n                                  \'from_logits\': self.from_logits}\n            if self.has_embed_features:\n                self.architecture_desc[\'num_categs_per_feature\'] = num_categs_per_feature\n                self.architecture_desc[\'embed_dims\'] = embed_dims\n            if self.has_language_features:\n                self.architecture_desc[\'text_TODO\'] = None # TODO: store text architecture\n    \n    def forward(self, data_batch):\n        if self.has_vector_features:\n            numerical_data = data_batch[\'vector\'] # NDArray\n            numerical_activations = self.numeric_block(numerical_data)\n            input_activations = numerical_activations\n        if self.has_embed_features:\n            embed_data = data_batch[\'embed\'] # List\n\n            # TODO: Remove below lines or write logic to switch between using these lines and the multithreaded version once multithreaded version is optimized\n            embed_activations = self.embed_blocks[0](embed_data[0])\n            for i in range(1, len(self.embed_blocks)):\n                embed_activations = nd.concat(embed_activations,\n                                              self.embed_blocks[i](embed_data[i]), dim=2)\n\n            # TODO: Optimize below to perform better before using\n            # lock = threading.Lock()\n            # results = {}\n            #\n            # def _worker(i, results, embed_block, embed_data, is_recording, is_training, lock):\n            #     if is_recording:\n            #         with mx.autograd.record(is_training):\n            #             output = embed_block(embed_data)\n            #     else:\n            #         output = embed_block(embed_data)\n            #     output.wait_to_read()\n            #     with lock:\n            #         results[i] = output\n            #\n            # is_training = mx.autograd.is_training()\n            # is_recording = mx.autograd.is_recording()\n            # threads = [threading.Thread(target=_worker,\n            #                     args=(i, results, embed_block, embed_data,\n            #                           is_recording, is_training, lock),\n            #                     )\n            #    for i, (embed_block, embed_data) in\n            #    enumerate(zip(self.embed_blocks, embed_data))]\n            #\n            # for thread in threads:\n            #     thread.start()\n            # for thread in threads:\n            #     thread.join()\n            #\n            # embed_activations = []\n            # for i in range(len(results)):\n            #     output = results[i]\n            #     embed_activations.append(output)\n            #\n            # #embed_activations = []\n            # #for i in range(len(self.embed_blocks)):\n            # #    embed_activations.append(self.embed_blocks[i](embed_data[i]))\n            # embed_activations = nd.concat(*embed_activations, dim=2)\n            embed_activations = embed_activations.flatten()\n            if not self.has_vector_features:\n                input_activations = embed_activations\n            else:\n                input_activations = nd.concat(embed_activations, input_activations)\n        if self.has_language_features:\n            language_data = data_batch[\'language\']\n            language_activations = self.text_block(language_data) # TODO: create block to embed text fields\n            if (not self.has_vector_features) and (not self.has_embed_features):\n                input_activations = language_activations\n            else:\n                input_activations = nd.concat(language_activations, input_activations)\n        if self.y_constraint is None:\n            return self.output_block(input_activations)\n        else:\n            unscaled_pred = self.output_block(input_activations)\n            if self.y_constraint == \'nonnegative\':\n                return self.y_lower + nd.abs(unscaled_pred)\n            elif self.y_constraint == \'nonpositive\':\n                return self.y_upper - nd.abs(unscaled_pred)\n            else:\n                """"""\n                print(""unscaled_pred"",unscaled_pred)\n                print(""nd.sigmoid(unscaled_pred)"", nd.sigmoid(unscaled_pred))\n                print(""self.y_span"", self.y_span)\n                print(""self.y_lower"", self.y_lower)\n                print(""self.y_lower.shape"", self.y_lower.shape)\n                print(""nd.sigmoid(unscaled_pred).shape"", nd.sigmoid(unscaled_pred).shape)\n                """"""\n                return nd.sigmoid(unscaled_pred) * self.y_span + self.y_lower\n\n\n"""""" OLD \n    def _create_embednet_from_architecture(architecture_desc):\n        # Recreate network architecture based on provided description\n        self.architecture_desc = architecture_desc\n        self.has_vector_features = architecture_desc[\'has_vector_features\']\n        self.has_embed_features = architecture_desc[\'has_embed_features\']\n        self.has_language_features = architecture_desc[\'has_language_features\']\n        self.from_logits = architecture_desc[\'from_logits\']\n        num_net_outputs = architecture_desc[\'num_net_outputs\']\n        params = architecture_desc[\'params\']\n        if self.has_vector_features:\n            self.numeric_block = NumericBlock(params)\n        if self.has_embed_features:\n            self.embed_blocks = gluon.nn.HybridSequential()\n            num_categs_per_feature = architecture_desc[\'num_categs_per_feature\']\n            embed_dims = architecture_desc[\'embed_dims\']\n            for i in range(len(num_categs_per_feature)):\n                self.embed_blocks.add(EmbedBlock(embed_dims[i], num_categs_per_feature[i]))\n        if self.has_language_features:\n            self.text_block = architecture_desc[\'text_TODO\']\n        if \n        self.output_block = FeedforwardBlock(params, num_net_outputs) # TODO\n        self.from_logits = False\n""""""\n\n\ndef getEmbedSizes(train_dataset, params, num_categs_per_feature):  \n    """""" Returns list of embedding sizes for each categorical variable.\n        Selects this adaptively based on training_datset.\n        Note: Assumes there is at least one embed feature.\n    """"""\n    max_embedding_dim = params[\'max_embedding_dim\']\n    embed_exponent = params[\'embed_exponent\']\n    size_factor = params[\'embedding_size_factor\']\n    embed_dims = [int(size_factor*max(2, min(max_embedding_dim, \n                                      1.6 * num_categs_per_feature[i]**embed_exponent)))\n                   for i in range(len(num_categs_per_feature))]\n    return embed_dims\n'"
autogluon/utils/tabular/ml/models/tabular_nn/tabular_nn_dataset.py,0,"b'import logging\nfrom collections import OrderedDict\nimport numpy as np\nimport pandas as pd\nimport mxnet as mx\n\nfrom ....utils.loaders import load_pkl\nfrom ....utils.savers import save_pkl\nfrom ...constants import BINARY, MULTICLASS, REGRESSION, SOFTCLASS\n\nlogger = logging.getLogger(__name__) # TODO: Currently unused\n\n\nclass TabularNNDataset:\n    """""" Class for preprocessing & storing/feeding data batches used by tabular data neural networks. Assumes entire dataset can be loaded into numpy arrays.\n        Original Data table may contain numerical, categorical, and text (language) fields.\n\n        Attributes:\n            dataset (mxnet.gluon.data.dataset): Contains the raw data (use dataset._data to access).\n                                                Different indices in this list correspond to different types of inputs to the neural network (each is 2D ND array)\n                                                All vector-valued (continuous & one-hot) features are concatenated together into a single index of the dataset.\n            data_desc (list[str]): Describes the data type of each index of dataset (options: \'vector\',\'embed_<featname>\', \'language_<featname>\')\n            dataloader (mxnet.gluon.data.DataLoader): Loads batches of data from dataset for neural net training and inference.\n            embed_indices (list): which columns in dataset correspond to embed features (order matters!)\n            language_indices (list): which columns in dataset correspond to language features (order matters!)\n            vecfeature_col_map (dict): maps vector_feature_name ->  columns of dataset._data[vector] array that contain the data for this feature\n            feature_dataindex_map (dict): maps feature_name -> i such that dataset._data[i] = data array for this feature. Cannot be used for vector-valued features, instead use vecfeature_col_map\n            feature_groups (dict): maps feature_type (ie. \'vector\' or \'embed\' or \'language\') to list of feature names of this type (empty list if there are no features of this type)\n            vectordata_index (int): describes which element of the dataset._data list holds the vector data matrix (access via self.dataset._data[self.vectordata_index]); None if no vector features\n            label_index (int): describing which element of the dataset._data list holds labels (access via self.dataset._data[self.label_index].asnumpy()); None if no labels\n            num_categories_per_embedfeature (list): Number of categories for each embedding feature (order matters!)\n            num_examples (int): number of examples in this dataset\n            num_features (int): number of features (we only consider original variables as features, so num_features may not correspond to dimensionality of the data eg in the case of one-hot encoding)\n            num_classes (int): number of classes (only used for multiclass classification)\n\n        Note: Default numerical data-type is converted to float32 (as well as labels in regression).\n    """"""\n\n    DATAOBJ_SUFFIX = \'_tabNNdataset.pkl\' # hard-coded names for files. This file contains pickled TabularNNDataset object\n    DATAVALUES_SUFFIX = \'_tabNNdata.npz\' # This file contains raw data values as data_list of NDArrays\n\n    def __init__(self, processed_array, feature_arraycol_map, feature_type_map, batch_size, num_dataloading_workers, problem_type,\n                 labels=None, is_test=True):\n        """""" Args:\n                processed_array: 2D numpy array returned by preprocessor. Contains raw data of all features as columns\n                feature_arraycol_map (OrderedDict): Mapsfeature-name -> list of column-indices in processed_array corresponding to this feature\n                feature_type_map (OrderedDict): Maps feature-name -> feature_type string (options: \'vector\', \'embed\', \'language\')\n                labels (pd.Series): list of labels (y) if available\n                batch_size (int): number of examples to put in each mini-batch\n                num_dataloading_workers (int): number of threads to devote to loading mini-batches of data rather than model-training\n        """"""\n        self.dataset = None\n        self.dataloader = None\n        self.problem_type = problem_type\n        self.num_examples = processed_array.shape[0]\n        self.num_features = len(feature_arraycol_map) # number of features (!=dim(processed_array) because some features may be vector-valued, eg one-hot)\n        self.batch_size = min(self.num_examples, batch_size)\n        self.is_test = is_test\n        self.num_dataloading_workers = num_dataloading_workers\n        last_batch_size = self.num_examples % self.batch_size\n        if last_batch_size == 0:\n            last_batch_size = self.batch_size\n        # TODO: The code fixes the crash on mxnet gluon interpreting a single value in a batch incorrectly.\n        #  Comment out to see crash if data would have single row as final batch on test prediction (such as 1025 rows for batch size 512)\n        if (self.num_examples != 1) and self.is_test and (last_batch_size == 1):\n            init_batch_size = self.batch_size\n            while last_batch_size == 1:\n                self.batch_size = self.batch_size + 1\n                last_batch_size = self.num_examples % self.batch_size\n                if last_batch_size == 0:\n                    last_batch_size = self.batch_size\n                if self.batch_size > init_batch_size+10:\n                    # Hard set to avoid potential infinite loop, don\'t think its mathematically possible to reach this code however.\n                    self.batch_size = self.num_examples\n                    last_batch_size = 0\n\n        if feature_arraycol_map.keys() != feature_type_map.keys():\n            raise ValueError(""feature_arraycol_map and feature_type_map must share same keys"")\n        self.feature_groups = {\'vector\': [], \'embed\': [], \'language\': []} # maps feature_type -> list of feature_names (order is preserved in list)\n        self.feature_type_map = feature_type_map\n        for feature in feature_type_map:\n            if feature_type_map[feature] == \'vector\':\n                self.feature_groups[\'vector\'].append(feature)\n            elif feature_type_map[feature] == \'embed\':\n                self.feature_groups[\'embed\'].append(feature)\n            elif feature_type_map[feature] == \'language\':\n                self.feature_groups[\'language\'].append(feature)\n            else:\n                raise ValueError(""unknown feature type: %s"" % feature)\n\n        if not self.is_test and labels is None:\n            raise ValueError(""labels must be provided when is_test = False"")\n        if labels is not None and len(labels) != self.num_examples:\n            raise ValueError(""number of labels and training examples do not match"")\n\n        data_list = [] # stores all data of each feature-type in list used to construct MXNet dataset. Each index of list = 2D NDArray.\n        self.label_index = None # int describing which element of the dataset._data list holds labels\n        self.data_desc = [] # describes feature-type of each index of data_list\n        self.vectordata_index = None # int describing which element of the dataset._data list holds the vector data matrix\n        self.vecfeature_col_map = {} # maps vector_feature_name ->  columns of dataset._data[vector] array that contain data for this feature\n        self.feature_dataindex_map = {} # maps feature_name -> i such that dataset._data[i] = data array for this feature. Cannot be used for vector-valued features, instead use: self.vecfeature_col_map\n\n        if len(self.feature_groups[\'vector\']) > 0:\n            vector_inds = [] # columns of processed_array corresponding to vector data\n            for feature in feature_type_map:\n                if feature_type_map[feature] == \'vector\':\n                    current_last_ind = len(vector_inds) # current last index of the vector datamatrix\n                    vector_inds += feature_arraycol_map[feature]\n                    new_last_ind = len(vector_inds) # new last index of the vector datamatrix\n                    self.vecfeature_col_map[feature] = list(range(current_last_ind, new_last_ind))\n            data_list.append(mx.nd.array(processed_array[:,vector_inds], dtype=\'float32\')) # Matrix of data from all vector features\n            self.data_desc.append(""vector"")\n            self.vectordata_index = len(data_list) - 1\n\n        if len(self.feature_groups[\'embed\']) > 0:\n            for feature in feature_type_map:\n                if feature_type_map[feature] == \'embed\':\n                    feature_colind = feature_arraycol_map[feature]\n                    data_list.append(mx.nd.array(processed_array[:,feature_colind], dtype=\'int32\')) # array of ints with data for this embedding feature\n                    self.data_desc.append(""embed"")\n                    self.feature_dataindex_map[feature]  = len(data_list)-1\n\n        if len(self.feature_groups[\'language\']) > 0:\n            for feature in feature_type_map:\n                if feature_type_map[feature] == \'language\':\n                    feature_colinds = feature_arraycol_map[feature]\n                    data_list.append(mx.nd.array(processed_array[:,feature_colinds], dtype=\'int32\')) # array of ints with data for this language feature\n                    self.data_desc.append(""language"")\n                    self.feature_dataindex_map[feature]  = len(data_list)-1\n\n        self.num_classes = None\n        if labels is not None:\n            labels = np.array(labels)\n            self.data_desc.append(""label"")\n            self.label_index = len(data_list) # To access data labels, use: self.dataset._data[self.label_index]\n            self.num_classes = None\n            if self.problem_type == SOFTCLASS:\n                self.num_classes = labels.shape[1]\n                data_list.append(mx.nd.array(labels))\n            else:\n                if self.problem_type == REGRESSION and labels.dtype != np.float32:\n                    labels = labels.astype(\'float32\') # Convert to proper float-type if not already\n                elif self.problem_type in [BINARY, MULTICLASS]:\n                    self.num_classes = len(set(labels))\n                data_list.append(mx.nd.array(labels.reshape(len(labels),1)))\n\n        self.embed_indices = [i for i in range(len(self.data_desc)) if \'embed\' in self.data_desc[i]] # list of indices of embedding features in self.dataset, order matters!\n        self.language_indices = [i for i in range(len(self.data_desc)) if \'language\' in self.data_desc[i]]  # list of indices of language features in self.dataset, order matters!\n        self.num_categories_per_embed_feature = None\n        self.generate_dataset_and_dataloader(data_list=data_list)\n        if not self.is_test:\n            self.num_categories_per_embedfeature = self.getNumCategoriesEmbeddings()\n\n    def generate_dataset_and_dataloader(self, data_list):\n        self.dataset = mx.gluon.data.dataset.ArrayDataset(*data_list)  # Access ith embedding-feature via: self.dataset._data[self.data_desc.index(\'embed_\'+str(i))].asnumpy()\n        self.dataloader = mx.gluon.data.DataLoader(self.dataset, self.batch_size, shuffle=not self.is_test,\n                                                   last_batch=\'keep\' if self.is_test else \'rollover\',\n                                                   num_workers=self.num_dataloading_workers)  # no need to shuffle test data\n\n    def has_vector_features(self):\n        """""" Returns boolean indicating whether this dataset contains vector features """"""\n        return self.vectordata_index is not None\n\n    def num_embed_features(self):\n        """""" Returns number of embed features in this dataset """"""\n        return len(self.feature_groups[\'embed\'])\n\n    def num_language_features(self):\n        """""" Returns number of language features in this dataset """"""\n        return len(self.feature_groups[\'language\'])\n\n    def num_vector_features(self):\n        """""" Number of vector features (each onehot feature counts = 1, regardless of how many categories) """"""\n        return len(self.feature_groups[\'vector\'])\n\n    def get_labels(self):\n        """""" Returns numpy array of labels for this dataset """"""\n        if self.label_index is not None:\n            if self.problem_type == SOFTCLASS:\n                return self.dataset._data[self.label_index].asnumpy()\n            else:\n                return self.dataset._data[self.label_index].asnumpy().flatten()\n        else:\n            return None\n\n    def getNumCategoriesEmbeddings(self):\n        """""" Returns number of categories for each embedding feature.\n            Should only be applied to training data.\n            If training data feature contains unique levels 1,...,n-1, there are actually n categories,\n            since category n is reserved for unknown test-time categories.\n        """"""\n        if self.num_categories_per_embed_feature is not None:\n            return self.num_categories_per_embedfeature\n        else:\n            num_embed_feats = self.num_embed_features()\n            num_categories_per_embedfeature = [0] * num_embed_feats\n            for i in range(num_embed_feats):\n                feat_i = self.feature_groups[\'embed\'][i]\n                feat_i_data = self.get_feature_data(feat_i).flatten().tolist()\n                num_categories_i = len(set(feat_i_data)) # number of categories for ith feature\n                num_categories_per_embedfeature[i] = num_categories_i + 1 # to account for unknown test-time categories\n            return num_categories_per_embedfeature\n\n    def get_feature_data(self, feature, asnumpy=True):\n        """""" Returns all data for this feature.\n            Args:\n                feature (str): name of feature of interest (in processed dataframe)\n                asnumpy (bool): should we return 2D numpy array or MXNet NDarray\n        """"""\n        nonvector_featuretypes = set([\'embed\', \'language\'])\n        if feature not in self.feature_type_map:\n            raise ValueError(""unknown feature encountered: %s"" % feature)\n        if self.feature_type_map[feature] == \'vector\':\n            vector_datamatrix = self.dataset._data[self.vectordata_index] # does not work for one-hot...\n            feature_data = vector_datamatrix[:, self.vecfeature_col_map[feature]]\n        elif self.feature_type_map[feature] in nonvector_featuretypes:\n            feature_idx = self.feature_dataindex_map[feature]\n            feature_data = self.dataset._data[feature_idx]\n        else:\n            raise ValueError(""Unknown feature specified: "" % feature)\n        if asnumpy:\n            return feature_data.asnumpy()\n        else:\n            return feature_data\n\n    def get_feature_batch(self, feature, data_batch, asnumpy=False):\n        """""" Returns part of this batch corresponding to data from a single feature\n            Args:\n                data_batch (nd.array): the batch of data as provided by self.dataloader\n            Returns:\n\n        """"""\n        nonvector_featuretypes = set([\'embed\', \'language\'])\n        if feature not in self.feature_type_map:\n            raise ValueError(""unknown feature encountered: %s"" % feature)\n        if self.feature_type_map[feature] == \'vector\':\n            vector_datamatrix = data_batch[self.vectordata_index]\n            feature_data = vector_datamatrix[:, self.vecfeature_col_map[feature]]\n        elif self.feature_type_map[feature] in nonvector_featuretypes:\n            feature_idx = self.feature_dataindex_map[feature]\n            feature_data = data_batch[feature_idx]\n        else:\n            raise ValueError(""Unknown feature specified: "" % feature)\n        if asnumpy:\n            return feature_data.asnumpy()\n        else:\n            return feature_data\n\n    def format_batch_data(self, data_batch, ctx):\n        """""" Partitions data from this batch into different data types.\n            Args:\n                data_batch (nd.array): the batch of data as provided by self.dataloader\n            Returns:\n                formatted_batch (dict): {\'vector\': array of vector_datamatrix,\n                                         \'embed\': list of embedding features\' batch data,\n                                         \'language\': list of language features batch data,\n                                         \'label\': array of labels}\n                                        where each key in dict may be missing.\n        """"""\n        if not isinstance(data_batch, list):\n            data_batch = [data_batch] # Need to convert to list if dimension was dropped during batching\n\n        if len(data_batch[0].shape) == 1:\n            data_batch[0] = data_batch[0].expand_dims(axis=0)\n        formatted_batch = {}\n        if self.has_vector_features(): # None if there is no vector data\n            formatted_batch[\'vector\'] = data_batch[self.vectordata_index].as_in_context(ctx)\n        if self.num_embed_features() > 0:\n            formatted_batch[\'embed\'] = []\n            for i in self.embed_indices:\n                formatted_batch[\'embed\'].append(data_batch[i].as_in_context(ctx))\n        if self.num_language_features() > 0:\n            formatted_batch[\'language\'] = []\n            for i in self.language_indices:\n                formatted_batch[\'language\'].append(data_batch[i].as_in_context(ctx))\n        if self.label_index is not None: # is None if there are no labels\n            formatted_batch[\'label\'] = data_batch[self.label_index].as_in_context(ctx)\n\n        return formatted_batch\n\n    def mask_features_batch(self, features, mask_value, data_batch):\n        """""" Returns new batch where all values of the indicated features have been replaced by the provided mask_value.\n            Args:\n                features (list[str]): list of feature names that should be masked.\n                mask_value (float): value of mask which original feature values should be replaced by. If None, we replace by mean/mode/unknown\n                data_batch (nd.array): the batch of data as provided by self.dataloader\n            Returns:\n                new_batch (nd.array): batch of masked data in same format as data_batch\n        """"""\n        return None # TODO\n\n    def save(self, file_prefix=""""):\n        """""" Additional naming changes will be appended to end of file_prefix (must contain full absolute path) """"""\n        dataobj_file = file_prefix + self.DATAOBJ_SUFFIX\n        datalist_file = file_prefix + self.DATAVALUES_SUFFIX\n        data_list = self.dataset._data\n        self.dataset = None  # Avoid pickling these\n        self.dataloader = None\n        save_pkl.save(path=dataobj_file, object=self)\n        mx.nd.save(datalist_file, data_list)\n        logger.debug(""TabularNN Dataset saved to files: \\n %s \\n %s"" % (dataobj_file, datalist_file))\n\n    @classmethod\n    def load(cls, file_prefix=""""):\n        """""" Additional naming changes will be appended to end of file_prefix (must contain full absolute path) """"""\n        dataobj_file = file_prefix + cls.DATAOBJ_SUFFIX\n        datalist_file = file_prefix + cls.DATAVALUES_SUFFIX\n        dataset: TabularNNDataset = load_pkl.load(path=dataobj_file)\n        data_list = mx.nd.load(datalist_file)\n        dataset.generate_dataset_and_dataloader(data_list=data_list)\n        logger.debug(""TabularNN Dataset loaded from files: \\n %s \\n %s"" % (dataobj_file, datalist_file))\n        return dataset\n'"
autogluon/utils/tabular/ml/models/tabular_nn/tabular_nn_model.py,0,"b'"""""" MXNet neural networks for tabular data containing numerical, categorical, and text fields.\n    First performs neural network specific pre-processing of the data.\n    Contains separate input modules which are applied to different columns of the data depending on the type of values they contain:\n    - Numeric columns are pased through single Dense layer (binary categorical variables are treated as numeric)\n    - Categorical columns are passed through separate Embedding layers\n    - Text columns are passed through separate LanguageModel layers\n    Vectors produced by different input layers are then concatenated and passed to multi-layer MLP model with problem_type determined output layer.\n    Hyperparameters are passed as dict params, including options for preprocessing stages.\n""""""\nimport random, json, time, os, logging, warnings\nfrom collections import OrderedDict\nimport numpy as np\nimport pandas as pd\nimport mxnet as mx\nfrom mxnet import nd, autograd, gluon\nfrom gluoncv.utils import LRSequential, LRScheduler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, QuantileTransformer  # PowerTransformer\n\nfrom ......core import Space\nfrom ......utils import try_import_mxboard\nfrom ......task.base import BasePredictor\nfrom ....utils.loaders import load_pkl\nfrom ..abstract.abstract_model import AbstractModel, fixedvals_from_searchspaces\nfrom ....utils.savers import save_pkl\nfrom ...constants import BINARY, MULTICLASS, REGRESSION, SOFTCLASS\nfrom ....metrics import log_loss\nfrom .categorical_encoders import OneHotMergeRaresHandleUnknownEncoder, OrdinalMergeRaresHandleUnknownEncoder\nfrom .tabular_nn_dataset import TabularNNDataset\nfrom .embednet import EmbedNet\nfrom .tabular_nn_trial import tabular_nn_trial\nfrom .hyperparameters.parameters import get_default_param\nfrom .hyperparameters.searchspaces import get_default_searchspace\n\n# __all__ = [\'TabularNeuralNetModel\', \'EPS\']\n\nwarnings.filterwarnings(""ignore"", module=\'sklearn.preprocessing\') # sklearn processing n_quantiles warning\nlogger = logging.getLogger(__name__)\nEPS = 1e-10 # small number\n\n\n# TODO: Gets stuck after infering feature types near infinitely in nyc-jiashenliu-515k-hotel-reviews-data-in-europe dataset, 70 GB of memory, c5.9xlarge\n#  Suspect issue is coming from embeddings due to text features with extremely large categorical counts.\nclass TabularNeuralNetModel(AbstractModel):\n    """""" Class for neural network models that operate on tabular data.\n        These networks use different types of input layers to process different types of data in various columns.\n\n        Attributes:\n            types_of_features (dict): keys = \'continuous\', \'skewed\', \'onehot\', \'embed\', \'language\'; values = column-names of Dataframe corresponding to the features of this type\n            feature_arraycol_map (OrderedDict): maps feature-name -> list of column-indices in df corresponding to this feature\n        self.feature_type_map (OrderedDict): maps feature-name -> feature_type string (options: \'vector\', \'embed\', \'language\')\n        processor (sklearn.ColumnTransformer): scikit-learn preprocessor object.\n\n        Note: This model always assumes higher values of self.objective_func indicate better performance.\n\n    """"""\n\n    # Constants used throughout this class:\n    # model_internals_file_name = \'model-internals.pkl\' # store model internals here\n    unique_category_str = \'!missing!\' # string used to represent missing values and unknown categories for categorical features. Should not appear in the dataset\n    # TODO: remove: metric_map = {REGRESSION: \'Rsquared\', BINARY: \'accuracy\', MULTICLASS: \'accuracy\'}  # string used to represent different evaluation metrics. metric_map[self.problem_type] produces str corresponding to metric used here.\n    # TODO: should be using self.objective_func as the metric of interest. Should have method: get_metric_name(self.objective_func)\n    rescale_losses = {gluon.loss.L1Loss:\'std\', gluon.loss.HuberLoss:\'std\', gluon.loss.L2Loss:\'var\'} # dict of loss names where we should rescale loss, value indicates how to rescale. Call self.loss_func.name\n    params_file_name = \'net.params\' # Stores parameters of final network\n    temp_file_name = \'temp_net.params\' # Stores temporary network parameters (eg. during the course of training)\n\n    def __init__(self, path: str, name: str, problem_type: str, objective_func, stopping_metric=None, hyperparameters=None, features=None, **kwargs):\n        super().__init__(path=path, name=name, problem_type=problem_type, objective_func=objective_func, stopping_metric=stopping_metric, hyperparameters=hyperparameters, features=features, **kwargs)\n        """"""\n        TabularNeuralNetModel object.\n\n        Parameters\n        ----------\n        path (str): file-path to directory where to save files associated with this model\n        name (str): name used to refer to this model\n        problem_type (str): what type of prediction problem is this model used for\n        objective_func (func): function used to evaluate performance (Note: we assume higher = better)\n        hyperparameters (dict): various hyperparameters for neural network and the NN-specific data processing\n        features (list): List of predictive features to use, other features are ignored by the model.\n        """"""\n        self.eval_metric_name = self.stopping_metric.name\n        self.feature_types_metadata = None\n        self.types_of_features = None\n        self.feature_arraycol_map = None\n        self.feature_type_map = None\n        self.processor = None # data processor\n        self.summary_writer = None\n        self.ctx = mx.cpu()\n        self.batch_size = None\n        self.num_dataloading_workers = None\n        self.num_dataloading_workers_inference = 0\n        self.params_post_fit = None\n        self.num_net_outputs = None\n        self._architecture_desc = None\n        self.optimizer = None\n        self.verbosity = None\n\n    def _set_default_params(self):\n        """""" Specifies hyperparameter values to use by default """"""\n        default_params = get_default_param(self.problem_type)\n        for param, val in default_params.items():\n            self._set_default_param_value(param, val)\n\n    def _get_default_searchspace(self):\n        return get_default_searchspace(self.problem_type, num_classes=None)\n\n    def set_net_defaults(self, train_dataset, params):\n        """""" Sets dataset-adaptive default values to use for our neural network """"""\n        if (self.problem_type == MULTICLASS) or (self.problem_type == SOFTCLASS):\n            self.num_net_outputs = train_dataset.num_classes\n        elif self.problem_type == REGRESSION:\n            self.num_net_outputs = 1\n            if params[\'y_range\'] is None:  # Infer default y-range\n                y_vals = train_dataset.dataset._data[train_dataset.label_index].asnumpy()\n                min_y = float(min(y_vals))\n                max_y = float(max(y_vals))\n                std_y = np.std(y_vals)\n                y_ext = params[\'y_range_extend\'] * std_y\n                if min_y >= 0: # infer y must be nonnegative\n                    min_y = max(0, min_y-y_ext)\n                else:\n                    min_y = min_y-y_ext\n                if max_y <= 0: # infer y must be non-positive\n                    max_y = min(0, max_y+y_ext)\n                else:\n                    max_y = max_y+y_ext\n                params[\'y_range\'] = (min_y, max_y)\n        elif self.problem_type == BINARY:\n            self.num_net_outputs = 2\n        else:\n            raise ValueError(""unknown problem_type specified: %s"" % self.problem_type)\n\n        if params[\'layers\'] is None:  # Use default choices for MLP architecture\n            if self.problem_type == REGRESSION:\n                default_layer_sizes = [256, 128] # overall network will have 4 layers. Input layer, 256-unit hidden layer, 128-unit hidden layer, output layer.\n            else:\n                default_sizes = [256, 128] # will be scaled adaptively\n                # base_size = max(1, min(self.num_net_outputs, 20)/2.0) # scale layer width based on number of classes\n                base_size = max(1, min(self.num_net_outputs, 100) / 50)  # TODO: Updated because it improved model quality and made training far faster\n                default_layer_sizes = [defaultsize*base_size for defaultsize in default_sizes]\n            # TODO: This gets really large on 100K+ rows... It takes hours on gpu for nyc-albert: 78 float/int features which get expanded to 1734, it also overfits and maxes accuracy on epoch\n            #  LGBM takes 120 seconds on 4 cpu\'s and gets far better accuracy\n            #  Perhaps we should add an order of magnitude to the pre-req with -3, or else scale based on feature count instead of row count.\n            # layer_expansion_factor = np.log10(max(train_dataset.num_examples, 1000)) - 2 # scale layers based on num_training_examples\n            layer_expansion_factor = 1  # TODO: Hardcoded to 1 because it results in both better model quality and far faster training time\n            max_layer_width = params[\'max_layer_width\']\n            params[\'layers\'] = [int(min(max_layer_width, layer_expansion_factor*defaultsize)) for defaultsize in default_layer_sizes]\n\n        if train_dataset.has_vector_features() and params[\'numeric_embed_dim\'] is None:\n            # Use default choices for numeric embedding size\n            vector_dim = train_dataset.dataset._data[train_dataset.vectordata_index].shape[1]  # total dimensionality of vector features\n            prop_vector_features = train_dataset.num_vector_features() / float(train_dataset.num_features) # Fraction of features that are numeric\n            min_numeric_embed_dim = 32\n            max_numeric_embed_dim = params[\'max_layer_width\']\n            params[\'numeric_embed_dim\'] = int(min(max_numeric_embed_dim, max(min_numeric_embed_dim,\n                                                    params[\'layers\'][0]*prop_vector_features*np.log10(vector_dim+10) )))\n        return\n\n    def fit(self, X_train, Y_train, X_test=None, Y_test=None, time_limit=None, reporter=None, **kwargs):\n        """""" X_train (pd.DataFrame): training data features (not necessarily preprocessed yet)\n            X_test (pd.DataFrame): test data features (should have same column names as Xtrain)\n            Y_train (pd.Series):\n            Y_test (pd.Series): are pandas Series\n            kwargs: Can specify amount of compute resources to utilize (num_cpus, num_gpus).\n        """"""\n        start_time = time.time()\n        params = self.params.copy()\n        self.verbosity = kwargs.get(\'verbosity\', 2)\n        params = fixedvals_from_searchspaces(params)\n        if self.feature_types_metadata is None:\n            raise ValueError(""Trainer class must set feature_types_metadata for this model"")\n        # print(\'features: \', self.features)\n        if \'num_cpus\' in kwargs:\n            self.num_dataloading_workers = max(1, int(kwargs[\'num_cpus\']/2.0))\n        else:\n            self.num_dataloading_workers = 1\n        if self.num_dataloading_workers == 1:\n            self.num_dataloading_workers = 0  # 0 is always faster and uses less memory than 1\n        self.batch_size = params[\'batch_size\']\n        train_dataset, test_dataset = self.generate_datasets(X_train=X_train, y_train=Y_train, params=params, X_test=X_test, y_test=Y_test)\n        logger.log(15, ""Training data for neural network has: %d examples, %d features (%d vector, %d embedding, %d language)"" %\n              (train_dataset.num_examples, train_dataset.num_features,\n               len(train_dataset.feature_groups[\'vector\']), len(train_dataset.feature_groups[\'embed\']),\n               len(train_dataset.feature_groups[\'language\']) ))\n        # self._save_preprocessor() # TODO: should save these things for hyperparam tunning. Need one HP tuner for network-specific HPs, another for preprocessing HPs.\n\n        if \'num_gpus\' in kwargs and kwargs[\'num_gpus\'] >= 1:  # Currently cannot use >1 GPU\n            self.ctx = mx.gpu()  # Currently cannot use more than 1 GPU\n        else:\n            self.ctx = mx.cpu()\n        self.get_net(train_dataset, params=params)\n\n        if time_limit:\n            time_elapsed = time.time() - start_time\n            time_limit = time_limit - time_elapsed\n\n        self.train_net(train_dataset=train_dataset, params=params, test_dataset=test_dataset, initialize=True, setup_trainer=True, time_limit=time_limit, reporter=reporter)\n        self.params_post_fit = params\n        """"""\n        # TODO: if we don\'t want to save intermediate network parameters, need to do something like saving in temp directory to clean up after training:\n        with make_temp_directory() as temp_dir:\n            save_callback = SaveModelCallback(self.model, monitor=self.metric, mode=save_callback_mode, name=self.name)\n            with progress_disabled_ctx(self.model) as model:\n                original_path = model.path\n                model.path = Path(temp_dir)\n                model.fit_one_cycle(self.epochs, self.lr, callbacks=save_callback)\n\n                # Load the best one and export it\n                model.load(self.name)\n                print(f\'Model validation metrics: {model.validate()}\')\n                model.path = original_path\\\n        """"""\n\n    def get_net(self, train_dataset, params):\n        """""" Creates a Gluon neural net and context for this dataset.\n            Also sets up trainer/optimizer as necessary.\n        """"""\n        self.set_net_defaults(train_dataset, params)\n        self.model = EmbedNet(train_dataset=train_dataset, params=params, num_net_outputs=self.num_net_outputs, ctx=self.ctx)\n\n        # TODO: Below should not occur until at time of saving\n        if not os.path.exists(self.path):\n            os.makedirs(self.path)\n\n    def train_net(self, train_dataset, params, test_dataset=None, initialize=True, setup_trainer=True, time_limit=None, reporter=None):\n        """""" Trains neural net on given train dataset, early stops based on test_dataset.\n            Args:\n                train_dataset (TabularNNDataset): training data used to learn network weights\n                test_dataset (TabularNNDataset): validation data used for hyperparameter tuning\n                initialize (bool): set = False to continue training of a previously trained model, otherwise initializes network weights randomly\n                setup_trainer (bool): set = False to reuse the same trainer from a previous training run, otherwise creates new trainer from scratch\n        """"""\n        start_time = time.time()\n        logger.log(15, ""Training neural network for up to %s epochs..."" % params[\'num_epochs\'])\n        seed_value = params.get(\'seed_value\')\n        if seed_value is not None:  # Set seed\n            random.seed(seed_value)\n            np.random.seed(seed_value)\n            mx.random.seed(seed_value)\n        if initialize:  # Initialize the weights of network\n            logging.debug(""initializing neural network..."")\n            self.model.collect_params().initialize(ctx=self.ctx)\n            self.model.hybridize()\n            logging.debug(""initialized"")\n        if setup_trainer:\n            # Also setup mxboard to monitor training if visualizer has been specified:\n            visualizer = params.get(\'visualizer\', \'none\')\n            if visualizer == \'tensorboard\' or visualizer == \'mxboard\':\n                try_import_mxboard()\n                from mxboard import SummaryWriter\n                self.summary_writer = SummaryWriter(logdir=self.path, flush_secs=5, verbose=False)\n            self.optimizer = self.setup_trainer(params=params, train_dataset=train_dataset)\n        best_val_metric = -np.inf  # higher = better\n        val_metric = None\n        best_val_epoch = 0\n        num_epochs = params[\'num_epochs\']\n        if test_dataset is not None:\n            y_test = test_dataset.get_labels()\n        else:\n            y_test = None\n\n        if params[\'loss_function\'] is None:\n            if self.problem_type == REGRESSION:\n                params[\'loss_function\'] = gluon.loss.L1Loss()\n            elif self.problem_type == SOFTCLASS:\n                params[\'loss_function\'] = gluon.loss.SoftmaxCrossEntropyLoss(sparse_label=False, from_logits=self.model.from_logits)\n            else:\n                params[\'loss_function\'] = gluon.loss.SoftmaxCrossEntropyLoss(from_logits=self.model.from_logits)\n\n        loss_func = params[\'loss_function\']\n        epochs_wo_improve = params[\'epochs_wo_improve\']\n        loss_scaling_factor = 1.0  # we divide loss by this quantity to stabilize gradients\n        loss_torescale = [key for key in self.rescale_losses if isinstance(loss_func, key)]\n        if len(loss_torescale) > 0:\n            loss_torescale = loss_torescale[0]\n            if self.rescale_losses[loss_torescale] == \'std\':\n                loss_scaling_factor = np.std(train_dataset.get_labels())/5.0 + EPS  # std-dev of labels\n            elif self.rescale_losses[loss_torescale] == \'var\':\n                loss_scaling_factor = np.var(train_dataset.get_labels())/5.0 + EPS  # variance of labels\n            else:\n                raise ValueError(""Unknown loss-rescaling type %s specified for loss_func==%s"" % (self.rescale_losses[loss_torescale], loss_func))\n\n        if self.verbosity <= 1:\n            verbose_eval = -1  # Print losses every verbose epochs, Never if -1\n        elif self.verbosity == 2:\n            verbose_eval = 50\n        elif self.verbosity == 3:\n            verbose_eval = 10\n        else:\n            verbose_eval = 1\n\n        net_filename = self.path + self.temp_file_name\n        if num_epochs == 0:  # use dummy training loop that stops immediately (useful for using NN just for data preprocessing / debugging)\n            logger.log(20, ""Not training Neural Net since num_epochs == 0.  Neural network architecture is:"")\n            for batch_idx, data_batch in enumerate(train_dataset.dataloader):\n                data_batch = train_dataset.format_batch_data(data_batch, self.ctx)\n                with autograd.record():\n                    output = self.model(data_batch)\n                    labels = data_batch[\'label\']\n                    loss = loss_func(output, labels) / loss_scaling_factor\n                    # print(str(nd.mean(loss).asscalar()), end=""\\r"")  # prints per-batch losses\n                loss.backward()\n                self.optimizer.step(labels.shape[0])\n                if batch_idx > 0:\n                    break\n            self.model.save_parameters(net_filename)\n            logger.log(15, ""untrained Neural Net saved to file"")\n            return\n\n        # Training Loop:\n        for e in range(num_epochs):\n            if e == 0:  # special actions during first epoch:\n                logger.log(15, ""Neural network architecture:"")\n                logger.log(15, str(self.model))  # TODO: remove?\n            cumulative_loss = 0\n            for batch_idx, data_batch in enumerate(train_dataset.dataloader):\n                data_batch = train_dataset.format_batch_data(data_batch, self.ctx)\n                with autograd.record():\n                    output = self.model(data_batch)\n                    labels = data_batch[\'label\']\n                    loss = loss_func(output, labels) / loss_scaling_factor\n                    # print(str(nd.mean(loss).asscalar()), end=""\\r"")  # prints per-batch losses\n                loss.backward()\n                self.optimizer.step(labels.shape[0])\n                cumulative_loss += loss.sum()\n            train_loss = cumulative_loss/float(train_dataset.num_examples)  # training loss this epoch\n            if test_dataset is not None:\n                # val_metric = self.evaluate_metric(test_dataset)  # Evaluate after each epoch\n                val_metric = self.score(X=test_dataset, y=y_test, eval_metric=self.stopping_metric, metric_needs_y_pred=self.stopping_metric_needs_y_pred)\n            if (test_dataset is None) or (val_metric >= best_val_metric) or (e == 0):  # keep training if score has improved\n                if test_dataset is not None:\n                    if not np.isnan(val_metric):\n                        best_val_metric = val_metric\n                best_val_epoch = e\n                # Until functionality is added to restart training from a particular epoch, there is no point in saving params without test_dataset\n                if test_dataset is not None:\n                    self.model.save_parameters(net_filename)\n            if test_dataset is not None:\n                if verbose_eval > 0 and e % verbose_eval == 0:\n                    logger.log(15, ""Epoch %s.  Train loss: %s, Val %s: %s"" %\n                      (e, train_loss.asscalar(), self.eval_metric_name, val_metric))\n                if self.summary_writer is not None:\n                    self.summary_writer.add_scalar(tag=\'val_\'+self.eval_metric_name,\n                                                   value=val_metric, global_step=e)\n            else:\n                if verbose_eval > 0 and e % verbose_eval == 0:\n                    logger.log(15, ""Epoch %s.  Train loss: %s"" % (e, train_loss.asscalar()))\n            if self.summary_writer is not None:\n                self.summary_writer.add_scalar(tag=\'train_loss\', value=train_loss.asscalar(), global_step=e)  # TODO: do we want to keep mxboard support?\n            if reporter is not None:\n                # TODO: Ensure reporter/scheduler properly handle None/nan values after refactor\n                if test_dataset is not None and (not np.isnan(val_metric)):  # TODO: This might work without the if statement\n                    # epoch must be number of epochs done (starting at 1)\n                    reporter(epoch=e+1, validation_performance=val_metric, train_loss=float(train_loss.asscalar()))  # Higher val_metric = better\n            if e - best_val_epoch > epochs_wo_improve:\n                break\n            if time_limit:\n                time_elapsed = time.time() - start_time\n                time_left = time_limit - time_elapsed\n                if time_left <= 0:\n                    logger.log(20, ""\\tRan out of time, stopping training early."")\n                    break\n\n        if test_dataset is not None:\n            self.model.load_parameters(net_filename)  # Revert back to best model\n            try:\n                os.remove(net_filename)\n            except FileNotFoundError:\n                pass\n        if test_dataset is None:\n            logger.log(15, ""Best model found in epoch %d"" % best_val_epoch)\n        else: # evaluate one final time:\n            final_val_metric = self.score(X=test_dataset, y=y_test, eval_metric=self.stopping_metric, metric_needs_y_pred=self.stopping_metric_needs_y_pred)\n            if np.isnan(final_val_metric):\n                final_val_metric = -np.inf\n            logger.log(15, ""Best model found in epoch %d. Val %s: %s"" %\n                  (best_val_epoch, self.eval_metric_name, final_val_metric))\n        self.params_trained[\'num_epochs\'] = best_val_epoch\n        return\n\n    def predict_proba(self, X, preprocess=True):\n        """""" To align predict wiht abstract_model API.\n            Preprocess here only refers to feature processing stesp done by all AbstractModel objects,\n            not tabularNN-specific preprocessing steps.\n            If X is not DataFrame but instead TabularNNDataset object, we can still produce predictions,\n            but cannot use preprocess in this case (needs to be already processed).\n        """"""\n        if isinstance(X, TabularNNDataset):\n            return self._predict_tabular_data(new_data=X, process=False, predict_proba=True)\n        elif isinstance(X, pd.DataFrame):\n            if preprocess:\n                X = self.preprocess(X)\n            return self._predict_tabular_data(new_data=X, process=True, predict_proba=True)\n        else:\n            raise ValueError(""X must be of type pd.DataFrame or TabularNNDataset, not type: %s"" % type(X))\n\n    def _predict_tabular_data(self, new_data, process=True, predict_proba=True):  # TODO ensure API lines up with tabular.Model class.\n        """""" Specific TabularNN method to produce predictions on new (unprocessed) data.\n            Returns 1D numpy array unless predict_proba=True and task is multi-class classification (not binary).\n            Args:\n                new_data (pd.Dataframe or TabularNNDataset): new data to make predictions on.\n                If you want to make prediction for just a single row of new_data, pass in: new_data.iloc[[row_index]]\n                process (bool): should new data be processed (if False, new_data must be TabularNNDataset)\n                predict_proba (bool): should we output class-probabilities (not used for regression)\n        """"""\n        if process:\n            new_data = self.process_test_data(new_data, batch_size=self.batch_size, num_dataloading_workers=self.num_dataloading_workers_inference, labels=None)\n        if not isinstance(new_data, TabularNNDataset):\n            raise ValueError(""new_data must of of type TabularNNDataset if process=False"")\n        if self.problem_type == REGRESSION or not predict_proba:\n            preds = nd.zeros((new_data.num_examples,1))\n        else:\n            preds = nd.zeros((new_data.num_examples, self.num_net_outputs))\n        i = 0\n        for batch_idx, data_batch in enumerate(new_data.dataloader):\n            data_batch = new_data.format_batch_data(data_batch, self.ctx)\n            preds_batch = self.model(data_batch)\n            batch_size = len(preds_batch)\n            if self.problem_type != REGRESSION:\n                if not predict_proba: # need to take argmax\n                    preds_batch = nd.argmax(preds_batch, axis=1, keepdims=True)\n                else: # need to take softmax\n                    preds_batch = nd.softmax(preds_batch, axis=1)\n            preds[i:(i+batch_size)] = preds_batch\n            i = i+batch_size\n        if self.problem_type == REGRESSION or not predict_proba:\n            return preds.asnumpy().flatten() # return 1D numpy array\n        elif self.problem_type == BINARY and predict_proba:\n            preds = preds[:,1].asnumpy() # for binary problems, only return P(Y==+1)\n            if self.stopping_metric == log_loss or self.objective_func == log_loss:\n                # Ensure nonzero predicted probabilities under log-loss:\n                min_pred = 0.0\n                max_pred = 1.0\n                preds =  EPS + ((1 - 2*EPS)/(max_pred - min_pred)) * (preds - min_pred)\n            return preds\n        elif (predict_proba and (self.problem_type == MULTICLASS or self.problem_type == SOFTCLASS) and\n              (self.stopping_metric == log_loss or self.objective_func == log_loss)):\n            # Ensure nonzero predicted probabilities under log-loss:\n            preds = preds.asnumpy()\n            most_negative_rowvals = np.clip(np.min(preds, axis=1), a_min=None, a_max=0)\n            preds = preds - most_negative_rowvals[:,None] # ensure nonnegative rows\n            preds = np.clip(preds, a_min = EPS, a_max = None) # ensure no zeros\n            return preds / preds.sum(axis=1, keepdims=1) # renormalize\n\n        return preds.asnumpy() # return 2D numpy array\n\n    def generate_datasets(self, X_train, y_train, params, X_test=None, y_test=None):\n        impute_strategy = params[\'proc.impute_strategy\']\n        max_category_levels = params[\'proc.max_category_levels\']\n        skew_threshold = params[\'proc.skew_threshold\']\n        embed_min_categories = params[\'proc.embed_min_categories\']\n        use_ngram_features = params[\'use_ngram_features\']\n\n        if isinstance(X_train, TabularNNDataset):\n            train_dataset = X_train\n        else:\n            X_train = self.preprocess(X_train)\n            if self.features is None:\n                self.features = list(X_train.columns)\n            train_dataset = self.process_train_data(\n                df=X_train, labels=y_train, batch_size=self.batch_size, num_dataloading_workers=self.num_dataloading_workers,\n                impute_strategy=impute_strategy, max_category_levels=max_category_levels, skew_threshold=skew_threshold, embed_min_categories=embed_min_categories, use_ngram_features=use_ngram_features,\n            )\n        if X_test is not None:\n            if isinstance(X_test, TabularNNDataset):\n                test_dataset = X_test\n            else:\n                X_test = self.preprocess(X_test)\n                test_dataset = self.process_test_data(df=X_test, labels=y_test, batch_size=self.batch_size, num_dataloading_workers=self.num_dataloading_workers_inference)\n        else:\n            test_dataset = None\n        return train_dataset, test_dataset\n\n    def process_test_data(self, df, batch_size, num_dataloading_workers, labels=None):\n        """""" Process train or test DataFrame into a form fit for neural network models.\n        Args:\n            df (pd.DataFrame): Data to be processed (X)\n            labels (pd.Series): labels to be processed (y)\n            test (bool): Is this test data where each datapoint should be processed separately using predetermined preprocessing steps.\n                         Otherwise preprocessor uses all data to determine propreties like best scaling factors, number of categories, etc.\n        Returns:\n            Dataset object\n        """"""\n        warnings.filterwarnings(""ignore"", module=\'sklearn.preprocessing\') # sklearn processing n_quantiles warning\n        if set(df.columns) != set(self.features):\n            raise ValueError(""Column names in provided Dataframe do not match self.features"")\n        if labels is not None and len(labels) != len(df):\n            raise ValueError(""Number of examples in Dataframe does not match number of labels"")\n        if (self.processor is None or self.types_of_features is None\n           or self.feature_arraycol_map is None or self.feature_type_map is None):\n            raise ValueError(""Need to process training data before test data"")\n        df = self.ensure_onehot_object(df)\n        df = self.processor.transform(df) # 2D numpy array. self.feature_arraycol_map, self.feature_type_map have been previously set while processing training data.\n        return TabularNNDataset(df, self.feature_arraycol_map, self.feature_type_map,\n                                batch_size=batch_size, num_dataloading_workers=num_dataloading_workers,\n                                problem_type=self.problem_type, labels=labels, is_test=True)\n\n    def process_train_data(self, df, batch_size, num_dataloading_workers, impute_strategy, max_category_levels, skew_threshold, embed_min_categories, use_ngram_features, labels):\n        """""" Preprocess training data and create self.processor object that can be used to process future data.\n            This method should only be used once per TabularNeuralNetModel object, otherwise will produce Warning.\n\n        # TODO no label processing for now\n        # TODO: language features are ignored for now\n        # TODO: how to add new features such as time features and remember to do the same for test data?\n        # TODO: no filtering of data-frame columns based on statistics, e.g. categorical columns with all unique variables or zero-variance features.\n                This should be done in default_learner class for all models not just TabularNeuralNetModel...\n        """"""\n        warnings.filterwarnings(""ignore"", module=\'sklearn.preprocessing\')  # sklearn processing n_quantiles warning\n        if set(df.columns) != set(self.features):\n            raise ValueError(""Column names in provided Dataframe do not match self.features"")\n        if labels is None:\n            raise ValueError(""Attempting process training data without labels"")\n        if len(labels) != len(df):\n            raise ValueError(""Number of examples in Dataframe does not match number of labels"")\n\n        self.types_of_features = self._get_types_of_features(df, skew_threshold=skew_threshold, embed_min_categories=embed_min_categories, use_ngram_features=use_ngram_features) # dict with keys: : \'continuous\', \'skewed\', \'onehot\', \'embed\', \'language\', values = column-names of df\n        df = df[self.features]\n        logger.log(15, ""AutoGluon Neural Network infers features are of the following types:"")\n        logger.log(15, json.dumps(self.types_of_features, indent=4))\n        logger.log(15, ""\\n"")\n        df = self.ensure_onehot_object(df)\n        self.processor = self._create_preprocessor(impute_strategy=impute_strategy, max_category_levels=max_category_levels)\n        df = self.processor.fit_transform(df) # 2D numpy array\n        self.feature_arraycol_map = self._get_feature_arraycol_map(max_category_levels=max_category_levels) # OrderedDict of feature-name -> list of column-indices in df corresponding to this feature\n        num_array_cols = np.sum([len(self.feature_arraycol_map[key]) for key in self.feature_arraycol_map]) # should match number of columns in processed array\n        # print(""self.feature_arraycol_map"", self.feature_arraycol_map)\n        # print(""num_array_cols"", num_array_cols)\n        # print(""df.shape"",df.shape)\n        if num_array_cols != df.shape[1]:\n            raise ValueError(""Error during one-hot encoding data processing for neural network. Number of columns in df array does not match feature_arraycol_map."")\n\n        # print(self.feature_arraycol_map)\n        self.feature_type_map = self._get_feature_type_map() # OrderedDict of feature-name -> feature_type string (options: \'vector\', \'embed\', \'language\')\n        # print(self.feature_type_map)\n        return TabularNNDataset(df, self.feature_arraycol_map, self.feature_type_map,\n                                batch_size=batch_size, num_dataloading_workers=num_dataloading_workers,\n                                problem_type=self.problem_type, labels=labels, is_test=False)\n\n    def setup_trainer(self, params, train_dataset=None):\n        """""" Set up optimizer needed for training.\n            Network must first be initialized before this.\n        """"""\n        optimizer_opts = {\'learning_rate\': params[\'learning_rate\'], \'wd\': params[\'weight_decay\'], \'clip_gradient\': params[\'clip_gradient\']}\n        if \'lr_scheduler\' in params and params[\'lr_scheduler\'] is not None:\n            if train_dataset is None:\n                raise ValueError(""train_dataset cannot be None when lr_scheduler is specified."")\n            base_lr = params.get(\'base_lr\', 1e-6)\n            target_lr = params.get(\'target_lr\', 1.0)\n            warmup_epochs = params.get(\'warmup_epochs\', 10)\n            lr_decay = params.get(\'lr_decay\', 0.1)\n            lr_mode = params[\'lr_scheduler\']\n            num_batches = train_dataset.num_examples // params[\'batch_size\']\n            lr_decay_epoch = [max(warmup_epochs, int(params[\'num_epochs\']/3)), max(warmup_epochs+1, int(params[\'num_epochs\']/2)),\n                              max(warmup_epochs+2, int(2*params[\'num_epochs\']/3))]\n            lr_scheduler = LRSequential([\n                LRScheduler(\'linear\', base_lr=base_lr, target_lr=target_lr, nepochs=warmup_epochs, iters_per_epoch=num_batches),\n                LRScheduler(lr_mode, base_lr=target_lr, target_lr=base_lr, nepochs=params[\'num_epochs\'] - warmup_epochs,\n                            iters_per_epoch=num_batches, step_epoch=lr_decay_epoch, step_factor=lr_decay, power=2)\n            ])\n            optimizer_opts[\'lr_scheduler\'] = lr_scheduler\n        if params[\'optimizer\'] == \'sgd\':\n            if \'momentum\' in params:\n                optimizer_opts[\'momentum\'] = params[\'momentum\']\n            optimizer = gluon.Trainer(self.model.collect_params(), \'sgd\', optimizer_opts)\n        elif params[\'optimizer\'] == \'adam\':  # TODO: Can we try AdamW?\n            optimizer = gluon.Trainer(self.model.collect_params(), \'adam\', optimizer_opts)\n        else:\n            raise ValueError(""Unknown optimizer specified: %s"" % params[\'optimizer\'])\n        return optimizer\n\n    def ensure_onehot_object(self, df):\n        """""" Converts all numerical one-hot columns to object-dtype.\n            Note: self.types_of_features must already exist!\n        """"""\n        new_df = df.copy() # To avoid SettingWithCopyWarning\n        for feature in self.types_of_features[\'onehot\']:\n            if df[feature].dtype != \'object\':\n                new_df.loc[:,feature] = df.loc[:,feature].astype(str)\n        return new_df\n\n    def __get_feature_type_if_present(self, feature_type):\n        """""" Returns crude categorization of feature types """"""\n        return self.feature_types_metadata[feature_type] if feature_type in self.feature_types_metadata else []\n\n    def _get_types_of_features(self, df, skew_threshold, embed_min_categories, use_ngram_features):\n        """""" Returns dict with keys: : \'continuous\', \'skewed\', \'onehot\', \'embed\', \'language\', values = ordered list of feature-names falling into each category.\n            Each value is a list of feature-names corresponding to columns in original dataframe.\n            TODO: ensure features with zero variance have already been removed before this function is called.\n        """"""\n        if self.types_of_features is not None:\n            Warning(""Attempting to _get_types_of_features for TabularNeuralNetModel, but previously already did this."")\n\n        # TODO: Consider setting use_ngram_features=True by default once performance is improved\n        if not use_ngram_features:\n            vectorizers_featnames = self.__get_feature_type_if_present(\'vectorizers\')\n            nlp_featnames = self.__get_feature_type_if_present(\'nlp\')\n            self.feature_types_metadata[\'int\'] = [feature for feature in self.feature_types_metadata[\'int\'] if feature not in vectorizers_featnames]\n            self.feature_types_metadata[\'object\'] = [feature for feature in self.feature_types_metadata[\'object\'] if feature not in nlp_featnames]\n\n        categorical_featnames = self.__get_feature_type_if_present(\'object\') + self.__get_feature_type_if_present(\'bool\')\n        continuous_featnames = self.__get_feature_type_if_present(\'float\') + self.__get_feature_type_if_present(\'int\') + self.__get_feature_type_if_present(\'datetime\')\n        language_featnames = [] # TODO: not implemented. This should fetch text features present in the data\n        valid_features = categorical_featnames + continuous_featnames + language_featnames\n        if len(categorical_featnames) + len(continuous_featnames) + len(language_featnames) != df.shape[1]:\n            unknown_features = [feature for feature in df.columns if feature not in valid_features]\n            # print(\'unknown features:\', unknown_features)\n            df = df.drop(columns=unknown_features)\n            self.features = list(df.columns)\n            # raise ValueError(""unknown feature types present in DataFrame"")\n\n        types_of_features = {\'continuous\': [], \'skewed\': [], \'onehot\': [], \'embed\': [], \'language\': []}\n        # continuous = numeric features to rescale\n        # skewed = features to which we will apply power (ie. log / box-cox) transform before normalization\n        # onehot = features to one-hot encode (unknown categories for these features encountered at test-time are encoded as all zeros). We one-hot encode any features encountered that only have two unique values.\n        for feature in self.features:\n            feature_data = df[feature] # pd.Series\n            num_unique_vals = len(feature_data.unique())\n            if num_unique_vals == 2:  # will be onehot encoded regardless of proc.embed_min_categories value\n                types_of_features[\'onehot\'].append(feature)\n            elif feature in continuous_featnames:\n                if np.abs(feature_data.skew()) > skew_threshold:\n                    types_of_features[\'skewed\'].append(feature)\n                else:\n                    types_of_features[\'continuous\'].append(feature)\n            elif feature in categorical_featnames:\n                if num_unique_vals >= embed_min_categories: # sufficiently many cateories to warrant learned embedding dedicated to this feature\n                    types_of_features[\'embed\'].append(feature)\n                else:\n                    types_of_features[\'onehot\'].append(feature)\n            elif feature in language_featnames:\n                types_of_features[\'language\'].append(feature)\n        return types_of_features\n\n    def _get_feature_arraycol_map(self, max_category_levels):\n        """""" Returns OrderedDict of feature-name -> list of column-indices in processed data array corresponding to this feature """"""\n        feature_preserving_transforms = set([\'continuous\',\'skewed\', \'ordinal\', \'language\']) # these transforms do not alter dimensionality of feature\n        feature_arraycol_map = {} # unordered version\n        current_colindex = 0\n        for transformer in self.processor.transformers_:\n            transformer_name = transformer[0]\n            transformed_features = transformer[2]\n            if transformer_name in feature_preserving_transforms:\n                for feature in transformed_features:\n                    if feature in feature_arraycol_map:\n                        raise ValueError(""same feature is processed by two different column transformers: %s"" % feature)\n                    feature_arraycol_map[feature] = [current_colindex]\n                    current_colindex += 1\n            elif transformer_name == \'onehot\':\n                oh_encoder = [step for (name, step) in transformer[1].steps if name == \'onehot\'][0]\n                for i in range(len(transformed_features)):\n                    feature = transformed_features[i]\n                    if feature in feature_arraycol_map:\n                        raise ValueError(""same feature is processed by two different column transformers: %s"" % feature)\n                    oh_dimensionality = min(len(oh_encoder.categories_[i]), max_category_levels+1)\n                    # print(""feature: %s, oh_dimensionality: %s"" % (feature, oh_dimensionality)) # TODO! debug\n                    feature_arraycol_map[feature] = list(range(current_colindex, current_colindex+oh_dimensionality))\n                    current_colindex += oh_dimensionality\n            else:\n                raise ValueError(""unknown transformer encountered: %s"" % transformer_name)\n        if set(feature_arraycol_map.keys()) != set(self.features):\n            raise ValueError(""failed to account for all features when determining column indices in processed array"")\n        return OrderedDict([(key, feature_arraycol_map[key]) for key in feature_arraycol_map])\n\n    def _get_feature_type_map(self):\n        """""" Returns OrderedDict of feature-name -> feature_type string (options: \'vector\', \'embed\', \'language\') """"""\n        if self.feature_arraycol_map is None:\n            raise ValueError(""must first call _get_feature_arraycol_map() before _get_feature_type_map()"")\n        vector_features = self.types_of_features[\'continuous\'] + self.types_of_features[\'skewed\'] + self.types_of_features[\'onehot\']\n        feature_type_map = OrderedDict()\n        for feature_name in self.feature_arraycol_map:\n            if feature_name in vector_features:\n                feature_type_map[feature_name] = \'vector\'\n            elif feature_name in self.types_of_features[\'embed\']:\n                feature_type_map[feature_name] = \'embed\'\n            elif feature_name in self.types_of_features[\'language\']:\n                feature_type_map[feature_name] = \'language\'\n            else:\n                raise ValueError(""unknown feature type encountered"")\n        return feature_type_map\n\n    def _create_preprocessor(self, impute_strategy, max_category_levels):\n        """""" Defines data encoders used to preprocess different data types and creates instance variable which is sklearn ColumnTransformer object """"""\n        if self.processor is not None:\n            Warning(""Attempting to process training data for TabularNeuralNetModel, but previously already did this."")\n        continuous_features = self.types_of_features[\'continuous\']\n        skewed_features = self.types_of_features[\'skewed\']\n        onehot_features = self.types_of_features[\'onehot\']\n        embed_features = self.types_of_features[\'embed\']\n        language_features = self.types_of_features[\'language\']\n        transformers = [] # order of various column transformers in this list is important!\n        if len(continuous_features) > 0:\n            continuous_transformer = Pipeline(steps=[\n                (\'imputer\', SimpleImputer(strategy=impute_strategy)),\n                (\'scaler\', StandardScaler())])\n            transformers.append( (\'continuous\', continuous_transformer, continuous_features) )\n        if len(skewed_features) > 0:\n            power_transformer = Pipeline(steps=[\n                (\'imputer\', SimpleImputer(strategy=impute_strategy)),\n                (\'quantile\', QuantileTransformer(output_distribution=\'normal\')) ]) # Or output_distribution = \'uniform\'\n                # TODO: remove old code: (\'power\', PowerTransformer(method=self.params[\'proc.power_transform_method\'])) ])\n            transformers.append( (\'skewed\', power_transformer, skewed_features) )\n        if len(onehot_features) > 0:\n            onehot_transformer = Pipeline(steps=[\n                (\'imputer\', SimpleImputer(strategy=\'constant\', fill_value=self.unique_category_str)),\n                (\'onehot\', OneHotMergeRaresHandleUnknownEncoder(max_levels=max_category_levels, sparse=False))]) # test-time unknown values will be encoded as all zeros vector\n            transformers.append( (\'onehot\', onehot_transformer, onehot_features) )\n        if len(embed_features) > 0: # Ordinal transformer applied to convert to-be-embedded categorical features to integer levels\n            ordinal_transformer = Pipeline(steps=[\n                (\'imputer\', SimpleImputer(strategy=\'constant\', fill_value=self.unique_category_str)),\n                (\'ordinal\', OrdinalMergeRaresHandleUnknownEncoder(max_levels=max_category_levels))]) # returns 0-n when max_category_levels = n-1. category n is reserved for unknown test-time categories.\n            transformers.append( (\'ordinal\', ordinal_transformer, embed_features) )\n        if len(language_features) > 0:\n            raise NotImplementedError(""language_features cannot be used at the moment"")\n        return ColumnTransformer(transformers=transformers) # numeric features are processed in the same order as in numeric_features vector, so feature-names remain the same.\n\n    def save(self, file_prefix="""", directory=None, return_filename=False, verbose=True):\n        """""" file_prefix (str): Appended to beginning of file-name (does not affect directory in file-path).\n            directory (str): if unspecified, use self.path as directory\n            return_filename (bool): return the file-name corresponding to this save\n        """"""\n        if directory is not None:\n            path = directory + file_prefix\n        else:\n            path = self.path + file_prefix\n\n        params_filepath = path + self.params_file_name\n        # TODO: Don\'t use os.makedirs here, have save_parameters function in tabular_nn_model that checks if local path or S3 path\n        os.makedirs(os.path.dirname(path), exist_ok=True)\n        if self.model is not None:\n            self.model.save_parameters(params_filepath)\n            self._architecture_desc = self.model.architecture_desc\n        temp_model = self.model\n        temp_sw = self.summary_writer\n        self.model = None\n        self.summary_writer = None\n        modelobj_filepath = super().save(file_prefix=file_prefix, directory=directory, return_filename=True, verbose=verbose)\n        self.model = temp_model\n        self.summary_writer = temp_sw\n        self._architecture_desc = None\n        if return_filename:\n            return modelobj_filepath\n\n    @classmethod\n    def load(cls, path, file_prefix="""", reset_paths=False, verbose=True):\n        """""" file_prefix (str): Appended to beginning of file-name.\n            If you want to load files with given prefix, can also pass arg: path = directory+file_prefix\n        """"""\n        path = path + file_prefix\n        obj: TabularNeuralNetModel = load_pkl.load(path=path + cls.model_file_name, verbose=verbose)\n        if reset_paths:\n            obj.set_contexts(path)\n        if obj._architecture_desc is not None:\n            obj.model = EmbedNet(architecture_desc=obj._architecture_desc, ctx=obj.ctx)  # recreate network from architecture description\n            obj._architecture_desc = None\n            # TODO: maybe need to initialize/hybridize??\n            obj.model.load_parameters(path + cls.params_file_name, ctx=obj.ctx)\n            obj.summary_writer = None\n        return obj\n\n    def hyperparameter_tune(self, X_train, X_test, Y_train, Y_test, scheduler_options, **kwargs):\n        time_start = time.time()\n        """""" Performs HPO and sets self.params to best hyperparameter values """"""\n        self.verbosity = kwargs.get(\'verbosity\', 2)\n        logger.log(15, ""Beginning hyperparameter tuning for Neural Network..."")\n        self._set_default_searchspace() # changes non-specified default hyperparams from fixed values to search-spaces.\n        if self.feature_types_metadata is None:\n            raise ValueError(""Trainer class must set feature_types_metadata for this model"")\n        scheduler_func = scheduler_options[0]\n        scheduler_options = scheduler_options[1]\n        if scheduler_func is None or scheduler_options is None:\n            raise ValueError(""scheduler_func and scheduler_options cannot be None for hyperparameter tuning"")\n        num_cpus = scheduler_options[\'resource\'][\'num_cpus\']\n        # num_gpus = scheduler_options[\'resource\'][\'num_gpus\']  # TODO: Currently unused\n\n        params_copy = self.params.copy()\n\n        self.num_dataloading_workers = max(1, int(num_cpus/2.0))\n        self.batch_size = params_copy[\'batch_size\']\n        train_dataset, test_dataset = self.generate_datasets(X_train=X_train, y_train=Y_train, params=params_copy, X_test=X_test, y_test=Y_test)\n        train_path = self.path + ""train""\n        test_path = self.path + ""validation""\n        train_dataset.save(file_prefix=train_path)\n        test_dataset.save(file_prefix=test_path)\n\n        if not np.any([isinstance(params_copy[hyperparam], Space) for hyperparam in params_copy]):\n            logger.warning(""Warning: Attempting to do hyperparameter optimization without any search space (all hyperparameters are already fixed values)"")\n        else:\n            logger.log(15, ""Hyperparameter search space for Neural Network: "")\n            for hyperparam in params_copy:\n                if isinstance(params_copy[hyperparam], Space):\n                    logger.log(15, str(hyperparam)+ "":   ""+str(params_copy[hyperparam]))\n\n        util_args = dict(\n            train_path=train_path,\n            test_path=test_path,\n            model=self,\n            time_start=time_start,\n            time_limit=scheduler_options[\'time_out\']\n        )\n        tabular_nn_trial.register_args(util_args=util_args, **params_copy)\n        scheduler = scheduler_func(tabular_nn_trial, **scheduler_options)\n        if (\'dist_ip_addrs\' in scheduler_options) and (len(scheduler_options[\'dist_ip_addrs\']) > 0):\n            # TODO: Ensure proper working directory setup on remote machines\n            # This is multi-machine setting, so need to copy dataset to workers:\n            logger.log(15, ""Uploading preprocessed data to remote workers..."")\n            scheduler.upload_files([train_path+TabularNNDataset.DATAOBJ_SUFFIX,\n                                train_path+TabularNNDataset.DATAVALUES_SUFFIX,\n                                test_path+TabularNNDataset.DATAOBJ_SUFFIX,\n                                test_path+TabularNNDataset.DATAVALUES_SUFFIX])  # TODO: currently does not work.\n            logger.log(15, ""uploaded"")\n\n        scheduler.run()\n        scheduler.join_jobs()\n        scheduler.get_training_curves(plot=False, use_legend=False)\n\n        return self._get_hpo_results(scheduler=scheduler, scheduler_options=scheduler_options, time_start=time_start)\n\n    def get_info(self):\n        info = super().get_info()\n        info[\'hyperparameters_post_fit\'] = self.params_post_fit\n        return info\n\n    def reduce_memory_size(self, remove_fit=True, requires_save=True, **kwargs):\n        super().reduce_memory_size(remove_fit=remove_fit, requires_save=requires_save, **kwargs)\n        if remove_fit and requires_save:\n            self.optimizer = None\n\n\n"""""" General TODOs:\n\n- Automatically decrease batch-size if memory issue arises\n\n- Retrain final NN on full dataset (train+val). How to ensure stability here?\n- OrdinalEncoder class in sklearn currently cannot handle rare categories or unknown ones at test-time, so we have created our own Encoder in category_encoders.py\nThere is open PR in sklearn to address this: https://github.com/scikit-learn/scikit-learn/pull/13833/files\nCurrently, our code uses category_encoders package (BSD license) instead: https://github.com/scikit-learn-contrib/categorical-encoding\nOnce PR is merged into sklearn, may want to switch: category_encoders.Ordinal -> sklearn.preprocessing.OrdinalEncoder in preprocess_train_data()\n\n- Save preprocessed data so that we can do HPO of neural net hyperparameters more efficiently, while also doing HPO of preprocessing hyperparameters?\n      Naive full HPO method requires redoing preprocessing in each trial even if we did not change preprocessing hyperparameters.\n      Alternative is we save each proprocessed dataset & corresponding TabularNeuralNetModel object with its unique param names in the file. Then when we try a new HP-config, we first try loading from file if one exists.\n\n""""""\n'"
autogluon/utils/tabular/ml/models/tabular_nn/tabular_nn_trial.py,0,"b'import logging\n\nfrom .tabular_nn_dataset import TabularNNDataset\nfrom ..abstract import model_trial\nfrom ....utils.exceptions import TimeLimitExceeded\nfrom ......core import args\n\nlogger = logging.getLogger(__name__)\n\n\n@args()\ndef tabular_nn_trial(args, reporter):\n    """""" Training and evaluation function used during a single trial of HPO """"""\n    try:\n        model, args, util_args = model_trial.prepare_inputs(args=args)\n\n        train_dataset = TabularNNDataset.load(util_args.train_path)\n        test_dataset = TabularNNDataset.load(util_args.test_path)\n        y_test = test_dataset.get_labels()\n\n        fit_model_args = dict(X_train=train_dataset, Y_train=None, X_test=test_dataset)\n        predict_proba_args = dict(X=test_dataset)\n        model_trial.fit_and_save_model(model=model, params=args, fit_args=fit_model_args, predict_proba_args=predict_proba_args, y_test=y_test,\n                                       time_start=util_args.time_start, time_limit=util_args.get(\'time_limit\', None), reporter=reporter)\n    except Exception as e:\n        if not isinstance(e, TimeLimitExceeded):\n            logger.exception(e, exc_info=True)\n        reporter.terminate()\n'"
autogluon/utils/tabular/ml/models/xt/__init__.py,0,b''
autogluon/utils/tabular/ml/models/xt/xt_model.py,0,"b'from sklearn.ensemble import ExtraTreesClassifier, ExtraTreesRegressor\n\nfrom ..rf.rf_model import RFModel\nfrom ...constants import REGRESSION\n\n\nclass XTModel(RFModel):\n    def _get_model_type(self):\n        if self.problem_type == REGRESSION:\n            return ExtraTreesRegressor\n        else:\n            return ExtraTreesClassifier\n'"
autogluon/utils/tabular/ml/trainer/model_presets/__init__.py,0,b''
autogluon/utils/tabular/ml/trainer/model_presets/presets.py,0,"b'import copy\nimport logging\nfrom collections import defaultdict\n\nfrom sklearn.linear_model import LogisticRegression, LinearRegression\n\nfrom ...constants import AG_ARGS, AG_ARGS_FIT, BINARY, MULTICLASS, REGRESSION, SOFTCLASS, PROBLEM_TYPES_CLASSIFICATION\nfrom ...models.abstract.abstract_model import AbstractModel\nfrom ...models.lgb.lgb_model import LGBModel\nfrom ...models.lr.lr_model import LinearModel\nfrom ...models.tabular_nn.tabular_nn_model import TabularNeuralNetModel\nfrom ...models.rf.rf_model import RFModel\nfrom ...models.knn.knn_model import KNNModel\nfrom ...models.catboost.catboost_model import CatboostModel\nfrom ...models.xt.xt_model import XTModel\nfrom ....metrics import soft_log_loss\n\nlogger = logging.getLogger(__name__)\n\n# Higher values indicate higher priority, priority dictates the order models are trained for a given level.\nDEFAULT_MODEL_PRIORITY = dict(\n    RF=100,\n    XT=90,\n    KNN=80,\n    GBM=70,\n    CAT=60,\n    NN=50,\n    LR=40,\n    custom=0,\n)\n\nMODEL_TYPES = dict(\n    RF=RFModel,\n    XT=XTModel,\n    KNN=KNNModel,\n    GBM=LGBModel,\n    CAT=CatboostModel,\n    NN=TabularNeuralNetModel,\n    LR=LinearModel,\n)\n\nDEFAULT_MODEL_NAMES = {\n    RFModel: \'RandomForest\',\n    XTModel: \'ExtraTrees\',\n    KNNModel: \'KNeighbors\',\n    LGBModel: \'LightGBM\',\n    CatboostModel: \'Catboost\',\n    TabularNeuralNetModel: \'NeuralNet\',\n    LinearModel: \'LinearModel\',\n}\n\n\ndef _dd_classifier():\n    return \'Classifier\'\n\n\ndef _dd_regressor():\n    return \'Regressor\'\n\n\nDEFAULT_MODEL_TYPE_SUFFIX = dict(\n    classifier=defaultdict(_dd_classifier),\n    regressor=defaultdict(_dd_regressor),\n)\nDEFAULT_MODEL_TYPE_SUFFIX[\'classifier\'].update({LinearModel: \'\'})\nDEFAULT_MODEL_TYPE_SUFFIX[\'regressor\'].update({LinearModel: \'\'})\n\n\n# DONE: Add levels, including \'default\'\n# DONE: Add lists\n# DONE: Add custom which can append to lists\n# DONE: Add special optional AG args for things like name prefix, name suffix, name, etc.\n# TODO: Move creation of stack ensemble internally into this function? Requires passing base models in as well.\n# DONE: Add special optional AG args for training order\n# TODO: Add special optional AG args for base models\n# TODO: Consider making hyperparameters arg in fit() accept lists, concatenate hyperparameter sets together.\n# TODO: Consider adding special optional AG args for #cores,#gpus,num_early_stopping_iterations,etc.\n# TODO: Consider adding special optional AG args for max train time, max memory size, etc.\n# TODO: Consider adding special optional AG args for use_original_features,features_to_use,etc.\n# TODO: Consider adding optional AG args to dynamically disable models such as valid_num_classes_range, valid_row_count_range, valid_feature_count_range, etc.\n# TODO: Args such as max_repeats, num_folds\n# TODO: Add banned_model_types arg\n# TODO: Add option to update hyperparameters with only added keys, so disabling CatBoost would just be {\'CAT\': []}, which keeps the other models as is.\n# TODO: special optional AG arg for only training model if eval_metric in list / not in list. Useful for F1 and \'is_unbalanced\' arg in LGBM.\ndef get_preset_models(path, problem_type, objective_func, hyperparameters, stopping_metric=None, num_classes=None, hyperparameter_tune=False, level=\'default\', extra_ag_args_fit=None, name_suffix=\'\'):\n    if problem_type not in [BINARY, MULTICLASS, REGRESSION]:\n        raise NotImplementedError\n\n    if level in hyperparameters.keys():\n        level_key = level\n    else:\n        level_key = \'default\'\n    hp_level = hyperparameters[level_key]\n    priority_dict = defaultdict(list)\n    for model_type in hp_level:\n        for model in hp_level[model_type]:\n            model = copy.deepcopy(model)\n            try:\n                model_priority = model[AG_ARGS][\'priority\']\n            except:\n                model_priority = DEFAULT_MODEL_PRIORITY[model_type]\n            if AG_ARGS not in model:\n                model[AG_ARGS] = dict()\n            if \'model_type\' not in model[AG_ARGS]:\n                model[AG_ARGS][\'model_type\'] = model_type\n            # Check if model is valid\n            if hyperparameter_tune and model[AG_ARGS].get(\'disable_in_hpo\', False):\n                continue  # Not valid\n            priority_dict[model_priority].append(model)\n    model_priority_list = [model for priority in sorted(priority_dict.keys(), reverse=True) for model in priority_dict[priority]]\n    model_names_set = set()\n    models = []\n    for model in model_priority_list:\n        model_type = model[AG_ARGS][\'model_type\']\n        if not isinstance(model_type, AbstractModel):\n            model_type = MODEL_TYPES[model_type]\n        name_orig = model[AG_ARGS].get(\'name\', None)\n        if name_orig is None:\n            name_main = model[AG_ARGS].get(\'name_main\', DEFAULT_MODEL_NAMES[model_type])\n            name_prefix = model[AG_ARGS].get(\'name_prefix\', \'\')\n            name_type_suffix = model[AG_ARGS].get(\'name_type_suffix\', None)\n            if name_type_suffix is None:\n                suffix_key = \'classifier\' if problem_type in PROBLEM_TYPES_CLASSIFICATION else \'regressor\'\n                name_type_suffix = DEFAULT_MODEL_TYPE_SUFFIX[suffix_key][model_type]\n            name_suff = model[AG_ARGS].get(\'name_suffix\', \'\')\n            name_orig = name_prefix + name_main + name_type_suffix + name_suff\n        name = name_orig\n        num_increment = 2\n        while name in model_names_set:  # Ensure name is unique\n            name = f\'{name_orig}_{num_increment}\'\n            num_increment += 1\n        model_names_set.add(name)\n        model_params = copy.deepcopy(model)\n        model_params.pop(AG_ARGS)\n        if extra_ag_args_fit is not None:\n            if AG_ARGS_FIT not in model_params:\n                model_params[AG_ARGS_FIT] = {}\n            model_params[AG_ARGS_FIT].update(extra_ag_args_fit.copy())  # TODO: Consider case of overwriting user specified extra args.\n        model_init = model_type(path=path, name=name, problem_type=problem_type, objective_func=objective_func, stopping_metric=stopping_metric, num_classes=num_classes, hyperparameters=model_params)\n        models.append(model_init)\n\n    for model in models:\n        model.rename(model.name + name_suffix)\n\n    return models\n\n\ndef get_preset_stacker_model(path, problem_type, objective_func, num_classes=None,\n                             hyperparameters={\'NN\': {}, \'GBM\': {}}, hyperparameter_tune=False):\n    # TODO: Expand options to RF and NN\n    if problem_type == REGRESSION:\n        model = RFModel(path=path, name=\'LinearRegression\', model=LinearRegression(),\n                        problem_type=problem_type, objective_func=objective_func)\n    else:\n        model = RFModel(path=path, name=\'LogisticRegression\', model=LogisticRegression(\n            solver=\'liblinear\', multi_class=\'auto\', max_iter=500,  # n_jobs=-1  # TODO: HP set to hide warnings, but we should find optimal HP for this\n        ), problem_type=problem_type, objective_func=objective_func)\n    return model\n\n\ndef get_preset_models_softclass(path, hyperparameters={}, hyperparameter_tune=False, name_suffix=\'\'):\n    # print(""Neural Net is currently the only model supported for multi-class distillation."")\n    models = []\n    # TODO: only NN supported for now. add other models. We use a big NN for distillation to ensure it has high capacity to approximate ensemble:\n    nn_options = {\'num_epochs\': 500, \'dropout_prob\': 0, \'weight_decay\': 1e-7, \'epochs_wo_improve\': 50, \'layers\': [2048]*2 + [512], \'numeric_embed_dim\': 2048, \'activation\': \'softrelu\', \'embedding_size_factor\': 2.0}\n    models.append(\n        TabularNeuralNetModel(path=path, name=\'NeuralNetSoftClassifier\', problem_type=SOFTCLASS,\n                              objective_func=soft_log_loss, stopping_metric=soft_log_loss, hyperparameters=nn_options.copy())\n    )\n    rf_options = dict(criterion=\'mse\')\n    models.append(\n        RFModel(path=path, name=\'RandomForestRegressorMSE\', problem_type=REGRESSION,\n                objective_func=soft_log_loss, hyperparameters=rf_options),\n    )\n\n    for model in models:\n        model.rename(model.name + name_suffix)\n\n    return models\n'"
autogluon/utils/tabular/ml/trainer/model_presets/presets_custom.py,0,"b""\nfrom ...constants import AG_ARGS\nfrom ...models.lgb.hyperparameters.parameters import get_param_baseline_custom\n\n\n# Returns list of models created by a custom name preset.\ndef get_preset_custom(name, problem_type, num_classes):\n    if not isinstance(name, str):\n        raise ValueError(f'Expected string value for custom model, but was given {name}')\n    # Custom model\n    if name == 'GBM':\n        model = get_param_baseline_custom(problem_type, num_classes=num_classes)\n        model[AG_ARGS] = dict(model_type='GBM', name_suffix='Custom', disable_in_hpo=True)\n        return [model]\n    else:\n        raise ValueError(f'Unknown custom model preset: {name}')\n"""
autogluon/utils/tabular/ml/models/catboost/hyperparameters/__init__.py,0,b''
autogluon/utils/tabular/ml/models/catboost/hyperparameters/parameters.py,0,"b""from ....constants import BINARY, MULTICLASS, REGRESSION\n\nDEFAULT_ITERATIONS = 10000\n\n\ndef get_param_baseline(problem_type, num_classes=None):\n    if problem_type == BINARY:\n        return get_param_binary_baseline()\n    elif problem_type == MULTICLASS:\n        return get_param_multiclass_baseline(num_classes=num_classes)\n    elif problem_type == REGRESSION:\n        return get_param_regression_baseline()\n    else:\n        return get_param_binary_baseline()\n\n\ndef get_param_binary_baseline():\n    params = {\n        'iterations': DEFAULT_ITERATIONS,\n        'learning_rate': 0.1,\n    }\n    return params\n\n\ndef get_param_multiclass_baseline(num_classes):\n    params = {\n        'iterations': DEFAULT_ITERATIONS,\n        'learning_rate': 0.1,\n    }\n    return params\n\n\ndef get_param_regression_baseline():\n    params = {\n        'iterations': DEFAULT_ITERATIONS,\n        'learning_rate': 0.1,\n    }\n    return params\n"""
autogluon/utils/tabular/ml/models/catboost/hyperparameters/searchspaces.py,0,"b'"""""" Default hyperparameter search spaces used in CatBoost Boosting model """"""\nfrom ....constants import BINARY, MULTICLASS, REGRESSION\nfrom .......core import Real, Int\n\n\ndef get_default_searchspace(problem_type, num_classes=None):\n    if problem_type == BINARY:\n        return get_searchspace_binary_baseline()\n    elif problem_type == MULTICLASS:\n        return get_searchspace_multiclass_baseline(num_classes=num_classes)\n    elif problem_type == REGRESSION:\n        return get_searchspace_regression_baseline()\n    else:\n        return get_searchspace_binary_baseline()\n\n\ndef get_searchspace_multiclass_baseline(num_classes):\n    params = {\n        \'learning_rate\': Real(lower=5e-3, upper=0.2, default=0.1, log=True),\n        \'depth\': Int(lower=5, upper=8, default=6),\n        \'l2_leaf_reg\': Real(lower=1, upper=5, default=3),\n    }\n    return params\n\n\ndef get_searchspace_binary_baseline():\n    params = {\n        \'learning_rate\': Real(lower=5e-3, upper=0.2, default=0.1, log=True),\n        \'depth\': Int(lower=5, upper=8, default=6),\n        \'l2_leaf_reg\': Real(lower=1, upper=5, default=3),\n    }\n    return params\n\n\ndef get_searchspace_regression_baseline():\n    params = {\n        \'learning_rate\': Real(lower=5e-3, upper=0.2, default=0.1, log=True),\n        \'depth\': Int(lower=5, upper=8, default=6),\n        \'l2_leaf_reg\': Real(lower=1, upper=5, default=3),\n    }\n    return params\n'"
autogluon/utils/tabular/ml/models/lgb/hyperparameters/__init__.py,0,b''
autogluon/utils/tabular/ml/models/lgb/hyperparameters/lgb_trial.py,0,"b'import logging\n\nfrom ...abstract import model_trial\nfrom .....utils.loaders import load_pkl\nfrom .....utils.exceptions import TimeLimitExceeded\nfrom ......try_import import try_import_lightgbm\nfrom .......core import args\n\nlogger = logging.getLogger(__name__)\n\n\n# TODO: Further simplify to align with model_trial.py\n# FIXME: Hyperband does not work with LightGBM\n# FIXME: If stopping metric != eval_metric, score will be wrong!\n@args()\ndef lgb_trial(args, reporter):\n    """""" Training script for hyperparameter evaluation of Gradient Boosting model """"""\n    try:\n        model, args, util_args = model_trial.prepare_inputs(args=args)\n\n        try_import_lightgbm()\n        import lightgbm as lgb\n\n        dataset_train = lgb.Dataset(util_args.directory + util_args.dataset_train_filename)\n        dataset_val = lgb.Dataset(util_args.directory + util_args.dataset_val_filename)\n        X_val, y_val = load_pkl.load(util_args.directory + util_args.dataset_val_pkl_filename)\n\n        fit_model_args = dict(dataset_train=dataset_train, dataset_val=dataset_val)\n        predict_proba_args = dict(X=X_val)\n        model_trial.fit_and_save_model(model=model, params=args, fit_args=fit_model_args, predict_proba_args=predict_proba_args, y_test=y_val,\n                                       time_start=util_args.time_start, time_limit=util_args.get(\'time_limit\', None), reporter=reporter)\n    except Exception as e:\n        if not isinstance(e, TimeLimitExceeded):\n            logger.exception(e, exc_info=True)\n        reporter.terminate()\n\n    # FIXME: If stopping metric and eval metric differ, the previous reported scores will not align as they will be evaluated with stopping_metric, whereas this is evaluated with eval_metric\n    #  This should only impact if the reporter data is used\n    # FIXME: If stopping metric score > eval metric score, stopping metric score will be recorded as best score, this is a defect!\n    # FIXME: It might be the case that if a reporter has been recorded and the model crash, AutoGluon will try to access the invalid model and fail.\n    # reporter(epoch=model.params_trained[\'num_boost_round\'] + 1, validation_performance=score)\n'"
autogluon/utils/tabular/ml/models/lgb/hyperparameters/parameters.py,0,"b'"""""" Default (fixed) hyperparameter values used in Gradient Boosting model. """"""\n\nfrom ....constants import BINARY, MULTICLASS, REGRESSION\n\nDEFAULT_NUM_BOOST_ROUND = 10000  # default for single training run\n\n\ndef get_param_baseline_custom(problem_type, num_classes=None):\n    if problem_type == BINARY:\n        return get_param_binary_baseline_custom()\n    elif problem_type == MULTICLASS:\n        return get_param_multiclass_baseline_custom(num_classes=num_classes)\n    elif problem_type == REGRESSION:\n        return get_param_regression_baseline_custom()\n    else:\n        return get_param_binary_baseline_custom()\n\n\ndef get_param_baseline(problem_type, num_classes=None):\n    if problem_type == BINARY:\n        return get_param_binary_baseline()\n    elif problem_type == MULTICLASS:\n        return get_param_multiclass_baseline(num_classes=num_classes)\n    elif problem_type == REGRESSION:\n        return get_param_regression_baseline()\n    else:\n        return get_param_binary_baseline()\n\n\ndef get_param_multiclass_baseline_custom(num_classes):\n    params = {\n        \'num_boost_round\': DEFAULT_NUM_BOOST_ROUND,\n        \'num_threads\': -1,\n        \'objective\': \'multiclass\',\n        \'metric\': \'multi_error,multi_logloss\',\n        \'num_classes\': num_classes,\n        \'verbose\': -1,\n        \'boosting_type\': \'gbdt\',\n        \'learning_rate\': 0.03,\n        \'num_leaves\': 128,\n        \'feature_fraction\': 0.9,\n        \'min_data_in_leaf\': 3,\n        \'two_round\': True,\n        \'seed_value\': 0,\n        # \'device\': \'gpu\'  # needs GPU-enabled lightGBM build\n        # TODO: Bin size max increase\n    }\n    return params.copy()\n\n\ndef get_param_binary_baseline():\n    params = {\n        \'num_boost_round\': DEFAULT_NUM_BOOST_ROUND,\n        \'num_threads\': -1,\n        \'objective\': \'binary\',\n        \'metric\': \'binary_logloss,binary_error\',\n        \'verbose\': -1,\n        \'boosting_type\': \'gbdt\',\n        \'two_round\': True,\n    }\n    return params\n\n\ndef get_param_multiclass_baseline(num_classes):\n    params = {\n        \'num_boost_round\': DEFAULT_NUM_BOOST_ROUND,\n        \'num_threads\': -1,\n        \'objective\': \'multiclass\',\n        \'metric\': \'multi_error,multi_logloss\',\n        \'num_classes\': num_classes,\n        \'verbose\': -1,\n\n        \'boosting_type\': \'gbdt\',\n        \'two_round\': True,\n    }\n    return params\n\n\ndef get_param_regression_baseline():\n    params = {\n        \'num_boost_round\': DEFAULT_NUM_BOOST_ROUND,\n        \'num_threads\': -1,\n        \'objective\': \'regression\',\n        \'metric\': \'regression\',\n        \'verbose\': -1,\n        \'boosting_type\': \'gbdt\',\n        \'two_round\': True,\n    }\n    return params\n\n\ndef get_param_binary_baseline_dummy_gpu():\n    params = {\n        \'num_boost_round\': DEFAULT_NUM_BOOST_ROUND,\n        \'num_threads\': -1,\n        \'objective\': \'binary\',\n        \'metric\': \'binary_logloss,binary_error\',\n        \'verbose\': -1,\n        \'boosting_type\': \'gbdt\',\n        \'two_round\': True,\n        \'device_type\': \'gpu\',\n    }\n    return params\n\n\ndef get_param_binary_baseline_custom():\n    params = {\n        \'num_boost_round\': DEFAULT_NUM_BOOST_ROUND,\n        \'num_threads\': -1,\n        \'objective\': \'binary\',\n        \'metric\': \'binary_logloss,binary_error\',\n        \'verbose\': -1,\n        \'boosting_type\': \'gbdt\',\n        \'learning_rate\': 0.03,\n        \'num_leaves\': 128,\n        \'feature_fraction\': 0.9,\n        \'min_data_in_leaf\': 5,\n        # \'is_unbalance\': True,  # TODO: Set is_unbalanced: True for F1-score, AUC!\n        \'two_round\': True,\n        \'seed_value\': 0,\n    }\n    return params.copy()\n\n\ndef get_param_regression_baseline_custom():\n    params = {\n        \'num_boost_round\': DEFAULT_NUM_BOOST_ROUND,\n        \'num_threads\': -1,\n        \'objective\': \'regression\',\n        \'metric\': \'regression\',\n        \'verbose\': -1,\n        \'boosting_type\': \'gbdt\',\n        \'learning_rate\': 0.03,\n        \'num_leaves\': 128,\n        \'feature_fraction\': 0.9,\n        \'min_data_in_leaf\': 5,\n        \'two_round\': True,\n        \'seed_value\': 0,\n    }\n    return params.copy()\n'"
autogluon/utils/tabular/ml/models/lgb/hyperparameters/searchspaces.py,0,"b'"""""" Default hyperparameter search spaces used in Gradient Boosting model """"""\nfrom .......core import Real, Int\nfrom ....constants import BINARY, MULTICLASS, REGRESSION\n\nDEFAULT_NUM_BOOST_ROUND = 10000  # default for HPO\n\n\ndef get_default_searchspace(problem_type, num_classes=None):\n    if problem_type == BINARY:\n        return get_searchspace_binary_baseline()\n    elif problem_type == MULTICLASS:\n        return get_searchspace_multiclass_baseline(num_classes=num_classes)\n    elif problem_type == REGRESSION:\n        return get_searchspace_regression_baseline()\n    else:\n        return get_searchspace_binary_baseline()\n\n\ndef get_searchspace_multiclass_baseline(num_classes):\n    params = {\n        \'objective\': \'multiclass\',\n        \'metric\': \'multi_error,multi_logloss\',\n        \'num_classes\': num_classes,\n        \'learning_rate\': Real(lower=5e-3, upper=0.2, default=0.1, log=True),\n        \'feature_fraction\': Real(lower=0.75, upper=1.0, default=1.0),\n        \'min_data_in_leaf\': Int(lower=2, upper=30, default=20),  # TODO: Use size of dataset to set upper, if row count is small upper should be small\n        \'num_leaves\': Int(lower=16, upper=96, default=31),  # TODO: Use row count and feature count to set this, the higher feature count the higher num_leaves upper\n        \'num_boost_round\': DEFAULT_NUM_BOOST_ROUND,\n        \'boosting_type\': \'gbdt\',\n        \'verbose\': -1,\n        \'two_round\': True,\n        \'seed_value\': None,\n        # \'device\': \'gpu\'  # needs GPU-enabled lightGBM build\n        # TODO: Bin size max increase\n    }\n    return params\n\n\ndef get_searchspace_binary_baseline():\n    params = {\n        \'objective\': \'binary\',\n        \'metric\': \'binary_logloss,binary_error\',\n        \'learning_rate\': Real(lower=5e-3, upper=0.2, default=0.1, log=True),\n        \'feature_fraction\': Real(lower=0.75, upper=1.0, default=1.0),\n        \'min_data_in_leaf\': Int(lower=2, upper=30, default=20),\n        \'num_leaves\': Int(lower=16, upper=96, default=31),\n        \'num_boost_round\': DEFAULT_NUM_BOOST_ROUND,\n        \'boosting_type\': \'gbdt\',\n        \'verbose\': -1,\n        \'two_round\': True,\n        \'seed_value\': None,\n    }\n    return params\n\n\ndef get_searchspace_regression_baseline():\n    params = {\n        \'objective\': \'regression\',\n        \'metric\': \'l2\',\n        \'learning_rate\': Real(lower=5e-3, upper=0.2, default=0.1, log=True),\n        \'feature_fraction\': Real(lower=0.75, upper=1.0, default=1.0),\n        \'min_data_in_leaf\': Int(lower=2, upper=30, default=20),\n        \'num_leaves\': Int(lower=16, upper=96, default=31),\n        \'num_boost_round\': DEFAULT_NUM_BOOST_ROUND,\n        \'boosting_type\': \'gbdt\',\n        \'verbose\': -1,\n        \'two_round\': True,\n        \'seed_value\': None,\n    }\n    return params\n\n\n'"
autogluon/utils/tabular/ml/models/lr/hyperparameters/__init__.py,0,b''
autogluon/utils/tabular/ml/models/lr/hyperparameters/parameters.py,0,"b""import logging\n\nfrom sklearn.linear_model import LogisticRegression, Ridge, Lasso\n\nfrom .....ml.constants import BINARY\nfrom .....ml.constants import REGRESSION\n\nL1 = 'L1'\nL2 = 'L2'\nIGNORE = 'ignore'\nONLY = 'only'\nINCLUDE = 'include'\n\nlogger = logging.getLogger(__name__)\n\n\ndef get_param_baseline():\n    default_params = {\n        'C': 1,\n        'vectorizer_dict_size': 75000,  # size of TFIDF vectorizer dictionary; used only in text model\n        'proc.ngram_range': (1, 5),  # range of n-grams for TFIDF vectorizer dictionary; used only in text model\n        'proc.skew_threshold': 0.99,  # numerical features whose absolute skewness is greater than this receive special power-transform preprocessing. Choose big value to avoid using power-transforms\n        'proc.impute_strategy': 'median',  # strategy argument of sklearn.SimpleImputer() used to impute missing numeric values\n        'penalty': L2,  # regularization to use with regression models\n        'handle_text': IGNORE, # how text should be handled: `ignore` - don't use NLP features; `only` - only use NLP features; `include` - use both regular and NLP features\n    }\n    return default_params\n\n\ndef get_model_params(problem_type: str, hyperparameters):\n    penalty = hyperparameters.get('penalty', L2)\n    handle_text = hyperparameters.get('handle_text', IGNORE)\n    if problem_type == REGRESSION:\n        if penalty == L2:\n            model_class = Ridge\n        elif penalty == L1:\n            model_class = Lasso\n        else:\n            logger.warning('Unknown value for penalty {} - supported types are [l1, l2] - falling back to l2'.format(penalty))\n            penalty = L2\n            model_class = Ridge\n    else:\n        model_class = LogisticRegression\n\n    return model_class, penalty, handle_text\n\n\ndef get_default_params(problem_type: str, penalty: str):\n    # TODO: get seed from seeds provider\n    if problem_type == REGRESSION:\n        default_params = {'C': None, 'random_state': 0, 'fit_intercept': True}\n        if penalty == L2:\n            default_params['solver'] = 'auto'\n    else:\n        default_params = {'C': None, 'random_state': 0, 'solver': _get_solver(problem_type), 'n_jobs': -1, 'fit_intercept': True}\n    model_params = list(default_params.keys())\n    return model_params, default_params\n\n\ndef _get_solver(problem_type):\n    if problem_type == BINARY:\n        # TODO explore using liblinear for smaller datasets\n        solver = 'lbfgs'\n    else:\n        solver = 'lbfgs'\n    return solver\n"""
autogluon/utils/tabular/ml/models/lr/hyperparameters/searchspaces.py,0,"b""def get_default_searchspace(problem_type, num_classes=None):\n    spaces = {\n        # 'C': Real(lower=1e-4, upper=1e5, default=1),\n        # 'tokenizer': Categorical('split', 'sentencepiece'),\n        # 'fit_intercept': Categorical(True', False),\n    }\n    return spaces\n"""
autogluon/utils/tabular/ml/models/tabular_nn/hyperparameters/__init__.py,0,b''
autogluon/utils/tabular/ml/models/tabular_nn/hyperparameters/parameters.py,0,"b'"""""" Default (fixed) hyperparameter values used in Neural network model """"""\n\nfrom ....constants import BINARY, MULTICLASS, REGRESSION\n\n\ndef get_fixed_params():\n    """""" Parameters that currently cannot be searched during HPO """"""\n    fixed_params = {\n        \'num_epochs\': 500,  # maximum number of epochs for training NN\n        \'epochs_wo_improve\': 20,  # we terminate training if validation performance hasn\'t improved in the last \'epochs_wo_improve\' # of epochs\n        # TODO: Epochs could take a very long time, we may want smarter logic than simply # of epochs without improvement (slope, difference in score, etc.)\n        \'seed_value\': None,  # random seed for reproducibility (set = None to ignore)\n        # For data processing:\n        \'proc.embed_min_categories\': 4,  # apply embedding layer to categorical features with at least this many levels. Features with fewer levels are one-hot encoded. Choose big value to avoid use of Embedding layers\n        # Options: [3,4,10, 100, 1000]\n        \'proc.impute_strategy\': \'median\',  # # strategy argument of sklearn.SimpleImputer() used to impute missing numeric values\n        # Options: [\'median\', \'mean\', \'most_frequent\']\n        \'proc.max_category_levels\': 100,  # maximum number of allowed levels per categorical feature\n        # Options: [10, 100, 200, 300, 400, 500, 1000, 10000]\n        \'proc.skew_threshold\': 0.99,  # numerical features whose absolute skewness is greater than this receive special power-transform preprocessing. Choose big value to avoid using power-transforms\n        # Options: [0.2, 0.3, 0.5, 0.8, 1.0, 10.0, 100.0]\n        # Old params: These are now set based off of nthreads_per_trial, ngpus_per_trial.\n        # \'num_dataloading_workers\': 1,  # Will be overwritten by nthreads_per_trial, can be >= 1 \n        # \'ctx\': mx.cpu(),  # Will be overwritten by ngpus_per_trial if unspecified (can alternatively be: mx.gpu())\n    }\n    return fixed_params\n\n\ndef get_hyper_params():\n    """""" Parameters that currently can be tuned during HPO """"""\n    hyper_params = {\n        ## Hyperparameters for neural net architecture:\n        \'network_type\': \'widedeep\',  # Type of neural net used to produce predictions\n        # Options: [\'widedeep\', \'feedforward\']\n        \'layers\': None,  # List of widths (num_units) for each hidden layer (Note: only specifies hidden layers. These numbers are not absolute, they will also be scaled based on number of training examples and problem type)\n        # Options: List of lists that are manually created\n        \'numeric_embed_dim\': None,  # Size of joint embedding for all numeric+one-hot features.\n        # Options: integer values between 10-10000\n        \'activation\': \'relu\',  # Activation function\n        # Options: [\'relu\', \'softrelu\', \'tanh\', \'softsign\']\n        \'max_layer_width\': 2056,  # maximum number of hidden units in network layer (integer > 0)\n        # Does not need to be searched by default\n        \'embedding_size_factor\': 1.0,  # scaling factor to adjust size of embedding layers (float > 0)\n        # Options: range[0.01 - 100] on log-scale\n        \'embed_exponent\': 0.56,  # exponent used to determine size of embedding layers based on # categories.\n        \'max_embedding_dim\': 100,  # maximum size of embedding layer for a single categorical feature (int > 0).\n        ## Regression-specific hyperparameters:\n        \'y_range\': None,  # Tuple specifying whether (min_y, max_y). Can be = (-np.inf, np.inf).\n        # If None, inferred based on training labels. Note: MUST be None for classification tasks!\n        \'y_range_extend\': 0.05,  # Only used to extend size of inferred y_range when y_range = None.\n        ## Hyperparameters for neural net training:\n        \'use_batchnorm\': True,  # whether or not to utilize Batch-normalization\n        # Options: [True, False]\n        \'dropout_prob\': 0.1,  # dropout probability, = 0 turns off Dropout.\n        # Options: range(0.0, 0.5)\n        \'batch_size\': 512,  # batch-size used for NN training\n        # Options: [32, 64, 128. 256, 512, 1024, 2048]\n        \'loss_function\': None,  # MXNet loss function minimized during training\n        \'optimizer\': \'adam\',  # MXNet optimizer to use.\n        # Options include: [\'adam\',\'sgd\']\n        \'learning_rate\': 3e-4,  # learning rate used for NN training (float > 0)\n        \'weight_decay\': 1e-6,  # weight decay regularizer (float > 0)\n        \'clip_gradient\': 100.0,  # gradient clipping threshold (float > 0)\n        \'momentum\': 0.9,  # momentum which is only used for SGD optimizer\n        \'lr_scheduler\': None,  # If not None, string specifying what type of learning rate scheduler to use (may override learning_rate).\n        # Options: [None, \'cosine\', \'step\', \'poly\', \'constant\']\n        # Below are hyperparameters specific to the LR scheduler (only used if lr_scheduler != None). For more info, see: https://gluon-cv.mxnet.io/api/utils.html#gluoncv.utils.LRScheduler\n        \'base_lr\': 3e-5,  # smallest LR (float > 0)\n        \'target_lr\': 1.0,  # largest LR (float > 0)\n        \'lr_decay\': 0.1,  # step factor used to decay LR (float in (0,1))\n        \'warmup_epochs\': 10,  # number of epochs at beginning of training in which LR is linearly ramped up (float > 1).\n        ## Feature-specific hyperparameters:\n        \'use_ngram_features\': False,  # If False, will drop automatically generated ngram features from language features. This results in worse model quality but far faster inference and training times.\n        # Options: [True, False]\n    }\n    return hyper_params\n\n\n# Note: params for original NNTabularModel were:\n# weight_decay=0.01, dropout_prob = 0.1, batch_size = 2048, lr = 1e-2, epochs=30, layers= [200, 100] (semi-equivalent to our layers = [100],numeric_embed_dim=200)\ndef get_default_param(problem_type, num_classes=None):\n    if problem_type == BINARY:\n        return get_param_binary()\n    elif problem_type == MULTICLASS:\n        return get_param_multiclass(num_classes=num_classes)\n    elif problem_type == REGRESSION:\n        return get_param_regression()\n    else:\n        return get_param_binary()\n\n\ndef get_param_multiclass(num_classes):\n    params = get_fixed_params()\n    params.update(get_hyper_params())\n    return params\n\n\ndef get_param_binary():\n    params = get_fixed_params()\n    params.update(get_hyper_params())\n    return params\n\n\ndef get_param_regression():\n    params = get_fixed_params()\n    params.update(get_hyper_params())\n    return params\n'"
autogluon/utils/tabular/ml/models/tabular_nn/hyperparameters/searchspaces.py,0,"b'"""""" Default hyperparameter search spaces used in Neural network model """"""\nfrom ....constants import BINARY, MULTICLASS, REGRESSION\nfrom .......core import Categorical, Real\n\n\ndef get_default_searchspace(problem_type, num_classes=None):\n    if problem_type == BINARY:\n        return get_searchspace_binary().copy()\n    elif problem_type == MULTICLASS:\n        return get_searchspace_multiclass(num_classes=num_classes)\n    elif problem_type == REGRESSION:\n        return get_searchspace_regression().copy()\n    else:\n        return get_searchspace_binary().copy()\n\n\ndef get_searchspace_multiclass(num_classes):\n    # Search space we use by default (only specify non-fixed hyperparameters here):  # TODO: move to separate file\n    params = {\n        \'learning_rate\': Real(1e-4, 3e-2, default=3e-4, log=True),\n        \'weight_decay\': Real(1e-12, 0.1, default=1e-6, log=True),\n        \'dropout_prob\': Real(0.0, 0.5, default=0.1),\n        # \'layers\': Categorical(None, [200, 100], [256], [2056], [1024, 512, 128], [1024, 1024, 1024]),\n        \'layers\': Categorical(None, [200, 100], [256], [100, 50], [200, 100, 50], [50, 25], [300, 150]),\n        \'embedding_size_factor\': Real(0.5, 1.5, default=1.0),\n        \'network_type\': Categorical(\'widedeep\', \'feedforward\'),\n        \'use_batchnorm\': Categorical(True, False),\n        \'activation\': Categorical(\'relu\', \'softrelu\'),\n        # \'batch_size\': Categorical(512, 1024, 2056, 128), # this is used in preprocessing so cannot search atm\n    }\n    return params\n\n\ndef get_searchspace_binary():\n    params = {\n        \'learning_rate\': Real(1e-4, 3e-2, default=3e-4, log=True),\n        \'weight_decay\': Real(1e-12, 0.1, default=1e-6, log=True),\n        \'dropout_prob\': Real(0.0, 0.5, default=0.1),\n        # \'layers\': Categorical(None, [200, 100], [256], [2056], [1024, 512, 128], [1024, 1024, 1024]),\n        \'layers\': Categorical(None, [200, 100], [256], [100, 50], [200, 100, 50], [50, 25], [300, 150]),\n        \'embedding_size_factor\': Real(0.5, 1.5, default=1.0),\n        \'network_type\': Categorical(\'widedeep\', \'feedforward\'),\n        \'use_batchnorm\': Categorical(True, False),\n        \'activation\': Categorical(\'relu\', \'softrelu\'),\n        # \'batch_size\': Categorical(512, 1024, 2056, 128), # this is used in preprocessing so cannot search atm\n    }\n    return params\n\n\ndef get_searchspace_regression():\n    params = {\n        \'learning_rate\': Real(1e-4, 3e-2, default=3e-4, log=True),\n        \'weight_decay\': Real(1e-12, 0.1, default=1e-6, log=True),\n        \'dropout_prob\': Real(0.0, 0.5, default=0.1),\n        # \'layers\': Categorical(None, [200, 100], [256], [2056], [1024, 512, 128], [1024, 1024, 1024]),\n        \'layers\': Categorical(None, [200, 100], [256], [100, 50], [200, 100, 50], [50, 25], [300, 150]),\n        \'embedding_size_factor\': Real(0.5, 1.5, default=1.0),\n        \'network_type\': Categorical(\'widedeep\', \'feedforward\'),\n        \'use_batchnorm\': Categorical(True, False),\n        \'activation\': Categorical(\'relu\', \'softrelu\', \'tanh\'),\n        # \'batch_size\': Categorical(512, 1024, 2056, 128), # this is used in preprocessing so cannot search atm\n    }\n    return params\n'"
