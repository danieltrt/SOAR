file_path,api_count,code
scripts/generate_stats.py,0,"b'from bs4 import BeautifulSoup\nimport math\nimport operator\nimport requests\nimport time\n\ndef search(keywords, n_pages=10, sort=\'stars\'):\n    res = []\n    for k in keywords:\n        for i in range(1, n_pages+1):\n            time.sleep(10)\n            url = ""https://github.com/search?o=desc&q=%s&p=%d&s=%s&type=Repositories"" % (k, i, sort)\n            r = requests.get(url)\n            info = extract_search_info(r.content)\n            for r in info:\n                res.append(r)\n    return res\n\ndef extract_search_info(html):\n    info = []\n    html = BeautifulSoup(html, \'html.parser\')\n    for c in html.find_all(\'li\', {""class"": ""repo-list-item""}):\n        try:\n            stars = str(c.find_all(\'a\', {""class"": ""muted-link""})[-1].text.strip())\n            stars_unparsed = stars\n            if \'k\' in stars:\n                stars = float(stars.replace(\'k\', \'\')) * 1000\n            stars = int(stars)\n            lang = c.find(\'span\', {\'itemprop\': ""programmingLanguage""})\n            lang = lang.text.strip() if lang else lang\n            info.append({\n                ""url"": ""https://github.com"" + c.find(\'a\', {""class"": ""v-align-middle""}).attrs[""href""],\n                ""title"": c.find(\'a\', {""class"": ""v-align-middle""}).text.strip(),\n                ""desc"": c.find(\'p\', {""class"": ""col-12""}).text.strip(),\n                ""stars_unparsed"": stars_unparsed,\n                ""stars"": stars,\n                ""lang"": lang\n            })\n        except Exception as e:\n            print e\n    return info\n\ndef get_topic(keywords, n_pages=10):\n    res = []\n    next_token = None\n    for k in keywords:\n        for i in range(1, n_pages+1):\n            time.sleep(10)\n            url = ""https://github.com/topics/%s"" % k\n            if next_token:\n                url += ""?after=%s"" % next_token\n            r = requests.get(url)\n            info, next_token = extract_topic_info(r.content)\n            for r in info:\n                res.append(r)\n    return res\n\ndef extract_topic_info(html):\n    info = []\n    html = BeautifulSoup(html, \'html.parser\')\n    next_token = html.find(\'input\', {""name"": ""after""}).attrs[""value""]\n    for c in html.find_all(\'article\', {""class"": ""py-4""}):\n        try:\n            stars = str(c.find_all(\'a\', {""class"": ""link-gray""})[-1].text.strip())\n            stars_unparsed = stars\n            if \'k\' in stars:\n                stars = float(stars.replace(\'k\', \'\')) * 1000\n            stars = int(stars)\n            lang = c.find(\'span\', {\'itemprop\': ""programmingLanguage""})\n            lang = lang.text.strip() if lang else lang\n            info.append({\n                ""url"": ""https://github.com"" + c.find(\'h3\').find(\'a\').attrs[""href""],\n                ""title"": c.find(\'h3\').find(\'a\').text.strip().replace("" / "", ""/""),\n                ""desc"": c.find(\'div\', {""class"": ""mb-3""}).text.strip(),\n                ""stars_unparsed"": stars_unparsed,\n                ""stars"": stars,\n                ""lang"": lang\n            })\n        except Exception as e:\n            print e\n    return info, next_token\n\ndef parse_results(results):\n    results = {v[\'url\']:v for v in results}.values()\n    results = sorted(results, key=lambda x: x[\'stars\'], reverse=True)\n    return [r for r in results if r[\'stars\'] >= 1000 and (r[\'lang\'] and r[\'lang\'] != ""TeX"")]\n\ndef build_table(results_list):\n\n    def build_html_fields(d):\n        return [\'<a href=""%s"">%s</a>\' % (d[\'url\'], d[\'title\'].split(\'/\')[-1]), d[\'stars_unparsed\'], d[\'desc\']]\n\n    def build_md_fields(d):\n        return [\'[%s](%s)\' % (d[\'title\'].split(\'/\')[-1], d[\'url\']), d[\'stars_unparsed\'], d[\'desc\']]\n\n    html = \'<table><thead><tr><td>Project Name</td><td>Stars</td><td>Description</td></tr></thead>\'\n    md = \'| Project Name | Stars | Description |\\n| ------- | ------ | ------ |\\n\'\n    for r in results_list:\n        html += \'<tr><td>\' + \'</td><td>\'.join(build_html_fields(r)) + \'</td></tr>\'\n        md += \'|\' + \'|\'.join(build_md_fields(r)) + \'|\\n\'\n    html += \'</table>\'\n    return html, md\n\ntopics = get_topic([\'tensorflow\', \'deep-learning\', \'pytorch\', \'machine-learning\'], n_pages=5)\nsearches = search([\'tensorflow\', \'deep learning\', \'pytorch\', \'cntk\', \'machine learning\'], n_pages=5)\n\nr = parse_results(topics + searches)\n\nprint len(r)\n\nwith open(\'out.html\', \'w\') as f:\n    f.write(build_table(r)[0].encode(\'utf-8\'))\n\nwith open(\'out.md\', \'w\') as f:\n    f.write(build_table(r)[1].encode(\'utf-8\'))\n'"
