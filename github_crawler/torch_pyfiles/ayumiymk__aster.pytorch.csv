file_path,api_count,code
config.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport sys\nsys.path.append(\'./\')\n\nimport six\nimport os\nimport os.path as osp\nimport math\nimport argparse\n\n\nparser = argparse.ArgumentParser(description=""Softmax loss classification"")\n# data\nparser.add_argument(\'--synthetic_train_data_dir\', nargs=\'+\', type=str, metavar=\'PATH\',\n                    default=[\'/share/zhui/reg_dataset/NIPS2014\'])\nparser.add_argument(\'--real_train_data_dir\', type=str, metavar=\'PATH\',\n                    default=\'/data/zhui/benchmark/cocotext_trainval\')\nparser.add_argument(\'--extra_train_data_dir\', nargs=\'+\', type=str, metavar=\'PATH\',\n                    default=[\'/share/zhui/reg_dataset/CVPR2016\'])\nparser.add_argument(\'--test_data_dir\', type=str, metavar=\'PATH\',\n                    default=\'/share/zhui/reg_dataset/IIIT5K_3000\')\nparser.add_argument(\'--MULTI_TRAINDATA\', action=\'store_true\', default=False,\n                    help=\'whether use the extra_train_data for training.\')\nparser.add_argument(\'-b\', \'--batch_size\', type=int, default=128)\nparser.add_argument(\'-j\', \'--workers\', type=int, default=8)\nparser.add_argument(\'--height\', type=int, default=64,\n                    help=""input height, default: 256 for resnet*, """"64 for inception"")\nparser.add_argument(\'--width\', type=int, default=256,\n                    help=""input width, default: 128 for resnet*, """"256 for inception"")\nparser.add_argument(\'--keep_ratio\', action=\'store_true\', default=False,\n                    help=\'length fixed or lenghth variable.\')\nparser.add_argument(\'--voc_type\', type=str, default=\'ALLCASES_SYMBOLS\',\n                    choices=[\'LOWERCASE\', \'ALLCASES\', \'ALLCASES_SYMBOLS\'])\nparser.add_argument(\'--mix_data\', action=\'store_true\',\n                    help=""whether combine multi datasets in the training stage."")\nparser.add_argument(\'--num_train\', type=int, default=math.inf)\nparser.add_argument(\'--num_test\', type=int, default=math.inf)\nparser.add_argument(\'--aug\', action=\'store_true\', default=False,\n                    help=\'whether use data augmentation.\')\nparser.add_argument(\'--lexicon_type\', type=str, default=\'0\', choices=[\'0\', \'50\', \'1k\', \'full\'],\n                    help=\'which lexicon associated to image is used.\')\nparser.add_argument(\'--image_path\', type=str, default=\'\',\n                    help=\'the path of single image, used in demo.py.\')\nparser.add_argument(\'--tps_inputsize\', nargs=\'+\', type=int, default=[32, 64])\nparser.add_argument(\'--tps_outputsize\', nargs=\'+\', type=int, default=[32, 100])\n# model\nparser.add_argument(\'-a\', \'--arch\', type=str, default=\'ResNet_ASTER\')\nparser.add_argument(\'--dropout\', type=float, default=0.5)\nparser.add_argument(\'--max_len\', type=int, default=100)\nparser.add_argument(\'--n_group\', type=int, default=1)\nparser.add_argument(\'--STN_ON\', action=\'store_true\',\n                    help=\'add the stn head.\')\nparser.add_argument(\'--tps_margins\', nargs=\'+\', type=float, default=[0.05,0.05])\nparser.add_argument(\'--stn_activation\', type=str, default=\'none\')\nparser.add_argument(\'--num_control_points\', type=int, default=20)\nparser.add_argument(\'--stn_with_dropout\', action=\'store_true\', default=False)\n## lstm\nparser.add_argument(\'--with_lstm\', action=\'store_true\', default=False,\n                    help=\'whether append lstm after cnn in the encoder part.\')\nparser.add_argument(\'--decoder_sdim\', type=int, default=512,\n                    help=""the dim of hidden layer in decoder."")\nparser.add_argument(\'--attDim\', type=int, default=512,\n                    help=""the dim for attention."")\n# optimizer\nparser.add_argument(\'--lr\', type=float, default=1,\n                    help=""learning rate of new parameters, for pretrained ""\n                         ""parameters it is 10 times smaller than this"")\nparser.add_argument(\'--momentum\', type=float, default=0.9)\nparser.add_argument(\'--weight_decay\', type=float, default=0.0) # the model maybe under-fitting, 0.0 gives much better results.\nparser.add_argument(\'--grad_clip\', type=float, default=1.0)\nparser.add_argument(\'--loss_weights\', nargs=\'+\', type=float, default=[1,1,1])\n# training configs\nparser.add_argument(\'--resume\', type=str, default=\'\', metavar=\'PATH\')\nparser.add_argument(\'--evaluate\', action=\'store_true\',\n                    help=""evaluation only"")\nparser.add_argument(\'--epochs\', type=int, default=6)\nparser.add_argument(\'--start_save\', type=int, default=0,\n                    help=""start saving checkpoints after specific epoch"")\nparser.add_argument(\'--seed\', type=int, default=1)\nparser.add_argument(\'--print_freq\', type=int, default=100)\nparser.add_argument(\'--cuda\', default=True, type=bool,\n                    help=\'whether use cuda support.\')\n# testing configs\nparser.add_argument(\'--evaluation_metric\', type=str, default=\'accuracy\')\nparser.add_argument(\'--evaluate_with_lexicon\', action=\'store_true\', default=False)\nparser.add_argument(\'--beam_width\', type=int, default=5)\n# misc\nworking_dir = osp.dirname(osp.dirname(osp.abspath(__file__)))\nparser.add_argument(\'--logs_dir\', type=str, metavar=\'PATH\',\n                    default=osp.join(working_dir, \'logs\'))\nparser.add_argument(\'--real_logs_dir\', type=str, metavar=\'PATH\',\n                    default=\'/media/mkyang/research/recognition/selfattention_rec\')\nparser.add_argument(\'--debug\', action=\'store_true\',\n                    help=""if debugging, some steps will be passed."")\nparser.add_argument(\'--vis_dir\', type=str, metavar=\'PATH\', default=\'\',\n                    help=""whether visualize the results while evaluation."")\nparser.add_argument(\'--run_on_remote\', action=\'store_true\', default=False,\n                    help=""run the code on remote or local."")\n\ndef get_args(sys_args):\n  global_args = parser.parse_args(sys_args)\n  return global_args'"
demo.py,12,"b'from __future__ import absolute_import\nimport sys\nsys.path.append(\'./\')\n\nimport argparse\nimport os\nimport os.path as osp\nimport numpy as np\nimport math\nimport time\nfrom PIL import Image, ImageFile\n\nimport torch\nfrom torch import nn, optim\nfrom torch.backends import cudnn\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\n\nfrom config import get_args\nfrom lib import datasets, evaluation_metrics, models\nfrom lib.models.model_builder import ModelBuilder\nfrom lib.datasets.dataset import LmdbDataset, AlignCollate\nfrom lib.loss import SequenceCrossEntropyLoss\nfrom lib.trainers import Trainer\nfrom lib.evaluators import Evaluator\nfrom lib.utils.logging import Logger, TFLogger\nfrom lib.utils.serialization import load_checkpoint, save_checkpoint\nfrom lib.utils.osutils import make_symlink_if_not_exists\nfrom lib.evaluation_metrics.metrics import get_str_list\nfrom lib.utils.labelmaps import get_vocabulary, labels2strs\n\nglobal_args = get_args(sys.argv[1:])\n\ndef image_process(image_path, imgH=32, imgW=100, keep_ratio=False, min_ratio=1):\n  img = Image.open(image_path).convert(\'RGB\')\n\n  if keep_ratio:\n    w, h = img.size\n    ratio = w / float(h)\n    imgW = int(np.floor(ratio * imgH))\n    imgW = max(imgH * min_ratio, imgW)\n\n  img = img.resize((imgW, imgH), Image.BILINEAR)\n  img = transforms.ToTensor()(img)\n  img.sub_(0.5).div_(0.5)\n\n  return img\n\nclass DataInfo(object):\n  """"""\n  Save the info about the dataset.\n  This a code snippet from dataset.py\n  """"""\n  def __init__(self, voc_type):\n    super(DataInfo, self).__init__()\n    self.voc_type = voc_type\n\n    assert voc_type in [\'LOWERCASE\', \'ALLCASES\', \'ALLCASES_SYMBOLS\']\n    self.EOS = \'EOS\'\n    self.PADDING = \'PADDING\'\n    self.UNKNOWN = \'UNKNOWN\'\n    self.voc = get_vocabulary(voc_type, EOS=self.EOS, PADDING=self.PADDING, UNKNOWN=self.UNKNOWN)\n    self.char2id = dict(zip(self.voc, range(len(self.voc))))\n    self.id2char = dict(zip(range(len(self.voc)), self.voc))\n\n    self.rec_num_classes = len(self.voc)\n\n\ndef main(args):\n  np.random.seed(args.seed)\n  torch.manual_seed(args.seed)\n  torch.cuda.manual_seed(args.seed)\n  torch.cuda.manual_seed_all(args.seed)\n  cudnn.benchmark = True\n  torch.backends.cudnn.deterministic = True\n\n  args.cuda = args.cuda and torch.cuda.is_available()\n  if args.cuda:\n    print(\'using cuda.\')\n    torch.set_default_tensor_type(\'torch.cuda.FloatTensor\')\n  else:\n    torch.set_default_tensor_type(\'torch.FloatTensor\')\n  \n  # Create data loaders\n  if args.height is None or args.width is None:\n    args.height, args.width = (32, 100)\n\n  dataset_info = DataInfo(args.voc_type)\n\n  # Create model\n  model = ModelBuilder(arch=args.arch, rec_num_classes=dataset_info.rec_num_classes,\n                       sDim=args.decoder_sdim, attDim=args.attDim, max_len_labels=args.max_len,\n                       eos=dataset_info.char2id[dataset_info.EOS], STN_ON=args.STN_ON)\n\n  # Load from checkpoint\n  if args.resume:\n    checkpoint = load_checkpoint(args.resume)\n    model.load_state_dict(checkpoint[\'state_dict\'])\n\n  if args.cuda:\n    device = torch.device(""cuda"")\n    model = model.to(device)\n    model = nn.DataParallel(model)\n\n  # Evaluation\n  model.eval()\n  img = image_process(args.image_path)\n  with torch.no_grad():\n    img = img.to(device)\n  input_dict = {}\n  input_dict[\'images\'] = img.unsqueeze(0)\n  # TODO: testing should be more clean.\n  # to be compatible with the lmdb-based testing, need to construct some meaningless variables.\n  rec_targets = torch.IntTensor(1, args.max_len).fill_(1)\n  rec_targets[:,args.max_len-1] = dataset_info.char2id[dataset_info.EOS]\n  input_dict[\'rec_targets\'] = rec_targets\n  input_dict[\'rec_lengths\'] = [args.max_len]\n  output_dict = model(input_dict)\n  pred_rec = output_dict[\'output\'][\'pred_rec\']\n  pred_str, _ = get_str_list(pred_rec, input_dict[\'rec_targets\'], dataset=dataset_info)\n  print(\'Recognition result: {0}\'.format(pred_str[0]))\n\n\nif __name__ == \'__main__\':\n  # parse the config\n  args = get_args(sys.argv[1:])\n  main(args)'"
main.py,10,"b'from __future__ import absolute_import\nimport sys\nsys.path.append(\'./\')\n\nimport argparse\nimport os\nos.environ[""TF_CPP_MIN_LOG_LEVEL""] = ""3""\n\nimport os.path as osp\nimport numpy as np\nimport math\nimport time\n\nimport torch\nfrom torch import nn, optim\nfrom torch.backends import cudnn\nfrom torch.utils.data import DataLoader, SubsetRandomSampler\n\nfrom config import get_args\nfrom lib import datasets, evaluation_metrics, models\nfrom lib.models.model_builder import ModelBuilder\nfrom lib.datasets.dataset import LmdbDataset, AlignCollate\nfrom lib.datasets.concatdataset import ConcatDataset\nfrom lib.loss import SequenceCrossEntropyLoss\nfrom lib.trainers import Trainer\nfrom lib.evaluators import Evaluator\nfrom lib.utils.logging import Logger, TFLogger\nfrom lib.utils.serialization import load_checkpoint, save_checkpoint\nfrom lib.utils.osutils import make_symlink_if_not_exists\n\nglobal_args = get_args(sys.argv[1:])\n\n\ndef get_data(data_dir, voc_type, max_len, num_samples, height, width, batch_size, workers, is_train, keep_ratio):\n  if isinstance(data_dir, list):\n    dataset_list = []\n    for data_dir_ in data_dir:\n      dataset_list.append(LmdbDataset(data_dir_, voc_type, max_len, num_samples))\n    dataset = ConcatDataset(dataset_list)\n  else:\n    dataset = LmdbDataset(data_dir, voc_type, max_len, num_samples)\n  print(\'total image: \', len(dataset))\n\n  if is_train:\n    data_loader = DataLoader(dataset, batch_size=batch_size, num_workers=workers,\n      shuffle=True, pin_memory=True, drop_last=True,\n      collate_fn=AlignCollate(imgH=height, imgW=width, keep_ratio=keep_ratio))\n  else:\n    data_loader = DataLoader(dataset, batch_size=batch_size, num_workers=workers,\n      shuffle=False, pin_memory=True, drop_last=False,\n      collate_fn=AlignCollate(imgH=height, imgW=width, keep_ratio=keep_ratio))\n\n  return dataset, data_loader\n\n\ndef get_dataset(data_dir, voc_type, max_len, num_samples):\n  if isinstance(data_dir, list):\n    dataset_list = []\n    for data_dir_ in data_dir:\n      dataset_list.append(LmdbDataset(data_dir_, voc_type, max_len, num_samples))\n    dataset = ConcatDataset(dataset_list)\n  else:\n    dataset = LmdbDataset(data_dir, voc_type, max_len, num_samples)\n  print(\'total image: \', len(dataset))\n  return dataset\n\n\ndef get_dataloader(synthetic_dataset, real_dataset, height, width, batch_size, workers,\n                   is_train, keep_ratio):\n  num_synthetic_dataset = len(synthetic_dataset)\n  num_real_dataset = len(real_dataset)\n\n  synthetic_indices = list(np.random.permutation(num_synthetic_dataset))\n  synthetic_indices = synthetic_indices[num_real_dataset:]\n  real_indices = list(np.random.permutation(num_real_dataset) + num_synthetic_dataset)\n  concated_indices = synthetic_indices + real_indices\n  assert len(concated_indices) == num_synthetic_dataset\n\n  sampler = SubsetRandomSampler(concated_indices)\n  concated_dataset = ConcatDataset([synthetic_dataset, real_dataset])\n  print(\'total image: \', len(concated_dataset))\n\n  data_loader = DataLoader(concated_dataset, batch_size=batch_size, num_workers=workers,\n    shuffle=False, pin_memory=True, drop_last=True, sampler=sampler,\n    collate_fn=AlignCollate(imgH=height, imgW=width, keep_ratio=keep_ratio))\n  return concated_dataset, data_loader\n\ndef main(args):\n  np.random.seed(args.seed)\n  torch.manual_seed(args.seed)\n  torch.cuda.manual_seed(args.seed)\n  torch.cuda.manual_seed_all(args.seed)\n  cudnn.benchmark = True\n  torch.backends.cudnn.deterministic = True\n\n  args.cuda = args.cuda and torch.cuda.is_available()\n  if args.cuda:\n    print(\'using cuda.\')\n    torch.set_default_tensor_type(\'torch.cuda.FloatTensor\')\n  else:\n    torch.set_default_tensor_type(\'torch.FloatTensor\')\n  \n  # Redirect print to both console and log file\n  if not args.evaluate:\n    # make symlink\n    make_symlink_if_not_exists(osp.join(args.real_logs_dir, args.logs_dir), osp.dirname(osp.normpath(args.logs_dir)))\n    sys.stdout = Logger(osp.join(args.logs_dir, \'log.txt\'))\n    train_tfLogger = TFLogger(osp.join(args.logs_dir, \'train\'))\n    eval_tfLogger = TFLogger(osp.join(args.logs_dir, \'eval\'))\n\n  # Save the args to disk\n  if not args.evaluate:\n    cfg_save_path = osp.join(args.logs_dir, \'cfg.txt\')\n    cfgs = vars(args)\n    with open(cfg_save_path, \'w\') as f:\n      for k, v in cfgs.items():\n        f.write(\'{}: {}\\n\'.format(k, v))\n\n  # Create data loaders\n  if args.height is None or args.width is None:\n    args.height, args.width = (32, 100)\n\n  if not args.evaluate: \n    train_dataset, train_loader = \\\n      get_data(args.synthetic_train_data_dir, args.voc_type, args.max_len, args.num_train,\n               args.height, args.width, args.batch_size, args.workers, True, args.keep_ratio)\n  test_dataset, test_loader = \\\n    get_data(args.test_data_dir, args.voc_type, args.max_len, args.num_test,\n             args.height, args.width, args.batch_size, args.workers, False, args.keep_ratio)\n\n  if args.evaluate:\n    max_len = test_dataset.max_len\n  else:\n    max_len = max(train_dataset.max_len, test_dataset.max_len)\n    train_dataset.max_len = test_dataset.max_len = max_len\n  # Create model\n  model = ModelBuilder(arch=args.arch, rec_num_classes=test_dataset.rec_num_classes,\n                       sDim=args.decoder_sdim, attDim=args.attDim, max_len_labels=max_len,\n                       eos=test_dataset.char2id[test_dataset.EOS], STN_ON=args.STN_ON)\n\n  # Load from checkpoint\n  if args.evaluation_metric == \'accuracy\':\n    best_res = 0\n  elif args.evaluation_metric == \'editdistance\':\n    best_res = math.inf\n  else:\n    raise ValueError(""Unsupported evaluation metric:"", args.evaluation_metric)\n  start_epoch = 0\n  start_iters = 0\n  if args.resume:\n    checkpoint = load_checkpoint(args.resume)\n    model.load_state_dict(checkpoint[\'state_dict\'])\n\n    # compatibility with the epoch-wise evaluation version\n    if \'epoch\' in checkpoint.keys():\n      start_epoch = checkpoint[\'epoch\']\n    else:\n      start_iters = checkpoint[\'iters\']\n      start_epoch = int(start_iters // len(train_loader)) if not args.evaluate else 0\n    best_res = checkpoint[\'best_res\']\n    print(""=> Start iters {}  best res {:.1%}""\n          .format(start_iters, best_res))\n  \n  if args.cuda:\n    device = torch.device(""cuda"")\n    model = model.to(device)\n    model = nn.DataParallel(model)\n\n  # Evaluator\n  evaluator = Evaluator(model, args.evaluation_metric, args.cuda)\n\n  if args.evaluate:\n    print(\'Test on {0}:\'.format(args.test_data_dir))\n    if len(args.vis_dir) > 0:\n      vis_dir = osp.join(args.logs_dir, args.vis_dir)\n      if not osp.exists(vis_dir):\n        os.makedirs(vis_dir)\n    else:\n      vis_dir = None\n\n    start = time.time()\n    evaluator.evaluate(test_loader, dataset=test_dataset, vis_dir=vis_dir)\n    print(\'it took {0} s.\'.format(time.time() - start))\n    return\n\n  # Optimizer\n  param_groups = model.parameters()\n  param_groups = filter(lambda p: p.requires_grad, param_groups)\n  optimizer = optim.Adadelta(param_groups, lr=args.lr, weight_decay=args.weight_decay)\n  scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[4,5], gamma=0.1)\n\n  # Trainer\n  loss_weights = {}\n  loss_weights[\'loss_rec\'] = 1.\n  if args.debug:\n    args.print_freq = 1\n  trainer = Trainer(model, args.evaluation_metric, args.logs_dir, \n                    iters=start_iters, best_res=best_res, grad_clip=args.grad_clip,\n                    use_cuda=args.cuda, loss_weights=loss_weights)\n\n  # Start training\n  evaluator.evaluate(test_loader, step=0, tfLogger=eval_tfLogger, dataset=test_dataset)\n  for epoch in range(start_epoch, args.epochs):\n    scheduler.step(epoch)\n    current_lr = optimizer.param_groups[0][\'lr\']\n    trainer.train(epoch, train_loader, optimizer, current_lr,\n                  print_freq=args.print_freq,\n                  train_tfLogger=train_tfLogger, \n                  is_debug=args.debug,\n                  evaluator=evaluator, \n                  test_loader=test_loader, \n                  eval_tfLogger=eval_tfLogger,\n                  test_dataset=test_dataset)\n\n  # Final test\n  print(\'Test with best model:\')\n  checkpoint = load_checkpoint(osp.join(args.logs_dir, \'model_best.pth.tar\'))\n  model.module.load_state_dict(checkpoint[\'state_dict\'])\n  evaluator.evaluate(test_loader, dataset=test_dataset)\n\n  # Close the tensorboard logger\n  train_tfLogger.close()\n  eval_tfLogger.close()\n\n\nif __name__ == \'__main__\':\n  # parse the config\n  args = get_args(sys.argv[1:])\n  main(args)'"
lib/__init__.py,0,"b""from __future__ import absolute_import\n\nfrom . import datasets\nfrom . import evaluation_metrics\nfrom . import loss\nfrom . import models\nfrom . import utils\nfrom . import evaluators\nfrom . import trainers\n\n__version__ = '1.0.1.post2'"""
lib/evaluators.py,6,"b'from __future__ import print_function, absolute_import\nimport time\nfrom time import gmtime, strftime\nfrom datetime import datetime\nfrom collections import OrderedDict\n\nimport torch\n\nimport numpy as np\nfrom random import randint\nfrom PIL import Image\nimport sys\n\nfrom . import evaluation_metrics\nfrom .evaluation_metrics import Accuracy, EditDistance, RecPostProcess\nfrom .utils.meters import AverageMeter\nfrom .utils.visualization_utils import recognition_vis, stn_vis\n\nmetrics_factory = evaluation_metrics.factory()\n\nfrom config import get_args\nglobal_args = get_args(sys.argv[1:])\n\nclass BaseEvaluator(object):\n  def __init__(self, model, metric, use_cuda=True):\n    super(BaseEvaluator, self).__init__()\n    self.model = model\n    self.metric = metric\n    self.use_cuda = use_cuda\n    self.device = torch.device(""cuda"" if use_cuda else ""cpu"")\n\n  def evaluate(self, data_loader, step=1, print_freq=1, tfLogger=None, dataset=None, vis_dir=None):\n    self.model.eval()\n\n    batch_time = AverageMeter()\n    data_time  = AverageMeter()\n\n    # forward the network\n    images, outputs, targets, losses = [], {}, [], []\n    file_names = []\n\n    end = time.time()\n    for i, inputs in enumerate(data_loader):\n      data_time.update(time.time() - end)\n\n      input_dict = self._parse_data(inputs)\n      output_dict = self._forward(input_dict)\n\n      batch_size = input_dict[\'images\'].size(0)\n\n      total_loss_batch = 0.\n      for k, loss in output_dict[\'losses\'].items():\n        loss = loss.mean(dim=0, keepdim=True)\n        total_loss_batch += loss.item() * batch_size\n\n      images.append(input_dict[\'images\'])\n      targets.append(input_dict[\'rec_targets\'])\n      losses.append(total_loss_batch)\n      if global_args.evaluate_with_lexicon:\n        file_names += input_dict[\'file_name\']\n      for k, v in output_dict[\'output\'].items():\n        if k not in outputs:\n          outputs[k] = []\n        outputs[k].append(v.cpu())\n\n      batch_time.update(time.time() - end)\n      end = time.time()\n\n      if (i + 1) % print_freq == 0:\n        print(\'[{}]\\t\'\n              \'Evaluation: [{}/{}]\\t\'\n              \'Time {:.3f} ({:.3f})\\t\'\n              \'Data {:.3f} ({:.3f})\\t\'\n              # .format(strftime(""%Y-%m-%d %H:%M:%S"", gmtime()),\n              .format(datetime.now().strftime(\'%Y-%m-%d %H:%M:%S\'),\n                      i + 1, len(data_loader),\n                      batch_time.val, batch_time.avg,\n                      data_time.val, data_time.avg))\n\n    if not global_args.keep_ratio:\n      images = torch.cat(images)\n      num_samples = images.size(0)\n    else:\n      num_samples = sum([subimages.size(0) for subimages in images])\n    targets = torch.cat(targets)\n    losses = np.sum(losses) / (1.0 * num_samples)\n    for k, v in outputs.items():\n      outputs[k] = torch.cat(outputs[k])\n\n    # save info for recognition\n    if \'pred_rec\' in outputs:\n      # evaluation with metric\n      if global_args.evaluate_with_lexicon:\n        eval_res = metrics_factory[self.metric+\'_with_lexicon\'](outputs[\'pred_rec\'], targets, dataset, file_names)\n        print(\'lexicon0: {0}, {1:.3f}\'.format(self.metric, eval_res[0]))\n        print(\'lexicon50: {0}, {1:.3f}\'.format(self.metric, eval_res[1]))\n        print(\'lexicon1k: {0}, {1:.3f}\'.format(self.metric, eval_res[2]))\n        print(\'lexiconfull: {0}, {1:.3f}\'.format(self.metric, eval_res[3]))\n        eval_res = eval_res[0]\n      else:\n        eval_res = metrics_factory[self.metric](outputs[\'pred_rec\'], targets, dataset)\n        print(\'lexicon0: {0}: {1:.3f}\'.format(self.metric, eval_res))\n      pred_list, targ_list, score_list = RecPostProcess(outputs[\'pred_rec\'], targets, outputs[\'pred_rec_score\'], dataset)\n\n      if tfLogger is not None:\n        # (1) Log the scalar values\n        info = {\n          \'loss\': losses,\n          self.metric: eval_res,\n        }\n        for tag, value in info.items():\n          tfLogger.scalar_summary(tag, value, step)\n\n    #====== Visualization ======#\n    if vis_dir is not None:\n      # recognition_vis(images, outputs[\'pred_rec\'], targets, score_list, dataset, vis_dir)\n      stn_vis(images, outputs[\'rectified_images\'], outputs[\'ctrl_points\'], outputs[\'pred_rec\'],\n              targets, score_list, outputs[\'pred_score\'] if \'pred_score\' in outputs else None, dataset, vis_dir)\n    return eval_res\n\n\n  def _parse_data(self, inputs):\n    raise NotImplementedError\n\n  def _forward(self, inputs):\n    raise NotImplementedError\n    \n\nclass Evaluator(BaseEvaluator):\n  def _parse_data(self, inputs):\n    input_dict = {}\n    if global_args.evaluate_with_lexicon:\n      imgs, label_encs, lengths, file_name = inputs\n    else:\n      imgs, label_encs, lengths = inputs\n\n    with torch.no_grad():\n      images = imgs.to(self.device)\n      if label_encs is not None:\n        labels = label_encs.to(self.device)\n\n    input_dict[\'images\'] = images\n    input_dict[\'rec_targets\'] = labels\n    input_dict[\'rec_lengths\'] = lengths\n    if global_args.evaluate_with_lexicon:\n      input_dict[\'file_name\'] = file_name\n    return input_dict\n\n  def _forward(self, input_dict):\n    self.model.eval()\n    with torch.no_grad():\n      output_dict = self.model(input_dict)\n    return output_dict'"
lib/trainers.py,2,"b'from __future__ import print_function, absolute_import\nimport time\nfrom time import gmtime, strftime\nfrom datetime import datetime\nimport gc\nimport os.path as osp\nimport sys\nfrom PIL import Image\nimport numpy as np\n\nimport torch\nfrom torchvision import transforms\n\nfrom . import evaluation_metrics\nfrom .evaluation_metrics import Accuracy, EditDistance\nfrom .utils import to_numpy\nfrom .utils.meters import AverageMeter\nfrom .utils.serialization import load_checkpoint, save_checkpoint\n\nmetrics_factory = evaluation_metrics.factory()\n\nfrom config import get_args\nglobal_args = get_args(sys.argv[1:])\n\nclass BaseTrainer(object):\n  def __init__(self, model, metric, logs_dir, iters=0, best_res=-1, grad_clip=-1, use_cuda=True, loss_weights={}):\n    super(BaseTrainer, self).__init__()\n    self.model = model\n    self.metric = metric\n    self.logs_dir = logs_dir\n    self.iters = iters\n    self.best_res = best_res\n    self.grad_clip = grad_clip\n    self.use_cuda = use_cuda\n    self.loss_weights = loss_weights\n\n    self.device = torch.device(""cuda"" if use_cuda else ""cpu"")\n\n  def train(self, epoch, data_loader, optimizer, current_lr=0.0, \n            print_freq=100, train_tfLogger=None, is_debug=False,\n            evaluator=None, test_loader=None, eval_tfLogger=None,\n            test_dataset=None, test_freq=1000):\n\n    self.model.train()\n\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n\n    end = time.time()\n\n    for i, inputs in enumerate(data_loader):\n      self.model.train()\n      self.iters += 1\n\n      data_time.update(time.time() - end)\n\n      input_dict = self._parse_data(inputs)\n      output_dict = self._forward(input_dict)\n\n      batch_size = input_dict[\'images\'].size(0)\n\n      total_loss = 0\n      loss_dict = {}\n      for k, loss in output_dict[\'losses\'].items():\n        loss = loss.mean(dim=0, keepdim=True)\n        total_loss += self.loss_weights[k] * loss\n        loss_dict[k] = loss.item()\n        # print(\'{0}: {1}\'.format(k, loss.item()))\n\n      losses.update(total_loss.item(), batch_size)\n\n      optimizer.zero_grad()\n      total_loss.backward()\n      if self.grad_clip > 0:\n        torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.grad_clip)\n      optimizer.step()\n\n      # # debug: check the parameters fixed or not.\n      # print(self.model.parameters())\n      # for tag, value in self.model.named_parameters():\n      #   if tag == \'module.base.resnet.layer4.0.conv1.weight\':\n      #     print(value[:10,0,0,0])\n      #   if tag == \'module.rec_head.decoder.attention_unit.sEmbed.weight\':\n      #     print(value[0, :10])\n\n      batch_time.update(time.time() - end)\n      end = time.time()\n\n      if self.iters % print_freq == 0:\n        print(\'[{}]\\t\'\n              \'Epoch: [{}][{}/{}]\\t\'\n              \'Time {:.3f} ({:.3f})\\t\'\n              \'Data {:.3f} ({:.3f})\\t\'\n              \'Loss {:.3f} ({:.3f})\\t\'\n              # .format(strftime(""%Y-%m-%d %H:%M:%S"", gmtime()),\n              .format(datetime.now().strftime(\'%Y-%m-%d %H:%M:%S\'),\n                      epoch, i + 1, len(data_loader),\n                      batch_time.val, batch_time.avg,\n                      data_time.val, data_time.avg,\n                      losses.val, losses.avg))\n\n      #====== TensorBoard logging ======#\n      if self.iters % print_freq*10 == 0:\n        if train_tfLogger is not None:\n          step = epoch * len(data_loader) + (i + 1)\n          info = {\n            \'lr\': current_lr,\n            \'loss\': total_loss.item(), # this is total loss\n          }\n          ## add each loss\n          for k, loss in loss_dict.items():\n            info[k] = loss\n          for tag, value in info.items():\n            train_tfLogger.scalar_summary(tag, value, step)\n\n          # if is_debug and (i + 1) % (print_freq*100) == 0: # this time-consuming and space-consuming\n          #   # (2) Log values and gradients of the parameters (histogram)\n          #   for tag, value in self.model.named_parameters():\n          #     tag = tag.replace(\'.\', \'/\')\n          #     train_tfLogger.histo_summary(tag, to_numpy(value.data), step)\n          #     train_tfLogger.histo_summary(tag+\'/grad\', to_numpy(value.grad.data), step)\n\n          # # (3) Log the images\n          # images, _, pids, _ = inputs\n          # offsets = to_numpy(offsets)\n          # info = {\n          #   \'images\': to_numpy(images[:10])\n          # }\n          # for tag, images in info.items():\n          #   train_tfLogger.image_summary(tag, images, step)\n\n      #====== evaluation ======#\n      if self.iters % test_freq == 0:\n        # only symmetry branch\n        if \'loss_rec\' not in output_dict[\'losses\']:\n          is_best = True\n          # self.best_res is alwarys equal to 1.0 \n          self.best_res = evaluator.evaluate(test_loader, step=self.iters, tfLogger=eval_tfLogger, dataset=test_dataset)\n        else:\n          res = evaluator.evaluate(test_loader, step=self.iters, tfLogger=eval_tfLogger, dataset=test_dataset)\n\n          if self.metric == \'accuracy\':\n            is_best = res > self.best_res\n            self.best_res = max(res, self.best_res)\n          elif self.metric == \'editdistance\':\n            is_best = res < self.best_res\n            self.best_res = min(res, self.best_res)\n          else:\n            raise ValueError(""Unsupported evaluation metric:"", self.metric)\n\n          print(\'\\n * Finished iters {:3d}  accuracy: {:5.1%}  best: {:5.1%}{}\\n\'.\n            format(self.iters, res, self.best_res, \' *\' if is_best else \'\'))\n\n        # if epoch < 1:\n        #   continue\n        save_checkpoint({\n          \'state_dict\': self.model.module.state_dict(),\n          \'iters\': self.iters,\n          \'best_res\': self.best_res,\n        }, is_best, fpath=osp.join(self.logs_dir, \'checkpoint.pth.tar\'))\n\n\n    # collect garbage (not work)\n    # gc.collect()\n\n  def _parse_data(self, inputs):\n    raise NotImplementedError\n\n  def _forward(self, inputs, targets):\n    raise NotImplementedError\n\n\nclass Trainer(BaseTrainer):\n  def _parse_data(self, inputs):\n    input_dict = {}\n    imgs, label_encs, lengths = inputs\n    images = imgs.to(self.device)\n    if label_encs is not None:\n      labels = label_encs.to(self.device)\n\n    input_dict[\'images\'] = images\n    input_dict[\'rec_targets\'] = labels\n    input_dict[\'rec_lengths\'] = lengths\n    return input_dict\n\n  def _forward(self, input_dict):\n    self.model.train()\n    output_dict = self.model(input_dict)\n    return output_dict'"
lib/datasets/__init__.py,0,b''
lib/datasets/concatdataset.py,2,"b'from __future__ import absolute_import\nimport bisect\nimport warnings\n\nimport torch\nfrom torch import randperm\nfrom torch._utils import _accumulate\nfrom torch.utils.data import Dataset\n\nclass ConcatDataset(Dataset):\n    """"""\n    Dataset to concatenate multiple datasets.\n    Purpose: useful to assemble different existing datasets, possibly\n    large-scale datasets as the concatenation operation is done in an\n    on-the-fly manner.\n    Arguments:\n        datasets (sequence): List of datasets to be concatenated\n    """"""\n\n    @staticmethod\n    def cumsum(sequence):\n        r, s = [], 0\n        for e in sequence:\n            l = len(e)\n            r.append(l + s)\n            s += l\n        return r\n\n    def __init__(self, datasets):\n        super(ConcatDataset, self).__init__()\n        assert len(datasets) > 0, \'datasets should not be an empty iterable\'\n        self.datasets = list(datasets)\n        self.cumulative_sizes = self.cumsum(self.datasets)\n        self.max_len = max([_dataset.max_len for _dataset in self.datasets])\n        for _dataset in self.datasets:\n            _dataset.max_len = self.max_len\n\n    def __len__(self):\n        return self.cumulative_sizes[-1]\n\n    def __getitem__(self, idx):\n        dataset_idx = bisect.bisect_right(self.cumulative_sizes, idx)\n        if dataset_idx == 0:\n            sample_idx = idx\n        else:\n            sample_idx = idx - self.cumulative_sizes[dataset_idx - 1]\n        return self.datasets[dataset_idx][sample_idx]\n\n    @property\n    def cummulative_sizes(self):\n        warnings.warn(""cummulative_sizes attribute is renamed to ""\n                      ""cumulative_sizes"", DeprecationWarning, stacklevel=2)\n        return self.cumulative_sizes'"
lib/datasets/dataset.py,8,"b'from __future__ import absolute_import\n\n# import sys\n# sys.path.append(\'./\')\n\nimport os\n# import moxing as mox\n\nimport pickle\nfrom tqdm import tqdm\nfrom PIL import Image, ImageFile\nimport numpy as np\nimport random\nimport cv2\nimport lmdb\nimport sys\nimport six\n\nimport torch\nfrom torch.utils import data\nfrom torch.utils.data import sampler\nfrom torchvision import transforms\n\nfrom lib.utils.labelmaps import get_vocabulary, labels2strs\nfrom lib.utils import to_numpy\n\nImageFile.LOAD_TRUNCATED_IMAGES = True\n\n\nfrom config import get_args\nglobal_args = get_args(sys.argv[1:])\n\nif global_args.run_on_remote:\n  import moxing as mox\n\nclass LmdbDataset(data.Dataset):\n  def __init__(self, root, voc_type, max_len, num_samples, transform=None):\n    super(LmdbDataset, self).__init__()\n\n    if global_args.run_on_remote:\n      dataset_name = os.path.basename(root)\n      data_cache_url = ""/cache/%s"" % dataset_name\n      if not os.path.exists(data_cache_url):\n        os.makedirs(data_cache_url)\n      if mox.file.exists(root):\n        mox.file.copy_parallel(root, data_cache_url)\n      else:\n        raise ValueError(""%s not exists!"" % root)\n      \n      self.env = lmdb.open(data_cache_url, max_readers=32, readonly=True)\n    else:\n      self.env = lmdb.open(root, max_readers=32, readonly=True)\n\n    assert self.env is not None, ""cannot create lmdb from %s"" % root\n    self.txn = self.env.begin()\n\n    self.voc_type = voc_type\n    self.transform = transform\n    self.max_len = max_len\n    self.nSamples = int(self.txn.get(b""num-samples""))\n    self.nSamples = min(self.nSamples, num_samples)\n\n    assert voc_type in [\'LOWERCASE\', \'ALLCASES\', \'ALLCASES_SYMBOLS\']\n    self.EOS = \'EOS\'\n    self.PADDING = \'PADDING\'\n    self.UNKNOWN = \'UNKNOWN\'\n    self.voc = get_vocabulary(voc_type, EOS=self.EOS, PADDING=self.PADDING, UNKNOWN=self.UNKNOWN)\n    self.char2id = dict(zip(self.voc, range(len(self.voc))))\n    self.id2char = dict(zip(range(len(self.voc)), self.voc))\n\n    self.rec_num_classes = len(self.voc)\n    self.lowercase = (voc_type == \'LOWERCASE\')\n\n  def __len__(self):\n    return self.nSamples\n\n  def __getitem__(self, index):\n    assert index <= len(self), \'index range error\'\n    index += 1\n    img_key = b\'image-%09d\' % index\n    imgbuf = self.txn.get(img_key)\n\n    buf = six.BytesIO()\n    buf.write(imgbuf)\n    buf.seek(0)\n    try:\n      img = Image.open(buf).convert(\'RGB\')\n      # img = Image.open(buf).convert(\'L\')\n      # img = img.convert(\'RGB\')\n    except IOError:\n      print(\'Corrupted image for %d\' % index)\n      return self[index + 1]\n\n    # reconition labels\n    label_key = b\'label-%09d\' % index\n    word = self.txn.get(label_key).decode()\n    if self.lowercase:\n      word = word.lower()\n    ## fill with the padding token\n    label = np.full((self.max_len,), self.char2id[self.PADDING], dtype=np.int)\n    label_list = []\n    for char in word:\n      if char in self.char2id:\n        label_list.append(self.char2id[char])\n      else:\n        ## add the unknown token\n        print(\'{0} is out of vocabulary.\'.format(char))\n        label_list.append(self.char2id[self.UNKNOWN])\n    ## add a stop token\n    label_list = label_list + [self.char2id[self.EOS]]\n    assert len(label_list) <= self.max_len\n    label[:len(label_list)] = np.array(label_list)\n\n    if len(label) <= 0:\n      return self[index + 1]\n\n    # label length\n    label_len = len(label_list)\n\n    if self.transform is not None:\n      img = self.transform(img)\n    return img, label, label_len\n\n\nclass ResizeNormalize(object):\n  def __init__(self, size, interpolation=Image.BILINEAR):\n    self.size = size\n    self.interpolation = interpolation\n    self.toTensor = transforms.ToTensor()\n\n  def __call__(self, img):\n    img = img.resize(self.size, self.interpolation)\n    img = self.toTensor(img)\n    img.sub_(0.5).div_(0.5)\n    return img\n\n\nclass RandomSequentialSampler(sampler.Sampler):\n\n  def __init__(self, data_source, batch_size):\n    self.num_samples = len(data_source)\n    self.batch_size = batch_size\n\n  def __len__(self):\n    return self.num_samples\n\n  def __iter__(self):\n    n_batch = len(self) // self.batch_size\n    tail = len(self) % self.batch_size\n    index = torch.LongTensor(len(self)).fill_(0)\n    for i in range(n_batch):\n      random_start = random.randint(0, len(self) - self.batch_size)\n      batch_index = random_start + torch.arange(0, self.batch_size)\n      index[i * self.batch_size:(i + 1) * self.batch_size] = batch_index\n    # deal with tail\n    if tail:\n      random_start = random.randint(0, len(self) - self.batch_size)\n      tail_index = random_start + torch.arange(0, tail)\n      index[(i + 1) * self.batch_size:] = tail_index\n\n    return iter(index.tolist())\n\n\nclass AlignCollate(object):\n\n  def __init__(self, imgH=32, imgW=100, keep_ratio=False, min_ratio=1):\n    self.imgH = imgH\n    self.imgW = imgW\n    self.keep_ratio = keep_ratio\n    self.min_ratio = min_ratio\n\n  def __call__(self, batch):\n    images, labels, lengths = zip(*batch)\n    b_lengths = torch.IntTensor(lengths)\n    b_labels = torch.IntTensor(labels)\n\n    imgH = self.imgH\n    imgW = self.imgW\n    if self.keep_ratio:\n      ratios = []\n      for image in images:\n        w, h = image.size\n        ratios.append(w / float(h))\n      ratios.sort()\n      max_ratio = ratios[-1]\n      imgW = int(np.floor(max_ratio * imgH))\n      imgW = max(imgH * self.min_ratio, imgW)  # assure imgH >= imgW\n      imgW = min(imgW, 400)\n\n    transform = ResizeNormalize((imgW, imgH))\n    images = [transform(image) for image in images]\n    b_images = torch.stack(images)\n\n    return b_images, b_labels, b_lengths\n\n\ndef test():\n  # lmdb_path = ""/share/zhui/reg_dataset/NIPS2014""\n  lmdb_path = ""/share/zhui/reg_dataset/IIIT5K_3000""\n  train_dataset = LmdbDataset(root=lmdb_path, voc_type=\'ALLCASES_SYMBOLS\', max_len=50)\n  batch_size = 1\n  train_dataloader = data.DataLoader(\n        train_dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=4,\n        collate_fn=AlignCollate(imgH=64, imgW=256, keep_ratio=False))\n\n  for i, (images, labels, label_lens) in enumerate(train_dataloader):\n    # visualization of input image\n    # toPILImage = transforms.ToPILImage()\n    images = images.permute(0,2,3,1)\n    images = to_numpy(images)\n    images = images * 0.5 + 0.5\n    images = images * 255\n    for id, (image, label, label_len) in enumerate(zip(images, labels, label_lens)):\n      image = Image.fromarray(np.uint8(image))\n      # image = toPILImage(image)\n      image.show()\n      print(image.size)\n      print(labels2strs(label, train_dataset.id2char, train_dataset.char2id))\n      print(label_len.item())\n      input()\n\n\nif __name__ == ""__main__"":\n    test()'"
lib/evaluation_metrics/__init__.py,0,"b""from __future__ import absolute_import\n\nfrom .metrics import Accuracy, EditDistance, RecPostProcess, Accuracy_with_lexicon, EditDistance_with_lexicon\n\n\n__factory = {\n  'accuracy': Accuracy,\n  'editdistance': EditDistance,\n  'accuracy_with_lexicon': Accuracy_with_lexicon,\n  'editdistance_with_lexicon': EditDistance_with_lexicon,\n}\n\ndef names():\n  return sorted(__factory.keys())\n\ndef factory():\n  return __factory"""
lib/evaluation_metrics/metrics.py,1,"b""from __future__ import absolute_import\n\nimport numpy as np\nimport editdistance\nimport string\nimport math\n\nimport torch\nimport torch.nn.functional as F\n\nfrom ..utils import to_torch, to_numpy\n\n\ndef _normalize_text(text):\n  text = ''.join(filter(lambda x: x in (string.digits + string.ascii_letters), text))\n  return text.lower()\n\n\ndef get_str_list(output, target, dataset=None):\n  # label_seq\n  assert output.dim() == 2 and target.dim() == 2\n\n  end_label = dataset.char2id[dataset.EOS]\n  unknown_label = dataset.char2id[dataset.UNKNOWN]\n  num_samples, max_len_labels = output.size()\n  num_classes = len(dataset.char2id.keys())\n  assert num_samples == target.size(0) and max_len_labels == target.size(1)\n  output = to_numpy(output)\n  target = to_numpy(target)\n\n  # list of char list\n  pred_list, targ_list = [], []\n  for i in range(num_samples):\n    pred_list_i = []\n    for j in range(max_len_labels):\n      if output[i, j] != end_label:\n        if output[i, j] != unknown_label:\n          pred_list_i.append(dataset.id2char[output[i, j]])\n      else:\n        break\n    pred_list.append(pred_list_i)\n\n  for i in range(num_samples):\n    targ_list_i = []\n    for j in range(max_len_labels):\n      if target[i, j] != end_label:\n        if target[i, j] != unknown_label:\n          targ_list_i.append(dataset.id2char[target[i, j]])\n      else:\n        break\n    targ_list.append(targ_list_i)\n\n  # char list to string\n  # if dataset.lowercase:\n  if True:\n    # pred_list = [''.join(pred).lower() for pred in pred_list]\n    # targ_list = [''.join(targ).lower() for targ in targ_list]\n    pred_list = [_normalize_text(pred) for pred in pred_list]\n    targ_list = [_normalize_text(targ) for targ in targ_list]\n  else:\n    pred_list = [''.join(pred) for pred in pred_list]\n    targ_list = [''.join(targ) for targ in targ_list]\n\n  return pred_list, targ_list\n\n\ndef _lexicon_search(lexicon, word):\n  edit_distances = []\n  for lex_word in lexicon:\n    edit_distances.append(editdistance.eval(_normalize_text(lex_word), _normalize_text(word)))\n  edit_distances = np.asarray(edit_distances, dtype=np.int)\n  argmin = np.argmin(edit_distances)\n  return lexicon[argmin]\n\n\ndef Accuracy(output, target, dataset=None):\n  pred_list, targ_list = get_str_list(output, target, dataset)\n\n  acc_list = [(pred == targ) for pred, targ in zip(pred_list, targ_list)]\n  accuracy = 1.0 * sum(acc_list) / len(acc_list)\n  return accuracy\n\n\ndef Accuracy_with_lexicon(output, target, dataset=None, file_names=None):\n  pred_list, targ_list = get_str_list(output, target, dataset)\n  accuracys = []\n\n  # with no lexicon\n  acc_list = [(pred == targ) for pred, targ in zip(pred_list, targ_list)]\n  accuracy = 1.0 * sum(acc_list) / len(acc_list)\n  accuracys.append(accuracy)\n\n  # lexicon50\n  if len(file_names) == 0 or len(dataset.lexicons50[file_names[0]]) == 0:\n    accuracys.append(0)\n  else:\n    refined_pred_list = [_lexicon_search(dataset.lexicons50[file_name], pred) for file_name, pred in zip(file_names, pred_list)]\n    acc_list = [(pred == targ) for pred, targ in zip(refined_pred_list, targ_list)]\n    accuracy = 1.0 * sum(acc_list) / len(acc_list)\n    accuracys.append(accuracy)\n\n  # lexicon1k\n  if len(file_names) == 0 or len(dataset.lexicons1k[file_names[0]]) == 0:\n    accuracys.append(0)\n  else:\n    refined_pred_list = [_lexicon_search(dataset.lexicons1k[file_name], pred) for file_name, pred in zip(file_names, pred_list)]\n    acc_list = [(pred == targ) for pred, targ in zip(refined_pred_list, targ_list)]\n    accuracy = 1.0 * sum(acc_list) / len(acc_list)\n    accuracys.append(accuracy)\n\n  # lexiconfull\n  if len(file_names) == 0 or len(dataset.lexiconsfull[file_names[0]]) == 0:\n    accuracys.append(0)\n  else:\n    refined_pred_list = [_lexicon_search(dataset.lexiconsfull[file_name], pred) for file_name, pred in zip(file_names, pred_list)]\n    acc_list = [(pred == targ) for pred, targ in zip(refined_pred_list, targ_list)]\n    accuracy = 1.0 * sum(acc_list) / len(acc_list)\n    accuracys.append(accuracy)    \n\n  return accuracys\n\n\ndef EditDistance(output, target, dataset=None):\n  pred_list, targ_list = get_str_list(output, target, dataset)\n\n  ed_list = [editdistance.eval(pred, targ) for pred, targ in zip(pred_list, targ_list)]\n  eds = sum(ed_list)\n  return eds\n\n\ndef EditDistance_with_lexicon(output, target, dataset=None, file_names=None):\n  pred_list, targ_list = get_str_list(output, target, dataset)\n  eds = []\n\n  # with no lexicon\n  ed_list = [editdistance.eval(pred, targ) for pred, targ in zip(pred_list, targ_list)]\n  ed = sum(ed_list)\n  eds.append(ed)\n\n  # lexicon50\n  if len(file_names) == 0 or len(dataset.lexicons50[file_names[0]]) == 0:\n    eds.append(0)\n  else:\n    refined_pred_list = [_lexicon_search(dataset.lexicons50[file_name], pred) for file_name, pred in zip(file_names, pred_list)]\n    ed_list = [editdistance.eval(pred, targ) for pred, targ in zip(refined_pred_list, targ_list)]\n    ed = sum(ed_list)\n    eds.append(ed)\n\n  # lexicon1k\n  if len(file_names) == 0 or len(dataset.lexicons1k[file_names[0]]) == 0:\n    eds.append(0)\n  else:\n    refined_pred_list = [_lexicon_search(dataset.lexicons1k[file_name], pred) for file_name, pred in zip(file_names, pred_list)]\n    ed_list = [editdistance.eval(pred, targ) for pred, targ in zip(refined_pred_list, targ_list)]\n    ed = sum(ed_list)\n    eds.append(ed)\n\n  # lexiconfull\n  if len(file_names) == 0 or len(dataset.lexiconsfull[file_names[0]]) == 0:\n    eds.append(0)\n  else:\n    refined_pred_list = [_lexicon_search(dataset.lexiconsfull[file_name], pred) for file_name, pred in zip(file_names, pred_list)]\n    ed_list = [editdistance.eval(pred, targ) for pred, targ in zip(refined_pred_list, targ_list)]\n    ed = sum(ed_list)\n    eds.append(ed)\n\n  return eds\n\n\ndef RecPostProcess(output, target, score, dataset=None):\n  pred_list, targ_list = get_str_list(output, target, dataset)\n  max_len_labels = output.size(1)\n  score_list = []\n\n  score = to_numpy(score)\n  for i, pred in enumerate(pred_list):\n    len_pred = len(pred) + 1 # eos should be included\n    len_pred = min(max_len_labels, len_pred) # maybe the predicted string don't include a eos.\n    score_i = score[i,:len_pred]\n    score_i = math.exp(sum(map(math.log, score_i)))\n    score_list.append(score_i)\n\n  return pred_list, targ_list, score_list"""
lib/loss/__init__.py,0,"b""from __future__ import absolute_import\n\nfrom .sequenceCrossEntropyLoss import SequenceCrossEntropyLoss\n\n\n__all__ = [\n  'SequenceCrossEntropyLoss',\n]"""
lib/loss/sequenceCrossEntropyLoss.py,7,"b'from __future__ import absolute_import\n\nimport torch\nfrom torch import nn\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\n\ndef to_contiguous(tensor):\n  if tensor.is_contiguous():\n    return tensor\n  else:\n    return tensor.contiguous()\n\ndef _assert_no_grad(variable):\n  assert not variable.requires_grad, \\\n    ""nn criterions don\'t compute the gradient w.r.t. targets - please "" \\\n    ""mark these variables as not requiring gradients""\n\nclass SequenceCrossEntropyLoss(nn.Module):\n  def __init__(self, \n               weight=None,\n               size_average=True,\n               ignore_index=-100,\n               sequence_normalize=False,\n               sample_normalize=True):\n    super(SequenceCrossEntropyLoss, self).__init__()\n    self.weight = weight\n    self.size_average = size_average\n    self.ignore_index = ignore_index\n    self.sequence_normalize = sequence_normalize\n    self.sample_normalize = sample_normalize\n\n    assert (sequence_normalize and sample_normalize) == False\n\n  def forward(self, input, target, length):\n    _assert_no_grad(target)\n    # length to mask\n    batch_size, def_max_length = target.size(0), target.size(1)\n    mask = torch.zeros(batch_size, def_max_length)\n    for i in range(batch_size):\n      mask[i,:length[i]].fill_(1)\n    mask = mask.type_as(input)\n    # truncate to the same size\n    max_length = max(length)\n    assert max_length == input.size(1)\n    target = target[:, :max_length]\n    mask =  mask[:, :max_length]\n    input = to_contiguous(input).view(-1, input.size(2))\n    input = F.log_softmax(input, dim=1)\n    target = to_contiguous(target).view(-1, 1)\n    mask = to_contiguous(mask).view(-1, 1)\n    output = - input.gather(1, target.long()) * mask\n    # if self.size_average:\n    #   output = torch.sum(output) / torch.sum(mask)\n    # elif self.reduce:\n    #   output = torch.sum(output)\n    ##\n    output = torch.sum(output)\n    if self.sequence_normalize:\n      output = output / torch.sum(mask)\n    if self.sample_normalize:\n      output = output / batch_size\n\n    return output'"
lib/models/__init__.py,0,"b'from __future__ import absolute_import\n\nfrom .resnet_aster import *\n\n__factory = {\n  \'ResNet_ASTER\': ResNet_ASTER,\n}\n\ndef names():\n  return sorted(__factory.keys())\n\n\ndef create(name, *args, **kwargs):\n  """"""Create a model instance.\n  \n  Parameters\n  ----------\n  name: str\n    Model name. One of __factory\n  pretrained: bool, optional\n    If True, will use ImageNet pretrained model. Default: True\n  num_classes: int, optional\n    If positive, will change the original classifier the fit the new classifier with num_classes. Default: True\n  with_words: bool, optional\n    If True, the input of this model is the combination of image and word. Default: False\n  """"""\n  if name not in __factory:\n    raise KeyError(\'Unknown model:\', name)\n  return __factory[name](*args, **kwargs)'"
lib/models/attention_recognition_head.py,20,"b'from __future__ import absolute_import\n\nimport sys\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.nn import init\n\n\nclass AttentionRecognitionHead(nn.Module):\n  """"""\n  input: [b x 16 x 64 x in_planes]\n  output: probability sequence: [b x T x num_classes]\n  """"""\n  def __init__(self, num_classes, in_planes, sDim, attDim, max_len_labels):\n    super(AttentionRecognitionHead, self).__init__()\n    self.num_classes = num_classes # this is the output classes. So it includes the <EOS>.\n    self.in_planes = in_planes\n    self.sDim = sDim\n    self.attDim = attDim\n    self.max_len_labels = max_len_labels\n\n    self.decoder = DecoderUnit(sDim=sDim, xDim=in_planes, yDim=num_classes, attDim=attDim)\n\n  def forward(self, x):\n    x, targets, lengths = x\n    batch_size = x.size(0)\n    # Decoder\n    state = torch.zeros(1, batch_size, self.sDim)\n    outputs = []\n\n    for i in range(max(lengths)):\n      if i == 0:\n        y_prev = torch.zeros((batch_size)).fill_(self.num_classes) # the last one is used as the <BOS>.\n      else:\n        y_prev = targets[:,i-1]\n\n      output, state = self.decoder(x, state, y_prev)\n      outputs.append(output)\n    outputs = torch.cat([_.unsqueeze(1) for _ in outputs], 1)\n    return outputs\n\n  # inference stage.\n  def sample(self, x):\n    x, _, _ = x\n    batch_size = x.size(0)\n    # Decoder\n    state = torch.zeros(1, batch_size, self.sDim)\n\n    predicted_ids, predicted_scores = [], []\n    for i in range(self.max_len_labels):\n      if i == 0:\n        y_prev = torch.zeros((batch_size)).fill_(self.num_classes)\n      else:\n        y_prev = predicted\n\n      output, state = self.decoder(x, state, y_prev)\n      output = F.softmax(output, dim=1)\n      score, predicted = output.max(1)\n      predicted_ids.append(predicted.unsqueeze(1))\n      predicted_scores.append(score.unsqueeze(1))\n    predicted_ids = torch.cat(predicted_ids, 1)\n    predicted_scores = torch.cat(predicted_scores, 1)\n    # return predicted_ids.squeeze(), predicted_scores.squeeze()\n    return predicted_ids, predicted_scores\n\n  def beam_search(self, x, beam_width, eos):\n\n    def _inflate(tensor, times, dim):\n      repeat_dims = [1] * tensor.dim()\n      repeat_dims[dim] = times\n      return tensor.repeat(*repeat_dims)\n\n    # https://github.com/IBM/pytorch-seq2seq/blob/fede87655ddce6c94b38886089e05321dc9802af/seq2seq/models/TopKDecoder.py\n    batch_size, l, d = x.size()\n    # inflated_encoder_feats = _inflate(encoder_feats, beam_width, 0) # ABC --> AABBCC -/-> ABCABC\n    inflated_encoder_feats = x.unsqueeze(1).permute((1,0,2,3)).repeat((beam_width,1,1,1)).permute((1,0,2,3)).contiguous().view(-1, l, d)\n\n    # Initialize the decoder\n    state = torch.zeros(1, batch_size * beam_width, self.sDim)\n    pos_index = (torch.Tensor(range(batch_size)) * beam_width).long().view(-1, 1)\n\n    # Initialize the scores\n    sequence_scores = torch.Tensor(batch_size * beam_width, 1)\n    sequence_scores.fill_(-float(\'Inf\'))\n    sequence_scores.index_fill_(0, torch.Tensor([i * beam_width for i in range(0, batch_size)]).long(), 0.0)\n    # sequence_scores.fill_(0.0)\n\n    # Initialize the input vector\n    y_prev = torch.zeros((batch_size * beam_width)).fill_(self.num_classes)\n\n    # Store decisions for backtracking\n    stored_scores          = list()\n    stored_predecessors    = list()\n    stored_emitted_symbols = list()\n\n    for i in range(self.max_len_labels):\n      output, state = self.decoder(inflated_encoder_feats, state, y_prev)\n      log_softmax_output = F.log_softmax(output, dim=1)\n\n      sequence_scores = _inflate(sequence_scores, self.num_classes, 1)\n      sequence_scores += log_softmax_output\n      scores, candidates = sequence_scores.view(batch_size, -1).topk(beam_width, dim=1)\n\n      # Reshape input = (bk, 1) and sequence_scores = (bk, 1)\n      y_prev = (candidates % self.num_classes).view(batch_size * beam_width)\n      sequence_scores = scores.view(batch_size * beam_width, 1)\n\n      # Update fields for next timestep\n      predecessors = (candidates / self.num_classes + pos_index.expand_as(candidates)).view(batch_size * beam_width, 1)\n      state = state.index_select(1, predecessors.squeeze())\n\n      # Update sequence socres and erase scores for <eos> symbol so that they aren\'t expanded\n      stored_scores.append(sequence_scores.clone())\n      eos_indices = y_prev.view(-1, 1).eq(eos)\n      if eos_indices.nonzero().dim() > 0:\n        sequence_scores.masked_fill_(eos_indices, -float(\'inf\'))\n\n      # Cache results for backtracking\n      stored_predecessors.append(predecessors)\n      stored_emitted_symbols.append(y_prev)\n\n    # Do backtracking to return the optimal values\n    #====== backtrak ======#\n    # Initialize return variables given different types\n    p = list()\n    l = [[self.max_len_labels] * beam_width for _ in range(batch_size)]  # Placeholder for lengths of top-k sequences\n\n    # the last step output of the beams are not sorted\n    # thus they are sorted here\n    sorted_score, sorted_idx = stored_scores[-1].view(batch_size, beam_width).topk(beam_width)\n    # initialize the sequence scores with the sorted last step beam scores\n    s = sorted_score.clone()\n\n    batch_eos_found = [0] * batch_size  # the number of EOS found\n                                        # in the backward loop below for each batch\n    t = self.max_len_labels - 1\n    # initialize the back pointer with the sorted order of the last step beams.\n    # add pos_index for indexing variable with b*k as the first dimension.\n    t_predecessors = (sorted_idx + pos_index.expand_as(sorted_idx)).view(batch_size * beam_width)\n    while t >= 0:\n      # Re-order the variables with the back pointer\n      current_symbol = stored_emitted_symbols[t].index_select(0, t_predecessors)\n      t_predecessors = stored_predecessors[t].index_select(0, t_predecessors).squeeze()\n      eos_indices = stored_emitted_symbols[t].eq(eos).nonzero()\n      if eos_indices.dim() > 0:\n        for i in range(eos_indices.size(0)-1, -1, -1):\n          # Indices of the EOS symbol for both variables\n          # with b*k as the first dimension, and b, k for\n          # the first two dimensions\n          idx = eos_indices[i]\n          b_idx = int(idx[0] / beam_width)\n          # The indices of the replacing position\n          # according to the replacement strategy noted above\n          res_k_idx = beam_width - (batch_eos_found[b_idx] % beam_width) - 1\n          batch_eos_found[b_idx] += 1\n          res_idx = b_idx * beam_width + res_k_idx\n\n          # Replace the old information in return variables\n          # with the new ended sequence information\n          t_predecessors[res_idx] = stored_predecessors[t][idx[0]]\n          current_symbol[res_idx] = stored_emitted_symbols[t][idx[0]]\n          s[b_idx, res_k_idx] = stored_scores[t][idx[0], [0]]\n          l[b_idx][res_k_idx] = t + 1\n\n      # record the back tracked results\n      p.append(current_symbol)\n\n      t -= 1\n\n    # Sort and re-order again as the added ended sequences may change\n    # the order (very unlikely)\n    s, re_sorted_idx = s.topk(beam_width)\n    for b_idx in range(batch_size):\n      l[b_idx] = [l[b_idx][k_idx.item()] for k_idx in re_sorted_idx[b_idx,:]]\n\n    re_sorted_idx = (re_sorted_idx + pos_index.expand_as(re_sorted_idx)).view(batch_size*beam_width)\n\n    # Reverse the sequences and re-order at the same time\n    # It is reversed because the backtracking happens in reverse time order\n    p = [step.index_select(0, re_sorted_idx).view(batch_size, beam_width, -1) for step in reversed(p)]\n    p = torch.cat(p, -1)[:,0,:]\n    return p, torch.ones_like(p)\n\n\nclass AttentionUnit(nn.Module):\n  def __init__(self, sDim, xDim, attDim):\n    super(AttentionUnit, self).__init__()\n\n    self.sDim = sDim\n    self.xDim = xDim\n    self.attDim = attDim\n\n    self.sEmbed = nn.Linear(sDim, attDim)\n    self.xEmbed = nn.Linear(xDim, attDim)\n    self.wEmbed = nn.Linear(attDim, 1)\n\n    # self.init_weights()\n\n  def init_weights(self):\n    init.normal_(self.sEmbed.weight, std=0.01)\n    init.constant_(self.sEmbed.bias, 0)\n    init.normal_(self.xEmbed.weight, std=0.01)\n    init.constant_(self.xEmbed.bias, 0)\n    init.normal_(self.wEmbed.weight, std=0.01)\n    init.constant_(self.wEmbed.bias, 0)\n\n  def forward(self, x, sPrev):\n    batch_size, T, _ = x.size()                      # [b x T x xDim]\n    x = x.view(-1, self.xDim)                        # [(b x T) x xDim]\n    xProj = self.xEmbed(x)                           # [(b x T) x attDim]\n    xProj = xProj.view(batch_size, T, -1)            # [b x T x attDim]\n\n    sPrev = sPrev.squeeze(0)\n    sProj = self.sEmbed(sPrev)                       # [b x attDim]\n    sProj = torch.unsqueeze(sProj, 1)                # [b x 1 x attDim]\n    sProj = sProj.expand(batch_size, T, self.attDim) # [b x T x attDim]\n\n    sumTanh = torch.tanh(sProj + xProj)\n    sumTanh = sumTanh.view(-1, self.attDim)\n\n    vProj = self.wEmbed(sumTanh) # [(b x T) x 1]\n    vProj = vProj.view(batch_size, T)\n\n    alpha = F.softmax(vProj, dim=1) # attention weights for each sample in the minibatch\n\n    return alpha\n\n\nclass DecoderUnit(nn.Module):\n  def __init__(self, sDim, xDim, yDim, attDim):\n    super(DecoderUnit, self).__init__()\n    self.sDim = sDim\n    self.xDim = xDim\n    self.yDim = yDim\n    self.attDim = attDim\n    self.emdDim = attDim\n\n    self.attention_unit = AttentionUnit(sDim, xDim, attDim)\n    self.tgt_embedding = nn.Embedding(yDim+1, self.emdDim) # the last is used for <BOS> \n    self.gru = nn.GRU(input_size=xDim+self.emdDim, hidden_size=sDim, batch_first=True)\n    self.fc = nn.Linear(sDim, yDim)\n\n    # self.init_weights()\n\n  def init_weights(self):\n    init.normal_(self.tgt_embedding.weight, std=0.01)\n    init.normal_(self.fc.weight, std=0.01)\n    init.constant_(self.fc.bias, 0)\n\n  def forward(self, x, sPrev, yPrev):\n    # x: feature sequence from the image decoder.\n    batch_size, T, _ = x.size()\n    alpha = self.attention_unit(x, sPrev)\n    context = torch.bmm(alpha.unsqueeze(1), x).squeeze(1)\n    yProj = self.tgt_embedding(yPrev.long())\n    # self.gru.flatten_parameters()\n    output, state = self.gru(torch.cat([yProj, context], 1).unsqueeze(1), sPrev)\n    output = output.squeeze(1)\n\n    output = self.fc(output)\n    return output, state'"
lib/models/model_builder.py,2,"b'from __future__ import absolute_import\n\nfrom PIL import Image\nimport numpy as np\nfrom collections import OrderedDict\nimport sys\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.nn import init\n\nfrom . import create\nfrom .attention_recognition_head import AttentionRecognitionHead\nfrom ..loss.sequenceCrossEntropyLoss import SequenceCrossEntropyLoss\nfrom .tps_spatial_transformer import TPSSpatialTransformer\nfrom .stn_head import STNHead\n\n\nfrom config import get_args\nglobal_args = get_args(sys.argv[1:])\n\n\nclass ModelBuilder(nn.Module):\n  """"""\n  This is the integrated model.\n  """"""\n  def __init__(self, arch, rec_num_classes, sDim, attDim, max_len_labels, eos, STN_ON=False):\n    super(ModelBuilder, self).__init__()\n\n    self.arch = arch\n    self.rec_num_classes = rec_num_classes\n    self.sDim = sDim\n    self.attDim = attDim\n    self.max_len_labels = max_len_labels\n    self.eos = eos\n    self.STN_ON = STN_ON\n    self.tps_inputsize = global_args.tps_inputsize\n\n    self.encoder = create(self.arch,\n                      with_lstm=global_args.with_lstm,\n                      n_group=global_args.n_group)\n    encoder_out_planes = self.encoder.out_planes\n\n    self.decoder = AttentionRecognitionHead(\n                      num_classes=rec_num_classes,\n                      in_planes=encoder_out_planes,\n                      sDim=sDim,\n                      attDim=attDim,\n                      max_len_labels=max_len_labels)\n    self.rec_crit = SequenceCrossEntropyLoss()\n\n    if self.STN_ON:\n      self.tps = TPSSpatialTransformer(\n        output_image_size=tuple(global_args.tps_outputsize),\n        num_control_points=global_args.num_control_points,\n        margins=tuple(global_args.tps_margins))\n      self.stn_head = STNHead(\n        in_planes=3,\n        num_ctrlpoints=global_args.num_control_points,\n        activation=global_args.stn_activation)\n\n  def forward(self, input_dict):\n    return_dict = {}\n    return_dict[\'losses\'] = {}\n    return_dict[\'output\'] = {}\n\n    x, rec_targets, rec_lengths = input_dict[\'images\'], \\\n                                  input_dict[\'rec_targets\'], \\\n                                  input_dict[\'rec_lengths\']\n\n    # rectification\n    if self.STN_ON:\n      # input images are downsampled before being fed into stn_head.\n      stn_input = F.interpolate(x, self.tps_inputsize, mode=\'bilinear\', align_corners=True)\n      stn_img_feat, ctrl_points = self.stn_head(stn_input)\n      x, _ = self.tps(x, ctrl_points)\n      if not self.training:\n        # save for visualization\n        return_dict[\'output\'][\'ctrl_points\'] = ctrl_points\n        return_dict[\'output\'][\'rectified_images\'] = x\n\n    encoder_feats = self.encoder(x)\n    encoder_feats = encoder_feats.contiguous()\n\n    if self.training:\n      rec_pred = self.decoder([encoder_feats, rec_targets, rec_lengths])\n      loss_rec = self.rec_crit(rec_pred, rec_targets, rec_lengths)\n      return_dict[\'losses\'][\'loss_rec\'] = loss_rec\n    else:\n      rec_pred, rec_pred_scores = self.decoder.beam_search(encoder_feats, global_args.beam_width, self.eos)\n      rec_pred_ = self.decoder([encoder_feats, rec_targets, rec_lengths])\n      loss_rec = self.rec_crit(rec_pred_, rec_targets, rec_lengths)\n      return_dict[\'losses\'][\'loss_rec\'] = loss_rec\n      return_dict[\'output\'][\'pred_rec\'] = rec_pred\n      return_dict[\'output\'][\'pred_rec_score\'] = rec_pred_scores\n\n    # pytorch0.4 bug on gathering scalar(0-dim) tensors\n    for k, v in return_dict[\'losses\'].items():\n      return_dict[\'losses\'][k] = v.unsqueeze(0)\n\n    return return_dict'"
lib/models/resnet_aster.py,7,"b'import torch\nimport torch.nn as nn\nimport torchvision\n\nimport sys\nimport math\n\nfrom config import get_args\nglobal_args = get_args(sys.argv[1:])\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n  """"""3x3 convolution with padding""""""\n  return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                   padding=1, bias=False)\n\n\ndef conv1x1(in_planes, out_planes, stride=1):\n  """"""1x1 convolution""""""\n  return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n\n\ndef get_sinusoid_encoding(n_position, feat_dim, wave_length=10000):\n  # [n_position]\n  positions = torch.arange(0, n_position)#.cuda()\n  # [feat_dim]\n  dim_range = torch.arange(0, feat_dim)#.cuda()\n  dim_range = torch.pow(wave_length, 2 * (dim_range // 2) / feat_dim)\n  # [n_position, feat_dim]\n  angles = positions.unsqueeze(1) / dim_range.unsqueeze(0)\n  angles = angles.float()\n  angles[:, 0::2] = torch.sin(angles[:, 0::2])\n  angles[:, 1::2] = torch.cos(angles[:, 1::2])\n  return angles\n\n\nclass AsterBlock(nn.Module):\n\n  def __init__(self, inplanes, planes, stride=1, downsample=None):\n    super(AsterBlock, self).__init__()\n    self.conv1 = conv1x1(inplanes, planes, stride)\n    self.bn1 = nn.BatchNorm2d(planes)\n    self.relu = nn.ReLU(inplace=True)\n    self.conv2 = conv3x3(planes, planes)\n    self.bn2 = nn.BatchNorm2d(planes)\n    self.downsample = downsample\n    self.stride = stride\n\n  def forward(self, x):\n    residual = x\n    out = self.conv1(x)\n    out = self.bn1(out)\n    out = self.relu(out)\n    out = self.conv2(out)\n    out = self.bn2(out)\n\n    if self.downsample is not None:\n      residual = self.downsample(x)\n    out += residual\n    out = self.relu(out)\n    return out\n\n\nclass ResNet_ASTER(nn.Module):\n  """"""For aster or crnn""""""\n\n  def __init__(self, with_lstm=False, n_group=1):\n    super(ResNet_ASTER, self).__init__()\n    self.with_lstm = with_lstm\n    self.n_group = n_group\n\n    in_channels = 3\n    self.layer0 = nn.Sequential(\n        nn.Conv2d(in_channels, 32, kernel_size=(3, 3), stride=1, padding=1, bias=False),\n        nn.BatchNorm2d(32),\n        nn.ReLU(inplace=True))\n\n    self.inplanes = 32\n    self.layer1 = self._make_layer(32,  3, [2, 2]) # [16, 50]\n    self.layer2 = self._make_layer(64,  4, [2, 2]) # [8, 25]\n    self.layer3 = self._make_layer(128, 6, [2, 1]) # [4, 25]\n    self.layer4 = self._make_layer(256, 6, [2, 1]) # [2, 25]\n    self.layer5 = self._make_layer(512, 3, [2, 1]) # [1, 25]\n\n    if with_lstm:\n      self.rnn = nn.LSTM(512, 256, bidirectional=True, num_layers=2, batch_first=True)\n      self.out_planes = 2 * 256\n    else:\n      self.out_planes = 512\n\n    for m in self.modules():\n      if isinstance(m, nn.Conv2d):\n        nn.init.kaiming_normal_(m.weight, mode=""fan_out"", nonlinearity=""relu"")\n      elif isinstance(m, nn.BatchNorm2d):\n        nn.init.constant_(m.weight, 1)\n        nn.init.constant_(m.bias, 0)\n\n  def _make_layer(self, planes, blocks, stride):\n    downsample = None\n    if stride != [1, 1] or self.inplanes != planes:\n      downsample = nn.Sequential(\n          conv1x1(self.inplanes, planes, stride),\n          nn.BatchNorm2d(planes))\n\n    layers = []\n    layers.append(AsterBlock(self.inplanes, planes, stride, downsample))\n    self.inplanes = planes\n    for _ in range(1, blocks):\n      layers.append(AsterBlock(self.inplanes, planes))\n    return nn.Sequential(*layers)\n\n  def forward(self, x):\n    x0 = self.layer0(x)\n    x1 = self.layer1(x0)\n    x2 = self.layer2(x1)\n    x3 = self.layer3(x2)\n    x4 = self.layer4(x3)\n    x5 = self.layer5(x4)\n\n    cnn_feat = x5.squeeze(2) # [N, c, w]\n    cnn_feat = cnn_feat.transpose(2, 1)\n    if self.with_lstm:\n      rnn_feat, _ = self.rnn(cnn_feat)\n      return rnn_feat\n    else:\n      return cnn_feat\n\n\nif __name__ == ""__main__"":\n  x = torch.randn(3, 3, 32, 100)\n  net = ResNet_ASTER(use_self_attention=True, use_position_embedding=True)\n  encoder_feat = net(x)\n  print(encoder_feat.size())'"
lib/models/stn_head.py,4,"b'from __future__ import absolute_import\n\nimport math\nimport numpy as np\nimport sys\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.nn import init\n\n\ndef conv3x3_block(in_planes, out_planes, stride=1):\n  """"""3x3 convolution with padding""""""\n  conv_layer = nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=1, padding=1)\n\n  block = nn.Sequential(\n    conv_layer,\n    nn.BatchNorm2d(out_planes),\n    nn.ReLU(inplace=True),\n  )\n  return block\n\n\nclass STNHead(nn.Module):\n  def __init__(self, in_planes, num_ctrlpoints, activation=\'none\'):\n    super(STNHead, self).__init__()\n\n    self.in_planes = in_planes\n    self.num_ctrlpoints = num_ctrlpoints\n    self.activation = activation\n    self.stn_convnet = nn.Sequential(\n                          conv3x3_block(in_planes, 32), # 32*64\n                          nn.MaxPool2d(kernel_size=2, stride=2),\n                          conv3x3_block(32, 64), # 16*32\n                          nn.MaxPool2d(kernel_size=2, stride=2),\n                          conv3x3_block(64, 128), # 8*16\n                          nn.MaxPool2d(kernel_size=2, stride=2),\n                          conv3x3_block(128, 256), # 4*8\n                          nn.MaxPool2d(kernel_size=2, stride=2),\n                          conv3x3_block(256, 256), # 2*4,\n                          nn.MaxPool2d(kernel_size=2, stride=2),\n                          conv3x3_block(256, 256)) # 1*2\n\n    self.stn_fc1 = nn.Sequential(\n                      nn.Linear(2*256, 512),\n                      nn.BatchNorm1d(512),\n                      nn.ReLU(inplace=True))\n    self.stn_fc2 = nn.Linear(512, num_ctrlpoints*2)\n\n    self.init_weights(self.stn_convnet)\n    self.init_weights(self.stn_fc1)\n    self.init_stn(self.stn_fc2)\n\n  def init_weights(self, module):\n    for m in module.modules():\n      if isinstance(m, nn.Conv2d):\n        n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n        m.weight.data.normal_(0, math.sqrt(2. / n))\n        if m.bias is not None:\n          m.bias.data.zero_()\n      elif isinstance(m, nn.BatchNorm2d):\n        m.weight.data.fill_(1)\n        m.bias.data.zero_()\n      elif isinstance(m, nn.Linear):\n        m.weight.data.normal_(0, 0.001)\n        m.bias.data.zero_()\n\n  def init_stn(self, stn_fc2):\n    margin = 0.01\n    sampling_num_per_side = int(self.num_ctrlpoints / 2)\n    ctrl_pts_x = np.linspace(margin, 1.-margin, sampling_num_per_side)\n    ctrl_pts_y_top = np.ones(sampling_num_per_side) * margin\n    ctrl_pts_y_bottom = np.ones(sampling_num_per_side) * (1-margin)\n    ctrl_pts_top = np.stack([ctrl_pts_x, ctrl_pts_y_top], axis=1)\n    ctrl_pts_bottom = np.stack([ctrl_pts_x, ctrl_pts_y_bottom], axis=1)\n    ctrl_points = np.concatenate([ctrl_pts_top, ctrl_pts_bottom], axis=0).astype(np.float32)\n    if self.activation is \'none\':\n      pass\n    elif self.activation == \'sigmoid\':\n      ctrl_points = -np.log(1. / ctrl_points - 1.)\n    stn_fc2.weight.data.zero_()\n    stn_fc2.bias.data = torch.Tensor(ctrl_points).view(-1)\n\n  def forward(self, x):\n    x = self.stn_convnet(x)\n    batch_size, _, h, w = x.size()\n    x = x.view(batch_size, -1)\n    img_feat = self.stn_fc1(x)\n    x = self.stn_fc2(0.1 * img_feat)\n    if self.activation == \'sigmoid\':\n      x = F.sigmoid(x)\n    x = x.view(-1, self.num_ctrlpoints, 2)\n    return img_feat, x\n\n\nif __name__ == ""__main__"":\n  in_planes = 3\n  num_ctrlpoints = 20\n  activation=\'none\' # \'sigmoid\'\n  stn_head = STNHead(in_planes, num_ctrlpoints, activation)\n  input = torch.randn(10, 3, 32, 64)\n  control_points = stn_head(input)    \n  print(control_points.size())'"
lib/models/tps_spatial_transformer.py,16,"b""\nfrom __future__ import absolute_import\n\nimport numpy as np\nimport itertools\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ndef grid_sample(input, grid, canvas = None):\n  output = F.grid_sample(input, grid)\n  if canvas is None:\n    return output\n  else:\n    input_mask = input.data.new(input.size()).fill_(1)\n    output_mask = F.grid_sample(input_mask, grid)\n    padded_output = output * output_mask + canvas * (1 - output_mask)\n    return padded_output\n\n\n# phi(x1, x2) = r^2 * log(r), where r = ||x1 - x2||_2\ndef compute_partial_repr(input_points, control_points):\n  N = input_points.size(0)\n  M = control_points.size(0)\n  pairwise_diff = input_points.view(N, 1, 2) - control_points.view(1, M, 2)\n  # original implementation, very slow\n  # pairwise_dist = torch.sum(pairwise_diff ** 2, dim = 2) # square of distance\n  pairwise_diff_square = pairwise_diff * pairwise_diff\n  pairwise_dist = pairwise_diff_square[:, :, 0] + pairwise_diff_square[:, :, 1]\n  repr_matrix = 0.5 * pairwise_dist * torch.log(pairwise_dist)\n  # fix numerical error for 0 * log(0), substitute all nan with 0\n  mask = repr_matrix != repr_matrix\n  repr_matrix.masked_fill_(mask, 0)\n  return repr_matrix\n\n\n# output_ctrl_pts are specified, according to our task.\ndef build_output_control_points(num_control_points, margins):\n  margin_x, margin_y = margins\n  num_ctrl_pts_per_side = num_control_points // 2\n  ctrl_pts_x = np.linspace(margin_x, 1.0 - margin_x, num_ctrl_pts_per_side)\n  ctrl_pts_y_top = np.ones(num_ctrl_pts_per_side) * margin_y\n  ctrl_pts_y_bottom = np.ones(num_ctrl_pts_per_side) * (1.0 - margin_y)\n  ctrl_pts_top = np.stack([ctrl_pts_x, ctrl_pts_y_top], axis=1)\n  ctrl_pts_bottom = np.stack([ctrl_pts_x, ctrl_pts_y_bottom], axis=1)\n  # ctrl_pts_top = ctrl_pts_top[1:-1,:]\n  # ctrl_pts_bottom = ctrl_pts_bottom[1:-1,:]\n  output_ctrl_pts_arr = np.concatenate([ctrl_pts_top, ctrl_pts_bottom], axis=0)\n  output_ctrl_pts = torch.Tensor(output_ctrl_pts_arr)\n  return output_ctrl_pts\n\n\n# demo: ~/test/models/test_tps_transformation.py\nclass TPSSpatialTransformer(nn.Module):\n\n  def __init__(self, output_image_size=None, num_control_points=None, margins=None):\n    super(TPSSpatialTransformer, self).__init__()\n    self.output_image_size = output_image_size\n    self.num_control_points = num_control_points\n    self.margins = margins\n\n    self.target_height, self.target_width = output_image_size\n    target_control_points = build_output_control_points(num_control_points, margins)\n    N = num_control_points\n    # N = N - 4\n\n    # create padded kernel matrix\n    forward_kernel = torch.zeros(N + 3, N + 3)\n    target_control_partial_repr = compute_partial_repr(target_control_points, target_control_points)\n    forward_kernel[:N, :N].copy_(target_control_partial_repr)\n    forward_kernel[:N, -3].fill_(1)\n    forward_kernel[-3, :N].fill_(1)\n    forward_kernel[:N, -2:].copy_(target_control_points)\n    forward_kernel[-2:, :N].copy_(target_control_points.transpose(0, 1))\n    # compute inverse matrix\n    inverse_kernel = torch.inverse(forward_kernel)\n\n    # create target cordinate matrix\n    HW = self.target_height * self.target_width\n    target_coordinate = list(itertools.product(range(self.target_height), range(self.target_width)))\n    target_coordinate = torch.Tensor(target_coordinate) # HW x 2\n    Y, X = target_coordinate.split(1, dim = 1)\n    Y = Y / (self.target_height - 1)\n    X = X / (self.target_width - 1)\n    target_coordinate = torch.cat([X, Y], dim = 1) # convert from (y, x) to (x, y)\n    target_coordinate_partial_repr = compute_partial_repr(target_coordinate, target_control_points)\n    target_coordinate_repr = torch.cat([\n      target_coordinate_partial_repr, torch.ones(HW, 1), target_coordinate\n    ], dim = 1)\n\n    # register precomputed matrices\n    self.register_buffer('inverse_kernel', inverse_kernel)\n    self.register_buffer('padding_matrix', torch.zeros(3, 2))\n    self.register_buffer('target_coordinate_repr', target_coordinate_repr)\n    self.register_buffer('target_control_points', target_control_points)\n\n  def forward(self, input, source_control_points):\n    assert source_control_points.ndimension() == 3\n    assert source_control_points.size(1) == self.num_control_points\n    assert source_control_points.size(2) == 2\n    batch_size = source_control_points.size(0)\n\n    Y = torch.cat([source_control_points, self.padding_matrix.expand(batch_size, 3, 2)], 1)\n    mapping_matrix = torch.matmul(self.inverse_kernel, Y)\n    source_coordinate = torch.matmul(self.target_coordinate_repr, mapping_matrix)\n\n    grid = source_coordinate.view(-1, self.target_height, self.target_width, 2)\n    grid = torch.clamp(grid, 0, 1) # the source_control_points may be out of [0, 1].\n    # the input to grid_sample is normalized [-1, 1], but what we get is [0, 1]\n    grid = 2.0 * grid - 1.0\n    output_maps = grid_sample(input, grid, canvas=None)\n    return output_maps, source_coordinate"""
lib/tools/create_sub_lmdb.py,0,"b'import lmdb\nimport six\nimport numpy as np\nfrom PIL import Image\n\nread_root_dir = \'/data/zhui/back/NIPS2014\'\nwrite_root_dir = \'/home/mkyang/data/sub_nips2014\'\nread_env = lmdb.open(read_root_dir, max_readers=32, readonly=True)\nwrite_env = lmdb.open(write_root_dir, map_size=1099511627776)\n\ndef writeCache(env, cache):\n  with env.begin(write=True) as txn:\n    for k, v in cache.items():\n      txn.put(k.encode(), v)\n\nassert read_env is not None, ""cannot create lmdb from %s"" % read_root_dir\nread_txn = read_env.begin()\nnSamples = int(read_txn.get(b""num-samples""))\nsub_nsamples = 10000\nindices = list(np.random.permutation(nSamples))\nindices = indices[:sub_nsamples]\n\ncache = {}\nfor i, index in enumerate(indices):\n  img_key = b\'image-%09d\' % index\n  label_key = b\'label-%09d\' % index\n\n  imgbuf = read_txn.get(img_key)\n  word = read_txn.get(label_key)\n\n  new_img_key = \'image-%09d\' % (i+1)\n  new_label_key = \'label-%09d\' % (i+1)\n  cache[new_img_key] = imgbuf\n  cache[new_label_key] = word\n\ncache[\'num-samples\'] = str(sub_nsamples).encode()\nwriteCache(write_env, cache)'"
lib/tools/create_svtp_lmdb.py,0,"b'import os\nimport lmdb # install lmdb by ""pip install lmdb""\nimport cv2\nimport numpy as np\nfrom tqdm import tqdm\nimport six\nfrom PIL import Image\nimport scipy.io as sio\nfrom tqdm import tqdm\nimport re\n\ndef checkImageIsValid(imageBin):\n  if imageBin is None:\n    return False\n  imageBuf = np.fromstring(imageBin, dtype=np.uint8)\n  img = cv2.imdecode(imageBuf, cv2.IMREAD_GRAYSCALE)\n  imgH, imgW = img.shape[0], img.shape[1]\n  if imgH * imgW == 0:\n    return False\n  return True\n\n\ndef writeCache(env, cache):\n  with env.begin(write=True) as txn:\n    for k, v in cache.items():\n      txn.put(k.encode(), v)\n\n\ndef _is_difficult(word):\n  assert isinstance(word, str)\n  return not re.match(\'^[\\w]+$\', word)\n\n\ndef createDataset(outputPath, imagePathList, labelList, lexiconList=None, checkValid=True):\n  """"""\n  Create LMDB dataset for CRNN training.\n  ARGS:\n      outputPath    : LMDB output path\n      imagePathList : list of image path\n      labelList     : list of corresponding groundtruth texts\n      lexiconList   : (optional) list of lexicon lists\n      checkValid    : if true, check the validity of every image\n  """"""\n  assert(len(imagePathList) == len(labelList))\n  nSamples = len(imagePathList)\n  env = lmdb.open(outputPath, map_size=1099511627776)\n  cache = {}\n  cnt = 1\n  for i in range(nSamples):\n    imagePath = imagePathList[i]\n    label = labelList[i]\n    if len(label) == 0:\n      continue\n    if not os.path.exists(imagePath):\n      print(\'%s does not exist\' % imagePath)\n      continue\n    with open(imagePath, \'rb\') as f:\n      imageBin = f.read()\n    if checkValid:\n      if not checkImageIsValid(imageBin):\n        print(\'%s is not a valid image\' % imagePath)\n        continue\n\n    imageKey = \'image-%09d\' % cnt\n    labelKey = \'label-%09d\' % cnt\n    cache[imageKey] = imageBin\n    cache[labelKey] = label.encode()\n    if lexiconList:\n      lexiconKey = \'lexicon-%09d\' % cnt\n      cache[lexiconKey] = \' \'.join(lexiconList[i])\n    if cnt % 1000 == 0:\n      writeCache(env, cache)\n      cache = {}\n      print(\'Written %d / %d\' % (cnt, nSamples))\n    cnt += 1\n  nSamples = cnt-1\n  cache[\'num-samples\'] = str(nSamples).encode()\n  writeCache(env, cache)\n  print(\'Created dataset with %d samples\' % nSamples)\n\nif __name__ == ""__main__"":\n  data_dir = \'/data/mkyang/datasets/English/benchmark/svtp/\'\n  lmdb_output_path = \'/data/mkyang/datasets/English/benchmark_lmdbs_new/svt_p_645\'\n  gt_file = os.path.join(data_dir, \'gt.txt\')\n  image_dir = data_dir\n  with open(gt_file, \'r\') as f:\n    lines = [line.strip(\'\\n\') for line in f.readlines()]\n\n  imagePathList, labelList = [], []\n  for i, line in enumerate(lines):\n    splits = line.split(\' \')\n    image_name = splits[0]\n    gt_text = splits[1]\n    print(image_name, gt_text)\n    imagePathList.append(os.path.join(image_dir, image_name))\n    labelList.append(gt_text)\n\n  createDataset(lmdb_output_path, imagePathList, labelList)'"
lib/utils/__init__.py,3,"b'from __future__ import absolute_import\n\nimport torch\n\n\ndef to_numpy(tensor):\n  if torch.is_tensor(tensor):\n    return tensor.cpu().numpy()\n  elif type(tensor).__module__ != \'numpy\':\n    raise ValueError(""Cannot convert {} to numpy array""\n                     .format(type(tensor)))\n  return tensor\n\n\ndef to_torch(ndarray):\n  if type(ndarray).__module__ == \'numpy\':\n    return torch.from_numpy(ndarray)\n  elif not torch.is_tensor(ndarray):\n    raise ValueError(""Cannot convert {} to torch tensor""\n                     .format(type(ndarray)))\n  return ndarray'"
lib/utils/labelmaps.py,0,"b'from __future__ import absolute_import\n\nimport string\n\nfrom . import to_torch, to_numpy\n\ndef get_vocabulary(voc_type, EOS=\'EOS\', PADDING=\'PADDING\', UNKNOWN=\'UNKNOWN\'):\n  \'\'\'\n  voc_type: str: one of \'LOWERCASE\', \'ALLCASES\', \'ALLCASES_SYMBOLS\'\n  \'\'\'\n  voc = None\n  types = [\'LOWERCASE\', \'ALLCASES\', \'ALLCASES_SYMBOLS\']\n  if voc_type == \'LOWERCASE\':\n    voc = list(string.digits + string.ascii_lowercase)\n  elif voc_type == \'ALLCASES\':\n    voc = list(string.digits + string.ascii_letters)\n  elif voc_type == \'ALLCASES_SYMBOLS\':\n    voc = list(string.printable[:-6])\n  else:\n    raise KeyError(\'voc_type must be one of ""LOWERCASE"", ""ALLCASES"", ""ALLCASES_SYMBOLS""\')\n\n  # update the voc with specifical chars\n  voc.append(EOS)\n  voc.append(PADDING)\n  voc.append(UNKNOWN)\n\n  return voc\n\n## param voc: the list of vocabulary\ndef char2id(voc):\n  return dict(zip(voc, range(len(voc))))\n\ndef id2char(voc):\n  return dict(zip(range(len(voc)), voc))\n\ndef labels2strs(labels, id2char, char2id):\n  # labels: batch_size x len_seq\n  if labels.ndimension() == 1:\n    labels = labels.unsqueeze(0)\n  assert labels.dim() == 2\n  labels = to_numpy(labels)\n  strings = []\n  batch_size = labels.shape[0]\n\n  for i in range(batch_size):\n    label = labels[i]\n    string = []\n    for l in label:\n      if l == char2id[\'EOS\']:\n        break\n      else:\n        string.append(id2char[l])\n    string = \'\'.join(string)\n    strings.append(string)\n\n  return strings'"
lib/utils/logging.py,0,"b'from __future__ import absolute_import\nimport os\nimport sys\nimport numpy as np\nimport tensorflow as tf\nimport scipy.misc \ntry:\n  from StringIO import StringIO  # Python 2.7\nexcept ImportError:\n  from io import BytesIO         # Python 3.x\n\nfrom .osutils import mkdir_if_missing\n\nfrom config import get_args\nglobal_args = get_args(sys.argv[1:])\n\nif global_args.run_on_remote:\n  import moxing as mox\n  mox.file.shift(""os"", ""mox"")\n\nclass Logger(object):\n  def __init__(self, fpath=None):\n    self.console = sys.stdout\n    self.file = None\n    if fpath is not None:\n      if global_args.run_on_remote:\n        dir_name = os.path.dirname(fpath)\n        if not mox.file.exists(dir_name):\n          mox.file.make_dirs(dir_name)\n          print(\'=> making dir \', dir_name)\n        self.file = mox.file.File(fpath, \'w\')\n        # self.file = open(fpath, \'w\')\n      else:\n        mkdir_if_missing(os.path.dirname(fpath))\n        self.file = open(fpath, \'w\')\n\n  def __del__(self):\n    self.close()\n\n  def __enter__(self):\n    pass\n\n  def __exit__(self, *args):\n    self.close()\n\n  def write(self, msg):\n    self.console.write(msg)\n    if self.file is not None:\n      self.file.write(msg)\n\n  def flush(self):\n    self.console.flush()\n    if self.file is not None:\n      self.file.flush()\n      os.fsync(self.file.fileno())\n\n  def close(self):\n    self.console.close()\n    if self.file is not None:\n      self.file.close()\n\n\nclass TFLogger(object):\n  def __init__(self, log_dir=None):\n    """"""Create a summary writer logging to log_dir.""""""\n    if log_dir is not None:\n      mkdir_if_missing(log_dir)\n    self.writer = tf.summary.FileWriter(log_dir)\n\n  def scalar_summary(self, tag, value, step):\n    """"""Log a scalar variable.""""""\n    summary = tf.Summary(value=[tf.Summary.Value(tag=tag, simple_value=value)])\n    self.writer.add_summary(summary, step)\n    self.writer.flush()\n\n  def image_summary(self, tag, images, step):\n    """"""Log a list of images.""""""\n\n    img_summaries = []\n    for i, img in enumerate(images):\n      # Write the image to a string\n      try:\n        s = StringIO()\n      except:\n        s = BytesIO()\n      scipy.misc.toimage(img).save(s, format=""png"")\n\n      # Create an Image object\n      img_sum = tf.Summary.Image(encoded_image_string=s.getvalue(),\n                                 height=img.shape[0],\n                                 width=img.shape[1])\n      # Create a Summary value\n      img_summaries.append(tf.Summary.Value(tag=\'%s/%d\' % (tag, i), image=img_sum))\n\n    # Create and write Summary\n    summary = tf.Summary(value=img_summaries)\n    self.writer.add_summary(summary, step)\n    self.writer.flush()\n        \n  def histo_summary(self, tag, values, step, bins=1000):\n    """"""Log a histogram of the tensor of values.""""""\n\n    # Create a histogram using numpy\n    counts, bin_edges = np.histogram(values, bins=bins)\n\n    # Fill the fields of the histogram proto\n    hist = tf.HistogramProto()\n    hist.min = float(np.min(values))\n    hist.max = float(np.max(values))\n    hist.num = int(np.prod(values.shape))\n    hist.sum = float(np.sum(values))\n    hist.sum_squares = float(np.sum(values**2))\n\n    # Drop the start of the first bin\n    bin_edges = bin_edges[1:]\n\n    # Add bin edges and counts\n    for edge in bin_edges:\n      hist.bucket_limit.append(edge)\n    for c in counts:\n      hist.bucket.append(c)\n\n    # Create and write Summary\n    summary = tf.Summary(value=[tf.Summary.Value(tag=tag, histo=hist)])\n    self.writer.add_summary(summary, step)\n    self.writer.flush()\n\n  def close(self):\n    self.writer.close()'"
lib/utils/meters.py,0,"b'from __future__ import absolute_import\n\n\nclass AverageMeter(object):\n  """"""Computes and stores the average and current value""""""\n\n  def __init__(self):\n    self.val = 0\n    self.avg = 0\n    self.sum = 0\n    self.count = 0\n\n  def reset(self):\n    self.val = 0\n    self.avg = 0\n    self.sum = 0\n    self.count = 0\n\n  def update(self, val, n=1):\n    self.val = val\n    self.sum += val * n\n    self.count += n\n    self.avg = self.sum / self.count'"
lib/utils/osutils.py,0,"b""from __future__ import absolute_import\nimport os\nimport errno\n\n\ndef mkdir_if_missing(dir_path):\n  try:\n    os.makedirs(dir_path)\n  except OSError as e:\n    if e.errno != errno.EEXIST:\n      raise\n\n\ndef make_symlink_if_not_exists(real_path, link_path):\n  '''\n  param real_path: str the path linked\n  param link_path: str the path with only the symbol\n  '''\n  try:\n    os.makedirs(real_path)\n  except OSError as e:\n    if e.errno != errno.EEXIST:\n      raise\n\n  cmd = 'ln -s {0} {1}'.format(real_path, link_path)\n  os.system(cmd)"""
lib/utils/serialization.py,5,"b'from __future__ import print_function, absolute_import\nimport json\nimport os\nimport sys\n# import moxing as mox\nimport os.path as osp\nimport shutil\n\nimport torch\nfrom torch.nn import Parameter\n\nfrom .osutils import mkdir_if_missing\n\nfrom config import get_args\nglobal_args = get_args(sys.argv[1:])\n\nif global_args.run_on_remote:\n  import moxing as mox\n\n\ndef read_json(fpath):\n  with open(fpath, \'r\') as f:\n    obj = json.load(f)\n  return obj\n\n\ndef write_json(obj, fpath):\n  mkdir_if_missing(osp.dirname(fpath))\n  with open(fpath, \'w\') as f:\n    json.dump(obj, f, indent=4, separators=(\',\', \': \'))\n\n\ndef save_checkpoint(state, is_best, fpath=\'checkpoint.pth.tar\'):\n  print(\'=> saving checkpoint \', fpath)\n  if global_args.run_on_remote:\n    dir_name = osp.dirname(fpath)\n    if not mox.file.exists(dir_name):\n      mox.file.make_dirs(dir_name)\n      print(\'=> makding dir \', dir_name)\n    local_path = ""local_checkpoint.pth.tar""\n    torch.save(state, local_path)\n    mox.file.copy(local_path, fpath)\n    if is_best:\n      mox.file.copy(local_path, osp.join(dir_name, \'model_best.pth.tar\'))\n  else:\n    mkdir_if_missing(osp.dirname(fpath))\n    torch.save(state, fpath)\n    if is_best:\n      shutil.copy(fpath, osp.join(osp.dirname(fpath), \'model_best.pth.tar\'))\n\n\ndef load_checkpoint(fpath):\n  if global_args.run_on_remote:\n    mox.file.shift(\'os\', \'mox\')\n    checkpoint = torch.load(fpath)\n    print(""=> Loaded checkpoint \'{}\'"".format(fpath))\n    return checkpoint\n  else:\n    load_path = fpath\n\n    if osp.isfile(load_path):\n      checkpoint = torch.load(load_path)\n      print(""=> Loaded checkpoint \'{}\'"".format(load_path))\n      return checkpoint\n    else:\n      raise ValueError(""=> No checkpoint found at \'{}\'"".format(load_path))\n\n\ndef copy_state_dict(state_dict, model, strip=None):\n  tgt_state = model.state_dict()\n  copied_names = set()\n  for name, param in state_dict.items():\n    if strip is not None and name.startswith(strip):\n      name = name[len(strip):]\n    if name not in tgt_state:\n      continue\n    if isinstance(param, Parameter):\n      param = param.data\n    if param.size() != tgt_state[name].size():\n      print(\'mismatch:\', name, param.size(), tgt_state[name].size())\n      continue\n    tgt_state[name].copy_(param)\n    copied_names.add(name)\n\n  missing = set(tgt_state.keys()) - copied_names\n  if len(missing) > 0:\n      print(""missing keys in state_dict:"", missing)\n\n  return model'"
lib/utils/visualization_utils.py,1,"b'from __future__ import absolute_import\n\nfrom PIL import Image\nimport os\nimport numpy as np\nfrom collections import OrderedDict\nfrom scipy.misc import imresize\nimport matplotlib\nmatplotlib.use(\'Agg\')\nimport matplotlib.pyplot as plt\nfrom matplotlib.gridspec import GridSpec\nfrom io import BytesIO\nfrom multiprocessing import Pool\nimport math\nimport sys\n\nimport torch\nfrom torch.nn import functional as F\n\nfrom . import to_torch, to_numpy\nfrom ..evaluation_metrics.metrics import get_str_list\n\n\ndef recognition_vis(images, preds, targets, scores, dataset, vis_dir):\n  images = images.permute(0,2,3,1)\n  images = to_numpy(images)\n  images = (images * 0.5 + 0.5)*255\n  pred_list, targ_list = get_str_list(preds, targets, dataset)\n  for id, (image, pred, target, score) in enumerate(zip(images, pred_list, targ_list, scores)):\n    if pred.lower() == target.lower():\n      flag = \'right\'\n    else:\n      flag = \'error\'\n    file_name = \'{:}_{:}_{:}_{:}_{:.3f}.jpg\'.format(flag, id, pred, target, score)\n    file_path = os.path.join(vis_dir, file_name)\n    image = Image.fromarray(np.uint8(image))\n    image.save(file_path)\n\n\n# save to disk sub process\ndef _save_plot_pool(vis_image, save_file_path):\n  vis_image = Image.fromarray(np.uint8(vis_image))\n  vis_image.save(save_file_path)\n\n\ndef stn_vis(raw_images, rectified_images, ctrl_points, preds, targets, real_scores, pred_scores, dataset, vis_dir):\n  """"""\n    raw_images: images without rectification\n    rectified_images: rectified images with stn\n    ctrl_points: predicted ctrl points\n    preds: predicted label sequences\n    targets: target label sequences\n    real_scores: scores of recognition model\n    pred_scores: predicted scores by the score branch\n    dataset: xxx\n    vis_dir: xxx\n  """"""\n  if raw_images.ndimension() == 3:\n    raw_images = raw_images.unsqueeze(0)\n    rectified_images = rectified_images.unsqueeze(0)\n  batch_size, _, raw_height, raw_width = raw_images.size()\n\n  # translate the coordinates of ctrlpoints to image size\n  ctrl_points = to_numpy(ctrl_points)\n  ctrl_points[:,:,0] = ctrl_points[:,:,0] * (raw_width-1)\n  ctrl_points[:,:,1] = ctrl_points[:,:,1] * (raw_height-1)\n  ctrl_points = ctrl_points.astype(np.int)\n\n  # tensors to pil images\n  raw_images = raw_images.permute(0,2,3,1)\n  raw_images = to_numpy(raw_images)\n  raw_images = (raw_images * 0.5 + 0.5)*255\n  rectified_images = rectified_images.permute(0,2,3,1)\n  rectified_images = to_numpy(rectified_images)\n  rectified_images = (rectified_images * 0.5 + 0.5)*255\n\n  # draw images on canvas\n  vis_images = []\n  num_sub_plot = 2\n  raw_images = raw_images.astype(np.uint8)\n  rectified_images = rectified_images.astype(np.uint8)\n  for i in range(batch_size):\n    fig = plt.figure()\n    ax = [fig.add_subplot(num_sub_plot,1,i+1) for i in range(num_sub_plot)]\n    for a in ax:\n      a.set_xticklabels([])\n      a.set_yticklabels([])\n      a.axis(\'off\')\n    ax[0].imshow(raw_images[i])\n    ax[0].scatter(ctrl_points[i,:,0], ctrl_points[i,:,1], marker=\'+\', s=5)\n    ax[1].imshow(rectified_images[i])\n    # plt.subplots_adjust(wspace=0, hspace=0)\n    plt.show()\n    buffer_ = BytesIO()\n    plt.savefig(buffer_, format=\'png\', bbox_inches=\'tight\', pad_inches=0)\n    plt.close()\n    buffer_.seek(0)\n    dataPIL = Image.open(buffer_)\n    data = np.asarray(dataPIL).astype(np.uint8)\n    buffer_.close()\n\n    vis_images.append(data)\n\n  # save to disk\n  if vis_dir is None:\n    return vis_images\n  else:\n    pred_list, targ_list = get_str_list(preds, targets, dataset)\n    file_path_list = []\n    for id, (image, pred, target, real_score) in enumerate(zip(vis_images, pred_list, targ_list, real_scores)):\n      if pred.lower() == target.lower():\n        flag = \'right\'\n      else:\n        flag = \'error\'\n      if pred_scores is None:\n        file_name = \'{:}_{:}_{:}_{:}_{:.3f}.png\'.format(flag, id, pred, target, real_score)\n      else:\n        file_name = \'{:}_{:}_{:}_{:}_{:.3f}_{:.3f}.png\'.format(flag, id, pred, target, real_score, pred_scores[id])\n      file_path = os.path.join(vis_dir, file_name)\n      file_path_list.append(file_path)\n\n    with Pool(os.cpu_count()) as pool:\n      pool.starmap(_save_plot_pool, zip(vis_images, file_path_list))'"
