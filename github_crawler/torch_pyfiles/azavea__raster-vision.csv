file_path,api_count,code
setup.py,0,"b'# flake8: noqa\n\nfrom os import path as op\nimport os\nimport json\nimport io\nimport re\nfrom setuptools import (setup, find_namespace_packages)\nfrom imp import load_source\n\n__version__ = load_source(\'rastervision.version\',\n                          \'rastervision/version.py\').__version__\n\nhere = op.abspath(op.dirname(__file__))\n\n# get the dependencies and installs\nwith io.open(op.join(here, \'requirements.txt\'), encoding=\'utf-8\') as f:\n    all_reqs = f.read().split(\'\\n\')\n\n# The RTD build environment fails with the reqs in bad_reqs.\nif \'READTHEDOCS\' in os.environ:\n    bad_reqs = [\'pyproj\', \'h5py\']\n    all_reqs = list(filter(lambda r: r.split(\'==\')[0] not in bad_reqs, all_reqs))\n\ninstall_requires = [x.strip() for x in all_reqs if \'git+\' not in x]\ndependency_links = [\n    x.strip().replace(\'git+\', \'\') for x in all_reqs if \'git+\' not in x\n]\n\ndef replace_images(readme):\n    """"""Replaces image links in the README with static links to\n    the GitHub release branch.""""""\n    release_branch = \'.\'.join(__version__.split(\'.\')[:2])\n    r = r\'\\((docs/)(.*\\.png)\'\n    rep = (r\'(https://raw.githubusercontent.com/azavea/raster-vision/\'\n           \'{}/docs/\\g<2>\'.format(release_branch))\n\n    return re.sub(r, rep, readme)\n\n# Dependencies for extras, which pertain to installing specific backends.\nwith io.open(op.join(here, \'extras_requirements.json\'), encoding=\'utf-8\') as f:\n    extras_require = json.loads(f.read())\n\n# Uncomment this line if we are using a commit of mask-to-polygons\n# (as opposed to released version) to avoid error.\ndel extras_require[\'feature-extraction\']\n\nsetup(\n    name=\'rastervision\',\n    version=__version__,\n    description=\'An open source framework for deep learning \'\n    \'on satellite and aerial imagery\',\n    long_description=replace_images(open(\'README.md\').read()),\n    long_description_content_type=\'text/markdown\',\n    url=\'https://github.com/azavea/raster-vision\',\n    author=\'Azavea\',\n    author_email=\'info@azavea.com\',\n    license=\'Apache License 2.0\',\n    classifiers=[\n        \'Intended Audience :: Developers\',\n        \'License :: OSI Approved :: Apache Software License\',\n        \'Programming Language :: Python\',\n        \'Programming Language :: Python :: 3\',\n    ],\n    keywords=\n    \'raster deep-learning ml computer-vision earth-observation geospatial geospatial-processing\',\n    packages=find_namespace_packages(exclude=[\'integration_tests*\', \'tests*\']),\n    include_package_data=True,\n    install_requires=install_requires,\n    extras_require=extras_require,\n    dependency_links=dependency_links,\n    entry_points=\'\'\'\n        [console_scripts]\n        rastervision=rastervision.cli.main:main\n        rastervision2=rastervision2.pipeline.cli:main\n    \'\'\',\n)\n'"
docs/conf.py,0,"b'from pallets_sphinx_themes import ProjectLink, get_version\n\n# -*- coding: utf-8 -*-\n#\n# Configuration file for the Sphinx documentation builder.\n#\n# This file does only contain a selection of the most common options. For a\n# full list see the documentation:\n# http://www.sphinx-doc.org/en/stable/config\n\n# -- Path setup --------------------------------------------------------------\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\n# import os\n# import sys\n# sys.path.insert(0, os.path.abspath(\'.\'))\n\n\n# -- Project information -----------------------------------------------------\n\nproject = \'Raster Vision\'\ncopyright = \'2018, Azavea\'\nauthor = \'Azavea\'\n\n# The short X.Y version\nversion = \'0.10\'\n# The full version, including alpha/beta/rc tags\nrelease = \'0.10.0\'\n\n\n# -- General configuration ---------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\n    \'sphinx.ext.autodoc\',\n    \'sphinx.ext.intersphinx\',\n    \'pallets_sphinx_themes\',\n    \'sphinxcontrib.programoutput\',\n    \'sphinxcontrib.napoleon\',\n]\n\n# https://read-the-docs.readthedocs.io/en/latest/faq.html#i-get-import-errors-on-libraries-that-depend-on-c-modules\nimport sys\nfrom unittest.mock import MagicMock\n\nclass Mock(MagicMock):\n    @classmethod\n    def __getattr__(cls, name):\n        return MagicMock()\n\nMOCK_MODULES = [\'pyproj\', \'h5py\']\nsys.modules.update((mod_name, Mock()) for mod_name in MOCK_MODULES)\n\nautodoc_mock_imports = [\'torch\', \'torchvision\', \'pycocotools\']\n\nintersphinx_mapping = {\'python\': (\'https://docs.python.org/3/\', None)}\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\n# source_suffix = [\'.rst\', \'.md\']\nsource_suffix = \'.rst\'\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This pattern also affects html_static_path and html_extra_path .\nexclude_patterns = [\'_build\', \'Thumbs.db\', \'.DS_Store\', \'README.md\']\n\n# The name of the Pygments (syntax highlighting) style to use.\n# pygments_style = \'sphinx\'\n\n# HTML -----------------------------------------------------------------\n\nhtml_theme = \'click\'\nhtml_theme_options = {\'index_sidebar_logo\': False}\nhtml_context = {\n    \'project_links\': [\n        ProjectLink(\'Quickstart\', \'quickstart.html\'),\n        ProjectLink(\'Documentation TOC\', \'index.html#documentation\'),\n        ProjectLink(\'API Reference TOC\', \'index.html#api-reference\'),\n        ProjectLink(\'Project Website\', \'https://rastervision.io/\'),\n        ProjectLink(\'PyPI releases\', \'https://pypi.org/project/rastervision/\'),\n        ProjectLink(\'GitHub\', \'https://github.com/azavea/raster-vision\'),\n        ProjectLink(\'Gitter Channel\', \'https://gitter.im/azavea/raster-vision\'),\n        ProjectLink(\'Raster Vision Examples\', \'https://github.com/azavea/raster-vision-examples\'),\n        ProjectLink(\'AWS Batch Setup\', \'https://github.com/azavea/raster-vision-aws\'),\n        ProjectLink(\'Issue Tracker\', \'https://github.com/azavea/raster-vision/issues/\'),\n        ProjectLink(\'CHANGELOG\', \'changelog.html\'),\n        ProjectLink(\'Azavea\', \'https://www.azavea.com/\'),\n    ],\n    \'css_files\': [\n        \'_static/rastervision.css\',\n        \'https://media.readthedocs.org/css/badge_only.css\'\n    ]\n}\nhtml_sidebars = {\n    \'index\': [\'project.html\', \'versions.html\', \'searchbox.html\'],\n    \'**\': [\'project.html\', \'localtoc.html\', \'relations.html\', \'versions.html\', \'searchbox.html\'],\n}\nsinglehtml_sidebars = {\'index\': [\'project.html\', \'versions.html\', \'localtoc.html\']}\nhtml_static_path = [\'_static\']\nhtml_favicon = \'_static/raster-vision-icon.png\'\nhtml_logo = \'_static/raster-vision-logo.png\'\nhtml_title = \'Raster Vision Documentation ({})\'.format(version)\nhtml_show_sourcelink = False\nhtml_domain_indices = False\nhtml_experimental_html5_writer = True\n\n# -- Options for HTMLHelp output ---------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'RasterVisiondoc\'\n\n\n# -- Options for LaTeX output ------------------------------------------------\n\nlatex_elements = {\n    # The paper size (\'letterpaper\' or \'a4paper\').\n    #\n    # \'papersize\': \'letterpaper\',\n\n    # The font size (\'10pt\', \'11pt\' or \'12pt\').\n    #\n    # \'pointsize\': \'10pt\',\n\n    # Additional stuff for the LaTeX preamble.\n    #\n    # \'preamble\': \'\',\n\n    # Latex figure (float) alignment\n    #\n    # \'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, \'RasterVision.tex\', \'Raster Vision Documentation\',\n     \'Azavea\', \'manual\'),\n]\n\n\n# -- Options for manual page output ------------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (master_doc, \'RasterVisoin-{}.tex\', html_title,\n     [author], \'manual\')\n]\n\n\n# -- Options for Texinfo output ----------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (master_doc, \'RasterVision\', \'Raster Vision Documentation\',\n     author, \'RasterVision\', \'One line description of project.\',\n     \'Miscellaneous\'),\n]\n\n\n# -- Extension configuration -------------------------------------------------\n\nprogramoutput_prompt_template = \'> {command}\\n{output}\'\n\n# -- Options for todo extension ----------------------------------------------\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = True\n'"
integration_tests/__init__.py,0,b''
integration_tests/__main__.py,0,"b""from integration_tests.integration_tests import main\n\nif __name__ == '__main__':\n    main()\n"""
integration_tests/integration_tests.py,0,"b'#!/usr/bin/env python\n\nfrom copy import deepcopy\nimport json\nimport os\nimport math\nimport traceback\n\nimport click\nimport numpy as np\n\nimport rastervision as rv\n\nfrom integration_tests.chip_classification_tests.experiment \\\n    import ChipClassificationIntegrationTest\nfrom integration_tests.object_detection_tests.experiment \\\n    import ObjectDetectionIntegrationTest\nfrom integration_tests.semantic_segmentation_tests.experiment \\\n    import SemanticSegmentationIntegrationTest\nfrom rastervision.rv_config import RVConfig\n\nall_tests = [\n    rv.CHIP_CLASSIFICATION, rv.OBJECT_DETECTION, rv.SEMANTIC_SEGMENTATION\n]\n\nnp.random.seed(1234)\nif rv.backend.tf_available:\n    import tensorflow\n    tensorflow.set_random_seed(5678)\n\n# Suppress warnings and info to avoid cluttering CI log\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\n\nTEST_ROOT_DIR = os.path.dirname(os.path.abspath(__file__))\n\n\nclass IntegrationTestExperimentRunner(rv.runner.LocalExperimentRunner):\n    def __init__(self, tmp_dir=None):\n        super().__init__(tmp_dir)\n\n    def _run_experiment(self, command_dag):\n        """"""Check serialization of all commands.""""""\n        for command_config in command_dag.get_sorted_commands():\n            deepcopy(\n                rv.command.CommandConfig.from_proto(command_config.to_proto()))\n\n        super()._run_experiment(command_dag)\n\n\ndef console_info(msg):\n    click.echo(click.style(msg, fg=\'green\'))\n\n\ndef console_warning(msg):\n    click.echo(click.style(msg, fg=\'yellow\'))\n\n\ndef console_error(msg):\n    click.echo(click.style(msg, fg=\'red\', err=True))\n\n\nclass TestError():\n    def __init__(self, test, message, details=None):\n        self.test = test\n        self.message = message\n        self.details = details\n\n    def __str__(self):\n        return (\'Error\\n\' + \'------\\n\' + \'Test: {}\\n\'.format(self.test) +\n                \'Message: {}\\n\'.format(self.message) + \'Details: {}\'.format(\n                    str(self.details)) if self.details else \'\' + \'\\n\')\n\n\ndef get_test_dir(test):\n    return os.path.join(TEST_ROOT_DIR, test.lower().replace(\'-\', \'_\'))\n\n\ndef get_expected_eval_path(test):\n    return os.path.join(\'{}_tests\'.format(get_test_dir(test)),\n                        \'expected-output/eval.json\')\n\n\ndef get_actual_eval_path(test, temp_dir):\n    return os.path.join(temp_dir, test.lower(), \'eval/default/eval.json\')\n\n\ndef open_json(path):\n    with open(path, \'r\') as file:\n        return json.load(file)\n\n\ndef check_eval_item(test, expected_item, actual_item):\n    errors = []\n    f1_threshold = 0.01\n    class_name = expected_item[\'class_name\']\n\n    expected_f1 = expected_item[\'f1\'] or 0.0\n    actual_f1 = actual_item[\'f1\'] or 0.0\n    if math.fabs(expected_f1 - actual_f1) > f1_threshold:\n        errors.append(\n            TestError(\n                test, \'F1 scores are not close enough\',\n                \'for class_name: {} expected f1: {}, actual f1: {}\'.format(\n                    class_name, expected_item[\'f1\'], actual_item[\'f1\'])))\n\n    return errors\n\n\ndef check_eval(test, temp_dir):\n    errors = []\n\n    actual_eval_path = get_actual_eval_path(test, temp_dir)\n    expected_eval_path = get_expected_eval_path(test)\n\n    if os.path.isfile(actual_eval_path):\n        expected_eval = open_json(expected_eval_path)[\'overall\']\n        actual_eval = open_json(actual_eval_path)[\'overall\']\n\n        for expected_item in expected_eval:\n            class_name = expected_item[\'class_name\']\n            actual_item = \\\n                next(filter(\n                    lambda x: x[\'class_name\'] == class_name, actual_eval))\n            errors.extend(check_eval_item(test, expected_item, actual_item))\n    else:\n        errors.append(\n            TestError(test, \'actual eval file does not exist\',\n                      actual_eval_path))\n\n    return errors\n\n\ndef get_experiment(test, use_tf, tmp_dir):\n    if test == rv.OBJECT_DETECTION:\n        return ObjectDetectionIntegrationTest().exp_main(\n            os.path.join(tmp_dir, test.lower()), use_tf=use_tf)\n    if test == rv.CHIP_CLASSIFICATION:\n        return ChipClassificationIntegrationTest().exp_main(\n            os.path.join(tmp_dir, test.lower()), use_tf=use_tf)\n    if test == rv.SEMANTIC_SEGMENTATION:\n        return SemanticSegmentationIntegrationTest().exp_main(\n            os.path.join(tmp_dir, test.lower()), use_tf=use_tf)\n\n    raise Exception(\'Unknown test {}\'.format(test))\n\n\ndef test_prediction_package_validation(experiment, test, temp_dir, image_uri):\n    console_info(\'Checking predict command validation...\')\n    errors = []\n    pp = experiment.task.predict_package_uri\n    predict = rv.Predictor(pp, temp_dir, channel_order=[0, 1, 7]).predict\n    try:\n        predict(image_uri, \'x.txt\')\n        e = TestError(test,\n                      (\'Predictor should have raised exception due to invalid \'\n                       \'channel_order, but did not.\'),\n                      \'in experiment {}\'.format(experiment.id))\n        errors.append(e)\n    except ValueError:\n        pass\n\n    return errors\n\n\ndef test_prediction_package_results(experiment, test, temp_dir, scenes,\n                                    scenes_to_uris):\n    console_info(\'Checking predict package produces same results...\')\n    errors = []\n    pp = experiment.task.predict_package_uri\n    predict = rv.Predictor(pp, temp_dir).predict\n\n    for scene_config in scenes:\n        # Need to write out labels and read them back,\n        # otherwise the floating point precision direct box\n        # coordinates will not match those from the PREDICT\n        # command, which are rounded to pixel coordinates\n        # via pyproj logic (in the case of rasterio crs transformer.\n        predictor_label_store_uri = os.path.join(\n            temp_dir, test.lower(), \'predictor/{}\'.format(scene_config.id))\n        uri = scenes_to_uris[scene_config.id]\n\n        predict(uri, predictor_label_store_uri)\n\n        scene = scene_config.create_scene(experiment.task, temp_dir)\n\n        scene_labels = scene.prediction_label_store.get_labels()\n\n        extent = scene.raster_source.get_extent()\n        crs_transformer = scene.raster_source.get_crs_transformer()\n        predictor_label_store = scene_config.label_store \\\n                                       .for_prediction(\n                                           predictor_label_store_uri) \\\n                                       .create_store(\n                                           experiment.task,\n                                           extent,\n                                           crs_transformer,\n                                           temp_dir)\n\n        from rastervision.data import ActivateMixin\n        with ActivateMixin.compose(scene, predictor_label_store):\n            if not predictor_label_store.get_labels() == scene_labels:\n                e = TestError(\n                    test, (\'Predictor did not produce the same labels \'\n                           \'as the Predict command\'),\n                    \'for scene {} in experiment {}\'.format(\n                        scene_config.id, experiment.id))\n                errors.append(e)\n\n    return errors\n\n\ndef test_prediction_package(experiment,\n                            test,\n                            temp_dir,\n                            check_channel_order=False):\n    # Check the prediction package\n    # This will only work with raster_sources that\n    # have a single URI.\n    skip = False\n    errors = []\n    experiment = experiment.fully_resolve()\n\n    scenes_to_uris = {}\n    scenes = experiment.dataset.validation_scenes\n    for scene in scenes:\n        rs = scene.raster_source\n        if hasattr(rs, \'uri\'):\n            scenes_to_uris[scene.id] = rs.uri\n        elif hasattr(rs, \'uris\'):\n            uris = rs.uris\n            if len(uris) > 1:\n                skip = True\n            else:\n                scenes_to_uris[scene.id] = uris[0]\n        else:\n            skip = True\n\n    if skip:\n        console_warning(\'Skipping predict package test for \'\n                        \'test {}, experiment {}\'.format(test, experiment.id))\n    else:\n        if check_channel_order:\n            errors.extend(\n                test_prediction_package_validation(experiment, test, temp_dir,\n                                                   uris[0]))\n        else:\n            errors.extend(\n                test_prediction_package_results(experiment, test, temp_dir,\n                                                scenes, scenes_to_uris))\n\n    return errors\n\n\ndef run_test(test, use_tf, temp_dir):\n    errors = []\n    experiment = get_experiment(test, use_tf, temp_dir)\n    commands_to_run = rv.all_commands()\n\n    # Check serialization\n    pp_uri = os.path.join(experiment.bundle_uri, \'predict_package.zip\')\n    experiment.task.predict_package_uri = pp_uri\n    msg = experiment.to_proto()\n    experiment = rv.ExperimentConfig.from_proto(msg)\n\n    # Check that running doesn\'t raise any exceptions.\n    try:\n        IntegrationTestExperimentRunner(os.path.join(temp_dir, test.lower())) \\\n            .run(experiment, rerun_commands=True, splits=2,\n                 commands_to_run=commands_to_run)\n\n    except Exception:\n        errors.append(\n            TestError(test, \'raised an exception while running\',\n                      traceback.format_exc()))\n        return errors\n\n    # Check that the eval is similar to expected eval.\n    errors.extend(check_eval(test, temp_dir))\n\n    if not errors:\n        errors.extend(test_prediction_package(experiment, test, temp_dir))\n        errors.extend(\n            test_prediction_package(\n                experiment, test, temp_dir, check_channel_order=True))\n\n    return errors\n\n\n@click.command()\n@click.argument(\'tests\', nargs=-1)\n@click.option(\n    \'--rv_root\',\n    \'-t\',\n    help=(\'Sets the rv_root directory used. \'\n          \'If set, test will not clean this directory up.\'))\n@click.option(\n    \'--verbose\', \'-v\', is_flag=True, help=(\'Sets the logging level to DEBUG.\'))\n@click.option(\n    \'--use-tf\', \'-v\', is_flag=True, help=(\'Run using TF-based backends.\'))\ndef main(tests, rv_root, verbose, use_tf):\n    """"""Runs RV end-to-end and checks that evaluation metrics are correct.""""""\n    if len(tests) == 0:\n        tests = all_tests\n\n    if verbose:\n        rv._registry.initialize_config(\n            verbosity=rv.cli.verbosity.Verbosity.DEBUG)\n\n    tests = list(map(lambda x: x.upper(), tests))\n\n    with RVConfig.get_tmp_dir() as temp_dir:\n        if rv_root:\n            temp_dir = rv_root\n\n        errors = []\n        for test in tests:\n            if test not in all_tests:\n                print(\'{} is not a valid test.\'.format(test))\n                return\n\n            errors.extend(run_test(test, use_tf, temp_dir))\n\n            for error in errors:\n                print(error)\n\n        for test in tests:\n            nb_test_errors = len(\n                list(filter(lambda error: error.test == test, errors)))\n            if nb_test_errors == 0:\n                print(\'{} test passed!\'.format(test))\n\n        if errors:\n            exit(1)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
integration_tests2/__init__.py,0,b''
integration_tests2/__main__.py,0,"b""from integration_tests2.integration_tests import main\n\nif __name__ == '__main__':\n    main()\n"""
integration_tests2/integration_tests.py,0,"b'#!/usr/bin/env python\n\nfrom os.path import join, dirname, abspath, isfile\nimport math\nimport traceback\nimport importlib\n\nimport click\nimport numpy as np\n\nfrom rastervision2.core import Predictor\nfrom rastervision2.pipeline import rv_config, Verbosity\nfrom rastervision2.pipeline.file_system import file_to_json\nfrom rastervision2.pipeline.runner import InProcessRunner\nfrom rastervision2.pipeline.cli import _run_pipeline\n\nchip_classification = \'chip_classification\'\nobject_detection = \'object_detection\'\nsemantic_segmentation = \'semantic_segmentation\'\nall_tests = [chip_classification, object_detection, semantic_segmentation]\nTEST_ROOT_DIR = dirname(abspath(__file__))\n\nnp.random.seed(1234)\n\n\ndef console_info(msg):\n    click.echo(click.style(msg, fg=\'green\'))\n\n\ndef console_warning(msg):\n    click.echo(click.style(msg, fg=\'yellow\'))\n\n\ndef console_error(msg):\n    click.echo(click.style(msg, fg=\'red\', err=True))\n\n\nclass TestError():\n    def __init__(self, test, message, details=None):\n        self.test = test\n        self.message = message\n        self.details = details\n\n    def __str__(self):\n        return (\'Error\\n\' + \'------\\n\' + \'Test: {}\\n\'.format(self.test) +\n                \'Message: {}\\n\'.format(self.message) + \'Details: {}\'.format(\n                    str(self.details)) if self.details else \'\' + \'\\n\')\n\n\ndef get_test_dir(test):\n    return join(TEST_ROOT_DIR, test.lower().replace(\'-\', \'_\'))\n\n\ndef get_expected_eval_path(test):\n    return join(get_test_dir(test), \'expected-output/eval.json\')\n\n\ndef get_actual_eval_path(test, tmp_dir):\n    return join(tmp_dir, test.lower(), \'eval/eval.json\')\n\n\ndef check_eval_item(test, expected_item, actual_item):\n    errors = []\n    f1_threshold = 0.05\n    class_name = expected_item[\'class_name\']\n\n    expected_f1 = expected_item[\'f1\'] or 0.0\n    actual_f1 = actual_item[\'f1\'] or 0.0\n    if math.fabs(expected_f1 - actual_f1) > f1_threshold:\n        errors.append(\n            TestError(\n                test, \'F1 scores are not close enough\',\n                \'for class_name: {} expected f1: {}, actual f1: {}\'.format(\n                    class_name, expected_item[\'f1\'], actual_item[\'f1\'])))\n\n    return errors\n\n\ndef check_eval(test, tmp_dir):\n    errors = []\n\n    actual_eval_path = get_actual_eval_path(test, tmp_dir)\n    expected_eval_path = get_expected_eval_path(test)\n\n    if isfile(actual_eval_path):\n        expected_eval = file_to_json(expected_eval_path)[\'overall\']\n        actual_eval = file_to_json(actual_eval_path)[\'overall\']\n\n        for expected_item in expected_eval:\n            class_name = expected_item[\'class_name\']\n            actual_item = \\\n                next(filter(\n                    lambda x: x[\'class_name\'] == class_name, actual_eval))\n            errors.extend(check_eval_item(test, expected_item, actual_item))\n    else:\n        errors.append(\n            TestError(test, \'actual eval file does not exist\',\n                      actual_eval_path))\n\n    return errors\n\n\ndef test_model_bundle_validation(pipeline, test, tmp_dir, image_uri):\n    console_info(\'Checking predict command validation...\')\n    errors = []\n    model_bundle_uri = pipeline.get_model_bundle_uri()\n    predictor = Predictor(model_bundle_uri, tmp_dir, channel_order=[0, 1, 7])\n    try:\n        predictor.predict([image_uri], \'x.txt\')\n        e = TestError(test,\n                      (\'Predictor should have raised exception due to invalid \'\n                       \'channel_order, but did not.\'))\n        errors.append(e)\n    except ValueError:\n        pass\n\n    return errors\n\n\ndef test_model_bundle_results(pipeline, test, tmp_dir, scenes, scenes_to_uris):\n    console_info(\'Checking model bundle produces same results...\')\n    errors = []\n    model_bundle_uri = pipeline.get_model_bundle_uri()\n    predictor = Predictor(model_bundle_uri, tmp_dir)\n\n    for scene_cfg in scenes:\n        # Need to write out labels and read them back,\n        # otherwise the floating point precision direct box\n        # coordinates will not match those from the PREDICT\n        # command, which are rounded to pixel coordinates\n        # via pyproj logic (in the case of rasterio crs transformer.\n        scene = scene_cfg.build(pipeline.dataset.class_config, tmp_dir)\n\n        predictor_label_store_uri = join(\n            tmp_dir, test.lower(), \'predictor/{}\'.format(scene_cfg.id))\n        image_uri = scenes_to_uris[scene_cfg.id]\n        predictor.predict([image_uri], predictor_label_store_uri)\n\n        extent = scene.raster_source.get_extent()\n        crs_transformer = scene.raster_source.get_crs_transformer()\n        predictor_label_store = scene_cfg.label_store.copy()\n        predictor_label_store.uri = predictor_label_store_uri\n        predictor_label_store = predictor_label_store.build(\n            pipeline.dataset.class_config, crs_transformer, extent, tmp_dir)\n\n        from rastervision2.core.data import ActivateMixin\n        with ActivateMixin.compose(scene, predictor_label_store):\n            if not (predictor_label_store.get_labels() ==\n                    scene.prediction_label_store.get_labels()):\n                e = TestError(\n                    test, (\'Predictor did not produce the same labels \'\n                           \'as the Predict command\'),\n                    \'for scene {}\'.format(scene_cfg.id))\n                errors.append(e)\n\n    return errors\n\n\ndef test_model_bundle(pipeline, test, tmp_dir, check_channel_order=False):\n    # Check the model bundle.\n    # This will only work with raster_sources that\n    # have a single URI.\n    skip = False\n    errors = []\n\n    scenes_to_uris = {}\n    scenes = pipeline.dataset.validation_scenes\n    for scene in scenes:\n        rs = scene.raster_source\n        if hasattr(rs, \'uri\'):\n            scenes_to_uris[scene.id] = rs.uri\n        elif hasattr(rs, \'uris\'):\n            uris = rs.uris\n            if len(uris) > 1:\n                skip = True\n            else:\n                scenes_to_uris[scene.id] = uris[0]\n        else:\n            skip = True\n\n    if skip:\n        console_warning(\'Skipping predict package test for test {}.\')\n    else:\n        if check_channel_order:\n            errors.extend(\n                test_model_bundle_validation(pipeline, test, tmp_dir, uris[0]))\n        else:\n            errors.extend(\n                test_model_bundle_results(\n                    pipeline, test, tmp_dir, scenes, scenes_to_uris))\n\n    return errors\n\n\ndef run_test(test, tmp_dir):\n    errors = []\n    config_mod = importlib.import_module(\n        \'integration_tests2.{}.config\'.format(test))\n    runner = \'inprocess\'\n    root_uri = join(tmp_dir, test)\n    pipeline_cfg = config_mod.get_config(runner, root_uri)\n    pipeline_cfg.update()\n    runner = InProcessRunner()\n\n    # Check that running doesn\'t raise any exceptions.\n    try:\n        _run_pipeline(pipeline_cfg, runner, tmp_dir)\n    except Exception:\n        errors.append(\n            TestError(test, \'raised an exception while running\', traceback.format_exc()))\n        return errors\n\n    # Check that the eval is similar to expected eval.\n    errors.extend(check_eval(test, tmp_dir))\n\n    if not errors:\n        errors.extend(test_model_bundle(pipeline_cfg, test, tmp_dir))\n        errors.extend(\n            test_model_bundle(pipeline_cfg, test, tmp_dir, check_channel_order=True))\n\n    return errors\n\n\n@click.command()\n@click.argument(\'tests\', nargs=-1)\n@click.option(\n    \'--root-uri\',\n    \'-t\',\n    help=(\'Sets the rv_root directory used. \'\n          \'If set, test will not clean this directory up.\'))\n@click.option(\n    \'--verbose\', \'-v\', is_flag=True, help=(\'Sets the logging level to DEBUG.\'))\ndef main(tests, root_uri, verbose):\n    """"""Runs RV end-to-end and checks that evaluation metrics are correct.""""""\n    if len(tests) == 0:\n        tests = all_tests\n\n    if verbose:\n        rv_config.set(verbosity=Verbosity.DEBUG)\n\n    with rv_config.get_tmp_dir() as tmp_dir:\n        if root_uri:\n            tmp_dir = root_uri\n\n        errors = []\n        for test in tests:\n            if test not in all_tests:\n                print(\'{} is not a valid test.\'.format(test))\n                return\n\n            errors.extend(run_test(test, tmp_dir))\n\n            for error in errors:\n                print(error)\n\n        for test in tests:\n            nb_test_errors = len(\n                list(filter(lambda error: error.test == test, errors)))\n            if nb_test_errors == 0:\n                print(\'{} test passed!\'.format(test))\n\n        if errors:\n            exit(1)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
rastervision/__init__.py,0,"b""# flake8: noqa\nimport logging\n\nfrom rastervision.core import ConfigError\nfrom rastervision.analyzer.api import *\nfrom rastervision.augmentor.api import *\nfrom rastervision.backend.api import *\nfrom rastervision.command.api import *\nfrom rastervision.data.api import *\nfrom rastervision.evaluation.api import *\nfrom rastervision.experiment.api import *\nfrom rastervision.runner.api import *\nfrom rastervision.task.api import *\n\nfrom rastervision.predictor import Predictor\n\nfrom rastervision.cli.main import main\n\nfrom rastervision.registry import Registry\n\nroot_logger = logging.getLogger('rastervision')\nsh = logging.StreamHandler()\nsh.setLevel(logging.DEBUG)\nformatter = logging.Formatter(\n    '%(asctime)s:%(name)s: %(levelname)s - %(message)s', '%Y-%m-%d %H:%M:%S')\nsh.setFormatter(formatter)\nroot_logger.addHandler(sh)\n\n_registry = None\n\n\ndef _initialize():\n    global _registry\n\n    if not _registry:\n        _registry = Registry()\n\n\n_initialize()\n"""
rastervision/__main__.py,0,"b'""""""Main entry point""""""\nimport sys\n\nimport rastervision as rv\n\n# Code taken from unittest\nif sys.argv[0].endswith(\'__main__.py\'):\n    import os.path\n    # We change sys.argv[0] to make help message more useful\n    # use executable without path, unquoted\n    # (it\'s just a hint anyway)\n    # (if you have spaces in your executable you get what you deserve!)\n    executable = os.path.basename(sys.executable)\n    sys.argv[0] = executable + \' -m rastervision\'\n    del os\n\nrv.main()\n'"
rastervision/plugin.py,0,"b'import os\nimport json\nimport importlib\n\nfrom pluginbase import PluginBase\n\nimport rastervision as rv\nfrom rastervision.protos.plugin_pb2 import PluginConfig as PluginConfigMsg\nfrom rastervision.utils.files import download_if_needed\n\n\nclass PluginError(Exception):\n    pass\n\n\ndef load_conf_list(s):\n    """"""Loads a list of items from the config.\n\n    Lists should be comma separated.\n\n    This takes into account that previous versions of Raster Vision\n    allowed for a `[ ""module"" ]` like syntax, even though that didn\'t\n    work for multi-value lists.\n    """"""\n    try:\n        # A comma separated list of values will be transformed to\n        # having a list-like string, with \' instead of "". Replacing\n        # single quotes with double quotes lets us parse it as a JSON list.\n        return json.loads(s.replace(""\'"", \'""\'))\n    except json.JSONDecodeError:\n        return list(map(lambda x: x.strip(), s.split(\',\')))\n\n\nclass PluginRegistry:\n    @staticmethod\n    def get_instance():\n        return rv._registry._get_plugin_registry()\n\n    def __init__(self, plugin_config, rv_home):\n        """"""Initializes this plugin registry.\n\n        A plugin registry is passed to plugins in a call\n        to their ""register_plugin"" method.\n\n        Args:\n           plugin_config - the everett ConfigManager for the plugin\n                           section of the application configuration.\n        """"""\n        self.plugin_root_dir = os.path.join(rv_home, \'plugins\')\n        self.config_builders = {}\n        self.command_config_builders = {}\n        self.commands = []\n        self.aux_command_classes = {}\n        self.default_raster_sources = []\n        self.default_vector_sources = []\n        self.default_label_sources = []\n        self.default_label_stores = []\n        self.default_evaluators = []\n        self.experiment_runners = {}\n        self.filesystems = []\n\n        plugin_files = load_conf_list(plugin_config(\'files\', default=\'[]\'))\n        self._load_from_files(plugin_files)\n        self.plugin_files = plugin_files\n\n        plugin_modules = load_conf_list(plugin_config(\'modules\', default=\'[]\'))\n        self._load_from_modules(plugin_modules)\n        self.plugin_modules = plugin_modules\n\n    def _load_plugin(self, plugin, identifier):\n        # Check the plugin is valid\n        if not hasattr(plugin, \'register_plugin\'):\n            raise PluginError(\'Plugin at {} does not have \'\n                              \'""register_plugin"" method.\'.format(identifier))\n\n        register_method = getattr(plugin, \'register_plugin\')\n        if not callable(register_method):\n            raise PluginError(\'Plugin at {} has a \'\n                              \'""register_plugin"" attribute, \'\n                              \'but it is not callable\'.format(identifier))\n\n        # TODO: Log loading plugin.\n        register_method(self)\n\n    def _load_from_files(self, plugin_paths):\n        if not plugin_paths:\n            return\n\n        self.plugin_sources = []\n\n        plugin_base = PluginBase(package=\'rastervision.plugins\')\n        for uri in plugin_paths:\n            plugin_name = os.path.splitext(os.path.basename(uri))[0]\n            plugin_path = os.path.join(self.plugin_root_dir, plugin_name)\n            fs = rv._registry.get_file_system(uri, search_plugins=False)\n            local_path = download_if_needed(uri, plugin_path, fs=fs)\n            local_dir = os.path.dirname(local_path)\n\n            plugin_source = plugin_base.make_plugin_source(\n                searchpath=[local_dir])\n\n            # We\'re required to hang onto the source\n            # to keep it from getting GC\'d.\n            self.plugin_sources.append(plugin_source)\n\n            self._load_plugin(plugin_source.load_plugin(plugin_name), uri)\n\n    def _load_from_modules(self, plugin_modules):\n        if not plugin_modules:\n            return\n\n        for module in plugin_modules:\n            plugin = importlib.import_module(module)\n            self._load_plugin(plugin, module)\n\n    def add_plugins_from_proto(self, plugin_msg):\n        new_plugin_files = list(\n            set(plugin_msg.plugin_uris) - set(self.plugin_files))\n        self._load_from_files(new_plugin_files)\n        self.plugin_files.extend(new_plugin_files)\n\n        new_plugin_modules = list(\n            set(plugin_msg.plugin_modules) - set(self.plugin_modules))\n        self._load_from_modules(new_plugin_modules)\n        self.plugin_modules.extend(new_plugin_modules)\n\n    def to_proto(self):\n        """"""Returns a protobuf message that records the\n        plugin sources for plugins that are currently loaded\n        in the registry.\n        """"""\n        return PluginConfigMsg(\n            plugin_uris=self.plugin_files, plugin_modules=self.plugin_modules)\n\n    def register_config_builder(self, group, key, builder_class):\n        """"""Registers a ConfigBuilder as a plugin.\n\n        Args:\n           group - The Config group, e.g. rv.BACKEND, rv.TASK.\n           key - The key used for this plugin. This will be used to\n                 construct the builder in a "".builder(key)"" call.\n           builder_class - The subclass of ConfigBuilder that builds\n                           the Config for this plugin.\n        """"""\n        if (group, key) in self.config_builders:\n            raise PluginError(\'ConfigBuilder already registered for group \'\n                              \'{} and key {}\'.format(group, key))\n        self.config_builders[(group, key)] = builder_class\n\n    def register_command_config_builder(self, command_type, builder_class):\n        """"""Registers a ConfigBuilder as a plugin.\n\n        Args:\n           command_type - The key used for this plugin. This will be used to\n                          construct the builder in a "".builder(key)"" call.\n           builder_class - The subclass of CommandConfigBuilder that builds\n                           the CommandConfig for this plugin.\n        """"""\n        if command_type in self.command_config_builders:\n            raise PluginError(\n                \'CommandConfigBuilder already registered for command\'\n                \'with type {}\'.format(command_type))\n        self.command_config_builders[command_type] = builder_class\n        self.commands.append(command_type)\n\n    def register_aux_command(self, command_type, command_class):\n        """"""Registers a custom AuxCommand as a plugin.\n\n        Args:\n           command_type - The key used for this plugin. This will be used to\n                          construct the builder in a "".builder(key)"" call.\n           command_class - The subclass of AuxCommand subclass to register.\n        """"""\n        if command_type in self.command_config_builders:\n            raise PluginError(\n                \'CommandConfigBuilder is already registered for command\'\n                \'with type {}\'.format(command_type))\n        if command_type in self.aux_command_classes:\n            raise PluginError(\'AuxCommand is already registered for command\'\n                              \'with type {}\'.format(command_type))\n        self.aux_command_classes[command_type] = command_class\n        if command_class.options.include_by_default:\n            self.commands.append(command_type)\n\n    def register_default_raster_source(self, provider_class):\n        """"""Registers a RasterSourceDefaultProvider for use as a plugin.""""""\n\n        self.default_raster_sources.append(provider_class)\n\n    def register_default_vector_source(self, provider_class):\n        """"""Registers a VectorSourceDefaultProvider for use as a plugin.""""""\n        self.default_vector_sources.append(provider_class)\n\n    def register_default_label_source(self, provider_class):\n        """"""Registers a LabelSourceDefaultProvider for use as a plugin.""""""\n        self.default_label_sources.append(provider_class)\n\n    def register_default_label_store(self, provider_class):\n        """"""Registers a LabelStoreDefaultProvider for use as a plugin.""""""\n        self.default_label_stores.append(provider_class)\n\n    def register_default_evaluator(self, provider_class):\n        """"""Registers an EvaluatorDefaultProvider for use as a plugin.""""""\n        self.default_evaluators.append(provider_class)\n\n    def register_experiment_runner(self, runner_key, runner_class):\n        """"""Registers an ExperimentRunner as a plugin.\n\n        Args:\n           runner_key - The key used to reference this plugin runner.\n                        This is a string that will match the command line\n                        argument used to reference this runner; e.g. if the\n                        key is ""FOO_RUNNER"", then users can use the runner\n                        by issuing a ""rastervision run  foo_runner ..."" command.\n           runner_class - The class of the ExperimentRunner plugin.\n        """"""\n        if runner_key in self.experiment_runners:\n            raise PluginError(\'ExperimentRunner already registered for \'\n                              \'key {}\'.format(runner_key))\n        self.experiment_runners[runner_key] = runner_class\n\n    def register_filesystem(self, filesystem_class):\n        """"""Registers a FileSystem as a plugin.""""""\n        self.filesystems.append(filesystem_class)\n'"
rastervision/predictor.py,0,"b'import os\nimport zipfile\n\nimport rastervision as rv\nfrom rastervision.utils.files import (download_if_needed, make_dir,\n                                      load_json_config, save_json_config)\nfrom rastervision.protos.command_pb2 import CommandConfig as CommandConfigMsg\nfrom rastervision.data.raster_source import ChannelOrderError\n\n\nclass Predictor():\n    """"""Class for making predictions based off of a prediction package.""""""\n\n    def __init__(self,\n                 prediction_package_uri,\n                 tmp_dir,\n                 update_stats=False,\n                 channel_order=None):\n        """"""Creates a new Predictor.\n\n        Args:\n            prediction_package_uri: The URI of the prediction package to use.\n                Can be any type of URI that Raster Vision can read.\n            tmp_dir: Temporary directory in which to store files that are used\n                by the Predictor. This directory is not cleaned up by this\n                class.\n            update_stats: Option indicating if any Analyzers should be run on\n                the image to be predicted on. Otherwise, the Predictor will use\n                the output of Analyzers that are bundled with the predict\n                package. This is useful, for instance, if you are predicting\n                against imagery that needs to be normalized with a\n                StatsAnalyzer, and the color profile of the new imagery is\n                significantly different then the imagery the model was trained\n                on.\n            channel_order: Option for a new channel order to use for the\n                imagery being predicted against. If not present, the\n                channel_order from the original configuration in the predict\n                package will be used.\n        """"""\n        self.tmp_dir = tmp_dir\n        self.update_stats = update_stats\n        self.model_loaded = False\n\n        package_zip_path = download_if_needed(prediction_package_uri, tmp_dir)\n        package_dir = os.path.join(tmp_dir, \'package\')\n        make_dir(package_dir)\n        with zipfile.ZipFile(package_zip_path, \'r\') as package_zip:\n            package_zip.extractall(path=package_dir)\n\n        # Read bundle command config\n        bundle_config_path = os.path.join(package_dir, \'bundle_config.json\')\n        msg = load_json_config(bundle_config_path, CommandConfigMsg())\n        bundle_config = msg.bundle_config\n\n        self.task_config = rv.TaskConfig.from_proto(bundle_config.task) \\\n                                        .load_bundle_files(package_dir)\n\n        self.backend_config = rv.BackendConfig.from_proto(bundle_config.backend) \\\n                                              .load_bundle_files(package_dir)\n\n        scene_config = rv.SceneConfig.from_proto(bundle_config.scene)\n        scene_builder = scene_config.load_bundle_files(package_dir) \\\n                                    .to_builder() \\\n                                    .clear_label_source() \\\n                                    .clear_aois() \\\n                                    .with_id(\'PREDICTOR\')\n\n        # If the scene does not have a label store, generate a default one.\n        if not scene_config.label_store:\n            scene_builder = scene_builder.with_task(self.task_config) \\\n                                         .with_label_store()\n\n        if channel_order:\n            raster_source = scene_builder.config[\'raster_source\'] \\\n                                         .to_builder() \\\n                                         .with_channel_order(channel_order) \\\n                                         .build()\n            scene_builder = scene_builder.with_raster_source(raster_source)\n\n        self.scene_config = scene_builder.build()\n\n        self.analyzer_configs = []\n        if update_stats:\n            for analyzer in bundle_config.analyzers:\n                a = rv.AnalyzerConfig.from_proto(analyzer) \\\n                                     .load_bundle_files(package_dir)\n                self.analyzer_configs.append(a)\n\n        self.bundle_config = rv.command.CommandConfig \\\n                                       .from_proto(msg) \\\n                                       .to_builder() \\\n                                       .with_task(self.task_config) \\\n                                       .with_backend(self.backend_config) \\\n                                       .with_scene(self.scene_config) \\\n                                       .with_analyzers(self.analyzer_configs) \\\n                                       .build()\n\n    def load_model(self):\n        """"""Load the model for this Predictor.\n\n        This is useful if you are going to make multiple predictions with the\n        model, and want it to be fast on the first prediction.\n\n        Note: This is called implicitly on the first call of \'predict\' if it\n        hasn\'t been called already.\n        """"""\n        self.backend = self.backend_config.create_backend(self.task_config)\n        self.backend.load_model(self.tmp_dir)\n        self.task = self.task_config.create_task(self.backend)\n        self.analyzers = []\n        for analyzer_config in self.analyzer_configs:\n            self.analyzers.append(analyzer_config.create_analyzer())\n        self.model_loaded = True\n\n    def predict(self, image_uri, label_uri=None, config_uri=None):\n        """"""Generate predictions for the given image.\n\n        Args:\n            image_uri: URI of the image to make predictions against.\n                This can be any type of URI readable by Raster Vision\n                FileSystems.\n            label_uri: Optional URI to save labels off into.\n            config_uri: Optional URI in which to save the bundle_config,\n                which can be useful to client applications for understanding\n                how to interpret the labels.\n\n            Returns:\n                rastervision.data.labels.Labels containing the predicted labels.\n        """"""\n        if not self.model_loaded:\n            self.load_model()\n        scene_config = self.scene_config.for_prediction(image_uri, label_uri) \\\n                                        .create_local(self.tmp_dir)\n\n        try:\n            scene = scene_config.create_scene(self.task_config, self.tmp_dir)\n            # If we are analyzing per scene, run analyzers\n            # Analyzers should overwrite files in the tmp_dir\n            if self.update_stats:\n                for analyzer in self.analyzers:\n                    analyzer.process([scene], self.tmp_dir)\n\n                # Reload scene to refresh any new analyzer config\n                scene = scene_config.create_scene(self.task_config,\n                                                  self.tmp_dir)\n        except ChannelOrderError:\n            raise ValueError(\n                \'The predict package is using a channel_order \'\n                \'with channels unavailable in the imagery.\\nTo set a new \'\n                \'channel_order that only uses channels available in the \'\n                \'imagery, use the --channel-order option.\')\n\n        with scene.activate():\n            labels = self.task.predict_scene(scene, self.tmp_dir)\n            if label_uri:\n                scene.prediction_label_store.save(labels)\n\n        if config_uri:\n            msg = self.bundle_config.to_builder() \\\n                                    .with_scene(scene_config) \\\n                                    .build() \\\n                                    .to_proto()\n            save_json_config(msg, config_uri)\n\n        return labels\n'"
rastervision/registry.py,0,"b'import rastervision as rv\nimport rastervision.filesystem as rvfs\nfrom rastervision.cli import Verbosity\nfrom rastervision.rv_config import RVConfig\nfrom rastervision.plugin import PluginRegistry\nfrom rastervision.data.raster_source.default import (\n    RasterioSourceDefaultProvider)\nfrom rastervision.data.vector_source.default import (\n    GeoJSONVectorSourceDefaultProvider, VectorTileVectorSourceDefaultProvider)\nfrom rastervision.data.label_source.default import (\n    ObjectDetectionLabelSourceDefaultProvider,\n    ChipClassificationLabelSourceDefaultProvider,\n    SemanticSegmentationLabelSourceDefaultProvider)\nfrom rastervision.data.label_store.default import (\n    ObjectDetectionGeoJSONStoreDefaultProvider,\n    ChipClassificationGeoJSONStoreDefaultProvider,\n    SemanticSegmentationRasterStoreDefaultProvider)\nfrom rastervision.evaluation.default import (\n    ObjectDetectionEvaluatorDefaultProvider,\n    ChipClassificationEvaluatorDefaultProvider,\n    SemanticSegmentationEvaluatorDefaultProvider)\n\n\nclass RegistryError(Exception):\n    pass\n\n\nclass Registry:\n    """"""Singleton that holds instances of Raster Vision types,\n       to be referenced by configuration code by key.\n    """"""\n\n    def __init__(self):\n        self._rv_config = None\n        self._plugin_registry = None\n\n        self._internal_config_builders = {\n            # Tasks\n            (rv.TASK, rv.OBJECT_DETECTION):\n            rv.task.ObjectDetectionConfigBuilder,\n            (rv.TASK, rv.CHIP_CLASSIFICATION):\n            rv.task.ChipClassificationConfigBuilder,\n            (rv.TASK, rv.SEMANTIC_SEGMENTATION):\n            rv.task.SemanticSegmentationConfigBuilder,\n\n            # Raster Transformers\n            (rv.RASTER_TRANSFORMER, rv.STATS_TRANSFORMER):\n            rv.data.StatsTransformerConfigBuilder,\n\n            # Raster Sources\n            (rv.RASTER_SOURCE, rv.RASTERIZED_SOURCE):\n            rv.data.RasterizedSourceConfigBuilder,\n            (rv.RASTER_SOURCE, rv.RASTERIO_SOURCE):\n            rv.data.RasterioSourceConfigBuilder,\n\n            # Alias provided for backwards compatibility.\n            (rv.RASTER_SOURCE, rv.GEOJSON_SOURCE):\n            rv.data.RasterizedSourceConfigBuilder,\n            (rv.RASTER_SOURCE, rv.GEOTIFF_SOURCE):\n            rv.data.RasterioSourceConfigBuilder,\n            (rv.RASTER_SOURCE, rv.IMAGE_SOURCE):\n            rv.data.RasterioSourceConfigBuilder,\n\n            # Vector Sources\n            (rv.VECTOR_SOURCE, rv.VECTOR_TILE_SOURCE):\n            rv.data.VectorTileVectorSourceConfigBuilder,\n            (rv.VECTOR_SOURCE, rv.GEOJSON_SOURCE):\n            rv.data.GeoJSONVectorSourceConfigBuilder,\n\n            # Label Sources\n            (rv.LABEL_SOURCE, rv.CHIP_CLASSIFICATION):\n            rv.data.ChipClassificationLabelSourceConfigBuilder,\n            (rv.LABEL_SOURCE, rv.OBJECT_DETECTION):\n            rv.data.ObjectDetectionLabelSourceConfigBuilder,\n            (rv.LABEL_SOURCE, rv.SEMANTIC_SEGMENTATION):\n            rv.data.SemanticSegmentationLabelSourceConfigBuilder,\n\n            # Label Source aliases provided for backward-compatibility\n            (rv.LABEL_SOURCE, rv.OBJECT_DETECTION_GEOJSON):\n            rv.data.ObjectDetectionLabelSourceConfigBuilder,\n            (rv.LABEL_SOURCE, rv.CHIP_CLASSIFICATION_GEOJSON):\n            rv.data.ChipClassificationLabelSourceConfigBuilder,\n            (rv.LABEL_SOURCE, rv.SEMANTIC_SEGMENTATION_RASTER):\n            rv.data.SemanticSegmentationLabelSourceConfigBuilder,\n\n            # Label Stores\n            (rv.LABEL_STORE, rv.OBJECT_DETECTION_GEOJSON):\n            rv.data.ObjectDetectionGeoJSONStoreConfigBuilder,\n            (rv.LABEL_STORE, rv.CHIP_CLASSIFICATION_GEOJSON):\n            rv.data.ChipClassificationGeoJSONStoreConfigBuilder,\n            (rv.LABEL_STORE, rv.SEMANTIC_SEGMENTATION_RASTER):\n            rv.data.SemanticSegmentationRasterStoreConfigBuilder,\n\n            # Analyzers\n            (rv.ANALYZER, rv.STATS_ANALYZER):\n            rv.analyzer.StatsAnalyzerConfigBuilder,\n\n            # Augmentors\n            (rv.AUGMENTOR, rv.NODATA_AUGMENTOR):\n            rv.augmentor.NodataAugmentorConfigBuilder,\n\n            # Evaluators\n            (rv.EVALUATOR, rv.CHIP_CLASSIFICATION_EVALUATOR):\n            rv.evaluation.ChipClassificationEvaluatorConfigBuilder,\n            (rv.EVALUATOR, rv.OBJECT_DETECTION_EVALUATOR):\n            rv.evaluation.ObjectDetectionEvaluatorConfigBuilder,\n            (rv.EVALUATOR, rv.SEMANTIC_SEGMENTATION_EVALUATOR):\n            rv.evaluation.SemanticSegmentationEvaluatorConfigBuilder,\n        }\n\n        if rv.backend.tf_available:\n            self._internal_config_builders[(rv.BACKEND, rv.TF_OBJECT_DETECTION)] = \\\n                rv.backend.TFObjectDetectionConfigBuilder\n            self._internal_config_builders[(rv.BACKEND, rv.KERAS_CLASSIFICATION)] = \\\n                rv.backend.KerasClassificationConfigBuilder\n            self._internal_config_builders[(rv.BACKEND, rv.TF_DEEPLAB)] = \\\n                rv.backend.TFDeeplabConfigBuilder\n\n        if rv.backend.pytorch_available:\n            key_val = (rv.BACKEND, rv.PYTORCH_CHIP_CLASSIFICATION)\n            self._internal_config_builders[key_val] = \\\n                rv.backend.PyTorchChipClassificationConfigBuilder\n            key_val = (rv.BACKEND, rv.PYTORCH_SEMANTIC_SEGMENTATION)\n            self._internal_config_builders[key_val] = \\\n                rv.backend.PyTorchSemanticSegmentationConfigBuilder\n            key_val = (rv.BACKEND, rv.PYTORCH_OBJECT_DETECTION)\n            self._internal_config_builders[key_val] = \\\n                rv.backend.PyTorchObjectDetectionConfigBuilder\n\n        self._internal_default_raster_sources = [RasterioSourceDefaultProvider]\n\n        self._internal_default_vector_sources = [\n            GeoJSONVectorSourceDefaultProvider,\n            VectorTileVectorSourceDefaultProvider\n        ]\n\n        self._internal_default_label_sources = [\n            ObjectDetectionLabelSourceDefaultProvider,\n            ChipClassificationLabelSourceDefaultProvider,\n            SemanticSegmentationLabelSourceDefaultProvider\n        ]\n\n        self._internal_default_label_stores = [\n            ObjectDetectionGeoJSONStoreDefaultProvider,\n            ChipClassificationGeoJSONStoreDefaultProvider,\n            SemanticSegmentationRasterStoreDefaultProvider\n        ]\n\n        self._internal_default_evaluators = [\n            ObjectDetectionEvaluatorDefaultProvider,\n            ChipClassificationEvaluatorDefaultProvider,\n            SemanticSegmentationEvaluatorDefaultProvider\n        ]\n\n        self.command_config_builders = {\n            rv.ANALYZE: rv.command.AnalyzeCommandConfigBuilder,\n            rv.CHIP: rv.command.ChipCommandConfigBuilder,\n            rv.TRAIN: rv.command.TrainCommandConfigBuilder,\n            rv.PREDICT: rv.command.PredictCommandConfigBuilder,\n            rv.EVAL: rv.command.EvalCommandConfigBuilder,\n            rv.BUNDLE: rv.command.BundleCommandConfigBuilder\n        }\n\n        self.aux_command_classes = {rv.COGIFY: rv.command.aux.CogifyCommand}\n\n        self.commands = [\n            rv.ANALYZE, rv.CHIP, rv.TRAIN, rv.PREDICT, rv.EVAL, rv.BUNDLE\n        ]\n\n        self.experiment_runners = {\n            rv.INPROCESS: rv.runner.InProcessExperimentRunner,\n            rv.AWS_BATCH: rv.runner.AwsBatchExperimentRunner,\n            rv.LOCAL: rv.runner.LocalExperimentRunner\n        }\n\n        self.filesystems = [\n            rvfs.HttpFileSystem,\n            rvfs.S3FileSystem,\n            # This is the catch-all case, ensure it\'s on the bottom of the search stack.\n            rvfs.LocalFileSystem\n        ]\n\n    def initialize_config(self,\n                          profile=None,\n                          rv_home=None,\n                          config_overrides=None,\n                          verbosity=Verbosity.NORMAL):\n        self._rv_config = RVConfig(\n            profile=profile,\n            rv_home=rv_home,\n            config_overrides=config_overrides,\n            verbosity=verbosity)\n        # Reset the plugins in case this is a re-initialization,\n        self._plugin_registry = None\n\n    def _get_rv_config(self):\n        """"""Returns the application configuration""""""\n        if self._rv_config is None:\n            self.initialize_config()\n        return self._rv_config\n\n    def _ensure_plugins_loaded(self):\n        if not self._plugin_registry:\n            self._load_plugins()\n\n    def _get_plugin_registry(self):\n        self._ensure_plugins_loaded()\n        return self._plugin_registry\n\n    def _load_plugins(self):\n        rv_config = self._get_rv_config()\n        plugin_config = rv_config.get_subconfig(\'PLUGINS\')\n        self._plugin_registry = PluginRegistry(plugin_config,\n                                               rv_config.rv_home)\n\n    def get_config_builder(self, group, key):\n        internal_builder = self._internal_config_builders.get((group, key))\n        if internal_builder:\n            return internal_builder\n        else:\n            self._ensure_plugins_loaded()\n            plugin_builder = self._plugin_registry.config_builders.get((group,\n                                                                        key))\n            if plugin_builder:\n                return plugin_builder\n\n        if group == rv.BACKEND:\n            if key in [\n                    rv.TF_OBJECT_DETECTION, rv.TF_DEEPLAB,\n                    rv.KERAS_CLASSIFICATION\n            ]:\n                raise RegistryError(\n                    \'Backend type {} not available. Perhaps you forgot to switch to \'\n                    \'the TF Docker image.\'.format(key))\n            if key in [\n                    rv.PYTORCH_CHIP_CLASSIFICATION,\n                    rv.PYTORCH_SEMANTIC_SEGMENTATION,\n                    rv.PYTORCH_OBJECT_DETECTION\n            ]:\n                raise RegistryError(\n                    \'Backend type {} not available. Perhaps you forgot to switch to \'\n                    \'the PyTorch Docker image.\'.format(key))\n\n        raise RegistryError(\'Unknown type {} for {} \'.format(key, group))\n\n    def get_file_system(self, uri: str, mode: str = \'r\',\n                        search_plugins=True) -> rvfs.FileSystem:\n        # If we are currently loading plugins, don\'t search for\n        # plugin filesystems.\n        if search_plugins:\n            self._ensure_plugins_loaded()\n            filesystems = (\n                self._plugin_registry.filesystems + self.filesystems)\n        else:\n            filesystems = self.filesystems\n\n        for fs in filesystems:\n            if fs.matches_uri(uri, mode):\n                return fs\n        if mode == \'w\':\n            raise RegistryError(\'No matching filesystem to handle \'\n                                \'writing to uri {}\'.format(uri))\n        else:\n            raise RegistryError(\'No matching filesystem to handle \'\n                                \'reading from uri {}\'.format(uri))\n\n    def get_raster_source_default_provider(self, s):\n        """"""\n        Gets the RasterSourceDefaultProvider for a given input string.\n        """"""\n        self._ensure_plugins_loaded()\n        providers = (self._plugin_registry.default_raster_sources +\n                     self._internal_default_raster_sources)\n\n        for provider in providers:\n            if provider.handles(s):\n                return provider\n\n        raise RegistryError(\n            \'No RasterSourceDefaultProvider found for {}\'.format(s))\n\n    def get_vector_source_default_provider(self, s):\n        """"""\n        Gets the VectorSourceDefaultProvider for a given input string.\n        """"""\n        self._ensure_plugins_loaded()\n        providers = (self._plugin_registry.default_vector_sources +\n                     self._internal_default_vector_sources)\n\n        for provider in providers:\n            if provider.handles(s):\n                return provider\n\n        raise RegistryError(\n            \'No VectorSourceDefaultProvider found for {}\'.format(s))\n\n    def get_label_source_default_provider(self, task_type, s):\n        """"""\n        Gets the RasterSourceDefaultProvider for a given input string.\n        """"""\n        self._ensure_plugins_loaded()\n        providers = (self._plugin_registry.default_label_sources +\n                     self._internal_default_label_sources)\n\n        for provider in providers:\n            if provider.handles(task_type, s):\n                return provider\n\n        raise RegistryError(\'No LabelSourceDefaultProvider \'\n                            \'found for {} and task type {}\'.format(\n                                s, task_type))\n\n    def get_label_store_default_provider(self, task_type, s=None):\n        """"""\n        Gets the RasterSourceDefaultProvider for a given input string.\n        """"""\n\n        self._ensure_plugins_loaded()\n        providers = (self._plugin_registry.default_label_stores +\n                     self._internal_default_label_stores)\n\n        for provider in providers:\n            if s:\n                if provider.handles(task_type, s):\n                    return provider\n            else:\n                if provider.is_default_for(task_type):\n                    return provider\n\n        if s:\n            raise RegistryError(\'No LabelStoreDefaultProvider \'\n                                \'found for {} and task type {}\'.format(\n                                    s, task_type))\n        else:\n            raise RegistryError(\'No LabelStoreDefaultProvider \'\n                                \'found for task type {}\'.format(task_type))\n\n    def get_evaluator_default_provider(self, task_type):\n        """"""\n        Gets the EvaluatorDefaultProvider for a given task\n        """"""\n\n        self._ensure_plugins_loaded()\n        providers = (self._plugin_registry.default_evaluators +\n                     self._internal_default_evaluators)\n\n        for provider in providers:\n            if provider.is_default_for(task_type):\n                return provider\n\n        raise RegistryError(\'No EvaluatorDefaultProvider \'\n                            \'found for task type {}\'.format(task_type))\n\n    def _get_aux_command_class(self, command_type):\n        aux_command_class = self.aux_command_classes.get(command_type)\n\n        if aux_command_class is None:\n            self._ensure_plugins_loaded()\n            aux_command_class = self._plugin_registry.aux_command_classes.get(\n                command_type)\n\n        return aux_command_class\n\n    def get_command_config_builder(self, command_type):\n        builder = self.command_config_builders.get(command_type)\n        if builder is None:\n            self._ensure_plugins_loaded()\n            builder = self._plugin_registry.command_config_builders.get(\n                command_type)\n\n        if builder:\n\n            def create_builder(*args, **kwargs):\n                return builder(command_type, *args, **kwargs)\n\n            return create_builder\n\n        # Aux commands\n\n        aux_command_class = self._get_aux_command_class(command_type)\n        if aux_command_class:\n            from rastervision.command.aux_command_config import AuxCommandConfigBuilder\n\n            def create_builder(*args, **kwargs):\n                return AuxCommandConfigBuilder(command_type, *args, **kwargs) \\\n                    .with_command_class(aux_command_class)\n\n            return create_builder\n\n        raise RegistryError(\n            \'No command found for type {}\'.format(command_type))\n\n    def get_aux_command_class(self, command_type):\n        aux_command_class = self._get_aux_command_class(command_type)\n\n        if aux_command_class:\n            return aux_command_class\n        else:\n            raise RegistryError(\n                \'No command class found for type {}\'.format(command_type))\n\n    def get_commands(self):\n        self._ensure_plugins_loaded()\n\n        aux_commands_to_include = [\n            command_type\n            for command_type, cls in self.aux_command_classes.items()\n            if cls.options.include_by_default\n        ] + [\n            command_type for command_type, cls in\n            self._plugin_registry.aux_command_classes.items()\n            if cls.options.include_by_default\n        ]\n\n        return self.commands + self._plugin_registry.commands + \\\n            aux_commands_to_include\n\n    def get_experiment_runner(self, runner_type):\n        internal_runner = self.experiment_runners.get(runner_type)\n        if internal_runner:\n            return internal_runner()\n        else:\n            self._ensure_plugins_loaded()\n            plugin_runner = self._plugin_registry.experiment_runners.get(\n                runner_type)\n            if plugin_runner:\n                return plugin_runner()\n        raise RegistryError(\n            \'No experiment runner for type {}\'.format(runner_type))\n\n    def get_experiment_runner_keys(self):\n        self._ensure_plugins_loaded()\n        return (list(self.experiment_runners.keys()) + list(\n            self._plugin_registry.experiment_runners.keys()))\n'"
rastervision/rv_config.py,0,"b'import os\nimport json\nimport tempfile\nfrom pathlib import Path\nimport logging\n\nfrom everett.manager import (ConfigManager, ConfigDictEnv, ConfigEnvFileEnv,\n                             ConfigIniEnv, ConfigOSEnv)\n\nimport rastervision as rv\nfrom rastervision.cli import Verbosity\nfrom rastervision.filesystem.local_filesystem import make_dir\nfrom rastervision.utils.files import file_to_str\n\nlog = logging.getLogger(__name__)\n\n\nclass RVConfig:\n    DEFAULT_PROFILE = \'default\'\n\n    tmp_dir = None\n\n    @staticmethod\n    def get_tmp_dir():\n        if RVConfig.tmp_dir is None:\n            RVConfig.set_tmp_dir()\n        return tempfile.TemporaryDirectory(dir=RVConfig.tmp_dir)\n\n    @staticmethod\n    def get_tmp_dir_root():\n        if RVConfig.tmp_dir is None:\n            RVConfig.set_tmp_dir()\n        return RVConfig.tmp_dir\n\n    @staticmethod\n    def set_tmp_dir(tmp_dir=None):\n        """"""Set RVConfig.tmp_dir to well-known value.\n\n        This static method sets the value of RVConfig.tmp_dir to some\n        well-known value.  The value is chosen from one of the\n        following (in order of preference): an explicit value\n        (presumably from the command line) is considered first, then\n        values from the environment are considered, then the current\n        value of RVConfig.tmp_dir is considered, then a directory from\n        tempfile.TemporaryDirectory() is considered.\n\n        Args:\n            tmp_dir: Either a string or None.\n\n        """"""\n        DEFAULT_DIR = \'/opt/data/tmp/\'\n\n        # Check the various possibilities in order of priority.\n        tmp_dir_array = [tmp_dir]\n        env_array = [\n            os.environ.get(k) for k in [\'TMPDIR\', \'TEMP\', \'TMP\']\n            if k in os.environ\n        ]\n        current_array = [RVConfig.tmp_dir]\n        it = tmp_dir_array + env_array + current_array\n        it = list(filter(lambda p: p is not None, it))\n        if it:\n            explicit_tmp_dir = it[0]\n        else:\n            explicit_tmp_dir = tempfile.TemporaryDirectory().name\n\n        try:\n            # Try to create directory\n            if not os.path.exists(explicit_tmp_dir):\n                os.makedirs(explicit_tmp_dir, exist_ok=True)\n            # Check that it is actually a directory\n            if not os.path.isdir(explicit_tmp_dir):\n                raise Exception(\n                    \'{} is not a directory.\'.format(explicit_tmp_dir))\n            # Can we interact with directory?\n            Path.touch(Path(os.path.join(explicit_tmp_dir, \'.can_touch\')))\n            # All checks have passed by this point\n            RVConfig.tmp_dir = explicit_tmp_dir\n\n        # If directory cannot be made and/or cannot be interacted\n        # with, fall back to default.\n        except Exception as e:\n            log.warning(\n                \'Root temporary directory cannot be used: {}. Using root: {}\'.\n                format(explicit_tmp_dir, DEFAULT_DIR))\n            RVConfig.tmp_dir = DEFAULT_DIR\n        finally:\n            make_dir(RVConfig.tmp_dir)\n            log.debug(\'Temporary directory is: {}\'.format(RVConfig.tmp_dir))\n\n    @staticmethod\n    def get_instance():\n        return rv._registry._get_rv_config()\n\n    def __init__(self,\n                 profile=None,\n                 rv_home=None,\n                 config_overrides=None,\n                 tmp_dir=None,\n                 verbosity=Verbosity.NORMAL):\n        self.verbosity = verbosity\n\n        # Set logging level\n        root_log = logging.getLogger(\'rastervision\')\n        if self.verbosity >= Verbosity.VERBOSE:\n            root_log.setLevel(logging.DEBUG)\n        elif self.verbosity >= Verbosity.NORMAL:\n            root_log.setLevel(logging.INFO)\n        else:\n            root_log.setLevel(logging.WARN)\n\n        if tmp_dir is not None:\n            self.set_tmp_dir(tmp_dir)\n\n        if profile is None:\n            if os.environ.get(\'RV_PROFILE\'):\n                profile = os.environ.get(\'RV_PROFILE\')\n            else:\n                profile = RVConfig.DEFAULT_PROFILE\n\n        if config_overrides is None:\n            config_overrides = {}\n\n        if rv_home is None:\n            home = os.path.expanduser(\'~\')\n            rv_home = os.path.join(home, \'.rastervision\')\n        self.rv_home = rv_home\n\n        config_file_locations = self._discover_config_file_locations(profile)\n\n        help_doc = (\'Check https://docs.rastervision.io/ for docs.\')\n        self.config = ConfigManager(\n            # Specify one or more configuration environments in\n            # the order they should be checked\n            [\n                # Allow overrides\n                ConfigDictEnv(config_overrides),\n\n                # Looks in OS environment first\n                ConfigOSEnv(),\n\n                # Look for an .env file\n                ConfigEnvFileEnv(\'.env\'),\n\n                # Looks in INI files in order specified\n                ConfigIniEnv(config_file_locations),\n            ],\n\n            # Make it easy for users to find your configuration docs\n            doc=help_doc)\n\n    def _discover_config_file_locations(self, profile):\n        result = []\n\n        # Allow for user to specify specific config file\n        # in the RASTERVISION_CONFIG env variable.\n        env_specified_path = os.environ.get(\'RV_CONFIG\')\n        if env_specified_path:\n            result.append(env_specified_path)\n\n        # Allow user to specify config directory that will\n        # contain profile configs in RASTERVISION_CONFIG_DIR\n        # env variable. Otherwise, use ""$HOME/.rastervision""\n        env_specified_dir_path = os.environ.get(\'RV_CONFIG_DIR\')\n        if env_specified_dir_path:\n            result.append(os.path.join(env_specified_dir_path, profile))\n        else:\n            result.append(os.path.join(self.rv_home, profile))\n        result.append(os.path.join(os.getcwd(), \'.rastervision\'))\n\n        # Filter out any that do not exist.\n        results_that_exist = list(filter(lambda x: os.path.exists(x), result))\n\n        # If the profile is not default, and there is no config that exists,\n        # then throw an error.\n        if not any(results_that_exist) and profile != RVConfig.DEFAULT_PROFILE:\n            raise rv.ConfigError(\'Configuration Profile {} not found. \'\n                                 \'Checked: {}\'.format(profile,\n                                                      \', \'.join(result)))\n\n        return results_that_exist\n\n    def get_subconfig(self, namespace):\n        return self.config.with_namespace(namespace)\n\n    def get_model_defaults(self):\n        """"""Return the ""model defaults""\n\n        The model defaults is a json file that lists a set of default\n        configurations for models, per backend and model key.\n        There are defaults that are installed with Raster Vision, but\n        users can override these defaults with their own by setting\n        the ""model_defaults_uri"" in the [RV] section of\n        thier configuration file, or by setting the RV_MODEL_DEFAULT_URI\n        environment variable.\n        """"""\n        subconfig = self.get_subconfig(\'RV\')\n        default_path = os.path.join(\n            os.path.dirname(rv.backend.__file__), \'model_defaults.json\')\n        model_default_uri = subconfig(\n            \'model_defaults_uri\', default=default_path)\n\n        model_defaults = json.loads(file_to_str(model_default_uri))\n\n        return model_defaults\n\n    def get_verbosity(self):\n        return self.verbosity\n'"
rastervision/version.py,0,"b'""""""Library verison""""""\n__version__ = \'0.10.0\'\n'"
tests/__init__.py,0,"b""import os\n\n\ndef data_file_path(rel_path):\n    data_dir = os.path.join(os.path.dirname(__file__), 'data-files')\n    return os.path.join(data_dir, rel_path)\n"""
tests/test_plugin.py,0,"b'import os\nimport unittest\n\nimport rastervision as rv\n\nfrom tests import data_file_path\n\n\nclass TestPlugin(unittest.TestCase):\n    def test_single_plugin_from_path(self):\n        config = {\n            \'PLUGINS_files\':\n            \'[""{}""]\'.format(\n                data_file_path(\'plugins/noop_raster_transformer.py\'))\n        }\n        rv._registry.initialize_config(config_overrides=config)\n\n        try:\n            transformer = rv.RasterTransformerConfig.builder(\'NOOP_TRANSFORMER\') \\\n                                                    .build() \\\n                                                    .create_transformer()\n\n            self.assertIsInstance(transformer,\n                                  rv.data.raster_transformer.RasterTransformer)\n        finally:\n            # Reset config\n            rv._registry.initialize_config()\n\n    def test_plugin_from_module(self):\n        config = {\'PLUGINS_modules\': \'[""{}""]\'.format(__name__)}\n        rv._registry.initialize_config(config_overrides=config)\n\n        try:\n            augmentor = rv.AnalyzerConfig.builder(\'NOOP_ANALYZER\') \\\n                                         .build() \\\n                                         .create_analyzer()\n\n            self.assertIsInstance(augmentor, rv.analyzer.Analyzer)\n        finally:\n            # Reset config\n            rv._registry.initialize_config()\n\n    def test_runs_noop_experiment_from_plugins(self):\n        # set the env var to have rv pick up this configuration\n        # which adds the tests.test_plugin and tests.test_plugin_2 modules\n        # as a plugin.\n        old_rv_config = os.environ.get(\'RV_CONFIG\')\n        os.environ[\'RV_CONFIG\'] = data_file_path(\'plugins/default\')\n\n        try:\n            plugin_files = [\n                data_file_path(\'plugins/noop_task.py\'),\n                data_file_path(\'plugins/noop_backend.py\'),\n                data_file_path(\'plugins/noop_raster_transformer.py\'),\n                data_file_path(\'plugins/noop_raster_source.py\'),\n                data_file_path(\'plugins/noop_label_source.py\'),\n                data_file_path(\'plugins/noop_label_store.py\'),\n                data_file_path(\'plugins/noop_evaluator.py\'),\n                data_file_path(\'plugins/noop_runner.py\'),\n                data_file_path(\'plugins/noop_command.py\')\n            ]\n\n            config = {\n                \'PLUGINS_files\': \'[""{}""]\'.format(\'"",""\'.join(plugin_files))\n            }\n            rv._registry.initialize_config(config_overrides=config)\n\n            # Check proto serialization\n\n            msg = rv.AnalyzerConfig.builder(\'NOOP_ANALYZER\') \\\n                                   .build() \\\n                                   .to_proto()\n            analyzer = rv.AnalyzerConfig.from_proto(msg)\n\n            msg = rv.AugmentorConfig.builder(\'NOOP_AUGMENTOR\') \\\n                                   .build() \\\n                                   .to_proto()\n            augmentor = rv.AugmentorConfig.from_proto(msg)\n\n            msg = rv.TaskConfig.builder(\'NOOP_TASK\') \\\n                               .build() \\\n                               .to_proto()\n            task = rv.TaskConfig.from_proto(msg)\n\n            msg = rv.BackendConfig.builder(\'NOOP_BACKEND\') \\\n                                  .build() \\\n                                  .to_proto()\n            backend = rv.BackendConfig.from_proto(msg)\n\n            msg = rv.RasterTransformerConfig.builder(\'NOOP_TRANSFORMER\') \\\n                                            .build() \\\n                                            .to_proto()\n            raster_transformer = rv.RasterTransformerConfig.from_proto(msg)\n\n            msg = rv.RasterSourceConfig.builder(\'NOOP_SOURCE\') \\\n                                       .with_transformer(raster_transformer) \\\n                                       .build() \\\n                                       .to_proto()\n            raster_source = rv.RasterSourceConfig.from_proto(msg)\n\n            msg = rv.LabelSourceConfig.builder(\'NOOP_SOURCE\') \\\n                                       .build() \\\n                                       .to_proto()\n            label_source = rv.LabelSourceConfig.from_proto(msg)\n\n            msg = rv.LabelStoreConfig.builder(\'NOOP_STORE\') \\\n                                     .build() \\\n                                     .to_proto()\n            label_store = rv.LabelStoreConfig.from_proto(msg)\n\n            msg = rv.EvaluatorConfig.builder(\'NOOP_EVALUATOR\') \\\n                                   .build() \\\n                                   .to_proto()\n            evaluator = rv.EvaluatorConfig.from_proto(msg)\n\n            train_scene = rv.SceneConfig.builder() \\\n                                        .with_id(\'train\') \\\n                                        .with_raster_source(raster_source) \\\n                                        .with_label_source(label_source) \\\n                                        .build()\n\n            val_scene = rv.SceneConfig.builder() \\\n                                      .with_id(\'val\') \\\n                                      .with_raster_source(raster_source) \\\n                                      .with_label_source(label_source) \\\n                                      .with_label_store(label_store) \\\n                                      .build()\n\n            dataset = rv.DatasetConfig.builder() \\\n                                      .with_train_scene(train_scene) \\\n                                      .with_validation_scene(val_scene) \\\n                                      .with_augmentor(augmentor) \\\n                                      .build()\n\n            e = rv.ExperimentConfig.builder() \\\n                                   .with_id(\'plugin_test\') \\\n                                   .with_task(task) \\\n                                   .with_backend(backend) \\\n                                   .with_dataset(dataset) \\\n                                   .with_analyzer(analyzer) \\\n                                   .with_evaluator(evaluator) \\\n                                   .with_root_uri(\'/no/matter\') \\\n                                   .with_custom_config({\'noop_key\': \'noop\'}) \\\n                                   .build()\n\n            rv.ExperimentRunner.get_runner(\'NOOP_RUNNER\').run(\n                e, rerun_commands=True)\n        finally:\n            # Reset environment var\n            if old_rv_config:\n                os.environ[\'RV_CONFIG\'] = old_rv_config\n            else:\n                del os.environ[\'RV_CONFIG\']\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n\nfrom rastervision.analyzer import (Analyzer, AnalyzerConfig,\n                                   AnalyzerConfigBuilder)  # noqa\nfrom rastervision.protos.analyzer_pb2 \\\n    import AnalyzerConfig as AnalyzerConfigMsg # noqa\n\nNOOP_ANALYZER = \'NOOP_ANALYZER\'\n\n\nclass NoopAnalyzer(Analyzer):\n    def process(self, training_data, tmp_dir):\n        pass\n\n\nclass NoopAnalyzerConfig(AnalyzerConfig):\n    def __init__(self):\n        super().__init__(NOOP_ANALYZER)\n\n    def to_proto(self):\n        return AnalyzerConfigMsg(analyzer_type=self.analyzer_type)\n\n    def create_analyzer(self):\n        return NoopAnalyzer()\n\n    def report_io(self, command_type, io_def):\n        pass\n\n    def save_bundle_files(self, bundle_dir):\n        return (self, [])\n\n    def load_bundle_files(self, bundle_dir):\n        return self\n\n\nclass NoopAnalyzerConfigBuilder(AnalyzerConfigBuilder):\n    def __init__(self, prev=None):\n        super().__init__(NoopAnalyzerConfig, {})\n\n    def from_proto(self, msg):\n        return NoopAnalyzerConfigBuilder()\n\n\ndef register_plugin(plugin_registry):\n    plugin_registry.register_config_builder(rv.ANALYZER, NOOP_ANALYZER,\n                                            NoopAnalyzerConfigBuilder)\n'"
tests/test_plugin_2.py,0,"b""import rastervision as rv\nfrom rastervision.augmentor import (Augmentor, AugmentorConfig,\n                                    AugmentorConfigBuilder)\nfrom rastervision.protos.augmentor_pb2 import AugmentorConfig as AugmentorConfigMsg\n\nNOOP_AUGMENTOR = 'NOOP_AUGMENTOR'\n\n\nclass NoopAugmentor(Augmentor):\n    def process(self, training_data, tmp_dir):\n        return training_data\n\n\nclass NoopAugmentorConfig(AugmentorConfig):\n    def __init__(self):\n        super().__init__(NOOP_AUGMENTOR)\n\n    def to_proto(self):\n        msg = AugmentorConfigMsg(\n            augmentor_type=self.augmentor_type, custom_config={})\n        return msg\n\n    def create_augmentor(self):\n        return NoopAugmentor()\n\n    def report_io(self, command_type, io_def):\n        pass\n\n\nclass NoopAugmentorConfigBuilder(AugmentorConfigBuilder):\n    def __init__(self, prev=None):\n        super().__init__(NoopAugmentorConfig, {})\n\n    def from_proto(self, msg):\n        return self\n\n\ndef register_plugin(plugin_registry):\n    plugin_registry.register_config_builder(rv.AUGMENTOR, NOOP_AUGMENTOR,\n                                            NoopAugmentorConfigBuilder)\n"""
tests/test_rv_config.py,0,"b""import os\nimport platform\nimport unittest\nimport shutil\n\nfrom rastervision.rv_config import RVConfig\n\n\nclass TestRVConfig(unittest.TestCase):\n    def setUp(self):\n        self.prev_tmp = RVConfig.tmp_dir\n\n    def tearDown(self):\n        RVConfig.tmp_dir = self.prev_tmp\n\n    def test_set_tmp_dir(self):\n        if platform.system() == 'Linux':\n            directory = '/tmp/xxx/'\n            while os.path.exists(directory):\n                directory = directory + 'xxx/'\n            self.assertFalse(os.path.exists(directory))\n            RVConfig.set_tmp_dir(directory)\n            self.assertTrue(os.path.exists(directory))\n            self.assertTrue(os.path.isdir(directory))\n            shutil.rmtree(directory)\n"""
tests_v2/__init__.py,0,"b""import os\n\n\ndef data_file_path(rel_path):\n    data_dir = os.path.join(os.path.dirname(__file__), 'data_files')\n    return os.path.join(data_dir, rel_path)\n"""
integration_tests/chip_classification_tests/experiment.py,0,"b""import os\n\nimport rastervision as rv\nfrom integration_tests.util.misc import str_to_bool\n\n\nclass ChipClassificationIntegrationTest(rv.ExperimentSet):\n    def exp_main(self, root_uri, data_uri=None, full_train=False,\n                 use_tf=False):\n        full_train = str_to_bool(full_train)\n        use_tf = str_to_bool(use_tf)\n\n        def get_path(part):\n            if full_train:\n                return os.path.join(data_uri, part)\n            else:\n                return os.path.join(os.path.dirname(__file__), part)\n\n        img_path = get_path('scene/image.tif')\n        label_path = get_path('scene/labels.json')\n\n        img2_path = get_path('scene/image2.tif')\n        label2_path = get_path('scene/labels2.json')\n\n        backend_conf_path = get_path('configs/backend.config')\n\n        task = rv.TaskConfig.builder(rv.CHIP_CLASSIFICATION) \\\n                            .with_chip_size(200) \\\n                            .with_classes({\n                                'car': (1, 'red'),\n                                'building': (2, 'blue'),\n                                'background': (3, 'black')\n                            }) \\\n                            .with_debug(True) \\\n                            .build()\n\n        if use_tf:\n            pretrained_model = (\n                'https://github.com/azavea/raster-vision-data/'\n                'releases/download/v0.0.7/chip-classification-test-weights.hdf5'\n            )\n\n            backend = rv.BackendConfig.builder(rv.KERAS_CLASSIFICATION) \\\n                .with_task(task) \\\n                .with_debug(True) \\\n                .with_template(backend_conf_path) \\\n                .with_num_epochs(8) \\\n                .with_pretrained_model(pretrained_model) \\\n                .with_train_options(sync_interval=None,\n                                    do_monitoring=False,\n                                    replace_model=True) \\\n                .build()\n        else:\n            if full_train:\n                backend = rv.BackendConfig.builder(rv.PYTORCH_CHIP_CLASSIFICATION) \\\n                    .with_task(task) \\\n                    .with_train_options(\n                        batch_size=16,\n                        num_epochs=10,\n                        sync_interval=200) \\\n                    .build()\n            else:\n                pretrained_uri = (\n                    'https://github.com/azavea/raster-vision-data/releases/download/'\n                    'v0.9.0/pytorch_chip_classification_test.pth')\n                backend = rv.BackendConfig.builder(rv.PYTORCH_CHIP_CLASSIFICATION) \\\n                    .with_task(task) \\\n                    .with_train_options(\n                        batch_size=2,\n                        num_epochs=1,\n                        lr=1e-9) \\\n                    .with_pretrained_uri(pretrained_uri) \\\n                    .build()\n\n        def make_scene(i_path, l_path):\n            label_source = rv.LabelSourceConfig.builder(rv.CHIP_CLASSIFICATION) \\\n                                               .with_uri(l_path) \\\n                                               .with_ioa_thresh(0.5) \\\n                                               .with_use_intersection_over_cell(False) \\\n                                               .with_pick_min_class_id(True) \\\n                                               .with_background_class_id(3) \\\n                                               .with_infer_cells(True) \\\n                                               .build()\n\n            raster_source = rv.RasterSourceConfig.builder(rv.GEOTIFF_SOURCE) \\\n                                                 .with_uri(i_path) \\\n                                                 .with_channel_order([0, 1, 2]) \\\n                                                 .with_stats_transformer() \\\n                                                 .build()\n\n            return rv.SceneConfig.builder() \\\n                                 .with_task(task) \\\n                                 .with_id(os.path.basename(i_path)) \\\n                                 .with_raster_source(raster_source) \\\n                                 .with_label_source(label_source) \\\n                                 .build()\n\n        scene_1 = make_scene(img_path, label_path)\n        scene_2 = make_scene(img2_path, label2_path)\n\n        dataset = rv.DatasetConfig.builder() \\\n                                  .with_train_scenes([scene_1, scene_2]) \\\n                                  .with_validation_scenes([scene_1, scene_2]) \\\n                                  .build()\n\n        experiment = rv.ExperimentConfig.builder() \\\n                                        .with_id('chip-classification-test') \\\n                                        .with_root_uri(root_uri) \\\n                                        .with_task(task) \\\n                                        .with_backend(backend) \\\n                                        .with_dataset(dataset) \\\n                                        .with_stats_analyzer() \\\n                                        .with_eval_key('default') \\\n                                        .build()\n\n        return experiment\n\n\nif __name__ == '__main__':\n    rv.main()\n"""
integration_tests/object_detection_tests/__init__.py,0,b''
integration_tests/object_detection_tests/experiment.py,0,"b""import os\n\nimport rastervision as rv\nfrom integration_tests.util.misc import str_to_bool\n\n\nclass ObjectDetectionIntegrationTest(rv.ExperimentSet):\n    def exp_main(self, root_uri, data_uri=None, full_train=False,\n                 use_tf=False):\n        full_train = str_to_bool(full_train)\n        use_tf = str_to_bool(use_tf)\n\n        def get_path(part):\n            if full_train:\n                return os.path.join(data_uri, part)\n            else:\n                return os.path.join(os.path.dirname(__file__), part)\n\n        img_path = get_path('scene/image.tif')\n        label_path = get_path('scene/labels.json')\n        img2_path = get_path('scene/image2.tif')\n        label2_path = get_path('scene/labels2.json')\n        backend_conf_path = get_path('configs/backend.config')\n\n        task = rv.TaskConfig.builder(rv.OBJECT_DETECTION) \\\n                            .with_chip_size(300) \\\n                            .with_classes({\n                                'car': (1, 'blue'),\n                                'building': (2, 'red')\n                            }) \\\n                            .with_chip_options(neg_ratio=1.0,\n                                               ioa_thresh=1.0,\n                                               window_method='sliding') \\\n                            .with_predict_options(merge_thresh=0.1,\n                                                  score_thresh=0.5) \\\n                            .build()\n\n        if use_tf:\n            pretrained_model = (\n                'https://github.com/azavea/raster-vision-data/'\n                'releases/download/v0.0.7/object-detection-test.tar.gz')\n\n            backend = rv.BackendConfig.builder(rv.TF_OBJECT_DETECTION) \\\n                                    .with_task(task) \\\n                                    .with_num_steps(200) \\\n                                    .with_template(backend_conf_path) \\\n                                    .with_pretrained_model(pretrained_model) \\\n                                    .with_train_options(sync_interval=None,\n                                                        do_monitoring=False,\n                                                        replace_model=True) \\\n                                    .with_debug(True) \\\n                                    .build()\n        else:\n            if full_train:\n                backend = rv.BackendConfig.builder(rv.PYTORCH_OBJECT_DETECTION) \\\n                    .with_task(task) \\\n                    .with_train_options(\n                        batch_size=8,\n                        num_epochs=200,\n                        sync_interval=200) \\\n                    .build()\n            else:\n                pretrained_uri = (\n                    'https://github.com/azavea/raster-vision-data/releases/download/'\n                    'v0.9.0/pytorch_object_detection_test.pth')\n\n                backend = rv.BackendConfig.builder(rv.PYTORCH_OBJECT_DETECTION) \\\n                    .with_task(task) \\\n                    .with_train_options(\n                        batch_size=2,\n                        num_epochs=1,\n                        lr=1e-9) \\\n                    .with_pretrained_uri(pretrained_uri) \\\n                    .build()\n\n        scene = rv.SceneConfig.builder() \\\n                              .with_task(task) \\\n                              .with_id('od_test') \\\n                              .with_raster_source(img_path, channel_order=[0, 1, 2]) \\\n                              .with_label_source(label_path) \\\n                              .build()\n\n        scene2 = rv.SceneConfig.builder() \\\n                               .with_task(task) \\\n                               .with_id('od_test-2') \\\n                               .with_raster_source(img2_path, channel_order=[0, 1, 2]) \\\n                               .with_label_source(label2_path) \\\n                               .build()\n\n        dataset = rv.DatasetConfig.builder() \\\n                                  .with_train_scenes([scene, scene2]) \\\n                                  .with_validation_scenes([scene, scene2]) \\\n                                  .build()\n\n        experiment = rv.ExperimentConfig.builder() \\\n                                        .with_id('object-detection-test') \\\n                                        .with_root_uri(root_uri) \\\n                                        .with_task(task) \\\n                                        .with_backend(backend) \\\n                                        .with_dataset(dataset) \\\n                                        .with_eval_key('default') \\\n                                        .build()\n\n        return experiment\n\n\nif __name__ == '__main__':\n    rv.main()\n"""
integration_tests/semantic_segmentation_tests/__init__.py,0,b''
integration_tests/semantic_segmentation_tests/experiment.py,0,"b""import os\n\nimport rastervision as rv\nfrom integration_tests.util.misc import str_to_bool\n\n\nclass SemanticSegmentationIntegrationTest(rv.ExperimentSet):\n    def exp_main(self, root_uri, data_uri=None, full_train=False,\n                 use_tf=False):\n        full_train = str_to_bool(full_train)\n        use_tf = str_to_bool(use_tf)\n\n        def get_path(part):\n            if full_train:\n                return os.path.join(data_uri, part)\n            else:\n                return os.path.join(os.path.dirname(__file__), part)\n\n        img_paths = [get_path('scene/image.tif'), get_path('scene/image2.tif')]\n        label_paths = [\n            get_path('scene/labels.tif'),\n            get_path('scene/labels2.tif')\n        ]\n        class_map = {'red': (1, 'red'), 'green': (2, 'green')}\n        num_steps = 1\n        batch_size = 1\n\n        # These are the parameters that were used to train the following pretrained\n        # model.\n        # num_steps = 5000\n        # batch_size = 8\n        # I found it was also possible to train for 2000 steps with the same eval,\n        # and even fewer steps may be possible. But because it takes\n        # 5 secs/step with batch size of 1 on a CPU, it doesn't seem feasible to actually\n        # train during CI. So instead we just use the model trained on the GPU and then\n        # fine-tune it for one step.\n\n        # This a divisor of the scene length.\n        chip_size = 300\n        task = rv.TaskConfig.builder(rv.SEMANTIC_SEGMENTATION) \\\n                            .with_chip_size(chip_size) \\\n                            .with_classes(class_map) \\\n                            .with_chip_options(window_method='sliding',\n                                               stride=chip_size,\n                                               debug_chip_probability=1.0) \\\n                            .build()\n\n        if use_tf:\n            pretrained_model = (\n                'https://github.com/azavea/raster-vision-data/releases/'\n                'download/0.0.6/deeplab-test-model.tar.gz')\n\n            # .with_config below needed to copy final layer from pretrained model.\n            backend = rv.BackendConfig.builder(rv.TF_DEEPLAB) \\\n                .with_task(task) \\\n                .with_model_defaults(rv.MOBILENET_V2) \\\n                .with_pretrained_model(pretrained_model) \\\n                .with_train_options(do_monitoring=True,\n                                    replace_model=True) \\\n                .with_num_steps(num_steps) \\\n                .with_batch_size(batch_size) \\\n                .with_debug(True) \\\n                .with_config({'initializeLastLayer': 'true'}) \\\n                .build()\n        else:\n            if full_train:\n                backend = rv.BackendConfig.builder(rv.PYTORCH_SEMANTIC_SEGMENTATION) \\\n                    .with_task(task) \\\n                    .with_train_options(\n                        batch_size=8,\n                        num_epochs=200,\n                        sync_interval=200) \\\n                    .build()\n            else:\n                pretrained_uri = (\n                    'https://github.com/azavea/raster-vision-data/releases/download/'\n                    'v0.9.0/pytorch_semantic_segmentation_test.pth')\n\n                backend = rv.BackendConfig.builder(rv.PYTORCH_SEMANTIC_SEGMENTATION) \\\n                    .with_task(task) \\\n                    .with_train_options(\n                        batch_size=2,\n                        num_epochs=1,\n                        lr=1e-9) \\\n                    .with_pretrained_uri(pretrained_uri) \\\n                    .build()\n\n        label_source = rv.LabelSourceConfig.builder(rv.SEMANTIC_SEGMENTATION_RASTER) \\\n                                           .with_rgb_class_map(task.class_map) \\\n                                           .with_raster_source(label_paths[0]) \\\n                                           .build()\n\n        label_source2 = rv.LabelSourceConfig.builder(rv.SEMANTIC_SEGMENTATION_RASTER) \\\n                                            .with_rgb_class_map(task.class_map) \\\n                                            .with_raster_source(label_paths[1]) \\\n                                            .build()\n\n        vector_output = [{\n            'mode': 'buildings',\n            'class_id': 1,\n            'building_options': {\n                'element_width_factor': 0.51\n            }\n        }, {\n            'denoise': 50,\n            'mode': 'polygons',\n            'class_id': 1\n        }]\n\n        label_store = rv.LabelStoreConfig.builder(rv.SEMANTIC_SEGMENTATION_RASTER) \\\n                                         .with_vector_output(vector_output) \\\n                                         .with_rgb(True) \\\n                                         .build()\n\n        label_store2 = rv.LabelStoreConfig.builder(rv.SEMANTIC_SEGMENTATION_RASTER) \\\n                                         .with_vector_output(vector_output) \\\n                                         .with_rgb(True) \\\n                                         .build()\n\n        scene = rv.SceneConfig.builder() \\\n                              .with_task(task) \\\n                              .with_id('test-scene') \\\n                              .with_raster_source(img_paths[0], channel_order=[0, 1, 2]) \\\n                              .with_label_source(label_source) \\\n                              .with_label_store(label_store) \\\n                              .build()\n\n        scene2 = rv.SceneConfig.builder() \\\n                              .with_task(task) \\\n                              .with_id('test-scene2') \\\n                              .with_raster_source(img_paths[1], channel_order=[0, 1, 2]) \\\n                              .with_label_source(label_source2) \\\n                              .with_label_store(label_store2) \\\n                              .build()\n\n        scenes = [scene, scene2]\n\n        dataset = rv.DatasetConfig.builder() \\\n                                  .with_train_scenes(scenes) \\\n                                  .with_validation_scenes(scenes) \\\n                                  .build()\n\n        experiment = rv.ExperimentConfig.builder() \\\n                                        .with_id('semantic-segmentation-test') \\\n                                        .with_root_uri(root_uri) \\\n                                        .with_task(task) \\\n                                        .with_backend(backend) \\\n                                        .with_dataset(dataset) \\\n                                        .with_eval_key('default') \\\n                                        .build()\n\n        return experiment\n\n\nif __name__ == '__main__':\n    rv.main()\n"""
integration_tests/util/__init__.py,0,b''
integration_tests/util/flip_scene.py,0,"b'import click\nimport numpy as np\nimport rasterio\nimport pyproj\nimport json\n\n\ndef flip_geom(m, b, geom):\n    """"""Flips a geom along a straight line y = mx + b.\n    """"""\n\n    def traverse_coords(coords, dst_coords):\n        for p in coords:\n            if type(p[0]) is list:\n                lst = []\n                traverse_coords(p, lst)\n                dst_coords.append(lst)\n            else:\n                x, y = p[0], p[1]\n                d = (x + (y - b) * m) / (1 + m * m)\n                x2 = 2 * d - x\n                y2 = 2 * d * m - y + 2 * b\n                dst_coords.append((x2, y2))\n        return dst_coords\n\n    return {\n        \'type\': geom[\'type\'],\n        \'coordinates\': traverse_coords(geom[\'coordinates\'], [])\n    }\n\n\n@click.command()\n@click.argument(\'src_tiff_path\')\n@click.argument(\'src_labels_path\')\n@click.argument(\'dst_tiff_path\')\n@click.argument(\'dst_labels_path\')\ndef flip_scene(src_tiff_path, src_labels_path, dst_tiff_path, dst_labels_path):\n    """"""Flips a scene and it\'s labels.\n\n    Useful for generating multiple training scenes for integration test usage.\n    """"""\n\n    labels_are_tif = src_labels_path.endswith(\'.tif\')\n\n    with rasterio.open(src_tiff_path) as src:\n        profile = src.profile\n        bands = src.read()\n\n        with rasterio.open(dst_tiff_path, \'w\', **profile) as dst:\n            fbands = np.flip(bands, 1)\n            dst.write(fbands)\n\n        if not labels_are_tif:\n\n            img_crs = pyproj.Proj(init=src.crs[\'init\'])\n            map_crs = pyproj.Proj(init=\'epsg:4326\')\n\n            def t(x, y):\n                return pyproj.transform(img_crs, map_crs, x, y)\n\n            # Find the center horizontal line through the image.\n\n            ll = (src.bounds.left, src.bounds.bottom)\n            ul = (src.bounds.left, src.bounds.top)\n            ur = (src.bounds.right, src.bounds.top)\n            lr = (src.bounds.right, src.bounds.bottom)\n\n            left = t(ul[0] - ((ul[0] - ll[0]) / 2),\n                     ul[1] - ((ul[1] - ll[1]) / 2))\n\n            right = t(ur[0] - ((ur[0] - lr[0]) / 2),\n                      ur[1] - ((ur[1] - lr[1]) / 2))\n\n            m = abs(left[1] - right[1]) / abs(left[0] - right[0])\n            b = left[1] - (m * left[0])\n\n    if labels_are_tif:\n        with rasterio.open(src_labels_path) as src:\n            profile = src.profile\n            bands = src.read()\n\n            with rasterio.open(dst_labels_path, \'w\', **profile) as dst:\n                fbands = np.flip(bands, 1)\n                dst.write(fbands)\n    else:\n\n        def traverse_labels(src, dst):\n            for key in src:\n                e = src[key]\n                if type(e) is dict:\n                    if key == \'geometry\':\n                        dst[key] = flip_geom(m, b, src[key])\n                    else:\n                        dst[key] = {}\n                        traverse_labels(e, dst[key])\n                elif type(e) is list:\n                    d_list = []\n                    for x in e:\n                        if type(x) is dict:\n                            ne = {}\n                            traverse_labels(x, ne)\n                            d_list.append(ne)\n                        else:\n                            d_list.append(x)\n                    dst[key] = d_list\n                else:\n                    dst[key] = e\n            return dst\n\n        with open(src_labels_path) as src_labels_file:\n            source_labels = json.loads(src_labels_file.read())\n\n        dst_labels = traverse_labels(source_labels, {})\n\n        with open(dst_labels_path, \'w\') as dst_labels_file:\n            dst_labels_file.write(json.dumps(dst_labels, indent=4))\n\n    print(\'done.\')\n\n\nif __name__ == \'__main__\':\n    flip_scene()\n'"
integration_tests/util/generate_scene.py,0,"b'import random\n\nimport click\nimport numpy as np\nimport rasterio\nfrom rasterio.transform import from_origin\n\nfrom rastervision.core.box import Box\nfrom rastervision.data import (RasterioCRSTransformer, ObjectDetectionLabels,\n                               ObjectDetectionGeoJSONStore)\nfrom rastervision.core.class_map import (ClassItem, ClassMap)\n\n\n@click.command()\n@click.option(\n    \'--task\',\n    \'-t\',\n    type=click.Choice([\'object_detection\', \'semantic_segmentation\']),\n    required=True)\n@click.option(\'--chip_size\', \'-c\', default=300, type=int)\n@click.option(\'--chips_per_dimension\', \'-s\', default=3, type=int)\n@click.argument(\'tiff_path\')\n@click.argument(\'labels_path\')\ndef generate_scene(task, tiff_path, labels_path, chip_size,\n                   chips_per_dimension):\n    """"""Generate a synthetic object detection scene.\n\n    Randomly generates a GeoTIFF with red and greed boxes denoting two\n    classes and a corresponding label file. This is useful for generating\n    synthetic scenes for testing purposes.\n    """"""\n    class_map = ClassMap([ClassItem(1, \'car\'), ClassItem(2, \'building\')])\n\n    # make extent that\'s divisible by chip_size\n    chip_size = chip_size\n    ymax = chip_size * chips_per_dimension\n    xmax = chip_size * chips_per_dimension\n    extent = Box(0, 0, ymax, xmax)\n\n    # make windows along grid\n    windows = extent.get_windows(chip_size, chip_size)\n\n    # for each window, make some random boxes within it and render to image\n    nb_channels = 3\n    image = np.zeros((ymax, xmax, nb_channels)).astype(np.uint8)\n    boxes = []\n    class_ids = []\n    for window in windows:\n        # leave some windows blank\n        if random.uniform(0, 1) > 0.3:\n            # pick a random class\n            class_id = random.randint(1, 2)\n            box = window.make_random_square(50).to_int()\n\n            boxes.append(box)\n            class_ids.append(class_id)\n\n            image[box.ymin:box.ymax, box.xmin:box.xmax, class_id - 1] = 255\n\n    # save image as geotiff centered in philly\n    transform = from_origin(-75.163506, 39.952536, 0.000001, 0.000001)\n\n    print(\'Generated {} boxes with {} different classes.\'.format(\n        len(boxes), len(set(class_ids))))\n\n    with rasterio.open(\n            tiff_path,\n            \'w\',\n            driver=\'GTiff\',\n            height=ymax,\n            transform=transform,\n            crs=\'EPSG:4326\',\n            compression=rasterio.enums.Compression.none,\n            width=xmax,\n            count=nb_channels,\n            dtype=\'uint8\') as dst:\n        for channel_ind in range(0, nb_channels):\n            dst.write(image[:, :, channel_ind], channel_ind + 1)\n\n    if task == \'object_detection\':\n        # make OD labels and make boxes\n        npboxes = Box.to_npboxes(boxes)\n        class_ids = np.array(class_ids)\n        labels = ObjectDetectionLabels(npboxes, class_ids)\n\n        # save labels to geojson\n        with rasterio.open(tiff_path) as image_dataset:\n            crs_transformer = RasterioCRSTransformer(image_dataset)\n            od_file = ObjectDetectionGeoJSONStore(labels_path, crs_transformer,\n                                                  class_map)\n            od_file.save(labels)\n    elif task == \'semantic_segmentation\':\n        label_image = np.zeros((ymax, xmax, 1)).astype(np.uint8)\n\n        for box, class_id in zip(boxes, class_ids):\n            label_image[box.ymin:box.ymax, box.xmin:box.xmax, 0] = class_id\n\n        # save labels to raster\n        with rasterio.open(\n                labels_path,\n                \'w\',\n                driver=\'GTiff\',\n                height=ymax,\n                transform=transform,\n                crs=\'EPSG:4326\',\n                compression=rasterio.enums.Compression.none,\n                width=xmax,\n                count=1,\n                dtype=\'uint8\') as dst:\n            dst.write(label_image[:, :, 0], 1)\n\n\nif __name__ == \'__main__\':\n    generate_scene()\n'"
integration_tests/util/misc.py,0,"b""def str_to_bool(x):\n    if type(x) == str:\n        if x.lower() == 'true':\n            return True\n        elif x.lower() == 'false':\n            return False\n        else:\n            raise ValueError('{} is expected to be true or false'.format(x))\n    return x\n"""
integration_tests2/chip_classification/config.py,0,"b""from os.path import join, dirname, basename\n\nfrom rastervision2.core.rv_pipeline import ChipClassificationConfig\nfrom rastervision2.core.data import (\n    ClassConfig, ChipClassificationLabelSourceConfig, GeoJSONVectorSourceConfig,\n    RasterioSourceConfig, StatsTransformerConfig, SceneConfig, DatasetConfig)\nfrom rastervision2.pytorch_backend import PyTorchChipClassificationConfig\nfrom rastervision2.pytorch_learner import (\n    Backbone, SolverConfig, ClassificationModelConfig)\n\n\ndef get_config(runner, root_uri, data_uri=None, full_train=False):\n    def get_path(part):\n        if full_train:\n            return join(data_uri, part)\n        else:\n            return join(dirname(__file__), part)\n\n    class_config = ClassConfig(\n        names=['car', 'building', 'background'],\n        colors=['red', 'blue', 'black'])\n\n    def make_scene(img_path, label_path):\n        id = basename(img_path)\n        label_source = ChipClassificationLabelSourceConfig(\n            vector_source=GeoJSONVectorSourceConfig(\n                uri=label_path, default_class_id=None, ignore_crs_field=True),\n            ioa_thresh=0.5,\n            use_intersection_over_cell=False,\n            pick_min_class_id=True,\n            background_class_id=2,\n            infer_cells=True)\n\n        raster_source = RasterioSourceConfig(\n            channel_order=[0, 1, 2], uris=[img_path],\n            transformers=[StatsTransformerConfig()])\n\n        return SceneConfig(\n            id=id,\n            raster_source=raster_source,\n            label_source=label_source)\n\n    scenes = [\n        make_scene(get_path('scene/image.tif'), get_path('scene/labels.json')),\n        make_scene(get_path('scene/image2.tif'), get_path('scene/labels2.json'))]\n    dataset = DatasetConfig(\n        class_config=class_config,\n        train_scenes=scenes,\n        validation_scenes=scenes)\n\n    if full_train:\n        model = ClassificationModelConfig(backbone=Backbone.resnet18)\n        solver = SolverConfig(\n            lr=1e-4, num_epochs=300, batch_sz=8, one_cycle=True,\n            sync_interval=300)\n    else:\n        pretrained_uri = (\n            's3://raster-vision-lf-dev/integration_tests/chip_classification/output/'\n            'train/last-model.pth')\n        model = ClassificationModelConfig(\n            backbone=Backbone.resnet18, init_weights=pretrained_uri)\n        solver = SolverConfig(\n            lr=1e-9, num_epochs=1, batch_sz=2, one_cycle=True, sync_interval=200)\n    backend = PyTorchChipClassificationConfig(\n        model=model,\n        solver=solver,\n        log_tensorboard=False,\n        run_tensorboard=False,\n        augmentors=[])\n\n    config = ChipClassificationConfig(\n        root_uri=root_uri,\n        dataset=dataset,\n        backend=backend,\n        train_chip_sz=200,\n        predict_chip_sz=200)\n\n    return config\n"""
integration_tests2/object_detection/__init__.py,0,b''
integration_tests2/object_detection/config.py,0,"b""from os.path import join, dirname\n\nfrom rastervision2.core.rv_pipeline import (\n    ObjectDetectionConfig, ObjectDetectionChipOptions, ObjectDetectionPredictOptions)\nfrom rastervision2.core.data import (\n    ClassConfig, ObjectDetectionLabelSourceConfig, GeoJSONVectorSourceConfig,\n    RasterioSourceConfig, SceneConfig, DatasetConfig)\nfrom rastervision2.pytorch_backend import PyTorchObjectDetectionConfig\nfrom rastervision2.pytorch_learner import (\n    Backbone, SolverConfig, ObjectDetectionModelConfig)\n\n\ndef get_config(runner, root_uri, data_uri=None, full_train=False):\n    def get_path(part):\n        if full_train:\n            return join(data_uri, part)\n        else:\n            return join(dirname(__file__), part)\n\n    class_config = ClassConfig(\n        names=['car', 'building'],\n        colors=['blue', 'red'])\n\n    def make_scene(scene_id, img_path, label_path):\n        raster_source = RasterioSourceConfig(\n            channel_order=[0, 1, 2], uris=[img_path])\n        label_source = ObjectDetectionLabelSourceConfig(\n            vector_source=GeoJSONVectorSourceConfig(\n                uri=label_path, default_class_id=None))\n        return SceneConfig(\n            id=scene_id,\n            raster_source=raster_source,\n            label_source=label_source)\n\n    if full_train:\n        model = ObjectDetectionModelConfig(backbone=Backbone.resnet18)\n        solver = SolverConfig(\n            lr=1e-4, num_epochs=300, batch_sz=8, one_cycle=True,\n            sync_interval=300)\n    else:\n        pretrained_uri = (\n            's3://raster-vision-lf-dev/integration_tests/object_detection/output/train/'\n            'last-model.pth')\n        model = ObjectDetectionModelConfig(\n            backbone=Backbone.resnet18, init_weights=pretrained_uri)\n        solver = SolverConfig(\n            lr=1e-9, num_epochs=1, batch_sz=2, one_cycle=True, sync_interval=200)\n    backend = PyTorchObjectDetectionConfig(\n        model=model,\n        solver=solver,\n        log_tensorboard=False,\n        run_tensorboard=False,\n        augmentors=[])\n\n    scenes = [\n        make_scene(\n            'od_test', get_path('scene/image.tif'), get_path('scene/labels.json')),\n        make_scene(\n            'od_test-2', get_path('scene/image2.tif'), get_path('scene/labels2.json'))]\n    dataset = DatasetConfig(\n        class_config=class_config,\n        train_scenes=scenes,\n        validation_scenes=scenes)\n\n    chip_options = ObjectDetectionChipOptions(neg_ratio=1.0, ioa_thresh=1.0)\n    predict_options = ObjectDetectionPredictOptions(\n        merge_thresh=0.1, score_thresh=0.5)\n\n    return ObjectDetectionConfig(\n        root_uri=root_uri,\n        dataset=dataset,\n        backend=backend,\n        train_chip_sz=300,\n        predict_chip_sz=300,\n        chip_options=chip_options,\n        predict_options=predict_options)\n"""
integration_tests2/semantic_segmentation/__init__.py,0,b''
integration_tests2/semantic_segmentation/config.py,0,"b""from os.path import join, dirname\n\nfrom rastervision2.core.data import (\n    ClassConfig, SemanticSegmentationLabelSourceConfig,\n    SemanticSegmentationLabelStoreConfig, RasterioSourceConfig, SceneConfig,\n    PolygonVectorOutputConfig, DatasetConfig, BuildingVectorOutputConfig)\nfrom rastervision2.core.rv_pipeline import (\n    SemanticSegmentationChipOptions, SemanticSegmentationWindowMethod,\n    SemanticSegmentationConfig)\nfrom rastervision2.pytorch_backend import PyTorchSemanticSegmentationConfig\nfrom rastervision2.pytorch_learner import (\n    Backbone, SolverConfig, SemanticSegmentationModelConfig)\n\n\ndef get_config(runner, root_uri, data_uri=None, full_train=False):\n    def get_path(part):\n        if full_train:\n            return join(data_uri, part)\n        else:\n            return join(dirname(__file__), part)\n\n    class_config = ClassConfig(\n        names=['red', 'green'],\n        colors=['red', 'green'])\n\n    def make_scene(id, img_path, label_path):\n        raster_source = RasterioSourceConfig(\n            channel_order=[0, 1, 2], uris=[img_path])\n        label_source = SemanticSegmentationLabelSourceConfig(\n            rgb_class_config=class_config,\n            raster_source=RasterioSourceConfig(uris=[label_path]))\n        label_store = SemanticSegmentationLabelStoreConfig(\n            rgb=True, vector_output=[\n                PolygonVectorOutputConfig(class_id=0),\n                BuildingVectorOutputConfig(class_id=1)])\n\n        return SceneConfig(\n            id=id,\n            raster_source=raster_source,\n            label_source=label_source,\n            label_store=label_store)\n\n    if full_train:\n        model = SemanticSegmentationModelConfig(backbone=Backbone.resnet50)\n        solver = SolverConfig(\n            lr=1e-4, num_epochs=300, batch_sz=8, one_cycle=True,\n            sync_interval=300)\n    else:\n        pretrained_uri = (\n            's3://raster-vision-lf-dev/integration_tests/semantic_segmentation/output/'\n            'train/last-model.pth')\n        model = SemanticSegmentationModelConfig(\n            backbone=Backbone.resnet50, init_weights=pretrained_uri)\n        solver = SolverConfig(\n            lr=1e-9, num_epochs=1, batch_sz=2, one_cycle=True, sync_interval=200)\n    backend = PyTorchSemanticSegmentationConfig(\n        model=model,\n        solver=solver,\n        log_tensorboard=False,\n        run_tensorboard=False,\n        augmentors=[])\n\n    scenes = [\n        make_scene(\n            'test-scene', get_path('scene/image.tif'), get_path('scene/labels.tif')),\n        make_scene(\n            'test-scene2', get_path('scene/image2.tif'), get_path('scene/labels2.tif'))]\n    dataset = DatasetConfig(\n        class_config=class_config,\n        train_scenes=scenes,\n        validation_scenes=scenes)\n\n    chip_options = SemanticSegmentationChipOptions(\n        window_method=SemanticSegmentationWindowMethod.sliding, stride=300)\n\n    return SemanticSegmentationConfig(\n        root_uri=root_uri,\n        dataset=dataset,\n        backend=backend,\n        train_chip_sz=300,\n        predict_chip_sz=300,\n        chip_options=chip_options)\n"""
integration_tests2/util/__init__.py,0,b''
integration_tests2/util/flip_scene.py,0,"b'import click\nimport numpy as np\nimport rasterio\nimport pyproj\nimport json\n\n\ndef flip_geom(m, b, geom):\n    """"""Flips a geom along a straight line y = mx + b.\n    """"""\n\n    def traverse_coords(coords, dst_coords):\n        for p in coords:\n            if type(p[0]) is list:\n                lst = []\n                traverse_coords(p, lst)\n                dst_coords.append(lst)\n            else:\n                x, y = p[0], p[1]\n                d = (x + (y - b) * m) / (1 + m * m)\n                x2 = 2 * d - x\n                y2 = 2 * d * m - y + 2 * b\n                dst_coords.append((x2, y2))\n        return dst_coords\n\n    return {\n        \'type\': geom[\'type\'],\n        \'coordinates\': traverse_coords(geom[\'coordinates\'], [])\n    }\n\n\n@click.command()\n@click.argument(\'src_tiff_path\')\n@click.argument(\'src_labels_path\')\n@click.argument(\'dst_tiff_path\')\n@click.argument(\'dst_labels_path\')\ndef flip_scene(src_tiff_path, src_labels_path, dst_tiff_path, dst_labels_path):\n    """"""Flips a scene and it\'s labels.\n\n    Useful for generating multiple training scenes for integration test usage.\n    """"""\n\n    labels_are_tif = src_labels_path.endswith(\'.tif\')\n\n    with rasterio.open(src_tiff_path) as src:\n        profile = src.profile\n        bands = src.read()\n\n        with rasterio.open(dst_tiff_path, \'w\', **profile) as dst:\n            fbands = np.flip(bands, 1)\n            dst.write(fbands)\n\n        if not labels_are_tif:\n\n            img_crs = pyproj.Proj(init=src.crs[\'init\'])\n            map_crs = pyproj.Proj(init=\'epsg:4326\')\n\n            def t(x, y):\n                return pyproj.transform(img_crs, map_crs, x, y)\n\n            # Find the center horizontal line through the image.\n\n            ll = (src.bounds.left, src.bounds.bottom)\n            ul = (src.bounds.left, src.bounds.top)\n            ur = (src.bounds.right, src.bounds.top)\n            lr = (src.bounds.right, src.bounds.bottom)\n\n            left = t(ul[0] - ((ul[0] - ll[0]) / 2),\n                     ul[1] - ((ul[1] - ll[1]) / 2))\n\n            right = t(ur[0] - ((ur[0] - lr[0]) / 2),\n                      ur[1] - ((ur[1] - lr[1]) / 2))\n\n            m = abs(left[1] - right[1]) / abs(left[0] - right[0])\n            b = left[1] - (m * left[0])\n\n    if labels_are_tif:\n        with rasterio.open(src_labels_path) as src:\n            profile = src.profile\n            bands = src.read()\n\n            with rasterio.open(dst_labels_path, \'w\', **profile) as dst:\n                fbands = np.flip(bands, 1)\n                dst.write(fbands)\n    else:\n\n        def traverse_labels(src, dst):\n            for key in src:\n                e = src[key]\n                if type(e) is dict:\n                    if key == \'geometry\':\n                        dst[key] = flip_geom(m, b, src[key])\n                    else:\n                        dst[key] = {}\n                        traverse_labels(e, dst[key])\n                elif type(e) is list:\n                    d_list = []\n                    for x in e:\n                        if type(x) is dict:\n                            ne = {}\n                            traverse_labels(x, ne)\n                            d_list.append(ne)\n                        else:\n                            d_list.append(x)\n                    dst[key] = d_list\n                else:\n                    dst[key] = e\n            return dst\n\n        with open(src_labels_path) as src_labels_file:\n            source_labels = json.loads(src_labels_file.read())\n\n        dst_labels = traverse_labels(source_labels, {})\n\n        with open(dst_labels_path, \'w\') as dst_labels_file:\n            dst_labels_file.write(json.dumps(dst_labels, indent=4))\n\n    print(\'done.\')\n\n\nif __name__ == \'__main__\':\n    flip_scene()\n'"
integration_tests2/util/generate_scene.py,0,"b'import random\n\nimport click\nimport numpy as np\nimport rasterio\nfrom rasterio.transform import from_origin\n\nfrom rastervision.core.box import Box\nfrom rastervision.data import (RasterioCRSTransformer, ObjectDetectionLabels,\n                               ObjectDetectionGeoJSONStore)\nfrom rastervision.core.class_map import (ClassItem, ClassMap)\n\n\n@click.command()\n@click.option(\n    \'--task\',\n    \'-t\',\n    type=click.Choice([\'object_detection\', \'semantic_segmentation\']),\n    required=True)\n@click.option(\'--chip_size\', \'-c\', default=300, type=int)\n@click.option(\'--chips_per_dimension\', \'-s\', default=3, type=int)\n@click.argument(\'tiff_path\')\n@click.argument(\'labels_path\')\ndef generate_scene(task, tiff_path, labels_path, chip_size,\n                   chips_per_dimension):\n    """"""Generate a synthetic object detection scene.\n\n    Randomly generates a GeoTIFF with red and greed boxes denoting two\n    classes and a corresponding label file. This is useful for generating\n    synthetic scenes for testing purposes.\n    """"""\n    class_map = ClassMap([ClassItem(1, \'car\'), ClassItem(2, \'building\')])\n\n    # make extent that\'s divisible by chip_size\n    chip_size = chip_size\n    ymax = chip_size * chips_per_dimension\n    xmax = chip_size * chips_per_dimension\n    extent = Box(0, 0, ymax, xmax)\n\n    # make windows along grid\n    windows = extent.get_windows(chip_size, chip_size)\n\n    # for each window, make some random boxes within it and render to image\n    nb_channels = 3\n    image = np.zeros((ymax, xmax, nb_channels)).astype(np.uint8)\n    boxes = []\n    class_ids = []\n    for window in windows:\n        # leave some windows blank\n        if random.uniform(0, 1) > 0.3:\n            # pick a random class\n            class_id = random.randint(1, 2)\n            box = window.make_random_square(50).to_int()\n\n            boxes.append(box)\n            class_ids.append(class_id)\n\n            image[box.ymin:box.ymax, box.xmin:box.xmax, class_id - 1] = 255\n\n    # save image as geotiff centered in philly\n    transform = from_origin(-75.163506, 39.952536, 0.000001, 0.000001)\n\n    print(\'Generated {} boxes with {} different classes.\'.format(\n        len(boxes), len(set(class_ids))))\n\n    with rasterio.open(\n            tiff_path,\n            \'w\',\n            driver=\'GTiff\',\n            height=ymax,\n            transform=transform,\n            crs=\'EPSG:4326\',\n            compression=rasterio.enums.Compression.none,\n            width=xmax,\n            count=nb_channels,\n            dtype=\'uint8\') as dst:\n        for channel_ind in range(0, nb_channels):\n            dst.write(image[:, :, channel_ind], channel_ind + 1)\n\n    if task == \'object_detection\':\n        # make OD labels and make boxes\n        npboxes = Box.to_npboxes(boxes)\n        class_ids = np.array(class_ids)\n        labels = ObjectDetectionLabels(npboxes, class_ids)\n\n        # save labels to geojson\n        with rasterio.open(tiff_path) as image_dataset:\n            crs_transformer = RasterioCRSTransformer(image_dataset)\n            od_file = ObjectDetectionGeoJSONStore(labels_path, crs_transformer,\n                                                  class_map)\n            od_file.save(labels)\n    elif task == \'semantic_segmentation\':\n        label_image = np.zeros((ymax, xmax, 1)).astype(np.uint8)\n\n        for box, class_id in zip(boxes, class_ids):\n            label_image[box.ymin:box.ymax, box.xmin:box.xmax, 0] = class_id\n\n        # save labels to raster\n        with rasterio.open(\n                labels_path,\n                \'w\',\n                driver=\'GTiff\',\n                height=ymax,\n                transform=transform,\n                crs=\'EPSG:4326\',\n                compression=rasterio.enums.Compression.none,\n                width=xmax,\n                count=1,\n                dtype=\'uint8\') as dst:\n            dst.write(label_image[:, :, 0], 1)\n\n\nif __name__ == \'__main__\':\n    generate_scene()\n'"
rastervision/analyzer/__init__.py,0,b'# flake8: noqa\n\nfrom rastervision.analyzer.analyzer import *\nfrom rastervision.analyzer.analyzer_config import *\nfrom rastervision.analyzer.stats_analyzer import *\nfrom rastervision.analyzer.stats_analyzer_config import *\n'
rastervision/analyzer/analyzer.py,0,"b'from abc import (ABC, abstractmethod)\n\n\nclass Analyzer(ABC):\n    """"""Class that processes scenes to produce\n       some output during the ANALYZE command, which\n       can be used for later procesess.\n\n    """"""\n\n    @abstractmethod\n    def process(self, scenes, tmp_dir):\n        pass\n'"
rastervision/analyzer/analyzer_config.py,0,"b'from abc import abstractmethod\n\nimport rastervision as rv\nfrom rastervision.core.config import (Config, ConfigBuilder,\n                                      BundledConfigMixin)\n\n\nclass AnalyzerConfig(BundledConfigMixin, Config):\n    def __init__(self, analyzer_type):\n        self.analyzer_type = analyzer_type\n\n    @abstractmethod\n    def create_analyzer(self):\n        """"""Create the Transformer that this configuration represents\n        """"""\n        pass\n\n    def to_builder(self):\n        return rv._registry.get_config_builder(rv.ANALYZER,\n                                               self.analyzer_type)(self)\n\n    @staticmethod\n    def builder(analyzer_type):\n        return rv._registry.get_config_builder(rv.ANALYZER, analyzer_type)()\n\n    @staticmethod\n    def from_proto(msg):\n        """"""Creates a TaskConfig from the specificed protobuf message\n        """"""\n        return rv._registry.get_config_builder(rv.ANALYZER, msg.analyzer_type)() \\\n                           .from_proto(msg) \\\n                           .build()\n\n\nclass AnalyzerConfigBuilder(ConfigBuilder):\n    pass\n'"
rastervision/analyzer/api.py,0,"b""# flake8: noqa\n\n# Registry keys\n\nANALYZER = 'ANALYZER'\n\nSTATS_ANALYZER = 'STATS_ANALYZER'\n\nfrom rastervision.analyzer.analyzer_config import AnalyzerConfig\n"""
rastervision/analyzer/stats_analyzer.py,0,"b'from rastervision.analyzer import Analyzer\nfrom rastervision.core import RasterStats\n\n\nclass StatsAnalyzer(Analyzer):\n    """"""Computes RasterStats against the entire scene set.\n    """"""\n\n    def __init__(self, stats_uri, sample_prob=None):\n        self.stats_uri = stats_uri\n        self.sample_prob = sample_prob\n\n    def process(self, scenes, tmp_dir):\n        stats = RasterStats()\n        stats.compute(\n            list(map(lambda s: s.raster_source, scenes)),\n            sample_prob=self.sample_prob)\n        stats.save(self.stats_uri)\n'"
rastervision/analyzer/stats_analyzer_config.py,0,"b'import os\nfrom copy import deepcopy\n\nimport rastervision as rv\nfrom rastervision.analyzer import (AnalyzerConfig, AnalyzerConfigBuilder,\n                                   StatsAnalyzer)\nfrom rastervision.protos.analyzer_pb2 import AnalyzerConfig as AnalyzerConfigMsg\n\n\nclass StatsAnalyzerConfig(AnalyzerConfig):\n    def __init__(self, stats_uri=None, sample_prob=None):\n        super().__init__(rv.STATS_ANALYZER)\n        self.stats_uri = stats_uri\n        self.sample_prob = sample_prob\n\n    def create_analyzer(self):\n        if not self.stats_uri:\n            raise rv.ConfigError(\'stats_uri is not set.\')\n        return StatsAnalyzer(self.stats_uri, self.sample_prob)\n\n    def to_proto(self):\n        msg = AnalyzerConfigMsg(analyzer_type=self.analyzer_type)\n        if self.stats_uri:\n            msg.stats_analyzer_config.stats_uri = self.stats_uri\n        msg.stats_analyzer_config.sample_prob = \\\n            (0.0 if self.sample_prob is None else self.sample_prob)\n        return msg\n\n    def save_bundle_files(self, bundle_dir):\n        if not self.stats_uri:\n            raise rv.ConfigError(\'stat_uri is not set.\')\n        # Only set the basename, do not contribute file\n        # as it is not and input and only an output of\n        # this analyzer. The StatsTransformer will save\n        # its input separately.\n        base_name = os.path.basename(self.stats_uri)\n        new_config = self.to_builder() \\\n                         .with_stats_uri(base_name) \\\n                         .build()\n        return (new_config, [])\n\n    def load_bundle_files(self, bundle_dir):\n        if not self.stats_uri:\n            raise rv.ConfigError(\'stat_uri is not set.\')\n        local_stats_uri = os.path.join(bundle_dir, self.stats_uri)\n        return self.to_builder() \\\n                   .with_stats_uri(local_stats_uri) \\\n                   .build()\n\n    def update_for_command(self, command_type, experiment_config,\n                           context=None):\n        if command_type == rv.ANALYZE:\n            if not self.stats_uri:\n                self.stats_uri = os.path.join(experiment_config.analyze_uri,\n                                              \'stats.json\')\n\n    def report_io(self, command_type, io_def):\n        if command_type == rv.ANALYZE:\n            io_def.add_output(self.stats_uri)\n\n\nclass StatsAnalyzerConfigBuilder(AnalyzerConfigBuilder):\n    def __init__(self, prev=None):\n        config = {}\n        if prev:\n            config = {\n                \'stats_uri\': prev.stats_uri,\n                \'sample_prob\': prev.sample_prob\n            }\n        super().__init__(StatsAnalyzerConfig, config)\n\n    def from_proto(self, msg):\n        stats_uri = (msg.stats_analyzer_config.stats_uri or msg.stats_uri)\n        b = self.with_stats_uri(stats_uri)\n\n        sample_prob = msg.stats_analyzer_config.sample_prob\n        sample_prob = (None if sample_prob == 0 else sample_prob)\n        b = b.with_sample_prob(sample_prob)\n        return b\n\n    def with_stats_uri(self, stats_uri):\n        """"""Set the stats_uri.\n\n            Args:\n                stats_uri: URI to the stats json to use\n        """"""\n        b = deepcopy(self)\n        b.config[\'stats_uri\'] = stats_uri\n        return b\n\n    def validate(self):\n        sample_prob = self.config.get(\'sample_prob\')\n        if sample_prob and (not isinstance(sample_prob, float)\n                            or sample_prob >= 1.0 or sample_prob <= 0):\n            raise rv.ConfigError(\n                \'sample_prob must be a float between 0 and 1 exclusive.\')\n\n    def with_sample_prob(self, sample_prob):\n        """"""Set the sample_prob used to sample a subset of each scene.\n\n        If sample_prob is set, then a subset of each scene is used to compute stats which\n        speeds up the computation. Roughly speaking, if sample_prob=0.5, then half the\n        pixels in the scene will be used. More precisely, the number of chips is equal to\n        sample_prob * (width * height / 300^2), or 1, whichever is greater. Each chip is\n        uniformly sampled from the scene with replacement. Otherwise, it uses a sliding\n        window over the entire scene to compute stats.\n\n        Args:\n            sample_prob: (float or None) between 0 and 1\n        """"""\n        b = deepcopy(self)\n        b.config[\'sample_prob\'] = sample_prob\n        return b\n'"
rastervision/augmentor/__init__.py,0,b'# flake8: noqa\n\nfrom rastervision.augmentor.augmentor import *\nfrom rastervision.augmentor.augmentor_config import *\nfrom rastervision.augmentor.nodata_augmentor import *\nfrom rastervision.augmentor.nodata_augmentor_config import *\n'
rastervision/augmentor/api.py,0,"b""# flake8: noqa\n\n# Registry keyss\n\nAUGMENTOR = 'AUGMENTOR'\n\nNODATA_AUGMENTOR = 'NODATA_AUGMENTOR'\n\nfrom rastervision.augmentor.augmentor_config import AugmentorConfig\n"""
rastervision/augmentor/augmentor.py,0,"b'from abc import (ABC, abstractmethod)\n\n\nclass Augmentor(ABC):\n    """"""Defines a method for augmenting training data.\n    """"""\n\n    @abstractmethod\n    def process(self, training_data, tmp_dir):\n        """"""Augment training data.\n\n        Args:\n           training_data: The TrainingData to augment\n\n        Returns:\n           augmented TrainingData\n        """"""\n        pass\n'"
rastervision/augmentor/augmentor_config.py,0,"b'from abc import abstractmethod\n\nimport rastervision as rv\nfrom rastervision.core import (Config, ConfigBuilder)\n\n\nclass AugmentorConfig(Config):\n    def __init__(self, augmentor_type):\n        self.augmentor_type = augmentor_type\n\n    @abstractmethod\n    def create_augmentor(self):\n        """"""Create the Augmentor that this configuration represents""""""\n        pass\n\n    def to_builder(self, augmentor_type):\n        return rv._registry.get_config_builder(rv.AUGMENTOR,\n                                               self.augmentor_type)(self)\n\n    @staticmethod\n    def builder(augmentor_type):\n        return rv._registry.get_config_builder(rv.AUGMENTOR, augmentor_type)()\n\n    @staticmethod\n    def from_proto(msg):\n        """"""Creates a AugmentorConfig from the specificed protobuf message\n        """"""\n        return rv._registry.get_config_builder(rv.AUGMENTOR, msg.augmentor_type)() \\\n                           .from_proto(msg) \\\n                           .build()\n\n\nclass AugmentorConfigBuilder(ConfigBuilder):\n    pass\n'"
rastervision/augmentor/nodata_augmentor.py,0,"b'import random\n\nimport numpy as np\n\nfrom rastervision.augmentor import Augmentor\nfrom rastervision.core import (TrainingData, Box)\n\n\nclass NodataAugmentor(Augmentor):\n    """"""Randomly add NoData values to negative chips.\n\n    This is useful for training the model to negatively predict\n    chips that are on boundaries or containing mostly NoData.\n    """"""\n\n    def __init__(self, aug_prob):\n        self.aug_prob = aug_prob\n\n    def process(self, training_data, tmp_dir):\n        augmented = TrainingData()\n        nodata_aug_prob = self.aug_prob\n\n        for chip, window, labels in training_data:\n            # If negative chip, with some probability, add a random black square\n            # to chip.\n            if len(labels) == 0 and random.uniform(0, 1) < nodata_aug_prob:\n                size = round(random.uniform(0, 1) * chip.shape[0])\n                square = Box(0, 0, chip.shape[0],\n                             chip.shape[1]).make_random_square(size)\n                chip = np.copy(chip)\n                chip[square.ymin:square.ymax, square.xmin:square.xmax, :] = 0\n\n            augmented.append(chip, window, labels)\n\n        return augmented\n'"
rastervision/augmentor/nodata_augmentor_config.py,0,"b'from copy import deepcopy\n\nimport rastervision as rv\nfrom rastervision.augmentor.nodata_augmentor import NodataAugmentor\nfrom rastervision.augmentor.augmentor_config \\\n    import (AugmentorConfig, AugmentorConfigBuilder)\nfrom rastervision.protos.augmentor_pb2 import AugmentorConfig as AugmentorConfigMsg\n\n\nclass NodataAugmentorConfig(AugmentorConfig):\n    def __init__(self, aug_prob=0.5):\n        super().__init__(rv.NODATA_AUGMENTOR)\n        self.aug_prob = aug_prob\n\n    def to_proto(self):\n        msg = AugmentorConfigMsg(\n            augmentor_type=self.augmentor_type, aug_prob=self.aug_prob)\n        return msg\n\n    def create_augmentor(self):\n        return NodataAugmentor(self.aug_prob)\n\n    def report_io(self, command_type, io_def):\n        pass\n\n\nclass NodataAugmentorConfigBuilder(AugmentorConfigBuilder):\n    def __init__(self, prev=None):\n        config = {}\n        if prev:\n            config = {\'aug_prob\': prev.aug_prob}\n        super().__init__(NodataAugmentorConfig, config)\n\n    def from_proto(self, msg):\n        return self.with_probablity(msg.aug_prob)\n\n    def with_probability(self, aug_prob):\n        """"""Sets the probability for this augmentation.\n\n        Determines how probable this augmentation will happen\n        to negative chips.\n\n        Args:\n           aug_prob: Float value between 0.0 and 1.0\n        """"""\n        b = deepcopy(self)\n        b.config[\'aug_prob\'] = aug_prob\n        return b\n'"
rastervision/backend/__init__.py,0,"b'# flake8: noqa\n\nfrom rastervision.backend.backend import *\nfrom rastervision.backend.backend_config import *\n\ntry:\n    import tensorflow\n    tf_available = True\nexcept ImportError as e:\n    tf_available = False\n\ntry:\n    import torch\n    pytorch_available = True\nexcept ImportError as e:\n    pytorch_available = False\n\nif tf_available:\n    from rastervision.backend.tf_object_detection import TFObjectDetection\n    from rastervision.backend.tf_object_detection_config import (\n        TFObjectDetectionConfig, TFObjectDetectionConfigBuilder)\n    from rastervision.backend.keras_classification import KerasClassification\n    from rastervision.backend.keras_classification_config import (\n        KerasClassificationConfig, KerasClassificationConfigBuilder)\n    from rastervision.backend.tf_deeplab import TFDeeplab\n    from rastervision.backend.tf_deeplab_config import (TFDeeplabConfig,\n                                                        TFDeeplabConfigBuilder)\n\nif pytorch_available:\n    from rastervision.backend.pytorch_chip_classification import (\n        PyTorchChipClassification)\n    from rastervision.backend.pytorch_chip_classification_config import (\n        PyTorchChipClassificationConfig,\n        PyTorchChipClassificationConfigBuilder)\n    from rastervision.backend.pytorch_semantic_segmentation import (\n        PyTorchSemanticSegmentation)\n    from rastervision.backend.pytorch_semantic_segmentation_config import (\n        PyTorchSemanticSegmentationConfig,\n        PyTorchSemanticSegmentationConfigBuilder)\n    from rastervision.backend.pytorch_object_detection_config import (\n        PyTorchObjectDetectionConfig, PyTorchObjectDetectionConfigBuilder)\n'"
rastervision/backend/api.py,0,"b""# flake8: noqa\n\n# Registry keys\n\nBACKEND = 'BACKEND'\n\n## Backend Keys\n\nTF_OBJECT_DETECTION = 'TF_OBJECT_DETECTION'\nKERAS_CLASSIFICATION = 'KERAS_CLASSIFICATION'\nTF_DEEPLAB = 'TF_DEEPLAB'\n\nPYTORCH_SEMANTIC_SEGMENTATION = 'PYTORCH_SEMANTIC_SEGMENTATION'\nPYTORCH_CHIP_CLASSIFICATION = 'PYTORCH_CHIP_CLASSIFICATION'\nPYTORCH_OBJECT_DETECTION = 'PYTORCH_OBJECT_DETECTION'\n\n## Model keys\n\n### TF Object Detection\nSSD_MOBILENET_V1_COCO = 'SSD_MOBILENET_V1_COCO'\nSSD_MOBILENET_V2_COCO = 'SSD_MOBILENET_V2_COCO'\nSSDLITE_MOBILENET_V2_COCO = 'SSDLITE_MOBILENET_V2_COCO'\nSSD_INCEPTION_V2_COCO = 'SSD_INCEPTION_V2_COCO'\nFASTER_RCNN_INCEPTION_V2_COCO = 'FASTER_RCNN_INCEPTION_V2_COCO'\nFASTER_RCNN_RESNET50_COCO = 'FASTER_RCNN_RESNET50_COCO'\nRFCN_RESNET101_COCO = 'RFCN_RESNET101_COCO'\nFASTER_RCNN_RESNET101_COCO = 'FASTER_RCNN_RESNET101_COCO'\nFASTER_RCNN_INCEPTION_RESNET_V2_ATROUS_COCO = \\\n'FASTER_RCNN_INCEPTION_RESNET_V2_ATROUS_COCO'\nMASK_RCNN_INCEPTION_RESNET_V2_ATROUS_COCO = \\\n'MASK_RCNN_INCEPTION_RESNET_V2_ATROUS_COCO'\nMASK_RCNN_INCEPTION_V2_COCO = 'MASK_RCNN_INCEPTION_V2_COCO'\nMASK_RCNN_RESNET101_ATROUS_COCO = 'MASK_RCNN_RESNET101_ATROUS_COCO'\nMASK_RCNN_RESNET50_ATROUS_COCO = 'MASK_RCNN_RESNET50_ATROUS_COCO'\n\n## Keras Classificaiton\nRESNET50_IMAGENET = 'RESNET50_IMAGENET'\n\n# TF Deeplab\nXCEPTION_65 = 'XCEPTION_65'\nMOBILENET_V2 = 'MOBILENET_V2'\n\nfrom .backend_config import BackendConfig\n"""
rastervision/backend/backend.py,0,"b'from abc import ABC, abstractmethod\n\n\nclass Backend(ABC):\n    """"""Functionality for a specific implementation of an MLTask.\n\n    This should be subclassed to provide a bridge to third party ML libraries.\n    There is a many-to-one relationship from backends to tasks.\n    """"""\n\n    @abstractmethod\n    def process_scene_data(self, scene, data, tmp_dir):\n        """"""Process each scene\'s training data\n\n        Args:\n            scene: Scene\n            data: TrainingData\n\n        Returns:\n            backend-specific data-structures consumed by backend\'s\n            process_sceneset_results\n        """"""\n        pass\n\n    @abstractmethod\n    def process_sceneset_results(self, training_results, validation_results,\n                                 tmp_dir):\n        """"""After all scenes have been processed, process the resultset\n\n        Args:\n            training_results: dependent on the ml_backend\'s process_scene_data\n            validation_results: dependent on the ml_backend\'s\n                process_scene_data\n        """"""\n        pass\n\n    @abstractmethod\n    def train(self, tmp_dir):\n        """"""Train a model.\n        """"""\n        pass\n\n    @abstractmethod\n    def load_model(self, tmp_dir):\n        """"""Load the model in preparation for one or more prediction calls.""""""\n        pass\n\n    @abstractmethod\n    def predict(self, chips, windows, tmp_dir):\n        """"""Return predictions for a chip using model.\n\n        Args:\n            chips: [[height, width, channels], ...] numpy array of chips\n            windows: List of boxes that are the windows aligned with the chips.\n\n        Return:\n            Labels object containing predictions\n        """"""\n        pass\n'"
rastervision/backend/backend_config.py,0,"b'from abc import abstractmethod\nfrom copy import deepcopy\n\nimport rastervision as rv\nfrom rastervision.rv_config import RVConfig\nfrom rastervision.core import (Config, ConfigBuilder, BundledConfigMixin)\n\n\nclass BackendConfig(BundledConfigMixin, Config):\n    def __init__(self, backend_type, pretrained_model_uri=None):\n        self.backend_type = backend_type\n        self.pretrained_model_uri = pretrained_model_uri\n\n    @abstractmethod\n    def create_backend(self, task_config):\n        """"""Create the Backend that this configuration represents\n\n           Args:\n              task_config: The task configuration for the task\n                           to be accomplished by this backend.\n        """"""\n        pass\n\n    def to_builder(self):\n        return rv._registry.get_config_builder(rv.BACKEND,\n                                               self.backend_type)(self)\n\n    @staticmethod\n    def builder(backend_type):\n        return rv._registry.get_config_builder(rv.BACKEND, backend_type)()\n\n    @staticmethod\n    def from_proto(msg):\n        """"""Creates a BackendConfig from the specificed protobuf message\n        """"""\n        return rv._registry.get_config_builder(rv.BACKEND, msg.backend_type)() \\\n                           .from_proto(msg) \\\n                           .build()\n\n    def report_io(self, command_type, io_def):\n        if command_type == rv.TRAIN:\n            if self.pretrained_model_uri:\n                io_def.add_input(self.pretrained_model_uri)\n\n\nclass BackendConfigBuilder(ConfigBuilder):\n    def __init__(self, backend_type, config_class, config=None, prev=None):\n        if config is None:\n            config = {}\n        super().__init__(config_class, config)\n        self.task = None\n        self.backend_type = backend_type\n\n    @abstractmethod\n    def _applicable_tasks(self):\n        """"""Returns the tasks that this backend can be applied to.\n        """"""\n        pass\n\n    @abstractmethod\n    def _process_task(self, task):\n        """"""Subclasses override this to set up configuration related\n           to this task\n        """"""\n        pass\n\n    def from_proto(self, msg):\n        return self \\\n            .with_pretrained_model(msg.pretrained_model_uri)\n\n    def with_task(self, task):\n        """"""Sets a specific task type.\n\n        Args:\n            task:  A TaskConfig object.\n\n        """"""\n        if task.task_type not in self._applicable_tasks():\n            raise Exception(\n                \'Backend of type {} cannot be applied to task type {}\'.format(\n                    self.backend_type, task.task_type))\n        b = deepcopy(self)\n        b.task = task\n        b = b._process_task()\n        return b\n\n    def with_pretrained_model(self, uri):\n        """"""Set a pretrained model URI. The filetype and meaning\n           for this model will be different based on the backend implementation.\n        """"""\n        b = deepcopy(self)\n        b.config[\'pretrained_model_uri\'] = uri\n        return b\n\n    def with_model_defaults(self, model_defaults_key):\n        """"""Sets the backend configuration and pretrained model defaults\n           according to the model defaults configuration.\n        """"""\n        model_defaults = RVConfig.get_instance().get_model_defaults()\n\n        if self.backend_type in model_defaults:\n            backend_defaults = model_defaults[self.backend_type]\n            if model_defaults_key in backend_defaults:\n                return self._load_model_defaults(\n                    backend_defaults[model_defaults_key])\n            else:\n                raise rv.ConfigError(\'No defaults found for model key {}\'\n                                     .format(model_defaults_key))\n        else:\n            raise rv.ConfigError(\'No model defaults for backend {}\'\n                                 .format(self.backend_type))\n        return self\n\n    def _load_model_defaults(self, model_defaults):\n        """"""Overriding classes should handle this if they\n           want to allow default parameters to be loaded\n           from the default configurations.\n        """"""\n        return self\n'"
rastervision/backend/keras_classification_config.py,0,"b'import os\nfrom copy import deepcopy\nfrom google.protobuf import (json_format)\n\nimport rastervision as rv\nfrom rastervision.backend import (BackendConfig, BackendConfigBuilder)\nfrom rastervision.utils.misc import set_nested_keys\nfrom rastervision.protos.backend_pb2 import BackendConfig as BackendConfigMsg\nfrom rastervision.utils.files import file_to_str\nfrom rastervision.protos.keras_classification.pipeline_pb2 import PipelineConfig\nfrom rastervision.task.chip_classification_config import ChipClassificationConfig\n\n\nclass KerasClassificationConfig(BackendConfig):\n    class TrainOptions:\n        def __init__(self,\n                     sync_interval=600,\n                     do_monitoring=True,\n                     replace_model=False):\n            self.sync_interval = sync_interval\n            self.do_monitoring = do_monitoring\n            self.replace_model = replace_model\n\n    def __init__(self,\n                 kc_config,\n                 pretrained_model_uri=None,\n                 train_options=None,\n                 debug=False,\n                 training_data_uri=None,\n                 training_output_uri=None,\n                 model_uri=None):\n        if train_options is None:\n            train_options = KerasClassificationConfig.TrainOptions()\n\n        super().__init__(rv.KERAS_CLASSIFICATION, pretrained_model_uri)\n        self.kc_config = kc_config\n        self.pretrained_model_uri = pretrained_model_uri\n        self.train_options = train_options\n        self.debug = debug\n\n        # Internally set from command preprocessing\n        self.training_data_uri = training_data_uri\n        self.training_output_uri = training_output_uri\n        self.model_uri = model_uri\n\n    def create_backend(self, task_config):\n        from rastervision.backend.keras_classification import KerasClassification\n        return KerasClassification(self, task_config)\n\n    def to_proto(self):\n        d = {\n            \'sync_interval\': self.train_options.sync_interval,\n            \'do_monitoring\': self.train_options.do_monitoring,\n            \'replace_model\': self.train_options.replace_model,\n            \'training_data_uri\': self.training_data_uri,\n            \'training_output_uri\': self.training_output_uri,\n            \'model_uri\': self.model_uri,\n            \'debug\': self.debug,\n            \'kc_config\': self.kc_config\n        }\n\n        conf = json_format.ParseDict(\n            d, BackendConfigMsg.KerasClassificationConfig())\n\n        msg = BackendConfigMsg(\n            backend_type=rv.KERAS_CLASSIFICATION,\n            keras_classification_config=conf)\n\n        if self.pretrained_model_uri:\n            msg.MergeFrom(\n                BackendConfigMsg(\n                    pretrained_model_uri=self.pretrained_model_uri))\n\n        return msg\n\n    def save_bundle_files(self, bundle_dir):\n        if not self.model_uri:\n            raise rv.ConfigError(\'model_uri is not set.\')\n        local_path, base_name = self.bundle_file(self.model_uri, bundle_dir)\n        new_config = self.to_builder() \\\n                         .with_model_uri(base_name) \\\n                         .build()\n        return (new_config, [local_path])\n\n    def load_bundle_files(self, bundle_dir):\n        if not self.model_uri:\n            raise rv.ConfigError(\'model_uri is not set.\')\n        local_model_uri = os.path.join(bundle_dir, self.model_uri)\n        return self.to_builder() \\\n                   .with_model_uri(local_model_uri) \\\n                   .build()\n\n    def update_for_command(self, command_type, experiment_config,\n                           context=None):\n        super().update_for_command(command_type, experiment_config, context)\n\n        if command_type == rv.CHIP:\n            if not self.training_data_uri:\n                self.training_data_uri = experiment_config.chip_uri\n\n        if command_type == rv.TRAIN:\n            if not self.training_output_uri:\n                self.training_output_uri = experiment_config.train_uri\n            if not self.model_uri:\n                self.model_uri = os.path.join(self.training_output_uri,\n                                              \'model\')\n\n    def report_io(self, command_type, io_def):\n        super().report_io(command_type, io_def)\n        if command_type == rv.CHIP:\n            io_def.add_output(self.training_data_uri)\n\n        if command_type == rv.TRAIN:\n            if not self.training_data_uri:\n                io_def.add_missing(\'Missing training_data_uri.\')\n            else:\n                io_def.add_input(self.training_data_uri)\n\n            io_def.add_output(self.model_uri)\n\n        if command_type in [rv.PREDICT, rv.BUNDLE]:\n            if not self.model_uri:\n                io_def.add_missing(\'Missing model_uri.\')\n            else:\n                io_def.add_input(self.model_uri)\n\n\nclass KerasClassificationConfigBuilder(BackendConfigBuilder):\n    def __init__(self, prev=None):\n        config = {}\n        if prev:\n            config = {\n                \'kc_config\': prev.kc_config,\n                \'pretrained_model_uri\': prev.pretrained_model_uri,\n                \'train_options\': prev.train_options,\n                \'debug\': prev.debug,\n                \'training_data_uri\': prev.training_data_uri,\n                \'training_output_uri\': prev.training_output_uri,\n                \'model_uri\': prev.model_uri\n            }\n        super().__init__(rv.KERAS_CLASSIFICATION, KerasClassificationConfig,\n                         config, prev)\n        self.config_mods = []\n        self.require_task = prev is None\n\n    def from_proto(self, msg):\n        b = super().from_proto(msg)\n        conf = msg.keras_classification_config\n        # Since this is coming from a serialized message,\n        # assume the task has already been set and do not\n        # require it during validation.\n        b.require_task = False\n        if self.config.get(\'pretrained_model_uri\'):\n            b = b.with_pretrained_model_uri(self.config.pretrained_model_uri)\n        b = b.with_train_options(\n            sync_interval=conf.sync_interval,\n            do_monitoring=conf.do_monitoring,\n            replace_model=conf.replace_model,\n        )\n        b = b.with_debug(conf.debug)\n\n        b = b.with_training_data_uri(conf.training_data_uri)\n        b = b.with_training_output_uri(conf.training_output_uri)\n        b = b.with_model_uri(conf.model_uri)\n\n        return b.with_template(json_format.MessageToDict(conf.kc_config))\n\n    def validate(self):\n        super().validate()\n        if not self.config.get(\'kc_config\'):\n            raise rv.ConfigError(\'You must specify a template for the backend \'\n                                 \'configuration - use ""with_template"".\')\n\n        if not isinstance(self.config.get(\'kc_config\'), dict):\n            raise rv.ConfigError(\n                \'kc_config must be of type dict, got {}\'.format(\n                    type(self.config.get(\'kc_config\'))))\n\n        if self.require_task and not self.task:\n            raise rv.ConfigError(\'You must specify the task this backend \'\n                                 \'is for - use ""with_task"".\')\n\n        if self.require_task and not isinstance(self.task,\n                                                ChipClassificationConfig):\n            raise rv.ConfigError(\'Task set with with_task must be of type \'\n                                 \'ChipClassificationConfig, got {}.\'.format(\n                                     type(self.task)))\n\n    def build(self):\n        """"""Build this configuration.""""""\n        self.validate()\n\n        b = deepcopy(self)\n\n        for config_mod, ignore_missing_keys, set_missing_keys in b.config_mods:\n            try:\n                set_nested_keys(b.config[\'kc_config\'], config_mod,\n                                ignore_missing_keys, set_missing_keys)\n            except Exception as e:\n                raise rv.ConfigError(\n                    \'Error setting configuration {}\'.format(config_mod)) from e\n\n        return KerasClassificationConfig(**b.config)\n\n    def _applicable_tasks(self):\n        return [rv.CHIP_CLASSIFICATION]\n\n    def _process_task(self):\n        return self.with_config(\n            {\n                \'model\': {\n                    \'inputSize\': self.task.chip_size\n                },\n                \'trainer\': {\n                    \'options\': {\n                        \'classNames\': self.task.class_map.get_class_names(),\n                        \'inputSize\': self.task.chip_size\n                    }\n                }\n            },\n            set_missing_keys=True)\n\n    def _load_model_defaults(self, model_defaults):\n        """"""Loads defaults.\n\n        Expected keys are ""pretrained_model_uri"" and ""pipeline_config_uri"",\n        neither of which is required.\n        """"""\n        expected_keys = [\'pretrained_model_uri\', \'kc_config\']\n        unknown_keys = set(model_defaults.keys()) - set(expected_keys)\n        if unknown_keys:\n            raise rv.ConfigError(\'Unexpected keys in model defaults:\'\n                                 \' {}. Expected keys: {}\'.format(\n                                     unknown_keys, expected_keys))\n\n        b = self\n        if \'pretrained_model_uri\' in model_defaults:\n            b = b.with_pretrained_model(model_defaults[\'pretrained_model_uri\'])\n        if \'kc_config\' in model_defaults:\n            b = b.with_template(model_defaults[\'kc_config\'])\n        return b\n\n    def with_template(self, template):\n        """"""Use a template as the base for configuring Keras Classification.\n\n        Args:\n            template: dict, string or uri\n        """"""\n        template_json = None\n        if type(template) is dict:\n            msg = json_format.ParseDict(template, PipelineConfig())\n\n            template_json = json_format.MessageToDict(msg)\n        else:\n            # Try parsing the string as a message, on fail assume it\'s a URI\n            msg = None\n            try:\n                msg = json_format.Parse(template, PipelineConfig())\n            except json_format.ParseError:\n                msg = json_format.Parse(\n                    file_to_str(template), PipelineConfig())\n            template_json = json_format.MessageToDict(msg)\n\n        b = deepcopy(self)\n        b.config[\'kc_config\'] = template_json\n        return b\n\n    def with_batch_size(self, batch_size):\n        """"""Sets the training batch size.""""""\n        return self.with_config({\n            \'trainer\': {\n                \'options\': {\n                    \'batchSize\': batch_size\n                }\n            }\n        })\n\n    def with_num_epochs(self, num_epochs):\n        """"""Sets the number of training epochs.""""""\n        return self.with_config({\n            \'trainer\': {\n                \'options\': {\n                    \'nbEpochs\': num_epochs\n                }\n            }\n        })\n\n    def with_config(self,\n                    config_mod,\n                    ignore_missing_keys=False,\n                    set_missing_keys=False):\n        """"""Modify the backend configuration.\n\n        Given a dict, modify the tensorflow pipeline configuration\n        such that keys that are found recursively in the configuration\n        are replaced with those values. TODO: better explanation.\n        """"""\n        b = deepcopy(self)\n        b.config_mods.append((config_mod, ignore_missing_keys,\n                              set_missing_keys))\n        return b\n\n    def with_debug(self, debug):\n        """"""Sets the debug flag for this backend.\n        """"""\n        b = deepcopy(self)\n        b.config[\'debug\'] = debug\n        return b\n\n    def with_training_data_uri(self, training_data_uri):\n        """"""Whence comes the training data?\n\n        Args:\n            training_data_uri: The location of the training data.\n        """"""\n        b = deepcopy(self)\n        b.config[\'training_data_uri\'] = training_data_uri\n        return b\n\n    def with_training_output_uri(self, training_output_uri):\n        """"""Whither goes the training output?\n\n        Args:\n            training_output_uri: The location where the training output will\n                be stored.\n        """"""\n        b = deepcopy(self)\n        b.config[\'training_output_uri\'] = training_output_uri\n        return b\n\n    def with_model_uri(self, model_uri):\n        """"""Sets the filename of the trained model.""""""\n        b = deepcopy(self)\n        b.config[\'model_uri\'] = model_uri\n        return b\n\n    def with_train_options(self,\n                           sync_interval=600,\n                           do_monitoring=True,\n                           replace_model=False):\n        """"""Sets the train options for this backend.\n\n        Args:\n            sync_interval: How often to sync output of training to\n                the cloud (in seconds).\n\n            do_monitoring: Run process to monitor training (eg. Tensorboard)\n\n            replace_model: Replace the model checkpoint if exists.\n                If false, this will continue training from the checkpoint\n                if it exists, if the backend allows for this.\n        """"""\n        b = deepcopy(self)\n        b.config[\'train_options\'] = KerasClassificationConfig.TrainOptions(\n            sync_interval, do_monitoring, replace_model)\n\n        return b\n'"
rastervision/backend/pytorch_chip_classification.py,10,"b'from os.path import (join, isfile)\nimport uuid\nimport zipfile\nimport glob\nimport logging\nimport json\nfrom subprocess import Popen\nimport os\nimport csv\nimport time\nimport datetime\n\nimport matplotlib\nmatplotlib.use(\'Agg\')  # noqa\nimport matplotlib.pyplot as plt\nimport torch\nfrom torch import optim\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torch.optim.lr_scheduler import CyclicLR\n\nfrom rastervision.utils.files import (get_local_path, make_dir, upload_or_copy,\n                                      list_paths, download_if_needed,\n                                      sync_from_dir, sync_to_dir, str_to_file,\n                                      zipdir, file_to_json, json_to_file)\nfrom rastervision.utils.misc import save_img\nfrom rastervision.backend import Backend\nfrom rastervision.data.label import ChipClassificationLabels\nfrom rastervision.utils.misc import terminate_at_exit\nfrom rastervision.backend.torch_utils.chip_classification.plot import plot_xy\nfrom rastervision.backend.torch_utils.chip_classification.data import build_databunch\nfrom rastervision.backend.torch_utils.chip_classification.train import (\n    train_epoch, validate_epoch)\nfrom rastervision.backend.torch_utils.chip_classification.model import (\n    get_model)\n\nlog = logging.getLogger(__name__)\n\n\ndef make_debug_chips(databunch, class_map, tmp_dir, train_uri, max_count=30):\n    """"""Save debug chips for a Databunch for a chip classification dataset\n\n    This saves a plot for each example in the training and validation sets into\n    train-debug-chips.zip and valid-debug-chips.zip under the train_uri. This\n    is useful for making sure we are feeding correct data into the model.\n\n    Args:\n        databunch: DataBunch for chip classification\n        class_map: (rv.ClassMap) class map used to map class ids to colors\n        tmp_dir: (str) path to temp directory\n        train_uri: (str) URI of root of training output\n        max_count: (int) maximum number of chips to generate. If None,\n            generates all of them.\n    """"""\n\n    def _make_debug_chips(split):\n        debug_chips_dir = join(tmp_dir, \'{}-debug-chips\'.format(split))\n        zip_path = join(tmp_dir, \'{}-debug-chips.zip\'.format(split))\n        zip_uri = join(train_uri, \'{}-debug-chips.zip\'.format(split))\n        make_dir(debug_chips_dir)\n        ds = databunch.train_ds if split == \'train\' else databunch.valid_ds\n        for i, (x, y) in enumerate(ds):\n            if i >= max_count:\n                break\n\n            fig, ax = plt.subplots(1)\n            plot_xy(ax, x, y, databunch.label_names)\n            plt.savefig(\n                join(debug_chips_dir, \'{}.png\'.format(i)), figsize=(6, 6))\n            plt.close()\n\n        zipdir(debug_chips_dir, zip_path)\n        upload_or_copy(zip_path, zip_uri)\n\n    _make_debug_chips(\'train\')\n    _make_debug_chips(\'valid\')\n\n\nclass PyTorchChipClassification(Backend):\n    """"""Chip classification backend using PyTorch and fastai.""""""\n\n    def __init__(self, task_config, backend_opts, train_opts):\n        """"""Constructor.\n\n        Args:\n            task_config: (ChipClassificationConfig)\n            backend_opts: (simple_backend_config.BackendOptions)\n            train_opts: (pytorch_chip_classification_config.TrainOptions)\n        """"""\n        self.task_config = task_config\n        self.backend_opts = backend_opts\n        self.train_opts = train_opts\n        self.inf_learner = None\n\n        torch_cache_dir = \'/opt/data/torch-cache\'\n        os.environ[\'TORCH_HOME\'] = torch_cache_dir\n\n        self.model = None\n        self.device = \'cuda\' if torch.cuda.is_available() else \'cpu\'\n        log.info(\'Device = {}\'.format(self.device))\n\n    def log_options(self):\n        log.info(\'backend_opts:\\n\' +\n                 json.dumps(self.backend_opts.__dict__, indent=4))\n        log.info(\'train_opts:\\n\' +\n                 json.dumps(self.train_opts.__dict__, indent=4))\n\n    def process_scene_data(self, scene, data, tmp_dir):\n        """"""Make training chips for a scene.\n\n        This writes a set of image chips to {scene_id}/{class_name}/{scene_id}-{ind}.png\n\n        Args:\n            scene: (rv.data.Scene)\n            data: (rv.data.Dataset)\n            tmp_dir: (str) path to temp directory\n\n        Returns:\n            (str) path to directory with scene chips {tmp_dir}/{scene_id}\n        """"""\n        scene_dir = join(tmp_dir, str(scene.id))\n\n        for ind, (chip, window, labels) in enumerate(data):\n            class_id = labels.get_cell_class_id(window)\n            # If a chip is not associated with a class, don\'t\n            # use it in training data.\n            if class_id is None:\n                continue\n\n            class_name = self.task_config.class_map.get_by_id(class_id).name\n            class_dir = join(scene_dir, class_name)\n            make_dir(class_dir)\n            chip_path = join(class_dir, \'{}-{}.png\'.format(scene.id, ind))\n            save_img(chip, chip_path)\n\n        return scene_dir\n\n    def process_sceneset_results(self, training_results, validation_results,\n                                 tmp_dir):\n        """"""Write zip file with chips for a set of scenes.\n\n        This writes a zip file for a group of scenes at {chip_uri}/{uuid}.zip containing:\n        train-img/{class_name}/{scene_id}-{ind}.png\n        valid-img/{class_name}/{scene_id}-{ind}.png\n\n        This method is called once per instance of the chip command.\n        A number of instances of the chip command can run simultaneously to\n        process chips in parallel. The uuid in the path above is what allows\n        separate instances to avoid overwriting each others\' output.\n\n        Args:\n            training_results: list of directories generated by process_scene_data\n                that all hold training chips\n            validation_results: list of directories generated by process_scene_data\n                that all hold validation chips\n        """"""\n        self.log_options()\n\n        group = str(uuid.uuid4())\n        group_uri = join(self.backend_opts.chip_uri, \'{}.zip\'.format(group))\n        group_path = get_local_path(group_uri, tmp_dir)\n        make_dir(group_path, use_dirname=True)\n\n        with zipfile.ZipFile(group_path, \'w\', zipfile.ZIP_DEFLATED) as zipf:\n\n            def _write_zip(scene_dirs, split):\n                for scene_dir in scene_dirs:\n                    scene_paths = glob.glob(join(scene_dir, \'**/*.png\'))\n                    for path in scene_paths:\n                        class_name, fn = path.split(\'/\')[-2:]\n                        zipf.write(path, join(split, class_name, fn))\n\n            _write_zip(training_results, \'train\')\n            _write_zip(validation_results, \'valid\')\n\n        upload_or_copy(group_path, group_uri)\n\n    def train(self, tmp_dir):\n        """"""Train a model.\n\n        This downloads any previous output saved to the train_uri,\n        starts training (or resumes from a checkpoint), periodically\n        syncs contents of train_dir to train_uri and after training finishes.\n\n        Args:\n            tmp_dir: (str) path to temp directory\n        """"""\n        self.log_options()\n\n        # Sync output of previous training run from cloud.\n        train_uri = self.backend_opts.train_uri\n        train_dir = get_local_path(train_uri, tmp_dir)\n        make_dir(train_dir)\n        sync_from_dir(train_uri, train_dir)\n\n        # Get zip file for each group, and unzip them into chip_dir.\n        chip_dir = join(tmp_dir, \'chips\')\n        make_dir(chip_dir)\n        for zip_uri in list_paths(self.backend_opts.chip_uri, \'zip\'):\n            zip_path = download_if_needed(zip_uri, tmp_dir)\n            with zipfile.ZipFile(zip_path, \'r\') as zipf:\n                zipf.extractall(chip_dir)\n\n        # Setup data loader.\n        batch_size = self.train_opts.batch_size\n        chip_size = self.task_config.chip_size\n        augmentors = self.train_opts.augmentors\n        databunch = build_databunch(\n            chip_dir, chip_size, batch_size,\n            self.task_config.class_map.get_class_names(),\n            self.train_opts.rare_classes, self.train_opts.desired_prob,\n            augmentors)\n        log.info(databunch)\n        num_labels = len(databunch.label_names)\n        if self.train_opts.debug:\n            make_debug_chips(databunch, self.task_config.class_map, tmp_dir,\n                             train_uri)\n\n        # Setup model\n        num_labels = len(databunch.label_names)\n        model = get_model(\n            self.train_opts.model_arch, num_labels, pretrained=True)\n        model = model.to(self.device)\n        model_path = join(train_dir, \'model\')\n\n        # Load weights from a pretrained model.\n        pretrained_uri = self.backend_opts.pretrained_uri\n        if pretrained_uri:\n            log.info(\'Loading weights from pretrained_uri: {}\'.format(\n                pretrained_uri))\n            pretrained_path = download_if_needed(pretrained_uri, tmp_dir)\n            model.load_state_dict(\n                torch.load(pretrained_path, map_location=self.device))\n\n        # Possibly resume training from checkpoint.\n        start_epoch = 0\n        train_state_path = join(train_dir, \'train_state.json\')\n        if isfile(train_state_path):\n            log.info(\'Resuming from checkpoint: {}\\n\'.format(model_path))\n            train_state = file_to_json(train_state_path)\n            start_epoch = train_state[\'epoch\'] + 1\n            model.load_state_dict(\n                torch.load(model_path, map_location=self.device))\n\n        # Write header of log CSV file.\n        metric_names = [\'precision\', \'recall\', \'f1\']\n        log_path = join(train_dir, \'log.csv\')\n        if not isfile(log_path):\n            with open(log_path, \'w\') as log_file:\n                log_writer = csv.writer(log_file)\n                row = [\'epoch\', \'time\', \'train_loss\'] + metric_names\n                log_writer.writerow(row)\n\n        # Setup Tensorboard logging.\n        if self.train_opts.log_tensorboard:\n            log_dir = join(train_dir, \'tb-logs\')\n            make_dir(log_dir)\n            tb_writer = SummaryWriter(log_dir=log_dir)\n            if self.train_opts.run_tensorboard:\n                log.info(\'Starting tensorboard process\')\n                tensorboard_process = Popen(\n                    [\'tensorboard\', \'--logdir={}\'.format(log_dir)])\n                terminate_at_exit(tensorboard_process)\n\n        # Setup optimizer, loss, and LR scheduler.\n        loss_fn = torch.nn.CrossEntropyLoss()\n        lr = self.train_opts.lr\n        opt = optim.Adam(model.parameters(), lr=lr)\n        step_scheduler, epoch_scheduler = None, None\n        num_epochs = self.train_opts.num_epochs\n\n        if self.train_opts.one_cycle and num_epochs > 1:\n            steps_per_epoch = len(databunch.train_ds) // batch_size\n            total_steps = num_epochs * steps_per_epoch\n            step_size_up = (num_epochs // 2) * steps_per_epoch\n            step_size_down = total_steps - step_size_up\n            step_scheduler = CyclicLR(\n                opt,\n                base_lr=lr / 10,\n                max_lr=lr,\n                step_size_up=step_size_up,\n                step_size_down=step_size_down,\n                cycle_momentum=False)\n            for _ in range(start_epoch * steps_per_epoch):\n                step_scheduler.step()\n\n        # Training loop.\n        for epoch in range(start_epoch, num_epochs):\n            # Train one epoch.\n            log.info(\'-----------------------------------------------------\')\n            log.info(\'epoch: {}\'.format(epoch))\n            start = time.time()\n            train_loss = train_epoch(model, self.device, databunch.train_dl,\n                                     opt, loss_fn, step_scheduler)\n            if epoch_scheduler:\n                epoch_scheduler.step()\n            log.info(\'train loss: {}\'.format(train_loss))\n\n            # Validate one epoch.\n            metrics = validate_epoch(model, self.device, databunch.valid_dl,\n                                     num_labels)\n            log.info(\'validation metrics: {}\'.format(metrics))\n\n            # Print elapsed time for epoch.\n            end = time.time()\n            epoch_time = datetime.timedelta(seconds=end - start)\n            log.info(\'epoch elapsed time: {}\'.format(epoch_time))\n\n            # Save model and state.\n            torch.save(model.state_dict(), model_path)\n            train_state = {\'epoch\': epoch}\n            json_to_file(train_state, train_state_path)\n\n            # Append to log CSV file.\n            with open(log_path, \'a\') as log_file:\n                log_writer = csv.writer(log_file)\n                row = [epoch, epoch_time, train_loss]\n                row += [metrics[k] for k in metric_names]\n                log_writer.writerow(row)\n\n            # Write to Tensorboard log.\n            if self.train_opts.log_tensorboard:\n                for key, val in metrics.items():\n                    tb_writer.add_scalar(key, val, epoch)\n                tb_writer.add_scalar(\'train_loss\', train_loss, epoch)\n                for name, param in model.named_parameters():\n                    tb_writer.add_histogram(name, param, epoch)\n\n            if (train_uri.startswith(\'s3://\')\n                    and (((epoch + 1) % self.train_opts.sync_interval) == 0)):\n                sync_to_dir(train_dir, train_uri)\n\n        # Close Tensorboard.\n        if self.train_opts.log_tensorboard:\n            tb_writer.close()\n            if self.train_opts.run_tensorboard:\n                tensorboard_process.terminate()\n\n        # Mark that the command has completed.\n        str_to_file(\'done!\', self.backend_opts.train_done_uri)\n\n        # Sync output to cloud.\n        sync_to_dir(train_dir, self.backend_opts.train_uri)\n\n    def load_model(self, tmp_dir):\n        """"""Load the model in preparation for one or more prediction calls.""""""\n        if self.model is None:\n            model_uri = self.backend_opts.model_uri\n            model_path = download_if_needed(model_uri, tmp_dir)\n\n            num_classes = len(self.task_config.class_map)\n            model = get_model(\n                self.train_opts.model_arch, num_classes, pretrained=False)\n            model = model.to(self.device)\n            model.load_state_dict(\n                torch.load(model_path, map_location=self.device))\n            self.model = model\n\n    def predict(self, chips, windows, tmp_dir):\n        """"""Return predictions for a batch of chips.\n\n        Args:\n            chips: (numpy.ndarray) of shape (n, height, width, nb_channels)\n                containing a batch of chips\n            windows: (List<Box>) windows that are aligned with the chips which\n                are aligned with the chips.\n\n        Return:\n            (ChipClassificationLabels) containing predictions\n        """"""\n        self.load_model(tmp_dir)\n\n        # (batch_size, h, w, nchannels) --> (batch_size, nchannels, h, w)\n        chips = torch.Tensor(chips).permute((0, 3, 1, 2)) / 255.\n        chips = chips.to(self.device)\n        model = self.model.eval()\n\n        with torch.no_grad():\n            out = model(chips).cpu()\n            labels = ChipClassificationLabels()\n\n            for class_probs, window in zip(out, windows):\n                # Add 1 to class_id since they start at 1.\n                class_id = int(class_probs.argmax() + 1)\n                labels.set_cell(window, class_id, class_probs.numpy())\n\n        return labels\n'"
rastervision/backend/pytorch_chip_classification_config.py,0,"b'from copy import deepcopy\n\nimport rastervision as rv\n\nfrom rastervision.backend.pytorch_chip_classification import (\n    PyTorchChipClassification)\nfrom rastervision.backend.simple_backend_config import (\n    SimpleBackendConfig, SimpleBackendConfigBuilder)\nfrom rastervision.backend.api import PYTORCH_CHIP_CLASSIFICATION\n\n\nclass TrainOptions():\n    def __init__(self,\n                 batch_size=None,\n                 lr=None,\n                 one_cycle=None,\n                 num_epochs=None,\n                 model_arch=None,\n                 sync_interval=None,\n                 debug=None,\n                 log_tensorboard=None,\n                 run_tensorboard=None,\n                 rare_classes=None,\n                 desired_prob=None,\n                 augmentors=[]):\n        self.batch_size = batch_size\n        self.lr = lr\n        self.one_cycle = one_cycle\n        self.num_epochs = num_epochs\n        self.model_arch = model_arch\n        self.sync_interval = sync_interval\n        self.debug = debug\n        self.log_tensorboard = log_tensorboard\n        self.run_tensorboard = run_tensorboard\n        self.rare_classes = rare_classes\n        self.desired_prob = desired_prob\n        self.augmentors = augmentors\n\n    def __setattr__(self, name, value):\n        if name in [\'batch_size\', \'num_epochs\', \'sync_interval\']:\n            value = int(value) if isinstance(value, float) else value\n        super().__setattr__(name, value)\n\n\nclass PyTorchChipClassificationConfig(SimpleBackendConfig):\n    train_opts_class = TrainOptions\n    backend_type = PYTORCH_CHIP_CLASSIFICATION\n    backend_class = PyTorchChipClassification\n\n\nclass PyTorchChipClassificationConfigBuilder(SimpleBackendConfigBuilder):\n    config_class = PyTorchChipClassificationConfig\n\n    def _applicable_tasks(self):\n        return [rv.CHIP_CLASSIFICATION]\n\n    def with_train_options(self,\n                           batch_size=8,\n                           lr=1e-4,\n                           one_cycle=True,\n                           num_epochs=1,\n                           model_arch=\'resnet18\',\n                           sync_interval=1,\n                           debug=False,\n                           log_tensorboard=True,\n                           run_tensorboard=True,\n                           rare_classes=[],\n                           desired_prob=None,\n                           augmentors=[]):\n        """"""Set options for training models.\n\n        Args:\n            batch_size: (int) the batch size\n            weight_decay: (float) the weight decay\n            lr: (float) the learning rate if using a fixed LR\n                (ie. one_cycle is False),\n                or the maximum LR to use if one_cycle is True\n            one_cycle: (bool) True if cyclic learning rate scheduler should\n                be used. This\n                cycles the LR once during the course of training and seems to\n                result in a pretty consistent improvement. See lr for more\n                details.\n            num_epochs: (int) number of epochs (sweeps through training set) to\n                train model for\n            model_arch: (str) Any classification model option in\n                torchvision.models is valid, for example, resnet18.\n            sync_interval: (int) sync training directory to cloud every\n                sync_interval epochs.\n            debug: (bool) if True, save debug chips (ie. visualizations of\n                input to model during training) during training and use\n                single-core for creating minibatches.\n            log_tensorboard: (bool) if True, write events to Tensorboard log\n                file\n            run_tensorboard: (bool) if True, run a Tensorboard server at\n                port 6006 that uses the logs generated by the log_tensorboard\n                option\n            rare_classes: (list) of integers with class indices that should be\n                oversampled during the training. The goal is to reduce the effect\n                of severe class imbalance influencing training.\n            desired_prob: (float) when a list of rare classes is given, a single\n                float can be given (between 0.0 and 1.0) indicating the desired\n                probability of the rare classes. If e.g. set to 0.5, the change of\n                drawing any rare class sample is 0.5.\n            augmentors: (list of str) any of [\'Blur\', \'RandomRotate90\', \'HorizontalFlip\',\n                \'VerticalFlip\', \'GaussianBlur\', or \'GaussNoise\', \'RGBShift\', \'ToGray\'].\n                These use the default settings for each of the transforms in\n                https://albumentations.readthedocs.io\n        """"""\n        b = deepcopy(self)\n        b.train_opts = TrainOptions(\n            batch_size=batch_size,\n            lr=lr,\n            one_cycle=one_cycle,\n            num_epochs=num_epochs,\n            model_arch=model_arch,\n            sync_interval=sync_interval,\n            debug=debug,\n            log_tensorboard=log_tensorboard,\n            run_tensorboard=run_tensorboard,\n            rare_classes=rare_classes,\n            desired_prob=desired_prob,\n            augmentors=augmentors)\n        return b\n\n    def with_pretrained_uri(self, pretrained_uri):\n        """"""pretrained_uri should be uri of exported model file.""""""\n        return super().with_pretrained_uri(pretrained_uri)\n'"
rastervision/backend/pytorch_object_detection.py,9,"b'import os\nfrom os.path import join, basename, isfile\nimport uuid\nimport zipfile\nimport glob\nimport logging\nimport json\nimport csv\nimport time\nimport datetime\nfrom subprocess import Popen\n\nimport matplotlib\nmatplotlib.use(\'Agg\')  # noqa\nimport matplotlib.pyplot as plt\nimport torch\nfrom torch import optim\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torch.optim.lr_scheduler import CyclicLR\n\nfrom rastervision.utils.files import (get_local_path, make_dir, upload_or_copy,\n                                      list_paths, download_if_needed,\n                                      sync_from_dir, sync_to_dir, str_to_file,\n                                      json_to_file, file_to_json, zipdir)\nfrom rastervision.utils.misc import save_img, terminate_at_exit\nfrom rastervision.backend import Backend\nfrom rastervision.data import ObjectDetectionLabels\nfrom rastervision.backend.torch_utils.object_detection.data import build_databunch\nfrom rastervision.backend.torch_utils.object_detection.plot import plot_xy\nfrom rastervision.backend.torch_utils.object_detection.model import MyFasterRCNN\nfrom rastervision.backend.torch_utils.object_detection.train import (\n    train_epoch, validate_epoch)\n\nlog = logging.getLogger(__name__)\n\n\ndef make_debug_chips(databunch, class_map, tmp_dir, train_uri, max_count=30):\n    """"""Save debug chips for a DataBunch.\n\n    This saves a plot for each example in the training and validation sets into\n    train-debug-chips.zip and valid-debug-chips.zip under the train_uri. This\n    is useful for making sure we are feeding correct data into the model.\n\n    Args:\n        data: DataBunch for an object detection problem\n        class_map: (rv.ClassMap) class map used to map class ids to colors\n        tmp_dir: (str) path to temp directory\n        train_uri: (str) URI of root of training output\n        max_count: (int) maximum number of chips to generate. If None,\n            generates all of them.\n    """"""\n\n    def _make_debug_chips(split):\n        debug_chips_dir = join(tmp_dir, \'{}-debug-chips\'.format(split))\n        zip_path = join(tmp_dir, \'{}-debug-chips.zip\'.format(split))\n        zip_uri = join(train_uri, \'{}-debug-chips.zip\'.format(split))\n        make_dir(debug_chips_dir)\n        ds = databunch.train_ds if split == \'train\' else databunch.valid_ds\n        for i, (x, y) in enumerate(ds):\n            if i >= max_count:\n                break\n\n            fig, ax = plt.subplots(1)\n            plot_xy(ax, x, y, ds.label_names)\n            plt.savefig(\n                join(debug_chips_dir, \'{}.png\'.format(i)), figsize=(6, 6))\n            plt.close()\n\n        zipdir(debug_chips_dir, zip_path)\n        upload_or_copy(zip_path, zip_uri)\n\n    _make_debug_chips(\'train\')\n    _make_debug_chips(\'val\')\n\n\nclass PyTorchObjectDetection(Backend):\n    """"""Object detection using PyTorch and Faster-RCNN/Resnet50 from torchvision.""""""\n\n    def __init__(self, task_config, backend_opts, train_opts):\n        """"""Constructor.\n\n        Args:\n            task_config: (ChipClassificationConfig)\n            backend_opts: (simple_backend_config.BackendOptions)\n            train_opts: (pytorch_chip_classification_config.TrainOptions)\n        """"""\n        self.task_config = task_config\n        self.backend_opts = backend_opts\n        self.train_opts = train_opts\n        self.inf_learner = None\n\n        # Setup caching for torchvision pretrained models.\n        torch_cache_dir = \'/opt/data/torch-cache\'\n        os.environ[\'TORCH_HOME\'] = torch_cache_dir\n\n        self.model = None\n        self.device = \'cuda\' if torch.cuda.is_available() else \'cpu\'\n        log.info(\'Device = {}\'.format(self.device))\n\n    def log_options(self):\n        log.info(\'backend_opts:\\n\' +\n                 json.dumps(self.backend_opts.__dict__, indent=4))\n        log.info(\'train_opts:\\n\' +\n                 json.dumps(self.train_opts.__dict__, indent=4))\n\n    def process_scene_data(self, scene, data, tmp_dir):\n        """"""Process each scene\'s training data.\n\n        This writes {scene_id}/{scene_id}-{ind}.png and\n        {scene_id}/{scene_id}-labels.json in COCO format.\n\n        Args:\n            scene: Scene\n            data: TrainingData\n\n        Returns:\n            backend-specific data-structures consumed by backend\'s\n            process_sceneset_results\n        """"""\n        scene_dir = join(tmp_dir, str(scene.id))\n        labels_path = join(scene_dir, \'{}-labels.json\'.format(scene.id))\n\n        make_dir(scene_dir)\n        images = []\n        annotations = []\n        categories = [{\n            \'id\': item.id,\n            \'name\': item.name\n        } for item in self.task_config.class_map.get_items()]\n\n        for im_ind, (chip, window, labels) in enumerate(data):\n            im_id = \'{}-{}\'.format(scene.id, im_ind)\n            fn = \'{}.png\'.format(im_id)\n            chip_path = join(scene_dir, fn)\n            save_img(chip, chip_path)\n            images.append({\n                \'file_name\': fn,\n                \'id\': im_id,\n                \'height\': chip.shape[0],\n                \'width\': chip.shape[1]\n            })\n\n            npboxes = labels.get_npboxes()\n            npboxes = ObjectDetectionLabels.global_to_local(npboxes, window)\n            for box_ind, (box, class_id) in enumerate(\n                    zip(npboxes, labels.get_class_ids())):\n                bbox = [box[1], box[0], box[3] - box[1], box[2] - box[0]]\n                bbox = [int(i) for i in bbox]\n                annotations.append({\n                    \'id\': \'{}-{}\'.format(im_id, box_ind),\n                    \'image_id\': im_id,\n                    \'bbox\': bbox,\n                    \'category_id\': int(class_id)\n                })\n\n        coco_dict = {\n            \'images\': images,\n            \'annotations\': annotations,\n            \'categories\': categories\n        }\n        json_to_file(coco_dict, labels_path)\n\n        return scene_dir\n\n    def process_sceneset_results(self, training_results, validation_results,\n                                 tmp_dir):\n        """"""After all scenes have been processed, process the result set.\n\n        This writes a zip file for a group of scenes at {chip_uri}/{uuid}.zip\n        containing:\n        train/{scene_id}-{ind}.png\n        train/{scene_id}-labels.json\n        valid/{scene_id}-{ind}.png\n        valid/{scene_id}-labels.json\n\n        Args:\n            training_results: dependent on the ml_backend\'s process_scene_data\n            validation_results: dependent on the ml_backend\'s\n                process_scene_data\n        """"""\n        self.log_options()\n\n        group = str(uuid.uuid4())\n        group_uri = join(self.backend_opts.chip_uri, \'{}.zip\'.format(group))\n        group_path = get_local_path(group_uri, tmp_dir)\n        make_dir(group_path, use_dirname=True)\n\n        with zipfile.ZipFile(group_path, \'w\', zipfile.ZIP_DEFLATED) as zipf:\n\n            def _write_zip(results, split):\n                for scene_dir in results:\n                    scene_paths = glob.glob(join(scene_dir, \'*\'))\n                    for p in scene_paths:\n                        zipf.write(p, join(split, basename(p)))\n\n            _write_zip(training_results, \'train\')\n            _write_zip(validation_results, \'valid\')\n\n        upload_or_copy(group_path, group_uri)\n\n    def train(self, tmp_dir):\n        """"""Train a model.\n\n        This downloads any previous output saved to the train_uri,\n        starts training (or resumes from a checkpoint), periodically\n        syncs contents of train_dir to train_uri and after training finishes.\n\n        Args:\n            tmp_dir: (str) path to temp directory\n        """"""\n        self.log_options()\n\n        # Sync output of previous training run from cloud.\n        train_uri = self.backend_opts.train_uri\n        train_dir = get_local_path(train_uri, tmp_dir)\n        make_dir(train_dir)\n        sync_from_dir(train_uri, train_dir)\n\n        # Get zip file for each group, and unzip them into chip_dir.\n        chip_dir = join(tmp_dir, \'chips\')\n        make_dir(chip_dir)\n        for zip_uri in list_paths(self.backend_opts.chip_uri, \'zip\'):\n            zip_path = download_if_needed(zip_uri, tmp_dir)\n            with zipfile.ZipFile(zip_path, \'r\') as zipf:\n                zipf.extractall(chip_dir)\n\n        # Setup dataset and dataloaders.\n        batch_size = self.train_opts.batch_size\n        chip_size = self.task_config.chip_size\n        databunch = build_databunch(chip_dir, chip_size, batch_size)\n        log.info(databunch)\n        num_labels = len(databunch.label_names)\n        if self.train_opts.debug:\n            make_debug_chips(databunch, self.task_config.class_map, tmp_dir,\n                             train_uri)\n\n        # Setup model\n        num_labels = len(databunch.label_names)\n        model = MyFasterRCNN(\n            self.train_opts.model_arch, num_labels, chip_size, pretrained=True)\n        model = model.to(self.device)\n        model_path = join(train_dir, \'model\')\n\n        # Load weights from a pretrained model.\n        pretrained_uri = self.backend_opts.pretrained_uri\n        if pretrained_uri:\n            log.info(\'Loading weights from pretrained_uri: {}\'.format(\n                pretrained_uri))\n            pretrained_path = download_if_needed(pretrained_uri, tmp_dir)\n            model.load_state_dict(\n                torch.load(pretrained_path, map_location=self.device))\n\n        # Possibly resume training from checkpoint.\n        start_epoch = 0\n        train_state_path = join(train_dir, \'train_state.json\')\n        if isfile(train_state_path):\n            log.info(\'Resuming from checkpoint: {}\\n\'.format(model_path))\n            train_state = file_to_json(train_state_path)\n            start_epoch = train_state[\'epoch\'] + 1\n            model.load_state_dict(\n                torch.load(model_path, map_location=self.device))\n\n        # Write header of log CSV file.\n        log_path = join(train_dir, \'log.csv\')\n        if not isfile(log_path):\n            with open(log_path, \'w\') as log_file:\n                log_writer = csv.writer(log_file)\n                row = [\'epoch\'] + [\'map50\', \'time\'] + model.subloss_names\n                log_writer.writerow(row)\n\n        # Setup Tensorboard logging.\n        if self.train_opts.log_tensorboard:\n            log_dir = join(train_dir, \'tb-logs\')\n            make_dir(log_dir)\n            tb_writer = SummaryWriter(log_dir=log_dir)\n            if self.train_opts.run_tensorboard:\n                log.info(\'Starting tensorboard process\')\n                tensorboard_process = Popen(\n                    [\'tensorboard\', \'--logdir={}\'.format(log_dir)])\n                terminate_at_exit(tensorboard_process)\n\n        # Setup optimizer.\n        lr = self.train_opts.lr\n        opt = optim.Adam(model.parameters(), lr=lr)\n        step_scheduler, epoch_scheduler = None, None\n        num_epochs = self.train_opts.num_epochs\n\n        if self.train_opts.one_cycle and num_epochs > 1:\n            steps_per_epoch = len(databunch.train_ds) // batch_size\n            total_steps = num_epochs * steps_per_epoch\n            step_size_up = (num_epochs // 2) * steps_per_epoch\n            step_size_down = total_steps - step_size_up\n            step_scheduler = CyclicLR(\n                opt,\n                base_lr=lr / 10,\n                max_lr=lr,\n                step_size_up=step_size_up,\n                step_size_down=step_size_down,\n                cycle_momentum=False)\n            for _ in range(start_epoch * steps_per_epoch):\n                step_scheduler.step()\n\n        # Training loop.\n        for epoch in range(start_epoch, num_epochs):\n            # Train one epoch.\n            log.info(\'-----------------------------------------------------\')\n            log.info(\'epoch: {}\'.format(epoch))\n            start = time.time()\n            train_loss = train_epoch(model, self.device, databunch.train_dl,\n                                     opt, step_scheduler, epoch_scheduler)\n            if epoch_scheduler:\n                epoch_scheduler.step()\n            log.info(\'train loss: {}\'.format(train_loss))\n\n            # Validate one epoch.\n            metrics = validate_epoch(model, self.device, databunch.valid_dl,\n                                     num_labels)\n            log.info(\'validation metrics: {}\'.format(metrics))\n\n            # Print elapsed time for epoch.\n            end = time.time()\n            epoch_time = datetime.timedelta(seconds=end - start)\n            log.info(\'epoch elapsed time: {}\'.format(epoch_time))\n\n            # Save model and state.\n            torch.save(model.state_dict(), model_path)\n            train_state = {\'epoch\': epoch}\n            json_to_file(train_state, train_state_path)\n\n            # Append to log CSV file.\n            with open(log_path, \'a\') as log_file:\n                log_writer = csv.writer(log_file)\n                row = [epoch]\n                row += [metrics[\'map50\'], epoch_time]\n                row += [train_loss[k] for k in model.subloss_names]\n                log_writer.writerow(row)\n\n            # Write to Tensorboard log.\n            if self.train_opts.log_tensorboard:\n                for key, val in metrics.items():\n                    tb_writer.add_scalar(key, val, epoch)\n                for key, val in train_loss.items():\n                    tb_writer.add_scalar(key, val, epoch)\n                for name, param in model.named_parameters():\n                    tb_writer.add_histogram(name, param, epoch)\n\n            if (train_uri.startswith(\'s3://\')\n                    and (((epoch + 1) % self.train_opts.sync_interval) == 0)):\n                sync_to_dir(train_dir, train_uri)\n\n        # Close Tensorboard.\n        if self.train_opts.log_tensorboard:\n            tb_writer.close()\n            if self.train_opts.run_tensorboard:\n                tensorboard_process.terminate()\n\n        # Mark that the command has completed.\n        str_to_file(\'done!\', self.backend_opts.train_done_uri)\n\n        # Sync output to cloud.\n        sync_to_dir(train_dir, self.backend_opts.train_uri)\n\n    def load_model(self, tmp_dir):\n        """"""Load the model in preparation for one or more prediction calls.""""""\n        if self.model is None:\n            model_uri = self.backend_opts.model_uri\n            model_path = download_if_needed(model_uri, tmp_dir)\n\n            # add one for background class\n            num_classes = len(self.task_config.class_map) + 1\n            model = MyFasterRCNN(\n                self.train_opts.model_arch,\n                num_classes,\n                self.task_config.chip_size,\n                pretrained=False)\n            model = model.to(self.device)\n            model.load_state_dict(\n                torch.load(model_path, map_location=self.device))\n            self.model = model\n\n    def predict(self, chips, windows, tmp_dir):\n        """"""Return predictions for a chip using model.\n\n        Args:\n            chips: [[height, width, channels], ...] numpy array of chips\n            windows: List of boxes that are the windows aligned with the chips.\n\n        Return:\n            Labels object containing predictions\n        """"""\n        self.load_model(tmp_dir)\n        labels = ObjectDetectionLabels.make_empty()\n        chips = torch.Tensor(chips).permute((0, 3, 1, 2)) / 255.\n        chips = chips.to(self.device)\n        model = self.model.eval()\n\n        with torch.no_grad():\n            output = model(chips)\n            for chip_ind, (chip, window) in enumerate(zip(chips, windows)):\n                boxlist = output[chip_ind].cpu()\n                boxes = boxlist.boxes.numpy()\n                class_ids = boxlist.get_field(\'labels\').numpy()\n                scores = boxlist.get_field(\'scores\').numpy()\n\n                boxes = ObjectDetectionLabels.local_to_global(boxes, window)\n                labels += ObjectDetectionLabels(\n                    boxes, class_ids, scores=scores)\n\n        return labels\n'"
rastervision/backend/pytorch_object_detection_config.py,0,"b'from copy import deepcopy\n\nimport rastervision as rv\n\nfrom rastervision.backend.pytorch_object_detection import (\n    PyTorchObjectDetection)\nfrom rastervision.backend.simple_backend_config import (\n    SimpleBackendConfig, SimpleBackendConfigBuilder)\nfrom rastervision.backend.api import PYTORCH_OBJECT_DETECTION\n\n\nclass TrainOptions():\n    def __init__(self,\n                 batch_size=None,\n                 lr=None,\n                 one_cycle=None,\n                 num_epochs=None,\n                 model_arch=None,\n                 sync_interval=None,\n                 log_tensorboard=None,\n                 run_tensorboard=None,\n                 debug=None):\n        self.batch_size = batch_size\n        self.lr = lr\n        self.one_cycle = one_cycle\n        self.num_epochs = num_epochs\n        self.model_arch = model_arch\n        self.sync_interval = sync_interval\n        self.log_tensorboard = log_tensorboard\n        self.run_tensorboard = run_tensorboard\n        self.debug = debug\n\n    def __setattr__(self, name, value):\n        if name in [\'batch_size\', \'num_epochs\', \'sync_interval\']:\n            value = int(value) if isinstance(value, float) else value\n        super().__setattr__(name, value)\n\n\nclass PyTorchObjectDetectionConfig(SimpleBackendConfig):\n    train_opts_class = TrainOptions\n    backend_type = PYTORCH_OBJECT_DETECTION\n    backend_class = PyTorchObjectDetection\n\n\nclass PyTorchObjectDetectionConfigBuilder(SimpleBackendConfigBuilder):\n    """"""Object detection using PyTorch and Faster-RCNN/Resnet50 from torchvision.""""""\n    config_class = PyTorchObjectDetectionConfig\n\n    def _applicable_tasks(self):\n        return [rv.OBJECT_DETECTION]\n\n    def with_train_options(self,\n                           batch_size=8,\n                           lr=1e-4,\n                           one_cycle=True,\n                           num_epochs=5,\n                           model_arch=\'resnet18\',\n                           sync_interval=1,\n                           log_tensorboard=True,\n                           run_tensorboard=True,\n                           debug=False):\n        """"""Set options for training models.\n\n        Args:\n            batch_size: (int) the batch size\n            lr: (float) the learning rate if using a fixed LR\n                (ie. one_cycle is False),\n                or the maximum LR to use if one_cycle is True\n            one_cycle: (bool) True if cyclic learning rate scheduler should\n                be used. This\n                cycles the LR once during the course of training and seems to\n                result in a pretty consistent improvement. See lr for more\n                details.\n            num_epochs: (int) number of epochs (sweeps through training set) to\n                train model for\n            model_arch: (str) classification model backbone to use.\n                Any Resnet option in torchvision.models is valid,\n                for example, resnet18.\n            sync_interval: (int) sync training directory to cloud every\n                sync_interval epochs.\n            log_tensorboard: (bool) if True, write events to Tensorboard log\n                file\n            run_tensorboard: (bool) if True, run a Tensorboard server at\n                port 6006 that uses the logs generated by the log_tensorboard\n                option\n            debug: (bool) if True, save debug chips (ie. visualizations of\n                input to model during training) during training and use\n                single-core for creating minibatches.\n        """"""\n        b = deepcopy(self)\n        b.train_opts = TrainOptions(\n            batch_size=batch_size,\n            lr=lr,\n            one_cycle=one_cycle,\n            num_epochs=num_epochs,\n            model_arch=model_arch,\n            sync_interval=sync_interval,\n            log_tensorboard=log_tensorboard,\n            run_tensorboard=run_tensorboard,\n            debug=debug)\n        return b\n\n    def with_pretrained_uri(self, pretrained_uri):\n        """"""pretrained_uri should be uri of exported model file.""""""\n        return super().with_pretrained_uri(pretrained_uri)\n'"
rastervision/backend/pytorch_semantic_segmentation.py,10,"b'from os.path import (join, isfile, basename, dirname)\nimport uuid\nimport zipfile\nimport glob\nimport logging\nimport json\nfrom subprocess import Popen\nimport os\nimport csv\nimport time\nimport datetime\n\nimport matplotlib\nmatplotlib.use(\'Agg\')  # noqa\nimport matplotlib.pyplot as plt\nimport torch\nfrom torch import optim\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torch.optim.lr_scheduler import CyclicLR\nimport numpy as np\n\nfrom rastervision.utils.files import (get_local_path, make_dir, upload_or_copy,\n                                      list_paths, download_if_needed,\n                                      sync_from_dir, sync_to_dir, str_to_file,\n                                      zipdir, file_to_json, json_to_file)\nfrom rastervision.utils.misc import save_img\nfrom rastervision.backend import Backend\nfrom rastervision.data.label import SemanticSegmentationLabels\nfrom rastervision.utils.misc import terminate_at_exit\nfrom rastervision.backend.torch_utils.semantic_segmentation.plot import plot_xy\nfrom rastervision.backend.torch_utils.semantic_segmentation.data import build_databunch\nfrom rastervision.backend.torch_utils.semantic_segmentation.train import (\n    train_epoch, validate_epoch)\nfrom rastervision.backend.torch_utils.semantic_segmentation.model import (\n    get_model)\n\nlog = logging.getLogger(__name__)\n\n\ndef make_debug_chips(databunch, class_map, tmp_dir, train_uri, max_count=30):\n    """"""Save debug chips for a Databunch for a semantic segmentation dataset.\n\n    This saves a plot for each example in the training and validation sets into\n    train-debug-chips.zip and valid-debug-chips.zip under the train_uri. This\n    is useful for making sure we are feeding correct data into the model.\n\n    Args:\n        databunch: DataBunch for semantic segmentation\n        class_map: (rv.ClassMap) class map used to map class ids to colors\n        tmp_dir: (str) path to temp directory\n        train_uri: (str) URI of root of training output\n        max_count: (int) maximum number of chips to generate. If None,\n            generates all of them.\n    """"""\n\n    def _make_debug_chips(split):\n        debug_chips_dir = join(tmp_dir, \'{}-debug-chips\'.format(split))\n        zip_path = join(tmp_dir, \'{}-debug-chips.zip\'.format(split))\n        zip_uri = join(train_uri, \'{}-debug-chips.zip\'.format(split))\n        make_dir(debug_chips_dir)\n        ds = databunch.train_ds if split == \'train\' else databunch.valid_ds\n        for i, (x, y) in enumerate(ds):\n            if i >= max_count:\n                break\n\n            fig, ax = plt.subplots(1)\n            plot_xy(ax, x, class_map, y=y)\n            plt.savefig(\n                join(debug_chips_dir, \'{}.png\'.format(i)), figsize=(6, 6))\n            plt.close()\n\n        zipdir(debug_chips_dir, zip_path)\n        upload_or_copy(zip_path, zip_uri)\n\n    _make_debug_chips(\'train\')\n    _make_debug_chips(\'valid\')\n\n\nclass PyTorchSemanticSegmentation(Backend):\n    """"""Semantic segmentation backend using PyTorch and fastai.""""""\n\n    def __init__(self, task_config, backend_opts, train_opts):\n        """"""Constructor.\n\n        Args:\n            task_config: (SemanticSegmentationConfig)\n            backend_opts: (simple_backend_config.BackendOptions)\n            train_opts: (pytorch_semantic_segmentation_backend_config.TrainOptions)\n        """"""\n        self.task_config = task_config\n        self.backend_opts = backend_opts\n        self.train_opts = train_opts\n        self.inf_learner = None\n\n        torch_cache_dir = \'/opt/data/torch-cache\'\n        os.environ[\'TORCH_HOME\'] = torch_cache_dir\n\n        self.model = None\n        self.device = \'cuda\' if torch.cuda.is_available() else \'cpu\'\n        log.info(\'Device = {}\'.format(self.device))\n        # TODO move this into the SemanticSegmentation RV task\n        self.class_map = self.task_config.class_map.copy()\n        self.class_map.add_nodata_item()\n\n    def log_options(self):\n        log.info(\'backend_opts:\\n\' +\n                 json.dumps(self.backend_opts.__dict__, indent=4))\n        log.info(\'train_opts:\\n\' +\n                 json.dumps(self.train_opts.__dict__, indent=4))\n\n    def process_scene_data(self, scene, data, tmp_dir):\n        """"""Make training chips for a scene.\n\n        This writes a set of image chips to {scene_id}/img/{scene_id}-{ind}.png\n        and corresponding label chips to {scene_id}/labels/{scene_id}-{ind}.png.\n\n        Args:\n            scene: (rv.data.Scene)\n            data: (rv.data.Dataset)\n            tmp_dir: (str) path to temp directory\n\n        Returns:\n            (str) path to directory with scene chips {tmp_dir}/{scene_id}\n        """"""\n        scene_dir = join(tmp_dir, str(scene.id))\n        img_dir = join(scene_dir, \'img\')\n        labels_dir = join(scene_dir, \'labels\')\n\n        make_dir(img_dir)\n        make_dir(labels_dir)\n\n        for ind, (chip, window, labels) in enumerate(data):\n            chip_path = join(img_dir, \'{}-{}.png\'.format(scene.id, ind))\n            label_path = join(labels_dir, \'{}-{}.png\'.format(scene.id, ind))\n\n            label_im = labels.get_label_arr(window).astype(np.uint8)\n            save_img(label_im, label_path)\n            save_img(chip, chip_path)\n\n        return scene_dir\n\n    def process_sceneset_results(self, training_results, validation_results,\n                                 tmp_dir):\n        """"""Write zip file with chips for a set of scenes.\n\n        This writes a zip file for a group of scenes at {chip_uri}/{uuid}.zip containing:\n        train/img/{scene_id}-{ind}.png\n        train/labels/{scene_id}-{ind}.png\n        val/img/{scene_id}-{ind}.png\n        val/labels/{scene_id}-{ind}.png\n\n        This method is called once per instance of the chip command.\n        A number of instances of the chip command can run simultaneously to\n        process chips in parallel. The uuid in the path above is what allows\n        separate instances to avoid overwriting each others\' output.\n\n        Args:\n            training_results: list of directories generated by process_scene_data\n                that all hold training chips\n            validation_results: list of directories generated by process_scene_data\n                that all hold validation chips\n        """"""\n        self.log_options()\n\n        group = str(uuid.uuid4())\n        group_uri = join(self.backend_opts.chip_uri, \'{}.zip\'.format(group))\n        group_path = get_local_path(group_uri, tmp_dir)\n        make_dir(group_path, use_dirname=True)\n\n        with zipfile.ZipFile(group_path, \'w\', zipfile.ZIP_DEFLATED) as zipf:\n\n            def _write_zip(results, split):\n                for scene_dir in results:\n                    scene_paths = glob.glob(join(scene_dir, \'**/*.png\'))\n                    for p in scene_paths:\n                        zipf.write(\n                            p,\n                            join(\n                                \'{}/{}\'.format(split,\n                                               dirname(p).split(\'/\')[-1]),\n                                basename(p)))\n\n            _write_zip(training_results, \'train\')\n            _write_zip(validation_results, \'valid\')\n\n        upload_or_copy(group_path, group_uri)\n\n    def train(self, tmp_dir):\n        """"""Train a model.\n\n        This downloads any previous output saved to the train_uri,\n        starts training (or resumes from a checkpoint), periodically\n        syncs contents of train_dir to train_uri and after training finishes.\n\n        Args:\n            tmp_dir: (str) path to temp directory\n        """"""\n        self.log_options()\n\n        # Sync output of previous training run from cloud.\n        train_uri = self.backend_opts.train_uri\n        train_dir = get_local_path(train_uri, tmp_dir)\n        make_dir(train_dir)\n        sync_from_dir(train_uri, train_dir)\n\n        # Get zip file for each group, and unzip them into chip_dir.\n        chip_dir = join(tmp_dir, \'chips\')\n        make_dir(chip_dir)\n        for zip_uri in list_paths(self.backend_opts.chip_uri, \'zip\'):\n            zip_path = download_if_needed(zip_uri, tmp_dir)\n            with zipfile.ZipFile(zip_path, \'r\') as zipf:\n                zipf.extractall(chip_dir)\n\n        # Setup data loader.\n        batch_size = self.train_opts.batch_size\n        chip_size = self.task_config.chip_size\n        class_names = self.class_map.get_class_names()\n        databunch = build_databunch(chip_dir, chip_size, batch_size,\n                                    class_names)\n        log.info(databunch)\n        num_labels = len(databunch.label_names)\n        if self.train_opts.debug:\n            make_debug_chips(databunch, self.class_map, tmp_dir, train_uri)\n\n        # Setup model\n        num_labels = len(databunch.label_names)\n        model = get_model(\n            self.train_opts.model_arch, num_labels, pretrained=True)\n        model = model.to(self.device)\n        model_path = join(train_dir, \'model\')\n\n        # Load weights from a pretrained model.\n        pretrained_uri = self.backend_opts.pretrained_uri\n        if pretrained_uri:\n            log.info(\'Loading weights from pretrained_uri: {}\'.format(\n                pretrained_uri))\n            pretrained_path = download_if_needed(pretrained_uri, tmp_dir)\n            model.load_state_dict(\n                torch.load(pretrained_path, map_location=self.device))\n\n        # Possibly resume training from checkpoint.\n        start_epoch = 0\n        train_state_path = join(train_dir, \'train_state.json\')\n        if isfile(train_state_path):\n            log.info(\'Resuming from checkpoint: {}\\n\'.format(model_path))\n            train_state = file_to_json(train_state_path)\n            start_epoch = train_state[\'epoch\'] + 1\n            model.load_state_dict(\n                torch.load(model_path, map_location=self.device))\n\n        # Write header of log CSV file.\n        metric_names = [\'precision\', \'recall\', \'f1\']\n        log_path = join(train_dir, \'log.csv\')\n        if not isfile(log_path):\n            with open(log_path, \'w\') as log_file:\n                log_writer = csv.writer(log_file)\n                row = [\'epoch\', \'time\', \'train_loss\'] + metric_names\n                log_writer.writerow(row)\n\n        # Setup Tensorboard logging.\n        if self.train_opts.log_tensorboard:\n            log_dir = join(train_dir, \'tb-logs\')\n            make_dir(log_dir)\n            tb_writer = SummaryWriter(log_dir=log_dir)\n            if self.train_opts.run_tensorboard:\n                log.info(\'Starting tensorboard process\')\n                tensorboard_process = Popen(\n                    [\'tensorboard\', \'--logdir={}\'.format(log_dir)])\n                terminate_at_exit(tensorboard_process)\n\n        # Setup optimizer, loss, and LR scheduler.\n        loss_fn = torch.nn.CrossEntropyLoss()\n        lr = self.train_opts.lr\n        opt = optim.Adam(model.parameters(), lr=lr)\n        step_scheduler, epoch_scheduler = None, None\n        num_epochs = self.train_opts.num_epochs\n\n        if self.train_opts.one_cycle and num_epochs > 1:\n            steps_per_epoch = len(databunch.train_ds) // batch_size\n            total_steps = num_epochs * steps_per_epoch\n            step_size_up = (num_epochs // 2) * steps_per_epoch\n            step_size_down = total_steps - step_size_up\n            step_scheduler = CyclicLR(\n                opt,\n                base_lr=lr / 10,\n                max_lr=lr,\n                step_size_up=step_size_up,\n                step_size_down=step_size_down,\n                cycle_momentum=False)\n            for _ in range(start_epoch * steps_per_epoch):\n                step_scheduler.step()\n\n        # Training loop.\n        for epoch in range(start_epoch, num_epochs):\n            # Train one epoch.\n            log.info(\'-----------------------------------------------------\')\n            log.info(\'epoch: {}\'.format(epoch))\n            start = time.time()\n            train_loss = train_epoch(model, self.device, databunch.train_dl,\n                                     opt, loss_fn, step_scheduler)\n            if epoch_scheduler:\n                epoch_scheduler.step()\n            log.info(\'train loss: {}\'.format(train_loss))\n\n            # Validate one epoch.\n            metrics = validate_epoch(model, self.device, databunch.valid_dl,\n                                     num_labels)\n            log.info(\'validation metrics: {}\'.format(metrics))\n\n            # Print elapsed time for epoch.\n            end = time.time()\n            epoch_time = datetime.timedelta(seconds=end - start)\n            log.info(\'epoch elapsed time: {}\'.format(epoch_time))\n\n            # Save model and state.\n            torch.save(model.state_dict(), model_path)\n            train_state = {\'epoch\': epoch}\n            json_to_file(train_state, train_state_path)\n\n            # Append to log CSV file.\n            with open(log_path, \'a\') as log_file:\n                log_writer = csv.writer(log_file)\n                row = [epoch, epoch_time, train_loss]\n                row += [metrics[k] for k in metric_names]\n                log_writer.writerow(row)\n\n            # Write to Tensorboard log.\n            if self.train_opts.log_tensorboard:\n                for key, val in metrics.items():\n                    tb_writer.add_scalar(key, val, epoch)\n                tb_writer.add_scalar(\'train_loss\', train_loss, epoch)\n                for name, param in model.named_parameters():\n                    tb_writer.add_histogram(name, param, epoch)\n\n            if (train_uri.startswith(\'s3://\')\n                    and (((epoch + 1) % self.train_opts.sync_interval) == 0)):\n                sync_to_dir(train_dir, train_uri)\n\n        # Close Tensorboard.\n        if self.train_opts.log_tensorboard:\n            tb_writer.close()\n            if self.train_opts.run_tensorboard:\n                tensorboard_process.terminate()\n\n        # Since model is exported every epoch, we need some other way to\n        # show that training is finished.\n        str_to_file(\'done!\', self.backend_opts.train_done_uri)\n\n        # Sync output to cloud.\n        sync_to_dir(train_dir, self.backend_opts.train_uri)\n\n    def load_model(self, tmp_dir):\n        """"""Load the model in preparation for one or more prediction calls.""""""\n        if self.model is None:\n            model_uri = self.backend_opts.model_uri\n            model_path = download_if_needed(model_uri, tmp_dir)\n\n            num_classes = len(self.class_map)\n            model = get_model(\n                self.train_opts.model_arch, num_classes, pretrained=False)\n            model = model.to(self.device)\n            model.load_state_dict(\n                torch.load(model_path, map_location=self.device))\n            self.model = model\n\n    def predict(self, chips, windows, tmp_dir):\n        """"""Return a prediction for a single chip.\n\n        Args:\n            chips: (numpy.ndarray) of shape (1, height, width, nb_channels)\n                containing a single imagery chip\n            windows: List containing a single (Box) window which is aligned\n                with the chip\n\n        Return:\n            (SemanticSegmentationLabels) containing predictions\n        """"""\n        self.load_model(tmp_dir)\n\n        chips = torch.Tensor(chips).permute((0, 3, 1, 2)) / 255.\n        chips = chips.to(self.device)\n        model = self.model.eval()\n\n        with torch.no_grad():\n            out = model(chips)[\'out\'].cpu()\n\n        def label_fn(_window):\n            if _window == windows[0]:\n                return out[0].argmax(0).squeeze().numpy()\n            else:\n                raise ValueError(\'Trying to get labels for unknown window.\')\n\n        return SemanticSegmentationLabels(windows, label_fn)\n'"
rastervision/backend/pytorch_semantic_segmentation_config.py,0,"b'from copy import deepcopy\n\nimport rastervision as rv\n\nfrom rastervision.backend.pytorch_semantic_segmentation import (\n    PyTorchSemanticSegmentation)\nfrom rastervision.backend.simple_backend_config import (\n    SimpleBackendConfig, SimpleBackendConfigBuilder)\nfrom rastervision.backend.api import PYTORCH_SEMANTIC_SEGMENTATION\n\n\nclass TrainOptions():\n    def __init__(self,\n                 batch_size=None,\n                 lr=None,\n                 one_cycle=None,\n                 num_epochs=None,\n                 model_arch=None,\n                 sync_interval=None,\n                 debug=None,\n                 log_tensorboard=None,\n                 run_tensorboard=None):\n        self.batch_size = batch_size\n        self.lr = lr\n        self.one_cycle = one_cycle\n        self.num_epochs = num_epochs\n        self.model_arch = model_arch\n        self.sync_interval = sync_interval\n        self.debug = debug\n        self.log_tensorboard = log_tensorboard\n        self.run_tensorboard = run_tensorboard\n\n    def __setattr__(self, name, value):\n        if name in [\'batch_size\', \'num_epochs\', \'sync_interval\']:\n            value = int(value) if isinstance(value, float) else value\n        super().__setattr__(name, value)\n\n\nclass PyTorchSemanticSegmentationConfig(SimpleBackendConfig):\n    train_opts_class = TrainOptions\n    backend_type = PYTORCH_SEMANTIC_SEGMENTATION\n    backend_class = PyTorchSemanticSegmentation\n\n\nclass PyTorchSemanticSegmentationConfigBuilder(SimpleBackendConfigBuilder):\n    config_class = PyTorchSemanticSegmentationConfig\n\n    def _applicable_tasks(self):\n        return [rv.SEMANTIC_SEGMENTATION]\n\n    def with_train_options(self,\n                           batch_size=8,\n                           lr=1e-4,\n                           one_cycle=True,\n                           num_epochs=5,\n                           model_arch=\'resnet50\',\n                           sync_interval=1,\n                           debug=False,\n                           log_tensorboard=True,\n                           run_tensorboard=True):\n        """"""Set options for training models.\n\n        Args:\n            batch_size: (int) the batch size\n            lr: (float) the learning rate if using a fixed LR\n                (ie. one_cycle is False),\n                or the maximum LR to use if one_cycle is True\n            one_cycle: (bool) True if cyclic learning rate scheduler should\n                be used. This\n                cycles the LR once during the course of training and seems to\n                result in a pretty consistent improvement. See lr for more\n                details.\n            num_epochs: (int) number of epochs (sweeps through training set) to\n                train model for\n            model_arch: (str) classification model backbone to use for DeepLabV3\n                architecture. Currently, only Resnet50 works.\n            sync_interval: (int) sync training directory to cloud every\n                sync_interval epochs.\n            debug: (bool) if True, save debug chips (ie. visualizations of\n                input to model during training) during training and use\n                single-core for creating minibatches.\n            log_tensorboard: (bool) if True, write events to Tensorboard log\n                file\n            run_tensorboard: (bool) if True, run a Tensorboard server at\n                port 6006 that uses the logs generated by the log_tensorboard\n                option\n        """"""\n        b = deepcopy(self)\n        b.train_opts = TrainOptions(\n            batch_size=batch_size,\n            lr=lr,\n            one_cycle=one_cycle,\n            num_epochs=num_epochs,\n            model_arch=model_arch,\n            sync_interval=sync_interval,\n            debug=debug,\n            log_tensorboard=log_tensorboard,\n            run_tensorboard=run_tensorboard)\n        return b\n\n    def with_pretrained_uri(self, pretrained_uri):\n        """"""pretrained_uri should be uri of exported model file.""""""\n        return super().with_pretrained_uri(pretrained_uri)\n'"
rastervision/backend/simple_backend_config.py,0,"b'from os.path import join\nfrom copy import deepcopy\nfrom abc import abstractmethod\nimport json\n\nfrom google.protobuf import struct_pb2\n\nimport rastervision as rv\nfrom rastervision.backend import (BackendConfig, BackendConfigBuilder)\nfrom rastervision.protos.backend_pb2 import BackendConfig as BackendConfigMsg\n\n\nclass BackendOptions():\n    """"""Options that pertain to backends created using SimpleBackendConfig.""""""\n\n    def __init__(self,\n                 chip_uri=None,\n                 train_uri=None,\n                 train_done_uri=None,\n                 model_uri=None,\n                 pretrained_uri=None):\n        self.chip_uri = chip_uri\n        self.train_uri = train_uri\n        self.train_done_uri = train_done_uri\n        self.model_uri = model_uri\n        self.pretrained_uri = pretrained_uri\n\n\nclass SimpleBackendConfig(BackendConfig):\n    """"""A simplified BackendConfig interface.\n\n    This class can be subclassed to created BackendConfigs with less effort\n    and a small loss of flexibility when compared to directly subclassing\n    BackendConfig. See subclasses of this for examples of how to write your\n    own subclass.\n    """"""\n\n    def __init__(self, backend_opts, train_opts):\n        """"""Constructor.\n\n        Args:\n            backend_opts: (BackendOptions)\n            train_opts: (train_opts_class) object containing options that are\n                set by with_train_options()\n        """"""\n        super().__init__(self.backend_type)\n        self.backend_opts = backend_opts\n        self.train_opts = train_opts\n\n    @property\n    @abstractmethod\n    def train_opts_class(self):\n        """"""The class that holds options set by with_train_options in the builder.""""""\n        pass\n\n    @property\n    @abstractmethod\n    def backend_type(self):\n        """"""The string representing this backend used in the registry.""""""\n        pass\n\n    @property\n    @abstractmethod\n    def backend_class(self):\n        """"""The class of the actual Backend that this class configures.""""""\n        pass\n\n    def to_proto(self):\n        config = {}\n        for k, v in self.backend_opts.__dict__.items():\n            config[k] = v\n        for k, v in self.train_opts.__dict__.items():\n            config[k] = v\n\n        custom_config = struct_pb2.Struct()\n        custom_config[\'json\'] = json.dumps(config)\n        msg = BackendConfigMsg(\n            backend_type=self.backend_type, custom_config=custom_config)\n        return msg\n\n    def create_backend(self, task_config):\n        return self.backend_class(task_config, self.backend_opts,\n                                  self.train_opts)\n\n    def update_for_command(self, command_type, experiment_config,\n                           context=None):\n        super().update_for_command(command_type, experiment_config, context)\n\n        if command_type == rv.CHIP:\n            self.backend_opts.chip_uri = join(experiment_config.chip_uri,\n                                              \'chips\')\n        elif command_type == rv.TRAIN:\n            self.backend_opts.train_uri = experiment_config.train_uri\n            self.backend_opts.model_uri = join(experiment_config.train_uri,\n                                               \'model\')\n            self.backend_opts.train_done_uri = join(\n                experiment_config.train_uri, \'done.txt\')\n\n    def report_io(self, command_type, io_def):\n        super().report_io(command_type, io_def)\n\n        if command_type == rv.CHIP:\n            io_def.add_output(self.backend_opts.chip_uri)\n        elif command_type == rv.TRAIN:\n            io_def.add_input(self.backend_opts.chip_uri)\n            io_def.add_output(self.backend_opts.model_uri)\n            io_def.add_output(self.backend_opts.train_done_uri)\n        elif command_type in [rv.PREDICT, rv.BUNDLE]:\n            if not self.backend_opts.model_uri:\n                io_def.add_missing(\'Missing model_uri.\')\n            else:\n                io_def.add_input(self.backend_opts.model_uri)\n                io_def.add_input(self.backend_opts.train_done_uri)\n        elif command_type == rv.EVAL:\n            io_def.add_input()\n\n    def save_bundle_files(self, bundle_dir):\n        model_uri = self.backend_opts.model_uri\n        if not model_uri:\n            raise rv.ConfigError(\'model_uri is not set.\')\n        local_path, base_name = self.bundle_file(model_uri, bundle_dir)\n        new_config = self.to_builder() \\\n                         .with_model_uri(base_name) \\\n                         .build()\n        return (new_config, [local_path])\n\n    def load_bundle_files(self, bundle_dir):\n        model_uri = self.backend_opts.model_uri\n        if not model_uri:\n            raise rv.ConfigError(\'model_uri is not set.\')\n        local_model_uri = join(bundle_dir, model_uri)\n        return self.to_builder() \\\n                   .with_model_uri(local_model_uri) \\\n                   .build()\n\n\nclass SimpleBackendConfigBuilder(BackendConfigBuilder):\n    """"""A simplified BackendConfigBuilder interface.\n\n    This class can be subclassed to created BackendConfigBuilders with less\n    effort and a small loss of flexibility when compared to directly\n    subclassing BackendConfigBuilder. See subclasses of this for examples of\n    how to write your own subclass.\n    """"""\n\n    def __init__(self, prev_config=None):\n        self.backend_opts = BackendOptions()\n        self.train_opts = self.config_class.train_opts_class()\n\n        if prev_config:\n            self.backend_opts = prev_config.backend_opts\n            self.train_opts = prev_config.train_opts\n\n        super().__init__(self.config_class.backend_type, self.config_class)\n        self.require_task = prev_config is None\n\n    @property\n    @abstractmethod\n    def config_class(self):\n        """"""The corresponding Config class for this builder.""""""\n        pass\n\n    @abstractmethod\n    def with_train_options(self):\n        """"""Sets the training options which are passed to the Backend.""""""\n        pass\n\n    def build(self):\n        self.validate()\n        return self.config_class(self.backend_opts, self.train_opts)\n\n    def from_proto(self, msg):\n        b = super().from_proto(msg)\n        custom_config = msg.custom_config\n\n        if \'json\' in custom_config:\n            config = json.loads(custom_config[\'json\'])\n        else:\n            config = custom_config\n\n        for k in self.backend_opts.__dict__.keys():\n            if k in config:\n                setattr(b.backend_opts, k, config[k])\n        for k in self.train_opts.__dict__.keys():\n            if k in config:\n                setattr(b.train_opts, k, config[k])\n        b.require_task = None\n        return b\n\n    def validate(self):\n        super().validate()\n\n        if self.require_task and not self.task:\n            raise rv.ConfigError(\'You must specify the task this backend \'\n                                 ""is for - use \'with_task\'."")\n\n        return True\n\n    def _process_task(self):\n        return self\n\n    def with_model_uri(self, model_uri):\n        b = deepcopy(self)\n        b.backend_opts.model_uri = model_uri\n        return b\n\n    def with_pretrained_uri(self, pretrained_uri):\n        b = deepcopy(self)\n        b.backend_opts.pretrained_uri = pretrained_uri\n        return b\n'"
rastervision/backend/tf_deeplab.py,0,"b'import os\nimport glob\nimport shutil\nimport tarfile\nimport uuid\nfrom typing import (Dict, List, Tuple)\nfrom os.path import join\nfrom subprocess import Popen\nimport logging\n\nimport numpy as np\nfrom google.protobuf import (json_format)\n\nimport rastervision as rv\nfrom rastervision.core.box import Box\nfrom rastervision.core.class_map import ClassMap\nfrom rastervision.backend import Backend\nfrom rastervision.data.scene import Scene\nfrom rastervision.data.label import SemanticSegmentationLabels\nfrom rastervision.core.training_data import TrainingData\nfrom rastervision.backend.tf_object_detection import (write_tf_record, TRAIN,\n                                                      VALIDATION)\nfrom rastervision.protos.deeplab.train_pb2 import (TrainingParameters as\n                                                   TrainingParametersMsg)\nfrom rastervision.utils.files import (\n    download_if_needed, get_local_path, make_dir, start_sync, upload_or_copy,\n    sync_to_dir, sync_from_dir, list_paths, file_exists)\nfrom rastervision.utils.misc import (numpy_to_png, png_to_numpy, save_img,\n                                     terminate_at_exit)\nfrom rastervision.data.label_source.utils import color_to_integer\nfrom rastervision.rv_config import RVConfig\n\nFROZEN_INFERENCE_GRAPH = \'model\'\nINPUT_TENSOR_NAME = \'ImageTensor:0\'\nOUTPUT_TENSOR_NAME = \'SemanticPredictions:0\'\n\nlog = logging.getLogger(__name__)\n\n\ndef make_tf_examples(training_data: TrainingData, class_map: ClassMap) -> List:\n    """"""Take training data and a class map and return a list of TFRecords.\n\n    Args:\n         training_data: A rastervision.core.training_data.TrainingData\n              object.\n         class_map: A rastervision.core.class_map.ClassMap object.\n\n    Returns:\n         list(tensorflow.core.example.example_pb2.Example)\n\n    """"""\n    tf_examples = []\n    log.info(\'Creating TFRecord\')\n    for chip, window, labels in training_data:\n        tf_example = create_tf_example(chip, window,\n                                       labels.get_label_arr(window), class_map)\n        tf_examples.append(tf_example)\n    return tf_examples\n\n\ndef merge_tf_records(output_path: str, src_records: List[str]) -> None:\n    """"""Merge multiple TFRecord files into one.\n\n    Args:\n         output_path: Where to write the merged TFRecord file.\n         src_records: A list of strings giving the location of the\n              input TFRecord files.\n\n    Returns:\n         None\n\n    """"""\n    import tensorflow as tf\n\n    records = 0\n    with tf.python_io.TFRecordWriter(output_path) as writer:\n        log.info(\'Merging TFRecords\')\n        for src_record in src_records:\n            for string_record in tf.python_io.tf_record_iterator(src_record):\n                writer.write(string_record)\n                records = records + 1\n        log.info(\'{} records\'.format(records))\n\n\ndef make_debug_images(record_path: str, output_dir: str, class_map: ClassMap,\n                      p: float) -> None:\n    """"""Render a random sample of the TFRecords in a given file as\n    human-viewable PNG files.\n\n    Args:\n         record_path: Path to the TFRecord file.\n         output_dir: Destination directory for the generated PNG files.\n         p: The probability of rendering a particular record.\n\n    Returns:\n         None\n\n    """"""\n    import tensorflow as tf\n    make_dir(output_dir)\n\n    ids = class_map.get_keys()\n    color_strs = list(map(lambda c: c.color, class_map.get_items()))\n    color_ints = list(map(lambda c: color_to_integer(c), color_strs))\n    correspondence = dict(zip(ids, color_ints))\n\n    def _label_fn(v: int) -> int:\n        if v in correspondence:\n            return correspondence.get(v)\n        else:\n            return 0\n\n    label_fn = np.vectorize(_label_fn, otypes=[np.uint64])\n\n    def _image_fn(pixel: int) -> int:\n        if (pixel & 0x00ffffff):\n            r = ((pixel >> 41 & 0x7f) + (pixel >> 17 & 0x7f)) << 16\n            g = ((pixel >> 33 & 0x7f) + (pixel >> 9 & 0x7f)) << 8\n            b = ((pixel >> 25 & 0x7f) + (pixel >> 1 & 0x7f)) << 0\n            return r + g + b\n        else:\n            return pixel >> 24\n\n    image_fn = np.vectorize(_image_fn, otypes=[np.uint64])\n\n    log.info(\'Generating debug chips\')\n    tfrecord_iter = tf.python_io.tf_record_iterator(record_path)\n    for ind, example in enumerate(tfrecord_iter):\n        if np.random.rand() <= p:\n            example = tf.train.Example.FromString(example)\n            im_unpacked, labels = parse_tf_example(example)\n\n            im_r = np.array(im_unpacked[:, :, 0], dtype=np.uint64) * 1 << 40\n            im_g = np.array(im_unpacked[:, :, 1], dtype=np.uint64) * 1 << 32\n            im_b = np.array(im_unpacked[:, :, 2], dtype=np.uint64) * 1 << 24\n            im_packed = im_r + im_g + im_b\n\n            labels_packed = label_fn(np.array(labels))\n            im_labels_packed = im_packed + labels_packed\n            im_packed = image_fn(im_labels_packed)\n\n            im_unpacked[:, :, 0] = np.bitwise_and(\n                im_packed >> 16, 0xff, dtype=np.uint8)\n            im_unpacked[:, :, 1] = np.bitwise_and(\n                im_packed >> 8, 0xff, dtype=np.uint8)\n            im_unpacked[:, :, 2] = np.bitwise_and(\n                im_packed >> 0, 0xff, dtype=np.uint8)\n\n            output_path = join(output_dir, \'{}.png\'.format(ind))\n            save_img(im_unpacked, output_path)\n\n\ndef parse_tf_example(example) -> Tuple[np.ndarray, np.ndarray]:\n    """"""Parse a TensorFlow Example into an image array and a label array.\n\n    Args:\n         example: A TensorFlow Example object.\n\n    Returns:\n         A np.ndarray \xc3\x97 np.ndarray pair.\n\n    """"""\n    ie = \'image/encoded\'\n    isce = \'image/segmentation/class/encoded\'\n    image_encoded = \\\n        example.features.feature[ie].bytes_list.value[0]\n    image_segmentation_class_encoded = \\\n        example.features.feature[isce].bytes_list.value[0]\n    im = png_to_numpy(image_encoded)\n    labels = png_to_numpy(image_segmentation_class_encoded)\n    return im, labels\n\n\ndef create_tf_example(image: np.ndarray,\n                      window: Box,\n                      labels: np.ndarray,\n                      class_map: ClassMap,\n                      chip_id: str = \'\'):\n    """"""Create a TensorFlow Example from an image, the labels, &c.\n\n    Args:\n         image: An np.ndarray containing the image data.\n         window: A Box object containing the bounding box for this example.\n         labels: An nd.array containing the label data.\n         class_map: A ClassMap object containing mappings between\n              numerical and textual labels.\n         chip_id: The chip id as a string.\n\n    Returns:\n         A DeepLab-compatible TensorFlow Example object containing the\n         given data.\n\n    """"""\n    import tensorflow as tf\n    from object_detection.utils import dataset_util\n\n    class_keys = set(class_map.get_keys())\n\n    def _clean(n):\n        return (n if n in class_keys else 0x00)\n\n    clean = np.vectorize(_clean, otypes=[np.uint8])\n\n    image_encoded = numpy_to_png(image)\n    image_filename = chip_id.encode(\'utf8\')\n    image_format = \'png\'.encode(\'utf8\')\n    image_height, image_width, image_channels = image.shape\n    image_segmentation_class_encoded = numpy_to_png(clean(labels))\n    image_segmentation_class_format = \'png\'.encode(\'utf8\')\n\n    features = tf.train.Features(\n        feature={\n            \'image/encoded\':\n            dataset_util.bytes_feature(image_encoded),\n            \'image/filename\':\n            dataset_util.bytes_feature(image_filename),\n            \'image/format\':\n            dataset_util.bytes_feature(image_format),\n            \'image/height\':\n            dataset_util.int64_feature(image_height),\n            \'image/width\':\n            dataset_util.int64_feature(image_width),\n            \'image/channels\':\n            dataset_util.int64_feature(image_channels),\n            \'image/segmentation/class/encoded\':\n            dataset_util.bytes_feature(image_segmentation_class_encoded),\n            \'image/segmentation/class/format\':\n            dataset_util.bytes_feature(image_segmentation_class_format),\n        })\n\n    return tf.train.Example(features=features)\n\n\ndef get_record_uri(base_uri: str, split: str, suffix: str) -> str:\n    """"""Given a base URI and a split, return a filename to use.\n\n    Args:\n         base_uri: The directory under-which the returned record uri\n              will reside.\n         split: The split (""train"", ""validate"", et cetera).\n         suffix: Unique identifier to place at the end  of the tf record file.\n    Returns:\n         A uri, under the base_uri, that can be used to store a record\n         file.\n\n    """"""\n    return join(base_uri, split, \'{}-{}.record\'.format(split, suffix))\n\n\ndef get_record_dir(base_uri: str, split: str) -> str:\n    """"""Given a base URI and a split, return the directory URI\n    that contains the split records.\n\n    Args:\n         base_uri: The directory under-which the returned record uri\n              will reside.\n         split: The split (""train"", ""validate"", et cetera).\n    Returns:\n         A uri for a directory, under the base_uri, that contains\n    the TF records for the given split.\n\n    """"""\n    return join(base_uri, split)\n\n\ndef get_latest_checkpoint(train_logdir_local: str) -> str:\n    """"""Return the most recently generated checkpoint.\n\n    Args:\n         train_logir_local: The directory in-which to look for the\n              latest checkpoint.\n\n    Returns:\n         Returns the (local) URI to the latest checkpoint.\n\n    """"""\n    ckpts = glob.glob(join(train_logdir_local, \'model.ckpt-*.meta\'))\n    times = map(os.path.getmtime, ckpts)\n    latest = sorted(zip(times, ckpts))[-1][1]\n    return latest[:len(latest) - len(\'.meta\')]\n\n\ndef get_evaluation_args(eval_py: str, train_logdir_local: str,\n                        dataset_dir_local: str, eval_logdir: str, tfdl_config):\n    """"""Generate the array of arguments needed to run the eval script.\n\n    Args:\n         eval_py: The URI of the eval script.\n         train_logdir_local: The directory in-which checkpoints can be\n              found.\n         dataset_dir_local: The directory in which the records are\n              found.\n         eval_logdir: The directory where evaluation events should be\n              logged.\n         tfdl_config: google.protobuf.Struct with fields from\n            rv.protos.deeplab.train.proto containing TF Deeplab training configuration\n\n    Returns:\n         A list of arguments suitable for running the eval script.\n\n    """"""\n    fields = [\n        \'dataset\',\n        \'output_stride\',\n        \'decoder_output_stride\',\n        \'model_variant\',\n        \'eval_split\',\n    ]\n\n    multi_fields = [\n        \'atrous_rates\',\n        \'eval_crop_size\',\n    ]\n\n    args = [\'python\', eval_py]\n\n    args.append(\'--checkpoint_dir={}\'.format(train_logdir_local))\n    args.append(\'--eval_logdir={}\'.format(eval_logdir))\n    args.append(\'--dataset_dir={}\'.format(dataset_dir_local))\n\n    for field in multi_fields:\n        for item in tfdl_config.__getattribute__(field):\n            args.append(\'--{}={}\'.format(field, item))\n\n    for field in fields:\n        field_value = tfdl_config.__getattribute__(field)\n        if (not type(field_value) is str) or (not len(field_value) == 0):\n            args.append(\'--{}={}\'.format(field, field_value))\n\n    return args\n\n\ndef get_training_args(train_py: str, train_logdir_local: str, tfic_ckpt: str,\n                      dataset_dir_local: str, num_classes: int,\n                      tfdl_config) -> Tuple[List[str], Dict[str, str]]:\n    """"""Generate the array of arguments needed to run the training script.\n\n    Args:\n         train_py: The URI of the training script.\n         train_logdir_local: The directory in-which checkpoints will\n              be placed.\n         tfic_ckpt: URI of the .ckpt ""file"" from the initial\n              checkpoint tarball.\n         dataset_dir_local: The directory in which the records are\n              found.\n         num_classes: The number of classes.\n         tfdl_config: google.protobuf.Struct with fields from\n            rv.protos.deeplab.train.proto containing TF Deeplab training configuration\n\n    Returns:\n         A tuple of two things: (1) a list of arguments suitable for\n         starting the training script and (2) an environment in-which\n         to start the training script.\n\n    """"""\n    fields = [\n        \'fine_tune_batch_norm\',\n        \'initialize_last_layer\',\n        \'last_layers_contain_logits_only\',\n        \'save_summaries_images\',\n        \'upsample_logits\',\n        \'base_learning_rate\',\n        \'last_layer_gradient_multiplier\',\n        \'learning_power\',\n        \'learning_rate_decay_factor\',\n        \'max_scale_factor\',\n        \'min_scale_factor\',\n        \'momentum\',\n        \'num_clones\',\n        \'scale_factor_step_size\',\n        \'slow_start_learning_rate\',\n        \'weight_decay\',\n        \'decoder_output_stride\',\n        \'learning_rate_decay_step\',\n        \'output_stride\',\n        \'save_interval_secs\',\n        \'save_summaries_secs\',\n        \'slow_start_step\',\n        \'train_batch_size\',\n        \'training_number_of_steps\',\n        \'dataset\',\n        \'learning_policy\',\n        \'model_variant\',\n        \'train_split\',\n    ]\n\n    multi_fields = [\n        \'atrous_rates\',\n        \'train_crop_size\',\n    ]\n\n    env_fields = [\n        \'dl_custom_train\',\n        \'dl_custom_validation\',\n    ]\n\n    args = [\'python\', train_py]\n\n    args.append(\'--train_logdir={}\'.format(train_logdir_local))\n    args.append(\'--tf_initial_checkpoint={}\'.format(tfic_ckpt))\n    args.append(\'--dataset_dir={}\'.format(dataset_dir_local))\n\n    for field in multi_fields:\n        for item in tfdl_config.__getattribute__(field):\n            args.append(\'--{}={}\'.format(field, item))\n\n    for field in fields:\n        field_value = tfdl_config.__getattribute__(field)\n        if (not type(field_value) is str) or (not len(field_value) == 0):\n            args.append(\'--{}={}\'.format(field, field_value))\n\n    env = os.environ.copy()\n    for field in env_fields:\n        field_value = tfdl_config.__getattribute__(field)\n        log.info(\'{}={}\'.format(field.upper(), field_value))\n        env[field.upper()] = str(field_value)\n    log.info(\'DL_CUSTOM_CLASSES={}\'.format(num_classes))\n    env[\'DL_CUSTOM_CLASSES\'] = str(num_classes)\n\n    return (args, env)\n\n\ndef get_export_args(export_py: str, train_logdir_local: str, num_classes: int,\n                    tfdl_config) -> List[str]:\n    """"""Generate the array of arguments needed to run the export script.\n\n    Args:\n         export_py: The URI of the export script.\n         train_logdir_local: The directory in-which checkpoints will\n              be placed.\n         num_classes: The number of classes.\n         tfdl_config: google.protobuf.Struct with fields from\n            rv.protos.deeplab.train.proto containing TF Deeplab training configuration\n\n    Returns:\n         A list of arguments suitable for starting the training\n         script.\n    """"""\n\n    fields = [\n        \'decoder_output_stride\',\n        \'output_stride\',\n        \'model_variant\',\n    ]\n\n    args = [\'python\', export_py]\n\n    args.append(\'--checkpoint_path={}\'.format(\n        get_latest_checkpoint(train_logdir_local)))\n    args.append(\'--export_path={}\'.format(\n        join(train_logdir_local, FROZEN_INFERENCE_GRAPH)))\n    args.append(\'--num_classes={}\'.format(num_classes))\n\n    for field in fields:\n        field_value = tfdl_config.__getattribute__(field)\n        args.append(\'--{}={}\'.format(field, field_value))\n\n    for item in tfdl_config.__getattribute__(\'atrous_rates\'):\n        args.append(\'--{}={}\'.format(\'atrous_rates\', item))\n\n    for item in tfdl_config.__getattribute__(\'train_crop_size\'):\n        args.append(\'--{}={}\'.format(\'crop_size\', item))\n\n    return args\n\n\nclass TFDeeplab(Backend):\n    """"""Backend-derived type that implements the TensorFlow DeepLab\n    backend.\n\n    """"""\n\n    def __init__(self, backend_config, task_config):\n        """"""Constructor.\n\n        Args:\n            backend_config: rv.backend.TFDeeplabConfig\n            task_config: rv.task.SemanticSegmentationConfig\n        """"""\n        self.sess = None\n        self.backend_config = backend_config\n        self.task_config = task_config\n        self.class_map = task_config.class_map\n\n    def process_scene_data(self, scene: Scene, data: TrainingData,\n                           tmp_dir: str) -> str:\n        """"""Process the given scene and data into a TFRecord file specifically\n        associated with that file.\n\n        Args:\n             scene: The scene data (labels stores, the raster sources,\n                  and so on).\n             data: The training data.\n             tmp_dir: (str) temporary directory to use\n        Returns:\n            The local path to the generated file.\n        """"""\n        # Currently TF Deeplab can only handle uint8\n        if scene.raster_source.get_dtype() != np.uint8:\n            raise Exception(\'Cannot use {} backend for imagery that does \'\n                            \'not have data type uint8. \'\n                            \'Use the StatsAnalyzer and StatsTransformer \'\n                            \'to turn the raster data into uint8 data\'.format(\n                                rv.TF_DEEPLAB))\n\n        tf_examples = make_tf_examples(data, self.class_map)\n\n        base_uri = self.backend_config.training_data_uri\n        split = \'{}-{}\'.format(scene.id, uuid.uuid4())\n        record_path = join(base_uri, \'{}.record\'.format(split))\n        record_path = get_local_path(record_path, tmp_dir)\n\n        make_dir(record_path, use_dirname=True)\n        write_tf_record(tf_examples, record_path)\n\n        return record_path\n\n    def process_sceneset_results(self, training_results: List[str],\n                                 validation_results: List[str],\n                                 tmp_dir: str) -> None:\n        """"""Merge TFRecord files from individual scenes into two at-large files\n        (one for training data and one for validation data).\n\n        Args:\n             training_results: A list of paths to TFRecords containing\n                  training data.\n             validation_results: A list of paths to TFRecords\n                  containing validation data.\n             tmp_dir: (str) temporary directory to use\n        Returns:\n             None\n\n        """"""\n        base_uri = self.backend_config.training_data_uri\n        chip_suffix = str(uuid.uuid4()).split(\'-\')[0]\n        training_record_path = get_record_uri(base_uri, TRAIN, chip_suffix)\n        training_record_path_local = get_local_path(training_record_path,\n                                                    tmp_dir)\n        validation_record_path = get_record_uri(base_uri, VALIDATION,\n                                                chip_suffix)\n        validation_record_path_local = get_local_path(validation_record_path,\n                                                      tmp_dir)\n\n        make_dir(training_record_path_local, use_dirname=True)\n        make_dir(validation_record_path_local, use_dirname=True)  # sic\n        merge_tf_records(training_record_path_local, training_results)\n        merge_tf_records(validation_record_path_local, validation_results)\n        upload_or_copy(training_record_path_local, training_record_path)\n        upload_or_copy(validation_record_path_local, validation_record_path)\n\n        if self.backend_config.debug:\n\n            def _make_debug_chips(split, record_path_local):\n                zip_path = join(base_uri, \'{}-debug\'.format(split),\n                                chip_suffix)\n                zip_path_local = get_local_path(zip_path, tmp_dir)\n\n                debug_dir = join(tmp_dir, \'{}-debug\'.format(split),\n                                 chip_suffix)\n                make_debug_images(\n                    record_path_local, debug_dir, self.class_map,\n                    self.task_config.chip_options.debug_chip_probability)\n                shutil.make_archive(zip_path_local, \'zip\', debug_dir)\n\n                upload_or_copy(\'{}.zip\'.format(zip_path_local),\n                               \'{}.zip\'.format(zip_path))\n\n            if training_results:\n                _make_debug_chips(TRAIN, training_record_path_local)\n            if validation_results:\n                _make_debug_chips(VALIDATION, validation_record_path_local)\n\n    def train(self, tmp_dir: str) -> None:\n        """"""Train a DeepLab model the task and backend config.\n\n        Args:\n            tmp_dir: (str) temporary directory to use\n\n        Returns:\n             None\n        """"""\n        train_py = self.backend_config.script_locations.train_py\n        eval_py = self.backend_config.script_locations.eval_py\n        export_py = self.backend_config.script_locations.export_py\n\n        # Setup local input and output directories\n        log.info(\'Setting up local input and output directories\')\n        train_logdir = self.backend_config.training_output_uri\n        train_logdir_local = get_local_path(train_logdir, tmp_dir)\n        dataset_dir = get_record_dir(self.backend_config.training_data_uri,\n                                     TRAIN)\n        dataset_dir_local = get_local_path(dataset_dir, tmp_dir)\n        make_dir(tmp_dir)\n        make_dir(train_logdir_local)\n        make_dir(dataset_dir_local)\n\n        # Download training data\n        log.info(\'Downloading training data\')\n        for i, record_file in enumerate(list_paths(dataset_dir)):\n            download_if_needed(record_file, tmp_dir)\n\n        # Download and untar initial checkpoint.\n        log.info(\'Downloading and untarring initial checkpoint\')\n        tf_initial_checkpoints_uri = self.backend_config.pretrained_model_uri\n        download_if_needed(tf_initial_checkpoints_uri, tmp_dir)\n        tfic_tarball = get_local_path(tf_initial_checkpoints_uri, tmp_dir)\n        tfic_dir = os.path.dirname(tfic_tarball)\n        with tarfile.open(tfic_tarball, \'r:gz\') as tar:\n            tar.extractall(tfic_dir)\n        tfic_ckpt = glob.glob(\'{}/*/*.index\'.format(tfic_dir))[0]\n        tfic_ckpt = tfic_ckpt[0:-len(\'.index\')]\n\n        # Restart support\n        train_restart_dir = self.backend_config.train_options.train_restart_dir\n        if type(train_restart_dir) is not str or len(train_restart_dir) == 0:\n            train_restart_dir = train_logdir\n\n        # Get output from potential previous run so we can resume training.\n        if type(train_restart_dir) is str and len(\n                train_restart_dir\n        ) > 0 and not self.backend_config.train_options.replace_model:\n            sync_from_dir(train_restart_dir, train_logdir_local)\n\n            # Need to update model_checkpoint_path in the checkpoint file,\n            # since it has the absolute paths from the previous run which\n            # was using a different temporary directory on another machine.\n            # If Deeplab could save relative paths instead (like the Object\n            # Detection API does), then we wouldn\'t need to do this.\n            checkpoint_path = join(train_logdir_local, \'checkpoint\')\n            if file_exists(checkpoint_path):\n                latest_checkpoint = get_latest_checkpoint(train_logdir_local)\n                with open(checkpoint_path, \'w\') as cf:\n                    cf.write(\'model_checkpoint_path: \\""{}\\""\'.format(\n                        latest_checkpoint))\n        else:\n            if self.backend_config.train_options.replace_model:\n                if os.path.exists(train_logdir_local):\n                    shutil.rmtree(train_logdir_local)\n                make_dir(train_logdir_local)\n\n        # Periodically synchronize with remote\n        sync = start_sync(\n            train_logdir_local,\n            train_logdir,\n            sync_interval=self.backend_config.train_options.sync_interval)\n\n        with sync:\n            # Setup TFDL config\n            tfdl_config = json_format.ParseDict(\n                self.backend_config.tfdl_config, TrainingParametersMsg())\n            log.info(\'tfdl_config={}\'.format(tfdl_config))\n            log.info(\'Training steps={}\'.format(\n                tfdl_config.training_number_of_steps))\n\n            # Additional training options\n            max_class = max(\n                list(map(lambda c: c.id, self.class_map.get_items())))\n            num_classes = len(self.class_map.get_items())\n            num_classes = max(max_class, num_classes) + 1\n            (train_args, train_env) = get_training_args(\n                train_py, train_logdir_local, tfic_ckpt, dataset_dir_local,\n                num_classes, tfdl_config)\n\n            # Start training\n            log.info(\'Starting training process\')\n            log.info(\' \'.join(train_args))\n            train_process = Popen(train_args, env=train_env)\n            terminate_at_exit(train_process)\n\n            if self.backend_config.train_options.do_monitoring:\n                # Start tensorboard\n                log.info(\'Starting tensorboard process\')\n                tensorboard_process = Popen(\n                    [\'tensorboard\', \'--logdir={}\'.format(train_logdir_local)])\n                terminate_at_exit(tensorboard_process)\n\n            if self.backend_config.train_options.do_eval:\n                # Start eval script\n                log.info(\'Starting eval script\')\n                eval_logdir = train_logdir_local\n                eval_args = get_evaluation_args(eval_py, train_logdir_local,\n                                                dataset_dir_local, eval_logdir,\n                                                tfdl_config)\n                eval_process = Popen(eval_args, env=train_env)\n                terminate_at_exit(eval_process)\n\n            # Wait for training and tensorboard\n            log.info(\'Waiting for training and tensorboard processes\')\n            train_process.wait()\n            if self.backend_config.train_options.do_monitoring:\n                tensorboard_process.terminate()\n\n            # Export frozen graph\n            log.info(\n                \'Exporting frozen graph ({}/model)\'.format(train_logdir_local))\n            export_args = get_export_args(export_py, train_logdir_local,\n                                          num_classes, tfdl_config)\n            export_process = Popen(export_args)\n            terminate_at_exit(export_process)\n            export_process.wait()\n\n            # Package up the model files for usage as fine tuning checkpoints\n            fine_tune_checkpoint_name = self.backend_config.fine_tune_checkpoint_name\n            latest_checkpoints = get_latest_checkpoint(train_logdir_local)\n            model_checkpoint_files = glob.glob(\n                \'{}*\'.format(latest_checkpoints))\n            inference_graph_path = os.path.join(train_logdir_local, \'model\')\n\n            with RVConfig.get_tmp_dir() as tmp_dir:\n                model_dir = os.path.join(tmp_dir, fine_tune_checkpoint_name)\n                make_dir(model_dir)\n                model_tar = os.path.join(\n                    train_logdir_local,\n                    \'{}.tar.gz\'.format(fine_tune_checkpoint_name))\n                shutil.copy(inference_graph_path,\n                            \'{}/frozen_inference_graph.pb\'.format(model_dir))\n                for path in model_checkpoint_files:\n                    shutil.copy(path, model_dir)\n                with tarfile.open(model_tar, \'w:gz\') as tar:\n                    tar.add(model_dir, arcname=os.path.basename(model_dir))\n\n        # Perform final sync\n        sync_to_dir(train_logdir_local, train_logdir, delete=False)\n\n    def load_model(self, tmp_dir: str):\n        """"""Load the model in preparation for one or more prediction calls.\n\n        Args:\n             tmp_dir: (str) temporary directory to use\n        """"""\n        # noqa Courtesy of https://github.com/tensorflow/models/blob/cbbb2ffcde66e646d4a47628ffe2ece2322b64e8/research/deeplab/deeplab_demo.ipynb\n        import tensorflow as tf\n        if self.sess is None:\n            FROZEN_GRAPH_NAME = download_if_needed(\n                self.backend_config.model_uri, tmp_dir)\n            graph = tf.Graph()\n            with open(FROZEN_GRAPH_NAME, \'rb\') as data:\n                graph_def = tf.GraphDef.FromString(data.read())\n            with graph.as_default():\n                tf.import_graph_def(graph_def, name=\'\')\n            self.sess = tf.Session(graph=graph)\n\n    def predict(self, chips: np.ndarray, windows: List[Box],\n                tmp_dir: str) -> SemanticSegmentationLabels:\n        """"""Predict using an already-trained DeepLab model.\n\n        Args:\n            chips: An np.ndarray containing the image data in a batch of size 1.\n            tmp_dir: (str) temporary directory to use\n\n        Returns:\n             SemanticSegmentationLabels object with predictions for a single chip\n        """"""\n        self.load_model(tmp_dir)\n        label_arr = self.sess.run(\n            OUTPUT_TENSOR_NAME, feed_dict={INPUT_TENSOR_NAME: [chips[0]]})[0]\n\n        # Return ""trivial"" instance of SemanticSegmentationLabels that holds a single\n        # window and has ability to get labels for that one window.\n        def label_fn(_window):\n            if _window == windows[0]:\n                return label_arr\n            else:\n                raise ValueError(\'Trying to get labels for unknown window.\')\n\n        labels = SemanticSegmentationLabels(windows, label_fn)\n\n        return labels\n'"
rastervision/backend/tf_deeplab_config.py,0,"b'import os\nfrom copy import deepcopy\nfrom google.protobuf import (json_format)\n\nimport rastervision as rv\nfrom rastervision.backend import (BackendConfig, BackendConfigBuilder)\nfrom rastervision.protos.backend_pb2 import BackendConfig as BackendConfigMsg\nfrom rastervision.utils.files import file_to_str\nfrom rastervision.utils.misc import set_nested_keys\nfrom rastervision.task import SemanticSegmentationConfig\n\n# Default location to Tensorflow Deeplab\'s scripts.\nDEFAULT_SCRIPT_TRAIN = \'/opt/tf-models/deeplab/train.py\'\nDEFAULT_SCRIPT_EVAL = \'/opt/tf-models/deeplab/eval.py\'\nDEFAULT_SCRIPT_EXPORT = \'/opt/tf-models/deeplab/export_model.py\'\n\n\nclass TFDeeplabConfig(BackendConfig):\n    class TrainOptions:\n        def __init__(self,\n                     train_restart_dir=None,\n                     sync_interval=600,\n                     do_monitoring=True,\n                     replace_model=False,\n                     do_eval=False):\n            self.train_restart_dir = train_restart_dir\n            self.sync_interval = sync_interval\n            self.do_monitoring = do_monitoring\n            self.replace_model = replace_model\n            self.do_eval = do_eval\n\n    class ScriptLocations:\n        def __init__(self,\n                     train_py=DEFAULT_SCRIPT_TRAIN,\n                     export_py=DEFAULT_SCRIPT_EXPORT,\n                     eval_py=DEFAULT_SCRIPT_EVAL):\n            self.train_py = train_py\n            self.eval_py = eval_py\n            self.export_py = export_py\n\n    def __init__(self,\n                 tfdl_config,\n                 pretrained_model_uri=None,\n                 train_options=None,\n                 script_locations=None,\n                 debug=False,\n                 training_data_uri=None,\n                 training_output_uri=None,\n                 model_uri=None,\n                 fine_tune_checkpoint_name=None):\n        if train_options is None:\n            train_options = TFDeeplabConfig.TrainOptions()\n        if script_locations is None:\n            script_locations = TFDeeplabConfig.ScriptLocations()\n\n        super().__init__(rv.TF_DEEPLAB, pretrained_model_uri)\n        self.tfdl_config = tfdl_config\n        self.pretrained_model_uri = pretrained_model_uri\n        self.train_options = train_options\n        self.script_locations = script_locations\n        self.debug = debug\n\n        # Internally set from command preprocessing\n        self.training_data_uri = training_data_uri\n        self.training_output_uri = training_output_uri\n        self.model_uri = model_uri\n        self.fine_tune_checkpoint_name = fine_tune_checkpoint_name\n\n    def create_backend(self, task_config):\n        from rastervision.backend.tf_deeplab import TFDeeplab\n        return TFDeeplab(self, task_config)\n\n    def to_proto(self):\n        d = {\n            \'train_py\': self.script_locations.train_py,\n            \'eval_py\': self.script_locations.eval_py,\n            \'export_py\': self.script_locations.export_py,\n            \'train_restart_dir\': self.train_options.train_restart_dir,\n            \'sync_interval\': self.train_options.sync_interval,\n            \'do_monitoring\': self.train_options.do_monitoring,\n            \'do_eval\': self.train_options.do_eval,\n            \'replace_model\': self.train_options.replace_model,\n            \'debug\': self.debug,\n            \'training_data_uri\': self.training_data_uri,\n            \'training_output_uri\': self.training_output_uri,\n            \'model_uri\': self.model_uri,\n            \'fine_tune_checkpoint_name\': self.fine_tune_checkpoint_name,\n            \'tfdl_config\': self.tfdl_config\n        }\n\n        conf = json_format.ParseDict(d, BackendConfigMsg.TFDeeplabConfig())\n\n        msg = BackendConfigMsg(\n            backend_type=rv.TF_DEEPLAB, tf_deeplab_config=conf)\n\n        if self.pretrained_model_uri:\n            msg.MergeFrom(\n                BackendConfigMsg(\n                    pretrained_model_uri=self.pretrained_model_uri))\n\n        return msg\n\n    def update_for_command(self, command_type, experiment_config,\n                           context=None):\n        super().update_for_command(command_type, experiment_config, context)\n\n        if command_type == rv.CHIP:\n            self.training_data_uri = experiment_config.chip_uri\n\n        if command_type == rv.TRAIN:\n            self.training_output_uri = experiment_config.train_uri\n\n            self.model_uri = os.path.join(self.training_output_uri, \'model\')\n\n            if not self.fine_tune_checkpoint_name:\n                # Set the fine tune checkpoint name to the experiment id\n                self.fine_tune_checkpoint_name = experiment_config.id\n\n    def report_io(self, command_type, io_def):\n        super().report_io(command_type, io_def)\n\n        if command_type == rv.CHIP:\n            io_def.add_output(self.training_data_uri)\n\n        if command_type == rv.TRAIN:\n            io_def.add_input(self.training_data_uri)\n            io_def.add_output(self.model_uri)\n\n            full_checkpoint_path = \'{}.tar.gz\'.format(\n                os.path.join(self.training_output_uri,\n                             self.fine_tune_checkpoint_name))\n            io_def.add_output(full_checkpoint_path)\n\n        if command_type in [rv.PREDICT, rv.BUNDLE]:\n            if not self.model_uri:\n                io_def.add_missing(\'Missing model_uri.\')\n            else:\n                io_def.add_input(self.model_uri)\n\n    def save_bundle_files(self, bundle_dir):\n        if not self.model_uri:\n            raise rv.ConfigError(\'model_uri is not set.\')\n        local_path, base_name = self.bundle_file(self.model_uri, bundle_dir)\n        new_config = self.to_builder() \\\n                         .with_model_uri(base_name) \\\n                         .build()\n        return (new_config, [local_path])\n\n    def load_bundle_files(self, bundle_dir):\n        if not self.model_uri:\n            raise rv.ConfigError(\'model_uri is not set.\')\n        local_model_uri = os.path.join(bundle_dir, self.model_uri)\n        return self.to_builder() \\\n                   .with_model_uri(local_model_uri) \\\n                   .build()\n\n\nclass TFDeeplabConfigBuilder(BackendConfigBuilder):\n    def __init__(self, prev=None):\n        config = {}\n        if prev:\n            config = {\n                \'tfdl_config\': prev.tfdl_config,\n                \'pretrained_model_uri\': prev.pretrained_model_uri,\n                \'train_options\': prev.train_options,\n                \'script_locations\': prev.script_locations,\n                \'debug\': prev.debug,\n                \'training_data_uri\': prev.training_data_uri,\n                \'training_output_uri\': prev.training_output_uri,\n                \'model_uri\': prev.model_uri,\n                \'fine_tune_checkpoint_name\': prev.fine_tune_checkpoint_name\n            }\n        super().__init__(rv.TF_DEEPLAB, TFDeeplabConfig, config, prev)\n        self.config_mods = []\n        self.require_task = prev is None\n\n    def from_proto(self, msg):\n        b = super().from_proto(msg)\n        conf = msg.tf_deeplab_config\n        # Since this is coming from a serialized message,\n        # assume the task has already been set and do not\n        # require it during validation.\n        b.require_task = False\n        b = b.with_train_options(\n            train_restart_dir=conf.train_restart_dir,\n            sync_interval=conf.sync_interval,\n            do_monitoring=conf.do_monitoring,\n            replace_model=conf.replace_model,\n            do_eval=conf.do_eval)\n        b = b.with_script_locations(\n            train_py=conf.train_py,\n            export_py=conf.export_py,\n            eval_py=conf.eval_py)\n        b = b.with_training_data_uri(conf.training_data_uri)\n        b = b.with_training_output_uri(conf.training_output_uri)\n        b = b.with_model_uri(conf.model_uri)\n        b = b.with_fine_tune_checkpoint_name(conf.fine_tune_checkpoint_name)\n        b = b.with_debug(conf.debug)\n        b = b.with_template(json_format.MessageToDict(conf.tfdl_config))\n        return b\n\n    def validate(self):\n        super().validate()\n        if not self.config.get(\'tfdl_config\'):\n            raise rv.ConfigError(\'You must specify a template for the backend \'\n                                 ""configuration - use \'with_template\'."")\n\n        if not isinstance(self.config.get(\'tfdl_config\'), dict):\n            raise rv.ConfigError(\n                \'tfdl_config must be of type dict, got {}\'.format(\n                    type(self.config.get(\'tfdl_config\'))))\n\n        if self.require_task and not self.task:\n            raise rv.ConfigError(\'You must specify the task this backend \'\n                                 ""is for - use \'with_task\'."")\n\n        if self.require_task and not isinstance(self.task,\n                                                SemanticSegmentationConfig):\n            raise rv.ConfigError(\'Task set with with_task must be of type\'\n                                 \' SemanticSegmentationConfig, got {}.\'.format(\n                                     type(self.task)))\n\n        if self.task and (self.task.predict_chip_size != self.task.chip_size):\n            raise rv.ConfigError(\n                \'TFDeepLab Backend does not currently support predict_chip_size\'\n                \' that is not equal to training chip_size.\')\n\n        return True\n\n    def build(self):\n        """"""Build this configuration.\n        """"""\n        self.validate()\n        b = deepcopy(self)\n\n        for config_mod, ignore_missing_keys, set_missing_keys in b.config_mods:\n            try:\n                set_nested_keys(b.config[\'tfdl_config\'], config_mod,\n                                ignore_missing_keys, set_missing_keys)\n            except Exception as e:\n                raise rv.ConfigError(\n                    \'Error setting configuration {}\'.format(config_mod)) from e\n\n        return TFDeeplabConfig(**b.config)\n\n    def _applicable_tasks(self):\n        return [rv.SEMANTIC_SEGMENTATION]\n\n    def _process_task(self):\n        return self.with_config(\n            {\n                \'trainCropSize\': [self.task.chip_size, self.task.chip_size],\n                \'evalCropSize\': [self.task.chip_size, self.task.chip_size]\n            },\n            ignore_missing_keys=True)\n\n    def _load_model_defaults(self, model_defaults):\n        """"""Loads defaults.\n\n        Expected keys are ""pretrained_model_uri"" and ""pipeline_config_uri"",\n        neither of which is required.\n        """"""\n        expected_keys = [\'pretrained_model_uri\', \'tfdl_config\']\n        unknown_keys = set(model_defaults.keys()) - set(expected_keys)\n        if unknown_keys:\n            raise rv.ConfigError((\'Unexpected keys in model defaults:\'\n                                  \' {}. Expected keys: {}\').format(\n                                      unknown_keys, expected_keys))\n\n        b = self\n        if \'pretrained_model_uri\' in model_defaults:\n            b = b.with_pretrained_model(model_defaults[\'pretrained_model_uri\'])\n        if \'tfdl_config\' in model_defaults:\n            b = b.with_template(model_defaults[\'tfdl_config\'])\n        return b\n\n    def with_template(self, template):\n        """"""Use a TFDL config template from dict, string or uri.""""""\n        from rastervision.protos.deeplab.train_pb2 import (\n            TrainingParameters as TrainingParametersMsg)\n\n        template_json = None\n        if type(template) is dict:\n            template_json = template\n        else:\n            # Try parsing the string as a message, on fail assume it\'s a URI\n            msg = None\n            try:\n                msg = json_format.Parse(template, TrainingParametersMsg())\n            except json_format.ParseError:\n                msg = json_format.Parse(\n                    file_to_str(template), TrainingParametersMsg())\n            template_json = json_format.MessageToDict(msg)\n        b = deepcopy(self)\n        b.config[\'tfdl_config\'] = template_json\n        return b\n\n    def with_num_clones(self, num_clones):\n        """"""Sets the number of clones (useful for multi-GPU training).""""""\n        return self.with_config({\'numClones\': num_clones})\n\n    def with_batch_size(self, batch_size):\n        """"""Sets the training batch size.""""""\n        return self.with_config({\'trainBatchSize\': batch_size})\n\n    def with_num_steps(self, num_steps):\n        """"""Sets the number of training steps.""""""\n        return self.with_config({\'trainingNumberOfSteps\': num_steps})\n\n    def with_config(self,\n                    config_mod,\n                    ignore_missing_keys=False,\n                    set_missing_keys=False):\n        """"""Given a dict, modify the tensorflow pipeline configuration.\n\n        Modify it such that keys that are found recursively in the\n        configuration are replaced with those values.\n        """"""\n        b = deepcopy(self)\n        b.config_mods.append((config_mod, ignore_missing_keys,\n                              set_missing_keys))\n        return b\n\n    def with_debug(self, debug):\n        """"""Sets the debug flag for this backend.""""""\n        b = deepcopy(self)\n        b.config[\'debug\'] = debug\n        return b\n\n    def with_train_options(self,\n                           train_restart_dir=None,\n                           sync_interval=600,\n                           do_monitoring=True,\n                           replace_model=False,\n                           do_eval=False):\n        """"""Sets the train options for this backend.\n\n        Args:\n            sync_interval: How often to sync output of training to the cloud\n                (in seconds).\n            do_monitoring: Run process to monitor training (eg. Tensorboard)\n            replace_model: Replace the model checkpoint if exists.\n                If false, this will continue training from checkpoint if\n                exists, if the backend allows for this.\n            do_eval: Boolean determining whether to run the eval\n               script.\n        """"""\n        b = deepcopy(self)\n        b.config[\'train_options\'] = TFDeeplabConfig.TrainOptions(\n            train_restart_dir=train_restart_dir,\n            sync_interval=sync_interval,\n            do_monitoring=do_monitoring,\n            replace_model=replace_model,\n            do_eval=do_eval)\n\n        return b.with_config(\n            {\n                \'saveIntervalSecs\': sync_interval,\n                \'saveSummariesSecs\': sync_interval\n            },\n            ignore_missing_keys=True)\n\n    def with_model_uri(self, model_uri):\n        """"""Sets the filename for the model that will be trained.""""""\n        b = deepcopy(self)\n        b.config[\'model_uri\'] = model_uri\n        return b\n\n    def with_fine_tune_checkpoint_name(self, fine_tune_checkpoint_name):\n        """"""Sets the name of the fine tune checkpoint for the model.""""""\n        b = deepcopy(self)\n        b.config[\'fine_tune_checkpoint_name\'] = fine_tune_checkpoint_name\n        return b\n\n    def with_training_data_uri(self, training_data_uri):\n        """"""Whence comes the training data?\n\n        Args:\n            training_data_uri: The location of the training data.\n        """"""\n        b = deepcopy(self)\n        b.config[\'training_data_uri\'] = training_data_uri\n        return b\n\n    def with_training_output_uri(self, training_output_uri):\n        """"""Whither goes the training output?\n\n        Args:\n            training_output_uri: The location where the training\n                output will be stored.\n        """"""\n        b = deepcopy(self)\n        b.config[\'training_output_uri\'] = training_output_uri\n        return b\n\n    def with_script_locations(self,\n                              train_py=DEFAULT_SCRIPT_TRAIN,\n                              export_py=DEFAULT_SCRIPT_EXPORT,\n                              eval_py=DEFAULT_SCRIPT_EVAL):\n        script_locs = TFDeeplabConfig.ScriptLocations(\n            train_py=train_py, export_py=export_py, eval_py=eval_py)\n        b = deepcopy(self)\n        b.config[\'script_locations\'] = script_locs\n        return b\n'"
rastervision/backend/tf_object_detection.py,0,"b'import rastervision as rv\n\nimport io\nimport os\nimport shutil\nimport tarfile\nfrom os.path import join\nfrom subprocess import (Popen, PIPE, STDOUT)\nimport glob\nimport re\nimport uuid\nimport logging\nfrom copy import deepcopy\n\nfrom PIL import Image\nimport numpy as np\nfrom google.protobuf import text_format, json_format\n\nfrom rastervision.backend import Backend\nfrom rastervision.data import ObjectDetectionLabels\nfrom rastervision.utils.files import (get_local_path, upload_or_copy, make_dir,\n                                      download_if_needed, sync_to_dir,\n                                      sync_from_dir, start_sync, list_paths)\nfrom rastervision.utils.misc import (save_img, replace_nones_in_dict,\n                                     terminate_at_exit)\nfrom rastervision.rv_config import RVConfig\n\nTRAIN = \'train\'\nVALIDATION = \'validation\'\n\nlog = logging.getLogger(__name__)\n\n\ndef save_debug_image(im, labels, class_map, output_path):\n    from object_detection.utils import visualization_utils as vis_util\n\n    npboxes = labels.get_npboxes()\n    class_ids = labels.get_class_ids()\n    scores = labels.get_scores()\n    if scores is None:\n        scores = [1.0] * len(labels)\n\n    vis_util.visualize_boxes_and_labels_on_image_array(\n        im,\n        npboxes,\n        class_ids,\n        scores,\n        class_map.get_category_index(),\n        use_normalized_coordinates=True,\n        line_thickness=2,\n        max_boxes_to_draw=None)\n    save_img(im, output_path)\n\n\ndef create_tf_example(image, window, labels, class_map, chip_id=\'\'):\n    import tensorflow as tf\n    from object_detection.utils import dataset_util\n\n    image = Image.fromarray(image)\n    image_format = \'png\'\n    encoded_image = io.BytesIO()\n    image.save(encoded_image, format=image_format)\n    width, height = image.size\n\n    npboxes = labels.get_npboxes()\n    npboxes = ObjectDetectionLabels.global_to_local(npboxes, window)\n    npboxes = ObjectDetectionLabels.local_to_normalized(npboxes, window)\n    ymins = npboxes[:, 0]\n    xmins = npboxes[:, 1]\n    ymaxs = npboxes[:, 2]\n    xmaxs = npboxes[:, 3]\n    class_ids = labels.get_class_ids()\n    class_names = [\n        class_map.get_by_id(class_id).name.encode(\'utf8\')\n        for class_id in class_ids\n    ]\n\n    tf_example = tf.train.Example(\n        features=tf.train.Features(\n            feature={\n                \'image/height\':\n                dataset_util.int64_feature(height),\n                \'image/width\':\n                dataset_util.int64_feature(width),\n                \'image/filename\':\n                dataset_util.bytes_feature(chip_id.encode(\'utf8\')),\n                \'image/source_id\':\n                dataset_util.bytes_feature(chip_id.encode(\'utf8\')),\n                \'image/encoded\':\n                dataset_util.bytes_feature(encoded_image.getvalue()),\n                \'image/format\':\n                dataset_util.bytes_feature(image_format.encode(\'utf8\')),\n                \'image/object/bbox/xmin\':\n                dataset_util.float_list_feature(xmins),\n                \'image/object/bbox/xmax\':\n                dataset_util.float_list_feature(xmaxs),\n                \'image/object/bbox/ymin\':\n                dataset_util.float_list_feature(ymins),\n                \'image/object/bbox/ymax\':\n                dataset_util.float_list_feature(ymaxs),\n                \'image/object/class/text\':\n                dataset_util.bytes_list_feature(class_names),\n                \'image/object/class/label\':\n                dataset_util.int64_list_feature(class_ids)\n            }))\n\n    return tf_example\n\n\ndef write_tf_record(tf_examples, output_path: str) -> None:\n    """"""Write an array of TFRecords to the given output path.\n\n    Args:\n         tf_examples: An array of TFRecords; a\n              list(tensorflow.core.example.example_pb2.Example)\n         output_path: The path where the records should be stored.\n\n    Returns:\n         None\n\n    """"""\n    import tensorflow as tf\n\n    with tf.python_io.TFRecordWriter(output_path) as writer:\n        for tf_example in tf_examples:\n            writer.write(tf_example.SerializeToString())\n\n\ndef merge_tf_records(output_path, src_records):\n    import tensorflow as tf\n\n    with tf.python_io.TFRecordWriter(output_path) as writer:\n        log.info(\'Merging TFRecords\')\n        for src_record in src_records:\n            for string_record in tf.python_io.tf_record_iterator(src_record):\n                writer.write(string_record)\n\n\ndef make_tf_class_map(class_map):\n    from rastervision.protos.tf_object_detection.string_int_label_map_pb2 import (\n        StringIntLabelMap, StringIntLabelMapItem)\n\n    tf_class_map = StringIntLabelMap()\n    tf_items = []\n    for class_item in class_map.get_items():\n        tf_item = StringIntLabelMapItem(id=class_item.id, name=class_item.name)\n        tf_items.append(tf_item)\n    tf_class_map.item.extend(tf_items)\n    return tf_class_map\n\n\ndef save_tf_class_map(tf_class_map, class_map_path):\n    tf_class_map_str = text_format.MessageToString(tf_class_map)\n    with open(class_map_path, \'w\') as class_map_file:\n        class_map_file.write(tf_class_map_str)\n\n\ndef make_tf_examples(training_data, class_map):\n    tf_examples = []\n    log.info(\'Creating TFRecord\')\n    for chip, window, labels in training_data:\n        tf_example = create_tf_example(chip, window, labels, class_map)\n        tf_examples.append(tf_example)\n    return tf_examples\n\n\ndef parse_tfexample(example):\n    # Parse image.\n    im_str = example.features.feature[\'image/encoded\'].bytes_list.value[0]\n    im = Image.open(io.BytesIO(im_str))\n    im = np.asarray(im, dtype=np.uint8).copy()\n\n    # Parse labels.\n    xmins = example.features.feature[\'image/object/bbox/xmin\'].float_list.value\n    ymins = example.features.feature[\'image/object/bbox/ymin\'].float_list.value\n    xmaxs = example.features.feature[\'image/object/bbox/xmax\'].float_list.value\n    ymaxs = example.features.feature[\'image/object/bbox/ymax\'].float_list.value\n\n    nb_boxes = len(xmins)\n    npboxes = np.empty((nb_boxes, 4))\n    npboxes[:, 0] = ymins\n    npboxes[:, 1] = xmins\n    npboxes[:, 2] = ymaxs\n    npboxes[:, 3] = xmaxs\n\n    class_ids = example.features.feature[\n        \'image/object/class/label\'].int64_list.value\n    class_ids = np.array(class_ids)\n\n    labels = ObjectDetectionLabels(npboxes, class_ids)\n    return im, labels\n\n\ndef make_debug_images(record_path, class_map, output_dir):\n    import tensorflow as tf\n\n    make_dir(output_dir, check_empty=True)\n\n    log.info(\'Generating debug chips\')\n    tfrecord_iter = tf.python_io.tf_record_iterator(record_path)\n    for ind, example in enumerate(tfrecord_iter):\n        example = tf.train.Example.FromString(example)\n        im, labels = parse_tfexample(example)\n        # Can\'t create debug images for non-3band images\n        if im.shape[2] != 3:\n            log.warning(\n                \'WARNING: Skipping debug images - Images are not 3 band rasters.\'\n            )\n            return\n        output_path = join(output_dir, \'{}.png\'.format(ind))\n        save_debug_image(im, labels, class_map, output_path)\n\n\ndef train(config_path,\n          output_dir,\n          num_steps,\n          model_main_py=None,\n          do_monitoring=True):\n    output_train_dir = join(output_dir, \'train\')\n    output_eval_dir = join(output_dir, \'eval\')\n\n    model_main_py = model_main_py or \'/opt/tf-models/object_detection/model_main.py\'\n\n    train_cmd = [\n        \'python\', model_main_py, \'--alsologtostderr\',\n        \'--pipeline_config_path={}\'.format(config_path),\n        \'--model_dir={}\'.format(output_train_dir),\n        \'--num_train_steps={}\'.format(num_steps),\n        \'--sample_1_of_n_eval_examples={}\'.format(1)\n    ]\n\n    log.info(\'Running train command: {}\'.format(\' \'.join(train_cmd)))\n\n    train_process = Popen(train_cmd, stdout=PIPE, stderr=STDOUT)\n    terminate_at_exit(train_process)\n\n    if do_monitoring:\n        eval_cmd = [\n            \'python\', model_main_py, \'--alsologtostderr\',\n            \'--pipeline_config_path={}\'.format(config_path),\n            \'--checkpoint_dir={}\'.format(output_train_dir),\n            \'--model_dir={}\'.format(output_eval_dir)\n        ]\n        log.info(\'Running eval command: {}\'.format(\' \'.join(eval_cmd)))\n\n        # Don\'t let the eval process take up GPU space\n        env = deepcopy(os.environ)\n        env[\'CUDA_VISIBLE_DEVICES\'] = \'-1\'\n        eval_process = Popen(eval_cmd, env=env)\n\n        tensorboard_process = Popen(\n            [\'tensorboard\', \'--logdir={}\'.format(output_dir)])\n        terminate_at_exit(eval_process)\n        terminate_at_exit(tensorboard_process)\n\n    with train_process:\n        for line in train_process.stdout:\n            log.info(line.decode(\'utf-8\'))\n\n    log.info(\'-----DONE TRAINING----\')\n    if do_monitoring:\n        eval_process.terminate()\n        tensorboard_process.terminate()\n\n\ndef get_last_checkpoint_path(train_root_dir):\n    index_paths = glob.glob(join(train_root_dir, \'train\', \'*.index\'))\n    checkpoint_ids = []\n    for index_path in index_paths:\n        match = re.match(r\'model.ckpt-(\\d+).index\',\n                         os.path.basename(index_path))\n        checkpoint_ids.append(int(match.group(1)))\n\n    if len(checkpoint_ids) == 0:\n        return None\n    checkpoint_id = max(checkpoint_ids)\n    checkpoint_path = join(train_root_dir, \'train\',\n                           \'model.ckpt-{}\'.format(checkpoint_id))\n    return checkpoint_path\n\n\ndef export_inference_graph(train_root_dir,\n                           config_path,\n                           output_dir,\n                           fine_tune_checkpoint_name,\n                           export_py=None):\n    export_py = (export_py or\n                 \'/opt/tf-models/object_detection/export_inference_graph.py\')\n    checkpoint_path = get_last_checkpoint_path(train_root_dir)\n    if checkpoint_path is None:\n        log.warning(\'No checkpoints could be found.\')\n    else:\n        log.info(\'Exporting checkpoint {}...\'.format(checkpoint_path))\n\n        train_process = Popen([\n            \'python\', export_py, \'--input_type\', \'image_tensor\',\n            \'--pipeline_config_path\', config_path,\n            \'--trained_checkpoint_prefix\', checkpoint_path,\n            \'--output_directory\', output_dir\n        ])\n        train_process.wait()\n\n        inference_graph_path = join(output_dir, \'frozen_inference_graph.pb\')\n\n        # Package up the model files for usage as fine tuning checkpoints\n        model_checkpoint_files = [\n            os.path.join(output_dir, fname) for fname in os.listdir(output_dir)\n            if fname.startswith(\'model.ckpt\')\n        ]\n\n        with RVConfig.get_tmp_dir() as tmp_dir:\n            model_dir = os.path.join(tmp_dir, fine_tune_checkpoint_name)\n            make_dir(model_dir)\n            model_tar = os.path.join(\n                output_dir, \'{}.tar.gz\'.format(fine_tune_checkpoint_name))\n            shutil.copy(inference_graph_path, model_dir)\n            for path in model_checkpoint_files:\n                shutil.copy(path, model_dir)\n            with tarfile.open(model_tar, \'w:gz\') as tar:\n                tar.add(model_dir, arcname=os.path.basename(model_dir))\n\n        # Move frozen inference graph and clean up generated files.\n\n        output_path = join(output_dir, \'model\')\n        shutil.move(inference_graph_path, output_path)\n        saved_model_dir = join(output_dir, \'saved_model\')\n        shutil.rmtree(saved_model_dir)\n\n\nclass TrainingPackage(object):\n    """"""Encapsulates the files needed to train a model.\n\n    This encapsulates the files that are generated by the make_chips\n    command and that are used by the train command. It takes the URI of the\n    directory used to store these files (which is remote or local depending on\n    where the command is being executed), generates the URIs of all the\n    individual files, and downloads and uploads them. This assumes the\n    directory has the following structure:\n        label-map.pbtxt\n        train-debug-chips.zip\n        train.record\n        validation-debug-chips.zip\n        validation.record\n    """"""\n\n    def __init__(self, base_uri, config, tmp_dir, partition_id):\n        """"""Constructor.\n\n        Creates a temporary directory.\n\n        Args:\n            base_uri: (string) URI of directory containing files used to\n                train model, possibly remote\n            config: TFObjectDetectionConfig\n        """"""\n\n        self.temp_dir = tmp_dir\n\n        self.base_uri = base_uri\n        self.base_dir = self.get_local_path(base_uri)\n\n        make_dir(self.base_dir)\n\n        self.config = config\n\n        self.partition_id = partition_id\n\n        self.training_record_uri = join(\n            base_uri, \'training\',\n            \'training-{}.record\'.format(self.partition_id))\n        self.training_local_record_path = join(\n            self.base_dir, \'training-{}.record\'.format(self.partition_id))\n        self.training_download_uri = self.get_local_path(\n            join(self.base_uri, \'training\'))\n\n        self.validation_record_uri = join(\n            base_uri, \'validation\',\n            \'validation-{}.record\'.format(self.partition_id))\n        self.validation_local_record_path = join(\n            self.base_dir, \'validation-{}.record\'.format(self.partition_id))\n        self.validation_download_uri = self.get_local_path(\n            join(self.base_uri, \'validation\'))\n\n    def get_local_path(self, uri):\n        """"""Get local version of uri.\n\n        Args:\n            uri: (string) URI of file, possibly remote\n\n        Returns:\n            (string) path of local version of file\n        """"""\n        return get_local_path(uri, self.temp_dir)\n\n    def upload_or_copy(self, uri):\n        """"""Upload file if it\'s remote.\n\n        This knows how to generate the path to the local copy of the file.\n\n        Args:\n            uri: (string) URI of file, possibly remote\n        """"""\n        upload_or_copy(self.get_local_path(uri), uri)\n\n    def download_if_needed(self, uri):\n        """"""Download file if it\'s remote.\n\n        Args:\n            uri: (string) URI of file, possibly remote\n\n        Returns:\n            (string) path of local file that was downloaded\n        """"""\n        return download_if_needed(uri, self.temp_dir)\n\n    def get_debug_chips_uri(self, split):\n        """"""Get URI of debug chips zip file for dataset split.\n\n        Args:\n            split: (string) \'train\' or \'validation\'\n\n        Returns:\n            (string) URI of zip file containing debug chips, possibly remote\n        """"""\n        return join(self.base_uri, \'{}-debug/chips-{}.zip\'.format(\n            split, self.partition_id))\n\n    def upload(self, debug=False):\n        """"""Upload training and validation data, and class map files.\n\n        Args:\n            debug: (bool) if True, also upload the corresponding debug chip\n                zip files\n        """"""\n        if os.path.exists(self.training_local_record_path):\n            upload_or_copy(self.training_local_record_path,\n                           self.training_record_uri)\n        if os.path.exists(self.validation_local_record_path):\n            upload_or_copy(self.validation_local_record_path,\n                           self.validation_record_uri)\n        if debug:\n            if os.path.exists(self.get_debug_chips_uri(TRAIN)):\n                self.upload_or_copy(self.get_debug_chips_uri(TRAIN))\n            if os.path.exists(self.get_debug_chips_uri(VALIDATION)):\n                self.upload_or_copy(self.get_debug_chips_uri(VALIDATION))\n\n    def download_data(self):\n        """"""Download training and validation data, and class map files.""""""\n\n        def _download(split, output_dir):\n            for uri in list_paths(self.base_uri, \'record\'):\n                base_name = os.path.basename(uri)\n                if base_name.startswith(split):\n                    record_path = self.download_if_needed(uri)\n                    target_record_path = os.path.join(\n                        output_dir, os.path.basename(record_path))\n                    shutil.move(record_path, target_record_path)\n\n        _download(\'training\', self.training_download_uri)\n        _download(\'validation\', self.validation_download_uri)\n\n    def download_pretrained_model(self, pretrained_model_zip_uri):\n        """"""Download pretrained model and unzip it.\n\n        This is used before training a model.\n\n        Args:\n            pretrained_model_zip_uri: (string) URI of .tar.gz file containing\n                pretrained model. This file is of the form that comes from the\n                Model Zoo at https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md # noqa\n                It contains a directory at the top level with the same name as\n                root of the zip file (if zip file is x.tar.gz, the directory\n                is x), and a set of files of the form model.ckpt.*. This file\n                can be located anywhere, and is not expected to be in the\n                directory encapsulated by this class that is generated by the\n                make_chips command. That is why it is passed in\n                separately.\n\n        Returns:\n            (string) path to pretrained model file (which is model.ckpt in\n                the zip file)\n        """"""\n        pretrained_model_zip_path = self.download_if_needed(\n            pretrained_model_zip_uri)\n        pretrained_model_dir = join(self.temp_dir, \'pretrained_model\')\n        make_dir(pretrained_model_dir)\n        with tarfile.open(pretrained_model_zip_path, \'r:gz\') as tar:\n            tar.extractall(pretrained_model_dir)\n        model_name = os.path.splitext(\n            os.path.splitext(os.path.basename(pretrained_model_zip_uri))[0])[0]\n        # The unzipped file is assumed to have a single directory with\n        # the name of the model derived from the zip file.\n        pretrained_model_path = join(pretrained_model_dir, model_name,\n                                     \'model.ckpt\')\n        return pretrained_model_path\n\n    def download_config(self, class_map):\n        from rastervision.protos.tf_object_detection.pipeline_pb2 \\\n            import TrainEvalPipelineConfig\n        """"""Download a model and backend config and update its fields.\n\n        This is used before training a model. This downloads and unzips a bunch\n        of files that are needed to train a model, and then downloads and\n        updates the backend config file with local paths to these files. These\n        files include the pretrained model, the class map, and the training and\n        validation datasets.\n\n        Args:\n            pretrained_model_zip_uri: (string) URI of .tar.gz file containing\n                pretrained model. (See download_pretrained_model method for more\n                details.)\n            backend_config_uri: (string) URI of backend config file which is\n                a config file for the TF Object Detection API. Examples can be\n                found here https://github.com/tensorflow/models/tree/master/research/object_detection/samples/configs  # noqa\n        """"""\n\n        # Parse configuration\n        # We must remove \'nulls\' that appear due to translating empty\n        # messages. These appear when translating between text and JSON based\n        # protobuf messages, and using the google.protobuf.Struct type to store\n        # the JSON. This appears when TFOD uses empty message types as an enum.\n        config = json_format.ParseDict(\n            replace_nones_in_dict(self.config.tfod_config, {}),\n            TrainEvalPipelineConfig())\n\n        # Update config using local paths.\n        if config.train_config.fine_tune_checkpoint:\n            pretrained_model_path = self.download_pretrained_model(\n                config.train_config.fine_tune_checkpoint)\n            config.train_config.fine_tune_checkpoint = pretrained_model_path\n\n        # Save TF label map based on class_map.\n        class_map_path = os.path.join(self.temp_dir, \'label-map.pbtxt\')\n        tf_class_map = make_tf_class_map(class_map)\n        save_tf_class_map(tf_class_map, class_map_path)\n\n        train_record_uris = list_paths(self.training_download_uri, \'record\')\n        config.train_input_reader.tf_record_input_reader.input_path[:] = train_record_uris\n        config.train_input_reader.label_map_path = class_map_path\n\n        eval_record_uris = list_paths(self.validation_download_uri, \'record\')\n        config.eval_input_reader[\n            0].tf_record_input_reader.input_path[:] = eval_record_uris\n        config.eval_input_reader[0].label_map_path = class_map_path\n\n        # Save an updated copy of the config file.\n        config_path = join(self.temp_dir, \'ml.config\')\n        config_str = text_format.MessageToString(config)\n        with open(config_path, \'w\') as config_file:\n            config_file.write(config_str)\n        return config_path\n\n\ndef load_frozen_graph(inference_graph_path):\n    import tensorflow as tf\n\n    detection_graph = tf.Graph()\n    with detection_graph.as_default():\n        od_graph_def = tf.GraphDef()\n        with tf.gfile.GFile(inference_graph_path, \'rb\') as fid:\n            serialized_graph = fid.read()\n            od_graph_def.ParseFromString(serialized_graph)\n            tf.import_graph_def(od_graph_def, name=\'\')\n    return detection_graph\n\n\ndef compute_prediction(image_nps, windows, detection_graph, session):\n    image_tensor = detection_graph.get_tensor_by_name(\'image_tensor:0\')\n    boxes = detection_graph.get_tensor_by_name(\'detection_boxes:0\')\n    scores = detection_graph.get_tensor_by_name(\'detection_scores:0\')\n    class_ids = detection_graph.get_tensor_by_name(\'detection_classes:0\')\n\n    (boxes, scores, class_ids) = session.run(\n        [boxes, scores, class_ids], feed_dict={image_tensor: image_nps})\n\n    labels = ObjectDetectionLabels.make_empty()\n    for chip_boxes, chip_scores, chip_class_ids, window in zip(\n            boxes, scores, class_ids, windows):\n        chip_boxes = ObjectDetectionLabels.normalized_to_local(\n            chip_boxes, window)\n        chip_boxes = ObjectDetectionLabels.local_to_global(chip_boxes, window)\n        chip_class_ids = chip_class_ids.astype(np.int32)\n        labels = (labels + ObjectDetectionLabels(\n            chip_boxes, chip_class_ids, scores=chip_scores))\n\n    return labels\n\n\nclass TFObjectDetection(Backend):\n    def __init__(self, backend_config, task_config):\n        self.detection_graph = None\n        # persist scene training packages for when output_uri is remote\n        self.scene_training_packages = []\n        self.config = backend_config\n        self.class_map = task_config.class_map\n\n        self.partition_id = uuid.uuid4()\n\n    def process_scene_data(self, scene, data, tmp_dir):\n        """"""Process each scene\'s training data\n\n        Args:\n            scene: Scene\n            data: TrainingData\n            class_map: ClassMap\n\n        Returns:\n            the local path to the scene\'s TFRecord\n        """"""\n        # Currently TF Object Detection can only handle uint8\n        if scene.raster_source.get_dtype() != np.uint8:\n            raise Exception(\'Cannot use {} backend for imagery that does \'\n                            \'not have data type uint8. \'\n                            \'Use the StatsAnalyzer and StatsTransformer \'\n                            \'to turn the raster data into uint8 data\'.format(\n                                rv.TF_OBJECT_DETECTION))\n\n        training_package = TrainingPackage(self.config.training_data_uri,\n                                           self.config, tmp_dir,\n                                           self.partition_id)\n        self.scene_training_packages.append(training_package)\n        tf_examples = make_tf_examples(data, self.class_map)\n        # Ensure directory is unique since scene id\'s could be shared between\n        # training and test sets.\n        record_path = os.path.join(\n            tmp_dir, \'{}-{}.record\'.format(scene.id, uuid.uuid4()))\n        log.debug(\'Writing to record path {}\'.format(record_path))\n        write_tf_record(tf_examples, record_path)\n\n        return record_path\n\n    def process_sceneset_results(self, training_results, validation_results,\n                                 tmp_dir):\n        """"""After all scenes have been processed, merge all TFRecords\n\n        Args:\n            training_results: list of training scenes\' TFRecords\n            validation_results: list of validation scenes\' TFRecords\n        """"""\n        training_package = TrainingPackage(self.config.training_data_uri,\n                                           self.config, tmp_dir,\n                                           self.partition_id)\n\n        def _merge_training_results(results, record_path, split):\n            # merge each scene\'s tfrecord into ""split"" tf record\n            merge_tf_records(record_path, results)\n\n            # Save debug chips.\n            if self.config.debug:\n                debug_zip_path = training_package.get_local_path(\n                    training_package.get_debug_chips_uri(split))\n                with RVConfig.get_tmp_dir() as debug_dir:\n                    make_debug_images(record_path, self.class_map, debug_dir)\n                    shutil.make_archive(\n                        os.path.splitext(debug_zip_path)[0], \'zip\', debug_dir)\n\n        if training_results:\n            _merge_training_results(\n                training_results, training_package.training_local_record_path,\n                TRAIN)\n        else:\n            log.warn(\'No training chips for partition {}\'.format(\n                training_package.partition_id))\n\n        if validation_results:\n            _merge_training_results(\n                validation_results,\n                training_package.validation_local_record_path, VALIDATION)\n        else:\n            log.warn(\'No validation chips for partition {}\'.format(\n                training_package.partition_id))\n\n        training_package.upload(debug=self.config.debug)\n\n        # clear scene training packages\n        del self.scene_training_packages[:]\n\n    def train(self, tmp_dir):\n        training_package = TrainingPackage(self.config.training_data_uri,\n                                           self.config, tmp_dir,\n                                           self.partition_id)\n        # Download training data and update config file.\n        training_package.download_data()\n        config_path = training_package.download_config(self.class_map)\n\n        # Setup output dirs.\n        output_dir = get_local_path(self.config.training_output_uri, tmp_dir)\n        make_dir(output_dir)\n\n        # Get output from potential previous run so we can resume training.\n        if not self.config.train_options.replace_model:\n            sync_from_dir(self.config.training_output_uri, output_dir)\n        else:\n            for f in os.listdir(output_dir):\n                if not f.startswith(\'command-config\'):\n                    path = os.path.join(output_dir, f)\n                    if os.path.isfile(path):\n                        os.remove(path)\n                    else:\n                        shutil.rmtree(path)\n\n        local_config_path = os.path.join(output_dir, \'pipeline.config\')\n        shutil.copy(config_path, local_config_path)\n\n        model_main_py = self.config.script_locations.model_main_uri\n        export_py = self.config.script_locations.export_uri\n\n        # Train model and sync output periodically.\n        sync = start_sync(\n            output_dir,\n            self.config.training_output_uri,\n            sync_interval=self.config.train_options.sync_interval)\n        with sync:\n            train(\n                local_config_path,\n                output_dir,\n                self.config.get_num_steps(),\n                model_main_py=model_main_py,\n                do_monitoring=self.config.train_options.do_monitoring)\n\n        export_inference_graph(\n            output_dir,\n            local_config_path,\n            output_dir,\n            fine_tune_checkpoint_name=self.config.fine_tune_checkpoint_name,\n            export_py=export_py)\n\n        # Perform final sync\n        sync_to_dir(output_dir, self.config.training_output_uri)\n\n    def load_model(self, tmp_dir):\n        import tensorflow as tf\n\n        # Load and memoize the detection graph and TF session.\n        if self.detection_graph is None:\n            model_path = download_if_needed(self.config.model_uri, tmp_dir)\n            self.detection_graph = load_frozen_graph(model_path)\n            self.session = tf.Session(graph=self.detection_graph)\n\n    def predict(self, chips, windows, tmp_dir):\n        # Ensure model is loaded\n        self.load_model(tmp_dir)\n\n        return compute_prediction(chips, windows, self.detection_graph,\n                                  self.session)\n'"
rastervision/backend/tf_object_detection_config.py,0,"b'import os\nfrom copy import deepcopy\nfrom google.protobuf import (text_format, json_format)\n\nimport rastervision as rv\nfrom rastervision.backend import (BackendConfig, BackendConfigBuilder)\nfrom rastervision.protos.backend_pb2 import BackendConfig as BackendConfigMsg\nfrom rastervision.protos.tf_object_detection.pipeline_pb2 import TrainEvalPipelineConfig\nfrom rastervision.utils.files import file_to_str\nfrom rastervision.utils.misc import set_nested_keys\nfrom rastervision.task import ObjectDetectionConfig\n\n# Default location to Tensorflow Object Detection\'s scripts.\nDEFAULT_SCRIPT_TRAIN = \'/opt/tf-models/object_detection/model_main.py\'\nDEFAULT_SCRIPT_EXPORT = \'/opt/tf-models/object_detection/export_inference_graph.py\'\nCHIP_OUTPUT_FILES = [\'training\', \'validation\']\nDEBUG_CHIP_OUTPUT_FILES = [\n    \'train-debug-chips.zip\', \'validation-debug-chips.zip\'\n]\n\n\nclass TFObjectDetectionConfig(BackendConfig):\n    class TrainOptions:\n        def __init__(self,\n                     sync_interval=600,\n                     do_monitoring=True,\n                     replace_model=False):\n            self.sync_interval = sync_interval\n            self.do_monitoring = do_monitoring\n            self.replace_model = replace_model\n\n    class ScriptLocations:\n        def __init__(self,\n                     model_main_uri=DEFAULT_SCRIPT_TRAIN,\n                     export_uri=DEFAULT_SCRIPT_EXPORT):\n            self.model_main_uri = model_main_uri\n            self.export_uri = export_uri\n\n    def __init__(self,\n                 tfod_config,\n                 pretrained_model_uri=None,\n                 train_options=None,\n                 script_locations=None,\n                 debug=False,\n                 training_data_uri=None,\n                 training_output_uri=None,\n                 model_uri=None,\n                 fine_tune_checkpoint_name=None):\n        if train_options is None:\n            train_options = TFObjectDetectionConfig.TrainOptions()\n        if script_locations is None:\n            script_locations = TFObjectDetectionConfig.ScriptLocations()\n\n        super().__init__(rv.TF_OBJECT_DETECTION, pretrained_model_uri)\n        self.tfod_config = tfod_config\n        self.pretrained_model_uri = pretrained_model_uri\n        self.train_options = train_options\n        self.script_locations = script_locations\n        self.debug = debug\n\n        # Internally set from command preprocessing\n        self.training_data_uri = training_data_uri\n        self.training_output_uri = training_output_uri\n        self.model_uri = model_uri\n        self.fine_tune_checkpoint_name = fine_tune_checkpoint_name\n\n    def create_backend(self, task_config):\n        from rastervision.backend.tf_object_detection import TFObjectDetection\n        return TFObjectDetection(self, task_config)\n\n    def get_num_steps(self):\n        return int(self.tfod_config[\'trainConfig\'][\'numSteps\'])\n\n    def get_batch_size(self, batch_size):\n        return int(self.tfod_config.train_config[\'trainConfig\'][\'batchSize\'])\n\n    def to_proto(self):\n        d = {\n            \'sync_interval\': self.train_options.sync_interval,\n            \'do_monitoring\': self.train_options.do_monitoring,\n            \'replace_model\': self.train_options.replace_model,\n            \'model_main_py\': self.script_locations.model_main_uri,\n            \'export_py\': self.script_locations.export_uri,\n            \'training_data_uri\': self.training_data_uri,\n            \'training_output_uri\': self.training_output_uri,\n            \'model_uri\': self.model_uri,\n            \'debug\': self.debug,\n            \'fine_tune_checkpoint_name\': self.fine_tune_checkpoint_name,\n            \'tfod_config\': self.tfod_config\n        }\n\n        conf = json_format.ParseDict(\n            d, BackendConfigMsg.TFObjectDetectionConfig())\n\n        msg = BackendConfigMsg(\n            backend_type=rv.TF_OBJECT_DETECTION,\n            tf_object_detection_config=conf)\n\n        if self.pretrained_model_uri:\n            msg.MergeFrom(\n                BackendConfigMsg(\n                    pretrained_model_uri=self.pretrained_model_uri))\n\n        return msg\n\n    def save_bundle_files(self, bundle_dir):\n        if not self.model_uri:\n            raise rv.ConfigError(\'model_uri is not set.\')\n        local_path, base_name = self.bundle_file(self.model_uri, bundle_dir)\n        new_config = self.to_builder() \\\n                         .with_model_uri(base_name) \\\n                         .build()\n        return (new_config, [local_path])\n\n    def load_bundle_files(self, bundle_dir):\n        if not self.model_uri:\n            raise rv.ConfigError(\'model_uri is not set.\')\n        local_model_uri = os.path.join(bundle_dir, self.model_uri)\n        return self.to_builder() \\\n                   .with_model_uri(local_model_uri) \\\n                   .build()\n\n    def update_for_command(self, command_type, experiment_config,\n                           context=None):\n        super().update_for_command(command_type, experiment_config, context)\n        if command_type == rv.CHIP:\n            if not self.training_data_uri:\n                self.training_data_uri = experiment_config.chip_uri\n\n        if command_type == rv.TRAIN:\n            if not self.training_output_uri:\n                self.training_output_uri = experiment_config.train_uri\n            if not self.model_uri:\n                self.model_uri = os.path.join(self.training_output_uri,\n                                              \'model\')\n            if not self.fine_tune_checkpoint_name:\n                # Set the fine tune checkpoint name to the experiment id\n                self.fine_tune_checkpoint_name = experiment_config.id\n\n    def report_io(self, command_type, io_def):\n        super().report_io(command_type, io_def)\n        if command_type == rv.CHIP:\n            outputs = list(\n                map(lambda x: os.path.join(self.training_data_uri, x),\n                    CHIP_OUTPUT_FILES))\n            if self.debug:\n                outputs.extend(\n                    list(\n                        map(lambda x: os.path.join(self.training_data_uri, x),\n                            DEBUG_CHIP_OUTPUT_FILES)))\n            io_def.add_outputs(outputs)\n\n        if command_type == rv.TRAIN:\n            if not self.training_data_uri:\n                io_def.add_missing(\'Missing training_data_uri.\')\n            else:\n                inputs = list(\n                    map(lambda x: os.path.join(self.training_data_uri, x),\n                        CHIP_OUTPUT_FILES))\n                io_def.add_inputs(inputs)\n\n            io_def.add_output(self.model_uri)\n            io_def.add_output(self.fine_tune_checkpoint_name)\n\n        if command_type in [rv.PREDICT, rv.BUNDLE]:\n            if not self.model_uri:\n                io_def.add_missing(\'Missing model_uri.\')\n            else:\n                io_def.add_input(self.model_uri)\n\n\nclass TFObjectDetectionConfigBuilder(BackendConfigBuilder):\n    def __init__(self, prev=None):\n        config = {}\n        if prev:\n            config = {\n                \'tfod_config\': prev.tfod_config,\n                \'pretrained_model_uri\': prev.pretrained_model_uri,\n                \'train_options\': prev.train_options,\n                \'script_locations\': prev.script_locations,\n                \'debug\': prev.debug,\n                \'training_data_uri\': prev.training_data_uri,\n                \'training_output_uri\': prev.training_output_uri,\n                \'model_uri\': prev.model_uri,\n                \'fine_tune_checkpoint_name\': prev.fine_tune_checkpoint_name\n            }\n        super().__init__(rv.TF_OBJECT_DETECTION, TFObjectDetectionConfig,\n                         config, prev)\n        self.config_mods = []\n        self.require_task = prev is None\n\n    def from_proto(self, msg):\n        b = super().from_proto(msg)\n        conf = msg.tf_object_detection_config\n        # Since this is coming from a serialized message,\n        # assume the task has already been set and do not\n        # require it during validation.\n        b.require_task = False\n        b = b.with_train_options(\n            sync_interval=conf.sync_interval,\n            do_monitoring=conf.do_monitoring,\n            replace_model=conf.replace_model)\n        b = b.with_script_locations(\n            model_main_uri=conf.model_main_py, export_uri=conf.export_py)\n        b = b.with_training_data_uri(conf.training_data_uri)\n        b = b.with_training_output_uri(conf.training_output_uri)\n        b = b.with_model_uri(conf.model_uri)\n        b = b.with_fine_tune_checkpoint_name(conf.fine_tune_checkpoint_name)\n        b = b.with_debug(conf.debug)\n\n        return b.with_template(json_format.MessageToDict(conf.tfod_config))\n\n    def validate(self):\n        super().validate()\n        if not self.config.get(\'tfod_config\'):\n            raise rv.ConfigError(\'You must specify a template for the backend \'\n                                 ""configuration - use \'with_template\'."")\n        if not isinstance(self.config.get(\'tfod_config\'), dict):\n            raise rv.ConfigError(\n                \'tfod_config must be of type dict, got {}\'.format(\n                    type(self.config.get(\'tfod_config\'))))\n        if self.require_task and not self.task:\n            raise rv.ConfigError(\'You must specify the task this backend \'\n                                 ""is for - use \'with_task\'."")\n        if self.require_task and not isinstance(self.task,\n                                                ObjectDetectionConfig):\n            raise rv.ConfigError(\n                \'Task set with with_task must be of type ObjectDetectionConfig, got {}.\'.\n                format(type(self.task)))\n        return True\n\n    def build(self):\n        """"""Build this configuration.\n\n        Set any values into the TF object detection pipeline config as\n        necessary.\n        """"""\n        self.validate()\n\n        b = deepcopy(self)\n\n        # Check if a pretrained model was assigned.\n        pretrained_model = b.config.get(\'pretrained_model_uri\')\n        if pretrained_model:\n            b = b.with_config({\'fineTuneCheckpoint\': pretrained_model})\n        else:\n            b = b.with_config(\n                {\n                    \'fineTuneCheckpoint\': \'\'\n                }, ignore_missing_keys=True)\n\n        for config_mod, ignore_missing_keys, set_missing_keys in b.config_mods:\n            try:\n                set_nested_keys(b.config[\'tfod_config\'], config_mod,\n                                ignore_missing_keys, set_missing_keys)\n            except Exception as e:\n                raise rv.ConfigError(\n                    \'Error setting configuration {}\'.format(config_mod)) from e\n\n        return TFObjectDetectionConfig(**b.config)\n\n    def _applicable_tasks(self):\n        return [rv.OBJECT_DETECTION]\n\n    def _process_task(self):\n        return self.with_config(\n            {\n                \'numClasses\': len(self.task.class_map.get_items()),\n                \'imageResizer\': {\n                    \'fixedShapeResizer\': {\n                        \'height\': self.task.chip_size,\n                        \'width\': self.task.chip_size\n                    },\n                    \'keepAspectRatioResizer\': {\n                        \'minDimension\': self.task.chip_size,\n                        \'maxDimension\': self.task.chip_size\n                    }\n                }\n            },\n            ignore_missing_keys=True)\n\n    def _load_model_defaults(self, model_defaults):\n        """"""Loads defaults. Expected keys are ""pretrained_model_uri"" and ""pipeline_config_uri"",\n           neither of which is required.\n        """"""\n        expected_keys = [\'pretrained_model_uri\', \'pipeline_config_uri\']\n        unknown_keys = set(model_defaults.keys()) - set(expected_keys)\n        if unknown_keys:\n            raise rv.ConfigError(\'Unexpected keys in model defaults:\'\n                                 \' {}. Expected keys: {}\'.format(\n                                     unknown_keys, expected_keys))\n\n        b = self\n        if \'pretrained_model_uri\' in model_defaults:\n            b = b.with_pretrained_model(model_defaults[\'pretrained_model_uri\'])\n        if \'pipeline_config_uri\' in model_defaults:\n            b = b.with_template(model_defaults[\'pipeline_config_uri\'])\n        return b\n\n    def with_template(self, template):\n        """"""Use a template for TF Object Detection pipeline config.\n\n        Args:\n            template: A dict, string or uri as the base for the TF\n                Object Detection API model training pipeline, for example those\n                found here:\n                https://github.com/tensorflow/models/tree/eef6bb5bd3b3cd5fcf54306bf29750b7f9f9a5ea/research/object_detection/samples/configs # noqa\n        """"""\n        template_json = None\n        if type(template) is dict:\n            template_json = template\n        else:\n            # Try parsing the string as a message, on fail assume it\'s a URI\n            msg = None\n            try:\n                msg = text_format.Parse(template, TrainEvalPipelineConfig())\n            except text_format.ParseError:\n                msg = text_format.Parse(\n                    file_to_str(template), TrainEvalPipelineConfig())\n            template_json = json_format.MessageToDict(msg)\n\n        b = deepcopy(self)\n        b.config[\'tfod_config\'] = template_json\n        return b\n\n    def with_batch_size(self, batch_size):\n        """"""Sets the training batch size.""""""\n        return self.with_config(\n            {\n                \'trainConfig\': {\n                    \'batchSize\': batch_size\n                }\n            }, set_missing_keys=True)\n\n    def with_num_steps(self, num_steps):\n        """"""Sets the number of training steps.""""""\n        return self.with_config(\n            {\n                \'trainConfig\': {\n                    \'numSteps\': num_steps\n                }\n            }, set_missing_keys=True)\n\n    def with_config(self,\n                    config_mod,\n                    ignore_missing_keys=False,\n                    set_missing_keys=False):\n        """"""Given a dict, modify the tensorflow pipeline configuration.\n\n        Modify it such that keys that are found recursively in the\n        configuration are replaced with those values. TODO: better explanation.\n        """"""\n        b = deepcopy(self)\n        b.config_mods.append((config_mod, ignore_missing_keys,\n                              set_missing_keys))\n        return b\n\n    def with_debug(self, debug):\n        """"""Sets the debug flag for this backend.\n        """"""\n        b = deepcopy(self)\n        b.config[\'debug\'] = debug\n        return b\n\n    def with_training_data_uri(self, training_data_uri):\n        """"""Whence comes the training data?\n\n        Args:\n            training_data_uri: The location of the training data.\n        """"""\n        b = deepcopy(self)\n        b.config[\'training_data_uri\'] = training_data_uri\n        return b\n\n    def with_training_output_uri(self, training_output_uri):\n        """"""Whither goes the training output?\n\n        Args:\n            training_output_uri: The location where the training\n                output will be stored.\n        """"""\n        b = deepcopy(self)\n        b.config[\'training_output_uri\'] = training_output_uri\n        return b\n\n    def with_model_uri(self, model_uri):\n        """"""Defines the name of the model file that will be created for\n        this model after training.\n\n        """"""\n        b = deepcopy(self)\n        b.config[\'model_uri\'] = model_uri\n        return b\n\n    def with_fine_tune_checkpoint_name(self, fine_tune_checkpoint_name):\n        """"""Defines the name of the fine tune checkpoint for this model.""""""\n        b = deepcopy(self)\n        b.config[\'fine_tune_checkpoint_name\'] = fine_tune_checkpoint_name\n        return b\n\n    def with_train_options(self,\n                           sync_interval=600,\n                           do_monitoring=True,\n                           replace_model=False):\n        """"""Sets the train options for this backend.\n\n       Args:\n          sync_interval: How often to sync output of training\n                         to the cloud (in seconds).\n\n          do_monitoring: Run process to monitor training (eg. Tensorboard)\n\n          replace_model: Replace the model checkpoint if exists.\n                         If false, this will continue training from\n                         checkpoing if exists, if the backend allows for this.\n        """"""\n        b = deepcopy(self)\n        b.config[\'train_options\'] = TFObjectDetectionConfig.TrainOptions(\n            sync_interval, do_monitoring, replace_model)\n        return b\n\n    def with_script_locations(self,\n                              model_main_uri=DEFAULT_SCRIPT_TRAIN,\n                              export_uri=DEFAULT_SCRIPT_EXPORT):\n        sl = TFObjectDetectionConfig.ScriptLocations(model_main_uri,\n                                                     export_uri)\n        b = deepcopy(self)\n        b.config[\'script_locations\'] = sl\n        return b\n'"
rastervision/cli/__init__.py,0,b'# flake8: noqa\n\nfrom rastervision.cli.verbosity import Verbosity\n'
rastervision/cli/main.py,0,"b'""""""Raster Vision main program""""""\nimport sys\nimport os\n\nimport click\nimport logging\n\nimport rastervision as rv\nfrom rastervision.experiment import (ExperimentLoader, LoaderError)\nfrom rastervision.runner import (ExperimentRunner)\nfrom rastervision.rv_config import RVConfig\n\nlog = logging.getLogger(__name__)\n\n\ndef print_error(msg):\n    click.echo(click.style(msg, fg=\'red\'), err=True)\n\n\n@click.group()\n@click.option(\n    \'--profile\', \'-p\', help=\'Sets the configuration profile name to use.\')\n@click.option(\n    \'-v\', \'--verbose\', help=\'Sets the output to  be verbose.\', count=True)\ndef main(profile, verbose):\n    # Make sure current directory is on PYTHON_PATH\n    # so that we can run against modules in current dir.\n    sys.path.append(os.curdir)\n\n    # Initialize configuration\n    rv._registry.initialize_config(profile=profile, verbosity=verbose + 1)\n\n\n@main.command(\n    \'run\', short_help=\'Run Raster Vision commands against Experiments.\')\n@click.argument(\'runner\')\n@click.argument(\'commands\', nargs=-1)\n@click.option(\n    \'--experiment_module\',\n    \'-e\',\n    help=(\'Name of an importable module to look for experiment sets \'\n          \'in. If not supplied, experiments will be loaded \'\n          \'from __main__\'))\n@click.option(\n    \'--path\',\n    \'-p\',\n    metavar=\'PATTERN\',\n    help=(\'Path of file containing ExprimentSet to run.\'))\n@click.option(\n    \'--dry-run\',\n    \'-n\',\n    is_flag=True,\n    help=(\'Execute a dry run, which will print out information \'\n          \'about the commands to be run, but will not actually \'\n          \'run the commands\'))\n@click.option(\n    \'--skip-file-check\',\n    \'-x\',\n    is_flag=True,\n    help=(\'Skip the step that verifies that file exist.\'))\n@click.option(\n    \'--arg\',\n    \'-a\',\n    type=(str, str),\n    multiple=True,\n    metavar=\'KEY VALUE\',\n    help=(\'Pass a parameter to the experiments if the method \'\n          \'parameter list takes in a parameter with that key. \'\n          \'Multiple args can be supplied\'))\n@click.option(\n    \'--prefix\',\n    metavar=\'PREFIX\',\n    default=\'exp_\',\n    help=(\'Prefix for methods containing experiments. (default: ""exp_"")\'))\n@click.option(\n    \'--method\',\n    \'-m\',\n    \'methods\',\n    multiple=True,\n    metavar=\'PATTERN\',\n    help=(\'Pattern to match method names to run.\'))\n@click.option(\n    \'--filter\',\n    \'-f\',\n    \'filters\',\n    multiple=True,\n    metavar=\'PATTERN\',\n    help=(\'Pattern to match experiment names to run.\'))\n@click.option(\n    \'--rerun\',\n    \'-r\',\n    is_flag=True,\n    default=False,\n    help=(\'Rerun commands, regardless if \'\n          \'their output files already exist.\'))\n@click.option(\'--tempdir\', help=(\'Temporary directory to use for this run.\'))\n@click.option(\n    \'--splits\',\n    \'-s\',\n    default=1,\n    metavar=\'INTEGER\',\n    help=(\'The number of processes to attempt to split each stage into.\'))\ndef run(runner, commands, experiment_module, dry_run, skip_file_check, arg,\n        prefix, methods, path, filters, rerun, tempdir, splits):\n    """"""Run Raster Vision commands from experiments, using the\n    experiment runner named RUNNER.""""""\n\n    if tempdir:\n        RVConfig.set_tmp_dir(tempdir)\n\n    # Validate runner\n    valid_runners = list(\n        map(lambda x: x.lower(), rv.ExperimentRunner.list_runners()))\n    if runner not in valid_runners:\n        print_error(\'Invalid experiment runner: ""{}"". \'\n                    \'Must be one of: ""{}""\'.format(runner,\n                                                  \'"", ""\'.join(valid_runners)))\n        sys.exit(1)\n\n    runner = ExperimentRunner.get_runner(runner)\n\n    if experiment_module and path:\n        print_error(\'Must specify only one of experiment_module or path\')\n        sys.exit(1)\n\n    if not commands:\n        commands = rv.all_commands()\n    else:\n        commands = list(map(lambda x: x.upper(), commands))\n\n    experiment_args = {}\n    for k, v in arg:\n        experiment_args[k] = v\n\n    loader = ExperimentLoader(\n        experiment_args=experiment_args,\n        experiment_method_prefix=prefix,\n        experiment_method_patterns=methods,\n        experiment_name_patterns=filters)\n    try:\n        if experiment_module:\n            experiments, command_configs = loader.load_from_module(\n                experiment_module)\n        elif path:\n            experiments, command_configs = loader.load_from_file(path)\n        else:\n            experiments, command_configs = loader.load_from_module(\'__main__\')\n    except LoaderError as e:\n        print_error(str(e))\n        sys.exit(1)\n\n    if not experiments and not commands:\n        if experiment_module:\n            print_error(\n                \'No experiments found in {}.\'.format(experiment_module))\n        elif path:\n            print_error(\'No experiments found in {}.\'.format(path))\n        else:\n            print_error(\'No experiments found.\')\n\n    runner.run(\n        experiments,\n        command_configs=command_configs,\n        commands_to_run=commands,\n        rerun_commands=rerun,\n        skip_file_check=skip_file_check,\n        dry_run=dry_run,\n        splits=splits)\n\n\n@main.command()\n@click.option(\n    \'--experiment-module\',\n    \'-e\',\n    help=(\'Name of an importable module to look for experiment sets \'\n          \'in. If not supplied, experiments will be loaded \'\n          \'from __main__\'))\n@click.option(\n    \'--arg\',\n    \'-a\',\n    type=(str, str),\n    multiple=True,\n    metavar=\'KEY VALUE\',\n    help=(\'Pass a parameter to the experiments if the method \'\n          \'parameter list takes in a parameter with that key. \'\n          \'Multiple args can be supplied\'))\ndef ls(experiment_module, arg):\n    """"""Print out a list of Experiment IDs.""""""\n    if experiment_module:\n        module_to_load = experiment_module\n    else:\n        module_to_load = \'__main__\'\n\n    experiment_args = {}\n    for k, v in arg:\n        experiment_args[k] = v\n\n    loader = ExperimentLoader(experiment_args=experiment_args)\n    try:\n        experiments = loader.load_from_module(module_to_load)\n    except LoaderError as e:\n        print_error(str(e))\n        sys.exit(1)\n\n    if not experiments:\n        if experiment_module:\n            print_error(\n                \'No experiments found in {}.\'.format(experiment_module))\n        else:\n            print_error(\'No experiments found.\')\n\n    for e in experiments:\n        click.echo(\'{}\'.format(e.id))\n\n\n# https://stackoverflow.com/questions/48391777/nargs-equivalent-for-options-in-click\nclass OptionEatAll(click.Option):\n    def __init__(self, *args, **kwargs):\n        self.save_other_options = kwargs.pop(\'save_other_options\', True)\n        nargs = kwargs.pop(\'nargs\', -1)\n        assert nargs == -1, \'nargs, if set, must be -1 not {}\'.format(nargs)\n        super(OptionEatAll, self).__init__(*args, **kwargs)\n        self._previous_parser_process = None\n        self._eat_all_parser = None\n\n    def add_to_parser(self, parser, ctx):\n        def parser_process(value, state):\n            value = str(value)\n            while state.rargs:\n                value = \'{} {}\'.format(value, state.rargs.pop(0))\n            self._previous_parser_process(value, state)\n\n        retval = super(OptionEatAll, self).add_to_parser(parser, ctx)\n\n        for name in self.opts:\n            our_parser = parser._long_opt.get(name) or parser._short_opt.get(\n                name)\n            if our_parser:\n                self._eat_all_parser = our_parser\n                self._previous_parser_process = our_parser.process\n                our_parser.process = parser_process\n                break\n\n        return retval\n\n\n@main.command(\n    \'predict\', short_help=\'Make predictions using a predict package.\')\n@click.argument(\'predict_package\')\n@click.argument(\'image_uri\')\n@click.argument(\'output_uri\')\n@click.option(\n    \'--update-stats\',\n    \'-a\',\n    is_flag=True,\n    help=(\'Run an analysis on this individual image, as \'\n          \'opposed to using any analysis like statistics \'\n          \'that exist in the prediction package\'))\n@click.option(\n    \'--channel-order\',\n    cls=OptionEatAll,\n    help=\'List of indices comprising channel_order. Example: 2 1 0\')\n@click.option(\n    \'--export-config\',\n    type=click.Path(exists=False),\n    help=\'Exports the configuration to the given output file.\')\ndef predict(predict_package, image_uri, output_uri, update_stats,\n            channel_order, export_config):\n    """"""Make predictions on the image at IMAGE_URI\n    using PREDICT_PACKAGE and store the\n    prediciton output at OUTPUT_URI.\n    """"""\n    if channel_order is not None:\n        channel_order = [\n            int(channel_ind) for channel_ind in channel_order.split(\' \')\n        ]\n\n    with RVConfig.get_tmp_dir() as tmp_dir:\n        predictor = rv.Predictor(predict_package, tmp_dir, update_stats,\n                                 channel_order)\n        predictor.predict(image_uri, output_uri, export_config)\n\n\n@main.command(\n    \'run_command\', short_help=\'Run a command from configuration file.\')\n@click.argument(\'command_config_uri\')\n@click.option(\'--tempdir\')\ndef run_command(command_config_uri, tempdir):\n    """"""Run a command from a serialized command configuration\n    at COMMAND_CONFIG_URI.\n    """"""\n    if tempdir is not None:\n        RVConfig.set_tmp_dir(tempdir)\n    rv.runner.CommandRunner.run(command_config_uri)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
rastervision/cli/verbosity.py,0,b'import rastervision as rv\n\n\nclass Verbosity:\n    QUIET = 0\n    NORMAL = 1\n    VERBOSE = 2\n    VERY_VERBOSE = 3\n    DEBUG = 4\n\n    @staticmethod\n    def get():\n        return rv._registry._get_rv_config().get_verbosity()\n'
rastervision/command/__init__.py,0,b'# flake8: noqa\n\nfrom rastervision.command.command import *\nfrom rastervision.command.command_config import *\nfrom rastervision.command.analyze_command import *\nfrom rastervision.command.analyze_command_config import *\nfrom rastervision.command.chip_command import *\nfrom rastervision.command.chip_command_config import *\nfrom rastervision.command.train_command import *\nfrom rastervision.command.train_command_config import *\nfrom rastervision.command.predict_command import *\nfrom rastervision.command.predict_command_config import *\nfrom rastervision.command.eval_command import *\nfrom rastervision.command.eval_command_config import *\nfrom rastervision.command.bundle_command import *\nfrom rastervision.command.bundle_command_config import *\n'
rastervision/command/analyze_command.py,0,"b""import click\n\nfrom rastervision.command import Command\n\n\nclass AnalyzeCommand(Command):\n    def __init__(self, command_config):\n        self.command_config = command_config\n\n    def run(self, tmp_dir=None):\n        if not tmp_dir:\n            tmp_dir = self.get_tmp_dir()\n\n        cc = self.command_config\n\n        analyzers = list(map(lambda a: a.create_analyzer(), cc.analyzers))\n        scenes = list(\n            map(lambda s: s.create_scene(cc.task, tmp_dir), cc.scenes))\n\n        for analyzer in analyzers:\n            msg = 'Running analyzer: {}...'.format(type(analyzer).__name__)\n            click.echo(click.style(msg, fg='green'))\n\n            analyzer.process(scenes, tmp_dir)\n"""
rastervision/command/analyze_command_config.py,0,"b'from copy import deepcopy\n\nimport rastervision as rv\nfrom rastervision.command import (AnalyzeCommand, CommandConfig,\n                                  CommandConfigBuilder, NoOpCommand)\nfrom rastervision.protos.command_pb2 \\\n    import CommandConfig as CommandConfigMsg\nfrom rastervision.command.utils import (check_scenes_type,\n                                        check_analyzers_type)\n\n\nclass AnalyzeCommandConfig(CommandConfig):\n    def __init__(self, root_uri, task, scenes, analyzers):\n        super().__init__(rv.ANALYZE, root_uri)\n        self.task = task\n        self.scenes = scenes\n        self.analyzers = analyzers\n\n    def create_command(self, tmp_dir=None):\n        if len(self.scenes) == 0 or len(self.analyzers) == 0:\n            return NoOpCommand()\n\n        retval = AnalyzeCommand(self)\n        retval.set_tmp_dir(tmp_dir)\n        return retval\n\n    def to_proto(self):\n        msg = super().to_proto()\n        task = self.task.to_proto()\n        scenes = list(map(lambda s: s.to_proto(), self.scenes))\n        analyzers = list(map(lambda a: a.to_proto(), self.analyzers))\n\n        msg.MergeFrom(\n            CommandConfigMsg(\n                analyze_config=CommandConfigMsg.AnalyzeConfig(\n                    task=task, scenes=scenes, analyzers=analyzers)))\n\n        return msg\n\n    def report_io(self):\n        io_def = rv.core.CommandIODefinition()\n        self.task.report_io(self.command_type, io_def)\n        for scene in self.scenes:\n            scene.report_io(self.command_type, io_def)\n        for analyzer in self.analyzers:\n            analyzer.report_io(self.command_type, io_def)\n        return io_def\n\n\nclass AnalyzeCommandConfigBuilder(CommandConfigBuilder):\n    def __init__(self, command_type, prev=None):\n        super().__init__(command_type, prev)\n        if prev is None:\n            self.task = None\n            self.scenes = None\n            self.analyzers = None\n        else:\n            self.task = prev.task\n            self.scenes = prev.scenes\n            self.analyzers = prev.analyzers\n\n    def validate(self):\n        super().validate()\n        if self.scenes is None:\n            raise rv.ConfigError(\'scenes not set for AnalyzeCommandConfig. Use\'\n                                 \' with_scenes or with_experiment\')\n        check_scenes_type(self.scenes)\n        if self.analyzers is None:\n            raise rv.ConfigError(\n                \'analyzers not set. Use with_analyzers or with_experiment\')\n        check_analyzers_type(self.analyzers)\n\n    def build(self):\n        self.validate()\n        return AnalyzeCommandConfig(self.root_uri, self.task, self.scenes,\n                                    self.analyzers)\n\n    def from_proto(self, msg):\n        b = super().from_proto(msg)\n\n        conf = msg.analyze_config\n\n        task = rv.TaskConfig.from_proto(conf.task)\n        scenes = list(map(rv.SceneConfig.from_proto, conf.scenes))\n        analyzers = list(map(rv.AnalyzerConfig.from_proto, conf.analyzers))\n\n        b = b.with_task(task)\n        b = b.with_scenes(scenes)\n        b = b.with_analyzers(analyzers)\n\n        return b\n\n    def get_root_uri(self, experiment_config):\n        return experiment_config.analyze_uri\n\n    def with_experiment(self, experiment_config):\n        b = super().with_experiment(experiment_config)\n        b = b.with_task(experiment_config.task)\n        b = b.with_scenes(experiment_config.dataset.all_scenes())\n        b = b.with_analyzers(experiment_config.analyzers)\n        return b\n\n    def with_task(self, task):\n        """"""Sets a specific task type.\n\n        Args:\n            task:  A TaskConfig object.\n\n        """"""\n        b = deepcopy(self)\n        b.task = task\n        return b\n\n    def with_scenes(self, scenes):\n        b = deepcopy(self)\n        b.scenes = scenes\n        return b\n\n    def with_analyzers(self, analyzers):\n        b = deepcopy(self)\n        b.analyzers = analyzers\n        return b\n'"
rastervision/command/api.py,0,"b""# flake8: noqa\n\nimport rastervision as rv\n\nANALYZE = 'ANALYZE'\nCHIP = 'CHIP'\nTRAIN = 'TRAIN'\nPREDICT = 'PREDICT'\nEVAL = 'EVAL'\nBUNDLE = 'BUNDLE'\n\nfrom .command_config import CommandConfig\nfrom .aux_command import (AuxCommand, AuxCommandOptions)\nfrom .aux.api import *\n\n\ndef all_commands():\n    return rv._registry.get_commands()\n"""
rastervision/command/aux_command.py,0,"b'from rastervision.command import Command\n\n\nclass AuxCommandOptions:\n    def __init__(self,\n                 split_on=None,\n                 inputs=lambda config: None,\n                 outputs=lambda config: None,\n                 include_by_default=False,\n                 required_fields=None):\n        """"""Instantiate an AuxCommandOptions object.\n\n        Args:\n            split_on (str): The property of the configuration to use when splitting.\n            The configuration at this property must be a list.\n\n            inputs: A function that, given the configuration, returns a list of\n            URIs that are inputs into the command. Along with outputs, this allows\n            Raster Vision to correctly determine if there are any missing inputs, or\n            if the command has already been run. It will also allow the command to\n            be run in the right sequence if run with other commands that will produce\n            this command\'s inputs as their outputs.\n\n            outputs: A function that, given the configuration, returns a list of\n            URIs that are outputs of the command. See the details on inputs.\n\n            include_by_default: Set this to True if you want this command to run\n            by default, meaning it will run every time no specific commands are issued\n            on the command line (e.g. how a standard command would run).\n\n            required_fields: Set this to properties of the configuration that are\n            required. If the user of the command does not set values into those\n            configuration properties, an error will be thrown at configuration building\n            time.\n\n        """"""\n        self.split_on = split_on\n        self.inputs = inputs\n        self.outputs = outputs\n        self.include_by_default = include_by_default\n        self.required_fields = required_fields\n\n\nclass AuxCommand(Command):\n    """"""An abstract class representing an auxiliary command.\n    The purpose of Aux commands is to make it easy to add\n    custom functionality to Raster Vision.\n    """"""\n\n    command_type = None\n    options = None\n\n    def __init__(self, command_config):\n        self.command_config = command_config\n'"
rastervision/command/aux_command_config.py,0,"b'import os\nfrom copy import deepcopy\nimport json\n\nfrom google.protobuf import struct_pb2\n\nimport rastervision as rv\nfrom rastervision.rv_config import RVConfig\nfrom rastervision.command import (CommandConfig, CommandConfigBuilder)\nfrom rastervision.protos.command_pb2 \\\n    import CommandConfig as CommandConfigMsg\nfrom rastervision.utils.misc import split_into_groups\n\n\nclass AuxCommandConfig(CommandConfig):\n    def __init__(self, command_class, root_uri, config, split_id=0):\n        super().__init__(command_class.command_type, root_uri)\n        self.command_class = command_class\n        self.command_options = command_class.options\n        self.config = config\n        self.split_id = split_id\n\n    def create_command(self, tmp_dir=None):\n        if not tmp_dir:\n            _tmp_dir = RVConfig.get_tmp_dir()\n            tmp_dir = _tmp_dir.name\n        else:\n            _tmp_dir = tmp_dir\n\n        retval = self.command_class(self.config)\n        retval.set_tmp_dir(_tmp_dir)\n\n        return retval\n\n    def to_proto(self):\n        msg = super().to_proto()\n        conf = struct_pb2.Struct()\n        conf[\'json\'] = json.dumps(self.config)\n        msg.MergeFrom(CommandConfigMsg(custom_config=conf))\n\n        return msg\n\n    def report_io(self):\n        io_def = rv.core.CommandIODefinition()\n        inputs = self.command_options.inputs(self.config)\n        outputs = self.command_options.outputs(self.config)\n\n        if inputs:\n            io_def.add_inputs(inputs)\n        if outputs:\n            io_def.add_outputs(outputs)\n\n        return io_def\n\n    def split(self, num_parts):\n        split_on = self.command_options.split_on\n        if split_on:\n            commands = []\n            for i, split_elements in enumerate(\n                    split_into_groups(self.config[split_on], num_parts)):\n                split_config = deepcopy(self.config)\n                split_config[split_on] = split_elements\n                c = self.to_builder() \\\n                        .with_config(**split_config) \\\n                        .with_split_id(i) \\\n                        .build()\n                commands.append(c)\n            return commands\n        else:\n            return [self]\n\n\nclass AuxCommandConfigBuilder(CommandConfigBuilder):\n    def __init__(self, command_type, prev=None):\n        super().__init__(command_type, prev)\n        if prev:\n            self.config = prev.config\n            self.command_class = prev.command_class\n        else:\n            self.config = None\n            self.command_class = None\n\n    def validate(self):\n        super().validate()\n\n        if not self.command_class:\n            raise rv.ConfigError(\n                \'AuxCommandConfigBuilder requires the command_class be set.\')\n\n        if self.config is None:\n            raise rv.ConfigError(\n                \'AuxCommandConfigBuilder requires a configuration be set, either \'\n                \'through with_config or by setting a dict with the property ""config"" \'\n                \'in an experiment custom configuration dict with the command name \'\n                \'as the key in the experiment custom configuration\')\n\n        if self.command_class.options.required_fields:\n            for field in self.command_class.options.required_fields:\n                if field not in self.config:\n                    raise rv.ConfigError(\'{} command requires the field {} \'\n                                         \'be set in the configuration.\'.format(\n                                             self.command_type, field))\n\n    def build(self):\n        self.validate()\n        return AuxCommandConfig(self.command_class, self.root_uri, self.config,\n                                self.split_id)\n\n    def from_proto(self, msg):\n        b = super().from_proto(msg)\n\n        self.command_class = rv._registry.get_aux_command_class(\n            self.command_type)\n\n        b = b.with_config(**json.loads(msg.custom_config[\'json\']))\n\n        return b\n\n    def _get_config_from_experiment(self, experiment_config):\n        command_name = self.command_type.lower()\n        matching_props = list(\n            filter(lambda x: x.lower() == command_name,\n                   experiment_config.custom_config.keys()))\n        if len(matching_props) > 1:\n            raise rv.ConfigError(\n                \'Multiple configurations matching the {} command found. \'\n                \'Use only one configuration - property match is not case sensitive.\'.\n                format(self.command_type))\n        elif len(matching_props) == 1:\n            return experiment_config.custom_config[matching_props[0]]\n        else:\n            return None\n\n    def get_root_uri(self, experiment_config):\n        if self.root_uri:\n            return self.root_uri\n\n        command_name = self.command_type.lower()\n        command_config = self._get_config_from_experiment(experiment_config)\n\n        if not command_config:\n            raise rv.ConfigError(\n                \'{} command requires experiment custom_config \'\n                \'contains a {} key\'.format(self.command_type, command_name))\n        key = command_config.get(\'key\')\n        if not key:\n            root_uri = command_config.get(\'root_uri\')\n            if not root_uri:\n                raise rv.ConfigError(\n                    \'{} command requires a ""key"" or ""root_uri"" \'\n                    \'be set in the command config dict inside the \'\n                    \'experiment custom_config\'.format(self.command_type))\n        else:\n            root_uri = os.path.join(experiment_config.root_uri, command_name,\n                                    key)\n\n        return root_uri\n\n    def with_experiment(self, experiment_config):\n        b = super().with_experiment(experiment_config)\n\n        command_config = self._get_config_from_experiment(experiment_config)\n\n        if command_config:\n            config_data = command_config.get(\'config\')\n            if not config_data:\n                raise rv.ConfigError(\n                    \'{} command requires a configuration dict set in the \'\n                    \'""config"" property of the experiment \'\n                    \'custom_config for this command.\'.format(\n                        self.command_type))\n            else:\n                b = b.with_config(**config_data)\n\n        return b\n\n    def with_config(self, **kwargs):\n        b = deepcopy(self)\n        b.config = kwargs\n        return b\n\n    def with_command_class(self, command_class):\n        b = deepcopy(self)\n        b.command_class = command_class\n        return b\n'"
rastervision/command/bundle_command.py,0,"b'import os\nimport zipfile\nimport logging\n\nimport click\nfrom google.protobuf import json_format\n\nfrom rastervision.command import Command\nfrom rastervision.utils.files import (upload_or_copy, make_dir)\n\nlog = logging.getLogger(__name__)\n\n\nclass BundleCommand(Command):\n    """"""Bundles all the necessary files together into a prediction package.""""""\n\n    def __init__(self, command_config):\n        self.command_config = command_config\n\n    def run(self, tmp_dir=None):\n        if not tmp_dir:\n            tmp_dir = self.get_tmp_dir()\n\n        cc = self.command_config\n\n        if not cc.task.predict_package_uri:\n            msg = \'Skipping bundling of prediction package, no URI is set...\'.format(\n                cc.task.predict_package_uri)\n            click.echo(click.style(msg, fg=\'yellow\'))\n            return\n\n        msg = \'Bundling prediction package to {}...\'.format(\n            cc.task.predict_package_uri)\n        log.info(msg)\n\n        bundle_dir = os.path.join(tmp_dir, \'bundle\')\n        make_dir(bundle_dir)\n        package_path = os.path.join(tmp_dir, \'predict_package.zip\')\n        bundle_files = []\n        new_task, task_files = cc.task.save_bundle_files(bundle_dir)\n        bundle_files.extend(task_files)\n        new_backend, backend_files = cc.backend.save_bundle_files(bundle_dir)\n        bundle_files.extend(backend_files)\n        new_scene, scene_files = cc.scene.save_bundle_files(bundle_dir)\n        bundle_files.extend(scene_files)\n        new_analyzers = []\n        for analyzer in cc.analyzers:\n            new_analyzer, analyzer_files = analyzer.save_bundle_files(\n                bundle_dir)\n            new_analyzers.append(new_analyzer)\n            bundle_files.extend(analyzer_files)\n\n        new_bundle_config = cc.to_builder() \\\n                              .with_task(new_task) \\\n                              .with_backend(new_backend) \\\n                              .with_scene(new_scene) \\\n                              .with_analyzers(new_analyzers) \\\n                              .build()\n\n        # Save bundle command config\n        bundle_config_path = os.path.join(tmp_dir, \'bundle_config.json\')\n        bundle_json = json_format.MessageToJson(new_bundle_config.to_proto())\n        with open(bundle_config_path, \'w\') as f:\n            f.write(bundle_json)\n\n        with zipfile.ZipFile(package_path, \'w\') as package_zip:\n            for path in bundle_files:\n                package_zip.write(path, arcname=os.path.basename(path))\n            package_zip.write(\n                bundle_config_path,\n                arcname=os.path.basename(bundle_config_path))\n\n        upload_or_copy(package_path, cc.task.predict_package_uri)\n'"
rastervision/command/bundle_command_config.py,0,"b'from copy import deepcopy\n\nimport rastervision as rv\nfrom rastervision.command import (CommandConfig, CommandConfigBuilder,\n                                  BundleCommand)\nfrom rastervision.protos.command_pb2 \\\n    import CommandConfig as CommandConfigMsg\nfrom rastervision.rv_config import RVConfig\nfrom rastervision.data import SceneConfig\nfrom rastervision.command.utils import (check_task_type, check_analyzers_type,\n                                        check_backend_type)\n\n\nclass BundleCommandConfig(CommandConfig):\n    def __init__(self, root_uri, task, backend, scene, analyzers):\n        super().__init__(rv.BUNDLE, root_uri)\n        self.task = task\n        self.backend = backend\n        self.scene = scene\n        self.analyzers = analyzers\n\n    def create_command(self, tmp_dir=None):\n        if not tmp_dir:\n            _tmp_dir = RVConfig.get_tmp_dir()\n            tmp_dir = _tmp_dir.name\n        else:\n            _tmp_dir = tmp_dir\n\n        retval = BundleCommand(self)\n        retval.set_tmp_dir(_tmp_dir)\n        return retval\n\n    def to_proto(self):\n        msg = super().to_proto()\n\n        task = self.task.to_proto()\n        backend = self.backend.to_proto()\n        scene = self.scene.to_proto()\n        analyzers = list(map(lambda a: a.to_proto(), self.analyzers))\n\n        b = CommandConfigMsg.BundleConfig(\n            task=task, backend=backend, scene=scene, analyzers=analyzers)\n\n        msg.MergeFrom(CommandConfigMsg(bundle_config=b))\n\n        return msg\n\n    def report_io(self):\n        io_def = rv.core.CommandIODefinition()\n        self.task.report_io(self.command_type, io_def)\n        self.backend.report_io(self.command_type, io_def)\n        self.scene.report_io(self.command_type, io_def)\n        for analyzer in self.analyzers:\n            analyzer.report_io(self.command_type, io_def)\n        return io_def\n\n\nclass BundleCommandConfigBuilder(CommandConfigBuilder):\n    def __init__(self, command_type, prev=None):\n        super().__init__(command_type, prev)\n        if prev is None:\n            self.task = None\n            self.backend = None\n            self.scene = None\n            self.analyzers = None\n        else:\n            self.task = prev.task\n            self.backend = prev.backend\n            self.scene = prev.scene\n            self.analyzers = prev.analyzers\n\n    def validate(self):\n        super().validate()\n        if self.task is None:\n            raise rv.ConfigError(\'Task not set for BundleCommandConfig. \'\n                                 \'Use with_task or with_experiment\')\n        check_task_type(self.task)\n        if self.backend is None:\n            raise rv.ConfigError(\'Backend not set for BundleCommandConfig. \'\n                                 \'Use with_backend or with_experiment\')\n        check_backend_type(self.backend)\n        if self.scene is None:\n            raise rv.ConfigError(\n                \'Template scene not set for BundleCommandConfig. \'\n                \'Use with_scene or with_experiment\')\n        if not isinstance(self.scene, SceneConfig):\n            raise rv.ConfigError(\n                \'Template scene must be of class SceneConfig, got {}\'.format(\n                    type(self.scene)))\n        if self.analyzers is None:\n            raise rv.ConfigError(\'Analyzers not set for BundleCommandConfig. \'\n                                 \'Use with_analyzers or with_experiment\')\n        check_analyzers_type(self.analyzers)\n\n    def build(self):\n        self.validate()\n        return BundleCommandConfig(self.root_uri, self.task, self.backend,\n                                   self.scene, self.analyzers)\n\n    def from_proto(self, msg):\n        b = super().from_proto(msg)\n\n        conf = msg.bundle_config\n\n        task = rv.TaskConfig.from_proto(conf.task)\n        backend = rv.BackendConfig.from_proto(conf.backend)\n        scene = rv.SceneConfig.from_proto(conf.scene)\n        analyzers = list(map(rv.AnalyzerConfig.from_proto, conf.analyzers))\n\n        b = b.with_task(task)\n        b = b.with_backend(backend)\n        b = b.with_scene(scene)\n        b = b.with_analyzers(analyzers)\n\n        return b\n\n    def get_root_uri(self, experiment_config):\n        return experiment_config.bundle_uri\n\n    def with_experiment(self, experiment_config):\n        b = super().with_experiment(experiment_config)\n        b = b.with_task(experiment_config.task)\n        b = b.with_backend(experiment_config.backend)\n        b = b.with_scene(experiment_config.dataset.all_scenes()[0])\n        b = b.with_analyzers(experiment_config.analyzers)\n        return b\n\n    def with_task(self, task):\n        """"""Sets a specific task type.\n\n        Args:\n            task:  A TaskConfig object.\n\n        """"""\n        b = deepcopy(self)\n        b.task = task\n        return b\n\n    def with_backend(self, backend):\n        b = deepcopy(self)\n        b.backend = backend\n        return b\n\n    def with_scene(self, scene):\n        b = deepcopy(self)\n        b.scene = scene\n        return b\n\n    def with_analyzers(self, analyzers):\n        b = deepcopy(self)\n        b.analyzers = analyzers\n        return b\n'"
rastervision/command/chip_command.py,0,"b""import click\n\nfrom rastervision.command import Command\n\n\nclass ChipCommand(Command):\n    def __init__(self, command_config):\n        self.command_config = command_config\n\n    def run(self, tmp_dir=None):\n        if not tmp_dir:\n            tmp_dir = self.get_tmp_dir()\n        msg = 'Making training chips...'\n        click.echo(click.style(msg, fg='green'))\n\n        cc = self.command_config\n\n        backend = cc.backend.create_backend(cc.task)\n        task = cc.task.create_task(backend)\n\n        train_scenes = list(\n            map(lambda s: s.create_scene(cc.task, tmp_dir), cc.train_scenes))\n\n        val_scenes = list(\n            map(lambda s: s.create_scene(cc.task, tmp_dir), cc.val_scenes))\n\n        augmentors = list(map(lambda a: a.create_augmentor(), cc.augmentors))\n\n        task.make_chips(train_scenes, val_scenes, augmentors, tmp_dir)\n"""
rastervision/command/chip_command_config.py,0,"b'from copy import deepcopy\n\nimport rastervision as rv\nfrom rastervision.command import (ChipCommand, CommandConfig,\n                                  CommandConfigBuilder, NoOpCommand)\nfrom rastervision.protos.command_pb2 \\\n    import CommandConfig as CommandConfigMsg\nfrom rastervision.rv_config import RVConfig\nfrom rastervision.data import SceneConfig\nfrom rastervision.command.utils import (check_task_type, check_backend_type)\nfrom rastervision.utils.misc import split_into_groups\n\n\nclass ChipCommandConfig(CommandConfig):\n    def __init__(self, root_uri, split_id, task, backend, augmentors,\n                 train_scenes, val_scenes):\n        super().__init__(rv.CHIP, root_uri, split_id)\n        self.task = task\n        self.backend = backend\n        self.augmentors = augmentors\n        self.train_scenes = train_scenes\n        self.val_scenes = val_scenes\n\n    def create_command(self, tmp_dir=None):\n        if len(self.train_scenes) == 0 and len(self.val_scenes) == 0:\n            return NoOpCommand()\n\n        if not tmp_dir:\n            _tmp_dir = RVConfig.get_tmp_dir()\n            tmp_dir = _tmp_dir.name\n        else:\n            _tmp_dir = tmp_dir\n\n        retval = ChipCommand(self)\n        retval.set_tmp_dir(_tmp_dir)\n        return retval\n\n    def to_proto(self):\n        msg = super().to_proto()\n\n        task = self.task.to_proto()\n        backend = self.backend.to_proto()\n        train_scenes = list(map(lambda s: s.to_proto(), self.train_scenes))\n        val_scenes = list(map(lambda s: s.to_proto(), self.val_scenes))\n        augmentors = list(map(lambda a: a.to_proto(), self.augmentors))\n        msg.MergeFrom(\n            CommandConfigMsg(\n                chip_config=CommandConfigMsg.ChipConfig(\n                    task=task,\n                    backend=backend,\n                    train_scenes=train_scenes,\n                    val_scenes=val_scenes,\n                    augmentors=augmentors)))\n\n        return msg\n\n    def report_io(self):\n        io_def = rv.core.CommandIODefinition()\n        self.task.report_io(self.command_type, io_def)\n        self.backend.report_io(self.command_type, io_def)\n        for scene in self.train_scenes:\n            scene.report_io(self.command_type, io_def)\n        for scene in self.val_scenes:\n            scene.report_io(self.command_type, io_def)\n        for augmentor in self.augmentors:\n            augmentor.report_io(self.command_type, io_def)\n        return io_def\n\n    def split(self, num_parts):\n        commands = []\n        t_scenes = list(map(lambda x: (0, x), self.train_scenes))\n        v_scenes = list(map(lambda x: (1, x), self.val_scenes))\n\n        for i, l in enumerate(\n                split_into_groups(t_scenes + v_scenes, num_parts)):\n            split_t_scenes = list(\n                map(lambda x: x[1], filter(lambda x: x[0] == 0, l)))\n            split_v_scenes = list(\n                map(lambda x: x[1], filter(lambda x: x[0] == 1, l)))\n            c = self.to_builder() \\\n                .with_train_scenes(split_t_scenes) \\\n                .with_val_scenes(split_v_scenes) \\\n                .with_split_id(i) \\\n                .build()\n            commands.append(c)\n        return commands\n\n\nclass ChipCommandConfigBuilder(CommandConfigBuilder):\n    def __init__(self, command_type, prev=None):\n        super().__init__(command_type, prev)\n        if prev is None:\n            self.task = None\n            self.backend = None\n            self.augmentors = []\n            self.train_scenes = []\n            self.val_scenes = []\n        else:\n            self.task = prev.task\n            self.backend = prev.backend\n            self.augmentors = prev.augmentors\n            self.train_scenes = prev.train_scenes\n            self.val_scenes = prev.val_scenes\n\n    def validate(self):\n        super().validate()\n        if self.task is None:\n            raise rv.ConfigError(\'Task not set for ChipCommandConfig. Use \'\n                                 \'with_task or with_experiment\')\n        check_task_type(self.task)\n        if self.backend is None:\n            raise rv.ConfigError(\'Backend not set for ChipCommandConfig. Use \'\n                                 \'with_backend or with_experiment\')\n        check_backend_type(self.backend)\n        if len(self.train_scenes) > 0:\n            for s in self.train_scenes:\n                if not isinstance(s, SceneConfig):\n                    raise rv.ConfigError(\n                        \'train_scenes must be a list of class SceneConfig, \'\n                        \'got a list of {}\'.format(type(s)))\n        if len(self.val_scenes) > 0:\n            for s in self.val_scenes:\n                if not isinstance(s, SceneConfig):\n                    raise rv.ConfigError(\n                        \'val_scenes must be a list of class SceneConfig, \'\n                        \'got a list of {}\'.format(type(s)))\n\n    def build(self):\n        self.validate()\n        return ChipCommandConfig(self.root_uri, self.split_id, self.task,\n                                 self.backend, self.augmentors,\n                                 self.train_scenes, self.val_scenes)\n\n    def from_proto(self, msg):\n        b = super().from_proto(msg)\n\n        conf = msg.chip_config\n\n        task = rv.TaskConfig.from_proto(conf.task)\n        backend = rv.BackendConfig.from_proto(conf.backend)\n        augmentors = list(map(rv.AugmentorConfig.from_proto, conf.augmentors))\n        train_scenes = list(map(rv.SceneConfig.from_proto, conf.train_scenes))\n        val_scenes = list(map(rv.SceneConfig.from_proto, conf.val_scenes))\n\n        b = b.with_task(task)\n        b = b.with_backend(backend)\n        b = b.with_augmentors(augmentors)\n        b = b.with_train_scenes(train_scenes)\n        b = b.with_val_scenes(val_scenes)\n\n        return b\n\n    def get_root_uri(self, experiment_config):\n        return experiment_config.chip_uri\n\n    def with_experiment(self, experiment_config):\n        b = super().with_experiment(experiment_config)\n        b = b.with_task(experiment_config.task)\n        b = b.with_backend(experiment_config.backend)\n        b = b.with_augmentors(experiment_config.dataset.augmentors)\n        b = b.with_train_scenes(experiment_config.dataset.train_scenes)\n        b = b.with_val_scenes(experiment_config.dataset.validation_scenes)\n        return b\n\n    def with_task(self, task):\n        """"""Sets a specific task type.\n\n        Args:\n            task:  A TaskConfig object.\n\n        """"""\n        b = deepcopy(self)\n        b.task = task\n        return b\n\n    def with_backend(self, backend):\n        b = deepcopy(self)\n        b.backend = backend\n        return b\n\n    def with_augmentors(self, augmentors):\n        b = deepcopy(self)\n        b.augmentors = augmentors\n        return b\n\n    def with_train_scenes(self, scenes):\n        b = deepcopy(self)\n        b.train_scenes = scenes\n        return b\n\n    def with_val_scenes(self, scenes):\n        b = deepcopy(self)\n        b.val_scenes = scenes\n        return b\n'"
rastervision/command/command.py,0,"b'from abc import ABC, abstractmethod\n\nfrom rastervision.rv_config import RVConfig\n\n\nclass Command(ABC):\n    @abstractmethod\n    def run(self, tmp_dir):\n        """"""Run the command.""""""\n        pass\n\n    def set_tmp_dir(self, tmp_dir):\n        self._tmp_dir = tmp_dir\n\n    def get_tmp_dir(self):\n        if hasattr(self, \'_tmp_dir\') and self._tmp_dir:\n            if isinstance(self._tmp_dir, str):\n                return self._tmp_dir\n            else:\n                return self._tmp_dir.name\n        else:\n            tmp_dir = RVConfig.get_tmp_dir()\n            self.set_tmp_dir(tmp_dir)\n            return tmp_dir.name\n\n\nclass NoOpCommand(Command):\n    """"""Defines a command that does nothing.\n    """"""\n\n    def run(self, tmp_dir):\n        pass\n'"
rastervision/command/command_config.py,0,"b'from abc import ABC, abstractmethod\nfrom copy import deepcopy\n\nimport rastervision as rv\nfrom rastervision.plugin import PluginRegistry\nfrom rastervision.protos.command_pb2 \\\n    import CommandConfig as CommandConfigMsg\n\n\nclass CommandConfig(ABC):\n    def __init__(self, command_type, root_uri, split_id=0):\n        self.command_type = command_type\n        self.root_uri = root_uri\n        self.split_id = split_id\n\n    @abstractmethod\n    def create_command(self):\n        """"""Run the command.""""""\n        pass\n\n    @abstractmethod\n    def report_io(self):\n        """"""Returns an IODefinition object with all inputs and outputs reported\n        for this command.\n        """"""\n        pass\n\n    def split(self, num_parts):\n        """"""Split this command config into num_parts parts if possible.\n        This will return a list of size between 1 and num_parts containing command\n        configurations. If the command configuration does not split, will return [self]\n        """"""\n        return [self]\n\n    def utilizes_gpu(self):\n        """"""Method that determines if this command can utilize a GPU.\n        This is useful for runners to know what resources to utilize\n        for command execution - e.g. don\'t spin up the more expensive\n        GPU machines if this command can\'t use the GPUs.\n\n        Defaults to False.\n        """"""\n        return False\n\n    def to_proto(self):\n        """"""Returns the protobuf configuration for this config.\n        """"""\n        plugin_config = PluginRegistry.get_instance().to_proto()\n        return CommandConfigMsg(\n            command_type=self.command_type,\n            root_uri=self.root_uri,\n            split_id=self.split_id,\n            plugins=plugin_config)\n\n    def to_builder(self):\n        return rv._registry.get_command_config_builder(self.command_type)(self)\n\n    @staticmethod\n    def builder(command_type):\n        """"""Returns a new builder for the given command type.\n        """"""\n        return rv._registry.get_command_config_builder(command_type)()\n\n    @staticmethod\n    def from_proto(msg):\n        """"""Creates a TaskConfig from the specificed protobuf message\n        """"""\n        command_type = msg.command_type\n        return rv._registry.get_command_config_builder(command_type)() \\\n                           .from_proto(msg) \\\n                           .build()\n\n\nclass CommandConfigBuilder(ABC):\n    def __init__(self, command_type, prev):\n        self.command_type = command_type\n\n        if prev is None:\n            self.root_uri = None\n            self.split_id = 0\n        else:\n            self.root_uri = prev.root_uri\n            self.split_id = prev.split_id\n\n    @abstractmethod\n    def build(self, prev=None):\n        """"""Returns the configuration that is built by this builder.\n        """"""\n        pass\n\n    @abstractmethod\n    def get_root_uri(self, experiment_config):\n        """"""Return the root URI for this command for a given experiment""""""\n        pass\n\n    def from_proto(self, msg):\n        """"""Return a builder that takes the configuration from the proto message\n           as its starting point.\n        """"""\n\n        # Process plugins from a command config protobuf message.\n        if msg.HasField(\'plugins\'):\n            PluginRegistry.get_instance().add_plugins_from_proto(msg.plugins)\n\n        return self.with_root_uri(msg.root_uri).with_split_id(msg.split_id)\n\n    def validate(self):\n        if self.root_uri is None:\n            raise rv.ConfigError(\n                \'root_uri not set. Use with_root_uri or with_experiment\')\n\n    def with_experiment(self, experiment_config):\n        """"""Generate all required information from this experiment.\n           It is sufficient to only call \'with_experiment\' before\n           calling .build()\n        """"""\n        return self.with_root_uri(self.get_root_uri(experiment_config))\n\n    def with_root_uri(self, root_uri):\n        b = deepcopy(self)\n        b.root_uri = root_uri\n        return b\n\n    def with_split_id(self, split_id):\n        b = deepcopy(self)\n        b.split_id = split_id\n        return b\n'"
rastervision/command/eval_command.py,0,"b""import click\n\nfrom rastervision.command import Command\n\n\nclass EvalCommand(Command):\n    def __init__(self, command_config):\n        self.command_config = command_config\n\n    def run(self, tmp_dir=None):\n        if not tmp_dir:\n            tmp_dir = self.get_tmp_dir()\n\n        cc = self.command_config\n\n        scenes = list(\n            map(lambda s: s.create_scene(cc.task, tmp_dir), cc.scenes))\n        evaluators = list(map(lambda a: a.create_evaluator(), cc.evaluators))\n\n        for evaluator in evaluators:\n            msg = 'Running evaluator: {}...'.format(type(evaluator).__name__)\n            click.echo(click.style(msg, fg='green'))\n\n            evaluator.process(scenes, tmp_dir)\n"""
rastervision/command/eval_command_config.py,0,"b'from copy import deepcopy\n\nimport rastervision as rv\nfrom rastervision.command import (EvalCommand, CommandConfig,\n                                  CommandConfigBuilder, NoOpCommand)\nfrom rastervision.protos.command_pb2 \\\n    import CommandConfig as CommandConfigMsg\nfrom rastervision.rv_config import RVConfig\nfrom rastervision.command.utils import (check_scenes_type, check_task_type)\nfrom rastervision.evaluation import EvaluatorConfig\n\n\nclass EvalCommandConfig(CommandConfig):\n    def __init__(self, root_uri, task, scenes, evaluators):\n        super().__init__(rv.EVAL, root_uri)\n        self.task = task\n        self.scenes = scenes\n        self.evaluators = evaluators\n\n    def create_command(self, tmp_dir=None):\n        if len(self.scenes) == 0 or len(self.evaluators) == 0:\n            return NoOpCommand()\n\n        if not tmp_dir:\n            _tmp_dir = RVConfig.get_tmp_dir()\n            tmp_dir = _tmp_dir.name\n        else:\n            _tmp_dir = tmp_dir\n\n        retval = EvalCommand(self)\n        retval.set_tmp_dir(_tmp_dir)\n        return retval\n\n    def to_proto(self):\n        msg = super().to_proto()\n        task = self.task.to_proto()\n        scenes = list(map(lambda s: s.to_proto(), self.scenes))\n        evaluators = list(map(lambda e: e.to_proto(), self.evaluators))\n\n        msg.MergeFrom(\n            CommandConfigMsg(\n                eval_config=CommandConfigMsg.EvalConfig(\n                    task=task, scenes=scenes, evaluators=evaluators)))\n\n        return msg\n\n    def report_io(self):\n        io_def = rv.core.CommandIODefinition()\n        self.task.report_io(self.command_type, io_def)\n        for scene in self.scenes:\n            scene.report_io(self.command_type, io_def)\n        for evaluator in self.evaluators:\n            evaluator.report_io(self.command_type, io_def)\n        return io_def\n\n\nclass EvalCommandConfigBuilder(CommandConfigBuilder):\n    def __init__(self, command_type, prev=None):\n        super().__init__(command_type, prev)\n        if prev is None:\n            self.task = None\n            self.scenes = None\n            self.evaluators = None\n        else:\n            self.task = prev.task\n            self.scenes = prev.scenes\n            self.evaluators = prev.evaluators\n\n    def validate(self):\n        super().validate()\n        if self.task is None:\n            raise rv.ConfigError(\n                \'task not set for EvalCommandConfig. Use with_task or \'\n                \'with_experiment\')\n        check_task_type(self.task)\n        if self.scenes is None:\n            raise rv.ConfigError(\n                \'scenes not set for EvalCommandConfig. Use with_scenes or \'\n                \'with_experiment\')\n        check_scenes_type(self.scenes)\n        if self.evaluators is None:\n            raise rv.ConfigError(\n                \'evaluators not set. Use with_evaluators or with_experiment\')\n        if not isinstance(self.evaluators, list):\n            raise rv.ConfigError(\n                \'evaluators must be a list of EvaluatorConfig objects, got {}\'.\n                format(type(self.evaluators)))\n        for evaluator in self.evaluators:\n            if not issubclass(type(evaluator), EvaluatorConfig):\n                if not isinstance(evaluator, str):\n                    raise rv.ConfigError(\n                        \'evaluators must be a subclass of EvaluatorConfig or string,\'\n                        \' got {}\'.format(type(evaluator)))\n\n    def build(self):\n        self.validate()\n        return EvalCommandConfig(self.root_uri, self.task, self.scenes,\n                                 self.evaluators)\n\n    def from_proto(self, msg):\n        b = super().from_proto(msg)\n\n        conf = msg.eval_config\n\n        task = rv.TaskConfig.from_proto(conf.task)\n        scenes = list(map(rv.SceneConfig.from_proto, conf.scenes))\n        evaluators = list(map(rv.EvaluatorConfig.from_proto, conf.evaluators))\n\n        b = b.with_task(task)\n        b = b.with_scenes(scenes)\n        b = b.with_evaluators(evaluators)\n\n        return b\n\n    def get_root_uri(self, experiment_config):\n        return experiment_config.eval_uri\n\n    def with_experiment(self, experiment_config):\n        b = super().with_experiment(experiment_config)\n        b = b.with_task(experiment_config.task)\n        b = b.with_scenes(experiment_config.dataset.validation_scenes)\n        b = b.with_evaluators(experiment_config.evaluators)\n        return b\n\n    def with_task(self, task):\n        """"""Sets a specific task type.\n\n        Args:\n            task:  A TaskConfig object.\n\n        """"""\n        b = deepcopy(self)\n        b.task = task\n        return b\n\n    def with_scenes(self, scenes):\n        b = deepcopy(self)\n        b.scenes = scenes\n        return b\n\n    def with_evaluators(self, evaluators):\n        b = deepcopy(self)\n        b.evaluators = evaluators\n        return b\n'"
rastervision/command/predict_command.py,0,"b""import click\n\nfrom rastervision.command import Command\n\n\nclass PredictCommand(Command):\n    def __init__(self, command_config):\n        self.command_config = command_config\n\n    def run(self, tmp_dir=None):\n        if not tmp_dir:\n            tmp_dir = self.get_tmp_dir()\n        msg = 'Making predictions...'\n\n        cc = self.command_config\n\n        backend = cc.backend.create_backend(cc.task)\n        task = cc.task.create_task(backend)\n\n        scenes = list(\n            map(lambda s: s.create_scene(cc.task, tmp_dir), cc.scenes))\n\n        click.echo(click.style(msg, fg='green'))\n        task.predict(scenes, tmp_dir)\n"""
rastervision/command/predict_command_config.py,0,"b'from copy import deepcopy\n\nimport rastervision as rv\nfrom rastervision.command import (PredictCommand, CommandConfig,\n                                  CommandConfigBuilder, NoOpCommand)\nfrom rastervision.protos.command_pb2 \\\n    import CommandConfig as CommandConfigMsg\nfrom rastervision.rv_config import RVConfig\nfrom rastervision.command.utils import (check_backend_type, check_task_type)\nfrom rastervision.utils.misc import split_into_groups\n\n\nclass PredictCommandConfig(CommandConfig):\n    def __init__(self, root_uri, split_id, task, backend, scenes):\n        super().__init__(rv.PREDICT, root_uri, split_id)\n        self.task = task\n        self.backend = backend\n        self.scenes = scenes\n\n    def utilizes_gpu(self):\n        # TODO: Make this backend dependent.\n        return True\n\n    def create_command(self, tmp_dir=None):\n        if len(self.scenes) == 0:\n            return NoOpCommand()\n\n        if not tmp_dir:\n            _tmp_dir = RVConfig.get_tmp_dir()\n            tmp_dir = _tmp_dir.name\n        else:\n            _tmp_dir = tmp_dir\n\n        retval = PredictCommand(self)\n        retval.set_tmp_dir(_tmp_dir)\n        return retval\n\n    def to_proto(self):\n        msg = super().to_proto()\n\n        task = self.task.to_proto()\n        backend = self.backend.to_proto()\n        scenes = list(map(lambda s: s.to_proto(), self.scenes))\n\n        msg.MergeFrom(\n            CommandConfigMsg(\n                predict_config=CommandConfigMsg.PredictConfig(\n                    task=task, backend=backend, scenes=scenes)))\n\n        return msg\n\n    def report_io(self):\n        io_def = rv.core.CommandIODefinition()\n        self.task.report_io(self.command_type, io_def)\n        self.backend.report_io(self.command_type, io_def)\n        for scene in self.scenes:\n            scene.report_io(self.command_type, io_def)\n        return io_def\n\n    def split(self, num_parts):\n        commands = []\n        for i, l in enumerate(split_into_groups(self.scenes, num_parts)):\n            c = self.to_builder() \\\n                .with_scenes(l) \\\n                .with_split_id(i) \\\n                .build()\n            commands.append(c)\n        return commands\n\n\nclass PredictCommandConfigBuilder(CommandConfigBuilder):\n    def __init__(self, command_type, prev=None):\n        super().__init__(command_type, prev)\n        if prev is None:\n            self.task = None\n            self.backend = None\n            self.scenes = []\n        else:\n            self.task = prev.task\n            self.backend = prev.backend\n            self.scenes = prev.scenes\n\n    def validate(self):\n        super().validate()\n        if self.task is None:\n            raise rv.ConfigError(\'Task not set for PredictCommandConfig. Use \'\n                                 \'with_task or with_experiment\')\n        check_task_type(self.task)\n\n        if self.backend is None:\n            raise rv.ConfigError(\n                \'Backend not set for PredictCommandConfig. Use \'\n                \'with_backend or with_experiment\')\n        check_backend_type(self.backend)\n\n    def build(self):\n        self.validate()\n        return PredictCommandConfig(self.root_uri, self.split_id, self.task,\n                                    self.backend, self.scenes)\n\n    def from_proto(self, msg):\n        b = super().from_proto(msg)\n\n        conf = msg.predict_config\n\n        task = rv.TaskConfig.from_proto(conf.task)\n        backend = rv.BackendConfig.from_proto(conf.backend)\n        scenes = list(map(rv.SceneConfig.from_proto, conf.scenes))\n\n        b = b.with_task(task)\n        b = b.with_backend(backend)\n        b = b.with_scenes(scenes)\n\n        return b\n\n    def get_root_uri(self, experiment_config):\n        return experiment_config.predict_uri\n\n    def with_experiment(self, experiment_config):\n        b = super().with_experiment(experiment_config)\n        b = b.with_task(experiment_config.task)\n        b = b.with_backend(experiment_config.backend)\n        b = b.with_scenes(experiment_config.dataset.validation_scenes +\n                          experiment_config.dataset.test_scenes)\n        return b\n\n    def with_task(self, task):\n        """"""Sets a specific task type.\n\n        Args:\n            task:  A TaskConfig object.\n\n        """"""\n        b = deepcopy(self)\n        b.task = task\n        return b\n\n    def with_backend(self, backend):\n        b = deepcopy(self)\n        b.backend = backend\n        return b\n\n    def with_scenes(self, scenes):\n        b = deepcopy(self)\n        b.scenes = scenes\n        return b\n'"
rastervision/command/train_command.py,0,"b""import click\n\nfrom rastervision.command import Command\n\n\nclass TrainCommand(Command):\n    def __init__(self, command_config):\n        self.command_config = command_config\n\n    def run(self, tmp_dir=None):\n        if not tmp_dir:\n            tmp_dir = self.get_tmp_dir()\n        msg = 'Training model...'\n        click.echo(click.style(msg, fg='green'))\n\n        cc = self.command_config\n\n        backend = cc.backend.create_backend(cc.task)\n        task = cc.task.create_task(backend)\n\n        task.train(tmp_dir)\n"""
rastervision/command/train_command_config.py,0,"b'from copy import deepcopy\n\nimport rastervision as rv\nfrom rastervision.command import (TrainCommand, CommandConfig,\n                                  CommandConfigBuilder)\nfrom rastervision.protos.command_pb2 \\\n    import CommandConfig as CommandConfigMsg\nfrom rastervision.rv_config import RVConfig\nfrom rastervision.command.utils import (check_task_type, check_backend_type)\n\n\nclass TrainCommandConfig(CommandConfig):\n    def __init__(self, root_uri, task, backend):\n        super().__init__(rv.TRAIN, root_uri)\n        self.task = task\n        self.backend = backend\n\n    def utilizes_gpu(self):\n        # TODO: Make this backend dependent.\n        return True\n\n    def create_command(self, tmp_dir=None):\n        if not tmp_dir:\n            _tmp_dir = RVConfig.get_tmp_dir()\n            tmp_dir = _tmp_dir.name\n        else:\n            _tmp_dir = tmp_dir\n\n        retval = TrainCommand(self)\n        retval.set_tmp_dir(_tmp_dir)\n        return retval\n\n    def to_proto(self):\n        msg = super().to_proto()\n\n        task = self.task.to_proto()\n        backend = self.backend.to_proto()\n\n        msg.MergeFrom(\n            CommandConfigMsg(\n                train_config=CommandConfigMsg.TrainConfig(\n                    task=task, backend=backend)))\n\n        return msg\n\n    def report_io(self):\n        io_def = rv.core.CommandIODefinition()\n        self.task.report_io(self.command_type, io_def)\n        self.backend.report_io(self.command_type, io_def)\n        return io_def\n\n\nclass TrainCommandConfigBuilder(CommandConfigBuilder):\n    def __init__(self, command_type, prev=None):\n        super().__init__(command_type, prev)\n        if prev is None:\n            self.task = None\n            self.backend = None\n        else:\n            self.task = prev.task\n            self.backend = prev.backend\n\n    def validate(self):\n        super().validate()\n        if self.task is None:\n            raise rv.ConfigError(\'Task not set for TrainCommandConfig. Use \'\n                                 \'with_task or with_experiment\')\n        check_task_type(self.task)\n\n        if self.backend is None:\n            raise rv.ConfigError(\'Backend not set for TrainCommandConfig. Use \'\n                                 \'with_task or with_experiment\')\n        check_backend_type(self.backend)\n\n    def build(self):\n        self.validate()\n        return TrainCommandConfig(self.root_uri, self.task, self.backend)\n\n    def from_proto(self, msg):\n        b = super().from_proto(msg)\n\n        conf = msg.train_config\n\n        task = rv.TaskConfig.from_proto(conf.task)\n        backend = rv.BackendConfig.from_proto(conf.backend)\n\n        b = b.with_task(task)\n        b = b.with_backend(backend)\n\n        return b\n\n    def get_root_uri(self, experiment_config):\n        return experiment_config.train_uri\n\n    def with_experiment(self, experiment_config):\n        b = super().with_experiment(experiment_config)\n        b = b.with_task(experiment_config.task)\n        b = b.with_backend(experiment_config.backend)\n        return b\n\n    def with_task(self, task):\n        """"""Sets a specific task type.\n\n        Args:\n            task:  A TaskConfig object.\n\n        """"""\n        b = deepcopy(self)\n        b.task = task\n        return b\n\n    def with_backend(self, backend):\n        b = deepcopy(self)\n        b.backend = backend\n        return b\n'"
rastervision/command/utils.py,0,"b""import rastervision as rv\nfrom rastervision.task import TaskConfig\nfrom rastervision.backend import BackendConfig\nfrom rastervision.analyzer import AnalyzerConfig\nfrom rastervision.data import SceneConfig\n\n\ndef check_analyzers_type(analyzers):\n    if not isinstance(analyzers, list):\n        raise rv.ConfigError(\n            'analyzers must be a list of AnalyzerConfig objects, got {}'.\n            format(type(analyzers)))\n    for analyzer in analyzers:\n        if not issubclass(type(analyzer), AnalyzerConfig):\n            if not isinstance(analyzer, str):\n                raise rv.ConfigError(\n                    'analyzers must be of class AnalyzerConfig or string, got {}'.\n                    format(type(analyzer)))\n\n\ndef check_backend_type(backend):\n    if not issubclass(type(backend), BackendConfig):\n        raise rv.ConfigError(\n            'Backend must be a child of class BackendConfig, got {}'.format(\n                type(backend)))\n\n\ndef check_scenes_type(scenes):\n    if not isinstance(scenes, list):\n        raise rv.ConfigError(\n            'scenes must be a list of SceneConfig objects, got {}'.format(\n                type(scenes)))\n    for scene in scenes:\n        if not isinstance(scene, SceneConfig):\n            if not isinstance(scene, str):\n                raise rv.ConfigError(\n                    'scene must be a SceneConfig object or str, got {}'.format(\n                        type(scene)))\n\n\ndef check_task_type(task):\n    if not issubclass(type(task), TaskConfig):\n        raise rv.ConfigError(\n            'Task must be a child class of TaskConfig, got {}'.format(\n                type(task)))\n"""
rastervision/core/__init__.py,0,b'# flake8: noqa\n\nfrom rastervision.core.box import *\nfrom rastervision.core.class_map import *\nfrom rastervision.core.command_io_definition import *\nfrom rastervision.core.config import *\nfrom rastervision.core.raster_stats import RasterStats\nfrom rastervision.core.training_data import *\n'
rastervision/core/box.py,0,"b'import math\nimport random\n\nimport numpy as np\nfrom shapely.geometry import box as ShapelyBox\n\n\nclass BoxSizeError(ValueError):\n    pass\n\n\nclass Box():\n    """"""A multi-purpose box (ie. rectangle).""""""\n\n    def __init__(self, ymin, xmin, ymax, xmax):\n        """"""Construct a bounding box.\n\n        Unless otherwise stated, the convention is that these coordinates are\n        in pixel coordinates and represent boxes that lie within a\n        RasterSource.\n\n        Args:\n            ymin: minimum y value (y is row)\n            xmin: minimum x value (x is column)\n            ymax: maximum y value\n            xmax: maximum x value\n\n        """"""\n        self.ymin = ymin\n        self.xmin = xmin\n        self.ymax = ymax\n        self.xmax = xmax\n\n    def __eq__(self, other):\n        """"""Return true if other has same coordinates.""""""\n        return self.tuple_format() == other.tuple_format()\n\n    def __ne__(self, other):\n        """"""Return true if other has different coordinates.""""""\n        return self.tuple_format() != other.tuple_format()\n\n    def get_height(self):\n        """"""Return height of Box.""""""\n        return self.ymax - self.ymin\n\n    def get_width(self):\n        """"""Return width of Box.""""""\n        return self.xmax - self.xmin\n\n    def get_area(self):\n        """"""Return area of Box.""""""\n        return self.get_height() * self.get_width()\n\n    def rasterio_format(self):\n        """"""Return Box in Rasterio format.""""""\n        return ((self.ymin, self.ymax), (self.xmin, self.xmax))\n\n    def tuple_format(self):\n        return (self.ymin, self.xmin, self.ymax, self.xmax)\n\n    def shapely_format(self):\n        return (self.xmin, self.ymin, self.xmax, self.ymax)\n\n    def to_int(self):\n        return Box(\n            int(self.ymin), int(self.xmin), int(self.ymax), int(self.xmax))\n\n    def npbox_format(self):\n        """"""Return Box in npbox format used by TF Object Detection API.\n\n        Returns:\n            Numpy array of form [ymin, xmin, ymax, xmax] with float type\n\n        """"""\n        return np.array(\n            [self.ymin, self.xmin, self.ymax, self.xmax], dtype=np.float)\n\n    @staticmethod\n    def to_npboxes(boxes):\n        """"""Return nx4 numpy array from list of Box.""""""\n        nb_boxes = len(boxes)\n        npboxes = np.empty((nb_boxes, 4))\n        for boxind, box in enumerate(boxes):\n            npboxes[boxind, :] = box.npbox_format()\n        return npboxes\n\n    def __str__(self):  # pragma: no cover\n        return str(self.npbox_format())\n\n    def __repr__(self):  # pragma: no cover\n        return str(self)\n\n    def geojson_coordinates(self):\n        """"""Return Box as GeoJSON coordinates.""""""\n        # Compass directions:\n        nw = [self.xmin, self.ymin]\n        ne = [self.xmin, self.ymax]\n        se = [self.xmax, self.ymax]\n        sw = [self.xmax, self.ymin]\n        return [nw, ne, se, sw, nw]\n\n    def make_random_square_container(self, size):\n        """"""Return a new square Box that contains this Box.\n\n        Args:\n            size: the width and height of the new Box\n\n        """"""\n        if size < self.get_width():\n            raise BoxSizeError(\'size of random container cannot be < width\')\n\n        if size < self.get_height():  # pragma: no cover\n            raise BoxSizeError(\'size of random container cannot be < height\')\n\n        lb = self.ymin - (size - self.get_height())\n        ub = self.ymin\n        rand_y = random.randint(int(lb), int(ub))\n\n        lb = self.xmin - (size - self.get_width())\n        ub = self.xmin\n        rand_x = random.randint(int(lb), int(ub))\n\n        return Box.make_square(rand_y, rand_x, size)\n\n    def make_random_square(self, size):\n        """"""Return new randomly positioned square Box that lies inside this Box.\n\n        Args:\n            size: the height and width of the new Box\n\n        """"""\n        if size >= self.get_width():\n            raise BoxSizeError(\'size of random square cannot be >= width\')\n\n        if size >= self.get_height():  # pragma: no cover\n            raise BoxSizeError(\'size of random square cannot be >= height\')\n\n        lb = self.ymin\n        ub = self.ymax - size\n        rand_y = random.randint(int(lb), int(ub))\n\n        lb = self.xmin\n        ub = self.xmax - size\n        rand_x = random.randint(int(lb), int(ub))\n\n        return Box.make_square(rand_y, rand_x, size)\n\n    def intersection(self, other):\n        """"""Return the intersection of this Box and the other.\n\n        Args:\n            other: The box to intersect with this one.\n\n        Returns:\n             The intersection of this box and the other one.\n\n        """"""\n        xmin = max(self.xmin, other.xmin)\n        ymin = max(self.ymin, other.ymin)\n        xmax = min(self.xmax, other.xmax)\n        ymax = min(self.ymax, other.ymax)\n        return Box(xmin=xmin, ymin=ymin, xmax=xmax, ymax=ymax)\n\n    @staticmethod\n    def from_npbox(npbox):\n        """"""Return new Box based on npbox format.\n\n        Args:\n            npbox: Numpy array of form [ymin, xmin, ymax, xmax] with float type\n\n        """"""\n        return Box(*npbox)\n\n    @staticmethod\n    def from_shapely(shape):\n        bounds = shape.bounds\n        return Box(bounds[1], bounds[0], bounds[3], bounds[2])\n\n    @staticmethod\n    def from_tuple(tup):\n        """"""Return new Box based on tuple format.\n\n        Args:\n           tup: Tuple format box (ymin, xmin, ymax, xmax)\n        """"""\n        return Box(tup[0], tup[1], tup[2], tup[3])\n\n    def to_shapely(self):\n        return ShapelyBox(*(self.shapely_format()))\n\n    def reproject(self, transform_fn):\n        """"""Reprojects this box based on a transform function.\n\n        Args:\n          transform_fn - A function that takes in a tuple (x, y)\n                         and reprojects that point to the target\n                         coordinate reference system.\n        """"""\n        (xmin, ymin) = transform_fn((self.xmin, self.ymin))\n        (xmax, ymax) = transform_fn((self.xmax, self.ymax))\n\n        return Box(ymin, xmin, ymax, xmax)\n\n    @staticmethod\n    def make_square(ymin, xmin, size):\n        """"""Return new square Box.""""""\n        return Box(ymin, xmin, ymin + size, xmin + size)\n\n    def make_eroded(self, erosion_size):\n        """"""Return new Box whose sides are eroded by erosion_size.""""""\n        return Box(self.ymin + erosion_size, self.xmin + erosion_size,\n                   self.ymax - erosion_size, self.xmax - erosion_size)\n\n    def make_buffer(self, buffer_size, max_extent):\n        """"""Return new Box whose sides are buffered by buffer_size.\n\n        The resulting box is clipped so that the values of the corners are\n        always greater than zero and less than the height and width of\n        max_extent.\n\n        """"""\n        buffer_size = max(0., buffer_size)\n        if buffer_size < 1.:\n            delta_width = int(round(buffer_size * self.get_width()))\n            delta_height = int(round(buffer_size * self.get_height()))\n        else:\n            delta_height = delta_width = int(round(buffer_size))\n\n        return Box(\n            max(0, math.floor(self.ymin - delta_height)),\n            max(0, math.floor(self.xmin - delta_width)),\n            min(max_extent.get_height(),\n                int(self.ymax) + delta_height),\n            min(max_extent.get_width(),\n                int(self.xmax) + delta_width))\n\n    def make_copy(self):\n        return Box(*(self.tuple_format()))\n\n    def get_windows(self, chip_size, stride):\n        """"""Return list of grid of boxes within this box.\n\n        Args:\n            chip_size: (int) the length of each square-shaped window in pixels\n            stride: (int) how much each window is offset from the last in pixels\n\n        """"""\n        height = self.get_height()\n        width = self.get_width()\n\n        result = []\n        for row_start in range(0, height, stride):\n            for col_start in range(0, width, stride):\n                result.append(Box.make_square(row_start, col_start, chip_size))\n        return result\n\n    def to_dict(self):\n        return {\n            \'xmin\': self.xmin,\n            \'ymin\': self.ymin,\n            \'xmax\': self.xmax,\n            \'ymax\': self.ymax\n        }\n\n    @classmethod\n    def from_dict(cls, d):\n        return cls(d[\'ymin\'], d[\'xmin\'], d[\'ymax\'], d[\'xmax\'])\n\n    @staticmethod\n    def filter_by_aoi(windows, aoi_polygons):\n        """"""Filters windows by a list of AOI polygons""""""\n        result = []\n        for window in windows:\n            w = window.to_shapely()\n            for polygon in aoi_polygons:\n                if w.within(polygon):\n                    result.append(window)\n                    break\n\n        return result\n'"
rastervision/core/class_map.py,0,"b'from rastervision.protos.class_item_pb2 \\\n    import ClassItem as ClassItemMsg\n\n\nclass ClassItem(object):\n    """"""A class id and associated data.""""""\n\n    def __init__(self, id: int, name: str = None, color=None):\n        """"""Construct a new ClassItem.\n\n        Color is picked randomly if it is a null value.\n\n        Args:\n            id: (int) class id\n            name: (string) name of the class\n            color: (string) Pillow color code\n        """"""\n        self.id = id\n        self.name = name\n        self.color = color\n\n    def __eq__(self, other):\n        if isinstance(other, ClassItem):\n            return (self.id == other.id and self.name == other.name\n                    and self.color == other.color)\n        return False  # pragma: no cover\n\n    def __repr__(self):  # pragma: no cover\n        s = \'CLASS ITEM: [{}] {}\'.format(self.id, self.name)\n        if self.color:\n            s += \' ({})\'.format(self.color)\n        return s\n\n    def to_proto(self):\n        return ClassItemMsg(id=self.id, name=self.name, color=self.color)\n\n    @staticmethod\n    def from_proto(msg):\n        return ClassItem(id=msg.id, name=msg.name, color=msg.color)\n\n\nclass ClassMap(object):\n    """"""A map from class_id to ClassItem.\n\n    The class ids should be integers >= 1. For semantic segmentation,\n    the class 0 is reserved for use as an ""ignore"" class, which denotes\n    pixels that should not be used for training the model or evaluating it.\n    """"""\n\n    def __init__(self, class_items):\n        """"""Construct a new ClassMap.\n\n        Args:\n            class_items: list of ClassItems\n        """"""\n        self.class_item_map = {}\n        for class_item in class_items:\n            self.class_item_map[class_item.id] = class_item\n\n    def copy(self):\n        return ClassMap(self.class_item_map.values())\n\n    def add_nodata_item(self):\n        if 0 not in self.get_keys():\n            self.class_item_map[0] = ClassItem(0, \'nodata\', \'grey\')\n\n    def get_by_id(self, id):\n        """"""Return a ClassItem by its id.\n\n        Args:\n            id: (int) id of class\n        """"""\n        return self.class_item_map[id]\n\n    def get_by_name(self, name):\n        for item in self.get_items():\n            if name == item.name:\n                return item\n        raise ValueError(\'{} is not a name in this ClassMap.\'.format(name))\n\n    def get_keys(self):\n        """"""Return the keys.""""""\n        return list(self.class_item_map.keys())\n\n    def get_items(self):\n        """"""Return list of ClassItems.""""""\n        return list(self.class_item_map.values())\n\n    def get_class_names(self):\n        """"""Return list of class names sorted by id.""""""\n        sorted_items = sorted(self.get_items(), key=lambda item: item.id)\n        return [item.name for item in sorted_items]\n\n    def __len__(self):\n        return len(self.get_items())\n\n    def has_all_colors(self):\n        for item in self.get_items():\n            if not item.color:\n                return False\n        return True\n\n    def get_category_index(self):\n        """"""Get the corresponding category_index used by TF Object Detection.""""""\n        category_index = {}\n        for class_item in self.get_items():\n            category_index[class_item.id] = {\n                \'id\': class_item.id,\n                \'name\': class_item.name\n            }\n        return category_index\n\n    def to_proto(self):\n        """"""Transform a ClassMap into\n        a list of ClassItem protobuf messages\n        """"""\n        return [item.to_proto() for item in self.get_items()]\n\n    @staticmethod\n    def construct_from(classes):\n        """"""Construct ClassMap from a number of different\n           representations.\n\n            Args:\n                classes: One of the following:\n                         - a ClassMap\n                         - a list of class names\n                         - a list of ClassItem protobuf messages\n                         - a list of ClassItems\n                         - a dict which maps class names to class ids\n                         - a dict which maps class names to a tuple of\n                           (class_id, color), where color is a PIL color string.\n        """"""\n        result = None\n        if type(classes) is ClassMap:\n            result = classes\n        elif type(classes) is dict:\n            item_list = []\n            if not len(classes.items()) == 0:\n                if type(list(classes.items())[0][1]) is tuple:\n                    # This dict already has colors mapped to class ids\n                    for name, (class_id, color) in classes.items():\n                        item_list.append(ClassItem(class_id, name, color))\n                else:\n                    # Map items to empty colors\n                    for name, class_id in classes.items():\n                        item_list.append(ClassItem(class_id, name))\n            result = ClassMap(item_list)\n        elif type(classes) is list:\n            item_list = []\n            if not len(classes) == 0:\n                if type(classes[0]) is ClassItemMsg:\n                    for item in classes:\n                        item_list.append(\n                            ClassItem(item.id, item.name, item.color))\n                elif type(classes[0]) is str:\n                    for i, name in enumerate(classes):\n                        item_list.append(ClassItem(i + 1, name))\n                else:\n                    item_list = classes\n            result = ClassMap(item_list)\n        else:\n            raise Exception(\'Cannot convert type {} to ClassMap\'.format(\n                type(classes)))\n\n        return result\n'"
rastervision/core/command_io_definition.py,0,"b'class CommandIODefinition:\n    """"""Class which contains a set of inputs and outputs for a command,\n       based on the configuration.\n    """"""\n\n    def __init__(self,\n                 input_uris=None,\n                 output_uris=None,\n                 missing_input_messages=None):\n        if input_uris is None:\n            input_uris = set([])\n        if output_uris is None:\n            output_uris = set([])\n        if missing_input_messages is None:\n            missing_input_messages = []\n\n        self.input_uris = input_uris\n        self.output_uris = output_uris\n\n        # Messages that declare missing inputs\n        self.missing_input_messages = missing_input_messages\n\n    def merge(self, other):\n        self.input_uris = self.input_uris.union(other.input_uris)\n        self.output_uris = self.output_uris.union(other.output_uris)\n        self.missing_input_messages = self.missing_input_messages + \\\n                                      other.missing_input_messages\n\n    def add_input(self, input_uri):\n        self.input_uris.add(input_uri)\n\n    def add_inputs(self, input_uris):\n        self.input_uris = self.input_uris.union(set(input_uris))\n\n    def add_output(self, output_uri):\n        self.output_uris.add(output_uri)\n\n    def add_outputs(self, output_uris):\n        self.output_uris = self.output_uris.union(set(output_uris))\n\n    def add_missing(self, message):\n        self.missing_input_messages.append(message)\n'"
rastervision/core/config.py,0,"b'from abc import (ABC, abstractmethod)\nimport os\nimport inspect\n\nfrom rastervision.utils.files import download_or_copy\n\n\nclass ConfigError(Exception):\n    pass\n\n\nclass Config(ABC):\n    @abstractmethod\n    def to_builder(self):\n        """"""Return a builder based on this config.\n        """"""\n        pass  # pragma: no cover\n\n    @abstractmethod\n    def to_proto(self):\n        """"""Returns the protobuf configuration for this config.\n        """"""\n        pass  # pragma: no cover\n\n    def update_for_command(self,\n                           command_type,\n                           experiment_config,\n                           context=None,\n                           io_def=None):\n        """"""Updates this configuration for the given command\n\n        Note: While configuration is immutable for client facing operations,\n        this is an internal operation and mutates the configuration.\n\n        Args:\n            command_type: The command type that is currently being\n                preprocessed. experiment_config: The experiment configuration\n                that this configuration is a part of.\n            context: Optional list of parent configurations, to allow for child\n                configurations contained in collections to understand their\n                context in the experiment configuration.\n\n        Returns:\n          Nothing. Call should mutate the configuration object itself.\n        """"""\n        pass  # pragma: no cover\n\n    @abstractmethod\n    def report_io(self, command_type, io_def):\n        """"""Updates the given CommandIODefinition.\n\n        So that it includes the inputs, outputs, and missing files for this\n        configuration at this command.\n\n        Args:\n            command_type: The command type that is currently being preprocessed.\n            io_def: The CommandIODefinition that this call should modify.\n\n        Returns: Nothing. This call should make the appropriate calls to the\n            given io_def to mutate its state.\n        """"""\n        pass\n\n    @staticmethod\n    @abstractmethod\n    def builder():\n        """"""Returns a new builder that takes this configuration\n           as its starting point.\n        """"""\n        pass  # pragma: no cover\n\n    @staticmethod\n    @abstractmethod\n    def from_proto(msg):\n        """"""Creates a Config from the specificed protobuf message\n        TODO: Allow loading from file uri or dict\n        """"""\n        pass  # pragma: no cover\n\n\nclass ConfigBuilder(ABC):\n    def __init__(self, config_class, config=None):\n        """"""Construct a builder.\n\n           Args:\n             config_class: The Config class that this builder builds.\n             config: A dictionary of **kwargs that will eventually be passed\n                     into the __init__ method of config_class to build the configuration.\n                     This config is modified with the fluent builder methods.\n        """"""\n        if config is None:  # pragma: no cover\n            config = {}\n\n        self.config_class = config_class\n        self.config = config\n\n    def build(self):\n        """"""Returns the configuration that is built by this builder.\n        """"""\n        self.validate()\n        arguments = set(inspect.getfullargspec(self.config_class).args)\n        keys = set(self.config.keys())\n        config = {k: self.config[k] for k in (arguments & keys)}\n        return self.config_class(**config)\n\n    def validate(self):\n        """"""Validate this config, if there is validation on the builder that\n           is not captured by the required arguments of the config.\n        """"""\n        pass  # pragma: no cover\n\n    @abstractmethod\n    def from_proto(self, msg):\n        """"""Return a builder that takes the configuration from the proto message\n           as its starting point.\n        """"""\n        pass  # pragma: no cover\n\n\nclass BundledConfigMixin(ABC):\n    """"""Mixin for configurations that participate in the bundling of a\n    prediction package""""""\n\n    @abstractmethod\n    def save_bundle_files(self, bundle_dir):\n        """"""Place files into a bundle directory for bundling into\n        a prediction package.\n\n        Returns: A tuple of (config, uris) of the modified configuration\n                 with the basenames of URIs in place of the original URIs,\n                 and a list of URIs that are to be bundled.\n        """"""\n        pass  # pragma: no cover\n\n    def bundle_file(self, uri, bundle_dir):\n        local_path = download_or_copy(uri, bundle_dir)\n        base_name = os.path.basename(local_path)\n        return (local_path, base_name)\n\n    @abstractmethod\n    def load_bundle_files(self, bundle_dir):\n        """"""Load files from a prediction package bundle directory.""""""\n        pass  # pragma: no cover\n'"
rastervision/core/raster_stats.py,0,"b'import json\n\nimport numpy as np\n\nfrom rastervision.utils.files import str_to_file, file_to_str\n\nchip_size = 300\n\n\ndef parallel_variance(mean_a, count_a, var_a, mean_b, count_b, var_b):\n    """"""Compute the variance based on stats from two partitions of the data.\n\n    See ""Parallel Algorithm"" in\n    https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance\n\n    Args:\n        mean_a: the mean of partition a\n        count_a: the number of elements in partition a\n        var_a: the variance of partition a\n        mean_b: the mean of partition b\n        count_b: the number of elements in partition b\n        var_b: the variance of partition b\n\n    Return:\n        the variance of the two partitions if they were combined\n    """"""\n    delta = mean_b - mean_a\n    m_a = var_a * (count_a - 1)\n    m_b = var_b * (count_b - 1)\n    M2 = m_a + m_b + delta**2 * count_a * count_b / (count_a + count_b)\n    var = M2 / (count_a + count_b - 1)\n    return var\n\n\ndef parallel_mean(mean_a, count_a, mean_b, count_b):\n    """"""Compute the mean based on stats from two partitions of the data.\n\n    See ""Parallel Algorithm"" in\n    https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance\n\n    Args:\n        mean_a: the mean of partition a\n        count_a: the number of elements in partition a\n        mean_b: the mean of partition b\n        count_b: the number of elements in partition b\n\n    Return:\n        the mean of the two partitions if they were combined\n    """"""\n    mean = (count_a * mean_a + count_b * mean_b) / (count_a + count_b)\n    return mean\n\n\nclass RasterStats():\n    def __init__(self):\n        self.means = None\n        self.stds = None\n\n    def compute(self, raster_sources, sample_prob=None):\n        """"""Compute the mean and stds over all the raster_sources.\n\n        This ignores NODATA values.\n\n        If sample_prob is set, then a subset of each scene is used to compute stats which\n        speeds up the computation. Roughly speaking, if sample_prob=0.5, then half the\n        pixels in the scene will be used. More precisely, the number of chips is equal to\n        sample_prob * (width * height / 300^2), or 1, whichever is greater. Each chip is\n        uniformly sampled from the scene with replacement. Otherwise, it uses a sliding\n        window over the entire scene to compute stats.\n\n        Args:\n            raster_sources: list of RasterSource\n            sample_prob: (float or None) between 0 and 1\n        """"""\n        stride = chip_size\n        nb_channels = raster_sources[0].num_channels\n\n        def get_chip(raster_source, window):\n            """"""Return chip or None if all values are NODATA.""""""\n            chip = raster_source.get_raw_chip(window).astype(np.float32)\n            # Convert shape from [h,w,c] to [c,h*w]\n            chip = np.reshape(np.transpose(chip, [2, 0, 1]), (nb_channels, -1))\n\n            # Ignore NODATA values.\n            chip[chip == 0.0] = np.nan\n            if np.any(~np.isnan(chip)):\n                return chip\n            return None\n\n        def sliding_chip_stream():\n            """"""Get stream of chips using a sliding window of size 300.""""""\n            for raster_source in raster_sources:\n                with raster_source.activate():\n                    windows = raster_source.get_extent().get_windows(\n                        chip_size, stride)\n                    for window in windows:\n                        chip = get_chip(raster_source, window)\n                        if chip is not None:\n                            yield chip\n\n        def random_chip_stream():\n            """"""Get random stream of chips.""""""\n            for raster_source in raster_sources:\n                with raster_source.activate():\n                    extent = raster_source.get_extent()\n                    num_pixels = extent.get_width() * extent.get_height()\n                    num_chips = round(\n                        sample_prob * (num_pixels / (chip_size**2)))\n                    num_chips = max(1, num_chips)\n                    for _ in range(num_chips):\n                        window = raster_source.get_extent().make_random_square(\n                            chip_size)\n                        chip = get_chip(raster_source, window)\n                        if chip is not None:\n                            yield chip\n\n        # For each chip, compute the mean and var of that chip and then update the\n        # running mean and var.\n        count = 0\n        mean = np.zeros((nb_channels, ))\n        var = np.zeros((nb_channels, ))\n        chip_stream = (sliding_chip_stream()\n                       if sample_prob is None else random_chip_stream())\n\n        for c in chip_stream:\n            chip_means = np.nanmean(c, axis=1)\n            chip_vars = np.nanvar(c, axis=1)\n            chip_count = np.sum(c[0] != np.nan)\n\n            var = parallel_variance(chip_means, chip_count, chip_vars, mean,\n                                    count, var)\n            mean = parallel_mean(chip_means, chip_count, mean, count)\n            count += chip_count\n\n        self.means = mean\n        self.stds = np.sqrt(var)\n\n    def save(self, stats_uri):\n        # Ensure lists\n        means = list(self.means)\n        stds = list(self.stds)\n        stats = {\'means\': means, \'stds\': stds}\n        str_to_file(json.dumps(stats), stats_uri)\n\n    @staticmethod\n    def load(stats_uri):\n        stats_json = json.loads(file_to_str(stats_uri))\n        stats = RasterStats()\n        stats.means = stats_json[\'means\']\n        stats.stds = stats_json[\'stds\']\n        return stats\n'"
rastervision/core/training_data.py,0,"b'import random\n\n\nclass TrainingData(object):\n    """"""A set of chips, windows, and labels used to train a model.""""""\n\n    def __init__(self):\n        """"""Construct a new TrainingData.""""""\n        self.chips = []\n        self.windows = []\n        self.labels = []\n\n    def append(self, chip, window, labels):\n        """"""Append a chip and associated labels to the dataset.\n\n        Args:\n            chip: [height, width, channels] numpy array\n            window: Box with coordinates of chip\n            labels: Labels\n        """"""\n        self.chips.append(chip)\n        self.windows.append(window)\n        self.labels.append(labels)\n\n    def __iter__(self):\n        return zip(self.chips, self.windows, self.labels)\n\n    def shuffle(self):\n        """"""Randomly shuffle the chips and labels in-place.\n\n        This maintains the correspondence between chips and labels.\n        """"""\n        if len(self.chips) > 0:\n            chip_windows_labels = list(self)\n            random.shuffle(chip_windows_labels)\n            # Unzip the list.\n            self.chips, self.windows, self.labels = zip(*chip_windows_labels)\n'"
rastervision/data/__init__.py,0,b'# flake8: noqa\n\nfrom rastervision.data.activate_mixin import *\nfrom rastervision.data.raster_transformer import *\nfrom rastervision.data.raster_source import *\nfrom rastervision.data.crs_transformer import *\nfrom rastervision.data.label import *\nfrom rastervision.data.vector_source import *\nfrom rastervision.data.label_source import *\nfrom rastervision.data.label_store import *\nfrom rastervision.data.scene import *\nfrom rastervision.data.scene_config import *\nfrom rastervision.data.dataset import *\nfrom rastervision.data.dataset_config import *\n'
rastervision/data/activate_mixin.py,0,"b'from abc import abstractmethod\n\n\nclass ActivationError(Exception):\n    pass\n\n\nclass ActivateMixin:\n    """"""Defines a mixin for data that can activate and deactivate.\n    These methods can open and close files, download files, and do\n    whatever has to be done to make the entity usable, and cleanup\n    after the entity is not needed anymore.\n    """"""\n\n    class ActivateContextManager:\n        def __init__(self, activate, deactivate):\n            self.activate = activate\n            self.deactivate = deactivate\n\n        def __enter__(self):\n            self.activate()\n            return self\n\n        def __exit__(self, type, value, traceback):\n            self.deactivate()\n\n        @classmethod\n        def dummy(cls):\n            def noop():\n                pass\n\n            return cls(noop, noop)\n\n    class CompositeContextManager:\n        def __init__(self, *managers):\n            self.managers = managers\n\n        def __enter__(self):\n            for manager in self.managers:\n                manager.__enter__()\n\n        def __exit__(self, type, value, traceback):\n            for manager in self.managers:\n                manager.__exit__(type, value, traceback)\n\n    def activate(self):\n        if hasattr(self, \'_mixin_activated\'):\n            if self._mixin_activated:\n                raise ActivationError(\'This {} is already activated\'.format(\n                    type(self)))\n\n        def do_activate():\n            self._mixin_activated = True\n            self._activate()\n\n        def do_deactivate():\n            self._deactivate()\n            self._mixin_activated = False\n\n        a = ActivateMixin.ActivateContextManager(do_activate, do_deactivate)\n        subcomponents = self._subcomponents_to_activate()\n        if subcomponents:\n            return ActivateMixin.CompositeContextManager(\n                a, ActivateMixin.compose(*subcomponents))\n        else:\n            return a\n\n    @abstractmethod\n    def _activate(self):\n        pass\n\n    @abstractmethod\n    def _deactivate(self):\n        pass\n\n    def _subcomponents_to_activate(self):\n        """"""Subclasses override this if they have subcomponents\n        that may need to be activated when this class is activated\n        """"""\n        return []\n\n    @staticmethod\n    def with_activation(obj):\n        """"""Method will give activate an object if it mixes in  the ActivateMixin and\n        return the context manager, or else return a dummy context manager.\n        """"""\n        if obj is None or not isinstance(obj, ActivateMixin):\n            return ActivateMixin.dummy()\n        else:\n            return obj.activate()\n\n    @staticmethod\n    def compose(*objs):\n        managers = [\n            obj.activate() for obj in objs\n            if obj is not None and isinstance(obj, ActivateMixin)\n        ]\n        return ActivateMixin.CompositeContextManager(*managers)\n'"
rastervision/data/api.py,0,b'# flake8: noqa\n\nfrom rastervision.data.raster_source.api import *\nfrom rastervision.data.raster_transformer.api import *\nfrom rastervision.data.label_source.api import *\nfrom rastervision.data.label_store.api import *\nfrom rastervision.data.vector_source.api import *\n\nfrom rastervision.data.dataset_config import DatasetConfig\nfrom rastervision.data.scene_config import SceneConfig\n'
rastervision/data/dataset.py,0,"b'class Dataset:\n    def __init__(self,\n                 train_scenes=[],\n                 validation_scenes=[],\n                 test_scenes=[],\n                 augmentors=[]):\n        self.train_scenes = train_scenes\n        self.validation_scenes = validation_scenes\n        self.test_scenes = test_scenes\n        self.augmentors = augmentors\n'"
rastervision/data/dataset_config.py,0,"b'import logging\n\nimport click\n\nimport rastervision as rv\nfrom rastervision.augmentor import AugmentorConfig\nfrom rastervision.data import (SceneConfig, Dataset)\nfrom rastervision.core.config import (Config, ConfigBuilder)\nfrom rastervision.protos.dataset_pb2 import DatasetConfig as DatasetConfigMsg\nfrom rastervision.cli import Verbosity\n\nlog = logging.getLogger(__name__)\n\n\nclass DatasetConfig(Config):\n    def __init__(self,\n                 train_scenes=None,\n                 validation_scenes=None,\n                 test_scenes=None,\n                 augmentors=None):\n        if train_scenes is None:\n            train_scenes = []\n        if validation_scenes is None:\n            validation_scenes = []\n        if test_scenes is None:\n            test_scenes = []\n        if augmentors is None:\n            augmentors = []\n\n        self.train_scenes = train_scenes\n        self.validation_scenes = validation_scenes\n        self.test_scenes = test_scenes\n        self.augmentors = augmentors\n\n    def all_scenes(self):\n        return self.train_scenes + \\\n            self.validation_scenes + \\\n            self.test_scenes\n\n    def to_builder(self):\n        return DatasetConfigBuilder(self)\n\n    def create_dataset(self,\n                       task_config,\n                       tmp_dir,\n                       include_train=True,\n                       include_val=True,\n                       include_test=True):\n        train_scenes = []\n        if include_train:\n            train_scenes = list(\n                map(lambda x: x.create_scene(task_config, tmp_dir),\n                    self.train_scenes))\n\n        val_scenes = []\n        if include_val:\n            val_scenes = list(\n                map(lambda x: x.create_scene(task_config, tmp_dir),\n                    self.validation_scenes))\n\n        test_scenes = []\n        if include_test:\n            test_scenes = list(\n                map(lambda x: x.create_scene(task_config, tmp_dir),\n                    self.test_scenes))\n\n        augmentors = list(map(lambda x: x.create_augmentor(), self.augmentors))\n\n        return Dataset(\n            train_scenes=train_scenes,\n            validation_scenes=val_scenes,\n            test_scenes=test_scenes,\n            augmentors=augmentors)\n\n    def to_proto(self):\n        """"""Returns the protobuf configuration for this config.\n        """"""\n        train_scenes = list(map(lambda x: x.to_proto(), self.train_scenes))\n        val_scenes = list(map(lambda x: x.to_proto(), self.validation_scenes))\n        test_scenes = list(map(lambda x: x.to_proto(), self.test_scenes))\n\n        augmentors = list(map(lambda x: x.to_proto(), self.augmentors))\n\n        return DatasetConfigMsg(\n            train_scenes=train_scenes,\n            validation_scenes=val_scenes,\n            test_scenes=test_scenes,\n            augmentors=augmentors)\n\n    def update_for_command(self,\n                           command_type,\n                           experiment_config,\n                           context=None,\n                           io_def=None):\n        verbosity = Verbosity.get()\n\n        def update_scenes(scenes_to_update, ensure_label_store):\n            for scene in scenes_to_update:\n                if ensure_label_store:\n                    # Ensure there is a label store associated with\n                    # validation and test scenes on PREDICT command.\n                    if not scene.label_store:\n                        scene.label_store = scene.to_builder() \\\n                                                 .with_task(experiment_config.task) \\\n                                                 .with_label_store() \\\n                                                 .build() \\\n                                                 .label_store\n                scene.update_for_command(command_type, experiment_config,\n                                         context)\n\n        if command_type in [rv.ANALYZE, rv.CHIP]:\n            log.debug(\n                \'Updating train scenes for command {}\'.format(command_type))\n            if verbosity >= Verbosity.VERBOSE:\n                with click.progressbar(\n                        self.train_scenes,\n                        label=\'Updating train scenes\') as scenes_to_update:\n                    update_scenes(scenes_to_update, False)\n            else:\n                update_scenes(self.train_scenes, False)\n\n        if command_type in [\n                rv.ANALYZE, rv.CHIP, rv.PREDICT, rv.EVAL, rv.BUNDLE\n        ]:\n            log.debug(\'Updating validation scenes for command {}\'.format(\n                command_type))\n            if Verbosity.get() >= Verbosity.VERBOSE:\n                with click.progressbar(\n                        self.validation_scenes,\n                        label=\'Updating validation scenes...  \'\n                ) as scenes_to_update:\n                    update_scenes(scenes_to_update, command_type == rv.PREDICT)\n            else:\n                update_scenes(self.validation_scenes,\n                              command_type == rv.PREDICT)\n\n        if command_type in [rv.ANALYZE, rv.PREDICT, rv.EVAL, rv.BUNDLE]:\n\n            log.debug(\n                \'Updating test scenes for command {}\'.format(command_type))\n            if Verbosity.get() >= Verbosity.VERBOSE:\n                with click.progressbar(\n                        self.test_scenes,\n                        label=\'Updating test scenes...  \') as scenes_to_update:\n                    update_scenes(scenes_to_update, command_type == rv.PREDICT)\n            else:\n                update_scenes(self.test_scenes, command_type == rv.PREDICT)\n\n        if command_type == rv.CHIP:\n            log.debug(\n                \'Updating augmentors for command {}\'.format(command_type))\n            for augmentor in self.augmentors:\n                augmentor.update_for_command(command_type, experiment_config,\n                                             context)\n\n    def report_io(self, command_type, io_def):\n        if command_type in [rv.ANALYZE, rv.CHIP]:\n            for scene in self.train_scenes:\n                scene.report_io(command_type, io_def)\n\n        if command_type in [\n                rv.ANALYZE, rv.CHIP, rv.PREDICT, rv.EVAL, rv.BUNDLE\n        ]:\n            for scene in self.validation_scenes:\n                scene.report_io(command_type, io_def)\n\n        if command_type in [rv.ANALYZE, rv.PREDICT, rv.EVAL, rv.BUNDLE]:\n            for scene in self.test_scenes:\n                scene.report_io(command_type, io_def)\n\n        if command_type == rv.CHIP:\n            for augmentor in self.augmentors:\n                augmentor.report_io(command_type, io_def)\n\n        return io_def\n\n    @staticmethod\n    def from_proto(msg):\n        """"""Creates a TaskConfig from the specificed protobuf message\n        """"""\n        return DatasetConfigBuilder().from_proto(msg).build()\n\n    @staticmethod\n    def builder():\n        return DatasetConfigBuilder()\n\n\nclass DatasetConfigBuilder(ConfigBuilder):\n    def __init__(self, prev=None):\n        config = {\n            \'train_scenes\': [],\n            \'validation_scenes\': [],\n            \'test_scenes\': [],\n            \'augmentors\': []\n        }\n        if prev:\n            config[\'train_scenes\'] = prev.train_scenes\n            config[\'validation_scenes\'] = prev.validation_scenes\n            config[\'test_scenes\'] = prev.test_scenes\n            config[\'augmentors\'] = prev.augmentors\n        super().__init__(DatasetConfig, config)\n\n    def _copy(self):\n        """"""Create a copy; avoid using deepcopy as it can have\n        performance implications with many scenes\n        """"""\n        ds = DatasetConfigBuilder()\n        ds.config[\'train_scenes\'] = self.config[\'train_scenes\']\n        ds.config[\'validation_scenes\'] = self.config[\'validation_scenes\']\n        ds.config[\'test_scenes\'] = self.config[\'test_scenes\']\n        ds.config[\'augmentors\'] = self.config[\'augmentors\']\n        return ds\n\n    def from_proto(self, msg):\n        train_scenes = list(\n            map(lambda x: SceneConfig.from_proto(x), msg.train_scenes))\n        val_scenes = list(\n            map(lambda x: SceneConfig.from_proto(x), msg.validation_scenes))\n        test_scenes = list(\n            map(lambda x: SceneConfig.from_proto(x), msg.test_scenes))\n        augmentors = list(\n            map(lambda x: AugmentorConfig.from_proto(x), msg.augmentors))\n        return self \\\n            .with_train_scenes(train_scenes) \\\n            .with_validation_scenes(val_scenes) \\\n            .with_test_scenes(test_scenes) \\\n            .with_augmentors(augmentors)\n\n    def with_train_scenes(self, scenes):\n        """"""Sets the scenes to be used for training.""""""\n        b = self._copy()\n        b.config[\'train_scenes\'] = list(scenes)\n        return b\n\n    def with_train_scene(self, scene):\n        """"""Sets the scene to be used for training.""""""\n        return self.with_train_scenes([scene])\n\n    def with_validation_scenes(self, scenes):\n        """"""Sets the scenes to be used for validation.""""""\n        b = self._copy()\n        b.config[\'validation_scenes\'] = list(scenes)\n        return b\n\n    def with_validation_scene(self, scene):\n        """"""Sets the scene to be used for validation.""""""\n        return self.with_validation_scenes([scene])\n\n    def with_test_scenes(self, scenes):\n        """"""Sets the scenes to be used for testing.""""""\n        b = self._copy()\n        b.config[\'test_scenes\'] = list(scenes)\n        return b\n\n    def with_test_scene(self, scene):\n        """"""Sets the scene to be used for testing.""""""\n        return self.with_test_scenes([scene])\n\n    def with_augmentors(self, augmentors):\n        """"""Sets the data augmentors to be used.""""""\n        b = self._copy()\n        b.config[\'augmentors\'] = list(augmentors)\n        return b\n\n    def with_augmentor(self, augmentor):\n        """"""Sets the data augmentor to be used.""""""\n        return self.with_augmentors([augmentor])\n'"
rastervision/data/scene.py,0,"b'from rastervision.data import ActivateMixin\n\n\nclass Scene(ActivateMixin):\n    """"""The raster data and labels associated with an area of interest.""""""\n\n    def __init__(self,\n                 id,\n                 raster_source,\n                 ground_truth_label_source=None,\n                 prediction_label_store=None,\n                 aoi_polygons=None):\n        """"""Construct a new Scene.\n\n        Args:\n            id: ID for this scene\n            raster_source: RasterSource for this scene\n            ground_truth_label_store: optional LabelSource\n            prediction_label_store: optional LabelStore\n            aoi: Optional list of AOI polygons\n        """"""\n        self.id = id\n        self.raster_source = raster_source\n        self.ground_truth_label_source = ground_truth_label_source\n        self.prediction_label_store = prediction_label_store\n        if aoi_polygons is None:\n            self.aoi_polygons = []\n        else:\n            self.aoi_polygons = aoi_polygons\n\n    def _subcomponents_to_activate(self):\n        return [\n            self.raster_source, self.ground_truth_label_source,\n            self.prediction_label_store\n        ]\n\n    def _activate(self):\n        pass\n\n    def _deactivate(self):\n        pass\n'"
rastervision/data/scene_config.py,0,"b'from copy import deepcopy\nfrom typing import Union\n\nfrom shapely.geometry import shape\n\nimport rastervision as rv\nfrom rastervision.core import (Config, ConfigBuilder, BundledConfigMixin)\nfrom rastervision.task import TaskConfig\nfrom rastervision.data import (Scene, RasterSourceConfig, LabelSourceConfig,\n                               LabelStoreConfig, GeoJSONVectorSource)\nfrom rastervision.protos.scene_pb2 \\\n    import SceneConfig as SceneConfigMsg\n\n\nclass SceneConfig(BundledConfigMixin, Config):\n    def __init__(self,\n                 id,\n                 raster_source,\n                 label_source=None,\n                 label_store=None,\n                 aoi_uris=None):\n        self.id = id\n        self.raster_source = raster_source\n        self.label_source = label_source\n        self.label_store = label_store\n        self.aoi_uris = aoi_uris\n\n    def create_scene(self, task_config: TaskConfig, tmp_dir: str) -> Scene:\n        """"""Create this scene.\n\n           Args:\n              task - TaskConfig\n              tmp_dir - Temporary directory to use\n        """"""\n        raster_source = self.raster_source.create_source(tmp_dir)\n        extent = raster_source.get_extent()\n        crs_transformer = raster_source.get_crs_transformer()\n\n        label_source = None\n        if self.label_source:\n            label_source = self.label_source.create_source(\n                task_config, extent, crs_transformer, tmp_dir)\n        label_store = None\n        if self.label_store:\n            label_store = self.label_store.create_store(\n                task_config, extent, crs_transformer, tmp_dir)\n        aoi_polygons = None\n        if self.aoi_uris:\n            aoi_polygons = []\n            for uri in self.aoi_uris:\n                aoi_geojson = GeoJSONVectorSource(\n                    uri, crs_transformer).get_geojson()\n                for f in aoi_geojson[\'features\']:\n                    aoi_polygons.append(shape(f[\'geometry\']))\n\n        return Scene(self.id, raster_source, label_source, label_store,\n                     aoi_polygons)\n\n    def to_proto(self):\n        msg = SceneConfigMsg(\n            id=self.id,\n            raster_source=self.raster_source.to_proto(),\n            aoi_uris=self.aoi_uris)\n\n        if self.label_source:\n            msg.ground_truth_label_source.CopyFrom(\n                self.label_source.to_proto())\n        if self.label_store:\n            msg.prediction_label_store.CopyFrom(self.label_store.to_proto())\n        return msg\n\n    def save_bundle_files(self, bundle_dir):\n        new_source, files = self.raster_source.save_bundle_files(bundle_dir)\n        new_config = self.to_builder() \\\n                         .with_raster_source(new_source) \\\n                         .build()\n        return (new_config, files)\n\n    def load_bundle_files(self, bundle_dir):\n        new_source = self.raster_source.load_bundle_files(bundle_dir)\n        return self.to_builder() \\\n                   .with_raster_source(new_source) \\\n                   .build()\n\n    def for_prediction(self, image_uri, label_uri=None):\n        """"""Creates a version of this scene that is set to\n        predict against the image_uri. If label_uri is set,\n        the scene must already have a label_store.\n        """"""\n        new_source = self.raster_source.for_prediction(image_uri)\n        b = self.to_builder().with_raster_source(new_source)\n\n        if label_uri:\n            if not self.label_store:\n                raise rv.ConfigError(\'Cannot call for_prediciton on  a \'\n                                     \'scene that does not have a label \'\n                                     \'store set.\')\n            new_store = self.label_store.for_prediction(label_uri)\n            b = b.with_label_store(new_store)\n\n        return b.build()\n\n    def create_local(self, tmp_dir):\n        new_source = self.raster_source.create_local(tmp_dir)\n        return self.to_builder() \\\n                   .with_raster_source(new_source) \\\n                   .build()\n\n    def to_builder(self):\n        return SceneConfigBuilder(self)\n\n    def update_for_command(self, command_type, experiment_config,\n                           context=None):\n        if context is None:\n            context = []\n        context = context + [self]\n\n        self.raster_source.update_for_command(command_type, experiment_config,\n                                              context)\n\n        if self.label_source:\n            self.label_source.update_for_command(command_type,\n                                                 experiment_config, context)\n\n        if self.label_store:\n            self.label_store.update_for_command(command_type,\n                                                experiment_config, context)\n\n    def report_io(self, command_type, io_def):\n        self.raster_source.report_io(command_type, io_def)\n\n        if self.label_source:\n            self.label_source.report_io(command_type, io_def)\n\n        if self.label_store:\n            self.label_store.report_io(command_type, io_def)\n\n        if self.aoi_uris:\n            for uri in self.aoi_uris:\n                io_def.add_input(uri)\n\n    @staticmethod\n    def builder():\n        return SceneConfigBuilder()\n\n    @staticmethod\n    def from_proto(msg):\n        """"""Creates a SceneConfig from the specificed protobuf message\n        """"""\n        return SceneConfigBuilder().from_proto(msg).build()\n\n\nclass SceneConfigBuilder(ConfigBuilder):\n    def __init__(self, prev=None):\n        config = {}\n        if prev:\n            config = {\n                \'id\': prev.id,\n                \'raster_source\': prev.raster_source,\n                \'label_source\': prev.label_source,\n                \'label_store\': prev.label_store,\n                \'aoi_uris\': prev.aoi_uris\n            }\n        super().__init__(SceneConfig, config)\n        self.task = None\n\n    def from_proto(self, msg):\n        b = self.with_id(msg.id) \\\n                .with_raster_source(RasterSourceConfig.from_proto(msg.raster_source))\n        if msg.HasField(\'ground_truth_label_source\'):\n            b = b.with_label_source(\n                LabelSourceConfig.from_proto(msg.ground_truth_label_source))\n        if msg.HasField(\'prediction_label_store\'):\n            b = b.with_label_store(\n                LabelStoreConfig.from_proto(msg.prediction_label_store))\n\n        # Here for backward compatibility.\n        if msg.HasField(\'aoi_uri\'):\n            b = b.with_aoi_uri(msg.aoi_uri)\n\n        if len(msg.aoi_uris):\n            b = b.with_aoi_uris(msg.aoi_uris)\n\n        return b\n\n    def with_task(self, task):\n        """"""Sets a specific task type, e.g. rv.OBJECT_DETECTION.""""""\n        b = deepcopy(self)\n        b.task = task\n        return b\n\n    def with_id(self, id):\n        """"""Sets an id for the scene.""""""\n        b = deepcopy(self)\n        b.config[\'id\'] = id\n        return b\n\n    def with_raster_source(self,\n                           raster_source: Union[str, RasterSourceConfig],\n                           channel_order=None):\n        """"""\n        Sets the raster source for this scene.\n\n        Args:\n            raster_source: Can either be a raster source configuration, or\n                a string. If a string, the registry will be queried\n                to grab the default RasterSourceConfig for the string.\n            channel_order: Optional channel order for this raster source.\n        """"""\n        b = deepcopy(self)\n        if isinstance(raster_source, RasterSourceConfig):\n            if channel_order is not None:\n                rs = raster_source.to_builder() \\\n                                  .with_channel_order(channel_order) \\\n                                  .build()\n                b.config[\'raster_source\'] = rs\n            else:\n                b.config[\'raster_source\'] = raster_source\n        else:\n            provider = rv._registry.get_raster_source_default_provider(\n                raster_source)\n            b.config[\'raster_source\'] = provider.construct(\n                raster_source, channel_order)\n\n        return b\n\n    def with_label_source(self, label_source: Union[str, LabelSourceConfig]):\n        """"""\n        Sets the raster source for this scene.\n\n        Args:\n           label_source: Can either be a label source configuration, or\n                         a string. If a string, the registry will be queried\n                         to grab the default LabelSourceConfig for the string.\n\n        Note:\n           A task must be set with `with_task` before calling this, if calling\n           with a string.\n        """"""\n        b = deepcopy(self)\n        if isinstance(label_source, LabelSourceConfig):\n            b.config[\'label_source\'] = label_source\n        else:\n            if not self.task:\n                raise rv.ConfigError(\n                    ""You must set a task with \'.with_task\' before ""\n                    \'creating a default label store for {}\'.format(\n                        label_source))\n            provider = rv._registry.get_label_source_default_provider(\n                self.task.task_type, label_source)\n            b.config[\'label_source\'] = provider.construct(label_source)\n\n        return b\n\n    def clear_label_source(self):\n        """"""Clears the label source for this scene""""""\n        b = deepcopy(self)\n        b.config[\'label_source\'] = None\n        return b\n\n    def with_label_store(\n            self, label_store: Union[str, LabelStoreConfig, None] = None):\n        """"""\n        Sets the raster store for this scene.\n\n        Args:\n           label_store: Can either be a label store configuration, or\n                        a string, or None. If a string, the registry will\n                        be queried to grab the default LabelStoreConfig for\n                        the string. If None, then the default for the task\n                        from the regsitry will be used.\n\n        Note:\n           A task must be set with `with_task` before calling this, if calling\n           with a string.\n        """"""\n        b = deepcopy(self)\n        if isinstance(label_store, LabelStoreConfig):\n            b.config[\'label_store\'] = label_store\n        elif isinstance(label_store, str):\n            if not self.task:\n                raise rv.ConfigError(\n                    ""You must set a task with \'.with_task\' before ""\n                    \'creating a default label store for {}\'.format(\n                        label_store))\n            provider = rv._registry.get_label_store_default_provider(\n                self.task.task_type, label_store)\n            b.config[\'label_store\'] = provider.construct(label_store)\n        else:\n            if not self.task:\n                raise rv.ConfigError(\n                    ""You must set a task with \'.with_task\' before ""\n                    \'creating a default label store.\')\n            provider = rv._registry.get_label_store_default_provider(\n                self.task.task_type)\n            b.config[\'label_store\'] = provider.construct()\n\n        return b\n\n    def clear_label_store(self):\n        """"""Clears the label store for this scene""""""\n        b = deepcopy(self)\n        b.config[\'label_store\'] = None\n        return b\n\n    def with_aoi_uri(self, uri):\n        """"""Sets the Area of Interest for the scene.\n\n            Args:\n                uri: a URI of a GeoJSON file with polygons.\n\n        """"""\n        b = deepcopy(self)\n        b.config[\'aoi_uris\'] = [uri]\n        return b\n\n    def with_aoi_uris(self, uris):\n        """"""Sets the areas of interest for the scene.\n\n            Args:\n                uris: List of URIs each to a GeoJSON file with polygons.\n        """"""\n        b = deepcopy(self)\n        b.config[\'aoi_uris\'] = uris\n        return b\n\n    def clear_aois(self):\n        """"""Clears the AOIs for this scene""""""\n        b = deepcopy(self)\n        b.config[\'aoi_uris\'] = None\n        return b\n'"
rastervision/data/utils.py,0,"b'def boxes_to_geojson(boxes, class_ids, crs_transformer, class_map,\n                     scores=None):\n    """"""Convert boxes and associated data into a GeoJSON dict.\n\n    Args:\n        boxes: list of Box in pixel row/col format.\n        class_ids: list of int (one for each box)\n        crs_transformer: CRSTransformer used to convert pixel coords to map\n            coords in the GeoJSON\n        class_map: ClassMap used to infer class_name from class_id\n        scores: optional list of score or scores.\n                If floats (one for each box), property name will be ""score"".\n                If lists of floats, property name will be ""scores"".\n\n    Returns:\n        dict in GeoJSON format\n    """"""\n    features = []\n    for box_ind, box in enumerate(boxes):\n        polygon = box.geojson_coordinates()\n        polygon = [list(crs_transformer.pixel_to_map(p)) for p in polygon]\n\n        class_id = int(class_ids[box_ind])\n        class_name = class_map.get_by_id(class_id).name\n\n        feature = {\n            \'type\': \'Feature\',\n            \'geometry\': {\n                \'type\': \'Polygon\',\n                \'coordinates\': [polygon]\n            },\n            \'properties\': {\n                \'class_id\': class_id,\n                \'class_name\': class_name\n            }\n        }\n\n        if scores is not None:\n            box_scores = scores[box_ind]\n\n            if box_scores is not None:\n                if type(box_scores) is list:\n                    feature[\'properties\'][\'scores\'] = box_scores\n                else:\n                    feature[\'properties\'][\'score\'] = box_scores\n\n        features.append(feature)\n\n    return {\'type\': \'FeatureCollection\', \'features\': features}\n'"
rastervision/evaluation/__init__.py,0,b'# flake8: noqa\n\nfrom rastervision.evaluation.evaluation_item import *\nfrom rastervision.evaluation.class_evaluation_item import *\nfrom rastervision.evaluation.evaluator import *\nfrom rastervision.evaluation.evaluator_config import *\nfrom rastervision.evaluation.classification_evaluation import *\nfrom rastervision.evaluation.chip_classification_evaluation import *\nfrom rastervision.evaluation.object_detection_evaluation import *\nfrom rastervision.evaluation.semantic_segmentation_evaluation import *\nfrom rastervision.evaluation.classification_evaluator import *\nfrom rastervision.evaluation.classification_evaluator_config import *\nfrom rastervision.evaluation.chip_classification_evaluator import *\nfrom rastervision.evaluation.chip_classification_evaluator_config import *\nfrom rastervision.evaluation.object_detection_evaluator import *\nfrom rastervision.evaluation.object_detection_evaluator_config import *\nfrom rastervision.evaluation.semantic_segmentation_evaluator import *\nfrom rastervision.evaluation.semantic_segmentation_evaluator_config import *\n'
rastervision/evaluation/api.py,0,"b""# flake8: noqa\n\n# Registry keys\n\nEVALUATOR = 'EVALUATOR'\n\nOBJECT_DETECTION_EVALUATOR = 'OBJECT_DETECTION_EVALUATOR'\nCHIP_CLASSIFICATION_EVALUATOR = 'CHIP_CLASSIFICATION_EVALUATOR'\nSEMANTIC_SEGMENTATION_EVALUATOR = 'SEMANTIC_SEGMENTATION_EVALUATOR'\n\nfrom rastervision.evaluation.evaluator_config import EvaluatorConfig\n"""
rastervision/evaluation/chip_classification_evaluation.py,0,"b'import numpy as np\nfrom sklearn import metrics\n\nfrom rastervision.evaluation import ClassificationEvaluation\nfrom rastervision.evaluation import ClassEvaluationItem\n\n\nclass ChipClassificationEvaluation(ClassificationEvaluation):\n    def __init__(self, class_map):\n        super().__init__()\n        self.class_map = class_map\n\n    def compute(self, ground_truth_labels, prediction_labels):\n        self.class_to_eval_item = ChipClassificationEvaluation.compute_eval_items(\n            ground_truth_labels, prediction_labels, self.class_map)\n\n        self.compute_avg()\n\n    @staticmethod\n    def compute_eval_items(gt_labels, pred_labels, class_map):\n        nb_classes = len(class_map)\n        class_to_eval_item = {}\n\n        gt_class_ids = []\n        pred_class_ids = []\n\n        gt_cells = gt_labels.get_cells()\n        for gt_cell in gt_cells:\n            gt_class_id = gt_labels.get_cell_class_id(gt_cell)\n            pred_class_id = pred_labels.get_cell_class_id(gt_cell)\n\n            if gt_class_id is not None and pred_class_id is not None:\n                gt_class_ids.append(gt_class_id)\n                pred_class_ids.append(pred_class_id)\n\n        # Add 1 because class_ids start at 1.\n        sklabels = np.arange(1 + nb_classes)\n        precision, recall, f1, support = metrics.precision_recall_fscore_support(\n            gt_class_ids, pred_class_ids, labels=sklabels, warn_for=())\n\n        for class_map_item in class_map.get_items():\n            class_id = class_map_item.id\n            class_name = class_map_item.name\n\n            eval_item = ClassEvaluationItem(\n                float(precision[class_id]),\n                float(recall[class_id]),\n                float(f1[class_id]),\n                gt_count=float(support[class_id]),\n                class_id=class_id,\n                class_name=class_name)\n            class_to_eval_item[class_id] = eval_item\n\n        return class_to_eval_item\n'"
rastervision/evaluation/chip_classification_evaluator.py,0,"b'from rastervision.evaluation import (ClassificationEvaluator,\n                                     ChipClassificationEvaluation)\n\n\nclass ChipClassificationEvaluator(ClassificationEvaluator):\n    """"""Evaluates predictions for a set of scenes.\n    """"""\n\n    def __init__(self, class_map, output_uri):\n        super().__init__(class_map, output_uri)\n\n    def create_evaluation(self):\n        return ChipClassificationEvaluation(self.class_map)\n'"
rastervision/evaluation/chip_classification_evaluator_config.py,0,"b'import rastervision as rv\nfrom rastervision.evaluation import ChipClassificationEvaluator\nfrom rastervision.evaluation \\\n    import (ClassificationEvaluatorConfig, ClassificationEvaluatorConfigBuilder)\n\n\nclass ChipClassificationEvaluatorConfig(ClassificationEvaluatorConfig):\n    def __init__(self, class_map, output_uri=None):\n        super().__init__(rv.CHIP_CLASSIFICATION_EVALUATOR, class_map,\n                         output_uri)\n\n    def create_evaluator(self):\n        return ChipClassificationEvaluator(self.class_map, self.output_uri)\n\n\nclass ChipClassificationEvaluatorConfigBuilder(\n        ClassificationEvaluatorConfigBuilder):\n    def __init__(self, prev=None):\n        super().__init__(ChipClassificationEvaluatorConfig, prev)\n'"
rastervision/evaluation/class_evaluation_item.py,0,"b'import numpy as np\n\nfrom rastervision.evaluation import EvaluationItem\n\n\nclass ClassEvaluationItem(EvaluationItem):\n    """"""Evaluation metrics for a single class (or average over classes).\n\n    None is used for values that are undefined because they involve a division\n    by zero (eg. precision when there are no predictions).\n    """"""\n\n    def __init__(self,\n                 precision=None,\n                 recall=None,\n                 f1=None,\n                 count_error=None,\n                 gt_count=0,\n                 class_id=None,\n                 class_name=None,\n                 conf_mat=None):\n        self.precision = precision\n        self.recall = recall\n        self.f1 = f1\n        self.count_error = count_error\n        # Ground truth count of elements (boxes for object detection, pixels\n        # for segmentation, cells for classification).\n        self.gt_count = gt_count\n        self.conf_mat = conf_mat\n        self.class_id = class_id\n        self.class_name = class_name\n\n    def merge(self, other):\n        """"""Merges another item from a different scene into this one.\n\n        This is used to average metrics over scenes. Merges by taking a\n        weighted average (by gt_count) of the metrics.\n        """"""\n        if other.gt_count > 0:\n            total_gt_count = self.gt_count + other.gt_count\n            self_ratio = self.gt_count / total_gt_count\n            other_ratio = other.gt_count / total_gt_count\n\n            def weighted_avg(self_val, other_val):\n                if self_val is None and other_val is None:\n                    return 0.0\n                # Handle a single None value by setting them to zero.\n                return (self_ratio * (self_val or 0) +\n                        other_ratio * (other_val or 0))\n\n            self.precision = weighted_avg(self.precision, other.precision)\n            self.recall = weighted_avg(self.recall, other.recall)\n            self.f1 = weighted_avg(self.f1, other.f1)\n            self.count_error = weighted_avg(self.count_error,\n                                            other.count_error)\n            self.gt_count = total_gt_count\n\n        if other.conf_mat is not None:\n            if self.class_name == \'average\':\n                if self.conf_mat is None:\n                    # Make first row all zeros so that the array indices\n                    # correspond to valid class ids (ie. >= 1).\n                    self.conf_mat = np.concatenate(\n                        [\n                            np.zeros_like(other.conf_mat)[np.newaxis, :],\n                            np.array(other.conf_mat)[np.newaxis, :]\n                        ],\n                        axis=0)\n                else:\n                    self.conf_mat = np.concatenate(\n                        [self.conf_mat, other.conf_mat[np.newaxis, :]], axis=0)\n            else:\n                self.conf_mat += other.conf_mat\n\n    def to_json(self):\n        new_dict = {}\n        for k, v in self.__dict__.items():\n            new_dict[k] = v.tolist() if isinstance(v, np.ndarray) else v\n        if new_dict[\'conf_mat\'] is None:\n            del new_dict[\'conf_mat\']\n        return new_dict\n\n    def __repr__(self):\n        return str(self.to_json())\n'"
rastervision/evaluation/classification_evaluation.py,0,"b'from abc import (ABC, abstractmethod)\nimport copy\n\nimport json\n\nfrom rastervision.evaluation import ClassEvaluationItem\nfrom rastervision.utils.files import str_to_file\n\n\nclass ClassificationEvaluation(ABC):\n    """"""Base class for evaluating predictions for tasks that have classes.\n\n    Evaluations can be keyed, for instance, if evaluations happen per class.\n    """"""\n\n    def __init__(self):\n        self.clear()\n        self._is_empty = True\n\n    def clear(self):\n        """"""Clear the Evaluation.""""""\n        self.class_to_eval_item = {}\n        self.scene_to_eval = {}\n        self.avg_item = None\n        self._is_empty = True\n\n    def is_empty(self):\n        return self._is_empty\n\n    def set_class_to_eval_item(self, class_to_eval_item):\n        self.class_to_eval_item = class_to_eval_item\n\n    def get_by_id(self, key):\n        """"""Gets the evaluation for a particular EvaluationItem key""""""\n        return self.class_to_eval_item[key]\n\n    def has_id(self, key):\n        """"""Answers whether or not the EvaluationItem key is represented""""""\n        return key in self.class_to_eval_item\n\n    def to_json(self):\n        json_rep = []\n        for eval_item in self.class_to_eval_item.values():\n            json_rep.append(eval_item.to_json())\n        if self.avg_item:\n            json_rep.append(self.avg_item.to_json())\n\n        if self.scene_to_eval:\n            json_rep = {\'overall\': json_rep}\n            scene_to_eval_json = {}\n            for scene_id, eval in self.scene_to_eval.items():\n                scene_to_eval_json[scene_id] = eval.to_json()\n            json_rep[\'per_scene\'] = scene_to_eval_json\n\n        return json_rep\n\n    def save(self, output_uri):\n        """"""Save this Evaluation to a file.\n\n        Args:\n            output_uri: string URI for the file to write.\n        """"""\n        json_str = json.dumps(self.to_json(), indent=4)\n        str_to_file(json_str, output_uri)\n\n    def merge(self, evaluation, scene_id=None):\n        """"""Merge Evaluation for another Scene into this one.\n\n        This is useful for computing the average metrics of a set of scenes.\n        The results of the averaging are stored in this Evaluation.\n\n        Args:\n            evaluation: Evaluation to merge into this one\n        """"""\n        if len(self.class_to_eval_item) == 0:\n            self.class_to_eval_item = evaluation.class_to_eval_item\n        else:\n            for key, other_eval_item in \\\n                    evaluation.class_to_eval_item.items():\n                if self.has_id(key):\n                    self.get_by_id(key).merge(other_eval_item)\n                else:\n                    self.class_to_eval_item[key] = other_eval_item\n\n        self._is_empty = False\n        self.compute_avg()\n\n        if scene_id is not None:\n            self.scene_to_eval[scene_id] = copy.deepcopy(evaluation)\n\n    def compute_avg(self):\n        """"""Compute average metrics over all keys.""""""\n        self.avg_item = ClassEvaluationItem(class_name=\'average\')\n        for eval_item in self.class_to_eval_item.values():\n            self.avg_item.merge(eval_item)\n\n    @abstractmethod\n    def compute(self, ground_truth_labels, prediction_labels):\n        """"""Compute metrics for a single scene.\n\n        Args:\n            ground_truth_labels: Ground Truth labels to evaluate against.\n            prediction_labels: The predicted labels to evaluate.\n        """"""\n        pass\n'"
rastervision/evaluation/classification_evaluator.py,0,"b'from abc import (abstractmethod)\nimport logging\n\nfrom rastervision.evaluation import Evaluator\nfrom rastervision.data import ActivateMixin\n\nlog = logging.getLogger(__name__)\n\n\nclass ClassificationEvaluator(Evaluator):\n    """"""Evaluates predictions for a set of scenes.\n    """"""\n\n    def __init__(self, class_map, output_uri):\n        self.class_map = class_map\n        self.output_uri = output_uri\n        self.eval = None\n\n    @abstractmethod\n    def create_evaluation(self):\n        pass\n\n    def process(self, scenes, tmp_dir):\n        evaluation = self.create_evaluation()\n\n        for scene in scenes:\n            log.info(\'Computing evaluation for scene {}...\'.format(scene.id))\n            label_source = scene.ground_truth_label_source\n            label_store = scene.prediction_label_store\n            with ActivateMixin.compose(label_source, label_store):\n                ground_truth = label_source.get_labels()\n                predictions = label_store.get_labels()\n\n                if scene.aoi_polygons:\n                    # Filter labels based on AOI.\n                    ground_truth = ground_truth.filter_by_aoi(\n                        scene.aoi_polygons)\n                    predictions = predictions.filter_by_aoi(scene.aoi_polygons)\n                scene_evaluation = self.create_evaluation()\n                scene_evaluation.compute(ground_truth, predictions)\n                evaluation.merge(scene_evaluation, scene_id=scene.id)\n        evaluation.save(self.output_uri)\n        self.eval = evaluation\n'"
rastervision/evaluation/classification_evaluator_config.py,0,"b'import os\nfrom copy import deepcopy\n\nimport rastervision as rv\nfrom rastervision.core import ClassMap\nfrom rastervision.evaluation \\\n    import (EvaluatorConfig, EvaluatorConfigBuilder)\nfrom rastervision.protos.evaluator_pb2 import EvaluatorConfig as EvaluatorConfigMsg\n\n\nclass ClassificationEvaluatorConfig(EvaluatorConfig):\n    """"""Abstract class for usage with simple evaluators that\n    are classification-based.\n    """"""\n\n    def __init__(self,\n                 evaluator_type,\n                 class_map,\n                 output_uri=None,\n                 vector_output_uri=None):\n        super().__init__(evaluator_type)\n        self.class_map = class_map\n        self.output_uri = output_uri\n        self.vector_output_uri = vector_output_uri\n\n    def to_proto(self):\n        sub_msg = EvaluatorConfigMsg.ClassificationEvaluatorConfig(\n            class_items=self.class_map.to_proto(),\n            output_uri=self.output_uri,\n            vector_output_uri=self.vector_output_uri)\n        msg = EvaluatorConfigMsg(\n            evaluator_type=self.evaluator_type, classification_config=sub_msg)\n\n        return msg\n\n    def update_for_command(self, command_type, experiment_config,\n                           context=None):\n        if command_type == rv.EVAL:\n            if not self.output_uri:\n                self.output_uri = os.path.join(experiment_config.eval_uri,\n                                               \'eval.json\')\n            if not self.vector_output_uri:\n                self.vector_output_uri = os.path.join(\n                    experiment_config.eval_uri, \'vector-eval.json\')\n\n    def report_io(self, command_type, io_def):\n        if command_type == rv.EVAL:\n            io_def.add_output(self.output_uri)\n            io_def.add_output(self.vector_output_uri)\n\n        return io_def\n\n\nclass ClassificationEvaluatorConfigBuilder(EvaluatorConfigBuilder):\n    def __init__(self, cls, prev=None):\n        self.config = {}\n        if prev:\n            self.config = {\n                \'output_uri\': prev.output_uri,\n                \'vector_output_uri\': prev.vector_output_uri,\n                \'class_map\': prev.class_map\n            }\n        super().__init__(cls, self.config)\n\n    def validate(self):\n        if self.config.get(\'class_map\') is None:\n            raise rv.ConfigError(\n                \'class_map not set for ClassificationEvaluatorConfig. \'\n                \'Use ""with_class_map"".\')\n        if not isinstance(self.config.get(\'class_map\'), ClassMap):\n            raise rv.ConfigError(\n                \'class_map set with ""with_class_map"" must be of type ClassMap, got {}\'.\n                format(type(self.config.get(\'class_map\'))))\n\n    @classmethod\n    def from_proto(cls, msg):\n        b = cls()\n        return b.with_output_uri(msg.classification_config.output_uri) \\\n                .with_vector_output_uri(msg.classification_config.vector_output_uri) \\\n                .with_class_map(list(msg.classification_config.class_items))\n\n    def with_output_uri(self, output_uri):\n        """"""Set the output_uri.\n\n            Args:\n                output_uri: URI to the stats json to use\n        """"""\n        b = deepcopy(self)\n        b.config[\'output_uri\'] = output_uri\n        return b\n\n    def with_vector_output_uri(self, vector_output_uri):\n        """"""Set the vector_output_uri.\n\n            Args:\n                vector_output_uri: URI to the vector stats json to use\n        """"""\n        b = deepcopy(self)\n        b.config[\'vector_output_uri\'] = vector_output_uri\n        return b\n\n    def with_task(self, task):\n        """"""Sets a specific task type, e.g. rv.OBJECT_DETECTION.""""""\n        if not hasattr(task, \'class_map\'):\n            raise rv.ConfigError(\'This evaluator requires a task \'\n                                 \'that has a class_map property\')\n        return self.with_class_map(task.class_map)\n\n    def with_class_map(self, class_map):\n        """"""Set the class map to be used for evaluation.\n\n            Args:\n                class_map: The class map to be used\n\n        """"""\n        b = deepcopy(self)\n        b.config[\'class_map\'] = ClassMap.construct_from(class_map)\n        return b\n'"
rastervision/evaluation/default.py,0,"b'from abc import (ABC, abstractmethod)\n\nimport rastervision as rv\n\n\nclass EvaluatorDefaultProvider(ABC):\n    @staticmethod\n    @abstractmethod\n    def is_default_for(task_type):\n        """"""Returns True if this evaluator is the default for this tasks_type""""""\n        pass\n\n    @abstractmethod\n    def construct(task):\n        """"""Constructs the default evaluator.\n        """"""\n        pass\n\n\nclass ObjectDetectionEvaluatorDefaultProvider(EvaluatorDefaultProvider):\n    @staticmethod\n    def is_default_for(task_type):\n        return task_type == rv.OBJECT_DETECTION\n\n    @staticmethod\n    def construct(task):\n        return rv.EvaluatorConfig.builder(rv.OBJECT_DETECTION_EVALUATOR) \\\n                                 .with_task(task) \\\n                                 .build()\n\n\nclass ChipClassificationEvaluatorDefaultProvider(EvaluatorDefaultProvider):\n    @staticmethod\n    def is_default_for(task_type):\n        return task_type == rv.CHIP_CLASSIFICATION\n\n    @staticmethod\n    def construct(task):\n        return rv.EvaluatorConfig.builder(rv.CHIP_CLASSIFICATION_EVALUATOR) \\\n                                 .with_task(task) \\\n                                 .build()\n\n\nclass SemanticSegmentationEvaluatorDefaultProvider(EvaluatorDefaultProvider):\n    @staticmethod\n    def is_default_for(task_type):\n        return task_type == rv.SEMANTIC_SEGMENTATION\n\n    @staticmethod\n    def construct(task):\n        return rv.EvaluatorConfig.builder(rv.SEMANTIC_SEGMENTATION_EVALUATOR) \\\n                                 .with_task(task) \\\n                                 .build()\n'"
rastervision/evaluation/evaluation_item.py,0,"b'from abc import (ABC, abstractmethod)\n\n\nclass EvaluationItem(ABC):\n    @abstractmethod\n    def merge(self, other):\n        """"""Merges another item from a different scene into this one.\n\n        This is used to average metrics over scenes. Merges by taking a\n        weighted average (by gt_count) of the metrics.\n        """"""\n        pass\n\n    @abstractmethod\n    def to_json(self):\n        return self.__dict__\n\n    def __repr__(self):\n        return str(self.to_json())\n'"
rastervision/evaluation/evaluator.py,0,"b'from abc import (ABC, abstractmethod)\n\n\nclass Evaluator(ABC):\n    """"""Evaluates predictions for a set of scenes.\n    """"""\n\n    @abstractmethod\n    def process(self, scenes, tmp_dir):\n        pass\n'"
rastervision/evaluation/evaluator_config.py,0,"b'from abc import abstractmethod\n\nimport rastervision as rv\nfrom rastervision.core import (Config, ConfigBuilder)\n\n\nclass EvaluatorConfig(Config):\n    def __init__(self, evaluator_type):\n        self.evaluator_type = evaluator_type\n\n    @abstractmethod\n    def create_evaluator(self):\n        """"""Create the Evaluator that this configuration represents""""""\n        pass\n\n    def to_builder(self):\n        return rv._registry.get_config_builder(rv.EVALUATOR,\n                                               self.evaluator_type)(self)\n\n    @staticmethod\n    def builder(evaluator_type):\n        return rv._registry.get_config_builder(rv.EVALUATOR, evaluator_type)()\n\n    @staticmethod\n    def from_proto(msg):\n        """"""Creates a EvaluatorConfig from the specificed protobuf message\n        """"""\n        return rv._registry.get_config_builder(rv.EVALUATOR, msg.evaluator_type)() \\\n                           .from_proto(msg) \\\n                           .build()\n\n\nclass EvaluatorConfigBuilder(ConfigBuilder):\n    pass\n'"
rastervision/evaluation/object_detection_evaluation.py,0,"b""import numpy as np\nimport shapely\nimport shapely.strtree\nimport shapely.geometry\n\nfrom rastervision.data import ObjectDetectionLabels\nfrom rastervision.evaluation import ClassEvaluationItem\nfrom rastervision.evaluation import ClassificationEvaluation\n\n\ndef compute_metrics(gt_labels: ObjectDetectionLabels,\n                    pred_labels: ObjectDetectionLabels,\n                    num_classes: int,\n                    iou_thresh=0.5):\n    gt_geoms = [b.to_shapely() for b in gt_labels.get_boxes()]\n    gt_classes = gt_labels.get_class_ids() - 1\n    pred_geoms = [b.to_shapely() for b in pred_labels.get_boxes()]\n    pred_classes = pred_labels.get_class_ids() - 1\n\n    for pred, class_id in zip(pred_geoms, pred_classes):\n        pred.class_id = class_id\n    pred_tree = shapely.strtree.STRtree(pred_geoms)\n\n    def iou(a, b):\n        return a.intersection(b).area / a.union(b).area\n\n    def is_matched(geom):\n        return hasattr(geom, 'iou_matched')\n\n    tp = np.zeros((num_classes, ))\n    fp = np.zeros((num_classes, ))\n    fn = np.zeros((num_classes, ))\n\n    for gt, gt_class in zip(gt_geoms, gt_classes):\n        matches = list(\n            filter(lambda g: (not is_matched(g)) and g.class_id == gt_class,\n                   pred_tree.query(gt)))\n        scores = [iou(m, gt) for m in matches]\n        if len(scores) > 0:\n            max_ind = np.argmax(scores)\n            if scores[max_ind] > iou_thresh:\n                matches[max_ind].iou_matched = True\n                tp[gt_class] += 1\n            else:\n                fn[gt_class] += 1\n        else:\n            fn[gt_class] += 1\n\n    for class_id in range(num_classes):\n        pred_not_matched = np.array([not is_matched(g) for g in pred_geoms])\n        fp[class_id] = np.sum(pred_not_matched[pred_classes == class_id])\n\n    return tp, fp, fn\n\n\nclass ObjectDetectionEvaluation(ClassificationEvaluation):\n    def __init__(self, class_map):\n        super().__init__()\n        self.class_map = class_map\n\n    def compute(self, ground_truth_labels, prediction_labels):\n        self.class_to_eval_item = ObjectDetectionEvaluation.compute_eval_items(\n            ground_truth_labels, prediction_labels, self.class_map)\n        self.compute_avg()\n\n    @staticmethod\n    def compute_eval_items(gt_labels, pred_labels, class_map):\n        iou_thresh = 0.5\n        num_classes = len(class_map)\n        tps, fps, fns = compute_metrics(gt_labels, pred_labels, num_classes,\n                                        iou_thresh)\n        class_to_eval_item = {}\n\n        for class_ind, (tp, fp, fn) in enumerate(zip(tps, fps, fns)):\n            class_id = class_ind + 1\n            gt_count = tp + fn\n            pred_count = tp + fp\n            class_name = class_map.get_by_id(class_id).name\n\n            if gt_count == 0:\n                eval_item = ClassEvaluationItem(\n                    class_id=class_id, class_name=class_name)\n            elif pred_count == 0:\n                eval_item = ClassEvaluationItem(\n                    precision=None,\n                    recall=0,\n                    gt_count=gt_count,\n                    class_id=class_id,\n                    class_name=class_name)\n            else:\n                prec = tp / (tp + fp)\n                recall = tp / (tp + fn)\n                f1 = 0.\n                if prec + recall != 0.0:\n                    f1 = 2 * (prec * recall) / (prec + recall)\n                count_err = pred_count - gt_count\n                norm_count_err = None\n                if gt_count > 0:\n                    norm_count_err = count_err / gt_count\n\n                eval_item = ClassEvaluationItem(\n                    precision=prec,\n                    recall=recall,\n                    f1=f1,\n                    count_error=norm_count_err,\n                    gt_count=gt_count,\n                    class_id=class_id,\n                    class_name=class_name)\n\n            class_to_eval_item[class_id] = eval_item\n\n        return class_to_eval_item\n"""
rastervision/evaluation/object_detection_evaluator.py,0,"b'from rastervision.evaluation import (ClassificationEvaluator,\n                                     ObjectDetectionEvaluation)\n\n\nclass ObjectDetectionEvaluator(ClassificationEvaluator):\n    """"""Evaluates predictions for a set of scenes.\n    """"""\n\n    def __init__(self, class_map, output_uri):\n        super().__init__(class_map, output_uri)\n\n    def create_evaluation(self):\n        return ObjectDetectionEvaluation(self.class_map)\n'"
rastervision/evaluation/object_detection_evaluator_config.py,0,"b'import rastervision as rv\nfrom rastervision.evaluation import ObjectDetectionEvaluator\nfrom rastervision.evaluation \\\n    import (ClassificationEvaluatorConfig, ClassificationEvaluatorConfigBuilder)\n\n\nclass ObjectDetectionEvaluatorConfig(ClassificationEvaluatorConfig):\n    def __init__(self, class_map, output_uri=None):\n        super().__init__(rv.OBJECT_DETECTION_EVALUATOR, class_map, output_uri)\n\n    def create_evaluator(self):\n        return ObjectDetectionEvaluator(self.class_map, self.output_uri)\n\n\nclass ObjectDetectionEvaluatorConfigBuilder(\n        ClassificationEvaluatorConfigBuilder):\n    def __init__(self, prev=None):\n        super().__init__(ObjectDetectionEvaluatorConfig, prev)\n'"
rastervision/evaluation/semantic_segmentation_evaluation.py,0,"b'import math\nimport logging\nimport json\n\nfrom sklearn.metrics import confusion_matrix\nimport numpy as np\n\nfrom rastervision.evaluation import ClassEvaluationItem\nfrom rastervision.evaluation import ClassificationEvaluation\n\nlog = logging.getLogger(__name__)\n\n\ndef is_geojson(data):\n    if isinstance(data, dict):\n        return True\n    else:\n        try:\n            json.loads(data)\n            retval = True\n        except ValueError:\n            retval = False\n        return retval\n\n\ndef get_class_eval_item(conf_mat, class_id, class_map):\n    class_name = class_map.get_by_id(class_id).name\n\n    if conf_mat.ravel().sum() == 0:\n        return ClassEvaluationItem(None, None, None, 0, 0, class_id,\n                                   class_name)\n\n    true_pos = conf_mat[class_id, class_id]\n    false_pos = conf_mat[1:, class_id].sum() - true_pos\n    false_neg = conf_mat[class_id, :].sum() - true_pos\n    precision = float(true_pos) / (true_pos + false_pos)\n    recall = float(true_pos) / (true_pos + false_neg)\n    f1 = 2 * (precision * recall) / (precision + recall)\n    count_error = int(false_pos + false_neg)\n    gt_count = conf_mat[class_id, :].sum()\n\n    if math.isnan(precision):\n        precision = None\n    else:\n        precision = float(precision)\n    if math.isnan(recall):\n        recall = None\n    else:\n        recall = float(recall)\n    if math.isnan(f1):\n        f1 = None\n    else:\n        f1 = float(f1)\n\n    return ClassEvaluationItem(precision, recall, f1, count_error, gt_count,\n                               class_id, class_name, conf_mat[class_id, :])\n\n\nclass SemanticSegmentationEvaluation(ClassificationEvaluation):\n    """"""Evaluation for semantic segmentation.""""""\n\n    def __init__(self, class_map):\n        super().__init__()\n        self.class_map = class_map\n\n    def compute(self, gt_labels, pred_labels):\n        self.clear()\n\n        # Ensure there is a row and column for each class_id even when\n        # class_ids are non-consecutive.\n        labels = np.arange(max(self.class_map.get_keys()) + 1)\n        conf_mat = np.zeros((len(labels), len(labels)))\n        for window in pred_labels.get_windows():\n            log.debug(\'Evaluating window: {}\'.format(window))\n            gt_arr = gt_labels.get_label_arr(window).ravel()\n            pred_arr = pred_labels.get_label_arr(window).ravel()\n            conf_mat += confusion_matrix(gt_arr, pred_arr, labels=labels)\n\n        for class_id in self.class_map.get_keys():\n            if class_id > 0:\n                self.class_to_eval_item[class_id] = get_class_eval_item(\n                    conf_mat, class_id, self.class_map)\n        self.compute_avg()\n\n    def compute_vector(self, gt, pred, mode, class_id):\n        """"""Compute evaluation over vector predictions.\n            Args:\n                gt: Ground-truth GeoJSON.  Either a string (containing\n                    unparsed GeoJSON or a file name), or a dictionary\n                    containing parsed GeoJSON.\n                pred: GeoJSON for predictions.  Either a string\n                    (containing unparsed GeoJSON or a file name), or a\n                    dictionary containing parsed GeoJSON.\n                mode: A string containing either \'buildings\' or\n                    \'polygons\'.\n                class_id: An integer containing the class id of\n                    interest.\n        """"""\n        import mask_to_polygons.vectorification as vectorification\n        import mask_to_polygons.processing.score as score\n\n        # Ground truth as list of geometries\n        def get_geoms(x):\n            if is_geojson(x):\n                _x = x\n                if \'features\' in _x.keys():\n                    _x = _x[\'features\']\n                geoms = []\n                for feature in _x:\n                    if \'geometry\' in feature.keys():\n                        geoms.append(feature[\'geometry\'])\n                    else:\n                        geoms.append(feature)\n            else:\n                geoms = vectorification.geometries_from_geojson(x)\n\n            return geoms\n\n        gt = get_geoms(gt)\n        pred = get_geoms(pred)\n\n        if len(gt) > 0 and len(pred) > 0:\n            results = score.spacenet(pred, gt)\n\n            true_positives = results[\'tp\']\n            false_positives = results[\'fp\']\n            false_negatives = results[\'fn\']\n            precision = float(true_positives) / (\n                true_positives + false_positives)\n            recall = float(true_positives) / (true_positives + false_negatives)\n            if precision + recall != 0:\n                f1 = 2 * (precision * recall) / (precision + recall)\n            else:\n                f1 = 0.0\n            count_error = int(false_positives + false_negatives)\n            gt_count = len(gt)\n            class_name = \'vector-{}-{}\'.format(\n                mode,\n                self.class_map.get_by_id(class_id).name)\n\n            evaluation_item = ClassEvaluationItem(precision, recall, f1,\n                                                  count_error, gt_count,\n                                                  class_id, class_name)\n\n            if hasattr(self, \'class_to_eval_item\') and isinstance(\n                    self.class_to_eval_item, dict):\n                self.class_to_eval_item[class_id] = evaluation_item\n            else:\n                self.class_to_eval_item = {class_id: evaluation_item}\n            self.compute_avg()\n'"
rastervision/evaluation/semantic_segmentation_evaluator.py,0,"b'import logging\n\nfrom shapely.geometry import shape, mapping\nfrom shapely.strtree import STRtree\n\nfrom rastervision.data import ActivateMixin, GeoJSONVectorSource\nfrom rastervision.evaluation import (ClassificationEvaluator,\n                                     SemanticSegmentationEvaluation)\n\nlog = logging.getLogger(__name__)\n\n\ndef filter_geojson_by_aoi(geojson, aoi_polygons):\n    # Note that this ignores class_id but that\'s ok because each prediction GeoJSON file\n    # covers a single class_id. But, this may change in the future.\n    tree = STRtree([shape(f[\'geometry\']) for f in geojson[\'features\']])\n    filtered_shapes = []\n    for aoi_poly in aoi_polygons:\n        shapes_in_aoi = tree.query(aoi_poly)\n        for s in shapes_in_aoi:\n            s_int = s.intersection(aoi_poly)\n            filtered_shapes.append(s_int)\n\n    features = [{\n        \'type\': \'feature\',\n        \'geometry\': mapping(s)\n    } for s in filtered_shapes]\n\n    return {\'type\': \'FeatureCollection\', \'features\': features}\n\n\nclass SemanticSegmentationEvaluator(ClassificationEvaluator):\n    """"""Evaluates predictions for a set of scenes.\n    """"""\n\n    def __init__(self, class_map, output_uri, vector_output_uri):\n        super().__init__(class_map, output_uri)\n        self.vector_output_uri = vector_output_uri\n\n    def create_evaluation(self):\n        return SemanticSegmentationEvaluation(self.class_map)\n\n    def process(self, scenes, tmp_dir):\n        evaluation = self.create_evaluation()\n        vect_evaluation = self.create_evaluation()\n\n        for scene in scenes:\n            log.info(\'Computing evaluation for scene {}...\'.format(scene.id))\n            label_source = scene.ground_truth_label_source\n            label_store = scene.prediction_label_store\n            with ActivateMixin.compose(label_source, label_store):\n                ground_truth = label_source.get_labels()\n                predictions = label_store.get_labels()\n\n                if scene.aoi_polygons:\n                    # Filter labels based on AOI.\n                    ground_truth = ground_truth.filter_by_aoi(\n                        scene.aoi_polygons)\n                    predictions = predictions.filter_by_aoi(scene.aoi_polygons)\n                scene_evaluation = self.create_evaluation()\n                scene_evaluation.compute(ground_truth, predictions)\n                evaluation.merge(scene_evaluation, scene_id=scene.id)\n\n            if hasattr(label_source, \'source\') and hasattr(\n                    label_source.source, \'vector_source\') and hasattr(\n                        label_store, \'vector_output\'):\n                gt_geojson = label_source.source.vector_source.get_geojson()\n                for vo in label_store.vector_output:\n                    pred_geojson_uri = vo[\'uri\']\n                    mode = vo[\'mode\']\n                    class_id = vo[\'class_id\']\n                    pred_geojson_source = GeoJSONVectorSource(\n                        pred_geojson_uri,\n                        scene.raster_source.get_crs_transformer())\n                    pred_geojson = pred_geojson_source.get_geojson()\n\n                    if scene.aoi_polygons:\n                        gt_geojson = filter_geojson_by_aoi(\n                            gt_geojson, scene.aoi_polygons)\n                        pred_geojson = filter_geojson_by_aoi(\n                            pred_geojson, scene.aoi_polygons)\n\n                    vect_scene_evaluation = self.create_evaluation()\n                    vect_scene_evaluation.compute_vector(\n                        gt_geojson, pred_geojson, mode, class_id)\n                    vect_evaluation.merge(\n                        vect_scene_evaluation, scene_id=scene.id)\n\n        if not evaluation.is_empty():\n            evaluation.save(self.output_uri)\n        if not vect_evaluation.is_empty():\n            vect_evaluation.save(self.vector_output_uri)\n'"
rastervision/evaluation/semantic_segmentation_evaluator_config.py,0,"b'import rastervision as rv\nfrom rastervision.evaluation import SemanticSegmentationEvaluator\nfrom rastervision.evaluation \\\n    import (ClassificationEvaluatorConfig, ClassificationEvaluatorConfigBuilder)\n\n\nclass SemanticSegmentationEvaluatorConfig(ClassificationEvaluatorConfig):\n    def __init__(self, class_map, output_uri=None, vector_output_uri=None):\n        super().__init__(rv.SEMANTIC_SEGMENTATION_EVALUATOR, class_map,\n                         output_uri, vector_output_uri)\n\n    def create_evaluator(self):\n        return SemanticSegmentationEvaluator(self.class_map, self.output_uri,\n                                             self.vector_output_uri)\n\n\nclass SemanticSegmentationEvaluatorConfigBuilder(\n        ClassificationEvaluatorConfigBuilder):\n    def __init__(self, prev=None):\n        super().__init__(SemanticSegmentationEvaluatorConfig, prev)\n'"
rastervision/experiment/__init__.py,0,b'# flake8: noqa\n\nfrom rastervision.experiment.experiment_config import *\nfrom rastervision.experiment.experiment_set import *\nfrom rastervision.experiment.experiment_loader import *\n'
rastervision/experiment/api.py,0,b'# flake8: noqa\n\nfrom rastervision.experiment.experiment_config import ExperimentConfig\nfrom rastervision.experiment.experiment_set import ExperimentSet\n'
rastervision/experiment/experiment_config.py,0,"b'import os\nfrom copy import deepcopy\nimport logging\nimport json\n\nfrom google.protobuf import (json_format, struct_pb2)\n\nimport rastervision as rv\nfrom rastervision.core.config import (Config, ConfigBuilder)\nfrom rastervision.utils.files import save_json_config\nfrom rastervision.protos.experiment_pb2 \\\n    import ExperimentConfig as ExperimentConfigMsg\nfrom rastervision.backend import BackendConfig\nfrom rastervision.task import TaskConfig\nfrom rastervision.data import DatasetConfig\n\nlog = logging.getLogger(__name__)\n\n\nclass ExperimentConfig(Config):\n    def __init__(self,\n                 id,\n                 task,\n                 backend,\n                 dataset,\n                 root_uri,\n                 analyze_uri,\n                 chip_uri,\n                 train_uri,\n                 predict_uri,\n                 eval_uri,\n                 bundle_uri,\n                 evaluators=None,\n                 analyzers=None,\n                 custom_config=None):\n        if analyzers is None:\n            analyzers = []\n\n        self.id = id\n        self.task = task\n        self.backend = backend\n        self.dataset = dataset\n        self.analyzers = analyzers\n        self.evaluators = evaluators\n        self.root_uri = root_uri\n        self.analyze_uri = analyze_uri\n        self.chip_uri = chip_uri\n        self.train_uri = train_uri\n        self.predict_uri = predict_uri\n        self.eval_uri = eval_uri\n        self.bundle_uri = bundle_uri\n        self.custom_config = custom_config or {}\n\n    def update_for_command(self, command_type, experiment_config,\n                           context=None):\n        log.debug(\'Updating task for command {}\'.format(command_type))\n        self.task.update_for_command(command_type, experiment_config, context)\n\n        log.debug(\'Updating backend for command {}\'.format(command_type))\n        self.backend.update_for_command(command_type, experiment_config,\n                                        context)\n\n        log.debug(\'Updating dataset for command {}\'.format(command_type))\n        self.dataset.update_for_command(command_type, experiment_config,\n                                        context)\n\n        log.debug(\'Updating analyzers for command {}\'.format(command_type))\n        for analyzer in self.analyzers:\n            analyzer.update_for_command(command_type, experiment_config,\n                                        context)\n\n        log.debug(\'Updating evaluators for command {}\'.format(command_type))\n        for evaluator in self.evaluators:\n            evaluator.update_for_command(command_type, experiment_config,\n                                         context)\n\n    def report_io(self, command_type, io_def):\n        log.debug(\'Reporting IO on task for command {}\'.format(command_type))\n        self.task.report_io(command_type, io_def)\n\n        log.debug(\n            \'Reporting IO on backend for command {}\'.format(command_type))\n        self.backend.report_io(command_type, io_def)\n\n        log.debug(\n            \'Reporting IO on dataset for command {}\'.format(command_type))\n        self.dataset.report_io(command_type, io_def)\n\n        log.debug(\n            \'Reporting IO on analyzers for command {}\'.format(command_type))\n        for analyzer in self.analyzers:\n            analyzer.report_io(command_type, io_def)\n\n        log.debug(\n            \'Reporting IO on evaluators for command {}\'.format(command_type))\n        for evaluator in self.evaluators:\n            evaluator.report_io_command(command_type, io_def)\n\n    def make_command_config(self, command_type):\n        return rv._registry.get_command_config_builder(command_type)() \\\n                           .with_experiment(self) \\\n                           .build()\n\n    def fully_resolve(self):\n        """"""Returns a fully resolved copy of this  experiment.\n\n        A fully resolved experiment has all implicit paths put into place,\n        and is constructed by calling update_for_command for each command.\n        """"""\n        e = deepcopy(self)\n        for command_type in rv.all_commands():\n            e.update_for_command(command_type, e)\n        return e\n\n    def save_config(self):\n        msg = self.to_proto()\n        uri = os.path.join(self.root_uri, \'experiments\',\n                           \'{}.json\'.format(self.id))\n        save_json_config(msg, uri)\n\n    def to_proto(self):\n        analyzers = list(map(lambda a: a.to_proto(), self.analyzers))\n        evaluators = list(map(lambda e: e.to_proto(), self.evaluators))\n\n        msg = ExperimentConfigMsg(\n            id=self.id,\n            task=self.task.to_proto(),\n            backend=self.backend.to_proto(),\n            dataset=self.dataset.to_proto(),\n            analyzers=analyzers,\n            evaluators=evaluators)\n        msg.root_uri = self.root_uri\n        msg.analyze_uri = self.analyze_uri\n        msg.chip_uri = self.chip_uri\n        msg.train_uri = self.train_uri\n        msg.predict_uri = self.predict_uri\n        msg.eval_uri = self.eval_uri\n        msg.bundle_uri = self.bundle_uri\n\n        if self.custom_config:\n            msg.MergeFrom(\n                ExperimentConfigMsg(\n                    custom_config=json_format.ParseDict(\n                        {\n                            \'config\': json.dumps(self.custom_config)\n                        }, struct_pb2.Struct())))\n\n        return msg\n\n    def to_builder(self):\n        return ExperimentConfigBuilder(self)\n\n    @staticmethod\n    def builder():\n        return ExperimentConfigBuilder()\n\n    @staticmethod\n    def from_proto(msg):\n        """"""Creates an ExperimentConfig from the specificed protobuf message\n        """"""\n        return ExperimentConfigBuilder().from_proto(msg).build()\n\n\nclass ExperimentConfigBuilder(ConfigBuilder):\n    def __init__(self, prev=None):\n        config = {}\n        if prev:\n            config = {\n                \'id\': prev.id,\n                \'task\': prev.task,\n                \'backend\': prev.backend,\n                \'dataset\': prev.dataset,\n                \'analyzers\': prev.analyzers,\n                \'evaluators\': prev.evaluators,\n                \'root_uri\': prev.root_uri,\n                \'analyze_uri\': prev.analyze_uri,\n                \'chip_uri\': prev.chip_uri,\n                \'train_uri\': prev.train_uri,\n                \'predict_uri\': prev.predict_uri,\n                \'eval_uri\': prev.eval_uri,\n                \'bundle_uri\': prev.bundle_uri,\n                \'custom_config\': prev.custom_config\n            }\n        super().__init__(ExperimentConfig, config)\n        self.analyze_key = None\n        self.chip_key = None\n        self.train_key = None\n        self.predict_key = None\n        self.eval_key = None\n        self.bundle_key = None\n        self.custom_config = None\n\n    def validate(self):\n\n        root_uri = self.config.get(\'root_uri\')\n        if not root_uri:\n            raise rv.ConfigError(\'root_uri must be set. Use ""with_root_uri""\')\n        if not isinstance(root_uri, str):\n            raise rv.ConfigError(\n                \'root_uri needs to be of type str, got {}\'.format(\n                    type(root_uri)))\n\n        for key in [\'task\', \'backend\', \'dataset\', \'id\']:\n            if self.config.get(key) is None:\n                raise rv.ConfigError(\n                    \'Experiment %s must be set. Use ""with_%s"".\' % (key, key))\n\n        task = self.config.get(\'task\')\n        if not issubclass(type(task), TaskConfig):\n            raise rv.ConfigError(\n                \'Experiment task set with ""with_task"" must be of class\'\n                \' TaskConfig, got {}\'.format(type(task)))\n\n        backend = self.config.get(\'backend\')\n        if not issubclass(type(backend), BackendConfig):\n            raise rv.ConfigError(\n                \'Backend set with ""with_backend"" needs to be of type \'\n                \'BackendConfig, got {}\'.format(type(backend)))\n\n        dataset = self.config.get(\'dataset\')\n        if not isinstance(dataset, DatasetConfig):\n            raise rv.ConfigError(\n                \'Dataset set with ""with_dataset"" needs to be of type\'\n                \'DatasetConfig, got {}\'.format(type(dataset)))\n\n        if not isinstance(self.config.get(\'id\'), str):\n            raise rv.ConfigError(\n                \'ID set with ""with_id"" needs to be of type str, got {}\'.format(\n                    type(self.config.get(\'id\'))))\n\n    def build(self):\n        self.validate()\n        # Build any missing paths through\n        b = self\n\n        if not self.config.get(\'analyze_uri\'):\n            if not self.analyze_key:\n                self.analyze_key = self.config[\'id\']\n            uri = os.path.join(self.config[\'root_uri\'], rv.ANALYZE.lower(),\n                               self.analyze_key)\n            b = b.with_analyze_uri(uri)\n        if not self.config.get(\'chip_uri\'):\n            if not self.chip_key:\n                self.chip_key = self.config[\'id\']\n            uri = os.path.join(self.config[\'root_uri\'], rv.CHIP.lower(),\n                               self.chip_key)\n            b = b.with_chip_uri(uri)\n        if not self.config.get(\'train_uri\'):\n            if not self.train_key:\n                self.train_key = self.config[\'id\']\n            uri = os.path.join(self.config[\'root_uri\'], rv.TRAIN.lower(),\n                               self.train_key)\n            b = b.with_train_uri(uri)\n        if not self.config.get(\'predict_uri\'):\n            if not self.predict_key:\n                self.predict_key = self.config[\'id\']\n            uri = os.path.join(self.config[\'root_uri\'], rv.PREDICT.lower(),\n                               self.predict_key)\n            b = b.with_predict_uri(uri)\n        if not self.config.get(\'eval_uri\'):\n            if not self.eval_key:\n                self.eval_key = self.config[\'id\']\n            uri = os.path.join(self.config[\'root_uri\'], rv.EVAL.lower(),\n                               self.eval_key)\n            b = b.with_eval_uri(uri)\n        if not self.config.get(\'bundle_uri\'):\n            if not self.bundle_key:\n                self.bundle_key = self.config[\'id\']\n            uri = os.path.join(self.config[\'root_uri\'], rv.BUNDLE.lower(),\n                               self.bundle_key)\n            b = b.with_bundle_uri(uri)\n\n        evaluators = self.config.get(\'evaluators\')\n        if not evaluators:\n            task_type = self.config[\'task\'].task_type\n            e = rv._registry.get_evaluator_default_provider(task_type) \\\n                            .construct(self.config[\'task\'])\n            b = b.with_evaluator(e)\n\n        return ExperimentConfig(**b.config)\n\n    def from_proto(self, msg):\n        analyzers = list(\n            map(lambda a: rv.AnalyzerConfig.from_proto(a), msg.analyzers))\n        evaluators = list(\n            map(lambda e: rv.EvaluatorConfig.from_proto(e), msg.evaluators))\n        b = self.with_id(msg.id) \\\n                .with_task(rv.TaskConfig.from_proto(msg.task)) \\\n                .with_backend(rv.BackendConfig.from_proto(msg.backend)) \\\n                .with_dataset(rv.DatasetConfig.from_proto(msg.dataset)) \\\n                .with_analyzers(analyzers) \\\n                .with_evaluators(evaluators) \\\n                .with_root_uri(msg.root_uri) \\\n                .with_analyze_uri(msg.analyze_uri) \\\n                .with_chip_uri(msg.chip_uri) \\\n                .with_train_uri(msg.train_uri) \\\n                .with_predict_uri(msg.predict_uri) \\\n                .with_eval_uri(msg.eval_uri) \\\n                .with_bundle_uri(msg.bundle_uri)\n\n        if msg.custom_config:\n            b = b.with_custom_config(json.loads(msg.custom_config[\'config\']))\n\n        return b\n\n    def _copy(self):\n        """"""Create a copy; avoid using deepcopy on the dataset\n        as it can have performance implicitions.\n        """"""\n        e = ExperimentConfigBuilder()\n        e.config[\'id\'] = self.config.get(\'id\')\n        e.config[\'task\'] = deepcopy(self.config.get(\'task\'))\n        e.config[\'backend\'] = deepcopy(self.config.get(\'backend\'))\n        e.config[\'dataset\'] = self.config.get(\'dataset\')\n        e.config[\'analyzers\'] = self.config.get(\'analyzers\')\n        e.config[\'evaluators\'] = self.config.get(\'evaluators\')\n        e.config[\'root_uri\'] = self.config.get(\'root_uri\')\n        e.config[\'analyze_uri\'] = self.config.get(\'analyze_uri\')\n        e.config[\'chip_uri\'] = self.config.get(\'chip_uri\')\n        e.config[\'train_uri\'] = self.config.get(\'train_uri\')\n        e.config[\'predict_uri\'] = self.config.get(\'predict_uri\')\n        e.config[\'eval_uri\'] = self.config.get(\'eval_uri\')\n        e.config[\'bundle_uri\'] = self.config.get(\'bundle_uri\')\n        e.config[\'custom_config\'] = self.config.get(\'custom_config\')\n        e.analyze_key = self.analyze_key\n        e.chip_key = self.chip_key\n        e.train_key = self.train_key\n        e.predict_key = self.predict_key\n        e.eval_key = self.eval_key\n        e.bundle_key = self.bundle_key\n\n        return e\n\n    def with_id(self, id):\n        """"""Sets an id for the experiment.""""""\n        b = self._copy()\n        b.config[\'id\'] = id\n        return b\n\n    def with_task(self, task):\n        """"""Sets a specific task type.\n\n        Args:\n            task:  A TaskConfig object.\n\n        """"""\n        b = self._copy()\n        b.config[\'task\'] = task\n        return b\n\n    def with_backend(self, backend):\n        """"""Specifies the backend to be used, e.g. rv.TF_DEEPLAB.""""""\n        b = self._copy()\n        b.config[\'backend\'] = backend\n        return b\n\n    def with_dataset(self, dataset):\n        """"""Specifies the dataset to be used.""""""\n        b = self._copy()\n        b.config[\'dataset\'] = dataset\n        return b\n\n    def with_analyzers(self, analyzers):\n        """"""Add analyzers to be used in the analysis stage.""""""\n        b = self._copy()\n        b.config[\'analyzers\'] = analyzers\n        return b\n\n    def with_analyzer(self, analyzer):\n        """"""Add an analyzer to be used in the analysis stage.""""""\n        return self.with_analyzers([analyzer])\n\n    def with_stats_analyzer(self):\n        """"""Add a stats analyzer to be used in the analysis stage.""""""\n        a = rv.AnalyzerConfig.builder(rv.STATS_ANALYZER).build()\n        return self.with_analyzer(a)\n\n    def with_evaluators(self, evaluators):\n        """"""Sets the evaluators to use for the evaluation stage.""""""\n        b = self._copy()\n        b.config[\'evaluators\'] = evaluators\n        return b\n\n    def with_evaluator(self, evaluator):\n        """"""Sets the evaluator to use for the evaluation stage.""""""\n        return self.with_evaluators([evaluator])\n\n    def with_root_uri(self, uri):\n        """"""Sets the root directory where all output will be stored unless\n        subsequently overridden.\n\n        """"""\n        b = self._copy()\n        b.config[\'root_uri\'] = uri\n        return b\n\n    def with_analyze_uri(self, uri):\n        """"""Sets the location where the results of the analysis stage will be\n           stored.\n\n        """"""\n        b = self._copy()\n        b.config[\'analyze_uri\'] = uri\n        return b\n\n    def with_chip_uri(self, uri):\n        """"""Sets the location where the results of the ""chip"" stage will be\n           stored.\n\n        """"""\n        b = self._copy()\n        b.config[\'chip_uri\'] = uri\n        return b\n\n    def with_train_uri(self, uri):\n        """"""Sets the location where the results of the training stage will be\n           stored.\n\n        """"""\n        b = self._copy()\n        b.config[\'train_uri\'] = uri\n        return b\n\n    def with_predict_uri(self, uri):\n        """"""Sets the location where the results of the prediction stage will be\n           stored.\n\n        """"""\n        b = self._copy()\n        b.config[\'predict_uri\'] = uri\n        return b\n\n    def with_eval_uri(self, uri):\n        """"""Sets the location where the results of the evaluation stage will be\n           stored.\n\n        """"""\n        b = self._copy()\n        b.config[\'eval_uri\'] = uri\n        return b\n\n    def with_bundle_uri(self, uri):\n        """"""Sets the location where the results of the bundling stage will be\n           stored.\n\n        """"""\n        b = self._copy()\n        b.config[\'bundle_uri\'] = uri\n        return b\n\n    def clear_command_uris(self):\n        """"""Clears existing command URIs and keys.\n        Useful for re-using experiment configs for new builders.\n        """"""\n        b = self._copy()\n        b = b.with_analyze_key(None) \\\n             .with_analyze_uri(None) \\\n             .with_chip_key(None) \\\n             .with_chip_uri(None) \\\n             .with_train_key(None) \\\n             .with_train_uri(None) \\\n             .with_predict_key(None) \\\n             .with_predict_uri(None) \\\n             .with_eval_key(None) \\\n             .with_eval_uri(None) \\\n             .with_bundle_key(None) \\\n             .with_bundle_uri(None) \\\n\n        return b\n\n    def with_custom_config(self, config):\n        """"""Sets custom configuration for this experiment.\n        This can be used by plugins such as custom commands.""""""\n        b = self._copy()\n        b.config[\'custom_config\'] = config\n        return b\n\n    def with_analyze_key(self, key):\n        """"""Sets the key associated with the analysis stage.""""""\n        b = self._copy()\n        b.analyze_key = key\n        return b\n\n    def with_chip_key(self, key):\n        """"""Sets the key associated with the ""chip"" stage.""""""\n        b = self._copy()\n        b.chip_key = key\n        return b\n\n    def with_train_key(self, key):\n        """"""Sets the key associated with the training stage.""""""\n        b = self._copy()\n        b.train_key = key\n        return b\n\n    def with_predict_key(self, key):\n        """"""Sets the key associated with the prediction stage.""""""\n        b = self._copy()\n        b.predict_key = key\n        return b\n\n    def with_eval_key(self, key):\n        """"""Sets the key associated with the evaluation stage.""""""\n        b = self._copy()\n        b.eval_key = key\n        return b\n\n    def with_bundle_key(self, key):\n        """"""Sets the key associated with the bundling stage.""""""\n        b = self._copy()\n        b.bundle_key = key\n        return b\n'"
rastervision/experiment/experiment_loader.py,0,"b'import inspect\nimport os\nfrom importlib import import_module\nfrom fnmatch import fnmatchcase\n\nimport rastervision as rv\n\n\nclass LoaderError(Exception):\n    pass\n\n\nclass ExperimentLoader:\n    def __init__(self,\n                 experiment_args=None,\n                 experiment_method_prefix=\'exp\',\n                 experiment_method_patterns=None,\n                 experiment_name_patterns=None):\n        if experiment_args is None:\n            experiment_args = {}\n        self.exp_args = experiment_args\n        self.exp_method_prefix = experiment_method_prefix\n        self.exp_method_patterns = experiment_method_patterns\n        self.exp_name_patterns = experiment_name_patterns\n\n        self._top_level_dir = os.path.abspath(os.curdir)\n\n    def _get_name_from_path(self, path):\n        """"""Gets an importable  name from a path.\n\n        Note: This code is from the python unittest library\n        """"""\n        if path == self._top_level_dir:\n            return \'.\'\n        path = os.path.splitext(os.path.normpath(path))[0]\n\n        _relpath = os.path.relpath(path, self._top_level_dir)\n        assert not os.path.isabs(_relpath), \'Path must be within the project\'\n        assert not _relpath.startswith(\'..\'), \'Path must be within the project\'\n\n        name = _relpath.replace(os.path.sep, \'.\')\n        return name\n\n    def load_from_file(self, path):\n        """"""Loads experiments and commands from an ExperimentSet contained\n        in the given file.\n\n        Returns a tuple (experiments, commands)""""""\n        name = self._get_name_from_path(path)\n        return self.load_from_module(name)\n\n    def load_from_module(self, name):\n        """"""Loads experiments and commands from an ExperimentSet contained\n        in the given module.\n\n        Returns a tuple (experiments, commands)""""""\n        experiments, commands = [], []\n        module = import_module(name)\n        for name, obj in inspect.getmembers(module):\n            if inspect.isclass(obj) and issubclass(obj, rv.ExperimentSet):\n                experiment_set = obj()\n                es, cs = self.load_from_set(experiment_set)\n                experiments += es\n                commands += cs\n        return (experiments, commands)\n\n    def load_from_set(self, experiment_set):\n        return self.load_from_sets([experiment_set])\n\n    def load_from_sets(self, experiment_sets):\n        experiments = []\n        commands = []\n        for experiment_set in experiment_sets:\n            for attrname in dir(experiment_set):\n                include_method = True\n                if not attrname.startswith(self.exp_method_prefix):\n                    include_method = False\n                exp_func = getattr(experiment_set, attrname)\n                if not callable(exp_func):\n                    include_method = False\n\n                if include_method:\n                    full_name = \'%s.%s\' % (experiment_set.__module__,\n                                           exp_func.__qualname__)\n\n                    if self.exp_method_patterns:\n                        include_method = any(\n                            fnmatchcase(full_name, pattern)\n                            for pattern in self.exp_method_patterns)\n\n                    if include_method:\n                        es, cs = self.load_from_experiment(exp_func, full_name)\n                        experiments.extend(es)\n                        commands.extend(cs)\n\n        if self.exp_name_patterns:\n            # Avoid running commands if experiment names are used to filter\n            commands = []\n\n            def include(e):\n                return any(\n                    fnmatchcase(e.id, pattern)\n                    for pattern in self.exp_name_patterns)\n\n            experiments = list(filter(include, experiments))\n\n        return (experiments, commands)\n\n    def load_from_experiment(self, exp_func, full_name):\n        experiments, commands = [], []\n        kwargs = {}\n        params = inspect.signature(exp_func).parameters.items()\n        required_params = [\n            key for key, param in params\n            if param.default == inspect.Parameter.empty\n        ]\n        missing_params = set(required_params) - set(self.exp_args.keys())\n        if missing_params:\n            raise LoaderError(\'Missing required parameters \'\n                              \'for experiment method \'\n                              \'{}: ""{}""\'.format(full_name,\n                                                \'"", ""\'.join(missing_params)))\n        for key, param in params:\n            if key in self.exp_args:\n                kwargs[key] = self.exp_args[key]\n\n        exp = exp_func(**kwargs)\n        if not isinstance(exp, list):\n            exp = [exp]\n\n        for o in exp:\n            if isinstance(o, rv.CommandConfig):\n                commands.append(o)\n            elif isinstance(o, rv.ExperimentConfig):\n                experiments.append(o)\n            else:\n                raise LoaderError(\n                    \'Unknown type for experiment or command: {}\'.format(\n                        type(o)))\n\n        return (experiments, commands)\n'"
rastervision/experiment/experiment_set.py,0,b'class ExperimentSet:\n    def __init__(self):\n        pass\n'
rastervision/filesystem/__init__.py,0,"b'# flake8: noqa\n\nfrom rastervision.filesystem.filesystem import (\n    FileSystem, NotReadableError, NotWritableError, ProtobufParseException)\nfrom rastervision.filesystem.local_filesystem import LocalFileSystem\nfrom rastervision.filesystem.s3_filesystem import S3FileSystem\nfrom rastervision.filesystem.http_filesystem import HttpFileSystem\n'"
rastervision/filesystem/filesystem.py,0,"b'from abc import (ABC, abstractmethod)\nfrom datetime import datetime\n\nimport rastervision as rv\n\n\nclass NotReadableError(Exception):\n    pass\n\n\nclass NotWritableError(Exception):\n    pass\n\n\nclass ProtobufParseException(Exception):\n    pass\n\n\nclass FileSystem(ABC):\n    @staticmethod\n    def get_file_system(uri, mode=\'r\'):\n        return rv._registry.get_file_system(uri, mode)\n\n    @staticmethod\n    @abstractmethod\n    def matches_uri(uri: str, mode: str) -> bool:\n        """"""Returns True if this FileSystem should be used\n        for the given URI under the given mode.\n\n        Mode can be \'r\' or \'w\'\n        """"""\n        pass  # pragma: no cover\n\n    @staticmethod\n    @abstractmethod\n    def file_exists(uri: str, include_dir: bool = True) -> bool:\n        """"""Check if a  file exists.\n        Args:\n          uri: The URI to check\n          include_dir: Include directories in check, if this filesystem\n                       supports directory reads. Otherwise only\n                       return true if a single file exists at the URI.\n        """"""\n        pass  # pragma: no cover\n\n    @staticmethod\n    @abstractmethod\n    def read_str(uri: str) -> str:\n        pass  # pragma: no cover\n\n    @staticmethod\n    @abstractmethod\n    def read_bytes(uri: str) -> bytes:\n        pass  # pragma: no cover\n\n    @staticmethod\n    @abstractmethod\n    def write_str(uri: str, data: str) -> None:\n        pass  # pragma: no cover\n\n    @staticmethod\n    @abstractmethod\n    def write_bytes(uri: str, data: bytes) -> None:\n        pass  # pragma: no cover\n\n    @staticmethod\n    @abstractmethod\n    def sync_to_dir(src_dir_uri: str, dest_dir_uri: str,\n                    delete: bool = False) -> None:\n        """"""Syncs a local directory to a destination.\n\n        Arguments:\n           - src_dir_uri: Source directory to sync from. Must be a local directoy.\n           - dest_dir_uri: A destination that can be sync to by this FileSystem.\n           - delete: True if the destination should be deleted first. Defaults to False.\n        """"""\n        pass  # pragma: no cover\n\n    @staticmethod\n    @abstractmethod\n    def sync_from_dir(src_dir_uri: str,\n                      dest_dir_uri: str,\n                      delete: bool = False) -> None:\n        """"""Syncs a local directory to a destination.\n\n        Arguments:\n           - src_dir_uri: Source directory to sync from. Must be a local directoy.\n           - dest_dir_uri: A destination that can be sync to by this FileSystem.\n           - delete: True if the destination should be deleted first. Defaults to False.\n        """"""\n        pass  # pragma: no cover\n\n    @staticmethod\n    @abstractmethod\n    def copy_to(src_path: str, dst_uri: str) -> None:\n        pass  # pragma: no cover\n\n    @staticmethod\n    @abstractmethod\n    def copy_from(uri: str, path: str) -> None:\n        pass  # pragma: no cover\n\n    @staticmethod\n    @abstractmethod\n    def local_path(uri: str, download_dir: str) -> None:\n        pass  # pragma: no cover\n\n    @staticmethod\n    @abstractmethod\n    def last_modified(uri: str) -> datetime:\n        """"""Returns the last modified  date in UTC of this URI,\n        or None if this FileSystem does not support this operation.\n        """"""\n        pass  # pragma: no cover\n\n    @staticmethod\n    @abstractmethod\n    def list_paths(uri, ext=None):\n        pass  # pragma: no cover\n'"
rastervision/filesystem/http_filesystem.py,0,"b""import os\nimport shutil\nimport urllib\nimport urllib.request\nfrom datetime import datetime\n\nfrom rastervision.filesystem import (FileSystem, NotReadableError,\n                                     NotWritableError)\nfrom urllib.parse import urlparse\n\n\nclass HttpFileSystem(FileSystem):\n    @staticmethod\n    def matches_uri(uri: str, mode: str) -> bool:\n        parsed_uri = urlparse(uri)\n        return parsed_uri.scheme in ['http', 'https']\n\n    @staticmethod\n    def file_exists(uri: str, include_dir: bool = True) -> bool:\n        try:\n            response = urllib.request.urlopen(uri)\n            if response.getcode() == 200:\n                return int(response.headers['content-length']) > 0\n            else:\n                return False  # pragma: no cover\n        except urllib.error.URLError:\n            return False\n\n    @staticmethod\n    def read_str(uri: str) -> str:\n        return HttpFileSystem.read_bytes(uri).decode('utf8')\n\n    @staticmethod\n    def read_bytes(uri: str) -> bytes:\n        with urllib.request.urlopen(uri) as req:\n            return req.read()\n\n    @staticmethod\n    def write_str(uri: str, data: str) -> None:\n        raise NotWritableError('Could not write {}'.format(uri))\n\n    @staticmethod\n    def write_bytes(uri: str, data: bytes) -> None:\n        raise NotWritableError('Could not write {}'.format(uri))\n\n    @staticmethod\n    def sync_to_dir(src_dir_uri: str, dest_dir_uri: str,\n                    delete: bool = False) -> None:\n        raise NotWritableError('Could not write {}'.format(dest_dir_uri))\n\n    @staticmethod\n    def sync_from_dir(src_dir_uri: str,\n                      dest_dir_uri: str,\n                      delete: bool = False) -> None:\n        raise NotReadableError(\n            'Cannot read directory from HTTP {}'.format(src_dir_uri))\n\n    @staticmethod\n    def copy_to(src_path: str, dst_uri: str) -> None:\n        raise NotWritableError('Could not write {}'.format(dst_uri))\n\n    @staticmethod\n    def copy_from(uri: str, path: str) -> None:\n        with urllib.request.urlopen(uri) as response:\n            with open(path, 'wb') as out_file:\n                try:\n                    shutil.copyfileobj(response, out_file)\n                except Exception:  # pragma: no cover\n                    raise NotReadableError('Could not read {}'.format(uri))\n\n    @staticmethod\n    def local_path(uri: str, download_dir: str) -> None:\n        parsed_uri = urlparse(uri)\n        path = os.path.join(download_dir, 'http', parsed_uri.netloc,\n                            parsed_uri.path[1:])\n        # This function is expected to return something that is file path-like\n        # (as opposed to directory-like),\n        # so if the path ends with / we strip it off. This was motivated by\n        # a URI that was a zxy tile schema that doesn't end in .png which is\n        # parsed by urlparse into a path that ends in a /.\n        if path.endswith('/'):\n            path = path[:-1]\n        return path\n\n    @staticmethod\n    def last_modified(uri: str) -> datetime:\n        return None\n\n    @staticmethod\n    def list_paths(uri, suffix=None):  # pragma: no cover\n        raise NotImplementedError()\n"""
rastervision/filesystem/local_filesystem.py,0,"b'import os\nimport shutil\nfrom datetime import datetime, timezone\nimport glob\n\nfrom rastervision.filesystem import (FileSystem, NotReadableError)\n\n\ndef make_dir(path, check_empty=False, force_empty=False, use_dirname=False):\n    """"""Make a local directory.\n\n    Args:\n        path: path to directory\n        check_empty: if True, check that directory is empty\n        force_empty: if True, delete files if necessary to make directory\n            empty\n        use_dirname: if True, use the the parent directory as path\n\n    Raises:\n        ValueError if check_empty is True and directory is not empty\n    """"""\n    directory = path\n    if use_dirname:\n        directory = os.path.abspath(os.path.dirname(path))\n\n    if force_empty and os.path.isdir(directory):\n        shutil.rmtree(directory)\n\n    os.makedirs(directory, exist_ok=True)\n\n    if check_empty and any(os.scandir(directory)):\n        raise ValueError(\n            \'{} needs to be an empty directory!\'.format(directory))\n\n\nclass LocalFileSystem(FileSystem):\n    @staticmethod\n    def matches_uri(uri: str, mode: str) -> bool:\n        return True\n\n    @staticmethod\n    def file_exists(uri: str, include_dir: bool = True) -> bool:\n        return (os.path.isfile(uri) or (include_dir and os.path.isdir(uri)))\n\n    @staticmethod\n    def read_str(file_uri: str) -> str:\n        if not os.path.isfile(file_uri):\n            raise NotReadableError(\'Could not read {}\'.format(file_uri))\n        with open(file_uri, \'r\') as file_buffer:\n            return file_buffer.read()\n\n    @staticmethod\n    def read_bytes(file_uri: str) -> bytes:\n        if not os.path.isfile(file_uri):\n            raise NotReadableError(\'Could not read {}\'.format(file_uri))\n        with open(file_uri, \'rb\') as file_buffer:\n            return file_buffer.read()\n\n    @staticmethod\n    def write_str(file_uri: str, data: str) -> None:\n        make_dir(file_uri, use_dirname=True)\n        with open(file_uri, \'w\') as content_file:\n            content_file.write(data)\n\n    @staticmethod\n    def write_bytes(file_uri: str, data: bytes) -> None:\n        make_dir(file_uri, use_dirname=True)\n        with open(file_uri, \'wb\') as content_file:\n            content_file.write(data)\n\n    @staticmethod\n    def sync_from_dir(src_dir_uri: str,\n                      dest_dir_uri: str,\n                      delete: bool = False) -> None:\n        if src_dir_uri == dest_dir_uri:\n            return\n\n        if delete:\n            shutil.rmtree(dest_dir_uri)\n\n        # https://stackoverflow.com/a/15824216/841563\n        def recursive_overwrite(src, dest):\n            if os.path.isdir(src):\n                if not os.path.isdir(dest):\n                    os.makedirs(dest)\n                    for entry in os.scandir(src):\n                        recursive_overwrite(entry.path,\n                                            os.path.join(dest, entry.name))\n            else:\n                shutil.copyfile(src, dest)\n\n        recursive_overwrite(src_dir_uri, dest_dir_uri)\n\n    @staticmethod\n    def sync_to_dir(src_dir_uri: str, dest_dir_uri: str,\n                    delete: bool = False) -> None:\n        LocalFileSystem.sync_from_dir(src_dir_uri, dest_dir_uri, delete)\n\n    @staticmethod\n    def copy_to(src_path: str, dst_uri: str) -> None:\n        if src_path != dst_uri:\n            make_dir(dst_uri, use_dirname=True)\n            shutil.copyfile(src_path, dst_uri)\n\n    @staticmethod\n    def copy_from(uri: str, path: str) -> None:\n        not_found = not os.path.isfile(path)\n        if not_found:\n            raise NotReadableError(\'Could not read {}\'.format(uri))\n\n    @staticmethod\n    def local_path(uri: str, download_dir: str) -> None:\n        path = uri\n        return path\n\n    @staticmethod\n    def last_modified(uri: str) -> datetime:\n        local_last_modified = datetime.utcfromtimestamp(os.path.getmtime(uri))\n        return local_last_modified.replace(tzinfo=timezone.utc)\n\n    @staticmethod\n    def list_paths(uri, ext=None):\n        if ext is None:\n            ext = \'\'\n        return glob.glob(os.path.join(uri, \'*\' + ext))\n'"
rastervision/filesystem/s3_filesystem.py,0,"b'import io\nimport os\nimport subprocess\nfrom datetime import datetime\nfrom urllib.parse import urlparse\n\nfrom rastervision.filesystem import (FileSystem, NotReadableError,\n                                     NotWritableError)\n\n\n# Code from https://alexwlchan.net/2017/07/listing-s3-keys/\ndef get_matching_s3_objects(bucket, prefix=\'\', suffix=\'\',\n                            request_payer=\'None\'):\n    """"""\n    Generate objects in an S3 bucket.\n\n    :param bucket: Name of the S3 bucket.\n    :param prefix: Only fetch objects whose key starts with\n        this prefix (optional).\n    :param suffix: Only fetch objects whose keys end with\n        this suffix (optional).\n    """"""\n    import boto3\n    s3 = boto3.client(\'s3\')\n    kwargs = {\'Bucket\': bucket, \'RequestPayer\': request_payer}\n\n    # If the prefix is a single string (not a tuple of strings), we can\n    # do the filtering directly in the S3 API.\n    if isinstance(prefix, str):\n        kwargs[\'Prefix\'] = prefix\n\n    while True:\n\n        # The S3 API response is a large blob of metadata.\n        # \'Contents\' contains information about the listed objects.\n        resp = s3.list_objects_v2(**kwargs)\n\n        try:\n            contents = resp[\'Contents\']\n        except KeyError:\n            return\n\n        for obj in contents:\n            key = obj[\'Key\']\n            if key.startswith(prefix) and key.endswith(suffix):\n                yield obj\n\n        # The S3 API is paginated, returning up to 1000 keys at a time.\n        # Pass the continuation token into the next response, until we\n        # reach the final page (when this field is missing).\n        try:\n            kwargs[\'ContinuationToken\'] = resp[\'NextContinuationToken\']\n        except KeyError:\n            break\n\n\ndef get_matching_s3_keys(bucket, prefix=\'\', suffix=\'\', request_payer=\'None\'):\n    """"""\n    Generate the keys in an S3 bucket.\n\n    :param bucket: Name of the S3 bucket.\n    :param prefix: Only fetch keys that start with this prefix (optional).\n    :param suffix: Only fetch keys that end with this suffix (optional).\n    """"""\n    for obj in get_matching_s3_objects(bucket, prefix, suffix, request_payer):\n        yield obj[\'Key\']\n\n\nclass S3FileSystem(FileSystem):\n    @staticmethod\n    def get_request_payer():\n        # Import here to avoid circular reference.\n        from rastervision.rv_config import RVConfig\n        rv_config = RVConfig.get_instance()\n        s3_config = rv_config.get_subconfig(\'AWS_S3\')\n\n        # \'None\' needs the quotes because boto3 cannot handle None.\n        return (\'requester\' if s3_config(\n            \'requester_pays\', parser=bool, default=\'False\') else \'None\')\n\n    @staticmethod\n    def get_session():\n        # Lazily load boto\n        import boto3\n        return boto3.Session()\n\n    @staticmethod\n    def matches_uri(uri: str, mode: str) -> bool:\n        parsed_uri = urlparse(uri)\n        return parsed_uri.scheme == \'s3\'\n\n    @staticmethod\n    def file_exists(uri: str, include_dir: bool = True) -> bool:\n        # Lazily load boto\n        import botocore\n\n        parsed_uri = urlparse(uri)\n        bucket = parsed_uri.netloc\n        key = parsed_uri.path[1:]\n        request_payer = S3FileSystem.get_request_payer()\n\n        if include_dir:\n            s3 = S3FileSystem.get_session().client(\'s3\')\n            try:\n                # Ensure key ends in slash so that this won\'t pick up files that\n                # contain the key as a prefix, but aren\'t actually directories.\n                # Example: if key is \'model\' then we don\'t want to consider\n                # model-123 a match.\n                dir_key = key if key[-1] == \'/\' else key + \'/\'\n                response = s3.list_objects_v2(\n                    Bucket=bucket,\n                    Prefix=dir_key,\n                    MaxKeys=1,\n                    RequestPayer=request_payer)\n                if response[\'KeyCount\'] == 0:\n                    return S3FileSystem.file_exists(uri, include_dir=False)\n                return True\n            except botocore.exceptions.ClientError as e:\n                return False\n        else:\n            s3r = S3FileSystem.get_session().resource(\'s3\')\n            try:\n                s3r.Object(bucket, key).load(RequestPayer=request_payer)\n                return True\n            except botocore.exceptions.ClientError as e:\n                return False\n\n    @staticmethod\n    def read_str(uri: str) -> str:\n        return S3FileSystem.read_bytes(uri).decode(\'utf-8\')\n\n    @staticmethod\n    def read_bytes(uri: str) -> bytes:\n        import botocore\n\n        s3 = S3FileSystem.get_session().client(\'s3\')\n        request_payer = S3FileSystem.get_request_payer()\n\n        parsed_uri = urlparse(uri)\n        with io.BytesIO() as file_buffer:\n            try:\n                s3.download_fileobj(\n                    parsed_uri.netloc,\n                    parsed_uri.path[1:],\n                    file_buffer,\n                    ExtraArgs={\'RequestPayer\': request_payer})\n                return file_buffer.getvalue()\n            except botocore.exceptions.ClientError as e:\n                raise NotReadableError(\'Could not read {}\'.format(uri)) from e\n\n    @staticmethod\n    def write_str(uri: str, data: str) -> None:\n        data = bytes(data, encoding=\'utf-8\')\n        S3FileSystem.write_bytes(uri, data)\n\n    @staticmethod\n    def write_bytes(uri: str, data: bytes) -> None:\n        s3 = S3FileSystem.get_session().client(\'s3\')\n\n        parsed_uri = urlparse(uri)\n        bucket = parsed_uri.netloc\n        key = parsed_uri.path[1:]\n        with io.BytesIO(data) as str_buffer:\n            try:\n                s3.upload_fileobj(str_buffer, bucket, key)\n            except Exception as e:\n                raise NotWritableError(\'Could not write {}\'.format(uri)) from e\n\n    @staticmethod\n    def sync_from_dir(src_dir_uri: str,\n                      dest_dir_uri: str,\n                      delete: bool = False) -> None:  # pragma: no cover\n        command = [\'aws\', \'s3\', \'sync\', src_dir_uri, dest_dir_uri]\n        if delete:\n            command.append(\'--delete\')\n        subprocess.run(command)\n\n    @staticmethod\n    def sync_to_dir(src_dir_uri: str, dest_dir_uri: str,\n                    delete: bool = False) -> None:  # pragma: no cover\n        command = [\'aws\', \'s3\', \'sync\', src_dir_uri, dest_dir_uri]\n        if delete:\n            command.append(\'--delete\')\n        request_payer = S3FileSystem.get_request_payer()\n        if request_payer:\n            command.append(\'--request-payer\')\n        subprocess.run(command)\n\n    @staticmethod\n    def copy_to(src_path: str, dst_uri: str) -> None:\n        s3 = S3FileSystem.get_session().client(\'s3\')\n\n        parsed_uri = urlparse(dst_uri)\n        if os.path.isfile(src_path):\n            try:\n                s3.upload_file(src_path, parsed_uri.netloc,\n                               parsed_uri.path[1:])\n            except Exception as e:\n                raise NotWritableError(\n                    \'Could not write {}\'.format(dst_uri)) from e\n        else:\n            S3FileSystem.sync_to_dir(src_path, dst_uri, delete=True)\n\n    @staticmethod\n    def copy_from(uri: str, path: str) -> None:\n        import botocore\n\n        s3 = S3FileSystem.get_session().client(\'s3\')\n        request_payer = S3FileSystem.get_request_payer()\n\n        parsed_uri = urlparse(uri)\n        try:\n            s3.download_file(\n                parsed_uri.netloc,\n                parsed_uri.path[1:],\n                path,\n                ExtraArgs={\'RequestPayer\': request_payer})\n        except botocore.exceptions.ClientError:\n            raise NotReadableError(\'Could not read {}\'.format(uri))\n\n    @staticmethod\n    def local_path(uri: str, download_dir: str) -> None:\n        parsed_uri = urlparse(uri)\n        path = os.path.join(download_dir, \'s3\', parsed_uri.netloc,\n                            parsed_uri.path[1:])\n        return path\n\n    @staticmethod\n    def last_modified(uri: str) -> datetime:\n        parsed_uri = urlparse(uri)\n        bucket, key = parsed_uri.netloc, parsed_uri.path[1:]\n        s3 = S3FileSystem.get_session().client(\'s3\')\n        request_payer = S3FileSystem.get_request_payer()\n        head_data = s3.head_object(\n            Bucket=bucket, Key=key, RequestPayer=request_payer)\n        return head_data[\'LastModified\']\n\n    @staticmethod\n    def list_paths(uri, ext=\'\'):\n        request_payer = S3FileSystem.get_request_payer()\n        parsed_uri = urlparse(uri)\n        bucket = parsed_uri.netloc\n        prefix = os.path.join(parsed_uri.path[1:])\n        keys = get_matching_s3_keys(\n            bucket, prefix, suffix=ext, request_payer=request_payer)\n        return [os.path.join(\'s3://\', bucket, key) for key in keys]\n'"
rastervision/protos/__init__.py,0,b''
rastervision/protos/analyzer_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: rastervision/protos/analyzer.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom google.protobuf import struct_pb2 as google_dot_protobuf_dot_struct__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'rastervision/protos/analyzer.proto\',\n  package=\'rv.protos\',\n  syntax=\'proto2\',\n  serialized_pb=_b(\'\\n\\""rastervision/protos/analyzer.proto\\x12\\trv.protos\\x1a\\x1cgoogle/protobuf/struct.proto\\""\\x9a\\x02\\n\\x0e\\x41nalyzerConfig\\x12\\x15\\n\\ranalyzer_type\\x18\\x01 \\x02(\\t\\x12N\\n\\x15stats_analyzer_config\\x18\\x04 \\x01(\\x0b\\x32-.rv.protos.AnalyzerConfig.StatsAnalyzerConfigH\\x00\\x12\\x13\\n\\tstats_uri\\x18\\x05 \\x01(\\tH\\x00\\x12\\x30\\n\\rcustom_config\\x18\\x03 \\x01(\\x0b\\x32\\x17.google.protobuf.StructH\\x00\\x1a=\\n\\x13StatsAnalyzerConfig\\x12\\x11\\n\\tstats_uri\\x18\\x01 \\x02(\\t\\x12\\x13\\n\\x0bsample_prob\\x18\\x02 \\x01(\\x02\\x42\\x1b\\n\\x19raster_transformer_config\')\n  ,\n  dependencies=[google_dot_protobuf_dot_struct__pb2.DESCRIPTOR,])\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\n\n\n\n_ANALYZERCONFIG_STATSANALYZERCONFIG = _descriptor.Descriptor(\n  name=\'StatsAnalyzerConfig\',\n  full_name=\'rv.protos.AnalyzerConfig.StatsAnalyzerConfig\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'stats_uri\', full_name=\'rv.protos.AnalyzerConfig.StatsAnalyzerConfig.stats_uri\', index=0,\n      number=1, type=9, cpp_type=9, label=2,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'sample_prob\', full_name=\'rv.protos.AnalyzerConfig.StatsAnalyzerConfig.sample_prob\', index=1,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=272,\n  serialized_end=333,\n)\n\n_ANALYZERCONFIG = _descriptor.Descriptor(\n  name=\'AnalyzerConfig\',\n  full_name=\'rv.protos.AnalyzerConfig\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'analyzer_type\', full_name=\'rv.protos.AnalyzerConfig.analyzer_type\', index=0,\n      number=1, type=9, cpp_type=9, label=2,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'stats_analyzer_config\', full_name=\'rv.protos.AnalyzerConfig.stats_analyzer_config\', index=1,\n      number=4, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'stats_uri\', full_name=\'rv.protos.AnalyzerConfig.stats_uri\', index=2,\n      number=5, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'custom_config\', full_name=\'rv.protos.AnalyzerConfig.custom_config\', index=3,\n      number=3, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[_ANALYZERCONFIG_STATSANALYZERCONFIG, ],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n    _descriptor.OneofDescriptor(\n      name=\'raster_transformer_config\', full_name=\'rv.protos.AnalyzerConfig.raster_transformer_config\',\n      index=0, containing_type=None, fields=[]),\n  ],\n  serialized_start=80,\n  serialized_end=362,\n)\n\n_ANALYZERCONFIG_STATSANALYZERCONFIG.containing_type = _ANALYZERCONFIG\n_ANALYZERCONFIG.fields_by_name[\'stats_analyzer_config\'].message_type = _ANALYZERCONFIG_STATSANALYZERCONFIG\n_ANALYZERCONFIG.fields_by_name[\'custom_config\'].message_type = google_dot_protobuf_dot_struct__pb2._STRUCT\n_ANALYZERCONFIG.oneofs_by_name[\'raster_transformer_config\'].fields.append(\n  _ANALYZERCONFIG.fields_by_name[\'stats_analyzer_config\'])\n_ANALYZERCONFIG.fields_by_name[\'stats_analyzer_config\'].containing_oneof = _ANALYZERCONFIG.oneofs_by_name[\'raster_transformer_config\']\n_ANALYZERCONFIG.oneofs_by_name[\'raster_transformer_config\'].fields.append(\n  _ANALYZERCONFIG.fields_by_name[\'stats_uri\'])\n_ANALYZERCONFIG.fields_by_name[\'stats_uri\'].containing_oneof = _ANALYZERCONFIG.oneofs_by_name[\'raster_transformer_config\']\n_ANALYZERCONFIG.oneofs_by_name[\'raster_transformer_config\'].fields.append(\n  _ANALYZERCONFIG.fields_by_name[\'custom_config\'])\n_ANALYZERCONFIG.fields_by_name[\'custom_config\'].containing_oneof = _ANALYZERCONFIG.oneofs_by_name[\'raster_transformer_config\']\nDESCRIPTOR.message_types_by_name[\'AnalyzerConfig\'] = _ANALYZERCONFIG\n\nAnalyzerConfig = _reflection.GeneratedProtocolMessageType(\'AnalyzerConfig\', (_message.Message,), dict(\n\n  StatsAnalyzerConfig = _reflection.GeneratedProtocolMessageType(\'StatsAnalyzerConfig\', (_message.Message,), dict(\n    DESCRIPTOR = _ANALYZERCONFIG_STATSANALYZERCONFIG,\n    __module__ = \'rastervision.protos.analyzer_pb2\'\n    # @@protoc_insertion_point(class_scope:rv.protos.AnalyzerConfig.StatsAnalyzerConfig)\n    ))\n  ,\n  DESCRIPTOR = _ANALYZERCONFIG,\n  __module__ = \'rastervision.protos.analyzer_pb2\'\n  # @@protoc_insertion_point(class_scope:rv.protos.AnalyzerConfig)\n  ))\n_sym_db.RegisterMessage(AnalyzerConfig)\n_sym_db.RegisterMessage(AnalyzerConfig.StatsAnalyzerConfig)\n\n\n# @@protoc_insertion_point(module_scope)\n'"
rastervision/protos/augmentor_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: rastervision/protos/augmentor.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom google.protobuf import struct_pb2 as google_dot_protobuf_dot_struct__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'rastervision/protos/augmentor.proto\',\n  package=\'rv.protos\',\n  syntax=\'proto2\',\n  serialized_pb=_b(\'\\n#rastervision/protos/augmentor.proto\\x12\\trv.protos\\x1a\\x1cgoogle/protobuf/struct.proto\\""\\x83\\x01\\n\\x0f\\x41ugmentorConfig\\x12\\x16\\n\\x0e\\x61ugmentor_type\\x18\\x01 \\x02(\\t\\x12\\x12\\n\\x08\\x61ug_prob\\x18\\x03 \\x01(\\x02H\\x00\\x12\\x30\\n\\rcustom_config\\x18\\x05 \\x01(\\x0b\\x32\\x17.google.protobuf.StructH\\x00\\x42\\x12\\n\\x10\\x61ugmentor_config\')\n  ,\n  dependencies=[google_dot_protobuf_dot_struct__pb2.DESCRIPTOR,])\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\n\n\n\n_AUGMENTORCONFIG = _descriptor.Descriptor(\n  name=\'AugmentorConfig\',\n  full_name=\'rv.protos.AugmentorConfig\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'augmentor_type\', full_name=\'rv.protos.AugmentorConfig.augmentor_type\', index=0,\n      number=1, type=9, cpp_type=9, label=2,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'aug_prob\', full_name=\'rv.protos.AugmentorConfig.aug_prob\', index=1,\n      number=3, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'custom_config\', full_name=\'rv.protos.AugmentorConfig.custom_config\', index=2,\n      number=5, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n    _descriptor.OneofDescriptor(\n      name=\'augmentor_config\', full_name=\'rv.protos.AugmentorConfig.augmentor_config\',\n      index=0, containing_type=None, fields=[]),\n  ],\n  serialized_start=81,\n  serialized_end=212,\n)\n\n_AUGMENTORCONFIG.fields_by_name[\'custom_config\'].message_type = google_dot_protobuf_dot_struct__pb2._STRUCT\n_AUGMENTORCONFIG.oneofs_by_name[\'augmentor_config\'].fields.append(\n  _AUGMENTORCONFIG.fields_by_name[\'aug_prob\'])\n_AUGMENTORCONFIG.fields_by_name[\'aug_prob\'].containing_oneof = _AUGMENTORCONFIG.oneofs_by_name[\'augmentor_config\']\n_AUGMENTORCONFIG.oneofs_by_name[\'augmentor_config\'].fields.append(\n  _AUGMENTORCONFIG.fields_by_name[\'custom_config\'])\n_AUGMENTORCONFIG.fields_by_name[\'custom_config\'].containing_oneof = _AUGMENTORCONFIG.oneofs_by_name[\'augmentor_config\']\nDESCRIPTOR.message_types_by_name[\'AugmentorConfig\'] = _AUGMENTORCONFIG\n\nAugmentorConfig = _reflection.GeneratedProtocolMessageType(\'AugmentorConfig\', (_message.Message,), dict(\n  DESCRIPTOR = _AUGMENTORCONFIG,\n  __module__ = \'rastervision.protos.augmentor_pb2\'\n  # @@protoc_insertion_point(class_scope:rv.protos.AugmentorConfig)\n  ))\n_sym_db.RegisterMessage(AugmentorConfig)\n\n\n# @@protoc_insertion_point(module_scope)\n'"
rastervision/protos/backend_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: rastervision/protos/backend.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom google.protobuf import struct_pb2 as google_dot_protobuf_dot_struct__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'rastervision/protos/backend.proto\',\n  package=\'rv.protos\',\n  syntax=\'proto2\',\n  serialized_pb=_b(\'\\n!rastervision/protos/backend.proto\\x12\\trv.protos\\x1a\\x1cgoogle/protobuf/struct.proto\\""\\xab\\x0c\\n\\rBackendConfig\\x12\\x14\\n\\x0c\\x62\\x61\\x63kend_type\\x18\\x01 \\x02(\\t\\x12\\x1c\\n\\x14pretrained_model_uri\\x18\\x03 \\x01(\\t\\x12V\\n\\x1atf_object_detection_config\\x18\\x04 \\x01(\\x0b\\x32\\x30.rv.protos.BackendConfig.TFObjectDetectionConfigH\\x00\\x12Y\\n\\x1bkeras_classification_config\\x18\\x05 \\x01(\\x0b\\x32\\x32.rv.protos.BackendConfig.KerasClassificationConfigH\\x00\\x12\\x45\\n\\x11tf_deeplab_config\\x18\\x07 \\x01(\\x0b\\x32(.rv.protos.BackendConfig.TFDeeplabConfigH\\x00\\x12\\x30\\n\\rcustom_config\\x18\\x06 \\x01(\\x0b\\x32\\x17.google.protobuf.StructH\\x00\\x1a\\xb6\\x03\\n\\x17TFObjectDetectionConfig\\x12\\x1a\\n\\rsync_interval\\x18\\x01 \\x01(\\x05:\\x03\\x36\\x30\\x30\\x12\\x1b\\n\\rdo_monitoring\\x18\\x02 \\x01(\\x08:\\x04true\\x12\\x1c\\n\\rreplace_model\\x18\\x03 \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\x44\\n\\rmodel_main_py\\x18\\x04 \\x01(\\t:-/opt/tf-models/object_detection/model_main.py\\x12L\\n\\texport_py\\x18\\x05 \\x01(\\t:9/opt/tf-models/object_detection/export_inference_graph.py\\x12\\x19\\n\\x11training_data_uri\\x18\\x06 \\x01(\\t\\x12\\x1b\\n\\x13training_output_uri\\x18\\x07 \\x01(\\t\\x12\\x11\\n\\tmodel_uri\\x18\\x08 \\x01(\\t\\x12!\\n\\x19\\x66ine_tune_checkpoint_name\\x18\\t \\x01(\\t\\x12\\x14\\n\\x05\\x64\\x65\\x62ug\\x18\\n \\x01(\\x08:\\x05\\x66\\x61lse\\x12,\\n\\x0btfod_config\\x18\\x0b \\x02(\\x0b\\x32\\x17.google.protobuf.Struct\\x1a\\xff\\x01\\n\\x19KerasClassificationConfig\\x12\\x1a\\n\\rsync_interval\\x18\\x01 \\x01(\\x05:\\x03\\x36\\x30\\x30\\x12\\x1b\\n\\rdo_monitoring\\x18\\x02 \\x01(\\x08:\\x04true\\x12\\x1c\\n\\rreplace_model\\x18\\x03 \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\x19\\n\\x11training_data_uri\\x18\\x04 \\x01(\\t\\x12\\x1b\\n\\x13training_output_uri\\x18\\x05 \\x01(\\t\\x12\\x11\\n\\tmodel_uri\\x18\\x06 \\x01(\\t\\x12\\x14\\n\\x05\\x64\\x65\\x62ug\\x18\\x07 \\x01(\\x08:\\x05\\x66\\x61lse\\x12*\\n\\tkc_config\\x18\\x08 \\x02(\\x0b\\x32\\x17.google.protobuf.Struct\\x1a\\xec\\x03\\n\\x0fTFDeeplabConfig\\x12\\x31\\n\\x08train_py\\x18\\x01 \\x01(\\t:\\x1f/opt/tf-models/deeplab/train.py\\x12/\\n\\x07\\x65val_py\\x18\\x0e \\x01(\\t:\\x1e/opt/tf-models/deeplab/eval.py\\x12\\x39\\n\\texport_py\\x18\\x02 \\x01(\\t:&/opt/tf-models/deeplab/export_model.py\\x12\\x19\\n\\x11train_restart_dir\\x18\\x03 \\x01(\\t\\x12\\x1a\\n\\rsync_interval\\x18\\x04 \\x01(\\x05:\\x03\\x36\\x30\\x30\\x12\\x1b\\n\\rdo_monitoring\\x18\\x05 \\x01(\\x08:\\x04true\\x12\\x16\\n\\x07\\x64o_eval\\x18\\r \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\x1c\\n\\rreplace_model\\x18\\x06 \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\x19\\n\\x11training_data_uri\\x18\\x07 \\x01(\\t\\x12\\x1b\\n\\x13training_output_uri\\x18\\x08 \\x01(\\t\\x12\\x11\\n\\tmodel_uri\\x18\\t \\x01(\\t\\x12!\\n\\x19\\x66ine_tune_checkpoint_name\\x18\\n \\x01(\\t\\x12,\\n\\x0btfdl_config\\x18\\x0b \\x02(\\x0b\\x32\\x17.google.protobuf.Struct\\x12\\x14\\n\\x05\\x64\\x65\\x62ug\\x18\\x0c \\x01(\\x08:\\x05\\x66\\x61lseB\\x10\\n\\x0e\\x62\\x61\\x63kend_config\')\n  ,\n  dependencies=[google_dot_protobuf_dot_struct__pb2.DESCRIPTOR,])\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\n\n\n\n_BACKENDCONFIG_TFOBJECTDETECTIONCONFIG = _descriptor.Descriptor(\n  name=\'TFObjectDetectionConfig\',\n  full_name=\'rv.protos.BackendConfig.TFObjectDetectionConfig\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'sync_interval\', full_name=\'rv.protos.BackendConfig.TFObjectDetectionConfig.sync_interval\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=600,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'do_monitoring\', full_name=\'rv.protos.BackendConfig.TFObjectDetectionConfig.do_monitoring\', index=1,\n      number=2, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=True,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'replace_model\', full_name=\'rv.protos.BackendConfig.TFObjectDetectionConfig.replace_model\', index=2,\n      number=3, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'model_main_py\', full_name=\'rv.protos.BackendConfig.TFObjectDetectionConfig.model_main_py\', index=3,\n      number=4, type=9, cpp_type=9, label=1,\n      has_default_value=True, default_value=_b(""/opt/tf-models/object_detection/model_main.py"").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'export_py\', full_name=\'rv.protos.BackendConfig.TFObjectDetectionConfig.export_py\', index=4,\n      number=5, type=9, cpp_type=9, label=1,\n      has_default_value=True, default_value=_b(""/opt/tf-models/object_detection/export_inference_graph.py"").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'training_data_uri\', full_name=\'rv.protos.BackendConfig.TFObjectDetectionConfig.training_data_uri\', index=5,\n      number=6, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'training_output_uri\', full_name=\'rv.protos.BackendConfig.TFObjectDetectionConfig.training_output_uri\', index=6,\n      number=7, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'model_uri\', full_name=\'rv.protos.BackendConfig.TFObjectDetectionConfig.model_uri\', index=7,\n      number=8, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'fine_tune_checkpoint_name\', full_name=\'rv.protos.BackendConfig.TFObjectDetectionConfig.fine_tune_checkpoint_name\', index=8,\n      number=9, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'debug\', full_name=\'rv.protos.BackendConfig.TFObjectDetectionConfig.debug\', index=9,\n      number=10, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'tfod_config\', full_name=\'rv.protos.BackendConfig.TFObjectDetectionConfig.tfod_config\', index=10,\n      number=11, type=11, cpp_type=10, label=2,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=449,\n  serialized_end=887,\n)\n\n_BACKENDCONFIG_KERASCLASSIFICATIONCONFIG = _descriptor.Descriptor(\n  name=\'KerasClassificationConfig\',\n  full_name=\'rv.protos.BackendConfig.KerasClassificationConfig\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'sync_interval\', full_name=\'rv.protos.BackendConfig.KerasClassificationConfig.sync_interval\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=600,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'do_monitoring\', full_name=\'rv.protos.BackendConfig.KerasClassificationConfig.do_monitoring\', index=1,\n      number=2, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=True,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'replace_model\', full_name=\'rv.protos.BackendConfig.KerasClassificationConfig.replace_model\', index=2,\n      number=3, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'training_data_uri\', full_name=\'rv.protos.BackendConfig.KerasClassificationConfig.training_data_uri\', index=3,\n      number=4, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'training_output_uri\', full_name=\'rv.protos.BackendConfig.KerasClassificationConfig.training_output_uri\', index=4,\n      number=5, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'model_uri\', full_name=\'rv.protos.BackendConfig.KerasClassificationConfig.model_uri\', index=5,\n      number=6, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'debug\', full_name=\'rv.protos.BackendConfig.KerasClassificationConfig.debug\', index=6,\n      number=7, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'kc_config\', full_name=\'rv.protos.BackendConfig.KerasClassificationConfig.kc_config\', index=7,\n      number=8, type=11, cpp_type=10, label=2,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=890,\n  serialized_end=1145,\n)\n\n_BACKENDCONFIG_TFDEEPLABCONFIG = _descriptor.Descriptor(\n  name=\'TFDeeplabConfig\',\n  full_name=\'rv.protos.BackendConfig.TFDeeplabConfig\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'train_py\', full_name=\'rv.protos.BackendConfig.TFDeeplabConfig.train_py\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=True, default_value=_b(""/opt/tf-models/deeplab/train.py"").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'eval_py\', full_name=\'rv.protos.BackendConfig.TFDeeplabConfig.eval_py\', index=1,\n      number=14, type=9, cpp_type=9, label=1,\n      has_default_value=True, default_value=_b(""/opt/tf-models/deeplab/eval.py"").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'export_py\', full_name=\'rv.protos.BackendConfig.TFDeeplabConfig.export_py\', index=2,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=True, default_value=_b(""/opt/tf-models/deeplab/export_model.py"").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'train_restart_dir\', full_name=\'rv.protos.BackendConfig.TFDeeplabConfig.train_restart_dir\', index=3,\n      number=3, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'sync_interval\', full_name=\'rv.protos.BackendConfig.TFDeeplabConfig.sync_interval\', index=4,\n      number=4, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=600,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'do_monitoring\', full_name=\'rv.protos.BackendConfig.TFDeeplabConfig.do_monitoring\', index=5,\n      number=5, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=True,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'do_eval\', full_name=\'rv.protos.BackendConfig.TFDeeplabConfig.do_eval\', index=6,\n      number=13, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'replace_model\', full_name=\'rv.protos.BackendConfig.TFDeeplabConfig.replace_model\', index=7,\n      number=6, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'training_data_uri\', full_name=\'rv.protos.BackendConfig.TFDeeplabConfig.training_data_uri\', index=8,\n      number=7, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'training_output_uri\', full_name=\'rv.protos.BackendConfig.TFDeeplabConfig.training_output_uri\', index=9,\n      number=8, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'model_uri\', full_name=\'rv.protos.BackendConfig.TFDeeplabConfig.model_uri\', index=10,\n      number=9, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'fine_tune_checkpoint_name\', full_name=\'rv.protos.BackendConfig.TFDeeplabConfig.fine_tune_checkpoint_name\', index=11,\n      number=10, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'tfdl_config\', full_name=\'rv.protos.BackendConfig.TFDeeplabConfig.tfdl_config\', index=12,\n      number=11, type=11, cpp_type=10, label=2,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'debug\', full_name=\'rv.protos.BackendConfig.TFDeeplabConfig.debug\', index=13,\n      number=12, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1148,\n  serialized_end=1640,\n)\n\n_BACKENDCONFIG = _descriptor.Descriptor(\n  name=\'BackendConfig\',\n  full_name=\'rv.protos.BackendConfig\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'backend_type\', full_name=\'rv.protos.BackendConfig.backend_type\', index=0,\n      number=1, type=9, cpp_type=9, label=2,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'pretrained_model_uri\', full_name=\'rv.protos.BackendConfig.pretrained_model_uri\', index=1,\n      number=3, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'tf_object_detection_config\', full_name=\'rv.protos.BackendConfig.tf_object_detection_config\', index=2,\n      number=4, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'keras_classification_config\', full_name=\'rv.protos.BackendConfig.keras_classification_config\', index=3,\n      number=5, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'tf_deeplab_config\', full_name=\'rv.protos.BackendConfig.tf_deeplab_config\', index=4,\n      number=7, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'custom_config\', full_name=\'rv.protos.BackendConfig.custom_config\', index=5,\n      number=6, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[_BACKENDCONFIG_TFOBJECTDETECTIONCONFIG, _BACKENDCONFIG_KERASCLASSIFICATIONCONFIG, _BACKENDCONFIG_TFDEEPLABCONFIG, ],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n    _descriptor.OneofDescriptor(\n      name=\'backend_config\', full_name=\'rv.protos.BackendConfig.backend_config\',\n      index=0, containing_type=None, fields=[]),\n  ],\n  serialized_start=79,\n  serialized_end=1658,\n)\n\n_BACKENDCONFIG_TFOBJECTDETECTIONCONFIG.fields_by_name[\'tfod_config\'].message_type = google_dot_protobuf_dot_struct__pb2._STRUCT\n_BACKENDCONFIG_TFOBJECTDETECTIONCONFIG.containing_type = _BACKENDCONFIG\n_BACKENDCONFIG_KERASCLASSIFICATIONCONFIG.fields_by_name[\'kc_config\'].message_type = google_dot_protobuf_dot_struct__pb2._STRUCT\n_BACKENDCONFIG_KERASCLASSIFICATIONCONFIG.containing_type = _BACKENDCONFIG\n_BACKENDCONFIG_TFDEEPLABCONFIG.fields_by_name[\'tfdl_config\'].message_type = google_dot_protobuf_dot_struct__pb2._STRUCT\n_BACKENDCONFIG_TFDEEPLABCONFIG.containing_type = _BACKENDCONFIG\n_BACKENDCONFIG.fields_by_name[\'tf_object_detection_config\'].message_type = _BACKENDCONFIG_TFOBJECTDETECTIONCONFIG\n_BACKENDCONFIG.fields_by_name[\'keras_classification_config\'].message_type = _BACKENDCONFIG_KERASCLASSIFICATIONCONFIG\n_BACKENDCONFIG.fields_by_name[\'tf_deeplab_config\'].message_type = _BACKENDCONFIG_TFDEEPLABCONFIG\n_BACKENDCONFIG.fields_by_name[\'custom_config\'].message_type = google_dot_protobuf_dot_struct__pb2._STRUCT\n_BACKENDCONFIG.oneofs_by_name[\'backend_config\'].fields.append(\n  _BACKENDCONFIG.fields_by_name[\'tf_object_detection_config\'])\n_BACKENDCONFIG.fields_by_name[\'tf_object_detection_config\'].containing_oneof = _BACKENDCONFIG.oneofs_by_name[\'backend_config\']\n_BACKENDCONFIG.oneofs_by_name[\'backend_config\'].fields.append(\n  _BACKENDCONFIG.fields_by_name[\'keras_classification_config\'])\n_BACKENDCONFIG.fields_by_name[\'keras_classification_config\'].containing_oneof = _BACKENDCONFIG.oneofs_by_name[\'backend_config\']\n_BACKENDCONFIG.oneofs_by_name[\'backend_config\'].fields.append(\n  _BACKENDCONFIG.fields_by_name[\'tf_deeplab_config\'])\n_BACKENDCONFIG.fields_by_name[\'tf_deeplab_config\'].containing_oneof = _BACKENDCONFIG.oneofs_by_name[\'backend_config\']\n_BACKENDCONFIG.oneofs_by_name[\'backend_config\'].fields.append(\n  _BACKENDCONFIG.fields_by_name[\'custom_config\'])\n_BACKENDCONFIG.fields_by_name[\'custom_config\'].containing_oneof = _BACKENDCONFIG.oneofs_by_name[\'backend_config\']\nDESCRIPTOR.message_types_by_name[\'BackendConfig\'] = _BACKENDCONFIG\n\nBackendConfig = _reflection.GeneratedProtocolMessageType(\'BackendConfig\', (_message.Message,), dict(\n\n  TFObjectDetectionConfig = _reflection.GeneratedProtocolMessageType(\'TFObjectDetectionConfig\', (_message.Message,), dict(\n    DESCRIPTOR = _BACKENDCONFIG_TFOBJECTDETECTIONCONFIG,\n    __module__ = \'rastervision.protos.backend_pb2\'\n    # @@protoc_insertion_point(class_scope:rv.protos.BackendConfig.TFObjectDetectionConfig)\n    ))\n  ,\n\n  KerasClassificationConfig = _reflection.GeneratedProtocolMessageType(\'KerasClassificationConfig\', (_message.Message,), dict(\n    DESCRIPTOR = _BACKENDCONFIG_KERASCLASSIFICATIONCONFIG,\n    __module__ = \'rastervision.protos.backend_pb2\'\n    # @@protoc_insertion_point(class_scope:rv.protos.BackendConfig.KerasClassificationConfig)\n    ))\n  ,\n\n  TFDeeplabConfig = _reflection.GeneratedProtocolMessageType(\'TFDeeplabConfig\', (_message.Message,), dict(\n    DESCRIPTOR = _BACKENDCONFIG_TFDEEPLABCONFIG,\n    __module__ = \'rastervision.protos.backend_pb2\'\n    # @@protoc_insertion_point(class_scope:rv.protos.BackendConfig.TFDeeplabConfig)\n    ))\n  ,\n  DESCRIPTOR = _BACKENDCONFIG,\n  __module__ = \'rastervision.protos.backend_pb2\'\n  # @@protoc_insertion_point(class_scope:rv.protos.BackendConfig)\n  ))\n_sym_db.RegisterMessage(BackendConfig)\n_sym_db.RegisterMessage(BackendConfig.TFObjectDetectionConfig)\n_sym_db.RegisterMessage(BackendConfig.KerasClassificationConfig)\n_sym_db.RegisterMessage(BackendConfig.TFDeeplabConfig)\n\n\n# @@protoc_insertion_point(module_scope)\n'"
rastervision/protos/class_item_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: rastervision/protos/class_item.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'rastervision/protos/class_item.proto\',\n  package=\'rv.protos\',\n  syntax=\'proto2\',\n  serialized_pb=_b(\'\\n$rastervision/protos/class_item.proto\\x12\\trv.protos\\""4\\n\\tClassItem\\x12\\n\\n\\x02id\\x18\\x01 \\x02(\\x05\\x12\\x0c\\n\\x04name\\x18\\x02 \\x02(\\t\\x12\\r\\n\\x05\\x63olor\\x18\\x03 \\x01(\\t\')\n)\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\n\n\n\n_CLASSITEM = _descriptor.Descriptor(\n  name=\'ClassItem\',\n  full_name=\'rv.protos.ClassItem\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'id\', full_name=\'rv.protos.ClassItem.id\', index=0,\n      number=1, type=5, cpp_type=1, label=2,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'name\', full_name=\'rv.protos.ClassItem.name\', index=1,\n      number=2, type=9, cpp_type=9, label=2,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'color\', full_name=\'rv.protos.ClassItem.color\', index=2,\n      number=3, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=51,\n  serialized_end=103,\n)\n\nDESCRIPTOR.message_types_by_name[\'ClassItem\'] = _CLASSITEM\n\nClassItem = _reflection.GeneratedProtocolMessageType(\'ClassItem\', (_message.Message,), dict(\n  DESCRIPTOR = _CLASSITEM,\n  __module__ = \'rastervision.protos.class_item_pb2\'\n  # @@protoc_insertion_point(class_scope:rv.protos.ClassItem)\n  ))\n_sym_db.RegisterMessage(ClassItem)\n\n\n# @@protoc_insertion_point(module_scope)\n'"
rastervision/protos/command_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: rastervision/protos/command.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom google.protobuf import struct_pb2 as google_dot_protobuf_dot_struct__pb2\nfrom rastervision.protos import analyzer_pb2 as rastervision_dot_protos_dot_analyzer__pb2\nfrom rastervision.protos import augmentor_pb2 as rastervision_dot_protos_dot_augmentor__pb2\nfrom rastervision.protos import task_pb2 as rastervision_dot_protos_dot_task__pb2\nfrom rastervision.protos import backend_pb2 as rastervision_dot_protos_dot_backend__pb2\nfrom rastervision.protos import scene_pb2 as rastervision_dot_protos_dot_scene__pb2\nfrom rastervision.protos import evaluator_pb2 as rastervision_dot_protos_dot_evaluator__pb2\nfrom rastervision.protos import plugin_pb2 as rastervision_dot_protos_dot_plugin__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'rastervision/protos/command.proto\',\n  package=\'rv.protos\',\n  syntax=\'proto2\',\n  serialized_pb=_b(\'\\n!rastervision/protos/command.proto\\x12\\trv.protos\\x1a\\x1cgoogle/protobuf/struct.proto\\x1a\\""rastervision/protos/analyzer.proto\\x1a#rastervision/protos/augmentor.proto\\x1a\\x1erastervision/protos/task.proto\\x1a!rastervision/protos/backend.proto\\x1a\\x1frastervision/protos/scene.proto\\x1a#rastervision/protos/evaluator.proto\\x1a rastervision/protos/plugin.proto\\""\\x80\\x0c\\n\\rCommandConfig\\x12\\x14\\n\\x0c\\x63ommand_type\\x18\\x01 \\x02(\\t\\x12\\x10\\n\\x08root_uri\\x18\\x02 \\x02(\\t\\x12@\\n\\x0e\\x61nalyze_config\\x18\\x03 \\x01(\\x0b\\x32&.rv.protos.CommandConfig.AnalyzeConfigH\\x00\\x12:\\n\\x0b\\x63hip_config\\x18\\x04 \\x01(\\x0b\\x32#.rv.protos.CommandConfig.ChipConfigH\\x00\\x12<\\n\\x0ctrain_config\\x18\\x05 \\x01(\\x0b\\x32$.rv.protos.CommandConfig.TrainConfigH\\x00\\x12@\\n\\x0epredict_config\\x18\\x06 \\x01(\\x0b\\x32&.rv.protos.CommandConfig.PredictConfigH\\x00\\x12:\\n\\x0b\\x65val_config\\x18\\x07 \\x01(\\x0b\\x32#.rv.protos.CommandConfig.EvalConfigH\\x00\\x12>\\n\\rbundle_config\\x18\\x08 \\x01(\\x0b\\x32%.rv.protos.CommandConfig.BundleConfigH\\x00\\x12\\x30\\n\\rcustom_config\\x18\\x0b \\x01(\\x0b\\x32\\x17.google.protobuf.StructH\\x00\\x12(\\n\\x07plugins\\x18\\t \\x01(\\x0b\\x32\\x17.rv.protos.PluginConfig\\x12\\x13\\n\\x08split_id\\x18\\n \\x01(\\x05:\\x01\\x30\\x1a\\x8a\\x01\\n\\rAnalyzeConfig\\x12#\\n\\x04task\\x18\\x01 \\x02(\\x0b\\x32\\x15.rv.protos.TaskConfig\\x12&\\n\\x06scenes\\x18\\x02 \\x03(\\x0b\\x32\\x16.rv.protos.SceneConfig\\x12,\\n\\tanalyzers\\x18\\x03 \\x03(\\x0b\\x32\\x19.rv.protos.AnalyzerConfig\\x1a\\xe6\\x01\\n\\nChipConfig\\x12#\\n\\x04task\\x18\\x01 \\x02(\\x0b\\x32\\x15.rv.protos.TaskConfig\\x12)\\n\\x07\\x62\\x61\\x63kend\\x18\\x02 \\x02(\\x0b\\x32\\x18.rv.protos.BackendConfig\\x12.\\n\\naugmentors\\x18\\x03 \\x03(\\x0b\\x32\\x1a.rv.protos.AugmentorConfig\\x12,\\n\\x0ctrain_scenes\\x18\\x04 \\x03(\\x0b\\x32\\x16.rv.protos.SceneConfig\\x12*\\n\\nval_scenes\\x18\\x05 \\x03(\\x0b\\x32\\x16.rv.protos.SceneConfig\\x1a]\\n\\x0bTrainConfig\\x12#\\n\\x04task\\x18\\x01 \\x02(\\x0b\\x32\\x15.rv.protos.TaskConfig\\x12)\\n\\x07\\x62\\x61\\x63kend\\x18\\x02 \\x02(\\x0b\\x32\\x18.rv.protos.BackendConfig\\x1a\\x87\\x01\\n\\rPredictConfig\\x12#\\n\\x04task\\x18\\x01 \\x02(\\x0b\\x32\\x15.rv.protos.TaskConfig\\x12)\\n\\x07\\x62\\x61\\x63kend\\x18\\x02 \\x02(\\x0b\\x32\\x18.rv.protos.BackendConfig\\x12&\\n\\x06scenes\\x18\\x03 \\x03(\\x0b\\x32\\x16.rv.protos.SceneConfig\\x1a\\xb4\\x01\\n\\nEvalConfig\\x12#\\n\\x04task\\x18\\x01 \\x02(\\x0b\\x32\\x15.rv.protos.TaskConfig\\x12)\\n\\x07\\x62\\x61\\x63kend\\x18\\x02 \\x02(\\x0b\\x32\\x18.rv.protos.BackendConfig\\x12&\\n\\x06scenes\\x18\\x03 \\x03(\\x0b\\x32\\x16.rv.protos.SceneConfig\\x12.\\n\\nevaluators\\x18\\x04 \\x03(\\x0b\\x32\\x1a.rv.protos.EvaluatorConfig\\x1a\\xb3\\x01\\n\\x0c\\x42undleConfig\\x12,\\n\\tanalyzers\\x18\\x01 \\x03(\\x0b\\x32\\x19.rv.protos.AnalyzerConfig\\x12#\\n\\x04task\\x18\\x02 \\x02(\\x0b\\x32\\x15.rv.protos.TaskConfig\\x12)\\n\\x07\\x62\\x61\\x63kend\\x18\\x03 \\x02(\\x0b\\x32\\x18.rv.protos.BackendConfig\\x12%\\n\\x05scene\\x18\\x04 \\x02(\\x0b\\x32\\x16.rv.protos.SceneConfigB\\x10\\n\\x0e\\x63ommand_config\')\n  ,\n  dependencies=[google_dot_protobuf_dot_struct__pb2.DESCRIPTOR,rastervision_dot_protos_dot_analyzer__pb2.DESCRIPTOR,rastervision_dot_protos_dot_augmentor__pb2.DESCRIPTOR,rastervision_dot_protos_dot_task__pb2.DESCRIPTOR,rastervision_dot_protos_dot_backend__pb2.DESCRIPTOR,rastervision_dot_protos_dot_scene__pb2.DESCRIPTOR,rastervision_dot_protos_dot_evaluator__pb2.DESCRIPTOR,rastervision_dot_protos_dot_plugin__pb2.DESCRIPTOR,])\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\n\n\n\n_COMMANDCONFIG_ANALYZECONFIG = _descriptor.Descriptor(\n  name=\'AnalyzeConfig\',\n  full_name=\'rv.protos.CommandConfig.AnalyzeConfig\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'task\', full_name=\'rv.protos.CommandConfig.AnalyzeConfig.task\', index=0,\n      number=1, type=11, cpp_type=10, label=2,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'scenes\', full_name=\'rv.protos.CommandConfig.AnalyzeConfig.scenes\', index=1,\n      number=2, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'analyzers\', full_name=\'rv.protos.CommandConfig.AnalyzeConfig.analyzers\', index=2,\n      number=3, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=872,\n  serialized_end=1010,\n)\n\n_COMMANDCONFIG_CHIPCONFIG = _descriptor.Descriptor(\n  name=\'ChipConfig\',\n  full_name=\'rv.protos.CommandConfig.ChipConfig\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'task\', full_name=\'rv.protos.CommandConfig.ChipConfig.task\', index=0,\n      number=1, type=11, cpp_type=10, label=2,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'backend\', full_name=\'rv.protos.CommandConfig.ChipConfig.backend\', index=1,\n      number=2, type=11, cpp_type=10, label=2,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'augmentors\', full_name=\'rv.protos.CommandConfig.ChipConfig.augmentors\', index=2,\n      number=3, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'train_scenes\', full_name=\'rv.protos.CommandConfig.ChipConfig.train_scenes\', index=3,\n      number=4, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'val_scenes\', full_name=\'rv.protos.CommandConfig.ChipConfig.val_scenes\', index=4,\n      number=5, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1013,\n  serialized_end=1243,\n)\n\n_COMMANDCONFIG_TRAINCONFIG = _descriptor.Descriptor(\n  name=\'TrainConfig\',\n  full_name=\'rv.protos.CommandConfig.TrainConfig\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'task\', full_name=\'rv.protos.CommandConfig.TrainConfig.task\', index=0,\n      number=1, type=11, cpp_type=10, label=2,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'backend\', full_name=\'rv.protos.CommandConfig.TrainConfig.backend\', index=1,\n      number=2, type=11, cpp_type=10, label=2,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1245,\n  serialized_end=1338,\n)\n\n_COMMANDCONFIG_PREDICTCONFIG = _descriptor.Descriptor(\n  name=\'PredictConfig\',\n  full_name=\'rv.protos.CommandConfig.PredictConfig\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'task\', full_name=\'rv.protos.CommandConfig.PredictConfig.task\', index=0,\n      number=1, type=11, cpp_type=10, label=2,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'backend\', full_name=\'rv.protos.CommandConfig.PredictConfig.backend\', index=1,\n      number=2, type=11, cpp_type=10, label=2,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'scenes\', full_name=\'rv.protos.CommandConfig.PredictConfig.scenes\', index=2,\n      number=3, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1341,\n  serialized_end=1476,\n)\n\n_COMMANDCONFIG_EVALCONFIG = _descriptor.Descriptor(\n  name=\'EvalConfig\',\n  full_name=\'rv.protos.CommandConfig.EvalConfig\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'task\', full_name=\'rv.protos.CommandConfig.EvalConfig.task\', index=0,\n      number=1, type=11, cpp_type=10, label=2,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'backend\', full_name=\'rv.protos.CommandConfig.EvalConfig.backend\', index=1,\n      number=2, type=11, cpp_type=10, label=2,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'scenes\', full_name=\'rv.protos.CommandConfig.EvalConfig.scenes\', index=2,\n      number=3, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'evaluators\', full_name=\'rv.protos.CommandConfig.EvalConfig.evaluators\', index=3,\n      number=4, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1479,\n  serialized_end=1659,\n)\n\n_COMMANDCONFIG_BUNDLECONFIG = _descriptor.Descriptor(\n  name=\'BundleConfig\',\n  full_name=\'rv.protos.CommandConfig.BundleConfig\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'analyzers\', full_name=\'rv.protos.CommandConfig.BundleConfig.analyzers\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'task\', full_name=\'rv.protos.CommandConfig.BundleConfig.task\', index=1,\n      number=2, type=11, cpp_type=10, label=2,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'backend\', full_name=\'rv.protos.CommandConfig.BundleConfig.backend\', index=2,\n      number=3, type=11, cpp_type=10, label=2,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'scene\', full_name=\'rv.protos.CommandConfig.BundleConfig.scene\', index=3,\n      number=4, type=11, cpp_type=10, label=2,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1662,\n  serialized_end=1841,\n)\n\n_COMMANDCONFIG = _descriptor.Descriptor(\n  name=\'CommandConfig\',\n  full_name=\'rv.protos.CommandConfig\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'command_type\', full_name=\'rv.protos.CommandConfig.command_type\', index=0,\n      number=1, type=9, cpp_type=9, label=2,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'root_uri\', full_name=\'rv.protos.CommandConfig.root_uri\', index=1,\n      number=2, type=9, cpp_type=9, label=2,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'analyze_config\', full_name=\'rv.protos.CommandConfig.analyze_config\', index=2,\n      number=3, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'chip_config\', full_name=\'rv.protos.CommandConfig.chip_config\', index=3,\n      number=4, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'train_config\', full_name=\'rv.protos.CommandConfig.train_config\', index=4,\n      number=5, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'predict_config\', full_name=\'rv.protos.CommandConfig.predict_config\', index=5,\n      number=6, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'eval_config\', full_name=\'rv.protos.CommandConfig.eval_config\', index=6,\n      number=7, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'bundle_config\', full_name=\'rv.protos.CommandConfig.bundle_config\', index=7,\n      number=8, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'custom_config\', full_name=\'rv.protos.CommandConfig.custom_config\', index=8,\n      number=11, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'plugins\', full_name=\'rv.protos.CommandConfig.plugins\', index=9,\n      number=9, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'split_id\', full_name=\'rv.protos.CommandConfig.split_id\', index=10,\n      number=10, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[_COMMANDCONFIG_ANALYZECONFIG, _COMMANDCONFIG_CHIPCONFIG, _COMMANDCONFIG_TRAINCONFIG, _COMMANDCONFIG_PREDICTCONFIG, _COMMANDCONFIG_EVALCONFIG, _COMMANDCONFIG_BUNDLECONFIG, ],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n    _descriptor.OneofDescriptor(\n      name=\'command_config\', full_name=\'rv.protos.CommandConfig.command_config\',\n      index=0, containing_type=None, fields=[]),\n  ],\n  serialized_start=323,\n  serialized_end=1859,\n)\n\n_COMMANDCONFIG_ANALYZECONFIG.fields_by_name[\'task\'].message_type = rastervision_dot_protos_dot_task__pb2._TASKCONFIG\n_COMMANDCONFIG_ANALYZECONFIG.fields_by_name[\'scenes\'].message_type = rastervision_dot_protos_dot_scene__pb2._SCENECONFIG\n_COMMANDCONFIG_ANALYZECONFIG.fields_by_name[\'analyzers\'].message_type = rastervision_dot_protos_dot_analyzer__pb2._ANALYZERCONFIG\n_COMMANDCONFIG_ANALYZECONFIG.containing_type = _COMMANDCONFIG\n_COMMANDCONFIG_CHIPCONFIG.fields_by_name[\'task\'].message_type = rastervision_dot_protos_dot_task__pb2._TASKCONFIG\n_COMMANDCONFIG_CHIPCONFIG.fields_by_name[\'backend\'].message_type = rastervision_dot_protos_dot_backend__pb2._BACKENDCONFIG\n_COMMANDCONFIG_CHIPCONFIG.fields_by_name[\'augmentors\'].message_type = rastervision_dot_protos_dot_augmentor__pb2._AUGMENTORCONFIG\n_COMMANDCONFIG_CHIPCONFIG.fields_by_name[\'train_scenes\'].message_type = rastervision_dot_protos_dot_scene__pb2._SCENECONFIG\n_COMMANDCONFIG_CHIPCONFIG.fields_by_name[\'val_scenes\'].message_type = rastervision_dot_protos_dot_scene__pb2._SCENECONFIG\n_COMMANDCONFIG_CHIPCONFIG.containing_type = _COMMANDCONFIG\n_COMMANDCONFIG_TRAINCONFIG.fields_by_name[\'task\'].message_type = rastervision_dot_protos_dot_task__pb2._TASKCONFIG\n_COMMANDCONFIG_TRAINCONFIG.fields_by_name[\'backend\'].message_type = rastervision_dot_protos_dot_backend__pb2._BACKENDCONFIG\n_COMMANDCONFIG_TRAINCONFIG.containing_type = _COMMANDCONFIG\n_COMMANDCONFIG_PREDICTCONFIG.fields_by_name[\'task\'].message_type = rastervision_dot_protos_dot_task__pb2._TASKCONFIG\n_COMMANDCONFIG_PREDICTCONFIG.fields_by_name[\'backend\'].message_type = rastervision_dot_protos_dot_backend__pb2._BACKENDCONFIG\n_COMMANDCONFIG_PREDICTCONFIG.fields_by_name[\'scenes\'].message_type = rastervision_dot_protos_dot_scene__pb2._SCENECONFIG\n_COMMANDCONFIG_PREDICTCONFIG.containing_type = _COMMANDCONFIG\n_COMMANDCONFIG_EVALCONFIG.fields_by_name[\'task\'].message_type = rastervision_dot_protos_dot_task__pb2._TASKCONFIG\n_COMMANDCONFIG_EVALCONFIG.fields_by_name[\'backend\'].message_type = rastervision_dot_protos_dot_backend__pb2._BACKENDCONFIG\n_COMMANDCONFIG_EVALCONFIG.fields_by_name[\'scenes\'].message_type = rastervision_dot_protos_dot_scene__pb2._SCENECONFIG\n_COMMANDCONFIG_EVALCONFIG.fields_by_name[\'evaluators\'].message_type = rastervision_dot_protos_dot_evaluator__pb2._EVALUATORCONFIG\n_COMMANDCONFIG_EVALCONFIG.containing_type = _COMMANDCONFIG\n_COMMANDCONFIG_BUNDLECONFIG.fields_by_name[\'analyzers\'].message_type = rastervision_dot_protos_dot_analyzer__pb2._ANALYZERCONFIG\n_COMMANDCONFIG_BUNDLECONFIG.fields_by_name[\'task\'].message_type = rastervision_dot_protos_dot_task__pb2._TASKCONFIG\n_COMMANDCONFIG_BUNDLECONFIG.fields_by_name[\'backend\'].message_type = rastervision_dot_protos_dot_backend__pb2._BACKENDCONFIG\n_COMMANDCONFIG_BUNDLECONFIG.fields_by_name[\'scene\'].message_type = rastervision_dot_protos_dot_scene__pb2._SCENECONFIG\n_COMMANDCONFIG_BUNDLECONFIG.containing_type = _COMMANDCONFIG\n_COMMANDCONFIG.fields_by_name[\'analyze_config\'].message_type = _COMMANDCONFIG_ANALYZECONFIG\n_COMMANDCONFIG.fields_by_name[\'chip_config\'].message_type = _COMMANDCONFIG_CHIPCONFIG\n_COMMANDCONFIG.fields_by_name[\'train_config\'].message_type = _COMMANDCONFIG_TRAINCONFIG\n_COMMANDCONFIG.fields_by_name[\'predict_config\'].message_type = _COMMANDCONFIG_PREDICTCONFIG\n_COMMANDCONFIG.fields_by_name[\'eval_config\'].message_type = _COMMANDCONFIG_EVALCONFIG\n_COMMANDCONFIG.fields_by_name[\'bundle_config\'].message_type = _COMMANDCONFIG_BUNDLECONFIG\n_COMMANDCONFIG.fields_by_name[\'custom_config\'].message_type = google_dot_protobuf_dot_struct__pb2._STRUCT\n_COMMANDCONFIG.fields_by_name[\'plugins\'].message_type = rastervision_dot_protos_dot_plugin__pb2._PLUGINCONFIG\n_COMMANDCONFIG.oneofs_by_name[\'command_config\'].fields.append(\n  _COMMANDCONFIG.fields_by_name[\'analyze_config\'])\n_COMMANDCONFIG.fields_by_name[\'analyze_config\'].containing_oneof = _COMMANDCONFIG.oneofs_by_name[\'command_config\']\n_COMMANDCONFIG.oneofs_by_name[\'command_config\'].fields.append(\n  _COMMANDCONFIG.fields_by_name[\'chip_config\'])\n_COMMANDCONFIG.fields_by_name[\'chip_config\'].containing_oneof = _COMMANDCONFIG.oneofs_by_name[\'command_config\']\n_COMMANDCONFIG.oneofs_by_name[\'command_config\'].fields.append(\n  _COMMANDCONFIG.fields_by_name[\'train_config\'])\n_COMMANDCONFIG.fields_by_name[\'train_config\'].containing_oneof = _COMMANDCONFIG.oneofs_by_name[\'command_config\']\n_COMMANDCONFIG.oneofs_by_name[\'command_config\'].fields.append(\n  _COMMANDCONFIG.fields_by_name[\'predict_config\'])\n_COMMANDCONFIG.fields_by_name[\'predict_config\'].containing_oneof = _COMMANDCONFIG.oneofs_by_name[\'command_config\']\n_COMMANDCONFIG.oneofs_by_name[\'command_config\'].fields.append(\n  _COMMANDCONFIG.fields_by_name[\'eval_config\'])\n_COMMANDCONFIG.fields_by_name[\'eval_config\'].containing_oneof = _COMMANDCONFIG.oneofs_by_name[\'command_config\']\n_COMMANDCONFIG.oneofs_by_name[\'command_config\'].fields.append(\n  _COMMANDCONFIG.fields_by_name[\'bundle_config\'])\n_COMMANDCONFIG.fields_by_name[\'bundle_config\'].containing_oneof = _COMMANDCONFIG.oneofs_by_name[\'command_config\']\n_COMMANDCONFIG.oneofs_by_name[\'command_config\'].fields.append(\n  _COMMANDCONFIG.fields_by_name[\'custom_config\'])\n_COMMANDCONFIG.fields_by_name[\'custom_config\'].containing_oneof = _COMMANDCONFIG.oneofs_by_name[\'command_config\']\nDESCRIPTOR.message_types_by_name[\'CommandConfig\'] = _COMMANDCONFIG\n\nCommandConfig = _reflection.GeneratedProtocolMessageType(\'CommandConfig\', (_message.Message,), dict(\n\n  AnalyzeConfig = _reflection.GeneratedProtocolMessageType(\'AnalyzeConfig\', (_message.Message,), dict(\n    DESCRIPTOR = _COMMANDCONFIG_ANALYZECONFIG,\n    __module__ = \'rastervision.protos.command_pb2\'\n    # @@protoc_insertion_point(class_scope:rv.protos.CommandConfig.AnalyzeConfig)\n    ))\n  ,\n\n  ChipConfig = _reflection.GeneratedProtocolMessageType(\'ChipConfig\', (_message.Message,), dict(\n    DESCRIPTOR = _COMMANDCONFIG_CHIPCONFIG,\n    __module__ = \'rastervision.protos.command_pb2\'\n    # @@protoc_insertion_point(class_scope:rv.protos.CommandConfig.ChipConfig)\n    ))\n  ,\n\n  TrainConfig = _reflection.GeneratedProtocolMessageType(\'TrainConfig\', (_message.Message,), dict(\n    DESCRIPTOR = _COMMANDCONFIG_TRAINCONFIG,\n    __module__ = \'rastervision.protos.command_pb2\'\n    # @@protoc_insertion_point(class_scope:rv.protos.CommandConfig.TrainConfig)\n    ))\n  ,\n\n  PredictConfig = _reflection.GeneratedProtocolMessageType(\'PredictConfig\', (_message.Message,), dict(\n    DESCRIPTOR = _COMMANDCONFIG_PREDICTCONFIG,\n    __module__ = \'rastervision.protos.command_pb2\'\n    # @@protoc_insertion_point(class_scope:rv.protos.CommandConfig.PredictConfig)\n    ))\n  ,\n\n  EvalConfig = _reflection.GeneratedProtocolMessageType(\'EvalConfig\', (_message.Message,), dict(\n    DESCRIPTOR = _COMMANDCONFIG_EVALCONFIG,\n    __module__ = \'rastervision.protos.command_pb2\'\n    # @@protoc_insertion_point(class_scope:rv.protos.CommandConfig.EvalConfig)\n    ))\n  ,\n\n  BundleConfig = _reflection.GeneratedProtocolMessageType(\'BundleConfig\', (_message.Message,), dict(\n    DESCRIPTOR = _COMMANDCONFIG_BUNDLECONFIG,\n    __module__ = \'rastervision.protos.command_pb2\'\n    # @@protoc_insertion_point(class_scope:rv.protos.CommandConfig.BundleConfig)\n    ))\n  ,\n  DESCRIPTOR = _COMMANDCONFIG,\n  __module__ = \'rastervision.protos.command_pb2\'\n  # @@protoc_insertion_point(class_scope:rv.protos.CommandConfig)\n  ))\n_sym_db.RegisterMessage(CommandConfig)\n_sym_db.RegisterMessage(CommandConfig.AnalyzeConfig)\n_sym_db.RegisterMessage(CommandConfig.ChipConfig)\n_sym_db.RegisterMessage(CommandConfig.TrainConfig)\n_sym_db.RegisterMessage(CommandConfig.PredictConfig)\n_sym_db.RegisterMessage(CommandConfig.EvalConfig)\n_sym_db.RegisterMessage(CommandConfig.BundleConfig)\n\n\n# @@protoc_insertion_point(module_scope)\n'"
rastervision/protos/dataset_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: rastervision/protos/dataset.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom rastervision.protos import scene_pb2 as rastervision_dot_protos_dot_scene__pb2\nfrom rastervision.protos import augmentor_pb2 as rastervision_dot_protos_dot_augmentor__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'rastervision/protos/dataset.proto\',\n  package=\'rv.protos\',\n  syntax=\'proto2\',\n  serialized_pb=_b(\'\\n!rastervision/protos/dataset.proto\\x12\\trv.protos\\x1a\\x1frastervision/protos/scene.proto\\x1a#rastervision/protos/augmentor.proto\\""\\xcd\\x01\\n\\rDatasetConfig\\x12,\\n\\x0ctrain_scenes\\x18\\x03 \\x03(\\x0b\\x32\\x16.rv.protos.SceneConfig\\x12\\x31\\n\\x11validation_scenes\\x18\\x04 \\x03(\\x0b\\x32\\x16.rv.protos.SceneConfig\\x12+\\n\\x0btest_scenes\\x18\\x05 \\x03(\\x0b\\x32\\x16.rv.protos.SceneConfig\\x12.\\n\\naugmentors\\x18\\x06 \\x03(\\x0b\\x32\\x1a.rv.protos.AugmentorConfig\')\n  ,\n  dependencies=[rastervision_dot_protos_dot_scene__pb2.DESCRIPTOR,rastervision_dot_protos_dot_augmentor__pb2.DESCRIPTOR,])\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\n\n\n\n_DATASETCONFIG = _descriptor.Descriptor(\n  name=\'DatasetConfig\',\n  full_name=\'rv.protos.DatasetConfig\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'train_scenes\', full_name=\'rv.protos.DatasetConfig.train_scenes\', index=0,\n      number=3, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'validation_scenes\', full_name=\'rv.protos.DatasetConfig.validation_scenes\', index=1,\n      number=4, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'test_scenes\', full_name=\'rv.protos.DatasetConfig.test_scenes\', index=2,\n      number=5, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'augmentors\', full_name=\'rv.protos.DatasetConfig.augmentors\', index=3,\n      number=6, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=119,\n  serialized_end=324,\n)\n\n_DATASETCONFIG.fields_by_name[\'train_scenes\'].message_type = rastervision_dot_protos_dot_scene__pb2._SCENECONFIG\n_DATASETCONFIG.fields_by_name[\'validation_scenes\'].message_type = rastervision_dot_protos_dot_scene__pb2._SCENECONFIG\n_DATASETCONFIG.fields_by_name[\'test_scenes\'].message_type = rastervision_dot_protos_dot_scene__pb2._SCENECONFIG\n_DATASETCONFIG.fields_by_name[\'augmentors\'].message_type = rastervision_dot_protos_dot_augmentor__pb2._AUGMENTORCONFIG\nDESCRIPTOR.message_types_by_name[\'DatasetConfig\'] = _DATASETCONFIG\n\nDatasetConfig = _reflection.GeneratedProtocolMessageType(\'DatasetConfig\', (_message.Message,), dict(\n  DESCRIPTOR = _DATASETCONFIG,\n  __module__ = \'rastervision.protos.dataset_pb2\'\n  # @@protoc_insertion_point(class_scope:rv.protos.DatasetConfig)\n  ))\n_sym_db.RegisterMessage(DatasetConfig)\n\n\n# @@protoc_insertion_point(module_scope)\n'"
rastervision/protos/evaluator_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: rastervision/protos/evaluator.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom rastervision.protos import class_item_pb2 as rastervision_dot_protos_dot_class__item__pb2\nfrom google.protobuf import struct_pb2 as google_dot_protobuf_dot_struct__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'rastervision/protos/evaluator.proto\',\n  package=\'rv.protos\',\n  syntax=\'proto2\',\n  serialized_pb=_b(\'\\n#rastervision/protos/evaluator.proto\\x12\\trv.protos\\x1a$rastervision/protos/class_item.proto\\x1a\\x1cgoogle/protobuf/struct.proto\\""\\xc5\\x02\\n\\x0f\\x45valuatorConfig\\x12\\x16\\n\\x0e\\x65valuator_type\\x18\\x01 \\x02(\\t\\x12Y\\n\\x15\\x63lassification_config\\x18\\x02 \\x01(\\x0b\\x32\\x38.rv.protos.EvaluatorConfig.ClassificationEvaluatorConfigH\\x00\\x12\\x30\\n\\rcustom_config\\x18\\x03 \\x01(\\x0b\\x32\\x17.google.protobuf.StructH\\x00\\x1ay\\n\\x1d\\x43lassificationEvaluatorConfig\\x12\\x12\\n\\noutput_uri\\x18\\x01 \\x02(\\t\\x12\\x19\\n\\x11vector_output_uri\\x18\\x03 \\x01(\\t\\x12)\\n\\x0b\\x63lass_items\\x18\\x02 \\x03(\\x0b\\x32\\x14.rv.protos.ClassItemB\\x12\\n\\x10\\x65valuator_config\')\n  ,\n  dependencies=[rastervision_dot_protos_dot_class__item__pb2.DESCRIPTOR,google_dot_protobuf_dot_struct__pb2.DESCRIPTOR,])\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\n\n\n\n_EVALUATORCONFIG_CLASSIFICATIONEVALUATORCONFIG = _descriptor.Descriptor(\n  name=\'ClassificationEvaluatorConfig\',\n  full_name=\'rv.protos.EvaluatorConfig.ClassificationEvaluatorConfig\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'output_uri\', full_name=\'rv.protos.EvaluatorConfig.ClassificationEvaluatorConfig.output_uri\', index=0,\n      number=1, type=9, cpp_type=9, label=2,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'vector_output_uri\', full_name=\'rv.protos.EvaluatorConfig.ClassificationEvaluatorConfig.vector_output_uri\', index=1,\n      number=3, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'class_items\', full_name=\'rv.protos.EvaluatorConfig.ClassificationEvaluatorConfig.class_items\', index=2,\n      number=2, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=303,\n  serialized_end=424,\n)\n\n_EVALUATORCONFIG = _descriptor.Descriptor(\n  name=\'EvaluatorConfig\',\n  full_name=\'rv.protos.EvaluatorConfig\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'evaluator_type\', full_name=\'rv.protos.EvaluatorConfig.evaluator_type\', index=0,\n      number=1, type=9, cpp_type=9, label=2,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'classification_config\', full_name=\'rv.protos.EvaluatorConfig.classification_config\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'custom_config\', full_name=\'rv.protos.EvaluatorConfig.custom_config\', index=2,\n      number=3, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[_EVALUATORCONFIG_CLASSIFICATIONEVALUATORCONFIG, ],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n    _descriptor.OneofDescriptor(\n      name=\'evaluator_config\', full_name=\'rv.protos.EvaluatorConfig.evaluator_config\',\n      index=0, containing_type=None, fields=[]),\n  ],\n  serialized_start=119,\n  serialized_end=444,\n)\n\n_EVALUATORCONFIG_CLASSIFICATIONEVALUATORCONFIG.fields_by_name[\'class_items\'].message_type = rastervision_dot_protos_dot_class__item__pb2._CLASSITEM\n_EVALUATORCONFIG_CLASSIFICATIONEVALUATORCONFIG.containing_type = _EVALUATORCONFIG\n_EVALUATORCONFIG.fields_by_name[\'classification_config\'].message_type = _EVALUATORCONFIG_CLASSIFICATIONEVALUATORCONFIG\n_EVALUATORCONFIG.fields_by_name[\'custom_config\'].message_type = google_dot_protobuf_dot_struct__pb2._STRUCT\n_EVALUATORCONFIG.oneofs_by_name[\'evaluator_config\'].fields.append(\n  _EVALUATORCONFIG.fields_by_name[\'classification_config\'])\n_EVALUATORCONFIG.fields_by_name[\'classification_config\'].containing_oneof = _EVALUATORCONFIG.oneofs_by_name[\'evaluator_config\']\n_EVALUATORCONFIG.oneofs_by_name[\'evaluator_config\'].fields.append(\n  _EVALUATORCONFIG.fields_by_name[\'custom_config\'])\n_EVALUATORCONFIG.fields_by_name[\'custom_config\'].containing_oneof = _EVALUATORCONFIG.oneofs_by_name[\'evaluator_config\']\nDESCRIPTOR.message_types_by_name[\'EvaluatorConfig\'] = _EVALUATORCONFIG\n\nEvaluatorConfig = _reflection.GeneratedProtocolMessageType(\'EvaluatorConfig\', (_message.Message,), dict(\n\n  ClassificationEvaluatorConfig = _reflection.GeneratedProtocolMessageType(\'ClassificationEvaluatorConfig\', (_message.Message,), dict(\n    DESCRIPTOR = _EVALUATORCONFIG_CLASSIFICATIONEVALUATORCONFIG,\n    __module__ = \'rastervision.protos.evaluator_pb2\'\n    # @@protoc_insertion_point(class_scope:rv.protos.EvaluatorConfig.ClassificationEvaluatorConfig)\n    ))\n  ,\n  DESCRIPTOR = _EVALUATORCONFIG,\n  __module__ = \'rastervision.protos.evaluator_pb2\'\n  # @@protoc_insertion_point(class_scope:rv.protos.EvaluatorConfig)\n  ))\n_sym_db.RegisterMessage(EvaluatorConfig)\n_sym_db.RegisterMessage(EvaluatorConfig.ClassificationEvaluatorConfig)\n\n\n# @@protoc_insertion_point(module_scope)\n'"
rastervision/protos/experiment_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: rastervision/protos/experiment.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom google.protobuf import struct_pb2 as google_dot_protobuf_dot_struct__pb2\nfrom rastervision.protos import dataset_pb2 as rastervision_dot_protos_dot_dataset__pb2\nfrom rastervision.protos import task_pb2 as rastervision_dot_protos_dot_task__pb2\nfrom rastervision.protos import backend_pb2 as rastervision_dot_protos_dot_backend__pb2\nfrom rastervision.protos import analyzer_pb2 as rastervision_dot_protos_dot_analyzer__pb2\nfrom rastervision.protos import evaluator_pb2 as rastervision_dot_protos_dot_evaluator__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'rastervision/protos/experiment.proto\',\n  package=\'rv.protos\',\n  syntax=\'proto2\',\n  serialized_pb=_b(\'\\n$rastervision/protos/experiment.proto\\x12\\trv.protos\\x1a\\x1cgoogle/protobuf/struct.proto\\x1a!rastervision/protos/dataset.proto\\x1a\\x1erastervision/protos/task.proto\\x1a!rastervision/protos/backend.proto\\x1a\\""rastervision/protos/analyzer.proto\\x1a#rastervision/protos/evaluator.proto\\""\\xae\\x03\\n\\x10\\x45xperimentConfig\\x12\\n\\n\\x02id\\x18\\x01 \\x02(\\t\\x12)\\n\\x07\\x64\\x61taset\\x18\\x02 \\x02(\\x0b\\x32\\x18.rv.protos.DatasetConfig\\x12#\\n\\x04task\\x18\\x03 \\x02(\\x0b\\x32\\x15.rv.protos.TaskConfig\\x12)\\n\\x07\\x62\\x61\\x63kend\\x18\\x04 \\x02(\\x0b\\x32\\x18.rv.protos.BackendConfig\\x12,\\n\\tanalyzers\\x18\\x05 \\x03(\\x0b\\x32\\x19.rv.protos.AnalyzerConfig\\x12.\\n\\nevaluators\\x18\\x06 \\x03(\\x0b\\x32\\x1a.rv.protos.EvaluatorConfig\\x12\\x10\\n\\x08root_uri\\x18\\x07 \\x02(\\t\\x12\\x13\\n\\x0b\\x61nalyze_uri\\x18\\x08 \\x02(\\t\\x12\\x10\\n\\x08\\x63hip_uri\\x18\\t \\x02(\\t\\x12\\x11\\n\\ttrain_uri\\x18\\n \\x02(\\t\\x12\\x13\\n\\x0bpredict_uri\\x18\\x0b \\x02(\\t\\x12\\x10\\n\\x08\\x65val_uri\\x18\\x0c \\x02(\\t\\x12\\x12\\n\\nbundle_uri\\x18\\r \\x02(\\t\\x12.\\n\\rcustom_config\\x18\\x0e \\x01(\\x0b\\x32\\x17.google.protobuf.Struct\')\n  ,\n  dependencies=[google_dot_protobuf_dot_struct__pb2.DESCRIPTOR,rastervision_dot_protos_dot_dataset__pb2.DESCRIPTOR,rastervision_dot_protos_dot_task__pb2.DESCRIPTOR,rastervision_dot_protos_dot_backend__pb2.DESCRIPTOR,rastervision_dot_protos_dot_analyzer__pb2.DESCRIPTOR,rastervision_dot_protos_dot_evaluator__pb2.DESCRIPTOR,])\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\n\n\n\n_EXPERIMENTCONFIG = _descriptor.Descriptor(\n  name=\'ExperimentConfig\',\n  full_name=\'rv.protos.ExperimentConfig\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'id\', full_name=\'rv.protos.ExperimentConfig.id\', index=0,\n      number=1, type=9, cpp_type=9, label=2,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'dataset\', full_name=\'rv.protos.ExperimentConfig.dataset\', index=1,\n      number=2, type=11, cpp_type=10, label=2,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'task\', full_name=\'rv.protos.ExperimentConfig.task\', index=2,\n      number=3, type=11, cpp_type=10, label=2,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'backend\', full_name=\'rv.protos.ExperimentConfig.backend\', index=3,\n      number=4, type=11, cpp_type=10, label=2,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'analyzers\', full_name=\'rv.protos.ExperimentConfig.analyzers\', index=4,\n      number=5, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'evaluators\', full_name=\'rv.protos.ExperimentConfig.evaluators\', index=5,\n      number=6, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'root_uri\', full_name=\'rv.protos.ExperimentConfig.root_uri\', index=6,\n      number=7, type=9, cpp_type=9, label=2,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'analyze_uri\', full_name=\'rv.protos.ExperimentConfig.analyze_uri\', index=7,\n      number=8, type=9, cpp_type=9, label=2,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'chip_uri\', full_name=\'rv.protos.ExperimentConfig.chip_uri\', index=8,\n      number=9, type=9, cpp_type=9, label=2,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'train_uri\', full_name=\'rv.protos.ExperimentConfig.train_uri\', index=9,\n      number=10, type=9, cpp_type=9, label=2,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'predict_uri\', full_name=\'rv.protos.ExperimentConfig.predict_uri\', index=10,\n      number=11, type=9, cpp_type=9, label=2,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'eval_uri\', full_name=\'rv.protos.ExperimentConfig.eval_uri\', index=11,\n      number=12, type=9, cpp_type=9, label=2,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'bundle_uri\', full_name=\'rv.protos.ExperimentConfig.bundle_uri\', index=12,\n      number=13, type=9, cpp_type=9, label=2,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'custom_config\', full_name=\'rv.protos.ExperimentConfig.custom_config\', index=13,\n      number=14, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=257,\n  serialized_end=687,\n)\n\n_EXPERIMENTCONFIG.fields_by_name[\'dataset\'].message_type = rastervision_dot_protos_dot_dataset__pb2._DATASETCONFIG\n_EXPERIMENTCONFIG.fields_by_name[\'task\'].message_type = rastervision_dot_protos_dot_task__pb2._TASKCONFIG\n_EXPERIMENTCONFIG.fields_by_name[\'backend\'].message_type = rastervision_dot_protos_dot_backend__pb2._BACKENDCONFIG\n_EXPERIMENTCONFIG.fields_by_name[\'analyzers\'].message_type = rastervision_dot_protos_dot_analyzer__pb2._ANALYZERCONFIG\n_EXPERIMENTCONFIG.fields_by_name[\'evaluators\'].message_type = rastervision_dot_protos_dot_evaluator__pb2._EVALUATORCONFIG\n_EXPERIMENTCONFIG.fields_by_name[\'custom_config\'].message_type = google_dot_protobuf_dot_struct__pb2._STRUCT\nDESCRIPTOR.message_types_by_name[\'ExperimentConfig\'] = _EXPERIMENTCONFIG\n\nExperimentConfig = _reflection.GeneratedProtocolMessageType(\'ExperimentConfig\', (_message.Message,), dict(\n  DESCRIPTOR = _EXPERIMENTCONFIG,\n  __module__ = \'rastervision.protos.experiment_pb2\'\n  # @@protoc_insertion_point(class_scope:rv.protos.ExperimentConfig)\n  ))\n_sym_db.RegisterMessage(ExperimentConfig)\n\n\n# @@protoc_insertion_point(module_scope)\n'"
rastervision/protos/label_source_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: rastervision/protos/label_source.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom rastervision.protos import raster_source_pb2 as rastervision_dot_protos_dot_raster__source__pb2\nfrom google.protobuf import struct_pb2 as google_dot_protobuf_dot_struct__pb2\nfrom rastervision.protos import class_item_pb2 as rastervision_dot_protos_dot_class__item__pb2\nfrom rastervision.protos import vector_source_pb2 as rastervision_dot_protos_dot_vector__source__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'rastervision/protos/label_source.proto\',\n  package=\'rv.protos\',\n  syntax=\'proto2\',\n  serialized_pb=_b(\'\\n&rastervision/protos/label_source.proto\\x12\\trv.protos\\x1a\\\'rastervision/protos/raster_source.proto\\x1a\\x1cgoogle/protobuf/struct.proto\\x1a$rastervision/protos/class_item.proto\\x1a\\\'rastervision/protos/vector_source.proto\\""\\xb3\\x0c\\n\\x11LabelSourceConfig\\x12\\x13\\n\\x0bsource_type\\x18\\x01 \\x02(\\t\\x12\\x64\\n\\x1fobject_detection_geojson_source\\x18\\x02 \\x01(\\x0b\\x32\\x39.rv.protos.LabelSourceConfig.ObjectDetectionGeoJSONSourceH\\x00\\x12j\\n\\""chip_classification_geojson_source\\x18\\x03 \\x01(\\x0b\\x32<.rv.protos.LabelSourceConfig.ChipClassificationGeoJSONSourceH\\x00\\x12l\\n#semantic_segmentation_raster_source\\x18\\x04 \\x01(\\x0b\\x32=.rv.protos.LabelSourceConfig.SemanticSegmentationRasterSourceH\\x00\\x12\\x30\\n\\rcustom_config\\x18\\x05 \\x01(\\x0b\\x32\\x17.google.protobuf.StructH\\x00\\x12`\\n\\x1dobject_detection_label_source\\x18\\x06 \\x01(\\x0b\\x32\\x37.rv.protos.LabelSourceConfig.ObjectDetectionLabelSourceH\\x00\\x12\\x66\\n chip_classification_label_source\\x18\\x07 \\x01(\\x0b\\x32:.rv.protos.LabelSourceConfig.ChipClassificationLabelSourceH\\x00\\x12j\\n\\""semantic_segmentation_label_source\\x18\\x08 \\x01(\\x0b\\x32<.rv.protos.LabelSourceConfig.SemanticSegmentationLabelSourceH\\x00\\x1aR\\n\\x1aObjectDetectionLabelSource\\x12\\x34\\n\\rvector_source\\x18\\x01 \\x02(\\x0b\\x32\\x1d.rv.protos.VectorSourceConfig\\x1a\\xf4\\x01\\n\\x1d\\x43hipClassificationLabelSource\\x12\\x34\\n\\rvector_source\\x18\\x01 \\x02(\\x0b\\x32\\x1d.rv.protos.VectorSourceConfig\\x12\\x12\\n\\nioa_thresh\\x18\\x02 \\x01(\\x02\\x12\\""\\n\\x1ause_intersection_over_cell\\x18\\x03 \\x01(\\x08\\x12\\x19\\n\\x11pick_min_class_id\\x18\\x04 \\x01(\\x08\\x12\\x1b\\n\\x13\\x62\\x61\\x63kground_class_id\\x18\\x05 \\x01(\\x05\\x12\\x11\\n\\tcell_size\\x18\\x06 \\x01(\\x05\\x12\\x1a\\n\\x0binfer_cells\\x18\\x07 \\x01(\\x08:\\x05\\x66\\x61lse\\x1a\\x7f\\n\\x1fSemanticSegmentationLabelSource\\x12-\\n\\x06source\\x18\\x01 \\x02(\\x0b\\x32\\x1d.rv.protos.RasterSourceConfig\\x12-\\n\\x0frgb_class_items\\x18\\x02 \\x03(\\x0b\\x32\\x14.rv.protos.ClassItem\\x1a+\\n\\x1cObjectDetectionGeoJSONSource\\x12\\x0b\\n\\x03uri\\x18\\x01 \\x02(\\t\\x1a\\xcd\\x01\\n\\x1f\\x43hipClassificationGeoJSONSource\\x12\\x0b\\n\\x03uri\\x18\\x01 \\x02(\\t\\x12\\x12\\n\\nioa_thresh\\x18\\x02 \\x01(\\x02\\x12\\""\\n\\x1ause_intersection_over_cell\\x18\\x03 \\x01(\\x08\\x12\\x19\\n\\x11pick_min_class_id\\x18\\x04 \\x01(\\x08\\x12\\x1b\\n\\x13\\x62\\x61\\x63kground_class_id\\x18\\x05 \\x01(\\x05\\x12\\x11\\n\\tcell_size\\x18\\x06 \\x01(\\x05\\x12\\x1a\\n\\x0binfer_cells\\x18\\x07 \\x01(\\x08:\\x05\\x66\\x61lse\\x1a\\x80\\x01\\n SemanticSegmentationRasterSource\\x12-\\n\\x06source\\x18\\x01 \\x02(\\x0b\\x32\\x1d.rv.protos.RasterSourceConfig\\x12-\\n\\x0frgb_class_items\\x18\\x02 \\x03(\\x0b\\x32\\x14.rv.protos.ClassItemB\\x15\\n\\x13label_source_config\')\n  ,\n  dependencies=[rastervision_dot_protos_dot_raster__source__pb2.DESCRIPTOR,google_dot_protobuf_dot_struct__pb2.DESCRIPTOR,rastervision_dot_protos_dot_class__item__pb2.DESCRIPTOR,rastervision_dot_protos_dot_vector__source__pb2.DESCRIPTOR,])\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\n\n\n\n_LABELSOURCECONFIG_OBJECTDETECTIONLABELSOURCE = _descriptor.Descriptor(\n  name=\'ObjectDetectionLabelSource\',\n  full_name=\'rv.protos.LabelSourceConfig.ObjectDetectionLabelSource\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'vector_source\', full_name=\'rv.protos.LabelSourceConfig.ObjectDetectionLabelSource.vector_source\', index=0,\n      number=1, type=11, cpp_type=10, label=2,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=926,\n  serialized_end=1008,\n)\n\n_LABELSOURCECONFIG_CHIPCLASSIFICATIONLABELSOURCE = _descriptor.Descriptor(\n  name=\'ChipClassificationLabelSource\',\n  full_name=\'rv.protos.LabelSourceConfig.ChipClassificationLabelSource\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'vector_source\', full_name=\'rv.protos.LabelSourceConfig.ChipClassificationLabelSource.vector_source\', index=0,\n      number=1, type=11, cpp_type=10, label=2,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'ioa_thresh\', full_name=\'rv.protos.LabelSourceConfig.ChipClassificationLabelSource.ioa_thresh\', index=1,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'use_intersection_over_cell\', full_name=\'rv.protos.LabelSourceConfig.ChipClassificationLabelSource.use_intersection_over_cell\', index=2,\n      number=3, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'pick_min_class_id\', full_name=\'rv.protos.LabelSourceConfig.ChipClassificationLabelSource.pick_min_class_id\', index=3,\n      number=4, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'background_class_id\', full_name=\'rv.protos.LabelSourceConfig.ChipClassificationLabelSource.background_class_id\', index=4,\n      number=5, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'cell_size\', full_name=\'rv.protos.LabelSourceConfig.ChipClassificationLabelSource.cell_size\', index=5,\n      number=6, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'infer_cells\', full_name=\'rv.protos.LabelSourceConfig.ChipClassificationLabelSource.infer_cells\', index=6,\n      number=7, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1011,\n  serialized_end=1255,\n)\n\n_LABELSOURCECONFIG_SEMANTICSEGMENTATIONLABELSOURCE = _descriptor.Descriptor(\n  name=\'SemanticSegmentationLabelSource\',\n  full_name=\'rv.protos.LabelSourceConfig.SemanticSegmentationLabelSource\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'source\', full_name=\'rv.protos.LabelSourceConfig.SemanticSegmentationLabelSource.source\', index=0,\n      number=1, type=11, cpp_type=10, label=2,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'rgb_class_items\', full_name=\'rv.protos.LabelSourceConfig.SemanticSegmentationLabelSource.rgb_class_items\', index=1,\n      number=2, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1257,\n  serialized_end=1384,\n)\n\n_LABELSOURCECONFIG_OBJECTDETECTIONGEOJSONSOURCE = _descriptor.Descriptor(\n  name=\'ObjectDetectionGeoJSONSource\',\n  full_name=\'rv.protos.LabelSourceConfig.ObjectDetectionGeoJSONSource\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'uri\', full_name=\'rv.protos.LabelSourceConfig.ObjectDetectionGeoJSONSource.uri\', index=0,\n      number=1, type=9, cpp_type=9, label=2,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1386,\n  serialized_end=1429,\n)\n\n_LABELSOURCECONFIG_CHIPCLASSIFICATIONGEOJSONSOURCE = _descriptor.Descriptor(\n  name=\'ChipClassificationGeoJSONSource\',\n  full_name=\'rv.protos.LabelSourceConfig.ChipClassificationGeoJSONSource\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'uri\', full_name=\'rv.protos.LabelSourceConfig.ChipClassificationGeoJSONSource.uri\', index=0,\n      number=1, type=9, cpp_type=9, label=2,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'ioa_thresh\', full_name=\'rv.protos.LabelSourceConfig.ChipClassificationGeoJSONSource.ioa_thresh\', index=1,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'use_intersection_over_cell\', full_name=\'rv.protos.LabelSourceConfig.ChipClassificationGeoJSONSource.use_intersection_over_cell\', index=2,\n      number=3, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'pick_min_class_id\', full_name=\'rv.protos.LabelSourceConfig.ChipClassificationGeoJSONSource.pick_min_class_id\', index=3,\n      number=4, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'background_class_id\', full_name=\'rv.protos.LabelSourceConfig.ChipClassificationGeoJSONSource.background_class_id\', index=4,\n      number=5, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'cell_size\', full_name=\'rv.protos.LabelSourceConfig.ChipClassificationGeoJSONSource.cell_size\', index=5,\n      number=6, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'infer_cells\', full_name=\'rv.protos.LabelSourceConfig.ChipClassificationGeoJSONSource.infer_cells\', index=6,\n      number=7, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1432,\n  serialized_end=1637,\n)\n\n_LABELSOURCECONFIG_SEMANTICSEGMENTATIONRASTERSOURCE = _descriptor.Descriptor(\n  name=\'SemanticSegmentationRasterSource\',\n  full_name=\'rv.protos.LabelSourceConfig.SemanticSegmentationRasterSource\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'source\', full_name=\'rv.protos.LabelSourceConfig.SemanticSegmentationRasterSource.source\', index=0,\n      number=1, type=11, cpp_type=10, label=2,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'rgb_class_items\', full_name=\'rv.protos.LabelSourceConfig.SemanticSegmentationRasterSource.rgb_class_items\', index=1,\n      number=2, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1640,\n  serialized_end=1768,\n)\n\n_LABELSOURCECONFIG = _descriptor.Descriptor(\n  name=\'LabelSourceConfig\',\n  full_name=\'rv.protos.LabelSourceConfig\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'source_type\', full_name=\'rv.protos.LabelSourceConfig.source_type\', index=0,\n      number=1, type=9, cpp_type=9, label=2,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'object_detection_geojson_source\', full_name=\'rv.protos.LabelSourceConfig.object_detection_geojson_source\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'chip_classification_geojson_source\', full_name=\'rv.protos.LabelSourceConfig.chip_classification_geojson_source\', index=2,\n      number=3, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'semantic_segmentation_raster_source\', full_name=\'rv.protos.LabelSourceConfig.semantic_segmentation_raster_source\', index=3,\n      number=4, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'custom_config\', full_name=\'rv.protos.LabelSourceConfig.custom_config\', index=4,\n      number=5, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'object_detection_label_source\', full_name=\'rv.protos.LabelSourceConfig.object_detection_label_source\', index=5,\n      number=6, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'chip_classification_label_source\', full_name=\'rv.protos.LabelSourceConfig.chip_classification_label_source\', index=6,\n      number=7, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'semantic_segmentation_label_source\', full_name=\'rv.protos.LabelSourceConfig.semantic_segmentation_label_source\', index=7,\n      number=8, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[_LABELSOURCECONFIG_OBJECTDETECTIONLABELSOURCE, _LABELSOURCECONFIG_CHIPCLASSIFICATIONLABELSOURCE, _LABELSOURCECONFIG_SEMANTICSEGMENTATIONLABELSOURCE, _LABELSOURCECONFIG_OBJECTDETECTIONGEOJSONSOURCE, _LABELSOURCECONFIG_CHIPCLASSIFICATIONGEOJSONSOURCE, _LABELSOURCECONFIG_SEMANTICSEGMENTATIONRASTERSOURCE, ],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n    _descriptor.OneofDescriptor(\n      name=\'label_source_config\', full_name=\'rv.protos.LabelSourceConfig.label_source_config\',\n      index=0, containing_type=None, fields=[]),\n  ],\n  serialized_start=204,\n  serialized_end=1791,\n)\n\n_LABELSOURCECONFIG_OBJECTDETECTIONLABELSOURCE.fields_by_name[\'vector_source\'].message_type = rastervision_dot_protos_dot_vector__source__pb2._VECTORSOURCECONFIG\n_LABELSOURCECONFIG_OBJECTDETECTIONLABELSOURCE.containing_type = _LABELSOURCECONFIG\n_LABELSOURCECONFIG_CHIPCLASSIFICATIONLABELSOURCE.fields_by_name[\'vector_source\'].message_type = rastervision_dot_protos_dot_vector__source__pb2._VECTORSOURCECONFIG\n_LABELSOURCECONFIG_CHIPCLASSIFICATIONLABELSOURCE.containing_type = _LABELSOURCECONFIG\n_LABELSOURCECONFIG_SEMANTICSEGMENTATIONLABELSOURCE.fields_by_name[\'source\'].message_type = rastervision_dot_protos_dot_raster__source__pb2._RASTERSOURCECONFIG\n_LABELSOURCECONFIG_SEMANTICSEGMENTATIONLABELSOURCE.fields_by_name[\'rgb_class_items\'].message_type = rastervision_dot_protos_dot_class__item__pb2._CLASSITEM\n_LABELSOURCECONFIG_SEMANTICSEGMENTATIONLABELSOURCE.containing_type = _LABELSOURCECONFIG\n_LABELSOURCECONFIG_OBJECTDETECTIONGEOJSONSOURCE.containing_type = _LABELSOURCECONFIG\n_LABELSOURCECONFIG_CHIPCLASSIFICATIONGEOJSONSOURCE.containing_type = _LABELSOURCECONFIG\n_LABELSOURCECONFIG_SEMANTICSEGMENTATIONRASTERSOURCE.fields_by_name[\'source\'].message_type = rastervision_dot_protos_dot_raster__source__pb2._RASTERSOURCECONFIG\n_LABELSOURCECONFIG_SEMANTICSEGMENTATIONRASTERSOURCE.fields_by_name[\'rgb_class_items\'].message_type = rastervision_dot_protos_dot_class__item__pb2._CLASSITEM\n_LABELSOURCECONFIG_SEMANTICSEGMENTATIONRASTERSOURCE.containing_type = _LABELSOURCECONFIG\n_LABELSOURCECONFIG.fields_by_name[\'object_detection_geojson_source\'].message_type = _LABELSOURCECONFIG_OBJECTDETECTIONGEOJSONSOURCE\n_LABELSOURCECONFIG.fields_by_name[\'chip_classification_geojson_source\'].message_type = _LABELSOURCECONFIG_CHIPCLASSIFICATIONGEOJSONSOURCE\n_LABELSOURCECONFIG.fields_by_name[\'semantic_segmentation_raster_source\'].message_type = _LABELSOURCECONFIG_SEMANTICSEGMENTATIONRASTERSOURCE\n_LABELSOURCECONFIG.fields_by_name[\'custom_config\'].message_type = google_dot_protobuf_dot_struct__pb2._STRUCT\n_LABELSOURCECONFIG.fields_by_name[\'object_detection_label_source\'].message_type = _LABELSOURCECONFIG_OBJECTDETECTIONLABELSOURCE\n_LABELSOURCECONFIG.fields_by_name[\'chip_classification_label_source\'].message_type = _LABELSOURCECONFIG_CHIPCLASSIFICATIONLABELSOURCE\n_LABELSOURCECONFIG.fields_by_name[\'semantic_segmentation_label_source\'].message_type = _LABELSOURCECONFIG_SEMANTICSEGMENTATIONLABELSOURCE\n_LABELSOURCECONFIG.oneofs_by_name[\'label_source_config\'].fields.append(\n  _LABELSOURCECONFIG.fields_by_name[\'object_detection_geojson_source\'])\n_LABELSOURCECONFIG.fields_by_name[\'object_detection_geojson_source\'].containing_oneof = _LABELSOURCECONFIG.oneofs_by_name[\'label_source_config\']\n_LABELSOURCECONFIG.oneofs_by_name[\'label_source_config\'].fields.append(\n  _LABELSOURCECONFIG.fields_by_name[\'chip_classification_geojson_source\'])\n_LABELSOURCECONFIG.fields_by_name[\'chip_classification_geojson_source\'].containing_oneof = _LABELSOURCECONFIG.oneofs_by_name[\'label_source_config\']\n_LABELSOURCECONFIG.oneofs_by_name[\'label_source_config\'].fields.append(\n  _LABELSOURCECONFIG.fields_by_name[\'semantic_segmentation_raster_source\'])\n_LABELSOURCECONFIG.fields_by_name[\'semantic_segmentation_raster_source\'].containing_oneof = _LABELSOURCECONFIG.oneofs_by_name[\'label_source_config\']\n_LABELSOURCECONFIG.oneofs_by_name[\'label_source_config\'].fields.append(\n  _LABELSOURCECONFIG.fields_by_name[\'custom_config\'])\n_LABELSOURCECONFIG.fields_by_name[\'custom_config\'].containing_oneof = _LABELSOURCECONFIG.oneofs_by_name[\'label_source_config\']\n_LABELSOURCECONFIG.oneofs_by_name[\'label_source_config\'].fields.append(\n  _LABELSOURCECONFIG.fields_by_name[\'object_detection_label_source\'])\n_LABELSOURCECONFIG.fields_by_name[\'object_detection_label_source\'].containing_oneof = _LABELSOURCECONFIG.oneofs_by_name[\'label_source_config\']\n_LABELSOURCECONFIG.oneofs_by_name[\'label_source_config\'].fields.append(\n  _LABELSOURCECONFIG.fields_by_name[\'chip_classification_label_source\'])\n_LABELSOURCECONFIG.fields_by_name[\'chip_classification_label_source\'].containing_oneof = _LABELSOURCECONFIG.oneofs_by_name[\'label_source_config\']\n_LABELSOURCECONFIG.oneofs_by_name[\'label_source_config\'].fields.append(\n  _LABELSOURCECONFIG.fields_by_name[\'semantic_segmentation_label_source\'])\n_LABELSOURCECONFIG.fields_by_name[\'semantic_segmentation_label_source\'].containing_oneof = _LABELSOURCECONFIG.oneofs_by_name[\'label_source_config\']\nDESCRIPTOR.message_types_by_name[\'LabelSourceConfig\'] = _LABELSOURCECONFIG\n\nLabelSourceConfig = _reflection.GeneratedProtocolMessageType(\'LabelSourceConfig\', (_message.Message,), dict(\n\n  ObjectDetectionLabelSource = _reflection.GeneratedProtocolMessageType(\'ObjectDetectionLabelSource\', (_message.Message,), dict(\n    DESCRIPTOR = _LABELSOURCECONFIG_OBJECTDETECTIONLABELSOURCE,\n    __module__ = \'rastervision.protos.label_source_pb2\'\n    # @@protoc_insertion_point(class_scope:rv.protos.LabelSourceConfig.ObjectDetectionLabelSource)\n    ))\n  ,\n\n  ChipClassificationLabelSource = _reflection.GeneratedProtocolMessageType(\'ChipClassificationLabelSource\', (_message.Message,), dict(\n    DESCRIPTOR = _LABELSOURCECONFIG_CHIPCLASSIFICATIONLABELSOURCE,\n    __module__ = \'rastervision.protos.label_source_pb2\'\n    # @@protoc_insertion_point(class_scope:rv.protos.LabelSourceConfig.ChipClassificationLabelSource)\n    ))\n  ,\n\n  SemanticSegmentationLabelSource = _reflection.GeneratedProtocolMessageType(\'SemanticSegmentationLabelSource\', (_message.Message,), dict(\n    DESCRIPTOR = _LABELSOURCECONFIG_SEMANTICSEGMENTATIONLABELSOURCE,\n    __module__ = \'rastervision.protos.label_source_pb2\'\n    # @@protoc_insertion_point(class_scope:rv.protos.LabelSourceConfig.SemanticSegmentationLabelSource)\n    ))\n  ,\n\n  ObjectDetectionGeoJSONSource = _reflection.GeneratedProtocolMessageType(\'ObjectDetectionGeoJSONSource\', (_message.Message,), dict(\n    DESCRIPTOR = _LABELSOURCECONFIG_OBJECTDETECTIONGEOJSONSOURCE,\n    __module__ = \'rastervision.protos.label_source_pb2\'\n    # @@protoc_insertion_point(class_scope:rv.protos.LabelSourceConfig.ObjectDetectionGeoJSONSource)\n    ))\n  ,\n\n  ChipClassificationGeoJSONSource = _reflection.GeneratedProtocolMessageType(\'ChipClassificationGeoJSONSource\', (_message.Message,), dict(\n    DESCRIPTOR = _LABELSOURCECONFIG_CHIPCLASSIFICATIONGEOJSONSOURCE,\n    __module__ = \'rastervision.protos.label_source_pb2\'\n    # @@protoc_insertion_point(class_scope:rv.protos.LabelSourceConfig.ChipClassificationGeoJSONSource)\n    ))\n  ,\n\n  SemanticSegmentationRasterSource = _reflection.GeneratedProtocolMessageType(\'SemanticSegmentationRasterSource\', (_message.Message,), dict(\n    DESCRIPTOR = _LABELSOURCECONFIG_SEMANTICSEGMENTATIONRASTERSOURCE,\n    __module__ = \'rastervision.protos.label_source_pb2\'\n    # @@protoc_insertion_point(class_scope:rv.protos.LabelSourceConfig.SemanticSegmentationRasterSource)\n    ))\n  ,\n  DESCRIPTOR = _LABELSOURCECONFIG,\n  __module__ = \'rastervision.protos.label_source_pb2\'\n  # @@protoc_insertion_point(class_scope:rv.protos.LabelSourceConfig)\n  ))\n_sym_db.RegisterMessage(LabelSourceConfig)\n_sym_db.RegisterMessage(LabelSourceConfig.ObjectDetectionLabelSource)\n_sym_db.RegisterMessage(LabelSourceConfig.ChipClassificationLabelSource)\n_sym_db.RegisterMessage(LabelSourceConfig.SemanticSegmentationLabelSource)\n_sym_db.RegisterMessage(LabelSourceConfig.ObjectDetectionGeoJSONSource)\n_sym_db.RegisterMessage(LabelSourceConfig.ChipClassificationGeoJSONSource)\n_sym_db.RegisterMessage(LabelSourceConfig.SemanticSegmentationRasterSource)\n\n\n# @@protoc_insertion_point(module_scope)\n'"
rastervision/protos/label_store_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: rastervision/protos/label_store.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom google.protobuf import struct_pb2 as google_dot_protobuf_dot_struct__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'rastervision/protos/label_store.proto\',\n  package=\'rv.protos\',\n  syntax=\'proto2\',\n  serialized_pb=_b(\'\\n%rastervision/protos/label_store.proto\\x12\\trv.protos\\x1a\\x1cgoogle/protobuf/struct.proto\\""\\xd5\\x05\\n\\x10LabelStoreConfig\\x12\\x12\\n\\nstore_type\\x18\\x01 \\x02(\\t\\x12\\r\\n\\x03uri\\x18\\x02 \\x01(\\tH\\x00\\x12i\\n\\""semantic_segmentation_raster_store\\x18\\x03 \\x01(\\x0b\\x32;.rv.protos.LabelStoreConfig.SemanticSegmentationRasterStoreH\\x00\\x12\\x30\\n\\rcustom_config\\x18\\x04 \\x01(\\x0b\\x32\\x17.google.protobuf.StructH\\x00\\x1a\\xea\\x03\\n\\x1fSemanticSegmentationRasterStore\\x12\\x0b\\n\\x03uri\\x18\\x01 \\x01(\\t\\x12\\x12\\n\\x03rgb\\x18\\x02 \\x01(\\x08:\\x05\\x66\\x61lse\\x12_\\n\\rvector_output\\x18\\x03 \\x03(\\x0b\\x32H.rv.protos.LabelStoreConfig.SemanticSegmentationRasterStore.VectorOutput\\x1a\\x89\\x01\\n\\x0f\\x42uildingOptions\\x12\\x1f\\n\\x10min_aspect_ratio\\x18\\x01 \\x01(\\x02:\\x05\\x31.618\\x12\\x10\\n\\x08min_area\\x18\\x02 \\x01(\\x02\\x12!\\n\\x14\\x65lement_width_factor\\x18\\x03 \\x01(\\x02:\\x03\\x30.5\\x12 \\n\\x11\\x65lement_thickness\\x18\\x04 \\x01(\\x02:\\x05\\x30.001\\x1a\\xb8\\x01\\n\\x0cVectorOutput\\x12\\x12\\n\\x07\\x64\\x65noise\\x18\\x01 \\x01(\\x05:\\x01\\x30\\x12\\r\\n\\x03uri\\x18\\x02 \\x01(\\t:\\x00\\x12\\x0c\\n\\x04mode\\x18\\x03 \\x02(\\t\\x12\\x10\\n\\x08\\x63lass_id\\x18\\x04 \\x02(\\x05\\x12\\x65\\n\\x10\\x62uilding_options\\x18\\x05 \\x01(\\x0b\\x32K.rv.protos.LabelStoreConfig.SemanticSegmentationRasterStore.BuildingOptionsB\\x14\\n\\x12label_store_config\')\n  ,\n  dependencies=[google_dot_protobuf_dot_struct__pb2.DESCRIPTOR,])\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\n\n\n\n_LABELSTORECONFIG_SEMANTICSEGMENTATIONRASTERSTORE_BUILDINGOPTIONS = _descriptor.Descriptor(\n  name=\'BuildingOptions\',\n  full_name=\'rv.protos.LabelStoreConfig.SemanticSegmentationRasterStore.BuildingOptions\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'min_aspect_ratio\', full_name=\'rv.protos.LabelStoreConfig.SemanticSegmentationRasterStore.BuildingOptions.min_aspect_ratio\', index=0,\n      number=1, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(1.618),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'min_area\', full_name=\'rv.protos.LabelStoreConfig.SemanticSegmentationRasterStore.BuildingOptions.min_area\', index=1,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'element_width_factor\', full_name=\'rv.protos.LabelStoreConfig.SemanticSegmentationRasterStore.BuildingOptions.element_width_factor\', index=2,\n      number=3, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.5),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'element_thickness\', full_name=\'rv.protos.LabelStoreConfig.SemanticSegmentationRasterStore.BuildingOptions.element_thickness\', index=3,\n      number=4, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.001),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=462,\n  serialized_end=599,\n)\n\n_LABELSTORECONFIG_SEMANTICSEGMENTATIONRASTERSTORE_VECTOROUTPUT = _descriptor.Descriptor(\n  name=\'VectorOutput\',\n  full_name=\'rv.protos.LabelStoreConfig.SemanticSegmentationRasterStore.VectorOutput\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'denoise\', full_name=\'rv.protos.LabelStoreConfig.SemanticSegmentationRasterStore.VectorOutput.denoise\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'uri\', full_name=\'rv.protos.LabelStoreConfig.SemanticSegmentationRasterStore.VectorOutput.uri\', index=1,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=True, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'mode\', full_name=\'rv.protos.LabelStoreConfig.SemanticSegmentationRasterStore.VectorOutput.mode\', index=2,\n      number=3, type=9, cpp_type=9, label=2,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'class_id\', full_name=\'rv.protos.LabelStoreConfig.SemanticSegmentationRasterStore.VectorOutput.class_id\', index=3,\n      number=4, type=5, cpp_type=1, label=2,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'building_options\', full_name=\'rv.protos.LabelStoreConfig.SemanticSegmentationRasterStore.VectorOutput.building_options\', index=4,\n      number=5, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=602,\n  serialized_end=786,\n)\n\n_LABELSTORECONFIG_SEMANTICSEGMENTATIONRASTERSTORE = _descriptor.Descriptor(\n  name=\'SemanticSegmentationRasterStore\',\n  full_name=\'rv.protos.LabelStoreConfig.SemanticSegmentationRasterStore\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'uri\', full_name=\'rv.protos.LabelStoreConfig.SemanticSegmentationRasterStore.uri\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'rgb\', full_name=\'rv.protos.LabelStoreConfig.SemanticSegmentationRasterStore.rgb\', index=1,\n      number=2, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'vector_output\', full_name=\'rv.protos.LabelStoreConfig.SemanticSegmentationRasterStore.vector_output\', index=2,\n      number=3, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[_LABELSTORECONFIG_SEMANTICSEGMENTATIONRASTERSTORE_BUILDINGOPTIONS, _LABELSTORECONFIG_SEMANTICSEGMENTATIONRASTERSTORE_VECTOROUTPUT, ],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=296,\n  serialized_end=786,\n)\n\n_LABELSTORECONFIG = _descriptor.Descriptor(\n  name=\'LabelStoreConfig\',\n  full_name=\'rv.protos.LabelStoreConfig\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'store_type\', full_name=\'rv.protos.LabelStoreConfig.store_type\', index=0,\n      number=1, type=9, cpp_type=9, label=2,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'uri\', full_name=\'rv.protos.LabelStoreConfig.uri\', index=1,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'semantic_segmentation_raster_store\', full_name=\'rv.protos.LabelStoreConfig.semantic_segmentation_raster_store\', index=2,\n      number=3, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'custom_config\', full_name=\'rv.protos.LabelStoreConfig.custom_config\', index=3,\n      number=4, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[_LABELSTORECONFIG_SEMANTICSEGMENTATIONRASTERSTORE, ],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n    _descriptor.OneofDescriptor(\n      name=\'label_store_config\', full_name=\'rv.protos.LabelStoreConfig.label_store_config\',\n      index=0, containing_type=None, fields=[]),\n  ],\n  serialized_start=83,\n  serialized_end=808,\n)\n\n_LABELSTORECONFIG_SEMANTICSEGMENTATIONRASTERSTORE_BUILDINGOPTIONS.containing_type = _LABELSTORECONFIG_SEMANTICSEGMENTATIONRASTERSTORE\n_LABELSTORECONFIG_SEMANTICSEGMENTATIONRASTERSTORE_VECTOROUTPUT.fields_by_name[\'building_options\'].message_type = _LABELSTORECONFIG_SEMANTICSEGMENTATIONRASTERSTORE_BUILDINGOPTIONS\n_LABELSTORECONFIG_SEMANTICSEGMENTATIONRASTERSTORE_VECTOROUTPUT.containing_type = _LABELSTORECONFIG_SEMANTICSEGMENTATIONRASTERSTORE\n_LABELSTORECONFIG_SEMANTICSEGMENTATIONRASTERSTORE.fields_by_name[\'vector_output\'].message_type = _LABELSTORECONFIG_SEMANTICSEGMENTATIONRASTERSTORE_VECTOROUTPUT\n_LABELSTORECONFIG_SEMANTICSEGMENTATIONRASTERSTORE.containing_type = _LABELSTORECONFIG\n_LABELSTORECONFIG.fields_by_name[\'semantic_segmentation_raster_store\'].message_type = _LABELSTORECONFIG_SEMANTICSEGMENTATIONRASTERSTORE\n_LABELSTORECONFIG.fields_by_name[\'custom_config\'].message_type = google_dot_protobuf_dot_struct__pb2._STRUCT\n_LABELSTORECONFIG.oneofs_by_name[\'label_store_config\'].fields.append(\n  _LABELSTORECONFIG.fields_by_name[\'uri\'])\n_LABELSTORECONFIG.fields_by_name[\'uri\'].containing_oneof = _LABELSTORECONFIG.oneofs_by_name[\'label_store_config\']\n_LABELSTORECONFIG.oneofs_by_name[\'label_store_config\'].fields.append(\n  _LABELSTORECONFIG.fields_by_name[\'semantic_segmentation_raster_store\'])\n_LABELSTORECONFIG.fields_by_name[\'semantic_segmentation_raster_store\'].containing_oneof = _LABELSTORECONFIG.oneofs_by_name[\'label_store_config\']\n_LABELSTORECONFIG.oneofs_by_name[\'label_store_config\'].fields.append(\n  _LABELSTORECONFIG.fields_by_name[\'custom_config\'])\n_LABELSTORECONFIG.fields_by_name[\'custom_config\'].containing_oneof = _LABELSTORECONFIG.oneofs_by_name[\'label_store_config\']\nDESCRIPTOR.message_types_by_name[\'LabelStoreConfig\'] = _LABELSTORECONFIG\n\nLabelStoreConfig = _reflection.GeneratedProtocolMessageType(\'LabelStoreConfig\', (_message.Message,), dict(\n\n  SemanticSegmentationRasterStore = _reflection.GeneratedProtocolMessageType(\'SemanticSegmentationRasterStore\', (_message.Message,), dict(\n\n    BuildingOptions = _reflection.GeneratedProtocolMessageType(\'BuildingOptions\', (_message.Message,), dict(\n      DESCRIPTOR = _LABELSTORECONFIG_SEMANTICSEGMENTATIONRASTERSTORE_BUILDINGOPTIONS,\n      __module__ = \'rastervision.protos.label_store_pb2\'\n      # @@protoc_insertion_point(class_scope:rv.protos.LabelStoreConfig.SemanticSegmentationRasterStore.BuildingOptions)\n      ))\n    ,\n\n    VectorOutput = _reflection.GeneratedProtocolMessageType(\'VectorOutput\', (_message.Message,), dict(\n      DESCRIPTOR = _LABELSTORECONFIG_SEMANTICSEGMENTATIONRASTERSTORE_VECTOROUTPUT,\n      __module__ = \'rastervision.protos.label_store_pb2\'\n      # @@protoc_insertion_point(class_scope:rv.protos.LabelStoreConfig.SemanticSegmentationRasterStore.VectorOutput)\n      ))\n    ,\n    DESCRIPTOR = _LABELSTORECONFIG_SEMANTICSEGMENTATIONRASTERSTORE,\n    __module__ = \'rastervision.protos.label_store_pb2\'\n    # @@protoc_insertion_point(class_scope:rv.protos.LabelStoreConfig.SemanticSegmentationRasterStore)\n    ))\n  ,\n  DESCRIPTOR = _LABELSTORECONFIG,\n  __module__ = \'rastervision.protos.label_store_pb2\'\n  # @@protoc_insertion_point(class_scope:rv.protos.LabelStoreConfig)\n  ))\n_sym_db.RegisterMessage(LabelStoreConfig)\n_sym_db.RegisterMessage(LabelStoreConfig.SemanticSegmentationRasterStore)\n_sym_db.RegisterMessage(LabelStoreConfig.SemanticSegmentationRasterStore.BuildingOptions)\n_sym_db.RegisterMessage(LabelStoreConfig.SemanticSegmentationRasterStore.VectorOutput)\n\n\n# @@protoc_insertion_point(module_scope)\n'"
rastervision/protos/plugin_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: rastervision/protos/plugin.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'rastervision/protos/plugin.proto\',\n  package=\'rv.protos\',\n  syntax=\'proto2\',\n  serialized_pb=_b(\'\\n rastervision/protos/plugin.proto\\x12\\trv.protos\\"";\\n\\x0cPluginConfig\\x12\\x13\\n\\x0bplugin_uris\\x18\\x01 \\x03(\\t\\x12\\x16\\n\\x0eplugin_modules\\x18\\x02 \\x03(\\t\')\n)\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\n\n\n\n_PLUGINCONFIG = _descriptor.Descriptor(\n  name=\'PluginConfig\',\n  full_name=\'rv.protos.PluginConfig\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'plugin_uris\', full_name=\'rv.protos.PluginConfig.plugin_uris\', index=0,\n      number=1, type=9, cpp_type=9, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'plugin_modules\', full_name=\'rv.protos.PluginConfig.plugin_modules\', index=1,\n      number=2, type=9, cpp_type=9, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=47,\n  serialized_end=106,\n)\n\nDESCRIPTOR.message_types_by_name[\'PluginConfig\'] = _PLUGINCONFIG\n\nPluginConfig = _reflection.GeneratedProtocolMessageType(\'PluginConfig\', (_message.Message,), dict(\n  DESCRIPTOR = _PLUGINCONFIG,\n  __module__ = \'rastervision.protos.plugin_pb2\'\n  # @@protoc_insertion_point(class_scope:rv.protos.PluginConfig)\n  ))\n_sym_db.RegisterMessage(PluginConfig)\n\n\n# @@protoc_insertion_point(module_scope)\n'"
rastervision/protos/raster_source_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: rastervision/protos/raster_source.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom google.protobuf import struct_pb2 as google_dot_protobuf_dot_struct__pb2\nfrom rastervision.protos import raster_transformer_pb2 as rastervision_dot_protos_dot_raster__transformer__pb2\nfrom rastervision.protos import vector_source_pb2 as rastervision_dot_protos_dot_vector__source__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'rastervision/protos/raster_source.proto\',\n  package=\'rv.protos\',\n  syntax=\'proto2\',\n  serialized_pb=_b(\'\\n\\\'rastervision/protos/raster_source.proto\\x12\\trv.protos\\x1a\\x1cgoogle/protobuf/struct.proto\\x1a,rastervision/protos/raster_transformer.proto\\x1a\\\'rastervision/protos/vector_source.proto\\""\\xaa\\t\\n\\x12RasterSourceConfig\\x12\\x13\\n\\x0bsource_type\\x18\\x01 \\x02(\\t\\x12\\x38\\n\\x0ctransformers\\x18\\x02 \\x03(\\x0b\\x32\\"".rv.protos.RasterTransformerConfig\\x12\\x15\\n\\rchannel_order\\x18\\x03 \\x03(\\x05\\x12\\x43\\n\\rgeotiff_files\\x18\\x04 \\x01(\\x0b\\x32*.rv.protos.RasterSourceConfig.GeoTiffFilesH\\x00\\x12=\\n\\nimage_file\\x18\\x05 \\x01(\\x0b\\x32\\\'.rv.protos.RasterSourceConfig.ImageFileH\\x00\\x12\\x41\\n\\x0cgeojson_file\\x18\\x06 \\x01(\\x0b\\x32).rv.protos.RasterSourceConfig.GeoJSONFileH\\x00\\x12\\x30\\n\\rcustom_config\\x18\\x07 \\x01(\\x0b\\x32\\x17.google.protobuf.StructH\\x00\\x12K\\n\\x11rasterized_source\\x18\\x08 \\x01(\\x0b\\x32..rv.protos.RasterSourceConfig.RasterizedSourceH\\x00\\x12G\\n\\x0frasterio_source\\x18\\t \\x01(\\x0b\\x32,.rv.protos.RasterSourceConfig.RasterioSourceH\\x00\\x1aL\\n\\x0cGeoTiffFiles\\x12\\x0c\\n\\x04uris\\x18\\x01 \\x03(\\t\\x12\\x16\\n\\x0ex_shift_meters\\x18\\x02 \\x01(\\x02\\x12\\x16\\n\\x0ey_shift_meters\\x18\\x03 \\x01(\\x02\\x1a\\x18\\n\\tImageFile\\x12\\x0b\\n\\x03uri\\x18\\x01 \\x02(\\t\\x1aN\\n\\x0eRasterioSource\\x12\\x0c\\n\\x04uris\\x18\\x01 \\x03(\\t\\x12\\x16\\n\\x0ex_shift_meters\\x18\\x02 \\x01(\\x02\\x12\\x16\\n\\x0ey_shift_meters\\x18\\x03 \\x01(\\x02\\x1a\\x8d\\x02\\n\\x10RasterizedSource\\x12\\x34\\n\\rvector_source\\x18\\x01 \\x02(\\x0b\\x32\\x1d.rv.protos.VectorSourceConfig\\x12\\\\\\n\\x12rasterizer_options\\x18\\x02 \\x02(\\x0b\\x32@.rv.protos.RasterSourceConfig.RasterizedSource.RasterizerOptions\\x1a\\x65\\n\\x11RasterizerOptions\\x12\\x1b\\n\\x13\\x62\\x61\\x63kground_class_id\\x18\\x02 \\x02(\\x05\\x12\\x17\\n\\x0bline_buffer\\x18\\x03 \\x01(\\x05:\\x02\\x31\\x35\\x12\\x1a\\n\\x0b\\x61ll_touched\\x18\\x04 \\x01(\\x08:\\x05\\x66\\x61lse\\x1a\\xbe\\x01\\n\\x0bGeoJSONFile\\x12\\x0b\\n\\x03uri\\x18\\x01 \\x02(\\t\\x12W\\n\\x12rasterizer_options\\x18\\x02 \\x02(\\x0b\\x32;.rv.protos.RasterSourceConfig.GeoJSONFile.RasterizerOptions\\x1aI\\n\\x11RasterizerOptions\\x12\\x1b\\n\\x13\\x62\\x61\\x63kground_class_id\\x18\\x02 \\x02(\\x05\\x12\\x17\\n\\x0bline_buffer\\x18\\x03 \\x01(\\x05:\\x02\\x31\\x35\\x42\\x16\\n\\x14raster_source_config\')\n  ,\n  dependencies=[google_dot_protobuf_dot_struct__pb2.DESCRIPTOR,rastervision_dot_protos_dot_raster__transformer__pb2.DESCRIPTOR,rastervision_dot_protos_dot_vector__source__pb2.DESCRIPTOR,])\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\n\n\n\n_RASTERSOURCECONFIG_GEOTIFFFILES = _descriptor.Descriptor(\n  name=\'GeoTiffFiles\',\n  full_name=\'rv.protos.RasterSourceConfig.GeoTiffFiles\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'uris\', full_name=\'rv.protos.RasterSourceConfig.GeoTiffFiles.uris\', index=0,\n      number=1, type=9, cpp_type=9, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'x_shift_meters\', full_name=\'rv.protos.RasterSourceConfig.GeoTiffFiles.x_shift_meters\', index=1,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'y_shift_meters\', full_name=\'rv.protos.RasterSourceConfig.GeoTiffFiles.y_shift_meters\', index=2,\n      number=3, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=695,\n  serialized_end=771,\n)\n\n_RASTERSOURCECONFIG_IMAGEFILE = _descriptor.Descriptor(\n  name=\'ImageFile\',\n  full_name=\'rv.protos.RasterSourceConfig.ImageFile\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'uri\', full_name=\'rv.protos.RasterSourceConfig.ImageFile.uri\', index=0,\n      number=1, type=9, cpp_type=9, label=2,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=773,\n  serialized_end=797,\n)\n\n_RASTERSOURCECONFIG_RASTERIOSOURCE = _descriptor.Descriptor(\n  name=\'RasterioSource\',\n  full_name=\'rv.protos.RasterSourceConfig.RasterioSource\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'uris\', full_name=\'rv.protos.RasterSourceConfig.RasterioSource.uris\', index=0,\n      number=1, type=9, cpp_type=9, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'x_shift_meters\', full_name=\'rv.protos.RasterSourceConfig.RasterioSource.x_shift_meters\', index=1,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'y_shift_meters\', full_name=\'rv.protos.RasterSourceConfig.RasterioSource.y_shift_meters\', index=2,\n      number=3, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=799,\n  serialized_end=877,\n)\n\n_RASTERSOURCECONFIG_RASTERIZEDSOURCE_RASTERIZEROPTIONS = _descriptor.Descriptor(\n  name=\'RasterizerOptions\',\n  full_name=\'rv.protos.RasterSourceConfig.RasterizedSource.RasterizerOptions\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'background_class_id\', full_name=\'rv.protos.RasterSourceConfig.RasterizedSource.RasterizerOptions.background_class_id\', index=0,\n      number=2, type=5, cpp_type=1, label=2,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'line_buffer\', full_name=\'rv.protos.RasterSourceConfig.RasterizedSource.RasterizerOptions.line_buffer\', index=1,\n      number=3, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=15,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'all_touched\', full_name=\'rv.protos.RasterSourceConfig.RasterizedSource.RasterizerOptions.all_touched\', index=2,\n      number=4, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1048,\n  serialized_end=1149,\n)\n\n_RASTERSOURCECONFIG_RASTERIZEDSOURCE = _descriptor.Descriptor(\n  name=\'RasterizedSource\',\n  full_name=\'rv.protos.RasterSourceConfig.RasterizedSource\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'vector_source\', full_name=\'rv.protos.RasterSourceConfig.RasterizedSource.vector_source\', index=0,\n      number=1, type=11, cpp_type=10, label=2,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'rasterizer_options\', full_name=\'rv.protos.RasterSourceConfig.RasterizedSource.rasterizer_options\', index=1,\n      number=2, type=11, cpp_type=10, label=2,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[_RASTERSOURCECONFIG_RASTERIZEDSOURCE_RASTERIZEROPTIONS, ],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=880,\n  serialized_end=1149,\n)\n\n_RASTERSOURCECONFIG_GEOJSONFILE_RASTERIZEROPTIONS = _descriptor.Descriptor(\n  name=\'RasterizerOptions\',\n  full_name=\'rv.protos.RasterSourceConfig.GeoJSONFile.RasterizerOptions\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'background_class_id\', full_name=\'rv.protos.RasterSourceConfig.GeoJSONFile.RasterizerOptions.background_class_id\', index=0,\n      number=2, type=5, cpp_type=1, label=2,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'line_buffer\', full_name=\'rv.protos.RasterSourceConfig.GeoJSONFile.RasterizerOptions.line_buffer\', index=1,\n      number=3, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=15,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1048,\n  serialized_end=1121,\n)\n\n_RASTERSOURCECONFIG_GEOJSONFILE = _descriptor.Descriptor(\n  name=\'GeoJSONFile\',\n  full_name=\'rv.protos.RasterSourceConfig.GeoJSONFile\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'uri\', full_name=\'rv.protos.RasterSourceConfig.GeoJSONFile.uri\', index=0,\n      number=1, type=9, cpp_type=9, label=2,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'rasterizer_options\', full_name=\'rv.protos.RasterSourceConfig.GeoJSONFile.rasterizer_options\', index=1,\n      number=2, type=11, cpp_type=10, label=2,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[_RASTERSOURCECONFIG_GEOJSONFILE_RASTERIZEROPTIONS, ],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1152,\n  serialized_end=1342,\n)\n\n_RASTERSOURCECONFIG = _descriptor.Descriptor(\n  name=\'RasterSourceConfig\',\n  full_name=\'rv.protos.RasterSourceConfig\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'source_type\', full_name=\'rv.protos.RasterSourceConfig.source_type\', index=0,\n      number=1, type=9, cpp_type=9, label=2,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'transformers\', full_name=\'rv.protos.RasterSourceConfig.transformers\', index=1,\n      number=2, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'channel_order\', full_name=\'rv.protos.RasterSourceConfig.channel_order\', index=2,\n      number=3, type=5, cpp_type=1, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'geotiff_files\', full_name=\'rv.protos.RasterSourceConfig.geotiff_files\', index=3,\n      number=4, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'image_file\', full_name=\'rv.protos.RasterSourceConfig.image_file\', index=4,\n      number=5, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'geojson_file\', full_name=\'rv.protos.RasterSourceConfig.geojson_file\', index=5,\n      number=6, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'custom_config\', full_name=\'rv.protos.RasterSourceConfig.custom_config\', index=6,\n      number=7, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'rasterized_source\', full_name=\'rv.protos.RasterSourceConfig.rasterized_source\', index=7,\n      number=8, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'rasterio_source\', full_name=\'rv.protos.RasterSourceConfig.rasterio_source\', index=8,\n      number=9, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[_RASTERSOURCECONFIG_GEOTIFFFILES, _RASTERSOURCECONFIG_IMAGEFILE, _RASTERSOURCECONFIG_RASTERIOSOURCE, _RASTERSOURCECONFIG_RASTERIZEDSOURCE, _RASTERSOURCECONFIG_GEOJSONFILE, ],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n    _descriptor.OneofDescriptor(\n      name=\'raster_source_config\', full_name=\'rv.protos.RasterSourceConfig.raster_source_config\',\n      index=0, containing_type=None, fields=[]),\n  ],\n  serialized_start=172,\n  serialized_end=1366,\n)\n\n_RASTERSOURCECONFIG_GEOTIFFFILES.containing_type = _RASTERSOURCECONFIG\n_RASTERSOURCECONFIG_IMAGEFILE.containing_type = _RASTERSOURCECONFIG\n_RASTERSOURCECONFIG_RASTERIOSOURCE.containing_type = _RASTERSOURCECONFIG\n_RASTERSOURCECONFIG_RASTERIZEDSOURCE_RASTERIZEROPTIONS.containing_type = _RASTERSOURCECONFIG_RASTERIZEDSOURCE\n_RASTERSOURCECONFIG_RASTERIZEDSOURCE.fields_by_name[\'vector_source\'].message_type = rastervision_dot_protos_dot_vector__source__pb2._VECTORSOURCECONFIG\n_RASTERSOURCECONFIG_RASTERIZEDSOURCE.fields_by_name[\'rasterizer_options\'].message_type = _RASTERSOURCECONFIG_RASTERIZEDSOURCE_RASTERIZEROPTIONS\n_RASTERSOURCECONFIG_RASTERIZEDSOURCE.containing_type = _RASTERSOURCECONFIG\n_RASTERSOURCECONFIG_GEOJSONFILE_RASTERIZEROPTIONS.containing_type = _RASTERSOURCECONFIG_GEOJSONFILE\n_RASTERSOURCECONFIG_GEOJSONFILE.fields_by_name[\'rasterizer_options\'].message_type = _RASTERSOURCECONFIG_GEOJSONFILE_RASTERIZEROPTIONS\n_RASTERSOURCECONFIG_GEOJSONFILE.containing_type = _RASTERSOURCECONFIG\n_RASTERSOURCECONFIG.fields_by_name[\'transformers\'].message_type = rastervision_dot_protos_dot_raster__transformer__pb2._RASTERTRANSFORMERCONFIG\n_RASTERSOURCECONFIG.fields_by_name[\'geotiff_files\'].message_type = _RASTERSOURCECONFIG_GEOTIFFFILES\n_RASTERSOURCECONFIG.fields_by_name[\'image_file\'].message_type = _RASTERSOURCECONFIG_IMAGEFILE\n_RASTERSOURCECONFIG.fields_by_name[\'geojson_file\'].message_type = _RASTERSOURCECONFIG_GEOJSONFILE\n_RASTERSOURCECONFIG.fields_by_name[\'custom_config\'].message_type = google_dot_protobuf_dot_struct__pb2._STRUCT\n_RASTERSOURCECONFIG.fields_by_name[\'rasterized_source\'].message_type = _RASTERSOURCECONFIG_RASTERIZEDSOURCE\n_RASTERSOURCECONFIG.fields_by_name[\'rasterio_source\'].message_type = _RASTERSOURCECONFIG_RASTERIOSOURCE\n_RASTERSOURCECONFIG.oneofs_by_name[\'raster_source_config\'].fields.append(\n  _RASTERSOURCECONFIG.fields_by_name[\'geotiff_files\'])\n_RASTERSOURCECONFIG.fields_by_name[\'geotiff_files\'].containing_oneof = _RASTERSOURCECONFIG.oneofs_by_name[\'raster_source_config\']\n_RASTERSOURCECONFIG.oneofs_by_name[\'raster_source_config\'].fields.append(\n  _RASTERSOURCECONFIG.fields_by_name[\'image_file\'])\n_RASTERSOURCECONFIG.fields_by_name[\'image_file\'].containing_oneof = _RASTERSOURCECONFIG.oneofs_by_name[\'raster_source_config\']\n_RASTERSOURCECONFIG.oneofs_by_name[\'raster_source_config\'].fields.append(\n  _RASTERSOURCECONFIG.fields_by_name[\'geojson_file\'])\n_RASTERSOURCECONFIG.fields_by_name[\'geojson_file\'].containing_oneof = _RASTERSOURCECONFIG.oneofs_by_name[\'raster_source_config\']\n_RASTERSOURCECONFIG.oneofs_by_name[\'raster_source_config\'].fields.append(\n  _RASTERSOURCECONFIG.fields_by_name[\'custom_config\'])\n_RASTERSOURCECONFIG.fields_by_name[\'custom_config\'].containing_oneof = _RASTERSOURCECONFIG.oneofs_by_name[\'raster_source_config\']\n_RASTERSOURCECONFIG.oneofs_by_name[\'raster_source_config\'].fields.append(\n  _RASTERSOURCECONFIG.fields_by_name[\'rasterized_source\'])\n_RASTERSOURCECONFIG.fields_by_name[\'rasterized_source\'].containing_oneof = _RASTERSOURCECONFIG.oneofs_by_name[\'raster_source_config\']\n_RASTERSOURCECONFIG.oneofs_by_name[\'raster_source_config\'].fields.append(\n  _RASTERSOURCECONFIG.fields_by_name[\'rasterio_source\'])\n_RASTERSOURCECONFIG.fields_by_name[\'rasterio_source\'].containing_oneof = _RASTERSOURCECONFIG.oneofs_by_name[\'raster_source_config\']\nDESCRIPTOR.message_types_by_name[\'RasterSourceConfig\'] = _RASTERSOURCECONFIG\n\nRasterSourceConfig = _reflection.GeneratedProtocolMessageType(\'RasterSourceConfig\', (_message.Message,), dict(\n\n  GeoTiffFiles = _reflection.GeneratedProtocolMessageType(\'GeoTiffFiles\', (_message.Message,), dict(\n    DESCRIPTOR = _RASTERSOURCECONFIG_GEOTIFFFILES,\n    __module__ = \'rastervision.protos.raster_source_pb2\'\n    # @@protoc_insertion_point(class_scope:rv.protos.RasterSourceConfig.GeoTiffFiles)\n    ))\n  ,\n\n  ImageFile = _reflection.GeneratedProtocolMessageType(\'ImageFile\', (_message.Message,), dict(\n    DESCRIPTOR = _RASTERSOURCECONFIG_IMAGEFILE,\n    __module__ = \'rastervision.protos.raster_source_pb2\'\n    # @@protoc_insertion_point(class_scope:rv.protos.RasterSourceConfig.ImageFile)\n    ))\n  ,\n\n  RasterioSource = _reflection.GeneratedProtocolMessageType(\'RasterioSource\', (_message.Message,), dict(\n    DESCRIPTOR = _RASTERSOURCECONFIG_RASTERIOSOURCE,\n    __module__ = \'rastervision.protos.raster_source_pb2\'\n    # @@protoc_insertion_point(class_scope:rv.protos.RasterSourceConfig.RasterioSource)\n    ))\n  ,\n\n  RasterizedSource = _reflection.GeneratedProtocolMessageType(\'RasterizedSource\', (_message.Message,), dict(\n\n    RasterizerOptions = _reflection.GeneratedProtocolMessageType(\'RasterizerOptions\', (_message.Message,), dict(\n      DESCRIPTOR = _RASTERSOURCECONFIG_RASTERIZEDSOURCE_RASTERIZEROPTIONS,\n      __module__ = \'rastervision.protos.raster_source_pb2\'\n      # @@protoc_insertion_point(class_scope:rv.protos.RasterSourceConfig.RasterizedSource.RasterizerOptions)\n      ))\n    ,\n    DESCRIPTOR = _RASTERSOURCECONFIG_RASTERIZEDSOURCE,\n    __module__ = \'rastervision.protos.raster_source_pb2\'\n    # @@protoc_insertion_point(class_scope:rv.protos.RasterSourceConfig.RasterizedSource)\n    ))\n  ,\n\n  GeoJSONFile = _reflection.GeneratedProtocolMessageType(\'GeoJSONFile\', (_message.Message,), dict(\n\n    RasterizerOptions = _reflection.GeneratedProtocolMessageType(\'RasterizerOptions\', (_message.Message,), dict(\n      DESCRIPTOR = _RASTERSOURCECONFIG_GEOJSONFILE_RASTERIZEROPTIONS,\n      __module__ = \'rastervision.protos.raster_source_pb2\'\n      # @@protoc_insertion_point(class_scope:rv.protos.RasterSourceConfig.GeoJSONFile.RasterizerOptions)\n      ))\n    ,\n    DESCRIPTOR = _RASTERSOURCECONFIG_GEOJSONFILE,\n    __module__ = \'rastervision.protos.raster_source_pb2\'\n    # @@protoc_insertion_point(class_scope:rv.protos.RasterSourceConfig.GeoJSONFile)\n    ))\n  ,\n  DESCRIPTOR = _RASTERSOURCECONFIG,\n  __module__ = \'rastervision.protos.raster_source_pb2\'\n  # @@protoc_insertion_point(class_scope:rv.protos.RasterSourceConfig)\n  ))\n_sym_db.RegisterMessage(RasterSourceConfig)\n_sym_db.RegisterMessage(RasterSourceConfig.GeoTiffFiles)\n_sym_db.RegisterMessage(RasterSourceConfig.ImageFile)\n_sym_db.RegisterMessage(RasterSourceConfig.RasterioSource)\n_sym_db.RegisterMessage(RasterSourceConfig.RasterizedSource)\n_sym_db.RegisterMessage(RasterSourceConfig.RasterizedSource.RasterizerOptions)\n_sym_db.RegisterMessage(RasterSourceConfig.GeoJSONFile)\n_sym_db.RegisterMessage(RasterSourceConfig.GeoJSONFile.RasterizerOptions)\n\n\n# @@protoc_insertion_point(module_scope)\n'"
rastervision/protos/raster_transformer_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: rastervision/protos/raster_transformer.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom google.protobuf import struct_pb2 as google_dot_protobuf_dot_struct__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'rastervision/protos/raster_transformer.proto\',\n  package=\'rv.protos\',\n  syntax=\'proto2\',\n  serialized_pb=_b(\'\\n,rastervision/protos/raster_transformer.proto\\x12\\trv.protos\\x1a\\x1cgoogle/protobuf/struct.proto\\""\\x97\\x01\\n\\x17RasterTransformerConfig\\x12\\x18\\n\\x10transformer_type\\x18\\x01 \\x02(\\t\\x12\\x13\\n\\tstats_uri\\x18\\x04 \\x01(\\tH\\x00\\x12\\x30\\n\\rcustom_config\\x18\\x05 \\x01(\\x0b\\x32\\x17.google.protobuf.StructH\\x00\\x42\\x1b\\n\\x19raster_transformer_config\')\n  ,\n  dependencies=[google_dot_protobuf_dot_struct__pb2.DESCRIPTOR,])\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\n\n\n\n_RASTERTRANSFORMERCONFIG = _descriptor.Descriptor(\n  name=\'RasterTransformerConfig\',\n  full_name=\'rv.protos.RasterTransformerConfig\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'transformer_type\', full_name=\'rv.protos.RasterTransformerConfig.transformer_type\', index=0,\n      number=1, type=9, cpp_type=9, label=2,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'stats_uri\', full_name=\'rv.protos.RasterTransformerConfig.stats_uri\', index=1,\n      number=4, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'custom_config\', full_name=\'rv.protos.RasterTransformerConfig.custom_config\', index=2,\n      number=5, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n    _descriptor.OneofDescriptor(\n      name=\'raster_transformer_config\', full_name=\'rv.protos.RasterTransformerConfig.raster_transformer_config\',\n      index=0, containing_type=None, fields=[]),\n  ],\n  serialized_start=90,\n  serialized_end=241,\n)\n\n_RASTERTRANSFORMERCONFIG.fields_by_name[\'custom_config\'].message_type = google_dot_protobuf_dot_struct__pb2._STRUCT\n_RASTERTRANSFORMERCONFIG.oneofs_by_name[\'raster_transformer_config\'].fields.append(\n  _RASTERTRANSFORMERCONFIG.fields_by_name[\'stats_uri\'])\n_RASTERTRANSFORMERCONFIG.fields_by_name[\'stats_uri\'].containing_oneof = _RASTERTRANSFORMERCONFIG.oneofs_by_name[\'raster_transformer_config\']\n_RASTERTRANSFORMERCONFIG.oneofs_by_name[\'raster_transformer_config\'].fields.append(\n  _RASTERTRANSFORMERCONFIG.fields_by_name[\'custom_config\'])\n_RASTERTRANSFORMERCONFIG.fields_by_name[\'custom_config\'].containing_oneof = _RASTERTRANSFORMERCONFIG.oneofs_by_name[\'raster_transformer_config\']\nDESCRIPTOR.message_types_by_name[\'RasterTransformerConfig\'] = _RASTERTRANSFORMERCONFIG\n\nRasterTransformerConfig = _reflection.GeneratedProtocolMessageType(\'RasterTransformerConfig\', (_message.Message,), dict(\n  DESCRIPTOR = _RASTERTRANSFORMERCONFIG,\n  __module__ = \'rastervision.protos.raster_transformer_pb2\'\n  # @@protoc_insertion_point(class_scope:rv.protos.RasterTransformerConfig)\n  ))\n_sym_db.RegisterMessage(RasterTransformerConfig)\n\n\n# @@protoc_insertion_point(module_scope)\n'"
rastervision/protos/scene_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: rastervision/protos/scene.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom rastervision.protos import raster_source_pb2 as rastervision_dot_protos_dot_raster__source__pb2\nfrom rastervision.protos import label_source_pb2 as rastervision_dot_protos_dot_label__source__pb2\nfrom rastervision.protos import label_store_pb2 as rastervision_dot_protos_dot_label__store__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'rastervision/protos/scene.proto\',\n  package=\'rv.protos\',\n  syntax=\'proto2\',\n  serialized_pb=_b(\'\\n\\x1frastervision/protos/scene.proto\\x12\\trv.protos\\x1a\\\'rastervision/protos/raster_source.proto\\x1a&rastervision/protos/label_source.proto\\x1a%rastervision/protos/label_store.proto\\""\\xf0\\x01\\n\\x0bSceneConfig\\x12\\n\\n\\x02id\\x18\\x01 \\x02(\\t\\x12\\x34\\n\\rraster_source\\x18\\x02 \\x02(\\x0b\\x32\\x1d.rv.protos.RasterSourceConfig\\x12?\\n\\x19ground_truth_label_source\\x18\\x03 \\x01(\\x0b\\x32\\x1c.rv.protos.LabelSourceConfig\\x12;\\n\\x16prediction_label_store\\x18\\x04 \\x01(\\x0b\\x32\\x1b.rv.protos.LabelStoreConfig\\x12\\x0f\\n\\x07\\x61oi_uri\\x18\\x05 \\x01(\\t\\x12\\x10\\n\\x08\\x61oi_uris\\x18\\x06 \\x03(\\t\')\n  ,\n  dependencies=[rastervision_dot_protos_dot_raster__source__pb2.DESCRIPTOR,rastervision_dot_protos_dot_label__source__pb2.DESCRIPTOR,rastervision_dot_protos_dot_label__store__pb2.DESCRIPTOR,])\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\n\n\n\n_SCENECONFIG = _descriptor.Descriptor(\n  name=\'SceneConfig\',\n  full_name=\'rv.protos.SceneConfig\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'id\', full_name=\'rv.protos.SceneConfig.id\', index=0,\n      number=1, type=9, cpp_type=9, label=2,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'raster_source\', full_name=\'rv.protos.SceneConfig.raster_source\', index=1,\n      number=2, type=11, cpp_type=10, label=2,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'ground_truth_label_source\', full_name=\'rv.protos.SceneConfig.ground_truth_label_source\', index=2,\n      number=3, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'prediction_label_store\', full_name=\'rv.protos.SceneConfig.prediction_label_store\', index=3,\n      number=4, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'aoi_uri\', full_name=\'rv.protos.SceneConfig.aoi_uri\', index=4,\n      number=5, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'aoi_uris\', full_name=\'rv.protos.SceneConfig.aoi_uris\', index=5,\n      number=6, type=9, cpp_type=9, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=167,\n  serialized_end=407,\n)\n\n_SCENECONFIG.fields_by_name[\'raster_source\'].message_type = rastervision_dot_protos_dot_raster__source__pb2._RASTERSOURCECONFIG\n_SCENECONFIG.fields_by_name[\'ground_truth_label_source\'].message_type = rastervision_dot_protos_dot_label__source__pb2._LABELSOURCECONFIG\n_SCENECONFIG.fields_by_name[\'prediction_label_store\'].message_type = rastervision_dot_protos_dot_label__store__pb2._LABELSTORECONFIG\nDESCRIPTOR.message_types_by_name[\'SceneConfig\'] = _SCENECONFIG\n\nSceneConfig = _reflection.GeneratedProtocolMessageType(\'SceneConfig\', (_message.Message,), dict(\n  DESCRIPTOR = _SCENECONFIG,\n  __module__ = \'rastervision.protos.scene_pb2\'\n  # @@protoc_insertion_point(class_scope:rv.protos.SceneConfig)\n  ))\n_sym_db.RegisterMessage(SceneConfig)\n\n\n# @@protoc_insertion_point(module_scope)\n'"
rastervision/protos/task_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: rastervision/protos/task.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom rastervision.protos import class_item_pb2 as rastervision_dot_protos_dot_class__item__pb2\nfrom google.protobuf import struct_pb2 as google_dot_protobuf_dot_struct__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'rastervision/protos/task.proto\',\n  package=\'rv.protos\',\n  syntax=\'proto2\',\n  serialized_pb=_b(\'\\n\\x1erastervision/protos/task.proto\\x12\\trv.protos\\x1a$rastervision/protos/class_item.proto\\x1a\\x1cgoogle/protobuf/struct.proto\\""\\x9e\\x0b\\n\\nTaskConfig\\x12\\x11\\n\\ttask_type\\x18\\x01 \\x02(\\t\\x12\\x1e\\n\\x12predict_batch_size\\x18\\x02 \\x01(\\x05:\\x02\\x31\\x30\\x12\\x1b\\n\\x13predict_package_uri\\x18\\x03 \\x01(\\t\\x12\\x13\\n\\x05\\x64\\x65\\x62ug\\x18\\x04 \\x01(\\x08:\\x04true\\x12\\x19\\n\\x11predict_debug_uri\\x18\\x05 \\x01(\\t\\x12N\\n\\x17object_detection_config\\x18\\x06 \\x01(\\x0b\\x32+.rv.protos.TaskConfig.ObjectDetectionConfigH\\x00\\x12T\\n\\x1a\\x63hip_classification_config\\x18\\x07 \\x01(\\x0b\\x32..rv.protos.TaskConfig.ChipClassificationConfigH\\x00\\x12X\\n\\x1csemantic_segmentation_config\\x18\\x08 \\x01(\\x0b\\x32\\x30.rv.protos.TaskConfig.SemanticSegmentationConfigH\\x00\\x12\\x30\\n\\rcustom_config\\x18\\t \\x01(\\x0b\\x32\\x17.google.protobuf.StructH\\x00\\x1a\\xb2\\x03\\n\\x15ObjectDetectionConfig\\x12)\\n\\x0b\\x63lass_items\\x18\\x01 \\x03(\\x0b\\x32\\x14.rv.protos.ClassItem\\x12\\x11\\n\\tchip_size\\x18\\x02 \\x02(\\x05\\x12M\\n\\x0c\\x63hip_options\\x18\\x03 \\x02(\\x0b\\x32\\x37.rv.protos.TaskConfig.ObjectDetectionConfig.ChipOptions\\x12S\\n\\x0fpredict_options\\x18\\x04 \\x02(\\x0b\\x32:.rv.protos.TaskConfig.ObjectDetectionConfig.PredictOptions\\x1ao\\n\\x0b\\x43hipOptions\\x12\\x11\\n\\tneg_ratio\\x18\\x01 \\x02(\\x02\\x12\\x17\\n\\nioa_thresh\\x18\\x02 \\x01(\\x02:\\x03\\x30.8\\x12\\x1b\\n\\rwindow_method\\x18\\x03 \\x01(\\t:\\x04\\x63hip\\x12\\x17\\n\\x0clabel_buffer\\x18\\x04 \\x01(\\x02:\\x01\\x30\\x1a\\x46\\n\\x0ePredictOptions\\x12\\x19\\n\\x0cmerge_thresh\\x18\\x02 \\x01(\\x02:\\x03\\x30.5\\x12\\x19\\n\\x0cscore_thresh\\x18\\x03 \\x01(\\x02:\\x03\\x30.5\\x1aX\\n\\x18\\x43hipClassificationConfig\\x12)\\n\\x0b\\x63lass_items\\x18\\x01 \\x03(\\x0b\\x32\\x14.rv.protos.ClassItem\\x12\\x11\\n\\tchip_size\\x18\\x02 \\x02(\\x05\\x1a\\xbf\\x03\\n\\x1aSemanticSegmentationConfig\\x12)\\n\\x0b\\x63lass_items\\x18\\x01 \\x03(\\x0b\\x32\\x14.rv.protos.ClassItem\\x12\\x11\\n\\tchip_size\\x18\\x02 \\x02(\\x05\\x12R\\n\\x0c\\x63hip_options\\x18\\x03 \\x02(\\x0b\\x32<.rv.protos.TaskConfig.SemanticSegmentationConfig.ChipOptions\\x12\\x1c\\n\\x11predict_chip_size\\x18\\x04 \\x01(\\x05:\\x01\\x30\\x1a\\xf0\\x01\\n\\x0b\\x43hipOptions\\x12$\\n\\rwindow_method\\x18\\x01 \\x01(\\t:\\rrandom_sample\\x12\\x16\\n\\x0etarget_classes\\x18\\x02 \\x03(\\x05\\x12$\\n\\x16\\x64\\x65\\x62ug_chip_probability\\x18\\x03 \\x01(\\x02:\\x04\\x30.25\\x12(\\n\\x1dnegative_survival_probability\\x18\\x04 \\x01(\\x02:\\x01\\x31\\x12\\x1d\\n\\x0f\\x63hips_per_scene\\x18\\x05 \\x01(\\x05:\\x04\\x31\\x30\\x30\\x30\\x12$\\n\\x16target_count_threshold\\x18\\x06 \\x01(\\x05:\\x04\\x32\\x30\\x34\\x38\\x12\\x0e\\n\\x06stride\\x18\\x07 \\x01(\\x05\\x42\\r\\n\\x0b\\x63onfig_type\')\n  ,\n  dependencies=[rastervision_dot_protos_dot_class__item__pb2.DESCRIPTOR,google_dot_protobuf_dot_struct__pb2.DESCRIPTOR,])\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\n\n\n\n_TASKCONFIG_OBJECTDETECTIONCONFIG_CHIPOPTIONS = _descriptor.Descriptor(\n  name=\'ChipOptions\',\n  full_name=\'rv.protos.TaskConfig.ObjectDetectionConfig.ChipOptions\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'neg_ratio\', full_name=\'rv.protos.TaskConfig.ObjectDetectionConfig.ChipOptions.neg_ratio\', index=0,\n      number=1, type=2, cpp_type=6, label=2,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'ioa_thresh\', full_name=\'rv.protos.TaskConfig.ObjectDetectionConfig.ChipOptions.ioa_thresh\', index=1,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.8),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'window_method\', full_name=\'rv.protos.TaskConfig.ObjectDetectionConfig.ChipOptions.window_method\', index=2,\n      number=3, type=9, cpp_type=9, label=1,\n      has_default_value=True, default_value=_b(""chip"").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'label_buffer\', full_name=\'rv.protos.TaskConfig.ObjectDetectionConfig.ChipOptions.label_buffer\', index=3,\n      number=4, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=814,\n  serialized_end=925,\n)\n\n_TASKCONFIG_OBJECTDETECTIONCONFIG_PREDICTOPTIONS = _descriptor.Descriptor(\n  name=\'PredictOptions\',\n  full_name=\'rv.protos.TaskConfig.ObjectDetectionConfig.PredictOptions\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'merge_thresh\', full_name=\'rv.protos.TaskConfig.ObjectDetectionConfig.PredictOptions.merge_thresh\', index=0,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.5),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'score_thresh\', full_name=\'rv.protos.TaskConfig.ObjectDetectionConfig.PredictOptions.score_thresh\', index=1,\n      number=3, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.5),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=927,\n  serialized_end=997,\n)\n\n_TASKCONFIG_OBJECTDETECTIONCONFIG = _descriptor.Descriptor(\n  name=\'ObjectDetectionConfig\',\n  full_name=\'rv.protos.TaskConfig.ObjectDetectionConfig\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'class_items\', full_name=\'rv.protos.TaskConfig.ObjectDetectionConfig.class_items\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'chip_size\', full_name=\'rv.protos.TaskConfig.ObjectDetectionConfig.chip_size\', index=1,\n      number=2, type=5, cpp_type=1, label=2,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'chip_options\', full_name=\'rv.protos.TaskConfig.ObjectDetectionConfig.chip_options\', index=2,\n      number=3, type=11, cpp_type=10, label=2,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'predict_options\', full_name=\'rv.protos.TaskConfig.ObjectDetectionConfig.predict_options\', index=3,\n      number=4, type=11, cpp_type=10, label=2,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[_TASKCONFIG_OBJECTDETECTIONCONFIG_CHIPOPTIONS, _TASKCONFIG_OBJECTDETECTIONCONFIG_PREDICTOPTIONS, ],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=563,\n  serialized_end=997,\n)\n\n_TASKCONFIG_CHIPCLASSIFICATIONCONFIG = _descriptor.Descriptor(\n  name=\'ChipClassificationConfig\',\n  full_name=\'rv.protos.TaskConfig.ChipClassificationConfig\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'class_items\', full_name=\'rv.protos.TaskConfig.ChipClassificationConfig.class_items\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'chip_size\', full_name=\'rv.protos.TaskConfig.ChipClassificationConfig.chip_size\', index=1,\n      number=2, type=5, cpp_type=1, label=2,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=999,\n  serialized_end=1087,\n)\n\n_TASKCONFIG_SEMANTICSEGMENTATIONCONFIG_CHIPOPTIONS = _descriptor.Descriptor(\n  name=\'ChipOptions\',\n  full_name=\'rv.protos.TaskConfig.SemanticSegmentationConfig.ChipOptions\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'window_method\', full_name=\'rv.protos.TaskConfig.SemanticSegmentationConfig.ChipOptions.window_method\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=True, default_value=_b(""random_sample"").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'target_classes\', full_name=\'rv.protos.TaskConfig.SemanticSegmentationConfig.ChipOptions.target_classes\', index=1,\n      number=2, type=5, cpp_type=1, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'debug_chip_probability\', full_name=\'rv.protos.TaskConfig.SemanticSegmentationConfig.ChipOptions.debug_chip_probability\', index=2,\n      number=3, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.25),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'negative_survival_probability\', full_name=\'rv.protos.TaskConfig.SemanticSegmentationConfig.ChipOptions.negative_survival_probability\', index=3,\n      number=4, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(1),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'chips_per_scene\', full_name=\'rv.protos.TaskConfig.SemanticSegmentationConfig.ChipOptions.chips_per_scene\', index=4,\n      number=5, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=1000,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'target_count_threshold\', full_name=\'rv.protos.TaskConfig.SemanticSegmentationConfig.ChipOptions.target_count_threshold\', index=5,\n      number=6, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=2048,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'stride\', full_name=\'rv.protos.TaskConfig.SemanticSegmentationConfig.ChipOptions.stride\', index=6,\n      number=7, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1297,\n  serialized_end=1537,\n)\n\n_TASKCONFIG_SEMANTICSEGMENTATIONCONFIG = _descriptor.Descriptor(\n  name=\'SemanticSegmentationConfig\',\n  full_name=\'rv.protos.TaskConfig.SemanticSegmentationConfig\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'class_items\', full_name=\'rv.protos.TaskConfig.SemanticSegmentationConfig.class_items\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'chip_size\', full_name=\'rv.protos.TaskConfig.SemanticSegmentationConfig.chip_size\', index=1,\n      number=2, type=5, cpp_type=1, label=2,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'chip_options\', full_name=\'rv.protos.TaskConfig.SemanticSegmentationConfig.chip_options\', index=2,\n      number=3, type=11, cpp_type=10, label=2,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'predict_chip_size\', full_name=\'rv.protos.TaskConfig.SemanticSegmentationConfig.predict_chip_size\', index=3,\n      number=4, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[_TASKCONFIG_SEMANTICSEGMENTATIONCONFIG_CHIPOPTIONS, ],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1090,\n  serialized_end=1537,\n)\n\n_TASKCONFIG = _descriptor.Descriptor(\n  name=\'TaskConfig\',\n  full_name=\'rv.protos.TaskConfig\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'task_type\', full_name=\'rv.protos.TaskConfig.task_type\', index=0,\n      number=1, type=9, cpp_type=9, label=2,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'predict_batch_size\', full_name=\'rv.protos.TaskConfig.predict_batch_size\', index=1,\n      number=2, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=10,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'predict_package_uri\', full_name=\'rv.protos.TaskConfig.predict_package_uri\', index=2,\n      number=3, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'debug\', full_name=\'rv.protos.TaskConfig.debug\', index=3,\n      number=4, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=True,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'predict_debug_uri\', full_name=\'rv.protos.TaskConfig.predict_debug_uri\', index=4,\n      number=5, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'object_detection_config\', full_name=\'rv.protos.TaskConfig.object_detection_config\', index=5,\n      number=6, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'chip_classification_config\', full_name=\'rv.protos.TaskConfig.chip_classification_config\', index=6,\n      number=7, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'semantic_segmentation_config\', full_name=\'rv.protos.TaskConfig.semantic_segmentation_config\', index=7,\n      number=8, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'custom_config\', full_name=\'rv.protos.TaskConfig.custom_config\', index=8,\n      number=9, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[_TASKCONFIG_OBJECTDETECTIONCONFIG, _TASKCONFIG_CHIPCLASSIFICATIONCONFIG, _TASKCONFIG_SEMANTICSEGMENTATIONCONFIG, ],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n    _descriptor.OneofDescriptor(\n      name=\'config_type\', full_name=\'rv.protos.TaskConfig.config_type\',\n      index=0, containing_type=None, fields=[]),\n  ],\n  serialized_start=114,\n  serialized_end=1552,\n)\n\n_TASKCONFIG_OBJECTDETECTIONCONFIG_CHIPOPTIONS.containing_type = _TASKCONFIG_OBJECTDETECTIONCONFIG\n_TASKCONFIG_OBJECTDETECTIONCONFIG_PREDICTOPTIONS.containing_type = _TASKCONFIG_OBJECTDETECTIONCONFIG\n_TASKCONFIG_OBJECTDETECTIONCONFIG.fields_by_name[\'class_items\'].message_type = rastervision_dot_protos_dot_class__item__pb2._CLASSITEM\n_TASKCONFIG_OBJECTDETECTIONCONFIG.fields_by_name[\'chip_options\'].message_type = _TASKCONFIG_OBJECTDETECTIONCONFIG_CHIPOPTIONS\n_TASKCONFIG_OBJECTDETECTIONCONFIG.fields_by_name[\'predict_options\'].message_type = _TASKCONFIG_OBJECTDETECTIONCONFIG_PREDICTOPTIONS\n_TASKCONFIG_OBJECTDETECTIONCONFIG.containing_type = _TASKCONFIG\n_TASKCONFIG_CHIPCLASSIFICATIONCONFIG.fields_by_name[\'class_items\'].message_type = rastervision_dot_protos_dot_class__item__pb2._CLASSITEM\n_TASKCONFIG_CHIPCLASSIFICATIONCONFIG.containing_type = _TASKCONFIG\n_TASKCONFIG_SEMANTICSEGMENTATIONCONFIG_CHIPOPTIONS.containing_type = _TASKCONFIG_SEMANTICSEGMENTATIONCONFIG\n_TASKCONFIG_SEMANTICSEGMENTATIONCONFIG.fields_by_name[\'class_items\'].message_type = rastervision_dot_protos_dot_class__item__pb2._CLASSITEM\n_TASKCONFIG_SEMANTICSEGMENTATIONCONFIG.fields_by_name[\'chip_options\'].message_type = _TASKCONFIG_SEMANTICSEGMENTATIONCONFIG_CHIPOPTIONS\n_TASKCONFIG_SEMANTICSEGMENTATIONCONFIG.containing_type = _TASKCONFIG\n_TASKCONFIG.fields_by_name[\'object_detection_config\'].message_type = _TASKCONFIG_OBJECTDETECTIONCONFIG\n_TASKCONFIG.fields_by_name[\'chip_classification_config\'].message_type = _TASKCONFIG_CHIPCLASSIFICATIONCONFIG\n_TASKCONFIG.fields_by_name[\'semantic_segmentation_config\'].message_type = _TASKCONFIG_SEMANTICSEGMENTATIONCONFIG\n_TASKCONFIG.fields_by_name[\'custom_config\'].message_type = google_dot_protobuf_dot_struct__pb2._STRUCT\n_TASKCONFIG.oneofs_by_name[\'config_type\'].fields.append(\n  _TASKCONFIG.fields_by_name[\'object_detection_config\'])\n_TASKCONFIG.fields_by_name[\'object_detection_config\'].containing_oneof = _TASKCONFIG.oneofs_by_name[\'config_type\']\n_TASKCONFIG.oneofs_by_name[\'config_type\'].fields.append(\n  _TASKCONFIG.fields_by_name[\'chip_classification_config\'])\n_TASKCONFIG.fields_by_name[\'chip_classification_config\'].containing_oneof = _TASKCONFIG.oneofs_by_name[\'config_type\']\n_TASKCONFIG.oneofs_by_name[\'config_type\'].fields.append(\n  _TASKCONFIG.fields_by_name[\'semantic_segmentation_config\'])\n_TASKCONFIG.fields_by_name[\'semantic_segmentation_config\'].containing_oneof = _TASKCONFIG.oneofs_by_name[\'config_type\']\n_TASKCONFIG.oneofs_by_name[\'config_type\'].fields.append(\n  _TASKCONFIG.fields_by_name[\'custom_config\'])\n_TASKCONFIG.fields_by_name[\'custom_config\'].containing_oneof = _TASKCONFIG.oneofs_by_name[\'config_type\']\nDESCRIPTOR.message_types_by_name[\'TaskConfig\'] = _TASKCONFIG\n\nTaskConfig = _reflection.GeneratedProtocolMessageType(\'TaskConfig\', (_message.Message,), dict(\n\n  ObjectDetectionConfig = _reflection.GeneratedProtocolMessageType(\'ObjectDetectionConfig\', (_message.Message,), dict(\n\n    ChipOptions = _reflection.GeneratedProtocolMessageType(\'ChipOptions\', (_message.Message,), dict(\n      DESCRIPTOR = _TASKCONFIG_OBJECTDETECTIONCONFIG_CHIPOPTIONS,\n      __module__ = \'rastervision.protos.task_pb2\'\n      # @@protoc_insertion_point(class_scope:rv.protos.TaskConfig.ObjectDetectionConfig.ChipOptions)\n      ))\n    ,\n\n    PredictOptions = _reflection.GeneratedProtocolMessageType(\'PredictOptions\', (_message.Message,), dict(\n      DESCRIPTOR = _TASKCONFIG_OBJECTDETECTIONCONFIG_PREDICTOPTIONS,\n      __module__ = \'rastervision.protos.task_pb2\'\n      # @@protoc_insertion_point(class_scope:rv.protos.TaskConfig.ObjectDetectionConfig.PredictOptions)\n      ))\n    ,\n    DESCRIPTOR = _TASKCONFIG_OBJECTDETECTIONCONFIG,\n    __module__ = \'rastervision.protos.task_pb2\'\n    # @@protoc_insertion_point(class_scope:rv.protos.TaskConfig.ObjectDetectionConfig)\n    ))\n  ,\n\n  ChipClassificationConfig = _reflection.GeneratedProtocolMessageType(\'ChipClassificationConfig\', (_message.Message,), dict(\n    DESCRIPTOR = _TASKCONFIG_CHIPCLASSIFICATIONCONFIG,\n    __module__ = \'rastervision.protos.task_pb2\'\n    # @@protoc_insertion_point(class_scope:rv.protos.TaskConfig.ChipClassificationConfig)\n    ))\n  ,\n\n  SemanticSegmentationConfig = _reflection.GeneratedProtocolMessageType(\'SemanticSegmentationConfig\', (_message.Message,), dict(\n\n    ChipOptions = _reflection.GeneratedProtocolMessageType(\'ChipOptions\', (_message.Message,), dict(\n      DESCRIPTOR = _TASKCONFIG_SEMANTICSEGMENTATIONCONFIG_CHIPOPTIONS,\n      __module__ = \'rastervision.protos.task_pb2\'\n      # @@protoc_insertion_point(class_scope:rv.protos.TaskConfig.SemanticSegmentationConfig.ChipOptions)\n      ))\n    ,\n    DESCRIPTOR = _TASKCONFIG_SEMANTICSEGMENTATIONCONFIG,\n    __module__ = \'rastervision.protos.task_pb2\'\n    # @@protoc_insertion_point(class_scope:rv.protos.TaskConfig.SemanticSegmentationConfig)\n    ))\n  ,\n  DESCRIPTOR = _TASKCONFIG,\n  __module__ = \'rastervision.protos.task_pb2\'\n  # @@protoc_insertion_point(class_scope:rv.protos.TaskConfig)\n  ))\n_sym_db.RegisterMessage(TaskConfig)\n_sym_db.RegisterMessage(TaskConfig.ObjectDetectionConfig)\n_sym_db.RegisterMessage(TaskConfig.ObjectDetectionConfig.ChipOptions)\n_sym_db.RegisterMessage(TaskConfig.ObjectDetectionConfig.PredictOptions)\n_sym_db.RegisterMessage(TaskConfig.ChipClassificationConfig)\n_sym_db.RegisterMessage(TaskConfig.SemanticSegmentationConfig)\n_sym_db.RegisterMessage(TaskConfig.SemanticSegmentationConfig.ChipOptions)\n\n\n# @@protoc_insertion_point(module_scope)\n'"
rastervision/protos/vector_source_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: rastervision/protos/vector_source.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom google.protobuf import struct_pb2 as google_dot_protobuf_dot_struct__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'rastervision/protos/vector_source.proto\',\n  package=\'rv.protos\',\n  syntax=\'proto2\',\n  serialized_pb=_b(\'\\n\\\'rastervision/protos/vector_source.proto\\x12\\trv.protos\\x1a\\x1cgoogle/protobuf/struct.proto\\""\\xe5\\x03\\n\\x12VectorSourceConfig\\x12;\\n\\x07mbtiles\\x18\\x01 \\x01(\\x0b\\x32(.rv.protos.VectorSourceConfig.VectorTileH\\x00\\x12\\x38\\n\\x07geojson\\x18\\x02 \\x01(\\x0b\\x32%.rv.protos.VectorSourceConfig.GeoJSONH\\x00\\x12\\x30\\n\\rcustom_config\\x18\\x03 \\x01(\\x0b\\x32\\x17.google.protobuf.StructH\\x00\\x12\\x13\\n\\x0bsource_type\\x18\\x04 \\x02(\\t\\x12\\x33\\n\\x12\\x63lass_id_to_filter\\x18\\x05 \\x01(\\x0b\\x32\\x17.google.protobuf.Struct\\x12\\x18\\n\\x10\\x64\\x65\\x66\\x61ult_class_id\\x18\\x06 \\x01(\\x05\\x12*\\n\\tline_bufs\\x18\\x07 \\x01(\\x0b\\x32\\x17.google.protobuf.Struct\\x12+\\n\\npoint_bufs\\x18\\x08 \\x01(\\x0b\\x32\\x17.google.protobuf.Struct\\x1a\\x39\\n\\nVectorTile\\x12\\x0b\\n\\x03uri\\x18\\x01 \\x02(\\t\\x12\\x0c\\n\\x04zoom\\x18\\x02 \\x02(\\x05\\x12\\x10\\n\\x08id_field\\x18\\x03 \\x02(\\t\\x1a\\x16\\n\\x07GeoJSON\\x12\\x0b\\n\\x03uri\\x18\\x01 \\x02(\\tB\\x16\\n\\x14vector_source_config\')\n  ,\n  dependencies=[google_dot_protobuf_dot_struct__pb2.DESCRIPTOR,])\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\n\n\n\n_VECTORSOURCECONFIG_VECTORTILE = _descriptor.Descriptor(\n  name=\'VectorTile\',\n  full_name=\'rv.protos.VectorSourceConfig.VectorTile\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'uri\', full_name=\'rv.protos.VectorSourceConfig.VectorTile.uri\', index=0,\n      number=1, type=9, cpp_type=9, label=2,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'zoom\', full_name=\'rv.protos.VectorSourceConfig.VectorTile.zoom\', index=1,\n      number=2, type=5, cpp_type=1, label=2,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'id_field\', full_name=\'rv.protos.VectorSourceConfig.VectorTile.id_field\', index=2,\n      number=3, type=9, cpp_type=9, label=2,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=465,\n  serialized_end=522,\n)\n\n_VECTORSOURCECONFIG_GEOJSON = _descriptor.Descriptor(\n  name=\'GeoJSON\',\n  full_name=\'rv.protos.VectorSourceConfig.GeoJSON\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'uri\', full_name=\'rv.protos.VectorSourceConfig.GeoJSON.uri\', index=0,\n      number=1, type=9, cpp_type=9, label=2,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=524,\n  serialized_end=546,\n)\n\n_VECTORSOURCECONFIG = _descriptor.Descriptor(\n  name=\'VectorSourceConfig\',\n  full_name=\'rv.protos.VectorSourceConfig\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'mbtiles\', full_name=\'rv.protos.VectorSourceConfig.mbtiles\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'geojson\', full_name=\'rv.protos.VectorSourceConfig.geojson\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'custom_config\', full_name=\'rv.protos.VectorSourceConfig.custom_config\', index=2,\n      number=3, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'source_type\', full_name=\'rv.protos.VectorSourceConfig.source_type\', index=3,\n      number=4, type=9, cpp_type=9, label=2,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'class_id_to_filter\', full_name=\'rv.protos.VectorSourceConfig.class_id_to_filter\', index=4,\n      number=5, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'default_class_id\', full_name=\'rv.protos.VectorSourceConfig.default_class_id\', index=5,\n      number=6, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'line_bufs\', full_name=\'rv.protos.VectorSourceConfig.line_bufs\', index=6,\n      number=7, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'point_bufs\', full_name=\'rv.protos.VectorSourceConfig.point_bufs\', index=7,\n      number=8, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[_VECTORSOURCECONFIG_VECTORTILE, _VECTORSOURCECONFIG_GEOJSON, ],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n    _descriptor.OneofDescriptor(\n      name=\'vector_source_config\', full_name=\'rv.protos.VectorSourceConfig.vector_source_config\',\n      index=0, containing_type=None, fields=[]),\n  ],\n  serialized_start=85,\n  serialized_end=570,\n)\n\n_VECTORSOURCECONFIG_VECTORTILE.containing_type = _VECTORSOURCECONFIG\n_VECTORSOURCECONFIG_GEOJSON.containing_type = _VECTORSOURCECONFIG\n_VECTORSOURCECONFIG.fields_by_name[\'mbtiles\'].message_type = _VECTORSOURCECONFIG_VECTORTILE\n_VECTORSOURCECONFIG.fields_by_name[\'geojson\'].message_type = _VECTORSOURCECONFIG_GEOJSON\n_VECTORSOURCECONFIG.fields_by_name[\'custom_config\'].message_type = google_dot_protobuf_dot_struct__pb2._STRUCT\n_VECTORSOURCECONFIG.fields_by_name[\'class_id_to_filter\'].message_type = google_dot_protobuf_dot_struct__pb2._STRUCT\n_VECTORSOURCECONFIG.fields_by_name[\'line_bufs\'].message_type = google_dot_protobuf_dot_struct__pb2._STRUCT\n_VECTORSOURCECONFIG.fields_by_name[\'point_bufs\'].message_type = google_dot_protobuf_dot_struct__pb2._STRUCT\n_VECTORSOURCECONFIG.oneofs_by_name[\'vector_source_config\'].fields.append(\n  _VECTORSOURCECONFIG.fields_by_name[\'mbtiles\'])\n_VECTORSOURCECONFIG.fields_by_name[\'mbtiles\'].containing_oneof = _VECTORSOURCECONFIG.oneofs_by_name[\'vector_source_config\']\n_VECTORSOURCECONFIG.oneofs_by_name[\'vector_source_config\'].fields.append(\n  _VECTORSOURCECONFIG.fields_by_name[\'geojson\'])\n_VECTORSOURCECONFIG.fields_by_name[\'geojson\'].containing_oneof = _VECTORSOURCECONFIG.oneofs_by_name[\'vector_source_config\']\n_VECTORSOURCECONFIG.oneofs_by_name[\'vector_source_config\'].fields.append(\n  _VECTORSOURCECONFIG.fields_by_name[\'custom_config\'])\n_VECTORSOURCECONFIG.fields_by_name[\'custom_config\'].containing_oneof = _VECTORSOURCECONFIG.oneofs_by_name[\'vector_source_config\']\nDESCRIPTOR.message_types_by_name[\'VectorSourceConfig\'] = _VECTORSOURCECONFIG\n\nVectorSourceConfig = _reflection.GeneratedProtocolMessageType(\'VectorSourceConfig\', (_message.Message,), dict(\n\n  VectorTile = _reflection.GeneratedProtocolMessageType(\'VectorTile\', (_message.Message,), dict(\n    DESCRIPTOR = _VECTORSOURCECONFIG_VECTORTILE,\n    __module__ = \'rastervision.protos.vector_source_pb2\'\n    # @@protoc_insertion_point(class_scope:rv.protos.VectorSourceConfig.VectorTile)\n    ))\n  ,\n\n  GeoJSON = _reflection.GeneratedProtocolMessageType(\'GeoJSON\', (_message.Message,), dict(\n    DESCRIPTOR = _VECTORSOURCECONFIG_GEOJSON,\n    __module__ = \'rastervision.protos.vector_source_pb2\'\n    # @@protoc_insertion_point(class_scope:rv.protos.VectorSourceConfig.GeoJSON)\n    ))\n  ,\n  DESCRIPTOR = _VECTORSOURCECONFIG,\n  __module__ = \'rastervision.protos.vector_source_pb2\'\n  # @@protoc_insertion_point(class_scope:rv.protos.VectorSourceConfig)\n  ))\n_sym_db.RegisterMessage(VectorSourceConfig)\n_sym_db.RegisterMessage(VectorSourceConfig.VectorTile)\n_sym_db.RegisterMessage(VectorSourceConfig.GeoJSON)\n\n\n# @@protoc_insertion_point(module_scope)\n'"
rastervision/runner/__init__.py,0,b'# flake8: noqa\n\nfrom rastervision.runner.command_definition import *\nfrom rastervision.runner.command_dag import *\nfrom rastervision.runner.experiment_runner import *\nfrom rastervision.runner.out_of_process_experiment_runner import *\nfrom rastervision.runner.inprocess_experiment_runner import *\nfrom rastervision.runner.aws_batch_experiment_runner import *\nfrom rastervision.runner.local_experiment_runner import *\nfrom rastervision.runner.command_runner import *\n'
rastervision/runner/api.py,0,"b""# flake8: noqa\n\n# Registry keys\n\nINPROCESS = 'INPROCESS'\nAWS_BATCH = 'AWS_BATCH'\nLOCAL = 'LOCAL'\n\nfrom rastervision.runner.experiment_runner import ExperimentRunner\n"""
rastervision/runner/aws_batch_experiment_runner.py,0,"b'import uuid\nimport click\nimport logging\n\nfrom rastervision.runner import OutOfProcessExperimentRunner\nfrom rastervision.rv_config import RVConfig\nfrom rastervision.utils.misc import grouped\n\nDEPENDENCY_GROUP_JOB = \'DEPENDENCY_GROUP\'\n\nNOOP_COMMAND = \'python -m rastervision --help\'\n\nlog = logging.getLogger(__name__)\n\n\nclass AwsBatchExperimentRunner(OutOfProcessExperimentRunner):\n    def __init__(self):\n        super().__init__()\n\n        rv_config = RVConfig.get_instance()\n\n        batch_config = rv_config.get_subconfig(\'AWS_BATCH\')\n\n        self.attempts = batch_config(\'attempts\', parser=int, default=\'1\')\n        self.gpu = batch_config(\'gpu\', parser=bool, default=\'true\')\n\n        job_queue = batch_config(\'job_queue\', default=\'\')\n        if not job_queue:\n            if self.gpu:\n                job_queue = \'raster-vision-gpu\'\n            else:\n                job_queue = \'raster-vision-cpu\'\n        self.job_queue = job_queue\n\n        cpu_job_queue = batch_config(\'cpu_job_queue\', default=\'\')\n        if not cpu_job_queue:\n            if self.gpu:\n                cpu_job_queue = \'raster-vision-cpu\'\n            else:\n                cpu_job_queue = job_queue\n        self.cpu_job_queue = cpu_job_queue\n\n        job_definition = batch_config(\'job_definition\', default=\'\')\n        if not job_definition:\n            if self.gpu:\n                job_definition = \'raster-vision-gpu\'\n            else:\n                job_definition = \'raster-vision-cpu\'\n        self.job_definition = job_definition\n\n        cpu_job_definition = batch_config(\'cpu_job_definition\', default=\'\')\n        if not cpu_job_definition:\n            if self.gpu:\n                cpu_job_definition = \'raster-vision-cpu\'\n            else:\n                cpu_job_definition = job_definition\n        self.cpu_job_definition = cpu_job_definition\n\n        self.submit = self.batch_submit\n        self.execution_environment = \'Batch\'\n\n    def _get_boto_client(self):\n        """"""Returns a boto client.\n        Encapsulated in a method for testability.\n        """"""\n        import boto3\n        return boto3.client(\'batch\')\n\n    def batch_submit(self,\n                     command_type,\n                     command_split_id,\n                     experiment_id,\n                     command,\n                     parent_job_ids=None,\n                     array_size=None,\n                     utilizes_gpu=False):\n        """"""\n        Submit a job to run on Batch.\n\n        Args:\n           command_type: (str) the type of command. ie. a value in rv.command.api\n           command_split_id: (int or str) the split ID of command.\n                             ie. the parallelized command ID\n           experiment_id: (str) id of experiment\n           command: (str) command to run inside Docker container\n           parent_job_ids (list of str): ids of jobs that this job depends on\n           array_size: (int) size of the Batch array job\n        """"""\n        if parent_job_ids is None:\n            parent_job_ids = []\n\n        if command_type == DEPENDENCY_GROUP_JOB:\n            full_command = NOOP_COMMAND.split()\n        else:\n            full_command = command.split()\n\n        # When running on Batch, we set the temp dir to be inside /opt/data\n        # since it is mounted to the host instance which has much more disk\n        # space.\n        full_command += [\'--tempdir\', \'/opt/data/temp/\']\n\n        client = self._get_boto_client()\n\n        uuid_part = str(uuid.uuid4()).split(\'-\')[0]\n        if experiment_id is None:\n            exp = \'COMMAND\'\n        else:\n            exp = \'\'.join(e for e in experiment_id if e.isalnum())\n        job_name = \'{}-{}_{}_{}\'.format(command_type, command_split_id, exp,\n                                        uuid_part)\n\n        group_level = 1\n        while len(parent_job_ids) > 20:\n            # AWS Batch only allows for 20 parent jobs.\n            # This hacks around that limit.\n            # Group dependencies  into batches of 20,\n            # and submit noop jobs that form a graph\n            # of dependencies where no set of leaves is\n            # greater than 20.\n            log.warn(\'More that 20 parent jobs detected, grouping [LEVEL {}]\'.\n                     format(group_level))\n            new_parents = []\n            group_id = str(uuid.uuid4()).split(\'-\')[0]\n            for i, group in enumerate(grouped(parent_job_ids, 20)):\n                group_split_id = \'{}_{}-{}_{}\'.format(group_id, command_type,\n                                                      command_split_id, i)\n                new_parents.append(\n                    self.batch_submit(DEPENDENCY_GROUP_JOB, group_split_id,\n                                      experiment_id, NOOP_COMMAND, group,\n                                      array_size))\n            parent_job_ids = new_parents\n            group_level += 1\n\n        depends_on = [{\'jobId\': job_id} for job_id in parent_job_ids]\n\n        if utilizes_gpu:\n            job_queue = self.job_queue\n            job_definition = self.job_definition\n        else:\n            job_queue = self.cpu_job_queue\n            job_definition = self.cpu_job_definition\n\n        kwargs = {\n            \'jobName\': job_name,\n            \'jobQueue\': job_queue,\n            \'jobDefinition\': job_definition,\n            \'containerOverrides\': {\n                \'command\': full_command\n            },\n            \'retryStrategy\': {\n                \'attempts\': self.attempts\n            },\n            \'dependsOn\': depends_on\n        }\n\n        if array_size is not None:\n            kwargs[\'arrayProperties\'] = {\'size\': array_size}\n\n        job_id = client.submit_job(**kwargs)[\'jobId\']\n\n        msg = \'{}-{} command submitted job with jobName={} and jobId={}\'.format(\n            command_type, command_split_id, job_name, job_id)\n        click.echo(click.style(msg, fg=\'green\'))\n\n        return job_id\n'"
rastervision/runner/command_dag.py,0,"b'import networkx as nx\nimport logging\n\nimport rastervision as rv\nfrom rastervision.utils.files import file_exists\n\nimport click\n\nlog = logging.getLogger(__name__)\n\n\nclass CommandDAG:\n    """""" A directed acyclic graph of command definitions.\n    """"""\n\n    def __init__(self,\n                 command_definitions,\n                 rerun_commands=False,\n                 skip_file_check=False):\n        """"""Generates a CommandDAG from a list of CommandDefinitions\n\n        This logic checks if there are any non-exsiting URIs that are\n        not produced as outputs by some command in the set. If so,\n        it raises a ConfigError stating the missing files.\n        """"""\n        # Create a set of edges, from input_uri to command_config and\n        # from command_config to output_uri. Nodes for commands are their\n        # index into command_definitions.\n\n        uri_dag = nx.DiGraph()\n\n        log.debug(\'Creating command and URI DAG from command definitions...\')\n        for idx, command_def in enumerate(command_definitions):\n            uri_dag.add_node(idx)\n            for input_uri in command_def.io_def.input_uris:\n                uri_dag.add_edge(input_uri, idx)\n\n            for output_uri in command_def.io_def.output_uris:\n                uri_dag.add_edge(idx, output_uri)\n\n        # Find all source input_uris, and ensure they exist.\n        if not skip_file_check:\n            log.debug(\'Ensuring input files exist...\')\n            unsolved_sources = [\n                uri for uri in uri_dag.nodes\n                if (type(uri) == str and len(uri_dag.in_edges(uri)) == 0)\n            ]\n\n            missing_files = []\n\n            with click.progressbar(\n                    unsolved_sources,\n                    label=\'Ensuring input files exist  \') as uris:\n                for uri in uris:\n                    if not file_exists(uri):\n                        missing_files.append(uri)\n\n            if any(missing_files):\n                raise rv.ConfigError(\n                    \'Files do not exist and are not supplied by commands:\\n\'\n                    \'\\t{}\\n\'.format(\',\\b\\t\'.join(missing_files)))\n\n        # If we are not rerunning, remove commands that have existing outputs.\n        self.skipped_commands = []\n        if not rerun_commands:\n            log.debug(\'Checking for existing output...\')\n            commands_to_outputs = [(idx, edge[1]) for idx in uri_dag.nodes\n                                   if type(idx) == int\n                                   for edge in uri_dag.out_edges(idx)]\n            with click.progressbar(\n                    commands_to_outputs,\n                    label=\'Checking for existing output\') as lst:\n                for idx, output_uri in lst:\n                    if file_exists(output_uri):\n                        uri_dag.remove_edge(idx, output_uri)\n\n            for idx in set(map(lambda x: x[0], commands_to_outputs)):\n                if len(uri_dag.out_edges(idx)) == 0:\n                    self.skipped_commands.append(command_definitions[idx])\n                    uri_dag.remove_node(idx)\n\n        # Collapse the graph to create edges from command to command.\n        command_id_dag = nx.DiGraph()\n\n        log.debug(\'Creating DAG of commands...\')\n        for idx in [idx for idx in uri_dag.nodes if (type(idx) == int)]:\n            command_id_dag.add_node(idx)\n            for upstream_idx in [\n                    edge2[0] for edge1 in uri_dag.in_edges(idx)\n                    for edge2 in uri_dag.in_edges(edge1[0])\n            ]:\n                command_id_dag.add_edge(upstream_idx, idx)\n\n        # Feed this digraph of commands to the child runner.\n        self.command_definitions = command_definitions\n        self.command_id_dag = command_id_dag\n\n    def get_sorted_commands(self):\n        """"""Return a topologically sorted list of commands configurations.\n\n        Returns a list of command configurations that are sorted such that every\n        command that depends on some other parent command appears later\n        than that parent command.\n        """"""\n        return [\n            self.command_definitions[idx].command_config\n            for idx in self.get_sorted_command_ids()\n        ]\n\n    def get_sorted_command_ids(self):\n        """"""Return a topologically sorted list of commands ids.\n\n        Returns a list of command IDs that can be used to retrieve\n        specific commands out of this DAG. These are sorted such that every\n        command that depends on some other parent command appears later\n        than that parent command.\n        """"""\n        return [idx for idx in nx.topological_sort(self.command_id_dag)]\n\n    def get_command(self, command_id):\n        """"""Retrieves a command configuration for the given ID""""""\n        return self.get_command_definition(command_id).command_config\n\n    def get_command_definition(self, command_id):\n        """"""Retrieves a command definition for the given ID""""""\n        return self.command_definitions[command_id]\n\n    def get_upstream_command_ids(self, command_id):\n        """"""Returns the command ids for upstream commands for the command\n        with the given id.\n        """"""\n        return list(\n            map(lambda x: x[0], self.command_id_dag.in_edges(command_id)))\n\n    def get_command_definitions(self):\n        """"""Returns the command definitions that will be run in this DAG.""""""\n        return [\n            self.command_definitions[idx] for idx in self.command_id_dag.nodes\n        ]\n'"
rastervision/runner/command_definition.py,0,"b'from typing import List\nimport logging\nfrom copy import deepcopy\nfrom collections import defaultdict\n\nimport rastervision as rv\n\nlog = logging.getLogger(__name__)\n\n\nclass CommandDefinition:\n    def __init__(self, experiment_id, command_config, io_def):\n        self.experiment_id = experiment_id\n        self.command_config = command_config\n        self.io_def = io_def\n\n    def _key(self):\n        return (self.command_config.command_type, self.command_config.split_id,\n                \'|\'.join(sorted(self.io_def.input_uris)), \'|\'.join(\n                    sorted(self.io_def.output_uris)))\n\n    def __eq__(self, other):\n        return self._key() == other._key()\n\n    def __hash__(self):\n        return hash(self._key())\n\n    @classmethod\n    def from_experiments(cls,\n                         experiments: List[rv.ExperimentConfig],\n                         commands_to_run: List[str],\n                         splits: int = 1):\n        command_definitions = []\n\n        for experiment in experiments:\n            e = deepcopy(experiment)\n            log.debug(\n                \'Generating command definitions for experiment {}...\'.format(\n                    e.id))\n            commands_to_update = rv.all_commands() + \\\n                                 list(set(commands_to_run) - set(rv.all_commands()))\n            for command_type in commands_to_update:\n                log.debug(\n                    \'Updating config for command {}...\'.format(command_type))\n                e.update_for_command(command_type, e)\n                if command_type in commands_to_run:\n                    log.debug(\n                        \'Creating command configurations for {}...\'.format(\n                            command_type))\n                    base_command_config = e.make_command_config(command_type)\n                    for command_config in base_command_config.split(splits):\n                        io_def = command_config.report_io()\n                        command_def = cls(e.id, command_config, io_def)\n                        command_definitions.append(command_def)\n\n        return command_definitions\n\n    @classmethod\n    def from_command_configs(cls,\n                             command_configs: List[rv.CommandConfig],\n                             commands_to_run: List[str],\n                             splits: int = 1):\n        command_definitions = []\n\n        for base_command_config in command_configs:\n            if base_command_config.command_type in commands_to_run:\n                log.debug(\'Creating command configurations for {}...\'.format(\n                    base_command_config.command_type))\n                for command_config in base_command_config.split(splits):\n                    io_def = command_config.report_io()\n                    command_def = cls(None, command_config, io_def)\n                    command_definitions.append(command_def)\n\n        return command_definitions\n\n    @staticmethod\n    def filter_no_output(command_definitions):\n        """"""Filters commands that have no output.""""""\n        result = []\n        skipped = []\n        for command_def in command_definitions:\n            if any(command_def.io_def.output_uris):\n                result.append(command_def)\n            else:\n                skipped.append(command_def)\n\n        return (result, skipped)\n\n    @staticmethod\n    def remove_duplicates(command_definitions):\n        """"""Remove duplicate commands.\n\n        Removes duplicated commands, defining equality for a command by\n        the tuple (command_type, input_uris, output_uris)\n        """"""\n\n        unique_commands = []\n        skipped_commands = []\n        seen_commands = set([])\n        for command_def in command_definitions:\n            if command_def not in seen_commands:\n                seen_commands.add(command_def)\n                unique_commands.append(command_def)\n            else:\n                skipped_commands.append(command_def)\n\n        return (unique_commands, skipped_commands)\n\n    @staticmethod\n    def get_missing_inputs(command_definitions):\n        """"""Gathers missing inputs from a set of commands.\n\n        Returns a dictionary of experiment id to list of missing input URIs.\n        """"""\n        missing_inputs = {}\n        for command_def in command_definitions:\n            if command_def.io_def.missing_input_messages:\n                mi = command_def.io_def.missing_input_messages\n                missing_inputs[command_def.experiment_id] = mi\n        return missing_inputs\n\n    @staticmethod\n    def get_clashing_commands(command_definitions):\n        """"""Reports commands that will overwrite each other\'s outputs.\n\n        Only reports commands as clashing if they are of the same command type.\n\n        Returns a List[str, List[CommandDefinition]] of output URIs\n        and clashing commands.\n        """"""\n        outputs_to_defs = defaultdict(lambda: [])\n        clashing_commands = []\n        for command_def in command_definitions:\n            command_type = command_def.command_config.command_type\n            split_id = command_def.command_config.split_id\n            for output_uri in command_def.io_def.output_uris:\n                outputs_to_defs[(output_uri, command_type,\n                                 split_id)].append(command_def)\n\n        for ((output_uri, _, _), command_defs) in outputs_to_defs.items():\n            if len(command_defs) > 1:\n                clashing_commands.append((output_uri, command_defs))\n\n        return clashing_commands\n'"
rastervision/runner/command_runner.py,0,"b'import rastervision as rv\nfrom rastervision.plugin import PluginRegistry\nfrom rastervision.protos.command_pb2 import CommandConfig as CommandConfigMsg\nfrom rastervision.utils.files import load_json_config\n\n\nclass CommandRunner:\n    @staticmethod\n    def run(command_config_uri):\n        msg = load_json_config(command_config_uri, CommandConfigMsg())\n        CommandRunner.run_from_proto(msg)\n\n    def run_from_proto(msg):\n        PluginRegistry.get_instance().add_plugins_from_proto(msg.plugins)\n        command_config = rv.command.CommandConfig.from_proto(msg)\n        command = command_config.create_command()\n        command.run()\n'"
rastervision/runner/experiment_runner.py,0,"b'from abc import (ABC, abstractmethod)\nfrom typing import (List, Union)\nimport logging\n\nimport click\nfrom google.protobuf import json_format\n\nimport rastervision as rv\nfrom rastervision.runner import (CommandDefinition, CommandDAG)\nfrom rastervision.cli import Verbosity\n\nlog = logging.getLogger(__name__)\n\n\nclass ExperimentRunner(ABC):\n    def print_command(self, command_def, command_id=None, command_dag=None):\n        verbosity = Verbosity.get()\n        command_type = command_def.command_config.command_type\n        split_id = command_def.command_config.split_id\n        experiment_id = command_def.experiment_id\n        click.echo(\n            click.style(\'{}-{} \'.format(command_type, split_id), bold=True),\n            nl=False)\n        click.echo(\'from {}\'.format(experiment_id))\n\n        if verbosity >= Verbosity.VERBOSE:\n            click.echo(\'  INPUTS:\')\n            for input_uri in command_def.io_def.input_uris:\n                click.echo(\'    {}\'.format(input_uri))\n            if not command_def.io_def.output_uris:\n                click.echo(\'  NO OUTPUTS!\')\n            else:\n                click.echo(\'  OUTPUTS:\')\n                for output_uri in command_def.io_def.output_uris:\n                    click.echo(\'    {}\'.format(output_uri))\n        if verbosity >= Verbosity.VERY_VERBOSE:\n            click.echo(click.style(\'  COMMAND CONFIGURATION\', bold=True))\n            click.echo(\'  ---------------------\\n\')\n            click.echo(\'{}\'.format(\n                json_format.MessageToJson(\n                    command_def.command_config.to_proto())))\n\n        if command_dag:\n            upstreams = command_dag.get_upstream_command_ids(command_id)\n            if upstreams:\n                for upstream_id in upstreams:\n                    cdef = command_dag.get_command_definition(upstream_id)\n                    msg = \'  DEPENDS ON: {}-{} from {}\'.format(\n                        cdef.command_config.command_type,\n                        cdef.command_config.split_id, cdef.experiment_id)\n                    click.echo(click.style(msg, fg=\'cyan\'))\n\n    def run(self,\n            experiments: Union[List[rv.ExperimentConfig], rv.ExperimentConfig],\n            command_configs: Union[List[rv.CommandConfig],\n                                   rv.CommandConfig] = None,\n            commands_to_run=None,\n            rerun_commands=False,\n            skip_file_check=False,\n            dry_run: bool = False,\n            splits: int = 1):\n        if not commands_to_run:\n            commands_to_run = rv.all_commands()\n\n        if not isinstance(experiments, list):\n            experiments = [experiments]\n\n        log.debug(\'Generating command definitions from experiments...\')\n        command_definitions = CommandDefinition.from_experiments(\n            experiments, commands_to_run, splits)\n\n        if command_configs:\n            if not isinstance(command_configs, list):\n                command_configs = [command_configs]\n            log.debug(\'Generating command definitions from commands...\')\n            command_definitions.extend(\n                CommandDefinition.from_command_configs(\n                    command_configs, commands_to_run, splits))\n\n        # Filter  out commands that don\'t have any output.\n        log.debug(\'Filtering commands that do not have any output...\')\n        (command_definitions,\n         no_output) = CommandDefinition.filter_no_output(command_definitions)\n\n        # Print commands that have no output\n        if dry_run:\n            if no_output:\n                print()\n                click.echo(\n                    click.style(\n                        \'Commands not run because they have no output:\',\n                        fg=\'yellow\',\n                        bold=True,\n                        underline=True))\n                for command in no_output:\n                    self.print_command(command)\n                print()\n\n        # Check if there are any unsatisfied inputs.\n        log.debug(\'Checking for missing input from configuration...\')\n        missing_inputs = CommandDefinition.get_missing_inputs(\n            command_definitions)\n        if missing_inputs:\n            # TODO: Replace exception with logging?\n            s = \'\'\n            for exp_id in missing_inputs:\n                s += \'In {}:\\n\\t{}\\n\'.format(\n                    exp_id, \'\\t{}\\n\'.join(missing_inputs[exp_id]))\n\n            raise rv.ConfigError(\'There were missing input URIs \'\n                                 \'that are required, but were not \'\n                                 \'able to be derived: \\n{}\'.format(s))\n\n        # Remove duplicate commands, defining equality for a command by\n        # the tuple (command_type, input_uris, output_uris)\n        log.debug(\'Removing duplicate commands...\')\n        (unique_commands, skipped_duplicate_commands\n         ) = CommandDefinition.remove_duplicates(command_definitions)\n\n        if dry_run:\n            if skipped_duplicate_commands:\n                print()\n                msg = (\'Commands determined to be \'\n                       \'duplicates based on input and output:\')\n                click.echo(\n                    click.style(msg, fg=\'yellow\', bold=True, underline=True))\n                for command in skipped_duplicate_commands:\n                    self.print_command(command)\n                print()\n\n        # Ensure that for each type of command, there are none that clobber\n        # each other\'s output.\n        log.debug(""Ensuring commands do not overwrite each other\'s outputs..."")\n        clashing_commands = CommandDefinition.get_clashing_commands(\n            unique_commands)\n\n        if clashing_commands:\n            clashing_msgs = []\n            for (output_uri, c_defs) in clashing_commands:\n                command_type = c_defs[0].command_config.command_type\n                experiments = \', \'.join(\n                    map(lambda c: c.experiment_id or command_type, c_defs))\n                clashing_msgs.append(\n                    \'The {} command in the following experiments \'\n                    \'output {}, but are not equal: {}\'.format(\n                        command_type, output_uri, experiments))\n            # TODO: Replace with logging?\n            s = \'\\t\\n\'.join(clashing_msgs)\n\n            raise rv.ConfigError(\'ERROR: Command outputs will\'\n                                 \'override each other: \\n{}\\n\'.format(s))\n\n        log.debug(\'Constructing command DAG...\')\n        command_dag = CommandDAG(\n            unique_commands, rerun_commands, skip_file_check=skip_file_check)\n\n        # Print conflicating or alread fulfilled commands\n        if dry_run:\n            skipped_commands = command_dag.skipped_commands\n            if skipped_commands:\n                print()\n                msg = \'Commands skipped because output already exists:\'\n                click.echo(\n                    click.style(msg, fg=\'yellow\', bold=True, underline=True))\n                for command in skipped_commands:\n                    self.print_command(command)\n                print()\n\n        # Save experiment configs\n        experiments_by_id = dict(map(lambda e: (e.id, e), experiments))\n        seen_ids = set([])\n        for command_def in command_dag.get_command_definitions():\n            if command_def.experiment_id is not None and \\\n               command_def.experiment_id not in seen_ids:\n                seen_ids.add(command_def.experiment_id)\n                experiment = experiments_by_id[command_def.experiment_id]\n                if not dry_run:\n                    log.debug(\n                        \'Saving experiment configuration for experiment {}\'.\n                        format(experiment.id))\n                    experiment.fully_resolve().save_config()\n\n        if dry_run:\n            print()\n            sorted_command_ids = command_dag.get_sorted_command_ids()\n            if len(sorted_command_ids) == 0:\n                click.echo(\n                    click.style(\'No commands to run!\', fg=\'red\', bold=True))\n                print()\n            else:\n                click.echo(\n                    click.style(\n                        \'Commands to be run in this order:\',\n                        fg=\'green\',\n                        bold=True,\n                        underline=True))\n                for command_id in command_dag.get_sorted_command_ids():\n                    command_def = command_dag.get_command_definition(\n                        command_id)\n                    self.print_command(command_def, command_id, command_dag)\n                    print()\n            self._dry_run(command_dag)\n        else:\n            log.debug(\'Running experiment...\')\n            self._run_experiment(command_dag)\n\n    @abstractmethod\n    def _run_experiment(self, command_dag):\n        pass\n\n    def _dry_run(self, command_dag):\n        """"""Overridden by subclasses if they contribute dry run information.\n        """"""\n        pass\n\n    @staticmethod\n    def get_runner(runner_type):\n        """"""Gets the runner associated with this runner type.""""""\n        # Runner keys are upper cased.\n        return rv._registry.get_experiment_runner(runner_type.upper())\n\n    @staticmethod\n    def list_runners():\n        """"""Returns a list of valid runner keys.""""""\n        return rv._registry.get_experiment_runner_keys()\n'"
rastervision/runner/inprocess_experiment_runner.py,0,"b'import os\n\nimport logging\n\nfrom rastervision.runner import ExperimentRunner\nfrom rastervision.utils.files import save_json_config\nfrom rastervision.rv_config import RVConfig\nimport rastervision as rv\n\nlog = logging.getLogger(__name__)\n\n\nclass InProcessExperimentRunner(ExperimentRunner):\n    """"""A class implementing functionality for running experiments within\n    the present process.\n\n    Note that the bulk of some stages still run in different process,\n    as for example when a separate training script is run as a shell\n    command.  The contrast between this behavior and the\n    out-of-process runners (those derived from\n    `OutOfProcessExperimentRunner`) is that in the latter, those shell\n    commands are run from stage-specific processes.\n\n    """"""\n\n    def __init__(self, tmp_dir=None):\n        self.tmp_dir = tmp_dir\n\n    def _run_experiment(self, command_dag):\n        """"""Runs all commands on this machine.""""""\n\n        def run_commands(tmp_dir):\n            for command_config in command_dag.get_sorted_commands():\n                msg = command_config.to_proto()\n                builder = rv._registry.get_command_config_builder(\n                    msg.command_type)()\n                command_config = builder.from_proto(msg).build()\n\n                command_root_uri = command_config.root_uri\n                command_basename = \'command-config-{}.json\'.format(\n                    command_config.split_id)\n                command_uri = os.path.join(command_root_uri, command_basename)\n                log.info(\'Saving command configuration to {}...\'.format(\n                    command_uri))\n                save_json_config(command_config.to_proto(), command_uri)\n\n                command = command_config.create_command()\n                command.run(tmp_dir)\n\n        if self.tmp_dir:\n            run_commands(self.tmp_dir)\n        else:\n            with RVConfig.get_tmp_dir() as tmp_dir:\n                run_commands(tmp_dir)\n'"
rastervision/runner/local_experiment_runner.py,0,"b""from subprocess import Popen\nimport sys\nimport logging\nimport os\n\nfrom rastervision.runner import OutOfProcessExperimentRunner\nfrom rastervision.runner import make_command\nfrom rastervision.utils.misc import (terminate_at_exit)\nfrom rastervision.utils.files import (save_json_config, make_dir)\nfrom rastervision.rv_config import RVConfig\n\nlog = logging.getLogger(__name__)\n\n\nclass LocalExperimentRunner(OutOfProcessExperimentRunner):\n    def __init__(self, tmp_dir=None):\n        super().__init__()\n\n        self.submit = None\n        self.execution_environment = 'Shell'\n        self.tmp_dir = tmp_dir\n\n    def _run_experiment(self, command_dag):\n        tmp_dir = self.tmp_dir or RVConfig.get_tmp_dir().name\n        make_dir(tmp_dir)\n        makefile_name = os.path.join(tmp_dir, 'Makefile')\n        with open(makefile_name, 'w') as makefile:\n            command_ids = command_dag.get_sorted_command_ids()\n\n            # .PHONY: 0 1 2 3 4 5\n            makefile.write('.PHONY:')\n            for command_id in command_ids:\n                makefile.write(' {}'.format(command_id))\n            makefile.write('\\n\\n')\n\n            # all: 0 1 2 3 4 5\n            makefile.write('all:')\n            for command_id in command_ids:\n                makefile.write(' {}'.format(command_id))\n            makefile.write('\\n\\n')\n\n            for command_id in command_ids:\n                # 0: 1 2\n                makefile.write('{}:'.format(command_id))\n                for upstream_id in command_dag.get_upstream_command_ids(\n                        command_id):\n                    makefile.write(' {}'.format(upstream_id))\n                makefile.write('\\n')\n\n                # \\t rastervision ...\n                command_def = command_dag.get_command_definition(command_id)\n                command_config = command_def.command_config\n                command_root_uri = command_config.root_uri\n                command_basename = 'command-config-{}.json'.format(\n                    command_config.split_id)\n                command_uri = os.path.join(command_root_uri, command_basename)\n                print('Saving command configuration to {}...'.format(\n                    command_uri))\n                save_json_config(command_config.to_proto(), command_uri)\n                run_command = make_command(command_uri, self.tmp_dir)\n                makefile.write('\\t{}\\n\\n'.format(run_command))\n\n        process = Popen(['make', '-j', '-f', makefile_name])\n        terminate_at_exit(process)\n        exitcode = process.wait()\n        if exitcode != 0:\n            sys.exit(exitcode)\n        else:\n            return 0\n"""
rastervision/runner/out_of_process_experiment_runner.py,0,"b'import os\nimport click\n\nfrom rastervision.runner import ExperimentRunner\nfrom rastervision.utils.files import save_json_config\nfrom rastervision.cli import Verbosity\n\n\ndef make_command(command_config_uri, tmp_dir=None):\n    verbosity = Verbosity.get()\n    v_flag = \'v\' * max(0, verbosity - 1)\n    if v_flag:\n        v_flag = \'-{}\'.format(v_flag)\n    if tmp_dir is None:\n        command = \'python -m rastervision {} run_command {}\'.format(\n            v_flag, command_config_uri)\n    else:\n        command = \'python -m rastervision {} run_command {} --tempdir {}\'.format(\n            v_flag, command_config_uri, tmp_dir)\n    return command\n\n\nclass OutOfProcessExperimentRunner(ExperimentRunner):\n    """"""A class implementing functionality for out-of-process running of experiments.\n\n    The term ""out-of-process"" refers to the fact that experiments are\n    not run within this process, but instead separate processes are\n    forked for each stage.\n\n    In the case of `AwsBatchExperimentRunner`, which derives from this\n    class, those processes are run remotely on AWS.  In case of\n    `LocalExperimentRunner`, which also derives from this class, the\n    processes run locally.  This behavior can be contrasted with\n    `InProcessExperimentRunner` wherein experiment stages are run\n    within the calling process.\n\n    """"""\n\n    def __init__(self):\n        self.tmp_dir = None\n\n    def _run_experiment(self, command_dag):\n        """"""Runs all commands.""""""\n\n        ids_to_job = {}\n        for command_id in command_dag.get_sorted_command_ids():\n            command_def = command_dag.get_command_definition(command_id)\n            command_config = command_def.command_config\n            command_root_uri = command_config.root_uri\n            command_basename = \'command-config-{}.json\'.format(\n                command_config.split_id)\n            command_uri = os.path.join(command_root_uri, command_basename)\n            print(\'Saving command configuration to {}...\'.format(command_uri))\n            save_json_config(command_config.to_proto(), command_uri)\n\n            parent_job_ids = []\n            for upstream_id in command_dag.get_upstream_command_ids(\n                    command_id):\n                if upstream_id not in ids_to_job:\n                    cur_command = (command_config.command_type, command_id)\n                    u = command_dag.get_command(upstream_id)\n                    upstream_command = (u.command_type, upstream_id)\n                    raise Exception(\n                        \'{} command has parent command of {}, \'\n                        \'but does not exist in previous submissions - \'\n                        \'topological sort on command_dag error.\'.format(\n                            cur_command, upstream_command))\n                parent_job_ids.append(ids_to_job[upstream_id])\n\n            run_command = make_command(command_uri, self.tmp_dir)\n            job_id = self.submit(\n                command_config.command_type,\n                command_config.split_id,\n                command_def.experiment_id,\n                run_command,\n                parent_job_ids,\n                utilizes_gpu=command_config.utilizes_gpu())\n\n            ids_to_job[command_id] = job_id\n\n    def _dry_run(self, command_dag):\n        """"""Runs all commands.""""""\n        click.echo(\n            click.style(\n                \'\\n{} commands to be issued:\'.format(\n                    self.execution_environment),\n                fg=\'green\',\n                bold=True,\n                underline=True))\n        for command_id in command_dag.get_sorted_command_ids():\n            command_def = command_dag.get_command_definition(command_id)\n            command_config = command_def.command_config\n            command_root_uri = command_config.root_uri\n            command_basename = \'command-config-{}.json\'.format(\n                command_config.split_id)\n            command_uri = os.path.join(command_root_uri, command_basename)\n            run_command = make_command(command_uri, self.tmp_dir)\n            click.echo(\'  {}\'.format(run_command))\n'"
rastervision/task/__init__.py,0,b'# flake8: noqa\n\nfrom rastervision.task.task import *\nfrom rastervision.task.task_config import *\nfrom rastervision.task.object_detection import *\nfrom rastervision.task.object_detection_config import *\nfrom rastervision.task.chip_classification import *\nfrom rastervision.task.chip_classification_config import *\nfrom rastervision.task.semantic_segmentation import *\nfrom rastervision.task.semantic_segmentation_config import *\n'
rastervision/task/api.py,0,"b""# flake8: noqa\n\n# Registry Keys\n\nTASK = 'TASK'\n\nOBJECT_DETECTION = 'OBJECT_DETECTION'\nCHIP_CLASSIFICATION = 'CHIP_CLASSIFICATION'\nSEMANTIC_SEGMENTATION = 'SEMANTIC_SEGMENTATION'\n\nfrom .task_config import TaskConfig\n"""
rastervision/task/chip_classification.py,0,"b""from os.path import join\n\nimport numpy as np\nfrom PIL import Image, ImageDraw\n\nfrom rastervision.rv_config import RVConfig\nfrom rastervision.core import Box\nfrom rastervision.task import Task\nfrom rastervision.utils.files import (get_local_path, upload_or_copy, make_dir)\n\n\ndef draw_debug_predict_image(scene, class_map):\n    img = scene.raster_source.get_image_array()\n    img = Image.fromarray(img)\n    draw = ImageDraw.Draw(img, 'RGB')\n    labels = scene.prediction_label_store.get_labels()\n    line_width = 4\n    default_colors = [\n        'red', 'orange', 'yellow', 'green', 'brown', 'pink', 'purple'\n    ]\n    for cell, class_id in zip(labels.get_cells(), labels.get_class_ids()):\n        cell = cell.make_eroded(line_width // 2)\n        coords = cell.geojson_coordinates()\n        color = class_map.get_by_id(class_id).color\n        if color is None:\n            color = default_colors[(class_id - 1) % len(default_colors)]\n        draw.line(coords, fill=color, width=line_width)\n    return img\n\n\nclass ChipClassification(Task):\n    def get_train_windows(self, scene):\n        result = []\n        extent = scene.raster_source.get_extent()\n        chip_size = self.config.chip_size\n        stride = chip_size\n        windows = extent.get_windows(chip_size, stride)\n        if scene.aoi_polygons:\n            windows = Box.filter_by_aoi(windows, scene.aoi_polygons)\n        for window in windows:\n            chip = scene.raster_source.get_chip(window)\n            if np.sum(chip.ravel()) > 0:\n                result.append(window)\n        return result\n\n    def get_train_labels(self, window, scene):\n        return scene.ground_truth_label_source.get_labels(window=window)\n\n    def post_process_predictions(self, labels, scene):\n        return labels\n\n    def get_predict_windows(self, extent):\n        chip_size = self.config.chip_size\n        stride = chip_size\n        return extent.get_windows(chip_size, stride)\n\n    def save_debug_predict_image(self, scene, debug_dir_uri):\n        img = draw_debug_predict_image(scene, self.config.class_map)\n        # Saving to a jpg leads to segfault for unknown reasons.\n        debug_image_uri = join(debug_dir_uri, scene.id + '.png')\n        with RVConfig.get_tmp_dir() as temp_dir:\n            debug_image_path = get_local_path(debug_image_uri, temp_dir)\n            make_dir(debug_image_path, use_dirname=True)\n            img.save(debug_image_path)\n            upload_or_copy(debug_image_path, debug_image_uri)\n"""
rastervision/task/chip_classification_config.py,0,"b'from copy import deepcopy\nfrom typing import (List, Dict, Tuple, Union)\n\nimport rastervision as rv\nfrom rastervision.task import ChipClassification\nfrom rastervision.core.class_map import (ClassMap, ClassItem)\nfrom rastervision.task import (TaskConfig, TaskConfigBuilder)\nfrom rastervision.protos.task_pb2 import TaskConfig as TaskConfigMsg\nfrom rastervision.protos.class_item_pb2 import ClassItem as ClassItemMsg\n\n\nclass ChipClassificationConfig(TaskConfig):\n    def __init__(self,\n                 class_map,\n                 predict_batch_size=10,\n                 predict_package_uri=None,\n                 debug=False,\n                 predict_debug_uri=None,\n                 chip_size=300):\n        super().__init__(rv.CHIP_CLASSIFICATION, predict_batch_size,\n                         predict_package_uri, debug, predict_debug_uri)\n        self.class_map = class_map\n        self.chip_size = chip_size\n\n    def create_task(self, backend):\n        return ChipClassification(self, backend)\n\n    def to_proto(self):\n        conf = TaskConfigMsg.ChipClassificationConfig(\n            chip_size=self.chip_size, class_items=self.class_map.to_proto())\n        return TaskConfigMsg(\n            task_type=rv.CHIP_CLASSIFICATION,\n            chip_classification_config=conf,\n            predict_package_uri=self.predict_package_uri)\n\n    def save_bundle_files(self, bundle_dir):\n        return (self, [])\n\n    def load_bundle_files(self, bundle_dir):\n        return self\n\n\nclass ChipClassificationConfigBuilder(TaskConfigBuilder):\n    def __init__(self, prev=None):\n        config = {}\n        if prev:\n            config = {\n                \'class_map\': prev.class_map,\n                \'chip_size\': prev.chip_size,\n                \'predict_batch_size\': prev.predict_batch_size,\n                \'predict_package_uri\': prev.predict_package_uri,\n                \'debug\': prev.debug,\n                \'predict_debug_uri\': prev.predict_debug_uri\n            }\n        super().__init__(ChipClassificationConfig, config)\n\n    def validate(self):\n        if \'class_map\' not in self.config:\n            raise rv.ConfigError(\'Class map required for this task. \'\n                                 \'Use ""with_classes""\')\n        if not isinstance(self.config[\'class_map\'], ClassMap):\n            raise rv.ConfigError(\n                \'Class map set with ""with_classes"" must be of type ClassMap, got {}\'.\n                format(type(self.config[\'class_map\'])))\n\n    def from_proto(self, msg):\n        b = super().from_proto(msg)\n        conf = msg.chip_classification_config\n        return b.with_classes(list(conf.class_items)) \\\n                .with_predict_package_uri(msg.predict_package_uri) \\\n                .with_chip_size(conf.chip_size)\n\n    def with_classes(\n            self, classes: Union[ClassMap, List[str], List[ClassItemMsg], List[\n                ClassItem], Dict[str, int], Dict[str, Tuple[int, str]]]):\n        """"""Set the classes for this task.\n\n            Args:\n                classes: Either a list of class names, a dict which\n                         maps class names to class ids, or a dict\n                         which maps class names to a tuple of (class_id, color),\n                         where color is a PIL color string.\n        """"""\n        b = deepcopy(self)\n        b.config[\'class_map\'] = ClassMap.construct_from(classes)\n        return b\n\n    def with_chip_size(self, chip_size):\n        """"""Set the chip_size for this task.\n\n        Note that some model implementations have a minimum size of input they\n        can handle. A value of > 200 is usually safe.\n\n        Args:\n            chip_size: (int) chip size in units of pixels\n        """"""\n        b = deepcopy(self)\n        b.config[\'chip_size\'] = chip_size\n        return b\n'"
rastervision/task/object_detection.py,0,"b""import numpy as np\nimport logging\n\nfrom rastervision.task import Task\nfrom rastervision.data import ObjectDetectionLabels\nfrom rastervision.core import Box\n\nlog = logging.getLogger(__name__)\n\n\ndef _make_chip_pos_windows(image_extent, label_store, chip_size):\n    chip_size = chip_size\n    pos_windows = []\n    boxes = label_store.get_labels().get_boxes()\n    done_boxes = set()\n\n    # Get a random window around each box. If a box was previously included\n    # in a window, then it is skipped.\n    for box in boxes:\n        if box.tuple_format() not in done_boxes:\n            # If this  object is bigger than the chip,\n            # don't use this box.\n            if chip_size < box.get_width() or chip_size < box.get_height():\n                log.warning('Label is larger than chip size: {} '\n                            'Skipping this label'.format(box.tuple_format()))\n                continue\n\n            window = box.make_random_square_container(chip_size)\n            pos_windows.append(window)\n\n            # Get boxes that lie completely within window\n            window_boxes = label_store.get_labels(window=window)\n            window_boxes = ObjectDetectionLabels.get_overlapping(\n                window_boxes, window, ioa_thresh=1.0)\n            window_boxes = window_boxes.get_boxes()\n            window_boxes = [box.tuple_format() for box in window_boxes]\n            done_boxes.update(window_boxes)\n\n    return pos_windows\n\n\ndef _make_label_pos_windows(image_extent, label_store, label_buffer):\n    pos_windows = []\n    for box in label_store.get_labels().get_boxes():\n        window = box.make_buffer(label_buffer, image_extent)\n        pos_windows.append(window)\n\n    return pos_windows\n\n\ndef make_pos_windows(image_extent, label_store, chip_size, window_method,\n                     label_buffer):\n    if window_method == 'label':\n        return _make_label_pos_windows(image_extent, label_store, label_buffer)\n    elif window_method == 'image':\n        return [image_extent.make_copy()]\n    else:\n        return _make_chip_pos_windows(image_extent, label_store, chip_size)\n\n\ndef make_neg_windows(raster_source, label_store, chip_size, nb_windows,\n                     max_attempts, filter_windows):\n    extent = raster_source.get_extent()\n    neg_windows = []\n    for _ in range(max_attempts):\n        for _ in range(max_attempts):\n            window = extent.make_random_square(chip_size)\n            if any(filter_windows([window])):\n                break\n        chip = raster_source.get_chip(window)\n        labels = ObjectDetectionLabels.get_overlapping(\n            label_store.get_labels(), window, ioa_thresh=0.2)\n\n        # If no labels and not blank, append the chip\n        if len(labels) == 0 and np.sum(chip.ravel()) > 0:\n            neg_windows.append(window)\n\n        if len(neg_windows) == nb_windows:\n            break\n\n    return list(neg_windows)\n\n\nclass ObjectDetection(Task):\n    def get_train_windows(self, scene):\n        raster_source = scene.raster_source\n        label_store = scene.ground_truth_label_source\n\n        def filter_windows(windows):\n            if scene.aoi_polygons:\n                windows = Box.filter_by_aoi(windows, scene.aoi_polygons)\n            return windows\n\n        window_method = self.config.chip_options.window_method\n        if window_method == 'sliding':\n            chip_size = self.config.chip_size\n            stride = chip_size\n            return list(\n                filter_windows((raster_source.get_extent().get_windows(\n                    chip_size, stride))))\n\n        # Make positive windows which contain labels.\n        pos_windows = filter_windows(\n            make_pos_windows(raster_source.get_extent(), label_store,\n                             self.config.chip_size,\n                             self.config.chip_options.window_method,\n                             self.config.chip_options.label_buffer))\n        nb_pos_windows = len(pos_windows)\n\n        # Make negative windows which do not contain labels.\n        # Generate randow windows and save the ones that don't contain\n        # any labels. It may take many attempts to generate a single\n        # negative window, and could get into an infinite loop in some cases,\n        # so we cap the number of attempts.\n        if nb_pos_windows:\n            nb_neg_windows = round(\n                self.config.chip_options.neg_ratio * nb_pos_windows)\n        else:\n            nb_neg_windows = 100  # just make some\n        max_attempts = 100 * nb_neg_windows\n        neg_windows = make_neg_windows(raster_source, label_store,\n                                       self.config.chip_size, nb_neg_windows,\n                                       max_attempts, filter_windows)\n\n        return pos_windows + neg_windows\n\n    def get_train_labels(self, window, scene):\n        window_labels = scene.ground_truth_label_source.get_labels(\n            window=window)\n        return ObjectDetectionLabels.get_overlapping(\n            window_labels,\n            window,\n            ioa_thresh=self.config.chip_options.ioa_thresh,\n            clip=True)\n\n    def get_predict_windows(self, extent):\n        chip_size = self.config.chip_size\n        stride = chip_size // 2\n        return extent.get_windows(chip_size, stride)\n\n    def post_process_predictions(self, labels, scene):\n        return ObjectDetectionLabels.prune_duplicates(\n            labels,\n            score_thresh=self.config.predict_options.score_thresh,\n            merge_thresh=self.config.predict_options.merge_thresh)\n\n    def save_debug_predict_image(self, scene, debug_dir_uri):\n        # TODO implement this\n        pass\n"""
rastervision/task/object_detection_config.py,0,"b'from copy import deepcopy\nfrom typing import (List, Dict, Tuple, Union)\n\nimport rastervision as rv\nfrom rastervision.task import ObjectDetection\nfrom rastervision.core.class_map import (ClassMap, ClassItem)\nfrom rastervision.task import (TaskConfig, TaskConfigBuilder)\nfrom rastervision.protos.task_pb2 import TaskConfig as TaskConfigMsg\nfrom rastervision.protos.class_item_pb2 import ClassItem as ClassItemMsg\n\n\nclass ObjectDetectionConfig(TaskConfig):\n    class ChipOptions:\n        def __init__(self,\n                     neg_ratio=1,\n                     ioa_thresh=0.8,\n                     window_method=\'chip\',\n                     label_buffer=0.0):\n            self.neg_ratio = neg_ratio\n            self.ioa_thresh = ioa_thresh\n            self.window_method = window_method\n            self.label_buffer = label_buffer\n\n    class PredictOptions:\n        def __init__(self, merge_thresh=0.5, score_thresh=0.5):\n            self.merge_thresh = merge_thresh\n            self.score_thresh = score_thresh\n\n    def __init__(self,\n                 class_map,\n                 predict_batch_size=10,\n                 predict_package_uri=None,\n                 debug=True,\n                 predict_debug_uri=None,\n                 chip_size=300,\n                 chip_options=ChipOptions(),\n                 predict_options=PredictOptions()):\n        super().__init__(rv.OBJECT_DETECTION, predict_batch_size,\n                         predict_package_uri, debug, predict_debug_uri)\n        self.class_map = class_map\n        self.chip_size = chip_size\n        self.chip_options = chip_options\n        self.predict_options = predict_options\n\n    def create_task(self, backend):\n        return ObjectDetection(self, backend)\n\n    def to_proto(self):\n        msg = super().to_proto()\n        chip_options = TaskConfigMsg.ObjectDetectionConfig.ChipOptions(\n            neg_ratio=self.chip_options.neg_ratio,\n            ioa_thresh=self.chip_options.ioa_thresh,\n            window_method=self.chip_options.window_method,\n            label_buffer=self.chip_options.label_buffer)\n\n        predict_options = TaskConfigMsg.ObjectDetectionConfig.PredictOptions(\n            merge_thresh=self.predict_options.merge_thresh,\n            score_thresh=self.predict_options.score_thresh)\n\n        conf = TaskConfigMsg.ObjectDetectionConfig(\n            chip_size=self.chip_size,\n            class_items=self.class_map.to_proto(),\n            chip_options=chip_options,\n            predict_options=predict_options)\n        msg.MergeFrom(\n            TaskConfigMsg(\n                object_detection_config=conf,\n                predict_package_uri=self.predict_package_uri))\n\n        return msg\n\n    def save_bundle_files(self, bundle_dir):\n        return (self, [])\n\n    def load_bundle_files(self, bundle_dir):\n        return self\n\n\nclass ObjectDetectionConfigBuilder(TaskConfigBuilder):\n    def __init__(self, prev=None):\n        config = {}\n        if prev:\n            config = {\n                \'predict_batch_size\': prev.predict_batch_size,\n                \'predict_package_uri\': prev.predict_package_uri,\n                \'debug\': prev.debug,\n                \'predict_debug_uri\': prev.predict_debug_uri,\n                \'class_map\': prev.class_map,\n                \'chip_size\': prev.chip_size,\n                \'chip_options\': prev.chip_options,\n                \'predict_options\': prev.predict_options\n            }\n        super().__init__(ObjectDetectionConfig, config)\n\n    def validate(self):\n        if \'class_map\' not in self.config:\n            raise rv.ConfigError(\'Class map required for this task. \'\n                                 \'Use ""with_classes""\')\n        if not isinstance(self.config[\'class_map\'], ClassMap):\n            raise rv.ConfigError(\n                \'Class map set with ""with_classes"" must be of type ClassMap, got {}\'.\n                format(type(self.config[\'class_map\'])))\n\n    def from_proto(self, msg):\n        b = super().from_proto(msg)\n        conf = msg.object_detection_config\n\n        return b.with_classes(list(conf.class_items)) \\\n                .with_predict_package_uri(msg.predict_package_uri) \\\n                .with_chip_size(conf.chip_size) \\\n                .with_chip_options(neg_ratio=conf.chip_options.neg_ratio,\n                                   ioa_thresh=conf.chip_options.ioa_thresh,\n                                   window_method=conf.chip_options.window_method,\n                                   label_buffer=conf.chip_options.label_buffer) \\\n                .with_predict_options(merge_thresh=conf.predict_options.merge_thresh,\n                                      score_thresh=conf.predict_options.score_thresh)\n\n    def with_classes(\n            self, classes: Union[ClassMap, List[str], List[ClassItemMsg], List[\n                ClassItem], Dict[str, int], Dict[str, Tuple[int, str]]]):\n        """"""Set the classes for this task.\n\n            Args:\n                classes: Either a list of class names, a dict which\n                         maps class names to class ids, or a dict\n                         which maps class names to a tuple of (class_id, color),\n                         where color is a PIL color string.\n        """"""\n        b = deepcopy(self)\n        b.config[\'class_map\'] = ClassMap.construct_from(classes)\n        return b\n\n    def with_chip_size(self, chip_size):\n        """"""Set the chip_size for this task.\n\n        Note that some model implementations have a minimum size of input they\n        can handle. A value of > 200 is usually safe.\n\n        Args:\n            chip_size: (int) chip size in units of pixels\n        """"""\n        b = deepcopy(self)\n        b.config[\'chip_size\'] = chip_size\n        return b\n\n    def with_chip_options(self,\n                          neg_ratio=1,\n                          ioa_thresh=0.8,\n                          window_method=\'chip\',\n                          label_buffer=0.0):\n        """"""Sets object detection configurations for the Chip command\n\n        Args:\n            neg_ratio: The ratio of negative chips (those containing no bounding\n                boxes) to positive chips. This can be useful if the statistics\n                of the background is different in positive chips. For example,\n                in car detection, the positive chips will always contain roads,\n                but no examples of rooftops since cars tend to not be near\n                rooftops. This option is not used when window_method is\n                `sliding`.\n\n            ioa_thresh: When a box is partially outside of a training chip, it\n                is not clear if (a clipped version) of the box should be\n                included in the chip. If the IOA (intersection over area) of the\n                box with the chip is greater than ioa_thresh, it is included in\n                the chip.\n\n            window_method: Different models in the Object Detection API have\n                different inputs. Some models allow variable size inputs so\n                several methods of building training data are required\n\n                Valid values are:\n                - chip (default)\n                - label\n                   - each label\'s bounding box is the positive window\n                - image\n                   - each image is the positive window\n                - sliding\n                   - each image is from a sliding window with 50% overlap\n\n            label_buffer:\n                If method is ""label"", the positive window can be buffered.\n                If value is >= 0. and < 1., the value is treated as a percentage\n                If value is >= 1., the value is treated in number of pixels\n        """"""\n        b = deepcopy(self)\n        b.config[\'chip_options\'] = ObjectDetectionConfig.ChipOptions(\n            neg_ratio=neg_ratio,\n            ioa_thresh=ioa_thresh,\n            window_method=window_method,\n            label_buffer=label_buffer)\n        return b\n\n    def with_predict_options(self, merge_thresh=0.5, score_thresh=0.5):\n        """"""Prediction options for this task.\n\n        Args:\n           merge_thresh: If predicted boxes have an IOA (intersection over area)\n                         greater than merge_thresh, then they are merged into a\n                         single box during postprocessing. This is needed since\n                         the sliding window approach results in some false\n                         duplicates.\n\n           score_thresh: Predicted boxes are only output if their score is\n                         above score_thresh.\n        """"""\n        b = deepcopy(self)\n        b.config[\'predict_options\'] = ObjectDetectionConfig.PredictOptions(\n            merge_thresh=merge_thresh, score_thresh=score_thresh)\n        return b\n'"
rastervision/task/semantic_segmentation.py,0,"b'from typing import List\nimport logging\n\nimport numpy as np\n\nfrom .task import Task\nfrom rastervision.core.box import Box\nfrom rastervision.data.scene import Scene\nfrom rastervision.data.label import SemanticSegmentationLabels\nfrom rastervision.core.training_data import TrainingData\n\nTRAIN = \'train\'\nVALIDATION = \'validation\'\n\nlog = logging.getLogger(__name__)\n\n\ndef get_random_sample_train_windows(label_store, chip_size, class_map, extent,\n                                    chip_options, filter_windows):\n    prob = chip_options.negative_survival_probability\n    target_count_threshold = chip_options.target_count_threshold\n    target_classes = chip_options.target_classes\n    chips_per_scene = chip_options.chips_per_scene\n\n    if not target_classes:\n        all_class_ids = [item.id for item in class_map.get_items()]\n        target_classes = all_class_ids\n\n    windows = []\n    attempts = 0\n    while (attempts < chips_per_scene):\n        candidate_window = extent.make_random_square(chip_size)\n        if not filter_windows([candidate_window]):\n            continue\n        attempts = attempts + 1\n\n        if (prob >= 1.0):\n            windows.append(candidate_window)\n        elif attempts == chips_per_scene and len(windows) == 0:\n            windows.append(candidate_window)\n        else:\n            good = label_store.enough_target_pixels(\n                candidate_window, target_count_threshold, target_classes)\n            if good or (np.random.rand() < prob):\n                windows.append(candidate_window)\n\n    return windows\n\n\nclass SemanticSegmentation(Task):\n    """"""Task-derived type that implements the semantic segmentation task.""""""\n\n    def get_train_windows(self, scene: Scene) -> List[Box]:\n        """"""Get training windows covering a scene.\n\n        Args:\n             scene: The scene over-which windows are to be generated.\n\n        Returns:\n             A list of windows, list(Box)\n\n        """"""\n        raster_source = scene.raster_source\n        extent = raster_source.get_extent()\n        label_source = scene.ground_truth_label_source\n        chip_size = self.config.chip_size\n        chip_options = self.config.chip_options\n\n        def filter_windows(windows):\n            if scene.aoi_polygons:\n                windows = Box.filter_by_aoi(windows, scene.aoi_polygons)\n\n            if 0 in self.config.class_map.get_keys():\n                filt_windows = []\n                for w in windows:\n                    label_arr = label_source.get_labels(w).get_label_arr(w)\n                    ignore_inds = label_arr.ravel() == 0\n                    if np.all(ignore_inds):\n                        pass\n                    else:\n                        filt_windows.append(w)\n                windows = filt_windows\n            return windows\n\n        if chip_options.window_method == \'random_sample\':\n            return get_random_sample_train_windows(\n                label_source, chip_size, self.config.class_map, extent,\n                chip_options, filter_windows)\n        elif chip_options.window_method == \'sliding\':\n            stride = chip_options.stride\n            if stride is None:\n                stride = chip_size / 2\n            stride = int(round(stride))\n\n            return list(\n                filter_windows((extent.get_windows(chip_size, stride))))\n\n    def make_chips(self, train_scenes, validation_scenes, augmentors, tmp_dir):\n        """"""Make training chips.\n\n        Convert Scenes with a ground_truth_label_store into training\n        chips in MLBackend-specific format, and write to URI specified in\n        options.\n\n        Args:\n            train_scenes: list of Scenes\n            validation_scenes: list of Scenes\n                (that is disjoint from train_scenes)\n            augmentors: Augmentors used to augment training data\n        """"""\n\n        def _process_scene(scene, type_, augment):\n            with scene.activate():\n                data = TrainingData()\n                log.info(\'Making {} chips for scene: {}\'.format(\n                    type_, scene.id))\n                windows = self.get_train_windows(scene)\n                for window in windows:\n                    chip = scene.raster_source.get_chip(window)\n                    labels = self.get_train_labels(window, scene)\n\n                    # If chip has ignore labels, fill in those pixels with\n                    # nodata.\n                    label_arr = labels.get_label_arr(window)\n                    zero_inds = label_arr.ravel() == 0\n                    chip_shape = chip.shape\n                    if np.any(zero_inds):\n                        chip = np.reshape(chip, (-1, chip.shape[2]))\n                        chip[zero_inds, :] = 0\n                        chip = np.reshape(chip, chip_shape)\n\n                    data.append(chip, window, labels)\n                # Shuffle data so the first N samples which are displayed in\n                # Tensorboard are more diverse.\n                data.shuffle()\n\n                # Process augmentation\n                if augment:\n                    for augmentor in augmentors:\n                        data = augmentor.process(data, tmp_dir)\n\n                return self.backend.process_scene_data(scene, data, tmp_dir)\n\n        def _process_scenes(scenes, type_, augment):\n            return [_process_scene(scene, type_, augment) for scene in scenes]\n\n        processed_training_results = _process_scenes(\n            train_scenes, TRAIN, augment=True)\n        processed_validation_results = _process_scenes(\n            validation_scenes, VALIDATION, augment=False)\n\n        self.backend.process_sceneset_results(\n            processed_training_results, processed_validation_results, tmp_dir)\n\n    def get_train_labels(self, window: Box, scene: Scene) -> np.ndarray:\n        """"""Get the training labels for the given window in the given scene.\n\n        Args:\n             window: The window over-which the labels are to be\n                  retrieved.\n             scene: The scene from-which the window of labels is to be\n                  extracted.\n\n        Returns:\n             An appropriately-shaped 2d np.ndarray with the labels\n             encoded as packed pixels.\n\n        """"""\n        label_store = scene.ground_truth_label_source\n        return label_store.get_labels(window)\n\n    def get_predict_windows(self, extent: Box) -> List[Box]:\n        """"""Get windows over-which predictions will be calculated.\n\n        Args:\n             extent: The overall extent of the area.\n\n        Returns:\n             An sequence of windows.\n\n        """"""\n        chip_size = self.config.predict_chip_size\n        return extent.get_windows(chip_size, chip_size)\n\n    def post_process_predictions(self, labels, scene):\n        return labels\n\n    def save_debug_predict_image(self, scene, debug_dir_uri):\n        # TODO implement this\n        pass\n\n    def predict_scene(self, scene, tmp_dir):\n        """"""Predict on a single scene, and return the labels.""""""\n        log.info(\'Making predictions for scene\')\n        raster_source = scene.raster_source\n        windows = self.get_predict_windows(raster_source.get_extent())\n\n        def label_fn(window):\n            chip = raster_source.get_chip(window)\n            labels = self.backend.predict([chip], [window], tmp_dir)\n            label_arr = labels.get_label_arr(window)\n\n            # Set NODATA pixels in imagery to predicted value of 0 (ie. ignore)\n            label_arr[np.sum(chip, axis=2) == 0] = 0\n\n            print(\'.\', end=\'\', flush=True)\n            return label_arr\n\n        return SemanticSegmentationLabels(windows, label_fn)\n'"
rastervision/task/semantic_segmentation_config.py,0,"b'from copy import deepcopy\nfrom typing import (List, Dict, Tuple, Union)\n\nimport rastervision as rv\nfrom rastervision.task import SemanticSegmentation\nfrom rastervision.core.class_map import (ClassMap, ClassItem)\nfrom rastervision.task import (TaskConfig, TaskConfigBuilder)\nfrom rastervision.protos.task_pb2 import TaskConfig as TaskConfigMsg\nfrom rastervision.protos.class_item_pb2 import ClassItem as ClassItemMsg\n\n\nclass SemanticSegmentationConfig(TaskConfig):\n    class ChipOptions:\n        def __init__(self,\n                     window_method=\'random_sample\',\n                     target_classes=None,\n                     debug_chip_probability=0.25,\n                     negative_survival_probability=1.0,\n                     chips_per_scene=1000,\n                     target_count_threshold=1000,\n                     stride=None):\n            self.window_method = window_method\n            self.target_classes = target_classes\n            self.debug_chip_probability = debug_chip_probability\n            self.negative_survival_probability = negative_survival_probability\n            self.chips_per_scene = chips_per_scene\n            self.target_count_threshold = target_count_threshold\n            self.stride = stride\n\n    def __init__(self,\n                 class_map,\n                 predict_batch_size=10,\n                 predict_package_uri=None,\n                 debug=True,\n                 chip_size=300,\n                 predict_chip_size=300,\n                 chip_options=None):\n        super().__init__(rv.SEMANTIC_SEGMENTATION, predict_batch_size,\n                         predict_package_uri, debug)\n        self.class_map = class_map\n        self.chip_size = chip_size\n        self.predict_chip_size = predict_chip_size\n        if chip_options is None:\n            chip_options = SemanticSegmentationConfig.ChipOptions()\n        self.chip_options = chip_options\n\n    def save_bundle_files(self, bundle_dir):\n        return (self, [])\n\n    def load_bundle_files(self, bundle_dir):\n        return self\n\n    def create_task(self, backend):\n        return SemanticSegmentation(self, backend)\n\n    def to_proto(self):\n        msg = super().to_proto()\n        chip_options = TaskConfigMsg.SemanticSegmentationConfig.ChipOptions(\n            window_method=self.chip_options.window_method,\n            target_classes=self.chip_options.target_classes,\n            debug_chip_probability=self.chip_options.debug_chip_probability,\n            negative_survival_probability=self.chip_options.\n            negative_survival_probability,\n            chips_per_scene=self.chip_options.chips_per_scene,\n            target_count_threshold=self.chip_options.target_count_threshold,\n            stride=self.chip_options.stride)\n\n        conf = TaskConfigMsg.SemanticSegmentationConfig(\n            chip_size=self.chip_size,\n            predict_chip_size=self.predict_chip_size,\n            class_items=self.class_map.to_proto(),\n            chip_options=chip_options)\n        msg.MergeFrom(\n            TaskConfigMsg(\n                semantic_segmentation_config=conf,\n                predict_package_uri=self.predict_package_uri))\n\n        return msg\n\n\nclass SemanticSegmentationConfigBuilder(TaskConfigBuilder):\n    def __init__(self, prev=None):\n        config = {}\n        if prev:\n            config = {\n                \'predict_batch_size\': prev.predict_batch_size,\n                \'predict_package_uri\': prev.predict_package_uri,\n                \'debug\': prev.debug,\n                \'class_map\': prev.class_map,\n                \'chip_size\': prev.chip_size,\n                \'predict_chip_size\': prev.predict_chip_size,\n                \'chip_options\': prev.chip_options\n            }\n        super().__init__(SemanticSegmentationConfig, config)\n\n    def from_proto(self, msg):\n        conf = msg.semantic_segmentation_config\n\n        negative_survival_probability = conf.chip_options \\\n                                            .negative_survival_probability\n\n        stride = conf.chip_options.stride\n        if stride == 0:\n            stride = None\n\n        # for backward compatibility\n        predict_chip_size = conf.predict_chip_size\n        if predict_chip_size == 0:\n            predict_chip_size = conf.chip_size\n\n        return self.with_classes(list(conf.class_items)) \\\n                .with_predict_batch_size(msg.predict_batch_size) \\\n                .with_predict_package_uri(msg.predict_package_uri) \\\n                .with_debug(msg.debug) \\\n                .with_chip_size(conf.chip_size) \\\n                .with_predict_chip_size(predict_chip_size) \\\n                .with_chip_options(\n                    window_method=conf.chip_options.window_method,\n                    target_classes=list(conf.chip_options.target_classes),\n                    debug_chip_probability=conf.chip_options.debug_chip_probability,\n                    negative_survival_probability=negative_survival_probability,\n                    chips_per_scene=conf.chip_options.chips_per_scene,\n                    target_count_threshold=conf.chip_options.target_count_threshold,\n                    stride=stride)\n\n    def validate(self):\n        super().validate()\n        # Segmentation masks are stored as uint8 to save space, so can only handle 256\n        # classes. If this is really needed, we can add an option for saving with uint16.\n        max_classes = 256\n        if len(self.config[\'class_map\']) > max_classes:\n            raise rv.ConfigError(\n                \'Cannot use more than {} classes with semantic segmentation.\'.\n                format(max_classes))\n\n    def with_classes(\n            self, classes: Union[ClassMap, List[str], List[ClassItemMsg], List[\n                ClassItem], Dict[str, int], Dict[str, Tuple[int, str]]]):\n        """"""Set the classes for this task.\n\n            Args:\n                classes: Either a list of class names, a dict which\n                         maps class names to class ids, or a dict\n                         which maps class names to a tuple of (class_id, color),\n                         where color is a PIL color string.\n        """"""\n        b = deepcopy(self)\n        b.config[\'class_map\'] = ClassMap.construct_from(classes)\n        return b\n\n    def with_chip_size(self, chip_size):\n        """"""Set the chip_size for this task.\n\n        Note that some model implementations have a minimum size of input they\n        can handle. A value of > 200 is usually safe.\n\n        Args:\n            chip_size: (int) chip size in units of pixels\n        """"""\n        b = deepcopy(self)\n        b.config[\'chip_size\'] = chip_size\n        return b\n\n    def with_predict_chip_size(self, chip_size):\n        """"""Set the chip_size to use only at prediction time for this task.\n\n            Args:\n                chip_size: Integer value chip size\n        """"""\n        b = deepcopy(self)\n        b.config[\'predict_chip_size\'] = chip_size\n        return b\n\n    def with_chip_options(self,\n                          window_method=\'random_sample\',\n                          target_classes=None,\n                          debug_chip_probability=0.25,\n                          negative_survival_probability=1.0,\n                          chips_per_scene=1000,\n                          target_count_threshold=1000,\n                          stride=None):\n        """"""Sets semantic segmentation configurations for the Chip command.\n\n        Args:\n            window_method: Window method to use for chipping. Options are:\n                random_sample, sliding\n            target_classes: list of class ids considered as targets (ie. those\n                to prioritize when creating chips) which is only used in\n                conjunction with the target_count_threshold and\n                negative_survival_probability options.\n            debug_chip_probability: probability of generating a debug chip.\n                Applies to the \'random_sample\' window method.\n            negative_survival_probability: probability that a sampled negative\n                chip will be utilized if it does not contain more pixels than\n                target_count_threshold. Applies to the \'random_sample\' window method.\n            chips_per_scene: number of chips to generate per scene. Applies to\n                the \'random_sample\' window method.\n            target_count_threshold: minimum number of pixels covering\n                target_classes that a chip must have. Applies to the\n                \'random_sample\' window method.\n            stride: Stride of windows across image. Defaults to half the chip\n                size. Applies to the \'sliding_window\' method.\n\n        Returns:\n            SemanticSegmentationConfigBuilder\n        """"""\n        b = deepcopy(self)\n\n        b.config[\'chip_options\'] = SemanticSegmentationConfig.ChipOptions(\n            window_method=window_method,\n            target_classes=target_classes,\n            debug_chip_probability=debug_chip_probability,\n            negative_survival_probability=negative_survival_probability,\n            chips_per_scene=chips_per_scene,\n            target_count_threshold=target_count_threshold,\n            stride=stride)\n        return b\n'"
rastervision/task/task.py,0,"b'from abc import abstractmethod\n\nimport numpy as np\nimport logging\n\nfrom rastervision.core.training_data import TrainingData\n\n# TODO: DRY... same keys as in ml_backends/tf_object_detection_api.py\nTRAIN = \'train\'\nVALIDATION = \'validation\'\n\nlog = logging.getLogger(__name__)\n\n\nclass Task(object):\n    """"""Functionality for a specific machine learning task.\n\n    This should be subclassed to add a new task, such as object detection\n    """"""\n\n    def __init__(self, task_config, backend):\n        """"""Construct a new Task.\n\n        Args:\n            task_config: TaskConfig\n            backend: Backend\n        """"""\n        self.config = task_config\n        self.backend = backend\n\n    @abstractmethod\n    def get_train_windows(self, scene):\n        """"""Return the training windows for a Scene.\n\n        The training windows represent the spatial extent of the training\n        chips to generate.\n\n        Args:\n            scene: Scene to generate windows for\n\n        Returns:\n            list of Boxes\n        """"""\n        pass\n\n    @abstractmethod\n    def get_train_labels(self, window, scene):\n        """"""Return the training labels in a window for a scene.\n\n        Args:\n            window: Box\n            scene: Scene\n\n        Returns:\n            Labels that lie within window\n        """"""\n        pass\n\n    @abstractmethod\n    def post_process_predictions(self, labels, scene):\n        """"""Runs a post-processing step on labels at end of prediction.\n\n        Returns:\n            Labels\n        """"""\n        pass\n\n    @abstractmethod\n    def get_predict_windows(self, extent):\n        """"""Return windows to compute predictions for.\n\n        Args:\n            extent: Box representing extent of RasterSource\n\n        Returns:\n            list of Boxes\n        """"""\n        pass\n\n    @abstractmethod\n    def save_debug_predict_image(self, scene, debug_dir_uri):\n        """"""Save a debug image of predictions.\n\n        This writes to debug_dir_uri/<scene.id>.jpg.\n        """"""\n        pass\n\n    def make_chips(self, train_scenes, validation_scenes, augmentors, tmp_dir):\n        """"""Make training chips.\n\n        Convert Scenes with a ground_truth_label_store into training\n        chips in MLBackend-specific format, and write to URI specified in\n        options.\n\n        Args:\n            train_scenes: list of Scenes\n            validation_scenes: list of Scenes\n                (that is disjoint from train_scenes)\n            augmentors: Augmentors used to augment training data\n        """"""\n\n        def _process_scene(scene, type_, augment):\n            with scene.activate():\n                data = TrainingData()\n                log.info(\'Making {} chips for scene: {}\'.format(\n                    type_, scene.id))\n                windows = self.get_train_windows(scene)\n                for window in windows:\n                    chip = scene.raster_source.get_chip(window)\n                    labels = self.get_train_labels(window, scene)\n                    data.append(chip, window, labels)\n                # Shuffle data so the first N samples which are displayed in\n                # Tensorboard are more diverse.\n                data.shuffle()\n\n                # Process augmentation\n                if augment:\n                    for augmentor in augmentors:\n                        data = augmentor.process(data, tmp_dir)\n\n                return self.backend.process_scene_data(scene, data, tmp_dir)\n\n        def _process_scenes(scenes, type_, augment):\n            return [_process_scene(scene, type_, augment) for scene in scenes]\n\n        processed_training_results = _process_scenes(\n            train_scenes, TRAIN, augment=True)\n        processed_validation_results = _process_scenes(\n            validation_scenes, VALIDATION, augment=False)\n\n        self.backend.process_sceneset_results(\n            processed_training_results, processed_validation_results, tmp_dir)\n\n    def train(self, tmp_dir):\n        """"""Train a model.\n        """"""\n        self.backend.train(tmp_dir)\n\n    def predict(self, scenes, tmp_dir):\n        """"""Make predictions for scenes.\n\n        The predictions are saved to the prediction_label_store in\n        each scene.\n\n        Args:\n            scenes: list of Scenes\n        """"""\n        self.backend.load_model(tmp_dir)\n\n        for scene in scenes:\n            with scene.activate():\n                labels = self.predict_scene(scene, tmp_dir)\n                label_store = scene.prediction_label_store\n                label_store.save(labels)\n\n                if self.config.debug and self.config.predict_debug_uri:\n                    self.save_debug_predict_image(\n                        scene, self.config.predict_debug_uri)\n\n    def predict_scene(self, scene, tmp_dir):\n        """"""Predict on a single scene, and return the labels.""""""\n        log.info(\'Making predictions for scene\')\n        raster_source = scene.raster_source\n        label_store = scene.prediction_label_store\n        labels = label_store.empty_labels()\n\n        windows = self.get_predict_windows(raster_source.get_extent())\n\n        def predict_batch(predict_chips, predict_windows):\n            nonlocal labels\n            new_labels = self.backend.predict(\n                np.array(predict_chips), predict_windows, tmp_dir)\n            labels += new_labels\n            print(\'.\' * len(predict_chips), end=\'\', flush=True)\n\n        batch_chips, batch_windows = [], []\n        for window in windows:\n            chip = raster_source.get_chip(window)\n            if np.any(chip):\n                batch_chips.append(chip)\n                batch_windows.append(window)\n\n            # Predict on batch\n            if len(batch_chips) >= self.config.predict_batch_size:\n                predict_batch(batch_chips, batch_windows)\n                batch_chips, batch_windows = [], []\n        print()\n\n        # Predict on remaining batch\n        if len(batch_chips) > 0:\n            predict_batch(batch_chips, batch_windows)\n\n        return self.post_process_predictions(labels, scene)\n'"
rastervision/task/task_config.py,0,"b'from abc import abstractmethod\nimport os\nfrom copy import deepcopy\n\nimport rastervision as rv\nfrom rastervision.core import (Config, ConfigBuilder, BundledConfigMixin)\nfrom rastervision.protos.task_pb2 import TaskConfig as TaskConfigMsg\n\n\nclass TaskConfig(BundledConfigMixin, Config):\n    def __init__(self,\n                 task_type,\n                 predict_batch_size=10,\n                 predict_package_uri=None,\n                 debug=True,\n                 predict_debug_uri=None):\n        self.task_type = task_type\n        self.predict_batch_size = predict_batch_size\n        self.predict_package_uri = predict_package_uri\n        self.debug = debug\n        self.predict_debug_uri = predict_debug_uri\n\n    @abstractmethod\n    def create_task(self, backend):\n        """"""Create the Task that this configuration represents\n\n           Args:\n              backend: The backend to be used by the task.\n        """"""\n        pass\n\n    def to_builder(self):\n        return rv._registry.get_config_builder(rv.TASK, self.task_type)(self)\n\n    def to_proto(self):\n        return TaskConfigMsg(\n            task_type=self.task_type,\n            predict_batch_size=self.predict_batch_size,\n            predict_package_uri=self.predict_package_uri,\n            debug=self.debug,\n            predict_debug_uri=self.predict_debug_uri)\n\n    @staticmethod\n    def builder(task_type):\n        return rv._registry.get_config_builder(rv.TASK, task_type)()\n\n    @staticmethod\n    def from_proto(msg):\n        """"""Creates a TaskConfig from the specificed protobuf message\n        """"""\n        return rv._registry.get_config_builder(rv.TASK, msg.task_type)() \\\n                           .from_proto(msg) \\\n                           .build()\n\n    def update_for_command(self, command_type, experiment_config,\n                           context=None):\n        if command_type == rv.BUNDLE:\n            if not self.predict_package_uri:\n                self.predict_package_uri = os.path.join(\n                    experiment_config.bundle_uri, \'predict_package.zip\')\n\n    def report_io(self, command_type, io_def):\n        if command_type == rv.BUNDLE:\n            io_def.add_output(self.predict_package_uri)\n\n\nclass TaskConfigBuilder(ConfigBuilder):\n    def from_proto(self, msg):\n        b = self.with_predict_batch_size(msg.predict_batch_size)\n        b = b.with_predict_package_uri(msg.predict_package_uri)\n        b = b.with_debug(msg.debug)\n        b = b.with_predict_debug_uri(msg.predict_debug_uri)\n        return b\n\n    def with_predict_batch_size(self, predict_batch_size):\n        """"""Sets the batch size to use during prediction.""""""\n        b = deepcopy(self)\n        b.config[\'predict_batch_size\'] = predict_batch_size\n        return b\n\n    def with_predict_package_uri(self, predict_package_uri):\n        """"""Sets the URI to save a predict package URI to during bundle.""""""\n        b = deepcopy(self)\n        b.config[\'predict_package_uri\'] = predict_package_uri\n        return b\n\n    def with_debug(self, debug):\n        """"""Flag for producing debug products.""""""\n        b = deepcopy(self)\n        b.config[\'debug\'] = debug\n        return b\n\n    def with_predict_debug_uri(self, predict_debug_uri):\n        """"""Set the directory to place prediction debug images""""""\n        b = deepcopy(self)\n        b.config[\'predict_debug_uri\'] = predict_debug_uri\n        return b\n'"
rastervision/utils/__init__.py,0,b''
rastervision/utils/files.py,0,"b'import os\nfrom os.path import join\nimport shutil\nimport gzip\nfrom threading import Timer\nimport time\nimport logging\nimport json\nimport zipfile\n\nfrom google.protobuf import json_format\n\nfrom rastervision.filesystem.filesystem import FileSystem\nfrom rastervision.filesystem.filesystem import ProtobufParseException\nfrom rastervision.filesystem.local_filesystem import make_dir\n\nlog = logging.getLogger(__name__)\n\n\ndef get_local_path(uri, download_dir, fs=None):\n    """"""Convert a URI into a corresponding local path.\n\n    If a uri is local, return it. If it\'s remote, we generate a path for it\n    within download_dir. For an S3 path of form s3://<bucket>/<key>, the path\n    is <download_dir>/s3/<bucket>/<key>.\n\n    Args:\n        uri: (string) URI of file\n        download_dir: (string) path to directory\n        fs: Optional FileSystem to use\n\n    Returns:\n        (string) a local path\n    """"""\n    if uri is None:\n        return None\n\n    if not fs:\n        fs = FileSystem.get_file_system(uri, \'r\')\n    path = fs.local_path(uri, download_dir)\n\n    return path\n\n\ndef sync_to_dir(src_dir_uri, dest_dir_uri, delete=False, fs=None):\n    """"""Synchronize a local to a local or remote directory.\n\n    Transfers files from source to destination directories so that the\n    destination has all the source files. If delete is True, also delete\n    files in the destination to match those in the source directory.\n\n    Args:\n        src_dir_uri: (string) URI of local source directory\n        dest_dir_uri: (string) URI of destination directory\n        delete: (bool)\n        fs: Optional FileSystem to use for destination\n    """"""\n    if not fs:\n        fs = FileSystem.get_file_system(dest_dir_uri, \'w\')\n    fs.sync_to_dir(src_dir_uri, dest_dir_uri, delete=delete)\n\n\ndef sync_from_dir(src_dir_uri, dest_dir_uri, delete=False, fs=None):\n    """"""Synchronize a local or remote directory to a local directory.\n\n    Transfers files from source to destination directories so that the\n    destination has all the source files. If delete is True, also delete\n    files in the destination to match those in the source directory.\n\n    Args:\n        src_dir_uri: (string) URI of source directory\n        dest_dir_uri: (string) URI of local destination directory\n        delete: (bool)\n        fs: Optional FileSystem to use\n    """"""\n    if not fs:\n        fs = FileSystem.get_file_system(src_dir_uri, \'r\')\n    fs.sync_from_dir(src_dir_uri, dest_dir_uri, delete=delete)\n\n\ndef start_sync(src_dir_uri, dest_dir_uri, sync_interval=600,\n               fs=None):  # pragma: no cover\n    """"""Start syncing a directory on a schedule.\n\n    Calls sync_to_dir on a schedule.\n\n    Args:\n        src_dir_uri: (string) Path of the local source directory\n        dest_dir_uri: (string) URI of destination directory\n        sync_interval: (int) period in seconds for syncing\n        fs:  Optional FileSystem to use\n    """"""\n\n    def _sync_dir():\n        while True:\n            time.sleep(sync_interval)\n            log.info(\'Syncing {} to {}...\'.format(src_dir_uri, dest_dir_uri))\n            sync_to_dir(src_dir_uri, dest_dir_uri, delete=False, fs=fs)\n\n    class SyncThread:\n        def __init__(self):\n            thread = Timer(0.68, _sync_dir)\n            thread.daemon = True\n            thread.start()\n            self.thread = thread\n\n        def __enter__(self):\n            return self.thread\n\n        def __exit__(self, type, value, traceback):\n            self.thread.cancel()\n\n    return SyncThread()\n\n\ndef download_if_needed(uri, download_dir, fs=None):\n    """"""Download a file into a directory if it\'s remote.\n\n    If uri is local, there is no need to download the file.\n\n    Args:\n        uri: (string) URI of file\n        download_dir: (string) local directory to download file into\n        fs: Optional FileSystem to use.\n\n    Returns:\n        (string) path to local file\n\n    Raises:\n        NotReadableError if URI cannot be read from\n    """"""\n    if uri is None:\n        return None\n\n    if not fs:\n        fs = FileSystem.get_file_system(uri, \'r\')\n\n    path = get_local_path(uri, download_dir, fs=fs)\n    make_dir(path, use_dirname=True)\n\n    if path != uri:\n        log.debug(\'Downloading {} to {}\'.format(uri, path))\n\n    fs.copy_from(uri, path)\n\n    return path\n\n\ndef download_or_copy(uri, target_dir, fs=None):\n    """"""Downloads or copies a file to a directory\n\n    Args:\n       uri: (string) URI of file\n       target_dir: (string) local directory to copy file to\n       fs: Optional FileSystem to use\n    """"""\n    local_path = download_if_needed(uri, target_dir, fs=fs)\n    shutil.copy(local_path, target_dir)\n    return local_path\n\n\ndef file_exists(uri, fs=None, include_dir=True):\n    if not fs:\n        fs = FileSystem.get_file_system(uri, \'r\')\n    return fs.file_exists(uri, include_dir)\n\n\ndef list_paths(uri, ext=\'\', fs=None):\n    if uri is None:\n        return None\n\n    if not fs:\n        fs = FileSystem.get_file_system(uri, \'r\')\n\n    return fs.list_paths(uri, ext=ext)\n\n\ndef upload_or_copy(src_path, dst_uri, fs=None):\n    """"""Upload a file if the destination is remote.\n\n    If dst_uri is local, the file is copied.\n\n    Args:\n        src_path: (string) path to source file\n        dst_uri: (string) URI of destination for file\n        fs: Optional FileSystem to use\n    Raises:\n        NotWritableError if URI cannot be written to\n    """"""\n    if dst_uri is None:\n        return\n\n    if not (os.path.isfile(src_path) or os.path.isdir(src_path)):\n        raise Exception(\'{} does not exist.\'.format(src_path))\n\n    if not src_path == dst_uri:\n        log.info(\'Uploading {} to {}\'.format(src_path, dst_uri))\n\n    if not fs:\n        fs = FileSystem.get_file_system(dst_uri, \'w\')\n    fs.copy_to(src_path, dst_uri)\n\n\ndef file_to_str(uri, fs=None):\n    """"""Download contents of text file into a string.\n\n    Args:\n        uri: (string) URI of file\n        fs: Optional FileSystem to use\n\n    Returns:\n        (string) with contents of text file\n\n    Raises:\n        NotReadableError if URI cannot be read from\n    """"""\n    if not fs:\n        fs = FileSystem.get_file_system(uri, \'r\')\n    return fs.read_str(uri)\n\n\ndef str_to_file(content_str, uri, fs=None):\n    """"""Writes string to text file.\n\n    Args:\n        content_str: string to write\n        uri: (string) URI of file to write\n        fs: Optional FileSystem to use\n\n    Raise:\n        NotWritableError if file_uri cannot be written\n    """"""\n    if not fs:\n        fs = FileSystem.get_file_system(uri, \'r\')\n    return fs.write_str(uri, content_str)\n\n\ndef load_json_config(uri, message, fs=None):\n    """"""Load a JSON-formatted protobuf config file.\n\n    Args:\n        uri: (string) URI of config file\n        message: (google.protobuf.message.Message) empty protobuf message of\n            to load the config into. The type needs to match the content of\n            uri.\n        fs: Optional FileSystem to use.\n\n    Returns:\n        the same message passed as input with fields filled in from uri\n\n    Raises:\n        ProtobufParseException if uri cannot be parsed\n    """"""\n    try:\n        return json_format.Parse(file_to_str(uri, fs=fs), message)\n    except json_format.ParseError as e:\n        error_msg = (\'Problem parsing protobuf file {}. \'.format(uri) +\n                     \'You might need to run scripts/compile\')\n        raise ProtobufParseException(error_msg) from e\n\n\ndef save_json_config(message, uri, fs=None):\n    """"""Save a protobuf object to a JSON file.\n\n    Args:\n        message: (google.protobuf.message.Message) protobuf message\n        uri: (string) URI of JSON file to write message to\n        fs: Optional FileSystem to use\n\n    Raises:\n        NotWritableError if uri cannot be written\n    """"""\n    json_str = json_format.MessageToJson(message)\n    str_to_file(json_str, uri, fs=fs)\n\n\ndef get_cached_file(cache_dir, uri):\n    """"""Download a file and unzip it, using a cache to avoid unnecessary operations.\n\n    This downloads a file if it isn\'t already in the cache, and unzips the file using\n    gunzip if it hasn\'t already been unzipped (and the uri has a .gz suffix).\n\n    Args:\n        cache_dir: (str) dir to use for cache directory\n        uri: (str) URI of a file that can be opened by a supported RV file system\n\n    Returns:\n        (str) path of the (downloaded and unzipped) cached file\n    """"""\n    # Only download if it isn\'t in the cache.\n    path = get_local_path(uri, cache_dir)\n    if not os.path.isfile(path):\n        path = download_if_needed(uri, cache_dir)\n\n    # Unzip if .gz file\n    if path.endswith(\'.gz\'):\n        # If local URI, then make ungz_path in temp cache, so it isn\'t unzipped\n        # alongside the original file.\n        if os.path.isfile(uri):\n            ungz_path = os.path.join(cache_dir, path)[:-3]\n        else:\n            ungz_path = path[:-3]\n\n        # Check to see if it is already unzipped before unzipping.\n        if not os.path.isfile(ungz_path):\n            with gzip.open(path, \'rb\') as f_in:\n                with open(ungz_path, \'wb\') as f_out:\n                    shutil.copyfileobj(f_in, f_out)\n        path = ungz_path\n\n    return path\n\n\ndef file_to_json(uri):\n    """"""Return JSON dict based on file at uri.""""""\n    return json.loads(file_to_str(uri))\n\n\ndef json_to_file(js, uri):\n    """"""Upload file to uri based on JSON dict.""""""\n    str_to_file(json.dumps(js), uri)\n\n\ndef zipdir(dir, zip_path):\n    """"""Save a zip file with contents of directory.\n\n    Contents of directory will be at root of zip file.\n\n    Args:\n        dir: (str) directory to zip\n        zip_path: (str) path to zip file to create\n    """"""\n    make_dir(zip_path, use_dirname=True)\n    with zipfile.ZipFile(zip_path, \'w\', zipfile.ZIP_DEFLATED) as ziph:\n        for dirpath, dirnames, filenames in os.walk(dir):\n            for fn in filenames:\n                ziph.write(join(dirpath, fn), join(dirpath[len(dir):], fn))\n\n\ndef unzip(zip_path, target_dir):\n    """"""Unzip contents of zip file at zip_path into target_dir""""""\n    make_dir(target_dir)\n    zip_ref = zipfile.ZipFile(zip_path, \'r\')\n    zip_ref.extractall(target_dir)\n    zip_ref.close()\n'"
rastervision/utils/filter_geojson.py,0,"b'import json\nimport copy\n\nimport click\n\nfrom rastervision.utils.files import file_to_str, str_to_file\n\n\n@click.command()\n@click.argument(\'labels_uri\')\n@click.argument(\'output_uri\')\n@click.argument(\'class_names\', nargs=-1)\ndef filter_geojson(labels_uri, output_uri, class_names):\n    """"""Remove features that aren\'t in class_names and remove class_ids.""""""\n    labels_str = file_to_str(labels_uri)\n    labels = json.loads(labels_str)\n    filtered_features = []\n\n    for feature in labels[\'features\']:\n        feature = copy.deepcopy(feature)\n        properties = feature.get(\'properties\')\n        if properties:\n            class_name = properties.get(\'class_name\') or properties(\'label\')\n            if class_name in class_names:\n                del properties[\'class_id\']\n                filtered_features.append(feature)\n\n    new_labels = {\'features\': filtered_features}\n    str_to_file(json.dumps(new_labels), output_uri)\n\n\nif __name__ == \'__main__\':\n    filter_geojson()\n'"
rastervision/utils/misc.py,0,"b'import io\nfrom math import ceil\n\nfrom PIL import Image\nimport numpy as np\nimport imageio\nimport atexit\nimport logging\n\nlog = logging.getLogger(__name__)\n\n\ndef save_img(im_array, output_path):\n    imageio.imwrite(output_path, im_array)\n\n\ndef numpy_to_png(array: np.ndarray) -> str:\n    """"""Get a PNG string from a Numpy array.\n\n    Args:\n         array: A Numpy array of shape (w, h, 3) or (w, h), where the\n               former is meant to become a three-channel image and the\n               latter a one-channel image.  The dtype of the array\n               should be uint8.\n\n    Returns:\n         str\n\n    """"""\n    im = Image.fromarray(array)\n    output = io.BytesIO()\n    im.save(output, \'png\')\n    return output.getvalue()\n\n\ndef png_to_numpy(png: str, dtype=np.uint8) -> np.ndarray:\n    """"""Get a Numpy array from a PNG string.\n\n    Args:\n         png: A str containing a PNG-formatted image.\n\n    Returns:\n         numpy.ndarray\n\n    """"""\n    incoming = io.BytesIO(png)\n    im = Image.open(incoming)\n    return np.array(im)\n\n\ndef replace_nones_in_dict(target, replace_value):\n    """"""Recursively replaces Nones in a dictionary with the given value.""""""\n    for k in target:\n        if target[k] is None:\n            target[k] = replace_value\n        elif type(target[k]) is list:\n            result = []\n            for e in target[k]:\n                if type(e) is dict:\n                    result.append(replace_nones_in_dict(e, replace_value))\n                else:\n                    if e is None:\n                        result.append(replace_value)\n                    else:\n                        result.append(e)\n            target[k] = result\n        elif type(target[k]) is dict:\n            replace_nones_in_dict(target[k], replace_value)\n    return target\n\n\ndef set_nested_keys(target,\n                    mods,\n                    ignore_missing_keys=False,\n                    set_missing_keys=False):\n    """"""Sets dictionary keys based on modifications.\n\n    Args:\n       target - Target dictionary to be  modified in-place.\n       mods - Dictionary of values to set into the target dict.\n              This method will look for any keys matching the mod\n              key, even in nested dictionaries. If the mod has a nested\n              dictionary, then the leaf key value will only be set\n              if that parent dictionary key is found and is a dictionary.\n       ignore_missing_keys - If a key is not found, do not throw an error.\n       set_missing_keys - If a key is not found, set it. If the key is part\n                          of a nested set, and parent keys are found in the target\n                          dictionary, then set the key at whatever level of the nested\n                          set of keys where the key is first not found.\n    """"""\n    searched_keys, found_keys = [], []\n\n    def f(_target, _mods, parent_key=None, mod_parent_key=None):\n        for key in _target:\n            if key in _mods.keys():\n                found_keys.append(key)\n                if type(_target[key]) is dict:\n                    if type(_mods[key]) is dict:\n                        f(_target[key],\n                          _mods[key],\n                          parent_key=key,\n                          mod_parent_key=key)\n                    else:\n                        raise Exception(\'Error: cannot modify dict with value\')\n                else:\n                    _target[key] = _mods[key]\n            else:\n                if type(_target[key]) is dict:\n                    f(_target[key],\n                      _mods,\n                      parent_key=key,\n                      mod_parent_key=mod_parent_key)\n        searched_keys.extend(list(_mods.keys()))\n\n        if set_missing_keys:\n            for key in set(_mods.keys()) - set(found_keys):\n                if not type(\n                        _mods[key]) is dict and parent_key == mod_parent_key:\n                    _target[key] = _mods[key]\n                    found_keys.append(key)\n\n    f(target, mods)\n    if not ignore_missing_keys:\n        d = set(searched_keys) - set(found_keys)\n        if d:\n            raise Exception(\'Mod keys not found in target dict: {}\'.format(d))\n\n\ndef terminate_at_exit(process):\n    def terminate():\n        log.debug(\'Terminating {}...\'.format(process.pid))\n        process.terminate()\n\n    atexit.register(terminate)\n\n\ndef grouped(lst, size):\n    """"""Returns a list of lists of length \'size\'.\n    The last list will have size <= \'size\'.\n    """"""\n    return [lst[n:n + size] for n in range(0, len(lst), size)]\n\n\ndef split_into_groups(lst, num_groups):\n    """"""Attempts to split a list into a given number of groups.\n    The number of groups will be at least 1 and at most\n    num_groups.\n\n    Args:\n       lst:             The list to split\n       num_groups:      The number of groups to create.\n    Returns:\n       A list of size between 1 and num_groups containing lists\n       of items of l.""""""\n    group_size = max(int(ceil((len(lst)) / num_groups)), 1)\n\n    return grouped(lst, group_size)\n'"
rastervision/utils/zxy2geotiff.py,0,"b'import tempfile\n\nfrom PIL import Image\nimport numpy as np\nimport click\nimport mercantile\nimport rasterio\nfrom rasterio.windows import Window\nimport pyproj\n\nfrom rastervision.utils.files import (download_if_needed, get_local_path,\n                                      upload_or_copy)\nfrom rastervision.command.aux.cogify_command import create_cog\n\n\ndef lnglat2merc(lng, lat):\n    """"""Convert lng, lat point to x/y Web Mercator tuple.""""""\n    return pyproj.transform(\n        pyproj.Proj(init=\'epsg:4326\'), pyproj.Proj(init=\'epsg:3857\'), lng, lat)\n\n\ndef merc2lnglat(x, y):\n    """"""Convert x, y Web Mercator point to lng/lat tuple.""""""\n    return pyproj.transform(\n        pyproj.Proj(init=\'epsg:3857\'), pyproj.Proj(init=\'epsg:4326\'), x, y)\n\n\ndef merc2pixel(tile_x, tile_y, zoom, merc_x, merc_y, tile_sz=256):\n    """"""Convert Web Mercator point to pixel coordinates.\n\n    This is within the coordinate frame of a single ZXY tile.\n\n    Args:\n        tile_x: (int) x coordinate of ZXY tile\n        tile_y: (int) y coordinate of ZXY tile\n        zoom: (int) zoom level of ZXY tile\n        merc_x: (float) Web Mercator x axis of point\n        merc_y: (float) Web Mercator y axis of point\n        tile_sz: (int) size of ZXY tile\n    """"""\n    tile_merc_bounds = mercantile.xy_bounds(tile_x, tile_y, zoom)\n    pix_y = int(\n        round(tile_sz * ((tile_merc_bounds.top - merc_y) /\n                         (tile_merc_bounds.top - tile_merc_bounds.bottom))))\n    pix_x = int(\n        round(tile_sz * ((merc_x - tile_merc_bounds.left) /\n                         (tile_merc_bounds.right - tile_merc_bounds.left))))\n    return (pix_x, pix_y)\n\n\ndef _zxy2geotiff(tile_schema, zoom, bounds, output_uri, make_cog=False):\n    """"""Generates a GeoTIFF of a bounded region from a ZXY tile server.\n\n    Args:\n        tile_schema: (str) the URI schema for zxy tiles (ie. a slippy map tile server)\n            of the form /tileserver-uri/{z}/{x}/{y}.png. If {-y} is used, the tiles\n            are assumed to be indexed using TMS coordinates, where the y axis starts\n            at the southernmost point. The URI can be for http, S3, or the local\n            file system.\n        zoom: (int) the zoom level to use when retrieving tiles\n        bounds: (list) a list of length 4 containing min_lat, min_lng,\n            max_lat, max_lng\n        output_uri: (str) where to save the GeoTIFF. The URI can be for http, S3, or the\n            local file system\n    """"""\n    min_lat, min_lng, max_lat, max_lng = bounds\n    if min_lat >= max_lat:\n        raise ValueError(\'min_lat must be < max_lat\')\n    if min_lng >= max_lng:\n        raise ValueError(\'min_lng must be < max_lng\')\n\n    is_tms = False\n    if \'{-y}\' in tile_schema:\n        tile_schema = tile_schema.replace(\'{-y}\', \'{y}\')\n        is_tms = True\n\n    tmp_dir_obj = tempfile.TemporaryDirectory()\n    tmp_dir = tmp_dir_obj.name\n\n    # Get range of tiles that cover bounds.\n    output_path = get_local_path(output_uri, tmp_dir)\n    tile_sz = 256\n    t = mercantile.tile(min_lng, max_lat, zoom)\n    xmin, ymin = t.x, t.y\n    t = mercantile.tile(max_lng, min_lat, zoom)\n    xmax, ymax = t.x, t.y\n\n    # The supplied bounds are contained within the ""tile bounds"" -- ie. the\n    # bounds of the set of tiles that covers the supplied bounds. Therefore,\n    # we need to crop out the imagery that lies within the supplied bounds.\n    # We do this by computing a top, bottom, left, and right offset in pixel\n    # units of the supplied bounds against the tile bounds. Getting the offsets\n    # in pixel units involves converting lng/lat to web mercator units since we\n    # assume that is the CRS of the tiles. These offsets are then used to crop\n    # individual tiles and place them correctly into the output raster.\n    nw_merc_x, nw_merc_y = lnglat2merc(min_lng, max_lat)\n    left_pix_offset, top_pix_offset = merc2pixel(xmin, ymin, zoom, nw_merc_x,\n                                                 nw_merc_y)\n\n    se_merc_x, se_merc_y = lnglat2merc(max_lng, min_lat)\n    se_left_pix_offset, se_top_pix_offset = merc2pixel(xmax, ymax, zoom,\n                                                       se_merc_x, se_merc_y)\n    right_pix_offset = tile_sz - se_left_pix_offset\n    bottom_pix_offset = tile_sz - se_top_pix_offset\n\n    uncropped_height = tile_sz * (ymax - ymin + 1)\n    uncropped_width = tile_sz * (xmax - xmin + 1)\n    height = uncropped_height - top_pix_offset - bottom_pix_offset\n    width = uncropped_width - left_pix_offset - right_pix_offset\n\n    transform = rasterio.transform.from_bounds(nw_merc_x, se_merc_y, se_merc_x,\n                                               nw_merc_y, width, height)\n    with rasterio.open(\n            output_path,\n            \'w\',\n            driver=\'GTiff\',\n            height=height,\n            width=width,\n            count=3,\n            crs=\'epsg:3857\',\n            transform=transform,\n            dtype=rasterio.uint8) as dataset:\n        out_x = 0\n        for xi, x in enumerate(range(xmin, xmax + 1)):\n            tile_xmin, tile_xmax = 0, tile_sz - 1\n            if x == xmin:\n                tile_xmin += left_pix_offset\n            if x == xmax:\n                tile_xmax -= right_pix_offset\n            window_width = tile_xmax - tile_xmin + 1\n\n            out_y = 0\n            for yi, y in enumerate(range(ymin, ymax + 1)):\n                tile_ymin, tile_ymax = 0, tile_sz - 1\n                if y == ymin:\n                    tile_ymin += top_pix_offset\n                if y == ymax:\n                    tile_ymax -= bottom_pix_offset\n                window_height = tile_ymax - tile_ymin + 1\n\n                # Convert from xyz to tms if needed.\n                # https://gist.github.com/tmcw/4954720\n                if is_tms:\n                    y = (2**zoom) - y - 1\n                tile_uri = tile_schema.format(x=x, y=y, z=zoom)\n                tile_path = download_if_needed(tile_uri, tmp_dir)\n                img = np.array(Image.open(tile_path))\n                img = img[tile_ymin:tile_ymax + 1, tile_xmin:tile_xmax + 1, :]\n\n                window = Window(out_x, out_y, window_width, window_height)\n                dataset.write(\n                    np.transpose(img[:, :, 0:3], (2, 0, 1)), window=window)\n                out_y += window_height\n            out_x += window_width\n\n    if make_cog:\n        create_cog(output_path, output_uri, tmp_dir)\n    else:\n        upload_or_copy(output_path, output_uri)\n\n\n@click.command()\n@click.argument(\'tile_schema\')\n@click.argument(\'zoom\')\n@click.argument(\'bounds\')\n@click.argument(\'output_uri\')\n@click.option(\'--make-cog\', is_flag=True, default=False)\ndef zxy2geotiff(tile_schema, zoom, bounds, output_uri, make_cog):\n    """"""Generates a GeoTIFF of a bounded region from a ZXY tile server.\n\n    TILE_SCHEMA: the URI schema for zxy tiles (ie. a slippy map tile server) of\n    the form /tileserver-uri/{z}/{x}/{y}.png. If {-y} is used, the tiles are\n    assumed to be indexed using TMS coordinates, where the y axis starts at\n    the southernmost point. The URI can be for http, S3, or the local file\n    system.\n\n    ZOOM: the zoom level to use when retrieving tiles\n\n    BOUNDS: a space-separated string containing min_lat, min_lng, max_lat,\n    max_lng\n\n    OUTPUT_URI: where to save the GeoTIFF. The URI can be for http, S3, or the\n    local file system.\n    """"""\n    bounds = [float(x) for x in bounds.split(\' \')]\n    _zxy2geotiff(tile_schema, int(zoom), bounds, output_uri, make_cog=make_cog)\n\n\nif __name__ == \'__main__\':\n    zxy2geotiff()\n'"
rastervision2/aws_batch/__init__.py,0,"b""# flake8: noqa\n\nimport rastervision2.pipeline\nfrom rastervision2.aws_batch.aws_batch_runner import *\n\n\ndef register_plugin(registry):\n    registry.add_runner(AWS_BATCH, AWSBatchRunner)\n    registry.add_rv_config_schema(AWS_BATCH, [\n        'gpu_job_queue', 'gpu_job_def', 'cpu_job_queue', 'cpu_job_def',\n        'attempts'\n    ])\n"""
rastervision2/aws_batch/aws_batch_runner.py,0,"b'import logging\nimport os\nimport uuid\nfrom inspect import signature\nfrom typing import List, Optional\n\nfrom rastervision2.pipeline import rv_config\nfrom rastervision2.pipeline.runner import Runner\n\nlog = logging.getLogger(__name__)\nAWS_BATCH = \'aws_batch\'\n\n\ndef submit_job(cmd: List[str],\n               debug: bool = False,\n               profile: str = False,\n               attempts: int = 5,\n               parent_job_ids: List[str] = None,\n               num_array_jobs: Optional[int] = None,\n               use_gpu: bool = False,\n               job_queue: Optional[str] = None,\n               job_def: Optional[str] = None) -> str:\n    """"""Submit a job to run on AWS Batch.\n\n    Args:\n        cmd: a command to run in the Docker container for the remote job\n        debug: if True, run the command using a ptvsd wrapper which sets up a remote\n            VS Code Python debugger server\n        profile: if True, run the command using kernprof, a line profiler\n        attempts: the number of times to try running the command which is useful\n            in case of failure.\n        parent_job_ids: optional list of parent Batch job ids. The job created by this\n            will only run after the parent jobs complete successfully.\n        num_array_jobs: if set, make this a Batch array job with size equal to\n            num_array_jobs\n        use_gpu: if True, run the job in a GPU-enabled queue\n        job_queue: if set, use this job queue\n        job_def: if set, use this job definition\n    """"""\n    batch_config = rv_config.get_namespace_config(AWS_BATCH)\n\n    if job_queue is None:\n        if use_gpu:\n            job_queue = batch_config(\'gpu_job_queue\')\n        else:\n            job_queue = batch_config(\'cpu_job_queue\')\n\n    if job_def is None:\n        if use_gpu:\n            job_def = batch_config(\'gpu_job_def\')\n        else:\n            job_def = batch_config(\'cpu_job_def\')\n\n    import boto3\n    client = boto3.client(\'batch\')\n    job_name = \'raster-vision-{}\'.format(uuid.uuid4())\n\n    cmd_list = cmd.split(\' \')\n    if debug:\n        cmd_list = [\n            \'python\', \'-m\', \'ptvsd\', \'--host\', \'0.0.0.0\', \'--port\', \'6006\',\n            \'--wait\', \'-m\'\n        ] + cmd_list\n\n    if profile:\n        cmd_list = [\'kernprof\', \'-v\', \'-l\'] + cmd_list\n\n    kwargs = {\n        \'jobName\': job_name,\n        \'jobQueue\': job_queue,\n        \'jobDefinition\': job_def,\n        \'containerOverrides\': {\n            \'command\': cmd_list\n        },\n        \'retryStrategy\': {\n            \'attempts\': attempts\n        },\n    }\n    if parent_job_ids:\n        kwargs[\'dependsOn\'] = [{\'jobId\': id} for id in parent_job_ids]\n    if num_array_jobs:\n        kwargs[\'arrayProperties\'] = {\'size\': num_array_jobs}\n\n    job_id = client.submit_job(**kwargs)[\'jobId\']\n    msg = \'submitted job with jobName={} and jobId={}\'.format(job_name, job_id)\n    log.info(msg)\n    log.info(cmd_list)\n\n    return job_id\n\n\nclass AWSBatchRunner(Runner):\n    """"""Runs pipelines remotely using AWS Batch.\n\n    Requires Everett configuration of form:\n\n    ```\n    [AWS_BATCH]\n    cpu_job_queue=\n    cpu_job_def=\n    gpu_job_queue=\n    gpu_job_def=\n    attempts=\n    ```\n    """"""\n\n    def run(self, cfg_json_uri, pipeline, commands, num_splits=1):\n        parent_job_ids = []\n\n        # pipeline-specific job queue\n        if hasattr(pipeline, \'job_queue\'):\n            pipeline_job_queue = pipeline.job_queue\n        else:\n            pipeline_job_queue = None\n\n        # pipeline-specific job definition\n        if hasattr(pipeline, \'job_def\'):\n            pipeline_job_def = pipeline.job_def\n        else:\n            pipeline_job_def = None\n\n        for command in commands:\n\n            # detect external command\n            if hasattr(pipeline, command):\n                fn = getattr(pipeline, command)\n                params = signature(fn).parameters\n                external = hasattr(fn, \'external\') and len(params) == 0\n            else:\n                external = False\n\n            # command-specific job queue, job definition\n            job_def = pipeline_job_def\n            job_queue = pipeline_job_queue\n            if hasattr(pipeline, command):\n                fn = getattr(pipeline, command)\n                if hasattr(fn, \'job_def\'):\n                    job_def = fn.job_def\n                if hasattr(fn, \'job_queue\'):\n                    job_queue = fn.job_queue\n\n            if not external:\n                cmd = [\n                    \'python\', \'-m\', \'rastervision2.pipeline.cli run_command\',\n                    cfg_json_uri, command, \'--runner\', AWS_BATCH\n                ]\n            else:\n                cmd = fn()\n\n            num_array_jobs = None\n            use_gpu = command in pipeline.gpu_commands\n\n            if not external:\n                if command in pipeline.split_commands and num_splits > 1:\n                    num_array_jobs = num_splits\n                    if num_splits > 1:\n                        cmd += [\'--num-splits\', str(num_splits)]\n\n            cmd = \' \'.join(cmd)\n\n            job_id = submit_job(\n                cmd,\n                parent_job_ids=parent_job_ids,\n                num_array_jobs=num_array_jobs,\n                use_gpu=use_gpu,\n                job_queue=job_queue,\n                job_def=job_def)\n            parent_job_ids = [job_id]\n\n            job_queue = None\n            job_def = None\n\n    def get_split_ind(self):\n        return int(os.environ.get(\'AWS_BATCH_JOB_ARRAY_INDEX\', 0))\n'"
rastervision2/aws_s3/__init__.py,0,"b""# flake8: noqa\n\nimport rastervision2.pipeline\nfrom rastervision2.aws_s3.s3_file_system import (AWS_S3, S3FileSystem)\n\n\ndef register_plugin(registry):\n    registry.add_file_system(S3FileSystem)\n    registry.add_rv_config_schema(AWS_S3, ['requester_pays'])\n"""
rastervision2/aws_s3/s3_file_system.py,0,"b'import io\nimport os\nimport subprocess\nfrom datetime import datetime\nfrom urllib.parse import urlparse\n\nfrom rastervision2.pipeline.file_system import (FileSystem, NotReadableError,\n                                                NotWritableError)\n\nAWS_S3 = \'aws_s3\'\n\n\n# Code from https://alexwlchan.net/2017/07/listing-s3-keys/\ndef get_matching_s3_objects(bucket, prefix=\'\', suffix=\'\',\n                            request_payer=\'None\'):\n    """"""\n    Generate objects in an S3 bucket.\n\n    :param bucket: Name of the S3 bucket.\n    :param prefix: Only fetch objects whose key starts with\n        this prefix (optional).\n    :param suffix: Only fetch objects whose keys end with\n        this suffix (optional).\n    """"""\n    import boto3\n    s3 = boto3.client(\'s3\')\n    kwargs = {\'Bucket\': bucket, \'RequestPayer\': request_payer}\n\n    # If the prefix is a single string (not a tuple of strings), we can\n    # do the filtering directly in the S3 API.\n    if isinstance(prefix, str):\n        kwargs[\'Prefix\'] = prefix\n\n    while True:\n\n        # The S3 API response is a large blob of metadata.\n        # \'Contents\' contains information about the listed objects.\n        resp = s3.list_objects_v2(**kwargs)\n\n        try:\n            contents = resp[\'Contents\']\n        except KeyError:\n            return\n\n        for obj in contents:\n            key = obj[\'Key\']\n            if key.startswith(prefix) and key.endswith(suffix):\n                yield obj\n\n        # The S3 API is paginated, returning up to 1000 keys at a time.\n        # Pass the continuation token into the next response, until we\n        # reach the final page (when this field is missing).\n        try:\n            kwargs[\'ContinuationToken\'] = resp[\'NextContinuationToken\']\n        except KeyError:\n            break\n\n\ndef get_matching_s3_keys(bucket, prefix=\'\', suffix=\'\', request_payer=\'None\'):\n    """"""\n    Generate the keys in an S3 bucket.\n\n    :param bucket: Name of the S3 bucket.\n    :param prefix: Only fetch keys that start with this prefix (optional).\n    :param suffix: Only fetch keys that end with this suffix (optional).\n    """"""\n    for obj in get_matching_s3_objects(bucket, prefix, suffix, request_payer):\n        yield obj[\'Key\']\n\n\nclass S3FileSystem(FileSystem):\n    """"""A FileSystem for interacting with files stored on AWS S3.\n\n    Uses Everett configuration of form:\n    ```\n    [AWS_S3]\n    requester_pays=True\n    ```\n\n    """"""\n\n    @staticmethod\n    def get_request_payer():\n        # Import here to avoid circular reference.\n        from rastervision2.pipeline import rv_config\n        s3_config = rv_config.get_namespace_config(AWS_S3)\n\n        # \'None\' needs the quotes because boto3 cannot handle None.\n        return (\'requester\' if s3_config(\n            \'requester_pays\', parser=bool, default=\'False\') else \'None\')\n\n    @staticmethod\n    def get_session():\n        # Lazily load boto\n        import boto3\n        return boto3.Session()\n\n    @staticmethod\n    def matches_uri(uri: str, mode: str) -> bool:\n        parsed_uri = urlparse(uri)\n        return parsed_uri.scheme == \'s3\'\n\n    @staticmethod\n    def file_exists(uri: str, include_dir: bool = True) -> bool:\n        # Lazily load boto\n        import botocore\n\n        parsed_uri = urlparse(uri)\n        bucket = parsed_uri.netloc\n        key = parsed_uri.path[1:]\n        request_payer = S3FileSystem.get_request_payer()\n\n        if include_dir:\n            s3 = S3FileSystem.get_session().client(\'s3\')\n            try:\n                # Ensure key ends in slash so that this won\'t pick up files that\n                # contain the key as a prefix, but aren\'t actually directories.\n                # Example: if key is \'model\' then we don\'t want to consider\n                # model-123 a match.\n                dir_key = key if key[-1] == \'/\' else key + \'/\'\n                response = s3.list_objects_v2(\n                    Bucket=bucket,\n                    Prefix=dir_key,\n                    MaxKeys=1,\n                    RequestPayer=request_payer)\n                if response[\'KeyCount\'] == 0:\n                    return S3FileSystem.file_exists(uri, include_dir=False)\n                return True\n            except botocore.exceptions.ClientError as e:\n                return False\n        else:\n            s3r = S3FileSystem.get_session().resource(\'s3\')\n            try:\n                s3r.Object(bucket, key).load(RequestPayer=request_payer)\n                return True\n            except botocore.exceptions.ClientError as e:\n                return False\n\n    @staticmethod\n    def read_str(uri: str) -> str:\n        return S3FileSystem.read_bytes(uri).decode(\'utf-8\')\n\n    @staticmethod\n    def read_bytes(uri: str) -> bytes:\n        import botocore\n\n        s3 = S3FileSystem.get_session().client(\'s3\')\n        request_payer = S3FileSystem.get_request_payer()\n\n        parsed_uri = urlparse(uri)\n        with io.BytesIO() as file_buffer:\n            try:\n                s3.download_fileobj(\n                    parsed_uri.netloc,\n                    parsed_uri.path[1:],\n                    file_buffer,\n                    ExtraArgs={\'RequestPayer\': request_payer})\n                return file_buffer.getvalue()\n            except botocore.exceptions.ClientError as e:\n                raise NotReadableError(\'Could not read {}\'.format(uri)) from e\n\n    @staticmethod\n    def write_str(uri: str, data: str) -> None:\n        data = bytes(data, encoding=\'utf-8\')\n        S3FileSystem.write_bytes(uri, data)\n\n    @staticmethod\n    def write_bytes(uri: str, data: bytes) -> None:\n        s3 = S3FileSystem.get_session().client(\'s3\')\n\n        parsed_uri = urlparse(uri)\n        bucket = parsed_uri.netloc\n        key = parsed_uri.path[1:]\n        with io.BytesIO(data) as str_buffer:\n            try:\n                s3.upload_fileobj(str_buffer, bucket, key)\n            except Exception as e:\n                raise NotWritableError(\'Could not write {}\'.format(uri)) from e\n\n    @staticmethod\n    def sync_from_dir(src_dir_uri: str, dst_dir: str,\n                      delete: bool = False) -> None:  # pragma: no cover\n        command = [\'aws\', \'s3\', \'sync\', src_dir_uri, dst_dir]\n        if delete:\n            command.append(\'--delete\')\n        request_payer = S3FileSystem.get_request_payer()\n        if request_payer:\n            command.append(\'--request-payer\')\n        subprocess.run(command)\n\n    @staticmethod\n    def sync_to_dir(src_dir: str, dst_dir_uri: str,\n                    delete: bool = False) -> None:  # pragma: no cover\n        S3FileSystem.sync_from_dir(src_dir, dst_dir_uri, delete=delete)\n\n    @staticmethod\n    def copy_to(src_path: str, dst_uri: str) -> None:\n        s3 = S3FileSystem.get_session().client(\'s3\')\n\n        parsed_uri = urlparse(dst_uri)\n        if os.path.isfile(src_path):\n            try:\n                s3.upload_file(src_path, parsed_uri.netloc,\n                               parsed_uri.path[1:])\n            except Exception as e:\n                raise NotWritableError(\n                    \'Could not write {}\'.format(dst_uri)) from e\n        else:\n            S3FileSystem.sync_to_dir(src_path, dst_uri, delete=True)\n\n    @staticmethod\n    def copy_from(src_uri: str, dst_path: str) -> None:\n        import botocore\n\n        s3 = S3FileSystem.get_session().client(\'s3\')\n        request_payer = S3FileSystem.get_request_payer()\n\n        parsed_uri = urlparse(src_uri)\n        try:\n            s3.download_file(\n                parsed_uri.netloc,\n                parsed_uri.path[1:],\n                dst_path,\n                ExtraArgs={\'RequestPayer\': request_payer})\n        except botocore.exceptions.ClientError:\n            raise NotReadableError(\'Could not read {}\'.format(src_uri))\n\n    @staticmethod\n    def local_path(uri: str, download_dir: str) -> None:\n        parsed_uri = urlparse(uri)\n        path = os.path.join(download_dir, \'s3\', parsed_uri.netloc,\n                            parsed_uri.path[1:])\n        return path\n\n    @staticmethod\n    def last_modified(uri: str) -> datetime:\n        parsed_uri = urlparse(uri)\n        bucket, key = parsed_uri.netloc, parsed_uri.path[1:]\n        s3 = S3FileSystem.get_session().client(\'s3\')\n        request_payer = S3FileSystem.get_request_payer()\n        head_data = s3.head_object(\n            Bucket=bucket, Key=key, RequestPayer=request_payer)\n        return head_data[\'LastModified\']\n\n    @staticmethod\n    def list_paths(uri, ext=\'\'):\n        request_payer = S3FileSystem.get_request_payer()\n        parsed_uri = urlparse(uri)\n        bucket = parsed_uri.netloc\n        prefix = os.path.join(parsed_uri.path[1:])\n        keys = get_matching_s3_keys(\n            bucket, prefix, suffix=ext, request_payer=request_payer)\n        return [os.path.join(\'s3://\', bucket, key) for key in keys]\n'"
rastervision2/core/__init__.py,0,"b'# flake8: noqa\n\nimport rastervision2.pipeline\nfrom rastervision2.core.box import *\nfrom rastervision2.core.data_sample import *\nfrom rastervision2.core.predictor import *\nfrom rastervision2.core.raster_stats import *\n\n# We just need to import anything that contains a Config, so that all\n# the register_config decorators will be called which add Configs to the\n# registry.\nimport rastervision2.core.backend\nimport rastervision2.core.data\nimport rastervision2.core.rv_pipeline\nimport rastervision2.core.evaluation\nimport rastervision2.core.cli\n\n\ndef register_plugin(registry):\n    pass\n'"
rastervision2/core/box.py,0,"b'import math\nimport random\n\nimport numpy as np\nfrom shapely.geometry import box as ShapelyBox\n\n\nclass BoxSizeError(ValueError):\n    pass\n\n\nclass Box():\n    """"""A multi-purpose box (ie. rectangle).""""""\n\n    def __init__(self, ymin, xmin, ymax, xmax):\n        """"""Construct a bounding box.\n\n        Unless otherwise stated, the convention is that these coordinates are\n        in pixel coordinates and represent boxes that lie within a\n        RasterSource.\n\n        Args:\n            ymin: minimum y value (y is row)\n            xmin: minimum x value (x is column)\n            ymax: maximum y value\n            xmax: maximum x value\n\n        """"""\n        self.ymin = ymin\n        self.xmin = xmin\n        self.ymax = ymax\n        self.xmax = xmax\n\n    def __eq__(self, other):\n        """"""Return true if other has same coordinates.""""""\n        return self.tuple_format() == other.tuple_format()\n\n    def __ne__(self, other):\n        """"""Return true if other has different coordinates.""""""\n        return self.tuple_format() != other.tuple_format()\n\n    def get_height(self):\n        """"""Return height of Box.""""""\n        return self.ymax - self.ymin\n\n    def get_width(self):\n        """"""Return width of Box.""""""\n        return self.xmax - self.xmin\n\n    def get_area(self):\n        """"""Return area of Box.""""""\n        return self.get_height() * self.get_width()\n\n    def rasterio_format(self):\n        """"""Return Box in Rasterio format.""""""\n        return ((self.ymin, self.ymax), (self.xmin, self.xmax))\n\n    def tuple_format(self):\n        return (self.ymin, self.xmin, self.ymax, self.xmax)\n\n    def shapely_format(self):\n        return (self.xmin, self.ymin, self.xmax, self.ymax)\n\n    def to_int(self):\n        return Box(\n            int(self.ymin), int(self.xmin), int(self.ymax), int(self.xmax))\n\n    def npbox_format(self):\n        """"""Return Box in npbox format used by TF Object Detection API.\n\n        Returns:\n            Numpy array of form [ymin, xmin, ymax, xmax] with float type\n\n        """"""\n        return np.array(\n            [self.ymin, self.xmin, self.ymax, self.xmax], dtype=np.float)\n\n    @staticmethod\n    def to_npboxes(boxes):\n        """"""Return nx4 numpy array from list of Box.""""""\n        nb_boxes = len(boxes)\n        npboxes = np.empty((nb_boxes, 4))\n        for boxind, box in enumerate(boxes):\n            npboxes[boxind, :] = box.npbox_format()\n        return npboxes\n\n    def __str__(self):  # pragma: no cover\n        return str(self.npbox_format())\n\n    def __repr__(self):  # pragma: no cover\n        return str(self)\n\n    def geojson_coordinates(self):\n        """"""Return Box as GeoJSON coordinates.""""""\n        # Compass directions:\n        nw = [self.xmin, self.ymin]\n        ne = [self.xmin, self.ymax]\n        se = [self.xmax, self.ymax]\n        sw = [self.xmax, self.ymin]\n        return [nw, ne, se, sw, nw]\n\n    def make_random_square_container(self, size):\n        """"""Return a new square Box that contains this Box.\n\n        Args:\n            size: the width and height of the new Box\n\n        """"""\n        if size < self.get_width():\n            raise BoxSizeError(\'size of random container cannot be < width\')\n\n        if size < self.get_height():  # pragma: no cover\n            raise BoxSizeError(\'size of random container cannot be < height\')\n\n        lb = self.ymin - (size - self.get_height())\n        ub = self.ymin\n        rand_y = random.randint(int(lb), int(ub))\n\n        lb = self.xmin - (size - self.get_width())\n        ub = self.xmin\n        rand_x = random.randint(int(lb), int(ub))\n\n        return Box.make_square(rand_y, rand_x, size)\n\n    def make_random_square(self, size):\n        """"""Return new randomly positioned square Box that lies inside this Box.\n\n        Args:\n            size: the height and width of the new Box\n\n        """"""\n        if size >= self.get_width():\n            raise BoxSizeError(\'size of random square cannot be >= width\')\n\n        if size >= self.get_height():  # pragma: no cover\n            raise BoxSizeError(\'size of random square cannot be >= height\')\n\n        lb = self.ymin\n        ub = self.ymax - size\n        rand_y = random.randint(int(lb), int(ub))\n\n        lb = self.xmin\n        ub = self.xmax - size\n        rand_x = random.randint(int(lb), int(ub))\n\n        return Box.make_square(rand_y, rand_x, size)\n\n    def intersection(self, other):\n        """"""Return the intersection of this Box and the other.\n\n        Args:\n            other: The box to intersect with this one.\n\n        Returns:\n             The intersection of this box and the other one.\n\n        """"""\n        xmin = max(self.xmin, other.xmin)\n        ymin = max(self.ymin, other.ymin)\n        xmax = min(self.xmax, other.xmax)\n        ymax = min(self.ymax, other.ymax)\n        return Box(xmin=xmin, ymin=ymin, xmax=xmax, ymax=ymax)\n\n    @staticmethod\n    def from_npbox(npbox):\n        """"""Return new Box based on npbox format.\n\n        Args:\n            npbox: Numpy array of form [ymin, xmin, ymax, xmax] with float type\n\n        """"""\n        return Box(*npbox)\n\n    @staticmethod\n    def from_shapely(shape):\n        bounds = shape.bounds\n        return Box(bounds[1], bounds[0], bounds[3], bounds[2])\n\n    @staticmethod\n    def from_tuple(tup):\n        """"""Return new Box based on tuple format.\n\n        Args:\n           tup: Tuple format box (ymin, xmin, ymax, xmax)\n        """"""\n        return Box(tup[0], tup[1], tup[2], tup[3])\n\n    def to_shapely(self):\n        return ShapelyBox(*(self.shapely_format()))\n\n    def reproject(self, transform_fn):\n        """"""Reprojects this box based on a transform function.\n\n        Args:\n          transform_fn - A function that takes in a tuple (x, y)\n                         and reprojects that point to the target\n                         coordinate reference system.\n        """"""\n        (xmin, ymin) = transform_fn((self.xmin, self.ymin))\n        (xmax, ymax) = transform_fn((self.xmax, self.ymax))\n\n        return Box(ymin, xmin, ymax, xmax)\n\n    @staticmethod\n    def make_square(ymin, xmin, size):\n        """"""Return new square Box.""""""\n        return Box(ymin, xmin, ymin + size, xmin + size)\n\n    def make_eroded(self, erosion_sz):\n        """"""Return new Box whose sides are eroded by erosion_sz.""""""\n        return Box(self.ymin + erosion_sz, self.xmin + erosion_sz,\n                   self.ymax - erosion_sz, self.xmax - erosion_sz)\n\n    def make_buffer(self, buffer_sz, max_extent):\n        """"""Return new Box whose sides are buffered by buffer_sz.\n\n        The resulting box is clipped so that the values of the corners are\n        always greater than zero and less than the height and width of\n        max_extent.\n\n        """"""\n        buffer_sz = max(0., buffer_sz)\n        if buffer_sz < 1.:\n            delta_width = int(round(buffer_sz * self.get_width()))\n            delta_height = int(round(buffer_sz * self.get_height()))\n        else:\n            delta_height = delta_width = int(round(buffer_sz))\n\n        return Box(\n            max(0, math.floor(self.ymin - delta_height)),\n            max(0, math.floor(self.xmin - delta_width)),\n            min(max_extent.get_height(),\n                int(self.ymax) + delta_height),\n            min(max_extent.get_width(),\n                int(self.xmax) + delta_width))\n\n    def make_copy(self):\n        return Box(*(self.tuple_format()))\n\n    def get_windows(self, chip_sz, stride):\n        """"""Return list of grid of boxes within this box.\n\n        Args:\n            chip_sz: (int) the length of each square-shaped window in pixels\n            stride: (int) how much each window is offset from the last in pixels\n\n        """"""\n        height = self.get_height()\n        width = self.get_width()\n\n        result = []\n        for row_start in range(0, height, stride):\n            for col_start in range(0, width, stride):\n                result.append(Box.make_square(row_start, col_start, chip_sz))\n        return result\n\n    def to_dict(self):\n        return {\n            \'xmin\': self.xmin,\n            \'ymin\': self.ymin,\n            \'xmax\': self.xmax,\n            \'ymax\': self.ymax\n        }\n\n    @classmethod\n    def from_dict(cls, d):\n        return cls(d[\'ymin\'], d[\'xmin\'], d[\'ymax\'], d[\'xmax\'])\n\n    @staticmethod\n    def filter_by_aoi(windows, aoi_polygons):\n        """"""Filters windows by a list of AOI polygons""""""\n        result = []\n        for window in windows:\n            w = window.to_shapely()\n            for polygon in aoi_polygons:\n                if w.within(polygon):\n                    result.append(window)\n                    break\n\n        return result\n'"
rastervision2/core/cli.py,0,"b'import click\n\nfrom rastervision2.pipeline import rv_config\nfrom rastervision2.pipeline.cli import main\nfrom rastervision2.core.predictor import Predictor\n\n\n# https://stackoverflow.com/questions/48391777/nargs-equivalent-for-options-in-click\nclass OptionEatAll(click.Option):\n    def __init__(self, *args, **kwargs):\n        self.save_other_options = kwargs.pop(\'save_other_options\', True)\n        nargs = kwargs.pop(\'nargs\', -1)\n        assert nargs == -1, \'nargs, if set, must be -1 not {}\'.format(nargs)\n        super(OptionEatAll, self).__init__(*args, **kwargs)\n        self._previous_parser_process = None\n        self._eat_all_parser = None\n\n    def add_to_parser(self, parser, ctx):\n        def parser_process(value, state):\n            value = str(value)\n            while state.rargs:\n                value = \'{} {}\'.format(value, state.rargs.pop(0))\n            self._previous_parser_process(value, state)\n\n        retval = super(OptionEatAll, self).add_to_parser(parser, ctx)\n\n        for name in self.opts:\n            our_parser = parser._long_opt.get(name) or parser._short_opt.get(\n                name)\n            if our_parser:\n                self._eat_all_parser = our_parser\n                self._previous_parser_process = our_parser.process\n                our_parser.process = parser_process\n                break\n\n        return retval\n\n\n@main.command(\n    \'predict\', short_help=\'Use a model bundle to predict on new images.\')\n@click.pass_context\n@click.argument(\'model_bundle\')\n@click.argument(\'image_uri\')\n@click.argument(\'output_uri\')\n@click.option(\n    \'--update-stats\',\n    \'-a\',\n    is_flag=True,\n    help=(\'Run an analysis on this individual image, as \'\n          \'opposed to using any analysis like statistics \'\n          \'that exist in the prediction package\'))\n@click.option(\n    \'--channel-order\',\n    cls=OptionEatAll,\n    help=\'List of indices comprising channel_order. Example: 2 1 0\')\ndef predict(ctx, model_bundle, image_uri, output_uri, update_stats,\n            channel_order):\n    """"""Make predictions on the images at IMAGE_URI\n    using MODEL_BUNDLE and store the prediction output at OUTPUT_URI.\n    """"""\n    if channel_order is not None:\n        channel_order = [\n            int(channel_ind) for channel_ind in channel_order.split(\' \')\n        ]\n\n    with rv_config.get_tmp_dir() as tmp_dir:\n        predictor = Predictor(model_bundle, tmp_dir, update_stats,\n                              channel_order)\n        predictor.predict([image_uri], output_uri)\n'"
rastervision2/core/data_sample.py,0,"b'from pydantic import BaseModel\nfrom numpy import ndarray\n\nfrom rastervision2.core.box import Box\nfrom rastervision2.core.data import Labels\n\n\nclass DataSample(BaseModel):\n    """"""A chip and labels along with metadata.""""""\n    chip: ndarray\n    window: Box\n    labels: Labels\n    scene_id: str = \'default\'\n    is_train: bool = True\n\n    class Config:\n        arbitrary_types_allowed = True\n'"
rastervision2/core/predictor.py,0,"b'from os.path import join\nimport zipfile\n\nfrom rastervision2.pipeline import rv_config\nfrom rastervision2.pipeline.config import build_config\nfrom rastervision2.pipeline.file_system.utils import (download_if_needed,\n                                                      make_dir, file_to_json)\nfrom rastervision2.core.data.raster_source import ChannelOrderError\nfrom rastervision2.core.analyzer import StatsAnalyzerConfig\n\n\nclass Predictor():\n    """"""Class for making predictions based off of a model bundle.""""""\n\n    def __init__(self,\n                 model_bundle_uri,\n                 tmp_dir,\n                 update_stats=False,\n                 channel_order=None):\n        """"""Creates a new Predictor.\n\n        Args:\n            model_bundle_uri: URI of the model bundle to use. Can be any\n                type of URI that Raster Vision can read.\n            tmp_dir: Temporary directory in which to store files that are used\n                by the Predictor. This directory is not cleaned up by this\n                class.\n            channel_order: Option for a new channel order to use for the\n                imagery being predicted against. If not present, the\n                channel_order from the original configuration in the predict\n                package will be used.\n        """"""\n        self.tmp_dir = tmp_dir\n        self.update_stats = update_stats\n        self.model_loaded = False\n\n        bundle_path = download_if_needed(model_bundle_uri, tmp_dir)\n        bundle_dir = join(tmp_dir, \'bundle\')\n        make_dir(bundle_dir)\n        with zipfile.ZipFile(bundle_path, \'r\') as bundle_zip:\n            bundle_zip.extractall(path=bundle_dir)\n\n        config_path = join(bundle_dir, \'pipeline-config.json\')\n        config_dict = file_to_json(config_path)\n        rv_config.set_everett_config(\n            config_overrides=config_dict.get(\'rv_config\'))\n\n        self.pipeline = build_config(config_dict).build(tmp_dir)\n        self.scene = None\n\n        if not hasattr(self.pipeline, \'predict\'):\n            raise Exception(\n                \'pipeline in model bundle must have predict method\')\n\n        self.scene = self.pipeline.config.dataset.validation_scenes[0]\n\n        if not hasattr(self.scene.raster_source, \'uris\'):\n            raise Exception(\n                \'raster_source in model bundle must have uris as field\')\n\n        if not hasattr(self.scene.label_store, \'uri\'):\n            raise Exception(\n                \'label_store in model bundle must have uri as field\')\n\n        for t in self.scene.raster_source.transformers:\n            t.update_root(bundle_dir)\n\n        if self.update_stats:\n            stats_analyzer = StatsAnalyzerConfig(\n                output_uri=join(bundle_dir, \'stats.json\'))\n            self.pipeline.config.analyzers = [stats_analyzer]\n\n        self.scene.label_source = None\n        self.scene.aoi_uris = None\n        self.pipeline.config.dataset.train_scenes = [self.scene]\n        self.pipeline.config.dataset.validation_scenes = [self.scene]\n        self.pipeline.config.dataset.test_scenes = None\n        if channel_order is not None:\n            self.scene.raster_source.channel_order = channel_order\n\n    def predict(self, image_uris, label_uri):\n        """"""Generate predictions for the given image.\n\n        Args:\n            image_uris: URIs of the images to make predictions against.\n                This can be any type of URI readable by Raster Vision\n                FileSystems.\n            label_uri: URI to save labels off into.\n        """"""\n        try:\n            self.scene.raster_source.uris = image_uris\n            self.scene.label_store.uri = label_uri\n            if self.update_stats:\n                self.pipeline.analyze()\n            self.pipeline.predict()\n        except ChannelOrderError:\n            raise ValueError(\n                \'The predict package is using a channel_order \'\n                \'with channels unavailable in the imagery.\\nTo set a new \'\n                \'channel_order that only uses channels available in the \'\n                \'imagery, use the --channel-order option.\')\n'"
rastervision2/core/raster_stats.py,0,"b'import json\n\nimport numpy as np\n\nfrom rastervision2.pipeline.file_system import str_to_file, file_to_str\n\nchip_sz = 300\n\n\ndef parallel_variance(mean_a, count_a, var_a, mean_b, count_b, var_b):\n    """"""Compute the variance based on stats from two partitions of the data.\n\n    See ""Parallel Algorithm"" in\n    https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance\n\n    Args:\n        mean_a: the mean of partition a\n        count_a: the number of elements in partition a\n        var_a: the variance of partition a\n        mean_b: the mean of partition b\n        count_b: the number of elements in partition b\n        var_b: the variance of partition b\n\n    Return:\n        the variance of the two partitions if they were combined\n    """"""\n    delta = mean_b - mean_a\n    m_a = var_a * (count_a - 1)\n    m_b = var_b * (count_b - 1)\n    M2 = m_a + m_b + delta**2 * count_a * count_b / (count_a + count_b)\n    var = M2 / (count_a + count_b - 1)\n    return var\n\n\ndef parallel_mean(mean_a, count_a, mean_b, count_b):\n    """"""Compute the mean based on stats from two partitions of the data.\n\n    See ""Parallel Algorithm"" in\n    https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance\n\n    Args:\n        mean_a: the mean of partition a\n        count_a: the number of elements in partition a\n        mean_b: the mean of partition b\n        count_b: the number of elements in partition b\n\n    Return:\n        the mean of the two partitions if they were combined\n    """"""\n    mean = (count_a * mean_a + count_b * mean_b) / (count_a + count_b)\n    return mean\n\n\nclass RasterStats():\n    def __init__(self):\n        self.means = None\n        self.stds = None\n\n    def compute(self, raster_sources, sample_prob=None):\n        """"""Compute the mean and stds over all the raster_sources.\n\n        This ignores NODATA values.\n\n        If sample_prob is set, then a subset of each scene is used to compute stats which\n        speeds up the computation. Roughly speaking, if sample_prob=0.5, then half the\n        pixels in the scene will be used. More precisely, the number of chips is equal to\n        sample_prob * (width * height / 300^2), or 1, whichever is greater. Each chip is\n        uniformly sampled from the scene with replacement. Otherwise, it uses a sliding\n        window over the entire scene to compute stats.\n\n        Args:\n            raster_sources: list of RasterSource\n            sample_prob: (float or None) between 0 and 1\n        """"""\n        stride = chip_sz\n        nb_channels = raster_sources[0].num_channels\n\n        def get_chip(raster_source, window):\n            """"""Return chip or None if all values are NODATA.""""""\n            chip = raster_source.get_raw_chip(window).astype(np.float32)\n            # Convert shape from [h,w,c] to [c,h*w]\n            chip = np.reshape(np.transpose(chip, [2, 0, 1]), (nb_channels, -1))\n\n            # Ignore NODATA values.\n            chip[chip == 0.0] = np.nan\n            if np.any(~np.isnan(chip)):\n                return chip\n            return None\n\n        def sliding_chip_stream():\n            """"""Get stream of chips using a sliding window of size 300.""""""\n            for raster_source in raster_sources:\n                with raster_source.activate():\n                    windows = raster_source.get_extent().get_windows(\n                        chip_sz, stride)\n                    for window in windows:\n                        chip = get_chip(raster_source, window)\n                        if chip is not None:\n                            yield chip\n\n        def random_chip_stream():\n            """"""Get random stream of chips.""""""\n            for raster_source in raster_sources:\n                with raster_source.activate():\n                    extent = raster_source.get_extent()\n                    num_pixels = extent.get_width() * extent.get_height()\n                    num_chips = round(\n                        sample_prob * (num_pixels / (chip_sz**2)))\n                    num_chips = max(1, num_chips)\n                    for _ in range(num_chips):\n                        window = raster_source.get_extent().make_random_square(\n                            chip_sz)\n                        chip = get_chip(raster_source, window)\n                        if chip is not None:\n                            yield chip\n\n        # For each chip, compute the mean and var of that chip and then update the\n        # running mean and var.\n        count = 0\n        mean = np.zeros((nb_channels, ))\n        var = np.zeros((nb_channels, ))\n        chip_stream = (sliding_chip_stream()\n                       if sample_prob is None else random_chip_stream())\n\n        for c in chip_stream:\n            chip_means = np.nanmean(c, axis=1)\n            chip_vars = np.nanvar(c, axis=1)\n            chip_count = np.sum(c[0] != np.nan)\n\n            var = parallel_variance(chip_means, chip_count, chip_vars, mean,\n                                    count, var)\n            mean = parallel_mean(chip_means, chip_count, mean, count)\n            count += chip_count\n\n        self.means = mean\n        self.stds = np.sqrt(var)\n\n    def save(self, stats_uri):\n        # Ensure lists\n        means = list(self.means)\n        stds = list(self.stds)\n        stats = {\'means\': means, \'stds\': stds}\n        str_to_file(json.dumps(stats), stats_uri)\n\n    @staticmethod\n    def load(stats_uri):\n        stats_json = json.loads(file_to_str(stats_uri))\n        stats = RasterStats()\n        stats.means = stats_json[\'means\']\n        stats.stds = stats_json[\'stds\']\n        return stats\n'"
rastervision2/examples/__init__.py,0,b'# flake8: noqa\n\nimport rastervision2.pipeline\n'
rastervision2/examples/chip_classification.py,0,"b""# flake8: noqa\n\nimport os\nfrom os.path import join\n\nfrom rastervision2.core.rv_pipeline import *\nfrom rastervision2.core.backend import *\nfrom rastervision2.core.data import *\nfrom rastervision2.core.analyzer import *\nfrom rastervision2.pytorch_backend import *\nfrom rastervision2.pytorch_learner import *\nfrom rastervision2.examples.utils import get_scene_info, save_image_crop\n\naoi_path = 'AOIs/AOI_1_Rio/srcData/buildingLabels/Rio_OUTLINE_Public_AOI.geojson'\n\n\ndef get_config(runner, test=False, output_dir='output'):\n    if runner in ['inprocess']:\n        raw_uri = '/opt/data/raw-data/spacenet-dataset'\n        processed_uri = '/opt/data/examples/spacenet/rio/processed-data'\n        root_uri = '/opt/data/examples/spacenet-rio-cc'\n    else:\n        raw_uri = 's3://spacenet-dataset/'\n        processed_uri = 's3://raster-vision-lf-dev/examples/spacenet/rio/processed-data'\n        root_uri = 's3://raster-vision-lf-dev/examples/spacenet-rio-cc'\n    root_uri = join(root_uri, output_dir)\n\n    debug = False\n    train_scene_info = get_scene_info(join(processed_uri, 'train-scenes.csv'))\n    val_scene_info = get_scene_info(join(processed_uri, 'val-scenes.csv'))\n    log_tensorboard = True\n    run_tensorboard = True\n    class_config = ClassConfig(names=['no_building', 'building'])\n\n    if test:\n        debug = True\n        train_scene_info = train_scene_info[0:1]\n        val_scene_info = val_scene_info[0:1]\n\n    def make_scene(scene_info):\n        (raster_uri, label_uri) = scene_info\n        raster_uri = join(raw_uri, raster_uri)\n        label_uri = join(processed_uri, label_uri)\n        aoi_uri = join(raw_uri, aoi_path)\n\n        if test:\n            crop_uri = join(processed_uri, 'crops',\n                            os.path.basename(raster_uri))\n            label_crop_uri = join(processed_uri, 'crops',\n                                  os.path.basename(label_uri))\n\n            save_image_crop(\n                raster_uri,\n                crop_uri,\n                label_uri=label_uri,\n                label_crop_uri=label_crop_uri,\n                size=600,\n                min_features=20,\n                class_config=class_config)\n            raster_uri = crop_uri\n            label_uri = label_crop_uri\n\n        id = os.path.splitext(os.path.basename(raster_uri))[0]\n        raster_source = RasterioSourceConfig(\n            channel_order=[0, 1, 2], uris=[raster_uri])\n        label_source = ChipClassificationLabelSourceConfig(\n            vector_source=GeoJSONVectorSourceConfig(\n                uri=label_uri, default_class_id=1, ignore_crs_field=True),\n            ioa_thresh=0.5,\n            use_intersection_over_cell=False,\n            pick_min_class_id=False,\n            background_class_id=0,\n            infer_cells=True)\n\n        return SceneConfig(\n            id=id,\n            raster_source=raster_source,\n            label_source=label_source,\n            aoi_uris=[aoi_uri])\n\n    chip_sz = 200\n    train_scenes = [make_scene(info) for info in train_scene_info]\n    val_scenes = [make_scene(info) for info in val_scene_info]\n    dataset = DatasetConfig(\n        class_config=class_config,\n        train_scenes=train_scenes,\n        validation_scenes=val_scenes)\n\n    model = ClassificationModelConfig(backbone=Backbone.resnet50)\n    solver = SolverConfig(\n        lr=1e-4, num_epochs=20, test_num_epochs=3, batch_sz=32, one_cycle=True)\n    backend = PyTorchChipClassificationConfig(\n        model=model,\n        solver=solver,\n        log_tensorboard=log_tensorboard,\n        run_tensorboard=run_tensorboard,\n        test_mode=test)\n\n    config = ChipClassificationConfig(\n        root_uri=root_uri,\n        dataset=dataset,\n        backend=backend,\n        train_chip_sz=chip_sz,\n        predict_chip_sz=chip_sz)\n    return config\n"""
rastervision2/examples/object_detection.py,0,"b""# flake8: noqa\n\nimport os\nfrom os.path import join\n\nfrom rastervision2.core.rv_pipeline import *\nfrom rastervision2.core.backend import *\nfrom rastervision2.core.data import *\nfrom rastervision2.core.analyzer import *\nfrom rastervision2.pytorch_backend import *\nfrom rastervision2.pytorch_learner import *\nfrom rastervision2.examples.utils import get_scene_info, save_image_crop\n\n\ndef get_config(runner, test=False, output_dir='output'):\n    if runner in ['inprocess']:\n        raw_uri = '/opt/data/raw-data/isprs-potsdam'\n        processed_uri = '/opt/data/examples/cowc-potsdam/processed-data'\n        root_uri = '/opt/data/examples/cowc-potsdam-od'\n    else:\n        raw_uri = 's3://raster-vision-raw-data/isprs-potsdam'\n        processed_uri = 's3://raster-vision-lf-dev/examples/cowc-potsdam/processed-data'\n        root_uri = 's3://raster-vision-lf-dev/examples/cowc-potsdam-od'\n    root_uri = join(root_uri, output_dir)\n\n    train_ids = [\n        '2_10', '2_11', '2_12', '2_14', '3_11', '3_13', '4_10', '5_10', '6_7',\n        '6_9'\n    ]\n    val_ids = ['2_13', '6_8', '3_10']\n\n    if test:\n        train_ids = train_ids[0:1]\n        val_ids = val_ids[0:1]\n\n    def make_scene(id):\n        raster_uri = join(raw_uri,\n                          '4_Ortho_RGBIR/top_potsdam_{}_RGBIR.tif'.format(id))\n        label_uri = join(processed_uri, 'labels', 'all',\n                         'top_potsdam_{}_RGBIR.json'.format(id))\n\n        if test:\n            crop_uri = join(processed_uri, 'crops',\n                            os.path.basename(raster_uri))\n            save_image_crop(\n                raster_uri,\n                crop_uri,\n                label_uri=label_uri,\n                size=1000,\n                min_features=5)\n            raster_uri = crop_uri\n\n        raster_source = RasterioSourceConfig(\n            uris=[raster_uri], channel_order=[0, 1, 2])\n\n        label_source = ObjectDetectionLabelSourceConfig(\n            vector_source=GeoJSONVectorSourceConfig(\n                uri=label_uri, default_class_id=0, ignore_crs_field=True))\n\n        return SceneConfig(\n            id=id, raster_source=raster_source, label_source=label_source)\n\n    class_config = ClassConfig(names=['vehicle'])\n    chip_sz = 300\n    dataset = DatasetConfig(\n        class_config=class_config,\n        train_scenes=[make_scene(id) for id in train_ids],\n        validation_scenes=[make_scene(id) for id in val_ids])\n    chip_options = ObjectDetectionChipOptions(neg_ratio=5.0, ioa_thresh=0.9)\n    predict_options = ObjectDetectionPredictOptions(\n        merge_thresh=0.5, score_thresh=0.9)\n\n    backend = PyTorchObjectDetectionConfig(\n        model=ObjectDetectionModelConfig(backbone=Backbone.resnet50),\n        solver=SolverConfig(\n            lr=1e-4,\n            num_epochs=10,\n            test_num_epochs=2,\n            batch_sz=16,\n            one_cycle=True),\n        log_tensorboard=True,\n        run_tensorboard=False,\n        test_mode=test)\n\n    return ObjectDetectionConfig(\n        root_uri=root_uri,\n        dataset=dataset,\n        backend=backend,\n        train_chip_sz=chip_sz,\n        predict_chip_sz=chip_sz,\n        chip_options=chip_options,\n        predict_options=predict_options)\n"""
rastervision2/examples/pipeline.py,0,"b""from rastervision2.pipeline.pipeline_config import PipelineConfig\n\n\ndef get_config(runner):\n    root_uri = ('s3://raster-vision-lf-dev/examples/test-output/pipeline'\n                if runner == 'aws_batch' else\n                '/opt/data/examples/test-output/pipeline')\n    return PipelineConfig(root_uri=root_uri)\n"""
rastervision2/examples/semantic_segmentation.py,0,"b""# flake8: noqa\n\nimport os\nfrom os.path import join\n\nfrom rastervision2.core.rv_pipeline import *\nfrom rastervision2.core.backend import *\nfrom rastervision2.core.data import *\nfrom rastervision2.core.analyzer import *\nfrom rastervision2.pytorch_backend import *\nfrom rastervision2.pytorch_learner import *\nfrom rastervision2.examples.utils import get_scene_info, save_image_crop\n\n\ndef get_config(runner, test=False, output_dir='output'):\n    if runner in ['inprocess']:\n        raw_uri = '/opt/data/raw-data/isprs-potsdam/'\n        processed_uri = '/opt/data/examples/potsdam/processed-data'\n        root_uri = '/opt/data/examples/potsdam-semseg/'\n    else:\n        raw_uri = 's3://raster-vision-raw-data/isprs-potsdam'\n        processed_uri = 's3://raster-vision-lf-dev/examples/potsdam/processed-data'\n        root_uri = 's3://raster-vision-lf-dev/examples/potsdam-semseg'\n    root_uri = join(root_uri, output_dir)\n\n    train_ids = [\n        '2-10', '2-11', '3-10', '3-11', '4-10', '4-11', '4-12', '5-10', '5-11',\n        '5-12', '6-10', '6-11', '6-7', '6-9', '7-10', '7-11', '7-12', '7-7',\n        '7-8', '7-9'\n    ]\n    val_ids = ['2-12', '3-12', '6-12']\n\n    if test:\n        train_ids = train_ids[0:1]\n        val_ids = val_ids[0:1]\n\n    class_config = ClassConfig(\n        names=[\n            'Car', 'Building', 'Low Vegetation', 'Tree', 'Impervious',\n            'Clutter'\n        ],\n        colors=[\n            '#ffff00', '#0000ff', '#00ffff', '#00ff00', '#ffffff', '#ff0000'\n        ])\n\n    def make_scene(id):\n        id = id.replace('-', '_')\n        raster_uri = '{}/4_Ortho_RGBIR/top_potsdam_{}_RGBIR.tif'.format(\n            raw_uri, id)\n        label_uri = '{}/5_Labels_for_participants/top_potsdam_{}_label.tif'.format(\n            raw_uri, id)\n\n        if test:\n            crop_uri = join(processed_uri, 'crops',\n                            os.path.basename(raster_uri))\n            label_crop_uri = join(processed_uri, 'crops',\n                                  os.path.basename(label_uri))\n            save_image_crop(\n                raster_uri,\n                crop_uri,\n                label_uri=label_uri,\n                label_crop_uri=label_crop_uri,\n                size=600,\n                vector_labels=False)\n            raster_uri = crop_uri\n            label_uri = label_crop_uri\n\n        # infrared, red, green\n        channel_order = [3, 0, 1]\n        raster_source = RasterioSourceConfig(\n            uris=[raster_uri], channel_order=channel_order)\n\n        # Using with_rgb_class_map because label TIFFs have classes encoded as\n        # RGB colors.\n        label_source = SemanticSegmentationLabelSourceConfig(\n            rgb_class_config=class_config,\n            raster_source=RasterioSourceConfig(uris=[label_uri]))\n\n        # URI will be injected by scene config.\n        # Using rgb=True because we want prediction TIFFs to be in\n        # RGB format.\n        label_store = SemanticSegmentationLabelStoreConfig(\n            rgb=True, vector_output=[PolygonVectorOutputConfig(class_id=0)])\n\n        scene = SceneConfig(\n            id=id,\n            raster_source=raster_source,\n            label_source=label_source,\n            label_store=label_store)\n\n        return scene\n\n    dataset = DatasetConfig(\n        class_config=class_config,\n        train_scenes=[make_scene(id) for id in train_ids],\n        validation_scenes=[make_scene(id) for id in val_ids])\n    chip_sz = 300\n    chip_options = SemanticSegmentationChipOptions(\n        window_method=SemanticSegmentationWindowMethod.sliding, stride=chip_sz)\n\n    backend = PyTorchSemanticSegmentationConfig(\n        model=SemanticSegmentationModelConfig(backbone=Backbone.resnet50),\n        solver=SolverConfig(\n            lr=1e-4,\n            num_epochs=10,\n            test_num_epochs=2,\n            batch_sz=8,\n            one_cycle=True),\n        log_tensorboard=True,\n        run_tensorboard=False,\n        test_mode=test)\n\n    return SemanticSegmentationConfig(\n        root_uri=root_uri,\n        dataset=dataset,\n        backend=backend,\n        train_chip_sz=chip_sz,\n        predict_chip_sz=chip_sz,\n        chip_options=chip_options)\n"""
rastervision2/examples/tiny_spacenet.py,0,"b'# flake8: noqa\n\nfrom os.path import join\n\nfrom rastervision2.core.rv_pipeline import *\nfrom rastervision2.core.backend import *\nfrom rastervision2.core.data import *\nfrom rastervision2.pytorch_backend import *\nfrom rastervision2.pytorch_learner import *\n\n\ndef get_config(runner):\n    root_uri = \'/opt/data/output/\'\n    base_uri = (\'https://s3.amazonaws.com/azavea-research-public-data/\'\n                \'raster-vision/examples/spacenet\')\n    train_image_uri = \'{}/RGB-PanSharpen_AOI_2_Vegas_img205.tif\'.format(\n        base_uri)\n    train_label_uri = \'{}/buildings_AOI_2_Vegas_img205.geojson\'.format(\n        base_uri)\n    val_image_uri = \'{}/RGB-PanSharpen_AOI_2_Vegas_img25.tif\'.format(base_uri)\n    val_label_uri = \'{}/buildings_AOI_2_Vegas_img25.geojson\'.format(base_uri)\n    channel_order = [0, 1, 2]\n    class_config = ClassConfig(\n        names=[\'building\', \'background\'], colors=[\'red\', \'black\'])\n\n    def make_scene(scene_id, image_uri, label_uri):\n        """"""\n        - StatsTransformer is used to convert uint16 values to uint8.\n        - The GeoJSON does not have a class_id property for each geom,\n          so it is inferred as 0 (ie. building) because the default_class_id\n          is set to 0.\n        - The labels are in the form of GeoJSON which needs to be rasterized\n          to use as label for semantic segmentation, so we use a RasterizedSource.\n        - The rasterizer set the background (as opposed to foreground) pixels\n          to 1 because background_class_id is set to 1.\n        """"""\n        raster_source = RasterioSourceConfig(\n            uris=[image_uri],\n            channel_order=channel_order,\n            transformers=[StatsTransformerConfig()])\n        label_source = SemanticSegmentationLabelSourceConfig(\n            raster_source=RasterizedSourceConfig(\n                vector_source=GeoJSONVectorSourceConfig(\n                    uri=label_uri, default_class_id=0, ignore_crs_field=True),\n                rasterizer_config=RasterizerConfig(background_class_id=1)))\n        return SceneConfig(\n            id=scene_id,\n            raster_source=raster_source,\n            label_source=label_source)\n\n    dataset = DatasetConfig(\n        class_config=class_config,\n        train_scenes=[\n            make_scene(\'scene_205\', train_image_uri, train_label_uri)\n        ],\n        validation_scenes=[\n            make_scene(\'scene_25\', val_image_uri, val_label_uri)\n        ])\n\n    # Use the PyTorch backend for the SemanticSegmentation pipeline.\n    chip_sz = 300\n    backend = PyTorchSemanticSegmentationConfig(\n        model=SemanticSegmentationModelConfig(backbone=Backbone.resnet50),\n        solver=SolverConfig(lr=1e-4, num_epochs=1, batch_sz=2))\n    chip_options = SemanticSegmentationChipOptions(\n        window_method=SemanticSegmentationWindowMethod.random_sample,\n        chips_per_scene=10)\n\n    return SemanticSegmentationConfig(\n        root_uri=root_uri,\n        dataset=dataset,\n        backend=backend,\n        train_chip_sz=chip_sz,\n        predict_chip_sz=chip_sz,\n        chip_options=chip_options)\n'"
rastervision2/examples/utils.py,0,"b'import csv\nfrom io import StringIO\nimport tempfile\nimport os\n\nimport rasterio\nfrom shapely.strtree import STRtree\nfrom shapely.geometry import shape, mapping\nfrom shapely.ops import transform\n\nfrom rastervision2.core import Box\nfrom rastervision2.core.data import (RasterioCRSTransformer,\n                                     GeoJSONVectorSourceConfig)\nfrom rastervision2.pipeline.file_system import (file_to_str, file_exists,\n                                                get_local_path, upload_or_copy,\n                                                make_dir, json_to_file)\nfrom rastervision2.aws_s3 import S3FileSystem\n\n\ndef str_to_bool(x):\n    if type(x) == str:\n        if x.lower() == \'true\':\n            return True\n        elif x.lower() == \'false\':\n            return False\n        else:\n            raise ValueError(\'{} is expected to be true or false\'.format(x))\n    return x\n\n\ndef get_scene_info(csv_uri):\n    csv_str = file_to_str(csv_uri)\n    reader = csv.reader(StringIO(csv_str), delimiter=\',\')\n    return list(reader)\n\n\ndef crop_image(image_uri, window, crop_uri):\n    im_dataset = rasterio.open(image_uri)\n    rasterio_window = window.rasterio_format()\n    im = im_dataset.read(window=rasterio_window)\n\n    with tempfile.TemporaryDirectory() as tmp_dir:\n        crop_path = get_local_path(crop_uri, tmp_dir)\n        make_dir(crop_path, use_dirname=True)\n\n        meta = im_dataset.meta\n        meta[\'width\'], meta[\'height\'] = window.get_width(), window.get_height()\n        meta[\'transform\'] = rasterio.windows.transform(rasterio_window,\n                                                       im_dataset.transform)\n\n        with rasterio.open(crop_path, \'w\', **meta) as dst:\n            dst.colorinterp = im_dataset.colorinterp\n            dst.write(im)\n\n        upload_or_copy(crop_path, crop_uri)\n\n\ndef save_image_crop(image_uri,\n                    image_crop_uri,\n                    label_uri=None,\n                    label_crop_uri=None,\n                    size=600,\n                    min_features=10,\n                    vector_labels=True,\n                    class_config=None):\n    """"""Save a crop of an image to use for testing.\n\n    If label_uri is set, the crop needs to cover >= min_features.\n\n    Args:\n        image_uri: URI of original image\n        image_crop_uri: URI of cropped image to save\n        label_uri: optional URI of label file\n        label_crop_uri: optional URI of cropped labels to save\n        size: height and width of crop\n\n    Raises:\n        ValueError if cannot find a crop satisfying min_features constraint.\n    """"""\n    if not file_exists(image_crop_uri):\n        print(\'Saving test crop to {}...\'.format(image_crop_uri))\n        old_environ = os.environ.copy()\n        try:\n            request_payer = S3FileSystem.get_request_payer()\n            if request_payer == \'requester\':\n                os.environ[\'AWS_REQUEST_PAYER\'] = request_payer\n            im_dataset = rasterio.open(image_uri)\n            h, w = im_dataset.height, im_dataset.width\n\n            extent = Box(0, 0, h, w)\n            windows = extent.get_windows(size, size)\n            if label_uri and vector_labels:\n                crs_transformer = RasterioCRSTransformer.from_dataset(\n                    im_dataset)\n                geojson_vs_config = GeoJSONVectorSourceConfig(uri=label_uri)\n                vs = geojson_vs_config.build(class_config, crs_transformer)\n                geojson = vs.get_geojson()\n                geoms = []\n                for f in geojson[\'features\']:\n                    g = shape(f[\'geometry\'])\n                    geoms.append(g)\n                tree = STRtree(geoms)\n\n            def p2m(x, y, z=None):\n                return crs_transformer.pixel_to_map((x, y))\n\n            for w in windows:\n                use_window = True\n                if label_uri and vector_labels:\n                    w_polys = tree.query(w.to_shapely())\n                    use_window = len(w_polys) >= min_features\n                    if use_window and label_crop_uri is not None:\n                        print(\'Saving test crop labels to {}...\'.format(\n                            label_crop_uri))\n\n                        label_crop_features = [\n                            mapping(transform(p2m, wp)) for wp in w_polys\n                        ]\n                        label_crop_json = {\n                            \'type\':\n                            \'FeatureCollection\',\n                            \'features\': [{\n                                \'geometry\': f\n                            } for f in label_crop_features]\n                        }\n                        json_to_file(label_crop_json, label_crop_uri)\n\n                if use_window:\n                    crop_image(image_uri, w, image_crop_uri)\n\n                    if not vector_labels and label_uri and label_crop_uri:\n                        crop_image(label_uri, w, label_crop_uri)\n\n                    break\n\n            if not use_window:\n                raise ValueError(\'Could not find a good crop.\')\n        finally:\n            os.environ.clear()\n            os.environ.update(old_environ)\n'"
rastervision2/gdal_vsi/__init__.py,0,b'# flake8: noqa\n\nimport rastervision2.pipeline\nfrom rastervision2.gdal_vsi.vsi_file_system import VsiFileSystem\n\n\ndef register_plugin(registry):\n    registry.add_file_system(VsiFileSystem)\n'
rastervision2/gdal_vsi/vsi_file_system.py,0,"b'from datetime import datetime\nimport os\nfrom pathlib import Path\nfrom typing import (List, Optional)\nfrom urllib.parse import urlparse\n\nfrom rastervision2.pipeline.file_system import FileSystem\n\nfrom osgeo import gdal\n\n\nclass VsiFileSystem(FileSystem):\n    """"""A FileSystem to access files over any protocol supported by GDAL\'s VSI""""""\n\n    @staticmethod\n    def uri_to_vsi_path(uri: str) -> str:\n        """"""A function to convert Rasterio-like URIs to VSI path strings\n\n        Args:\n            uri: URI of the file, possibly nested within archives as follows\n                 <archive_scheme>+<archive_URI>!path/to/contained/file.ext\n                 Acceptable URI schemes are file, s3, gs, http, https, and ftp\n                 Allowable archive schema are tar, zip, and gzip\n        """"""\n        parsed = urlparse(uri)\n        scheme = parsed.scheme.split(\'+\')[0]\n\n        archive_content = uri.rfind(\'!\')\n        if archive_content == -1:\n            # regular URI\n            if scheme == \'http\' or scheme == \'https\' or scheme == \'ftp\':\n                return \'/vsicurl/{}\'.format(uri)\n            elif scheme == \'s3\' or scheme == \'gs\':\n                return \'/vsi{}/{}{}\'.format(scheme, parsed.netloc, parsed.path)\n            else:\n                # assume file schema\n                return os.path.abspath(os.path.join(parsed.netloc, parsed.path))\n        else:\n            archive_target = uri.find(\'+\')\n            assert archive_target != -1\n\n            if scheme in [\'zip\', \'tar\', \'gzip\']:\n                return \'/vsi{}/{}/{}\'.format(\n                    scheme,\n                    VsiFileSystem.uri_to_vsi_path(\n                        uri[archive_target + 1:archive_content]),\n                    uri[archive_content + 1:])\n            else:\n                raise ValueError(\n                    \'Attempted access into archive with unsupported scheme ""{}""\'.format(\n                        scheme))\n\n    @staticmethod\n    def matches_uri(vsipath: str, mode: str) -> bool:\n        """"""Returns True if this FS can be used for the given URI/mode pair.\n\n        Args:\n            uri: URI of file\n            mode: mode to open file in, \'r\' or \'w\'\n        """"""\n        if mode == \'r\' and vsipath.startswith(\'/vsi\'):\n            return True\n        elif mode == \'w\' and vsipath.startswith(\'/vsi\') and \'/vsicurl/\' not in vsipath:\n            return True\n        else:\n            return False\n\n    @staticmethod\n    def file_exists(vsipath: str, include_dir: bool = True) -> bool:\n        """"""Check if a file exists.\n\n        Args:\n          uri: The URI to check\n          include_dir: Include directories in check, if this file_system\n            supports directory reads. Otherwise only return true if a single\n            file exists at the URI.\n        """"""\n        file_stats = gdal.VSIStatL(vsipath)\n        if include_dir:\n            return True if file_stats else False\n        else:\n            return True if file_stats and not file_stats.IsDirectory() else False\n\n    @staticmethod\n    def read_bytes(vsipath: str) -> bytes:\n        stats = gdal.VSIStatL(vsipath)\n        if not stats or stats.IsDirectory():\n            raise FileNotFoundError(\'{} does not exist\'.format(vsipath))\n\n        try:\n            handle = gdal.VSIFOpenL(vsipath, \'rb\')\n            return gdal.VSIFReadL(1, stats.size, handle)\n        finally:\n            gdal.VSIFCloseL(handle)\n\n    @staticmethod\n    def read_str(uri: str) -> str:\n        """"""Read contents of URI to a string.""""""\n        return VsiFileSystem.read_bytes(uri).decode(""UTF-8"")\n\n    @staticmethod\n    def write_bytes(vsipath: str, data: bytes):\n        try:\n            handle = gdal.VSIFOpenL(vsipath, \'wb\')\n            gdal.VSIFWriteL(data, 1, len(data), handle)\n        finally:\n            gdal.VSIFCloseL(handle)\n\n    @staticmethod\n    def write_str(uri: str, data: str):\n        """"""Write string in data to URI.""""""\n        VsiFileSystem.write_bytes(uri, data.encode())\n\n    @staticmethod\n    def sync_to_dir(src_dir: str, dst_dir_uri: str, delete: bool = False):\n        """"""Syncs a local source directory to a destination directory.\n\n        If the FileSystem is remote, this involves uploading.\n\n        Args:\n            src_dir: local source directory to sync from\n            dst_dir_uri: A destination directory that can be synced to by this\n                FileSystem\n            delete: True if the destination should be deleted first.\n        """"""\n        def work(src, vsi_dest):\n            gdal.Mkdir(vsi_dest, 0o777)\n\n            for item in src.iterdir():\n                item_vsi_dest = os.path.join(vsi_dest, item.name)\n                if item.is_dir():\n                    work(item, item_vsi_dest)\n                else:\n                    VsiFileSystem.copy_to(str(item), item_vsi_dest)\n\n        stats = gdal.VSIStatL(dst_dir_uri)\n        if stats:\n            assert delete, ""Cannot overwrite existing files if delete=False""\n            if stats.IsDirectory():\n                gdal.RmdirRecursive(dst_dir_uri)\n            else:\n                gdal.Unlink(dst_dir_uri)\n\n        src = Path(src_dir)\n        assert src.exists() and src.is_dir(), \\\n            ""Local source ({}) must be a directory"".format(src_dir)\n\n        work(src, dst_dir_uri)\n\n    @staticmethod\n    def sync_from_dir(src_dir_uri: str, dst_dir: str, delete: bool = False):\n        """"""Syncs a source directory to a local destination directory.\n\n        If the FileSystem is remote, this involves downloading.\n\n        Args:\n            src_dir_uri: source directory that can be synced from by this FileSystem\n            dst_dir: A local destination directory\n            delete: True if the destination should be deleted first.\n        """"""\n        def work(vsi_src, dest):\n            if dest.exists():\n                assert dest.is_dir(), ""Local target ({}) must be a directory"".format(dest)\n            else:\n                dest.mkdir()\n\n            for item in gdal.ReadDir(vsi_src):\n                item_vsi_src = os.path.join(vsi_src, item)\n                target = dest.joinpath(item)\n                if gdal.VSIStatL(item_vsi_src).IsDirectory():\n                    work(item_vsi_src, target)\n                else:\n                    assert not target.exists() or delete, \\\n                        ""Target location must not exist if delete=False""\n                    VsiFileSystem.copy_from(item_vsi_src, str(target))\n\n        stats = gdal.VSIStatL(src_dir_uri)\n        assert stats and stats.IsDirectory(), ""Source must be a directory""\n\n        work(src_dir_uri, Path(dst_dir))\n\n    @staticmethod\n    def copy_to(src_path: str, dst_uri: str):\n        """"""Copy a local source file to a destination.\n\n        If the FileSystem is remote, this involves uploading.\n\n        Args:\n            src_path: local path to source file\n            dst_uri: uri of destination that can be copied to by this FileSystem\n        """"""\n        with open(src_path, \'rb\') as f:\n            buf = f.read()\n        VsiFileSystem.write_bytes(dst_uri, buf)\n\n    @staticmethod\n    def copy_from(src_uri: str, dst_path: str):\n        """"""Copy a source file to a local destination.\n\n        If the FileSystem is remote, this involves downloading.\n\n        Args:\n            src_uri: uri of source that can be copied from by this FileSystem\n            dst_path: local path to destination file\n        """"""\n        buf = VsiFileSystem.read_bytes(src_uri)\n        with open(dst_path, \'wb\') as f:\n            f.write(buf)\n\n    @staticmethod\n    def local_path(vsipath: str, download_dir: str) -> str:\n        """"""Return the path where a local copy should be stored.\n\n        Args:\n            uri: the URI of the file to be copied\n            download_dir: path of the local directory in which files should\n                be copied\n        """"""\n        filename = Path(vsipath).name\n        return os.path.join(download_dir, filename)\n\n    @staticmethod\n    def last_modified(vsipath: str) -> Optional[datetime]:\n        """"""Get the last modified date of a file.\n\n        Args:\n            uri: the URI of the file\n\n        Returns:\n            the last modified date in UTC of a file or None if this FileSystem\n            does not support this operation.\n        """"""\n        stats = gdal.VSIStatL(vsipath)\n        return datetime.fromtimestamp(stats.mtime) if stats else None\n\n    @staticmethod\n    def list_paths(vsipath: str, ext: Optional[str] = None) -> List[str]:\n        """"""List paths rooted at URI.\n\n        Optionally only includes paths with a certain file extension.\n\n        Args:\n            uri: the URI of a directory\n            ext: the optional file extension to filter by\n        """"""\n        items = gdal.ReadDir(vsipath)\n        ext = ext if ext else \'\'\n        return [os.path.join(vsipath, item)  # This may not work for windows paths\n                for item\n                in filter(lambda x: x.endswith(ext), items)]\n'"
rastervision2/pipeline/__init__.py,0,"b""# flake8: noqa\nimport logging\nimport json\n\nfrom rastervision2.pipeline.rv_config import RVConfig\nfrom rastervision2.pipeline.registry import Registry\nfrom rastervision2.pipeline.verbosity import Verbosity\n\nroot_logger = logging.getLogger('rastervision2')\nsh = logging.StreamHandler()\nsh.setLevel(logging.DEBUG)\nformatter = logging.Formatter(\n    '%(asctime)s:%(name)s: %(levelname)s - %(message)s', '%Y-%m-%d %H:%M:%S')\nsh.setFormatter(formatter)\nroot_logger.addHandler(sh)\n\nrv_config = RVConfig()\nregistry = Registry()\nregistry.load_plugins()\nregistry.load_builtins()\n"""
rastervision2/pipeline/cli.py,0,"b'import sys\nimport os\nimport logging\nimport importlib\nimport importlib.util\nfrom typing import List, Dict, Optional, Tuple\n\nimport click\n\nfrom rastervision2.pipeline import (registry, rv_config)\nfrom rastervision2.pipeline.file_system import (file_to_json, str_to_file)\nfrom rastervision2.pipeline.config import build_config\nfrom rastervision2.pipeline.pipeline_config import PipelineConfig\n\nlog = logging.getLogger(__name__)\n\n\ndef print_error(msg):\n    """"""Print error message to console in red.""""""\n    click.echo(click.style(msg, fg=\'red\'), err=True)\n\n\ndef convert_bool_args(args: dict) -> dict:\n    """"""Convert boolean CLI arguments from string to bool.\n\n    Args:\n        args: a mapping from CLI argument names to values\n\n    Returns:\n        copy of args with boolean string values convert to bool\n    """"""\n    new_args = {}\n    for k, v in args.items():\n        if v.lower() == \'true\':\n            v = True\n        elif v.lower() == \'false\':\n            v = False\n        new_args[k] = v\n    return new_args\n\n\ndef get_configs(cfg_module_path: str, runner: str, args: Dict[str, any]\n                ) -> List[\'rastervision2.pipeline.PipelineConfig\']:  # noqa\n    """"""Get PipelineConfigs from a module.\n\n    Calls a get_config(s) function with some arguments from the CLI\n    to get a list of PipelineConfigs.\n\n    Args:\n        cfg_module_path: the module with `get_configs` function that returns\n            PipelineConfigs. This can either be a Python module path or a local path to\n            a .py file.\n        runner: name of the runner\n        args: CLI args to pass to the get_config(s) function that comes from\n            the --args option\n    """"""\n    if cfg_module_path.endswith(\'.py\'):\n        # From https://stackoverflow.com/questions/67631/how-to-import-a-module-given-the-full-path  # noqa\n        spec = importlib.util.spec_from_file_location(\'rastervision2.pipeline\',\n                                                      cfg_module_path)\n        cfg_module = importlib.util.module_from_spec(spec)\n        spec.loader.exec_module(cfg_module)\n    else:\n        cfg_module = importlib.import_module(cfg_module_path)\n\n    _get_config = getattr(cfg_module, \'get_config\', None)\n    _get_configs = _get_config\n    if _get_config is None:\n        _get_configs = getattr(cfg_module, \'get_configs\', None)\n    if _get_configs is None:\n        raise Exception(\n            \'There must be a get_config or get_configs function in {}.\'.format(\n                cfg_module_path))\n    cfgs = _get_configs(runner, **args)\n    if not isinstance(cfgs, list):\n        cfgs = [cfgs]\n\n    for cfg in cfgs:\n        if not issubclass(type(cfg), PipelineConfig):\n            raise Exception(\n                (\'All objects returned by get_configs in {} must be \'\n                 \'PipelineConfigs.\').format(cfg_module_path))\n    return cfgs\n\n\n@click.group()\n@click.pass_context\n@click.option(\n    \'--profile\', \'-p\', help=\'Sets the configuration profile name to use.\')\n@click.option(\n    \'-v\', \'--verbose\', help=\'Increment the verbosity level.\', count=True)\n@click.option(\'--tmpdir\', help=\'Root of temporary directories to use.\')\ndef main(ctx: click.Context, profile: Optional[str], verbose: int,\n         tmpdir: str):\n    """"""The main click command.\n\n    Sets the profile, verbosity, and tmp_dir in RVConfig.\n    """"""\n    # Make sure current directory is on PYTHON_PATH\n    # so that we can run against modules in current dir.\n    sys.path.append(os.curdir)\n    rv_config.set_verbosity(verbosity=verbose + 1)\n    rv_config.set_tmp_dir_root(tmp_dir_root=tmpdir)\n    rv_config.set_everett_config(profile=profile)\n\n\ndef _run_pipeline(cfg, runner, tmp_dir, splits=1, commands=None):\n    cfg.update()\n    cfg.rv_config = rv_config.get_config_dict(registry.rv_config_schema)\n    cfg.recursive_validate_config()\n    # This is to run the validation again to check any fields that may have changed\n    # after the Config was constructed, possibly by the update method.\n    build_config(cfg.dict())\n\n    cfg_json = cfg.json()\n    cfg_json_uri = cfg.get_config_uri()\n    str_to_file(cfg_json, cfg_json_uri)\n\n    pipeline = cfg.build(tmp_dir)\n    if not commands:\n        commands = pipeline.commands\n\n    runner.run(cfg_json_uri, pipeline, commands, num_splits=splits)\n\n\n@main.command(\'run\', short_help=\'Run sequence of commands within pipeline(s).\')\n@click.argument(\'runner\')\n@click.argument(\'cfg_module\')\n@click.argument(\'commands\', nargs=-1)\n@click.option(\n    \'--arg\',\n    \'-a\',\n    type=(str, str),\n    multiple=True,\n    metavar=\'KEY VALUE\',\n    help=\'Arguments to pass to get_config function\')\n@click.option(\n    \'--splits\',\n    \'-s\',\n    default=1,\n    help=\'Number of splits to run in parallel for splittable commands\')\ndef run(runner: str, cfg_module: str, commands: List[str],\n        arg: List[Tuple[str, str]], splits: int):\n    """"""Run COMMANDS within pipelines in CFG_MODULE using RUNNER.\n\n    RUNNER: name of the Runner to use\n\n    CFG_MODULE: the module with `get_configs` function that returns PipelineConfigs.\n    This can either be a Python module path or a local path to a .py file.\n\n    COMMANDS: space separated sequence of commands to run within pipeline. The order in\n    which to run them is based on the Pipeline.commands attribute. If this is omitted,\n    all commands will be run.\n    """"""\n    tmp_dir_obj = rv_config.get_tmp_dir()\n    tmp_dir = tmp_dir_obj.name\n\n    args = dict(arg)\n    args = convert_bool_args(args)\n    cfgs = get_configs(cfg_module, runner, args)\n    runner = registry.get_runner(runner)()\n\n    for cfg in cfgs:\n        _run_pipeline(cfg, runner, tmp_dir, splits, commands)\n\n\ndef _run_command(cfg_json_uri: str,\n                 command: str,\n                 split_ind: Optional[int] = None,\n                 num_splits: Optional[int] = None,\n                 runner: Optional[str] = None):\n    """"""Run a single command using a serialized PipelineConfig.\n\n    Args:\n        cfg_json_uri: URI of a JSON file with a serialized PipelineConfig\n        command: name of command to run\n        split_ind: the index that a split command should assume\n        num_splits: the total number of splits to use\n        runner: the name of the runner to use\n    """"""\n    pipeline_cfg_dict = file_to_json(cfg_json_uri)\n    rv_config_dict = pipeline_cfg_dict.get(\'rv_config\')\n    rv_config.set_everett_config(\n        profile=rv_config.profile, config_overrides=rv_config_dict)\n\n    tmp_dir_obj = rv_config.get_tmp_dir()\n    tmp_dir = tmp_dir_obj.name\n\n    cfg = build_config(pipeline_cfg_dict)\n    pipeline = cfg.build(tmp_dir)\n\n    if num_splits is not None and split_ind is None and runner is not None:\n        runner = registry.get_runner(runner)()\n        split_ind = runner.get_split_ind()\n\n    command_fn = getattr(pipeline, command)\n\n    if num_splits is not None and num_splits > 1:\n        msg = \'Running {} command split {}/{}...\'.format(\n            command, split_ind + 1, num_splits)\n        click.echo(click.style(msg, fg=\'green\'))\n        command_fn(split_ind=split_ind, num_splits=num_splits)\n    else:\n        msg = \'Running {} command...\'.format(command)\n        click.echo(click.style(msg, fg=\'green\'))\n        command_fn()\n\n\n@main.command(\n    \'run_command\', short_help=\'Run an individual command within a pipeline.\')\n@click.argument(\'cfg_json_uri\')\n@click.argument(\'command\')\n@click.option(\n    \'--split-ind\', type=int, help=\'The process index of a split command\')\n@click.option(\n    \'--num-splits\',\n    type=int,\n    help=\'The number of processes to use for running splittable commands\')\n@click.option(\n    \'--runner\', type=str, help=\'Name of runner to use\', default=\'inprocess\')\ndef run_command(cfg_json_uri: str, command: str, split_ind: Optional[int],\n                num_splits: Optional[int], runner: str):\n    """"""Run a single COMMAND using a serialized PipelineConfig in CFG_JSON_URI.""""""\n    _run_command(\n        cfg_json_uri,\n        command,\n        split_ind=split_ind,\n        num_splits=num_splits,\n        runner=runner)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
rastervision2/pipeline/config.py,0,"b'from abc import ABC, abstractmethod\nfrom typing import List, Union\n\nfrom pydantic import BaseModel, create_model, Field, validator  # noqa\nfrom typing_extensions import Literal\n\nfrom rastervision2.pipeline import registry\n\n\nclass ConfigError(ValueError):\n    """"""Exception raised for invalid configuration.""""""\n    pass\n\n\nclass Config(BaseModel):\n    """"""Base class that can be extended to provide custom configurations.\n\n    This adds some extra methods to Pydantic BaseModel.\n    See https://pydantic-docs.helpmanual.io/\n\n    The general idea is that configuration schemas can be defined by\n    subclassing this and adding class attributes with types and\n    default values for each field. Configs can be defined hierarchically,\n    ie. a Config can have fields which are of type Config.\n    Validation, serialization, deserialization, and IDE support is\n    provided automatically based on this schema.\n    """"""\n\n    # This is here to forbid instantiating Configs with fields that do not\n    # exist in the schema, which helps avoid a command source of bugs.\n    class Config:\n        extra = \'forbid\'\n\n    @classmethod\n    def get_field_summary(cls: type) -> str:\n        """"""Returns class attributes PyDoc summarizing all Config fields.""""""\n        summary = \'Attributes:\\n\'\n        for _, field in cls.__fields__.items():\n            if field.name != \'type_hint\':\n                desc = field.field_info.description or \'\'\n                summary += \'\\t{} ({}): {}\'.format(field.name,\n                                                  field._type_display(), desc)\n                if not field.required:\n                    summary += \'{} Defaults to {}.\'.format(\n                        \'.\' if desc and not desc.endswith(\'.\') else \'\',\n                        repr(field.default))\n                summary += \'\\n\'\n        return summary\n\n    def update(self):\n        """"""Update any fields before validation.\n\n        Subclasses should override this to provide complex default behavior, for\n        example, setting default values as a function of the values of other\n        fields. The arguments to this method will vary depending on the type of Config.\n        """"""\n        pass\n\n    def build(self):\n        """"""Build an instance of the corresponding type of object using this config.\n\n        For example, BackendConfig will build a Backend object. The arguments to this\n        method will vary depending on the type of Config.\n        """"""\n        pass\n\n    def validate_config(self):\n        """"""Validate fields that should be checked after update is called.\n\n        This is to complement the builtin validation that Pydantic performs at the time\n        of object construction.\n        """"""\n        pass\n\n    def recursive_validate_config(self):\n        """"""Recursively validate hierarchies of Configs.\n\n        This uses reflection to call validate_config on a hierarchy of Configs\n        using a depth-first pre-order traversal.\n        """"""\n        class_hierarchy = type(self).mro()\n        for klass in class_hierarchy:\n            if issubclass(klass, Config):\n                klass.validate_config(self)\n\n        child_configs = [\n            x for x in self.__dict__.values() if isinstance(x, Config)\n        ]\n        for c in child_configs:\n            c.recursive_validate_config()\n\n    def validate_list(self, field: str, valid_options: List[str]):\n        """"""Validate a list field.\n\n        Args:\n            field: name of field to validate\n            valid_options: values that field is allowed to take\n\n        Raises:\n            ConfigError if field is invalid\n        """"""\n        val = getattr(self, field)\n        if isinstance(val, list):\n            for v in val:\n                if v not in valid_options:\n                    raise ConfigError(\'{} is not a valid option for {}\'.format(\n                        v, field))\n        else:\n            if val not in valid_options:\n                raise ConfigError(\'{} is not a valid option for {}\'.format(\n                    val, field))\n\n\ndef build_config(x: Union[dict, List[Union[dict, Config]], Config]\n                 ) -> Union[Config, List[Config]]:  # noqa\n    """"""Build a Config from various types of input.\n\n    This is useful for deserializing from JSON. It implements polymorphic\n    deserialization by using the `type_hint` in each dict to get the\n    corresponding Config class from the registry.\n\n    Args:\n        x: some representation of Config(s)\n\n    Returns:\n        the corresponding Config(s)\n    """"""\n    if isinstance(x, dict):\n        new_x = {}\n        for k, v in x.items():\n            new_x[k] = build_config(v)\n        type_hint = new_x.get(\'type_hint\')\n        if type_hint is not None:\n            # The following try/except logic has the following\n            # motivation.  If one has an custom raster-vision pipeline\n            # but wants to be able to run internal raster-vision\n            # commands in processes or containers that do not have\n            # that pipeline loaded in (e.g. in a ""stock"" raster-vision\n            # container on AWS) then this code enables that.\n            #\n            # When a pipeline config is being loaded here, the first\n            # attempt is to load it using the normal `type_hint` field\n            # found within it.  If that succeeds, then everything\n            # proceeds.\n            #\n            # If the the attempt fails but the pipeline config\n            # contains a `fallback_type_hint`, then that is used,\n            # instead.\n            #\n            # What that allows one to do is create a custom pipeline,\n            # derived from a stock pipeline, and tell raster-vision to\n            # treat it as a stock pipeline -- for purposes of the\n            # present function -- if the custom pipeline has not been\n            # registered.\n            #\n            # Please see https://github.com/azavea/raster-vision/pull/914\n            try:\n                # Try to use the given type hint\n                config_cls = registry.get_config(type_hint)\n            except Exception as e:\n                # ... if that fails, try to downgrade to fallback type\n                try:\n                    type_hint = new_x.get(\'fallback_type_hint\')\n                    config_cls = registry.get_config(type_hint)\n                    new_x[\'type_hint\'] = type_hint\n                    permitted_keys = config_cls().__dict__.keys()\n                    current_keys = set(new_x.keys())\n                    for k in current_keys:\n                        if k not in permitted_keys:\n                            del new_x[k]\n                # ... if that fails, throw the original exception\n                except Exception:\n                    raise e\n            new_x = config_cls(**new_x)\n        return new_x\n    elif isinstance(x, list):\n        return [build_config(v) for v in x]\n    else:\n        return x\n\n\ndef upgrade_config(x: Union[dict, List[dict]]) -> Union[dict, List[dict]]:\n    """"""Upgrade serialized Config(s) to the latest version.\n\n    Used to implement backward compatibility of Configs using upgraders stored\n    in the registry.\n\n    Args:\n        x: serialized Config(s) which are potentially of a\n            non-current version\n\n    Returns:\n        the corresponding serialized Config(s) that have been upgraded to the\n            current version\n    """"""\n    if isinstance(x, dict):\n        new_x = {}\n        for k, v in x.items():\n            new_x[k] = upgrade_config(v)\n        type_hint = new_x.get(\'type_hint\')\n        if type_hint is not None:\n            version = new_x.get(\'version\')\n            if version is not None:\n                curr_version, upgraders = registry.get_config_upgraders(\n                    type_hint)\n                for upgrader in upgraders[version:]:\n                    new_x = upgrader.upgrade(new_x)\n                new_x[\'version\'] = curr_version\n        return new_x\n    elif isinstance(x, list):\n        return [upgrade_config(v) for v in x]\n    else:\n        return x\n\n\nclass Upgrader(ABC):\n    """"""Upgrades a config from one version to the next.""""""\n\n    @abstractmethod\n    def upgrade(self, config: dict) -> dict:\n        """"""Returns upgraded version of config.\n\n        Args:\n            config: dict representation of a serialized Config\n        """"""\n        pass\n\n\ndef register_config(type_hint: str, version: int = 0, upgraders=None):\n    """"""Class decorator used to register Config classes with registry.\n\n    All Configs must be registered! Registering a Config does the following:\n    1) associates Config classes with type_hints, which is\n    necessary for polymorphic deserialization. See build_config() for more\n    details.\n    2) associates Configs with the current version number and a\n    list of Upgraders which can be applied to upgrade a Config to the current\n    version. This is useful for backward compatibility.\n    3) adds a constant `type_hint` field to the Config which is set to\n    type_hint\n    4) generates PyDocs based on Pydantic fields\n\n    Args:\n        type_hint: a type hint used to deserialize Configs. Needs to be unique\n            across all registered Configs.\n        version: the current version number of the Config.\n        upgraders: a list of upgraders, one for each version.\n    """"""\n\n    def _register_config(cls):\n        if version > 0:\n            new_cls = create_model(\n                cls.__name__,\n                version=(Literal[version], version),\n                type_hint=(Literal[type_hint], type_hint),\n                __base__=cls)\n            if upgraders is None or len(upgraders) != version:\n                raise ValueError(\n                    \'If version > 0, must supply list of upgraders with length\'\n                    \' equal to version.\')\n        else:\n            new_cls = create_model(\n                cls.__name__,\n                type_hint=(Literal[type_hint], type_hint),\n                __base__=cls)\n        registry.add_config(\n            type_hint, new_cls, version=version, upgraders=upgraders)\n\n        new_cls.__doc__ = (cls.__doc__\n                           or \'\') + \'\\n\\n\' + cls.get_field_summary()\n        return new_cls\n\n    return _register_config\n'"
rastervision2/pipeline/pipeline.py,0,"b'import logging\nfrom typing import List, TYPE_CHECKING\n\nlog = logging.getLogger(__name__)\n\nif TYPE_CHECKING:\n    from rastervision2.pipeline.pipeline_config import PipelineConfig  # noqa\n\n\nclass Pipeline():\n    """"""A pipeline of commands to run sequentially.\n\n    This is an abstraction over a sequence of commands. Each command is\n    represented by a method. This base class has two test commands, and\n    new pipelines should be created by subclassing this.\n\n    Note that any split command methods should have the following signature:\n    def my_command(self, split_ind: int = 0, num_splits: int = 1)\n    The num_splits represents how many parallel jobs should be created, and\n    the split_ind is the index of the current job within that set.\n\n    Attributes:\n        commands: command names listed in the order in which they should run\n        split_commands: names of commands that can be split and run in parallel\n        gpu_commands: names of commands that should be executed on GPUs if\n            available\n    """"""\n    commands: List[str] = [\'test_cpu\', \'test_gpu\']\n    split_commands: List[str] = [\'test_cpu\']\n    gpu_commands: List[str] = [\'test_gpu\']\n\n    def __init__(self, config: \'PipelineConfig\', tmp_dir: str):\n        """"""Constructor\n\n        Args:\n            config: the configuration of this pipeline\n            tmp_dir: the root any temporary directories created by running this\n                pipeline\n        """"""\n        self.config = config\n        self.tmp_dir = tmp_dir\n\n    def test_cpu(self, split_ind: int = 0, num_splits: int = 1):\n        """"""A command to test the ability to run split jobs on CPU.""""""\n        log.info(\'test_cpu split: {}/{}\'.format(split_ind, num_splits))\n        log.info(self.config)\n\n    def test_gpu(self):\n        """"""A command to test the ability to run on GPU.""""""\n        log.info(self.config)\n'"
rastervision2/pipeline/pipeline_config.py,0,"b'from os.path import join\nfrom typing import TYPE_CHECKING\n\nfrom rastervision2.pipeline.config import Config, Field\nfrom rastervision2.pipeline.config import register_config\n\nif TYPE_CHECKING:\n    from rastervision2.pipeline.pipeline import Pipeline  # noqa\n\n\n@register_config(\'pipeline\')\nclass PipelineConfig(Config):\n    """"""Base class for configuring Pipelines.\n\n    This should be subclassed to configure new Pipelines.\n    """"""\n    root_uri: str = Field(\n        None, description=\'the root URI for output generated by the pipeline\')\n    rv_config: dict = Field(\n        None,\n        description=\'used to store serialized RVConfig so pipeline can \'\n        \'run in remote environment with the local RVConfig. This should \'\n        \'not be set explicitly by users -- it is only used by the runner \'\n        \'when running a remote pipeline.\')\n\n    def get_config_uri(self) -> str:\n        """"""Get URI of serialized version of this PipelineConfig.""""""\n        return join(self.root_uri, \'pipeline-config.json\')\n\n    def build(self, tmp_dir: str) -> \'Pipeline\':\n        """"""Return a pipeline based on this configuration.\n\n        Subclasses should override this to return an instance of the\n        corresponding subclass of Pipeline.\n\n        Args:\n            tmp_dir: root of any temporary directory to pass to pipeline\n        """"""\n        from rastervision2.pipeline.pipeline import Pipeline  # noqa\n        return Pipeline(self, tmp_dir)\n'"
rastervision2/pipeline/registry.py,0,"b'from typing import List, Type, TYPE_CHECKING\n\nif TYPE_CHECKING:\n    from rastervision2.pipeline.runner import Runner  # noqa\n    from rastervision2.pipeline.file_system import FileSystem  # noqa\n    from rastervision2.pipeline.config import Upgrader, Config  # noqa\n\n\nclass RegistryError(Exception):\n    """"""Exception raised for invalid use of registry.""""""\n    pass\n\n\nclass Registry():\n    """"""A registry for resources that are built-in or contributed by plugins.""""""\n\n    def __init__(self):\n        self.runners = {}\n        self.file_systems = []\n        self.configs = {}\n        self.config_upgraders = {}\n        self.rv_config_schema = {}\n\n    def add_runner(self, runner_name: str, runner: Type[\'Runner\']):\n        """"""Add a Runner.\n\n        Args:\n            runner_name: the name of the runner that is passed to the CLI\n            runner: the Runner class\n        """"""\n        if runner_name in self.runners:\n            raise RegistryError(\n                \'There is already a {} runner in the registry.\'.format(\n                    runner_name))\n\n        self.runners[runner_name] = runner\n\n    def get_runner(self, runner_name: str) -> Type[\'Runner\']:  # noqa\n        """"""Return a Runner class based on its name.""""""\n        runner = self.runners.get(runner_name)\n        if runner:\n            return runner\n        else:\n            raise RegistryError(\n                \'{} is not a registered runner.\'.format(runner_name))\n\n    def add_file_system(self, file_system: \'FileSystem\'):\n        """"""Add a FileSystem.\n\n        Args:\n            file_system: the FileSystem class to add\n        """"""\n        self.file_systems.append(file_system)\n\n    def get_file_system(self, uri: str,\n                        mode: str = \'r\') -> Type[\'FileSystem\']:  # noqa\n        """"""Get a FileSystem used to handle the file type of a URI.\n\n        Args:\n            uri: a URI to be opened by a registered FileSystem\n            mode: mode for opening file (eg. r or w)\n\n        Returns:\n            the first FileSystem class which can handle opening the uri\n        """"""\n        for fs in self.file_systems:\n            if fs.matches_uri(uri, mode):\n                return fs\n        if mode == \'w\':\n            raise RegistryError(\'No matching file_system to handle \'\n                                \'writing to uri {}\'.format(uri))\n        else:\n            raise RegistryError(\'No matching file_system to handle \'\n                                \'reading from uri {}\'.format(uri))\n\n    def add_config(self,\n                   type_hint: str,\n                   config: Type[\'Config\'],\n                   version: int = 0,\n                   upgraders: List[\'Upgrader\'] = None):\n        """"""Add a Config.\n\n        Args:\n            type_hint: the type hint used for deserialization of dict to\n                an instance of config\n            config: Config class\n            version: the current version of the Config\n            upgraders: a sequence of Upgraders that go from version 0 to\n                version\n        """"""\n        if type_hint in self.configs:\n            raise RegistryError(\n                \'There is already a config registered for type_hint {}\'.format(\n                    type_hint))\n\n        self.configs[type_hint] = config\n\n        if type_hint in self.config_upgraders:\n            raise RegistryError(\n                \'There are already config upgraders registered for type_hint {}\'.\n                format(type_hint))\n        self.config_upgraders[type_hint] = (version, upgraders)\n\n    def get_config(self, type_hint: str) -> Type[\'Config\']:\n        """"""Get a Config class associated with a type_hint.""""""\n        config = self.configs.get(type_hint)\n        if config:\n            return config\n        else:\n            raise RegistryError((\n                \'{} is not a registered config type hint.\'\n                \'This may be because you forgot to use the register_config decorator, \'\n                \'or forgot to import the module in the top-level __init__.py file for \'\n                \'the plugin.\').format(type_hint))\n\n    def get_config_upgraders(self, type_hint: str) -> List[\'Upgrader\']:  # noqa\n        """"""Get config upgraders associated with type_hint.""""""\n        out = self.config_upgraders.get(type_hint)\n        if out:\n            return out\n        else:\n            raise RegistryError(\n                \'{} is not a registered config upgrader type hint.\'.format(\n                    type_hint))\n\n    def add_rv_config_schema(self, config_section: str,\n                             config_fields: List[str]):\n        """"""Add section of schema used by RVConfig.\n\n        Args:\n            config_section: name of section\n            config_fields: list of field names within section\n        """"""\n        self.rv_config_schema[config_section] = config_fields\n\n    def get_rv_config_schema(self):\n        """"""Return RVConfig schema.""""""\n        return self.rv_config_schema\n\n    def load_builtins(self):\n        """"""Add all builtin resources.""""""\n        from rastervision2.pipeline.runner import (InProcessRunner, INPROCESS,\n                                                   LocalRunner, LOCAL)\n        from rastervision2.pipeline.file_system import (HttpFileSystem,\n                                                        LocalFileSystem)\n\n        self.add_runner(INPROCESS, InProcessRunner)\n        self.add_runner(LOCAL, LocalRunner)\n        self.add_file_system(HttpFileSystem)\n        self.add_file_system(LocalFileSystem)\n\n        # import so register_config decorators are called\n        # TODO can we get rid of this now?\n        import rastervision2.pipeline.pipeline_config  # noqa\n\n    def load_plugins(self):\n        """"""Discover all plugins and register their resources.\n\n        Import each Python module within the rastervision2 namespace package\n        and call the register_plugin function at its root (if it exists).\n        """"""\n        import importlib\n        import pkgutil\n        import rastervision2\n\n        # From https://packaging.python.org/guides/creating-and-discovering-plugins/#using-namespace-packages  # noqa\n        def iter_namespace(ns_pkg):\n            # Specifying the second argument (prefix) to iter_modules makes the\n            # returned name an absolute name instead of a relative one. This allows\n            # import_module to work without having to do additional modification to\n            # the name.\n            return pkgutil.iter_modules(ns_pkg.__path__, ns_pkg.__name__ + \'.\')\n\n        discovered_plugins = {\n            name: importlib.import_module(name)\n            for finder, name, ispkg in iter_namespace(rastervision2)\n        }\n\n        for name, module in discovered_plugins.items():\n            register_plugin = getattr(module, \'register_plugin\', None)\n            if register_plugin:\n                register_plugin(self)\n'"
rastervision2/pipeline/rv_config.py,0,"b'import os\nfrom tempfile import TemporaryDirectory\nfrom pathlib import Path\nimport logging\nimport json\nfrom typing import Optional, List, Dict\n\nfrom everett import NO_VALUE\nfrom everett.manager import (ConfigManager, ConfigDictEnv, ConfigIniEnv,\n                             ConfigOSEnv)\n\nfrom rastervision2.pipeline.verbosity import Verbosity\n\nlog = logging.getLogger(__name__)\n\n\nclass LocalEnv(object):\n    def get(self, key, namespace=None):\n        if isinstance(namespace, list):\n            if \'aws_batch\' in namespace or \'aws_s3\' in namespace:\n                return \'\'\n        else:\n            return NO_VALUE\n\n\ndef load_conf_list(s):\n    """"""Loads a list of items from the config.\n\n    Lists should be comma separated.\n\n    This takes into account that previous versions of Raster Vision\n    allowed for a `[ ""module"" ]` like syntax, even though that didn\'t\n    work for multi-value lists.\n    """"""\n    try:\n        # A comma separated list of values will be transformed to\n        # having a list-like string, with \' instead of "". Replacing\n        # single quotes with double quotes lets us parse it as a JSON list.\n        return json.loads(s.replace(""\'"", \'""\'))\n    except json.JSONDecodeError:\n        return list(map(lambda x: x.strip(), s.split(\',\')))\n\n\n# TODO change name to SystemConfig so it\'s not tied to RV?\nclass RVConfig:\n    """"""A store of global user-specific configuration not tied to particular pipelines.\n\n    This is used to store user-specific configuration like the root temporary\n    directory, verbosity, and other system-wide configuration handled by Everett\n    (eg. which AWS Batch job queue to use).\n\n    Attributes:\n        DEFAULT_PROFILE: the default RV configuration profile name\n        DEFAULT_TMP_DIR_ROOT: the default location for root of temporary directories\n    """"""\n    DEFAULT_PROFILE: str = \'default\'\n    DEFAULT_TMP_DIR_ROOT: str = \'/opt/data/tmp\'\n\n    def __init__(self):\n        self.set_verbosity()\n        self.set_tmp_dir_root()\n        self.set_everett_config()\n\n    def set_verbosity(self, verbosity: Verbosity = Verbosity.NORMAL):\n        """"""Set verbosity level for logging.""""""\n        self.verbosity = verbosity\n        root_log = logging.getLogger(\'rastervision2\')\n        if self.verbosity >= Verbosity.VERBOSE:\n            root_log.setLevel(logging.DEBUG)\n        elif self.verbosity >= Verbosity.NORMAL:\n            root_log.setLevel(logging.INFO)\n        else:\n            root_log.setLevel(logging.WARN)\n\n    def get_verbosity(self) -> Verbosity:\n        """"""Returns verbosity level for logging.""""""\n        return self.verbosity\n\n    def get_tmp_dir(self) -> TemporaryDirectory:\n        """"""Return a new TemporaryDirectory object.""""""\n        return TemporaryDirectory(dir=self.tmp_dir_root)\n\n    def get_tmp_dir_root(self) -> str:\n        """"""Return the root of all temp dirs.""""""\n        return self.tmp_dir_root\n\n    def set_tmp_dir_root(self, tmp_dir_root: Optional[str] = None):\n        """"""Set root of all temporary directories.\n\n        To set the value, the following rules are used in decreasing priority:\n\n        1) the tmp_dir_root argument if it is not None\n        2) an environment variable (TMPDIR, TEMP, or TMP)\n        3) a default temporary directory which is\n        4) a directory returned by tempfile.TemporaryDirectory()\n        """"""\n        # Check the various possibilities in order of priority.\n        env_arr = [\n            os.environ.get(k) for k in [\'TMPDIR\', \'TEMP\', \'TMP\']\n            if k in os.environ\n        ]\n\n        dir_arr = [tmp_dir_root] + env_arr + [RVConfig.DEFAULT_TMP_DIR_ROOT]\n        dir_arr = [d for d in dir_arr if d is not None]\n        tmp_dir_root = dir_arr[0]\n\n        try:\n            # Try to create directory\n            if not os.path.exists(tmp_dir_root):\n                os.makedirs(tmp_dir_root, exist_ok=True)\n            # Check that it is actually a directory\n            if not os.path.isdir(tmp_dir_root):\n                raise Exception(\'{} is not a directory.\'.format(tmp_dir_root))\n            # Can we interact with directory?\n            Path.touch(Path(os.path.join(tmp_dir_root, \'.can_touch\')))\n            # All checks have passed by this point\n            self.tmp_dir_root = tmp_dir_root\n\n        # If directory cannot be made and/or cannot be interacted\n        # with, fall back to default system location.\n        except Exception as e:\n            system_tmp_dir = TemporaryDirectory().name\n            log.warning(\n                \'Root temporary directory cannot be used: {}. Using root: {}\'.\n                format(tmp_dir_root, system_tmp_dir))\n            self.tmp_dir_root = system_tmp_dir\n        finally:\n            os.makedirs(self.tmp_dir_root, exist_ok=True)\n            log.debug(\'Temporary directory root is: {}\'.format(\n                self.tmp_dir_root))\n\n    def set_everett_config(self,\n                           profile: str = None,\n                           rv_home: str = None,\n                           config_overrides: Dict[str, str] = None):\n        """"""Set Everett config.\n\n        This sets up any other configuration using the Everett library.\n        See https://everett.readthedocs.io/\n\n        It roughly mimics the behavior of how the AWS CLI is configured, if that\n        is a helpful analogy. Configuration can be specified through configuration\n        files, environment variables, and the config_overrides argument in increasing\n        order of precedence.\n\n        Configuration files are in the following format:\n        ```\n        [namespace_1]\n        key_11=val_11\n        ...\n        key_1n=val_1n\n\n        ...\n\n        [namespace_m]\n        key_m1=val_m1\n        ...\n        key_mn=val_mn\n        ```\n\n        Each namespace can be used for the configuration of a different plugin.\n        Each configuration file is a ""profile"" with the name of the file being the name\n        of the profile. This supports switching between different configuration sets.\n        The corresponding environment variable setting for namespace_i and key_ij is\n        `namespace_i_key_ij=val_ij`.\n\n        Args:\n            profile: name of the RV configuration profile to use. If not set, defaults\n                to value of RV_PROFILE env var, or DEFAULT_PROFILE.\n            rv_home: a local dir with RV configuration files. If not set, attempts to\n                use ~/.rastervision.\n            config_overrides: any configuration to override. Each key is of form\n                namespace_i_key_ij with corresponding value val_ij.\n        """"""\n        if profile is None:\n            if os.environ.get(\'RV_PROFILE\'):\n                profile = os.environ.get(\'RV_PROFILE\')\n            else:\n                profile = RVConfig.DEFAULT_PROFILE\n        self.profile = profile\n\n        if config_overrides is None:\n            config_overrides = {}\n\n        if rv_home is None:\n            home = os.path.expanduser(\'~\')\n            rv_home = os.path.join(home, \'.rastervision\')\n        self.rv_home = rv_home\n\n        if self.profile == \'local\':\n            config_ini_env = LocalEnv()\n        else:\n            config_file_locations = self._discover_config_file_locations(\n                self.profile)\n            config_ini_env = ConfigIniEnv(config_file_locations)\n\n        self.config = ConfigManager(\n            [\n                ConfigDictEnv(config_overrides),\n                ConfigOSEnv(),\n                config_ini_env,\n            ],\n            doc=(\n                \'Check https://docs.rastervision.io/ for docs. \'\n                \'Switch to the version being run and search for Raster Vision \'\n                \'Configuration.\'))\n\n    def get_namespace_config(self, namespace: str) -> Dict[str, str]:\n        """"""Get the key-val pairs associated with a namespace.""""""\n        return self.config.with_namespace(namespace)\n\n    def get_config_dict(\n            self, rv_config_schema: Dict[str, List[str]]) -> Dict[str, str]:\n        """"""Get all Everett configuration.\n\n        This method is used to serialize an Everett configuration so it can be used on\n        a remote instance.\n\n        Args:\n            rv_config_schema: each key is a namespace; each value is list of keys within\n                that namespace\n\n        Returns:\n            Each key is of form namespace_i_key_ij with corresponding value val_ij.\n        """"""\n        config_dict = {}\n        for namespace, keys in rv_config_schema.items():\n            for key in keys:\n                config_dict[namespace + \'_\' + key] = \\\n                    self.get_namespace_config(namespace)(key)\n        return config_dict\n\n    def _discover_config_file_locations(self, profile) -> List[str]:\n        """"""Discover the location of RV config files.\n\n        Args:\n            profile: the name of the RV profile to use\n\n        Returns:\n            a list of paths to RV config files matching the profile name\n        """"""\n        result = []\n\n        # Allow for user to specify specific config file\n        # in the RV_CONFIG env variable.\n        env_specified_path = os.environ.get(\'RV_CONFIG\')\n        if env_specified_path:\n            result.append(env_specified_path)\n\n        # Allow user to specify config directory that will\n        # contain profile configs in RV_CONFIG_DIR\n        # env variable. Otherwise, use ""$HOME/.rastervision""\n        env_specified_dir_path = os.environ.get(\'RV_CONFIG_DIR\')\n        if env_specified_dir_path:\n            result.append(os.path.join(env_specified_dir_path, profile))\n        else:\n            result.append(os.path.join(self.rv_home, profile))\n        result.append(os.path.join(os.getcwd(), \'.rastervision\'))\n\n        # Filter out any that do not exist.\n        results_that_exist = list(filter(lambda x: os.path.exists(x), result))\n\n        # If the profile is not default, and there is no config that exists,\n        # then throw an error.\n        if not any(results_that_exist) and profile != RVConfig.DEFAULT_PROFILE:\n            raise Exception(\'Configuration Profile {} not found. \'\n                            \'Checked: {}\'.format(profile, \', \'.join(result)))\n\n        return results_that_exist\n'"
rastervision2/pipeline/utils.py,0,"b'import atexit\nimport logging\nfrom math import ceil\n\nlog = logging.getLogger(__name__)\n\n\ndef terminate_at_exit(process):\n    def terminate():\n        log.debug(\'Terminating {}...\'.format(process.pid))\n        process.terminate()\n\n    atexit.register(terminate)\n\n\ndef grouped(lst, size):\n    """"""Returns a list of lists of length \'size\'.\n    The last list will have size <= \'size\'.\n    """"""\n    return [lst[n:n + size] for n in range(0, len(lst), size)]\n\n\ndef split_into_groups(lst, num_groups):\n    """"""Attempts to split a list into a given number of groups.\n    The number of groups will be at least 1 and at most\n    num_groups.\n\n    Args:\n       lst:             The list to split\n       num_groups:      The number of groups to create.\n    Returns:\n       A list of size between 1 and num_groups containing lists\n       of items of l.""""""\n    group_sz = max(int(ceil((len(lst)) / num_groups)), 1)\n\n    return grouped(lst, group_sz)\n'"
rastervision2/pipeline/verbosity.py,0,"b'class Verbosity:\n    """"""Verbosity level for the sake of logging.""""""\n    QUIET = 0\n    NORMAL = 1\n    VERBOSE = 2\n    VERY_VERBOSE = 3\n    DEBUG = 4\n\n    @staticmethod\n    def get() -> \'Verbosity\':\n        """"""Get the verbosity from RVConfig.""""""\n        from rastervision2.pipeline import rv_config\n        return rv_config.get_verbosity()\n'"
rastervision2/pytorch_backend/__init__.py,0,b'# flake8: noqa\n\nimport rastervision2.pipeline\nfrom rastervision2.pytorch_backend.pytorch_chip_classification_config import *\nfrom rastervision2.pytorch_backend.pytorch_chip_classification import *\nfrom rastervision2.pytorch_backend.pytorch_semantic_segmentation_config import *\nfrom rastervision2.pytorch_backend.pytorch_semantic_segmentation import *\nfrom rastervision2.pytorch_backend.pytorch_object_detection_config import *\nfrom rastervision2.pytorch_backend.pytorch_object_detection import *\n\n\ndef register_plugin(registry):\n    pass\n'
rastervision2/pytorch_backend/pytorch_chip_classification.py,0,"b'from os.path import join\nimport uuid\n\nfrom rastervision2.pipeline.file_system import (make_dir)\nfrom rastervision2.core.data.label import ChipClassificationLabels\nfrom rastervision2.core.utils.misc import save_img\nfrom rastervision2.core.data_sample import DataSample\nfrom rastervision2.pytorch_backend.pytorch_learner_backend import (\n    PyTorchLearnerSampleWriter, PyTorchLearnerBackend)\n\n\nclass PyTorchChipClassificationSampleWriter(PyTorchLearnerSampleWriter):\n    def write_sample(self, sample: DataSample):\n        """"""\n        This writes a training or validation sample to\n        (train|valid)/{class_name}/{scene_id}-{ind}.png\n        """"""\n        class_id = sample.labels.get_cell_class_id(sample.window)\n        # If a chip is not associated with a class, don\'t\n        # use it in training data.\n        if class_id is None:\n            return\n\n        split_name = \'train\' if sample.is_train else \'valid\'\n        class_name = self.class_config.names[class_id]\n        class_dir = join(self.sample_dir, split_name, class_name)\n        make_dir(class_dir)\n        chip_path = join(class_dir, \'{}-{}.png\'.format(sample.scene_id,\n                                                       self.sample_ind))\n        save_img(sample.chip, chip_path)\n        self.sample_ind += 1\n\n\nclass PyTorchChipClassification(PyTorchLearnerBackend):\n    def get_sample_writer(self):\n        output_uri = join(self.pipeline_cfg.chip_uri, \'{}.zip\'.format(\n            str(uuid.uuid4())))\n        return PyTorchChipClassificationSampleWriter(\n            output_uri, self.pipeline_cfg.dataset.class_config, self.tmp_dir)\n\n    def predict(self, chips, windows):\n        if self.learner is None:\n            self.load_model()\n\n        out = self.learner.numpy_predict(chips, raw_out=True)\n        labels = ChipClassificationLabels()\n\n        for class_probs, window in zip(out, windows):\n            class_id = class_probs.argmax()\n            labels.set_cell(window, class_id, class_probs)\n\n        return labels\n'"
rastervision2/pytorch_backend/pytorch_chip_classification_config.py,0,"b""from rastervision2.pipeline.config import register_config\nfrom rastervision2.pytorch_backend.pytorch_learner_backend_config import (\n    PyTorchLearnerBackendConfig)\nfrom rastervision2.pytorch_learner.classification_learner_config import (\n    ClassificationModelConfig, ClassificationLearnerConfig,\n    ClassificationDataConfig)\n\nfrom rastervision2.pytorch_backend.pytorch_chip_classification import (\n    PyTorchChipClassification)\n\n\n@register_config('pytorch_chip_classification_backend')\nclass PyTorchChipClassificationConfig(PyTorchLearnerBackendConfig):\n    model: ClassificationModelConfig\n\n    def get_learner_config(self, pipeline):\n        data = ClassificationDataConfig()\n        data.uri = pipeline.chip_uri\n        data.class_names = pipeline.dataset.class_config.names\n        data.class_colors = pipeline.dataset.class_config.colors\n        data.img_sz = pipeline.train_chip_sz\n        data.augmentors = self.augmentors\n\n        learner = ClassificationLearnerConfig(\n            data=data,\n            model=self.model,\n            solver=self.solver,\n            test_mode=self.test_mode,\n            output_uri=pipeline.train_uri,\n            log_tensorboard=self.log_tensorboard,\n            run_tensorboard=self.run_tensorboard)\n        learner.update()\n        return learner\n\n    def build(self, pipeline, tmp_dir):\n        learner = self.get_learner_config(pipeline)\n        return PyTorchChipClassification(pipeline, learner, tmp_dir)\n"""
rastervision2/pytorch_backend/pytorch_learner_backend.py,0,"b'from os.path import join\nimport tempfile\n\nfrom rastervision2.pipeline.file_system import (make_dir, upload_or_copy,\n                                                zipdir)\nfrom rastervision2.core.backend import Backend, SampleWriter\nfrom rastervision2.core.data_sample import DataSample\nfrom rastervision2.core.data import ClassConfig\nfrom rastervision2.core.rv_pipeline import RVPipelineConfig\nfrom rastervision2.pytorch_learner.learner_config import LearnerConfig\nfrom rastervision2.pytorch_learner.learner import Learner\n\n\nclass PyTorchLearnerSampleWriter(SampleWriter):\n    def __init__(self, output_uri: str, class_config: ClassConfig,\n                 tmp_dir: str):\n        """"""Constructor.\n\n        Args:\n            output_uri: URI of directory where zip file of chips should be placed\n            class_config: used to convert class ids to names which may be needed for some\n                training data formats\n            tmp_dir: local directory which is root of any temporary directories that\n                are created\n        """"""\n        self.output_uri = output_uri\n        self.class_config = class_config\n        self.tmp_dir = tmp_dir\n\n    def __enter__(self):\n        self.tmp_dir_obj = tempfile.TemporaryDirectory(dir=self.tmp_dir)\n        self.sample_dir = join(self.tmp_dir_obj.name, \'samples\')\n        make_dir(self.sample_dir)\n        self.sample_ind = 0\n\n        return self\n\n    def __exit__(self, type, value, traceback):\n        """"""\n        This writes a zip file for a group of scenes at {output_uri}/{uuid}.zip.\n\n        This method is called once per instance of the chip command.\n        A number of instances of the chip command can run simultaneously to\n        process chips in parallel. The uuid in the zip path above is what allows\n        separate instances to avoid overwriting each others\' output.\n        """"""\n        output_path = join(self.tmp_dir_obj.name, \'output.zip\')\n        zipdir(self.sample_dir, output_path)\n        upload_or_copy(output_path, self.output_uri)\n        self.tmp_dir_obj.cleanup()\n\n    def write_sample(self, sample: DataSample):\n        """"""Write a single sample to disk.""""""\n        raise NotImplementedError()\n\n\nclass PyTorchLearnerBackend(Backend):\n    """"""Backend that uses the rastervision2.pytorch_learner package to train models.""""""\n\n    def __init__(self, pipeline_cfg: RVPipelineConfig,\n                 learner_cfg: LearnerConfig, tmp_dir: str):\n        self.pipeline_cfg = pipeline_cfg\n        self.learner_cfg = learner_cfg\n        self.tmp_dir = tmp_dir\n        self.learner = None\n\n    def train(self):\n        learner = self.learner_cfg.build(self.tmp_dir)\n        learner.main()\n\n    def load_model(self):\n        self.learner = Learner.from_model_bundle(\n            self.learner_cfg.get_model_bundle_uri(), self.tmp_dir)\n\n    def get_sample_writer(self):\n        raise NotImplementedError()\n\n    def predict(self, chips, windows):\n        raise NotImplementedError()\n'"
rastervision2/pytorch_backend/pytorch_learner_backend_config.py,0,"b""from typing import List\n\nfrom rastervision2.pipeline.config import register_config, Field\nfrom rastervision2.core.backend import BackendConfig\nfrom rastervision2.pytorch_learner.learner_config import (\n    SolverConfig, ModelConfig, default_augmentors, augmentors as\n    augmentor_list)\n\n\n@register_config('pytorch_learner_backend')\nclass PyTorchLearnerBackendConfig(BackendConfig):\n    model: ModelConfig\n    solver: SolverConfig\n    log_tensorboard: bool = Field(\n        True, description='If True, log events to Tensorboard log files.')\n    run_tensorboard: bool = Field(\n        False,\n        description='If True, run Tensorboard server pointing at log files.')\n    augmentors: List[str] = Field(\n        default_augmentors,\n        description=(\n            'Names of albumentations augmentors to use for training batches. '\n            'Choices include: ' + str(augmentor_list)))\n    test_mode: bool = Field(\n        False,\n        description=\n        ('This field is passed along to the LearnerConfig which is returned by '\n         'get_learner_config(). For more info, see the docs for'\n         'pytorch_learner.learner_config.LearnerConfig.test_mode.'))\n\n    def get_bundle_filenames(self):\n        return ['model-bundle.zip']\n\n    def get_learner_config(self, pipeline):\n        raise NotImplementedError()\n\n    def build(self, pipeline, tmp_dir):\n        raise NotImplementedError()\n"""
rastervision2/pytorch_backend/pytorch_object_detection.py,0,"b'from os.path import join\nimport uuid\n\nfrom rastervision2.pipeline.file_system import (make_dir, json_to_file)\nfrom rastervision2.core.data.label import ObjectDetectionLabels\nfrom rastervision2.core.utils.misc import save_img\nfrom rastervision2.core.data_sample import DataSample\nfrom rastervision2.pytorch_backend.pytorch_learner_backend import (\n    PyTorchLearnerSampleWriter, PyTorchLearnerBackend)\n\n\nclass PyTorchObjectDetectionSampleWriter(PyTorchLearnerSampleWriter):\n    def __enter__(self):\n        super().__enter__()\n\n        self.splits = {\n            \'train\': {\n                \'images\': [],\n                \'annotations\': []\n            },\n            \'valid\': {\n                \'images\': [],\n                \'annotations\': []\n            }\n        }\n        self.categories = [{\n            \'id\': class_id,\n            \'name\': class_name\n        } for class_id, class_name in enumerate(self.class_config.names)]\n\n        return self\n\n    def __exit__(self, type, value, traceback):\n        for split in [\'train\', \'valid\']:\n            if len(self.splits[split][\'images\']) > 0:\n                split_dir = join(self.sample_dir, split)\n                labels_path = join(split_dir, \'labels.json\')\n\n                images = self.splits[split][\'images\']\n                annotations = self.splits[split][\'annotations\']\n                coco_dict = {\n                    \'images\': images,\n                    \'annotations\': annotations,\n                    \'categories\': self.categories\n                }\n                json_to_file(coco_dict, labels_path)\n\n        super().__exit__(type, value, traceback)\n\n    def write_sample(self, sample: DataSample):\n        """"""\n        This writes a training or validation sample to\n        (train|valid)/img/{scene_id}-{ind}.png and updates\n        some COCO data structures.\n        """"""\n        split = \'train\' if sample.is_train else \'valid\'\n        split_dir = join(self.sample_dir, split)\n        img_dir = join(split_dir, \'img\')\n        make_dir(img_dir)\n        img_fn = \'{}-{}.png\'.format(sample.scene_id, self.sample_ind)\n        img_path = join(img_dir, img_fn)\n        save_img(sample.chip, img_path)\n\n        images = self.splits[split][\'images\']\n        annotations = self.splits[split][\'annotations\']\n\n        images.append({\n            \'file_name\': img_fn,\n            \'id\': self.sample_ind,\n            \'height\': sample.chip.shape[0],\n            \'width\': sample.chip.shape[1]\n        })\n\n        npboxes = sample.labels.get_npboxes()\n        npboxes = ObjectDetectionLabels.global_to_local(npboxes, sample.window)\n        for box_ind, (box, class_id) in enumerate(\n                zip(npboxes, sample.labels.get_class_ids())):\n            bbox = [box[1], box[0], box[3] - box[1], box[2] - box[0]]\n            bbox = [int(i) for i in bbox]\n            annotations.append({\n                \'id\': \'{}-{}\'.format(self.sample_ind, box_ind),\n                \'image_id\': self.sample_ind,\n                \'bbox\': bbox,\n                \'category_id\': int(class_id)\n            })\n\n        self.sample_ind += 1\n\n\nclass PyTorchObjectDetection(PyTorchLearnerBackend):\n    def get_sample_writer(self):\n        output_uri = join(self.pipeline_cfg.chip_uri, \'{}.zip\'.format(\n            str(uuid.uuid4())))\n        return PyTorchObjectDetectionSampleWriter(\n            output_uri, self.pipeline_cfg.dataset.class_config, self.tmp_dir)\n\n    def predict(self, chips, windows):\n        """"""Return predictions for a chip using model.\n\n        Args:\n            chips: [[height, width, channels], ...] numpy array of chips\n            windows: List of boxes that are the windows aligned with the chips.\n\n        Return:\n            Labels object containing predictions\n        """"""\n        if self.learner is None:\n            self.load_model()\n\n        batch_out = self.learner.numpy_predict(chips, raw_out=False)\n        labels = ObjectDetectionLabels.make_empty()\n\n        for chip_ind, out in enumerate(batch_out):\n            window = windows[chip_ind]\n            boxes = out[\'boxes\']\n            class_ids = out[\'class_ids\']\n            scores = out[\'scores\']\n            boxes = ObjectDetectionLabels.local_to_global(boxes, window)\n            labels += ObjectDetectionLabels(boxes, class_ids, scores=scores)\n\n        return labels\n'"
rastervision2/pytorch_backend/pytorch_object_detection_config.py,0,"b""from rastervision2.pipeline.config import register_config\nfrom rastervision2.pytorch_backend.pytorch_learner_backend_config import (\n    PyTorchLearnerBackendConfig)\nfrom rastervision2.pytorch_learner.object_detection_learner_config import (\n    ObjectDetectionModelConfig, ObjectDetectionLearnerConfig,\n    ObjectDetectionDataConfig)\nfrom rastervision2.pytorch_backend.pytorch_object_detection import (\n    PyTorchObjectDetection)\n\n\n@register_config('pytorch_object_detection_backend')\nclass PyTorchObjectDetectionConfig(PyTorchLearnerBackendConfig):\n    model: ObjectDetectionModelConfig\n\n    def get_learner_config(self, pipeline):\n        data = ObjectDetectionDataConfig()\n        data.uri = pipeline.chip_uri\n        data.class_names = pipeline.dataset.class_config.names\n        data.class_colors = pipeline.dataset.class_config.colors\n        data.img_sz = pipeline.train_chip_sz\n        data.augmentors = self.augmentors\n\n        learner = ObjectDetectionLearnerConfig(\n            data=data,\n            model=self.model,\n            solver=self.solver,\n            test_mode=self.test_mode,\n            output_uri=pipeline.train_uri,\n            log_tensorboard=self.log_tensorboard,\n            run_tensorboard=self.run_tensorboard)\n        learner.update()\n        return learner\n\n    def build(self, pipeline, tmp_dir):\n        learner = self.get_learner_config(pipeline)\n        return PyTorchObjectDetection(pipeline, learner, tmp_dir)\n"""
rastervision2/pytorch_backend/pytorch_semantic_segmentation.py,0,"b'from os.path import join\nimport uuid\n\nimport numpy as np\n\nfrom rastervision2.pipeline.file_system import (make_dir)\nfrom rastervision2.core.data.label import SemanticSegmentationLabels\nfrom rastervision2.core.utils.misc import save_img\nfrom rastervision2.core.data_sample import DataSample\nfrom rastervision2.pytorch_backend.pytorch_learner_backend import (\n    PyTorchLearnerSampleWriter, PyTorchLearnerBackend)\n\n\nclass PyTorchSemanticSegmentationSampleWriter(PyTorchLearnerSampleWriter):\n    def write_sample(self, sample: DataSample):\n        """"""\n        This writes a training or validation sample to\n        (train|valid)/img/{scene_id}-{ind}.png and\n        (train|valid)/labels/{scene_id}-{ind}.png\n        """"""\n        split_name = \'train\' if sample.is_train else \'valid\'\n        label_arr = sample.labels.get_label_arr(sample.window).astype(np.uint8)\n\n        img_dir = join(self.sample_dir, split_name, \'img\')\n        labels_dir = join(self.sample_dir, split_name, \'labels\')\n        make_dir(img_dir)\n        make_dir(labels_dir)\n\n        img_path = join(img_dir, \'{}-{}.png\'.format(sample.scene_id,\n                                                    self.sample_ind))\n        labels_path = join(\n            labels_dir, \'{}-{}.png\'.format(sample.scene_id, self.sample_ind))\n        save_img(sample.chip, img_path)\n        save_img(label_arr, labels_path)\n\n        self.sample_ind += 1\n\n\nclass PyTorchSemanticSegmentation(PyTorchLearnerBackend):\n    def get_sample_writer(self):\n        output_uri = join(self.pipeline_cfg.chip_uri, \'{}.zip\'.format(\n            str(uuid.uuid4())))\n        return PyTorchSemanticSegmentationSampleWriter(\n            output_uri, self.pipeline_cfg.dataset.class_config, self.tmp_dir)\n\n    def predict(self, chips, windows):\n        if self.learner is None:\n            self.load_model()\n\n        batch_out = self.learner.numpy_predict(chips, raw_out=False)\n        labels = SemanticSegmentationLabels()\n        for out, window in zip(batch_out, windows):\n            labels.set_label_arr(window, out)\n\n        return labels\n'"
rastervision2/pytorch_backend/pytorch_semantic_segmentation_config.py,0,"b""from rastervision2.pipeline.config import register_config\nfrom rastervision2.pytorch_backend.pytorch_learner_backend_config import (\n    PyTorchLearnerBackendConfig)\nfrom rastervision2.pytorch_learner.semantic_segmentation_learner_config import (\n    SemanticSegmentationModelConfig, SemanticSegmentationLearnerConfig,\n    SemanticSegmentationDataConfig)\nfrom rastervision2.pytorch_backend.pytorch_semantic_segmentation import (\n    PyTorchSemanticSegmentation)\n\n\n@register_config('pytorch_semantic_segmentation_backend')\nclass PyTorchSemanticSegmentationConfig(PyTorchLearnerBackendConfig):\n    model: SemanticSegmentationModelConfig\n\n    def get_learner_config(self, pipeline):\n        data = SemanticSegmentationDataConfig()\n        data.uri = pipeline.chip_uri\n        data.class_names = pipeline.dataset.class_config.names\n        data.class_colors = pipeline.dataset.class_config.colors\n        data.img_sz = pipeline.train_chip_sz\n        data.augmentors = self.augmentors\n\n        learner = SemanticSegmentationLearnerConfig(\n            data=data,\n            model=self.model,\n            solver=self.solver,\n            test_mode=self.test_mode,\n            output_uri=pipeline.train_uri,\n            log_tensorboard=self.log_tensorboard,\n            run_tensorboard=self.run_tensorboard)\n        learner.update()\n        return learner\n\n    def build(self, pipeline, tmp_dir):\n        learner = self.get_learner_config(pipeline)\n        return PyTorchSemanticSegmentation(pipeline, learner, tmp_dir)\n"""
rastervision2/pytorch_learner/__init__.py,0,b'# flake8: noqa\n\nimport rastervision2.pipeline\nfrom rastervision2.pytorch_learner.learner_config import *\nfrom rastervision2.pytorch_learner.learner import *\nfrom rastervision2.pytorch_learner.classification_learner_config import *\nfrom rastervision2.pytorch_learner.classification_learner import *\nfrom rastervision2.pytorch_learner.regression_learner_config import *\nfrom rastervision2.pytorch_learner.regression_learner import *\nfrom rastervision2.pytorch_learner.semantic_segmentation_learner_config import *\nfrom rastervision2.pytorch_learner.semantic_segmentation_learner import *\nfrom rastervision2.pytorch_learner.object_detection_learner_config import *\nfrom rastervision2.pytorch_learner.object_detection_learner import *\n\n\ndef register_plugin(registry):\n    pass\n'
rastervision2/pytorch_learner/classification_learner.py,5,"b""import warnings\nwarnings.filterwarnings('ignore')  # noqa\nfrom os.path import join, isdir\nimport logging\n\nimport torch\nfrom torchvision import models\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import ConcatDataset\n\nfrom rastervision2.pytorch_learner.learner import Learner\nfrom rastervision2.pytorch_learner.utils import (\n    compute_conf_mat_metrics, compute_conf_mat, AlbumentationsDataset)\nfrom rastervision2.pytorch_learner.image_folder import (ImageFolder)\nfrom rastervision2.pytorch_learner.classification_learner_config import (\n    ClassificationDataFormat)\n\nlog = logging.getLogger(__name__)\n\n\nclass ClassificationLearner(Learner):\n    def build_model(self):\n        pretrained = self.cfg.model.pretrained\n        model = getattr(\n            models, self.cfg.model.get_backbone_str())(pretrained=pretrained)\n        in_features = model.fc.in_features\n        num_labels = len(self.cfg.data.class_names)\n        model.fc = nn.Linear(in_features, num_labels)\n        return model\n\n    def _get_datasets(self, uri):\n        cfg = self.cfg\n        class_names = cfg.data.class_names\n\n        if cfg.data.data_format == ClassificationDataFormat.image_folder:\n            data_dirs = self.unzip_data(uri)\n\n        transform, aug_transform = self.get_data_transforms()\n\n        train_ds, valid_ds, test_ds = [], [], []\n        for data_dir in data_dirs:\n            train_dir = join(data_dir, 'train')\n            valid_dir = join(data_dir, 'valid')\n\n            if isdir(train_dir):\n                if cfg.overfit_mode:\n                    train_ds.append(\n                        AlbumentationsDataset(\n                            ImageFolder(train_dir, classes=class_names),\n                            transform=transform))\n                else:\n                    train_ds.append(\n                        AlbumentationsDataset(\n                            ImageFolder(train_dir, classes=class_names),\n                            transform=aug_transform))\n\n            if isdir(valid_dir):\n                valid_ds.append(\n                    AlbumentationsDataset(\n                        ImageFolder(valid_dir, classes=class_names),\n                        transform=transform))\n                test_ds.append(\n                    AlbumentationsDataset(\n                        ImageFolder(valid_dir, classes=class_names),\n                        transform=transform))\n\n        train_ds, valid_ds, test_ds = \\\n            ConcatDataset(train_ds), ConcatDataset(valid_ds), ConcatDataset(test_ds)\n\n        return train_ds, valid_ds, test_ds\n\n    def train_step(self, batch, batch_ind):\n        x, y = batch\n        out = self.post_forward(self.model(x))\n        return {'train_loss': F.cross_entropy(out, y)}\n\n    def validate_step(self, batch, batch_ind):\n        x, y = batch\n        out = self.post_forward(self.model(x))\n        val_loss = F.cross_entropy(out, y)\n\n        num_labels = len(self.cfg.data.class_names)\n        out = self.prob_to_pred(out)\n        conf_mat = compute_conf_mat(out, y, num_labels)\n\n        return {'val_loss': val_loss, 'conf_mat': conf_mat}\n\n    def validate_end(self, outputs, num_samples):\n        conf_mat = sum([o['conf_mat'] for o in outputs])\n        val_loss = torch.stack([o['val_loss']\n                                for o in outputs]).sum() / num_samples\n        conf_mat_metrics = compute_conf_mat_metrics(conf_mat,\n                                                    self.cfg.data.class_names)\n\n        metrics = {'val_loss': val_loss.item()}\n        metrics.update(conf_mat_metrics)\n\n        return metrics\n\n    def prob_to_pred(self, x):\n        return x.argmax(-1)\n\n    def plot_xyz(self, ax, x, y, z=None):\n        x = x.permute(1, 2, 0)\n        if x.shape[2] == 1:\n            x = torch.cat([x for _ in range(3)], dim=2)\n        ax.imshow(x)\n        title = 'true: {}'.format(self.cfg.data.class_names[y])\n        if z is not None:\n            title += ' / pred: {}'.format(self.cfg.data.class_names[z])\n        ax.set_title(title, fontsize=8)\n        ax.axis('off')\n"""
rastervision2/pytorch_learner/classification_learner_config.py,0,"b""from enum import Enum\n\nfrom rastervision2.pipeline.config import register_config\nfrom rastervision2.pytorch_learner.learner_config import (\n    LearnerConfig, DataConfig, ModelConfig)\n\n\nclass ClassificationDataFormat(Enum):\n    image_folder = 1\n\n\n@register_config('classification_data')\nclass ClassificationDataConfig(DataConfig):\n    data_format: ClassificationDataFormat = ClassificationDataFormat.image_folder\n\n\n@register_config('classification_model')\nclass ClassificationModelConfig(ModelConfig):\n    pass\n\n\n@register_config('classification_learner')\nclass ClassificationLearnerConfig(LearnerConfig):\n    data: ClassificationDataConfig\n    model: ClassificationModelConfig\n\n    def build(self, tmp_dir, model_path=None):\n        from rastervision2.pytorch_learner.classification_learner import (\n            ClassificationLearner)\n        return ClassificationLearner(self, tmp_dir, model_path=model_path)\n"""
rastervision2/pytorch_learner/image_folder.py,0,"b'# flake8: noqa\n# Adapted from https://github.com/pytorch/vision/blob/master/torchvision/datasets/folder.py\n# This makes it so a list of classes can be passed to ImageFolder in order to\n# force it to use a specific mapping from class_name to index.\n\nfrom torchvision.datasets.vision import VisionDataset\n\nfrom PIL import Image\n\nimport os\nimport os.path\nimport sys\n\n\ndef has_file_allowed_extension(filename, extensions):\n    """"""Checks if a file is an allowed extension.\n    Args:\n        filename (string): path to a file\n        extensions (tuple of strings): extensions to consider (lowercase)\n    Returns:\n        bool: True if the filename ends with one of given extensions\n    """"""\n    return filename.lower().endswith(extensions)\n\n\ndef is_image_file(filename):\n    """"""Checks if a file is an allowed image extension.\n    Args:\n        filename (string): path to a file\n    Returns:\n        bool: True if the filename ends with a known image extension\n    """"""\n    return has_file_allowed_extension(filename, IMG_EXTENSIONS)\n\n\ndef make_dataset(dir, class_to_idx, extensions=None, is_valid_file=None):\n    images = []\n    dir = os.path.expanduser(dir)\n    if not ((extensions is None) ^ (is_valid_file is None)):\n        raise ValueError(\n            \'Both extensions and is_valid_file cannot be None or not None at the same time\'\n        )\n    if extensions is not None:\n\n        def is_valid_file(x):\n            return has_file_allowed_extension(x, extensions)\n\n    for target in sorted(class_to_idx.keys()):\n        d = os.path.join(dir, target)\n        if not os.path.isdir(d):\n            continue\n        for root, _, fnames in sorted(os.walk(d)):\n            for fname in sorted(fnames):\n                path = os.path.join(root, fname)\n                if is_valid_file(path):\n                    item = (path, class_to_idx[target])\n                    images.append(item)\n\n    return images\n\n\nclass DatasetFolder(VisionDataset):\n    """"""A generic data loader where the samples are arranged in this way: ::\n        root/class_x/xxx.ext\n        root/class_x/xxy.ext\n        root/class_x/xxz.ext\n        root/class_y/123.ext\n        root/class_y/nsdf3.ext\n        root/class_y/asd932_.ext\n    Args:\n        root (string): Root directory path.\n        loader (callable): A function to load a sample given its path.\n        extensions (tuple[string]): A list of allowed extensions.\n            both extensions and is_valid_file should not be passed.\n        transform (callable, optional): A function/transform that takes in\n            a sample and returns a transformed version.\n            E.g, ``transforms.RandomCrop`` for images.\n        target_transform (callable, optional): A function/transform that takes\n            in the target and transforms it.\n        is_valid_file (callable, optional): A function that takes path of a file\n            and check if the file is a valid file (used to check of corrupt files)\n            both extensions and is_valid_file should not be passed.\n     Attributes:\n        classes (list): List of the class names.\n        class_to_idx (dict): Dict with items (class_name, class_index).\n        samples (list): List of (sample path, class_index) tuples\n        targets (list): The class_index value for each image in the dataset\n    """"""\n\n    def __init__(self,\n                 root,\n                 loader,\n                 extensions=None,\n                 transform=None,\n                 target_transform=None,\n                 is_valid_file=None,\n                 classes=None):\n        super(DatasetFolder, self).__init__(\n            root, transform=transform, target_transform=target_transform)\n        _classes, _class_to_idx = self._find_classes(self.root)\n\n        if classes is not None:\n            class_to_idx = dict(zip(classes, range(len(classes))))\n        else:\n            classes = _classes\n            class_to_idx = _class_to_idx\n\n        samples = make_dataset(self.root, class_to_idx, extensions,\n                               is_valid_file)\n        if len(samples) == 0:\n            raise (RuntimeError(\n                \'Found 0 files in subfolders of: \' + self.root + \'\\n\'\n                \'Supported extensions are: \' + \',\'.join(extensions)))\n\n        self.loader = loader\n        self.extensions = extensions\n\n        self.classes = classes\n        self.class_to_idx = class_to_idx\n        self.samples = samples\n        self.targets = [s[1] for s in samples]\n\n    def _find_classes(self, dir):\n        """"""\n        Finds the class folders in a dataset.\n        Args:\n            dir (string): Root directory path.\n        Returns:\n            tuple: (classes, class_to_idx) where classes are relative to (dir), and class_to_idx is a dictionary.\n        Ensures:\n            No class is a subdirectory of another.\n        """"""\n        if sys.version_info >= (3, 5):\n            # Faster and available in Python 3.5 and above\n            classes = [d.name for d in os.scandir(dir) if d.is_dir()]\n        else:\n            classes = [\n                d for d in os.listdir(dir)\n                if os.path.isdir(os.path.join(dir, d))\n            ]\n        classes.sort()\n        class_to_idx = {classes[i]: i for i in range(len(classes))}\n        return classes, class_to_idx\n\n    def __getitem__(self, index):\n        """"""\n        Args:\n            index (int): Index\n        Returns:\n            tuple: (sample, target) where target is class_index of the target class.\n        """"""\n        path, target = self.samples[index]\n        sample = self.loader(path)\n        if self.transform is not None:\n            sample = self.transform(sample)\n        if self.target_transform is not None:\n            target = self.target_transform(target)\n\n        return sample, target\n\n    def __len__(self):\n        return len(self.samples)\n\n\nIMG_EXTENSIONS = (\'.jpg\', \'.jpeg\', \'.png\', \'.ppm\', \'.bmp\', \'.pgm\', \'.tif\',\n                  \'.tiff\', \'.webp\')\n\n\ndef pil_loader(path):\n    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n    with open(path, \'rb\') as f:\n        img = Image.open(f)\n        return img.convert(\'RGB\')\n\n\ndef accimage_loader(path):\n    import accimage\n    try:\n        return accimage.Image(path)\n    except IOError:\n        # Potentially a decoding problem, fall back to PIL.Image\n        return pil_loader(path)\n\n\ndef default_loader(path):\n    from torchvision import get_image_backend\n    if get_image_backend() == \'accimage\':\n        return accimage_loader(path)\n    else:\n        return pil_loader(path)\n\n\nclass ImageFolder(DatasetFolder):\n    """"""A generic data loader where the images are arranged in this way: ::\n        root/dog/xxx.png\n        root/dog/xxy.png\n        root/dog/xxz.png\n        root/cat/123.png\n        root/cat/nsdf3.png\n        root/cat/asd932_.png\n    Args:\n        root (string): Root directory path.\n        transform (callable, optional): A function/transform that  takes in an PIL image\n            and returns a transformed version. E.g, ``transforms.RandomCrop``\n        target_transform (callable, optional): A function/transform that takes in the\n            target and transforms it.\n        loader (callable, optional): A function to load an image given its path.\n        is_valid_file (callable, optional): A function that takes path of an Image file\n            and check if the file is a valid file (used to check of corrupt files)\n     Attributes:\n        classes (list): List of the class names.\n        class_to_idx (dict): Dict with items (class_name, class_index).\n        imgs (list): List of (image path, class_index) tuples\n    """"""\n\n    def __init__(self,\n                 root,\n                 transform=None,\n                 target_transform=None,\n                 loader=default_loader,\n                 is_valid_file=None,\n                 classes=None):\n        super(ImageFolder, self).__init__(\n            root,\n            loader,\n            IMG_EXTENSIONS if is_valid_file is None else None,\n            transform=transform,\n            target_transform=target_transform,\n            is_valid_file=is_valid_file,\n            classes=classes)\n        self.imgs = self.samples\n'"
rastervision2/pytorch_learner/learner.py,20,"b'from os.path import join, isfile, basename\nimport csv\nimport warnings\nwarnings.filterwarnings(\'ignore\')  # noqa\nimport time\nimport datetime\nfrom abc import ABC, abstractmethod\nimport shutil\nimport os\nimport math\nimport logging\nfrom subprocess import Popen\nimport numbers\nimport zipfile\nfrom typing import Optional, List, Tuple, Dict, Union\nimport random\nimport uuid\n\nimport click\nimport matplotlib\nmatplotlib.use(\'Agg\')  # noqa\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport torch\nfrom torch import Tensor\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import CyclicLR, MultiStepLR, _LRScheduler\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torch.utils.data import DataLoader, Subset, Dataset, ConcatDataset\nfrom albumentations.augmentations.transforms import (\n    Blur, RandomRotate90, HorizontalFlip, VerticalFlip, GaussianBlur,\n    GaussNoise, RGBShift, ToGray, Resize)\nfrom albumentations import BboxParams, BasicTransform\nfrom albumentations.core.composition import Compose\nimport numpy as np\n\nfrom rastervision2.pipeline.file_system import (\n    sync_to_dir, json_to_file, file_to_json, make_dir, zipdir,\n    download_if_needed, sync_from_dir, get_local_path, unzip, list_paths,\n    str_to_file)\nfrom rastervision2.pipeline.utils import terminate_at_exit\nfrom rastervision2.pipeline.config import build_config, ConfigError\nfrom rastervision2.pytorch_learner.learner_config import LearnerConfig\n\nlog = logging.getLogger(__name__)\n\nMetricDict = Dict[str, float]\n\n\nclass Learner(ABC):\n    """"""Abstract training and prediction routines for a model.\n\n    This can be subclassed to handle different computer vision tasks. If a model_path\n    is passed to the constructor, the Learner can only be used for prediction (ie. only\n    predict and numpy_predict should be called). Otherwise, the Learner can be used for\n    training using the main() method.\n\n    Note that the validation set is used to validate at the end of each epoch, and the\n    test set is only used at the end of training. It\'s possible to set these to the same\n    dataset if desired.\n    """"""\n\n    def __init__(self,\n                 cfg: LearnerConfig,\n                 tmp_dir: str,\n                 model_path: Optional[str] = None):\n        """"""Constructor.\n\n        Args:\n            cfg: configuration\n            tmp_dir: root of temp dirs\n            model_path: a local path to model weights. If provided, the model is loaded\n                and it is assumed that this Learner will be used for prediction only.\n        """"""\n        self.cfg = cfg\n        self.tmp_dir = tmp_dir\n\n        # TODO make cache dirs configurable\n        torch_cache_dir = \'/opt/data/torch-cache\'\n        os.environ[\'TORCH_HOME\'] = torch_cache_dir\n        self.device = \'cuda\' if torch.cuda.is_available() else \'cpu\'\n        self.data_cache_dir = \'/opt/data/data-cache\'\n        make_dir(self.data_cache_dir)\n\n        self.model = self.build_model()\n        self.model.to(self.device)\n\n        if model_path is not None:\n            if isfile(model_path):\n                self.model.load_state_dict(\n                    torch.load(model_path, map_location=self.device))\n            else:\n                raise Exception(\n                    \'Model could not be found at {}\'.format(model_path))\n            self.model.eval()\n        else:\n            log.info(self.cfg)\n\n            # ds = dataset, dl = dataloader\n            self.train_ds = None\n            self.train_dl = None\n            self.valid_ds = None\n            self.valid_dl = None\n            self.test_ds = None\n            self.test_dl = None\n\n            if cfg.output_uri.startswith(\'s3://\'):\n                self.output_dir = get_local_path(cfg.output_uri, tmp_dir)\n                make_dir(self.output_dir, force_empty=True)\n                if not cfg.overfit_mode:\n                    self.sync_from_cloud()\n            else:\n                self.output_dir = cfg.output_uri\n                make_dir(self.output_dir)\n\n            self.last_model_path = join(self.output_dir, \'last-model.pth\')\n            self.config_path = join(self.output_dir, \'learner-config.json\')\n            self.train_state_path = join(self.output_dir, \'train-state.json\')\n            self.log_path = join(self.output_dir, \'log.csv\')\n            model_bundle_fn = basename(cfg.get_model_bundle_uri())\n            self.model_bundle_path = join(self.output_dir, model_bundle_fn)\n            self.metric_names = self.build_metric_names()\n\n            str_to_file(self.cfg.json(), self.config_path)\n            self.load_init_weights()\n            self.load_checkpoint()\n            self.opt = self.build_optimizer()\n            self.setup_data()\n            self.start_epoch = self.get_start_epoch()\n            self.steps_per_epoch = len(\n                self.train_ds) // self.cfg.solver.batch_sz\n            self.step_scheduler = self.build_step_scheduler()\n            self.epoch_scheduler = self.build_epoch_scheduler()\n            self.setup_tensorboard()\n\n    def main(self):\n        """"""Main training sequence.\n\n        This plots the dataset, runs a training and validation loop (which will resume if\n        interrupted), logs stats, plots predictions, and syncs results to the cloud.\n        """"""\n        self.run_tensorboard()\n        cfg = self.cfg\n        self.log_data_stats()\n        if not cfg.predict_mode:\n            self.plot_dataloaders()\n            if cfg.overfit_mode:\n                self.overfit()\n            else:\n                self.train()\n                if cfg.save_model_bundle:\n                    self.save_model_bundle()\n\n        self.load_checkpoint()\n        if cfg.eval_train:\n            self.eval_model(\'train\')\n        self.eval_model(\'test\')\n        self.sync_to_cloud()\n        self.stop_tensorboard()\n\n    def sync_to_cloud(self):\n        """"""Sync any output to the cloud at output_uri.""""""\n        if self.cfg.output_uri.startswith(\'s3://\'):\n            sync_to_dir(self.output_dir, self.cfg.output_uri)\n\n    def sync_from_cloud(self):\n        """"""Sync any previous output in the cloud to output_dir.""""""\n        if self.cfg.output_uri.startswith(\'s3://\'):\n            sync_from_dir(self.cfg.output_uri, self.output_dir)\n\n    def setup_tensorboard(self):\n        """"""Setup for logging stats to TB.""""""\n        self.tb_writer = None\n        if self.cfg.log_tensorboard:\n            self.tb_log_dir = join(self.output_dir, \'tb-logs\')\n            make_dir(self.tb_log_dir)\n            self.tb_writer = SummaryWriter(log_dir=self.tb_log_dir)\n\n    def run_tensorboard(self):\n        """"""Run TB server serving logged stats.""""""\n        if self.cfg.run_tensorboard:\n            log.info(\'Starting tensorboard process\')\n            self.tb_process = Popen(\n                [\'tensorboard\', \'--logdir={}\'.format(self.tb_log_dir)])\n            terminate_at_exit(self.tb_process)\n\n    def stop_tensorboard(self):\n        """"""Stop TB logging and server if it\'s running.""""""\n        if self.cfg.log_tensorboard:\n            self.tb_writer.close()\n            if self.cfg.run_tensorboard:\n                self.tb_process.terminate()\n\n    @abstractmethod\n    def build_model(self) -> nn.Module:\n        """"""Build a PyTorch model.""""""\n        pass\n\n    def unzip_data(self, uri: Union[str, List[str]]) -> List[str]:\n        """"""Unzip dataset zip files.\n\n        Args:\n            uri: a list of URIs of zip files or the URI of a directory containing\n                zip files\n\n        Returns:\n            paths to directories that each contain contents of one zip file\n        """"""\n        cfg = self.cfg\n        data_dirs = []\n\n        if isinstance(uri, list):\n            zip_uris = uri\n        else:\n            # TODO generalize this to work with any file system\n            if uri.startswith(\'s3://\') or uri.startswith(\'/\'):\n                data_uri = uri\n            else:\n                data_uri = join(cfg.base_uri, uri)\n            zip_uris = ([data_uri]\n                        if data_uri.endswith(\'.zip\') else list_paths(\n                            data_uri, \'zip\'))\n\n        for zip_ind, zip_uri in enumerate(zip_uris):\n            zip_path = get_local_path(zip_uri, self.data_cache_dir)\n            if not isfile(zip_path):\n                zip_path = download_if_needed(zip_uri, self.data_cache_dir)\n            with zipfile.ZipFile(zip_path, \'r\') as zipf:\n                data_dir = join(self.tmp_dir, \'data\', str(uuid.uuid4()), str(zip_ind))\n                data_dirs.append(data_dir)\n                zipf.extractall(data_dir)\n\n        return data_dirs\n\n    def get_bbox_params(self) -> Optional[BboxParams]:\n        """"""Returns BboxParams used by albumentations for data augmentation.""""""\n        return None\n\n    def get_data_transforms(self) -> Tuple[BasicTransform, BasicTransform]:\n        """"""Get albumentations transform objects for data augmentation.\n\n        Returns:\n           1st tuple arg: a transform that doesn\'t do any data augmentation\n           2nd tuple arg: a transform with data augmentation\n        """"""\n        cfg = self.cfg\n        bbox_params = self.get_bbox_params()\n        transform = Compose(\n            [Resize(cfg.data.img_sz, cfg.data.img_sz)],\n            bbox_params=bbox_params)\n\n        augmentors_dict = {\n            \'Blur\': Blur(),\n            \'RandomRotate90\': RandomRotate90(),\n            \'HorizontalFlip\': HorizontalFlip(),\n            \'VerticalFlip\': VerticalFlip(),\n            \'GaussianBlur\': GaussianBlur(),\n            \'GaussNoise\': GaussNoise(),\n            \'RGBShift\': RGBShift(),\n            \'ToGray\': ToGray()\n        }\n        aug_transforms = []\n        for augmentor in cfg.data.augmentors:\n            try:\n                aug_transforms.append(augmentors_dict[augmentor])\n            except KeyError as e:\n                log.warning(\n                    \'{0} is an unknown augmentor. Continuing without {0}. \\\n                    Known augmentors are: {1}\'.format(\n                        e, list(augmentors_dict.keys())))\n        aug_transforms.append(Resize(cfg.data.img_sz, cfg.data.img_sz))\n        aug_transform = Compose(aug_transforms, bbox_params=bbox_params)\n\n        return transform, aug_transform\n\n    def get_collate_fn(self) -> Optional[callable]:\n        """"""Returns a custom collate_fn to use in DataLoader.\n\n        None is returned if default collate_fn should be used.\n\n        See https://pytorch.org/docs/stable/data.html#working-with-collate-fn\n        """"""\n        return None\n\n    def _get_datasets(self, uri: Union[str, List[str]]) -> Tuple[Dataset, Dataset, Dataset]:  # noqa\n        """"""Gets Datasets for a single group of chips.\n\n        This should be overridden for each Learner subclass.\n\n        Args:\n            uri: a list of URIs of zip files or the URI of a directory containing\n                zip files\n\n        Returns:\n            train, validation, and test DataSets.""""""\n        raise NotImplementedError()\n\n    def get_datasets(self) -> Tuple[Dataset, Dataset, Dataset]:\n        """"""Returns train, validation, and test DataSets.""""""\n        if self.cfg.data.group_uris:\n            train_ds_lst, valid_ds_lst, test_ds_lst = [], [], []\n            for group_uri in self.cfg.data.group_uris:\n                train_ds, valid_ds, test_ds = self._get_datasets(group_uri)\n                group_train_sz = self.cfg.data.group_train_sz\n                if group_train_sz is not None:\n                    train_inds = list(range(len(train_ds)))\n                    random.shuffle(train_inds)\n                    train_inds = train_inds[0:group_train_sz]\n                    train_ds = Subset(train_ds, train_inds)\n                train_ds_lst.append(train_ds)\n                valid_ds_lst.append(valid_ds)\n                test_ds_lst.append(test_ds)\n\n            train_ds, valid_ds, test_ds = (\n                ConcatDataset(train_ds_lst), ConcatDataset(valid_ds_lst),\n                ConcatDataset(test_ds_lst))\n            return train_ds, valid_ds, test_ds\n        else:\n            return self._get_datasets(self.cfg.data.uri)\n\n    def setup_data(self):\n        """"""Set the the DataSet and DataLoaders for train, validation, and test sets.""""""\n        cfg = self.cfg\n        batch_sz = self.cfg.solver.batch_sz\n        num_workers = self.cfg.data.num_workers\n\n        train_ds, valid_ds, test_ds = self.get_datasets()\n        if len(train_ds) < batch_sz:\n            raise ConfigError(\n                \'Training dataset has fewer elements than batch size.\')\n        if len(valid_ds) < batch_sz:\n            raise ConfigError(\n                \'Validation dataset has fewer elements than batch size.\')\n        if len(test_ds) < batch_sz:\n            raise ConfigError(\n                \'Test dataset has fewer elements than batch size.\')\n\n        if cfg.overfit_mode:\n            train_ds = Subset(train_ds, range(batch_sz))\n            valid_ds = train_ds\n            test_ds = train_ds\n        elif cfg.test_mode:\n            train_ds = Subset(train_ds, range(batch_sz))\n            valid_ds = Subset(valid_ds, range(batch_sz))\n            test_ds = Subset(test_ds, range(batch_sz))\n\n        if cfg.data.train_sz is not None:\n            train_inds = list(range(len(train_ds)))\n            random.shuffle(train_inds)\n            train_inds = train_inds[0:cfg.data.train_sz]\n            train_ds = Subset(train_ds, train_inds)\n\n        collate_fn = self.get_collate_fn()\n        train_dl = DataLoader(\n            train_ds,\n            shuffle=True,\n            batch_size=batch_sz,\n            num_workers=num_workers,\n            pin_memory=True,\n            collate_fn=collate_fn)\n        valid_dl = DataLoader(\n            valid_ds,\n            shuffle=True,\n            batch_size=batch_sz,\n            num_workers=num_workers,\n            pin_memory=True,\n            collate_fn=collate_fn)\n        test_dl = DataLoader(\n            test_ds,\n            shuffle=True,\n            batch_size=batch_sz,\n            num_workers=num_workers,\n            pin_memory=True,\n            collate_fn=collate_fn)\n\n        self.train_ds, self.valid_ds, self.test_ds = (train_ds, valid_ds,\n                                                      test_ds)\n        self.train_dl, self.valid_dl, self.test_dl = (train_dl, valid_dl,\n                                                      test_dl)\n\n    def log_data_stats(self):\n        """"""Log stats about each DataSet.""""""\n        if self.train_ds:\n            log.info(\'train_ds: {} items\'.format(len(self.train_ds)))\n        if self.valid_ds:\n            log.info(\'valid_ds: {} items\'.format(len(self.valid_ds)))\n        if self.test_ds:\n            log.info(\'test_ds: {} items\'.format(len(self.test_ds)))\n\n    def build_optimizer(self) -> optim.Optimizer:\n        """"""Returns optimizer.""""""\n        return optim.Adam(self.model.parameters(), lr=self.cfg.solver.lr)\n\n    def build_step_scheduler(self) -> _LRScheduler:\n        """"""Returns an LR scheduler that changes the LR each step.\n\n        This is used to implement the ""one cycle"" schedule popularized by\n        fastai.\n        """"""\n        scheduler = None\n        cfg = self.cfg\n        if cfg.solver.one_cycle and cfg.solver.num_epochs > 1:\n            total_steps = cfg.solver.num_epochs * self.steps_per_epoch\n            step_size_up = (cfg.solver.num_epochs // 2) * self.steps_per_epoch\n            step_size_down = total_steps - step_size_up\n            step_scheduler = CyclicLR(\n                self.opt,\n                base_lr=cfg.solver.lr / 10,\n                max_lr=cfg.solver.lr,\n                step_size_up=step_size_up,\n                step_size_down=step_size_down,\n                cycle_momentum=False)\n            for _ in range(self.start_epoch * self.steps_per_epoch):\n                step_scheduler.step()\n        return scheduler\n\n    def build_epoch_scheduler(self) -> _LRScheduler:\n        """"""Returns an LR scheduler tha changes the LR each epoch.\n\n        This is used to divide the LR by 10 at certain epochs.\n        """"""\n        scheduler = None\n        if self.cfg.solver.multi_stage:\n            scheduler = MultiStepLR(\n                self.opt, milestones=self.cfg.solver.multi_stage, gamma=0.1)\n            for _ in range(self.start_epoch):\n                scheduler.step()\n        return scheduler\n\n    def build_metric_names(self) -> List[str]:\n        """"""Returns names of metrics used to validate model at each epoch.""""""\n        metric_names = [\n            \'epoch\', \'train_time\', \'valid_time\', \'train_loss\', \'val_loss\',\n            \'avg_f1\', \'avg_precision\', \'avg_recall\'\n        ]\n\n        for label in self.cfg.data.class_names:\n            metric_names.extend([\n                \'{}_f1\'.format(label), \'{}_precision\'.format(label),\n                \'{}_recall\'.format(label)\n            ])\n        return metric_names\n\n    @abstractmethod\n    def train_step(self, batch: any, batch_ind: int) -> MetricDict:\n        """"""Compute loss for a single training batch.\n\n        Args:\n            batch: batch data needed to compute loss\n            batch_ind: index of batch within epoch\n\n        Returns:\n            dict with \'train_loss\' as key and possibly other losses\n        """"""\n        pass\n\n    @abstractmethod\n    def validate_step(self, batch: any, batch_ind: int) -> MetricDict:\n        """"""Compute metrics on validation batch.\n\n        Args:\n            batch: batch data needed to compute validation metrics\n            batch_ind: index of batch within epoch\n\n        Returns:\n            dict with metric names mapped to metric values\n        """"""\n        pass\n\n    def train_end(self, outputs: List[MetricDict],\n                  num_samples: int) -> MetricDict:\n        """"""Aggregate the ouput of train_step at the end of the epoch.\n\n        Args:\n            outputs: a list of outputs of train_step\n            num_samples: total number of training samples processed in epoch\n        """"""\n        metrics = {}\n        for k in outputs[0].keys():\n            metrics[k] = torch.stack([o[k] for o in outputs\n                                      ]).sum().item() / num_samples\n        return metrics\n\n    def validate_end(self, outputs: List[MetricDict],\n                     num_samples: int) -> MetricDict:\n        """"""Aggregate the ouput of validate_step at the end of the epoch.\n\n        Args:\n            outputs: a list of outputs of validate_step\n            num_samples: total number of validation samples processed in epoch\n        """"""\n        metrics = {}\n        for k in outputs[0].keys():\n            metrics[k] = torch.stack([o[k] for o in outputs\n                                      ]).sum().item() / num_samples\n        return metrics\n\n    def post_forward(self, x: any) -> any:\n        """"""Post process output of call to model().\n\n        Useful for when predictions are inside a structure returned by model().\n        """"""\n        return x\n\n    def prob_to_pred(self, x: Tensor) -> Tensor:\n        """"""Convert a Tensor with prediction probabilities to class ids.\n\n        The class ids should be the classes with the maximum probability.\n        """"""\n        raise NotImplementedError()\n\n    def to_batch(self, x: Tensor) -> Tensor:\n        """"""Ensure that image array has batch dimension.\n\n        Args:\n            x: assumed to be either image or batch of images\n\n        Returns:\n            x with extra batch dimension of length 1 if needed\n        """"""\n        if x.ndim == 3:\n            x = x.unsqueeze(0)\n        return x\n\n    def normalize_input(self, x: Tensor) -> Tensor:\n        """"""Normalize an input image to have values between 0 and 1.\n\n        Args:\n            x: an image or batch of images assumed to be in uint8 format\n\n        Returns:\n            the same tensor that has been scaled to [0-1].\n\n        """"""\n        return x.float() / 255.0\n\n    def predict(self,\n                x: Tensor,\n                normalize: bool = False,\n                raw_out: bool = False) -> any:\n        """"""Make prediction for an image or batch of images.\n\n        Args:\n            x: image or batch of images\n            normalize: if True, call normalize_input() on x before passing into model\n            raw_out: if True, return prediction probabilities\n\n        Returns:\n            the predictions, in probability form if raw_out is True, in class_id form\n                otherwise\n        """"""\n        x = self.to_batch(x)\n        if normalize:\n            x = self.normalize_input(x)\n        x = self.to_device(x, self.device)\n        with torch.no_grad():\n            out = self.model(x)\n            if not raw_out:\n                out = self.prob_to_pred(self.post_forward(out))\n        out = self.to_device(out, \'cpu\')\n        return out\n\n    def output_to_numpy(self, out: any) -> any:\n        """"""Convert output of model to numpy format.\n\n        Args:\n            out: the output of the model in PyTorch format\n\n        Returns: the output of the model in numpy format\n        """"""\n        return out.numpy()\n\n    def numpy_predict(self, x: np.ndarray, raw_out: bool = False) -> any:\n        """"""Make a prediction using an image or batch of images in numpy format.\n\n        Args:\n            x: (ndarray) of shape [height, width, channels] or\n                [batch_sz, height, width, channels] in uint8 format\n            raw_out: if True, return prediction probabilities\n\n        Returns:\n            predictions using numpy arrays\n        """"""\n        x = torch.tensor(x)\n        x = self.to_batch(x)\n        x = x.permute((0, 3, 1, 2))\n        out = self.predict(x, normalize=True, raw_out=raw_out)\n        return self.output_to_numpy(out)\n\n    def predict_dataloader(self,\n                           dl: DataLoader,\n                           one_batch: bool = False,\n                           return_x: bool = True):\n        """"""Make predictions over all batches in a DataLoader.\n\n        Args:\n            dl: the DataLoader\n            one_batch: if True, just makes predictions over the first batch\n            return_x: if True, returns all the inputs in addition to the predictions and\n                targets\n\n        Returns:\n            if return_x: (x, y, z) ie. all images, labels, predictions for dl\n            else: (y, z) ie. all labels, predictions for dl\n        """"""\n        self.model.eval()\n\n        xs, ys, zs = [], [], []\n        with torch.no_grad():\n            for x, y in dl:\n                x = self.to_device(x, self.device)\n                z = self.prob_to_pred(self.post_forward(self.model(x)))\n                x = self.to_device(x, \'cpu\')\n                z = self.to_device(z, \'cpu\')\n                if one_batch:\n                    return x, y, z\n                if return_x:\n                    xs.append(x)\n                ys.append(y)\n                zs.append(z)\n\n        if return_x:\n            return torch.cat(xs), torch.cat(ys), torch.cat(zs)\n        return torch.cat(ys), torch.cat(zs)\n\n    def get_dataloader(self, split: str) -> DataLoader:\n        """"""Get the DataLoader for a split.\n\n        Args:\n            split: a split name which can be train, valid, or test\n        """"""\n        if split == \'train\':\n            return self.train_dl\n        elif split == \'valid\':\n            return self.valid_dl\n        elif split == \'test\':\n            return self.test_dl\n        else:\n            raise ValueError(\'{} is not a valid split\'.format(split))\n\n    @abstractmethod\n    def plot_xyz(self, ax, x: Tensor, y, z=None):\n        """"""Plot image, ground truth labels, and predicted labels.\n\n        Args:\n            ax: matplotlib axis on which to plot\n            x: image\n            y: ground truth labels\n            z: optional predicted labels\n        """"""\n        pass\n\n    def plot_batch(self, x: Tensor, y, output_path: str, z=None):\n        """"""Plot a whole batch in a grid using plot_xyz.\n\n        Args:\n            x: batch of images\n            y: ground truth labels\n            output_path: local path where to save plot image\n            z: optional predicted labels\n        """"""\n        batch_sz = x.shape[0]\n        ncols = nrows = math.ceil(math.sqrt(batch_sz))\n        fig = plt.figure(\n            constrained_layout=True, figsize=(3 * ncols, 3 * nrows))\n        grid = gridspec.GridSpec(ncols=ncols, nrows=nrows, figure=fig)\n\n        for i in range(batch_sz):\n            ax = fig.add_subplot(grid[i])\n            if z is None:\n                self.plot_xyz(ax, x[i], y[i])\n            else:\n                self.plot_xyz(ax, x[i], y[i], z=z[i])\n\n        make_dir(output_path, use_dirname=True)\n        plt.savefig(output_path)\n        plt.close()\n\n    def plot_predictions(self, split: str):\n        """"""Plot predictions for a split.\n\n        Uses the first batch for the corresponding DataLoader.\n\n        Args:\n            split: dataset split. Can be train, valid, or test.\n        """"""\n        log.info(\'Plotting predictions...\')\n        dl = self.get_dataloader(split)\n        output_path = join(self.output_dir, \'{}_preds.png\'.format(split))\n        x, y, z = self.predict_dataloader(dl, one_batch=True)\n        self.plot_batch(x, y, output_path, z=z)\n\n    def plot_dataloader(self, dl: DataLoader, output_path: str):\n        """"""Plot images and ground truth labels for a DataLoader.""""""\n        x, y = next(iter(dl))\n        self.plot_batch(x, y, output_path)\n\n    def plot_dataloaders(self):\n        """"""Plot images and ground truth labels for all DataLoaders.""""""\n        if self.train_dl:\n            self.plot_dataloader(\n                self.train_dl, join(self.output_dir, \'dataloaders/train.png\'))\n        if self.valid_dl:\n            self.plot_dataloader(\n                self.valid_dl, join(self.output_dir, \'dataloaders/valid.png\'))\n        if self.test_dl:\n            self.plot_dataloader(self.test_dl,\n                                 join(self.output_dir, \'dataloaders/test.png\'))\n\n    @staticmethod\n    def from_model_bundle(model_bundle_uri: str, tmp_dir: str):\n        """"""Create a Learner from a model bundle.""""""\n        model_bundle_path = download_if_needed(model_bundle_uri, tmp_dir)\n        model_bundle_dir = join(tmp_dir, \'model-bundle\')\n        unzip(model_bundle_path, model_bundle_dir)\n\n        config_path = join(model_bundle_dir, \'learner-config.json\')\n        model_path = join(model_bundle_dir, \'model.pth\')\n        cfg = build_config(file_to_json(config_path))\n        return cfg.build(tmp_dir, model_path=model_path)\n\n    def save_model_bundle(self):\n        """"""Save a model bundle.\n\n        This is a zip file with the model weights in .pth format and a serialized\n        copy of the LearningConfig, which allows for making predictions in the future.\n        """"""\n        model_bundle_dir = join(self.tmp_dir, \'model-bundle\')\n        make_dir(model_bundle_dir)\n        shutil.copyfile(self.last_model_path,\n                        join(model_bundle_dir, \'model.pth\'))\n        shutil.copyfile(self.config_path,\n                        join(model_bundle_dir, \'learner-config.json\'))\n        zipdir(model_bundle_dir, self.model_bundle_path)\n\n    def get_start_epoch(self) -> int:\n        """"""Get start epoch.\n\n        If training was interrupted, this returns the last complete epoch + 1.\n        """"""\n        start_epoch = 0\n        if isfile(self.log_path):\n            with open(self.log_path) as log_file:\n                last_line = log_file.readlines()[-1]\n            last_epoch = int(last_line.split(\',\')[0].strip())\n            start_epoch = last_epoch + 1\n        return start_epoch\n\n    def load_init_weights(self):\n        """"""Load the weights to initialize model.""""""\n        if self.cfg.model.init_weights:\n            weights_path = download_if_needed(self.cfg.model.init_weights,\n                                              self.tmp_dir)\n            self.model.load_state_dict(\n                torch.load(weights_path, map_location=self.device))\n\n    def load_checkpoint(self):\n        """"""Load last weights from previous run if available.""""""\n        if isfile(self.last_model_path):\n            log.info(\'Loading checkpoint from {}\'.format(self.last_model_path))\n            self.model.load_state_dict(\n                torch.load(self.last_model_path, map_location=self.device))\n\n    def to_device(self, x: any, device: str) -> any:\n        """"""Load Tensors onto a device.\n\n        Args:\n            x: some object with Tensors in it\n            device: \'cpu\' or \'cuda\'\n\n        Returns:\n            x but with any Tensors in it on the device\n        """"""\n        if isinstance(x, list):\n            return [_x.to(device) for _x in x]\n        else:\n            return x.to(device)\n\n    def train_epoch(self) -> MetricDict:\n        """"""Train for a single epoch.""""""\n        start = time.time()\n        self.model.train()\n        num_samples = 0\n        outputs = []\n        with click.progressbar(self.train_dl, label=\'Training\') as bar:\n            for batch_ind, (x, y) in enumerate(bar):\n                x = self.to_device(x, self.device)\n                y = self.to_device(y, self.device)\n                batch = (x, y)\n                self.opt.zero_grad()\n                output = self.train_step(batch, batch_ind)\n                outputs.append(output)\n                loss = output[\'train_loss\']\n                loss.backward()\n                self.opt.step()\n                if self.step_scheduler:\n                    self.step_scheduler.step()\n                num_samples += x.shape[0]\n        metrics = self.train_end(outputs, num_samples)\n        end = time.time()\n        train_time = datetime.timedelta(seconds=end - start)\n        metrics[\'train_time\'] = str(train_time)\n        return metrics\n\n    def validate_epoch(self, dl: DataLoader) -> MetricDict:\n        """"""Validate for a single epoch.""""""\n        start = time.time()\n        self.model.eval()\n        num_samples = 0\n        outputs = []\n        with torch.no_grad():\n            with click.progressbar(dl, label=\'Validating\') as bar:\n                for batch_ind, (x, y) in enumerate(bar):\n                    x = self.to_device(x, self.device)\n                    y = self.to_device(y, self.device)\n                    batch = (x, y)\n                    output = self.validate_step(batch, batch_ind)\n                    outputs.append(output)\n                    num_samples += x.shape[0]\n        end = time.time()\n        validate_time = datetime.timedelta(seconds=end - start)\n\n        metrics = self.validate_end(outputs, num_samples)\n        metrics[\'valid_time\'] = str(validate_time)\n        return metrics\n\n    def overfit(self):\n        """"""Optimize model using the same batch repeatedly.""""""\n        self.on_overfit_start()\n\n        x, y = next(iter(self.train_dl))\n        x = self.to_device(x, self.device)\n        y = self.to_device(y, self.device)\n        batch = (x, y)\n\n        with click.progressbar(\n                range(self.cfg.solver.overfit_num_steps),\n                label=\'Overfitting\') as bar:\n            for step in bar:\n                loss = self.train_step(batch, step)[\'train_loss\']\n                loss.backward()\n                self.opt.step()\n\n                if (step + 1) % 25 == 0:\n                    log.info(\'\\nstep: {}\'.format(step))\n                    log.info(\'train_loss: {}\'.format(loss))\n\n        torch.save(self.model.state_dict(), self.last_model_path)\n\n    def train(self):\n        """"""Training loop that will attempt to resume training if appropriate.""""""\n        self.on_train_start()\n\n        if self.start_epoch > 0 and self.start_epoch <= self.cfg.solver.num_epochs:\n            log.info(\'Resuming training from epoch {}\'.format(\n                self.start_epoch))\n\n        for epoch in range(self.start_epoch, self.cfg.solver.num_epochs):\n            log.info(\'epoch: {}\'.format(epoch))\n            train_metrics = self.train_epoch()\n            if self.epoch_scheduler:\n                self.epoch_scheduler.step()\n            valid_metrics = self.validate_epoch(self.valid_dl)\n            metrics = dict(epoch=epoch, **train_metrics, **valid_metrics)\n            log.info(\'metrics: {}\'.format(metrics))\n\n            self.on_epoch_end(epoch, metrics)\n\n    def on_overfit_start(self):\n        """"""Hook that is called at start of overfit routine.""""""\n        pass\n\n    def on_train_start(self):\n        """"""Hook that is called at start of train routine.""""""\n        pass\n\n    def on_epoch_end(self, curr_epoch, metrics):\n        """"""Hook that is called at end of epoch.\n\n        Writes metrics to CSV and TB, and saves model.\n        """"""\n        if not isfile(self.log_path):\n            with open(self.log_path, \'w\') as log_file:\n                log_writer = csv.writer(log_file)\n                row = self.metric_names\n                log_writer.writerow(row)\n\n        with open(self.log_path, \'a\') as log_file:\n            log_writer = csv.writer(log_file)\n            row = [metrics[k] for k in self.metric_names]\n            log_writer.writerow(row)\n\n        if self.cfg.log_tensorboard:\n            for key, val in metrics.items():\n                if isinstance(val, numbers.Number):\n                    self.tb_writer.add_scalar(key, val, curr_epoch)\n            for name, param in self.model.named_parameters():\n                self.tb_writer.add_histogram(name, param, curr_epoch)\n\n        torch.save(self.model.state_dict(), self.last_model_path)\n\n        if (curr_epoch + 1) % self.cfg.solver.sync_interval == 0:\n            self.sync_to_cloud()\n\n    def eval_model(self, split: str):\n        """"""Evaluate model using a particular dataset split.\n\n        Gets validation metrics and saves them along with prediction plots.\n\n        Args:\n            split: the dataset split to use: train, valid, or test.\n        """"""\n        log.info(\'Evaluating on {} set...\'.format(split))\n        dl = self.get_dataloader(split)\n        metrics = self.validate_epoch(dl)\n        log.info(\'metrics: {}\'.format(metrics))\n        json_to_file(metrics,\n                     join(self.output_dir, \'{}_metrics.json\'.format(split)))\n        self.plot_predictions(split)\n'"
rastervision2/pytorch_learner/learner_config.py,0,"b'from typing import List, Optional, Union, Tuple, TYPE_CHECKING\nfrom os.path import join\nimport importlib\nfrom enum import Enum\n\nfrom pydantic import PositiveFloat, PositiveInt\n\nfrom rastervision2.pipeline.config import (Config, register_config,\n                                           ConfigError, Field)\nfrom rastervision2.pytorch_learner.utils import color_to_triple\n\ndefault_augmentors = [\'RandomRotate90\', \'HorizontalFlip\', \'VerticalFlip\']\naugmentors = [\n    \'Blur\', \'RandomRotate90\', \'HorizontalFlip\', \'VerticalFlip\', \'GaussianBlur\',\n    \'GaussNoise\', \'RGBShift\', \'ToGray\'\n]\n\nif TYPE_CHECKING:\n    from rastervision2.pytorch_learner.learner import Learner  # noqa\n\n\ndef get_torchvision_backbones():\n    backbones = []\n    # This may need to be updated after upgrading torchvision.\n    packages = [\n        \'alexnet\', \'densenet\', \'googlenet\', \'inception\', \'mnasnet\',\n        \'mobilenet\', \'resnet\', \'shufflenetv2\', \'squeezenet\', \'vgg\'\n    ]\n    for package in packages:\n        module = importlib.import_module(\n            \'torchvision.models.{}\'.format(package))\n        backbones.extend(module.__all__)\n\n    return backbones\n\n\nbackbones = get_torchvision_backbones()\n\n\nclass Backbone(Enum):\n    alexnet = 1\n    densenet121 = 2\n    densenet169 = 3\n    densenet201 = 4\n    densenet161 = 5\n    googlenet = 6\n    inception_v3 = 7\n    mnasnet0_5 = 8\n    mnasnet0_75 = 9\n    mnasnet1_0 = 10\n    mnasnet1_3 = 11\n    mobilenet_v2 = 12\n    resnet18 = 13\n    resnet34 = 14\n    resnet50 = 15\n    resnet101 = 16\n    resnet152 = 17\n    resnext50_32x4d = 18\n    resnext101_32x8d = 19\n    wide_resnet50_2 = 20\n    wide_resnet101_2 = 21\n    shufflenet_v2_x0_5 = 22\n    shufflenet_v2_x1_0 = 23\n    shufflenet_v2_x1_5 = 24\n    shufflenet_v2_x2_0 = 25\n    squeezenet1_0 = 26\n    squeezenet1_1 = 27\n    vgg11 = 28\n    vgg11_bn = 29\n    vgg13 = 30\n    vgg13_bn = 31\n    vgg16 = 32\n    vgg16_bn = 33\n    vgg19_bn = 34\n    vgg19 = 35\n\n\n@register_config(\'model\')\nclass ModelConfig(Config):\n    """"""Config related to models.""""""\n    backbone: Backbone = Field(\n        Backbone.resnet18,\n        description=\'The torchvision.models backbone to use.\')\n    pretrained: bool = Field(\n        True,\n        description=(\n            \'If True, use ImageNet weights. If False, use random initialization.\'\n        ))\n    init_weights: Optional[str] = Field(\n        None,\n        description=(\'URI of PyTorch model weights used to initialize model. \'\n                     \'If set, this supercedes the pretrained option.\'))\n\n    def update(self, learner: Optional[\'LearnerConfig\'] = None):\n        pass\n\n    def get_backbone_str(self):\n        return self.backbone.name\n\n\n@register_config(\'solver\')\nclass SolverConfig(Config):\n    """"""Config related to solver aka optimizer.""""""\n    lr: PositiveFloat = Field(1e-4, description=\'Learning rate.\')\n    num_epochs: PositiveInt = Field(\n        10,\n        description=\n        \'Number of epochs (ie. sweeps through the whole training set).\')\n    test_num_epochs: PositiveInt = Field(\n        2, description=\'Number of epochs to use in test mode.\')\n    test_batch_sz: PositiveInt = Field(\n        4, description=\'Batch size to use in test mode.\')\n    overfit_num_steps: PositiveInt = Field(\n        1, description=\'Number of optimizer steps to use in overfit mode.\')\n    sync_interval: PositiveInt = Field(\n        1, description=\'The interval in epochs for each sync to the cloud.\')\n    batch_sz: PositiveInt = Field(32, description=\'Batch size.\')\n    one_cycle: bool = Field(\n        True,\n        description=\n        (\'If True, use triangular LR scheduler with a single cycle across all \'\n         \'epochs with start and end LR being lr/10 and the peak being lr.\'))\n    multi_stage: List = Field(\n        [], description=(\'List of epoch indices at which to divide LR by 10.\'))\n\n    def update(self, learner: Optional[\'LearnerConfig\'] = None):\n        pass\n\n\n@register_config(\'data\')\nclass DataConfig(Config):\n    """"""Config related to dataset for training and testing.""""""\n    uri: Union[None, str, List[str]] = Field(\n        None,\n        description=\n        (\'URI of the dataset. This can be a zip file, a list of zip files, or a \'\n         \'directory which contains a set of zip files.\'))\n    train_sz: Optional[int] = Field(\n        None, description=(\n            \'If set, the number of training images to use. If fewer images exist, \'\n            \'then an exception will be raised.\'))\n    group_uris: Union[None, List[Union[str, List[str]]]] = Field(None, description=(\n        \'This can be set instead of uri in order to specify groups of chips. Each \'\n        \'element in the list is expected to be an object of the same form accepted by \'\n        \'the uri field. The purpose of separating chips into groups is to be able to \'\n        \'use the group_train_sz field.\'))\n    group_train_sz: Optional[int] = Field(None, description=(\n        \'If group_uris is set, this can be used to specify the number of chips to use \'\n        \'per group.\'))\n    data_format: Optional[str] = Field(\n        None, description=\'Name of dataset format.\')\n    class_names: List[str] = Field([], description=\'Names of classes.\')\n    class_colors: Union[None, List[str], List[Tuple]] = Field(\n        None, description=\'Colors used to display classes.\')\n    img_sz: PositiveInt = Field(\n        256,\n        description=\n        (\'Length of a side of each image in pixels. This is the size to transform \'\n         \'it to during training, not the size in the raw dataset.\'))\n    num_workers: int = Field(\n        4,\n        description=\'Number of workers to use when DataLoader makes batches.\')\n    # TODO support setting parameters of augmentors?\n    augmentors: List[str] = Field(\n        default_augmentors,\n        description=(\n            \'Names of albumentations augmentors to use for training batches. \'\n            \'Choices include: \' + str(augmentors)))\n\n    def update(self, learner: Optional[\'LearnerConfig\'] = None):\n        if not self.class_colors:\n            self.class_colors = [color_to_triple() for _ in self.class_names]\n\n    def validate_augmentors(self):\n        self.validate_list(\'augmentors\', augmentors)\n\n    def validate_config(self):\n        self.validate_augmentors()\n\n\n@register_config(\'learner\')\nclass LearnerConfig(Config):\n    """"""Config for Learner.""""""\n    model: ModelConfig\n    solver: SolverConfig\n    data: DataConfig\n\n    predict_mode: bool = Field(\n        False,\n        description=\'If True, skips training, loads model, and does final eval.\'\n    )\n    test_mode: bool = Field(\n        False,\n        description=\n        (\'If True, uses test_num_epochs, test_batch_sz, truncated datasets with \'\n         \'only a single batch, image_sz that is cut in half, and num_workers = 0. \'\n         \'This is useful for testing that code runs correctly on CPU without \'\n         \'multithreading before running full job on GPU.\'))\n    overfit_mode: bool = Field(\n        False,\n        description=\n        (\'If True, uses half image size, and instead of doing epoch-based training, \'\n         \'optimizes the model using a single batch repeatedly for \'\n         \'overfit_num_steps number of steps.\'))\n    eval_train: bool = Field(\n        False,\n        description=\n        (\'If True, runs final evaluation on training set (in addition to test set). \'\n         \'Useful for debugging.\'))\n    save_model_bundle: bool = Field(\n        True,\n        description=\n        (\'If True, saves a model bundle at the end of training which \'\n         \'is zip file with model and this LearnerConfig which can be used to make \'\n         \'predictions on new images at a later time.\'))\n    log_tensorboard: bool = Field(\n        True,\n        description=\'Save Tensorboard log files at the end of each epoch.\')\n    run_tensorboard: bool = Field(\n        False, description=\'run Tensorboard server during training\')\n    output_uri: Optional[str] = Field(\n        None, description=\'URI of where to save output\')\n\n    def update(self):\n        super().update()\n\n        if self.overfit_mode:\n            self.data.img_sz = self.data.img_sz // 2\n            if self.test_mode:\n                self.solver.overfit_num_steps = self.solver.test_overfit_num_steps\n\n        if self.test_mode:\n            self.solver.num_epochs = self.solver.test_num_epochs\n            self.solver.batch_sz = self.solver.test_batch_sz\n            self.data.img_sz = self.data.img_sz // 2\n            self.data.num_workers = 0\n\n        self.model.update(learner=self)\n        self.solver.update(learner=self)\n        self.data.update(learner=self)\n\n    def validate_config(self):\n        if self.run_tensorboard and not self.log_tensorboard:\n            raise ConfigError(\n                \'Cannot run_tensorboard if log_tensorboard is False\')\n\n    def build(self, tmp_dir: str,\n              model_path: Optional[str] = None) -> \'Learner\':\n        """"""Returns a Learner instantiated using this Config.\n\n        Args:\n            tmp_dir: root of temp dirs\n            model_path: local path to model weights. If this is passed, the Learner\n                is assumed to be used to make predictions and not train a model.\n        """"""\n        raise NotImplementedError()\n\n    def get_model_bundle_uri(self) -> str:\n        """"""Returns the URI of where the model bundel is stored.""""""\n        return join(self.output_uri, \'model-bundle.zip\')\n'"
rastervision2/pytorch_learner/learner_pipeline.py,0,"b'from rastervision2.pipeline.pipeline import Pipeline\nfrom rastervision2.pytorch_learner import LearnerConfig\n\n\nclass LearnerPipeline(Pipeline):\n    """"""Simple Pipeline that is a wrapper around Learner.main()\n\n    This supports the ability to use the pytorch_learner package to train models using\n    the RV pipeline package and its runner functionality without the rest of RV.\n    """"""\n    commands = [\'train\']\n    gpu_commands = [\'train\']\n\n    def train(self):\n        learner_cfg: LearnerConfig = self.config.learner\n        learner = learner_cfg.build(learner_cfg, self.tmp_dir)\n        learner.main()\n'"
rastervision2/pytorch_learner/learner_pipeline_config.py,0,"b""from rastervision2.pipeline.config import register_config\nfrom rastervision2.pipeline.pipeline_config import PipelineConfig\nfrom rastervision2.pytorch_learner import LearnerConfig\n\n\n@register_config('learner_pipeline')\nclass LearnerPipelineConfig(PipelineConfig):\n    learner: LearnerConfig\n\n    def update(self):\n        super().update()\n\n        if self.learner.output_uri is None:\n            self.learner.output_uri = self.root_uri\n\n        self.learner.update()\n\n    def build(self, tmp_dir):\n        from rastervision2.pytorch_learner.learner_pipeline import LearnerPipeline\n        return LearnerPipeline(self, tmp_dir)\n"""
rastervision2/pytorch_learner/object_detection_learner.py,1,"b""import warnings\nwarnings.filterwarnings('ignore')  # noqa\nfrom os.path import join, isdir\nimport logging\n\nimport matplotlib\nmatplotlib.use('Agg')  # noqa\nfrom torch.utils.data import ConcatDataset\nfrom albumentations import BboxParams\n\nfrom rastervision2.pytorch_learner.learner import Learner\nfrom rastervision2.pytorch_learner.object_detection_utils import (\n    MyFasterRCNN, CocoDataset, compute_coco_eval, collate_fn, plot_xyz)\nfrom rastervision2.pytorch_learner.object_detection_learner_config import (\n    ObjectDetectionDataFormat)\n\nlog = logging.getLogger(__name__)\n\n\nclass ObjectDetectionLearner(Learner):\n    def build_model(self):\n        # TODO we shouldn't need to pass the image size here\n        pretrained = self.cfg.model.pretrained\n        model = MyFasterRCNN(\n            self.cfg.model.get_backbone_str(),\n            len(self.cfg.data.class_names),\n            self.cfg.data.img_sz,\n            pretrained=pretrained)\n        return model\n\n    def build_metric_names(self):\n        metric_names = [\n            'epoch', 'train_time', 'valid_time', 'train_loss', 'map', 'map50'\n        ]\n        return metric_names\n\n    def get_bbox_params(self):\n        return BboxParams(format='coco', label_fields=['category_id'])\n\n    def get_collate_fn(self):\n        return collate_fn\n\n    def _get_datasets(self, uri):\n        cfg = self.cfg\n\n        if cfg.data.data_format == ObjectDetectionDataFormat.default:\n            data_dirs = self.unzip_data(uri)\n\n        transform, aug_transform = self.get_data_transforms()\n\n        train_ds, valid_ds, test_ds = [], [], []\n        for data_dir in data_dirs:\n            train_dir = join(data_dir, 'train')\n            valid_dir = join(data_dir, 'valid')\n\n            if isdir(train_dir):\n                img_dir = join(train_dir, 'img')\n                annotation_uri = join(train_dir, 'labels.json')\n                if cfg.overfit_mode:\n                    train_ds.append(\n                        CocoDataset(\n                            img_dir, annotation_uri, transform=transform))\n                else:\n                    train_ds.append(\n                        CocoDataset(\n                            img_dir, annotation_uri, transform=aug_transform))\n\n            if isdir(valid_dir):\n                img_dir = join(valid_dir, 'img')\n                annotation_uri = join(valid_dir, 'labels.json')\n                valid_ds.append(\n                    CocoDataset(img_dir, annotation_uri, transform=transform))\n                test_ds.append(\n                    CocoDataset(img_dir, annotation_uri, transform=transform))\n\n        train_ds, valid_ds, test_ds = \\\n            ConcatDataset(train_ds), ConcatDataset(valid_ds), ConcatDataset(test_ds)\n\n        return train_ds, valid_ds, test_ds\n\n    def train_step(self, batch, batch_ind):\n        x, y = batch\n        loss_dict = self.model(x, y)\n        return {'train_loss': loss_dict['total_loss']}\n\n    def validate_step(self, batch, batch_ind):\n        x, y = batch\n        outs = self.model(x)\n        ys = self.to_device(y, 'cpu')\n        outs = self.to_device(outs, 'cpu')\n\n        return {'ys': ys, 'outs': outs}\n\n    def validate_end(self, outputs, num_samples):\n        outs = []\n        ys = []\n        for o in outputs:\n            outs.extend(o['outs'])\n            ys.extend(o['ys'])\n        num_class_ids = len(self.cfg.data.class_names)\n        coco_eval = compute_coco_eval(outs, ys, num_class_ids)\n\n        metrics = {'map': 0.0, 'map50': 0.0}\n        if coco_eval is not None:\n            coco_metrics = coco_eval.stats\n            metrics = {'map': coco_metrics[0], 'map50': coco_metrics[1]}\n        return metrics\n\n    def output_to_numpy(self, out):\n        numpy_out = []\n        for boxlist in out:\n            boxlist = boxlist.cpu()\n            numpy_out.append({\n                'boxes':\n                boxlist.boxes.numpy(),\n                'class_ids':\n                boxlist.get_field('class_ids').numpy(),\n                'scores':\n                boxlist.get_field('scores').numpy()\n            })\n        return numpy_out\n\n    def plot_xyz(self, ax, x, y, z=None):\n        plot_xyz(ax, x, y, self.cfg.data.class_names, z=z)\n\n    def prob_to_pred(self, x):\n        return x\n"""
rastervision2/pytorch_learner/object_detection_learner_config.py,0,"b""from enum import Enum\n\nfrom rastervision2.pipeline.config import register_config, Field, validator\nfrom rastervision2.pytorch_learner.learner_config import (\n    LearnerConfig, DataConfig, ModelConfig, Backbone)\n\n\nclass ObjectDetectionDataFormat(Enum):\n    default = 1\n\n\n@register_config('object_detection_data')\nclass ObjectDetectionDataConfig(DataConfig):\n    data_format: ObjectDetectionDataFormat = ObjectDetectionDataFormat.default\n\n\n@register_config('object_detection_model')\nclass ObjectDetectionModelConfig(ModelConfig):\n    backbone: Backbone = Field(\n        Backbone.resnet50,\n        description=\n        ('The torchvision.models backbone to use, which must be in the resnet* '\n         'family.'))\n\n    @validator('backbone')\n    def only_valid_backbones(cls, v):\n        if v not in [\n                Backbone.resnet18, Backbone.resnet34, Backbone.resnet50,\n                Backbone.resnet101, Backbone.resnet152\n        ]:\n            raise ValueError(\n                'The backbone for Faster-RCNN must be in the resnet* '\n                'family.')\n        return v\n\n\n@register_config('object_detection_learner')\nclass ObjectDetectionLearnerConfig(LearnerConfig):\n    data: ObjectDetectionDataConfig\n    model: ObjectDetectionModelConfig\n\n    def build(self, tmp_dir, model_path=None):\n        from rastervision2.pytorch_learner.object_detection_learner import (\n            ObjectDetectionLearner)\n        return ObjectDetectionLearner(self, tmp_dir, model_path=model_path)\n"""
rastervision2/pytorch_learner/object_detection_utils.py,25,"b'from collections import defaultdict\nfrom os.path import join\nimport tempfile\n\nimport torch\nfrom torch.utils.data import Dataset\nimport torch.nn as nn\nfrom torchvision.ops.boxes import batched_nms\nfrom torchvision.models.detection.faster_rcnn import FasterRCNN\nfrom torchvision.models.detection.backbone_utils import BackboneWithFPN\nfrom torchvision.models import resnet\nfrom torchvision.ops import misc as misc_nn_ops\nimport pycocotools\nfrom pycocotools.coco import COCO\nfrom pycocotools.cocoeval import COCOeval\nimport numpy as np\nfrom PIL import Image\nimport matplotlib.patches as patches\n\nfrom rastervision2.pipeline.file_system import file_to_json, json_to_file\n\n\ndef get_coco_gt(targets, num_class_ids):\n    images = []\n    annotations = []\n    ann_id = 1\n    for img_id, target in enumerate(targets, 1):\n        # Use fake height, width, and filename because they don\'t matter.\n        images.append({\n            \'id\': img_id,\n            \'height\': 1000,\n            \'width\': 1000,\n            \'file_name\': \'{}.png\'.format(img_id)\n        })\n        boxes, class_ids = target.boxes, target.get_field(\'class_ids\')\n        for box, class_id in zip(boxes, class_ids):\n            box = box.float().tolist()\n            class_id = class_id.item()\n            annotations.append({\n                \'id\':\n                ann_id,\n                \'image_id\':\n                img_id,\n                \'category_id\':\n                class_id + 1,\n                \'area\': (box[2] - box[0]) * (box[3] - box[1]),\n                \'bbox\': [box[1], box[0], box[3] - box[1], box[2] - box[0]],\n                \'iscrowd\':\n                0\n            })\n            ann_id += 1\n\n    categories = [{\n        \'id\': class_id + 1,\n        \'name\': str(class_id + 1),\n        \'supercategory\': \'super\'\n    } for class_id in range(num_class_ids)]\n    coco = {\n        \'images\': images,\n        \'annotations\': annotations,\n        \'categories\': categories\n    }\n    return coco\n\n\ndef get_coco_preds(outputs):\n    preds = []\n    for img_id, output in enumerate(outputs, 1):\n        for box, class_id, score in zip(output.boxes,\n                                        output.get_field(\'class_ids\'),\n                                        output.get_field(\'scores\')):\n            box = box.float().tolist()\n            class_id = class_id.item() + 1\n            score = score.item()\n            preds.append({\n                \'image_id\':\n                img_id,\n                \'category_id\':\n                class_id,\n                \'bbox\': [box[1], box[0], box[3] - box[1], box[2] - box[0]],\n                \'score\':\n                score\n            })\n    return preds\n\n\ndef compute_coco_eval(outputs, targets, num_class_ids):\n    """"""Return mAP averaged over 0.5-0.95 using pycocotools eval.\n\n    Note: boxes are in (ymin, xmin, ymax, xmax) format with values ranging\n        from 0 to h or w.\n\n    Args:\n        outputs: (list) of length m containing dicts of form\n            {\'boxes\': <tensor with shape (n, 4)>,\n             \'class_ids\': <tensor with shape (n,)>,\n             \'scores\': <tensor with shape (n,)>}\n        targets: (list) of length m containing dicts of form\n            {\'boxes\': <tensor with shape (n, 4)>,\n             \'class_ids\': <tensor with shape (n,)>}\n    """"""\n    with tempfile.TemporaryDirectory() as tmp_dir:\n        preds = get_coco_preds(outputs)\n        # ap is undefined when there are no predicted boxes\n        if len(preds) == 0:\n            return None\n\n        gt = get_coco_gt(targets, num_class_ids)\n        gt_path = join(tmp_dir, \'gt.json\')\n        json_to_file(gt, gt_path)\n        coco_gt = COCO(gt_path)\n\n        pycocotools.coco.unicode = None\n        coco_preds = coco_gt.loadRes(preds)\n\n        ann_type = \'bbox\'\n        coco_eval = COCOeval(coco_gt, coco_preds, ann_type)\n\n        coco_eval.evaluate()\n        coco_eval.accumulate()\n        coco_eval.summarize()\n\n        return coco_eval\n\n\ndef to_box_pixel(boxes, img_height, img_width):\n    # convert from (ymin, xmin, ymax, xmax) in range [-1,1] to\n    # range [0, h) or [0, w)\n    boxes = ((boxes + 1.0) / 2.0) * torch.tensor(\n        [[img_height, img_width, img_height, img_width]]).to(\n            device=boxes.device, dtype=torch.float)\n    return boxes\n\n\nclass BoxList():\n    def __init__(self, boxes, **extras):\n        """"""Constructor.\n\n        Args:\n            boxes: tensor<n, 4> with order ymin, xmin, ymax, xmax in pixels coords\n            extras: dict with values that are tensors with first dimension corresponding\n                to boxes first dimension\n        """"""\n        self.boxes = boxes\n        self.extras = extras\n\n    def get_field(self, name):\n        if name == \'boxes\':\n            return self.boxes\n        else:\n            return self.extras.get(name)\n\n    def _map_extras(self, func):\n        new_extras = {}\n        for k, v in self.extras.items():\n            new_extras[k] = func(v)\n        return new_extras\n\n    def copy(self):\n        return BoxList(self.boxes.copy(),\n                       **self._map_extras(lambda x: x.copy()))\n\n    def cpu(self):\n        return BoxList(self.boxes.cpu(), **self._map_extras(lambda x: x.cpu()))\n\n    def cuda(self):\n        return BoxList(self.boxes.cuda(),\n                       **self._map_extras(lambda x: x.cuda()))\n\n    def to(self, device):\n        return self.cpu() if device == \'cpu\' else self.cuda()\n\n    def xyxy(self):\n        boxes = self.boxes[:, [1, 0, 3, 2]]\n        return BoxList(boxes, **self.extras)\n\n    def yxyx(self):\n        boxes = self.boxes[:, [1, 0, 3, 2]]\n        return BoxList(boxes, **self.extras)\n\n    def __len__(self):\n        return self.boxes.shape[0]\n\n    @staticmethod\n    def cat(box_lists):\n        boxes = []\n        extras = defaultdict(list)\n        for bl in box_lists:\n            boxes.append(bl.boxes)\n            for k, v in bl.extras.items():\n                extras[k].append(v)\n        boxes = torch.cat(boxes)\n        for k, v in extras.items():\n            extras[k] = torch.cat(v)\n        return BoxList(boxes, **extras)\n\n    def equal(self, other):\n        if len(other) != len(self):\n            return False\n\n        # Ignore order of boxes.\n        extras = [(v.float().unsqueeze(1) if v.ndim == 1 else v.float())\n                  for v in self.extras.values()]\n        cat_arr = torch.cat([self.boxes] + extras, 1)\n        self_tups = set([tuple([x.item() for x in row]) for row in cat_arr])\n\n        extras = [(v.float().unsqueeze(1) if v.ndim == 1 else v.float())\n                  for v in other.extras.values()]\n        cat_arr = torch.cat([other.boxes] + extras, 1)\n        other_tups = set([tuple([x.item() for x in row]) for row in cat_arr])\n        return self_tups == other_tups\n\n    def ind_filter(self, inds):\n        new_extras = {}\n        for k, v in self.extras.items():\n            new_extras[k] = v[inds, ...]\n        return BoxList(self.boxes[inds, :], **new_extras)\n\n    def score_filter(self, score_thresh=0.25):\n        scores = self.extras.get(\'scores\')\n        if scores is not None:\n            return self.ind_filter(scores > score_thresh)\n        else:\n            raise ValueError(\'must have scores as key in extras\')\n\n    def clamp(self, img_height, img_width):\n        boxes = torch.stack(\n            [\n                torch.clamp(self.boxes[:, 0], 0, img_height),\n                torch.clamp(self.boxes[:, 1], 0, img_width),\n                torch.clamp(self.boxes[:, 2], 0, img_height),\n                torch.clamp(self.boxes[:, 3], 0, img_width)\n            ],\n            dim=1)\n        return BoxList(boxes, **self.extras)\n\n    def nms(self, iou_thresh=0.5):\n        if len(self) == 0:\n            return self\n\n        good_inds = batched_nms(self.boxes, self.get_field(\'scores\'),\n                                self.get_field(\'class_ids\'), iou_thresh)\n        return self.ind_filter(good_inds)\n\n    def scale(self, yscale, xscale):\n        boxes = self.boxes * torch.tensor(\n            [[yscale, xscale, yscale, xscale]], device=self.boxes.device)\n        return BoxList(boxes, **self.extras)\n\n    def pin_memory(self):\n        self.boxes = self.boxes.pin_memory()\n        for k, v in self.extras.items():\n            self.extras[k] = v.pin_memory()\n        return self\n\n\ndef collate_fn(data):\n    x = [d[0].unsqueeze(0) for d in data]\n    y = [d[1] for d in data]\n    return (torch.cat(x), y)\n\n\nclass CocoDataset(Dataset):\n    def __init__(self, img_dir, annotation_uri, transform=None):\n        self.img_dir = img_dir\n        self.annotation_uri = annotation_uri\n        self.transform = transform\n\n        self.img_ids = []\n        self.id2ann = {}\n        ann_json = file_to_json(annotation_uri)\n\n        for img in ann_json[\'images\']:\n            img_id = img[\'id\']\n            self.img_ids.append(img_id)\n            self.id2ann[img_id] = {\n                \'image\': img[\'file_name\'],\n                \'bboxes\': [],\n                \'category_id\': []\n            }\n        for ann in ann_json[\'annotations\']:\n            img_id = ann[\'image_id\']\n            bboxes = self.id2ann[img_id][\'bboxes\']\n            category_ids = self.id2ann[img_id][\'category_id\']\n            bboxes.append(ann[\'bbox\'])\n            category_ids.append(ann[\'category_id\'])\n\n    def __getitem__(self, ind):\n        img_id = self.img_ids[ind]\n        ann = dict(self.id2ann[img_id])\n\n        img_fn = ann[\'image\']\n        img = Image.open(join(self.img_dir, img_fn))\n\n        if self.transform:\n            ann[\'image\'] = np.array(img)\n            out = self.transform(**ann)\n\n            x = out[\'image\']\n            x = torch.tensor(x).permute(2, 0, 1).float() / 255.0\n\n            b = torch.tensor(out[\'bboxes\'])\n            if b.shape[0] == 0:\n                y = BoxList(\n                    torch.empty((0, 4)), class_ids=torch.empty((0, )).long())\n            else:\n                boxes = torch.cat(\n                    [\n                        b[:, 1:2], b[:, 0:1], b[:, 1:2] + b[:, 3:4],\n                        b[:, 0:1] + b[:, 2:3]\n                    ],\n                    dim=1)\n                class_ids = torch.tensor(out[\'category_id\'])\n                y = BoxList(boxes, class_ids=class_ids)\n        else:\n            # TODO\n            pass\n\n        return (x, y)\n\n    def __len__(self):\n        return len(self.id2ann)\n\n\ndef get_out_channels(model):\n    out = {}\n\n    def make_save_output(layer_name):\n        def save_output(layer, input, output):\n            out[layer_name] = output.shape[1]\n\n        return save_output\n\n    model.layer1.register_forward_hook(make_save_output(\'layer1\'))\n    model.layer2.register_forward_hook(make_save_output(\'layer2\'))\n    model.layer3.register_forward_hook(make_save_output(\'layer3\'))\n    model.layer4.register_forward_hook(make_save_output(\'layer4\'))\n\n    model(torch.empty((1, 3, 128, 128)))\n    return [out[\'layer1\'], out[\'layer2\'], out[\'layer3\'], out[\'layer4\']]\n\n\n# This fixes a bug in torchvision.\ndef resnet_fpn_backbone(backbone_name, pretrained):\n    backbone = resnet.__dict__[backbone_name](\n        pretrained=pretrained, norm_layer=misc_nn_ops.FrozenBatchNorm2d)\n\n    # freeze layers\n    for name, parameter in backbone.named_parameters():\n        if \'layer2\' not in name and \'layer3\' not in name and \'layer4\' not in name:\n            parameter.requires_grad_(False)\n\n    return_layers = {\'layer1\': 0, \'layer2\': 1, \'layer3\': 2, \'layer4\': 3}\n\n    out_channels = 256\n    in_channels_list = get_out_channels(backbone)\n    return BackboneWithFPN(backbone, return_layers, in_channels_list,\n                           out_channels)\n\n\ndef plot_xyz(ax, x, y, class_names, z=None):\n    ax.imshow(x.permute(1, 2, 0))\n    y = y if z is None else z\n\n    scores = y.get_field(\'scores\')\n    for box_ind, (box, class_id) in enumerate(\n            zip(y.boxes, y.get_field(\'class_ids\'))):\n        rect = patches.Rectangle(\n            (box[1], box[0]),\n            box[3] - box[1],\n            box[2] - box[0],\n            linewidth=1,\n            edgecolor=\'cyan\',\n            facecolor=\'none\')\n        ax.add_patch(rect)\n\n        box_label = class_names[class_id]\n        if scores is not None:\n            score = scores[box_ind]\n            box_label += \' {:.2f}\'.format(score)\n\n        h, w = x.shape[1:]\n        label_height = h * 0.03\n        label_width = w * 0.15\n        rect = patches.Rectangle(\n            (box[1], box[0] - label_height),\n            label_width,\n            label_height,\n            color=\'cyan\')\n        ax.add_patch(rect)\n\n        ax.text(box[1] + w * 0.003, box[0] - h * 0.003, box_label, fontsize=7)\n\n    ax.axis(\'off\')\n\n\nclass MyFasterRCNN(nn.Module):\n    """"""Adapter around torchvision Faster-RCNN.\n\n    The purpose of the adapter is to use a different input and output format\n    and inject bogus boxes to circumvent torchvision\'s inability to handle\n    training examples with no ground truth boxes.\n    """"""\n\n    def __init__(self, backbone_arch, num_class_ids, img_sz, pretrained=True):\n        super().__init__()\n\n        backbone = resnet_fpn_backbone(backbone_arch, pretrained)\n        # Add an extra null class for the bogus boxes.\n        self.null_class_id = num_class_ids\n\n        # class_ids must start at 1, and there is an extra null class, so\n        # that\'s why we add 2 to the num_class_ids\n        self.model = FasterRCNN(\n            backbone, num_class_ids + 2, min_size=img_sz, max_size=img_sz)\n        self.subloss_names = [\n            \'total_loss\', \'loss_box_reg\', \'loss_classifier\', \'loss_objectness\',\n            \'loss_rpn_box_reg\'\n        ]\n        self.batch_ind = 0\n\n    def forward(self, input, targets=None):\n        """"""Forward pass\n\n        Args:\n            input: tensor<n, 3, h, w> with batch of images\n            targets: None or list<BoxList> of length n with boxes and class_ids\n\n        Returns:\n            if targets is None, returns list<BoxList> of length n, containing\n            boxes, class_ids, and scores for boxes with score > 0.05. Further\n            filtering based on score should be done before considering the\n            prediction ""final"".\n\n            if targets is a list, returns the losses as dict with keys from\n            self.subloss_names.\n        """"""\n        if targets:\n            # Add bogus background class box for each image to workaround\n            # the inability of torchvision to train on images with\n            # no ground truth boxes. This is important for being able\n            # to handle negative chips generated by RV.\n            # See https://github.com/pytorch/vision/issues/1598\n\n            # Note class_ids must start at 1.\n            new_targets = []\n            for x, y in zip(input, targets):\n                h, w = x.shape[1:]\n                boxes = torch.cat(\n                    [\n                        y.boxes,\n                        torch.tensor([[0., 0, h, w]], device=input.device)\n                    ],\n                    dim=0)\n                class_ids = torch.cat(\n                    [\n                        y.get_field(\'class_ids\') + 1,\n                        torch.tensor(\n                            [self.null_class_id + 1], device=input.device),\n                    ],\n                    dim=0)\n                bl = BoxList(boxes, class_ids=class_ids)\n                new_targets.append(bl)\n            targets = new_targets\n\n            _targets = [bl.xyxy() for bl in targets]\n            _targets = [{\n                \'boxes\': bl.boxes,\n                \'labels\': bl.get_field(\'class_ids\')\n            } for bl in _targets]\n            loss_dict = self.model(input, _targets)\n            loss_dict[\'total_loss\'] = sum(list(loss_dict.values()))\n\n            return loss_dict\n\n        out = self.model(input)\n        boxlists = [\n            BoxList(\n                _out[\'boxes\'], class_ids=_out[\'labels\'],\n                scores=_out[\'scores\']).yxyx() for _out in out\n        ]\n\n        # Remove bogus background boxes.\n        new_boxlists = []\n        for bl in boxlists:\n            class_ids = bl.get_field(\'class_ids\') - 1\n            non_null_inds = class_ids != self.null_class_id\n            bl = bl.ind_filter(non_null_inds)\n            bl.extras[\'class_ids\'] -= 1\n            new_boxlists.append(bl)\n        return new_boxlists\n'"
rastervision2/pytorch_learner/regression_learner.py,9,"b""import warnings\nwarnings.filterwarnings('ignore')  # noqa\nfrom os.path import join, isdir\nimport csv\n\nimport matplotlib\nmatplotlib.use('Agg')  # noqa\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\n\nimport torch\nfrom torchvision import models\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, ConcatDataset\nfrom PIL import Image\n\nfrom rastervision2.pytorch_learner.learner import Learner\nfrom rastervision2.pytorch_learner.utils import AlbumentationsDataset\n\n\nclass ImageRegressionDataset(Dataset):\n    def __init__(self, data_dir, class_names, transform=None):\n        self.data_dir = data_dir\n        self.transform = transform\n\n        labels_path = join(data_dir, 'labels.csv')\n        with open(labels_path, 'r') as labels_file:\n            labels_reader = csv.reader(labels_file, skipinitialspace=True)\n            header = next(labels_reader)\n            self.output_inds = [header.index(col) for col in class_names]\n            self.labels = list(labels_reader)[1:]\n        self.img_dir = join(data_dir, 'img')\n\n    def __getitem__(self, ind):\n        label_row = self.labels[ind]\n        img_fn = label_row[0]\n\n        y = [float(label_row[i]) for i in self.output_inds]\n        y = torch.tensor(y).float()\n        img = Image.open(join(self.img_dir, img_fn))\n        if self.transform:\n            img = self.transform(img)\n        return (img, y)\n\n    def __len__(self):\n        return len(self.labels)\n\n\nclass RegressionModel(nn.Module):\n    def __init__(self,\n                 backbone_arch,\n                 out_features,\n                 pretrained=True,\n                 pos_out_inds=None):\n        super().__init__()\n        self.backbone = getattr(models, backbone_arch)(pretrained=pretrained)\n        in_features = self.backbone.fc.in_features\n        self.backbone.fc = nn.Linear(in_features, out_features)\n        self.pos_out_inds = pos_out_inds\n\n    def forward(self, x):\n        out = self.backbone(x)\n        if self.pos_out_inds:\n            for ind in self.pos_out_inds:\n                out[:, ind] = out[:, ind].exp()\n        return out\n\n\nclass RegressionLearner(Learner):\n    def build_model(self):\n        pretrained = self.cfg.model.pretrained\n        backbone = self.cfg.model.get_backbone_str()\n        out_features = len(self.cfg.data.class_names)\n        pos_out_inds = [\n            self.cfg.data.class_names.index(l)\n            for l in self.cfg.data.pos_class_names\n        ]\n        model = RegressionModel(\n            backbone,\n            out_features,\n            pretrained=pretrained,\n            pos_out_inds=pos_out_inds)\n        return model\n\n    def _get_datasets(self, uri):\n        cfg = self.cfg\n        data_dirs = self.unzip_data(uri)\n        transform, aug_transform = self.get_data_transforms()\n\n        train_ds, valid_ds, test_ds = [], [], []\n        for data_dir in data_dirs:\n            train_dir = join(data_dir, 'train')\n            valid_dir = join(data_dir, 'valid')\n\n            if isdir(train_dir):\n                if cfg.overfit_mode:\n                    train_ds.append(\n                        AlbumentationsDataset(\n                            ImageRegressionDataset(train_dir,\n                                                   cfg.data.class_names),\n                            transform=transform))\n                else:\n                    train_ds.append(\n                        AlbumentationsDataset(\n                            ImageRegressionDataset(train_dir,\n                                                   cfg.data.class_names),\n                            transform=aug_transform))\n\n            if isdir(valid_dir):\n                valid_ds.append(\n                    AlbumentationsDataset(\n                        ImageRegressionDataset(valid_dir,\n                                               cfg.data.class_names),\n                        transform=transform))\n                test_ds.append(\n                    AlbumentationsDataset(\n                        ImageRegressionDataset(valid_dir,\n                                               cfg.data.class_names),\n                        transform=transform))\n\n        train_ds, valid_ds, test_ds = \\\n            ConcatDataset(train_ds), ConcatDataset(valid_ds), ConcatDataset(test_ds)\n\n        return train_ds, valid_ds, test_ds\n\n    def on_overfit_start(self):\n        self.on_train_start()\n\n    def on_train_start(self):\n        ys = []\n        for _, y in self.train_dl:\n            ys.append(y)\n        y = torch.cat(ys, dim=0)\n        self.target_medians = y.median(dim=0).values.to(self.device)\n\n    def build_metric_names(self):\n        metric_names = [\n            'epoch', 'train_time', 'valid_time', 'train_loss', 'val_loss'\n        ]\n        for label in self.cfg.data.class_names:\n            metric_names.extend([\n                '{}_abs_error'.format(label),\n                '{}_scaled_abs_error'.format(label)\n            ])\n        return metric_names\n\n    def train_step(self, batch, batch_ind):\n        x, y = batch\n        out = self.post_forward(self.model(x))\n        return {'train_loss': F.l1_loss(out, y, reduction='sum')}\n\n    def validate_step(self, batch, batch_nb):\n        x, y = batch\n        out = self.post_forward(self.model(x))\n        val_loss = F.l1_loss(out, y, reduction='sum')\n        abs_error = torch.abs(out - y).sum(dim=0)\n        scaled_abs_error = (\n            torch.abs(out - y) / self.target_medians).sum(dim=0)\n\n        metrics = {'val_loss': val_loss}\n        for ind, label in enumerate(self.cfg.data.class_names):\n            metrics['{}_abs_error'.format(label)] = abs_error[ind]\n            metrics['{}_scaled_abs_error'.format(label)] = scaled_abs_error[\n                ind]\n\n        return metrics\n\n    def prob_to_pred(self, x):\n        return x\n\n    def plot_xyz(self, ax, x, y, z=None):\n        x = x.permute(1, 2, 0)\n        if x.shape[2] == 1:\n            x = torch.cat([x for _ in range(3)], dim=2)\n        ax.imshow(x)\n\n        title = 'true: '\n        for _y in y:\n            title += '{:.2f} '.format(_y)\n        if z is not None:\n            title += '\\npred: '\n            for _z in z:\n                title += '{:.2f} '.format(_z)\n        ax.set_title(title)\n        ax.axis('off')\n\n    def eval_model(self, split):\n        super().eval_model(split)\n\n        y, out = self.predict_dataloader(\n            self.get_dataloader(split), return_x=False)\n        # make scatter plot\n        num_labels = len(self.cfg.data.class_names)\n        ncols = num_labels\n        nrows = 1\n        fig = plt.figure(\n            constrained_layout=True, figsize=(5 * ncols, 5 * nrows))\n        grid = gridspec.GridSpec(ncols=ncols, nrows=nrows, figure=fig)\n\n        for label_ind, label in enumerate(self.cfg.data.class_names):\n            ax = fig.add_subplot(grid[label_ind])\n            ax.scatter(\n                y[:, label_ind], out[:, label_ind], c='blue', alpha=0.02)\n            ax.set_title('{} on {} set'.format(label, split))\n            ax.set_xlabel('ground truth')\n            ax.set_ylabel('predictions')\n        scatter_path = join(self.output_dir, '{}_scatter.png'.format(split))\n        plt.savefig(scatter_path)\n\n        # make histogram of errors\n        fig = plt.figure(\n            constrained_layout=True, figsize=(5 * ncols, 5 * nrows))\n        grid = gridspec.GridSpec(ncols=ncols, nrows=nrows, figure=fig)\n\n        for label_ind, label in enumerate(self.cfg.data.class_names):\n            ax = fig.add_subplot(grid[label_ind])\n            errs = torch.abs(y[:, label_ind] - out[:, label_ind])\n            ax.hist(errs, bins=100, density=True)\n            ax.set_title('{} on {} set'.format(label, split))\n            ax.set_xlabel('prediction error')\n        scatter_path = join(self.output_dir, '{}_err_hist.png'.format(split))\n        plt.savefig(scatter_path)\n"""
rastervision2/pytorch_learner/regression_learner_config.py,0,"b""from typing import List\nfrom enum import Enum\n\nfrom rastervision2.pipeline.config import register_config\nfrom rastervision2.pytorch_learner.learner_config import (\n    LearnerConfig, DataConfig, ModelConfig)\n\n\nclass RegressionDataFormat(Enum):\n    csv = 1\n\n\n@register_config('regression_data')\nclass RegressionDataConfig(DataConfig):\n    pos_class_names: List[str] = []\n    data_format: RegressionDataFormat = RegressionDataFormat.csv\n\n\n@register_config('regression_model')\nclass RegressionModelConfig(ModelConfig):\n    output_multiplier: List[float] = None\n\n    def update(self, learner=None):\n        if learner is not None and self.output_multiplier is None:\n            self.output_multiplier = [1.0] * len(learner.data.class_names)\n\n\n@register_config('regression_learner')\nclass RegressionLearnerConfig(LearnerConfig):\n    model: RegressionModelConfig\n    data: RegressionDataConfig\n\n    def build(self, tmp_dir, model_path=None):\n        from rastervision2.pytorch_learner.regression_learner import (\n            RegressionLearner)\n        return RegressionLearner(self, tmp_dir, model_path=model_path)\n"""
rastervision2/pytorch_learner/semantic_segmentation_learner.py,6,"b""import warnings\nwarnings.filterwarnings('ignore')  # noqa\nfrom os.path import join, isdir, basename\nimport logging\nimport glob\n\nimport numpy as np\nimport matplotlib\nmatplotlib.use('Agg')  # noqa\nimport torch\nfrom torch.utils.data import Dataset, ConcatDataset\nimport torch.nn.functional as F\nfrom torchvision import models\nfrom PIL import Image\n\nfrom rastervision2.pytorch_learner.learner import Learner\nfrom rastervision2.pytorch_learner.utils import (\n    compute_conf_mat_metrics, compute_conf_mat, color_to_triple)\n\nlog = logging.getLogger(__name__)\n\n\nclass SemanticSegmentationDataset(Dataset):\n    def __init__(self, data_dir, transform=None):\n        self.data_dir = data_dir\n        self.img_paths = glob.glob(join(data_dir, 'img', '*.png'))\n        self.transform = transform\n\n    def __getitem__(self, ind):\n        img_path = self.img_paths[ind]\n        label_path = join(self.data_dir, 'labels', basename(img_path))\n        x = Image.open(img_path)\n        y = Image.open(label_path)\n\n        x = np.array(x)\n        y = np.array(y)\n        if self.transform is not None:\n            out = self.transform(image=x, mask=y)\n            x = out['image']\n            y = out['mask']\n\n        x = torch.tensor(x).permute(2, 0, 1).float() / 255.0\n        y = torch.tensor(y).long()\n\n        return (x, y)\n\n    def __len__(self):\n        return len(self.img_paths)\n\n\nclass SemanticSegmentationLearner(Learner):\n    def build_model(self):\n        # TODO support FCN option\n        pretrained = self.cfg.model.pretrained\n        model = models.segmentation.segmentation._segm_resnet(\n            'deeplabv3',\n            self.cfg.model.get_backbone_str(),\n            len(self.cfg.data.class_names),\n            False,\n            pretrained_backbone=pretrained)\n        return model\n\n    def _get_datasets(self, uri):\n        cfg = self.cfg\n\n        data_dirs = self.unzip_data(uri)\n        transform, aug_transform = self.get_data_transforms()\n\n        train_ds, valid_ds, test_ds = [], [], []\n        for data_dir in data_dirs:\n            train_dir = join(data_dir, 'train')\n            valid_dir = join(data_dir, 'valid')\n\n            if isdir(train_dir):\n                if cfg.overfit_mode:\n                    train_ds.append(\n                        SemanticSegmentationDataset(\n                            train_dir, transform=transform))\n                else:\n                    train_ds.append(\n                        SemanticSegmentationDataset(\n                            train_dir, transform=aug_transform))\n\n            if isdir(valid_dir):\n                valid_ds.append(\n                    SemanticSegmentationDataset(\n                        valid_dir, transform=transform))\n                test_ds.append(\n                    SemanticSegmentationDataset(\n                        valid_dir, transform=transform))\n\n        train_ds, valid_ds, test_ds = \\\n            ConcatDataset(train_ds), ConcatDataset(valid_ds), ConcatDataset(test_ds)\n\n        return train_ds, valid_ds, test_ds\n\n    def train_step(self, batch, batch_ind):\n        x, y = batch\n        out = self.post_forward(self.model(x))\n        return {'train_loss': F.cross_entropy(out, y)}\n\n    def validate_step(self, batch, batch_ind):\n        x, y = batch\n        out = self.post_forward(self.model(x))\n        val_loss = F.cross_entropy(out, y)\n\n        num_labels = len(self.cfg.data.class_names)\n        y = y.view(-1)\n        out = self.prob_to_pred(out).view(-1)\n        conf_mat = compute_conf_mat(out, y, num_labels)\n\n        return {'val_loss': val_loss, 'conf_mat': conf_mat}\n\n    def validate_end(self, outputs, num_samples):\n        conf_mat = sum([o['conf_mat'] for o in outputs])\n        val_loss = torch.stack([o['val_loss']\n                                for o in outputs]).sum() / num_samples\n        conf_mat_metrics = compute_conf_mat_metrics(conf_mat,\n                                                    self.cfg.data.class_names)\n\n        metrics = {'val_loss': val_loss.item()}\n        metrics.update(conf_mat_metrics)\n\n        return metrics\n\n    def post_forward(self, x):\n        return x['out']\n\n    def prob_to_pred(self, x):\n        return x.argmax(1)\n\n    def plot_xyz(self, ax, x, y, z=None):\n        x = x.permute(1, 2, 0)\n        if x.shape[2] == 1:\n            x = torch.cat([x for _ in range(3)], dim=2)\n        ax.imshow(x)\n        ax.axis('off')\n\n        labels = z if z is not None else y\n        colors = [color_to_triple(c) for c in self.cfg.data.class_colors]\n        colors = [tuple([_c / 255 for _c in c]) for c in colors]\n        cmap = matplotlib.colors.ListedColormap(colors)\n        labels = labels.numpy()\n        ax.imshow(labels, alpha=0.4, vmin=0, vmax=len(colors), cmap=cmap)\n"""
rastervision2/pytorch_learner/semantic_segmentation_learner_config.py,0,"b""from enum import Enum\n\nfrom rastervision2.pipeline.config import register_config, Field, validator\nfrom rastervision2.pytorch_learner.learner_config import (\n    LearnerConfig, DataConfig, ModelConfig)\nfrom rastervision2.pytorch_learner.learner_config import Backbone\n\n\nclass SemanticSegmentationDataFormat(Enum):\n    default = 1\n\n\n@register_config('semantic_segmentation_data')\nclass SemanticSegmentationDataConfig(DataConfig):\n    data_format: SemanticSegmentationDataFormat = SemanticSegmentationDataFormat.default\n\n\n@register_config('semantic_segmentation_model')\nclass SemanticSegmentationModelConfig(ModelConfig):\n    backbone: Backbone = Field(\n        Backbone.resnet50,\n        description=(\n            'The torchvision.models backbone to use. At the moment only '\n            'resnet50 or resnet101 will work.'))\n\n    @validator('backbone')\n    def only_valid_backbones(cls, v):\n        if v not in [Backbone.resnet50, Backbone.resnet101]:\n            raise ValueError(\n                'The only valid backbones for DeepLabv3 are resnet50 '\n                'and resnet101.')\n        return v\n\n\n@register_config('semantic_segmentation_learner')\nclass SemanticSegmentationLearnerConfig(LearnerConfig):\n    data: SemanticSegmentationDataConfig\n    model: SemanticSegmentationModelConfig\n\n    def build(self, tmp_dir, model_path=None):\n        from rastervision2.pytorch_learner.semantic_segmentation_learner import (\n            SemanticSegmentationLearner)\n        return SemanticSegmentationLearner(\n            self, tmp_dir, model_path=model_path)\n"""
rastervision2/pytorch_learner/utils.py,10,"b'from typing import Tuple, Optional\n\nimport torch\nfrom torch.utils.data import Dataset\nimport numpy as np\nfrom PIL import ImageColor\n\n\ndef color_to_triple(color: Optional[str] = None) -> Tuple[int, int, int]:\n    """"""Given a PIL ImageColor string, return a triple of integers\n    representing the red, green, and blue values.\n\n    If color is None, return a random color.\n\n    Args:\n         color: A PIL ImageColor string\n\n    Returns:\n         An triple of integers\n\n    """"""\n    if color is None:\n        r = np.random.randint(0, 0x100)\n        g = np.random.randint(0, 0x100)\n        b = np.random.randint(0, 0x100)\n        return (r, g, b)\n    else:\n        return ImageColor.getrgb(color)\n\n\ndef compute_conf_mat(out, y, num_labels):\n    labels = torch.arange(0, num_labels).to(out.device)\n    return ((out == labels[:, None]) & (y == labels[:, None, None])).sum(\n        dim=2, dtype=torch.float32)\n\n\ndef compute_conf_mat_metrics(conf_mat, label_names, eps=1e-6):\n    # eps is to avoid dividing by zero.\n    eps = torch.tensor(eps)\n    conf_mat = conf_mat.cpu()\n    gt_count = conf_mat.sum(dim=1)\n    pred_count = conf_mat.sum(dim=0)\n    total = conf_mat.sum()\n    true_pos = torch.diag(conf_mat)\n    precision = true_pos / torch.max(pred_count, eps)\n    recall = true_pos / torch.max(gt_count, eps)\n    f1 = (2 * precision * recall) / torch.max(precision + recall, eps)\n\n    weights = gt_count / total\n    weighted_precision = (weights * precision).sum()\n    weighted_recall = (weights * recall).sum()\n    weighted_f1 = ((2 * weighted_precision * weighted_recall) / torch.max(\n        weighted_precision + weighted_recall, eps))\n\n    metrics = {\n        \'avg_precision\': weighted_precision.item(),\n        \'avg_recall\': weighted_recall.item(),\n        \'avg_f1\': weighted_f1.item()\n    }\n    for ind, label in enumerate(label_names):\n        metrics.update({\n            \'{}_precision\'.format(label): precision[ind].item(),\n            \'{}_recall\'.format(label): recall[ind].item(),\n            \'{}_f1\'.format(label): f1[ind].item(),\n        })\n    return metrics\n\n\nclass AlbumentationsDataset(Dataset):\n    """"""An adapter to use arbitrary datasets with albumentations transforms.""""""\n\n    def __init__(self, orig_dataset, transform=None):\n        """"""Constructor.\n\n        Args:\n            orig_dataset: (Dataset) which is assumed to return PIL Image objects\n                and not perform any transforms of its own\n            transform: (albumentations.core.transforms_interface.ImageOnlyTransform)\n        """"""\n        self.orig_dataset = orig_dataset\n        self.transform = transform\n\n    def __getitem__(self, ind):\n        x, y = self.orig_dataset[ind]\n        x = np.array(x)\n        if self.transform:\n            x = self.transform(image=x)[\'image\']\n        x = torch.tensor(x).permute(2, 0, 1).float() / 255.0\n        return x, y\n\n    def __len__(self):\n        return len(self.orig_dataset)\n'"
tests/backend/__init__.py,0,b''
tests/backend/test_keras_classification_config.py,0,"b""import unittest\n\nfrom google.protobuf import json_format\n\nimport rastervision as rv\nfrom rastervision.protos.backend_pb2 import BackendConfig as BackendConfigMsg\n\nfrom tests import data_file_path\n\nCLASSES = ['car', 'background']\n\n\n@unittest.skipIf(not rv.backend.tf_available, 'TF is not available')\nclass TestKerasClassificationConfig(unittest.TestCase):\n    template_uri = data_file_path('keras-classification/resnet50.json')\n\n    def generate_task(self, classes=CLASSES, chip_size=300):\n        return rv.TaskConfig.builder(rv.CHIP_CLASSIFICATION) \\\n                            .with_classes(classes) \\\n                            .with_chip_size(chip_size) \\\n                            .build()\n\n    def get_template_uri(self):\n        return TestKerasClassificationConfig.template_uri\n\n    def test_build_backend(self):\n        b = rv.BackendConfig.builder(rv.KERAS_CLASSIFICATION) \\\n                            .with_task(self.generate_task()) \\\n                            .with_template(self.get_template_uri()) \\\n                            .with_batch_size(100) \\\n                            .build()\n\n        self.assertEqual(b.kc_config['trainer']['options']['batchSize'], 100)\n        self.assertListEqual(b.kc_config['trainer']['options']['classNames'],\n                             CLASSES)\n\n    def test_build_task_from_proto(self):\n        config = {\n            'backend_type': rv.KERAS_CLASSIFICATION,\n            'keras_classification_config': {\n                'kc_config': {\n                    'model': {\n                        'input_size': 300,\n                        'type': 'RESNET50',\n                        'load_weights_by_name': False,\n                        'model_path': ''\n                    },\n                    'trainer': {\n                        'optimizer': {\n                            'type': 'ADAM',\n                            'init_lr': 0.0001\n                        },\n                        'options': {\n                            'training_data_dir': '',\n                            'validation_data_dir': '',\n                            'nb_epochs': 1,\n                            'batch_size': 1,\n                            'input_size': 300,\n                            'output_dir': '',\n                            'class_names': ['TEMPLATE'],\n                            'short_epoch': True\n                        }\n                    }\n                }\n            }\n        }\n\n        msg = json_format.ParseDict(config, BackendConfigMsg())\n        b = rv.BackendConfig.from_proto(msg)\n\n        self.assertEqual(b.kc_config['model']['type'], 'RESNET50')\n\n    def test_create_proto_from_backend(self):\n        t = rv.BackendConfig.builder(rv.KERAS_CLASSIFICATION) \\\n                            .with_task(self.generate_task()) \\\n                            .with_template(self.get_template_uri()) \\\n                            .with_batch_size(100) \\\n                            .build()\n\n        msg = t.to_proto()\n\n        self.assertEqual(msg.backend_type, rv.KERAS_CLASSIFICATION)\n        self.assertEqual(\n            msg.keras_classification_config.kc_config['model']['type'],\n            'RESNET50')\n\n    def test_requires_backend(self):\n        with self.assertRaises(rv.ConfigError):\n            rv.BackendConfig.builder(rv.KERAS_CLASSIFICATION) \\\n                            .with_task(self.generate_task()) \\\n                            .build()\n\n    def test_copies_config_mods(self):\n        bb1 = rv.BackendConfig.builder(rv.KERAS_CLASSIFICATION) \\\n                              .with_task(self.generate_task()) \\\n                              .with_template(self.get_template_uri()) \\\n                              .with_batch_size(100)\n\n        bb2 = bb1.with_batch_size(200)\n\n        b1 = bb1.build()\n        b2 = bb2.build()\n\n        self.assertEqual(b1.kc_config['trainer']['options']['batchSize'], 100)\n        self.assertEqual(b2.kc_config['trainer']['options']['batchSize'], 200)\n\n    def test_raise_error_on_no_backend_field(self):\n        # Will raise since this backend template does not have numSteps\n        with self.assertRaises(rv.ConfigError):\n            rv.BackendConfig.builder(rv.KERAS_CLASSIFICATION) \\\n                            .with_task(self.generate_task()) \\\n                            .with_template(self.get_template_uri()) \\\n                            .with_batch_size(100) \\\n                            .with_config({'numSteps': 100}) \\\n                            .build()\n\n    def test_with_config_fails_key_not_found(self):\n        with self.assertRaises(rv.ConfigError):\n            rv.BackendConfig.builder(rv.KERAS_CLASSIFICATION) \\\n                            .with_task(self.generate_task()) \\\n                            .with_template(self.get_template_uri()) \\\n                            .with_config({'key_does_not_exist': 3}) \\\n                            .build()\n\n    def test_config_missing_template(self):\n        with self.assertRaises(rv.ConfigError):\n            rv.BackendConfig.builder(rv.KERAS_CLASSIFICATION) \\\n                            .build()\n\n    def test_default_model_config(self):\n        b = rv.BackendConfig.builder(rv.KERAS_CLASSIFICATION) \\\n                            .with_task(self.generate_task()) \\\n                            .with_model_defaults(rv.RESNET50_IMAGENET) \\\n                            .build()\n\n        expected = ('https://github.com/fchollet/deep-learning-models/'\n                    'releases/download/v0.2/'\n                    'resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5')\n        self.assertEqual(b.pretrained_model_uri, expected)\n\n    def test_custom_default_model_config(self):\n        model_defaults_path = data_file_path('custom-model-defaults.json')\n        overrides = {'RV_model_defaults_uri': model_defaults_path}\n        rv._registry.initialize_config(config_overrides=overrides)\n\n        try:\n            b = rv.BackendConfig.builder(rv.KERAS_CLASSIFICATION) \\\n                                .with_task(self.generate_task()) \\\n                                .with_model_defaults('CUSTOM_MODEL') \\\n                                .build()\n\n            expected = 'https://www.azavea.com'\n            self.assertEqual(b.pretrained_model_uri, expected)\n        finally:\n            # Reset the config.\n            rv._registry.initialize_config()\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests/backend/test_pytorch_chip_classification.py,0,"b""import unittest\n\nimport rastervision as rv\n\n\n@unittest.skipIf(not rv.backend.pytorch_available, 'PyTorch is not available')\nclass TestPyTorchChipClassificationConfig(unittest.TestCase):\n    def test_builder(self):\n        batch_size = 10\n        num_epochs = 10\n\n        task = rv.TaskConfig.builder(rv.CHIP_CLASSIFICATION) \\\n                            .with_chip_size(200) \\\n                            .with_classes({\n                                'building': (1, 'red'),\n                                'no_building': (2, 'black')\n                            }) \\\n                            .build()\n\n        backend = rv.BackendConfig.builder(rv.PYTORCH_CHIP_CLASSIFICATION) \\\n            .with_task(task) \\\n            .with_train_options(\n                batch_size=batch_size,\n                num_epochs=num_epochs) \\\n            .build()\n\n        msg = backend.to_proto()\n        backend = rv.BackendConfig.builder(rv.PYTORCH_CHIP_CLASSIFICATION) \\\n            .from_proto(msg).build()\n\n        self.assertEqual(backend.train_opts.batch_size, batch_size)\n        self.assertEqual(backend.train_opts.num_epochs, num_epochs)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests/backend/test_pytorch_semantic_segmentation.py,0,"b""import unittest\n\nimport rastervision as rv\n\n\n@unittest.skipIf(not rv.backend.pytorch_available, 'PyTorch is not available')\nclass TestPyTorchSemanticSegmentationConfig(unittest.TestCase):\n    def test_builder(self):\n        batch_size = 10\n        num_epochs = 10\n\n        chip_size = 300\n        class_map = {'red': (1, 'red'), 'green': (2, 'green')}\n        task = rv.TaskConfig.builder(rv.SEMANTIC_SEGMENTATION) \\\n                            .with_chip_size(chip_size) \\\n                            .with_classes(class_map) \\\n                            .with_chip_options(window_method='sliding',\n                                               stride=chip_size,\n                                               debug_chip_probability=1.0) \\\n                            .build()\n\n        backend = rv.BackendConfig.builder(rv.PYTORCH_SEMANTIC_SEGMENTATION) \\\n            .with_task(task) \\\n            .with_train_options(\n                batch_size=batch_size,\n                num_epochs=num_epochs) \\\n            .build()\n\n        msg = backend.to_proto()\n        backend = rv.BackendConfig.builder(rv.PYTORCH_SEMANTIC_SEGMENTATION) \\\n            .from_proto(msg).build()\n\n        self.assertEqual(backend.train_opts.batch_size, batch_size)\n        self.assertEqual(backend.train_opts.num_epochs, num_epochs)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests/backend/test_tf_deeplab_config.py,0,"b""import unittest\nfrom google.protobuf import json_format\n\nimport rastervision as rv\nfrom rastervision.protos.backend_pb2 import BackendConfig as BackendConfigMsg\n\nfrom tests import data_file_path\nfrom tests.mock import (MockMixin, create_mock_experiment)\n\n\n@unittest.skipIf(not rv.backend.tf_available, 'TF is not available')\nclass TestTFDeeplabConfig(MockMixin, unittest.TestCase):\n    template_uri = data_file_path('tf_deeplab/mobilenet_v2.json')\n\n    def generate_task(self, classes=['one', 'two'], chip_size=300):\n        return rv.TaskConfig.builder(rv.SEMANTIC_SEGMENTATION) \\\n                            .with_classes(classes) \\\n                            .with_chip_size(chip_size) \\\n                            .build()\n\n    def get_template_uri(self):\n        return TestTFDeeplabConfig.template_uri\n\n    def test_build_backend(self):\n        model_uri = 'pretrained_model'\n        b = rv.BackendConfig.builder(rv.TF_DEEPLAB) \\\n                            .with_task(self.generate_task()) \\\n                            .with_template(self.get_template_uri()) \\\n                            .with_batch_size(100) \\\n                            .with_model_uri(model_uri) \\\n                            .with_fine_tune_checkpoint_name('foo') \\\n                            .build()\n\n        self.assertEqual(b.tfdl_config['trainBatchSize'], 100)\n        self.assertEqual(b.tfdl_config['modelVariant'], 'mobilenet_v2')\n        self.assertEqual(b.model_uri, model_uri)\n        self.assertEqual(b.fine_tune_checkpoint_name, 'foo')\n\n    def test_update_for_command_and_report_io(self):\n        b = rv.BackendConfig.builder(rv.TF_DEEPLAB) \\\n                            .with_task(self.generate_task()) \\\n                            .with_template(self.get_template_uri()) \\\n                            .with_batch_size(100) \\\n                            .build()\n\n        exp_id = 'exp_id'\n        training_data_uri = '/chip_uri'\n        train_uri = '/train_uri'\n        model_uri = '/train_uri/model'\n        checkpoint_path = '/train_uri/{}.tar.gz'.format(exp_id)\n\n        exp = create_mock_experiment()\n        exp.id = exp_id\n        exp.chip_uri = training_data_uri\n        exp.train_uri = train_uri\n\n        b.update_for_command(rv.CHIP, exp)\n        b.update_for_command(rv.TRAIN, exp)\n\n        self.assertEqual(b.training_data_uri, training_data_uri)\n        self.assertEqual(b.model_uri, model_uri)\n        self.assertEqual(b.fine_tune_checkpoint_name, exp_id)\n\n        chip_io = rv.core.CommandIODefinition()\n        b.report_io(rv.CHIP, chip_io)\n        self.assertEqual(chip_io.output_uris, set([training_data_uri]))\n\n        train_io = rv.core.CommandIODefinition()\n        b.report_io(rv.TRAIN, train_io)\n        self.assertEqual(train_io.input_uris, set([training_data_uri]))\n        self.assertEqual(train_io.output_uris,\n                         set([model_uri, checkpoint_path]))\n\n        bundle_io = rv.core.CommandIODefinition()\n        b.report_io(rv.BUNDLE, bundle_io)\n        self.assertEqual(bundle_io.input_uris, set([model_uri]))\n\n    def test_build_backend_from_proto(self):\n        config = {\n            'backend_type': rv.TF_DEEPLAB,\n            'tf_deeplab_config': {\n                'tfdl_config': {\n                    'decoderOutputStride': 2,\n                    'outputStride': 17\n                }\n            }\n        }\n\n        msg = json_format.ParseDict(config, BackendConfigMsg())\n        b = rv.BackendConfig.from_proto(msg)\n\n        self.assertEqual(b.tfdl_config['decoderOutputStride'], 2)\n        self.assertEqual(b.tfdl_config['outputStride'], 17)\n\n    def test_create_proto_from_backend(self):\n        t = rv.BackendConfig.builder(rv.TF_DEEPLAB) \\\n                            .with_task(self.generate_task()) \\\n                            .with_template(self.get_template_uri()) \\\n                            .with_batch_size(100) \\\n                            .with_fine_tune_checkpoint_name('foo') \\\n                            .build()\n\n        msg = t.to_proto()\n\n        self.assertEqual(msg.backend_type, rv.TF_DEEPLAB)\n        self.assertEqual(msg.tf_deeplab_config.tfdl_config['trainBatchSize'],\n                         100)\n        self.assertEqual(msg.tf_deeplab_config.fine_tune_checkpoint_name,\n                         'foo')\n\n    def test_sets_fine_tune_checkpoint_to_experiment_name(self):\n        task = self.generate_task()\n        backend = rv.BackendConfig.builder(rv.TF_DEEPLAB) \\\n                                  .with_task(task) \\\n                                  .with_template(self.get_template_uri()) \\\n                                  .with_batch_size(100) \\\n                                  .build()\n        dataset = rv.DatasetConfig.builder().build()\n\n        e = rv.ExperimentConfig.builder() \\\n                               .with_task(task) \\\n                               .with_backend(backend) \\\n                               .with_dataset(dataset) \\\n                               .with_id('foo-exp') \\\n                               .with_root_uri('.') \\\n                               .build()\n\n        resolved_e = e.fully_resolve()\n\n        self.assertEqual(resolved_e.backend.fine_tune_checkpoint_name,\n                         'foo-exp')\n\n    def test_requires_backend(self):\n        with self.assertRaises(rv.ConfigError):\n            rv.BackendConfig.builder(rv.TF_DEEPLAB) \\\n                            .with_task(self.generate_task()) \\\n                            .build()\n\n    def test_copies_config_mods(self):\n        bb1 = rv.BackendConfig.builder(rv.TF_DEEPLAB) \\\n                              .with_task(self.generate_task()) \\\n                              .with_template(self.get_template_uri()) \\\n                              .with_batch_size(100)\n\n        bb2 = bb1.with_batch_size(200)\n\n        b1 = bb1.build()\n        b2 = bb2.build()\n\n        self.assertEqual(b1.tfdl_config['trainBatchSize'], 100)\n        self.assertEqual(b2.tfdl_config['trainBatchSize'], 200)\n\n    def test_with_config_fails_key_not_found(self):\n        with self.assertRaises(rv.ConfigError):\n            rv.BackendConfig.builder(rv.TF_DEEPLAB) \\\n                            .with_task(self.generate_task()) \\\n                            .with_template(self.get_template_uri()) \\\n                            .with_config({'key_does_not_exist': 3}) \\\n                            .build()\n\n    def test_config_missing_tfdl_config(self):\n        with self.assertRaises(rv.ConfigError):\n            rv.BackendConfig.builder(rv.TF_DEEPLAB) \\\n                            .build()\n\n    def test_default_model_config(self):\n        b = rv.BackendConfig.builder(rv.TF_DEEPLAB) \\\n                            .with_task(self.generate_task()) \\\n                            .with_model_defaults(rv.MOBILENET_V2) \\\n                            .build()\n\n        self.assertEqual(b.pretrained_model_uri,\n                         ('http://download.tensorflow.org/models/'\n                          'deeplabv3_mnv2_pascal_train_aug_2018_01_29.tar.gz'))\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests/backend/test_tf_object_detection_config.py,0,"b""import unittest\nfrom google.protobuf import json_format\n\nimport rastervision as rv\nfrom rastervision.protos.backend_pb2 import BackendConfig as BackendConfigMsg\n\nfrom tests import data_file_path\n\n\n@unittest.skipIf(not rv.backend.tf_available, 'TF is not available')\nclass TestTFObjectDetectionConfig(unittest.TestCase):\n    template_uri = data_file_path(\n        'tf_object_detection/embedded_ssd_mobilenet_v1_coco.config')\n\n    def generate_task(self, classes=['one', 'two'], chip_size=300):\n        return rv.TaskConfig.builder(rv.OBJECT_DETECTION) \\\n                            .with_classes(classes) \\\n                            .with_chip_size(chip_size) \\\n                            .build()\n\n    def get_template_uri(self):\n        return TestTFObjectDetectionConfig.template_uri\n\n    def test_build_backend(self):\n        b = rv.BackendConfig.builder(rv.TF_OBJECT_DETECTION) \\\n                            .with_task(self.generate_task()) \\\n                            .with_template(self.get_template_uri()) \\\n                            .with_batch_size(100) \\\n                            .with_fine_tune_checkpoint_name('foo') \\\n                            .build()\n\n        self.assertEqual(b.tfod_config['trainConfig']['batchSize'], 100)\n        self.assertEqual(b.tfod_config['model']['ssd']['numClasses'], 2)\n        self.assertEqual(b.fine_tune_checkpoint_name, 'foo')\n\n    def test_build_task_from_proto(self):\n        config = {\n            'backend_type': rv.TF_OBJECT_DETECTION,\n            'tf_object_detection_config': {\n                'tfod_config': {\n                    'model': {\n                        'dummy': {\n                            'numClasses': 5,\n                            'imageResizer': {\n                                'keepAspectRatioResizer': {\n                                    'minDimension': 300,\n                                    'maxDimension': 300\n                                }\n                            }\n                        }\n                    },\n                    'trainConfig': {\n                        'batchSize': 1,\n                        'numSteps': 200\n                    }\n                }\n            }\n        }\n\n        msg = json_format.ParseDict(config, BackendConfigMsg())\n        b = rv.BackendConfig.from_proto(msg)\n\n        self.assertEqual(b.tfod_config['trainConfig']['batchSize'], 1)\n        self.assertEqual(b.tfod_config['trainConfig']['numSteps'], 200)\n\n    def test_create_proto_from_backend(self):\n        t = rv.BackendConfig.builder(rv.TF_OBJECT_DETECTION) \\\n                            .with_task(self.generate_task()) \\\n                            .with_template(self.get_template_uri()) \\\n                            .with_batch_size(100) \\\n                            .with_fine_tune_checkpoint_name('foo') \\\n                            .build()\n\n        msg = t.to_proto()\n\n        self.assertEqual(msg.backend_type, rv.TF_OBJECT_DETECTION)\n        self.assertEqual(\n            msg.tf_object_detection_config.tfod_config['trainConfig'][\n                'batchSize'], 100)\n        self.assertEqual(\n            msg.tf_object_detection_config.fine_tune_checkpoint_name, 'foo')\n\n    def test_sets_fine_tune_checkpoint_to_experiment_name(self):\n        task = self.generate_task()\n        backend = rv.BackendConfig.builder(rv.TF_OBJECT_DETECTION) \\\n                                  .with_task(task) \\\n                                  .with_template(self.get_template_uri()) \\\n                                  .with_batch_size(100) \\\n                                  .build()\n        dataset = rv.DatasetConfig.builder().build()\n\n        e = rv.ExperimentConfig.builder() \\\n                               .with_task(task) \\\n                               .with_backend(backend) \\\n                               .with_dataset(dataset) \\\n                               .with_id('foo-exp') \\\n                               .with_root_uri('.') \\\n                               .build()\n\n        resolved_e = e.fully_resolve()\n\n        self.assertEqual(resolved_e.backend.fine_tune_checkpoint_name,\n                         'foo-exp')\n\n    def test_requires_backend(self):\n        with self.assertRaises(rv.ConfigError):\n            rv.BackendConfig.builder(rv.TF_OBJECT_DETECTION) \\\n                            .with_task(self.generate_task()) \\\n                            .build()\n\n    def test_copies_config_mods(self):\n        bb1 = rv.BackendConfig.builder(rv.TF_OBJECT_DETECTION) \\\n                              .with_task(self.generate_task()) \\\n                              .with_template(self.get_template_uri()) \\\n                              .with_batch_size(100)\n\n        bb2 = bb1.with_batch_size(200)\n\n        b1 = bb1.build()\n        b2 = bb2.build()\n\n        self.assertEqual(b1.tfod_config['trainConfig']['batchSize'], 100)\n        self.assertEqual(b2.tfod_config['trainConfig']['batchSize'], 200)\n\n    def test_raise_error_on_no_backend_field(self):\n        with self.assertRaises(rv.ConfigError):\n            rv.BackendConfig.builder(rv.TF_OBJECT_DETECTION) \\\n                            .with_task(self.generate_task()) \\\n                            .with_template(self.get_template_uri()) \\\n                            .with_batch_size(100) \\\n                            .with_config({'bogusField': 1}) \\\n                            .build()\n\n    def test_with_config_fails_key_not_found(self):\n        with self.assertRaises(rv.ConfigError):\n            rv.BackendConfig.builder(rv.TF_OBJECT_DETECTION) \\\n                            .with_task(self.generate_task()) \\\n                            .with_template(self.get_template_uri()) \\\n                            .with_config({'key_does_not_exist': 3}) \\\n                            .build()\n\n    def test_config_missing_tfod_config(self):\n        with self.assertRaises(rv.ConfigError):\n            rv.BackendConfig.builder(rv.TF_OBJECT_DETECTION) \\\n                            .build()\n\n    def test_sets_pretrained_model(self):\n        pretrained_model_uri = ('http://download.tensorflow.org/'\n                                'models/object_detection/'\n                                'ssd_mobilenet_v1_coco_2017_11_17.tar.gz')\n        b = rv.BackendConfig.builder(rv.TF_OBJECT_DETECTION) \\\n                            .with_task(self.generate_task()) \\\n                            .with_template(self.get_template_uri()) \\\n                            .with_batch_size(100) \\\n                            .with_pretrained_model(pretrained_model_uri) \\\n                            .build()\n\n        self.assertEqual(b.tfod_config['trainConfig']['fineTuneCheckpoint'],\n                         pretrained_model_uri)\n\n    def test_default_model_config(self):\n        b = rv.BackendConfig.builder(rv.TF_OBJECT_DETECTION) \\\n                            .with_task(self.generate_task()) \\\n                            .with_model_defaults(rv.SSD_MOBILENET_V1_COCO) \\\n                            .build()\n\n        self.assertEqual(b.pretrained_model_uri,\n                         ('http://download.tensorflow.org/'\n                          'models/object_detection/'\n                          'ssd_mobilenet_v1_coco_2017_11_17.tar.gz'))\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests/command/__init__.py,0,b''
tests/command/test_analyze_command.py,0,"b""import os\nimport unittest\n\nimport numpy as np\n\nimport rastervision as rv\nfrom rastervision.rv_config import RVConfig\nfrom rastervision.utils.misc import save_img\n\nimport tests.mock as mk\n\n\nclass TestAnalyzeCommand(mk.MockMixin, unittest.TestCase):\n    def test_command_create(self):\n        task = rv.TaskConfig.builder(mk.MOCK_TASK).build()\n        with RVConfig.get_tmp_dir() as tmp_dir:\n            img_path = os.path.join(tmp_dir, 'img.tif')\n            chip = np.ones((2, 2, 4)).astype(np.uint8)\n            chip[:, :, :] *= np.array([0, 1, 2, 3]).astype(np.uint8)\n            save_img(chip, img_path)\n\n            source = rv.data.RasterioSourceConfig(img_path)\n\n            scenes = [rv.data.SceneConfig('', source)]\n            analyzers = [\n                rv.analyzer.StatsAnalyzerConfig(stats_uri='dummy_path')\n            ]\n\n            cmd_conf = rv.CommandConfig.builder(rv.ANALYZE) \\\n                                       .with_task(task) \\\n                                       .with_root_uri(tmp_dir) \\\n                                       .with_scenes(scenes) \\\n                                       .with_analyzers(analyzers) \\\n                                       .build()\n\n            cmd_conf = rv.command.CommandConfig.from_proto(cmd_conf.to_proto())\n            cmd = cmd_conf.create_command()\n\n            self.assertTrue(cmd, rv.command.AnalyzeCommand)\n\n    def test_no_config_error(self):\n        task = rv.task.ChipClassificationConfig({})\n        try:\n            with RVConfig.get_tmp_dir() as tmp_dir:\n                rv.CommandConfig.builder(rv.ANALYZE) \\\n                                .with_task(task) \\\n                                .with_root_uri(tmp_dir) \\\n                                .with_scenes(['']) \\\n                                .with_analyzers(['']) \\\n                                .build()\n        except rv.ConfigError:\n            self.fail('rv.ConfigError raised unexpectedly')\n\n    def test_missing_config_task(self):\n        with self.assertRaises(rv.ConfigError):\n            rv.CommandConfig.builder(rv.ANALYZE) \\\n                            .with_scenes('') \\\n                            .with_analyzers('') \\\n                            .build()\n\n    def test_missing_config_scenes(self):\n        with self.assertRaises(rv.ConfigError):\n            rv.CommandConfig.builder(rv.ANALYZE) \\\n                            .with_task('') \\\n                            .with_analyzers('') \\\n                            .build()\n\n    def test_missing_config_analyzers(self):\n        with self.assertRaises(rv.ConfigError):\n            rv.CommandConfig.builder(rv.ANALYZE) \\\n                            .with_task('') \\\n                            .with_scenes('') \\\n                            .build()\n\n    def test_command_run_with_mocks(self):\n        task = rv.TaskConfig.builder(mk.MOCK_TASK).build()\n        scene = mk.create_mock_scene()\n        analyzer_config = rv.AnalyzerConfig.builder(mk.MOCK_ANALYZER).build()\n        analyzer = analyzer_config.create_analyzer()\n        analyzer_config.mock.create_analyzer.return_value = analyzer\n        cmd = rv.CommandConfig.builder(rv.ANALYZE) \\\n                              .with_task(task) \\\n                              .with_scenes([scene]) \\\n                              .with_root_uri('.') \\\n                              .with_analyzers([analyzer_config]) \\\n                              .build() \\\n                              .create_command()\n        cmd.run()\n\n        self.assertTrue(analyzer.mock.process.called)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests/command/test_aux_command.py,0,"b""import unittest\nfrom functools import reduce\n\nimport rastervision as rv\nfrom rastervision.rv_config import RVConfig\nfrom rastervision.runner import CommandDefinition\n\nimport tests.mock as mk\n\n\nclass TestAuxCommand(mk.MockMixin, unittest.TestCase):\n    def test_command_create(self):\n        with RVConfig.get_tmp_dir() as tmp_dir:\n            uris = [('one', '1'), ('two', '2')]\n            cmd_conf = rv.CommandConfig.builder(mk.MOCK_AUX_COMMAND) \\\n                                       .with_config(uris=uris) \\\n                                       .with_root_uri(tmp_dir) \\\n                                       .build()\n\n            cmd_conf = rv.command.CommandConfig.from_proto(cmd_conf.to_proto())\n            cmd = cmd_conf.create_command()\n\n            self.assertTrue(cmd, mk.MockAuxCommand)\n\n            cmd.run(tmp_dir)\n\n            self.assertTrue(cmd.mock.run.called)\n\n    def test_command_from_experiment(self):\n        with RVConfig.get_tmp_dir() as tmp_dir:\n            uris = [('one', '1'), ('two', '2'), ('three', '3'), ('four', '4')]\n\n            e = mk.create_mock_experiment().to_builder() \\\n                                           .with_root_uri(tmp_dir) \\\n                                           .with_custom_config({\n                                               'mock_aux_command': {\n                                                   'key': 'mock',\n                                                   'config': {\n                                                       'uris': uris\n                                                   }\n                                               }\n                                           }) \\\n                                           .build()\n\n            rv.ExperimentRunner.get_runner(rv.LOCAL).run(\n                e, splits=2, commands_to_run=[mk.MOCK_AUX_COMMAND])\n\n            # Nothing to assert here, just ensures code path runs.\n\n    def test_command_from_experiment_case_insensitive(self):\n        with RVConfig.get_tmp_dir() as tmp_dir:\n            uris = [('one', '1'), ('two', '2'), ('three', '3'), ('four', '4')]\n\n            e = mk.create_mock_experiment().to_builder() \\\n                                           .with_root_uri(tmp_dir) \\\n                                           .with_custom_config({\n                                               'MOCK_AUX_COMMAND': {\n                                                   'key': 'mock',\n                                                   'config': {\n                                                       'uris': uris\n                                                   }\n                                               }\n                                           }) \\\n                                           .build()\n\n            rv.ExperimentRunner.get_runner(rv.LOCAL).run(\n                e, splits=2, commands_to_run=[mk.MOCK_AUX_COMMAND])\n\n            # Nothing to assert here, just ensures code path runs.\n\n    def test_command_split(self):\n        with RVConfig.get_tmp_dir() as tmp_dir:\n            uris = [('one', '1'), ('two', '2'), ('three', '3'), ('four', '4')]\n\n            cmd_conf = rv.CommandConfig.builder(mk.MOCK_AUX_COMMAND) \\\n                                       .with_config(uris=uris) \\\n                                       .with_root_uri(tmp_dir) \\\n                                       .build()\n\n            defs = CommandDefinition.from_command_configs(\n                [cmd_conf], [mk.MOCK_AUX_COMMAND], 2)\n\n            self.assertEqual(len(defs), 2)\n\n            outputs = reduce(lambda a, b: a.union(b),\n                             map(lambda x: x.io_def.output_uris, defs))\n\n            self.assertEqual(outputs, set(['1', '2', '3', '4']))\n\n    def test_required_fields(self):\n        with RVConfig.get_tmp_dir() as tmp_dir:\n            b = rv.CommandConfig.builder(mk.MOCK_AUX_COMMAND) \\\n                                .with_config() \\\n                                .with_root_uri(tmp_dir)\n            with self.assertRaises(rv.ConfigError) as context:\n                b.build()\n\n            self.assertTrue('uris' in str(context.exception))\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests/command/test_bundle_command.py,0,"b""import os\nimport unittest\nimport zipfile\n\nimport rastervision as rv\nfrom rastervision.protos.command_pb2 import CommandConfig as CommandConfigMsg\nfrom rastervision.utils.files import (make_dir, load_json_config)\nfrom rastervision.rv_config import RVConfig\n\nimport tests.mock as mk\nfrom tests import data_file_path\n\n\n@unittest.skipIf(not rv.backend.tf_available, 'TF is not available')\nclass TestBundleCommand(mk.MockMixin, unittest.TestCase):\n    def get_analyzer(self, tmp_dir):\n        stats_uri = os.path.join(tmp_dir, 'stats.json')\n        a = rv.AnalyzerConfig.builder(rv.STATS_ANALYZER) \\\n                             .with_stats_uri(stats_uri) \\\n                             .build()\n        return a\n\n    def get_scene(self, tmp_dir):\n        stats_uri = os.path.join(tmp_dir, 'stats.json')\n        with open(stats_uri, 'w') as f:\n            f.write('DUMMY')\n        transformer = rv.RasterTransformerConfig \\\n                        .builder(rv.STATS_TRANSFORMER) \\\n                        .with_stats_uri(stats_uri) \\\n                        .build()\n\n        raster_source = rv.RasterSourceConfig \\\n                          .builder(rv.RASTERIO_SOURCE) \\\n                          .with_uri('TEST') \\\n                          .with_transformer(transformer) \\\n                          .build()\n\n        scene = rv.SceneConfig.builder() \\\n                              .with_id('TEST') \\\n                              .with_raster_source(raster_source) \\\n                              .build()\n        return scene\n\n    def test_bundle_cc_command(self):\n        def get_task(tmp_dir):\n            predict_package_uri = os.path.join(tmp_dir, 'predict_package.zip')\n            t = rv.TaskConfig.builder(rv.CHIP_CLASSIFICATION) \\\n                             .with_predict_package_uri(predict_package_uri) \\\n                             .with_classes(['class1']) \\\n                             .build()\n            return t\n\n        def get_backend(task, tmp_dir):\n            model_uri = os.path.join(tmp_dir, 'model')\n            with open(model_uri, 'w') as f:\n                f.write('DUMMY')\n            b = rv.BackendConfig.builder(rv.KERAS_CLASSIFICATION) \\\n                                .with_task(task) \\\n                                .with_model_defaults(rv.RESNET50_IMAGENET) \\\n                                .with_model_uri(model_uri) \\\n                                .build()\n            return b\n\n        with RVConfig.get_tmp_dir() as tmp_dir:\n            task = get_task(tmp_dir)\n            backend = get_backend(task, tmp_dir)\n            analyzer = self.get_analyzer(tmp_dir)\n            scene = self.get_scene(tmp_dir)\n            cmd = rv.CommandConfig.builder(rv.BUNDLE) \\\n                                  .with_task(task) \\\n                                  .with_root_uri(tmp_dir) \\\n                                  .with_backend(backend) \\\n                                  .with_analyzers([analyzer]) \\\n                                  .with_scene(scene) \\\n                                  .build() \\\n                                  .create_command(tmp_dir)\n\n            cmd.run(tmp_dir)\n\n            package_dir = os.path.join(tmp_dir, 'package')\n            make_dir(package_dir)\n            with zipfile.ZipFile(task.predict_package_uri, 'r') as package_zip:\n                package_zip.extractall(path=package_dir)\n\n            bundle_config_path = os.path.join(package_dir,\n                                              'bundle_config.json')\n            bundle_config = load_json_config(bundle_config_path,\n                                             CommandConfigMsg())\n\n            self.assertEqual(bundle_config.command_type, rv.BUNDLE)\n\n            actual = set(os.listdir(package_dir))\n            expected = set(['stats.json', 'model', 'bundle_config.json'])\n\n            self.assertEqual(actual, expected)\n\n    def test_bundle_od_command(self):\n        def get_task(tmp_dir):\n            predict_package_uri = os.path.join(tmp_dir, 'predict_package.zip')\n            t = rv.TaskConfig.builder(rv.OBJECT_DETECTION) \\\n                             .with_predict_package_uri(predict_package_uri) \\\n                             .with_classes(['class1']) \\\n                             .build()\n            return t\n\n        def get_backend(task, tmp_dir):\n            model_uri = os.path.join(tmp_dir, 'model')\n            template_uri = data_file_path(\n                'tf_object_detection/embedded_ssd_mobilenet_v1_coco.config')\n            with open(model_uri, 'w') as f:\n                f.write('DUMMY')\n            b = rv.BackendConfig.builder(rv.TF_OBJECT_DETECTION) \\\n                                .with_task(task) \\\n                                .with_template(template_uri) \\\n                                .with_model_uri(model_uri) \\\n                                .build()\n            return b\n\n        with RVConfig.get_tmp_dir() as tmp_dir:\n            task = get_task(tmp_dir)\n            backend = get_backend(task, tmp_dir)\n            analyzer = self.get_analyzer(tmp_dir)\n            scene = self.get_scene(tmp_dir)\n            cmd = rv.CommandConfig.builder(rv.BUNDLE) \\\n                                  .with_task(task) \\\n                                  .with_root_uri(tmp_dir) \\\n                                  .with_backend(backend) \\\n                                  .with_analyzers([analyzer]) \\\n                                  .with_scene(scene) \\\n                                  .build() \\\n                                  .create_command()\n\n            cmd.run(tmp_dir)\n\n            package_dir = os.path.join(tmp_dir, 'package')\n            make_dir(package_dir)\n            with zipfile.ZipFile(task.predict_package_uri, 'r') as package_zip:\n                package_zip.extractall(path=package_dir)\n\n            bundle_config_path = os.path.join(package_dir,\n                                              'bundle_config.json')\n            bundle_config = load_json_config(bundle_config_path,\n                                             CommandConfigMsg())\n\n            self.assertEqual(bundle_config.command_type, rv.BUNDLE)\n\n            actual = set(os.listdir(package_dir))\n            expected = set(['stats.json', 'model', 'bundle_config.json'])\n\n            self.assertEqual(actual, expected)\n\n    def test_missing_config_task(self):\n        with self.assertRaises(rv.ConfigError):\n            rv.command.rv.CommandConfig.builder(rv.BUNDLE) \\\n                                       .with_scene('') \\\n                                       .with_backend('') \\\n                                       .with_analyzers([]) \\\n                                       .build()\n\n    def test_missing_config_backendf(self):\n        with self.assertRaises(rv.ConfigError):\n            rv.command.rv.CommandConfig.builder(rv.BUNDLE) \\\n                                       .with_task('') \\\n                                       .with_scene('') \\\n                                       .with_analyzers([]) \\\n                                       .build()\n\n    def test_missing_config_scene(self):\n        with self.assertRaises(rv.ConfigError):\n            rv.command.rv.CommandConfig.builder(rv.BUNDLE) \\\n                                       .with_task('') \\\n                                       .with_backend('') \\\n                                       .with_analyzers([]) \\\n                                       .build()\n\n    def test_missing_config_analyzers(self):\n        with self.assertRaises(rv.ConfigError):\n            rv.command.rv.CommandConfig.builder(rv.BUNDLE) \\\n                                       .with_task('') \\\n                                       .with_scene('') \\\n                                       .with_backend('') \\\n                                       .build()\n\n    def test_command_run_with_mocks(self):\n        with RVConfig.get_tmp_dir() as tmp_dir:\n            predict_package_uri = os.path.join(tmp_dir, 'predict_package.zip')\n\n            task_config = rv.TaskConfig.builder(mk.MOCK_TASK).build()\n\n            backend_config = rv.BackendConfig.builder(mk.MOCK_BACKEND).build()\n            scene = mk.create_mock_scene()\n            analyzer_config = rv.AnalyzerConfig.builder(\n                mk.MOCK_ANALYZER).build()\n\n            cmd_conf = rv.command.rv.CommandConfig.builder(rv.BUNDLE) \\\n                                                  .with_task(task_config) \\\n                                                  .with_backend(backend_config) \\\n                                                  .with_scene(scene) \\\n                                                  .with_analyzers([analyzer_config]) \\\n                                                  .with_root_uri('.') \\\n                                                  .build()\n\n            cmd_conf = rv.command.CommandConfig.from_proto(cmd_conf.to_proto())\n\n            cmd_conf.task.predict_package_uri = predict_package_uri\n            analyzer_config = cmd_conf.analyzers[0]\n\n            cmd = cmd_conf.create_command()\n\n            cmd.run()\n\n            self.assertTrue(os.path.exists(predict_package_uri))\n            self.assertTrue(analyzer_config.mock.save_bundle_files.called)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests/command/test_chip_command.py,0,"b""import unittest\n\nimport rastervision as rv\nfrom rastervision.rv_config import RVConfig\n\nimport tests.mock as mk\n\n\n@unittest.skipIf(not rv.backend.tf_available, 'TF is not available')\nclass TestChipCommand(mk.MockMixin, unittest.TestCase):\n    def test_command_create(self):\n        task = rv.TaskConfig.builder(mk.MOCK_TASK).build()\n        backend = rv.BackendConfig.builder(mk.MOCK_BACKEND).build()\n        with RVConfig.get_tmp_dir() as tmp_dir:\n            cmd_conf = rv.CommandConfig.builder(rv.CHIP) \\\n                                       .with_task(task) \\\n                                       .with_backend(backend) \\\n                                       .with_train_scenes([]) \\\n                                       .with_val_scenes([]) \\\n                                       .with_root_uri(tmp_dir) \\\n                                       .build()\n\n            cmd_conf = rv.command.CommandConfig.from_proto(cmd_conf.to_proto())\n            cmd = cmd_conf.create_command()\n\n            self.assertTrue(cmd, rv.command.ChipCommand)\n\n    def test_missing_config_task(self):\n        with self.assertRaises(rv.ConfigError):\n            rv.CommandConfig.builder(rv.CHIP) \\\n                            .with_backend('') \\\n                            .with_train_scenes('') \\\n                            .with_val_scenes('') \\\n                            .build()\n\n    def test_missing_config_backend(self):\n        with self.assertRaises(rv.ConfigError):\n            rv.CommandConfig.builder(rv.CHIP) \\\n                            .with_task('') \\\n                            .with_train_scenes('') \\\n                            .with_val_scenes('') \\\n                            .build()\n\n    def test_missing_config_train_scenes(self):\n        with self.assertRaises(rv.ConfigError):\n            rv.CommandConfig.builder(rv.CHIP) \\\n                            .with_task('') \\\n                            .with_backend('') \\\n                            .with_val_scenes('') \\\n                            .build()\n\n    def test_missing_config_val_scenes(self):\n        with self.assertRaises(rv.ConfigError):\n            rv.CommandConfig.builder(rv.CHIP) \\\n                            .with_task('') \\\n                            .with_backend('') \\\n                            .with_train_scenes('') \\\n                            .build()\n\n    def test_no_config_error(self):\n        task = rv.task.ChipClassificationConfig({})\n        backend = rv.backend.KerasClassificationConfig('')\n        try:\n            with RVConfig.get_tmp_dir() as tmp_dir:\n                rv.CommandConfig.builder(rv.CHIP) \\\n                                .with_task(task) \\\n                                .with_root_uri(tmp_dir) \\\n                                .with_backend(backend) \\\n                                .with_train_scenes('') \\\n                                .with_val_scenes('') \\\n                                .build()\n        except rv.ConfigError:\n            self.fail('rv.ConfigError raised unexpectedly')\n\n    def test_command_run_with_mocks(self):\n        task_config = rv.TaskConfig.builder(mk.MOCK_TASK).build()\n        backend_config = rv.BackendConfig.builder(mk.MOCK_BACKEND).build()\n        backend = backend_config.create_backend(task_config)\n        task = task_config.create_task(backend)\n        task_config.mock.create_task.return_value = task\n        backend_config.mock.create_backend.return_value = backend\n        scene = mk.create_mock_scene()\n        cmd = rv.CommandConfig.builder(rv.CHIP) \\\n                              .with_task(task_config) \\\n                              .with_backend(backend_config) \\\n                              .with_train_scenes([scene]) \\\n                              .with_val_scenes([scene]) \\\n                              .with_root_uri('.') \\\n                              .build() \\\n                              .create_command()\n        cmd.run()\n\n        self.assertTrue(task.mock.get_train_windows.called)\n        self.assertTrue(backend.mock.process_sceneset_results.called)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests/command/test_eval_command.py,0,"b""import os\nimport unittest\n\nimport numpy as np\n\nimport rastervision as rv\nfrom rastervision.rv_config import RVConfig\nfrom rastervision.utils.misc import save_img\n\nimport tests.mock as mk\n\n\nclass TestEvalCommand(mk.MockMixin, unittest.TestCase):\n    def test_command_create(self):\n        task = rv.TaskConfig.builder(mk.MOCK_TASK).build()\n        with RVConfig.get_tmp_dir() as tmp_dir:\n            img_path = os.path.join(tmp_dir, 'img.tif')\n            chip = np.ones((2, 2, 4)).astype(np.uint8)\n            chip[:, :, :] *= np.array([0, 1, 2, 3]).astype(np.uint8)\n            save_img(chip, img_path)\n\n            source = rv.data.RasterioSourceConfig(img_path)\n\n            scenes = [rv.data.SceneConfig('scene_id', source)]\n            evaluator = rv.EvaluatorConfig.builder(mk.MOCK_EVALUATOR).build()\n\n            cmd_conf = rv.CommandConfig.builder(rv.EVAL) \\\n                                       .with_task(task) \\\n                                       .with_root_uri(tmp_dir) \\\n                                       .with_scenes(scenes) \\\n                                       .with_evaluators([evaluator]) \\\n                                       .build()\n\n            cmd_conf = rv.command.CommandConfig.from_proto(cmd_conf.to_proto())\n            cmd = cmd_conf.create_command()\n\n            self.assertTrue(cmd, rv.command.EvalCommand)\n\n    def test_missing_config_task(self):\n        with self.assertRaises(rv.ConfigError):\n            rv.CommandConfig.builder(rv.EVAL) \\\n                            .with_scenes('') \\\n                            .with_evaluators('') \\\n                            .build()\n\n    def test_missing_config_scenes(self):\n        with self.assertRaises(rv.ConfigError):\n            rv.CommandConfig.builder(rv.EVAL) \\\n                            .with_task('') \\\n                            .with_evaluators('') \\\n                            .build()\n\n    def test_missing_config_evaluators(self):\n        with self.assertRaises(rv.ConfigError):\n            rv.CommandConfig.builder(rv.EVAL) \\\n                            .with_task('') \\\n                            .with_scenes('') \\\n                            .build()\n\n    def test_no_config_error(self):\n        task = rv.task.ChipClassificationConfig({})\n        try:\n            with RVConfig.get_tmp_dir() as tmp_dir:\n                rv.CommandConfig.builder(rv.EVAL) \\\n                                .with_task(task) \\\n                                .with_root_uri(tmp_dir) \\\n                                .with_scenes(['']) \\\n                                .with_evaluators(['']) \\\n                                .build()\n        except rv.ConfigError:\n            self.fail('rv.ConfigError raised unexpectedly')\n\n    def test_command_run_with_mocks(self):\n        task_config = rv.TaskConfig.builder(mk.MOCK_TASK).build()\n        scene = mk.create_mock_scene()\n        evaluator_config = rv.EvaluatorConfig.builder(\n            mk.MOCK_EVALUATOR).build()\n        evaluator = evaluator_config.create_evaluator()\n        evaluator_config.mock.create_evaluator.return_value = evaluator\n\n        cmd = rv.CommandConfig.builder(rv.EVAL) \\\n                              .with_task(task_config) \\\n                              .with_scenes([scene]) \\\n                              .with_evaluators([evaluator_config]) \\\n                              .with_root_uri('.') \\\n                              .build() \\\n                              .create_command()\n        cmd.run()\n\n        self.assertTrue(evaluator.mock.process.called)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests/command/test_predict_command.py,0,"b""import unittest\n\nimport rastervision as rv\nfrom rastervision.rv_config import RVConfig\nfrom rastervision.core import Box\n\nimport tests.mock as mk\n\n\n@unittest.skipIf(not rv.backend.tf_available, 'TF is not available')\nclass PredictCommand(mk.MockMixin, unittest.TestCase):\n    def test_command_create(self):\n        task = rv.TaskConfig.builder(mk.MOCK_TASK).build()\n        backend = rv.BackendConfig.builder(mk.MOCK_BACKEND).build()\n        with RVConfig.get_tmp_dir() as tmp_dir:\n            cmd_conf = rv.CommandConfig.builder(rv.PREDICT) \\\n                                       .with_task(task) \\\n                                       .with_root_uri(tmp_dir) \\\n                                       .with_scenes([]) \\\n                                       .with_backend(backend) \\\n                                       .build()\n\n            cmd_conf = rv.command.CommandConfig.from_proto(cmd_conf.to_proto())\n            cmd = cmd_conf.create_command()\n\n            self.assertTrue(cmd, rv.command.PredictCommand)\n\n    def test_missing_config_task(self):\n        with self.assertRaises(rv.ConfigError):\n            rv.CommandConfig.builder(rv.PREDICT) \\\n                            .with_backend('') \\\n                            .with_scenes(['']) \\\n                            .build()\n\n    def test_missing_config_backend(self):\n        with self.assertRaises(rv.ConfigError):\n            rv.CommandConfig.builder(rv.PREDICT) \\\n                            .with_task('') \\\n                            .with_scenes(['']) \\\n                            .build()\n\n    def test_missing_config_scenes(self):\n        with self.assertRaises(rv.ConfigError):\n            rv.CommandConfig.builder(rv.PREDICT) \\\n                            .with_task('') \\\n                            .with_backend('') \\\n                            .build()\n\n    def test_no_config_error(self):\n        task = rv.task.ChipClassificationConfig({})\n        backend = rv.backend.KerasClassificationConfig('')\n        try:\n            with RVConfig.get_tmp_dir() as tmp_dir:\n                rv.CommandConfig.builder(rv.PREDICT) \\\n                                .with_task(task) \\\n                                .with_root_uri(tmp_dir) \\\n                                .with_backend(backend) \\\n                                .with_scenes(['']) \\\n                                .build()\n        except rv.ConfigError:\n            self.fail('rv.ConfigError raised unexpectedly')\n\n    def test_command_run_with_mocks(self):\n        task_config = rv.TaskConfig.builder(mk.MOCK_TASK).build()\n        backend_config = rv.BackendConfig.builder(mk.MOCK_BACKEND).build()\n        backend = backend_config.create_backend(task_config)\n        backend_config.mock.create_backend.return_value = backend\n        task = task_config.create_task(backend)\n        task_config.mock.create_task.return_value = task\n        scene = mk.create_mock_scene()\n\n        task.mock.get_predict_windows.return_value = [Box(0, 0, 1, 1)]\n\n        cmd = rv.CommandConfig.builder(rv.PREDICT) \\\n                              .with_task(task_config) \\\n                              .with_backend(backend_config) \\\n                              .with_scenes([scene]) \\\n                              .with_root_uri('.') \\\n                              .build() \\\n                              .create_command()\n        cmd.run()\n\n        self.assertTrue(backend.mock.predict.called)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests/command/test_train_command.py,0,"b""import unittest\n\nimport rastervision as rv\nfrom rastervision.rv_config import RVConfig\n\nimport tests.mock as mk\n\n\n@unittest.skipIf(not rv.backend.tf_available, 'TF is not available')\nclass TrainCommand(mk.MockMixin, unittest.TestCase):\n    def test_missing_config_task(self):\n        with self.assertRaises(rv.ConfigError):\n            rv.CommandConfig.builder(rv.TRAIN) \\\n                            .with_backend('') \\\n                            .build()\n\n    def test_missing_config_backend(self):\n        with self.assertRaises(rv.ConfigError):\n            rv.CommandConfig.builder(rv.TRAIN) \\\n                            .with_task('') \\\n                            .build()\n\n    def test_no_config_error(self):\n        task = rv.task.ChipClassificationConfig({})\n        backend = rv.backend.KerasClassificationConfig('')\n        try:\n            with RVConfig.get_tmp_dir() as tmp_dir:\n                rv.CommandConfig.builder(rv.TRAIN) \\\n                                .with_task(task) \\\n                                .with_root_uri(tmp_dir) \\\n                                .with_backend(backend) \\\n                                .build()\n        except rv.ConfigError:\n            self.fail('rv.ConfigError raised unexpectedly')\n\n    def test_command_run_with_mocks(self):\n        task_config = rv.TaskConfig.builder(mk.MOCK_TASK).build()\n        backend_config = rv.BackendConfig.builder(mk.MOCK_BACKEND).build()\n        backend = backend_config.create_backend(task_config)\n\n        cmd_conf = rv.CommandConfig.builder(rv.TRAIN) \\\n                                   .with_task(task_config) \\\n                                   .with_backend(backend_config) \\\n                                   .with_root_uri('.') \\\n                                   .build()\n\n        cmd_conf = rv.command.CommandConfig.from_proto(cmd_conf.to_proto())\n\n        cmd_conf.backend.mock.create_backend.return_value = backend\n\n        cmd = cmd_conf.create_command()\n\n        cmd.run()\n\n        self.assertTrue(backend.mock.train.called)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests/core/__init__.py,0,b''
tests/core/test_box.py,0,"b""import unittest\n\nimport numpy as np\n\nfrom rastervision.core.box import Box, BoxSizeError\nfrom shapely.geometry import box as ShapelyBox\n\nnp.random.seed(1)\n\n\nclass TestBox(unittest.TestCase):\n    def setUp(self):\n        self.ymin = 0\n        self.xmin = 0\n        self.ymax = 2\n        self.xmax = 3\n        self.box = Box(self.ymin, self.xmin, self.ymax, self.xmax)\n\n    def test_reproject(self):\n        def transform(point):\n            (y, x) = point\n            return ((y + 1) // 2, x // 2)\n\n        reproj = self.box.reproject(transform)\n        self.assertTrue(reproj.xmin == 0)\n        self.assertTrue(reproj.ymin == 0)\n        self.assertTrue(reproj.ymax == 1)\n        self.assertTrue(reproj.xmax == 2)\n\n    def test_dict(self):\n        dictionary = self.box.to_dict()\n        other = Box.from_dict(dictionary)\n        self.assertTrue(self.box == other)\n\n    def test_bad_square(self):\n        self.assertRaises(BoxSizeError,\n                          lambda: self.box.make_random_square(10))\n\n    def test_bad_conatiner(self):\n        self.assertRaises(BoxSizeError,\n                          lambda: self.box.make_random_square_container(1))\n\n    def test_neq(self):\n        other = Box(self.ymin + 1, self.xmin, self.ymax, self.xmax)\n        self.assertTrue(self.box != other)\n\n    def test_int(self):\n        other = Box(\n            float(self.ymin) + 0.01, float(self.xmin), float(self.ymax),\n            float(self.xmax))\n        self.assertTrue(other.to_int() == self.box)\n\n    def test_get_height(self):\n        height = self.ymax - self.ymin\n        self.assertEqual(self.box.get_height(), height)\n\n    def test_get_width(self):\n        width = self.xmax - self.xmin\n        self.assertEqual(self.box.get_width(), width)\n\n    def test_get_area(self):\n        area = self.box.get_height() * self.box.get_width()\n        self.assertEqual(self.box.get_area(), area)\n\n    def test_rasterio_format(self):\n        rasterio_box = ((self.ymin, self.ymax), (self.xmin, self.xmax))\n        self.assertEqual(self.box.rasterio_format(), rasterio_box)\n\n    def test_tuple_format(self):\n        box_tuple = (0, 0, 2, 3)\n        output_box = self.box.tuple_format()\n        self.assertEqual(output_box, box_tuple)\n\n    def test_shapely_format(self):\n        shapely_box = (self.xmin, self.ymin, self.xmax, self.ymax)\n        self.assertEqual(self.box.shapely_format(), shapely_box)\n\n    def test_npbox_format(self):\n        self.assertEqual(\n            tuple(self.box.npbox_format()), self.box.tuple_format())\n        self.assertEqual(self.box.npbox_format().dtype, np.float)\n\n    def test_geojson_coordinates(self):\n        nw = [self.xmin, self.ymin]\n        ne = [self.xmin, self.ymax]\n        se = [self.xmax, self.ymax]\n        sw = [self.xmax, self.ymin]\n        geojson_coords = [nw, ne, se, sw, nw]\n        self.assertEqual(self.box.geojson_coordinates(), geojson_coords)\n\n    def test_make_random_square_container(self):\n        size = 5\n        nb_tests = 100\n        for _ in range(nb_tests):\n            container = self.box.make_random_square_container(size)\n            self.assertEqual(container.get_width(), container.get_height())\n            self.assertEqual(container.get_width(), size)\n            self.assertTrue(container.to_shapely().contains(\n                self.box.to_shapely()))\n\n    def test_make_random_square_container_too_big(self):\n        size = 1\n        with self.assertRaises(BoxSizeError):\n            self.box.make_random_square_container(size)\n\n    def test_make_random_square(self):\n        window = Box(5, 5, 15, 15)\n        size = 5\n        nb_tests = 100\n        for _ in range(nb_tests):\n            box = window.make_random_square(size)\n            self.assertEqual(box.get_width(), box.get_height())\n            self.assertEqual(box.get_width(), size)\n            self.assertTrue(window.to_shapely().contains(box.to_shapely()))\n\n    def test_from_npbox(self):\n        npbox = np.array([self.ymin, self.xmin, self.ymax, self.xmax])\n        output_box = Box.from_npbox(npbox)\n        self.assertEqual(output_box, self.box)\n\n    def test_from_shapely(self):\n        shape = ShapelyBox(self.xmin, self.ymin, self.xmax, self.ymax)\n        output_box = Box.from_shapely(shape)\n        self.assertEqual(output_box, self.box)\n\n    def test_to_shapely(self):\n        bounds = self.box.to_shapely().bounds\n        self.assertEqual((bounds[1], bounds[0], bounds[3], bounds[2]),\n                         self.box.tuple_format())\n\n    def test_make_square(self):\n        square = Box(0, 0, 10, 10)\n        output_square = Box.make_square(0, 0, 10)\n        self.assertEqual(output_square, square)\n        self.assertEqual(output_square.get_width(), output_square.get_height())\n\n    def test_make_eroded(self):\n        max_extent = Box.make_square(0, 0, 10)\n        box = Box(1, 1, 3, 4)\n        buffer_size = erosion_size = 1\n        eroded_box = box.make_buffer(buffer_size, max_extent) \\\n                        .make_eroded(erosion_size)\n        self.assertEqual(eroded_box, box)\n\n    def test_make_buffer(self):\n        buffer_size = 1\n        max_extent = Box.make_square(0, 0, 3)\n        buffer_box = Box(0, 0, 3, 3)\n        output_buffer_box = self.box.make_buffer(buffer_size, max_extent)\n        self.assertEqual(output_buffer_box, buffer_box)\n\n        buffer_size = 0.5\n        max_extent = Box.make_square(0, 0, 5)\n        buffer_box = Box(0, 0, 3, 5)\n        output_buffer_box = self.box.make_buffer(buffer_size, max_extent)\n        self.assertEqual(output_buffer_box, buffer_box)\n\n    def test_make_copy(self):\n        copy_box = self.box.make_copy()\n        self.assertIsNot(copy_box, self.box)\n        self.assertEqual(copy_box, self.box)\n\n    def test_get_windows(self):\n        extent = Box(0, 0, 100, 100)\n        windows = list(extent.get_windows(10, 10))\n        self.assertEqual(len(windows), 100)\n\n        extent = Box(0, 0, 100, 100)\n        windows = list(extent.get_windows(10, 5))\n        self.assertEqual(len(windows), 400)\n\n        extent = Box(0, 0, 20, 20)\n        windows = set(\n            [window.tuple_format() for window in extent.get_windows(10, 10)])\n        expected_windows = [\n            Box.make_square(0, 0, 10),\n            Box.make_square(10, 0, 10),\n            Box.make_square(0, 10, 10),\n            Box.make_square(10, 10, 10)\n        ]\n        expected_windows = set(\n            [window.tuple_format() for window in expected_windows])\n        self.assertSetEqual(windows, expected_windows)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests/core/test_class_map.py,0,"b""import unittest\n\nfrom rastervision.core import (ClassMap, ClassItem)\nfrom rastervision.protos.class_item_pb2 \\\n    import ClassItem as ClassItemMsg\n\n\nclass TestClassMap(unittest.TestCase):\n    def test_construct_from_list_str(self):\n        source = ['one', 'two', 'three']\n        cm = ClassMap.construct_from(source)\n        for i, name in enumerate(source):\n            expected = ClassItem(id=i + 1, name=name, color=None)\n            actual = cm.get_by_id(i + 1)\n            self.assertEqual(actual, expected)\n\n    def test_construct_from(self):\n        self.assertRaises(Exception,\n                          lambda: ClassMap.construct_from('some string'))\n\n    def test_all_color_true(self):\n        source = [\n            ClassItemMsg(id=1, name='one', color='red'),\n            ClassItemMsg(id=2, name='two', color='green'),\n            ClassItemMsg(id=3, name='three', color='blue')\n        ]\n        cm = ClassMap.construct_from(source)\n        self.assertTrue(cm.has_all_colors())\n\n    def test_all_color_false(self):\n        source = [\n            ClassItemMsg(id=1, name='one', color='red'),\n            ClassItemMsg(id=2, name='two', color='green'),\n            ClassItemMsg(id=3, name='three')\n        ]\n        cm = ClassMap.construct_from(source)\n        self.assertFalse(cm.has_all_colors())\n\n    def test_category_index(self):\n        source = [\n            ClassItemMsg(id=1, name='one', color='red'),\n            ClassItemMsg(id=2, name='two', color='green'),\n            ClassItemMsg(id=3, name='three')\n        ]\n        cm = ClassMap.construct_from(source)\n        index = cm.get_category_index()\n        self.assertEqual(index[1], {'id': 1, 'name': 'one'})\n\n    def test_get_by_name_negative(self):\n        source = [\n            ClassItemMsg(id=1, name='one', color='red'),\n            ClassItemMsg(id=2, name='two', color='green'),\n            ClassItemMsg(id=3, name='three', color='blue')\n        ]\n        cm = ClassMap.construct_from(source)\n        self.assertRaises(ValueError, lambda: cm.get_by_name('four'))\n\n    def test_construct_from_protos(self):\n        source = [\n            ClassItemMsg(id=1, name='one', color='red'),\n            ClassItemMsg(id=2, name='two', color='green'),\n            ClassItemMsg(id=3, name='three', color='blue')\n        ]\n        cm = ClassMap.construct_from(source)\n        for i, msg in enumerate(source):\n            expected = ClassItem.from_proto(msg)\n            actual = cm.get_by_id(i + 1)\n            self.assertEqual(actual, expected)\n\n    def test_construct_from_class_items(self):\n        source = [\n            ClassItem(id=1, name='one', color='red'),\n            ClassItem(id=2, name='two', color='green'),\n            ClassItem(id=3, name='three', color='blue')\n        ]\n        cm = ClassMap.construct_from(source)\n        for i, item in enumerate(source):\n            expected = item\n            actual = cm.get_by_id(i + 1)\n            self.assertEqual(actual, expected)\n\n    def test_construct_from_dict_no_color(self):\n        source = {'one': 1, 'two': 2, 'three': 3}\n        cm = ClassMap.construct_from(source)\n        for name in source:\n            expected = ClassItem(id=source[name], name=name, color=None)\n            actual = cm.get_by_id(source[name])\n            self.assertEqual(actual, expected)\n\n    def test_construct_from_dict_with_color(self):\n        source = {'one': (1, 'red'), 'two': (2, 'green'), 'three': (3, 'blue')}\n        cm = ClassMap.construct_from(source)\n        for name in source:\n            expected = ClassItem(\n                id=source[name][0], name=name, color=source[name][1])\n            actual = cm.get_by_id(source[name][0])\n            self.assertEqual(actual, expected)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests/core/test_command_io_definition.py,0,"b""import unittest\n\nfrom rastervision.core import CommandIODefinition\n\n\nclass TestCommandIoDefinition(unittest.TestCase):\n    def test_add_missing(self):\n        cid = CommandIODefinition()\n        cid.add_missing('message')\n        self.assertEqual(cid.missing_input_messages[0], 'message')\n"""
tests/core/test_config.py,0,"b""from copy import deepcopy\nimport unittest\nfrom rastervision.core.config import ConfigBuilder\n\n\nclass DummyConfig(object):\n    def __init__(self, foo: str):\n        self.foo = foo\n\n\nclass DummyConfigBuilder(ConfigBuilder):\n    config_class = DummyConfig\n    config = {'foo': 'bar'}\n\n    def __init__(self):\n        return\n\n    def with_foo(self, foo):\n        b = deepcopy(self)\n        b.foo = foo\n        return b\n\n    def from_proto(self, msg):\n        return self.with_foo(msg.foo)\n\n\nclass TestConfig(unittest.TestCase):\n    def test_build_with_annotations(self):\n        self.assertTrue(DummyConfigBuilder().build().foo == 'bar')\n"""
tests/core/test_stats_analyzer.py,0,"b""import unittest\nimport os\n\nimport numpy as np\n\nfrom rastervision.core.raster_stats import RasterStats, chip_size\nfrom rastervision.data import Scene\nfrom tests.mock.raster_source import MockRasterSource\nimport rastervision as rv\nfrom rastervision.rv_config import RVConfig\n\n\nclass TestStatsAnalyzer(unittest.TestCase):\n    def setUp(self):\n        self.temp_dir = RVConfig.get_tmp_dir()\n\n    def tearDown(self):\n        self.temp_dir.cleanup()\n\n    def _test(self, is_random=False, is_backcompat=False):\n        stats_uri = os.path.join(self.temp_dir.name, 'stats.json')\n        scenes = []\n        raster_sources = []\n        imgs = []\n        sample_prob = 0.5\n        for i in range(3):\n            rs = MockRasterSource([0, 1, 2], 3)\n            img = np.zeros((600, 600, 3))\n            img[:, :, 0] = 1 + i\n            img[:, :, 1] = 2 + i\n            img[:, :, 2] = 3 + i\n            if not is_random:\n                img[300:, 300:, :] = np.nan\n\n            imgs.append(img)\n            rs.set_raster(img)\n            raster_sources.append(rs)\n            scenes.append(Scene(str(i), rs))\n\n        channel_vals = list(map(lambda x: np.expand_dims(x, axis=0), imgs))\n        channel_vals = np.concatenate(channel_vals, axis=0)\n        channel_vals = np.transpose(channel_vals, [3, 0, 1, 2])\n        channel_vals = np.reshape(channel_vals, (3, -1))\n        exp_means = np.nanmean(channel_vals, axis=1)\n        exp_stds = np.nanstd(channel_vals, axis=1)\n\n        analyzer_builder = rv.AnalyzerConfig.builder(rv.STATS_ANALYZER)\n        if is_random:\n            analyzer_builder = analyzer_builder.with_sample_prob(sample_prob)\n        analyzer_msg = analyzer_builder.with_stats_uri(stats_uri) \\\n                                       .build().to_proto()\n        if is_backcompat:\n            analyzer_msg.stats_analyzer_config.stats_uri = ''\n            analyzer_msg.stats_uri = stats_uri\n\n        analyzer_config = rv.AnalyzerConfig.builder(rv.STATS_ANALYZER) \\\n                            .from_proto(analyzer_msg).build()\n        analyzer = analyzer_config.create_analyzer()\n        analyzer.process(scenes, self.temp_dir.name)\n\n        stats = RasterStats.load(stats_uri)\n        np.testing.assert_array_almost_equal(stats.means, exp_means, decimal=3)\n        np.testing.assert_array_almost_equal(stats.stds, exp_stds, decimal=3)\n        if is_random:\n            for rs in raster_sources:\n                width = rs.get_extent().get_width()\n                height = rs.get_extent().get_height()\n                exp_num_chips = round(\n                    ((width * height) / (chip_size**2)) * sample_prob)\n                self.assertEqual(rs.mock._get_chip.call_count, exp_num_chips)\n\n    def test_random(self):\n        self._test(is_random=True)\n\n    def test_sliding(self):\n        self._test(is_random=False)\n\n    def test_sliding_backcompat(self):\n        self._test(is_random=False, is_backcompat=True)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests/data/__init__.py,0,b''
tests/data/mock_crs_transformer.py,0,"b'from rastervision.data import CRSTransformer\n\n\nclass DoubleCRSTransformer(CRSTransformer):\n    """"""Mock CRSTransformer used for testing.\n\n    Assumes map coords are 2x pixels coords.\n    """"""\n\n    def map_to_pixel(self, web_point):\n        return (web_point[0] * 2.0, web_point[1] * 2.0)\n\n    def pixel_to_map(self, pixel_point):\n        return (pixel_point[0] / 2.0, pixel_point[1] / 2.0)\n'"
tests/data/test_activate_mixin.py,0,"b'import unittest\n\nfrom rastervision.data import (ActivateMixin, ActivationError)\n\n\nclass TestActivateMixin(unittest.TestCase):\n    class Foo(ActivateMixin):\n        def __init__(self):\n            self.activated = False\n\n        def _activate(self):\n            self.activated = True\n\n        def _deactivate(self):\n            self.activated = False\n\n    class Bar(ActivateMixin):\n        def __init__(self):\n            self.activated = False\n            self.foo = TestActivateMixin.Foo()\n\n        def _activate(self):\n            self.activated = True\n\n        def _deactivate(self):\n            self.activated = False\n\n        def _subcomponents_to_activate(self):\n            return [self.foo]\n\n    def test_activates_and_deactivates(self):\n        foo = TestActivateMixin.Foo()\n        self.assertFalse(foo.activated)\n        with foo.activate():\n            self.assertTrue(foo.activated)\n        self.assertFalse(foo.activated)\n\n    def test_activated_and_deactivates_subcomponents(self):\n        bar = TestActivateMixin.Bar()\n        self.assertFalse(bar.activated)\n        self.assertFalse(bar.foo.activated)\n        with bar.activate():\n            self.assertTrue(bar.activated)\n            self.assertTrue(bar.foo.activated)\n        self.assertFalse(bar.activated)\n        self.assertFalse(bar.foo.activated)\n\n    def test_no_activate_twice(self):\n        bar = TestActivateMixin.Bar()\n        with self.assertRaises(ActivationError):\n            with bar.activate():\n                with bar.activate():\n                    pass\n        self.assertFalse(bar.activated)\n'"
tests/data/test_scene.py,0,"b'import unittest\n\nimport rastervision as rv\nfrom rastervision.rv_config import RVConfig\n\nimport tests.mock as mk\nfrom tests import data_file_path\n\n\nclass TestScene(unittest.TestCase):\n    def setUp(self):\n        config = {\'PLUGINS_modules\': \'[""{}""]\'.format(\'tests.mock\')}\n        rv._registry.initialize_config(config_overrides=config)\n\n        self.temp_dir = RVConfig.get_tmp_dir()\n\n    def tearDown(self):\n        rv._registry.initialize_config()\n        self.temp_dir.cleanup()\n\n    def test_with_aois(self):\n        aoi_uri = data_file_path(\'evaluator/cc-label-aoi.json\')\n        aoi_uris = [aoi_uri, aoi_uri]\n        task_config = rv.TaskConfig.builder(mk.MOCK_TASK).build()\n\n        scene_config = mk.create_mock_scene()\n        scene_config = scene_config.to_builder().with_aoi_uris(\n            aoi_uris).build()\n        scene = scene_config.create_scene(task_config, self.temp_dir.name)\n        self.assertEqual(2, len(scene.aoi_polygons))\n\n    def test_with_aoi(self):\n        aoi_uri = data_file_path(\'evaluator/cc-label-aoi.json\')\n        task_config = rv.TaskConfig.builder(mk.MOCK_TASK).build()\n\n        scene_config = mk.create_mock_scene()\n        scene_config = scene_config.to_builder().with_aoi_uri(aoi_uri).build()\n        scene = scene_config.create_scene(task_config, self.temp_dir.name)\n        self.assertEqual(1, len(scene.aoi_polygons))\n\n    def test_with_aoi_back_compat(self):\n        aoi_uri = data_file_path(\'evaluator/cc-label-aoi.json\')\n        task_config = rv.TaskConfig.builder(mk.MOCK_TASK).build()\n        scene_config = mk.create_mock_scene()\n        scene_msg = scene_config.to_proto()\n        # Use deprecated aoi_uri field.\n        del scene_msg.aoi_uris[:]\n        scene_msg.aoi_uri = aoi_uri\n        scene_config = rv.SceneConfig.from_proto(scene_msg)\n        scene = scene_config.create_scene(task_config, self.temp_dir.name)\n        self.assertEqual(1, len(scene.aoi_polygons))\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
tests/evaluation/__init__.py,0,b''
tests/evaluation/test_chip_classification_evaluation.py,0,"b'import unittest\n\nfrom rastervision.evaluation import ChipClassificationEvaluation\nfrom rastervision.core.class_map import ClassItem, ClassMap\nfrom rastervision.core.box import Box\nfrom rastervision.data.label import ChipClassificationLabels\n\n\nclass TestChipClassificationEvaluation(unittest.TestCase):\n    def make_class_map(self):\n        class_items = [ClassItem(1, \'grassy\'), ClassItem(2, \'urban\')]\n        return ClassMap(class_items)\n\n    def make_labels(self, class_ids):\n        """"""Make 2x2 grid label store.\n\n        Args:\n            class_ids: 2x2 array of class_ids to use\n        """"""\n        cell_size = 200\n        y_cells = 2\n        x_cells = 2\n        labels = ChipClassificationLabels()\n\n        for yind in range(y_cells):\n            for xind in range(x_cells):\n                ymin = yind * cell_size\n                xmin = xind * cell_size\n                ymax = ymin + cell_size\n                xmax = xmin + cell_size\n                window = Box(ymin, xmin, ymax, xmax)\n                class_id = class_ids[yind][xind]\n                new_labels = ChipClassificationLabels()\n                new_labels.set_cell(window, class_id)\n                labels.extend(new_labels)\n\n        return labels\n\n    def assert_eval_single_null(self, eval):\n        eval_item1 = eval.class_to_eval_item[1]\n        self.assertEqual(eval_item1.gt_count, 2)\n        self.assertEqual(eval_item1.precision, 1.0)\n        self.assertEqual(eval_item1.recall, 0.5)\n        self.assertAlmostEqual(eval_item1.f1, 2 / 3, places=2)\n\n        eval_item2 = eval.class_to_eval_item[2]\n        self.assertEqual(eval_item2.gt_count, 1)\n        self.assertEqual(eval_item2.precision, 0.5)\n        self.assertEqual(eval_item2.recall, 1.0)\n        self.assertAlmostEqual(eval_item2.f1, 2 / 3, places=2)\n\n        avg_item = eval.avg_item\n        self.assertEqual(avg_item.gt_count, 3)\n        self.assertAlmostEqual(avg_item.precision, 0.83, places=2)\n        self.assertAlmostEqual(avg_item.recall, 2 / 3, places=2)\n        self.assertAlmostEqual(avg_item.f1, 2 / 3, places=2)\n\n    def test_compute_single_pred_null(self):\n        class_map = self.make_class_map()\n        eval = ChipClassificationEvaluation(class_map)\n        gt_class_ids = [[1, 2], [1, 2]]\n        gt_labels = self.make_labels(gt_class_ids)\n        pred_class_ids = [[1, None], [2, 2]]\n        pred_labels = self.make_labels(pred_class_ids)\n        eval.compute(gt_labels, pred_labels)\n        self.assert_eval_single_null(eval)\n\n    def test_compute_single_gt_null(self):\n        class_map = self.make_class_map()\n        eval = ChipClassificationEvaluation(class_map)\n        gt_class_ids = [[1, None], [1, 2]]\n        gt_labels = self.make_labels(gt_class_ids)\n        pred_class_ids = [[1, 2], [2, 2]]\n        pred_labels = self.make_labels(pred_class_ids)\n        eval.compute(gt_labels, pred_labels)\n        self.assert_eval_single_null(eval)\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
tests/evaluation/test_chip_classification_evaluator.py,0,"b""import unittest\nimport os\nimport json\n\nimport rastervision as rv\nfrom rastervision.rv_config import RVConfig\n\nfrom tests import data_file_path\nfrom tests.mock import (MockMixin, create_mock_experiment)\n\n\nclass TestChipClassificationEvaluator(MockMixin, unittest.TestCase):\n    def test_accounts_for_aoi(self):\n        task = rv.TaskConfig.builder(rv.CHIP_CLASSIFICATION) \\\n                            .with_classes(['car', 'building', 'background']) \\\n                            .build()\n\n        label_source_uri = data_file_path('evaluator/cc-label-filtered.json')\n        label_source = rv.LabelSourceConfig.builder(rv.CHIP_CLASSIFICATION_GEOJSON) \\\n                                           .with_uri(label_source_uri) \\\n                                           .build()\n\n        label_store_uri = data_file_path('evaluator/cc-label-full.json')\n        label_store = rv.LabelStoreConfig.builder(rv.CHIP_CLASSIFICATION_GEOJSON) \\\n                                         .with_uri(label_store_uri) \\\n                                         .build()\n\n        source_uri = data_file_path('evaluator/cc-label-img-blank.tif')\n        raster_source = rv.RasterSourceConfig.builder(rv.GEOTIFF_SOURCE) \\\n                                             .with_uri(source_uri) \\\n                                             .build()\n\n        aoi_uri = data_file_path('evaluator/cc-label-aoi.json')\n        s = rv.SceneConfig.builder() \\\n                          .with_id('test') \\\n                          .with_raster_source(raster_source) \\\n                          .with_label_source(label_source) \\\n                          .with_label_store(label_store) \\\n                          .with_aoi_uri(aoi_uri) \\\n                          .build()\n\n        with RVConfig.get_tmp_dir() as tmp_dir:\n            scene = s.create_scene(task, tmp_dir)\n\n            output_uri = os.path.join(tmp_dir, 'eval.json')\n\n            e = rv.EvaluatorConfig.builder(rv.CHIP_CLASSIFICATION_EVALUATOR) \\\n                                  .with_task(task) \\\n                                  .with_output_uri(output_uri) \\\n                                  .build()\n\n            e.update_for_command(rv.EVAL, create_mock_experiment())\n\n            evaluator = e.create_evaluator()\n\n            evaluator.process([scene], tmp_dir)\n\n            results = None\n            with open(output_uri) as f:\n                results = json.loads(f.read())['overall']\n\n            for result in results:\n                self.assertEqual(result['f1'], 1.0)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests/evaluation/test_class_evaluation_item.py,0,"b""import unittest\n\nfrom rastervision.evaluation import ClassEvaluationItem\n\n\nclass TestClassEvaluationItem(unittest.TestCase):\n    def setUp(self):\n        pass\n\n    def test_merge_both_empty(self):\n        a = ClassEvaluationItem()\n        b = ClassEvaluationItem()\n        a.merge(b)\n        self.assertEqual(a.precision, None)\n        self.assertEqual(a.recall, None)\n        self.assertEqual(a.f1, None)\n        self.assertEqual(a.count_error, None)\n        self.assertEqual(a.gt_count, 0)\n\n    def test_merge_first_empty(self):\n        a = ClassEvaluationItem()\n        b = ClassEvaluationItem(\n            precision=1, recall=1, f1=1, count_error=0, gt_count=1)\n        a.merge(b)\n        self.assertEqual(a.precision, 1)\n        self.assertEqual(a.recall, 1)\n        self.assertEqual(a.f1, 1)\n        self.assertEqual(a.count_error, 0)\n        self.assertEqual(a.gt_count, 1)\n\n    def test_merge_second_empty(self):\n        a = ClassEvaluationItem(\n            precision=1, recall=1, f1=1, count_error=0, gt_count=1)\n        b = ClassEvaluationItem()\n        a.merge(b)\n        self.assertEqual(a.precision, 1)\n        self.assertEqual(a.recall, 1)\n        self.assertEqual(a.f1, 1)\n        self.assertEqual(a.count_error, 0)\n        self.assertEqual(a.gt_count, 1)\n\n    def test_merge(self):\n        a = ClassEvaluationItem(\n            precision=1, recall=1, f1=1, count_error=0, gt_count=1)\n        b = ClassEvaluationItem(\n            precision=0, recall=0, f1=0, count_error=1, gt_count=2)\n        a.merge(b)\n        self.assertEqual(a.precision, 1 / 3)\n        self.assertEqual(a.recall, 1 / 3)\n        self.assertEqual(a.f1, 1 / 3)\n        self.assertEqual(a.count_error, 2 / 3)\n        self.assertEqual(a.gt_count, 3)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests/evaluation/test_classification_evaluator.py,0,"b""import unittest\n\nimport rastervision as rv\n\n\nclass TestClassificationEvaluation(unittest.TestCase):\n    def test_missing_config_class_map(self):\n        with self.assertRaises(rv.ConfigError):\n            rv.evaluation.ClassificationEvaluatorConfig.builder(\n                rv.CHIP_CLASSIFICATION_EVALUATOR).build()\n\n    def test_no_missing_config(self):\n        try:\n            rv.evaluation.ClassificationEvaluatorConfig.builder(\n                rv.CHIP_CLASSIFICATION_EVALUATOR).with_class_map(['']).build()\n        except rv.ConfigError:\n            self.fail('ConfigError raised unexpectedly')\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests/evaluation/test_object_detection_evaluation.py,0,"b""import unittest\n\nimport numpy as np\n\nfrom rastervision.evaluation import ObjectDetectionEvaluation\nfrom rastervision.core.class_map import ClassItem, ClassMap\nfrom rastervision.core.box import Box\nfrom rastervision.data.label import ObjectDetectionLabels\n\n\nclass TestObjectDetectionEvaluation(unittest.TestCase):\n    def make_class_map(self):\n        class_items = [ClassItem(1, 'car'), ClassItem(2, 'building')]\n        return ClassMap(class_items)\n\n    def make_ground_truth_labels(self):\n        size = 100\n        nw = Box.make_square(0, 0, size)\n        ne = Box.make_square(0, 200, size)\n        se = Box.make_square(200, 200, size)\n        sw = Box.make_square(200, 0, size)\n        npboxes = Box.to_npboxes([nw, ne, se, sw])\n        class_ids = np.array([1, 1, 2, 2])\n        return ObjectDetectionLabels(npboxes, class_ids)\n\n    def make_predicted_labels(self):\n        size = 100\n        # Predicted labels are only there for three of the ground truth boxes,\n        # and are offset by 10 pixels.\n        nw = Box.make_square(10, 0, size)\n        ne = Box.make_square(10, 200, size)\n        se = Box.make_square(210, 200, size)\n        npboxes = Box.to_npboxes([nw, ne, se])\n        class_ids = np.array([1, 1, 2])\n        scores = np.ones(class_ids.shape)\n        return ObjectDetectionLabels(npboxes, class_ids, scores=scores)\n\n    def test_compute(self):\n        class_map = self.make_class_map()\n        eval = ObjectDetectionEvaluation(class_map)\n        gt_labels = self.make_ground_truth_labels()\n        pred_labels = self.make_predicted_labels()\n\n        eval.compute(gt_labels, pred_labels)\n        eval_item1 = eval.class_to_eval_item[1]\n        self.assertEqual(eval_item1.gt_count, 2)\n        self.assertEqual(eval_item1.precision, 1.0)\n        self.assertEqual(eval_item1.recall, 1.0)\n        self.assertEqual(eval_item1.f1, 1.0)\n\n        eval_item2 = eval.class_to_eval_item[2]\n        self.assertEqual(eval_item2.gt_count, 2)\n        self.assertEqual(eval_item2.precision, 1.0)\n        self.assertEqual(eval_item2.recall, 0.5)\n        self.assertEqual(eval_item2.f1, 2 / 3)\n\n        avg_item = eval.avg_item\n        self.assertEqual(avg_item.gt_count, 4)\n        self.assertAlmostEqual(avg_item.precision, 1.0)\n        self.assertEqual(avg_item.recall, 0.75)\n        self.assertAlmostEqual(avg_item.f1, 0.83, places=2)\n\n    def test_compute_no_preds(self):\n        class_map = self.make_class_map()\n        eval = ObjectDetectionEvaluation(class_map)\n        gt_labels = self.make_ground_truth_labels()\n        pred_labels = ObjectDetectionLabels.make_empty()\n\n        eval.compute(gt_labels, pred_labels)\n        eval_item1 = eval.class_to_eval_item[1]\n        self.assertEqual(eval_item1.gt_count, 2)\n        self.assertEqual(eval_item1.precision, None)\n        self.assertEqual(eval_item1.recall, 0.0)\n        self.assertEqual(eval_item1.f1, None)\n\n        eval_item2 = eval.class_to_eval_item[2]\n        self.assertEqual(eval_item2.gt_count, 2)\n        self.assertEqual(eval_item2.precision, None)\n        self.assertEqual(eval_item2.recall, 0.0)\n        self.assertEqual(eval_item2.f1, None)\n\n        avg_item = eval.avg_item\n        self.assertEqual(avg_item.gt_count, 4)\n        self.assertEqual(avg_item.precision, 0.0)\n        self.assertEqual(avg_item.recall, 0.0)\n        self.assertEqual(avg_item.f1, 0.0)\n\n    def test_compute_no_ground_truth(self):\n        class_map = self.make_class_map()\n        eval = ObjectDetectionEvaluation(class_map)\n        gt_labels = ObjectDetectionLabels.make_empty()\n        pred_labels = self.make_predicted_labels()\n\n        eval.compute(gt_labels, pred_labels)\n        eval_item1 = eval.class_to_eval_item[1]\n        self.assertEqual(eval_item1.gt_count, 0)\n        self.assertEqual(eval_item1.precision, None)\n        self.assertEqual(eval_item1.recall, None)\n        self.assertEqual(eval_item1.f1, None)\n\n        eval_item2 = eval.class_to_eval_item[2]\n        self.assertEqual(eval_item2.gt_count, 0)\n        self.assertEqual(eval_item2.precision, None)\n        self.assertEqual(eval_item2.recall, None)\n        self.assertEqual(eval_item2.f1, None)\n\n        avg_item = eval.avg_item\n        self.assertEqual(avg_item.gt_count, 0)\n        self.assertEqual(avg_item.precision, None)\n        self.assertEqual(avg_item.recall, None)\n        self.assertEqual(avg_item.f1, None)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests/evaluation/test_semantic_segmentation_evaluation.py,0,"b""import unittest\n\nimport numpy as np\n\nfrom rastervision.core.class_map import (ClassItem, ClassMap)\nfrom rastervision.evaluation.semantic_segmentation_evaluation import (\n    SemanticSegmentationEvaluation)\nfrom rastervision.data.label_source.semantic_segmentation_label_source import (\n    SemanticSegmentationLabelSource)\nfrom tests.mock import MockRasterSource\nfrom tests import data_file_path\n\n\nclass TestSemanticSegmentationEvaluation(unittest.TestCase):\n    def test_compute(self):\n        class_map = ClassMap(\n            [ClassItem(id=1, name='one'),\n             ClassItem(id=2, name='two')])\n\n        # Mismatches: 0 -> 1, 2 -> 1, 1 -> 0\n        gt_array = np.ones((4, 4, 1), dtype=np.uint8)\n        gt_array[0, 0, 0] = 0\n        gt_array[2, 2, 0] = 2\n        gt_raster = MockRasterSource([0], 1)\n        gt_raster.set_raster(gt_array)\n        gt_label_source = SemanticSegmentationLabelSource(source=gt_raster)\n\n        p_array = np.ones((4, 4, 1), dtype=np.uint8)\n        p_array[1, 1, 0] = 0\n        p_raster = MockRasterSource([0], 1)\n        p_raster.set_raster(p_array)\n        p_label_source = SemanticSegmentationLabelSource(source=p_raster)\n\n        eval = SemanticSegmentationEvaluation(class_map)\n        eval.compute(gt_label_source.get_labels(), p_label_source.get_labels())\n\n        tp1 = 16 - 3  # 4*4 - 3 true positives for class 1\n        fp1 = 1  # 1 false positive (2,2) and one don't care at (0,0)\n        fn1 = 1  # one false negative (1,1)\n        precision1 = float(tp1) / (tp1 + fp1)\n        recall1 = float(tp1) / (tp1 + fn1)\n        f11 = 2 * float(precision1 * recall1) / (precision1 + recall1)\n\n        tp2 = 0  # 0 true positives for class 2\n        fn2 = 1  # one false negative (2,2)\n        precision2 = None  # float(tp2) / (tp2 + fp2) where fp2 == 0\n        recall2 = float(tp2) / (tp2 + fn2)\n        f12 = None\n\n        self.assertAlmostEqual(precision1,\n                               eval.class_to_eval_item[1].precision)\n        self.assertAlmostEqual(recall1, eval.class_to_eval_item[1].recall)\n        self.assertAlmostEqual(f11, eval.class_to_eval_item[1].f1)\n\n        self.assertEqual(precision2, eval.class_to_eval_item[2].precision)\n        self.assertAlmostEqual(recall2, eval.class_to_eval_item[2].recall)\n        self.assertAlmostEqual(f12, eval.class_to_eval_item[2].f1)\n\n        avg_conf_mat = np.array([[0, 0, 0], [1., 13, 0], [0, 1, 0]])\n        avg_recall = (14 / 15) * recall1 + (1 / 15) * recall2\n        self.assertTrue(np.array_equal(avg_conf_mat, eval.avg_item.conf_mat))\n        self.assertEqual(avg_recall, eval.avg_item.recall)\n\n    def test_compute_ignore_class(self):\n        # All ones except for a zero\n        gt_array = np.ones((4, 4, 1), dtype=np.uint8)\n        gt_array[0, 0, 0] = 0\n        gt_raster = MockRasterSource([0], 1)\n        gt_raster.set_raster(gt_array)\n        gt_label_source = SemanticSegmentationLabelSource(source=gt_raster)\n\n        # All ones\n        pred_array = np.ones((4, 4, 1), dtype=np.uint8)\n        pred_raster = MockRasterSource([0], 1)\n        pred_raster.set_raster(pred_array)\n        pred_label_source = SemanticSegmentationLabelSource(source=pred_raster)\n\n        class_map = ClassMap(\n            [ClassItem(id=0, name='ignore'),\n             ClassItem(id=1, name='one')])\n        eval = SemanticSegmentationEvaluation(class_map)\n        eval.compute(gt_label_source.get_labels(),\n                     pred_label_source.get_labels())\n        self.assertAlmostEqual(1, len(eval.class_to_eval_item))\n        self.assertAlmostEqual(1.0, eval.class_to_eval_item[1].f1)\n        self.assertAlmostEqual(1.0, eval.avg_item.f1)\n\n    def test_vector_compute(self):\n        class_map = ClassMap([ClassItem(id=1, name='one', color='#000021')])\n        gt_uri = data_file_path('3-gt-polygons.geojson')\n        pred_uri = data_file_path('3-pred-polygons.geojson')\n\n        eval = SemanticSegmentationEvaluation(class_map)\n        eval.compute_vector(gt_uri, pred_uri, 'polygons', 1)\n\n        # NOTE: The  two geojson files referenced  above contain three\n        # unique geometries total, each  file contains two geometries,\n        # and there is one geometry shared between the two.\n        tp = 1.0\n        fp = 1.0\n        fn = 1.0\n        precision = float(tp) / (tp + fp)\n        recall = float(tp) / (tp + fn)\n\n        self.assertAlmostEqual(precision, eval.class_to_eval_item[1].precision)\n        self.assertAlmostEqual(recall, eval.class_to_eval_item[1].recall)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests/evaluation/test_semantic_segmentation_evaluator.py,0,"b""import unittest\nfrom os.path import join\nimport json\n\nimport numpy as np\nfrom shapely.geometry import shape\n\nfrom rastervision.core.class_map import (ClassItem, ClassMap)\nfrom rastervision.core import Box\nfrom rastervision.data import (Scene, RasterizedSource, GeoJSONVectorSource,\n                               IdentityCRSTransformer,\n                               SemanticSegmentationLabelSource)\nfrom rastervision.data.raster_source.rasterized_source_config import (\n    RasterizedSourceConfig)\nfrom tests.mock import (MockRasterSource)\nfrom rastervision.evaluation import SemanticSegmentationEvaluator\nfrom rastervision.rv_config import RVConfig\nfrom rastervision.utils.files import file_to_str\nfrom tests import data_file_path\n\n\nclass TestSemanticSegmentationEvaluator(unittest.TestCase):\n    def setUp(self):\n        self.tmp_dir = RVConfig.get_tmp_dir()\n\n    def tearDown(self):\n        self.tmp_dir.cleanup()\n\n    def get_scene(self, class_id):\n        # Make scene where ground truth is all set to class_id\n        # and predictions are set to half 1's and half 2's\n        scene_id = str(class_id)\n        rs = MockRasterSource(channel_order=[0, 1, 2], num_channels=3)\n        rs.set_raster(np.zeros((10, 10, 3)))\n\n        gt_rs = MockRasterSource(channel_order=[0], num_channels=1)\n        gt_arr = np.full((10, 10, 1), class_id)\n        gt_rs.set_raster(gt_arr)\n        gt_ls = SemanticSegmentationLabelSource(source=gt_rs)\n\n        pred_rs = MockRasterSource(channel_order=[0], num_channels=1)\n        pred_arr = np.ones((10, 10, 1))\n        pred_arr[5:10, :, :] = 2\n        pred_rs.set_raster(pred_arr)\n        pred_ls = SemanticSegmentationLabelSource(source=pred_rs)\n\n        return Scene(scene_id, rs, gt_ls, pred_ls)\n\n    def get_vector_scene(self, class_id, use_aoi=False):\n        gt_uri = data_file_path('{}-gt-polygons.geojson'.format(class_id))\n        pred_uri = data_file_path('{}-pred-polygons.geojson'.format(class_id))\n\n        scene_id = str(class_id)\n        rs = MockRasterSource(channel_order=[0, 1, 3], num_channels=3)\n        rs.set_raster(np.zeros((10, 10, 3)))\n\n        crs_transformer = IdentityCRSTransformer()\n        extent = Box.make_square(0, 0, 360)\n\n        gt_rs = RasterizedSource(\n            GeoJSONVectorSource(gt_uri, crs_transformer),\n            RasterizedSourceConfig.RasterizerOptions(2), extent,\n            crs_transformer)\n        gt_ls = SemanticSegmentationLabelSource(source=gt_rs)\n\n        pred_rs = RasterizedSource(\n            GeoJSONVectorSource(pred_uri, crs_transformer),\n            RasterizedSourceConfig.RasterizerOptions(2), extent,\n            crs_transformer)\n        pred_ls = SemanticSegmentationLabelSource(source=pred_rs)\n        pred_ls.vector_output = [{\n            'uri': pred_uri,\n            'denoise': 0,\n            'mode': 'polygons',\n            'class_id': class_id\n        }]\n\n        if use_aoi:\n            aoi_uri = data_file_path('{}-aoi.geojson'.format(class_id))\n            aoi_geojson = json.loads(file_to_str(aoi_uri))\n            aoi_polygons = [shape(aoi_geojson['features'][0]['geometry'])]\n            return Scene(scene_id, rs, gt_ls, pred_ls, aoi_polygons)\n\n        return Scene(scene_id, rs, gt_ls, pred_ls)\n\n    def test_evaluator(self):\n        class_map = ClassMap([\n            ClassItem(id=1, name='one'),\n            ClassItem(id=2, name='two'),\n        ])\n        output_uri = join(self.tmp_dir.name, 'out.json')\n        scenes = [self.get_scene(1), self.get_scene(2)]\n        evaluator = SemanticSegmentationEvaluator(class_map, output_uri, None)\n        evaluator.process(scenes, self.tmp_dir.name)\n        eval_json = json.loads(file_to_str(output_uri))\n        exp_eval_json = json.loads(\n            file_to_str(data_file_path('expected-eval.json')))\n        self.assertDictEqual(eval_json, exp_eval_json)\n\n    def test_vector_evaluator(self):\n        class_map = ClassMap([\n            ClassItem(id=1, name='one'),\n            ClassItem(id=2, name='two'),\n        ])\n        output_uri = join(self.tmp_dir.name, 'raster-out.json')\n        vector_output_uri = join(self.tmp_dir.name, 'vector-out.json')\n        scenes = [self.get_vector_scene(1), self.get_vector_scene(2)]\n        evaluator = SemanticSegmentationEvaluator(class_map, output_uri,\n                                                  vector_output_uri)\n        evaluator.process(scenes, self.tmp_dir.name)\n        vector_eval_json = json.loads(file_to_str(vector_output_uri))\n        exp_vector_eval_json = json.loads(\n            file_to_str(data_file_path('expected-vector-eval.json')))\n        # NOTE:  The precision  and recall  values found  in the  file\n        # `expected-vector-eval.json`  are equal to fractions of  the\n        # form (n-1)/n for  n <= 7 which  can be seen to  be (and have\n        # been manually verified to be) correct.\n        self.assertDictEqual(vector_eval_json, exp_vector_eval_json)\n\n    def test_vector_evaluator_with_aoi(self):\n        class_map = ClassMap([\n            ClassItem(id=1, name='one'),\n        ])\n        output_uri = join(self.tmp_dir.name, 'raster-out.json')\n        vector_output_uri = join(self.tmp_dir.name, 'vector-out.json')\n        scenes = [self.get_vector_scene(1, use_aoi=True)]\n        evaluator = SemanticSegmentationEvaluator(class_map, output_uri,\n                                                  vector_output_uri)\n        evaluator.process(scenes, self.tmp_dir.name)\n        vector_eval_json = json.loads(file_to_str(vector_output_uri))\n        exp_vector_eval_json = json.loads(\n            file_to_str(data_file_path('expected-vector-eval-with-aoi.json')))\n\n        # NOTE:  The precision  and recall  values found  in the  file\n        # `expected-vector-eval.json`  are equal to fractions of  the\n        # form (n-1)/n for  n <= 7 which  can be seen to  be (and have\n        # been manually verified to be) correct.\n        self.assertDictEqual(vector_eval_json, exp_vector_eval_json)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests/experiment/__init__.py,0,b''
tests/experiment/test_experiment_config.py,0,"b""import unittest\n\nimport rastervision as rv\n\nfrom tests import data_file_path\n\n\n@unittest.skipIf(not rv.backend.tf_available, 'TF is not available')\nclass TestExperimentConfig(unittest.TestCase):\n    @staticmethod\n    def get_test_task():\n        task = rv.TaskConfig.builder(rv.OBJECT_DETECTION) \\\n                            .with_chip_size(300) \\\n                            .with_classes({\n                                'car': (1, 'blue'),\n                                'building': (2, 'red')}) \\\n                            .with_chip_options(neg_ratio=0.0,\n                                               ioa_thresh=1.0,\n                                               window_method='sliding') \\\n                            .with_predict_options(merge_thresh=0.1,\n                                                  score_thresh=0.5) \\\n                            .build()\n\n        return task\n\n    def get_test_backend(self):\n        task = self.get_test_task()\n        backend = rv.backend.BackendConfig.builder(rv.TF_OBJECT_DETECTION) \\\n                                          .with_task(task) \\\n                                          .with_model_defaults(rv.SSD_MOBILENET_V2_COCO) \\\n                                          .build()\n        return backend\n\n    def get_test_dataset(self):\n        dataset = rv.DatasetConfig.builder() \\\n                                  .build()\n        return dataset\n\n    def get_valid_exp_builder(self):\n        root_uri = '/some/dummy/root'\n        img_path = '/dummy.tif'\n        label_path = '/dummy.json'\n        backend_conf_path = data_file_path(\n            'tf_object_detection/'\n            'embedded_ssd_mobilenet_v1_coco.config')\n\n        pretrained_model = ('https://dummy.com/model.gz')\n\n        task = self.get_test_task()\n\n        backend = rv.BackendConfig.builder(rv.TF_OBJECT_DETECTION) \\\n                                  .with_task(task) \\\n                                  .with_template(backend_conf_path) \\\n                                  .with_pretrained_model(pretrained_model) \\\n                                  .with_train_options(sync_interval=None,\n                                                      do_monitoring=False) \\\n                                  .build()\n\n        raster_source = rv.RasterSourceConfig.builder(rv.GEOTIFF_SOURCE) \\\n                          .with_uri(img_path) \\\n                          .with_channel_order([0, 1, 2]) \\\n                          .with_stats_transformer() \\\n                          .build()\n\n        scene = rv.SceneConfig.builder() \\\n                              .with_task(task) \\\n                              .with_id('od_test') \\\n                              .with_raster_source(raster_source) \\\n                              .with_label_source(label_path) \\\n                              .build()\n\n        dataset = rv.DatasetConfig.builder() \\\n                                  .with_train_scene(scene) \\\n                                  .with_validation_scene(scene) \\\n                                  .build()\n\n        analyzer = rv.analyzer.StatsAnalyzerConfig()\n\n        return rv.ExperimentConfig.builder() \\\n                               .with_id('object-detection-test') \\\n                               .with_root_uri(root_uri) \\\n                               .with_task(task) \\\n                               .with_backend(backend) \\\n                               .with_dataset(dataset) \\\n                               .with_analyzer(analyzer) \\\n                               .with_train_key('model_name')\n\n    def test_object_detection_exp(self):\n        e = self.get_valid_exp_builder().build()\n\n        msg = e.to_proto()\n        e2 = rv.ExperimentConfig.from_proto(msg)\n\n        self.assertEqual(e.train_uri, '/some/dummy/root/train/model_name')\n        self.assertEqual(e.analyze_uri,\n                         '/some/dummy/root/analyze/object-detection-test')\n\n        self.assertEqual(e.analyze_uri, e2.analyze_uri)\n        self.assertEqual(e.chip_uri, e2.chip_uri)\n        self.assertEqual(e.train_uri, e2.train_uri)\n        self.assertEqual(e.predict_uri, e2.predict_uri)\n        self.assertEqual(e.eval_uri, e2.eval_uri)\n\n        self.assertEqual(\n            e2.dataset.train_scenes[0].label_source.vector_source.uri,\n            '/dummy.json')\n        self.assertEqual(\n            e2.dataset.train_scenes[0].raster_source.channel_order, [0, 1, 2])\n\n    def test_experiment_missing_configs_id(self):\n        task = self.get_test_task()\n\n        # missing ID\n        with self.assertRaises(rv.ConfigError):\n            rv.ExperimentConfig.builder()          \\\n                               .with_root_uri('')  \\\n                               .with_task(task)    \\\n                               .with_backend('')   \\\n                               .with_dataset('')   \\\n                               .with_analyzer('')  \\\n                               .with_train_uri('') \\\n                               .build()\n\n    def test_experiment_missing_configs_backend(self):\n        task = self.get_test_task()\n\n        # missing backend\n        with self.assertRaises(rv.ConfigError):\n            rv.ExperimentConfig.builder()          \\\n                               .with_id('')        \\\n                               .with_root_uri('')  \\\n                               .with_task(task)    \\\n                               .with_dataset('')   \\\n                               .with_analyzer('')  \\\n                               .with_train_uri('') \\\n                               .build()\n\n    def test_experiment_missing_train_key(self):\n        task = self.get_test_task()\n        # missing root_uri and other uris\n        with self.assertRaises(rv.ConfigError):\n            rv.ExperimentConfig.builder()          \\\n                               .with_id('')        \\\n                               .with_task(task)    \\\n                               .with_backend('')   \\\n                               .with_dataset('')   \\\n                               .build()\n\n    def test_experiment_missing_multiple_configs(self):\n        task = self.get_test_task()\n        # missing root_uri and dataset and analyzer\n        with self.assertRaises(rv.ConfigError):\n            rv.ExperimentConfig.builder()           \\\n                               .with_id('')         \\\n                               .with_task(task)     \\\n                               .with_backend('')    \\\n                               .with_train_uri('')  \\\n                               .with_evaluators(['']) \\\n                               .build()\n\n    def test_no_missing_config_max_with_root(self):\n        task = self.get_test_task()\n        backend = self.get_test_backend()\n        dataset = self.get_test_dataset()\n        # maximum args with root_uri\n        try:\n            rv.ExperimentConfig.builder()            \\\n                               .with_id('')          \\\n                               .with_root_uri('/dummy/root/uri')    \\\n                               .with_task(task)      \\\n                               .with_backend(backend)     \\\n                               .with_dataset(dataset)     \\\n                               .with_evaluators([''])  \\\n                               .with_analyze_uri('') \\\n                               .with_chip_uri('')    \\\n                               .with_predict_uri('') \\\n                               .with_eval_uri('')    \\\n                               .with_bundle_uri('')  \\\n                               .build()\n        except rv.ConfigError:\n            self.fail('ConfigError raised unexpectedly')\n\n    def test_no_missing_config_min_with_root(self):\n        task = self.get_test_task()\n        backend = self.get_test_backend()\n        dataset = self.get_test_dataset()\n        # minimum args with_root_uri\n        try:\n            rv.ExperimentConfig.builder()            \\\n                               .with_id('')          \\\n                               .with_evaluators([''])  \\\n                               .with_root_uri('/dummy/root/uri')    \\\n                               .with_task(task)      \\\n                               .with_backend(backend)     \\\n                               .with_dataset(dataset)     \\\n                               .build()\n        except rv.ConfigError:\n            self.fail('ConfigError raised unexpectedly')\n\n    def test_keys_are_copied(self):\n        e = self.get_valid_exp_builder()\n        e = e.with_analyze_key('a') \\\n             .with_chip_key('b') \\\n             .with_train_key('c') \\\n             .with_predict_key('d') \\\n             .with_eval_key('e') \\\n             .with_bundle_key('f') \\\n             .with_id('something')\n\n        e = e._copy()\n\n        self.assertEqual(e.analyze_key, 'a')\n        self.assertEqual(e.chip_key, 'b')\n        self.assertEqual(e.train_key, 'c')\n        self.assertEqual(e.predict_key, 'd')\n        self.assertEqual(e.eval_key, 'e')\n        self.assertEqual(e.bundle_key, 'f')\n\n    def test_incorrect_backend_type(self):\n        task = self.get_test_task()\n        dataset = self.get_test_dataset()\n        # minimum args with_root_uri\n        with self.assertRaises(rv.ConfigError):\n            rv.ExperimentConfig.builder()            \\\n                               .with_id('')          \\\n                               .with_evaluators([''])  \\\n                               .with_root_uri('/dummy/root/uri')    \\\n                               .with_task(task)      \\\n                               .with_backend('')     \\\n                               .with_dataset(dataset)     \\\n                               .build()\n\n    def test_incorrect_dataset_type(self):\n        task = self.get_test_task()\n        backend = self.get_test_backend()\n        # minimum args with_root_uri\n        with self.assertRaises(rv.ConfigError):\n            rv.ExperimentConfig.builder()            \\\n                               .with_id('')          \\\n                               .with_evaluators([''])  \\\n                               .with_root_uri('/dummy/root/uri')    \\\n                               .with_task(task)      \\\n                               .with_backend(backend)     \\\n                               .with_dataset('')     \\\n                               .build()\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests/experiment/test_experiment_loader.py,0,"b""import unittest\nimport os\n\nimport rastervision as rv\nfrom rastervision.experiment import ExperimentLoader\n\nfrom tests import data_file_path\n\n\n@unittest.skipIf(not rv.backend.tf_available, 'TF is not available')\nclass DummyExperimentSet(rv.ExperimentSet):\n    def get_base(self):\n        root_uri = '/some/dummy/root'\n        img_path = '/dummy.tif'\n        label_path = '/dummy.json'\n        backend_conf_path = data_file_path(\n            'tf_object_detection/'\n            'embedded_ssd_mobilenet_v1_coco.config')\n\n        pretrained_model = ('https://dummy.com/model.gz')\n\n        task = rv.TaskConfig.builder(rv.OBJECT_DETECTION) \\\n                            .with_chip_size(300) \\\n                            .with_classes({\n                                'car': (1, 'blue'),\n                                'building': (2, 'red')\n                            }) \\\n                            .with_chip_options(neg_ratio=0.0,\n                                               ioa_thresh=1.0,\n                                               window_method='sliding') \\\n                            .with_predict_options(merge_thresh=0.1,\n                                                  score_thresh=0.5) \\\n                            .build()\n\n        backend = rv.BackendConfig.builder(rv.TF_OBJECT_DETECTION) \\\n                                  .with_task(task) \\\n                                  .with_template(backend_conf_path) \\\n                                  .with_pretrained_model(pretrained_model) \\\n                                  .with_train_options(sync_interval=None,\n                                                      do_monitoring=False) \\\n                                  .build()\n\n        raster_source = rv.RasterSourceConfig.builder(rv.RASTERIO_SOURCE) \\\n                          .with_uri(img_path) \\\n                          .with_channel_order([0, 1, 2]) \\\n                          .with_stats_transformer() \\\n                          .build()\n\n        scene = rv.SceneConfig.builder() \\\n                              .with_task(task) \\\n                              .with_id('od_test') \\\n                              .with_raster_source(raster_source) \\\n                              .with_label_source(label_path) \\\n                              .build()\n\n        dataset = rv.DatasetConfig.builder() \\\n                                  .with_train_scene(scene) \\\n                                  .with_validation_scene(scene) \\\n                                  .build()\n\n        analyzer = rv.analyzer.StatsAnalyzerConfig()\n\n        return rv.ExperimentConfig.builder() \\\n                                  .with_root_uri(root_uri) \\\n                                  .with_task(task) \\\n                                  .with_backend(backend) \\\n                                  .with_dataset(dataset) \\\n                                  .with_analyzer(analyzer) \\\n                                  .with_train_key('model_name')\n\n    def exp_experiment_1(self):\n        return self.get_base() \\\n                   .with_id('experiment_1') \\\n                   .build()\n\n    def exp_experiment_2(self, required_param):\n        es = []\n        for i in range(0, 2):\n            es.append(self.get_base().with_id('experiment_{}_{}'.format(\n                i + 1, required_param)).build())\n        return es\n\n\n@unittest.skipIf(not rv.backend.tf_available, 'TF is not available')\nclass TestExperimentConfig(unittest.TestCase):\n    def test_load_module(self):\n        args = {'required_param': 'yes', 'dummy': 1}\n        loader = ExperimentLoader(experiment_args=args)\n        experiments, commands = loader.load_from_module(__name__)\n        self.assertEqual(len(experiments), 3)\n        e_names = set(map(lambda e: e.id, experiments))\n        self.assertEqual(\n            e_names,\n            set(['experiment_1', 'experiment_1_yes', 'experiment_2_yes']))\n\n    def test_load_file(self):\n        path = os.path.abspath(__file__)\n        args = {'required_param': 'yes', 'dummy': 1}\n        loader = ExperimentLoader(experiment_args=args)\n        experiments, commands = loader.load_from_file(path)\n        self.assertEqual(len(experiments), 3)\n        e_names = set(map(lambda e: e.id, experiments))\n        self.assertEqual(\n            e_names,\n            set(['experiment_1', 'experiment_1_yes', 'experiment_2_yes']))\n\n    def test_filter_module_by_method(self):\n        name = '*2'\n        args = {'required_param': 'x'}\n        loader = ExperimentLoader(\n            experiment_args=args, experiment_method_patterns=[name])\n        experiments, commands = loader.load_from_module(__name__)\n        e_names = set(map(lambda e: e.id, experiments))\n        self.assertEqual(e_names, set(['experiment_1_x', 'experiment_2_x']))\n\n    def test_filter_module_by_name(self):\n        name = '*2*y*'\n        args = {'required_param': 'yes'}\n        loader = ExperimentLoader(\n            experiment_args=args, experiment_name_patterns=[name])\n        experiments, commands = loader.load_from_module(__name__)\n        e_names = set(map(lambda e: e.id, experiments))\n        self.assertEqual(e_names, set(['experiment_2_yes']))\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests/mock/__init__.py,0,"b'# flake8: noqa\n\nimport rastervision as rv\n\n\nclass SupressDeepCopyMixin:\n    """"""Supress deep copy in mock objects, since we want to check mocks after processing.""""""\n\n    def __deepcopy__(self, memodict={}):\n        return self\n\n\nfrom tests.mock.task import *\nfrom tests.mock.backend import *\nfrom tests.mock.raster_source import *\nfrom tests.mock.label_source import *\nfrom tests.mock.label_store import *\nfrom tests.mock.raster_transformer import *\nfrom tests.mock.augmentor import *\nfrom tests.mock.analyzer import *\nfrom tests.mock.evaluator import *\nfrom tests.mock.command import *\nfrom tests.mock.aux_command import *\n\n\nclass MockMixin:\n    def mock_config(self):\n        return {\'PLUGINS_modules\': \'[""{}""]\'.format(__name__)}\n\n    def setUp(self):\n        config = self.mock_config()\n        rv._registry.initialize_config(config_overrides=config)\n        super().setUp()\n\n    def tearDown(self):\n        rv._registry.initialize_config()\n        super().tearDown()\n\n\ndef create_mock_scene():\n    raster_transformer_config = rv.RasterTransformerConfig.builder(MOCK_TRANSFORMER) \\\n                                                          .build()\n\n    raster_source_config = rv.RasterSourceConfig.builder(MOCK_SOURCE) \\\n                                                .with_transformer(raster_transformer_config) \\\n                                                .build()\n\n    label_source_config = rv.LabelSourceConfig.builder(MOCK_SOURCE) \\\n                                                .build()\n\n    label_store_config = rv.LabelStoreConfig.builder(MOCK_STORE) \\\n                                                .build()\n\n    return rv.SceneConfig.builder() \\\n                         .with_id(\'test\') \\\n                         .with_raster_source(raster_source_config) \\\n                         .with_label_source(label_source_config) \\\n                         .with_label_store(label_store_config) \\\n                         .build()\n\n\ndef create_mock_experiment():\n    b = rv.BackendConfig.builder(MOCK_BACKEND).build()\n    t = rv.TaskConfig.builder(MOCK_TASK).build()\n    ds = rv.DatasetConfig.builder() \\\n                         .with_test_scenes([create_mock_scene()]) \\\n                         .with_validation_scenes([create_mock_scene()]) \\\n                         .build()\n    e = rv.EvaluatorConfig.builder(MOCK_EVALUATOR).build()\n\n    return rv.ExperimentConfig.builder() \\\n                              .with_backend(b) \\\n                              .with_task(t) \\\n                              .with_dataset(ds) \\\n                              .with_evaluator(e) \\\n                              .with_root_uri(\'/dev/null\') \\\n                              .with_id(\'mock_experiment\') \\\n                              .build()\n\n\ndef register_plugin(plugin_registry):\n    plugin_registry.register_config_builder(rv.TASK, MOCK_TASK,\n                                            MockTaskConfigBuilder)\n    plugin_registry.register_config_builder(rv.BACKEND, MOCK_BACKEND,\n                                            MockBackendConfigBuilder)\n    plugin_registry.register_config_builder(rv.RASTER_SOURCE, MOCK_SOURCE,\n                                            MockRasterSourceConfigBuilder)\n    plugin_registry.register_config_builder(rv.LABEL_SOURCE, MOCK_SOURCE,\n                                            MockLabelSourceConfigBuilder)\n    plugin_registry.register_config_builder(rv.LABEL_STORE, MOCK_STORE,\n                                            MockLabelStoreConfigBuilder)\n    plugin_registry.register_config_builder(\n        rv.RASTER_TRANSFORMER, MOCK_TRANSFORMER,\n        MockRasterTransformerConfigBuilder)\n    plugin_registry.register_config_builder(rv.AUGMENTOR, MOCK_AUGMENTOR,\n                                            MockAugmentorConfigBuilder)\n    plugin_registry.register_config_builder(rv.ANALYZER, MOCK_ANALYZER,\n                                            MockAnalyzerConfigBuilder)\n    plugin_registry.register_config_builder(rv.EVALUATOR, MOCK_EVALUATOR,\n                                            MockEvaluatorConfigBuilder)\n\n    plugin_registry.register_command_config_builder(MOCK_COMMAND,\n                                                    MockCommandConfigBuilder)\n    plugin_registry.register_aux_command(MOCK_AUX_COMMAND, MockAuxCommand)\n'"
tests/mock/analyzer.py,0,"b""import unittest\nfrom unittest.mock import Mock\n\nfrom rastervision.analyzer import (Analyzer, AnalyzerConfig,\n                                   AnalyzerConfigBuilder)  # noqa\nfrom rastervision.protos.analyzer_pb2 \\\n    import AnalyzerConfig as AnalyzerConfigMsg # noqa\n\nfrom tests.mock import SupressDeepCopyMixin\n\nMOCK_ANALYZER = 'MOCK_ANALYZER'\n\n\nclass MockAnalyzer(Analyzer):\n    def __init__(self):\n        self.mock = Mock()\n\n    def process(self, training_data, tmp_dir):\n        self.mock.process(training_data, tmp_dir)\n\n\nclass MockAnalyzerConfig(SupressDeepCopyMixin, AnalyzerConfig):\n    def __init__(self):\n        super().__init__(MOCK_ANALYZER)\n        self.mock = Mock()\n\n        self.mock.to_proto.return_value = None\n        self.mock.create_analyzer.return_value = None\n        self.mock.update_for_command.return_value = None\n        self.mock.save_bundle_files.return_value = (self, [])\n        self.mock.load_bundle_files.return_value = self\n\n    def to_proto(self):\n        result = self.mock.to_proto()\n        if result is None:\n            return AnalyzerConfigMsg(analyzer_type=self.analyzer_type)\n        else:\n            return result\n\n    def create_analyzer(self):\n        result = self.mock.create_analyzer()\n        if result is None:\n            return MockAnalyzer()\n        else:\n            return result\n\n    def update_for_command(self, command_type, experiment_config,\n                           context=None):\n        super().update_for_command(command_type, experiment_config, context)\n        self.mock.update_for_command(command_type, experiment_config, context)\n\n    def report_io(self, command_type, io_def):\n        self.mock.report_io(command_type, io_def)\n\n    def save_bundle_files(self, bundle_dir):\n        return self.mock.save_bundle_files(bundle_dir)\n\n    def load_bundle_files(self, bundle_dir):\n        return self.mock.load_bundle_files(bundle_dir)\n\n\nclass MockAnalyzerConfigBuilder(SupressDeepCopyMixin, AnalyzerConfigBuilder):\n    def __init__(self, prev=None):\n        super().__init__(MockAnalyzerConfig, {})\n        self.mock = Mock()\n        self.mock.from_proto.return_value = None\n\n    def from_proto(self, msg):\n        result = self.mock.from_proto(msg)\n        if result is None:\n            return self\n        else:\n            return result\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests/mock/augmentor.py,0,"b""import unittest\nfrom unittest.mock import Mock\n\nfrom rastervision.augmentor import (Augmentor, AugmentorConfig,\n                                    AugmentorConfigBuilder)\nfrom rastervision.protos.augmentor_pb2 import AugmentorConfig as AugmentorConfigMsg\n\nfrom tests.mock import SupressDeepCopyMixin\n\nMOCK_AUGMENTOR = 'MOCK_AUGMENTOR'\n\n\nclass MockAugmentor(Augmentor):\n    def __init__(self):\n        self.mock = Mock()\n\n        self.mock.process.return_value = None\n\n    def process(self, training_data, tmp_dir):\n        result = self.mock.process(training_data, tmp_dir)\n        if result is None:\n            return training_data\n        else:\n            return result\n\n\nclass MockAugmentorConfig(SupressDeepCopyMixin, AugmentorConfig):\n    def __init__(self):\n        super().__init__(MOCK_AUGMENTOR)\n        self.mock = Mock()\n\n        self.mock.to_proto.return_value = None\n        self.mock.create_augmentor.return_value = None\n        self.mock.update_for_command.return_value = None\n\n    def to_proto(self):\n        result = self.mock.to_proto()\n        if result is None:\n            return AugmentorConfigMsg(\n                augmentor_type=self.augmentor_type, custom_config={})\n        else:\n            return result\n\n    def create_augmentor(self):\n        result = self.mock.create_augmentor()\n        if result is None:\n            return MockAugmentor()\n        else:\n            return result\n\n    def update_for_command(self, command_type, experiment_config,\n                           context=None):\n        super().update_for_command(command_type, experiment_config, context)\n        self.mock.update_for_command(command_type, experiment_config, context)\n\n    def report_io(self, command_type, io_def):\n        self.mock.report_io(command_type, io_def)\n\n\nclass MockAugmentorConfigBuilder(SupressDeepCopyMixin, AugmentorConfigBuilder):\n    def __init__(self, prev=None):\n        super().__init__(MockAugmentorConfig, {})\n        self.mock = Mock()\n\n        self.mock.from_proto.return_value = None\n\n    def from_proto(self, msg):\n        result = self.mock.from_proto(msg)\n        if result is None:\n            return self\n        else:\n            return result\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests/mock/aux_command.py,0,"b""from unittest.mock import Mock\n\nimport rastervision as rv\n\nMOCK_AUX_COMMAND = 'MOCK_AUX_COMMAND'\n\n\nclass MockAuxCommand(rv.AuxCommand):\n    command_type = MOCK_AUX_COMMAND\n    options = rv.AuxCommandOptions(\n        split_on='uris',\n        outputs=lambda conf: list(map(lambda x: x[1], conf['uris'])),\n        required_fields=['uris'])\n    mock = Mock()\n\n    def run(self, tmp_dir=None):\n        self.mock.run(tmp_dir)\n"""
tests/mock/backend.py,0,"b""import unittest\nfrom unittest.mock import Mock\n\nimport rastervision as rv\nfrom rastervision.backend import (Backend, BackendConfig, BackendConfigBuilder)\nfrom rastervision.protos.backend_pb2 import BackendConfig as BackendConfigMsg\n\nfrom tests.mock import SupressDeepCopyMixin\nfrom .task import MOCK_TASK\n\nMOCK_BACKEND = 'MOCK_BACKEND'\n\n\nclass MockBackend(Backend):\n    def __init__(self):\n        self.mock = Mock()\n\n        self.mock.predict.return_value = None\n\n    def process_scene_data(self, scene, data, tmp_dir):\n        return self.mock.process_scene_data(scene, data, tmp_dir)\n\n    def process_sceneset_results(self, training_results, validation_results,\n                                 tmp_dir):\n        return self.mock.process_sceneset_results(training_results,\n                                                  validation_results, tmp_dir)\n\n    def train(self, tmp_dir):\n        return self.mock.train(tmp_dir)\n\n    def load_model(self, tmp_dir):\n        return self.mock.load_model(tmp_dir)\n\n    def predict(self, chips, windows, tmp_dir):\n        result = self.mock.predict(chips, windows, tmp_dir)\n        if result is None:\n            return rv.data.ChipClassificationLabels()\n        else:\n            return result\n\n\nclass MockBackendConfig(SupressDeepCopyMixin, BackendConfig):\n    def __init__(self):\n        super().__init__(MOCK_BACKEND)\n        self.mock = Mock()\n\n        self.mock.to_proto.return_value = None\n        self.mock.create_backend.return_value = None\n        self.mock.update_for_command.return_value = None\n        self.mock.save_bundle_files.return_value = (self, [])\n        self.mock.load_bundle_files.return_value = self\n\n    def to_proto(self):\n        result = self.mock.to_proto()\n        if result is None:\n            return BackendConfigMsg(\n                backend_type=self.backend_type, custom_config={})\n        else:\n            return result\n\n    def create_backend(self, task_config):\n        result = self.mock.create_backend(task_config)\n        if result is None:\n            return MockBackend()\n        else:\n            return result\n\n    def update_for_command(self, command_type, experiment_config,\n                           context=None):\n        super().update_for_command(command_type, experiment_config, context)\n        self.mock.update_for_command(command_type, experiment_config, context)\n\n    def report_io(self, command_type, io_def):\n        self.mock.report_io(command_type, io_def)\n\n    def save_bundle_files(self, bundle_dir):\n        return self.mock.save_bundle_files(bundle_dir)\n\n    def load_bundle_files(self, bundle_dir):\n        return self.mock.load_bundle_files(bundle_dir)\n\n\nclass MockBackendConfigBuilder(SupressDeepCopyMixin, BackendConfigBuilder):\n    def __init__(self, prev=None):\n        super().__init__(MOCK_BACKEND, MockBackendConfig, {})\n        self.mock = Mock()\n\n        self.mock.from_proto.return_value = None\n        self.mock._applicable_tasks.return_value = None\n\n    def from_proto(self, msg):\n        result = self.mock.from_proto(msg)\n        if result is None:\n            return MockBackendConfigBuilder()\n        else:\n            return result\n\n    def _applicable_tasks(self):\n        result = self.mock._applicable_tasks\n        if result is None:\n            return [MOCK_TASK]\n        else:\n            return result\n\n    def _process_task(self, task):\n        self.mock._process_task(task)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests/mock/command.py,0,"b""import os\nfrom unittest.mock import Mock\n\nfrom google.protobuf import struct_pb2\n\nimport rastervision as rv\nfrom rastervision.command import (Command, CommandConfig, CommandConfigBuilder)\nfrom rastervision.protos.command_pb2 \\\n    import CommandConfig as CommandConfigMsg\n\nMOCK_COMMAND = 'MOCK_COMMAND'\n\n\nclass MockCommand(Command):\n    def __init__(self, command_config):\n        self.command_config = command_config\n        self.mock = Mock()\n\n    def run(self, tmp_dir=None):\n        self.mock.run(tmp_dir)\n\n\nclass MockCommandConfig(CommandConfig):\n    def __init__(self, root_uri):\n        super().__init__(MOCK_COMMAND, root_uri)\n        self.mock = Mock()\n\n        self.mock.to_proto.return_value = None\n        self.mock.create_command.return_value = None\n        self.mock.update_for_command.return_value = None\n        self.mock.report_io.return_value = None\n\n    def create_command(self, tmp_dir=None):\n        result = self.mock.to_proto()\n        if result is None:\n            retval = MockCommand(self)\n            retval.set_tmp_dir(tmp_dir)\n            return retval\n        else:\n            return result\n\n    def to_proto(self):\n        result = self.mock.to_proto()\n        if result is None:\n            msg = super().to_proto()\n            msg.MergeFrom(CommandConfigMsg(custom_config=struct_pb2.Struct()))\n\n            return msg\n        else:\n            return result\n\n    def report_io(self):\n        result = self.mock.to_proto()\n        if result is None:\n            return rv.core.CommandIODefinition()\n        else:\n            return result\n\n\nclass MockCommandConfigBuilder(CommandConfigBuilder):\n    def __init__(self, command_type, prev=None):\n        super().__init__(command_type, prev)\n\n    def validate(self):\n        super().validate()\n\n    def build(self):\n        self.validate()\n        return MockCommandConfig(self.root_uri)\n\n    def from_proto(self, msg):\n        b = super().from_proto(msg)\n\n        return b\n\n    def get_root_uri(self, experiment_config):\n        mock_key = experiment_config.custom_config.get('mock_key')\n        if not mock_key:\n            mock_uri = experiment_config.custom_config.get('mock_uri')\n            if not mock_uri:\n                raise rv.ConfigError(\n                    'MockCommand requires a mock_key or mock_uri '\n                    'be set in the experiment custom_config')\n        else:\n            mock_uri = os.path.join(experiment_config.root_uri, 'mock',\n                                    mock_key)\n\n        return mock_uri\n\n    def with_experiment(self, experiment_config):\n        b = super().with_experiment(experiment_config)\n        return b\n"""
tests/mock/evaluator.py,0,"b""import unittest\nfrom unittest.mock import Mock\n\nfrom rastervision.evaluation import (Evaluator, EvaluatorConfig,\n                                     EvaluatorConfigBuilder)\nfrom rastervision.protos.evaluator_pb2 \\\n    import EvaluatorConfig as EvaluatorConfigMsg\n\nfrom tests.mock import SupressDeepCopyMixin\n\nMOCK_EVALUATOR = 'MOCK_EVALUATOR'\n\n\nclass MockEvaluator(Evaluator):\n    def __init__(self):\n        self.mock = Mock()\n\n    def process(self, scenes, tmp_dir):\n        self.mock.process(scenes, tmp_dir)\n\n\nclass MockEvaluatorConfig(SupressDeepCopyMixin, EvaluatorConfig):\n    def __init__(self):\n        super().__init__(MOCK_EVALUATOR)\n        self.mock = Mock()\n\n        self.mock.to_proto.return_value = None\n        self.mock.create_evaluator.return_value = None\n        self.mock.update_for_command.return_value = None\n\n    def to_proto(self):\n        result = self.mock.to_proto()\n        if result is None:\n            return EvaluatorConfigMsg(evaluator_type=self.evaluator_type)\n        else:\n            return result\n\n    def create_evaluator(self):\n        result = self.mock.create_evaluator()\n        if result is None:\n            return MockEvaluator()\n        else:\n            return result\n\n    def update_for_command(self, command_type, experiment_config,\n                           context=None):\n        super().update_for_command(command_type, experiment_config, context)\n        self.mock.update_for_command(command_type, experiment_config, context)\n\n    def report_io(self, command_type, io_def):\n        self.mock.report_io(command_type, io_def)\n\n\nclass MockEvaluatorConfigBuilder(SupressDeepCopyMixin, EvaluatorConfigBuilder):\n    def __init__(self, prev=None):\n        super().__init__(MockEvaluatorConfig, {})\n        self.mock = Mock()\n        self.mock.from_proto.return_value = None\n\n    def from_proto(self, msg):\n        result = self.mock.from_proto(msg)\n        if result is None:\n            return self\n        else:\n            return result\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests/mock/label_source.py,0,"b""import unittest\nfrom unittest.mock import Mock\n\nfrom rastervision.data import (LabelSource, LabelSourceConfig,\n                               LabelSourceConfigBuilder,\n                               ChipClassificationLabels)\nfrom rastervision.protos.label_source_pb2 \\\n    import LabelSourceConfig as LabelSourceConfigMsg\n\nfrom tests.mock import SupressDeepCopyMixin\nfrom tests.mock.raster_source import MOCK_SOURCE\n\n\nclass MockLabelSource(LabelSource):\n    def __init__(self):\n        self.mock = Mock()\n\n        self.mock.get_labels.return_value = None\n\n    def get_labels(self, window=None):\n        result = self.mock.get_labels(window)\n        if result is None:\n            return ChipClassificationLabels()\n        else:\n            return result\n\n\nclass MockLabelSourceConfig(SupressDeepCopyMixin, LabelSourceConfig):\n    def __init__(self):\n        super().__init__(MOCK_SOURCE)\n        self.mock = Mock()\n\n        self.mock.to_proto.return_value = None\n        self.mock.create_source.return_value = None\n        self.mock.update_for_command.return_value = None\n\n    def to_proto(self):\n        result = self.mock.to_proto()\n        if result is None:\n            msg = super().to_proto()\n            msg.MergeFrom(LabelSourceConfigMsg(custom_config={}))\n            return msg\n        else:\n            return result\n\n    def create_source(self, task_config, extent, crs_transformer, tmp_dir):\n        result = self.mock.create_source(task_config, extent, crs_transformer,\n                                         tmp_dir)\n        if result is None:\n            return MockLabelSource()\n        else:\n            return result\n\n    def update_for_command(self, command_type, experiment_config,\n                           context=None):\n        super().update_for_command(command_type, experiment_config, context)\n        self.mock.update_for_command(command_type, experiment_config, context)\n\n    def report_io(self, command_type, io_def):\n        self.mock.report_io(command_type, io_def)\n\n\nclass MockLabelSourceConfigBuilder(SupressDeepCopyMixin,\n                                   LabelSourceConfigBuilder):\n    def __init__(self, prev=None):\n        super().__init__(MockLabelSourceConfig, {})\n        self.mock = Mock()\n\n        self.mock.from_proto.return_value = None\n\n    def from_proto(self, msg):\n        result = self.mock.from_proto(msg)\n        if result is None:\n            return self\n        else:\n            return result\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests/mock/label_store.py,0,"b""import unittest\nfrom unittest.mock import Mock\n\nfrom rastervision.data import (LabelStore, LabelStoreConfig,\n                               LabelStoreConfigBuilder,\n                               ChipClassificationLabels)\nfrom rastervision.protos.label_store_pb2 \\\n    import LabelStoreConfig as LabelStoreConfigMsg\n\nfrom tests.mock import SupressDeepCopyMixin\n\nMOCK_STORE = 'MOCK_STORE'\n\n\nclass MockLabelStore(LabelStore):\n    def __init__(self):\n        self.mock = Mock()\n\n        self.mock.get_labels.return_value = None\n        self.mock.empty_labels.return_value = None\n\n    def save(self, labels):\n        self.mock.save(labels)\n\n    def get_labels(self):\n        result = self.mock.get_labels()\n        if result is None:\n            return ChipClassificationLabels()\n        else:\n            return result\n\n    def empty_labels(self):\n        result = self.mock.empty_labels()\n        if result is None:\n            return ChipClassificationLabels()\n        else:\n            return result\n\n\nclass MockLabelStoreConfig(SupressDeepCopyMixin, LabelStoreConfig):\n    def __init__(self):\n        super().__init__(MOCK_STORE)\n        self.mock = Mock()\n\n        self.mock.to_proto.return_value = None\n        self.mock.create_store.return_value = None\n        self.mock.update_for_command.return_value = None\n        self.mock.for_prediction.return_value = None\n\n    def to_proto(self):\n        result = self.mock.to_proto()\n        if result is None:\n            msg = super().to_proto()\n            msg.MergeFrom(LabelStoreConfigMsg(custom_config={}))\n            return msg\n        else:\n            return result\n\n    def create_store(self, task_config, extent, crs_transformer, tmp_dir):\n        result = self.mock.create_store(task_config, extent, crs_transformer,\n                                        tmp_dir)\n        if result is None:\n            return MockLabelStore()\n        else:\n            return result\n\n    def update_for_command(self, command_type, experiment_config,\n                           context=None):\n        super().update_for_command(command_type, experiment_config, context)\n        self.mock.update_for_command(command_type, experiment_config, context)\n\n    def report_io(self, command_type, io_def):\n        self.mock.report_io(command_type, io_def)\n\n    def for_prediction(self, label_store_uri):\n        result = self.mock.for_prediction(label_store_uri)\n        if result is None:\n            return self\n        else:\n            return result\n\n\nclass MockLabelStoreConfigBuilder(SupressDeepCopyMixin,\n                                  LabelStoreConfigBuilder):\n    def __init__(self, prev=None):\n        super().__init__(MockLabelStoreConfig, {})\n        self.mock = Mock()\n\n        self.mock.from_proto.return_value = None\n\n    def from_proto(self, msg):\n        result = self.mock.from_proto(msg)\n        if result is None:\n            return self\n        else:\n            return result\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests/mock/raster_source.py,0,"b""import unittest\nfrom unittest.mock import Mock\nimport numpy as np\n\nfrom rastervision.core import Box\nfrom rastervision.data import (RasterSource, RasterSourceConfig,\n                               RasterSourceConfigBuilder,\n                               IdentityCRSTransformer)\nfrom rastervision.data import (ActivateMixin)\nfrom rastervision.protos.raster_source_pb2 \\\n    import RasterSourceConfig as RasterSourceConfigMsg\n\nfrom tests.mock import SupressDeepCopyMixin\n\nMOCK_SOURCE = 'MOCK_SOURCE'\n\n\nclass MockRasterSource(RasterSource, ActivateMixin):\n    def __init__(self, channel_order, num_channels, raster_transformers=[]):\n        super().__init__(channel_order, num_channels, raster_transformers)\n        self.mock = Mock()\n        self.set_return_vals()\n\n    def set_return_vals(self, raster=None):\n        self.mock.get_extent.return_value = Box.make_square(0, 0, 2)\n        self.mock.get_dtype.return_value = np.uint8\n        self.mock.get_crs_transformer.return_value = IdentityCRSTransformer()\n        self.mock._get_chip.return_value = np.random.rand(1, 2, 2, 3)\n\n        if raster is not None:\n            self.mock.get_extent.return_value = Box(0, 0, raster.shape[0],\n                                                    raster.shape[1])\n            self.mock.get_dtype.return_value = raster.dtype\n\n            def get_chip(window):\n                return raster[window.ymin:window.ymax, window.xmin:\n                              window.xmax, :]\n\n            self.mock._get_chip.side_effect = get_chip\n\n    def get_extent(self):\n        return self.mock.get_extent()\n\n    def get_dtype(self):\n        return self.mock.get_dtype()\n\n    def get_crs_transformer(self):\n        return self.mock.get_crs_transformer()\n\n    def _get_chip(self, window):\n        return self.mock._get_chip(window)\n\n    def set_raster(self, raster):\n        self.set_return_vals(raster=raster)\n\n    def _activate(self):\n        pass\n\n    def _deactivate(self):\n        pass\n\n\nclass MockRasterSourceConfig(SupressDeepCopyMixin, RasterSourceConfig):\n    def __init__(self, transformers=None, channel_order=None):\n        super().__init__(MOCK_SOURCE, transformers, channel_order)\n        self.mock = Mock()\n        self.mock.to_proto.return_value = None\n        self.mock.create_source.return_value = None\n        self.mock.update_for_command.return_value = None\n        self.mock.save_bundle_files.return_value = (self, [])\n        self.mock.load_bundle_files.return_value = self\n        self.mock.for_prediction.return_value = self\n        self.mock.create_local.return_value = self\n\n    def to_proto(self):\n        result = self.mock.to_proto()\n        if result is None:\n            msg = super().to_proto()\n            msg.MergeFrom(RasterSourceConfigMsg(custom_config={}))\n            return msg\n        else:\n            return result\n\n    def create_source(self, tmp_dir):\n        result = self.mock.create_source(tmp_dir)\n        if result is None:\n            self.mock.create_source(tmp_dir)\n            transformers = self.create_transformers()\n            return MockRasterSource(self.channel_order, 3, transformers)\n        else:\n            return result\n\n    def update_for_command(self, command_type, experiment_config,\n                           context=None):\n        super().update_for_command(command_type, experiment_config, context)\n        self.mock.update_for_command(command_type, experiment_config, context)\n\n    def report_io(self, command_type, io_def):\n        self.mock.report_io(command_type, io_def)\n\n    def save_bundle_files(self, bundle_dir):\n        return self.mock.save_bundle_files(bundle_dir)\n\n    def load_bundle_files(self, bundle_dir):\n        return self.mock.load_bundle_files(bundle_dir)\n\n    def for_prediction(self, image_uri):\n        return self.mock.for_prediction(image_uri)\n\n    def create_local(self, tmp_dir):\n        return self.mock.create_local(tmp_dir)\n\n\nclass MockRasterSourceConfigBuilder(SupressDeepCopyMixin,\n                                    RasterSourceConfigBuilder):\n    def __init__(self, prev=None):\n        super().__init__(MockRasterSourceConfig, {})\n        self.mock = Mock()\n        self.mock.from_proto.return_value = None\n\n    def from_proto(self, msg):\n        result = self.mock.from_proto(msg)\n        if result is None:\n            return super().from_proto(msg)\n        else:\n            return result\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests/mock/raster_transformer.py,0,"b""import unittest\nfrom unittest.mock import Mock\n\nfrom rastervision.data import (\n    RasterTransformerConfig, RasterTransformerConfigBuilder, RasterTransformer)\nfrom rastervision.protos.raster_transformer_pb2 \\\n    import RasterTransformerConfig as RasterTransformerConfigMsg\n\nfrom tests.mock import SupressDeepCopyMixin\n\nMOCK_TRANSFORMER = 'MOCK_TRANSFORMER'\n\n\nclass MockRasterTransformer(RasterTransformer):\n    def __init__(self):\n        self.mock = Mock()\n\n        self.mock.transform.return_value = None\n\n    def transform(self, chip, channel_order=None):\n        result = self.mock.transform(chip, channel_order)\n        if result is None:\n            return chip\n        else:\n            return result\n\n\nclass MockRasterTransformerConfig(SupressDeepCopyMixin,\n                                  RasterTransformerConfig):\n    def __init__(self):\n        super().__init__(MOCK_TRANSFORMER)\n        self.mock = Mock()\n\n        self.mock.to_proto.return_value = None\n        self.mock.create_transformer.return_value = None\n        self.mock.update_for_command.return_value = None\n        self.mock.save_bundle_files.return_value = (self, [])\n        self.mock.load_bundle_files.return_value = self\n        self.mock.for_prediction.return_value = self\n        self.mock.create_local.return_value = self\n\n    def to_proto(self):\n        result = self.mock.to_proto()\n        if result is None:\n            msg = RasterTransformerConfigMsg(\n                transformer_type=self.transformer_type, custom_config={})\n            return msg\n        else:\n            return result\n\n    def create_transformer(self):\n        result = self.mock.create_transformer()\n        if result is None:\n            return MockRasterTransformer()\n        else:\n            return result\n\n    def update_for_command(self, command_type, experiment_config,\n                           context=None):\n        super().update_for_command(command_type, experiment_config, context)\n        self.mock.update_for_command(command_type, experiment_config, context)\n\n    def report_io(self, command_type, io_def):\n        self.mock.report_io(command_type, io_def)\n\n    def save_bundle_files(self, bundle_dir):\n        return self.mock.save_bundle_files(bundle_dir)\n\n    def load_bundle_files(self, bundle_dir):\n        return self.mock.load_bundle_files(bundle_dir)\n\n    def for_prediction(self, image_uri):\n        return self.mock.for_prediction(image_uri)\n\n    def create_local(self, tmp_dir):\n        return self.mock.create_local(tmp_dir)\n\n\nclass MockRasterTransformerConfigBuilder(SupressDeepCopyMixin,\n                                         RasterTransformerConfigBuilder):\n    def __init__(self, prev=None):\n        super().__init__(MockRasterTransformerConfig, {})\n        self.mock = Mock()\n\n        self.mock.from_proto.return_value = None\n\n    def from_proto(self, msg):\n        result = self.mock.from_proto(msg)\n        if result is None:\n            mock_config_builder = super().from_proto(msg)\n            if not mock_config_builder and msg.transformer_type == 'MOCK_TRANSFORMER':\n                mock_config_builder = MockRasterTransformerConfigBuilder()\n            return mock_config_builder\n        else:\n            return result\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests/mock/task.py,0,"b""import unittest\nfrom unittest.mock import Mock\n\nfrom rastervision.task import (Task, TaskConfig, TaskConfigBuilder)\nfrom rastervision.protos.task_pb2 import TaskConfig as TaskConfigMsg\n\nfrom tests.mock import SupressDeepCopyMixin\n\nMOCK_TASK = 'MOCK_TASK'\n\n\nclass MockTask(Task):\n    def __init__(self, task_config, backend):\n        self.config = task_config\n        self.backend = backend\n        self.mock = Mock()\n\n        self.mock.get_train_windows.return_value = None\n        self.mock.get_train_labels.return_value = None\n        self.mock.get_predict_windows.return_value = None\n\n    def get_train_windows(self, scene):\n        result = self.mock.get_train_windows(scene)\n        if result is None:\n            return []\n        else:\n            return result\n\n    def get_train_labels(self, window, scene):\n        result = self.mock.get_train_labels(window, scene)\n        if result is None:\n            return []\n        else:\n            return result\n\n    def post_process_predictions(self, labels, scene):\n        return self.mock.post_process_predictions(labels, scene)\n\n    def get_predict_windows(self, extent):\n        result = self.mock.get_predict_windows(extent)\n        if result is None:\n            return []\n        else:\n            return result\n\n    def save_debug_predict_image(self, scene, debug_dir_uri):\n        return self.mock.save_debug_predict_image(scene, debug_dir_uri)\n\n\nclass MockTaskConfig(SupressDeepCopyMixin, TaskConfig):\n    def __init__(self):\n        super().__init__(MOCK_TASK)\n        self.mock = Mock()\n\n        self.mock.create_task.return_value = None\n        self.mock.to_proto.return_value = None\n        self.mock.update_for_command.return_value = None\n        self.mock.save_bundle_files.return_value = (self, [])\n        self.mock.load_bundle_files.return_value = self\n\n    def create_task(self, backend):\n        result = self.mock.create_task(backend)\n        if result is None:\n            return MockTask(self, backend)\n        else:\n            return result\n\n    def to_proto(self):\n        result = self.mock.to_proto()\n        if result is None:\n            return TaskConfigMsg(task_type=self.task_type, custom_config={})\n        else:\n            return result\n\n    def save_bundle_files(self, bundle_dir):\n        return self.mock.save_bundle_files(bundle_dir)\n\n    def load_bundle_files(self, bundle_dir):\n        return self.mock.load_bundle_files(bundle_dir)\n\n    def update_for_command(self, command_type, experiment_config,\n                           context=None):\n        super().update_for_command(command_type, experiment_config, context)\n        self.mock.update_for_command(command_type, experiment_config, context)\n\n    def report_io(self, command_type, io_def):\n        self.mock.report_io(command_type, io_def)\n\n        # Have input always be this file, and output be a non-existant file,\n        # so commands always run\n        super().report_io(command_type, io_def)\n        io_def.add_inputs([__file__])\n        io_def.add_outputs(['{}{}'.format(__file__, 'xxxx')])\n\n\nclass MockTaskConfigBuilder(SupressDeepCopyMixin, TaskConfigBuilder):\n    def __init__(self, prev=None):\n        super().__init__(MockTaskConfig, {})\n        self.mock = Mock()\n\n        self.mock.from_proto.return_value = None\n\n    def from_proto(self, msg):\n        result = self.mock.from_proto(msg)\n        if result is None:\n            return MockTaskConfigBuilder()\n        else:\n            return result\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests/mock/test_mocks.py,0,"b'import unittest\nimport tempfile\n\nimport rastervision as rv\nfrom rastervision.core import Box\n\nimport tests.mock as mk\n\n\nclass TestMocks(mk.MockMixin, unittest.TestCase):\n    def test_mocks(self):\n        """"""Test to ensure all mocks are working as expected.""""""\n        task_config = rv.TaskConfig.builder(mk.MOCK_TASK) \\\n                                   .build()\n\n        backend_config = rv.BackendConfig.builder(mk.MOCK_BACKEND) \\\n                                         .build()\n\n        raster_transformer_config = rv.RasterTransformerConfig.builder(\n            mk.MOCK_TRANSFORMER).build()\n\n        raster_source_config = rv.RasterSourceConfig.builder(mk.MOCK_SOURCE) \\\n                                                    .with_transformer(\n                                                        raster_transformer_config) \\\n                                                    .build()\n\n        label_source_config = rv.LabelSourceConfig.builder(mk.MOCK_SOURCE) \\\n                                                    .build()\n\n        label_store_config = rv.LabelStoreConfig.builder(mk.MOCK_STORE) \\\n                                                    .build()\n\n        scene_config = rv.SceneConfig.builder() \\\n                                     .with_id(\'test\') \\\n                                     .with_raster_source(raster_source_config) \\\n                                     .with_label_source(label_source_config) \\\n                                     .with_label_store(label_store_config) \\\n                                     .build()\n\n        augmentor_config = rv.AugmentorConfig.builder(mk.MOCK_AUGMENTOR) \\\n                                             .build()\n\n        dataset = rv.DatasetConfig.builder() \\\n                                  .with_train_scene(scene_config) \\\n                                  .with_validation_scene(scene_config) \\\n                                  .with_augmentor(augmentor_config)  \\\n                                  .build()\n\n        analyzer_config = rv.AnalyzerConfig.builder(mk.MOCK_ANALYZER).build()\n        evaluator_config = rv.EvaluatorConfig.builder(\n            mk.MOCK_EVALUATOR).build()\n\n        # Create entities from configuration\n\n        backend = backend_config.create_backend(task_config)\n        task = task_config.create_task(backend)\n        scene = scene_config.create_scene(task_config, \'.\')\n        _ = augmentor_config.create_augmentor()  # noqa\n        _ = analyzer_config.create_analyzer()  # noqa\n        _ = evaluator_config.create_evaluator()  # noqa\n\n        # Assert some things\n\n        task_config.mock.create_task.assert_called_once_with(backend)\n        self.assertEqual(task.get_predict_windows(Box(0, 0, 1, 1)), [])\n\n        _ = scene.raster_source.get_chip(Box(0, 0, 1, 1))  # noqa\n        self.assertTrue(\n            scene.raster_source.raster_transformers[0].mock.transform.called)\n\n        # Create and run experiment\n\n        with tempfile.TemporaryDirectory() as tmp_dir:\n            e = rv.ExperimentConfig.builder() \\\n                                   .with_task(task_config) \\\n                                   .with_backend(backend_config) \\\n                                   .with_dataset(dataset) \\\n                                   .with_analyzer(analyzer_config) \\\n                                   .with_evaluator(evaluator_config) \\\n                                   .with_root_uri(tmp_dir) \\\n                                   .with_custom_config({\'mock_key\': \'mock\'}) \\\n                                   .with_id(\'test\') \\\n                                   .build()\n\n            rv.ExperimentRunner.get_runner(rv.LOCAL).run(e, splits=7)\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
tests/runner/test_aws_batch_experiment_runner.py,0,"b""import unittest\nfrom unittest.mock import Mock\n\nimport rastervision as rv\nfrom rastervision.rv_config import RVConfig\nfrom rastervision.runner import AwsBatchExperimentRunner\n\nimport tests.mock as mk\n\n\nclass MockAwsBatchExperimentRunner(AwsBatchExperimentRunner):\n    def __init__(self):\n        super().__init__()\n\n        self.mock_client = Mock()\n        self.mock_client.submit_job.return_value = {'jobId': 'MOCK'}\n\n    def _get_boto_client(self):\n        return self.mock_client\n\n\nclass TestAwsBatchExperimentRunner(mk.MockMixin, unittest.TestCase):\n    def test_respects_utilizes_gpu(self):\n        config = self.mock_config()\n        config['AWS_BATCH_job_queue'] = 'GPU_JOB_QUEUE'\n        config['AWS_BATCH_job_definition'] = 'GPU_JOB_DEF'\n        config['AWS_BATCH_cpu_job_queue'] = 'CPU_JOB_QUEUE'\n        config['AWS_BATCH_cpu_job_definition'] = 'CPU_JOB_DEF'\n\n        rv._registry.initialize_config(config_overrides=config)\n\n        with RVConfig.get_tmp_dir() as tmp_dir:\n            e = mk.create_mock_experiment().to_builder() \\\n                                           .with_root_uri(tmp_dir) \\\n                                           .clear_command_uris() \\\n                                           .build()\n\n            runner = MockAwsBatchExperimentRunner()\n\n            runner.run(\n                e, commands_to_run=[rv.CHIP, rv.TRAIN, rv.PREDICT, rv.EVAL])\n\n            submit_args = runner.mock_client.submit_job.call_args_list\n\n            self.assertEqual(len(submit_args), 4)\n\n            for args in submit_args:\n                jobName, jobQueue = args[1]['jobName'], args[1]['jobQueue']\n\n                if 'EVAL' in jobName or 'CHIP' in jobName:\n                    self.assertTrue('CPU' in jobQueue)\n                else:\n                    self.assertTrue('GPU' in jobQueue)\n"""
tests/task/__init__.py,0,b''
tests/task/test_chip_classification.py,0,"b""import os\nimport unittest\n\nimport rastervision as rv\nfrom rastervision.rv_config import RVConfig\n\nfrom tests import data_file_path\n\n\n@unittest.skipIf(not rv.backend.tf_available, 'TF is not available')\nclass TestChipClassification(unittest.TestCase):\n    def test_make_predict_windows_with_aoi(self):\n        task_config = rv.TaskConfig.builder(rv.CHIP_CLASSIFICATION) \\\n                                   .with_chip_size(200) \\\n                                   .with_classes(['car', 'building', 'background']) \\\n                                   .build()\n\n        backend_config = rv.BackendConfig.builder(rv.KERAS_CLASSIFICATION) \\\n                                         .with_task(task_config) \\\n                                         .with_model_defaults(rv.RESNET50_IMAGENET) \\\n                                         .with_pretrained_model(None) \\\n                                         .build()\n\n        label_source_uri = data_file_path('evaluator/cc-label-full.json')\n        label_source = rv.LabelSourceConfig.builder(rv.CHIP_CLASSIFICATION_GEOJSON) \\\n                                           .with_uri(label_source_uri) \\\n                                           .build()\n\n        label_source_2_uri = data_file_path('evaluator/cc-label-filtered.json')\n        label_source_2 = rv.LabelSourceConfig.builder(rv.CHIP_CLASSIFICATION_GEOJSON) \\\n                                           .with_uri(label_source_2_uri) \\\n                                           .build()\n\n        source_uri = data_file_path('evaluator/cc-label-img-blank.tif')\n        raster_source = rv.RasterSourceConfig.builder(rv.RASTERIO_SOURCE) \\\n                                             .with_uri(source_uri) \\\n                                             .build()\n\n        aoi_uri = data_file_path('evaluator/cc-label-aoi.json')\n        s = rv.SceneConfig.builder() \\\n                          .with_id('test') \\\n                          .with_raster_source(raster_source) \\\n                          .with_label_source(label_source) \\\n                          .with_aoi_uri(aoi_uri) \\\n                          .build()\n\n        with RVConfig.get_tmp_dir() as tmp_dir:\n            scene = s.create_scene(task_config, tmp_dir)\n            backend = backend_config.create_backend(task_config)\n            task = task_config.create_task(backend)\n\n            with scene.activate():\n                windows = task.get_train_windows(scene)\n\n            from rastervision.data import (ChipClassificationLabels,\n                                           ChipClassificationGeoJSONStore)\n            labels = ChipClassificationLabels()\n            for w in windows:\n                labels.set_cell(w, 1)\n            store = ChipClassificationGeoJSONStore(\n                os.path.join(tmp_dir, 'test.json'),\n                scene.raster_source.get_crs_transformer(),\n                task_config.class_map)\n            store.save(labels)\n\n            ls = label_source_2.create_source(\n                task_config, scene.raster_source.get_extent(),\n                scene.raster_source.get_crs_transformer(), tmp_dir)\n            actual = ls.get_labels().get_cells()\n\n            self.assertEqual(len(windows), len(actual))\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests/task/test_chip_classification_config.py,0,"b""import unittest\nfrom google.protobuf import json_format\nimport json\n\nimport rastervision as rv\nfrom rastervision.core.class_map import ClassItem\nfrom rastervision.protos.task_pb2 import TaskConfig as TaskConfigMsg\nfrom rastervision.protos.class_item_pb2 import ClassItem as ClassItemMsg\n\n\nclass TestChipClassificationConfig(unittest.TestCase):\n    def test_build_task(self):\n        classes = ['one', 'two']\n        expected = [ClassItem(1, 'one'), ClassItem(2, 'two')]\n\n        t = rv.TaskConfig.builder(rv.CHIP_CLASSIFICATION) \\\n                   .with_classes(classes) \\\n                   .build()\n\n        self.assertEqual(t.task_type, rv.CHIP_CLASSIFICATION)\n        self.assertListEqual(t.class_map.get_items(), expected)\n\n    def test_build_task_from_proto(self):\n        task_config = {\n            'task_type': rv.CHIP_CLASSIFICATION,\n            'chip_classification_config': {\n                'chip_size':\n                500,\n                'class_items': [{\n                    'id': 1,\n                    'name': 'car',\n                    'color': 'red'\n                }, {\n                    'id': 2,\n                    'name': 'building',\n                    'color': 'blue'\n                }, {\n                    'id': 3,\n                    'name': 'background',\n                    'color': 'black'\n                }]\n            }\n        }\n        msg = json_format.Parse(json.dumps(task_config), TaskConfigMsg())\n\n        task = rv.TaskConfig.from_proto(msg)\n\n        self.assertEqual(task.class_map.get_by_name('building').id, 2)\n        self.assertEqual(task.chip_size, 500)\n\n    def test_create_proto_from_task(self):\n        t = rv.TaskConfig.builder(rv.CHIP_CLASSIFICATION) \\\n                         .with_classes(['car', 'boat']) \\\n                         .with_chip_size(500) \\\n                         .build()\n\n        msg = t.to_proto()\n\n        expected_classes = [\n            ClassItemMsg(name='car', id=1),\n            ClassItemMsg(name='boat', id=2)\n        ]\n\n        self.assertEqual(msg.task_type, rv.CHIP_CLASSIFICATION)\n        self.assertEqual(msg.chip_classification_config.chip_size, 500)\n\n        actual_class_items = dict(\n            [(i.id, i) for i in msg.chip_classification_config.class_items])\n        expected_class_items = dict([(i.id, i) for i in expected_classes])\n\n        self.assertDictEqual(actual_class_items, expected_class_items)\n\n    def test_missing_config_class_map(self):\n        with self.assertRaises(rv.ConfigError):\n            rv.TaskConfig.builder(rv.CHIP_CLASSIFICATION).build()\n\n    def test_no_missing_config(self):\n        try:\n            rv.TaskConfig.builder(rv.CHIP_CLASSIFICATION).with_classes(\n                ['car']).build()\n        except rv.ConfigError:\n            self.fail('ConfigError raised unexpectedly')\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests/task/test_object_detection_config.py,0,"b""import unittest\nfrom google.protobuf import json_format\nimport json\n\nimport rastervision as rv\nfrom rastervision.core.class_map import ClassItem\nfrom rastervision.protos.task_pb2 import TaskConfig as TaskConfigMsg\nfrom rastervision.protos.class_item_pb2 import ClassItem as ClassItemMsg\n\n\nclass TestObjectDetectionConfig(unittest.TestCase):\n    def test_build_task(self):\n        classes = ['one', 'two']\n        expected = [ClassItem(1, 'one'), ClassItem(2, 'two')]\n\n        t = rv.TaskConfig.builder(rv.OBJECT_DETECTION) \\\n                   .with_classes(classes) \\\n                   .build()\n\n        self.assertEqual(t.task_type, rv.OBJECT_DETECTION)\n        self.assertListEqual(t.class_map.get_items(), expected)\n\n    def test_build_task_from_proto(self):\n        task_config = {\n            'task_type': rv.OBJECT_DETECTION,\n            'object_detection_config': {\n                'chip_size':\n                500,\n                'class_items': [{\n                    'id': 1,\n                    'name': 'car',\n                    'color': 'red'\n                }, {\n                    'id': 2,\n                    'name': 'building',\n                    'color': 'blue'\n                }, {\n                    'id': 3,\n                    'name': 'background',\n                    'color': 'black'\n                }]\n            }\n        }\n        msg = json_format.Parse(json.dumps(task_config), TaskConfigMsg())\n        task = rv.TaskConfig.from_proto(msg)\n\n        self.assertEqual(task.class_map.get_by_name('building').id, 2)\n        self.assertEqual(task.chip_size, 500)\n\n    def test_create_proto_from_task(self):\n        t = rv.TaskConfig.builder(rv.OBJECT_DETECTION) \\\n                         .with_classes(['car', 'boat']) \\\n                         .with_chip_size(500) \\\n                         .build()\n\n        msg = t.to_proto()\n\n        expected_classes = [\n            ClassItemMsg(name='car', id=1),\n            ClassItemMsg(name='boat', id=2)\n        ]\n\n        self.assertEqual(msg.task_type, rv.OBJECT_DETECTION)\n        self.assertEqual(msg.object_detection_config.chip_size, 500)\n\n        actual_class_items = dict(\n            [(i.id, i) for i in msg.object_detection_config.class_items])\n        expected_class_items = dict([(i.id, i) for i in expected_classes])\n\n        self.assertDictEqual(actual_class_items, expected_class_items)\n\n    def test_missing_config_class_map(self):\n        with self.assertRaises(rv.ConfigError):\n            rv.TaskConfig.builder(rv.OBJECT_DETECTION).build()\n\n    def test_no_missing_config(self):\n        try:\n            rv.TaskConfig.builder(rv.OBJECT_DETECTION).with_classes(\n                ['car']).build()\n        except rv.ConfigError:\n            self.fail('ConfigError raised unexpectedly')\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests/task/test_semantic_segmentation_config.py,0,"b""import unittest\nfrom google.protobuf import json_format\nimport json\n\nimport rastervision as rv\nfrom rastervision.core.class_map import ClassItem\nfrom rastervision.protos.task_pb2 import TaskConfig as TaskConfigMsg\nfrom rastervision.protos.class_item_pb2 import ClassItem as ClassItemMsg\n\n\nclass TestSemanticSegmentationConfig(unittest.TestCase):\n    def test_build_task(self):\n        classes = ['one', 'two']\n        expected = [ClassItem(1, 'one'), ClassItem(2, 'two')]\n\n        t = rv.TaskConfig.builder(rv.SEMANTIC_SEGMENTATION) \\\n                         .with_classes(classes) \\\n                         .with_chip_options(target_classes=[1, 2]) \\\n                         .build()\n\n        self.assertEqual(t.task_type, rv.SEMANTIC_SEGMENTATION)\n        self.assertListEqual(t.class_map.get_items(), expected)\n        self.assertListEqual(t.chip_options.target_classes, [1, 2])\n\n    def test_build_task_from_proto(self):\n        task_config = {\n            'task_type': rv.SEMANTIC_SEGMENTATION,\n            'semantic_segmentation_config': {\n                'chip_options': {\n                    'debug_chip_probability': 0.75\n                },\n                'chip_size':\n                500,\n                'class_items': [{\n                    'id': 1,\n                    'name': 'car',\n                    'color': 'red'\n                }, {\n                    'id': 2,\n                    'name': 'building',\n                    'color': 'blue'\n                }]\n            }\n        }\n        msg = json_format.Parse(json.dumps(task_config), TaskConfigMsg())\n        task = rv.TaskConfig.from_proto(msg)\n\n        self.assertEqual(task.class_map.get_by_name('building').id, 2)\n        self.assertEqual(task.chip_size, 500)\n        self.assertEqual(task.chip_options.debug_chip_probability, 0.75)\n\n    def test_create_proto_from_task(self):\n        t = rv.TaskConfig.builder(rv.SEMANTIC_SEGMENTATION) \\\n                         .with_classes(['car', 'boat']) \\\n                         .with_chip_size(500) \\\n                         .build()\n\n        msg = t.to_proto()\n\n        expected_classes = [\n            ClassItemMsg(name='car', id=1),\n            ClassItemMsg(name='boat', id=2)\n        ]\n\n        self.assertEqual(msg.task_type, rv.SEMANTIC_SEGMENTATION)\n        self.assertEqual(msg.semantic_segmentation_config.chip_size, 500)\n\n        actual_class_items = dict(\n            [(i.id, i) for i in msg.semantic_segmentation_config.class_items])\n        expected_class_items = dict([(i.id, i) for i in expected_classes])\n\n        self.assertDictEqual(actual_class_items, expected_class_items)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests/utils/__init__.py,0,b''
tests/utils/test_files.py,0,"b'import os\nimport unittest\nfrom unittest.mock import patch\nimport json\nimport datetime\nimport gzip\n\nimport boto3\nfrom moto import mock_s3\n\nimport rastervision as rv\nfrom rastervision.utils.files import (\n    file_to_str, str_to_file, download_if_needed, upload_or_copy,\n    load_json_config, ProtobufParseException, make_dir, get_local_path,\n    file_exists, sync_from_dir, sync_to_dir, list_paths, get_cached_file)\nfrom rastervision.filesystem import (NotReadableError, NotWritableError)\nfrom rastervision.filesystem.filesystem import FileSystem\nfrom rastervision.protos.task_pb2 import TaskConfig as TaskConfigMsg\nfrom rastervision.rv_config import RVConfig\n\nLOREM = """""" Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do\n        eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut\n        enim ad minim veniam, quis nostrud exercitation ullamco\n        laboris nisi ut aliquip ex ea commodo consequat. Duis aute\n        irure dolor in reprehenderit in voluptate velit esse cillum\n        dolore eu fugiat nulla pariatur. Excepteur sint occaecat\n        cupidatat non proident, sunt in culpa qui officia deserunt\n        mollit anim id est laborum.  """"""\n\n\nclass TestMakeDir(unittest.TestCase):\n    def setUp(self):\n        self.lorem = LOREM\n\n        # Mock S3 bucket\n        self.mock_s3 = mock_s3()\n        self.mock_s3.start()\n        self.s3 = boto3.client(\'s3\')\n        self.bucket_name = \'mock_bucket\'\n        self.s3.create_bucket(Bucket=self.bucket_name)\n\n        # Temporary directory\n        self.temp_dir = RVConfig.get_tmp_dir()\n\n    def tearDown(self):\n        self.temp_dir.cleanup()\n        self.mock_s3.stop()\n\n    def test_default_args(self):\n        dir = os.path.join(self.temp_dir.name, \'hello\')\n        make_dir(dir)\n        self.assertTrue(os.path.isdir(dir))\n\n    def test_file_exists_local_true(self):\n        path = os.path.join(self.temp_dir.name, \'lorem\', \'ipsum.txt\')\n        directory = os.path.dirname(path)\n        make_dir(directory, check_empty=False)\n\n        str_to_file(self.lorem, path)\n\n        self.assertTrue(file_exists(path))\n\n    def test_file_exists_local_false(self):\n        path = os.path.join(self.temp_dir.name, \'hello\', \'hello.txt\')\n        directory = os.path.dirname(path)\n        make_dir(directory, check_empty=False)\n\n        self.assertFalse(file_exists(path))\n\n    def test_file_exists_s3_true(self):\n        path = os.path.join(self.temp_dir.name, \'lorem\', \'ipsum.txt\')\n        directory = os.path.dirname(path)\n        make_dir(directory, check_empty=False)\n\n        str_to_file(self.lorem, path)\n\n        s3_path = \'s3://{}/lorem.txt\'.format(self.bucket_name)\n        upload_or_copy(path, s3_path)\n\n        self.assertTrue(file_exists(s3_path))\n\n    def test_file_exists_s3_false(self):\n        s3_path = \'s3://{}/hello.txt\'.format(self.bucket_name)\n        self.assertFalse(file_exists(s3_path))\n\n    def test_check_empty(self):\n        path = os.path.join(self.temp_dir.name, \'hello\', \'hello.txt\')\n        dir = os.path.dirname(path)\n        str_to_file(\'hello\', path)\n\n        make_dir(dir, check_empty=False)\n        with self.assertRaises(Exception):\n            make_dir(dir, check_empty=True)\n\n    def test_force_empty(self):\n        path = os.path.join(self.temp_dir.name, \'hello\', \'hello.txt\')\n        dir = os.path.dirname(path)\n        str_to_file(\'hello\', path)\n\n        make_dir(dir, force_empty=False)\n        self.assertTrue(os.path.isfile(path))\n        make_dir(dir, force_empty=True)\n        is_empty = len(os.listdir(dir)) == 0\n        self.assertTrue(is_empty)\n\n    def test_use_dirname(self):\n        path = os.path.join(self.temp_dir.name, \'hello\', \'hello.txt\')\n        dir = os.path.dirname(path)\n        make_dir(path, use_dirname=True)\n        self.assertTrue(os.path.isdir(dir))\n\n\nclass TestGetLocalPath(unittest.TestCase):\n    def test_local(self):\n        download_dir = \'/download_dir\'\n        uri = \'/my/file.txt\'\n        path = get_local_path(uri, download_dir)\n        self.assertEqual(path, uri)\n\n    def test_s3(self):\n        download_dir = \'/download_dir\'\n        uri = \'s3://bucket/my/file.txt\'\n        path = get_local_path(uri, download_dir)\n        self.assertEqual(path, \'/download_dir/s3/bucket/my/file.txt\')\n\n    def test_http(self):\n        download_dir = \'/download_dir\'\n        uri = \'http://bucket/my/file.txt\'\n        path = get_local_path(uri, download_dir)\n        self.assertEqual(path, \'/download_dir/http/bucket/my/file.txt\')\n\n        # simulate a zxy tile URI\n        uri = \'http://bucket/10/25/53?auth=426753\'\n        path = get_local_path(uri, download_dir)\n        self.assertEqual(path, \'/download_dir/http/bucket/10/25/53\')\n\n\nclass TestFileToStr(unittest.TestCase):\n    """"""Test file_to_str and str_to_file.""""""\n\n    def setUp(self):\n        # Setup mock S3 bucket.\n        self.mock_s3 = mock_s3()\n        self.mock_s3.start()\n        self.s3 = boto3.client(\'s3\')\n        self.bucket_name = \'mock_bucket\'\n        self.s3.create_bucket(Bucket=self.bucket_name)\n\n        self.content_str = \'hello\'\n        self.file_name = \'hello.txt\'\n        self.s3_path = \'s3://{}/{}\'.format(self.bucket_name, self.file_name)\n\n        self.temp_dir = RVConfig.get_tmp_dir()\n        self.local_path = os.path.join(self.temp_dir.name, self.file_name)\n\n    def tearDown(self):\n        self.temp_dir.cleanup()\n        self.mock_s3.stop()\n\n    def test_file_to_str_local(self):\n        str_to_file(self.content_str, self.local_path)\n        content_str = file_to_str(self.local_path)\n        self.assertEqual(self.content_str, content_str)\n\n        wrong_path = \'/wrongpath/x.txt\'\n        with self.assertRaises(NotReadableError):\n            file_to_str(wrong_path)\n\n    def test_file_to_str_s3(self):\n        wrong_path = \'s3://wrongpath/x.txt\'\n\n        with self.assertRaises(NotWritableError):\n            str_to_file(self.content_str, wrong_path)\n\n        str_to_file(self.content_str, self.s3_path)\n        content_str = file_to_str(self.s3_path)\n        self.assertEqual(self.content_str, content_str)\n\n        with self.assertRaises(NotReadableError):\n            file_to_str(wrong_path)\n\n\nclass TestDownloadIfNeeded(unittest.TestCase):\n    """"""Test download_if_needed and upload_or_copy and str_to_file.""""""\n\n    def setUp(self):\n        # Setup mock S3 bucket.\n        self.mock_s3 = mock_s3()\n        self.mock_s3.start()\n        self.s3 = boto3.client(\'s3\')\n        self.bucket_name = \'mock_bucket\'\n        self.s3.create_bucket(Bucket=self.bucket_name)\n\n        self.content_str = \'hello\'\n        self.file_name = \'hello.txt\'\n        self.s3_path = \'s3://{}/{}\'.format(self.bucket_name, self.file_name)\n\n        self.temp_dir = RVConfig.get_tmp_dir()\n        self.local_path = os.path.join(self.temp_dir.name, self.file_name)\n\n    def tearDown(self):\n        self.temp_dir.cleanup()\n        self.mock_s3.stop()\n\n    def test_download_if_needed_local(self):\n        with self.assertRaises(NotReadableError):\n            download_if_needed(self.local_path, self.temp_dir.name)\n\n        str_to_file(self.content_str, self.local_path)\n        upload_or_copy(self.local_path, self.local_path)\n        local_path = download_if_needed(self.local_path, self.temp_dir.name)\n        self.assertEqual(local_path, self.local_path)\n\n    def test_download_if_needed_s3(self):\n        with self.assertRaises(NotReadableError):\n            download_if_needed(self.s3_path, self.temp_dir.name)\n\n        str_to_file(self.content_str, self.local_path)\n        upload_or_copy(self.local_path, self.s3_path)\n        local_path = download_if_needed(self.s3_path, self.temp_dir.name)\n        content_str = file_to_str(local_path)\n        self.assertEqual(self.content_str, content_str)\n\n        wrong_path = \'s3://wrongpath/x.txt\'\n        with self.assertRaises(NotWritableError):\n            upload_or_copy(local_path, wrong_path)\n\n\nclass TestLoadJsonConfig(unittest.TestCase):\n    def setUp(self):\n        self.file_name = \'config.json\'\n        self.temp_dir = RVConfig.get_tmp_dir()\n        self.file_path = os.path.join(self.temp_dir.name, self.file_name)\n\n    def tearDown(self):\n        self.temp_dir.cleanup()\n\n    def write_config_file(self, config):\n        file_contents = json.dumps(config)\n        str_to_file(file_contents, self.file_path)\n\n    def test_valid(self):\n        config = {\n            \'taskType\': \'CHIP_CLASSIFICATION\',\n            \'chipClassificationConfig\': {\n                \'classItems\': [{\n                    \'id\': 1,\n                    \'name\': \'car\'\n                }]\n            }\n        }\n        self.write_config_file(config)\n        task = load_json_config(self.file_path, TaskConfigMsg())\n        self.assertEqual(task.task_type, rv.CHIP_CLASSIFICATION)\n        self.assertEqual(task.chip_classification_config.class_items[0].id, 1)\n        self.assertEqual(task.chip_classification_config.class_items[0].name,\n                         \'car\')\n        self.assertEqual(len(task.chip_classification_config.class_items), 1)\n\n    def test_bogus_field(self):\n        config = {\n            \'taskType\': \'CHIP_CLASSIFICATION\',\n            \'chipClassificationConfig\': {\n                \'classItems\': [{\n                    \'id\': 1,\n                    \'name\': \'car\'\n                }]\n            },\n            \'bogus_field\': 0\n        }\n\n        self.write_config_file(config)\n        with self.assertRaises(ProtobufParseException):\n            load_json_config(self.file_path, TaskConfigMsg())\n\n    def test_bogus_value(self):\n        config = {\'task\': \'bogus_value\'}\n        self.write_config_file(config)\n        with self.assertRaises(ProtobufParseException):\n            load_json_config(self.file_path, TaskConfigMsg())\n\n    def test_invalid_json(self):\n        invalid_json_str = \'\'\'\n            {\n                ""taskType"": ""CHIP_CLASSIFICATION\n            }\n        \'\'\'\n        str_to_file(invalid_json_str, self.file_path)\n\n        with self.assertRaises(ProtobufParseException):\n            load_json_config(self.file_path, TaskConfigMsg())\n\n\nclass TestS3Misc(unittest.TestCase):\n    def setUp(self):\n        self.lorem = LOREM\n\n        # Mock S3 bucket\n        self.mock_s3 = mock_s3()\n        self.mock_s3.start()\n        self.s3 = boto3.client(\'s3\')\n        self.bucket_name = \'mock_bucket\'\n        self.s3.create_bucket(Bucket=self.bucket_name)\n\n        # Temporary directory\n        self.temp_dir = RVConfig.get_tmp_dir()\n\n    def tearDown(self):\n        self.temp_dir.cleanup()\n        self.mock_s3.stop()\n\n    def test_last_modified_s3(self):\n        path = os.path.join(self.temp_dir.name, \'lorem\', \'ipsum1.txt\')\n        s3_path = \'s3://{}/lorem1.txt\'.format(self.bucket_name)\n        directory = os.path.dirname(path)\n        make_dir(directory, check_empty=False)\n\n        fs = FileSystem.get_file_system(s3_path, \'r\')\n\n        str_to_file(self.lorem, path)\n        upload_or_copy(path, s3_path)\n        stamp = fs.last_modified(s3_path)\n\n        self.assertTrue(isinstance(stamp, datetime.datetime))\n\n    def test_list_paths_s3(self):\n        path = os.path.join(self.temp_dir.name, \'lorem\', \'ipsum.txt\')\n        s3_path = \'s3://{}/xxx/lorem.txt\'.format(self.bucket_name)\n        s3_directory = \'s3://{}/xxx/\'.format(self.bucket_name)\n        directory = os.path.dirname(path)\n        make_dir(directory, check_empty=False)\n\n        str_to_file(self.lorem, path)\n        upload_or_copy(path, s3_path)\n\n        list_paths(s3_directory)\n        self.assertEqual(len(list_paths(s3_directory)), 1)\n\n    def test_file_exists(self):\n        path = os.path.join(self.temp_dir.name, \'lorem\', \'ipsum.txt\')\n        s3_path = \'s3://{}/xxx/lorem.txt\'.format(self.bucket_name)\n        s3_path_prefix = \'s3://{}/xxx/lorem\'.format(self.bucket_name)\n        s3_directory = \'s3://{}/xxx/\'.format(self.bucket_name)\n        make_dir(path, check_empty=False, use_dirname=True)\n\n        str_to_file(self.lorem, path)\n        upload_or_copy(path, s3_path)\n\n        self.assertTrue(file_exists(s3_directory, include_dir=True))\n        self.assertTrue(file_exists(s3_path, include_dir=False))\n        self.assertFalse(file_exists(s3_path_prefix, include_dir=True))\n        self.assertFalse(file_exists(s3_directory, include_dir=False))\n        self.assertFalse(\n            file_exists(s3_directory + \'NOTPOSSIBLE\', include_dir=False))\n\n\nclass TestLocalMisc(unittest.TestCase):\n    def setUp(self):\n        self.lorem = LOREM\n        self.temp_dir = RVConfig.get_tmp_dir()\n\n    def tearDown(self):\n        self.temp_dir.cleanup()\n\n    def test_bytes_local(self):\n        path = os.path.join(self.temp_dir.name, \'lorem\', \'ipsum.txt\')\n        directory = os.path.dirname(path)\n        make_dir(directory, check_empty=False)\n\n        expected = bytes([0x00, 0x01, 0x02])\n        fs = FileSystem.get_file_system(path, \'r\')\n\n        fs.write_bytes(path, expected)\n        actual = fs.read_bytes(path)\n\n        self.assertEqual(actual, expected)\n\n    def test_bytes_local_false(self):\n        path = os.path.join(self.temp_dir.name, \'xxx\')\n        fs = FileSystem.get_file_system(path, \'r\')\n        self.assertRaises(NotReadableError, lambda: fs.read_bytes(path))\n\n    def test_sync_from_dir_local(self):\n        path = os.path.join(self.temp_dir.name, \'lorem\', \'ipsum.txt\')\n        src = os.path.dirname(path)\n        dst = os.path.join(self.temp_dir.name, \'xxx\')\n        make_dir(src, check_empty=False)\n        make_dir(dst, check_empty=False)\n\n        fs = FileSystem.get_file_system(path, \'r\')\n        fs.write_bytes(path, bytes([0x00, 0x01]))\n        sync_from_dir(src, dst, delete=True)\n\n        self.assertEqual(len(list_paths(dst)), 1)\n\n    def test_sync_from_dir_noop_local(self):\n        path = os.path.join(self.temp_dir.name, \'lorem\', \'ipsum.txt\')\n        src = os.path.join(self.temp_dir.name, \'lorem\')\n        make_dir(src, check_empty=False)\n\n        fs = FileSystem.get_file_system(src, \'r\')\n        fs.write_bytes(path, bytes([0x00, 0x01]))\n        sync_from_dir(src, src, delete=True)\n\n        self.assertEqual(len(list_paths(src)), 1)\n\n    def test_sync_to_dir_local(self):\n        path = os.path.join(self.temp_dir.name, \'lorem\', \'ipsum.txt\')\n        src = os.path.dirname(path)\n        dst = os.path.join(self.temp_dir.name, \'xxx\')\n        make_dir(src, check_empty=False)\n        make_dir(dst, check_empty=False)\n\n        fs = FileSystem.get_file_system(path, \'r\')\n        fs.write_bytes(path, bytes([0x00, 0x01]))\n        sync_to_dir(src, dst, delete=True)\n\n        self.assertEqual(len(list_paths(dst)), 1)\n\n    def test_copy_to_local(self):\n        path1 = os.path.join(self.temp_dir.name, \'lorem\', \'ipsum.txt\')\n        path2 = os.path.join(self.temp_dir.name, \'yyy\', \'ipsum.txt\')\n        dir1 = os.path.dirname(path1)\n        dir2 = os.path.dirname(path2)\n        make_dir(dir1, check_empty=False)\n        make_dir(dir2, check_empty=False)\n\n        str_to_file(self.lorem, path1)\n\n        upload_or_copy(path1, path2)\n        self.assertEqual(len(list_paths(dir2)), 1)\n\n    def test_last_modified(self):\n        path = os.path.join(self.temp_dir.name, \'lorem\', \'ipsum1.txt\')\n        directory = os.path.dirname(path)\n        make_dir(directory, check_empty=False)\n\n        fs = FileSystem.get_file_system(path, \'r\')\n\n        str_to_file(self.lorem, path)\n        stamp = fs.last_modified(path)\n\n        self.assertTrue(isinstance(stamp, datetime.datetime))\n\n    def test_file_exists(self):\n        fs = FileSystem.get_file_system(self.temp_dir.name, \'r\')\n\n        path1 = os.path.join(self.temp_dir.name, \'lorem\', \'ipsum.txt\')\n        dir1 = os.path.dirname(path1)\n        make_dir(dir1, check_empty=False)\n\n        str_to_file(self.lorem, path1)\n\n        self.assertTrue(fs.file_exists(dir1, include_dir=True))\n        self.assertTrue(fs.file_exists(path1, include_dir=False))\n        self.assertFalse(fs.file_exists(dir1, include_dir=False))\n        self.assertFalse(\n            fs.file_exists(dir1 + \'NOTPOSSIBLE\', include_dir=False))\n\n\nclass TestHttpMisc(unittest.TestCase):\n    def setUp(self):\n        self.lorem = LOREM\n        self.temp_dir = RVConfig.get_tmp_dir()\n\n    def tearDown(self):\n        self.temp_dir.cleanup()\n\n    def test_file_exists_http_true(self):\n        http_path = (\'https://raw.githubusercontent.com/tensorflow/models/\'\n                     \'17fa52864bfc7a7444a8b921d8a8eb1669e14ebd/README.md\')\n        self.assertTrue(file_exists(http_path))\n\n    def test_file_exists_http_false(self):\n        http_path = (\'https://raw.githubusercontent.com/tensorflow/models/\'\n                     \'17fa52864bfc7a7444a8b921d8a8eb1669e14ebd/XXX\')\n        self.assertFalse(file_exists(http_path))\n\n    def test_write_str_http(self):\n        self.assertRaises(NotWritableError,\n                          lambda: str_to_file(\'xxx\', \'http://localhost/\'))\n\n    def test_sync_to_http(self):\n        src = self.temp_dir.name\n        dst = \'http://localhost/\'\n        self.assertRaises(NotWritableError, lambda: sync_to_dir(src, dst))\n\n    def test_sync_from_http(self):\n        src = \'http://localhost/\'\n        dst = self.temp_dir.name\n        self.assertRaises(NotReadableError, lambda: sync_from_dir(src, dst))\n\n    def test_copy_to_http(self):\n        path = os.path.join(self.temp_dir.name, \'lorem\', \'ipsum.txt\')\n        dst = \'http://localhost/\'\n        directory = os.path.dirname(path)\n        make_dir(directory, check_empty=False)\n\n        str_to_file(self.lorem, path)\n\n        self.assertRaises(NotWritableError, lambda: upload_or_copy(path, dst))\n        os.remove(path)\n\n    def test_copy_from_http(self):\n        http_path = (\'https://raw.githubusercontent.com/tensorflow/models/\'\n                     \'17fa52864bfc7a7444a8b921d8a8eb1669e14ebd/README.md\')\n        expected = os.path.join(\n            self.temp_dir.name, \'http\', \'raw.githubusercontent.com\',\n            \'tensorflow/models\',\n            \'17fa52864bfc7a7444a8b921d8a8eb1669e14ebd/README.md\')\n        download_if_needed(http_path, self.temp_dir.name)\n\n        self.assertTrue(file_exists(expected))\n        os.remove(expected)\n\n    def test_last_modified_http(self):\n        uri = \'http://localhost/\'\n        fs = FileSystem.get_file_system(uri, \'r\')\n        self.assertEqual(fs.last_modified(uri), None)\n\n    def test_write_bytes_http(self):\n        uri = \'http://localhost/\'\n        fs = FileSystem.get_file_system(uri, \'r\')\n        self.assertRaises(NotWritableError,\n                          lambda: fs.write_bytes(uri, bytes([0x00, 0x01])))\n\n\nclass TestGetCachedFile(unittest.TestCase):\n    def setUp(self):\n        # Setup mock S3 bucket.\n        self.mock_s3 = mock_s3()\n        self.mock_s3.start()\n        self.s3 = boto3.client(\'s3\')\n        self.bucket_name = \'mock_bucket\'\n        self.s3.create_bucket(Bucket=self.bucket_name)\n\n        self.content_str = \'hello\'\n        self.file_name = \'hello.txt\'\n        self.temp_dir = RVConfig.get_tmp_dir()\n        self.cache_dir = os.path.join(self.temp_dir.name, \'cache\')\n\n    def tearDown(self):\n        self.temp_dir.cleanup()\n        self.mock_s3.stop()\n\n    def test_local(self):\n        local_path = os.path.join(self.temp_dir.name, self.file_name)\n        str_to_file(self.content_str, local_path)\n\n        path = get_cached_file(self.cache_dir, local_path)\n        self.assertTrue(os.path.isfile(path))\n\n    def test_local_zip(self):\n        local_path = os.path.join(self.temp_dir.name, self.file_name)\n        local_gz_path = local_path + \'.gz\'\n        with gzip.open(local_gz_path, \'wb\') as f:\n            f.write(bytes(self.content_str, encoding=\'utf-8\'))\n\n        with patch(\'gzip.open\', side_effect=gzip.open) as patched_gzip_open:\n            path = get_cached_file(self.cache_dir, local_gz_path)\n            self.assertTrue(os.path.isfile(path))\n            self.assertNotEqual(path, local_gz_path)\n            with open(path, \'r\') as f:\n                self.assertEqual(f.read(), self.content_str)\n\n            # Check that calling it again doesn\'t invoke the gzip.open method again.\n            path = get_cached_file(self.cache_dir, local_gz_path)\n            self.assertTrue(os.path.isfile(path))\n            self.assertNotEqual(path, local_gz_path)\n            with open(path, \'r\') as f:\n                self.assertEqual(f.read(), self.content_str)\n            self.assertEqual(patched_gzip_open.call_count, 1)\n\n    def test_remote(self):\n        with patch(\n                \'rastervision.utils.files.download_if_needed\',\n                side_effect=download_if_needed) as patched_download:\n            s3_path = \'s3://{}/{}\'.format(self.bucket_name, self.file_name)\n            str_to_file(self.content_str, s3_path)\n            path = get_cached_file(self.cache_dir, s3_path)\n            self.assertTrue(os.path.isfile(path))\n\n            # Check that calling it again doesn\'t invoke the download method again.\n            path = get_cached_file(self.cache_dir, s3_path)\n            self.assertTrue(os.path.isfile(path))\n            self.assertEqual(patched_download.call_count, 1)\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
tests/utils/test_misc.py,0,"b""import unittest\n\nfrom rastervision.utils.misc import (replace_nones_in_dict, set_nested_keys,\n                                     split_into_groups)\n\n\nclass TestMiscUtils(unittest.TestCase):\n    def test_replace_nones_in_dict(self):\n        d = {\n            'one': None,\n            'two': 2,\n            'three': {\n                'four': 4,\n                'five': None,\n                'six': [\n                    {\n                        'seven': None\n                    },\n                    {\n                        'eight': None\n                    },\n                    {\n                        'nine': 9\n                    },\n                ],\n                'ten': {\n                    'eleven': None\n                }\n            }\n        }\n\n        expected = {\n            'one': 'A',\n            'two': 2,\n            'three': {\n                'four': 4,\n                'five': 'A',\n                'six': [\n                    {\n                        'seven': 'A'\n                    },\n                    {\n                        'eight': 'A'\n                    },\n                    {\n                        'nine': 9\n                    },\n                ],\n                'ten': {\n                    'eleven': 'A'\n                }\n            }\n        }\n\n        self.assertEqual(replace_nones_in_dict(d, 'A'), expected)\n\n    def test_set_nested_keys_finds_nested(self):\n        d = {\n            'one': 1,\n            'two': 2,\n            'three': {\n                'four': 4,\n                'five': 5,\n                'six': [\n                    {\n                        'seven': 7\n                    },\n                    {\n                        'eight': 8\n                    },\n                    {\n                        'nine': 9\n                    },\n                ],\n                'ten': {\n                    'eleven': 11\n                }\n            }\n        }\n\n        expected = {\n            'one': 1,\n            'two': 2,\n            'three': {\n                'four': 4,\n                'five': 55,\n                'six': [\n                    {\n                        'seven': 7\n                    },\n                    {\n                        'eight': 8\n                    },\n                    {\n                        'nine': 9\n                    },\n                ],\n                'ten': {\n                    'eleven': 11\n                }\n            }\n        }\n\n        set_nested_keys(d, {'five': 55})\n\n        self.assertEqual(d, expected)\n\n    def test_set_nested_keys_ignores_missing_keys(self):\n        d = {\n            'one': 1,\n            'two': 2,\n            'three': {\n                'four': 4,\n                'five': 5,\n                'six': [\n                    {\n                        'seven': 7\n                    },\n                    {\n                        'eight': 8\n                    },\n                    {\n                        'nine': 9\n                    },\n                ],\n                'ten': {\n                    'eleven': 11\n                }\n            }\n        }\n\n        expected = {\n            'one': 1,\n            'two': 2,\n            'three': {\n                'four': 4,\n                'five': 5,\n                'six': [\n                    {\n                        'seven': 7\n                    },\n                    {\n                        'eight': 8\n                    },\n                    {\n                        'nine': 9\n                    },\n                ],\n                'ten': {\n                    'eleven': 11\n                }\n            }\n        }\n\n        set_nested_keys(d, {'twenty': 20}, ignore_missing_keys=True)\n\n        self.assertEqual(d, expected)\n\n    def test_set_nested_keys_sets_missing_keys_in_dict(self):\n        d = {\n            'one': 1,\n            'two': 2,\n            'three': {\n                'four': 4,\n                'five': 5,\n                'six': [\n                    {\n                        'seven': 7\n                    },\n                    {\n                        'eight': 8\n                    },\n                    {\n                        'nine': 9\n                    },\n                ],\n                'ten': {\n                    'eleven': 11\n                }\n            }\n        }\n\n        expected = {\n            'one': 1,\n            'two': 2,\n            'three': {\n                'four': 4,\n                'five': 5,\n                'six': [\n                    {\n                        'seven': 7\n                    },\n                    {\n                        'eight': 8\n                    },\n                    {\n                        'nine': 9\n                    },\n                ],\n                'ten': {\n                    'eleven': 11\n                },\n                'twelve': 12\n            }\n        }\n\n        mod = {'three': {'twelve': 12}}\n\n        set_nested_keys(d, mod, set_missing_keys=True)\n\n        self.assertEqual(d, expected)\n\n    def test_split_into_groups(self):\n        lst = [1, 2, 3, 4, 5, 6]\n\n        g1 = split_into_groups(lst[:5], 3)\n        self.assertEqual(g1, [[1, 2], [3, 4], [5]])\n\n        g2 = split_into_groups(lst, 7)\n        self.assertEqual(g2, [[1], [2], [3], [4], [5], [6]])\n\n        g3 = split_into_groups(lst[0:1], 7)\n        self.assertEqual(g3, [[1]])\n\n        g4 = split_into_groups(lst, 3)\n        self.assertEqual(g4, [[1, 2], [3, 4], [5, 6]])\n"""
tests/utils/test_zxy2geotiff.py,0,"b""import unittest\nimport tempfile\nfrom os.path import join\n\nimport mercantile\nfrom PIL import Image\nimport numpy as np\nimport rasterio\nfrom rastervision.utils.files import make_dir\nfrom rastervision.utils.zxy2geotiff import _zxy2geotiff, merc2lnglat\n\n\nclass TestZXY2Geotiff(unittest.TestCase):\n    def setUp(self):\n        tmp_dir_obj = tempfile.TemporaryDirectory()\n        self.tmp_dir = tmp_dir_obj.name\n        self.tmp_dir = '/opt/data/test-zxy'\n\n    def _test_zxy2geotiff(self, use_tms=False, make_cog=False):\n        # We generate a 3x3 grid of zxy tiles and save them. Then,\n        # get the lng/lat of the center of the NW (northwest) and SE tiles,\n        # and pass those as bounds to zxy2geotiff. We open the resulting\n        # geotiff and check that the content is correct.\n        img_arr = np.random.randint(\n            0, 256, (3 * 256, 3 * 256, 3), dtype=np.uint8)\n        zoom = 18\n\n        i = 0\n        for y in range(3):\n            for x in range(3):\n                im = Image.fromarray(\n                    img_arr[y * 256:(y + 1) * 256, x * 256:(x + 1) * 256, :])\n\n                tile_y = y\n                # The TMS convention is for the y axis to start at the bottom\n                # rather than the top.\n                if use_tms:\n                    tile_y = (2**zoom) - y - 1\n                im_path = join(self.tmp_dir, '{}/{}/{}.png'.format(\n                    zoom, x, tile_y))\n                make_dir(im_path, use_dirname=True)\n                im.save(im_path)\n                i += 1\n\n        tile_schema = join(self.tmp_dir, '{z}/{x}/{y}.png')\n        if use_tms:\n            tile_schema = join(self.tmp_dir, '{z}/{x}/{-y}.png')\n\n        # Get center of NW and SE tiles.\n        nw_bounds = mercantile.xy_bounds(0, 0, zoom)\n        nw_merc_y = nw_bounds.bottom + (nw_bounds.top - nw_bounds.bottom) / 2\n        nw_merc_x = nw_bounds.left + (nw_bounds.right - nw_bounds.left) / 2\n        nw_lng, nw_lat = merc2lnglat(nw_merc_x, nw_merc_y)\n\n        se_bounds = mercantile.xy_bounds(2, 2, zoom)\n        se_merc_y = se_bounds.bottom + (se_bounds.top - se_bounds.bottom) / 2\n        se_merc_x = se_bounds.left + (se_bounds.right - se_bounds.left) / 2\n        se_lng, se_lat = merc2lnglat(se_merc_x, se_merc_y)\n\n        # min_lat, min_lng, max_lat, max_lng = bounds\n        bounds = [se_lat, nw_lng, nw_lat, se_lng]\n        output_uri = join(self.tmp_dir, 'output.tif')\n        _zxy2geotiff(tile_schema, zoom, bounds, output_uri, make_cog=make_cog)\n\n        with rasterio.open(output_uri) as dataset:\n            tiff_arr = dataset.read()\n            self.assertEqual(tiff_arr.shape, (3, 512, 512))\n            exp_arr = np.transpose(img_arr, (2, 0, 1))[:, 128:-128, 128:-128]\n            np.testing.assert_array_equal(tiff_arr, exp_arr)\n\n    def test_zxy2geotiff(self):\n        self._test_zxy2geotiff()\n\n    def test_zxy2geotiff_cog(self):\n        self._test_zxy2geotiff(make_cog=True)\n\n    def test_zxy2geotiff_tms(self):\n        self._test_zxy2geotiff(use_tms=True)\n\n    def test_zxy2geotiff_single_tile(self):\n        # Same as above test except it uses a single tile instead of a 3x3\n        # grid.\n        img_arr = np.random.randint(0, 256, (256, 256, 3), dtype=np.uint8)\n        zoom = 18\n\n        im = Image.fromarray(img_arr)\n        x = 0\n        y = 0\n        im_path = join(self.tmp_dir, '{}/{}/{}.png'.format(zoom, x, y))\n        make_dir(im_path, use_dirname=True)\n        im.save(im_path)\n\n        tile_schema = join(self.tmp_dir, '{z}/{x}/{y}.png')\n\n        # Get NW and SE corner of central half of tile.\n        nw_bounds = mercantile.xy_bounds(0, 0, zoom)\n        nw_merc_y = nw_bounds.top - (nw_bounds.top - nw_bounds.bottom) / 4\n        nw_merc_x = nw_bounds.left + (nw_bounds.right - nw_bounds.left) / 4\n        nw_lng, nw_lat = merc2lnglat(nw_merc_x, nw_merc_y)\n\n        se_bounds = mercantile.xy_bounds(0, 0, zoom)\n        se_merc_y = se_bounds.bottom + (se_bounds.top - se_bounds.bottom) / 4\n        se_merc_x = se_bounds.right - (se_bounds.right - se_bounds.left) / 4\n        se_lng, se_lat = merc2lnglat(se_merc_x, se_merc_y)\n\n        # min_lat, min_lng, max_lat, max_lng = bounds\n        bounds = [se_lat, nw_lng, nw_lat, se_lng]\n        output_uri = join(self.tmp_dir, 'output.tif')\n        _zxy2geotiff(tile_schema, zoom, bounds, output_uri)\n\n        with rasterio.open(output_uri) as dataset:\n            tiff_arr = dataset.read()\n            self.assertEqual(tiff_arr.shape, (3, 128, 128))\n            exp_arr = np.transpose(img_arr, (2, 0, 1))[:, 64:-64, 64:-64]\n            np.testing.assert_array_equal(tiff_arr, exp_arr)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests_v2/core/__init__.py,0,b''
tests_v2/core/test_box.py,0,"b""import unittest\n\nimport numpy as np\nfrom shapely.geometry import box as ShapelyBox\n\nfrom rastervision2.core.box import Box, BoxSizeError\n\nnp.random.seed(1)\n\n\nclass TestBox(unittest.TestCase):\n    def setUp(self):\n        self.ymin = 0\n        self.xmin = 0\n        self.ymax = 2\n        self.xmax = 3\n        self.box = Box(self.ymin, self.xmin, self.ymax, self.xmax)\n\n    def test_reproject(self):\n        def transform(point):\n            (y, x) = point\n            return ((y + 1) // 2, x // 2)\n\n        reproj = self.box.reproject(transform)\n        self.assertTrue(reproj.xmin == 0)\n        self.assertTrue(reproj.ymin == 0)\n        self.assertTrue(reproj.ymax == 1)\n        self.assertTrue(reproj.xmax == 2)\n\n    def test_dict(self):\n        dictionary = self.box.to_dict()\n        other = Box.from_dict(dictionary)\n        self.assertTrue(self.box == other)\n\n    def test_bad_square(self):\n        self.assertRaises(BoxSizeError,\n                          lambda: self.box.make_random_square(10))\n\n    def test_bad_conatiner(self):\n        self.assertRaises(BoxSizeError,\n                          lambda: self.box.make_random_square_container(1))\n\n    def test_neq(self):\n        other = Box(self.ymin + 1, self.xmin, self.ymax, self.xmax)\n        self.assertTrue(self.box != other)\n\n    def test_int(self):\n        other = Box(\n            float(self.ymin) + 0.01, float(self.xmin), float(self.ymax),\n            float(self.xmax))\n        self.assertTrue(other.to_int() == self.box)\n\n    def test_get_height(self):\n        height = self.ymax - self.ymin\n        self.assertEqual(self.box.get_height(), height)\n\n    def test_get_width(self):\n        width = self.xmax - self.xmin\n        self.assertEqual(self.box.get_width(), width)\n\n    def test_get_area(self):\n        area = self.box.get_height() * self.box.get_width()\n        self.assertEqual(self.box.get_area(), area)\n\n    def test_rasterio_format(self):\n        rasterio_box = ((self.ymin, self.ymax), (self.xmin, self.xmax))\n        self.assertEqual(self.box.rasterio_format(), rasterio_box)\n\n    def test_tuple_format(self):\n        box_tuple = (0, 0, 2, 3)\n        output_box = self.box.tuple_format()\n        self.assertEqual(output_box, box_tuple)\n\n    def test_shapely_format(self):\n        shapely_box = (self.xmin, self.ymin, self.xmax, self.ymax)\n        self.assertEqual(self.box.shapely_format(), shapely_box)\n\n    def test_npbox_format(self):\n        self.assertEqual(\n            tuple(self.box.npbox_format()), self.box.tuple_format())\n        self.assertEqual(self.box.npbox_format().dtype, np.float)\n\n    def test_geojson_coordinates(self):\n        nw = [self.xmin, self.ymin]\n        ne = [self.xmin, self.ymax]\n        se = [self.xmax, self.ymax]\n        sw = [self.xmax, self.ymin]\n        geojson_coords = [nw, ne, se, sw, nw]\n        self.assertEqual(self.box.geojson_coordinates(), geojson_coords)\n\n    def test_make_random_square_container(self):\n        size = 5\n        nb_tests = 100\n        for _ in range(nb_tests):\n            container = self.box.make_random_square_container(size)\n            self.assertEqual(container.get_width(), container.get_height())\n            self.assertEqual(container.get_width(), size)\n            self.assertTrue(container.to_shapely().contains(\n                self.box.to_shapely()))\n\n    def test_make_random_square_container_too_big(self):\n        size = 1\n        with self.assertRaises(BoxSizeError):\n            self.box.make_random_square_container(size)\n\n    def test_make_random_square(self):\n        window = Box(5, 5, 15, 15)\n        size = 5\n        nb_tests = 100\n        for _ in range(nb_tests):\n            box = window.make_random_square(size)\n            self.assertEqual(box.get_width(), box.get_height())\n            self.assertEqual(box.get_width(), size)\n            self.assertTrue(window.to_shapely().contains(box.to_shapely()))\n\n    def test_from_npbox(self):\n        npbox = np.array([self.ymin, self.xmin, self.ymax, self.xmax])\n        output_box = Box.from_npbox(npbox)\n        self.assertEqual(output_box, self.box)\n\n    def test_from_shapely(self):\n        shape = ShapelyBox(self.xmin, self.ymin, self.xmax, self.ymax)\n        output_box = Box.from_shapely(shape)\n        self.assertEqual(output_box, self.box)\n\n    def test_to_shapely(self):\n        bounds = self.box.to_shapely().bounds\n        self.assertEqual((bounds[1], bounds[0], bounds[3], bounds[2]),\n                         self.box.tuple_format())\n\n    def test_make_square(self):\n        square = Box(0, 0, 10, 10)\n        output_square = Box.make_square(0, 0, 10)\n        self.assertEqual(output_square, square)\n        self.assertEqual(output_square.get_width(), output_square.get_height())\n\n    def test_make_eroded(self):\n        max_extent = Box.make_square(0, 0, 10)\n        box = Box(1, 1, 3, 4)\n        buffer_size = erosion_size = 1\n        eroded_box = box.make_buffer(buffer_size, max_extent) \\\n                        .make_eroded(erosion_size)\n        self.assertEqual(eroded_box, box)\n\n    def test_make_buffer(self):\n        buffer_size = 1\n        max_extent = Box.make_square(0, 0, 3)\n        buffer_box = Box(0, 0, 3, 3)\n        output_buffer_box = self.box.make_buffer(buffer_size, max_extent)\n        self.assertEqual(output_buffer_box, buffer_box)\n\n        buffer_size = 0.5\n        max_extent = Box.make_square(0, 0, 5)\n        buffer_box = Box(0, 0, 3, 5)\n        output_buffer_box = self.box.make_buffer(buffer_size, max_extent)\n        self.assertEqual(output_buffer_box, buffer_box)\n\n    def test_make_copy(self):\n        copy_box = self.box.make_copy()\n        self.assertIsNot(copy_box, self.box)\n        self.assertEqual(copy_box, self.box)\n\n    def test_get_windows(self):\n        extent = Box(0, 0, 100, 100)\n        windows = list(extent.get_windows(10, 10))\n        self.assertEqual(len(windows), 100)\n\n        extent = Box(0, 0, 100, 100)\n        windows = list(extent.get_windows(10, 5))\n        self.assertEqual(len(windows), 400)\n\n        extent = Box(0, 0, 20, 20)\n        windows = set(\n            [window.tuple_format() for window in extent.get_windows(10, 10)])\n        expected_windows = [\n            Box.make_square(0, 0, 10),\n            Box.make_square(10, 0, 10),\n            Box.make_square(0, 10, 10),\n            Box.make_square(10, 10, 10)\n        ]\n        expected_windows = set(\n            [window.tuple_format() for window in expected_windows])\n        self.assertSetEqual(windows, expected_windows)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests_v2/core/test_stats_analyzer.py,0,"b""import unittest\nimport os\n\nimport numpy as np\n\nfrom rastervision2.pipeline import rv_config\nfrom rastervision2.core.raster_stats import RasterStats, chip_sz\nfrom rastervision2.core.data import Scene\nfrom rastervision2.core.analyzer import StatsAnalyzerConfig\nfrom tests_v2.core.data.mock_raster_source import MockRasterSource\n\n\nclass TestStatsAnalyzer(unittest.TestCase):\n    def setUp(self):\n        self.tmp_dir = rv_config.get_tmp_dir()\n\n    def tearDown(self):\n        self.tmp_dir.cleanup()\n\n    def _test(self, is_random=False):\n        stats_uri = os.path.join(self.tmp_dir.name, 'stats.json')\n        scenes = []\n        raster_sources = []\n        imgs = []\n        sample_prob = 0.5\n        for i in range(3):\n            rs = MockRasterSource([0, 1, 2], 3)\n            img = np.zeros((600, 600, 3))\n            img[:, :, 0] = 1 + i\n            img[:, :, 1] = 2 + i\n            img[:, :, 2] = 3 + i\n            if not is_random:\n                img[300:, 300:, :] = np.nan\n\n            imgs.append(img)\n            rs.set_raster(img)\n            raster_sources.append(rs)\n            scenes.append(Scene(str(i), rs))\n\n        channel_vals = list(map(lambda x: np.expand_dims(x, axis=0), imgs))\n        channel_vals = np.concatenate(channel_vals, axis=0)\n        channel_vals = np.transpose(channel_vals, [3, 0, 1, 2])\n        channel_vals = np.reshape(channel_vals, (3, -1))\n        exp_means = np.nanmean(channel_vals, axis=1)\n        exp_stds = np.nanstd(channel_vals, axis=1)\n\n        analyzer_cfg = StatsAnalyzerConfig(output_uri=stats_uri, sample_prob=None)\n        if is_random:\n            analyzer_cfg = StatsAnalyzerConfig(\n                output_uri=stats_uri, sample_prob=sample_prob)\n        analyzer = analyzer_cfg.build()\n        analyzer.process(scenes, self.tmp_dir.name)\n\n        stats = RasterStats.load(stats_uri)\n        np.testing.assert_array_almost_equal(stats.means, exp_means, decimal=3)\n        np.testing.assert_array_almost_equal(stats.stds, exp_stds, decimal=3)\n        if is_random:\n            for rs in raster_sources:\n                width = rs.get_extent().get_width()\n                height = rs.get_extent().get_height()\n                exp_num_chips = round(\n                    ((width * height) / (chip_sz**2)) * sample_prob)\n                self.assertEqual(rs.mock._get_chip.call_count, exp_num_chips)\n\n    def test_random(self):\n        self._test(is_random=True)\n\n    def test_sliding(self):\n        self._test(is_random=False)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests_v2/pipeline/__init__.py,0,b''
tests_v2/pipeline/test_config.py,0,"b""from typing import List\nimport unittest\nimport copy\n\nfrom pydantic.error_wrappers import ValidationError\n\nfrom rastervision2.pipeline.config import (Config, register_config, build_config,\n                                           upgrade_config, Upgrader)\n\n\nclass AConfig(Config):\n    x: str = 'x'\n\n\n@register_config('asub1')\nclass ASub1Config(AConfig):\n    y: str = 'y'\n\n\n@register_config('asub2')\nclass ASub2Config(AConfig):\n    y: str = 'y'\n\n\nclass BConfig(Config):\n    x: str = 'x'\n\n\nclass UpgradeC1(Upgrader):\n    def upgrade(self, cfg_dict):\n        cfg_dict = copy.deepcopy(cfg_dict)\n        cfg_dict['x'] = cfg_dict['y']\n        del cfg_dict['y']\n        return cfg_dict\n\n\n@register_config('c', version=1, upgraders=[UpgradeC1()])\nclass CConfig(Config):\n    al: List[AConfig]\n    bl: List[BConfig]\n    a: AConfig\n    b: BConfig\n    x: str = 'x'\n\n\nclass TestConfig(unittest.TestCase):\n    def test_to_from(self):\n        cfg = CConfig(\n            al=[AConfig(), ASub1Config(),\n                ASub2Config()],\n            bl=[BConfig()],\n            a=ASub1Config(),\n            b=BConfig())\n\n        exp_dict = {\n            'type_hint':\n            'c',\n            'version':\n            1,\n            'a': {\n                'type_hint': 'asub1',\n                'x': 'x',\n                'y': 'y'\n            },\n            'al': [{\n                'x': 'x'\n            }, {\n                'type_hint': 'asub1',\n                'x': 'x',\n                'y': 'y'\n            }, {\n                'type_hint': 'asub2',\n                'x': 'x',\n                'y': 'y'\n            }],\n            'b': {\n                'x': 'x'\n            },\n            'bl': [{\n                'x': 'x'\n            }],\n            'x':\n            'x'\n        }\n\n        self.assertDictEqual(cfg.dict(), exp_dict)\n        self.assertEqual(build_config(exp_dict), cfg)\n\n    def test_no_extras(self):\n        with self.assertRaises(ValidationError):\n            BConfig(zz='abc')\n\n    def test_upgrade(self):\n        c_dict_v0 = {\n            'type_hint':\n            'c',\n            'version':\n            0,\n            'a': {\n                'type_hint': 'asub1',\n                'x': 'x',\n                'y': 'y'\n            },\n            'al': [{\n                'x': 'x'\n            }, {\n                'type_hint': 'asub1',\n                'x': 'x',\n                'y': 'y'\n            }, {\n                'type_hint': 'asub2',\n                'x': 'x',\n                'y': 'y'\n            }],\n            'b': {\n                'x': 'x'\n            },\n            'bl': [{\n                'x': 'x'\n            }],\n            'y':\n            'x'\n        }\n\n        c_dict_v1 = {\n            'type_hint':\n            'c',\n            'version':\n            1,\n            'a': {\n                'type_hint': 'asub1',\n                'x': 'x',\n                'y': 'y'\n            },\n            'al': [{\n                'x': 'x'\n            }, {\n                'type_hint': 'asub1',\n                'x': 'x',\n                'y': 'y'\n            }, {\n                'type_hint': 'asub2',\n                'x': 'x',\n                'y': 'y'\n            }],\n            'b': {\n                'x': 'x'\n            },\n            'bl': [{\n                'x': 'x'\n            }],\n            'x':\n            'x'\n        }\n        upgraded_c_dict = upgrade_config(c_dict_v0)\n        self.assertDictEqual(upgraded_c_dict, c_dict_v1)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests_v2/pipeline/test_file_system.py,0,"b'import os\nimport unittest\nfrom unittest.mock import patch\nimport datetime\nimport gzip\n\nimport boto3\nfrom moto import mock_s3\n\nfrom rastervision2.pipeline.file_system import (\n    file_to_str, str_to_file, download_if_needed, upload_or_copy,\n    make_dir, get_local_path, file_exists, sync_from_dir, sync_to_dir, list_paths,\n    get_cached_file, NotReadableError, NotWritableError, FileSystem)\nfrom rastervision2.pipeline import rv_config\n\nLOREM = """""" Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do\n        eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut\n        enim ad minim veniam, quis nostrud exercitation ullamco\n        laboris nisi ut aliquip ex ea commodo consequat. Duis aute\n        irure dolor in reprehenderit in voluptate velit esse cillum\n        dolore eu fugiat nulla pariatur. Excepteur sint occaecat\n        cupidatat non proident, sunt in culpa qui officia deserunt\n        mollit anim id est laborum.  """"""\n\n\nclass TestMakeDir(unittest.TestCase):\n    def setUp(self):\n        self.lorem = LOREM\n\n        # Mock S3 bucket\n        self.mock_s3 = mock_s3()\n        self.mock_s3.start()\n        self.s3 = boto3.client(\'s3\')\n        self.bucket_name = \'mock_bucket\'\n        self.s3.create_bucket(Bucket=self.bucket_name)\n\n        # temporary directory\n        self.tmp_dir = rv_config.get_tmp_dir()\n\n    def tearDown(self):\n        self.tmp_dir.cleanup()\n        self.mock_s3.stop()\n\n    def test_default_args(self):\n        dir = os.path.join(self.tmp_dir.name, \'hello\')\n        make_dir(dir)\n        self.assertTrue(os.path.isdir(dir))\n\n    def test_file_exists_local_true(self):\n        path = os.path.join(self.tmp_dir.name, \'lorem\', \'ipsum.txt\')\n        directory = os.path.dirname(path)\n        make_dir(directory, check_empty=False)\n\n        str_to_file(self.lorem, path)\n\n        self.assertTrue(file_exists(path))\n\n    def test_file_exists_local_false(self):\n        path = os.path.join(self.tmp_dir.name, \'hello\', \'hello.txt\')\n        directory = os.path.dirname(path)\n        make_dir(directory, check_empty=False)\n\n        self.assertFalse(file_exists(path))\n\n    def test_file_exists_s3_true(self):\n        path = os.path.join(self.tmp_dir.name, \'lorem\', \'ipsum.txt\')\n        directory = os.path.dirname(path)\n        make_dir(directory, check_empty=False)\n\n        str_to_file(self.lorem, path)\n\n        s3_path = \'s3://{}/lorem.txt\'.format(self.bucket_name)\n        upload_or_copy(path, s3_path)\n\n        self.assertTrue(file_exists(s3_path))\n\n    def test_file_exists_s3_false(self):\n        s3_path = \'s3://{}/hello.txt\'.format(self.bucket_name)\n        self.assertFalse(file_exists(s3_path))\n\n    def test_check_empty(self):\n        path = os.path.join(self.tmp_dir.name, \'hello\', \'hello.txt\')\n        dir = os.path.dirname(path)\n        str_to_file(\'hello\', path)\n\n        make_dir(dir, check_empty=False)\n        with self.assertRaises(Exception):\n            make_dir(dir, check_empty=True)\n\n    def test_force_empty(self):\n        path = os.path.join(self.tmp_dir.name, \'hello\', \'hello.txt\')\n        dir = os.path.dirname(path)\n        str_to_file(\'hello\', path)\n\n        make_dir(dir, force_empty=False)\n        self.assertTrue(os.path.isfile(path))\n        make_dir(dir, force_empty=True)\n        is_empty = len(os.listdir(dir)) == 0\n        self.assertTrue(is_empty)\n\n    def test_use_dirname(self):\n        path = os.path.join(self.tmp_dir.name, \'hello\', \'hello.txt\')\n        dir = os.path.dirname(path)\n        make_dir(path, use_dirname=True)\n        self.assertTrue(os.path.isdir(dir))\n\n\nclass TestGetLocalPath(unittest.TestCase):\n    def test_local(self):\n        download_dir = \'/download_dir\'\n        uri = \'/my/file.txt\'\n        path = get_local_path(uri, download_dir)\n        self.assertEqual(path, uri)\n\n    def test_s3(self):\n        download_dir = \'/download_dir\'\n        uri = \'s3://bucket/my/file.txt\'\n        path = get_local_path(uri, download_dir)\n        self.assertEqual(path, \'/download_dir/s3/bucket/my/file.txt\')\n\n    def test_http(self):\n        download_dir = \'/download_dir\'\n        uri = \'http://bucket/my/file.txt\'\n        path = get_local_path(uri, download_dir)\n        self.assertEqual(path, \'/download_dir/http/bucket/my/file.txt\')\n\n        # simulate a zxy tile URI\n        uri = \'http://bucket/10/25/53?auth=426753\'\n        path = get_local_path(uri, download_dir)\n        self.assertEqual(path, \'/download_dir/http/bucket/10/25/53\')\n\n\nclass TestFileToStr(unittest.TestCase):\n    """"""Test file_to_str and str_to_file.""""""\n\n    def setUp(self):\n        # Setup mock S3 bucket.\n        self.mock_s3 = mock_s3()\n        self.mock_s3.start()\n        self.s3 = boto3.client(\'s3\')\n        self.bucket_name = \'mock_bucket\'\n        self.s3.create_bucket(Bucket=self.bucket_name)\n\n        self.content_str = \'hello\'\n        self.file_name = \'hello.txt\'\n        self.s3_path = \'s3://{}/{}\'.format(self.bucket_name, self.file_name)\n\n        self.tmp_dir = rv_config.get_tmp_dir()\n        self.local_path = os.path.join(self.tmp_dir.name, self.file_name)\n\n    def tearDown(self):\n        self.tmp_dir.cleanup()\n        self.mock_s3.stop()\n\n    def test_file_to_str_local(self):\n        str_to_file(self.content_str, self.local_path)\n        content_str = file_to_str(self.local_path)\n        self.assertEqual(self.content_str, content_str)\n\n        wrong_path = \'/wrongpath/x.txt\'\n        with self.assertRaises(NotReadableError):\n            file_to_str(wrong_path)\n\n    def test_file_to_str_s3(self):\n        wrong_path = \'s3://wrongpath/x.txt\'\n\n        with self.assertRaises(NotWritableError):\n            str_to_file(self.content_str, wrong_path)\n\n        str_to_file(self.content_str, self.s3_path)\n        content_str = file_to_str(self.s3_path)\n        self.assertEqual(self.content_str, content_str)\n\n        with self.assertRaises(NotReadableError):\n            file_to_str(wrong_path)\n\n\nclass TestDownloadIfNeeded(unittest.TestCase):\n    """"""Test download_if_needed and upload_or_copy and str_to_file.""""""\n\n    def setUp(self):\n        # Setup mock S3 bucket.\n        self.mock_s3 = mock_s3()\n        self.mock_s3.start()\n        self.s3 = boto3.client(\'s3\')\n        self.bucket_name = \'mock_bucket\'\n        self.s3.create_bucket(Bucket=self.bucket_name)\n\n        self.content_str = \'hello\'\n        self.file_name = \'hello.txt\'\n        self.s3_path = \'s3://{}/{}\'.format(self.bucket_name, self.file_name)\n\n        self.tmp_dir = rv_config.get_tmp_dir()\n        self.local_path = os.path.join(self.tmp_dir.name, self.file_name)\n\n    def tearDown(self):\n        self.tmp_dir.cleanup()\n        self.mock_s3.stop()\n\n    def test_download_if_needed_local(self):\n        with self.assertRaises(NotReadableError):\n            file_to_str(self.local_path)\n\n        str_to_file(self.content_str, self.local_path)\n        upload_or_copy(self.local_path, self.local_path)\n        local_path = download_if_needed(self.local_path, self.tmp_dir.name)\n        self.assertEqual(local_path, self.local_path)\n\n    def test_download_if_needed_s3(self):\n        with self.assertRaises(NotReadableError):\n            file_to_str(self.s3_path)\n\n        str_to_file(self.content_str, self.local_path)\n        upload_or_copy(self.local_path, self.s3_path)\n        local_path = download_if_needed(self.s3_path, self.tmp_dir.name)\n        content_str = file_to_str(local_path)\n        self.assertEqual(self.content_str, content_str)\n\n        wrong_path = \'s3://wrongpath/x.txt\'\n        with self.assertRaises(NotWritableError):\n            upload_or_copy(local_path, wrong_path)\n\n\nclass TestS3Misc(unittest.TestCase):\n    def setUp(self):\n        self.lorem = LOREM\n\n        # Mock S3 bucket\n        self.mock_s3 = mock_s3()\n        self.mock_s3.start()\n        self.s3 = boto3.client(\'s3\')\n        self.bucket_name = \'mock_bucket\'\n        self.s3.create_bucket(Bucket=self.bucket_name)\n\n        # temporary directory\n        self.tmp_dir = rv_config.get_tmp_dir()\n\n    def tearDown(self):\n        self.tmp_dir.cleanup()\n        self.mock_s3.stop()\n\n    def test_last_modified_s3(self):\n        path = os.path.join(self.tmp_dir.name, \'lorem\', \'ipsum1.txt\')\n        s3_path = \'s3://{}/lorem1.txt\'.format(self.bucket_name)\n        directory = os.path.dirname(path)\n        make_dir(directory, check_empty=False)\n\n        fs = FileSystem.get_file_system(s3_path, \'r\')\n\n        str_to_file(self.lorem, path)\n        upload_or_copy(path, s3_path)\n        stamp = fs.last_modified(s3_path)\n\n        self.assertTrue(isinstance(stamp, datetime.datetime))\n\n    def test_list_paths_s3(self):\n        path = os.path.join(self.tmp_dir.name, \'lorem\', \'ipsum.txt\')\n        s3_path = \'s3://{}/xxx/lorem.txt\'.format(self.bucket_name)\n        s3_directory = \'s3://{}/xxx/\'.format(self.bucket_name)\n        directory = os.path.dirname(path)\n        make_dir(directory, check_empty=False)\n\n        str_to_file(self.lorem, path)\n        upload_or_copy(path, s3_path)\n\n        list_paths(s3_directory)\n        self.assertEqual(len(list_paths(s3_directory)), 1)\n\n    def test_file_exists(self):\n        path = os.path.join(self.tmp_dir.name, \'lorem\', \'ipsum.txt\')\n        s3_path = \'s3://{}/xxx/lorem.txt\'.format(self.bucket_name)\n        s3_path_prefix = \'s3://{}/xxx/lorem\'.format(self.bucket_name)\n        s3_directory = \'s3://{}/xxx/\'.format(self.bucket_name)\n        make_dir(path, check_empty=False, use_dirname=True)\n\n        str_to_file(self.lorem, path)\n        upload_or_copy(path, s3_path)\n\n        self.assertTrue(file_exists(s3_directory, include_dir=True))\n        self.assertTrue(file_exists(s3_path, include_dir=False))\n        self.assertFalse(file_exists(s3_path_prefix, include_dir=True))\n        self.assertFalse(file_exists(s3_directory, include_dir=False))\n        self.assertFalse(\n            file_exists(s3_directory + \'NOTPOSSIBLE\', include_dir=False))\n\n\nclass TestLocalMisc(unittest.TestCase):\n    def setUp(self):\n        self.lorem = LOREM\n        self.tmp_dir = rv_config.get_tmp_dir()\n\n    def tearDown(self):\n        self.tmp_dir.cleanup()\n\n    def test_bytes_local(self):\n        path = os.path.join(self.tmp_dir.name, \'lorem\', \'ipsum.txt\')\n        directory = os.path.dirname(path)\n        make_dir(directory, check_empty=False)\n\n        expected = bytes([0x00, 0x01, 0x02])\n        fs = FileSystem.get_file_system(path, \'r\')\n\n        fs.write_bytes(path, expected)\n        actual = fs.read_bytes(path)\n\n        self.assertEqual(actual, expected)\n\n    def test_bytes_local_false(self):\n        path = os.path.join(self.tmp_dir.name, \'xxx\')\n        fs = FileSystem.get_file_system(path, \'r\')\n        self.assertRaises(NotReadableError, lambda: fs.read_bytes(path))\n\n    def test_sync_from_dir_local(self):\n        path = os.path.join(self.tmp_dir.name, \'lorem\', \'ipsum.txt\')\n        src = os.path.dirname(path)\n        dst = os.path.join(self.tmp_dir.name, \'xxx\')\n        make_dir(src, check_empty=False)\n        make_dir(dst, check_empty=False)\n\n        fs = FileSystem.get_file_system(path, \'r\')\n        fs.write_bytes(path, bytes([0x00, 0x01]))\n        sync_from_dir(src, dst, delete=True)\n\n        self.assertEqual(len(list_paths(dst)), 1)\n\n    def test_sync_from_dir_noop_local(self):\n        path = os.path.join(self.tmp_dir.name, \'lorem\', \'ipsum.txt\')\n        src = os.path.join(self.tmp_dir.name, \'lorem\')\n        make_dir(src, check_empty=False)\n\n        fs = FileSystem.get_file_system(src, \'r\')\n        fs.write_bytes(path, bytes([0x00, 0x01]))\n        sync_from_dir(src, src, delete=True)\n\n        self.assertEqual(len(list_paths(src)), 1)\n\n    def test_sync_to_dir_local(self):\n        path = os.path.join(self.tmp_dir.name, \'lorem\', \'ipsum.txt\')\n        src = os.path.dirname(path)\n        dst = os.path.join(self.tmp_dir.name, \'xxx\')\n        make_dir(src, check_empty=False)\n        make_dir(dst, check_empty=False)\n\n        fs = FileSystem.get_file_system(path, \'r\')\n        fs.write_bytes(path, bytes([0x00, 0x01]))\n        sync_to_dir(src, dst, delete=True)\n\n        self.assertEqual(len(list_paths(dst)), 1)\n\n    def test_copy_to_local(self):\n        path1 = os.path.join(self.tmp_dir.name, \'lorem\', \'ipsum.txt\')\n        path2 = os.path.join(self.tmp_dir.name, \'yyy\', \'ipsum.txt\')\n        dir1 = os.path.dirname(path1)\n        dir2 = os.path.dirname(path2)\n        make_dir(dir1, check_empty=False)\n        make_dir(dir2, check_empty=False)\n\n        str_to_file(self.lorem, path1)\n\n        upload_or_copy(path1, path2)\n        self.assertEqual(len(list_paths(dir2)), 1)\n\n    def test_last_modified(self):\n        path = os.path.join(self.tmp_dir.name, \'lorem\', \'ipsum1.txt\')\n        directory = os.path.dirname(path)\n        make_dir(directory, check_empty=False)\n\n        fs = FileSystem.get_file_system(path, \'r\')\n\n        str_to_file(self.lorem, path)\n        stamp = fs.last_modified(path)\n\n        self.assertTrue(isinstance(stamp, datetime.datetime))\n\n    def test_file_exists(self):\n        fs = FileSystem.get_file_system(self.tmp_dir.name, \'r\')\n\n        path1 = os.path.join(self.tmp_dir.name, \'lorem\', \'ipsum.txt\')\n        dir1 = os.path.dirname(path1)\n        make_dir(dir1, check_empty=False)\n\n        str_to_file(self.lorem, path1)\n\n        self.assertTrue(fs.file_exists(dir1, include_dir=True))\n        self.assertTrue(fs.file_exists(path1, include_dir=False))\n        self.assertFalse(fs.file_exists(dir1, include_dir=False))\n        self.assertFalse(\n            fs.file_exists(dir1 + \'NOTPOSSIBLE\', include_dir=False))\n\n\nclass TestHttpMisc(unittest.TestCase):\n    def setUp(self):\n        self.lorem = LOREM\n        self.tmp_dir = rv_config.get_tmp_dir()\n\n    def tearDown(self):\n        self.tmp_dir.cleanup()\n\n    def test_file_exists_http_true(self):\n        http_path = (\'https://raw.githubusercontent.com/tensorflow/models/\'\n                     \'17fa52864bfc7a7444a8b921d8a8eb1669e14ebd/README.md\')\n        self.assertTrue(file_exists(http_path))\n\n    def test_file_exists_http_false(self):\n        http_path = (\'https://raw.githubusercontent.com/tensorflow/models/\'\n                     \'17fa52864bfc7a7444a8b921d8a8eb1669e14ebd/XXX\')\n        self.assertFalse(file_exists(http_path))\n\n    def test_write_str_http(self):\n        self.assertRaises(NotWritableError,\n                          lambda: str_to_file(\'xxx\', \'http://localhost/\'))\n\n    def test_sync_to_http(self):\n        src = self.tmp_dir.name\n        dst = \'http://localhost/\'\n        self.assertRaises(NotWritableError, lambda: sync_to_dir(src, dst))\n\n    def test_sync_from_http(self):\n        src = \'http://localhost/\'\n        dst = self.tmp_dir.name\n        self.assertRaises(NotReadableError, lambda: sync_from_dir(src, dst))\n\n    def test_copy_to_http(self):\n        path = os.path.join(self.tmp_dir.name, \'lorem\', \'ipsum.txt\')\n        dst = \'http://localhost/\'\n        directory = os.path.dirname(path)\n        make_dir(directory, check_empty=False)\n\n        str_to_file(self.lorem, path)\n\n        self.assertRaises(NotWritableError, lambda: upload_or_copy(path, dst))\n        os.remove(path)\n\n    def test_copy_from_http(self):\n        http_path = (\'https://raw.githubusercontent.com/tensorflow/models/\'\n                     \'17fa52864bfc7a7444a8b921d8a8eb1669e14ebd/README.md\')\n        expected = os.path.join(\n            self.tmp_dir.name, \'http\', \'raw.githubusercontent.com\',\n            \'tensorflow/models\',\n            \'17fa52864bfc7a7444a8b921d8a8eb1669e14ebd/README.md\')\n        download_if_needed(http_path, self.tmp_dir.name)\n\n        self.assertTrue(file_exists(expected))\n        os.remove(expected)\n\n    def test_last_modified_http(self):\n        uri = \'http://localhost/\'\n        fs = FileSystem.get_file_system(uri, \'r\')\n        self.assertEqual(fs.last_modified(uri), None)\n\n    def test_write_bytes_http(self):\n        uri = \'http://localhost/\'\n        fs = FileSystem.get_file_system(uri, \'r\')\n        self.assertRaises(NotWritableError,\n                          lambda: fs.write_bytes(uri, bytes([0x00, 0x01])))\n\n\nclass TestGetCachedFile(unittest.TestCase):\n    def setUp(self):\n        # Setup mock S3 bucket.\n        self.mock_s3 = mock_s3()\n        self.mock_s3.start()\n        self.s3 = boto3.client(\'s3\')\n        self.bucket_name = \'mock_bucket\'\n        self.s3.create_bucket(Bucket=self.bucket_name)\n\n        self.content_str = \'hello\'\n        self.file_name = \'hello.txt\'\n        self.tmp_dir = rv_config.get_tmp_dir()\n        self.cache_dir = os.path.join(self.tmp_dir.name, \'cache\')\n\n    def tearDown(self):\n        self.tmp_dir.cleanup()\n        self.mock_s3.stop()\n\n    def test_local(self):\n        local_path = os.path.join(self.tmp_dir.name, self.file_name)\n        str_to_file(self.content_str, local_path)\n\n        path = get_cached_file(self.cache_dir, local_path)\n        self.assertTrue(os.path.isfile(path))\n\n    def test_local_zip(self):\n        local_path = os.path.join(self.tmp_dir.name, self.file_name)\n        local_gz_path = local_path + \'.gz\'\n        with gzip.open(local_gz_path, \'wb\') as f:\n            f.write(bytes(self.content_str, encoding=\'utf-8\'))\n\n        with patch(\'gzip.open\', side_effect=gzip.open) as patched_gzip_open:\n            path = get_cached_file(self.cache_dir, local_gz_path)\n            self.assertTrue(os.path.isfile(path))\n            self.assertNotEqual(path, local_gz_path)\n            with open(path, \'r\') as f:\n                self.assertEqual(f.read(), self.content_str)\n\n            # Check that calling it again doesn\'t invoke the gzip.open method again.\n            path = get_cached_file(self.cache_dir, local_gz_path)\n            self.assertTrue(os.path.isfile(path))\n            self.assertNotEqual(path, local_gz_path)\n            with open(path, \'r\') as f:\n                self.assertEqual(f.read(), self.content_str)\n            self.assertEqual(patched_gzip_open.call_count, 1)\n\n    def test_remote(self):\n        with patch(\n                \'rastervision2.pipeline.file_system.utils.download_if_needed\',\n                side_effect=download_if_needed) as patched_download:\n            s3_path = \'s3://{}/{}\'.format(self.bucket_name, self.file_name)\n            str_to_file(self.content_str, s3_path)\n            path = get_cached_file(self.cache_dir, s3_path)\n            self.assertTrue(os.path.isfile(path))\n\n            # Check that calling it again doesn\'t invoke the download method again.\n            self.assertTrue(os.path.isfile(path))\n            self.assertEqual(patched_download.call_count, 1)\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
tests_v2/pipeline/test_utils.py,0,"b""import unittest\n\nfrom rastervision2.pipeline.utils import split_into_groups\n\n\nclass TestUtils(unittest.TestCase):\n    def test_split_into_groups(self):\n        lst = [1, 2, 3, 4, 5, 6]\n\n        g1 = split_into_groups(lst[:5], 3)\n        self.assertEqual(g1, [[1, 2], [3, 4], [5]])\n\n        g2 = split_into_groups(lst, 7)\n        self.assertEqual(g2, [[1], [2], [3], [4], [5], [6]])\n\n        g3 = split_into_groups(lst[0:1], 7)\n        self.assertEqual(g3, [[1]])\n\n        g4 = split_into_groups(lst, 3)\n        self.assertEqual(g4, [[1, 2], [3, 4], [5, 6]])\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests_v2/pytorch_learner/__init__.py,0,b''
tests_v2/pytorch_learner/test_utils.py,9,"b""import unittest\n\nimport torch\n\nfrom rastervision2.pytorch_learner.utils import (\n    compute_conf_mat, compute_conf_mat_metrics)\n\n\nclass TestComputeConfMat(unittest.TestCase):\n    def test1(self):\n        y = torch.tensor([0, 1, 0, 1])\n        out = torch.tensor([0, 1, 0, 1])\n        num_labels = 2\n        conf_mat = compute_conf_mat(out, y, num_labels)\n        exp_conf_mat = torch.tensor([[2., 0], [0, 2]])\n        self.assertTrue(conf_mat.equal(exp_conf_mat))\n\n    def test2(self):\n        y = torch.tensor([0, 1, 0, 1])\n        out = torch.tensor([1, 1, 1, 1])\n        num_labels = 2\n        conf_mat = compute_conf_mat(out, y, num_labels)\n        exp_conf_mat = torch.tensor([[0., 2], [0, 2]])\n        self.assertTrue(conf_mat.equal(exp_conf_mat))\n\n\nclass TestComputeConfMatMetrics(unittest.TestCase):\n    def test1(self):\n        label_names = ['a', 'b']\n        conf_mat = torch.tensor([[2., 0], [0, 2]])\n        metrics = compute_conf_mat_metrics(conf_mat, label_names)\n        exp_metrics = {\n            'avg_precision': 1.0, 'avg_recall': 1.0, 'avg_f1': 1.0,\n            'a_precision': 1.0, 'a_recall': 1.0, 'a_f1': 1.0,\n            'b_precision': 1.0, 'b_recall': 1.0, 'b_f1': 1.0}\n        self.assertDictEqual(metrics, exp_metrics)\n\n    def test2(self):\n        label_names = ['a', 'b']\n        conf_mat = torch.tensor([[0, 2.], [2, 0]])\n        metrics = compute_conf_mat_metrics(conf_mat, label_names)\n        exp_metrics = {\n            'avg_precision': 0.0, 'avg_recall': 0.0, 'avg_f1': 0.0,\n            'a_precision': 0.0, 'a_recall': 0.0, 'a_f1': 0.0,\n            'b_precision': 0.0, 'b_recall': 0.0, 'b_f1': 0.0}\n        self.assertDictEqual(metrics, exp_metrics)\n\n    def test3(self):\n        label_names = ['a', 'b']\n        conf_mat = torch.tensor([[1, 2], [1, 2.]])\n        metrics = compute_conf_mat_metrics(conf_mat, label_names, eps=0.0)\n\n        def f1(prec, rec):\n            return 2 * (prec * rec) / (prec + rec)\n\n        def mean(a, b):\n            return (a + b) / 2\n\n        def round_dict(d):\n            return dict([(k, round(v, 3)) for k, v in d.items()])\n\n        a_prec = 1 / 2\n        a_rec = 1 / 3\n        a_f1 = f1(a_prec, a_rec)\n        b_prec = 2 / 4\n        b_rec = 2 / 3\n        b_f1 = f1(b_prec, b_rec)\n        avg_prec = mean(a_prec, b_prec)\n        avg_rec = mean(a_rec, b_rec)\n        avg_f1 = f1(avg_prec, avg_rec)\n\n        exp_metrics = {\n            'avg_precision': avg_prec, 'avg_recall': avg_rec, 'avg_f1': avg_f1,\n            'a_precision': a_prec, 'a_recall': a_rec, 'a_f1': a_f1,\n            'b_precision': b_prec, 'b_recall': b_rec, 'b_f1': b_f1}\n        self.assertDictEqual(round_dict(metrics), round_dict(exp_metrics))\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
rastervision/backend/keras_classification/__init__.py,0,b'# flake8: noqa\n\nfrom rastervision.backend.keras_classification.backend import KerasClassification\n'
rastervision/backend/keras_classification/backend.py,0,"b'from os.path import join\nimport os\nimport shutil\nimport uuid\nimport logging\n\nimport numpy as np\nfrom google.protobuf import json_format\n\nfrom rastervision.backend import Backend\nfrom rastervision.utils.files import (make_dir, get_local_path, upload_or_copy,\n                                      download_if_needed, start_sync,\n                                      sync_to_dir, sync_from_dir, list_paths)\nfrom rastervision.utils.misc import save_img\nfrom rastervision.data import ChipClassificationLabels\n\nlog = logging.getLogger(__name__)\n\n\ndef merge_class_dirs(scene_class_dirs, output_dir):\n    seen_classes = set([])\n    chip_ind = 0\n    for scene_class_dir in scene_class_dirs:\n        for class_name, src_class_dir in scene_class_dir.items():\n            dst_class_dir = join(output_dir, class_name)\n            if class_name not in seen_classes:\n                make_dir(dst_class_dir)\n                seen_classes.add(class_name)\n\n            for src_class_file in [\n                    join(src_class_dir, class_file)\n                    for class_file in os.listdir(src_class_dir)\n            ]:\n                dst_class_file = join(dst_class_dir, \'{}.png\'.format(chip_ind))\n                shutil.move(src_class_file, dst_class_file)\n                chip_ind += 1\n\n\nclass FileGroup(object):\n    def __init__(self, base_uri, tmp_dir):\n        self.tmp_dir = tmp_dir\n        self.base_uri = base_uri\n        self.base_dir = self.get_local_path(base_uri)\n\n        make_dir(self.base_dir)\n\n    def get_local_path(self, uri):\n        return get_local_path(uri, self.tmp_dir)\n\n    def upload_or_copy(self, uri):\n        upload_or_copy(self.get_local_path(uri), uri)\n\n    def download_if_needed(self, uri):\n        return download_if_needed(uri, self.tmp_dir)\n\n\nclass DatasetFiles(FileGroup):\n    """"""Utilities for files produced when calling convert_training_data.""""""\n\n    def __init__(self, base_uri, tmp_dir):\n        FileGroup.__init__(self, base_uri, tmp_dir)\n\n        self.partition_id = uuid.uuid4()\n\n        self.training_zip_uri = join(\n            base_uri, \'training-{}.zip\'.format(self.partition_id))\n        self.training_local_uri = join(self.base_dir,\n                                       \'training-{}\'.format(self.partition_id))\n        self.training_download_uri = self.get_local_path(\n            join(self.base_uri, \'training\'))\n        make_dir(self.training_local_uri)\n\n        self.validation_zip_uri = join(\n            base_uri, \'validation-{}.zip\'.format(self.partition_id))\n        self.validation_local_uri = join(\n            self.base_dir, \'validation-{}\'.format(self.partition_id))\n        self.validation_download_uri = self.get_local_path(\n            join(self.base_uri, \'validation\'))\n        make_dir(self.validation_local_uri)\n\n    def download(self):\n        def _download(split, output_dir):\n            scene_class_dirs = []\n            for uri in list_paths(self.base_uri, \'zip\'):\n                base_name = os.path.basename(uri)\n                if base_name.startswith(split):\n                    data_zip_path = self.download_if_needed(uri)\n                    data_dir = os.path.splitext(data_zip_path)[0]\n                    shutil.unpack_archive(data_zip_path, data_dir)\n\n                    # Append each of the directories containing this partitions\'\n                    # labeled images based on the class directory.\n                    data_dir_subdirectories = next(os.walk(data_dir))[1]\n                    scene_class_dirs.append(\n                        dict([(class_name, os.path.join(data_dir, class_name))\n                              for class_name in data_dir_subdirectories]))\n            merge_class_dirs(scene_class_dirs, output_dir)\n\n        _download(\'training\', self.training_download_uri)\n        _download(\'validation\', self.validation_download_uri)\n\n    def upload(self):\n        def _upload(data_dir, zip_uri, split):\n            if not any(os.scandir(data_dir)):\n                log.warn(\n                    \'No data to write for split {} in partition {}...\'.format(\n                        split, self.partition_id))\n            else:\n                shutil.make_archive(data_dir, \'zip\', data_dir)\n                upload_or_copy(data_dir + \'.zip\', zip_uri)\n\n        _upload(self.training_local_uri, self.training_zip_uri, \'training\')\n        _upload(self.validation_local_uri, self.validation_zip_uri,\n                \'validation\')\n\n\nclass ModelFiles(FileGroup):\n    """"""Utilities for files produced when calling train.""""""\n\n    def __init__(self, base_uri, tmp_dir, replace_model=False):\n        """"""Create these model files.\n\n        Args:\n            base_uri: Base URI of the model files\n            replace_model: If the model file exists, remove.\n                           Used for the training step, to retrain\n                           existing models.\n\n        Returns:\n            A new ModelFile instance.\n        """"""\n        FileGroup.__init__(self, base_uri, tmp_dir)\n\n        self.model_uri = join(self.base_uri, \'model\')\n        self.log_uri = join(self.base_uri, \'log.csv\')\n\n        if replace_model:\n            if os.path.exists(self.model_uri):\n                os.remove(self.model_uri)\n            if os.path.exists(self.log_uri):\n                os.remove(self.log_uri)\n\n    def download_backend_config(self, pretrained_model_uri, kc_config,\n                                dataset_files, class_map):\n        from rastervision.protos.keras_classification.pipeline_pb2 \\\n            import PipelineConfig\n\n        config = json_format.ParseDict(kc_config, PipelineConfig())\n\n        # Update config using local paths.\n        config.trainer.options.output_dir = self.get_local_path(self.base_uri)\n        config.model.model_path = self.get_local_path(self.model_uri)\n        config.model.nb_classes = len(class_map)\n\n        config.trainer.options.training_data_dir = \\\n            dataset_files.training_download_uri\n        config.trainer.options.validation_data_dir = \\\n            dataset_files.validation_download_uri\n\n        del config.trainer.options.class_names[:]\n        config.trainer.options.class_names.extend(class_map.get_class_names())\n\n        # Save the pretrained weights locally\n        pretrained_model_path = None\n        if pretrained_model_uri:\n            pretrained_model_path = self.download_if_needed(\n                pretrained_model_uri)\n\n        # Save an updated copy of the config file.\n        config_path = os.path.join(self.tmp_dir, \'kc_config.json\')\n        config_str = json_format.MessageToJson(config)\n        with open(config_path, \'w\') as config_file:\n            config_file.write(config_str)\n\n        return (config_path, pretrained_model_path)\n\n\nclass KerasClassification(Backend):\n    def __init__(self, backend_config, task_config):\n        self.model = None\n        self.config = backend_config\n        self.class_map = task_config.class_map\n\n    def process_scene_data(self, scene, data, tmp_dir):\n        """"""Process each scene\'s training data\n\n        Args:\n            scene: Scene\n            data: TrainingData\n\n        Returns:\n            dictionary of Scene\'s classes and corresponding local directory\n                path\n        """"""\n        scratch_dir = join(tmp_dir, \'scratch-{}\'.format(uuid.uuid4()))\n        # Ensure directory is unique since scene id\'s could be shared between\n        # training and test sets.\n        scene_dir = join(scratch_dir, \'{}-{}\'.format(scene.id, uuid.uuid4()))\n        class_dirs = {}\n\n        for chip_idx, (chip, window, labels) in enumerate(data):\n            class_id = labels.get_cell_class_id(window)\n            # If a chip is not associated with a class, don\'t\n            # use it in training data.\n            if class_id is None:\n                continue\n            class_name = self.class_map.get_by_id(class_id).name\n            class_dir = join(scene_dir, class_name)\n            make_dir(class_dir)\n            class_dirs[class_name] = class_dir\n            chip_name = \'{}.png\'.format(chip_idx)\n            chip_path = join(class_dir, chip_name)\n            save_img(chip, chip_path)\n\n        return class_dirs\n\n    def process_sceneset_results(self, training_results, validation_results,\n                                 tmp_dir):\n        """"""After all scenes have been processed, collect all the images of\n        each class across all scenes\n\n        Args:\n            training_results: list of dictionaries of training scenes\'\n                classes and corresponding local directory path\n            validation_results: list of dictionaries of validation scenes\'\n                classes and corresponding local directory path\n        """"""\n        dataset_files = DatasetFiles(self.config.training_data_uri, tmp_dir)\n        training_dir = dataset_files.training_local_uri\n        validation_dir = dataset_files.validation_local_uri\n\n        merge_class_dirs(training_results, training_dir)\n        merge_class_dirs(validation_results, validation_dir)\n        dataset_files.upload()\n\n    def train(self, tmp_dir):\n        from rastervision.backend.keras_classification.commands.train \\\n            import _train\n\n        dataset_files = DatasetFiles(self.config.training_data_uri, tmp_dir)\n        dataset_files.download()\n\n        model_files = ModelFiles(\n            self.config.training_output_uri,\n            tmp_dir,\n            replace_model=self.config.train_options.replace_model)\n        model_paths = model_files.download_backend_config(\n            self.config.pretrained_model_uri, self.config.kc_config,\n            dataset_files, self.class_map)\n        backend_config_path, pretrained_model_path = model_paths\n\n        # Get output from potential previous run so we can resume training.\n        if not self.config.train_options.replace_model:\n            sync_from_dir(self.config.training_output_uri,\n                          model_files.base_dir)\n\n        sync = start_sync(\n            model_files.base_dir,\n            self.config.training_output_uri,\n            sync_interval=self.config.train_options.sync_interval)\n        with sync:\n            do_monitoring = self.config.train_options.do_monitoring\n            _train(backend_config_path, pretrained_model_path, do_monitoring)\n\n        # Perform final sync\n        sync_to_dir(\n            model_files.base_dir, self.config.training_output_uri, delete=True)\n\n    def load_model(self, tmp_dir):\n        from rastervision.backend.keras_classification.builders \\\n            import model_builder\n\n        if self.model is None:\n            model_path = download_if_needed(self.config.model_uri, tmp_dir)\n            self.model = model_builder.build_from_path(model_path)\n            self.model._make_predict_function()\n\n    def predict(self, chips, windows, tmp_dir):\n        from rastervision.backend.keras_classification.utils \\\n            import predict\n\n        # Ensure model is loaded\n        self.load_model(tmp_dir)\n\n        probs = predict(chips, self.model)\n\n        labels = ChipClassificationLabels()\n\n        for chip_probs, window in zip(probs, windows):\n            # Add 1 to class_id since they start at 1.\n            class_id = int(np.argmax(chip_probs) + 1)\n\n            labels.set_cell(window, class_id, chip_probs)\n\n        return labels\n'"
rastervision/backend/keras_classification/utils.py,0,"b""import os\nimport shutil\n\nfrom google.protobuf import json_format\n\n\ndef file_to_str(file_path):\n    with open(file_path, 'r') as file_buffer:\n        return file_buffer.read()\n\n\ndef load_json_config(uri, message):\n    return json_format.Parse(file_to_str(uri), message)\n\n\ndef make_dir(path, check_empty=False, force_empty=False, use_dirname=False):\n    directory = path\n    if use_dirname:\n        directory = os.path.dirname(path)\n\n    if force_empty and os.path.isdir(directory):\n        shutil.rmtree(directory)\n\n    os.makedirs(directory, exist_ok=True)\n\n    is_empty = len(os.listdir(directory)) == 0\n    if check_empty and not is_empty:\n        raise ValueError(\n            '{} needs to be an empty directory!'.format(directory))\n\n\ndef predict(batch, model):\n    # Apply same transform to input as when training.\n    # TODO be able to configure this transform and the one in the\n    # training generator.\n    return model.predict(batch / 255.0)\n"""
rastervision/backend/torch_utils/__init__.py,0,b''
rastervision/backend/torch_utils/data.py,0,"b""class DataBunch():\n    def __init__(self, train_ds, train_dl, valid_ds, valid_dl, label_names):\n        self.train_ds = train_ds\n        self.train_dl = train_dl\n        self.valid_ds = valid_ds\n        self.valid_dl = valid_dl\n        self.label_names = label_names\n\n    def __repr__(self):\n        rep = ''\n        rep += 'train_ds: {} items\\n'.format(len(self.train_ds))\n        rep += 'valid_ds: {} items\\n'.format(len(self.valid_ds))\n        rep += 'label_names: ' + ','.join(self.label_names)\n        return rep\n"""
rastervision/backend/torch_utils/metrics.py,7,"b""import torch\n\n\ndef compute_conf_mat(out, y, num_labels):\n    labels = torch.arange(0, num_labels)\n    return ((out == labels[:, None]) & (y == labels[:, None, None])).sum(\n        dim=2, dtype=torch.float32)\n\n\ndef compute_conf_mat_metrics(conf_mat, eps=1e-6):\n    # eps is to avoid dividing by zero.\n    eps = torch.tensor(eps)\n    gt_count = conf_mat.sum(dim=1)\n    pred_count = conf_mat.sum(dim=0)\n    total = conf_mat.sum()\n    true_pos = torch.diag(conf_mat)\n    precision = true_pos / torch.max(pred_count, eps)\n    recall = true_pos / torch.max(gt_count, eps)\n\n    weights = gt_count / total\n    weighted_precision = (weights * precision).sum()\n    weighted_recall = (weights * recall).sum()\n    weighted_f1 = ((2 * weighted_precision * weighted_recall) / torch.max(\n        weighted_precision + weighted_recall, eps))\n    metrics = {\n        'precision': weighted_precision.item(),\n        'recall': weighted_recall.item(),\n        'f1': weighted_f1.item()\n    }\n    return metrics\n"""
rastervision/command/aux/__init__.py,0,b'# flake8: noqa\n\nfrom .cogify_command import *\n'
rastervision/command/aux/api.py,0,b'# flake8: noqa\n\nfrom .cogify_command import COGIFY\n'
rastervision/command/aux/cogify_command.py,0,"b'import os\nfrom subprocess import Popen\n\nfrom rastervision.command.aux_command import (AuxCommand, AuxCommandOptions)\nfrom rastervision.utils.files import (download_or_copy, upload_or_copy)\n\nCOGIFY = \'COGIFY\'\n\nDEFAULT_BLOCK_SIZE = 512\nDEFAULT_RESAMPLE_METHOD = \'near\'\nDEFAULT_COMPRESSION = \'deflate\'\nDEFAULT_OVERVIEWS = [2, 4, 8, 16, 32]\n\n\ndef gdal_cog_commands(input_path,\n                      tmp_dir,\n                      block_size=DEFAULT_BLOCK_SIZE,\n                      resample_method=DEFAULT_RESAMPLE_METHOD,\n                      compression=DEFAULT_COMPRESSION,\n                      overviews=None):\n    """"""\n    GDAL commands to create a COG from an input file.\n    Returns a tuple (commands, output_path)\n    """"""\n\n    if not overviews:\n        overviews = DEFAULT_OVERVIEWS\n\n    def get_output_path(command):\n        fname = os.path.splitext(os.path.basename(input_path))[0]\n        return os.path.join(tmp_dir, \'{}-{}.tif\'.format(fname, command))\n\n    compression = compression.lower()\n\n    def add_compression(cmd, overview=False):\n        if compression != \'none\':\n            if not overview:\n                return cmd[:1] + [\'-co\', \'compress={}\'.format(compression)\n                                  ] + cmd[1:]\n            else:\n                return cmd[:1] + [\n                    \'--config\', \'COMPRESS_OVERVIEW\', compression\n                ] + cmd[1:]\n        else:\n            return cmd\n\n    # Step 1: Translate to a GeoTiff.\n    translate_path = get_output_path(\'translate\')\n    translate = add_compression([\n        \'gdal_translate\', \'-of\', \'GTiff\', \'-co\', \'tiled=YES\', \'-co\',\n        \'BIGTIFF=IF_SAFER\', input_path, translate_path\n    ])\n\n    # Step 2: Add overviews\n    add_overviews = add_compression(\n        [\'gdaladdo\', \'-r\', resample_method, translate_path] + list(\n            map(lambda x: str(x), overviews)),\n        overview=True)\n\n    # Step 3: Translate to COG\n    output_path = get_output_path(\'cog\')\n\n    create_cog = add_compression([\n        \'gdal_translate\', \'-co\', \'TILED=YES\', \'-co\', \'COPY_SRC_OVERVIEWS=YES\',\n        \'-co\', \'BLOCKXSIZE={}\'.format(block_size), \'-co\',\n        \'BLOCKYSIZE={}\'.format(block_size), \'-co\', \'BIGTIFF=IF_SAFER\',\n        \'--config\', \'GDAL_TIFF_OVR_BLOCKSIZE\',\n        str(block_size), translate_path, output_path\n    ])\n\n    return ([translate, add_overviews, create_cog], output_path)\n\n\ndef run_cmd(cmd):\n    p = Popen(cmd)\n    (out, err) = p.communicate(input)\n    if p.returncode != 0:\n        s = \'Command failed:\\n\'\n        s += \' \'.join(cmd) + \'\\n\\n\'\n        if out:\n            s += out + \'\\n\\n\'\n        if err:\n            s += err\n        raise Exception(s)\n\n\ndef create_cog(source_uri,\n               dest_uri,\n               local_dir,\n               block_size=DEFAULT_BLOCK_SIZE,\n               resample_method=DEFAULT_RESAMPLE_METHOD,\n               compression=DEFAULT_COMPRESSION,\n               overviews=None):\n    local_path = download_or_copy(source_uri, local_dir)\n\n    commands, output_path = gdal_cog_commands(\n        local_path,\n        local_dir,\n        block_size=block_size,\n        resample_method=resample_method,\n        compression=compression,\n        overviews=overviews)\n    for command in commands:\n        run_cmd(command)\n\n    upload_or_copy(output_path, dest_uri)\n\n\nclass CogifyCommand(AuxCommand):\n    """"""Turns a GDAL-readable raster into a Cloud Optimized GeoTiff.\n\n    Configuration:\n\n        uris: A list of tuples of (src_path, dest_path) where dest_path is\n              the COG URI.\n\n        block_size: The tile size for the COG. Defaults to 512.\n\n        resample_method: The resample method to use for overviews. Defaults to \'near\'.\n\n        compression: The compression method to use. Defaults to \'deflate\'.\n        Use \'none\' for no compression.\n\n        overviews: The overview levels to create. Defaults to [2, 4, 8, 16, 32]\n\n    """"""\n    command_type = COGIFY\n    options = AuxCommandOptions(\n        split_on=\'uris\',\n        inputs=lambda conf: list(map(lambda x: x[0], conf[\'uris\'])),\n        outputs=lambda conf: list(map(lambda x: x[1], conf[\'uris\'])),\n        required_fields=[\'uris\'])\n\n    def run(self, tmp_dir=None):\n        if not tmp_dir:\n            tmp_dir = self.get_tmp_dir()\n\n        uris = self.command_config[\'uris\']\n        block_size = self.command_config.get(\'block_size\', DEFAULT_BLOCK_SIZE)\n        resample_method = self.command_config.get(\'resample_method\',\n                                                  DEFAULT_RESAMPLE_METHOD)\n        compression = self.command_config.get(\'compression\',\n                                              DEFAULT_COMPRESSION)\n        overviews = self.command_config.get(\'overviews\', DEFAULT_OVERVIEWS)\n\n        for src, dst in uris:\n            create_cog(\n                src,\n                dst,\n                tmp_dir,\n                block_size=block_size,\n                resample_method=resample_method,\n                compression=compression,\n                overviews=overviews)\n'"
rastervision/data/crs_transformer/__init__.py,0,b'# flake8: noqa\n\nfrom rastervision.data.crs_transformer.crs_transformer import *\nfrom rastervision.data.crs_transformer.identity_crs_transformer import *\nfrom rastervision.data.crs_transformer.rasterio_crs_transformer import *\n'
rastervision/data/crs_transformer/crs_transformer.py,0,"b'class CRSTransformer(object):\n    """"""Transforms map points in some CRS into pixel coordinates.\n\n    Each transformer is associated with a particular RasterSource.""""""\n\n    def __init__(self, image_crs=None, map_crs=None, transform=None):\n        self.image_crs = image_crs\n        self.map_crs = map_crs\n        self.transform = transform\n\n    def map_to_pixel(self, map_point):\n        """"""Transform point from map to pixel-based coordinates.\n\n        Args:\n            map_point: (x, y) tuple in map coordinates (eg. lon/lat). x and y can be\n            single values or array-like.\n\n        Returns:\n            (x, y) tuple in pixel coordinates\n        """"""\n        pass\n\n    def pixel_to_map(self, pixel_point):\n        """"""Transform point from pixel to map-based coordinates.\n\n        Args:\n            pixel_point: (x, y) tuple in pixel coordinates. x and y can be\n            single values or array-like.\n\n        Returns:\n            (x, y) tuple in map coordinates (eg. lon/lat)\n        """"""\n        pass\n\n    def get_image_crs(self):\n        return self.image_crs\n\n    def get_map_crs(self):\n        return self.map_crs\n\n    def get_affine_transform(self):\n        return self.transform\n'"
rastervision/data/crs_transformer/identity_crs_transformer.py,0,"b'from rastervision.data.crs_transformer import CRSTransformer\n\n\nclass IdentityCRSTransformer(CRSTransformer):\n    """"""Transformer for when map coordinates are already in pixel coordinates.\n\n    This is useful for non-georeferenced imagery.\n    """"""\n\n    def map_to_pixel(self, map_point):\n        """"""Identity function.\n\n        Args:\n            map_point: (x, y) tuple in pixel coordinates\n\n        Returns:\n            (x, y) tuple in pixel coordinates\n        """"""\n        return map_point\n\n    def pixel_to_map(self, pixel_point):\n        """"""Identity function.\n\n        Args:\n            pixel_point: (x, y) tuple in pixel coordinates\n\n        Returns:\n            (x, y) tuple in pixel coordinates\n        """"""\n        return pixel_point\n'"
rastervision/data/crs_transformer/rasterio_crs_transformer.py,0,"b'import pyproj\n\nfrom rasterio.transform import (rowcol, xy)\n\nfrom rastervision.data.crs_transformer import (CRSTransformer,\n                                               IdentityCRSTransformer)\n\n\nclass RasterioCRSTransformer(CRSTransformer):\n    """"""Transformer for a RasterioRasterSource.""""""\n\n    def __init__(self, transform, image_crs, map_crs=\'epsg:4326\'):\n        """"""Construct transformer.\n\n        Args:\n            image_dataset: Rasterio DatasetReader\n            map_crs: CRS code\n        """"""\n        self.map_proj = pyproj.Proj(init=map_crs)\n        self.image_proj = pyproj.Proj(image_crs)\n\n        super().__init__(image_crs, map_crs, transform)\n\n    def map_to_pixel(self, map_point):\n        """"""Transform point from map to pixel-based coordinates.\n\n        Args:\n            map_point: (x, y) tuple in map coordinates\n\n        Returns:\n            (x, y) tuple in pixel coordinates\n        """"""\n        image_point = pyproj.transform(self.map_proj, self.image_proj,\n                                       map_point[0], map_point[1])\n        pixel_point = rowcol(self.transform, image_point[0], image_point[1])\n        pixel_point = (pixel_point[1], pixel_point[0])\n        return pixel_point\n\n    def pixel_to_map(self, pixel_point):\n        """"""Transform point from pixel to map-based coordinates.\n\n        Args:\n            pixel_point: (x, y) tuple in pixel coordinates\n\n        Returns:\n            (x, y) tuple in map coordinates\n        """"""\n        image_point = xy(self.transform, int(pixel_point[1]),\n                         int(pixel_point[0]))\n        map_point = pyproj.transform(self.image_proj, self.map_proj,\n                                     image_point[0], image_point[1])\n        return map_point\n\n    @classmethod\n    def from_dataset(cls, dataset, map_crs=\'epsg:4326\'):\n        if dataset.crs is None:\n            return IdentityCRSTransformer()\n        transform = dataset.transform\n        image_crs = dataset.crs\n        return cls(transform, image_crs, map_crs)\n\n    def get_affine_transform(self):\n        return self.transform\n'"
rastervision/data/label/__init__.py,0,b'# flake8: noqa\n\nfrom rastervision.data.label.labels import *\nfrom rastervision.data.label.chip_classification_labels import *\nfrom rastervision.data.label.object_detection_labels import *\nfrom rastervision.data.label.semantic_segmentation_labels import *\n'
rastervision/data/label/chip_classification_labels.py,0,"b'from rastervision.core.box import Box\nfrom rastervision.data.label import Labels\n\n\nclass ChipClassificationLabels(Labels):\n    """"""Represents a spatial grid of cells associated with classes.""""""\n\n    def __init__(self):\n        self.cell_to_class_id = {}\n\n    def __len__(self):\n        return len(self.cell_to_class_id)\n\n    def __eq__(self, other):\n        return (isinstance(other, ChipClassificationLabels)\n                and self.cell_to_class_id == other.cell_to_class_id)\n\n    def __add__(self, other):\n        result = ChipClassificationLabels()\n        result.extend(self)\n        result.extend(other)\n        return result\n\n    def filter_by_aoi(self, aoi_polygons):\n        result = ChipClassificationLabels()\n        for cell in self.cell_to_class_id:\n            cell_box = Box.from_tuple(cell)\n            cell_poly = cell_box.to_shapely()\n            for aoi in aoi_polygons:\n                if cell_poly.within(aoi):\n                    (class_id, scores) = self.cell_to_class_id[cell]\n                    result.set_cell(cell_box, class_id, scores)\n        return result\n\n    def set_cell(self, cell, class_id, scores=None):\n        """"""Set cell and its class_id.\n\n        Args:\n            cell: (Box)\n            class_id: int\n            scores: 1d numpy array of probabilities for each class\n        """"""\n        if scores is not None:\n            scores = list(map(lambda x: float(x), list(scores)))\n        self.cell_to_class_id[cell.tuple_format()] = (class_id, scores)\n\n    def get_cell_class_id(self, cell):\n        """"""Return class_id for a cell.\n\n        Args:\n            cell: (Box)\n        """"""\n        result = self.cell_to_class_id.get(cell.tuple_format())\n        if result:\n            return result[0]\n        else:\n            return None\n\n    def get_cell_scores(self, cell):\n        """"""Return scores for a cell.\n\n        Args:\n            cell: (Box)\n        """"""\n        result = self.cell_to_class_id.get(cell.tuple_format())\n        if result:\n            return result[1]\n        else:\n            return None\n\n    def get_cell_values(self, cell):\n        """"""Return a tuple of (class_id, scores) for a cell.\n\n        Args:\n            cell: (Box)\n        """"""\n        return self.cell_to_class_id.get(cell.tuple_format())\n\n    def get_singleton_labels(self, cell):\n        """"""Return Labels object representing a single cell.\n\n        Args:\n            cell: (Box)\n        """"""\n        class_id, scores = self.get_cell_values(cell)\n        labels = ChipClassificationLabels()\n        labels.set_cell(cell, class_id, scores)\n        return labels\n\n    def get_cells(self):\n        """"""Return list of all cells (list of Box).""""""\n        return [\n            Box.from_npbox(box_tup)\n            for box_tup in self.cell_to_class_id.keys()\n        ]\n\n    def get_class_ids(self):\n        """"""Return list of class_ids for all cells.""""""\n        return list(map(lambda x: x[0], self.cell_to_class_id.values()))\n\n    def get_scores(self):\n        """"""Return list of scores for all cells.""""""\n        return list(map(lambda x: x[1], self.cell_to_class_id.values()))\n\n    def get_values(self):\n        """"""Return list of class_ids and scores for all cells.""""""\n        return list(self.cell_to_class_id.values())\n\n    def extend(self, labels):\n        """"""Adds cells contained in labels.\n\n        Args:\n            labels: ChipClassificationLabels\n        """"""\n        for cell in labels.get_cells():\n            class_id, scores = labels.get_cell_values(cell)\n            self.set_cell(cell, class_id, scores)\n'"
rastervision/data/label/labels.py,0,"b'from abc import (ABC, abstractmethod)\n\n\nclass Labels(ABC):\n    """"""A set of spatially referenced labels.\n\n    A set of labels predicted by a model or provided by human labelers for the\n    sake of training.\n    """"""\n\n    @abstractmethod\n    def __add__(self, other):\n        """"""Add labels to these labels.\n\n        Returns a concatenation of this and the other labels.\n        """"""\n        pass\n\n    @abstractmethod\n    def filter_by_aoi(self, aoi_polygons):\n        """"""Returns a copy of these labels filtered by a given set of AOI polygons\n\n        Args:\n          aoi_polygons - A list of AOI polygons to filter by, in pixel coordinates.\n        """"""\n        pass\n\n    @abstractmethod\n    def __eq__(self, other):\n        pass\n'"
rastervision/data/label/object_detection_labels.py,0,"b'import numpy as np\nfrom shapely.geometry import shape\n\nfrom rastervision.core.box import Box\nfrom rastervision.data.label import Labels\nfrom rastervision.data.label.tfod_utils.np_box_list import BoxList\nfrom rastervision.data.label.tfod_utils.np_box_list_ops import (\n    prune_non_overlapping_boxes, clip_to_window, concatenate,\n    non_max_suppression)\n\n\nclass ObjectDetectionLabels(Labels):\n    """"""A set of boxes and associated class_ids and scores.\n\n    Implemented using the Tensorflow Object Detection API\'s BoxList class.\n    """"""\n\n    def __init__(self, npboxes, class_ids, scores=None):\n        """"""Construct a set of object detection labels.\n\n        Args:\n            npboxes: float numpy array of size nx4 with cols\n                ymin, xmin, ymax, xmax. Should be in pixel coordinates within\n                the global frame of reference.\n            class_ids: int numpy array of size n with class ids starting at 1\n            scores: float numpy array of size n\n        """"""\n        self.boxlist = BoxList(npboxes)\n        # This field name actually needs to be \'classes\' to be able to use\n        # certain utility functions in the TF Object Detection API.\n        self.boxlist.add_field(\'classes\', class_ids)\n        # We need to ensure that there is always a scores field so that the\n        # concatenate method will work with empty labels objects.\n        if scores is None:\n            scores = np.ones(class_ids.shape)\n        self.boxlist.add_field(\'scores\', scores)\n\n    def __add__(self, other):\n        return ObjectDetectionLabels.concatenate(self, other)\n\n    def __eq__(self, other):\n        return (isinstance(other, ObjectDetectionLabels)\n                and self.to_dict() == other.to_dict())\n\n    def assert_equal(self, expected_labels):\n        np.testing.assert_array_equal(self.get_npboxes(),\n                                      expected_labels.get_npboxes())\n        np.testing.assert_array_equal(self.get_class_ids(),\n                                      expected_labels.get_class_ids())\n        np.testing.assert_array_equal(self.get_scores(),\n                                      expected_labels.get_scores())\n\n    def filter_by_aoi(self, aoi_polygons):\n        boxes = self.get_boxes()\n        class_ids = self.get_class_ids()\n        scores = self.get_scores()\n\n        new_boxes = []\n        new_class_ids = []\n        new_scores = []\n        for box, class_id, score in zip(boxes, class_ids, scores):\n            box_poly = box.to_shapely()\n            for aoi in aoi_polygons:\n                if box_poly.within(aoi):\n                    new_boxes.append(box.npbox_format())\n                    new_class_ids.append(class_id)\n                    new_scores.append(score)\n                    break\n\n        if len(new_boxes) == 0:\n            return ObjectDetectionLabels.make_empty()\n\n        return ObjectDetectionLabels(\n            np.array(new_boxes), np.array(new_class_ids), np.array(new_scores))\n\n    @staticmethod\n    def make_empty():\n        npboxes = np.empty((0, 4))\n        class_ids = np.empty((0, ))\n        scores = np.empty((0, ))\n        return ObjectDetectionLabels(npboxes, class_ids, scores)\n\n    @staticmethod\n    def from_boxlist(boxlist):\n        """"""Make ObjectDetectionLabels from BoxList object.""""""\n        scores = (boxlist.get_field(\'scores\')\n                  if boxlist.has_field(\'scores\') else None)\n        return ObjectDetectionLabels(\n            boxlist.get(), boxlist.get_field(\'classes\'), scores=scores)\n\n    @staticmethod\n    def from_geojson(geojson, extent=None):\n        """"""Convert GeoJSON to ObjectDetectionLabels object.\n\n        If extent is provided, filter out the boxes that lie ""more than a little\n        bit"" outside the extent.\n\n        Args:\n            geojson: (dict) normalized GeoJSON (see VectorSource)\n            extent: (Box) in pixel coords\n\n        Returns:\n            ObjectDetectionLabels\n        """"""\n        boxes = []\n        class_ids = []\n        scores = []\n\n        for f in geojson[\'features\']:\n            geom = shape(f[\'geometry\'])\n            (xmin, ymin, xmax, ymax) = geom.bounds\n            boxes.append(Box(ymin, xmin, ymax, xmax))\n\n            props = f[\'properties\']\n            class_ids.append(props[\'class_id\'])\n            scores.append(props.get(\'score\', 1.0))\n\n        if len(boxes):\n            boxes = np.array(\n                [box.npbox_format() for box in boxes], dtype=float)\n            class_ids = np.array(class_ids)\n            scores = np.array(scores)\n            labels = ObjectDetectionLabels(boxes, class_ids, scores=scores)\n        else:\n            labels = ObjectDetectionLabels.make_empty()\n\n        if extent is not None:\n            labels = ObjectDetectionLabels.get_overlapping(\n                labels, extent, ioa_thresh=0.8, clip=True)\n        return labels\n\n    def get_boxes(self):\n        """"""Return list of Boxes.""""""\n        return [Box.from_npbox(npbox) for npbox in self.boxlist.get()]\n\n    def get_npboxes(self):\n        return self.boxlist.get()\n\n    def get_scores(self):\n        if self.boxlist.has_field(\'scores\'):\n            return self.boxlist.get_field(\'scores\')\n        return None\n\n    def get_class_ids(self):\n        return self.boxlist.get_field(\'classes\')\n\n    def __len__(self):\n        return self.boxlist.get().shape[0]\n\n    def __str__(self):\n        return str(self.boxlist.get())\n\n    def to_boxlist(self):\n        return self.boxlist\n\n    def to_dict(self):\n        """"""Returns a dict version of these labels.\n\n        The Dict has a Box as a key, and a tuple of (class_id, score)\n        as the values.\n        """"""\n        d = {}\n        boxes = list(map(Box.from_npbox, self.get_npboxes()))\n        classes = list(self.get_class_ids())\n        scores = list(self.get_scores())\n        for box, class_id, score in zip(boxes, classes, scores):\n            d[box.tuple_format()] = (class_id, score)\n        return d\n\n    @staticmethod\n    def local_to_global(npboxes, window):\n        """"""Convert from local to global coordinates.\n\n        The local coordinates are row/col within the window frame of reference.\n        The global coordinates are row/col within the extent of a RasterSource.\n        """"""\n        xmin = window.xmin\n        ymin = window.ymin\n        return npboxes + np.array([[ymin, xmin, ymin, xmin]])\n\n    @staticmethod\n    def global_to_local(npboxes, window):\n        """"""Convert from global to local coordinates.\n\n        The global coordinates are row/col within the extent of a RasterSource.\n        The local coordinates are row/col within the window frame of reference.\n        """"""\n        xmin = window.xmin\n        ymin = window.ymin\n        return npboxes - np.array([[ymin, xmin, ymin, xmin]])\n\n    @staticmethod\n    def local_to_normalized(npboxes, window):\n        """"""Convert from local to normalized coordinates.\n\n        The local coordinates are row/col within the window frame of reference.\n        Normalized coordinates range from 0 to 1 on each (height/width) axis.\n        """"""\n        height = window.get_height()\n        width = window.get_width()\n        return npboxes / np.array([[height, width, height, width]])\n\n    @staticmethod\n    def normalized_to_local(npboxes, window):\n        """"""Convert from normalized to local coordinates.\n\n        Normalized coordinates range from 0 to 1 on each (height/width) axis.\n        The local coordinates are row/col within the window frame of reference.\n        """"""\n        height = window.get_height()\n        width = window.get_width()\n        return npboxes * np.array([[height, width, height, width]])\n\n    @staticmethod\n    def get_overlapping(labels, window, ioa_thresh=0.000001, clip=False):\n        """"""Return subset of labels that overlap with window.\n\n        Args:\n            labels: ObjectDetectionLabels\n            window: Box\n            ioa_thresh: the minimum IOA for a box to be considered as\n                overlapping\n            clip: if True, clip label boxes to the window\n        """"""\n        window_npbox = window.npbox_format()\n        window_boxlist = BoxList(np.expand_dims(window_npbox, axis=0))\n        boxlist = prune_non_overlapping_boxes(\n            labels.boxlist, window_boxlist, minoverlap=ioa_thresh)\n        if clip:\n            boxlist = clip_to_window(boxlist, window_npbox)\n\n        return ObjectDetectionLabels.from_boxlist(boxlist)\n\n    @staticmethod\n    def concatenate(labels1, labels2):\n        """"""Return concatenation of labels.\n\n        Args:\n            labels1: ObjectDetectionLabels\n            labels2: ObjectDetectionLabels\n        """"""\n        new_boxlist = concatenate([labels1.to_boxlist(), labels2.to_boxlist()])\n        return ObjectDetectionLabels.from_boxlist(new_boxlist)\n\n    @staticmethod\n    def prune_duplicates(labels, score_thresh, merge_thresh):\n        """"""Remove duplicate boxes.\n\n        Runs non-maximum suppression to remove duplicate boxes that result from\n        sliding window prediction algorithm.\n\n        Args:\n            labels: ObjectDetectionLabels\n            score_thresh: the minimum allowed score of boxes\n            merge_thresh: the minimum IOA allowed when merging two boxes\n                together\n\n        Returns:\n            ObjectDetectionLabels\n        """"""\n        max_output_size = 1000000\n        pruned_boxlist = non_max_suppression(\n            labels.boxlist,\n            max_output_size=max_output_size,\n            iou_threshold=merge_thresh,\n            score_threshold=score_thresh)\n        return ObjectDetectionLabels.from_boxlist(pruned_boxlist)\n'"
rastervision/data/label/semantic_segmentation_labels.py,0,"b'from rastervision.data.label import Labels\n\nimport numpy as np\nfrom rasterio.features import rasterize\nimport shapely\n\n\nclass SemanticSegmentationLabels(Labels):\n    """"""A set of spatially referenced semantic segmentation labels.\n\n    Since labels are represented as rasters, the labels for a scene can take up a lot of\n    memory. Therefore, to avoid running out of memory, labels are computed as needed for\n    windows.\n    """"""\n\n    def __init__(self, windows, label_fn, aoi_polygons=None):\n        """"""Constructor\n\n        Args:\n            windows: a list of Box representing the windows covering a scene\n            label_fn: a function that takes a window (Box) and returns a label array\n                of the same shape with each value a class id.\n            aoi_polygons: a list of shapely.geom that contains the AOIs\n                (areas of interest) for a scene.\n\n        """"""\n        self.windows = windows\n        self.label_fn = label_fn\n        self.aoi_polygons = aoi_polygons\n\n    def __add__(self, other):\n        """"""Add labels to these labels.\n\n        Returns a concatenation of this and the other labels.\n        """"""\n        return SemanticSegmentationLabels(\n            self.windows + other.windows,\n            self.label_fn,\n            aoi_polygons=self.aoi_polygons)\n\n    def __eq__(self, other):\n        for window in self.get_windows():\n            if not np.array_equal(\n                    self.get_label_arr(window), other.get_label_arr(window)):\n                return False\n        return True\n\n    def filter_by_aoi(self, aoi_polygons):\n        """"""Returns a new SemanticSegmentationLabels object with aoi_polygons set.""""""\n        return SemanticSegmentationLabels(\n            self.windows, self.label_fn, aoi_polygons=aoi_polygons)\n\n    def add_window(self, window):\n        self.windows.append(window)\n\n    def get_windows(self):\n        return self.windows\n\n    def get_label_arr(self, window, clip_extent=None):\n        """"""Get the label array for a window.\n\n        Note: the window should be kept relatively small to avoid running out of memory.\n\n        Args:\n            window: Box\n            clip_extent: a Box representing the extent of the corresponding Scene\n\n        Returns:\n            np.ndarray of class_ids with zeros filled in outside the AOIs and clipped\n                to the clip_extent\n        """"""\n        window_geom = window.to_shapely()\n\n        if not self.aoi_polygons:\n            label_arr = self.label_fn(window)\n        else:\n            # For each aoi_polygon, intersect with window, and put in window frame of\n            # reference.\n            window_aois = []\n            for aoi in self.aoi_polygons:\n                window_aoi = aoi.intersection(window_geom)\n                if not window_aoi.is_empty:\n\n                    def transform_shape(x, y, z=None):\n                        return (x - window.xmin, y - window.ymin)\n\n                    window_aoi = shapely.ops.transform(transform_shape,\n                                                       window_aoi)\n                    window_aois.append(window_aoi)\n\n            if window_aois:\n                # If window intersects with AOI, set pixels outside the AOI polygon to 0,\n                # so they are ignored during eval.\n                label_arr = self.label_fn(window)\n                mask = rasterize(\n                    [(p, 0) for p in window_aois],\n                    out_shape=label_arr.shape,\n                    fill=1,\n                    dtype=np.uint8)\n                label_arr[mask.astype(np.bool)] = 0\n            else:\n                # If window does\'t overlap with any AOI, then return all zeros.\n                label_arr = np.zeros((window.get_height(), window.get_width()))\n\n        if clip_extent is not None:\n            clip_window = window.intersection(clip_extent)\n            label_arr = label_arr[0:clip_window.get_height(), 0:\n                                  clip_window.get_width()]\n\n        return label_arr\n'"
rastervision/data/label_source/__init__.py,0,b'# flake8: noqa\n\nfrom rastervision.data.label_source.label_source import *\nfrom rastervision.data.label_source.label_source_config import *\nfrom rastervision.data.label_source.chip_classification_label_source import *\nfrom rastervision.data.label_source.chip_classification_label_source_config import *\nfrom rastervision.data.label_source.object_detection_label_source import *\nfrom rastervision.data.label_source.object_detection_label_source_config import *\nfrom rastervision.data.label_source.segmentation_class_transformer import SegmentationClassTransformer\nfrom rastervision.data.label_source.semantic_segmentation_label_source import *\nfrom rastervision.data.label_source.semantic_segmentation_label_source_config import *\n'
rastervision/data/label_source/api.py,0,"b""# flake8: noqa\n\nfrom rastervision.task.api import (OBJECT_DETECTION, CHIP_CLASSIFICATION,\n                                   SEMANTIC_SEGMENTATION)\n\n# Registry Keys\nLABEL_SOURCE = 'LABEL_SOURCE'\n\n# Deprecated keys provided for backward compatibility.\nOBJECT_DETECTION_GEOJSON = 'OBJECT_DETECTION_GEOJSON'\nCHIP_CLASSIFICATION_GEOJSON = 'CHIP_CLASSIFICATION_GEOJSON'\nSEMANTIC_SEGMENTATION_RASTER = 'SEMANTIC_SEGMENTATION_RASTER'\n\nlabel_source_deprecated_map = {\n    OBJECT_DETECTION_GEOJSON: OBJECT_DETECTION,\n    CHIP_CLASSIFICATION_GEOJSON: CHIP_CLASSIFICATION,\n    SEMANTIC_SEGMENTATION_RASTER: SEMANTIC_SEGMENTATION\n}\n\nfrom rastervision.data.label_source.label_source_config import LabelSourceConfig\n"""
rastervision/data/label_source/chip_classification_label_source.py,0,"b'import numpy as np\nfrom shapely.strtree import STRtree\nfrom shapely.geometry import shape\n\nimport rastervision as rv\nfrom rastervision.data import ChipClassificationLabels\nfrom rastervision.data.label_source import LabelSource\nfrom rastervision.core import Box\n\n\ndef infer_cell(cell, str_tree, ioa_thresh, use_intersection_over_cell,\n               background_class_id, pick_min_class_id):\n    """"""Infer the class_id of a cell given a set of polygons.\n\n    Given a cell and a set of polygons, the problem is to infer the class_id\n    that best captures the content of the cell. This is non-trivial since there\n    can be multiple polygons of differing classes overlapping with the cell.\n    Any polygons that sufficiently overlaps with the cell are in the running for\n    setting the class_id. If there are none in the running, the cell is either\n    considered null or background. See args for more details.\n\n    Args:\n        cell: Box\n        str_tree: (STRtree) collection of geoms in scene used for geometric queries.\n            The geoms need to have class_id monkey-patched onto them.\n        ioa_thresh: (float) the minimum IOA of a polygon and cell for that\n            polygon to be a candidate for setting the class_id\n        use_intersection_over_cell: (bool) If true, then use the area of the\n            cell as the denominator in the IOA. Otherwise, use the area of the\n            polygon.\n        background_class_id: (None or int) If not None, class_id to use as the\n            background class; ie. the one that is used when a window contains\n            no boxes. If not set, empty windows have None set as their class_id\n            which is considered a null value.\n        pick_min_class_id: If true, the class_id for a cell is the minimum\n            class_id of the boxes in that cell. Otherwise, pick the class_id of\n            the box covering the greatest area.\n    """"""\n    cell_geom = cell.to_shapely()\n    inter_polys = str_tree.query(cell_geom)\n\n    inter_over_cells = []\n    inter_over_polys = []\n    class_ids = []\n\n    # Find polygons whose intersection with the cell is big enough.\n    for poly in inter_polys:\n        inter = poly.intersection(cell_geom)\n        inter_over_cell = inter.area / cell_geom.area\n        inter_over_poly = inter.area / poly.area\n\n        if use_intersection_over_cell:\n            enough_inter = inter_over_cell >= ioa_thresh\n        else:\n            enough_inter = inter_over_poly >= ioa_thresh\n\n        if enough_inter:\n            inter_over_cells.append(inter_over_cell)\n            inter_over_polys.append(inter_over_poly)\n            class_ids.append(poly.class_id)\n\n    # Infer class id for cell.\n    if len(class_ids) == 0:\n        class_id = (None if background_class_id == 0 else background_class_id)\n    elif pick_min_class_id:\n        class_id = min(class_ids)\n    else:\n        # Pick class_id of the polygon with the biggest intersection over\n        # cell. If there is a tie, pick the first.\n        class_id = class_ids[np.argmax(inter_over_cells)]\n\n    return class_id\n\n\ndef infer_labels(geojson, extent, cell_size, ioa_thresh,\n                 use_intersection_over_cell, pick_min_class_id,\n                 background_class_id):\n    """"""Infer ChipClassificationLabels grid from GeoJSON containing polygons.\n\n    Given GeoJSON with polygons associated with class_ids, infer a grid of\n    cells and class_ids that best captures the contents of each cell. See infer_cell for\n    info on the args.\n\n    Args:\n        geojson: dict in normalized GeoJSON format (see VectorSource)\n        extent: Box representing the bounds of the grid\n\n    Returns:\n        ChipClassificationLabels\n    """"""\n    labels = ChipClassificationLabels()\n    cells = extent.get_windows(cell_size, cell_size)\n\n    # We need to associate class_id with each geom. Monkey-patching it onto the geom\n    # seems like a bad idea, but it\'s the only straightforward way of doing this\n    # that I\'ve been able to find.\n    geoms = []\n    for f in geojson[\'features\']:\n        g = shape(f[\'geometry\'])\n        g.class_id = f[\'properties\'][\'class_id\']\n        geoms.append(g)\n    str_tree = STRtree(geoms)\n\n    for cell in cells:\n        class_id = infer_cell(cell, str_tree, ioa_thresh,\n                              use_intersection_over_cell, background_class_id,\n                              pick_min_class_id)\n        labels.set_cell(cell, class_id)\n    return labels\n\n\ndef read_labels(geojson, extent=None):\n    """"""Convert GeoJSON to ChipClassificationLabels.\n\n    If the GeoJSON already contains a grid of cells, then it can be constructed\n    in a straightforward manner without having to infer the class of cells.\n\n    If extent is given, only labels that intersect with the extent are returned.\n\n    Args:\n        geojson: dict in normalized GeoJSON format (see VectorSource)\n        extent: Box in pixel coords\n\n    Returns:\n       ChipClassificationLabels\n    """"""\n    labels = ChipClassificationLabels()\n\n    for f in geojson[\'features\']:\n        geom = shape(f[\'geometry\'])\n        (xmin, ymin, xmax, ymax) = geom.bounds\n        cell = Box(ymin, xmin, ymax, xmax)\n        if extent is not None and not cell.to_shapely().intersects(\n                extent.to_shapely()):\n            continue\n\n        props = f[\'properties\']\n        class_id = props[\'class_id\']\n        scores = props.get(\'scores\')\n        labels.set_cell(cell, class_id, scores)\n\n    return labels\n\n\nclass ChipClassificationLabelSource(LabelSource):\n    """"""A source of chip classification labels.\n\n    Ideally the vector_source contains a square for each cell in the grid. But\n    in reality, it can be difficult to label imagery in such an exhaustive way.\n    So, this can also handle sources with non-overlapping polygons that\n    do not necessarily cover the entire extent. It infers the grid of cells\n    and associated class_ids using the extent and options if infer_cells is\n    set to True.\n    """"""\n\n    def __init__(self,\n                 vector_source,\n                 crs_transformer,\n                 class_map,\n                 extent=None,\n                 ioa_thresh=None,\n                 use_intersection_over_cell=False,\n                 pick_min_class_id=False,\n                 background_class_id=None,\n                 cell_size=None,\n                 infer_cells=False):\n        """"""Constructs a LabelSource for ChipClassificaiton backed by a GeoJSON file.\n\n        Args:\n            vector_source: (VectorSource or str)\n            crs_transformer: CRSTransformer to convert from map coords in label\n                in GeoJSON file to pixel coords.\n            class_map: ClassMap used to infer class_ids from class_name\n                (or label) field\n            extent: Box used to filter the labels by extent or compute grid\n        """"""\n        if isinstance(vector_source, str):\n            provider = rv._registry.get_vector_source_default_provider(\n                vector_source)\n            vector_source = provider.construct(vector_source) \\\n                .create_source(\n                    crs_transformer=crs_transformer, extent=extent, class_map=class_map)\n\n        geojson = vector_source.get_geojson()\n\n        if infer_cells:\n            self.labels = infer_labels(geojson, extent, cell_size, ioa_thresh,\n                                       use_intersection_over_cell,\n                                       pick_min_class_id, background_class_id)\n        else:\n            self.labels = read_labels(geojson, extent)\n\n    def get_labels(self, window=None):\n        if window is None:\n            return self.labels\n        return self.labels.get_singleton_labels(window)\n'"
rastervision/data/label_source/chip_classification_label_source_config.py,0,"b'from copy import deepcopy\n\nimport rastervision as rv\nfrom rastervision.data.label_source import (\n    LabelSourceConfig, LabelSourceConfigBuilder, ChipClassificationLabelSource)\nfrom rastervision.data.vector_source import VectorSourceConfig\nfrom rastervision.protos.label_source_pb2 import (LabelSourceConfig as\n                                                  LabelSourceConfigMsg)\n\n\nclass ChipClassificationLabelSourceConfig(LabelSourceConfig):\n    def __init__(self,\n                 vector_source,\n                 ioa_thresh=None,\n                 use_intersection_over_cell=False,\n                 pick_min_class_id=False,\n                 background_class_id=None,\n                 cell_size=None,\n                 infer_cells=False):\n        super().__init__(source_type=rv.CHIP_CLASSIFICATION)\n        self.vector_source = vector_source\n        self.ioa_thresh = ioa_thresh\n        self.use_intersection_over_cell = use_intersection_over_cell\n        self.pick_min_class_id = pick_min_class_id\n        self.background_class_id = background_class_id\n        self.cell_size = cell_size\n        self.infer_cells = infer_cells\n\n    def to_proto(self):\n        msg = super().to_proto()\n        options = LabelSourceConfigMsg.ChipClassificationLabelSource(\n            vector_source=self.vector_source.to_proto(),\n            ioa_thresh=self.ioa_thresh,\n            use_intersection_over_cell=self.use_intersection_over_cell,\n            pick_min_class_id=self.pick_min_class_id,\n            background_class_id=self.background_class_id,\n            cell_size=self.cell_size,\n            infer_cells=self.infer_cells)\n        msg.chip_classification_label_source.CopyFrom(options)\n        return msg\n\n    def create_source(self, task_config, extent, crs_transformer, tmp_dir):\n        vector_source = self.vector_source.create_source(\n            crs_transformer=crs_transformer,\n            extent=extent,\n            class_map=task_config.class_map)\n        return ChipClassificationLabelSource(\n            vector_source, crs_transformer, task_config.class_map, extent,\n            self.ioa_thresh, self.use_intersection_over_cell,\n            self.pick_min_class_id, self.background_class_id, self.cell_size,\n            self.infer_cells)\n\n    def update_for_command(self, command_type, experiment_config,\n                           context=None):\n        self.vector_source.update_for_command(command_type, experiment_config,\n                                              context)\n\n        if not self.cell_size:\n            self.cell_size = experiment_config.task.chip_size\n\n    def report_io(self, command_type, io_def):\n        self.vector_source.report_io(command_type, io_def)\n\n\nclass ChipClassificationLabelSourceConfigBuilder(LabelSourceConfigBuilder):\n    def __init__(self, prev=None):\n        config = {}\n        if prev:\n            config = {\n                \'vector_source\': prev.vector_source,\n                \'ioa_thresh\': prev.ioa_thresh,\n                \'use_intersection_over_cell\': prev.use_intersection_over_cell,\n                \'pick_min_class_id\': prev.pick_min_class_id,\n                \'background_class_id\': prev.background_class_id,\n                \'cell_size\': prev.cell_size,\n                \'infer_cells\': prev.infer_cells\n            }\n\n        super().__init__(ChipClassificationLabelSourceConfig, config)\n\n    def validate(self):\n        super().validate()\n\n        vector_source = self.config.get(\'vector_source\')\n        if vector_source is None:\n            raise rv.ConfigError(\n                \'You must set the vector_source for ChipClassificationLabelSourceConfig\'\n                \' Use ""with_vector_source"".\')\n        if not isinstance(vector_source, VectorSourceConfig):\n            raise rv.ConfigError(\n                \'vector source must be a child of class VectorSourceConfig, got {}\'.\n                format(type(vector_source)))\n        if vector_source.has_null_class_bufs():\n            raise rv.ConfigError(\n                \'Setting buffer to None for a class in the vector_source is not allowed \'\n                \'for ChipClassificationLabelSourceConfig.\')\n\n    def from_proto(self, msg):\n        # Added for backwards compatibility.\n        if msg.HasField(\'chip_classification_geojson_source\'):\n            conf = msg.chip_classification_geojson_source\n            vector_source = conf.uri\n        else:\n            conf = msg.chip_classification_label_source\n            vector_source = rv.VectorSourceConfig.from_proto(\n                conf.vector_source)\n\n        return self \\\n            .with_vector_source(vector_source) \\\n            .with_ioa_thresh(conf.ioa_thresh) \\\n            .with_use_intersection_over_cell(conf.use_intersection_over_cell) \\\n            .with_pick_min_class_id(conf.pick_min_class_id) \\\n            .with_background_class_id(conf.background_class_id) \\\n            .with_cell_size(conf.cell_size) \\\n            .with_infer_cells(conf.infer_cells)\n\n    def with_vector_source(self, vector_source):\n        """"""Set the vector_source.\n\n        Args:\n            vector_source (str or VectorSource) if a string, assume it is\n                a URI and use the default provider to construct a VectorSource.\n        """"""\n        if isinstance(vector_source, str):\n            return self.with_uri(vector_source)\n\n        b = deepcopy(self)\n        if isinstance(vector_source, VectorSourceConfig):\n            b.config[\'vector_source\'] = vector_source\n        else:\n            raise rv.ConfigError(\n                \'vector_source must be of type str or VectorSource\')\n\n        return b\n\n    def with_uri(self, uri):\n        b = deepcopy(self)\n        provider = rv._registry.get_vector_source_default_provider(uri)\n        b.config[\'vector_source\'] = provider.construct(uri)\n        return b\n\n    def with_ioa_thresh(self, ioa_thresh):\n        """"""The minimum IOA of a polygon and cell.""""""\n        b = deepcopy(self)\n        b.config[\'ioa_thresh\'] = ioa_thresh\n        return b\n\n    def with_use_intersection_over_cell(self, use_intersection_over_cell):\n        """""" Set this label source to use intersection over cell or not.\n\n        If use_intersection_over_cell is true, then use the area of the\n        cell as the denominator in the IOA. Otherwise, use the area of the\n        polygon.\n        """"""\n        b = deepcopy(self)\n        b.config[\'use_intersection_over_cell\'] = use_intersection_over_cell\n        return b\n\n    def with_pick_min_class_id(self, pick_min_class_id):\n        """"""Set this label source to pick min class ID\n\n        If true, the class_id for a cell is the minimum class_id of the\n        boxes in that cell. Otherwise, pick the class_id of the box\n        covering the greatest area.\n        """"""\n        b = deepcopy(self)\n        b.config[\'pick_min_class_id\'] = pick_min_class_id\n        return b\n\n    def with_background_class_id(self, background_class_id):\n        """"""Sets the background class ID.\n\n        Optional class_id to use as the background class; ie. the one that\n        is used when a window contains no boxes. If not set, empty windows\n        have None set as their class_id.\n        """"""\n        b = deepcopy(self)\n        b.config[\'background_class_id\'] = background_class_id\n        return b\n\n    def with_infer_cells(self, infer_cells):\n        """"""Set if this label source should infer cells.\n\n        If true, the label source will infer the cell polygon and label\n        from the polygons in the vector_source. If the labels are already\n        cells and properly labeled, this can be False.\n        """"""\n        b = deepcopy(self)\n        b.config[\'infer_cells\'] = infer_cells\n        return b\n\n    def with_cell_size(self, cell_size):\n        """"""Sets the cell size of the chips.\n\n        If not explicitly set, the chip size will be used if this object is\n        created as part of an experiment.\n\n        Args:\n            cell_size: (int) the size of the cells in units of pixels\n        """"""\n        b = deepcopy(self)\n        b.config[\'cell_size\'] = cell_size\n        return b\n'"
rastervision/data/label_source/default.py,0,"b'from abc import (ABC, abstractmethod)\nimport os\n\nimport rastervision as rv\n\n\nclass LabelSourceDefaultProvider(ABC):\n    @staticmethod\n    @abstractmethod\n    def handles(task_type, s):\n        """"""Returns True of this provider is a default for this task_type and string""""""\n        pass\n\n    @abstractmethod\n    def construct(s):\n        """"""Construts a default LabelSource based on the string.\n        """"""\n        pass\n\n\nclass ObjectDetectionLabelSourceDefaultProvider(LabelSourceDefaultProvider):\n    @staticmethod\n    def handles(task_type, uri):\n        if task_type == rv.OBJECT_DETECTION:\n            ext = os.path.splitext(uri)[1]\n            return ext.lower() in [\'.geojson\', \'.json\', \'.mbtiles\']\n\n    @staticmethod\n    def construct(uri):\n        return rv.LabelSourceConfig.builder(rv.OBJECT_DETECTION) \\\n                                   .with_uri(uri) \\\n                                   .build()\n\n\nclass ChipClassificationLabelSourceDefaultProvider(LabelSourceDefaultProvider):\n    @staticmethod\n    def handles(task_type, uri):\n        if task_type == rv.CHIP_CLASSIFICATION:\n            ext = os.path.splitext(uri)[1]\n            return ext.lower() in [\'.geojson\', \'.json\', \'.mbtiles\']\n\n    @staticmethod\n    def construct(uri):\n        return rv.LabelSourceConfig.builder(rv.CHIP_CLASSIFICATION) \\\n                                   .with_uri(uri) \\\n                                   .build()\n\n\nclass SemanticSegmentationLabelSourceDefaultProvider(\n        LabelSourceDefaultProvider):\n    @staticmethod\n    def handles(task_type, uri):\n        if task_type == rv.SEMANTIC_SEGMENTATION:\n            ext = os.path.splitext(uri)[1]\n            return ext.lower() in [\'.tif\', \'.tiff\']\n        return False\n\n    @staticmethod\n    def construct(uri):\n        return rv.LabelSourceConfig.builder(rv.SEMANTIC_SEGMENTATION) \\\n                                   .with_raster_source(uri) \\\n                                   .build()\n'"
rastervision/data/label_source/label_source.py,0,"b'from abc import ABC, abstractmethod\n\n\nclass LabelSource(ABC):\n    """"""An interface for storage of labels for a scene.\n\n    An LabelSource is a read source of labels for a scene\n    that could be backed by a file, a database, an API, etc. The difference\n    between LabelSources and Labels can be understood by analogy to the\n    difference between a database and result sets queried from a database.\n    """"""\n\n    @abstractmethod\n    def get_labels(self, window=None):\n        """"""Return labels overlapping with window.\n\n        Args:\n            window: Box\n\n        Returns:\n            Labels overlapping with window. If window is None,\n                returns all labels.\n        """"""\n        pass\n'"
rastervision/data/label_source/label_source_config.py,0,"b'from abc import abstractmethod\nimport logging\n\nimport rastervision as rv\nfrom rastervision.core.config import (Config, ConfigBuilder)\nfrom rastervision.protos.label_source_pb2 import LabelSourceConfig as LabelSourceConfigMsg\n\nlog = logging.getLogger(__name__)\n\n\nclass LabelSourceConfig(Config):\n    deprecation_warnings = []\n\n    def __init__(self, source_type):\n        self.source_type = source_type\n\n    def to_proto(self):\n        msg = LabelSourceConfigMsg()\n        msg.source_type = self.source_type\n        return msg\n\n    @abstractmethod\n    def create_source(self, task_config, extent, crs_transformer, tmp_dir):\n        """"""Create the Label Source for this configuration.\n\n           Args:\n              task_config: The TaskConfig for which this label source is supplying labels.\n              extent: extent that this label source is applicable for\n              crs_transformer: The crs_transformer used by the raster source this\n                               label source is describing.\n              tmp_dir: The temporary directory to use if files will need to be downloaded,\n                       or None if only using local files.\n        """"""\n        pass\n\n    def to_builder(self):\n        return rv._registry.get_config_builder(rv.LABEL_SOURCE,\n                                               self.source_type)(self)\n\n    @staticmethod\n    def check_deprecation(source_type):\n        # If source_type is deprecated and warning hasn\'t been shown yet, then warn.\n        if (source_type in rv.label_source_deprecated_map\n                and source_type not in LabelSourceConfig.deprecation_warnings):\n            LabelSourceConfig.deprecation_warnings.append(source_type)\n            new_source_type = rv.label_source_deprecated_map[source_type]\n            log.warn(\n                \'LabelSource {} is deprecated. Please use {} instead.\'.format(\n                    source_type, new_source_type))\n\n    @staticmethod\n    def builder(source_type):\n        LabelSourceConfig.check_deprecation(source_type)\n        return rv._registry.get_config_builder(rv.LABEL_SOURCE, source_type)()\n\n    @staticmethod\n    def from_proto(msg):\n        """"""Creates a config from the specificed protobuf message\n        """"""\n        return rv._registry.get_config_builder(rv.LABEL_SOURCE, msg.source_type)() \\\n                           .from_proto(msg) \\\n                           .build()\n\n\nclass LabelSourceConfigBuilder(ConfigBuilder):\n    pass\n'"
rastervision/data/label_source/object_detection_label_source.py,0,"b'import rastervision as rv\nfrom rastervision.data.label import ObjectDetectionLabels\nfrom rastervision.data.label_source import LabelSource\n\n\nclass ObjectDetectionLabelSource(LabelSource):\n    def __init__(self, vector_source, crs_transformer, class_map, extent):\n        """"""Constructor.\n\n        Args:\n            vector_source: (VectorSource or str)\n            crs_transformer: CRSTransformer to convert from map coords in label\n                in GeoJSON file to pixel coords.\n            class_map: ClassMap used to infer class_ids from class_name\n                (or label) field\n            extent: Box used to filter the labels by extent\n        """"""\n        if isinstance(vector_source, str):\n            provider = rv._registry.get_vector_source_default_provider(\n                vector_source)\n            vector_source = provider.construct(vector_source) \\\n                .create_source(\n                    crs_transformer=crs_transformer, extent=extent, class_map=class_map)\n\n        self.labels = ObjectDetectionLabels.from_geojson(\n            vector_source.get_geojson(), extent=extent)\n\n    def get_labels(self, window=None):\n        if window is None:\n            return self.labels\n\n        return ObjectDetectionLabels.get_overlapping(self.labels, window)\n'"
rastervision/data/label_source/object_detection_label_source_config.py,0,"b'from copy import deepcopy\n\nimport rastervision as rv\nfrom rastervision.data.label_source import (\n    LabelSourceConfig, LabelSourceConfigBuilder, ObjectDetectionLabelSource)\nfrom rastervision.data.vector_source import VectorSourceConfig\nfrom rastervision.protos.label_source_pb2 import LabelSourceConfig as LabelSourceConfigMsg\n\n\nclass ObjectDetectionLabelSourceConfig(LabelSourceConfig):\n    def __init__(self, vector_source):\n        super().__init__(source_type=rv.OBJECT_DETECTION)\n        self.vector_source = vector_source\n\n    def to_proto(self):\n        msg = super().to_proto()\n        opts = LabelSourceConfigMsg.ObjectDetectionLabelSource(\n            vector_source=self.vector_source.to_proto())\n        msg.object_detection_label_source.CopyFrom(opts)\n        return msg\n\n    def create_source(self, task_config, extent, crs_transformer, tmp_dir):\n        vector_source = self.vector_source.create_source(\n            crs_transformer=crs_transformer,\n            extent=extent,\n            class_map=task_config.class_map)\n        return ObjectDetectionLabelSource(vector_source, crs_transformer,\n                                          task_config.class_map, extent)\n\n    def update_for_command(self, command_type, experiment_config,\n                           context=None):\n        self.vector_source.update_for_command(command_type, experiment_config,\n                                              context)\n\n    def report_io(self, command_type, io_def):\n        self.vector_source.report_io(command_type, io_def)\n\n\nclass ObjectDetectionLabelSourceConfigBuilder(LabelSourceConfigBuilder):\n    def __init__(self, prev=None):\n        config = {}\n        if prev:\n            config = {\'vector_source\': prev.vector_source}\n\n        super().__init__(ObjectDetectionLabelSourceConfig, config)\n\n    def validate(self):\n        super().validate()\n        vector_source = self.config.get(\'vector_source\')\n        if vector_source is None:\n            raise rv.ConfigError(\n                \'You must set the vector_source for ObjectDetectionLabelSourceConfig\'\n                \' Use ""with_vector_source"".\')\n        if not isinstance(vector_source, VectorSourceConfig):\n            raise rv.ConfigError(\n                \'vector source must be a child of class VectorSourceConfig, got {}\'.\n                format(type(vector_source)))\n        if vector_source.has_null_class_bufs():\n            raise rv.ConfigError(\n                \'Setting buffer to None for a class in the vector_source is not allowed \'\n                \'for ObjectDetectionLabelSourceConfig.\')\n\n    def from_proto(self, msg):\n        b = ObjectDetectionLabelSourceConfigBuilder()\n\n        # Added for backwards compatibility.\n        if msg.HasField(\'object_detection_geojson_source\'):\n            vector_source = msg.object_detection_geojson_source.uri\n        else:\n            vector_source = rv.VectorSourceConfig.from_proto(\n                msg.object_detection_label_source.vector_source)\n\n        return b.with_vector_source(vector_source)\n\n    def with_vector_source(self, vector_source):\n        """"""Set the vector_source.\n\n        Args:\n            vector_source (str or VectorSource) if a string, assume it is\n                a URI and use the default provider to construct a VectorSource.\n        """"""\n        if isinstance(vector_source, str):\n            return self.with_uri(vector_source)\n\n        b = deepcopy(self)\n        if isinstance(vector_source, VectorSourceConfig):\n            b.config[\'vector_source\'] = vector_source\n        else:\n            raise rv.ConfigError(\n                \'vector_source must be of type str or VectorSource\')\n\n        return b\n\n    def with_uri(self, uri):\n        b = deepcopy(self)\n        provider = rv._registry.get_vector_source_default_provider(uri)\n        b.config[\'vector_source\'] = provider.construct(uri)\n        return b\n'"
rastervision/data/label_source/segmentation_class_transformer.py,0,"b'import numpy as np\n\nfrom rastervision.data.label_source.utils import (\n    color_to_triple, color_to_integer, rgb_to_int_array)\n\n\nclass SegmentationClassTransformer():\n    def __init__(self, class_map):\n        color_to_class = dict(\n            [(item.color, item.id) for item in class_map.get_items()])\n\n        # color int to class\n        color_int_to_class = dict(\n            zip([color_to_integer(c) for c in color_to_class.keys()],\n                color_to_class.values()))\n\n        def color_int_to_class_fn(color: int) -> int:\n            # Convert unspecified colors to class 0 which is ""don\'t care""\n            return color_int_to_class.get(color, 0x00)\n\n        self.transform_color_int_to_class = \\\n            np.vectorize(color_int_to_class_fn, otypes=[np.uint8])\n\n        # class to color triple\n        class_to_color_triple = dict(\n            zip(color_to_class.values(),\n                [color_to_triple(c) for c in color_to_class.keys()]))\n\n        def class_to_channel_color(channel: int, class_id: int) -> int:\n            """"""Given a channel (red, green, or blue) and a class, return the\n            intensity of that channel.\n\n            Args:\n                 channel: An integer with value 0, 1, or 2\n                      representing the channel.\n                 class_id: The class id represented as an integer.\n            Returns:\n                 The intensity of the channel for the color associated\n                      with the given class.\n            """"""\n            default_triple = (0x00, 0x00, 0x00)\n            return class_to_color_triple.get(class_id, default_triple)[channel]\n\n        class_to_r = np.vectorize(\n            lambda c: class_to_channel_color(0, c), otypes=[np.uint8])\n        class_to_g = np.vectorize(\n            lambda c: class_to_channel_color(1, c), otypes=[np.uint8])\n        class_to_b = np.vectorize(\n            lambda c: class_to_channel_color(2, c), otypes=[np.uint8])\n        self.transform_class_to_color = [class_to_r, class_to_g, class_to_b]\n\n    def rgb_to_class(self, rgb_labels):\n        color_int_labels = rgb_to_int_array(rgb_labels)\n        class_labels = self.transform_color_int_to_class(color_int_labels)\n        return class_labels.astype(np.uint8)\n\n    def class_to_rgb(self, class_labels):\n        rgb_labels = np.empty(class_labels.shape + (3, ))\n        for chan in range(3):\n            class_to_channel_color = self.transform_class_to_color[chan]\n            rgb_labels[:, :, chan] = class_to_channel_color(class_labels)\n        return rgb_labels.astype(np.uint8)\n'"
rastervision/data/label_source/semantic_segmentation_label_source.py,0,"b'from typing import (List, Union)\n\nimport numpy as np\n\nfrom rastervision.core.box import Box\nfrom rastervision.core.class_map import ClassMap\nfrom rastervision.data import ActivateMixin\nfrom rastervision.data.label import SemanticSegmentationLabels\nfrom rastervision.data.label_source import LabelSource, SegmentationClassTransformer\nfrom rastervision.data.raster_source import RasterSource\n\n\nclass SemanticSegmentationLabelSource(ActivateMixin, LabelSource):\n    """"""A read-only label source for semantic segmentation.""""""\n\n    def __init__(self, source: RasterSource, rgb_class_map: ClassMap = None):\n        """"""Constructor.\n\n        Args:\n            source: (RasterSource) A raster source that returns a single channel\n                raster with class_ids as values, or a 3 channel raster with\n                RGB values that are mapped to class_ids using the rgb_class_map\n            rgb_class_map: (ClassMap) with color values filled in. Optional and used to\n                transform RGB values to class ids. Only use if the raster source\n                is RGB.\n        """"""\n        self.source = source\n        self.class_transformer = None\n        if rgb_class_map is not None:\n            self.class_transformer = SegmentationClassTransformer(\n                rgb_class_map)\n\n    def enough_target_pixels(self, window: Box, target_count_threshold: int,\n                             target_classes: List[int]) -> bool:\n        """"""Given a window, answer whether the window contains enough pixels in\n        the target classes.\n\n        Args:\n             window: The larger window from-which the sub-window will\n                  be clipped.\n             target_count_threshold:  Minimum number of target pixels.\n             target_classes: The classes of interest.  The given\n                  window is examined to make sure that it contains a\n                  sufficient number of target pixels.\n        Returns:\n             True (the window does contain interesting pixels) or False.\n        """"""\n        raw_labels = self.source.get_raw_chip(window)\n        if self.class_transformer is not None:\n            labels = self.class_transformer.rgb_to_class(raw_labels)\n        else:\n            labels = np.squeeze(raw_labels)\n\n        target_count = 0\n        for class_id in target_classes:\n            target_count = target_count + (labels == class_id).sum()\n\n        return target_count >= target_count_threshold\n\n    def get_labels(self, window: Union[Box, None] = None,\n                   chip_size=1000) -> SemanticSegmentationLabels:\n        """"""Get labels for a window.\n\n        To avoid running out of memory, if window is None and defaults to using the full\n        extent, a set of sub-windows of size chip_size are used which cover the full\n        extent with no overlap.\n\n        Args:\n             window: Either None or a window given as a Box object. Uses full extent of\n                scene if window is not provided.\n             chip_size: size of sub-windows to use if full extent is used (in\n                units of pixels)\n        Returns:\n             SemanticSegmentationLabels\n        """"""\n\n        def label_fn(_window):\n            raw_labels = self.source.get_raw_chip(_window)\n\n            if self.class_transformer is not None:\n                labels = self.class_transformer.rgb_to_class(raw_labels)\n            else:\n                labels = np.squeeze(raw_labels)\n\n            return labels\n\n        windows = [window]\n        if window is None:\n            window = self.source.get_extent()\n            windows = window.get_windows(chip_size, chip_size)\n\n        return SemanticSegmentationLabels(windows, label_fn)\n\n    def _subcomponents_to_activate(self):\n        return [self.source]\n\n    def _activate(self):\n        pass\n\n    def _deactivate(self):\n        pass\n'"
rastervision/data/label_source/semantic_segmentation_label_source_config.py,0,"b'from copy import deepcopy\n\nimport rastervision as rv\nfrom rastervision.core.class_map import ClassMap\nfrom rastervision.data.label_source import (LabelSourceConfig,\n                                            LabelSourceConfigBuilder,\n                                            SemanticSegmentationLabelSource)\nfrom rastervision.protos.label_source_pb2 import LabelSourceConfig as LabelSourceConfigMsg\nfrom rastervision.data.raster_source import RasterSourceConfig\n\n\nclass SemanticSegmentationLabelSourceConfig(LabelSourceConfig):\n    def __init__(self, source, rgb_class_map=None):\n        super().__init__(source_type=rv.SEMANTIC_SEGMENTATION_RASTER)\n        self.source = source\n        self.rgb_class_map = rgb_class_map\n\n    def to_proto(self):\n        msg = super().to_proto()\n\n        rgb_class_items = None\n        if self.rgb_class_map is not None:\n            rgb_class_items = self.rgb_class_map.to_proto()\n        opts = LabelSourceConfigMsg.SemanticSegmentationLabelSource(\n            source=self.source.to_proto(), rgb_class_items=rgb_class_items)\n        msg.semantic_segmentation_label_source.CopyFrom(opts)\n        return msg\n\n    def create_source(self, task_config, extent, crs_transformer, tmp_dir):\n        raster_source = self.source.create_source(\n            tmp_dir, crs_transformer, extent, task_config.class_map)\n        return SemanticSegmentationLabelSource(raster_source,\n                                               self.rgb_class_map)\n\n    def update_for_command(self, command_type, experiment_config,\n                           context=None):\n        if context is None:\n            context = []\n        context = context + [self]\n        self.source.update_for_command(command_type, experiment_config,\n                                       context)\n\n    def report_io(self, command_type, io_def):\n        self.source.update_for_command(command_type, io_def)\n\n\nclass SemanticSegmentationLabelSourceConfigBuilder(LabelSourceConfigBuilder):\n    def __init__(self, prev=None):\n        config = {}\n        if prev:\n            config = {\n                \'source\': prev.source,\n                \'rgb_class_map\': prev.rgb_class_map\n            }\n\n        super().__init__(SemanticSegmentationLabelSourceConfig, config)\n\n    def from_proto(self, msg):\n        b = SemanticSegmentationLabelSourceConfigBuilder()\n\n        label_source_msg = msg.semantic_segmentation_label_source\n        # Add for backwards compatibility.\n        if msg.HasField(\'semantic_segmentation_raster_source\'):\n            label_source_msg = msg.semantic_segmentation_raster_source\n\n        raster_source_config = rv.RasterSourceConfig.from_proto(\n            label_source_msg.source)\n\n        b = self.with_raster_source(raster_source_config)\n        rgb_class_items = msg.semantic_segmentation_raster_source.rgb_class_items\n\n        b = b.with_raster_source(raster_source_config)\n        rgb_class_items = label_source_msg.rgb_class_items\n\n        if rgb_class_items:\n            b = b.with_rgb_class_map(\n                ClassMap.construct_from(list(rgb_class_items)))\n\n        return b\n\n    def with_raster_source(self, source, channel_order=None):\n        """"""Set raster_source.\n\n        Args:\n            source: (RasterSourceConfig) A RasterSource assumed to have RGB values that\n                are mapped to class_ids using the rgb_class_map.\n\n        Returns:\n            SemanticSegmentationLabelSourceConfigBuilder\n        """"""\n        b = deepcopy(self)\n        if isinstance(source, RasterSourceConfig):\n            b.config[\'source\'] = source\n        elif isinstance(source, str):\n            provider = rv._registry.get_raster_source_default_provider(source)\n            source = provider.construct(source, channel_order=channel_order)\n            b.config[\'source\'] = source\n        else:\n            raise rv.ConfigError(\n                \'source must be either string or RasterSourceConfig, \'\n                \' not {}\'.format(str(type(source))))\n\n        return b\n\n    def with_rgb_class_map(self, rgb_class_map):\n        """"""Set rgb_class_map.\n\n        Args:\n            rgb_class_map: (something accepted by ClassMap.construct_from) a class\n                map with color values used to map RGB values to class ids\n\n        Returns:\n            SemanticSegmentationLabelSourceConfigBuilder\n        """"""\n        b = deepcopy(self)\n        b.config[\'rgb_class_map\'] = ClassMap.construct_from(rgb_class_map)\n        return b\n\n    def validate(self):\n        source = self.config.get(\'source\')\n\n        if source is None:\n            raise rv.ConfigError(\n                \'You must set the source for SemanticSegmentationLabelSourceConfig\'\n                \' Use ""with_raster_source"".\')\n\n        if not isinstance(source, (str, RasterSourceConfig)):\n            raise rv.ConfigError(\n                \'raster_source must be of type str or RasterSourceConfig, got {}\'.\n                format(type(source)))\n'"
rastervision/data/label_source/utils.py,0,"b'from typing import Tuple\n\nimport numpy as np\nfrom PIL import ImageColor\n\n\ndef color_to_triple(color: str) -> Tuple[int, int, int]:\n    """"""Given a PIL ImageColor string, return a triple of integers\n    representing the red, green, and blue values.\n\n    Args:\n         color: A PIL ImageColor string\n\n    Returns:\n         An triple of integers\n\n    """"""\n    if color is None:\n        r = np.random.randint(0, 0x100)\n        g = np.random.randint(0, 0x100)\n        b = np.random.randint(0, 0x100)\n        return (r, g, b)\n    else:\n        return ImageColor.getrgb(color)\n\n\ndef color_to_integer(color: str) -> int:\n    """"""Given a PIL ImageColor string, return a packed integer.\n\n    Args:\n         color: A PIL ImageColor string\n\n    Returns:\n         An integer containing the packed RGB values.\n\n    """"""\n    triple = color_to_triple(color)\n    r = triple[0] * (1 << 16)\n    g = triple[1] * (1 << 8)\n    b = triple[2] * (1 << 0)\n    integer = r + g + b\n    return integer\n\n\ndef rgb_to_int_array(rgb_array):\n    r = np.array(rgb_array[:, :, 0], dtype=np.uint32) * (1 << 16)\n    g = np.array(rgb_array[:, :, 1], dtype=np.uint32) * (1 << 8)\n    b = np.array(rgb_array[:, :, 2], dtype=np.uint32) * (1 << 0)\n    return r + g + b\n'"
rastervision/data/label_store/__init__.py,0,b'# flake8: noqa\n\nfrom rastervision.data.label_store.label_store import *\nfrom rastervision.data.label_store.label_store_config import *\nfrom rastervision.data.label_store.chip_classification_geojson_store import *\nfrom rastervision.data.label_store.chip_classification_geojson_store_config import *\nfrom rastervision.data.label_store.object_detection_geojson_store import *\nfrom rastervision.data.label_store.object_detection_geojson_store_config import *\nfrom rastervision.data.label_store.semantic_segmentation_raster_store import *\nfrom rastervision.data.label_store.semantic_segmentation_raster_store_config import *\n'
rastervision/data/label_store/api.py,0,"b""# flake8: noqa\n\n# Registry Keys\nLABEL_STORE = 'LABEL_STORE'\n\n# Use the same labels as for the source for these label stores.\nfrom rastervision.data.label_source.api import (OBJECT_DETECTION_GEOJSON,\n                                                CHIP_CLASSIFICATION_GEOJSON,\n                                                SEMANTIC_SEGMENTATION_RASTER)\n\nfrom rastervision.data.label_store.label_store_config import LabelStoreConfig\n"""
rastervision/data/label_store/chip_classification_geojson_store.py,0,"b'from rastervision.data.label import ChipClassificationLabels\nfrom rastervision.data.label_store import LabelStore\nfrom rastervision.data.label_store.utils import boxes_to_geojson\nfrom rastervision.data.label_source import ChipClassificationLabelSource\nfrom rastervision.data.vector_source import GeoJSONVectorSource\nfrom rastervision.utils.files import json_to_file\n\n\nclass ChipClassificationGeoJSONStore(LabelStore):\n    """"""A GeoJSON file with classification labels in it.\n    """"""\n\n    def __init__(self, uri, crs_transformer, class_map):\n        """"""Construct ClassificationLabelStore backed by a GeoJSON file.\n\n        Args:\n            uri: uri of GeoJSON file containing labels\n            crs_transformer: CRSTransformer to convert from map coords in label\n                in GeoJSON file to pixel coords.\n            class_map: ClassMap used to infer class_ids from class_name\n                (or label) field\n        """"""\n        self.uri = uri\n        self.crs_transformer = crs_transformer\n        self.class_map = class_map\n\n    def save(self, labels):\n        """"""Save labels to URI if writable.\n\n        Note that if the grid is inferred from polygons, only the grid will be\n        written, not the original polygons.\n        """"""\n        boxes = labels.get_cells()\n        class_ids = labels.get_class_ids()\n        scores = list(labels.get_scores())\n        geojson = boxes_to_geojson(\n            boxes,\n            class_ids,\n            self.crs_transformer,\n            self.class_map,\n            scores=scores)\n        json_to_file(geojson, self.uri)\n\n    def get_labels(self):\n        vector_source = GeoJSONVectorSource(self.uri, self.crs_transformer)\n        source = ChipClassificationLabelSource(\n            vector_source, self.crs_transformer, self.class_map)\n        return source.get_labels()\n\n    def empty_labels(self):\n        return ChipClassificationLabels()\n'"
rastervision/data/label_store/chip_classification_geojson_store_config.py,0,"b'import os\nfrom copy import deepcopy\n\nimport rastervision as rv\nfrom rastervision.data.label_store import (\n    LabelStoreConfig, LabelStoreConfigBuilder, ChipClassificationGeoJSONStore)\n\n\nclass ChipClassificationGeoJSONStoreConfig(LabelStoreConfig):\n    def __init__(self, uri=None):\n        super().__init__(store_type=rv.CHIP_CLASSIFICATION_GEOJSON)\n        self.uri = uri\n\n    def to_proto(self):\n        msg = super().to_proto()\n        if self.uri:\n            msg.uri = self.uri\n        return msg\n\n    def for_prediction(self, label_uri):\n        return self.to_builder() \\\n                   .with_uri(label_uri) \\\n                   .build()\n\n    def create_store(self, task_config, extent, crs_transformer, tmp_dir):\n        return ChipClassificationGeoJSONStore(self.uri, crs_transformer,\n                                              task_config.class_map)\n\n    def update_for_command(self, command_type, experiment_config,\n                           context=None):\n        if command_type == rv.PREDICT:\n            if not self.uri:\n                # Construct the  URI for this prediction store,\n                # using the scene ID.\n                root = experiment_config.predict_uri\n                uri = None\n                for c in context:\n                    if isinstance(c, rv.SceneConfig):\n                        uri = os.path.join(root, \'{}.json\'.format(c.id))\n                if uri:\n                    self.uri = uri\n                else:\n                    raise rv.ConfigError(\n                        \'ChipClassificationGeoJSONStoreConfig has no \'\n                        \'URI set, and is not associated with a SceneConfig.\')\n\n    def report_io(self, command_type, io_def):\n        if command_type == rv.PREDICT:\n            io_def.add_output(self.uri)\n\n        if command_type == rv.EVAL:\n            if self.uri:\n                io_def.add_input(self.uri)\n            else:\n                msg = \'No URI set for ChipClassificationGeoJSONStoreConfig\'\n                io_def.add_missing(msg)\n\n\nclass ChipClassificationGeoJSONStoreConfigBuilder(LabelStoreConfigBuilder):\n    def __init__(self, prev=None):\n        config = {}\n        if prev:\n            config = {\'uri\': prev.uri}\n\n        super().__init__(ChipClassificationGeoJSONStoreConfig, config)\n\n    def from_proto(self, msg):\n        return self \\\n            .with_uri(msg.uri)\n\n    def with_uri(self, uri):\n        """"""Set URI for a GeoJSON used to read/write predictions.""""""\n        b = deepcopy(self)\n        b.config[\'uri\'] = uri\n        return b\n'"
rastervision/data/label_store/default.py,0,"b'from abc import (ABC, abstractmethod)\nimport os\n\nimport rastervision as rv\n\n\nclass LabelStoreDefaultProvider(ABC):\n    @staticmethod\n    @abstractmethod\n    def is_default_for(task_type):\n        """"""Returns True if this label store is the default for this tasks_type""""""\n        pass\n\n    @staticmethod\n    @abstractmethod\n    def handles(task_type, s):\n        """"""Returns True of this provider is a default for this task_type and string""""""\n        pass\n\n    @abstractmethod\n    def construct(s=None):\n        """"""Construts a default LabelStore based on the string.\n        """"""\n        pass\n\n\nclass ObjectDetectionGeoJSONStoreDefaultProvider(LabelStoreDefaultProvider):\n    @staticmethod\n    def is_default_for(task_type):\n        return task_type == rv.OBJECT_DETECTION\n\n    @staticmethod\n    def handles(task_type, uri):\n        if task_type == rv.OBJECT_DETECTION:\n            ext = os.path.splitext(uri)[1]\n            return ext.lower() in [\'.json\', \'.geojson\']\n        return False\n\n    @staticmethod\n    def construct(uri=None):\n        b = rv.LabelStoreConfig.builder(rv.OBJECT_DETECTION_GEOJSON)\n        if uri:\n            b = b.with_uri(uri)\n\n        return b.build()\n\n\nclass ChipClassificationGeoJSONStoreDefaultProvider(LabelStoreDefaultProvider):\n    @staticmethod\n    def is_default_for(task_type):\n        return task_type == rv.CHIP_CLASSIFICATION\n\n    @staticmethod\n    def handles(task_type, uri):\n        if task_type == rv.CHIP_CLASSIFICATION:\n            ext = os.path.splitext(uri)[1]\n            return ext.lower() in [\'.json\', \'.geojson\']\n        return False\n\n    @staticmethod\n    def construct(uri=None):\n        b = rv.LabelStoreConfig.builder(rv.CHIP_CLASSIFICATION_GEOJSON)\n        if uri:\n            b = b.with_uri(uri)\n        return b.build()\n\n\nclass SemanticSegmentationRasterStoreDefaultProvider(\n        LabelStoreDefaultProvider):\n    @staticmethod\n    def is_default_for(task_type):\n        return task_type == rv.SEMANTIC_SEGMENTATION\n\n    @staticmethod\n    def handles(task_type, uri):\n        if task_type == rv.SEMANTIC_SEGMENTATION:\n            ext = os.path.splitext(uri)[1]\n            return ext.lower() in [\'.tiff\', \'.tif\']\n        return False\n\n    @staticmethod\n    def construct(uri=None):\n        b = rv.LabelStoreConfig.builder(rv.SEMANTIC_SEGMENTATION_RASTER)\n        if uri:\n            b = b.with_uri(uri)\n        return b.build()\n'"
rastervision/data/label_store/label_store.py,0,"b'from abc import ABC, abstractmethod\n\nfrom rastervision.data import ActivateMixin\n\n\nclass LabelStore(ABC, ActivateMixin):\n    """"""This defines how to store prediction labels are stored for a scene.\n    """"""\n\n    @abstractmethod\n    def save(self, labels):\n        """"""Save.\n\n        Args:\n           labels - Labels to be saved, the type of which will be dependant on the type\n                    of task.\n        """"""\n        pass\n\n    @abstractmethod\n    def get_labels(self):\n        """"""Loads Labels from this label store.""""""\n        pass\n\n    @abstractmethod\n    def empty_labels(self):\n        """"""Produces an empty Labels""""""\n        pass\n\n    def _subcomponents_to_activate(self):\n        pass\n\n    def _activate(self):\n        pass\n\n    def _deactivate(self):\n        pass\n'"
rastervision/data/label_store/label_store_config.py,0,"b'from abc import abstractmethod\n\nimport rastervision as rv\nfrom rastervision.core.config import (Config, ConfigBuilder)\nfrom rastervision.protos.label_store_pb2 import LabelStoreConfig as LabelStoreConfigMsg\n\n\nclass LabelStoreConfig(Config):\n    def __init__(self, store_type):\n        self.store_type = store_type\n\n    def to_proto(self):\n        msg = LabelStoreConfigMsg()\n        msg.store_type = self.store_type\n        return msg\n\n    @abstractmethod\n    def for_prediction(self, label_uri):\n        """"""Build a copy of this config with a label_uri set.\n\n        This is used in the Predictor to save labels to a specific path.\n        """"""\n        pass\n\n    @abstractmethod\n    def create_store(self, task_config, extent, crs_transformer, tmp_dir):\n        """"""Create the Label Store for this configuration.\n\n           Args:\n              task_config: The TaskConfig for which this label source is supplying labels.\n              crs_transformer: The crs_transformer used by the raster store this\n                               label store is describing.\n              tmp_dir: The temporary directory to use if files will need to be downloaded,\n                       or None if only using local files.\n        """"""\n        pass\n\n    def to_builder(self):\n        return rv._registry.get_config_builder(rv.LABEL_STORE,\n                                               self.store_type)(self)\n\n    @staticmethod\n    def builder(store_type):\n        return rv._registry.get_config_builder(rv.LABEL_STORE, store_type)()\n\n    @staticmethod\n    def from_proto(msg):\n        """"""Creates a TaskConfig from the specificed protobuf message\n        """"""\n        return rv._registry.get_config_builder(rv.LABEL_STORE, msg.store_type)() \\\n                           .from_proto(msg) \\\n                           .build()\n\n\nclass LabelStoreConfigBuilder(ConfigBuilder):\n    pass\n'"
rastervision/data/label_store/object_detection_geojson_store.py,0,"b'from rastervision.data.label import ObjectDetectionLabels\nfrom rastervision.data.label_store import LabelStore\nfrom rastervision.data.label_store.utils import boxes_to_geojson\nfrom rastervision.data.vector_source import GeoJSONVectorSource\nfrom rastervision.utils.files import json_to_file\n\n\nclass ObjectDetectionGeoJSONStore(LabelStore):\n    def __init__(self, uri, crs_transformer, class_map):\n        """"""Construct LabelStore backed by a GeoJSON file for object detection labels.\n\n        Args:\n            uri: uri of GeoJSON file containing labels\n            crs_transformer: CRSTransformer to convert from map coords in label\n                in GeoJSON file to pixel coords.\n            class_map: ClassMap used to infer class_ids from class_name\n                (or label) field\n        """"""\n        self.uri = uri\n        self.crs_transformer = crs_transformer\n        self.class_map = class_map\n\n    def save(self, labels):\n        """"""Save labels to URI.""""""\n        boxes = labels.get_boxes()\n        class_ids = labels.get_class_ids().tolist()\n        scores = labels.get_scores().tolist()\n        geojson = boxes_to_geojson(\n            boxes,\n            class_ids,\n            self.crs_transformer,\n            self.class_map,\n            scores=scores)\n        json_to_file(geojson, self.uri)\n\n    def get_labels(self):\n        vector_source = GeoJSONVectorSource(self.uri, self.crs_transformer)\n        return ObjectDetectionLabels.from_geojson(vector_source.get_geojson())\n\n    def empty_labels(self):\n        return ObjectDetectionLabels.make_empty()\n'"
rastervision/data/label_store/object_detection_geojson_store_config.py,0,"b'import os\nfrom copy import deepcopy\n\nimport rastervision as rv\nfrom rastervision.data.label_store import (\n    LabelStoreConfig, LabelStoreConfigBuilder, ObjectDetectionGeoJSONStore)\n\n\nclass ObjectDetectionGeoJSONStoreConfig(LabelStoreConfig):\n    def __init__(self, uri=None):\n        super().__init__(store_type=rv.OBJECT_DETECTION_GEOJSON)\n        self.uri = uri\n\n    def to_proto(self):\n        msg = super().to_proto()\n        if self.uri:\n            msg.uri = self.uri\n        return msg\n\n    def for_prediction(self, label_uri):\n        return self.to_builder() \\\n                   .with_uri(label_uri) \\\n                   .build()\n\n    def create_store(self, task_config, extent, crs_transformer, tmp_dir):\n        return ObjectDetectionGeoJSONStore(self.uri, crs_transformer,\n                                           task_config.class_map)\n\n    def update_for_command(self, command_type, experiment_config,\n                           context=None):\n        if command_type == rv.PREDICT:\n            if not self.uri:\n                # Construct the  URI for this prediction store,\n                # using the scene ID.\n                root = experiment_config.predict_uri\n                uri = None\n                for c in context:\n                    if isinstance(c, rv.SceneConfig):\n                        uri = os.path.join(root, \'{}.json\'.format(c.id))\n                if uri:\n                    self.uri = uri\n                else:\n                    raise rv.ConfigError(\n                        \'ObjectDetectionGeoJSONStoreConfig has no \'\n                        \'URI set, and is not associated with a SceneConfig.\')\n\n    def report_io(self, command_type, io_def):\n        if command_type == rv.PREDICT:\n            io_def.add_output(self.uri)\n\n        if command_type == rv.EVAL:\n            if self.uri:\n                io_def.add_input(self.uri)\n            else:\n                msg = \'No URI set for ObjectDetectionGeoJSONStoreConfig\'\n                io_def.add_missing(msg)\n\n\nclass ObjectDetectionGeoJSONStoreConfigBuilder(LabelStoreConfigBuilder):\n    def __init__(self, prev=None):\n        config = {}\n        if prev:\n            config = {\'uri\': prev.uri}\n\n        super().__init__(ObjectDetectionGeoJSONStoreConfig, config)\n\n    def from_proto(self, msg):\n        return self \\\n            .with_uri(msg.uri)\n\n    def with_uri(self, uri):\n        """"""Set URI for a GeoJSON used to read/write predictions.""""""\n        b = deepcopy(self)\n        b.config[\'uri\'] = uri\n        return b\n'"
rastervision/data/label_store/semantic_segmentation_raster_store.py,0,"b'import numpy as np\nimport rasterio\n\nimport rastervision as rv\nfrom rastervision.utils.files import (get_local_path, make_dir, upload_or_copy,\n                                      file_exists)\nfrom rastervision.data.label import SemanticSegmentationLabels\nfrom rastervision.data.label_store import LabelStore\nfrom rastervision.data.label_source import SegmentationClassTransformer\n\n\nclass SemanticSegmentationRasterStore(LabelStore):\n    """"""A prediction label store for segmentation raster files.\n    """"""\n\n    def __init__(self,\n                 uri,\n                 extent,\n                 crs_transformer,\n                 tmp_dir,\n                 vector_output=None,\n                 class_map=None):\n        """"""Constructor.\n\n        Args:\n            uri: (str) URI of GeoTIFF file used for storing predictions as RGB values\n            extent: (Box) The extent of the scene\n            crs_transformer: (CRSTransformer)\n            tmp_dir: (str) temp directory to use\n            vector_output: (None or array of dicts) containing vectorifiction\n                configuration information\n            class_map: (ClassMap) with color values used to convert class ids to\n                RGB values\n\n        """"""\n        self.uri = uri\n        self.vector_output = vector_output\n        self.extent = extent\n        self.crs_transformer = crs_transformer\n        self.tmp_dir = tmp_dir\n        # Note: can\'t name this class_transformer due to Python using that attribute\n        if class_map:\n            self.class_trans = SegmentationClassTransformer(class_map)\n        else:\n            self.class_trans = None\n\n        self.source = None\n        if file_exists(uri):\n            self.source = rv.RasterSourceConfig.builder(rv.RASTERIO_SOURCE) \\\n                                               .with_uri(self.uri) \\\n                                               .build() \\\n                                               .create_source(self.tmp_dir)\n\n    def _subcomponents_to_activate(self):\n        if self.source is not None:\n            return [self.source]\n        return []\n\n    def get_labels(self, chip_size=1000):\n        """"""Get all labels.\n\n        Returns:\n            SemanticSegmentationLabels with windows of size chip_size covering the\n                scene with no overlap.\n        """"""\n\n        def label_fn(window):\n            raw_labels = self.source.get_raw_chip(window)\n            if self.class_trans:\n                labels = self.class_trans.rgb_to_class(raw_labels)\n            else:\n                labels = np.squeeze(raw_labels)\n            return labels\n\n        if self.source is None:\n            raise Exception(\'Raster source at {} does not exist\'.format(\n                self.uri))\n\n        extent = self.source.get_extent()\n        windows = extent.get_windows(chip_size, chip_size)\n        return SemanticSegmentationLabels(windows, label_fn)\n\n    def save(self, labels):\n        """"""Save.\n\n        Args:\n            labels - (SemanticSegmentationLabels) labels to be saved\n        """"""\n        local_path = get_local_path(self.uri, self.tmp_dir)\n        make_dir(local_path, use_dirname=True)\n\n        transform = self.crs_transformer.get_affine_transform()\n        crs = self.crs_transformer.get_image_crs()\n\n        band_count = 1\n        dtype = np.uint8\n        if self.class_trans:\n            band_count = 3\n\n        if self.vector_output:\n            # We need to store the whole output mask to run feature extraction.\n            # If the raster is large, this will result in running out of memory, so\n            # more work will be needed to get this to work in a scalable way. But this\n            # is complicated because of the need to merge features that are split\n            # across windows.\n            mask = np.zeros(\n                (self.extent.ymax, self.extent.xmax), dtype=np.uint8)\n        else:\n            mask = None\n\n        # https://github.com/mapbox/rasterio/blob/master/docs/quickstart.rst\n        # https://rasterio.readthedocs.io/en/latest/topics/windowed-rw.html\n        with rasterio.open(\n                local_path,\n                \'w\',\n                driver=\'GTiff\',\n                height=self.extent.ymax,\n                width=self.extent.xmax,\n                count=band_count,\n                dtype=dtype,\n                transform=transform,\n                crs=crs) as dataset:\n            for window in labels.get_windows():\n                class_labels = labels.get_label_arr(\n                    window, clip_extent=self.extent)\n                clipped_window = ((window.ymin,\n                                   window.ymin + class_labels.shape[0]),\n                                  (window.xmin,\n                                   window.xmin + class_labels.shape[1]))\n                if mask is not None:\n                    mask[clipped_window[0][0]:clipped_window[0][1],\n                         clipped_window[1][0]:clipped_window[1][\n                             1]] = class_labels\n                if self.class_trans:\n                    rgb_labels = self.class_trans.class_to_rgb(class_labels)\n                    for chan in range(3):\n                        dataset.write_band(\n                            chan + 1,\n                            rgb_labels[:, :, chan],\n                            window=clipped_window)\n                else:\n                    img = class_labels.astype(dtype)\n                    dataset.write_band(1, img, window=clipped_window)\n\n        upload_or_copy(local_path, self.uri)\n\n        if self.vector_output:\n            import mask_to_polygons.vectorification as vectorification\n            import mask_to_polygons.processing.denoise as denoise\n\n            for vo in self.vector_output:\n                denoise_radius = vo[\'denoise\']\n                uri = vo[\'uri\']\n                mode = vo[\'mode\']\n                class_id = vo[\'class_id\']\n                class_mask = np.array(mask == class_id, dtype=np.uint8)\n                local_geojson_path = get_local_path(uri, self.tmp_dir)\n\n                def transform(x, y):\n                    return self.crs_transformer.pixel_to_map((x, y))\n\n                if denoise_radius > 0:\n                    class_mask = denoise.denoise(class_mask, denoise_radius)\n\n                if uri and mode == \'buildings\':\n                    options = vo[\'building_options\']\n                    geojson = vectorification.geojson_from_mask(\n                        mask=class_mask,\n                        transform=transform,\n                        mode=mode,\n                        min_aspect_ratio=options[\'min_aspect_ratio\'],\n                        min_area=options[\'min_area\'],\n                        width_factor=options[\'element_width_factor\'],\n                        thickness=options[\'element_thickness\'])\n                elif uri and mode == \'polygons\':\n                    geojson = vectorification.geojson_from_mask(\n                        mask=class_mask, transform=transform, mode=mode)\n\n                if local_geojson_path:\n                    with open(local_geojson_path, \'w\') as file_out:\n                        file_out.write(geojson)\n                        upload_or_copy(local_geojson_path, uri)\n\n    def empty_labels(self):\n        """"""Returns an empty SemanticSegmentationLabels object.""""""\n        return SemanticSegmentationLabels()\n'"
rastervision/data/label_store/semantic_segmentation_raster_store_config.py,0,"b'import os\nfrom copy import deepcopy\n\nimport rastervision as rv\nfrom rastervision.data.label_store import (\n    LabelStoreConfig, LabelStoreConfigBuilder, SemanticSegmentationRasterStore)\nfrom rastervision.protos.label_store_pb2 import LabelStoreConfig as LabelStoreConfigMsg\n\nVectorOutput = LabelStoreConfigMsg.SemanticSegmentationRasterStore.VectorOutput\n\n\nclass SemanticSegmentationRasterStoreConfig(LabelStoreConfig):\n    def __init__(self, uri=None, vector_output=[], rgb=False):\n        super().__init__(store_type=rv.SEMANTIC_SEGMENTATION_RASTER)\n        self.uri = uri\n        self.vector_output = vector_output\n        self.rgb = rgb\n\n    def to_proto(self):\n        """"""Turn this configuration into a ProtoBuf message.\n\n        The fields in the message are as follows:\n            - `denoise` gives the radius of the structural element\n              used to remove high-frequency signals from the image.\n            - `uri` is the location where vector output should be\n              written\n            - `mode` is the vectorification mode (currently only\n              ""polygons"" and ""buildings"" are acceptable values).\n            - `class_id` specifies the predication class that is to\n              turned into vectors\n            - `building_options` communicates options useful for\n              vectorification of building predictions (it is intended\n              to break-up clusters of buildings):\n                - `min_aspect_ratio` is the ratio between length and\n                  height (or height and length) of anything that can\n                  be considered to be a cluster of buildings.  The\n                  goal is to distinguish between rows of buildings and\n                  (say) a single building.\n                - `min_area` is the minimum area of anything that can\n                  be considered to be a cluster of buildings.  The\n                  goal is to distinguish between buildings and\n                  artifacts.\n                - `element_width_factor` is the width of the\n                  structural element used to break building clusters\n                  as a fraction of the width of the cluster.\n                - `element_thickness` is the thickness of the\n                  structural element that is used to break building\n                  clusters.\n        """"""\n        msg = super().to_proto()\n        if self.uri:\n            msg.semantic_segmentation_raster_store.uri = self.uri\n        if self.vector_output:\n            ar = []\n            for vo in self.vector_output:\n                vo_msg = VectorOutput()\n                vo_msg.denoise = vo[\'denoise\'] if \'denoise\' in vo.keys() else 0\n                vo_msg.uri = vo[\'uri\'] if \'uri\' in vo.keys() else \'\'\n                vo_msg.mode = vo[\'mode\']\n                vo_msg.class_id = vo[\'class_id\']\n                if \'building_options\' in vo.keys():\n                    options = vo[\'building_options\']\n                else:\n                    options = {}\n                bldg_msg = vo_msg.building_options\n                if \'min_aspect_ratio\' in options.keys():\n                    bldg_msg.min_aspect_ratio = options[\'min_aspect_ratio\']\n                if \'min_area\' in options.keys() and options[\'min_area\']:\n                    bldg_msg.min_area = options[\'min_area\']\n                if \'element_width_factor\' in options.keys():\n                    bldg_msg.element_width_factor = options[\n                        \'element_width_factor\']\n                if \'element_thickness\' in options.keys():\n                    bldg_msg.element_thickness = options[\'element_thickness\']\n                ar.append(vo_msg)\n            msg.semantic_segmentation_raster_store.vector_output.extend(ar)\n        msg.semantic_segmentation_raster_store.rgb = self.rgb\n        return msg\n\n    def for_prediction(self, label_uri):\n        return self.to_builder() \\\n                   .with_uri(label_uri) \\\n                   .build()\n\n    def create_store(self, task_config, extent, crs_transformer, tmp_dir):\n        class_map = None\n        if self.rgb:\n            class_map = task_config.class_map\n\n        return SemanticSegmentationRasterStore(\n            self.uri,\n            extent,\n            crs_transformer,\n            tmp_dir,\n            vector_output=self.vector_output,\n            class_map=class_map)\n\n    def update_for_command(self, command_type, experiment_config,\n                           context=None):\n        if command_type == rv.PREDICT:\n            if not self.uri:\n                # Construct the URI for this prediction store,\n                # using the scene ID.\n                root = experiment_config.predict_uri\n                uri = None\n                for c in context:\n                    if isinstance(c, rv.SceneConfig):\n                        uri = os.path.join(root, \'{}.tif\'.format(c.id))\n                if uri:\n                    self.uri = uri\n                else:\n                    raise rv.ConfigError(\n                        \'SemanticSegmentationRasterStoreConfig has no \'\n                        \'URI set, and is not associated with a SceneConfig.\')\n\n            # Construct URIs for vector predictions\n            for vo in self.vector_output:\n                for c in context:\n                    if isinstance(c,\n                                  rv.SceneConfig) and (\'uri\' not in vo.keys()\n                                                       or not vo[\'uri\']):\n                        root = experiment_config.predict_uri\n                        mode = vo[\'mode\']\n                        class_id = vo[\'class_id\']\n                        vo[\'uri\'] = os.path.join(\n                            root, \'{}-{}-{}.geojson\'.format(\n                                c.id, class_id, mode))\n\n    def report_io(self, command_type, io_def):\n        if command_type == rv.PREDICT:\n            # Construct URIs for vector predictions\n            for vo in self.vector_output:\n                io_def.add_output(vo[\'uri\'])\n            io_def.add_output(self.uri)\n\n        if command_type == rv.EVAL:\n            if self.uri:\n                io_def.add_input(self.uri)\n            else:\n                msg = \'No URI set for SemanticSegmentationRasterStoreConfig\'\n                io_def.add_missing(msg)\n\n            for vo in self.vector_output:\n                io_def.add_input(vo[\'uri\'])\n\n\nclass SemanticSegmentationRasterStoreConfigBuilder(LabelStoreConfigBuilder):\n    def __init__(self, prev=None):\n        config = {}\n        if prev:\n            config = {\n                \'uri\': prev.uri,\n                \'vector_output\': prev.vector_output,\n                \'rgb\': prev.rgb,\n            }\n\n        super().__init__(SemanticSegmentationRasterStoreConfig, config)\n        self.valid_modes = set([\'buildings\', \'polygons\'])\n\n    def from_proto(self, msg):\n        uri = msg.semantic_segmentation_raster_store.uri\n        rgb = msg.semantic_segmentation_raster_store.rgb\n        vo_msg = msg.semantic_segmentation_raster_store.vector_output\n\n        return self.with_uri(uri) \\\n                   .with_vector_output(vo_msg) \\\n                   .with_rgb(rgb)\n\n    def with_uri(self, uri):\n        """"""Set URI for a GeoTIFF used to read/write predictions.""""""\n        b = deepcopy(self)\n        b.config[\'uri\'] = uri\n        return b\n\n    def with_vector_output(self, vector_output):\n        """"""Configure vector output for predictions.\n\n            Args:\n                vector_output: Either a list of dictionaries or a\n                    protobuf object.  The dictionary or the object\n                    contain (respectively) keys (attributes) called\n                    \'denoise\', \'uri\', \'class_id\', and \'mode\'.  The\n                    value associated with the \'denoise\' key specifies\n                    the radius of the structural element used to\n                    perform a low-pass filtering process on the mask\n                    (see\n                    https://en.wikipedia.org/wiki/Mathematical_morphology#Opening).\n                    The value associated with the \'uri\' key is either\n                    a file where the GeoJSON prediction will be\n                    written, or """" indicating that the filename should\n                    be auto-generated.  \'class_id\' is the integer\n                    prediction class that is of interest.  The \'mode\'\n                    key must be set to \'buildings\' or \'polygons\'.\n\n        """"""\n        b = deepcopy(self)\n        ar = []\n\n        if isinstance(vector_output, list):\n            for vo in vector_output:\n                ar.append(vo.copy())\n        else:\n            for vo_msg in vector_output:\n                bldg_msg = vo_msg.building_options\n                ar.append({\n                    \'denoise\': vo_msg.denoise,\n                    \'uri\': vo_msg.uri,\n                    \'mode\': vo_msg.mode,\n                    \'class_id\': vo_msg.class_id,\n                    \'building_options\': {\n                        \'min_aspect_ratio\':\n                        bldg_msg.min_aspect_ratio,\n                        \'min_area\':\n                        bldg_msg.min_area if bldg_msg.min_area > 0 else None,\n                        \'element_width_factor\':\n                        bldg_msg.element_width_factor,\n                        \'element_thickness\':\n                        bldg_msg.element_thickness,\n                    },\n                })\n\n        b.config[\'vector_output\'] = ar\n        return b\n\n    def with_rgb(self, rgb):\n        """"""Set flag for writing RGB data using the class map.\n\n        Otherwise this method will write the class ID into a single band.\n        """"""\n        b = deepcopy(self)\n        b.config[\'rgb\'] = rgb\n        return b\n\n    def validate(self):\n        vector_output = self.config.get(\'vector_output\')\n\n        if vector_output and not isinstance(vector_output, list):\n            for vo in vector_output:\n                if not hasattr(vo, \'mode\'):\n                    raise rv.ConfigError(\n                        \'The attribute vector_output of\'\n                        \' SemanticSegmentationRasterStoreConfig\'\n                        \' must be either trivial, a protobuf configuration\'\n                        \' object, or a list of\'\n                        \' appropriate dictionaries.\')\n                if vo.mode not in self.valid_modes:\n                    raise rv.ConfigError(\n                        \'mode key in vector_output dictionary must be one of {}\'\n                        .format(self.valid_modes))\n        elif vector_output and isinstance(vector_output, list):\n            for vo in vector_output:\n                if not isinstance(vo, dict):\n                    raise rv.ConfigError(\n                        \'The attribute vector_output of\'\n                        \' SemanticSegmentationRasterStoreConfig\'\n                        \' must be either trivial, a protobuf configuration\'\n                        \' object, or a list of\'\n                        \' appropriate dictionaries.\')\n                if \'mode\' not in vo.keys(\n                ) or vo[\'mode\'] not in self.valid_modes:\n                    raise rv.ConfigError(\n                        \'mode key in vector_output dictionary must be one of {}\'\n                        .format(self.valid_modes))\n'"
rastervision/data/label_store/utils.py,0,"b'def boxes_to_geojson(boxes, class_ids, crs_transformer, class_map,\n                     scores=None):\n    """"""Convert boxes and associated data into a GeoJSON dict.\n\n    Args:\n        boxes: list of Box in pixel row/col format.\n        class_ids: list of int (one for each box)\n        crs_transformer: CRSTransformer used to convert pixel coords to map\n            coords in the GeoJSON\n        class_map: ClassMap used to infer class_name from class_id\n        scores: optional list of score or scores.\n                If floats (one for each box), property name will be ""score"".\n                If lists of floats, property name will be ""scores"".\n\n    Returns:\n        dict in GeoJSON format\n    """"""\n    features = []\n    for box_ind, box in enumerate(boxes):\n        polygon = box.geojson_coordinates()\n        polygon = [list(crs_transformer.pixel_to_map(p)) for p in polygon]\n\n        class_id = int(class_ids[box_ind])\n        class_name = class_map.get_by_id(class_id).name\n\n        feature = {\n            \'type\': \'Feature\',\n            \'geometry\': {\n                \'type\': \'Polygon\',\n                \'coordinates\': [polygon]\n            },\n            \'properties\': {\n                \'class_id\': class_id,\n                \'class_name\': class_name\n            }\n        }\n\n        if scores is not None:\n            box_scores = scores[box_ind]\n\n            if box_scores is not None:\n                if type(box_scores) is list:\n                    feature[\'properties\'][\'scores\'] = box_scores\n                else:\n                    feature[\'properties\'][\'score\'] = box_scores\n\n        features.append(feature)\n\n    return {\'type\': \'FeatureCollection\', \'features\': features}\n'"
rastervision/data/raster_source/__init__.py,0,b'# flake8: noqa\n\nfrom rastervision.data.raster_source.raster_source import *\nfrom rastervision.data.raster_source.raster_source_config import *\nfrom rastervision.data.raster_source.rasterio_source import *\nfrom rastervision.data.raster_source.rasterio_source_config import *\nfrom rastervision.data.raster_source.rasterized_source import *\nfrom rastervision.data.raster_source.rasterized_source_config import *\n'
rastervision/data/raster_source/api.py,0,"b""# flake8: noqa\n\nfrom rastervision.data.vector_source.api import GEOJSON_SOURCE\n\n# Registry Keys\nRASTER_SOURCE = 'RASTER_SOURCE'\n\n# For backward compatibility\nGEOTIFF_SOURCE = 'GEOTIFF_SOURCE'\nIMAGE_SOURCE = 'IMAGE_SOURCE'\n\nRASTERIO_SOURCE = 'RASTERIO_SOURCE'\nRASTERIZED_SOURCE = 'RASTERIZED_SOURCE'\n\nraster_source_deprecated_map = {\n    GEOJSON_SOURCE: RASTERIZED_SOURCE,\n    GEOTIFF_SOURCE: RASTERIO_SOURCE,\n    IMAGE_SOURCE: RASTERIO_SOURCE\n}\n\nfrom .raster_source_config import RasterSourceConfig\n"""
rastervision/data/raster_source/default.py,0,"b'from abc import (ABC, abstractmethod)\n\nimport rastervision as rv\n\n\nclass RasterSourceDefaultProvider(ABC):\n    @staticmethod\n    @abstractmethod\n    def handles(s):\n        """"""Returns True if this provider is a default for this string""""""\n        pass\n\n    @abstractmethod\n    def construct(s, channel_order=None):\n        """"""Constructs default based on the string and an optional channel order.""""""\n        pass\n\n\nclass RasterioSourceDefaultProvider(RasterSourceDefaultProvider):\n    @staticmethod\n    def handles(uri):\n        # Since there are so many types handled by Rasterio/GDAL, the RasterioSource\n        # will be the catch-all. More specific types can be handled by other\n        # RasterSources.\n        return True\n\n    @staticmethod\n    def construct(uri, channel_order=None):\n        return rv.RasterSourceConfig.builder(rv.RASTERIO_SOURCE) \\\n                                    .with_uri(uri) \\\n                                    .with_channel_order(channel_order) \\\n                                    .build()\n'"
rastervision/data/raster_source/raster_source.py,0,"b'from abc import ABC, abstractmethod\n\n\nclass ChannelOrderError(Exception):\n    def __init__(self, channel_order, num_channels):\n        self.channel_order = channel_order\n        self.num_channels = num_channels\n        msg = \'The channel_order={} contains a channel index >= num_channels={}\'\n        super().__init__(msg.format(str(channel_order), num_channels))\n\n\nclass RasterSource(ABC):\n    """"""A source of raster data.\n\n    This should be subclassed when adding a new source of raster data such as\n    a set of files, an API, a TMS URI schema, etc.\n    """"""\n\n    def __init__(self, channel_order, num_channels, raster_transformers=[]):\n        """"""Construct a new RasterSource.\n\n        Args:\n            channel_order: list of channel indices to use when extracting chip from\n                raw imagery.\n            num_channels: Number of channels in the raw imagery before applying\n                channel_order.\n            raster_transformers: RasterTransformers used to transform chips\n                whenever they are retrieved.\n        """"""\n        self.channel_order = channel_order\n        self.num_channels = num_channels\n        self.raster_transformers = raster_transformers\n\n    def validate_channel_order(self, channel_order, num_channels):\n        for c in channel_order:\n            if c >= num_channels:\n                raise ChannelOrderError(channel_order, num_channels)\n\n    @abstractmethod\n    def get_extent(self):\n        """"""Return the extent of the RasterSource.\n\n        Returns:\n            Box in pixel coordinates with extent\n        """"""\n        pass\n\n    @abstractmethod\n    def get_dtype(self):\n        """"""Return the numpy.dtype of this scene""""""\n        pass\n\n    @abstractmethod\n    def get_crs_transformer(self):\n        """"""Return the associated CRSTransformer.""""""\n        pass\n\n    @abstractmethod\n    def _get_chip(self, window):\n        """"""Return the raw chip located in the window.\n\n        Args:\n            window: Box\n\n        Returns:\n            [height, width, channels] numpy array\n        """"""\n        pass\n\n    def get_chip(self, window):\n        """"""Return the transformed chip in the window.\n\n        Get a raw chip, extract subset of channels using channel_order, and then apply\n        transformations.\n\n        Args:\n            window: Box\n\n        Returns:\n            np.ndarray with shape [height, width, channels]\n        """"""\n        chip = self._get_chip(window)\n\n        if self.channel_order:\n            chip = chip[:, :, self.channel_order]\n\n        for transformer in self.raster_transformers:\n            chip = transformer.transform(chip, self.channel_order)\n\n        return chip\n\n    def get_raw_chip(self, window):\n        """"""Return raw chip without using channel_order or applying transforms.\n\n        Args:\n            window: (Box) the window for which to get the chip\n\n        Returns:\n            np.ndarray with shape [height, width, channels]\n        """"""\n        return self._get_chip(window)\n\n    def get_image_array(self):\n        """"""Return entire transformed image array.\n\n        Not safe to call on very large RasterSources.\n        """"""\n        return self.get_chip(self.get_extent())\n\n    def get_raw_image_array(self):\n        """"""Return entire raw image without using channel_order or applying transforms.\n\n        Not safe to call on very large RasterSources.\n        """"""\n        return self.get_raw_chip(self.get_extent())\n'"
rastervision/data/raster_source/raster_source_config.py,0,"b'from abc import abstractmethod\nfrom copy import deepcopy\nimport logging\n\nimport rastervision as rv\nfrom rastervision.core.config import (Config, ConfigBuilder,\n                                      BundledConfigMixin)\nfrom rastervision.data import (RasterTransformerConfig, StatsTransformerConfig)\nfrom rastervision.protos.raster_source_pb2 \\\n    import RasterSourceConfig as RasterSourceConfigMsg\n\nlog = logging.getLogger(__name__)\n\n\nclass RasterSourceConfig(BundledConfigMixin, Config):\n    deprecation_warnings = []\n\n    def __init__(self, source_type, transformers=None, channel_order=None):\n        if transformers is None:\n            transformers = []\n\n        self.source_type = source_type\n        self.transformers = transformers\n        self.channel_order = channel_order\n\n    def to_proto(self):\n        transformers = list(map(lambda c: c.to_proto(), self.transformers))\n        msg = RasterSourceConfigMsg(\n            source_type=self.source_type,\n            channel_order=self.channel_order,\n            transformers=transformers)\n        return msg\n\n    def save_bundle_files(self, bundle_dir):\n        new_transformers = []\n        files = []\n        for transformer in self.transformers:\n            new_transformer, t_files = transformer.save_bundle_files(\n                bundle_dir)\n            new_transformers.append(new_transformer)\n            files.extend(t_files)\n\n        new_config = self.to_builder() \\\n                         .with_transformers(new_transformers) \\\n                         .build()\n        return (new_config, files)\n\n    def load_bundle_files(self, bundle_dir):\n        new_transformers = []\n        for transformer in self.transformers:\n            new_transformer = transformer.load_bundle_files(bundle_dir)\n            new_transformers.append(new_transformer)\n        return self.to_builder() \\\n                   .with_transformers(new_transformers) \\\n                   .build()\n\n    @abstractmethod\n    def create_source(self, tmp_dir, crs_transformer, extent, class_map):\n        """"""Create the Raster Source for this configuration.\n        """"""\n        pass\n\n    def to_builder(self):\n        return rv._registry.get_config_builder(rv.RASTER_SOURCE,\n                                               self.source_type)(self)\n\n    @staticmethod\n    def check_deprecation(source_type):\n        # If source_type is deprecated and warning hasn\'t been shown yet, then warn.\n        if (source_type in rv.raster_source_deprecated_map and\n                source_type not in RasterSourceConfig.deprecation_warnings):\n            RasterSourceConfig.deprecation_warnings.append(source_type)\n            new_source_type = rv.raster_source_deprecated_map[source_type]\n            log.warn(\n                \'RasterSource {} is deprecated. Please use {} instead.\'.format(\n                    source_type, new_source_type))\n\n    def builder(source_type):\n        RasterSourceConfig.check_deprecation(source_type)\n        return rv._registry.get_config_builder(rv.RASTER_SOURCE, source_type)()\n\n    @staticmethod\n    def from_proto(msg):\n        """"""Creates a TaskConfig from the specificed protobuf message\n        """"""\n        return rv._registry.get_config_builder(rv.RASTER_SOURCE, msg.source_type)() \\\n                           .from_proto(msg) \\\n                           .build()\n\n    @abstractmethod\n    def for_prediction(self, image_uri):\n        """"""Creates a new config with the image_uri.""""""\n        pass\n\n    @abstractmethod\n    def create_local(self, tmp_dir):\n        """"""Returns a new config with a local copy of the image data\n        if this image is remote.\n        """"""\n        pass\n\n    def create_transformers(self):\n        return list(map(lambda c: c.create_transformer(), self.transformers))\n\n    def update_for_command(self, command_type, experiment_config,\n                           context=None):\n        for transformer in self.transformers:\n            transformer.update_for_command(command_type, experiment_config,\n                                           context)\n\n    def report_io(self, command_type, io_def):\n        for transformer in self.transformers:\n            transformer.report_io(command_type, io_def)\n\n\nclass RasterSourceConfigBuilder(ConfigBuilder):\n    def from_proto(self, msg):\n        transformers = list(\n            map(lambda m: RasterTransformerConfig.from_proto(m),\n                msg.transformers))\n\n        channel_order = list(msg.channel_order)\n        if len(channel_order) == 0:\n            channel_order = None\n        return self.with_channel_order(channel_order) \\\n                   .with_transformers(transformers)\n\n    def with_channel_order(self, channel_order):\n        """"""Defines the channel order for this raster source.\n\n        This defines the subset of channel indices and their order to use when extracting\n        chips from raw imagery.\n\n        Args:\n            channel_order: list of channel indices\n        """"""\n        b = deepcopy(self)\n        b.config[\'channel_order\'] = channel_order\n        return b\n\n    def with_transformers(self, transformers):\n        """"""Transformers to be applied to the raster data.\n\n            Args:\n                transformers: A list of transformers to apply to the\n                    raster data.\n\n        """"""\n        b = deepcopy(self)\n        b.config[\'transformers\'] = list(transformers)\n        return b\n\n    def with_transformer(self, transformer):\n        """"""A transformer to be applied to the raster data.\n\n            Args:\n                transformer: A transformer to apply to the raster\n                    data.\n\n        """"""\n        return self.with_transformers([transformer])\n\n    def with_stats_transformer(self):\n        """"""Add a stats transformer to the raster source.""""""\n        b = deepcopy(self)\n        transformers = b.config.get(\'transformers\')\n        if transformers:\n            b.config[\'transformers\'] = transformers.append(\n                StatsTransformerConfig())\n        else:\n            b.config[\'transformers\'] = [StatsTransformerConfig()]\n        return b\n'"
rastervision/data/raster_source/rasterio_source.py,0,"b'import logging\nimport math\nimport os\nimport pyproj\nimport subprocess\nfrom decimal import Decimal\nimport tempfile\n\nimport numpy as np\nimport rasterio\nfrom rasterio.enums import (ColorInterp, MaskFlags)\n\nfrom rastervision.core.box import Box\nfrom rastervision.data.crs_transformer import RasterioCRSTransformer\nfrom rastervision.utils.files import download_if_needed\nfrom rastervision.data import (ActivateMixin, ActivationError)\nfrom rastervision.data.raster_source import RasterSource\n\nlog = logging.getLogger(__name__)\nwgs84 = pyproj.Proj({\'init\': \'epsg:4326\'})\nwgs84_proj4 = \'+init=epsg:4326\'\nmeters_per_degree = 111319.5\n\n\ndef build_vrt(vrt_path, image_paths):\n    """"""Build a VRT for a set of TIFF files.""""""\n    cmd = [\'gdalbuildvrt\', vrt_path]\n    cmd.extend(image_paths)\n    subprocess.run(cmd)\n\n\ndef download_and_build_vrt(image_uris, temp_dir):\n    log.info(\'Building VRT...\')\n    image_paths = [download_if_needed(uri, temp_dir) for uri in image_uris]\n    image_path = os.path.join(temp_dir, \'index.vrt\')\n    build_vrt(image_path, image_paths)\n    return image_path\n\n\ndef load_window(image_dataset, window=None, is_masked=False):\n    """"""Load a window of an image using Rasterio.\n\n    Args:\n        image_dataset: a Rasterio dataset\n        window: ((row_start, row_stop), (col_start, col_stop)) or\n        ((y_min, y_max), (x_min, x_max))\n        is_masked: If True, read a masked array from rasterio\n\n    Returns:\n        np.ndarray of shape (height, width, channels) where channels is the number of\n            channels in the image_dataset.\n    """"""\n    if is_masked:\n        im = image_dataset.read(window=window, boundless=True, masked=True)\n        im = np.ma.filled(im, fill_value=0)\n    else:\n        im = image_dataset.read(window=window, boundless=True)\n\n    # Handle non-zero NODATA values by setting the data to 0.\n    for channel, nodata in enumerate(image_dataset.nodatavals):\n        if nodata is not None and nodata != 0:\n            im[channel, im[channel] == nodata] = 0\n\n    im = np.transpose(im, axes=[1, 2, 0])\n    return im\n\n\nclass RasterioSource(ActivateMixin, RasterSource):\n    def __init__(self,\n                 uris,\n                 raster_transformers,\n                 temp_dir,\n                 channel_order=None,\n                 x_shift_meters=0.0,\n                 y_shift_meters=0.0):\n        """"""Constructor.\n\n        This RasterSource can read any file that can be opened by Rasterio/GDAL\n        including georeferenced formats such as GeoTIFF and non-georeferenced formats\n        such as JPG. See https://www.gdal.org/formats_list.html for more details.\n\n        If channel_order is None, then use non-alpha channels. This also sets any\n        masked or NODATA pixel values to be zeros.\n\n        Args:\n            channel_order: list of indices of channels to extract from raw imagery\n        """"""\n        self.uris = uris\n        self.temp_dir = temp_dir\n        self.image_temp_dir = None\n        self.image_dataset = None\n        self.x_shift_meters = x_shift_meters\n        self.y_shift_meters = y_shift_meters\n\n        num_channels = None\n\n        # Activate in order to get information out of the raster\n        with self.activate():\n            num_channels = self.image_dataset.count\n            if channel_order is None:\n                colorinterp = self.image_dataset.colorinterp\n                if colorinterp:\n                    channel_order = [\n                        i for i, color_interp in enumerate(colorinterp)\n                        if color_interp != ColorInterp.alpha\n                    ]\n                else:\n                    channel_order = list(range(0, num_channels))\n            self.validate_channel_order(channel_order, num_channels)\n\n            mask_flags = self.image_dataset.mask_flag_enums\n            self.is_masked = any(\n                [m for m in mask_flags if m != MaskFlags.all_valid])\n\n            self.height = self.image_dataset.height\n            self.width = self.image_dataset.width\n\n            # Get 1x1 chip and apply raster transformers to test dtype.\n            test_chip = self.get_raw_chip(Box.make_square(0, 0, 1))\n            test_chip = test_chip[:, :, channel_order]\n            for transformer in raster_transformers:\n                test_chip = transformer.transform(test_chip, channel_order)\n            self.dtype = test_chip.dtype\n\n            self._set_crs_transformer()\n\n        super().__init__(channel_order, num_channels, raster_transformers)\n\n    def _download_data(self, temp_dir):\n        """"""Download any data needed for this Raster Source.\n\n        Return a single local path representing the image or a VRT of the data.\n        """"""\n        if len(self.uris) == 1:\n            return download_if_needed(self.uris[0], temp_dir)\n        else:\n            return download_and_build_vrt(self.uris, temp_dir)\n\n    def get_crs_transformer(self):\n        return self.crs_transformer\n\n    def get_extent(self):\n        return Box(0, 0, self.height, self.width)\n\n    def get_dtype(self):\n        """"""Return the numpy.dtype of this scene""""""\n        return self.dtype\n\n    def _get_chip(self, window):\n        if self.image_dataset is None:\n            raise ActivationError(\'RasterSource must be activated before use\')\n        shifted_window = self._get_shifted_window(window)\n        return load_window(\n            self.image_dataset,\n            window=shifted_window.rasterio_format(),\n            is_masked=self.is_masked)\n\n    def _activate(self):\n        # Download images to temporary directory and delete it when done.\n        self.image_temp_dir = tempfile.TemporaryDirectory(dir=self.temp_dir)\n        self.imagery_path = self._download_data(self.image_temp_dir.name)\n        self.image_dataset = rasterio.open(self.imagery_path)\n        self._set_crs_transformer()\n\n    def _set_crs_transformer(self):\n        self.crs_transformer = RasterioCRSTransformer.from_dataset(\n            self.image_dataset)\n        self.crs = self.image_dataset.crs\n        if self.crs:\n            self.proj = pyproj.Proj(self.crs)\n        else:\n            self.proj = None\n        self.crs = str(self.crs)\n\n    def _deactivate(self):\n        self.image_dataset.close()\n        self.image_dataset = None\n        self.image_temp_dir.cleanup()\n        self.image_temp_dir = None\n\n    def _get_shifted_window(self, window):\n        do_shift = self.x_shift_meters != 0.0 or self.y_shift_meters != 0.0\n        if do_shift:\n            ymin, xmin, ymax, xmax = window.tuple_format()\n            width = window.get_width()\n            height = window.get_height()\n\n            # Transform image coordinates into world coordinates\n            transform = self.image_dataset.transform\n            xmin2, ymin2 = transform * (xmin, ymin)\n\n            # Transform from world coordinates to WGS84\n            if self.crs != wgs84_proj4 and self.proj:\n                lon, lat = pyproj.transform(self.proj, wgs84, xmin2, ymin2)\n            else:\n                lon, lat = xmin2, ymin2\n\n            # Shift.  This is performed by computing the shifts in\n            # meters to shifts in degrees.  Those shifts are then\n            # applied to the WGS84 coordinate.\n            #\n            # Courtesy of https://gis.stackexchange.com/questions/2951/algorithm-for-offsetting-a-latitude-longitude-by-some-amount-of-meters  # noqa\n            lat_radians = math.pi * lat / 180.0\n            dlon = Decimal(self.x_shift_meters) / Decimal(\n                meters_per_degree * math.cos(lat_radians))\n            dlat = Decimal(self.y_shift_meters) / Decimal(meters_per_degree)\n            lon = float(Decimal(lon) + dlon)\n            lat = float(Decimal(lat) + dlat)\n\n            # Transform from WGS84 to world coordinates\n            if self.crs != wgs84_proj4 and self.proj:\n                xmin3, ymin3 = pyproj.transform(wgs84, self.proj, lon, lat)\n                xmin3 = int(round(xmin3))\n                ymin3 = int(round(ymin3))\n            else:\n                xmin3, ymin3 = lon, lat\n\n            # Trasnform from world coordinates back into image coordinates\n            xmin4, ymin4 = ~transform * (xmin3, ymin3)\n\n            window = Box(ymin4, xmin4, ymin4 + height, xmin4 + width)\n        return window\n'"
rastervision/data/raster_source/rasterio_source_config.py,0,"b'from copy import deepcopy\n\nimport rastervision as rv\nfrom rastervision.data.raster_source.rasterio_source import RasterioSource\nfrom rastervision.data.raster_source.raster_source_config \\\n    import (RasterSourceConfig, RasterSourceConfigBuilder)\nfrom rastervision.protos.raster_source_pb2 \\\n    import RasterSourceConfig as RasterSourceConfigMsg\nfrom rastervision.utils.files import download_if_needed\n\n\nclass RasterioSourceConfig(RasterSourceConfig):\n    def __init__(self,\n                 uris,\n                 x_shift_meters=0.0,\n                 y_shift_meters=0.0,\n                 transformers=None,\n                 channel_order=None):\n        super().__init__(\n            source_type=rv.RASTERIO_SOURCE,\n            transformers=transformers,\n            channel_order=channel_order)\n        self.uris = uris\n        self.x_shift_meters = x_shift_meters\n        self.y_shift_meters = y_shift_meters\n\n    def to_proto(self):\n        msg = super().to_proto()\n        msg.rasterio_source.CopyFrom(\n            RasterSourceConfigMsg.RasterioSource(\n                uris=self.uris,\n                x_shift_meters=self.x_shift_meters,\n                y_shift_meters=self.y_shift_meters))\n        return msg\n\n    def save_bundle_files(self, bundle_dir):\n        (conf, files) = super().save_bundle_files(bundle_dir)\n\n        # Replace the URI with a template value.\n        new_config = conf.to_builder() \\\n                         .with_uri(\'BUNDLE\') \\\n                         .build()\n        return (new_config, files)\n\n    def for_prediction(self, image_uri):\n        return self.to_builder() \\\n                   .with_uri(image_uri) \\\n                   .build()\n\n    def create_local(self, tmp_dir):\n        new_uris = [download_if_needed(uri, tmp_dir) for uri in self.uris]\n        return self.to_builder() \\\n                   .with_uris(new_uris) \\\n                   .build()\n\n    def create_source(self,\n                      tmp_dir,\n                      crs_transformer=None,\n                      extent=None,\n                      class_map=None):\n        transformers = self.create_transformers()\n        x_shift_meters = self.x_shift_meters\n        y_shift_meters = self.y_shift_meters\n        return RasterioSource(\n            uris=self.uris,\n            raster_transformers=transformers,\n            temp_dir=tmp_dir,\n            channel_order=self.channel_order,\n            x_shift_meters=x_shift_meters,\n            y_shift_meters=y_shift_meters)\n\n    def report_io(self, command_type, io_def):\n        super().report_io(command_type, io_def)\n        io_def.add_inputs(self.uris)\n\n\nclass RasterioSourceConfigBuilder(RasterSourceConfigBuilder):\n    """"""This RasterSource can read any file that can be opened by Rasterio/GDAL.\n\n    This includes georeferenced formats such as GeoTIFF and non-georeferenced formats\n    such as JPG. See https://www.gdal.org/formats_list.html for more details.\n    """"""\n\n    def __init__(self, prev=None):\n        config = {}\n        if prev:\n            config = {\n                \'uris\': prev.uris,\n                \'transformers\': prev.transformers,\n                \'channel_order\': prev.channel_order,\n                \'x_shift_meters\': prev.x_shift_meters,\n                \'y_shift_meters\': prev.y_shift_meters,\n            }\n\n        super().__init__(RasterioSourceConfig, config)\n\n    def validate(self):\n        super().validate()\n        if self.config.get(\'uris\') is None:\n            raise rv.ConfigError(\n                \'You must specify uris for the RasterioSourceConfig. Use \'\n                \'""with_uris"".\')\n        if not isinstance(self.config.get(\'uris\'), list):\n            raise rv.ConfigError(\n                \'uris set with ""with_uris"" must be a list, got {}\'.format(\n                    type(self.config.get(\'uris\'))))\n        for uri in self.config.get(\'uris\'):\n            if not isinstance(uri, str):\n                raise rv.ConfigError(\'uri must be a string, got {}\'.format(\n                    type(uri)))\n\n    def from_proto(self, msg):\n        b = super().from_proto(msg)\n\n        # Need to do this for backward compatibility.\n        if msg.HasField(\'geotiff_files\') or msg.HasField(\'rasterio_source\'):\n            if msg.HasField(\'geotiff_files\'):\n                source = msg.geotiff_files\n            else:\n                source = msg.rasterio_source\n\n            return b \\\n                .with_uris(source.uris) \\\n                .with_shifts(source.x_shift_meters, source.y_shift_meters)\n        elif msg.HasField(\'image_file\'):\n            source = msg.image_file\n            return b.with_uri(source.uri)\n        else:\n            raise rv.ConfigError(\n                \'RasterioSourceConfig protobuf message should contain geotiff_files, \'\n                \'rasterio_source, or image_file\')\n\n    def with_uris(self, uris):\n        """"""Set URIs for raster files that can be read by Rasterio.""""""\n        b = deepcopy(self)\n        b.config[\'uris\'] = list(uris)\n        return b\n\n    def with_uri(self, uri):\n        """"""Set URI for raster files that can be read by Rasterio.""""""\n        b = deepcopy(self)\n        b.config[\'uris\'] = [uri]\n        return b\n\n    def with_shifts(self, x, y):\n        """"""Set the x- and y-shifts in meters.\n\n        This will only have an effect on georeferenced imagery.\n\n        Args:\n            x: A number of meters to shift along the x-axis.  A\n                positive shift moves the ""camera"" to the right.\n\n            y: A number of meters to shift along the y-axis.  A\n                positive shift moves the ""camera"" down.\n        """"""\n        b = deepcopy(self)\n        b.config[\'x_shift_meters\'] = x\n        b.config[\'y_shift_meters\'] = y\n        return b\n'"
rastervision/data/raster_source/rasterized_source.py,0,"b'import logging\n\nfrom rasterio.features import rasterize\nimport numpy as np\nfrom shapely.geometry import shape\nfrom shapely.strtree import STRtree\nimport shapely\n\nfrom rastervision.core import Box\nfrom rastervision.data import (ActivateMixin, ActivationError)\nfrom rastervision.data.raster_source import RasterSource\n\nlog = logging.getLogger(__name__)\n\n\ndef geoms_to_raster(str_tree, rasterizer_options, window, extent):\n    background_class_id = rasterizer_options.background_class_id\n    all_touched = rasterizer_options.all_touched\n\n    log.debug(\'Cropping shapes to window...\')\n    # Crop shapes against window, remove empty shapes, and put in window frame of\n    # reference.\n    window_geom = window.to_shapely()\n    shapes = str_tree.query(window_geom)\n    shapes = [(s, s.class_id) for s in shapes]\n    shapes = [(s.intersection(window_geom), c) for s, c in shapes]\n    shapes = [(s, c) for s, c in shapes if not s.is_empty]\n\n    def to_window_frame(x, y, z=None):\n        return (x - window.xmin, y - window.ymin)\n\n    shapes = [(shapely.ops.transform(to_window_frame, s), c)\n              for s, c in shapes]\n    log.debug(\'# of shapes in window: {}\'.format(len(shapes)))\n\n    out_shape = (window.get_height(), window.get_width())\n\n    # rasterize needs to be passed >= 1 shapes.\n    if shapes:\n        log.debug(\'rasterio.rasterize()...\')\n        raster = rasterize(\n            shapes,\n            out_shape=out_shape,\n            fill=background_class_id,\n            dtype=np.uint8,\n            all_touched=all_touched)\n    else:\n        raster = np.full(out_shape, background_class_id, dtype=np.uint8)\n\n    # Ensure that parts of window outside of extent have zero values which are counted as\n    # the don\'t-care class for segmentation.\n    valid_window = window_geom.intersection(extent.to_shapely())\n    if valid_window.is_empty:\n        raster[:, :] = 0\n    else:\n        vw = shapely.ops.transform(to_window_frame, valid_window)\n        vw = Box.from_shapely(vw).to_int()\n        new_raster = np.zeros(out_shape)\n        new_raster[vw.ymin:vw.ymax, vw.xmin:vw.xmax] = \\\n            raster[vw.ymin:vw.ymax, vw.xmin:vw.xmax]\n        raster = new_raster\n\n    return raster\n\n\nclass RasterizedSource(ActivateMixin, RasterSource):\n    """"""A RasterSource based on the rasterization of a VectorSource.""""""\n\n    def __init__(self, vector_source, rasterizer_options, extent,\n                 crs_transformer):\n        """"""Constructor.\n\n        Args:\n            vector_source: (VectorSource)\n            rasterizer_options:\n                rastervision.data.raster_source.GeoJSONSourceConfig.RasterizerOptions\n            extent: (Box) extent of corresponding imagery RasterSource\n            crs_transformer: (CRSTransformer)\n        """"""\n        self.vector_source = vector_source\n        self.rasterizer_options = rasterizer_options\n        self.extent = extent\n        self.crs_transformer = crs_transformer\n        self.activated = False\n\n        super().__init__(channel_order=[0], num_channels=1)\n\n    def get_extent(self):\n        """"""Return the extent of the RasterSource.\n\n        Returns:\n            Box in pixel coordinates with extent\n        """"""\n        return self.extent\n\n    def get_dtype(self):\n        """"""Return the numpy.dtype of this scene""""""\n        return np.uint8\n\n    def get_crs_transformer(self):\n        """"""Return the associated CRSTransformer.""""""\n        return self.crs_transformer\n\n    def _get_chip(self, window):\n        """"""Return the chip located in the window.\n\n        Polygons falling within the window are rasterized using the class_id, and\n        the background is filled with background_class_id. Also, any pixels in the\n        window outside the extent are zero, which is the don\'t-care class for\n        segmentation.\n\n        Args:\n            window: Box\n\n        Returns:\n            [height, width, channels] numpy array\n        """"""\n        if not self.activated:\n            raise ActivationError(\'GeoJSONSource must be activated before use\')\n\n        log.debug(\'Rasterizing window: {}\'.format(window))\n        chip = geoms_to_raster(self.str_tree, self.rasterizer_options, window,\n                               self.get_extent())\n        # Add third singleton dim since rasters must have >=1 channel.\n        return np.expand_dims(chip, 2)\n\n    def _activate(self):\n        geojson = self.vector_source.get_geojson()\n        geoms = []\n        for f in geojson[\'features\']:\n            geom = shape(f[\'geometry\'])\n            geom.class_id = f[\'properties\'][\'class_id\']\n            geoms.append(geom)\n        self.str_tree = STRtree(geoms)\n        self.activated = True\n\n    def _deactivate(self):\n        self.str_tree = None\n        self.activated = False\n'"
rastervision/data/raster_source/rasterized_source_config.py,0,"b'from copy import deepcopy\n\nimport rastervision as rv\nfrom rastervision.data.raster_source.rasterized_source import (\n    RasterizedSource)\nfrom rastervision.data.raster_source.raster_source_config \\\n    import (RasterSourceConfig, RasterSourceConfigBuilder)\nfrom rastervision.protos.raster_source_pb2 \\\n    import RasterSourceConfig as RasterSourceConfigMsg\nfrom rastervision.data.vector_source import VectorSourceConfig\nfrom rastervision.utils.files import download_if_needed\n\n\nclass RasterizedSourceConfig(RasterSourceConfig):\n    class RasterizerOptions(object):\n        def __init__(self, background_class_id, all_touched=False):\n            """"""Constructor.\n\n            Args:\n                background_class_id: The class_id to use for background pixels that don\'t\n                    overlap with any shapes in the GeoJSON file.\n                all_touched: If True, all pixels touched by geometries will be burned in.\n                             If false, only pixels whose center is within the polygon or\n                             that are selected by Bresenham\xe2\x80\x99s line algorithm will be\n                             burned in. (See rasterio.features.rasterize).\n            """"""\n            self.background_class_id = background_class_id\n            self.all_touched = all_touched\n\n        def to_proto(self):\n            return RasterSourceConfigMsg.RasterizedSource.RasterizerOptions(\n                background_class_id=self.background_class_id,\n                all_touched=self.all_touched)\n\n    def __init__(self,\n                 vector_source,\n                 rasterizer_options,\n                 transformers=None,\n                 channel_order=None):\n        super().__init__(\n            source_type=rv.RASTERIZED_SOURCE,\n            transformers=transformers,\n            channel_order=channel_order)\n        self.vector_source = vector_source\n        self.rasterizer_options = rasterizer_options\n\n    def to_proto(self):\n        msg = super().to_proto()\n        msg.MergeFrom(\n            RasterSourceConfigMsg(\n                rasterized_source=RasterSourceConfigMsg.RasterizedSource(\n                    vector_source=self.vector_source.to_proto(),\n                    rasterizer_options=self.rasterizer_options.to_proto())))\n        return msg\n\n    def save_bundle_files(self, bundle_dir):\n        (conf, files) = super().save_bundle_files(bundle_dir)\n        # Replace the URI with a template value.\n        new_config = conf.to_builder() \\\n                         .with_uri(\'BUNDLE\') \\\n                         .build()\n        return (new_config, files)\n\n    def for_prediction(self, uri):\n        return self.to_builder() \\\n                   .with_uri(uri) \\\n                   .build()\n\n    def create_local(self, tmp_dir):\n        new_uri = download_if_needed(self.uri, tmp_dir)\n        return self.to_builder() \\\n                   .with_uri(new_uri) \\\n                   .build()\n\n    def create_source(self, tmp_dir, crs_transformer, extent, class_map=None):\n        vector_source = self.vector_source.create_source(\n            crs_transformer=crs_transformer,\n            extent=extent,\n            class_map=class_map)\n        return RasterizedSource(vector_source, self.rasterizer_options, extent,\n                                crs_transformer)\n\n    def update_for_command(self, command_type, experiment_config,\n                           context=None):\n        super().update_for_command(command_type, experiment_config, context)\n        self.vector_source.update_for_command(command_type, experiment_config,\n                                              context)\n\n    def report_io(self, command_type, io_def):\n        super().update_for_command(command_type, io_def)\n        self.vector_source.report_io(command_type, io_def)\n\n\nclass RasterizedSourceConfigBuilder(RasterSourceConfigBuilder):\n    def __init__(self, prev=None):\n        config = {}\n        if prev:\n            config = {\n                \'vector_source\': prev.vector_source,\n                \'rasterizer_options\': prev.rasterizer_options\n            }\n\n        super().__init__(RasterizedSourceConfig, config)\n\n    def validate(self):\n        super().validate()\n\n        vector_source = self.config.get(\'vector_source\')\n        if vector_source is None:\n            raise rv.ConfigError(\n                \'You must specify a vector_source for the RasterizedSourceConfig. \'\n                \'Use ""with_vector_source""\')\n        if not isinstance(vector_source, VectorSourceConfig):\n            raise rv.ConfigError(\n                \'vector source must be a child of class VectorSourceConfig, got {}\'.\n                format(type(vector_source)))\n        if vector_source.has_null_class_bufs():\n            raise rv.ConfigError(\n                \'Setting buffer to None for a class in the vector_source is not allowed \'\n                \'for RasterizedSourceConfig.\')\n\n        if self.config.get(\'rasterizer_options\') is None:\n            raise rv.ConfigError(\n                \'You must configure the rasterizer for the RasterizedSourceConfig. \'\n                \'Use ""with_rasterizer_options""\')\n        if not isinstance(\n                self.config.get(\'rasterizer_options\'),\n                RasterizedSourceConfig.RasterizerOptions):\n            raise rv.ConfigError(\n                \'rasterizer_options must be of type \'\n                \'GeoJSONSourceConfig.RasterizerOptions, got\'.format(\n                    type(self.config.get(\'rasterizer_options\'))))\n\n    def from_proto(self, msg):\n        b = super().from_proto(msg)\n\n        # Added for backwards compatibility.\n        if msg.HasField(\'geojson_file\'):\n            vector_source = msg.geojson_file.uri\n            rasterizer_options = msg.geojson_file.rasterizer_options\n        else:\n            vector_source = VectorSourceConfig.from_proto(\n                msg.rasterized_source.vector_source)\n            rasterizer_options = msg.rasterized_source.rasterizer_options\n\n        return b \\\n            .with_vector_source(vector_source) \\\n            .with_rasterizer_options(\n                rasterizer_options.background_class_id,\n                rasterizer_options.all_touched)\n\n    def with_vector_source(self, vector_source):\n        """"""Set the vector_source.\n\n        Args:\n            vector_source (str or VectorSource) if a string, assume it is\n                a URI and use the default provider to construct a VectorSource.\n        """"""\n        if isinstance(vector_source, str):\n            return self.with_uri(vector_source)\n\n        b = deepcopy(self)\n        if isinstance(vector_source, VectorSourceConfig):\n            b.config[\'vector_source\'] = vector_source\n        else:\n            raise rv.ConfigError(\n                \'vector_source must be of type str or VectorSource\')\n\n        return b\n\n    def with_uri(self, uri):\n        b = deepcopy(self)\n        provider = rv._registry.get_vector_source_default_provider(uri)\n        b.config[\'vector_source\'] = provider.construct(uri)\n        return b\n\n    def with_rasterizer_options(self, background_class_id, all_touched=False):\n        """"""Specify options for converting GeoJSON to raster.\n\n        Args:\n            background_class_id: The class_id to use for background pixels that don\'t\n                overlap with any shapes in the GeoJSON file.\n            all_touched: If True, all pixels touched by geometries will be burned in.\n                         If false, only pixels whose center is within the polygon or that\n                         are selected by Bresenham\xe2\x80\x99s line algorithm will be burned in.\n                         (See rasterio.features.rasterize).\n\n        """"""\n        b = deepcopy(self)\n        b.config[\n            \'rasterizer_options\'] = RasterizedSourceConfig.RasterizerOptions(\n                background_class_id, all_touched)\n        return b\n'"
rastervision/data/raster_transformer/__init__.py,0,b'# flake8: noqa\n\nfrom rastervision.data.raster_transformer.raster_transformer import *\nfrom rastervision.data.raster_transformer.raster_transformer_config import *\nfrom rastervision.data.raster_transformer.noop_transformer import *\nfrom rastervision.data.raster_transformer.stats_transformer import *\nfrom rastervision.data.raster_transformer.stats_transformer_config import *\n'
rastervision/data/raster_transformer/api.py,0,"b""# flake8: noqa\n\n# Registry Keys\n\nRASTER_TRANSFORMER = 'RASTER_TRANSFORMER'\n\nCHANNEL_TRANSFORMER = 'CHANNEL_TRANSFORMER'\nSTATS_TRANSFORMER = 'STATS_TRANSFORMER'\n\nfrom rastervision.data.raster_transformer.raster_transformer_config \\\n    import RasterTransformerConfig\n"""
rastervision/data/raster_transformer/noop_transformer.py,0,"b'from rastervision.data.raster_transformer.raster_transformer \\\n    import RasterTransformer\n\n\nclass NoopTransformer(RasterTransformer):\n    """"""No-op transformer\n    """"""\n\n    def __init__(self):\n        pass\n\n    def transform(self, chip, channel_order=None):\n        return chip\n'"
rastervision/data/raster_transformer/raster_transformer.py,0,"b'from abc import (ABC, abstractmethod)\n\n\nclass RasterTransformer(ABC):\n    """"""Transforms raw chips to be input to a neural network.""""""\n\n    @abstractmethod\n    def transform(self, chip, channel_order=None):\n        """"""Transform a chip of a raster source.\n\n        Args:\n            chip: ndarray of shape [height, width, channels] This is assumed to already\n                have the channel_order applied to it if channel_order is set. In other\n                words, channels should be equal to len(channel_order).\n            channel_order: list of indices of channels that were extracted from the\n                raw imagery.\n\n        Returns:\n            [height, width, channels] numpy array\n        """"""\n        pass\n'"
rastervision/data/raster_transformer/raster_transformer_config.py,0,"b'from abc import abstractmethod\n\nimport rastervision as rv\nfrom rastervision.core.config import (Config, ConfigBuilder,\n                                      BundledConfigMixin)\n\n\nclass RasterTransformerConfig(BundledConfigMixin, Config):\n    def __init__(self, transformer_type):\n        self.transformer_type = transformer_type\n\n    @abstractmethod\n    def create_transformer(self):\n        """"""Create the Transformer that this configuration represents\n        """"""\n        pass\n\n    def to_builder(self):\n        return rv._registry.get_config_builder(rv.RASTER_TRANSFORMER,\n                                               self.transformer_type)(self)\n\n    @staticmethod\n    def builder(transformer_type):\n        return rv._registry.get_config_builder(rv.RASTER_TRANSFORMER,\n                                               transformer_type)()\n\n    @staticmethod\n    def from_proto(msg):\n        """"""Creates a TaskConfig from the specificed protobuf message\n        """"""\n        return rv._registry.get_config_builder(rv.RASTER_TRANSFORMER,\n                                               msg.transformer_type)() \\\n                           .from_proto(msg) \\\n                           .build()\n\n\nclass RasterTransformerConfigBuilder(ConfigBuilder):\n    pass\n'"
rastervision/data/raster_transformer/stats_transformer.py,0,"b'import numpy as np\n\nfrom rastervision.data.raster_transformer.raster_transformer \\\n    import RasterTransformer\n\n\nclass StatsTransformer(RasterTransformer):\n    """"""Transforms non-uint8 to uint8 values using raster_stats.\n    """"""\n\n    def __init__(self, raster_stats=None):\n        """"""Construct a new StatsTransformer.\n\n        Args:\n            raster_stats: (RasterStats) used to transform chip to have\n                desired statistics\n        """"""\n        self.raster_stats = raster_stats\n\n    def transform(self, chip, channel_order=None):\n        """"""Transform a chip.\n\n        Transforms non-uint8 to uint8 values using raster_stats.\n\n        Args:\n            chip: ndarray of shape [height, width, channels] This is assumed to already\n                have the channel_order applied to it if channel_order is set. In other\n                words, channels should be equal to len(channel_order).\n            channel_order: list of indices of channels that were extracted from the\n                raw imagery.\n\n        Returns:\n            [height, width, channels] uint8 numpy array\n\n        """"""\n        if chip.dtype != np.uint8:\n            if self.raster_stats:\n                if channel_order is None:\n                    channel_order = np.arange(chip.shape[2])\n\n                # Subtract mean and divide by std to get zscores.\n                means = np.array(self.raster_stats.means)\n                means = means[np.newaxis, np.newaxis, channel_order].astype(\n                    np.float)\n                stds = np.array(self.raster_stats.stds)\n                stds = stds[np.newaxis, np.newaxis, channel_order].astype(\n                    np.float)\n\n                # Don\'t transform NODATA zero values.\n                nodata = chip == 0\n\n                chip = chip - means\n                chip = chip / stds\n\n                # Make zscores that fall between -3 and 3 span 0 to 255.\n                chip += 3\n                chip /= 6\n\n                chip = np.clip(chip, 0, 1)\n                chip *= 255\n                chip = chip.astype(np.uint8)\n\n                chip[nodata] = 0\n            else:\n                raise ValueError(\'raster_stats not defined.\')\n\n        return chip\n'"
rastervision/data/raster_transformer/stats_transformer_config.py,0,"b'import os\nfrom copy import deepcopy\n\nimport rastervision as rv\nfrom rastervision.core.raster_stats import RasterStats\nfrom rastervision.data.raster_transformer import (\n    RasterTransformerConfig, RasterTransformerConfigBuilder, StatsTransformer,\n    NoopTransformer)\nfrom rastervision.protos.raster_transformer_pb2 \\\n    import RasterTransformerConfig as RasterTransformerConfigMsg\n\n\nclass StatsTransformerConfig(RasterTransformerConfig):\n    def __init__(self, stats_uri=None):\n        super().__init__(rv.STATS_TRANSFORMER)\n        self.stats_uri = stats_uri\n\n    def to_proto(self):\n        msg = RasterTransformerConfigMsg(\n            transformer_type=self.transformer_type, stats_uri=self.stats_uri)\n        return msg\n\n    def save_bundle_files(self, bundle_dir):\n        if not self.stats_uri:\n            raise rv.ConfigError(\'stat_uri is not set.\')\n        local_path, base_name = self.bundle_file(self.stats_uri, bundle_dir)\n        new_config = self.to_builder() \\\n                         .with_stats_uri(base_name) \\\n                         .build()\n        return (new_config, [local_path])\n\n    def load_bundle_files(self, bundle_dir):\n        if not self.stats_uri:\n            raise rv.ConfigError(\'stat_uri is not set.\')\n        local_stats_uri = os.path.join(bundle_dir, self.stats_uri)\n        return self.to_builder() \\\n                   .with_stats_uri(local_stats_uri) \\\n                   .build()\n\n    def create_transformer(self):\n        if not self.stats_uri:\n            return NoopTransformer()\n\n        return StatsTransformer(RasterStats.load(self.stats_uri))\n\n    def update_for_command(self, command_type, experiment_config,\n                           context=None):\n        if command_type != rv.ANALYZE:\n            if not self.stats_uri:\n                # Find the stats URI from a StatsAnalyzer\n                for analyzer in experiment_config.analyzers:\n                    if analyzer.analyzer_type == rv.STATS_ANALYZER:\n                        self.stats_uri = analyzer.stats_uri\n\n    def report_io(self, command_type, io_def):\n        if command_type != rv.ANALYZE:\n            if not self.stats_uri:\n                io_def.add_missing(\n                    ""StatsTransformerConfig is missing \'stats_uri\' property ""\n                    \'in command {}. \'\n                    \'This must be set on the configuration, or a \'\n                    \'StatsAnalyzerConfig must be added to \'\n                    \'this experiment.\'.format(command_type))\n            else:\n                io_def.add_input(self.stats_uri)\n\n\nclass StatsTransformerConfigBuilder(RasterTransformerConfigBuilder):\n    def __init__(self, prev=None):\n        config = {}\n        if prev:\n            config = {\'stats_uri\': prev.stats_uri}\n        super().__init__(StatsTransformerConfig, config)\n\n    def from_proto(self, msg):\n        return self.with_stats_uri(msg.stats_uri)\n\n    def with_stats_uri(self, stats_uri):\n        """"""Set the stats_uri.\n\n            Args:\n                stats_uri: URI to the stats json to use\n        """"""\n        b = deepcopy(self)\n        b.config[\'stats_uri\'] = stats_uri\n        return b\n'"
rastervision/data/vector_source/__init__.py,0,b'# flake8: noqa\n\nfrom rastervision.data.vector_source.geojson_vector_source import *\nfrom rastervision.data.vector_source.geojson_vector_source_config import *\nfrom rastervision.data.vector_source.vector_tile_vector_source import *\nfrom rastervision.data.vector_source.vector_tile_vector_source_config import *\nfrom rastervision.data.vector_source.vector_source import *\nfrom rastervision.data.vector_source.vector_source_config import *\nfrom rastervision.data.vector_source.class_inference import *\n'
rastervision/data/vector_source/api.py,0,"b""# flake8: noqa\n\n# Registry Keys\nVECTOR_SOURCE = 'VECTOR_SOURCE'\n\nGEOJSON_SOURCE = 'GEOJSON_SOURCE'\nVECTOR_TILE_SOURCE = 'VECTOR_TILE_SOURCE'\n\nfrom .vector_source_config import VectorSourceConfig\n"""
rastervision/data/vector_source/class_inference.py,0,"b'import copy\n\nfrom rastervision.data.vector_source.label_maker.filter import create_filter\n\n\nclass ClassInferenceOptions():\n    def __init__(self,\n                 class_map=None,\n                 class_id_to_filter=None,\n                 default_class_id=1):\n        self.class_map = class_map\n        self.class_id_to_filter = class_id_to_filter\n        self.default_class_id = default_class_id\n\n\nclass ClassInference():\n    """"""Infers missing class_ids from GeoJSON features.""""""\n\n    def __init__(self, options):\n        """"""Constructor.\n\n        Args:\n            options: (ClassInferenceOptions)\n        """"""\n        self.options = options\n\n        if self.options.class_id_to_filter is not None:\n            self.class_id_to_filter = {}\n            for class_id, filter_exp in self.options.class_id_to_filter.items(\n            ):\n                self.class_id_to_filter[class_id] = create_filter(filter_exp)\n\n    def infer_class_id(self, feature):\n        """"""Infer the class_id for a GeoJSON feature.\n\n        Args:\n            feature: (dict) GeoJSON feature\n\n        Rules:\n            1) If class_id is in feature[\'properties\'], use it.\n            2) If class_name or label are in feature[\'properties\'] and in class_map,\n                use corresponding class_id.\n            3) If class_id_to_filter is set and filter is true when applied to feature,\n                use corresponding class_id.\n            4) Otherwise, return the default_class_id\n        """"""\n        class_id = feature.get(\'properties\', {}).get(\'class_id\')\n        if class_id is not None:\n            return class_id\n\n        if self.options.class_map is not None:\n            class_name = feature.get(\'properties\', {}).get(\'class_name\')\n            if class_name in self.options.class_map.get_class_names():\n                return self.options.class_map.get_by_name(class_name).id\n\n            label = feature.get(\'properties\', {}).get(\'label\')\n            if label in self.options.class_map.get_class_names():\n                return self.options.class_map.get_by_name(label).id\n\n        if self.options.class_id_to_filter is not None:\n            for class_id, filter_fn in self.class_id_to_filter.items():\n                if filter_fn(feature):\n                    return class_id\n\n        return self.options.default_class_id\n\n    def transform_geojson(self, geojson):\n        """"""Transform GeoJSON by appending class_ids and removing features with no class.\n\n        For each feature in geojson, the class_id is inferred and is set into\n        feature[\'properties\']. If the class_id is None (because none of the rules apply\n        and the default_class_id is None), the feature is dropped.\n        """"""\n        new_features = []\n        for feature in geojson[\'features\']:\n            class_id = self.infer_class_id(feature)\n            if class_id is not None:\n                feature = copy.deepcopy(feature)\n                properties = feature.get(\'properties\', {})\n                properties[\'class_id\'] = class_id\n                feature[\'properties\'] = properties\n                new_features.append(feature)\n        new_geojson = {\'type\': \'FeatureCollection\', \'features\': new_features}\n        return new_geojson\n'"
rastervision/data/vector_source/default.py,0,"b'from abc import (ABC, abstractmethod)\nimport os\n\nimport rastervision as rv\n\n\nclass VectorSourceDefaultProvider(ABC):\n    @staticmethod\n    @abstractmethod\n    def handles(s):\n        """"""Returns True of this provider is a default for this string""""""\n        pass\n\n    @abstractmethod\n    def construct(s):\n        """"""Constructs a default VectorSource based on the\n           string.\n        """"""\n        pass\n\n\nclass GeoJSONVectorSourceDefaultProvider(VectorSourceDefaultProvider):\n    @staticmethod\n    def handles(uri):\n        ext = os.path.splitext(uri)[1]\n        return ext.lower() in [\'.json\', \'.geojson\']\n\n    @staticmethod\n    def construct(uri):\n        return rv.VectorSourceConfig.builder(rv.GEOJSON_SOURCE) \\\n                                    .with_uri(uri) \\\n                                    .build()\n\n\nclass VectorTileVectorSourceDefaultProvider(VectorSourceDefaultProvider):\n    @staticmethod\n    def handles(uri):\n        ext = os.path.splitext(uri)[1]\n        return ext.lower() in [\'.pbf\', \'.mvt\']\n\n    @staticmethod\n    def construct(uri):\n        return rv.VectorSourceConfig.builder(rv.VECTOR_TILE_SOURCE) \\\n                                    .with_uri(uri) \\\n                                    .build()\n'"
rastervision/data/vector_source/geojson_vector_source.py,0,"b'import json\n\nfrom rastervision.data.vector_source.vector_source import VectorSource\nfrom rastervision.utils.files import file_to_str\n\n\nclass GeoJSONVectorSource(VectorSource):\n    def __init__(self,\n                 uri,\n                 crs_transformer,\n                 line_bufs=None,\n                 point_bufs=None,\n                 class_inf_opts=None):\n        """"""Constructor.\n\n        Args:\n            uri: (str) uri of GeoJSON file\n            class_inf_opts: ClassInferenceOptions\n        """"""\n        self.uri = uri\n        super().__init__(crs_transformer, line_bufs, point_bufs,\n                         class_inf_opts)\n\n    def _get_geojson(self):\n        geojson = json.loads(file_to_str(self.uri))\n        return self.class_inference.transform_geojson(geojson)\n'"
rastervision/data/vector_source/geojson_vector_source_config.py,0,"b'from copy import deepcopy\n\nimport rastervision as rv\nfrom rastervision.data.vector_source.vector_source_config import (\n    VectorSourceConfig, VectorSourceConfigBuilder)\nfrom rastervision.data.vector_source.geojson_vector_source import GeoJSONVectorSource\nfrom rastervision.data.vector_source.class_inference import ClassInferenceOptions\n\n\nclass GeoJSONVectorSourceConfig(VectorSourceConfig):\n    def __init__(self,\n                 uri,\n                 class_id_to_filter=None,\n                 default_class_id=1,\n                 line_bufs=None,\n                 point_bufs=None):\n        self.uri = uri\n        super().__init__(\n            rv.GEOJSON_SOURCE,\n            class_id_to_filter=class_id_to_filter,\n            default_class_id=default_class_id,\n            line_bufs=line_bufs,\n            point_bufs=point_bufs)\n\n    def to_proto(self):\n        msg = super().to_proto()\n        msg.geojson.uri = self.uri\n        return msg\n\n    def create_source(self, crs_transformer=None, extent=None, class_map=None):\n        return GeoJSONVectorSource(\n            self.uri,\n            crs_transformer,\n            line_bufs=self.line_bufs,\n            point_bufs=self.point_bufs,\n            class_inf_opts=ClassInferenceOptions(\n                class_map=class_map,\n                class_id_to_filter=self.class_id_to_filter,\n                default_class_id=self.default_class_id))\n\n    def report_io(self, command_type, io_def):\n        io_def.add_input(self.uri)\n\n\nclass GeoJSONVectorSourceConfigBuilder(VectorSourceConfigBuilder):\n    def __init__(self, prev=None):\n        config = {}\n        if prev:\n            config = {\n                \'uri\': prev.uri,\n                \'class_id_to_filter\': prev.class_id_to_filter,\n                \'default_class_id\': prev.default_class_id,\n                \'line_bufs\': prev.line_bufs,\n                \'point_bufs\': prev.point_bufs\n            }\n\n        super().__init__(GeoJSONVectorSourceConfig, config)\n\n    def validate(self):\n        if self.config.get(\'uri\') is None:\n            raise rv.ConfigError(\n                \'GeoJSONVectorSourceConfigBuilder requires uri which \'\n                \'can be set using ""with_uri"".\')\n\n        super().validate()\n\n    def from_proto(self, msg):\n        b = super().from_proto(msg)\n        b = b.with_uri(msg.geojson.uri)\n        return b\n\n    def with_uri(self, uri):\n        b = deepcopy(self)\n        b.config[\'uri\'] = uri\n        return b\n'"
rastervision/data/vector_source/vector_source.py,0,"b'from abc import ABC, abstractmethod\n\nfrom shapely.geometry import shape, mapping\nimport shapely\n\nfrom rastervision.data.vector_source.class_inference import (\n    ClassInference, ClassInferenceOptions)\n\n\ndef transform_geojson(geojson,\n                      crs_transformer,\n                      line_bufs=None,\n                      point_bufs=None,\n                      to_map_coords=False):\n    def is_empty_feat(f):\n        # This was added to handle empty geoms which appear when using\n        # OSM vector tiles.\n        return ((not f.get(\'geometry\'))\n                or ((not f[\'geometry\'].get(\'coordinates\')) and\n                    (not f[\'geometry\'].get(\'geometries\'))))\n\n    new_features = []\n    for f in geojson[\'features\']:\n        if is_empty_feat(f):\n            continue\n\n        geom = shape(f[\'geometry\'])\n\n        # Convert map to pixel coords. We need to convert to pixel coords before applying\n        # buffering because those values are assumed to be in pixel units.\n        def m2p(x, y, z=None):\n            return crs_transformer.map_to_pixel((x, y))\n\n        geom = shapely.ops.transform(m2p, geom)\n\n        # Split GeometryCollection into list of geoms.\n        geoms = [geom]\n        if geom.geom_type == \'GeometryCollection\':\n            geoms = list(geom)\n\n        # Split any MultiX to list of X.\n        new_geoms = []\n        for g in geoms:\n            if g.geom_type in [\n                    \'MultiPolygon\', \'MultiPoint\', \'MultiLineString\'\n            ]:\n                new_geoms.extend(list(g))\n            else:\n                new_geoms.append(g)\n        geoms = new_geoms\n\n        # Buffer geoms.\n        class_id = f[\'properties\'][\'class_id\']\n        new_geoms = []\n        for g in geoms:\n            if g.geom_type == \'LineString\':\n                line_buf = 1\n                if line_bufs is not None:\n                    line_buf = line_bufs.get(class_id, 1)\n                # If line_buf for the class_id was explicitly set as None, then\n                # don\'t buffer.\n                if line_buf is not None:\n                    g = g.buffer(line_buf)\n                new_geoms.append(g)\n            elif g.geom_type == \'Point\':\n                point_buf = 1\n                if point_bufs is not None:\n                    point_buf = point_bufs.get(class_id, 1)\n                # If point_buf for the class_id was explicitly set as None, then\n                # don\'t buffer.\n                if point_buf is not None:\n                    g = g.buffer(point_buf)\n                new_geoms.append(g)\n            else:\n                # Use buffer trick to handle self-intersecting polygons. Buffer returns\n                # a MultiPolygon if there is a bowtie, so we have to convert it to a\n                # list of Polygons.\n                poly_buf = g.buffer(0)\n                if poly_buf.geom_type == \'MultiPolygon\':\n                    new_geoms.extend(list(poly_buf))\n                else:\n                    new_geoms.append(poly_buf)\n        geoms = new_geoms\n\n        # Convert back to map coords if desired. This is here so the QGIS plugin can\n        # take the GeoJSON produced by a VectorSource and display it on a map.\n        if to_map_coords:\n\n            def p2m(x, y, z=None):\n                return crs_transformer.pixel_to_map((x, y))\n\n            geoms = [shapely.ops.transform(p2m, g) for g in geoms]\n\n        for g in geoms:\n            new_f = {\n                \'type\': \'Feature\',\n                \'geometry\': mapping(g),\n                \'properties\': f[\'properties\']\n            }\n            # Have to check for empty features again which could have been introduced\n            # when splitting apart multi-geoms.\n            if not is_empty_feat(new_f):\n                new_features.append(new_f)\n\n    return {\'type\': \'FeatureCollection\', \'features\': new_features}\n\n\nclass VectorSource(ABC):\n    """"""A source of vector data.""""""\n\n    def __init__(self,\n                 crs_transformer,\n                 line_bufs=None,\n                 point_bufs=None,\n                 class_inf_opts=None):\n        """"""Constructor.\n\n        Args:\n            crs_transformer: (CRSTransformer)\n            line_bufs: (dict or None) If none, uses default buffer value of 1. Otherwise,\n                a map from class_id to number of pixels to buffer by. If the buffer value\n                is None, then no buffering will be performed and the LineString or Point\n                won\'t get converted to a Polygon. Not converting to Polygon is\n                incompatible with the currently available LabelSources, but may be useful\n                in the future.\n            point_bufs: (dict or None) same as above, but used for buffering Points into\n                Polygons.\n            class_inf_opts: (ClassInferenceOptions)\n        """"""\n        self.crs_transformer = crs_transformer\n        self.line_bufs = line_bufs\n        self.point_bufs = point_bufs\n        if class_inf_opts is None:\n            class_inf_opts = ClassInferenceOptions()\n        self.class_inference = ClassInference(class_inf_opts)\n\n        self.geojson = None\n\n    def get_geojson(self, to_map_coords=False):\n        """"""Return normalized GeoJSON.\n\n        This infers a class_id property for each feature, converts to pixels coords (by\n        default), removes empty features, splits apart multi-geoms and geom collections\n        into single geometries, and buffers lines and points into Polygons.\n\n        Args:\n            to_map_coords: If true, will return GeoJSON in map coordinates.\n\n        Returns:\n            dict in GeoJSON format\n        """"""\n        if self.geojson is None:\n            self.geojson = self._get_geojson()\n\n        geojson = transform_geojson(\n            self.geojson,\n            self.crs_transformer,\n            self.line_bufs,\n            self.point_bufs,\n            to_map_coords=to_map_coords)\n\n        return geojson\n\n    @abstractmethod\n    def _get_geojson(self):\n        """"""Return GeoJSON with class_ids in the properties.""""""\n        pass\n'"
rastervision/data/vector_source/vector_source_config.py,0,"b'from abc import abstractmethod\nfrom copy import deepcopy\nimport numbers\n\nfrom google.protobuf import (json_format)\n\nimport rastervision as rv\nfrom rastervision.core.config import Config, ConfigBuilder\nfrom rastervision.protos.vector_source_pb2 import (VectorSourceConfig as\n                                                   VectorSourceConfigMsg)\n\n\nclass VectorSourceConfig(Config):\n    def __init__(self,\n                 source_type,\n                 class_id_to_filter=None,\n                 default_class_id=1,\n                 line_bufs=None,\n                 point_bufs=None):\n        self.source_type = source_type\n        self.class_id_to_filter = class_id_to_filter\n        self.default_class_id = default_class_id\n        self.line_bufs = line_bufs\n        self.point_bufs = point_bufs\n\n    def has_null_class_bufs(self):\n        if self.point_bufs is not None:\n            for c, v in self.point_bufs.items():\n                if v is None:\n                    return True\n\n        if self.line_bufs is not None:\n            for c, v in self.line_bufs.items():\n                if v is None:\n                    return True\n\n        return False\n\n    def to_proto(self):\n        msg = VectorSourceConfigMsg(source_type=self.source_type)\n\n        if self.class_id_to_filter is not None:\n            # Convert class_ids to str to put into json format.\n            class_id_to_filter = dict(\n                [(str(class_id), filter)\n                 for class_id, filter in self.class_id_to_filter.items()])\n            d = {\'class_id_to_filter\': class_id_to_filter}\n            msg.MergeFrom(json_format.ParseDict(d, VectorSourceConfigMsg()))\n\n        if self.default_class_id is not None:\n            msg.default_class_id = self.default_class_id\n\n        if self.line_bufs is not None:\n            # Convert class_ids to str to put into json format.\n            line_bufs = dict([(str(c), v) for c, v in self.line_bufs.items()])\n            d = {\'line_bufs\': line_bufs}\n            msg.MergeFrom(json_format.ParseDict(d, VectorSourceConfigMsg()))\n\n        if self.point_bufs is not None:\n            # Convert class_ids to str to put into json format.\n            point_bufs = dict(\n                [(str(c), v) for c, v in self.point_bufs.items()])\n            d = {\'point_bufs\': point_bufs}\n            msg.MergeFrom(json_format.ParseDict(d, VectorSourceConfigMsg()))\n\n        return msg\n\n    @staticmethod\n    def builder(source_type):\n        return rv._registry.get_config_builder(rv.VECTOR_SOURCE, source_type)()\n\n    def to_builder(self):\n        return rv._registry.get_config_builder(rv.VECTOR_SOURCE,\n                                               self.source_type)(self)\n\n    @staticmethod\n    def from_proto(msg):\n        """"""Creates a from the specificed protobuf message.\n        """"""\n        return rv._registry.get_config_builder(rv.VECTOR_SOURCE, msg.source_type)() \\\n                           .from_proto(msg) \\\n                           .build()\n\n    @abstractmethod\n    def create_source(self, crs_transformer=None, extent=None, class_map=None):\n        pass\n\n\nclass VectorSourceConfigBuilder(ConfigBuilder):\n    def from_proto(self, msg):\n        b = self\n        d = json_format.MessageToDict(msg)\n\n        class_id_to_filter = None\n        if msg.HasField(\'class_id_to_filter\'):\n            # Convert class_ids from strs to ints.\n            # Have to use camel case after parsing from json :(\n            class_id_to_filter = dict(\n                [(int(class_id), filter)\n                 for class_id, filter in d[\'classIdToFilter\'].items()])\n\n        default_class_id = None\n        if msg.HasField(\'default_class_id\'):\n            default_class_id = msg.default_class_id\n\n        b = b.with_class_inference(\n            class_id_to_filter=class_id_to_filter,\n            default_class_id=default_class_id)\n\n        line_bufs = msg.line_bufs if msg.HasField(\'line_bufs\') else None\n        point_bufs = msg.point_bufs if msg.HasField(\'point_bufs\') else None\n        b = b.with_buffers(line_bufs, point_bufs)\n\n        return b\n\n    def validate(self):\n        def _validate(bufs):\n            if bufs is not None:\n                for c, v in bufs.items():\n                    if not (v is None or isinstance(v, numbers.Number)):\n                        raise rv.ConfigError(\n                            \'Buffer size {} must be a number or None.\'.format(\n                                v))\n\n        _validate(self.config.get(\'point_bufs\'))\n        _validate(self.config.get(\'line_bufs\'))\n        super().validate()\n\n    def with_class_inference(self, class_id_to_filter=None,\n                             default_class_id=1):\n        """"""Set options for inferring the class of each feature.\n\n        For more info on how class inference works, see ClassInference.infer_class()\n\n        Args:\n            class_id_to_filter: (dict) map from class_id to JSON filter.\n                The filter schema is according to\n                https://github.com/mapbox/mapbox-gl-js/blob/c9900db279db776f493ce8b6749966cedc2d6b8a/src/style-spec/feature_filter/index.js  # noqa\n            default_class_id: (int) the default class_id to use if class can\'t be\n                inferred using other mechanisms. If a feature defaults to a class_id of\n                None, then that feature will be deleted.\n        """"""\n        b = deepcopy(self)\n        # Ensure class_ids are ints.\n        if class_id_to_filter is not None:\n            class_id_to_filter = dict(\n                [(int(c), f) for c, f in class_id_to_filter.items()])\n        b.config[\'class_id_to_filter\'] = class_id_to_filter\n        b.config[\'default_class_id\'] = default_class_id\n        return b\n\n    def with_buffers(self, line_bufs=None, point_bufs=None):\n        """"""Set options for buffering lines and points into polygons.\n\n        For example, this is useful for buffering lines representing roads so that\n        their width roughly matches the width of roads in the imagery.\n\n        Args:\n            line_bufs: (dict or None) If none, uses default buffer value of 1.\n                Otherwise, a map from class_id to number of pixels to buffer by.\n                If the buffer value is None, then no buffering will be performed\n                and the LineString or Point won\'t get converted to a Polygon.\n                Not converting to Polygon is incompatible with the currently\n                available LabelSources, but may be useful in the future.\n            point_bufs: (dict or None) same as above, but used for buffering\n                Points into Polygons.\n        """"""\n        b = deepcopy(self)\n\n        if line_bufs is not None:\n            line_bufs = dict([(int(c), v) for c, v in line_bufs.items()])\n        b.config[\'line_bufs\'] = line_bufs\n\n        if point_bufs is not None:\n            point_bufs = dict([(int(c), v) for c, v in point_bufs.items()])\n        b.config[\'point_bufs\'] = point_bufs\n\n        return b\n'"
rastervision/data/vector_source/vector_tile_vector_source.py,0,"b'import json\nimport logging\nimport copy\nfrom subprocess import check_output\nimport os\nfrom functools import lru_cache\n\nfrom supermercado.burntiles import burn\nfrom shapely.geometry import shape, mapping\nfrom shapely.ops import cascaded_union\n\nfrom rastervision.data.vector_source.vector_source import VectorSource\nfrom rastervision.utils.files import get_cached_file\nfrom rastervision.rv_config import RVConfig\n\nlog = logging.getLogger(__name__)\n\n\ndef merge_geojson(geojson, id_field):\n    """"""Merge features that share an id.\n\n    Args:\n        geojson: (dict) GeoJSON with features that have an id property used to merge\n            features that are split across multiple tiles.\n        id_field: (str) name of field in feature[\'properties\'] that contains the\n            feature\'s unique id. Used for merging features that are split across\n            tile boundaries.\n    """"""\n    id_to_features = {}\n    for f in geojson[\'features\']:\n        id = f[\'properties\'][id_field]\n        id_features = id_to_features.get(id, [])\n        id_features.append(f)\n        id_to_features[id] = id_features\n\n    proc_features = []\n    for id, id_features in id_to_features.items():\n        id_geoms = []\n        for f in id_features:\n            g = shape(f[\'geometry\'])\n            # Self-intersection trick.\n            if f[\'geometry\'][\'type\'] in [\'Polygon\', \'MultiPolygon\']:\n                g = g.buffer(0)\n            id_geoms.append(g)\n\n        union_geom = cascaded_union(id_geoms)\n        union_feature = copy.deepcopy(id_features[0])\n        union_feature[\'geometry\'] = mapping(union_geom)\n        proc_features.append(union_feature)\n\n    return {\'type\': \'FeatureCollection\', \'features\': proc_features}\n\n\n@lru_cache(maxsize=32)\ndef get_tile_features(tile_uri, z, x, y):\n    """"""Get GeoJSON features for a specific tile using in-memory caching.""""""\n    cache_dir = os.path.join(RVConfig.get_tmp_dir_root(), \'vector-tiles\')\n    tile_path = get_cached_file(cache_dir, tile_uri)\n    cmd = [\'tippecanoe-decode\', \'-f\', \'-c\', tile_path, str(z), str(x), str(y)]\n    log.debug(\'Running command: {}\'.format(\' \'.join(cmd)))\n    tile_geojson_str = check_output(cmd).decode(\'utf-8\')\n    tile_features = [\n        json.loads(ts) for ts in tile_geojson_str.split(\'\\n\')\n        if ts.startswith(\'{\')\n    ]\n\n    return tile_features\n\n\ndef vector_tile_to_geojson(uri, zoom, map_extent):\n    """"""Get GeoJSON features that overlap with an extent from a vector tile endpoint.""""""\n    log.info(\'Downloading and converting vector tiles to GeoJSON...\')\n\n    # Figure out which tiles cover the extent.\n    extent_polys = [{\n        \'type\': \'Feature\',\n        \'properties\': {},\n        \'geometry\': {\n            \'type\': \'Polygon\',\n            \'coordinates\': [map_extent.geojson_coordinates()]\n        }\n    }]\n    xyzs = burn(extent_polys, zoom)\n\n    # Retrieve tile features.\n    features = []\n    for xyz in xyzs:\n        x, y, z = xyz\n        # If this isn\'t a zxy schema, this is a no-op.\n        tile_uri = uri.format(x=x, y=y, z=z)\n        tile_features = get_tile_features(tile_uri, z, x, y)\n        features.extend(tile_features)\n\n    # Crop features to extent\n    extent_geom = map_extent.to_shapely()\n    cropped_features = []\n    for f in features:\n        geom = shape(f[\'geometry\'])\n        if f[\'geometry\'][\'type\'].lower() in [\'polygon\', \'multipolygon\']:\n            geom = geom.buffer(0)\n        geom = geom.intersection(extent_geom)\n        if not geom.is_empty:\n            f = dict(f)\n            f[\'geometry\'] = mapping(geom)\n            cropped_features.append(f)\n\n    return {\'type\': \'FeatureCollection\', \'features\': cropped_features}\n\n\nclass VectorTileVectorSource(VectorSource):\n    def __init__(self,\n                 uri,\n                 zoom,\n                 id_field,\n                 crs_transformer,\n                 extent,\n                 line_bufs=None,\n                 point_bufs=None,\n                 class_inf_opts=None):\n        """"""Constructor.\n\n        Args:\n            uri: (str) URI of vector tile endpoint. Should either contain {z}/{x}/{y} or\n                point to .mbtiles file.\n            zoom: (int) valid zoom level to use when fetching tiles from endpoint\n            id_field: (str) name of field in feature[\'properties\'] that contains the\n                feature\'s unique id. Used for merging features that are split across\n                tile boundaries.\n            crs_transformer: (CRSTransformer)\n            extent: (Box) extent of scene which determines which features to return\n            class_inf_opts: (ClassInferenceOptions)\n        """"""\n        self.uri = uri\n        self.zoom = zoom\n        self.id_field = id_field\n        self.extent = extent\n        super().__init__(crs_transformer, line_bufs, point_bufs,\n                         class_inf_opts)\n\n    def _get_geojson(self):\n        # This attempts to do things in an efficient order. First, we extract raw\n        # GeoJSON from the vector tiles covering the extent. This uses caching, so that\n        # we never have to process the same vector tile twice (even across scenes). Then,\n        # we infer class ids, which drops any irrelevant features which should speed up\n        # the next phase which merges features that are split across tiles.\n        map_extent = self.extent.reproject(\n            lambda point: self.crs_transformer.pixel_to_map(point))\n        log.debug(\n            \'Reading and converting vector tiles to GeoJSON for extent...\')\n        geojson = vector_tile_to_geojson(self.uri, self.zoom, map_extent)\n        log.debug(\'Inferring class_ids...\')\n        geojson = self.class_inference.transform_geojson(geojson)\n        log.debug(\'Merging GeoJSON features...\')\n        geojson = merge_geojson(geojson, self.id_field)\n        return geojson\n'"
rastervision/data/vector_source/vector_tile_vector_source_config.py,0,"b'from copy import deepcopy\n\nimport rastervision as rv\nfrom rastervision.data.vector_source.vector_source_config import (\n    VectorSourceConfig, VectorSourceConfigBuilder)\nfrom rastervision.data.vector_source.vector_tile_vector_source import (\n    VectorTileVectorSource)\nfrom rastervision.data.vector_source.class_inference import ClassInferenceOptions\n\n\nclass VectorTileVectorSourceConfig(VectorSourceConfig):\n    def __init__(self,\n                 uri,\n                 zoom,\n                 id_field,\n                 class_id_to_filter=None,\n                 default_class_id=1,\n                 line_bufs=None,\n                 point_bufs=None):\n        self.uri = uri\n        self.zoom = zoom\n        self.id_field = id_field\n        super().__init__(\n            rv.VECTOR_TILE_SOURCE,\n            class_id_to_filter=class_id_to_filter,\n            default_class_id=default_class_id,\n            line_bufs=line_bufs,\n            point_bufs=point_bufs)\n\n    def to_proto(self):\n        msg = super().to_proto()\n        msg.mbtiles.uri = self.uri\n        msg.mbtiles.zoom = self.zoom\n        msg.mbtiles.id_field = self.id_field\n        return msg\n\n    def create_source(self, crs_transformer=None, extent=None, class_map=None):\n        return VectorTileVectorSource(\n            self.uri,\n            self.zoom,\n            self.id_field,\n            crs_transformer,\n            extent,\n            line_bufs=self.line_bufs,\n            point_bufs=self.point_bufs,\n            class_inf_opts=ClassInferenceOptions(\n                class_map=class_map,\n                class_id_to_filter=self.class_id_to_filter,\n                default_class_id=self.default_class_id))\n\n    def report_io(self, command_type, io_def):\n        # We shouldn\'t include self.uri as an input because it is just a URI schema\n        # and the file checker will raise an error if it\'s included.\n        pass\n\n\nclass VectorTileVectorSourceConfigBuilder(VectorSourceConfigBuilder):\n    def __init__(self, prev=None):\n        config = {}\n        if prev:\n            config = {\n                \'uri\': prev.uri,\n                \'zoom\': prev.zoom,\n                \'id_field\': prev.id_field,\n                \'class_id_to_filter\': prev.class_id_to_filter,\n                \'default_class_id\': prev.default_class_id,\n                \'line_bufs\': prev.line_bufs,\n                \'point_bufs\': prev.point_bufs\n            }\n\n        super().__init__(VectorTileVectorSourceConfig, config)\n\n    def validate(self):\n        if self.config.get(\'uri\') is None:\n            raise rv.ConfigError(\n                \'VectorTileVectorSourceConfigBuilder requires uri which \'\n                \'can be set using ""with_uri"".\')\n\n        if self.config.get(\'zoom\') is None:\n            raise rv.ConfigError(\n                \'VectorTileVectorSourceConfigBuilder requires zoom which \'\n                \'can be set using ""with_zoom"".\')\n\n        # If not set explicitly, set it using default value.\n        if self.config.get(\'id_field\') is None:\n            self.with_id_field()\n\n        super().validate()\n\n    def from_proto(self, msg):\n        b = super().from_proto(msg)\n        b = b.with_uri(msg.mbtiles.uri)\n        b = b.with_zoom(msg.mbtiles.zoom)\n        b = b.with_id_field(msg.mbtiles.id_field)\n        return b\n\n    def with_uri(self, uri):\n        """"""Set the URI of the vector tiles.\n\n        Args:\n            uri: (str) URI of vector tile endpoint. Should either contain\n                {z}/{x}/{y} or point to .mbtiles file.\n        """"""\n        b = deepcopy(self)\n        b.config[\'uri\'] = uri\n        return b\n\n    def with_zoom(self, zoom):\n        """"""Set the zoom level to use when accessing vector tiles.\n\n        Note: the vector tiles need to support the zoom level. Typically only\n        a subset of zoom levels are supported.\n        """"""\n        b = deepcopy(self)\n        b.config[\'zoom\'] = zoom\n        return b\n\n    def with_id_field(self, id_field=\'@id\'):\n        """"""Set the name of the id field.\n\n        Args:\n            id_field: (str) name of field in feature[\'properties\'] that\n                contains the feature\'s unique id. Used for merging features\n                that are split across tile boundaries.\n        """"""\n        b = deepcopy(self)\n        b.config[\'id_field\'] = id_field\n        return b\n'"
rastervision/protos/deeplab/__init__.py,0,b''
rastervision/protos/deeplab/train_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: rastervision/protos/deeplab/train.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'rastervision/protos/deeplab/train.proto\',\n  package=\'deeplab.protos\',\n  syntax=\'proto2\',\n  serialized_pb=_b(\'\\n\\\'rastervision/protos/deeplab/train.proto\\x12\\x0e\\x64\\x65\\x65plab.protos\\""\\xeb\\x07\\n\\x12TrainingParameters\\x12\\x1d\\n\\x15initialize_last_layer\\x18\\x01 \\x01(\\t\\x12\\\'\\n\\x1flast_layers_contain_logits_only\\x18\\x02 \\x01(\\t\\x12\\x17\\n\\x0fupsample_logits\\x18\\x03 \\x01(\\t\\x12\\x1c\\n\\x14\\x66ine_tune_batch_norm\\x18\\x04 \\x01(\\t\\x12\\x1a\\n\\x12\\x62\\x61se_learning_rate\\x18\\x05 \\x01(\\t\\x12&\\n\\x1elast_layer_gradient_multiplier\\x18\\x06 \\x01(\\t\\x12\\x16\\n\\x0elearning_power\\x18\\x07 \\x01(\\t\\x12\\""\\n\\x1alearning_rate_decay_factor\\x18\\x08 \\x01(\\t\\x12\\x10\\n\\x08momentum\\x18\\t \\x01(\\t\\x12 \\n\\x18slow_start_learning_rate\\x18\\n \\x01(\\t\\x12\\x14\\n\\x0cweight_decay\\x18\\x0b \\x01(\\t\\x12 \\n\\x18learning_rate_decay_step\\x18\\x0c \\x01(\\t\\x12\\x17\\n\\x0fslow_start_step\\x18\\r \\x01(\\t\\x12\\x17\\n\\x0flearning_policy\\x18\\x0e \\x01(\\t\\x12\\x18\\n\\x10max_scale_factor\\x18\\x0f \\x01(\\t\\x12\\x18\\n\\x10min_scale_factor\\x18\\x10 \\x01(\\t\\x12\\x1e\\n\\x16scale_factor_step_size\\x18\\x11 \\x01(\\t\\x12\\x1d\\n\\x15\\x64\\x65\\x63oder_output_stride\\x18\\x12 \\x02(\\x05\\x12\\x15\\n\\routput_stride\\x18\\x13 \\x02(\\x05\\x12\\x15\\n\\rmodel_variant\\x18\\x14 \\x02(\\t\\x12\\x14\\n\\x0c\\x61trous_rates\\x18\\x15 \\x03(\\x05\\x12\\x1a\\n\\x0btrain_split\\x18\\x16 \\x01(\\t:\\x05train\\x12\\x1e\\n\\neval_split\\x18! \\x01(\\t:\\nvalidation\\x12\\x17\\n\\x07\\x64\\x61taset\\x18\\x17 \\x01(\\t:\\x06\\x63ustom\\x12\\x18\\n\\x10train_batch_size\\x18\\x18 \\x02(\\x05\\x12 \\n\\x18training_number_of_steps\\x18\\x19 \\x02(\\x05\\x12\\x17\\n\\x0ftrain_crop_size\\x18\\x1a \\x03(\\x05\\x12\\x16\\n\\x0e\\x65val_crop_size\\x18\\"" \\x03(\\x05\\x12\\x1d\\n\\x0f\\x64l_custom_train\\x18\\x1b \\x01(\\x05:\\x04\\x31\\x33\\x33\\x33\\x12\\""\\n\\x14\\x64l_custom_validation\\x18\\x1c \\x01(\\x05:\\x04\\x31\\x33\\x33\\x33\\x12#\\n\\x15save_summaries_images\\x18\\x1d \\x01(\\x08:\\x04true\\x12\\x1f\\n\\x12save_interval_secs\\x18\\x1e \\x01(\\x05:\\x03\\x36\\x30\\x30\\x12\\x1f\\n\\x13save_summaries_secs\\x18\\x1f \\x01(\\x05:\\x02\\x36\\x30\\x12\\x15\\n\\nnum_clones\\x18  \\x01(\\x05:\\x01\\x31\')\n)\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\n\n\n\n_TRAININGPARAMETERS = _descriptor.Descriptor(\n  name=\'TrainingParameters\',\n  full_name=\'deeplab.protos.TrainingParameters\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'initialize_last_layer\', full_name=\'deeplab.protos.TrainingParameters.initialize_last_layer\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'last_layers_contain_logits_only\', full_name=\'deeplab.protos.TrainingParameters.last_layers_contain_logits_only\', index=1,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'upsample_logits\', full_name=\'deeplab.protos.TrainingParameters.upsample_logits\', index=2,\n      number=3, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'fine_tune_batch_norm\', full_name=\'deeplab.protos.TrainingParameters.fine_tune_batch_norm\', index=3,\n      number=4, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'base_learning_rate\', full_name=\'deeplab.protos.TrainingParameters.base_learning_rate\', index=4,\n      number=5, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'last_layer_gradient_multiplier\', full_name=\'deeplab.protos.TrainingParameters.last_layer_gradient_multiplier\', index=5,\n      number=6, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'learning_power\', full_name=\'deeplab.protos.TrainingParameters.learning_power\', index=6,\n      number=7, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'learning_rate_decay_factor\', full_name=\'deeplab.protos.TrainingParameters.learning_rate_decay_factor\', index=7,\n      number=8, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'momentum\', full_name=\'deeplab.protos.TrainingParameters.momentum\', index=8,\n      number=9, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'slow_start_learning_rate\', full_name=\'deeplab.protos.TrainingParameters.slow_start_learning_rate\', index=9,\n      number=10, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'weight_decay\', full_name=\'deeplab.protos.TrainingParameters.weight_decay\', index=10,\n      number=11, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'learning_rate_decay_step\', full_name=\'deeplab.protos.TrainingParameters.learning_rate_decay_step\', index=11,\n      number=12, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'slow_start_step\', full_name=\'deeplab.protos.TrainingParameters.slow_start_step\', index=12,\n      number=13, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'learning_policy\', full_name=\'deeplab.protos.TrainingParameters.learning_policy\', index=13,\n      number=14, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'max_scale_factor\', full_name=\'deeplab.protos.TrainingParameters.max_scale_factor\', index=14,\n      number=15, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'min_scale_factor\', full_name=\'deeplab.protos.TrainingParameters.min_scale_factor\', index=15,\n      number=16, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'scale_factor_step_size\', full_name=\'deeplab.protos.TrainingParameters.scale_factor_step_size\', index=16,\n      number=17, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'decoder_output_stride\', full_name=\'deeplab.protos.TrainingParameters.decoder_output_stride\', index=17,\n      number=18, type=5, cpp_type=1, label=2,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'output_stride\', full_name=\'deeplab.protos.TrainingParameters.output_stride\', index=18,\n      number=19, type=5, cpp_type=1, label=2,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'model_variant\', full_name=\'deeplab.protos.TrainingParameters.model_variant\', index=19,\n      number=20, type=9, cpp_type=9, label=2,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'atrous_rates\', full_name=\'deeplab.protos.TrainingParameters.atrous_rates\', index=20,\n      number=21, type=5, cpp_type=1, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'train_split\', full_name=\'deeplab.protos.TrainingParameters.train_split\', index=21,\n      number=22, type=9, cpp_type=9, label=1,\n      has_default_value=True, default_value=_b(""train"").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'eval_split\', full_name=\'deeplab.protos.TrainingParameters.eval_split\', index=22,\n      number=33, type=9, cpp_type=9, label=1,\n      has_default_value=True, default_value=_b(""validation"").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'dataset\', full_name=\'deeplab.protos.TrainingParameters.dataset\', index=23,\n      number=23, type=9, cpp_type=9, label=1,\n      has_default_value=True, default_value=_b(""custom"").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'train_batch_size\', full_name=\'deeplab.protos.TrainingParameters.train_batch_size\', index=24,\n      number=24, type=5, cpp_type=1, label=2,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'training_number_of_steps\', full_name=\'deeplab.protos.TrainingParameters.training_number_of_steps\', index=25,\n      number=25, type=5, cpp_type=1, label=2,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'train_crop_size\', full_name=\'deeplab.protos.TrainingParameters.train_crop_size\', index=26,\n      number=26, type=5, cpp_type=1, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'eval_crop_size\', full_name=\'deeplab.protos.TrainingParameters.eval_crop_size\', index=27,\n      number=34, type=5, cpp_type=1, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'dl_custom_train\', full_name=\'deeplab.protos.TrainingParameters.dl_custom_train\', index=28,\n      number=27, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=1333,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'dl_custom_validation\', full_name=\'deeplab.protos.TrainingParameters.dl_custom_validation\', index=29,\n      number=28, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=1333,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'save_summaries_images\', full_name=\'deeplab.protos.TrainingParameters.save_summaries_images\', index=30,\n      number=29, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=True,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'save_interval_secs\', full_name=\'deeplab.protos.TrainingParameters.save_interval_secs\', index=31,\n      number=30, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=600,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'save_summaries_secs\', full_name=\'deeplab.protos.TrainingParameters.save_summaries_secs\', index=32,\n      number=31, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=60,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'num_clones\', full_name=\'deeplab.protos.TrainingParameters.num_clones\', index=33,\n      number=32, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=60,\n  serialized_end=1063,\n)\n\nDESCRIPTOR.message_types_by_name[\'TrainingParameters\'] = _TRAININGPARAMETERS\n\nTrainingParameters = _reflection.GeneratedProtocolMessageType(\'TrainingParameters\', (_message.Message,), dict(\n  DESCRIPTOR = _TRAININGPARAMETERS,\n  __module__ = \'rastervision.protos.deeplab.train_pb2\'\n  # @@protoc_insertion_point(class_scope:deeplab.protos.TrainingParameters)\n  ))\n_sym_db.RegisterMessage(TrainingParameters)\n\n\n# @@protoc_insertion_point(module_scope)\n'"
rastervision/protos/keras_classification/__init__.py,0,b''
rastervision/protos/keras_classification/model_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: rastervision/protos/keras_classification/model.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'rastervision/protos/keras_classification/model.proto\',\n  package=\'keras_classification.protos\',\n  syntax=\'proto2\',\n  serialized_pb=_b(\'\\n4rastervision/protos/keras_classification/model.proto\\x12\\x1bkeras_classification.protos\\""\\xb5\\x01\\n\\x05Model\\x12\\x12\\n\\ninput_size\\x18\\x01 \\x02(\\x05\\x12\\x12\\n\\nnb_classes\\x18\\x02 \\x02(\\x05\\x12\\x35\\n\\x04type\\x18\\x03 \\x02(\\x0e\\x32\\\'.keras_classification.protos.Model.Type\\x12\\x12\\n\\nmodel_path\\x18\\x04 \\x02(\\t\\x12#\\n\\x14load_weights_by_name\\x18\\x05 \\x01(\\x08:\\x05\\x66\\x61lse\\""\\x14\\n\\x04Type\\x12\\x0c\\n\\x08RESNET50\\x10\\x01\')\n)\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\n\n\n_MODEL_TYPE = _descriptor.EnumDescriptor(\n  name=\'Type\',\n  full_name=\'keras_classification.protos.Model.Type\',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name=\'RESNET50\', index=0, number=1,\n      options=None,\n      type=None),\n  ],\n  containing_type=None,\n  options=None,\n  serialized_start=247,\n  serialized_end=267,\n)\n_sym_db.RegisterEnumDescriptor(_MODEL_TYPE)\n\n\n_MODEL = _descriptor.Descriptor(\n  name=\'Model\',\n  full_name=\'keras_classification.protos.Model\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'input_size\', full_name=\'keras_classification.protos.Model.input_size\', index=0,\n      number=1, type=5, cpp_type=1, label=2,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'nb_classes\', full_name=\'keras_classification.protos.Model.nb_classes\', index=1,\n      number=2, type=5, cpp_type=1, label=2,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'type\', full_name=\'keras_classification.protos.Model.type\', index=2,\n      number=3, type=14, cpp_type=8, label=2,\n      has_default_value=False, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'model_path\', full_name=\'keras_classification.protos.Model.model_path\', index=3,\n      number=4, type=9, cpp_type=9, label=2,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'load_weights_by_name\', full_name=\'keras_classification.protos.Model.load_weights_by_name\', index=4,\n      number=5, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n    _MODEL_TYPE,\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=86,\n  serialized_end=267,\n)\n\n_MODEL.fields_by_name[\'type\'].enum_type = _MODEL_TYPE\n_MODEL_TYPE.containing_type = _MODEL\nDESCRIPTOR.message_types_by_name[\'Model\'] = _MODEL\n\nModel = _reflection.GeneratedProtocolMessageType(\'Model\', (_message.Message,), dict(\n  DESCRIPTOR = _MODEL,\n  __module__ = \'rastervision.protos.keras_classification.model_pb2\'\n  # @@protoc_insertion_point(class_scope:keras_classification.protos.Model)\n  ))\n_sym_db.RegisterMessage(Model)\n\n\n# @@protoc_insertion_point(module_scope)\n'"
rastervision/protos/keras_classification/optimizer_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: rastervision/protos/keras_classification/optimizer.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'rastervision/protos/keras_classification/optimizer.proto\',\n  package=\'keras_classification.protos\',\n  syntax=\'proto2\',\n  serialized_pb=_b(\'\\n8rastervision/protos/keras_classification/optimizer.proto\\x12\\x1bkeras_classification.protos\\""r\\n\\tOptimizer\\x12\\x39\\n\\x04type\\x18\\x04 \\x02(\\x0e\\x32+.keras_classification.protos.Optimizer.Type\\x12\\x0f\\n\\x07init_lr\\x18\\x05 \\x02(\\x02\\""\\x19\\n\\x04Type\\x12\\x08\\n\\x04\\x41\\x44\\x41M\\x10\\x01\\x12\\x07\\n\\x03SGD\\x10\\x02\')\n)\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\n\n\n_OPTIMIZER_TYPE = _descriptor.EnumDescriptor(\n  name=\'Type\',\n  full_name=\'keras_classification.protos.Optimizer.Type\',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name=\'ADAM\', index=0, number=1,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'SGD\', index=1, number=2,\n      options=None,\n      type=None),\n  ],\n  containing_type=None,\n  options=None,\n  serialized_start=178,\n  serialized_end=203,\n)\n_sym_db.RegisterEnumDescriptor(_OPTIMIZER_TYPE)\n\n\n_OPTIMIZER = _descriptor.Descriptor(\n  name=\'Optimizer\',\n  full_name=\'keras_classification.protos.Optimizer\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'type\', full_name=\'keras_classification.protos.Optimizer.type\', index=0,\n      number=4, type=14, cpp_type=8, label=2,\n      has_default_value=False, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'init_lr\', full_name=\'keras_classification.protos.Optimizer.init_lr\', index=1,\n      number=5, type=2, cpp_type=6, label=2,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n    _OPTIMIZER_TYPE,\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=89,\n  serialized_end=203,\n)\n\n_OPTIMIZER.fields_by_name[\'type\'].enum_type = _OPTIMIZER_TYPE\n_OPTIMIZER_TYPE.containing_type = _OPTIMIZER\nDESCRIPTOR.message_types_by_name[\'Optimizer\'] = _OPTIMIZER\n\nOptimizer = _reflection.GeneratedProtocolMessageType(\'Optimizer\', (_message.Message,), dict(\n  DESCRIPTOR = _OPTIMIZER,\n  __module__ = \'rastervision.protos.keras_classification.optimizer_pb2\'\n  # @@protoc_insertion_point(class_scope:keras_classification.protos.Optimizer)\n  ))\n_sym_db.RegisterMessage(Optimizer)\n\n\n# @@protoc_insertion_point(module_scope)\n'"
rastervision/protos/keras_classification/pipeline_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: rastervision/protos/keras_classification/pipeline.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom rastervision.protos.keras_classification import model_pb2 as rastervision_dot_protos_dot_keras__classification_dot_model__pb2\nfrom rastervision.protos.keras_classification import trainer_pb2 as rastervision_dot_protos_dot_keras__classification_dot_trainer__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'rastervision/protos/keras_classification/pipeline.proto\',\n  package=\'keras_classification.protos\',\n  syntax=\'proto2\',\n  serialized_pb=_b(\'\\n7rastervision/protos/keras_classification/pipeline.proto\\x12\\x1bkeras_classification.protos\\x1a\\x34rastervision/protos/keras_classification/model.proto\\x1a\\x36rastervision/protos/keras_classification/trainer.proto\\""z\\n\\x0ePipelineConfig\\x12\\x31\\n\\x05model\\x18\\x01 \\x02(\\x0b\\x32\\"".keras_classification.protos.Model\\x12\\x35\\n\\x07trainer\\x18\\x02 \\x02(\\x0b\\x32$.keras_classification.protos.Trainer\')\n  ,\n  dependencies=[rastervision_dot_protos_dot_keras__classification_dot_model__pb2.DESCRIPTOR,rastervision_dot_protos_dot_keras__classification_dot_trainer__pb2.DESCRIPTOR,])\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\n\n\n\n_PIPELINECONFIG = _descriptor.Descriptor(\n  name=\'PipelineConfig\',\n  full_name=\'keras_classification.protos.PipelineConfig\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'model\', full_name=\'keras_classification.protos.PipelineConfig.model\', index=0,\n      number=1, type=11, cpp_type=10, label=2,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'trainer\', full_name=\'keras_classification.protos.PipelineConfig.trainer\', index=1,\n      number=2, type=11, cpp_type=10, label=2,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=198,\n  serialized_end=320,\n)\n\n_PIPELINECONFIG.fields_by_name[\'model\'].message_type = rastervision_dot_protos_dot_keras__classification_dot_model__pb2._MODEL\n_PIPELINECONFIG.fields_by_name[\'trainer\'].message_type = rastervision_dot_protos_dot_keras__classification_dot_trainer__pb2._TRAINER\nDESCRIPTOR.message_types_by_name[\'PipelineConfig\'] = _PIPELINECONFIG\n\nPipelineConfig = _reflection.GeneratedProtocolMessageType(\'PipelineConfig\', (_message.Message,), dict(\n  DESCRIPTOR = _PIPELINECONFIG,\n  __module__ = \'rastervision.protos.keras_classification.pipeline_pb2\'\n  # @@protoc_insertion_point(class_scope:keras_classification.protos.PipelineConfig)\n  ))\n_sym_db.RegisterMessage(PipelineConfig)\n\n\n# @@protoc_insertion_point(module_scope)\n'"
rastervision/protos/keras_classification/trainer_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: rastervision/protos/keras_classification/trainer.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom rastervision.protos.keras_classification import optimizer_pb2 as rastervision_dot_protos_dot_keras__classification_dot_optimizer__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'rastervision/protos/keras_classification/trainer.proto\',\n  package=\'keras_classification.protos\',\n  syntax=\'proto2\',\n  serialized_pb=_b(\'\\n6rastervision/protos/keras_classification/trainer.proto\\x12\\x1bkeras_classification.protos\\x1a\\x38rastervision/protos/keras_classification/optimizer.proto\\""\\xe6\\x03\\n\\x07Trainer\\x12\\x39\\n\\toptimizer\\x18\\x01 \\x02(\\x0b\\x32&.keras_classification.protos.Optimizer\\x12=\\n\\x07options\\x18\\x02 \\x02(\\x0b\\x32,.keras_classification.protos.Trainer.Options\\x1a+\\n\\x0eLRScheduleItem\\x12\\r\\n\\x05\\x65poch\\x18\\x01 \\x02(\\x05\\x12\\n\\n\\x02lr\\x18\\x02 \\x02(\\x02\\x1a\\xb3\\x02\\n\\x07Options\\x12\\x19\\n\\x11training_data_dir\\x18\\x01 \\x02(\\t\\x12\\x1b\\n\\x13validation_data_dir\\x18\\x02 \\x02(\\t\\x12\\x11\\n\\tnb_epochs\\x18\\x04 \\x02(\\x05\\x12H\\n\\x0blr_schedule\\x18\\n \\x03(\\x0b\\x32\\x33.keras_classification.protos.Trainer.LRScheduleItem\\x12\\x12\\n\\nbatch_size\\x18\\x05 \\x02(\\x05\\x12\\x12\\n\\ninput_size\\x18\\x06 \\x02(\\x05\\x12\\x12\\n\\noutput_dir\\x18\\x07 \\x02(\\t\\x12\\x13\\n\\x0b\\x63lass_names\\x18\\x08 \\x03(\\t\\x12\\x1a\\n\\x0bshort_epoch\\x18\\t \\x02(\\x08:\\x05\\x66\\x61lse\\x12\\r\\n\\x05\\x64\\x65\\x62ug\\x18\\x0b \\x02(\\x08\\x12\\x17\\n\\tsave_best\\x18\\x0c \\x02(\\x08:\\x04true\')\n  ,\n  dependencies=[rastervision_dot_protos_dot_keras__classification_dot_optimizer__pb2.DESCRIPTOR,])\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\n\n\n\n_TRAINER_LRSCHEDULEITEM = _descriptor.Descriptor(\n  name=\'LRScheduleItem\',\n  full_name=\'keras_classification.protos.Trainer.LRScheduleItem\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'epoch\', full_name=\'keras_classification.protos.Trainer.LRScheduleItem.epoch\', index=0,\n      number=1, type=5, cpp_type=1, label=2,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'lr\', full_name=\'keras_classification.protos.Trainer.LRScheduleItem.lr\', index=1,\n      number=2, type=2, cpp_type=6, label=2,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=279,\n  serialized_end=322,\n)\n\n_TRAINER_OPTIONS = _descriptor.Descriptor(\n  name=\'Options\',\n  full_name=\'keras_classification.protos.Trainer.Options\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'training_data_dir\', full_name=\'keras_classification.protos.Trainer.Options.training_data_dir\', index=0,\n      number=1, type=9, cpp_type=9, label=2,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'validation_data_dir\', full_name=\'keras_classification.protos.Trainer.Options.validation_data_dir\', index=1,\n      number=2, type=9, cpp_type=9, label=2,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'nb_epochs\', full_name=\'keras_classification.protos.Trainer.Options.nb_epochs\', index=2,\n      number=4, type=5, cpp_type=1, label=2,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'lr_schedule\', full_name=\'keras_classification.protos.Trainer.Options.lr_schedule\', index=3,\n      number=10, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'batch_size\', full_name=\'keras_classification.protos.Trainer.Options.batch_size\', index=4,\n      number=5, type=5, cpp_type=1, label=2,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'input_size\', full_name=\'keras_classification.protos.Trainer.Options.input_size\', index=5,\n      number=6, type=5, cpp_type=1, label=2,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'output_dir\', full_name=\'keras_classification.protos.Trainer.Options.output_dir\', index=6,\n      number=7, type=9, cpp_type=9, label=2,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'class_names\', full_name=\'keras_classification.protos.Trainer.Options.class_names\', index=7,\n      number=8, type=9, cpp_type=9, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'short_epoch\', full_name=\'keras_classification.protos.Trainer.Options.short_epoch\', index=8,\n      number=9, type=8, cpp_type=7, label=2,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'debug\', full_name=\'keras_classification.protos.Trainer.Options.debug\', index=9,\n      number=11, type=8, cpp_type=7, label=2,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'save_best\', full_name=\'keras_classification.protos.Trainer.Options.save_best\', index=10,\n      number=12, type=8, cpp_type=7, label=2,\n      has_default_value=True, default_value=True,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=325,\n  serialized_end=632,\n)\n\n_TRAINER = _descriptor.Descriptor(\n  name=\'Trainer\',\n  full_name=\'keras_classification.protos.Trainer\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'optimizer\', full_name=\'keras_classification.protos.Trainer.optimizer\', index=0,\n      number=1, type=11, cpp_type=10, label=2,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'options\', full_name=\'keras_classification.protos.Trainer.options\', index=1,\n      number=2, type=11, cpp_type=10, label=2,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[_TRAINER_LRSCHEDULEITEM, _TRAINER_OPTIONS, ],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=146,\n  serialized_end=632,\n)\n\n_TRAINER_LRSCHEDULEITEM.containing_type = _TRAINER\n_TRAINER_OPTIONS.fields_by_name[\'lr_schedule\'].message_type = _TRAINER_LRSCHEDULEITEM\n_TRAINER_OPTIONS.containing_type = _TRAINER\n_TRAINER.fields_by_name[\'optimizer\'].message_type = rastervision_dot_protos_dot_keras__classification_dot_optimizer__pb2._OPTIMIZER\n_TRAINER.fields_by_name[\'options\'].message_type = _TRAINER_OPTIONS\nDESCRIPTOR.message_types_by_name[\'Trainer\'] = _TRAINER\n\nTrainer = _reflection.GeneratedProtocolMessageType(\'Trainer\', (_message.Message,), dict(\n\n  LRScheduleItem = _reflection.GeneratedProtocolMessageType(\'LRScheduleItem\', (_message.Message,), dict(\n    DESCRIPTOR = _TRAINER_LRSCHEDULEITEM,\n    __module__ = \'rastervision.protos.keras_classification.trainer_pb2\'\n    # @@protoc_insertion_point(class_scope:keras_classification.protos.Trainer.LRScheduleItem)\n    ))\n  ,\n\n  Options = _reflection.GeneratedProtocolMessageType(\'Options\', (_message.Message,), dict(\n    DESCRIPTOR = _TRAINER_OPTIONS,\n    __module__ = \'rastervision.protos.keras_classification.trainer_pb2\'\n    # @@protoc_insertion_point(class_scope:keras_classification.protos.Trainer.Options)\n    ))\n  ,\n  DESCRIPTOR = _TRAINER,\n  __module__ = \'rastervision.protos.keras_classification.trainer_pb2\'\n  # @@protoc_insertion_point(class_scope:keras_classification.protos.Trainer)\n  ))\n_sym_db.RegisterMessage(Trainer)\n_sym_db.RegisterMessage(Trainer.LRScheduleItem)\n_sym_db.RegisterMessage(Trainer.Options)\n\n\n# @@protoc_insertion_point(module_scope)\n'"
rastervision/protos/tf_object_detection/__init__.py,0,b''
rastervision/protos/tf_object_detection/anchor_generator_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: rastervision/protos/tf_object_detection/anchor_generator.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom rastervision.protos.tf_object_detection import grid_anchor_generator_pb2 as rastervision_dot_protos_dot_tf__object__detection_dot_grid__anchor__generator__pb2\nfrom rastervision.protos.tf_object_detection import ssd_anchor_generator_pb2 as rastervision_dot_protos_dot_tf__object__detection_dot_ssd__anchor__generator__pb2\nfrom rastervision.protos.tf_object_detection import multiscale_anchor_generator_pb2 as rastervision_dot_protos_dot_tf__object__detection_dot_multiscale__anchor__generator__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'rastervision/protos/tf_object_detection/anchor_generator.proto\',\n  package=\'rastervision.protos.tf_object_detection\',\n  syntax=\'proto2\',\n  serialized_pb=_b(\'\\n>rastervision/protos/tf_object_detection/anchor_generator.proto\\x12\\\'rastervision.protos.tf_object_detection\\x1a\\x43rastervision/protos/tf_object_detection/grid_anchor_generator.proto\\x1a\\x42rastervision/protos/tf_object_detection/ssd_anchor_generator.proto\\x1aIrastervision/protos/tf_object_detection/multiscale_anchor_generator.proto\\""\\xd2\\x02\\n\\x0f\\x41nchorGenerator\\x12]\\n\\x15grid_anchor_generator\\x18\\x01 \\x01(\\x0b\\x32<.rastervision.protos.tf_object_detection.GridAnchorGeneratorH\\x00\\x12[\\n\\x14ssd_anchor_generator\\x18\\x02 \\x01(\\x0b\\x32;.rastervision.protos.tf_object_detection.SsdAnchorGeneratorH\\x00\\x12i\\n\\x1bmultiscale_anchor_generator\\x18\\x03 \\x01(\\x0b\\x32\\x42.rastervision.protos.tf_object_detection.MultiscaleAnchorGeneratorH\\x00\\x42\\x18\\n\\x16\\x61nchor_generator_oneof\')\n  ,\n  dependencies=[rastervision_dot_protos_dot_tf__object__detection_dot_grid__anchor__generator__pb2.DESCRIPTOR,rastervision_dot_protos_dot_tf__object__detection_dot_ssd__anchor__generator__pb2.DESCRIPTOR,rastervision_dot_protos_dot_tf__object__detection_dot_multiscale__anchor__generator__pb2.DESCRIPTOR,])\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\n\n\n\n_ANCHORGENERATOR = _descriptor.Descriptor(\n  name=\'AnchorGenerator\',\n  full_name=\'rastervision.protos.tf_object_detection.AnchorGenerator\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'grid_anchor_generator\', full_name=\'rastervision.protos.tf_object_detection.AnchorGenerator.grid_anchor_generator\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'ssd_anchor_generator\', full_name=\'rastervision.protos.tf_object_detection.AnchorGenerator.ssd_anchor_generator\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'multiscale_anchor_generator\', full_name=\'rastervision.protos.tf_object_detection.AnchorGenerator.multiscale_anchor_generator\', index=2,\n      number=3, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n    _descriptor.OneofDescriptor(\n      name=\'anchor_generator_oneof\', full_name=\'rastervision.protos.tf_object_detection.AnchorGenerator.anchor_generator_oneof\',\n      index=0, containing_type=None, fields=[]),\n  ],\n  serialized_start=320,\n  serialized_end=658,\n)\n\n_ANCHORGENERATOR.fields_by_name[\'grid_anchor_generator\'].message_type = rastervision_dot_protos_dot_tf__object__detection_dot_grid__anchor__generator__pb2._GRIDANCHORGENERATOR\n_ANCHORGENERATOR.fields_by_name[\'ssd_anchor_generator\'].message_type = rastervision_dot_protos_dot_tf__object__detection_dot_ssd__anchor__generator__pb2._SSDANCHORGENERATOR\n_ANCHORGENERATOR.fields_by_name[\'multiscale_anchor_generator\'].message_type = rastervision_dot_protos_dot_tf__object__detection_dot_multiscale__anchor__generator__pb2._MULTISCALEANCHORGENERATOR\n_ANCHORGENERATOR.oneofs_by_name[\'anchor_generator_oneof\'].fields.append(\n  _ANCHORGENERATOR.fields_by_name[\'grid_anchor_generator\'])\n_ANCHORGENERATOR.fields_by_name[\'grid_anchor_generator\'].containing_oneof = _ANCHORGENERATOR.oneofs_by_name[\'anchor_generator_oneof\']\n_ANCHORGENERATOR.oneofs_by_name[\'anchor_generator_oneof\'].fields.append(\n  _ANCHORGENERATOR.fields_by_name[\'ssd_anchor_generator\'])\n_ANCHORGENERATOR.fields_by_name[\'ssd_anchor_generator\'].containing_oneof = _ANCHORGENERATOR.oneofs_by_name[\'anchor_generator_oneof\']\n_ANCHORGENERATOR.oneofs_by_name[\'anchor_generator_oneof\'].fields.append(\n  _ANCHORGENERATOR.fields_by_name[\'multiscale_anchor_generator\'])\n_ANCHORGENERATOR.fields_by_name[\'multiscale_anchor_generator\'].containing_oneof = _ANCHORGENERATOR.oneofs_by_name[\'anchor_generator_oneof\']\nDESCRIPTOR.message_types_by_name[\'AnchorGenerator\'] = _ANCHORGENERATOR\n\nAnchorGenerator = _reflection.GeneratedProtocolMessageType(\'AnchorGenerator\', (_message.Message,), dict(\n  DESCRIPTOR = _ANCHORGENERATOR,\n  __module__ = \'rastervision.protos.tf_object_detection.anchor_generator_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.AnchorGenerator)\n  ))\n_sym_db.RegisterMessage(AnchorGenerator)\n\n\n# @@protoc_insertion_point(module_scope)\n'"
rastervision/protos/tf_object_detection/argmax_matcher_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: rastervision/protos/tf_object_detection/argmax_matcher.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'rastervision/protos/tf_object_detection/argmax_matcher.proto\',\n  package=\'rastervision.protos.tf_object_detection\',\n  syntax=\'proto2\',\n  serialized_pb=_b(\'\\n<rastervision/protos/tf_object_detection/argmax_matcher.proto\\x12\\\'rastervision.protos.tf_object_detection\\""\\xec\\x01\\n\\rArgMaxMatcher\\x12\\x1e\\n\\x11matched_threshold\\x18\\x01 \\x01(\\x02:\\x03\\x30.5\\x12 \\n\\x13unmatched_threshold\\x18\\x02 \\x01(\\x02:\\x03\\x30.5\\x12 \\n\\x11ignore_thresholds\\x18\\x03 \\x01(\\x08:\\x05\\x66\\x61lse\\x12,\\n\\x1enegatives_lower_than_unmatched\\x18\\x04 \\x01(\\x08:\\x04true\\x12\\\'\\n\\x18\\x66orce_match_for_each_row\\x18\\x05 \\x01(\\x08:\\x05\\x66\\x61lse\\x12 \\n\\x11use_matmul_gather\\x18\\x06 \\x01(\\x08:\\x05\\x66\\x61lse\')\n)\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\n\n\n\n_ARGMAXMATCHER = _descriptor.Descriptor(\n  name=\'ArgMaxMatcher\',\n  full_name=\'rastervision.protos.tf_object_detection.ArgMaxMatcher\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'matched_threshold\', full_name=\'rastervision.protos.tf_object_detection.ArgMaxMatcher.matched_threshold\', index=0,\n      number=1, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.5),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'unmatched_threshold\', full_name=\'rastervision.protos.tf_object_detection.ArgMaxMatcher.unmatched_threshold\', index=1,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.5),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'ignore_thresholds\', full_name=\'rastervision.protos.tf_object_detection.ArgMaxMatcher.ignore_thresholds\', index=2,\n      number=3, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'negatives_lower_than_unmatched\', full_name=\'rastervision.protos.tf_object_detection.ArgMaxMatcher.negatives_lower_than_unmatched\', index=3,\n      number=4, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=True,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'force_match_for_each_row\', full_name=\'rastervision.protos.tf_object_detection.ArgMaxMatcher.force_match_for_each_row\', index=4,\n      number=5, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'use_matmul_gather\', full_name=\'rastervision.protos.tf_object_detection.ArgMaxMatcher.use_matmul_gather\', index=5,\n      number=6, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=106,\n  serialized_end=342,\n)\n\nDESCRIPTOR.message_types_by_name[\'ArgMaxMatcher\'] = _ARGMAXMATCHER\n\nArgMaxMatcher = _reflection.GeneratedProtocolMessageType(\'ArgMaxMatcher\', (_message.Message,), dict(\n  DESCRIPTOR = _ARGMAXMATCHER,\n  __module__ = \'rastervision.protos.tf_object_detection.argmax_matcher_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.ArgMaxMatcher)\n  ))\n_sym_db.RegisterMessage(ArgMaxMatcher)\n\n\n# @@protoc_insertion_point(module_scope)\n'"
rastervision/protos/tf_object_detection/bipartite_matcher_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: rastervision/protos/tf_object_detection/bipartite_matcher.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'rastervision/protos/tf_object_detection/bipartite_matcher.proto\',\n  package=\'rastervision.protos.tf_object_detection\',\n  syntax=\'proto2\',\n  serialized_pb=_b(\'\\n?rastervision/protos/tf_object_detection/bipartite_matcher.proto\\x12\\\'rastervision.protos.tf_object_detection\\""4\\n\\x10\\x42ipartiteMatcher\\x12 \\n\\x11use_matmul_gather\\x18\\x06 \\x01(\\x08:\\x05\\x66\\x61lse\')\n)\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\n\n\n\n_BIPARTITEMATCHER = _descriptor.Descriptor(\n  name=\'BipartiteMatcher\',\n  full_name=\'rastervision.protos.tf_object_detection.BipartiteMatcher\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'use_matmul_gather\', full_name=\'rastervision.protos.tf_object_detection.BipartiteMatcher.use_matmul_gather\', index=0,\n      number=6, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=108,\n  serialized_end=160,\n)\n\nDESCRIPTOR.message_types_by_name[\'BipartiteMatcher\'] = _BIPARTITEMATCHER\n\nBipartiteMatcher = _reflection.GeneratedProtocolMessageType(\'BipartiteMatcher\', (_message.Message,), dict(\n  DESCRIPTOR = _BIPARTITEMATCHER,\n  __module__ = \'rastervision.protos.tf_object_detection.bipartite_matcher_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.BipartiteMatcher)\n  ))\n_sym_db.RegisterMessage(BipartiteMatcher)\n\n\n# @@protoc_insertion_point(module_scope)\n'"
rastervision/protos/tf_object_detection/box_coder_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: rastervision/protos/tf_object_detection/box_coder.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom rastervision.protos.tf_object_detection import faster_rcnn_box_coder_pb2 as rastervision_dot_protos_dot_tf__object__detection_dot_faster__rcnn__box__coder__pb2\nfrom rastervision.protos.tf_object_detection import keypoint_box_coder_pb2 as rastervision_dot_protos_dot_tf__object__detection_dot_keypoint__box__coder__pb2\nfrom rastervision.protos.tf_object_detection import mean_stddev_box_coder_pb2 as rastervision_dot_protos_dot_tf__object__detection_dot_mean__stddev__box__coder__pb2\nfrom rastervision.protos.tf_object_detection import square_box_coder_pb2 as rastervision_dot_protos_dot_tf__object__detection_dot_square__box__coder__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'rastervision/protos/tf_object_detection/box_coder.proto\',\n  package=\'rastervision.protos.tf_object_detection\',\n  syntax=\'proto2\',\n  serialized_pb=_b(\'\\n7rastervision/protos/tf_object_detection/box_coder.proto\\x12\\\'rastervision.protos.tf_object_detection\\x1a\\x43rastervision/protos/tf_object_detection/faster_rcnn_box_coder.proto\\x1a@rastervision/protos/tf_object_detection/keypoint_box_coder.proto\\x1a\\x43rastervision/protos/tf_object_detection/mean_stddev_box_coder.proto\\x1a>rastervision/protos/tf_object_detection/square_box_coder.proto\\""\\x87\\x03\\n\\x08\\x42oxCoder\\x12\\\\\\n\\x15\\x66\\x61ster_rcnn_box_coder\\x18\\x01 \\x01(\\x0b\\x32;.rastervision.protos.tf_object_detection.FasterRcnnBoxCoderH\\x00\\x12\\\\\\n\\x15mean_stddev_box_coder\\x18\\x02 \\x01(\\x0b\\x32;.rastervision.protos.tf_object_detection.MeanStddevBoxCoderH\\x00\\x12S\\n\\x10square_box_coder\\x18\\x03 \\x01(\\x0b\\x32\\x37.rastervision.protos.tf_object_detection.SquareBoxCoderH\\x00\\x12W\\n\\x12keypoint_box_coder\\x18\\x04 \\x01(\\x0b\\x32\\x39.rastervision.protos.tf_object_detection.KeypointBoxCoderH\\x00\\x42\\x11\\n\\x0f\\x62ox_coder_oneof\')\n  ,\n  dependencies=[rastervision_dot_protos_dot_tf__object__detection_dot_faster__rcnn__box__coder__pb2.DESCRIPTOR,rastervision_dot_protos_dot_tf__object__detection_dot_keypoint__box__coder__pb2.DESCRIPTOR,rastervision_dot_protos_dot_tf__object__detection_dot_mean__stddev__box__coder__pb2.DESCRIPTOR,rastervision_dot_protos_dot_tf__object__detection_dot_square__box__coder__pb2.DESCRIPTOR,])\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\n\n\n\n_BOXCODER = _descriptor.Descriptor(\n  name=\'BoxCoder\',\n  full_name=\'rastervision.protos.tf_object_detection.BoxCoder\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'faster_rcnn_box_coder\', full_name=\'rastervision.protos.tf_object_detection.BoxCoder.faster_rcnn_box_coder\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'mean_stddev_box_coder\', full_name=\'rastervision.protos.tf_object_detection.BoxCoder.mean_stddev_box_coder\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'square_box_coder\', full_name=\'rastervision.protos.tf_object_detection.BoxCoder.square_box_coder\', index=2,\n      number=3, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'keypoint_box_coder\', full_name=\'rastervision.protos.tf_object_detection.BoxCoder.keypoint_box_coder\', index=3,\n      number=4, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n    _descriptor.OneofDescriptor(\n      name=\'box_coder_oneof\', full_name=\'rastervision.protos.tf_object_detection.BoxCoder.box_coder_oneof\',\n      index=0, containing_type=None, fields=[]),\n  ],\n  serialized_start=369,\n  serialized_end=760,\n)\n\n_BOXCODER.fields_by_name[\'faster_rcnn_box_coder\'].message_type = rastervision_dot_protos_dot_tf__object__detection_dot_faster__rcnn__box__coder__pb2._FASTERRCNNBOXCODER\n_BOXCODER.fields_by_name[\'mean_stddev_box_coder\'].message_type = rastervision_dot_protos_dot_tf__object__detection_dot_mean__stddev__box__coder__pb2._MEANSTDDEVBOXCODER\n_BOXCODER.fields_by_name[\'square_box_coder\'].message_type = rastervision_dot_protos_dot_tf__object__detection_dot_square__box__coder__pb2._SQUAREBOXCODER\n_BOXCODER.fields_by_name[\'keypoint_box_coder\'].message_type = rastervision_dot_protos_dot_tf__object__detection_dot_keypoint__box__coder__pb2._KEYPOINTBOXCODER\n_BOXCODER.oneofs_by_name[\'box_coder_oneof\'].fields.append(\n  _BOXCODER.fields_by_name[\'faster_rcnn_box_coder\'])\n_BOXCODER.fields_by_name[\'faster_rcnn_box_coder\'].containing_oneof = _BOXCODER.oneofs_by_name[\'box_coder_oneof\']\n_BOXCODER.oneofs_by_name[\'box_coder_oneof\'].fields.append(\n  _BOXCODER.fields_by_name[\'mean_stddev_box_coder\'])\n_BOXCODER.fields_by_name[\'mean_stddev_box_coder\'].containing_oneof = _BOXCODER.oneofs_by_name[\'box_coder_oneof\']\n_BOXCODER.oneofs_by_name[\'box_coder_oneof\'].fields.append(\n  _BOXCODER.fields_by_name[\'square_box_coder\'])\n_BOXCODER.fields_by_name[\'square_box_coder\'].containing_oneof = _BOXCODER.oneofs_by_name[\'box_coder_oneof\']\n_BOXCODER.oneofs_by_name[\'box_coder_oneof\'].fields.append(\n  _BOXCODER.fields_by_name[\'keypoint_box_coder\'])\n_BOXCODER.fields_by_name[\'keypoint_box_coder\'].containing_oneof = _BOXCODER.oneofs_by_name[\'box_coder_oneof\']\nDESCRIPTOR.message_types_by_name[\'BoxCoder\'] = _BOXCODER\n\nBoxCoder = _reflection.GeneratedProtocolMessageType(\'BoxCoder\', (_message.Message,), dict(\n  DESCRIPTOR = _BOXCODER,\n  __module__ = \'rastervision.protos.tf_object_detection.box_coder_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.BoxCoder)\n  ))\n_sym_db.RegisterMessage(BoxCoder)\n\n\n# @@protoc_insertion_point(module_scope)\n'"
rastervision/protos/tf_object_detection/box_predictor_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: rastervision/protos/tf_object_detection/box_predictor.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom rastervision.protos.tf_object_detection import hyperparams_pb2 as rastervision_dot_protos_dot_tf__object__detection_dot_hyperparams__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'rastervision/protos/tf_object_detection/box_predictor.proto\',\n  package=\'rastervision.protos.tf_object_detection\',\n  syntax=\'proto2\',\n  serialized_pb=_b(\'\\n;rastervision/protos/tf_object_detection/box_predictor.proto\\x12\\\'rastervision.protos.tf_object_detection\\x1a\\x39rastervision/protos/tf_object_detection/hyperparams.proto\\""\\xd1\\x03\\n\\x0c\\x42oxPredictor\\x12i\\n\\x1b\\x63onvolutional_box_predictor\\x18\\x01 \\x01(\\x0b\\x32\\x42.rastervision.protos.tf_object_detection.ConvolutionalBoxPredictorH\\x00\\x12`\\n\\x17mask_rcnn_box_predictor\\x18\\x02 \\x01(\\x0b\\x32=.rastervision.protos.tf_object_detection.MaskRCNNBoxPredictorH\\x00\\x12W\\n\\x12rfcn_box_predictor\\x18\\x03 \\x01(\\x0b\\x32\\x39.rastervision.protos.tf_object_detection.RfcnBoxPredictorH\\x00\\x12\\x83\\x01\\n)weight_shared_convolutional_box_predictor\\x18\\x04 \\x01(\\x0b\\x32N.rastervision.protos.tf_object_detection.WeightSharedConvolutionalBoxPredictorH\\x00\\x42\\x15\\n\\x13\\x62ox_predictor_oneof\\""c\\n\\x08MaskHead\\x12\\x17\\n\\x0bmask_height\\x18\\x01 \\x01(\\x05:\\x02\\x31\\x35\\x12\\x16\\n\\nmask_width\\x18\\x02 \\x01(\\x05:\\x02\\x31\\x35\\x12&\\n\\x18masks_are_class_agnostic\\x18\\x03 \\x01(\\x08:\\x04true\\""\\xe6\\x03\\n\\x19\\x43onvolutionalBoxPredictor\\x12N\\n\\x10\\x63onv_hyperparams\\x18\\x01 \\x01(\\x0b\\x32\\x34.rastervision.protos.tf_object_detection.Hyperparams\\x12\\x14\\n\\tmin_depth\\x18\\x02 \\x01(\\x05:\\x01\\x30\\x12\\x14\\n\\tmax_depth\\x18\\x03 \\x01(\\x05:\\x01\\x30\\x12&\\n\\x1bnum_layers_before_predictor\\x18\\x04 \\x01(\\x05:\\x01\\x30\\x12\\x19\\n\\x0buse_dropout\\x18\\x05 \\x01(\\x08:\\x04true\\x12%\\n\\x18\\x64ropout_keep_probability\\x18\\x06 \\x01(\\x02:\\x03\\x30.8\\x12\\x16\\n\\x0bkernel_size\\x18\\x07 \\x01(\\x05:\\x01\\x31\\x12\\x18\\n\\rbox_code_size\\x18\\x08 \\x01(\\x05:\\x01\\x34\\x12&\\n\\x17\\x61pply_sigmoid_to_scores\\x18\\t \\x01(\\x08:\\x05\\x66\\x61lse\\x12%\\n\\x1a\\x63lass_prediction_bias_init\\x18\\n \\x01(\\x02:\\x01\\x30\\x12\\x1c\\n\\ruse_depthwise\\x18\\x0b \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\x44\\n\\tmask_head\\x18\\x0c \\x01(\\x0b\\x32\\x31.rastervision.protos.tf_object_detection.MaskHead\\""\\xc4\\x06\\n%WeightSharedConvolutionalBoxPredictor\\x12N\\n\\x10\\x63onv_hyperparams\\x18\\x01 \\x01(\\x0b\\x32\\x34.rastervision.protos.tf_object_detection.Hyperparams\\x12&\\n\\x1bnum_layers_before_predictor\\x18\\x04 \\x01(\\x05:\\x01\\x30\\x12\\x10\\n\\x05\\x64\\x65pth\\x18\\x02 \\x01(\\x05:\\x01\\x30\\x12\\x16\\n\\x0bkernel_size\\x18\\x07 \\x01(\\x05:\\x01\\x33\\x12\\x18\\n\\rbox_code_size\\x18\\x08 \\x01(\\x05:\\x01\\x34\\x12%\\n\\x1a\\x63lass_prediction_bias_init\\x18\\n \\x01(\\x02:\\x01\\x30\\x12\\x1a\\n\\x0buse_dropout\\x18\\x0b \\x01(\\x08:\\x05\\x66\\x61lse\\x12%\\n\\x18\\x64ropout_keep_probability\\x18\\x0c \\x01(\\x02:\\x03\\x30.8\\x12%\\n\\x16share_prediction_tower\\x18\\r \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\x1c\\n\\ruse_depthwise\\x18\\x0e \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\x44\\n\\tmask_head\\x18\\x0f \\x01(\\x0b\\x32\\x31.rastervision.protos.tf_object_detection.MaskHead\\x12\\x80\\x01\\n\\x0fscore_converter\\x18\\x10 \\x01(\\x0e\\x32].rastervision.protos.tf_object_detection.WeightSharedConvolutionalBoxPredictor.ScoreConverter:\\x08IDENTITY\\x12\\x86\\x01\\n\\x18\\x62ox_encodings_clip_range\\x18\\x11 \\x01(\\x0b\\x32\\x64.rastervision.protos.tf_object_detection.WeightSharedConvolutionalBoxPredictor.BoxEncodingsClipRange\\x1a\\x31\\n\\x15\\x42oxEncodingsClipRange\\x12\\x0b\\n\\x03min\\x18\\x01 \\x01(\\x02\\x12\\x0b\\n\\x03max\\x18\\x02 \\x01(\\x02\\""+\\n\\x0eScoreConverter\\x12\\x0c\\n\\x08IDENTITY\\x10\\x00\\x12\\x0b\\n\\x07SIGMOID\\x10\\x01\\""\\xb2\\x04\\n\\x14MaskRCNNBoxPredictor\\x12L\\n\\x0e\\x66\\x63_hyperparams\\x18\\x01 \\x01(\\x0b\\x32\\x34.rastervision.protos.tf_object_detection.Hyperparams\\x12\\x1a\\n\\x0buse_dropout\\x18\\x02 \\x01(\\x08:\\x05\\x66\\x61lse\\x12%\\n\\x18\\x64ropout_keep_probability\\x18\\x03 \\x01(\\x02:\\x03\\x30.5\\x12\\x18\\n\\rbox_code_size\\x18\\x04 \\x01(\\x05:\\x01\\x34\\x12N\\n\\x10\\x63onv_hyperparams\\x18\\x05 \\x01(\\x0b\\x32\\x34.rastervision.protos.tf_object_detection.Hyperparams\\x12%\\n\\x16predict_instance_masks\\x18\\x06 \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\\'\\n\\x1amask_prediction_conv_depth\\x18\\x07 \\x01(\\x05:\\x03\\x32\\x35\\x36\\x12 \\n\\x11predict_keypoints\\x18\\x08 \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\x17\\n\\x0bmask_height\\x18\\t \\x01(\\x05:\\x02\\x31\\x35\\x12\\x16\\n\\nmask_width\\x18\\n \\x01(\\x05:\\x02\\x31\\x35\\x12*\\n\\x1fmask_prediction_num_conv_layers\\x18\\x0b \\x01(\\x05:\\x01\\x32\\x12\\\'\\n\\x18masks_are_class_agnostic\\x18\\x0c \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\\'\\n\\x18share_box_across_classes\\x18\\r \\x01(\\x08:\\x05\\x66\\x61lse\\""\\x89\\x02\\n\\x10RfcnBoxPredictor\\x12N\\n\\x10\\x63onv_hyperparams\\x18\\x01 \\x01(\\x0b\\x32\\x34.rastervision.protos.tf_object_detection.Hyperparams\\x12\\""\\n\\x17num_spatial_bins_height\\x18\\x02 \\x01(\\x05:\\x01\\x33\\x12!\\n\\x16num_spatial_bins_width\\x18\\x03 \\x01(\\x05:\\x01\\x33\\x12\\x13\\n\\x05\\x64\\x65pth\\x18\\x04 \\x01(\\x05:\\x04\\x31\\x30\\x32\\x34\\x12\\x18\\n\\rbox_code_size\\x18\\x05 \\x01(\\x05:\\x01\\x34\\x12\\x17\\n\\x0b\\x63rop_height\\x18\\x06 \\x01(\\x05:\\x02\\x31\\x32\\x12\\x16\\n\\ncrop_width\\x18\\x07 \\x01(\\x05:\\x02\\x31\\x32\')\n  ,\n  dependencies=[rastervision_dot_protos_dot_tf__object__detection_dot_hyperparams__pb2.DESCRIPTOR,])\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\n\n\n_WEIGHTSHAREDCONVOLUTIONALBOXPREDICTOR_SCORECONVERTER = _descriptor.EnumDescriptor(\n  name=\'ScoreConverter\',\n  full_name=\'rastervision.protos.tf_object_detection.WeightSharedConvolutionalBoxPredictor.ScoreConverter\',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name=\'IDENTITY\', index=0, number=0,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'SIGMOID\', index=1, number=1,\n      options=None,\n      type=None),\n  ],\n  containing_type=None,\n  options=None,\n  serialized_start=2015,\n  serialized_end=2058,\n)\n_sym_db.RegisterEnumDescriptor(_WEIGHTSHAREDCONVOLUTIONALBOXPREDICTOR_SCORECONVERTER)\n\n\n_BOXPREDICTOR = _descriptor.Descriptor(\n  name=\'BoxPredictor\',\n  full_name=\'rastervision.protos.tf_object_detection.BoxPredictor\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'convolutional_box_predictor\', full_name=\'rastervision.protos.tf_object_detection.BoxPredictor.convolutional_box_predictor\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'mask_rcnn_box_predictor\', full_name=\'rastervision.protos.tf_object_detection.BoxPredictor.mask_rcnn_box_predictor\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'rfcn_box_predictor\', full_name=\'rastervision.protos.tf_object_detection.BoxPredictor.rfcn_box_predictor\', index=2,\n      number=3, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'weight_shared_convolutional_box_predictor\', full_name=\'rastervision.protos.tf_object_detection.BoxPredictor.weight_shared_convolutional_box_predictor\', index=3,\n      number=4, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n    _descriptor.OneofDescriptor(\n      name=\'box_predictor_oneof\', full_name=\'rastervision.protos.tf_object_detection.BoxPredictor.box_predictor_oneof\',\n      index=0, containing_type=None, fields=[]),\n  ],\n  serialized_start=164,\n  serialized_end=629,\n)\n\n\n_MASKHEAD = _descriptor.Descriptor(\n  name=\'MaskHead\',\n  full_name=\'rastervision.protos.tf_object_detection.MaskHead\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'mask_height\', full_name=\'rastervision.protos.tf_object_detection.MaskHead.mask_height\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=15,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'mask_width\', full_name=\'rastervision.protos.tf_object_detection.MaskHead.mask_width\', index=1,\n      number=2, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=15,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'masks_are_class_agnostic\', full_name=\'rastervision.protos.tf_object_detection.MaskHead.masks_are_class_agnostic\', index=2,\n      number=3, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=True,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=631,\n  serialized_end=730,\n)\n\n\n_CONVOLUTIONALBOXPREDICTOR = _descriptor.Descriptor(\n  name=\'ConvolutionalBoxPredictor\',\n  full_name=\'rastervision.protos.tf_object_detection.ConvolutionalBoxPredictor\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'conv_hyperparams\', full_name=\'rastervision.protos.tf_object_detection.ConvolutionalBoxPredictor.conv_hyperparams\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'min_depth\', full_name=\'rastervision.protos.tf_object_detection.ConvolutionalBoxPredictor.min_depth\', index=1,\n      number=2, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'max_depth\', full_name=\'rastervision.protos.tf_object_detection.ConvolutionalBoxPredictor.max_depth\', index=2,\n      number=3, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'num_layers_before_predictor\', full_name=\'rastervision.protos.tf_object_detection.ConvolutionalBoxPredictor.num_layers_before_predictor\', index=3,\n      number=4, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'use_dropout\', full_name=\'rastervision.protos.tf_object_detection.ConvolutionalBoxPredictor.use_dropout\', index=4,\n      number=5, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=True,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'dropout_keep_probability\', full_name=\'rastervision.protos.tf_object_detection.ConvolutionalBoxPredictor.dropout_keep_probability\', index=5,\n      number=6, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.8),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'kernel_size\', full_name=\'rastervision.protos.tf_object_detection.ConvolutionalBoxPredictor.kernel_size\', index=6,\n      number=7, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'box_code_size\', full_name=\'rastervision.protos.tf_object_detection.ConvolutionalBoxPredictor.box_code_size\', index=7,\n      number=8, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=4,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'apply_sigmoid_to_scores\', full_name=\'rastervision.protos.tf_object_detection.ConvolutionalBoxPredictor.apply_sigmoid_to_scores\', index=8,\n      number=9, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'class_prediction_bias_init\', full_name=\'rastervision.protos.tf_object_detection.ConvolutionalBoxPredictor.class_prediction_bias_init\', index=9,\n      number=10, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'use_depthwise\', full_name=\'rastervision.protos.tf_object_detection.ConvolutionalBoxPredictor.use_depthwise\', index=10,\n      number=11, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'mask_head\', full_name=\'rastervision.protos.tf_object_detection.ConvolutionalBoxPredictor.mask_head\', index=11,\n      number=12, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=733,\n  serialized_end=1219,\n)\n\n\n_WEIGHTSHAREDCONVOLUTIONALBOXPREDICTOR_BOXENCODINGSCLIPRANGE = _descriptor.Descriptor(\n  name=\'BoxEncodingsClipRange\',\n  full_name=\'rastervision.protos.tf_object_detection.WeightSharedConvolutionalBoxPredictor.BoxEncodingsClipRange\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'min\', full_name=\'rastervision.protos.tf_object_detection.WeightSharedConvolutionalBoxPredictor.BoxEncodingsClipRange.min\', index=0,\n      number=1, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'max\', full_name=\'rastervision.protos.tf_object_detection.WeightSharedConvolutionalBoxPredictor.BoxEncodingsClipRange.max\', index=1,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1964,\n  serialized_end=2013,\n)\n\n_WEIGHTSHAREDCONVOLUTIONALBOXPREDICTOR = _descriptor.Descriptor(\n  name=\'WeightSharedConvolutionalBoxPredictor\',\n  full_name=\'rastervision.protos.tf_object_detection.WeightSharedConvolutionalBoxPredictor\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'conv_hyperparams\', full_name=\'rastervision.protos.tf_object_detection.WeightSharedConvolutionalBoxPredictor.conv_hyperparams\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'num_layers_before_predictor\', full_name=\'rastervision.protos.tf_object_detection.WeightSharedConvolutionalBoxPredictor.num_layers_before_predictor\', index=1,\n      number=4, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'depth\', full_name=\'rastervision.protos.tf_object_detection.WeightSharedConvolutionalBoxPredictor.depth\', index=2,\n      number=2, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'kernel_size\', full_name=\'rastervision.protos.tf_object_detection.WeightSharedConvolutionalBoxPredictor.kernel_size\', index=3,\n      number=7, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=3,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'box_code_size\', full_name=\'rastervision.protos.tf_object_detection.WeightSharedConvolutionalBoxPredictor.box_code_size\', index=4,\n      number=8, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=4,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'class_prediction_bias_init\', full_name=\'rastervision.protos.tf_object_detection.WeightSharedConvolutionalBoxPredictor.class_prediction_bias_init\', index=5,\n      number=10, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'use_dropout\', full_name=\'rastervision.protos.tf_object_detection.WeightSharedConvolutionalBoxPredictor.use_dropout\', index=6,\n      number=11, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'dropout_keep_probability\', full_name=\'rastervision.protos.tf_object_detection.WeightSharedConvolutionalBoxPredictor.dropout_keep_probability\', index=7,\n      number=12, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.8),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'share_prediction_tower\', full_name=\'rastervision.protos.tf_object_detection.WeightSharedConvolutionalBoxPredictor.share_prediction_tower\', index=8,\n      number=13, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'use_depthwise\', full_name=\'rastervision.protos.tf_object_detection.WeightSharedConvolutionalBoxPredictor.use_depthwise\', index=9,\n      number=14, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'mask_head\', full_name=\'rastervision.protos.tf_object_detection.WeightSharedConvolutionalBoxPredictor.mask_head\', index=10,\n      number=15, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'score_converter\', full_name=\'rastervision.protos.tf_object_detection.WeightSharedConvolutionalBoxPredictor.score_converter\', index=11,\n      number=16, type=14, cpp_type=8, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'box_encodings_clip_range\', full_name=\'rastervision.protos.tf_object_detection.WeightSharedConvolutionalBoxPredictor.box_encodings_clip_range\', index=12,\n      number=17, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[_WEIGHTSHAREDCONVOLUTIONALBOXPREDICTOR_BOXENCODINGSCLIPRANGE, ],\n  enum_types=[\n    _WEIGHTSHAREDCONVOLUTIONALBOXPREDICTOR_SCORECONVERTER,\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1222,\n  serialized_end=2058,\n)\n\n\n_MASKRCNNBOXPREDICTOR = _descriptor.Descriptor(\n  name=\'MaskRCNNBoxPredictor\',\n  full_name=\'rastervision.protos.tf_object_detection.MaskRCNNBoxPredictor\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'fc_hyperparams\', full_name=\'rastervision.protos.tf_object_detection.MaskRCNNBoxPredictor.fc_hyperparams\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'use_dropout\', full_name=\'rastervision.protos.tf_object_detection.MaskRCNNBoxPredictor.use_dropout\', index=1,\n      number=2, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'dropout_keep_probability\', full_name=\'rastervision.protos.tf_object_detection.MaskRCNNBoxPredictor.dropout_keep_probability\', index=2,\n      number=3, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.5),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'box_code_size\', full_name=\'rastervision.protos.tf_object_detection.MaskRCNNBoxPredictor.box_code_size\', index=3,\n      number=4, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=4,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'conv_hyperparams\', full_name=\'rastervision.protos.tf_object_detection.MaskRCNNBoxPredictor.conv_hyperparams\', index=4,\n      number=5, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'predict_instance_masks\', full_name=\'rastervision.protos.tf_object_detection.MaskRCNNBoxPredictor.predict_instance_masks\', index=5,\n      number=6, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'mask_prediction_conv_depth\', full_name=\'rastervision.protos.tf_object_detection.MaskRCNNBoxPredictor.mask_prediction_conv_depth\', index=6,\n      number=7, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=256,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'predict_keypoints\', full_name=\'rastervision.protos.tf_object_detection.MaskRCNNBoxPredictor.predict_keypoints\', index=7,\n      number=8, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'mask_height\', full_name=\'rastervision.protos.tf_object_detection.MaskRCNNBoxPredictor.mask_height\', index=8,\n      number=9, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=15,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'mask_width\', full_name=\'rastervision.protos.tf_object_detection.MaskRCNNBoxPredictor.mask_width\', index=9,\n      number=10, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=15,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'mask_prediction_num_conv_layers\', full_name=\'rastervision.protos.tf_object_detection.MaskRCNNBoxPredictor.mask_prediction_num_conv_layers\', index=10,\n      number=11, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=2,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'masks_are_class_agnostic\', full_name=\'rastervision.protos.tf_object_detection.MaskRCNNBoxPredictor.masks_are_class_agnostic\', index=11,\n      number=12, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'share_box_across_classes\', full_name=\'rastervision.protos.tf_object_detection.MaskRCNNBoxPredictor.share_box_across_classes\', index=12,\n      number=13, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=2061,\n  serialized_end=2623,\n)\n\n\n_RFCNBOXPREDICTOR = _descriptor.Descriptor(\n  name=\'RfcnBoxPredictor\',\n  full_name=\'rastervision.protos.tf_object_detection.RfcnBoxPredictor\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'conv_hyperparams\', full_name=\'rastervision.protos.tf_object_detection.RfcnBoxPredictor.conv_hyperparams\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'num_spatial_bins_height\', full_name=\'rastervision.protos.tf_object_detection.RfcnBoxPredictor.num_spatial_bins_height\', index=1,\n      number=2, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=3,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'num_spatial_bins_width\', full_name=\'rastervision.protos.tf_object_detection.RfcnBoxPredictor.num_spatial_bins_width\', index=2,\n      number=3, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=3,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'depth\', full_name=\'rastervision.protos.tf_object_detection.RfcnBoxPredictor.depth\', index=3,\n      number=4, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=1024,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'box_code_size\', full_name=\'rastervision.protos.tf_object_detection.RfcnBoxPredictor.box_code_size\', index=4,\n      number=5, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=4,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'crop_height\', full_name=\'rastervision.protos.tf_object_detection.RfcnBoxPredictor.crop_height\', index=5,\n      number=6, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=12,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'crop_width\', full_name=\'rastervision.protos.tf_object_detection.RfcnBoxPredictor.crop_width\', index=6,\n      number=7, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=12,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=2626,\n  serialized_end=2891,\n)\n\n_BOXPREDICTOR.fields_by_name[\'convolutional_box_predictor\'].message_type = _CONVOLUTIONALBOXPREDICTOR\n_BOXPREDICTOR.fields_by_name[\'mask_rcnn_box_predictor\'].message_type = _MASKRCNNBOXPREDICTOR\n_BOXPREDICTOR.fields_by_name[\'rfcn_box_predictor\'].message_type = _RFCNBOXPREDICTOR\n_BOXPREDICTOR.fields_by_name[\'weight_shared_convolutional_box_predictor\'].message_type = _WEIGHTSHAREDCONVOLUTIONALBOXPREDICTOR\n_BOXPREDICTOR.oneofs_by_name[\'box_predictor_oneof\'].fields.append(\n  _BOXPREDICTOR.fields_by_name[\'convolutional_box_predictor\'])\n_BOXPREDICTOR.fields_by_name[\'convolutional_box_predictor\'].containing_oneof = _BOXPREDICTOR.oneofs_by_name[\'box_predictor_oneof\']\n_BOXPREDICTOR.oneofs_by_name[\'box_predictor_oneof\'].fields.append(\n  _BOXPREDICTOR.fields_by_name[\'mask_rcnn_box_predictor\'])\n_BOXPREDICTOR.fields_by_name[\'mask_rcnn_box_predictor\'].containing_oneof = _BOXPREDICTOR.oneofs_by_name[\'box_predictor_oneof\']\n_BOXPREDICTOR.oneofs_by_name[\'box_predictor_oneof\'].fields.append(\n  _BOXPREDICTOR.fields_by_name[\'rfcn_box_predictor\'])\n_BOXPREDICTOR.fields_by_name[\'rfcn_box_predictor\'].containing_oneof = _BOXPREDICTOR.oneofs_by_name[\'box_predictor_oneof\']\n_BOXPREDICTOR.oneofs_by_name[\'box_predictor_oneof\'].fields.append(\n  _BOXPREDICTOR.fields_by_name[\'weight_shared_convolutional_box_predictor\'])\n_BOXPREDICTOR.fields_by_name[\'weight_shared_convolutional_box_predictor\'].containing_oneof = _BOXPREDICTOR.oneofs_by_name[\'box_predictor_oneof\']\n_CONVOLUTIONALBOXPREDICTOR.fields_by_name[\'conv_hyperparams\'].message_type = rastervision_dot_protos_dot_tf__object__detection_dot_hyperparams__pb2._HYPERPARAMS\n_CONVOLUTIONALBOXPREDICTOR.fields_by_name[\'mask_head\'].message_type = _MASKHEAD\n_WEIGHTSHAREDCONVOLUTIONALBOXPREDICTOR_BOXENCODINGSCLIPRANGE.containing_type = _WEIGHTSHAREDCONVOLUTIONALBOXPREDICTOR\n_WEIGHTSHAREDCONVOLUTIONALBOXPREDICTOR.fields_by_name[\'conv_hyperparams\'].message_type = rastervision_dot_protos_dot_tf__object__detection_dot_hyperparams__pb2._HYPERPARAMS\n_WEIGHTSHAREDCONVOLUTIONALBOXPREDICTOR.fields_by_name[\'mask_head\'].message_type = _MASKHEAD\n_WEIGHTSHAREDCONVOLUTIONALBOXPREDICTOR.fields_by_name[\'score_converter\'].enum_type = _WEIGHTSHAREDCONVOLUTIONALBOXPREDICTOR_SCORECONVERTER\n_WEIGHTSHAREDCONVOLUTIONALBOXPREDICTOR.fields_by_name[\'box_encodings_clip_range\'].message_type = _WEIGHTSHAREDCONVOLUTIONALBOXPREDICTOR_BOXENCODINGSCLIPRANGE\n_WEIGHTSHAREDCONVOLUTIONALBOXPREDICTOR_SCORECONVERTER.containing_type = _WEIGHTSHAREDCONVOLUTIONALBOXPREDICTOR\n_MASKRCNNBOXPREDICTOR.fields_by_name[\'fc_hyperparams\'].message_type = rastervision_dot_protos_dot_tf__object__detection_dot_hyperparams__pb2._HYPERPARAMS\n_MASKRCNNBOXPREDICTOR.fields_by_name[\'conv_hyperparams\'].message_type = rastervision_dot_protos_dot_tf__object__detection_dot_hyperparams__pb2._HYPERPARAMS\n_RFCNBOXPREDICTOR.fields_by_name[\'conv_hyperparams\'].message_type = rastervision_dot_protos_dot_tf__object__detection_dot_hyperparams__pb2._HYPERPARAMS\nDESCRIPTOR.message_types_by_name[\'BoxPredictor\'] = _BOXPREDICTOR\nDESCRIPTOR.message_types_by_name[\'MaskHead\'] = _MASKHEAD\nDESCRIPTOR.message_types_by_name[\'ConvolutionalBoxPredictor\'] = _CONVOLUTIONALBOXPREDICTOR\nDESCRIPTOR.message_types_by_name[\'WeightSharedConvolutionalBoxPredictor\'] = _WEIGHTSHAREDCONVOLUTIONALBOXPREDICTOR\nDESCRIPTOR.message_types_by_name[\'MaskRCNNBoxPredictor\'] = _MASKRCNNBOXPREDICTOR\nDESCRIPTOR.message_types_by_name[\'RfcnBoxPredictor\'] = _RFCNBOXPREDICTOR\n\nBoxPredictor = _reflection.GeneratedProtocolMessageType(\'BoxPredictor\', (_message.Message,), dict(\n  DESCRIPTOR = _BOXPREDICTOR,\n  __module__ = \'rastervision.protos.tf_object_detection.box_predictor_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.BoxPredictor)\n  ))\n_sym_db.RegisterMessage(BoxPredictor)\n\nMaskHead = _reflection.GeneratedProtocolMessageType(\'MaskHead\', (_message.Message,), dict(\n  DESCRIPTOR = _MASKHEAD,\n  __module__ = \'rastervision.protos.tf_object_detection.box_predictor_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.MaskHead)\n  ))\n_sym_db.RegisterMessage(MaskHead)\n\nConvolutionalBoxPredictor = _reflection.GeneratedProtocolMessageType(\'ConvolutionalBoxPredictor\', (_message.Message,), dict(\n  DESCRIPTOR = _CONVOLUTIONALBOXPREDICTOR,\n  __module__ = \'rastervision.protos.tf_object_detection.box_predictor_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.ConvolutionalBoxPredictor)\n  ))\n_sym_db.RegisterMessage(ConvolutionalBoxPredictor)\n\nWeightSharedConvolutionalBoxPredictor = _reflection.GeneratedProtocolMessageType(\'WeightSharedConvolutionalBoxPredictor\', (_message.Message,), dict(\n\n  BoxEncodingsClipRange = _reflection.GeneratedProtocolMessageType(\'BoxEncodingsClipRange\', (_message.Message,), dict(\n    DESCRIPTOR = _WEIGHTSHAREDCONVOLUTIONALBOXPREDICTOR_BOXENCODINGSCLIPRANGE,\n    __module__ = \'rastervision.protos.tf_object_detection.box_predictor_pb2\'\n    # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.WeightSharedConvolutionalBoxPredictor.BoxEncodingsClipRange)\n    ))\n  ,\n  DESCRIPTOR = _WEIGHTSHAREDCONVOLUTIONALBOXPREDICTOR,\n  __module__ = \'rastervision.protos.tf_object_detection.box_predictor_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.WeightSharedConvolutionalBoxPredictor)\n  ))\n_sym_db.RegisterMessage(WeightSharedConvolutionalBoxPredictor)\n_sym_db.RegisterMessage(WeightSharedConvolutionalBoxPredictor.BoxEncodingsClipRange)\n\nMaskRCNNBoxPredictor = _reflection.GeneratedProtocolMessageType(\'MaskRCNNBoxPredictor\', (_message.Message,), dict(\n  DESCRIPTOR = _MASKRCNNBOXPREDICTOR,\n  __module__ = \'rastervision.protos.tf_object_detection.box_predictor_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.MaskRCNNBoxPredictor)\n  ))\n_sym_db.RegisterMessage(MaskRCNNBoxPredictor)\n\nRfcnBoxPredictor = _reflection.GeneratedProtocolMessageType(\'RfcnBoxPredictor\', (_message.Message,), dict(\n  DESCRIPTOR = _RFCNBOXPREDICTOR,\n  __module__ = \'rastervision.protos.tf_object_detection.box_predictor_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.RfcnBoxPredictor)\n  ))\n_sym_db.RegisterMessage(RfcnBoxPredictor)\n\n\n# @@protoc_insertion_point(module_scope)\n'"
rastervision/protos/tf_object_detection/eval_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: rastervision/protos/tf_object_detection/eval.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'rastervision/protos/tf_object_detection/eval.proto\',\n  package=\'rastervision.protos.tf_object_detection\',\n  syntax=\'proto2\',\n  serialized_pb=_b(\'\\n2rastervision/protos/tf_object_detection/eval.proto\\x12\\\'rastervision.protos.tf_object_detection\\""\\xf7\\x05\\n\\nEvalConfig\\x12\\x15\\n\\nbatch_size\\x18\\x19 \\x01(\\r:\\x01\\x31\\x12\\x1e\\n\\x12num_visualizations\\x18\\x01 \\x01(\\r:\\x02\\x31\\x30\\x12\\x1e\\n\\x0cnum_examples\\x18\\x02 \\x01(\\r:\\x04\\x35\\x30\\x30\\x30\\x42\\x02\\x18\\x01\\x12\\x1f\\n\\x12\\x65val_interval_secs\\x18\\x03 \\x01(\\r:\\x03\\x33\\x30\\x30\\x12\\x18\\n\\tmax_evals\\x18\\x04 \\x01(\\r:\\x01\\x30\\x42\\x02\\x18\\x01\\x12\\x19\\n\\nsave_graph\\x18\\x05 \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\""\\n\\x18visualization_export_dir\\x18\\x06 \\x01(\\t:\\x00\\x12\\x15\\n\\x0b\\x65val_master\\x18\\x07 \\x01(\\t:\\x00\\x12\\x13\\n\\x0bmetrics_set\\x18\\x08 \\x03(\\t\\x12\\x15\\n\\x0b\\x65xport_path\\x18\\t \\x01(\\t:\\x00\\x12!\\n\\x12ignore_groundtruth\\x18\\n \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\""\\n\\x13use_moving_averages\\x18\\x0b \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\""\\n\\x13\\x65val_instance_masks\\x18\\x0c \\x01(\\x08:\\x05\\x66\\x61lse\\x12 \\n\\x13min_score_threshold\\x18\\r \\x01(\\x02:\\x03\\x30.5\\x12&\\n\\x1amax_num_boxes_to_visualize\\x18\\x0e \\x01(\\x05:\\x02\\x32\\x30\\x12\\x1a\\n\\x0bskip_scores\\x18\\x0f \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\x1a\\n\\x0bskip_labels\\x18\\x10 \\x01(\\x08:\\x05\\x66\\x61lse\\x12*\\n\\x1bvisualize_groundtruth_boxes\\x18\\x11 \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\x32\\n#groundtruth_box_visualization_color\\x18\\x12 \\x01(\\t:\\x05\\x62lack\\x12\\x35\\n&keep_image_id_for_visualization_export\\x18\\x13 \\x01(\\x08:\\x05\\x66\\x61lse\\x12$\\n\\x16retain_original_images\\x18\\x17 \\x01(\\x08:\\x04true\\x12+\\n\\x1cinclude_metrics_per_category\\x18\\x18 \\x01(\\x08:\\x05\\x66\\x61lse\')\n)\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\n\n\n\n_EVALCONFIG = _descriptor.Descriptor(\n  name=\'EvalConfig\',\n  full_name=\'rastervision.protos.tf_object_detection.EvalConfig\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'batch_size\', full_name=\'rastervision.protos.tf_object_detection.EvalConfig.batch_size\', index=0,\n      number=25, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'num_visualizations\', full_name=\'rastervision.protos.tf_object_detection.EvalConfig.num_visualizations\', index=1,\n      number=1, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=10,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'num_examples\', full_name=\'rastervision.protos.tf_object_detection.EvalConfig.num_examples\', index=2,\n      number=2, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=5000,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=_descriptor._ParseOptions(descriptor_pb2.FieldOptions(), _b(\'\\030\\001\'))),\n    _descriptor.FieldDescriptor(\n      name=\'eval_interval_secs\', full_name=\'rastervision.protos.tf_object_detection.EvalConfig.eval_interval_secs\', index=3,\n      number=3, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=300,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'max_evals\', full_name=\'rastervision.protos.tf_object_detection.EvalConfig.max_evals\', index=4,\n      number=4, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=_descriptor._ParseOptions(descriptor_pb2.FieldOptions(), _b(\'\\030\\001\'))),\n    _descriptor.FieldDescriptor(\n      name=\'save_graph\', full_name=\'rastervision.protos.tf_object_detection.EvalConfig.save_graph\', index=5,\n      number=5, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'visualization_export_dir\', full_name=\'rastervision.protos.tf_object_detection.EvalConfig.visualization_export_dir\', index=6,\n      number=6, type=9, cpp_type=9, label=1,\n      has_default_value=True, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'eval_master\', full_name=\'rastervision.protos.tf_object_detection.EvalConfig.eval_master\', index=7,\n      number=7, type=9, cpp_type=9, label=1,\n      has_default_value=True, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'metrics_set\', full_name=\'rastervision.protos.tf_object_detection.EvalConfig.metrics_set\', index=8,\n      number=8, type=9, cpp_type=9, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'export_path\', full_name=\'rastervision.protos.tf_object_detection.EvalConfig.export_path\', index=9,\n      number=9, type=9, cpp_type=9, label=1,\n      has_default_value=True, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'ignore_groundtruth\', full_name=\'rastervision.protos.tf_object_detection.EvalConfig.ignore_groundtruth\', index=10,\n      number=10, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'use_moving_averages\', full_name=\'rastervision.protos.tf_object_detection.EvalConfig.use_moving_averages\', index=11,\n      number=11, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'eval_instance_masks\', full_name=\'rastervision.protos.tf_object_detection.EvalConfig.eval_instance_masks\', index=12,\n      number=12, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'min_score_threshold\', full_name=\'rastervision.protos.tf_object_detection.EvalConfig.min_score_threshold\', index=13,\n      number=13, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.5),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'max_num_boxes_to_visualize\', full_name=\'rastervision.protos.tf_object_detection.EvalConfig.max_num_boxes_to_visualize\', index=14,\n      number=14, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=20,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'skip_scores\', full_name=\'rastervision.protos.tf_object_detection.EvalConfig.skip_scores\', index=15,\n      number=15, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'skip_labels\', full_name=\'rastervision.protos.tf_object_detection.EvalConfig.skip_labels\', index=16,\n      number=16, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'visualize_groundtruth_boxes\', full_name=\'rastervision.protos.tf_object_detection.EvalConfig.visualize_groundtruth_boxes\', index=17,\n      number=17, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'groundtruth_box_visualization_color\', full_name=\'rastervision.protos.tf_object_detection.EvalConfig.groundtruth_box_visualization_color\', index=18,\n      number=18, type=9, cpp_type=9, label=1,\n      has_default_value=True, default_value=_b(""black"").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'keep_image_id_for_visualization_export\', full_name=\'rastervision.protos.tf_object_detection.EvalConfig.keep_image_id_for_visualization_export\', index=19,\n      number=19, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'retain_original_images\', full_name=\'rastervision.protos.tf_object_detection.EvalConfig.retain_original_images\', index=20,\n      number=23, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=True,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'include_metrics_per_category\', full_name=\'rastervision.protos.tf_object_detection.EvalConfig.include_metrics_per_category\', index=21,\n      number=24, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=96,\n  serialized_end=855,\n)\n\nDESCRIPTOR.message_types_by_name[\'EvalConfig\'] = _EVALCONFIG\n\nEvalConfig = _reflection.GeneratedProtocolMessageType(\'EvalConfig\', (_message.Message,), dict(\n  DESCRIPTOR = _EVALCONFIG,\n  __module__ = \'rastervision.protos.tf_object_detection.eval_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.EvalConfig)\n  ))\n_sym_db.RegisterMessage(EvalConfig)\n\n\n_EVALCONFIG.fields_by_name[\'num_examples\'].has_options = True\n_EVALCONFIG.fields_by_name[\'num_examples\']._options = _descriptor._ParseOptions(descriptor_pb2.FieldOptions(), _b(\'\\030\\001\'))\n_EVALCONFIG.fields_by_name[\'max_evals\'].has_options = True\n_EVALCONFIG.fields_by_name[\'max_evals\']._options = _descriptor._ParseOptions(descriptor_pb2.FieldOptions(), _b(\'\\030\\001\'))\n# @@protoc_insertion_point(module_scope)\n'"
rastervision/protos/tf_object_detection/faster_rcnn_box_coder_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: rastervision/protos/tf_object_detection/faster_rcnn_box_coder.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'rastervision/protos/tf_object_detection/faster_rcnn_box_coder.proto\',\n  package=\'rastervision.protos.tf_object_detection\',\n  syntax=\'proto2\',\n  serialized_pb=_b(\'\\nCrastervision/protos/tf_object_detection/faster_rcnn_box_coder.proto\\x12\\\'rastervision.protos.tf_object_detection\\""o\\n\\x12\\x46\\x61sterRcnnBoxCoder\\x12\\x13\\n\\x07y_scale\\x18\\x01 \\x01(\\x02:\\x02\\x31\\x30\\x12\\x13\\n\\x07x_scale\\x18\\x02 \\x01(\\x02:\\x02\\x31\\x30\\x12\\x17\\n\\x0cheight_scale\\x18\\x03 \\x01(\\x02:\\x01\\x35\\x12\\x16\\n\\x0bwidth_scale\\x18\\x04 \\x01(\\x02:\\x01\\x35\')\n)\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\n\n\n\n_FASTERRCNNBOXCODER = _descriptor.Descriptor(\n  name=\'FasterRcnnBoxCoder\',\n  full_name=\'rastervision.protos.tf_object_detection.FasterRcnnBoxCoder\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'y_scale\', full_name=\'rastervision.protos.tf_object_detection.FasterRcnnBoxCoder.y_scale\', index=0,\n      number=1, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(10),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'x_scale\', full_name=\'rastervision.protos.tf_object_detection.FasterRcnnBoxCoder.x_scale\', index=1,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(10),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'height_scale\', full_name=\'rastervision.protos.tf_object_detection.FasterRcnnBoxCoder.height_scale\', index=2,\n      number=3, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(5),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'width_scale\', full_name=\'rastervision.protos.tf_object_detection.FasterRcnnBoxCoder.width_scale\', index=3,\n      number=4, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(5),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=112,\n  serialized_end=223,\n)\n\nDESCRIPTOR.message_types_by_name[\'FasterRcnnBoxCoder\'] = _FASTERRCNNBOXCODER\n\nFasterRcnnBoxCoder = _reflection.GeneratedProtocolMessageType(\'FasterRcnnBoxCoder\', (_message.Message,), dict(\n  DESCRIPTOR = _FASTERRCNNBOXCODER,\n  __module__ = \'rastervision.protos.tf_object_detection.faster_rcnn_box_coder_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.FasterRcnnBoxCoder)\n  ))\n_sym_db.RegisterMessage(FasterRcnnBoxCoder)\n\n\n# @@protoc_insertion_point(module_scope)\n'"
rastervision/protos/tf_object_detection/faster_rcnn_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: rastervision/protos/tf_object_detection/faster_rcnn.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom rastervision.protos.tf_object_detection import anchor_generator_pb2 as rastervision_dot_protos_dot_tf__object__detection_dot_anchor__generator__pb2\nfrom rastervision.protos.tf_object_detection import box_predictor_pb2 as rastervision_dot_protos_dot_tf__object__detection_dot_box__predictor__pb2\nfrom rastervision.protos.tf_object_detection import hyperparams_pb2 as rastervision_dot_protos_dot_tf__object__detection_dot_hyperparams__pb2\nfrom rastervision.protos.tf_object_detection import image_resizer_pb2 as rastervision_dot_protos_dot_tf__object__detection_dot_image__resizer__pb2\nfrom rastervision.protos.tf_object_detection import losses_pb2 as rastervision_dot_protos_dot_tf__object__detection_dot_losses__pb2\nfrom rastervision.protos.tf_object_detection import post_processing_pb2 as rastervision_dot_protos_dot_tf__object__detection_dot_post__processing__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'rastervision/protos/tf_object_detection/faster_rcnn.proto\',\n  package=\'rastervision.protos.tf_object_detection\',\n  syntax=\'proto2\',\n  serialized_pb=_b(\'\\n9rastervision/protos/tf_object_detection/faster_rcnn.proto\\x12\\\'rastervision.protos.tf_object_detection\\x1a>rastervision/protos/tf_object_detection/anchor_generator.proto\\x1a;rastervision/protos/tf_object_detection/box_predictor.proto\\x1a\\x39rastervision/protos/tf_object_detection/hyperparams.proto\\x1a;rastervision/protos/tf_object_detection/image_resizer.proto\\x1a\\x34rastervision/protos/tf_object_detection/losses.proto\\x1a=rastervision/protos/tf_object_detection/post_processing.proto\\""\\xc3\\x0e\\n\\nFasterRcnn\\x12\\x1b\\n\\x10number_of_stages\\x18\\x01 \\x01(\\x05:\\x01\\x32\\x12\\x13\\n\\x0bnum_classes\\x18\\x03 \\x01(\\x05\\x12L\\n\\rimage_resizer\\x18\\x04 \\x01(\\x0b\\x32\\x35.rastervision.protos.tf_object_detection.ImageResizer\\x12^\\n\\x11\\x66\\x65\\x61ture_extractor\\x18\\x05 \\x01(\\x0b\\x32\\x43.rastervision.protos.tf_object_detection.FasterRcnnFeatureExtractor\\x12^\\n\\x1c\\x66irst_stage_anchor_generator\\x18\\x06 \\x01(\\x0b\\x32\\x38.rastervision.protos.tf_object_detection.AnchorGenerator\\x12\\""\\n\\x17\\x66irst_stage_atrous_rate\\x18\\x07 \\x01(\\x05:\\x01\\x31\\x12h\\n*first_stage_box_predictor_conv_hyperparams\\x18\\x08 \\x01(\\x0b\\x32\\x34.rastervision.protos.tf_object_detection.Hyperparams\\x12\\x30\\n%first_stage_box_predictor_kernel_size\\x18\\t \\x01(\\x05:\\x01\\x33\\x12,\\n\\x1f\\x66irst_stage_box_predictor_depth\\x18\\n \\x01(\\x05:\\x03\\x35\\x31\\x32\\x12\\\'\\n\\x1a\\x66irst_stage_minibatch_size\\x18\\x0b \\x01(\\x05:\\x03\\x32\\x35\\x36\\x12\\x32\\n%first_stage_positive_balance_fraction\\x18\\x0c \\x01(\\x02:\\x03\\x30.5\\x12*\\n\\x1f\\x66irst_stage_nms_score_threshold\\x18\\r \\x01(\\x02:\\x01\\x30\\x12*\\n\\x1d\\x66irst_stage_nms_iou_threshold\\x18\\x0e \\x01(\\x02:\\x03\\x30.7\\x12&\\n\\x19\\x66irst_stage_max_proposals\\x18\\x0f \\x01(\\x05:\\x03\\x33\\x30\\x30\\x12/\\n$first_stage_localization_loss_weight\\x18\\x10 \\x01(\\x02:\\x01\\x31\\x12-\\n\\""first_stage_objectness_loss_weight\\x18\\x11 \\x01(\\x02:\\x01\\x31\\x12\\x19\\n\\x11initial_crop_size\\x18\\x12 \\x01(\\x05\\x12\\x1b\\n\\x13maxpool_kernel_size\\x18\\x13 \\x01(\\x05\\x12\\x16\\n\\x0emaxpool_stride\\x18\\x14 \\x01(\\x05\\x12Y\\n\\x1asecond_stage_box_predictor\\x18\\x15 \\x01(\\x0b\\x32\\x35.rastervision.protos.tf_object_detection.BoxPredictor\\x12#\\n\\x17second_stage_batch_size\\x18\\x16 \\x01(\\x05:\\x02\\x36\\x34\\x12+\\n\\x1dsecond_stage_balance_fraction\\x18\\x17 \\x01(\\x02:\\x04\\x30.25\\x12]\\n\\x1csecond_stage_post_processing\\x18\\x18 \\x01(\\x0b\\x32\\x37.rastervision.protos.tf_object_detection.PostProcessing\\x12\\x30\\n%second_stage_localization_loss_weight\\x18\\x19 \\x01(\\x02:\\x01\\x31\\x12\\x32\\n\\\'second_stage_classification_loss_weight\\x18\\x1a \\x01(\\x02:\\x01\\x31\\x12\\x33\\n(second_stage_mask_prediction_loss_weight\\x18\\x1b \\x01(\\x02:\\x01\\x31\\x12U\\n\\x12hard_example_miner\\x18\\x1c \\x01(\\x0b\\x32\\x39.rastervision.protos.tf_object_detection.HardExampleMiner\\x12\\x65\\n second_stage_classification_loss\\x18\\x1d \\x01(\\x0b\\x32;.rastervision.protos.tf_object_detection.ClassificationLoss\\x12\\\'\\n\\x18inplace_batchnorm_update\\x18\\x1e \\x01(\\x08:\\x05\\x66\\x61lse\\x12)\\n\\x1ause_matmul_crop_and_resize\\x18\\x1f \\x01(\\x08:\\x05\\x66\\x61lse\\x12$\\n\\x15\\x63lip_anchors_to_image\\x18  \\x01(\\x08:\\x05\\x66\\x61lse\\x12+\\n\\x1cuse_matmul_gather_in_matcher\\x18! \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\x30\\n!use_static_balanced_label_sampler\\x18\\"" \\x01(\\x08:\\x05\\x66\\x61lse\\x12 \\n\\x11use_static_shapes\\x18# \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\x1a\\n\\x0cresize_masks\\x18$ \\x01(\\x08:\\x04true\\""x\\n\\x1a\\x46\\x61sterRcnnFeatureExtractor\\x12\\x0c\\n\\x04type\\x18\\x01 \\x01(\\t\\x12\\\'\\n\\x1b\\x66irst_stage_features_stride\\x18\\x02 \\x01(\\x05:\\x02\\x31\\x36\\x12#\\n\\x14\\x62\\x61tch_norm_trainable\\x18\\x03 \\x01(\\x08:\\x05\\x66\\x61lse\')\n  ,\n  dependencies=[rastervision_dot_protos_dot_tf__object__detection_dot_anchor__generator__pb2.DESCRIPTOR,rastervision_dot_protos_dot_tf__object__detection_dot_box__predictor__pb2.DESCRIPTOR,rastervision_dot_protos_dot_tf__object__detection_dot_hyperparams__pb2.DESCRIPTOR,rastervision_dot_protos_dot_tf__object__detection_dot_image__resizer__pb2.DESCRIPTOR,rastervision_dot_protos_dot_tf__object__detection_dot_losses__pb2.DESCRIPTOR,rastervision_dot_protos_dot_tf__object__detection_dot_post__processing__pb2.DESCRIPTOR,])\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\n\n\n\n_FASTERRCNN = _descriptor.Descriptor(\n  name=\'FasterRcnn\',\n  full_name=\'rastervision.protos.tf_object_detection.FasterRcnn\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'number_of_stages\', full_name=\'rastervision.protos.tf_object_detection.FasterRcnn.number_of_stages\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=2,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'num_classes\', full_name=\'rastervision.protos.tf_object_detection.FasterRcnn.num_classes\', index=1,\n      number=3, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'image_resizer\', full_name=\'rastervision.protos.tf_object_detection.FasterRcnn.image_resizer\', index=2,\n      number=4, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'feature_extractor\', full_name=\'rastervision.protos.tf_object_detection.FasterRcnn.feature_extractor\', index=3,\n      number=5, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'first_stage_anchor_generator\', full_name=\'rastervision.protos.tf_object_detection.FasterRcnn.first_stage_anchor_generator\', index=4,\n      number=6, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'first_stage_atrous_rate\', full_name=\'rastervision.protos.tf_object_detection.FasterRcnn.first_stage_atrous_rate\', index=5,\n      number=7, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'first_stage_box_predictor_conv_hyperparams\', full_name=\'rastervision.protos.tf_object_detection.FasterRcnn.first_stage_box_predictor_conv_hyperparams\', index=6,\n      number=8, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'first_stage_box_predictor_kernel_size\', full_name=\'rastervision.protos.tf_object_detection.FasterRcnn.first_stage_box_predictor_kernel_size\', index=7,\n      number=9, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=3,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'first_stage_box_predictor_depth\', full_name=\'rastervision.protos.tf_object_detection.FasterRcnn.first_stage_box_predictor_depth\', index=8,\n      number=10, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=512,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'first_stage_minibatch_size\', full_name=\'rastervision.protos.tf_object_detection.FasterRcnn.first_stage_minibatch_size\', index=9,\n      number=11, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=256,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'first_stage_positive_balance_fraction\', full_name=\'rastervision.protos.tf_object_detection.FasterRcnn.first_stage_positive_balance_fraction\', index=10,\n      number=12, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.5),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'first_stage_nms_score_threshold\', full_name=\'rastervision.protos.tf_object_detection.FasterRcnn.first_stage_nms_score_threshold\', index=11,\n      number=13, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'first_stage_nms_iou_threshold\', full_name=\'rastervision.protos.tf_object_detection.FasterRcnn.first_stage_nms_iou_threshold\', index=12,\n      number=14, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.7),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'first_stage_max_proposals\', full_name=\'rastervision.protos.tf_object_detection.FasterRcnn.first_stage_max_proposals\', index=13,\n      number=15, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=300,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'first_stage_localization_loss_weight\', full_name=\'rastervision.protos.tf_object_detection.FasterRcnn.first_stage_localization_loss_weight\', index=14,\n      number=16, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(1),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'first_stage_objectness_loss_weight\', full_name=\'rastervision.protos.tf_object_detection.FasterRcnn.first_stage_objectness_loss_weight\', index=15,\n      number=17, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(1),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'initial_crop_size\', full_name=\'rastervision.protos.tf_object_detection.FasterRcnn.initial_crop_size\', index=16,\n      number=18, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'maxpool_kernel_size\', full_name=\'rastervision.protos.tf_object_detection.FasterRcnn.maxpool_kernel_size\', index=17,\n      number=19, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'maxpool_stride\', full_name=\'rastervision.protos.tf_object_detection.FasterRcnn.maxpool_stride\', index=18,\n      number=20, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'second_stage_box_predictor\', full_name=\'rastervision.protos.tf_object_detection.FasterRcnn.second_stage_box_predictor\', index=19,\n      number=21, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'second_stage_batch_size\', full_name=\'rastervision.protos.tf_object_detection.FasterRcnn.second_stage_batch_size\', index=20,\n      number=22, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=64,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'second_stage_balance_fraction\', full_name=\'rastervision.protos.tf_object_detection.FasterRcnn.second_stage_balance_fraction\', index=21,\n      number=23, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.25),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'second_stage_post_processing\', full_name=\'rastervision.protos.tf_object_detection.FasterRcnn.second_stage_post_processing\', index=22,\n      number=24, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'second_stage_localization_loss_weight\', full_name=\'rastervision.protos.tf_object_detection.FasterRcnn.second_stage_localization_loss_weight\', index=23,\n      number=25, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(1),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'second_stage_classification_loss_weight\', full_name=\'rastervision.protos.tf_object_detection.FasterRcnn.second_stage_classification_loss_weight\', index=24,\n      number=26, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(1),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'second_stage_mask_prediction_loss_weight\', full_name=\'rastervision.protos.tf_object_detection.FasterRcnn.second_stage_mask_prediction_loss_weight\', index=25,\n      number=27, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(1),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'hard_example_miner\', full_name=\'rastervision.protos.tf_object_detection.FasterRcnn.hard_example_miner\', index=26,\n      number=28, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'second_stage_classification_loss\', full_name=\'rastervision.protos.tf_object_detection.FasterRcnn.second_stage_classification_loss\', index=27,\n      number=29, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'inplace_batchnorm_update\', full_name=\'rastervision.protos.tf_object_detection.FasterRcnn.inplace_batchnorm_update\', index=28,\n      number=30, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'use_matmul_crop_and_resize\', full_name=\'rastervision.protos.tf_object_detection.FasterRcnn.use_matmul_crop_and_resize\', index=29,\n      number=31, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'clip_anchors_to_image\', full_name=\'rastervision.protos.tf_object_detection.FasterRcnn.clip_anchors_to_image\', index=30,\n      number=32, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'use_matmul_gather_in_matcher\', full_name=\'rastervision.protos.tf_object_detection.FasterRcnn.use_matmul_gather_in_matcher\', index=31,\n      number=33, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'use_static_balanced_label_sampler\', full_name=\'rastervision.protos.tf_object_detection.FasterRcnn.use_static_balanced_label_sampler\', index=32,\n      number=34, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'use_static_shapes\', full_name=\'rastervision.protos.tf_object_detection.FasterRcnn.use_static_shapes\', index=33,\n      number=35, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'resize_masks\', full_name=\'rastervision.protos.tf_object_detection.FasterRcnn.resize_masks\', index=34,\n      number=36, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=True,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=465,\n  serialized_end=2324,\n)\n\n\n_FASTERRCNNFEATUREEXTRACTOR = _descriptor.Descriptor(\n  name=\'FasterRcnnFeatureExtractor\',\n  full_name=\'rastervision.protos.tf_object_detection.FasterRcnnFeatureExtractor\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'type\', full_name=\'rastervision.protos.tf_object_detection.FasterRcnnFeatureExtractor.type\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'first_stage_features_stride\', full_name=\'rastervision.protos.tf_object_detection.FasterRcnnFeatureExtractor.first_stage_features_stride\', index=1,\n      number=2, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=16,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'batch_norm_trainable\', full_name=\'rastervision.protos.tf_object_detection.FasterRcnnFeatureExtractor.batch_norm_trainable\', index=2,\n      number=3, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=2326,\n  serialized_end=2446,\n)\n\n_FASTERRCNN.fields_by_name[\'image_resizer\'].message_type = rastervision_dot_protos_dot_tf__object__detection_dot_image__resizer__pb2._IMAGERESIZER\n_FASTERRCNN.fields_by_name[\'feature_extractor\'].message_type = _FASTERRCNNFEATUREEXTRACTOR\n_FASTERRCNN.fields_by_name[\'first_stage_anchor_generator\'].message_type = rastervision_dot_protos_dot_tf__object__detection_dot_anchor__generator__pb2._ANCHORGENERATOR\n_FASTERRCNN.fields_by_name[\'first_stage_box_predictor_conv_hyperparams\'].message_type = rastervision_dot_protos_dot_tf__object__detection_dot_hyperparams__pb2._HYPERPARAMS\n_FASTERRCNN.fields_by_name[\'second_stage_box_predictor\'].message_type = rastervision_dot_protos_dot_tf__object__detection_dot_box__predictor__pb2._BOXPREDICTOR\n_FASTERRCNN.fields_by_name[\'second_stage_post_processing\'].message_type = rastervision_dot_protos_dot_tf__object__detection_dot_post__processing__pb2._POSTPROCESSING\n_FASTERRCNN.fields_by_name[\'hard_example_miner\'].message_type = rastervision_dot_protos_dot_tf__object__detection_dot_losses__pb2._HARDEXAMPLEMINER\n_FASTERRCNN.fields_by_name[\'second_stage_classification_loss\'].message_type = rastervision_dot_protos_dot_tf__object__detection_dot_losses__pb2._CLASSIFICATIONLOSS\nDESCRIPTOR.message_types_by_name[\'FasterRcnn\'] = _FASTERRCNN\nDESCRIPTOR.message_types_by_name[\'FasterRcnnFeatureExtractor\'] = _FASTERRCNNFEATUREEXTRACTOR\n\nFasterRcnn = _reflection.GeneratedProtocolMessageType(\'FasterRcnn\', (_message.Message,), dict(\n  DESCRIPTOR = _FASTERRCNN,\n  __module__ = \'rastervision.protos.tf_object_detection.faster_rcnn_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.FasterRcnn)\n  ))\n_sym_db.RegisterMessage(FasterRcnn)\n\nFasterRcnnFeatureExtractor = _reflection.GeneratedProtocolMessageType(\'FasterRcnnFeatureExtractor\', (_message.Message,), dict(\n  DESCRIPTOR = _FASTERRCNNFEATUREEXTRACTOR,\n  __module__ = \'rastervision.protos.tf_object_detection.faster_rcnn_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.FasterRcnnFeatureExtractor)\n  ))\n_sym_db.RegisterMessage(FasterRcnnFeatureExtractor)\n\n\n# @@protoc_insertion_point(module_scope)\n'"
rastervision/protos/tf_object_detection/graph_rewriter_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: rastervision/protos/tf_object_detection/graph_rewriter.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'rastervision/protos/tf_object_detection/graph_rewriter.proto\',\n  package=\'rastervision.protos.tf_object_detection\',\n  syntax=\'proto2\',\n  serialized_pb=_b(\'\\n<rastervision/protos/tf_object_detection/graph_rewriter.proto\\x12\\\'rastervision.protos.tf_object_detection\\""\\\\\\n\\rGraphRewriter\\x12K\\n\\x0cquantization\\x18\\x01 \\x01(\\x0b\\x32\\x35.rastervision.protos.tf_object_detection.Quantization\\""Y\\n\\x0cQuantization\\x12\\x15\\n\\x05\\x64\\x65lay\\x18\\x01 \\x01(\\x05:\\x06\\x35\\x30\\x30\\x30\\x30\\x30\\x12\\x16\\n\\x0bweight_bits\\x18\\x02 \\x01(\\x05:\\x01\\x38\\x12\\x1a\\n\\x0f\\x61\\x63tivation_bits\\x18\\x03 \\x01(\\x05:\\x01\\x38\')\n)\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\n\n\n\n_GRAPHREWRITER = _descriptor.Descriptor(\n  name=\'GraphRewriter\',\n  full_name=\'rastervision.protos.tf_object_detection.GraphRewriter\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'quantization\', full_name=\'rastervision.protos.tf_object_detection.GraphRewriter.quantization\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=105,\n  serialized_end=197,\n)\n\n\n_QUANTIZATION = _descriptor.Descriptor(\n  name=\'Quantization\',\n  full_name=\'rastervision.protos.tf_object_detection.Quantization\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'delay\', full_name=\'rastervision.protos.tf_object_detection.Quantization.delay\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=500000,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'weight_bits\', full_name=\'rastervision.protos.tf_object_detection.Quantization.weight_bits\', index=1,\n      number=2, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=8,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'activation_bits\', full_name=\'rastervision.protos.tf_object_detection.Quantization.activation_bits\', index=2,\n      number=3, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=8,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=199,\n  serialized_end=288,\n)\n\n_GRAPHREWRITER.fields_by_name[\'quantization\'].message_type = _QUANTIZATION\nDESCRIPTOR.message_types_by_name[\'GraphRewriter\'] = _GRAPHREWRITER\nDESCRIPTOR.message_types_by_name[\'Quantization\'] = _QUANTIZATION\n\nGraphRewriter = _reflection.GeneratedProtocolMessageType(\'GraphRewriter\', (_message.Message,), dict(\n  DESCRIPTOR = _GRAPHREWRITER,\n  __module__ = \'rastervision.protos.tf_object_detection.graph_rewriter_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.GraphRewriter)\n  ))\n_sym_db.RegisterMessage(GraphRewriter)\n\nQuantization = _reflection.GeneratedProtocolMessageType(\'Quantization\', (_message.Message,), dict(\n  DESCRIPTOR = _QUANTIZATION,\n  __module__ = \'rastervision.protos.tf_object_detection.graph_rewriter_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.Quantization)\n  ))\n_sym_db.RegisterMessage(Quantization)\n\n\n# @@protoc_insertion_point(module_scope)\n'"
rastervision/protos/tf_object_detection/grid_anchor_generator_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: rastervision/protos/tf_object_detection/grid_anchor_generator.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'rastervision/protos/tf_object_detection/grid_anchor_generator.proto\',\n  package=\'rastervision.protos.tf_object_detection\',\n  syntax=\'proto2\',\n  serialized_pb=_b(\'\\nCrastervision/protos/tf_object_detection/grid_anchor_generator.proto\\x12\\\'rastervision.protos.tf_object_detection\\""\\xcd\\x01\\n\\x13GridAnchorGenerator\\x12\\x13\\n\\x06height\\x18\\x01 \\x01(\\x05:\\x03\\x32\\x35\\x36\\x12\\x12\\n\\x05width\\x18\\x02 \\x01(\\x05:\\x03\\x32\\x35\\x36\\x12\\x19\\n\\rheight_stride\\x18\\x03 \\x01(\\x05:\\x02\\x31\\x36\\x12\\x18\\n\\x0cwidth_stride\\x18\\x04 \\x01(\\x05:\\x02\\x31\\x36\\x12\\x18\\n\\rheight_offset\\x18\\x05 \\x01(\\x05:\\x01\\x30\\x12\\x17\\n\\x0cwidth_offset\\x18\\x06 \\x01(\\x05:\\x01\\x30\\x12\\x0e\\n\\x06scales\\x18\\x07 \\x03(\\x02\\x12\\x15\\n\\raspect_ratios\\x18\\x08 \\x03(\\x02\')\n)\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\n\n\n\n_GRIDANCHORGENERATOR = _descriptor.Descriptor(\n  name=\'GridAnchorGenerator\',\n  full_name=\'rastervision.protos.tf_object_detection.GridAnchorGenerator\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'height\', full_name=\'rastervision.protos.tf_object_detection.GridAnchorGenerator.height\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=256,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'width\', full_name=\'rastervision.protos.tf_object_detection.GridAnchorGenerator.width\', index=1,\n      number=2, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=256,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'height_stride\', full_name=\'rastervision.protos.tf_object_detection.GridAnchorGenerator.height_stride\', index=2,\n      number=3, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=16,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'width_stride\', full_name=\'rastervision.protos.tf_object_detection.GridAnchorGenerator.width_stride\', index=3,\n      number=4, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=16,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'height_offset\', full_name=\'rastervision.protos.tf_object_detection.GridAnchorGenerator.height_offset\', index=4,\n      number=5, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'width_offset\', full_name=\'rastervision.protos.tf_object_detection.GridAnchorGenerator.width_offset\', index=5,\n      number=6, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'scales\', full_name=\'rastervision.protos.tf_object_detection.GridAnchorGenerator.scales\', index=6,\n      number=7, type=2, cpp_type=6, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'aspect_ratios\', full_name=\'rastervision.protos.tf_object_detection.GridAnchorGenerator.aspect_ratios\', index=7,\n      number=8, type=2, cpp_type=6, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=113,\n  serialized_end=318,\n)\n\nDESCRIPTOR.message_types_by_name[\'GridAnchorGenerator\'] = _GRIDANCHORGENERATOR\n\nGridAnchorGenerator = _reflection.GeneratedProtocolMessageType(\'GridAnchorGenerator\', (_message.Message,), dict(\n  DESCRIPTOR = _GRIDANCHORGENERATOR,\n  __module__ = \'rastervision.protos.tf_object_detection.grid_anchor_generator_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.GridAnchorGenerator)\n  ))\n_sym_db.RegisterMessage(GridAnchorGenerator)\n\n\n# @@protoc_insertion_point(module_scope)\n'"
rastervision/protos/tf_object_detection/hyperparams_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: rastervision/protos/tf_object_detection/hyperparams.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'rastervision/protos/tf_object_detection/hyperparams.proto\',\n  package=\'rastervision.protos.tf_object_detection\',\n  syntax=\'proto2\',\n  serialized_pb=_b(\'\\n9rastervision/protos/tf_object_detection/hyperparams.proto\\x12\\\'rastervision.protos.tf_object_detection\\""\\xfc\\x03\\n\\x0bHyperparams\\x12I\\n\\x02op\\x18\\x01 \\x01(\\x0e\\x32\\x37.rastervision.protos.tf_object_detection.Hyperparams.Op:\\x04\\x43ONV\\x12I\\n\\x0bregularizer\\x18\\x02 \\x01(\\x0b\\x32\\x34.rastervision.protos.tf_object_detection.Regularizer\\x12I\\n\\x0binitializer\\x18\\x03 \\x01(\\x0b\\x32\\x34.rastervision.protos.tf_object_detection.Initializer\\x12Y\\n\\nactivation\\x18\\x04 \\x01(\\x0e\\x32?.rastervision.protos.tf_object_detection.Hyperparams.Activation:\\x04RELU\\x12\\x46\\n\\nbatch_norm\\x18\\x05 \\x01(\\x0b\\x32\\x32.rastervision.protos.tf_object_detection.BatchNorm\\x12#\\n\\x14regularize_depthwise\\x18\\x06 \\x01(\\x08:\\x05\\x66\\x61lse\\""\\x16\\n\\x02Op\\x12\\x08\\n\\x04\\x43ONV\\x10\\x01\\x12\\x06\\n\\x02\\x46\\x43\\x10\\x02\\"",\\n\\nActivation\\x12\\x08\\n\\x04NONE\\x10\\x00\\x12\\x08\\n\\x04RELU\\x10\\x01\\x12\\n\\n\\x06RELU_6\\x10\\x02\\""\\xc6\\x01\\n\\x0bRegularizer\\x12P\\n\\x0el1_regularizer\\x18\\x01 \\x01(\\x0b\\x32\\x36.rastervision.protos.tf_object_detection.L1RegularizerH\\x00\\x12P\\n\\x0el2_regularizer\\x18\\x02 \\x01(\\x0b\\x32\\x36.rastervision.protos.tf_object_detection.L2RegularizerH\\x00\\x42\\x13\\n\\x11regularizer_oneof\\""\\""\\n\\rL1Regularizer\\x12\\x11\\n\\x06weight\\x18\\x01 \\x01(\\x02:\\x01\\x31\\""\\""\\n\\rL2Regularizer\\x12\\x11\\n\\x06weight\\x18\\x01 \\x01(\\x02:\\x01\\x31\\""\\xe3\\x02\\n\\x0bInitializer\\x12k\\n\\x1ctruncated_normal_initializer\\x18\\x01 \\x01(\\x0b\\x32\\x43.rastervision.protos.tf_object_detection.TruncatedNormalInitializerH\\x00\\x12k\\n\\x1cvariance_scaling_initializer\\x18\\x02 \\x01(\\x0b\\x32\\x43.rastervision.protos.tf_object_detection.VarianceScalingInitializerH\\x00\\x12\\x65\\n\\x19random_normal_initializer\\x18\\x03 \\x01(\\x0b\\x32@.rastervision.protos.tf_object_detection.RandomNormalInitializerH\\x00\\x42\\x13\\n\\x11initializer_oneof\\""@\\n\\x1aTruncatedNormalInitializer\\x12\\x0f\\n\\x04mean\\x18\\x01 \\x01(\\x02:\\x01\\x30\\x12\\x11\\n\\x06stddev\\x18\\x02 \\x01(\\x02:\\x01\\x31\\""\\xd5\\x01\\n\\x1aVarianceScalingInitializer\\x12\\x11\\n\\x06\\x66\\x61\\x63tor\\x18\\x01 \\x01(\\x02:\\x01\\x32\\x12\\x16\\n\\x07uniform\\x18\\x02 \\x01(\\x08:\\x05\\x66\\x61lse\\x12^\\n\\x04mode\\x18\\x03 \\x01(\\x0e\\x32H.rastervision.protos.tf_object_detection.VarianceScalingInitializer.Mode:\\x06\\x46\\x41N_IN\\"",\\n\\x04Mode\\x12\\n\\n\\x06\\x46\\x41N_IN\\x10\\x00\\x12\\x0b\\n\\x07\\x46\\x41N_OUT\\x10\\x01\\x12\\x0b\\n\\x07\\x46\\x41N_AVG\\x10\\x02\\""=\\n\\x17RandomNormalInitializer\\x12\\x0f\\n\\x04mean\\x18\\x01 \\x01(\\x02:\\x01\\x30\\x12\\x11\\n\\x06stddev\\x18\\x02 \\x01(\\x02:\\x01\\x31\\""z\\n\\tBatchNorm\\x12\\x14\\n\\x05\\x64\\x65\\x63\\x61y\\x18\\x01 \\x01(\\x02:\\x05\\x30.999\\x12\\x14\\n\\x06\\x63\\x65nter\\x18\\x02 \\x01(\\x08:\\x04true\\x12\\x14\\n\\x05scale\\x18\\x03 \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\x16\\n\\x07\\x65psilon\\x18\\x04 \\x01(\\x02:\\x05\\x30.001\\x12\\x13\\n\\x05train\\x18\\x05 \\x01(\\x08:\\x04true\')\n)\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\n\n\n_HYPERPARAMS_OP = _descriptor.EnumDescriptor(\n  name=\'Op\',\n  full_name=\'rastervision.protos.tf_object_detection.Hyperparams.Op\',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name=\'CONV\', index=0, number=1,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'FC\', index=1, number=2,\n      options=None,\n      type=None),\n  ],\n  containing_type=None,\n  options=None,\n  serialized_start=543,\n  serialized_end=565,\n)\n_sym_db.RegisterEnumDescriptor(_HYPERPARAMS_OP)\n\n_HYPERPARAMS_ACTIVATION = _descriptor.EnumDescriptor(\n  name=\'Activation\',\n  full_name=\'rastervision.protos.tf_object_detection.Hyperparams.Activation\',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name=\'NONE\', index=0, number=0,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'RELU\', index=1, number=1,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'RELU_6\', index=2, number=2,\n      options=None,\n      type=None),\n  ],\n  containing_type=None,\n  options=None,\n  serialized_start=567,\n  serialized_end=611,\n)\n_sym_db.RegisterEnumDescriptor(_HYPERPARAMS_ACTIVATION)\n\n_VARIANCESCALINGINITIALIZER_MODE = _descriptor.EnumDescriptor(\n  name=\'Mode\',\n  full_name=\'rastervision.protos.tf_object_detection.VarianceScalingInitializer.Mode\',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name=\'FAN_IN\', index=0, number=0,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'FAN_OUT\', index=1, number=1,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'FAN_AVG\', index=2, number=2,\n      options=None,\n      type=None),\n  ],\n  containing_type=None,\n  options=None,\n  serialized_start=1480,\n  serialized_end=1524,\n)\n_sym_db.RegisterEnumDescriptor(_VARIANCESCALINGINITIALIZER_MODE)\n\n\n_HYPERPARAMS = _descriptor.Descriptor(\n  name=\'Hyperparams\',\n  full_name=\'rastervision.protos.tf_object_detection.Hyperparams\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'op\', full_name=\'rastervision.protos.tf_object_detection.Hyperparams.op\', index=0,\n      number=1, type=14, cpp_type=8, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'regularizer\', full_name=\'rastervision.protos.tf_object_detection.Hyperparams.regularizer\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'initializer\', full_name=\'rastervision.protos.tf_object_detection.Hyperparams.initializer\', index=2,\n      number=3, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'activation\', full_name=\'rastervision.protos.tf_object_detection.Hyperparams.activation\', index=3,\n      number=4, type=14, cpp_type=8, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'batch_norm\', full_name=\'rastervision.protos.tf_object_detection.Hyperparams.batch_norm\', index=4,\n      number=5, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'regularize_depthwise\', full_name=\'rastervision.protos.tf_object_detection.Hyperparams.regularize_depthwise\', index=5,\n      number=6, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n    _HYPERPARAMS_OP,\n    _HYPERPARAMS_ACTIVATION,\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=103,\n  serialized_end=611,\n)\n\n\n_REGULARIZER = _descriptor.Descriptor(\n  name=\'Regularizer\',\n  full_name=\'rastervision.protos.tf_object_detection.Regularizer\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'l1_regularizer\', full_name=\'rastervision.protos.tf_object_detection.Regularizer.l1_regularizer\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'l2_regularizer\', full_name=\'rastervision.protos.tf_object_detection.Regularizer.l2_regularizer\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n    _descriptor.OneofDescriptor(\n      name=\'regularizer_oneof\', full_name=\'rastervision.protos.tf_object_detection.Regularizer.regularizer_oneof\',\n      index=0, containing_type=None, fields=[]),\n  ],\n  serialized_start=614,\n  serialized_end=812,\n)\n\n\n_L1REGULARIZER = _descriptor.Descriptor(\n  name=\'L1Regularizer\',\n  full_name=\'rastervision.protos.tf_object_detection.L1Regularizer\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'weight\', full_name=\'rastervision.protos.tf_object_detection.L1Regularizer.weight\', index=0,\n      number=1, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(1),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=814,\n  serialized_end=848,\n)\n\n\n_L2REGULARIZER = _descriptor.Descriptor(\n  name=\'L2Regularizer\',\n  full_name=\'rastervision.protos.tf_object_detection.L2Regularizer\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'weight\', full_name=\'rastervision.protos.tf_object_detection.L2Regularizer.weight\', index=0,\n      number=1, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(1),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=850,\n  serialized_end=884,\n)\n\n\n_INITIALIZER = _descriptor.Descriptor(\n  name=\'Initializer\',\n  full_name=\'rastervision.protos.tf_object_detection.Initializer\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'truncated_normal_initializer\', full_name=\'rastervision.protos.tf_object_detection.Initializer.truncated_normal_initializer\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'variance_scaling_initializer\', full_name=\'rastervision.protos.tf_object_detection.Initializer.variance_scaling_initializer\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'random_normal_initializer\', full_name=\'rastervision.protos.tf_object_detection.Initializer.random_normal_initializer\', index=2,\n      number=3, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n    _descriptor.OneofDescriptor(\n      name=\'initializer_oneof\', full_name=\'rastervision.protos.tf_object_detection.Initializer.initializer_oneof\',\n      index=0, containing_type=None, fields=[]),\n  ],\n  serialized_start=887,\n  serialized_end=1242,\n)\n\n\n_TRUNCATEDNORMALINITIALIZER = _descriptor.Descriptor(\n  name=\'TruncatedNormalInitializer\',\n  full_name=\'rastervision.protos.tf_object_detection.TruncatedNormalInitializer\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'mean\', full_name=\'rastervision.protos.tf_object_detection.TruncatedNormalInitializer.mean\', index=0,\n      number=1, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'stddev\', full_name=\'rastervision.protos.tf_object_detection.TruncatedNormalInitializer.stddev\', index=1,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(1),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1244,\n  serialized_end=1308,\n)\n\n\n_VARIANCESCALINGINITIALIZER = _descriptor.Descriptor(\n  name=\'VarianceScalingInitializer\',\n  full_name=\'rastervision.protos.tf_object_detection.VarianceScalingInitializer\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'factor\', full_name=\'rastervision.protos.tf_object_detection.VarianceScalingInitializer.factor\', index=0,\n      number=1, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(2),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'uniform\', full_name=\'rastervision.protos.tf_object_detection.VarianceScalingInitializer.uniform\', index=1,\n      number=2, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'mode\', full_name=\'rastervision.protos.tf_object_detection.VarianceScalingInitializer.mode\', index=2,\n      number=3, type=14, cpp_type=8, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n    _VARIANCESCALINGINITIALIZER_MODE,\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1311,\n  serialized_end=1524,\n)\n\n\n_RANDOMNORMALINITIALIZER = _descriptor.Descriptor(\n  name=\'RandomNormalInitializer\',\n  full_name=\'rastervision.protos.tf_object_detection.RandomNormalInitializer\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'mean\', full_name=\'rastervision.protos.tf_object_detection.RandomNormalInitializer.mean\', index=0,\n      number=1, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'stddev\', full_name=\'rastervision.protos.tf_object_detection.RandomNormalInitializer.stddev\', index=1,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(1),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1526,\n  serialized_end=1587,\n)\n\n\n_BATCHNORM = _descriptor.Descriptor(\n  name=\'BatchNorm\',\n  full_name=\'rastervision.protos.tf_object_detection.BatchNorm\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'decay\', full_name=\'rastervision.protos.tf_object_detection.BatchNorm.decay\', index=0,\n      number=1, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.999),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'center\', full_name=\'rastervision.protos.tf_object_detection.BatchNorm.center\', index=1,\n      number=2, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=True,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'scale\', full_name=\'rastervision.protos.tf_object_detection.BatchNorm.scale\', index=2,\n      number=3, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'epsilon\', full_name=\'rastervision.protos.tf_object_detection.BatchNorm.epsilon\', index=3,\n      number=4, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.001),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'train\', full_name=\'rastervision.protos.tf_object_detection.BatchNorm.train\', index=4,\n      number=5, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=True,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1589,\n  serialized_end=1711,\n)\n\n_HYPERPARAMS.fields_by_name[\'op\'].enum_type = _HYPERPARAMS_OP\n_HYPERPARAMS.fields_by_name[\'regularizer\'].message_type = _REGULARIZER\n_HYPERPARAMS.fields_by_name[\'initializer\'].message_type = _INITIALIZER\n_HYPERPARAMS.fields_by_name[\'activation\'].enum_type = _HYPERPARAMS_ACTIVATION\n_HYPERPARAMS.fields_by_name[\'batch_norm\'].message_type = _BATCHNORM\n_HYPERPARAMS_OP.containing_type = _HYPERPARAMS\n_HYPERPARAMS_ACTIVATION.containing_type = _HYPERPARAMS\n_REGULARIZER.fields_by_name[\'l1_regularizer\'].message_type = _L1REGULARIZER\n_REGULARIZER.fields_by_name[\'l2_regularizer\'].message_type = _L2REGULARIZER\n_REGULARIZER.oneofs_by_name[\'regularizer_oneof\'].fields.append(\n  _REGULARIZER.fields_by_name[\'l1_regularizer\'])\n_REGULARIZER.fields_by_name[\'l1_regularizer\'].containing_oneof = _REGULARIZER.oneofs_by_name[\'regularizer_oneof\']\n_REGULARIZER.oneofs_by_name[\'regularizer_oneof\'].fields.append(\n  _REGULARIZER.fields_by_name[\'l2_regularizer\'])\n_REGULARIZER.fields_by_name[\'l2_regularizer\'].containing_oneof = _REGULARIZER.oneofs_by_name[\'regularizer_oneof\']\n_INITIALIZER.fields_by_name[\'truncated_normal_initializer\'].message_type = _TRUNCATEDNORMALINITIALIZER\n_INITIALIZER.fields_by_name[\'variance_scaling_initializer\'].message_type = _VARIANCESCALINGINITIALIZER\n_INITIALIZER.fields_by_name[\'random_normal_initializer\'].message_type = _RANDOMNORMALINITIALIZER\n_INITIALIZER.oneofs_by_name[\'initializer_oneof\'].fields.append(\n  _INITIALIZER.fields_by_name[\'truncated_normal_initializer\'])\n_INITIALIZER.fields_by_name[\'truncated_normal_initializer\'].containing_oneof = _INITIALIZER.oneofs_by_name[\'initializer_oneof\']\n_INITIALIZER.oneofs_by_name[\'initializer_oneof\'].fields.append(\n  _INITIALIZER.fields_by_name[\'variance_scaling_initializer\'])\n_INITIALIZER.fields_by_name[\'variance_scaling_initializer\'].containing_oneof = _INITIALIZER.oneofs_by_name[\'initializer_oneof\']\n_INITIALIZER.oneofs_by_name[\'initializer_oneof\'].fields.append(\n  _INITIALIZER.fields_by_name[\'random_normal_initializer\'])\n_INITIALIZER.fields_by_name[\'random_normal_initializer\'].containing_oneof = _INITIALIZER.oneofs_by_name[\'initializer_oneof\']\n_VARIANCESCALINGINITIALIZER.fields_by_name[\'mode\'].enum_type = _VARIANCESCALINGINITIALIZER_MODE\n_VARIANCESCALINGINITIALIZER_MODE.containing_type = _VARIANCESCALINGINITIALIZER\nDESCRIPTOR.message_types_by_name[\'Hyperparams\'] = _HYPERPARAMS\nDESCRIPTOR.message_types_by_name[\'Regularizer\'] = _REGULARIZER\nDESCRIPTOR.message_types_by_name[\'L1Regularizer\'] = _L1REGULARIZER\nDESCRIPTOR.message_types_by_name[\'L2Regularizer\'] = _L2REGULARIZER\nDESCRIPTOR.message_types_by_name[\'Initializer\'] = _INITIALIZER\nDESCRIPTOR.message_types_by_name[\'TruncatedNormalInitializer\'] = _TRUNCATEDNORMALINITIALIZER\nDESCRIPTOR.message_types_by_name[\'VarianceScalingInitializer\'] = _VARIANCESCALINGINITIALIZER\nDESCRIPTOR.message_types_by_name[\'RandomNormalInitializer\'] = _RANDOMNORMALINITIALIZER\nDESCRIPTOR.message_types_by_name[\'BatchNorm\'] = _BATCHNORM\n\nHyperparams = _reflection.GeneratedProtocolMessageType(\'Hyperparams\', (_message.Message,), dict(\n  DESCRIPTOR = _HYPERPARAMS,\n  __module__ = \'rastervision.protos.tf_object_detection.hyperparams_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.Hyperparams)\n  ))\n_sym_db.RegisterMessage(Hyperparams)\n\nRegularizer = _reflection.GeneratedProtocolMessageType(\'Regularizer\', (_message.Message,), dict(\n  DESCRIPTOR = _REGULARIZER,\n  __module__ = \'rastervision.protos.tf_object_detection.hyperparams_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.Regularizer)\n  ))\n_sym_db.RegisterMessage(Regularizer)\n\nL1Regularizer = _reflection.GeneratedProtocolMessageType(\'L1Regularizer\', (_message.Message,), dict(\n  DESCRIPTOR = _L1REGULARIZER,\n  __module__ = \'rastervision.protos.tf_object_detection.hyperparams_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.L1Regularizer)\n  ))\n_sym_db.RegisterMessage(L1Regularizer)\n\nL2Regularizer = _reflection.GeneratedProtocolMessageType(\'L2Regularizer\', (_message.Message,), dict(\n  DESCRIPTOR = _L2REGULARIZER,\n  __module__ = \'rastervision.protos.tf_object_detection.hyperparams_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.L2Regularizer)\n  ))\n_sym_db.RegisterMessage(L2Regularizer)\n\nInitializer = _reflection.GeneratedProtocolMessageType(\'Initializer\', (_message.Message,), dict(\n  DESCRIPTOR = _INITIALIZER,\n  __module__ = \'rastervision.protos.tf_object_detection.hyperparams_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.Initializer)\n  ))\n_sym_db.RegisterMessage(Initializer)\n\nTruncatedNormalInitializer = _reflection.GeneratedProtocolMessageType(\'TruncatedNormalInitializer\', (_message.Message,), dict(\n  DESCRIPTOR = _TRUNCATEDNORMALINITIALIZER,\n  __module__ = \'rastervision.protos.tf_object_detection.hyperparams_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.TruncatedNormalInitializer)\n  ))\n_sym_db.RegisterMessage(TruncatedNormalInitializer)\n\nVarianceScalingInitializer = _reflection.GeneratedProtocolMessageType(\'VarianceScalingInitializer\', (_message.Message,), dict(\n  DESCRIPTOR = _VARIANCESCALINGINITIALIZER,\n  __module__ = \'rastervision.protos.tf_object_detection.hyperparams_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.VarianceScalingInitializer)\n  ))\n_sym_db.RegisterMessage(VarianceScalingInitializer)\n\nRandomNormalInitializer = _reflection.GeneratedProtocolMessageType(\'RandomNormalInitializer\', (_message.Message,), dict(\n  DESCRIPTOR = _RANDOMNORMALINITIALIZER,\n  __module__ = \'rastervision.protos.tf_object_detection.hyperparams_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.RandomNormalInitializer)\n  ))\n_sym_db.RegisterMessage(RandomNormalInitializer)\n\nBatchNorm = _reflection.GeneratedProtocolMessageType(\'BatchNorm\', (_message.Message,), dict(\n  DESCRIPTOR = _BATCHNORM,\n  __module__ = \'rastervision.protos.tf_object_detection.hyperparams_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.BatchNorm)\n  ))\n_sym_db.RegisterMessage(BatchNorm)\n\n\n# @@protoc_insertion_point(module_scope)\n'"
rastervision/protos/tf_object_detection/image_resizer_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: rastervision/protos/tf_object_detection/image_resizer.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf.internal import enum_type_wrapper\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'rastervision/protos/tf_object_detection/image_resizer.proto\',\n  package=\'rastervision.protos.tf_object_detection\',\n  syntax=\'proto2\',\n  serialized_pb=_b(\'\\n;rastervision/protos/tf_object_detection/image_resizer.proto\\x12\\\'rastervision.protos.tf_object_detection\\""\\xe6\\x01\\n\\x0cImageResizer\\x12\\x64\\n\\x19keep_aspect_ratio_resizer\\x18\\x01 \\x01(\\x0b\\x32?.rastervision.protos.tf_object_detection.KeepAspectRatioResizerH\\x00\\x12Y\\n\\x13\\x66ixed_shape_resizer\\x18\\x02 \\x01(\\x0b\\x32:.rastervision.protos.tf_object_detection.FixedShapeResizerH\\x00\\x42\\x15\\n\\x13image_resizer_oneof\\""\\x90\\x02\\n\\x16KeepAspectRatioResizer\\x12\\x1a\\n\\rmin_dimension\\x18\\x01 \\x01(\\x05:\\x03\\x36\\x30\\x30\\x12\\x1b\\n\\rmax_dimension\\x18\\x02 \\x01(\\x05:\\x04\\x31\\x30\\x32\\x34\\x12T\\n\\rresize_method\\x18\\x03 \\x01(\\x0e\\x32\\x33.rastervision.protos.tf_object_detection.ResizeType:\\x08\\x42ILINEAR\\x12#\\n\\x14pad_to_max_dimension\\x18\\x04 \\x01(\\x08:\\x05\\x66\\x61lse\\x12#\\n\\x14\\x63onvert_to_grayscale\\x18\\x05 \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\x1d\\n\\x15per_channel_pad_value\\x18\\x06 \\x03(\\x02\\""\\xb7\\x01\\n\\x11\\x46ixedShapeResizer\\x12\\x13\\n\\x06height\\x18\\x01 \\x01(\\x05:\\x03\\x33\\x30\\x30\\x12\\x12\\n\\x05width\\x18\\x02 \\x01(\\x05:\\x03\\x33\\x30\\x30\\x12T\\n\\rresize_method\\x18\\x03 \\x01(\\x0e\\x32\\x33.rastervision.protos.tf_object_detection.ResizeType:\\x08\\x42ILINEAR\\x12#\\n\\x14\\x63onvert_to_grayscale\\x18\\x04 \\x01(\\x08:\\x05\\x66\\x61lse*G\\n\\nResizeType\\x12\\x0c\\n\\x08\\x42ILINEAR\\x10\\x00\\x12\\x14\\n\\x10NEAREST_NEIGHBOR\\x10\\x01\\x12\\x0b\\n\\x07\\x42ICUBIC\\x10\\x02\\x12\\x08\\n\\x04\\x41REA\\x10\\x03\')\n)\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\n_RESIZETYPE = _descriptor.EnumDescriptor(\n  name=\'ResizeType\',\n  full_name=\'rastervision.protos.tf_object_detection.ResizeType\',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name=\'BILINEAR\', index=0, number=0,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'NEAREST_NEIGHBOR\', index=1, number=1,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'BICUBIC\', index=2, number=2,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'AREA\', index=3, number=3,\n      options=None,\n      type=None),\n  ],\n  containing_type=None,\n  options=None,\n  serialized_start=798,\n  serialized_end=869,\n)\n_sym_db.RegisterEnumDescriptor(_RESIZETYPE)\n\nResizeType = enum_type_wrapper.EnumTypeWrapper(_RESIZETYPE)\nBILINEAR = 0\nNEAREST_NEIGHBOR = 1\nBICUBIC = 2\nAREA = 3\n\n\n\n_IMAGERESIZER = _descriptor.Descriptor(\n  name=\'ImageResizer\',\n  full_name=\'rastervision.protos.tf_object_detection.ImageResizer\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'keep_aspect_ratio_resizer\', full_name=\'rastervision.protos.tf_object_detection.ImageResizer.keep_aspect_ratio_resizer\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'fixed_shape_resizer\', full_name=\'rastervision.protos.tf_object_detection.ImageResizer.fixed_shape_resizer\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n    _descriptor.OneofDescriptor(\n      name=\'image_resizer_oneof\', full_name=\'rastervision.protos.tf_object_detection.ImageResizer.image_resizer_oneof\',\n      index=0, containing_type=None, fields=[]),\n  ],\n  serialized_start=105,\n  serialized_end=335,\n)\n\n\n_KEEPASPECTRATIORESIZER = _descriptor.Descriptor(\n  name=\'KeepAspectRatioResizer\',\n  full_name=\'rastervision.protos.tf_object_detection.KeepAspectRatioResizer\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'min_dimension\', full_name=\'rastervision.protos.tf_object_detection.KeepAspectRatioResizer.min_dimension\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=600,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'max_dimension\', full_name=\'rastervision.protos.tf_object_detection.KeepAspectRatioResizer.max_dimension\', index=1,\n      number=2, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=1024,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'resize_method\', full_name=\'rastervision.protos.tf_object_detection.KeepAspectRatioResizer.resize_method\', index=2,\n      number=3, type=14, cpp_type=8, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'pad_to_max_dimension\', full_name=\'rastervision.protos.tf_object_detection.KeepAspectRatioResizer.pad_to_max_dimension\', index=3,\n      number=4, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'convert_to_grayscale\', full_name=\'rastervision.protos.tf_object_detection.KeepAspectRatioResizer.convert_to_grayscale\', index=4,\n      number=5, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'per_channel_pad_value\', full_name=\'rastervision.protos.tf_object_detection.KeepAspectRatioResizer.per_channel_pad_value\', index=5,\n      number=6, type=2, cpp_type=6, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=338,\n  serialized_end=610,\n)\n\n\n_FIXEDSHAPERESIZER = _descriptor.Descriptor(\n  name=\'FixedShapeResizer\',\n  full_name=\'rastervision.protos.tf_object_detection.FixedShapeResizer\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'height\', full_name=\'rastervision.protos.tf_object_detection.FixedShapeResizer.height\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=300,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'width\', full_name=\'rastervision.protos.tf_object_detection.FixedShapeResizer.width\', index=1,\n      number=2, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=300,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'resize_method\', full_name=\'rastervision.protos.tf_object_detection.FixedShapeResizer.resize_method\', index=2,\n      number=3, type=14, cpp_type=8, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'convert_to_grayscale\', full_name=\'rastervision.protos.tf_object_detection.FixedShapeResizer.convert_to_grayscale\', index=3,\n      number=4, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=613,\n  serialized_end=796,\n)\n\n_IMAGERESIZER.fields_by_name[\'keep_aspect_ratio_resizer\'].message_type = _KEEPASPECTRATIORESIZER\n_IMAGERESIZER.fields_by_name[\'fixed_shape_resizer\'].message_type = _FIXEDSHAPERESIZER\n_IMAGERESIZER.oneofs_by_name[\'image_resizer_oneof\'].fields.append(\n  _IMAGERESIZER.fields_by_name[\'keep_aspect_ratio_resizer\'])\n_IMAGERESIZER.fields_by_name[\'keep_aspect_ratio_resizer\'].containing_oneof = _IMAGERESIZER.oneofs_by_name[\'image_resizer_oneof\']\n_IMAGERESIZER.oneofs_by_name[\'image_resizer_oneof\'].fields.append(\n  _IMAGERESIZER.fields_by_name[\'fixed_shape_resizer\'])\n_IMAGERESIZER.fields_by_name[\'fixed_shape_resizer\'].containing_oneof = _IMAGERESIZER.oneofs_by_name[\'image_resizer_oneof\']\n_KEEPASPECTRATIORESIZER.fields_by_name[\'resize_method\'].enum_type = _RESIZETYPE\n_FIXEDSHAPERESIZER.fields_by_name[\'resize_method\'].enum_type = _RESIZETYPE\nDESCRIPTOR.message_types_by_name[\'ImageResizer\'] = _IMAGERESIZER\nDESCRIPTOR.message_types_by_name[\'KeepAspectRatioResizer\'] = _KEEPASPECTRATIORESIZER\nDESCRIPTOR.message_types_by_name[\'FixedShapeResizer\'] = _FIXEDSHAPERESIZER\nDESCRIPTOR.enum_types_by_name[\'ResizeType\'] = _RESIZETYPE\n\nImageResizer = _reflection.GeneratedProtocolMessageType(\'ImageResizer\', (_message.Message,), dict(\n  DESCRIPTOR = _IMAGERESIZER,\n  __module__ = \'rastervision.protos.tf_object_detection.image_resizer_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.ImageResizer)\n  ))\n_sym_db.RegisterMessage(ImageResizer)\n\nKeepAspectRatioResizer = _reflection.GeneratedProtocolMessageType(\'KeepAspectRatioResizer\', (_message.Message,), dict(\n  DESCRIPTOR = _KEEPASPECTRATIORESIZER,\n  __module__ = \'rastervision.protos.tf_object_detection.image_resizer_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.KeepAspectRatioResizer)\n  ))\n_sym_db.RegisterMessage(KeepAspectRatioResizer)\n\nFixedShapeResizer = _reflection.GeneratedProtocolMessageType(\'FixedShapeResizer\', (_message.Message,), dict(\n  DESCRIPTOR = _FIXEDSHAPERESIZER,\n  __module__ = \'rastervision.protos.tf_object_detection.image_resizer_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.FixedShapeResizer)\n  ))\n_sym_db.RegisterMessage(FixedShapeResizer)\n\n\n# @@protoc_insertion_point(module_scope)\n'"
rastervision/protos/tf_object_detection/input_reader_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: rastervision/protos/tf_object_detection/input_reader.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf.internal import enum_type_wrapper\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'rastervision/protos/tf_object_detection/input_reader.proto\',\n  package=\'rastervision.protos.tf_object_detection\',\n  syntax=\'proto2\',\n  serialized_pb=_b(\'\\n:rastervision/protos/tf_object_detection/input_reader.proto\\x12\\\'rastervision.protos.tf_object_detection\\""\\xb3\\x07\\n\\x0bInputReader\\x12\\x0e\\n\\x04name\\x18\\x17 \\x01(\\t:\\x00\\x12\\x18\\n\\x0elabel_map_path\\x18\\x01 \\x01(\\t:\\x00\\x12\\x15\\n\\x07shuffle\\x18\\x02 \\x01(\\x08:\\x04true\\x12!\\n\\x13shuffle_buffer_size\\x18\\x0b \\x01(\\r:\\x04\\x32\\x30\\x34\\x38\\x12*\\n\\x1d\\x66ilenames_shuffle_buffer_size\\x18\\x0c \\x01(\\r:\\x03\\x31\\x30\\x30\\x12\\x15\\n\\nnum_epochs\\x18\\x05 \\x01(\\r:\\x01\\x30\\x12!\\n\\x16sample_1_of_n_examples\\x18\\x16 \\x01(\\r:\\x01\\x31\\x12\\x17\\n\\x0bnum_readers\\x18\\x06 \\x01(\\r:\\x02\\x36\\x34\\x12\\x1f\\n\\x14num_parallel_batches\\x18\\x13 \\x01(\\r:\\x01\\x38\\x12\\x1f\\n\\x14num_prefetch_batches\\x18\\x14 \\x01(\\x05:\\x01\\x32\\x12 \\n\\x0equeue_capacity\\x18\\x03 \\x01(\\r:\\x04\\x32\\x30\\x30\\x30\\x42\\x02\\x18\\x01\\x12#\\n\\x11min_after_dequeue\\x18\\x04 \\x01(\\r:\\x04\\x31\\x30\\x30\\x30\\x42\\x02\\x18\\x01\\x12\\x1d\\n\\x11read_block_length\\x18\\x0f \\x01(\\r:\\x02\\x33\\x32\\x12\\x1e\\n\\rprefetch_size\\x18\\r \\x01(\\r:\\x03\\x35\\x31\\x32\\x42\\x02\\x18\\x01\\x12&\\n\\x16num_parallel_map_calls\\x18\\x0e \\x01(\\r:\\x02\\x36\\x34\\x42\\x02\\x18\\x01\\x12\\""\\n\\x17num_additional_channels\\x18\\x12 \\x01(\\x05:\\x01\\x30\\x12\\x18\\n\\rnum_keypoints\\x18\\x10 \\x01(\\r:\\x01\\x30\\x12 \\n\\x13max_number_of_boxes\\x18\\x15 \\x01(\\x05:\\x03\\x31\\x30\\x30\\x12\\""\\n\\x13load_instance_masks\\x18\\x07 \\x01(\\x08:\\x05\\x66\\x61lse\\x12]\\n\\tmask_type\\x18\\n \\x01(\\x0e\\x32\\x39.rastervision.protos.tf_object_detection.InstanceMaskType:\\x0fNUMERICAL_MASKS\\x12\\x1f\\n\\x10use_display_name\\x18\\x11 \\x01(\\x08:\\x05\\x66\\x61lse\\x12^\\n\\x16tf_record_input_reader\\x18\\x08 \\x01(\\x0b\\x32<.rastervision.protos.tf_object_detection.TFRecordInputReaderH\\x00\\x12]\\n\\x15\\x65xternal_input_reader\\x18\\t \\x01(\\x0b\\x32<.rastervision.protos.tf_object_detection.ExternalInputReaderH\\x00\\x42\\x0e\\n\\x0cinput_reader\\"")\\n\\x13TFRecordInputReader\\x12\\x12\\n\\ninput_path\\x18\\x01 \\x03(\\t\\""\\x1c\\n\\x13\\x45xternalInputReader*\\x05\\x08\\x01\\x10\\xe8\\x07*C\\n\\x10InstanceMaskType\\x12\\x0b\\n\\x07\\x44\\x45\\x46\\x41ULT\\x10\\x00\\x12\\x13\\n\\x0fNUMERICAL_MASKS\\x10\\x01\\x12\\r\\n\\tPNG_MASKS\\x10\\x02\')\n)\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\n_INSTANCEMASKTYPE = _descriptor.EnumDescriptor(\n  name=\'InstanceMaskType\',\n  full_name=\'rastervision.protos.tf_object_detection.InstanceMaskType\',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name=\'DEFAULT\', index=0, number=0,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'NUMERICAL_MASKS\', index=1, number=1,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'PNG_MASKS\', index=2, number=2,\n      options=None,\n      type=None),\n  ],\n  containing_type=None,\n  options=None,\n  serialized_start=1126,\n  serialized_end=1193,\n)\n_sym_db.RegisterEnumDescriptor(_INSTANCEMASKTYPE)\n\nInstanceMaskType = enum_type_wrapper.EnumTypeWrapper(_INSTANCEMASKTYPE)\nDEFAULT = 0\nNUMERICAL_MASKS = 1\nPNG_MASKS = 2\n\n\n\n_INPUTREADER = _descriptor.Descriptor(\n  name=\'InputReader\',\n  full_name=\'rastervision.protos.tf_object_detection.InputReader\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'name\', full_name=\'rastervision.protos.tf_object_detection.InputReader.name\', index=0,\n      number=23, type=9, cpp_type=9, label=1,\n      has_default_value=True, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'label_map_path\', full_name=\'rastervision.protos.tf_object_detection.InputReader.label_map_path\', index=1,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=True, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'shuffle\', full_name=\'rastervision.protos.tf_object_detection.InputReader.shuffle\', index=2,\n      number=2, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=True,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'shuffle_buffer_size\', full_name=\'rastervision.protos.tf_object_detection.InputReader.shuffle_buffer_size\', index=3,\n      number=11, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=2048,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'filenames_shuffle_buffer_size\', full_name=\'rastervision.protos.tf_object_detection.InputReader.filenames_shuffle_buffer_size\', index=4,\n      number=12, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=100,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'num_epochs\', full_name=\'rastervision.protos.tf_object_detection.InputReader.num_epochs\', index=5,\n      number=5, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'sample_1_of_n_examples\', full_name=\'rastervision.protos.tf_object_detection.InputReader.sample_1_of_n_examples\', index=6,\n      number=22, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'num_readers\', full_name=\'rastervision.protos.tf_object_detection.InputReader.num_readers\', index=7,\n      number=6, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=64,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'num_parallel_batches\', full_name=\'rastervision.protos.tf_object_detection.InputReader.num_parallel_batches\', index=8,\n      number=19, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=8,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'num_prefetch_batches\', full_name=\'rastervision.protos.tf_object_detection.InputReader.num_prefetch_batches\', index=9,\n      number=20, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=2,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'queue_capacity\', full_name=\'rastervision.protos.tf_object_detection.InputReader.queue_capacity\', index=10,\n      number=3, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=2000,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=_descriptor._ParseOptions(descriptor_pb2.FieldOptions(), _b(\'\\030\\001\'))),\n    _descriptor.FieldDescriptor(\n      name=\'min_after_dequeue\', full_name=\'rastervision.protos.tf_object_detection.InputReader.min_after_dequeue\', index=11,\n      number=4, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=1000,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=_descriptor._ParseOptions(descriptor_pb2.FieldOptions(), _b(\'\\030\\001\'))),\n    _descriptor.FieldDescriptor(\n      name=\'read_block_length\', full_name=\'rastervision.protos.tf_object_detection.InputReader.read_block_length\', index=12,\n      number=15, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=32,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'prefetch_size\', full_name=\'rastervision.protos.tf_object_detection.InputReader.prefetch_size\', index=13,\n      number=13, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=512,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=_descriptor._ParseOptions(descriptor_pb2.FieldOptions(), _b(\'\\030\\001\'))),\n    _descriptor.FieldDescriptor(\n      name=\'num_parallel_map_calls\', full_name=\'rastervision.protos.tf_object_detection.InputReader.num_parallel_map_calls\', index=14,\n      number=14, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=64,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=_descriptor._ParseOptions(descriptor_pb2.FieldOptions(), _b(\'\\030\\001\'))),\n    _descriptor.FieldDescriptor(\n      name=\'num_additional_channels\', full_name=\'rastervision.protos.tf_object_detection.InputReader.num_additional_channels\', index=15,\n      number=18, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'num_keypoints\', full_name=\'rastervision.protos.tf_object_detection.InputReader.num_keypoints\', index=16,\n      number=16, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'max_number_of_boxes\', full_name=\'rastervision.protos.tf_object_detection.InputReader.max_number_of_boxes\', index=17,\n      number=21, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=100,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'load_instance_masks\', full_name=\'rastervision.protos.tf_object_detection.InputReader.load_instance_masks\', index=18,\n      number=7, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'mask_type\', full_name=\'rastervision.protos.tf_object_detection.InputReader.mask_type\', index=19,\n      number=10, type=14, cpp_type=8, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'use_display_name\', full_name=\'rastervision.protos.tf_object_detection.InputReader.use_display_name\', index=20,\n      number=17, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'tf_record_input_reader\', full_name=\'rastervision.protos.tf_object_detection.InputReader.tf_record_input_reader\', index=21,\n      number=8, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'external_input_reader\', full_name=\'rastervision.protos.tf_object_detection.InputReader.external_input_reader\', index=22,\n      number=9, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n    _descriptor.OneofDescriptor(\n      name=\'input_reader\', full_name=\'rastervision.protos.tf_object_detection.InputReader.input_reader\',\n      index=0, containing_type=None, fields=[]),\n  ],\n  serialized_start=104,\n  serialized_end=1051,\n)\n\n\n_TFRECORDINPUTREADER = _descriptor.Descriptor(\n  name=\'TFRecordInputReader\',\n  full_name=\'rastervision.protos.tf_object_detection.TFRecordInputReader\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'input_path\', full_name=\'rastervision.protos.tf_object_detection.TFRecordInputReader.input_path\', index=0,\n      number=1, type=9, cpp_type=9, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1053,\n  serialized_end=1094,\n)\n\n\n_EXTERNALINPUTREADER = _descriptor.Descriptor(\n  name=\'ExternalInputReader\',\n  full_name=\'rastervision.protos.tf_object_detection.ExternalInputReader\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=True,\n  syntax=\'proto2\',\n  extension_ranges=[(1, 1000), ],\n  oneofs=[\n  ],\n  serialized_start=1096,\n  serialized_end=1124,\n)\n\n_INPUTREADER.fields_by_name[\'mask_type\'].enum_type = _INSTANCEMASKTYPE\n_INPUTREADER.fields_by_name[\'tf_record_input_reader\'].message_type = _TFRECORDINPUTREADER\n_INPUTREADER.fields_by_name[\'external_input_reader\'].message_type = _EXTERNALINPUTREADER\n_INPUTREADER.oneofs_by_name[\'input_reader\'].fields.append(\n  _INPUTREADER.fields_by_name[\'tf_record_input_reader\'])\n_INPUTREADER.fields_by_name[\'tf_record_input_reader\'].containing_oneof = _INPUTREADER.oneofs_by_name[\'input_reader\']\n_INPUTREADER.oneofs_by_name[\'input_reader\'].fields.append(\n  _INPUTREADER.fields_by_name[\'external_input_reader\'])\n_INPUTREADER.fields_by_name[\'external_input_reader\'].containing_oneof = _INPUTREADER.oneofs_by_name[\'input_reader\']\nDESCRIPTOR.message_types_by_name[\'InputReader\'] = _INPUTREADER\nDESCRIPTOR.message_types_by_name[\'TFRecordInputReader\'] = _TFRECORDINPUTREADER\nDESCRIPTOR.message_types_by_name[\'ExternalInputReader\'] = _EXTERNALINPUTREADER\nDESCRIPTOR.enum_types_by_name[\'InstanceMaskType\'] = _INSTANCEMASKTYPE\n\nInputReader = _reflection.GeneratedProtocolMessageType(\'InputReader\', (_message.Message,), dict(\n  DESCRIPTOR = _INPUTREADER,\n  __module__ = \'rastervision.protos.tf_object_detection.input_reader_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.InputReader)\n  ))\n_sym_db.RegisterMessage(InputReader)\n\nTFRecordInputReader = _reflection.GeneratedProtocolMessageType(\'TFRecordInputReader\', (_message.Message,), dict(\n  DESCRIPTOR = _TFRECORDINPUTREADER,\n  __module__ = \'rastervision.protos.tf_object_detection.input_reader_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.TFRecordInputReader)\n  ))\n_sym_db.RegisterMessage(TFRecordInputReader)\n\nExternalInputReader = _reflection.GeneratedProtocolMessageType(\'ExternalInputReader\', (_message.Message,), dict(\n  DESCRIPTOR = _EXTERNALINPUTREADER,\n  __module__ = \'rastervision.protos.tf_object_detection.input_reader_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.ExternalInputReader)\n  ))\n_sym_db.RegisterMessage(ExternalInputReader)\n\n\n_INPUTREADER.fields_by_name[\'queue_capacity\'].has_options = True\n_INPUTREADER.fields_by_name[\'queue_capacity\']._options = _descriptor._ParseOptions(descriptor_pb2.FieldOptions(), _b(\'\\030\\001\'))\n_INPUTREADER.fields_by_name[\'min_after_dequeue\'].has_options = True\n_INPUTREADER.fields_by_name[\'min_after_dequeue\']._options = _descriptor._ParseOptions(descriptor_pb2.FieldOptions(), _b(\'\\030\\001\'))\n_INPUTREADER.fields_by_name[\'prefetch_size\'].has_options = True\n_INPUTREADER.fields_by_name[\'prefetch_size\']._options = _descriptor._ParseOptions(descriptor_pb2.FieldOptions(), _b(\'\\030\\001\'))\n_INPUTREADER.fields_by_name[\'num_parallel_map_calls\'].has_options = True\n_INPUTREADER.fields_by_name[\'num_parallel_map_calls\']._options = _descriptor._ParseOptions(descriptor_pb2.FieldOptions(), _b(\'\\030\\001\'))\n# @@protoc_insertion_point(module_scope)\n'"
rastervision/protos/tf_object_detection/keypoint_box_coder_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: rastervision/protos/tf_object_detection/keypoint_box_coder.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'rastervision/protos/tf_object_detection/keypoint_box_coder.proto\',\n  package=\'rastervision.protos.tf_object_detection\',\n  syntax=\'proto2\',\n  serialized_pb=_b(\'\\n@rastervision/protos/tf_object_detection/keypoint_box_coder.proto\\x12\\\'rastervision.protos.tf_object_detection\\""\\x84\\x01\\n\\x10KeypointBoxCoder\\x12\\x15\\n\\rnum_keypoints\\x18\\x01 \\x01(\\x05\\x12\\x13\\n\\x07y_scale\\x18\\x02 \\x01(\\x02:\\x02\\x31\\x30\\x12\\x13\\n\\x07x_scale\\x18\\x03 \\x01(\\x02:\\x02\\x31\\x30\\x12\\x17\\n\\x0cheight_scale\\x18\\x04 \\x01(\\x02:\\x01\\x35\\x12\\x16\\n\\x0bwidth_scale\\x18\\x05 \\x01(\\x02:\\x01\\x35\')\n)\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\n\n\n\n_KEYPOINTBOXCODER = _descriptor.Descriptor(\n  name=\'KeypointBoxCoder\',\n  full_name=\'rastervision.protos.tf_object_detection.KeypointBoxCoder\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'num_keypoints\', full_name=\'rastervision.protos.tf_object_detection.KeypointBoxCoder.num_keypoints\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'y_scale\', full_name=\'rastervision.protos.tf_object_detection.KeypointBoxCoder.y_scale\', index=1,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(10),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'x_scale\', full_name=\'rastervision.protos.tf_object_detection.KeypointBoxCoder.x_scale\', index=2,\n      number=3, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(10),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'height_scale\', full_name=\'rastervision.protos.tf_object_detection.KeypointBoxCoder.height_scale\', index=3,\n      number=4, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(5),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'width_scale\', full_name=\'rastervision.protos.tf_object_detection.KeypointBoxCoder.width_scale\', index=4,\n      number=5, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(5),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=110,\n  serialized_end=242,\n)\n\nDESCRIPTOR.message_types_by_name[\'KeypointBoxCoder\'] = _KEYPOINTBOXCODER\n\nKeypointBoxCoder = _reflection.GeneratedProtocolMessageType(\'KeypointBoxCoder\', (_message.Message,), dict(\n  DESCRIPTOR = _KEYPOINTBOXCODER,\n  __module__ = \'rastervision.protos.tf_object_detection.keypoint_box_coder_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.KeypointBoxCoder)\n  ))\n_sym_db.RegisterMessage(KeypointBoxCoder)\n\n\n# @@protoc_insertion_point(module_scope)\n'"
rastervision/protos/tf_object_detection/losses_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: rastervision/protos/tf_object_detection/losses.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'rastervision/protos/tf_object_detection/losses.proto\',\n  package=\'rastervision.protos.tf_object_detection\',\n  syntax=\'proto2\',\n  serialized_pb=_b(\'\\n4rastervision/protos/tf_object_detection/losses.proto\\x12\\\'rastervision.protos.tf_object_detection\\""\\xae\\x03\\n\\x04Loss\\x12T\\n\\x11localization_loss\\x18\\x01 \\x01(\\x0b\\x32\\x39.rastervision.protos.tf_object_detection.LocalizationLoss\\x12X\\n\\x13\\x63lassification_loss\\x18\\x02 \\x01(\\x0b\\x32;.rastervision.protos.tf_object_detection.ClassificationLoss\\x12U\\n\\x12hard_example_miner\\x18\\x03 \\x01(\\x0b\\x32\\x39.rastervision.protos.tf_object_detection.HardExampleMiner\\x12 \\n\\x15\\x63lassification_weight\\x18\\x04 \\x01(\\x02:\\x01\\x31\\x12\\x1e\\n\\x13localization_weight\\x18\\x05 \\x01(\\x02:\\x01\\x31\\x12]\\n\\x16random_example_sampler\\x18\\x06 \\x01(\\x0b\\x32=.rastervision.protos.tf_object_detection.RandomExampleSampler\\""\\xca\\x02\\n\\x10LocalizationLoss\\x12Z\\n\\x0bweighted_l2\\x18\\x01 \\x01(\\x0b\\x32\\x43.rastervision.protos.tf_object_detection.WeightedL2LocalizationLossH\\x00\\x12g\\n\\x12weighted_smooth_l1\\x18\\x02 \\x01(\\x0b\\x32I.rastervision.protos.tf_object_detection.WeightedSmoothL1LocalizationLossH\\x00\\x12\\\\\\n\\x0cweighted_iou\\x18\\x03 \\x01(\\x0b\\x32\\x44.rastervision.protos.tf_object_detection.WeightedIOULocalizationLossH\\x00\\x42\\x13\\n\\x11localization_loss\\"">\\n\\x1aWeightedL2LocalizationLoss\\x12 \\n\\x11\\x61nchorwise_output\\x18\\x01 \\x01(\\x08:\\x05\\x66\\x61lse\\""V\\n WeightedSmoothL1LocalizationLoss\\x12 \\n\\x11\\x61nchorwise_output\\x18\\x01 \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\x10\\n\\x05\\x64\\x65lta\\x18\\x02 \\x01(\\x02:\\x01\\x31\\""\\x1d\\n\\x1bWeightedIOULocalizationLoss\\""\\xd2\\x04\\n\\x12\\x43lassificationLoss\\x12\\x66\\n\\x10weighted_sigmoid\\x18\\x01 \\x01(\\x0b\\x32J.rastervision.protos.tf_object_detection.WeightedSigmoidClassificationLossH\\x00\\x12\\x66\\n\\x10weighted_softmax\\x18\\x02 \\x01(\\x0b\\x32J.rastervision.protos.tf_object_detection.WeightedSoftmaxClassificationLossH\\x00\\x12z\\n\\x17weighted_logits_softmax\\x18\\x05 \\x01(\\x0b\\x32W.rastervision.protos.tf_object_detection.WeightedSoftmaxClassificationAgainstLogitsLossH\\x00\\x12n\\n\\x14\\x62ootstrapped_sigmoid\\x18\\x03 \\x01(\\x0b\\x32N.rastervision.protos.tf_object_detection.BootstrappedSigmoidClassificationLossH\\x00\\x12i\\n\\x16weighted_sigmoid_focal\\x18\\x04 \\x01(\\x0b\\x32G.rastervision.protos.tf_object_detection.SigmoidFocalClassificationLossH\\x00\\x42\\x15\\n\\x13\\x63lassification_loss\\""E\\n!WeightedSigmoidClassificationLoss\\x12 \\n\\x11\\x61nchorwise_output\\x18\\x01 \\x01(\\x08:\\x05\\x66\\x61lse\\""c\\n\\x1eSigmoidFocalClassificationLoss\\x12 \\n\\x11\\x61nchorwise_output\\x18\\x01 \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\x10\\n\\x05gamma\\x18\\x02 \\x01(\\x02:\\x01\\x32\\x12\\r\\n\\x05\\x61lpha\\x18\\x03 \\x01(\\x02\\""]\\n!WeightedSoftmaxClassificationLoss\\x12 \\n\\x11\\x61nchorwise_output\\x18\\x01 \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\x16\\n\\x0blogit_scale\\x18\\x02 \\x01(\\x02:\\x01\\x31\\""j\\n.WeightedSoftmaxClassificationAgainstLogitsLoss\\x12 \\n\\x11\\x61nchorwise_output\\x18\\x01 \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\x16\\n\\x0blogit_scale\\x18\\x02 \\x01(\\x02:\\x01\\x31\\""w\\n%BootstrappedSigmoidClassificationLoss\\x12\\r\\n\\x05\\x61lpha\\x18\\x01 \\x01(\\x02\\x12\\x1d\\n\\x0ehard_bootstrap\\x18\\x02 \\x01(\\x08:\\x05\\x66\\x61lse\\x12 \\n\\x11\\x61nchorwise_output\\x18\\x03 \\x01(\\x08:\\x05\\x66\\x61lse\\""\\xb1\\x02\\n\\x10HardExampleMiner\\x12\\x1d\\n\\x11num_hard_examples\\x18\\x01 \\x01(\\x05:\\x02\\x36\\x34\\x12\\x1a\\n\\riou_threshold\\x18\\x02 \\x01(\\x02:\\x03\\x30.7\\x12[\\n\\tloss_type\\x18\\x03 \\x01(\\x0e\\x32\\x42.rastervision.protos.tf_object_detection.HardExampleMiner.LossType:\\x04\\x42OTH\\x12%\\n\\x1amax_negatives_per_positive\\x18\\x04 \\x01(\\x05:\\x01\\x30\\x12\\""\\n\\x17min_negatives_per_image\\x18\\x05 \\x01(\\x05:\\x01\\x30\\"":\\n\\x08LossType\\x12\\x08\\n\\x04\\x42OTH\\x10\\x00\\x12\\x12\\n\\x0e\\x43LASSIFICATION\\x10\\x01\\x12\\x10\\n\\x0cLOCALIZATION\\x10\\x02\\"">\\n\\x14RandomExampleSampler\\x12&\\n\\x18positive_sample_fraction\\x18\\x01 \\x01(\\x02:\\x04\\x30.01\')\n)\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\n\n\n_HARDEXAMPLEMINER_LOSSTYPE = _descriptor.EnumDescriptor(\n  name=\'LossType\',\n  full_name=\'rastervision.protos.tf_object_detection.HardExampleMiner.LossType\',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name=\'BOTH\', index=0, number=0,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'CLASSIFICATION\', index=1, number=1,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'LOCALIZATION\', index=2, number=2,\n      options=None,\n      type=None),\n  ],\n  containing_type=None,\n  options=None,\n  serialized_start=2387,\n  serialized_end=2445,\n)\n_sym_db.RegisterEnumDescriptor(_HARDEXAMPLEMINER_LOSSTYPE)\n\n\n_LOSS = _descriptor.Descriptor(\n  name=\'Loss\',\n  full_name=\'rastervision.protos.tf_object_detection.Loss\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'localization_loss\', full_name=\'rastervision.protos.tf_object_detection.Loss.localization_loss\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'classification_loss\', full_name=\'rastervision.protos.tf_object_detection.Loss.classification_loss\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'hard_example_miner\', full_name=\'rastervision.protos.tf_object_detection.Loss.hard_example_miner\', index=2,\n      number=3, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'classification_weight\', full_name=\'rastervision.protos.tf_object_detection.Loss.classification_weight\', index=3,\n      number=4, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(1),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'localization_weight\', full_name=\'rastervision.protos.tf_object_detection.Loss.localization_weight\', index=4,\n      number=5, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(1),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'random_example_sampler\', full_name=\'rastervision.protos.tf_object_detection.Loss.random_example_sampler\', index=5,\n      number=6, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=98,\n  serialized_end=528,\n)\n\n\n_LOCALIZATIONLOSS = _descriptor.Descriptor(\n  name=\'LocalizationLoss\',\n  full_name=\'rastervision.protos.tf_object_detection.LocalizationLoss\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'weighted_l2\', full_name=\'rastervision.protos.tf_object_detection.LocalizationLoss.weighted_l2\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'weighted_smooth_l1\', full_name=\'rastervision.protos.tf_object_detection.LocalizationLoss.weighted_smooth_l1\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'weighted_iou\', full_name=\'rastervision.protos.tf_object_detection.LocalizationLoss.weighted_iou\', index=2,\n      number=3, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n    _descriptor.OneofDescriptor(\n      name=\'localization_loss\', full_name=\'rastervision.protos.tf_object_detection.LocalizationLoss.localization_loss\',\n      index=0, containing_type=None, fields=[]),\n  ],\n  serialized_start=531,\n  serialized_end=861,\n)\n\n\n_WEIGHTEDL2LOCALIZATIONLOSS = _descriptor.Descriptor(\n  name=\'WeightedL2LocalizationLoss\',\n  full_name=\'rastervision.protos.tf_object_detection.WeightedL2LocalizationLoss\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'anchorwise_output\', full_name=\'rastervision.protos.tf_object_detection.WeightedL2LocalizationLoss.anchorwise_output\', index=0,\n      number=1, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=863,\n  serialized_end=925,\n)\n\n\n_WEIGHTEDSMOOTHL1LOCALIZATIONLOSS = _descriptor.Descriptor(\n  name=\'WeightedSmoothL1LocalizationLoss\',\n  full_name=\'rastervision.protos.tf_object_detection.WeightedSmoothL1LocalizationLoss\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'anchorwise_output\', full_name=\'rastervision.protos.tf_object_detection.WeightedSmoothL1LocalizationLoss.anchorwise_output\', index=0,\n      number=1, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'delta\', full_name=\'rastervision.protos.tf_object_detection.WeightedSmoothL1LocalizationLoss.delta\', index=1,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(1),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=927,\n  serialized_end=1013,\n)\n\n\n_WEIGHTEDIOULOCALIZATIONLOSS = _descriptor.Descriptor(\n  name=\'WeightedIOULocalizationLoss\',\n  full_name=\'rastervision.protos.tf_object_detection.WeightedIOULocalizationLoss\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1015,\n  serialized_end=1044,\n)\n\n\n_CLASSIFICATIONLOSS = _descriptor.Descriptor(\n  name=\'ClassificationLoss\',\n  full_name=\'rastervision.protos.tf_object_detection.ClassificationLoss\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'weighted_sigmoid\', full_name=\'rastervision.protos.tf_object_detection.ClassificationLoss.weighted_sigmoid\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'weighted_softmax\', full_name=\'rastervision.protos.tf_object_detection.ClassificationLoss.weighted_softmax\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'weighted_logits_softmax\', full_name=\'rastervision.protos.tf_object_detection.ClassificationLoss.weighted_logits_softmax\', index=2,\n      number=5, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'bootstrapped_sigmoid\', full_name=\'rastervision.protos.tf_object_detection.ClassificationLoss.bootstrapped_sigmoid\', index=3,\n      number=3, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'weighted_sigmoid_focal\', full_name=\'rastervision.protos.tf_object_detection.ClassificationLoss.weighted_sigmoid_focal\', index=4,\n      number=4, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n    _descriptor.OneofDescriptor(\n      name=\'classification_loss\', full_name=\'rastervision.protos.tf_object_detection.ClassificationLoss.classification_loss\',\n      index=0, containing_type=None, fields=[]),\n  ],\n  serialized_start=1047,\n  serialized_end=1641,\n)\n\n\n_WEIGHTEDSIGMOIDCLASSIFICATIONLOSS = _descriptor.Descriptor(\n  name=\'WeightedSigmoidClassificationLoss\',\n  full_name=\'rastervision.protos.tf_object_detection.WeightedSigmoidClassificationLoss\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'anchorwise_output\', full_name=\'rastervision.protos.tf_object_detection.WeightedSigmoidClassificationLoss.anchorwise_output\', index=0,\n      number=1, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1643,\n  serialized_end=1712,\n)\n\n\n_SIGMOIDFOCALCLASSIFICATIONLOSS = _descriptor.Descriptor(\n  name=\'SigmoidFocalClassificationLoss\',\n  full_name=\'rastervision.protos.tf_object_detection.SigmoidFocalClassificationLoss\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'anchorwise_output\', full_name=\'rastervision.protos.tf_object_detection.SigmoidFocalClassificationLoss.anchorwise_output\', index=0,\n      number=1, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'gamma\', full_name=\'rastervision.protos.tf_object_detection.SigmoidFocalClassificationLoss.gamma\', index=1,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(2),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'alpha\', full_name=\'rastervision.protos.tf_object_detection.SigmoidFocalClassificationLoss.alpha\', index=2,\n      number=3, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1714,\n  serialized_end=1813,\n)\n\n\n_WEIGHTEDSOFTMAXCLASSIFICATIONLOSS = _descriptor.Descriptor(\n  name=\'WeightedSoftmaxClassificationLoss\',\n  full_name=\'rastervision.protos.tf_object_detection.WeightedSoftmaxClassificationLoss\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'anchorwise_output\', full_name=\'rastervision.protos.tf_object_detection.WeightedSoftmaxClassificationLoss.anchorwise_output\', index=0,\n      number=1, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'logit_scale\', full_name=\'rastervision.protos.tf_object_detection.WeightedSoftmaxClassificationLoss.logit_scale\', index=1,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(1),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1815,\n  serialized_end=1908,\n)\n\n\n_WEIGHTEDSOFTMAXCLASSIFICATIONAGAINSTLOGITSLOSS = _descriptor.Descriptor(\n  name=\'WeightedSoftmaxClassificationAgainstLogitsLoss\',\n  full_name=\'rastervision.protos.tf_object_detection.WeightedSoftmaxClassificationAgainstLogitsLoss\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'anchorwise_output\', full_name=\'rastervision.protos.tf_object_detection.WeightedSoftmaxClassificationAgainstLogitsLoss.anchorwise_output\', index=0,\n      number=1, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'logit_scale\', full_name=\'rastervision.protos.tf_object_detection.WeightedSoftmaxClassificationAgainstLogitsLoss.logit_scale\', index=1,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(1),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1910,\n  serialized_end=2016,\n)\n\n\n_BOOTSTRAPPEDSIGMOIDCLASSIFICATIONLOSS = _descriptor.Descriptor(\n  name=\'BootstrappedSigmoidClassificationLoss\',\n  full_name=\'rastervision.protos.tf_object_detection.BootstrappedSigmoidClassificationLoss\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'alpha\', full_name=\'rastervision.protos.tf_object_detection.BootstrappedSigmoidClassificationLoss.alpha\', index=0,\n      number=1, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'hard_bootstrap\', full_name=\'rastervision.protos.tf_object_detection.BootstrappedSigmoidClassificationLoss.hard_bootstrap\', index=1,\n      number=2, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'anchorwise_output\', full_name=\'rastervision.protos.tf_object_detection.BootstrappedSigmoidClassificationLoss.anchorwise_output\', index=2,\n      number=3, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=2018,\n  serialized_end=2137,\n)\n\n\n_HARDEXAMPLEMINER = _descriptor.Descriptor(\n  name=\'HardExampleMiner\',\n  full_name=\'rastervision.protos.tf_object_detection.HardExampleMiner\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'num_hard_examples\', full_name=\'rastervision.protos.tf_object_detection.HardExampleMiner.num_hard_examples\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=64,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'iou_threshold\', full_name=\'rastervision.protos.tf_object_detection.HardExampleMiner.iou_threshold\', index=1,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.7),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'loss_type\', full_name=\'rastervision.protos.tf_object_detection.HardExampleMiner.loss_type\', index=2,\n      number=3, type=14, cpp_type=8, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'max_negatives_per_positive\', full_name=\'rastervision.protos.tf_object_detection.HardExampleMiner.max_negatives_per_positive\', index=3,\n      number=4, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'min_negatives_per_image\', full_name=\'rastervision.protos.tf_object_detection.HardExampleMiner.min_negatives_per_image\', index=4,\n      number=5, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n    _HARDEXAMPLEMINER_LOSSTYPE,\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=2140,\n  serialized_end=2445,\n)\n\n\n_RANDOMEXAMPLESAMPLER = _descriptor.Descriptor(\n  name=\'RandomExampleSampler\',\n  full_name=\'rastervision.protos.tf_object_detection.RandomExampleSampler\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'positive_sample_fraction\', full_name=\'rastervision.protos.tf_object_detection.RandomExampleSampler.positive_sample_fraction\', index=0,\n      number=1, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.01),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=2447,\n  serialized_end=2509,\n)\n\n_LOSS.fields_by_name[\'localization_loss\'].message_type = _LOCALIZATIONLOSS\n_LOSS.fields_by_name[\'classification_loss\'].message_type = _CLASSIFICATIONLOSS\n_LOSS.fields_by_name[\'hard_example_miner\'].message_type = _HARDEXAMPLEMINER\n_LOSS.fields_by_name[\'random_example_sampler\'].message_type = _RANDOMEXAMPLESAMPLER\n_LOCALIZATIONLOSS.fields_by_name[\'weighted_l2\'].message_type = _WEIGHTEDL2LOCALIZATIONLOSS\n_LOCALIZATIONLOSS.fields_by_name[\'weighted_smooth_l1\'].message_type = _WEIGHTEDSMOOTHL1LOCALIZATIONLOSS\n_LOCALIZATIONLOSS.fields_by_name[\'weighted_iou\'].message_type = _WEIGHTEDIOULOCALIZATIONLOSS\n_LOCALIZATIONLOSS.oneofs_by_name[\'localization_loss\'].fields.append(\n  _LOCALIZATIONLOSS.fields_by_name[\'weighted_l2\'])\n_LOCALIZATIONLOSS.fields_by_name[\'weighted_l2\'].containing_oneof = _LOCALIZATIONLOSS.oneofs_by_name[\'localization_loss\']\n_LOCALIZATIONLOSS.oneofs_by_name[\'localization_loss\'].fields.append(\n  _LOCALIZATIONLOSS.fields_by_name[\'weighted_smooth_l1\'])\n_LOCALIZATIONLOSS.fields_by_name[\'weighted_smooth_l1\'].containing_oneof = _LOCALIZATIONLOSS.oneofs_by_name[\'localization_loss\']\n_LOCALIZATIONLOSS.oneofs_by_name[\'localization_loss\'].fields.append(\n  _LOCALIZATIONLOSS.fields_by_name[\'weighted_iou\'])\n_LOCALIZATIONLOSS.fields_by_name[\'weighted_iou\'].containing_oneof = _LOCALIZATIONLOSS.oneofs_by_name[\'localization_loss\']\n_CLASSIFICATIONLOSS.fields_by_name[\'weighted_sigmoid\'].message_type = _WEIGHTEDSIGMOIDCLASSIFICATIONLOSS\n_CLASSIFICATIONLOSS.fields_by_name[\'weighted_softmax\'].message_type = _WEIGHTEDSOFTMAXCLASSIFICATIONLOSS\n_CLASSIFICATIONLOSS.fields_by_name[\'weighted_logits_softmax\'].message_type = _WEIGHTEDSOFTMAXCLASSIFICATIONAGAINSTLOGITSLOSS\n_CLASSIFICATIONLOSS.fields_by_name[\'bootstrapped_sigmoid\'].message_type = _BOOTSTRAPPEDSIGMOIDCLASSIFICATIONLOSS\n_CLASSIFICATIONLOSS.fields_by_name[\'weighted_sigmoid_focal\'].message_type = _SIGMOIDFOCALCLASSIFICATIONLOSS\n_CLASSIFICATIONLOSS.oneofs_by_name[\'classification_loss\'].fields.append(\n  _CLASSIFICATIONLOSS.fields_by_name[\'weighted_sigmoid\'])\n_CLASSIFICATIONLOSS.fields_by_name[\'weighted_sigmoid\'].containing_oneof = _CLASSIFICATIONLOSS.oneofs_by_name[\'classification_loss\']\n_CLASSIFICATIONLOSS.oneofs_by_name[\'classification_loss\'].fields.append(\n  _CLASSIFICATIONLOSS.fields_by_name[\'weighted_softmax\'])\n_CLASSIFICATIONLOSS.fields_by_name[\'weighted_softmax\'].containing_oneof = _CLASSIFICATIONLOSS.oneofs_by_name[\'classification_loss\']\n_CLASSIFICATIONLOSS.oneofs_by_name[\'classification_loss\'].fields.append(\n  _CLASSIFICATIONLOSS.fields_by_name[\'weighted_logits_softmax\'])\n_CLASSIFICATIONLOSS.fields_by_name[\'weighted_logits_softmax\'].containing_oneof = _CLASSIFICATIONLOSS.oneofs_by_name[\'classification_loss\']\n_CLASSIFICATIONLOSS.oneofs_by_name[\'classification_loss\'].fields.append(\n  _CLASSIFICATIONLOSS.fields_by_name[\'bootstrapped_sigmoid\'])\n_CLASSIFICATIONLOSS.fields_by_name[\'bootstrapped_sigmoid\'].containing_oneof = _CLASSIFICATIONLOSS.oneofs_by_name[\'classification_loss\']\n_CLASSIFICATIONLOSS.oneofs_by_name[\'classification_loss\'].fields.append(\n  _CLASSIFICATIONLOSS.fields_by_name[\'weighted_sigmoid_focal\'])\n_CLASSIFICATIONLOSS.fields_by_name[\'weighted_sigmoid_focal\'].containing_oneof = _CLASSIFICATIONLOSS.oneofs_by_name[\'classification_loss\']\n_HARDEXAMPLEMINER.fields_by_name[\'loss_type\'].enum_type = _HARDEXAMPLEMINER_LOSSTYPE\n_HARDEXAMPLEMINER_LOSSTYPE.containing_type = _HARDEXAMPLEMINER\nDESCRIPTOR.message_types_by_name[\'Loss\'] = _LOSS\nDESCRIPTOR.message_types_by_name[\'LocalizationLoss\'] = _LOCALIZATIONLOSS\nDESCRIPTOR.message_types_by_name[\'WeightedL2LocalizationLoss\'] = _WEIGHTEDL2LOCALIZATIONLOSS\nDESCRIPTOR.message_types_by_name[\'WeightedSmoothL1LocalizationLoss\'] = _WEIGHTEDSMOOTHL1LOCALIZATIONLOSS\nDESCRIPTOR.message_types_by_name[\'WeightedIOULocalizationLoss\'] = _WEIGHTEDIOULOCALIZATIONLOSS\nDESCRIPTOR.message_types_by_name[\'ClassificationLoss\'] = _CLASSIFICATIONLOSS\nDESCRIPTOR.message_types_by_name[\'WeightedSigmoidClassificationLoss\'] = _WEIGHTEDSIGMOIDCLASSIFICATIONLOSS\nDESCRIPTOR.message_types_by_name[\'SigmoidFocalClassificationLoss\'] = _SIGMOIDFOCALCLASSIFICATIONLOSS\nDESCRIPTOR.message_types_by_name[\'WeightedSoftmaxClassificationLoss\'] = _WEIGHTEDSOFTMAXCLASSIFICATIONLOSS\nDESCRIPTOR.message_types_by_name[\'WeightedSoftmaxClassificationAgainstLogitsLoss\'] = _WEIGHTEDSOFTMAXCLASSIFICATIONAGAINSTLOGITSLOSS\nDESCRIPTOR.message_types_by_name[\'BootstrappedSigmoidClassificationLoss\'] = _BOOTSTRAPPEDSIGMOIDCLASSIFICATIONLOSS\nDESCRIPTOR.message_types_by_name[\'HardExampleMiner\'] = _HARDEXAMPLEMINER\nDESCRIPTOR.message_types_by_name[\'RandomExampleSampler\'] = _RANDOMEXAMPLESAMPLER\n\nLoss = _reflection.GeneratedProtocolMessageType(\'Loss\', (_message.Message,), dict(\n  DESCRIPTOR = _LOSS,\n  __module__ = \'rastervision.protos.tf_object_detection.losses_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.Loss)\n  ))\n_sym_db.RegisterMessage(Loss)\n\nLocalizationLoss = _reflection.GeneratedProtocolMessageType(\'LocalizationLoss\', (_message.Message,), dict(\n  DESCRIPTOR = _LOCALIZATIONLOSS,\n  __module__ = \'rastervision.protos.tf_object_detection.losses_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.LocalizationLoss)\n  ))\n_sym_db.RegisterMessage(LocalizationLoss)\n\nWeightedL2LocalizationLoss = _reflection.GeneratedProtocolMessageType(\'WeightedL2LocalizationLoss\', (_message.Message,), dict(\n  DESCRIPTOR = _WEIGHTEDL2LOCALIZATIONLOSS,\n  __module__ = \'rastervision.protos.tf_object_detection.losses_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.WeightedL2LocalizationLoss)\n  ))\n_sym_db.RegisterMessage(WeightedL2LocalizationLoss)\n\nWeightedSmoothL1LocalizationLoss = _reflection.GeneratedProtocolMessageType(\'WeightedSmoothL1LocalizationLoss\', (_message.Message,), dict(\n  DESCRIPTOR = _WEIGHTEDSMOOTHL1LOCALIZATIONLOSS,\n  __module__ = \'rastervision.protos.tf_object_detection.losses_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.WeightedSmoothL1LocalizationLoss)\n  ))\n_sym_db.RegisterMessage(WeightedSmoothL1LocalizationLoss)\n\nWeightedIOULocalizationLoss = _reflection.GeneratedProtocolMessageType(\'WeightedIOULocalizationLoss\', (_message.Message,), dict(\n  DESCRIPTOR = _WEIGHTEDIOULOCALIZATIONLOSS,\n  __module__ = \'rastervision.protos.tf_object_detection.losses_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.WeightedIOULocalizationLoss)\n  ))\n_sym_db.RegisterMessage(WeightedIOULocalizationLoss)\n\nClassificationLoss = _reflection.GeneratedProtocolMessageType(\'ClassificationLoss\', (_message.Message,), dict(\n  DESCRIPTOR = _CLASSIFICATIONLOSS,\n  __module__ = \'rastervision.protos.tf_object_detection.losses_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.ClassificationLoss)\n  ))\n_sym_db.RegisterMessage(ClassificationLoss)\n\nWeightedSigmoidClassificationLoss = _reflection.GeneratedProtocolMessageType(\'WeightedSigmoidClassificationLoss\', (_message.Message,), dict(\n  DESCRIPTOR = _WEIGHTEDSIGMOIDCLASSIFICATIONLOSS,\n  __module__ = \'rastervision.protos.tf_object_detection.losses_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.WeightedSigmoidClassificationLoss)\n  ))\n_sym_db.RegisterMessage(WeightedSigmoidClassificationLoss)\n\nSigmoidFocalClassificationLoss = _reflection.GeneratedProtocolMessageType(\'SigmoidFocalClassificationLoss\', (_message.Message,), dict(\n  DESCRIPTOR = _SIGMOIDFOCALCLASSIFICATIONLOSS,\n  __module__ = \'rastervision.protos.tf_object_detection.losses_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.SigmoidFocalClassificationLoss)\n  ))\n_sym_db.RegisterMessage(SigmoidFocalClassificationLoss)\n\nWeightedSoftmaxClassificationLoss = _reflection.GeneratedProtocolMessageType(\'WeightedSoftmaxClassificationLoss\', (_message.Message,), dict(\n  DESCRIPTOR = _WEIGHTEDSOFTMAXCLASSIFICATIONLOSS,\n  __module__ = \'rastervision.protos.tf_object_detection.losses_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.WeightedSoftmaxClassificationLoss)\n  ))\n_sym_db.RegisterMessage(WeightedSoftmaxClassificationLoss)\n\nWeightedSoftmaxClassificationAgainstLogitsLoss = _reflection.GeneratedProtocolMessageType(\'WeightedSoftmaxClassificationAgainstLogitsLoss\', (_message.Message,), dict(\n  DESCRIPTOR = _WEIGHTEDSOFTMAXCLASSIFICATIONAGAINSTLOGITSLOSS,\n  __module__ = \'rastervision.protos.tf_object_detection.losses_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.WeightedSoftmaxClassificationAgainstLogitsLoss)\n  ))\n_sym_db.RegisterMessage(WeightedSoftmaxClassificationAgainstLogitsLoss)\n\nBootstrappedSigmoidClassificationLoss = _reflection.GeneratedProtocolMessageType(\'BootstrappedSigmoidClassificationLoss\', (_message.Message,), dict(\n  DESCRIPTOR = _BOOTSTRAPPEDSIGMOIDCLASSIFICATIONLOSS,\n  __module__ = \'rastervision.protos.tf_object_detection.losses_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.BootstrappedSigmoidClassificationLoss)\n  ))\n_sym_db.RegisterMessage(BootstrappedSigmoidClassificationLoss)\n\nHardExampleMiner = _reflection.GeneratedProtocolMessageType(\'HardExampleMiner\', (_message.Message,), dict(\n  DESCRIPTOR = _HARDEXAMPLEMINER,\n  __module__ = \'rastervision.protos.tf_object_detection.losses_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.HardExampleMiner)\n  ))\n_sym_db.RegisterMessage(HardExampleMiner)\n\nRandomExampleSampler = _reflection.GeneratedProtocolMessageType(\'RandomExampleSampler\', (_message.Message,), dict(\n  DESCRIPTOR = _RANDOMEXAMPLESAMPLER,\n  __module__ = \'rastervision.protos.tf_object_detection.losses_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.RandomExampleSampler)\n  ))\n_sym_db.RegisterMessage(RandomExampleSampler)\n\n\n# @@protoc_insertion_point(module_scope)\n'"
rastervision/protos/tf_object_detection/matcher_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: rastervision/protos/tf_object_detection/matcher.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom rastervision.protos.tf_object_detection import argmax_matcher_pb2 as rastervision_dot_protos_dot_tf__object__detection_dot_argmax__matcher__pb2\nfrom rastervision.protos.tf_object_detection import bipartite_matcher_pb2 as rastervision_dot_protos_dot_tf__object__detection_dot_bipartite__matcher__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'rastervision/protos/tf_object_detection/matcher.proto\',\n  package=\'rastervision.protos.tf_object_detection\',\n  syntax=\'proto2\',\n  serialized_pb=_b(\'\\n5rastervision/protos/tf_object_detection/matcher.proto\\x12\\\'rastervision.protos.tf_object_detection\\x1a<rastervision/protos/tf_object_detection/argmax_matcher.proto\\x1a?rastervision/protos/tf_object_detection/bipartite_matcher.proto\\""\\xc4\\x01\\n\\x07Matcher\\x12P\\n\\x0e\\x61rgmax_matcher\\x18\\x01 \\x01(\\x0b\\x32\\x36.rastervision.protos.tf_object_detection.ArgMaxMatcherH\\x00\\x12V\\n\\x11\\x62ipartite_matcher\\x18\\x02 \\x01(\\x0b\\x32\\x39.rastervision.protos.tf_object_detection.BipartiteMatcherH\\x00\\x42\\x0f\\n\\rmatcher_oneof\')\n  ,\n  dependencies=[rastervision_dot_protos_dot_tf__object__detection_dot_argmax__matcher__pb2.DESCRIPTOR,rastervision_dot_protos_dot_tf__object__detection_dot_bipartite__matcher__pb2.DESCRIPTOR,])\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\n\n\n\n_MATCHER = _descriptor.Descriptor(\n  name=\'Matcher\',\n  full_name=\'rastervision.protos.tf_object_detection.Matcher\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'argmax_matcher\', full_name=\'rastervision.protos.tf_object_detection.Matcher.argmax_matcher\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'bipartite_matcher\', full_name=\'rastervision.protos.tf_object_detection.Matcher.bipartite_matcher\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n    _descriptor.OneofDescriptor(\n      name=\'matcher_oneof\', full_name=\'rastervision.protos.tf_object_detection.Matcher.matcher_oneof\',\n      index=0, containing_type=None, fields=[]),\n  ],\n  serialized_start=226,\n  serialized_end=422,\n)\n\n_MATCHER.fields_by_name[\'argmax_matcher\'].message_type = rastervision_dot_protos_dot_tf__object__detection_dot_argmax__matcher__pb2._ARGMAXMATCHER\n_MATCHER.fields_by_name[\'bipartite_matcher\'].message_type = rastervision_dot_protos_dot_tf__object__detection_dot_bipartite__matcher__pb2._BIPARTITEMATCHER\n_MATCHER.oneofs_by_name[\'matcher_oneof\'].fields.append(\n  _MATCHER.fields_by_name[\'argmax_matcher\'])\n_MATCHER.fields_by_name[\'argmax_matcher\'].containing_oneof = _MATCHER.oneofs_by_name[\'matcher_oneof\']\n_MATCHER.oneofs_by_name[\'matcher_oneof\'].fields.append(\n  _MATCHER.fields_by_name[\'bipartite_matcher\'])\n_MATCHER.fields_by_name[\'bipartite_matcher\'].containing_oneof = _MATCHER.oneofs_by_name[\'matcher_oneof\']\nDESCRIPTOR.message_types_by_name[\'Matcher\'] = _MATCHER\n\nMatcher = _reflection.GeneratedProtocolMessageType(\'Matcher\', (_message.Message,), dict(\n  DESCRIPTOR = _MATCHER,\n  __module__ = \'rastervision.protos.tf_object_detection.matcher_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.Matcher)\n  ))\n_sym_db.RegisterMessage(Matcher)\n\n\n# @@protoc_insertion_point(module_scope)\n'"
rastervision/protos/tf_object_detection/mean_stddev_box_coder_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: rastervision/protos/tf_object_detection/mean_stddev_box_coder.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'rastervision/protos/tf_object_detection/mean_stddev_box_coder.proto\',\n  package=\'rastervision.protos.tf_object_detection\',\n  syntax=\'proto2\',\n  serialized_pb=_b(\'\\nCrastervision/protos/tf_object_detection/mean_stddev_box_coder.proto\\x12\\\'rastervision.protos.tf_object_detection\\""*\\n\\x12MeanStddevBoxCoder\\x12\\x14\\n\\x06stddev\\x18\\x01 \\x01(\\x02:\\x04\\x30.01\')\n)\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\n\n\n\n_MEANSTDDEVBOXCODER = _descriptor.Descriptor(\n  name=\'MeanStddevBoxCoder\',\n  full_name=\'rastervision.protos.tf_object_detection.MeanStddevBoxCoder\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'stddev\', full_name=\'rastervision.protos.tf_object_detection.MeanStddevBoxCoder.stddev\', index=0,\n      number=1, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.01),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=112,\n  serialized_end=154,\n)\n\nDESCRIPTOR.message_types_by_name[\'MeanStddevBoxCoder\'] = _MEANSTDDEVBOXCODER\n\nMeanStddevBoxCoder = _reflection.GeneratedProtocolMessageType(\'MeanStddevBoxCoder\', (_message.Message,), dict(\n  DESCRIPTOR = _MEANSTDDEVBOXCODER,\n  __module__ = \'rastervision.protos.tf_object_detection.mean_stddev_box_coder_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.MeanStddevBoxCoder)\n  ))\n_sym_db.RegisterMessage(MeanStddevBoxCoder)\n\n\n# @@protoc_insertion_point(module_scope)\n'"
rastervision/protos/tf_object_detection/model_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: rastervision/protos/tf_object_detection/model.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom rastervision.protos.tf_object_detection import faster_rcnn_pb2 as rastervision_dot_protos_dot_tf__object__detection_dot_faster__rcnn__pb2\nfrom rastervision.protos.tf_object_detection import ssd_pb2 as rastervision_dot_protos_dot_tf__object__detection_dot_ssd__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'rastervision/protos/tf_object_detection/model.proto\',\n  package=\'rastervision.protos.tf_object_detection\',\n  syntax=\'proto2\',\n  serialized_pb=_b(\'\\n3rastervision/protos/tf_object_detection/model.proto\\x12\\\'rastervision.protos.tf_object_detection\\x1a\\x39rastervision/protos/tf_object_detection/faster_rcnn.proto\\x1a\\x31rastervision/protos/tf_object_detection/ssd.proto\\""\\xa2\\x01\\n\\x0e\\x44\\x65tectionModel\\x12J\\n\\x0b\\x66\\x61ster_rcnn\\x18\\x01 \\x01(\\x0b\\x32\\x33.rastervision.protos.tf_object_detection.FasterRcnnH\\x00\\x12;\\n\\x03ssd\\x18\\x02 \\x01(\\x0b\\x32,.rastervision.protos.tf_object_detection.SsdH\\x00\\x42\\x07\\n\\x05model\')\n  ,\n  dependencies=[rastervision_dot_protos_dot_tf__object__detection_dot_faster__rcnn__pb2.DESCRIPTOR,rastervision_dot_protos_dot_tf__object__detection_dot_ssd__pb2.DESCRIPTOR,])\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\n\n\n\n_DETECTIONMODEL = _descriptor.Descriptor(\n  name=\'DetectionModel\',\n  full_name=\'rastervision.protos.tf_object_detection.DetectionModel\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'faster_rcnn\', full_name=\'rastervision.protos.tf_object_detection.DetectionModel.faster_rcnn\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'ssd\', full_name=\'rastervision.protos.tf_object_detection.DetectionModel.ssd\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n    _descriptor.OneofDescriptor(\n      name=\'model\', full_name=\'rastervision.protos.tf_object_detection.DetectionModel.model\',\n      index=0, containing_type=None, fields=[]),\n  ],\n  serialized_start=207,\n  serialized_end=369,\n)\n\n_DETECTIONMODEL.fields_by_name[\'faster_rcnn\'].message_type = rastervision_dot_protos_dot_tf__object__detection_dot_faster__rcnn__pb2._FASTERRCNN\n_DETECTIONMODEL.fields_by_name[\'ssd\'].message_type = rastervision_dot_protos_dot_tf__object__detection_dot_ssd__pb2._SSD\n_DETECTIONMODEL.oneofs_by_name[\'model\'].fields.append(\n  _DETECTIONMODEL.fields_by_name[\'faster_rcnn\'])\n_DETECTIONMODEL.fields_by_name[\'faster_rcnn\'].containing_oneof = _DETECTIONMODEL.oneofs_by_name[\'model\']\n_DETECTIONMODEL.oneofs_by_name[\'model\'].fields.append(\n  _DETECTIONMODEL.fields_by_name[\'ssd\'])\n_DETECTIONMODEL.fields_by_name[\'ssd\'].containing_oneof = _DETECTIONMODEL.oneofs_by_name[\'model\']\nDESCRIPTOR.message_types_by_name[\'DetectionModel\'] = _DETECTIONMODEL\n\nDetectionModel = _reflection.GeneratedProtocolMessageType(\'DetectionModel\', (_message.Message,), dict(\n  DESCRIPTOR = _DETECTIONMODEL,\n  __module__ = \'rastervision.protos.tf_object_detection.model_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.DetectionModel)\n  ))\n_sym_db.RegisterMessage(DetectionModel)\n\n\n# @@protoc_insertion_point(module_scope)\n'"
rastervision/protos/tf_object_detection/multiscale_anchor_generator_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: rastervision/protos/tf_object_detection/multiscale_anchor_generator.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'rastervision/protos/tf_object_detection/multiscale_anchor_generator.proto\',\n  package=\'rastervision.protos.tf_object_detection\',\n  syntax=\'proto2\',\n  serialized_pb=_b(\'\\nIrastervision/protos/tf_object_detection/multiscale_anchor_generator.proto\\x12\\\'rastervision.protos.tf_object_detection\\""\\xba\\x01\\n\\x19MultiscaleAnchorGenerator\\x12\\x14\\n\\tmin_level\\x18\\x01 \\x01(\\x05:\\x01\\x33\\x12\\x14\\n\\tmax_level\\x18\\x02 \\x01(\\x05:\\x01\\x37\\x12\\x17\\n\\x0c\\x61nchor_scale\\x18\\x03 \\x01(\\x02:\\x01\\x34\\x12\\x15\\n\\raspect_ratios\\x18\\x04 \\x03(\\x02\\x12\\x1c\\n\\x11scales_per_octave\\x18\\x05 \\x01(\\x05:\\x01\\x32\\x12#\\n\\x15normalize_coordinates\\x18\\x06 \\x01(\\x08:\\x04true\')\n)\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\n\n\n\n_MULTISCALEANCHORGENERATOR = _descriptor.Descriptor(\n  name=\'MultiscaleAnchorGenerator\',\n  full_name=\'rastervision.protos.tf_object_detection.MultiscaleAnchorGenerator\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'min_level\', full_name=\'rastervision.protos.tf_object_detection.MultiscaleAnchorGenerator.min_level\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=3,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'max_level\', full_name=\'rastervision.protos.tf_object_detection.MultiscaleAnchorGenerator.max_level\', index=1,\n      number=2, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=7,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'anchor_scale\', full_name=\'rastervision.protos.tf_object_detection.MultiscaleAnchorGenerator.anchor_scale\', index=2,\n      number=3, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(4),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'aspect_ratios\', full_name=\'rastervision.protos.tf_object_detection.MultiscaleAnchorGenerator.aspect_ratios\', index=3,\n      number=4, type=2, cpp_type=6, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'scales_per_octave\', full_name=\'rastervision.protos.tf_object_detection.MultiscaleAnchorGenerator.scales_per_octave\', index=4,\n      number=5, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=2,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'normalize_coordinates\', full_name=\'rastervision.protos.tf_object_detection.MultiscaleAnchorGenerator.normalize_coordinates\', index=5,\n      number=6, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=True,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=119,\n  serialized_end=305,\n)\n\nDESCRIPTOR.message_types_by_name[\'MultiscaleAnchorGenerator\'] = _MULTISCALEANCHORGENERATOR\n\nMultiscaleAnchorGenerator = _reflection.GeneratedProtocolMessageType(\'MultiscaleAnchorGenerator\', (_message.Message,), dict(\n  DESCRIPTOR = _MULTISCALEANCHORGENERATOR,\n  __module__ = \'rastervision.protos.tf_object_detection.multiscale_anchor_generator_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.MultiscaleAnchorGenerator)\n  ))\n_sym_db.RegisterMessage(MultiscaleAnchorGenerator)\n\n\n# @@protoc_insertion_point(module_scope)\n'"
rastervision/protos/tf_object_detection/optimizer_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: rastervision/protos/tf_object_detection/optimizer.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'rastervision/protos/tf_object_detection/optimizer.proto\',\n  package=\'rastervision.protos.tf_object_detection\',\n  syntax=\'proto2\',\n  serialized_pb=_b(\'\\n7rastervision/protos/tf_object_detection/optimizer.proto\\x12\\\'rastervision.protos.tf_object_detection\\""\\xe5\\x02\\n\\tOptimizer\\x12W\\n\\x12rms_prop_optimizer\\x18\\x01 \\x01(\\x0b\\x32\\x39.rastervision.protos.tf_object_detection.RMSPropOptimizerH\\x00\\x12X\\n\\x12momentum_optimizer\\x18\\x02 \\x01(\\x0b\\x32:.rastervision.protos.tf_object_detection.MomentumOptimizerH\\x00\\x12P\\n\\x0e\\x61\\x64\\x61m_optimizer\\x18\\x03 \\x01(\\x0b\\x32\\x36.rastervision.protos.tf_object_detection.AdamOptimizerH\\x00\\x12 \\n\\x12use_moving_average\\x18\\x04 \\x01(\\x08:\\x04true\\x12$\\n\\x14moving_average_decay\\x18\\x05 \\x01(\\x02:\\x06\\x30.9999B\\x0b\\n\\toptimizer\\""\\xaf\\x01\\n\\x10RMSPropOptimizer\\x12L\\n\\rlearning_rate\\x18\\x01 \\x01(\\x0b\\x32\\x35.rastervision.protos.tf_object_detection.LearningRate\\x12%\\n\\x18momentum_optimizer_value\\x18\\x02 \\x01(\\x02:\\x03\\x30.9\\x12\\x12\\n\\x05\\x64\\x65\\x63\\x61y\\x18\\x03 \\x01(\\x02:\\x03\\x30.9\\x12\\x12\\n\\x07\\x65psilon\\x18\\x04 \\x01(\\x02:\\x01\\x31\\""\\x88\\x01\\n\\x11MomentumOptimizer\\x12L\\n\\rlearning_rate\\x18\\x01 \\x01(\\x0b\\x32\\x35.rastervision.protos.tf_object_detection.LearningRate\\x12%\\n\\x18momentum_optimizer_value\\x18\\x02 \\x01(\\x02:\\x03\\x30.9\\""]\\n\\rAdamOptimizer\\x12L\\n\\rlearning_rate\\x18\\x01 \\x01(\\x0b\\x32\\x35.rastervision.protos.tf_object_detection.LearningRate\\""\\xc0\\x03\\n\\x0cLearningRate\\x12_\\n\\x16\\x63onstant_learning_rate\\x18\\x01 \\x01(\\x0b\\x32=.rastervision.protos.tf_object_detection.ConstantLearningRateH\\x00\\x12p\\n\\x1f\\x65xponential_decay_learning_rate\\x18\\x02 \\x01(\\x0b\\x32\\x45.rastervision.protos.tf_object_detection.ExponentialDecayLearningRateH\\x00\\x12\\x64\\n\\x19manual_step_learning_rate\\x18\\x03 \\x01(\\x0b\\x32?.rastervision.protos.tf_object_detection.ManualStepLearningRateH\\x00\\x12\\x66\\n\\x1a\\x63osine_decay_learning_rate\\x18\\x04 \\x01(\\x0b\\x32@.rastervision.protos.tf_object_detection.CosineDecayLearningRateH\\x00\\x42\\x0f\\n\\rlearning_rate\\""4\\n\\x14\\x43onstantLearningRate\\x12\\x1c\\n\\rlearning_rate\\x18\\x01 \\x01(\\x02:\\x05\\x30.002\\""\\xef\\x01\\n\\x1c\\x45xponentialDecayLearningRate\\x12$\\n\\x15initial_learning_rate\\x18\\x01 \\x01(\\x02:\\x05\\x30.002\\x12\\x1c\\n\\x0b\\x64\\x65\\x63\\x61y_steps\\x18\\x02 \\x01(\\r:\\x07\\x34\\x30\\x30\\x30\\x30\\x30\\x30\\x12\\x1a\\n\\x0c\\x64\\x65\\x63\\x61y_factor\\x18\\x03 \\x01(\\x02:\\x04\\x30.95\\x12\\x17\\n\\tstaircase\\x18\\x04 \\x01(\\x08:\\x04true\\x12\\x1f\\n\\x14\\x62urnin_learning_rate\\x18\\x05 \\x01(\\x02:\\x01\\x30\\x12\\x17\\n\\x0c\\x62urnin_steps\\x18\\x06 \\x01(\\r:\\x01\\x30\\x12\\x1c\\n\\x11min_learning_rate\\x18\\x07 \\x01(\\x02:\\x01\\x30\\""\\x81\\x02\\n\\x16ManualStepLearningRate\\x12$\\n\\x15initial_learning_rate\\x18\\x01 \\x01(\\x02:\\x05\\x30.002\\x12\\x66\\n\\x08schedule\\x18\\x02 \\x03(\\x0b\\x32T.rastervision.protos.tf_object_detection.ManualStepLearningRate.LearningRateSchedule\\x12\\x15\\n\\x06warmup\\x18\\x03 \\x01(\\x08:\\x05\\x66\\x61lse\\x1a\\x42\\n\\x14LearningRateSchedule\\x12\\x0c\\n\\x04step\\x18\\x01 \\x01(\\r\\x12\\x1c\\n\\rlearning_rate\\x18\\x02 \\x01(\\x02:\\x05\\x30.002\\""\\xbe\\x01\\n\\x17\\x43osineDecayLearningRate\\x12!\\n\\x12learning_rate_base\\x18\\x01 \\x01(\\x02:\\x05\\x30.002\\x12\\x1c\\n\\x0btotal_steps\\x18\\x02 \\x01(\\r:\\x07\\x34\\x30\\x30\\x30\\x30\\x30\\x30\\x12$\\n\\x14warmup_learning_rate\\x18\\x03 \\x01(\\x02:\\x06\\x30.0002\\x12\\x1b\\n\\x0cwarmup_steps\\x18\\x04 \\x01(\\r:\\x05\\x31\\x30\\x30\\x30\\x30\\x12\\x1f\\n\\x14hold_base_rate_steps\\x18\\x05 \\x01(\\r:\\x01\\x30\')\n)\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\n\n\n\n_OPTIMIZER = _descriptor.Descriptor(\n  name=\'Optimizer\',\n  full_name=\'rastervision.protos.tf_object_detection.Optimizer\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'rms_prop_optimizer\', full_name=\'rastervision.protos.tf_object_detection.Optimizer.rms_prop_optimizer\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'momentum_optimizer\', full_name=\'rastervision.protos.tf_object_detection.Optimizer.momentum_optimizer\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'adam_optimizer\', full_name=\'rastervision.protos.tf_object_detection.Optimizer.adam_optimizer\', index=2,\n      number=3, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'use_moving_average\', full_name=\'rastervision.protos.tf_object_detection.Optimizer.use_moving_average\', index=3,\n      number=4, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=True,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'moving_average_decay\', full_name=\'rastervision.protos.tf_object_detection.Optimizer.moving_average_decay\', index=4,\n      number=5, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.9999),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n    _descriptor.OneofDescriptor(\n      name=\'optimizer\', full_name=\'rastervision.protos.tf_object_detection.Optimizer.optimizer\',\n      index=0, containing_type=None, fields=[]),\n  ],\n  serialized_start=101,\n  serialized_end=458,\n)\n\n\n_RMSPROPOPTIMIZER = _descriptor.Descriptor(\n  name=\'RMSPropOptimizer\',\n  full_name=\'rastervision.protos.tf_object_detection.RMSPropOptimizer\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'learning_rate\', full_name=\'rastervision.protos.tf_object_detection.RMSPropOptimizer.learning_rate\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'momentum_optimizer_value\', full_name=\'rastervision.protos.tf_object_detection.RMSPropOptimizer.momentum_optimizer_value\', index=1,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.9),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'decay\', full_name=\'rastervision.protos.tf_object_detection.RMSPropOptimizer.decay\', index=2,\n      number=3, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.9),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'epsilon\', full_name=\'rastervision.protos.tf_object_detection.RMSPropOptimizer.epsilon\', index=3,\n      number=4, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(1),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=461,\n  serialized_end=636,\n)\n\n\n_MOMENTUMOPTIMIZER = _descriptor.Descriptor(\n  name=\'MomentumOptimizer\',\n  full_name=\'rastervision.protos.tf_object_detection.MomentumOptimizer\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'learning_rate\', full_name=\'rastervision.protos.tf_object_detection.MomentumOptimizer.learning_rate\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'momentum_optimizer_value\', full_name=\'rastervision.protos.tf_object_detection.MomentumOptimizer.momentum_optimizer_value\', index=1,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.9),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=639,\n  serialized_end=775,\n)\n\n\n_ADAMOPTIMIZER = _descriptor.Descriptor(\n  name=\'AdamOptimizer\',\n  full_name=\'rastervision.protos.tf_object_detection.AdamOptimizer\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'learning_rate\', full_name=\'rastervision.protos.tf_object_detection.AdamOptimizer.learning_rate\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=777,\n  serialized_end=870,\n)\n\n\n_LEARNINGRATE = _descriptor.Descriptor(\n  name=\'LearningRate\',\n  full_name=\'rastervision.protos.tf_object_detection.LearningRate\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'constant_learning_rate\', full_name=\'rastervision.protos.tf_object_detection.LearningRate.constant_learning_rate\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'exponential_decay_learning_rate\', full_name=\'rastervision.protos.tf_object_detection.LearningRate.exponential_decay_learning_rate\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'manual_step_learning_rate\', full_name=\'rastervision.protos.tf_object_detection.LearningRate.manual_step_learning_rate\', index=2,\n      number=3, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'cosine_decay_learning_rate\', full_name=\'rastervision.protos.tf_object_detection.LearningRate.cosine_decay_learning_rate\', index=3,\n      number=4, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n    _descriptor.OneofDescriptor(\n      name=\'learning_rate\', full_name=\'rastervision.protos.tf_object_detection.LearningRate.learning_rate\',\n      index=0, containing_type=None, fields=[]),\n  ],\n  serialized_start=873,\n  serialized_end=1321,\n)\n\n\n_CONSTANTLEARNINGRATE = _descriptor.Descriptor(\n  name=\'ConstantLearningRate\',\n  full_name=\'rastervision.protos.tf_object_detection.ConstantLearningRate\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'learning_rate\', full_name=\'rastervision.protos.tf_object_detection.ConstantLearningRate.learning_rate\', index=0,\n      number=1, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.002),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1323,\n  serialized_end=1375,\n)\n\n\n_EXPONENTIALDECAYLEARNINGRATE = _descriptor.Descriptor(\n  name=\'ExponentialDecayLearningRate\',\n  full_name=\'rastervision.protos.tf_object_detection.ExponentialDecayLearningRate\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'initial_learning_rate\', full_name=\'rastervision.protos.tf_object_detection.ExponentialDecayLearningRate.initial_learning_rate\', index=0,\n      number=1, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.002),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'decay_steps\', full_name=\'rastervision.protos.tf_object_detection.ExponentialDecayLearningRate.decay_steps\', index=1,\n      number=2, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=4000000,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'decay_factor\', full_name=\'rastervision.protos.tf_object_detection.ExponentialDecayLearningRate.decay_factor\', index=2,\n      number=3, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.95),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'staircase\', full_name=\'rastervision.protos.tf_object_detection.ExponentialDecayLearningRate.staircase\', index=3,\n      number=4, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=True,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'burnin_learning_rate\', full_name=\'rastervision.protos.tf_object_detection.ExponentialDecayLearningRate.burnin_learning_rate\', index=4,\n      number=5, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'burnin_steps\', full_name=\'rastervision.protos.tf_object_detection.ExponentialDecayLearningRate.burnin_steps\', index=5,\n      number=6, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'min_learning_rate\', full_name=\'rastervision.protos.tf_object_detection.ExponentialDecayLearningRate.min_learning_rate\', index=6,\n      number=7, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1378,\n  serialized_end=1617,\n)\n\n\n_MANUALSTEPLEARNINGRATE_LEARNINGRATESCHEDULE = _descriptor.Descriptor(\n  name=\'LearningRateSchedule\',\n  full_name=\'rastervision.protos.tf_object_detection.ManualStepLearningRate.LearningRateSchedule\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'step\', full_name=\'rastervision.protos.tf_object_detection.ManualStepLearningRate.LearningRateSchedule.step\', index=0,\n      number=1, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'learning_rate\', full_name=\'rastervision.protos.tf_object_detection.ManualStepLearningRate.LearningRateSchedule.learning_rate\', index=1,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.002),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1811,\n  serialized_end=1877,\n)\n\n_MANUALSTEPLEARNINGRATE = _descriptor.Descriptor(\n  name=\'ManualStepLearningRate\',\n  full_name=\'rastervision.protos.tf_object_detection.ManualStepLearningRate\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'initial_learning_rate\', full_name=\'rastervision.protos.tf_object_detection.ManualStepLearningRate.initial_learning_rate\', index=0,\n      number=1, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.002),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'schedule\', full_name=\'rastervision.protos.tf_object_detection.ManualStepLearningRate.schedule\', index=1,\n      number=2, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'warmup\', full_name=\'rastervision.protos.tf_object_detection.ManualStepLearningRate.warmup\', index=2,\n      number=3, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[_MANUALSTEPLEARNINGRATE_LEARNINGRATESCHEDULE, ],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1620,\n  serialized_end=1877,\n)\n\n\n_COSINEDECAYLEARNINGRATE = _descriptor.Descriptor(\n  name=\'CosineDecayLearningRate\',\n  full_name=\'rastervision.protos.tf_object_detection.CosineDecayLearningRate\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'learning_rate_base\', full_name=\'rastervision.protos.tf_object_detection.CosineDecayLearningRate.learning_rate_base\', index=0,\n      number=1, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.002),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'total_steps\', full_name=\'rastervision.protos.tf_object_detection.CosineDecayLearningRate.total_steps\', index=1,\n      number=2, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=4000000,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'warmup_learning_rate\', full_name=\'rastervision.protos.tf_object_detection.CosineDecayLearningRate.warmup_learning_rate\', index=2,\n      number=3, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.0002),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'warmup_steps\', full_name=\'rastervision.protos.tf_object_detection.CosineDecayLearningRate.warmup_steps\', index=3,\n      number=4, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=10000,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'hold_base_rate_steps\', full_name=\'rastervision.protos.tf_object_detection.CosineDecayLearningRate.hold_base_rate_steps\', index=4,\n      number=5, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1880,\n  serialized_end=2070,\n)\n\n_OPTIMIZER.fields_by_name[\'rms_prop_optimizer\'].message_type = _RMSPROPOPTIMIZER\n_OPTIMIZER.fields_by_name[\'momentum_optimizer\'].message_type = _MOMENTUMOPTIMIZER\n_OPTIMIZER.fields_by_name[\'adam_optimizer\'].message_type = _ADAMOPTIMIZER\n_OPTIMIZER.oneofs_by_name[\'optimizer\'].fields.append(\n  _OPTIMIZER.fields_by_name[\'rms_prop_optimizer\'])\n_OPTIMIZER.fields_by_name[\'rms_prop_optimizer\'].containing_oneof = _OPTIMIZER.oneofs_by_name[\'optimizer\']\n_OPTIMIZER.oneofs_by_name[\'optimizer\'].fields.append(\n  _OPTIMIZER.fields_by_name[\'momentum_optimizer\'])\n_OPTIMIZER.fields_by_name[\'momentum_optimizer\'].containing_oneof = _OPTIMIZER.oneofs_by_name[\'optimizer\']\n_OPTIMIZER.oneofs_by_name[\'optimizer\'].fields.append(\n  _OPTIMIZER.fields_by_name[\'adam_optimizer\'])\n_OPTIMIZER.fields_by_name[\'adam_optimizer\'].containing_oneof = _OPTIMIZER.oneofs_by_name[\'optimizer\']\n_RMSPROPOPTIMIZER.fields_by_name[\'learning_rate\'].message_type = _LEARNINGRATE\n_MOMENTUMOPTIMIZER.fields_by_name[\'learning_rate\'].message_type = _LEARNINGRATE\n_ADAMOPTIMIZER.fields_by_name[\'learning_rate\'].message_type = _LEARNINGRATE\n_LEARNINGRATE.fields_by_name[\'constant_learning_rate\'].message_type = _CONSTANTLEARNINGRATE\n_LEARNINGRATE.fields_by_name[\'exponential_decay_learning_rate\'].message_type = _EXPONENTIALDECAYLEARNINGRATE\n_LEARNINGRATE.fields_by_name[\'manual_step_learning_rate\'].message_type = _MANUALSTEPLEARNINGRATE\n_LEARNINGRATE.fields_by_name[\'cosine_decay_learning_rate\'].message_type = _COSINEDECAYLEARNINGRATE\n_LEARNINGRATE.oneofs_by_name[\'learning_rate\'].fields.append(\n  _LEARNINGRATE.fields_by_name[\'constant_learning_rate\'])\n_LEARNINGRATE.fields_by_name[\'constant_learning_rate\'].containing_oneof = _LEARNINGRATE.oneofs_by_name[\'learning_rate\']\n_LEARNINGRATE.oneofs_by_name[\'learning_rate\'].fields.append(\n  _LEARNINGRATE.fields_by_name[\'exponential_decay_learning_rate\'])\n_LEARNINGRATE.fields_by_name[\'exponential_decay_learning_rate\'].containing_oneof = _LEARNINGRATE.oneofs_by_name[\'learning_rate\']\n_LEARNINGRATE.oneofs_by_name[\'learning_rate\'].fields.append(\n  _LEARNINGRATE.fields_by_name[\'manual_step_learning_rate\'])\n_LEARNINGRATE.fields_by_name[\'manual_step_learning_rate\'].containing_oneof = _LEARNINGRATE.oneofs_by_name[\'learning_rate\']\n_LEARNINGRATE.oneofs_by_name[\'learning_rate\'].fields.append(\n  _LEARNINGRATE.fields_by_name[\'cosine_decay_learning_rate\'])\n_LEARNINGRATE.fields_by_name[\'cosine_decay_learning_rate\'].containing_oneof = _LEARNINGRATE.oneofs_by_name[\'learning_rate\']\n_MANUALSTEPLEARNINGRATE_LEARNINGRATESCHEDULE.containing_type = _MANUALSTEPLEARNINGRATE\n_MANUALSTEPLEARNINGRATE.fields_by_name[\'schedule\'].message_type = _MANUALSTEPLEARNINGRATE_LEARNINGRATESCHEDULE\nDESCRIPTOR.message_types_by_name[\'Optimizer\'] = _OPTIMIZER\nDESCRIPTOR.message_types_by_name[\'RMSPropOptimizer\'] = _RMSPROPOPTIMIZER\nDESCRIPTOR.message_types_by_name[\'MomentumOptimizer\'] = _MOMENTUMOPTIMIZER\nDESCRIPTOR.message_types_by_name[\'AdamOptimizer\'] = _ADAMOPTIMIZER\nDESCRIPTOR.message_types_by_name[\'LearningRate\'] = _LEARNINGRATE\nDESCRIPTOR.message_types_by_name[\'ConstantLearningRate\'] = _CONSTANTLEARNINGRATE\nDESCRIPTOR.message_types_by_name[\'ExponentialDecayLearningRate\'] = _EXPONENTIALDECAYLEARNINGRATE\nDESCRIPTOR.message_types_by_name[\'ManualStepLearningRate\'] = _MANUALSTEPLEARNINGRATE\nDESCRIPTOR.message_types_by_name[\'CosineDecayLearningRate\'] = _COSINEDECAYLEARNINGRATE\n\nOptimizer = _reflection.GeneratedProtocolMessageType(\'Optimizer\', (_message.Message,), dict(\n  DESCRIPTOR = _OPTIMIZER,\n  __module__ = \'rastervision.protos.tf_object_detection.optimizer_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.Optimizer)\n  ))\n_sym_db.RegisterMessage(Optimizer)\n\nRMSPropOptimizer = _reflection.GeneratedProtocolMessageType(\'RMSPropOptimizer\', (_message.Message,), dict(\n  DESCRIPTOR = _RMSPROPOPTIMIZER,\n  __module__ = \'rastervision.protos.tf_object_detection.optimizer_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.RMSPropOptimizer)\n  ))\n_sym_db.RegisterMessage(RMSPropOptimizer)\n\nMomentumOptimizer = _reflection.GeneratedProtocolMessageType(\'MomentumOptimizer\', (_message.Message,), dict(\n  DESCRIPTOR = _MOMENTUMOPTIMIZER,\n  __module__ = \'rastervision.protos.tf_object_detection.optimizer_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.MomentumOptimizer)\n  ))\n_sym_db.RegisterMessage(MomentumOptimizer)\n\nAdamOptimizer = _reflection.GeneratedProtocolMessageType(\'AdamOptimizer\', (_message.Message,), dict(\n  DESCRIPTOR = _ADAMOPTIMIZER,\n  __module__ = \'rastervision.protos.tf_object_detection.optimizer_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.AdamOptimizer)\n  ))\n_sym_db.RegisterMessage(AdamOptimizer)\n\nLearningRate = _reflection.GeneratedProtocolMessageType(\'LearningRate\', (_message.Message,), dict(\n  DESCRIPTOR = _LEARNINGRATE,\n  __module__ = \'rastervision.protos.tf_object_detection.optimizer_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.LearningRate)\n  ))\n_sym_db.RegisterMessage(LearningRate)\n\nConstantLearningRate = _reflection.GeneratedProtocolMessageType(\'ConstantLearningRate\', (_message.Message,), dict(\n  DESCRIPTOR = _CONSTANTLEARNINGRATE,\n  __module__ = \'rastervision.protos.tf_object_detection.optimizer_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.ConstantLearningRate)\n  ))\n_sym_db.RegisterMessage(ConstantLearningRate)\n\nExponentialDecayLearningRate = _reflection.GeneratedProtocolMessageType(\'ExponentialDecayLearningRate\', (_message.Message,), dict(\n  DESCRIPTOR = _EXPONENTIALDECAYLEARNINGRATE,\n  __module__ = \'rastervision.protos.tf_object_detection.optimizer_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.ExponentialDecayLearningRate)\n  ))\n_sym_db.RegisterMessage(ExponentialDecayLearningRate)\n\nManualStepLearningRate = _reflection.GeneratedProtocolMessageType(\'ManualStepLearningRate\', (_message.Message,), dict(\n\n  LearningRateSchedule = _reflection.GeneratedProtocolMessageType(\'LearningRateSchedule\', (_message.Message,), dict(\n    DESCRIPTOR = _MANUALSTEPLEARNINGRATE_LEARNINGRATESCHEDULE,\n    __module__ = \'rastervision.protos.tf_object_detection.optimizer_pb2\'\n    # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.ManualStepLearningRate.LearningRateSchedule)\n    ))\n  ,\n  DESCRIPTOR = _MANUALSTEPLEARNINGRATE,\n  __module__ = \'rastervision.protos.tf_object_detection.optimizer_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.ManualStepLearningRate)\n  ))\n_sym_db.RegisterMessage(ManualStepLearningRate)\n_sym_db.RegisterMessage(ManualStepLearningRate.LearningRateSchedule)\n\nCosineDecayLearningRate = _reflection.GeneratedProtocolMessageType(\'CosineDecayLearningRate\', (_message.Message,), dict(\n  DESCRIPTOR = _COSINEDECAYLEARNINGRATE,\n  __module__ = \'rastervision.protos.tf_object_detection.optimizer_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.CosineDecayLearningRate)\n  ))\n_sym_db.RegisterMessage(CosineDecayLearningRate)\n\n\n# @@protoc_insertion_point(module_scope)\n'"
rastervision/protos/tf_object_detection/pipeline_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: rastervision/protos/tf_object_detection/pipeline.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom rastervision.protos.tf_object_detection import eval_pb2 as rastervision_dot_protos_dot_tf__object__detection_dot_eval__pb2\nfrom rastervision.protos.tf_object_detection import graph_rewriter_pb2 as rastervision_dot_protos_dot_tf__object__detection_dot_graph__rewriter__pb2\nfrom rastervision.protos.tf_object_detection import input_reader_pb2 as rastervision_dot_protos_dot_tf__object__detection_dot_input__reader__pb2\nfrom rastervision.protos.tf_object_detection import model_pb2 as rastervision_dot_protos_dot_tf__object__detection_dot_model__pb2\nfrom rastervision.protos.tf_object_detection import train_pb2 as rastervision_dot_protos_dot_tf__object__detection_dot_train__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'rastervision/protos/tf_object_detection/pipeline.proto\',\n  package=\'rastervision.protos.tf_object_detection\',\n  syntax=\'proto2\',\n  serialized_pb=_b(\'\\n6rastervision/protos/tf_object_detection/pipeline.proto\\x12\\\'rastervision.protos.tf_object_detection\\x1a\\x32rastervision/protos/tf_object_detection/eval.proto\\x1a<rastervision/protos/tf_object_detection/graph_rewriter.proto\\x1a:rastervision/protos/tf_object_detection/input_reader.proto\\x1a\\x33rastervision/protos/tf_object_detection/model.proto\\x1a\\x33rastervision/protos/tf_object_detection/train.proto\\""\\xf5\\x03\\n\\x17TrainEvalPipelineConfig\\x12\\x46\\n\\x05model\\x18\\x01 \\x01(\\x0b\\x32\\x37.rastervision.protos.tf_object_detection.DetectionModel\\x12J\\n\\x0ctrain_config\\x18\\x02 \\x01(\\x0b\\x32\\x34.rastervision.protos.tf_object_detection.TrainConfig\\x12P\\n\\x12train_input_reader\\x18\\x03 \\x01(\\x0b\\x32\\x34.rastervision.protos.tf_object_detection.InputReader\\x12H\\n\\x0b\\x65val_config\\x18\\x04 \\x01(\\x0b\\x32\\x33.rastervision.protos.tf_object_detection.EvalConfig\\x12O\\n\\x11\\x65val_input_reader\\x18\\x05 \\x03(\\x0b\\x32\\x34.rastervision.protos.tf_object_detection.InputReader\\x12N\\n\\x0egraph_rewriter\\x18\\x06 \\x01(\\x0b\\x32\\x36.rastervision.protos.tf_object_detection.GraphRewriter*\\t\\x08\\xe8\\x07\\x10\\x80\\x80\\x80\\x80\\x02\')\n  ,\n  dependencies=[rastervision_dot_protos_dot_tf__object__detection_dot_eval__pb2.DESCRIPTOR,rastervision_dot_protos_dot_tf__object__detection_dot_graph__rewriter__pb2.DESCRIPTOR,rastervision_dot_protos_dot_tf__object__detection_dot_input__reader__pb2.DESCRIPTOR,rastervision_dot_protos_dot_tf__object__detection_dot_model__pb2.DESCRIPTOR,rastervision_dot_protos_dot_tf__object__detection_dot_train__pb2.DESCRIPTOR,])\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\n\n\n\n_TRAINEVALPIPELINECONFIG = _descriptor.Descriptor(\n  name=\'TrainEvalPipelineConfig\',\n  full_name=\'rastervision.protos.tf_object_detection.TrainEvalPipelineConfig\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'model\', full_name=\'rastervision.protos.tf_object_detection.TrainEvalPipelineConfig.model\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'train_config\', full_name=\'rastervision.protos.tf_object_detection.TrainEvalPipelineConfig.train_config\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'train_input_reader\', full_name=\'rastervision.protos.tf_object_detection.TrainEvalPipelineConfig.train_input_reader\', index=2,\n      number=3, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'eval_config\', full_name=\'rastervision.protos.tf_object_detection.TrainEvalPipelineConfig.eval_config\', index=3,\n      number=4, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'eval_input_reader\', full_name=\'rastervision.protos.tf_object_detection.TrainEvalPipelineConfig.eval_input_reader\', index=4,\n      number=5, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'graph_rewriter\', full_name=\'rastervision.protos.tf_object_detection.TrainEvalPipelineConfig.graph_rewriter\', index=5,\n      number=6, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=True,\n  syntax=\'proto2\',\n  extension_ranges=[(1000, 536870912), ],\n  oneofs=[\n  ],\n  serialized_start=380,\n  serialized_end=881,\n)\n\n_TRAINEVALPIPELINECONFIG.fields_by_name[\'model\'].message_type = rastervision_dot_protos_dot_tf__object__detection_dot_model__pb2._DETECTIONMODEL\n_TRAINEVALPIPELINECONFIG.fields_by_name[\'train_config\'].message_type = rastervision_dot_protos_dot_tf__object__detection_dot_train__pb2._TRAINCONFIG\n_TRAINEVALPIPELINECONFIG.fields_by_name[\'train_input_reader\'].message_type = rastervision_dot_protos_dot_tf__object__detection_dot_input__reader__pb2._INPUTREADER\n_TRAINEVALPIPELINECONFIG.fields_by_name[\'eval_config\'].message_type = rastervision_dot_protos_dot_tf__object__detection_dot_eval__pb2._EVALCONFIG\n_TRAINEVALPIPELINECONFIG.fields_by_name[\'eval_input_reader\'].message_type = rastervision_dot_protos_dot_tf__object__detection_dot_input__reader__pb2._INPUTREADER\n_TRAINEVALPIPELINECONFIG.fields_by_name[\'graph_rewriter\'].message_type = rastervision_dot_protos_dot_tf__object__detection_dot_graph__rewriter__pb2._GRAPHREWRITER\nDESCRIPTOR.message_types_by_name[\'TrainEvalPipelineConfig\'] = _TRAINEVALPIPELINECONFIG\n\nTrainEvalPipelineConfig = _reflection.GeneratedProtocolMessageType(\'TrainEvalPipelineConfig\', (_message.Message,), dict(\n  DESCRIPTOR = _TRAINEVALPIPELINECONFIG,\n  __module__ = \'rastervision.protos.tf_object_detection.pipeline_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.TrainEvalPipelineConfig)\n  ))\n_sym_db.RegisterMessage(TrainEvalPipelineConfig)\n\n\n# @@protoc_insertion_point(module_scope)\n'"
rastervision/protos/tf_object_detection/post_processing_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: rastervision/protos/tf_object_detection/post_processing.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'rastervision/protos/tf_object_detection/post_processing.proto\',\n  package=\'rastervision.protos.tf_object_detection\',\n  syntax=\'proto2\',\n  serialized_pb=_b(\'\\n=rastervision/protos/tf_object_detection/post_processing.proto\\x12\\\'rastervision.protos.tf_object_detection\\""\\xbc\\x01\\n\\x16\\x42\\x61tchNonMaxSuppression\\x12\\x1a\\n\\x0fscore_threshold\\x18\\x01 \\x01(\\x02:\\x01\\x30\\x12\\x1a\\n\\riou_threshold\\x18\\x02 \\x01(\\x02:\\x03\\x30.6\\x12%\\n\\x18max_detections_per_class\\x18\\x03 \\x01(\\x05:\\x03\\x31\\x30\\x30\\x12!\\n\\x14max_total_detections\\x18\\x05 \\x01(\\x05:\\x03\\x31\\x30\\x30\\x12 \\n\\x11use_static_shapes\\x18\\x06 \\x01(\\x08:\\x05\\x66\\x61lse\\""\\xb1\\x02\\n\\x0ePostProcessing\\x12\\x62\\n\\x19\\x62\\x61tch_non_max_suppression\\x18\\x01 \\x01(\\x0b\\x32?.rastervision.protos.tf_object_detection.BatchNonMaxSuppression\\x12i\\n\\x0fscore_converter\\x18\\x02 \\x01(\\x0e\\x32\\x46.rastervision.protos.tf_object_detection.PostProcessing.ScoreConverter:\\x08IDENTITY\\x12\\x16\\n\\x0blogit_scale\\x18\\x03 \\x01(\\x02:\\x01\\x31\\""8\\n\\x0eScoreConverter\\x12\\x0c\\n\\x08IDENTITY\\x10\\x00\\x12\\x0b\\n\\x07SIGMOID\\x10\\x01\\x12\\x0b\\n\\x07SOFTMAX\\x10\\x02\')\n)\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\n\n\n_POSTPROCESSING_SCORECONVERTER = _descriptor.EnumDescriptor(\n  name=\'ScoreConverter\',\n  full_name=\'rastervision.protos.tf_object_detection.PostProcessing.ScoreConverter\',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name=\'IDENTITY\', index=0, number=0,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'SIGMOID\', index=1, number=1,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'SOFTMAX\', index=2, number=2,\n      options=None,\n      type=None),\n  ],\n  containing_type=None,\n  options=None,\n  serialized_start=547,\n  serialized_end=603,\n)\n_sym_db.RegisterEnumDescriptor(_POSTPROCESSING_SCORECONVERTER)\n\n\n_BATCHNONMAXSUPPRESSION = _descriptor.Descriptor(\n  name=\'BatchNonMaxSuppression\',\n  full_name=\'rastervision.protos.tf_object_detection.BatchNonMaxSuppression\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'score_threshold\', full_name=\'rastervision.protos.tf_object_detection.BatchNonMaxSuppression.score_threshold\', index=0,\n      number=1, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'iou_threshold\', full_name=\'rastervision.protos.tf_object_detection.BatchNonMaxSuppression.iou_threshold\', index=1,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.6),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'max_detections_per_class\', full_name=\'rastervision.protos.tf_object_detection.BatchNonMaxSuppression.max_detections_per_class\', index=2,\n      number=3, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=100,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'max_total_detections\', full_name=\'rastervision.protos.tf_object_detection.BatchNonMaxSuppression.max_total_detections\', index=3,\n      number=5, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=100,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'use_static_shapes\', full_name=\'rastervision.protos.tf_object_detection.BatchNonMaxSuppression.use_static_shapes\', index=4,\n      number=6, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=107,\n  serialized_end=295,\n)\n\n\n_POSTPROCESSING = _descriptor.Descriptor(\n  name=\'PostProcessing\',\n  full_name=\'rastervision.protos.tf_object_detection.PostProcessing\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'batch_non_max_suppression\', full_name=\'rastervision.protos.tf_object_detection.PostProcessing.batch_non_max_suppression\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'score_converter\', full_name=\'rastervision.protos.tf_object_detection.PostProcessing.score_converter\', index=1,\n      number=2, type=14, cpp_type=8, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'logit_scale\', full_name=\'rastervision.protos.tf_object_detection.PostProcessing.logit_scale\', index=2,\n      number=3, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(1),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n    _POSTPROCESSING_SCORECONVERTER,\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=298,\n  serialized_end=603,\n)\n\n_POSTPROCESSING.fields_by_name[\'batch_non_max_suppression\'].message_type = _BATCHNONMAXSUPPRESSION\n_POSTPROCESSING.fields_by_name[\'score_converter\'].enum_type = _POSTPROCESSING_SCORECONVERTER\n_POSTPROCESSING_SCORECONVERTER.containing_type = _POSTPROCESSING\nDESCRIPTOR.message_types_by_name[\'BatchNonMaxSuppression\'] = _BATCHNONMAXSUPPRESSION\nDESCRIPTOR.message_types_by_name[\'PostProcessing\'] = _POSTPROCESSING\n\nBatchNonMaxSuppression = _reflection.GeneratedProtocolMessageType(\'BatchNonMaxSuppression\', (_message.Message,), dict(\n  DESCRIPTOR = _BATCHNONMAXSUPPRESSION,\n  __module__ = \'rastervision.protos.tf_object_detection.post_processing_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.BatchNonMaxSuppression)\n  ))\n_sym_db.RegisterMessage(BatchNonMaxSuppression)\n\nPostProcessing = _reflection.GeneratedProtocolMessageType(\'PostProcessing\', (_message.Message,), dict(\n  DESCRIPTOR = _POSTPROCESSING,\n  __module__ = \'rastervision.protos.tf_object_detection.post_processing_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.PostProcessing)\n  ))\n_sym_db.RegisterMessage(PostProcessing)\n\n\n# @@protoc_insertion_point(module_scope)\n'"
rastervision/protos/tf_object_detection/preprocessor_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: rastervision/protos/tf_object_detection/preprocessor.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'rastervision/protos/tf_object_detection/preprocessor.proto\',\n  package=\'rastervision.protos.tf_object_detection\',\n  syntax=\'proto2\',\n  serialized_pb=_b(\'\\n:rastervision/protos/tf_object_detection/preprocessor.proto\\x12\\\'rastervision.protos.tf_object_detection\\""\\x8b\\x15\\n\\x11PreprocessingStep\\x12R\\n\\x0fnormalize_image\\x18\\x01 \\x01(\\x0b\\x32\\x37.rastervision.protos.tf_object_detection.NormalizeImageH\\x00\\x12_\\n\\x16random_horizontal_flip\\x18\\x02 \\x01(\\x0b\\x32=.rastervision.protos.tf_object_detection.RandomHorizontalFlipH\\x00\\x12\\x62\\n\\x18random_pixel_value_scale\\x18\\x03 \\x01(\\x0b\\x32>.rastervision.protos.tf_object_detection.RandomPixelValueScaleH\\x00\\x12W\\n\\x12random_image_scale\\x18\\x04 \\x01(\\x0b\\x32\\x39.rastervision.protos.tf_object_detection.RandomImageScaleH\\x00\\x12V\\n\\x12random_rgb_to_gray\\x18\\x05 \\x01(\\x0b\\x32\\x38.rastervision.protos.tf_object_detection.RandomRGBtoGrayH\\x00\\x12\\x63\\n\\x18random_adjust_brightness\\x18\\x06 \\x01(\\x0b\\x32?.rastervision.protos.tf_object_detection.RandomAdjustBrightnessH\\x00\\x12_\\n\\x16random_adjust_contrast\\x18\\x07 \\x01(\\x0b\\x32=.rastervision.protos.tf_object_detection.RandomAdjustContrastH\\x00\\x12U\\n\\x11random_adjust_hue\\x18\\x08 \\x01(\\x0b\\x32\\x38.rastervision.protos.tf_object_detection.RandomAdjustHueH\\x00\\x12\\x63\\n\\x18random_adjust_saturation\\x18\\t \\x01(\\x0b\\x32?.rastervision.protos.tf_object_detection.RandomAdjustSaturationH\\x00\\x12[\\n\\x14random_distort_color\\x18\\n \\x01(\\x0b\\x32;.rastervision.protos.tf_object_detection.RandomDistortColorH\\x00\\x12Y\\n\\x13random_jitter_boxes\\x18\\x0b \\x01(\\x0b\\x32:.rastervision.protos.tf_object_detection.RandomJitterBoxesH\\x00\\x12U\\n\\x11random_crop_image\\x18\\x0c \\x01(\\x0b\\x32\\x38.rastervision.protos.tf_object_detection.RandomCropImageH\\x00\\x12S\\n\\x10random_pad_image\\x18\\r \\x01(\\x0b\\x32\\x37.rastervision.protos.tf_object_detection.RandomPadImageH\\x00\\x12\\\\\\n\\x15random_crop_pad_image\\x18\\x0e \\x01(\\x0b\\x32;.rastervision.protos.tf_object_detection.RandomCropPadImageH\\x00\\x12g\\n\\x1brandom_crop_to_aspect_ratio\\x18\\x0f \\x01(\\x0b\\x32@.rastervision.protos.tf_object_detection.RandomCropToAspectRatioH\\x00\\x12[\\n\\x14random_black_patches\\x18\\x10 \\x01(\\x0b\\x32;.rastervision.protos.tf_object_detection.RandomBlackPatchesH\\x00\\x12[\\n\\x14random_resize_method\\x18\\x11 \\x01(\\x0b\\x32;.rastervision.protos.tf_object_detection.RandomResizeMethodH\\x00\\x12q\\n scale_boxes_to_pixel_coordinates\\x18\\x12 \\x01(\\x0b\\x32\\x45.rastervision.protos.tf_object_detection.ScaleBoxesToPixelCoordinatesH\\x00\\x12L\\n\\x0cresize_image\\x18\\x13 \\x01(\\x0b\\x32\\x34.rastervision.protos.tf_object_detection.ResizeImageH\\x00\\x12]\\n\\x15subtract_channel_mean\\x18\\x14 \\x01(\\x0b\\x32<.rastervision.protos.tf_object_detection.SubtractChannelMeanH\\x00\\x12Q\\n\\x0fssd_random_crop\\x18\\x15 \\x01(\\x0b\\x32\\x36.rastervision.protos.tf_object_detection.SSDRandomCropH\\x00\\x12X\\n\\x13ssd_random_crop_pad\\x18\\x16 \\x01(\\x0b\\x32\\x39.rastervision.protos.tf_object_detection.SSDRandomCropPadH\\x00\\x12t\\n\\""ssd_random_crop_fixed_aspect_ratio\\x18\\x17 \\x01(\\x0b\\x32\\x46.rastervision.protos.tf_object_detection.SSDRandomCropFixedAspectRatioH\\x00\\x12{\\n&ssd_random_crop_pad_fixed_aspect_ratio\\x18\\x18 \\x01(\\x0b\\x32I.rastervision.protos.tf_object_detection.SSDRandomCropPadFixedAspectRatioH\\x00\\x12[\\n\\x14random_vertical_flip\\x18\\x19 \\x01(\\x0b\\x32;.rastervision.protos.tf_object_detection.RandomVerticalFlipH\\x00\\x12V\\n\\x11random_rotation90\\x18\\x1a \\x01(\\x0b\\x32\\x39.rastervision.protos.tf_object_detection.RandomRotation90H\\x00\\x12I\\n\\x0brgb_to_gray\\x18\\x1b \\x01(\\x0b\\x32\\x32.rastervision.protos.tf_object_detection.RGBtoGrayH\\x00\\x12o\\n\\x1f\\x63onvert_class_logits_to_softmax\\x18\\x1c \\x01(\\x0b\\x32\\x44.rastervision.protos.tf_object_detection.ConvertClassLogitsToSoftmaxH\\x00\\x42\\x14\\n\\x12preprocessing_step\\""v\\n\\x0eNormalizeImage\\x12\\x17\\n\\x0foriginal_minval\\x18\\x01 \\x01(\\x02\\x12\\x17\\n\\x0foriginal_maxval\\x18\\x02 \\x01(\\x02\\x12\\x18\\n\\rtarget_minval\\x18\\x03 \\x01(\\x02:\\x01\\x30\\x12\\x18\\n\\rtarget_maxval\\x18\\x04 \\x01(\\x02:\\x01\\x31\\""9\\n\\x14RandomHorizontalFlip\\x12!\\n\\x19keypoint_flip_permutation\\x18\\x01 \\x03(\\x05\\""7\\n\\x12RandomVerticalFlip\\x12!\\n\\x19keypoint_flip_permutation\\x18\\x01 \\x03(\\x05\\""\\x12\\n\\x10RandomRotation90\\""A\\n\\x15RandomPixelValueScale\\x12\\x13\\n\\x06minval\\x18\\x01 \\x01(\\x02:\\x03\\x30.9\\x12\\x13\\n\\x06maxval\\x18\\x02 \\x01(\\x02:\\x03\\x31.1\\""L\\n\\x10RandomImageScale\\x12\\x1c\\n\\x0fmin_scale_ratio\\x18\\x01 \\x01(\\x02:\\x03\\x30.5\\x12\\x1a\\n\\x0fmax_scale_ratio\\x18\\x02 \\x01(\\x02:\\x01\\x32\\""+\\n\\x0fRandomRGBtoGray\\x12\\x18\\n\\x0bprobability\\x18\\x01 \\x01(\\x02:\\x03\\x30.1\\""0\\n\\x16RandomAdjustBrightness\\x12\\x16\\n\\tmax_delta\\x18\\x01 \\x01(\\x02:\\x03\\x30.2\\""G\\n\\x14RandomAdjustContrast\\x12\\x16\\n\\tmin_delta\\x18\\x01 \\x01(\\x02:\\x03\\x30.8\\x12\\x17\\n\\tmax_delta\\x18\\x02 \\x01(\\x02:\\x04\\x31.25\\""*\\n\\x0fRandomAdjustHue\\x12\\x17\\n\\tmax_delta\\x18\\x01 \\x01(\\x02:\\x04\\x30.02\\""I\\n\\x16RandomAdjustSaturation\\x12\\x16\\n\\tmin_delta\\x18\\x01 \\x01(\\x02:\\x03\\x30.8\\x12\\x17\\n\\tmax_delta\\x18\\x02 \\x01(\\x02:\\x04\\x31.25\\"",\\n\\x12RandomDistortColor\\x12\\x16\\n\\x0e\\x63olor_ordering\\x18\\x01 \\x01(\\x05\\""(\\n\\x11RandomJitterBoxes\\x12\\x13\\n\\x05ratio\\x18\\x01 \\x01(\\x02:\\x04\\x30.05\\""\\xd1\\x01\\n\\x0fRandomCropImage\\x12\\x1d\\n\\x12min_object_covered\\x18\\x01 \\x01(\\x02:\\x01\\x31\\x12\\x1e\\n\\x10min_aspect_ratio\\x18\\x02 \\x01(\\x02:\\x04\\x30.75\\x12\\x1e\\n\\x10max_aspect_ratio\\x18\\x03 \\x01(\\x02:\\x04\\x31.33\\x12\\x15\\n\\x08min_area\\x18\\x04 \\x01(\\x02:\\x03\\x30.1\\x12\\x13\\n\\x08max_area\\x18\\x05 \\x01(\\x02:\\x01\\x31\\x12\\x1b\\n\\x0eoverlap_thresh\\x18\\x06 \\x01(\\x02:\\x03\\x30.3\\x12\\x16\\n\\x0brandom_coef\\x18\\x07 \\x01(\\x02:\\x01\\x30\\""\\x89\\x01\\n\\x0eRandomPadImage\\x12\\x18\\n\\x10min_image_height\\x18\\x01 \\x01(\\x02\\x12\\x17\\n\\x0fmin_image_width\\x18\\x02 \\x01(\\x02\\x12\\x18\\n\\x10max_image_height\\x18\\x03 \\x01(\\x02\\x12\\x17\\n\\x0fmax_image_width\\x18\\x04 \\x01(\\x02\\x12\\x11\\n\\tpad_color\\x18\\x05 \\x03(\\x02\\""\\xa5\\x02\\n\\x12RandomCropPadImage\\x12\\x1d\\n\\x12min_object_covered\\x18\\x01 \\x01(\\x02:\\x01\\x31\\x12\\x1e\\n\\x10min_aspect_ratio\\x18\\x02 \\x01(\\x02:\\x04\\x30.75\\x12\\x1e\\n\\x10max_aspect_ratio\\x18\\x03 \\x01(\\x02:\\x04\\x31.33\\x12\\x15\\n\\x08min_area\\x18\\x04 \\x01(\\x02:\\x03\\x30.1\\x12\\x13\\n\\x08max_area\\x18\\x05 \\x01(\\x02:\\x01\\x31\\x12\\x1b\\n\\x0eoverlap_thresh\\x18\\x06 \\x01(\\x02:\\x03\\x30.3\\x12\\x16\\n\\x0brandom_coef\\x18\\x07 \\x01(\\x02:\\x01\\x30\\x12\\x1d\\n\\x15min_padded_size_ratio\\x18\\x08 \\x03(\\x02\\x12\\x1d\\n\\x15max_padded_size_ratio\\x18\\t \\x03(\\x02\\x12\\x11\\n\\tpad_color\\x18\\n \\x03(\\x02\\""O\\n\\x17RandomCropToAspectRatio\\x12\\x17\\n\\x0c\\x61spect_ratio\\x18\\x01 \\x01(\\x02:\\x01\\x31\\x12\\x1b\\n\\x0eoverlap_thresh\\x18\\x02 \\x01(\\x02:\\x03\\x30.3\\""o\\n\\x12RandomBlackPatches\\x12\\x1d\\n\\x11max_black_patches\\x18\\x01 \\x01(\\x05:\\x02\\x31\\x30\\x12\\x18\\n\\x0bprobability\\x18\\x02 \\x01(\\x02:\\x03\\x30.5\\x12 \\n\\x13size_to_image_ratio\\x18\\x03 \\x01(\\x02:\\x03\\x30.1\\""A\\n\\x12RandomResizeMethod\\x12\\x15\\n\\rtarget_height\\x18\\x01 \\x01(\\x02\\x12\\x14\\n\\x0ctarget_width\\x18\\x02 \\x01(\\x02\\""\\x0b\\n\\tRGBtoGray\\""\\x1e\\n\\x1cScaleBoxesToPixelCoordinates\\""\\xd0\\x01\\n\\x0bResizeImage\\x12\\x12\\n\\nnew_height\\x18\\x01 \\x01(\\x05\\x12\\x11\\n\\tnew_width\\x18\\x02 \\x01(\\x05\\x12U\\n\\x06method\\x18\\x03 \\x01(\\x0e\\x32;.rastervision.protos.tf_object_detection.ResizeImage.Method:\\x08\\x42ILINEAR\\""C\\n\\x06Method\\x12\\x08\\n\\x04\\x41REA\\x10\\x01\\x12\\x0b\\n\\x07\\x42ICUBIC\\x10\\x02\\x12\\x0c\\n\\x08\\x42ILINEAR\\x10\\x03\\x12\\x14\\n\\x10NEAREST_NEIGHBOR\\x10\\x04\\""$\\n\\x13SubtractChannelMean\\x12\\r\\n\\x05means\\x18\\x01 \\x03(\\x02\\""\\xb9\\x01\\n\\x16SSDRandomCropOperation\\x12\\x1a\\n\\x12min_object_covered\\x18\\x01 \\x01(\\x02\\x12\\x18\\n\\x10min_aspect_ratio\\x18\\x02 \\x01(\\x02\\x12\\x18\\n\\x10max_aspect_ratio\\x18\\x03 \\x01(\\x02\\x12\\x10\\n\\x08min_area\\x18\\x04 \\x01(\\x02\\x12\\x10\\n\\x08max_area\\x18\\x05 \\x01(\\x02\\x12\\x16\\n\\x0eoverlap_thresh\\x18\\x06 \\x01(\\x02\\x12\\x13\\n\\x0brandom_coef\\x18\\x07 \\x01(\\x02\\""d\\n\\rSSDRandomCrop\\x12S\\n\\noperations\\x18\\x01 \\x03(\\x0b\\x32?.rastervision.protos.tf_object_detection.SSDRandomCropOperation\\""\\xb9\\x02\\n\\x19SSDRandomCropPadOperation\\x12\\x1a\\n\\x12min_object_covered\\x18\\x01 \\x01(\\x02\\x12\\x18\\n\\x10min_aspect_ratio\\x18\\x02 \\x01(\\x02\\x12\\x18\\n\\x10max_aspect_ratio\\x18\\x03 \\x01(\\x02\\x12\\x10\\n\\x08min_area\\x18\\x04 \\x01(\\x02\\x12\\x10\\n\\x08max_area\\x18\\x05 \\x01(\\x02\\x12\\x16\\n\\x0eoverlap_thresh\\x18\\x06 \\x01(\\x02\\x12\\x13\\n\\x0brandom_coef\\x18\\x07 \\x01(\\x02\\x12\\x1d\\n\\x15min_padded_size_ratio\\x18\\x08 \\x03(\\x02\\x12\\x1d\\n\\x15max_padded_size_ratio\\x18\\t \\x03(\\x02\\x12\\x13\\n\\x0bpad_color_r\\x18\\n \\x01(\\x02\\x12\\x13\\n\\x0bpad_color_g\\x18\\x0b \\x01(\\x02\\x12\\x13\\n\\x0bpad_color_b\\x18\\x0c \\x01(\\x02\\""j\\n\\x10SSDRandomCropPad\\x12V\\n\\noperations\\x18\\x01 \\x03(\\x0b\\x32\\x42.rastervision.protos.tf_object_detection.SSDRandomCropPadOperation\\""\\x95\\x01\\n&SSDRandomCropFixedAspectRatioOperation\\x12\\x1a\\n\\x12min_object_covered\\x18\\x01 \\x01(\\x02\\x12\\x10\\n\\x08min_area\\x18\\x04 \\x01(\\x02\\x12\\x10\\n\\x08max_area\\x18\\x05 \\x01(\\x02\\x12\\x16\\n\\x0eoverlap_thresh\\x18\\x06 \\x01(\\x02\\x12\\x13\\n\\x0brandom_coef\\x18\\x07 \\x01(\\x02\\""\\x9d\\x01\\n\\x1dSSDRandomCropFixedAspectRatio\\x12\\x63\\n\\noperations\\x18\\x01 \\x03(\\x0b\\x32O.rastervision.protos.tf_object_detection.SSDRandomCropFixedAspectRatioOperation\\x12\\x17\\n\\x0c\\x61spect_ratio\\x18\\x02 \\x01(\\x02:\\x01\\x31\\""\\xcc\\x01\\n)SSDRandomCropPadFixedAspectRatioOperation\\x12\\x1a\\n\\x12min_object_covered\\x18\\x01 \\x01(\\x02\\x12\\x18\\n\\x10min_aspect_ratio\\x18\\x02 \\x01(\\x02\\x12\\x18\\n\\x10max_aspect_ratio\\x18\\x03 \\x01(\\x02\\x12\\x10\\n\\x08min_area\\x18\\x04 \\x01(\\x02\\x12\\x10\\n\\x08max_area\\x18\\x05 \\x01(\\x02\\x12\\x16\\n\\x0eoverlap_thresh\\x18\\x06 \\x01(\\x02\\x12\\x13\\n\\x0brandom_coef\\x18\\x07 \\x01(\\x02\\""\\xe1\\x01\\n SSDRandomCropPadFixedAspectRatio\\x12\\x66\\n\\noperations\\x18\\x01 \\x03(\\x0b\\x32R.rastervision.protos.tf_object_detection.SSDRandomCropPadFixedAspectRatioOperation\\x12\\x17\\n\\x0c\\x61spect_ratio\\x18\\x02 \\x01(\\x02:\\x01\\x31\\x12\\x1d\\n\\x15min_padded_size_ratio\\x18\\x03 \\x03(\\x02\\x12\\x1d\\n\\x15max_padded_size_ratio\\x18\\x04 \\x03(\\x02\\""5\\n\\x1b\\x43onvertClassLogitsToSoftmax\\x12\\x16\\n\\x0btemperature\\x18\\x01 \\x01(\\x02:\\x01\\x31\')\n)\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\n\n\n_RESIZEIMAGE_METHOD = _descriptor.EnumDescriptor(\n  name=\'Method\',\n  full_name=\'rastervision.protos.tf_object_detection.ResizeImage.Method\',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name=\'AREA\', index=0, number=1,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'BICUBIC\', index=1, number=2,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'BILINEAR\', index=2, number=3,\n      options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'NEAREST_NEIGHBOR\', index=3, number=4,\n      options=None,\n      type=None),\n  ],\n  containing_type=None,\n  options=None,\n  serialized_start=4677,\n  serialized_end=4744,\n)\n_sym_db.RegisterEnumDescriptor(_RESIZEIMAGE_METHOD)\n\n\n_PREPROCESSINGSTEP = _descriptor.Descriptor(\n  name=\'PreprocessingStep\',\n  full_name=\'rastervision.protos.tf_object_detection.PreprocessingStep\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'normalize_image\', full_name=\'rastervision.protos.tf_object_detection.PreprocessingStep.normalize_image\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'random_horizontal_flip\', full_name=\'rastervision.protos.tf_object_detection.PreprocessingStep.random_horizontal_flip\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'random_pixel_value_scale\', full_name=\'rastervision.protos.tf_object_detection.PreprocessingStep.random_pixel_value_scale\', index=2,\n      number=3, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'random_image_scale\', full_name=\'rastervision.protos.tf_object_detection.PreprocessingStep.random_image_scale\', index=3,\n      number=4, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'random_rgb_to_gray\', full_name=\'rastervision.protos.tf_object_detection.PreprocessingStep.random_rgb_to_gray\', index=4,\n      number=5, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'random_adjust_brightness\', full_name=\'rastervision.protos.tf_object_detection.PreprocessingStep.random_adjust_brightness\', index=5,\n      number=6, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'random_adjust_contrast\', full_name=\'rastervision.protos.tf_object_detection.PreprocessingStep.random_adjust_contrast\', index=6,\n      number=7, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'random_adjust_hue\', full_name=\'rastervision.protos.tf_object_detection.PreprocessingStep.random_adjust_hue\', index=7,\n      number=8, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'random_adjust_saturation\', full_name=\'rastervision.protos.tf_object_detection.PreprocessingStep.random_adjust_saturation\', index=8,\n      number=9, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'random_distort_color\', full_name=\'rastervision.protos.tf_object_detection.PreprocessingStep.random_distort_color\', index=9,\n      number=10, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'random_jitter_boxes\', full_name=\'rastervision.protos.tf_object_detection.PreprocessingStep.random_jitter_boxes\', index=10,\n      number=11, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'random_crop_image\', full_name=\'rastervision.protos.tf_object_detection.PreprocessingStep.random_crop_image\', index=11,\n      number=12, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'random_pad_image\', full_name=\'rastervision.protos.tf_object_detection.PreprocessingStep.random_pad_image\', index=12,\n      number=13, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'random_crop_pad_image\', full_name=\'rastervision.protos.tf_object_detection.PreprocessingStep.random_crop_pad_image\', index=13,\n      number=14, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'random_crop_to_aspect_ratio\', full_name=\'rastervision.protos.tf_object_detection.PreprocessingStep.random_crop_to_aspect_ratio\', index=14,\n      number=15, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'random_black_patches\', full_name=\'rastervision.protos.tf_object_detection.PreprocessingStep.random_black_patches\', index=15,\n      number=16, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'random_resize_method\', full_name=\'rastervision.protos.tf_object_detection.PreprocessingStep.random_resize_method\', index=16,\n      number=17, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'scale_boxes_to_pixel_coordinates\', full_name=\'rastervision.protos.tf_object_detection.PreprocessingStep.scale_boxes_to_pixel_coordinates\', index=17,\n      number=18, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'resize_image\', full_name=\'rastervision.protos.tf_object_detection.PreprocessingStep.resize_image\', index=18,\n      number=19, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'subtract_channel_mean\', full_name=\'rastervision.protos.tf_object_detection.PreprocessingStep.subtract_channel_mean\', index=19,\n      number=20, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'ssd_random_crop\', full_name=\'rastervision.protos.tf_object_detection.PreprocessingStep.ssd_random_crop\', index=20,\n      number=21, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'ssd_random_crop_pad\', full_name=\'rastervision.protos.tf_object_detection.PreprocessingStep.ssd_random_crop_pad\', index=21,\n      number=22, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'ssd_random_crop_fixed_aspect_ratio\', full_name=\'rastervision.protos.tf_object_detection.PreprocessingStep.ssd_random_crop_fixed_aspect_ratio\', index=22,\n      number=23, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'ssd_random_crop_pad_fixed_aspect_ratio\', full_name=\'rastervision.protos.tf_object_detection.PreprocessingStep.ssd_random_crop_pad_fixed_aspect_ratio\', index=23,\n      number=24, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'random_vertical_flip\', full_name=\'rastervision.protos.tf_object_detection.PreprocessingStep.random_vertical_flip\', index=24,\n      number=25, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'random_rotation90\', full_name=\'rastervision.protos.tf_object_detection.PreprocessingStep.random_rotation90\', index=25,\n      number=26, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'rgb_to_gray\', full_name=\'rastervision.protos.tf_object_detection.PreprocessingStep.rgb_to_gray\', index=26,\n      number=27, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'convert_class_logits_to_softmax\', full_name=\'rastervision.protos.tf_object_detection.PreprocessingStep.convert_class_logits_to_softmax\', index=27,\n      number=28, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n    _descriptor.OneofDescriptor(\n      name=\'preprocessing_step\', full_name=\'rastervision.protos.tf_object_detection.PreprocessingStep.preprocessing_step\',\n      index=0, containing_type=None, fields=[]),\n  ],\n  serialized_start=104,\n  serialized_end=2803,\n)\n\n\n_NORMALIZEIMAGE = _descriptor.Descriptor(\n  name=\'NormalizeImage\',\n  full_name=\'rastervision.protos.tf_object_detection.NormalizeImage\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'original_minval\', full_name=\'rastervision.protos.tf_object_detection.NormalizeImage.original_minval\', index=0,\n      number=1, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'original_maxval\', full_name=\'rastervision.protos.tf_object_detection.NormalizeImage.original_maxval\', index=1,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'target_minval\', full_name=\'rastervision.protos.tf_object_detection.NormalizeImage.target_minval\', index=2,\n      number=3, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'target_maxval\', full_name=\'rastervision.protos.tf_object_detection.NormalizeImage.target_maxval\', index=3,\n      number=4, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(1),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=2805,\n  serialized_end=2923,\n)\n\n\n_RANDOMHORIZONTALFLIP = _descriptor.Descriptor(\n  name=\'RandomHorizontalFlip\',\n  full_name=\'rastervision.protos.tf_object_detection.RandomHorizontalFlip\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'keypoint_flip_permutation\', full_name=\'rastervision.protos.tf_object_detection.RandomHorizontalFlip.keypoint_flip_permutation\', index=0,\n      number=1, type=5, cpp_type=1, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=2925,\n  serialized_end=2982,\n)\n\n\n_RANDOMVERTICALFLIP = _descriptor.Descriptor(\n  name=\'RandomVerticalFlip\',\n  full_name=\'rastervision.protos.tf_object_detection.RandomVerticalFlip\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'keypoint_flip_permutation\', full_name=\'rastervision.protos.tf_object_detection.RandomVerticalFlip.keypoint_flip_permutation\', index=0,\n      number=1, type=5, cpp_type=1, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=2984,\n  serialized_end=3039,\n)\n\n\n_RANDOMROTATION90 = _descriptor.Descriptor(\n  name=\'RandomRotation90\',\n  full_name=\'rastervision.protos.tf_object_detection.RandomRotation90\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=3041,\n  serialized_end=3059,\n)\n\n\n_RANDOMPIXELVALUESCALE = _descriptor.Descriptor(\n  name=\'RandomPixelValueScale\',\n  full_name=\'rastervision.protos.tf_object_detection.RandomPixelValueScale\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'minval\', full_name=\'rastervision.protos.tf_object_detection.RandomPixelValueScale.minval\', index=0,\n      number=1, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.9),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'maxval\', full_name=\'rastervision.protos.tf_object_detection.RandomPixelValueScale.maxval\', index=1,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(1.1),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=3061,\n  serialized_end=3126,\n)\n\n\n_RANDOMIMAGESCALE = _descriptor.Descriptor(\n  name=\'RandomImageScale\',\n  full_name=\'rastervision.protos.tf_object_detection.RandomImageScale\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'min_scale_ratio\', full_name=\'rastervision.protos.tf_object_detection.RandomImageScale.min_scale_ratio\', index=0,\n      number=1, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.5),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'max_scale_ratio\', full_name=\'rastervision.protos.tf_object_detection.RandomImageScale.max_scale_ratio\', index=1,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(2),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=3128,\n  serialized_end=3204,\n)\n\n\n_RANDOMRGBTOGRAY = _descriptor.Descriptor(\n  name=\'RandomRGBtoGray\',\n  full_name=\'rastervision.protos.tf_object_detection.RandomRGBtoGray\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'probability\', full_name=\'rastervision.protos.tf_object_detection.RandomRGBtoGray.probability\', index=0,\n      number=1, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.1),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=3206,\n  serialized_end=3249,\n)\n\n\n_RANDOMADJUSTBRIGHTNESS = _descriptor.Descriptor(\n  name=\'RandomAdjustBrightness\',\n  full_name=\'rastervision.protos.tf_object_detection.RandomAdjustBrightness\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'max_delta\', full_name=\'rastervision.protos.tf_object_detection.RandomAdjustBrightness.max_delta\', index=0,\n      number=1, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.2),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=3251,\n  serialized_end=3299,\n)\n\n\n_RANDOMADJUSTCONTRAST = _descriptor.Descriptor(\n  name=\'RandomAdjustContrast\',\n  full_name=\'rastervision.protos.tf_object_detection.RandomAdjustContrast\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'min_delta\', full_name=\'rastervision.protos.tf_object_detection.RandomAdjustContrast.min_delta\', index=0,\n      number=1, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.8),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'max_delta\', full_name=\'rastervision.protos.tf_object_detection.RandomAdjustContrast.max_delta\', index=1,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(1.25),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=3301,\n  serialized_end=3372,\n)\n\n\n_RANDOMADJUSTHUE = _descriptor.Descriptor(\n  name=\'RandomAdjustHue\',\n  full_name=\'rastervision.protos.tf_object_detection.RandomAdjustHue\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'max_delta\', full_name=\'rastervision.protos.tf_object_detection.RandomAdjustHue.max_delta\', index=0,\n      number=1, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.02),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=3374,\n  serialized_end=3416,\n)\n\n\n_RANDOMADJUSTSATURATION = _descriptor.Descriptor(\n  name=\'RandomAdjustSaturation\',\n  full_name=\'rastervision.protos.tf_object_detection.RandomAdjustSaturation\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'min_delta\', full_name=\'rastervision.protos.tf_object_detection.RandomAdjustSaturation.min_delta\', index=0,\n      number=1, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.8),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'max_delta\', full_name=\'rastervision.protos.tf_object_detection.RandomAdjustSaturation.max_delta\', index=1,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(1.25),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=3418,\n  serialized_end=3491,\n)\n\n\n_RANDOMDISTORTCOLOR = _descriptor.Descriptor(\n  name=\'RandomDistortColor\',\n  full_name=\'rastervision.protos.tf_object_detection.RandomDistortColor\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'color_ordering\', full_name=\'rastervision.protos.tf_object_detection.RandomDistortColor.color_ordering\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=3493,\n  serialized_end=3537,\n)\n\n\n_RANDOMJITTERBOXES = _descriptor.Descriptor(\n  name=\'RandomJitterBoxes\',\n  full_name=\'rastervision.protos.tf_object_detection.RandomJitterBoxes\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'ratio\', full_name=\'rastervision.protos.tf_object_detection.RandomJitterBoxes.ratio\', index=0,\n      number=1, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.05),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=3539,\n  serialized_end=3579,\n)\n\n\n_RANDOMCROPIMAGE = _descriptor.Descriptor(\n  name=\'RandomCropImage\',\n  full_name=\'rastervision.protos.tf_object_detection.RandomCropImage\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'min_object_covered\', full_name=\'rastervision.protos.tf_object_detection.RandomCropImage.min_object_covered\', index=0,\n      number=1, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(1),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'min_aspect_ratio\', full_name=\'rastervision.protos.tf_object_detection.RandomCropImage.min_aspect_ratio\', index=1,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.75),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'max_aspect_ratio\', full_name=\'rastervision.protos.tf_object_detection.RandomCropImage.max_aspect_ratio\', index=2,\n      number=3, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(1.33),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'min_area\', full_name=\'rastervision.protos.tf_object_detection.RandomCropImage.min_area\', index=3,\n      number=4, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.1),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'max_area\', full_name=\'rastervision.protos.tf_object_detection.RandomCropImage.max_area\', index=4,\n      number=5, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(1),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'overlap_thresh\', full_name=\'rastervision.protos.tf_object_detection.RandomCropImage.overlap_thresh\', index=5,\n      number=6, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.3),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'random_coef\', full_name=\'rastervision.protos.tf_object_detection.RandomCropImage.random_coef\', index=6,\n      number=7, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=3582,\n  serialized_end=3791,\n)\n\n\n_RANDOMPADIMAGE = _descriptor.Descriptor(\n  name=\'RandomPadImage\',\n  full_name=\'rastervision.protos.tf_object_detection.RandomPadImage\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'min_image_height\', full_name=\'rastervision.protos.tf_object_detection.RandomPadImage.min_image_height\', index=0,\n      number=1, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'min_image_width\', full_name=\'rastervision.protos.tf_object_detection.RandomPadImage.min_image_width\', index=1,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'max_image_height\', full_name=\'rastervision.protos.tf_object_detection.RandomPadImage.max_image_height\', index=2,\n      number=3, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'max_image_width\', full_name=\'rastervision.protos.tf_object_detection.RandomPadImage.max_image_width\', index=3,\n      number=4, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'pad_color\', full_name=\'rastervision.protos.tf_object_detection.RandomPadImage.pad_color\', index=4,\n      number=5, type=2, cpp_type=6, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=3794,\n  serialized_end=3931,\n)\n\n\n_RANDOMCROPPADIMAGE = _descriptor.Descriptor(\n  name=\'RandomCropPadImage\',\n  full_name=\'rastervision.protos.tf_object_detection.RandomCropPadImage\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'min_object_covered\', full_name=\'rastervision.protos.tf_object_detection.RandomCropPadImage.min_object_covered\', index=0,\n      number=1, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(1),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'min_aspect_ratio\', full_name=\'rastervision.protos.tf_object_detection.RandomCropPadImage.min_aspect_ratio\', index=1,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.75),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'max_aspect_ratio\', full_name=\'rastervision.protos.tf_object_detection.RandomCropPadImage.max_aspect_ratio\', index=2,\n      number=3, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(1.33),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'min_area\', full_name=\'rastervision.protos.tf_object_detection.RandomCropPadImage.min_area\', index=3,\n      number=4, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.1),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'max_area\', full_name=\'rastervision.protos.tf_object_detection.RandomCropPadImage.max_area\', index=4,\n      number=5, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(1),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'overlap_thresh\', full_name=\'rastervision.protos.tf_object_detection.RandomCropPadImage.overlap_thresh\', index=5,\n      number=6, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.3),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'random_coef\', full_name=\'rastervision.protos.tf_object_detection.RandomCropPadImage.random_coef\', index=6,\n      number=7, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'min_padded_size_ratio\', full_name=\'rastervision.protos.tf_object_detection.RandomCropPadImage.min_padded_size_ratio\', index=7,\n      number=8, type=2, cpp_type=6, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'max_padded_size_ratio\', full_name=\'rastervision.protos.tf_object_detection.RandomCropPadImage.max_padded_size_ratio\', index=8,\n      number=9, type=2, cpp_type=6, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'pad_color\', full_name=\'rastervision.protos.tf_object_detection.RandomCropPadImage.pad_color\', index=9,\n      number=10, type=2, cpp_type=6, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=3934,\n  serialized_end=4227,\n)\n\n\n_RANDOMCROPTOASPECTRATIO = _descriptor.Descriptor(\n  name=\'RandomCropToAspectRatio\',\n  full_name=\'rastervision.protos.tf_object_detection.RandomCropToAspectRatio\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'aspect_ratio\', full_name=\'rastervision.protos.tf_object_detection.RandomCropToAspectRatio.aspect_ratio\', index=0,\n      number=1, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(1),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'overlap_thresh\', full_name=\'rastervision.protos.tf_object_detection.RandomCropToAspectRatio.overlap_thresh\', index=1,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.3),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=4229,\n  serialized_end=4308,\n)\n\n\n_RANDOMBLACKPATCHES = _descriptor.Descriptor(\n  name=\'RandomBlackPatches\',\n  full_name=\'rastervision.protos.tf_object_detection.RandomBlackPatches\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'max_black_patches\', full_name=\'rastervision.protos.tf_object_detection.RandomBlackPatches.max_black_patches\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=10,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'probability\', full_name=\'rastervision.protos.tf_object_detection.RandomBlackPatches.probability\', index=1,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.5),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'size_to_image_ratio\', full_name=\'rastervision.protos.tf_object_detection.RandomBlackPatches.size_to_image_ratio\', index=2,\n      number=3, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.1),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=4310,\n  serialized_end=4421,\n)\n\n\n_RANDOMRESIZEMETHOD = _descriptor.Descriptor(\n  name=\'RandomResizeMethod\',\n  full_name=\'rastervision.protos.tf_object_detection.RandomResizeMethod\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'target_height\', full_name=\'rastervision.protos.tf_object_detection.RandomResizeMethod.target_height\', index=0,\n      number=1, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'target_width\', full_name=\'rastervision.protos.tf_object_detection.RandomResizeMethod.target_width\', index=1,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=4423,\n  serialized_end=4488,\n)\n\n\n_RGBTOGRAY = _descriptor.Descriptor(\n  name=\'RGBtoGray\',\n  full_name=\'rastervision.protos.tf_object_detection.RGBtoGray\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=4490,\n  serialized_end=4501,\n)\n\n\n_SCALEBOXESTOPIXELCOORDINATES = _descriptor.Descriptor(\n  name=\'ScaleBoxesToPixelCoordinates\',\n  full_name=\'rastervision.protos.tf_object_detection.ScaleBoxesToPixelCoordinates\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=4503,\n  serialized_end=4533,\n)\n\n\n_RESIZEIMAGE = _descriptor.Descriptor(\n  name=\'ResizeImage\',\n  full_name=\'rastervision.protos.tf_object_detection.ResizeImage\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'new_height\', full_name=\'rastervision.protos.tf_object_detection.ResizeImage.new_height\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'new_width\', full_name=\'rastervision.protos.tf_object_detection.ResizeImage.new_width\', index=1,\n      number=2, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'method\', full_name=\'rastervision.protos.tf_object_detection.ResizeImage.method\', index=2,\n      number=3, type=14, cpp_type=8, label=1,\n      has_default_value=True, default_value=3,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n    _RESIZEIMAGE_METHOD,\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=4536,\n  serialized_end=4744,\n)\n\n\n_SUBTRACTCHANNELMEAN = _descriptor.Descriptor(\n  name=\'SubtractChannelMean\',\n  full_name=\'rastervision.protos.tf_object_detection.SubtractChannelMean\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'means\', full_name=\'rastervision.protos.tf_object_detection.SubtractChannelMean.means\', index=0,\n      number=1, type=2, cpp_type=6, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=4746,\n  serialized_end=4782,\n)\n\n\n_SSDRANDOMCROPOPERATION = _descriptor.Descriptor(\n  name=\'SSDRandomCropOperation\',\n  full_name=\'rastervision.protos.tf_object_detection.SSDRandomCropOperation\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'min_object_covered\', full_name=\'rastervision.protos.tf_object_detection.SSDRandomCropOperation.min_object_covered\', index=0,\n      number=1, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'min_aspect_ratio\', full_name=\'rastervision.protos.tf_object_detection.SSDRandomCropOperation.min_aspect_ratio\', index=1,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'max_aspect_ratio\', full_name=\'rastervision.protos.tf_object_detection.SSDRandomCropOperation.max_aspect_ratio\', index=2,\n      number=3, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'min_area\', full_name=\'rastervision.protos.tf_object_detection.SSDRandomCropOperation.min_area\', index=3,\n      number=4, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'max_area\', full_name=\'rastervision.protos.tf_object_detection.SSDRandomCropOperation.max_area\', index=4,\n      number=5, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'overlap_thresh\', full_name=\'rastervision.protos.tf_object_detection.SSDRandomCropOperation.overlap_thresh\', index=5,\n      number=6, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'random_coef\', full_name=\'rastervision.protos.tf_object_detection.SSDRandomCropOperation.random_coef\', index=6,\n      number=7, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=4785,\n  serialized_end=4970,\n)\n\n\n_SSDRANDOMCROP = _descriptor.Descriptor(\n  name=\'SSDRandomCrop\',\n  full_name=\'rastervision.protos.tf_object_detection.SSDRandomCrop\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'operations\', full_name=\'rastervision.protos.tf_object_detection.SSDRandomCrop.operations\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=4972,\n  serialized_end=5072,\n)\n\n\n_SSDRANDOMCROPPADOPERATION = _descriptor.Descriptor(\n  name=\'SSDRandomCropPadOperation\',\n  full_name=\'rastervision.protos.tf_object_detection.SSDRandomCropPadOperation\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'min_object_covered\', full_name=\'rastervision.protos.tf_object_detection.SSDRandomCropPadOperation.min_object_covered\', index=0,\n      number=1, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'min_aspect_ratio\', full_name=\'rastervision.protos.tf_object_detection.SSDRandomCropPadOperation.min_aspect_ratio\', index=1,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'max_aspect_ratio\', full_name=\'rastervision.protos.tf_object_detection.SSDRandomCropPadOperation.max_aspect_ratio\', index=2,\n      number=3, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'min_area\', full_name=\'rastervision.protos.tf_object_detection.SSDRandomCropPadOperation.min_area\', index=3,\n      number=4, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'max_area\', full_name=\'rastervision.protos.tf_object_detection.SSDRandomCropPadOperation.max_area\', index=4,\n      number=5, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'overlap_thresh\', full_name=\'rastervision.protos.tf_object_detection.SSDRandomCropPadOperation.overlap_thresh\', index=5,\n      number=6, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'random_coef\', full_name=\'rastervision.protos.tf_object_detection.SSDRandomCropPadOperation.random_coef\', index=6,\n      number=7, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'min_padded_size_ratio\', full_name=\'rastervision.protos.tf_object_detection.SSDRandomCropPadOperation.min_padded_size_ratio\', index=7,\n      number=8, type=2, cpp_type=6, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'max_padded_size_ratio\', full_name=\'rastervision.protos.tf_object_detection.SSDRandomCropPadOperation.max_padded_size_ratio\', index=8,\n      number=9, type=2, cpp_type=6, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'pad_color_r\', full_name=\'rastervision.protos.tf_object_detection.SSDRandomCropPadOperation.pad_color_r\', index=9,\n      number=10, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'pad_color_g\', full_name=\'rastervision.protos.tf_object_detection.SSDRandomCropPadOperation.pad_color_g\', index=10,\n      number=11, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'pad_color_b\', full_name=\'rastervision.protos.tf_object_detection.SSDRandomCropPadOperation.pad_color_b\', index=11,\n      number=12, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=5075,\n  serialized_end=5388,\n)\n\n\n_SSDRANDOMCROPPAD = _descriptor.Descriptor(\n  name=\'SSDRandomCropPad\',\n  full_name=\'rastervision.protos.tf_object_detection.SSDRandomCropPad\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'operations\', full_name=\'rastervision.protos.tf_object_detection.SSDRandomCropPad.operations\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=5390,\n  serialized_end=5496,\n)\n\n\n_SSDRANDOMCROPFIXEDASPECTRATIOOPERATION = _descriptor.Descriptor(\n  name=\'SSDRandomCropFixedAspectRatioOperation\',\n  full_name=\'rastervision.protos.tf_object_detection.SSDRandomCropFixedAspectRatioOperation\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'min_object_covered\', full_name=\'rastervision.protos.tf_object_detection.SSDRandomCropFixedAspectRatioOperation.min_object_covered\', index=0,\n      number=1, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'min_area\', full_name=\'rastervision.protos.tf_object_detection.SSDRandomCropFixedAspectRatioOperation.min_area\', index=1,\n      number=4, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'max_area\', full_name=\'rastervision.protos.tf_object_detection.SSDRandomCropFixedAspectRatioOperation.max_area\', index=2,\n      number=5, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'overlap_thresh\', full_name=\'rastervision.protos.tf_object_detection.SSDRandomCropFixedAspectRatioOperation.overlap_thresh\', index=3,\n      number=6, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'random_coef\', full_name=\'rastervision.protos.tf_object_detection.SSDRandomCropFixedAspectRatioOperation.random_coef\', index=4,\n      number=7, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=5499,\n  serialized_end=5648,\n)\n\n\n_SSDRANDOMCROPFIXEDASPECTRATIO = _descriptor.Descriptor(\n  name=\'SSDRandomCropFixedAspectRatio\',\n  full_name=\'rastervision.protos.tf_object_detection.SSDRandomCropFixedAspectRatio\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'operations\', full_name=\'rastervision.protos.tf_object_detection.SSDRandomCropFixedAspectRatio.operations\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'aspect_ratio\', full_name=\'rastervision.protos.tf_object_detection.SSDRandomCropFixedAspectRatio.aspect_ratio\', index=1,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(1),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=5651,\n  serialized_end=5808,\n)\n\n\n_SSDRANDOMCROPPADFIXEDASPECTRATIOOPERATION = _descriptor.Descriptor(\n  name=\'SSDRandomCropPadFixedAspectRatioOperation\',\n  full_name=\'rastervision.protos.tf_object_detection.SSDRandomCropPadFixedAspectRatioOperation\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'min_object_covered\', full_name=\'rastervision.protos.tf_object_detection.SSDRandomCropPadFixedAspectRatioOperation.min_object_covered\', index=0,\n      number=1, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'min_aspect_ratio\', full_name=\'rastervision.protos.tf_object_detection.SSDRandomCropPadFixedAspectRatioOperation.min_aspect_ratio\', index=1,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'max_aspect_ratio\', full_name=\'rastervision.protos.tf_object_detection.SSDRandomCropPadFixedAspectRatioOperation.max_aspect_ratio\', index=2,\n      number=3, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'min_area\', full_name=\'rastervision.protos.tf_object_detection.SSDRandomCropPadFixedAspectRatioOperation.min_area\', index=3,\n      number=4, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'max_area\', full_name=\'rastervision.protos.tf_object_detection.SSDRandomCropPadFixedAspectRatioOperation.max_area\', index=4,\n      number=5, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'overlap_thresh\', full_name=\'rastervision.protos.tf_object_detection.SSDRandomCropPadFixedAspectRatioOperation.overlap_thresh\', index=5,\n      number=6, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'random_coef\', full_name=\'rastervision.protos.tf_object_detection.SSDRandomCropPadFixedAspectRatioOperation.random_coef\', index=6,\n      number=7, type=2, cpp_type=6, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=5811,\n  serialized_end=6015,\n)\n\n\n_SSDRANDOMCROPPADFIXEDASPECTRATIO = _descriptor.Descriptor(\n  name=\'SSDRandomCropPadFixedAspectRatio\',\n  full_name=\'rastervision.protos.tf_object_detection.SSDRandomCropPadFixedAspectRatio\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'operations\', full_name=\'rastervision.protos.tf_object_detection.SSDRandomCropPadFixedAspectRatio.operations\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'aspect_ratio\', full_name=\'rastervision.protos.tf_object_detection.SSDRandomCropPadFixedAspectRatio.aspect_ratio\', index=1,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(1),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'min_padded_size_ratio\', full_name=\'rastervision.protos.tf_object_detection.SSDRandomCropPadFixedAspectRatio.min_padded_size_ratio\', index=2,\n      number=3, type=2, cpp_type=6, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'max_padded_size_ratio\', full_name=\'rastervision.protos.tf_object_detection.SSDRandomCropPadFixedAspectRatio.max_padded_size_ratio\', index=3,\n      number=4, type=2, cpp_type=6, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=6018,\n  serialized_end=6243,\n)\n\n\n_CONVERTCLASSLOGITSTOSOFTMAX = _descriptor.Descriptor(\n  name=\'ConvertClassLogitsToSoftmax\',\n  full_name=\'rastervision.protos.tf_object_detection.ConvertClassLogitsToSoftmax\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'temperature\', full_name=\'rastervision.protos.tf_object_detection.ConvertClassLogitsToSoftmax.temperature\', index=0,\n      number=1, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(1),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=6245,\n  serialized_end=6298,\n)\n\n_PREPROCESSINGSTEP.fields_by_name[\'normalize_image\'].message_type = _NORMALIZEIMAGE\n_PREPROCESSINGSTEP.fields_by_name[\'random_horizontal_flip\'].message_type = _RANDOMHORIZONTALFLIP\n_PREPROCESSINGSTEP.fields_by_name[\'random_pixel_value_scale\'].message_type = _RANDOMPIXELVALUESCALE\n_PREPROCESSINGSTEP.fields_by_name[\'random_image_scale\'].message_type = _RANDOMIMAGESCALE\n_PREPROCESSINGSTEP.fields_by_name[\'random_rgb_to_gray\'].message_type = _RANDOMRGBTOGRAY\n_PREPROCESSINGSTEP.fields_by_name[\'random_adjust_brightness\'].message_type = _RANDOMADJUSTBRIGHTNESS\n_PREPROCESSINGSTEP.fields_by_name[\'random_adjust_contrast\'].message_type = _RANDOMADJUSTCONTRAST\n_PREPROCESSINGSTEP.fields_by_name[\'random_adjust_hue\'].message_type = _RANDOMADJUSTHUE\n_PREPROCESSINGSTEP.fields_by_name[\'random_adjust_saturation\'].message_type = _RANDOMADJUSTSATURATION\n_PREPROCESSINGSTEP.fields_by_name[\'random_distort_color\'].message_type = _RANDOMDISTORTCOLOR\n_PREPROCESSINGSTEP.fields_by_name[\'random_jitter_boxes\'].message_type = _RANDOMJITTERBOXES\n_PREPROCESSINGSTEP.fields_by_name[\'random_crop_image\'].message_type = _RANDOMCROPIMAGE\n_PREPROCESSINGSTEP.fields_by_name[\'random_pad_image\'].message_type = _RANDOMPADIMAGE\n_PREPROCESSINGSTEP.fields_by_name[\'random_crop_pad_image\'].message_type = _RANDOMCROPPADIMAGE\n_PREPROCESSINGSTEP.fields_by_name[\'random_crop_to_aspect_ratio\'].message_type = _RANDOMCROPTOASPECTRATIO\n_PREPROCESSINGSTEP.fields_by_name[\'random_black_patches\'].message_type = _RANDOMBLACKPATCHES\n_PREPROCESSINGSTEP.fields_by_name[\'random_resize_method\'].message_type = _RANDOMRESIZEMETHOD\n_PREPROCESSINGSTEP.fields_by_name[\'scale_boxes_to_pixel_coordinates\'].message_type = _SCALEBOXESTOPIXELCOORDINATES\n_PREPROCESSINGSTEP.fields_by_name[\'resize_image\'].message_type = _RESIZEIMAGE\n_PREPROCESSINGSTEP.fields_by_name[\'subtract_channel_mean\'].message_type = _SUBTRACTCHANNELMEAN\n_PREPROCESSINGSTEP.fields_by_name[\'ssd_random_crop\'].message_type = _SSDRANDOMCROP\n_PREPROCESSINGSTEP.fields_by_name[\'ssd_random_crop_pad\'].message_type = _SSDRANDOMCROPPAD\n_PREPROCESSINGSTEP.fields_by_name[\'ssd_random_crop_fixed_aspect_ratio\'].message_type = _SSDRANDOMCROPFIXEDASPECTRATIO\n_PREPROCESSINGSTEP.fields_by_name[\'ssd_random_crop_pad_fixed_aspect_ratio\'].message_type = _SSDRANDOMCROPPADFIXEDASPECTRATIO\n_PREPROCESSINGSTEP.fields_by_name[\'random_vertical_flip\'].message_type = _RANDOMVERTICALFLIP\n_PREPROCESSINGSTEP.fields_by_name[\'random_rotation90\'].message_type = _RANDOMROTATION90\n_PREPROCESSINGSTEP.fields_by_name[\'rgb_to_gray\'].message_type = _RGBTOGRAY\n_PREPROCESSINGSTEP.fields_by_name[\'convert_class_logits_to_softmax\'].message_type = _CONVERTCLASSLOGITSTOSOFTMAX\n_PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\'].fields.append(\n  _PREPROCESSINGSTEP.fields_by_name[\'normalize_image\'])\n_PREPROCESSINGSTEP.fields_by_name[\'normalize_image\'].containing_oneof = _PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\']\n_PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\'].fields.append(\n  _PREPROCESSINGSTEP.fields_by_name[\'random_horizontal_flip\'])\n_PREPROCESSINGSTEP.fields_by_name[\'random_horizontal_flip\'].containing_oneof = _PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\']\n_PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\'].fields.append(\n  _PREPROCESSINGSTEP.fields_by_name[\'random_pixel_value_scale\'])\n_PREPROCESSINGSTEP.fields_by_name[\'random_pixel_value_scale\'].containing_oneof = _PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\']\n_PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\'].fields.append(\n  _PREPROCESSINGSTEP.fields_by_name[\'random_image_scale\'])\n_PREPROCESSINGSTEP.fields_by_name[\'random_image_scale\'].containing_oneof = _PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\']\n_PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\'].fields.append(\n  _PREPROCESSINGSTEP.fields_by_name[\'random_rgb_to_gray\'])\n_PREPROCESSINGSTEP.fields_by_name[\'random_rgb_to_gray\'].containing_oneof = _PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\']\n_PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\'].fields.append(\n  _PREPROCESSINGSTEP.fields_by_name[\'random_adjust_brightness\'])\n_PREPROCESSINGSTEP.fields_by_name[\'random_adjust_brightness\'].containing_oneof = _PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\']\n_PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\'].fields.append(\n  _PREPROCESSINGSTEP.fields_by_name[\'random_adjust_contrast\'])\n_PREPROCESSINGSTEP.fields_by_name[\'random_adjust_contrast\'].containing_oneof = _PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\']\n_PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\'].fields.append(\n  _PREPROCESSINGSTEP.fields_by_name[\'random_adjust_hue\'])\n_PREPROCESSINGSTEP.fields_by_name[\'random_adjust_hue\'].containing_oneof = _PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\']\n_PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\'].fields.append(\n  _PREPROCESSINGSTEP.fields_by_name[\'random_adjust_saturation\'])\n_PREPROCESSINGSTEP.fields_by_name[\'random_adjust_saturation\'].containing_oneof = _PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\']\n_PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\'].fields.append(\n  _PREPROCESSINGSTEP.fields_by_name[\'random_distort_color\'])\n_PREPROCESSINGSTEP.fields_by_name[\'random_distort_color\'].containing_oneof = _PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\']\n_PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\'].fields.append(\n  _PREPROCESSINGSTEP.fields_by_name[\'random_jitter_boxes\'])\n_PREPROCESSINGSTEP.fields_by_name[\'random_jitter_boxes\'].containing_oneof = _PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\']\n_PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\'].fields.append(\n  _PREPROCESSINGSTEP.fields_by_name[\'random_crop_image\'])\n_PREPROCESSINGSTEP.fields_by_name[\'random_crop_image\'].containing_oneof = _PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\']\n_PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\'].fields.append(\n  _PREPROCESSINGSTEP.fields_by_name[\'random_pad_image\'])\n_PREPROCESSINGSTEP.fields_by_name[\'random_pad_image\'].containing_oneof = _PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\']\n_PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\'].fields.append(\n  _PREPROCESSINGSTEP.fields_by_name[\'random_crop_pad_image\'])\n_PREPROCESSINGSTEP.fields_by_name[\'random_crop_pad_image\'].containing_oneof = _PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\']\n_PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\'].fields.append(\n  _PREPROCESSINGSTEP.fields_by_name[\'random_crop_to_aspect_ratio\'])\n_PREPROCESSINGSTEP.fields_by_name[\'random_crop_to_aspect_ratio\'].containing_oneof = _PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\']\n_PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\'].fields.append(\n  _PREPROCESSINGSTEP.fields_by_name[\'random_black_patches\'])\n_PREPROCESSINGSTEP.fields_by_name[\'random_black_patches\'].containing_oneof = _PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\']\n_PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\'].fields.append(\n  _PREPROCESSINGSTEP.fields_by_name[\'random_resize_method\'])\n_PREPROCESSINGSTEP.fields_by_name[\'random_resize_method\'].containing_oneof = _PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\']\n_PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\'].fields.append(\n  _PREPROCESSINGSTEP.fields_by_name[\'scale_boxes_to_pixel_coordinates\'])\n_PREPROCESSINGSTEP.fields_by_name[\'scale_boxes_to_pixel_coordinates\'].containing_oneof = _PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\']\n_PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\'].fields.append(\n  _PREPROCESSINGSTEP.fields_by_name[\'resize_image\'])\n_PREPROCESSINGSTEP.fields_by_name[\'resize_image\'].containing_oneof = _PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\']\n_PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\'].fields.append(\n  _PREPROCESSINGSTEP.fields_by_name[\'subtract_channel_mean\'])\n_PREPROCESSINGSTEP.fields_by_name[\'subtract_channel_mean\'].containing_oneof = _PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\']\n_PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\'].fields.append(\n  _PREPROCESSINGSTEP.fields_by_name[\'ssd_random_crop\'])\n_PREPROCESSINGSTEP.fields_by_name[\'ssd_random_crop\'].containing_oneof = _PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\']\n_PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\'].fields.append(\n  _PREPROCESSINGSTEP.fields_by_name[\'ssd_random_crop_pad\'])\n_PREPROCESSINGSTEP.fields_by_name[\'ssd_random_crop_pad\'].containing_oneof = _PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\']\n_PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\'].fields.append(\n  _PREPROCESSINGSTEP.fields_by_name[\'ssd_random_crop_fixed_aspect_ratio\'])\n_PREPROCESSINGSTEP.fields_by_name[\'ssd_random_crop_fixed_aspect_ratio\'].containing_oneof = _PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\']\n_PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\'].fields.append(\n  _PREPROCESSINGSTEP.fields_by_name[\'ssd_random_crop_pad_fixed_aspect_ratio\'])\n_PREPROCESSINGSTEP.fields_by_name[\'ssd_random_crop_pad_fixed_aspect_ratio\'].containing_oneof = _PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\']\n_PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\'].fields.append(\n  _PREPROCESSINGSTEP.fields_by_name[\'random_vertical_flip\'])\n_PREPROCESSINGSTEP.fields_by_name[\'random_vertical_flip\'].containing_oneof = _PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\']\n_PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\'].fields.append(\n  _PREPROCESSINGSTEP.fields_by_name[\'random_rotation90\'])\n_PREPROCESSINGSTEP.fields_by_name[\'random_rotation90\'].containing_oneof = _PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\']\n_PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\'].fields.append(\n  _PREPROCESSINGSTEP.fields_by_name[\'rgb_to_gray\'])\n_PREPROCESSINGSTEP.fields_by_name[\'rgb_to_gray\'].containing_oneof = _PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\']\n_PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\'].fields.append(\n  _PREPROCESSINGSTEP.fields_by_name[\'convert_class_logits_to_softmax\'])\n_PREPROCESSINGSTEP.fields_by_name[\'convert_class_logits_to_softmax\'].containing_oneof = _PREPROCESSINGSTEP.oneofs_by_name[\'preprocessing_step\']\n_RESIZEIMAGE.fields_by_name[\'method\'].enum_type = _RESIZEIMAGE_METHOD\n_RESIZEIMAGE_METHOD.containing_type = _RESIZEIMAGE\n_SSDRANDOMCROP.fields_by_name[\'operations\'].message_type = _SSDRANDOMCROPOPERATION\n_SSDRANDOMCROPPAD.fields_by_name[\'operations\'].message_type = _SSDRANDOMCROPPADOPERATION\n_SSDRANDOMCROPFIXEDASPECTRATIO.fields_by_name[\'operations\'].message_type = _SSDRANDOMCROPFIXEDASPECTRATIOOPERATION\n_SSDRANDOMCROPPADFIXEDASPECTRATIO.fields_by_name[\'operations\'].message_type = _SSDRANDOMCROPPADFIXEDASPECTRATIOOPERATION\nDESCRIPTOR.message_types_by_name[\'PreprocessingStep\'] = _PREPROCESSINGSTEP\nDESCRIPTOR.message_types_by_name[\'NormalizeImage\'] = _NORMALIZEIMAGE\nDESCRIPTOR.message_types_by_name[\'RandomHorizontalFlip\'] = _RANDOMHORIZONTALFLIP\nDESCRIPTOR.message_types_by_name[\'RandomVerticalFlip\'] = _RANDOMVERTICALFLIP\nDESCRIPTOR.message_types_by_name[\'RandomRotation90\'] = _RANDOMROTATION90\nDESCRIPTOR.message_types_by_name[\'RandomPixelValueScale\'] = _RANDOMPIXELVALUESCALE\nDESCRIPTOR.message_types_by_name[\'RandomImageScale\'] = _RANDOMIMAGESCALE\nDESCRIPTOR.message_types_by_name[\'RandomRGBtoGray\'] = _RANDOMRGBTOGRAY\nDESCRIPTOR.message_types_by_name[\'RandomAdjustBrightness\'] = _RANDOMADJUSTBRIGHTNESS\nDESCRIPTOR.message_types_by_name[\'RandomAdjustContrast\'] = _RANDOMADJUSTCONTRAST\nDESCRIPTOR.message_types_by_name[\'RandomAdjustHue\'] = _RANDOMADJUSTHUE\nDESCRIPTOR.message_types_by_name[\'RandomAdjustSaturation\'] = _RANDOMADJUSTSATURATION\nDESCRIPTOR.message_types_by_name[\'RandomDistortColor\'] = _RANDOMDISTORTCOLOR\nDESCRIPTOR.message_types_by_name[\'RandomJitterBoxes\'] = _RANDOMJITTERBOXES\nDESCRIPTOR.message_types_by_name[\'RandomCropImage\'] = _RANDOMCROPIMAGE\nDESCRIPTOR.message_types_by_name[\'RandomPadImage\'] = _RANDOMPADIMAGE\nDESCRIPTOR.message_types_by_name[\'RandomCropPadImage\'] = _RANDOMCROPPADIMAGE\nDESCRIPTOR.message_types_by_name[\'RandomCropToAspectRatio\'] = _RANDOMCROPTOASPECTRATIO\nDESCRIPTOR.message_types_by_name[\'RandomBlackPatches\'] = _RANDOMBLACKPATCHES\nDESCRIPTOR.message_types_by_name[\'RandomResizeMethod\'] = _RANDOMRESIZEMETHOD\nDESCRIPTOR.message_types_by_name[\'RGBtoGray\'] = _RGBTOGRAY\nDESCRIPTOR.message_types_by_name[\'ScaleBoxesToPixelCoordinates\'] = _SCALEBOXESTOPIXELCOORDINATES\nDESCRIPTOR.message_types_by_name[\'ResizeImage\'] = _RESIZEIMAGE\nDESCRIPTOR.message_types_by_name[\'SubtractChannelMean\'] = _SUBTRACTCHANNELMEAN\nDESCRIPTOR.message_types_by_name[\'SSDRandomCropOperation\'] = _SSDRANDOMCROPOPERATION\nDESCRIPTOR.message_types_by_name[\'SSDRandomCrop\'] = _SSDRANDOMCROP\nDESCRIPTOR.message_types_by_name[\'SSDRandomCropPadOperation\'] = _SSDRANDOMCROPPADOPERATION\nDESCRIPTOR.message_types_by_name[\'SSDRandomCropPad\'] = _SSDRANDOMCROPPAD\nDESCRIPTOR.message_types_by_name[\'SSDRandomCropFixedAspectRatioOperation\'] = _SSDRANDOMCROPFIXEDASPECTRATIOOPERATION\nDESCRIPTOR.message_types_by_name[\'SSDRandomCropFixedAspectRatio\'] = _SSDRANDOMCROPFIXEDASPECTRATIO\nDESCRIPTOR.message_types_by_name[\'SSDRandomCropPadFixedAspectRatioOperation\'] = _SSDRANDOMCROPPADFIXEDASPECTRATIOOPERATION\nDESCRIPTOR.message_types_by_name[\'SSDRandomCropPadFixedAspectRatio\'] = _SSDRANDOMCROPPADFIXEDASPECTRATIO\nDESCRIPTOR.message_types_by_name[\'ConvertClassLogitsToSoftmax\'] = _CONVERTCLASSLOGITSTOSOFTMAX\n\nPreprocessingStep = _reflection.GeneratedProtocolMessageType(\'PreprocessingStep\', (_message.Message,), dict(\n  DESCRIPTOR = _PREPROCESSINGSTEP,\n  __module__ = \'rastervision.protos.tf_object_detection.preprocessor_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.PreprocessingStep)\n  ))\n_sym_db.RegisterMessage(PreprocessingStep)\n\nNormalizeImage = _reflection.GeneratedProtocolMessageType(\'NormalizeImage\', (_message.Message,), dict(\n  DESCRIPTOR = _NORMALIZEIMAGE,\n  __module__ = \'rastervision.protos.tf_object_detection.preprocessor_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.NormalizeImage)\n  ))\n_sym_db.RegisterMessage(NormalizeImage)\n\nRandomHorizontalFlip = _reflection.GeneratedProtocolMessageType(\'RandomHorizontalFlip\', (_message.Message,), dict(\n  DESCRIPTOR = _RANDOMHORIZONTALFLIP,\n  __module__ = \'rastervision.protos.tf_object_detection.preprocessor_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.RandomHorizontalFlip)\n  ))\n_sym_db.RegisterMessage(RandomHorizontalFlip)\n\nRandomVerticalFlip = _reflection.GeneratedProtocolMessageType(\'RandomVerticalFlip\', (_message.Message,), dict(\n  DESCRIPTOR = _RANDOMVERTICALFLIP,\n  __module__ = \'rastervision.protos.tf_object_detection.preprocessor_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.RandomVerticalFlip)\n  ))\n_sym_db.RegisterMessage(RandomVerticalFlip)\n\nRandomRotation90 = _reflection.GeneratedProtocolMessageType(\'RandomRotation90\', (_message.Message,), dict(\n  DESCRIPTOR = _RANDOMROTATION90,\n  __module__ = \'rastervision.protos.tf_object_detection.preprocessor_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.RandomRotation90)\n  ))\n_sym_db.RegisterMessage(RandomRotation90)\n\nRandomPixelValueScale = _reflection.GeneratedProtocolMessageType(\'RandomPixelValueScale\', (_message.Message,), dict(\n  DESCRIPTOR = _RANDOMPIXELVALUESCALE,\n  __module__ = \'rastervision.protos.tf_object_detection.preprocessor_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.RandomPixelValueScale)\n  ))\n_sym_db.RegisterMessage(RandomPixelValueScale)\n\nRandomImageScale = _reflection.GeneratedProtocolMessageType(\'RandomImageScale\', (_message.Message,), dict(\n  DESCRIPTOR = _RANDOMIMAGESCALE,\n  __module__ = \'rastervision.protos.tf_object_detection.preprocessor_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.RandomImageScale)\n  ))\n_sym_db.RegisterMessage(RandomImageScale)\n\nRandomRGBtoGray = _reflection.GeneratedProtocolMessageType(\'RandomRGBtoGray\', (_message.Message,), dict(\n  DESCRIPTOR = _RANDOMRGBTOGRAY,\n  __module__ = \'rastervision.protos.tf_object_detection.preprocessor_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.RandomRGBtoGray)\n  ))\n_sym_db.RegisterMessage(RandomRGBtoGray)\n\nRandomAdjustBrightness = _reflection.GeneratedProtocolMessageType(\'RandomAdjustBrightness\', (_message.Message,), dict(\n  DESCRIPTOR = _RANDOMADJUSTBRIGHTNESS,\n  __module__ = \'rastervision.protos.tf_object_detection.preprocessor_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.RandomAdjustBrightness)\n  ))\n_sym_db.RegisterMessage(RandomAdjustBrightness)\n\nRandomAdjustContrast = _reflection.GeneratedProtocolMessageType(\'RandomAdjustContrast\', (_message.Message,), dict(\n  DESCRIPTOR = _RANDOMADJUSTCONTRAST,\n  __module__ = \'rastervision.protos.tf_object_detection.preprocessor_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.RandomAdjustContrast)\n  ))\n_sym_db.RegisterMessage(RandomAdjustContrast)\n\nRandomAdjustHue = _reflection.GeneratedProtocolMessageType(\'RandomAdjustHue\', (_message.Message,), dict(\n  DESCRIPTOR = _RANDOMADJUSTHUE,\n  __module__ = \'rastervision.protos.tf_object_detection.preprocessor_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.RandomAdjustHue)\n  ))\n_sym_db.RegisterMessage(RandomAdjustHue)\n\nRandomAdjustSaturation = _reflection.GeneratedProtocolMessageType(\'RandomAdjustSaturation\', (_message.Message,), dict(\n  DESCRIPTOR = _RANDOMADJUSTSATURATION,\n  __module__ = \'rastervision.protos.tf_object_detection.preprocessor_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.RandomAdjustSaturation)\n  ))\n_sym_db.RegisterMessage(RandomAdjustSaturation)\n\nRandomDistortColor = _reflection.GeneratedProtocolMessageType(\'RandomDistortColor\', (_message.Message,), dict(\n  DESCRIPTOR = _RANDOMDISTORTCOLOR,\n  __module__ = \'rastervision.protos.tf_object_detection.preprocessor_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.RandomDistortColor)\n  ))\n_sym_db.RegisterMessage(RandomDistortColor)\n\nRandomJitterBoxes = _reflection.GeneratedProtocolMessageType(\'RandomJitterBoxes\', (_message.Message,), dict(\n  DESCRIPTOR = _RANDOMJITTERBOXES,\n  __module__ = \'rastervision.protos.tf_object_detection.preprocessor_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.RandomJitterBoxes)\n  ))\n_sym_db.RegisterMessage(RandomJitterBoxes)\n\nRandomCropImage = _reflection.GeneratedProtocolMessageType(\'RandomCropImage\', (_message.Message,), dict(\n  DESCRIPTOR = _RANDOMCROPIMAGE,\n  __module__ = \'rastervision.protos.tf_object_detection.preprocessor_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.RandomCropImage)\n  ))\n_sym_db.RegisterMessage(RandomCropImage)\n\nRandomPadImage = _reflection.GeneratedProtocolMessageType(\'RandomPadImage\', (_message.Message,), dict(\n  DESCRIPTOR = _RANDOMPADIMAGE,\n  __module__ = \'rastervision.protos.tf_object_detection.preprocessor_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.RandomPadImage)\n  ))\n_sym_db.RegisterMessage(RandomPadImage)\n\nRandomCropPadImage = _reflection.GeneratedProtocolMessageType(\'RandomCropPadImage\', (_message.Message,), dict(\n  DESCRIPTOR = _RANDOMCROPPADIMAGE,\n  __module__ = \'rastervision.protos.tf_object_detection.preprocessor_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.RandomCropPadImage)\n  ))\n_sym_db.RegisterMessage(RandomCropPadImage)\n\nRandomCropToAspectRatio = _reflection.GeneratedProtocolMessageType(\'RandomCropToAspectRatio\', (_message.Message,), dict(\n  DESCRIPTOR = _RANDOMCROPTOASPECTRATIO,\n  __module__ = \'rastervision.protos.tf_object_detection.preprocessor_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.RandomCropToAspectRatio)\n  ))\n_sym_db.RegisterMessage(RandomCropToAspectRatio)\n\nRandomBlackPatches = _reflection.GeneratedProtocolMessageType(\'RandomBlackPatches\', (_message.Message,), dict(\n  DESCRIPTOR = _RANDOMBLACKPATCHES,\n  __module__ = \'rastervision.protos.tf_object_detection.preprocessor_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.RandomBlackPatches)\n  ))\n_sym_db.RegisterMessage(RandomBlackPatches)\n\nRandomResizeMethod = _reflection.GeneratedProtocolMessageType(\'RandomResizeMethod\', (_message.Message,), dict(\n  DESCRIPTOR = _RANDOMRESIZEMETHOD,\n  __module__ = \'rastervision.protos.tf_object_detection.preprocessor_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.RandomResizeMethod)\n  ))\n_sym_db.RegisterMessage(RandomResizeMethod)\n\nRGBtoGray = _reflection.GeneratedProtocolMessageType(\'RGBtoGray\', (_message.Message,), dict(\n  DESCRIPTOR = _RGBTOGRAY,\n  __module__ = \'rastervision.protos.tf_object_detection.preprocessor_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.RGBtoGray)\n  ))\n_sym_db.RegisterMessage(RGBtoGray)\n\nScaleBoxesToPixelCoordinates = _reflection.GeneratedProtocolMessageType(\'ScaleBoxesToPixelCoordinates\', (_message.Message,), dict(\n  DESCRIPTOR = _SCALEBOXESTOPIXELCOORDINATES,\n  __module__ = \'rastervision.protos.tf_object_detection.preprocessor_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.ScaleBoxesToPixelCoordinates)\n  ))\n_sym_db.RegisterMessage(ScaleBoxesToPixelCoordinates)\n\nResizeImage = _reflection.GeneratedProtocolMessageType(\'ResizeImage\', (_message.Message,), dict(\n  DESCRIPTOR = _RESIZEIMAGE,\n  __module__ = \'rastervision.protos.tf_object_detection.preprocessor_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.ResizeImage)\n  ))\n_sym_db.RegisterMessage(ResizeImage)\n\nSubtractChannelMean = _reflection.GeneratedProtocolMessageType(\'SubtractChannelMean\', (_message.Message,), dict(\n  DESCRIPTOR = _SUBTRACTCHANNELMEAN,\n  __module__ = \'rastervision.protos.tf_object_detection.preprocessor_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.SubtractChannelMean)\n  ))\n_sym_db.RegisterMessage(SubtractChannelMean)\n\nSSDRandomCropOperation = _reflection.GeneratedProtocolMessageType(\'SSDRandomCropOperation\', (_message.Message,), dict(\n  DESCRIPTOR = _SSDRANDOMCROPOPERATION,\n  __module__ = \'rastervision.protos.tf_object_detection.preprocessor_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.SSDRandomCropOperation)\n  ))\n_sym_db.RegisterMessage(SSDRandomCropOperation)\n\nSSDRandomCrop = _reflection.GeneratedProtocolMessageType(\'SSDRandomCrop\', (_message.Message,), dict(\n  DESCRIPTOR = _SSDRANDOMCROP,\n  __module__ = \'rastervision.protos.tf_object_detection.preprocessor_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.SSDRandomCrop)\n  ))\n_sym_db.RegisterMessage(SSDRandomCrop)\n\nSSDRandomCropPadOperation = _reflection.GeneratedProtocolMessageType(\'SSDRandomCropPadOperation\', (_message.Message,), dict(\n  DESCRIPTOR = _SSDRANDOMCROPPADOPERATION,\n  __module__ = \'rastervision.protos.tf_object_detection.preprocessor_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.SSDRandomCropPadOperation)\n  ))\n_sym_db.RegisterMessage(SSDRandomCropPadOperation)\n\nSSDRandomCropPad = _reflection.GeneratedProtocolMessageType(\'SSDRandomCropPad\', (_message.Message,), dict(\n  DESCRIPTOR = _SSDRANDOMCROPPAD,\n  __module__ = \'rastervision.protos.tf_object_detection.preprocessor_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.SSDRandomCropPad)\n  ))\n_sym_db.RegisterMessage(SSDRandomCropPad)\n\nSSDRandomCropFixedAspectRatioOperation = _reflection.GeneratedProtocolMessageType(\'SSDRandomCropFixedAspectRatioOperation\', (_message.Message,), dict(\n  DESCRIPTOR = _SSDRANDOMCROPFIXEDASPECTRATIOOPERATION,\n  __module__ = \'rastervision.protos.tf_object_detection.preprocessor_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.SSDRandomCropFixedAspectRatioOperation)\n  ))\n_sym_db.RegisterMessage(SSDRandomCropFixedAspectRatioOperation)\n\nSSDRandomCropFixedAspectRatio = _reflection.GeneratedProtocolMessageType(\'SSDRandomCropFixedAspectRatio\', (_message.Message,), dict(\n  DESCRIPTOR = _SSDRANDOMCROPFIXEDASPECTRATIO,\n  __module__ = \'rastervision.protos.tf_object_detection.preprocessor_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.SSDRandomCropFixedAspectRatio)\n  ))\n_sym_db.RegisterMessage(SSDRandomCropFixedAspectRatio)\n\nSSDRandomCropPadFixedAspectRatioOperation = _reflection.GeneratedProtocolMessageType(\'SSDRandomCropPadFixedAspectRatioOperation\', (_message.Message,), dict(\n  DESCRIPTOR = _SSDRANDOMCROPPADFIXEDASPECTRATIOOPERATION,\n  __module__ = \'rastervision.protos.tf_object_detection.preprocessor_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.SSDRandomCropPadFixedAspectRatioOperation)\n  ))\n_sym_db.RegisterMessage(SSDRandomCropPadFixedAspectRatioOperation)\n\nSSDRandomCropPadFixedAspectRatio = _reflection.GeneratedProtocolMessageType(\'SSDRandomCropPadFixedAspectRatio\', (_message.Message,), dict(\n  DESCRIPTOR = _SSDRANDOMCROPPADFIXEDASPECTRATIO,\n  __module__ = \'rastervision.protos.tf_object_detection.preprocessor_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.SSDRandomCropPadFixedAspectRatio)\n  ))\n_sym_db.RegisterMessage(SSDRandomCropPadFixedAspectRatio)\n\nConvertClassLogitsToSoftmax = _reflection.GeneratedProtocolMessageType(\'ConvertClassLogitsToSoftmax\', (_message.Message,), dict(\n  DESCRIPTOR = _CONVERTCLASSLOGITSTOSOFTMAX,\n  __module__ = \'rastervision.protos.tf_object_detection.preprocessor_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.ConvertClassLogitsToSoftmax)\n  ))\n_sym_db.RegisterMessage(ConvertClassLogitsToSoftmax)\n\n\n# @@protoc_insertion_point(module_scope)\n'"
rastervision/protos/tf_object_detection/region_similarity_calculator_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: rastervision/protos/tf_object_detection/region_similarity_calculator.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'rastervision/protos/tf_object_detection/region_similarity_calculator.proto\',\n  package=\'rastervision.protos.tf_object_detection\',\n  syntax=\'proto2\',\n  serialized_pb=_b(\'\\nJrastervision/protos/tf_object_detection/region_similarity_calculator.proto\\x12\\\'rastervision.protos.tf_object_detection\\""\\x9e\\x03\\n\\x1aRegionSimilarityCalculator\\x12^\\n\\x16neg_sq_dist_similarity\\x18\\x01 \\x01(\\x0b\\x32<.rastervision.protos.tf_object_detection.NegSqDistSimilarityH\\x00\\x12P\\n\\x0eiou_similarity\\x18\\x02 \\x01(\\x0b\\x32\\x36.rastervision.protos.tf_object_detection.IouSimilarityH\\x00\\x12P\\n\\x0eioa_similarity\\x18\\x03 \\x01(\\x0b\\x32\\x36.rastervision.protos.tf_object_detection.IoaSimilarityH\\x00\\x12g\\n\\x1athresholded_iou_similarity\\x18\\x04 \\x01(\\x0b\\x32\\x41.rastervision.protos.tf_object_detection.ThresholdedIouSimilarityH\\x00\\x42\\x13\\n\\x11region_similarity\\""\\x15\\n\\x13NegSqDistSimilarity\\""\\x0f\\n\\rIouSimilarity\\""\\x0f\\n\\rIoaSimilarity\\""6\\n\\x18ThresholdedIouSimilarity\\x12\\x1a\\n\\riou_threshold\\x18\\x01 \\x01(\\x02:\\x03\\x30.5\')\n)\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\n\n\n\n_REGIONSIMILARITYCALCULATOR = _descriptor.Descriptor(\n  name=\'RegionSimilarityCalculator\',\n  full_name=\'rastervision.protos.tf_object_detection.RegionSimilarityCalculator\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'neg_sq_dist_similarity\', full_name=\'rastervision.protos.tf_object_detection.RegionSimilarityCalculator.neg_sq_dist_similarity\', index=0,\n      number=1, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'iou_similarity\', full_name=\'rastervision.protos.tf_object_detection.RegionSimilarityCalculator.iou_similarity\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'ioa_similarity\', full_name=\'rastervision.protos.tf_object_detection.RegionSimilarityCalculator.ioa_similarity\', index=2,\n      number=3, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'thresholded_iou_similarity\', full_name=\'rastervision.protos.tf_object_detection.RegionSimilarityCalculator.thresholded_iou_similarity\', index=3,\n      number=4, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n    _descriptor.OneofDescriptor(\n      name=\'region_similarity\', full_name=\'rastervision.protos.tf_object_detection.RegionSimilarityCalculator.region_similarity\',\n      index=0, containing_type=None, fields=[]),\n  ],\n  serialized_start=120,\n  serialized_end=534,\n)\n\n\n_NEGSQDISTSIMILARITY = _descriptor.Descriptor(\n  name=\'NegSqDistSimilarity\',\n  full_name=\'rastervision.protos.tf_object_detection.NegSqDistSimilarity\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=536,\n  serialized_end=557,\n)\n\n\n_IOUSIMILARITY = _descriptor.Descriptor(\n  name=\'IouSimilarity\',\n  full_name=\'rastervision.protos.tf_object_detection.IouSimilarity\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=559,\n  serialized_end=574,\n)\n\n\n_IOASIMILARITY = _descriptor.Descriptor(\n  name=\'IoaSimilarity\',\n  full_name=\'rastervision.protos.tf_object_detection.IoaSimilarity\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=576,\n  serialized_end=591,\n)\n\n\n_THRESHOLDEDIOUSIMILARITY = _descriptor.Descriptor(\n  name=\'ThresholdedIouSimilarity\',\n  full_name=\'rastervision.protos.tf_object_detection.ThresholdedIouSimilarity\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'iou_threshold\', full_name=\'rastervision.protos.tf_object_detection.ThresholdedIouSimilarity.iou_threshold\', index=0,\n      number=1, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.5),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=593,\n  serialized_end=647,\n)\n\n_REGIONSIMILARITYCALCULATOR.fields_by_name[\'neg_sq_dist_similarity\'].message_type = _NEGSQDISTSIMILARITY\n_REGIONSIMILARITYCALCULATOR.fields_by_name[\'iou_similarity\'].message_type = _IOUSIMILARITY\n_REGIONSIMILARITYCALCULATOR.fields_by_name[\'ioa_similarity\'].message_type = _IOASIMILARITY\n_REGIONSIMILARITYCALCULATOR.fields_by_name[\'thresholded_iou_similarity\'].message_type = _THRESHOLDEDIOUSIMILARITY\n_REGIONSIMILARITYCALCULATOR.oneofs_by_name[\'region_similarity\'].fields.append(\n  _REGIONSIMILARITYCALCULATOR.fields_by_name[\'neg_sq_dist_similarity\'])\n_REGIONSIMILARITYCALCULATOR.fields_by_name[\'neg_sq_dist_similarity\'].containing_oneof = _REGIONSIMILARITYCALCULATOR.oneofs_by_name[\'region_similarity\']\n_REGIONSIMILARITYCALCULATOR.oneofs_by_name[\'region_similarity\'].fields.append(\n  _REGIONSIMILARITYCALCULATOR.fields_by_name[\'iou_similarity\'])\n_REGIONSIMILARITYCALCULATOR.fields_by_name[\'iou_similarity\'].containing_oneof = _REGIONSIMILARITYCALCULATOR.oneofs_by_name[\'region_similarity\']\n_REGIONSIMILARITYCALCULATOR.oneofs_by_name[\'region_similarity\'].fields.append(\n  _REGIONSIMILARITYCALCULATOR.fields_by_name[\'ioa_similarity\'])\n_REGIONSIMILARITYCALCULATOR.fields_by_name[\'ioa_similarity\'].containing_oneof = _REGIONSIMILARITYCALCULATOR.oneofs_by_name[\'region_similarity\']\n_REGIONSIMILARITYCALCULATOR.oneofs_by_name[\'region_similarity\'].fields.append(\n  _REGIONSIMILARITYCALCULATOR.fields_by_name[\'thresholded_iou_similarity\'])\n_REGIONSIMILARITYCALCULATOR.fields_by_name[\'thresholded_iou_similarity\'].containing_oneof = _REGIONSIMILARITYCALCULATOR.oneofs_by_name[\'region_similarity\']\nDESCRIPTOR.message_types_by_name[\'RegionSimilarityCalculator\'] = _REGIONSIMILARITYCALCULATOR\nDESCRIPTOR.message_types_by_name[\'NegSqDistSimilarity\'] = _NEGSQDISTSIMILARITY\nDESCRIPTOR.message_types_by_name[\'IouSimilarity\'] = _IOUSIMILARITY\nDESCRIPTOR.message_types_by_name[\'IoaSimilarity\'] = _IOASIMILARITY\nDESCRIPTOR.message_types_by_name[\'ThresholdedIouSimilarity\'] = _THRESHOLDEDIOUSIMILARITY\n\nRegionSimilarityCalculator = _reflection.GeneratedProtocolMessageType(\'RegionSimilarityCalculator\', (_message.Message,), dict(\n  DESCRIPTOR = _REGIONSIMILARITYCALCULATOR,\n  __module__ = \'rastervision.protos.tf_object_detection.region_similarity_calculator_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.RegionSimilarityCalculator)\n  ))\n_sym_db.RegisterMessage(RegionSimilarityCalculator)\n\nNegSqDistSimilarity = _reflection.GeneratedProtocolMessageType(\'NegSqDistSimilarity\', (_message.Message,), dict(\n  DESCRIPTOR = _NEGSQDISTSIMILARITY,\n  __module__ = \'rastervision.protos.tf_object_detection.region_similarity_calculator_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.NegSqDistSimilarity)\n  ))\n_sym_db.RegisterMessage(NegSqDistSimilarity)\n\nIouSimilarity = _reflection.GeneratedProtocolMessageType(\'IouSimilarity\', (_message.Message,), dict(\n  DESCRIPTOR = _IOUSIMILARITY,\n  __module__ = \'rastervision.protos.tf_object_detection.region_similarity_calculator_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.IouSimilarity)\n  ))\n_sym_db.RegisterMessage(IouSimilarity)\n\nIoaSimilarity = _reflection.GeneratedProtocolMessageType(\'IoaSimilarity\', (_message.Message,), dict(\n  DESCRIPTOR = _IOASIMILARITY,\n  __module__ = \'rastervision.protos.tf_object_detection.region_similarity_calculator_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.IoaSimilarity)\n  ))\n_sym_db.RegisterMessage(IoaSimilarity)\n\nThresholdedIouSimilarity = _reflection.GeneratedProtocolMessageType(\'ThresholdedIouSimilarity\', (_message.Message,), dict(\n  DESCRIPTOR = _THRESHOLDEDIOUSIMILARITY,\n  __module__ = \'rastervision.protos.tf_object_detection.region_similarity_calculator_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.ThresholdedIouSimilarity)\n  ))\n_sym_db.RegisterMessage(ThresholdedIouSimilarity)\n\n\n# @@protoc_insertion_point(module_scope)\n'"
rastervision/protos/tf_object_detection/square_box_coder_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: rastervision/protos/tf_object_detection/square_box_coder.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'rastervision/protos/tf_object_detection/square_box_coder.proto\',\n  package=\'rastervision.protos.tf_object_detection\',\n  syntax=\'proto2\',\n  serialized_pb=_b(\'\\n>rastervision/protos/tf_object_detection/square_box_coder.proto\\x12\\\'rastervision.protos.tf_object_detection\\""S\\n\\x0eSquareBoxCoder\\x12\\x13\\n\\x07y_scale\\x18\\x01 \\x01(\\x02:\\x02\\x31\\x30\\x12\\x13\\n\\x07x_scale\\x18\\x02 \\x01(\\x02:\\x02\\x31\\x30\\x12\\x17\\n\\x0clength_scale\\x18\\x03 \\x01(\\x02:\\x01\\x35\')\n)\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\n\n\n\n_SQUAREBOXCODER = _descriptor.Descriptor(\n  name=\'SquareBoxCoder\',\n  full_name=\'rastervision.protos.tf_object_detection.SquareBoxCoder\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'y_scale\', full_name=\'rastervision.protos.tf_object_detection.SquareBoxCoder.y_scale\', index=0,\n      number=1, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(10),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'x_scale\', full_name=\'rastervision.protos.tf_object_detection.SquareBoxCoder.x_scale\', index=1,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(10),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'length_scale\', full_name=\'rastervision.protos.tf_object_detection.SquareBoxCoder.length_scale\', index=2,\n      number=3, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(5),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=107,\n  serialized_end=190,\n)\n\nDESCRIPTOR.message_types_by_name[\'SquareBoxCoder\'] = _SQUAREBOXCODER\n\nSquareBoxCoder = _reflection.GeneratedProtocolMessageType(\'SquareBoxCoder\', (_message.Message,), dict(\n  DESCRIPTOR = _SQUAREBOXCODER,\n  __module__ = \'rastervision.protos.tf_object_detection.square_box_coder_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.SquareBoxCoder)\n  ))\n_sym_db.RegisterMessage(SquareBoxCoder)\n\n\n# @@protoc_insertion_point(module_scope)\n'"
rastervision/protos/tf_object_detection/ssd_anchor_generator_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: rastervision/protos/tf_object_detection/ssd_anchor_generator.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'rastervision/protos/tf_object_detection/ssd_anchor_generator.proto\',\n  package=\'rastervision.protos.tf_object_detection\',\n  syntax=\'proto2\',\n  serialized_pb=_b(\'\\nBrastervision/protos/tf_object_detection/ssd_anchor_generator.proto\\x12\\\'rastervision.protos.tf_object_detection\\""\\xf2\\x02\\n\\x12SsdAnchorGenerator\\x12\\x15\\n\\nnum_layers\\x18\\x01 \\x01(\\x05:\\x01\\x36\\x12\\x16\\n\\tmin_scale\\x18\\x02 \\x01(\\x02:\\x03\\x30.2\\x12\\x17\\n\\tmax_scale\\x18\\x03 \\x01(\\x02:\\x04\\x30.95\\x12\\x0e\\n\\x06scales\\x18\\x0c \\x03(\\x02\\x12\\x15\\n\\raspect_ratios\\x18\\x04 \\x03(\\x02\\x12*\\n\\x1finterpolated_scale_aspect_ratio\\x18\\r \\x01(\\x02:\\x01\\x31\\x12*\\n\\x1creduce_boxes_in_lowest_layer\\x18\\x05 \\x01(\\x08:\\x04true\\x12\\x1d\\n\\x12\\x62\\x61se_anchor_height\\x18\\x06 \\x01(\\x02:\\x01\\x31\\x12\\x1c\\n\\x11\\x62\\x61se_anchor_width\\x18\\x07 \\x01(\\x02:\\x01\\x31\\x12\\x15\\n\\rheight_stride\\x18\\x08 \\x03(\\x05\\x12\\x14\\n\\x0cwidth_stride\\x18\\t \\x03(\\x05\\x12\\x15\\n\\rheight_offset\\x18\\n \\x03(\\x05\\x12\\x14\\n\\x0cwidth_offset\\x18\\x0b \\x03(\\x05\')\n)\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\n\n\n\n_SSDANCHORGENERATOR = _descriptor.Descriptor(\n  name=\'SsdAnchorGenerator\',\n  full_name=\'rastervision.protos.tf_object_detection.SsdAnchorGenerator\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'num_layers\', full_name=\'rastervision.protos.tf_object_detection.SsdAnchorGenerator.num_layers\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=6,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'min_scale\', full_name=\'rastervision.protos.tf_object_detection.SsdAnchorGenerator.min_scale\', index=1,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.2),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'max_scale\', full_name=\'rastervision.protos.tf_object_detection.SsdAnchorGenerator.max_scale\', index=2,\n      number=3, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0.95),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'scales\', full_name=\'rastervision.protos.tf_object_detection.SsdAnchorGenerator.scales\', index=3,\n      number=12, type=2, cpp_type=6, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'aspect_ratios\', full_name=\'rastervision.protos.tf_object_detection.SsdAnchorGenerator.aspect_ratios\', index=4,\n      number=4, type=2, cpp_type=6, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'interpolated_scale_aspect_ratio\', full_name=\'rastervision.protos.tf_object_detection.SsdAnchorGenerator.interpolated_scale_aspect_ratio\', index=5,\n      number=13, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(1),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'reduce_boxes_in_lowest_layer\', full_name=\'rastervision.protos.tf_object_detection.SsdAnchorGenerator.reduce_boxes_in_lowest_layer\', index=6,\n      number=5, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=True,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'base_anchor_height\', full_name=\'rastervision.protos.tf_object_detection.SsdAnchorGenerator.base_anchor_height\', index=7,\n      number=6, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(1),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'base_anchor_width\', full_name=\'rastervision.protos.tf_object_detection.SsdAnchorGenerator.base_anchor_width\', index=8,\n      number=7, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(1),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'height_stride\', full_name=\'rastervision.protos.tf_object_detection.SsdAnchorGenerator.height_stride\', index=9,\n      number=8, type=5, cpp_type=1, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'width_stride\', full_name=\'rastervision.protos.tf_object_detection.SsdAnchorGenerator.width_stride\', index=10,\n      number=9, type=5, cpp_type=1, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'height_offset\', full_name=\'rastervision.protos.tf_object_detection.SsdAnchorGenerator.height_offset\', index=11,\n      number=10, type=5, cpp_type=1, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'width_offset\', full_name=\'rastervision.protos.tf_object_detection.SsdAnchorGenerator.width_offset\', index=12,\n      number=11, type=5, cpp_type=1, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=112,\n  serialized_end=482,\n)\n\nDESCRIPTOR.message_types_by_name[\'SsdAnchorGenerator\'] = _SSDANCHORGENERATOR\n\nSsdAnchorGenerator = _reflection.GeneratedProtocolMessageType(\'SsdAnchorGenerator\', (_message.Message,), dict(\n  DESCRIPTOR = _SSDANCHORGENERATOR,\n  __module__ = \'rastervision.protos.tf_object_detection.ssd_anchor_generator_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.SsdAnchorGenerator)\n  ))\n_sym_db.RegisterMessage(SsdAnchorGenerator)\n\n\n# @@protoc_insertion_point(module_scope)\n'"
rastervision/protos/tf_object_detection/ssd_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: rastervision/protos/tf_object_detection/ssd.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom rastervision.protos.tf_object_detection import anchor_generator_pb2 as rastervision_dot_protos_dot_tf__object__detection_dot_anchor__generator__pb2\nfrom rastervision.protos.tf_object_detection import box_coder_pb2 as rastervision_dot_protos_dot_tf__object__detection_dot_box__coder__pb2\nfrom rastervision.protos.tf_object_detection import box_predictor_pb2 as rastervision_dot_protos_dot_tf__object__detection_dot_box__predictor__pb2\nfrom rastervision.protos.tf_object_detection import hyperparams_pb2 as rastervision_dot_protos_dot_tf__object__detection_dot_hyperparams__pb2\nfrom rastervision.protos.tf_object_detection import image_resizer_pb2 as rastervision_dot_protos_dot_tf__object__detection_dot_image__resizer__pb2\nfrom rastervision.protos.tf_object_detection import matcher_pb2 as rastervision_dot_protos_dot_tf__object__detection_dot_matcher__pb2\nfrom rastervision.protos.tf_object_detection import losses_pb2 as rastervision_dot_protos_dot_tf__object__detection_dot_losses__pb2\nfrom rastervision.protos.tf_object_detection import post_processing_pb2 as rastervision_dot_protos_dot_tf__object__detection_dot_post__processing__pb2\nfrom rastervision.protos.tf_object_detection import region_similarity_calculator_pb2 as rastervision_dot_protos_dot_tf__object__detection_dot_region__similarity__calculator__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'rastervision/protos/tf_object_detection/ssd.proto\',\n  package=\'rastervision.protos.tf_object_detection\',\n  syntax=\'proto2\',\n  serialized_pb=_b(\'\\n1rastervision/protos/tf_object_detection/ssd.proto\\x12\\\'rastervision.protos.tf_object_detection\\x1a>rastervision/protos/tf_object_detection/anchor_generator.proto\\x1a\\x37rastervision/protos/tf_object_detection/box_coder.proto\\x1a;rastervision/protos/tf_object_detection/box_predictor.proto\\x1a\\x39rastervision/protos/tf_object_detection/hyperparams.proto\\x1a;rastervision/protos/tf_object_detection/image_resizer.proto\\x1a\\x35rastervision/protos/tf_object_detection/matcher.proto\\x1a\\x34rastervision/protos/tf_object_detection/losses.proto\\x1a=rastervision/protos/tf_object_detection/post_processing.proto\\x1aJrastervision/protos/tf_object_detection/region_similarity_calculator.proto\\""\\x94\\t\\n\\x03Ssd\\x12\\x13\\n\\x0bnum_classes\\x18\\x01 \\x01(\\x05\\x12L\\n\\rimage_resizer\\x18\\x02 \\x01(\\x0b\\x32\\x35.rastervision.protos.tf_object_detection.ImageResizer\\x12W\\n\\x11\\x66\\x65\\x61ture_extractor\\x18\\x03 \\x01(\\x0b\\x32<.rastervision.protos.tf_object_detection.SsdFeatureExtractor\\x12\\x44\\n\\tbox_coder\\x18\\x04 \\x01(\\x0b\\x32\\x31.rastervision.protos.tf_object_detection.BoxCoder\\x12\\x41\\n\\x07matcher\\x18\\x05 \\x01(\\x0b\\x32\\x30.rastervision.protos.tf_object_detection.Matcher\\x12\\x62\\n\\x15similarity_calculator\\x18\\x06 \\x01(\\x0b\\x32\\x43.rastervision.protos.tf_object_detection.RegionSimilarityCalculator\\x12)\\n\\x1a\\x65ncode_background_as_zeros\\x18\\x0c \\x01(\\x08:\\x05\\x66\\x61lse\\x12 \\n\\x15negative_class_weight\\x18\\r \\x01(\\x02:\\x01\\x31\\x12L\\n\\rbox_predictor\\x18\\x07 \\x01(\\x0b\\x32\\x35.rastervision.protos.tf_object_detection.BoxPredictor\\x12R\\n\\x10\\x61nchor_generator\\x18\\x08 \\x01(\\x0b\\x32\\x38.rastervision.protos.tf_object_detection.AnchorGenerator\\x12P\\n\\x0fpost_processing\\x18\\t \\x01(\\x0b\\x32\\x37.rastervision.protos.tf_object_detection.PostProcessing\\x12+\\n\\x1dnormalize_loss_by_num_matches\\x18\\n \\x01(\\x08:\\x04true\\x12-\\n\\x1enormalize_loc_loss_by_codesize\\x18\\x0e \\x01(\\x08:\\x05\\x66\\x61lse\\x12;\\n\\x04loss\\x18\\x0b \\x01(\\x0b\\x32-.rastervision.protos.tf_object_detection.Loss\\x12\\x1f\\n\\x10\\x66reeze_batchnorm\\x18\\x10 \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\\'\\n\\x18inplace_batchnorm_update\\x18\\x0f \\x01(\\x08:\\x05\\x66\\x61lse\\x12.\\n\\x1fweight_regression_loss_by_score\\x18\\x11 \\x01(\\x08:\\x05\\x66\\x61lse\\x12>\\n/use_expected_classification_loss_under_sampling\\x18\\x12 \\x01(\\x08:\\x05\\x66\\x61lse\\x12$\\n\\x19minimum_negative_sampling\\x18\\x13 \\x01(\\x02:\\x01\\x30\\x12*\\n\\x1f\\x64\\x65sired_negative_sampling_ratio\\x18\\x14 \\x01(\\x02:\\x01\\x33\\""\\x96\\x03\\n\\x13SsdFeatureExtractor\\x12\\x0c\\n\\x04type\\x18\\x01 \\x01(\\t\\x12\\x1b\\n\\x10\\x64\\x65pth_multiplier\\x18\\x02 \\x01(\\x02:\\x01\\x31\\x12\\x15\\n\\tmin_depth\\x18\\x03 \\x01(\\x05:\\x02\\x31\\x36\\x12N\\n\\x10\\x63onv_hyperparams\\x18\\x04 \\x01(\\x0b\\x32\\x34.rastervision.protos.tf_object_detection.Hyperparams\\x12:\\n+override_base_feature_extractor_hyperparams\\x18\\t \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\x1a\\n\\x0fpad_to_multiple\\x18\\x05 \\x01(\\x05:\\x01\\x31\\x12#\\n\\x14use_explicit_padding\\x18\\x07 \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\x1c\\n\\ruse_depthwise\\x18\\x08 \\x01(\\x08:\\x05\\x66\\x61lse\\x12L\\n\\x03\\x66pn\\x18\\n \\x01(\\x0b\\x32?.rastervision.protos.tf_object_detection.FeaturePyramidNetworksJ\\x04\\x08\\x06\\x10\\x07\\""i\\n\\x16\\x46\\x65\\x61turePyramidNetworks\\x12\\x14\\n\\tmin_level\\x18\\x01 \\x01(\\x05:\\x01\\x33\\x12\\x14\\n\\tmax_level\\x18\\x02 \\x01(\\x05:\\x01\\x37\\x12#\\n\\x16\\x61\\x64\\x64itional_layer_depth\\x18\\x03 \\x01(\\x05:\\x03\\x32\\x35\\x36\')\n  ,\n  dependencies=[rastervision_dot_protos_dot_tf__object__detection_dot_anchor__generator__pb2.DESCRIPTOR,rastervision_dot_protos_dot_tf__object__detection_dot_box__coder__pb2.DESCRIPTOR,rastervision_dot_protos_dot_tf__object__detection_dot_box__predictor__pb2.DESCRIPTOR,rastervision_dot_protos_dot_tf__object__detection_dot_hyperparams__pb2.DESCRIPTOR,rastervision_dot_protos_dot_tf__object__detection_dot_image__resizer__pb2.DESCRIPTOR,rastervision_dot_protos_dot_tf__object__detection_dot_matcher__pb2.DESCRIPTOR,rastervision_dot_protos_dot_tf__object__detection_dot_losses__pb2.DESCRIPTOR,rastervision_dot_protos_dot_tf__object__detection_dot_post__processing__pb2.DESCRIPTOR,rastervision_dot_protos_dot_tf__object__detection_dot_region__similarity__calculator__pb2.DESCRIPTOR,])\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\n\n\n\n_SSD = _descriptor.Descriptor(\n  name=\'Ssd\',\n  full_name=\'rastervision.protos.tf_object_detection.Ssd\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'num_classes\', full_name=\'rastervision.protos.tf_object_detection.Ssd.num_classes\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'image_resizer\', full_name=\'rastervision.protos.tf_object_detection.Ssd.image_resizer\', index=1,\n      number=2, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'feature_extractor\', full_name=\'rastervision.protos.tf_object_detection.Ssd.feature_extractor\', index=2,\n      number=3, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'box_coder\', full_name=\'rastervision.protos.tf_object_detection.Ssd.box_coder\', index=3,\n      number=4, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'matcher\', full_name=\'rastervision.protos.tf_object_detection.Ssd.matcher\', index=4,\n      number=5, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'similarity_calculator\', full_name=\'rastervision.protos.tf_object_detection.Ssd.similarity_calculator\', index=5,\n      number=6, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'encode_background_as_zeros\', full_name=\'rastervision.protos.tf_object_detection.Ssd.encode_background_as_zeros\', index=6,\n      number=12, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'negative_class_weight\', full_name=\'rastervision.protos.tf_object_detection.Ssd.negative_class_weight\', index=7,\n      number=13, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(1),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'box_predictor\', full_name=\'rastervision.protos.tf_object_detection.Ssd.box_predictor\', index=8,\n      number=7, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'anchor_generator\', full_name=\'rastervision.protos.tf_object_detection.Ssd.anchor_generator\', index=9,\n      number=8, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'post_processing\', full_name=\'rastervision.protos.tf_object_detection.Ssd.post_processing\', index=10,\n      number=9, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'normalize_loss_by_num_matches\', full_name=\'rastervision.protos.tf_object_detection.Ssd.normalize_loss_by_num_matches\', index=11,\n      number=10, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=True,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'normalize_loc_loss_by_codesize\', full_name=\'rastervision.protos.tf_object_detection.Ssd.normalize_loc_loss_by_codesize\', index=12,\n      number=14, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'loss\', full_name=\'rastervision.protos.tf_object_detection.Ssd.loss\', index=13,\n      number=11, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'freeze_batchnorm\', full_name=\'rastervision.protos.tf_object_detection.Ssd.freeze_batchnorm\', index=14,\n      number=16, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'inplace_batchnorm_update\', full_name=\'rastervision.protos.tf_object_detection.Ssd.inplace_batchnorm_update\', index=15,\n      number=15, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'weight_regression_loss_by_score\', full_name=\'rastervision.protos.tf_object_detection.Ssd.weight_regression_loss_by_score\', index=16,\n      number=17, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'use_expected_classification_loss_under_sampling\', full_name=\'rastervision.protos.tf_object_detection.Ssd.use_expected_classification_loss_under_sampling\', index=17,\n      number=18, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'minimum_negative_sampling\', full_name=\'rastervision.protos.tf_object_detection.Ssd.minimum_negative_sampling\', index=18,\n      number=19, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'desired_negative_sampling_ratio\', full_name=\'rastervision.protos.tf_object_detection.Ssd.desired_negative_sampling_ratio\', index=19,\n      number=20, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(3),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=645,\n  serialized_end=1817,\n)\n\n\n_SSDFEATUREEXTRACTOR = _descriptor.Descriptor(\n  name=\'SsdFeatureExtractor\',\n  full_name=\'rastervision.protos.tf_object_detection.SsdFeatureExtractor\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'type\', full_name=\'rastervision.protos.tf_object_detection.SsdFeatureExtractor.type\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'depth_multiplier\', full_name=\'rastervision.protos.tf_object_detection.SsdFeatureExtractor.depth_multiplier\', index=1,\n      number=2, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(1),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'min_depth\', full_name=\'rastervision.protos.tf_object_detection.SsdFeatureExtractor.min_depth\', index=2,\n      number=3, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=16,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'conv_hyperparams\', full_name=\'rastervision.protos.tf_object_detection.SsdFeatureExtractor.conv_hyperparams\', index=3,\n      number=4, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'override_base_feature_extractor_hyperparams\', full_name=\'rastervision.protos.tf_object_detection.SsdFeatureExtractor.override_base_feature_extractor_hyperparams\', index=4,\n      number=9, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'pad_to_multiple\', full_name=\'rastervision.protos.tf_object_detection.SsdFeatureExtractor.pad_to_multiple\', index=5,\n      number=5, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'use_explicit_padding\', full_name=\'rastervision.protos.tf_object_detection.SsdFeatureExtractor.use_explicit_padding\', index=6,\n      number=7, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'use_depthwise\', full_name=\'rastervision.protos.tf_object_detection.SsdFeatureExtractor.use_depthwise\', index=7,\n      number=8, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'fpn\', full_name=\'rastervision.protos.tf_object_detection.SsdFeatureExtractor.fpn\', index=8,\n      number=10, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=1820,\n  serialized_end=2226,\n)\n\n\n_FEATUREPYRAMIDNETWORKS = _descriptor.Descriptor(\n  name=\'FeaturePyramidNetworks\',\n  full_name=\'rastervision.protos.tf_object_detection.FeaturePyramidNetworks\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'min_level\', full_name=\'rastervision.protos.tf_object_detection.FeaturePyramidNetworks.min_level\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=3,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'max_level\', full_name=\'rastervision.protos.tf_object_detection.FeaturePyramidNetworks.max_level\', index=1,\n      number=2, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=7,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'additional_layer_depth\', full_name=\'rastervision.protos.tf_object_detection.FeaturePyramidNetworks.additional_layer_depth\', index=2,\n      number=3, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=256,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=2228,\n  serialized_end=2333,\n)\n\n_SSD.fields_by_name[\'image_resizer\'].message_type = rastervision_dot_protos_dot_tf__object__detection_dot_image__resizer__pb2._IMAGERESIZER\n_SSD.fields_by_name[\'feature_extractor\'].message_type = _SSDFEATUREEXTRACTOR\n_SSD.fields_by_name[\'box_coder\'].message_type = rastervision_dot_protos_dot_tf__object__detection_dot_box__coder__pb2._BOXCODER\n_SSD.fields_by_name[\'matcher\'].message_type = rastervision_dot_protos_dot_tf__object__detection_dot_matcher__pb2._MATCHER\n_SSD.fields_by_name[\'similarity_calculator\'].message_type = rastervision_dot_protos_dot_tf__object__detection_dot_region__similarity__calculator__pb2._REGIONSIMILARITYCALCULATOR\n_SSD.fields_by_name[\'box_predictor\'].message_type = rastervision_dot_protos_dot_tf__object__detection_dot_box__predictor__pb2._BOXPREDICTOR\n_SSD.fields_by_name[\'anchor_generator\'].message_type = rastervision_dot_protos_dot_tf__object__detection_dot_anchor__generator__pb2._ANCHORGENERATOR\n_SSD.fields_by_name[\'post_processing\'].message_type = rastervision_dot_protos_dot_tf__object__detection_dot_post__processing__pb2._POSTPROCESSING\n_SSD.fields_by_name[\'loss\'].message_type = rastervision_dot_protos_dot_tf__object__detection_dot_losses__pb2._LOSS\n_SSDFEATUREEXTRACTOR.fields_by_name[\'conv_hyperparams\'].message_type = rastervision_dot_protos_dot_tf__object__detection_dot_hyperparams__pb2._HYPERPARAMS\n_SSDFEATUREEXTRACTOR.fields_by_name[\'fpn\'].message_type = _FEATUREPYRAMIDNETWORKS\nDESCRIPTOR.message_types_by_name[\'Ssd\'] = _SSD\nDESCRIPTOR.message_types_by_name[\'SsdFeatureExtractor\'] = _SSDFEATUREEXTRACTOR\nDESCRIPTOR.message_types_by_name[\'FeaturePyramidNetworks\'] = _FEATUREPYRAMIDNETWORKS\n\nSsd = _reflection.GeneratedProtocolMessageType(\'Ssd\', (_message.Message,), dict(\n  DESCRIPTOR = _SSD,\n  __module__ = \'rastervision.protos.tf_object_detection.ssd_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.Ssd)\n  ))\n_sym_db.RegisterMessage(Ssd)\n\nSsdFeatureExtractor = _reflection.GeneratedProtocolMessageType(\'SsdFeatureExtractor\', (_message.Message,), dict(\n  DESCRIPTOR = _SSDFEATUREEXTRACTOR,\n  __module__ = \'rastervision.protos.tf_object_detection.ssd_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.SsdFeatureExtractor)\n  ))\n_sym_db.RegisterMessage(SsdFeatureExtractor)\n\nFeaturePyramidNetworks = _reflection.GeneratedProtocolMessageType(\'FeaturePyramidNetworks\', (_message.Message,), dict(\n  DESCRIPTOR = _FEATUREPYRAMIDNETWORKS,\n  __module__ = \'rastervision.protos.tf_object_detection.ssd_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.FeaturePyramidNetworks)\n  ))\n_sym_db.RegisterMessage(FeaturePyramidNetworks)\n\n\n# @@protoc_insertion_point(module_scope)\n'"
rastervision/protos/tf_object_detection/string_int_label_map_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: rastervision/protos/tf_object_detection/string_int_label_map.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'rastervision/protos/tf_object_detection/string_int_label_map.proto\',\n  package=\'rastervision.protos.tf_object_detection\',\n  syntax=\'proto2\',\n  serialized_pb=_b(\'\\nBrastervision/protos/tf_object_detection/string_int_label_map.proto\\x12\\\'rastervision.protos.tf_object_detection\\""G\\n\\x15StringIntLabelMapItem\\x12\\x0c\\n\\x04name\\x18\\x01 \\x01(\\t\\x12\\n\\n\\x02id\\x18\\x02 \\x01(\\x05\\x12\\x14\\n\\x0c\\x64isplay_name\\x18\\x03 \\x01(\\t\\""a\\n\\x11StringIntLabelMap\\x12L\\n\\x04item\\x18\\x01 \\x03(\\x0b\\x32>.rastervision.protos.tf_object_detection.StringIntLabelMapItem\')\n)\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\n\n\n\n_STRINGINTLABELMAPITEM = _descriptor.Descriptor(\n  name=\'StringIntLabelMapItem\',\n  full_name=\'rastervision.protos.tf_object_detection.StringIntLabelMapItem\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'name\', full_name=\'rastervision.protos.tf_object_detection.StringIntLabelMapItem.name\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'id\', full_name=\'rastervision.protos.tf_object_detection.StringIntLabelMapItem.id\', index=1,\n      number=2, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'display_name\', full_name=\'rastervision.protos.tf_object_detection.StringIntLabelMapItem.display_name\', index=2,\n      number=3, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=111,\n  serialized_end=182,\n)\n\n\n_STRINGINTLABELMAP = _descriptor.Descriptor(\n  name=\'StringIntLabelMap\',\n  full_name=\'rastervision.protos.tf_object_detection.StringIntLabelMap\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'item\', full_name=\'rastervision.protos.tf_object_detection.StringIntLabelMap.item\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=184,\n  serialized_end=281,\n)\n\n_STRINGINTLABELMAP.fields_by_name[\'item\'].message_type = _STRINGINTLABELMAPITEM\nDESCRIPTOR.message_types_by_name[\'StringIntLabelMapItem\'] = _STRINGINTLABELMAPITEM\nDESCRIPTOR.message_types_by_name[\'StringIntLabelMap\'] = _STRINGINTLABELMAP\n\nStringIntLabelMapItem = _reflection.GeneratedProtocolMessageType(\'StringIntLabelMapItem\', (_message.Message,), dict(\n  DESCRIPTOR = _STRINGINTLABELMAPITEM,\n  __module__ = \'rastervision.protos.tf_object_detection.string_int_label_map_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.StringIntLabelMapItem)\n  ))\n_sym_db.RegisterMessage(StringIntLabelMapItem)\n\nStringIntLabelMap = _reflection.GeneratedProtocolMessageType(\'StringIntLabelMap\', (_message.Message,), dict(\n  DESCRIPTOR = _STRINGINTLABELMAP,\n  __module__ = \'rastervision.protos.tf_object_detection.string_int_label_map_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.StringIntLabelMap)\n  ))\n_sym_db.RegisterMessage(StringIntLabelMap)\n\n\n# @@protoc_insertion_point(module_scope)\n'"
rastervision/protos/tf_object_detection/train_pb2.py,0,"b'# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: rastervision/protos/tf_object_detection/train.proto\n\nimport sys\n_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode(\'latin1\'))\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\nfrom google.protobuf import descriptor_pb2\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\nfrom rastervision.protos.tf_object_detection import optimizer_pb2 as rastervision_dot_protos_dot_tf__object__detection_dot_optimizer__pb2\nfrom rastervision.protos.tf_object_detection import preprocessor_pb2 as rastervision_dot_protos_dot_tf__object__detection_dot_preprocessor__pb2\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'rastervision/protos/tf_object_detection/train.proto\',\n  package=\'rastervision.protos.tf_object_detection\',\n  syntax=\'proto2\',\n  serialized_pb=_b(\'\\n3rastervision/protos/tf_object_detection/train.proto\\x12\\\'rastervision.protos.tf_object_detection\\x1a\\x37rastervision/protos/tf_object_detection/optimizer.proto\\x1a:rastervision/protos/tf_object_detection/preprocessor.proto\\""\\x90\\x08\\n\\x0bTrainConfig\\x12\\x16\\n\\nbatch_size\\x18\\x01 \\x01(\\r:\\x02\\x33\\x32\\x12]\\n\\x19\\x64\\x61ta_augmentation_options\\x18\\x02 \\x03(\\x0b\\x32:.rastervision.protos.tf_object_detection.PreprocessingStep\\x12\\x1c\\n\\rsync_replicas\\x18\\x03 \\x01(\\x08:\\x05\\x66\\x61lse\\x12+\\n\\x1dkeep_checkpoint_every_n_hours\\x18\\x04 \\x01(\\r:\\x04\\x31\\x30\\x30\\x30\\x12\\x45\\n\\toptimizer\\x18\\x05 \\x01(\\x0b\\x32\\x32.rastervision.protos.tf_object_detection.Optimizer\\x12$\\n\\x19gradient_clipping_by_norm\\x18\\x06 \\x01(\\x02:\\x01\\x30\\x12\\x1e\\n\\x14\\x66ine_tune_checkpoint\\x18\\x07 \\x01(\\t:\\x00\\x12#\\n\\x19\\x66ine_tune_checkpoint_type\\x18\\x16 \\x01(\\t:\\x00\\x12,\\n\\x19\\x66rom_detection_checkpoint\\x18\\x08 \\x01(\\x08:\\x05\\x66\\x61lseB\\x02\\x18\\x01\\x12\\x31\\n\\""load_all_detection_checkpoint_vars\\x18\\x13 \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\x14\\n\\tnum_steps\\x18\\t \\x01(\\r:\\x01\\x30\\x12\\x1f\\n\\x13startup_delay_steps\\x18\\n \\x01(\\x02:\\x02\\x31\\x35\\x12\\x1f\\n\\x14\\x62ias_grad_multiplier\\x18\\x0b \\x01(\\x02:\\x01\\x30\\x12\\""\\n\\x1aupdate_trainable_variables\\x18\\x19 \\x03(\\t\\x12\\x18\\n\\x10\\x66reeze_variables\\x18\\x0c \\x03(\\t\\x12 \\n\\x15replicas_to_aggregate\\x18\\r \\x01(\\x05:\\x01\\x31\\x12!\\n\\x14\\x62\\x61tch_queue_capacity\\x18\\x0e \\x01(\\x05:\\x03\\x31\\x35\\x30\\x12\\""\\n\\x17num_batch_queue_threads\\x18\\x0f \\x01(\\x05:\\x01\\x38\\x12\\""\\n\\x17prefetch_queue_capacity\\x18\\x10 \\x01(\\x05:\\x01\\x35\\x12)\\n\\x1amerge_multiple_label_boxes\\x18\\x11 \\x01(\\x08:\\x05\\x66\\x61lse\\x12$\\n\\x15use_multiclass_scores\\x18\\x18 \\x01(\\x08:\\x05\\x66\\x61lse\\x12%\\n\\x17\\x61\\x64\\x64_regularization_loss\\x18\\x12 \\x01(\\x08:\\x04true\\x12$\\n\\x13max_number_of_boxes\\x18\\x14 \\x01(\\x05:\\x03\\x31\\x30\\x30\\x42\\x02\\x18\\x01\\x12\\\'\\n\\x19unpad_groundtruth_tensors\\x18\\x15 \\x01(\\x08:\\x04true\\x12%\\n\\x16retain_original_images\\x18\\x17 \\x01(\\x08:\\x05\\x66\\x61lse\\x12\\x1b\\n\\x0cuse_bfloat16\\x18\\x1a \\x01(\\x08:\\x05\\x66\\x61lse\')\n  ,\n  dependencies=[rastervision_dot_protos_dot_tf__object__detection_dot_optimizer__pb2.DESCRIPTOR,rastervision_dot_protos_dot_tf__object__detection_dot_preprocessor__pb2.DESCRIPTOR,])\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\n\n\n\n_TRAINCONFIG = _descriptor.Descriptor(\n  name=\'TrainConfig\',\n  full_name=\'rastervision.protos.tf_object_detection.TrainConfig\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'batch_size\', full_name=\'rastervision.protos.tf_object_detection.TrainConfig.batch_size\', index=0,\n      number=1, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=32,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'data_augmentation_options\', full_name=\'rastervision.protos.tf_object_detection.TrainConfig.data_augmentation_options\', index=1,\n      number=2, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'sync_replicas\', full_name=\'rastervision.protos.tf_object_detection.TrainConfig.sync_replicas\', index=2,\n      number=3, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'keep_checkpoint_every_n_hours\', full_name=\'rastervision.protos.tf_object_detection.TrainConfig.keep_checkpoint_every_n_hours\', index=3,\n      number=4, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=1000,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'optimizer\', full_name=\'rastervision.protos.tf_object_detection.TrainConfig.optimizer\', index=4,\n      number=5, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'gradient_clipping_by_norm\', full_name=\'rastervision.protos.tf_object_detection.TrainConfig.gradient_clipping_by_norm\', index=5,\n      number=6, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'fine_tune_checkpoint\', full_name=\'rastervision.protos.tf_object_detection.TrainConfig.fine_tune_checkpoint\', index=6,\n      number=7, type=9, cpp_type=9, label=1,\n      has_default_value=True, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'fine_tune_checkpoint_type\', full_name=\'rastervision.protos.tf_object_detection.TrainConfig.fine_tune_checkpoint_type\', index=7,\n      number=22, type=9, cpp_type=9, label=1,\n      has_default_value=True, default_value=_b("""").decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'from_detection_checkpoint\', full_name=\'rastervision.protos.tf_object_detection.TrainConfig.from_detection_checkpoint\', index=8,\n      number=8, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=_descriptor._ParseOptions(descriptor_pb2.FieldOptions(), _b(\'\\030\\001\'))),\n    _descriptor.FieldDescriptor(\n      name=\'load_all_detection_checkpoint_vars\', full_name=\'rastervision.protos.tf_object_detection.TrainConfig.load_all_detection_checkpoint_vars\', index=9,\n      number=19, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'num_steps\', full_name=\'rastervision.protos.tf_object_detection.TrainConfig.num_steps\', index=10,\n      number=9, type=13, cpp_type=3, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'startup_delay_steps\', full_name=\'rastervision.protos.tf_object_detection.TrainConfig.startup_delay_steps\', index=11,\n      number=10, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(15),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'bias_grad_multiplier\', full_name=\'rastervision.protos.tf_object_detection.TrainConfig.bias_grad_multiplier\', index=12,\n      number=11, type=2, cpp_type=6, label=1,\n      has_default_value=True, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'update_trainable_variables\', full_name=\'rastervision.protos.tf_object_detection.TrainConfig.update_trainable_variables\', index=13,\n      number=25, type=9, cpp_type=9, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'freeze_variables\', full_name=\'rastervision.protos.tf_object_detection.TrainConfig.freeze_variables\', index=14,\n      number=12, type=9, cpp_type=9, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'replicas_to_aggregate\', full_name=\'rastervision.protos.tf_object_detection.TrainConfig.replicas_to_aggregate\', index=15,\n      number=13, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=1,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'batch_queue_capacity\', full_name=\'rastervision.protos.tf_object_detection.TrainConfig.batch_queue_capacity\', index=16,\n      number=14, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=150,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'num_batch_queue_threads\', full_name=\'rastervision.protos.tf_object_detection.TrainConfig.num_batch_queue_threads\', index=17,\n      number=15, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=8,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'prefetch_queue_capacity\', full_name=\'rastervision.protos.tf_object_detection.TrainConfig.prefetch_queue_capacity\', index=18,\n      number=16, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=5,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'merge_multiple_label_boxes\', full_name=\'rastervision.protos.tf_object_detection.TrainConfig.merge_multiple_label_boxes\', index=19,\n      number=17, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'use_multiclass_scores\', full_name=\'rastervision.protos.tf_object_detection.TrainConfig.use_multiclass_scores\', index=20,\n      number=24, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'add_regularization_loss\', full_name=\'rastervision.protos.tf_object_detection.TrainConfig.add_regularization_loss\', index=21,\n      number=18, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=True,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'max_number_of_boxes\', full_name=\'rastervision.protos.tf_object_detection.TrainConfig.max_number_of_boxes\', index=22,\n      number=20, type=5, cpp_type=1, label=1,\n      has_default_value=True, default_value=100,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=_descriptor._ParseOptions(descriptor_pb2.FieldOptions(), _b(\'\\030\\001\'))),\n    _descriptor.FieldDescriptor(\n      name=\'unpad_groundtruth_tensors\', full_name=\'rastervision.protos.tf_object_detection.TrainConfig.unpad_groundtruth_tensors\', index=23,\n      number=21, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=True,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'retain_original_images\', full_name=\'rastervision.protos.tf_object_detection.TrainConfig.retain_original_images\', index=24,\n      number=23, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n    _descriptor.FieldDescriptor(\n      name=\'use_bfloat16\', full_name=\'rastervision.protos.tf_object_detection.TrainConfig.use_bfloat16\', index=25,\n      number=26, type=8, cpp_type=7, label=1,\n      has_default_value=True, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      options=None),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=214,\n  serialized_end=1254,\n)\n\n_TRAINCONFIG.fields_by_name[\'data_augmentation_options\'].message_type = rastervision_dot_protos_dot_tf__object__detection_dot_preprocessor__pb2._PREPROCESSINGSTEP\n_TRAINCONFIG.fields_by_name[\'optimizer\'].message_type = rastervision_dot_protos_dot_tf__object__detection_dot_optimizer__pb2._OPTIMIZER\nDESCRIPTOR.message_types_by_name[\'TrainConfig\'] = _TRAINCONFIG\n\nTrainConfig = _reflection.GeneratedProtocolMessageType(\'TrainConfig\', (_message.Message,), dict(\n  DESCRIPTOR = _TRAINCONFIG,\n  __module__ = \'rastervision.protos.tf_object_detection.train_pb2\'\n  # @@protoc_insertion_point(class_scope:rastervision.protos.tf_object_detection.TrainConfig)\n  ))\n_sym_db.RegisterMessage(TrainConfig)\n\n\n_TRAINCONFIG.fields_by_name[\'from_detection_checkpoint\'].has_options = True\n_TRAINCONFIG.fields_by_name[\'from_detection_checkpoint\']._options = _descriptor._ParseOptions(descriptor_pb2.FieldOptions(), _b(\'\\030\\001\'))\n_TRAINCONFIG.fields_by_name[\'max_number_of_boxes\'].has_options = True\n_TRAINCONFIG.fields_by_name[\'max_number_of_boxes\']._options = _descriptor._ParseOptions(descriptor_pb2.FieldOptions(), _b(\'\\030\\001\'))\n# @@protoc_insertion_point(module_scope)\n'"
rastervision2/core/analyzer/__init__.py,0,b'# flake8: noqa\n\nfrom rastervision2.core.analyzer.analyzer import *\nfrom rastervision2.core.analyzer.analyzer_config import *\nfrom rastervision2.core.analyzer.stats_analyzer import *\nfrom rastervision2.core.analyzer.stats_analyzer_config import *\n'
rastervision2/core/analyzer/analyzer.py,0,"b'from typing import List\nfrom abc import (ABC, abstractmethod)\n\nfrom rastervision2.core.data import Scene\n\n\nclass Analyzer(ABC):\n    """"""Analyzes scenes and writes some output while running the analyze command.\n\n    This output can be used to normalize images, for example.\n    """"""\n\n    @abstractmethod\n    def process(self, scenes: List[Scene], tmp_dir: str):\n        """"""Process scenes and save result.""""""\n        pass\n'"
rastervision2/core/analyzer/analyzer_config.py,0,"b'from typing import List, TYPE_CHECKING\n\nfrom rastervision2.pipeline.config import register_config, Config\n\nif TYPE_CHECKING:\n    from rastervision2.core.analyzer.analyzer import Analyzer  # noqa\n\n\n@register_config(\'analyzer\')\nclass AnalyzerConfig(Config):\n    def build(self) -> \'Analyzer\':\n        pass\n\n    def get_bundle_filenames(self) -> List[str]:\n        """"""Returns the names of files that should be included in a model bundle.\n\n        The files are assumed to be in the analyze/ directory generated by the analyze\n        command. Note that only the names, not the full paths should be returned.\n        """"""\n        return []\n'"
rastervision2/core/analyzer/stats_analyzer.py,0,"b'from typing import List\n\nfrom rastervision2.core.analyzer import Analyzer\nfrom rastervision2.core.raster_stats import RasterStats\nfrom rastervision2.core.data import Scene\n\n\nclass StatsAnalyzer(Analyzer):\n    """"""Computes RasterStats against the entire scene set.""""""\n\n    def __init__(self, stats_uri: str, sample_prob: float = 0.1):\n        self.stats_uri = stats_uri\n        self.sample_prob = sample_prob\n\n    def process(self, scenes: List[Scene], tmp_dir: str):\n        stats = RasterStats()\n        stats.compute(\n            [s.raster_source for s in scenes], sample_prob=self.sample_prob)\n        stats.save(self.stats_uri)\n'"
rastervision2/core/analyzer/stats_analyzer_config.py,0,"b""from typing import Optional\nfrom os.path import join\n\nfrom rastervision2.pipeline.config import register_config, ConfigError, Field\nfrom rastervision2.core.analyzer import AnalyzerConfig\n\n\n@register_config('stats_analyzer')\nclass StatsAnalyzerConfig(AnalyzerConfig):\n    output_uri: Optional[str] = Field(\n        None,\n        description=(\n            'URI for output. If None and this is part of an RVPipeline, this is '\n            'auto-generated.'))\n    sample_prob: Optional[float] = Field(\n        0.1,\n        description=(\n            'The probability of using a random window for computing statistics. '\n            'If None, will use a sliding window.'))\n\n    def update(self, pipeline=None):\n        if pipeline is not None and self.output_uri is None:\n            self.output_uri = join(pipeline.analyze_uri, 'stats.json')\n\n    def validate_config(self):\n        if self.sample_prob > 1 or self.sample_prob <= 0:\n            raise ConfigError('sample_prob must be <= 1 and > 0')\n\n    def build(self):\n        from rastervision2.core.analyzer import StatsAnalyzer\n        return StatsAnalyzer(self.output_uri, self.sample_prob)\n\n    def get_bundle_filenames(self):\n        return ['stats.json']\n"""
rastervision2/core/backend/__init__.py,0,b'# flake8: noqa\n\nfrom rastervision2.core.backend.backend import *\nfrom rastervision2.core.backend.backend_config import *\n'
rastervision2/core/backend/backend.py,0,"b'from abc import ABC, abstractmethod\nfrom typing import List\nfrom contextlib import AbstractContextManager\n\nimport numpy as np\n\nfrom rastervision2.core.data_sample import DataSample\nfrom rastervision2.core.box import Box\nfrom rastervision2.core.data import Labels\n\n\nclass SampleWriter(AbstractContextManager):\n    """"""Writes DataSamples in a streaming fashion.\n\n    This is a context manager used for creating training and validation chips, and\n    should be subclassed for each Backend.\n    """"""\n\n    @abstractmethod\n    def write_sample(self, sample: DataSample):\n        """"""Writes a single sample.""""""\n        pass\n\n\nclass Backend(ABC):\n    """"""Abstraction around core ML functionality used by an RVPipeline.\n\n    This should be subclassed to enable use of a third party ML library with an\n    RVPipeline. There is a one-to-many relationship from RVPipeline to Backend.\n    """"""\n\n    @abstractmethod\n    def get_sample_writer(self):\n        """"""Returns a SampleWriter for this Backend.""""""\n        pass\n\n    @abstractmethod\n    def train(self):\n        """"""Train a model.\n\n        This should download chips created by the SampleWriter, train the model, and\n        then saving it to disk.\n        """"""\n        pass\n\n    @abstractmethod\n    def load_model(self):\n        """"""Load the model in preparation for one or more prediction calls.""""""\n        pass\n\n    @abstractmethod\n    def predict(self, chips: np.ndarray, windows: List[Box]) -> Labels:\n        """"""Return predictions for a batch of chips using the model.\n\n        Args:\n            chips: input images of shape [height, width, channels]\n            windows: the windows corresponding to the chips in pixel coords\n\n        Return:\n            Labels object containing predictions\n        """"""\n        pass\n'"
rastervision2/core/backend/backend_config.py,0,"b'from typing import Optional, List, TYPE_CHECKING\n\nfrom rastervision2.pipeline.config import Config, register_config\n\nif TYPE_CHECKING:\n    from rastervision2.core.rv_pipeline.rv_pipeline import RVPipeline  # noqa\n    from rastervision2.core.backend.backend import Backend  # noqa\n\n\n@register_config(\'backend\')\nclass BackendConfig(Config):\n    """"""Configuration for Backend.""""""\n\n    def build(self, pipeline: \'RVPipeline\', tmp_dir: str) -> \'Backend\':\n        raise NotImplementedError()\n\n    def get_bundle_filenames(self) -> List[str]:\n        """"""Returns the names of files that should be included in a model bundle.\n\n        The files are assumed to be in the train/ directory generated by the train\n        command. Note that only the names, not the full paths should be returned.\n        """"""\n        raise NotImplementedError()\n\n    def update(self, pipeline: Optional[\'RVPipeline\'] = None):  # noqa\n        pass\n'"
rastervision2/core/data/__init__.py,0,b'# flake8: noqa\n\nfrom rastervision2.core.data.activate_mixin import *\nfrom rastervision2.core.data.raster_source import *\nfrom rastervision2.core.data.crs_transformer import *\nfrom rastervision2.core.data.label import *\nfrom rastervision2.core.data.vector_source import *\nfrom rastervision2.core.data.label_source import *\nfrom rastervision2.core.data.label_store import *\nfrom rastervision2.core.data.scene import *\nfrom rastervision2.core.data.scene_config import *\nfrom rastervision2.core.data.dataset import *\nfrom rastervision2.core.data.dataset_config import *\nfrom rastervision2.core.data.class_config import *\nfrom rastervision2.core.data.raster_transformer import *\n'
rastervision2/core/data/activate_mixin.py,0,"b'from abc import abstractmethod\n\n\nclass ActivationError(Exception):\n    pass\n\n\nclass ActivateMixin:\n    """"""Defines a mixin for data that can activate and deactivate.\n    These methods can open and close files, download files, and do\n    whatever has to be done to make the entity usable, and cleanup\n    after the entity is not needed anymore.\n    """"""\n\n    class ActivateContextManager:\n        def __init__(self, activate, deactivate):\n            self.activate = activate\n            self.deactivate = deactivate\n\n        def __enter__(self):\n            self.activate()\n            return self\n\n        def __exit__(self, type, value, traceback):\n            self.deactivate()\n\n        @classmethod\n        def dummy(cls):\n            def noop():\n                pass\n\n            return cls(noop, noop)\n\n    class CompositeContextManager:\n        def __init__(self, *managers):\n            self.managers = managers\n\n        def __enter__(self):\n            for manager in self.managers:\n                manager.__enter__()\n\n        def __exit__(self, type, value, traceback):\n            for manager in self.managers:\n                manager.__exit__(type, value, traceback)\n\n    def activate(self):\n        if hasattr(self, \'_mixin_activated\'):\n            if self._mixin_activated:\n                raise ActivationError(\'This {} is already activated\'.format(\n                    type(self)))\n\n        def do_activate():\n            self._mixin_activated = True\n            self._activate()\n\n        def do_deactivate():\n            self._deactivate()\n            self._mixin_activated = False\n\n        a = ActivateMixin.ActivateContextManager(do_activate, do_deactivate)\n        subcomponents = self._subcomponents_to_activate()\n        if subcomponents:\n            return ActivateMixin.CompositeContextManager(\n                a, ActivateMixin.compose(*subcomponents))\n        else:\n            return a\n\n    @abstractmethod\n    def _activate(self):\n        pass\n\n    @abstractmethod\n    def _deactivate(self):\n        pass\n\n    def _subcomponents_to_activate(self):\n        """"""Subclasses override this if they have subcomponents\n        that may need to be activated when this class is activated\n        """"""\n        return []\n\n    @staticmethod\n    def with_activation(obj):\n        """"""Method will give activate an object if it mixes in  the ActivateMixin and\n        return the context manager, or else return a dummy context manager.\n        """"""\n        if obj is None or not isinstance(obj, ActivateMixin):\n            return ActivateMixin.dummy()\n        else:\n            return obj.activate()\n\n    @staticmethod\n    def compose(*objs):\n        managers = [\n            obj.activate() for obj in objs\n            if obj is not None and isinstance(obj, ActivateMixin)\n        ]\n        return ActivateMixin.CompositeContextManager(*managers)\n'"
rastervision2/core/data/class_config.py,0,"b'from typing import List, Optional, Union, Tuple\n\nfrom rastervision2.pipeline.config import (Config, register_config,\n                                           ConfigError, Field)\nfrom rastervision2.core.data.utils import color_to_triple\n\n\n@register_config(\'class_config\')\nclass ClassConfig(Config):\n    """"""Configures the class names that are being predicted.""""""\n    names: List[str] = Field(..., description=\'Names of classes.\')\n    colors: Optional[List[Union[Tuple, str]]] = Field(\n        None,\n        description=\n        (\'Colors used to visualize classes. Can be color strings accepted by \'\n         \'matplotlib or RGB tuples. If None, a random color will be auto-generated \'\n         \'for each class.\'))\n    null_class: Optional[str] = Field(\n        None,\n        description=\n        (\'Optional name of class in `names` to use as the null class. This is used in \'\n         \'semantic segmentation to represent the label for imagery pixels that are \'\n         \'NODATA or that are missing a label. If None, and this Config is part of a \'\n         \'SemanticSegmentationConfig, a null class will be added automatically.\'\n         ))\n\n    def get_class_id(self, name):\n        return self.names.index(name)\n\n    def get_name(self, id):\n        return self.names[id]\n\n    def get_null_class_id(self):\n        if self.null_class is None:\n            raise ValueError(\'null_class is not set\')\n        return self.get_class_id(self.null_class)\n\n    def get_color_to_class_id(self):\n        return dict([(self.colors[i], i) for i in range(len(self.colors))])\n\n    def ensure_null_class(self):\n        """"""Add a null class if one isn\'t set.""""""\n        if self.null_class is None:\n            self.null_class = \'null\'\n            self.names.append(\'null\')\n            self.colors.append(\'black\')\n\n    def update(self, pipeline=None):\n        if not self.colors:\n            self.colors = [color_to_triple() for _ in self.names]\n\n    def validate_config(self):\n        if self.null_class is not None and self.null_class not in self.names:\n            raise ConfigError(\n                \'The null_class: {} must be in list of class names.\'.format(\n                    self.null_class))\n\n    def __len__(self):\n        return len(self.names)\n'"
rastervision2/core/data/dataset.py,0,"b'class Dataset:\n    def __init__(self, train_scenes=[], validation_scenes=[], test_scenes=[]):\n        self.train_scenes = train_scenes\n        self.validation_scenes = validation_scenes\n        self.test_scenes = test_scenes\n'"
rastervision2/core/data/dataset_config.py,0,"b""from typing import List\n\nfrom rastervision2.pipeline.config import Config, register_config, ConfigError\nfrom rastervision2.pipeline.utils import split_into_groups\nfrom rastervision2.core.data.scene_config import SceneConfig\nfrom rastervision2.core.data.class_config import ClassConfig\n\n\n@register_config('dataset')\nclass DatasetConfig(Config):\n    class_config: ClassConfig\n    train_scenes: List[SceneConfig]\n    validation_scenes: List[SceneConfig]\n    test_scenes: List[SceneConfig] = []\n\n    def update(self, pipeline=None):\n        super().update()\n\n        self.class_config.update(pipeline=pipeline)\n        for s in self.train_scenes:\n            s.update(pipeline=pipeline)\n        for s in self.validation_scenes:\n            s.update(pipeline=pipeline)\n        if self.test_scenes is not None:\n            for s in self.test_scenes:\n                s.update(pipeline=pipeline)\n\n    def validate_config(self):\n        ids = [s.id for s in self.train_scenes]\n        if len(set(ids)) != len(ids):\n            raise ConfigError('All training scene ids must be unique.')\n\n        ids = [s.id for s in self.validation_scenes + self.test_scenes]\n        if len(set(ids)) != len(ids):\n            raise ConfigError(\n                'All validation and test scene ids must be unique.')\n\n    def get_split_config(self, split_ind, num_splits):\n        new_cfg = self.copy()\n\n        groups = split_into_groups(self.train_scenes, num_splits)\n        new_cfg.train_scenes = groups[\n            split_ind] if split_ind < len(groups) else []\n\n        groups = split_into_groups(self.validation_scenes, num_splits)\n        new_cfg.validation_scenes = groups[\n            split_ind] if split_ind < len(groups) else []\n\n        if self.test_scenes:\n            groups = split_into_groups(self.test_scenes, num_splits)\n            new_cfg.test_scenes = groups[\n                split_ind] if split_ind < len(groups) else []\n\n        return new_cfg\n\n    def get_all_scenes(self):\n        return self.train_scenes + self.validation_scenes + self.test_scenes\n"""
rastervision2/core/data/scene.py,0,"b'from rastervision2.core.data import ActivateMixin\n\n\nclass Scene(ActivateMixin):\n    """"""The raster data and labels associated with an area of interest.""""""\n\n    def __init__(self,\n                 id,\n                 raster_source,\n                 ground_truth_label_source=None,\n                 prediction_label_store=None,\n                 aoi_polygons=None):\n        """"""Construct a new Scene.\n\n        Args:\n            id: ID for this scene\n            raster_source: RasterSource for this scene\n            ground_truth_label_store: optional LabelSource\n            prediction_label_store: optional LabelStore\n            aoi: Optional list of AOI polygons\n        """"""\n        self.id = id\n        self.raster_source = raster_source\n        self.ground_truth_label_source = ground_truth_label_source\n        self.prediction_label_store = prediction_label_store\n        if aoi_polygons is None:\n            self.aoi_polygons = []\n        else:\n            self.aoi_polygons = aoi_polygons\n\n    def _subcomponents_to_activate(self):\n        return [\n            self.raster_source, self.ground_truth_label_source,\n            self.prediction_label_store\n        ]\n\n    def _activate(self):\n        pass\n\n    def _deactivate(self):\n        pass\n'"
rastervision2/core/data/scene_config.py,0,"b""from typing import Optional, List\n\nfrom shapely.geometry import shape\n\nfrom rastervision2.pipeline.config import Config, register_config, Field\nfrom rastervision2.core.data.raster_source import RasterSourceConfig\nfrom rastervision2.core.data.label_source import LabelSourceConfig\nfrom rastervision2.core.data.label_store import LabelStoreConfig\nfrom rastervision2.core.data.scene import Scene\nfrom rastervision2.core.data.vector_source import GeoJSONVectorSourceConfig\n\n\n@register_config('scene')\nclass SceneConfig(Config):\n    id: str\n    raster_source: RasterSourceConfig\n    label_source: LabelSourceConfig\n    label_store: Optional[LabelStoreConfig] = None\n    aoi_uris: Optional[List[str]] = Field(\n        None,\n        description=\n        ('List of URIs of GeoJSON files that define the AOIs for the scene. Each polygon'\n         'defines an AOI which is a piece of the scene that is assumed to be fully '\n         'labeled and usable for training or validation.'))\n\n    def build(self, class_config, tmp_dir, use_transformers=True):\n        raster_source = self.raster_source.build(\n            tmp_dir, use_transformers=use_transformers)\n        crs_transformer = raster_source.get_crs_transformer()\n        extent = raster_source.get_extent()\n\n        label_source = (self.label_source.build(class_config, crs_transformer,\n                                                extent, tmp_dir)\n                        if self.label_source is not None else None)\n        label_store = (self.label_store.build(class_config, crs_transformer,\n                                              extent, tmp_dir)\n                       if self.label_store is not None else None)\n\n        aoi_polygons = None\n        if self.aoi_uris is not None:\n            aoi_polygons = []\n            for uri in self.aoi_uris:\n                # Set default class id to 0 to avoid deleting features. If it was\n                # set to None, they would all be deleted.\n                aoi_geojson = GeoJSONVectorSourceConfig(\n                    uri=uri, default_class_id=0, ignore_crs_field=True).build(\n                        class_config, crs_transformer).get_geojson()\n                for f in aoi_geojson['features']:\n                    aoi_polygons.append(shape(f['geometry']))\n\n        return Scene(\n            self.id,\n            raster_source,\n            ground_truth_label_source=label_source,\n            prediction_label_store=label_store,\n            aoi_polygons=aoi_polygons)\n\n    def update(self, pipeline=None):\n        super().update()\n\n        self.raster_source.update(pipeline=pipeline, scene=self)\n        self.label_source.update(pipeline=pipeline, scene=self)\n        if self.label_store is None and pipeline is not None:\n            self.label_store = pipeline.get_default_label_store(scene=self)\n        if self.label_store is not None:\n            self.label_store.update(pipeline=pipeline, scene=self)\n"""
rastervision2/core/data/utils.py,0,"b'from typing import Tuple, Optional\n\nimport numpy as np\nfrom PIL import ImageColor\n\n\ndef color_to_triple(color: Optional[str] = None) -> Tuple[int, int, int]:\n    """"""Given a PIL ImageColor string, return a triple of integers\n    representing the red, green, and blue values.\n\n    If color is None, return a random color.\n\n    Args:\n         color: A PIL ImageColor string\n\n    Returns:\n         An triple of integers\n\n    """"""\n    if color is None:\n        r = np.random.randint(0, 0x100)\n        g = np.random.randint(0, 0x100)\n        b = np.random.randint(0, 0x100)\n        return (r, g, b)\n    else:\n        return ImageColor.getrgb(color)\n\n\ndef color_to_integer(color: str) -> int:\n    """"""Given a PIL ImageColor string, return a packed integer.\n\n    Args:\n         color: A PIL ImageColor string\n\n    Returns:\n         An integer containing the packed RGB values.\n\n    """"""\n    triple = color_to_triple(color)\n    r = triple[0] * (1 << 16)\n    g = triple[1] * (1 << 8)\n    b = triple[2] * (1 << 0)\n    integer = r + g + b\n    return integer\n\n\ndef rgb_to_int_array(rgb_array):\n    r = np.array(rgb_array[:, :, 0], dtype=np.uint32) * (1 << 16)\n    g = np.array(rgb_array[:, :, 1], dtype=np.uint32) * (1 << 8)\n    b = np.array(rgb_array[:, :, 2], dtype=np.uint32) * (1 << 0)\n    return r + g + b\n\n\ndef boxes_to_geojson(boxes, class_ids, crs_transformer, class_map,\n                     scores=None):\n    """"""Convert boxes and associated data into a GeoJSON dict.\n\n    Args:\n        boxes: list of Box in pixel row/col format.\n        class_ids: list of int (one for each box)\n        crs_transformer: CRSTransformer used to convert pixel coords to map\n            coords in the GeoJSON\n        class_map: ClassMap used to infer class_name from class_id\n        scores: optional list of score or scores.\n                If floats (one for each box), property name will be ""score"".\n                If lists of floats, property name will be ""scores"".\n\n    Returns:\n        dict in GeoJSON format\n    """"""\n    features = []\n    for box_ind, box in enumerate(boxes):\n        polygon = box.geojson_coordinates()\n        polygon = [list(crs_transformer.pixel_to_map(p)) for p in polygon]\n\n        class_id = int(class_ids[box_ind])\n        class_name = class_map.get_by_id(class_id).name\n\n        feature = {\n            \'type\': \'Feature\',\n            \'geometry\': {\n                \'type\': \'Polygon\',\n                \'coordinates\': [polygon]\n            },\n            \'properties\': {\n                \'class_id\': class_id,\n                \'class_name\': class_name\n            }\n        }\n\n        if scores is not None:\n            box_scores = scores[box_ind]\n\n            if box_scores is not None:\n                if type(box_scores) is list:\n                    feature[\'properties\'][\'scores\'] = box_scores\n                else:\n                    feature[\'properties\'][\'score\'] = box_scores\n\n        features.append(feature)\n\n    return {\'type\': \'FeatureCollection\', \'features\': features}\n'"
rastervision2/core/evaluation/__init__.py,0,b'# flake8: noqa\n\nfrom rastervision2.core.evaluation.evaluation_item import *\nfrom rastervision2.core.evaluation.class_evaluation_item import *\nfrom rastervision2.core.evaluation.evaluator import *\nfrom rastervision2.core.evaluation.evaluator_config import *\nfrom rastervision2.core.evaluation.classification_evaluation import *\nfrom rastervision2.core.evaluation.classification_evaluator import *\nfrom rastervision2.core.evaluation.classification_evaluator_config import *\nfrom rastervision2.core.evaluation.chip_classification_evaluation import *\nfrom rastervision2.core.evaluation.chip_classification_evaluator import *\nfrom rastervision2.core.evaluation.chip_classification_evaluator_config import *\nfrom rastervision2.core.evaluation.semantic_segmentation_evaluation import *\nfrom rastervision2.core.evaluation.semantic_segmentation_evaluator import *\nfrom rastervision2.core.evaluation.semantic_segmentation_evaluator_config import *\nfrom rastervision2.core.evaluation.object_detection_evaluation import *\nfrom rastervision2.core.evaluation.object_detection_evaluator import *\nfrom rastervision2.core.evaluation.object_detection_evaluator_config import *\n'
rastervision2/core/evaluation/chip_classification_evaluation.py,0,"b'import numpy as np\nfrom sklearn import metrics\n\nfrom rastervision2.core.evaluation import (ClassificationEvaluation,\n                                           ClassEvaluationItem)\n\n\nclass ChipClassificationEvaluation(ClassificationEvaluation):\n    def __init__(self, class_config):\n        super().__init__()\n        self.class_config = class_config\n\n    def compute(self, ground_truth_labels, prediction_labels):\n        self.class_to_eval_item = ChipClassificationEvaluation.compute_eval_items(\n            ground_truth_labels, prediction_labels, self.class_config)\n\n        self.compute_avg()\n\n    @staticmethod\n    def compute_eval_items(gt_labels, pred_labels, class_config):\n        nb_classes = len(class_config.names)\n        class_to_eval_item = {}\n\n        gt_class_ids = []\n        pred_class_ids = []\n\n        gt_cells = gt_labels.get_cells()\n        for gt_cell in gt_cells:\n            gt_class_id = gt_labels.get_cell_class_id(gt_cell)\n            pred_class_id = pred_labels.get_cell_class_id(gt_cell)\n\n            if gt_class_id is not None and pred_class_id is not None:\n                gt_class_ids.append(gt_class_id)\n                pred_class_ids.append(pred_class_id)\n\n        sklabels = np.arange(nb_classes)\n        precision, recall, f1, support = metrics.precision_recall_fscore_support(\n            gt_class_ids, pred_class_ids, labels=sklabels, warn_for=())\n\n        for class_id, class_name in enumerate(class_config.names):\n            eval_item = ClassEvaluationItem(\n                float(precision[class_id]),\n                float(recall[class_id]),\n                float(f1[class_id]),\n                gt_count=float(support[class_id]),\n                class_id=class_id,\n                class_name=class_name)\n            class_to_eval_item[class_id] = eval_item\n\n        return class_to_eval_item\n'"
rastervision2/core/evaluation/chip_classification_evaluator.py,0,"b'from rastervision2.core.evaluation import (ClassificationEvaluator,\n                                           ChipClassificationEvaluation)\n\n\nclass ChipClassificationEvaluator(ClassificationEvaluator):\n    """"""Evaluates predictions for a set of scenes.\n    """"""\n\n    def __init__(self, class_config, output_uri):\n        super().__init__(class_config, output_uri)\n\n    def create_evaluation(self):\n        return ChipClassificationEvaluation(self.class_config)\n'"
rastervision2/core/evaluation/chip_classification_evaluator_config.py,0,"b""from rastervision2.pipeline.config import register_config\nfrom rastervision2.core.evaluation.classification_evaluator_config import (\n    ClassificationEvaluatorConfig)\nfrom rastervision2.core.evaluation.chip_classification_evaluator import (\n    ChipClassificationEvaluator)\n\n\n@register_config('chip_classification_evaluator')\nclass ChipClassificationEvaluatorConfig(ClassificationEvaluatorConfig):\n    def build(self, class_config):\n        return ChipClassificationEvaluator(class_config, self.output_uri)\n"""
rastervision2/core/evaluation/class_evaluation_item.py,0,"b'import numpy as np\n\nfrom rastervision2.core.evaluation import EvaluationItem\n\n\nclass ClassEvaluationItem(EvaluationItem):\n    """"""Evaluation metrics for a single class (or average over classes).\n\n    None is used for values that are undefined because they involve a division\n    by zero (eg. precision when there are no predictions).\n    """"""\n\n    def __init__(self,\n                 precision=None,\n                 recall=None,\n                 f1=None,\n                 count_error=None,\n                 gt_count=0,\n                 class_id=None,\n                 class_name=None,\n                 conf_mat=None):\n        self.precision = precision\n        self.recall = recall\n        self.f1 = f1\n        self.count_error = count_error\n        # Ground truth count of elements (boxes for object detection, pixels\n        # for segmentation, cells for classification).\n        self.gt_count = gt_count\n        self.conf_mat = conf_mat\n        self.class_id = class_id\n        self.class_name = class_name\n\n    def merge(self, other):\n        """"""Merges another item from a different scene into this one.\n\n        This is used to average metrics over scenes. Merges by taking a\n        weighted average (by gt_count) of the metrics.\n        """"""\n        if other.gt_count > 0:\n            total_gt_count = self.gt_count + other.gt_count\n            self_ratio = self.gt_count / total_gt_count\n            other_ratio = other.gt_count / total_gt_count\n\n            def weighted_avg(self_val, other_val):\n                if self_val is None and other_val is None:\n                    return 0.0\n                # Handle a single None value by setting them to zero.\n                return (self_ratio * (self_val or 0) +\n                        other_ratio * (other_val or 0))\n\n            self.precision = weighted_avg(self.precision, other.precision)\n            self.recall = weighted_avg(self.recall, other.recall)\n            self.f1 = weighted_avg(self.f1, other.f1)\n            self.count_error = weighted_avg(self.count_error,\n                                            other.count_error)\n            self.gt_count = total_gt_count\n\n        if other.conf_mat is not None:\n            if self.class_name == \'average\':\n                if self.conf_mat is None:\n                    # Make first row all zeros so that the array indices\n                    # correspond to valid class ids (ie. >= 1).\n                    self.conf_mat = np.concatenate(\n                        [\n                            np.zeros_like(other.conf_mat)[np.newaxis, :],\n                            np.array(other.conf_mat)[np.newaxis, :]\n                        ],\n                        axis=0)\n                else:\n                    self.conf_mat = np.concatenate(\n                        [self.conf_mat, other.conf_mat[np.newaxis, :]], axis=0)\n            else:\n                self.conf_mat += other.conf_mat\n\n    def to_json(self):\n        new_dict = {}\n        for k, v in self.__dict__.items():\n            new_dict[k] = v.tolist() if isinstance(v, np.ndarray) else v\n        if new_dict[\'conf_mat\'] is None:\n            del new_dict[\'conf_mat\']\n        return new_dict\n\n    def __repr__(self):\n        return str(self.to_json())\n'"
rastervision2/core/evaluation/classification_evaluation.py,0,"b'from abc import (ABC, abstractmethod)\nimport copy\n\nimport json\n\nfrom rastervision2.core.evaluation import ClassEvaluationItem\nfrom rastervision2.pipeline.file_system import str_to_file\n\n\nclass ClassificationEvaluation(ABC):\n    """"""Base class for evaluating predictions for pipelines that have classes.\n\n    Evaluations can be keyed, for instance, if evaluations happen per class.\n    """"""\n\n    def __init__(self):\n        self.clear()\n        self._is_empty = True\n\n    def clear(self):\n        """"""Clear the Evaluation.""""""\n        self.class_to_eval_item = {}\n        self.scene_to_eval = {}\n        self.avg_item = None\n        self._is_empty = True\n\n    def is_empty(self):\n        return self._is_empty\n\n    def set_class_to_eval_item(self, class_to_eval_item):\n        self.class_to_eval_item = class_to_eval_item\n\n    def get_by_id(self, key):\n        """"""Gets the evaluation for a particular EvaluationItem key""""""\n        return self.class_to_eval_item[key]\n\n    def has_id(self, key):\n        """"""Answers whether or not the EvaluationItem key is represented""""""\n        return key in self.class_to_eval_item\n\n    def to_json(self):\n        json_rep = []\n        for eval_item in self.class_to_eval_item.values():\n            json_rep.append(eval_item.to_json())\n        if self.avg_item:\n            json_rep.append(self.avg_item.to_json())\n\n        if self.scene_to_eval:\n            json_rep = {\'overall\': json_rep}\n            scene_to_eval_json = {}\n            for scene_id, eval in self.scene_to_eval.items():\n                scene_to_eval_json[scene_id] = eval.to_json()\n            json_rep[\'per_scene\'] = scene_to_eval_json\n\n        return json_rep\n\n    def save(self, output_uri):\n        """"""Save this Evaluation to a file.\n\n        Args:\n            output_uri: string URI for the file to write.\n        """"""\n        json_str = json.dumps(self.to_json(), indent=4)\n        str_to_file(json_str, output_uri)\n\n    def merge(self, evaluation, scene_id=None):\n        """"""Merge Evaluation for another Scene into this one.\n\n        This is useful for computing the average metrics of a set of scenes.\n        The results of the averaging are stored in this Evaluation.\n\n        Args:\n            evaluation: Evaluation to merge into this one\n        """"""\n        if len(self.class_to_eval_item) == 0:\n            self.class_to_eval_item = evaluation.class_to_eval_item\n        else:\n            for key, other_eval_item in \\\n                    evaluation.class_to_eval_item.items():\n                if self.has_id(key):\n                    self.get_by_id(key).merge(other_eval_item)\n                else:\n                    self.class_to_eval_item[key] = other_eval_item\n\n        self._is_empty = False\n        self.compute_avg()\n\n        if scene_id is not None:\n            self.scene_to_eval[scene_id] = copy.deepcopy(evaluation)\n\n    def compute_avg(self):\n        """"""Compute average metrics over all keys.""""""\n        self.avg_item = ClassEvaluationItem(class_name=\'average\')\n        for eval_item in self.class_to_eval_item.values():\n            self.avg_item.merge(eval_item)\n\n    @abstractmethod\n    def compute(self, ground_truth_labels, prediction_labels):\n        """"""Compute metrics for a single scene.\n\n        Args:\n            ground_truth_labels: Ground Truth labels to evaluate against.\n            prediction_labels: The predicted labels to evaluate.\n        """"""\n        pass\n'"
rastervision2/core/evaluation/classification_evaluator.py,0,"b'from abc import (abstractmethod)\nimport logging\n\nfrom rastervision2.core.evaluation import Evaluator\nfrom rastervision2.core.data import ActivateMixin\n\nlog = logging.getLogger(__name__)\n\n\nclass ClassificationEvaluator(Evaluator):\n    """"""Evaluates predictions for a set of scenes.\n    """"""\n\n    def __init__(self, class_config, output_uri):\n        self.class_config = class_config\n        self.output_uri = output_uri\n        self.eval = None\n\n    @abstractmethod\n    def create_evaluation(self):\n        pass\n\n    def process(self, scenes, tmp_dir):\n        evaluation = self.create_evaluation()\n\n        for scene in scenes:\n            log.info(\'Computing evaluation for scene {}...\'.format(scene.id))\n            label_source = scene.ground_truth_label_source\n            label_store = scene.prediction_label_store\n            with ActivateMixin.compose(label_source, label_store):\n                ground_truth = label_source.get_labels()\n                predictions = label_store.get_labels()\n\n                if scene.aoi_polygons:\n                    # Filter labels based on AOI.\n                    ground_truth = ground_truth.filter_by_aoi(\n                        scene.aoi_polygons)\n                    predictions = predictions.filter_by_aoi(scene.aoi_polygons)\n                scene_evaluation = self.create_evaluation()\n                scene_evaluation.compute(ground_truth, predictions)\n                evaluation.merge(scene_evaluation, scene_id=scene.id)\n        evaluation.save(self.output_uri)\n        self.eval = evaluation\n'"
rastervision2/core/evaluation/classification_evaluator_config.py,0,"b""from rastervision2.pipeline.config import register_config\nfrom rastervision2.core.evaluation.evaluator_config import EvaluatorConfig\n\n\n@register_config('classification_evaluator')\nclass ClassificationEvaluatorConfig(EvaluatorConfig):\n    pass\n"""
rastervision2/core/evaluation/evaluation_item.py,0,"b'from abc import (ABC, abstractmethod)\n\n\nclass EvaluationItem(ABC):\n    @abstractmethod\n    def merge(self, other):\n        """"""Merges another item from a different scene into this one.\n\n        This is used to average metrics over scenes. Merges by taking a\n        weighted average (by gt_count) of the metrics.\n        """"""\n        pass\n\n    @abstractmethod\n    def to_json(self):\n        return self.__dict__\n\n    def __repr__(self):\n        return str(self.to_json())\n'"
rastervision2/core/evaluation/evaluator.py,0,"b'from abc import (ABC, abstractmethod)\n\n\nclass Evaluator(ABC):\n    """"""Evaluates predictions for a set of scenes.\n    """"""\n\n    @abstractmethod\n    def process(self, scenes):\n        pass\n'"
rastervision2/core/evaluation/evaluator_config.py,0,"b""from typing import Optional\nfrom os.path import join\n\nfrom rastervision2.pipeline.config import register_config, Config, Field\n\n\n@register_config('evaluator')\nclass EvaluatorConfig(Config):\n    output_uri: Optional[str] = Field(\n        None,\n        description=\n        ('URI of JSON output by evaluator. If None, and this Config is part of an '\n         'RVPipeline, then this field will be auto-generated.'))\n\n    def update(self, pipeline=None):\n        if pipeline is not None and self.output_uri is None:\n            self.output_uri = join(pipeline.eval_uri, 'eval.json')\n"""
rastervision2/core/evaluation/object_detection_evaluation.py,0,"b""import numpy as np\nimport shapely\nimport shapely.strtree\nimport shapely.geometry\n\nfrom rastervision2.core.data import ObjectDetectionLabels\nfrom rastervision2.core.evaluation import ClassEvaluationItem\nfrom rastervision2.core.evaluation import ClassificationEvaluation\n\n\ndef compute_metrics(gt_labels: ObjectDetectionLabels,\n                    pred_labels: ObjectDetectionLabels,\n                    num_classes: int,\n                    iou_thresh=0.5):\n    gt_geoms = [b.to_shapely() for b in gt_labels.get_boxes()]\n    gt_classes = gt_labels.get_class_ids()\n    pred_geoms = [b.to_shapely() for b in pred_labels.get_boxes()]\n    pred_classes = pred_labels.get_class_ids()\n\n    for pred, class_id in zip(pred_geoms, pred_classes):\n        pred.class_id = class_id\n    pred_tree = shapely.strtree.STRtree(pred_geoms)\n\n    def iou(a, b):\n        return a.intersection(b).area / a.union(b).area\n\n    def is_matched(geom):\n        return hasattr(geom, 'iou_matched')\n\n    tp = np.zeros((num_classes, ))\n    fp = np.zeros((num_classes, ))\n    fn = np.zeros((num_classes, ))\n\n    for gt, gt_class in zip(gt_geoms, gt_classes):\n        matches = list(\n            filter(lambda g: (not is_matched(g)) and g.class_id == gt_class,\n                   pred_tree.query(gt)))\n        scores = [iou(m, gt) for m in matches]\n        if len(scores) > 0:\n            max_ind = np.argmax(scores)\n            if scores[max_ind] > iou_thresh:\n                matches[max_ind].iou_matched = True\n                tp[gt_class] += 1\n            else:\n                fn[gt_class] += 1\n        else:\n            fn[gt_class] += 1\n\n    for class_id in range(num_classes):\n        pred_not_matched = np.array([not is_matched(g) for g in pred_geoms])\n        fp[class_id] = np.sum(pred_not_matched[pred_classes == class_id])\n\n    return tp, fp, fn\n\n\nclass ObjectDetectionEvaluation(ClassificationEvaluation):\n    def __init__(self, class_config):\n        super().__init__()\n        self.class_config = class_config\n\n    def compute(self, ground_truth_labels, prediction_labels):\n        self.class_to_eval_item = ObjectDetectionEvaluation.compute_eval_items(\n            ground_truth_labels, prediction_labels, self.class_config)\n        self.compute_avg()\n\n    @staticmethod\n    def compute_eval_items(gt_labels, pred_labels, class_config):\n        iou_thresh = 0.5\n        num_classes = len(class_config)\n        tps, fps, fns = compute_metrics(gt_labels, pred_labels, num_classes,\n                                        iou_thresh)\n        class_to_eval_item = {}\n\n        for class_id, (tp, fp, fn) in enumerate(zip(tps, fps, fns)):\n            gt_count = tp + fn\n            pred_count = tp + fp\n            class_name = class_config.get_name(class_id)\n\n            if gt_count == 0:\n                eval_item = ClassEvaluationItem(\n                    class_id=class_id, class_name=class_name)\n            elif pred_count == 0:\n                eval_item = ClassEvaluationItem(\n                    precision=None,\n                    recall=0,\n                    gt_count=gt_count,\n                    class_id=class_id,\n                    class_name=class_name)\n            else:\n                prec = tp / (tp + fp)\n                recall = tp / (tp + fn)\n                f1 = 0.\n                if prec + recall != 0.0:\n                    f1 = 2 * (prec * recall) / (prec + recall)\n                count_err = pred_count - gt_count\n                norm_count_err = None\n                if gt_count > 0:\n                    norm_count_err = count_err / gt_count\n\n                eval_item = ClassEvaluationItem(\n                    precision=prec,\n                    recall=recall,\n                    f1=f1,\n                    count_error=norm_count_err,\n                    gt_count=gt_count,\n                    class_id=class_id,\n                    class_name=class_name)\n\n            class_to_eval_item[class_id] = eval_item\n\n        return class_to_eval_item\n"""
rastervision2/core/evaluation/object_detection_evaluator.py,0,"b'from rastervision2.core.evaluation import (ClassificationEvaluator,\n                                           ObjectDetectionEvaluation)\n\n\nclass ObjectDetectionEvaluator(ClassificationEvaluator):\n    """"""Evaluates predictions for a set of scenes.\n    """"""\n\n    def __init__(self, class_config, output_uri):\n        super().__init__(class_config, output_uri)\n\n    def create_evaluation(self):\n        return ObjectDetectionEvaluation(self.class_config)\n'"
rastervision2/core/evaluation/object_detection_evaluator_config.py,0,"b""from rastervision2.pipeline.config import register_config\nfrom rastervision2.core.evaluation.classification_evaluator_config import (\n    ClassificationEvaluatorConfig)\nfrom rastervision2.core.evaluation.object_detection_evaluator import (\n    ObjectDetectionEvaluator)\n\n\n@register_config('object_detection_evaluator')\nclass ObjectDetectionEvaluatorConfig(ClassificationEvaluatorConfig):\n    def build(self, class_config):\n        return ObjectDetectionEvaluator(class_config, self.output_uri)\n"""
rastervision2/core/evaluation/semantic_segmentation_evaluation.py,0,"b'import math\nimport logging\nimport json\n\nfrom sklearn.metrics import confusion_matrix\nimport numpy as np\n\nfrom rastervision2.core.evaluation import ClassEvaluationItem\nfrom rastervision2.core.evaluation import ClassificationEvaluation\n\nlog = logging.getLogger(__name__)\n\n\ndef is_geojson(data):\n    if isinstance(data, dict):\n        return True\n    else:\n        try:\n            json.loads(data)\n            retval = True\n        except ValueError:\n            retval = False\n        return retval\n\n\ndef get_class_eval_item(conf_mat, class_id, class_name, null_class_id):\n    if conf_mat.ravel().sum() == 0:\n        return ClassEvaluationItem(None, None, None, 0, 0, class_id,\n                                   class_name)\n\n    non_null_class_ids = list(range(conf_mat.shape[0]))\n    non_null_class_ids.remove(null_class_id)\n\n    true_pos = conf_mat[class_id, class_id]\n    false_pos = conf_mat[non_null_class_ids, class_id].sum() - true_pos\n    false_neg = conf_mat[class_id, :].sum() - true_pos\n    precision = float(true_pos) / (true_pos + false_pos)\n    recall = float(true_pos) / (true_pos + false_neg)\n    f1 = 2 * (precision * recall) / (precision + recall)\n    count_error = int(false_pos + false_neg)\n    gt_count = conf_mat[class_id, :].sum()\n\n    if math.isnan(precision):\n        precision = None\n    else:\n        precision = float(precision)\n    if math.isnan(recall):\n        recall = None\n    else:\n        recall = float(recall)\n    if math.isnan(f1):\n        f1 = None\n    else:\n        f1 = float(f1)\n\n    return ClassEvaluationItem(precision, recall, f1, count_error, gt_count,\n                               class_id, class_name, conf_mat[class_id, :])\n\n\nclass SemanticSegmentationEvaluation(ClassificationEvaluation):\n    """"""Evaluation for semantic segmentation.""""""\n\n    def __init__(self, class_config):\n        super().__init__()\n        self.class_config = class_config\n\n    def compute(self, gt_labels, pred_labels):\n        self.clear()\n\n        labels = np.arange(len(self.class_config.names))\n        conf_mat = np.zeros((len(labels), len(labels)))\n        for window in pred_labels.get_windows():\n            log.debug(\'Evaluating window: {}\'.format(window))\n            gt_arr = gt_labels.get_label_arr(window).ravel()\n            pred_arr = pred_labels.get_label_arr(window).ravel()\n            conf_mat += confusion_matrix(gt_arr, pred_arr, labels=labels)\n\n        for class_id, class_name in enumerate(self.class_config.names):\n            if class_id != self.class_config.get_null_class_id():\n                class_name = self.class_config.names[class_id]\n                self.class_to_eval_item[class_id] = get_class_eval_item(\n                    conf_mat, class_id, class_name,\n                    self.class_config.get_null_class_id())\n\n        self.compute_avg()\n\n    def compute_vector(self, gt, pred, mode, class_id):\n        """"""Compute evaluation over vector predictions.\n            Args:\n                gt: Ground-truth GeoJSON.  Either a string (containing\n                    unparsed GeoJSON or a file name), or a dictionary\n                    containing parsed GeoJSON.\n                pred: GeoJSON for predictions.  Either a string\n                    (containing unparsed GeoJSON or a file name), or a\n                    dictionary containing parsed GeoJSON.\n                mode: A string containing either \'buildings\' or\n                    \'polygons\'.\n                class_id: An integer containing the class id of\n                    interest.\n        """"""\n        import mask_to_polygons.vectorification as vectorification\n        import mask_to_polygons.processing.score as score\n\n        # Ground truth as list of geometries\n        def get_geoms(x):\n            if is_geojson(x):\n                _x = x\n                if \'features\' in _x.keys():\n                    _x = _x[\'features\']\n                geoms = []\n                for feature in _x:\n                    if \'geometry\' in feature.keys():\n                        geoms.append(feature[\'geometry\'])\n                    else:\n                        geoms.append(feature)\n            else:\n                geoms = vectorification.geometries_from_geojson(x)\n\n            return geoms\n\n        gt = get_geoms(gt)\n        pred = get_geoms(pred)\n\n        if len(gt) > 0 and len(pred) > 0:\n            results = score.spacenet(pred, gt)\n\n            true_positives = results[\'tp\']\n            false_positives = results[\'fp\']\n            false_negatives = results[\'fn\']\n            precision = float(true_positives) / (\n                true_positives + false_positives)\n            recall = float(true_positives) / (true_positives + false_negatives)\n            if precision + recall != 0:\n                f1 = 2 * (precision * recall) / (precision + recall)\n            else:\n                f1 = 0.0\n            count_error = int(false_positives + false_negatives)\n            gt_count = len(gt)\n            class_name = \'vector-{}-{}\'.format(\n                mode,\n                self.class_config.names[class_id])\n\n            evaluation_item = ClassEvaluationItem(precision, recall, f1,\n                                                  count_error, gt_count,\n                                                  class_id, class_name)\n\n            if hasattr(self, \'class_to_eval_item\') and isinstance(\n                    self.class_to_eval_item, dict):\n                self.class_to_eval_item[class_id] = evaluation_item\n            else:\n                self.class_to_eval_item = {class_id: evaluation_item}\n            self.compute_avg()\n'"
rastervision2/core/evaluation/semantic_segmentation_evaluator.py,0,"b'import logging\n\nfrom shapely.geometry import shape, mapping\nfrom shapely.strtree import STRtree\n\nfrom rastervision2.core.data import ActivateMixin\nfrom rastervision2.core.data.vector_source import GeoJSONVectorSourceConfig\nfrom rastervision2.core.evaluation import (ClassificationEvaluator,\n                                           SemanticSegmentationEvaluation)\n\nlog = logging.getLogger(__name__)\n\n\ndef filter_geojson_by_aoi(geojson, aoi_polygons):\n    # Note that this ignores class_id but that\'s ok because each prediction GeoJSON file\n    # covers a single class_id. But, this may change in the future.\n    tree = STRtree([shape(f[\'geometry\']) for f in geojson[\'features\']])\n    filtered_shapes = []\n    for aoi_poly in aoi_polygons:\n        shapes_in_aoi = tree.query(aoi_poly)\n        for s in shapes_in_aoi:\n            s_int = s.intersection(aoi_poly)\n            filtered_shapes.append(s_int)\n\n    features = [{\n        \'type\': \'feature\',\n        \'geometry\': mapping(s)\n    } for s in filtered_shapes]\n\n    return {\'type\': \'FeatureCollection\', \'features\': features}\n\n\nclass SemanticSegmentationEvaluator(ClassificationEvaluator):\n    """"""Evaluates predictions for a set of scenes.\n    """"""\n\n    def __init__(self, class_config, output_uri, vector_output_uri):\n        super().__init__(class_config, output_uri)\n        self.vector_output_uri = vector_output_uri\n\n    def create_evaluation(self):\n        return SemanticSegmentationEvaluation(self.class_config)\n\n    def process(self, scenes, tmp_dir):\n        evaluation = self.create_evaluation()\n        vect_evaluation = self.create_evaluation()\n        null_class_id = self.class_config.get_null_class_id()\n\n        for scene in scenes:\n            log.info(\'Computing evaluation for scene {}...\'.format(scene.id))\n            label_source = scene.ground_truth_label_source\n            label_store = scene.prediction_label_store\n            with ActivateMixin.compose(label_source, label_store):\n                ground_truth = label_source.get_labels()\n                predictions = label_store.get_labels()\n\n                if scene.aoi_polygons:\n                    # Filter labels based on AOI.\n                    ground_truth = ground_truth.filter_by_aoi(\n                        scene.aoi_polygons, null_class_id)\n                    predictions = predictions.filter_by_aoi(\n                        scene.aoi_polygons, null_class_id)\n                scene_evaluation = self.create_evaluation()\n                scene_evaluation.compute(ground_truth, predictions)\n                evaluation.merge(scene_evaluation, scene_id=scene.id)\n\n            if hasattr(label_source, \'raster_source\') and hasattr(\n                    label_source.raster_source, \'vector_source\') and hasattr(\n                        label_store, \'vector_output\'):\n                gt_geojson = label_source.raster_source.vector_source.get_geojson(\n                )\n                for vo in label_store.vector_output:\n                    pred_geojson_uri = vo.uri\n                    mode = vo.get_mode()\n                    class_id = vo.class_id\n                    pred_geojson_source = GeoJSONVectorSourceConfig(\n                        uri=pred_geojson_uri, default_class_id=None).build(\n                        self.class_config,\n                        scene.raster_source.get_crs_transformer())\n                    pred_geojson = pred_geojson_source.get_geojson()\n\n                    if scene.aoi_polygons:\n                        gt_geojson = filter_geojson_by_aoi(\n                            gt_geojson, scene.aoi_polygons)\n                        pred_geojson = filter_geojson_by_aoi(\n                            pred_geojson, scene.aoi_polygons)\n\n                    vect_scene_evaluation = self.create_evaluation()\n                    vect_scene_evaluation.compute_vector(\n                        gt_geojson, pred_geojson, mode, class_id)\n                    vect_evaluation.merge(\n                        vect_scene_evaluation, scene_id=scene.id)\n\n        if not evaluation.is_empty():\n            evaluation.save(self.output_uri)\n        if not vect_evaluation.is_empty():\n            vect_evaluation.save(self.vector_output_uri)\n'"
rastervision2/core/evaluation/semantic_segmentation_evaluator_config.py,0,"b""from typing import Optional\nfrom os.path import join\n\nfrom rastervision2.pipeline.config import register_config, Field\nfrom rastervision2.core.evaluation.classification_evaluator_config import (\n    ClassificationEvaluatorConfig)\nfrom rastervision2.core.evaluation.semantic_segmentation_evaluator import (\n    SemanticSegmentationEvaluator)\n\n\n@register_config('semantic_segmentation_evaluator')\nclass SemanticSegmentationEvaluatorConfig(ClassificationEvaluatorConfig):\n    vector_output_uri: Optional[str] = Field(\n        None,\n        description=\n        ('URI of evaluation of vector output. If None, and this Config is part of '\n         'an RVPipeline, then this field will be auto-generated.'))\n\n    def build(self, class_config):\n        return SemanticSegmentationEvaluator(class_config, self.output_uri,\n                                             self.vector_output_uri)\n\n    def update(self, pipeline=None):\n        super().update(pipeline)\n\n        if pipeline is not None and self.vector_output_uri is None:\n            self.vector_output_uri = join(pipeline.eval_uri,\n                                          'vector-eval.json')\n"""
rastervision2/core/rv_pipeline/__init__.py,0,"b""# flake8: noqa\n\nTRAIN = 'train'\nVALIDATION = 'validation'\n\nfrom rastervision2.core.rv_pipeline.rv_pipeline import *\nfrom rastervision2.core.rv_pipeline.rv_pipeline_config import *\nfrom rastervision2.core.rv_pipeline.chip_classification import *\nfrom rastervision2.core.rv_pipeline.chip_classification_config import *\nfrom rastervision2.core.rv_pipeline.semantic_segmentation import *\nfrom rastervision2.core.rv_pipeline.semantic_segmentation_config import *\nfrom rastervision2.core.rv_pipeline.object_detection import *\nfrom rastervision2.core.rv_pipeline.object_detection_config import *\n"""
rastervision2/core/rv_pipeline/chip_classification.py,0,"b'import logging\n\nimport numpy as np\n\nfrom rastervision2.core.rv_pipeline.rv_pipeline import RVPipeline\nfrom rastervision2.core.box import Box\n\nlog = logging.getLogger(__name__)\n\n\ndef get_train_windows(scene, chip_size):\n    train_windows = []\n    extent = scene.raster_source.get_extent()\n    stride = chip_size\n    windows = extent.get_windows(chip_size, stride)\n    if scene.aoi_polygons:\n        windows = Box.filter_by_aoi(windows, scene.aoi_polygons)\n    for window in windows:\n        chip = scene.raster_source.get_chip(window)\n        if np.sum(chip.ravel()) > 0:\n            train_windows.append(window)\n    return train_windows\n\n\nclass ChipClassification(RVPipeline):\n    def get_train_windows(self, scene):\n        return get_train_windows(scene, self.config.train_chip_sz)\n\n    def get_train_labels(self, window, scene):\n        return scene.ground_truth_label_source.get_labels(window=window)\n'"
rastervision2/core/rv_pipeline/chip_classification_config.py,0,"b""from rastervision2.pipeline.config import register_config, ConfigError\nfrom rastervision2.core.rv_pipeline import RVPipelineConfig\nfrom rastervision2.core.data.label_store import (\n    ChipClassificationGeoJSONStoreConfig)\nfrom rastervision2.core.evaluation import ChipClassificationEvaluatorConfig\n\n\n@register_config('chip_classification')\nclass ChipClassificationConfig(RVPipelineConfig):\n    def build(self, tmp_dir):\n        from rastervision2.core.rv_pipeline.chip_classification import ChipClassification\n        return ChipClassification(self, tmp_dir)\n\n    def validate_config(self):\n        if self.train_chip_sz != self.predict_chip_sz:\n            raise ConfigError(\n                'train_chip_sz must be equal to predict_chip_sz for chip '\n                'classification.')\n\n    def get_default_label_store(self, scene):\n        return ChipClassificationGeoJSONStoreConfig()\n\n    def get_default_evaluator(self):\n        return ChipClassificationEvaluatorConfig()\n"""
rastervision2/core/rv_pipeline/object_detection.py,0,"b""import logging\n\nimport numpy as np\n\nfrom rastervision2.core.rv_pipeline.rv_pipeline import RVPipeline\nfrom rastervision2.core.rv_pipeline.object_detection_config import (\n    ObjectDetectionWindowMethod)\nfrom rastervision2.core.box import Box\nfrom rastervision2.core.data.label import ObjectDetectionLabels\n\nlog = logging.getLogger(__name__)\n\n\ndef _make_chip_pos_windows(image_extent, label_store, chip_size):\n    chip_size = chip_size\n    pos_windows = []\n    boxes = label_store.get_labels().get_boxes()\n    done_boxes = set()\n\n    # Get a random window around each box. If a box was previously included\n    # in a window, then it is skipped.\n    for box in boxes:\n        if box.tuple_format() not in done_boxes:\n            # If this  object is bigger than the chip,\n            # don't use this box.\n            if chip_size < box.get_width() or chip_size < box.get_height():\n                log.warning('Label is larger than chip size: {} '\n                            'Skipping this label'.format(box.tuple_format()))\n                continue\n\n            window = box.make_random_square_container(chip_size)\n            pos_windows.append(window)\n\n            # Get boxes that lie completely within window\n            window_boxes = label_store.get_labels(window=window)\n            window_boxes = ObjectDetectionLabels.get_overlapping(\n                window_boxes, window, ioa_thresh=1.0)\n            window_boxes = window_boxes.get_boxes()\n            window_boxes = [box.tuple_format() for box in window_boxes]\n            done_boxes.update(window_boxes)\n\n    return pos_windows\n\n\ndef _make_label_pos_windows(image_extent, label_store, label_buffer):\n    pos_windows = []\n    for box in label_store.get_labels().get_boxes():\n        window = box.make_buffer(label_buffer, image_extent)\n        pos_windows.append(window)\n\n    return pos_windows\n\n\ndef make_pos_windows(image_extent, label_store, chip_size, window_method,\n                     label_buffer):\n    if window_method == 'label':\n        return _make_label_pos_windows(image_extent, label_store, label_buffer)\n    elif window_method == 'image':\n        return [image_extent.make_copy()]\n    else:\n        return _make_chip_pos_windows(image_extent, label_store, chip_size)\n\n\ndef make_neg_windows(raster_source, label_store, chip_size, nb_windows,\n                     max_attempts, filter_windows):\n    extent = raster_source.get_extent()\n    neg_windows = []\n    for _ in range(max_attempts):\n        for _ in range(max_attempts):\n            window = extent.make_random_square(chip_size)\n            if any(filter_windows([window])):\n                break\n        chip = raster_source.get_chip(window)\n        labels = ObjectDetectionLabels.get_overlapping(\n            label_store.get_labels(), window, ioa_thresh=0.2)\n\n        # If no labels and not blank, append the chip\n        if len(labels) == 0 and np.sum(chip.ravel()) > 0:\n            neg_windows.append(window)\n\n        if len(neg_windows) == nb_windows:\n            break\n\n    return list(neg_windows)\n\n\ndef get_train_windows(scene, chip_opts, chip_size):\n    raster_source = scene.raster_source\n    label_store = scene.ground_truth_label_source\n\n    def filter_windows(windows):\n        if scene.aoi_polygons:\n            windows = Box.filter_by_aoi(windows, scene.aoi_polygons)\n        return windows\n\n    window_method = chip_opts.window_method\n    if window_method == ObjectDetectionWindowMethod.chip:\n        stride = chip_size\n        return list(\n            filter_windows((raster_source.get_extent().get_windows(\n                chip_size, stride))))\n\n    # Make positive windows which contain labels.\n    pos_windows = filter_windows(\n        make_pos_windows(raster_source.get_extent(), label_store, chip_size,\n                         chip_opts.window_method, chip_opts.label_buffer))\n    nb_pos_windows = len(pos_windows)\n\n    # Make negative windows which do not contain labels.\n    # Generate randow windows and save the ones that don't contain\n    # any labels. It may take many attempts to generate a single\n    # negative window, and could get into an infinite loop in some cases,\n    # so we cap the number of attempts.\n    if nb_pos_windows:\n        nb_neg_windows = round(chip_opts.neg_ratio * nb_pos_windows)\n    else:\n        nb_neg_windows = 100  # just make some\n    max_attempts = 100 * nb_neg_windows\n    neg_windows = make_neg_windows(raster_source, label_store, chip_size,\n                                   nb_neg_windows, max_attempts,\n                                   filter_windows)\n\n    return pos_windows + neg_windows\n\n\nclass ObjectDetection(RVPipeline):\n    def get_train_windows(self, scene):\n        return get_train_windows(scene, self.config.chip_options,\n                                 self.config.train_chip_sz)\n\n    def get_train_labels(self, window, scene):\n        window_labels = scene.ground_truth_label_source.get_labels(\n            window=window)\n        return ObjectDetectionLabels.get_overlapping(\n            window_labels,\n            window,\n            ioa_thresh=self.config.chip_options.ioa_thresh,\n            clip=True)\n\n    def get_predict_windows(self, extent):\n        chip_sz = self.config.train_chip_sz\n        stride = chip_sz // 2\n        return extent.get_windows(chip_sz, stride)\n\n    def post_process_predictions(self, labels, scene):\n        return ObjectDetectionLabels.prune_duplicates(\n            labels,\n            score_thresh=self.config.predict_options.score_thresh,\n            merge_thresh=self.config.predict_options.merge_thresh)\n"""
rastervision2/core/rv_pipeline/object_detection_config.py,0,"b""from enum import Enum\n\nfrom rastervision2.pipeline.config import register_config, Config\nfrom rastervision2.core.rv_pipeline import RVPipelineConfig\nfrom rastervision2.core.data.label_store import ObjectDetectionGeoJSONStoreConfig\nfrom rastervision2.core.evaluation import ObjectDetectionEvaluatorConfig\n\n\nclass ObjectDetectionWindowMethod(Enum):\n    chip = 1\n\n\n@register_config('object_detection_chip_options')\nclass ObjectDetectionChipOptions(Config):\n    neg_ratio: float = 1.0\n    ioa_thresh: float = 0.8\n    window_method: ObjectDetectionWindowMethod = ObjectDetectionWindowMethod.chip\n    label_buffer: float = 0.0\n\n\n@register_config('object_detection_predict_options')\nclass ObjectDetectionPredictOptions(Config):\n    merge_thresh: float = 0.5\n    score_thresh: float = 0.5\n\n\n@register_config('object_detection')\nclass ObjectDetectionConfig(RVPipelineConfig):\n    chip_options: ObjectDetectionChipOptions = ObjectDetectionChipOptions()\n    predict_options: ObjectDetectionPredictOptions = ObjectDetectionPredictOptions(\n    )\n\n    def build(self, tmp_dir):\n        from rastervision2.core.rv_pipeline.object_detection import ObjectDetection\n        return ObjectDetection(self, tmp_dir)\n\n    def get_default_label_store(self, scene):\n        return ObjectDetectionGeoJSONStoreConfig()\n\n    def get_default_evaluator(self):\n        return ObjectDetectionEvaluatorConfig()\n"""
rastervision2/core/rv_pipeline/rv_pipeline.py,0,"b'import logging\nfrom os.path import join\nimport tempfile\nfrom typing import TYPE_CHECKING, Optional, List\n\nimport numpy as np\n\nfrom rastervision2.pipeline.pipeline import Pipeline\nfrom rastervision2.core.box import Box\nfrom rastervision2.core.data_sample import DataSample\nfrom rastervision2.core.data import Scene, Labels\nfrom rastervision2.core.backend import Backend\nfrom rastervision2.core.rv_pipeline import TRAIN, VALIDATION\nfrom rastervision2.pipeline.file_system.utils import (\n    download_or_copy, zipdir, get_local_path, upload_or_copy)\n\nlog = logging.getLogger(__name__)\n\nif TYPE_CHECKING:\n    from rastervision2.core.rv_pipeline.rv_pipeline_config import RVPipelineConfig  # noqa\n\n\nclass RVPipeline(Pipeline):\n    """"""Base class of all Raster Vision Pipelines.\n\n    This can be subclassed to implement Pipelines for different computer vision tasks\n    over geospatial imagery. The commands and what they produce include:\n        - analyze: metrics on the imagery and labels\n        - chip: small training and validation images taken from larger scenes\n        - train: model trained on chips\n        - predict: predictions over entire validation and test scenes\n        - eval: evaluation metrics for predictions generated by model\n        - bundle: bundle containing model and any other files needed to make predictions\n            using the Predictor\n    """"""\n    commands = [\'analyze\', \'chip\', \'train\', \'predict\', \'eval\', \'bundle\']\n    split_commands = [\'chip\', \'predict\']\n    gpu_commands = [\'train\', \'predict\']\n\n    def __init__(self, config: \'RVPipelineConfig\', tmp_dir: str):\n        super().__init__(config, tmp_dir)\n        self.backend: Optional[\'Backend\'] = None\n        self.config: \'RVPipelineConfig\'\n\n    def analyze(self):\n        """"""Run each analyzer over training scenes.""""""\n        class_config = self.config.dataset.class_config\n        scenes = [\n            s.build(class_config, self.tmp_dir, use_transformers=False)\n            for s in self.config.dataset.train_scenes\n        ]\n        analyzers = [a.build() for a in self.config.analyzers]\n        for analyzer in analyzers:\n            log.info(\'Running analyzers: {}...\'.format(\n                type(analyzer).__name__))\n            analyzer.process(scenes, self.tmp_dir)\n\n    def get_train_windows(self, scene: Scene) -> List[Box]:\n        """"""Return the training windows for a Scene.\n\n        Each training window represents the spatial extent of a training chip to\n        generate.\n\n        Args:\n            scene: Scene to generate windows for\n        """"""\n        raise NotImplementedError()\n\n    def get_train_labels(self, window: Box, scene: Scene) -> Labels:\n        """"""Return the training labels in a window for a scene.\n\n        Returns:\n            Labels that lie within window\n        """"""\n        raise NotImplementedError()\n\n    def chip(self, split_ind: int = 0, num_splits: int = 1):\n        """"""Save training and validation chips.""""""\n        cfg = self.config\n        backend = cfg.backend.build(cfg, self.tmp_dir)\n        dataset = cfg.dataset.get_split_config(split_ind, num_splits)\n        if not dataset.train_scenes and not dataset.validation_scenes:\n            return\n\n        class_cfg = dataset.class_config\n        with backend.get_sample_writer() as writer:\n\n            def chip_scene(scene, split):\n                with scene.activate():\n                    log.info(\'Making {} chips for scene: {}\'.format(\n                        split, scene.id))\n                    windows = self.get_train_windows(scene)\n                    for window in windows:\n                        chip = scene.raster_source.get_chip(window)\n                        labels = self.get_train_labels(window, scene)\n                        sample = DataSample(\n                            chip=chip,\n                            window=window,\n                            labels=labels,\n                            scene_id=str(scene.id),\n                            is_train=split == TRAIN)\n                        sample = self.post_process_sample(sample)\n                        writer.write_sample(sample)\n\n            for s in dataset.train_scenes:\n                chip_scene(s.build(class_cfg, self.tmp_dir), TRAIN)\n            for s in dataset.validation_scenes:\n                chip_scene(s.build(class_cfg, self.tmp_dir), VALIDATION)\n\n    def train(self):\n        """"""Train a model and save it.""""""\n        backend = self.config.backend.build(self.config, self.tmp_dir)\n        backend.train()\n\n    def post_process_sample(self, sample: DataSample) -> DataSample:\n        """"""Post-process sample in pipeline-specific way.\n\n        This should be called before writing a sample during chipping.\n        """"""\n        return sample\n\n    def post_process_batch(self, windows: List[Box], chips: np.ndarray,\n                           labels: Labels) -> Labels:\n        """"""Post-process a batch of predictions.""""""\n        return labels\n\n    def post_process_predictions(self, labels: Labels, scene: Scene) -> Labels:\n        """"""Post-process all labels at end of prediction.""""""\n        return labels\n\n    def get_predict_windows(self, extent: Box) -> List[Box]:\n        """"""Returns windows to compute predictions for.\n\n        Args:\n            extent: extent of RasterSource\n        """"""\n        chip_sz = stride = self.config.predict_chip_sz\n        return extent.get_windows(chip_sz, stride)\n\n    def predict(self, split_ind=0, num_splits=1):\n        """"""Make predictions over each validation and test scene.\n\n        This uses a sliding window.\n        """"""\n        # Cache backend so subsquent calls will be faster. This is useful for\n        # the predictor.\n        if self.backend is None:\n            self.backend = self.config.backend.build(self.config, self.tmp_dir)\n            self.backend.load_model()\n\n        class_config = self.config.dataset.class_config\n        dataset = self.config.dataset.get_split_config(split_ind, num_splits)\n\n        def _predict(scenes):\n            for scene in scenes:\n                with scene.activate():\n                    labels = self.predict_scene(scene, self.backend)\n                    label_store = scene.prediction_label_store\n                    label_store.save(labels)\n\n        _predict([\n            s.build(class_config, self.tmp_dir)\n            for s in dataset.validation_scenes\n        ])\n        if dataset.test_scenes:\n            _predict([\n                s.build(class_config, self.tmp_dir)\n                for s in dataset.test_scenes\n            ])\n\n    def predict_scene(self, scene: Scene, backend: Backend) -> Labels:\n        """"""Returns predictions for a single scene.""""""\n        log.info(\'Making predictions for scene\')\n        raster_source = scene.raster_source\n        label_store = scene.prediction_label_store\n        labels = label_store.empty_labels()\n\n        windows = self.get_predict_windows(raster_source.get_extent())\n\n        def predict_batch(chips, windows):\n            nonlocal labels\n            chips = np.array(chips)\n            batch_labels = backend.predict(chips, windows)\n            batch_labels = self.post_process_batch(windows, chips,\n                                                   batch_labels)\n            labels += batch_labels\n\n            print(\'.\' * len(chips), end=\'\')\n\n        batch_chips, batch_windows = [], []\n        for window in windows:\n            chip = raster_source.get_chip(window)\n            if np.any(chip):\n                batch_chips.append(chip)\n                batch_windows.append(window)\n\n            # Predict on batch\n            if len(batch_chips) >= self.config.predict_batch_sz:\n                predict_batch(batch_chips, batch_windows)\n                batch_chips, batch_windows = [], []\n        print()\n\n        # Predict on remaining batch\n        if len(batch_chips) > 0:\n            predict_batch(batch_chips, batch_windows)\n\n        return self.post_process_predictions(labels, scene)\n\n    def eval(self):\n        """"""Evaluate predictions against ground truth.""""""\n        class_config = self.config.dataset.class_config\n        scenes = [\n            s.build(class_config, self.tmp_dir)\n            for s in self.config.dataset.validation_scenes\n        ]\n        evaluators = [e.build(class_config) for e in self.config.evaluators]\n        for evaluator in evaluators:\n            log.info(\'Running evaluator: {}...\'.format(\n                type(evaluator).__name__))\n            evaluator.process(scenes, self.tmp_dir)\n\n    def bundle(self):\n        """"""Save a model bundle with whatever is needed to make predictions.\n\n        The model bundle is a zip file and it is used by the Predictor and\n        predict CLI subcommand.\n        """"""\n        with tempfile.TemporaryDirectory(dir=self.tmp_dir) as tmp_dir:\n            for fn in self.config.backend.get_bundle_filenames():\n                download_or_copy(\n                    join(self.config.train_uri, fn), join(tmp_dir, fn))\n\n            for a in self.config.analyzers:\n                for fn in a.get_bundle_filenames():\n                    download_or_copy(\n                        join(self.config.analyze_uri, fn), join(tmp_dir, fn))\n\n            download_or_copy(self.config.get_config_uri(),\n                             join(tmp_dir, \'pipeline-config.json\'))\n\n            model_bundle_uri = self.config.get_model_bundle_uri()\n            model_bundle_path = get_local_path(model_bundle_uri, self.tmp_dir)\n            zipdir(tmp_dir, model_bundle_path)\n            upload_or_copy(model_bundle_path, model_bundle_uri)\n'"
rastervision2/core/rv_pipeline/rv_pipeline_config.py,0,"b'from os.path import join\nfrom typing import List, TYPE_CHECKING, Optional\n\nfrom rastervision2.pipeline.pipeline_config import PipelineConfig\nfrom rastervision2.core.data import (DatasetConfig, StatsTransformerConfig,\n                                     LabelStoreConfig, SceneConfig)\nfrom rastervision2.core.analyzer import StatsAnalyzerConfig\nfrom rastervision2.core.backend import BackendConfig\nfrom rastervision2.core.evaluation import EvaluatorConfig\nfrom rastervision2.core.analyzer import AnalyzerConfig\nfrom rastervision2.pipeline.config import register_config, Field\n\nif TYPE_CHECKING:\n    from rastervision2.core.backend.backend import Backend  # noqa\n\n\n@register_config(\'rv_pipeline\')\nclass RVPipelineConfig(PipelineConfig):\n    """"""Config for RVPipeline.""""""\n    dataset: DatasetConfig = Field(\n        ...,\n        description=\n        \'Dataset containing train, validation, and optional test scenes.\')\n    backend: BackendConfig = Field(\n        ..., description=\'Backend to use for interfacing with ML library.\')\n    evaluators: List[EvaluatorConfig] = Field(\n        [],\n        description=(\n            \'Evaluators to run during analyzer command. If list is empty \'\n            \'the default evaluator is added.\'))\n    analyzers: List[AnalyzerConfig] = Field(\n        [],\n        description=\n        (\'Analyzers to run during analyzer command. A StatsAnalyzer will be added \'\n         \'automatically if any scenes have a RasterTransformer.\'))\n\n    train_chip_sz: int = Field(\n        300, description=\'Size of training chips in pixels.\')\n    predict_chip_sz: int = Field(\n        300, description=\'Size of predictions chips in pixels.\')\n    predict_batch_sz: int = Field(\n        8, description=\'Batch size to use during prediction.\')\n\n    analyze_uri: Optional[str] = Field(\n        None,\n        description=\n        \'URI for output of analyze. If None, will be auto-generated.\')\n    chip_uri: Optional[str] = Field(\n        None,\n        description=\'URI for output of chip. If None, will be auto-generated.\')\n    train_uri: Optional[str] = Field(\n        None,\n        description=\'URI for output of train. If None, will be auto-generated.\'\n    )\n    predict_uri: Optional[str] = Field(\n        None,\n        description=\n        \'URI for output of predict. If None, will be auto-generated.\')\n    eval_uri: Optional[str] = Field(\n        None,\n        description=\'URI for output of eval. If None, will be auto-generated.\')\n    bundle_uri: Optional[str] = Field(\n        None,\n        description=\'URI for output of bundle. If None, will be auto-generated.\'\n    )\n\n    def update(self):\n        super().update()\n\n        if self.analyze_uri is None:\n            self.analyze_uri = join(self.root_uri, \'analyze\')\n        if self.chip_uri is None:\n            self.chip_uri = join(self.root_uri, \'chip\')\n        if self.train_uri is None:\n            self.train_uri = join(self.root_uri, \'train\')\n        if self.predict_uri is None:\n            self.predict_uri = join(self.root_uri, \'predict\')\n        if self.eval_uri is None:\n            self.eval_uri = join(self.root_uri, \'eval\')\n        if self.bundle_uri is None:\n            self.bundle_uri = join(self.root_uri, \'bundle\')\n\n        self.dataset.update(pipeline=self)\n        self.backend.update(pipeline=self)\n        if not self.evaluators:\n            self.evaluators.append(self.get_default_evaluator())\n        for evaluator in self.evaluators:\n            evaluator.update(pipeline=self)\n\n        self._insert_analyzers()\n        for analyzer in self.analyzers:\n            analyzer.update(pipeline=self)\n\n    def get_model_bundle_uri(self):\n        return join(self.bundle_uri, \'model-bundle.zip\')\n\n    def _insert_analyzers(self):\n        # Inserts StatsAnalyzer if it\'s needed because a RasterSource has a\n        # StatsTransformer, but there isn\'t a StatsAnalyzer in the list of Analyzers.\n        has_stats_transformer = False\n        for s in self.dataset.get_all_scenes():\n            for t in s.raster_source.transformers:\n                if isinstance(t, StatsTransformerConfig):\n                    has_stats_transformer = True\n\n        has_stats_analyzer = False\n        for a in self.analyzers:\n            if isinstance(a, StatsAnalyzerConfig):\n                has_stats_analyzer = True\n                break\n\n        if has_stats_transformer and not has_stats_analyzer:\n            self.analyzers.append(StatsAnalyzerConfig())\n\n    def get_default_label_store(self, scene: SceneConfig) -> LabelStoreConfig:\n        """"""Returns a default LabelStoreConfig to fill in any missing ones.""""""\n        raise NotImplementedError()\n\n    def get_default_evaluator(self) -> EvaluatorConfig:\n        """"""Returns a default EvaluatorConfig to use if one isn\'t set.""""""\n        raise NotImplementedError()\n'"
rastervision2/core/rv_pipeline/semantic_segmentation.py,0,"b'import logging\nfrom typing import List\n\nimport numpy as np\n\nfrom rastervision2.core.rv_pipeline.rv_pipeline import RVPipeline\nfrom rastervision2.core.box import Box\nfrom rastervision2.core.rv_pipeline.semantic_segmentation_config import (\n    SemanticSegmentationWindowMethod)\n\nlog = logging.getLogger(__name__)\n\n\ndef get_train_windows(scene, class_config, chip_size,\n                      chip_options) -> List[Box]:\n    """"""Get training windows covering a scene.\n\n    Args:\n        scene: The scene over-which windows are to be generated.\n\n    Returns:\n        A list of windows, list(Box)\n    """"""\n    co = chip_options\n    raster_source = scene.raster_source\n    extent = raster_source.get_extent()\n    label_source = scene.ground_truth_label_source\n\n    def filter_windows(windows):\n        if scene.aoi_polygons:\n            windows = Box.filter_by_aoi(windows, scene.aoi_polygons)\n\n            filt_windows = []\n            for w in windows:\n                label_arr = label_source.get_labels(w).get_label_arr(w)\n                null_inds = (\n                    label_arr.ravel() == class_config.get_null_class_id())\n                if not np.all(null_inds):\n                    filt_windows.append(w)\n            windows = filt_windows\n        return windows\n\n    if co.window_method == SemanticSegmentationWindowMethod.sliding:\n        stride = co.stride or int(round(chip_size / 2))\n        windows = list(filter_windows((extent.get_windows(chip_size, stride))))\n    elif co.window_method == SemanticSegmentationWindowMethod.random_sample:\n        target_class_ids = co.target_class_ids or list(\n            range(len(class_config)))\n        windows = []\n        attempts = 0\n\n        while attempts < co.chips_per_scene:\n            window = extent.make_random_square(chip_size)\n            if not filter_windows([window]):\n                continue\n\n            attempts += 1\n            if co.negative_survival_prob >= 1.0:\n                windows.append(window)\n            elif attempts == co.chips_per_scene and len(windows) == 0:\n                # Ensure there is at least one window per scene.\n                windows.append(window)\n            else:\n                is_pos = label_source.enough_target_pixels(\n                    window, co.target_count_threshold, target_class_ids)\n                if is_pos or (np.random.rand() < co.negative_survival_prob):\n                    windows.append(window)\n\n    return windows\n\n\ndef fill_no_data(img, label_arr, null_class_id):\n    # If chip has null labels, fill in those pixels with\n    # nodata.\n    null_inds = label_arr.ravel() == null_class_id\n    img_shape = img.shape\n    if np.any(null_inds):\n        img = np.reshape(img, (-1, img_shape[2]))\n        img[null_inds, :] = 0\n        img = np.reshape(img, img_shape)\n    return img\n\n\nclass SemanticSegmentation(RVPipeline):\n    def get_train_windows(self, scene):\n        return get_train_windows(scene, self.config.dataset.class_config,\n                                 self.config.train_chip_sz,\n                                 self.config.chip_options)\n\n    def get_train_labels(self, window, scene):\n        return scene.ground_truth_label_source.get_labels(window=window)\n\n    def post_process_sample(self, sample):\n        # Use null label for each pixel with NODATA.\n        img = sample.chip\n        label_arr = sample.labels.get_label_arr(sample.window)\n        null_class_id = self.config.dataset.class_config.get_null_class_id()\n        sample.chip = fill_no_data(img, label_arr, null_class_id)\n        return sample\n\n    def post_process_batch(self, windows, chips, labels):\n        # Fill in null class for any NODATA pixels.\n        null_class_id = self.config.dataset.class_config.get_null_class_id()\n        for window, chip in zip(windows, chips):\n            label_arr = labels.get_label_arr(window)\n            label_arr[np.sum(chip, axis=2) == 0] = null_class_id\n            labels.set_label_arr(window, label_arr)\n\n        return labels\n'"
rastervision2/core/rv_pipeline/semantic_segmentation_config.py,0,"b'from typing import (List, Optional)\nfrom enum import Enum\n\nfrom rastervision2.pipeline.config import register_config, Config, Field\nfrom rastervision2.core.rv_pipeline import RVPipelineConfig\nfrom rastervision2.core.data import SemanticSegmentationLabelStoreConfig\nfrom rastervision2.core.evaluation import SemanticSegmentationEvaluatorConfig\n\n\nclass SemanticSegmentationWindowMethod(Enum):\n    sliding = 1\n    random_sample = 2\n\n\n@register_config(\'semantic_segmentation_chip_options\')\nclass SemanticSegmentationChipOptions(Config):\n    """"""Chipping options for semantic segmentation.""""""\n    window_method: SemanticSegmentationWindowMethod = Field(\n        SemanticSegmentationWindowMethod.sliding,\n        description=(\'Window method to use for chipping.\'))\n    target_class_ids: Optional[List[int]] = Field(\n        None,\n        description=\n        (\'List of class ids considered as targets (ie. those to prioritize when \'\n         \'creating chips) which is only used in conjunction with the \'\n         \'target_count_threshold and negative_survival_probability options. Applies \'\n         \'to the random_sample window method.\'))\n    negative_survival_prob: float = Field(\n        1.0,\n        description=\n        (\'List of class ids considered as targets (ie. those to prioritize when creating \'\n         \'chips) which is only used in conjunction with the target_count_threshold and \'\n         \'negative_survival_probability options. Applies to the random_sample window \'\n         \'method.\'))\n    chips_per_scene: int = Field(\n        1000,\n        description=\n        (\'Number of chips to generate per scene. Applies to the random_sample window \'\n         \'method.\'))\n    target_count_threshold: int = Field(\n        1000,\n        description=\n        (\'Minimum number of pixels covering target_classes that a chip must have. \'\n         \'Applies to the random_sample window method.\'))\n    stride: Optional[int] = Field(\n        None,\n        description=\n        (\'Stride of windows across image. Defaults to half the chip size. Applies to \'\n         \'the sliding_window method.\'))\n\n\n@register_config(\'semantic_segmentation\')\nclass SemanticSegmentationConfig(RVPipelineConfig):\n    chip_options: SemanticSegmentationChipOptions = SemanticSegmentationChipOptions(\n    )\n\n    def build(self, tmp_dir):\n        from rastervision2.core.rv_pipeline.semantic_segmentation import (\n            SemanticSegmentation)\n        return SemanticSegmentation(self, tmp_dir)\n\n    def get_default_label_store(self, scene):\n        return SemanticSegmentationLabelStoreConfig()\n\n    def get_default_evaluator(self):\n        return SemanticSegmentationEvaluatorConfig()\n\n    def update(self):\n        super().update()\n\n        self.dataset.class_config.ensure_null_class()\n'"
rastervision2/core/utils/__init__.py,0,b''
rastervision2/core/utils/cog.py,0,"b'import os\nfrom subprocess import Popen\n\nfrom rastervision2.pipeline.file_system import (download_or_copy,\n                                                upload_or_copy)\n\nCOGIFY = \'COGIFY\'\n\nDEFAULT_BLOCK_SIZE = 512\nDEFAULT_RESAMPLE_METHOD = \'near\'\nDEFAULT_COMPRESSION = \'deflate\'\nDEFAULT_OVERVIEWS = [2, 4, 8, 16, 32]\n\n\ndef gdal_cog_commands(input_path,\n                      tmp_dir,\n                      block_size=DEFAULT_BLOCK_SIZE,\n                      resample_method=DEFAULT_RESAMPLE_METHOD,\n                      compression=DEFAULT_COMPRESSION,\n                      overviews=None):\n    """"""\n    GDAL commands to create a COG from an input file.\n    Returns a tuple (commands, output_path)\n    """"""\n\n    if not overviews:\n        overviews = DEFAULT_OVERVIEWS\n\n    def get_output_path(command):\n        fname = os.path.splitext(os.path.basename(input_path))[0]\n        return os.path.join(tmp_dir, \'{}-{}.tif\'.format(fname, command))\n\n    compression = compression.lower()\n\n    def add_compression(cmd, overview=False):\n        if compression != \'none\':\n            if not overview:\n                return cmd[:1] + [\'-co\', \'compress={}\'.format(compression)\n                                  ] + cmd[1:]\n            else:\n                return cmd[:1] + [\n                    \'--config\', \'COMPRESS_OVERVIEW\', compression\n                ] + cmd[1:]\n        else:\n            return cmd\n\n    # Step 1: Translate to a GeoTiff.\n    translate_path = get_output_path(\'translate\')\n    translate = add_compression([\n        \'gdal_translate\', \'-of\', \'GTiff\', \'-co\', \'tiled=YES\', \'-co\',\n        \'BIGTIFF=IF_SAFER\', input_path, translate_path\n    ])\n\n    # Step 2: Add overviews\n    add_overviews = add_compression(\n        [\'gdaladdo\', \'-r\', resample_method, translate_path] + list(\n            map(lambda x: str(x), overviews)),\n        overview=True)\n\n    # Step 3: Translate to COG\n    output_path = get_output_path(\'cog\')\n\n    create_cog = add_compression([\n        \'gdal_translate\', \'-co\', \'TILED=YES\', \'-co\', \'COPY_SRC_OVERVIEWS=YES\',\n        \'-co\', \'BLOCKXSIZE={}\'.format(block_size), \'-co\',\n        \'BLOCKYSIZE={}\'.format(block_size), \'-co\', \'BIGTIFF=IF_SAFER\',\n        \'--config\', \'GDAL_TIFF_OVR_BLOCKSIZE\',\n        str(block_size), translate_path, output_path\n    ])\n\n    return ([translate, add_overviews, create_cog], output_path)\n\n\ndef run_cmd(cmd):\n    p = Popen(cmd)\n    (out, err) = p.communicate(input)\n    if p.returncode != 0:\n        s = \'Command failed:\\n\'\n        s += \' \'.join(cmd) + \'\\n\\n\'\n        if out:\n            s += out + \'\\n\\n\'\n        if err:\n            s += err\n        raise Exception(s)\n\n\ndef create_cog(source_uri,\n               dest_uri,\n               local_dir,\n               block_size=DEFAULT_BLOCK_SIZE,\n               resample_method=DEFAULT_RESAMPLE_METHOD,\n               compression=DEFAULT_COMPRESSION,\n               overviews=None):\n    local_path = download_or_copy(source_uri, local_dir)\n\n    commands, output_path = gdal_cog_commands(\n        local_path,\n        local_dir,\n        block_size=block_size,\n        resample_method=resample_method,\n        compression=compression,\n        overviews=overviews)\n    for command in commands:\n        run_cmd(command)\n\n    upload_or_copy(output_path, dest_uri)\n'"
rastervision2/core/utils/filter_geojson.py,0,"b'import json\nimport copy\n\nimport click\n\nfrom rastervision2.pipeline.file_system import file_to_str, str_to_file\n\n\n@click.command()\n@click.argument(\'labels_uri\')\n@click.argument(\'output_uri\')\n@click.argument(\'class_names\', nargs=-1)\ndef filter_geojson(labels_uri, output_uri, class_names):\n    """"""Remove features that aren\'t in class_names and remove class_ids.""""""\n    labels_str = file_to_str(labels_uri)\n    labels = json.loads(labels_str)\n    filtered_features = []\n\n    for feature in labels[\'features\']:\n        feature = copy.deepcopy(feature)\n        properties = feature.get(\'properties\')\n        if properties:\n            class_name = properties.get(\'class_name\') or properties(\'label\')\n            if class_name in class_names:\n                del properties[\'class_id\']\n                filtered_features.append(feature)\n\n    new_labels = {\'features\': filtered_features}\n    str_to_file(json.dumps(new_labels), output_uri)\n\n\nif __name__ == \'__main__\':\n    filter_geojson()\n'"
rastervision2/core/utils/misc.py,0,"b'import io\n\nfrom PIL import Image\nimport numpy as np\nimport imageio\nimport logging\n\nlog = logging.getLogger(__name__)\n\n\ndef save_img(im_array, output_path):\n    imageio.imwrite(output_path, im_array)\n\n\ndef numpy_to_png(array: np.ndarray) -> str:\n    """"""Get a PNG string from a Numpy array.\n\n    Args:\n         array: A Numpy array of shape (w, h, 3) or (w, h), where the\n               former is meant to become a three-channel image and the\n               latter a one-channel image.  The dtype of the array\n               should be uint8.\n\n    Returns:\n         str\n\n    """"""\n    im = Image.fromarray(array)\n    output = io.BytesIO()\n    im.save(output, \'png\')\n    return output.getvalue()\n\n\ndef png_to_numpy(png: str, dtype=np.uint8) -> np.ndarray:\n    """"""Get a Numpy array from a PNG string.\n\n    Args:\n         png: A str containing a PNG-formatted image.\n\n    Returns:\n         numpy.ndarray\n\n    """"""\n    incoming = io.BytesIO(png)\n    im = Image.open(incoming)\n    return np.array(im)\n'"
rastervision2/core/utils/stac.py,0,"b""from urllib.parse import urlparse\n\nimport boto3\nfrom pystac import STAC_IO\nfrom pystac import Catalog, LabelItem\nfrom shapely.geometry import box\n\n\ndef setup_stac_s3():\n    def my_read_method(uri):\n        parsed = urlparse(uri)\n        if parsed.scheme == 's3':\n            bucket = parsed.netloc\n            key = parsed.path[1:]\n            s3 = boto3.resource('s3')\n            obj = s3.Object(bucket, key)\n            return obj.get()['Body'].read().decode('utf-8')\n        else:\n            return STAC_IO.default_read_text_method(uri)\n\n    def my_write_method(uri, txt):\n        parsed = urlparse(uri)\n        if parsed.scheme == 's3':\n            bucket = parsed.netloc\n            key = parsed.path[1:]\n            s3 = boto3.resource('s3')\n            s3.Object(bucket, key).put(Body=txt)\n        else:\n            STAC_IO.default_write_text_method(uri, txt)\n\n    STAC_IO.read_text_method = my_read_method\n    STAC_IO.write_text_method = my_write_method\n\n\ndef parse_stac(stac_uri):\n    setup_stac_s3()\n    cat = Catalog.from_file(stac_uri)\n    cat.make_all_asset_hrefs_absolute()\n    labels_uri = None\n    geotiff_uris = []\n    for item in cat.get_all_items():\n        if isinstance(item, LabelItem):\n            labels_uri = list(item.assets.values())[0].href\n            labels_box = box(*item.bbox)\n\n    # only use geotiffs that intersect with bbox of labels\n    for item in cat.get_all_items():\n        if not isinstance(item, LabelItem):\n            geotiff_uri = list(item.assets.values())[0].href\n            geotiff_box = box(*item.bbox)\n            if labels_box.intersects(geotiff_box):\n                geotiff_uri = geotiff_uri.replace('%7C', '|')\n                geotiff_uris.append(geotiff_uri)\n\n    if not labels_uri:\n        raise ValueError('Unable to read labels URI from STAC.')\n    if not geotiff_uris:\n        raise ValueError('Unable to read GeoTIFF URIs from STAC.')\n    return labels_uri, labels_box, geotiff_uris\n"""
rastervision2/core/utils/zxy2geotiff.py,0,"b'import tempfile\n\nfrom PIL import Image\nimport numpy as np\nimport click\nimport mercantile\nimport rasterio\nfrom rasterio.windows import Window\nimport pyproj\n\nfrom rastervision2.pipeline.file_system import (download_if_needed,\n                                                get_local_path, upload_or_copy)\nfrom rastervision2.core.utils.cog import create_cog\n\n\ndef lnglat2merc(lng, lat):\n    """"""Convert lng, lat point to x/y Web Mercator tuple.""""""\n    return pyproj.transform(\n        pyproj.Proj(init=\'epsg:4326\'), pyproj.Proj(init=\'epsg:3857\'), lng, lat)\n\n\ndef merc2lnglat(x, y):\n    """"""Convert x, y Web Mercator point to lng/lat tuple.""""""\n    return pyproj.transform(\n        pyproj.Proj(init=\'epsg:3857\'), pyproj.Proj(init=\'epsg:4326\'), x, y)\n\n\ndef merc2pixel(tile_x, tile_y, zoom, merc_x, merc_y, tile_sz=256):\n    """"""Convert Web Mercator point to pixel coordinates.\n\n    This is within the coordinate frame of a single ZXY tile.\n\n    Args:\n        tile_x: (int) x coordinate of ZXY tile\n        tile_y: (int) y coordinate of ZXY tile\n        zoom: (int) zoom level of ZXY tile\n        merc_x: (float) Web Mercator x axis of point\n        merc_y: (float) Web Mercator y axis of point\n        tile_sz: (int) size of ZXY tile\n    """"""\n    tile_merc_bounds = mercantile.xy_bounds(tile_x, tile_y, zoom)\n    pix_y = int(\n        round(tile_sz * ((tile_merc_bounds.top - merc_y) /\n                         (tile_merc_bounds.top - tile_merc_bounds.bottom))))\n    pix_x = int(\n        round(tile_sz * ((merc_x - tile_merc_bounds.left) /\n                         (tile_merc_bounds.right - tile_merc_bounds.left))))\n    return (pix_x, pix_y)\n\n\ndef _zxy2geotiff(tile_schema, zoom, bounds, output_uri, make_cog=False):\n    """"""Generates a GeoTIFF of a bounded region from a ZXY tile server.\n\n    Args:\n        tile_schema: (str) the URI schema for zxy tiles (ie. a slippy map tile server)\n            of the form /tileserver-uri/{z}/{x}/{y}.png. If {-y} is used, the tiles\n            are assumed to be indexed using TMS coordinates, where the y axis starts\n            at the southernmost point. The URI can be for http, S3, or the local\n            file system.\n        zoom: (int) the zoom level to use when retrieving tiles\n        bounds: (list) a list of length 4 containing min_lat, min_lng,\n            max_lat, max_lng\n        output_uri: (str) where to save the GeoTIFF. The URI can be for http, S3, or the\n            local file system\n    """"""\n    min_lat, min_lng, max_lat, max_lng = bounds\n    if min_lat >= max_lat:\n        raise ValueError(\'min_lat must be < max_lat\')\n    if min_lng >= max_lng:\n        raise ValueError(\'min_lng must be < max_lng\')\n\n    is_tms = False\n    if \'{-y}\' in tile_schema:\n        tile_schema = tile_schema.replace(\'{-y}\', \'{y}\')\n        is_tms = True\n\n    tmp_dir_obj = tempfile.TemporaryDirectory()\n    tmp_dir = tmp_dir_obj.name\n\n    # Get range of tiles that cover bounds.\n    output_path = get_local_path(output_uri, tmp_dir)\n    tile_sz = 256\n    t = mercantile.tile(min_lng, max_lat, zoom)\n    xmin, ymin = t.x, t.y\n    t = mercantile.tile(max_lng, min_lat, zoom)\n    xmax, ymax = t.x, t.y\n\n    # The supplied bounds are contained within the ""tile bounds"" -- ie. the\n    # bounds of the set of tiles that covers the supplied bounds. Therefore,\n    # we need to crop out the imagery that lies within the supplied bounds.\n    # We do this by computing a top, bottom, left, and right offset in pixel\n    # units of the supplied bounds against the tile bounds. Getting the offsets\n    # in pixel units involves converting lng/lat to web mercator units since we\n    # assume that is the CRS of the tiles. These offsets are then used to crop\n    # individual tiles and place them correctly into the output raster.\n    nw_merc_x, nw_merc_y = lnglat2merc(min_lng, max_lat)\n    left_pix_offset, top_pix_offset = merc2pixel(xmin, ymin, zoom, nw_merc_x,\n                                                 nw_merc_y)\n\n    se_merc_x, se_merc_y = lnglat2merc(max_lng, min_lat)\n    se_left_pix_offset, se_top_pix_offset = merc2pixel(xmax, ymax, zoom,\n                                                       se_merc_x, se_merc_y)\n    right_pix_offset = tile_sz - se_left_pix_offset\n    bottom_pix_offset = tile_sz - se_top_pix_offset\n\n    uncropped_height = tile_sz * (ymax - ymin + 1)\n    uncropped_width = tile_sz * (xmax - xmin + 1)\n    height = uncropped_height - top_pix_offset - bottom_pix_offset\n    width = uncropped_width - left_pix_offset - right_pix_offset\n\n    transform = rasterio.transform.from_bounds(nw_merc_x, se_merc_y, se_merc_x,\n                                               nw_merc_y, width, height)\n    with rasterio.open(\n            output_path,\n            \'w\',\n            driver=\'GTiff\',\n            height=height,\n            width=width,\n            count=3,\n            crs=\'epsg:3857\',\n            transform=transform,\n            dtype=rasterio.uint8) as dataset:\n        out_x = 0\n        for xi, x in enumerate(range(xmin, xmax + 1)):\n            tile_xmin, tile_xmax = 0, tile_sz - 1\n            if x == xmin:\n                tile_xmin += left_pix_offset\n            if x == xmax:\n                tile_xmax -= right_pix_offset\n            window_width = tile_xmax - tile_xmin + 1\n\n            out_y = 0\n            for yi, y in enumerate(range(ymin, ymax + 1)):\n                tile_ymin, tile_ymax = 0, tile_sz - 1\n                if y == ymin:\n                    tile_ymin += top_pix_offset\n                if y == ymax:\n                    tile_ymax -= bottom_pix_offset\n                window_height = tile_ymax - tile_ymin + 1\n\n                # Convert from xyz to tms if needed.\n                # https://gist.github.com/tmcw/4954720\n                if is_tms:\n                    y = (2**zoom) - y - 1\n                tile_uri = tile_schema.format(x=x, y=y, z=zoom)\n                tile_path = download_if_needed(tile_uri, tmp_dir)\n                img = np.array(Image.open(tile_path))\n                img = img[tile_ymin:tile_ymax + 1, tile_xmin:tile_xmax + 1, :]\n\n                window = Window(out_x, out_y, window_width, window_height)\n                dataset.write(\n                    np.transpose(img[:, :, 0:3], (2, 0, 1)), window=window)\n                out_y += window_height\n            out_x += window_width\n\n    if make_cog:\n        create_cog(output_path, output_uri, tmp_dir)\n    else:\n        upload_or_copy(output_path, output_uri)\n\n\n@click.command()\n@click.argument(\'tile_schema\')\n@click.argument(\'zoom\')\n@click.argument(\'bounds\')\n@click.argument(\'output_uri\')\n@click.option(\'--make-cog\', is_flag=True, default=False)\ndef zxy2geotiff(tile_schema, zoom, bounds, output_uri, make_cog):\n    """"""Generates a GeoTIFF of a bounded region from a ZXY tile server.\n\n    TILE_SCHEMA: the URI schema for zxy tiles (ie. a slippy map tile server) of\n    the form /tileserver-uri/{z}/{x}/{y}.png. If {-y} is used, the tiles are\n    assumed to be indexed using TMS coordinates, where the y axis starts at\n    the southernmost point. The URI can be for http, S3, or the local file\n    system.\n\n    ZOOM: the zoom level to use when retrieving tiles\n\n    BOUNDS: a space-separated string containing min_lat, min_lng, max_lat,\n    max_lng\n\n    OUTPUT_URI: where to save the GeoTIFF. The URI can be for http, S3, or the\n    local file system.\n    """"""\n    bounds = [float(x) for x in bounds.split(\' \')]\n    _zxy2geotiff(tile_schema, int(zoom), bounds, output_uri, make_cog=make_cog)\n\n\nif __name__ == \'__main__\':\n    zxy2geotiff()\n'"
rastervision2/examples/deluxe_message_maker/__init__.py,0,b'# flake8: noqa\n\n# Always need to import first.\nimport rastervision2.pipeline\n\n# Need to import any modules with register_config decorators.\nimport rastervision2.examples.deluxe_message_maker.deluxe_message_maker\n\n\ndef register_plugin(registry):\n    # Can be used to manually update the registry. Useful\n    # for adding new FileSystems and Runners.\n    pass\n'
rastervision2/examples/deluxe_message_maker/deluxe_message_maker.py,0,"b""from rastervision2.pipeline.config import register_config\nfrom rastervision2.examples.sample_pipeline2.sample_pipeline2 import (\n    MessageMakerConfig, MessageMaker)\n\n\n# You always need to use the register_config decorator.\n@register_config('deluxe_message_maker')\nclass DeluxeMessageMakerConfig(MessageMakerConfig):\n    # Note that this inherits the greeting field from MessageMakerConfig.\n    level: int = 1\n\n    def build(self):\n        return DeluxeMessageMaker(self)\n\n\nclass DeluxeMessageMaker(MessageMaker):\n    def make_message(self, name):\n        # Uses the level field to determine the number of exclamation marks.\n        exclamation_marks = '!' * self.config.level\n        return '{} {}{}'.format(self.config.greeting, name, exclamation_marks)\n"""
rastervision2/examples/deluxe_message_maker/my_config.py,0,"b""from rastervision2.examples.sample_pipeline2.sample_pipeline2 import (\n    SamplePipeline2Config)\nfrom rastervision2.examples.deluxe_message_maker.deluxe_message_maker import (\n    DeluxeMessageMakerConfig)\n\n\ndef get_config(runner, root_uri):\n    names = ['alice', 'bob', 'susan']\n    # Note that we use the DeluxeMessageMakerConfig and set the level to 3.\n    message_maker = DeluxeMessageMakerConfig(greeting='hola', level=3)\n    return SamplePipeline2Config(\n        root_uri=root_uri, names=names, message_maker=message_maker)\n"""
rastervision2/examples/sample_pipeline/__init__.py,0,b'# flake8: noqa\nimport rastervision2.pipeline\nimport rastervision2.examples.sample_pipeline.sample_pipeline\n\n\ndef register_plugin(registry):\n    pass\n'
rastervision2/examples/sample_pipeline/my_config.py,0,"b'from rastervision2.examples.sample_pipeline.sample_pipeline import (\n    SamplePipelineConfig)\n\n\ndef get_config(runner, root_uri):\n    # The get_config function returns an instantiated PipelineConfig and\n    # plays a similar role as a typical ""config file"" used in other systems.\n    # It\'s different in that it can have loops, conditionals, local variables,\n    # etc. The runner argument is the name of the runner used to run the\n    # pipeline (eg. local or aws_batch). Any other arguments are passed from\n    # the CLI using the -a option.\n    names = [\'alice\', \'bob\', \'susan\']\n\n    # Note that root_uri is a field that is inherited from PipelineConfig,\n    # the parent class of SamplePipelineConfig, and specifies the root URI\n    # where any output files are saved.\n    return SamplePipelineConfig(root_uri=root_uri, names=names)\n'"
rastervision2/examples/sample_pipeline/sample_pipeline.py,0,"b""from typing import List, Optional\nfrom os.path import join\n\nfrom rastervision2.pipeline.pipeline import Pipeline\nfrom rastervision2.pipeline.file_system import str_to_file, file_to_str\nfrom rastervision2.pipeline.pipeline_config import PipelineConfig\nfrom rastervision2.pipeline.config import register_config\nfrom rastervision2.pipeline.utils import split_into_groups\n\n\n# Each Config needs to be registered with a type hint which is used for\n# serializing and deserializing to JSON.\n@register_config('sample_pipeline')\nclass SamplePipelineConfig(PipelineConfig):\n    # Config classes are configuration schemas. Each field is an attributes\n    # with a type and optional default value.\n    names: List[str] = ['alice', 'bob']\n    message_uris: Optional[List[str]] = None\n\n    def build(self, tmp_dir):\n        # The build method is used to instantiate the corresponding object\n        # using this configuration.\n        return SamplePipeline(self, tmp_dir)\n\n    def update(self):\n        # The update method is used to set default values as a function of\n        # other values.\n        if self.message_uris is None:\n            self.message_uris = [\n                join(self.root_uri, '{}.txt'.format(name))\n                for name in self.names\n            ]\n\n\nclass SamplePipeline(Pipeline):\n    # The order in which commands run. Each command correspond to a method.\n    commands: List[str] = ['save_messages', 'print_messages']\n\n    # Split commands can be split up and run in parallel.\n    split_commands = ['save_messages']\n\n    # GPU commands are run using GPUs if available. There are no commands worth running\n    # on a GPU in this pipeline.\n    gpu_commands = []\n\n    def save_messages(self, split_ind=0, num_splits=1):\n        # Save a file for each name with a message.\n\n        # The num_splits is the number of parallel jobs to use and\n        # split_ind tracks the index of the parallel job. In this case\n        # we are splitting on the names/message_uris.\n        split_groups = split_into_groups(\n            list(zip(self.config.names, self.config.message_uris)), num_splits)\n        split_group = split_groups[split_ind]\n\n        for name, message_uri in split_group:\n            message = 'hello {}!'.format(name)\n            # str_to_file and most functions in the file_system package can\n            # read and write transparently to different file systems based on\n            # the URI pattern.\n            str_to_file(message, message_uri)\n            print('Saved message to {}'.format(message_uri))\n\n    def print_messages(self):\n        # Read all the message files and print them.\n        for message_uri in self.config.message_uris:\n            message = file_to_str(message_uri)\n            print(message)\n"""
rastervision2/examples/sample_pipeline2/__init__.py,0,b'# flake8: noqa\nimport rastervision2.pipeline\nimport rastervision2.examples.sample_pipeline2.sample_pipeline2\n\n\ndef register_plugin(registry):\n    pass\n'
rastervision2/examples/sample_pipeline2/my_config.py,0,"b""from rastervision2.examples.sample_pipeline2.sample_pipeline2 import (\n    SamplePipeline2Config, MessageMakerConfig)\n\n\ndef get_config(runner, root_uri):\n    names = ['alice', 'bob', 'susan']\n    # Same as before except we can set the greeting to be\n    # 'hola' instead of 'hello'.\n    message_maker = MessageMakerConfig(greeting='hola')\n    return SamplePipeline2Config(\n        root_uri=root_uri, names=names, message_maker=message_maker)\n"""
rastervision2/examples/sample_pipeline2/sample_pipeline2.py,0,"b""from typing import List, Optional\nfrom os.path import join\n\nfrom rastervision2.pipeline.pipeline import Pipeline\nfrom rastervision2.pipeline.file_system import str_to_file, file_to_str\nfrom rastervision2.pipeline.pipeline_config import PipelineConfig\nfrom rastervision2.pipeline.config import register_config, Config\nfrom rastervision2.pipeline.utils import split_into_groups\n\n\n@register_config('message_maker')\nclass MessageMakerConfig(Config):\n    greeting: str = 'hello'\n\n    def build(self):\n        return MessageMaker(self)\n\n\nclass MessageMaker():\n    def __init__(self, config):\n        self.config = config\n\n    def make_message(self, name):\n        # Use the greeting field to make the message.\n        return '{} {}!'.format(self.config.greeting, name)\n\n\n@register_config('sample_pipeline2')\nclass SamplePipeline2Config(PipelineConfig):\n    names: List[str] = ['alice', 'bob']\n    message_uris: Optional[List[str]] = None\n    # Fields can have other Configs as types.\n    message_maker: MessageMakerConfig = MessageMakerConfig()\n\n    def build(self, tmp_dir):\n        return SamplePipeline2(self, tmp_dir)\n\n    def update(self):\n        if self.message_uris is None:\n            self.message_uris = [\n                join(self.root_uri, '{}.txt'.format(name))\n                for name in self.names\n            ]\n\n\nclass SamplePipeline2(Pipeline):\n    commands: List[str] = ['save_messages', 'print_messages']\n    split_commands = ['save_messages']\n    gpu_commands = []\n\n    def save_messages(self, split_ind=0, num_splits=1):\n        message_maker = self.config.message_maker.build()\n\n        split_groups = split_into_groups(\n            list(zip(self.config.names, self.config.message_uris)), num_splits)\n        split_group = split_groups[split_ind]\n\n        for name, message_uri in split_group:\n            # Unlike before, we use the message_maker to make the message.\n            message = message_maker.make_message(name)\n            str_to_file(message, message_uri)\n            print('Saved message to {}'.format(message_uri))\n\n    def print_messages(self):\n        for message_uri in self.config.message_uris:\n            message = file_to_str(message_uri)\n            print(message)\n"""
rastervision2/pipeline/file_system/__init__.py,0,"b'# flake8: noqa\n\nfrom rastervision2.pipeline.file_system.file_system import (\n    FileSystem, NotReadableError, NotWritableError)\nfrom rastervision2.pipeline.file_system.local_file_system import LocalFileSystem\nfrom rastervision2.pipeline.file_system.http_file_system import HttpFileSystem\nfrom rastervision2.pipeline.file_system.utils import *\n'"
rastervision2/pipeline/file_system/file_system.py,0,"b'from abc import (ABC, abstractmethod)\nfrom datetime import datetime\nfrom typing import Optional, List\n\nfrom rastervision2.pipeline import registry\n\n\nclass NotReadableError(Exception):\n    """"""Exception raised when files are not readable.""""""\n    pass\n\n\nclass NotWritableError(Exception):\n    """"""Exception raised when files are not writable.""""""\n    pass\n\n\nclass FileSystem(ABC):\n    """"""Abstraction for a local or remote file system.\n\n    This can be subclassed to handle different cloud storage providers, etc.\n    """"""\n\n    @staticmethod\n    def get_file_system(uri: str, mode: str = \'r\') -> \'FileSystem\':\n        """"""Return FileSystem that should be used for the given URI/mode pair.\n\n        Args:\n            uri: URI of file\n            mode: mode to open file in, \'r\' or \'w\'\n        """"""\n        return registry.get_file_system(uri, mode)\n\n    @staticmethod\n    @abstractmethod\n    def matches_uri(uri: str, mode: str) -> bool:\n        """"""Returns True if this FS can be used for the given URI/mode pair.\n\n        Args:\n            uri: URI of file\n            mode: mode to open file in, \'r\' or \'w\'\n        """"""\n        pass  # pragma: no cover\n\n    @staticmethod\n    @abstractmethod\n    def file_exists(uri: str, include_dir: bool = True) -> bool:\n        """"""Check if a file exists.\n\n        Args:\n          uri: The URI to check\n          include_dir: Include directories in check, if this file_system\n            supports directory reads. Otherwise only return true if a single\n            file exists at the URI.\n        """"""\n        pass  # pragma: no cover\n\n    @staticmethod\n    @abstractmethod\n    def read_str(uri: str) -> str:\n        """"""Read contents of URI to a string.""""""\n        pass  # pragma: no cover\n\n    @staticmethod\n    @abstractmethod\n    def read_bytes(uri: str) -> bytes:\n        """"""Read contents of URI to bytes.""""""\n        pass  # pragma: no cover\n\n    @staticmethod\n    @abstractmethod\n    def write_str(uri: str, data: str):\n        """"""Write string in data to URI.""""""\n        pass  # pragma: no cover\n\n    @staticmethod\n    @abstractmethod\n    def write_bytes(uri: str, data: bytes):\n        """"""Write bytes in data to URI.""""""\n        pass  # pragma: no cover\n\n    @staticmethod\n    @abstractmethod\n    def sync_to_dir(src_dir: str, dst_dir_uri: str, delete: bool = False):\n        """"""Syncs a local source directory to a destination directory.\n\n        If the FileSystem is remote, this involves uploading.\n\n        Args:\n            src_dir: local source directory to sync from\n            dst_dir_uri: A destination directory that can be synced to by this\n                FileSystem\n            delete: True if the destination should be deleted first.\n        """"""\n        pass  # pragma: no cover\n\n    @staticmethod\n    @abstractmethod\n    def sync_from_dir(src_dir_uri: str, dst_dir: str, delete: bool = False):\n        """"""Syncs a source directory to a local destination directory.\n\n        If the FileSystem is remote, this involves downloading.\n\n        Args:\n            src_dir_uri: source directory that can be synced from by this FileSystem\n            dst_dir: A local destination directory\n            delete: True if the destination should be deleted first.\n        """"""\n        pass  # pragma: no cover\n\n    @staticmethod\n    @abstractmethod\n    def copy_to(src_path: str, dst_uri: str):\n        """"""Copy a local source file to a destination.\n\n        If the FileSystem is remote, this involves uploading.\n\n        Args:\n            src_path: local path to source file\n            dst_uri: uri of destination that can be copied to by this FileSystem\n        """"""\n        pass  # pragma: no cover\n\n    @staticmethod\n    @abstractmethod\n    def copy_from(src_uri: str, dst_path: str):\n        """"""Copy a source file to a local destination.\n\n        If the FileSystem is remote, this involves downloading.\n\n        Args:\n            src_uri: uri of source that can be copied from by this FileSystem\n            dst_path: local path to destination file\n        """"""\n        pass  # pragma: no cover\n\n    @staticmethod\n    @abstractmethod\n    def local_path(uri: str, download_dir: str) -> str:\n        """"""Return the path where a local copy should be stored.\n\n        Args:\n            uri: the URI of the file to be copied\n            download_dir: path of the local directory in which files should\n                be copied\n        """"""\n        pass  # pragma: no cover\n\n    @staticmethod\n    @abstractmethod\n    def last_modified(uri: str) -> Optional[datetime]:\n        """"""Get the last modified date of a file.\n\n        Args:\n            uri: the URI of the file\n\n        Returns:\n            the last modified date in UTC of a file or None if this FileSystem\n            does not support this operation.\n        """"""\n        pass  # pragma: no cover\n\n    @staticmethod\n    @abstractmethod\n    def list_paths(uri: str, ext: Optional[str] = None) -> List[str]:\n        """"""List paths rooted at URI.\n\n        Optionally only includes paths with a certain file extension.\n\n        Args:\n            uri: the URI of a directory\n            ext: the optional file extension to filter by\n        """"""\n        pass  # pragma: no cover\n'"
rastervision2/pipeline/file_system/http_file_system.py,0,"b'import os\nimport shutil\nimport urllib\nimport urllib.request\nfrom datetime import datetime\n\nfrom rastervision2.pipeline.file_system import (FileSystem, NotReadableError,\n                                                NotWritableError)\nfrom urllib.parse import urlparse\n\n\nclass HttpFileSystem(FileSystem):\n    """"""A FileSystem for downloading files over HTTP.""""""\n\n    @staticmethod\n    def matches_uri(uri: str, mode: str) -> bool:\n        parsed_uri = urlparse(uri)\n        return parsed_uri.scheme in [\'http\', \'https\']\n\n    @staticmethod\n    def file_exists(uri: str, include_dir: bool = True) -> bool:\n        try:\n            response = urllib.request.urlopen(uri)\n            if response.getcode() == 200:\n                return int(response.headers[\'content-length\']) > 0\n            else:\n                return False  # pragma: no cover\n        except urllib.error.URLError:\n            return False\n\n    @staticmethod\n    def read_str(uri: str) -> str:\n        return HttpFileSystem.read_bytes(uri).decode(\'utf8\')\n\n    @staticmethod\n    def read_bytes(uri: str) -> bytes:\n        with urllib.request.urlopen(uri) as req:\n            return req.read()\n\n    @staticmethod\n    def write_str(uri: str, data: str) -> None:\n        raise NotWritableError(\'Could not write {}\'.format(uri))\n\n    @staticmethod\n    def write_bytes(uri: str, data: bytes) -> None:\n        raise NotWritableError(\'Could not write {}\'.format(uri))\n\n    @staticmethod\n    def sync_to_dir(src_dir: str, dst_dir_uri: str,\n                    delete: bool = False) -> None:\n        raise NotWritableError(\'Could not write {}\'.format(dst_dir_uri))\n\n    @staticmethod\n    def sync_from_dir(src_dir_uri: str, dst_dir: str,\n                      delete: bool = False) -> None:\n        raise NotReadableError(\n            \'Cannot read directory from HTTP {}\'.format(src_dir_uri))\n\n    @staticmethod\n    def copy_to(src_path: str, dst_uri: str) -> None:\n        raise NotWritableError(\'Could not write {}\'.format(dst_uri))\n\n    @staticmethod\n    def copy_from(src_uri: str, dst_path: str) -> None:\n        with urllib.request.urlopen(src_uri) as response:\n            with open(dst_path, \'wb\') as out_file:\n                try:\n                    shutil.copyfileobj(response, out_file)\n                except Exception:  # pragma: no cover\n                    raise NotReadableError(\'Could not read {}\'.format(src_uri))\n\n    @staticmethod\n    def local_path(uri: str, download_dir: str) -> None:\n        parsed_uri = urlparse(uri)\n        path = os.path.join(download_dir, \'http\', parsed_uri.netloc,\n                            parsed_uri.path[1:])\n        # This function is expected to return something that is file path-like\n        # (as opposed to directory-like),\n        # so if the path ends with / we strip it off. This was motivated by\n        # a URI that was a zxy tile schema that doesn\'t end in .png which is\n        # parsed by urlparse into a path that ends in a /.\n        if path.endswith(\'/\'):\n            path = path[:-1]\n        return path\n\n    @staticmethod\n    def last_modified(uri: str) -> datetime:\n        return None\n\n    @staticmethod\n    def list_paths(uri, suffix=None):  # pragma: no cover\n        raise NotImplementedError()\n'"
rastervision2/pipeline/file_system/local_file_system.py,0,"b'import os\nimport shutil\nfrom datetime import datetime, timezone\nimport glob\n\nfrom rastervision2.pipeline.file_system import (FileSystem, NotReadableError)\n\n\ndef make_dir(path, check_empty=False, force_empty=False, use_dirname=False):\n    """"""Make a local directory.\n\n    Args:\n        path: path to directory\n        check_empty: if True, check that directory is empty\n        force_empty: if True, delete files if necessary to make directory\n            empty\n        use_dirname: if True, use the the parent directory as path\n\n    Raises:\n        ValueError if check_empty is True and directory is not empty\n    """"""\n    directory = path\n    if use_dirname:\n        directory = os.path.abspath(os.path.dirname(path))\n\n    if force_empty and os.path.isdir(directory):\n        shutil.rmtree(directory)\n\n    os.makedirs(directory, exist_ok=True)\n\n    if check_empty and any(os.scandir(directory)):\n        raise ValueError(\n            \'{} needs to be an empty directory!\'.format(directory))\n\n\nclass LocalFileSystem(FileSystem):\n    """"""A FileSystem for interacting with the local file system.""""""\n\n    @staticmethod\n    def matches_uri(uri: str, mode: str) -> bool:\n        return True\n\n    @staticmethod\n    def file_exists(uri: str, include_dir: bool = True) -> bool:\n        return (os.path.isfile(uri) or (include_dir and os.path.isdir(uri)))\n\n    @staticmethod\n    def read_str(file_uri: str) -> str:\n        if not os.path.isfile(file_uri):\n            raise NotReadableError(\'Could not read {}\'.format(file_uri))\n        with open(file_uri, \'r\') as file_buffer:\n            return file_buffer.read()\n\n    @staticmethod\n    def read_bytes(file_uri: str) -> bytes:\n        if not os.path.isfile(file_uri):\n            raise NotReadableError(\'Could not read {}\'.format(file_uri))\n        with open(file_uri, \'rb\') as file_buffer:\n            return file_buffer.read()\n\n    @staticmethod\n    def write_str(file_uri: str, data: str) -> None:\n        make_dir(file_uri, use_dirname=True)\n        with open(file_uri, \'w\') as content_file:\n            content_file.write(data)\n\n    @staticmethod\n    def write_bytes(file_uri: str, data: bytes) -> None:\n        make_dir(file_uri, use_dirname=True)\n        with open(file_uri, \'wb\') as content_file:\n            content_file.write(data)\n\n    @staticmethod\n    def sync_from_dir(src_dir_uri: str, dst_dir: str,\n                      delete: bool = False) -> None:\n        if src_dir_uri == dst_dir:\n            return\n\n        if delete:\n            shutil.rmtree(dst_dir)\n\n        # https://stackoverflow.com/a/15824216/841563\n        def recursive_overwrite(src, dest):\n            if os.path.isdir(src):\n                if not os.path.isdir(dest):\n                    os.makedirs(dest)\n                    for entry in os.scandir(src):\n                        recursive_overwrite(entry.path,\n                                            os.path.join(dest, entry.name))\n            else:\n                shutil.copyfile(src, dest)\n\n        recursive_overwrite(src_dir_uri, dst_dir)\n\n    @staticmethod\n    def sync_to_dir(src_dir: str, dst_dir_uri: str,\n                    delete: bool = False) -> None:\n        LocalFileSystem.sync_from_dir(src_dir, dst_dir_uri, delete)\n\n    @staticmethod\n    def copy_to(src_path: str, dst_uri: str) -> None:\n        if src_path != dst_uri:\n            make_dir(dst_uri, use_dirname=True)\n            shutil.copyfile(src_path, dst_uri)\n\n    @staticmethod\n    def copy_from(src_uri: str, dst_path: str) -> None:\n        LocalFileSystem.copy_to(src_uri, dst_path)\n\n    @staticmethod\n    def local_path(uri: str, download_dir: str) -> None:\n        path = uri\n        return path\n\n    @staticmethod\n    def last_modified(uri: str) -> datetime:\n        local_last_modified = datetime.utcfromtimestamp(os.path.getmtime(uri))\n        return local_last_modified.replace(tzinfo=timezone.utc)\n\n    @staticmethod\n    def list_paths(uri, ext=None):\n        if ext is None:\n            ext = \'\'\n        return glob.glob(os.path.join(uri, \'*\' + ext))\n'"
rastervision2/pipeline/file_system/utils.py,0,"b'import os\nfrom os.path import join\nimport shutil\nimport gzip\nfrom threading import Timer\nimport time\nimport logging\nimport json\nimport zipfile\nfrom typing import Optional, List\n\nfrom rastervision2.pipeline.file_system import FileSystem\nfrom rastervision2.pipeline.file_system.local_file_system import make_dir\n\nlog = logging.getLogger(__name__)\n\n\ndef get_local_path(uri: str,\n                   download_dir: str,\n                   fs: Optional[FileSystem] = None) -> str:\n    """"""Return the path where a local copy of URI should be stored.\n\n    If URI is local, return it. If it\'s remote, we generate a path for it\n    within download_dir.\n\n    Args:\n        uri: the URI of the file to be copied\n        download_dir: path of the local directory in which files should\n            be copied\n        fs: if supplied, use fs instead of automatically chosen FileSystem for\n            URI\n\n    Returns:\n        a local path\n    """"""\n    if uri is None:\n        return None\n\n    if not fs:\n        fs = FileSystem.get_file_system(uri, \'r\')\n    path = fs.local_path(uri, download_dir)\n\n    return path\n\n\ndef sync_to_dir(src_dir: str,\n                dst_dir_uri: str,\n                delete: bool = False,\n                fs: Optional[FileSystem] = None):\n    """"""Synchronize a local source directory to destination directory.\n\n    Transfers files from source to destination directories so that the\n    destination has all the source files. If FileSystem is remote, this involves\n    uploading.\n\n    Args:\n        src_dir: path of local source directory\n        dst_dir_uri: URI of destination directory\n        delete: if True, delete files in the destination to match those in the\n            source directory\n        fs: if supplied, use fs instead of automatically chosen FileSystem for\n            dst_dir_uri\n    """"""\n    if not fs:\n        fs = FileSystem.get_file_system(dst_dir_uri, \'w\')\n    fs.sync_to_dir(src_dir, dst_dir_uri, delete=delete)\n\n\ndef sync_from_dir(src_dir_uri: str,\n                  dst_dir: str,\n                  delete: bool = False,\n                  fs: Optional[FileSystem] = None):\n    """"""Synchronize a source directory to local destination directory.\n\n    Transfers files from source to destination directories so that the\n    destination has all the source files. If FileSystem is remote, this involves\n    downloading.\n\n    Args:\n        src_dir_uri: URI of source directory\n        dst_dir: path of local destination directory\n        delete: if True, delete files in the destination to match those in the\n            source directory\n        fs: if supplied, use fs instead of automatically chosen FileSystem for\n            dst_dir_uri\n    """"""\n    if not fs:\n        fs = FileSystem.get_file_system(src_dir_uri, \'r\')\n    fs.sync_from_dir(src_dir_uri, dst_dir, delete=delete)\n\n\ndef start_sync(src_dir: str,\n               dst_dir_uri: str,\n               sync_interval: int = 600,\n               fs: Optional[FileSystem] = None):  # pragma: no cover\n    """"""Repeatedly sync a local source directory to a destination on a schedule.\n\n    Calls sync_to_dir on a schedule.\n\n    Args:\n        src_dir: path of the local source directory\n        dst_dir_uri: URI of destination directory\n        sync_interval: period in seconds for syncing\n        fs: if supplied, use fs instead of automatically chosen FileSystem\n    """"""\n\n    def _sync_dir():\n        while True:\n            time.sleep(sync_interval)\n            log.info(\'Syncing {} to {}...\'.format(src_dir, dst_dir_uri))\n            sync_to_dir(src_dir, dst_dir_uri, delete=False, fs=fs)\n\n    class SyncThread:\n        def __init__(self):\n            thread = Timer(0.68, _sync_dir)\n            thread.daemon = True\n            thread.start()\n            self.thread = thread\n\n        def __enter__(self):\n            return self.thread\n\n        def __exit__(self, type, value, traceback):\n            self.thread.cancel()\n\n    return SyncThread()\n\n\ndef download_if_needed(uri: str,\n                       download_dir: str,\n                       fs: Optional[FileSystem] = None) -> str:\n    """"""Download a file into a directory if it\'s remote.\n\n    If uri is local, there is no need to download the file.\n\n    Args:\n        uri: URI of file\n        download_dir: local directory to download file into\n        fs: if supplied, use fs instead of automatically chosen FileSystem for\n            uri\n\n    Returns:\n        path to local file\n\n    Raises:\n        NotReadableError if URI cannot be read from\n    """"""\n    if uri is None:\n        return None\n\n    if not fs:\n        fs = FileSystem.get_file_system(uri, \'r\')\n\n    path = get_local_path(uri, download_dir, fs=fs)\n    make_dir(path, use_dirname=True)\n\n    if path != uri:\n        log.debug(\'Downloading {} to {}\'.format(uri, path))\n\n    fs.copy_from(uri, path)\n\n    return path\n\n\ndef download_or_copy(uri, target_dir, fs=None) -> str:\n    """"""Downloads or copies a file to a directory.\n\n    Downloads or copies URI into target_dir.\n\n    Args:\n        uri: URI of file\n        target_dir: local directory to download or copy file to\n        fs: if supplied, use fs instead of automatically chosen FileSystem for\n            uri\n\n    Returns:\n        the local path of file\n    """"""\n    local_path = download_if_needed(uri, target_dir, fs=fs)\n    shutil.copy(local_path, target_dir)\n    return local_path\n\n\ndef file_exists(uri, fs=None, include_dir=True) -> bool:\n    """"""Check if file exists.\n\n    Args:\n        uri: URI of file\n        fs: if supplied, use fs instead of automatically chosen FileSystem for\n            uri\n    """"""\n    if not fs:\n        fs = FileSystem.get_file_system(uri, \'r\')\n    return fs.file_exists(uri, include_dir)\n\n\ndef list_paths(uri: str, ext: str = \'\',\n               fs: Optional[FileSystem] = None) -> List[str]:\n    """"""List paths rooted at URI.\n\n    Optionally only includes paths with a certain file extension.\n\n    Args:\n        uri: the URI of a directory\n        ext: the optional file extension to filter by\n        fs: if supplied, use fs instead of automatically chosen FileSystem for\n            uri\n    """"""\n    if uri is None:\n        return None\n\n    if not fs:\n        fs = FileSystem.get_file_system(uri, \'r\')\n\n    return fs.list_paths(uri, ext=ext)\n\n\ndef upload_or_copy(src_path: str,\n                   dst_uri: str,\n                   fs: Optional[FileSystem] = None) -> List[str]:\n    """"""Upload or copy a file.\n\n    If dst_uri is local, the file is copied. Otherwise, it is uploaded.\n\n    Args:\n        src_path: path to source file\n        dst_uri: URI of destination for file\n        fs: if supplied, use fs instead of automatically chosen FileSystem for\n            dst_uri\n\n    Raises:\n        NotWritableError if dst_uri cannot be written to\n    """"""\n    if dst_uri is None:\n        return\n\n    if not (os.path.isfile(src_path) or os.path.isdir(src_path)):\n        raise Exception(\'{} does not exist.\'.format(src_path))\n\n    if not src_path == dst_uri:\n        log.info(\'Uploading {} to {}\'.format(src_path, dst_uri))\n\n    if not fs:\n        fs = FileSystem.get_file_system(dst_uri, \'w\')\n    fs.copy_to(src_path, dst_uri)\n\n\ndef file_to_str(uri: str, fs: Optional[FileSystem] = None) -> str:\n    """"""Load contents of text file into a string.\n\n    Args:\n        uri: URI of file\n        fs: if supplied, use fs instead of automatically chosen FileSystem\n\n    Returns:\n        contents of text file\n\n    Raises:\n        NotReadableError if URI cannot be read\n    """"""\n    if not fs:\n        fs = FileSystem.get_file_system(uri, \'r\')\n    return fs.read_str(uri)\n\n\ndef str_to_file(content_str: str, uri: str, fs: Optional[FileSystem] = None):\n    """"""Writes string to text file.\n\n    Args:\n        content_str: string to write\n        uri: URI of file to write\n        fs: if supplied, use fs instead of automatically chosen FileSystem\n\n    Raise:\n        NotWritableError if uri cannot be written\n    """"""\n    if not fs:\n        fs = FileSystem.get_file_system(uri, \'r\')\n    return fs.write_str(uri, content_str)\n\n\ndef get_cached_file(cache_dir: str, uri: str) -> str:\n    """"""Download a file and unzip it using a cache.\n\n    This downloads a file if it isn\'t already in the cache, and unzips\n    the file using gunzip if it hasn\'t already been unzipped (and the uri\n    has a .gz suffix).\n\n    Args:\n        cache_dir: dir to use for cache directory\n        uri: URI of a file that can be opened by a supported RV file system\n\n    Returns:\n        path of the (downloaded and unzipped) cached file\n    """"""\n    # Only download if it isn\'t in the cache.\n    path = get_local_path(uri, cache_dir)\n    if not os.path.isfile(path):\n        path = download_if_needed(uri, cache_dir)\n\n    # Unzip if .gz file\n    if path.endswith(\'.gz\'):\n        # If local URI, then make ungz_path in temp cache, so it isn\'t unzipped\n        # alongside the original file.\n        if os.path.isfile(uri):\n            ungz_path = os.path.join(cache_dir, path)[:-3]\n        else:\n            ungz_path = path[:-3]\n\n        # Check to see if it is already unzipped before unzipping.\n        if not os.path.isfile(ungz_path):\n            with gzip.open(path, \'rb\') as f_in:\n                with open(ungz_path, \'wb\') as f_out:\n                    shutil.copyfileobj(f_in, f_out)\n        path = ungz_path\n\n    return path\n\n\ndef file_to_json(uri: str) -> dict:\n    """"""Return JSON dict based on file at uri.""""""\n    return json.loads(file_to_str(uri))\n\n\ndef json_to_file(content_dict: dict, uri: str):\n    """"""Upload JSON file to uri based on content_dict.""""""\n    str_to_file(json.dumps(content_dict), uri)\n\n\ndef zipdir(dir: str, zip_path: str):\n    """"""Save a zip file with contents of directory.\n\n    Contents of directory will be at root of zip file.\n\n    Args:\n        dir: directory to zip\n        zip_path: path to zip file to create\n    """"""\n    make_dir(zip_path, use_dirname=True)\n    with zipfile.ZipFile(zip_path, \'w\', zipfile.ZIP_DEFLATED) as ziph:\n        for dirpath, dirnames, filenames in os.walk(dir):\n            for fn in filenames:\n                ziph.write(join(dirpath, fn), join(dirpath[len(dir):], fn))\n\n\ndef unzip(zip_path: str, target_dir: str):\n    """"""Unzip contents of zip file at zip_path into target_dir.\n\n    Creates target_dir if needed.\n    """"""\n    make_dir(target_dir)\n    zip_ref = zipfile.ZipFile(zip_path, \'r\')\n    zip_ref.extractall(target_dir)\n    zip_ref.close()\n'"
rastervision2/pipeline/runner/__init__.py,0,b'# flake8: noqa\n\nfrom rastervision2.pipeline.runner.inprocess_runner import *\nfrom rastervision2.pipeline.runner.local_runner import *\nfrom rastervision2.pipeline.runner.runner import *\n'
rastervision2/pipeline/runner/inprocess_runner.py,0,"b'import subprocess\nfrom inspect import signature\n\nfrom rastervision2.pipeline.cli import _run_command\nfrom rastervision2.pipeline.runner.runner import Runner\n\nINPROCESS = \'inprocess\'\n\n\nclass InProcessRunner(Runner):\n    """"""Runs each command sequentially within a single process.\n\n    Useful for testing and debugging.\n    """"""\n\n    def run(self, cfg_json_uri, pipeline, commands, num_splits=1):\n        for command in commands:\n\n            # detect external command\n            if hasattr(pipeline, command):\n                fn = getattr(pipeline, command)\n                params = signature(fn).parameters\n                external = hasattr(fn, \'external\') and len(params) == 0\n            else:\n                external = False\n\n            if not external:\n                if command in pipeline.split_commands and num_splits > 1:\n                    for split_ind in range(num_splits):\n                        _run_command(cfg_json_uri, command, split_ind,\n                                     num_splits)\n                else:\n                    _run_command(cfg_json_uri, command, 0, 1)\n            else:\n                cmd = fn()\n                subprocess.run(cmd, check=True)\n'"
rastervision2/pipeline/runner/local_runner.py,0,"b'import sys\nfrom inspect import signature\nfrom os.path import dirname, join\nfrom subprocess import Popen\n\nfrom rastervision2.pipeline.file_system import str_to_file\nfrom rastervision2.pipeline.runner.runner import Runner\nfrom rastervision2.pipeline.utils import terminate_at_exit\n\nLOCAL = \'local\'\n\n\nclass LocalRunner(Runner):\n    """"""Runs each command locally using different processes for each command/split.\n\n    This is implemented by generating a Makefile and then running it using make.\n    """"""\n\n    def run(self, cfg_json_uri, pipeline, commands, num_splits=1):\n        num_commands = 0\n        for command in commands:\n            if command in pipeline.split_commands and num_splits > 1:\n                num_commands += num_splits\n            else:\n                num_commands += 1\n\n        makefile = \'.PHONY: \'\n        makefile += \' \'.join([str(ci) for ci in range(num_commands)])\n        makefile += \'\\n\\n\'\n\n        makefile += \'all: \'\n        makefile += \' \'.join([str(ci) for ci in range(num_commands)])\n        makefile += \'\\n\\n\'\n\n        prev_command_inds = []\n        curr_command_ind = 0\n        for command in commands:\n\n            # detect external command\n            if hasattr(pipeline, command):\n                fn = getattr(pipeline, command)\n                params = signature(fn).parameters\n                external = hasattr(fn, \'external\') and len(params) == 0\n            else:\n                external = False\n\n            curr_command_inds = []\n            if not external and command in pipeline.split_commands and num_splits > 1:\n                for split_ind in range(num_splits):\n                    makefile += \'{}: \'.format(curr_command_ind)\n                    makefile += \' \'.join([str(ci) for ci in prev_command_inds])\n                    makefile += \'\\n\'\n                    invocation = (\n                        \'python -m rastervision2.pipeline.cli run_command \'\n                        \'{} {} --split-ind {} --num-splits {}\').format(\n                            cfg_json_uri, command, split_ind, num_splits)\n                    makefile += \'\\t{}\\n\\n\'.format(invocation)\n                    curr_command_inds.append(curr_command_ind)\n                    curr_command_ind += 1\n            else:\n                makefile += \'{}: \'.format(curr_command_ind)\n                makefile += \' \'.join([str(ci) for ci in prev_command_inds])\n                makefile += \'\\n\'\n                if not external:\n                    invocation = (\n                        \'python -m rastervision2.pipeline.cli run_command \'\n                        \'{} {}\'.format(cfg_json_uri, command))\n                else:\n                    invocation = (\' \'.join(fn()))\n                makefile += \'\\t{}\\n\\n\'.format(invocation)\n                curr_command_inds.append(curr_command_ind)\n                curr_command_ind += 1\n\n            prev_command_inds = curr_command_inds\n\n        makefile_path = join(dirname(cfg_json_uri), \'Makefile\')\n        str_to_file(makefile, makefile_path)\n        process = Popen([\'make\', \'-j\', \'-f\', makefile_path])\n        terminate_at_exit(process)\n        exitcode = process.wait()\n        if exitcode != 0:\n            sys.exit(exitcode)\n        else:\n            return 0\n'"
rastervision2/pipeline/runner/runner.py,0,"b'from abc import abstractmethod\nfrom typing import Optional, List\n\nfrom rastervision2.pipeline.pipeline import Pipeline\n\n\nclass Runner():\n    """"""A method for running a Pipeline.\n\n    This can be subclassed to provide the ability to run on different cloud\n    providers, etc.\n    """"""\n\n    @abstractmethod\n    def run(self,\n            cfg_json_uri: str,\n            pipeline: Pipeline,\n            commands: List[str],\n            num_splits: int = 1):\n        """"""Run commands in a Pipeline using a serialized PipelineConfig.\n\n        Args:\n            cfg_json_uri: URI of a JSON file with a serialized PipelineConfig\n            pipeline: the Pipeline to run\n            commands: names of commands to run\n            num_splits: number of splits to use for splittable commands\n        """"""\n        pass\n\n    def get_split_ind(self) -> Optional[int]:\n        """"""Get the split_ind for the process.\n\n        For split commands, the split_ind determines which split of work to perform\n        within the current OS process. The CLI has a --split-ind option, but some runners\n        may have their own means of communicating the split_ind, and this method should\n        be overridden in such cases. If this method returns None, then the --split-ind\n        option will be used. If both are null, then it won\'t be possible to run the\n        command.\n        """"""\n        return None\n'"
rastervision2/pytorch_learner/examples/__init__.py,0,b''
rastervision2/pytorch_learner/examples/classification.py,0,"b""from os.path import join\n\nfrom rastervision2.pytorch_learner.classification_learner_config import (\n    ClassificationLearnerConfig, ClassificationDataConfig)\nfrom rastervision2.pytorch_learner.learner_config import (SolverConfig,\n                                                          ModelConfig)\nfrom rastervision2.pytorch_learner.learner_pipeline_config import LearnerPipelineConfig\n\n\ndef get_config(runner, test=False):\n    base_uri = ('s3://raster-vision-lf-dev/learner/classification' if\n                runner == 'aws_batch' else '/opt/data/learner/classification')\n    root_uri = join(base_uri, 'output')\n    data_uri = join(base_uri, 'tiny-buildings.zip')\n\n    model = ModelConfig(backbone='resnet50')\n    solver = SolverConfig(lr=2e-4, num_epochs=3, batch_sz=8, one_cycle=True)\n    data = ClassificationDataConfig(\n        data_format='image_folder',\n        uri=data_uri,\n        img_sz=200,\n        labels=['building', 'no_building'])\n    learner = ClassificationLearnerConfig(\n        model=model, solver=solver, data=data, test_mode=test)\n    pipeline = LearnerPipelineConfig(root_uri=root_uri, learner=learner)\n    return pipeline\n"""
rastervision2/pytorch_learner/examples/regression.py,0,"b""from os.path import join\n\nfrom rastervision2.pytorch_learner.regression.config import (\n    RegressionLearnerConfig, RegressionDataConfig, RegressionModelConfig)\nfrom rastervision2.pytorch_learner.learner_config import (SolverConfig)\nfrom rastervision2.pytorch_learner.learner_pipeline_config import LearnerPipelineConfig\n\n\ndef get_config(runner, test=False):\n    base_uri = ('s3://raster-vision-lf-dev/learner/regression'\n                if runner == 'aws_batch' else '/opt/data/learner/regression')\n    root_uri = join(base_uri, 'output')\n    data_uri = join(base_uri, 'tiny-buildings.zip')\n\n    model = RegressionModelConfig(backbone='resnet50')\n    solver = SolverConfig(lr=1e-4, num_epochs=10, batch_sz=8, one_cycle=True)\n    data = RegressionDataConfig(\n        data_format='image_csv',\n        uri=data_uri,\n        img_sz=200,\n        labels=['has_buildings'])\n    learner = RegressionLearnerConfig(\n        model=model, solver=solver, data=data, test_mode=test)\n\n    pipeline = LearnerPipelineConfig(root_uri=root_uri, learner=learner)\n    return pipeline\n"""
tests/backend/torch_utils/test_metrics.py,9,"b""import unittest\n\nfrom rastervision.backend.torch_utils.metrics import (compute_conf_mat,\n                                                      compute_conf_mat_metrics)\nimport rastervision as rv\n\n\n@unittest.skipIf(not rv.backend.pytorch_available, 'PyTorch is not available')\nclass TestComputeConfMat(unittest.TestCase):\n    def test1(self):\n        import torch\n        y = torch.tensor([0, 1, 0, 1])\n        out = torch.tensor([0, 1, 0, 1])\n        num_labels = 2\n        conf_mat = compute_conf_mat(out, y, num_labels)\n        exp_conf_mat = torch.tensor([[2., 0], [0, 2]])\n        self.assertTrue(conf_mat.equal(exp_conf_mat))\n\n    def test2(self):\n        import torch\n        y = torch.tensor([0, 1, 0, 1])\n        out = torch.tensor([1, 1, 1, 1])\n        num_labels = 2\n        conf_mat = compute_conf_mat(out, y, num_labels)\n        exp_conf_mat = torch.tensor([[0., 2], [0, 2]])\n        self.assertTrue(conf_mat.equal(exp_conf_mat))\n\n\n@unittest.skipIf(not rv.backend.pytorch_available, 'PyTorch is not available')\nclass TestComputeConfMatMetrics(unittest.TestCase):\n    def test1(self):\n        import torch\n        conf_mat = torch.tensor([[2., 0], [0, 2]])\n        metrics = compute_conf_mat_metrics(conf_mat)\n        exp_metrics = {'precision': 1.0, 'recall': 1.0, 'f1': 1.0}\n        self.assertDictEqual(metrics, exp_metrics)\n\n    def test2(self):\n        import torch\n        conf_mat = torch.tensor([[0, 2.], [2, 0]])\n        metrics = compute_conf_mat_metrics(conf_mat)\n        exp_metrics = {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}\n        self.assertDictEqual(metrics, exp_metrics)\n\n    def test3(self):\n        import torch\n        conf_mat = torch.tensor([[1, 2], [1, 2.]])\n        metrics = compute_conf_mat_metrics(conf_mat)\n        exp_metrics = {'precision': 0.5, 'recall': 0.5, 'f1': 0.5}\n        self.assertDictEqual(metrics, exp_metrics)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests/command/aux/__init__.py,0,b''
tests/command/aux/test_cogify_command.py,0,"b""import os\nimport unittest\n\nimport rasterio\n\nimport rastervision as rv\nfrom rastervision.rv_config import RVConfig\nfrom tests import data_file_path\nimport tests.mock as mk\n\n\nclass TestCogifyCommand(mk.MockMixin, unittest.TestCase):\n    def test_command_create(self):\n        src_path = data_file_path('small-rgb-tile.tif')\n        with RVConfig.get_tmp_dir() as tmp_dir:\n            cog_path = os.path.join(tmp_dir, 'cog.tif')\n\n            cmd_conf = rv.CommandConfig.builder(rv.COGIFY) \\\n                                       .with_root_uri(tmp_dir) \\\n                                       .with_config(uris=[(src_path, cog_path)],\n                                                    block_size=128) \\\n                                       .build()\n\n            cmd_conf = rv.command.CommandConfig.from_proto(cmd_conf.to_proto())\n            cmd = cmd_conf.create_command()\n\n            self.assertTrue(cmd, rv.command.aux.CogifyCommand)\n\n            cmd.run(tmp_dir)\n\n            # Check that it's cogified\n            with rasterio.open(cog_path) as ds:\n                self.assertEqual(ds.block_shapes, [(128, 128), (128, 128),\n                                                   (128, 128)])\n                self.assertEqual(ds.overviews(1), [2, 4, 8, 16, 32])\n                self.assertEqual(ds.compression.value, 'DEFLATE')\n\n    def test_command_create_no_compression(self):\n        src_path = data_file_path('small-rgb-tile.tif')\n        with RVConfig.get_tmp_dir() as tmp_dir:\n            cog_path = os.path.join(tmp_dir, 'cog.tif')\n\n            cmd_conf = rv.CommandConfig.builder(rv.COGIFY) \\\n                                       .with_root_uri(tmp_dir) \\\n                                       .with_config(uris=[(src_path, cog_path)],\n                                                    block_size=128,\n                                                    compression='none') \\\n                                       .build()\n\n            cmd_conf = rv.command.CommandConfig.from_proto(cmd_conf.to_proto())\n            cmd = cmd_conf.create_command()\n\n            self.assertTrue(cmd, rv.command.aux.CogifyCommand)\n\n            cmd.run(tmp_dir)\n\n            # Check that it's cogified\n            with rasterio.open(cog_path) as ds:\n                self.assertEqual(ds.block_shapes, [(128, 128), (128, 128),\n                                                   (128, 128)])\n                self.assertEqual(ds.overviews(1), [2, 4, 8, 16, 32])\n                self.assertIsNone(ds.compression)\n\n    def test_command_through_experiment(self):\n        src_path = data_file_path('small-rgb-tile.tif')\n        with RVConfig.get_tmp_dir() as tmp_dir:\n            cog_path = os.path.join(tmp_dir, 'cog.tif')\n\n            e = mk.create_mock_experiment().to_builder() \\\n                                           .with_root_uri(tmp_dir) \\\n                                           .with_custom_config({\n                                               'cogify': {\n                                                   'key': 'test',\n                                                   'config': {\n                                                       'uris': [(src_path, cog_path)],\n                                                       'block_size': 128\n                                                   }\n                                               }\n                                           }) \\\n                                           .build()\n\n            rv.ExperimentRunner.get_runner(rv.LOCAL).run(\n                e, splits=2, commands_to_run=[rv.COGIFY])\n\n            # Check that it's cogified\n            with rasterio.open(cog_path) as ds:\n                self.assertEqual(ds.block_shapes, [(128, 128), (128, 128),\n                                                   (128, 128)])\n                self.assertEqual(ds.overviews(1), [2, 4, 8, 16, 32])\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests/data-files/plugins/noop_backend.py,0,"b""import rastervision as rv\nfrom rastervision.backend import (Backend, BackendConfig, BackendConfigBuilder)\nfrom rastervision.protos.backend_pb2 import BackendConfig as BackendConfigMsg\n\nfrom .noop_task import NOOP_TASK\n\nNOOP_BACKEND = 'NOOP_BACKEND'\n\n\nclass NoopBackend(Backend):\n    def process_scene_data(self, scene, data, tmp_dir):\n        pass\n\n    def process_sceneset_results(self, training_results, validation_results,\n                                 tmp_dir):\n        pass\n\n    def train(self, tmp_dir):\n        pass\n\n    def load_model(self, chips, windows, tmp_dir):\n        pass\n\n    def predict(self, chips, windows, tmp_dir):\n        pass\n\n\nclass NoopBackendConfig(BackendConfig):\n    def __init__(self):\n        super().__init__(NOOP_BACKEND)\n\n    def to_proto(self):\n        msg = BackendConfigMsg(\n            backend_type=self.backend_type, custom_config={})\n        return msg\n\n    def create_backend(self, task_config):\n        return NoopBackend()\n\n    def report_io(self, command_type, io_def):\n        pass\n\n    def save_bundle_files(self, bundle_dir):\n        return (self, [])\n\n    def load_bundle_files(self, bundle_dir):\n        return self\n\n\nclass NoopBackendConfigBuilder(BackendConfigBuilder):\n    def __init__(self, prev=None):\n        super().__init__(NOOP_BACKEND, NoopBackendConfig, {})\n\n    def from_proto(self, msg):\n        return self\n\n    def _applicable_tasks(self):\n        return [NOOP_TASK]\n\n    def _process_task(self, task):\n        pass\n\n\ndef register_plugin(plugin_registry):\n    plugin_registry.register_config_builder(rv.BACKEND, NOOP_BACKEND,\n                                            NoopBackendConfigBuilder)\n"""
tests/data-files/plugins/noop_command.py,0,"b""import os\n\nfrom google.protobuf import struct_pb2\n\nimport rastervision as rv\nfrom rastervision.command import (Command, CommandConfig, CommandConfigBuilder)\nfrom rastervision.protos.command_pb2 \\\n    import CommandConfig as CommandConfigMsg\n\nNOOP_COMMAND = 'NOOP_COMMAND'\n\n\nclass NoopCommand(Command):\n    def __init__(self, command_config):\n        self.command_config = command_config\n\n    def run(self, tmp_dir=None):\n        pass\n\n\nclass NoopCommandConfig(CommandConfig):\n    def __init__(self, root_uri):\n        super().__init__(NOOP_COMMAND, root_uri)\n\n    def create_command(self, tmp_dir=None):\n        retval = NoopCommand(self)\n        retval.set_tmp_dir(tmp_dir)\n        return retval\n\n    def to_proto(self):\n        msg = super().to_proto()\n        msg.MergeFrom(CommandConfigMsg(custom_config=struct_pb2.Struct()))\n\n        return msg\n\n    def report_io(self):\n        return rv.core.CommandIODefinition()\n\n\nclass NoopCommandConfigBuilder(CommandConfigBuilder):\n    def __init__(self, command_type, prev=None):\n        super().__init__(command_type, prev)\n\n    def validate(self):\n        super().validate()\n\n    def build(self):\n        self.validate()\n        return NoopCommandConfig(self.root_uri)\n\n    def from_proto(self, msg):\n        return super().from_proto(msg)\n\n    def get_root_uri(self, experiment_config):\n        noop_key = experiment_config.custom_config.get('noop_key')\n        if not noop_key:\n            noop_uri = experiment_config.custom_config.get('noop_uri')\n            if not noop_uri:\n                raise rv.ConfigError(\n                    'NoopCommand requires a noop_key or noop_uri '\n                    'be set in the experiment custom_config')\n        else:\n            noop_uri = os.path.join(experiment_config.root_uri, 'noop',\n                                    noop_key)\n\n        return noop_uri\n\n    def with_experiment(self, experiment_config):\n        b = super().with_experiment(experiment_config)\n        return b\n\n\ndef register_plugin(plugin_registry):\n    plugin_registry.register_command_config_builder(NOOP_COMMAND,\n                                                    NoopCommandConfigBuilder)\n"""
tests/data-files/plugins/noop_evaluator.py,0,"b""import rastervision as rv\nfrom rastervision.evaluation import (Evaluator, EvaluatorConfig,\n                                     EvaluatorConfigBuilder)\nfrom rastervision.protos.evaluator_pb2 import EvaluatorConfig as EvaluatorConfigMsg\n\nfrom .noop_utils import noop\n\nNOOP_EVALUATOR = 'NOOP_EVALUATOR'\n\n\nclass NoopEvaluator(Evaluator):\n    def process(self, scenes, tmp_dir):\n        return noop(scenes)\n\n\nclass NoopEvaluatorConfig(EvaluatorConfig):\n    def __init__(self):\n        super().__init__(NOOP_EVALUATOR)\n\n    def to_proto(self):\n        msg = EvaluatorConfigMsg(\n            evaluator_type=self.evaluator_type, custom_config={})\n        return msg\n\n    def create_evaluator(self):\n        return NoopEvaluator()\n\n    def report_io(self, command_type, io_def):\n        pass\n\n\nclass NoopEvaluatorConfigBuilder(EvaluatorConfigBuilder):\n    def __init__(self, prev=None):\n        super().__init__(NoopEvaluatorConfig, {})\n\n    def from_proto(self, msg):\n        return self\n\n\ndef register_plugin(plugin_registry):\n    plugin_registry.register_config_builder(rv.EVALUATOR, NOOP_EVALUATOR,\n                                            NoopEvaluatorConfigBuilder)\n"""
tests/data-files/plugins/noop_filesystem.py,0,"b""from rastervision.filesystem import FileSystem\n\n\nclass NoopFileSystem(FileSystem):\n    @staticmethod\n    def matches_uri(uri: str) -> bool:\n        True\n\n    @staticmethod\n    def file_exists(uri: str) -> bool:\n        False\n\n    @staticmethod\n    def read_str(uri: str) -> str:\n        return ''\n\n    @staticmethod\n    def read_bytes(uri: str) -> bytes:\n        return None\n\n    @staticmethod\n    def write_str(uri: str, data: str) -> None:\n        pass\n\n    @staticmethod\n    def write_bytes(uri: str, data: bytes) -> None:\n        pass\n\n    @staticmethod\n    def sync_from_dir(src_dir_uri: str,\n                      dest_dir_uri: str,\n                      delete: bool = False) -> None:\n        pass\n\n    @staticmethod\n    def sync_to_dir(src_dir_uri: str, dest_dir_uri: str,\n                    delete: bool = False) -> None:\n        pass\n\n    @staticmethod\n    def copy_to(src_path: str, dst_uri: str) -> None:\n        pass\n\n    @staticmethod\n    def copy_from(uri: str, path: str) -> None:\n        pass\n\n    @staticmethod\n    def local_path(uri: str, download_dir: str) -> None:\n        pass\n\n\ndef register_plugin(plugin_registry):\n    plugin_registry.register_filesysystem(NoopFileSystem)\n"""
tests/data-files/plugins/noop_label_source.py,0,"b""import rastervision as rv\nfrom rastervision.data import (LabelSource, LabelSourceConfig,\n                               LabelSourceConfigBuilder,\n                               ChipClassificationLabels)\nfrom rastervision.protos.label_source_pb2 \\\n    import LabelSourceConfig as LabelSourceConfigMsg\n\nNOOP_SOURCE = 'NOOP_SOURCE'\n\n\nclass NoopLabelSource(LabelSource):\n    def get_labels(self, window=None):\n        return ChipClassificationLabels()\n\n\nclass NoopLabelSourceConfig(LabelSourceConfig):\n    def __init__(self):\n        super().__init__(NOOP_SOURCE)\n\n    def to_proto(self):\n        msg = super().to_proto()\n        msg.MergeFrom(LabelSourceConfigMsg(custom_config={}))\n        return msg\n\n    def create_source(self, task_config, extent, crs_transformer, tmp_dir):\n        return NoopLabelSource()\n\n    def report_io(self, command_type, io_def):\n        pass\n\n\nclass NoopLabelSourceConfigBuilder(LabelSourceConfigBuilder):\n    def __init__(self, prev=None):\n        super().__init__(NoopLabelSourceConfig, {})\n\n    def from_proto(self, msg):\n        return self\n\n\ndef register_plugin(plugin_registry):\n    plugin_registry.register_config_builder(rv.LABEL_SOURCE, NOOP_SOURCE,\n                                            NoopLabelSourceConfigBuilder)\n"""
tests/data-files/plugins/noop_label_store.py,0,"b""import rastervision as rv\nfrom rastervision.data import (LabelStore, LabelStoreConfig,\n                               LabelStoreConfigBuilder,\n                               ChipClassificationLabels)\nfrom rastervision.protos.label_store_pb2 \\\n    import LabelStoreConfig as LabelStoreConfigMsg\n\nNOOP_STORE = 'NOOP_STORE'\n\n\nclass NoopLabelStore(LabelStore):\n    def save(self, labels):\n        pass\n\n    def get_labels(self):\n        return ChipClassificationLabels()\n\n    def empty_labels(self):\n        return ChipClassificationLabels()\n\n\nclass NoopLabelStoreConfig(LabelStoreConfig):\n    def __init__(self):\n        super().__init__(NOOP_STORE)\n\n    def to_proto(self):\n        msg = super().to_proto()\n        msg.MergeFrom(LabelStoreConfigMsg(custom_config={}))\n        return msg\n\n    def create_store(self, task_config, extent, crs_transformer, tmp_dir):\n        return NoopLabelStore()\n\n    def report_io(self, command_type, io_def):\n        pass\n\n    def for_prediction(self, label_store_uri):\n        return self\n\n\nclass NoopLabelStoreConfigBuilder(LabelStoreConfigBuilder):\n    def __init__(self, prev=None):\n        super().__init__(NoopLabelStoreConfig, {})\n\n    def from_proto(self, msg):\n        return self\n\n\ndef register_plugin(plugin_registry):\n    plugin_registry.register_config_builder(rv.LABEL_STORE, NOOP_STORE,\n                                            NoopLabelStoreConfigBuilder)\n"""
tests/data-files/plugins/noop_raster_source.py,0,"b""import numpy as np\n\nimport rastervision as rv\nfrom rastervision.core import Box\nfrom rastervision.data import (RasterSource, RasterSourceConfig,\n                               RasterSourceConfigBuilder,\n                               IdentityCRSTransformer)\nfrom rastervision.protos.raster_source_pb2 \\\n    import RasterSourceConfig as RasterSourceConfigMsg\n\nNOOP_SOURCE = 'NOOP_SOURCE'\n\n\nclass NoopRasterSource(RasterSource):\n    def get_extent(self):\n        Box.make_square(0, 0, 2)\n\n    def get_dtype(self):\n        return np.uint8\n\n    def get_crs_transformer(self):\n        return IdentityCRSTransformer()\n\n    def _get_chip(self, window):\n        return np.random.rand(1, 2, 2, 3)\n\n\nclass NoopRasterSourceConfig(RasterSourceConfig):\n    def __init__(self, transformers=None, channel_order=None):\n        super().__init__(NOOP_SOURCE, transformers, channel_order)\n\n    def to_proto(self):\n        msg = super().to_proto()\n        msg.MergeFrom(RasterSourceConfigMsg(custom_config={}))\n        return msg\n\n    def create_source(self, tmp_dir):\n        transformers = self.create_transformers()\n        return NoopRasterSource(transformers, tmp_dir)\n\n    def report_io(self, command_type, io_def):\n        pass\n\n    def save_bundle_files(self, bundle_dir):\n        return (self, [])\n\n    def load_bundle_files(self, bundle_dir):\n        return self\n\n    def for_prediction(self, image_uri):\n        return self\n\n    def create_local(self, tmp_dir):\n        return self\n\n\nclass NoopRasterSourceConfigBuilder(RasterSourceConfigBuilder):\n    def __init__(self, prev=None):\n        super().__init__(NoopRasterSourceConfig, {})\n\n    def from_proto(self, msg):\n        return super().from_proto(msg)\n\n\ndef register_plugin(plugin_registry):\n    plugin_registry.register_config_builder(rv.RASTER_SOURCE, NOOP_SOURCE,\n                                            NoopRasterSourceConfigBuilder)\n"""
tests/data-files/plugins/noop_raster_transformer.py,0,"b""import rastervision as rv\nfrom rastervision.data import (\n    RasterTransformerConfig, RasterTransformerConfigBuilder,\n    NoopTransformer)  # Already a real noop transformer\nfrom rastervision.protos.raster_transformer_pb2 \\\n    import RasterTransformerConfig as RasterTransformerConfigMsg\n\nNOOP_TRANSFORMER = 'NOOP_TRANSFORMER'\n\n\nclass NoopRasterTransformerConfig(RasterTransformerConfig):\n    def __init__(self):\n        super().__init__(NOOP_TRANSFORMER)\n\n    def to_proto(self):\n        msg = RasterTransformerConfigMsg(\n            transformer_type=self.transformer_type, custom_config={})\n        return msg\n\n    def create_transformer(self):\n        return NoopTransformer()\n\n    def report_io(self, command_type, io_def):\n        pass\n\n    def save_bundle_files(self, bundle_dir):\n        return (self, [])\n\n    def load_bundle_files(self, bundle_dir):\n        return self\n\n    def for_prediction(self, image_uri):\n        return self\n\n    def create_local(self, tmp_dir):\n        return self\n\n\nclass NoopRasterTransformerConfigBuilder(RasterTransformerConfigBuilder):\n    def __init__(self, prev=None):\n        super().__init__(NoopRasterTransformerConfig, {})\n\n    def from_proto(self, msg):\n        return self\n\n\ndef register_plugin(plugin_registry):\n    plugin_registry.register_config_builder(\n        rv.RASTER_TRANSFORMER, NOOP_TRANSFORMER,\n        NoopRasterTransformerConfigBuilder)\n"""
tests/data-files/plugins/noop_runner.py,0,"b""from rastervision.runner import (ExperimentRunner, CommandRunner)\n\nNOOP_RUNNER = 'NOOP_RUNNER'\n\n\nclass NoopExperimentRunner(ExperimentRunner):\n    def _run_experiment(self, command_dag):\n        for command_config in command_dag.get_sorted_commands():\n            msg = command_config.to_proto()\n            CommandRunner.run_from_proto(msg)\n\n\ndef register_plugin(plugin_registry):\n    plugin_registry.register_experiment_runner(NOOP_RUNNER,\n                                               NoopExperimentRunner)\n"""
tests/data-files/plugins/noop_task.py,0,"b""import rastervision as rv\nfrom rastervision.task import (Task, TaskConfig, TaskConfigBuilder)\nfrom rastervision.protos.task_pb2 import TaskConfig as TaskConfigMsg\n\nNOOP_TASK = 'NOOP_TASK'\n\n\nclass NoopTask(Task):\n    def get_train_windows(self, scene):\n        return []\n\n    def get_train_labels(self, window, scene):\n        return []\n\n    def post_process_predictions(self, labels):\n        return labels\n\n    def get_predict_windows(self, extent):\n        return []\n\n    def save_debug_predict_image(self, scene, debug_dir_uri):\n        pass\n\n    def make_chips(self, train_scenes, validation_scenes, augmentors, tmp_dir):\n        pass\n\n    def train(self, tmp_dir):\n        pass\n\n    def predict(self, scenes, tmp_dir):\n        pass\n\n\nclass NoopTaskConfig(TaskConfig):\n    def __init__(self):\n        super().__init__(NOOP_TASK)\n\n    def to_proto(self):\n        msg = TaskConfigMsg(task_type=self.task_type, custom_config={})\n        return msg\n\n    def create_task(self, backend):\n        return NoopTask(self, backend)\n\n    def save_bundle_files(self, bundle_dir):\n        return (self, [])\n\n    def load_bundle_files(self, bundle_dir):\n        return self\n\n\nclass NoopTaskConfigBuilder(TaskConfigBuilder):\n    def __init__(self, prev=None):\n        super().__init__(NoopTaskConfig, {})\n\n    def from_proto(self, msg):\n        return self\n\n\ndef register_plugin(plugin_registry):\n    plugin_registry.register_config_builder(rv.TASK, NOOP_TASK,\n                                            NoopTaskConfigBuilder)\n"""
tests/data-files/plugins/noop_utils.py,0,b'# Used to test import statements for the NoopAugmentor plugin.\n\n\ndef noop(x):\n    return x\n'
tests/data/label/__init__.py,0,b''
tests/data/label/test_chip_classification_labels.py,0,"b""import unittest\n\nfrom rastervision.core.box import Box\nfrom rastervision.data.label.chip_classification_labels import ChipClassificationLabels\n\n\nclass TestChipClassificationLabels(unittest.TestCase):\n    def setUp(self):\n        self.labels = ChipClassificationLabels()\n\n        self.cell1 = Box.make_square(0, 0, 2)\n        self.class_id1 = 1\n        self.labels.set_cell(self.cell1, self.class_id1)\n\n        self.cell2 = Box.make_square(0, 2, 2)\n        self.class_id2 = 2\n        self.labels.set_cell(self.cell2, self.class_id2)\n\n    def test_get_cell(self):\n        cell = Box.make_square(0, 2, 3)\n        class_id = self.labels.get_cell_class_id(cell)\n        self.assertEqual(class_id, None)\n\n        class_id = self.labels.get_cell_class_id(self.cell1)\n        self.assertEqual(class_id, self.class_id1)\n\n        class_id = self.labels.get_cell_class_id(self.cell2)\n        self.assertEqual(class_id, self.class_id2)\n\n    def test_get_singleton_labels(self):\n        labels = self.labels.get_singleton_labels(self.cell1)\n\n        cells = labels.get_cells()\n        self.assertEqual(len(cells), 1)\n\n        class_id = labels.get_cell_class_id(self.cell1)\n        self.assertEqual(class_id, self.class_id1)\n\n    def test_get_cells(self):\n        cells = self.labels.get_cells()\n        self.assertEqual(len(cells), 2)\n        # ordering of cells isn't known\n        self.assertTrue((cells[0] == self.cell1 and cells[1] == self.cell2)\n                        or (cells[1] == self.cell1 and cells[0] == self.cell2))\n\n    def test_get_class_ids(self):\n        cells = self.labels.get_cells()\n        class_ids = self.labels.get_class_ids()\n        # check that order of class_ids corresponds to order of cells\n        if (cells[0] == self.cell1 and cells[1] == self.cell2):\n            self.assertListEqual(class_ids, [1, 2])\n        elif (cells[1] == self.cell1 and cells[0] == self.cell2):\n            self.assertListEqual(class_ids, [2, 1])\n\n    def test_extend(self):\n        labels = ChipClassificationLabels()\n        cell3 = Box.make_square(0, 4, 2)\n        class_id3 = 1\n        labels.set_cell(cell3, class_id3)\n\n        self.labels.extend(labels)\n        cells = self.labels.get_cells()\n        self.assertEqual(len(cells), 3)\n        self.assertTrue(cell3 in cells)\n\n    def test_filter_by_aoi(self):\n        aois = [Box.make_square(0, 0, 2).to_shapely()]\n        filt_labels = self.labels.filter_by_aoi(aois)\n\n        exp_labels = ChipClassificationLabels()\n        cell1 = Box.make_square(0, 0, 2)\n        class_id1 = 1\n        exp_labels.set_cell(cell1, class_id1)\n        self.assertEqual(filt_labels, exp_labels)\n\n        aois = [Box.make_square(4, 4, 2).to_shapely()]\n        filt_labels = self.labels.filter_by_aoi(aois)\n\n        exp_labels = ChipClassificationLabels()\n        self.assertEqual(filt_labels, exp_labels)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests/data/label/test_object_detection_labels.py,0,"b""import unittest\n\nimport numpy as np\n\nfrom rastervision.core.box import Box\nfrom rastervision.core.class_map import ClassMap, ClassItem\nfrom rastervision.data.label.object_detection_labels import (\n    ObjectDetectionLabels)\n\n\nclass ObjectDetectionLabelsTest(unittest.TestCase):\n    def setUp(self):\n        self.class_map = ClassMap([ClassItem(1, 'car'), ClassItem(2, 'house')])\n\n        self.npboxes = np.array([\n            [0., 0., 2., 2.],\n            [2., 2., 4., 4.],\n        ])\n        self.class_ids = np.array([1, 2])\n        self.scores = np.array([0.9, 0.9])\n        self.labels = ObjectDetectionLabels(\n            self.npboxes, self.class_ids, scores=self.scores)\n\n    def test_from_boxlist(self):\n        from rastervision.data.label.tfod_utils.np_box_list import BoxList\n        boxlist = BoxList(self.npboxes)\n        boxlist.add_field('classes', self.class_ids)\n        boxlist.add_field('scores', self.scores)\n        labels = ObjectDetectionLabels.from_boxlist(boxlist)\n        labels.assert_equal(self.labels)\n\n    def test_make_empty(self):\n        npboxes = np.empty((0, 4))\n        class_ids = np.empty((0, ))\n        scores = np.empty((0, ))\n        expected_labels = ObjectDetectionLabels(\n            npboxes, class_ids, scores=scores)\n\n        labels = ObjectDetectionLabels.make_empty()\n        labels.assert_equal(expected_labels)\n\n    def test_constructor(self):\n        labels = ObjectDetectionLabels(\n            self.npboxes, self.class_ids, scores=self.scores)\n        expected_labels = ObjectDetectionLabels(self.npboxes, self.class_ids,\n                                                self.scores)\n        labels.assert_equal(expected_labels)\n\n        labels = ObjectDetectionLabels(self.npboxes, self.class_ids)\n        scores = np.ones(self.class_ids.shape)\n        expected_labels = ObjectDetectionLabels(\n            self.npboxes, self.class_ids, scores=scores)\n        labels.assert_equal(expected_labels)\n\n    def test_get_boxes(self):\n        boxes = self.labels.get_boxes()\n        self.assertEqual(len(boxes), 2)\n        np.testing.assert_array_equal(boxes[0].npbox_format(),\n                                      self.npboxes[0, :])\n        np.testing.assert_array_equal(boxes[1].npbox_format(),\n                                      self.npboxes[1, :])\n\n    def test_len(self):\n        nb_labels = len(self.labels)\n        self.assertEqual(self.npboxes.shape[0], nb_labels)\n\n    def test_local_to_global(self):\n        local_npboxes = np.array([[0., 0., 2., 2.], [2., 2., 4., 4.]])\n        window = Box.make_square(10, 10, 10)\n        global_npboxes = ObjectDetectionLabels.local_to_global(\n            local_npboxes, window)\n\n        expected_global_npboxes = np.array([[10., 10., 12., 12.],\n                                            [12., 12., 14., 14.]])\n        np.testing.assert_array_equal(global_npboxes, expected_global_npboxes)\n\n    def test_global_to_local(self):\n        global_npboxes = np.array([[10., 10., 12., 12.], [12., 12., 14., 14.]])\n        window = Box.make_square(10, 10, 10)\n        local_npboxes = ObjectDetectionLabels.global_to_local(\n            global_npboxes, window)\n\n        expected_local_npboxes = np.array([[0., 0., 2., 2.], [2., 2., 4., 4.]])\n        np.testing.assert_array_equal(local_npboxes, expected_local_npboxes)\n\n    def test_local_to_normalized(self):\n        local_npboxes = np.array([[0., 0., 2., 2.], [2., 2., 4., 4.]])\n        window = Box(0, 0, 10, 100)\n        norm_npboxes = ObjectDetectionLabels.local_to_normalized(\n            local_npboxes, window)\n\n        expected_norm_npboxes = np.array([[0., 0., 0.2, 0.02],\n                                          [0.2, 0.02, 0.4, 0.04]])\n        np.testing.assert_array_equal(norm_npboxes, expected_norm_npboxes)\n\n    def test_normalized_to_local(self):\n        norm_npboxes = np.array([[0., 0., 0.2, 0.02], [0.2, 0.02, 0.4, 0.04]])\n        window = Box(0, 0, 10, 100)\n        local_npboxes = ObjectDetectionLabels.normalized_to_local(\n            norm_npboxes, window)\n\n        expected_local_npboxes = np.array([[0., 0., 2., 2.], [2., 2., 4., 4.]])\n        np.testing.assert_array_equal(local_npboxes, expected_local_npboxes)\n\n    def test_get_overlapping(self):\n        window = Box.make_square(0, 0, 2.01)\n        labels = ObjectDetectionLabels.get_overlapping(self.labels, window)\n        labels.assert_equal(self.labels)\n\n        window = Box.make_square(0, 0, 3)\n        labels = ObjectDetectionLabels.get_overlapping(\n            self.labels, window, ioa_thresh=0.5)\n        npboxes = np.array([[0., 0., 2., 2.]])\n        class_ids = np.array([1])\n        scores = np.array([0.9])\n        expected_labels = ObjectDetectionLabels(\n            npboxes, class_ids, scores=scores)\n        labels.assert_equal(expected_labels)\n\n        window = Box.make_square(0, 0, 3)\n        labels = ObjectDetectionLabels.get_overlapping(\n            self.labels, window, ioa_thresh=0.1, clip=True)\n        expected_npboxes = np.array([\n            [0., 0., 2., 2.],\n            [2., 2., 3., 3.],\n        ])\n        expected_labels = ObjectDetectionLabels(\n            expected_npboxes, self.class_ids, scores=self.scores)\n        labels.assert_equal(expected_labels)\n\n    def test_concatenate(self):\n        npboxes = np.array([[4., 4., 5., 5.]])\n        class_ids = np.array([2])\n        scores = np.array([0.3])\n        labels = ObjectDetectionLabels(npboxes, class_ids, scores=scores)\n        new_labels = ObjectDetectionLabels.concatenate(self.labels, labels)\n\n        npboxes = np.array([[0., 0., 2., 2.], [2., 2., 4., 4.],\n                            [4., 4., 5., 5.]])\n        class_ids = np.array([1, 2, 2])\n        scores = np.array([0.9, 0.9, 0.3])\n        expected_labels = ObjectDetectionLabels(\n            npboxes, class_ids, scores=scores)\n        new_labels.assert_equal(expected_labels)\n\n    def test_prune_duplicates(self):\n        # This first box has a score below score_thresh so it should get\n        # pruned. The third box overlaps with the second, but has higher score,\n        # so the second one should get pruned. The fourth box overlaps with\n        # the second less than merge_thresh, so it should not get pruned.\n        npboxes = np.array([[0., 0., 2., 2.], [2., 2., 4., 4.],\n                            [2.1, 2.1, 4.1, 4.1], [3.5, 3.5, 5.5, 5.5]])\n        class_ids = np.array([1, 2, 1, 2])\n        scores = np.array([0.2, 0.9, 0.9, 1.0])\n        labels = ObjectDetectionLabels(npboxes, class_ids, scores=scores)\n        score_thresh = 0.5\n        merge_thresh = 0.5\n        pruned_labels = ObjectDetectionLabels.prune_duplicates(\n            labels, score_thresh, merge_thresh)\n\n        self.assertEqual(len(pruned_labels), 2)\n\n        expected_npboxes = np.array([[2.1, 2.1, 4.1, 4.1],\n                                     [3.5, 3.5, 5.5, 5.5]])\n        expected_class_ids = np.array([1, 2])\n        expected_scores = np.array([0.9, 1.0])\n\n        # prune_duplicates does not maintain ordering of boxes, so find match\n        # between pruned boxes and expected_npboxes.\n        pruned_npboxes = pruned_labels.get_npboxes()\n        pruned_inds = [None, None]\n        for box_ind, box in enumerate(expected_npboxes):\n            for pruned_box_ind, pruned_box in enumerate(pruned_npboxes):\n                if np.array_equal(pruned_box, box):\n                    pruned_inds[box_ind] = pruned_box_ind\n        self.assertTrue(np.all(pruned_inds is not None))\n\n        expected_labels = ObjectDetectionLabels(\n            expected_npboxes[pruned_inds],\n            expected_class_ids[pruned_inds],\n            scores=expected_scores[pruned_inds])\n        pruned_labels.assert_equal(expected_labels)\n\n    def test_filter_by_aoi(self):\n        aois = [Box.make_square(0, 0, 2).to_shapely()]\n        filt_labels = self.labels.filter_by_aoi(aois)\n\n        npboxes = np.array([[0., 0., 2., 2.]])\n        class_ids = np.array([1])\n        scores = np.array([0.9])\n        exp_labels = ObjectDetectionLabels(npboxes, class_ids, scores=scores)\n        self.assertEqual(filt_labels, exp_labels)\n\n        aois = [Box.make_square(4, 4, 2).to_shapely()]\n        filt_labels = self.labels.filter_by_aoi(aois)\n        exp_labels = ObjectDetectionLabels.make_empty()\n        self.assertEqual(filt_labels, exp_labels)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests/data/label/test_semantic_segmentation_labels.py,0,"b""import unittest\n\nimport numpy as np\n\nfrom rastervision.core.box import Box\nfrom rastervision.data.label import SemanticSegmentationLabels\n\n\nclass TestSemanticSegmentationLabels(unittest.TestCase):\n    def setUp(self):\n        self.windows = [Box.make_square(0, 0, 10), Box.make_square(0, 10, 10)]\n        self.label_arr0 = np.random.choice([1, 2], (10, 10))\n        self.label_arr1 = np.random.choice([1, 2], (10, 10))\n\n        def label_fn(window):\n            if window == self.windows[0]:\n                return self.label_arr0.copy()\n            elif window == self.windows[1]:\n                return self.label_arr1.copy()\n            else:\n                raise ValueError('Unknown window: {}'.format(window))\n\n        self.label_fn = label_fn\n        self.labels = SemanticSegmentationLabels(self.windows, label_fn)\n\n    def test_get(self):\n        np.testing.assert_array_equal(\n            self.labels.get_label_arr(self.windows[0]), self.label_arr0)\n\n    def test_get_with_aoi(self):\n        aoi_polygons = [Box.make_square(5, 15, 2).to_shapely()]\n        exp_label_arr = self.label_arr1.copy()\n        mask = np.zeros(exp_label_arr.shape)\n        mask[5:7, 5:7] = 1\n        exp_label_arr = exp_label_arr * mask\n\n        labels = self.labels.filter_by_aoi(aoi_polygons)\n        label_arr = labels.get_label_arr(self.windows[1])\n        np.testing.assert_array_equal(label_arr, exp_label_arr)\n\n        # Set clip_extent\n        clip_extent = Box(0, 0, 10, 18)\n        label_arr = labels.get_label_arr(\n            self.windows[1], clip_extent=clip_extent)\n        np.testing.assert_array_equal(label_arr, exp_label_arr[:, 0:8])\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests/data/label_source/__init__.py,0,b''
tests/data/label_source/test_chip_classification_label_source.py,0,"b""import unittest\nimport os\n\nfrom shapely.geometry import shape\nfrom shapely.strtree import STRtree\n\nimport rastervision as rv\nfrom rastervision.rv_config import RVConfig\nfrom rastervision.data.label_source import (infer_cell)\nfrom rastervision.core.box import Box\nfrom rastervision.core.class_map import ClassMap, ClassItem\nfrom rastervision.utils.files import json_to_file\n\nfrom tests.data.mock_crs_transformer import DoubleCRSTransformer\n\n\nclass TestChipClassificationLabelSource(unittest.TestCase):\n    def setUp(self):\n        self.crs_transformer = DoubleCRSTransformer()\n        self.geojson = {\n            'type':\n            'FeatureCollection',\n            'features': [{\n                'type': 'Feature',\n                'geometry': {\n                    'type':\n                    'MultiPolygon',\n                    'coordinates': [[[[0., 0.], [0., 2.], [2., 2.], [2., 0.],\n                                      [0., 0.]]]]\n                },\n                'properties': {\n                    'class_name': 'car',\n                    'class_id': 1,\n                    'score': 0.0\n                }\n            }, {\n                'type': 'Feature',\n                'geometry': {\n                    'type':\n                    'Polygon',\n                    'coordinates': [[[2., 2.], [2., 4.], [4., 4.], [4., 2.],\n                                     [2., 2.]]]\n                },\n                'properties': {\n                    'score': 0.0,\n                    'class_name': 'house',\n                    'class_id': 2\n                }\n            }]\n        }\n\n        self.class_map = ClassMap([ClassItem(1, 'car'), ClassItem(2, 'house')])\n\n        class MockTaskConfig():\n            def __init__(self, class_map):\n                self.class_map = class_map\n\n        self.task_config = MockTaskConfig(self.class_map)\n\n        self.box1 = Box.make_square(0, 0, 4)\n        self.box2 = Box.make_square(4, 4, 4)\n        self.class_id1 = 1\n        self.class_id2 = 2\n        self.background_class_id = 3\n\n        geoms = []\n        for f in self.geojson['features']:\n            g = shape(f['geometry'])\n            g.class_id = f['properties']['class_id']\n            geoms.append(g)\n        self.str_tree = STRtree(geoms)\n\n        self.file_name = 'labels.json'\n        self.temp_dir = RVConfig.get_tmp_dir()\n        self.uri = os.path.join(self.temp_dir.name, self.file_name)\n        json_to_file(self.geojson, self.uri)\n\n    def tearDown(self):\n        self.temp_dir.cleanup()\n\n    def test_infer_cell1(self):\n        # More of box 1 is in cell.\n        cell = Box.make_square(0, 0, 3)\n        ioa_thresh = 0.5\n        use_intersection_over_cell = False\n        background_class_id = None\n        pick_min_class_id = False\n\n        class_id = infer_cell(cell, self.str_tree, ioa_thresh,\n                              use_intersection_over_cell, background_class_id,\n                              pick_min_class_id)\n        self.assertEqual(class_id, self.class_id1)\n\n    def test_infer_cell2(self):\n        # More of box 2 is in cell.\n        cell = Box.make_square(1, 1, 3)\n        ioa_thresh = 0.5\n        use_intersection_over_cell = False\n        background_class_id = None\n        pick_min_class_id = False\n\n        class_id = infer_cell(cell, self.str_tree, ioa_thresh,\n                              use_intersection_over_cell, background_class_id,\n                              pick_min_class_id)\n        self.assertEqual(class_id, self.class_id2)\n\n    def test_infer_cell3(self):\n        # Only box 2 is in cell, but IOA isn't high enough.\n        cell = Box.make_square(3, 3, 3)\n        ioa_thresh = 0.5\n        use_intersection_over_cell = False\n        background_class_id = None\n        pick_min_class_id = False\n\n        class_id = infer_cell(cell, self.str_tree, ioa_thresh,\n                              use_intersection_over_cell, background_class_id,\n                              pick_min_class_id)\n        self.assertEqual(class_id, None)\n\n    def test_infer_cell4(self):\n        # Both boxes inside cell, but using intersection_over_cell,\n        # the IOA isn't high enough.\n        cell = Box.make_square(0, 0, 10)\n        ioa_thresh = 0.5\n        use_intersection_over_cell = True\n        background_class_id = None\n        pick_min_class_id = False\n\n        class_id = infer_cell(cell, self.str_tree, ioa_thresh,\n                              use_intersection_over_cell, background_class_id,\n                              pick_min_class_id)\n        self.assertEqual(class_id, None)\n\n    def test_infer_cell5(self):\n        # More of box1 in cell, using intersection_over_cell with the\n        # IOA high enough.\n        cell = Box.make_square(0, 0, 3)\n        ioa_thresh = 0.4\n        use_intersection_over_cell = True\n        background_class_id = None\n        pick_min_class_id = False\n\n        class_id = infer_cell(cell, self.str_tree, ioa_thresh,\n                              use_intersection_over_cell, background_class_id,\n                              pick_min_class_id)\n        self.assertEqual(class_id, self.class_id1)\n\n    def test_infer_cell6(self):\n        # No boxes overlap enough, use background_class_id\n        cell = Box.make_square(0, 0, 10)\n        ioa_thresh = 0.5\n        use_intersection_over_cell = True\n        background_class_id = self.background_class_id\n        pick_min_class_id = False\n\n        class_id = infer_cell(cell, self.str_tree, ioa_thresh,\n                              use_intersection_over_cell, background_class_id,\n                              pick_min_class_id)\n        self.assertEqual(class_id, self.background_class_id)\n\n    def test_infer_cell7(self):\n        # Cell doesn't overlap with any boxes.\n        cell = Box.make_square(10, 10, 1)\n        ioa_thresh = 0.5\n        use_intersection_over_cell = True\n        background_class_id = None\n        pick_min_class_id = False\n\n        class_id = infer_cell(cell, self.str_tree, ioa_thresh,\n                              use_intersection_over_cell, background_class_id,\n                              pick_min_class_id)\n        self.assertEqual(class_id, None)\n\n    def test_infer_cell8(self):\n        # box2 overlaps more than box1, but using pick_min_class_id, so\n        # picks box1.\n        cell = Box.make_square(1, 1, 3)\n        ioa_thresh = 0.5\n        use_intersection_over_cell = False\n        background_class_id = None\n        pick_min_class_id = True\n\n        class_id = infer_cell(cell, self.str_tree, ioa_thresh,\n                              use_intersection_over_cell, background_class_id,\n                              pick_min_class_id)\n        self.assertEqual(class_id, self.class_id2)\n\n    def test_get_labels_inferred(self):\n        extent = Box.make_square(0, 0, 8)\n\n        msg = rv.LabelSourceConfig.builder(rv.CHIP_CLASSIFICATION) \\\n                .with_uri(self.uri) \\\n                .with_ioa_thresh(0.5) \\\n                .with_use_intersection_over_cell(False) \\\n                .with_pick_min_class_id(False) \\\n                .with_background_class_id(self.background_class_id) \\\n                .with_infer_cells(True) \\\n                .with_cell_size(4) \\\n                .build().to_proto()\n        config = rv.LabelSourceConfig.builder(rv.CHIP_CLASSIFICATION) \\\n                   .from_proto(msg).build()\n        source = config.create_source(self.task_config, extent,\n                                      self.crs_transformer, self.temp_dir.name)\n\n        labels = source.get_labels()\n        cells = labels.get_cells()\n\n        self.assertEqual(len(cells), 4)\n        self.assertEqual(labels.get_cell_class_id(self.box1), self.class_id1)\n        self.assertEqual(labels.get_cell_class_id(self.box2), self.class_id2)\n        self.assertEqual(\n            labels.get_cell_class_id(Box.make_square(0, 4, 4)),\n            self.background_class_id)\n        self.assertEqual(\n            labels.get_cell_class_id(Box.make_square(4, 0, 4)),\n            self.background_class_id)\n\n    def test_get_labels_small_extent(self):\n        # Extent only has enough of first box in it.\n        extent = Box.make_square(0, 0, 2)\n\n        msg = rv.LabelSourceConfig.builder(rv.CHIP_CLASSIFICATION) \\\n                .with_uri(self.uri) \\\n                .build().to_proto()\n        config = rv.LabelSourceConfig.builder(rv.CHIP_CLASSIFICATION) \\\n                   .from_proto(msg).build()\n        source = config.create_source(self.task_config, extent,\n                                      self.crs_transformer, self.temp_dir.name)\n        labels = source.get_labels()\n\n        cells = labels.get_cells()\n        self.assertEqual(len(cells), 1)\n        class_id = labels.get_cell_class_id(self.box1)\n        self.assertEqual(class_id, self.class_id1)\n        class_id = labels.get_cell_class_id(self.box2)\n        self.assertEqual(class_id, None)\n\n    def test_get_labels(self):\n        # Extent contains both boxes.\n        extent = Box.make_square(0, 0, 8)\n\n        msg = rv.LabelSourceConfig.builder(rv.CHIP_CLASSIFICATION) \\\n                .with_uri(self.uri) \\\n                .build().to_proto()\n        config = rv.LabelSourceConfig.builder(rv.CHIP_CLASSIFICATION) \\\n                   .from_proto(msg).build()\n        source = config.create_source(self.task_config, extent,\n                                      self.crs_transformer, self.temp_dir.name)\n        labels = source.get_labels()\n\n        cells = labels.get_cells()\n        self.assertEqual(len(cells), 2)\n        class_id = labels.get_cell_class_id(self.box1)\n        self.assertEqual(class_id, self.class_id1)\n        class_id = labels.get_cell_class_id(self.box2)\n        self.assertEqual(class_id, self.class_id2)\n\n    def test_missing_config_uri(self):\n        with self.assertRaises(rv.ConfigError):\n            rv.data.ChipClassificationLabelSourceConfig.builder(\n                rv.CHIP_CLASSIFICATION).build()\n\n    def test_no_missing_config(self):\n        try:\n            rv.data.ChipClassificationLabelSourceConfig.builder(\n                rv.CHIP_CLASSIFICATION).with_uri('x.geojson').build()\n        except rv.ConfigError:\n            self.fail('ConfigError raised unexpectedly')\n\n    def test_deprecated_builder(self):\n        try:\n            rv.LabelSourceConfig.builder(rv.CHIP_CLASSIFICATION_GEOJSON) \\\n              .with_uri('x.geojson') \\\n              .build()\n        except rv.ConfigError:\n            self.fail('ConfigError raised unexpectedly')\n\n    def test_using_null_class_bufs(self):\n        vs = rv.VectorSourceConfig.builder(rv.GEOJSON_SOURCE) \\\n            .with_uri(self.uri) \\\n            .with_buffers(point_bufs={1: None}) \\\n            .build()\n        with self.assertRaises(rv.ConfigError):\n            rv.LabelSourceConfig.builder(rv.CHIP_CLASSIFICATION) \\\n                .with_vector_source(vs) \\\n                .build()\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests/data/label_source/test_object_detection_label_source.py,0,"b""import unittest\nimport os\n\nimport numpy as np\nfrom moto import mock_s3\n\nimport rastervision as rv\n\nfrom rastervision.data.label_source import ObjectDetectionLabelSource\nfrom rastervision.data import ObjectDetectionLabels\nfrom rastervision.core.box import Box\nfrom rastervision.core.class_map import ClassMap, ClassItem\nfrom rastervision.filesystem import NotReadableError\nfrom rastervision.rv_config import RVConfig\nfrom rastervision.data.crs_transformer import IdentityCRSTransformer\nfrom rastervision.utils.files import json_to_file\n\nfrom tests import data_file_path\nfrom tests.data.mock_crs_transformer import DoubleCRSTransformer\n\n\nclass TestObjectDetectionLabelSource(unittest.TestCase):\n    def setUp(self):\n        self.prev_keys = (os.environ.get('AWS_ACCESS_KEY_ID'),\n                          os.environ.get('AWS_SECRET_ACCESS_KEY'))\n        os.environ['AWS_ACCESS_KEY_ID'] = 'DUMMY'\n        os.environ['AWS_SECRET_ACCESS_KEY'] = 'DUMMY'\n        self.mock_s3 = mock_s3()\n        self.mock_s3.start()\n\n        self.file_name = 'labels.json'\n        self.temp_dir = RVConfig.get_tmp_dir()\n        self.file_path = os.path.join(self.temp_dir.name, self.file_name)\n\n        self.crs_transformer = DoubleCRSTransformer()\n        self.geojson = {\n            'type':\n            'FeatureCollection',\n            'features': [{\n                'type': 'Feature',\n                'geometry': {\n                    'type':\n                    'Polygon',\n                    'coordinates': [[[0., 0.], [0., 1.], [1., 1.], [1., 0.],\n                                     [0., 0.]]]\n                },\n                'properties': {\n                    'class_id': 1,\n                    'score': 0.9\n                }\n            }, {\n                'type': 'Feature',\n                'geometry': {\n                    'type':\n                    'Polygon',\n                    'coordinates': [[[1., 1.], [1., 2.], [2., 2.], [2., 1.],\n                                     [1., 1.]]]\n                },\n                'properties': {\n                    'score': 0.9,\n                    'class_id': 2\n                }\n            }]\n        }\n\n        self.extent = Box.make_square(0, 0, 10)\n        self.class_map = ClassMap([ClassItem(1, 'car'), ClassItem(2, 'house')])\n\n        json_to_file(self.geojson, self.file_path)\n\n    def tearDown(self):\n        access, secret = self.prev_keys\n        if access:\n            os.environ['AWS_ACCESS_KEY_ID'] = access\n        else:\n            del os.environ['AWS_ACCESS_KEY_ID']\n        if secret:\n            os.environ['AWS_SECRET_ACCESS_KEY'] = secret\n        else:\n            del os.environ['AWS_SECRET_ACCESS_KEY']\n\n        self.mock_s3.stop()\n        self.temp_dir.cleanup()\n\n    def test_read_invalid_uri_readable_true(self):\n        with self.assertRaises(NotReadableError):\n            invalid_uri = 's3://invalid_path/invalid.json'\n            ObjectDetectionLabelSource(\n                invalid_uri,\n                self.crs_transformer,\n                self.class_map,\n                extent=self.extent)\n\n    def test_read_without_extent(self):\n        store = ObjectDetectionLabelSource(\n            self.file_path, self.crs_transformer, self.class_map, extent=None)\n        labels = store.get_labels()\n\n        npboxes = np.array([[0., 0., 2., 2.], [2., 2., 4., 4.]])\n        class_ids = np.array([1, 2])\n        scores = np.array([0.9, 0.9])\n        expected_labels = ObjectDetectionLabels(\n            npboxes, class_ids, scores=scores)\n        labels.assert_equal(expected_labels)\n\n    def test_read_with_extent(self):\n        # Extent only includes the first box.\n        extent = Box.make_square(0, 0, 3)\n        store = ObjectDetectionLabelSource(\n            self.file_path,\n            self.crs_transformer,\n            self.class_map,\n            extent=extent)\n        labels = store.get_labels()\n\n        npboxes = np.array([[0., 0., 2., 2.]])\n        class_ids = np.array([1])\n        scores = np.array([0.9])\n        expected_labels = ObjectDetectionLabels(\n            npboxes, class_ids, scores=scores)\n        labels.assert_equal(expected_labels)\n\n        # Extent includes both boxes, but clips the second.\n        extent = Box.make_square(0, 0, 3.9)\n        store = ObjectDetectionLabelSource(\n            self.file_path,\n            self.crs_transformer,\n            self.class_map,\n            extent=extent)\n        labels = store.get_labels()\n\n        npboxes = np.array([[0., 0., 2., 2.], [2., 2., 3.9, 3.9]])\n        class_ids = np.array([1, 2])\n        scores = np.array([0.9, 0.9])\n        expected_labels = ObjectDetectionLabels(\n            npboxes, class_ids, scores=scores)\n        labels.assert_equal(expected_labels)\n\n    def test_missing_config_uri(self):\n        with self.assertRaises(rv.ConfigError):\n            rv.data.ObjectDetectionLabelSourceConfig.builder(\n                rv.OBJECT_DETECTION).build()\n\n    def test_no_missing_config(self):\n        try:\n            rv.data.ObjectDetectionLabelSourceConfig.builder(\n                rv.OBJECT_DETECTION).with_uri('x.geojson').build()\n        except rv.ConfigError:\n            self.fail('ConfigError raised unexpectedly')\n\n    def test_deprecated_builder(self):\n        try:\n            rv.LabelSourceConfig.builder(rv.OBJECT_DETECTION_GEOJSON) \\\n              .with_uri('x.geojson') \\\n              .build()\n        except rv.ConfigError:\n            self.fail('ConfigError raised unexpectedly')\n\n    def test_builder(self):\n        uri = data_file_path('polygon-labels.geojson')\n        msg = rv.LabelSourceConfig.builder(rv.OBJECT_DETECTION) \\\n                .with_vector_source(uri) \\\n                .build().to_proto()\n        config = rv.LabelSourceConfig.builder(rv.OBJECT_DETECTION) \\\n                   .from_proto(msg).build()\n        self.assertEqual(config.vector_source.uri, uri)\n\n        classes = ['one', 'two']\n        extent = Box.make_square(0, 0, 10)\n        crs_transformer = IdentityCRSTransformer()\n        with RVConfig.get_tmp_dir() as tmp_dir:\n            task_config = rv.TaskConfig.builder(rv.OBJECT_DETECTION) \\\n                            .with_classes(classes) \\\n                            .build()\n            config.create_source(task_config, extent, crs_transformer, tmp_dir)\n\n    def test_using_null_class_bufs(self):\n        uri = data_file_path('polygon-labels.geojson')\n        vs = rv.VectorSourceConfig.builder(rv.GEOJSON_SOURCE) \\\n                .with_uri(uri) \\\n                .with_buffers(line_bufs={1: None}) \\\n                .build()\n        with self.assertRaises(rv.ConfigError):\n            rv.LabelSourceConfig.builder(rv.OBJECT_DETECTION) \\\n                .with_vector_source(vs) \\\n                .build()\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests/data/label_source/test_segmentation_class_transformer.py,0,"b""import unittest\n\nimport numpy as np\n\nfrom rastervision.data.label_source.segmentation_class_transformer \\\n    import (SegmentationClassTransformer, color_to_triple)\nfrom rastervision.core.class_map import ClassMap, ClassItem\n\n\nclass TestSegmentationClassTransformer(unittest.TestCase):\n    def setUp(self):\n        self.class_map = ClassMap([\n            ClassItem(id=1, color='red'),\n            ClassItem(id=2, color='green'),\n            ClassItem(id=3, color='blue')\n        ])\n        self.transformer = SegmentationClassTransformer(self.class_map)\n\n        self.rgb_image = np.zeros((1, 3, 3))\n        self.rgb_image[0, 0, :] = color_to_triple('red')\n        self.rgb_image[0, 1, :] = color_to_triple('green')\n        self.rgb_image[0, 2, :] = color_to_triple('blue')\n\n        self.class_image = np.array([[1, 2, 3]])\n\n    def test_rgb_to_class(self):\n        class_image = self.transformer.rgb_to_class(self.rgb_image)\n        expected_class_image = self.class_image\n        np.testing.assert_array_equal(class_image, expected_class_image)\n\n    def test_class_to_rgb(self):\n        rgb_image = self.transformer.class_to_rgb(self.class_image)\n        expected_rgb_image = self.rgb_image\n        np.testing.assert_array_equal(rgb_image, expected_rgb_image)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests/data/label_source/test_semantic_segmentation_label_source.py,0,"b""import unittest\n\nimport numpy as np\n\nimport rastervision as rv\nfrom rastervision.core.box import Box\nfrom rastervision.core.class_map import ClassMap, ClassItem\nfrom rastervision.data.label_source.semantic_segmentation_label_source import (\n    SemanticSegmentationLabelSource)\nfrom tests.mock import MockRasterSource\n\n\nclass TestSemanticSegmentationLabelSource(unittest.TestCase):\n    def test_enough_target_pixels_true(self):\n        data = np.zeros((10, 10, 3), dtype=np.uint8)\n        data[4:, 4:, :] = [1, 1, 1]\n        raster_source = MockRasterSource([0, 1, 2], 3)\n        raster_source.set_raster(data)\n        rgb_class_map = ClassMap([ClassItem(id=1, color='#010101')])\n        label_source = SemanticSegmentationLabelSource(\n            source=raster_source, rgb_class_map=rgb_class_map)\n        with label_source.activate():\n            extent = Box(0, 0, 10, 10)\n            self.assertTrue(label_source.enough_target_pixels(extent, 30, [1]))\n\n    def test_enough_target_pixels_false(self):\n        data = np.zeros((10, 10, 3), dtype=np.uint8)\n        data[7:, 7:, :] = [1, 1, 1]\n        raster_source = MockRasterSource([0, 1, 2], 3)\n        raster_source.set_raster(data)\n        rgb_class_map = ClassMap([ClassItem(id=1, color='#010101')])\n        label_source = SemanticSegmentationLabelSource(\n            source=raster_source, rgb_class_map=rgb_class_map)\n        with label_source.activate():\n            extent = Box(0, 0, 10, 10)\n            self.assertFalse(\n                label_source.enough_target_pixels(extent, 30, [1]))\n\n    def test_get_labels(self):\n        data = np.zeros((10, 10, 1), dtype=np.uint8)\n        data[7:, 7:, 0] = 1\n        raster_source = MockRasterSource([0, 1, 2], 3)\n        raster_source.set_raster(data)\n        label_source = SemanticSegmentationLabelSource(source=raster_source)\n        with label_source.activate():\n            window = Box.make_square(7, 7, 3)\n            labels = label_source.get_labels(window=window)\n            label_arr = labels.get_label_arr(window)\n            expected_label_arr = np.ones((3, 3))\n            np.testing.assert_array_equal(label_arr, expected_label_arr)\n\n    def test_get_labels_rgb(self):\n        data = np.zeros((10, 10, 3), dtype=np.uint8)\n        data[7:, 7:, :] = [1, 1, 1]\n        raster_source = MockRasterSource([0, 1, 2], 3)\n        raster_source.set_raster(data)\n        rgb_class_map = ClassMap([ClassItem(id=1, color='#010101')])\n        label_source = SemanticSegmentationLabelSource(\n            source=raster_source, rgb_class_map=rgb_class_map)\n        with label_source.activate():\n            window = Box.make_square(7, 7, 3)\n            labels = label_source.get_labels(window=window)\n            label_arr = labels.get_label_arr(window)\n            expected_label_arr = np.ones((3, 3))\n            np.testing.assert_array_equal(label_arr, expected_label_arr)\n\n    def test_build_missing(self):\n        with self.assertRaises(rv.ConfigError):\n            rv.LabelSourceConfig.builder(rv.SEMANTIC_SEGMENTATION) \\\n              .build()\n\n    def test_build(self):\n        try:\n            rv.LabelSourceConfig.builder(rv.SEMANTIC_SEGMENTATION) \\\n              .with_raster_source('x.tif') \\\n              .with_rgb_class_map([]) \\\n              .build()\n        except rv.ConfigError:\n            self.fail('ConfigError raised unexpectedly')\n\n    def test_build_deprecated(self):\n        try:\n            rv.LabelSourceConfig.builder(rv.SEMANTIC_SEGMENTATION_RASTER) \\\n              .with_raster_source('x.tif') \\\n              .with_rgb_class_map([]) \\\n              .build()\n        except rv.ConfigError:\n            self.fail('ConfigError raised unexpectedly')\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests/data/label_store/__init__.py,0,b''
tests/data/label_store/test_chip_classification_geojson_store.py,0,"b""import unittest\nimport os\n\nimport rastervision as rv\nfrom rastervision.core.box import Box\nfrom rastervision.core.class_map import ClassMap, ClassItem\nfrom rastervision.rv_config import RVConfig\nfrom rastervision.utils.files import json_to_file\n\nfrom tests.data.mock_crs_transformer import DoubleCRSTransformer\n\n\nclass TestChipClassificationGeoJSONStore(unittest.TestCase):\n    def setUp(self):\n        self.crs_transformer = DoubleCRSTransformer()\n        self.geojson = {\n            'type':\n            'FeatureCollection',\n            'features': [{\n                'type': 'Feature',\n                'geometry': {\n                    'type':\n                    'Polygon',\n                    'coordinates': [[[0., 0.], [0., 1.], [1., 1.], [1., 0.],\n                                     [0., 0.]]]\n                },\n                'properties': {\n                    'class_name': 'car',\n                    'class_id': 1\n                }\n            }, {\n                'type': 'Feature',\n                'geometry': {\n                    'type':\n                    'Polygon',\n                    'coordinates': [[[1., 1.], [1., 2.], [2., 2.], [2., 1.],\n                                     [1., 1.]]]\n                },\n                'properties': {\n                    'class_name': 'house',\n                    'class_id': 2\n                }\n            }]\n        }\n\n        self.class_map = ClassMap([ClassItem(1, 'car'), ClassItem(2, 'house')])\n\n        class MockTaskConfig():\n            def __init__(self, class_map):\n                self.class_map = class_map\n\n        self.task_config = MockTaskConfig(self.class_map)\n        self.temp_dir = RVConfig.get_tmp_dir()\n        self.uri = os.path.join(self.temp_dir.name, 'labels.json')\n\n        json_to_file(self.geojson, self.uri)\n\n    def tearDown(self):\n        self.temp_dir.cleanup()\n\n    def test_constructor_save(self):\n        # Read it, write it using label_store, read it again, and compare.\n        extent = Box.make_square(0, 0, 10)\n\n        msg = rv.LabelStoreConfig.builder(rv.CHIP_CLASSIFICATION_GEOJSON) \\\n                .with_uri(self.uri) \\\n                .build().to_proto()\n        config = rv.LabelStoreConfig.builder(rv.CHIP_CLASSIFICATION_GEOJSON) \\\n                   .from_proto(msg).build()\n        label_store = config.create_store(\n            self.task_config, extent, self.crs_transformer, self.temp_dir.name)\n\n        labels1 = label_store.get_labels()\n        new_uri = os.path.join(self.temp_dir.name, 'test_save_reload.json')\n        msg = rv.LabelStoreConfig.builder(rv.CHIP_CLASSIFICATION_GEOJSON) \\\n                .with_uri(new_uri) \\\n                .build().to_proto()\n        config = rv.LabelStoreConfig.builder(rv.CHIP_CLASSIFICATION_GEOJSON) \\\n                   .from_proto(msg).build()\n        label_store = config.create_store(\n            self.task_config, extent, self.crs_transformer, self.temp_dir.name)\n        label_store.save(labels1)\n        labels2 = label_store.get_labels()\n\n        self.assertDictEqual(labels1.cell_to_class_id,\n                             labels2.cell_to_class_id)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests/data/label_store/test_object_detection_geojson_store.py,0,"b""import unittest\nimport os\n\nfrom moto import mock_s3\n\nfrom rastervision.data import (ObjectDetectionGeoJSONStore,\n                               ObjectDetectionLabelSource,\n                               ObjectDetectionLabels)\nfrom rastervision.core.box import Box\nfrom rastervision.core.class_map import ClassMap, ClassItem\nfrom rastervision.filesystem import NotWritableError\nfrom rastervision.rv_config import RVConfig\nfrom rastervision.utils.files import json_to_file\n\nfrom tests.data.mock_crs_transformer import DoubleCRSTransformer\n\n\nclass TestObjectDetectionLabelSource(unittest.TestCase):\n    def setUp(self):\n        self.file_name = 'labels.json'\n        self.temp_dir = RVConfig.get_tmp_dir()\n        self.file_path = os.path.join(self.temp_dir.name, self.file_name)\n\n        self.crs_transformer = DoubleCRSTransformer()\n        self.geojson = {\n            'type':\n            'FeatureCollection',\n            'features': [{\n                'type': 'Feature',\n                'geometry': {\n                    'type':\n                    'Polygon',\n                    'coordinates': [[[0., 0.], [0., 1.], [1., 1.], [1., 0.],\n                                     [0., 0.]]]\n                },\n                'properties': {\n                    'class_id': 1,\n                    'score': 0.9\n                }\n            }, {\n                'type': 'Feature',\n                'geometry': {\n                    'type':\n                    'Polygon',\n                    'coordinates': [[[1., 1.], [1., 2.], [2., 2.], [2., 1.],\n                                     [1., 1.]]]\n                },\n                'properties': {\n                    'score': 0.9,\n                    'class_id': 2\n                }\n            }]\n        }\n\n        self.extent = Box.make_square(0, 0, 10)\n        self.class_map = ClassMap([ClassItem(1, 'car'), ClassItem(2, 'house')])\n\n        json_to_file(self.geojson, self.file_path)\n\n    def tearDown(self):\n        self.temp_dir.cleanup()\n\n    @mock_s3\n    def test_write_invalid_uri(self):\n        labels = ObjectDetectionLabels.make_empty()\n        invalid_uri = 's3://invalid_path/invalid.json'\n        label_store = ObjectDetectionGeoJSONStore(\n            invalid_uri, self.crs_transformer, self.class_map)\n        with self.assertRaises(NotWritableError):\n            label_store.save(labels)\n\n    def test_valid_uri(self):\n        # Read it, write it using label_store, read it again, and compare.\n        label_source = ObjectDetectionLabelSource(\n            self.file_path, self.crs_transformer, self.class_map, self.extent)\n        labels1 = label_source.get_labels()\n\n        new_path = os.path.join(self.temp_dir.name, 'test_save_reload.json')\n\n        label_store = ObjectDetectionGeoJSONStore(\n            new_path, self.crs_transformer, self.class_map)\n        label_store.save(labels1)\n\n        label_store = ObjectDetectionLabelSource(\n            self.file_path, self.crs_transformer, self.class_map, self.extent)\n        labels2 = label_store.get_labels()\n\n        labels1.assert_equal(labels2)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests/data/raster_source/__init__.py,0,b''
tests/data/raster_source/test_rasterio_source.py,0,"b""import unittest\nimport os\n\nimport numpy as np\nimport rasterio\nfrom rasterio.enums import ColorInterp\n\nimport rastervision as rv\nfrom rastervision.core import (RasterStats)\nfrom rastervision.utils.misc import save_img\nfrom rastervision.data.raster_source import ChannelOrderError, RasterioSourceConfig\nfrom rastervision.rv_config import RVConfig\nfrom rastervision.utils.files import make_dir\nfrom rastervision.protos.raster_source_pb2 import RasterSourceConfig as RasterSourceMsg\n\nfrom tests import data_file_path\n\n\nclass TestRasterioSource(unittest.TestCase):\n    def test_nodata_val(self):\n        with RVConfig.get_tmp_dir() as temp_dir:\n            # make geotiff filled with ones and zeros with nodata == 1\n            image_path = os.path.join(temp_dir, 'temp.tif')\n            height = 100\n            width = 100\n            nb_channels = 3\n            with rasterio.open(\n                    image_path,\n                    'w',\n                    driver='GTiff',\n                    height=height,\n                    width=width,\n                    count=nb_channels,\n                    dtype=np.uint8,\n                    nodata=1) as image_dataset:\n                im = np.random.randint(\n                    0, 2, (height, width, nb_channels)).astype(np.uint8)\n                for channel in range(nb_channels):\n                    image_dataset.write(im[:, :, channel], channel + 1)\n\n            source = rv.RasterSourceConfig.builder(rv.RASTERIO_SOURCE) \\\n                                          .with_uri(image_path) \\\n                                          .build() \\\n                                          .create_source(tmp_dir=temp_dir)\n            with source.activate():\n                out_chip = source.get_image_array()\n                expected_out_chip = np.zeros((height, width, nb_channels))\n                np.testing.assert_equal(out_chip, expected_out_chip)\n\n    def test_mask(self):\n        with RVConfig.get_tmp_dir() as temp_dir:\n            # make geotiff filled with ones and zeros and mask the whole image\n            image_path = os.path.join(temp_dir, 'temp.tif')\n            height = 100\n            width = 100\n            nb_channels = 3\n            with rasterio.open(\n                    image_path,\n                    'w',\n                    driver='GTiff',\n                    height=height,\n                    width=width,\n                    count=nb_channels,\n                    dtype=np.uint8) as image_dataset:\n                im = np.random.randint(\n                    0, 2, (height, width, nb_channels)).astype(np.uint8)\n                for channel in range(nb_channels):\n                    image_dataset.write(im[:, :, channel], channel + 1)\n                image_dataset.write_mask(\n                    np.zeros(im.shape[0:2]).astype(np.bool))\n\n            source = rv.RasterSourceConfig.builder(rv.RASTERIO_SOURCE) \\\n                                          .with_uri(image_path) \\\n                                          .build() \\\n                                          .create_source(tmp_dir=temp_dir)\n            with source.activate():\n                out_chip = source.get_image_array()\n                expected_out_chip = np.zeros((height, width, nb_channels))\n                np.testing.assert_equal(out_chip, expected_out_chip)\n\n    def test_get_dtype(self):\n        img_path = data_file_path('small-rgb-tile.tif')\n        with RVConfig.get_tmp_dir() as tmp_dir:\n            source = rv.data.RasterioSourceConfig(uris=[img_path]) \\\n                            .create_source(tmp_dir)\n\n            self.assertEqual(source.get_dtype(), np.uint8)\n\n    def test_gets_raw_chip(self):\n        img_path = data_file_path('small-rgb-tile.tif')\n        channel_order = [0, 1]\n\n        source = rv.data.RasterioSourceConfig(uris=[img_path],\n                                              channel_order=channel_order) \\\n                        .create_source(tmp_dir=None)\n\n        with source.activate():\n            out_chip = source.get_raw_image_array()\n            self.assertEqual(out_chip.shape[2], 3)\n\n    def test_shift_x(self):\n        # Specially-engineered image w/ one meter per pixel resolution\n        # in the x direction.\n        img_path = data_file_path('ones.tif')\n        channel_order = [0]\n\n        msg = rv.data.RasterioSourceConfig(uris=[img_path],\n                                           x_shift_meters=1.0,\n                                           y_shift_meters=0.0,\n                                           channel_order=channel_order) \\\n                     .to_proto()\n\n        tmp_dir = RVConfig.get_tmp_dir().name\n        make_dir(tmp_dir)\n        source = rv.RasterSourceConfig.from_proto(msg) \\\n                                      .create_source(tmp_dir=tmp_dir)\n\n        with source.activate():\n            extent = source.get_extent()\n            data = source.get_chip(extent)\n            self.assertEqual(data.sum(), 2**16 - 256)\n            column = data[:, 255, 0]\n            self.assertEqual(column.sum(), 0)\n\n    def test_shift_y(self):\n        # Specially-engineered image w/ one meter per pixel resolution\n        # in the y direction.\n        img_path = data_file_path('ones.tif')\n        channel_order = [0]\n\n        msg = rv.data.RasterioSourceConfig(uris=[img_path],\n                                           x_shift_meters=0.0,\n                                           y_shift_meters=1.0,\n                                           channel_order=channel_order) \\\n                     .to_proto()\n\n        tmp_dir = RVConfig.get_tmp_dir().name\n        make_dir(tmp_dir)\n        source = rv.RasterSourceConfig.from_proto(msg) \\\n                                      .create_source(tmp_dir=tmp_dir)\n\n        with source.activate():\n            extent = source.get_extent()\n            data = source.get_chip(extent)\n            self.assertEqual(data.sum(), 2**16 - 256)\n            row = data[0, :, 0]\n            self.assertEqual(row.sum(), 0)\n\n    def test_gets_raw_chip_from_proto(self):\n        img_path = data_file_path('small-rgb-tile.tif')\n        channel_order = [0, 1]\n\n        msg = rv.RasterSourceConfig.builder(rv.RASTERIO_SOURCE) \\\n                                   .with_uri(img_path) \\\n                                   .with_channel_order(channel_order) \\\n                                   .build() \\\n                                   .to_proto()\n\n        source = rv.RasterSourceConfig.from_proto(msg) \\\n                                      .create_source(tmp_dir=None)\n\n        with source.activate():\n            out_chip = source.get_raw_image_array()\n            self.assertEqual(out_chip.shape[2], 3)\n\n    def test_gets_raw_chip_from_uint16_transformed_proto(self):\n        img_path = data_file_path('small-uint16-tile.tif')\n        channel_order = [0, 1]\n\n        with RVConfig.get_tmp_dir() as temp_dir:\n            stats_uri = os.path.join(temp_dir, 'temp.tif')\n            stats = RasterStats()\n            stats.compute([\n                rv.RasterSourceConfig.builder(rv.RASTERIO_SOURCE)\n                .with_uri(img_path).build().create_source(temp_dir)\n            ])\n            stats.save(stats_uri)\n\n            transformer = rv.RasterTransformerConfig.builder(rv.STATS_TRANSFORMER) \\\n                                                    .with_stats_uri(stats_uri) \\\n                                                    .build()\n\n            msg = rv.RasterSourceConfig.builder(rv.RASTERIO_SOURCE) \\\n                                       .with_uri(img_path) \\\n                                       .with_channel_order(channel_order) \\\n                                       .with_transformer(transformer) \\\n                                       .build() \\\n                                       .to_proto()\n\n            source = rv.RasterSourceConfig.from_proto(msg) \\\n                                          .create_source(tmp_dir=None)\n\n            with source.activate():\n                out_chip = source.get_raw_image_array()\n                self.assertEqual(out_chip.shape[2], 3)\n\n    def test_uses_channel_order(self):\n        with RVConfig.get_tmp_dir() as tmp_dir:\n            img_path = os.path.join(tmp_dir, 'img.tif')\n            chip = np.ones((2, 2, 4)).astype(np.uint8)\n            chip[:, :, :] *= np.array([0, 1, 2, 3]).astype(np.uint8)\n            save_img(chip, img_path)\n\n            channel_order = [0, 1, 2]\n            source = rv.RasterSourceConfig.builder(rv.RASTERIO_SOURCE) \\\n                                          .with_uri(img_path) \\\n                                          .with_channel_order(channel_order) \\\n                                          .build() \\\n                                          .create_source(tmp_dir=tmp_dir)\n            with source.activate():\n                out_chip = source.get_image_array()\n                expected_out_chip = np.ones((2, 2, 3)).astype(np.uint8)\n                expected_out_chip[:, :, :] *= np.array([0, 1,\n                                                        2]).astype(np.uint8)\n                np.testing.assert_equal(out_chip, expected_out_chip)\n\n    def test_channel_order_error(self):\n        with RVConfig.get_tmp_dir() as tmp_dir:\n            img_path = os.path.join(tmp_dir, 'img.tif')\n            chip = np.ones((2, 2, 3)).astype(np.uint8)\n            chip[:, :, :] *= np.array([0, 1, 2]).astype(np.uint8)\n            save_img(chip, img_path)\n\n            channel_order = [3, 1, 0]\n            with self.assertRaises(ChannelOrderError):\n                rv.RasterSourceConfig.builder(rv.RASTERIO_SOURCE) \\\n                                     .with_uri(img_path) \\\n                                     .with_channel_order(channel_order) \\\n                                     .build() \\\n                                     .create_source(tmp_dir=tmp_dir)\n\n    def test_detects_alpha(self):\n        # Set first channel to alpha. Expectation is that when omitting channel_order,\n        # only the second and third channels will be in output.\n        with RVConfig.get_tmp_dir() as tmp_dir:\n            img_path = os.path.join(tmp_dir, 'img.tif')\n            chip = np.ones((2, 2, 3)).astype(np.uint8)\n            chip[:, :, :] *= np.array([0, 1, 2]).astype(np.uint8)\n            save_img(chip, img_path)\n\n            ci = (ColorInterp.alpha, ColorInterp.blue, ColorInterp.green)\n            with rasterio.open(img_path, 'r+') as src:\n                src.colorinterp = ci\n\n            source = rv.RasterSourceConfig.builder(rv.RASTERIO_SOURCE) \\\n                                          .with_uri(img_path) \\\n                                          .build() \\\n                                          .create_source(tmp_dir=tmp_dir)\n            with source.activate():\n                out_chip = source.get_image_array()\n                expected_out_chip = np.ones((2, 2, 2)).astype(np.uint8)\n                expected_out_chip[:, :, :] *= np.array([1, 2]).astype(np.uint8)\n                np.testing.assert_equal(out_chip, expected_out_chip)\n\n    def test_empty_channel_order(self):\n        config = rv.RasterSourceConfig.builder(rv.RASTERIO_SOURCE) \\\n                                      .with_uri('a.tif') \\\n                                      .build()\n        msg = config.to_proto()\n        config = rv.RasterSourceConfig.from_proto(msg)\n        self.assertEqual(config.channel_order, None)\n\n    def test_non_geo(self):\n        # Check if non-georeferenced image files can be read and CRSTransformer\n        # implements the identity function.\n        with RVConfig.get_tmp_dir() as tmp_dir:\n            img_path = os.path.join(tmp_dir, 'img.png')\n            chip = np.ones((2, 2, 3)).astype(np.uint8)\n            save_img(chip, img_path)\n\n            source = rv.RasterSourceConfig.builder(rv.RASTERIO_SOURCE) \\\n                                          .with_uri(img_path) \\\n                                          .build() \\\n                                          .create_source(tmp_dir=tmp_dir)\n            with source.activate():\n                out_chip = source.get_image_array()\n                np.testing.assert_equal(out_chip, chip)\n\n                p = (3, 4)\n                out_p = source.get_crs_transformer().map_to_pixel(p)\n                np.testing.assert_equal(out_p, p)\n\n                out_p = source.get_crs_transformer().pixel_to_map(p)\n                np.testing.assert_equal(out_p, p)\n\n    def test_with_stats_transformer(self):\n        config = rv.RasterSourceConfig.builder(rv.RASTERIO_SOURCE) \\\n                                      .with_uri('dummy') \\\n                                      .with_stats_transformer() \\\n                                      .build()\n\n        self.assertEqual(len(config.transformers), 1)\n        self.assertIsInstance(config.transformers[0],\n                              rv.data.StatsTransformerConfig)\n\n    def test_missing_config_uri(self):\n        with self.assertRaises(rv.ConfigError):\n            rv.data.RasterSourceConfig.builder(rv.RASTERIO_SOURCE).build()\n\n    def test_no_missing_config(self):\n        try:\n            rv.data.RasterSourceConfig.builder(\n                rv.RASTERIO_SOURCE).with_uri('').build()\n        except rv.ConfigError:\n            self.fail('ConfigError raised unexpectedly')\n\n    def test_backcompat_geotiff_source(self):\n        msg = RasterSourceMsg()\n        uris = ['a', 'b']\n        x = 5\n        y = 6\n        msg.geotiff_files.uris.extend(uris)\n        msg.geotiff_files.x_shift_meters = x\n        msg.geotiff_files.y_shift_meters = y\n\n        rs_config = rv.RasterSourceConfig.builder(rv.RASTERIO_SOURCE) \\\n                        .from_proto(msg).build()\n        self.assertEqual(rs_config.uris, uris)\n        self.assertEqual(rs_config.x_shift_meters, x)\n        self.assertEqual(rs_config.y_shift_meters, y)\n\n    def test_backcompat_image_source(self):\n        msg = RasterSourceMsg()\n        uri = 'a'\n        msg.image_file.uri = uri\n\n        rs_config = rv.RasterSourceConfig.builder(rv.RASTERIO_SOURCE) \\\n                        .from_proto(msg).build()\n        self.assertEqual(rs_config.uris, [uri])\n\n    def test_no_epsg(self):\n        crs = rasterio.crs.CRS()\n        with RVConfig.get_tmp_dir() as tmp_dir:\n            image_path = os.path.join(tmp_dir, 'temp.tif')\n            height = 100\n            width = 100\n            nb_channels = 3\n            with rasterio.open(\n                    image_path,\n                    'w',\n                    driver='GTiff',\n                    height=height,\n                    width=width,\n                    count=nb_channels,\n                    dtype=np.uint8,\n                    crs=crs) as image_dataset:\n                im = np.zeros((height, width, nb_channels)).astype(np.uint8)\n                for channel in range(nb_channels):\n                    image_dataset.write(im[:, :, channel], channel + 1)\n\n            try:\n                rv.RasterSourceConfig.builder(rv.RASTERIO_SOURCE) \\\n                                     .with_uri(image_path) \\\n                                     .build() \\\n                                     .create_source(tmp_dir=tmp_dir)\n            except Exception:\n                self.fail(\n                    'Creating RasterioSource with CRS with no EPSG attribute '\n                    'raised an exception when it should not have.')\n\n    def test_default_provider(self):\n        # The default provider should return a RasterioSourceConfig for any uri not\n        # caught by some other default provider.\n        uri = 'x.abcdefg'\n        provider = rv._registry.get_raster_source_default_provider(uri)\n        rs_config = provider.construct(uri)\n        self.assertIsInstance(rs_config, RasterioSourceConfig)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests/data/raster_source/test_rasterized_source.py,0,"b""import unittest\nimport os\n\nimport numpy as np\n\nimport rastervision as rv\nfrom rastervision.core import Box\nfrom rastervision.data.raster_source import RasterSourceConfig\nfrom rastervision.data.crs_transformer import IdentityCRSTransformer\nfrom rastervision.utils.files import json_to_file\nfrom rastervision.rv_config import RVConfig\n\n\nclass TestRasterizedSource(unittest.TestCase):\n    def setUp(self):\n        self.crs_transformer = IdentityCRSTransformer()\n        self.extent = Box.make_square(0, 0, 10)\n        self.tmp_dir = RVConfig.get_tmp_dir()\n        self.class_id = 2\n        self.background_class_id = 3\n        self.line_buffer = 1\n        self.uri = os.path.join(self.tmp_dir.name, 'temp.json')\n\n    def tearDown(self):\n        self.tmp_dir.cleanup()\n\n    def build_source(self, geojson, all_touched=False):\n        json_to_file(geojson, self.uri)\n\n        config = RasterSourceConfig.builder(rv.RASTERIZED_SOURCE) \\\n            .with_uri(self.uri) \\\n            .with_rasterizer_options(self.background_class_id, all_touched=all_touched) \\\n            .build()\n\n        # Convert to proto and back as a test.\n        config = RasterSourceConfig.builder(rv.RASTERIZED_SOURCE) \\\n            .from_proto(config.to_proto()) \\\n            .build()\n\n        source = config.create_source(self.uri, self.crs_transformer,\n                                      self.extent)\n\n        return source\n\n    def test_get_chip(self):\n        geojson = {\n            'type':\n            'FeatureCollection',\n            'features': [{\n                'type': 'Feature',\n                'geometry': {\n                    'type':\n                    'Polygon',\n                    'coordinates': [[[0., 0.], [0., 5.], [5., 5.], [5., 0.],\n                                     [0., 0.]]]\n                },\n                'properties': {\n                    'class_id': self.class_id,\n                }\n            }, {\n                'type': 'Feature',\n                'geometry': {\n                    'type': 'LineString',\n                    'coordinates': [[7., 0.], [7., 9.]]\n                },\n                'properties': {\n                    'class_id': self.class_id\n                }\n            }]\n        }\n\n        source = self.build_source(geojson)\n        with source.activate():\n            self.assertEqual(source.get_extent(), self.extent)\n            chip = source.get_image_array()\n            self.assertEqual(chip.shape, (10, 10, 1))\n\n            expected_chip = self.background_class_id * np.ones((10, 10, 1))\n            expected_chip[0:5, 0:5, 0] = self.class_id\n            expected_chip[0:10, 6:8] = self.class_id\n            np.testing.assert_array_equal(chip, expected_chip)\n\n    def test_get_chip_no_polygons(self):\n        geojson = {'type': 'FeatureCollection', 'features': []}\n\n        source = self.build_source(geojson)\n        with source.activate():\n            # Get chip that partially overlaps extent. Expect that chip has zeros\n            # outside of extent, and background_class_id otherwise.\n            self.assertEqual(source.get_extent(), self.extent)\n            chip = source.get_chip(Box.make_square(5, 5, 10))\n            self.assertEqual(chip.shape, (10, 10, 1))\n\n            expected_chip = np.zeros((10, 10, 1))\n            expected_chip[0:5, 0:5, :] = self.background_class_id\n\n            np.testing.assert_array_equal(chip, expected_chip)\n\n    def test_get_chip_all_touched(self):\n        geojson = {\n            'type':\n            'FeatureCollection',\n            'features': [{\n                'type': 'Feature',\n                'geometry': {\n                    'type':\n                    'Polygon',\n                    'coordinates': [[[0., 0.], [0., 0.4], [0.4, 0.4],\n                                     [0.4, 0.], [0., 0.]]]\n                },\n                'properties': {\n                    'class_id': self.class_id,\n                }\n            }]\n        }\n\n        false_source = self.build_source(geojson, all_touched=False)\n        true_source = self.build_source(geojson, all_touched=True)\n        with false_source.activate():\n            chip = false_source.get_image_array()\n            expected_chip = self.background_class_id * np.ones((10, 10, 1))\n            np.testing.assert_array_equal(chip, expected_chip)\n\n        with true_source.activate():\n            chip = true_source.get_image_array()\n            expected_chip = self.background_class_id * np.ones((10, 10, 1))\n            expected_chip[0:1, 0:1, 0] = self.class_id\n            np.testing.assert_array_equal(chip, expected_chip)\n\n    def test_using_null_class_bufs(self):\n        vs = rv.VectorSourceConfig.builder(rv.GEOJSON_SOURCE) \\\n            .with_uri(self.uri) \\\n            .with_buffers(line_bufs={1: None}) \\\n            .build()\n        with self.assertRaises(rv.ConfigError):\n            RasterSourceConfig.builder(rv.RASTERIZED_SOURCE) \\\n                .with_vector_source(vs) \\\n                .with_rasterizer_options(self.background_class_id) \\\n                .build()\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests/data/raster_transformer/__init__.py,0,b''
tests/data/raster_transformer/test_raster_transformer.py,0,"b""import unittest\nimport os\n\nimport numpy as np\n\nimport rastervision as rv\nfrom rastervision.core.raster_stats import RasterStats\nfrom rastervision.rv_config import RVConfig\n\n\nclass TestRasterTransformer(unittest.TestCase):\n    def test_stats_transformer(self):\n        raster_stats = RasterStats()\n        raster_stats.means = list(np.ones((4, )))\n        raster_stats.stds = list(np.ones((4, )) * 2)\n\n        with RVConfig.get_tmp_dir() as tmp_dir:\n            stats_uri = os.path.join(tmp_dir, 'stats.json')\n            raster_stats.save(stats_uri)\n\n            # All values have z-score of 1, which translates to\n            # uint8 value of 170.\n            transformer = rv.RasterTransformerConfig.builder(rv.STATS_TRANSFORMER) \\\n                                                    .with_stats_uri(stats_uri) \\\n                                                    .build() \\\n                                                    .create_transformer()\n            chip = np.ones((2, 2, 4)) * 3\n            out_chip = transformer.transform(chip)\n            expected_out_chip = np.ones((2, 2, 4)) * 170\n            np.testing.assert_equal(out_chip, expected_out_chip)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests/data/vector_source/__init__.py,0,b''
tests/data/vector_source/test_geojson_vector_source.py,0,"b'import unittest\nimport os\n\nfrom shapely.geometry import shape\n\nimport rastervision as rv\nfrom rastervision.data.vector_source import (GeoJSONVectorSourceConfigBuilder,\n                                             GeoJSONVectorSourceConfig)\nfrom rastervision.core.class_map import ClassMap\nfrom rastervision.utils.files import json_to_file\nfrom rastervision.rv_config import RVConfig\nfrom rastervision.data.crs_transformer import IdentityCRSTransformer\n\nfrom tests.data.mock_crs_transformer import DoubleCRSTransformer\n\n\nclass TestGeoJSONVectorSource(unittest.TestCase):\n    """"""This also indirectly tests the ClassInference class.""""""\n\n    def setUp(self):\n        self.temp_dir = RVConfig.get_tmp_dir()\n        self.uri = os.path.join(self.temp_dir.name, \'vectors.json\')\n\n    def tearDown(self):\n        self.temp_dir.cleanup()\n\n    def _test_class_inf(self, props, exp_class_ids, default_class_id=None):\n        geojson = {\n            \'type\':\n            \'FeatureCollection\',\n            \'features\': [{\n                \'properties\': props,\n                \'geometry\': {\n                    \'type\': \'Point\',\n                    \'coordinates\': [1, 1]\n                }\n            }]\n        }\n        json_to_file(geojson, self.uri)\n\n        class_map = ClassMap.construct_from([\'building\', \'car\', \'tree\'])\n        class_id_to_filter = {\n            1: [\'==\', \'type\', \'building\'],\n            2: [\'any\', [\'==\', \'type\', \'car\'], [\'==\', \'type\', \'auto\']]\n        }\n        b = GeoJSONVectorSourceConfigBuilder() \\\n            .with_class_inference(class_id_to_filter=class_id_to_filter,\n                                  default_class_id=default_class_id) \\\n            .with_uri(self.uri) \\\n            .build()\n        msg = b.to_proto()\n        config = GeoJSONVectorSourceConfig.from_proto(msg)\n        source = config.create_source(\n            crs_transformer=IdentityCRSTransformer(), class_map=class_map)\n        trans_geojson = source.get_geojson()\n        class_ids = [\n            f[\'properties\'][\'class_id\'] for f in trans_geojson[\'features\']\n        ]\n        self.assertEqual(class_ids, exp_class_ids)\n\n    def test_class_inf_class_id(self):\n        self._test_class_inf({\'class_id\': 3}, [3])\n\n    def test_class_inf_label(self):\n        self._test_class_inf({\'label\': \'car\'}, [2])\n\n    def test_class_inf_filter(self):\n        self._test_class_inf({\'type\': \'auto\'}, [2])\n\n    def test_class_inf_default(self):\n        self._test_class_inf({}, [4], default_class_id=4)\n\n    def test_class_inf_no_default(self):\n        self._test_class_inf({}, [])\n\n    def geom_to_geojson(self, geom):\n        return {\'type\': \'FeatureCollection\', \'features\': [{\'geometry\': geom}]}\n\n    def transform_geojson(self,\n                          geojson,\n                          line_bufs=None,\n                          point_bufs=None,\n                          crs_transformer=None,\n                          to_map_coords=False):\n        if crs_transformer is None:\n            crs_transformer = IdentityCRSTransformer()\n        class_map = ClassMap.construct_from([\'building\'])\n        json_to_file(geojson, self.uri)\n        b = GeoJSONVectorSourceConfigBuilder() \\\n            .with_uri(self.uri) \\\n            .with_buffers(line_bufs=line_bufs, point_bufs=point_bufs) \\\n            .build()\n        msg = b.to_proto()\n        config = GeoJSONVectorSourceConfig.from_proto(msg)\n        source = config.create_source(\n            crs_transformer=crs_transformer, class_map=class_map)\n        return source.get_geojson(to_map_coords=to_map_coords)\n\n    def test_transform_geojson_no_coords(self):\n        geom = {\'type\': \'Point\', \'coordinates\': []}\n        geojson = self.geom_to_geojson(geom)\n        trans_geojson = self.transform_geojson(geojson)\n\n        self.assertEqual(0, len(trans_geojson[\'features\']))\n\n    def test_transform_geojson_geom_coll(self):\n        geom = {\n            \'type\':\n            \'GeometryCollection\',\n            \'geometries\': [{\n                \'type\': \'MultiPoint\',\n                \'coordinates\': [[10, 10], [20, 20]]\n            }]\n        }\n        geojson = self.geom_to_geojson(geom)\n        trans_geojson = self.transform_geojson(geojson)\n\n        feats = trans_geojson[\'features\']\n        self.assertEqual(len(feats), 2)\n        self.assertEqual(feats[0][\'geometry\'][\'type\'], \'Polygon\')\n        self.assertEqual(feats[1][\'geometry\'][\'type\'], \'Polygon\')\n\n    def test_transform_geojson_multi(self):\n        geom = {\'type\': \'MultiPoint\', \'coordinates\': [[10, 10], [20, 20]]}\n        geojson = self.geom_to_geojson(geom)\n        trans_geojson = self.transform_geojson(geojson)\n\n        feats = trans_geojson[\'features\']\n        self.assertEqual(len(feats), 2)\n        self.assertEqual(feats[0][\'geometry\'][\'type\'], \'Polygon\')\n        self.assertEqual(feats[1][\'geometry\'][\'type\'], \'Polygon\')\n\n    def test_transform_geojson_line_buf(self):\n        geom = {\'type\': \'LineString\', \'coordinates\': [[10, 10], [10, 20]]}\n        geojson = self.geom_to_geojson(geom)\n\n        trans_geojson = self.transform_geojson(geojson, line_bufs={1: 5.0})\n        trans_geom = trans_geojson[\'features\'][0][\'geometry\']\n        self.assertTrue(shape(geom).buffer(5.0).equals(shape(trans_geom)))\n\n        trans_geojson = self.transform_geojson(geojson, line_bufs={2: 5.0})\n        trans_geom = trans_geojson[\'features\'][0][\'geometry\']\n        self.assertTrue(shape(geom).buffer(1.0).equals(shape(trans_geom)))\n\n        trans_geojson = self.transform_geojson(geojson, line_bufs={1: None})\n        trans_geom = trans_geojson[\'features\'][0][\'geometry\']\n        self.assertTrue(shape(geom).equals(shape(trans_geom)))\n\n    def test_transform_point_buf(self):\n        geom = {\'type\': \'Point\', \'coordinates\': [10, 10]}\n        geojson = self.geom_to_geojson(geom)\n\n        trans_geojson = self.transform_geojson(geojson, point_bufs={1: 5.0})\n        trans_geom = trans_geojson[\'features\'][0][\'geometry\']\n        self.assertTrue(shape(geom).buffer(5.0).equals(shape(trans_geom)))\n\n        trans_geojson = self.transform_geojson(geojson, point_bufs={2: 5.0})\n        trans_geom = trans_geojson[\'features\'][0][\'geometry\']\n        self.assertTrue(shape(geom).buffer(1.0).equals(shape(trans_geom)))\n\n        trans_geojson = self.transform_geojson(geojson, point_bufs={1: None})\n        trans_geom = trans_geojson[\'features\'][0][\'geometry\']\n        self.assertTrue(shape(geom).equals(shape(trans_geom)))\n\n    def test_transform_polygon(self):\n        geom = {\n            \'type\': \'Polygon\',\n            \'coordinates\': [[[0, 0], [0, 10], [10, 10], [10, 0], [0, 0]]]\n        }\n        geojson = self.geom_to_geojson(geom)\n\n        trans_geojson = self.transform_geojson(geojson)\n        trans_geom = trans_geojson[\'features\'][0][\'geometry\']\n        self.assertTrue(shape(geom).equals(shape(trans_geom)))\n\n        trans_geojson = self.transform_geojson(\n            geojson, crs_transformer=DoubleCRSTransformer())\n        trans_geom = trans_geojson[\'features\'][0][\'geometry\']\n        exp_geom = {\n            \'type\': \'Polygon\',\n            \'coordinates\': [[[0, 0], [0, 20], [20, 20], [20, 0], [0, 0]]]\n        }\n        self.assertTrue(shape(exp_geom).equals(shape(trans_geom)))\n\n        trans_geojson = self.transform_geojson(\n            geojson,\n            crs_transformer=DoubleCRSTransformer(),\n            to_map_coords=True)\n        trans_geom = trans_geojson[\'features\'][0][\'geometry\']\n        self.assertTrue(shape(geom).equals(shape(trans_geom)))\n\n    def test_validate_config(self):\n        with self.assertRaises(rv.ConfigError):\n            GeoJSONVectorSourceConfigBuilder() \\\n                .with_uri(self.uri) \\\n                .with_buffers(line_bufs={1: \'a\'}) \\\n                .build()\n\n        with self.assertRaises(rv.ConfigError):\n            GeoJSONVectorSourceConfigBuilder() \\\n                .with_uri(self.uri) \\\n                .with_buffers(point_bufs={1: \'a\'}) \\\n                .build()\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
tests/data/vector_source/test_vector_tile_vector_source.py,0,"b""import unittest\nimport json\n\nfrom shapely.geometry import shape\n\nfrom rastervision.data.vector_source import (\n    VectorTileVectorSourceConfigBuilder, VectorTileVectorSourceConfig)\nfrom rastervision.core.box import Box\nfrom rastervision.core.class_map import ClassMap\nfrom rastervision.data.crs_transformer import IdentityCRSTransformer\nfrom rastervision.utils.files import file_to_str\nfrom tests import data_file_path\n\n\nclass TestVectorTileVectorSource(unittest.TestCase):\n    def setUp(self):\n        self.class_id_to_filter = {1: ['has', 'building']}\n        self.class_map = ClassMap.construct_from(['building'])\n        self.crs_transformer = IdentityCRSTransformer()\n\n    def _get_source(self, uri):\n        b = VectorTileVectorSourceConfigBuilder() \\\n            .with_class_inference(class_id_to_filter=self.class_id_to_filter,\n                                  default_class_id=None) \\\n            .with_uri(uri) \\\n            .with_zoom(14) \\\n            .with_id_field('__id') \\\n            .with_buffers(line_bufs={1: 0.0001}, point_bufs={1: 0.0001}) \\\n            .build()\n        config = VectorTileVectorSourceConfig.from_proto(b.to_proto())\n        aoi_path = data_file_path('vector_tiles/lv-aoi.json')\n        extent_geojson = json.loads(file_to_str(aoi_path))\n        extent = Box.from_shapely(\n            shape(extent_geojson['features'][0]['geometry']))\n        source = config.create_source(self.crs_transformer, extent,\n                                      self.class_map)\n        return source\n\n    def _test_get_geojson(self, vector_tile_uri, json_uri):\n        source = self._get_source(vector_tile_uri)\n        geojson = source.get_geojson()\n        expected_geojson = json.loads(file_to_str(data_file_path(json_uri)))\n\n        # Need to convert to JSON and back again because geojson object has tuples\n        # instead of lists because of a quirk of shapely.geometry.mapping\n        # See https://github.com/Toblerity/Shapely/issues/245\n        geojson = json.loads(json.dumps(geojson))\n        geojson['features'].sort(key=lambda f: f['properties']['__id'])\n        expected_geojson['features'].sort(\n            key=lambda f: f['properties']['__id'])\n\n        self.assertDictEqual(geojson, expected_geojson)\n\n    def test_get_geojson_from_zxy(self):\n        vector_tile_uri = data_file_path('vector_tiles/{z}/{x}/{y}.mvt')\n        json_uri = 'vector_tiles/lv-zxy.json'\n        self._test_get_geojson(vector_tile_uri, json_uri)\n\n    def test_get_geojson_from_mbtiles(self):\n        vector_tile_uri = data_file_path('vector_tiles/lv.mbtiles')\n        json_uri = 'vector_tiles/lv-mbtiles.json'\n        self._test_get_geojson(vector_tile_uri, json_uri)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests_v2/core/data/__init__.py,0,b''
tests_v2/core/data/mock_crs_transformer.py,0,"b'from rastervision2.core.data import CRSTransformer\n\n\nclass DoubleCRSTransformer(CRSTransformer):\n    """"""Mock CRSTransformer used for testing.\n\n    Assumes map coords are 2x pixels coords.\n    """"""\n\n    def map_to_pixel(self, web_point):\n        return (web_point[0] * 2.0, web_point[1] * 2.0)\n\n    def pixel_to_map(self, pixel_point):\n        return (pixel_point[0] / 2.0, pixel_point[1] / 2.0)\n'"
tests_v2/core/data/mock_raster_source.py,0,"b'from unittest.mock import Mock\nimport numpy as np\n\nfrom rastervision2.core import Box\nfrom rastervision2.core.data import (RasterSource, IdentityCRSTransformer)\nfrom rastervision.data import (ActivateMixin)\n\n\nclass MockRasterSource(RasterSource, ActivateMixin):\n    def __init__(self, channel_order, num_channels, raster_transformers=[]):\n        super().__init__(channel_order, num_channels, raster_transformers)\n        self.mock = Mock()\n        self.set_return_vals()\n\n    def set_return_vals(self, raster=None):\n        self.mock.get_extent.return_value = Box.make_square(0, 0, 2)\n        self.mock.get_dtype.return_value = np.uint8\n        self.mock.get_crs_transformer.return_value = IdentityCRSTransformer()\n        self.mock._get_chip.return_value = np.random.rand(1, 2, 2, 3)\n\n        if raster is not None:\n            self.mock.get_extent.return_value = Box(0, 0, raster.shape[0],\n                                                    raster.shape[1])\n            self.mock.get_dtype.return_value = raster.dtype\n\n            def get_chip(window):\n                return raster[window.ymin:window.ymax, window.xmin:\n                              window.xmax, :]\n\n            self.mock._get_chip.side_effect = get_chip\n\n    def get_extent(self):\n        return self.mock.get_extent()\n\n    def get_dtype(self):\n        return self.mock.get_dtype()\n\n    def get_crs_transformer(self):\n        return self.mock.get_crs_transformer()\n\n    def _get_chip(self, window):\n        return self.mock._get_chip(window)\n\n    def set_raster(self, raster):\n        self.set_return_vals(raster=raster)\n\n    def _activate(self):\n        pass\n\n    def _deactivate(self):\n        pass\n'"
tests_v2/core/evaluation/__init__.py,0,b''
tests_v2/core/evaluation/test_chip_classification_evaluation.py,0,"b'import unittest\n\nfrom rastervision2.core.evaluation import ChipClassificationEvaluation\nfrom rastervision2.core.data.class_config import ClassConfig\nfrom rastervision2.core import Box\nfrom rastervision2.core.data.label import ChipClassificationLabels\n\n\nclass TestChipClassificationEvaluation(unittest.TestCase):\n    def make_class_config(self):\n        return ClassConfig(names=[\'grassy\', \'urban\'])\n\n    def make_labels(self, class_ids):\n        """"""Make 2x2 grid label store.\n\n        Args:\n            class_ids: 2x2 array of class_ids to use\n        """"""\n        cell_size = 200\n        y_cells = 2\n        x_cells = 2\n        labels = ChipClassificationLabels()\n\n        for yind in range(y_cells):\n            for xind in range(x_cells):\n                ymin = yind * cell_size\n                xmin = xind * cell_size\n                ymax = ymin + cell_size\n                xmax = xmin + cell_size\n                window = Box(ymin, xmin, ymax, xmax)\n                class_id = class_ids[yind][xind]\n                new_labels = ChipClassificationLabels()\n                new_labels.set_cell(window, class_id)\n                labels.extend(new_labels)\n\n        return labels\n\n    def assert_eval_single_null(self, eval):\n        eval_item0 = eval.class_to_eval_item[0]\n        self.assertEqual(eval_item0.gt_count, 2)\n        self.assertEqual(eval_item0.precision, 1.0)\n        self.assertEqual(eval_item0.recall, 0.5)\n        self.assertAlmostEqual(eval_item0.f1, 2 / 3, places=2)\n\n        eval_item1 = eval.class_to_eval_item[1]\n        self.assertEqual(eval_item1.gt_count, 1)\n        self.assertEqual(eval_item1.precision, 0.5)\n        self.assertEqual(eval_item1.recall, 1.0)\n        self.assertAlmostEqual(eval_item1.f1, 2 / 3, places=2)\n\n        avg_item = eval.avg_item\n        self.assertEqual(avg_item.gt_count, 3)\n        self.assertAlmostEqual(avg_item.precision, 0.83, places=2)\n        self.assertAlmostEqual(avg_item.recall, 2 / 3, places=2)\n        self.assertAlmostEqual(avg_item.f1, 2 / 3, places=2)\n\n    def test_compute_single_pred_null(self):\n        class_config = self.make_class_config()\n        eval = ChipClassificationEvaluation(class_config)\n        gt_class_ids = [[0, 1], [0, 1]]\n        gt_labels = self.make_labels(gt_class_ids)\n        pred_class_ids = [[0, None], [1, 1]]\n        pred_labels = self.make_labels(pred_class_ids)\n        eval.compute(gt_labels, pred_labels)\n        self.assert_eval_single_null(eval)\n\n    def test_compute_single_gt_null(self):\n        class_config = self.make_class_config()\n        eval = ChipClassificationEvaluation(class_config)\n        gt_class_ids = [[0, None], [0, 1]]\n        gt_labels = self.make_labels(gt_class_ids)\n        pred_class_ids = [[0, 1], [1, 1]]\n        pred_labels = self.make_labels(pred_class_ids)\n        eval.compute(gt_labels, pred_labels)\n        self.assert_eval_single_null(eval)\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
tests_v2/core/evaluation/test_chip_classification_evaluator.py,0,"b""import unittest\nimport os\n\nfrom rastervision2.pipeline import rv_config\nfrom rastervision2.pipeline.file_system import file_to_json\nfrom rastervision2.core.data import (\n    ClassConfig, ChipClassificationLabelSourceConfig, GeoJSONVectorSourceConfig,\n    ChipClassificationGeoJSONStoreConfig, RasterioSourceConfig, SceneConfig)\nfrom rastervision2.core.evaluation import (ChipClassificationEvaluatorConfig)\n\nfrom tests_v2 import data_file_path\n\n\nclass TestChipClassificationEvaluator(unittest.TestCase):\n    def test_accounts_for_aoi(self):\n        class_config = ClassConfig(names=['car', 'building', 'background'])\n\n        label_source_uri = data_file_path('evaluator/cc-label-filtered.json')\n        label_source_cfg = ChipClassificationLabelSourceConfig(\n            vector_source=GeoJSONVectorSourceConfig(\n                uri=label_source_uri, default_class_id=None))\n\n        label_store_uri = data_file_path('evaluator/cc-label-full.json')\n        label_store_cfg = ChipClassificationGeoJSONStoreConfig(uri=label_store_uri)\n\n        raster_source_uri = data_file_path('evaluator/cc-label-img-blank.tif')\n        raster_source_cfg = RasterioSourceConfig(uris=[raster_source_uri])\n\n        aoi_uri = data_file_path('evaluator/cc-label-aoi.json')\n        s = SceneConfig(\n            id='test',\n            raster_source=raster_source_cfg,\n            label_source=label_source_cfg,\n            label_store=label_store_cfg,\n            aoi_uris=[aoi_uri])\n\n        with rv_config.get_tmp_dir() as tmp_dir:\n            scene = s.build(class_config, tmp_dir)\n            output_uri = os.path.join(tmp_dir, 'eval.json')\n\n            evaluator = ChipClassificationEvaluatorConfig(\n                output_uri=output_uri).build(class_config)\n            evaluator.process([scene], tmp_dir)\n\n            overall = file_to_json(output_uri)['overall']\n            for item in overall:\n                self.assertEqual(item['f1'], 1.0)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests_v2/core/evaluation/test_class_evaluation_item.py,0,"b""import unittest\n\nfrom rastervision2.core.evaluation import ClassEvaluationItem\n\n\nclass TestClassEvaluationItem(unittest.TestCase):\n    def setUp(self):\n        pass\n\n    def test_merge_both_empty(self):\n        a = ClassEvaluationItem()\n        b = ClassEvaluationItem()\n        a.merge(b)\n        self.assertEqual(a.precision, None)\n        self.assertEqual(a.recall, None)\n        self.assertEqual(a.f1, None)\n        self.assertEqual(a.count_error, None)\n        self.assertEqual(a.gt_count, 0)\n\n    def test_merge_first_empty(self):\n        a = ClassEvaluationItem()\n        b = ClassEvaluationItem(\n            precision=1, recall=1, f1=1, count_error=0, gt_count=1)\n        a.merge(b)\n        self.assertEqual(a.precision, 1)\n        self.assertEqual(a.recall, 1)\n        self.assertEqual(a.f1, 1)\n        self.assertEqual(a.count_error, 0)\n        self.assertEqual(a.gt_count, 1)\n\n    def test_merge_second_empty(self):\n        a = ClassEvaluationItem(\n            precision=1, recall=1, f1=1, count_error=0, gt_count=1)\n        b = ClassEvaluationItem()\n        a.merge(b)\n        self.assertEqual(a.precision, 1)\n        self.assertEqual(a.recall, 1)\n        self.assertEqual(a.f1, 1)\n        self.assertEqual(a.count_error, 0)\n        self.assertEqual(a.gt_count, 1)\n\n    def test_merge(self):\n        a = ClassEvaluationItem(\n            precision=1, recall=1, f1=1, count_error=0, gt_count=1)\n        b = ClassEvaluationItem(\n            precision=0, recall=0, f1=0, count_error=1, gt_count=2)\n        a.merge(b)\n        self.assertEqual(a.precision, 1 / 3)\n        self.assertEqual(a.recall, 1 / 3)\n        self.assertEqual(a.f1, 1 / 3)\n        self.assertEqual(a.count_error, 2 / 3)\n        self.assertEqual(a.gt_count, 3)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests_v2/core/evaluation/test_object_detection_evaluation.py,0,"b""import unittest\n\nimport numpy as np\n\nfrom rastervision2.core.evaluation import ObjectDetectionEvaluation\nfrom rastervision2.core.data import ClassConfig, ObjectDetectionLabels\nfrom rastervision2.core import Box\n\n\nclass TestObjectDetectionEvaluation(unittest.TestCase):\n    def make_class_config(self):\n        return ClassConfig(names=['name', 'building'])\n\n    def make_ground_truth_labels(self):\n        size = 100\n        nw = Box.make_square(0, 0, size)\n        ne = Box.make_square(0, 200, size)\n        se = Box.make_square(200, 200, size)\n        sw = Box.make_square(200, 0, size)\n        npboxes = Box.to_npboxes([nw, ne, se, sw])\n        class_ids = np.array([0, 0, 1, 1])\n        return ObjectDetectionLabels(npboxes, class_ids)\n\n    def make_predicted_labels(self):\n        size = 100\n        # Predicted labels are only there for three of the ground truth boxes,\n        # and are offset by 10 pixels.\n        nw = Box.make_square(10, 0, size)\n        ne = Box.make_square(10, 200, size)\n        se = Box.make_square(210, 200, size)\n        npboxes = Box.to_npboxes([nw, ne, se])\n        class_ids = np.array([0, 0, 1])\n        scores = np.ones(class_ids.shape)\n        return ObjectDetectionLabels(npboxes, class_ids, scores=scores)\n\n    def test_compute(self):\n        class_config = self.make_class_config()\n        eval = ObjectDetectionEvaluation(class_config)\n        gt_labels = self.make_ground_truth_labels()\n        pred_labels = self.make_predicted_labels()\n\n        eval.compute(gt_labels, pred_labels)\n        eval_item1 = eval.class_to_eval_item[0]\n        self.assertEqual(eval_item1.gt_count, 2)\n        self.assertEqual(eval_item1.precision, 1.0)\n        self.assertEqual(eval_item1.recall, 1.0)\n        self.assertEqual(eval_item1.f1, 1.0)\n\n        eval_item2 = eval.class_to_eval_item[1]\n        self.assertEqual(eval_item2.gt_count, 2)\n        self.assertEqual(eval_item2.precision, 1.0)\n        self.assertEqual(eval_item2.recall, 0.5)\n        self.assertEqual(eval_item2.f1, 2 / 3)\n\n        avg_item = eval.avg_item\n        self.assertEqual(avg_item.gt_count, 4)\n        self.assertAlmostEqual(avg_item.precision, 1.0)\n        self.assertEqual(avg_item.recall, 0.75)\n        self.assertAlmostEqual(avg_item.f1, 0.83, places=2)\n\n    def test_compute_no_preds(self):\n        class_config = self.make_class_config()\n        eval = ObjectDetectionEvaluation(class_config)\n        gt_labels = self.make_ground_truth_labels()\n        pred_labels = ObjectDetectionLabels.make_empty()\n\n        eval.compute(gt_labels, pred_labels)\n        eval_item1 = eval.class_to_eval_item[0]\n        self.assertEqual(eval_item1.gt_count, 2)\n        self.assertEqual(eval_item1.precision, None)\n        self.assertEqual(eval_item1.recall, 0.0)\n        self.assertEqual(eval_item1.f1, None)\n\n        eval_item2 = eval.class_to_eval_item[1]\n        self.assertEqual(eval_item2.gt_count, 2)\n        self.assertEqual(eval_item2.precision, None)\n        self.assertEqual(eval_item2.recall, 0.0)\n        self.assertEqual(eval_item2.f1, None)\n\n        avg_item = eval.avg_item\n        self.assertEqual(avg_item.gt_count, 4)\n        self.assertEqual(avg_item.precision, 0.0)\n        self.assertEqual(avg_item.recall, 0.0)\n        self.assertEqual(avg_item.f1, 0.0)\n\n    def test_compute_no_ground_truth(self):\n        class_config = self.make_class_config()\n        eval = ObjectDetectionEvaluation(class_config)\n        gt_labels = ObjectDetectionLabels.make_empty()\n        pred_labels = self.make_predicted_labels()\n\n        eval.compute(gt_labels, pred_labels)\n        eval_item1 = eval.class_to_eval_item[0]\n        self.assertEqual(eval_item1.gt_count, 0)\n        self.assertEqual(eval_item1.precision, None)\n        self.assertEqual(eval_item1.recall, None)\n        self.assertEqual(eval_item1.f1, None)\n\n        eval_item2 = eval.class_to_eval_item[1]\n        self.assertEqual(eval_item2.gt_count, 0)\n        self.assertEqual(eval_item2.precision, None)\n        self.assertEqual(eval_item2.recall, None)\n        self.assertEqual(eval_item2.f1, None)\n\n        avg_item = eval.avg_item\n        self.assertEqual(avg_item.gt_count, 0)\n        self.assertEqual(avg_item.precision, None)\n        self.assertEqual(avg_item.recall, None)\n        self.assertEqual(avg_item.f1, None)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests_v2/core/evaluation/test_semantic_segmentation_evaluation.py,0,"b""import unittest\n\nimport numpy as np\n\nfrom rastervision2.core.data import ClassConfig\nfrom rastervision2.core.evaluation import SemanticSegmentationEvaluation\nfrom rastervision2.core.data import SemanticSegmentationLabelSource\nfrom tests_v2.core.data.mock_raster_source import MockRasterSource\nfrom tests_v2 import data_file_path\n\n\nclass TestSemanticSegmentationEvaluation(unittest.TestCase):\n    def test_compute(self):\n        class_config = ClassConfig(names=['one', 'two'])\n        class_config.update()\n        class_config.ensure_null_class()\n\n        gt_array = np.zeros((4, 4, 1), dtype=np.uint8)\n        gt_array[2, 2, 0] = 1\n        gt_array[0, 0, 0] = 2\n        gt_raster = MockRasterSource([0], 1)\n        gt_raster.set_raster(gt_array)\n        gt_label_source = SemanticSegmentationLabelSource(raster_source=gt_raster)\n\n        p_array = np.zeros((4, 4, 1), dtype=np.uint8)\n        p_array[1, 1, 0] = 1\n        p_raster = MockRasterSource([0], 1)\n        p_raster.set_raster(p_array)\n        p_label_source = SemanticSegmentationLabelSource(raster_source=p_raster)\n\n        eval = SemanticSegmentationEvaluation(class_config)\n        eval.compute(gt_label_source.get_labels(), p_label_source.get_labels())\n\n        tp0 = 16 - 3  # 4*4 - 3 true positives for class 0\n        fp0 = 1  # 1 false positive (2,2) and one don't care at (0,0)\n        fn0 = 1  # one false negative (1,1)\n        precision0 = float(tp0) / (tp0 + fp0)\n        recall0 = float(tp0) / (tp0 + fn0)\n        f10 = 2 * float(precision0 * recall0) / (precision0 + recall0)\n\n        tp1 = 0  # 0 true positives for class 1\n        fn1 = 1  # one false negative (2,2)\n        precision1 = 0  # float(tp1) / (tp1 + fp1) where fp1 == 1\n        recall1 = float(tp1) / (tp1 + fn1)\n        f11 = None\n\n        self.assertAlmostEqual(precision0,\n                               eval.class_to_eval_item[0].precision)\n        self.assertAlmostEqual(recall0, eval.class_to_eval_item[0].recall)\n        self.assertAlmostEqual(f10, eval.class_to_eval_item[0].f1)\n\n        self.assertEqual(precision1, eval.class_to_eval_item[1].precision)\n        self.assertAlmostEqual(recall1, eval.class_to_eval_item[1].recall)\n        self.assertAlmostEqual(f11, eval.class_to_eval_item[1].f1)\n\n        avg_conf_mat = np.array([[0, 0, 0], [13., 1, 0], [1, 0, 0]])\n        avg_recall = (14 / 15) * recall0 + (1 / 15) * recall1\n        self.assertTrue(np.array_equal(avg_conf_mat, eval.avg_item.conf_mat))\n        self.assertEqual(avg_recall, eval.avg_item.recall)\n\n    def test_compute_ignore_class(self):\n        class_config = ClassConfig(names=['one', 'two'])\n        class_config.update()\n        class_config.ensure_null_class()\n\n        gt_array = np.zeros((4, 4, 1), dtype=np.uint8)\n        gt_array[0, 0, 0] = 2\n        gt_raster = MockRasterSource([0], 1)\n        gt_raster.set_raster(gt_array)\n        gt_label_source = SemanticSegmentationLabelSource(raster_source=gt_raster)\n\n        pred_array = np.zeros((4, 4, 1), dtype=np.uint8)\n        pred_array[0, 0, 0] = 1\n        pred_raster = MockRasterSource([0], 1)\n        pred_raster.set_raster(pred_array)\n        pred_label_source = SemanticSegmentationLabelSource(raster_source=pred_raster)\n\n        eval = SemanticSegmentationEvaluation(class_config)\n        eval.compute(gt_label_source.get_labels(),\n                     pred_label_source.get_labels())\n        self.assertAlmostEqual(1.0, eval.class_to_eval_item[0].f1)\n        self.assertAlmostEqual(1.0, eval.avg_item.f1)\n\n    def test_vector_compute(self):\n        class_config = ClassConfig(names=['one', 'two'])\n        class_config.update()\n        class_config.ensure_null_class()\n\n        gt_uri = data_file_path('2-gt-polygons.geojson')\n        pred_uri = data_file_path('2-pred-polygons.geojson')\n\n        eval = SemanticSegmentationEvaluation(class_config)\n        eval.compute_vector(gt_uri, pred_uri, 'polygons', 0)\n\n        # NOTE: The  two geojson files referenced  above contain three\n        # unique geometries total, each  file contains two geometries,\n        # and there is one geometry shared between the two.\n        tp = 1.0\n        fp = 1.0\n        fn = 1.0\n        precision = float(tp) / (tp + fp)\n        recall = float(tp) / (tp + fn)\n\n        self.assertAlmostEqual(precision, eval.class_to_eval_item[0].precision)\n        self.assertAlmostEqual(recall, eval.class_to_eval_item[0].recall)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests_v2/core/evaluation/test_semantic_segmentation_evaluator.py,0,"b""import unittest\nfrom os.path import join\n\nimport numpy as np\nfrom shapely.geometry import shape\n\nfrom rastervision2.core.data import ClassConfig\nfrom rastervision2.core import Box\nfrom rastervision2.core.data import (\n    Scene, IdentityCRSTransformer,\n    SemanticSegmentationLabelSource, RasterizedSourceConfig, RasterizerConfig,\n    GeoJSONVectorSourceConfig, PolygonVectorOutputConfig)\nfrom rastervision2.core.evaluation import SemanticSegmentationEvaluator\nfrom rastervision2.pipeline import rv_config\nfrom rastervision2.pipeline.file_system import file_to_json\n\nfrom tests_v2.core.data.mock_raster_source import (MockRasterSource)\nfrom tests_v2 import data_file_path\n\n\nclass TestSemanticSegmentationEvaluator(unittest.TestCase):\n    def setUp(self):\n        self.tmp_dir = rv_config.get_tmp_dir()\n\n        self.class_config = ClassConfig(names=['one', 'two'])\n        self.class_config.update()\n        self.class_config.ensure_null_class()\n\n    def tearDown(self):\n        self.tmp_dir.cleanup()\n\n    def get_scene(self, class_id):\n        # Make scene where ground truth is all set to class_id\n        # and predictions are set to half 0's and half 1's\n        scene_id = str(class_id)\n        rs = MockRasterSource(channel_order=[0, 1, 2], num_channels=3)\n        rs.set_raster(np.zeros((10, 10, 3)))\n\n        gt_rs = MockRasterSource(channel_order=[0], num_channels=1)\n        gt_arr = np.full((10, 10, 1), class_id)\n        gt_rs.set_raster(gt_arr)\n        gt_ls = SemanticSegmentationLabelSource(raster_source=gt_rs)\n\n        pred_rs = MockRasterSource(channel_order=[0], num_channels=1)\n        pred_arr = np.zeros((10, 10, 1))\n        pred_arr[5:10, :, :] = 1\n        pred_rs.set_raster(pred_arr)\n        pred_ls = SemanticSegmentationLabelSource(raster_source=pred_rs)\n\n        return Scene(scene_id, rs, gt_ls, pred_ls)\n\n    def test_evaluator(self):\n        output_uri = join(self.tmp_dir.name, 'out.json')\n        scenes = [self.get_scene(0), self.get_scene(1)]\n        evaluator = SemanticSegmentationEvaluator(self.class_config, output_uri, None)\n        evaluator.process(scenes, self.tmp_dir.name)\n        eval_json = file_to_json(output_uri)\n        exp_eval_json = file_to_json(data_file_path('expected-eval.json'))\n        self.assertDictEqual(eval_json, exp_eval_json)\n\n    def get_vector_scene(self, class_id, use_aoi=False):\n        gt_uri = data_file_path('{}-gt-polygons.geojson'.format(class_id))\n        pred_uri = data_file_path('{}-pred-polygons.geojson'.format(class_id))\n\n        scene_id = str(class_id)\n        rs = MockRasterSource(channel_order=[0, 1, 3], num_channels=3)\n        rs.set_raster(np.zeros((10, 10, 3)))\n\n        crs_transformer = IdentityCRSTransformer()\n        extent = Box.make_square(0, 0, 360)\n\n        config = RasterizedSourceConfig(\n            vector_source=GeoJSONVectorSourceConfig(uri=gt_uri, default_class_id=0),\n            rasterizer_config=RasterizerConfig(\n                background_class_id=1))\n        gt_rs = config.build(self.class_config, crs_transformer, extent)\n        gt_ls = SemanticSegmentationLabelSource(raster_source=gt_rs)\n\n        config = RasterizedSourceConfig(\n            vector_source=GeoJSONVectorSourceConfig(uri=pred_uri, default_class_id=0),\n            rasterizer_config=RasterizerConfig(\n                background_class_id=1))\n        pred_rs = config.build(self.class_config, crs_transformer, extent)\n        pred_ls = SemanticSegmentationLabelSource(raster_source=pred_rs)\n        pred_ls.vector_output = [\n            PolygonVectorOutputConfig(\n                uri=pred_uri,\n                denoise=0,\n                class_id=class_id)\n        ]\n\n        if use_aoi:\n            aoi_uri = data_file_path('{}-aoi.geojson'.format(class_id))\n            aoi_geojson = file_to_json(aoi_uri)\n            aoi_polygons = [shape(aoi_geojson['features'][0]['geometry'])]\n            return Scene(scene_id, rs, gt_ls, pred_ls, aoi_polygons)\n\n        return Scene(scene_id, rs, gt_ls, pred_ls)\n\n    def test_vector_evaluator(self):\n        output_uri = join(self.tmp_dir.name, 'raster-out.json')\n        vector_output_uri = join(self.tmp_dir.name, 'vector-out.json')\n        scenes = [self.get_vector_scene(0), self.get_vector_scene(1)]\n        evaluator = SemanticSegmentationEvaluator(\n            self.class_config, output_uri, vector_output_uri)\n        evaluator.process(scenes, self.tmp_dir.name)\n        vector_eval_json = file_to_json(vector_output_uri)\n        exp_vector_eval_json = file_to_json(data_file_path('expected-vector-eval.json'))\n\n        # NOTE:  The precision  and recall  values found  in the  file\n        # `expected-vector-eval.json`  are equal to fractions of  the\n        # form (n-1)/n for  n <= 7 which  can be seen to  be (and have\n        # been manually verified to be) correct.\n        self.assertDictEqual(vector_eval_json, exp_vector_eval_json)\n\n    def test_vector_evaluator_with_aoi(self):\n        output_uri = join(self.tmp_dir.name, 'raster-out.json')\n        vector_output_uri = join(self.tmp_dir.name, 'vector-out.json')\n        scenes = [self.get_vector_scene(0, use_aoi=True)]\n        evaluator = SemanticSegmentationEvaluator(\n            self.class_config, output_uri, vector_output_uri)\n        evaluator.process(scenes, self.tmp_dir.name)\n        vector_eval_json = file_to_json(vector_output_uri)\n        exp_vector_eval_json = file_to_json(\n            data_file_path('expected-vector-eval-with-aoi.json'))\n\n        # NOTE:  The precision  and recall  values found  in the  file\n        # `expected-vector-eval.json`  are equal to fractions of  the\n        # form (n-1)/n for  n <= 7 which  can be seen to  be (and have\n        # been manually verified to be) correct.\n        self.assertDictEqual(vector_eval_json, exp_vector_eval_json)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
rastervision/backend/keras_classification/builders/__init__.py,0,b''
rastervision/backend/keras_classification/builders/model_builder.py,0,"b""import os\n\nfrom rastervision.backend.keras_classification.models.resnet50 import ResNet50\nfrom rastervision.protos.keras_classification.model_pb2 import Model\n\n\ndef build_from_path(model_path):\n    import keras\n\n    return keras.models.load_model(model_path)\n\n\ndef build(model_options, pretrained_model_path):\n    if os.path.isfile(model_options.model_path):\n        return build_from_path(model_options.model_path)\n\n    nb_channels = 3\n    input_shape = (model_options.input_size, model_options.input_size,\n                   nb_channels)\n    activation = 'softmax'\n\n    if model_options.type == Model.Type.Value('RESNET50'):\n        model = ResNet50(\n            include_top=True,\n            weights=pretrained_model_path,\n            load_weights_by_name=model_options.load_weights_by_name,\n            input_shape=input_shape,\n            classes=model_options.nb_classes,\n            activation=activation)\n    else:\n        raise ValueError(\n            Model.Type.Name(model_options.type) + ' is not a valid model_type')\n\n    return model\n"""
rastervision/backend/keras_classification/builders/optimizer_builder.py,0,"b""import keras\nfrom rastervision.protos.keras_classification.optimizer_pb2 import Optimizer\n\n\ndef build(optimizer_options):\n    if optimizer_options.type == Optimizer.Type.Value('ADAM'):\n        optimizer = keras.optimizers.Adam(lr=optimizer_options.init_lr)\n    elif optimizer_options.type == Optimizer.Type.Value('SGD'):\n        optimizer = keras.optimizers.SGD(lr=optimizer_options.init_lr)\n    else:\n        raise ValueError((Optimizer.Type.Name(optimizer_options.type) +\n                          ' is not a valid optimizer type'))\n\n    return optimizer\n"""
rastervision/backend/keras_classification/builders/trainer_builder.py,0,"b'from rastervision.backend.keras_classification.builders import optimizer_builder\nfrom rastervision.backend.keras_classification.core.trainer import Trainer\n\n\ndef build(trainer_config, model):\n    optimizer = optimizer_builder.build(trainer_config.optimizer)\n    trainer = Trainer(model, optimizer, trainer_config.options)\n    return trainer\n'"
rastervision/backend/keras_classification/commands/__init__.py,0,b''
rastervision/backend/keras_classification/commands/train.py,0,"b""import click\n\nfrom rastervision.backend.keras_classification.utils \\\n    import load_json_config\nfrom rastervision.backend.keras_classification.builders \\\n    import trainer_builder, model_builder\nfrom rastervision.protos.keras_classification.pipeline_pb2 \\\n    import PipelineConfig\n\n\ndef _train(config_path, pretrained_model_path, do_monitoring):\n    config = load_json_config(config_path, PipelineConfig())\n    model = model_builder.build(config.model, pretrained_model_path)\n    trainer = trainer_builder.build(config.trainer, model)\n    trainer.train(do_monitoring)\n\n\n@click.command()\n@click.argument('config_path')\ndef train(config_path):\n    _train(config_path)\n\n\nif __name__ == '__main__':\n    train()\n"""
rastervision/backend/keras_classification/core/__init__.py,0,b''
rastervision/backend/keras_classification/core/trainer.py,0,"b'import os\nfrom pathlib import Path\nfrom subprocess import Popen\nimport logging\n\nfrom rastervision.utils.misc import terminate_at_exit\nfrom rastervision.backend.keras_classification.utils import make_dir\nlog = logging.getLogger(__name__)\n\n\ndef get_nb_images(image_dir):\n    count = 0\n\n    pathlist = Path(image_dir).glob(\'**/*.png\')\n    for path in pathlist:\n        count += 1\n\n    pathlist = Path(image_dir).glob(\'**/*.jpg\')\n    for path in pathlist:\n        count += 1\n\n    return count\n\n\nclass Trainer(object):\n    def __init__(self, model, optimizer, options):\n        self.model = model\n        self.optimizer = optimizer\n        self.options = options\n\n        self.model_path = os.path.join(options.output_dir, \'model\')\n        make_dir(self.model_path, use_dirname=True)\n        self.weights_path = os.path.join(options.output_dir,\n                                         \'model-weights.hdf5\')\n        make_dir(self.weights_path, use_dirname=True)\n        self.log_path = os.path.join(options.output_dir, \'log.csv\')\n        make_dir(self.log_path, use_dirname=True)\n\n        self.training_gen = self.make_data_generator(options.training_data_dir)\n        self.validation_gen = self.make_data_generator(\n            options.validation_data_dir, validation_mode=True)\n\n        self.nb_training_samples = get_nb_images(options.training_data_dir)\n        self.nb_validation_samples = get_nb_images(options.validation_data_dir)\n\n        self.tf_logs_path = os.path.join(options.output_dir, \'logs\')\n\n    def make_callbacks(self, do_monitoring):\n        import keras\n\n        model_checkpoint = keras.callbacks.ModelCheckpoint(\n            filepath=self.model_path,\n            save_best_only=self.options.save_best,\n            save_weights_only=False)\n\n        weights_checkpoint = keras.callbacks.ModelCheckpoint(\n            filepath=self.weights_path,\n            save_best_only=self.options.save_best,\n            save_weights_only=True)\n\n        csv_logger = keras.callbacks.CSVLogger(self.log_path, append=True)\n\n        callbacks = [model_checkpoint, weights_checkpoint, csv_logger]\n\n        if self.options.lr_schedule:\n            lr_schedule = sorted(\n                self.options.lr_schedule, key=lambda x: x.epoch, reverse=True)\n\n            def schedule(curr_epoch):\n                for lr_schedule_item in lr_schedule:\n                    if curr_epoch >= lr_schedule_item.epoch:\n                        if self.options.debug:\n                            log.info(\'New lr: {}\'.format(lr_schedule_item.lr))\n                        return lr_schedule_item.lr\n\n            lr_scheduler = keras.callbacks.LearningRateScheduler(schedule)\n\n            callbacks.append(lr_scheduler)\n\n        if do_monitoring:\n            tensorboard = keras.callbacks.TensorBoard(\n                log_dir=self.tf_logs_path, write_images=True)\n\n            callbacks.append(tensorboard)\n\n        return callbacks\n\n    def get_initial_epoch(self):\n        """"""Get initial_epoch from the last line in the log CSV file.""""""\n        initial_epoch = 0\n        if os.path.isfile(self.log_path):\n            with open(self.log_path, \'r\') as log_file:\n                line_ind = 0\n                for line_ind, _ in enumerate(log_file):\n                    pass\n                initial_epoch = line_ind\n\n        return initial_epoch\n\n    def make_data_generator(self, image_folder_dir, validation_mode=False):\n        from keras.preprocessing.image import ImageDataGenerator\n\n        # Don\'t apply randomized data transforms if in validation mode.\n        # This will make the validation scores more comparable between epochs.\n        if validation_mode:\n            generator = ImageDataGenerator(rescale=1. / 255)\n        else:\n            generator = ImageDataGenerator(\n                rescale=1. / 255, horizontal_flip=True, vertical_flip=True)\n\n        generator = generator.flow_from_directory(\n            image_folder_dir,\n            classes=self.options.class_names,\n            target_size=(self.options.input_size, self.options.input_size),\n            batch_size=self.options.batch_size,\n            class_mode=\'categorical\')\n\n        return generator\n\n    def train(self, do_monitoring):\n        loss_function = \'categorical_crossentropy\'\n        metrics = [\'accuracy\']\n        initial_epoch = self.get_initial_epoch()\n        steps_per_epoch = int(\n            self.nb_training_samples / self.options.batch_size)\n        validation_steps = int(\n            self.nb_validation_samples / self.options.batch_size)\n\n        # Useful for testing\n        if self.options.short_epoch:\n            steps_per_epoch = 1\n            validation_steps = 1\n\n        callbacks = self.make_callbacks(do_monitoring)\n\n        self.model.compile(self.optimizer, loss_function, metrics=metrics)\n\n        if do_monitoring:\n            tensorboard_process = Popen(\n                [\'tensorboard\', \'--logdir={}\'.format(self.tf_logs_path)])\n            terminate_at_exit(tensorboard_process)\n\n        self.model.fit_generator(\n            self.training_gen,\n            initial_epoch=initial_epoch,\n            steps_per_epoch=steps_per_epoch,\n            epochs=self.options.nb_epochs,\n            validation_data=self.validation_gen,\n            validation_steps=validation_steps,\n            callbacks=callbacks)\n\n        if do_monitoring:\n            tensorboard_process.terminate()\n'"
rastervision/backend/keras_classification/models/__init__.py,0,b''
rastervision/backend/keras_classification/models/resnet50.py,0,"b'# flake8: noqa\n# -*- coding: utf-8 -*-\n""""""ResNet50 model for Keras.\n# Reference:\n- [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)\nAdapted from code contributed by BigMoyan.\nAdapted from Keras.\n""""""\nfrom __future__ import print_function\nfrom __future__ import absolute_import\n\nimport warnings\n\nfrom keras.layers import Input\nfrom keras import layers\nfrom keras.layers import Dense\nfrom keras.layers import Activation\nfrom keras.layers import Flatten\nfrom keras.layers import Conv2D\nfrom keras.layers import MaxPooling2D\nfrom keras.layers import ZeroPadding2D\nfrom keras.layers import AveragePooling2D\nfrom keras.layers import GlobalAveragePooling2D\nfrom keras.layers import GlobalMaxPooling2D\nfrom keras.layers import BatchNormalization\nfrom keras.models import Model\nfrom keras import backend as K\nfrom keras.engine.topology import get_source_inputs\nfrom keras.utils import layer_utils\nfrom keras.utils.data_utils import get_file\nfrom keras.applications.imagenet_utils import (decode_predictions,\n                                               preprocess_input)\n\n\ndef identity_block(input_tensor, kernel_size, filters, stage, block):\n    """"""The identity block is the block that has no conv layer at shortcut.\n    # Arguments\n        input_tensor: input tensor\n        kernel_size: defualt 3, the kernel size of middle conv layer at main path\n        filters: list of integers, the filterss of 3 conv layer at main path\n        stage: integer, current stage label, used for generating layer names\n        block: \'a\',\'b\'..., current block label, used for generating layer names\n    # Returns\n        Output tensor for the block.\n    """"""\n    filters1, filters2, filters3 = filters\n    if K.image_data_format() == \'channels_last\':\n        bn_axis = 3\n    else:\n        bn_axis = 1\n    conv_name_base = \'res\' + str(stage) + block + \'_branch\'\n    bn_name_base = \'bn\' + str(stage) + block + \'_branch\'\n    act_name = \'act\' + str(stage) + block\n\n    x = Conv2D(filters1, (1, 1), name=conv_name_base + \'2a\')(input_tensor)\n    x = BatchNormalization(axis=bn_axis, name=bn_name_base + \'2a\')(x)\n    x = Activation(\'relu\')(x)\n\n    x = Conv2D(\n        filters2, kernel_size, padding=\'same\', name=conv_name_base + \'2b\')(x)\n    x = BatchNormalization(axis=bn_axis, name=bn_name_base + \'2b\')(x)\n    x = Activation(\'relu\')(x)\n\n    x = Conv2D(filters3, (1, 1), name=conv_name_base + \'2c\')(x)\n    x = BatchNormalization(axis=bn_axis, name=bn_name_base + \'2c\')(x)\n\n    x = layers.add([x, input_tensor])\n    x = Activation(\'relu\', name=act_name)(x)\n    return x\n\n\ndef conv_block(input_tensor,\n               kernel_size,\n               filters,\n               stage,\n               block,\n               strides=(2, 2)):\n    """"""conv_block is the block that has a conv layer at shortcut\n    # Arguments\n        input_tensor: input tensor\n        kernel_size: defualt 3, the kernel size of middle conv layer at main path\n        filters: list of integers, the filterss of 3 conv layer at main path\n        stage: integer, current stage label, used for generating layer names\n        block: \'a\',\'b\'..., current block label, used for generating layer names\n    # Returns\n        Output tensor for the block.\n    Note that from stage 3, the first conv layer at main path is with strides=(2,2)\n    And the shortcut should have strides=(2,2) as well\n    """"""\n    filters1, filters2, filters3 = filters\n    if K.image_data_format() == \'channels_last\':\n        bn_axis = 3\n    else:\n        bn_axis = 1\n    conv_name_base = \'res\' + str(stage) + block + \'_branch\'\n    bn_name_base = \'bn\' + str(stage) + block + \'_branch\'\n    act_name = \'act\' + str(stage) + block\n\n    x = Conv2D(\n        filters1, (1, 1), strides=strides,\n        name=conv_name_base + \'2a\')(input_tensor)\n    x = BatchNormalization(axis=bn_axis, name=bn_name_base + \'2a\')(x)\n    x = Activation(\'relu\')(x)\n\n    x = Conv2D(\n        filters2, kernel_size, padding=\'same\', name=conv_name_base + \'2b\')(x)\n    x = BatchNormalization(axis=bn_axis, name=bn_name_base + \'2b\')(x)\n    x = Activation(\'relu\')(x)\n\n    x = Conv2D(filters3, (1, 1), name=conv_name_base + \'2c\')(x)\n    x = BatchNormalization(axis=bn_axis, name=bn_name_base + \'2c\')(x)\n\n    shortcut = Conv2D(\n        filters3, (1, 1), strides=strides,\n        name=conv_name_base + \'1\')(input_tensor)\n    shortcut = BatchNormalization(\n        axis=bn_axis, name=bn_name_base + \'1\')(shortcut)\n\n    x = layers.add([x, shortcut])\n    x = Activation(\'relu\', name=act_name)(x)\n    return x\n\n\ndef ResNet50(include_top=True,\n             weights=None,\n             load_weights_by_name=False,\n             input_tensor=None,\n             input_shape=None,\n             pooling=None,\n             classes=1000,\n             activation=\'softmax\'):\n    """"""Instantiates the ResNet50 architecture.\n    Optionally loads weights pre-trained\n    on ImageNet.\n    # Arguments\n        include_top: whether to include the fully-connected\n            layer at the top of the network.\n        weights: one of `None` (random initialization)\n            or a path to model weights.\n        input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)\n            to use as image input for the model.\n        input_shape: optional shape tuple, only to be specified\n            if `include_top` is False (otherwise the input shape\n            has to be `(224, 224, 3)` (with `channels_last` data format)\n            or `(3, 224, 244)` (with `channels_first` data format).\n            It should have exactly 3 inputs channels,\n            and width and height should be no smaller than 197.\n            E.g. `(200, 200, 3)` would be one valid value.\n        pooling: Optional pooling mode for feature extraction\n            when `include_top` is `False`.\n            - `None` means that the output of the model will be\n                the 4D tensor output of the\n                last convolutional layer.\n            - `avg` means that global average pooling\n                will be applied to the output of the\n                last convolutional layer, and thus\n                the output of the model will be a 2D tensor.\n            - `max` means that global max pooling will\n                be applied.\n        classes: optional number of classes to classify images\n            into, only to be specified if `include_top` is True, and\n            if no `weights` argument is specified.\n        activation: activation function for top fully-connected layer\n    # Returns\n        A Keras model instance.\n    # Raises\n        ValueError: in case of invalid argument for `weights`,\n            or invalid input shape.\n    """"""\n    if input_tensor is None:\n        img_input = Input(shape=input_shape)\n    else:\n        img_input = input_tensor\n\n    bn_axis = 3\n\n    x = ZeroPadding2D((3, 3))(img_input)\n    x = Conv2D(64, (7, 7), strides=(2, 2), name=\'conv1\')(x)\n    x = BatchNormalization(axis=bn_axis, name=\'bn_conv1\')(x)\n    x = Activation(\'relu\')(x)\n    x = MaxPooling2D((3, 3), strides=(2, 2))(x)\n\n    x = conv_block(x, 3, [64, 64, 256], stage=2, block=\'a\', strides=(1, 1))\n    x = identity_block(x, 3, [64, 64, 256], stage=2, block=\'b\')\n    x = identity_block(x, 3, [64, 64, 256], stage=2, block=\'c\')\n\n    x = conv_block(x, 3, [128, 128, 512], stage=3, block=\'a\')\n    x = identity_block(x, 3, [128, 128, 512], stage=3, block=\'b\')\n    x = identity_block(x, 3, [128, 128, 512], stage=3, block=\'c\')\n    x = identity_block(x, 3, [128, 128, 512], stage=3, block=\'d\')\n\n    x = conv_block(x, 3, [256, 256, 1024], stage=4, block=\'a\')\n    x = identity_block(x, 3, [256, 256, 1024], stage=4, block=\'b\')\n    x = identity_block(x, 3, [256, 256, 1024], stage=4, block=\'c\')\n    x = identity_block(x, 3, [256, 256, 1024], stage=4, block=\'d\')\n    x = identity_block(x, 3, [256, 256, 1024], stage=4, block=\'e\')\n    x = identity_block(x, 3, [256, 256, 1024], stage=4, block=\'f\')\n\n    x = conv_block(x, 3, [512, 512, 2048], stage=5, block=\'a\')\n    x = identity_block(x, 3, [512, 512, 2048], stage=5, block=\'b\')\n    x = identity_block(x, 3, [512, 512, 2048], stage=5, block=\'c\')\n\n    x = AveragePooling2D((7, 7), name=\'avg_pool\')(x)\n\n    if include_top:\n        x = Flatten()(x)\n        x = Dense(classes, activation=activation, name=\'dense\')(x)\n    else:\n        if pooling == \'avg\':\n            x = GlobalAveragePooling2D()(x)\n        elif pooling == \'max\':\n            x = GlobalMaxPooling2D()(x)\n\n    # Ensure that the model takes into account\n    # any potential predecessors of `input_tensor`.\n    if input_tensor is not None:\n        inputs = get_source_inputs(input_tensor)\n    else:\n        inputs = img_input\n    # Create model.\n    model = Model(inputs, x, name=\'resnet50\')\n\n    # load weights\n    if weights:\n        model.load_weights(weights, by_name=load_weights_by_name)\n\n    return model\n'"
rastervision/backend/torch_utils/chip_classification/__init__.py,0,b''
rastervision/backend/torch_utils/chip_classification/data.py,4,"b'import logging\nfrom os.path import join\n\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.utils.data.sampler import WeightedRandomSampler\nfrom albumentations.core.composition import Compose\nfrom albumentations.augmentations.transforms import (\n    Blur, RandomRotate90, HorizontalFlip, VerticalFlip, GaussianBlur,\n    GaussNoise, RGBShift, ToGray)\n\nfrom rastervision.backend.torch_utils.data import DataBunch\nfrom rastervision.backend.torch_utils.chip_classification.folder import ImageFolder\n\nlog = logging.getLogger(__name__)\n\n\ndef calculate_oversampling_weights(targets, rare_classes, desired_prob):\n    \'\'\'\n    Calculates weights tensor for oversampling\n\n    args:\n        targets: (list) containing the class labels/targets/y-data\n        rare classes: (list) of ints of the class labels that should be oversamples\n        desired prob: (float) a single probability that the rare classes should\n            have.\n    returns:\n        (tensor) with weights per index, e.g [0.5,0.1,0.9]\n    \'\'\'\n\n    chip_idx = []\n    for idx, target in enumerate(targets):\n        if target in rare_classes:\n            chip_idx.append(idx)\n\n    rare_weight = desired_prob / len(chip_idx)\n    common_weight = (1.0 - desired_prob) / (len(targets) - len(chip_idx))\n\n    weights = torch.full((len(targets), ), common_weight)\n    weights[chip_idx] = rare_weight\n\n    return weights\n\n\nclass AlbumentationDataset(Dataset):\n    """"""An adapter to use arbitrary datasets with albumentations transforms.""""""\n\n    def __init__(self, orig_dataset, transform=None):\n        """"""Constructor.\n\n        Args:\n            orig_dataset: (Dataset) which is assumed to return PIL Image objects\n                and not perform any transforms of its own\n            transform: (albumentations.core.transforms_interface.ImageOnlyTransform)\n        """"""\n        self.orig_dataset = orig_dataset\n        self.transform = transform\n\n    def __getitem__(self, ind):\n        x, y = self.orig_dataset[ind]\n        x = np.array(x)\n        if self.transform:\n            x = self.transform(image=x)[\'image\']\n        x = torch.tensor(x).permute(2, 0, 1).float() / 255.0\n        return x, y\n\n    def __len__(self):\n        return len(self.orig_dataset)\n\n\ndef build_databunch(data_dir, img_sz, batch_sz, class_names, rare_classes,\n                    desired_prob, augmentors):\n    num_workers = 4\n\n    train_dir = join(data_dir, \'train\')\n    valid_dir = join(data_dir, \'valid\')\n\n    augmentors_dict = {\n        \'Blur\': Blur(),\n        \'RandomRotate90\': RandomRotate90(),\n        \'HorizontalFlip\': HorizontalFlip(),\n        \'VerticalFlip\': VerticalFlip(),\n        \'GaussianBlur\': GaussianBlur(),\n        \'GaussNoise\': GaussNoise(),\n        \'RGBShift\': RGBShift(),\n        \'ToGray\': ToGray()\n    }\n\n    aug_transforms = []\n    for augmentor in augmentors:\n        try:\n            aug_transforms.append(augmentors_dict[augmentor])\n        except KeyError as e:\n            log.warning(\'{0} is an unknown augmentor. Continuing without {0}. \\\n                Known augmentors are: {1}\'\n                        .format(e, list(augmentors_dict.keys())))\n    aug_transforms = Compose(aug_transforms)\n\n    train_ds = AlbumentationDataset(\n        ImageFolder(train_dir, classes=class_names), transform=aug_transforms)\n    valid_ds = AlbumentationDataset(\n        ImageFolder(valid_dir, classes=class_names))\n\n    if rare_classes != []:\n        targets = [target for _, target in train_ds.orig_dataset.imgs]\n        train_sample_weights = calculate_oversampling_weights(\n            targets, rare_classes, desired_prob)\n        num_train_samples = len(train_ds)\n        train_sampler = WeightedRandomSampler(\n            weights=train_sample_weights,\n            num_samples=num_train_samples,\n            replacement=True)\n        shuffle = False\n    else:\n        train_sampler = None\n        shuffle = True\n\n    train_dl = DataLoader(\n        train_ds,\n        shuffle=shuffle,\n        batch_size=batch_sz,\n        num_workers=num_workers,\n        drop_last=True,\n        pin_memory=True,\n        sampler=train_sampler)\n    valid_dl = DataLoader(\n        valid_ds,\n        batch_size=batch_sz,\n        num_workers=num_workers,\n        pin_memory=True)\n\n    return DataBunch(train_ds, train_dl, valid_ds, valid_dl, class_names)\n'"
rastervision/backend/torch_utils/chip_classification/folder.py,0,"b'# flake8: noqa\n# Adapted from https://github.com/pytorch/vision/blob/master/torchvision/datasets/folder.py\n# This makes it so a list of classes can be passed to ImageFolder in order to\n# force it to use a specific mapping from class_name to index.\n\nfrom torchvision.datasets.vision import VisionDataset\n\nfrom PIL import Image\n\nimport os\nimport os.path\nimport sys\n\n\ndef has_file_allowed_extension(filename, extensions):\n    """"""Checks if a file is an allowed extension.\n    Args:\n        filename (string): path to a file\n        extensions (tuple of strings): extensions to consider (lowercase)\n    Returns:\n        bool: True if the filename ends with one of given extensions\n    """"""\n    return filename.lower().endswith(extensions)\n\n\ndef is_image_file(filename):\n    """"""Checks if a file is an allowed image extension.\n    Args:\n        filename (string): path to a file\n    Returns:\n        bool: True if the filename ends with a known image extension\n    """"""\n    return has_file_allowed_extension(filename, IMG_EXTENSIONS)\n\n\ndef make_dataset(dir, class_to_idx, extensions=None, is_valid_file=None):\n    images = []\n    dir = os.path.expanduser(dir)\n    if not ((extensions is None) ^ (is_valid_file is None)):\n        raise ValueError(\n            \'Both extensions and is_valid_file cannot be None or not None at the same time\'\n        )\n    if extensions is not None:\n\n        def is_valid_file(x):\n            return has_file_allowed_extension(x, extensions)\n\n    for target in sorted(class_to_idx.keys()):\n        d = os.path.join(dir, target)\n        if not os.path.isdir(d):\n            continue\n        for root, _, fnames in sorted(os.walk(d)):\n            for fname in sorted(fnames):\n                path = os.path.join(root, fname)\n                if is_valid_file(path):\n                    item = (path, class_to_idx[target])\n                    images.append(item)\n\n    return images\n\n\nclass DatasetFolder(VisionDataset):\n    """"""A generic data loader where the samples are arranged in this way: ::\n        root/class_x/xxx.ext\n        root/class_x/xxy.ext\n        root/class_x/xxz.ext\n        root/class_y/123.ext\n        root/class_y/nsdf3.ext\n        root/class_y/asd932_.ext\n    Args:\n        root (string): Root directory path.\n        loader (callable): A function to load a sample given its path.\n        extensions (tuple[string]): A list of allowed extensions.\n            both extensions and is_valid_file should not be passed.\n        transform (callable, optional): A function/transform that takes in\n            a sample and returns a transformed version.\n            E.g, ``transforms.RandomCrop`` for images.\n        target_transform (callable, optional): A function/transform that takes\n            in the target and transforms it.\n        is_valid_file (callable, optional): A function that takes path of a file\n            and check if the file is a valid file (used to check of corrupt files)\n            both extensions and is_valid_file should not be passed.\n     Attributes:\n        classes (list): List of the class names.\n        class_to_idx (dict): Dict with items (class_name, class_index).\n        samples (list): List of (sample path, class_index) tuples\n        targets (list): The class_index value for each image in the dataset\n    """"""\n\n    def __init__(self,\n                 root,\n                 loader,\n                 extensions=None,\n                 transform=None,\n                 target_transform=None,\n                 is_valid_file=None,\n                 classes=None):\n        super(DatasetFolder, self).__init__(\n            root, transform=transform, target_transform=target_transform)\n        _classes, _class_to_idx = self._find_classes(self.root)\n\n        if classes is not None:\n            if set(classes) != set(_classes):\n                raise RuntimeError(\n                    \'classes must contain all class folders present in root\')\n            class_to_idx = dict(zip(classes, range(len(classes))))\n        else:\n            classes = _classes\n            class_to_idx = _class_to_idx\n\n        samples = make_dataset(self.root, class_to_idx, extensions,\n                               is_valid_file)\n        if len(samples) == 0:\n            raise (RuntimeError(\n                \'Found 0 files in subfolders of: \' + self.root + \'\\n\'\n                \'Supported extensions are: \' + \',\'.join(extensions)))\n\n        self.loader = loader\n        self.extensions = extensions\n\n        self.classes = classes\n        self.class_to_idx = class_to_idx\n        self.samples = samples\n        self.targets = [s[1] for s in samples]\n\n    def _find_classes(self, dir):\n        """"""\n        Finds the class folders in a dataset.\n        Args:\n            dir (string): Root directory path.\n        Returns:\n            tuple: (classes, class_to_idx) where classes are relative to (dir), and class_to_idx is a dictionary.\n        Ensures:\n            No class is a subdirectory of another.\n        """"""\n        if sys.version_info >= (3, 5):\n            # Faster and available in Python 3.5 and above\n            classes = [d.name for d in os.scandir(dir) if d.is_dir()]\n        else:\n            classes = [\n                d for d in os.listdir(dir)\n                if os.path.isdir(os.path.join(dir, d))\n            ]\n        classes.sort()\n        class_to_idx = {classes[i]: i for i in range(len(classes))}\n        return classes, class_to_idx\n\n    def __getitem__(self, index):\n        """"""\n        Args:\n            index (int): Index\n        Returns:\n            tuple: (sample, target) where target is class_index of the target class.\n        """"""\n        path, target = self.samples[index]\n        sample = self.loader(path)\n        if self.transform is not None:\n            sample = self.transform(sample)\n        if self.target_transform is not None:\n            target = self.target_transform(target)\n\n        return sample, target\n\n    def __len__(self):\n        return len(self.samples)\n\n\nIMG_EXTENSIONS = (\'.jpg\', \'.jpeg\', \'.png\', \'.ppm\', \'.bmp\', \'.pgm\', \'.tif\',\n                  \'.tiff\', \'.webp\')\n\n\ndef pil_loader(path):\n    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n    with open(path, \'rb\') as f:\n        img = Image.open(f)\n        return img.convert(\'RGB\')\n\n\ndef accimage_loader(path):\n    import accimage\n    try:\n        return accimage.Image(path)\n    except IOError:\n        # Potentially a decoding problem, fall back to PIL.Image\n        return pil_loader(path)\n\n\ndef default_loader(path):\n    from torchvision import get_image_backend\n    if get_image_backend() == \'accimage\':\n        return accimage_loader(path)\n    else:\n        return pil_loader(path)\n\n\nclass ImageFolder(DatasetFolder):\n    """"""A generic data loader where the images are arranged in this way: ::\n        root/dog/xxx.png\n        root/dog/xxy.png\n        root/dog/xxz.png\n        root/cat/123.png\n        root/cat/nsdf3.png\n        root/cat/asd932_.png\n    Args:\n        root (string): Root directory path.\n        transform (callable, optional): A function/transform that  takes in an PIL image\n            and returns a transformed version. E.g, ``transforms.RandomCrop``\n        target_transform (callable, optional): A function/transform that takes in the\n            target and transforms it.\n        loader (callable, optional): A function to load an image given its path.\n        is_valid_file (callable, optional): A function that takes path of an Image file\n            and check if the file is a valid file (used to check of corrupt files)\n     Attributes:\n        classes (list): List of the class names.\n        class_to_idx (dict): Dict with items (class_name, class_index).\n        imgs (list): List of (image path, class_index) tuples\n    """"""\n\n    def __init__(self,\n                 root,\n                 transform=None,\n                 target_transform=None,\n                 loader=default_loader,\n                 is_valid_file=None,\n                 classes=None):\n        super(ImageFolder, self).__init__(\n            root,\n            loader,\n            IMG_EXTENSIONS if is_valid_file is None else None,\n            transform=transform,\n            target_transform=target_transform,\n            is_valid_file=is_valid_file,\n            classes=classes)\n        self.imgs = self.samples\n'"
rastervision/backend/torch_utils/chip_classification/model.py,0,"b'from torchvision import models\nfrom torch import nn\n\n\ndef get_model(model_arch, num_labels, pretrained=True):\n    model = getattr(models, model_arch)(pretrained=pretrained, progress=True)\n    model.fc = nn.Linear(model.fc.in_features, num_labels)\n    return model\n'"
rastervision/backend/torch_utils/chip_classification/plot.py,0,"b""def plot_xy(ax, x, y=None, label_names=None):\n    ax.imshow(x.permute(1, 2, 0))\n    if y is not None:\n        ax.set_title(label_names[y])\n    ax.axis('off')\n"""
rastervision/backend/torch_utils/chip_classification/train.py,2,"b""import click\nimport torch\n\nfrom rastervision.backend.torch_utils.metrics import (compute_conf_mat_metrics,\n                                                      compute_conf_mat)\n\n\ndef train_epoch(model, device, data_loader, opt, loss_fn, step_scheduler=None):\n    model.train()\n    total_loss = 0.0\n    num_samples = 0\n\n    with click.progressbar(data_loader, label='Training') as bar:\n        for batch_ind, (x, y) in enumerate(bar):\n            x = x.to(device)\n            y = y.to(device)\n\n            opt.zero_grad()\n            out = model(x)\n            loss = loss_fn(out, y)\n            loss.backward()\n            total_loss += loss.item()\n            opt.step()\n            if step_scheduler:\n                step_scheduler.step()\n            num_samples += x.shape[0]\n\n    return total_loss / num_samples\n\n\ndef validate_epoch(model, device, data_loader, num_labels):\n    model.eval()\n\n    conf_mat = torch.zeros((num_labels, num_labels))\n    with torch.no_grad():\n        with click.progressbar(data_loader, label='Validating') as bar:\n            for batch_ind, (x, y) in enumerate(bar):\n                x = x.to(device)\n                out = model(x)\n\n                out = out.argmax(-1).view(-1).cpu()\n                y = y.cpu()\n                conf_mat += compute_conf_mat(out, y, num_labels)\n\n    return compute_conf_mat_metrics(conf_mat)\n"""
rastervision/backend/torch_utils/object_detection/__init__.py,0,b''
rastervision/backend/torch_utils/object_detection/boxlist.py,12,"b'from collections import defaultdict\n\nimport torch\n\nfrom torchvision.ops.boxes import batched_nms\n\n\ndef to_box_pixel(boxes, img_height, img_width):\n    # convert from (ymin, xmin, ymax, xmax) in range [-1,1] to\n    # range [0, h) or [0, w)\n    boxes = ((boxes + 1.0) / 2.0) * torch.tensor(\n        [[img_height, img_width, img_height, img_width]]).to(\n            device=boxes.device, dtype=torch.float)\n    return boxes\n\n\nclass BoxList():\n    def __init__(self, boxes, **extras):\n        """"""Constructor.\n\n        Args:\n            boxes: tensor<n, 4> with order ymin, xmin, ymax, xmax in pixels coords\n            extras: dict with values that are tensors with first dimension corresponding\n                to boxes first dimension\n        """"""\n        self.boxes = boxes\n        self.extras = extras\n\n    def get_field(self, name):\n        if name == \'boxes\':\n            return self.boxes\n        else:\n            return self.extras.get(name)\n\n    def _map_extras(self, func):\n        new_extras = {}\n        for k, v in self.extras.items():\n            new_extras[k] = func(v)\n        return new_extras\n\n    def copy(self):\n        return BoxList(self.boxes.copy(),\n                       **self._map_extras(lambda x: x.copy()))\n\n    def cpu(self):\n        return BoxList(self.boxes.cpu(), **self._map_extras(lambda x: x.cpu()))\n\n    def cuda(self):\n        return BoxList(self.boxes.cuda(),\n                       **self._map_extras(lambda x: x.cuda()))\n\n    def to(self, device):\n        return self.cpu() if device == \'cpu\' else self.cuda()\n\n    def xyxy(self):\n        boxes = self.boxes[:, [1, 0, 3, 2]]\n        return BoxList(boxes, **self.extras)\n\n    def yxyx(self):\n        boxes = self.boxes[:, [1, 0, 3, 2]]\n        return BoxList(boxes, **self.extras)\n\n    def __len__(self):\n        return self.boxes.shape[0]\n\n    @staticmethod\n    def cat(box_lists):\n        boxes = []\n        extras = defaultdict(list)\n        for bl in box_lists:\n            boxes.append(bl.boxes)\n            for k, v in bl.extras.items():\n                extras[k].append(v)\n        boxes = torch.cat(boxes)\n        for k, v in extras.items():\n            extras[k] = torch.cat(v)\n        return BoxList(boxes, **extras)\n\n    def equal(self, other):\n        if len(other) != len(self):\n            return False\n\n        # Ignore order of boxes.\n        extras = [(v.float().unsqueeze(1) if v.ndim == 1 else v.float())\n                  for v in self.extras.values()]\n        cat_arr = torch.cat([self.boxes] + extras, 1)\n        self_tups = set([tuple([x.item() for x in row]) for row in cat_arr])\n\n        extras = [(v.float().unsqueeze(1) if v.ndim == 1 else v.float())\n                  for v in other.extras.values()]\n        cat_arr = torch.cat([other.boxes] + extras, 1)\n        other_tups = set([tuple([x.item() for x in row]) for row in cat_arr])\n        return self_tups == other_tups\n\n    def ind_filter(self, inds):\n        new_extras = {}\n        for k, v in self.extras.items():\n            new_extras[k] = v[inds, ...]\n        return BoxList(self.boxes[inds, :], **new_extras)\n\n    def score_filter(self, score_thresh=0.25):\n        scores = self.extras.get(\'scores\')\n        if scores is not None:\n            return self.ind_filter(scores > score_thresh)\n        else:\n            raise ValueError(\'must have scores as key in extras\')\n\n    def clamp(self, img_height, img_width):\n        boxes = torch.stack(\n            [\n                torch.clamp(self.boxes[:, 0], 0, img_height),\n                torch.clamp(self.boxes[:, 1], 0, img_width),\n                torch.clamp(self.boxes[:, 2], 0, img_height),\n                torch.clamp(self.boxes[:, 3], 0, img_width)\n            ],\n            dim=1)\n        return BoxList(boxes, **self.extras)\n\n    def nms(self, iou_thresh=0.5):\n        if len(self) == 0:\n            return self\n\n        good_inds = batched_nms(self.boxes, self.get_field(\'scores\'),\n                                self.get_field(\'labels\'), iou_thresh)\n        return self.ind_filter(good_inds)\n\n    def scale(self, yscale, xscale):\n        boxes = self.boxes * torch.tensor(\n            [[yscale, xscale, yscale, xscale]], device=self.boxes.device)\n        return BoxList(boxes, **self.extras)\n\n    def pin_memory(self):\n        self.boxes = self.boxes.pin_memory()\n        for k, v in self.extras.items():\n            self.extras[k] = v.pin_memory()\n        return self\n'"
rastervision/backend/torch_utils/object_detection/data.py,7,"b""from os.path import join\nfrom collections import defaultdict\nimport random\nimport glob\n\nfrom PIL import Image\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn.functional as F\nimport torchvision\n\nfrom rastervision.utils.files import (file_to_json)\nfrom rastervision.backend.torch_utils.object_detection.boxlist import BoxList\nfrom rastervision.backend.torch_utils.data import DataBunch\n\n\nclass ToTensor(object):\n    def __init__(self):\n        self.to_tensor = torchvision.transforms.ToTensor()\n\n    def __call__(self, x, y):\n        return (self.to_tensor(x), y)\n\n\nclass ScaleTransform(object):\n    def __init__(self, height, width):\n        self.height = height\n        self.width = width\n\n    def __call__(self, x, y):\n        yscale = self.height / x.shape[1]\n        xscale = self.width / x.shape[2]\n        x = F.interpolate(\n            x.unsqueeze(0), size=(self.height, self.width), mode='bilinear')[0]\n        return (x, y.scale(yscale, xscale))\n\n\nclass RandomHorizontalFlip(object):\n    def __init__(self, prob=0.5):\n        self.prob = prob\n\n    def __call__(self, x, y):\n        if random.random() < self.prob:\n            height, width = x.shape[-2:]\n            x = x.flip(-1)\n\n            boxes = y.boxes\n            boxes[:, [1, 3]] = width - boxes[:, [3, 1]]\n            y.boxes = boxes\n\n        return (x, y)\n\n\nclass ComposeTransforms(object):\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, x, y):\n        for t in self.transforms:\n            x, y = t(x, y)\n        return x, y\n\n\ndef collate_fn(data):\n    x = [d[0].unsqueeze(0) for d in data]\n    y = [d[1] for d in data]\n    return (torch.cat(x), y)\n\n\nclass CocoDataset(Dataset):\n    def __init__(self, img_dir, annotation_uris, transforms=None):\n        self.img_dir = img_dir\n        self.annotation_uris = annotation_uris\n        self.transforms = transforms\n\n        self.imgs = []\n        self.img2id = {}\n        self.id2img = {}\n        self.id2boxes = defaultdict(lambda: [])\n        self.id2labels = defaultdict(lambda: [])\n        self.label2name = {}\n        for annotation_uri in annotation_uris:\n            ann_json = file_to_json(annotation_uri)\n            for img in ann_json['images']:\n                self.imgs.append(img['file_name'])\n                self.img2id[img['file_name']] = img['id']\n                self.id2img[img['id']] = img['file_name']\n            for ann in ann_json['annotations']:\n                img_id = ann['image_id']\n                box = ann['bbox']\n                label = ann['category_id']\n                box = torch.tensor(\n                    [[box[1], box[0], box[1] + box[3], box[0] + box[2]]])\n                self.id2boxes[img_id].append(box)\n                self.id2labels[img_id].append(label)\n        self.id2boxes = dict([(id, torch.cat(boxes).float())\n                              for id, boxes in self.id2boxes.items()])\n        self.id2labels = dict([(id, torch.tensor(labels))\n                               for id, labels in self.id2labels.items()])\n\n    def __getitem__(self, ind):\n        img_fn = self.imgs[ind]\n        img_id = self.img2id[img_fn]\n        img = Image.open(join(self.img_dir, img_fn))\n\n        if img_id in self.id2boxes:\n            boxes, labels = self.id2boxes[img_id], self.id2labels[img_id]\n            boxlist = BoxList(boxes, labels=labels)\n        else:\n            boxlist = BoxList(\n                torch.empty((0, 4)), labels=torch.empty((0, )).long())\n        if self.transforms:\n            return self.transforms(img, boxlist)\n        return (img, boxlist)\n\n    def __len__(self):\n        return len(self.imgs)\n\n\ndef get_label_names(coco_path):\n    categories = file_to_json(coco_path)['categories']\n    label2name = dict([(cat['id'], cat['name']) for cat in categories])\n    labels = ['background'\n              ] + [label2name[i] for i in range(1,\n                                                len(label2name) + 1)]\n    return labels\n\n\ndef build_databunch(data_dir, img_sz, batch_sz):\n    # TODO This is to avoid freezing in the middle of the first epoch. Would be nice\n    # to fix this.\n    num_workers = 0\n\n    train_dir = join(data_dir, 'train')\n    train_anns = glob.glob(join(train_dir, '*.json'))\n    valid_dir = join(data_dir, 'valid')\n    valid_anns = glob.glob(join(valid_dir, '*.json'))\n\n    label_names = get_label_names(train_anns[0])\n    aug_transforms = ComposeTransforms(\n        [ToTensor(),\n         ScaleTransform(img_sz, img_sz),\n         RandomHorizontalFlip()])\n    transforms = ComposeTransforms(\n        [ToTensor(), ScaleTransform(img_sz, img_sz)])\n\n    train_ds = CocoDataset(train_dir, train_anns, transforms=aug_transforms)\n    valid_ds = CocoDataset(valid_dir, valid_anns, transforms=transforms)\n    train_ds.label_names = label_names\n    valid_ds.label_names = label_names\n\n    train_dl = DataLoader(\n        train_ds,\n        shuffle=True,\n        collate_fn=collate_fn,\n        batch_size=batch_sz,\n        num_workers=num_workers,\n        drop_last=True,\n        pin_memory=True)\n    valid_dl = DataLoader(\n        valid_ds,\n        collate_fn=collate_fn,\n        batch_size=batch_sz,\n        num_workers=num_workers,\n        pin_memory=True)\n    return DataBunch(train_ds, train_dl, valid_ds, valid_dl, label_names)\n"""
rastervision/backend/torch_utils/object_detection/metrics.py,0,"b'from os.path import join\nimport tempfile\n\nimport pycocotools\nfrom pycocotools.coco import COCO\nfrom pycocotools.cocoeval import COCOeval\nimport numpy as np\n\nfrom rastervision.utils.files import json_to_file\n\n\ndef get_coco_gt(targets, num_labels):\n    images = []\n    annotations = []\n    ann_id = 1\n    for img_id, target in enumerate(targets, 1):\n        # Use fake height, width, and filename because they don\'t matter.\n        images.append({\n            \'id\': img_id,\n            \'height\': 1000,\n            \'width\': 1000,\n            \'file_name\': \'{}.png\'.format(img_id)\n        })\n        boxes, labels = target.boxes, target.get_field(\'labels\')\n        for box, label in zip(boxes, labels):\n            box = box.float().tolist()\n            label = label.item()\n            annotations.append({\n                \'id\':\n                ann_id,\n                \'image_id\':\n                img_id,\n                \'category_id\':\n                label,\n                \'area\': (box[2] - box[0]) * (box[3] - box[1]),\n                \'bbox\': [box[1], box[0], box[3] - box[1], box[2] - box[0]],\n                \'iscrowd\':\n                0\n            })\n            ann_id += 1\n\n    categories = [{\n        \'id\': label,\n        \'name\': str(label),\n        \'supercategory\': \'super\'\n    } for label in range(num_labels)]\n    coco = {\n        \'images\': images,\n        \'annotations\': annotations,\n        \'categories\': categories\n    }\n    return coco\n\n\ndef get_coco_preds(outputs):\n    preds = []\n    for img_id, output in enumerate(outputs, 1):\n        for box, label, score in zip(output.boxes, output.get_field(\'labels\'),\n                                     output.get_field(\'scores\')):\n            box = box.float().tolist()\n            label = label.item()\n            score = score.item()\n            preds.append({\n                \'image_id\':\n                img_id,\n                \'category_id\':\n                label,\n                \'bbox\': [box[1], box[0], box[3] - box[1], box[2] - box[0]],\n                \'score\':\n                score\n            })\n    return preds\n\n\ndef compute_coco_eval(outputs, targets, num_labels):\n    """"""Return mAP averaged over 0.5-0.95 using pycocotools eval.\n\n    Note: boxes are in (ymin, xmin, ymax, xmax) format with values ranging\n        from 0 to h or w.\n\n    Args:\n        outputs: (list) of length m containing dicts of form\n            {\'boxes\': <tensor with shape (n, 4)>,\n             \'labels\': <tensor with shape (n,)>,\n             \'scores\': <tensor with shape (n,)>}\n        targets: (list) of length m containing dicts of form\n            {\'boxes\': <tensor with shape (n, 4)>,\n             \'labels\': <tensor with shape (n,)>}\n    """"""\n    with tempfile.TemporaryDirectory() as tmp_dir:\n        preds = get_coco_preds(outputs)\n        # ap is undefined when there are no predicted boxes\n        if len(preds) == 0:\n            return None\n\n        gt = get_coco_gt(targets, num_labels)\n        gt_path = join(tmp_dir, \'gt.json\')\n        json_to_file(gt, gt_path)\n        coco_gt = COCO(gt_path)\n\n        pycocotools.coco.unicode = None\n        coco_preds = coco_gt.loadRes(preds)\n\n        ann_type = \'bbox\'\n        coco_eval = COCOeval(coco_gt, coco_preds, ann_type)\n\n        coco_eval.evaluate()\n        coco_eval.accumulate()\n        coco_eval.summarize()\n\n        return coco_eval\n\n\ndef compute_class_f1(coco_eval):\n    precision = coco_eval.eval[\'precision\'][0, :, :, 0, -1]\n    scores = coco_eval.eval[\'scores\'][0, :, :, 0, -1]\n    recall = np.linspace(0, 1, num=precision.shape[0])\n    recall = recall[:, None]\n\n    f1s = (2 * precision * recall) / (np.maximum(precision + recall, 1e-4))\n    best_f1s = f1s.max(axis=0)\n    best_f1_inds = f1s.argmax(axis=0)\n    best_scores = scores[best_f1_inds, range(len(best_f1_inds))]\n\n    return best_f1s, best_scores\n'"
rastervision/backend/torch_utils/object_detection/model.py,6,"b'import torch\nimport torch.nn as nn\nfrom torchvision.models.detection.faster_rcnn import FasterRCNN\nfrom torchvision.models.detection.backbone_utils import BackboneWithFPN\nfrom torchvision.models import resnet\nfrom torchvision.ops import misc as misc_nn_ops\n\nfrom rastervision.backend.torch_utils.object_detection.boxlist import BoxList\n\n\ndef get_out_channels(model):\n    out = {}\n\n    def make_save_output(layer_name):\n        def save_output(layer, input, output):\n            out[layer_name] = output.shape[1]\n\n        return save_output\n\n    model.layer1.register_forward_hook(make_save_output(\'layer1\'))\n    model.layer2.register_forward_hook(make_save_output(\'layer2\'))\n    model.layer3.register_forward_hook(make_save_output(\'layer3\'))\n    model.layer4.register_forward_hook(make_save_output(\'layer4\'))\n\n    model(torch.empty((1, 3, 128, 128)))\n    return [out[\'layer1\'], out[\'layer2\'], out[\'layer3\'], out[\'layer4\']]\n\n\n# This fixes a bug in torchvision.\ndef resnet_fpn_backbone(backbone_name, pretrained):\n    backbone = resnet.__dict__[backbone_name](\n        pretrained=pretrained, norm_layer=misc_nn_ops.FrozenBatchNorm2d)\n\n    # freeze layers\n    for name, parameter in backbone.named_parameters():\n        if \'layer2\' not in name and \'layer3\' not in name and \'layer4\' not in name:\n            parameter.requires_grad_(False)\n\n    return_layers = {\'layer1\': 0, \'layer2\': 1, \'layer3\': 2, \'layer4\': 3}\n\n    out_channels = 256\n    in_channels_list = get_out_channels(backbone)\n    return BackboneWithFPN(backbone, return_layers, in_channels_list,\n                           out_channels)\n\n\nclass MyFasterRCNN(nn.Module):\n    """"""Adapter around torchvision Faster-RCNN.\n\n    The purpose of the adapter is to use a different input and output format\n    and inject bogus boxes to circumvent torchvision\'s inability to handle\n    training examples with no ground truth boxes.\n    """"""\n\n    def __init__(self, backbone_arch, num_labels, img_sz, pretrained=True):\n        super().__init__()\n\n        backbone = resnet_fpn_backbone(backbone_arch, pretrained)\n        self.model = FasterRCNN(\n            backbone, num_labels, min_size=img_sz, max_size=img_sz)\n        self.subloss_names = [\n            \'total_loss\', \'loss_box_reg\', \'loss_classifier\', \'loss_objectness\',\n            \'loss_rpn_box_reg\'\n        ]\n\n    def forward(self, input, targets=None):\n        """"""Forward pass\n\n        Args:\n            input: tensor<n, 3, h, w> with batch of images\n            targets: None or list<BoxList> of length n with boxes and labels\n\n        Returns:\n            if targets is None, returns list<BoxList> of length n, containing\n            boxes, labels, and scores for boxes with score > 0.05. Further\n            filtering based on score should be done before considering the\n            prediction ""final"".\n\n            if targets is a list, returns the losses as dict with keys from\n            self.subloss_names.\n        """"""\n        if targets:\n            # Add bogus background class box for each image to workaround\n            # the inability of torchvision to train on images with\n            # no ground truth boxes. This is important for being able\n            # to handle negative chips generated by RV.\n            new_targets = []\n            for x, y in zip(input, targets):\n                h, w = x.shape[1:]\n                boxes = torch.cat(\n                    [\n                        y.boxes,\n                        torch.tensor([[0., 0, h, w]], device=input.device)\n                    ],\n                    dim=0)\n                labels = torch.cat(\n                    [\n                        y.get_field(\'labels\'),\n                        torch.tensor([0], device=input.device)\n                    ],\n                    dim=0)\n                bl = BoxList(boxes, labels=labels)\n                new_targets.append(bl)\n            targets = new_targets\n\n            _targets = [bl.xyxy() for bl in targets]\n            _targets = [{\n                \'boxes\': bl.boxes,\n                \'labels\': bl.get_field(\'labels\')\n            } for bl in _targets]\n            loss_dict = self.model(input, _targets)\n            loss_dict[\'total_loss\'] = sum(list(loss_dict.values()))\n            return loss_dict\n\n        out = self.model(input)\n        boxlists = [\n            BoxList(\n                _out[\'boxes\'], labels=_out[\'labels\'],\n                scores=_out[\'scores\']).yxyx() for _out in out\n        ]\n\n        # Remove bogus background boxes.\n        new_boxlists = []\n        for bl in boxlists:\n            labels = bl.get_field(\'labels\')\n            non_zero_inds = labels != 0\n            new_boxlists.append(bl.ind_filter(non_zero_inds))\n        return new_boxlists\n'"
rastervision/backend/torch_utils/object_detection/plot.py,0,"b""import matplotlib.patches as patches\n\n\ndef plot_xy(ax, x, y=None, label_names=None):\n    ax.imshow(x.permute(1, 2, 0))\n\n    if y is not None:\n        scores = y.get_field('scores')\n        for box_ind, (box, label) in enumerate(\n                zip(y.boxes, y.get_field('labels'))):\n            rect = patches.Rectangle(\n                (box[1], box[0]),\n                box[3] - box[1],\n                box[2] - box[0],\n                linewidth=1,\n                edgecolor='cyan',\n                facecolor='none')\n            ax.add_patch(rect)\n\n            label_name = label_names[label]\n            if scores is not None:\n                score = scores[box_ind]\n                label_name += ' {:.2f}'.format(score)\n\n            h, w = x.shape[1:]\n            label_height = h * 0.03\n            label_width = w * 0.15\n            rect = patches.Rectangle(\n                (box[1], box[0] - label_height),\n                label_width,\n                label_height,\n                color='cyan')\n            ax.add_patch(rect)\n\n            ax.text(\n                box[1] + w * 0.003, box[0] - h * 0.003, label_name, fontsize=7)\n\n    ax.axis('off')\n"""
rastervision/backend/torch_utils/object_detection/train.py,1,"b""from collections import defaultdict\nimport warnings\n\nimport click\nimport torch\nimport numpy as np\n\nfrom rastervision.backend.torch_utils.object_detection.metrics import (\n    compute_coco_eval, compute_class_f1)\n\nwarnings.filterwarnings('ignore')\n\n\ndef train_epoch(model,\n                device,\n                data_loader,\n                opt,\n                step_scheduler=None,\n                epoch_scheduler=None):\n    model.train()\n    train_loss = defaultdict(lambda: 0.0)\n    num_samples = 0\n\n    with click.progressbar(data_loader, label='Training') as bar:\n        for batch_ind, (x, y) in enumerate(bar):\n            x = x.to(device)\n            y = [_y.to(device) for _y in y]\n\n            opt.zero_grad()\n            loss_dict = model(x, y)\n            loss_dict['total_loss'].backward()\n            opt.step()\n            if step_scheduler:\n                step_scheduler.step()\n\n            for k, v in loss_dict.items():\n                train_loss[k] += v.item()\n            num_samples += x.shape[0]\n\n    for k, v in train_loss.items():\n        train_loss[k] = v / num_samples\n\n    return dict(train_loss)\n\n\ndef validate_epoch(model, device, data_loader, num_labels):\n    model.eval()\n\n    ys = []\n    outs = []\n    with torch.no_grad():\n        with click.progressbar(data_loader, label='Validating') as bar:\n            for batch_ind, (x, y) in enumerate(bar):\n                x = x.to(device)\n                out = model(x)\n\n                ys.extend([_y.cpu() for _y in y])\n                outs.extend([_out.cpu() for _out in out])\n\n    coco_eval = compute_coco_eval(outs, ys, num_labels)\n\n    metrics = {\n        'map': 0.0,\n        'map50': 0.0,\n        'mean_f1': 0.0,\n        'mean_score_thresh': 0.5\n    }\n    if coco_eval is not None:\n        coco_metrics = coco_eval.stats\n        best_f1s, best_scores = compute_class_f1(coco_eval)\n        mean_f1 = np.mean(best_f1s[1:])\n        mean_score_thresh = np.mean(best_scores[1:])\n        metrics = {\n            'map': coco_metrics[0],\n            'map50': coco_metrics[1],\n            'mean_f1': mean_f1,\n            'mean_score_thresh': mean_score_thresh\n        }\n    return metrics\n"""
rastervision/backend/torch_utils/semantic_segmentation/__init__.py,0,b''
rastervision/backend/torch_utils/semantic_segmentation/data.py,1,"b'from os.path import join, basename\nimport glob\n\nfrom PIL import Image\nimport torchvision\nfrom torch.utils.data import DataLoader, Dataset\n\nfrom rastervision.backend.torch_utils.data import DataBunch\n\n\nclass ToTensor(object):\n    def __init__(self):\n        self.to_tensor = torchvision.transforms.ToTensor()\n\n    def __call__(self, x, y):\n        return (self.to_tensor(x), (255 * self.to_tensor(y)).squeeze().long())\n\n\nclass ComposeTransforms(object):\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, x, y):\n        for t in self.transforms:\n            x, y = t(x, y)\n        return x, y\n\n\nclass SegmentationDataset(Dataset):\n    def __init__(self, data_dir, transforms=None):\n        self.data_dir = data_dir\n        self.img_paths = glob.glob(join(data_dir, \'img\', \'*.png\'))\n        self.transforms = transforms\n\n    def __getitem__(self, ind):\n        img_path = self.img_paths[ind]\n        label_path = join(self.data_dir, \'labels\', basename(img_path))\n        x = Image.open(img_path)\n        y = Image.open(label_path)\n\n        if self.transforms is not None:\n            x, y = self.transforms(x, y)\n        return (x, y)\n\n    def __len__(self):\n        return len(self.img_paths)\n\n\ndef build_databunch(data_dir, img_sz, batch_sz, class_names):\n    # set to zero to prevent ""dataloader is killed by signal""\n    # TODO fix this\n    num_workers = 0\n\n    train_dir = join(data_dir, \'train\')\n    valid_dir = join(data_dir, \'valid\')\n\n    aug_transforms = ComposeTransforms([ToTensor()])\n    transforms = ComposeTransforms([ToTensor()])\n\n    train_ds = SegmentationDataset(train_dir, transforms=aug_transforms)\n    valid_ds = SegmentationDataset(valid_dir, transforms=transforms)\n\n    train_dl = DataLoader(\n        train_ds,\n        shuffle=True,\n        batch_size=batch_sz,\n        num_workers=num_workers,\n        drop_last=True,\n        pin_memory=True)\n    valid_dl = DataLoader(\n        valid_ds,\n        batch_size=batch_sz,\n        num_workers=num_workers,\n        pin_memory=True)\n\n    return DataBunch(train_ds, train_dl, valid_ds, valid_dl, class_names)\n'"
rastervision/backend/torch_utils/semantic_segmentation/model.py,0,"b""from torchvision import models\n\n\ndef get_model(model_arch, num_labels, pretrained=True):\n    model = models.segmentation.segmentation._segm_resnet(\n        'deeplabv3',\n        'resnet50',\n        num_labels,\n        False,\n        pretrained_backbone=pretrained)\n    return model\n"""
rastervision/backend/torch_utils/semantic_segmentation/plot.py,0,"b""from rastervision.data.label_source.utils import color_to_triple\nimport matplotlib\nmatplotlib.use('Agg')  # noqa\n\n\ndef plot_xy(ax, x, class_map, y=None):\n    ax.axis('off')\n    x = x.permute((1, 2, 0)).numpy()\n    ax.imshow(x)\n\n    if y is not None:\n        colors = [class_map.get_by_id(i).color for i in range(len(class_map))]\n        colors = [color_to_triple(c) for c in colors]\n        colors = [tuple([_c / 255 for _c in c]) for c in colors]\n        cmap = matplotlib.colors.ListedColormap(colors)\n\n        y = y.numpy()\n        ax.imshow(y, alpha=0.4, vmin=0, vmax=len(colors), cmap=cmap)\n"""
rastervision/backend/torch_utils/semantic_segmentation/train.py,2,"b""import click\nimport torch\n\nfrom rastervision.backend.torch_utils.metrics import (compute_conf_mat,\n                                                      compute_conf_mat_metrics)\n\n\ndef train_epoch(model, device, data_loader, opt, loss_fn, step_scheduler=None):\n    model.train()\n    total_loss = 0.0\n    num_samples = 0\n\n    with click.progressbar(data_loader, label='Training') as bar:\n        for batch_ind, (x, y) in enumerate(bar):\n            x = x.to(device)\n            y = y.to(device)\n\n            opt.zero_grad()\n            out = model(x)['out']\n            loss = loss_fn(out, y)\n            loss.backward()\n            total_loss += loss.item()\n            opt.step()\n            if step_scheduler:\n                step_scheduler.step()\n            num_samples += x.shape[0]\n\n    return total_loss / num_samples\n\n\ndef validate_epoch(model, device, data_loader, num_labels):\n    model.eval()\n\n    conf_mat = torch.zeros((num_labels, num_labels))\n    with torch.no_grad():\n        with click.progressbar(data_loader, label='Validating') as bar:\n            for batch_ind, (x, y) in enumerate(bar):\n                x = x.to(device)\n                out = model(x)['out']\n\n                out = out.argmax(1).view(-1).cpu()\n                y = y.view(-1).cpu()\n                conf_mat += compute_conf_mat(out, y, num_labels)\n\n    # Ignore index zero.\n    conf_mat = conf_mat[1:, 1:]\n    return compute_conf_mat_metrics(conf_mat)\n"""
rastervision/data/label/tfod_utils/__init__.py,0,b'# flake8: noqa\n'
rastervision/data/label/tfod_utils/np_box_list.py,0,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Numpy BoxList classes and functions.""""""\n\nimport numpy as np\n\n\nclass BoxList(object):\n  """"""Box collection.\n  BoxList represents a list of bounding boxes as numpy array, where each\n  bounding box is represented as a row of 4 numbers,\n  [y_min, x_min, y_max, x_max].  It is assumed that all bounding boxes within a\n  given list correspond to a single image.\n  Optionally, users can add additional related fields (such as\n  objectness/classification scores).\n  """"""\n\n  def __init__(self, data):\n    """"""Constructs box collection.\n    Args:\n      data: a numpy array of shape [N, 4] representing box coordinates\n    Raises:\n      ValueError: if bbox data is not a numpy array\n      ValueError: if invalid dimensions for bbox data\n    """"""\n    if not isinstance(data, np.ndarray):\n      raise ValueError(\'data must be a numpy array.\')\n    if len(data.shape) != 2 or data.shape[1] != 4:\n      raise ValueError(\'Invalid dimensions for box data.\')\n    if data.dtype != np.float32 and data.dtype != np.float64:\n      raise ValueError(\'Invalid data type for box data: float is required.\')\n    if not self._is_valid_boxes(data):\n      raise ValueError(\'Invalid box data. data must be a numpy array of \'\n                       \'N*[y_min, x_min, y_max, x_max]\')\n    self.data = {\'boxes\': data}\n\n  def num_boxes(self):\n    """"""Return number of boxes held in collections.""""""\n    return self.data[\'boxes\'].shape[0]\n\n  def get_extra_fields(self):\n    """"""Return all non-box fields.""""""\n    return [k for k in self.data.keys() if k != \'boxes\']\n\n  def has_field(self, field):\n    return field in self.data\n\n  def add_field(self, field, field_data):\n    """"""Add data to a specified field.\n    Args:\n      field: a string parameter used to speficy a related field to be accessed.\n      field_data: a numpy array of [N, ...] representing the data associated\n          with the field.\n    Raises:\n      ValueError: if the field is already exist or the dimension of the field\n          data does not matches the number of boxes.\n    """"""\n    if self.has_field(field):\n      raise ValueError(\'Field \' + field + \'already exists\')\n    if len(field_data.shape) < 1 or field_data.shape[0] != self.num_boxes():\n      raise ValueError(\'Invalid dimensions for field data\')\n    self.data[field] = field_data\n\n  def get(self):\n    """"""Convenience function for accesssing box coordinates.\n    Returns:\n      a numpy array of shape [N, 4] representing box corners\n    """"""\n    return self.get_field(\'boxes\')\n\n  def get_field(self, field):\n    """"""Accesses data associated with the specified field in the box collection.\n    Args:\n      field: a string parameter used to speficy a related field to be accessed.\n    Returns:\n      a numpy 1-d array representing data of an associated field\n    Raises:\n      ValueError: if invalid field\n    """"""\n    if not self.has_field(field):\n      raise ValueError(\'field {} does not exist\'.format(field))\n    return self.data[field]\n\n  def get_coordinates(self):\n    """"""Get corner coordinates of boxes.\n    Returns:\n     a list of 4 1-d numpy arrays [y_min, x_min, y_max, x_max]\n    """"""\n    box_coordinates = self.get()\n    y_min = box_coordinates[:, 0]\n    x_min = box_coordinates[:, 1]\n    y_max = box_coordinates[:, 2]\n    x_max = box_coordinates[:, 3]\n    return [y_min, x_min, y_max, x_max]\n\n  def _is_valid_boxes(self, data):\n    """"""Check whether data fullfills the format of N*[ymin, xmin, ymax, xmin].\n    Args:\n      data: a numpy array of shape [N, 4] representing box coordinates\n    Returns:\n      a boolean indicating whether all ymax of boxes are equal or greater than\n          ymin, and all xmax of boxes are equal or greater than xmin.\n    """"""\n    if data.shape[0] > 0:\n      for i in range(data.shape[0]):\n        if data[i, 0] > data[i, 2] or data[i, 1] > data[i, 3]:\n          return False\n    return True\n'"
rastervision/data/label/tfod_utils/np_box_list_ops.py,0,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Bounding Box List operations for Numpy BoxLists.\nExample box operations that are supported:\n  * Areas: compute bounding box areas\n  * IOU: pairwise intersection-over-union scores\n""""""\nimport numpy as np\n\nfrom rastervision.data.label.tfod_utils import np_box_list\nfrom rastervision.data.label.tfod_utils import np_box_ops\n\n\nclass SortOrder(object):\n  """"""Enum class for sort order.\n  Attributes:\n    ascend: ascend order.\n    descend: descend order.\n  """"""\n  ASCEND = 1\n  DESCEND = 2\n\n\ndef area(boxlist):\n  """"""Computes area of boxes.\n  Args:\n    boxlist: BoxList holding N boxes\n  Returns:\n    a numpy array with shape [N*1] representing box areas\n  """"""\n  y_min, x_min, y_max, x_max = boxlist.get_coordinates()\n  return (y_max - y_min) * (x_max - x_min)\n\n\ndef intersection(boxlist1, boxlist2):\n  """"""Compute pairwise intersection areas between boxes.\n  Args:\n    boxlist1: BoxList holding N boxes\n    boxlist2: BoxList holding M boxes\n  Returns:\n    a numpy array with shape [N*M] representing pairwise intersection area\n  """"""\n  return np_box_ops.intersection(boxlist1.get(), boxlist2.get())\n\n\ndef iou(boxlist1, boxlist2):\n  """"""Computes pairwise intersection-over-union between box collections.\n  Args:\n    boxlist1: BoxList holding N boxes\n    boxlist2: BoxList holding M boxes\n  Returns:\n    a numpy array with shape [N, M] representing pairwise iou scores.\n  """"""\n  return np_box_ops.iou(boxlist1.get(), boxlist2.get())\n\n\ndef ioa(boxlist1, boxlist2):\n  """"""Computes pairwise intersection-over-area between box collections.\n  Intersection-over-area (ioa) between two boxes box1 and box2 is defined as\n  their intersection area over box2\'s area. Note that ioa is not symmetric,\n  that is, IOA(box1, box2) != IOA(box2, box1).\n  Args:\n    boxlist1: BoxList holding N boxes\n    boxlist2: BoxList holding M boxes\n  Returns:\n    a numpy array with shape [N, M] representing pairwise ioa scores.\n  """"""\n  return np_box_ops.ioa(boxlist1.get(), boxlist2.get())\n\n\ndef gather(boxlist, indices, fields=None):\n  """"""Gather boxes from BoxList according to indices and return new BoxList.\n  By default, gather returns boxes corresponding to the input index list, as\n  well as all additional fields stored in the boxlist (indexing into the\n  first dimension).  However one can optionally only gather from a\n  subset of fields.\n  Args:\n    boxlist: BoxList holding N boxes\n    indices: a 1-d numpy array of type int_\n    fields: (optional) list of fields to also gather from.  If None (default),\n        all fields are gathered from.  Pass an empty fields list to only gather\n        the box coordinates.\n  Returns:\n    subboxlist: a BoxList corresponding to the subset of the input BoxList\n        specified by indices\n  Raises:\n    ValueError: if specified field is not contained in boxlist or if the\n        indices are not of type int_\n  """"""\n  if indices.size:\n    if np.amax(indices) >= boxlist.num_boxes() or np.amin(indices) < 0:\n      raise ValueError(\'indices are out of valid range.\')\n  subboxlist = np_box_list.BoxList(boxlist.get()[indices, :])\n  if fields is None:\n    fields = boxlist.get_extra_fields()\n  for field in fields:\n    extra_field_data = boxlist.get_field(field)\n    subboxlist.add_field(field, extra_field_data[indices, ...])\n  return subboxlist\n\n\ndef sort_by_field(boxlist, field, order=SortOrder.DESCEND):\n  """"""Sort boxes and associated fields according to a scalar field.\n  A common use case is reordering the boxes according to descending scores.\n  Args:\n    boxlist: BoxList holding N boxes.\n    field: A BoxList field for sorting and reordering the BoxList.\n    order: (Optional) \'descend\' or \'ascend\'. Default is descend.\n  Returns:\n    sorted_boxlist: A sorted BoxList with the field in the specified order.\n  Raises:\n    ValueError: if specified field does not exist or is not of single dimension.\n    ValueError: if the order is not either descend or ascend.\n  """"""\n  if not boxlist.has_field(field):\n    raise ValueError(\'Field \' + field + \' does not exist\')\n  if len(boxlist.get_field(field).shape) != 1:\n    raise ValueError(\'Field \' + field + \'should be single dimension.\')\n  if order != SortOrder.DESCEND and order != SortOrder.ASCEND:\n    raise ValueError(\'Invalid sort order\')\n\n  field_to_sort = boxlist.get_field(field)\n  sorted_indices = np.argsort(field_to_sort)\n  if order == SortOrder.DESCEND:\n    sorted_indices = sorted_indices[::-1]\n  return gather(boxlist, sorted_indices)\n\n\ndef non_max_suppression(boxlist,\n                        max_output_size=10000,\n                        iou_threshold=1.0,\n                        score_threshold=-10.0):\n  """"""Non maximum suppression.\n  This op greedily selects a subset of detection bounding boxes, pruning\n  away boxes that have high IOU (intersection over union) overlap (> thresh)\n  with already selected boxes. In each iteration, the detected bounding box with\n  highest score in the available pool is selected.\n  Args:\n    boxlist: BoxList holding N boxes.  Must contain a \'scores\' field\n      representing detection scores. All scores belong to the same class.\n    max_output_size: maximum number of retained boxes\n    iou_threshold: intersection over union threshold.\n    score_threshold: minimum score threshold. Remove the boxes with scores\n                     less than this value. Default value is set to -10. A very\n                     low threshold to pass pretty much all the boxes, unless\n                     the user sets a different score threshold.\n  Returns:\n    a BoxList holding M boxes where M <= max_output_size\n  Raises:\n    ValueError: if \'scores\' field does not exist\n    ValueError: if threshold is not in [0, 1]\n    ValueError: if max_output_size < 0\n  """"""\n  if not boxlist.has_field(\'scores\'):\n    raise ValueError(\'Field scores does not exist\')\n  if iou_threshold < 0. or iou_threshold > 1.0:\n    raise ValueError(\'IOU threshold must be in [0, 1]\')\n  if max_output_size < 0:\n    raise ValueError(\'max_output_size must be bigger than 0.\')\n\n  boxlist = filter_scores_greater_than(boxlist, score_threshold)\n  if boxlist.num_boxes() == 0:\n    return boxlist\n\n  boxlist = sort_by_field(boxlist, \'scores\')\n\n  # Prevent further computation if NMS is disabled.\n  if iou_threshold == 1.0:\n    if boxlist.num_boxes() > max_output_size:\n      selected_indices = np.arange(max_output_size)\n      return gather(boxlist, selected_indices)\n    else:\n      return boxlist\n\n  boxes = boxlist.get()\n  num_boxes = boxlist.num_boxes()\n  # is_index_valid is True only for all remaining valid boxes,\n  is_index_valid = np.full(num_boxes, 1, dtype=bool)\n  selected_indices = []\n  num_output = 0\n  for i in range(num_boxes):\n    if num_output < max_output_size:\n      if is_index_valid[i]:\n        num_output += 1\n        selected_indices.append(i)\n        is_index_valid[i] = False\n        valid_indices = np.where(is_index_valid)[0]\n        if valid_indices.size == 0:\n          break\n\n        intersect_over_union = np_box_ops.iou(\n            np.expand_dims(boxes[i, :], axis=0), boxes[valid_indices, :])\n        intersect_over_union = np.squeeze(intersect_over_union, axis=0)\n        is_index_valid[valid_indices] = np.logical_and(\n            is_index_valid[valid_indices],\n            intersect_over_union <= iou_threshold)\n  return gather(boxlist, np.array(selected_indices))\n\n\ndef multi_class_non_max_suppression(boxlist, score_thresh, iou_thresh,\n                                    max_output_size):\n  """"""Multi-class version of non maximum suppression.\n  This op greedily selects a subset of detection bounding boxes, pruning\n  away boxes that have high IOU (intersection over union) overlap (> thresh)\n  with already selected boxes.  It operates independently for each class for\n  which scores are provided (via the scores field of the input box_list),\n  pruning boxes with score less than a provided threshold prior to\n  applying NMS.\n  Args:\n    boxlist: BoxList holding N boxes.  Must contain a \'scores\' field\n      representing detection scores.  This scores field is a tensor that can\n      be 1 dimensional (in the case of a single class) or 2-dimensional, which\n      which case we assume that it takes the shape [num_boxes, num_classes].\n      We further assume that this rank is known statically and that\n      scores.shape[1] is also known (i.e., the number of classes is fixed\n      and known at graph construction time).\n    score_thresh: scalar threshold for score (low scoring boxes are removed).\n    iou_thresh: scalar threshold for IOU (boxes that that high IOU overlap\n      with previously selected boxes are removed).\n    max_output_size: maximum number of retained boxes per class.\n  Returns:\n    a BoxList holding M boxes with a rank-1 scores field representing\n      corresponding scores for each box with scores sorted in decreasing order\n      and a rank-1 classes field representing a class label for each box.\n  Raises:\n    ValueError: if iou_thresh is not in [0, 1] or if input boxlist does not have\n      a valid scores field.\n  """"""\n  if not 0 <= iou_thresh <= 1.0:\n    raise ValueError(\'thresh must be between 0 and 1\')\n  if not isinstance(boxlist, np_box_list.BoxList):\n    raise ValueError(\'boxlist must be a BoxList\')\n  if not boxlist.has_field(\'scores\'):\n    raise ValueError(\'input boxlist must have \\\'scores\\\' field\')\n  scores = boxlist.get_field(\'scores\')\n  if len(scores.shape) == 1:\n    scores = np.reshape(scores, [-1, 1])\n  elif len(scores.shape) == 2:\n    if scores.shape[1] is None:\n      raise ValueError(\'scores field must have statically defined second \'\n                       \'dimension\')\n  else:\n    raise ValueError(\'scores field must be of rank 1 or 2\')\n  num_boxes = boxlist.num_boxes()\n  num_scores = scores.shape[0]\n  num_classes = scores.shape[1]\n\n  if num_boxes != num_scores:\n    raise ValueError(\'Incorrect scores field length: actual vs expected.\')\n\n  selected_boxes_list = []\n  for class_idx in range(num_classes):\n    boxlist_and_class_scores = np_box_list.BoxList(boxlist.get())\n    class_scores = np.reshape(scores[0:num_scores, class_idx], [-1])\n    boxlist_and_class_scores.add_field(\'scores\', class_scores)\n    boxlist_filt = filter_scores_greater_than(boxlist_and_class_scores,\n                                              score_thresh)\n    nms_result = non_max_suppression(boxlist_filt,\n                                     max_output_size=max_output_size,\n                                     iou_threshold=iou_thresh,\n                                     score_threshold=score_thresh)\n    nms_result.add_field(\n        \'classes\', np.zeros_like(nms_result.get_field(\'scores\')) + class_idx)\n    selected_boxes_list.append(nms_result)\n  selected_boxes = concatenate(selected_boxes_list)\n  sorted_boxes = sort_by_field(selected_boxes, \'scores\')\n  return sorted_boxes\n\n\ndef scale(boxlist, y_scale, x_scale):\n  """"""Scale box coordinates in x and y dimensions.\n  Args:\n    boxlist: BoxList holding N boxes\n    y_scale: float\n    x_scale: float\n  Returns:\n    boxlist: BoxList holding N boxes\n  """"""\n  y_min, x_min, y_max, x_max = np.array_split(boxlist.get(), 4, axis=1)\n  y_min = y_scale * y_min\n  y_max = y_scale * y_max\n  x_min = x_scale * x_min\n  x_max = x_scale * x_max\n  scaled_boxlist = np_box_list.BoxList(np.hstack([y_min, x_min, y_max, x_max]))\n\n  fields = boxlist.get_extra_fields()\n  for field in fields:\n    extra_field_data = boxlist.get_field(field)\n    scaled_boxlist.add_field(field, extra_field_data)\n\n  return scaled_boxlist\n\n\ndef clip_to_window(boxlist, window):\n  """"""Clip bounding boxes to a window.\n  This op clips input bounding boxes (represented by bounding box\n  corners) to a window, optionally filtering out boxes that do not\n  overlap at all with the window.\n  Args:\n    boxlist: BoxList holding M_in boxes\n    window: a numpy array of shape [4] representing the\n            [y_min, x_min, y_max, x_max] window to which the op\n            should clip boxes.\n  Returns:\n    a BoxList holding M_out boxes where M_out <= M_in\n  """"""\n  y_min, x_min, y_max, x_max = np.array_split(boxlist.get(), 4, axis=1)\n  win_y_min = window[0]\n  win_x_min = window[1]\n  win_y_max = window[2]\n  win_x_max = window[3]\n  y_min_clipped = np.fmax(np.fmin(y_min, win_y_max), win_y_min)\n  y_max_clipped = np.fmax(np.fmin(y_max, win_y_max), win_y_min)\n  x_min_clipped = np.fmax(np.fmin(x_min, win_x_max), win_x_min)\n  x_max_clipped = np.fmax(np.fmin(x_max, win_x_max), win_x_min)\n  clipped = np_box_list.BoxList(\n      np.hstack([y_min_clipped, x_min_clipped, y_max_clipped, x_max_clipped]))\n  clipped = _copy_extra_fields(clipped, boxlist)\n  areas = area(clipped)\n  nonzero_area_indices = np.reshape(np.nonzero(np.greater(areas, 0.0)),\n                                    [-1]).astype(np.int32)\n  return gather(clipped, nonzero_area_indices)\n\n\ndef prune_non_overlapping_boxes(boxlist1, boxlist2, minoverlap=0.0):\n  """"""Prunes the boxes in boxlist1 that overlap less than thresh with boxlist2.\n  For each box in boxlist1, we want its IOA to be more than minoverlap with\n  at least one of the boxes in boxlist2. If it does not, we remove it.\n  Args:\n    boxlist1: BoxList holding N boxes.\n    boxlist2: BoxList holding M boxes.\n    minoverlap: Minimum required overlap between boxes, to count them as\n                overlapping.\n  Returns:\n    A pruned boxlist with size [N\', 4].\n  """"""\n  intersection_over_area = ioa(boxlist2, boxlist1)  # [M, N] tensor\n  intersection_over_area = np.amax(intersection_over_area, axis=0)  # [N] tensor\n  keep_bool = np.greater_equal(intersection_over_area, np.array(minoverlap))\n  keep_inds = np.nonzero(keep_bool)[0]\n  new_boxlist1 = gather(boxlist1, keep_inds)\n  return new_boxlist1\n\n\ndef prune_outside_window(boxlist, window):\n  """"""Prunes bounding boxes that fall outside a given window.\n  This function prunes bounding boxes that even partially fall outside the given\n  window. See also ClipToWindow which only prunes bounding boxes that fall\n  completely outside the window, and clips any bounding boxes that partially\n  overflow.\n  Args:\n    boxlist: a BoxList holding M_in boxes.\n    window: a numpy array of size 4, representing [ymin, xmin, ymax, xmax]\n            of the window.\n  Returns:\n    pruned_corners: a tensor with shape [M_out, 4] where M_out <= M_in.\n    valid_indices: a tensor with shape [M_out] indexing the valid bounding boxes\n     in the input tensor.\n  """"""\n\n  y_min, x_min, y_max, x_max = np.array_split(boxlist.get(), 4, axis=1)\n  win_y_min = window[0]\n  win_x_min = window[1]\n  win_y_max = window[2]\n  win_x_max = window[3]\n  coordinate_violations = np.hstack([np.less(y_min, win_y_min),\n                                     np.less(x_min, win_x_min),\n                                     np.greater(y_max, win_y_max),\n                                     np.greater(x_max, win_x_max)])\n  valid_indices = np.reshape(\n      np.where(np.logical_not(np.max(coordinate_violations, axis=1))), [-1])\n  return gather(boxlist, valid_indices), valid_indices\n\n\ndef concatenate(boxlists, fields=None):\n  """"""Concatenate list of BoxLists.\n  This op concatenates a list of input BoxLists into a larger BoxList.  It also\n  handles concatenation of BoxList fields as long as the field tensor shapes\n  are equal except for the first dimension.\n  Args:\n    boxlists: list of BoxList objects\n    fields: optional list of fields to also concatenate.  By default, all\n      fields from the first BoxList in the list are included in the\n      concatenation.\n  Returns:\n    a BoxList with number of boxes equal to\n      sum([boxlist.num_boxes() for boxlist in BoxList])\n  Raises:\n    ValueError: if boxlists is invalid (i.e., is not a list, is empty, or\n      contains non BoxList objects), or if requested fields are not contained in\n      all boxlists\n  """"""\n  if not isinstance(boxlists, list):\n    raise ValueError(\'boxlists should be a list\')\n  if not boxlists:\n    raise ValueError(\'boxlists should have nonzero length\')\n  for boxlist in boxlists:\n    if not isinstance(boxlist, np_box_list.BoxList):\n      raise ValueError(\'all elements of boxlists should be BoxList objects\')\n  concatenated = np_box_list.BoxList(\n      np.vstack([boxlist.get() for boxlist in boxlists]))\n  if fields is None:\n    fields = boxlists[0].get_extra_fields()\n  for field in fields:\n    first_field_shape = boxlists[0].get_field(field).shape\n    first_field_shape = first_field_shape[1:]\n    for boxlist in boxlists:\n      if not boxlist.has_field(field):\n        raise ValueError(\'boxlist must contain all requested fields\')\n      field_shape = boxlist.get_field(field).shape\n      field_shape = field_shape[1:]\n      if field_shape != first_field_shape:\n        raise ValueError(\'field %s must have same shape for all boxlists \'\n                         \'except for the 0th dimension.\' % field)\n    concatenated_field = np.concatenate(\n        [boxlist.get_field(field) for boxlist in boxlists], axis=0)\n    concatenated.add_field(field, concatenated_field)\n  return concatenated\n\n\ndef filter_scores_greater_than(boxlist, thresh):\n  """"""Filter to keep only boxes with score exceeding a given threshold.\n  This op keeps the collection of boxes whose corresponding scores are\n  greater than the input threshold.\n  Args:\n    boxlist: BoxList holding N boxes.  Must contain a \'scores\' field\n      representing detection scores.\n    thresh: scalar threshold\n  Returns:\n    a BoxList holding M boxes where M <= N\n  Raises:\n    ValueError: if boxlist not a BoxList object or if it does not\n      have a scores field\n  """"""\n  if not isinstance(boxlist, np_box_list.BoxList):\n    raise ValueError(\'boxlist must be a BoxList\')\n  if not boxlist.has_field(\'scores\'):\n    raise ValueError(\'input boxlist must have \\\'scores\\\' field\')\n  scores = boxlist.get_field(\'scores\')\n  if len(scores.shape) > 2:\n    raise ValueError(\'Scores should have rank 1 or 2\')\n  if len(scores.shape) == 2 and scores.shape[1] != 1:\n    raise ValueError(\'Scores should have rank 1 or have shape \'\n                     \'consistent with [None, 1]\')\n  high_score_indices = np.reshape(np.where(np.greater(scores, thresh)),\n                                  [-1]).astype(np.int32)\n  return gather(boxlist, high_score_indices)\n\n\ndef change_coordinate_frame(boxlist, window):\n  """"""Change coordinate frame of the boxlist to be relative to window\'s frame.\n  Given a window of the form [ymin, xmin, ymax, xmax],\n  changes bounding box coordinates from boxlist to be relative to this window\n  (e.g., the min corner maps to (0,0) and the max corner maps to (1,1)).\n  An example use case is data augmentation: where we are given groundtruth\n  boxes (boxlist) and would like to randomly crop the image to some\n  window (window). In this case we need to change the coordinate frame of\n  each groundtruth box to be relative to this new window.\n  Args:\n    boxlist: A BoxList object holding N boxes.\n    window: a size 4 1-D numpy array.\n  Returns:\n    Returns a BoxList object with N boxes.\n  """"""\n  win_height = window[2] - window[0]\n  win_width = window[3] - window[1]\n  boxlist_new = scale(\n      np_box_list.BoxList(boxlist.get() -\n                          [window[0], window[1], window[0], window[1]]),\n      1.0 / win_height, 1.0 / win_width)\n  _copy_extra_fields(boxlist_new, boxlist)\n\n  return boxlist_new\n\n\ndef _copy_extra_fields(boxlist_to_copy_to, boxlist_to_copy_from):\n  """"""Copies the extra fields of boxlist_to_copy_from to boxlist_to_copy_to.\n  Args:\n    boxlist_to_copy_to: BoxList to which extra fields are copied.\n    boxlist_to_copy_from: BoxList from which fields are copied.\n  Returns:\n    boxlist_to_copy_to with extra fields.\n  """"""\n  for field in boxlist_to_copy_from.get_extra_fields():\n    boxlist_to_copy_to.add_field(field, boxlist_to_copy_from.get_field(field))\n  return boxlist_to_copy_to\n\n\ndef _update_valid_indices_by_removing_high_iou_boxes(\n    selected_indices, is_index_valid, intersect_over_union, threshold):\n  max_iou = np.max(intersect_over_union[:, selected_indices], axis=1)\n  return np.logical_and(is_index_valid, max_iou <= threshold)'"
rastervision/data/label/tfod_utils/np_box_ops.py,0,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Operations for [N, 4] numpy arrays representing bounding boxes.\nExample box operations that are supported:\n  * Areas: compute bounding box areas\n  * IOU: pairwise intersection-over-union scores\n""""""\nimport numpy as np\n\n\ndef area(boxes):\n  """"""Computes area of boxes.\n  Args:\n    boxes: Numpy array with shape [N, 4] holding N boxes\n  Returns:\n    a numpy array with shape [N*1] representing box areas\n  """"""\n  return (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])\n\n\ndef intersection(boxes1, boxes2):\n  """"""Compute pairwise intersection areas between boxes.\n  Args:\n    boxes1: a numpy array with shape [N, 4] holding N boxes\n    boxes2: a numpy array with shape [M, 4] holding M boxes\n  Returns:\n    a numpy array with shape [N*M] representing pairwise intersection area\n  """"""\n  [y_min1, x_min1, y_max1, x_max1] = np.split(boxes1, 4, axis=1)\n  [y_min2, x_min2, y_max2, x_max2] = np.split(boxes2, 4, axis=1)\n\n  all_pairs_min_ymax = np.minimum(y_max1, np.transpose(y_max2))\n  all_pairs_max_ymin = np.maximum(y_min1, np.transpose(y_min2))\n  intersect_heights = np.maximum(\n      np.zeros(all_pairs_max_ymin.shape),\n      all_pairs_min_ymax - all_pairs_max_ymin)\n  all_pairs_min_xmax = np.minimum(x_max1, np.transpose(x_max2))\n  all_pairs_max_xmin = np.maximum(x_min1, np.transpose(x_min2))\n  intersect_widths = np.maximum(\n      np.zeros(all_pairs_max_xmin.shape),\n      all_pairs_min_xmax - all_pairs_max_xmin)\n  return intersect_heights * intersect_widths\n\n\ndef iou(boxes1, boxes2):\n  """"""Computes pairwise intersection-over-union between box collections.\n  Args:\n    boxes1: a numpy array with shape [N, 4] holding N boxes.\n    boxes2: a numpy array with shape [M, 4] holding N boxes.\n  Returns:\n    a numpy array with shape [N, M] representing pairwise iou scores.\n  """"""\n  intersect = intersection(boxes1, boxes2)\n  area1 = area(boxes1)\n  area2 = area(boxes2)\n  union = np.expand_dims(area1, axis=1) + np.expand_dims(\n      area2, axis=0) - intersect\n  return intersect / union\n\n\ndef ioa(boxes1, boxes2):\n  """"""Computes pairwise intersection-over-area between box collections.\n  Intersection-over-area (ioa) between two boxes box1 and box2 is defined as\n  their intersection area over box2\'s area. Note that ioa is not symmetric,\n  that is, IOA(box1, box2) != IOA(box2, box1).\n  Args:\n    boxes1: a numpy array with shape [N, 4] holding N boxes.\n    boxes2: a numpy array with shape [M, 4] holding N boxes.\n  Returns:\n    a numpy array with shape [N, M] representing pairwise ioa scores.\n  """"""\n  intersect = intersection(boxes1, boxes2)\n  areas = np.expand_dims(area(boxes2), axis=0)\n  return intersect / areas'"
rastervision/data/vector_source/label_maker/__init__.py,0,b''
rastervision/data/vector_source/label_maker/filter.py,0,"b'# Copied from https://github.com/developmentseed/label-maker/blob/master/label_maker/filter.py\n# flake8: noqa\n\n# pylint: disable=eval-used,too-many-return-statements\n""""""Create a feature filtering function from a Mapbox GL Filter.""""""\n\n# Python port of https://github.com/mapbox/mapbox-gl-js/blob/c9900db279db776f493ce8b6749966cedc2d6b8a/src/style-spec/feature_filter/index.js\n\n\ndef create_filter(filt):\n    """"""Create a feature filtering function from a Mapbox GL Filter.\n\n    Given a filter expressed as nested lists, return a new function\n    that evaluates whether a given feature (with a .properties or .tags property)\n    passes its test. More information:\n    - https://www.mapbox.com/mapbox-gl-js/style-spec/#other-filter\n    - https://github.com/mapbox/mapbox-gl-js/tree/master/src/style-spec/feature_filter\n\n    Parameters\n    ------------\n    filt: list\n        Mapbox GL filter\n\n    Returns\n    --------\n    func: function\n        A function which evaluates whether a GeoJSON feature meets the input filter criteria\n    """"""\n\n    def func(f):\n        """"""evaluates whether a given feature passes its filter""""""\n        p = f.get(\'properties\', {}) if f else {}  # pylint: disable=unused-variable\n        return eval(_compile(filt))\n\n    return func\n\n\ndef _compile(filt):\n    """"""Return a string represented the compiled filter function""""""\n    if not filt:\n        return \'True\'\n    op = filt[0]\n    if len(filt) == 1:\n        return \'False\' if op == \'any\' else \'True\'\n    if op in [\'==\', \'!=\', \'<\', \'>\', \'<=\', \'>=\']:\n        return _compile_comparison_op(filt[1], filt[2], op)\n    elif op == \'any\':\n        return _compile_logical_op(filt[1:], \' or \')\n    elif op == \'all\':\n        return _compile_logical_op(filt[1:], \' and \')\n    elif op == \'none\':\n        return _compile_negation(_compile_logical_op(filt[1:], \' or \'))\n    elif op == \'in\':\n        return _compile_in_op(filt[1], filt[2:])\n    elif op == \'!in\':\n        return _compile_negation(_compile_in_op(filt[1], filt[2:]))\n    elif op == \'has\':\n        return _compile_has_op(filt[1])\n    elif op == \'!has\':\n        return _compile_negation(_compile_has_op(filt[1]))\n    return \'True\'\n\n\ndef _compile_property_reference(prop):\n    """"""Find the correct reference on the input feature""""""\n    if prop == \'$type\':\n        return \'f.get(""geometry"").get(""type"")\'\n    elif prop == \'$id\':\n        return \'f.get(""id"")\'\n    return \'p.get(""{}"")\'.format(prop)\n\n\ndef _compile_comparison_op(prop, value, op):\n    """"""Combine two values with a comparison operator""""""\n    left = _compile_property_reference(prop)\n    right = _stringify(value)\n    return left + op + right\n\n\ndef _compile_logical_op(expressions, op):\n    """"""Join multiple logical expressions""""""\n    return op.join(map(_compile, expressions))\n\n\ndef _compile_in_op(prop, values):\n    """"""Test if a property is within a list of values""""""\n    return \'{} in {}\'.format(_compile_property_reference(prop), values)\n\n\ndef _compile_has_op(prop):\n    """"""Test if a property exists on a feature""""""\n    return \'""id"" in f\' if prop == \'$id\' else \'{} in p\'.format(_stringify(prop))\n\n\ndef _compile_negation(expression):\n    """"""Negate the input expression""""""\n    return \'not ({})\'.format(expression)\n\n\ndef _stringify(s):\n    """"""Convert input to string, wrap with quotes if already a string""""""\n    return \'""{}""\'.format(s) if isinstance(s, str) else str(s)\n'"
rastervision2/core/data/crs_transformer/__init__.py,0,b'# flake8: noqa\n\nfrom rastervision2.core.data.crs_transformer.crs_transformer import *\nfrom rastervision2.core.data.crs_transformer.identity_crs_transformer import *\nfrom rastervision2.core.data.crs_transformer.rasterio_crs_transformer import *\n'
rastervision2/core/data/crs_transformer/crs_transformer.py,0,"b'from typing import Optional\n\n\nclass CRSTransformer():\n    """"""Transforms map points in some CRS into pixel coordinates.\n\n    Each transformer is associated with a particular RasterSource.\n    """"""\n\n    def __init__(self,\n                 image_crs: Optional[str] = None,\n                 map_crs: Optional[str] = None,\n                 transform=None):\n        self.image_crs = image_crs\n        self.map_crs = map_crs\n        self.transform = transform\n\n    def map_to_pixel(self, map_point):\n        """"""Transform point from map to pixel-based coordinates.\n\n        Args:\n            map_point: (x, y) tuple in map coordinates (eg. lon/lat). x and y can be\n            single values or array-like.\n\n        Returns:\n            (x, y) tuple in pixel coordinates\n        """"""\n        pass\n\n    def pixel_to_map(self, pixel_point):\n        """"""Transform point from pixel to map-based coordinates.\n\n        Args:\n            pixel_point: (x, y) tuple in pixel coordinates. x and y can be\n            single values or array-like.\n\n        Returns:\n            (x, y) tuple in map coordinates (eg. lon/lat)\n        """"""\n        pass\n\n    def get_image_crs(self):\n        return self.image_crs\n\n    def get_map_crs(self):\n        return self.map_crs\n\n    def get_affine_transform(self):\n        return self.transform\n'"
rastervision2/core/data/crs_transformer/identity_crs_transformer.py,0,"b'from rastervision2.core.data.crs_transformer import CRSTransformer\n\n\nclass IdentityCRSTransformer(CRSTransformer):\n    """"""Transformer for when map coordinates are already in pixel coordinates.\n\n    This is useful for non-georeferenced imagery.\n    """"""\n\n    def map_to_pixel(self, map_point):\n        """"""Identity function.\n\n        Args:\n            map_point: (x, y) tuple in pixel coordinates\n\n        Returns:\n            (x, y) tuple in pixel coordinates\n        """"""\n        return map_point\n\n    def pixel_to_map(self, pixel_point):\n        """"""Identity function.\n\n        Args:\n            pixel_point: (x, y) tuple in pixel coordinates\n\n        Returns:\n            (x, y) tuple in pixel coordinates\n        """"""\n        return pixel_point\n'"
rastervision2/core/data/crs_transformer/rasterio_crs_transformer.py,0,"b'import pyproj\n\nfrom rasterio.transform import (rowcol, xy)\n\nfrom rastervision2.core.data.crs_transformer import (CRSTransformer,\n                                                     IdentityCRSTransformer)\n\n\nclass RasterioCRSTransformer(CRSTransformer):\n    """"""Transformer for a RasterioRasterSource.""""""\n\n    def __init__(self, transform, image_crs, map_crs=\'epsg:4326\'):\n        """"""Construct transformer.\n\n        Args:\n            image_dataset: Rasterio DatasetReader\n            map_crs: CRS code\n        """"""\n        self.map_proj = pyproj.Proj(init=map_crs)\n        self.image_proj = pyproj.Proj(image_crs)\n\n        super().__init__(image_crs, map_crs, transform)\n\n    def map_to_pixel(self, map_point):\n        """"""Transform point from map to pixel-based coordinates.\n\n        Args:\n            map_point: (x, y) tuple in map coordinates\n\n        Returns:\n            (x, y) tuple in pixel coordinates\n        """"""\n        image_point = pyproj.transform(self.map_proj, self.image_proj,\n                                       map_point[0], map_point[1])\n        pixel_point = rowcol(self.transform, image_point[0], image_point[1])\n        pixel_point = (pixel_point[1], pixel_point[0])\n        return pixel_point\n\n    def pixel_to_map(self, pixel_point):\n        """"""Transform point from pixel to map-based coordinates.\n\n        Args:\n            pixel_point: (x, y) tuple in pixel coordinates\n\n        Returns:\n            (x, y) tuple in map coordinates\n        """"""\n        image_point = xy(self.transform, int(pixel_point[1]),\n                         int(pixel_point[0]))\n        map_point = pyproj.transform(self.image_proj, self.map_proj,\n                                     image_point[0], image_point[1])\n        return map_point\n\n    @classmethod\n    def from_dataset(cls, dataset, map_crs=\'epsg:4326\'):\n        if dataset.crs is None:\n            return IdentityCRSTransformer()\n        transform = dataset.transform\n        image_crs = dataset.crs\n        return cls(transform, image_crs, map_crs)\n\n    def get_affine_transform(self):\n        return self.transform\n'"
rastervision2/core/data/label/__init__.py,0,b'# flake8: noqa\n\nfrom rastervision2.core.data.label.labels import *\nfrom rastervision2.core.data.label.chip_classification_labels import *\nfrom rastervision2.core.data.label.semantic_segmentation_labels import *\nfrom rastervision2.core.data.label.object_detection_labels import *\n'
rastervision2/core/data/label/chip_classification_labels.py,0,"b'from rastervision2.core.box import Box\nfrom rastervision2.core.data.label import Labels\n\n\nclass ChipClassificationLabels(Labels):\n    """"""Represents a spatial grid of cells associated with classes.""""""\n\n    def __init__(self):\n        self.cell_to_class_id = {}\n\n    def __len__(self):\n        return len(self.cell_to_class_id)\n\n    def __eq__(self, other):\n        return (isinstance(other, ChipClassificationLabels)\n                and self.cell_to_class_id == other.cell_to_class_id)\n\n    def __add__(self, other):\n        result = ChipClassificationLabels()\n        result.extend(self)\n        result.extend(other)\n        return result\n\n    def filter_by_aoi(self, aoi_polygons):\n        result = ChipClassificationLabels()\n        for cell in self.cell_to_class_id:\n            cell_box = Box.from_tuple(cell)\n            cell_poly = cell_box.to_shapely()\n            for aoi in aoi_polygons:\n                if cell_poly.within(aoi):\n                    (class_id, scores) = self.cell_to_class_id[cell]\n                    result.set_cell(cell_box, class_id, scores)\n        return result\n\n    def set_cell(self, cell, class_id, scores=None):\n        """"""Set cell and its class_id.\n\n        Args:\n            cell: (Box)\n            class_id: int\n            scores: 1d numpy array of probabilities for each class\n        """"""\n        if scores is not None:\n            scores = list(map(lambda x: float(x), list(scores)))\n        self.cell_to_class_id[cell.tuple_format()] = (class_id, scores)\n\n    def get_cell_class_id(self, cell):\n        """"""Return class_id for a cell.\n\n        Args:\n            cell: (Box)\n        """"""\n        result = self.cell_to_class_id.get(cell.tuple_format())\n        if result:\n            return result[0]\n        else:\n            return None\n\n    def get_cell_scores(self, cell):\n        """"""Return scores for a cell.\n\n        Args:\n            cell: (Box)\n        """"""\n        result = self.cell_to_class_id.get(cell.tuple_format())\n        if result:\n            return result[1]\n        else:\n            return None\n\n    def get_cell_values(self, cell):\n        """"""Return a tuple of (class_id, scores) for a cell.\n\n        Args:\n            cell: (Box)\n        """"""\n        return self.cell_to_class_id.get(cell.tuple_format())\n\n    def get_singleton_labels(self, cell):\n        """"""Return Labels object representing a single cell.\n\n        Args:\n            cell: (Box)\n        """"""\n        class_id, scores = self.get_cell_values(cell)\n        labels = ChipClassificationLabels()\n        labels.set_cell(cell, class_id, scores)\n        return labels\n\n    def get_cells(self):\n        """"""Return list of all cells (list    of Box).""""""\n        return [\n            Box.from_npbox(box_tup)\n            for box_tup in self.cell_to_class_id.keys()\n        ]\n\n    def get_class_ids(self):\n        """"""Return list of class_ids for all cells.""""""\n        return list(map(lambda x: x[0], self.cell_to_class_id.values()))\n\n    def get_scores(self):\n        """"""Return list of scores for all cells.""""""\n        return list(map(lambda x: x[1], self.cell_to_class_id.values()))\n\n    def get_values(self):\n        """"""Return list of class_ids and scores for all cells.""""""\n        return list(self.cell_to_class_id.values())\n\n    def extend(self, labels):\n        """"""Adds cells contained in labels.\n\n        Args:\n            labels: ChipClassificationLabels\n        """"""\n        for cell in labels.get_cells():\n            class_id, scores = labels.get_cell_values(cell)\n            self.set_cell(cell, class_id, scores)\n'"
rastervision2/core/data/label/labels.py,0,"b'from abc import (ABC, abstractmethod)\n\n\nclass Labels(ABC):\n    """"""A set of spatially referenced labels.\n\n    A set of labels predicted by a model or provided by human labelers for the\n    sake of training.\n    """"""\n\n    @abstractmethod\n    def __add__(self, other):\n        """"""Add labels to these labels.\n\n        Returns a concatenation of this and the other labels.\n        """"""\n        pass\n\n    @abstractmethod\n    def filter_by_aoi(self, aoi_polygons):\n        """"""Returns a copy of these labels filtered by a given set of AOI polygons\n\n        Args:\n          aoi_polygons - A list of AOI polygons to filter by, in pixel coordinates.\n        """"""\n        pass\n\n    @abstractmethod\n    def __eq__(self, other):\n        pass\n'"
rastervision2/core/data/label/object_detection_labels.py,0,"b'import numpy as np\nfrom shapely.geometry import shape\n\nfrom rastervision2.core.box import Box\nfrom rastervision2.core.data.label.labels import Labels\nfrom rastervision2.core.data.label.tfod_utils.np_box_list import BoxList\nfrom rastervision2.core.data.label.tfod_utils.np_box_list_ops import (\n    prune_non_overlapping_boxes, clip_to_window, concatenate,\n    non_max_suppression)\n\n\nclass ObjectDetectionLabels(Labels):\n    """"""A set of boxes and associated class_ids and scores.\n\n    Implemented using the Tensorflow Object Detection API\'s BoxList class.\n    """"""\n\n    def __init__(self, npboxes, class_ids, scores=None):\n        """"""Construct a set of object detection labels.\n\n        Args:\n            npboxes: float numpy array of size nx4 with cols\n                ymin, xmin, ymax, xmax. Should be in pixel coordinates within\n                the global frame of reference.\n            class_ids: int numpy array of size n with class ids\n            scores: float numpy array of size n\n        """"""\n        self.boxlist = BoxList(npboxes)\n        # This field name actually needs to be \'classes\' to be able to use\n        # certain utility functions in the TF Object Detection API.\n        self.boxlist.add_field(\'classes\', class_ids)\n        # We need to ensure that there is always a scores field so that the\n        # concatenate method will work with empty labels objects.\n        if scores is None:\n            scores = np.ones(class_ids.shape)\n        self.boxlist.add_field(\'scores\', scores)\n\n    def __add__(self, other):\n        return ObjectDetectionLabels.concatenate(self, other)\n\n    def __eq__(self, other):\n        return (isinstance(other, ObjectDetectionLabels)\n                and self.to_dict() == other.to_dict())\n\n    def assert_equal(self, expected_labels):\n        np.testing.assert_array_equal(self.get_npboxes(),\n                                      expected_labels.get_npboxes())\n        np.testing.assert_array_equal(self.get_class_ids(),\n                                      expected_labels.get_class_ids())\n        np.testing.assert_array_equal(self.get_scores(),\n                                      expected_labels.get_scores())\n\n    def filter_by_aoi(self, aoi_polygons):\n        boxes = self.get_boxes()\n        class_ids = self.get_class_ids()\n        scores = self.get_scores()\n\n        new_boxes = []\n        new_class_ids = []\n        new_scores = []\n        for box, class_id, score in zip(boxes, class_ids, scores):\n            box_poly = box.to_shapely()\n            for aoi in aoi_polygons:\n                if box_poly.within(aoi):\n                    new_boxes.append(box.npbox_format())\n                    new_class_ids.append(class_id)\n                    new_scores.append(score)\n                    break\n\n        if len(new_boxes) == 0:\n            return ObjectDetectionLabels.make_empty()\n\n        return ObjectDetectionLabels(\n            np.array(new_boxes), np.array(new_class_ids), np.array(new_scores))\n\n    @staticmethod\n    def make_empty():\n        npboxes = np.empty((0, 4))\n        class_ids = np.empty((0, ))\n        scores = np.empty((0, ))\n        return ObjectDetectionLabels(npboxes, class_ids, scores)\n\n    @staticmethod\n    def from_boxlist(boxlist):\n        """"""Make ObjectDetectionLabels from BoxList object.""""""\n        scores = (boxlist.get_field(\'scores\')\n                  if boxlist.has_field(\'scores\') else None)\n        return ObjectDetectionLabels(\n            boxlist.get(), boxlist.get_field(\'classes\'), scores=scores)\n\n    @staticmethod\n    def from_geojson(geojson, extent=None):\n        """"""Convert GeoJSON to ObjectDetectionLabels object.\n\n        If extent is provided, filter out the boxes that lie ""more than a little\n        bit"" outside the extent.\n\n        Args:\n            geojson: (dict) normalized GeoJSON (see VectorSource)\n            extent: (Box) in pixel coords\n\n        Returns:\n            ObjectDetectionLabels\n        """"""\n        boxes = []\n        class_ids = []\n        scores = []\n\n        for f in geojson[\'features\']:\n            geom = shape(f[\'geometry\'])\n            (xmin, ymin, xmax, ymax) = geom.bounds\n            boxes.append(Box(ymin, xmin, ymax, xmax))\n\n            props = f[\'properties\']\n            class_ids.append(props[\'class_id\'])\n            scores.append(props.get(\'score\', 1.0))\n\n        if len(boxes):\n            boxes = np.array(\n                [box.npbox_format() for box in boxes], dtype=float)\n            class_ids = np.array(class_ids)\n            scores = np.array(scores)\n            labels = ObjectDetectionLabels(boxes, class_ids, scores=scores)\n        else:\n            labels = ObjectDetectionLabels.make_empty()\n\n        if extent is not None:\n            labels = ObjectDetectionLabels.get_overlapping(\n                labels, extent, ioa_thresh=0.8, clip=True)\n        return labels\n\n    def get_boxes(self):\n        """"""Return list of Boxes.""""""\n        return [Box.from_npbox(npbox) for npbox in self.boxlist.get()]\n\n    def get_npboxes(self):\n        return self.boxlist.get()\n\n    def get_scores(self):\n        if self.boxlist.has_field(\'scores\'):\n            return self.boxlist.get_field(\'scores\')\n        return None\n\n    def get_class_ids(self):\n        return self.boxlist.get_field(\'classes\')\n\n    def __len__(self):\n        return self.boxlist.get().shape[0]\n\n    def __str__(self):\n        return str(self.boxlist.get())\n\n    def to_boxlist(self):\n        return self.boxlist\n\n    def to_dict(self):\n        """"""Returns a dict version of these labels.\n\n        The Dict has a Box as a key, and a tuple of (class_id, score)\n        as the values.\n        """"""\n        d = {}\n        boxes = list(map(Box.from_npbox, self.get_npboxes()))\n        classes = list(self.get_class_ids())\n        scores = list(self.get_scores())\n        for box, class_id, score in zip(boxes, classes, scores):\n            d[box.tuple_format()] = (class_id, score)\n        return d\n\n    @staticmethod\n    def local_to_global(npboxes, window):\n        """"""Convert from local to global coordinates.\n\n        The local coordinates are row/col within the window frame of reference.\n        The global coordinates are row/col within the extent of a RasterSource.\n        """"""\n        xmin = window.xmin\n        ymin = window.ymin\n        return npboxes + np.array([[ymin, xmin, ymin, xmin]])\n\n    @staticmethod\n    def global_to_local(npboxes, window):\n        """"""Convert from global to local coordinates.\n\n        The global coordinates are row/col within the extent of a RasterSource.\n        The local coordinates are row/col within the window frame of reference.\n        """"""\n        xmin = window.xmin\n        ymin = window.ymin\n        return npboxes - np.array([[ymin, xmin, ymin, xmin]])\n\n    @staticmethod\n    def local_to_normalized(npboxes, window):\n        """"""Convert from local to normalized coordinates.\n\n        The local coordinates are row/col within the window frame of reference.\n        Normalized coordinates range from 0 to 1 on each (height/width) axis.\n        """"""\n        height = window.get_height()\n        width = window.get_width()\n        return npboxes / np.array([[height, width, height, width]])\n\n    @staticmethod\n    def normalized_to_local(npboxes, window):\n        """"""Convert from normalized to local coordinates.\n\n        Normalized coordinates range from 0 to 1 on each (height/width) axis.\n        The local coordinates are row/col within the window frame of reference.\n        """"""\n        height = window.get_height()\n        width = window.get_width()\n        return npboxes * np.array([[height, width, height, width]])\n\n    @staticmethod\n    def get_overlapping(labels, window, ioa_thresh=0.000001, clip=False):\n        """"""Return subset of labels that overlap with window.\n\n        Args:\n            labels: ObjectDetectionLabels\n            window: Box\n            ioa_thresh: the minimum IOA for a box to be considered as\n                overlapping\n            clip: if True, clip label boxes to the window\n        """"""\n        window_npbox = window.npbox_format()\n        window_boxlist = BoxList(np.expand_dims(window_npbox, axis=0))\n        boxlist = prune_non_overlapping_boxes(\n            labels.boxlist, window_boxlist, minoverlap=ioa_thresh)\n        if clip:\n            boxlist = clip_to_window(boxlist, window_npbox)\n\n        return ObjectDetectionLabels.from_boxlist(boxlist)\n\n    @staticmethod\n    def concatenate(labels1, labels2):\n        """"""Return concatenation of labels.\n\n        Args:\n            labels1: ObjectDetectionLabels\n            labels2: ObjectDetectionLabels\n        """"""\n        new_boxlist = concatenate([labels1.to_boxlist(), labels2.to_boxlist()])\n        return ObjectDetectionLabels.from_boxlist(new_boxlist)\n\n    @staticmethod\n    def prune_duplicates(labels, score_thresh, merge_thresh):\n        """"""Remove duplicate boxes.\n\n        Runs non-maximum suppression to remove duplicate boxes that result from\n        sliding window prediction algorithm.\n\n        Args:\n            labels: ObjectDetectionLabels\n            score_thresh: the minimum allowed score of boxes\n            merge_thresh: the minimum IOA allowed when merging two boxes\n                together\n\n        Returns:\n            ObjectDetectionLabels\n        """"""\n        max_output_size = 1000000\n        pruned_boxlist = non_max_suppression(\n            labels.boxlist,\n            max_output_size=max_output_size,\n            iou_threshold=merge_thresh,\n            score_threshold=score_thresh)\n        return ObjectDetectionLabels.from_boxlist(pruned_boxlist)\n'"
rastervision2/core/data/label/semantic_segmentation_labels.py,0,"b'from rastervision2.core.data.label import Labels\n\nimport numpy as np\nfrom rasterio.features import rasterize\nfrom shapely.ops import transform\n\nfrom rastervision2.core.box import Box\n\n\nclass SemanticSegmentationLabels(Labels):\n    """"""A set of spatially referenced semantic segmentation labels.\n    """"""\n\n    def __init__(self):\n        self.window_to_label_arr = {}\n\n    def __add__(self, other):\n        """"""Add labels to these labels.\n\n        Returns a concatenation of this and the other labels.\n        """"""\n        self.window_to_label_arr.update(other.window_to_label_arr)\n        return self\n\n    def __eq__(self, other):\n        self_windows = set([w.tuple_format() for w in self.get_windows()])\n        other_windows = set([w.tuple_format() for w in other.get_windows()])\n        if self_windows != other_windows:\n            return False\n\n        for w in self.get_windows():\n            if not np.array_equal(\n                    self.get_label_arr(w), other.get_label_arr(w)):\n                return False\n\n        return True\n\n    def get_windows(self):\n        return [Box.from_tuple(w) for w in self.window_to_label_arr.keys()]\n\n    def set_label_arr(self, window, label_arr):\n        self.window_to_label_arr[window.tuple_format()] = label_arr\n\n    def get_label_arr(self, window):\n        return self.window_to_label_arr[window.tuple_format()]\n\n    def filter_by_aoi(self, aoi_polygons, null_class_id):\n        new_labels = SemanticSegmentationLabels()\n\n        for window in self.get_windows():\n            window_geom = window.to_shapely()\n            label_arr = self.get_label_arr(window)\n\n            if not aoi_polygons:\n                return self\n            else:\n                # For each aoi_polygon, intersect with window, and put in window frame of\n                # reference.\n                window_aois = []\n                for aoi in aoi_polygons:\n                    window_aoi = aoi.intersection(window_geom)\n                    if not window_aoi.is_empty:\n\n                        def transform_shape(x, y, z=None):\n                            return (x - window.xmin, y - window.ymin)\n\n                        window_aoi = transform(transform_shape, window_aoi)\n                        window_aois.append(window_aoi)\n\n                # If window does\'t overlap with any AOI, then it won\'t be in\n                # new_labels.\n                if window_aois:\n                    # If window intersects with AOI, set pixels outside the\n                    # AOI polygon to 0 so they are ignored during eval.\n                    mask = rasterize(\n                        [(p, 0) for p in window_aois],\n                        out_shape=label_arr.shape,\n                        fill=1,\n                        dtype=np.uint8)\n                    label_arr[mask.astype(np.bool)] = null_class_id\n                    new_labels.set_label_arr(window, label_arr)\n\n        return new_labels\n'"
rastervision2/core/data/label_source/__init__.py,0,b'# flake8: noqa\n\nfrom rastervision2.core.data.label_source.label_source import *\nfrom rastervision2.core.data.label_source.label_source_config import *\nfrom rastervision2.core.data.label_source.chip_classification_label_source import *\nfrom rastervision2.core.data.label_source.chip_classification_label_source_config import *\nfrom rastervision2.core.data.label_source.semantic_segmentation_label_source import *\nfrom rastervision2.core.data.label_source.semantic_segmentation_label_source_config import *\nfrom rastervision2.core.data.label_source.segmentation_class_transformer import *\nfrom rastervision2.core.data.label_source.object_detection_label_source import *\nfrom rastervision2.core.data.label_source.object_detection_label_source_config import *\n'
rastervision2/core/data/label_source/chip_classification_label_source.py,0,"b'import numpy as np\nfrom shapely.strtree import STRtree\nfrom shapely.geometry import shape\n\nfrom rastervision2.core.data import ChipClassificationLabels\nfrom rastervision2.core.data.label_source import LabelSource\nfrom rastervision2.core.box import Box\n\n\ndef infer_cell(cell, str_tree, ioa_thresh, use_intersection_over_cell,\n               background_class_id, pick_min_class_id):\n    """"""Infer the class_id of a cell given a set of polygons.\n\n    Given a cell and a set of polygons, the problem is to infer the class_id\n    that best captures the content of the cell. This is non-trivial since there\n    can be multiple polygons of differing classes overlapping with the cell.\n    Any polygons that sufficiently overlaps with the cell are in the running for\n    setting the class_id. If there are none in the running, the cell is either\n    considered null or background. See args for more details.\n\n    Args:\n        cell: Box\n        str_tree: (STRtree) collection of geoms in scene used for geometric queries.\n            The geoms need to have class_id monkey-patched onto them.\n        ioa_thresh: (float) the minimum IOA of a polygon and cell for that\n            polygon to be a candidate for setting the class_id\n        use_intersection_over_cell: (bool) If true, then use the area of the\n            cell as the denominator in the IOA. Otherwise, use the area of the\n            polygon.\n        background_class_id: (None or int) If not None, class_id to use as the\n            background class; ie. the one that is used when a window contains\n            no boxes. If not set, empty windows have None set as their class_id\n            which is considered a null value.\n        pick_min_class_id: If true, the class_id for a cell is the minimum\n            class_id of the boxes in that cell. Otherwise, pick the class_id of\n            the box covering the greatest area.\n    """"""\n    cell_geom = cell.to_shapely()\n    inter_polys = str_tree.query(cell_geom)\n\n    inter_over_cells = []\n    inter_over_polys = []\n    class_ids = []\n\n    # Find polygons whose intersection with the cell is big enough.\n    for poly in inter_polys:\n        inter = poly.intersection(cell_geom)\n        inter_over_cell = inter.area / cell_geom.area\n        inter_over_poly = inter.area / poly.area\n\n        if use_intersection_over_cell:\n            enough_inter = inter_over_cell >= ioa_thresh\n        else:\n            enough_inter = inter_over_poly >= ioa_thresh\n\n        if enough_inter:\n            inter_over_cells.append(inter_over_cell)\n            inter_over_polys.append(inter_over_poly)\n            class_ids.append(poly.class_id)\n\n    # Infer class id for cell.\n    if len(class_ids) == 0:\n        class_id = background_class_id\n    elif pick_min_class_id:\n        class_id = min(class_ids)\n    else:\n        # Pick class_id of the polygon with the biggest intersection over\n        # cell. If there is a tie, pick the first.\n        class_id = class_ids[np.argmax(inter_over_cells)]\n\n    return class_id\n\n\ndef infer_labels(geojson, extent, cell_sz, ioa_thresh,\n                 use_intersection_over_cell, pick_min_class_id,\n                 background_class_id):\n    """"""Infer ChipClassificationLabels grid from GeoJSON containing polygons.\n\n    Given GeoJSON with polygons associated with class_ids, infer a grid of\n    cells and class_ids that best captures the contents of each cell. See infer_cell for\n    info on the args.\n\n    Args:\n        geojson: dict in normalized GeoJSON format (see VectorSource)\n        extent: Box representing the bounds of the grid\n\n    Returns:\n        ChipClassificationLabels\n    """"""\n    labels = ChipClassificationLabels()\n    cells = extent.get_windows(cell_sz, cell_sz)\n\n    # We need to associate class_id with each geom. Monkey-patching it onto the geom\n    # seems like a bad idea, but it\'s the only straightforward way of doing this\n    # that I\'ve been able to find.\n    geoms = []\n    for f in geojson[\'features\']:\n        g = shape(f[\'geometry\'])\n        g.class_id = f[\'properties\'][\'class_id\']\n        geoms.append(g)\n    str_tree = STRtree(geoms)\n\n    for cell in cells:\n        class_id = infer_cell(cell, str_tree, ioa_thresh,\n                              use_intersection_over_cell, background_class_id,\n                              pick_min_class_id)\n        labels.set_cell(cell, class_id)\n    return labels\n\n\ndef read_labels(geojson, extent=None):\n    """"""Convert GeoJSON to ChipClassificationLabels.\n\n    If the GeoJSON already contains a grid of cells, then it can be constructed\n    in a straightforward manner without having to infer the class of cells.\n\n    If extent is given, only labels that intersect with the extent are returned.\n\n    Args:\n        geojson: dict in normalized GeoJSON format (see VectorSource)\n        extent: Box in pixel coords\n\n    Returns:\n       ChipClassificationLabels\n    """"""\n    labels = ChipClassificationLabels()\n\n    for f in geojson[\'features\']:\n        geom = shape(f[\'geometry\'])\n        (xmin, ymin, xmax, ymax) = geom.bounds\n        cell = Box(ymin, xmin, ymax, xmax)\n        if extent is not None and not cell.to_shapely().intersects(\n                extent.to_shapely()):\n            continue\n\n        props = f[\'properties\']\n        class_id = props[\'class_id\']\n        scores = props.get(\'scores\')\n        labels.set_cell(cell, class_id, scores)\n\n    return labels\n\n\nclass ChipClassificationLabelSource(LabelSource):\n    """"""A source of chip classification labels.\n\n    Ideally the vector_source contains a square for each cell in the grid. But\n    in reality, it can be difficult to label imagery in such an exhaustive way.\n    So, this can also handle sources with non-overlapping polygons that\n    do not necessarily cover the entire extent. It infers the grid of cells\n    and associated class_ids using the extent and options if infer_cells is\n    set to True.\n    """"""\n\n    def __init__(self,\n                 label_source_config,\n                 vector_source,\n                 class_config,\n                 crs_transformer,\n                 extent=None):\n        # extent is only needed if infer_cells is True or if it is False and\n        # you want to filter cells by extent\n        cfg = label_source_config\n        geojson = vector_source.get_geojson()\n\n        if cfg.infer_cells:\n            self.labels = infer_labels(\n                geojson, extent, cfg.cell_sz, cfg.ioa_thresh,\n                cfg.use_intersection_over_cell, cfg.pick_min_class_id,\n                cfg.background_class_id)\n        else:\n            self.labels = read_labels(geojson, extent)\n\n    def get_labels(self, window=None):\n        if window is None:\n            return self.labels\n        return self.labels.get_singleton_labels(window)\n'"
rastervision2/core/data/label_source/chip_classification_label_source_config.py,0,"b""from typing import Optional\n\nfrom rastervision2.core.data.vector_source import (VectorSourceConfig)\nfrom rastervision2.core.data.label_source import (\n    LabelSourceConfig, ChipClassificationLabelSource)\nfrom rastervision2.pipeline.config import (register_config, ConfigError, Field)\n\n\n@register_config('chip_classification_label_source')\nclass ChipClassificationLabelSourceConfig(LabelSourceConfig):\n    vector_source: VectorSourceConfig\n    ioa_thresh: Optional[float] = Field(\n        None,\n        description=\n        ('Minimum IOA of a polygon and cell for that polygon to be a candidate for '\n         'setting the class_id.'))\n    use_intersection_over_cell: bool = Field(\n        False,\n        description=\n        ('If True, then use the area of the cell as the denominator in the IOA. '\n         'Otherwise, use the area of the polygon.'))\n    pick_min_class_id: bool = Field(\n        False,\n        description=\n        ('If True, the class_id for a cell is the minimum class_id of the boxes in that '\n         'cell. Otherwise, pick the class_id of the box covering the greatest area.'\n         ))\n    background_class_id: Optional[int] = Field(\n        None,\n        description=\n        ('If not None, class_id to use as the background class; ie. the one that is used '\n         'when a window contains no boxes. If not set, empty windows have None set as '\n         'their class_id which is considered a null value.'))\n    infer_cells: bool = Field(\n        False,\n        description='If True, infers a grid of cells based on the cell_sz.')\n    cell_sz: Optional[int] = Field(\n        None,\n        description=\n        ('Size of a cell to use in pixels. If None, and this Config is part '\n         'of an RVPipeline, this field will be set from RVPipeline.train_chip_sz.'\n         ))\n\n    def build(self, class_config, crs_transformer, extent=None, tmp_dir=None):\n        vector_source = self.vector_source.build(class_config, crs_transformer)\n        return ChipClassificationLabelSource(\n            self, vector_source, class_config, crs_transformer, extent=extent)\n\n    def update(self, pipeline=None, scene=None):\n        super().update(pipeline, scene)\n        if self.cell_sz is None and pipeline is not None:\n            self.cell_sz = pipeline.train_chip_sz\n        self.vector_source.update(pipeline, scene)\n\n    def validate_config(self):\n        if self.vector_source.has_null_class_bufs():\n            raise ConfigError(\n                'Setting buffer to None for a class in the vector_source is '\n                'not allowed for ChipClassificationLabelSourceConfig.')\n"""
rastervision2/core/data/label_source/label_source.py,0,"b'from abc import ABC, abstractmethod\n\n\nclass LabelSource(ABC):\n    """"""An interface for storage of labels for a scene.\n\n    An LabelSource is a read source of labels for a scene\n    that could be backed by a file, a database, an API, etc. The difference\n    between LabelSources and Labels can be understood by analogy to the\n    difference between a database and result sets queried from a database.\n    """"""\n\n    @abstractmethod\n    def get_labels(self, window=None):\n        """"""Return labels overlapping with window.\n\n        Args:\n            window: Box\n\n        Returns:\n            Labels overlapping with window. If window is None,\n                returns all labels.\n        """"""\n        pass\n'"
rastervision2/core/data/label_source/label_source_config.py,0,"b""from rastervision2.pipeline.config import Config, register_config\n\n\n@register_config('label_source')\nclass LabelSourceConfig(Config):\n    def build(self, class_config, crs_transformer, extent, tmp_dir):\n        raise NotImplementedError()\n\n    def update(self, pipeline=None, scene=None):\n        pass\n"""
rastervision2/core/data/label_source/object_detection_label_source.py,0,"b'from rastervision2.core.data.label import ObjectDetectionLabels\nfrom rastervision2.core.data.label_source import LabelSource\n\n\nclass ObjectDetectionLabelSource(LabelSource):\n    def __init__(self, vector_source, extent=None):\n        """"""Constructor.\n\n        Args:\n            vector_source: (VectorSource)\n            extent: Box used to filter the labels by extent\n        """"""\n        self.labels = ObjectDetectionLabels.from_geojson(\n            vector_source.get_geojson(), extent=extent)\n\n    def get_labels(self, window=None):\n        if window is None:\n            return self.labels\n\n        return ObjectDetectionLabels.get_overlapping(self.labels, window)\n'"
rastervision2/core/data/label_source/object_detection_label_source_config.py,0,"b""from rastervision2.core.data.label_source import (LabelSourceConfig,\n                                                  ObjectDetectionLabelSource)\nfrom rastervision2.core.data.vector_source import (VectorSourceConfig)\nfrom rastervision2.pipeline.config import (register_config)\n\n\n@register_config('object_detection_label_source')\nclass ObjectDetectionLabelSourceConfig(LabelSourceConfig):\n    vector_source: VectorSourceConfig\n\n    def build(self, class_config, crs_transformer, extent, tmp_dir):\n        vs = self.vector_source.build(class_config, crs_transformer)\n        return ObjectDetectionLabelSource(vs, extent)\n"""
rastervision2/core/data/label_source/segmentation_class_transformer.py,0,"b'import numpy as np\n\nfrom rastervision2.core.data.utils import (color_to_triple, color_to_integer,\n                                           rgb_to_int_array)\n\n\nclass SegmentationClassTransformer():\n    def __init__(self, class_config):\n        color_to_class = class_config.get_color_to_class_id()\n        color_int_to_class = dict(\n            zip([color_to_integer(c) for c in color_to_class.keys()],\n                color_to_class.values()))\n        null_class_id = class_config.get_null_class_id()\n\n        def color_int_to_class_fn(color: int) -> int:\n            # Convert unspecified colors to null class\n            return color_int_to_class.get(color, null_class_id)\n\n        self.transform_color_int_to_class = \\\n            np.vectorize(color_int_to_class_fn, otypes=[np.uint8])\n\n        # class to color triple\n        class_to_color_triple = dict(\n            zip(color_to_class.values(),\n                [color_to_triple(c) for c in color_to_class.keys()]))\n\n        def class_to_channel_color(channel: int, class_id: int) -> int:\n            """"""Given a channel (red, green, or blue) and a class, return the\n            intensity of that channel.\n\n            Args:\n                 channel: An integer with value 0, 1, or 2\n                      representing the channel.\n                 class_id: The class id represented as an integer.\n            Returns:\n                 The intensity of the channel for the color associated\n                      with the given class.\n            """"""\n            default_triple = (0x00, 0x00, 0x00)\n            return class_to_color_triple.get(class_id, default_triple)[channel]\n\n        class_to_r = np.vectorize(\n            lambda c: class_to_channel_color(0, c), otypes=[np.uint8])\n        class_to_g = np.vectorize(\n            lambda c: class_to_channel_color(1, c), otypes=[np.uint8])\n        class_to_b = np.vectorize(\n            lambda c: class_to_channel_color(2, c), otypes=[np.uint8])\n        self.transform_class_to_color = [class_to_r, class_to_g, class_to_b]\n\n    def rgb_to_class(self, rgb_labels):\n        color_int_labels = rgb_to_int_array(rgb_labels)\n        class_labels = self.transform_color_int_to_class(color_int_labels)\n        return class_labels.astype(np.uint8)\n\n    def class_to_rgb(self, class_labels):\n        rgb_labels = np.empty(class_labels.shape + (3, ))\n        for chan in range(3):\n            class_to_channel_color = self.transform_class_to_color[chan]\n            rgb_labels[:, :, chan] = class_to_channel_color(class_labels)\n        return rgb_labels.astype(np.uint8)\n'"
rastervision2/core/data/label_source/semantic_segmentation_label_source.py,0,"b'from typing import (List, Optional)\n\nimport numpy as np\n\nfrom rastervision2.core.box import Box\nfrom rastervision2.core.data.class_config import ClassConfig\nfrom rastervision2.core.data import ActivateMixin\nfrom rastervision2.core.data.label import SemanticSegmentationLabels\nfrom rastervision2.core.data.label_source.label_source import (LabelSource)\nfrom rastervision2.core.data.label_source.segmentation_class_transformer import (\n    SegmentationClassTransformer)\nfrom rastervision2.core.data.raster_source import RasterSource\n\n\nclass SemanticSegmentationLabelSource(ActivateMixin, LabelSource):\n    """"""A read-only label source for semantic segmentation.""""""\n\n    def __init__(self,\n                 raster_source: RasterSource,\n                 rgb_class_config: ClassConfig = None):\n        """"""Constructor.\n\n        Args:\n            raster_source: (RasterSource) A raster source that returns a single channel\n                raster with class_ids as values, or a 3 channel raster with\n                RGB values that are mapped to class_ids using the rgb_class_map\n            rgb_class_config: (ClassConfig) with color values filled in.\n                Optional and used to\n                transform RGB values to class ids. Only use if the raster source\n                is RGB.\n        """"""\n        self.raster_source = raster_source\n        self.class_transformer = None\n        if rgb_class_config is not None:\n            self.class_transformer = SegmentationClassTransformer(\n                rgb_class_config)\n\n    def enough_target_pixels(self, window: Box, target_count_threshold: int,\n                             target_classes: List[int]) -> bool:\n        """"""Given a window, answer whether the window contains enough pixels in\n        the target classes.\n\n        Args:\n             window: The larger window from-which the sub-window will\n                  be clipped.\n             target_count_threshold:  Minimum number of target pixels.\n             target_classes: The classes of interest.  The given\n                  window is examined to make sure that it contains a\n                  sufficient number of target pixels.\n        Returns:\n             True (the window does contain interesting pixels) or False.\n        """"""\n        raw_labels = self.raster_source.get_raw_chip(window)\n        if self.class_transformer is not None:\n            labels = self.class_transformer.rgb_to_class(raw_labels)\n        else:\n            labels = np.squeeze(raw_labels)\n\n        target_count = 0\n        for class_id in target_classes:\n            target_count = target_count + (labels == class_id).sum()\n\n        return target_count >= target_count_threshold\n\n    def get_labels(self, window: Optional[Box] = None,\n                   chip_size=1000) -> SemanticSegmentationLabels:\n        """"""Get labels for a window.\n\n        To avoid running out of memory, if window is None and defaults to using the full\n        extent, a set of sub-windows of size chip_size are used which cover the full\n        extent with no overlap.\n\n        Args:\n             window: Either None or a window given as a Box object. Uses full extent of\n                scene if window is not provided.\n             chip_size: size of sub-windows to use if full extent is used (in\n                units of pixels)\n        Returns:\n             SemanticSegmentationLabels\n        """"""\n        labels = SemanticSegmentationLabels()\n        windows = (self.raster_source.get_extent().get_windows(\n            chip_size, chip_size) if window is None else [window])\n        for w in windows:\n            raw_labels = self.raster_source.get_raw_chip(w)\n            label_arr = (np.squeeze(raw_labels)\n                         if self.class_transformer is None else\n                         self.class_transformer.rgb_to_class(raw_labels))\n            labels.set_label_arr(w, label_arr)\n        return labels\n\n    def _subcomponents_to_activate(self):\n        return [self.raster_source]\n\n    def _activate(self):\n        pass\n\n    def _deactivate(self):\n        pass\n'"
rastervision2/core/data/label_source/semantic_segmentation_label_source_config.py,0,"b""from typing import Optional, Union\n\nfrom rastervision2.core.data.raster_source import (RasterSourceConfig,\n                                                   RasterizedSourceConfig)\nfrom rastervision2.core.data.label_source import (\n    LabelSourceConfig, SemanticSegmentationLabelSource)\nfrom rastervision2.core.data.class_config import (ClassConfig)\nfrom rastervision2.pipeline.config import (register_config, Field)\n\n\n@register_config('semantic_segmentation_label_source')\nclass SemanticSegmentationLabelSourceConfig(LabelSourceConfig):\n    raster_source: Union[RasterSourceConfig, RasterizedSourceConfig] = Field(\n        ..., description='The labels in the form of rasters.')\n    rgb_class_config: Optional[ClassConfig] = Field(\n        None,\n        description=\n        ('If set, will infer the class_ids for the labels using the colors field. This '\n         'assumes the labels are stored as RGB rasters.'))\n\n    def build(self, class_config, crs_transformer, extent, tmp_dir):\n        if isinstance(self.raster_source, RasterizedSourceConfig):\n            rs = self.raster_source.build(class_config, crs_transformer,\n                                          extent)\n        else:\n            rs = self.raster_source.build(tmp_dir)\n        return SemanticSegmentationLabelSource(rs, self.rgb_class_config)\n"""
rastervision2/core/data/label_store/__init__.py,0,b'# flake8: noqa\n\nfrom rastervision2.core.data.label_store.label_store import *\nfrom rastervision2.core.data.label_store.label_store_config import *\nfrom rastervision2.core.data.label_store.chip_classification_geojson_store import *\nfrom rastervision2.core.data.label_store.chip_classification_geojson_store_config import *\nfrom rastervision2.core.data.label_store.semantic_segmentation_label_store import *\nfrom rastervision2.core.data.label_store.semantic_segmentation_label_store_config import *\nfrom rastervision2.core.data.label_store.object_detection_geojson_store import *\nfrom rastervision2.core.data.label_store.object_detection_geojson_store_config import *\n'
rastervision2/core/data/label_store/chip_classification_geojson_store.py,0,"b'from rastervision2.pipeline.file_system import json_to_file\nfrom rastervision2.core.data.label import ChipClassificationLabels\nfrom rastervision2.core.data.label_store import LabelStore\nfrom rastervision2.core.data.label_store.utils import boxes_to_geojson\nfrom rastervision2.core.data.label_source import (\n    ChipClassificationLabelSourceConfig)\nfrom rastervision2.core.data.vector_source import (GeoJSONVectorSourceConfig)\n\n\nclass ChipClassificationGeoJSONStore(LabelStore):\n    """"""A GeoJSON file with classification labels in it.\n    """"""\n\n    def __init__(self, uri, class_config, crs_transformer):\n        """"""Construct ClassificationLabelStore backed by a GeoJSON file.\n\n        Args:\n            uri: uri of GeoJSON file containing labels\n            crs_transformer: CRSTransformer to convert from map coords in label\n                in GeoJSON file to pixel coords.\n            class_map: ClassMap used to infer class_ids from class_name\n                (or label) field\n        """"""\n        self.uri = uri\n        self.class_config = class_config\n        self.crs_transformer = crs_transformer\n\n    def save(self, labels):\n        """"""Save labels to URI if writable.\n\n        Note that if the grid is inferred from polygons, only the grid will be\n        written, not the original polygons.\n        """"""\n        boxes = labels.get_cells()\n        class_ids = labels.get_class_ids()\n        scores = list(labels.get_scores())\n        geojson = boxes_to_geojson(\n            boxes,\n            class_ids,\n            self.crs_transformer,\n            self.class_config,\n            scores=scores)\n        json_to_file(geojson, self.uri)\n\n    def get_labels(self):\n        vs = GeoJSONVectorSourceConfig(uri=self.uri, default_class_id=None)\n        ls = ChipClassificationLabelSourceConfig(vector_source=vs).build(\n            self.class_config, self.crs_transformer)\n        return ls.get_labels()\n\n    def empty_labels(self):\n        return ChipClassificationLabels()\n'"
rastervision2/core/data/label_store/chip_classification_geojson_store_config.py,0,"b""from typing import Optional\nfrom os.path import join\n\nfrom rastervision2.core.data.label_store import (\n    LabelStoreConfig, ChipClassificationGeoJSONStore)\nfrom rastervision2.pipeline.config import register_config, Field\n\n\n@register_config('chip_classification_geojson_store')\nclass ChipClassificationGeoJSONStoreConfig(LabelStoreConfig):\n    uri: Optional[str] = Field(\n        None,\n        description=\n        ('URI of GeoJSON file with predictions. If None, and this Config is part of '\n         'a SceneConfig inside an RVPipelineConfig, it will be auto-generated.'\n         ))\n\n    def build(self, class_config, crs_transformer, extent=None, tmp_dir=None):\n        return ChipClassificationGeoJSONStore(self.uri, class_config,\n                                              crs_transformer)\n\n    def update(self, pipeline=None, scene=None):\n        if self.uri is None and pipeline is not None and scene is not None:\n            self.uri = join(pipeline.predict_uri, '{}.json'.format(scene.id))\n"""
rastervision2/core/data/label_store/label_store.py,0,"b'from abc import ABC, abstractmethod\n\nfrom rastervision2.core.data import ActivateMixin\n\n\nclass LabelStore(ABC, ActivateMixin):\n    """"""This defines how to store prediction labels are stored for a scene.\n    """"""\n\n    @abstractmethod\n    def save(self, labels):\n        """"""Save.\n\n        Args:\n           labels - Labels to be saved, the type of which will be dependant on the type\n                    of pipeline.\n        """"""\n        pass\n\n    @abstractmethod\n    def get_labels(self):\n        """"""Loads Labels from this label store.""""""\n        pass\n\n    @abstractmethod\n    def empty_labels(self):\n        """"""Produces an empty Labels""""""\n        pass\n\n    def _subcomponents_to_activate(self):\n        pass\n\n    def _activate(self):\n        pass\n\n    def _deactivate(self):\n        pass\n'"
rastervision2/core/data/label_store/label_store_config.py,0,"b""from rastervision2.pipeline.config import Config, register_config\n\n\n@register_config('label_store')\nclass LabelStoreConfig(Config):\n    def build(self, class_config, crs_transformer, extent, tmp_dir):\n        pass\n\n    def update(self, pipeline=None, scene=None):\n        pass\n"""
rastervision2/core/data/label_store/object_detection_geojson_store.py,0,"b'from rastervision2.core.data.label import ObjectDetectionLabels\nfrom rastervision2.core.data.label_store import LabelStore\nfrom rastervision2.core.data.label_store.utils import boxes_to_geojson\nfrom rastervision2.core.data.vector_source import GeoJSONVectorSourceConfig\nfrom rastervision2.pipeline.file_system import json_to_file\n\n\nclass ObjectDetectionGeoJSONStore(LabelStore):\n    def __init__(self, uri, class_config, crs_transformer):\n        """"""Construct LabelStore backed by a GeoJSON file for object detection labels.\n\n        Args:\n            uri: uri of GeoJSON file containing labels\n            crs_transformer: CRSTransformer to convert from map coords in label\n                in GeoJSON file to pixel coords.\n            class_map: ClassMap used to infer class_ids from class_name\n                (or label) field\n        """"""\n        self.uri = uri\n        self.crs_transformer = crs_transformer\n        self.class_config = class_config\n\n    def save(self, labels):\n        """"""Save labels to URI.""""""\n        boxes = labels.get_boxes()\n        class_ids = labels.get_class_ids().tolist()\n        scores = labels.get_scores().tolist()\n        geojson = boxes_to_geojson(\n            boxes,\n            class_ids,\n            self.crs_transformer,\n            self.class_config,\n            scores=scores)\n        json_to_file(geojson, self.uri)\n\n    def get_labels(self):\n        vector_source = GeoJSONVectorSourceConfig(\n            uri=self.uri, default_class_id=None).build(self.class_config,\n                                                       self.crs_transformer)\n        return ObjectDetectionLabels.from_geojson(vector_source.get_geojson())\n\n    def empty_labels(self):\n        return ObjectDetectionLabels.make_empty()\n'"
rastervision2/core/data/label_store/object_detection_geojson_store_config.py,0,"b""from typing import Optional\nfrom os.path import join\n\nfrom rastervision2.core.data.label_store import (LabelStoreConfig,\n                                                 ObjectDetectionGeoJSONStore)\nfrom rastervision2.pipeline.config import register_config, Field\n\n\n@register_config('object_detection_geojson_store')\nclass ObjectDetectionGeoJSONStoreConfig(LabelStoreConfig):\n    uri: Optional[str] = Field(\n        None,\n        description=\n        ('URI of GeoJSON file with predictions. If None, and this Config is part of '\n         'a SceneConfig inside an RVPipelineConfig, it will be auto-generated.'\n         ))\n\n    def build(self, class_config, crs_transformer, extent=None, tmp_dir=None):\n        return ObjectDetectionGeoJSONStore(self.uri, class_config,\n                                           crs_transformer)\n\n    def update(self, pipeline=None, scene=None):\n        if pipeline is not None and scene is not None:\n            if self.uri is None:\n                self.uri = join(pipeline.predict_uri,\n                                '{}.json'.format(scene.id))\n"""
rastervision2/core/data/label_store/semantic_segmentation_label_store.py,0,"b'import numpy as np\nimport rasterio\n\nfrom rastervision2.pipeline.file_system import (get_local_path, make_dir,\n                                                upload_or_copy, file_exists)\nfrom rastervision2.core.data.label import SemanticSegmentationLabels\nfrom rastervision2.core.data.label_store import LabelStore\nfrom rastervision2.core.data.label_source import SegmentationClassTransformer\nfrom rastervision2.core.data.raster_source import RasterioSourceConfig\n\n\nclass SemanticSegmentationLabelStore(LabelStore):\n    def __init__(self,\n                 uri,\n                 extent,\n                 crs_transformer,\n                 tmp_dir,\n                 vector_output=None,\n                 class_config=None):\n        """"""Constructor.\n\n        Args:\n            uri: (str) URI of GeoTIFF file used for storing predictions as RGB values\n            extent: (Box) The extent of the scene\n            crs_transformer: (CRSTransformer)\n            tmp_dir: (str) temp directory to use\n            vector_output: (None or array of dicts) containing vectorifiction\n                configuration information\n            class_config: (ClassConfig) with color values used to convert\n                class ids to RGB value\n        """"""\n        self.uri = uri\n        self.vector_output = vector_output\n        self.extent = extent\n        self.crs_transformer = crs_transformer\n        self.tmp_dir = tmp_dir\n        # Note: can\'t name this class_transformer due to Python using that attribute\n        if class_config:\n            self.class_trans = SegmentationClassTransformer(class_config)\n        else:\n            self.class_trans = None\n\n        self.source = None\n        if file_exists(uri):\n            self.source = RasterioSourceConfig(uris=[uri]).build(tmp_dir)\n\n    def _subcomponents_to_activate(self):\n        if self.source is not None:\n            return [self.source]\n        return []\n\n    def get_labels(self, chip_size=1000):\n        """"""Get all labels.\n\n        Returns:\n            SemanticSegmentationLabels with windows of size chip_size covering the\n                scene with no overlap.\n        """"""\n        if self.source is None:\n            raise Exception(\'Raster source at {} does not exist\'.format(\n                self.uri))\n\n        labels = SemanticSegmentationLabels()\n        extent = self.source.get_extent()\n        windows = extent.get_windows(chip_size, chip_size)\n        for w in windows:\n            raw_labels = self.source.get_raw_chip(w)\n            label_arr = (np.squeeze(raw_labels) if self.class_trans is None\n                         else self.class_trans.rgb_to_class(raw_labels))\n            labels.set_label_arr(w, label_arr)\n        return labels\n\n    def save(self, labels):\n        """"""Save.\n\n        Args:\n            labels - (SemanticSegmentationLabels) labels to be saved\n        """"""\n        local_path = get_local_path(self.uri, self.tmp_dir)\n        make_dir(local_path, use_dirname=True)\n\n        transform = self.crs_transformer.get_affine_transform()\n        crs = self.crs_transformer.get_image_crs()\n\n        band_count = 1\n        dtype = np.uint8\n        if self.class_trans:\n            band_count = 3\n\n        mask = (np.zeros((self.extent.ymax, self.extent.xmax), dtype=np.uint8)\n                if self.vector_output else None)\n\n        # https://github.com/mapbox/rasterio/blob/master/docs/quickstart.rst\n        # https://rasterio.readthedocs.io/en/latest/topics/windowed-rw.html\n        with rasterio.open(\n                local_path,\n                \'w\',\n                driver=\'GTiff\',\n                height=self.extent.ymax,\n                width=self.extent.xmax,\n                count=band_count,\n                dtype=dtype,\n                transform=transform,\n                crs=crs) as dataset:\n            for window in labels.get_windows():\n                label_arr = labels.get_label_arr(window)\n                window = window.intersection(self.extent)\n                label_arr = label_arr[0:window.get_height(), 0:\n                                      window.get_width()]\n\n                if mask is not None:\n                    mask[window.ymin:window.ymax, window.xmin:\n                         window.xmax] = label_arr\n\n                window = window.rasterio_format()\n                if self.class_trans:\n                    rgb_labels = self.class_trans.class_to_rgb(label_arr)\n                    for chan in range(3):\n                        dataset.write_band(\n                            chan + 1, rgb_labels[:, :, chan], window=window)\n                else:\n                    img = label_arr.astype(dtype)\n                    dataset.write_band(1, img, window=window)\n\n        upload_or_copy(local_path, self.uri)\n\n        if self.vector_output:\n            import mask_to_polygons.vectorification as vectorification\n            import mask_to_polygons.processing.denoise as denoise\n\n            for vo in self.vector_output:\n                denoise_radius = vo.denoise\n                uri = vo.uri\n                mode = vo.get_mode()\n                class_id = vo.class_id\n                class_mask = np.array(mask == class_id, dtype=np.uint8)\n                local_geojson_path = get_local_path(uri, self.tmp_dir)\n\n                def transform(x, y):\n                    return self.crs_transformer.pixel_to_map((x, y))\n\n                if denoise_radius > 0:\n                    class_mask = denoise.denoise(class_mask, denoise_radius)\n\n                if uri and mode == \'buildings\':\n                    geojson = vectorification.geojson_from_mask(\n                        mask=class_mask,\n                        transform=transform,\n                        mode=mode,\n                        min_aspect_ratio=vo.min_aspect_ratio,\n                        min_area=vo.min_area,\n                        width_factor=vo.element_width_factor,\n                        thickness=vo.element_thickness)\n                elif uri and mode == \'polygons\':\n                    geojson = vectorification.geojson_from_mask(\n                        mask=class_mask, transform=transform, mode=mode)\n\n                if local_geojson_path:\n                    with open(local_geojson_path, \'w\') as file_out:\n                        file_out.write(geojson)\n                        upload_or_copy(local_geojson_path, uri)\n\n    def empty_labels(self):\n        """"""Returns an empty SemanticSegmentationLabels object.""""""\n        return SemanticSegmentationLabels()\n'"
rastervision2/core/data/label_store/semantic_segmentation_label_store_config.py,0,"b'from typing import Optional, List\nfrom os.path import join\n\nfrom rastervision2.core.data.label_store import (\n    LabelStoreConfig, SemanticSegmentationLabelStore)\nfrom rastervision2.pipeline.config import register_config, Config, Field\n\n\n@register_config(\'vector_output\')\nclass VectorOutputConfig(Config):\n    uri: Optional[str] = Field(\n        None,\n        description=\n        (\'URI of vector output. If None, and this Config is part of a SceneConfig and \'\n         \'RVPipeline, this field will be auto-generated.\'))\n    class_id: int = Field(\n        ...,\n        description=\'The prediction class that is to turned into vectors.\')\n    denoise: int = Field(\n        0,\n        description=\n        (\'Radius of the structural element used to remove high-frequency signals from \'\n         \'the image.\'))\n\n    def update(self, pipeline=None, scene=None):\n        if pipeline and scene:\n            self.uri = join(\n                pipeline.root_uri, \'predict\', \'{}-{}-{}.json\'.format(\n                    scene.id, self.class_id, self.get_mode()))\n\n    def get_mode(self):\n        raise NotImplementedError()\n\n\n@register_config(\'polygon_vector_output\')\nclass PolygonVectorOutputConfig(VectorOutputConfig):\n    def get_mode(self):\n        return \'polygons\'\n\n\n@register_config(\'building_vector_output\')\nclass BuildingVectorOutputConfig(VectorOutputConfig):\n    """"""Options useful for vectorization of building predictions.\n\n    Intended to break up clusters of buildings.\n    """"""\n    min_aspect_ratio: float = Field(\n        1.618,\n        description=\n        (\'Ratio between length and height (or height and length) of anything that can \'\n         \'be considered to be a cluster of buildings. The goal is to distinguish between \'\n         \'rows of buildings and (say) a single building.\'))\n    min_area: float = Field(\n        0.0,\n        description=\n        (\'Minimum area of anything that can be considered to be a cluster of buildings. \'\n         \'The goal is to distinguish between buildings and artifacts.\'))\n    element_width_factor: float = Field(\n        0.5,\n        description=\n        (\'Width of the structural element used to break building clusters as a fraction \'\n         \'of the width of the cluster.\'))\n    element_thickness: float = Field(\n        0.001,\n        description=\n        (\'Thickness of the structural element that is used to break building clusters.\'\n         ))\n\n    def get_mode(self):\n        return \'buildings\'\n\n\n@register_config(\'semantic_segmentation_label_store\')\nclass SemanticSegmentationLabelStoreConfig(LabelStoreConfig):\n    uri: Optional[str] = Field(\n        None,\n        description=(\n            \'URI of file with predictions. If None, and this Config is part of \'\n            \'a SceneConfig inside an RVPipelineConfig, this fiend will be \'\n            \'auto-generated.\'))\n    vector_output: List[VectorOutputConfig] = []\n    rgb: bool = Field(\n        False,\n        description=\n        (\'If True, save prediction class_ids in RGB format using the colors in \'\n         \'class_config.\'))\n\n    def build(self, class_config, crs_transformer, extent, tmp_dir):\n        return SemanticSegmentationLabelStore(\n            self.uri,\n            extent,\n            crs_transformer,\n            tmp_dir,\n            vector_output=self.vector_output,\n            class_config=class_config)\n\n    def update(self, pipeline=None, scene=None):\n        if pipeline is not None and scene is not None:\n            if self.uri is None:\n                self.uri = join(pipeline.predict_uri,\n                                \'{}.tif\'.format(scene.id))\n\n        for vo in self.vector_output:\n            vo.update(pipeline, scene)\n'"
rastervision2/core/data/label_store/utils.py,0,"b'def boxes_to_geojson(  # noqa\n        boxes,  # noqa\n        class_ids,\n        crs_transformer,\n        class_config,\n        scores=None):\n    """"""Convert boxes and associated data into a GeoJSON dict.\n\n    Args:\n        boxes: list of Box in pixel row/col format.\n        class_ids: list of int (one for each box)\n        crs_transformer: CRSTransformer used to convert pixel coords to map\n            coords in the GeoJSON\n        scores: optional list of score or scores.\n                If floats (one for each box), property name will be ""score"".\n                If lists of floats, property name will be ""scores"".\n\n    Returns:\n        dict in GeoJSON format\n    """"""\n    features = []\n    for box_ind, box in enumerate(boxes):\n        polygon = box.geojson_coordinates()\n        polygon = [list(crs_transformer.pixel_to_map(p)) for p in polygon]\n\n        class_id = int(class_ids[box_ind])\n        class_name = class_config.get_name(class_id)\n\n        feature = {\n            \'type\': \'Feature\',\n            \'geometry\': {\n                \'type\': \'Polygon\',\n                \'coordinates\': [polygon]\n            },\n            \'properties\': {\n                \'class_id\': class_id,\n                \'class_name\': class_name\n            }\n        }\n\n        if scores is not None:\n            box_scores = scores[box_ind]\n\n            if box_scores is not None:\n                if type(box_scores) is list:\n                    feature[\'properties\'][\'scores\'] = box_scores\n                else:\n                    feature[\'properties\'][\'score\'] = box_scores\n\n        features.append(feature)\n\n    return {\'type\': \'FeatureCollection\', \'features\': features}\n'"
rastervision2/core/data/raster_source/__init__.py,0,b'# flake8: noqa\n\nfrom rastervision2.core.data.raster_source.raster_source import *\nfrom rastervision2.core.data.raster_source.raster_source_config import *\nfrom rastervision2.core.data.raster_source.rasterio_source import *\nfrom rastervision2.core.data.raster_source.rasterio_source_config import *\nfrom rastervision2.core.data.raster_source.rasterized_source import *\nfrom rastervision2.core.data.raster_source.rasterized_source_config import *\n'
rastervision2/core/data/raster_source/raster_source.py,0,"b'from abc import ABC, abstractmethod\n\n\nclass ChannelOrderError(Exception):\n    def __init__(self, channel_order, num_channels):\n        self.channel_order = channel_order\n        self.num_channels = num_channels\n        msg = \'The channel_order={} contains a channel index >= num_channels={}\'\n        super().__init__(msg.format(str(channel_order), num_channels))\n\n\nclass RasterSource(ABC):\n    """"""A source of raster data.\n\n    This should be subclassed when adding a new source of raster data such as\n    a set of files, an API, a TMS URI schema, etc.\n    """"""\n\n    def __init__(self, channel_order, num_channels, raster_transformers=[]):\n        """"""Construct a new RasterSource.\n\n        Args:\n            channel_order: list of channel indices to use when extracting chip from\n                raw imagery.\n            num_channels: Number of channels in the raw imagery before applying\n                channel_order.\n            raster_transformers: RasterTransformers used to transform chips\n                whenever they are retrieved.\n        """"""\n        self.channel_order = channel_order\n        self.num_channels = num_channels\n        self.raster_transformers = raster_transformers\n\n    def validate_channel_order(self, channel_order, num_channels):\n        for c in channel_order:\n            if c >= num_channels:\n                raise ChannelOrderError(channel_order, num_channels)\n\n    @abstractmethod\n    def get_extent(self):\n        """"""Return the extent of the RasterSource.\n\n        Returns:\n            Box in pixel coordinates with extent\n        """"""\n        pass\n\n    @abstractmethod\n    def get_dtype(self):\n        """"""Return the numpy.dtype of this scene""""""\n        pass\n\n    @abstractmethod\n    def get_crs_transformer(self):\n        """"""Return the associated CRSTransformer.""""""\n        pass\n\n    @abstractmethod\n    def _get_chip(self, window):\n        """"""Return the raw chip located in the window.\n\n        Args:\n            window: Box\n\n        Returns:\n            [height, width, channels] numpy array\n        """"""\n        pass\n\n    def get_chip(self, window):\n        """"""Return the transformed chip in the window.\n\n        Get a raw chip, extract subset of channels using channel_order, and then apply\n        transformations.\n\n        Args:\n            window: Box\n\n        Returns:\n            np.ndarray with shape [height, width, channels]\n        """"""\n        chip = self._get_chip(window)\n\n        if self.channel_order:\n            chip = chip[:, :, self.channel_order]\n\n        for transformer in self.raster_transformers:\n            chip = transformer.transform(chip, self.channel_order)\n\n        return chip\n\n    def get_raw_chip(self, window):\n        """"""Return raw chip without using channel_order or applying transforms.\n\n        Args:\n            window: (Box) the window for which to get the chip\n\n        Returns:\n            np.ndarray with shape [height, width, channels]\n        """"""\n        return self._get_chip(window)\n\n    def get_image_array(self):\n        """"""Return entire transformed image array.\n\n        Not safe to call on very large RasterSources.\n        """"""\n        return self.get_chip(self.get_extent())\n\n    def get_raw_image_array(self):\n        """"""Return entire raw image without using channel_order or applying transforms.\n\n        Not safe to call on very large RasterSources.\n        """"""\n        return self.get_raw_chip(self.get_extent())\n'"
rastervision2/core/data/raster_source/raster_source_config.py,0,"b""from typing import List, Optional\n\nfrom rastervision2.pipeline.config import Config, register_config, Field\nfrom rastervision2.core.data.raster_transformer import RasterTransformerConfig\n\n\n@register_config('raster_source')\nclass RasterSourceConfig(Config):\n    channel_order: Optional[List[int]] = Field(\n        None,\n        description=\n        'The sequence of channel indices to use when reading imagery.')\n    transformers: List[RasterTransformerConfig] = []\n\n    def build(self, tmp_dir, use_transformers=True):\n        raise NotImplementedError()\n\n    def update(self, pipeline=None, scene=None):\n        for t in self.transformers:\n            t.update(pipeline, scene)\n"""
rastervision2/core/data/raster_source/rasterio_source.py,0,"b'import logging\nimport math\nimport os\nimport pyproj\nimport subprocess\nfrom decimal import Decimal\nimport tempfile\n\nimport numpy as np\nimport rasterio\nfrom rasterio.enums import (ColorInterp, MaskFlags)\n\nfrom rastervision2.pipeline.file_system import download_if_needed\nfrom rastervision2.core.box import Box\nfrom rastervision2.core.data.crs_transformer import RasterioCRSTransformer\nfrom rastervision2.core.data.raster_source import RasterSource\nfrom rastervision2.core.data import (ActivateMixin, ActivationError)\n\nlog = logging.getLogger(__name__)\nwgs84 = pyproj.Proj({\'init\': \'epsg:4326\'})\nwgs84_proj4 = \'+init=epsg:4326\'\nmeters_per_degree = 111319.5\n\n\ndef build_vrt(vrt_path, image_paths):\n    """"""Build a VRT for a set of TIFF files.""""""\n    cmd = [\'gdalbuildvrt\', vrt_path]\n    cmd.extend(image_paths)\n    subprocess.run(cmd)\n\n\ndef download_and_build_vrt(image_uris, tmp_dir):\n    log.info(\'Building VRT...\')\n    image_paths = [download_if_needed(uri, tmp_dir) for uri in image_uris]\n    image_path = os.path.join(tmp_dir, \'index.vrt\')\n    build_vrt(image_path, image_paths)\n    return image_path\n\n\ndef load_window(image_dataset, window=None, is_masked=False):\n    """"""Load a window of an image using Rasterio.\n\n    Args:\n        image_dataset: a Rasterio dataset\n        window: ((row_start, row_stop), (col_start, col_stop)) or\n        ((y_min, y_max), (x_min, x_max))\n        is_masked: If True, read a masked array from rasterio\n\n    Returns:\n        np.ndarray of shape (height, width, channels) where channels is the number of\n            channels in the image_dataset.\n    """"""\n    if is_masked:\n        im = image_dataset.read(window=window, boundless=True, masked=True)\n        im = np.ma.filled(im, fill_value=0)\n    else:\n        im = image_dataset.read(window=window, boundless=True)\n\n    # Handle non-zero NODATA values by setting the data to 0.\n    for channel, nodata in enumerate(image_dataset.nodatavals):\n        if nodata is not None and nodata != 0:\n            im[channel, im[channel] == nodata] = 0\n\n    im = np.transpose(im, axes=[1, 2, 0])\n    return im\n\n\nclass RasterioSource(ActivateMixin, RasterSource):\n    def __init__(self,\n                 uris,\n                 raster_transformers,\n                 tmp_dir,\n                 channel_order=None,\n                 x_shift=0.0,\n                 y_shift=0.0):\n        """"""Constructor.\n\n        This RasterSource can read any file that can be opened by Rasterio/GDAL\n        including georeferenced formats such as GeoTIFF and non-georeferenced formats\n        such as JPG. See https://www.gdal.org/formats_list.html for more details.\n\n        If channel_order is None, then use non-alpha channels. This also sets any\n        masked or NODATA pixel values to be zeros.\n\n        Args:\n            channel_order: list of indices of channels to extract from raw imagery\n        """"""\n        self.uris = uris\n        self.tmp_dir = tmp_dir\n        self.image_tmp_dir = None\n        self.image_dataset = None\n        self.x_shift = x_shift\n        self.y_shift = y_shift\n\n        num_channels = None\n\n        # Activate in order to get information out of the raster\n        with self.activate():\n            num_channels = self.image_dataset.count\n            if channel_order is None:\n                colorinterp = self.image_dataset.colorinterp\n                if colorinterp:\n                    channel_order = [\n                        i for i, color_interp in enumerate(colorinterp)\n                        if color_interp != ColorInterp.alpha\n                    ]\n                else:\n                    channel_order = list(range(0, num_channels))\n            self.validate_channel_order(channel_order, num_channels)\n\n            mask_flags = self.image_dataset.mask_flag_enums\n            self.is_masked = any(\n                [m for m in mask_flags if m != MaskFlags.all_valid])\n\n            self.height = self.image_dataset.height\n            self.width = self.image_dataset.width\n\n            # Get 1x1 chip and apply raster transformers to test dtype.\n            test_chip = self.get_raw_chip(Box.make_square(0, 0, 1))\n            test_chip = test_chip[:, :, channel_order]\n            for transformer in raster_transformers:\n                test_chip = transformer.transform(test_chip, channel_order)\n            self.dtype = test_chip.dtype\n\n            self._set_crs_transformer()\n\n        super().__init__(channel_order, num_channels, raster_transformers)\n\n    def _download_data(self, tmp_dir):\n        """"""Download any data needed for this Raster Source.\n\n        Return a single local path representing the image or a VRT of the data.\n        """"""\n        if len(self.uris) == 1:\n            return download_if_needed(self.uris[0], tmp_dir)\n        else:\n            return download_and_build_vrt(self.uris, tmp_dir)\n\n    def get_crs_transformer(self):\n        return self.crs_transformer\n\n    def get_extent(self):\n        return Box(0, 0, self.height, self.width)\n\n    def get_dtype(self):\n        """"""Return the numpy.dtype of this scene""""""\n        return self.dtype\n\n    def _get_chip(self, window):\n        if self.image_dataset is None:\n            raise ActivationError(\'RasterSource must be activated before use\')\n        shifted_window = self._get_shifted_window(window)\n        return load_window(\n            self.image_dataset,\n            window=shifted_window.rasterio_format(),\n            is_masked=self.is_masked)\n\n    def _activate(self):\n        # Download images to temporary directory and delete it when done.\n        self.image_tmp_dir = tempfile.TemporaryDirectory(dir=self.tmp_dir)\n        self.imagery_path = self._download_data(self.image_tmp_dir.name)\n        self.image_dataset = rasterio.open(self.imagery_path)\n        self._set_crs_transformer()\n\n    def _set_crs_transformer(self):\n        self.crs_transformer = RasterioCRSTransformer.from_dataset(\n            self.image_dataset)\n        self.crs = self.image_dataset.crs\n        if self.crs:\n            self.proj = pyproj.Proj(self.crs)\n        else:\n            self.proj = None\n        self.crs = str(self.crs)\n\n    def _deactivate(self):\n        self.image_dataset.close()\n        self.image_dataset = None\n        self.image_tmp_dir.cleanup()\n        self.image_tmp_dir = None\n\n    def _get_shifted_window(self, window):\n        do_shift = self.x_shift != 0.0 or self.y_shift != 0.0\n        if do_shift:\n            ymin, xmin, ymax, xmax = window.tuple_format()\n            width = window.get_width()\n            height = window.get_height()\n\n            # Transform image coordinates into world coordinates\n            transform = self.image_dataset.transform\n            xmin2, ymin2 = transform * (xmin, ymin)\n\n            # Transform from world coordinates to WGS84\n            if self.crs != wgs84_proj4 and self.proj:\n                lon, lat = pyproj.transform(self.proj, wgs84, xmin2, ymin2)\n            else:\n                lon, lat = xmin2, ymin2\n\n            # Shift.  This is performed by computing the shifts in\n            # meters to shifts in degrees.  Those shifts are then\n            # applied to the WGS84 coordinate.\n            #\n            # Courtesy of https://gis.stackexchange.com/questions/2951/algorithm-for-offsetting-a-latitude-longitude-by-some-amount-of-meters  # noqa\n            lat_radians = math.pi * lat / 180.0\n            dlon = Decimal(self.x_shift) / Decimal(\n                meters_per_degree * math.cos(lat_radians))\n            dlat = Decimal(self.y_shift) / Decimal(meters_per_degree)\n            lon = float(Decimal(lon) + dlon)\n            lat = float(Decimal(lat) + dlat)\n\n            # Transform from WGS84 to world coordinates\n            if self.crs != wgs84_proj4 and self.proj:\n                xmin3, ymin3 = pyproj.transform(wgs84, self.proj, lon, lat)\n                xmin3 = int(round(xmin3))\n                ymin3 = int(round(ymin3))\n            else:\n                xmin3, ymin3 = lon, lat\n\n            # Trasnform from world coordinates back into image coordinates\n            xmin4, ymin4 = ~transform * (xmin3, ymin3)\n\n            window = Box(ymin4, xmin4, ymin4 + height, xmin4 + width)\n        return window\n'"
rastervision2/core/data/raster_source/rasterio_source_config.py,0,"b'from typing import List\n\nfrom rastervision2.core.data.raster_source import RasterSourceConfig, RasterioSource\nfrom rastervision2.pipeline.config import register_config, Field\n\n\n@register_config(\'rasterio_source\')\nclass RasterioSourceConfig(RasterSourceConfig):\n    uris: List[str] = Field(\n        ...,\n        description=\n        (\'List of image URIs that comprise imagery for a scene. The format of each file \'\n         \'can be any that can be read by Rasterio/GDAL. If > 1 URI is provided, a VRT \'\n         \'will be created to mosaic together the individual images.\'))\n    x_shift: float = Field(\n        0.0,\n        descriptions=\n        (\'A number of meters to shift along the x-axis. A positive shift moves the \'\n         \'""camera"" to the right.\'))\n    y_shift: float = Field(\n        0.0,\n        descriptions=\n        (\'A number of meters to shift along the y-axis. A positive shift moves the \'\n         \'""camera"" down.\'))\n\n    def build(self, tmp_dir, use_transformers=True):\n        raster_transformers = ([rt.build() for rt in self.transformers]\n                               if use_transformers else [])\n\n        return RasterioSource(\n            self.uris,\n            raster_transformers,\n            tmp_dir,\n            channel_order=self.channel_order,\n            x_shift=self.x_shift,\n            y_shift=self.y_shift)\n'"
rastervision2/core/data/raster_source/rasterized_source.py,0,"b'import logging\n\nfrom rasterio.features import rasterize\nimport numpy as np\nfrom shapely.geometry import shape\nfrom shapely.strtree import STRtree\nfrom shapely.ops import transform\n\nfrom rastervision2.core.box import Box\nfrom rastervision2.core.data import (ActivateMixin, ActivationError)\nfrom rastervision2.core.data.raster_source import RasterSource\n\nlog = logging.getLogger(__name__)\n\n\ndef geoms_to_raster(str_tree, rasterizer_config, window, extent):\n    background_class_id = rasterizer_config.background_class_id\n    all_touched = rasterizer_config.all_touched\n\n    log.debug(\'Cropping shapes to window...\')\n    # Crop shapes against window, remove empty shapes, and put in window frame of\n    # reference.\n    window_geom = window.to_shapely()\n    shapes = str_tree.query(window_geom)\n    shapes = [(s, s.class_id) for s in shapes]\n    shapes = [(s.intersection(window_geom), c) for s, c in shapes]\n    shapes = [(s, c) for s, c in shapes if not s.is_empty]\n\n    def to_window_frame(x, y, z=None):\n        return (x - window.xmin, y - window.ymin)\n\n    shapes = [(transform(to_window_frame, s), c) for s, c in shapes]\n    log.debug(\'# of shapes in window: {}\'.format(len(shapes)))\n\n    out_shape = (window.get_height(), window.get_width())\n\n    # rasterize needs to be passed >= 1 shapes.\n    if shapes:\n        log.debug(\'rasterio.rasterize()...\')\n        raster = rasterize(\n            shapes,\n            out_shape=out_shape,\n            fill=background_class_id,\n            dtype=np.uint8,\n            all_touched=all_touched)\n    else:\n        raster = np.full(out_shape, background_class_id, dtype=np.uint8)\n\n    # Ensure that parts of window outside of extent have zero values which are counted as\n    # the don\'t-care class for segmentation.\n    valid_window = window_geom.intersection(extent.to_shapely())\n    if valid_window.is_empty:\n        raster[:, :] = 0\n    else:\n        vw = transform(to_window_frame, valid_window)\n        vw = Box.from_shapely(vw).to_int()\n        new_raster = np.zeros(out_shape)\n        new_raster[vw.ymin:vw.ymax, vw.xmin:vw.xmax] = \\\n            raster[vw.ymin:vw.ymax, vw.xmin:vw.xmax]\n        raster = new_raster\n\n    return raster\n\n\nclass RasterizedSource(ActivateMixin, RasterSource):\n    """"""A RasterSource based on the rasterization of a VectorSource.""""""\n\n    def __init__(self, vector_source, rasterizer_config, extent,\n                 crs_transformer):\n        """"""Constructor.\n\n        Args:\n            vector_source: (VectorSource)\n            rasterizer_config: (RasterizerConfig)\n            extent: (Box) extent of corresponding imagery RasterSource\n            crs_transformer: (CRSTransformer)\n        """"""\n        self.vector_source = vector_source\n        self.rasterizer_config = rasterizer_config\n        self.extent = extent\n        self.crs_transformer = crs_transformer\n        self.activated = False\n\n        super().__init__(channel_order=[0], num_channels=1)\n\n    def get_extent(self):\n        """"""Return the extent of the RasterSource.\n\n        Returns:\n            Box in pixel coordinates with extent\n        """"""\n        return self.extent\n\n    def get_dtype(self):\n        """"""Return the numpy.dtype of this scene""""""\n        return np.uint8\n\n    def get_crs_transformer(self):\n        """"""Return the associated CRSTransformer.""""""\n        return self.crs_transformer\n\n    def _get_chip(self, window):\n        """"""Return the chip located in the window.\n\n        Polygons falling within the window are rasterized using the class_id, and\n        the background is filled with background_class_id. Also, any pixels in the\n        window outside the extent are zero, which is the don\'t-care class for\n        segmentation.\n\n        Args:\n            window: Box\n\n        Returns:\n            [height, width, channels] numpy array\n        """"""\n        if not self.activated:\n            raise ActivationError(\'GeoJSONSource must be activated before use\')\n\n        log.debug(\'Rasterizing window: {}\'.format(window))\n        chip = geoms_to_raster(self.str_tree, self.rasterizer_config, window,\n                               self.get_extent())\n        # Add third singleton dim since rasters must have >=1 channel.\n        return np.expand_dims(chip, 2)\n\n    def _activate(self):\n        geojson = self.vector_source.get_geojson()\n        geoms = []\n        for f in geojson[\'features\']:\n            geom = shape(f[\'geometry\'])\n            geom.class_id = f[\'properties\'][\'class_id\']\n            geoms.append(geom)\n        self.str_tree = STRtree(geoms)\n        self.activated = True\n\n    def _deactivate(self):\n        self.str_tree = None\n        self.activated = False\n'"
rastervision2/core/data/raster_source/rasterized_source_config.py,0,"b""from rastervision2.core.data.raster_source import (RasterizedSource)\nfrom rastervision2.core.data.vector_source import (VectorSourceConfig)\nfrom rastervision2.pipeline.config import register_config, Config, Field, ConfigError\n\n\n@register_config('rasterizer')\nclass RasterizerConfig(Config):\n    background_class_id: int = Field(\n        ...,\n        description=\n        ('The class_id to use for any background pixels, ie. pixels not covered by a '\n         'polygon.'))\n    all_touched: bool = Field(\n        False,\n        description=(\n            'If True, all pixels touched by geometries will be burned in. '\n            'If false, only pixels whose center is within the polygon or '\n            'that are selected by Bresenham\xe2\x80\x99s line algorithm will be '\n            'burned in. (See rasterio.features.rasterize).'))\n\n\n@register_config('rasterized_source')\nclass RasterizedSourceConfig(Config):\n    vector_source: VectorSourceConfig\n    rasterizer_config: RasterizerConfig\n\n    def build(self, class_config, crs_transformer, extent):\n        vector_source = self.vector_source.build(class_config, crs_transformer)\n\n        return RasterizedSource(vector_source, self.rasterizer_config, extent,\n                                crs_transformer)\n\n    def validate_config(self):\n        if self.vector_source.has_null_class_bufs():\n            raise ConfigError(\n                'Setting buffer to None for a class in the vector_source is '\n                'not allowed for RasterizedSourceConfig.')\n"""
rastervision2/core/data/raster_transformer/__init__.py,0,b'# flake8: noqa\n\nfrom rastervision2.core.data.raster_transformer.raster_transformer import *\nfrom rastervision2.core.data.raster_transformer.raster_transformer_config import *\nfrom rastervision2.core.data.raster_transformer.stats_transformer import *\nfrom rastervision2.core.data.raster_transformer.stats_transformer_config import *\n'
rastervision2/core/data/raster_transformer/raster_transformer.py,0,"b'from abc import (ABC, abstractmethod)\n\n\nclass RasterTransformer(ABC):\n    """"""Transforms raw chips to be input to a neural network.""""""\n\n    @abstractmethod\n    def transform(self, chip, channel_order=None):\n        """"""Transform a chip of a raster source.\n\n        Args:\n            chip: ndarray of shape [height, width, channels] This is assumed to already\n                have the channel_order applied to it if channel_order is set. In other\n                words, channels should be equal to len(channel_order).\n            channel_order: list of indices of channels that were extracted from the\n                raw imagery.\n\n        Returns:\n            [height, width, channels] numpy array\n        """"""\n        pass\n'"
rastervision2/core/data/raster_transformer/raster_transformer_config.py,0,"b""from rastervision2.pipeline.config import Config, register_config\n\n\n@register_config('raster_transformer')\nclass RasterTransformerConfig(Config):\n    def update(self, pipeline=None, scene=None):\n        pass\n\n    def update_root(self, root_dir):\n        pass\n"""
rastervision2/core/data/raster_transformer/stats_transformer.py,0,"b'import numpy as np\n\nfrom rastervision2.core.data.raster_transformer.raster_transformer \\\n    import RasterTransformer\n\n\nclass StatsTransformer(RasterTransformer):\n    """"""Transforms non-uint8 to uint8 values using raster_stats.\n    """"""\n\n    def __init__(self, raster_stats=None):\n        """"""Construct a new StatsTransformer.\n\n        Args:\n            raster_stats: (RasterStats) used to transform chip to have\n                desired statistics\n        """"""\n        self.raster_stats = raster_stats\n\n    def transform(self, chip, channel_order=None):\n        """"""Transform a chip.\n\n        Transforms non-uint8 to uint8 values using raster_stats.\n\n        Args:\n            chip: ndarray of shape [height, width, channels] This is assumed to already\n                have the channel_order applied to it if channel_order is set. In other\n                words, channels should be equal to len(channel_order).\n            channel_order: list of indices of channels that were extracted from the\n                raw imagery.\n\n        Returns:\n            [height, width, channels] uint8 numpy array\n\n        """"""\n        if chip.dtype != np.uint8:\n            if self.raster_stats:\n                if channel_order is None:\n                    channel_order = np.arange(chip.shape[2])\n\n                # Subtract mean and divide by std to get zscores.\n                means = np.array(self.raster_stats.means)\n                means = means[np.newaxis, np.newaxis, channel_order].astype(\n                    np.float)\n                stds = np.array(self.raster_stats.stds)\n                stds = stds[np.newaxis, np.newaxis, channel_order].astype(\n                    np.float)\n\n                # Don\'t transform NODATA zero values.\n                nodata = chip == 0\n\n                chip = chip - means\n                chip = chip / stds\n\n                # Make zscores that fall between -3 and 3 span 0 to 255.\n                chip += 3\n                chip /= 6\n\n                chip = np.clip(chip, 0, 1)\n                chip *= 255\n                chip = chip.astype(np.uint8)\n\n                chip[nodata] = 0\n            else:\n                raise ValueError(\'raster_stats not defined.\')\n\n        return chip\n'"
rastervision2/core/data/raster_transformer/stats_transformer_config.py,0,"b""from typing import Optional\nfrom os.path import join, basename\n\nfrom rastervision2.pipeline.config import register_config, Field\nfrom rastervision2.core.data.raster_transformer.raster_transformer_config import (  # noqa\n    RasterTransformerConfig)\nfrom rastervision2.core.data.raster_transformer.stats_transformer import (  # noqa\n    StatsTransformer)\nfrom rastervision2.core.raster_stats import RasterStats\n\n\n@register_config('stats_transformer')\nclass StatsTransformerConfig(RasterTransformerConfig):\n    stats_uri: Optional[str] = Field(\n        None,\n        description=\n        ('The URI of the output of the StatsAnalyzer. If None, and this Config is '\n         'inside an RVPipeline, then this field will be auto-generated.'))\n\n    def update(self, pipeline=None, scene=None):\n        if pipeline is not None:\n            self.stats_uri = join(pipeline.analyze_uri, 'stats.json')\n\n    def build(self):\n        return StatsTransformer(RasterStats.load(self.stats_uri))\n\n    def update_root(self, root_dir):\n        self.stats_uri = join(root_dir, basename(self.stats_uri))\n"""
rastervision2/core/data/vector_source/__init__.py,0,b'# flake8: noqa\n\nfrom rastervision2.core.data.vector_source.geojson_vector_source import *\nfrom rastervision2.core.data.vector_source.geojson_vector_source_config import *\nfrom rastervision2.core.data.vector_source.vector_source import *\nfrom rastervision2.core.data.vector_source.vector_source_config import *\nfrom rastervision2.core.data.vector_source.class_inference import *\n'
rastervision2/core/data/vector_source/class_inference.py,0,"b'import copy\n\nfrom rastervision2.core.data.vector_source.label_maker.filter import (\n    create_filter)\n\n\nclass ClassInference():\n    """"""Infers missing class_ids from GeoJSON features.""""""\n\n    def __init__(self,\n                 default_class_id,\n                 class_config=None,\n                 class_id_to_filter=None):\n        self.class_config = class_config\n        self.class_id_to_filter = class_id_to_filter\n        self.default_class_id = default_class_id\n\n        if self.class_id_to_filter is not None:\n            self.class_id_to_filter = {}\n            for class_id, filter_exp in class_id_to_filter.items():\n                self.class_id_to_filter[class_id] = create_filter(filter_exp)\n\n    def infer_class_id(self, feature):\n        """"""Infer the class_id for a GeoJSON feature.\n\n        Args:\n            feature: (dict) GeoJSON feature\n\n        Rules:\n            1) If class_id is in feature[\'properties\'], use it.\n            2) If class_name or label are in feature[\'properties\'] and in class_config,\n                use corresponding class_id.\n            3) If class_id_to_filter is set and filter is true when applied to feature,\n                use corresponding class_id.\n            4) Otherwise, return the default_class_id\n        """"""\n        class_id = feature.get(\'properties\', {}).get(\'class_id\')\n        if class_id is not None:\n            return class_id\n\n        if self.class_config is not None:\n            class_name = feature.get(\'properties\', {}).get(\'class_name\')\n            if class_name in self.class_config.names:\n                return self.class_config.names.index(class_name)\n\n            label = feature.get(\'properties\', {}).get(\'label\')\n            if label in self.class_config.names:\n                return self.class_config.names.index(label)\n\n        if self.class_id_to_filter is not None:\n            for class_id, filter_fn in self.class_id_to_filter.items():\n                if filter_fn(feature):\n                    return class_id\n\n        return self.default_class_id\n\n    def transform_geojson(self, geojson):\n        """"""Transform GeoJSON by appending class_ids and removing features with no class.\n\n        For each feature in geojson, the class_id is inferred and is set into\n        feature[\'properties\']. If the class_id is None (because none of the rules apply\n        and the default_class_id is None), the feature is dropped.\n        """"""\n        new_features = []\n        for feature in geojson[\'features\']:\n            class_id = self.infer_class_id(feature)\n            if class_id is not None:\n                feature = copy.deepcopy(feature)\n                properties = feature.get(\'properties\', {})\n                properties[\'class_id\'] = class_id\n                feature[\'properties\'] = properties\n                new_features.append(feature)\n        new_geojson = {\'type\': \'FeatureCollection\', \'features\': new_features}\n        return new_geojson\n'"
rastervision2/core/data/vector_source/geojson_vector_source.py,0,"b""import json\n\nfrom rastervision2.core.data.vector_source.vector_source import VectorSource\nfrom rastervision2.pipeline.file_system import file_to_str\n\n\nclass GeoJSONVectorSource(VectorSource):\n    def __init__(self, geojson_vs_config, class_config, crs_transformer):\n        super().__init__(geojson_vs_config, class_config, crs_transformer)\n\n    def _get_geojson(self):\n        geojson = json.loads(file_to_str(self.vs_config.uri))\n        if not self.vs_config.ignore_crs_field and 'crs' in geojson:\n            raise Exception((\n                'The GeoJSON file at {} contains a CRS field which is not '\n                'allowed by the current GeoJSON standard or by Raster Vision. '\n                'All coordinates are expected to be in EPSG:4326 CRS. If the file uses '\n                'EPSG:4326 (ie. lat/lng on the WGS84 reference ellipsoid) and you would '\n                'like to ignore the CRS field, set ignore_crs_field=True in '\n                'GeoJSONVectorSourceConfig.').format(self.vs_config.uri))\n\n        return self.class_inference.transform_geojson(geojson)\n"""
rastervision2/core/data/vector_source/geojson_vector_source_config.py,0,"b""from rastervision2.core.data.vector_source.vector_source_config import (\n    VectorSourceConfig)\nfrom rastervision2.core.data.vector_source.geojson_vector_source import (\n    GeoJSONVectorSource)\nfrom rastervision2.pipeline.config import register_config, Field\n\n\n@register_config('geojson_vector_source')\nclass GeoJSONVectorSourceConfig(VectorSourceConfig):\n    uri: str = Field(..., description='The URI of a GeoJSON file.')\n    ignore_crs_field: bool = False\n\n    def build(self, class_config, crs_transformer):\n        return GeoJSONVectorSource(self, class_config, crs_transformer)\n"""
rastervision2/core/data/vector_source/vector_source.py,0,"b'from abc import ABC, abstractmethod\n\nfrom shapely.geometry import shape, mapping\nfrom shapely.ops import transform\n\nfrom rastervision2.core.data.vector_source.class_inference import (\n    ClassInference)\n\n\ndef transform_geojson(geojson,\n                      crs_transformer,\n                      line_bufs=None,\n                      point_bufs=None,\n                      to_map_coords=False):\n    def is_empty_feat(f):\n        # This was added to handle empty geoms which appear when using\n        # OSM vector tiles.\n        return ((not f.get(\'geometry\'))\n                or ((not f[\'geometry\'].get(\'coordinates\')) and\n                    (not f[\'geometry\'].get(\'geometries\'))))\n\n    new_features = []\n    for f in geojson[\'features\']:\n        if is_empty_feat(f):\n            continue\n\n        geom = shape(f[\'geometry\'])\n\n        # Convert map to pixel coords. We need to convert to pixel coords before applying\n        # buffering because those values are assumed to be in pixel units.\n        def m2p(x, y, z=None):\n            return crs_transformer.map_to_pixel((x, y))\n\n        geom = transform(m2p, geom)\n\n        # Split GeometryCollection into list of geoms.\n        geoms = [geom]\n        if geom.geom_type == \'GeometryCollection\':\n            geoms = list(geom)\n\n        # Split any MultiX to list of X.\n        new_geoms = []\n        for g in geoms:\n            if g.geom_type in [\n                    \'MultiPolygon\', \'MultiPoint\', \'MultiLineString\'\n            ]:\n                new_geoms.extend(list(g))\n            else:\n                new_geoms.append(g)\n        geoms = new_geoms\n\n        # Buffer geoms.\n        class_id = f[\'properties\'][\'class_id\']\n        new_geoms = []\n        for g in geoms:\n            if g.geom_type == \'LineString\':\n                line_buf = 1\n                if line_bufs is not None:\n                    line_buf = line_bufs.get(class_id, 1)\n                # If line_buf for the class_id was explicitly set as None, then\n                # don\'t buffer.\n                if line_buf is not None:\n                    g = g.buffer(line_buf)\n                new_geoms.append(g)\n            elif g.geom_type == \'Point\':\n                point_buf = 1\n                if point_bufs is not None:\n                    point_buf = point_bufs.get(class_id, 1)\n                # If point_buf for the class_id was explicitly set as None, then\n                # don\'t buffer.\n                if point_buf is not None:\n                    g = g.buffer(point_buf)\n                new_geoms.append(g)\n            else:\n                # Use buffer trick to handle self-intersecting polygons. Buffer returns\n                # a MultiPolygon if there is a bowtie, so we have to convert it to a\n                # list of Polygons.\n                poly_buf = g.buffer(0)\n                if poly_buf.geom_type == \'MultiPolygon\':\n                    new_geoms.extend(list(poly_buf))\n                else:\n                    new_geoms.append(poly_buf)\n        geoms = new_geoms\n\n        # Convert back to map coords if desired. This is here so the QGIS plugin can\n        # take the GeoJSON produced by a VectorSource and display it on a map.\n        if to_map_coords:\n\n            def p2m(x, y, z=None):\n                return crs_transformer.pixel_to_map((x, y))\n\n            geoms = [transform(p2m, g) for g in geoms]\n\n        for g in geoms:\n            new_f = {\n                \'type\': \'Feature\',\n                \'geometry\': mapping(g),\n                \'properties\': f[\'properties\']\n            }\n            # Have to check for empty features again which could have been introduced\n            # when splitting apart multi-geoms.\n            if not is_empty_feat(new_f):\n                new_features.append(new_f)\n\n    return {\'type\': \'FeatureCollection\', \'features\': new_features}\n\n\nclass VectorSource(ABC):\n    """"""A source of vector data.""""""\n\n    def __init__(self, vs_config, class_config, crs_transformer):\n        self.vs_config = vs_config\n        self.class_config = class_config\n        self.crs_transformer = crs_transformer\n        self.class_inference = ClassInference(\n            vs_config.default_class_id,\n            class_config=class_config,\n            class_id_to_filter=vs_config.class_id_to_filter)\n        self.geojson = None\n\n    def get_geojson(self, to_map_coords=False):\n        """"""Return normalized GeoJSON.\n\n        This infers a class_id property for each feature, converts to pixels coords (by\n        default), removes empty features, splits apart multi-geoms and geom collections\n        into single geometries, and buffers lines and points into Polygons.\n\n        Args:\n            to_map_coords: If true, will return GeoJSON in map coordinates.\n\n        Returns:\n            dict in GeoJSON format\n        """"""\n        if self.geojson is None:\n            self.geojson = self._get_geojson()\n\n        geojson = transform_geojson(\n            self.geojson,\n            self.crs_transformer,\n            self.vs_config.line_bufs,\n            self.vs_config.point_bufs,\n            to_map_coords=to_map_coords)\n\n        return geojson\n\n    @abstractmethod\n    def _get_geojson(self):\n        """"""Return GeoJSON with class_ids in the properties.""""""\n        pass\n'"
rastervision2/core/data/vector_source/vector_source_config.py,0,"b""from typing import Dict, Optional, Sequence, Union\n\nfrom rastervision2.pipeline.config import Config, register_config, Field\n\nClassFilter = Sequence[Union[str, 'ClassFilter']]\n\n\n@register_config('vector_source')\nclass VectorSourceConfig(Config):\n    default_class_id: Optional[int] = Field(\n        ...,\n        description=\n        ('The default class_id to use if class cannot be inferred using other '\n         'mechanisms. If a feature defaults to a class_id of None, then that feature '\n         'will be deleted.'))\n    class_id_to_filter: Optional[Dict[int, Optional[ClassFilter]]] = Field(\n        None,\n        description=(\n            'Map from class_id to JSON filter used to infer missing class_ids. The '\n            'filter schema is according to '\n            'https://github.com/mapbox/mapbox-gl-js/blob/c9900db279db776f493ce8b6749966cedc2d6b8a/src/style-spec/feature_filter/index.js.'  # noqa\n        ))\n    line_bufs: Optional[Dict[int, Union[int, float, None]]] = Field(\n        None,\n        description=\n        ('This is useful, for example, for buffering lines representing roads so that '\n         'their width roughly matches the width of roads in the imagery. If None, uses '\n         'default buffer value of 1. Otherwise, a map from class_id to '\n         'number of pixels to buffer by. If the buffer value is None, then no buffering '\n         'will be performed and the LineString or Point won\\'t get converted to a '\n         'Polygon. Not converting to Polygon is incompatible with the currently '\n         'available LabelSources, but may be useful in the future.'))\n    point_bufs: Optional[Dict[int, Union[int, float, None]]] = Field(\n        None,\n        description=\n        'Same as above, but used for buffering Points into Polygons.')\n\n    def has_null_class_bufs(self):\n        if self.point_bufs is not None:\n            for c, v in self.point_bufs.items():\n                if v is None:\n                    return True\n\n        if self.line_bufs is not None:\n            for c, v in self.line_bufs.items():\n                if v is None:\n                    return True\n\n        return False\n\n    def build(self, class_config, crs_transformer):\n        raise NotImplementedError()\n\n    def update(self, pipeline=None, scene=None):\n        pass\n"""
tests_v2/core/data/label/__init__.py,0,b''
tests_v2/core/data/label/test_chip_classification_labels.py,0,"b""import unittest\n\nfrom rastervision2.core.box import Box\nfrom rastervision2.core.data.label.chip_classification_labels import (\n    ChipClassificationLabels)\n\n\nclass TestChipClassificationLabels(unittest.TestCase):\n    def setUp(self):\n        self.labels = ChipClassificationLabels()\n\n        self.cell1 = Box.make_square(0, 0, 2)\n        self.class_id1 = 1\n        self.labels.set_cell(self.cell1, self.class_id1)\n\n        self.cell2 = Box.make_square(0, 2, 2)\n        self.class_id2 = 2\n        self.labels.set_cell(self.cell2, self.class_id2)\n\n    def test_get_cell(self):\n        cell = Box.make_square(0, 2, 3)\n        class_id = self.labels.get_cell_class_id(cell)\n        self.assertEqual(class_id, None)\n\n        class_id = self.labels.get_cell_class_id(self.cell1)\n        self.assertEqual(class_id, self.class_id1)\n\n        class_id = self.labels.get_cell_class_id(self.cell2)\n        self.assertEqual(class_id, self.class_id2)\n\n    def test_get_singleton_labels(self):\n        labels = self.labels.get_singleton_labels(self.cell1)\n\n        cells = labels.get_cells()\n        self.assertEqual(len(cells), 1)\n\n        class_id = labels.get_cell_class_id(self.cell1)\n        self.assertEqual(class_id, self.class_id1)\n\n    def test_get_cells(self):\n        cells = self.labels.get_cells()\n        self.assertEqual(len(cells), 2)\n        # ordering of cells isn't known\n        self.assertTrue((cells[0] == self.cell1 and cells[1] == self.cell2)\n                        or (cells[1] == self.cell1 and cells[0] == self.cell2))\n\n    def test_get_class_ids(self):\n        cells = self.labels.get_cells()\n        class_ids = self.labels.get_class_ids()\n        # check that order of class_ids corresponds to order of cells\n        if (cells[0] == self.cell1 and cells[1] == self.cell2):\n            self.assertListEqual(class_ids, [1, 2])\n        elif (cells[1] == self.cell1 and cells[0] == self.cell2):\n            self.assertListEqual(class_ids, [2, 1])\n\n    def test_extend(self):\n        labels = ChipClassificationLabels()\n        cell3 = Box.make_square(0, 4, 2)\n        class_id3 = 1\n        labels.set_cell(cell3, class_id3)\n\n        self.labels.extend(labels)\n        cells = self.labels.get_cells()\n        self.assertEqual(len(cells), 3)\n        self.assertTrue(cell3 in cells)\n\n    def test_filter_by_aoi(self):\n        aois = [Box.make_square(0, 0, 2).to_shapely()]\n        filt_labels = self.labels.filter_by_aoi(aois)\n\n        exp_labels = ChipClassificationLabels()\n        cell1 = Box.make_square(0, 0, 2)\n        class_id1 = 1\n        exp_labels.set_cell(cell1, class_id1)\n        self.assertEqual(filt_labels, exp_labels)\n\n        aois = [Box.make_square(4, 4, 2).to_shapely()]\n        filt_labels = self.labels.filter_by_aoi(aois)\n\n        exp_labels = ChipClassificationLabels()\n        self.assertEqual(filt_labels, exp_labels)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests_v2/core/data/label/test_object_detection_labels.py,0,"b""import unittest\n\nimport numpy as np\n\nfrom rastervision2.core.box import Box\nfrom rastervision2.core.data.class_config import ClassConfig\nfrom rastervision2.core.data.label.object_detection_labels import (\n    ObjectDetectionLabels)\nfrom rastervision2.core.data.label.tfod_utils.np_box_list import BoxList\n\n\nclass ObjectDetectionLabelsTest(unittest.TestCase):\n    def setUp(self):\n        self.class_config = ClassConfig(names=['car', 'house'])\n        self.npboxes = np.array([\n            [0., 0., 2., 2.],\n            [2., 2., 4., 4.],\n        ])\n        self.class_ids = np.array([0, 1])\n        self.scores = np.array([0.9, 0.9])\n        self.labels = ObjectDetectionLabels(\n            self.npboxes, self.class_ids, scores=self.scores)\n\n    def test_from_boxlist(self):\n        boxlist = BoxList(self.npboxes)\n        boxlist.add_field('classes', self.class_ids)\n        boxlist.add_field('scores', self.scores)\n        labels = ObjectDetectionLabels.from_boxlist(boxlist)\n        labels.assert_equal(self.labels)\n\n    def test_make_empty(self):\n        npboxes = np.empty((0, 4))\n        class_ids = np.empty((0, ))\n        scores = np.empty((0, ))\n        expected_labels = ObjectDetectionLabels(\n            npboxes, class_ids, scores=scores)\n\n        labels = ObjectDetectionLabels.make_empty()\n        labels.assert_equal(expected_labels)\n\n    def test_constructor(self):\n        labels = ObjectDetectionLabels(\n            self.npboxes, self.class_ids, scores=self.scores)\n        expected_labels = ObjectDetectionLabels(self.npboxes, self.class_ids,\n                                                self.scores)\n        labels.assert_equal(expected_labels)\n\n        labels = ObjectDetectionLabels(self.npboxes, self.class_ids)\n        scores = np.ones(self.class_ids.shape)\n        expected_labels = ObjectDetectionLabels(\n            self.npboxes, self.class_ids, scores=scores)\n        labels.assert_equal(expected_labels)\n\n    def test_get_boxes(self):\n        boxes = self.labels.get_boxes()\n        self.assertEqual(len(boxes), 2)\n        np.testing.assert_array_equal(boxes[0].npbox_format(),\n                                      self.npboxes[0, :])\n        np.testing.assert_array_equal(boxes[1].npbox_format(),\n                                      self.npboxes[1, :])\n\n    def test_len(self):\n        nb_labels = len(self.labels)\n        self.assertEqual(self.npboxes.shape[0], nb_labels)\n\n    def test_local_to_global(self):\n        local_npboxes = np.array([[0., 0., 2., 2.], [2., 2., 4., 4.]])\n        window = Box.make_square(10, 10, 10)\n        global_npboxes = ObjectDetectionLabels.local_to_global(\n            local_npboxes, window)\n\n        expected_global_npboxes = np.array([[10., 10., 12., 12.],\n                                            [12., 12., 14., 14.]])\n        np.testing.assert_array_equal(global_npboxes, expected_global_npboxes)\n\n    def test_global_to_local(self):\n        global_npboxes = np.array([[10., 10., 12., 12.], [12., 12., 14., 14.]])\n        window = Box.make_square(10, 10, 10)\n        local_npboxes = ObjectDetectionLabels.global_to_local(\n            global_npboxes, window)\n\n        expected_local_npboxes = np.array([[0., 0., 2., 2.], [2., 2., 4., 4.]])\n        np.testing.assert_array_equal(local_npboxes, expected_local_npboxes)\n\n    def test_local_to_normalized(self):\n        local_npboxes = np.array([[0., 0., 2., 2.], [2., 2., 4., 4.]])\n        window = Box(0, 0, 10, 100)\n        norm_npboxes = ObjectDetectionLabels.local_to_normalized(\n            local_npboxes, window)\n\n        expected_norm_npboxes = np.array([[0., 0., 0.2, 0.02],\n                                          [0.2, 0.02, 0.4, 0.04]])\n        np.testing.assert_array_equal(norm_npboxes, expected_norm_npboxes)\n\n    def test_normalized_to_local(self):\n        norm_npboxes = np.array([[0., 0., 0.2, 0.02], [0.2, 0.02, 0.4, 0.04]])\n        window = Box(0, 0, 10, 100)\n        local_npboxes = ObjectDetectionLabels.normalized_to_local(\n            norm_npboxes, window)\n\n        expected_local_npboxes = np.array([[0., 0., 2., 2.], [2., 2., 4., 4.]])\n        np.testing.assert_array_equal(local_npboxes, expected_local_npboxes)\n\n    def test_get_overlapping(self):\n        window = Box.make_square(0, 0, 2.01)\n        labels = ObjectDetectionLabels.get_overlapping(self.labels, window)\n        labels.assert_equal(self.labels)\n\n        window = Box.make_square(0, 0, 3)\n        labels = ObjectDetectionLabels.get_overlapping(\n            self.labels, window, ioa_thresh=0.5)\n        npboxes = np.array([[0., 0., 2., 2.]])\n        class_ids = np.array([0])\n        scores = np.array([0.9])\n        expected_labels = ObjectDetectionLabels(\n            npboxes, class_ids, scores=scores)\n        labels.assert_equal(expected_labels)\n\n        window = Box.make_square(0, 0, 3)\n        labels = ObjectDetectionLabels.get_overlapping(\n            self.labels, window, ioa_thresh=0.1, clip=True)\n        expected_npboxes = np.array([\n            [0., 0., 2., 2.],\n            [2., 2., 3., 3.],\n        ])\n        expected_labels = ObjectDetectionLabels(\n            expected_npboxes, self.class_ids, scores=self.scores)\n        labels.assert_equal(expected_labels)\n\n    def test_concatenate(self):\n        npboxes = np.array([[4., 4., 5., 5.]])\n        class_ids = np.array([1])\n        scores = np.array([0.3])\n        labels = ObjectDetectionLabels(npboxes, class_ids, scores=scores)\n        new_labels = ObjectDetectionLabels.concatenate(self.labels, labels)\n\n        npboxes = np.array([[0., 0., 2., 2.], [2., 2., 4., 4.],\n                            [4., 4., 5., 5.]])\n        class_ids = np.array([0, 1, 1])\n        scores = np.array([0.9, 0.9, 0.3])\n        expected_labels = ObjectDetectionLabels(\n            npboxes, class_ids, scores=scores)\n        new_labels.assert_equal(expected_labels)\n\n    def test_prune_duplicates(self):\n        # This first box has a score below score_thresh so it should get\n        # pruned. The third box overlaps with the second, but has higher score,\n        # so the second one should get pruned. The fourth box overlaps with\n        # the second less than merge_thresh, so it should not get pruned.\n        npboxes = np.array([[0., 0., 2., 2.], [2., 2., 4., 4.],\n                            [2.1, 2.1, 4.1, 4.1], [3.5, 3.5, 5.5, 5.5]])\n        class_ids = np.array([0, 1, 0, 1])\n        scores = np.array([0.2, 0.9, 0.9, 1.0])\n        labels = ObjectDetectionLabels(npboxes, class_ids, scores=scores)\n        score_thresh = 0.5\n        merge_thresh = 0.5\n        pruned_labels = ObjectDetectionLabels.prune_duplicates(\n            labels, score_thresh, merge_thresh)\n\n        self.assertEqual(len(pruned_labels), 2)\n\n        expected_npboxes = np.array([[2.1, 2.1, 4.1, 4.1],\n                                     [3.5, 3.5, 5.5, 5.5]])\n        expected_class_ids = np.array([0, 1])\n        expected_scores = np.array([0.9, 1.0])\n\n        # prune_duplicates does not maintain ordering of boxes, so find match\n        # between pruned boxes and expected_npboxes.\n        pruned_npboxes = pruned_labels.get_npboxes()\n        pruned_inds = [None, None]\n        for box_ind, box in enumerate(expected_npboxes):\n            for pruned_box_ind, pruned_box in enumerate(pruned_npboxes):\n                if np.array_equal(pruned_box, box):\n                    pruned_inds[box_ind] = pruned_box_ind\n        self.assertTrue(np.all(pruned_inds is not None))\n\n        expected_labels = ObjectDetectionLabels(\n            expected_npboxes[pruned_inds],\n            expected_class_ids[pruned_inds],\n            scores=expected_scores[pruned_inds])\n        pruned_labels.assert_equal(expected_labels)\n\n    def test_filter_by_aoi(self):\n        aois = [Box.make_square(0, 0, 2).to_shapely()]\n        filt_labels = self.labels.filter_by_aoi(aois)\n\n        npboxes = np.array([[0., 0., 2., 2.]])\n        class_ids = np.array([0])\n        scores = np.array([0.9])\n        exp_labels = ObjectDetectionLabels(npboxes, class_ids, scores=scores)\n        self.assertEqual(filt_labels, exp_labels)\n\n        aois = [Box.make_square(4, 4, 2).to_shapely()]\n        filt_labels = self.labels.filter_by_aoi(aois)\n        exp_labels = ObjectDetectionLabels.make_empty()\n        self.assertEqual(filt_labels, exp_labels)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests_v2/core/data/label/test_semantic_segmentation_labels.py,0,"b""import unittest\n\nimport numpy as np\n\nfrom rastervision2.core.box import Box\nfrom rastervision2.core.data.label import SemanticSegmentationLabels\n\n\nclass TestSemanticSegmentationLabels(unittest.TestCase):\n    def setUp(self):\n        self.windows = [Box.make_square(0, 0, 10), Box.make_square(0, 10, 10)]\n        self.label_arr0 = np.random.choice([0, 1], (10, 10))\n        self.label_arr1 = np.random.choice([0, 1], (10, 10))\n        self.labels = SemanticSegmentationLabels()\n        self.labels.set_label_arr(self.windows[0], self.label_arr0)\n        self.labels.set_label_arr(self.windows[1], self.label_arr1)\n\n    def test_get(self):\n        np.testing.assert_array_equal(\n            self.labels.get_label_arr(self.windows[0]), self.label_arr0)\n\n    def test_get_with_aoi(self):\n        null_class_id = 2\n\n        aoi_polygons = [Box.make_square(5, 15, 2).to_shapely()]\n        exp_label_arr = np.full(self.label_arr1.shape, null_class_id)\n        exp_label_arr[5:7, 5:7] = self.label_arr1[5:7, 5:7]\n\n        labels = self.labels.filter_by_aoi(aoi_polygons, null_class_id)\n        label_arr = labels.get_label_arr(self.windows[1])\n        np.testing.assert_array_equal(label_arr, exp_label_arr)\n        self.assertEqual(1, len(labels.window_to_label_arr))\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests_v2/core/data/label_source/__init__.py,0,b''
tests_v2/core/data/label_source/test_chip_classification_label_source.py,0,"b""import unittest\nimport os\n\nfrom shapely.geometry import shape\nfrom shapely.strtree import STRtree\n\nfrom rastervision2.pipeline import rv_config\nfrom rastervision2.pipeline.file_system import json_to_file\nfrom rastervision2.core.box import Box\nfrom rastervision2.core.data import (\n    ClassConfig, infer_cell, ChipClassificationLabelSourceConfig,\n    GeoJSONVectorSourceConfig)\n\nfrom tests_v2.core.data.mock_crs_transformer import DoubleCRSTransformer\n\n\nclass TestChipClassificationLabelSource(unittest.TestCase):\n    def setUp(self):\n        self.crs_transformer = DoubleCRSTransformer()\n        self.geojson = {\n            'type':\n            'FeatureCollection',\n            'features': [{\n                'type': 'Feature',\n                'geometry': {\n                    'type':\n                    'MultiPolygon',\n                    'coordinates': [[[[0., 0.], [0., 2.], [2., 2.], [2., 0.],\n                                      [0., 0.]]]]\n                },\n                'properties': {\n                    'class_name': 'car',\n                    'class_id': 0,\n                    'score': 0.0\n                }\n            }, {\n                'type': 'Feature',\n                'geometry': {\n                    'type':\n                    'Polygon',\n                    'coordinates': [[[2., 2.], [2., 4.], [4., 4.], [4., 2.],\n                                     [2., 2.]]]\n                },\n                'properties': {\n                    'score': 0.0,\n                    'class_name': 'house',\n                    'class_id': 1\n                }\n            }]\n        }\n\n        self.class_config = ClassConfig(names=['car', 'house'])\n\n        self.box1 = Box.make_square(0, 0, 4)\n        self.box2 = Box.make_square(4, 4, 4)\n        self.class_id1 = 0\n        self.class_id2 = 1\n        self.background_class_id = 2\n\n        geoms = []\n        for f in self.geojson['features']:\n            g = shape(f['geometry'])\n            g.class_id = f['properties']['class_id']\n            geoms.append(g)\n        self.str_tree = STRtree(geoms)\n\n        self.file_name = 'labels.json'\n        self.tmp_dir = rv_config.get_tmp_dir()\n        self.uri = os.path.join(self.tmp_dir.name, self.file_name)\n        json_to_file(self.geojson, self.uri)\n\n    def tearDown(self):\n        self.tmp_dir.cleanup()\n\n    def test_infer_cell1(self):\n        # More of box 1 is in cell.\n        cell = Box.make_square(0, 0, 3)\n        ioa_thresh = 0.5\n        use_intersection_over_cell = False\n        background_class_id = None\n        pick_min_class_id = False\n\n        class_id = infer_cell(cell, self.str_tree, ioa_thresh,\n                              use_intersection_over_cell, background_class_id,\n                              pick_min_class_id)\n        self.assertEqual(class_id, self.class_id1)\n\n    def test_infer_cell2(self):\n        # More of box 2 is in cell.\n        cell = Box.make_square(1, 1, 3)\n        ioa_thresh = 0.5\n        use_intersection_over_cell = False\n        background_class_id = None\n        pick_min_class_id = False\n\n        class_id = infer_cell(cell, self.str_tree, ioa_thresh,\n                              use_intersection_over_cell, background_class_id,\n                              pick_min_class_id)\n        self.assertEqual(class_id, self.class_id2)\n\n    def test_infer_cell3(self):\n        # Only box 2 is in cell, but IOA isn't high enough.\n        cell = Box.make_square(3, 3, 3)\n        ioa_thresh = 0.5\n        use_intersection_over_cell = False\n        background_class_id = None\n        pick_min_class_id = False\n\n        class_id = infer_cell(cell, self.str_tree, ioa_thresh,\n                              use_intersection_over_cell, background_class_id,\n                              pick_min_class_id)\n        self.assertEqual(class_id, None)\n\n    def test_infer_cell4(self):\n        # Both boxes inside cell, but using intersection_over_cell,\n        # the IOA isn't high enough.\n        cell = Box.make_square(0, 0, 10)\n        ioa_thresh = 0.5\n        use_intersection_over_cell = True\n        background_class_id = None\n        pick_min_class_id = False\n\n        class_id = infer_cell(cell, self.str_tree, ioa_thresh,\n                              use_intersection_over_cell, background_class_id,\n                              pick_min_class_id)\n        self.assertEqual(class_id, None)\n\n    def test_infer_cell5(self):\n        # More of box1 in cell, using intersection_over_cell with the\n        # IOA high enough.\n        cell = Box.make_square(0, 0, 3)\n        ioa_thresh = 0.4\n        use_intersection_over_cell = True\n        background_class_id = None\n        pick_min_class_id = False\n\n        class_id = infer_cell(cell, self.str_tree, ioa_thresh,\n                              use_intersection_over_cell, background_class_id,\n                              pick_min_class_id)\n        self.assertEqual(class_id, self.class_id1)\n\n    def test_infer_cell6(self):\n        # No boxes overlap enough, use background_class_id\n        cell = Box.make_square(0, 0, 10)\n        ioa_thresh = 0.5\n        use_intersection_over_cell = True\n        background_class_id = self.background_class_id\n        pick_min_class_id = False\n\n        class_id = infer_cell(cell, self.str_tree, ioa_thresh,\n                              use_intersection_over_cell, background_class_id,\n                              pick_min_class_id)\n        self.assertEqual(class_id, self.background_class_id)\n\n    def test_infer_cell7(self):\n        # Cell doesn't overlap with any boxes.\n        cell = Box.make_square(10, 10, 1)\n        ioa_thresh = 0.5\n        use_intersection_over_cell = True\n        background_class_id = None\n        pick_min_class_id = False\n\n        class_id = infer_cell(cell, self.str_tree, ioa_thresh,\n                              use_intersection_over_cell, background_class_id,\n                              pick_min_class_id)\n        self.assertEqual(class_id, None)\n\n    def test_infer_cell8(self):\n        # box2 overlaps more than box1, but using pick_min_class_id, so\n        # picks box1.\n        cell = Box.make_square(1, 1, 3)\n        ioa_thresh = 0.5\n        use_intersection_over_cell = False\n        background_class_id = None\n        pick_min_class_id = True\n\n        class_id = infer_cell(cell, self.str_tree, ioa_thresh,\n                              use_intersection_over_cell, background_class_id,\n                              pick_min_class_id)\n        self.assertEqual(class_id, self.class_id2)\n\n    def test_get_labels_inferred(self):\n        extent = Box.make_square(0, 0, 8)\n\n        config = ChipClassificationLabelSourceConfig(\n            vector_source=GeoJSONVectorSourceConfig(\n                uri=self.uri, default_class_id=None),\n            ioa_thresh=0.5,\n            use_intersection_over_cell=False,\n            pick_min_class_id=False,\n            background_class_id=self.background_class_id,\n            infer_cells=True,\n            cell_sz=4)\n        source = config.build(\n            self.class_config, self.crs_transformer, extent, self.tmp_dir.name)\n        labels = source.get_labels()\n        cells = labels.get_cells()\n\n        self.assertEqual(len(cells), 4)\n        self.assertEqual(labels.get_cell_class_id(self.box1), self.class_id1)\n        self.assertEqual(labels.get_cell_class_id(self.box2), self.class_id2)\n        self.assertEqual(\n            labels.get_cell_class_id(Box.make_square(0, 4, 4)),\n            self.background_class_id)\n        self.assertEqual(\n            labels.get_cell_class_id(Box.make_square(4, 0, 4)),\n            self.background_class_id)\n\n    def test_get_labels_small_extent(self):\n        # Extent only has enough of first box in it.\n        extent = Box.make_square(0, 0, 2)\n\n        config = ChipClassificationLabelSourceConfig(\n            vector_source=GeoJSONVectorSourceConfig(\n                uri=self.uri, default_class_id=None))\n        source = config.build(\n            self.class_config, self.crs_transformer, extent, self.tmp_dir.name)\n        labels = source.get_labels()\n\n        cells = labels.get_cells()\n        self.assertEqual(len(cells), 1)\n        class_id = labels.get_cell_class_id(self.box1)\n        self.assertEqual(class_id, self.class_id1)\n        class_id = labels.get_cell_class_id(self.box2)\n        self.assertEqual(class_id, None)\n\n    def test_get_labels(self):\n        # Extent contains both boxes.\n        extent = Box.make_square(0, 0, 8)\n\n        config = ChipClassificationLabelSourceConfig(\n            vector_source=GeoJSONVectorSourceConfig(\n                uri=self.uri, default_class_id=None))\n        source = config.build(\n            self.class_config, self.crs_transformer, extent, self.tmp_dir.name)\n        labels = source.get_labels()\n\n        cells = labels.get_cells()\n        self.assertEqual(len(cells), 2)\n        class_id = labels.get_cell_class_id(self.box1)\n        self.assertEqual(class_id, self.class_id1)\n        class_id = labels.get_cell_class_id(self.box2)\n        self.assertEqual(class_id, self.class_id2)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests_v2/core/data/label_source/test_object_detection_label_source.py,0,"b""import unittest\nimport os\n\nimport numpy as np\n\nfrom rastervision2.core.data import (\n    ObjectDetectionLabelSourceConfig, GeoJSONVectorSourceConfig, ObjectDetectionLabels,\n    ClassConfig)\nfrom rastervision2.core import Box\nfrom rastervision2.pipeline import rv_config\nfrom rastervision2.pipeline.file_system import json_to_file\n\nfrom tests_v2.core.data.mock_crs_transformer import DoubleCRSTransformer\n\n\nclass TestObjectDetectionLabelSource(unittest.TestCase):\n    def setUp(self):\n        self.file_name = 'labels.json'\n        self.tmp_dir = rv_config.get_tmp_dir()\n        self.file_path = os.path.join(self.tmp_dir.name, self.file_name)\n\n        self.crs_transformer = DoubleCRSTransformer()\n        self.geojson = {\n            'type':\n            'FeatureCollection',\n            'features': [{\n                'type': 'Feature',\n                'geometry': {\n                    'type':\n                    'Polygon',\n                    'coordinates': [[[0., 0.], [0., 1.], [1., 1.], [1., 0.],\n                                     [0., 0.]]]\n                },\n                'properties': {\n                    'class_id': 0,\n                    'score': 0.9\n                }\n            }, {\n                'type': 'Feature',\n                'geometry': {\n                    'type':\n                    'Polygon',\n                    'coordinates': [[[1., 1.], [1., 2.], [2., 2.], [2., 1.],\n                                     [1., 1.]]]\n                },\n                'properties': {\n                    'score': 0.9,\n                    'class_id': 1\n                }\n            }]\n        }\n\n        self.extent = Box.make_square(0, 0, 10)\n        self.class_config = ClassConfig(names=['car', 'house'])\n        json_to_file(self.geojson, self.file_path)\n\n    def tearDown(self):\n        self.tmp_dir.cleanup()\n\n    def test_read_without_extent(self):\n        config = ObjectDetectionLabelSourceConfig(\n            vector_source=GeoJSONVectorSourceConfig(\n                uri=self.file_path, default_class_id=None))\n        extent = None\n        source = config.build(\n            self.class_config, self.crs_transformer, extent, self.tmp_dir.name)\n        labels = source.get_labels()\n\n        npboxes = np.array([[0., 0., 2., 2.], [2., 2., 4., 4.]])\n        class_ids = np.array([0, 1])\n        scores = np.array([0.9, 0.9])\n        expected_labels = ObjectDetectionLabels(\n            npboxes, class_ids, scores=scores)\n        labels.assert_equal(expected_labels)\n\n    def test_read_with_extent(self):\n        # Extent only includes the first box.\n        extent = Box.make_square(0, 0, 3)\n        config = ObjectDetectionLabelSourceConfig(\n            vector_source=GeoJSONVectorSourceConfig(\n                uri=self.file_path, default_class_id=None))\n        source = config.build(\n            self.class_config, self.crs_transformer, extent, self.tmp_dir.name)\n        labels = source.get_labels()\n\n        npboxes = np.array([[0., 0., 2., 2.]])\n        class_ids = np.array([0])\n        scores = np.array([0.9])\n        expected_labels = ObjectDetectionLabels(\n            npboxes, class_ids, scores=scores)\n        labels.assert_equal(expected_labels)\n\n        # Extent includes both boxes, but clips the second.\n        extent = Box.make_square(0, 0, 3.9)\n        config = ObjectDetectionLabelSourceConfig(\n            vector_source=GeoJSONVectorSourceConfig(\n                uri=self.file_path, default_class_id=None))\n        source = config.build(\n            self.class_config, self.crs_transformer, extent, self.tmp_dir.name)\n        labels = source.get_labels()\n\n        npboxes = np.array([[0., 0., 2., 2.], [2., 2., 3.9, 3.9]])\n        class_ids = np.array([0, 1])\n        scores = np.array([0.9, 0.9])\n        expected_labels = ObjectDetectionLabels(\n            npboxes, class_ids, scores=scores)\n        labels.assert_equal(expected_labels)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests_v2/core/data/label_source/test_segmentation_class_transformer.py,0,"b""import unittest\n\nimport numpy as np\n\nfrom rastervision2.core.data import SegmentationClassTransformer\nfrom rastervision2.core.data.utils import color_to_triple\nfrom rastervision2.core.data.class_config import ClassConfig\n\n\nclass TestSegmentationClassTransformer(unittest.TestCase):\n    def setUp(self):\n        self.class_config = ClassConfig(\n            names=['a', 'b', 'c'], colors=['red', 'green', 'blue'])\n        self.class_config.ensure_null_class()\n        self.transformer = SegmentationClassTransformer(self.class_config)\n\n        self.rgb_image = np.zeros((1, 3, 3))\n        self.rgb_image[0, 0, :] = color_to_triple('red')\n        self.rgb_image[0, 1, :] = color_to_triple('green')\n        self.rgb_image[0, 2, :] = color_to_triple('blue')\n\n        self.class_image = np.array([[0, 1, 2]])\n\n    def test_rgb_to_class(self):\n        class_image = self.transformer.rgb_to_class(self.rgb_image)\n        expected_class_image = self.class_image\n        np.testing.assert_array_equal(class_image, expected_class_image)\n\n    def test_class_to_rgb(self):\n        rgb_image = self.transformer.class_to_rgb(self.class_image)\n        expected_rgb_image = self.rgb_image\n        np.testing.assert_array_equal(rgb_image, expected_rgb_image)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests_v2/core/data/label_source/test_semantic_segmentation_label_source.py,0,"b""import unittest\n\nimport numpy as np\n\nfrom rastervision2.core import Box\nfrom rastervision2.core.data import ClassConfig, SemanticSegmentationLabelSource\nfrom tests_v2.core.data.mock_raster_source import MockRasterSource\n\n\nclass TestSemanticSegmentationLabelSource(unittest.TestCase):\n    def test_enough_target_pixels_true(self):\n        data = np.zeros((10, 10, 1), dtype=np.uint8)\n        data[4:, 4:, :] = 1\n        raster_source = MockRasterSource([0], 1)\n        raster_source.set_raster(data)\n        label_source = SemanticSegmentationLabelSource(raster_source=raster_source)\n        with label_source.activate():\n            extent = Box(0, 0, 10, 10)\n            self.assertTrue(label_source.enough_target_pixels(extent, 30, [1]))\n\n    def test_enough_target_pixels_false(self):\n        data = np.zeros((10, 10, 1), dtype=np.uint8)\n        data[7:, 7:, :] = 1\n        raster_source = MockRasterSource([0], 1)\n        raster_source.set_raster(data)\n        label_source = SemanticSegmentationLabelSource(raster_source=raster_source)\n        with label_source.activate():\n            extent = Box(0, 0, 10, 10)\n            self.assertFalse(\n                label_source.enough_target_pixels(extent, 30, [1]))\n\n    def test_get_labels(self):\n        data = np.zeros((10, 10, 1), dtype=np.uint8)\n        data[7:, 7:, 0] = 1\n        raster_source = MockRasterSource([0], 1)\n        raster_source.set_raster(data)\n        label_source = SemanticSegmentationLabelSource(raster_source=raster_source)\n        with label_source.activate():\n            window = Box.make_square(7, 7, 3)\n            labels = label_source.get_labels(window=window)\n            label_arr = labels.get_label_arr(window)\n            expected_label_arr = np.ones((3, 3))\n            np.testing.assert_array_equal(label_arr, expected_label_arr)\n\n    def test_get_labels_rgb(self):\n        data = np.zeros((10, 10, 3), dtype=np.uint8)\n        data[7:, 7:, :] = [1, 1, 1]\n        raster_source = MockRasterSource([0, 1, 2], 3)\n        raster_source.set_raster(data)\n        rgb_class_config = ClassConfig(names=['a'], colors=['#010101'])\n        rgb_class_config.ensure_null_class()\n        label_source = SemanticSegmentationLabelSource(\n            raster_source=raster_source, rgb_class_config=rgb_class_config)\n        with label_source.activate():\n            window = Box.make_square(7, 7, 3)\n            labels = label_source.get_labels(window=window)\n            label_arr = labels.get_label_arr(window)\n            expected_label_arr = np.zeros((3, 3))\n            np.testing.assert_array_equal(label_arr, expected_label_arr)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests_v2/core/data/raster_source/__init__.py,0,b''
tests_v2/core/data/raster_source/test_rasterio_source.py,0,"b""import unittest\nfrom os.path import join\n\nimport numpy as np\nimport rasterio\nfrom rasterio.enums import ColorInterp\n\nfrom rastervision2.core import (RasterStats)\nfrom rastervision2.core.utils.misc import save_img\nfrom rastervision2.core.data import (\n    ChannelOrderError, RasterioSourceConfig, StatsTransformerConfig)\nfrom rastervision2.pipeline import rv_config\n\nfrom tests_v2 import data_file_path\n\n\nclass TestRasterioSource(unittest.TestCase):\n    def setUp(self):\n        self.tmp_dir_obj = rv_config.get_tmp_dir()\n        self.tmp_dir = self.tmp_dir_obj.name\n\n    def tearDown(self):\n        self.tmp_dir_obj.cleanup()\n\n    def test_nodata_val(self):\n        # make geotiff filled with ones and zeros with nodata == 1\n        img_path = join(self.tmp_dir, 'tmp.tif')\n        height = 100\n        width = 100\n        nb_channels = 3\n        with rasterio.open(\n                img_path,\n                'w',\n                driver='GTiff',\n                height=height,\n                width=width,\n                count=nb_channels,\n                dtype=np.uint8,\n                nodata=1) as img_dataset:\n            im = np.random.randint(\n                0, 2, (height, width, nb_channels)).astype(np.uint8)\n            for channel in range(nb_channels):\n                img_dataset.write(im[:, :, channel], channel + 1)\n\n        config = RasterioSourceConfig(uris=[img_path])\n        source = config.build(tmp_dir=self.tmp_dir)\n        with source.activate():\n            out_chip = source.get_image_array()\n            expected_out_chip = np.zeros((height, width, nb_channels))\n            np.testing.assert_equal(out_chip, expected_out_chip)\n\n    def test_mask(self):\n        # make geotiff filled with ones and zeros and mask the whole image\n        img_path = join(self.tmp_dir, 'tmp.tif')\n        height = 100\n        width = 100\n        nb_channels = 3\n        with rasterio.open(\n                img_path,\n                'w',\n                driver='GTiff',\n                height=height,\n                width=width,\n                count=nb_channels,\n                dtype=np.uint8) as img_dataset:\n            im = np.random.randint(\n                0, 2, (height, width, nb_channels)).astype(np.uint8)\n            for channel in range(nb_channels):\n                img_dataset.write(im[:, :, channel], channel + 1)\n            img_dataset.write_mask(\n                np.zeros(im.shape[0:2]).astype(np.bool))\n\n        config = RasterioSourceConfig(uris=[img_path])\n        source = config.build(tmp_dir=self.tmp_dir)\n        with source.activate():\n            out_chip = source.get_image_array()\n            expected_out_chip = np.zeros((height, width, nb_channels))\n            np.testing.assert_equal(out_chip, expected_out_chip)\n\n    def test_get_dtype(self):\n        img_path = data_file_path('small-rgb-tile.tif')\n        config = RasterioSourceConfig(uris=[img_path])\n        source = config.build(tmp_dir=self.tmp_dir)\n        self.assertEqual(source.get_dtype(), np.uint8)\n\n    def test_gets_raw_chip(self):\n        img_path = data_file_path('small-rgb-tile.tif')\n        channel_order = [0, 1]\n\n        config = RasterioSourceConfig(uris=[img_path], channel_order=channel_order)\n        source = config.build(tmp_dir=self.tmp_dir)\n        with source.activate():\n            out_chip = source.get_raw_image_array()\n            self.assertEqual(out_chip.shape[2], 3)\n\n    def test_shift_x(self):\n        # Specially-engineered image w/ one meter per pixel resolution\n        # in the x direction.\n        img_path = data_file_path('ones.tif')\n        channel_order = [0]\n\n        config = RasterioSourceConfig(\n            uris=[img_path], channel_order=channel_order,\n            x_shift=1.0, y_shift=0.0)\n        source = config.build(tmp_dir=self.tmp_dir)\n\n        with source.activate():\n            extent = source.get_extent()\n            data = source.get_chip(extent)\n            self.assertEqual(data.sum(), 2**16 - 256)\n            column = data[:, 255, 0]\n            self.assertEqual(column.sum(), 0)\n\n    def test_shift_y(self):\n        # Specially-engineered image w/ one meter per pixel resolution\n        # in the y direction.\n        img_path = data_file_path('ones.tif')\n        channel_order = [0]\n\n        config = RasterioSourceConfig(\n            uris=[img_path], channel_order=channel_order,\n            x_shift=0.0, y_shift=1.0)\n        source = config.build(tmp_dir=self.tmp_dir)\n\n        with source.activate():\n            extent = source.get_extent()\n            data = source.get_chip(extent)\n            self.assertEqual(data.sum(), 2**16 - 256)\n            row = data[0, :, 0]\n            self.assertEqual(row.sum(), 0)\n\n    def test_gets_raw_chip_from_uint16_transformed_proto(self):\n        img_path = data_file_path('small-uint16-tile.tif')\n        channel_order = [0, 1]\n\n        config = RasterioSourceConfig(uris=[img_path])\n        raw_rs = config.build(tmp_dir=self.tmp_dir)\n\n        stats_uri = join(self.tmp_dir, 'tmp.tif')\n        stats = RasterStats()\n        stats.compute([raw_rs])\n        stats.save(stats_uri)\n\n        transformer = StatsTransformerConfig(stats_uri=stats_uri)\n        config = RasterioSourceConfig(\n            uris=[img_path],\n            channel_order=channel_order,\n            transformers=[transformer])\n        rs = config.build(tmp_dir=self.tmp_dir)\n\n        with rs.activate():\n            out_chip = rs.get_raw_image_array()\n            self.assertEqual(out_chip.shape[2], 3)\n\n    def test_uses_channel_order(self):\n        img_path = join(self.tmp_dir, 'img.tif')\n        chip = np.ones((2, 2, 4)).astype(np.uint8)\n        chip[:, :, :] *= np.array([0, 1, 2, 3]).astype(np.uint8)\n        save_img(chip, img_path)\n\n        channel_order = [0, 1, 2]\n        config = RasterioSourceConfig(\n            uris=[img_path], channel_order=channel_order)\n        source = config.build(tmp_dir=self.tmp_dir)\n\n        with source.activate():\n            out_chip = source.get_image_array()\n            expected_out_chip = np.ones((2, 2, 3)).astype(np.uint8)\n            expected_out_chip[:, :, :] *= np.array([0, 1,\n                                                    2]).astype(np.uint8)\n            np.testing.assert_equal(out_chip, expected_out_chip)\n\n    def test_channel_order_error(self):\n        img_path = join(self.tmp_dir, 'img.tif')\n        chip = np.ones((2, 2, 3)).astype(np.uint8)\n        chip[:, :, :] *= np.array([0, 1, 2]).astype(np.uint8)\n        save_img(chip, img_path)\n\n        channel_order = [3, 1, 0]\n        with self.assertRaises(ChannelOrderError):\n            config = RasterioSourceConfig(\n                uris=[img_path], channel_order=channel_order)\n            config.build(tmp_dir=self.tmp_dir)\n\n    def test_detects_alpha(self):\n        # Set first channel to alpha. Expectation is that when omitting channel_order,\n        # only the second and third channels will be in output.\n        img_path = join(self.tmp_dir, 'img.tif')\n        chip = np.ones((2, 2, 3)).astype(np.uint8)\n        chip[:, :, :] *= np.array([0, 1, 2]).astype(np.uint8)\n        save_img(chip, img_path)\n\n        ci = (ColorInterp.alpha, ColorInterp.blue, ColorInterp.green)\n        with rasterio.open(img_path, 'r+') as src:\n            src.colorinterp = ci\n\n        config = RasterioSourceConfig(uris=[img_path])\n        source = config.build(tmp_dir=self.tmp_dir)\n        with source.activate():\n            out_chip = source.get_image_array()\n            expected_out_chip = np.ones((2, 2, 2)).astype(np.uint8)\n            expected_out_chip[:, :, :] *= np.array([1, 2]).astype(np.uint8)\n            np.testing.assert_equal(out_chip, expected_out_chip)\n\n    def test_non_geo(self):\n        # Check if non-georeferenced image files can be read and CRSTransformer\n        # implements the identity function.\n        img_path = join(self.tmp_dir, 'img.png')\n        chip = np.ones((2, 2, 3)).astype(np.uint8)\n        save_img(chip, img_path)\n\n        config = RasterioSourceConfig(uris=[img_path])\n        source = config.build(tmp_dir=self.tmp_dir)\n        with source.activate():\n            out_chip = source.get_image_array()\n            np.testing.assert_equal(out_chip, chip)\n\n            p = (3, 4)\n            out_p = source.get_crs_transformer().map_to_pixel(p)\n            np.testing.assert_equal(out_p, p)\n\n            out_p = source.get_crs_transformer().pixel_to_map(p)\n            np.testing.assert_equal(out_p, p)\n\n    def test_no_epsg(self):\n        crs = rasterio.crs.CRS()\n        img_path = join(self.tmp_dir, 'tmp.tif')\n        height = 100\n        width = 100\n        nb_channels = 3\n        with rasterio.open(\n                img_path,\n                'w',\n                driver='GTiff',\n                height=height,\n                width=width,\n                count=nb_channels,\n                dtype=np.uint8,\n                crs=crs) as img_dataset:\n            im = np.zeros((height, width, nb_channels)).astype(np.uint8)\n            for channel in range(nb_channels):\n                img_dataset.write(im[:, :, channel], channel + 1)\n\n        try:\n            config = RasterioSourceConfig(uris=[img_path])\n            config.build(tmp_dir=self.tmp_dir)\n        except Exception:\n            self.fail(\n                'Creating RasterioSource with CRS with no EPSG attribute '\n                'raised an exception when it should not have.')\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests_v2/core/data/raster_source/test_rasterized_source.py,0,"b""import unittest\nfrom os.path import join\n\nimport numpy as np\n\nfrom rastervision2.core import Box\nfrom rastervision2.core.data import (\n    IdentityCRSTransformer, RasterizedSourceConfig, RasterizerConfig,\n    GeoJSONVectorSourceConfig, ClassConfig)\nfrom rastervision2.pipeline.file_system import json_to_file\nfrom rastervision2.pipeline.config import ConfigError\nfrom rastervision2.pipeline import rv_config\n\n\nclass TestRasterizedSource(unittest.TestCase):\n    def setUp(self):\n        self.crs_transformer = IdentityCRSTransformer()\n        self.extent = Box.make_square(0, 0, 10)\n        self.tmp_dir_obj = rv_config.get_tmp_dir()\n        self.tmp_dir = self.tmp_dir_obj.name\n        self.class_id = 0\n        self.background_class_id = 1\n        self.line_buffer = 1\n        self.class_config = ClassConfig(names=['a'])\n        self.uri = join(self.tmp_dir, 'tmp.json')\n\n    def tearDown(self):\n        self.tmp_dir_obj.cleanup()\n\n    def build_source(self, geojson, all_touched=False):\n        json_to_file(geojson, self.uri)\n\n        config = RasterizedSourceConfig(\n            vector_source=GeoJSONVectorSourceConfig(uri=self.uri, default_class_id=None),\n            rasterizer_config=RasterizerConfig(\n                background_class_id=self.background_class_id,\n                all_touched=all_touched))\n        source = config.build(self.class_config, self.crs_transformer, self.extent)\n        return source\n\n    def test_get_chip(self):\n        geojson = {\n            'type':\n            'FeatureCollection',\n            'features': [{\n                'type': 'Feature',\n                'geometry': {\n                    'type':\n                    'Polygon',\n                    'coordinates': [[[0., 0.], [0., 5.], [5., 5.], [5., 0.],\n                                     [0., 0.]]]\n                },\n                'properties': {\n                    'class_id': self.class_id,\n                }\n            }, {\n                'type': 'Feature',\n                'geometry': {\n                    'type': 'LineString',\n                    'coordinates': [[7., 0.], [7., 9.]]\n                },\n                'properties': {\n                    'class_id': self.class_id\n                }\n            }]\n        }\n\n        source = self.build_source(geojson)\n        with source.activate():\n            self.assertEqual(source.get_extent(), self.extent)\n            chip = source.get_image_array()\n            self.assertEqual(chip.shape, (10, 10, 1))\n\n            expected_chip = self.background_class_id * np.ones((10, 10, 1))\n            expected_chip[0:5, 0:5, 0] = self.class_id\n            expected_chip[0:10, 6:8] = self.class_id\n            np.testing.assert_array_equal(chip, expected_chip)\n\n    def test_get_chip_no_polygons(self):\n        geojson = {'type': 'FeatureCollection', 'features': []}\n\n        source = self.build_source(geojson)\n        with source.activate():\n            # Get chip that partially overlaps extent. Expect that chip has zeros\n            # outside of extent, and background_class_id otherwise.\n            self.assertEqual(source.get_extent(), self.extent)\n            chip = source.get_chip(Box.make_square(5, 5, 10))\n            self.assertEqual(chip.shape, (10, 10, 1))\n\n            expected_chip = np.zeros((10, 10, 1))\n            expected_chip[0:5, 0:5, :] = self.background_class_id\n\n            np.testing.assert_array_equal(chip, expected_chip)\n\n    def test_get_chip_all_touched(self):\n        geojson = {\n            'type':\n            'FeatureCollection',\n            'features': [{\n                'type': 'Feature',\n                'geometry': {\n                    'type':\n                    'Polygon',\n                    'coordinates': [[[0., 0.], [0., 0.4], [0.4, 0.4],\n                                     [0.4, 0.], [0., 0.]]]\n                },\n                'properties': {\n                    'class_id': self.class_id,\n                }\n            }]\n        }\n\n        false_source = self.build_source(geojson, all_touched=False)\n        true_source = self.build_source(geojson, all_touched=True)\n        with false_source.activate():\n            chip = false_source.get_image_array()\n            expected_chip = self.background_class_id * np.ones((10, 10, 1))\n            np.testing.assert_array_equal(chip, expected_chip)\n\n        with true_source.activate():\n            chip = true_source.get_image_array()\n            expected_chip = self.background_class_id * np.ones((10, 10, 1))\n            expected_chip[0:1, 0:1, 0] = self.class_id\n            np.testing.assert_array_equal(chip, expected_chip)\n\n    def test_using_null_class_bufs(self):\n        vs = GeoJSONVectorSourceConfig(\n            uri=self.uri,\n            default_class_id=None,\n            line_bufs={0: None})\n        with self.assertRaises(ConfigError):\n            config = RasterizedSourceConfig(\n                vector_source=vs,\n                rasterizer_config=RasterizerConfig(\n                    background_class_id=self.background_class_id))\n            config.validate_config()\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests_v2/core/data/raster_transformer/__init__.py,0,b''
tests_v2/core/data/raster_transformer/test_raster_transformer.py,0,"b""import unittest\nimport os\n\nimport numpy as np\n\nfrom rastervision2.core.data import RasterStats, StatsTransformerConfig\nfrom rastervision2.pipeline import rv_config\n\n\nclass TestRasterTransformer(unittest.TestCase):\n    def test_stats_transformer(self):\n        raster_stats = RasterStats()\n        raster_stats.means = list(np.ones((4, )))\n        raster_stats.stds = list(np.ones((4, )) * 2)\n\n        with rv_config.get_tmp_dir() as tmp_dir:\n            stats_uri = os.path.join(tmp_dir, 'stats.json')\n            raster_stats.save(stats_uri)\n\n            # All values have z-score of 1, which translates to\n            # uint8 value of 170.\n            transformer = StatsTransformerConfig(stats_uri=stats_uri).build()\n            chip = np.ones((2, 2, 4)) * 3\n            out_chip = transformer.transform(chip)\n            expected_out_chip = np.ones((2, 2, 4)) * 170\n            np.testing.assert_equal(out_chip, expected_out_chip)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests_v2/core/data/vector_source/__init__.py,0,b''
tests_v2/core/data/vector_source/test_geojson_vector_source.py,0,"b'import unittest\nimport os\n\nfrom shapely.geometry import shape\n\nfrom rastervision2.core.data import (\n    GeoJSONVectorSourceConfig, ClassConfig, IdentityCRSTransformer)\nfrom rastervision2.pipeline.file_system import json_to_file\nfrom rastervision2.pipeline import rv_config\n\nfrom tests_v2.core.data.mock_crs_transformer import DoubleCRSTransformer\n\n\nclass TestGeoJSONVectorSource(unittest.TestCase):\n    """"""This also indirectly tests the ClassInference class.""""""\n\n    def setUp(self):\n        self.tmp_dir = rv_config.get_tmp_dir()\n        self.uri = os.path.join(self.tmp_dir.name, \'vectors.json\')\n\n    def tearDown(self):\n        self.tmp_dir.cleanup()\n\n    def _test_class_inf(self, props, exp_class_ids, default_class_id=None):\n        geojson = {\n            \'type\':\n            \'FeatureCollection\',\n            \'features\': [{\n                \'properties\': props,\n                \'geometry\': {\n                    \'type\': \'Point\',\n                    \'coordinates\': [1, 1]\n                }\n            }]\n        }\n        json_to_file(geojson, self.uri)\n\n        class_config = ClassConfig(names=[\'building\', \'car\', \'tree\'])\n        class_id_to_filter = {\n            0: [\'==\', \'type\', \'building\'],\n            1: [\'any\', [\'==\', \'type\', \'car\'], [\'==\', \'type\', \'auto\']]\n        }\n        vs_cfg = GeoJSONVectorSourceConfig(\n            uri=self.uri,\n            class_id_to_filter=class_id_to_filter,\n            default_class_id=default_class_id)\n        vs = vs_cfg.build(class_config, IdentityCRSTransformer())\n        trans_geojson = vs.get_geojson()\n        class_ids = [\n            f[\'properties\'][\'class_id\'] for f in trans_geojson[\'features\']\n        ]\n        self.assertEqual(class_ids, exp_class_ids)\n\n    def test_class_inf_class_id(self):\n        self._test_class_inf({\'class_id\': 2}, [2])\n\n    def test_class_inf_label(self):\n        self._test_class_inf({\'label\': \'car\'}, [1])\n\n    def test_class_inf_filter(self):\n        self._test_class_inf({\'type\': \'auto\'}, [1])\n\n    def test_class_inf_default(self):\n        self._test_class_inf({}, [3], default_class_id=3)\n\n    def test_class_inf_no_default(self):\n        self._test_class_inf({}, [])\n\n    def geom_to_geojson(self, geom):\n        return {\'type\': \'FeatureCollection\', \'features\': [{\'geometry\': geom}]}\n\n    def transform_geojson(self,\n                          geojson,\n                          line_bufs=None,\n                          point_bufs=None,\n                          crs_transformer=None,\n                          to_map_coords=False):\n        if crs_transformer is None:\n            crs_transformer = IdentityCRSTransformer()\n        class_config = ClassConfig(names=[\'building\'])\n        json_to_file(geojson, self.uri)\n        cfg = GeoJSONVectorSourceConfig(\n            uri=self.uri,\n            line_bufs=line_bufs,\n            point_bufs=point_bufs,\n            default_class_id=0)\n        source = cfg.build(class_config, crs_transformer)\n        return source.get_geojson(to_map_coords=to_map_coords)\n\n    def test_transform_geojson_no_coords(self):\n        geom = {\'type\': \'Point\', \'coordinates\': []}\n        geojson = self.geom_to_geojson(geom)\n        trans_geojson = self.transform_geojson(geojson)\n\n        self.assertEqual(0, len(trans_geojson[\'features\']))\n\n    def test_transform_geojson_geom_coll(self):\n        geom = {\n            \'type\':\n            \'GeometryCollection\',\n            \'geometries\': [{\n                \'type\': \'MultiPoint\',\n                \'coordinates\': [[10, 10], [20, 20]]\n            }]\n        }\n        geojson = self.geom_to_geojson(geom)\n        trans_geojson = self.transform_geojson(geojson)\n\n        feats = trans_geojson[\'features\']\n        self.assertEqual(len(feats), 2)\n        self.assertEqual(feats[0][\'geometry\'][\'type\'], \'Polygon\')\n        self.assertEqual(feats[1][\'geometry\'][\'type\'], \'Polygon\')\n\n    def test_transform_geojson_multi(self):\n        geom = {\'type\': \'MultiPoint\', \'coordinates\': [[10, 10], [20, 20]]}\n        geojson = self.geom_to_geojson(geom)\n        trans_geojson = self.transform_geojson(geojson)\n\n        feats = trans_geojson[\'features\']\n        self.assertEqual(len(feats), 2)\n        self.assertEqual(feats[0][\'geometry\'][\'type\'], \'Polygon\')\n        self.assertEqual(feats[1][\'geometry\'][\'type\'], \'Polygon\')\n\n    def test_transform_geojson_line_buf(self):\n        geom = {\'type\': \'LineString\', \'coordinates\': [[10, 10], [10, 20]]}\n        geojson = self.geom_to_geojson(geom)\n\n        trans_geojson = self.transform_geojson(geojson, line_bufs={0: 5.0})\n        trans_geom = trans_geojson[\'features\'][0][\'geometry\']\n        self.assertTrue(shape(geom).buffer(5.0).equals(shape(trans_geom)))\n\n        trans_geojson = self.transform_geojson(geojson, line_bufs={1: 5.0})\n        trans_geom = trans_geojson[\'features\'][0][\'geometry\']\n        self.assertTrue(shape(geom).buffer(1.0).equals(shape(trans_geom)))\n\n        trans_geojson = self.transform_geojson(geojson, line_bufs={0: None})\n        trans_geom = trans_geojson[\'features\'][0][\'geometry\']\n        self.assertTrue(shape(geom).equals(shape(trans_geom)))\n\n    def test_transform_point_buf(self):\n        geom = {\'type\': \'Point\', \'coordinates\': [10, 10]}\n        geojson = self.geom_to_geojson(geom)\n\n        trans_geojson = self.transform_geojson(geojson, point_bufs={0: 5.0})\n        trans_geom = trans_geojson[\'features\'][0][\'geometry\']\n        self.assertTrue(shape(geom).buffer(5.0).equals(shape(trans_geom)))\n\n        trans_geojson = self.transform_geojson(geojson, point_bufs={1: 5.0})\n        trans_geom = trans_geojson[\'features\'][0][\'geometry\']\n        self.assertTrue(shape(geom).buffer(1.0).equals(shape(trans_geom)))\n\n        trans_geojson = self.transform_geojson(geojson, point_bufs={0: None})\n        trans_geom = trans_geojson[\'features\'][0][\'geometry\']\n        self.assertTrue(shape(geom).equals(shape(trans_geom)))\n\n    def test_transform_polygon(self):\n        geom = {\n            \'type\': \'Polygon\',\n            \'coordinates\': [[[0, 0], [0, 10], [10, 10], [10, 0], [0, 0]]]\n        }\n        geojson = self.geom_to_geojson(geom)\n\n        trans_geojson = self.transform_geojson(geojson)\n        trans_geom = trans_geojson[\'features\'][0][\'geometry\']\n        self.assertTrue(shape(geom).equals(shape(trans_geom)))\n\n        trans_geojson = self.transform_geojson(\n            geojson, crs_transformer=DoubleCRSTransformer())\n        trans_geom = trans_geojson[\'features\'][0][\'geometry\']\n        exp_geom = {\n            \'type\': \'Polygon\',\n            \'coordinates\': [[[0, 0], [0, 20], [20, 20], [20, 0], [0, 0]]]\n        }\n        self.assertTrue(shape(exp_geom).equals(shape(trans_geom)))\n\n        trans_geojson = self.transform_geojson(\n            geojson,\n            crs_transformer=DoubleCRSTransformer(),\n            to_map_coords=True)\n        trans_geom = trans_geojson[\'features\'][0][\'geometry\']\n        self.assertTrue(shape(geom).equals(shape(trans_geom)))\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
rastervision2/core/data/label/tfod_utils/__init__.py,0,b'# flake8: noqa\n'
rastervision2/core/data/label/tfod_utils/np_box_list.py,0,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Numpy BoxList classes and functions.""""""\n\nimport numpy as np\n\n\nclass BoxList(object):\n    """"""Box collection.\n  BoxList represents a list of bounding boxes as numpy array, where each\n  bounding box is represented as a row of 4 numbers,\n  [y_min, x_min, y_max, x_max].  It is assumed that all bounding boxes within a\n  given list correspond to a single image.\n  Optionally, users can add additional related fields (such as\n  objectness/classification scores).\n  """"""\n\n    def __init__(self, data):\n        """"""Constructs box collection.\n    Args:\n      data: a numpy array of shape [N, 4] representing box coordinates\n    Raises:\n      ValueError: if bbox data is not a numpy array\n      ValueError: if invalid dimensions for bbox data\n    """"""\n        if not isinstance(data, np.ndarray):\n            raise ValueError(\'data must be a numpy array.\')\n        if len(data.shape) != 2 or data.shape[1] != 4:\n            raise ValueError(\'Invalid dimensions for box data.\')\n        if data.dtype != np.float32 and data.dtype != np.float64:\n            raise ValueError(\n                \'Invalid data type for box data: float is required.\')\n        if not self._is_valid_boxes(data):\n            raise ValueError(\'Invalid box data. data must be a numpy array of \'\n                             \'N*[y_min, x_min, y_max, x_max]\')\n        self.data = {\'boxes\': data}\n\n    def num_boxes(self):\n        """"""Return number of boxes held in collections.""""""\n        return self.data[\'boxes\'].shape[0]\n\n    def get_extra_fields(self):\n        """"""Return all non-box fields.""""""\n        return [k for k in self.data.keys() if k != \'boxes\']\n\n    def has_field(self, field):\n        return field in self.data\n\n    def add_field(self, field, field_data):\n        """"""Add data to a specified field.\n    Args:\n      field: a string parameter used to speficy a related field to be accessed.\n      field_data: a numpy array of [N, ...] representing the data associated\n          with the field.\n    Raises:\n      ValueError: if the field is already exist or the dimension of the field\n          data does not matches the number of boxes.\n    """"""\n        if self.has_field(field):\n            raise ValueError(\'Field \' + field + \'already exists\')\n        if len(field_data.shape) < 1 or field_data.shape[0] != self.num_boxes(\n        ):\n            raise ValueError(\'Invalid dimensions for field data\')\n        self.data[field] = field_data\n\n    def get(self):\n        """"""Convenience function for accesssing box coordinates.\n    Returns:\n      a numpy array of shape [N, 4] representing box corners\n    """"""\n        return self.get_field(\'boxes\')\n\n    def get_field(self, field):\n        """"""Accesses data associated with the specified field in the box collection.\n    Args:\n      field: a string parameter used to speficy a related field to be accessed.\n    Returns:\n      a numpy 1-d array representing data of an associated field\n    Raises:\n      ValueError: if invalid field\n    """"""\n        if not self.has_field(field):\n            raise ValueError(\'field {} does not exist\'.format(field))\n        return self.data[field]\n\n    def get_coordinates(self):\n        """"""Get corner coordinates of boxes.\n    Returns:\n     a list of 4 1-d numpy arrays [y_min, x_min, y_max, x_max]\n    """"""\n        box_coordinates = self.get()\n        y_min = box_coordinates[:, 0]\n        x_min = box_coordinates[:, 1]\n        y_max = box_coordinates[:, 2]\n        x_max = box_coordinates[:, 3]\n        return [y_min, x_min, y_max, x_max]\n\n    def _is_valid_boxes(self, data):\n        """"""Check whether data fullfills the format of N*[ymin, xmin, ymax, xmin].\n    Args:\n      data: a numpy array of shape [N, 4] representing box coordinates\n    Returns:\n      a boolean indicating whether all ymax of boxes are equal or greater than\n          ymin, and all xmax of boxes are equal or greater than xmin.\n    """"""\n        if data.shape[0] > 0:\n            for i in range(data.shape[0]):\n                if data[i, 0] > data[i, 2] or data[i, 1] > data[i, 3]:\n                    return False\n        return True\n'"
rastervision2/core/data/label/tfod_utils/np_box_list_ops.py,0,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Bounding Box List operations for Numpy BoxLists.\nExample box operations that are supported:\n  * Areas: compute bounding box areas\n  * IOU: pairwise intersection-over-union scores\n""""""\nimport numpy as np\n\nfrom rastervision2.core.data.label.tfod_utils import np_box_list\nfrom rastervision2.core.data.label.tfod_utils import np_box_ops\n\n\nclass SortOrder(object):\n    """"""Enum class for sort order.\n  Attributes:\n    ascend: ascend order.\n    descend: descend order.\n  """"""\n    ASCEND = 1\n    DESCEND = 2\n\n\ndef area(boxlist):\n    """"""Computes area of boxes.\n  Args:\n    boxlist: BoxList holding N boxes\n  Returns:\n    a numpy array with shape [N*1] representing box areas\n  """"""\n    y_min, x_min, y_max, x_max = boxlist.get_coordinates()\n    return (y_max - y_min) * (x_max - x_min)\n\n\ndef intersection(boxlist1, boxlist2):\n    """"""Compute pairwise intersection areas between boxes.\n  Args:\n    boxlist1: BoxList holding N boxes\n    boxlist2: BoxList holding M boxes\n  Returns:\n    a numpy array with shape [N*M] representing pairwise intersection area\n  """"""\n    return np_box_ops.intersection(boxlist1.get(), boxlist2.get())\n\n\ndef iou(boxlist1, boxlist2):\n    """"""Computes pairwise intersection-over-union between box collections.\n  Args:\n    boxlist1: BoxList holding N boxes\n    boxlist2: BoxList holding M boxes\n  Returns:\n    a numpy array with shape [N, M] representing pairwise iou scores.\n  """"""\n    return np_box_ops.iou(boxlist1.get(), boxlist2.get())\n\n\ndef ioa(boxlist1, boxlist2):\n    """"""Computes pairwise intersection-over-area between box collections.\n  Intersection-over-area (ioa) between two boxes box1 and box2 is defined as\n  their intersection area over box2\'s area. Note that ioa is not symmetric,\n  that is, IOA(box1, box2) != IOA(box2, box1).\n  Args:\n    boxlist1: BoxList holding N boxes\n    boxlist2: BoxList holding M boxes\n  Returns:\n    a numpy array with shape [N, M] representing pairwise ioa scores.\n  """"""\n    return np_box_ops.ioa(boxlist1.get(), boxlist2.get())\n\n\ndef gather(boxlist, indices, fields=None):\n    """"""Gather boxes from BoxList according to indices and return new BoxList.\n  By default, gather returns boxes corresponding to the input index list, as\n  well as all additional fields stored in the boxlist (indexing into the\n  first dimension).  However one can optionally only gather from a\n  subset of fields.\n  Args:\n    boxlist: BoxList holding N boxes\n    indices: a 1-d numpy array of type int_\n    fields: (optional) list of fields to also gather from.  If None (default),\n        all fields are gathered from.  Pass an empty fields list to only gather\n        the box coordinates.\n  Returns:\n    subboxlist: a BoxList corresponding to the subset of the input BoxList\n        specified by indices\n  Raises:\n    ValueError: if specified field is not contained in boxlist or if the\n        indices are not of type int_\n  """"""\n    if indices.size:\n        if np.amax(indices) >= boxlist.num_boxes() or np.amin(indices) < 0:\n            raise ValueError(\'indices are out of valid range.\')\n    subboxlist = np_box_list.BoxList(boxlist.get()[indices, :])\n    if fields is None:\n        fields = boxlist.get_extra_fields()\n    for field in fields:\n        extra_field_data = boxlist.get_field(field)\n        subboxlist.add_field(field, extra_field_data[indices, ...])\n    return subboxlist\n\n\ndef sort_by_field(boxlist, field, order=SortOrder.DESCEND):\n    """"""Sort boxes and associated fields according to a scalar field.\n  A common use case is reordering the boxes according to descending scores.\n  Args:\n    boxlist: BoxList holding N boxes.\n    field: A BoxList field for sorting and reordering the BoxList.\n    order: (Optional) \'descend\' or \'ascend\'. Default is descend.\n  Returns:\n    sorted_boxlist: A sorted BoxList with the field in the specified order.\n  Raises:\n    ValueError: if specified field does not exist or is not of single dimension.\n    ValueError: if the order is not either descend or ascend.\n  """"""\n    if not boxlist.has_field(field):\n        raise ValueError(\'Field \' + field + \' does not exist\')\n    if len(boxlist.get_field(field).shape) != 1:\n        raise ValueError(\'Field \' + field + \'should be single dimension.\')\n    if order != SortOrder.DESCEND and order != SortOrder.ASCEND:\n        raise ValueError(\'Invalid sort order\')\n\n    field_to_sort = boxlist.get_field(field)\n    sorted_indices = np.argsort(field_to_sort)\n    if order == SortOrder.DESCEND:\n        sorted_indices = sorted_indices[::-1]\n    return gather(boxlist, sorted_indices)\n\n\ndef non_max_suppression(boxlist,\n                        max_output_size=10000,\n                        iou_threshold=1.0,\n                        score_threshold=-10.0):\n    """"""Non maximum suppression.\n  This op greedily selects a subset of detection bounding boxes, pruning\n  away boxes that have high IOU (intersection over union) overlap (> thresh)\n  with already selected boxes. In each iteration, the detected bounding box with\n  highest score in the available pool is selected.\n  Args:\n    boxlist: BoxList holding N boxes.  Must contain a \'scores\' field\n      representing detection scores. All scores belong to the same class.\n    max_output_size: maximum number of retained boxes\n    iou_threshold: intersection over union threshold.\n    score_threshold: minimum score threshold. Remove the boxes with scores\n                     less than this value. Default value is set to -10. A very\n                     low threshold to pass pretty much all the boxes, unless\n                     the user sets a different score threshold.\n  Returns:\n    a BoxList holding M boxes where M <= max_output_size\n  Raises:\n    ValueError: if \'scores\' field does not exist\n    ValueError: if threshold is not in [0, 1]\n    ValueError: if max_output_size < 0\n  """"""\n    if not boxlist.has_field(\'scores\'):\n        raise ValueError(\'Field scores does not exist\')\n    if iou_threshold < 0. or iou_threshold > 1.0:\n        raise ValueError(\'IOU threshold must be in [0, 1]\')\n    if max_output_size < 0:\n        raise ValueError(\'max_output_size must be bigger than 0.\')\n\n    boxlist = filter_scores_greater_than(boxlist, score_threshold)\n    if boxlist.num_boxes() == 0:\n        return boxlist\n\n    boxlist = sort_by_field(boxlist, \'scores\')\n\n    # Prevent further computation if NMS is disabled.\n    if iou_threshold == 1.0:\n        if boxlist.num_boxes() > max_output_size:\n            selected_indices = np.arange(max_output_size)\n            return gather(boxlist, selected_indices)\n        else:\n            return boxlist\n\n    boxes = boxlist.get()\n    num_boxes = boxlist.num_boxes()\n    # is_index_valid is True only for all remaining valid boxes,\n    is_index_valid = np.full(num_boxes, 1, dtype=bool)\n    selected_indices = []\n    num_output = 0\n    for i in range(num_boxes):\n        if num_output < max_output_size:\n            if is_index_valid[i]:\n                num_output += 1\n                selected_indices.append(i)\n                is_index_valid[i] = False\n                valid_indices = np.where(is_index_valid)[0]\n                if valid_indices.size == 0:\n                    break\n\n                intersect_over_union = np_box_ops.iou(\n                    np.expand_dims(boxes[i, :], axis=0),\n                    boxes[valid_indices, :])\n                intersect_over_union = np.squeeze(intersect_over_union, axis=0)\n                is_index_valid[valid_indices] = np.logical_and(\n                    is_index_valid[valid_indices],\n                    intersect_over_union <= iou_threshold)\n    return gather(boxlist, np.array(selected_indices))\n\n\ndef multi_class_non_max_suppression(boxlist, score_thresh, iou_thresh,\n                                    max_output_size):\n    """"""Multi-class version of non maximum suppression.\n  This op greedily selects a subset of detection bounding boxes, pruning\n  away boxes that have high IOU (intersection over union) overlap (> thresh)\n  with already selected boxes.  It operates independently for each class for\n  which scores are provided (via the scores field of the input box_list),\n  pruning boxes with score less than a provided threshold prior to\n  applying NMS.\n  Args:\n    boxlist: BoxList holding N boxes.  Must contain a \'scores\' field\n      representing detection scores.  This scores field is a tensor that can\n      be 1 dimensional (in the case of a single class) or 2-dimensional, which\n      which case we assume that it takes the shape [num_boxes, num_classes].\n      We further assume that this rank is known statically and that\n      scores.shape[1] is also known (i.e., the number of classes is fixed\n      and known at graph construction time).\n    score_thresh: scalar threshold for score (low scoring boxes are removed).\n    iou_thresh: scalar threshold for IOU (boxes that that high IOU overlap\n      with previously selected boxes are removed).\n    max_output_size: maximum number of retained boxes per class.\n  Returns:\n    a BoxList holding M boxes with a rank-1 scores field representing\n      corresponding scores for each box with scores sorted in decreasing order\n      and a rank-1 classes field representing a class label for each box.\n  Raises:\n    ValueError: if iou_thresh is not in [0, 1] or if input boxlist does not have\n      a valid scores field.\n  """"""\n    if not 0 <= iou_thresh <= 1.0:\n        raise ValueError(\'thresh must be between 0 and 1\')\n    if not isinstance(boxlist, np_box_list.BoxList):\n        raise ValueError(\'boxlist must be a BoxList\')\n    if not boxlist.has_field(\'scores\'):\n        raise ValueError(\'input boxlist must have \\\'scores\\\' field\')\n    scores = boxlist.get_field(\'scores\')\n    if len(scores.shape) == 1:\n        scores = np.reshape(scores, [-1, 1])\n    elif len(scores.shape) == 2:\n        if scores.shape[1] is None:\n            raise ValueError(\n                \'scores field must have statically defined second \'\n                \'dimension\')\n    else:\n        raise ValueError(\'scores field must be of rank 1 or 2\')\n    num_boxes = boxlist.num_boxes()\n    num_scores = scores.shape[0]\n    num_classes = scores.shape[1]\n\n    if num_boxes != num_scores:\n        raise ValueError(\'Incorrect scores field length: actual vs expected.\')\n\n    selected_boxes_list = []\n    for class_idx in range(num_classes):\n        boxlist_and_class_scores = np_box_list.BoxList(boxlist.get())\n        class_scores = np.reshape(scores[0:num_scores, class_idx], [-1])\n        boxlist_and_class_scores.add_field(\'scores\', class_scores)\n        boxlist_filt = filter_scores_greater_than(boxlist_and_class_scores,\n                                                  score_thresh)\n        nms_result = non_max_suppression(\n            boxlist_filt,\n            max_output_size=max_output_size,\n            iou_threshold=iou_thresh,\n            score_threshold=score_thresh)\n        nms_result.add_field(\n            \'classes\',\n            np.zeros_like(nms_result.get_field(\'scores\')) + class_idx)\n        selected_boxes_list.append(nms_result)\n    selected_boxes = concatenate(selected_boxes_list)\n    sorted_boxes = sort_by_field(selected_boxes, \'scores\')\n    return sorted_boxes\n\n\ndef scale(boxlist, y_scale, x_scale):\n    """"""Scale box coordinates in x and y dimensions.\n  Args:\n    boxlist: BoxList holding N boxes\n    y_scale: float\n    x_scale: float\n  Returns:\n    boxlist: BoxList holding N boxes\n  """"""\n    y_min, x_min, y_max, x_max = np.array_split(boxlist.get(), 4, axis=1)\n    y_min = y_scale * y_min\n    y_max = y_scale * y_max\n    x_min = x_scale * x_min\n    x_max = x_scale * x_max\n    scaled_boxlist = np_box_list.BoxList(\n        np.hstack([y_min, x_min, y_max, x_max]))\n\n    fields = boxlist.get_extra_fields()\n    for field in fields:\n        extra_field_data = boxlist.get_field(field)\n        scaled_boxlist.add_field(field, extra_field_data)\n\n    return scaled_boxlist\n\n\ndef clip_to_window(boxlist, window):\n    """"""Clip bounding boxes to a window.\n  This op clips input bounding boxes (represented by bounding box\n  corners) to a window, optionally filtering out boxes that do not\n  overlap at all with the window.\n  Args:\n    boxlist: BoxList holding M_in boxes\n    window: a numpy array of shape [4] representing the\n            [y_min, x_min, y_max, x_max] window to which the op\n            should clip boxes.\n  Returns:\n    a BoxList holding M_out boxes where M_out <= M_in\n  """"""\n    y_min, x_min, y_max, x_max = np.array_split(boxlist.get(), 4, axis=1)\n    win_y_min = window[0]\n    win_x_min = window[1]\n    win_y_max = window[2]\n    win_x_max = window[3]\n    y_min_clipped = np.fmax(np.fmin(y_min, win_y_max), win_y_min)\n    y_max_clipped = np.fmax(np.fmin(y_max, win_y_max), win_y_min)\n    x_min_clipped = np.fmax(np.fmin(x_min, win_x_max), win_x_min)\n    x_max_clipped = np.fmax(np.fmin(x_max, win_x_max), win_x_min)\n    clipped = np_box_list.BoxList(\n        np.hstack([y_min_clipped, x_min_clipped, y_max_clipped,\n                   x_max_clipped]))\n    clipped = _copy_extra_fields(clipped, boxlist)\n    areas = area(clipped)\n    nonzero_area_indices = np.reshape(\n        np.nonzero(np.greater(areas, 0.0)), [-1]).astype(np.int32)\n    return gather(clipped, nonzero_area_indices)\n\n\ndef prune_non_overlapping_boxes(boxlist1, boxlist2, minoverlap=0.0):\n    """"""Prunes the boxes in boxlist1 that overlap less than thresh with boxlist2.\n  For each box in boxlist1, we want its IOA to be more than minoverlap with\n  at least one of the boxes in boxlist2. If it does not, we remove it.\n  Args:\n    boxlist1: BoxList holding N boxes.\n    boxlist2: BoxList holding M boxes.\n    minoverlap: Minimum required overlap between boxes, to count them as\n                overlapping.\n  Returns:\n    A pruned boxlist with size [N\', 4].\n  """"""\n    intersection_over_area = ioa(boxlist2, boxlist1)  # [M, N] tensor\n    intersection_over_area = np.amax(\n        intersection_over_area, axis=0)  # [N] tensor\n    keep_bool = np.greater_equal(intersection_over_area, np.array(minoverlap))\n    keep_inds = np.nonzero(keep_bool)[0]\n    new_boxlist1 = gather(boxlist1, keep_inds)\n    return new_boxlist1\n\n\ndef prune_outside_window(boxlist, window):\n    """"""Prunes bounding boxes that fall outside a given window.\n  This function prunes bounding boxes that even partially fall outside the given\n  window. See also ClipToWindow which only prunes bounding boxes that fall\n  completely outside the window, and clips any bounding boxes that partially\n  overflow.\n  Args:\n    boxlist: a BoxList holding M_in boxes.\n    window: a numpy array of size 4, representing [ymin, xmin, ymax, xmax]\n            of the window.\n  Returns:\n    pruned_corners: a tensor with shape [M_out, 4] where M_out <= M_in.\n    valid_indices: a tensor with shape [M_out] indexing the valid bounding boxes\n     in the input tensor.\n  """"""\n\n    y_min, x_min, y_max, x_max = np.array_split(boxlist.get(), 4, axis=1)\n    win_y_min = window[0]\n    win_x_min = window[1]\n    win_y_max = window[2]\n    win_x_max = window[3]\n    coordinate_violations = np.hstack([\n        np.less(y_min, win_y_min),\n        np.less(x_min, win_x_min),\n        np.greater(y_max, win_y_max),\n        np.greater(x_max, win_x_max)\n    ])\n    valid_indices = np.reshape(\n        np.where(np.logical_not(np.max(coordinate_violations, axis=1))), [-1])\n    return gather(boxlist, valid_indices), valid_indices\n\n\ndef concatenate(boxlists, fields=None):\n    """"""Concatenate list of BoxLists.\n  This op concatenates a list of input BoxLists into a larger BoxList.  It also\n  handles concatenation of BoxList fields as long as the field tensor shapes\n  are equal except for the first dimension.\n  Args:\n    boxlists: list of BoxList objects\n    fields: optional list of fields to also concatenate.  By default, all\n      fields from the first BoxList in the list are included in the\n      concatenation.\n  Returns:\n    a BoxList with number of boxes equal to\n      sum([boxlist.num_boxes() for boxlist in BoxList])\n  Raises:\n    ValueError: if boxlists is invalid (i.e., is not a list, is empty, or\n      contains non BoxList objects), or if requested fields are not contained in\n      all boxlists\n  """"""\n    if not isinstance(boxlists, list):\n        raise ValueError(\'boxlists should be a list\')\n    if not boxlists:\n        raise ValueError(\'boxlists should have nonzero length\')\n    for boxlist in boxlists:\n        if not isinstance(boxlist, np_box_list.BoxList):\n            raise ValueError(\n                \'all elements of boxlists should be BoxList objects\')\n    concatenated = np_box_list.BoxList(\n        np.vstack([boxlist.get() for boxlist in boxlists]))\n    if fields is None:\n        fields = boxlists[0].get_extra_fields()\n    for field in fields:\n        first_field_shape = boxlists[0].get_field(field).shape\n        first_field_shape = first_field_shape[1:]\n        for boxlist in boxlists:\n            if not boxlist.has_field(field):\n                raise ValueError(\'boxlist must contain all requested fields\')\n            field_shape = boxlist.get_field(field).shape\n            field_shape = field_shape[1:]\n            if field_shape != first_field_shape:\n                raise ValueError(\n                    \'field %s must have same shape for all boxlists \'\n                    \'except for the 0th dimension.\' % field)\n        concatenated_field = np.concatenate(\n            [boxlist.get_field(field) for boxlist in boxlists], axis=0)\n        concatenated.add_field(field, concatenated_field)\n    return concatenated\n\n\ndef filter_scores_greater_than(boxlist, thresh):\n    """"""Filter to keep only boxes with score exceeding a given threshold.\n  This op keeps the collection of boxes whose corresponding scores are\n  greater than the input threshold.\n  Args:\n    boxlist: BoxList holding N boxes.  Must contain a \'scores\' field\n      representing detection scores.\n    thresh: scalar threshold\n  Returns:\n    a BoxList holding M boxes where M <= N\n  Raises:\n    ValueError: if boxlist not a BoxList object or if it does not\n      have a scores field\n  """"""\n    if not isinstance(boxlist, np_box_list.BoxList):\n        raise ValueError(\'boxlist must be a BoxList\')\n    if not boxlist.has_field(\'scores\'):\n        raise ValueError(\'input boxlist must have \\\'scores\\\' field\')\n    scores = boxlist.get_field(\'scores\')\n    if len(scores.shape) > 2:\n        raise ValueError(\'Scores should have rank 1 or 2\')\n    if len(scores.shape) == 2 and scores.shape[1] != 1:\n        raise ValueError(\'Scores should have rank 1 or have shape \'\n                         \'consistent with [None, 1]\')\n    high_score_indices = np.reshape(\n        np.where(np.greater(scores, thresh)), [-1]).astype(np.int32)\n    return gather(boxlist, high_score_indices)\n\n\ndef change_coordinate_frame(boxlist, window):\n    """"""Change coordinate frame of the boxlist to be relative to window\'s frame.\n  Given a window of the form [ymin, xmin, ymax, xmax],\n  changes bounding box coordinates from boxlist to be relative to this window\n  (e.g., the min corner maps to (0,0) and the max corner maps to (1,1)).\n  An example use case is data augmentation: where we are given groundtruth\n  boxes (boxlist) and would like to randomly crop the image to some\n  window (window). In this case we need to change the coordinate frame of\n  each groundtruth box to be relative to this new window.\n  Args:\n    boxlist: A BoxList object holding N boxes.\n    window: a size 4 1-D numpy array.\n  Returns:\n    Returns a BoxList object with N boxes.\n  """"""\n    win_height = window[2] - window[0]\n    win_width = window[3] - window[1]\n    boxlist_new = scale(\n        np_box_list.BoxList(boxlist.get() -\n                            [window[0], window[1], window[0], window[1]]),\n        1.0 / win_height, 1.0 / win_width)\n    _copy_extra_fields(boxlist_new, boxlist)\n\n    return boxlist_new\n\n\ndef _copy_extra_fields(boxlist_to_copy_to, boxlist_to_copy_from):\n    """"""Copies the extra fields of boxlist_to_copy_from to boxlist_to_copy_to.\n  Args:\n    boxlist_to_copy_to: BoxList to which extra fields are copied.\n    boxlist_to_copy_from: BoxList from which fields are copied.\n  Returns:\n    boxlist_to_copy_to with extra fields.\n  """"""\n    for field in boxlist_to_copy_from.get_extra_fields():\n        boxlist_to_copy_to.add_field(field,\n                                     boxlist_to_copy_from.get_field(field))\n    return boxlist_to_copy_to\n\n\ndef _update_valid_indices_by_removing_high_iou_boxes(\n        selected_indices, is_index_valid, intersect_over_union, threshold):\n    max_iou = np.max(intersect_over_union[:, selected_indices], axis=1)\n    return np.logical_and(is_index_valid, max_iou <= threshold)\n'"
rastervision2/core/data/label/tfod_utils/np_box_ops.py,0,"b'# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Operations for [N, 4] numpy arrays representing bounding boxes.\nExample box operations that are supported:\n  * Areas: compute bounding box areas\n  * IOU: pairwise intersection-over-union scores\n""""""\nimport numpy as np\n\n\ndef area(boxes):\n    """"""Computes area of boxes.\n  Args:\n    boxes: Numpy array with shape [N, 4] holding N boxes\n  Returns:\n    a numpy array with shape [N*1] representing box areas\n  """"""\n    return (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])\n\n\ndef intersection(boxes1, boxes2):\n    """"""Compute pairwise intersection areas between boxes.\n  Args:\n    boxes1: a numpy array with shape [N, 4] holding N boxes\n    boxes2: a numpy array with shape [M, 4] holding M boxes\n  Returns:\n    a numpy array with shape [N*M] representing pairwise intersection area\n  """"""\n    [y_min1, x_min1, y_max1, x_max1] = np.split(boxes1, 4, axis=1)\n    [y_min2, x_min2, y_max2, x_max2] = np.split(boxes2, 4, axis=1)\n\n    all_pairs_min_ymax = np.minimum(y_max1, np.transpose(y_max2))\n    all_pairs_max_ymin = np.maximum(y_min1, np.transpose(y_min2))\n    intersect_heights = np.maximum(\n        np.zeros(all_pairs_max_ymin.shape),\n        all_pairs_min_ymax - all_pairs_max_ymin)\n    all_pairs_min_xmax = np.minimum(x_max1, np.transpose(x_max2))\n    all_pairs_max_xmin = np.maximum(x_min1, np.transpose(x_min2))\n    intersect_widths = np.maximum(\n        np.zeros(all_pairs_max_xmin.shape),\n        all_pairs_min_xmax - all_pairs_max_xmin)\n    return intersect_heights * intersect_widths\n\n\ndef iou(boxes1, boxes2):\n    """"""Computes pairwise intersection-over-union between box collections.\n  Args:\n    boxes1: a numpy array with shape [N, 4] holding N boxes.\n    boxes2: a numpy array with shape [M, 4] holding N boxes.\n  Returns:\n    a numpy array with shape [N, M] representing pairwise iou scores.\n  """"""\n    intersect = intersection(boxes1, boxes2)\n    area1 = area(boxes1)\n    area2 = area(boxes2)\n    union = np.expand_dims(\n        area1, axis=1) + np.expand_dims(\n            area2, axis=0) - intersect\n    return intersect / union\n\n\ndef ioa(boxes1, boxes2):\n    """"""Computes pairwise intersection-over-area between box collections.\n  Intersection-over-area (ioa) between two boxes box1 and box2 is defined as\n  their intersection area over box2\'s area. Note that ioa is not symmetric,\n  that is, IOA(box1, box2) != IOA(box2, box1).\n  Args:\n    boxes1: a numpy array with shape [N, 4] holding N boxes.\n    boxes2: a numpy array with shape [M, 4] holding N boxes.\n  Returns:\n    a numpy array with shape [N, M] representing pairwise ioa scores.\n  """"""\n    intersect = intersection(boxes1, boxes2)\n    areas = np.expand_dims(area(boxes2), axis=0)\n    return intersect / areas\n'"
rastervision2/core/data/vector_source/label_maker/__init__.py,0,b''
rastervision2/core/data/vector_source/label_maker/filter.py,0,"b'# Copied from https://github.com/developmentseed/label-maker/blob/master/label_maker/filter.py\n# flake8: noqa\n\n# pylint: disable=eval-used,too-many-return-statements\n""""""Create a feature filtering function from a Mapbox GL Filter.""""""\n\n# Python port of https://github.com/mapbox/mapbox-gl-js/blob/c9900db279db776f493ce8b6749966cedc2d6b8a/src/style-spec/feature_filter/index.js\n\n\ndef create_filter(filt):\n    """"""Create a feature filtering function from a Mapbox GL Filter.\n\n    Given a filter expressed as nested lists, return a new function\n    that evaluates whether a given feature (with a .properties or .tags property)\n    passes its test. More information:\n    - https://www.mapbox.com/mapbox-gl-js/style-spec/#other-filter\n    - https://github.com/mapbox/mapbox-gl-js/tree/master/src/style-spec/feature_filter\n\n    Parameters\n    ------------\n    filt: list\n        Mapbox GL filter\n\n    Returns\n    --------\n    func: function\n        A function which evaluates whether a GeoJSON feature meets the input filter criteria\n    """"""\n\n    def func(f):\n        """"""evaluates whether a given feature passes its filter""""""\n        p = f.get(\'properties\', {}) if f else {}  # pylint: disable=unused-variable\n        return eval(_compile(filt))\n\n    return func\n\n\ndef _compile(filt):\n    """"""Return a string represented the compiled filter function""""""\n    if not filt:\n        return \'True\'\n    op = filt[0]\n    if len(filt) == 1:\n        return \'False\' if op == \'any\' else \'True\'\n    if op in [\'==\', \'!=\', \'<\', \'>\', \'<=\', \'>=\']:\n        return _compile_comparison_op(filt[1], filt[2], op)\n    elif op == \'any\':\n        return _compile_logical_op(filt[1:], \' or \')\n    elif op == \'all\':\n        return _compile_logical_op(filt[1:], \' and \')\n    elif op == \'none\':\n        return _compile_negation(_compile_logical_op(filt[1:], \' or \'))\n    elif op == \'in\':\n        return _compile_in_op(filt[1], filt[2:])\n    elif op == \'!in\':\n        return _compile_negation(_compile_in_op(filt[1], filt[2:]))\n    elif op == \'has\':\n        return _compile_has_op(filt[1])\n    elif op == \'!has\':\n        return _compile_negation(_compile_has_op(filt[1]))\n    return \'True\'\n\n\ndef _compile_property_reference(prop):\n    """"""Find the correct reference on the input feature""""""\n    if prop == \'$type\':\n        return \'f.get(""geometry"").get(""type"")\'\n    elif prop == \'$id\':\n        return \'f.get(""id"")\'\n    return \'p.get(""{}"")\'.format(prop)\n\n\ndef _compile_comparison_op(prop, value, op):\n    """"""Combine two values with a comparison operator""""""\n    left = _compile_property_reference(prop)\n    right = _stringify(value)\n    return left + op + right\n\n\ndef _compile_logical_op(expressions, op):\n    """"""Join multiple logical expressions""""""\n    return op.join(map(_compile, expressions))\n\n\ndef _compile_in_op(prop, values):\n    """"""Test if a property is within a list of values""""""\n    return \'{} in {}\'.format(_compile_property_reference(prop), values)\n\n\ndef _compile_has_op(prop):\n    """"""Test if a property exists on a feature""""""\n    return \'""id"" in f\' if prop == \'$id\' else \'{} in p\'.format(_stringify(prop))\n\n\ndef _compile_negation(expression):\n    """"""Negate the input expression""""""\n    return \'not ({})\'.format(expression)\n\n\ndef _stringify(s):\n    """"""Convert input to string, wrap with quotes if already a string""""""\n    return \'""{}""\'.format(s) if isinstance(s, str) else str(s)\n'"
rastervision2/examples/cookiecutter_template/{{cookiecutter.project_name}}/configs/__init__.py,0,b''
rastervision2/examples/cookiecutter_template/{{cookiecutter.project_name}}/configs/test.py,0,"b""from rastervision2.{{cookiecutter.project_name}}.test_pipeline_config import (\n    TestPipelineConfig)\n\n\ndef get_config(runner, root_uri='/opt/data/test-pipeline', message='hello world'):\n    return TestPipelineConfig(root_uri=root_uri, message=message)\n"""
rastervision2/examples/cookiecutter_template/{{cookiecutter.project_name}}/rastervision2/{{cookiecutter.project_name}}/__init__.py,0,b'# flake8: noqa\nimport rastervision2.pipeline\nimport rastervision2.{{cookiecutter.project_name}}.test_pipeline_config\n\ndef register_plugin(registry):\n    pass\n'
rastervision2/examples/cookiecutter_template/{{cookiecutter.project_name}}/rastervision2/{{cookiecutter.project_name}}/test_pipeline.py,0,"b""from typing import List\n\nfrom rastervision2.pipeline.pipeline import Pipeline\n\n\nclass TestPipeline(Pipeline):\n    commands: List[str] = ['print_msg']\n\n    def print_msg(self):\n        print(self.config.message)\n"""
rastervision2/examples/cookiecutter_template/{{cookiecutter.project_name}}/rastervision2/{{cookiecutter.project_name}}/test_pipeline_config.py,0,"b""from rastervision2.pipeline.pipeline_config import PipelineConfig\nfrom rastervision2.pipeline.config import register_config\n\n\n@register_config('test_pipeline_config')\nclass TestPipelineConfig(PipelineConfig):\n    message: str = 'hello'\n\n    def build(self, tmp_dir):\n        from rastervision2.{{cookiecutter.project_name}}.test_pipeline import TestPipeline\n        return TestPipeline(self, tmp_dir)\n"""
