file_path,api_count,code
coordinates.py,0,"b""import numpy as np\n\n\ndef fit_line(p1, p2):\n    # fit a line ax+by+c = 0\n    if p1[0] == p1[1]:\n        return [1., 0., -p1[0]]\n    else:\n        [k, b] = np.polyfit(p1, p2, deg=1)\n        return [k, -1., b]\n\n\ndef line_cross_point(line1, line2):\n    # line1 0= ax+by+c, compute the cross point of line1 and line2\n    if line1[0] != 0 and line1[0] == line2[0]:\n        print('Cross point does not exist')\n        return None\n    if line1[0] == 0 and line2[0] == 0:\n        print('Cross point does not exist')\n        return None\n    if line1[1] == 0:\n        x = -line1[2]\n        y = line2[0] * x + line2[2]\n    elif line2[1] == 0:\n        x = -line2[2]\n        y = line1[0] * x + line1[2]\n    else:\n        k1, _, b1 = line1\n        k2, _, b2 = line2\n        x = -(b1-b2)/(k1-k2)\n        y = k1*x + b1\n    return np.array([x, y], dtype=np.float32)\n\n\ndef line_verticle(line, point):\n    # get the verticle line from line across point\n    if line[1] == 0:\n        verticle = [0, -1, point[1]]\n    else:\n        if line[0] == 0:\n            verticle = [1, 0, -point[0]]\n        else:\n            verticle = [-1./line[0], -1, point[1] - (-1/line[0] * point[0])]\n    return verticle\n\n\n\ndef rectangle_from_parallelogram(poly):\n    '''\n    fit a rectangle from a parallelogram\n    :param poly:\n    :return:\n    '''\n    p0, p1, p2, p3 = poly\n    angle_p0 = np.arccos(np.dot(p1-p0, p3-p0)/(np.linalg.norm(p0-p1) * np.linalg.norm(p3-p0)))\n    if angle_p0 < 0.5 * np.pi:\n        if np.linalg.norm(p0 - p1) > np.linalg.norm(p0-p3):\n            # p0 and p2\n            ## p0\n            p2p3 = fit_line([p2[0], p3[0]], [p2[1], p3[1]])\n            p2p3_verticle = line_verticle(p2p3, p0)\n\n            new_p3 = line_cross_point(p2p3, p2p3_verticle)\n            ## p2\n            p0p1 = fit_line([p0[0], p1[0]], [p0[1], p1[1]])\n            p0p1_verticle = line_verticle(p0p1, p2)\n\n            new_p1 = line_cross_point(p0p1, p0p1_verticle)\n            return np.array([p0, new_p1, p2, new_p3], dtype=np.float32)\n        else:\n            p1p2 = fit_line([p1[0], p2[0]], [p1[1], p2[1]])\n            p1p2_verticle = line_verticle(p1p2, p0)\n\n            new_p1 = line_cross_point(p1p2, p1p2_verticle)\n            p0p3 = fit_line([p0[0], p3[0]], [p0[1], p3[1]])\n            p0p3_verticle = line_verticle(p0p3, p2)\n\n            new_p3 = line_cross_point(p0p3, p0p3_verticle)\n            return np.array([p0, new_p1, p2, new_p3], dtype=np.float32)\n    else:\n        if np.linalg.norm(p0-p1) > np.linalg.norm(p0-p3):\n            # p1 and p3\n            ## p1\n            p2p3 = fit_line([p2[0], p3[0]], [p2[1], p3[1]])\n            p2p3_verticle = line_verticle(p2p3, p1)\n\n            new_p2 = line_cross_point(p2p3, p2p3_verticle)\n            ## p3\n            p0p1 = fit_line([p0[0], p1[0]], [p0[1], p1[1]])\n            p0p1_verticle = line_verticle(p0p1, p3)\n\n            new_p0 = line_cross_point(p0p1, p0p1_verticle)\n            return np.array([new_p0, p1, new_p2, p3], dtype=np.float32)\n        else:\n            p0p3 = fit_line([p0[0], p3[0]], [p0[1], p3[1]])\n            p0p3_verticle = line_verticle(p0p3, p1)\n\n            new_p0 = line_cross_point(p0p3, p0p3_verticle)\n            p1p2 = fit_line([p1[0], p2[0]], [p1[1], p2[1]])\n            p1p2_verticle = line_verticle(p1p2, p3)\n\n            new_p2 = line_cross_point(p1p2, p1p2_verticle)\n            return np.array([new_p0, p1, new_p2, p3], dtype=np.float32)\n\n\ndef sort_rectangle(poly):\n    # sort the four coordinates of the polygon, points in poly should be sorted clockwise\n    # First find the lowest point\n    p_lowest = np.argmax(poly[:, 1])\n    if np.count_nonzero(poly[:, 1] == poly[p_lowest, 1]) == 2:\n        # \xe5\xba\x95\xe8\xbe\xb9\xe5\xb9\xb3\xe8\xa1\x8c\xe4\xba\x8eX\xe8\xbd\xb4, \xe9\x82\xa3\xe4\xb9\x88p0\xe4\xb8\xba\xe5\xb7\xa6\xe4\xb8\x8a\xe8\xa7\x92 - if the bottom line is parallel to x-axis, then p0 must be the upper-left corner\n        p0_index = np.argmin(np.sum(poly, axis=1))\n        p1_index = (p0_index + 1) % 4\n        p2_index = (p0_index + 2) % 4\n        p3_index = (p0_index + 3) % 4\n        return poly[[p0_index, p1_index, p2_index, p3_index]], 0.\n    else:\n        # \xe6\x89\xbe\xe5\x88\xb0\xe6\x9c\x80\xe4\xbd\x8e\xe7\x82\xb9\xe5\x8f\xb3\xe8\xbe\xb9\xe7\x9a\x84\xe7\x82\xb9 - find the point that sits right to the lowest point\n        p_lowest_right = (p_lowest - 1) % 4\n        p_lowest_left = (p_lowest + 1) % 4\n        angle = np.arctan(-(poly[p_lowest][1] - poly[p_lowest_right][1])/(poly[p_lowest][0] - poly[p_lowest_right][0]))\n        # assert angle > 0\n        # if angle <= 0:\n        #     print(angle, poly[p_lowest], poly[p_lowest_right])\n        if angle/np.pi * 180 > 45:\n            # \xe8\xbf\x99\xe4\xb8\xaa\xe7\x82\xb9\xe4\xb8\xbap2 - this point is p2\n            p2_index = p_lowest\n            p1_index = (p2_index - 1) % 4\n            p0_index = (p2_index - 2) % 4\n            p3_index = (p2_index + 1) % 4\n            return poly[[p0_index, p1_index, p2_index, p3_index]], -(np.pi/2 - angle)\n        else:\n            # \xe8\xbf\x99\xe4\xb8\xaa\xe7\x82\xb9\xe4\xb8\xbap3 - this point is p3\n            p3_index = p_lowest\n            p0_index = (p3_index + 1) % 4\n            p1_index = (p3_index + 2) % 4\n            p2_index = (p3_index + 3) % 4\n            return poly[[p0_index, p1_index, p2_index, p3_index]], angle"""
craft.py,9,"b'""""""  \nCopyright (c) 2019-present NAVER Corp.\nMIT License\n""""""\n\n# -*- coding: utf-8 -*-\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom collections import OrderedDict\nimport torch.nn.init as init\nfrom torchutil import *\n\nfrom basenet.vgg16_bn import vgg16_bn\n\n\nclass double_conv(nn.Module):\n    def __init__(self, in_ch, mid_ch, out_ch):\n        super(double_conv, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_ch + mid_ch, mid_ch, kernel_size=1),\n            nn.BatchNorm2d(mid_ch),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(mid_ch, out_ch, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        x = self.conv(x)\n        return x\n\n\nclass CRAFT(nn.Module):\n    def __init__(self, pretrained=True, freeze=False):\n        super(CRAFT, self).__init__()\n\n        """""" Base network """"""\n        # self.net = vgg16_bn(pretrained, freeze)\n        # self.net.load_state_dict(copyStateDict(torch.load(\'vgg16_bn-6c64b313.pth\')))\n        # self.basenet = self.net\n        self.basenet = vgg16_bn(pretrained, freeze)\n        """""" U network """"""\n        self.upconv1 = double_conv(1024, 512, 256)\n        self.upconv2 = double_conv(512, 256, 128)\n        self.upconv3 = double_conv(256, 128, 64)\n        self.upconv4 = double_conv(128, 64, 32)\n\n        num_class = 2\n        self.conv_cls = nn.Sequential(\n            nn.Conv2d(32, 32, kernel_size=3, padding=1), nn.ReLU(inplace=True),\n            nn.Conv2d(32, 32, kernel_size=3, padding=1), nn.ReLU(inplace=True),\n            nn.Conv2d(32, 16, kernel_size=3, padding=1), nn.ReLU(inplace=True),\n            nn.Conv2d(16, 16, kernel_size=1), nn.ReLU(inplace=True),\n            nn.Conv2d(16, num_class, kernel_size=1),\n        )\n\n        init_weights(self.upconv1.modules())\n        init_weights(self.upconv2.modules())\n        init_weights(self.upconv3.modules())\n        init_weights(self.upconv4.modules())\n        init_weights(self.conv_cls.modules())\n\n    def forward(self, x):\n        """""" Base network """"""\n        sources = self.basenet(x)\n\n        """""" U network """"""\n        y = torch.cat([sources[0], sources[1]], dim=1)\n        y = self.upconv1(y)\n\n        y = F.interpolate(y, size=sources[2].size()[2:], mode=\'bilinear\', align_corners=False)\n        y = torch.cat([y, sources[2]], dim=1)\n        y = self.upconv2(y)\n\n        y = F.interpolate(y, size=sources[3].size()[2:], mode=\'bilinear\', align_corners=False)\n        y = torch.cat([y, sources[3]], dim=1)\n        y = self.upconv3(y)\n\n        y = F.interpolate(y, size=sources[4].size()[2:], mode=\'bilinear\', align_corners=False)\n        y = torch.cat([y, sources[4]], dim=1)\n        feature = self.upconv4(y)\n\n        y = self.conv_cls(feature)\n\n        return y.permute(0, 2, 3, 1), feature\n\n\nif __name__ == \'__main__\':\n    model = CRAFT(pretrained=True).cuda()\n    output, _ = model(torch.randn(1, 3, 768, 768).cuda())\n    print(output.shape)\n'"
craft_utils.py,0,"b'""""""  \nCopyright (c) 2019-present NAVER Corp.\nMIT License\n""""""\n\n# -*- coding: utf-8 -*-\nimport numpy as np\nimport cv2\nimport math\n\n"""""" auxilary functions """"""\n# unwarp corodinates\ndef warpCoord(Minv, pt):\n    out = np.matmul(Minv, (pt[0], pt[1], 1))\n    return np.array([out[0]/out[2], out[1]/out[2]])\n"""""" end of auxilary functions """"""\n\n\ndef getDetBoxes_core(textmap, linkmap, text_threshold, link_threshold, low_text):\n    # prepare data\n    linkmap = linkmap.copy()\n    textmap = textmap.copy()\n    img_h, img_w = textmap.shape\n\n    """""" labeling method """"""\n    ret, text_score = cv2.threshold(textmap, low_text, 1, 0)\n    ret, link_score = cv2.threshold(linkmap, link_threshold, 1, 0)\n\n    text_score_comb = np.clip(text_score + link_score, 0, 1)\n    nLabels, labels, stats, centroids = cv2.connectedComponentsWithStats(text_score_comb.astype(np.uint8), connectivity=4)\n\n    det = []\n    mapper = []\n    for k in range(1,nLabels):\n        # size filtering\n        size = stats[k, cv2.CC_STAT_AREA]\n        if size < 10: continue\n\n        # thresholding\n        if np.max(textmap[labels==k]) < text_threshold: continue\n\n        # make segmentation map\n        segmap = np.zeros(textmap.shape, dtype=np.uint8)\n        segmap[labels==k] = 255\n        segmap[np.logical_and(link_score==1, text_score==0)] = 0   # remove link area\n        x, y = stats[k, cv2.CC_STAT_LEFT], stats[k, cv2.CC_STAT_TOP]\n        w, h = stats[k, cv2.CC_STAT_WIDTH], stats[k, cv2.CC_STAT_HEIGHT]\n        niter = int(math.sqrt(size * min(w, h) / (w * h)) * 2)\n        sx, ex, sy, ey = x - niter, x + w + niter + 1, y - niter, y + h + niter + 1\n        # boundary check\n        if sx < 0 : sx = 0\n        if sy < 0 : sy = 0\n        if ex >= img_w: ex = img_w\n        if ey >= img_h: ey = img_h\n        kernel = cv2.getStructuringElement(cv2.MORPH_RECT,(1 + niter, 1 + niter))\n        segmap[sy:ey, sx:ex] = cv2.dilate(segmap[sy:ey, sx:ex], kernel, iterations=1)\n        #kernel1 = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 5))\n        #segmap[sy:ey, sx:ex] = cv2.dilate(segmap[sy:ey, sx:ex], kernel1, iterations=1)\n\n\n        # make box\n        np_contours = np.roll(np.array(np.where(segmap!=0)),1,axis=0).transpose().reshape(-1,2)\n        rectangle = cv2.minAreaRect(np_contours)\n        box = cv2.boxPoints(rectangle)\n\n        # align diamond-shape\n        w, h = np.linalg.norm(box[0] - box[1]), np.linalg.norm(box[1] - box[2])\n        box_ratio = max(w, h) / (min(w, h) + 1e-5)\n        if abs(1 - box_ratio) <= 0.1:\n            l, r = min(np_contours[:,0]), max(np_contours[:,0])\n            t, b = min(np_contours[:,1]), max(np_contours[:,1])\n            box = np.array([[l, t], [r, t], [r, b], [l, b]], dtype=np.float32)\n\n        # make clock-wise order\n        startidx = box.sum(axis=1).argmin()\n        box = np.roll(box, 4-startidx, 0)\n        box = np.array(box)\n\n        det.append(box)\n        mapper.append(k)\n\n    return det, labels, mapper\n\ndef getPoly_core(boxes, labels, mapper, linkmap):\n    # configs\n    num_cp = 5\n    max_len_ratio = 0.7\n    expand_ratio = 1.45\n    max_r = 2.0\n    step_r = 0.2\n\n    polys = []  \n    for k, box in enumerate(boxes):\n        # size filter for small instance\n        w, h = int(np.linalg.norm(box[0] - box[1]) + 1), int(np.linalg.norm(box[1] - box[2]) + 1)\n        if w < 30 or h < 30:\n            polys.append(None); continue\n\n        # warp image\n        tar = np.float32([[0,0],[w,0],[w,h],[0,h]])\n        M = cv2.getPerspectiveTransform(box, tar)\n        word_label = cv2.warpPerspective(labels, M, (w, h), flags=cv2.INTER_NEAREST)\n        try:\n            Minv = np.linalg.inv(M)\n        except:\n            polys.append(None); continue\n\n        # binarization for selected label\n        cur_label = mapper[k]\n        word_label[word_label != cur_label] = 0\n        word_label[word_label > 0] = 1\n\n        """""" Polygon generation """"""\n        # find top/bottom contours\n        cp = []\n        max_len = -1\n        for i in range(w):\n            region = np.where(word_label[:,i] != 0)[0]\n            if len(region) < 2 : continue\n            cp.append((i, region[0], region[-1]))\n            length = region[-1] - region[0] + 1\n            if length > max_len: max_len = length\n\n        # pass if max_len is similar to h\n        if h * max_len_ratio < max_len:\n            polys.append(None); continue\n\n        # get pivot points with fixed length\n        tot_seg = num_cp * 2 + 1\n        seg_w = w / tot_seg     # segment width\n        pp = [None] * num_cp    # init pivot points\n        cp_section = [[0, 0]] * tot_seg\n        seg_height = [0] * num_cp\n        seg_num = 0\n        num_sec = 0\n        prev_h = -1\n        for i in range(0,len(cp)):\n            (x, sy, ey) = cp[i]\n            if (seg_num + 1) * seg_w <= x and seg_num <= tot_seg:\n                # average previous segment\n                if num_sec == 0: break\n                cp_section[seg_num] = [cp_section[seg_num][0] / num_sec, cp_section[seg_num][1] / num_sec]\n                num_sec = 0\n\n                # reset variables\n                seg_num += 1\n                prev_h = -1\n\n            # accumulate center points\n            cy = (sy + ey) * 0.5\n            cur_h = ey - sy + 1\n            cp_section[seg_num] = [cp_section[seg_num][0] + x, cp_section[seg_num][1] + cy]\n            num_sec += 1\n\n            if seg_num % 2 == 0: continue # No polygon area\n\n            if prev_h < cur_h:\n                pp[int((seg_num - 1)/2)] = (x, cy)\n                seg_height[int((seg_num - 1)/2)] = cur_h\n                prev_h = cur_h\n\n        # processing last segment\n        if num_sec != 0:\n            cp_section[-1] = [cp_section[-1][0] / num_sec, cp_section[-1][1] / num_sec]\n\n        # pass if num of pivots is not sufficient or segment widh is smaller than character height \n        if None in pp or seg_w < np.max(seg_height) * 0.25:\n            polys.append(None); continue\n\n        # calc median maximum of pivot points\n        half_char_h = np.median(seg_height) * expand_ratio / 2\n\n        # calc gradiant and apply to make horizontal pivots\n        new_pp = []\n        for i, (x, cy) in enumerate(pp):\n            dx = cp_section[i * 2 + 2][0] - cp_section[i * 2][0]\n            dy = cp_section[i * 2 + 2][1] - cp_section[i * 2][1]\n            if dx == 0:     # gradient if zero\n                new_pp.append([x, cy - half_char_h, x, cy + half_char_h])\n                continue\n            rad = - math.atan2(dy, dx)\n            c, s = half_char_h * math.cos(rad), half_char_h * math.sin(rad)\n            new_pp.append([x - s, cy - c, x + s, cy + c])\n\n        # get edge points to cover character heatmaps\n        isSppFound, isEppFound = False, False\n        grad_s = (pp[1][1] - pp[0][1]) / (pp[1][0] - pp[0][0]) + (pp[2][1] - pp[1][1]) / (pp[2][0] - pp[1][0])\n        grad_e = (pp[-2][1] - pp[-1][1]) / (pp[-2][0] - pp[-1][0]) + (pp[-3][1] - pp[-2][1]) / (pp[-3][0] - pp[-2][0])\n        for r in np.arange(0.5, max_r, step_r):\n            dx = 2 * half_char_h * r\n            if not isSppFound:\n                line_img = np.zeros(word_label.shape, dtype=np.uint8)\n                dy = grad_s * dx\n                p = np.array(new_pp[0]) - np.array([dx, dy, dx, dy])\n                cv2.line(line_img, (int(p[0]), int(p[1])), (int(p[2]), int(p[3])), 1, thickness=1)\n                if np.sum(np.logical_and(word_label, line_img)) == 0 or r + 2 * step_r >= max_r:\n                    spp = p\n                    isSppFound = True\n            if not isEppFound:\n                line_img = np.zeros(word_label.shape, dtype=np.uint8)\n                dy = grad_e * dx\n                p = np.array(new_pp[-1]) + np.array([dx, dy, dx, dy])\n                cv2.line(line_img, (int(p[0]), int(p[1])), (int(p[2]), int(p[3])), 1, thickness=1)\n                if np.sum(np.logical_and(word_label, line_img)) == 0 or r + 2 * step_r >= max_r:\n                    epp = p\n                    isEppFound = True\n            if isSppFound and isEppFound:\n                break\n\n        # pass if boundary of polygon is not found\n        if not (isSppFound and isEppFound):\n            polys.append(None); continue\n\n        # make final polygon\n        poly = []\n        poly.append(warpCoord(Minv, (spp[0], spp[1])))\n        for p in new_pp:\n            poly.append(warpCoord(Minv, (p[0], p[1])))\n        poly.append(warpCoord(Minv, (epp[0], epp[1])))\n        poly.append(warpCoord(Minv, (epp[2], epp[3])))\n        for p in reversed(new_pp):\n            poly.append(warpCoord(Minv, (p[2], p[3])))\n        poly.append(warpCoord(Minv, (spp[2], spp[3])))\n\n        # add to final result\n        polys.append(np.array(poly))\n\n    return polys\n\ndef getDetBoxes(textmap, linkmap, text_threshold, link_threshold, low_text, poly=False):\n    boxes, labels, mapper = getDetBoxes_core(textmap, linkmap, text_threshold, link_threshold, low_text)\n\n    if poly:\n        polys = getPoly_core(boxes, labels, mapper, linkmap)\n    else:\n        polys = [None] * len(boxes)\n\n    return boxes, polys\n\ndef adjustResultCoordinates(polys, ratio_w, ratio_h, ratio_net = 2):\n    if len(polys) > 0:\n        polys = np.array(polys)\n        for k in range(len(polys)):\n            if polys[k] is not None:\n                polys[k] *= (ratio_w * ratio_net, ratio_h * ratio_net)\n    return polys\n'"
data_loader.py,12,"b'\n\n\n###for icdar2015####\n\n\n\nimport torch\nimport torch.utils.data as data\nimport scipy.io as scio\nfrom gaussian import GaussianTransformer\nfrom watershed import watershed\nimport re\nimport itertools\nfrom file_utils import *\nfrom mep import mep\nimport random\nfrom PIL import Image\nimport torchvision.transforms as transforms\nimport craft_utils\nimport Polygon as plg\nimport time\n\n\ndef ratio_area(h, w, box):\n    area = h * w\n    ratio = 0\n    for i in range(len(box)):\n        poly = plg.Polygon(box[i])\n        box_area = poly.area()\n        tem = box_area / area\n        if tem > ratio:\n            ratio = tem\n    return ratio, area\n\ndef rescale_img(img, box, h, w):\n    image = np.zeros((768,768,3),dtype = np.uint8)\n    length = max(h, w)\n    scale = 768 / length           ###768 is the train image size\n    img = cv2.resize(img, dsize=None, fx=scale, fy=scale)\n    image[:img.shape[0], :img.shape[1]] = img\n    box *= scale\n    return image\n\ndef random_scale(img, bboxes, min_size):\n    h, w = img.shape[0:2]\n    # ratio, _ = ratio_area(h, w, bboxes)\n    # if ratio > 0.5:\n    #     image = rescale_img(img.copy(), bboxes, h, w)\n    #     return image\n    scale = 1.0\n    if max(h, w) > 1280:\n        scale = 1280.0 / max(h, w)\n    random_scale = np.array([0.5, 1.0, 1.5, 2.0])\n    scale1 = np.random.choice(random_scale)\n    if min(h, w) * scale * scale1 <= min_size:\n        scale = (min_size + 10) * 1.0 / min(h, w)\n    else:\n        scale = scale * scale1\n    bboxes *= scale\n    img = cv2.resize(img, dsize=None, fx=scale, fy=scale)\n    return img\n\ndef padding_image(image,imgsize):\n    length = max(image.shape[0:2])\n    if len(image.shape) == 3:\n        img = np.zeros((imgsize, imgsize, len(image.shape)), dtype = np.uint8)\n    else:\n        img = np.zeros((imgsize, imgsize), dtype = np.uint8)\n    scale = imgsize / length\n    image = cv2.resize(image, dsize=None, fx=scale, fy=scale)\n    if len(image.shape) == 3:\n        img[:image.shape[0], :image.shape[1], :] = image\n    else:\n        img[:image.shape[0], :image.shape[1]] = image\n    return img\n\ndef random_crop(imgs, img_size, character_bboxes):\n    h, w = imgs[0].shape[0:2]\n    th, tw = img_size\n    crop_h, crop_w = img_size\n    if w == tw and h == th:\n        return imgs\n\n    word_bboxes = []\n    if len(character_bboxes) > 0:\n        for bboxes in character_bboxes:\n            word_bboxes.append(\n                [[bboxes[:, :, 0].min(), bboxes[:, :, 1].min()], [bboxes[:, :, 0].max(), bboxes[:, :, 1].max()]])\n    word_bboxes = np.array(word_bboxes, np.int32)\n\n    #### IC15 for 0.6, MLT for 0.35 #####\n    if random.random() > 0.6 and len(word_bboxes) > 0:\n        sample_bboxes = word_bboxes[random.randint(0, len(word_bboxes) - 1)]\n        left = max(sample_bboxes[1, 0] - img_size[0], 0)\n        top = max(sample_bboxes[1, 1] - img_size[0], 0)\n\n        if min(sample_bboxes[0, 1], h - th) < top or min(sample_bboxes[0, 0], w - tw) < left:\n            i = random.randint(0, h - th)\n            j = random.randint(0, w - tw)\n        else:\n            i = random.randint(top, min(sample_bboxes[0, 1], h - th))\n            j = random.randint(left, min(sample_bboxes[0, 0], w - tw))\n\n        crop_h = sample_bboxes[1, 1] if th < sample_bboxes[1, 1] - i else th\n        crop_w = sample_bboxes[1, 0] if tw < sample_bboxes[1, 0] - j else tw\n    else:\n        ### train for IC15 dataset####\n        # i = random.randint(0, h - th)\n        # j = random.randint(0, w - tw)\n\n        #### train for MLT dataset ###\n        i, j = 0, 0\n        crop_h, crop_w = h + 1, w + 1  # make the crop_h, crop_w > tw, th\n\n    for idx in range(len(imgs)):\n        # crop_h = sample_bboxes[1, 1] if th < sample_bboxes[1, 1] else th\n        # crop_w = sample_bboxes[1, 0] if tw < sample_bboxes[1, 0] else tw\n\n        if len(imgs[idx].shape) == 3:\n            imgs[idx] = imgs[idx][i:i + crop_h, j:j + crop_w, :]\n        else:\n            imgs[idx] = imgs[idx][i:i + crop_h, j:j + crop_w]\n\n        if crop_w > tw or crop_h > th:\n            imgs[idx] = padding_image(imgs[idx], tw)\n\n    return imgs\n\n\ndef random_horizontal_flip(imgs):\n    if random.random() < 0.5:\n        for i in range(len(imgs)):\n            imgs[i] = np.flip(imgs[i], axis=1).copy()\n    return imgs\n\n\ndef random_rotate(imgs):\n    max_angle = 10\n    angle = random.random() * 2 * max_angle - max_angle\n    for i in range(len(imgs)):\n        img = imgs[i]\n        w, h = img.shape[:2]\n        rotation_matrix = cv2.getRotationMatrix2D((h / 2, w / 2), angle, 1)\n        img_rotation = cv2.warpAffine(img, rotation_matrix, (h, w))\n        imgs[i] = img_rotation\n    return imgs\n\n\nclass craft_base_dataset(data.Dataset):\n    def __init__(self, target_size=768, viz=False, debug=False):\n        self.target_size = target_size\n        self.viz = viz\n        self.debug = debug\n        self.gaussianTransformer = GaussianTransformer(imgSize=1024, region_threshold=0.35, affinity_threshold=0.15)\n\n    def load_image_gt_and_confidencemask(self, index):\n        \'\'\'\n        \xe6\xa0\xb9\xe6\x8d\xae\xe7\xb4\xa2\xe5\xbc\x95\xe5\x80\xbc\xe8\xbf\x94\xe5\x9b\x9e\xe5\x9b\xbe\xe5\x83\x8f\xe3\x80\x81\xe5\xad\x97\xe7\xac\xa6\xe6\xa1\x86\xe3\x80\x81\xe6\x96\x87\xe5\xad\x97\xe8\xa1\x8c\xe5\x86\x85\xe5\xae\xb9\xe3\x80\x81confidence mask\n        :param index:\n        :return:\n        \'\'\'\n        return None, None, None, None, None\n\n    def crop_image_by_bbox(self, image, box):\n        w = (int)(np.linalg.norm(box[0] - box[1]))\n        h = (int)(np.linalg.norm(box[0] - box[3]))\n        width = w\n        height = h\n        if h > w * 1.5:\n            width = h\n            height = w\n            M = cv2.getPerspectiveTransform(np.float32(box),\n                                            np.float32(np.array([[width, 0], [width, height], [0, height], [0, 0]])))\n        else:\n            M = cv2.getPerspectiveTransform(np.float32(box),\n                                            np.float32(np.array([[0, 0], [width, 0], [width, height], [0, height]])))\n\n        warped = cv2.warpPerspective(image, M, (width, height))\n        return warped, M\n\n    def get_confidence(self, real_len, pursedo_len):\n        if pursedo_len == 0:\n            return 0.\n        return (real_len - min(real_len, abs(real_len - pursedo_len))) / real_len\n\n    def inference_pursedo_bboxes(self, net, image, word_bbox, word, viz=False):\n\n        word_image, MM = self.crop_image_by_bbox(image, word_bbox)\n\n        real_word_without_space = word.replace(\'\\s\', \'\')\n        real_char_nums = len(real_word_without_space)\n        input = word_image.copy()\n        scale = 64.0 / input.shape[0]\n        input = cv2.resize(input, None, fx=scale, fy=scale)\n\n        img_torch = torch.from_numpy(imgproc.normalizeMeanVariance(input, mean=(0.485, 0.456, 0.406),\n                                                                   variance=(0.229, 0.224, 0.225)))\n        img_torch = img_torch.permute(2, 0, 1).unsqueeze(0)\n        img_torch = img_torch.type(torch.FloatTensor).cuda()\n        scores, _ = net(img_torch)\n        region_scores = scores[0, :, :, 0].cpu().data.numpy()\n        region_scores = np.uint8(np.clip(region_scores, 0, 1) * 255)\n        bgr_region_scores = cv2.resize(region_scores, (input.shape[1], input.shape[0]))\n        bgr_region_scores = cv2.cvtColor(bgr_region_scores, cv2.COLOR_GRAY2BGR)\n        pursedo_bboxes = watershed(input, bgr_region_scores, False)\n\n        _tmp = []\n        for i in range(pursedo_bboxes.shape[0]):\n            if np.mean(pursedo_bboxes[i].ravel()) > 2:\n                _tmp.append(pursedo_bboxes[i])\n            else:\n                print(""filter bboxes"", pursedo_bboxes[i])\n        pursedo_bboxes = np.array(_tmp, np.float32)\n        if pursedo_bboxes.shape[0] > 1:\n            index = np.argsort(pursedo_bboxes[:, 0, 0])\n            pursedo_bboxes = pursedo_bboxes[index]\n\n        confidence = self.get_confidence(real_char_nums, len(pursedo_bboxes))\n\n        bboxes = []\n        if confidence <= 0.5:\n            width = input.shape[1]\n            height = input.shape[0]\n\n            width_per_char = width / len(word)\n            for i, char in enumerate(word):\n                if char == \' \':\n                    continue\n                left = i * width_per_char\n                right = (i + 1) * width_per_char\n                bbox = np.array([[left, 0], [right, 0], [right, height],\n                                 [left, height]])\n                bboxes.append(bbox)\n\n            bboxes = np.array(bboxes, np.float32)\n            confidence = 0.5\n\n        else:\n            bboxes = pursedo_bboxes\n        if False:\n            _tmp_bboxes = np.int32(bboxes.copy())\n            _tmp_bboxes[:, :, 0] = np.clip(_tmp_bboxes[:, :, 0], 0, input.shape[1])\n            _tmp_bboxes[:, :, 1] = np.clip(_tmp_bboxes[:, :, 1], 0, input.shape[0])\n            for bbox in _tmp_bboxes:\n                cv2.polylines(np.uint8(input), [np.reshape(bbox, (-1, 1, 2))], True, (255, 0, 0))\n            region_scores_color = cv2.applyColorMap(np.uint8(region_scores), cv2.COLORMAP_JET)\n            region_scores_color = cv2.resize(region_scores_color, (input.shape[1], input.shape[0]))\n            target = self.gaussianTransformer.generate_region(region_scores_color.shape, [_tmp_bboxes])\n            target_color = cv2.applyColorMap(target, cv2.COLORMAP_JET)\n            viz_image = np.hstack([input[:, :, ::-1], region_scores_color, target_color])\n            cv2.imshow(""crop_image"", viz_image)\n            cv2.waitKey()\n        bboxes /= scale\n        try:\n            for j in range(len(bboxes)):\n                ones = np.ones((4, 1))\n                tmp = np.concatenate([bboxes[j], ones], axis=-1)\n                I = np.matrix(MM).I\n                ori = np.matmul(I, tmp.transpose(1, 0)).transpose(1, 0)\n                bboxes[j] = ori[:, :2]\n        except Exception as e:\n            print(e, gt_path)\n\n#         for j in range(len(bboxes)):\n#             ones = np.ones((4, 1))\n#             tmp = np.concatenate([bboxes[j], ones], axis=-1)\n#             I = np.matrix(MM).I\n#             ori = np.matmul(I, tmp.transpose(1, 0)).transpose(1, 0)\n#             bboxes[j] = ori[:, :2]\n\n        bboxes[:, :, 1] = np.clip(bboxes[:, :, 1], 0., image.shape[0] - 1)\n        bboxes[:, :, 0] = np.clip(bboxes[:, :, 0], 0., image.shape[1] - 1)\n\n        return bboxes, region_scores, confidence\n\n    def resizeGt(self, gtmask):\n        return cv2.resize(gtmask, (self.target_size // 2, self.target_size // 2))\n\n    def get_imagename(self, index):\n        return None\n\n    def saveInput(self, imagename, image, region_scores, affinity_scores, confidence_mask):\n\n        boxes, polys = craft_utils.getDetBoxes(region_scores / 255, affinity_scores / 255, 0.7, 0.4, 0.4, False)\n        boxes = np.array(boxes, np.int32) * 2\n        if len(boxes) > 0:\n            np.clip(boxes[:, :, 0], 0, image.shape[1])\n            np.clip(boxes[:, :, 1], 0, image.shape[0])\n            for box in boxes:\n                cv2.polylines(image, [np.reshape(box, (-1, 1, 2))], True, (0, 0, 255))\n        target_gaussian_heatmap_color = imgproc.cvt2HeatmapImg(region_scores / 255)\n        target_gaussian_affinity_heatmap_color = imgproc.cvt2HeatmapImg(affinity_scores / 255)\n        confidence_mask_gray = imgproc.cvt2HeatmapImg(confidence_mask)\n        gt_scores = np.hstack([target_gaussian_heatmap_color, target_gaussian_affinity_heatmap_color])\n        confidence_mask_gray = np.hstack([np.zeros_like(confidence_mask_gray), confidence_mask_gray])\n        output = np.concatenate([gt_scores, confidence_mask_gray],\n                                axis=0)\n        output = np.hstack([image, output])\n        outpath = os.path.join(os.path.join(os.path.dirname(__file__) + \'/output\'), ""%s_input.jpg"" % imagename)\n        print(outpath)\n        if not os.path.exists(os.path.dirname(outpath)):\n            os.mkdir(os.path.dirname(outpath))\n        cv2.imwrite(outpath, output)\n\n    def saveImage(self, imagename, image, bboxes, affinity_bboxes, region_scores, affinity_scores, confidence_mask):\n        output_image = np.uint8(image.copy())\n        output_image = cv2.cvtColor(output_image, cv2.COLOR_RGB2BGR)\n        if len(bboxes) > 0:\n            affinity_bboxes = np.int32(affinity_bboxes)\n            for i in range(affinity_bboxes.shape[0]):\n                cv2.polylines(output_image, [np.reshape(affinity_bboxes[i], (-1, 1, 2))], True, (255, 0, 0))\n            for i in range(len(bboxes)):\n                _bboxes = np.int32(bboxes[i])\n                for j in range(_bboxes.shape[0]):\n                    cv2.polylines(output_image, [np.reshape(_bboxes[j], (-1, 1, 2))], True, (0, 0, 255))\n\n        target_gaussian_heatmap_color = imgproc.cvt2HeatmapImg(region_scores / 255)\n        target_gaussian_affinity_heatmap_color = imgproc.cvt2HeatmapImg(affinity_scores / 255)\n        heat_map = np.concatenate([target_gaussian_heatmap_color, target_gaussian_affinity_heatmap_color], axis=1)\n        confidence_mask_gray = imgproc.cvt2HeatmapImg(confidence_mask)\n        output = np.concatenate([output_image, heat_map, confidence_mask_gray], axis=1)\n        outpath = os.path.join(os.path.join(os.path.dirname(__file__) + \'/output\'), imagename)\n\n        if not os.path.exists(os.path.dirname(outpath)):\n            os.mkdir(os.path.dirname(outpath))\n        cv2.imwrite(outpath, output)\n\n    def pull_item(self, index):\n        # if self.get_imagename(index) == \'img_59.jpg\':\n        #     pass\n        # else:\n        #     return [], [], [], [], np.array([0])\n        image, character_bboxes, words, confidence_mask, confidences = self.load_image_gt_and_confidencemask(index)\n        if len(confidences) == 0:\n            confidences = 1.0\n        else:\n            confidences = np.array(confidences).mean()\n        region_scores = np.zeros((image.shape[0], image.shape[1]), dtype=np.float32)\n        affinity_scores = np.zeros((image.shape[0], image.shape[1]), dtype=np.float32)\n        affinity_bboxes = []\n\n        if len(character_bboxes) > 0:\n            region_scores = self.gaussianTransformer.generate_region(region_scores.shape, character_bboxes)\n            affinity_scores, affinity_bboxes = self.gaussianTransformer.generate_affinity(region_scores.shape,\n                                                                                          character_bboxes,\n                                                                                          words)\n        if self.viz:\n            self.saveImage(self.get_imagename(index), image.copy(), character_bboxes, affinity_bboxes, region_scores,\n                           affinity_scores,\n                           confidence_mask)\n        random_transforms = [image, region_scores, affinity_scores, confidence_mask]\n        random_transforms = random_crop(random_transforms, (self.target_size, self.target_size), character_bboxes)\n        random_transforms = random_horizontal_flip(random_transforms)\n        random_transforms = random_rotate(random_transforms)\n\n        cvimage, region_scores, affinity_scores, confidence_mask = random_transforms\n\n        region_scores = self.resizeGt(region_scores)\n        affinity_scores = self.resizeGt(affinity_scores)\n        confidence_mask = self.resizeGt(confidence_mask)\n\n        if self.viz:\n            self.saveInput(self.get_imagename(index), cvimage, region_scores, affinity_scores, confidence_mask)\n        image = Image.fromarray(cvimage)\n        image = image.convert(\'RGB\')\n        image = transforms.ColorJitter(brightness=32.0 / 255, saturation=0.5)(image)\n\n        image = imgproc.normalizeMeanVariance(np.array(image), mean=(0.485, 0.456, 0.406),\n                                              variance=(0.229, 0.224, 0.225))\n        image = torch.from_numpy(image).float().permute(2, 0, 1)\n        region_scores_torch = torch.from_numpy(region_scores / 255).float()\n        affinity_scores_torch = torch.from_numpy(affinity_scores / 255).float()\n        confidence_mask_torch = torch.from_numpy(confidence_mask).float()\n        return image, region_scores_torch, affinity_scores_torch, confidence_mask_torch, confidences\n\n\nclass Synth80k(craft_base_dataset):\n\n    def __init__(self, synthtext_folder, target_size=768, viz=False, debug=False):\n        super(Synth80k, self).__init__(target_size, viz, debug)\n        self.synthtext_folder = synthtext_folder\n        gt = scio.loadmat(os.path.join(synthtext_folder, \'gt.mat\'))\n        self.charbox = gt[\'charBB\'][0]\n        self.image = gt[\'imnames\'][0]\n        self.imgtxt = gt[\'txt\'][0]\n\n    def __getitem__(self, index):\n        return self.pull_item(index)\n\n    def __len__(self):\n        return len(self.imgtxt)\n\n    def get_imagename(self, index):\n        return self.image[index][0]\n\n    def load_image_gt_and_confidencemask(self, index):\n        \'\'\'\n        \xe6\xa0\xb9\xe6\x8d\xae\xe7\xb4\xa2\xe5\xbc\x95\xe5\x8a\xa0\xe8\xbd\xbdground truth\n        :param index:\xe7\xb4\xa2\xe5\xbc\x95\n        :return:bboxes \xe5\xad\x97\xe7\xac\xa6\xe7\x9a\x84\xe6\xa1\x86\xef\xbc\x8c\n        \'\'\'\n        img_path = os.path.join(self.synthtext_folder, self.image[index][0])\n        image = cv2.imread(img_path, cv2.IMREAD_COLOR)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        _charbox = self.charbox[index].transpose((2, 1, 0))\n        image = random_scale(image, _charbox, self.target_size)\n\n        words = [re.split(\' \\n|\\n |\\n| \', t.strip()) for t in self.imgtxt[index]]\n        words = list(itertools.chain(*words))\n        words = [t for t in words if len(t) > 0]\n        character_bboxes = []\n        total = 0\n        confidences = []\n        for i in range(len(words)):\n            bboxes = _charbox[total:total + len(words[i])]\n            assert (len(bboxes) == len(words[i]))\n            total += len(words[i])\n            bboxes = np.array(bboxes)\n            character_bboxes.append(bboxes)\n            confidences.append(1.0)\n\n        return image, character_bboxes, words, np.ones((image.shape[0], image.shape[1]), np.float32), confidences\n\n\nclass ICDAR2013(craft_base_dataset):\n    def __init__(self, net, icdar2013_folder, target_size=768, viz=False, debug=False):\n        super(ICDAR2013, self).__init__(target_size, viz, debug)\n        self.net = net\n        self.net.eval()\n        self.img_folder = os.path.join(icdar2013_folder, \'images/ch8_training_images\')\n        self.gt_folder = os.path.join(icdar2013_folder, \'gt\')\n        imagenames = os.listdir(self.img_folder)\n        self.images_path = []\n        for imagename in imagenames:\n            self.images_path.append(imagename)\n\n    def __getitem__(self, index):\n        return self.pull_item(index)\n\n    def __len__(self):\n        return len(self.images_path)\n\n    def get_imagename(self, index):\n        return self.images_path[index]\n\n    # def convert2013(self,box):\n    #     str = box[-1][1:-1]\n    #     bboxes = [box[0], box[1], box[2], box[1],\n    #               box[2], box[3], box[0], box[3],\n    #               str]\n    #     return bboxes\n\n    def load_image_gt_and_confidencemask(self, index):\n        \'\'\'\n        \xe6\xa0\xb9\xe6\x8d\xae\xe7\xb4\xa2\xe5\xbc\x95\xe5\x8a\xa0\xe8\xbd\xbdground truth\n        :param index:\xe7\xb4\xa2\xe5\xbc\x95\n        :return:bboxes \xe5\xad\x97\xe7\xac\xa6\xe7\x9a\x84\xe6\xa1\x86\xef\xbc\x8c\n        \'\'\'\n        imagename = self.images_path[index]\n        gt_path = os.path.join(self.gt_folder, ""gt_%s.txt"" % os.path.splitext(imagename)[0])\n        word_bboxes, words = self.load_gt(gt_path)\n        word_bboxes = np.float32(word_bboxes)\n\n        image_path = os.path.join(self.img_folder, imagename)\n        image = cv2.imread(image_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n        image = random_scale(image, word_bboxes, self.target_size)\n\n        confidence_mask = np.ones((image.shape[0], image.shape[1]), np.float32)\n\n        character_bboxes = []\n        new_words = []\n        confidences = []\n        if len(word_bboxes) > 0:\n            for i in range(len(word_bboxes)):\n                if words[i] == \'###\' or len(words[i].strip()) == 0:\n                    cv2.fillPoly(confidence_mask, [np.int32(word_bboxes[i])], (0))\n            for i in range(len(word_bboxes)):\n                if words[i] == \'###\' or len(words[i].strip()) == 0:\n                    continue\n                pursedo_bboxes, bbox_region_scores, confidence = self.inference_pursedo_bboxes(self.net, image,\n                                                                                               word_bboxes[i],\n                                                                                               words[i],\n                                                                                               gt_path,\n                                                                                               viz=self.viz)\n                confidences.append(confidence)\n                cv2.fillPoly(confidence_mask, [np.int32(word_bboxes[i])], (confidence))\n                new_words.append(words[i])\n                character_bboxes.append(pursedo_bboxes)\n        return image, character_bboxes, new_words, confidence_mask, confidences\n\n    def load_gt(self, gt_path):\n        lines = open(gt_path, encoding=\'utf-8\').readlines()\n        bboxes = []\n        words = []\n        for line in lines:\n            ori_box = line.strip().encode(\'utf-8\').decode(\'utf-8-sig\').split(\',\')\n            box = [int(ori_box[j]) for j in range(8)]\n            word = ori_box[9:]\n            word = \',\'.join(word)\n            box = np.array(box, np.int32).reshape(4, 2)\n            if word == \'###\':\n                words.append(\'###\')\n                bboxes.append(box)\n                continue\n            if len(word.strip()) == 0:\n                continue\n\n            try:\n                area, p0, p3, p2, p1, _, _ = mep(box)\n            except Exception as e:\n                print(e,gt_path)\n\n            bbox = np.array([p0, p1, p2, p3])\n            distance = 10000000\n            index = 0\n            for i in range(4):\n                d = np.linalg.norm(box[0] - bbox[i])\n                if distance > d:\n                    index = i\n                    distance = d\n            new_box = []\n            for i in range(index, index + 4):\n                new_box.append(bbox[i % 4])\n            new_box = np.array(new_box)\n            bboxes.append(np.array(new_box))\n            words.append(word)\n        return bboxes, words\n\n\nclass ICDAR2015(craft_base_dataset):\n    def __init__(self, net, icdar2015_folder, target_size=768, viz=False, debug=False):\n        super(ICDAR2015, self).__init__(target_size, viz, debug)\n        self.net = net\n        self.net.eval()\n        self.img_folder = os.path.join(icdar2015_folder, \'ch4_training_images\')\n        self.gt_folder = os.path.join(icdar2015_folder, \'ch4_training_localization_transcription_gt\')\n        imagenames = os.listdir(self.img_folder)\n        self.images_path = []\n        for imagename in imagenames:\n            self.images_path.append(imagename)\n\n    def __getitem__(self, index):\n        return self.pull_item(index)\n\n    def __len__(self):\n        return len(self.images_path)\n\n    def get_imagename(self, index):\n        return self.images_path[index]\n\n    def load_image_gt_and_confidencemask(self, index):\n        \'\'\'\n        \xe6\xa0\xb9\xe6\x8d\xae\xe7\xb4\xa2\xe5\xbc\x95\xe5\x8a\xa0\xe8\xbd\xbdground truth\n        :param index:\xe7\xb4\xa2\xe5\xbc\x95\n        :return:bboxes \xe5\xad\x97\xe7\xac\xa6\xe7\x9a\x84\xe6\xa1\x86\xef\xbc\x8c\n        \'\'\'\n        imagename = self.images_path[index]\n        gt_path = os.path.join(self.gt_folder, ""gt_%s.txt"" % os.path.splitext(imagename)[0])\n        word_bboxes, words = self.load_gt(gt_path)\n        word_bboxes = np.float32(word_bboxes)\n\n        image_path = os.path.join(self.img_folder, imagename)\n        image = cv2.imread(image_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n        image = random_scale(image, word_bboxes, self.target_size)\n\n        confidence_mask = np.ones((image.shape[0], image.shape[1]), np.float32)\n\n        character_bboxes = []\n        new_words = []\n        confidences = []\n        if len(word_bboxes) > 0:\n            for i in range(len(word_bboxes)):\n                if words[i] == \'###\' or len(words[i].strip()) == 0:\n                    cv2.fillPoly(confidence_mask, [np.int32(word_bboxes[i])], (0))\n            for i in range(len(word_bboxes)):\n                if words[i] == \'###\' or len(words[i].strip()) == 0:\n                    continue\n                pursedo_bboxes, bbox_region_scores, confidence = self.inference_pursedo_bboxes(self.net, image,\n                                                                                               word_bboxes[i],\n                                                                                               words[i],\n                                                                                               viz=self.viz)\n                confidences.append(confidence)\n                cv2.fillPoly(confidence_mask, [np.int32(word_bboxes[i])], (confidence))\n                new_words.append(words[i])\n                character_bboxes.append(pursedo_bboxes)\n        return image, character_bboxes, new_words, confidence_mask, confidences\n\n    def load_gt(self, gt_path):\n        lines = open(gt_path, encoding=\'utf-8\').readlines()\n        bboxes = []\n        words = []\n        for line in lines:\n            ori_box = line.strip().encode(\'utf-8\').decode(\'utf-8-sig\').split(\',\')\n            box = [int(ori_box[j]) for j in range(8)]\n            word = ori_box[8:]\n            word = \',\'.join(word)\n            box = np.array(box, np.int32).reshape(4, 2)\n            if word == \'###\':\n                words.append(\'###\')\n                bboxes.append(box)\n                continue\n            area, p0, p3, p2, p1, _, _ = mep(box)\n\n            bbox = np.array([p0, p1, p2, p3])\n            distance = 10000000\n            index = 0\n            for i in range(4):\n                d = np.linalg.norm(box[0] - bbox[i])\n                if distance > d:\n                    index = i\n                    distance = d\n            new_box = []\n            for i in range(index, index + 4):\n                new_box.append(bbox[i % 4])\n            new_box = np.array(new_box)\n            bboxes.append(np.array(new_box))\n            words.append(word)\n        return bboxes, words\n\n\nif __name__ == \'__main__\':\n    # synthtextloader = Synth80k(\'/home/jiachx/publicdatasets/SynthText/SynthText\', target_size=768, viz=True, debug=True)\n    # train_loader = torch.utils.data.DataLoader(\n    #     synthtextloader,\n    #     batch_size=1,\n    #     shuffle=False,\n    #     num_workers=0,\n    #     drop_last=True,\n    #     pin_memory=True)\n    # train_batch = iter(train_loader)\n    # image_origin, target_gaussian_heatmap, target_gaussian_affinity_heatmap, mask = next(train_batch)\n    from craft import CRAFT\n    from torchutil import copyStateDict\n\n    net = CRAFT(freeze=True)\n    net.load_state_dict(\n        copyStateDict(torch.load(\'/data/CRAFT-pytorch/1-7.pth\')))\n    net = net.cuda()\n    net = torch.nn.DataParallel(net)\n    net.eval()\n    dataloader = ICDAR2015(net, \'/data/CRAFT-pytorch/icdar2015\', target_size=768, viz=True)\n    train_loader = torch.utils.data.DataLoader(\n        dataloader,\n        batch_size=1,\n        shuffle=False,\n        num_workers=0,\n        drop_last=True,\n        pin_memory=True)\n    total = 0\n    total_sum = 0\n    for index, (opimage, region_scores, affinity_scores, confidence_mask, confidences_mean) in enumerate(train_loader):\n        total += 1\n        # confidence_mean = confidences_mean.mean()\n        # total_sum += confidence_mean\n        # print(index, confidence_mean)\n    print(""mean="", total_sum / total)\n\n\n'"
evaluation.py,0,"b'from test import test_net\nimport time\nimport file_utils\nimport os\nimport imgproc\nimport cv2\nfrom eval.icdar2015.script import eval_2015\nfrom eval.icdar2013.script import eval_2013\n\n\ndef eval2013(craft, test_folder, result_folder, text_threshold=0.7, link_threshold=0.4, low_text=0.4):\n    image_list, _, _ = file_utils.get_files(test_folder)\n    t = time.time()\n    res_gt_folder = os.path.join(result_folder, \'gt\')\n    res_mask_folder = os.path.join(result_folder, \'mask\')\n    # load data\n    for k, image_path in enumerate(image_list):\n        print(""Test image {:d}/{:d}: {:s}"".format(k + 1, len(image_list), image_path), end=\'\\n\')\n        image = imgproc.loadImage(image_path)\n\n        bboxes, polys, score_text = test_net(craft, image, text_threshold, link_threshold, low_text, True, False, 980,\n                                             1.5, False)\n\n        # save score text\n        filename, file_ext = os.path.splitext(os.path.basename(image_path))\n        mask_file = os.path.join(res_mask_folder, ""/res_"" + filename + \'_mask.jpg\')\n        cv2.imwrite(mask_file, score_text)\n\n        file_utils.saveResult13(image_path, polys, dirname=res_gt_folder)\n\n    eval_2013(res_gt_folder)\n    print(""elapsed time : {}s"".format(time.time() - t))\n\n\ndef eval2015(craft, test_folder, result_folder, text_threshold=0.7, link_threshold=0.4, low_text=0.4):\n    image_list, _, _ = file_utils.get_files(test_folder)\n    t = time.time()\n    res_gt_folder = os.path.join(result_folder, \'gt\')\n    res_mask_folder = os.path.join(result_folder, \'mask\')\n    # load data\n    for k, image_path in enumerate(image_list):\n        print(""Test image {:d}/{:d}: {:s}"".format(k + 1, len(image_list), image_path), end=\'\\n\')\n        image = imgproc.loadImage(image_path)\n\n        bboxes, polys, score_text = test_net(craft, image, text_threshold, link_threshold, low_text, True, False, 2240,\n                                             1.5, False)\n\n        # save score text\n        filename, file_ext = os.path.splitext(os.path.basename(image_path))\n        mask_file = os.path.join(res_mask_folder, ""/res_"" + filename + \'_mask.jpg\')\n        cv2.imwrite(mask_file, score_text)\n\n        file_utils.saveResult15(image_path, polys, dirname=res_gt_folder)\n\n    eval_2015(os.path.join(result_folder, \'gt\'))\n    print(""elapsed time : {}s"".format(time.time() - t))\n'"
file_utils.py,0,"b'# -*- coding: utf-8 -*-\nimport os\nimport numpy as np\nimport cv2\nimport imgproc\n\n# borrowed from https://github.com/lengstrom/fast-style-transfer/blob/master/src/utils.py\ndef get_files(img_dir):\n    imgs, masks, xmls = list_files(img_dir)\n    return imgs, masks, xmls\n\ndef list_files(in_path):\n    img_files = []\n    mask_files = []\n    gt_files = []\n    for (dirpath, dirnames, filenames) in os.walk(in_path):\n        for file in filenames:\n            filename, ext = os.path.splitext(file)\n            ext = str.lower(ext)\n            if ext == \'.jpg\' or ext == \'.jpeg\' or ext == \'.gif\' or ext == \'.png\' or ext == \'.pgm\':\n                img_files.append(os.path.join(dirpath, file))\n            elif ext == \'.bmp\':\n                mask_files.append(os.path.join(dirpath, file))\n            elif ext == \'.xml\' or ext == \'.gt\' or ext == \'.txt\':\n                gt_files.append(os.path.join(dirpath, file))\n            elif ext == \'.zip\':\n                continue\n    # img_files.sort()\n    # mask_files.sort()\n    # gt_files.sort()\n    return img_files, mask_files, gt_files\n\ndef saveResult(img_file, img, boxes, dirname=\'./result/\', verticals=None, texts=None):\n        """""" save text detection result one by one\n        Args:\n            img_file (str): image file name\n            img (array): raw image context\n            boxes (array): array of result file\n                Shape: [num_detections, 4] for BB output / [num_detections, 4] for QUAD output\n        Return:\n            None\n        """"""\n        img = np.array(img)\n\n        # make result file list\n        filename, file_ext = os.path.splitext(os.path.basename(img_file))\n\n        # result directory\n        res_file = dirname + ""res_"" + filename + \'.txt\'\n        res_img_file = dirname + ""res_"" + filename + \'.jpg\'\n\n        if not os.path.isdir(dirname):\n            os.mkdir(dirname)\n\n        with open(res_file, \'w\') as f:\n            for i, box in enumerate(boxes):\n                poly = np.array(box).astype(np.int32).reshape((-1))\n                strResult = \',\'.join([str(p) for p in poly]) + \'\\r\\n\'\n                # poly = np.array(box).astype(np.int32)\n                # min_x = np.min(poly[:,0])\n                # max_x = np.max(poly[:,0])\n                # min_y = np.min(poly[:,1])\n                # max_y = np.max(poly[:,1])\n                # strResult = \',\'.join([str(min_x), str(min_y), str(max_x), str(max_y)]) + \'\\r\\n\'\n                f.write(strResult)\n\n        #         poly = poly.reshape(-1, 2)\n        #         cv2.polylines(img, [poly.reshape((-1, 1, 2))], True, color=(0, 0, 255), thickness=2)\n        #         ptColor = (0, 255, 255)\n        #         if verticals is not None:\n        #             if verticals[i]:\n        #                 ptColor = (255, 0, 0)\n        #\n        #         if texts is not None:\n        #             font = cv2.FONT_HERSHEY_SIMPLEX\n        #             font_scale = 0.5\n        #             cv2.putText(img, ""{}"".format(texts[i]), (poly[0][0]+1, poly[0][1]+1), font, font_scale, (0, 0, 0), thickness=1)\n        #             cv2.putText(img, ""{}"".format(texts[i]), tuple(poly[0]), font, font_scale, (0, 255, 255), thickness=1)\n        #\n        # #Save result image\n        # cv2.imwrite(res_img_file, img)\n\n'"
gaussian.py,0,"b'from math import exp\nimport numpy as np\nimport cv2\nimport os\nimport imgproc\n\n\nclass GaussianTransformer(object):\n\n    def __init__(self, imgSize=512, region_threshold=0.4,\n                 affinity_threshold=0.2):\n        distanceRatio = 3.34\n        scaledGaussian = lambda x: exp(-(1 / 2) * (x ** 2))\n        self.region_threshold = region_threshold\n        self.imgSize = imgSize\n        self.standardGaussianHeat = self._gen_gaussian_heatmap(imgSize, distanceRatio)\n\n        _, binary = cv2.threshold(self.standardGaussianHeat, region_threshold * 255, 255, 0)\n        np_contours = np.roll(np.array(np.where(binary != 0)), 1, axis=0).transpose().reshape(-1, 2)\n        x, y, w, h = cv2.boundingRect(np_contours)\n        self.regionbox = np.array([[x, y], [x + w, y], [x + w, y + h], [x, y + h]], dtype=np.int32)\n        # print(""regionbox"", self.regionbox)\n        _, binary = cv2.threshold(self.standardGaussianHeat, affinity_threshold * 255, 255, 0)\n        np_contours = np.roll(np.array(np.where(binary != 0)), 1, axis=0).transpose().reshape(-1, 2)\n        x, y, w, h = cv2.boundingRect(np_contours)\n        self.affinitybox = np.array([[x, y], [x + w, y], [x + w, y + h], [x, y + h]], dtype=np.int32)\n        # print(""affinitybox"", self.affinitybox)\n        self.oribox = np.array([[0, 0, 1], [imgSize - 1, 0, 1], [imgSize - 1, imgSize - 1, 1], [0, imgSize - 1, 1]],\n                               dtype=np.int32)\n\n    def _gen_gaussian_heatmap(self, imgSize, distanceRatio):\n        scaledGaussian = lambda x: exp(-(1 / 2) * (x ** 2))\n        heat = np.zeros((imgSize, imgSize), np.uint8)\n        for i in range(imgSize):\n            for j in range(imgSize):\n                distanceFromCenter = np.linalg.norm(np.array([i - imgSize / 2, j - imgSize / 2]))\n                distanceFromCenter = distanceRatio * distanceFromCenter / (imgSize / 2)\n                scaledGaussianProb = scaledGaussian(distanceFromCenter)\n                heat[i, j] = np.clip(scaledGaussianProb * 255, 0, 255)\n        return heat\n\n    def _test(self):\n        sigma = 10\n        spread = 3\n        extent = int(spread * sigma)\n        center = spread * sigma / 2\n        gaussian_heatmap = np.zeros([extent, extent], dtype=np.float32)\n\n        for i_ in range(extent):\n            for j_ in range(extent):\n                gaussian_heatmap[i_, j_] = 1 / 2 / np.pi / (sigma ** 2) * np.exp(\n                    -1 / 2 * ((i_ - center - 0.5) ** 2 + (j_ - center - 0.5) ** 2) / (sigma ** 2))\n\n        gaussian_heatmap = (gaussian_heatmap / np.max(gaussian_heatmap) * 255).astype(np.uint8)\n        images_folder = os.path.abspath(os.path.dirname(__file__)) + \'/images\'\n        threshhold_guassian = cv2.applyColorMap(gaussian_heatmap, cv2.COLORMAP_JET)\n        cv2.imwrite(os.path.join(images_folder, \'test_guassian.jpg\'), threshhold_guassian)\n\n    def add_region_character(self, image, target_bbox, regionbox=None):\n\n        if np.any(target_bbox < 0) or np.any(target_bbox[:, 0] > image.shape[1]) or np.any(\n                target_bbox[:, 1] > image.shape[0]):\n            return image\n        affi = False\n        if regionbox is None:\n            regionbox = self.regionbox.copy()\n        else:\n            affi = True\n\n        M = cv2.getPerspectiveTransform(np.float32(regionbox), np.float32(target_bbox))\n        oribox = np.array(\n            [[[0, 0], [self.imgSize - 1, 0], [self.imgSize - 1, self.imgSize - 1], [0, self.imgSize - 1]]],\n            dtype=np.float32)\n        test1 = cv2.perspectiveTransform(np.array([regionbox], np.float32), M)[0]\n        real_target_box = cv2.perspectiveTransform(oribox, M)[0]\n        # print(""test\\ntarget_bbox"", target_bbox, ""\\ntest1"", test1, ""\\nreal_target_box"", real_target_box)\n        real_target_box = np.int32(real_target_box)\n        real_target_box[:, 0] = np.clip(real_target_box[:, 0], 0, image.shape[1])\n        real_target_box[:, 1] = np.clip(real_target_box[:, 1], 0, image.shape[0])\n\n        # warped = cv2.warpPerspective(self.standardGaussianHeat.copy(), M, (image.shape[1], image.shape[0]))\n        # warped = np.array(warped, np.uint8)\n        # image = np.where(warped > image, warped, image)\n        if np.any(target_bbox[0] < real_target_box[0]) or (\n                target_bbox[3, 0] < real_target_box[3, 0] or target_bbox[3, 1] > real_target_box[3, 1]) or (\n                target_bbox[1, 0] > real_target_box[1, 0] or target_bbox[1, 1] < real_target_box[1, 1]) or (\n                target_bbox[2, 0] > real_target_box[2, 0] or target_bbox[2, 1] > real_target_box[2, 1]):\n            # if False:\n            warped = cv2.warpPerspective(self.standardGaussianHeat.copy(), M, (image.shape[1], image.shape[0]))\n            warped = np.array(warped, np.uint8)\n            image = np.where(warped > image, warped, image)\n            # _M = cv2.getPerspectiveTransform(np.float32(regionbox), np.float32(_target_box))\n            # warped = cv2.warpPerspective(self.standardGaussianHeat.copy(), _M, (width, height))\n            # warped = np.array(warped, np.uint8)\n            #\n            # # if affi:\n            # # print(""warped"", warped.shape, real_target_box, target_bbox, _target_box)\n            # # cv2.imshow(""1123"", warped)\n            # # cv2.waitKey()\n            # image[ymin:ymax, xmin:xmax] = np.where(warped > image[ymin:ymax, xmin:xmax], warped,\n            #                                        image[ymin:ymax, xmin:xmax])\n        else:\n            xmin = real_target_box[:, 0].min()\n            xmax = real_target_box[:, 0].max()\n            ymin = real_target_box[:, 1].min()\n            ymax = real_target_box[:, 1].max()\n\n            width = xmax - xmin\n            height = ymax - ymin\n            _target_box = target_bbox.copy()\n            _target_box[:, 0] -= xmin\n            _target_box[:, 1] -= ymin\n            _M = cv2.getPerspectiveTransform(np.float32(regionbox), np.float32(_target_box))\n            warped = cv2.warpPerspective(self.standardGaussianHeat.copy(), _M, (width, height))\n            warped = np.array(warped, np.uint8)\n            if warped.shape[0] != (ymax - ymin) or warped.shape[1] != (xmax - xmin):\n                print(""region (%d:%d,%d:%d) warped shape (%d,%d)"" % (\n                    ymin, ymax, xmin, xmax, warped.shape[1], warped.shape[0]))\n                return image\n            # if affi:\n            # print(""warped"", warped.shape, real_target_box, target_bbox, _target_box)\n            # cv2.imshow(""1123"", warped)\n            # cv2.waitKey()\n            image[ymin:ymax, xmin:xmax] = np.where(warped > image[ymin:ymax, xmin:xmax], warped,\n                                                   image[ymin:ymax, xmin:xmax])\n        return image\n\n    def add_affinity_character(self, image, target_bbox):\n        return self.add_region_character(image, target_bbox, self.affinitybox)\n\n    def add_affinity(self, image, bbox_1, bbox_2):\n        center_1, center_2 = np.mean(bbox_1, axis=0), np.mean(bbox_2, axis=0)\n        tl = np.mean([bbox_1[0], bbox_1[1], center_1], axis=0)\n        bl = np.mean([bbox_1[2], bbox_1[3], center_1], axis=0)\n        tr = np.mean([bbox_2[0], bbox_2[1], center_2], axis=0)\n        br = np.mean([bbox_2[2], bbox_2[3], center_2], axis=0)\n\n        affinity = np.array([tl, tr, br, bl])\n\n        return self.add_affinity_character(image, affinity.copy()), np.expand_dims(affinity, axis=0)\n\n    def generate_region(self, image_size, bboxes):\n        height, width = image_size[0], image_size[1]\n        target = np.zeros([height, width], dtype=np.uint8)\n        for i in range(len(bboxes)):\n            character_bbox = np.array(bboxes[i].copy())\n            for j in range(bboxes[i].shape[0]):\n                target = self.add_region_character(target, character_bbox[j])\n\n        return target\n\n    def generate_affinity(self, image_size, bboxes, words):\n        height, width = image_size[0], image_size[1]\n        target = np.zeros([height, width], dtype=np.uint8)\n        affinities = []\n        for i in range(len(words)):\n            character_bbox = np.array(bboxes[i])\n            total_letters = 0\n            for char_num in range(character_bbox.shape[0] - 1):\n                target, affinity = self.add_affinity(target, character_bbox[total_letters],\n                                                     character_bbox[total_letters + 1])\n                affinities.append(affinity)\n                total_letters += 1\n        if len(affinities) > 0:\n            affinities = np.concatenate(affinities, axis=0)\n        return target, affinities\n\n    def saveGaussianHeat(self):\n        images_folder = os.path.abspath(os.path.dirname(__file__)) + \'/images\'\n        cv2.imwrite(os.path.join(images_folder, \'standard.jpg\'), self.standardGaussianHeat)\n        warped_color = cv2.applyColorMap(self.standardGaussianHeat, cv2.COLORMAP_JET)\n        cv2.polylines(warped_color, [np.reshape(self.regionbox, (-1, 1, 2))], True, (255, 255, 255), thickness=1)\n        cv2.imwrite(os.path.join(images_folder, \'standard_color.jpg\'), warped_color)\n        standardGaussianHeat1 = self.standardGaussianHeat.copy()\n        threshhold = self.region_threshold * 255\n        standardGaussianHeat1[standardGaussianHeat1 > 0] = 255\n        threshhold_guassian = cv2.applyColorMap(standardGaussianHeat1, cv2.COLORMAP_JET)\n        cv2.polylines(threshhold_guassian, [np.reshape(self.regionbox, (-1, 1, 2))], True, (255, 255, 255), thickness=1)\n        cv2.imwrite(os.path.join(images_folder, \'threshhold_guassian.jpg\'), threshhold_guassian)\n\n\nif __name__ == \'__main__\':\n    gaussian = GaussianTransformer(512, 0.4, 0.2)\n    gaussian.saveGaussianHeat()\n    gaussian._test()\n    bbox0 = np.array([[[0, 0], [100, 0], [100, 100], [0, 100]]])\n    image = np.zeros((500, 500), np.uint8)\n    # image = gaussian.add_region_character(image, bbox)\n    bbox1 = np.array([[[100, 0], [200, 0], [200, 100], [100, 100]]])\n    bbox2 = np.array([[[100, 100], [200, 100], [200, 200], [100, 200]]])\n    bbox3 = np.array([[[0, 100], [100, 100], [100, 200], [0, 200]]])\n\n    bbox4 = np.array([[[96, 0], [151, 9], [139, 64], [83, 58]]])\n    # image = gaussian.add_region_character(image, bbox)\n    # print(image.max())\n    image = gaussian.generate_region((500, 500, 1), [bbox4])\n    target_gaussian_heatmap_color = imgproc.cvt2HeatmapImg(image.copy() / 255)\n    cv2.imshow(""test"", target_gaussian_heatmap_color)\n    cv2.imwrite(""test.jpg"", target_gaussian_heatmap_color)\n    cv2.waitKey()\n    # weight, target = gaussian.generate_target((1024, 1024, 3), bbox.copy())\n    # target_gaussian_heatmap_color = imgproc.cvt2HeatmapImg(weight.copy() / 255)\n    # cv2.imshow(\'test\', target_gaussian_heatmap_color)\n    # cv2.waitKey()\n    # cv2.imwrite(""test.jpg"", target_gaussian_heatmap_color)\n\n\n\n\n# # coding=utf-8\n# from math import exp\n# import numpy as np\n# import cv2\n# import os\n# import imgproc\n#\n#\n# class GaussianTransformer(object):\n#\n#     def __init__(self, imgSize=512, distanceRatio=1.70):\n#         scaledGaussian = lambda x: exp(-(1 / 2) * (x ** 2))\n#\n#         self.standardGaussianHeat = np.zeros((imgSize, imgSize), np.uint8)\n#\n#         for i in range(imgSize):\n#             for j in range(imgSize):\n#                 distanceFromCenter = np.linalg.norm(np.array([i - imgSize / 2, j - imgSize / 2]))\n#                 distanceFromCenter = distanceRatio * distanceFromCenter / (imgSize / 2)\n#                 scaledGaussianProb = scaledGaussian(distanceFromCenter)\n#\n#                 self.standardGaussianHeat[i, j] = np.clip(scaledGaussianProb * 255, 0, 255)\n#         #print(""gaussian heatmap min pixel is"", self.standardGaussianHeat.min() / 255)\n#         # self.standardGaussianHeat[self.standardGaussianHeat < (0.4 * 255)] = 255\n#         self._test()\n#\n#     def _test(self):\n#         sigma = 10\n#         spread = 3\n#         extent = int(spread * sigma)\n#         center = spread * sigma / 2\n#         gaussian_heatmap = np.zeros([extent, extent], dtype=np.float32)\n#\n#         for i_ in range(extent):\n#             for j_ in range(extent):\n#                 gaussian_heatmap[i_, j_] = 1 / 2 / np.pi / (sigma ** 2) * np.exp(\n#                     -1 / 2 * ((i_ - center - 0.5) ** 2 + (j_ - center - 0.5) ** 2) / (sigma ** 2))\n#\n#         gaussian_heatmap = (gaussian_heatmap / np.max(gaussian_heatmap) * 255).astype(np.uint8)\n#         images_folder = os.path.abspath(os.path.dirname(__file__)) + \'/images\'\n#         threshhold_guassian = cv2.applyColorMap(gaussian_heatmap, cv2.COLORMAP_JET)\n#         cv2.imwrite(os.path.join(images_folder, \'test_guassian.jpg\'), threshhold_guassian)\n#\n#     def four_point_transform(self, target_bbox, save_dir=None):\n#         \'\'\'\n#\n#         :param target_bbox:\xe7\x9b\xae\xe6\xa0\x87bbox\n#         :param save_dir:\xe5\xa6\x82\xe6\x9e\x9c\xe4\xb8\x8d\xe6\x98\xafNone\xef\xbc\x8c\xe5\x88\x99\xe4\xbf\x9d\xe5\xad\x98\xe5\x9b\xbe\xe7\x89\x87\xe5\x88\xb0save_dir\xe4\xb8\xad\n#         :return:\n#         \'\'\'\n#         width, height = np.max(target_bbox[:, 0]).astype(np.int32), np.max(target_bbox[:, 1]).astype(np.int32)\n#\n#         right = self.standardGaussianHeat.shape[1] - 1\n#         bottom = self.standardGaussianHeat.shape[0] - 1\n#         ori = np.array([[0, 0], [right, 0],\n#                         [right, bottom],\n#                         [0, bottom]], dtype=""float32"")\n#         M = cv2.getPerspectiveTransform(ori, target_bbox)\n#         warped = cv2.warpPerspective(self.standardGaussianHeat.copy(), M, (width, height))\n#         warped = np.array(warped, np.uint8)\n#         if save_dir:\n#             warped_color = cv2.applyColorMap(warped, cv2.COLORMAP_JET)\n#             cv2.imwrite(os.path.join(save_dir, \'warped.jpg\'), warped_color)\n#         #print(warped.shape,(width, height))\n#\n#         return warped, width, height\n#\n#     def add_character(self, image, bbox):\n#         if np.any(bbox < 0) or np.any(bbox[:, 0] > image.shape[1]) or np.any(bbox[:, 1] > image.shape[0]):\n#             return image\n#         top_left = np.array([np.min(bbox[:, 0]), np.min(bbox[:, 1])]).astype(np.int32)\n#         bbox -= top_left[None, :]\n#         transformed, width, height = self.four_point_transform(bbox.astype(np.float32))\n#         if width * height < 10:\n#             return image\n#\n#         try:\n#             score_map = image[top_left[1]:top_left[1] + transformed.shape[0],\n#                         top_left[0]:top_left[0] + transformed.shape[1]]\n#             score_map = np.where(transformed > score_map, transformed, score_map)\n#             image[top_left[1]:top_left[1] + transformed.shape[0],\n#             top_left[0]:top_left[0] + transformed.shape[1]] = score_map\n#         except Exception as e:\n#             print(e)\n#         return image\n#\n#     def add_affinity(self, image, bbox_1, bbox_2):\n#         center_1, center_2 = np.mean(bbox_1, axis=0), np.mean(bbox_2, axis=0)\n#         tl = np.mean([bbox_1[0], bbox_1[1], center_1], axis=0)\n#         bl = np.mean([bbox_1[2], bbox_1[3], center_1], axis=0)\n#         tr = np.mean([bbox_2[0], bbox_2[1], center_2], axis=0)\n#         br = np.mean([bbox_2[2], bbox_2[3], center_2], axis=0)\n#\n#         affinity = np.array([tl, tr, br, bl])\n#\n#         return self.add_character(image, affinity.copy()), np.expand_dims(affinity, axis=0)\n#\n#     def generate_region(self, image_size, bboxes):\n#         height, width, channel = image_size\n#         target = np.zeros([height, width], dtype=np.uint8)\n#         for i in range(len(bboxes)):\n#             character_bbox = np.array(bboxes[i])\n#             for j in range(bboxes[i].shape[0]):\n#                 target = self.add_character(target, character_bbox[j])\n#\n#         return target\n#\n#     def saveGaussianHeat(self):\n#         images_folder = os.path.abspath(os.path.dirname(__file__)) + \'/images\'\n#         cv2.imwrite(os.path.join(images_folder, \'standard.jpg\'), self.standardGaussianHeat)\n#         warped_color = cv2.applyColorMap(self.standardGaussianHeat, cv2.COLORMAP_JET)\n#         cv2.imwrite(os.path.join(images_folder, \'standard_color.jpg\'), warped_color)\n#         standardGaussianHeat1 = self.standardGaussianHeat.copy()\n#         standardGaussianHeat1[standardGaussianHeat1 < (0.4 * 255)] = 255\n#         threshhold_guassian = cv2.applyColorMap(standardGaussianHeat1, cv2.COLORMAP_JET)\n#         cv2.imwrite(os.path.join(images_folder, \'threshhold_guassian.jpg\'), threshhold_guassian)\n#\n#     def generate_affinity(self, image_size, bboxes, words):\n#         height, width, channel = image_size\n#\n#         target = np.zeros([height, width], dtype=np.uint8)\n#         affinities = []\n#         for i in range(len(words)):\n#             character_bbox = np.array(bboxes[i])\n#             total_letters = 0\n#             for char_num in range(character_bbox.shape[0] - 1):\n#                 target, affinity = self.add_affinity(target, character_bbox[total_letters],\n#                                                      character_bbox[total_letters + 1])\n#                 affinities.append(affinity)\n#                 total_letters += 1\n#         if len(affinities) > 0:\n#             affinities = np.concatenate(affinities, axis=0)\n#         return target, affinities\n#\n#\n# if __name__ == \'__main__\':\n#     gaussian = GaussianTransformer(1024, 1.5)\n#     gaussian.saveGaussianHeat()\n#\n#     bbox = np.array([[[1, 200], [510, 200], [510, 510], [1, 510]]])\n#     print(bbox.shape)\n#     bbox = bbox.transpose((2, 1, 0))\n#     print(bbox.shape)\n#     weight, target = gaussian.generate_target((1024, 1024, 3), bbox.copy())\n#     target_gaussian_heatmap_color = imgproc.cvt2HeatmapImg(weight.copy() / 255)\n#     cv2.imshow(\'test\', target_gaussian_heatmap_color)\n#     cv2.waitKey()\n#     cv2.imwrite(""test.jpg"", target_gaussian_heatmap_color)\n\n\n\n# coding=utf-8\n# coding=utf-8\n'"
imgproc.py,0,"b'""""""  \nCopyright (c) 2019-present NAVER Corp.\nMIT License\n""""""\n\n# -*- coding: utf-8 -*-\nimport numpy as np\nfrom skimage import io\nimport cv2\n\ndef loadImage(img_file):\n    img = io.imread(img_file)           # RGB order\n    if img.shape[0] == 2: img = img[0]\n    if len(img.shape) == 2 : img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n    if img.shape[2] == 4:   img = img[:,:,:3]\n    img = np.array(img)\n\n    return img\n\ndef normalizeMeanVariance(in_img, mean=(0.485, 0.456, 0.406), variance=(0.229, 0.224, 0.225)):\n    # should be RGB order\n    img = in_img.copy().astype(np.float32)\n\n    img -= np.array([mean[0] * 255.0, mean[1] * 255.0, mean[2] * 255.0], dtype=np.float32)\n    img /= np.array([variance[0] * 255.0, variance[1] * 255.0, variance[2] * 255.0], dtype=np.float32)\n    return img\n\ndef denormalizeMeanVariance(in_img, mean=(0.485, 0.456, 0.406), variance=(0.229, 0.224, 0.225)):\n    # should be RGB order\n    img = in_img.copy()\n    img *= variance\n    img += mean\n    img *= 255.0\n    img = np.clip(img, 0, 255).astype(np.uint8)\n    return img\n\ndef resize_aspect_ratio(img, square_size, interpolation, mag_ratio=1):\n    height, width, channel = img.shape\n\n    # magnify image size\n    target_size = mag_ratio * max(height, width)\n\n    # set original image size\n    if target_size > square_size:\n        target_size = square_size\n    \n    ratio = target_size / max(height, width)    \n\n    target_h, target_w = int(height * ratio), int(width * ratio)\n    proc = cv2.resize(img, (target_w, target_h), interpolation = interpolation)\n\n\n    # make canvas and paste image\n    target_h32, target_w32 = target_h, target_w\n    if target_h % 32 != 0:\n        target_h32 = target_h + (32 - target_h % 32)\n    if target_w % 32 != 0:\n        target_w32 = target_w + (32 - target_w % 32)\n    resized = np.zeros((target_h32, target_w32, channel), dtype=np.float32)\n    resized[0:target_h, 0:target_w, :] = proc\n    target_h, target_w = target_h32, target_w32\n\n    size_heatmap = (int(target_w/2), int(target_h/2))\n\n    return resized, ratio, size_heatmap\n\ndef cvt2HeatmapImg(img):\n    img = (np.clip(img, 0, 1) * 255).astype(np.uint8)\n    img = cv2.applyColorMap(img, cv2.COLORMAP_JET)\n    return img\n'"
mep.py,0,"b'import math\nimport numpy as np\n\n""""""\nMinimal Enclosing Parallelogram\n\narea, v1, v2, v3, v4 = mep(convex_polygon)\n\nconvex_polygon - array of points. Each point is a array [x, y] (1d array of 2 elements)\npoints should be presented in clockwise order.\n\nthe algorithm used is described in the following paper:\nhttp://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.53.9659&rep=rep1&type=pdf\n""""""\n\n\ndef distance(p1, p2, p):\n    return abs(((p2[1] - p1[1]) * p[0] - (p2[0] - p1[0]) * p[1] + p2[0] * p1[1] - p2[1] * p1[0]) /\n               math.sqrt((p2[1] - p1[1]) ** 2 + (p2[0] - p1[0]) ** 2))\n\n\ndef antipodal_pairs(convex_polygon):\n    l = []\n    n = len(convex_polygon)\n    p1, p2 = convex_polygon[0], convex_polygon[1]\n\n    t, d_max = None, 0\n    for p in range(1, n):\n        d = distance(p1, p2, convex_polygon[p])\n        if d > d_max:\n            t, d_max = p, d\n    l.append(t)\n\n    for p in range(1, n):\n        p1, p2 = convex_polygon[p % n], convex_polygon[(p + 1) % n]\n        _p, _pp = convex_polygon[t % n], convex_polygon[(t + 1) % n]\n        while distance(p1, p2, _pp) > distance(p1, p2, _p):\n            t = (t + 1) % n\n            _p, _pp = convex_polygon[t % n], convex_polygon[(t + 1) % n]\n        l.append(t)\n\n    return l\n\n\n# returns score, area, points from top-left, clockwise , favouring low area\ndef mep(convex_polygon):\n    def compute_parallelogram(convex_polygon, l, z1, z2):\n        def parallel_vector(a, b, c):\n            v0 = [c[0] - a[0], c[1] - a[1]]\n            v1 = [b[0] - c[0], b[1] - c[1]]\n            return [c[0] - v0[0] - v1[0], c[1] - v0[1] - v1[1]]\n\n        # finds intersection between lines, given 2 points on each line.\n        # (x1, y1), (x2, y2) on 1st line, (x3, y3), (x4, y4) on 2nd line.\n        def line_intersection(x1, y1, x2, y2, x3, y3, x4, y4):\n            px = ((x1 * y2 - y1 * x2) * (x3 - x4) - (x1 - x2) * (x3 * y4 - y3 * x4)) / (\n                    (x1 - x2) * (y3 - y4) - (y1 - y2) * (x3 - x4))\n            py = ((x1 * y2 - y1 * x2) * (y3 - y4) - (y1 - y2) * (x3 * y4 - y3 * x4)) / (\n                    (x1 - x2) * (y3 - y4) - (y1 - y2) * (x3 - x4))\n            return px, py\n\n        # from each antipodal point, draw a parallel vector,\n        # so ap1->ap2 is parallel to p1->p2\n        #    aq1->aq2 is parallel to q1->q2\n        p1, p2 = convex_polygon[z1 % n], convex_polygon[(z1 + 1) % n]\n        q1, q2 = convex_polygon[z2 % n], convex_polygon[(z2 + 1) % n]\n        ap1, aq1 = convex_polygon[l[z1 % n]], convex_polygon[l[z2 % n]]\n        ap2, aq2 = parallel_vector(p1, p2, ap1), parallel_vector(q1, q2, aq1)\n\n        a = line_intersection(p1[0], p1[1], p2[0], p2[1], q1[0], q1[1], q2[0], q2[1])\n        b = line_intersection(p1[0], p1[1], p2[0], p2[1], aq1[0], aq1[1], aq2[0], aq2[1])\n        d = line_intersection(ap1[0], ap1[1], ap2[0], ap2[1], q1[0], q1[1], q2[0], q2[1])\n        c = line_intersection(ap1[0], ap1[1], ap2[0], ap2[1], aq1[0], aq1[1], aq2[0], aq2[1])\n\n        s = distance(a, b, c) * math.sqrt((b[0] - a[0]) ** 2 + (b[1] - a[1]) ** 2)\n        return s, a, b, c, d\n\n    z1, z2 = 0, 0\n    n = len(convex_polygon)\n\n    # for each edge, find antipodal vertice for it (step 1 in paper).\n    l = antipodal_pairs(convex_polygon)\n\n    so, ao, bo, co, do, z1o, z2o = 100000000000, None, None, None, None, None, None\n\n    # step 2 in paper.\n    for z1 in range(0, n):\n        if z1 >= z2:\n            z2 = z1 + 1\n        p1, p2 = convex_polygon[z1 % n], convex_polygon[(z1 + 1) % n]\n        a, b, c = convex_polygon[z2 % n], convex_polygon[(z2 + 1) % n], convex_polygon[l[z2 % n]]\n        if distance(p1, p2, a) >= distance(p1, p2, b):\n            continue\n\n        while distance(p1, p2, c) > distance(p1, p2, b):\n            z2 += 1\n            a, b, c = convex_polygon[z2 % n], convex_polygon[(z2 + 1) % n], convex_polygon[l[z2 % n]]\n\n        st, at, bt, ct, dt = compute_parallelogram(convex_polygon, l, z1, z2)\n\n        if st < so:\n            so, ao, bo, co, do, z1o, z2o = st, at, bt, ct, dt, z1, z2\n\n    return so, ao, bo, co, do, z1o, z2o\n\n\ndef sort_rectangle(poly):\n    # sort the four coordinates of the polygon, points in poly should be sorted clockwise\n    # First find the lowest point\n    p_lowest = np.argmax(poly[:, 1])\n    if np.count_nonzero(poly[:, 1] == poly[p_lowest, 1]) == 2:\n        # - if the bottom line is parallel to x-axis, then p0 must be the upper-left corner\n        p0_index = np.argmin(np.sum(poly, axis=1))\n        p1_index = (p0_index + 1) % 4\n        p2_index = (p0_index + 2) % 4\n        p3_index = (p0_index + 3) % 4\n        return poly[[p0_index, p1_index, p2_index, p3_index]], 0.\n    else:\n        #- find the point that sits right to the lowest point\n        p_lowest_right = (p_lowest - 1) % 4\n        p_lowest_left = (p_lowest + 1) % 4\n        angle = np.arctan(\n            -(poly[p_lowest][1] - poly[p_lowest_right][1]) / (poly[p_lowest][0] - poly[p_lowest_right][0]))\n        # assert angle > 0\n        # if angle <= 0:\n        #     print(angle, poly[p_lowest], poly[p_lowest_right])\n        if angle / np.pi * 180 > 45:\n            #p2 - this point is p2\n            p2_index = p_lowest\n            p1_index = (p2_index - 1) % 4\n            p0_index = (p2_index - 2) % 4\n            p3_index = (p2_index + 1) % 4\n            return poly[[p0_index, p1_index, p2_index, p3_index]], -(np.pi / 2 - angle)\n        else:\n            #p3 - this point is p3\n            p3_index = p_lowest\n            p0_index = (p3_index + 1) % 4\n            p1_index = (p3_index + 2) % 4\n            p2_index = (p3_index + 3) % 4\n            return poly[[p0_index, p1_index, p2_index, p3_index]], angle\n'"
mseloss.py,10,"b'import numpy as np\nimport torch\nimport torch.nn as nn\n\n\nclass Maploss(nn.Module):\n    def __init__(self, use_gpu = True):\n\n        super(Maploss,self).__init__()\n\n    def single_image_loss(self, pre_loss, loss_label):\n        batch_size = pre_loss.shape[0]\n        sum_loss = torch.mean(pre_loss.view(-1))*0\n        pre_loss = pre_loss.view(batch_size, -1)\n        loss_label = loss_label.view(batch_size, -1)\n        internel = batch_size\n        for i in range(batch_size):\n            average_number = 0\n            loss = torch.mean(pre_loss.view(-1)) * 0\n            positive_pixel = len(pre_loss[i][(loss_label[i] >= 0.1)])\n            average_number += positive_pixel\n            if positive_pixel != 0:\n                posi_loss = torch.mean(pre_loss[i][(loss_label[i] >= 0.1)])\n                sum_loss += posi_loss\n                if len(pre_loss[i][(loss_label[i] < 0.1)]) < 3*positive_pixel:\n                    nega_loss = torch.mean(pre_loss[i][(loss_label[i] < 0.1)])\n                    average_number += len(pre_loss[i][(loss_label[i] < 0.1)])\n                else:\n                    nega_loss = torch.mean(torch.topk(pre_loss[i][(loss_label[i] < 0.1)], 3*positive_pixel)[0])\n                    average_number += 3*positive_pixel\n                sum_loss += nega_loss\n            else:\n                nega_loss = torch.mean(torch.topk(pre_loss[i], 500)[0])\n                average_number += 500\n                sum_loss += nega_loss\n            #sum_loss += loss/average_number\n\n        return sum_loss\n\n\n\n    def forward(self, gh_label, gah_label, p_gh, p_gah, mask):\n        gh_label = gh_label\n        gah_label = gah_label\n        p_gh = p_gh\n        p_gah = p_gah\n        loss_fn = torch.nn.MSELoss(reduce=False, size_average=False)\n\n        assert p_gh.size() == gh_label.size() and p_gah.size() == gah_label.size()\n        loss1 = loss_fn(p_gh, gh_label)\n        loss2 = loss_fn(p_gah, gah_label)\n        loss_g = torch.mul(loss1, mask)\n        loss_a = torch.mul(loss2, mask)\n\n        char_loss = self.single_image_loss(loss_g, gh_label)\n        affi_loss = self.single_image_loss(loss_a, gah_label)\n        return char_loss/loss_g.shape[0] + affi_loss/loss_a.shape[0]'"
test.py,7,"b'""""""  \nCopyright (c) 2019-present NAVER Corp.\nMIT License\n""""""\n\n# -*- coding: utf-8 -*-\nimport sys\nimport os\nimport time\nimport argparse\n\nimport torch\nimport torch.nn as nn\nimport torch.backends.cudnn as cudnn\nfrom torch.autograd import Variable\n\nfrom PIL import Image\n\nimport cv2\nfrom skimage import io\nimport numpy as np\nimport craft_utils\nimport imgproc\nimport file_utils\nimport json\nimport zipfile\n\nfrom craft import CRAFT\n\nfrom collections import OrderedDict\ndef copyStateDict(state_dict):\n    if list(state_dict.keys())[0].startswith(""module""):\n        start_idx = 1\n    else:\n        start_idx = 0\n    new_state_dict = OrderedDict()\n    for k, v in state_dict.items():\n        name = ""."".join(k.split(""."")[start_idx:])\n        new_state_dict[name] = v\n    return new_state_dict\n\ndef str2bool(v):\n    return v.lower() in (""yes"", ""y"", ""true"", ""t"", ""1"")\n\nparser = argparse.ArgumentParser(description=\'CRAFT Text Detection\')\nparser.add_argument(\'--trained_model\', default=\'weights/craft_mlt_25k.pth\', type=str, help=\'pretrained model\')\nparser.add_argument(\'--text_threshold\', default=0.7, type=float, help=\'text confidence threshold\')\nparser.add_argument(\'--low_text\', default=0.4, type=float, help=\'text low-bound score\')\nparser.add_argument(\'--link_threshold\', default=0.4, type=float, help=\'link confidence threshold\')\nparser.add_argument(\'--cuda\', default=True, type=str2bool, help=\'Use cuda to train model\')\nparser.add_argument(\'--canvas_size\', default=2240, type=int, help=\'image size for inference\')\nparser.add_argument(\'--mag_ratio\', default=2, type=float, help=\'image magnification ratio\')\nparser.add_argument(\'--poly\', default=False, action=\'store_true\', help=\'enable polygon type\')\nparser.add_argument(\'--show_time\', default=False, action=\'store_true\', help=\'show processing time\')\nparser.add_argument(\'--test_folder\', default=\'/data/\', type=str, help=\'folder path to input images\')\n\nargs = parser.parse_args()\n\n\n"""""" For test images in a folder """"""\nimage_list, _, _ = file_utils.get_files(\'/data/CRAFT-pytorch/test\')\n\nresult_folder = \'/data/CRAFT-pytorch/result/\'\nif not os.path.isdir(result_folder):\n    os.mkdir(result_folder)\n\ndef test_net(net, image, text_threshold, link_threshold, low_text, cuda, poly):\n    t0 = time.time()\n\n    # resize\n    img_resized, target_ratio, size_heatmap = imgproc.resize_aspect_ratio(image, args.canvas_size, interpolation=cv2.INTER_LINEAR, mag_ratio=args.mag_ratio)\n    ratio_h = ratio_w = 1 / target_ratio\n\n    # preprocessing\n    x = imgproc.normalizeMeanVariance(img_resized)\n    x = torch.from_numpy(x).permute(2, 0, 1)    # [h, w, c] to [c, h, w]\n    x = Variable(x.unsqueeze(0))                # [c, h, w] to [b, c, h, w]\n    if cuda:\n        x = x.cuda()\n\n    # forward pass\n    y, _ = net(x)\n\n    # make score and link map\n    score_text = y[0,:,:,0].cpu().data.numpy()\n    score_link = y[0,:,:,1].cpu().data.numpy()\n\n    t0 = time.time() - t0\n    t1 = time.time()\n\n    # Post-processing\n    boxes, polys = craft_utils.getDetBoxes(score_text, score_link, text_threshold, link_threshold, low_text, poly)\n\n    # coordinate adjustment\n    boxes = craft_utils.adjustResultCoordinates(boxes, ratio_w, ratio_h)\n    polys = craft_utils.adjustResultCoordinates(polys, ratio_w, ratio_h)\n    for k in range(len(polys)):\n        if polys[k] is None: polys[k] = boxes[k]\n\n    t1 = time.time() - t1\n\n    # render results (optional)\n    render_img = score_text.copy()\n    render_img = np.hstack((render_img, score_link))\n    ret_score_text = imgproc.cvt2HeatmapImg(render_img)\n\n    if args.show_time : print(""\\ninfer/postproc time : {:.3f}/{:.3f}"".format(t0, t1))\n\n    return boxes, polys, ret_score_text\n\n\n\ndef test(modelpara):\n    # load net\n    net = CRAFT()     # initialize\n\n    print(\'Loading weights from checkpoint {}\'.format(modelpara))\n    if args.cuda:\n        net.load_state_dict(copyStateDict(torch.load(modelpara)))\n    else:\n        net.load_state_dict(copyStateDict(torch.load(modelpara, map_location=\'cpu\')))\n\n    if args.cuda:\n        net = net.cuda()\n        net = torch.nn.DataParallel(net)\n        cudnn.benchmark = False\n\n    net.eval()\n\n    t = time.time()\n\n    # load data\n    for k, image_path in enumerate(image_list):\n        print(""Test image {:d}/{:d}: {:s}"".format(k+1, len(image_list), image_path), end=\'\\r\')\n        image = imgproc.loadImage(image_path)\n\n        bboxes, polys, score_text = test_net(net, image, args.text_threshold, args.link_threshold, args.low_text, args.cuda, args.poly)\n        # save score text\n        filename, file_ext = os.path.splitext(os.path.basename(image_path))\n        mask_file = result_folder + ""/res_"" + filename + \'_mask.jpg\'\n        #cv2.imwrite(mask_file, score_text)\n\n        file_utils.saveResult(image_path, image[:,:,::-1], polys, dirname=result_folder)\n\n    print(""elapsed time : {}s"".format(time.time() - t))\n'"
torchutil.py,2,"b'# coding=utf-8\nfrom collections import OrderedDict\nimport torch.nn as nn\nimport torch.nn.init as init\n\n\ndef copyStateDict(state_dict):\n    if list(state_dict.keys())[0].startswith(""module""):\n        start_idx = 1\n    else:\n        start_idx = 0\n    new_state_dict = OrderedDict()\n    for k, v in state_dict.items():\n        name = ""."".join(k.split(""."")[start_idx:])\n        new_state_dict[name] = v\n    return new_state_dict\n\n\ndef init_weights(modules):\n    for m in modules:\n        if isinstance(m, nn.Conv2d):\n            init.xavier_uniform_(m.weight.data)\n            if m.bias is not None:\n                m.bias.data.zero_()\n        elif isinstance(m, nn.BatchNorm2d):\n            m.weight.data.fill_(1)\n            m.bias.data.zero_()\n        elif isinstance(m, nn.Linear):\n            m.weight.data.normal_(0, 0.01)\n            m.bias.data.zero_()\n'"
train-MLT_data.py,27,"b'import os\nimport sys\nimport torch\nimport torch.utils.data as data\nimport cv2\nimport numpy as np\nimport scipy.io as scio\nimport argparse\nimport time\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.backends.cudnn as cudnn\nimport torch.optim as optim\nimport random\nimport h5py\nimport re\nimport water\n\nfrom data_loader import ICDAR2015, Synth80k, ICDAR2013\n\nfrom test import test\n\n\nfrom math import exp\n\n###import file#######\nfrom augmentation import random_rot, crop_img_bboxes\nfrom gaussianmap import gaussion_transform, four_point_transform\nfrom generateheatmap import add_character, generate_target, add_affinity, generate_affinity, sort_box, real_affinity, generate_affinity_box\nfrom mseloss import Maploss\n\n\n\nfrom collections import OrderedDict\nfrom eval13.script import getresult\n\n\n\nfrom PIL import Image\nfrom torchvision.transforms import transforms\nfrom craft import CRAFT\nfrom torch.autograd import Variable\nfrom multiprocessing import Pool\n\n#3.2768e-5\nrandom.seed(42)\n\n# class SynAnnotationTransform(object):\n#     def __init__(self):\n#         pass\n#     def __call__(self, gt):\n#         image_name = gt[\'imnames\'][0]\nparser = argparse.ArgumentParser(description=\'CRAFT reimplementation\')\n\n\nparser.add_argument(\'--resume\', default=None, type=str,\n                    help=\'Checkpoint state_dict file to resume training from\')\nparser.add_argument(\'--batch_size\', default=128, type = int,\n                    help=\'batch size of training\')\n#parser.add_argument(\'--cdua\', default=True, type=str2bool,\n                    #help=\'Use CUDA to train model\')\nparser.add_argument(\'--lr\', \'--learning-rate\', default=3.2768e-5, type=float,\n                    help=\'initial learning rate\')\nparser.add_argument(\'--momentum\', default=0.9, type=float,\n                    help=\'Momentum value for optim\')\nparser.add_argument(\'--weight_decay\', default=5e-4, type=float,\n                    help=\'Weight decay for SGD\')\nparser.add_argument(\'--gamma\', default=0.1, type=float,\n                    help=\'Gamma update for SGD\')\nparser.add_argument(\'--num_workers\', default=32, type=int,\n                    help=\'Number of workers used in dataloading\')\n\n\nargs = parser.parse_args()\n\n\n\ndef copyStateDict(state_dict):\n    if list(state_dict.keys())[0].startswith(""module""):\n        start_idx = 1\n    else:\n        start_idx = 0\n    new_state_dict = OrderedDict()\n    for k, v in state_dict.items():\n        name = ""."".join(k.split(""."")[start_idx:])\n        new_state_dict[name] = v\n    return new_state_dict\n\n\n\n\n\n\n\n\ndef adjust_learning_rate(optimizer, gamma, step):\n    """"""Sets the learning rate to the initial LR decayed by 10 at every\n        specified step\n    # Adapted from PyTorch Imagenet example:\n    # https://github.com/pytorch/examples/blob/master/imagenet/main.py\n    """"""\n    lr = args.lr * (0.8 ** step)\n    print(lr)\n    for param_group in optimizer.param_groups:\n        param_group[\'lr\'] = lr\n\n\nif __name__ == \'__main__\':\n\n    # gaussian = gaussion_transform()\n    # box = scio.loadmat(\'/data/CRAFT-pytorch/SynthText/gt.mat\')\n    # bbox = box[\'wordBB\'][0][0][0]\n    # charbox = box[\'charBB\'][0]\n    # imgname = box[\'imnames\'][0]\n    # imgtxt = box[\'txt\'][0]\n    #\n    # dataloader = syndata(imgname, charbox, imgtxt)\n    dataloader = Synth80k(\'/data/CRAFT-pytorch/SynthText\', target_size = 768)\n    train_loader = torch.utils.data.DataLoader(\n        dataloader,\n        batch_size=2,\n        shuffle=True,\n        num_workers=0,\n        drop_last=True,\n        pin_memory=True)\n    batch_syn = iter(train_loader)\n    # prefetcher = data_prefetcher(dataloader)\n    # input, target1, target2 = prefetcher.next()\n    #print(input.size())\n    net = CRAFT()\n    #net.load_state_dict(copyStateDict(torch.load(\'/data/CRAFT-pytorch/CRAFT_net_050000.pth\')))\n    net.load_state_dict(copyStateDict(torch.load(\'/data/CRAFT-pytorch/1-7.pth\')))\n    #net.load_state_dict(copyStateDict(torch.load(\'/data/CRAFT-pytorch/craft_mlt_25k.pth\')))\n    #net.load_state_dict(copyStateDict(torch.load(\'/data/CRAFT-pytorch/synweights/syn_0_20000.pth\')))\n    #realdata = realdata(net)\n\n    net = net.cuda()\n    #net = CRAFT_net\n\n    # if args.cdua:\n    net = torch.nn.DataParallel(net,device_ids=[0,1,2,3]).cuda()\n    net.train()\n\n    cudnn.benchmark = False\n    realdata = ICDAR2013(net, \'/data/CRAFT-pytorch/icdar1317\', target_size = 768, viz = False)\n    real_data_loader = torch.utils.data.DataLoader(\n        realdata,\n        batch_size=10,\n        shuffle=True,\n        num_workers=0,\n        drop_last=True,\n        pin_memory=True)\n\n\n    #net.train()\n    optimizer = optim.Adam(net.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n    criterion = Maploss()\n    #criterion = torch.nn.MSELoss(reduce=True, size_average=True)\n    #net.train()\n\n\n    step_index = 0\n\n\n    loss_time = 0\n    loss_value = 0\n    compare_loss = 1\n    for epoch in range(1000):\n        train_time_st = time.time()\n        loss_value = 0\n        if epoch % 27 == 0 and epoch != 0:\n            step_index += 1\n            adjust_learning_rate(optimizer, args.gamma, step_index)\n\n        st = time.time()\n        for index, (real_images, real_gh_label, real_gah_label, real_mask, _) in enumerate(real_data_loader):\n            #net.train()\n            #real_images, real_gh_label, real_gah_label, real_mask = next(batch_real)\n            syn_images, syn_gh_label, syn_gah_label, syn_mask, __ = next(batch_syn)\n            #net.train()\n            images = torch.cat((syn_images,real_images), 0)\n            gh_label = torch.cat((syn_gh_label, real_gh_label), 0)\n            gah_label = torch.cat((syn_gah_label, real_gah_label), 0)\n            mask = torch.cat((syn_mask, real_mask), 0)\n            #affinity_mask = torch.cat((syn_mask, real_affinity_mask), 0)\n\n\n            images = Variable(images.type(torch.FloatTensor)).cuda()\n            gh_label = gh_label.type(torch.FloatTensor)\n            gah_label = gah_label.type(torch.FloatTensor)\n            gh_label = Variable(gh_label).cuda()\n            gah_label = Variable(gah_label).cuda()\n            mask = mask.type(torch.FloatTensor)\n            mask = Variable(mask).cuda()\n            # affinity_mask = affinity_mask.type(torch.FloatTensor)\n            # affinity_mask = Variable(affinity_mask).cuda()\n\n            out, _ = net(images)\n\n            optimizer.zero_grad()\n\n            out1 = out[:, :, :, 0].cuda()\n            out2 = out[:, :, :, 1].cuda()\n            loss = criterion(gh_label, gah_label, out1, out2, mask)\n\n            loss.backward()\n            optimizer.step()\n            loss_value += loss.item()\n            if index % 2 == 0 and index > 0:\n                et = time.time()\n                print(\'epoch {}:({}/{}) batch || training time for 2 batch {} || training loss {} ||\'.format(epoch, index, len(real_data_loader), et-st, loss_value/2))\n                loss_time = 0\n                loss_value = 0\n                st = time.time()\n            # if loss < compare_loss:\n            #     print(\'save the lower loss iter, loss:\',loss)\n            #     compare_loss = loss\n            #     torch.save(net.module.state_dict(),\n            #                \'/data/CRAFT-pytorch/real_weights/lower_loss.pth\')\n\n            #net.eval()\n            if index % 350 ==0 and index != 0:\n                print(\'Saving state, iter:\', index)\n                torch.save(net.module.state_dict(),\n                           \'/data/CRAFT-pytorch/weights/mlt\'+\'_\'+repr(epoch)+\'_\' + repr(index) + \'.pth\')\n                test(\'/data/CRAFT-pytorch/weights/mlt\'+\'_\'+repr(epoch)+\'_\' + repr(index) + \'.pth\')\n                #test(\'/data/CRAFT-pytorch/craft_mlt_25k.pth\')\n                getresult()\n        print(\'Saving state, iter:\', epoch)\n        torch.save(net.module.state_dict(),\n                   \'/data/CRAFT-pytorch/epoch_weights/mlt\' + \'_\' + repr(epoch) + \'.pth\')\n        test(\'/data/CRAFT-pytorch/epoch_weights/mlt\' + \'_\' + repr(epoch) + \'.pth\')\n        # test(\'/data/CRAFT-pytorch/craft_mlt_25k.pth\')\n        getresult()\n            \n\n\n\n\n\n\n'"
trainSyndata.py,27,"b'import os\nimport sys\nimport torch\nimport torch.utils.data as data\nimport cv2\nimport numpy as np\nimport scipy.io as scio\nimport argparse\nimport time\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.backends.cudnn as cudnn\nimport torch.optim as optim\nimport random\nimport h5py\nimport re\nimport water\nfrom test import test\n\n\nfrom math import exp\nfrom data_loader import ICDAR2015, Synth80k, ICDAR2013\n\n###import file#######\nfrom augmentation import random_rot, crop_img_bboxes\nfrom gaussianmap import gaussion_transform, four_point_transform\nfrom generateheatmap import add_character, generate_target, add_affinity, generate_affinity, sort_box, real_affinity, generate_affinity_box\nfrom mseloss import Maploss\n\n\n\nfrom collections import OrderedDict\nfrom eval.script import getresult\n\n\n\nfrom PIL import Image\nfrom torchvision.transforms import transforms\nfrom craft import CRAFT\nfrom torch.autograd import Variable\nfrom multiprocessing import Pool\n\n#3.2768e-5\nrandom.seed(42)\n\n# class SynAnnotationTransform(object):\n#     def __init__(self):\n#         pass\n#     def __call__(self, gt):\n#         image_name = gt[\'imnames\'][0]\nparser = argparse.ArgumentParser(description=\'CRAFT reimplementation\')\n\n\nparser.add_argument(\'--resume\', default=None, type=str,\n                    help=\'Checkpoint state_dict file to resume training from\')\nparser.add_argument(\'--batch_size\', default=128, type = int,\n                    help=\'batch size of training\')\n#parser.add_argument(\'--cdua\', default=True, type=str2bool,\n                    #help=\'Use CUDA to train model\')\nparser.add_argument(\'--lr\', \'--learning-rate\', default=3.2768e-5, type=float,\n                    help=\'initial learning rate\')\nparser.add_argument(\'--momentum\', default=0.9, type=float,\n                    help=\'Momentum value for optim\')\nparser.add_argument(\'--weight_decay\', default=5e-4, type=float,\n                    help=\'Weight decay for SGD\')\nparser.add_argument(\'--gamma\', default=0.1, type=float,\n                    help=\'Gamma update for SGD\')\nparser.add_argument(\'--num_workers\', default=32, type=int,\n                    help=\'Number of workers used in dataloading\')\n\n\nargs = parser.parse_args()\n\n\ndef copyStateDict(state_dict):\n    if list(state_dict.keys())[0].startswith(""module""):\n        start_idx = 1\n    else:\n        start_idx = 0\n    new_state_dict = OrderedDict()\n    for k, v in state_dict.items():\n        name = ""."".join(k.split(""."")[start_idx:])\n        new_state_dict[name] = v\n    return new_state_dict\n\ndef adjust_learning_rate(optimizer, gamma, step):\n    """"""Sets the learning rate to the initial LR decayed by 10 at every\n        specified step\n    # Adapted from PyTorch Imagenet example:\n    # https://github.com/pytorch/examples/blob/master/imagenet/main.py\n    """"""\n    lr = args.lr * (0.8 ** step)\n    print(lr)\n    for param_group in optimizer.param_groups:\n        param_group[\'lr\'] = lr\n\n\nif __name__ == \'__main__\':\n\n    # gaussian = gaussion_transform()\n    # box = scio.loadmat(\'/data/CRAFT-pytorch/syntext/SynthText/gt.mat\')\n    # bbox = box[\'wordBB\'][0][0][0]\n    # charbox = box[\'charBB\'][0]\n    # imgname = box[\'imnames\'][0]\n    # imgtxt = box[\'txt\'][0]\n\n    #dataloader = syndata(imgname, charbox, imgtxt)\n    dataloader = Synth80k(\'/data/CRAFT-pytorch/syntext/SynthText/SynthText\', target_size = 768)\n    train_loader = torch.utils.data.DataLoader(\n        dataloader,\n        batch_size=16,\n        shuffle=True,\n        num_workers=0,\n        drop_last=True,\n        pin_memory=True)\n    #batch_syn = iter(train_loader)\n    # prefetcher = data_prefetcher(dataloader)\n    # input, target1, target2 = prefetcher.next()\n    #print(input.size())\n    net = CRAFT()\n    #net.load_state_dict(copyStateDict(torch.load(\'/data/CRAFT-pytorch/CRAFT_net_050000.pth\')))\n    #net.load_state_dict(copyStateDict(torch.load(\'/data/CRAFT-pytorch/1-7.pth\')))\n    #net.load_state_dict(copyStateDict(torch.load(\'/data/CRAFT-pytorch/craft_mlt_25k.pth\')))\n    #net.load_state_dict(copyStateDict(torch.load(\'vgg16_bn-6c64b313.pth\')))\n    #realdata = realdata(net)\n    # realdata = ICDAR2015(net, \'/data/CRAFT-pytorch/icdar2015\', target_size = 768)\n    # real_data_loader = torch.utils.data.DataLoader(\n    #     realdata,\n    #     batch_size=10,\n    #     shuffle=True,\n    #     num_workers=0,\n    #     drop_last=True,\n    #     pin_memory=True)\n    net = net.cuda()\n    #net = CRAFT_net\n\n    # if args.cdua:\n    net = torch.nn.DataParallel(net,device_ids=[0,1,2,3]).cuda()\n    cudnn.benchmark = True\n    # realdata = ICDAR2015(net, \'/data/CRAFT-pytorch/icdar2015\', target_size=768)\n    # real_data_loader = torch.utils.data.DataLoader(\n    #     realdata,\n    #     batch_size=10,\n    #     shuffle=True,\n    #     num_workers=0,\n    #     drop_last=True,\n    #     pin_memory=True)\n\n\n    optimizer = optim.Adam(net.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n    criterion = Maploss()\n    #criterion = torch.nn.MSELoss(reduce=True, size_average=True)\n    net.train()\n\n\n    step_index = 0\n\n\n    loss_time = 0\n    loss_value = 0\n    compare_loss = 1\n    for epoch in range(1000):\n        loss_value = 0\n        # if epoch % 50 == 0 and epoch != 0:\n        #     step_index += 1\n        #     adjust_learning_rate(optimizer, args.gamma, step_index)\n\n        st = time.time()\n        for index, (images, gh_label, gah_label, mask, _) in enumerate(train_loader):\n            if index % 20000 == 0 and index != 0:\n                step_index += 1\n                adjust_learning_rate(optimizer, args.gamma, step_index)\n            #real_images, real_gh_label, real_gah_label, real_mask = next(batch_real)\n\n            # syn_images, syn_gh_label, syn_gah_label, syn_mask = next(batch_syn)\n            # images = torch.cat((syn_images,real_images), 0)\n            # gh_label = torch.cat((syn_gh_label, real_gh_label), 0)\n            # gah_label = torch.cat((syn_gah_label, real_gah_label), 0)\n            # mask = torch.cat((syn_mask, real_mask), 0)\n\n            #affinity_mask = torch.cat((syn_mask, real_affinity_mask), 0)\n\n\n            images = Variable(images.type(torch.FloatTensor)).cuda()\n            gh_label = gh_label.type(torch.FloatTensor)\n            gah_label = gah_label.type(torch.FloatTensor)\n            gh_label = Variable(gh_label).cuda()\n            gah_label = Variable(gah_label).cuda()\n            mask = mask.type(torch.FloatTensor)\n            mask = Variable(mask).cuda()\n            # affinity_mask = affinity_mask.type(torch.FloatTensor)\n            # affinity_mask = Variable(affinity_mask).cuda()\n\n            out, _ = net(images)\n\n            optimizer.zero_grad()\n\n            out1 = out[:, :, :, 0].cuda()\n            out2 = out[:, :, :, 1].cuda()\n            loss = criterion(gh_label, gah_label, out1, out2, mask)\n\n            loss.backward()\n            optimizer.step()\n            loss_value += loss.item()\n            if index % 2 == 0 and index > 0:\n                et = time.time()\n                print(\'epoch {}:({}/{}) batch || training time for 2 batch {} || training loss {} ||\'.format(epoch, index, len(train_loader), et-st, loss_value/2))\n                loss_time = 0\n                loss_value = 0\n                st = time.time()\n            # if loss < compare_loss:\n            #     print(\'save the lower loss iter, loss:\',loss)\n            #     compare_loss = loss\n            #     torch.save(net.module.state_dict(),\n            #                \'/data/CRAFT-pytorch/real_weights/lower_loss.pth\'\n\n            if index % 5000 == 0 and index != 0:\n                print(\'Saving state, index:\', index)\n                torch.save(net.module.state_dict(),\n                           \'/data/CRAFT-pytorch/synweights/synweights_\' + repr(index) + \'.pth\')\n                test(\'/data/CRAFT-pytorch/synweights/synweights_\' + repr(index) + \'.pth\')\n                #test(\'/data/CRAFT-pytorch/craft_mlt_25k.pth\')\n                getresult()\n\n\n\n\n\n\n\n\n'"
trainic15data.py,23,"b'import os\nimport sys\nimport torch\nimport torch.utils.data as data\nimport cv2\nimport numpy as np\nimport scipy.io as scio\nimport argparse\nimport time\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.backends.cudnn as cudnn\nimport torch.optim as optim\nimport random\nimport h5py\nimport re\nimport water\nfrom test import test\n\n\nfrom math import exp\nfrom data_loader import ICDAR2015, Synth80k, ICDAR2013\n\n###import file#######\nfrom augmentation import random_rot, crop_img_bboxes\nfrom gaussianmap import gaussion_transform, four_point_transform\nfrom generateheatmap import add_character, generate_target, add_affinity, generate_affinity, sort_box, real_affinity, generate_affinity_box\nfrom mseloss import Maploss\n\n\n\nfrom collections import OrderedDict\nfrom eval.script import getresult\n\n\n\nfrom PIL import Image\nfrom torchvision.transforms import transforms\nfrom craft import CRAFT\nfrom torch.autograd import Variable\nfrom multiprocessing import Pool\n\n#3.2768e-5\nrandom.seed(42)\n\n# class SynAnnotationTransform(object):\n#     def __init__(self):\n#         pass\n#     def __call__(self, gt):\n#         image_name = gt[\'imnames\'][0]\nparser = argparse.ArgumentParser(description=\'CRAFT reimplementation\')\n\n\nparser.add_argument(\'--resume\', default=None, type=str,\n                    help=\'Checkpoint state_dict file to resume training from\')\nparser.add_argument(\'--batch_size\', default=128, type = int,\n                    help=\'batch size of training\')\n#parser.add_argument(\'--cdua\', default=True, type=str2bool,\n                    #help=\'Use CUDA to train model\')\nparser.add_argument(\'--lr\', \'--learning-rate\', default=3.2768e-5, type=float,\n                    help=\'initial learning rate\')\nparser.add_argument(\'--momentum\', default=0.9, type=float,\n                    help=\'Momentum value for optim\')\nparser.add_argument(\'--weight_decay\', default=5e-4, type=float,\n                    help=\'Weight decay for SGD\')\nparser.add_argument(\'--gamma\', default=0.1, type=float,\n                    help=\'Gamma update for SGD\')\nparser.add_argument(\'--num_workers\', default=32, type=int,\n                    help=\'Number of workers used in dataloading\')\n\n\nargs = parser.parse_args()\n\n\n\n\n\n\ndef copyStateDict(state_dict):\n    if list(state_dict.keys())[0].startswith(""module""):\n        start_idx = 1\n    else:\n        start_idx = 0\n    new_state_dict = OrderedDict()\n    for k, v in state_dict.items():\n        name = ""."".join(k.split(""."")[start_idx:])\n        new_state_dict[name] = v\n    return new_state_dict\n\ndef adjust_learning_rate(optimizer, gamma, step):\n    """"""Sets the learning rate to the initial LR decayed by 10 at every\n        specified step\n    # Adapted from PyTorch Imagenet example:\n    # https://github.com/pytorch/examples/blob/master/imagenet/main.py\n    """"""\n    lr = args.lr * (0.8 ** step)\n    print(lr)\n    for param_group in optimizer.param_groups:\n        param_group[\'lr\'] = lr\n\n\nif __name__ == \'__main__\':\n\n    dataloader = Synth80k(\'/data/CRAFT-pytorch/syntext/SynthText/SynthText\', target_size = 768)\n    train_loader = torch.utils.data.DataLoader(\n        dataloader,\n        batch_size=2,\n        shuffle=True,\n        num_workers=0,\n        drop_last=True,\n        pin_memory=True)\n    batch_syn = iter(train_loader)\n    \n    net = CRAFT()\n\n    net.load_state_dict(copyStateDict(torch.load(\'/data/CRAFT-pytorch/1-7.pth\')))\n    \n    net = net.cuda()\n\n\n\n    net = torch.nn.DataParallel(net,device_ids=[0,1,2,3]).cuda()\n    cudnn.benchmark = True\n    net.train()\n    realdata = ICDAR2015(net, \'/data/CRAFT-pytorch/icdar2015\', target_size=768)\n    real_data_loader = torch.utils.data.DataLoader(\n        realdata,\n        batch_size=10,\n        shuffle=True,\n        num_workers=0,\n        drop_last=True,\n        pin_memory=True)\n\n\n    optimizer = optim.Adam(net.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n    criterion = Maploss()\n    #criterion = torch.nn.MSELoss(reduce=True, size_average=True)\n    \n\n\n    step_index = 0\n\n\n    loss_time = 0\n    loss_value = 0\n    compare_loss = 1\n    for epoch in range(1000):\n        train_time_st = time.time()\n        loss_value = 0\n        if epoch % 50 == 0 and epoch != 0:\n            step_index += 1\n            adjust_learning_rate(optimizer, args.gamma, step_index)\n\n        st = time.time()\n        for index, (real_images, real_gh_label, real_gah_label, real_mask, _) in enumerate(real_data_loader):\n            #real_images, real_gh_label, real_gah_label, real_mask = next(batch_real)\n            syn_images, syn_gh_label, syn_gah_label, syn_mask, __ = next(batch_syn)\n            images = torch.cat((syn_images,real_images), 0)\n            gh_label = torch.cat((syn_gh_label, real_gh_label), 0)\n            gah_label = torch.cat((syn_gah_label, real_gah_label), 0)\n            mask = torch.cat((syn_mask, real_mask), 0)\n            #affinity_mask = torch.cat((syn_mask, real_affinity_mask), 0)\n\n\n            images = Variable(images.type(torch.FloatTensor)).cuda()\n            gh_label = gh_label.type(torch.FloatTensor)\n            gah_label = gah_label.type(torch.FloatTensor)\n            gh_label = Variable(gh_label).cuda()\n            gah_label = Variable(gah_label).cuda()\n            mask = mask.type(torch.FloatTensor)\n            mask = Variable(mask).cuda()\n            # affinity_mask = affinity_mask.type(torch.FloatTensor)\n            # affinity_mask = Variable(affinity_mask).cuda()\n\n            out, _ = net(images)\n\n            optimizer.zero_grad()\n\n            out1 = out[:, :, :, 0].cuda()\n            out2 = out[:, :, :, 1].cuda()\n            loss = criterion(gh_label, gah_label, out1, out2, mask)\n\n            loss.backward()\n            optimizer.step()\n            loss_value += loss.item()\n            if index % 2 == 0 and index > 0:\n                et = time.time()\n                print(\'epoch {}:({}/{}) batch || training time for 2 batch {} || training loss {} ||\'.format(epoch, index, len(real_data_loader), et-st, loss_value/2))\n                loss_time = 0\n                loss_value = 0\n                st = time.time()\n            # if loss < compare_loss:\n            #     print(\'save the lower loss iter, loss:\',loss)\n            #     compare_loss = loss\n            #     torch.save(net.module.state_dict(),\n            #                \'/data/CRAFT-pytorch/real_weights/lower_loss.pth\')\n\n        print(\'Saving state, iter:\', epoch)\n        torch.save(net.module.state_dict(),\n                   \'/data/CRAFT-pytorch/real_weights/CRAFT_clr_\' + repr(epoch) + \'.pth\')\n        test(\'/data/CRAFT-pytorch/real_weights/CRAFT_clr_\' + repr(epoch) + \'.pth\')\n        #test(\'/data/CRAFT-pytorch/craft_mlt_25k.pth\')\n        getresult()\n        \n\n\n\n\n\n\n\n\n'"
watershed.py,0,"b'import cv2\nimport numpy as np\nimport math\nimport Polygon as plg\n\n\ndef watershed1(image, viz=False):\n    boxes = []\n    if len(image.shape) == 3:\n        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    else:\n        gray = image\n    if viz:\n        cv2.imshow(""gray"", gray)\n        cv2.waitKey()\n    ret, binary = cv2.threshold(gray, 0.6 * np.max(gray), 255, cv2.THRESH_BINARY)\n    if viz:\n        cv2.imshow(""binary"", binary)\n        cv2.waitKey()\n    # \xe5\xbd\xa2\xe6\x80\x81\xe5\xad\xa6\xe6\x93\x8d\xe4\xbd\x9c\xef\xbc\x8c\xe8\xbf\x9b\xe4\xb8\x80\xe6\xad\xa5\xe6\xb6\x88\xe9\x99\xa4\xe5\x9b\xbe\xe5\x83\x8f\xe4\xb8\xad\xe5\x99\xaa\xe7\x82\xb9\n    kernel = np.ones((3, 3), np.uint8)\n    # kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))\n    mb = cv2.morphologyEx(binary, cv2.MORPH_OPEN, kernel, iterations=2)  # iterations\xe8\xbf\x9e\xe7\xbb\xad\xe4\xb8\xa4\xe6\xac\xa1\xe5\xbc\x80\xe6\x93\x8d\xe4\xbd\x9c\n    sure_bg = cv2.dilate(mb, kernel, iterations=3)  # 3\xe6\xac\xa1\xe8\x86\xa8\xe8\x83\x80,\xe5\x8f\xaf\xe4\xbb\xa5\xe8\x8e\xb7\xe5\x8f\x96\xe5\x88\xb0\xe5\xa4\xa7\xe9\x83\xa8\xe5\x88\x86\xe9\x83\xbd\xe6\x98\xaf\xe8\x83\x8c\xe6\x99\xaf\xe7\x9a\x84\xe5\x8c\xba\xe5\x9f\x9f\n    if viz:\n        cv2.imshow(""sure_bg"", sure_bg)\n        cv2.waitKey()\n\n    # \xe8\xb7\x9d\xe7\xa6\xbb\xe5\x8f\x98\xe6\x8d\xa2\n    dist = cv2.distanceTransform(mb, cv2.DIST_L2, 5)\n    if viz:\n        cv2.imshow(""dist"", dist)\n        cv2.waitKey()\n    ret, sure_fg = cv2.threshold(dist, 0.2 * np.max(dist), 255, cv2.THRESH_BINARY)\n    surface_fg = np.uint8(sure_fg)  # \xe4\xbf\x9d\xe6\x8c\x81\xe8\x89\xb2\xe5\xbd\xa9\xe7\xa9\xba\xe9\x97\xb4\xe4\xb8\x80\xe8\x87\xb4\xe6\x89\x8d\xe8\x83\xbd\xe8\xbf\x9b\xe8\xa1\x8c\xe8\xbf\x90\xe7\xae\x97\xef\xbc\x8c\xe7\x8e\xb0\xe5\x9c\xa8\xe6\x98\xaf\xe8\x83\x8c\xe6\x99\xaf\xe7\xa9\xba\xe9\x97\xb4\xe4\xb8\xba\xe6\x95\xb4\xe5\x9e\x8b\xe7\xa9\xba\xe9\x97\xb4\xef\xbc\x8c\xe5\x89\x8d\xe6\x99\xaf\xe4\xb8\xba\xe6\xb5\xae\xe7\x82\xb9\xe5\x9e\x8b\xe7\xa9\xba\xe9\x97\xb4\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe8\xbf\x9b\xe8\xa1\x8c\xe8\xbd\xac\xe6\x8d\xa2\n    if viz:\n        cv2.imshow(""surface_fg"", surface_fg)\n        cv2.waitKey()\n    unknown = cv2.subtract(sure_bg, surface_fg)\n    # \xe8\x8e\xb7\xe5\x8f\x96maskers,\xe5\x9c\xa8markers\xe4\xb8\xad\xe5\x90\xab\xe6\x9c\x89\xe7\xa7\x8d\xe5\xad\x90\xe5\x8c\xba\xe5\x9f\x9f\n    ret, markers = cv2.connectedComponents(surface_fg)\n\n    # \xe5\x88\x86\xe6\xb0\xb4\xe5\xb2\xad\xe5\x8f\x98\xe6\x8d\xa2\n    markers = markers + 1\n    markers[unknown == 255] = 0\n\n    if viz:\n        color_markers = np.uint8(markers)\n        color_markers = cv2.applyColorMap(color_markers, cv2.COLORMAP_JET)\n        cv2.imshow(""color_markers"", color_markers)\n        cv2.waitKey()\n\n    markers = cv2.watershed(image, markers=markers)\n    image[markers == -1] = [0, 0, 255]\n    if viz:\n        cv2.imshow(""image"", image)\n        cv2.waitKey()\n    for i in range(2, np.max(markers) + 1):\n        np_contours = np.roll(np.array(np.where(markers == i)), 1, axis=0).transpose().reshape(-1, 2)\n        # print(np_contours.shape)\n        rectangle = cv2.minAreaRect(np_contours)\n        box = cv2.boxPoints(rectangle)\n        w, h = np.linalg.norm(box[0] - box[1]), np.linalg.norm(box[1] - box[2])\n        box_ratio = max(w, h) / (min(w, h) + 1e-5)\n        if abs(1 - box_ratio) <= 0.1:\n            l, r = min(np_contours[:, 0]), max(np_contours[:, 0])\n            t, b = min(np_contours[:, 1]), max(np_contours[:, 1])\n            box = np.array([[l, t], [r, t], [r, b], [l, b]], dtype=np.float32)\n\n        # make clock-wise order\n        startidx = box.sum(axis=1).argmin()\n        box = np.roll(box, 4 - startidx, 0)\n        box = np.array(box)\n        boxes.append(box)\n    return np.array(boxes)\n\n\ndef getDetCharBoxes_core(textmap, text_threshold=0.5, low_text=0.4):\n    # prepare data\n    textmap = textmap.copy()\n    img_h, img_w = textmap.shape\n\n    """""" labeling method """"""\n    ret, text_score = cv2.threshold(textmap, low_text, 1, 0)\n    nLabels, labels, stats, centroids = cv2.connectedComponentsWithStats(text_score.astype(np.uint8),\n                                                                         connectivity=4)\n\n    det = []\n    mapper = []\n    for k in range(1, nLabels):\n        # size filtering\n        size = stats[k, cv2.CC_STAT_AREA]\n        if size < 10: continue\n\n        # thresholding\n        if np.max(textmap[labels == k]) < text_threshold: continue\n\n        # make segmentation map\n        segmap = np.zeros(textmap.shape, dtype=np.uint8)\n        segmap[labels == k] = 255\n        # segmap[np.logical_and(link_score == 1, text_score == 0)] = 0  # remove link area\n        x, y = stats[k, cv2.CC_STAT_LEFT], stats[k, cv2.CC_STAT_TOP]\n        w, h = stats[k, cv2.CC_STAT_WIDTH], stats[k, cv2.CC_STAT_HEIGHT]\n        niter = int(math.sqrt(size * min(w, h) / (w * h)) * 2)\n        sx, ex, sy, ey = x - niter, x + w + niter + 1, y - niter, y + h + niter + 1\n        # boundary check\n        if sx < 0: sx = 0\n        if sy < 0: sy = 0\n        if ex >= img_w: ex = img_w\n        if ey >= img_h: ey = img_h\n        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1 + niter, 1 + niter))\n        segmap[sy:ey, sx:ex] = cv2.dilate(segmap[sy:ey, sx:ex], kernel)\n\n        # make box\n        np_contours = np.roll(np.array(np.where(segmap != 0)), 1, axis=0).transpose().reshape(-1, 2)\n        rectangle = cv2.minAreaRect(np_contours)\n        box = cv2.boxPoints(rectangle)\n\n        # align diamond-shape\n        w, h = np.linalg.norm(box[0] - box[1]), np.linalg.norm(box[1] - box[2])\n        box_ratio = max(w, h) / (min(w, h) + 1e-5)\n        if abs(1 - box_ratio) <= 0.1:\n            l, r = min(np_contours[:, 0]), max(np_contours[:, 0])\n            t, b = min(np_contours[:, 1]), max(np_contours[:, 1])\n            box = np.array([[l, t], [r, t], [r, b], [l, b]], dtype=np.float32)\n\n        # make clock-wise order\n        startidx = box.sum(axis=1).argmin()\n        box = np.roll(box, 4 - startidx, 0)\n        box = np.array(box)\n\n        det.append(box)\n        mapper.append(k)\n\n    return det, labels, mapper\n\n\ndef watershed2(image, viz=False):\n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    gray = np.float32(gray) / 255.0\n    boxes, _, _ = getDetCharBoxes_core(gray)\n    return np.array(boxes)\n\n\ndef watershed(oriimage, image, viz=False):\n    # viz = True\n    boxes = []\n    if len(image.shape) == 3:\n        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n    else:\n        gray = image\n    if viz:\n        cv2.imshow(""gray"", gray)\n        cv2.waitKey()\n    ret, binary = cv2.threshold(gray, 0.2 * np.max(gray), 255, cv2.THRESH_BINARY)\n    if viz:\n        cv2.imshow(""binary"", binary)\n        cv2.waitKey()\n    # \xe5\xbd\xa2\xe6\x80\x81\xe5\xad\xa6\xe6\x93\x8d\xe4\xbd\x9c\xef\xbc\x8c\xe8\xbf\x9b\xe4\xb8\x80\xe6\xad\xa5\xe6\xb6\x88\xe9\x99\xa4\xe5\x9b\xbe\xe5\x83\x8f\xe4\xb8\xad\xe5\x99\xaa\xe7\x82\xb9\n    kernel = np.ones((3, 3), np.uint8)\n    # kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))\n    mb = cv2.morphologyEx(binary, cv2.MORPH_OPEN, kernel, iterations=2)  # iterations\xe8\xbf\x9e\xe7\xbb\xad\xe4\xb8\xa4\xe6\xac\xa1\xe5\xbc\x80\xe6\x93\x8d\xe4\xbd\x9c\n    sure_bg = cv2.dilate(mb, kernel, iterations=3)  # 3\xe6\xac\xa1\xe8\x86\xa8\xe8\x83\x80,\xe5\x8f\xaf\xe4\xbb\xa5\xe8\x8e\xb7\xe5\x8f\x96\xe5\x88\xb0\xe5\xa4\xa7\xe9\x83\xa8\xe5\x88\x86\xe9\x83\xbd\xe6\x98\xaf\xe8\x83\x8c\xe6\x99\xaf\xe7\x9a\x84\xe5\x8c\xba\xe5\x9f\x9f\n    sure_bg = mb\n    if viz:\n        cv2.imshow(""sure_bg"", mb)\n        cv2.waitKey()\n    # \xe8\xb7\x9d\xe7\xa6\xbb\xe5\x8f\x98\xe6\x8d\xa2\n    # dist = cv2.distanceTransform(mb, cv2.DIST_L2, 5)\n    # if viz:\n    #     cv2.imshow(""dist"", dist)\n    #     cv2.waitKey()\n    ret, sure_fg = cv2.threshold(gray, 0.6 * gray.max(), 255, cv2.THRESH_BINARY)\n    surface_fg = np.uint8(sure_fg)  # \xe4\xbf\x9d\xe6\x8c\x81\xe8\x89\xb2\xe5\xbd\xa9\xe7\xa9\xba\xe9\x97\xb4\xe4\xb8\x80\xe8\x87\xb4\xe6\x89\x8d\xe8\x83\xbd\xe8\xbf\x9b\xe8\xa1\x8c\xe8\xbf\x90\xe7\xae\x97\xef\xbc\x8c\xe7\x8e\xb0\xe5\x9c\xa8\xe6\x98\xaf\xe8\x83\x8c\xe6\x99\xaf\xe7\xa9\xba\xe9\x97\xb4\xe4\xb8\xba\xe6\x95\xb4\xe5\x9e\x8b\xe7\xa9\xba\xe9\x97\xb4\xef\xbc\x8c\xe5\x89\x8d\xe6\x99\xaf\xe4\xb8\xba\xe6\xb5\xae\xe7\x82\xb9\xe5\x9e\x8b\xe7\xa9\xba\xe9\x97\xb4\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe8\xbf\x9b\xe8\xa1\x8c\xe8\xbd\xac\xe6\x8d\xa2\n    if viz:\n        cv2.imshow(""surface_fg"", surface_fg)\n        cv2.waitKey()\n    unknown = cv2.subtract(sure_bg, surface_fg)\n    # \xe8\x8e\xb7\xe5\x8f\x96maskers,\xe5\x9c\xa8markers\xe4\xb8\xad\xe5\x90\xab\xe6\x9c\x89\xe7\xa7\x8d\xe5\xad\x90\xe5\x8c\xba\xe5\x9f\x9f\n    ret, markers = cv2.connectedComponents(surface_fg)\n\n    nLabels, labels, stats, centroids = cv2.connectedComponentsWithStats(surface_fg,\n                                                                         connectivity=4)\n    # \xe5\x88\x86\xe6\xb0\xb4\xe5\xb2\xad\xe5\x8f\x98\xe6\x8d\xa2\n    markers = labels.copy() + 1\n    # markers = markers+1\n    markers[unknown == 255] = 0\n\n    if viz:\n        color_markers = np.uint8(markers)\n        color_markers = color_markers / (color_markers.max() / 255)\n        color_markers = np.uint8(color_markers)\n        color_markers = cv2.applyColorMap(color_markers, cv2.COLORMAP_JET)\n        cv2.imshow(""color_markers"", color_markers)\n        cv2.waitKey()\n    # a = cv2.applyColorMap(gray, cv2.COLORMAP_JET)\n    markers = cv2.watershed(oriimage, markers=markers)\n    oriimage[markers == -1] = [0, 0, 255]\n\n    if viz:\n        color_markers = np.uint8(markers + 1)\n        color_markers = color_markers / (color_markers.max() / 255)\n        color_markers = np.uint8(color_markers)\n        color_markers = cv2.applyColorMap(color_markers, cv2.COLORMAP_JET)\n        cv2.imshow(""color_markers1"", color_markers)\n        cv2.waitKey()\n\n    if viz:\n        cv2.imshow(""image"", oriimage)\n        cv2.waitKey()\n    for i in range(2, np.max(markers) + 1):\n        np_contours = np.roll(np.array(np.where(markers == i)), 1, axis=0).transpose().reshape(-1, 2)\n        # segmap = np.zeros(gray.shape, dtype=np.uint8)\n        # segmap[markers == i] = 255\n        # size = np_contours.shape[0]\n        # x, y, w, h = cv2.boundingRect(np_contours)\n        # if w == 0 or h == 0:\n        #     continue\n        #\n        # niter = int(math.sqrt(size * min(w, h) / (w * h)) * 2)\n        # sx, ex, sy, ey = x - niter, x + w + niter + 1, y - niter, y + h + niter + 1\n        # # boundary check\n        # if sx < 0: sx = 0\n        # if sy < 0: sy = 0\n        # if ex >= gray.shape[1]: ex = gray.shape[1]\n        # if ey >= gray.shape[0]: ey = gray.shape[0]\n        # kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1 + niter, 1 + niter))\n        # segmap[sy:ey, sx:ex] = cv2.dilate(segmap[sy:ey, sx:ex], kernel)\n        # np_contours = np.roll(np.array(np.where(segmap != 0)), 1, axis=0).transpose().reshape(-1, 2)\n        rectangle = cv2.minAreaRect(np_contours)\n        box = cv2.boxPoints(rectangle)\n\n        startidx = box.sum(axis=1).argmin()\n        box = np.roll(box, 4 - startidx, 0)\n        poly = plg.Polygon(box)\n        area = poly.area()\n        if area < 10:\n            continue\n        box = np.array(box)\n        boxes.append(box)\n    return np.array(boxes)\n\n\nif __name__ == \'__main__\':\n    image = cv2.imread(\'images/standard.jpg\', cv2.IMREAD_COLOR)\n    boxes = watershed(image, True)\n    print(boxes)'"
basenet/vgg16_bn.py,8,"b'from collections import namedtuple\n\nimport torch\nfrom torchvision import models\nfrom torchvision.models.vgg import model_urls\nfrom torchutil import *\nimport os\n\nweights_folder = os.path.join(os.path.dirname(__file__) + \'/../pretrain\')\n\n\nclass vgg16_bn(torch.nn.Module):\n    def __init__(self, pretrained=True, freeze=False):\n        super(vgg16_bn, self).__init__()\n        model_urls[\'vgg16_bn\'] = model_urls[\'vgg16_bn\'].replace(\'https://\', \'http://\')\n        # vgg_pretrained_features = models.vgg16_bn(pretrained=pretrained).features\n        vgg_pretrained_features = models.vgg16_bn(pretrained=False)\n        if pretrained:\n            vgg_pretrained_features.load_state_dict(\n                copyStateDict(torch.load(os.path.join(weights_folder, \'/data/CRAFT-pytorch/vgg16_bn-6c64b313.pth\'))))\n        vgg_pretrained_features = vgg_pretrained_features.features\n        self.slice1 = torch.nn.Sequential()\n        self.slice2 = torch.nn.Sequential()\n        self.slice3 = torch.nn.Sequential()\n        self.slice4 = torch.nn.Sequential()\n        self.slice5 = torch.nn.Sequential()\n        for x in range(12):  # conv2_2\n            self.slice1.add_module(str(x), vgg_pretrained_features[x])\n        for x in range(12, 19):  # conv3_3\n            self.slice2.add_module(str(x), vgg_pretrained_features[x])\n        for x in range(19, 29):  # conv4_3\n            self.slice3.add_module(str(x), vgg_pretrained_features[x])\n        for x in range(29, 39):  # conv5_3\n            self.slice4.add_module(str(x), vgg_pretrained_features[x])\n\n        # fc6, fc7 without atrous conv\n        self.slice5 = torch.nn.Sequential(\n            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n            nn.Conv2d(512, 1024, kernel_size=3, padding=6, dilation=6),\n            nn.Conv2d(1024, 1024, kernel_size=1)\n        )\n\n        if not pretrained:\n            init_weights(self.slice1.modules())\n            init_weights(self.slice2.modules())\n            init_weights(self.slice3.modules())\n            init_weights(self.slice4.modules())\n\n        init_weights(self.slice5.modules())  # no pretrained model for fc6 and fc7\n\n        if freeze:\n            for param in self.slice1.parameters():  # only first conv\n                param.requires_grad = False\n\n    def forward(self, X):\n        h = self.slice1(X)\n        h_relu2_2 = h\n        h = self.slice2(h)\n        h_relu3_2 = h\n        h = self.slice3(h)\n        h_relu4_3 = h\n        h = self.slice4(h)\n        h_relu5_3 = h\n        h = self.slice5(h)\n        h_fc7 = h\n        vgg_outputs = namedtuple(""VggOutputs"", [\'fc7\', \'relu5_3\', \'relu4_3\', \'relu3_2\', \'relu2_2\'])\n        out = vgg_outputs(h_fc7, h_relu5_3, h_relu4_3, h_relu3_2, h_relu2_2)\n        return out\n'"
eval/__init__.py,0,b''
eval/rrc_evaluation_funcs.py,0,"b'#!/usr/bin/env python2\n# encoding: UTF-8\nimport json\nimport sys\n\nsys.path.append(\'./\')\nimport zipfile\nimport re\nimport sys\nimport os\nimport codecs\nimport importlib\nfrom io import StringIO\n\n\ndef print_help():\n    sys.stdout.write(\'Usage: python %s.py -g=<gtFile> -s=<submFile> [-o=<outputFolder> -p=<jsonParams>]\' % sys.argv[0])\n    sys.exit(2)\n\n\ndef load_zip_file_keys(file, fileNameRegExp=\'\'):\n    """"""\n    Returns an array with the entries of the ZIP file that match with the regular expression.\n    The key\'s are the names or the file or the capturing group definied in the fileNameRegExp\n    """"""\n    try:\n        archive = zipfile.ZipFile(file, mode=\'r\', allowZip64=True)\n    except:\n        raise Exception(\'Error loading the ZIP archive.\')\n\n    pairs = []\n\n    for name in archive.namelist():\n        addFile = True\n        keyName = name\n        if fileNameRegExp != """":\n            m = re.match(fileNameRegExp, name)\n            if m == None:\n                addFile = False\n            else:\n                if len(m.groups()) > 0:\n                    keyName = m.group(1)\n\n        if addFile:\n            print(keyName)\n            pairs.append(keyName)\n\n    return pairs\n\n\ndef load_zip_file(file, fileNameRegExp=\'\', allEntries=False):\n    """"""\n    Returns an array with the contents (filtered by fileNameRegExp) of a ZIP file.\n    The key\'s are the names or the file or the capturing group definied in the fileNameRegExp\n    allEntries validates that all entries in the ZIP file pass the fileNameRegExp\n    """"""\n    try:\n        archive = zipfile.ZipFile(file, mode=\'r\', allowZip64=True)\n    except:\n        raise Exception(\'Error loading the ZIP archive\')\n\n    pairs = []\n    for name in archive.namelist():\n        addFile = True\n        keyName = name\n        if fileNameRegExp != """":\n            m = re.match(fileNameRegExp, name)\n            if m == None:\n                addFile = False\n            else:\n                if len(m.groups()) > 0:\n                    keyName = m.group(1)\n\n        if addFile:\n            pairs.append([keyName, archive.read(name)])\n        else:\n            if allEntries:\n                pass\n                #raise Exception(\'ZIP entry not valid: %s\' % name)\n\n    return dict(pairs)\n\n\ndef decode_utf8(raw):\n    """"""\n    Returns a Unicode object on success, or None on failure\n    """"""\n    try:\n        raw = codecs.decode(raw, \'utf-8\', \'replace\')\n        # extracts BOM if exists\n        raw = raw.encode(\'utf8\')\n        if raw.startswith(codecs.BOM_UTF8):\n            raw = raw.replace(codecs.BOM_UTF8, \'\', 1)\n        return raw.decode(\'utf-8\')\n    except:\n        return None\n\n\ndef validate_lines_in_file(fileName, file_contents, CRLF=True, LTRB=True, withTranscription=False, withConfidence=False,\n                           imWidth=0, imHeight=0):\n    """"""\n    This function validates that all lines of the file calling the Line validation function for each line\n    """"""\n    utf8File = decode_utf8(file_contents)\n    if (utf8File is None):\n        raise Exception(""The file %s is not UTF-8"" % fileName)\n\n    lines = utf8File.split(""\\r\\n"" if CRLF else ""\\n"")\n    for line in lines:\n        line = line.replace(""\\r"", """").replace(""\\n"", """")\n        if (line != """"):\n            try:\n                validate_tl_line(line, LTRB, withTranscription, withConfidence, imWidth, imHeight)\n            except Exception as e:\n                raise Exception(\n                    (""Line in sample not valid. Sample: %s Line: %s Error: %s"" % (fileName, line, str(e))).encode(\n                        \'utf-8\', \'replace\'))\n\n\ndef validate_tl_line(line, LTRB=True, withTranscription=True, withConfidence=True, imWidth=0, imHeight=0):\n    """"""\n    Validate the format of the line. If the line is not valid an exception will be raised.\n    If maxWidth and maxHeight are specified, all points must be inside the imgage bounds.\n    Posible values are:\n    LTRB=True: xmin,ymin,xmax,ymax[,confidence][,transcription] \n    LTRB=False: x1,y1,x2,y2,x3,y3,x4,y4[,confidence][,transcription] \n    """"""\n    get_tl_line_values(line, LTRB, withTranscription, withConfidence, imWidth, imHeight)\n\n\ndef get_tl_line_values(line, LTRB=True, withTranscription=False, withConfidence=False, imWidth=0, imHeight=0):\n    """"""\n    Validate the format of the line. If the line is not valid an exception will be raised.\n    If maxWidth and maxHeight are specified, all points must be inside the imgage bounds.\n    Posible values are:\n    LTRB=True: xmin,ymin,xmax,ymax[,confidence][,transcription] \n    LTRB=False: x1,y1,x2,y2,x3,y3,x4,y4[,confidence][,transcription] \n    Returns values from a textline. Points , [Confidences], [Transcriptions]\n    """"""\n    confidence = 0.0\n    transcription = """";\n    points = []\n\n    numPoints = 4;\n\n    if LTRB:\n\n        numPoints = 4;\n\n        if withTranscription and withConfidence:\n            m = re.match(\n                r\'^\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*([0-9]+)\\s*,\\s*([0-9]+)\\s*,\\s*([0-1].?[0-9]*)\\s*,(.*)$\', line)\n            if m == None:\n                m = re.match(\n                    r\'^\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*([0-9]+)\\s*,\\s*([0-9]+)\\s*,\\s*([0-1].?[0-9]*)\\s*,(.*)$\',\n                    line)\n                raise Exception(""Format incorrect. Should be: xmin,ymin,xmax,ymax,confidence,transcription"")\n        elif withConfidence:\n            m = re.match(r\'^\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*([0-9]+)\\s*,\\s*([0-9]+)\\s*,\\s*([0-1].?[0-9]*)\\s*$\',\n                         line)\n            if m == None:\n                raise Exception(""Format incorrect. Should be: xmin,ymin,xmax,ymax,confidence"")\n        elif withTranscription:\n            m = re.match(r\'^\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*([0-9]+)\\s*,\\s*([0-9]+)\\s*,(.*)$\', line)\n            if m == None:\n                raise Exception(""Format incorrect. Should be: xmin,ymin,xmax,ymax,transcription"")\n        else:\n            m = re.match(r\'^\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*([0-9]+)\\s*,\\s*([0-9]+)\\s*,?\\s*$\', line)\n            if m == None:\n                raise Exception(""Format incorrect. Should be: xmin,ymin,xmax,ymax"")\n\n        xmin = int(m.group(1))\n        ymin = int(m.group(2))\n        xmax = int(m.group(3))\n        ymax = int(m.group(4))\n        if (xmax < xmin):\n            raise Exception(""Xmax value (%s) not valid (Xmax < Xmin)."" % (xmax))\n        if (ymax < ymin):\n            raise Exception(""Ymax value (%s)  not valid (Ymax < Ymin)."" % (ymax))\n\n        points = [float(m.group(i)) for i in range(1, (numPoints + 1))]\n\n        if (imWidth > 0 and imHeight > 0):\n            validate_point_inside_bounds(xmin, ymin, imWidth, imHeight);\n            validate_point_inside_bounds(xmax, ymax, imWidth, imHeight);\n\n    else:\n\n        numPoints = 8;\n\n        if withTranscription and withConfidence:\n            m = re.match(\n                r\'^\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*([0-1].?[0-9]*)\\s*,(.*)$\',\n                line)\n            if m == None:\n                raise Exception(""Format incorrect. Should be: x1,y1,x2,y2,x3,y3,x4,y4,confidence,transcription"")\n        elif withConfidence:\n            m = re.match(\n                r\'^\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*([0-1].?[0-9]*)\\s*$\',\n                line)\n            if m == None:\n                raise Exception(""Format incorrect. Should be: x1,y1,x2,y2,x3,y3,x4,y4,confidence"")\n        elif withTranscription:\n            m = re.match(\n                r\'^\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,(.*)$\',\n                line)\n            if m == None:\n                raise Exception(""Format incorrect. Should be: x1,y1,x2,y2,x3,y3,x4,y4,transcription"")\n        else:\n            m = re.match(\n                r\'^\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*,\\s*(-?[0-9]+)\\s*$\',\n                line)\n            if m == None:\n                raise Exception(""Format incorrect. Should be: x1,y1,x2,y2,x3,y3,x4,y4"")\n\n        points = [float(m.group(i)) for i in range(1, (numPoints + 1))]\n\n        validate_clockwise_points(points)\n\n        if (imWidth > 0 and imHeight > 0):\n            validate_point_inside_bounds(points[0], points[1], imWidth, imHeight);\n            validate_point_inside_bounds(points[2], points[3], imWidth, imHeight);\n            validate_point_inside_bounds(points[4], points[5], imWidth, imHeight);\n            validate_point_inside_bounds(points[6], points[7], imWidth, imHeight);\n\n    if withConfidence:\n        try:\n            confidence = float(m.group(numPoints + 1))\n        except ValueError:\n            raise Exception(""Confidence value must be a float"")\n\n    if withTranscription:\n        posTranscription = numPoints + (2 if withConfidence else 1)\n        transcription = m.group(posTranscription)\n        m2 = re.match(r\'^\\s*\\""(.*)\\""\\s*$\', transcription)\n        if m2 != None:  # Transcription with double quotes, we extract the value and replace escaped characters\n            transcription = m2.group(1).replace(""\\\\\\\\"", ""\\\\"").replace(""\\\\\\"""", ""\\"""")\n\n    return points, confidence, transcription\n\n\ndef validate_point_inside_bounds(x, y, imWidth, imHeight):\n    if (x < 0 or x > imWidth):\n        raise Exception(""X value (%s) not valid. Image dimensions: (%s,%s)"" % (x, imWidth, imHeight))\n    if (y < 0 or y > imHeight):\n        raise Exception(\n            ""Y value (%s)  not valid. Image dimensions: (%s,%s) Sample: %s Line:%s"" % (y, imWidth, imHeight))\n\n\ndef validate_clockwise_points(points):\n    """"""\n    Validates that the points that the 4 points that dlimite a polygon are in clockwise order.\n    """"""\n\n    if len(points) != 8:\n        raise Exception(""Points list not valid."" + str(len(points)))\n\n    point = [\n        [int(points[0]), int(points[1])],\n        [int(points[2]), int(points[3])],\n        [int(points[4]), int(points[5])],\n        [int(points[6]), int(points[7])]\n    ]\n    edge = [\n        (point[1][0] - point[0][0]) * (point[1][1] + point[0][1]),\n        (point[2][0] - point[1][0]) * (point[2][1] + point[1][1]),\n        (point[3][0] - point[2][0]) * (point[3][1] + point[2][1]),\n        (point[0][0] - point[3][0]) * (point[0][1] + point[3][1])\n    ]\n\n    summatory = edge[0] + edge[1] + edge[2] + edge[3];\n    if summatory > 0:\n        raise Exception(\n            ""Points are not clockwise. The coordinates of bounding quadrilaterals have to be given in clockwise order. Regarding the correct interpretation of \'clockwise\' remember that the image coordinate system used is the standard one, with the image origin at the upper left, the X axis extending to the right and Y axis extending downwards."")\n\n\ndef get_tl_line_values_from_file_contents(content, CRLF=True, LTRB=True, withTranscription=False, withConfidence=False,\n                                          imWidth=0, imHeight=0, sort_by_confidences=True):\n    """"""\n    Returns all points, confindences and transcriptions of a file in lists. Valid line formats:\n    xmin,ymin,xmax,ymax,[confidence],[transcription]\n    x1,y1,x2,y2,x3,y3,x4,y4,[confidence],[transcription]\n    """"""\n    pointsList = []\n    transcriptionsList = []\n    confidencesList = []\n\n    lines = content.split(""\\r\\n"" if CRLF else ""\\n"")\n    for line in lines:\n        line = line.replace(""\\r"", """").replace(""\\n"", """")\n        if (line != """"):\n            points, confidence, transcription = get_tl_line_values(line, LTRB, withTranscription, withConfidence,\n                                                                   imWidth, imHeight);\n            pointsList.append(points)\n            transcriptionsList.append(transcription)\n            confidencesList.append(confidence)\n\n    if withConfidence and len(confidencesList) > 0 and sort_by_confidences:\n        import numpy as np\n        sorted_ind = np.argsort(-np.array(confidencesList))\n        confidencesList = [confidencesList[i] for i in sorted_ind]\n        pointsList = [pointsList[i] for i in sorted_ind]\n        transcriptionsList = [transcriptionsList[i] for i in sorted_ind]\n\n    return pointsList, confidencesList, transcriptionsList\n\n\ndef main_evaluation(p, default_evaluation_params_fn, validate_data_fn, evaluate_method_fn, show_result=True,\n                    per_sample=True):\n    """"""\n    This process validates a method, evaluates it and if it succed generates a ZIP file with a JSON entry for each sample.\n    Params:\n    p: Dictionary of parmeters with the GT/submission locations. If None is passed, the parameters send by the system are used.\n    default_evaluation_params_fn: points to a function that returns a dictionary with the default parameters used for the evaluation\n    validate_data_fn: points to a method that validates the corrct format of the submission\n    evaluate_method_fn: points to a function that evaluated the submission and return a Dictionary with the results\n    """"""\n\n    if (p == None):\n        p = dict([s[1:].split(\'=\') for s in sys.argv[1:]])\n        if (len(sys.argv) < 3):\n            print_help()\n\n    evalParams = default_evaluation_params_fn()\n    if \'p\' in p.keys():\n        evalParams.update(p[\'p\'] if isinstance(p[\'p\'], dict) else json.loads(p[\'p\'][1:-1]))\n\n    resDict = {\'calculated\': True, \'Message\': \'\', \'method\': \'{}\', \'per_sample\': \'{}\'}\n    try:\n        validate_data_fn(p[\'g\'], p[\'s\'], evalParams)\n        evalData = evaluate_method_fn(p[\'g\'], p[\'s\'], evalParams)\n        resDict.update(evalData)\n\n    except Exception as e:\n        resDict[\'Message\'] = str(e)\n        resDict[\'calculated\'] = False\n\n    if \'o\' in p:\n        if not os.path.exists(p[\'o\']):\n            os.makedirs(p[\'o\'])\n\n        resultsOutputname = p[\'o\'] + \'/results.zip\'\n        outZip = zipfile.ZipFile(resultsOutputname, mode=\'w\', allowZip64=True)\n\n        del resDict[\'per_sample\']\n        if \'output_items\' in resDict.keys():\n            del resDict[\'output_items\']\n\n        outZip.writestr(\'method.json\', json.dumps(resDict))\n\n    if not resDict[\'calculated\']:\n        if show_result:\n            sys.stderr.write(\'Error!\\n\' + resDict[\'Message\'] + \'\\n\\n\')\n        if \'o\' in p:\n            outZip.close()\n        return resDict\n\n    if \'o\' in p:\n        if per_sample == True:\n            for k, v in evalData[\'per_sample\'].iteritems():\n                outZip.writestr(k + \'.json\', json.dumps(v))\n\n            if \'output_items\' in evalData.keys():\n                for k, v in evalData[\'output_items\'].iteritems():\n                    outZip.writestr(k, v)\n\n        outZip.close()\n\n    if show_result:\n        sys.stdout.write(""Calculated!"")\n        sys.stdout.write(json.dumps(resDict[\'method\']))\n\n    return resDict\n\n\ndef main_validation(default_evaluation_params_fn, validate_data_fn):\n    """"""\n    This process validates a method\n    Params:\n    default_evaluation_params_fn: points to a function that returns a dictionary with the default parameters used for the evaluation\n    validate_data_fn: points to a method that validates the corrct format of the submission\n    """"""\n    try:\n        p = dict([s[1:].split(\'=\') for s in sys.argv[1:]])\n        evalParams = default_evaluation_params_fn()\n        if \'p\' in p.keys():\n            evalParams.update(p[\'p\'] if isinstance(p[\'p\'], dict) else json.loads(p[\'p\'][1:-1]))\n\n        validate_data_fn(p[\'g\'], p[\'s\'], evalParams)\n        print\n        \'SUCCESS\'\n        sys.exit(0)\n    except Exception as e:\n        print\n        str(e)\n        sys.exit(101)\n'"
eval/script.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\nfrom collections import namedtuple\nfrom eval import rrc_evaluation_funcs\nimport importlib\nimport zipfile\nimport os\ndef evaluation_imports():\n    """"""\n    evaluation_imports: Dictionary ( key = module name , value = alias  )  with python modules used in the evaluation. \n    """"""\n    return {\n        \'Polygon\': \'plg\',\n        \'numpy\': \'np\'\n    }\n\n\ndef default_evaluation_params():\n    """"""\n    default_evaluation_params: Default parameters to use for the validation and evaluation.\n    """"""\n    return {\n        \'IOU_CONSTRAINT\': 0.5,\n        \'AREA_PRECISION_CONSTRAINT\': 0.5,\n        \'GT_SAMPLE_NAME_2_ID\': \'gt_img_([0-9]+).txt\',\n        \'DET_SAMPLE_NAME_2_ID\': \'res_img_([0-9]+).txt\',\n        \'LTRB\': False,  # LTRB:2points(left,top,right,bottom) or 4 points(x1,y1,x2,y2,x3,y3,x4,y4)\n        \'CRLF\': False,  # Lines are delimited by Windows CRLF format\n        \'CONFIDENCES\': False,  # Detections must include confidence value. AP will be calculated\n        \'PER_SAMPLE_RESULTS\': True  # Generate per sample results and produce data for visualization\n    }\n\n\ndef validate_data(gtFilePath, submFilePath, evaluationParams):\n    """"""\n    Method validate_data: validates that all files in the results folder are correct (have the correct name contents).\n                            Validates also that there are no missing files in the folder.\n                            If some error detected, the method raises the error\n    """"""\n    gt = rrc_evaluation_funcs.load_zip_file(gtFilePath, evaluationParams[\'GT_SAMPLE_NAME_2_ID\'])\n\n    subm = rrc_evaluation_funcs.load_zip_file(submFilePath, evaluationParams[\'DET_SAMPLE_NAME_2_ID\'], True)\n\n    # Validate format of GroundTruth\n    for k in gt:\n        rrc_evaluation_funcs.validate_lines_in_file(k, gt[k], evaluationParams[\'CRLF\'], evaluationParams[\'LTRB\'], True)\n\n    # Validate format of results\n    for k in subm:\n        if (k in gt) == False:\n            raise Exception(""The sample %s not present in GT"" % k)\n\n        rrc_evaluation_funcs.validate_lines_in_file(k, subm[k], evaluationParams[\'CRLF\'], evaluationParams[\'LTRB\'],\n                                                    False, evaluationParams[\'CONFIDENCES\'])\n\n\ndef evaluate_method(gtFilePath, submFilePath, evaluationParams):\n    """"""\n    Method evaluate_method: evaluate method and returns the results\n        Results. Dictionary with the following values:\n        - method (required)  Global method metrics. Ex: { \'Precision\':0.8,\'Recall\':0.9 }\n        - samples (optional) Per sample metrics. Ex: {\'sample1\' : { \'Precision\':0.8,\'Recall\':0.9 } , \'sample2\' : { \'Precision\':0.8,\'Recall\':0.9 }\n    """"""\n\n    for module, alias in evaluation_imports().items():\n        globals()[alias] = importlib.import_module(module)\n\n    def polygon_from_points(points):\n        """"""\n        Returns a Polygon object to use with the Polygon2 class from a list of 8 points: x1,y1,x2,y2,x3,y3,x4,y4\n        """"""\n        resBoxes = np.empty([1, 8], dtype=\'int32\')\n        resBoxes[0, 0] = int(points[0])\n        resBoxes[0, 4] = int(points[1])\n        resBoxes[0, 1] = int(points[2])\n        resBoxes[0, 5] = int(points[3])\n        resBoxes[0, 2] = int(points[4])\n        resBoxes[0, 6] = int(points[5])\n        resBoxes[0, 3] = int(points[6])\n        resBoxes[0, 7] = int(points[7])\n        pointMat = resBoxes[0].reshape([2, 4]).T\n        return plg.Polygon(pointMat)\n\n    def rectangle_to_polygon(rect):\n        resBoxes = np.empty([1, 8], dtype=\'int32\')\n        resBoxes[0, 0] = int(rect.xmin)\n        resBoxes[0, 4] = int(rect.ymax)\n        resBoxes[0, 1] = int(rect.xmin)\n        resBoxes[0, 5] = int(rect.ymin)\n        resBoxes[0, 2] = int(rect.xmax)\n        resBoxes[0, 6] = int(rect.ymin)\n        resBoxes[0, 3] = int(rect.xmax)\n        resBoxes[0, 7] = int(rect.ymax)\n\n        pointMat = resBoxes[0].reshape([2, 4]).T\n\n        return plg.Polygon(pointMat)\n\n    def rectangle_to_points(rect):\n        points = [int(rect.xmin), int(rect.ymax), int(rect.xmax), int(rect.ymax), int(rect.xmax), int(rect.ymin),\n                  int(rect.xmin), int(rect.ymin)]\n        return points\n\n    def get_union(pD, pG):\n        areaA = pD.area();\n        areaB = pG.area();\n        return areaA + areaB - get_intersection(pD, pG);\n\n    def get_intersection_over_union(pD, pG):\n        try:\n            return get_intersection(pD, pG) / get_union(pD, pG);\n        except:\n            return 0\n\n    def get_intersection(pD, pG):\n        pInt = pD & pG\n        if len(pInt) == 0:\n            return 0\n        return pInt.area()\n\n    def compute_ap(confList, matchList, numGtCare):\n        correct = 0\n        AP = 0\n        if len(confList) > 0:\n            confList = np.array(confList)\n            matchList = np.array(matchList)\n            sorted_ind = np.argsort(-confList)\n            confList = confList[sorted_ind]\n            matchList = matchList[sorted_ind]\n            for n in range(len(confList)):\n                match = matchList[n]\n                if match:\n                    correct += 1\n                    AP += float(correct) / (n + 1)\n\n            if numGtCare > 0:\n                AP /= numGtCare\n\n        return AP\n\n    perSampleMetrics = {}\n\n    matchedSum = 0\n\n    Rectangle = namedtuple(\'Rectangle\', \'xmin ymin xmax ymax\')\n\n    gt = rrc_evaluation_funcs.load_zip_file(gtFilePath, evaluationParams[\'GT_SAMPLE_NAME_2_ID\'])\n    subm = rrc_evaluation_funcs.load_zip_file(submFilePath, evaluationParams[\'DET_SAMPLE_NAME_2_ID\'], True)\n\n    numGlobalCareGt = 0;\n    numGlobalCareDet = 0;\n\n    arrGlobalConfidences = [];\n    arrGlobalMatches = [];\n\n    for resFile in gt:\n\n        gtFile = rrc_evaluation_funcs.decode_utf8(gt[resFile])\n        recall = 0\n        precision = 0\n        hmean = 0\n\n        detMatched = 0\n\n        iouMat = np.empty([1, 1])\n\n        gtPols = []\n        detPols = []\n\n        gtPolPoints = []\n        detPolPoints = []\n\n        # Array of Ground Truth Polygons\' keys marked as don\'t Care\n        gtDontCarePolsNum = []\n        # Array of Detected Polygons\' matched with a don\'t Care GT\n        detDontCarePolsNum = []\n\n        pairs = []\n        detMatchedNums = []\n\n        arrSampleConfidences = [];\n        arrSampleMatch = [];\n        sampleAP = 0;\n\n        evaluationLog = """"\n\n        pointsList, _, transcriptionsList = rrc_evaluation_funcs.get_tl_line_values_from_file_contents(gtFile,\n                                                                                                       evaluationParams[\n                                                                                                           \'CRLF\'],\n                                                                                                       evaluationParams[\n                                                                                                           \'LTRB\'],\n                                                                                                       True, False)\n        for n in range(len(pointsList)):\n            points = pointsList[n]\n            transcription = transcriptionsList[n]\n            dontCare = transcription == ""###""\n            if evaluationParams[\'LTRB\']:\n                gtRect = Rectangle(*points)\n                gtPol = rectangle_to_polygon(gtRect)\n            else:\n                gtPol = polygon_from_points(points)\n            gtPols.append(gtPol)\n            gtPolPoints.append(points)\n            if dontCare:\n                gtDontCarePolsNum.append(len(gtPols) - 1)\n\n        evaluationLog += ""GT polygons: "" + str(len(gtPols)) + (\n            "" ("" + str(len(gtDontCarePolsNum)) + "" don\'t care)\\n"" if len(gtDontCarePolsNum) > 0 else ""\\n"")\n\n        if resFile in subm:\n\n            detFile = rrc_evaluation_funcs.decode_utf8(subm[resFile])\n\n            pointsList, confidencesList, _ = rrc_evaluation_funcs.get_tl_line_values_from_file_contents(detFile,\n                                                                                                        evaluationParams[\n                                                                                                            \'CRLF\'],\n                                                                                                        evaluationParams[\n                                                                                                            \'LTRB\'],\n                                                                                                        False,\n                                                                                                        evaluationParams[\n                                                                                                            \'CONFIDENCES\'])\n            for n in range(len(pointsList)):\n                points = pointsList[n]\n\n                if evaluationParams[\'LTRB\']:\n                    detRect = Rectangle(*points)\n                    detPol = rectangle_to_polygon(detRect)\n                else:\n                    detPol = polygon_from_points(points)\n                detPols.append(detPol)\n                detPolPoints.append(points)\n                if len(gtDontCarePolsNum) > 0:\n                    for dontCarePol in gtDontCarePolsNum:\n                        dontCarePol = gtPols[dontCarePol]\n                        intersected_area = get_intersection(dontCarePol, detPol)\n                        pdDimensions = detPol.area()\n                        precision = 0 if pdDimensions == 0 else intersected_area / pdDimensions\n                        if (precision > evaluationParams[\'AREA_PRECISION_CONSTRAINT\']):\n                            detDontCarePolsNum.append(len(detPols) - 1)\n                            break\n\n            evaluationLog += ""DET polygons: "" + str(len(detPols)) + (\n                "" ("" + str(len(detDontCarePolsNum)) + "" don\'t care)\\n"" if len(detDontCarePolsNum) > 0 else ""\\n"")\n\n            if len(gtPols) > 0 and len(detPols) > 0:\n                # Calculate IoU and precision matrixs\n                outputShape = [len(gtPols), len(detPols)]\n                iouMat = np.empty(outputShape)\n                gtRectMat = np.zeros(len(gtPols), np.int8)\n                detRectMat = np.zeros(len(detPols), np.int8)\n                for gtNum in range(len(gtPols)):\n                    for detNum in range(len(detPols)):\n                        pG = gtPols[gtNum]\n                        pD = detPols[detNum]\n                        iouMat[gtNum, detNum] = get_intersection_over_union(pD, pG)\n\n                for gtNum in range(len(gtPols)):\n                    for detNum in range(len(detPols)):\n                        if gtRectMat[gtNum] == 0 and detRectMat[\n                            detNum] == 0 and gtNum not in gtDontCarePolsNum and detNum not in detDontCarePolsNum:\n                            if iouMat[gtNum, detNum] > evaluationParams[\'IOU_CONSTRAINT\']:\n                                gtRectMat[gtNum] = 1\n                                detRectMat[detNum] = 1\n                                detMatched += 1\n                                pairs.append({\'gt\': gtNum, \'det\': detNum})\n                                detMatchedNums.append(detNum)\n                                evaluationLog += ""Match GT #"" + str(gtNum) + "" with Det #"" + str(detNum) + ""\\n""\n\n            if evaluationParams[\'CONFIDENCES\']:\n                for detNum in range(len(detPols)):\n                    if detNum not in detDontCarePolsNum:\n                        # we exclude the don\'t care detections\n                        match = detNum in detMatchedNums\n\n                        arrSampleConfidences.append(confidencesList[detNum])\n                        arrSampleMatch.append(match)\n\n                        arrGlobalConfidences.append(confidencesList[detNum]);\n                        arrGlobalMatches.append(match);\n\n        numGtCare = (len(gtPols) - len(gtDontCarePolsNum))\n        numDetCare = (len(detPols) - len(detDontCarePolsNum))\n        if numGtCare == 0:\n            recall = float(1)\n            precision = float(0) if numDetCare > 0 else float(1)\n            sampleAP = precision\n        else:\n            recall = float(detMatched) / numGtCare\n            precision = 0 if numDetCare == 0 else float(detMatched) / numDetCare\n            if evaluationParams[\'CONFIDENCES\'] and evaluationParams[\'PER_SAMPLE_RESULTS\']:\n                sampleAP = compute_ap(arrSampleConfidences, arrSampleMatch, numGtCare)\n\n        hmean = 0 if (precision + recall) == 0 else 2.0 * precision * recall / (precision + recall)\n\n        matchedSum += detMatched\n        numGlobalCareGt += numGtCare\n        numGlobalCareDet += numDetCare\n\n        if evaluationParams[\'PER_SAMPLE_RESULTS\']:\n            perSampleMetrics[resFile] = {\n                \'precision\': precision,\n                \'recall\': recall,\n                \'hmean\': hmean,\n                \'pairs\': pairs,\n                \'AP\': sampleAP,\n                \'iouMat\': [] if len(detPols) > 100 else iouMat.tolist(),\n                \'gtPolPoints\': gtPolPoints,\n                \'detPolPoints\': detPolPoints,\n                \'gtDontCare\': gtDontCarePolsNum,\n                \'detDontCare\': detDontCarePolsNum,\n                \'evaluationParams\': evaluationParams,\n                \'evaluationLog\': evaluationLog\n            }\n\n    # Compute MAP and MAR\n    AP = 0\n    if evaluationParams[\'CONFIDENCES\']:\n        AP = compute_ap(arrGlobalConfidences, arrGlobalMatches, numGlobalCareGt)\n\n    methodRecall = 0 if numGlobalCareGt == 0 else float(matchedSum) / numGlobalCareGt\n    methodPrecision = 0 if numGlobalCareDet == 0 else float(matchedSum) / numGlobalCareDet\n    methodHmean = 0 if methodRecall + methodPrecision == 0 else 2 * methodRecall * methodPrecision / (\n            methodRecall + methodPrecision)\n\n    methodMetrics = {\'precision\': methodPrecision, \'recall\': methodRecall, \'hmean\': methodHmean, \'AP\': AP}\n\n    resDict = {\'calculated\': True, \'Message\': \'\', \'method\': methodMetrics, \'per_sample\': perSampleMetrics}\n\n    return resDict;\n\n\ndef eval_2015(res_folder):\n    params = {}\n    current_folder = os.path.join(os.path.dirname(__file__))\n    submitfile = os.path.join(current_folder, \'submit.zip\')\n    #print(submitfile)\n    filenames = os.listdir(res_folder)\n    zip = zipfile.ZipFile(submitfile, ""w"", zipfile.ZIP_DEFLATED)\n    for filename in filenames:\n        filepath = os.path.join(res_folder, filename)\n        zip.write(filepath, filename)\n    zip.close()\n    gtfile = os.path.join(current_folder, \'gt.zip\')\n    params[\'g\'] = gtfile\n    params[\'s\'] = submitfile\n    rrc_evaluation_funcs.main_evaluation(params, default_evaluation_params, validate_data, evaluate_method)\n\n\ndef getresult():\n    # rrc_evaluation_funcs.main_evaluation(None, default_evaluation_params, validate_data, evaluate_method)\n    #eval_2015(\'../../test\')\n    eval_2015(\'/data/CRAFT-pytorch/result\')\n'"
