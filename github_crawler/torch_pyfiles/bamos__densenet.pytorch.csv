file_path,api_count,code
compute-cifar10-mean.py,5,"b'#!/usr/bin/env python3\n\nimport argparse\nimport torch\n\nimport torch.nn as nn\nimport torch.optim as optim\n\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nimport torchvision.datasets as dset\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\n\nimport torchvision.models as models\n\nimport os\nimport sys\nimport math\n\nimport numpy as np\n\ndata = dset.CIFAR10(root=\'cifar\', train=True, download=True,\n                    transform=transforms.ToTensor()).train_data\ndata = data.astype(np.float32)/255.\n\nmeans = []\nstdevs = []\nfor i in range(3):\n    pixels = data[:,i,:,:].ravel()\n    means.append(np.mean(pixels))\n    stdevs.append(np.std(pixels))\n\nprint(""means: {}"".format(means))\nprint(""stdevs: {}"".format(stdevs))\nprint(\'transforms.Normalize(mean = {}, std = {})\'.format(means, stdevs))\n'"
densenet.py,8,"b'import torch\n\nimport torch.nn as nn\nimport torch.optim as optim\n\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nimport torchvision.datasets as dset\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\n\nimport torchvision.models as models\n\nimport sys\nimport math\n\nclass Bottleneck(nn.Module):\n    def __init__(self, nChannels, growthRate):\n        super(Bottleneck, self).__init__()\n        interChannels = 4*growthRate\n        self.bn1 = nn.BatchNorm2d(nChannels)\n        self.conv1 = nn.Conv2d(nChannels, interChannels, kernel_size=1,\n                               bias=False)\n        self.bn2 = nn.BatchNorm2d(interChannels)\n        self.conv2 = nn.Conv2d(interChannels, growthRate, kernel_size=3,\n                               padding=1, bias=False)\n\n    def forward(self, x):\n        out = self.conv1(F.relu(self.bn1(x)))\n        out = self.conv2(F.relu(self.bn2(out)))\n        out = torch.cat((x, out), 1)\n        return out\n\nclass SingleLayer(nn.Module):\n    def __init__(self, nChannels, growthRate):\n        super(SingleLayer, self).__init__()\n        self.bn1 = nn.BatchNorm2d(nChannels)\n        self.conv1 = nn.Conv2d(nChannels, growthRate, kernel_size=3,\n                               padding=1, bias=False)\n\n    def forward(self, x):\n        out = self.conv1(F.relu(self.bn1(x)))\n        out = torch.cat((x, out), 1)\n        return out\n\nclass Transition(nn.Module):\n    def __init__(self, nChannels, nOutChannels):\n        super(Transition, self).__init__()\n        self.bn1 = nn.BatchNorm2d(nChannels)\n        self.conv1 = nn.Conv2d(nChannels, nOutChannels, kernel_size=1,\n                               bias=False)\n\n    def forward(self, x):\n        out = self.conv1(F.relu(self.bn1(x)))\n        out = F.avg_pool2d(out, 2)\n        return out\n\n\nclass DenseNet(nn.Module):\n    def __init__(self, growthRate, depth, reduction, nClasses, bottleneck):\n        super(DenseNet, self).__init__()\n\n        nDenseBlocks = (depth-4) // 3\n        if bottleneck:\n            nDenseBlocks //= 2\n\n        nChannels = 2*growthRate\n        self.conv1 = nn.Conv2d(3, nChannels, kernel_size=3, padding=1,\n                               bias=False)\n        self.dense1 = self._make_dense(nChannels, growthRate, nDenseBlocks, bottleneck)\n        nChannels += nDenseBlocks*growthRate\n        nOutChannels = int(math.floor(nChannels*reduction))\n        self.trans1 = Transition(nChannels, nOutChannels)\n\n        nChannels = nOutChannels\n        self.dense2 = self._make_dense(nChannels, growthRate, nDenseBlocks, bottleneck)\n        nChannels += nDenseBlocks*growthRate\n        nOutChannels = int(math.floor(nChannels*reduction))\n        self.trans2 = Transition(nChannels, nOutChannels)\n\n        nChannels = nOutChannels\n        self.dense3 = self._make_dense(nChannels, growthRate, nDenseBlocks, bottleneck)\n        nChannels += nDenseBlocks*growthRate\n\n        self.bn1 = nn.BatchNorm2d(nChannels)\n        self.fc = nn.Linear(nChannels, nClasses)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.Linear):\n                m.bias.data.zero_()\n\n    def _make_dense(self, nChannels, growthRate, nDenseBlocks, bottleneck):\n        layers = []\n        for i in range(int(nDenseBlocks)):\n            if bottleneck:\n                layers.append(Bottleneck(nChannels, growthRate))\n            else:\n                layers.append(SingleLayer(nChannels, growthRate))\n            nChannels += growthRate\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = self.conv1(x)\n        out = self.trans1(self.dense1(out))\n        out = self.trans2(self.dense2(out))\n        out = self.dense3(out)\n        out = torch.squeeze(F.avg_pool2d(F.relu(self.bn1(out)), 8))\n        out = F.log_softmax(self.fc(out))\n        return out\n'"
make_graph.py,1,"b""# From https://gist.github.com/apaszke/01aae7a0494c55af6242f06fad1f8b70\nfrom graphviz import Digraph\nfrom torch.autograd import Variable\n\ndef save(fname, creator):\n    dot = Digraph(comment='LRP',\n                node_attr={'style': 'filled', 'shape': 'box'})\n    #, 'fillcolor': 'lightblue'})\n\n    seen = set()\n\n    def add_nodes(var):\n        if var not in seen:\n            if isinstance(var, Variable):\n                dot.node(str(id(var)), str(var.size()), fillcolor='lightblue')\n            else:\n                dot.node(str(id(var)), type(var).__name__)\n            seen.add(var)\n            if hasattr(var, 'previous_functions'):\n                for u in var.previous_functions:\n                    dot.edge(str(id(u[0])), str(id(var)))\n                    add_nodes(u[0])\n\n    add_nodes(creator)\n    dot.save(fname)\n"""
plot.py,0,"b""#!/usr/bin/env python3\n\nimport argparse\nimport os\nimport numpy as np\n\nimport matplotlib as mpl\nmpl.use('Agg')\nimport matplotlib.pyplot as plt\nplt.style.use('bmh')\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('expDir', type=str)\n    args = parser.parse_args()\n\n    trainP = os.path.join(args.expDir, 'train.csv')\n    trainData = np.loadtxt(trainP, delimiter=',').reshape(-1, 3)\n    testP = os.path.join(args.expDir, 'test.csv')\n    testData = np.loadtxt(testP, delimiter=',').reshape(-1, 3)\n\n    N = 392*2 # Rolling loss over the past epoch.\n\n    trainI, trainLoss, trainErr = np.split(trainData, [1,2], axis=1)\n    trainI, trainLoss, trainErr = [x.ravel() for x in\n                                   (trainI, trainLoss, trainErr)]\n    trainI_, trainLoss_, trainErr_ = rolling(N, trainI, trainLoss, trainErr)\n\n    testI, testLoss, testErr = np.split(testData, [1,2], axis=1)\n\n    fig, ax = plt.subplots(1, 1, figsize=(6, 5))\n    # plt.plot(trainI, trainLoss, label='Train')\n    plt.plot(trainI_, trainLoss_, label='Train')\n    plt.plot(testI, testLoss, label='Test')\n    plt.xlabel('Epoch')\n    plt.ylabel('Cross-Entropy Loss')\n    plt.legend()\n    ax.set_yscale('log')\n    loss_fname = os.path.join(args.expDir, 'loss.png')\n    plt.savefig(loss_fname)\n    print('Created {}'.format(loss_fname))\n\n    fig, ax = plt.subplots(1, 1, figsize=(6, 5))\n    # plt.plot(trainI, trainErr, label='Train')\n    plt.plot(trainI_, trainErr_, label='Train')\n    plt.plot(testI, testErr, label='Test')\n    plt.xlabel('Epoch')\n    plt.ylabel('Error')\n    ax.set_yscale('log')\n    plt.legend()\n    err_fname = os.path.join(args.expDir, 'error.png')\n    plt.savefig(err_fname)\n    print('Created {}'.format(err_fname))\n\n    loss_err_fname = os.path.join(args.expDir, 'loss-error.png')\n    os.system('convert +append {} {} {}'.format(loss_fname, err_fname, loss_err_fname))\n    print('Created {}'.format(loss_err_fname))\n\ndef rolling(N, i, loss, err):\n    i_ = i[N-1:]\n    K = np.full(N, 1./N)\n    loss_ = np.convolve(loss, K, 'valid')\n    err_ = np.convolve(err, K, 'valid')\n    return i_, loss_, err_\n\nif __name__ == '__main__':\n    main()\n"""
train.py,9,"b""#!/usr/bin/env python3\n\nimport argparse\nimport torch\n\nimport torch.nn as nn\nimport torch.optim as optim\n\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nimport torchvision.datasets as dset\nimport torchvision.transforms as transforms\nfrom torchvision.utils import save_image\n\nfrom torch.utils.data import DataLoader\n\nimport os\nimport sys\nimport math\n\nimport shutil\n\nimport setproctitle\n\nimport densenet\nimport make_graph\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--batchSz', type=int, default=64)\n    parser.add_argument('--nEpochs', type=int, default=300)\n    parser.add_argument('--no-cuda', action='store_true')\n    parser.add_argument('--save')\n    parser.add_argument('--seed', type=int, default=1)\n    parser.add_argument('--opt', type=str, default='sgd',\n                        choices=('sgd', 'adam', 'rmsprop'))\n    args = parser.parse_args()\n\n    args.cuda = not args.no_cuda and torch.cuda.is_available()\n    args.save = args.save or 'work/densenet.base'\n    setproctitle.setproctitle(args.save)\n\n    torch.manual_seed(args.seed)\n    if args.cuda:\n        torch.cuda.manual_seed(args.seed)\n\n    if os.path.exists(args.save):\n        shutil.rmtree(args.save)\n    os.makedirs(args.save, exist_ok=True)\n\n    normMean = [0.49139968, 0.48215827, 0.44653124]\n    normStd = [0.24703233, 0.24348505, 0.26158768]\n    normTransform = transforms.Normalize(normMean, normStd)\n\n    trainTransform = transforms.Compose([\n        transforms.RandomCrop(32, padding=4),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        normTransform\n    ])\n    testTransform = transforms.Compose([\n        transforms.ToTensor(),\n        normTransform\n    ])\n\n    kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}\n    trainLoader = DataLoader(\n        dset.CIFAR10(root='cifar', train=True, download=True,\n                     transform=trainTransform),\n        batch_size=args.batchSz, shuffle=True, **kwargs)\n    testLoader = DataLoader(\n        dset.CIFAR10(root='cifar', train=False, download=True,\n                     transform=testTransform),\n        batch_size=args.batchSz, shuffle=False, **kwargs)\n\n    net = densenet.DenseNet(growthRate=12, depth=100, reduction=0.5,\n                            bottleneck=True, nClasses=10)\n\n    print('  + Number of params: {}'.format(\n        sum([p.data.nelement() for p in net.parameters()])))\n    if args.cuda:\n        net = net.cuda()\n\n    if args.opt == 'sgd':\n        optimizer = optim.SGD(net.parameters(), lr=1e-1,\n                            momentum=0.9, weight_decay=1e-4)\n    elif args.opt == 'adam':\n        optimizer = optim.Adam(net.parameters(), weight_decay=1e-4)\n    elif args.opt == 'rmsprop':\n        optimizer = optim.RMSprop(net.parameters(), weight_decay=1e-4)\n\n    trainF = open(os.path.join(args.save, 'train.csv'), 'w')\n    testF = open(os.path.join(args.save, 'test.csv'), 'w')\n\n    for epoch in range(1, args.nEpochs + 1):\n        adjust_opt(args.opt, optimizer, epoch)\n        train(args, epoch, net, trainLoader, optimizer, trainF)\n        test(args, epoch, net, testLoader, optimizer, testF)\n        torch.save(net, os.path.join(args.save, 'latest.pth'))\n        os.system('./plot.py {} &'.format(args.save))\n\n    trainF.close()\n    testF.close()\n\ndef train(args, epoch, net, trainLoader, optimizer, trainF):\n    net.train()\n    nProcessed = 0\n    nTrain = len(trainLoader.dataset)\n    for batch_idx, (data, target) in enumerate(trainLoader):\n        if args.cuda:\n            data, target = data.cuda(), target.cuda()\n        data, target = Variable(data), Variable(target)\n        optimizer.zero_grad()\n        output = net(data)\n        loss = F.nll_loss(output, target)\n        # make_graph.save('/tmp/t.dot', loss.creator); assert(False)\n        loss.backward()\n        optimizer.step()\n        nProcessed += len(data)\n        pred = output.data.max(1)[1] # get the index of the max log-probability\n        incorrect = pred.ne(target.data).cpu().sum()\n        err = 100.*incorrect/len(data)\n        partialEpoch = epoch + batch_idx / len(trainLoader) - 1\n        print('Train Epoch: {:.2f} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tError: {:.6f}'.format(\n            partialEpoch, nProcessed, nTrain, 100. * batch_idx / len(trainLoader),\n            loss.data[0], err))\n\n        trainF.write('{},{},{}\\n'.format(partialEpoch, loss.data[0], err))\n        trainF.flush()\n\ndef test(args, epoch, net, testLoader, optimizer, testF):\n    net.eval()\n    test_loss = 0\n    incorrect = 0\n    for data, target in testLoader:\n        if args.cuda:\n            data, target = data.cuda(), target.cuda()\n        data, target = Variable(data, volatile=True), Variable(target)\n        output = net(data)\n        test_loss += F.nll_loss(output, target).data[0]\n        pred = output.data.max(1)[1] # get the index of the max log-probability\n        incorrect += pred.ne(target.data).cpu().sum()\n\n    test_loss = test_loss\n    test_loss /= len(testLoader) # loss function already averages over batch size\n    nTotal = len(testLoader.dataset)\n    err = 100.*incorrect/nTotal\n    print('\\nTest set: Average loss: {:.4f}, Error: {}/{} ({:.0f}%)\\n'.format(\n        test_loss, incorrect, nTotal, err))\n\n    testF.write('{},{},{}\\n'.format(epoch, test_loss, err))\n    testF.flush()\n\ndef adjust_opt(optAlg, optimizer, epoch):\n    if optAlg == 'sgd':\n        if epoch < 150: lr = 1e-1\n        elif epoch == 150: lr = 1e-2\n        elif epoch == 225: lr = 1e-3\n        else: return\n\n        for param_group in optimizer.param_groups:\n            param_group['lr'] = lr\n\nif __name__=='__main__':\n    main()\n"""
attic/compare-pytorch-and-torch-grads.py,14,"b""#!/usr/bin/env python3\n\nimport torch\n\nimport torch.nn as nn\nimport torch.legacy as legacy\nimport torch.optim as optim\n\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nimport torchvision.datasets as dset\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\n\nimport torchvision.models as models\n\nimport torch.utils.serialization\n\nimport sys\nimport math\n\nimport densenet\n\ntorch.manual_seed(0)\n\nnet = densenet.DenseNet(growthRate=12, depth=40, reduction=0.5,\n                        bottleneck=True, nClasses=10)\n\nnet_th = torch.utils.serialization.load_lua('dn.t7')\n\nconvs = []\nfcs = []\n\ndef printnorm_f(self, input, output):\n    print('{} norm: {}'.format(self.__class__.__name__, output.data.norm()))\n\n# def printnorm_back(self, grad_input, grad_output):\n    # import IPython, sys; IPython.embed(); sys.exit(-1)\n    # print('{} grad_out norm: {}'.format(self.__class__.__name__, self.weight.grad.data.norm()))\n\nfor m in net.modules():\n    if isinstance(m, nn.Conv2d):\n        n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n        m.weight.data.normal_(0, math.sqrt(2. / n))\n        convs.append(m.weight.data)\n        # m.register_forward_hook(printnorm_f)\n        # m.register_backward_hook(printnorm_back)\n    elif isinstance(m, nn.BatchNorm2d):\n        m.weight.data.fill_(1)\n        m.bias.data.zero_()\n    elif isinstance(m, nn.Linear):\n        fcs.append(m.weight.data)\n        m.bias.data.zero_()\n\n\nglobal convI, fcI\nconvI = fcI = 0\n\ndef init(mods):\n    global convI, fcI\n    for m in mods:\n        if isinstance(m, legacy.nn.SpatialConvolution):\n            m.weight = convs[convI]\n            convI += 1\n        elif isinstance(m, legacy.nn.Linear):\n            m.weight = fcs[fcI]\n            fcI += 1\n        elif isinstance(m, legacy.nn.Concat) or \\\n             isinstance(m, legacy.nn.Sequential):\n            init(m.modules)\n\ninit(net_th.modules)\n\nprint(convI, fcI, len(convs), len(fcs))\nassert(convI == len(convs))\nassert(fcI == len(fcs))\n\nx = torch.randn(7, 3, 32, 32)\nx_v = Variable(x)\n\npyOut = net(x_v)\nprint('out: {}'.format(pyOut))\n\nprint('===')\nluaOut_1 = net_th.forward(x)\nlsm = legacy.nn.LogSoftMax()\nluaOut = lsm.forward(luaOut_1)\n\ndef printM(mods):\n    for m in mods:\n        if isinstance(m, legacy.nn.SpatialConvolution):\n            print('Conv2d norm: {}'.format(torch.norm(m.output)))\n        elif isinstance(m, legacy.nn.Linear):\n            pass\n        elif isinstance(m, legacy.nn.Concat) or \\\n             isinstance(m, legacy.nn.Sequential):\n            printM(m.modules)\n\n# printM(net_th.modules)\nprint('out: {}'.format(luaOut))\n\nprint('===')\n\nprint('PyTorch weight gradients:')\n\ntarget = torch.LongTensor([3,2,1,3,2,3,4])\ntarget_v = Variable(target)\nloss = F.nll_loss(pyOut, target_v)\nloss.backward()\n\nl = []\nfor m in net.modules():\n    if isinstance(m, nn.Conv2d):\n        l.append(m.weight.grad.data.norm())\n    elif isinstance(m, nn.Linear):\n        l.append(m.weight.grad.data.norm())\nprint(l)\n\nprint('===')\n\nprint('LuaTorch weight gradients:')\n\ncriterion = legacy.nn.ClassNLLCriterion()\nloss_lua = criterion.forward(luaOut, target)\nt1 = criterion.backward(luaOut, target)\nt2 = lsm.backward(luaOut_1, t1)\nt3 = net_th.backward(x, t2)\n\nl = []\ndef getM(mods):\n    for m in mods:\n        if isinstance(m, legacy.nn.SpatialConvolution):\n            m.gradWeight[m.gradWeight.ne(m.gradWeight)] = 0\n            l.append(torch.norm(m.gradWeight))\n        elif isinstance(m, legacy.nn.Linear):\n            l.append(torch.norm(m.gradWeight))\n        elif isinstance(m, legacy.nn.Concat) or \\\n             isinstance(m, legacy.nn.Sequential):\n            getM(m.modules)\n\ngetM(net_th.modules)\nprint(l)\n"""
attic/numcheck-grads.py,10,"b'#!/usr/bin/env python3\n\nimport torch\n\nimport torch.nn as nn\nimport torch.optim as optim\n\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nimport numpy as np\nimport numdifftools as nd\n\ntorch.manual_seed(0)\ncuda = True\n\nclass FcCat(nn.Module):\n    def __init__(self, nIn, nOut):\n        super(FcCat, self).__init__()\n        self.fc = nn.Linear(nIn, nOut, bias=False)\n\n    def forward(self, x):\n        out = torch.cat((x, self.fc(x)), 1)\n        return out\n\nclass Net(nn.Module):\n    def __init__(self, nFeatures, nHidden1, nHidden2):\n        super(Net, self).__init__()\n        self.l1 = FcCat(nFeatures, nHidden1)\n        self.l2 = FcCat(nFeatures+nHidden1, nHidden2)\n\n    def forward(self, x):\n        out = self.l1(x)\n        out = self.l2(out)\n        return out\n\n\nnBatch, nFeatures, nHidden1, nHidden2 = 2, 2, 3, 4\nx = Variable(torch.randn(nBatch, nFeatures))\nexpected = Variable(torch.randn(nBatch, nFeatures+nHidden1+nHidden2))\nnet = Net(nFeatures, nHidden1, nHidden2)\ncriterion = torch.nn.loss.MSELoss()\n\nif cuda:\n    x = x.cuda()\n    net = net.cuda()\n    expected = expected.cuda()\n\npredicted = net(x)\nloss = criterion(predicted, expected)\nloss.backward()\n\nW1, W2 = list(net.parameters())\n\nx_np = x.data.cpu().numpy()\nexp_np = expected.data.cpu().numpy()\n\ndef f_loss(W12_flat):\n    """"""A function that has all of the parameters flattened\n    as input for numdifftools.""""""\n    W1, W2 = unpack(W12_flat)\n    out1 = x_np.dot(W1.T)\n    out1 = np.concatenate((x_np, out1), axis=1)\n    out2 = out1.dot(W2.T)\n    out2 = np.concatenate((out1, out2), axis=1)\n\n    mse_batch = np.mean(np.square(out2-exp_np), axis=1)\n    mse = np.mean(mse_batch)\n    return mse\n\ndef unpack(W12_flat):\n    W1, W2 = np.split(W12_flat, [nFeatures*nHidden1])\n    W1 = W1.reshape(nHidden1, nFeatures)\n    W2 = W2.reshape(nHidden2, nFeatures+nHidden1)\n    return W1, W2\n\nW12_flat = torch.cat((W1.data.view(-1), W2.data.view(-1))).cpu().numpy()\nprint(\'The PyTorch loss is {:.3f}. f_loss for numeric diff is {:.2f}.\'.format(\n    loss.data[0], f_loss(W12_flat)))\n\nassert(np.abs(loss.data[0] - f_loss(W12_flat)) < 1e-4)\n\ng = nd.Gradient(f_loss)\ndW12_flat = g(W12_flat)\ndW1, dW2 = unpack(dW12_flat)\n\ndef printGrads(tag, W, dW):\n    print(\'\\n\' + \'-\'*40 + \'\'\'\nThe gradient w.r.t. {0} from PyTorch is:\n\n{1}\n\nThe gradient w.r.t. {0} from numeric differentiation is:\n\n{2}\'\'\'.format(tag, W.grad, dW))\n\n\nprintGrads(\'W1\', W1, dW1)\nprintGrads(\'W2\', W2, dW2)\n'"
