file_path,api_count,code
__init__.py,0,b''
main.py,10,"b'# coding=utf-8\n# @Author : bamtercelboo\n# @Datetime : 2018/07/19 22:35\n# @File : main.py\n# @Last Modify Time : 2018/07/19 22:35\n# @Contact : bamtercelboo@{gmail.com, 163.com}\n\nimport os\nimport argparse\nimport datetime\nimport Config.config as configurable\nimport torch\nimport torchtext.data as data\nfrom models.model_CNN import CNN_Text\nfrom models.model_HighWay_CNN import HighWay_CNN\nfrom models.model_DeepCNN import DEEP_CNN\nfrom models.model_LSTM import LSTM\nfrom models.model_BiLSTM import BiLSTM\nfrom models.model_CNN_LSTM import CNN_LSTM\nfrom models.model_CLSTM import CLSTM\nfrom models.model_GRU import GRU\nfrom models.model_CBiLSTM import CBiLSTM\nfrom models.model_CGRU import CGRU\nfrom models.model_CNN_BiLSTM import CNN_BiLSTM\nfrom models.model_BiGRU import BiGRU\nfrom models.model_CNN_BiGRU import CNN_BiGRU\nfrom models.model_CNN_MUI import CNN_MUI\nfrom models.model_DeepCNN_MUI import DEEP_CNN_MUI\nfrom models.model_BiLSTM_1 import BiLSTM_1\nfrom models.model_HighWay_BiLSTM_1 import HighWay_BiLSTM_1\nimport train_ALL_CNN\nimport train_ALL_LSTM\nfrom DataLoader import mydatasets_self_five\nfrom DataLoader import mydatasets_self_two\nfrom DataUtils.Load_Pretrained_Embed import load_pretrained_emb_zeros, load_pretrained_emb_avg, load_pretrained_emb_Embedding, load_pretrained_emb_uniform\nimport multiprocessing as mu\nimport shutil\nimport numpy as np\nimport random\n\n# solve encoding\nfrom imp import reload\nimport sys\ndefaultencoding = \'utf-8\'\nif sys.getdefaultencoding() != defaultencoding:\n    reload(sys)\n    sys.setdefaultencoding(defaultencoding)\n\n# random seed\nfrom DataUtils.Common import seed_num, pad, unk\ntorch.manual_seed(seed_num)\nrandom.seed(seed_num)\ntorch.manual_seed(seed_num)\nnp.random.seed(seed_num)\nrandom.seed(seed_num)\ntorch.cuda.manual_seed(seed_num)\n\n\ndef mrs_two(path, train_name, dev_name, test_name, char_data, text_field, label_field, **kargs):\n    """"""\n    :function: load two-classification data\n    :param path:\n    :param train_name: train path\n    :param dev_name: dev path\n    :param test_name: test path\n    :param char_data: char data\n    :param text_field: text dict for finetune\n    :param label_field: label dict for finetune\n    :param kargs: others arguments\n    :return: batch train, batch dev, batch test\n    """"""\n    train_data, dev_data, test_data = mydatasets_self_two.MR.splits(path, train_name, dev_name, test_name, char_data, text_field, label_field)\n    print(""len(train_data) {} "".format(len(train_data)))\n    text_field.build_vocab(train_data.text, min_freq=config.min_freq)\n    # text_field.build_vocab(train_data.text, dev_data.text, test_data.text, min_freq=config.min_freq)\n    label_field.build_vocab(train_data.label)\n    train_iter, dev_iter, test_iter = data.Iterator.splits((train_data, dev_data, test_data),batch_sizes=(config.batch_size, len(dev_data), len(test_data)), device=-1, **kargs)\n    return train_iter, dev_iter, test_iter\n\n\ndef mrs_two_mui(path, train_name, dev_name, test_name, char_data, text_field, label_field, static_text_field, static_label_field, **kargs):\n    """"""\n    :function: load two-classification data\n    :param path:\n    :param train_name: train path\n    :param dev_name: dev path\n    :param test_name: test path\n    :param char_data: char data\n    :param text_field: text dict for finetune\n    :param label_field: label dict for finetune\n    :param static_text_field: text dict for static(no finetune)\n    :param static_label_field: label dict for static(no finetune)\n    :param kargs: others arguments\n    :return: batch train, batch dev, batch test\n    """"""\n    train_data, dev_data, test_data = mydatasets_self_two.MR.splits(path, train_name, dev_name, test_name, char_data, text_field, label_field)\n    static_train_data, static_dev_data, static_test_data = mydatasets_self_two.MR.splits(path, train_name, dev_name, test_name,char_data, static_text_field, static_label_field)\n    print(""len(train_data) {} "".format(len(train_data)))\n    print(""len(static_train_data) {} "".format(len(static_train_data)))\n    text_field.build_vocab(train_data, min_freq=config.min_freq)\n    label_field.build_vocab(train_data)\n    static_text_field.build_vocab(static_train_data, static_dev_data, static_test_data, min_freq=config.min_freq)\n    static_label_field.build_vocab(static_train_data, static_dev_data, static_test_data)\n    train_iter, dev_iter, test_iter = data.Iterator.splits((train_data, dev_data, test_data), batch_sizes=(config.batch_size, len(dev_data), len(test_data)), device=-1, **kargs)\n    return train_iter, dev_iter, test_iter\n\n\n# load five-classification data\ndef mrs_five(path, train_name, dev_name, test_name, char_data, text_field, label_field, **kargs):\n    """"""\n    :function: load five-classification data\n    :param path:\n    :param train_name: train path\n    :param dev_name: dev path\n    :param test_name: test path\n    :param char_data: char data\n    :param text_field: text dict for finetune\n    :param label_field: label dict for finetune\n    :param kargs: others arguments\n    :return: batch train, batch dev, batch test\n    """"""\n    train_data, dev_data, test_data = mydatasets_self_five.MR.splits(path, train_name, dev_name, test_name, char_data, text_field, label_field)\n    print(""len(train_data) {} "".format(len(train_data)))\n    text_field.build_vocab(train_data, min_freq=config.min_freq)\n    label_field.build_vocab(train_data)\n    train_iter, dev_iter, test_iter = data.Iterator.splits((train_data, dev_data, test_data), batch_sizes=(config.batch_size, len(dev_data), len(test_data)), device=-1, **kargs)\n    return train_iter, dev_iter, test_iter\n\n\ndef mrs_five_mui(path, train_name, dev_name, test_name, char_data, text_field, label_field, static_text_field, static_label_field, **kargs):\n    """"""\n    :function: load five-classification data\n    :param path:\n    :param train_name: train path\n    :param dev_name: dev path\n    :param test_name: test path\n    :param char_data: char data\n    :param text_field: text dict for finetune\n    :param label_field: label dict for finetune\n    :param static_text_field: text dict for static(no finetune)\n    :param static_label_field: label dict for static(no finetune)\n    :param kargs: others arguments\n    :return: batch train, batch dev, batch test\n    """"""\n    train_data, dev_data, test_data = mydatasets_self_five.MR.splits(path, train_name, dev_name, test_name, char_data, text_field, label_field)\n    static_train_data, static_dev_data, static_test_data = mydatasets_self_five.MR.splits(path, train_name, dev_name, test_name, char_data, static_text_field, static_label_field)\n    print(""len(train_data) {} "".format(len(train_data)))\n    print(""len(static_train_data) {} "".format(len(static_train_data)))\n    text_field.build_vocab(train_data, min_freq=config.min_freq)\n    label_field.build_vocab(train_data)\n    static_text_field.build_vocab(static_train_data, static_dev_data, static_test_data, min_freq=config.min_freq)\n    static_label_field.build_vocab(static_train_data, static_dev_data, static_test_data)\n    train_iter, dev_iter, test_iter = data.Iterator.splits((train_data, dev_data, test_data), batch_sizes=(config.batch_size, len(dev_data), len(test_data)), device=-1, **kargs)\n    return train_iter, dev_iter, test_iter\n\n\ndef load_preEmbedding():\n    # load word2vec\n    static_pretrain_embed = None\n    pretrain_embed = None\n    if config.word_Embedding:\n        print(""word_Embedding_Path {} "".format(config.word_Embedding_Path))\n        path = config.word_Embedding_Path\n        print(""loading pretrain embedding......"")\n        paddingkey = pad\n        pretrain_embed = load_pretrained_emb_avg(path=path, text_field_words_dict=config.text_field.vocab.itos,\n                                                       pad=paddingkey)\n        if config.CNN_MUI is True or config.DEEP_CNN_MUI is True:\n            static_pretrain_embed = load_pretrained_emb_avg(path=path, text_field_words_dict=config.static_text_field.vocab.itos,\n                                                                  pad=paddingkey)\n        config.pretrained_weight = pretrain_embed\n        if config.CNN_MUI is True or config.DEEP_CNN_MUI is True:\n            config.pretrained_weight_static = static_pretrain_embed\n\n        print(""pretrain embedding load finished!"")\n\n\ndef Load_Data():\n    """"""\n    load five classification task data and two classification task data\n    :return:\n    """"""\n    train_iter, dev_iter, test_iter = None, None, None\n    if config.FIVE_CLASS_TASK:\n        print(""Executing 5 Classification Task......"")\n        if config.CNN_MUI is True or config.DEEP_CNN_MUI is True:\n            train_iter, dev_iter, test_iter = mrs_five_mui(config.datafile_path, config.name_trainfile, config.name_devfile, config.name_testfile, config.char_data, text_field=config.text_field, label_field=config.label_field,\n                                                           static_text_field=config.static_text_field, static_label_field=config.static_label_field, repeat=False, shuffle=config.epochs_shuffle, sort=False)\n        else:\n            train_iter, dev_iter, test_iter = mrs_five(config.datafile_path, config.name_trainfile, config.name_devfile, config.name_testfile, config.char_data,\n                                                       config.text_field, config.label_field, repeat=False, shuffle=config.epochs_shuffle, sort=False)\n    elif config.TWO_CLASS_TASK:\n        print(""Executing 2 Classification Task......"")\n        if config.CNN_MUI is True or config.DEEP_CNN_MUI is True:\n            train_iter, dev_iter, test_iter = mrs_two_mui(config.datafile_path, config.name_trainfile, config.name_devfile, config.name_testfile, config.char_data, text_field=config.text_field, label_field=config.label_field,\n                                                          static_text_field=config.static_text_field, static_label_field=config.static_label_field, repeat=False, shuffle=config.epochs_shuffle, sort=False)\n        else:\n            train_iter, dev_iter, test_iter = mrs_two(config.datafile_path, config.name_trainfile, config.name_devfile, config.name_testfile, config.char_data, config.text_field,\n                                                      config.label_field, repeat=False, shuffle=config.epochs_shuffle, sort=False)\n\n    return train_iter, dev_iter, test_iter\n\n\ndef define_dict():\n    """"""\n     use torchtext to define word and label dict\n    """"""\n    print(""use torchtext to define word dict......"")\n    config.text_field = data.Field(lower=True)\n    config.label_field = data.Field(sequential=False)\n    config.static_text_field = data.Field(lower=True)\n    config.static_label_field = data.Field(sequential=False)\n    print(""use torchtext to define word dict finished."")\n    # return text_field\n\n\ndef save_arguments():\n    shutil.copytree(""./Config"", ""./snapshot/"" + config.mulu + ""/Config"")\n\n\ndef update_arguments():\n    config.lr = config.learning_rate\n    config.init_weight_decay = config.weight_decay\n    config.init_clip_max_norm = config.clip_max_norm\n    config.embed_num = len(config.text_field.vocab)\n    config.class_num = len(config.label_field.vocab) - 1\n    config.paddingId = config.text_field.vocab.stoi[pad]\n    config.unkId = config.text_field.vocab.stoi[unk]\n    if config.CNN_MUI is True or config.DEEP_CNN_MUI is True:\n        config.embed_num_mui = len(config.static_text_field.vocab)\n        config.paddingId_mui = config.static_text_field.vocab.stoi[pad]\n        config.unkId_mui = config.static_text_field.vocab.stoi[unk]\n    # config.kernel_sizes = [int(k) for k in config.kernel_sizes.split(\',\')]\n    print(config.kernel_sizes)\n    mulu = datetime.datetime.now().strftime(\'%Y-%m-%d_%H-%M-%S\')\n    config.mulu = mulu\n    config.save_dir = os.path.join(""""+config.save_dir, config.mulu)\n    if not os.path.isdir(config.save_dir):\n        os.makedirs(config.save_dir)\n\n\ndef load_model():\n    model = None\n    if config.snapshot is None:\n        if config.CNN:\n            print(""loading CNN model....."")\n            model = CNN_Text(config)\n            # save model in this time\n            shutil.copy(""./models/model_CNN.py"", ""./snapshot/"" + config.mulu)\n        elif config.DEEP_CNN:\n            print(""loading DEEP_CNN model......"")\n            model = DEEP_CNN(config)\n            shutil.copy(""./models/model_DeepCNN.py"", ""./snapshot/"" + config.mulu)\n        elif config.DEEP_CNN_MUI:\n            print(""loading DEEP_CNN_MUI model......"")\n            model = DEEP_CNN_MUI(config)\n            shutil.copy(""./models/model_DeepCNN_MUI.py"", ""./snapshot/"" + config.mulu)\n        elif config.LSTM:\n            print(""loading LSTM model......"")\n            model = LSTM(config)\n            shutil.copy(""./models/model_LSTM.py"", ""./snapshot/"" + config.mulu)\n        elif config.GRU:\n            print(""loading GRU model......"")\n            model = GRU(config)\n            shutil.copy(""./models/model_GRU.py"", ""./snapshot/"" + config.mulu)\n        elif config.BiLSTM:\n            print(""loading BiLSTM model......"")\n            model = BiLSTM(config)\n            shutil.copy(""./models/model_BiLSTM.py"", ""./snapshot/"" + config.mulu)\n        elif config.BiLSTM_1:\n            print(""loading BiLSTM_1 model......"")\n            # model = model_BiLSTM_lexicon.BiLSTM_1(config)\n            model = BiLSTM_1(config)\n            shutil.copy(""./models/model_BiLSTM_1.py"", ""./snapshot/"" + config.mulu)\n        elif config.CNN_LSTM:\n            print(""loading CNN_LSTM model......"")\n            model = CNN_LSTM(config)\n            shutil.copy(""./models/model_CNN_LSTM.py"", ""./snapshot/"" + config.mulu)\n        elif config.CLSTM:\n            print(""loading CLSTM model......"")\n            model = CLSTM(config)\n            shutil.copy(""./models/model_CLSTM.py"", ""./snapshot/"" + config.mulu)\n        elif config.CBiLSTM:\n            print(""loading CBiLSTM model......"")\n            model = CBiLSTM(config)\n            shutil.copy(""./models/model_CBiLSTM.py"", ""./snapshot/"" + config.mulu)\n        elif config.CGRU:\n            print(""loading CGRU model......"")\n            model = CGRU(config)\n            shutil.copy(""./models/model_CGRU.py"", ""./snapshot/"" + config.mulu)\n        elif config.CNN_BiLSTM:\n            print(""loading CNN_BiLSTM model......"")\n            model = CNN_BiLSTM(config)\n            shutil.copy(""./models/model_CNN_BiLSTM.py"", ""./snapshot/"" + config.mulu)\n        elif config.BiGRU:\n            print(""loading BiGRU model......"")\n            model = BiGRU(config)\n            shutil.copy(""./models/model_BiGRU.py"", ""./snapshot/"" + config.mulu)\n        elif config.CNN_BiGRU:\n            print(""loading CNN_BiGRU model......"")\n            model = CNN_BiGRU(config)\n            shutil.copy(""./models/model_CNN_BiGRU.py"", ""./snapshot/"" + config.mulu)\n        elif config.CNN_MUI:\n            print(""loading CNN_MUI model......"")\n            model = CNN_MUI(config)\n            shutil.copy(""./models/model_CNN_MUI.py"", ""./snapshot/"" + config.mulu)\n        elif config.HighWay_CNN is True:\n            print(""loading HighWay_CNN model......"")\n            model = HighWay_CNN(config)\n            shutil.copy(""./models/model_HighWay_CNN.py"", ""./snapshot/"" + config.mulu)\n        elif config.HighWay_BiLSTM_1 is True:\n            print(""loading HighWay_BiLSTM_1 model......"")\n            model = HighWay_BiLSTM_1(config)\n            shutil.copy(""./models/model_HighWay_BiLSTM_1.py"", ""./snapshot/"" + config.mulu)\n        print(model)\n    else:\n        print(\'\\nLoading model from [%s]...\' % config.snapshot)\n        try:\n            model = torch.load(config.snapshot)\n        except:\n            print(""Sorry, This snapshot doesn\'t exist."")\n            exit()\n    if config.cuda is True:\n        model = model.cuda()\n    return model\n\n\ndef start_train(model, train_iter, dev_iter, test_iter):\n    """"""\n    :function\xef\xbc\x9astart train\n    :param model:\n    :param train_iter:\n    :param dev_iter:\n    :param test_iter:\n    :return:\n    """"""\n    if config.predict is not None:\n        label = train_ALL_CNN.predict(config.predict, model, config.text_field, config.label_field)\n        print(\'\\n[Text]  {}[Label] {}\\n\'.format(config.predict, label))\n    elif config.test:\n        try:\n            print(test_iter)\n            train_ALL_CNN.test_eval(test_iter, model, config)\n        except Exception as e:\n            print(""\\nSorry. The test dataset doesn\'t  exist.\\n"")\n    else:\n        print(""\\n cpu_count \\n"", mu.cpu_count())\n        torch.set_num_threads(config.num_threads)\n        if os.path.exists(""./Test_Result.txt""):\n            os.remove(""./Test_Result.txt"")\n        if config.CNN:\n            print(""CNN training start......"")\n            model_count = train_ALL_CNN.train(train_iter, dev_iter, test_iter, model, config)\n        elif config.DEEP_CNN:\n            print(""DEEP_CNN training start......"")\n            model_count = train_ALL_CNN.train(train_iter, dev_iter, test_iter, model, config)\n        elif config.LSTM:\n            print(""LSTM training start......"")\n            model_count = train_ALL_LSTM.train(train_iter, dev_iter, test_iter, model, config)\n        elif config.GRU:\n            print(""GRU training start......"")\n            model_count = train_ALL_LSTM.train(train_iter, dev_iter, test_iter, model, config)\n        elif config.BiLSTM:\n            print(""BiLSTM training start......"")\n            model_count = train_ALL_LSTM.train(train_iter, dev_iter, test_iter, model, config)\n        elif config.BiLSTM_1:\n            print(""BiLSTM_1 training start......"")\n            model_count = train_ALL_LSTM.train(train_iter, dev_iter, test_iter, model, config)\n        elif config.CNN_LSTM:\n            print(""CNN_LSTM training start......"")\n            model_count = train_ALL_LSTM.train(train_iter, dev_iter, test_iter, model, config)\n        elif config.CLSTM:\n            print(""CLSTM training start......"")\n            model_count = train_ALL_LSTM.train(train_iter, dev_iter, test_iter, model, config)\n        elif config.CBiLSTM:\n            print(""CBiLSTM training start......"")\n            model_count = train_ALL_LSTM.train(train_iter, dev_iter, test_iter, model, config)\n        elif config.CGRU:\n            print(""CGRU training start......"")\n            model_count = train_ALL_LSTM.train(train_iter, dev_iter, test_iter, model, config)\n        elif config.CNN_BiLSTM:\n            print(""CNN_BiLSTM training start......"")\n            model_count = train_ALL_LSTM.train(train_iter, dev_iter, test_iter, model, config)\n        elif config.BiGRU:\n            print(""BiGRU training start......"")\n            model_count = train_ALL_LSTM.train(train_iter, dev_iter, test_iter, model, config)\n        elif config.CNN_BiGRU:\n            print(""CNN_BiGRU training start......"")\n            model_count = train_ALL_LSTM.train(train_iter, dev_iter, test_iter, model, config)\n        elif config.CNN_MUI:\n            print(""CNN_MUI training start......"")\n            model_count = train_ALL_CNN.train(train_iter, dev_iter, test_iter, model, config)\n        elif config.DEEP_CNN_MUI:\n            print(""DEEP_CNN_MUI training start......"")\n            model_count = train_ALL_CNN.train(train_iter, dev_iter, test_iter, model, config)\n        elif config.HighWay_CNN is True:\n            print(""HighWay_CNN training start......"")\n            model_count = train_ALL_CNN.train(train_iter, dev_iter, test_iter, model, config)\n        elif config.HighWay_BiLSTM_1 is True:\n            print(""HighWay_BiLSTM_1 training start......"")\n            model_count = train_ALL_LSTM.train(train_iter, dev_iter, test_iter, model, config)\n        print(""Model_count"", model_count)\n        resultlist = []\n        if os.path.exists(""./Test_Result.txt""):\n            file = open(""./Test_Result.txt"")\n            for line in file.readlines():\n                if line[:10] == ""Evaluation"":\n                    resultlist.append(float(line[34:41]))\n            result = sorted(resultlist)\n            file.close()\n            file = open(""./Test_Result.txt"", ""a"")\n            file.write(""\\nThe Best Result is : "" + str(result[len(result) - 1]))\n            file.write(""\\n"")\n            file.close()\n            shutil.copy(""./Test_Result.txt"", ""./snapshot/"" + config.mulu + ""/Test_Result.txt"")\n\n\ndef main():\n    """"""\n        main function\n    """"""\n    # define word dict\n    define_dict()\n    # load data\n    train_iter, dev_iter, test_iter = Load_Data()\n    # load pretrain embedding\n    load_preEmbedding()\n    # update config and print\n    update_arguments()\n    save_arguments()\n    model = load_model()\n    start_train(model, train_iter, dev_iter, test_iter)\n\n\nif __name__ == ""__main__"":\n    print(""Process ID {}, Process Parent ID {}"".format(os.getpid(), os.getppid()))\n    parser = argparse.ArgumentParser(description=""Neural Networks"")\n    parser.add_argument(\'--config_file\', default=""./Config/config.cfg"")\n    config = parser.parse_args()\n\n    config = configurable.Configurable(config_file=config.config_file)\n    if config.cuda is True:\n        print(""Using GPU To Train......"")\n        # torch.backends.cudnn.enabled = True\n        torch.backends.cudnn.deterministic = True\n        torch.cuda.manual_seed(seed_num)\n        torch.cuda.manual_seed_all(seed_num)\n        print(""torch.cuda.initial_seed"", torch.cuda.initial_seed())\n    main()\n\n\n\n\n\n'"
train_ALL_CNN.py,11,"b'# coding=utf-8\n# @Author : bamtercelboo\n# @Datetime : 2018/07/19 22:35\n# @File : train_ALL_CNN.py\n# @Last Modify Time : 2018/07/19 22:35\n# @Contact : bamtercelboo@{gmail.com, 163.com}\n\nimport os\nimport sys\nimport torch\nimport torch.autograd as autograd\nimport torch.nn.functional as F\nimport torch.nn.utils as utils\nimport torch.optim.lr_scheduler as lr_scheduler\nimport shutil\nimport random\nfrom DataUtils.Common import seed_num\ntorch.manual_seed(seed_num)\nrandom.seed(seed_num)\n\n\ndef train(train_iter, dev_iter, test_iter, model, args):\n    if args.cuda:\n        model.cuda()\n\n    if args.Adam is True:\n        print(""Adam Training......"")\n        optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.init_weight_decay)\n    elif args.SGD is True:\n        print(""SGD Training......."")\n        optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, weight_decay=args.init_weight_decay,\n                                    momentum=args.momentum_value)\n    elif args.Adadelta is True:\n        print(""Adadelta Training......."")\n        optimizer = torch.optim.Adadelta(model.parameters(), lr=args.lr, weight_decay=args.init_weight_decay)\n\n    steps = 0\n    epoch_step = 0\n    model_count = 0\n    best_accuracy = Best_Result()\n    model.train()\n    for epoch in range(1, args.epochs+1):\n        steps = 0\n        print(""\\n## The {} Epoch, All {} Epochs ! ##"".format(epoch, args.epochs))\n        for batch in train_iter:\n            feature, target = batch.text, batch.label\n            feature.data.t_(), target.data.sub_(1)  # batch first, index align\n            if args.cuda:\n                feature, target = feature.cuda(), target.cuda()\n\n            optimizer.zero_grad()\n            # model.zero_grad()\n\n            logit = model(feature)\n            loss = F.cross_entropy(logit, target)\n            loss.backward()\n            if args.init_clip_max_norm is not None:\n                utils.clip_grad_norm_(model.parameters(), max_norm=args.init_clip_max_norm)\n            optimizer.step()\n\n            steps += 1\n            if steps % args.log_interval == 0:\n                train_size = len(train_iter.dataset)\n                corrects = (torch.max(logit, 1)[1].view(target.size()).data == target.data).sum()\n                accuracy = float(corrects)/batch.batch_size * 100.0\n                sys.stdout.write(\n                    \'\\rBatch[{}/{}] - loss: {:.6f}  acc: {:.4f}%({}/{})\'.format(steps,\n                                                                            train_size,\n                                                                             loss.item(),\n                                                                             accuracy,\n                                                                             corrects,\n                                                                             batch.batch_size))\n            if steps % args.test_interval == 0:\n                print(""\\nDev  Accuracy: "", end="""")\n                eval(dev_iter, model, args, best_accuracy, epoch, test=False)\n                print(""Test Accuracy: "", end="""")\n                eval(test_iter, model, args, best_accuracy, epoch, test=True)\n            if steps % args.save_interval == 0:\n                if not os.path.isdir(args.save_dir):\n                    os.makedirs(args.save_dir)\n                save_prefix = os.path.join(args.save_dir, \'snapshot\')\n                save_path = \'{}_steps{}.pt\'.format(save_prefix, steps)\n                torch.save(model.state_dict(), save_path)\n                if os.path.isfile(save_path) and args.rm_model is True:\n                    os.remove(save_path)\n                model_count += 1\n    return model_count\n\n\ndef eval(data_iter, model, args, best_accuracy, epoch, test=False):\n    model.eval()\n    corrects, avg_loss = 0, 0\n    for batch in data_iter:\n        feature, target = batch.text, batch.label\n        feature.data.t_(), target.data.sub_(1)  # batch first, index align\n        if args.cuda:\n            feature, target = feature.cuda(), target.cuda()\n\n        logit = model(feature)\n        loss = F.cross_entropy(logit, target)\n        avg_loss += loss.item()\n        corrects += (torch.max(logit, 1)[1].view(target.size()).data == target.data).sum()\n\n    size = len(data_iter.dataset)\n    avg_loss = loss.item()/size\n    accuracy = 100.0 * float(corrects)/size\n    model.train()\n    print(\' Evaluation - loss: {:.6f}  acc: {:.4f}%({}/{})\'.format(avg_loss, accuracy, corrects, size))\n    if test is False:\n        if accuracy >= best_accuracy.best_dev_accuracy:\n            best_accuracy.best_dev_accuracy = accuracy\n            best_accuracy.best_epoch = epoch\n            best_accuracy.best_test = True\n    if test is True and best_accuracy.best_test is True:\n        best_accuracy.accuracy = accuracy\n\n    if test is True:\n        print(""The Current Best Dev Accuracy: {:.4f}, and Test Accuracy is :{:.4f}, locate on {} epoch.\\n"".format(\n            best_accuracy.best_dev_accuracy, best_accuracy.accuracy, best_accuracy.best_epoch))\n    if test is True:\n        best_accuracy.best_test = False\n\n\nclass Best_Result:\n    def __init__(self):\n        self.best_dev_accuracy = -1\n        self.best_accuracy = -1\n        self.best_epoch = 1\n        self.best_test = False\n        self.accuracy = -1\n\n\n'"
train_ALL_LSTM.py,11,"b'# coding=utf-8\n# @Author : bamtercelboo\n# @Datetime : 2018/07/19 22:35\n# @File : train_ALL_LSTM.py\n# @Last Modify Time : 2018/07/19 22:35\n# @Contact : bamtercelboo@{gmail.com, 163.com}\n\nimport os\nimport sys\nimport torch\nimport torch.autograd as autograd\nimport torch.nn.functional as F\nimport torch.nn.utils as utils\nimport torch.optim.lr_scheduler as lr_scheduler\nimport shutil\nimport random\nimport numpy as np\nfrom DataUtils.Common import seed_num\ntorch.manual_seed(seed_num)\nrandom.seed(seed_num)\n\n\ndef train(train_iter, dev_iter, test_iter, model, args):\n    if args.cuda:\n        model.cuda()\n\n    if args.Adam is True:\n        print(""Adam Training......"")\n        optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.init_weight_decay)\n    elif args.SGD is True:\n        print(""SGD Training......."")\n        optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, weight_decay=args.init_weight_decay,\n                                    momentum=args.momentum_value)\n    elif args.Adadelta is True:\n        print(""Adadelta Training......."")\n        optimizer = torch.optim.Adadelta(model.parameters(), lr=args.lr, weight_decay=args.init_weight_decay)\n\n    steps = 0\n    model_count = 0\n    best_accuracy = Best_Result()\n    model.train()\n    for epoch in range(1, args.epochs+1):\n        steps = 0\n        print(""\\n## The {} Epoch, All {} Epochs ! ##"".format(epoch, args.epochs))\n        for batch in train_iter:\n            feature, target = batch.text, batch.label.data.sub_(1)\n            if args.cuda is True:\n                feature, target = feature.cuda(), target.cuda()\n\n            target = autograd.Variable(target)  # question 1\n            optimizer.zero_grad()\n            logit = model(feature)\n            loss = F.cross_entropy(logit, target)\n            loss.backward()\n            if args.init_clip_max_norm is not None:\n                utils.clip_grad_norm_(model.parameters(), max_norm=args.init_clip_max_norm)\n            optimizer.step()\n\n            steps += 1\n            if steps % args.log_interval == 0:\n                train_size = len(train_iter.dataset)\n                corrects = (torch.max(logit, 1)[1].view(target.size()).data == target.data).sum()\n                accuracy = float(corrects)/batch.batch_size * 100.0\n                sys.stdout.write(\n                    \'\\rBatch[{}/{}] - loss: {:.6f}  acc: {:.4f}%({}/{})\'.format(steps,\n                                                                            train_size,\n                                                                             loss.item(),\n                                                                             accuracy,\n                                                                             corrects,\n                                                                             batch.batch_size))\n            if steps % args.test_interval == 0:\n                print(""\\nDev  Accuracy: "", end="""")\n                eval(dev_iter, model, args, best_accuracy, epoch, test=False)\n                print(""Test Accuracy: "", end="""")\n                eval(test_iter, model, args, best_accuracy, epoch, test=True)\n            if steps % args.save_interval == 0:\n                if not os.path.isdir(args.save_dir): os.makedirs(args.save_dir)\n                save_prefix = os.path.join(args.save_dir, \'snapshot\')\n                save_path = \'{}_steps{}.pt\'.format(save_prefix, steps)\n                torch.save(model.state_dict(), save_path)\n                if os.path.isfile(save_path) and args.rm_model is True:\n                    os.remove(save_path)\n                model_count += 1\n    return model_count\n\n\ndef eval(data_iter, model, args, best_accuracy, epoch, test=False):\n    model.eval()\n    corrects, avg_loss = 0, 0\n    for batch in data_iter:\n        feature, target = batch.text, batch.label\n        target.data.sub_(1)\n        if args.cuda is True:\n            feature, target = feature.cuda(), target.cuda()\n        logit = model(feature)\n        loss = F.cross_entropy(logit, target)\n\n        avg_loss += loss.item()\n        corrects += (torch.max(logit, 1)[1].view(target.size()).data == target.data).sum()\n\n    size = len(data_iter.dataset)\n    avg_loss = loss.item()/size\n    accuracy = float(corrects)/size * 100.0\n    model.train()\n    print(\' Evaluation - loss: {:.6f}  acc: {:.4f}%({}/{}) \'.format(avg_loss, accuracy, corrects, size))\n    if test is False:\n        if accuracy >= best_accuracy.best_dev_accuracy:\n            best_accuracy.best_dev_accuracy = accuracy\n            best_accuracy.best_epoch = epoch\n            best_accuracy.best_test = True\n    if test is True and best_accuracy.best_test is True:\n        best_accuracy.accuracy = accuracy\n\n    if test is True:\n        print(""The Current Best Dev Accuracy: {:.4f}, and Test Accuracy is :{:.4f}, locate on {} epoch.\\n"".format(\n            best_accuracy.best_dev_accuracy, best_accuracy.accuracy, best_accuracy.best_epoch))\n    if test is True:\n        best_accuracy.best_test = False\n\n\nclass Best_Result:\n    def __init__(self):\n        self.best_dev_accuracy = -1\n        self.best_accuracy = -1\n        self.best_epoch = 1\n        self.best_test = False\n        self.accuracy = -1\n\n\n'"
Config/config.py,0,"b'\nfrom configparser import ConfigParser\nimport os\n\n\nclass myconf(ConfigParser):\n    def __init__(self, defaults=None):\n        ConfigParser.__init__(self, defaults=defaults)\n\n    def optionxform(self, optionstr):\n        return optionstr\n\n\nclass Configurable(myconf):\n    def __init__(self, config_file):\n        config = myconf()\n        config.read(config_file)\n        self._config = config\n\n        print(\'Loaded config file sucessfully.\')\n        for section in config.sections():\n            for k, v in config.items(section):\n                print(k, "":"", v)\n        # if not os.path.isdir(self.save_dir):\n        #     os.mkdir(self.save_dir)\n        config.write(open(config_file, \'w\'))\n\n    # Data\n    @property\n    def word_Embedding(self):\n        return self._config.getboolean(\'Data\', \'word_Embedding\')\n\n    @property\n    def freq_1_unk(self):\n        return self._config.getboolean(\'Data\', \'freq_1_unk\')\n\n    @property\n    def word_Embedding_Path(self):\n        return self._config.get(\'Data\', \'word_Embedding_Path\')\n\n    @property\n    def datafile_path(self):\n        return self._config.get(\'Data\', \'datafile_path\')\n\n    @property\n    def name_trainfile(self):\n        return self._config.get(\'Data\', \'name_trainfile\')\n\n    @property\n    def name_devfile(self):\n        return self._config.get(\'Data\', \'name_devfile\')\n\n    @property\n    def name_testfile(self):\n        return self._config.get(\'Data\', \'name_testfile\')\n\n    @property\n    def min_freq(self):\n        return self._config.getint(\'Data\', \'min_freq\')\n\n    @property\n    def word_data(self):\n        return self._config.getboolean(\'Data\', \'word_data\')\n\n    @property\n    def char_data(self):\n        return self._config.getboolean(\'Data\', \'char_data\')\n\n    @property\n    def shuffle(self):\n        return self._config.getboolean(\'Data\', \'shuffle\')\n\n    @property\n    def epochs_shuffle(self):\n        return self._config.getboolean(\'Data\', \'epochs_shuffle\')\n\n    @property\n    def FIVE_CLASS_TASK(self):\n        return self._config.getboolean(\'Data\', \'FIVE_CLASS_TASK\')    \\\n\n    @property\n    def TWO_CLASS_TASK(self):\n        return self._config.getboolean(\'Data\', \'TWO_CLASS_TASK\')\n\n    # Save\n    @property\n    def snapshot(self):\n        value = self._config.get(\'Save\', \'snapshot\')\n        if value == ""None"" or value == ""none"":\n            return None\n        else:\n            return value\n\n    @property\n    def predict(self):\n        value = self._config.get(\'Save\', \'predict\')\n        if value == ""None"" or value == ""none"":\n            return None\n        else:\n            return value\n\n    @property\n    def test(self):\n        return self._config.getboolean(\'Save\', \'test\')\n\n    @property\n    def save_dir(self):\n        return self._config.get(\'Save\', \'save_dir\')\n\n    @save_dir.setter\n    def save_dir(self, value):\n        self._config.set(\'Save\', \'save_dir\', str(value))\n\n    @property\n    def rm_model(self):\n        return self._config.getboolean(\'Save\', \'rm_model\')\n\n    # Model\n    @property\n    def static(self):\n        return self._config.getboolean(""Model"", ""static"")\n\n    @property\n    def wide_conv(self):\n        return self._config.getboolean(""Model"", ""wide_conv"")\n\n    @property\n    def CNN(self):\n        return self._config.getboolean(""Model"", ""CNN"")\n\n    @property\n    def HighWay_CNN(self):\n        return self._config.getboolean(""Model"", ""HighWay_CNN"")\n\n    @property\n    def CNN_MUI(self):\n        return self._config.getboolean(""Model"", ""CNN_MUI"")\n\n    @property\n    def DEEP_CNN(self):\n        return self._config.getboolean(""Model"", ""DEEP_CNN"")\n\n    @property\n    def DEEP_CNN_MUI(self):\n        return self._config.getboolean(""Model"", ""DEEP_CNN_MUI"")\n\n    @property\n    def LSTM(self):\n        return self._config.getboolean(""Model"", ""LSTM"")\n\n    @property\n    def GRU(self):\n        return self._config.getboolean(""Model"", ""GRU"")\n\n    @property\n    def BiLSTM(self):\n        return self._config.getboolean(""Model"", ""BiLSTM"")\n\n    @property\n    def BiLSTM_1(self):\n        return self._config.getboolean(""Model"", ""BiLSTM_1"")\n\n    @property\n    def HighWay_BiLSTM_1(self):\n        return self._config.getboolean(""Model"", ""HighWay_BiLSTM_1"")\n\n    @property\n    def CNN_LSTM(self):\n        return self._config.getboolean(""Model"", ""CNN_LSTM"")\n\n    @property\n    def CNN_BiLSTM(self):\n        return self._config.getboolean(""Model"", ""CNN_BiLSTM"")\n\n    @property\n    def CLSTM(self):\n        return self._config.getboolean(""Model"", ""CLSTM"")\n\n    @property\n    def CBiLSTM(self):\n        return self._config.getboolean(""Model"", ""CBiLSTM"")\n\n    @property\n    def CGRU(self):\n        return self._config.getboolean(""Model"", ""CGRU"")\n\n    @property\n    def BiGRU(self):\n        return self._config.getboolean(""Model"", ""BiGRU"")\n\n    @property\n    def CNN_BiGRU(self):\n        return self._config.getboolean(""Model"", ""CNN_BiGRU"")\n\n    @property\n    def embed_dim(self):\n        return self._config.getint(""Model"", ""embed_dim"")\n\n    @property\n    def lstm_hidden_dim(self):\n        return self._config.getint(""Model"", ""lstm_hidden_dim"")\n\n    @property\n    def lstm_num_layers(self):\n        return self._config.getint(""Model"", ""lstm_num_layers"")\n\n    @property\n    def batch_normalizations(self):\n        return self._config.getboolean(""Model"", ""batch_normalizations"")\n\n    @property\n    def bath_norm_momentum(self):\n        return self._config.getfloat(""Model"", ""bath_norm_momentum"")\n\n    @property\n    def batch_norm_affine(self):\n        return self._config.getboolean(""Model"", ""batch_norm_affine"")\n\n    @property\n    def dropout(self):\n        return self._config.getfloat(""Model"", ""dropout"")\n\n    @property\n    def dropout_embed(self):\n        return self._config.getfloat(""Model"", ""dropout_embed"")\n\n    @property\n    def max_norm(self):\n        value = self._config.get(""Model"", ""max_norm"")\n        if value == ""None"" or value == ""none"":\n            return None\n        else:\n            return value\n\n    @property\n    def clip_max_norm(self):\n        return self._config.getint(""Model"", ""clip_max_norm"")\n\n    @property\n    def kernel_num(self):\n        return self._config.getint(""Model"", ""kernel_num"")\n\n    @property\n    def kernel_sizes(self):\n        value = self._config.get(""Model"", ""kernel_sizes"")\n        # print(list(value))\n        value = [int(k) for k in list(value) if k != "",""]\n        return value\n\n    @kernel_sizes.setter\n    def kernel_sizes(self, value):\n        self._config.set(""Model"", ""kernel_sizes"", str(value))\n\n    @property\n    def init_weight(self):\n        return self._config.getboolean(""Model"", ""init_weight"")\n\n    @property\n    def init_weight_value(self):\n        return self._config.getfloat(""Model"", ""init_weight_value"")\n\n    # Optimizer\n    @property\n    def learning_rate(self):\n        return self._config.getfloat(""Optimizer"", ""learning_rate"")\n\n    @property\n    def Adam(self):\n        return self._config.getboolean(""Optimizer"", ""Adam"")\n\n    @property\n    def SGD(self):\n        return self._config.getboolean(""Optimizer"", ""SGD"")\n\n    @property\n    def Adadelta(self):\n        return self._config.getboolean(""Optimizer"", ""Adadelta"")\n\n    @property\n    def momentum_value(self):\n        return self._config.getfloat(""Optimizer"", ""optim_momentum_value"")\n\n    @property\n    def weight_decay(self):\n        return self._config.getfloat(""Optimizer"", ""weight_decay"")\n\n    # Train\n    @property\n    def num_threads(self):\n        return self._config.getint(""Train"", ""num_threads"")\n\n    @property\n    def device(self):\n        return self._config.getint(""Train"", ""device"")\n\n    @property\n    def cuda(self):\n        return self._config.getboolean(""Train"", ""cuda"")\n\n    @property\n    def epochs(self):\n        return self._config.getint(""Train"", ""epochs"")\n\n    @property\n    def batch_size(self):\n        return self._config.getint(""Train"", ""batch_size"")\n\n    @property\n    def log_interval(self):\n        return self._config.getint(""Train"", ""log_interval"")\n\n    @property\n    def test_interval(self):\n        return self._config.getint(""Train"", ""test_interval"")\n\n    @property\n    def save_interval(self):\n        return self._config.getint(""Train"", ""save_interval"")\n\n\n\n\n'"
DataLoader/__init__.py,0,b''
DataLoader/mydatasets_self_five.py,1,"b'# coding=utf-8\n# @Author : bamtercelboo\n# @Datetime : 2018/07/19 22:35\n# @File : mydatasets_self_five.py\n# @Last Modify Time : 2018/07/19 22:35\n# @Contact : bamtercelboo@{gmail.com, 163.com}\n\nimport re\nimport os\nimport tarfile\nfrom six.moves import urllib\nfrom torchtext import data\nimport random\nimport torch\nfrom DataUtils.Common import seed_num\ntorch.manual_seed(seed_num)\nrandom.seed(seed_num)\n\n\nclass TarDataset(data.Dataset):\n    """"""Defines a Dataset loaded from a downloadable tar archive.\n\n    Attributes:\n        url: URL where the tar archive can be downloaded.\n        filename: Filename of the downloaded tar archive.\n        dirname: Name of the top-level directory within the zip archive that\n            contains the data files.\n    """"""\n\n    @classmethod\n    def download_or_unzip(cls, root):\n        path = os.path.join(root, cls.dirname)\n        if not os.path.isdir(path):\n            tpath = os.path.join(root, cls.filename)\n            if not os.path.isfile(tpath):\n                print(\'downloading\')\n                urllib.request.urlretrieve(cls.url, tpath)\n            with tarfile.open(tpath, \'r\') as tfile:\n                print(\'extracting\')\n                tfile.extractall(root)\n        return os.path.join(path, \'\')\n\n\nclass MR(TarDataset):\n\n    @staticmethod\n    def sort_key(ex):\n        return len(ex.text)\n\n    def __init__(self, text_field, label_field, path=None, file=None, examples=None, char_data=None, **kwargs):\n        """"""Create an MR dataset instance given a path and fields.\n\n        Arguments:\n            text_field: The field that will be used for text data.\n            label_field: The field that will be used for label data.\n            path: Path to the data file.\n            examples: The examples contain all the data.\n            Remaining keyword arguments: Passed to the constructor of\n                data.Dataset.\n        """"""\n        def clean_str(string):\n            """"""\n            Tokenization/string cleaning for all datasets except for SST.\n            Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n            """"""\n            string = re.sub(r""[^A-Za-z0-9(),!?\\\'\\`]"", "" "", string)\n            string = re.sub(r""\\\'s"", "" \\\'s"", string)\n            string = re.sub(r""\\\'ve"", "" \\\'ve"", string)\n            string = re.sub(r""n\\\'t"", "" n\\\'t"", string)\n            string = re.sub(r""\\\'re"", "" \\\'re"", string)\n            string = re.sub(r""\\\'d"", "" \\\'d"", string)\n            string = re.sub(r""\\\'ll"", "" \\\'ll"", string)\n            string = re.sub(r"","", "" , "", string)\n            string = re.sub(r""!"", "" ! "", string)\n            string = re.sub(r""\\("", "" \\( "", string)\n            string = re.sub(r""\\)"", "" \\) "", string)\n            string = re.sub(r""\\?"", "" \\? "", string)\n            string = re.sub(r""\\s{2,}"", "" "", string)\n\n            return string.strip()\n\n        text_field.preprocessing = data.Pipeline(clean_str)\n        fields = [(\'text\', text_field), (\'label\', label_field)]\n\n        if examples is None:\n            path = None if os.path.join(path, file) is None else os.path.join(path, file)\n            examples = []\n            with open(path) as f:\n                a, b, c, d, e = 0, 0, 0, 0, 0\n                for line in f:\n                    sentence, flag = line.strip().split(\' ||| \')\n                    if char_data is True:\n                        sentence = sentence.split("" "")\n                        sentence = MR.char_data(self, sentence)\n                    # print(sentence)\n                    # clear string in every sentence\n                    sentence = clean_str(sentence)\n                    if line[-2] == \'0\':\n                        a += 1\n                        examples += [data.Example.fromlist([sentence, ""very negative""], fields=fields)]\n                    elif line[-2] == \'1\':\n                        b += 1\n                        examples += [data.Example.fromlist([sentence, \'negative\'], fields=fields)]\n                    elif line[-2] == \'2\':\n                        c += 1\n                        examples += [data.Example.fromlist([sentence, \'neutral\'], fields=fields)]\n                    elif line[-2] == \'3\':\n                        d += 1\n                        examples += [data.Example.fromlist([sentence, \'positive\'], fields=fields)]\n                    else:\n                        e += 1\n                        examples += [data.Example.fromlist([sentence, \'very positive\'], fields=fields)]\n                print(""a {} b {} c {} d {} e {}"".format(a, b, c, d, e))\n        super(MR, self).__init__(examples, fields, **kwargs)\n\n    def char_data(self, list):\n        data = []\n        for i in range(len(list)):\n            for j in range(len(list[i])):\n                data += list[i][j]\n        return data\n\n\n    @classmethod\n    def splits(cls, path, train, dev, test, char_data, text_field, label_field, dev_ratio=.1, shuffle=True ,root=\'.\', **kwargs):\n        """"""Create dataset objects for splits of the MR dataset.\n\n        Arguments:\n            text_field: The field that will be used for the sentence.\n            label_field: The field that will be used for label data.\n            dev_ratio: The ratio that will be used to get split validation dataset.\n            shuffle: Whether to shuffle the data before split.\n            root: The root directory that the dataset\'s zip archive will be\n                expanded into; therefore the directory in whose trees\n                subdirectory the data files will be stored.\n            train: The filename of the train data. Default: \'train.txt\'.\n            Remaining keyword arguments: Passed to the splits method of\n                Dataset.\n        """"""\n        # path = cls.download_or_unzip(root)\n        # path = ""./data/""\n        print(path + train)\n        print(path + dev)\n        print(path + test)\n        examples_train = cls(text_field, label_field, path=path, file=train, char_data=char_data, **kwargs).examples\n        examples_dev = cls(text_field, label_field, path=path, file=dev, char_data=char_data, **kwargs).examples\n        examples_test = cls(text_field, label_field, path=path, file=test,char_data=char_data, **kwargs).examples\n        if shuffle:\n            print(""shuffle data examples......"")\n            random.shuffle(examples_train)\n            random.shuffle(examples_dev)\n            random.shuffle(examples_test)\n\n        return (cls(text_field, label_field, examples=examples_train),\n                cls(text_field, label_field, examples=examples_dev),\n                cls(text_field, label_field, examples=examples_test))\n'"
DataLoader/mydatasets_self_two.py,1,"b'# coding=utf-8\n# @Author : bamtercelboo\n# @Datetime : 2018/07/19 22:35\n# @File : mydatasets_self_two.py\n# @Last Modify Time : 2018/07/19 22:35\n# @Contact : bamtercelboo@{gmail.com, 163.com}\n\nimport re\nimport os\nfrom torchtext import data\nimport random\nimport torch\nfrom DataUtils.Common import seed_num\ntorch.manual_seed(seed_num)\nrandom.seed(seed_num)\n\n\nclass MR(data.Dataset):\n\n    def __init__(self, text_field, label_field, path=None, file=None, examples=None, char_data=None, **kwargs):\n        """"""\n        Arguments:\n            text_field: The field that will be used for text data.\n            label_field: The field that will be used for label data.\n            path: Path to the data file.\n            examples: The examples contain all the data.\n            char_data: The char level to solve\n            Remaining keyword arguments: Passed to the constructor of data.Dataset.\n        """"""\n        def clean_str(string):\n            """"""\n            Tokenization/string cleaning for all datasets except for SST.\n            Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n            """"""\n            string = re.sub(r""[^A-Za-z0-9(),!?\\\'\\`]"", "" "", string)\n            string = re.sub(r""\\\'s"", "" \\\'s"", string)\n            string = re.sub(r""\\\'ve"", "" \\\'ve"", string)\n            string = re.sub(r""n\\\'t"", "" n\\\'t"", string)\n            string = re.sub(r""\\\'re"", "" \\\'re"", string)\n            string = re.sub(r""\\\'d"", "" \\\'d"", string)\n            string = re.sub(r""\\\'ll"", "" \\\'ll"", string)\n            string = re.sub(r"","", "" , "", string)\n            string = re.sub(r""!"", "" ! "", string)\n            string = re.sub(r""\\("", "" \\( "", string)\n            string = re.sub(r""\\)"", "" \\) "", string)\n            string = re.sub(r""\\?"", "" \\? "", string)\n            string = re.sub(r""\\s{2,}"", "" "", string)\n\n            return string.strip()\n\n        text_field.preprocessing = data.Pipeline(clean_str)\n        fields = [(\'text\', text_field), (\'label\', label_field)]\n\n        if examples is None:\n            path = None if os.path.join(path, file) is None else os.path.join(path, file)\n            examples = []\n            with open(path) as f:\n                a, b = 0, 0\n                # v = f.readlines()\n                # print(len(v))\n                for line in f.readlines():\n                    sentence, flag = line.strip().split(\' ||| \')\n                    if char_data is True:\n                        sentence = sentence.split("" "")\n                        sentence = MR.char_data(self, sentence)\n                    # print(sentence)\n                    # clear string in every sentence\n                    sentence = clean_str(sentence)\n                    # print(sentence)\n                    if line[-2] == \'0\':\n                        a += 1\n                        examples += [data.Example.fromlist([sentence, \'negative\'], fields=fields)]\n                    elif line[-2] == \'1\':\n                        a += 1\n                        examples += [data.Example.fromlist([sentence, \'negative\'], fields=fields)]\n                    elif line[-2] == \'3\':\n                        b += 1\n                        examples += [data.Example.fromlist([sentence, \'positive\'], fields=fields)]\n                    elif line[-2] == \'4\':\n                        b += 1\n                        examples += [data.Example.fromlist([sentence, \'positive\'], fields=fields)]\n                print(""a {} b {} "".format(a, b))\n        super(MR, self).__init__(examples, fields, **kwargs)\n\n    def char_data(self, list):\n        data = []\n        for i in range(len(list)):\n            for j in range(len(list[i])):\n                data += list[i][j]\n        return data\n\n    @classmethod\n    def splits(cls, path, train, dev, test, char_data, text_field, label_field, dev_ratio=.1, shuffle=True ,root=\'.\', **kwargs):\n        """"""Create dataset objects for splits of the MR dataset.\n        Arguments:\n            text_field: The field that will be used for the sentence.\n            label_field: The field that will be used for label data.\n            dev_ratio: The ratio that will be used to get split validation dataset.\n            shuffle: Whether to shuffle the data before split.\n            root: The root directory that the dataset\'s zip archive will be\n                expanded into; therefore the directory in whose trees\n                subdirectory the data files will be stored.\n            train: The filename of the train data. Default: \'train.txt\'.\n            Remaining keyword arguments: Passed to the splits method of\n                Dataset.\n        """"""\n        print(path + train)\n        print(path + dev)\n        print(path + test)\n        examples_train = cls(text_field, label_field, path=path, file=train, char_data=char_data, **kwargs).examples\n        examples_dev = cls(text_field, label_field, path=path, file=dev, char_data=char_data, **kwargs).examples\n        examples_test = cls(text_field, label_field, path=path, file=test, char_data=char_data, **kwargs).examples\n        if shuffle:\n            print(""shuffle data examples......"")\n            random.shuffle(examples_train)\n            random.shuffle(examples_dev)\n            random.shuffle(examples_test)\n\n        return (cls(text_field, label_field, examples=examples_train),\n                cls(text_field, label_field, examples=examples_dev),\n                cls(text_field, label_field, examples=examples_test))\n'"
DataUtils/Common.py,0,"b'# @Author : bamtercelboo\n# @Datetime : 2018/07/19 22:35\n# @File : common.py\n# @Last Modify Time : 2018/07/19 22:35\n# @Contact : bamtercelboo@{gmail.com, 163.com}\n\n""""""\n    FILE :  common.py\n    FUNCTION : Common File\n""""""\n\n# random seed number\nseed_num = 233\npad = ""<pad>""\nunk = ""<unk>""\n# print(seed_num)\n'"
DataUtils/Load_Pretrained_Embed.py,7,"b'# @Author : bamtercelboo\n# @Datetime : 2018/07/10 16.03\n# @File : train.py\n# @Last Modify Time : 2018/07/10 16.03\n# @Contact : bamtercelboo@{gmail.com, 163.com}\n\n""""""\n    FILE :  Load_Pretrained_Embed.py\n    FUNCTION : loading pretrained word embedding\n""""""\nimport torch\nimport torch.nn as nn\nimport torch.nn.init as init\nfrom collections import OrderedDict\nimport numpy as np\nimport tqdm\n\nfrom DataUtils.Common import *\ntorch.manual_seed(seed_num)\nnp.random.seed(seed_num)\n\n\ndef load_pretrained_emb_zeros(path, text_field_words_dict, pad=None, set_padding=False):\n    print(""loading pre_train embedding by zeros......"")\n    if not isinstance(text_field_words_dict, dict):\n        text_field_words_dict = convert_list2dict(text_field_words_dict)\n    if pad is not None:\n        padID = text_field_words_dict[pad]\n    embedding_dim = -1\n    with open(path, encoding=\'utf-8\') as f:\n        for line in f:\n            line_split = line.strip().split(\' \')\n            if len(line_split) == 1:\n                embedding_dim = line_split[0]\n                break\n            elif len(line_split) == 2:\n                embedding_dim = line_split[1]\n                break\n            else:\n                embedding_dim = len(line_split) - 1\n                break\n    f.close()\n    word_count = len(text_field_words_dict)\n    print(\'The number of wordsDict is {} \\nThe dim of pretrained embedding is {}\'.format(str(word_count),\n                                                                                           str(embedding_dim)))\n    embeddings = np.zeros((int(word_count), int(embedding_dim)))\n    iv_num = 0\n    oov_num = 0\n    with open(path, encoding=\'utf-8\') as f:\n        lines = f.readlines()\n        # lines = tqdm.tqdm(lines)\n        for line in lines:\n            values = line.strip().split(\' \')\n            if len(values) == 1 or len(values) == 2:\n                continue\n            index = text_field_words_dict.get(values[0])  # digit or None\n            if index:\n                iv_num += 1\n                vector = np.array([float(i) for i in values[1:]], dtype=\'float32\')\n                embeddings[index] = vector\n\n    f.close()\n    oov_num = word_count - iv_num\n    print(""iv_num {} oov_num {} oov_radio {:.4f}%"".format(iv_num, oov_num, round((oov_num / word_count) * 100, 4)))\n    return torch.from_numpy(embeddings).float()\n\n\ndef load_pretrained_emb_Embedding(path, text_field_words_dict, pad=None, set_padding=False):\n    print(""loading pre_train embedding by nn.Embedding......"")\n    if not isinstance(text_field_words_dict, dict):\n        text_field_words_dict = convert_list2dict(text_field_words_dict)\n    if pad is not None:\n        padID = text_field_words_dict[pad]\n    embedding_dim = -1\n    with open(path, encoding=\'utf-8\') as f:\n        for line in f:\n            line_split = line.strip().split(\' \')\n            if len(line_split) == 1:\n                embedding_dim = line_split[0]\n                break\n            elif len(line_split) == 2:\n                embedding_dim = line_split[1]\n                break\n            else:\n                embedding_dim = len(line_split) - 1\n                break\n    f.close()\n    word_count = len(text_field_words_dict)\n    print(\'The number of wordsDict is {} \\nThe dim of pretrained embedding is {}\'.format(str(word_count),\n                                                                                         str(embedding_dim)))\n    embed = nn.Embedding(int(word_count), int(embedding_dim))\n    init.xavier_uniform(embed.weight.data)\n    embeddings = np.array(embed.weight.data)\n    iv_num = 0\n    oov_num = 0\n    with open(path, encoding=\'utf-8\') as f:\n        lines = f.readlines()\n        lines = tqdm.tqdm(lines)\n        for line in lines:\n            values = line.strip().split(\' \')\n            if len(values) == 1 or len(values) == 2:\n                continue\n            index = text_field_words_dict.get(values[0])  # digit or None\n            if index:\n                iv_num += 1\n                vector = np.array([float(i) for i in values[1:]], dtype=\'float32\')\n                embeddings[index] = vector\n\n    f.close()\n    oov_num = word_count - iv_num\n    print(""iv_num {} oov_num {} oov_radio {:.4f}%"".format(iv_num, oov_num, round((oov_num / word_count) * 100, 4)))\n    return torch.from_numpy(embeddings).float()\n\n\ndef load_pretrained_emb_avg(path, text_field_words_dict, pad=None, set_padding=False):\n    print(""loading pre_train embedding by avg......"")\n    if not isinstance(text_field_words_dict, dict):\n        text_field_words_dict = convert_list2dict(text_field_words_dict)\n    assert pad is not None, ""pad not allow with None""\n    padID = text_field_words_dict[pad]\n    embedding_dim = -1\n    with open(path, encoding=\'utf-8\') as f:\n        for line in f:\n            line_split = line.strip().split(\' \')\n            if len(line_split) == 1:\n                embedding_dim = line_split[0]\n                break\n            elif len(line_split) == 2:\n                embedding_dim = line_split[1]\n                break\n            else:\n                embedding_dim = len(line_split) - 1\n                break\n    f.close()\n    word_count = len(text_field_words_dict)\n    print(\'The number of wordsDict is {} \\nThe dim of pretrained embedding is {}\\n\'.format(str(word_count),\n                                                                                           str(embedding_dim)))\n    embeddings = np.zeros((int(word_count), int(embedding_dim)))\n\n    inword_list = {}\n    with open(path, encoding=\'utf-8\') as f:\n        lines = f.readlines()\n        lines = tqdm.tqdm(lines)\n        for line in lines:\n            lines.set_description(""Processing"")\n            values = line.strip().split("" "")\n            if len(values) == 1 or len(values) == 2:\n                continue\n            index = text_field_words_dict.get(values[0])  # digit or None\n            if index:\n                vector = np.array([float(i) for i in values[1:]], dtype=\'float32\')\n                embeddings[index] = vector\n                inword_list[index] = 1\n    f.close()\n    print(""oov words initial by avg embedding, maybe take a while......"")\n    sum_col = np.sum(embeddings, axis=0) / len(inword_list)     # avg\n    for i in range(len(text_field_words_dict)):\n        if i not in inword_list and i != padID:\n            embeddings[i] = sum_col\n\n    OOVWords = word_count - len(inword_list)\n    oov_radio = np.round(OOVWords / word_count, 6)\n    print(""All Words = {}, InWords = {}, OOVWords = {}, OOV Radio={}"".format(\n        word_count, len(inword_list), OOVWords, oov_radio))\n\n    return torch.from_numpy(embeddings).float()\n\n\ndef load_pretrained_emb_uniform(path, text_field_words_dict, pad=None, set_padding=False):\n    print(""loading pre_train embedding by uniform......"")\n    if not isinstance(text_field_words_dict, dict):\n        text_field_words_dict = convert_list2dict(text_field_words_dict)\n    assert pad is not None, ""pad not allow with None""\n    padID = text_field_words_dict[pad]\n    embedding_dim = -1\n    with open(path, encoding=\'utf-8\') as f:\n        for line in f:\n            line_split = line.strip().split(\' \')\n            if len(line_split) == 1:\n                embedding_dim = line_split[0]\n                break\n            elif len(line_split) == 2:\n                embedding_dim = line_split[1]\n                break\n            else:\n                embedding_dim = len(line_split) - 1\n                break\n    f.close()\n    word_count = len(text_field_words_dict)\n    print(\'The number of wordsDict is {} \\nThe dim of pretrained embedding is {}\\n\'.format(str(word_count),\n                                                                                           str(embedding_dim)))\n    embeddings = np.zeros((int(word_count), int(embedding_dim)))\n\n    inword_list = {}\n    with open(path, encoding=\'utf-8\') as f:\n        lines = f.readlines()\n        lines = tqdm.tqdm(lines)\n        for line in lines:\n            lines.set_description(""Processing"")\n            values = line.strip().split("" "")\n            if len(values) == 1 or len(values) == 2:\n                continue\n            index = text_field_words_dict.get(values[0])  # digit or None\n            if index:\n                vector = np.array([float(i) for i in values[1:]], dtype=\'float32\')\n                embeddings[index] = vector\n                inword_list[index] = 1\n    f.close()\n    print(""oov words initial by uniform embedding, maybe take a while......"")\n    # sum_col = np.sum(embeddings, axis=0) / len(inword_list)     # avg\n    uniform_col = np.random.uniform(-0.25, 0.25, int(embedding_dim)).round(6)    # avg\n    for i in range(len(text_field_words_dict)):\n        if i not in inword_list and i != padID:\n            embeddings[i] = uniform_col\n\n    OOVWords = word_count - len(inword_list)\n    oov_radio = np.round(OOVWords / word_count, 6)\n    print(""All Words = {}, InWords = {}, OOVWords = {}, OOV Radio={}"".format(\n        word_count, len(inword_list), OOVWords, oov_radio))\n\n    return torch.from_numpy(embeddings).float()\n\n\ndef convert_list2dict(convert_list):\n    list_dict = OrderedDict()\n    for index, word in enumerate(convert_list):\n        list_dict[word] = index\n    return list_dict\n\n\n'"
DataUtils/__init__.py,0,b''
DataUtils/handle_wordEmbedding2File.py,0,"b'# coding=utf-8\n# @Author : bamtercelboo\n# @Datetime : 2018/07/19 22:35\n# @File : handle_wordEmbedding2File.py\n# @Last Modify Time : 2018/07/19 22:35\n# @Contact : bamtercelboo@{gmail.com, 163.com}\n\n""""""\n    handle external word embedding to file\n""""""\nimport os\nimport tqdm\n\n\nclass WordEmbedding2File:\n    def __init__(self, wordEmbedding_path, data_path, extract_path):\n        print(""handling external word embedding to file"")\n        self.wordEmbedding_path = wordEmbedding_path\n        self.data_path = data_path\n        self.extract_path = extract_path\n        self.data_dict = self.read_data(data_path)\n        self.extract_dict = {}\n        self.dim = 100\n        self.read_vectors(self.wordEmbedding_path)\n        self.write(self.extract_path, self.extract_dict)\n        # print(self.data_dict)\n        # print(self.extract_dict)\n\n    def read_data(self, path):\n        print(""read data file {}"".format(path))\n        data_list = []\n        with open(path, encoding=""UTF-8"") as f:\n            for line in f.readlines():\n                line = line.strip(""\\n"").split("" "")[:-2]\n                data_list.extend(line)\n        return set(data_list)\n\n    def read_vectors(self, path):\n        print(""read embedding path {}"".format(path))\n        with open(path, encoding=\'utf-8\') as f:\n            lines = f.readlines()\n            self.dim = len(lines[2].strip(""\\n"").strip().split("" "")[1:-1])\n            # print(dim)\n            lines = tqdm.tqdm(lines)\n            for line in lines:\n                values = line.strip(""\\n"").strip().split("" "")\n                if len(values) == 1 or len(values) == 2 or len(values) == 3:\n                    continue\n                word, vector = values[0], values[1:-1]\n                if word in self.data_dict:\n                    self.extract_dict[word] = vector\n\n    def write(self, path, dict):\n        print(""writing to {}"".format(path))\n        if os.path.exists(path):\n            os.remove(path)\n        file = open(path, encoding=""UTF-8"", mode=""w"")\n        all_words, dim = len(dict), self.dim\n        print(all_words, dim)\n        file.write(str(all_words) + "" "" + str(dim) + ""\\n"")\n        for word in dict:\n            value = "" "".join(dict[word])\n            v = word + "" "" + value + ""\\n""\n            file.write(v)\n\n\nif __name__ == ""__main__"":\n    wordEmbedding_path = ""GoogleNews_wordEmbedding/vectors.utf-8""\n    data_path = ""./sst_all.txt""\n    extract_path = ""./extract_googleNews_embed_sst.txt""\n    WordEmbedding2File(wordEmbedding_path=wordEmbedding_path, data_path=data_path, extract_path=extract_path)\n\n'"
models/__init__.py,0,b''
models/model.py,7,"b'# @Author : bamtercelboo\n# @Datetime : 2018/07/19 22:35\n# @File : model.py\n# @Last Modify Time : 2018/07/19 22:35\n# @Contact : bamtercelboo@{gmail.com, 163.com}\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport numpy as np\nimport random\nfrom DataUtils.Common import seed_num\ntorch.manual_seed(seed_num)\nrandom.seed(seed_num)\n\n\nclass  CNN_Text(nn.Module):\n    \n    def __init__(self, args):\n        super(CNN_Text,self).__init__()\n        self.args = args\n        \n        V = args.embed_num\n        D = args.embed_dim\n        C = args.class_num\n        Ci = 1\n        Co = args.kernel_num\n        Ks = args.kernel_sizes\n        self.embed = nn.Embedding(V, D)\n        pretrained_weight = np.array(args.pretrained_weight)\n        self.embed.weight.data.copy_(torch.from_numpy(pretrained_weight))\n        # print(""bbbbbbbb"", self.embed.weight)\n        self.convs1 = [nn.Conv2d(Ci, Co, (K, D)) for K in Ks]\n        \'\'\'\n        self.conv13 = nn.Conv2d(Ci, Co, (3, D))\n        self.conv14 = nn.Conv2d(Ci, Co, (4, D))\n        self.conv15 = nn.Conv2d(Ci, Co, (5, D))\n        \'\'\'\n        self.dropout = nn.Dropout(args.dropout)\n        self.fc1 = nn.Linear(len(Ks)*Co, C)\n\n    def conv_and_pool(self, x, conv):\n        x = F.relu(conv(x)).squeeze(3) #(N,Co,W)\n        x = F.max_pool1d(x, x.size(2)).squeeze(2)\n        return x\n\n    def forward(self, x):\n        x = self.embed(x)   # (N,W,D)\n        if self.args.static:\n            x = Variable(x.data)\n        x = x.unsqueeze(1)  # (N,Ci,W,D)\n        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs1]    # [(N,Co,W), ...]*len(Ks)\n        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]  # [(N,Co), ...]*len(Ks)\n        x = torch.cat(x, 1)\n        \'\'\'\n        x1 = self.conv_and_pool(x,self.conv13) #(N,Co)\n        x2 = self.conv_and_pool(x,self.conv14) #(N,Co)\n        x3 = self.conv_and_pool(x,self.conv15) #(N,Co)\n        x = torch.cat((x1, x2, x3), 1) # (N,len(Ks)*Co)\n        \'\'\'\n        x = self.dropout(x)     # (N,len(Ks)*Co)\n        logit = self.fc1(x)     # (N,C)\n        return logit'"
models/model_BiGRU.py,6,"b'# @Author : bamtercelboo\n# @Datetime : 2018/07/19 22:35\n# @File : model_BiGRU.py\n# @Last Modify Time : 2018/07/19 22:35\n# @Contact : bamtercelboo@{gmail.com, 163.com}\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport numpy as np\nimport random\nfrom DataUtils.Common import seed_num\ntorch.manual_seed(seed_num)\nrandom.seed(seed_num)\n\n""""""\nNeural Networks model : Bidirection GRU\n""""""\n\n\nclass BiGRU(nn.Module):\n    \n    def __init__(self, args):\n        super(BiGRU, self).__init__()\n        self.args = args\n        self.hidden_dim = args.lstm_hidden_dim\n        self.num_layers = args.lstm_num_layers\n        V = args.embed_num\n        D = args.embed_dim\n        C = args.class_num\n        self.embed = nn.Embedding(V, D, padding_idx=args.paddingId)\n        # pretrained  embedding\n        if args.word_Embedding:\n            self.embed.weight.data.copy_(args.pretrained_weight)\n        # gru\n        self.bigru = nn.GRU(D, self.hidden_dim, dropout=args.dropout, num_layers=self.num_layers, bidirectional=True)\n        # linear\n        self.hidden2label = nn.Linear(self.hidden_dim * 2, C)\n        #  dropout\n        self.dropout = nn.Dropout(args.dropout)\n\n    def forward(self, input):\n        embed = self.embed(input)\n        embed = self.dropout(embed)\n        input = embed.view(len(input), embed.size(1), -1)\n        # gru\n        gru_out, _ = self.bigru(input)\n        gru_out = torch.transpose(gru_out, 0, 1)\n        gru_out = torch.transpose(gru_out, 1, 2)\n        # pooling\n        # gru_out = F.tanh(gru_out)\n        gru_out = F.max_pool1d(gru_out, gru_out.size(2)).squeeze(2)\n        gru_out = F.tanh(gru_out)\n        # linear\n        y = self.hidden2label(gru_out)\n        logit = y\n        return logit'"
models/model_BiLSTM.py,6,"b'# @Author : bamtercelboo\n# @Datetime : 2018/07/19 22:35\n# @File : model_BiLSTM.py\n# @Last Modify Time : 2018/07/19 22:35\n# @Contact : bamtercelboo@{gmail.com, 163.com}\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport numpy as np\nimport random\nfrom DataUtils.Common import seed_num\ntorch.manual_seed(seed_num)\nrandom.seed(seed_num)\n\n""""""\nNeural Networks model : Bidirection LSTM\n""""""\n\n\nclass BiLSTM(nn.Module):\n    \n    def __init__(self, args):\n        super(BiLSTM, self).__init__()\n        self.args = args\n        self.hidden_dim = args.lstm_hidden_dim\n        self.num_layers = args.lstm_num_layers\n        V = args.embed_num\n        D = args.embed_dim\n        C = args.class_num\n        # self.embed = nn.Embedding(V, D, max_norm=config.max_norm)\n        self.embed = nn.Embedding(V, D, padding_idx=args.paddingId)\n        # pretrained  embedding\n        if args.word_Embedding:\n            self.embed.weight.data.copy_(args.pretrained_weight)\n        self.bilstm = nn.LSTM(D, self.hidden_dim // 2, num_layers=1, dropout=args.dropout, bidirectional=True, bias=False)\n        print(self.bilstm)\n\n        self.hidden2label1 = nn.Linear(self.hidden_dim, self.hidden_dim // 2)\n        self.hidden2label2 = nn.Linear(self.hidden_dim // 2, C)\n        # self.dropout = nn.Dropout(config.dropout)\n\n    def forward(self, x):\n        embed = self.embed(x)\n        x = embed.view(len(x), embed.size(1), -1)\n        bilstm_out, _ = self.bilstm(x)\n\n        bilstm_out = torch.transpose(bilstm_out, 0, 1)\n        bilstm_out = torch.transpose(bilstm_out, 1, 2)\n        bilstm_out = F.tanh(bilstm_out)\n        bilstm_out = F.max_pool1d(bilstm_out, bilstm_out.size(2)).squeeze(2)\n        y = self.hidden2label1(bilstm_out)\n        y = self.hidden2label2(y)\n        logit = y\n        return logit'"
models/model_BiLSTM_1.py,7,"b'# @Author : bamtercelboo\n# @Datetime : 2018/07/19 22:35\n# @File : model_BiLSTM_1.py\n# @Last Modify Time : 2018/07/19 22:35\n# @Contact : bamtercelboo@{gmail.com, 163.com}\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport numpy as np\nimport random\nimport torch.nn.init as init\nfrom DataUtils.Common import seed_num\ntorch.manual_seed(seed_num)\nrandom.seed(seed_num)\n\n""""""\nNeural Networks model : Bidirection LSTM\n""""""\n\n\nclass BiLSTM_1(nn.Module):\n    def __init__(self, args):\n        super(BiLSTM_1, self).__init__()\n        self.args = args\n        self.hidden_dim = args.lstm_hidden_dim\n        self.num_layers = args.lstm_num_layers\n        V = args.embed_num\n        D = args.embed_dim\n        C = args.class_num\n        self.dropout = nn.Dropout(args.dropout)\n        self.dropout_embed = nn.Dropout(args.dropout_embed)\n        if args.max_norm is not None:\n            print(""max_norm = {} "".format(args.max_norm))\n            self.embed = nn.Embedding(V, D, max_norm=args.max_norm, scale_grad_by_freq=True, padding_idx=args.paddingId)\n            # pretrained  embedding\n            if args.word_Embedding:\n                self.embed.weight.data.copy_(args.pretrained_weight)\n        else:\n            print(""max_norm = {} |||||"".format(args.max_norm))\n            self.embed = nn.Embedding(V, D, scale_grad_by_freq=True, padding_idx=args.paddingId)\n            # pretrained  embedding\n            if args.word_Embedding:\n                self.embed.weight.data.copy_(args.pretrained_weight)\n\n        self.bilstm = nn.LSTM(D, self.hidden_dim, num_layers=self.num_layers, bias=True, bidirectional=True,\n                              dropout=self.args.dropout)\n        print(self.bilstm)\n        if args.init_weight:\n            print(""Initing W ......."")\n            init.xavier_normal(self.bilstm.all_weights[0][0], gain=np.sqrt(args.init_weight_value))\n            init.xavier_normal(self.bilstm.all_weights[0][1], gain=np.sqrt(args.init_weight_value))\n            init.xavier_normal(self.bilstm.all_weights[1][0], gain=np.sqrt(args.init_weight_value))\n            init.xavier_normal(self.bilstm.all_weights[1][1], gain=np.sqrt(args.init_weight_value))\n\n        self.hidden2label = nn.Linear(self.hidden_dim * 2, C)\n\n    def forward(self, x):\n        x = self.embed(x)\n        x = self.dropout_embed(x)\n        bilstm_out, _ = self.bilstm(x)\n\n        bilstm_out = torch.transpose(bilstm_out, 0, 1)\n        bilstm_out = torch.transpose(bilstm_out, 1, 2)\n        bilstm_out = F.tanh(bilstm_out)\n        bilstm_out = F.max_pool1d(bilstm_out, bilstm_out.size(2)).squeeze(2)\n        bilstm_out = F.tanh(bilstm_out)\n\n        logit = self.hidden2label(bilstm_out)\n\n        return logit'"
models/model_BiLSTM_lexicon.py,6,"b'# @Author : bamtercelboo\n# @Datetime : 2018/07/19 22:35\n# @File : model_BiLSTM_lexicon.py\n# @Last Modify Time : 2018/07/19 22:35\n# @Contact : bamtercelboo@{gmail.com, 163.com}\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport numpy as np\nimport random\nfrom DataUtils.Common import seed_num\ntorch.manual_seed(seed_num)\nrandom.seed(seed_num)\n\n\nclass BiLSTM_1(nn.Module):\n    \n    def __init__(self, args):\n        super(BiLSTM_1, self).__init__()\n        self.args = args\n        self.hidden_dim = args.lstm_hidden_dim\n        self.num_layers = args.lstm_num_layers\n        V = args.embed_num\n        D = args.embed_dim\n        C = args.class_num\n        # self.embed = nn.Embedding(V, D, max_norm=config.max_norm)\n        self.embed = nn.Embedding(V, D, padding_idx=args.paddingId)\n        # pretrained  embedding\n        if args.word_Embedding:\n            self.embed.weight.data.copy_(args.pretrained_weight)\n        self.bilstm = nn.LSTM(D, self.hidden_dim, num_layers=self.num_layers, dropout=args.dropout, bidirectional=True, bias=True)\n        # print(self.bilstm)\n        self.hidden2label = nn.Linear(self.hidden_dim * 2 * 2, C, bias=True)\n        self.dropout = nn.Dropout(args.dropout)\n\n    def forward(self, x):\n        embed = self.embed(x)\n        embed = self.dropout(embed)\n        x = embed.view(len(x), embed.size(1), -1)\n        bilstm_out, _ = self.bilstm(x)\n        hidden = torch.cat(self.hidden, 0)\n        hidden = torch.cat(hidden, 1)\n        logit = self.hidden2label(F.tanh(hidden))\n        return logit'"
models/model_CBiLSTM.py,8,"b'# @Author : bamtercelboo\n# @Datetime : 2018/07/19 22:35\n# @File : model_CBiLSTM.py\n# @Last Modify Time : 2018/07/19 22:35\n# @Contact : bamtercelboo@{gmail.com, 163.com}\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport numpy as np\nimport random\nfrom DataUtils.Common import seed_num\ntorch.manual_seed(seed_num)\nrandom.seed(seed_num)\n\n""""""\n    Neural Network: CBiLSTM\n    Detail: The inpout first cross CNN model ,then the output of CNN as the input of BiLSTM\n""""""\n\n\nclass CBiLSTM(nn.Module):\n    \n    def __init__(self, args):\n        super(CBiLSTM, self).__init__()\n        self.args = args\n        self.hidden_dim = args.lstm_hidden_dim\n        self.num_layers = args.lstm_num_layers\n        V = args.embed_num\n        D = args.embed_dim\n        C = args.class_num\n        Ci = 1\n        Co = args.kernel_num\n        Ks = args.kernel_sizes\n        self.embed = nn.Embedding(V, D, padding_idx=args.paddingId)\n        # pretrained  embedding\n        if args.word_Embedding:\n            self.embed.weight.data.copy_(args.pretrained_weight)\n\n        # CNN\n        KK = []\n        for K in Ks:\n            KK.append(K + 1 if K % 2 == 0 else K)\n        # self.convs1 = [nn.Conv2d(Ci, Co, (K, D), stride=1, padding=(K//2, 0)) for K in KK]\n        self.convs1 = [nn.Conv2d(Ci, D, (K, D), stride=1, padding=(K//2, 0)) for K in KK]\n\n        # for cnn cuda\n        if self.args.cuda is True:\n            for conv in self.convs1:\n                conv = conv.cuda()\n\n        # LSTM\n        self.bilstm = nn.LSTM(D, self.hidden_dim, num_layers=self.num_layers, dropout=args.dropout, bidirectional=True)\n\n        # linear\n        self.hidden2label1 = nn.Linear(self.hidden_dim * 2, self.hidden_dim)\n        self.hidden2label2 = nn.Linear(self.hidden_dim, C)\n\n        # dropout\n        self.dropout = nn.Dropout(args.dropout)\n\n    def forward(self, x):\n        embed = self.embed(x)\n        # CNN\n        embed = self.dropout(embed)\n        cnn_x = embed\n        cnn_x = cnn_x.unsqueeze(1)\n        cnn_x = [F.relu(conv(cnn_x)).squeeze(3) for conv in self.convs1]  # [(N,Co,W), ...]*len(Ks)\n        cnn_x = torch.cat(cnn_x, 0)\n        cnn_x = torch.transpose(cnn_x, 1, 2)\n\n        # BiLSTM\n        bilstm_out, _ = self.bilstm(cnn_x)\n        bilstm_out = torch.transpose(bilstm_out, 0, 1)\n        bilstm_out = torch.transpose(bilstm_out, 1, 2)\n        bilstm_out = F.max_pool1d(bilstm_out, bilstm_out.size(2)).squeeze(2)\n\n        # linear\n        cnn_bilstm_out = self.hidden2label1(F.tanh(bilstm_out))\n        cnn_bilstm_out = self.hidden2label2(F.tanh(cnn_bilstm_out))\n\n        # dropout\n        logit = self.dropout(cnn_bilstm_out)\n\n        return logit\n'"
models/model_CGRU.py,8,"b'# @Author : bamtercelboo\n# @Datetime : 2018/07/19 22:35\n# @File : model_CGRU.py\n# @Last Modify Time : 2018/07/19 22:35\n# @Contact : bamtercelboo@{gmail.com, 163.com}\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport numpy as np\nimport random\nfrom DataUtils.Common import seed_num\ntorch.manual_seed(seed_num)\nrandom.seed(seed_num)\n\n""""""\n    Neural Network: CGRU\n    Detail: The input first cross CNN model ,then the output of CNN as the input of GRU\n""""""\n\n\nclass CGRU(nn.Module):\n\n    def __init__(self, args):\n        super(CGRU, self).__init__()\n        self.args = args\n        self.hidden_dim = args.lstm_hidden_dim\n        self.num_layers = args.lstm_num_layers\n        V = args.embed_num\n        D = args.embed_dim\n        C = args.class_num\n        Ci = 1\n        Co = args.kernel_num\n        Ks = args.kernel_sizes\n        self.embed = nn.Embedding(V, D, padding_idx=args.paddingId)\n        # pretrained  embedding\n        if args.word_Embedding:\n            self.embed.weight.data.copy_(args.pretrained_weight)\n\n        # CNN\n        KK = []\n        for K in Ks:\n            KK.append( K + 1 if K % 2 == 0 else K)\n        # self.convs1 = [nn.Conv2d(Ci, Co, (K, D), stride=1, padding=(K//2, 0)) for K in KK]\n        self.convs1 = [nn.Conv2d(Ci, D, (K, D), stride=1, padding=(K//2, 0)) for K in KK]\n        # for cnn cuda\n        if self.args.cuda is True:\n            for conv in self.convs1:\n                conv = conv.cuda()\n        # GRU\n        self.gru = nn.GRU(D, self.hidden_dim, num_layers=self.num_layers, dropout=args.dropout)\n        # linear\n        self.hidden2label1 = nn.Linear(self.hidden_dim, self.hidden_dim // 2)\n        self.hidden2label2 = nn.Linear(self.hidden_dim // 2, C)\n        # dropout\n        self.dropout = nn.Dropout(args.dropout)\n\n    def forward(self, x):\n        embed = self.embed(x)\n        # CNN\n        cnn_x = embed\n        cnn_x = self.dropout(cnn_x)\n        cnn_x = cnn_x.unsqueeze(1)\n        cnn_x = [F.relu(conv(cnn_x)).squeeze(3) for conv in self.convs1]  # [(N,Co,W), ...]*len(Ks)\n        cnn_x = torch.cat(cnn_x, 0)\n        cnn_x = torch.transpose(cnn_x, 1, 2)\n        # GRU\n        lstm_out, _ = self.gru(cnn_x)\n        lstm_out = torch.transpose(lstm_out, 0, 1)\n        lstm_out = torch.transpose(lstm_out, 1, 2)\n        lstm_out = F.max_pool1d(lstm_out, lstm_out.size(2)).squeeze(2)\n        # linear\n        cnn_lstm_out = self.hidden2label1(F.tanh(lstm_out))\n        cnn_lstm_out = self.hidden2label2(F.tanh(cnn_lstm_out))\n        # output\n        logit = cnn_lstm_out\n\n        return logit\n'"
models/model_CLSTM.py,8,"b'# @Author : bamtercelboo\n# @Datetime : 2018/07/19 22:35\n# @File : model_CLSTM.py\n# @Last Modify Time : 2018/07/19 22:35\n# @Contact : bamtercelboo@{gmail.com, 163.com}\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport numpy as np\nimport random\nfrom DataUtils.Common import seed_num\ntorch.manual_seed(seed_num)\nrandom.seed(seed_num)\n\n""""""\n    Neural Network: CLSTM\n    Detail: The input first cross CNN model ,then the output of CNN as the input of LSTM\n""""""\n\n\nclass CLSTM(nn.Module):\n    \n    def __init__(self, args):\n        super(CLSTM, self).__init__()\n        self.args = args\n        self.hidden_dim = args.lstm_hidden_dim\n        self.num_layers = args.lstm_num_layers\n        V = args.embed_num\n        D = args.embed_dim\n        C = args.class_num\n        Ci = 1\n        Co = args.kernel_num\n        Ks = args.kernel_sizes\n        self.embed = nn.Embedding(V, D, padding_idx=args.paddingId)\n        # pretrained  embedding\n        if args.word_Embedding:\n            self.embed.weight.data.copy_(args.pretrained_weight)\n\n        # CNN\n        KK = []\n        for K in Ks:\n            KK.append( K + 1 if K % 2 == 0 else K)\n        # self.convs1 = [nn.Conv2d(Ci, Co, (K, D), stride=1, padding=(K//2, 0)) for K in KK]\n        self.convs1 = [nn.Conv2d(Ci, D, (K, D), stride=1, padding=(K//2, 0)) for K in KK]\n        # for cnn cuda\n        if self.args.cuda is True:\n            for conv in self.convs1:\n                conv = conv.cuda()\n        # LSTM\n        self.lstm = nn.LSTM(D, self.hidden_dim, num_layers= self.num_layers, dropout=args.dropout)\n\n        # linear\n        self.hidden2label1 = nn.Linear(self.hidden_dim, self.hidden_dim // 2)\n        self.hidden2label2 = nn.Linear(self.hidden_dim // 2, C)\n        # dropout\n        self.dropout = nn.Dropout(args.dropout)\n\n    def forward(self, x):\n        embed = self.embed(x)\n        # CNN\n        cnn_x = embed\n        cnn_x = self.dropout(cnn_x)\n        cnn_x = cnn_x.unsqueeze(1)\n        cnn_x = [F.relu(conv(cnn_x)).squeeze(3) for conv in self.convs1]  # [(N,Co,W), ...]*len(Ks)\n        cnn_x = torch.cat(cnn_x, 0)\n        cnn_x = torch.transpose(cnn_x, 1, 2)\n        # LSTM\n        lstm_out, _ = self.lstm(cnn_x)\n        lstm_out = torch.transpose(lstm_out, 0, 1)\n        lstm_out = torch.transpose(lstm_out, 1, 2)\n        lstm_out = F.max_pool1d(lstm_out, lstm_out.size(2)).squeeze(2)\n        # linear\n        cnn_lstm_out = self.hidden2label1(F.tanh(lstm_out))\n        cnn_lstm_out = self.hidden2label2(F.tanh(cnn_lstm_out))\n        # output\n        logit = cnn_lstm_out\n\n        return logit\n'"
models/model_CNN.py,6,"b'# @Author : bamtercelboo\n# @Datetime : 2018/07/19 22:35\n# @File : model_CNN.py\n# @Last Modify Time : 2018/07/19 22:35\n# @Contact : bamtercelboo@{gmail.com, 163.com}\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport numpy as np\nimport random\nimport torch.nn.init as init\nfrom DataUtils.Common import seed_num\ntorch.manual_seed(seed_num)\nrandom.seed(seed_num)\n\n""""""\n    Neural Network: CNN\n""""""\n\n\nclass CNN_Text(nn.Module):\n    \n    def __init__(self, args):\n        super(CNN_Text, self).__init__()\n        self.args = args\n        \n        V = args.embed_num\n        D = args.embed_dim\n        C = args.class_num\n        Ci = 1\n        Co = args.kernel_num\n        Ks = args.kernel_sizes\n\n        if args.max_norm is not None:\n            print(""max_norm = {} "".format(args.max_norm))\n            self.embed = nn.Embedding(V, D, max_norm=5, scale_grad_by_freq=True, padding_idx=args.paddingId)\n        else:\n            print(""max_norm = {} "".format(args.max_norm))\n            self.embed = nn.Embedding(V, D, scale_grad_by_freq=True, padding_idx=args.paddingId)\n        if args.word_Embedding:\n            self.embed.weight.data.copy_(args.pretrained_weight)\n            # fixed the word embedding\n            self.embed.weight.requires_grad = True\n        print(""dddd {} "".format(self.embed.weight.data.size()))\n\n        if args.wide_conv is True:\n            print(""using wide convolution"")\n            self.convs1 = [nn.Conv2d(in_channels=Ci, out_channels=Co, kernel_size=(K, D), stride=(1, 1),\n                                     padding=(K//2, 0), dilation=1, bias=False) for K in Ks]\n        else:\n            print(""using narrow convolution"")\n            self.convs1 = [nn.Conv2d(in_channels=Ci, out_channels=Co, kernel_size=(K, D), bias=True) for K in Ks]\n        print(self.convs1)\n\n        if args.init_weight:\n            print(""Initing W ......."")\n            for conv in self.convs1:\n                init.xavier_normal(conv.weight.data, gain=np.sqrt(args.init_weight_value))\n                fan_in, fan_out = CNN_Text.calculate_fan_in_and_fan_out(conv.weight.data)\n                print("" in {} out {} "".format(fan_in, fan_out))\n                std = np.sqrt(args.init_weight_value) * np.sqrt(2.0 / (fan_in + fan_out))\n        # for cnn cuda\n        if self.args.cuda is True:\n            for conv in self.convs1:\n                conv = conv.cuda()\n\n        self.dropout = nn.Dropout(args.dropout)\n        self.dropout_embed = nn.Dropout(args.dropout_embed)\n        in_fea = len(Ks) * Co\n        self.fc = nn.Linear(in_features=in_fea, out_features=C, bias=True)\n        # whether to use batch normalizations\n        if args.batch_normalizations is True:\n            print(""using batch_normalizations in the model......"")\n            self.convs1_bn = nn.BatchNorm2d(num_features=Co, momentum=args.bath_norm_momentum,\n                                            affine=args.batch_norm_affine)\n            self.fc1_bn = nn.BatchNorm1d(num_features=in_fea//2, momentum=args.bath_norm_momentum,\n                                         affine=args.batch_norm_affine)\n            self.fc2_bn = nn.BatchNorm1d(num_features=C, momentum=args.bath_norm_momentum,\n                                         affine=args.batch_norm_affine)\n\n    def calculate_fan_in_and_fan_out(tensor):\n        dimensions = tensor.ndimension()\n        if dimensions < 2:\n            raise ValueError(""Fan in and fan out can not be computed for tensor with less than 2 dimensions"")\n\n        if dimensions == 2:  # Linear\n            fan_in = tensor.size(1)\n            fan_out = tensor.size(0)\n        else:\n            num_input_fmaps = tensor.size(1)\n            num_output_fmaps = tensor.size(0)\n            receptive_field_size = 1\n            if tensor.dim() > 2:\n                receptive_field_size = tensor[0][0].numel()\n            fan_in = num_input_fmaps * receptive_field_size\n            fan_out = num_output_fmaps * receptive_field_size\n\n        return fan_in, fan_out\n\n    def forward(self, x):\n        x = self.embed(x)  # (N,W,D)\n        x = self.dropout_embed(x)\n        x = x.unsqueeze(1)  # (N,Ci,W,D)\n        if self.args.batch_normalizations is True:\n            x = [self.convs1_bn(F.tanh(conv(x))).squeeze(3) for conv in self.convs1] #[(N,Co,W), ...]*len(Ks)\n            x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x] #[(N,Co), ...]*len(Ks)\n        else:\n            x = [F.relu(conv(x)).squeeze(3) for conv in self.convs1] #[(N,Co,W), ...]*len(Ks)\n            x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x] #[(N,Co), ...]*len(Ks)\n        x = torch.cat(x, 1)\n        x = self.dropout(x)  # (N,len(Ks)*Co)\n        if self.args.batch_normalizations is True:\n            x = self.fc1_bn(self.fc1(x))\n            logit = self.fc2_bn(self.fc2(F.tanh(x)))\n        else:\n            logit = self.fc(x)\n        return logit\n'"
models/model_CNN_BiGRU.py,12,"b'# @Author : bamtercelboo\n# @Datetime : 2018/07/19 22:35\n# @File : model_CNN_BiGRU.py\n# @Last Modify Time : 2018/07/19 22:35\n# @Contact : bamtercelboo@{gmail.com, 163.com}\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport numpy as np\nimport random\nfrom DataUtils.Common import seed_num\ntorch.manual_seed(seed_num)\nrandom.seed(seed_num)\n\n\n""""""\n    Neural Network: CNN_BiGRU\n    Detail: the input crosss cnn model and GRU model independly, then the result of both concat\n""""""\n\n\nclass CNN_BiGRU(nn.Module):\n    \n    def __init__(self, args):\n        super(CNN_BiGRU,self).__init__()\n        self.args = args\n        self.hidden_dim = args.lstm_hidden_dim\n        self.num_layers = args.lstm_num_layers\n        V = args.embed_num\n        D = args.embed_dim\n        C = args.class_num\n        self.C = C\n        Ci = 1\n        Co = args.kernel_num\n        Ks = args.kernel_sizes\n        self.embed = nn.Embedding(V, D, padding_idx=args.paddingId)\n        # pretrained  embedding\n        if args.word_Embedding:\n            self.embed.weight.data.copy_(args.pretrained_weight)\n\n        # CNN\n        self.convs1 = [nn.Conv2d(Ci, Co, (K, D), padding=(K//2, 0), stride=1) for K in Ks]\n        print(self.convs1)\n        # for cnn cuda\n        if self.args.cuda is True:\n            for conv in self.convs1:\n                conv = conv.cuda()\n\n        # BiGRU\n        self.bigru = nn.GRU(D, self.hidden_dim, num_layers=self.num_layers, dropout=args.dropout, bidirectional=True, bias=True)\n\n        # linear\n        L = len(Ks) * Co + self.hidden_dim * 2\n        self.hidden2label1 = nn.Linear(L, L // 2)\n        self.hidden2label2 = nn.Linear(L // 2, C)\n\n        # dropout\n        self.dropout = nn.Dropout(args.dropout)\n\n    def forward(self, x):\n        embed = self.embed(x)\n        embed = self.dropout(embed)\n        # CNN\n        cnn_x = embed\n        cnn_x = torch.transpose(cnn_x, 0, 1)\n        cnn_x = cnn_x.unsqueeze(1)\n        cnn_x = [conv(cnn_x).squeeze(3) for conv in self.convs1]  # [(N,Co,W), ...]*len(Ks)\n        cnn_x = [F.tanh(F.max_pool1d(i, i.size(2)).squeeze(2)) for i in cnn_x]  # [(N,Co), ...]*len(Ks)\n        cnn_x = torch.cat(cnn_x, 1)\n        cnn_x = self.dropout(cnn_x)\n        # BiGRU\n        bigru_x = embed.view(len(x), embed.size(1), -1)\n        bigru_x, _ = self.bigru(bigru_x)\n        bigru_x = torch.transpose(bigru_x, 0, 1)\n        bigru_x = torch.transpose(bigru_x, 1, 2)\n        # bilstm_out = F.tanh(bilstm_out)\n        bigru_x = F.max_pool1d(bigru_x, bigru_x.size(2)).squeeze(2)\n        bigru_x = F.tanh(bigru_x)\n\n        # CNN and BiGRU CAT\n        cnn_x = torch.transpose(cnn_x, 0, 1)\n        bigru_x = torch.transpose(bigru_x, 0, 1)\n        cnn_bigru_out = torch.cat((cnn_x, bigru_x), 0)\n        cnn_bigru_out = torch.transpose(cnn_bigru_out, 0, 1)\n\n        # linear\n        cnn_bigru_out = self.hidden2label1(F.tanh(cnn_bigru_out))\n        logit = self.hidden2label2(F.tanh(cnn_bigru_out))\n\n        return logit'"
models/model_CNN_BiLSTM.py,12,"b'# @Author : bamtercelboo\n# @Datetime : 2018/07/19 22:35\n# @File : model_CNN_BiLSTM.py\n# @Last Modify Time : 2018/07/19 22:35\n# @Contact : bamtercelboo@{gmail.com, 163.com}\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport numpy as np\nimport random\nfrom DataUtils.Common import seed_num\ntorch.manual_seed(seed_num)\nrandom.seed(seed_num)\n\n\n""""""\n    Neural Network: CNN_BiLSTM\n    Detail: the input crosss cnn model and LSTM model independly, then the result of both concat\n""""""\n\n\nclass CNN_BiLSTM(nn.Module):\n\n    def __init__(self, args):\n        super(CNN_BiLSTM, self).__init__()\n        self.args = args\n        self.hidden_dim = args.lstm_hidden_dim\n        self.num_layers = args.lstm_num_layers\n        V = args.embed_num\n        D = args.embed_dim\n        C = args.class_num\n        self.C = C\n        Ci = 1\n        Co = args.kernel_num\n        Ks = args.kernel_sizes\n        self.embed = nn.Embedding(V, D, padding_idx=args.paddingId)\n        # pretrained  embedding\n        if args.word_Embedding:\n            self.embed.weight.data.copy_(args.pretrained_weight)\n\n        # CNN\n        self.convs1 = [nn.Conv2d(Ci, Co, (K, D), padding=(K//2, 0), stride=1) for K in Ks]\n        print(self.convs1)\n        # for cnn cuda\n        if self.args.cuda is True:\n            for conv in self.convs1:\n                conv = conv.cuda()\n\n        # BiLSTM\n        self.bilstm = nn.LSTM(D, self.hidden_dim, num_layers=self.num_layers, dropout=args.dropout, bidirectional=True, bias=True)\n\n        # linear\n        L = len(Ks) * Co + self.hidden_dim * 2\n        self.hidden2label1 = nn.Linear(L, L // 2)\n        self.hidden2label2 = nn.Linear(L // 2, C)\n\n        # dropout\n        self.dropout = nn.Dropout(args.dropout)\n\n    def forward(self, x):\n        embed = self.embed(x)\n\n        # CNN\n        cnn_x = embed\n        cnn_x = torch.transpose(cnn_x, 0, 1)\n        cnn_x = cnn_x.unsqueeze(1)\n        cnn_x = [conv(cnn_x).squeeze(3) for conv in self.convs1]  # [(N,Co,W), ...]*len(Ks)\n        cnn_x = [F.tanh(F.max_pool1d(i, i.size(2)).squeeze(2)) for i in cnn_x]  # [(N,Co), ...]*len(Ks)\n        cnn_x = torch.cat(cnn_x, 1)\n        cnn_x = self.dropout(cnn_x)\n\n        # BiLSTM\n        bilstm_x = embed.view(len(x), embed.size(1), -1)\n        bilstm_out, _ = self.bilstm(bilstm_x)\n        bilstm_out = torch.transpose(bilstm_out, 0, 1)\n        bilstm_out = torch.transpose(bilstm_out, 1, 2)\n        bilstm_out = F.max_pool1d(bilstm_out, bilstm_out.size(2)).squeeze(2)\n        bilstm_out = F.tanh(bilstm_out)\n\n        # CNN and BiLSTM CAT\n        cnn_x = torch.transpose(cnn_x, 0, 1)\n        bilstm_out = torch.transpose(bilstm_out, 0, 1)\n        cnn_bilstm_out = torch.cat((cnn_x, bilstm_out), 0)\n        cnn_bilstm_out = torch.transpose(cnn_bilstm_out, 0, 1)\n\n        # linear\n        cnn_bilstm_out = self.hidden2label1(F.tanh(cnn_bilstm_out))\n        cnn_bilstm_out = self.hidden2label2(F.tanh(cnn_bilstm_out))\n\n        # output\n        logit = cnn_bilstm_out\n        return logit'"
models/model_CNN_LSTM.py,12,"b'# @Author : bamtercelboo\n# @Datetime : 2018/07/19 22:35\n# @File : model_CNN_LSTM.py\n# @Last Modify Time : 2018/07/19 22:35\n# @Contact : bamtercelboo@{gmail.com, 163.com}\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport numpy as np\nimport random\nfrom DataUtils.Common import seed_num\ntorch.manual_seed(seed_num)\nrandom.seed(seed_num)\n\n""""""\n    Neural Network: CNN_LSTM\n    Detail: the input crosss cnn model and LSTM model independly, then the result of both concat\n""""""\n\n\nclass CNN_LSTM(nn.Module):\n    \n    def __init__(self, args):\n        super(CNN_LSTM, self).__init__()\n        self.args = args\n        self.hidden_dim = args.lstm_hidden_dim\n        self.num_layers = args.lstm_num_layers\n        V = args.embed_num\n        D = args.embed_dim\n        C = args.class_num\n        Ci = 1\n        Co = args.kernel_num\n        Ks = args.kernel_sizes\n        self.C = C\n        self.embed = nn.Embedding(V, D, padding_idx=args.paddingId)\n        # pretrained  embedding\n        if args.word_Embedding:\n            self.embed.weight.data.copy_(args.pretrained_weight)\n\n        # CNN\n        self.convs1 = [nn.Conv2d(Ci, Co, (K, D)) for K in Ks]\n        self.dropout = nn.Dropout(args.dropout)\n        # for cnn cuda\n        if self.args.cuda is True:\n            for conv in self.convs1:\n                conv = conv.cuda()\n\n        # LSTM\n        self.lstm = nn.LSTM(D, self.hidden_dim, dropout=args.dropout, num_layers=self.num_layers)\n\n        # linear\n        L = len(Ks) * Co + self.hidden_dim\n        self.hidden2label1 = nn.Linear(L, L // 2)\n        self.hidden2label2 = nn.Linear(L // 2, C)\n\n    def forward(self, x):\n        embed = self.embed(x)\n\n        # CNN\n        cnn_x = embed\n        cnn_x = torch.transpose(cnn_x, 0, 1)\n        cnn_x = cnn_x.unsqueeze(1)\n        cnn_x = [F.relu(conv(cnn_x)).squeeze(3) for conv in self.convs1]  # [(N,Co,W), ...]*len(Ks)\n        cnn_x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in cnn_x]  # [(N,Co), ...]*len(Ks)\n        cnn_x = torch.cat(cnn_x, 1)\n        cnn_x = self.dropout(cnn_x)\n\n        # LSTM\n        lstm_x = embed.view(len(x), embed.size(1), -1)\n        lstm_out, _ = self.lstm(lstm_x)\n        lstm_out = torch.transpose(lstm_out, 0, 1)\n        lstm_out = torch.transpose(lstm_out, 1, 2)\n        lstm_out = F.max_pool1d(lstm_out, lstm_out.size(2)).squeeze(2)\n\n        # CNN and LSTM cat\n        cnn_x = torch.transpose(cnn_x, 0, 1)\n        lstm_out = torch.transpose(lstm_out, 0, 1)\n        cnn_lstm_out = torch.cat((cnn_x, lstm_out), 0)\n        cnn_lstm_out = torch.transpose(cnn_lstm_out, 0, 1)\n\n        # linear\n        cnn_lstm_out = self.hidden2label1(F.tanh(cnn_lstm_out))\n        cnn_lstm_out = self.hidden2label2(F.tanh(cnn_lstm_out))\n\n        # output\n        logit = cnn_lstm_out\n        return logit'"
models/model_CNN_MUI.py,7,"b'# @Author : bamtercelboo\n# @Datetime : 2018/07/19 22:35\n# @File : model_CNN_MUI.py\n# @Last Modify Time : 2018/07/19 22:35\n# @Contact : bamtercelboo@{gmail.com, 163.com}\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport numpy as np\nimport random\nimport torch.nn.init as init\nfrom DataUtils.Common import seed_num\ntorch.manual_seed(seed_num)\nrandom.seed(seed_num)\n\n""""""\nDescription:\n    the model is a mulit-channel CNNS model, \n    the model use two external word embedding, and then,\n    one of word embedding built from train/dev/test dataset,\n    and it be used to no-fine-tune,other one built from only\n    train dataset,and be used to fine-tune.\n    \n    my idea,even if the word embedding built from train/dev/test dataset, \n    whether can use fine-tune, in others words, whether can fine-tune with\n    two external word embedding.\n""""""\n\n\nclass CNN_MUI(nn.Module):\n    \n    def __init__(self, args):\n        super(CNN_MUI, self).__init__()\n        self.args = args\n        \n        V = args.embed_num\n        V_mui = args.embed_num_mui\n        D = args.embed_dim\n        C = args.class_num\n        Ci = 2\n        Co = args.kernel_num\n        Ks = args.kernel_sizes\n\n        if args.max_norm is not None:\n            print(""max_norm = {} "".format(args.max_norm))\n            self.embed_no_static = nn.Embedding(V, D, max_norm=args.max_norm, scale_grad_by_freq=True, padding_idx=args.paddingId)\n            self.embed_static = nn.Embedding(V_mui, D, max_norm=args.max_norm, scale_grad_by_freq=True, padding_idx=args.paddingId_mui)\n        else:\n            print(""max_norm = {} "".format(args.max_norm))\n            self.embed_no_static = nn.Embedding(V, D, scale_grad_by_freq=True, padding_idx=args.paddingId)\n            self.embed_static = nn.Embedding(V_mui, D, scale_grad_by_freq=True, padding_idx=args.paddingId_mui)\n        if args.word_Embedding:\n            self.embed_no_static.weight.data.copy_(args.pretrained_weight)\n            self.embed_static.weight.data.copy_(args.pretrained_weight_static)\n            # whether to fixed the word embedding\n            self.embed_no_static.weight.requires_grad = False\n\n        if args.wide_conv is True:\n            print(""using wide convolution"")\n            self.convs1 = [nn.Conv2d(in_channels=Ci, out_channels=Co, kernel_size=(K, D), stride=(1, 1),\n                                     padding=(K//2, 0), bias=True) for K in Ks]\n        else:\n            print(""using narrow convolution"")\n            self.convs1 = [nn.Conv2d(in_channels=Ci, out_channels=Co, kernel_size=(K, D), bias=True) for K in Ks]\n        print(self.convs1)\n\n        if args.init_weight:\n            print(""Initing W ......."")\n            for conv in self.convs1:\n                init.xavier_normal(conv.weight.data, gain=np.sqrt(args.init_weight_value))\n                init.uniform(conv.bias, 0, 0)\n        \'\'\'\n        self.conv13 = nn.Conv2d(Ci, Co, (3, D))\n        self.conv14 = nn.Conv2d(Ci, Co, (4, D))\n        self.conv15 = nn.Conv2d(Ci, Co, (5, D))\n        \'\'\'\n        self.dropout = nn.Dropout(args.dropout)\n\n        # for cnn cuda\n        if self.args.cuda is True:\n            for conv in self.convs1:\n                conv = conv.cuda()\n\n        in_fea = len(Ks) * Co\n        self.fc1 = nn.Linear(in_features=in_fea, out_features=in_fea // 2, bias=True)\n        self.fc2 = nn.Linear(in_features=in_fea // 2, out_features=C, bias=True)\n\n        if args.batch_normalizations is True:\n            print(""using batch_normalizations in the model......"")\n            self.convs1_bn = nn.BatchNorm2d(num_features=Co, momentum=args.bath_norm_momentum,\n                                            affine=args.batch_norm_affine)\n            self.fc1_bn = nn.BatchNorm1d(num_features=in_fea//2, momentum=args.bath_norm_momentum,\n                                         affine=args.batch_norm_affine)\n            self.fc2_bn = nn.BatchNorm1d(num_features=C, momentum=args.bath_norm_momentum,\n                                         affine=args.batch_norm_affine)\n\n    def conv_and_pool(self, x, conv):\n        x = F.relu(conv(x)).squeeze(3) #(N,Co,W)\n        x = F.max_pool1d(x, x.size(2)).squeeze(2)\n        return x\n\n    def forward(self, x):\n        x_no_static = self.embed_no_static(x)\n        x_static = self.embed_static(x)\n        x = torch.stack([x_static, x_no_static], 1)\n        x = self.dropout(x)\n        if self.args.batch_normalizations is True:\n            x = [F.relu(self.convs1_bn(conv(x))).squeeze(3) for conv in self.convs1] #[(N,Co,W), ...]*len(Ks)\n            x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x] #[(N,Co), ...]*len(Ks)\n        else:\n            x = [F.relu(conv(x)).squeeze(3) for conv in self.convs1] #[(N,Co,W), ...]*len(Ks)\n            x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]  # [(N,Co), ...]*len(Ks)\n        x = torch.cat(x, 1)\n        x = self.dropout(x)  # (N,len(Ks)*Co)\n        if self.args.batch_normalizations is True:\n            x = self.fc1(x)\n            logit = self.fc2(F.relu(x))\n        else:\n            x = self.fc1(x)\n            logit = self.fc2(F.relu(x))\n        return logit'"
models/model_DeepCNN.py,10,"b'# @Author : bamtercelboo\n# @Datetime : 2018/07/19 22:35\n# @File : model_DeepCNN.py\n# @Last Modify Time : 2018/07/19 22:35\n# @Contact : bamtercelboo@{gmail.com, 163.com}\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport numpy as np\nimport random\nimport torch.nn.init as init\nfrom DataUtils.Common import seed_num\ntorch.manual_seed(seed_num)\nrandom.seed(seed_num)\n\n""""""\n    Neural Network: DEEP_CNN\n    Detail: two layer cnn\n""""""\n\n\nclass DEEP_CNN(nn.Module):\n    \n    def __init__(self, args):\n        super(DEEP_CNN, self).__init__()\n        self.args = args\n        \n        V = args.embed_num\n        D = args.embed_dim\n        C = args.class_num\n        Ci = 1\n        Co = args.kernel_num\n        Ks = args.kernel_sizes\n        if args.max_norm is not None:\n            print(""max_norm = {} "".format(args.max_norm))\n            self.embed = nn.Embedding(V, D, max_norm=5, scale_grad_by_freq=True, padding_idx=args.paddingId)\n        else:\n            print(""max_norm = {} "".format(args.max_norm))\n            self.embed = nn.Embedding(V, D, scale_grad_by_freq=True, padding_idx=args.paddingId)\n        if args.word_Embedding:\n            self.embed.weight.data.copy_(args.pretrained_weight)\n            # fixed the word embedding\n            self.embed.weight.requires_grad = True\n\n        # cons layer\n        self.convs1 = [nn.Conv2d(Ci, D, (K, D), stride=1, padding=(K//2, 0), bias=True) for K in Ks]\n        self.convs2 = [nn.Conv2d(Ci, Co, (K, D), stride=1, padding=(K//2, 0), bias=True) for K in Ks]\n        print(self.convs1)\n        print(self.convs2)\n\n        if args.init_weight:\n            print(""Initing W ......."")\n            for (conv1, conv2) in zip(self.convs1, self.convs2):\n                init.xavier_normal(conv1.weight.data, gain=np.sqrt(args.init_weight_value))\n                init.uniform(conv1.bias, 0, 0)\n                init.xavier_normal(conv2.weight.data, gain=np.sqrt(args.init_weight_value))\n                init.uniform(conv2.bias, 0, 0)\n\n        # for cnn cuda\n        if self.args.cuda is True:\n            for conv in self.convs1:\n                conv = conv.cuda()\n\n        # for cnn cuda\n        if self.args.cuda is True:\n            for conv in self.convs2:\n                conv = conv.cuda()\n\n        # dropout\n        self.dropout = nn.Dropout(args.dropout)\n        # linear\n        in_fea = len(Ks) * Co\n        self.fc1 = nn.Linear(in_features=in_fea, out_features=in_fea // 2, bias=True)\n        self.fc2 = nn.Linear(in_features=in_fea // 2, out_features=C, bias=True)\n\n\n    def forward(self, x):\n        one_layer = self.embed(x)  # (N,W,D) #  torch.Size([64, 43, 300])\n        one_layer = one_layer.unsqueeze(1)  # (N,Ci,W,D)  #  torch.Size([64, 1, 43, 300])\n        # one layer\n        one_layer = [torch.transpose(F.relu(conv(one_layer)).squeeze(3), 1, 2) for conv in self.convs1] # torch.Size([64, 100, 36])\n        # two layer\n        two_layer = [F.relu(conv(one_layer.unsqueeze(1))).squeeze(3) for (conv, one_layer) in zip(self.convs2, one_layer)]\n        # print(""two_layer {}"".format(two_layer[0].size()))\n        # pooling\n        output = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in two_layer]   #  torch.Size([64, 100]) torch.Size([64, 100])\n        output = torch.cat(output, 1)  # torch.Size([64, 300])\n        # dropout\n        output = self.dropout(output)\n        # linear\n        output = self.fc1(F.relu(output))\n        logit = self.fc2(F.relu(output))\n        return logit'"
models/model_DeepCNN_MUI.py,12,"b'# @Author : bamtercelboo\n# @Datetime : 2018/07/19 22:35\n# @File : model_DeepCNN_MUI.py\n# @Last Modify Time : 2018/07/19 22:35\n# @Contact : bamtercelboo@{gmail.com, 163.com}\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport numpy as np\nimport random\nimport torch.nn.init as init\nfrom DataUtils.Common import seed_num\ntorch.manual_seed(seed_num)\nrandom.seed(seed_num)\n\n""""""\nDescription:\n    the model is a mulit-channel DeepCNNS model, \n    the model use two external word embedding, and then,\n    one of word embedding built from train/dev/test dataset,\n    and it be used to no-fine-tune,other one built from only\n    train dataset,and be used to fine-tune.\n\n    my idea,even if the word embedding built from train/dev/test dataset, \n    whether can use fine-tune, in others words, whether can fine-tune with\n    two external word embedding.\n""""""\n\n\nclass DEEP_CNN_MUI(nn.Module):\n    \n    def __init__(self, args):\n        super(DEEP_CNN_MUI, self).__init__()\n        self.args = args\n        \n        V = args.embed_num\n        V_mui = args.embed_num_mui\n        D = args.embed_dim\n        C = args.class_num\n        Ci = 2\n        Co = args.kernel_num\n        Ks = args.kernel_sizes\n        if args.max_norm is not None:\n            print(""max_norm = {} "".format(args.max_norm))\n            self.embed_no_static = nn.Embedding(V, D, max_norm=args.max_norm, scale_grad_by_freq=True, padding_idx=args.paddingId)\n            self.embed_static = nn.Embedding(V_mui, D, max_norm=args.max_norm, scale_grad_by_freq=True, padding_idx=args.paddingId_mui)\n        else:\n            print(""max_norm = {} "".format(args.max_norm))\n            self.embed_no_static = nn.Embedding(V, D, scale_grad_by_freq=True, padding_idx=args.paddingId)\n            self.embed_static = nn.Embedding(V_mui, D, scale_grad_by_freq=True, padding_idx=args.paddingId_mui)\n        if args.word_Embedding:\n            self.embed_no_static.weight.data.copy_(args.pretrained_weight)\n            self.embed_static.weight.data.copy_(args.pretrained_weight_static)\n            # whether to fixed the word embedding\n            self.embed_no_static.weight.requires_grad = False\n\n        # cons layer\n        self.convs1 = [nn.Conv2d(Ci, D, (K, D), stride=1, padding=(K//2, 0), bias=True) for K in Ks]\n        self.convs2 = [nn.Conv2d(1, Co, (K, D), stride=1, padding=(K//2, 0), bias=True) for K in Ks]\n        print(self.convs1)\n        print(self.convs2)\n\n        if args.init_weight:\n            print(""Initing W ......."")\n            for (conv1, conv2) in zip(self.convs1, self.convs2):\n                init.xavier_normal(conv1.weight.data, gain=np.sqrt(args.init_weight_value))\n                init.uniform(conv1.bias, 0, 0)\n                init.xavier_normal(conv2.weight.data, gain=np.sqrt(args.init_weight_value))\n                init.uniform(conv2.bias, 0, 0)\n\n        # for cnn cuda\n        if self.args.cuda is True:\n            for conv in self.convs1:\n                conv = conv.cuda()\n\n        # for cnn cuda\n        if self.args.cuda is True:\n            for conv in self.convs2:\n                conv = conv.cuda()\n\n        # dropout\n        self.dropout = nn.Dropout(args.dropout)\n        # linear\n        in_fea = len(Ks) * Co\n        self.fc1 = nn.Linear(in_features=in_fea, out_features=in_fea // 2, bias=True)\n        self.fc2 = nn.Linear(in_features=in_fea // 2, out_features=C, bias=True)\n\n    def forward(self, x):\n        x_no_static = self.embed_no_static(x)\n        # x_no_static = self.dropout(x_no_static)\n        x_static = self.embed_static(x)\n        # fix the embedding\n        x_static = Variable(x_static.data)\n        # x_static = self.dropout(x_static)\n        x = torch.stack([x_static, x_no_static], 1)\n        one_layer = x  # (N,W,D) #  torch.Size([64, 43, 300])\n        # print(""one_layer {}"".format(one_layer.size()))\n        # one_layer = self.dropout(one_layer)\n        # one_layer = one_layer.unsqueeze(1)  # (N,Ci,W,D)  #  torch.Size([64, 1, 43, 300])\n        # one layer\n        one_layer = [torch.transpose(F.relu(conv(one_layer)).squeeze(3), 1, 2).unsqueeze(1) for conv in self.convs1] # torch.Size([64, 100, 36])\n        # one_layer = [F.relu(conv(one_layer)).squeeze(3).unsqueeze(1) for conv in self.convs1] # torch.Size([64, 100, 36])\n        # print(one_layer[0].size())\n        # print(one_layer[1].size())\n        # two layer\n        two_layer = [F.relu(conv(one_layer)).squeeze(3) for (conv, one_layer) in zip(self.convs2, one_layer)]\n        # print(""two_layer {}"".format(two_layer[0].size()))\n        # print(""two_layer {}"".format(two_layer[1].size()))\n        # pooling\n        output = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in two_layer]   #  torch.Size([64, 100]) torch.Size([64, 100])\n        output = torch.cat(output, 1)  # torch.Size([64, 300])\n        # dropout\n        output = self.dropout(output)\n        # linear\n        output = self.fc1(output)\n        logit = self.fc2(F.relu(output))\n        return logit'"
models/model_GRU.py,6,"b'# @Author : bamtercelboo\n# @Datetime : 2018/07/19 22:35\n# @File : model_GRU.py\n# @Last Modify Time : 2018/07/19 22:35\n# @Contact : bamtercelboo@{gmail.com, 163.com}\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport numpy as np\nimport random\nfrom DataUtils.Common import seed_num\ntorch.manual_seed(seed_num)\nrandom.seed(seed_num)\n\n""""""\nNeural Networks model : GRU\n""""""\n\n\nclass GRU(nn.Module):\n    \n    def __init__(self, args):\n        super(GRU, self).__init__()\n        self.args = args\n        self.hidden_dim = args.lstm_hidden_dim\n        self.num_layers = args.lstm_num_layers\n        V = args.embed_num\n        D = args.embed_dim\n        C = args.class_num\n        self.embed = nn.Embedding(V, D, padding_idx=args.paddingId)\n        # pretrained  embedding\n        if args.word_Embedding:\n            self.embed.weight.data.copy_(args.pretrained_weight)\n\n        # gru\n        self.gru = nn.GRU(D, self.hidden_dim, dropout=args.dropout, num_layers=self.num_layers)\n        # linear\n        self.hidden2label = nn.Linear(self.hidden_dim, C)\n        #  dropout\n        self.dropout = nn.Dropout(args.dropout)\n\n    def forward(self, input):\n        embed = self.embed(input)\n        input = embed.view(len(input), embed.size(1), -1)\n        lstm_out, _ = self.gru(input)\n        lstm_out = torch.transpose(lstm_out, 0, 1)\n        lstm_out = torch.transpose(lstm_out, 1, 2)\n        # pooling\n        lstm_out = F.max_pool1d(lstm_out, lstm_out.size(2)).squeeze(2)\n        lstm_out = F.tanh(lstm_out)\n        # linear\n        y = self.hidden2label(lstm_out)\n        logit = y\n        return logit'"
models/model_HighWay_BiLSTM_1.py,11,"b'# @Author : bamtercelboo\n# @Datetime : 2018/07/19 22:35\n# @File : model_HighWay_BiLSTM_1.py\n# @Last Modify Time : 2018/07/19 22:35\n# @Contact : bamtercelboo@{gmail.com, 163.com}\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport numpy as np\nimport random\nimport torch.nn.init as init\nfrom DataUtils.Common import seed_num\ntorch.manual_seed(seed_num)\nrandom.seed(seed_num)\n\n""""""\n    Neural Networks model : Highway Networks and BiLSTM\n    Highway Networks : git@github.com:bamtercelboo/pytorch_Highway_Networks.git\n""""""\n\n\nclass HighWay_BiLSTM_1(nn.Module):\n    def __init__(self, args):\n        super(HighWay_BiLSTM_1, self).__init__()\n        self.args = args\n        self.hidden_dim = args.lstm_hidden_dim\n        self.num_layers = args.lstm_num_layers\n        V = args.embed_num\n        D = args.embed_dim\n        C = args.class_num\n        self.dropout = nn.Dropout(args.dropout)\n        self.embed = nn.Embedding(V, D, padding_idx=args.paddingId)\n        # pretrained  embedding\n        if args.word_Embedding:\n            self.embed.weight.data.copy_(args.pretrained_weight)\n\n        self.bilstm = nn.LSTM(D, self.hidden_dim, num_layers=self.num_layers, bias=True, bidirectional=True,\n                              dropout=self.args.dropout)\n        print(self.bilstm)\n        if args.init_weight:\n            print(""Initing W ......."")\n            init.xavier_normal(self.bilstm.all_weights[0][0], gain=np.sqrt(args.init_weight_value))\n            init.xavier_normal(self.bilstm.all_weights[0][1], gain=np.sqrt(args.init_weight_value))\n            init.xavier_normal(self.bilstm.all_weights[1][0], gain=np.sqrt(args.init_weight_value))\n            init.xavier_normal(self.bilstm.all_weights[1][1], gain=np.sqrt(args.init_weight_value))\n\n            # init weight of lstm gate\n            self.bilstm.all_weights[0][3].data[20:40].fill_(1)\n            self.bilstm.all_weights[0][3].data[0:20].fill_(0)\n            self.bilstm.all_weights[0][3].data[40:80].fill_(0)\n            # self.bilstm.all_weights[0][3].data[40:].fill_(0)\n            self.bilstm.all_weights[0][2].data[20:40].fill_(1)\n            self.bilstm.all_weights[0][2].data[0:20].fill_(0)\n            self.bilstm.all_weights[0][2].data[40:80].fill_(0)\n            # self.bilstm.all_weights[0][2].data[40:].fill_(0)\n            self.bilstm.all_weights[1][3].data[20:40].fill_(1)\n            self.bilstm.all_weights[1][3].data[0:20].fill_(0)\n            self.bilstm.all_weights[1][3].data[40:80].fill_(0)\n            # self.bilstm.all_weights[1][3].data[40:].fill_(0)\n            self.bilstm.all_weights[1][2].data[20:40].fill_(1)\n            self.bilstm.all_weights[1][2].data[0:20].fill_(0)\n            self.bilstm.all_weights[1][2].data[40:80].fill_(0)\n            # self.bilstm.all_weights[1][2].data[40:].fill_(0)\n\n        self.hidden2label1 = nn.Linear(in_features=self.hidden_dim * 2, out_features=self.hidden_dim * 2, bias=True)\n\n        # highway gate layer\n        self.gate_layer = nn.Linear(in_features=self.hidden_dim * 2, out_features=self.hidden_dim * 2, bias=True)\n\n        # last liner\n        self.logit_layer = nn.Linear(in_features=self.hidden_dim * 2, out_features=C, bias=True)\n\n    def forward(self, x):\n        x = self.embed(x)\n        x = self.dropout(x)\n        bilstm_out, _ = self.bilstm(x)\n\n        bilstm_out = torch.transpose(bilstm_out, 0, 1)\n        bilstm_out = torch.transpose(bilstm_out, 1, 2)\n        bilstm_out = F.max_pool1d(bilstm_out, bilstm_out.size(2))\n        bilstm_out = bilstm_out.squeeze(2)\n\n        hidden2lable = self.hidden2label1(F.tanh(bilstm_out))\n\n        gate_layer = F.sigmoid(self.gate_layer(bilstm_out))\n        # calculate highway layer values\n        gate_hidden_layer = torch.mul(hidden2lable, gate_layer)\n        # if write like follow ,can run,but not equal the HighWay NetWorks formula\n        # gate_input = torch.mul((1 - gate_layer), hidden2lable)\n        gate_input = torch.mul((1 - gate_layer), bilstm_out)\n        highway_output = torch.add(gate_hidden_layer, gate_input)\n\n        logit = self.logit_layer(highway_output)\n\n        return logit'"
models/model_HighWay_CNN.py,10,"b'# @Author : bamtercelboo\n# @Datetime : 2018/07/19 22:35\n# @File : model_HighWay_CNN.py\n# @Last Modify Time : 2018/07/19 22:35\n# @Contact : bamtercelboo@{gmail.com, 163.com}\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport numpy as np\nimport random\nimport torch.nn.init as init\nfrom DataUtils.Common import seed_num\ntorch.manual_seed(seed_num)\nrandom.seed(seed_num)\n\n""""""\n    Neural Networks model : Highway Networks and CNN\n    Highway Networks : git@github.com:bamtercelboo/pytorch_Highway_Networks.git\n""""""\n\n\nclass HighWay_CNN(nn.Module):\n    \n    def __init__(self, args):\n        super(HighWay_CNN, self).__init__()\n        self.args = args\n        \n        V = args.embed_num\n        D = args.embed_dim\n        C = args.class_num\n        Ci = 1\n        Co = args.kernel_num\n        Ks = args.kernel_sizes\n\n        self.embed = nn.Embedding(V, D, padding_idx=args.paddingId)\n        # pretrained  embedding\n        if args.word_Embedding:\n            self.embed.weight.data.copy_(args.pretrained_weight)\n\n        print(""dddd {} "".format(self.embed.weight.data.size()))\n\n        if args.wide_conv is True:\n            print(""using wide convolution"")\n            self.convs1 = [nn.Conv2d(in_channels=Ci, out_channels=Co, kernel_size=(K, D), stride=(1, 1),\n                                     padding=(K//2, 0), dilation=1, bias=True) for K in Ks]\n        else:\n            print(""using narrow convolution"")\n            self.convs1 = [nn.Conv2d(in_channels=Ci, out_channels=Co, kernel_size=(K, D), bias=True) for K in Ks]\n        print(self.convs1)\n\n        if args.init_weight:\n            print(""Initing W ......."")\n            for conv in self.convs1:\n                init.xavier_normal(conv.weight.data, gain=np.sqrt(args.init_weight_value))\n                fan_in, fan_out = HighWay_CNN.calculate_fan_in_and_fan_out(conv.weight.data)\n                print("" in {} out {} "".format(fan_in, fan_out))\n                std = np.sqrt(args.init_weight_value) * np.sqrt(2.0 / (fan_in + fan_out))\n                init.uniform(conv.bias, 0, 0)\n\n        # for cnn cuda\n        if self.args.cuda is True:\n            for conv in self.convs1:\n                conv = conv.cuda()\n\n        self.dropout = nn.Dropout(args.dropout)\n\n        in_fea = len(Ks) * Co\n        self.fc1 = nn.Linear(in_features=in_fea, out_features=in_fea, bias=True)\n\n        # highway gate layer\n        self.gate_layer = nn.Linear(in_features=in_fea, out_features=in_fea, bias=True)\n\n        # last liner\n        self.logit_layer = nn.Linear(in_features=in_fea, out_features=C, bias=True)\n\n        # whether to use batch normalizations\n        if args.batch_normalizations is True:\n            print(""using batch_normalizations in the model......"")\n            self.convs1_bn = nn.BatchNorm2d(num_features=Co, momentum=args.bath_norm_momentum,\n                                            affine=args.batch_norm_affine)\n            self.fc1_bn = nn.BatchNorm1d(num_features=in_fea//2, momentum=args.bath_norm_momentum,\n                                         affine=args.batch_norm_affine)\n            self.fc2_bn = nn.BatchNorm1d(num_features=C, momentum=args.bath_norm_momentum,\n                                         affine=args.batch_norm_affine)\n\n    def calculate_fan_in_and_fan_out(tensor):\n        dimensions = tensor.ndimension()\n        if dimensions < 2:\n            raise ValueError(""Fan in and fan out can not be computed for tensor with less than 2 dimensions"")\n\n        if dimensions == 2:  # Linear\n            fan_in = tensor.size(1)\n            fan_out = tensor.size(0)\n        else:\n            num_input_fmaps = tensor.size(1)\n            num_output_fmaps = tensor.size(0)\n            receptive_field_size = 1\n            if tensor.dim() > 2:\n                receptive_field_size = tensor[0][0].numel()\n            fan_in = num_input_fmaps * receptive_field_size\n            fan_out = num_output_fmaps * receptive_field_size\n\n        return fan_in, fan_out\n\n    def forward(self, x):\n        x = self.embed(x)  # (N,W,D)\n        x = self.dropout(x)\n        x = x.unsqueeze(1)  # (N,Ci,W,D)\n        if self.args.batch_normalizations is True:\n            x = [self.convs1_bn(F.tanh(conv(x))).squeeze(3) for conv in self.convs1] #[(N,Co,W), ...]*len(Ks)\n            x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x] #[(N,Co), ...]*len(Ks)\n        else:\n            x = [F.relu(conv(x)).squeeze(3) for conv in self.convs1] #[(N,Co,W), ...]*len(Ks)\n            x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x] #[(N,Co), ...]*len(Ks)\n        x = torch.cat(x, 1)\n        if self.args.batch_normalizations is True:\n            x = self.fc1_bn(self.fc1(x))\n            fc = self.fc2_bn(self.fc2(F.tanh(x)))\n        else:\n            fc = self.fc1(x)\n\n        gate_layer = F.sigmoid(self.gate_layer(x))\n\n        # calculate highway layer values\n        gate_fc_layer = torch.mul(fc, gate_layer)\n        # if write like follow ,can run,but not equal the HighWay NetWorks formula\n        # gate_input = torch.mul((1 - gate_layer), fc)\n        gate_input = torch.mul((1 - gate_layer), x)\n        highway_output = torch.add(gate_fc_layer, gate_input)\n\n        logit = self.logit_layer(highway_output)\n\n        return logit'"
models/model_LSTM.py,7,"b'# @Author : bamtercelboo\n# @Datetime : 2018/07/19 22:35\n# @File : model_LSTM.py\n# @Last Modify Time : 2018/07/19 22:35\n# @Contact : bamtercelboo@{gmail.com, 163.com}\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport numpy as np\nimport random\nimport torch.nn.init as init\nfrom DataUtils.Common import seed_num\ntorch.manual_seed(seed_num)\nrandom.seed(seed_num)\n""""""\nNeural Networks model : LSTM\n""""""\n\n\nclass LSTM(nn.Module):\n    \n    def __init__(self, args):\n        super(LSTM, self).__init__()\n        self.args = args\n\n        self.hidden_dim = args.lstm_hidden_dim\n        self.num_layers = args.lstm_num_layers\n        V = args.embed_num\n        D = args.embed_dim\n        C = args.class_num\n        self.embed = nn.Embedding(V, D, padding_idx=args.paddingId)\n        # pretrained  embedding\n        if args.word_Embedding:\n            self.embed.weight.data.copy_(args.pretrained_weight)\n\n        # lstm\n        self.lstm = nn.LSTM(D, self.hidden_dim, dropout=args.dropout, num_layers=self.num_layers)\n\n        if args.init_weight:\n            print(""Initing W ......."")\n            # n = self.lstm.input_size * self.lstm\n            init.xavier_normal(self.lstm.all_weights[0][0], gain=np.sqrt(args.init_weight_value))\n            init.xavier_normal(self.lstm.all_weights[0][1], gain=np.sqrt(args.init_weight_value))\n\n        # linear\n        self.hidden2label = nn.Linear(self.hidden_dim, C)\n        # dropout\n        self.dropout = nn.Dropout(args.dropout)\n        self.dropout_embed = nn.Dropout(args.dropout_embed)\n\n    def forward(self, x):\n        embed = self.embed(x)\n        embed = self.dropout_embed(embed)\n        x = embed.view(len(x), embed.size(1), -1)\n        # lstm\n        lstm_out, _ = self.lstm(x)\n        # lstm_out, self.hidden = self.lstm(x, self.hidden)\n        lstm_out = torch.transpose(lstm_out, 0, 1)\n        lstm_out = torch.transpose(lstm_out, 1, 2)\n        # pooling\n        lstm_out = F.tanh(lstm_out)\n        lstm_out = F.max_pool1d(lstm_out, lstm_out.size(2)).squeeze(2)\n        lstm_out = F.tanh(lstm_out)\n        # linear\n        logit = self.hidden2label(lstm_out)\n        return logit'"
