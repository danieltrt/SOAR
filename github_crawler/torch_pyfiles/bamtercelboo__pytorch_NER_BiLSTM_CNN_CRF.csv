file_path,api_count,code
__init__.py,0,"b'# @Author : bamtercelboo\n# @Datetime : 2018/8/24 15:30\n# @File : __init__.py\n# @Last Modify Time : 2018/8/24 15:30\n# @Contact : bamtercelboo@{gmail.com, 163.com}\n\n""""""\n    FILE :  __init__.py\n    FUNCTION : None\n""""""'"
main.py,8,"b'# @Author : bamtercelboo\n# @Datetime : 2018/1/30 19:50\n# @File : main_hyperparams.py.py\n# @Last Modify Time : 2018/1/30 19:50\n# @Contact : bamtercelboo@{gmail.com, 163.com}\n\n""""""\n    FILE :  main_hyperparams.py.py\n    FUNCTION : main\n""""""\n\nimport argparse\nimport datetime\nimport Config.config as configurable\nfrom DataUtils.mainHelp import *\nfrom DataUtils.Alphabet import *\nfrom test import load_test_data\nfrom test import T_Inference\nfrom trainer import Train\nimport random\n\n# solve default encoding problem\nfrom imp import reload\ndefaultencoding = \'utf-8\'\nif sys.getdefaultencoding() != defaultencoding:\n    reload(sys)\n    sys.setdefaultencoding(defaultencoding)\n\n# random seed\ntorch.manual_seed(seed_num)\nrandom.seed(seed_num)\n\n\ndef start_train(train_iter, dev_iter, test_iter, model, config):\n    """"""\n    :param train_iter:  train batch data iterator\n    :param dev_iter:  dev batch data iterator\n    :param test_iter:  test batch data iterator\n    :param model:  nn model\n    :param config:  config\n    :return:  None\n    """"""\n    t = Train(train_iter=train_iter, dev_iter=dev_iter, test_iter=test_iter, model=model, config=config)\n    t.train()\n    print(""Finish Train."")\n\n\ndef start_test(train_iter, dev_iter, test_iter, model, alphabet, config):\n    """"""\n    :param train_iter:  train batch data iterator\n    :param dev_iter:  dev batch data iterator\n    :param test_iter:  test batch data iterator\n    :param model:  nn model\n    :param alphabet:  alphabet dict\n    :param config:  config\n    :return:  None\n    """"""\n    print(""\\nTesting Start......"")\n    data, path_source, path_result = load_test_data(train_iter, dev_iter, test_iter, config)\n    infer = T_Inference(model=model, data=data, path_source=path_source, path_result=path_result, alphabet=alphabet,\n                        use_crf=config.use_crf, config=config)\n    infer.infer2file()\n    print(""Finished Test."")\n\n\ndef main():\n    """"""\n    main()\n    :return:\n    """"""\n    # save file\n    config.mulu = datetime.datetime.now().strftime(\'%Y-%m-%d_%H-%M-%S\')\n    # config.add_args(key=""mulu"", value=datetime.datetime.now().strftime(\'%Y-%m-%d_%H-%M-%S\'))\n    config.save_dir = os.path.join(config.save_direction, config.mulu)\n    if not os.path.isdir(config.save_dir): os.makedirs(config.save_dir)\n\n    # get data, iter, alphabet\n    train_iter, dev_iter, test_iter, alphabet = load_data(config=config)\n\n    # get params\n    get_params(config=config, alphabet=alphabet)\n\n    # save dictionary\n    save_dictionary(config=config)\n\n    model = load_model(config)\n\n    # print(""Training Start......"")\n    if config.train is True:\n        start_train(train_iter, dev_iter, test_iter, model, config)\n        exit()\n    elif config.test is True:\n        start_test(train_iter, dev_iter, test_iter, model, alphabet, config)\n        exit()\n\n\ndef parse_argument():\n    """"""\n    :argument\n    :return:\n    """"""\n    parser = argparse.ArgumentParser(description=""NER & POS"")\n    parser.add_argument(""-c"", ""--config"", dest=""config_file"", type=str, default=""./Config/config.cfg"",help=""config path"")\n    parser.add_argument(""-device"", ""--device"", dest=""device"", type=str, default=""cuda:0"", help=""device[\xe2\x80\x98cpu\xe2\x80\x99,\xe2\x80\x98cuda:0\xe2\x80\x99,\xe2\x80\x98cuda:1\xe2\x80\x99,......]"")\n    parser.add_argument(""--train"", dest=""train"", action=""store_true"", default=True, help=""train model"")\n    parser.add_argument(""-p"", ""--process"", dest=""process"", action=""store_true"", default=True, help=""data process"")\n    parser.add_argument(""-t"", ""--test"", dest=""test"", action=""store_true"", default=False, help=""test model"")\n    parser.add_argument(""--t_model"", dest=""t_model"", type=str, default=None, help=""model for test"")\n    parser.add_argument(""--t_data"", dest=""t_data"", type=str, default=None, help=""data[train, dev, test, None] for test model"")\n    parser.add_argument(""--predict"", dest=""predict"", action=""store_true"", default=False, help=""predict model"")\n    args = parser.parse_args()\n    # print(vars(args))\n    config = configurable.Configurable(config_file=args.config_file)\n    config.device = args.device\n    config.train = args.train\n    config.process = args.process\n    config.test = args.test\n    config.t_model = args.t_model\n    config.t_data = args.t_data\n    config.predict = args.predict\n    # config\n    if config.test is True:\n        config.train = False\n    if config.t_data not in [None, ""train"", ""dev"", ""test""]:\n        print(""\\nUsage"")\n        parser.print_help()\n        print(""t_data : {}, not in [None, \'train\', \'dev\', \'test\']"".format(config.t_data))\n        exit()\n    print(""***************************************"")\n    print(""Device : {}"".format(config.device))\n    print(""Data Process : {}"".format(config.process))\n    print(""Train model : {}"".format(config.train))\n    print(""Test model : {}"".format(config.test))\n    print(""t_model : {}"".format(config.t_model))\n    print(""t_data : {}"".format(config.t_data))\n    print(""predict : {}"".format(config.predict))\n    print(""***************************************"")\n\n    return config\n\n\nif __name__ == ""__main__"":\n\n    print(""Process ID {}, Process Parent ID {}"".format(os.getpid(), os.getppid()))\n    config = parse_argument()\n    if config.device != cpu_device:\n        print(""Using GPU To Train......"")\n        device_number = config.device[-1]\n        torch.cuda.set_device(int(device_number))\n        print(""Current Cuda Device {}"".format(torch.cuda.current_device()))\n        # torch.backends.cudnn.enabled = True\n        # torch.backends.cudnn.deterministic = True\n        torch.cuda.manual_seed(seed_num)\n        torch.cuda.manual_seed_all(seed_num)\n        print(""torch.cuda.initial_seed"", torch.cuda.initial_seed())\n\n    main()\n\n'"
test.py,1,"b'# @Author : bamtercelboo\n# @Datetime : 2018/8/24 15:27\n# @File : test.py\n# @Last Modify Time : 2018/8/24 15:27\n# @Contact : bamtercelboo@{gmail.com, 163.com}\n\n""""""\n    FILE :  test.py\n    FUNCTION : None\n""""""\nimport os\nimport sys\nimport torch\nfrom DataUtils.utils import *\nfrom DataUtils.Common import *\n\n\ndef load_test_model(model, config):\n    """"""\n    :param model:  initial model\n    :param config:  config\n    :return:  loaded model\n    """"""\n    if config.t_model is None:\n        test_model_dir = config.save_best_model_dir\n        test_model_name = ""{}.pt"".format(config.model_name)\n        test_model_path = os.path.join(test_model_dir, test_model_name)\n        print(""load default model from {}"".format(test_model_path))\n    else:\n        test_model_path = config.t_model\n        print(""load user model from {}"".format(test_model_path))\n    model.load_state_dict(torch.load(test_model_path))\n    return model\n\n\ndef load_test_data(train_iter=None, dev_iter=None, test_iter=None, config=None):\n    """"""\n    :param train_iter:  train data\n    :param dev_iter:  dev data\n    :param test_iter:  test data\n    :param config:  config\n    :return:  data for test\n    """"""\n    data, path_source, path_result = None, None, None\n    if config.t_data is None:\n        print(""default[test] for model test."")\n        data = test_iter\n        path_source = ""."".join([config.test_file, shuffle])\n        path_result = ""{}.out"".format(path_source)\n    elif config.t_data == ""train"":\n        print(""train data for model test."")\n        data = train_iter\n        path_source = ""."".join([config.train_file, shuffle])\n        path_result = ""{}.out"".format(path_source)\n    elif config.t_data == ""dev"":\n        print(""dev data for model test."")\n        data = dev_iter\n        path_source = ""."".join([config.dev_file, shuffle])\n        path_result = ""{}.out"".format(path_source)\n    elif config.t_data == ""test"":\n        print(""test data for model test."")\n        data = test_iter\n        path_source = ""."".join([config.test_file, shuffle])\n        path_result = ""{}.out"".format(path_source)\n    else:\n        print(""Error value --- t_data = {}, must in [None, \'train\', \'dev\', \'test\']."".format(config.t_data))\n        exit()\n    return data, path_source, path_result\n\n\nclass T_Inference(object):\n    """"""\n        Test Inference\n    """"""\n    def __init__(self, model, data, path_source, path_result, alphabet, use_crf, config):\n        """"""\n        :param model:  nn model\n        :param data:  infer data\n        :param path_source:  source data path\n        :param path_result:  result data path\n        :param alphabet:  alphabet\n        :param config:  config\n        """"""\n        print(""Initialize T_Inference"")\n        self.model = model\n        self.data = data\n        self.path_source = path_source\n        self.path_result = path_result\n        self.alphabet = alphabet\n        self.config = config\n        self.use_crf = use_crf\n\n    def infer2file(self):\n        """"""\n        :return: None\n        """"""\n        print(""infer....."")\n        self.model.eval()\n        predict_labels = []\n        predict_label = []\n        all_count = len(self.data)\n        now_count = 0\n        for data in self.data:\n            now_count += 1\n            sys.stdout.write(""\\rinfer with batch number {}/{} ."".format(now_count, all_count))\n            word, char, mask, sentence_length, tags = self._get_model_args(data)\n            logit = self.model(word, char, sentence_length, train=False)\n            if self.use_crf is False:\n                predict_ids = torch_max(logit)\n                for id_batch in range(data.batch_length):\n                    inst = data.inst[id_batch]\n                    label_ids = predict_ids[id_batch]\n                    # maxId_batch = getMaxindex_batch(logit[id_batch])\n                    for id_word in range(inst.words_size):\n                        predict_label.append(self.alphabet.label_alphabet.from_id(label_ids[id_word]))\n            else:\n                path_score, best_paths = self.model.crf_layer(logit, mask)\n                for id_batch in range(data.batch_length):\n                    inst = data.inst[id_batch]\n                    label_ids = best_paths[id_batch].cpu().data.numpy()[:inst.words_size]\n                    for i in label_ids:\n                        predict_label.append(self.alphabet.label_alphabet.from_id(i))\n\n        print(""\\ninfer finished."")\n        self.write2file(result=predict_label, path_source=self.path_source, path_result=self.path_result)\n\n    @staticmethod\n    def write2file(result, path_source, path_result):\n        """"""\n        :param result:\n        :param path_source:\n        :param path_result:\n        :return:\n        """"""\n        print(""write result to file {}"".format(path_result))\n        if os.path.exists(path_source) is False:\n            print(""source data path[path_source] is not exist."")\n        if os.path.exists(path_result):\n            os.remove(path_result)\n        file_out = open(path_result, encoding=""UTF-8"", mode=""w"")\n\n        with open(path_source, encoding=""UTF-8"") as file:\n            id = 0\n            for line in file.readlines():\n                sys.stdout.write(""\\rwrite with {}/{} ."".format(id+1, len(result)))\n                if line == ""\\n"":\n                    file_out.write(""\\n"")\n                    continue\n                line = line.strip().split()\n                line.append(result[id])\n                id += 1\n                file_out.write("" "".join(line) + ""\\n"")\n                if id >= len(result):\n                    break\n\n        file_out.close()\n        print(""\\nfinished."")\n\n    @staticmethod\n    def _get_model_args(batch_features):\n        """"""\n        :param batch_features:  Batch Instance\n        :return:\n        """"""\n        word = batch_features.word_features\n        char = batch_features.char_features\n        mask = word > 0\n        sentence_length = batch_features.sentence_length\n        # desorted_indices = batch_features.desorted_indices\n        tags = batch_features.label_features\n        return word, char, mask, sentence_length, tags\n\n\n\n\n\n\n\n\n'"
trainer.py,4,"b'# @Author : bamtercelboo\n# @Datetime : 2018/8/26 8:30\n# @File : trainer.py\n# @Last Modify Time : 2018/8/26 8:30\n# @Contact : bamtercelboo@{gmail.com, 163.com}\n\n""""""\n    FILE :  trainer.py\n    FUNCTION : None\n""""""\n\nimport os\nimport sys\nimport time\nimport numpy as np\nimport random\nimport torch\nimport torch.optim as optim\nimport torch.nn as nn\nimport torch.nn.utils as utils\nfrom DataUtils.Optim import Optimizer\nfrom DataUtils.utils import *\nfrom DataUtils.eval_bio import entity_evalPRF_exact, entity_evalPRF_propor, entity_evalPRF_binary\nfrom DataUtils.eval import Eval, EvalPRF\nfrom DataUtils.Common import *\ntorch.manual_seed(seed_num)\nrandom.seed(seed_num)\n\n\nclass Train(object):\n    """"""\n        Train\n    """"""\n    def __init__(self, **kwargs):\n        """"""\n        :param kwargs:\n        Args of data:\n            train_iter : train batch data iterator\n            dev_iter : dev batch data iterator\n            test_iter : test batch data iterator\n        Args of train:\n            model : nn model\n            config : config\n        """"""\n        print(""Training Start......"")\n        # for k, v in kwargs.items():\n        #     self.__setattr__(k, v)\n        self.train_iter = kwargs[""train_iter""]\n        self.dev_iter = kwargs[""dev_iter""]\n        self.test_iter = kwargs[""test_iter""]\n        self.model = kwargs[""model""]\n        self.config = kwargs[""config""]\n        self.use_crf = self.config.use_crf\n        self.average_batch = self.config.average_batch\n        self.early_max_patience = self.config.early_max_patience\n        self.optimizer = Optimizer(name=self.config.learning_algorithm, model=self.model, lr=self.config.learning_rate,\n                                   weight_decay=self.config.weight_decay, grad_clip=self.config.clip_max_norm)\n        self.loss_function = self._loss(learning_algorithm=self.config.learning_algorithm,\n                                        label_paddingId=self.config.label_paddingId, use_crf=self.use_crf)\n        print(self.optimizer)\n        print(self.loss_function)\n        self.best_score = Best_Result()\n        self.train_eval, self.dev_eval, self.test_eval = Eval(), Eval(), Eval()\n        self.train_iter_len = len(self.train_iter)\n\n    def _loss(self, learning_algorithm, label_paddingId, use_crf=False):\n        """"""\n        :param learning_algorithm:\n        :param label_paddingId:\n        :param use_crf:\n        :return:\n        """"""\n        if use_crf:\n            loss_function = self.model.crf_layer.neg_log_likelihood_loss\n            return loss_function\n        elif learning_algorithm == ""SGD"":\n            loss_function = nn.CrossEntropyLoss(ignore_index=label_paddingId, reduction=""sum"")\n            return loss_function\n        else:\n            loss_function = nn.CrossEntropyLoss(ignore_index=label_paddingId, reduction=""mean"")\n            return loss_function\n\n    def _clip_model_norm(self, clip_max_norm_use, clip_max_norm):\n        """"""\n        :param clip_max_norm_use:  whether to use clip max norm for nn model\n        :param clip_max_norm: clip max norm max values [float or None]\n        :return:\n        """"""\n        if clip_max_norm_use is True:\n            gclip = None if clip_max_norm == ""None"" else float(clip_max_norm)\n            assert isinstance(gclip, float)\n            utils.clip_grad_norm_(self.model.parameters(), max_norm=gclip)\n\n    def _dynamic_lr(self, config, epoch, new_lr):\n        """"""\n        :param config:  config\n        :param epoch:  epoch\n        :param new_lr:  learning rate\n        :return:\n        """"""\n        if config.use_lr_decay is True and epoch > config.max_patience and (\n                epoch - 1) % config.max_patience == 0 and new_lr > config.min_lrate:\n            new_lr = max(new_lr * config.lr_rate_decay, config.min_lrate)\n            set_lrate(self.optimizer, new_lr)\n        return new_lr\n\n    def _decay_learning_rate(self, epoch, init_lr):\n        """"""lr decay \n\n        Args:\n            epoch: int, epoch \n            init_lr:  initial lr\n        """"""\n        lr = init_lr / (1 + self.config.lr_rate_decay * epoch)\n        for param_group in self.optimizer.param_groups:\n            param_group[\'lr\'] = lr\n        return self.optimizer\n\n    def _optimizer_batch_step(self, config, backward_count):\n        """"""\n        :param config:\n        :param backward_count:\n        :return:\n        """"""\n        if backward_count % config.backward_batch_size == 0 or backward_count == self.train_iter_len:\n            self.optimizer.step()\n            self.optimizer.zero_grad()\n\n    def _early_stop(self, epoch):\n        """"""\n        :param epoch:\n        :return:\n        """"""\n        best_epoch = self.best_score.best_epoch\n        if epoch > best_epoch:\n            self.best_score.early_current_patience += 1\n            print(""Dev Has Not Promote {} / {}"".format(self.best_score.early_current_patience, self.early_max_patience))\n            if self.best_score.early_current_patience >= self.early_max_patience:\n                print(""Early Stop Train. Best Score Locate on {} Epoch."".format(self.best_score.best_epoch))\n                exit()\n\n    @staticmethod\n    def _get_model_args(batch_features):\n        """"""\n        :param batch_features:  Batch Instance\n        :return:\n        """"""\n        word = batch_features.word_features\n        char = batch_features.char_features\n        mask = word > 0\n        sentence_length = batch_features.sentence_length\n        tags = batch_features.label_features\n        return word, char, mask, sentence_length, tags\n\n    def _calculate_loss(self, feats, mask, tags):\n        """"""\n        Args:\n            feats: size = (batch_size, seq_len, tag_size)\n            mask: size = (batch_size, seq_len)\n            tags: size = (batch_size, seq_len)\n        """"""\n        if not self.use_crf:\n            batch_size, max_len = feats.size(0), feats.size(1)\n            lstm_feats = feats.view(batch_size * max_len, -1)\n            tags = tags.view(-1)\n            return self.loss_function(lstm_feats, tags)\n        else:\n            loss_value = self.loss_function(feats, mask, tags)\n        if self.average_batch:\n            batch_size = feats.size(0)\n            loss_value /= float(batch_size)\n        return loss_value\n\n    def train(self):\n        """"""\n        :return:\n        """"""\n        epochs = self.config.epochs\n        clip_max_norm_use = self.config.clip_max_norm_use\n        clip_max_norm = self.config.clip_max_norm\n        new_lr = self.config.learning_rate\n\n        for epoch in range(1, epochs + 1):\n            print(""\\n## The {} Epoch, All {} Epochs ! ##"".format(epoch, epochs))\n            # new_lr = self._dynamic_lr(config=self.config, epoch=epoch, new_lr=new_lr)\n            self.optimizer = self._decay_learning_rate(epoch=epoch - 1, init_lr=self.config.learning_rate)\n            print(""now lr is {}"".format(self.optimizer.param_groups[0].get(""lr"")), end="""")\n            start_time = time.time()\n            random.shuffle(self.train_iter)\n            self.model.train()\n            steps = 1\n            backward_count = 0\n            self.optimizer.zero_grad()\n            for batch_count, batch_features in enumerate(self.train_iter):\n                backward_count += 1\n                # self.optimizer.zero_grad()\n                word, char, mask, sentence_length, tags = self._get_model_args(batch_features)\n                logit = self.model(word, char, sentence_length, train=True)\n                loss = self._calculate_loss(logit, mask, tags)\n                loss.backward()\n                self._clip_model_norm(clip_max_norm_use, clip_max_norm)\n                self._optimizer_batch_step(config=self.config, backward_count=backward_count)\n                # self.optimizer.step()\n                steps += 1\n                if (steps - 1) % self.config.log_interval == 0:\n                    self.getAcc(self.train_eval, batch_features, logit, self.config)\n                    sys.stdout.write(\n                        ""\\nbatch_count = [{}] , loss is {:.6f}, [TAG-ACC is {:.6f}%]"".format(batch_count + 1, loss.item(), self.train_eval.acc()))\n            end_time = time.time()\n            print(""\\nTrain Time {:.3f}"".format(end_time - start_time), end="""")\n            self.eval(model=self.model, epoch=epoch, config=self.config)\n            self._model2file(model=self.model, config=self.config, epoch=epoch)\n            self._early_stop(epoch=epoch)\n\n    def eval(self, model, epoch, config):\n        """"""\n        :param model: nn model\n        :param epoch:  epoch\n        :param config:  config\n        :return:\n        """"""\n        self.dev_eval.clear_PRF()\n        eval_start_time = time.time()\n        self.eval_batch(self.dev_iter, model, self.dev_eval, self.best_score, epoch, config, test=False)\n        eval_end_time = time.time()\n        print(""Dev Time {:.3f}"".format(eval_end_time - eval_start_time))\n\n        self.test_eval.clear_PRF()\n        eval_start_time = time.time()\n        self.eval_batch(self.test_iter, model, self.test_eval, self.best_score, epoch, config, test=True)\n        eval_end_time = time.time()\n        print(""Test Time {:.3f}"".format(eval_end_time - eval_start_time))\n\n    def _model2file(self, model, config, epoch):\n        """"""\n        :param model:  nn model\n        :param config:  config\n        :param epoch:  epoch\n        :return:\n        """"""\n        if config.save_model and config.save_all_model:\n            save_model_all(model, config.save_dir, config.model_name, epoch)\n        elif config.save_model and config.save_best_model:\n            save_best_model(model, config.save_best_model_path, config.model_name, self.best_score)\n        else:\n            print()\n\n    def eval_batch(self, data_iter, model, eval_instance, best_score, epoch, config, test=False):\n        """"""\n        :param data_iter:  eval batch data iterator\n        :param model: eval model\n        :param eval_instance:\n        :param best_score:\n        :param epoch:\n        :param config: config\n        :param test:  whether to test\n        :return: None\n        """"""\n        model.eval()\n        # eval time\n        eval_acc = Eval()\n        eval_PRF = EvalPRF()\n        gold_labels = []\n        predict_labels = []\n        for batch_features in data_iter:\n            word, char, mask, sentence_length, tags = self._get_model_args(batch_features)\n            logit = model(word, char, sentence_length, train=False)\n            if self.use_crf is False:\n                predict_ids = torch_max(logit)\n                for id_batch in range(batch_features.batch_length):\n                    inst = batch_features.inst[id_batch]\n                    label_ids = predict_ids[id_batch]\n                    predict_label = []\n                    for id_word in range(inst.words_size):\n                        predict_label.append(config.create_alphabet.label_alphabet.from_id(label_ids[id_word]))\n                    gold_labels.append(inst.labels)\n                    predict_labels.append(predict_label)\n            else:\n                path_score, best_paths = model.crf_layer(logit, mask)\n                for id_batch in range(batch_features.batch_length):\n                    inst = batch_features.inst[id_batch]\n                    gold_labels.append(inst.labels)\n                    label_ids = best_paths[id_batch].cpu().data.numpy()[:inst.words_size]\n                    label = []\n                    for i in label_ids:\n                        # print(""\\n"", i)\n                        label.append(config.create_alphabet.label_alphabet.from_id(int(i)))\n                    predict_labels.append(label)\n        for p_label, g_label in zip(predict_labels, gold_labels):\n            eval_PRF.evalPRF(predict_labels=p_label, gold_labels=g_label, eval=eval_instance)\n        if eval_acc.gold_num == 0:\n            eval_acc.gold_num = 1\n        p, r, f = eval_instance.getFscore()\n        # p, r, f = entity_evalPRF_exact(gold_labels=gold_labels, predict_labels=predict_labels)\n        # p, r, f = entity_evalPRF_propor(gold_labels=gold_labels, predict_labels=predict_labels)\n        # p, r, f = entity_evalPRF_binary(gold_labels=gold_labels, predict_labels=predict_labels)\n        test_flag = ""Test""\n        if test is False:\n            print()\n            test_flag = ""Dev""\n            best_score.current_dev_score = f\n            if f >= best_score.best_dev_score:\n                best_score.best_dev_score = f\n                best_score.best_epoch = epoch\n                best_score.best_test = True\n        if test is True and best_score.best_test is True:\n            best_score.p = p\n            best_score.r = r\n            best_score.f = f\n        print(\n            ""{} eval: precision = {:.6f}%  recall = {:.6f}% , f-score = {:.6f}%,  [TAG-ACC = {:.6f}%]"".format(test_flag,\n                                                                                                              p, r, f,\n                                                                                                              0.0000))\n        if test is True:\n            print(""The Current Best Dev F-score: {:.6f}, Locate on {} Epoch."".format(best_score.best_dev_score,\n                                                                                     best_score.best_epoch))\n            print(""The Current Best Test Result: precision = {:.6f}%  recall = {:.6f}% , f-score = {:.6f}%"".format(\n                best_score.p, best_score.r, best_score.f))\n        if test is True:\n            best_score.best_test = False\n\n    @staticmethod\n    def getAcc(eval_acc, batch_features, logit, config):\n        """"""\n        :param eval_acc:  eval instance\n        :param batch_features:  batch data feature\n        :param logit:  model output\n        :param config:  config\n        :return:\n        """"""\n        eval_acc.clear_PRF()\n        predict_ids = torch_max(logit)\n        for id_batch in range(batch_features.batch_length):\n            inst = batch_features.inst[id_batch]\n            label_ids = predict_ids[id_batch]\n            predict_label = []\n            gold_lable = inst.labels\n            for id_word in range(inst.words_size):\n                predict_label.append(config.create_alphabet.label_alphabet.from_id(label_ids[id_word]))\n            assert len(predict_label) == len(gold_lable)\n            cor = 0\n            for p_lable, g_lable in zip(predict_label, gold_lable):\n                if p_lable == g_lable:\n                    cor += 1\n            eval_acc.correct_num += cor\n            eval_acc.gold_num += len(gold_lable)\n\n\n\n\n'"
Config/config.py,0,"b'\nfrom configparser import ConfigParser\nimport os\n\n\nclass myconf(ConfigParser):\n    def __init__(self, defaults=None):\n        ConfigParser.__init__(self, defaults=defaults)\n        self.add_sec = ""Additional""\n\n    def optionxform(self, optionstr):\n        return optionstr\n\n\nclass Configurable(myconf):\n    def __init__(self, config_file):\n        # config = ConfigParser()\n        super().__init__()\n\n        self.test = None\n        self.train = None\n        config = myconf()\n        config.read(config_file)\n        self._config = config\n        self.config_file = config_file\n\n        print(\'Loaded config file sucessfully.\')\n        for section in config.sections():\n            for k, v in config.items(section):\n                print(k, "":"", v)\n        if not os.path.isdir(self.save_direction):\n            os.mkdir(self.save_direction)\n        config.write(open(config_file, \'w\'))\n\n    def add_args(self, key, value):\n        self._config.set(self.add_sec, key, value)\n        self._config.write(open(self.config_file, \'w\'))\n\n    # Embed\n    @property\n    def pretrained_embed(self):\n        return self._config.getboolean(\'Embed\', \'pretrained_embed\')\n\n    @property\n    def zeros(self):\n        return self._config.getboolean(\'Embed\', \'zeros\')\n\n    @property\n    def avg(self):\n        return self._config.getboolean(\'Embed\', \'avg\')\n\n    @property\n    def uniform(self):\n        return self._config.getboolean(\'Embed\', \'uniform\')\n\n    @property\n    def nnembed(self):\n        return self._config.getboolean(\'Embed\', \'nnembed\')\n\n    @property\n    def pretrained_embed_file(self):\n        return self._config.get(\'Embed\', \'pretrained_embed_file\')\n\n    # Data\n    @property\n    def train_file(self):\n        return self._config.get(\'Data\', \'train_file\')\n\n    @property\n    def dev_file(self):\n        return self._config.get(\'Data\', \'dev_file\')\n\n    @property\n    def test_file(self):\n        return self._config.get(\'Data\', \'test_file\')\n\n    @property\n    def max_count(self):\n        return self._config.getint(\'Data\', \'max_count\')\n\n    @property\n    def min_freq(self):\n        return self._config.getint(\'Data\', \'min_freq\')\n\n    @property\n    def shuffle(self):\n        return self._config.getboolean(\'Data\', \'shuffle\')\n\n    @property\n    def epochs_shuffle(self):\n        return self._config.getboolean(\'Data\', \'epochs_shuffle\')\n\n    # Save\n    @property\n    def save_pkl(self):\n        return self._config.getboolean(\'Save\', \'save_pkl\')\n\n    @property\n    def pkl_directory(self):\n        return self._config.get(\'Save\', \'pkl_directory\')\n\n    @property\n    def pkl_data(self):\n        return self._config.get(\'Save\', \'pkl_data\')\n\n    @property\n    def pkl_alphabet(self):\n        return self._config.get(\'Save\', \'pkl_alphabet\')\n\n    @property\n    def pkl_iter(self):\n        return self._config.get(\'Save\', \'pkl_iter\')\n\n    @property\n    def pkl_embed(self):\n        return self._config.get(\'Save\', \'pkl_embed\')\n\n    @property\n    def save_dict(self):\n        return self._config.getboolean(\'Save\', \'save_dict\')\n\n    @property\n    def save_direction(self):\n        return self._config.get(\'Save\', \'save_direction\')\n\n    @property\n    def dict_directory(self):\n        return self._config.get(\'Save\', \'dict_directory\')\n\n    @property\n    def word_dict(self):\n        return self._config.get(\'Save\', \'word_dict\')\n\n    @property\n    def label_dict(self):\n        return self._config.get(\'Save\', \'label_dict\')\n\n    @property\n    def model_name(self):\n        return self._config.get(\'Save\', \'model_name\')\n\n    @property\n    def save_best_model_dir(self):\n        return self._config.get(\'Save\', \'save_best_model_dir\')\n\n    @property\n    def save_model(self):\n        return self._config.getboolean(\'Save\', \'save_model\')\n\n    @property\n    def save_all_model(self):\n        return self._config.getboolean(\'Save\', \'save_all_model\')\n\n    @property\n    def save_best_model(self):\n        return self._config.getboolean(\'Save\', \'save_best_model\')\n\n    @property\n    def rm_model(self):\n        return self._config.getboolean(\'Save\', \'rm_model\')\n\n    # Model\n    @property\n    def average_batch(self):\n        return self._config.getboolean(""Model"", ""average_batch"")\n\n    @property\n    def use_crf(self):\n        return self._config.getboolean(""Model"", ""use_crf"")\n\n    @property\n    def use_char(self):\n        return self._config.getboolean(""Model"", ""use_char"")\n\n    @property\n    def model_bilstm(self):\n        return self._config.getboolean(""Model"", ""model_bilstm"")\n\n    @property\n    def model_bilstm_context(self):\n        return self._config.getboolean(""Model"", ""model_bilstm_context"")\n\n    @property\n    def lstm_layers(self):\n        return self._config.getint(""Model"", ""lstm_layers"")\n\n    @property\n    def embed_dim(self):\n        return self._config.getint(""Model"", ""embed_dim"")\n\n    @property\n    def embed_finetune(self):\n        return self._config.getboolean(""Model"", ""embed_finetune"")\n\n    @property\n    def lstm_hiddens(self):\n        return self._config.getint(""Model"", ""lstm_hiddens"")\n\n    @property\n    def dropout_emb(self):\n        return self._config.getfloat(""Model"", ""dropout_emb"")\n\n    @property\n    def dropout(self):\n        return self._config.getfloat(""Model"", ""dropout"")\n\n    @property\n    def max_char_len(self):\n        return self._config.getint(""Model"", ""max_char_len"")\n\n    @property\n    def char_dim(self):\n        return self._config.getint(""Model"", ""char_dim"")\n\n    @property\n    def conv_filter_sizes(self):\n        return self._config.get(""Model"", ""conv_filter_sizes"")\n\n    @property\n    def conv_filter_nums(self):\n        return self._config.get(""Model"", ""conv_filter_nums"")\n\n    @property\n    def windows_size(self):\n        return self._config.getint(""Model"", ""windows_size"")\n\n    # Optimizer\n    @property\n    def adam(self):\n        return self._config.getboolean(""Optimizer"", ""adam"")\n\n    @property\n    def sgd(self):\n        return self._config.getboolean(""Optimizer"", ""sgd"")\n\n    @property\n    def learning_rate(self):\n        return self._config.getfloat(""Optimizer"", ""learning_rate"")\n\n    @property\n    def weight_decay(self):\n        return self._config.getfloat(""Optimizer"", ""weight_decay"")\n\n    @property\n    def momentum(self):\n        return self._config.getfloat(""Optimizer"", ""momentum"")\n\n    @property\n    def clip_max_norm_use(self):\n        return self._config.getboolean(""Optimizer"", ""clip_max_norm_use"")\n\n    @property\n    def clip_max_norm(self):\n        return self._config.get(""Optimizer"", ""clip_max_norm"")\n\n    @property\n    def use_lr_decay(self):\n        return self._config.getboolean(""Optimizer"", ""use_lr_decay"")\n\n    @property\n    def lr_rate_decay(self):\n        return self._config.getfloat(""Optimizer"", ""lr_rate_decay"")\n\n    @property\n    def min_lrate(self):\n        return self._config.getfloat(""Optimizer"", ""min_lrate"")\n\n    @property\n    def max_patience(self):\n        return self._config.getint(""Optimizer"", ""max_patience"")\n\n    # Train\n    @property\n    def num_threads(self):\n        return self._config.getint(""Train"", ""num_threads"")\n\n    @property\n    def epochs(self):\n        return self._config.getint(""Train"", ""epochs"")\n\n    @property\n    def early_max_patience(self):\n        return self._config.getint(""Train"", ""early_max_patience"")\n\n    @property\n    def backward_batch_size(self):\n        return self._config.getint(""Train"", ""backward_batch_size"")\n\n    @property\n    def batch_size(self):\n        return self._config.getint(""Train"", ""batch_size"")\n\n    @property\n    def dev_batch_size(self):\n        return self._config.getint(""Train"", ""dev_batch_size"")\n\n    @property\n    def test_batch_size(self):\n        return self._config.getint(""Train"", ""test_batch_size"")\n\n    @property\n    def log_interval(self):\n        return self._config.getint(""Train"", ""log_interval"")\n\n\n\n\n'"
DataUtils/Alphabet.py,1,"b'# @Author : bamtercelboo\n# @Datetime : 2018/1/30 15:54\n# @File : Alphabet.py\n# @Last Modify Time : 2018/1/30 15:54\n# @Contact : bamtercelboo@{gmail.com, 163.com}\n\n""""""\n    FILE :  Alphabet.py\n    FUNCTION : None\n""""""\n\nimport os\nimport sys\nimport torch\nimport random\nimport collections\nfrom DataUtils.Common import seed_num, unkkey, paddingkey\ntorch.manual_seed(seed_num)\nrandom.seed(seed_num)\n\n\nclass CreateAlphabet:\n    """"""\n        Class:      Create_Alphabet\n        Function:   Build Alphabet By Alphabet Class\n        Notice:     The Class Need To Change So That Complete All Kinds Of Tasks\n    """"""\n    def __init__(self, min_freq=1, train_data=None, dev_data=None, test_data=None, config=None):\n\n        # minimum vocab size\n        self.min_freq = min_freq\n        self.config = config\n        self.train_data = train_data\n        self.dev_data = dev_data\n        self.test_data = test_data\n\n        # storage word and label\n        self.word_state = collections.OrderedDict()\n        self.label_state = collections.OrderedDict()\n        self.char_state = collections.OrderedDict()\n        # self.word_state = {}\n        # self.label_state = {}\n\n        # unk and pad\n        self.word_state[unkkey] = self.min_freq\n        self.word_state[paddingkey] = self.min_freq\n        self.char_state[unkkey] = self.min_freq\n        self.char_state[paddingkey] = self.min_freq\n        # self.label_state[unkkey] = 1\n        self.label_state[paddingkey] = 1\n\n        # word and label Alphabet\n        self.word_alphabet = Alphabet(min_freq=self.min_freq)\n        self.char_alphabet = Alphabet(min_freq=self.min_freq)\n        self.label_alphabet = Alphabet()\n        self.pretrained_alphabet = Alphabet(min_freq=self.min_freq)\n        self.pretrained_alphabet_source = Alphabet(min_freq=self.min_freq)\n\n        # unk key\n        self.word_unkId = 0\n        self.char_unkId = 0\n        self.label_unkId = 0\n\n        # padding key\n        self.word_paddingId = 0\n        self.char_paddingId = 0\n        self.label_paddingId = 0\n\n    @staticmethod\n    def _build_data(train_data=None, dev_data=None, test_data=None):\n        """"""\n        :param train_data:\n        :param dev_data:\n        :param test_data:\n        :return:\n        """"""\n        # handle the data whether to fine_tune\n        """"""\n        :param train data:\n        :param dev data:\n        :param test data:\n        :return: merged data\n        """"""\n        assert train_data is not None, ""The Train Data Is Not Allow Empty.""\n        datasets = []\n        datasets.extend(train_data)\n        print(""the length of train data {}"".format(len(datasets)))\n        if dev_data is not None:\n            print(""the length of dev data {}"".format(len(dev_data)))\n            datasets.extend(dev_data)\n        if test_data is not None:\n            print(""the length of test data {}"".format(len(test_data)))\n            datasets.extend(test_data)\n        print(""the length of data that create Alphabet {}"".format(len(datasets)))\n        return datasets\n\n    def build_vocab(self):\n        """"""\n        :param train_data:\n        :param dev_data:\n        :param test_data:\n        :param debug_index:\n        :return:\n        """"""\n        train_data = self.train_data\n        dev_data = self.dev_data\n        test_data = self.test_data\n        print(""Build Vocab Start...... "")\n        datasets = self._build_data(train_data=train_data, dev_data=dev_data, test_data=test_data)\n        # create the word Alphabet\n\n        for index, data in enumerate(datasets):\n            # word\n            for word in data.words:\n                if word not in self.word_state:\n                    self.word_state[word] = 1\n                else:\n                    self.word_state[word] += 1\n\n            # char\n            for char in data.chars:\n                # print(char)\n                for c in char:\n                    if c.isalnum() is False:\n                        continue\n                    if c not in self.char_state:\n                        self.char_state[c] = 1\n                    else:\n                        self.char_state[c] += 1\n\n            # label\n            for label in data.labels:\n                if label not in self.label_state:\n                    self.label_state[label] = 1\n                else:\n                    self.label_state[label] += 1\n        # print(self.char_state)\n        # exit()\n\n        # self.label_state[unkkey] = 1\n\n        # Create id2words and words2id by the Alphabet Class\n        self.word_alphabet.initial(self.word_state)\n        self.char_alphabet.initial(self.char_state)\n        self.label_alphabet.initial(self.label_state)\n\n        # unkId and paddingId\n        self.word_unkId = self.word_alphabet.from_string(unkkey)\n        self.char_unkId = self.char_alphabet.from_string(unkkey)\n        # self.label_unkId = self.label_alphabet.loadWord2idAndId2Word(unkkey)\n        self.word_paddingId = self.word_alphabet.from_string(paddingkey)\n        self.char_paddingId = self.char_alphabet.from_string(paddingkey)\n        self.label_paddingId = self.label_alphabet.from_string(paddingkey)\n\n        # fix the vocab\n        self.word_alphabet.set_fixed_flag(True)\n        self.label_alphabet.set_fixed_flag(True)\n        self.char_alphabet.set_fixed_flag(True)\n\n\nclass Alphabet:\n    """"""\n        Class: Alphabet\n        Function: Build vocab\n        Params:\n              ******    id2words:   type(list),\n              ******    word2id:    type(dict)\n              ******    vocab_size: vocab size\n              ******    min_freq:   vocab minimum freq\n              ******    fixed_vocab: fix the vocab after build vocab\n              ******    max_cap: max vocab size\n    """"""\n    def __init__(self, min_freq=1):\n        self.id2words = []\n        self.words2id = collections.OrderedDict()\n        self.vocab_size = 0\n        self.min_freq = min_freq\n        self.max_cap = 1e8\n        self.fixed_vocab = False\n\n    def initial(self, data):\n        """"""\n        :param data:\n        :return:\n        """"""\n        for key in data:\n            if data[key] >= self.min_freq:\n                self.from_string(key)\n        self.set_fixed_flag(True)\n\n    def set_fixed_flag(self, bfixed):\n        """"""\n        :param bfixed:\n        :return:\n        """"""\n        self.fixed_vocab = bfixed\n        if (not self.fixed_vocab) and (self.vocab_size >= self.max_cap):\n            self.fixed_vocab = True\n\n    def from_string(self, string):\n        """"""\n        :param string:\n        :return:\n        """"""\n        if string in self.words2id:\n            return self.words2id[string]\n        else:\n            if not self.fixed_vocab:\n                newid = self.vocab_size\n                self.id2words.append(string)\n                self.words2id[string] = newid\n                self.vocab_size += 1\n                if self.vocab_size >= self.max_cap:\n                    self.fixed_vocab = True\n                return newid\n            else:\n                return -1\n\n    def from_id(self, qid, defineStr=""""):\n        """"""\n        :param qid:\n        :param defineStr:\n        :return:\n        """"""\n        if int(qid) < 0 or self.vocab_size <= qid:\n            return defineStr\n        else:\n            return self.id2words[qid]\n\n    def initial_from_pretrain(self, pretrain_file, unk, padding):\n        """"""\n        :param pretrain_file:\n        :param unk:\n        :param padding:\n        :return:\n        """"""\n        print(""initial alphabet from {}"".format(pretrain_file))\n        self.from_string(unk)\n        self.from_string(padding)\n        now_line = 0\n        with open(pretrain_file, encoding=""UTF-8"") as f:\n            for line in f.readlines():\n                now_line += 1\n                sys.stdout.write(""\\rhandling with {} line"".format(now_line))\n                info = line.split("" "")\n                self.from_string(info[0])\n        f.close()\n        print(""\\nHandle Finished."")\n\n\n\n'"
DataUtils/Batch_Iterator.py,10,"b'# @Author : bamtercelboo\n# @Datetime : 2018/1/30 15:55\n# @File : Batch_Iterator.py.py\n# @Last Modify Time : 2018/1/30 15:55\n# @Contact : bamtercelboo@{gmail.com, 163.com}\n\n""""""\n    FILE :  Batch_Iterator.py\n    FUNCTION : None\n""""""\n\nimport torch\nfrom torch.autograd import Variable\nimport random\nimport numpy as np\nfrom DataUtils.Common import *\ntorch.manual_seed(seed_num)\nrandom.seed(seed_num)\n\n\nclass Batch_Features:\n    """"""\n    Batch_Features\n    """"""\n    def __init__(self):\n\n        self.batch_length = 0\n        self.inst = None\n        self.word_features = 0\n        self.char_features = 0\n        self.label_features = 0\n        self.sentence_length = []\n        self.desorted_indices = None\n\n    @staticmethod\n    def cuda(features):\n        """"""\n        :param features:\n        :return:\n        """"""\n        features.word_features = features.word_features.cuda()\n        features.label_features = features.label_features.cuda()\n        features.char_features = features.char_features.cuda()\n\n\nclass Iterators:\n    """"""\n    Iterators\n    """"""\n    def __init__(self, batch_size=None, data=None, operator=None, device=None, config=None):\n        self.config = config\n        self.batch_size = batch_size\n        self.data = data\n        self.device = device\n        self.operator = operator\n        self.operator_static = None\n        self.iterator = []\n        self.batch = []\n        self.features = []\n        self.data_iter = []\n        self.max_char_len = config.max_char_len\n\n    def createIterator(self):\n        """"""\n        :param batch_size:  batch size\n        :param data:  data\n        :param operator:\n        :param config:\n        :return:\n        """"""\n        assert isinstance(self.data, list), ""ERROR: data must be in list [train_data,dev_data]""\n        assert isinstance(self.batch_size, list), ""ERROR: batch_size must be in list [16,1,1]""\n        for id_data in range(len(self.data)):\n            print(""*****************    create {} iterator    **************"".format(id_data + 1))\n            self._convert_word2id(self.data[id_data], self.operator)\n            self.features = self._Create_Each_Iterator(insts=self.data[id_data], batch_size=self.batch_size[id_data],\n                                                       operator=self.operator, device=self.device)\n            self.data_iter.append(self.features)\n            self.features = []\n        if len(self.data_iter) == 2:\n            return self.data_iter[0], self.data_iter[1]\n        if len(self.data_iter) == 3:\n            return self.data_iter[0], self.data_iter[1], self.data_iter[2]\n\n    @staticmethod\n    def _convert_word2id(insts, operator):\n        """"""\n        :param insts:\n        :param operator:\n        :return:\n        """"""\n        # print(len(insts))\n        # for index_inst, inst in enumerate(insts):\n        for inst in insts:\n            # copy with the word and pos\n            for index in range(inst.words_size):\n                word = inst.words[index]\n                wordId = operator.word_alphabet.from_string(word)\n                # if wordID is None:\n                if wordId == -1:\n                    wordId = operator.word_unkId\n                inst.words_index.append(wordId)\n\n                label = inst.labels[index]\n                labelId = operator.label_alphabet.from_string(label)\n                inst.label_index.append(labelId)\n\n                char_index = []\n                for char in inst.chars[index]:\n                    charId = operator.char_alphabet.from_string(char)\n                    if charId == -1:\n                        charId = operator.char_unkId\n                    char_index.append(charId)\n                inst.chars_index.append(char_index)\n\n    def _Create_Each_Iterator(self, insts, batch_size, operator, device):\n        """"""\n        :param insts:\n        :param batch_size:\n        :param operator:\n        :return:\n        """"""\n        batch = []\n        count_inst = 0\n        for index, inst in enumerate(insts):\n            batch.append(inst)\n            count_inst += 1\n            # print(batch)\n            if len(batch) == batch_size or count_inst == len(insts):\n                one_batch = self._Create_Each_Batch(insts=batch, batch_size=batch_size, operator=operator, device=device)\n                self.features.append(one_batch)\n                batch = []\n        print(""The all data has created iterator."")\n        return self.features\n\n    def _Create_Each_Batch(self, insts, batch_size, operator, device):\n        """"""\n        :param insts:\n        :param batch_size:\n        :param operator:\n        :return:\n        """"""\n        # print(""create one batch......"")\n        batch_length = len(insts)\n        # copy with the max length for padding\n        max_word_size = -1\n        max_label_size = -1\n        sentence_length = []\n        for inst in insts:\n            sentence_length.append(inst.words_size)\n            word_size = inst.words_size\n            if word_size > max_word_size:\n                max_word_size = word_size\n\n            if len(inst.labels) > max_label_size:\n                max_label_size = len(inst.labels)\n        assert max_word_size == max_label_size\n\n        # create with the Tensor/Variable\n        # word features\n        # batch_word_features = torch.zeros(batch_length, max_word_size, device=cpu_device, requires_grad=True).long()\n        # batch_char_features = torch.zeros(batch_length, max_word_size, self.max_char_len, device=cpu_device, requires_grad=True).long()\n        # batch_label_features = torch.zeros(batch_length * max_word_size, device=cpu_device, requires_grad=True).long()\n\n        batch_word_features = np.zeros((batch_length, max_word_size))\n        batch_char_features = np.zeros((batch_length, max_word_size, self.max_char_len))\n        batch_label_features = np.zeros((batch_length * max_word_size))\n\n        for id_inst in range(batch_length):\n            inst = insts[id_inst]\n            # copy with the word features\n            for id_word_index in range(max_word_size):\n                if id_word_index < inst.words_size:\n                    batch_word_features[id_inst][id_word_index] = inst.words_index[id_word_index]\n                else:\n                    batch_word_features[id_inst][id_word_index] = operator.word_paddingId\n\n                if id_word_index < len(inst.label_index):\n                    batch_label_features[id_inst * max_word_size + id_word_index] = inst.label_index[id_word_index]\n                else:\n                    batch_label_features[id_inst * max_word_size + id_word_index] = operator.label_paddingId\n\n                # char\n                max_char_size = len(inst.chars_index[id_word_index]) if id_word_index < inst.words_size else 0\n                for id_word_c in range(self.max_char_len):\n                    if id_word_c < max_char_size:\n                        batch_char_features[id_inst][id_word_index][id_word_c] = inst.chars_index[id_word_index][id_word_c]\n                    else:\n                        batch_char_features[id_inst][id_word_index][id_word_c] = operator.char_paddingId\n\n        batch_word_features = torch.from_numpy(batch_word_features).long()\n        batch_char_features = torch.from_numpy(batch_char_features).long()\n        batch_label_features = torch.from_numpy(batch_label_features).long()\n        \n        # batch\n        features = Batch_Features()\n        features.batch_length = batch_length\n        features.inst = insts\n        features.word_features = batch_word_features\n        features.char_features = batch_char_features\n        features.label_features = batch_label_features\n        features.sentence_length = sentence_length\n        features.desorted_indices = None\n\n        if device != cpu_device:\n            features.cuda(features)\n        return features\n\n    @staticmethod\n    def _prepare_pack_padded_sequence(inputs_words, seq_lengths, descending=True):\n        """"""\n        :param inputs_words:\n        :param seq_lengths:\n        :param descending:\n        :return:\n        """"""\n        sorted_seq_lengths, indices = torch.sort(torch.LongTensor(seq_lengths), descending=descending)\n        _, desorted_indices = torch.sort(indices, descending=False)\n        sorted_inputs_words = inputs_words[indices]\n        return sorted_inputs_words, sorted_seq_lengths.numpy(), desorted_indices\n\n\n\n'"
DataUtils/Batch_Iterator_torch.py,7,"b'# @Author : bamtercelboo\n# @Datetime : 2018/1/30 15:55\n# @File : Batch_Iterator.py.py\n# @Last Modify Time : 2018/1/30 15:55\n# @Contact : bamtercelboo@{gmail.com, 163.com}\n\n""""""\n    FILE :  Batch_Iterator.py\n    FUNCTION : None\n""""""\n\nimport torch\nfrom torch.autograd import Variable\nimport random\n\nfrom DataUtils.Common import *\ntorch.manual_seed(seed_num)\nrandom.seed(seed_num)\n\n\nclass Batch_Features:\n    """"""\n    Batch_Features\n    """"""\n    def __init__(self):\n\n        self.batch_length = 0\n        self.inst = None\n        self.word_features = 0\n        self.char_features = 0\n        self.label_features = 0\n        self.sentence_length = []\n        self.desorted_indices = None\n\n    @staticmethod\n    def cuda(features):\n        """"""\n        :param features:\n        :return:\n        """"""\n        features.word_features = features.word_features.cuda()\n        features.label_features = features.label_features.cuda()\n        features.char_features = features.char_features.cuda()\n\n\nclass Iterators:\n    """"""\n    Iterators\n    """"""\n    def __init__(self, batch_size=None, data=None, operator=None, device=None, config=None):\n        self.config = config\n        self.batch_size = batch_size\n        self.data = data\n        self.device = device\n        self.operator = operator\n        self.operator_static = None\n        self.iterator = []\n        self.batch = []\n        self.features = []\n        self.data_iter = []\n        self.max_char_len = config.max_char_len\n\n    def createIterator(self):\n        """"""\n        :param batch_size:  batch size\n        :param data:  data\n        :param operator:\n        :param config:\n        :return:\n        """"""\n        assert isinstance(self.data, list), ""ERROR: data must be in list [train_data,dev_data]""\n        assert isinstance(self.batch_size, list), ""ERROR: batch_size must be in list [16,1,1]""\n        for id_data in range(len(self.data)):\n            print(""*****************    create {} iterator    **************"".format(id_data + 1))\n            self._convert_word2id(self.data[id_data], self.operator)\n            self.features = self._Create_Each_Iterator(insts=self.data[id_data], batch_size=self.batch_size[id_data],\n                                                       operator=self.operator, device=self.device)\n            self.data_iter.append(self.features)\n            self.features = []\n        if len(self.data_iter) == 2:\n            return self.data_iter[0], self.data_iter[1]\n        if len(self.data_iter) == 3:\n            return self.data_iter[0], self.data_iter[1], self.data_iter[2]\n\n    @staticmethod\n    def _convert_word2id(insts, operator):\n        """"""\n        :param insts:\n        :param operator:\n        :return:\n        """"""\n        # print(len(insts))\n        # for index_inst, inst in enumerate(insts):\n        for inst in insts:\n            # copy with the word and pos\n            for index in range(inst.words_size):\n                word = inst.words[index]\n                wordId = operator.word_alphabet.from_string(word)\n                # if wordID is None:\n                if wordId == -1:\n                    wordId = operator.word_unkId\n                inst.words_index.append(wordId)\n\n                label = inst.labels[index]\n                labelId = operator.label_alphabet.from_string(label)\n                inst.label_index.append(labelId)\n\n                char_index = []\n                for char in inst.chars[index]:\n                    charId = operator.char_alphabet.from_string(char)\n                    if charId == -1:\n                        charId = operator.char_unkId\n                    char_index.append(charId)\n                inst.chars_index.append(char_index)\n\n    def _Create_Each_Iterator(self, insts, batch_size, operator, device):\n        """"""\n        :param insts:\n        :param batch_size:\n        :param operator:\n        :return:\n        """"""\n        batch = []\n        count_inst = 0\n        for index, inst in enumerate(insts):\n            batch.append(inst)\n            count_inst += 1\n            # print(batch)\n            if len(batch) == batch_size or count_inst == len(insts):\n                one_batch = self._Create_Each_Batch(insts=batch, batch_size=batch_size, operator=operator, device=device)\n                self.features.append(one_batch)\n                batch = []\n        print(""The all data has created iterator."")\n        return self.features\n\n    def _Create_Each_Batch(self, insts, batch_size, operator, device):\n        """"""\n        :param insts:\n        :param batch_size:\n        :param operator:\n        :return:\n        """"""\n        # print(""create one batch......"")\n        batch_length = len(insts)\n        # copy with the max length for padding\n        max_word_size = -1\n        max_label_size = -1\n        sentence_length = []\n        for inst in insts:\n            sentence_length.append(inst.words_size)\n            word_size = inst.words_size\n            if word_size > max_word_size:\n                max_word_size = word_size\n\n            if len(inst.labels) > max_label_size:\n                max_label_size = len(inst.labels)\n        assert max_word_size == max_label_size\n\n        # create with the Tensor/Variable\n        # word features\n        batch_word_features = torch.zeros(batch_length, max_word_size, device=cpu_device, requires_grad=True).long()\n        batch_char_features = torch.zeros(batch_length, max_word_size, self.max_char_len, device=cpu_device, requires_grad=True).long()\n        batch_label_features = torch.zeros(batch_length * max_word_size, device=cpu_device, requires_grad=True).long()\n\n        for id_inst in range(batch_length):\n            inst = insts[id_inst]\n            # copy with the word features\n            for id_word_index in range(max_word_size):\n                if id_word_index < inst.words_size:\n                    batch_word_features.data[id_inst][id_word_index] = inst.words_index[id_word_index]\n                else:\n                    batch_word_features.data[id_inst][id_word_index] = operator.word_paddingId\n\n                if id_word_index < len(inst.label_index):\n                    batch_label_features.data[id_inst * max_word_size + id_word_index] = inst.label_index[id_word_index]\n                else:\n                    batch_label_features.data[id_inst * max_word_size + id_word_index] = operator.label_paddingId\n                    # batch_label_features.data[id_inst * max_word_size + id_word_index] = 0\n\n                # char\n                max_char_size = len(inst.chars_index[id_word_index]) if id_word_index < inst.words_size else 0\n                for id_word_c in range(self.max_char_len):\n                    if id_word_c < max_char_size:\n                        batch_char_features.data[id_inst][id_word_index][id_word_c] = inst.chars_index[id_word_index][id_word_c]\n                    else:\n                        batch_char_features.data[id_inst][id_word_index][id_word_c] = operator.char_paddingId\n\n        # batch\n        features = Batch_Features()\n        features.batch_length = batch_length\n        features.inst = insts\n        features.word_features = batch_word_features\n        features.char_features = batch_char_features\n        features.label_features = batch_label_features\n        features.sentence_length = sentence_length\n        features.desorted_indices = None\n\n        if device != cpu_device:\n            features.cuda(features)\n        return features\n\n    @staticmethod\n    def _prepare_pack_padded_sequence(inputs_words, seq_lengths, descending=True):\n        """"""\n        :param inputs_words:\n        :param seq_lengths:\n        :param descending:\n        :return:\n        """"""\n        sorted_seq_lengths, indices = torch.sort(torch.LongTensor(seq_lengths), descending=descending)\n        _, desorted_indices = torch.sort(indices, descending=False)\n        sorted_inputs_words = inputs_words[indices]\n        return sorted_inputs_words, sorted_seq_lengths.numpy(), desorted_indices\n\n\n\n'"
DataUtils/Common.py,0,"b'# @Author : bamtercelboo\n# @Datetime : 2018/1/30 15:55\n# @File : common.py\n# @Last Modify Time : 2018/1/30 15:55\n# @Contact : bamtercelboo@{gmail.com, 163.com}\n\n""""""\n    FILE :  common.py\n    FUNCTION : Common File\n""""""\n\nseed_num = 233\nunkkey = ""<unk>""\npaddingkey = ""<pad>""\n# char_pad = ""cpad""\nchar_pad = ""##""\ncpu_device = ""cpu""\nshuffle = ""shuffle""\n\n'"
DataUtils/Embed.py,7,"b'# @Author : bamtercelboo\n# @Datetime : 2018/8/27 15:34\n# @File : Embed.py\n# @Last Modify Time : 2018/8/27 15:34\n# @Contact : bamtercelboo@{gmail.com, 163.com}\n\n""""""\n    FILE :  Embed.py\n    FUNCTION : None\n""""""\n\nimport os\nimport sys\nimport time\nimport tqdm\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.init as init\nfrom collections import OrderedDict\nfrom DataUtils.Common import *\ntorch.manual_seed(seed_num)\nnp.random.seed(seed_num)\n\n\nclass Embed(object):\n    """"""\n    Embed\n    """"""\n    def __init__(self, path, words_dict, embed_type, pad):\n        self.embed_type_enum = [""zero"", ""avg"", ""uniform"", ""nn""]\n        self.path = path\n        self.words_dict = words_dict\n        self.embed_type = embed_type\n        self.pad = pad\n        # print(self.words_dict)\n        if not isinstance(self.words_dict, dict):\n            self.words_dict, self.words_list = self._list2dict(self.words_dict)\n        if pad is not None: self.padID = self.words_dict[pad]\n        # print(self.words_dict)\n        self.dim, self.words_count = self._get_dim(path=self.path), len(self.words_dict)\n        self.exact_count, self.fuzzy_count, self.oov_count = 0, 0, 0\n\n    def get_embed(self):\n        """"""\n        :return:\n        """"""\n        embed_dict = None\n        if self.embed_type in self.embed_type_enum:\n            embed_dict = self._read_file(path=self.path)\n        else:\n            print(""embed_type illegal, must be in {}"".format(self.embed_type_enum))\n            exit()\n        # print(embed_dict)\n        embed = None\n        if self.embed_type == ""nn"":\n            embed = self._nn_embed(embed_dict=embed_dict, words_dict=self.words_dict)\n        elif self.embed_type == ""zero"":\n            embed = self._zeros_embed(embed_dict=embed_dict, words_dict=self.words_dict)\n        elif self.embed_type == ""uniform"":\n            embed = self._uniform_embed(embed_dict=embed_dict, words_dict=self.words_dict)\n        elif self.embed_type == ""avg"":\n            embed = self._avg_embed(embed_dict=embed_dict, words_dict=self.words_dict)\n        # print(embed)\n        self.info()\n        return embed\n\n    def _zeros_embed(self, embed_dict, words_dict):\n        """"""\n        :param embed_dict:\n        :param words_dict:\n        """"""\n        print(""loading pre_train embedding by zeros for out of vocabulary."")\n        embeddings = np.zeros((int(self.words_count), int(self.dim)))\n        for word in words_dict:\n            if word in embed_dict:\n                embeddings[words_dict[word]] = np.array([float(i) for i in embed_dict[word]], dtype=\'float32\')\n                self.exact_count += 1\n            elif word.lower() in embed_dict:\n                embeddings[words_dict[word]] = np.array([float(i) for i in embed_dict[word.lower()]], dtype=\'float32\')\n                self.fuzzy_count += 1\n            else:\n                self.oov_count += 1\n        final_embed = torch.from_numpy(embeddings).float()\n        return final_embed\n\n    def _nn_embed(self, embed_dict, words_dict):\n        """"""\n        :param embed_dict:\n        :param words_dict:\n        """"""\n        print(""loading pre_train embedding by nn.Embedding for out of vocabulary."")\n        embed = nn.Embedding(int(self.words_count), int(self.dim))\n        init.xavier_uniform_(embed.weight.data)\n        embeddings = np.array(embed.weight.data)\n        for word in words_dict:\n            if word in embed_dict:\n                embeddings[words_dict[word]] = np.array([float(i) for i in embed_dict[word]], dtype=\'float32\')\n                self.exact_count += 1\n            elif word.lower() in embed_dict:\n                embeddings[words_dict[word]] = np.array([float(i) for i in embed_dict[word.lower()]], dtype=\'float32\')\n                self.fuzzy_count += 1\n            else:\n                self.oov_count += 1\n        embeddings[self.padID] = 0\n        final_embed = torch.from_numpy(embeddings).float()\n        return final_embed\n\n    def _uniform_embed(self, embed_dict, words_dict):\n        """"""\n        :param embed_dict:\n        :param words_dict:\n        """"""\n        print(""loading pre_train embedding by uniform for out of vocabulary."")\n        embeddings = np.zeros((int(self.words_count), int(self.dim)))\n        inword_list = {}\n        for word in words_dict:\n            if word in embed_dict:\n                embeddings[words_dict[word]] = np.array([float(i) for i in embed_dict[word]], dtype=\'float32\')\n                inword_list[words_dict[word]] = 1\n                self.exact_count += 1\n            elif word.lower() in embed_dict:\n                embeddings[words_dict[word]] = np.array([float(i) for i in embed_dict[word.lower()]], dtype=\'float32\')\n                inword_list[words_dict[word]] = 1\n                self.fuzzy_count += 1\n            else:\n                self.oov_count += 1\n        uniform_col = np.random.uniform(-0.25, 0.25, int(self.dim)).round(6)  # uniform\n        for i in range(len(words_dict)):\n            if i not in inword_list and i != self.padID:\n                embeddings[i] = uniform_col\n        final_embed = torch.from_numpy(embeddings).float()\n        return final_embed\n\n    def _avg_embed(self, embed_dict, words_dict):\n        """"""\n        :param embed_dict:\n        :param words_dict:\n        """"""\n        print(""loading pre_train embedding by avg for out of vocabulary."")\n        embeddings = np.zeros((int(self.words_count), int(self.dim)))\n        inword_list = {}\n        for word in words_dict:\n            if word in embed_dict:\n                embeddings[words_dict[word]] = np.array([float(i) for i in embed_dict[word]], dtype=\'float32\')\n                inword_list[words_dict[word]] = 1\n                self.exact_count += 1\n            elif word.lower() in embed_dict:\n                embeddings[words_dict[word]] = np.array([float(i) for i in embed_dict[word.lower()]], dtype=\'float32\')\n                inword_list[words_dict[word]] = 1\n                self.fuzzy_count += 1\n            else:\n                self.oov_count += 1\n        sum_col = np.sum(embeddings, axis=0) / len(inword_list)  # avg\n        for i in range(len(words_dict)):\n            if i not in inword_list and i != self.padID:\n                embeddings[i] = sum_col\n        final_embed = torch.from_numpy(embeddings).float()\n        return final_embed\n\n    @staticmethod\n    def _read_file(path):\n        """"""\n        :param path: embed file path\n        :return:\n        """"""\n        embed_dict = {}\n        with open(path, encoding=\'utf-8\') as f:\n            lines = f.readlines()\n            lines = tqdm.tqdm(lines)\n            for line in lines:\n                values = line.strip().split(\' \')\n                if len(values) == 1 or len(values) == 2 or len(values) == 3:\n                    continue\n                w, v = values[0], values[1:]\n                embed_dict[w] = v\n        return embed_dict\n\n    def info(self):\n        """"""\n        :return:\n        """"""\n        total_count = self.exact_count + self.fuzzy_count\n        print(""Words count {}, Embed dim {}."".format(self.words_count, self.dim))\n        print(""Exact count {} / {}"".format(self.exact_count, self.words_count))\n        print(""Fuzzy count {} / {}"".format(self.fuzzy_count, self.words_count))\n        print(""  INV count {} / {}"".format(total_count, self.words_count))\n        print(""  OOV count {} / {}"".format(self.oov_count, self.words_count))\n        print(""  OOV radio ===> {}%"".format(np.round((self.oov_count / self.words_count) * 100, 2)))\n        print(40 * ""*"")\n\n    @staticmethod\n    def _get_dim(path):\n        """"""\n        :param path:\n        :return:\n        """"""\n        embedding_dim = -1\n        with open(path, encoding=\'utf-8\') as f:\n            for line in f:\n                line_split = line.strip().split(\' \')\n                if len(line_split) == 1:\n                    embedding_dim = line_split[0]\n                    break\n                elif len(line_split) == 2:\n                    embedding_dim = line_split[1]\n                    break\n                else:\n                    embedding_dim = len(line_split) - 1\n                    break\n        return embedding_dim\n\n    @staticmethod\n    def _list2dict(convert_list):\n        """"""\n        :param convert_list:\n        :return:\n        """"""\n        list_dict = OrderedDict()\n        list_lower = []\n        for index, word in enumerate(convert_list):\n            list_lower.append(word.lower())\n            list_dict[word] = index\n        assert len(list_lower) == len(list_dict)\n        return list_dict, list_lower\n\n'"
DataUtils/Embed_From_Pretrained.py,5,"b'# @Author : bamtercelboo\n# @Datetime : 2018/2/3 14:03\n# @File : Embed_From_Pretrained.py\n# @Last Modify Time : 2018/2/3 14:03\n# @Contact : bamtercelboo@{gmail.com, 163.com}\n\n""""""\n    FILE :  Embed_From_Pretrained.py\n    FUNCTION : None\n""""""\n\nimport os\nimport sys\nimport torch\nimport torch.nn.init as init\nimport numpy as np\nimport random\nimport torch.nn as nn\nimport hyperparams as hy\ntorch.manual_seed(hy.seed_num)\nrandom.seed(hy.seed_num)\n\n\ndef Pretrain_Embed(file, vocab_size, words2id, unk, padding):\n\n    # print(""load pretrained embedding from {}"".format(file))\n    # f = open(file, encoding=\'utf-8\')\n    # allLines = f.readlines()\n    # indexs = set()\n    # info = allLines[0].strip().split(\' \')\n    # embed_dim = len(info) - 1\n    # emb = nn.Embedding(vocab_size, embed_dim)\n    #\n    # # init.uniform(emb.weight, a=-np.sqrt(3 / embed_dim), b=np.sqrt(3 / embed_dim))\n    # oov_emb = torch.zeros(1, embed_dim).type(torch.FloatTensor)\n    # now_line = 0\n    # for line in allLines:\n    #     now_line += 1\n    #     sys.stdout.write(""\\rhandling with the {} line."".format(now_line))\n    #     info = line.split("" "")\n    #     wordID = words2id[info[0]]\n    #     if wordID >= 0:\n    #         indexs.add(wordID)\n    #         for idx in range(embed_dim):\n    #             val = float(info[idx + 1])\n    #             emb.weight.data[wordID][idx] = val\n    #             # oov_emb[0][idx] += val\n    # f.close()\n    # print(""\\nhandle finished"")\n    #\n    # unkID = words2id[unk]\n    # paddingID = words2id[padding]\n    # for idx in range(embed_dim):\n    #     emb.weight.data[paddingID][idx] = 0\n    #     emb.weight.data[unkID][idx] = 0\n    #\n    # return emb, embed_dim\n\n    with open(file, encoding=""UTF-8"") as f:\n        allLines = f.readlines()\n        indexs = set()\n        info = allLines[0].strip().split(\' \')\n        embDim = len(info) - 1\n        emb = nn.Embedding(vocab_size, embDim)\n        # init.uniform(emb.weight, a=-np.sqrt(3 / embDim), b=np.sqrt(3 / embDim))\n        oov_emb = torch.zeros(1, embDim).type(torch.FloatTensor)\n\n        now_line = 0\n        for line in allLines:\n            now_line += 1\n            sys.stdout.write(""\\rHandling with the {} line."".format(now_line))\n            info = line.split(\' \')\n            wordID = words2id[info[0]]\n            if wordID >= 0:\n                indexs.add(wordID)\n                for idx in range(embDim):\n                    val = float(info[idx + 1])\n                    emb.weight.data[wordID][idx] = val\n                    oov_emb[0][idx] += val\n        f.close()\n    print(""\\nHandle Finished."")\n    count = len(indexs) + 1\n    for idx in range(embDim):\n        oov_emb[0][idx] /= count\n    unkID = words2id[unk]\n    paddingID = words2id[padding]\n    for idx in range(embDim):\n        emb.weight.data[paddingID][idx] = 0\n    if unkID != -1:\n        for idx in range(embDim):\n            emb.weight.data[unkID][idx] = oov_emb[0][idx]\n    print(""Load Embedding file: "", file, "", size: "", embDim)\n    oov = 0\n    for idx in range(vocab_size):\n        if idx not in indexs:\n            oov += 1\n    print(""oov: "", oov, "" total: "", vocab_size, ""oov ratio: "", oov / vocab_size)\n    print(""oov "", unk, ""use avg value initialize"")\n    return emb, embDim'"
DataUtils/Load_Pretrained_Embed.py,7,"b'# @Author : bamtercelboo\n# @Datetime : 2018/08/27 09.59\n# @File : Load_Pretrained_Embed.py\n# @Last Modify Time : 2018/08/27 09.59\n# @Contact : bamtercelboo@{gmail.com, 163.com}\n\n""""""\n    FILE :  Load_Pretrained_Embed.py\n    FUNCTION : loading pretrained word embedding\n""""""\nimport torch\nimport torch.nn as nn\nimport torch.nn.init as init\nfrom collections import OrderedDict\nimport numpy as np\nimport tqdm\n\nfrom DataUtils.Common import *\ntorch.manual_seed(seed_num)\nnp.random.seed(seed_num)\n\n\ndef load_pretrained_emb_zeros(path, text_field_words_dict, pad=None, set_padding=False):\n    print(""loading pre_train embedding by zeros......"")\n    if not isinstance(text_field_words_dict, dict):\n        text_field_words_dict = convert_list2dict(text_field_words_dict)\n    if pad is not None:\n        padID = text_field_words_dict[pad]\n    embedding_dim = -1\n    with open(path, encoding=\'utf-8\') as f:\n        for line in f:\n            line_split = line.strip().split(\' \')\n            if len(line_split) == 1:\n                embedding_dim = line_split[0]\n                break\n            elif len(line_split) == 2:\n                embedding_dim = line_split[1]\n                break\n            else:\n                embedding_dim = len(line_split) - 1\n                break\n    f.close()\n    word_count = len(text_field_words_dict)\n    print(\'The number of wordsDict is {} \\nThe dim of pretrained embedding is {}\'.format(str(word_count),\n                                                                                           str(embedding_dim)))\n    embeddings = np.zeros((int(word_count), int(embedding_dim)))\n    iv_num = 0\n    oov_num = 0\n    with open(path, encoding=\'utf-8\') as f:\n        lines = f.readlines()\n        # lines = tqdm.tqdm(lines)\n        for line in lines:\n            values = line.strip().split(\' \')\n            if len(values) == 1 or len(values) == 2:\n                continue\n            index = text_field_words_dict.get(values[0])  # digit or None\n            if index:\n                iv_num += 1\n                vector = np.array([float(i) for i in values[1:]], dtype=\'float32\')\n                embeddings[index] = vector\n\n    f.close()\n    oov_num = word_count - iv_num\n    print(""iv_num {} oov_num {} oov_radio {:.4f}%"".format(iv_num, oov_num, round((oov_num / word_count) * 100, 4)))\n    return torch.from_numpy(embeddings).float()\n\n\ndef load_pretrained_emb_Embedding(path, text_field_words_dict, pad=None, set_padding=False):\n    """"""\n    :param path:\n    :param text_field_words_dict:\n    :param pad:\n    :param set_padding:\n    :return:\n    """"""\n    print(""loading pre_train embedding by nn.Embedding......"")\n    if not isinstance(text_field_words_dict, dict):\n        text_field_words_dict, text_field_words_list = convert_list2dict(text_field_words_dict)\n    if pad is not None:\n        padID = text_field_words_dict[pad]\n    # print(text_field_words_dict)\n    embedding_dim = -1\n    with open(path, encoding=\'utf-8\') as f:\n        for line in f:\n            line_split = line.strip().split(\' \')\n            if len(line_split) == 1:\n                embedding_dim = line_split[0]\n                break\n            elif len(line_split) == 2:\n                embedding_dim = line_split[1]\n                break\n            else:\n                embedding_dim = len(line_split) - 1\n                break\n    f.close()\n    word_count = len(text_field_words_dict)\n    print(\'The number of wordsDict is {} \\nThe dim of pretrained embedding is {}\'.format(str(word_count),\n                                                                                         str(embedding_dim)))\n    embed = nn.Embedding(int(word_count), int(embedding_dim))\n    init.xavier_uniform(embed.weight.data)\n    embeddings = np.array(embed.weight.data)\n    iv_num = 0\n    fuzzy_num = 0\n    oov_num = 0\n    with open(path, encoding=\'utf-8\') as f:\n        lines = f.readlines()\n        lines = tqdm.tqdm(lines)\n        for line in lines:\n            values = line.strip().split(\' \')\n            if len(values) == 1 or len(values) == 2:\n                continue\n            word = values[0]\n            # if word in\n            index = text_field_words_dict.get(word, None)  # digit or None\n            if index is None:\n                if word.lower() in text_field_words_list:\n                    fuzzy_num += 1\n                    index = text_field_words_list.index(word.lower())\n            if index:\n                iv_num += 1\n                vector = np.array([float(i) for i in values[1:]], dtype=\'float32\')\n                embeddings[index] = vector\n            # else:\n            #     print(word)\n\n    f.close()\n    oov_num = word_count - iv_num\n    print(""iv_num {}(fuzzy_num = {}) oov_num {} oov_radio {:.4f}%"".format(iv_num, fuzzy_num, oov_num, round((oov_num / word_count) * 100, 4)))\n    return torch.from_numpy(embeddings).float()\n\n\ndef load_pretrained_emb_avg(path, text_field_words_dict, pad=None, set_padding=False):\n    print(""loading pre_train embedding by avg......"")\n    if not isinstance(text_field_words_dict, dict):\n        text_field_words_dict = convert_list2dict(text_field_words_dict)\n    assert pad is not None, ""pad not allow with None""\n    padID = text_field_words_dict[pad]\n    embedding_dim = -1\n    with open(path, encoding=\'utf-8\') as f:\n        for line in f:\n            line_split = line.strip().split(\' \')\n            if len(line_split) == 1:\n                embedding_dim = line_split[0]\n                break\n            elif len(line_split) == 2:\n                embedding_dim = line_split[1]\n                break\n            else:\n                embedding_dim = len(line_split) - 1\n                break\n    f.close()\n    word_count = len(text_field_words_dict)\n    print(\'The number of wordsDict is {} \\nThe dim of pretrained embedding is {}\\n\'.format(str(word_count),\n                                                                                           str(embedding_dim)))\n    embeddings = np.zeros((int(word_count), int(embedding_dim)))\n\n    inword_list = {}\n    with open(path, encoding=\'utf-8\') as f:\n        lines = f.readlines()\n        lines = tqdm.tqdm(lines)\n        for line in lines:\n            lines.set_description(""Processing"")\n            values = line.strip().split("" "")\n            if len(values) == 1 or len(values) == 2:\n                continue\n            index = text_field_words_dict.get(values[0])  # digit or None\n            if index:\n                vector = np.array([float(i) for i in values[1:]], dtype=\'float32\')\n                embeddings[index] = vector\n                inword_list[index] = 1\n    f.close()\n    print(""oov words initial by avg embedding, maybe take a while......"")\n    sum_col = np.sum(embeddings, axis=0) / len(inword_list)     # avg\n    for i in range(len(text_field_words_dict)):\n        if i not in inword_list and i != padID:\n            embeddings[i] = sum_col\n\n    OOVWords = word_count - len(inword_list)\n    oov_radio = np.round(OOVWords / word_count, 6)\n    print(""All Words = {}, InWords = {}, OOVWords = {}, OOV Radio={}"".format(\n        word_count, len(inword_list), OOVWords, oov_radio))\n\n    return torch.from_numpy(embeddings).float()\n\n\ndef load_pretrained_emb_uniform(path, text_field_words_dict, pad=None, set_padding=False):\n    print(""loading pre_train embedding by uniform......"")\n    if not isinstance(text_field_words_dict, dict):\n        text_field_words_dict = convert_list2dict(text_field_words_dict)\n    assert pad is not None, ""pad not allow with None""\n    padID = text_field_words_dict[pad]\n    embedding_dim = -1\n    with open(path, encoding=\'utf-8\') as f:\n        for line in f:\n            line_split = line.strip().split(\' \')\n            if len(line_split) == 1:\n                embedding_dim = line_split[0]\n                break\n            elif len(line_split) == 2:\n                embedding_dim = line_split[1]\n                break\n            else:\n                embedding_dim = len(line_split) - 1\n                break\n    f.close()\n    word_count = len(text_field_words_dict)\n    print(\'The number of wordsDict is {} \\nThe dim of pretrained embedding is {}\\n\'.format(str(word_count),\n                                                                                           str(embedding_dim)))\n    embeddings = np.zeros((int(word_count), int(embedding_dim)))\n\n    inword_list = {}\n    with open(path, encoding=\'utf-8\') as f:\n        lines = f.readlines()\n        lines = tqdm.tqdm(lines)\n        for line in lines:\n            lines.set_description(""Processing"")\n            values = line.strip().split("" "")\n            if len(values) == 1 or len(values) == 2:\n                continue\n            index = text_field_words_dict.get(values[0])  # digit or None\n            if index:\n                vector = np.array([float(i) for i in values[1:]], dtype=\'float32\')\n                embeddings[index] = vector\n                inword_list[index] = 1\n    f.close()\n    print(""oov words initial by uniform embedding, maybe take a while......"")\n    # sum_col = np.sum(embeddings, axis=0) / len(inword_list)     # avg\n    uniform_col = np.random.uniform(-0.25, 0.25, int(embedding_dim)).round(6)    # avg\n    for i in range(len(text_field_words_dict)):\n        if i not in inword_list and i != padID:\n            embeddings[i] = uniform_col\n\n    OOVWords = word_count - len(inword_list)\n    oov_radio = np.round(OOVWords / word_count, 6)\n    print(""All Words = {}, InWords = {}, OOVWords = {}, OOV Radio={}"".format(\n        word_count, len(inword_list), OOVWords, oov_radio))\n\n    return torch.from_numpy(embeddings).float()\n\n\ndef convert_list2dict(convert_list):\n    """"""\n    :param convert_list:  list type\n    :return:  dict type\n    """"""\n    list_dict = OrderedDict()\n    list_lower = []\n    for index, word in enumerate(convert_list):\n        list_lower.append(word.lower())\n        list_dict[word] = index\n    assert len(list_lower) == len(list_dict)\n    return list_dict, list_lower\n\n\n'"
DataUtils/Optim.py,9,"b'# -*- coding: utf-8 -*-\nimport torch.optim\nfrom torch.nn.utils.clip_grad import clip_grad_norm_\n\n# Setup optimizer (should always come after model.cuda())\n# iterable of dicts for per-param options where each dict\n# is {\'params\' : [p1, p2, p3...]}.update(generic optimizer args)\n# Example:\n# optim.SGD([\n        # {\'params\': model.base.parameters()},\n        # {\'params\': model.classifier.parameters(), \'lr\': 1e-3}\n    # ], lr=1e-2, momentum=0.9)\n\n\ndef decay_learning_rate(optimizer, epoch, init_lr, lr_decay):\n    """"""\xe8\xa1\xb0\xe5\x87\x8f\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\n\n    Args:\n        epoch: int, \xe8\xbf\xad\xe4\xbb\xa3\xe6\xac\xa1\xe6\x95\xb0\n        init_lr: \xe5\x88\x9d\xe5\xa7\x8b\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\n    """"""\n    lr = init_lr / (1 + lr_decay * epoch)\n    print(\'learning rate: {0}\'.format(lr))\n    for param_group in optimizer.param_groups:\n        param_group[\'lr\'] = lr\n    return optimizer\n\n\nclass Optimizer(object):\n    # Class dict to map lowercase identifiers to actual classes\n    methods = {\n        \'Adadelta\':   torch.optim.Adadelta,\n        \'Adagrad\':    torch.optim.Adagrad,\n        \'Adam\':       torch.optim.Adam,\n        \'SGD\':        torch.optim.SGD,\n        \'ASGD\':       torch.optim.ASGD,\n        \'Rprop\':      torch.optim.Rprop,\n        \'RMSprop\':    torch.optim.RMSprop,\n    }\n\n    @staticmethod\n    def get_params(model):\n        """"""Returns all name, parameter pairs with requires_grad=True.""""""\n        return list(\n            filter(lambda p: p[1].requires_grad, model.named_parameters()))\n\n    def __init__(self,\n                 name,\n                 model,\n                 lr=0,\n                 weight_decay=0,\n                 grad_clip=None,\n                 optim_args=None,\n                 momentum=None,\n                 **kwargs):\n        """"""\n        :param decay_method: Method of learning rate decay.\n\n        """"""\n\n        self.name = name\n        self.model = model\n        self.init_lr = lr\n        self.weight_decay = weight_decay\n        self.momentum = momentum\n        # self.gclip = grad_clip\n        self.gclip = None if grad_clip == ""None"" else float(grad_clip)\n        # print(self.gclip)\n\n        self._count = 0\n\n        # TODO:\n        # pass external optimizer configs\n        if optim_args is None:\n            optim_args = {}\n\n        self.optim_args = optim_args\n\n        # If an explicit lr given, pass it to torch optimizer\n        if self.init_lr > 0:\n            self.optim_args[\'lr\'] = self.init_lr\n\n        if self.name == ""SGD"" and self.momentum is not None:\n            self.optim_args[\'momentum\'] = self.momentum\n\n        # Get all parameters that require grads\n        self.named_params = self.get_params(self.model)\n\n        # Filter out names for gradient clipping\n        self.params = [param for (name, param) in self.named_params]\n\n        if self.weight_decay > 0:\n            weight_group = {\n                \'params\': [p for n, p in self.named_params if \'bias\' not in n],\n                \'weight_decay\': self.weight_decay,\n            }\n            bias_group = {\n                \'params\': [p for n, p in self.named_params if \'bias\' in n],\n            }\n            self.param_groups = [weight_group, bias_group]\n\n        # elif self.name == ""SGD"" and self.momentum is not None:\n\n\n        else:\n            self.param_groups = [{\'params\': self.params}]\n\n        # Safety check\n        n_params = len(self.params)\n        for group in self.param_groups:\n            n_params -= len(group[\'params\'])\n        assert n_params == 0, ""Not all params are passed to the optimizer.""\n\n        # Create the actual optimizer\n        self.optim = self.methods[self.name](self.param_groups,\n                                             **self.optim_args)\n\n        # Assign shortcuts\n        self.zero_grad = self.optim.zero_grad\n\n        # Skip useless if evaluation logic if gradient_clip not requested\n        if self.gclip == 0 or self.gclip is None:\n            self.step = self.optim.step\n\n    def zero_grad(self):\n        self.optim.zero_grad()\n\n    def step(self, closure=None):\n        """"""Gradient clipping aware step().""""""\n        if self.gclip is not None and self.gclip > 0:\n            # print(""aaaa"")\n            clip_grad_norm_(self.params, self.gclip)\n        self.optim.step(closure)\n\n    def rescale_lrate(self, scale, min_lrate=-1.0):\n        if isinstance(scale, list):\n            for scale_, group in zip(scale, self.optim.param_groups):\n                group[\'lr\'] = max(group[\'lr\'] * scale_, min_lrate)\n        else:\n            for group in self.optim.param_groups:\n                group[\'lr\'] = max(group[\'lr\'] * scale, min_lrate)\n\n    def get_lrate(self):\n        for group in self.optim.param_groups:\n            yield group[\'lr\']\n\n    def set_lrate(self, lr):\n        if isinstance(lr, list):\n            for lr_, group in zip(lr, self.optim.param_groups):\n                group[\'lr\'] = lr_\n        else:\n            for group in self.optim.param_groups:\n                group[\'lr\'] = lr\n\n    def __repr__(self):\n        s = ""Optimizer => {} (lr: {}, weight_decay: {}, g_clip: {})"".format(\n            self.name, self.init_lr, self.weight_decay, self.gclip)\n        return s'"
DataUtils/Pickle.py,0,"b'# @Author : bamtercelboo\n# @Datetime : 2018/8/23 12:26\n# @File : Pickle.py\n# @Last Modify Time : 2018/8/23 12:26\n# @Contact : bamtercelboo@{gmail.com, 163.com}\n\n""""""\n    FILE :  Pickle.py\n    FUNCTION : None\n""""""\n\n# Introduce python packages\nimport sys\nimport os\nimport pickle\n\n# Introduce missing packages in here\n\n\nclass Pickle(object):\n    def __init__(self):\n        print(""Pickle"")\n        self.obj_count = 0\n\n    @staticmethod\n    def save(obj, path, mode=""wb""):\n        """"""\n        :param obj:  obj dict to dump\n        :param path: save path\n        :param mode:  file mode\n        """"""\n        print(""save obj to {}"".format(path))\n        # print(""obj"", obj)\n        assert isinstance(obj, dict), ""The type of obj must be a dict type.""\n        if os.path.exists(path):\n            os.remove(path)\n        pkl_file = open(path, mode=mode)\n        pickle.dump(obj, pkl_file)\n        pkl_file.close()\n\n    @staticmethod\n    def load(path, mode=""rb""):\n        """"""\n        :param path:  pkl path\n        :param mode: file mode\n        :return: data dict\n        """"""\n        print(""load obj from {}"".format(path))\n        if os.path.exists(path) is False:\n            print(""Path {} illegal."".format(path))\n        pkl_file = open(path, mode=mode)\n        data = pickle.load(pkl_file)\n        pkl_file.close()\n        return data\n\n\npcl = Pickle\n\n\n\n\n'"
DataUtils/__init__.py,0,"b'# @Author : bamtercelboo\n# @Datetime : 2018/8/23 17:39\n# @File : __init__.py\n# @Last Modify Time : 2018/8/23 17:39\n# @Contact : bamtercelboo@{gmail.com, 163.com}\n\n""""""\n    FILE :  __init__.py\n    FUNCTION : None\n""""""'"
DataUtils/eval.py,0,"b'""""""\n    FILE :  eval.py\n    FUNCTION : eval prf for NER POS Chunking\n    REFERENCE : https://github.com/yunan4nlp\n""""""\n\n\nclass Eval:\n    def __init__(self):\n        self.predict_num = 0\n        self.correct_num = 0\n        self.gold_num = 0\n\n        self.precision = 0\n        self.recall = 0\n        self.fscore = 0\n\n    def clear_PRF(self):\n        self.predict_num = 0\n        self.correct_num = 0\n        self.gold_num = 0\n\n        self.precision = 0\n        self.recall = 0\n        self.fscore = 0\n\n    def getFscore(self):\n        if self.predict_num == 0:\n            self.precision = 0\n        else:\n            self.precision = (self.correct_num / self.predict_num) * 100\n\n        if self.gold_num == 0:\n            self.recall = 0\n        else:\n            self.recall = (self.correct_num / self.gold_num) * 100\n\n        if self.precision + self.recall == 0:\n            self.fscore = 0\n        else:\n            self.fscore = (2 * (self.precision * self.recall)) / (self.precision + self.recall)\n\n        return self.precision, self.recall, self.fscore\n\n    def acc(self):\n        return (self.correct_num / self.gold_num) * 100\n\n\nclass EvalPRF:\n\n    def evalPRF(self, predict_labels, gold_labels, eval):\n        gold_ent = self.get_ent(gold_labels)\n        predict_ent = self.get_ent(predict_labels)\n        # print(""\\npredict_labels"", predict_labels)\n        # print(""predict_ent"", predict_ent)\n        # print(""gold_labels"", gold_labels)\n        # print(""gold_ent"", gold_ent)\n        eval.predict_num += len(predict_ent)\n        eval.gold_num += len(gold_ent)\n\n        count = 0\n        for p in predict_ent:\n            if p in gold_ent:\n                count += 1\n                eval.correct_num += 1\n\n    def get_ent(self, labels):\n        idx = 0\n        idy = 0\n        endpos = -1\n        ent = []\n        while(idx < len(labels)):\n            if (self.is_start_label(labels[idx])):\n                idy = idx\n                endpos = -1\n                while(idy < len(labels)):\n                    if not self.is_continue_label(labels[idy], labels[idx], idy - idx):\n                        endpos = idy - 1\n                        break\n                    endpos = idy\n                    idy += 1\n                ent.append(self.cleanLabel(labels[idx]) + \'[\' + str(idx) + \',\' + str(endpos) + \']\')\n                idx = endpos\n            idx += 1\n        return ent\n\n    def cleanLabel(self, label):\n        start_label = [\'B\', \'b\', \'M\', \'m\', \'E\', \'e\', \'S\', \'s\', \'I\', \'i\']\n        if len(label) > 2 and label[1] == \'-\':\n            if label[0] in start_label:\n                return label[2:]\n        return label\n\n    def is_continue_label(self, label, startLabel, distance):\n        if distance == 0:\n            return True\n        if len(label) < 3:\n            return False\n        if distance != 0 and self.is_start_label(label):\n            return False\n        if (startLabel[0] == \'s\' or startLabel[0] == \'S\') and startLabel[1] == \'-\':\n            return False\n        if self.cleanLabel(label) != self.cleanLabel(startLabel):\n            return False\n        return True\n\n    def is_start_label(self, label):\n        start = [\'b\', \'B\', \'s\', \'S\']\n        # start = [\'b\', \'B\']\n        if len(label) < 3:\n            return False\n        else:\n            return (label[0] in start) and label[1] == \'-\'\n'"
DataUtils/eval_bio.py,0,"b""class Entity():\n    def __init__(self, start, end, category):\n        super(Entity, self).__init__()\n        self.start = start\n        self.end = end\n        self.category = category\n\n    def equal(self, entity):\n        return self.start == entity.start and self.end == entity.end and self.category == entity.category\n\n    def match(self, entity):\n        span = set(range(int(self.start), int(self.end) + 1))\n        entity_span = set(range(int(entity.start), int(entity.end) + 1))\n        return len(span.intersection(entity_span)) and self.category == entity.category\n\n    def propor_score(self, entity):\n        span = set(range(int(self.start), int(self.end) + 1))\n        entity_span = set(range(int(entity.start), int(entity.end) + 1))\n        return float(len(span.intersection(entity_span))) / float(len(span))\n\n\ndef Extract_entity(labels, category_set, prefix_array):\n    idx = 0\n    ent = []\n    while (idx < len(labels)):\n        if (is_start_label(labels[idx], prefix_array)):\n            idy = idx\n            endpos = -1\n            while (idy < len(labels)):\n                if not is_continue(labels[idy], labels[idx], prefix_array, idy - idx):\n                    endpos = idy - 1\n                    break\n                endpos = idy\n                idy += 1\n            category = cleanLabel(labels[idx], prefix_array)\n            # print(category)\n            entity = Entity(idx, endpos, category)\n            ent.append(entity)\n            idx = endpos\n        idx += 1\n    category_num = len(category_set)\n    category_list = [e for e in category_set]\n    # print(category_list)\n    entity_group = []\n    for i in range(category_num):\n        entity_group.append([])\n    # print(entity_group)\n    for id, c in enumerate(category_list):\n        for entity in ent:\n            if entity.category == c:\n                entity_group[id].append(entity)\n    return set(ent), entity_group\n\n\ndef is_start_label(label, prefix_array):\n    if len(label) < 3:\n        return False\n    return (label[0] in prefix_array[0]) and (label[1] == '-')\n\n\ndef is_continue(label, startLabel, prefix_array, distance):\n    if distance == 0:\n        return True\n    if len(label) < 3 or label == '<pad>' or label == '<start>':\n        return False\n    if distance != 0 and is_start_label(label, prefix_array):\n        return False\n    if (startLabel[0] == 's' or startLabel[0] == 'S') and startLabel[1] == '-':\n        return False\n    if (startLabel[0] == 'b' or startLabel[0] == 'B') and startLabel[1] == '-':\n        return False\n    if cleanLabel(label, prefix_array) != cleanLabel(startLabel, prefix_array):\n        return False\n    return True\n\n\ndef Extract_category(label2id, prefix_array):\n    prefix = [e for ele in prefix_array for e in ele]\n    category_list = []\n    for key in label2id:\n        if '-' in key:\n            category_list.append(cleanLabel(key, prefix))\n    # print(set(category))    # {'AGENT', 'TARGET', 'DSE'}\n    category_set = set(category_list)\n    return category_set\n\n\ndef cleanLabel(label, prefix_array):\n    prefix = [e for ele in prefix_array for e in ele]\n    if len(label) > 2 and label[1] == '-':\n        if label[0] in prefix:\n            return label[2:]\n    return label\n\n\ndef read_file(read_path):\n    file = open(read_path, 'r')\n    content = file.readlines()\n    labels = []\n    for i in range(2, len(content[0].strip().split('['))):\n        middle = content[0].strip().split('[')[i].strip()[:-2].split('\\'')\n        for e in middle:\n            if e == '' or e == ', ':\n                middle.remove(e)\n        labels.append([i for i in middle])\n    file.close()\n    return labels\n\n\ndef createAlphabet_labeler(label):\n    id2label = []\n    id2label.append('<start>')\n    for index in range(len(label)):\n        for w in label[index]:\n            if w not in id2label:\n                id2label.append(w)\n        id2label.append('<pad>')\n    id2label = set(id2label)\n    id2label = [e for e in id2label]\n    return id2label\n\n\nclass Eval():\n    def __init__(self, category_set, dataset_num):\n        self.category_set = category_set\n        self.dataset_sum = dataset_num\n\n        self.precision_c = []\n        self.recall_c = []\n        self.f1_score_c = []\n\n    def clear(self):\n        self.real_num = 0\n        self.predict_num = 0\n        self.correct_num = 0\n        self.correct_num_p = 0\n\n    def set_eval_var(self):\n        category_num = len(self.category_set)\n        self.B = []\n        b = list(range(4))\n        for i in range(category_num + 1):\n            bb = [0 for e in b]\n            self.B.append(bb)\n\n    def Exact_match(self, predict_set, gold_set):\n        self.clear()\n        self.gold_num = len(gold_set)\n        self.predict_num = len(predict_set)\n        # correct_num = 0\n        for p in predict_set:\n            for g in gold_set:\n                if p.equal(g):\n                    self.correct_num += 1\n                    break\n        result = (self.gold_num, self.predict_num, self.correct_num)\n        return result\n\n    def Binary_evaluate(self, predict_set, gold_set):\n        self.clear()\n        self.gold_num = len(gold_set)\n        self.predict_num = len(predict_set)\n        for p in predict_set:\n            for g in gold_set:\n                if p.match(g):\n                    self.correct_num_p += 1\n                    break\n        for g in gold_set:\n            for p in predict_set:\n                if g.match(p):\n                    self.correct_num += 1\n                    break\n        result = (self.gold_num, self.predict_num, self.correct_num, self.correct_num_p)\n        return result\n\n    def Propor_evaluate(self, predict_set, gold_set):\n        self.clear()\n        self.gold_num = len(gold_set)\n        self.predict_num = len(predict_set)\n        for p in predict_set:\n            for g in gold_set:\n                if p.match(g):\n                    self.correct_num_p += p.propor_score(g)\n                    break\n        for g in gold_set:\n            for p in predict_set:\n                if g.match(p):\n                    self.correct_num += g.propor_score(p)\n                    break\n        result = (self.gold_num, self.predict_num, self.correct_num, self.correct_num_p)\n        return result\n\n    def calc_f1_score(self, eval_type):\n        category_list = [e for e in self.category_set]\n        category_num = len(self.category_set)\n        if eval_type == 'exact':\n            result = self.get_f1_score_e(self.B[0][0], self.B[0][1], self.B[0][2])\n            precision = result[0]\n            recall = result[1]\n            f1_score = result[2]\n            for iter in range(category_num):\n                result = self.get_f1_score_e(self.B[iter + 1][0], self.B[iter + 1][1], self.B[iter + 1][2])\n                self.precision_c.append(result[0])\n                self.recall_c.append(result[1])\n                self.f1_score_c.append(result[2])\n        else:\n            result = self.get_f1_score(self.B[0][0], self.B[0][1], self.B[0][2], self.B[0][3])\n            precision = result[0]\n            recall = result[1]\n            f1_score = result[2]\n            for iter in range(category_num):\n                result = self.get_f1_score(self.B[iter + 1][0], self.B[iter + 1][1], self.B[iter + 1][2], self.B[iter + 1][3])\n                self.precision_c.append(result[0])\n                self.recall_c.append(result[1])\n                self.f1_score_c.append(result[2])\n\n        # print('\\n(The total number of Dev dataset: {})\\n'.format(self.dataset_sum))\n        # print('\\rEvalution - precision: {:.4f}% recall: {:.4f}% f1_score: {:.4f} ({}/{}/{})'.format((precision * 100),(recall * 100),f1_score,self.B[0][2], self.B[0][1],self.B[0][0]))\n        # for index in range(category_num):\n        #     print('\\r   {} - precision: {:.4f}% recall: {:.4f}% f1_score: {:.4f} ({}/{}/{})'.format(category_list[index],(self.precision_c[index] * 100),(self.recall_c[index] * 100),self.f1_score_c[index],self.B[index + 1][2],self.B[index+1][1],self.B[index + 1][0]))\n        #\n        return precision, recall, f1_score\n\n    def overall_evaluate(self, predict_set, gold_set, eval_type):\n        if eval_type == 'exact':\n            return self.Exact_match(predict_set, gold_set)\n        elif eval_type == 'binary':\n            return self.Binary_evaluate(predict_set, gold_set)\n        elif eval_type == 'propor':\n            return self.Propor_evaluate(predict_set, gold_set)\n\n    def eval(self, gold_labels, predict_labels, eval_type, prefix_array):\n        for index in range(len(gold_labels)):\n            gold_set, gold_entity_group = Extract_entity(gold_labels[index], self.category_set, prefix_array)\n            predict_set, pre_entity_group = Extract_entity(predict_labels[index], self.category_set, prefix_array)\n            result = self.overall_evaluate(predict_set, gold_set, eval_type)  # g,p,c\n            # print(result)\n            for i in range(len(result)):\n                self.B[0][i] += result[i]\n            for iter in range(len(self.category_set)):\n                result = self.overall_evaluate(pre_entity_group[iter], gold_entity_group[iter], eval_type)\n                for i in range(len(result)):\n                    self.B[iter + 1][i] += result[i]\n        # return self.B\n\n    def get_f1_score_e(self, real_num, predict_num, correct_num):\n        if predict_num != 0:\n            precision = correct_num / predict_num\n        else:\n            precision = 0.0\n        if real_num != 0:\n            recall = correct_num / real_num\n        else:\n            recall = 0.0\n        if precision + recall == 0:\n            f1_score = 0.0\n        else:\n            f1_score = 2 * precision * recall / (precision + recall)\n        result = (precision, recall, f1_score)\n        return result\n\n    def get_f1_score(self, real_num, predict_num, correct_num_r, correct_num_p):\n        if predict_num != 0:\n            precision = correct_num_p / predict_num\n        else:\n            precision = 0.0\n        if real_num != 0:\n            recall = correct_num_r / real_num\n        else:\n            recall = 0.0\n        if precision + recall == 0:\n            f1_score = 0\n        else:\n            f1_score = 2 * precision * recall / (precision + recall)\n        result = (precision, recall, f1_score)\n        return result\n\n\ndef entity_eval(eval_type):\n    prefix_array = [['b', 'B', 's', 'S'], ['m', 'M', 'e', 'E']]\n\n    # eval_type = 'exact'\n    gold_path = 'Gold_labels.txt'\n    predict_path = 'Predict_labels.txt'\n    gold_labels = read_file(gold_path)\n    predict_labels = read_file(predict_path)\n    label_list = createAlphabet_labeler(gold_labels)\n    category_set = Extract_category(label_list, prefix_array)\n    dataset_num = len(gold_labels)\n    evaluation = Eval(category_set, dataset_num)\n    evaluation.set_eval_var()\n    evaluation.eval(gold_labels, predict_labels, eval_type, prefix_array)\n    f1_score = evaluation.calc_f1_score(eval_type)\n    return f1_score\n\n\ndef entity_evalPRF_exact(gold_labels, predict_labels):\n    # prefix_array = [['b', 'B', 's', 'S'], ['m', 'M', 'e', 'E']]\n    prefix_array = [['b', 'B'], ['i', 'I']]\n    # prefix_array = [['i', 'I'], ['b', 'B']]\n\n    eval_type = 'exact'\n    label_list = createAlphabet_labeler(gold_labels)\n    category_set = Extract_category(label_list, prefix_array)\n    dataset_num = len(gold_labels)\n    evaluation = Eval(category_set, dataset_num)\n    evaluation.set_eval_var()\n    evaluation.eval(gold_labels, predict_labels, eval_type, prefix_array)\n    precision, recall, f1_score = evaluation.calc_f1_score(eval_type)\n    return precision, recall, f1_score\n\n\ndef entity_evalPRF_propor(gold_labels, predict_labels):\n    # prefix_array = [['b', 'B', 's', 'S'], ['m', 'M', 'e', 'E']]\n    prefix_array = [['b', 'B'], ['i', 'I']]\n    # prefix_array = [['i', 'I'], ['b', 'B']]\n\n    eval_type = 'propor'\n    label_list = createAlphabet_labeler(gold_labels)\n    category_set = Extract_category(label_list, prefix_array)\n    dataset_num = len(gold_labels)\n    evaluation = Eval(category_set, dataset_num)\n    evaluation.set_eval_var()\n    evaluation.eval(gold_labels, predict_labels, eval_type, prefix_array)\n    precision, recall, f1_score = evaluation.calc_f1_score(eval_type)\n    return precision, recall, f1_score\n\n\ndef entity_evalPRF_binary(gold_labels, predict_labels):\n    # prefix_array = [['b', 'B', 's', 'S'], ['m', 'M', 'e', 'E']]\n    prefix_array = [['b', 'B'], ['i', 'I']]\n    # prefix_array = [['i', 'I'], ['b', 'B']]\n\n    eval_type = 'binary'\n    label_list = createAlphabet_labeler(gold_labels)\n    category_set = Extract_category(label_list, prefix_array)\n    dataset_num = len(gold_labels)\n    evaluation = Eval(category_set, dataset_num)\n    evaluation.set_eval_var()\n    evaluation.eval(gold_labels, predict_labels, eval_type, prefix_array)\n    precision, recall, f1_score = evaluation.calc_f1_score(eval_type)\n    return precision, recall, f1_score\n\n"""
DataUtils/mainHelp.py,8,"b'# @Author : bamtercelboo\n# @Datetime : 2018/9/3 10:50\n# @File : mainHelp.py\n# @Last Modify Time : 2018/9/3 10:50\n# @Contact : bamtercelboo@{gmail.com, 163.com}\n\n""""""\n    FILE :  mainHelp.py\n    FUNCTION : None\n""""""\n\nimport shutil\nimport time\nfrom DataUtils.Alphabet import *\nfrom DataUtils.Batch_Iterator import *\nfrom DataUtils.Pickle import pcl\nfrom DataUtils.Embed import Embed\nfrom Dataloader.DataLoader_NER import DataLoader\n# from models.BiLSTM_Context import *\n# from models.BiLSTM import BiLSTM\nfrom models.Sequence_Label import Sequence_Label\nfrom test import load_test_model\n\n# solve default encoding problem\nfrom imp import reload\ndefaultencoding = \'utf-8\'\nif sys.getdefaultencoding() != defaultencoding:\n    reload(sys)\n    sys.setdefaultencoding(defaultencoding)\n\n# random seed\ntorch.manual_seed(seed_num)\nrandom.seed(seed_num)\n\n\ndef get_learning_algorithm(config):\n    """"""\n    :param config:  config\n    :return:  optimizer algorithm\n    """"""\n    algorithm = None\n    if config.adam is True:\n        algorithm = ""Adam""\n    elif config.sgd is True:\n        algorithm = ""SGD""\n    print(""learning algorithm is {}."".format(algorithm))\n    return algorithm\n\n\ndef get_params(config, alphabet):\n    """"""\n    :param config: config\n    :param alphabet:  alphabet dict\n    :return:\n    """"""\n    # get algorithm\n    config.learning_algorithm = get_learning_algorithm(config)\n\n    # save best model path\n    config.save_best_model_path = config.save_best_model_dir\n    if config.test is False:\n        if os.path.exists(config.save_best_model_path):\n            shutil.rmtree(config.save_best_model_path)\n\n    # get params\n    config.embed_num = alphabet.word_alphabet.vocab_size\n    config.char_embed_num = alphabet.char_alphabet.vocab_size\n    config.class_num = alphabet.label_alphabet.vocab_size\n    config.paddingId = alphabet.word_paddingId\n    config.char_paddingId = alphabet.char_paddingId\n    config.label_paddingId = alphabet.label_paddingId\n    config.create_alphabet = alphabet\n    print(""embed_num : {}, char_embed_num: {}, class_num : {}"".format(config.embed_num, config.char_embed_num,\n                                                                      config.class_num))\n    print(""PaddingID {}"".format(config.paddingId))\n    print(""char PaddingID {}"".format(config.char_paddingId))\n\n\ndef save_dict2file(dict, path):\n    """"""\n    :param dict:  dict\n    :param path:  path to save dict\n    :return:\n    """"""\n    print(""Saving dictionary"")\n    if os.path.exists(path):\n        print(""path {} is exist, deleted."".format(path))\n    file = open(path, encoding=""UTF-8"", mode=""w"")\n    for word, index in dict.items():\n        # print(word, index)\n        file.write(str(word) + ""\\t"" + str(index) + ""\\n"")\n    file.close()\n    print(""Save dictionary finished."")\n\n\ndef save_dictionary(config):\n    """"""\n    :param config: config\n    :return:\n    """"""\n    if config.save_dict is True:\n        if os.path.exists(config.dict_directory):\n            shutil.rmtree(config.dict_directory)\n        if not os.path.isdir(config.dict_directory):\n            os.makedirs(config.dict_directory)\n\n        config.word_dict_path = ""/"".join([config.dict_directory, config.word_dict])\n        config.label_dict_path = ""/"".join([config.dict_directory, config.label_dict])\n        print(""word_dict_path : {}"".format(config.word_dict_path))\n        print(""label_dict_path : {}"".format(config.label_dict_path))\n        save_dict2file(config.create_alphabet.word_alphabet.words2id, config.word_dict_path)\n        save_dict2file(config.create_alphabet.label_alphabet.words2id, config.label_dict_path)\n        # copy to mulu\n        print(""copy dictionary to {}"".format(config.save_dir))\n        shutil.copytree(config.dict_directory, ""/"".join([config.save_dir, config.dict_directory]))\n\n\n# load data / create alphabet / create iterator\ndef preprocessing(config):\n    """"""\n    :param config: config\n    :return:\n    """"""\n    print(""Processing Data......"")\n    # read file\n    data_loader = DataLoader(path=[config.train_file, config.dev_file, config.test_file], shuffle=True, config=config)\n    train_data, dev_data, test_data = data_loader.dataLoader()\n    print(""train sentence {}, dev sentence {}, test sentence {}."".format(len(train_data), len(dev_data), len(test_data)))\n    data_dict = {""train_data"": train_data, ""dev_data"": dev_data, ""test_data"": test_data}\n    if config.save_pkl:\n        torch.save(obj=data_dict, f=os.path.join(config.pkl_directory, config.pkl_data))\n\n    # create the alphabet\n    alphabet = None\n    if config.embed_finetune is False:\n        alphabet = CreateAlphabet(min_freq=config.min_freq, train_data=train_data, dev_data=dev_data, test_data=test_data, config=config)\n        alphabet.build_vocab()\n    if config.embed_finetune is True:\n        alphabet = CreateAlphabet(min_freq=config.min_freq, train_data=train_data, dev_data=dev_data, test_data=test_data, config=config)\n        # alphabet = CreateAlphabet(min_freq=config.min_freq, train_data=train_data, config=config)\n        alphabet.build_vocab()\n    alphabet_dict = {""alphabet"": alphabet}\n    if config.save_pkl:\n        torch.save(obj=alphabet_dict, f=os.path.join(config.pkl_directory, config.pkl_alphabet))\n\n    # create iterator\n    create_iter = Iterators(batch_size=[config.batch_size, config.dev_batch_size, config.test_batch_size],\n                            data=[train_data, dev_data, test_data], operator=alphabet, device=config.device,\n                            config=config)\n    train_iter, dev_iter, test_iter = create_iter.createIterator()\n    iter_dict = {""train_iter"": train_iter, ""dev_iter"": dev_iter, ""test_iter"": test_iter}\n    if config.save_pkl:\n        torch.save(obj=iter_dict, f=os.path.join(config.pkl_directory, config.pkl_iter))\n    return train_iter, dev_iter, test_iter, alphabet\n\n\ndef pre_embed(config, alphabet):\n    """"""\n    :param config: config\n    :param alphabet:  alphabet dict\n    :return:  pre-train embed\n    """"""\n    print(""***************************************"")\n    pretrain_embed = None\n    embed_types = """"\n    if config.pretrained_embed and config.zeros:\n        embed_types = ""zero""\n    elif config.pretrained_embed and config.avg:\n        embed_types = ""avg""\n    elif config.pretrained_embed and config.uniform:\n        embed_types = ""uniform""\n    elif config.pretrained_embed and config.nnembed:\n        embed_types = ""nn""\n    if config.pretrained_embed is True:\n        p = Embed(path=config.pretrained_embed_file, words_dict=alphabet.word_alphabet.id2words, embed_type=embed_types,\n                  pad=paddingkey)\n        pretrain_embed = p.get_embed()\n\n        embed_dict = {""pretrain_embed"": pretrain_embed}\n        # pcl.save(obj=embed_dict, path=os.path.join(config.pkl_directory, config.pkl_embed))\n        torch.save(obj=embed_dict, f=os.path.join(config.pkl_directory, config.pkl_embed))\n\n    return pretrain_embed\n\n\ndef load_model(config):\n    """"""\n    :param config:  config\n    :return:  nn model\n    """"""\n    print(""***************************************"")\n    model = Sequence_Label(config)\n    print(""Copy models to {}"".format(config.save_dir))\n    shutil.copytree(""models"", ""/"".join([config.save_dir, ""models""]))\n    if config.device != cpu_device:\n        model = model.cuda()\n    if config.test is True:\n        model = load_test_model(model, config)\n    print(model)\n    return model\n\n\ndef load_data(config):\n    """"""\n    :param config:  config\n    :return: batch data iterator and alphabet\n    """"""\n    print(""load data for process or pkl data."")\n    train_iter, dev_iter, test_iter = None, None, None\n    alphabet = None\n    start_time = time.time()\n    if (config.train is True) and (config.process is True):\n        print(""process data"")\n        if os.path.exists(config.pkl_directory): shutil.rmtree(config.pkl_directory)\n        if not os.path.isdir(config.pkl_directory): os.makedirs(config.pkl_directory)\n        train_iter, dev_iter, test_iter, alphabet = preprocessing(config)\n        config.pretrained_weight = pre_embed(config=config, alphabet=alphabet)\n    elif ((config.train is True) and (config.process is False)) or (config.test is True):\n        print(""load data from pkl file"")\n        # load alphabet from pkl\n        # alphabet_dict = pcl.load(path=os.path.join(config.pkl_directory, config.pkl_alphabet))\n        alphabet_dict = torch.load(f=os.path.join(config.pkl_directory, config.pkl_alphabet))\n        print(alphabet_dict.keys())\n        alphabet = alphabet_dict[""alphabet""]\n        # load iter from pkl\n        # iter_dict = pcl.load(path=os.path.join(config.pkl_directory, config.pkl_iter))\n        iter_dict = torch.load(f=os.path.join(config.pkl_directory, config.pkl_iter))\n        print(iter_dict.keys())\n        train_iter, dev_iter, test_iter = iter_dict.values()\n        # train_iter, dev_iter, test_iter = iter_dict[""train_iter""], iter_dict[""dev_iter""], iter_dict[""test_iter""]\n        # load embed from pkl\n        config.pretrained_weight = None\n        if os.path.exists(os.path.join(config.pkl_directory, config.pkl_embed)):\n            # embed_dict = pcl.load(os.path.join(config.pkl_directory, config.pkl_embed))\n            embed_dict = torch.load(f=os.path.join(config.pkl_directory, config.pkl_embed))\n            print(embed_dict.keys())\n            embed = embed_dict[""pretrain_embed""]\n            config.pretrained_weight = embed\n    end_time = time.time()\n    print(""All Data/Alphabet/Iterator Use Time {:.4f}"".format(end_time - start_time))\n    print(""***************************************"")\n    return train_iter, dev_iter, test_iter, alphabet\n\n\n\n'"
DataUtils/pickle_test.py,0,"b'# @Author : bamtercelboo\n# @Datetime : 2018/8/23 14:17\n# @File : pickle_test.py\n# @Last Modify Time : 2018/8/23 14:17\n# @Contact : bamtercelboo@{gmail.com, 163.com}\n\n""""""\n    FILE :  pickle_test.py\n    FUNCTION : None\n""""""\n\nimport pickle\n\nfrom Pickle import pcl\n\n\nclass P(object):\n    def __init__(self):\n        # print(""pickle"")\n        self.word2id = {""I"": 1, ""love"": 2, ""Beijing"": 3}\n        self.id2word = [1, 2, 3]\n        self.mincount = 3\n\n\nw = P()\nl = P()\nd = [w, l, 1, 2, 3, 4]\nprint(w, l)\npath = ""../alphabet.pkl""\n# obj_dict = {""w"": w, ""l"": l, ""d"": d}\n# pcl.save(obj=w, path=path)\ndata = pcl.load(path)\nprint(data)\n# print(data[""w""].word2id)\n# print(data[""l""].id2word)\n# print(data[""d""])\n\n# output = open(\'alphabet.pkl\', \'wb\')\n# # output = open(\'alphabet.txt\', \'w\')\n# # pickle.dumps(w)\n# # pickle.loads(w)\n# # print(w.word2id)\n# pickle.sa(w, output)\n# # pickle.dump(l, output, -1)\n# # pickle.dump(d, output, -1)\n# output.close()\n'"
DataUtils/tagSchemeConverter.py,0,"b'# -*- coding: utf-8 -*-\n# @Author: Jie Yang\n# @Date:   2017-11-27 16:53:36\n# @Last Modified by:   Jie Yang,     Contact: jieynlp@gmail.com\n# @Last Modified time: 2018-06-25 10:26:30\n\n\n""""""\n    convert NER/Chunking tag schemes, i.e. BIO->BIOES, BIOES->BIO, IOB->BIO, IOB->BIOES\n""""""\nfrom __future__ import print_function\n\nimport sys\n\n\ndef BIO2BIOES(input_file, output_file):\n    print(""Convert BIO -> BIOES for file:"", input_file)\n    with open(input_file, \'r\') as in_file:\n        fins = in_file.readlines()\n    fout = open(output_file,\'w\')\n    words = []\n    labels = []\n    for line in fins:\n        if len(line) < 3:\n            sent_len = len(words)\n            for idx in range(sent_len):\n                if ""-"" not in labels[idx]:\n                    fout.write(words[idx]+"" ""+labels[idx]+""\\n"")\n                else:\n                    label_type = labels[idx].split(\'-\')[-1]\n                    if ""B-"" in labels[idx]:\n                        if (idx == sent_len - 1) or (""I-"" not in labels[idx+1]):\n                            fout.write(words[idx]+"" S-""+label_type+""\\n"")\n                        else:\n                            fout.write(words[idx]+"" B-""+label_type+""\\n"")\n                    elif ""I-"" in labels[idx]:\n                        if (idx == sent_len - 1) or (""I-"" not in labels[idx+1]):\n                            fout.write(words[idx]+"" E-""+label_type+""\\n"")\n                        else:\n                            fout.write(words[idx]+"" I-""+label_type+""\\n"")\n            fout.write(\'\\n\')\n            words = []\n            labels = []\n        else:\n            pair = line.strip(\'\\n\').split()\n            words.append(pair[0])\n            labels.append(pair[-1].upper())\n    fout.close()\n    print(""BIOES file generated:"", output_file)\n\n\n\ndef BIOES2BIO(input_file, output_file):\n    print(""Convert BIOES -> BIO for file:"", input_file)\n    with open(input_file,\'r\') as in_file:\n        fins = in_file.readlines()\n    fout = open(output_file,\'w\')\n    words = []\n    labels = []\n    for line in fins:\n        if len(line) < 3:\n            sent_len = len(words)\n            for idx in range(sent_len):\n                if ""-"" not in labels[idx]:\n                    fout.write(words[idx]+"" ""+labels[idx]+""\\n"")\n                else:\n                    label_type = labels[idx].split(\'-\')[-1]\n                    if ""E-"" in labels[idx]:\n                        fout.write(words[idx]+"" I-""+label_type+""\\n"")\n                    elif ""S-"" in labels[idx]:\n                        fout.write(words[idx]+"" B-""+label_type+""\\n"")\n                    else:\n                        fout.write(words[idx]+"" ""+labels[idx]+""\\n"")     \n            fout.write(\'\\n\')\n            words = []\n            labels = []\n        else:\n            pair = line.strip(\'\\n\').split()\n            words.append(pair[0])\n            labels.append(pair[-1].upper())\n    fout.close()\n    print(""BIO file generated:"", output_file)\n\n\ndef IOB2BIO(input_file, output_file):\n    print(""Convert IOB -> BIO for file:"", input_file)\n    with open(input_file,\'r\') as in_file:\n        fins = in_file.readlines()\n    fout = open(output_file,\'w\')\n    words = []\n    labels = []\n    for line in fins:\n        if len(line) < 3:\n            sent_len = len(words)\n            for idx in range(sent_len):\n                if ""I-"" in labels[idx]:\n                    label_type = labels[idx].split(\'-\')[-1]\n                    if (idx == 0) or (labels[idx-1] == ""O"") or (label_type != labels[idx-1].split(\'-\')[-1]):\n                        fout.write(words[idx]+"" B-""+label_type+""\\n"")\n                    else:\n                        fout.write(words[idx]+"" ""+labels[idx]+""\\n"")\n                else:\n                    fout.write(words[idx]+"" ""+labels[idx]+""\\n"")\n            fout.write(\'\\n\')\n            words = []\n            labels = []\n        else:\n            pair = line.strip(\'\\n\').split()\n            words.append(pair[0])\n            labels.append(pair[-1].upper())\n    fout.close()\n    print(""BIO file generated:"", output_file)\n\n\ndef choose_label(input_file, output_file):\n    with open(input_file,\'r\') as in_file:\n        fins = in_file.readlines()\n    with open(output_file,\'w\') as fout:\n        for line in fins:\n            if len(line) < 3:\n                fout.write(line)\n            else:\n                pairs = line.strip(\'\\n\').split(\' \')\n                fout.write(pairs[0]+"" ""+ pairs[-1]+""\\n"")\n\n\nif __name__ == ""__main__"":\n    input_path = ""../Data/Conll2003_NER/train.txt""\n    output_path = ""../Data/Conll2003_BMES/train.txt""\n    BIO2BIOES(input_path, output_path)\n\n# if __name__ == \'__main__\':\n#     \'\'\'Convert NER tag schemes among IOB/BIO/BIOES.\n#         For example: if you want to convert the IOB tag scheme to BIO, then you run as following:\n#             python NERSchemeConverter.py IOB2BIO input_iob_file output_bio_file\n#         Input data format is the standard CoNLL 2003 data format.\n#     \'\'\'\n#     if sys.argv[1].upper() == ""IOB2BIO"":\n#         IOB2BIO(sys.argv[2],sys.argv[3])\n#     elif sys.argv[1].upper() == ""BIO2BIOES"":\n#         BIO2BIOES(sys.argv[2],sys.argv[3])\n#     elif sys.argv[1].upper() == ""BIOES2BIO"":\n#         BIOES2BIO(sys.argv[2],sys.argv[3])\n#     elif sys.argv[1].upper() == ""IOB2BIOES"":\n#         IOB2BIO(sys.argv[2],""temp"")\n#         BIO2BIOES(""temp"",sys.argv[3])\n#     else:\n#         print(""Argument error: sys.argv[1] should belongs to \\""IOB2BIO/BIO2BIOES/BIOES2BIO/IOB2BIOES\\"""")\n'"
DataUtils/utils.py,5,"b'# @Author : bamtercelboo\n# @Datetime : 2018/8/24 9:58\n# @File : utils.py\n# @Last Modify Time : 2018/8/24 9:58\n# @Contact : bamtercelboo@{gmail.com, 163.com}\n\n""""""\n    FILE :  utils.py\n    FUNCTION : None\n""""""\nimport sys\nimport os\nimport torch\nimport numpy as np\n\n\nclass Best_Result:\n    """"""\n        Best_Result\n    """"""\n    def __init__(self):\n        self.current_dev_score = -1\n        self.best_dev_score = -1\n        self.best_score = -1\n        self.best_epoch = 1\n        self.best_test = False\n        self.early_current_patience = 0\n        self.p = -1\n        self.r = -1\n        self.f = -1\n\n\ndef getMaxindex(model_out, label_size, args):\n    """"""\n    :param model_out: model output\n    :param label_size: label size\n    :param args: argument\n    :return: max index for predict\n    """"""\n    max = model_out.data[0]\n    maxIndex = 0\n    for idx in range(1, label_size):\n        if model_out.data[idx] > max:\n            max = model_out.data[idx]\n            maxIndex = idx\n    return maxIndex\n\n\ndef getMaxindex_np(model_out):\n    """"""\n    :param model_out: model output\n    :return: max index for predict\n    """"""\n    model_out_list = model_out.data.tolist()\n    maxIndex = model_out_list.index(np.max(model_out_list))\n    return maxIndex\n\n\ndef getMaxindex_batch(model_out):\n    """"""\n    :param model_out: model output\n    :return: max index for predict\n    """"""\n    model_out_list = model_out.data.tolist()\n    maxIndex_batch = []\n    for l in model_out_list:\n        maxIndex_batch.append(l.index(np.max(l)))\n\n    return maxIndex_batch\n\n\ndef torch_max(output):\n    """"""\n    :param output: batch * seq_len * label_num\n    :return:\n    """"""\n    # print(output)\n    batch_size = output.size(0)\n    _, arg_max = torch.max(output, dim=2)\n    # print(arg_max)\n    label = []\n    for i in range(batch_size):\n        label.append(arg_max[i].cpu().data.numpy())\n    return label\n\n\ndef save_model_all(model, save_dir, model_name, epoch):\n    """"""\n    :param model:  nn model\n    :param save_dir: save model direction\n    :param model_name:  model name\n    :param epoch:  epoch\n    :return:  None\n    """"""\n    if not os.path.isdir(save_dir):\n        os.makedirs(save_dir)\n    save_prefix = os.path.join(save_dir, model_name)\n    save_path = \'{}_epoch_{}.pt\'.format(save_prefix, epoch)\n    print(""save all model to {}"".format(save_path))\n    output = open(save_path, mode=""wb"")\n    torch.save(model.state_dict(), output)\n    # torch.save(model.state_dict(), save_path)\n    output.close()\n\n\ndef save_best_model(model, save_dir, model_name, best_eval):\n    """"""\n    :param model:  nn model\n    :param save_dir:  save model direction\n    :param model_name:  model name\n    :param best_eval:  eval best\n    :return:  None\n    """"""\n    if best_eval.current_dev_score >= best_eval.best_dev_score:\n        if not os.path.isdir(save_dir): os.makedirs(save_dir)\n        model_name = ""{}.pt"".format(model_name)\n        save_path = os.path.join(save_dir, model_name)\n        print(""save best model to {}"".format(save_path))\n        # if os.path.exists(save_path):  os.remove(save_path)\n        output = open(save_path, mode=""wb"")\n        torch.save(model.state_dict(), output)\n        # torch.save(model.state_dict(), save_path)\n        output.close()\n        best_eval.early_current_patience = 0\n\n\n# adjust lr\ndef get_lrate(optim):\n    """"""\n    :param optim: optimizer\n    :return:\n    """"""\n    for group in optim.param_groups:\n        yield group[\'lr\']\n\n\ndef set_lrate(optim, lr):\n    """"""\n    :param optim:  optimizer\n    :param lr:  learning rate\n    :return:\n    """"""\n    for group in optim.param_groups:\n        group[\'lr\'] = lr\n\n'"
Dataloader/DataLoader_NER.py,1,"b'# @Author : bamtercelboo\n# @Datetime : 2018/1/30 15:58\n# @File : DataConll2003_Loader.py\n# @Last Modify Time : 2018/1/30 15:58\n# @Contact : bamtercelboo@{gmail.com, 163.com}\n\n""""""\n    FILE :\n    FUNCTION :\n""""""\nimport os\nimport re\nimport random\nimport torch\nfrom Dataloader.Instance import Instance\n\nfrom DataUtils.Common import *\ntorch.manual_seed(seed_num)\nrandom.seed(seed_num)\n\n\nclass DataLoaderHelp(object):\n    """"""\n    DataLoaderHelp\n    """"""\n\n    @staticmethod\n    def _clean_str(string):\n        """"""\n        Tokenization/string cleaning for all datasets except for SST.\n        Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n        """"""\n        string = re.sub(r""[^A-Za-z0-9(),!?\\\'\\`]"", "" "", string)\n        string = re.sub(r""\\\'s"", "" \\\'s"", string)\n        string = re.sub(r""\\\'ve"", "" \\\'ve"", string)\n        string = re.sub(r""n\\\'t"", "" n\\\'t"", string)\n        string = re.sub(r""\\\'re"", "" \\\'re"", string)\n        string = re.sub(r""\\\'d"", "" \\\'d"", string)\n        string = re.sub(r""\\\'ll"", "" \\\'ll"", string)\n        string = re.sub(r"","", "" , "", string)\n        string = re.sub(r""!"", "" ! "", string)\n        string = re.sub(r""\\("", "" \\( "", string)\n        string = re.sub(r""\\)"", "" \\) "", string)\n        string = re.sub(r""\\?"", "" \\? "", string)\n        string = re.sub(r""\\s{2,}"", "" "", string)\n        return string.strip().lower()\n\n    @staticmethod\n    def _normalize_word(word):\n        """"""\n        :param word:\n        :return:\n        """"""\n        new_word = """"\n        for char in word:\n            if char.isdigit():\n                new_word += \'0\'\n            else:\n                new_word += char\n        return new_word\n\n    @staticmethod\n    def _sort(insts):\n        """"""\n        :param insts:\n        :return:\n        """"""\n        sorted_insts = []\n        sorted_dict = {}\n        for id_inst, inst in enumerate(insts):\n            sorted_dict[id_inst] = inst.words_size\n        dict = sorted(sorted_dict.items(), key=lambda d: d[1], reverse=True)\n        for key, value in dict:\n            sorted_insts.append(insts[key])\n        print(""Sort Finished."")\n        return sorted_insts\n\n    @staticmethod\n    def _write_shuffle_inst_to_file(insts, path):\n        """"""\n        :return:\n        """"""\n        w_path = ""."".join([path, shuffle])\n        if os.path.exists(w_path):\n            os.remove(w_path)\n        file = open(w_path, encoding=""UTF-8"", mode=""w"")\n        for id, inst in enumerate(insts):\n            for word, label in zip(inst.words, inst.labels):\n                file.write("" "".join([word, label, ""\\n""]))\n            file.write(""\\n"")\n        print(""write shuffle insts to file {}"".format(w_path))\n\n\nclass DataLoader(DataLoaderHelp):\n    """"""\n    DataLoader\n    """"""\n    def __init__(self, path, shuffle, config):\n        """"""\n        :param path: data path list\n        :param shuffle:  shuffle bool\n        :param config:  config\n        """"""\n        #\n        print(""Loading Data......"")\n        self.data_list = []\n        self.max_count = config.max_count\n        self.path = path\n        self.shuffle = shuffle\n        # char feature\n        self.pad_char = [char_pad, char_pad]\n        # self.pad_char = []\n        self.max_char_len = config.max_char_len\n\n    def dataLoader(self):\n        """"""\n        :return:\n        """"""\n        path = self.path\n        shuffle = self.shuffle\n        assert isinstance(path, list), ""Path Must Be In List""\n        print(""Data Path {}"".format(path))\n        for id_data in range(len(path)):\n            print(""Loading Data Form {}"".format(path[id_data]))\n            insts = self._Load_Each_Data(path=path[id_data], shuffle=shuffle)\n            random.shuffle(insts)\n            self._write_shuffle_inst_to_file(insts, path=path[id_data])\n            self.data_list.append(insts)\n        # return train/dev/test data\n        if len(self.data_list) == 3:\n            return self.data_list[0], self.data_list[1], self.data_list[2]\n        elif len(self.data_list) == 2:\n            return self.data_list[0], self.data_list[1]\n\n    def _Load_Each_Data(self, path=None, shuffle=False):\n        """"""\n        :param path:\n        :param shuffle:\n        :return:\n        """"""\n        assert path is not None, ""The Data Path Is Not Allow Empty.""\n        insts = []\n        with open(path, encoding=""UTF-8"") as f:\n            inst = Instance()\n            for line in f.readlines():\n                line = line.strip()\n                if line == """" and len(inst.words) != 0:\n                    inst.words_size = len(inst.words)\n                    insts.append(inst)\n                    inst = Instance()\n                else:\n                    line = line.strip().split("" "")\n                    word = line[0]\n                    char = self._add_char(word)\n                    word = self._normalize_word(word)\n                    inst.chars.append(char)\n                    inst.words.append(word)\n                    inst.labels.append(line[-1])\n                if len(insts) == self.max_count:\n                    break\n            if len(inst.words) != 0:\n                inst.words_size = len(inst.words)\n                insts.append(inst)\n            # print(""\\n"")\n        return insts\n\n    def _add_char(self, word):\n        """"""\n        :param word:\n        :return:\n        """"""\n        char = []\n        # char feature\n        for i in range(len(word)):\n            char.append(word[i])\n        if len(char) > self.max_char_len:\n            half = self.max_char_len // 2\n            word_half = word[:half] + word[-(self.max_char_len - half):]\n            char = word_half\n        else:\n            for i in range(self.max_char_len - len(char)):\n                char.append(char_pad)\n        return char\n\n\n'"
Dataloader/Instance.py,1,"b'# coding=utf-8\n# @Author : bamtercelboo\n# @Datetime : 2018/1/30 15:56\n# @File : Instance.py\n# @Last Modify Time : 2018/1/30 15:56\n# @Contact : bamtercelboo@{gmail.com, 163.com}\n\n""""""\n    FILE :  Instance.py\n    FUNCTION : Data Instance\n""""""\n\nimport torch\nimport random\n\nfrom DataUtils.Common import *\ntorch.manual_seed(seed_num)\nrandom.seed(seed_num)\n\n\nclass Instance:\n    """"""\n        Instance\n    """"""\n    def __init__(self):\n        self.words = []\n        self.chars = []\n        self.labels = []\n        self.words_size = 0\n        self.chars_size = 0\n\n        self.words_index = []\n        self.chars_index = []\n        self.label_index = []\n\n\n'"
models/BiLSTM.py,5,"b'# @Author : bamtercelboo\n# @Datetime : 2018/8/17 16:06\n# @File : BiLSTM.py\n# @Last Modify Time : 2018/8/17 16:06\n# @Contact : bamtercelboo@{gmail.com, 163.com}\n\n""""""\n    FILE :  BiLSTM.py\n    FUNCTION : None\n""""""\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\nimport random\nfrom DataUtils.Common import *\nfrom models.initialize import *\nfrom models.modelHelp import prepare_pack_padded_sequence\ntorch.manual_seed(seed_num)\nrandom.seed(seed_num)\n\n\nclass BiLSTM(nn.Module):\n    """"""\n        BiLSTM\n    """"""\n\n    def __init__(self, **kwargs):\n        super(BiLSTM, self).__init__()\n        for k in kwargs:\n            self.__setattr__(k, kwargs[k])\n\n        V = self.embed_num\n        D = self.embed_dim\n        C = self.label_num\n        paddingId = self.paddingId\n\n        self.embed = nn.Embedding(V, D, padding_idx=paddingId)\n\n        if self.pretrained_embed:\n            self.embed.weight.data.copy_(self.pretrained_weight)\n        else:\n            init_embedding(self.embed.weight)\n\n        self.dropout_embed = nn.Dropout(self.dropout_emb)\n        self.dropout = nn.Dropout(self.dropout)\n\n        self.bilstm = nn.LSTM(input_size=D, hidden_size=self.lstm_hiddens, num_layers=self.lstm_layers,\n                              bidirectional=True, batch_first=True, bias=True)\n\n        self.linear = nn.Linear(in_features=self.lstm_hiddens * 2, out_features=C, bias=True)\n        init_linear(self.linear)\n\n    def forward(self, word, sentence_length):\n        """"""\n        :param word:\n        :param sentence_length:\n        :param desorted_indices:\n        :return:\n        """"""\n        word, sentence_length, desorted_indices = prepare_pack_padded_sequence(word, sentence_length, device=self.device)\n        x = self.embed(word)  # (N,W,D)\n        x = self.dropout_embed(x)\n        packed_embed = pack_padded_sequence(x, sentence_length, batch_first=True)\n        x, _ = self.bilstm(packed_embed)\n        x, _ = pad_packed_sequence(x, batch_first=True)\n        x = x[desorted_indices]\n        x = self.dropout(x)\n        x = torch.tanh(x)\n        logit = self.linear(x)\n        return logit\n'"
models/BiLSTM_CNN.py,9,"b'# @Author : bamtercelboo\n# @Datetime : 2018/8/17 16:06\n# @File : BiLSTM.py\n# @Last Modify Time : 2018/8/17 16:06\n# @Contact : bamtercelboo@{gmail.com, 163.com}\n\n""""""\n    FILE :  BiLSTM.py\n    FUNCTION : None\n""""""\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\nimport random\nfrom models.initialize import *\nfrom DataUtils.Common import *\nfrom torch.nn import init\nfrom models.modelHelp import prepare_pack_padded_sequence\ntorch.manual_seed(seed_num)\nrandom.seed(seed_num)\n\n\nclass BiLSTM_CNN(nn.Module):\n    """"""\n        BiLSTM_CNN\n    """"""\n\n    def __init__(self, **kwargs):\n        super(BiLSTM_CNN, self).__init__()\n        for k in kwargs:\n            self.__setattr__(k, kwargs[k])\n\n        V = self.embed_num\n        D = self.embed_dim\n        C = self.label_num\n        paddingId = self.paddingId\n        char_paddingId = self.char_paddingId\n\n        # word embedding layer\n        self.embed = nn.Embedding(V, D, padding_idx=paddingId)\n        if self.pretrained_embed:\n            self.embed.weight.data.copy_(self.pretrained_weight)\n\n        # char embedding layer\n        self.char_embedding = nn.Embedding(self.char_embed_num, self.char_dim, padding_idx=char_paddingId)\n        # init_embedding(self.char_embedding.weight)\n        init_embed(self.char_embedding.weight)\n\n        # dropout\n        self.dropout_embed = nn.Dropout(self.dropout_emb)\n        self.dropout = nn.Dropout(self.dropout)\n\n        # cnn\n        # self.char_encoders = nn.ModuleList()\n        self.char_encoders = []\n        for i, filter_size in enumerate(self.conv_filter_sizes):\n            f = nn.Conv3d(in_channels=1, out_channels=self.conv_filter_nums[i], kernel_size=(1, filter_size, self.char_dim))\n            self.char_encoders.append(f)\n        for conv in self.char_encoders:\n            if self.device != cpu_device:\n                conv.cuda()\n        lstm_input_dim = D + sum(self.conv_filter_nums)\n        self.bilstm = nn.LSTM(input_size=lstm_input_dim, hidden_size=self.lstm_hiddens, num_layers=self.lstm_layers,\n                              bidirectional=True, batch_first=True, bias=True)\n\n        self.linear = nn.Linear(in_features=self.lstm_hiddens * 2, out_features=C, bias=True)\n        init_linear_weight_bias(self.linear)\n\n    def _char_forward(self, inputs):\n        """"""\n        Args:\n            inputs: 3D tensor, [bs, max_len, max_len_char]\n\n        Returns:\n            char_conv_outputs: 3D tensor, [bs, max_len, output_dim]\n        """"""\n        max_len, max_len_char = inputs.size(1), inputs.size(2)\n        inputs = inputs.view(-1, max_len * max_len_char)  # [bs, -1]\n        input_embed = self.char_embedding(inputs)  # [bs, ml*ml_c, feature_dim]\n        # input_embed = self.dropout_embed(input_embed)\n        # [bs, 1, max_len, max_len_char, feature_dim]\n        input_embed = input_embed.view(-1, 1, max_len, max_len_char, self.char_dim)\n        # conv\n        char_conv_outputs = []\n        for char_encoder in self.char_encoders:\n            conv_output = char_encoder(input_embed)\n            pool_output = torch.squeeze(torch.max(conv_output, -2)[0], -1)\n            char_conv_outputs.append(pool_output)\n        char_conv_outputs = torch.cat(char_conv_outputs, dim=1)\n        char_conv_outputs = char_conv_outputs.permute(0, 2, 1)\n\n        return char_conv_outputs\n\n    def forward(self, word, char, sentence_length):\n        """"""\n        :param char:\n        :param word:\n        :param sentence_length:\n        :return:\n        """"""\n        char_conv = self._char_forward(char)\n        char_conv = self.dropout(char_conv)\n        word = self.embed(word)  # (N,W,D)\n        x = torch.cat((word, char_conv), -1)\n        x = self.dropout_embed(x)\n        x, _ = self.bilstm(x)\n        x = self.dropout(x)\n        x = torch.tanh(x)\n        logit = self.linear(x)\n        return logit\n\n'"
models/CRF.py,24,"b'# @Author : bamtercelboo\n# @Datetime : 2018/9/14 9:51\n# @File : CRF.py\n# @Last Modify Time : 2018/9/14 9:51\n# @Contact : bamtercelboo@{gmail.com, 163.com}\n\n""""""\n    FILE :  CRF.py\n    FUNCTION : None\n    REFERENCE : https://github.com/jiesutd/NCRFpp/blob/master/model/crf.py\n""""""\nimport torch\nfrom torch.autograd.variable import Variable\nimport torch.nn as nn\n\n\ndef log_sum_exp(vec, m_size):\n    """"""\n    Args:\n        vec: size=(batch_size, vanishing_dim, hidden_dim)\n        m_size: hidden_dim\n\n    Returns:\n        size=(batch_size, hidden_dim)\n    """"""\n    _, idx = torch.max(vec, 1)  # B * 1 * M\n    max_score = torch.gather(vec, 1, idx.view(-1, 1, m_size)).view(-1, 1, m_size)  # B * M\n    return max_score.view(-1, m_size) + torch.log(torch.sum(\n        torch.exp(vec - max_score.expand_as(vec)), 1)).view(-1, m_size)\n\n\nclass CRF(nn.Module):\n    """"""\n        CRF\n    """"""\n    def __init__(self, **kwargs):\n        """"""\n        kwargs:\n            target_size: int, target size\n            device: str, device\n        """"""\n        super(CRF, self).__init__()\n        for k in kwargs:\n            self.__setattr__(k, kwargs[k])\n        device = self.device\n\n        # init transitions\n        self.START_TAG, self.STOP_TAG = -2, -1\n        init_transitions = torch.zeros(self.target_size + 2, self.target_size + 2, device=device)\n        init_transitions[:, self.START_TAG] = -10000.0\n        init_transitions[self.STOP_TAG, :] = -10000.0\n        self.transitions = nn.Parameter(init_transitions)\n\n    def _forward_alg(self, feats, mask):\n        """"""\n        Do the forward algorithm to compute the partition function (batched).\n\n        Args:\n            feats: size=(batch_size, seq_len, self.target_size+2)\n            mask: size=(batch_size, seq_len)\n\n        Returns:\n            xxx\n        """"""\n        batch_size = feats.size(0)\n        seq_len = feats.size(1)\n        tag_size = feats.size(2)\n        mask = mask.transpose(1, 0).contiguous()\n        ins_num = seq_len * batch_size\n        """""" be careful the view shape, it is .view(ins_num, 1, tag_size) but not .view(ins_num, tag_size, 1) """"""\n        feats = feats.transpose(1,0).contiguous().view(ins_num,1, tag_size).expand(ins_num, tag_size, tag_size)\n        """""" need to consider start """"""\n        scores = feats + self.transitions.view(1, tag_size, tag_size).expand(ins_num, tag_size, tag_size)\n        scores = scores.view(seq_len, batch_size, tag_size, tag_size)\n        # build iter\n        seq_iter = enumerate(scores)\n        _, inivalues = next(seq_iter)  # bat_size * from_target_size * to_target_size\n        """""" only need start from start_tag """"""\n        partition = inivalues[:, self.START_TAG, :].clone().view(batch_size, tag_size, 1)  # bat_size * to_target_size\n\n        """"""\n        add start score (from start to all tag, duplicate to batch_size)\n        partition = partition + self.transitions[START_TAG,:].view(1, tag_size, 1).expand(batch_size, tag_size, 1)\n        iter over last scores\n        """"""\n        for idx, cur_values in seq_iter:\n            """"""\n            previous to_target is current from_target\n            partition: previous results log(exp(from_target)), #(batch_size * from_target)\n            cur_values: bat_size * from_target * to_target\n            """"""\n            cur_values = cur_values + partition.contiguous().view(batch_size, tag_size, 1).expand(batch_size, tag_size, tag_size)\n            cur_partition = log_sum_exp(cur_values, tag_size)\n\n            mask_idx = mask[idx, :].view(batch_size, 1).expand(batch_size, tag_size)\n\n            """""" effective updated partition part, only keep the partition value of mask value = 1 """"""\n            masked_cur_partition = cur_partition.masked_select(mask_idx)\n            """""" let mask_idx broadcastable, to disable warning """"""\n            mask_idx = mask_idx.contiguous().view(batch_size, tag_size, 1)\n\n            """""" replace the partition where the maskvalue=1, other partition value keeps the same """"""\n            partition.masked_scatter_(mask_idx, masked_cur_partition)\n        """""" \n        until the last state, add transition score for all partition (and do log_sum_exp) \n        then select the value in STOP_TAG \n        """"""\n        cur_values = self.transitions.view(1, tag_size, tag_size).expand(batch_size, tag_size, tag_size) + partition.contiguous().view(batch_size, tag_size, 1).expand(batch_size, tag_size, tag_size)\n        cur_partition = log_sum_exp(cur_values, tag_size)\n        final_partition = cur_partition[:, self.STOP_TAG]\n        return final_partition.sum(), scores\n\n    def _viterbi_decode(self, feats, mask):\n        """"""\n            input:\n                feats: (batch, seq_len, self.tag_size+2)\n                mask: (batch, seq_len)\n            output:\n                decode_idx: (batch, seq_len) decoded sequence\n                path_score: (batch, 1) corresponding score for each sequence (to be implementated)\n        """"""\n        # print(feats.size())\n        batch_size = feats.size(0)\n        seq_len = feats.size(1)\n        tag_size = feats.size(2)\n        # assert(tag_size == self.tagset_size+2)\n        """""" calculate sentence length for each sentence """"""\n        length_mask = torch.sum(mask.long(), dim=1).view(batch_size, 1).long()\n        """""" mask to (seq_len, batch_size) """"""\n        mask = mask.transpose(1, 0).contiguous()\n        ins_num = seq_len * batch_size\n        """""" be careful the view shape, it is .view(ins_num, 1, tag_size) but not .view(ins_num, tag_size, 1) """"""\n        feats = feats.transpose(1,0).contiguous().view(ins_num, 1, tag_size).expand(ins_num, tag_size, tag_size)\n        """""" need to consider start """"""\n        scores = feats + self.transitions.view(1, tag_size, tag_size).expand(ins_num, tag_size, tag_size)\n        scores = scores.view(seq_len, batch_size, tag_size, tag_size)\n\n        # build iter\n        seq_iter = enumerate(scores)\n        # record the position of best score\n        back_points = list()\n        partition_history = list()\n        ##  reverse mask (bug for mask = 1- mask, use this as alternative choice)\n        # mask = 1 + (-1)*mask\n        mask = (1 - mask.long()).byte()\n        _, inivalues = next(seq_iter)  # bat_size * from_target_size * to_target_size\n        """""" only need start from start_tag """"""\n        partition = inivalues[:, self.START_TAG, :].clone().view(batch_size, tag_size)  # bat_size * to_target_size\n        partition_history.append(partition)\n        # iter over last scores\n        for idx, cur_values in seq_iter:\n            """"""\n            previous to_target is current from_target\n            partition: previous results log(exp(from_target)), #(batch_size * from_target)\n            cur_values: batch_size * from_target * to_target\n            """"""\n            cur_values = cur_values + partition.contiguous().view(batch_size, tag_size, 1).expand(batch_size, tag_size, tag_size)\n            """""" forscores, cur_bp = torch.max(cur_values[:,:-2,:], 1) # do not consider START_TAG/STOP_TAG """"""\n            partition, cur_bp = torch.max(cur_values, 1)\n            partition_history.append(partition)\n            """"""\n            cur_bp: (batch_size, tag_size) max source score position in current tag\n            set padded label as 0, which will be filtered in post processing\n            """"""\n            cur_bp.masked_fill_(mask[idx].view(batch_size, 1).expand(batch_size, tag_size), 0)\n            back_points.append(cur_bp)\n        """""" add score to final STOP_TAG """"""\n        partition_history = torch.cat(partition_history, 0).view(seq_len, batch_size, -1).transpose(1, 0).contiguous() ## (batch_size, seq_len. tag_size)\n        """""" get the last position for each setences, and select the last partitions using gather() """"""\n        last_position = length_mask.view(batch_size,1,1).expand(batch_size, 1, tag_size) -1\n        last_partition = torch.gather(partition_history, 1, last_position).view(batch_size,tag_size,1)\n        """""" calculate the score from last partition to end state (and then select the STOP_TAG from it) """"""\n        last_values = last_partition.expand(batch_size, tag_size, tag_size) + self.transitions.view(1,tag_size, tag_size).expand(batch_size, tag_size, tag_size)\n        _, last_bp = torch.max(last_values, 1)\n        pad_zero = torch.zeros(batch_size, tag_size, device=self.device, requires_grad=True).long()\n        back_points.append(pad_zero)\n        back_points = torch.cat(back_points).view(seq_len, batch_size, tag_size)\n\n        """""" elect end ids in STOP_TAG """"""\n        pointer = last_bp[:, self.STOP_TAG]\n        insert_last = pointer.contiguous().view(batch_size,1,1).expand(batch_size,1, tag_size)\n        back_points = back_points.transpose(1,0).contiguous()\n        """"""move the end ids(expand to tag_size) to the corresponding position of back_points to replace the 0 values """"""\n        back_points.scatter_(1, last_position, insert_last)\n        back_points = back_points.transpose(1,0).contiguous()\n        """""" decode from the end, padded position ids are 0, which will be filtered if following evaluation """"""\n        # decode_idx = Variable(torch.LongTensor(seq_len, batch_size))\n        decode_idx = torch.empty(seq_len, batch_size, device=self.device, requires_grad=True).long()\n        decode_idx[-1] = pointer.detach()\n        for idx in range(len(back_points)-2, -1, -1):\n            pointer = torch.gather(back_points[idx], 1, pointer.contiguous().view(batch_size, 1))\n            decode_idx[idx] = pointer.detach().view(batch_size)\n        path_score = None\n        decode_idx = decode_idx.transpose(1, 0)\n        return path_score, decode_idx\n\n    def forward(self, feats, mask):\n        """"""\n        :param feats:\n        :param mask:\n        :return:\n        """"""\n        path_score, best_path = self._viterbi_decode(feats, mask)\n        return path_score, best_path\n\n    def _score_sentence(self, scores, mask, tags):\n        """"""\n        Args:\n            scores: size=(seq_len, batch_size, tag_size, tag_size)\n            mask: size=(batch_size, seq_len)\n            tags: size=(batch_size, seq_len)\n\n        Returns:\n            score:\n        """"""\n        # print(scores.size())\n        batch_size = scores.size(1)\n        seq_len = scores.size(0)\n        tag_size = scores.size(-1)\n        tags = tags.view(batch_size, seq_len)\n        """""" convert tag value into a new format, recorded label bigram information to index """"""\n        # new_tags = Variable(torch.LongTensor(batch_size, seq_len))\n        new_tags = torch.empty(batch_size, seq_len, device=self.device, requires_grad=True).long()\n        for idx in range(seq_len):\n            if idx == 0:\n                new_tags[:, 0] = (tag_size - 2) * tag_size + tags[:, 0]\n            else:\n                new_tags[:, idx] = tags[:, idx-1] * tag_size + tags[:, idx]\n\n        """""" transition for label to STOP_TAG """"""\n        end_transition = self.transitions[:, self.STOP_TAG].contiguous().view(1, tag_size).expand(batch_size, tag_size)\n        """""" length for batch,  last word position = length - 1 """"""\n        length_mask = torch.sum(mask, dim=1).view(batch_size, 1).long()\n        """""" index the label id of last word """"""\n        end_ids = torch.gather(tags, 1, length_mask-1)\n\n        """""" index the transition score for end_id to STOP_TAG """"""\n        end_energy = torch.gather(end_transition, 1, end_ids)\n\n        """""" convert tag as (seq_len, batch_size, 1) """"""\n        new_tags = new_tags.transpose(1, 0).contiguous().view(seq_len, batch_size, 1)\n        """""" need convert tags id to search from 400 positions of scores """"""\n        tg_energy = torch.gather(scores.view(seq_len, batch_size, -1), 2, new_tags).view(seq_len, batch_size)\n        tg_energy = tg_energy.masked_select(mask.transpose(1, 0))\n\n        """"""\n        add all score together\n        gold_score = start_energy.sum() + tg_energy.sum() + end_energy.sum()\n        """"""\n        gold_score = tg_energy.sum() + end_energy.sum()\n\n        return gold_score\n\n    def neg_log_likelihood_loss(self, feats, mask, tags):\n        """"""\n        Args:\n            feats: size=(batch_size, seq_len, tag_size)\n            mask: size=(batch_size, seq_len)\n            tags: size=(batch_size, seq_len)\n        """"""\n        batch_size = feats.size(0)\n        forward_score, scores = self._forward_alg(feats, mask)\n        gold_score = self._score_sentence(scores, mask, tags)\n        return forward_score - gold_score\n\n\n\n'"
models/Sequence_Label.py,2,"b'# @Author : bamtercelboo\n# @Datetime : 2018/9/14 8:43\n# @File : Sequence_Label.py\n# @Last Modify Time : 2018/9/14 8:43\n# @Contact : bamtercelboo@{gmail.com, 163.com}\n\n""""""\n    FILE :  Sequence_Label.py\n    FUNCTION : None\n""""""\n\nimport torch\nimport torch.nn as nn\nimport random\nimport numpy as np\nimport time\nfrom models.BiLSTM import BiLSTM\nfrom models.BiLSTM_CNN import BiLSTM_CNN\nfrom models.CRF import CRF\nfrom DataUtils.Common import *\ntorch.manual_seed(seed_num)\nrandom.seed(seed_num)\n\n\nclass Sequence_Label(nn.Module):\n    """"""\n        Sequence_Label\n    """"""\n\n    def __init__(self, config):\n        super(Sequence_Label, self).__init__()\n        self.config = config\n        # embed\n        self.embed_num = config.embed_num\n        self.embed_dim = config.embed_dim\n        self.label_num = config.class_num\n        self.paddingId = config.paddingId\n        # dropout\n        self.dropout_emb = config.dropout_emb\n        self.dropout = config.dropout\n        # lstm\n        self.lstm_hiddens = config.lstm_hiddens\n        self.lstm_layers = config.lstm_layers\n        # pretrain\n        self.pretrained_embed = config.pretrained_embed\n        self.pretrained_weight = config.pretrained_weight\n        # char\n        self.use_char = config.use_char\n        self.char_embed_num = config.char_embed_num\n        self.char_paddingId = config.char_paddingId\n        self.char_dim = config.char_dim\n        self.conv_filter_sizes = self._conv_filter(config.conv_filter_sizes)\n        self.conv_filter_nums = self._conv_filter(config.conv_filter_nums)\n        assert len(self.conv_filter_sizes) == len(self.conv_filter_nums)\n        # print(self.conv_filter_nums)\n        # print(self.conv_filter_sizes)\n        # exit()\n        # use crf\n        self.use_crf = config.use_crf\n\n        # cuda or cpu\n        self.device = config.device\n\n        self.target_size = self.label_num if self.use_crf is False else self.label_num + 2\n\n        if self.use_char is True:\n            self.encoder_model = BiLSTM_CNN(embed_num=self.embed_num, embed_dim=self.embed_dim, label_num=self.target_size,\n                                            paddingId=self.paddingId, dropout_emb=self.dropout_emb, dropout=self.dropout,\n                                            lstm_hiddens=self.lstm_hiddens, lstm_layers=self.lstm_layers,\n                                            pretrained_embed=self.pretrained_embed, pretrained_weight=self.pretrained_weight,\n                                            char_embed_num=self.char_embed_num, char_dim=self.char_dim,\n                                            char_paddingId=self.char_paddingId, conv_filter_sizes=self.conv_filter_sizes,\n                                            conv_filter_nums=self.conv_filter_nums, device=self.device)\n        else:\n            self.encoder_model = BiLSTM(embed_num=self.embed_num, embed_dim=self.embed_dim, label_num=self.target_size,\n                                        paddingId=self.paddingId, dropout_emb=self.dropout_emb, dropout=self.dropout,\n                                        lstm_hiddens=self.lstm_hiddens, lstm_layers=self.lstm_layers,\n                                        pretrained_embed=self.pretrained_embed, pretrained_weight=self.pretrained_weight,\n                                        device=self.device)\n        if self.use_crf is True:\n            args_crf = dict({\'target_size\': self.label_num, \'device\': self.device})\n            self.crf_layer = CRF(**args_crf)\n\n    @staticmethod\n    def _conv_filter(str_list):\n        """"""\n        :param str_list:\n        :return:\n        """"""\n        int_list = []\n        str_list = str_list.split("","")\n        for str in str_list:\n            int_list.append(int(str))\n        return int_list\n\n    def forward(self, word, char, sentence_length, train=False):\n        """"""\n        :param char:\n        :param word:\n        :param sentence_length:\n        :param train:\n        :return:\n        """"""\n        if self.use_char is True:\n            encoder_output = self.encoder_model(word, char, sentence_length)\n            return encoder_output\n        else:\n            encoder_output = self.encoder_model(word, sentence_length)\n            return encoder_output\n\n\n'"
models/__init__.py,0,"b'# @Author : bamtercelboo\n# @Datetime : 2018/9/14 8:46\n# @File : __init__.py\n# @Last Modify Time : 2018/9/14 8:46\n# @Contact : bamtercelboo@{gmail.com, 163.com}\n\n""""""\n    FILE :  __init__.py\n    FUNCTION : None\n""""""'"
models/initialize.py,10,"b'# @Author : bamtercelboo\n# @Datetime : 2018/8/25 9:15\n# @File : initialize.py\n# @Last Modify Time : 2018/8/25 9:15\n# @Contact : bamtercelboo@{gmail.com, 163.com}\n\n""""""\n    FILE :  initialize.py\n    FUNCTION : None\n""""""\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\n\n\ndef init_cnn_weight(cnn_layer, seed=1337):\n    """"""\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96cnn\xe5\xb1\x82\xe6\x9d\x83\xe9\x87\x8d\n    Args:\n        cnn_layer: weight.size() == [nb_filter, in_channels, [kernel_size]]\n        seed: int\n    """"""\n    filter_nums = cnn_layer.weight.size(0)\n    kernel_size = cnn_layer.weight.size()[2:]\n    scope = np.sqrt(2. / (filter_nums * np.prod(kernel_size)))\n    torch.manual_seed(seed)\n    nn.init.normal_(cnn_layer.weight, -scope, scope)\n    cnn_layer.bias.data.zero_()\n\n\ndef init_cnn(cnn_layer, seed=1337):\n    """"""\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96cnn\xe5\xb1\x82\xe6\x9d\x83\xe9\x87\x8d\n    Args:\n        cnn_layer: weight.size() == [nb_filter, in_channels, [kernel_size]]\n        seed: int\n    """"""\n    filter_nums = cnn_layer.weight.size(0)\n    kernel_size = cnn_layer.weight.size()[2:]\n    scope = np.sqrt(2. / (filter_nums * np.prod(kernel_size)))\n    torch.manual_seed(seed)\n    nn.init.xavier_normal_(cnn_layer.weight)\n    cnn_layer.bias.data.uniform_(-scope, scope)\n\n\ndef init_lstm_weight(lstm, num_layer=1, seed=1337):\n    """"""\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96lstm\xe6\x9d\x83\xe9\x87\x8d\n    Args:\n        lstm: torch.nn.LSTM\n        num_layer: int, lstm\xe5\xb1\x82\xe6\x95\xb0\n        seed: int\n    """"""\n    for i in range(num_layer):\n        weight_h = getattr(lstm, \'weight_hh_l{0}\'.format(i))\n        scope = np.sqrt(6.0 / (weight_h.size(0)/4. + weight_h.size(1)))\n        torch.manual_seed(seed)\n        nn.init.uniform_(getattr(lstm, \'weight_hh_l{0}\'.format(i)), -scope, scope)\n\n        weight_i = getattr(lstm, \'weight_ih_l{0}\'.format(i))\n        scope = np.sqrt(6.0 / (weight_i.size(0)/4. + weight_i.size(1)))\n        torch.manual_seed(seed)\n        nn.init.uniform_(getattr(lstm, \'weight_ih_l{0}\'.format(i)), -scope, scope)\n\n    if lstm.bias:\n        for i in range(num_layer):\n            weight_h = getattr(lstm, \'bias_hh_l{0}\'.format(i))\n            weight_h.data.zero_()\n            weight_h.data[lstm.hidden_size: 2*lstm.hidden_size] = 1\n            weight_i = getattr(lstm, \'bias_ih_l{0}\'.format(i))\n            weight_i.data.zero_()\n            weight_i.data[lstm.hidden_size: 2*lstm.hidden_size] = 1\n\n\ndef init_linear(input_linear, seed=1337):\n    """"""\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe5\xb1\x82\xe6\x9d\x83\xe9\x87\x8d\n    """"""\n    torch.manual_seed(seed)\n    scope = np.sqrt(6.0 / (input_linear.weight.size(0) + input_linear.weight.size(1)))\n    nn.init.uniform_(input_linear.weight, -scope, scope)\n    # nn.init.uniform(input_linear.bias, -scope, scope)\n    if input_linear.bias is not None:\n        input_linear.bias.data.zero_()\n\n\ndef init_linear_weight_bias(input_linear, seed=1337):\n    """"""\n    :param input_linear:\n    :param seed:\n    :return:\n    """"""\n    torch.manual_seed(seed)\n    nn.init.xavier_uniform_(input_linear.weight)\n    scope = np.sqrt(6.0 / (input_linear.weight.size(0) + 1))\n    if input_linear.bias is not None:\n        input_linear.bias.data.uniform_(-scope, scope)\n\n\ndef init_embedding(input_embedding, seed=666):\n    """"""\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96embedding\xe5\xb1\x82\xe6\x9d\x83\xe9\x87\x8d\n    """"""\n    torch.manual_seed(seed)\n    scope = np.sqrt(3.0 / input_embedding.size(1))\n    nn.init.uniform_(input_embedding, -scope, scope)\n\n\ndef init_embed(input_embedding, seed=656):\n    """"""\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96embedding\xe5\xb1\x82\xe6\x9d\x83\xe9\x87\x8d\n    """"""\n    torch.manual_seed(seed)\n    nn.init.xavier_uniform_(input_embedding)\n\n\n'"
models/modelHelp.py,3,"b'# @Author : bamtercelboo\n# @Datetime : 2018/9/15 19:09\n# @File : modelHelp.py\n# @Last Modify Time : 2018/9/15 19:09\n# @Contact : bamtercelboo@{gmail.com, 163.com}\n\n""""""\n    FILE :  modelHelp.py\n    FUNCTION : None\n""""""\n\nimport torch\nimport random\nfrom DataUtils.Common import *\ntorch.manual_seed(seed_num)\nrandom.seed(seed_num)\n\n\ndef prepare_pack_padded_sequence(inputs_words, seq_lengths, device=""cpu"", descending=True):\n    """"""\n    :param device:\n    :param inputs_words:\n    :param seq_lengths:\n    :param descending:\n    :return:\n    """"""\n    sorted_seq_lengths, indices = torch.sort(torch.Tensor(seq_lengths).long(), descending=descending)\n    if device != cpu_device:\n        sorted_seq_lengths, indices = sorted_seq_lengths.cuda(), indices.cuda()\n    _, desorted_indices = torch.sort(indices, descending=False)\n    sorted_inputs_words = inputs_words[indices]\n    return sorted_inputs_words, sorted_seq_lengths.cpu().numpy(), desorted_indices\n\n\n'"
