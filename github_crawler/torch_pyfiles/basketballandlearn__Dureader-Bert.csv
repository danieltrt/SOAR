file_path,api_count,code
Dureader/args.py,1,"b'import torch\n\nseed = 42\ndevice = torch.device(""cuda"", 0)\ntest_lines = 187818  # \xe5\xa4\x9a\xe5\xb0\x91\xe6\x9d\xa1\xe8\xae\xad\xe7\xbb\x83\xe6\x95\xb0\xe6\x8d\xae\xef\xbc\x8c\xe5\x8d\xb3\xef\xbc\x9alen(features), \xe8\xae\xb0\xe5\xbe\x97\xe4\xbf\xae\xe6\x94\xb9 !!!!!!!!!!\n\nsearch_input_file = ""../data/extracted/trainset/search.train.json""\nzhidao_input_file = ""../data/extracted/trainset/zhidao.train.json""\ndev_zhidao_input_file = ""../data/extracted/devset/zhidao.dev.json""\ndev_search_input_file = ""../data/extracted/devset/search.dev.json""\n\nmax_seq_length = 512\nmax_query_length = 60\n\noutput_dir = ""./model_dir""\npredict_example_files=\'predict.data\'\n\nmax_para_num=5  # \xe9\x80\x89\xe6\x8b\xa9\xe5\x87\xa0\xe7\xaf\x87\xe6\x96\x87\xe6\xa1\xa3\xe8\xbf\x9b\xe8\xa1\x8c\xe9\xa2\x84\xe6\xb5\x8b\nlearning_rate = 5e-5\nbatch_size = 4\nnum_train_epochs = 4\ngradient_accumulation_steps = 8   # \xe6\xa2\xaf\xe5\xba\xa6\xe7\xb4\xaf\xe7\xa7\xaf\nnum_train_optimization_steps = int(test_lines / gradient_accumulation_steps / batch_size) * num_train_epochs\nlog_step = int(test_lines / batch_size / 4)  # \xe6\xaf\x8f\xe4\xb8\xaaepoch\xe9\xaa\x8c\xe8\xaf\x81\xe5\x87\xa0\xe6\xac\xa1\xef\xbc\x8c\xe9\xbb\x98\xe8\xae\xa44\xe6\xac\xa1\n'"
Dureader/evaluate.py,1,"b'import torch\nimport args\n\ndef evaluate(model, dev_data):\n    total, losses = 0.0, []\n    device = args.device\n\n    with torch.no_grad():\n        model.eval()\n        for batch in dev_data:\n\n            input_ids, input_mask,segment_ids, start_positions, end_positions = batch.input_ids, batch.input_mask, batch.segment_ids, batch.start_position, batch.end_position\n            loss, _, _ = model(input_ids.to(device), \\\n                                     segment_ids.to(device), input_mask.to(device), start_positions.to(device), end_positions.to(device))\n            loss = loss / args.gradient_accumulation_steps\n            losses.append(loss.item())\n\n        for i in losses:\n            total += i\n        with open(""./log"", \'a\') as f:\n            f.write(""eval_loss: "" + str(total / len(losses)) + ""\\n"")\n\n        return total / len(losses)\n\n\n\n'"
Dureader/optimizer.py,6,"b'import math\nimport torch\nfrom torch.optim import Optimizer\nfrom torch.optim.optimizer import required\nfrom torch.nn.utils import clip_grad_norm_\nimport logging\n\nlogger = logging.getLogger(__name__)\n\ndef warmup_cosine(x, warmup=0.002):\n    if x < warmup:\n        return x/warmup\n    return 0.5 * (1.0 + torch.cos(math.pi * x))\n\ndef warmup_constant(x, warmup=0.002):\n    """""" Linearly increases learning rate over `warmup`*`t_total` (as provided to BertAdam) training steps.\n        Learning rate is 1. afterwards. """"""\n    if x < warmup:\n        return x/warmup\n    return 1.0\n\ndef warmup_linear(x, warmup=0.002):\n    """""" Specifies a triangular learning rate schedule where peak is reached at `warmup`*`t_total`-th (as provided to BertAdam) training step.\n        After `t_total`-th training step, learning rate is zero. """"""\n    if x < warmup:\n        return x/warmup\n    return max((x-1.)/(warmup-1.), 0)\n\nSCHEDULES = {\n    \'warmup_cosine\':   warmup_cosine,\n    \'warmup_constant\': warmup_constant,\n    \'warmup_linear\':   warmup_linear,\n}\n\n\nclass BertAdam(Optimizer):\n    """"""Implements BERT version of Adam algorithm with weight decay fix.\n    Params:\n        lr: learning rate\n        warmup: portion of t_total for the warmup, -1  means no warmup. Default: -1\n        t_total: total number of training steps for the learning\n            rate schedule, -1  means constant learning rate. Default: -1\n        schedule: schedule to use for the warmup (see above). Default: \'warmup_linear\'\n        b1: Adams b1. Default: 0.9\n        b2: Adams b2. Default: 0.999\n        e: Adams epsilon. Default: 1e-6\n        weight_decay: Weight decay. Default: 0.01\n        max_grad_norm: Maximum norm for the gradients (-1 means no clipping). Default: 1.0\n    """"""\n    def __init__(self, params, lr=required, warmup=-1, t_total=-1, schedule=\'warmup_linear\',\n                 b1=0.9, b2=0.999, e=1e-6, weight_decay=0.01,\n                 max_grad_norm=1.0):\n        if lr is not required and lr < 0.0:\n            raise ValueError(""Invalid learning rate: {} - should be >= 0.0"".format(lr))\n        if schedule not in SCHEDULES:\n            raise ValueError(""Invalid schedule parameter: {}"".format(schedule))\n        if not 0.0 <= warmup < 1.0 and not warmup == -1:\n            raise ValueError(""Invalid warmup: {} - should be in [0.0, 1.0[ or -1"".format(warmup))\n        if not 0.0 <= b1 < 1.0:\n            raise ValueError(""Invalid b1 parameter: {} - should be in [0.0, 1.0["".format(b1))\n        if not 0.0 <= b2 < 1.0:\n            raise ValueError(""Invalid b2 parameter: {} - should be in [0.0, 1.0["".format(b2))\n        if not e >= 0.0:\n            raise ValueError(""Invalid epsilon value: {} - should be >= 0.0"".format(e))\n        defaults = dict(lr=lr, schedule=schedule, warmup=warmup, t_total=t_total,\n                        b1=b1, b2=b2, e=e, weight_decay=weight_decay,\n                        max_grad_norm=max_grad_norm)\n        super(BertAdam, self).__init__(params, defaults)\n\n    def get_lr(self):\n        lr = []\n        for group in self.param_groups:\n            for p in group[\'params\']:\n                state = self.state[p]\n                if len(state) == 0:\n                    return [0]\n                if group[\'t_total\'] != -1:\n                    schedule_fct = SCHEDULES[group[\'schedule\']]\n                    lr_scheduled = group[\'lr\'] * schedule_fct(state[\'step\']/group[\'t_total\'], group[\'warmup\'])\n                else:\n                    lr_scheduled = group[\'lr\']\n                lr.append(lr_scheduled)\n        return lr\n\n    def step(self, closure=None):\n        """"""Performs a single optimization step.\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        """"""\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        warned_for_t_total = False\n\n        for group in self.param_groups:\n            for p in group[\'params\']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data\n                if grad.is_sparse:\n                    raise RuntimeError(\'Adam does not support sparse gradients, please consider SparseAdam instead\')\n\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state[\'step\'] = 0\n                    # Exponential moving average of gradient values\n                    state[\'next_m\'] = torch.zeros_like(p.data)\n                    # Exponential moving average of squared gradient values\n                    state[\'next_v\'] = torch.zeros_like(p.data)\n\n                next_m, next_v = state[\'next_m\'], state[\'next_v\']\n                beta1, beta2 = group[\'b1\'], group[\'b2\']\n\n                # Add grad clipping\n                if group[\'max_grad_norm\'] > 0:\n                    clip_grad_norm_(p, group[\'max_grad_norm\'])\n\n                # Decay the first and second moment running average coefficient\n                # In-place operations to update the averages at the same time\n                next_m.mul_(beta1).add_(1 - beta1, grad)\n                next_v.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n                update = next_m / (next_v.sqrt() + group[\'e\'])\n\n                # Just adding the square of the weights to the loss function is *not*\n                # the correct way of using L2 regularization/weight decay with Adam,\n                # since that will interact with the m and v parameters in strange ways.\n                #\n                # Instead we want to decay the weights in a manner that doesn\'t interact\n                # with the m/v parameters. This is equivalent to adding the square\n                # of the weights to the loss with plain (non-momentum) SGD.\n                if group[\'weight_decay\'] > 0.0:\n                    update += group[\'weight_decay\'] * p.data\n\n                if group[\'t_total\'] != -1:\n                    schedule_fct = SCHEDULES[group[\'schedule\']]\n                    progress = state[\'step\']/group[\'t_total\']\n              #      if (state[\'step\']+1) % 4 ==0:\n                    lr_scheduled = group[\'lr\'] * schedule_fct(progress, group[\'warmup\'])\n                    # warning for exceeding t_total (only active with warmup_linear\n                    if group[\'schedule\'] == ""warmup_linear"" and progress > 1. and not warned_for_t_total:\n                        logger.warning(\n                            ""Training beyond specified \'t_total\' steps with schedule \'{}\'. Learning rate set to {}. ""\n                            ""Please set \'t_total\' of {} correctly."".format(group[\'schedule\'], lr_scheduled, self.__class__.__name__))\n                        warned_for_t_total = True\n                    # end warning\n                else:\n                    lr_scheduled = group[\'lr\']\n\n                update_with_lr = lr_scheduled * update\n                p.data.add_(-update_with_lr)\n\n                state[\'step\'] += 1\n\n                # step_size = lr_scheduled * math.sqrt(bias_correction2) / bias_correction1\n                # No bias correction\n                # bias_correction1 = 1 - beta1 ** state[\'step\']\n                # bias_correction2 = 1 - beta2 ** state[\'step\']\n\n        return loss\n'"
Dureader/train.py,2,"b'import os\nimport args\nimport torch\nimport random\nimport pickle\nfrom tqdm import tqdm\nfrom torch import nn, optim\n\nimport evaluate\nfrom optimizer import BertAdam\nfrom dataset.dataloader import Dureader\nfrom dataset.file_utils import PYTORCH_PRETRAINED_BERT_CACHE\nfrom model_dir.modeling import BertForQuestionAnswering, BertConfig\n\n# \xe9\x9a\x8f\xe6\x9c\xba\xe7\xa7\x8d\xe5\xad\x90\nrandom.seed(args.seed)\ntorch.manual_seed(args.seed)\n\ndef train():\n    # \xe5\x8a\xa0\xe8\xbd\xbd\xe9\xa2\x84\xe8\xae\xad\xe7\xbb\x83bert\n    model = BertForQuestionAnswering.from_pretrained(""bert-base-chinese"",\n                    cache_dir=os.path.join(str(PYTORCH_PRETRAINED_BERT_CACHE), \'distributed_{}\'.format(-1)))\n    device = args.device\n    model.to(device)\n\n    # \xe5\x87\x86\xe5\xa4\x87 optimizer\n    param_optimizer = list(model.named_parameters())\n    param_optimizer = [n for n in param_optimizer if \'pooler\' not in n[0]]\n    no_decay = [\'bias\', \'LayerNorm.bias\', \'LayerNorm.weight\']\n    optimizer_grouped_parameters = [\n            {\'params\': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], \'weight_decay\': 0.01},\n            {\'params\': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], \'weight_decay\': 0.0}\n            ]\n    optimizer = BertAdam(optimizer_grouped_parameters, lr=args.learning_rate, warmup=0.1, t_total=args.num_train_optimization_steps)\n\n    # \xe5\x87\x86\xe5\xa4\x87\xe6\x95\xb0\xe6\x8d\xae\n    data = Dureader()\n    train_dataloader, dev_dataloader = data.train_iter, data.dev_iter\n\n    best_loss = 100000.0\n    model.train()\n    for i in range(args.num_train_epochs):\n        for step , batch in enumerate(tqdm(train_dataloader, desc=""Epoch"")):\n            input_ids, input_mask, segment_ids, start_positions, end_positions = \\\n                                        batch.input_ids, batch.input_mask, batch.segment_ids, batch.start_position, batch.end_position\n            input_ids, input_mask, segment_ids, start_positions, end_positions = \\\n                                        input_ids.to(device), input_mask.to(device), segment_ids.to(device), start_positions.to(device), end_positions.to(device)\n\n            # \xe8\xae\xa1\xe7\xae\x97loss\n            loss, _, _ = model(input_ids, token_type_ids=segment_ids, attention_mask=input_mask, start_positions=start_positions, end_positions=end_positions)\n            loss = loss / args.gradient_accumulation_steps\n            loss.backward()\n\n            # \xe6\x9b\xb4\xe6\x96\xb0\xe6\xa2\xaf\xe5\xba\xa6\n            if (step+1) % args.gradient_accumulation_steps == 0:\n                optimizer.step()\n                optimizer.zero_grad()\n\n            # \xe9\xaa\x8c\xe8\xaf\x81\n            if step % args.log_step == 4:\n                eval_loss = evaluate.evaluate(model, dev_dataloader)\n                if eval_loss < best_loss:\n                    best_loss = eval_loss\n                    torch.save(model.state_dict(), \'./model_dir/\' + ""best_model"")\n                    model.train()\n\nif __name__ == ""__main__"":\n    train()\n'"
Dureader/dataset/__init__.py,0,b''
Dureader/dataset/args.py,1,"b'import torch\n\nseed = 42\ndevice = torch.device(""cuda"", 0)\ntest_lines = 187818  # \xe5\xa4\x9a\xe5\xb0\x91\xe6\x9d\xa1\xe8\xae\xad\xe7\xbb\x83\xe6\x95\xb0\xe6\x8d\xae\xef\xbc\x8c\xe5\x8d\xb3\xef\xbc\x9alen(features)\n\nsearch_input_file = ""../data/extracted/trainset/search.train.json""\nzhidao_input_file = ""../data/extracted/trainset/zhidao.train.json""\ndev_zhidao_input_file = ""../data/extracted/devset/zhidao.dev.json""\ndev_search_input_file = ""../data/extracted/devset/search.dev.json""\n\nmax_seq_length = 512\nmax_query_length = 60\n\noutput_dir = ""./model_dir""\npredict_example_files=\'predict.data\'\n\nmax_para_num=5  # \xe9\x80\x89\xe6\x8b\xa9\xe5\x87\xa0\xe7\xaf\x87\xe6\x96\x87\xe6\xa1\xa3\xe8\xbf\x9b\xe8\xa1\x8c\xe9\xa2\x84\xe6\xb5\x8b\nlearning_rate = 5e-5\nbatch_size = 4\nnum_train_epochs = 4\ngradient_accumulation_steps = 8   # \xe6\xa2\xaf\xe5\xba\xa6\xe7\xb4\xaf\xe7\xa7\xaf\nnum_train_optimization_steps = int(test_lines / gradient_accumulation_steps / batch_size) * num_train_epochs\nlog_step = int(test_lines / batch_size / 4)  # \xe6\xaf\x8f\xe4\xb8\xaaepoch\xe9\xaa\x8c\xe8\xaf\x81\xe5\x87\xa0\xe6\xac\xa1\xef\xbc\x8c\xe9\xbb\x98\xe8\xae\xa44\xe6\xac\xa1\n'"
Dureader/dataset/dataloader.py,1,"b'import args\nimport torchtext\nfrom torchtext import data\nfrom torch.utils.data import DataLoader\n\ndef x_tokenize(ids):\n    return [int(i) for i in ids]\ndef y_tokenize(y):\n    return int(y)\n\nclass Dureader():\n    def __init__(self, path=\'./dataset\'):\n\n        self.WORD = torchtext.data.Field(batch_first=True, sequential=True, tokenize=x_tokenize,\n                               use_vocab=False, pad_token=0)\n        self.LABEL = torchtext.data.Field(sequential=False,tokenize=y_tokenize, use_vocab=False)\n\n        dict_fields = {\'input_ids\': (\'input_ids\', self.WORD),\n                       \'input_mask\': (\'input_mask\', self.WORD),\n                       \'segment_ids\': (\'segment_ids\', self.WORD),\n                       \'start_position\': (\'start_position\', self.LABEL),\n                       \'end_position\': (\'end_position\', self.LABEL) }\n\n        self.train, self.dev = torchtext.data.TabularDataset.splits(\n                path=path,\n                train=""train.data"",\n                validation=""dev.data"",\n                format=\'json\',\n                fields=dict_fields)\n        self.train_iter, self.dev_iter = torchtext.data.BucketIterator.splits(\n                                                                    [self.train, self.dev],  batch_size=args.batch_size,\n                                                                    sort_key=lambda x: len(x.input_ids) ,sort_within_batch=True, shuffle=True)\n'"
Dureader/dataset/file_utils.py,0,"b'import json\nimport logging\nimport os\nimport shutil\nimport tempfile\nfrom functools import wraps\nfrom hashlib import sha256\nimport sys\nfrom io import open\n\nimport requests\nfrom tqdm import tqdm\n\nfrom urllib.parse import urlparse\n\ntry:\n    from pathlib import Path\n    PYTORCH_PRETRAINED_BERT_CACHE = Path(os.getenv(\'PYTORCH_PRETRAINED_BERT_CACHE\',\n                                                   Path.home() / \'.pytorch_pretrained_bert\'))\nexcept (AttributeError, ImportError):\n    PYTORCH_PRETRAINED_BERT_CACHE = os.getenv(\'PYTORCH_PRETRAINED_BERT_CACHE\',\n                                              os.path.join(os.path.expanduser(""~""), \'.pytorch_pretrained_bert\'))\n\nlogger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n\ndef url_to_filename(url, etag=None):\n    """"""\n    Convert `url` into a hashed filename in a repeatable way.\n    If `etag` is specified, append its hash to the url\'s, delimited\n    by a period.\n    """"""\n    url_bytes = url.encode(\'utf-8\')\n    url_hash = sha256(url_bytes)\n    filename = url_hash.hexdigest()\n\n    if etag:\n        etag_bytes = etag.encode(\'utf-8\')\n        etag_hash = sha256(etag_bytes)\n        filename += \'.\' + etag_hash.hexdigest()\n\n    return filename\n\n\ndef filename_to_url(filename, cache_dir=None):\n    """"""\n    Return the url and etag (which may be ``None``) stored for `filename`.\n    Raise ``EnvironmentError`` if `filename` or its stored metadata do not exist.\n    """"""\n    if cache_dir is None:\n        cache_dir = PYTORCH_PRETRAINED_BERT_CACHE\n    if sys.version_info[0] == 3 and isinstance(cache_dir, Path):\n        cache_dir = str(cache_dir)\n\n    cache_path = os.path.join(cache_dir, filename)\n    if not os.path.exists(cache_path):\n        raise EnvironmentError(""file {} not found"".format(cache_path))\n\n    meta_path = cache_path + \'.json\'\n    if not os.path.exists(meta_path):\n        raise EnvironmentError(""file {} not found"".format(meta_path))\n\n    with open(meta_path, encoding=""utf-8"") as meta_file:\n        metadata = json.load(meta_file)\n    url = metadata[\'url\']\n    etag = metadata[\'etag\']\n\n    return url, etag\n\ndef cached_path(url_or_filename, cache_dir=None):\n    """"""\n    Given something that might be a URL (or might be a local path),\n    determine which. If it\'s a URL, download the file and cache it, and\n    return the path to the cached file. If it\'s already a local path,\n    make sure the file exists and then return the path.\n    """"""\n    if cache_dir is None:\n        cache_dir = PYTORCH_PRETRAINED_BERT_CACHE\n    if sys.version_info[0] == 3 and isinstance(url_or_filename, Path):\n        url_or_filename = str(url_or_filename)\n    if sys.version_info[0] == 3 and isinstance(cache_dir, Path):\n        cache_dir = str(cache_dir)\n\n    parsed = urlparse(url_or_filename)\n\n    if parsed.scheme in (\'http\', \'https\', \'s3\'):\n        # URL, so get it from the cache (downloading if necessary)\n        return get_from_cache(url_or_filename, cache_dir)\n    elif os.path.exists(url_or_filename):\n        # File, and it exists.\n        return url_or_filename\n    elif parsed.scheme == \'\':\n        # File, but it doesn\'t exist.\n        raise EnvironmentError(""file {} not found"".format(url_or_filename))\n    else:\n        # Something unknown\n        raise ValueError(""unable to parse {} as a URL or as a local path"".format(url_or_filename))\n\n\ndef split_s3_path(url):\n    """"""Split a full s3 path into the bucket name and path.""""""\n    parsed = urlparse(url)\n    if not parsed.netloc or not parsed.path:\n        raise ValueError(""bad s3 path {}"".format(url))\n    bucket_name = parsed.netloc\n    s3_path = parsed.path\n    # Remove \'/\' at beginning of path.\n    if s3_path.startswith(""/""):\n        s3_path = s3_path[1:]\n    return bucket_name, s3_path\n\n\ndef s3_request(func):\n    """"""\n    Wrapper function for s3 requests in order to create more helpful error\n    messages.\n    """"""\n\n    @wraps(func)\n    def wrapper(url, *args, **kwargs):\n        try:\n            return func(url, *args, **kwargs)\n        except ClientError as exc:\n            if int(exc.response[""Error""][""Code""]) == 404:\n                raise EnvironmentError(""file {} not found"".format(url))\n            else:\n                raise\n\n    return wrapper\n\n\n@s3_request\ndef s3_etag(url):\n    """"""Check ETag on S3 object.""""""\n    s3_resource = boto3.resource(""s3"")\n    bucket_name, s3_path = split_s3_path(url)\n    s3_object = s3_resource.Object(bucket_name, s3_path)\n    return s3_object.e_tag\n\n\n@s3_request\ndef s3_get(url, temp_file):\n    """"""Pull a file directly from S3.""""""\n    s3_resource = boto3.resource(""s3"")\n    bucket_name, s3_path = split_s3_path(url)\n    s3_resource.Bucket(bucket_name).download_fileobj(s3_path, temp_file)\n\n\ndef http_get(url, temp_file):\n    req = requests.get(url, stream=True)\n    content_length = req.headers.get(\'Content-Length\')\n    total = int(content_length) if content_length is not None else None\n    progress = tqdm(unit=""B"", total=total)\n    for chunk in req.iter_content(chunk_size=1024):\n        if chunk: # filter out keep-alive new chunks\n            progress.update(len(chunk))\n            temp_file.write(chunk)\n    progress.close()\n\n\ndef get_from_cache(url, cache_dir=None):\n    """"""\n    Given a URL, look for the corresponding dataset in the local cache.\n    If it\'s not there, download it. Then return the path to the cached file.\n    """"""\n    if cache_dir is None:\n        cache_dir = PYTORCH_PRETRAINED_BERT_CACHE\n    if sys.version_info[0] == 3 and isinstance(cache_dir, Path):\n        cache_dir = str(cache_dir)\n\n    if not os.path.exists(cache_dir):\n        os.makedirs(cache_dir)\n\n    # Get eTag to add to filename, if it exists.\n    if url.startswith(""s3://""):\n        etag = s3_etag(url)\n    else:\n        response = requests.head(url, allow_redirects=True)\n        if response.status_code != 200:\n            raise IOError(""HEAD request failed for url {} with status code {}""\n                          .format(url, response.status_code))\n        etag = response.headers.get(""ETag"")\n\n    filename = url_to_filename(url, etag)\n\n    # get cache path to put the file\n    cache_path = os.path.join(cache_dir, filename)\n\n    if not os.path.exists(cache_path):\n        # Download to temporary file, then copy to cache dir once finished.\n        # Otherwise you get corrupt cache entries if the download gets interrupted.\n        with tempfile.NamedTemporaryFile() as temp_file:\n            logger.info(""%s not found in cache, downloading to %s"", url, temp_file.name)\n\n            # GET file object\n            if url.startswith(""s3://""):\n                s3_get(url, temp_file)\n            else:\n                http_get(url, temp_file)\n\n            # we are copying the file before closing it, so flush to avoid truncation\n            temp_file.flush()\n            # shutil.copyfileobj() starts at the current position, so go to the start\n            temp_file.seek(0)\n\n            logger.info(""copying %s to cache at %s"", temp_file.name, cache_path)\n            with open(cache_path, \'wb\') as cache_file:\n                shutil.copyfileobj(temp_file, cache_file)\n\n            logger.info(""creating metadata file for %s"", cache_path)\n            meta = {\'url\': url, \'etag\': etag}\n            meta_path = cache_path + \'.json\'\n            with open(meta_path, \'w\', encoding=""utf-8"") as meta_file:\n                json.dump(meta, meta_file)\n\n            logger.info(""removing temp file %s"", temp_file.name)\n\n    return cache_path\n\n\ndef read_set_from_file(filename):\n    \'\'\'\n    Extract a de-duped collection (set) of text from a file.\n    Expected file format is one item per line.\n    \'\'\'\n    collection = set()\n    with open(filename, \'r\', encoding=\'utf-8\') as file_:\n        for line in file_:\n            collection.add(line.rstrip())\n    return collection\n\n\ndef get_file_extension(path, dot=True, lower=True):\n    ext = os.path.splitext(path)[1]\n    ext = ext if dot else ext[1:]\n    return ext.lower() if lower else ext\n'"
Dureader/dataset/run_squad.py,0,"b'import json\nimport args\nimport torch\nimport pickle\nimport random\nimport numpy as np\nfrom tqdm import tqdm\nfrom tokenization import BertTokenizer\n\nrandom.seed(args.seed)\n\ndef read_squad_examples(zhidao_input_file, search_input_file, is_training=True):\n    total, error = 0, 0\n    examples = []\n\n    with open(search_input_file, \'r\', encoding=\'utf-8\') as f:\n        for line in tqdm(f.readlines()):\n\n            source = json.loads(line.strip())\n            if (len(source[\'answer_spans\']) == 0):\n                continue\n            if source[\'answers\'] == []:\n                continue\n            if (source[\'match_scores\'][0] < 0.8):\n                continue\n            if (source[\'answer_spans\'][0][1] > args.max_seq_length):\n                continue\n\n            docs_index = source[\'answer_docs\'][0]\n\n            start_id = source[\'answer_spans\'][0][0]\n            end_id = source[\'answer_spans\'][0][1] + 1          ## !!!!!\n            question_type = source[\'question_type\']\n\n            passages = []\n            try:\n                answer_passage_idx = source[\'documents\'][docs_index][\'most_related_para\']\n            except:\n                continue\n\n            doc_tokens = source[\'documents\'][docs_index][\'segmented_paragraphs\'][answer_passage_idx]\n            ques_len = len(source[\'documents\'][docs_index][\'segmented_title\']) + 1\n            doc_tokens = doc_tokens[ques_len:]\n            start_id , end_id = start_id -  ques_len, end_id - ques_len\n\n            if start_id >= end_id or end_id > len(doc_tokens) or start_id >= len(doc_tokens):\n                continue\n \n            new_doc_tokens = """"\n            for idx, token in enumerate(doc_tokens):\n                if idx == start_id:\n                    new_start_id = len(new_doc_tokens)\n                    break\n                new_doc_tokens = new_doc_tokens + token\n\n            new_doc_tokens = """".join(doc_tokens)\n            new_end_id = new_start_id + len(source[\'fake_answers\'][0])            \n\n            if source[\'fake_answers\'][0] != """".join(new_doc_tokens[new_start_id:new_end_id]):\n                continue\n\n            if is_training:\n                new_end_id = new_end_id - 1\n                example = {\n                        ""qas_id"":source[\'question_id\'],\n                        ""question_text"":source[\'question\'].strip(),\n                        ""question_type"":question_type,\n                        ""doc_tokens"":new_doc_tokens.strip(),\n                        ""start_position"":new_start_id,\n                        ""end_position"":new_end_id }      \n\n                examples.append(example)\n    with open(zhidao_input_file, \'r\', encoding=\'utf-8\') as f:\n        for line in tqdm(f.readlines()):\n\n            source = json.loads(line.strip())\n            if (len(source[\'answer_spans\']) == 0):\n                continue\n            if source[\'answers\'] == []:\n                continue\n            if (source[\'match_scores\'][0] < 0.8):\n                continue\n            if (source[\'answer_spans\'][0][1] > args.max_seq_length):\n                continue\n            docs_index = source[\'answer_docs\'][0]\n\n            start_id = source[\'answer_spans\'][0][0]\n            end_id = source[\'answer_spans\'][0][1] + 1          ## !!!!!\n            question_type = source[\'question_type\']\n\n            passages = []\n            try:\n                answer_passage_idx = source[\'documents\'][docs_index][\'most_related_para\']\n            except:\n                continue\n\n            doc_tokens = source[\'documents\'][docs_index][\'segmented_paragraphs\'][answer_passage_idx]\n\n            ques_len = len(source[\'documents\'][docs_index][\'segmented_title\']) + 1\n            doc_tokens = doc_tokens[ques_len:]\n            start_id , end_id = start_id -  ques_len, end_id - ques_len\n\n            if start_id >= end_id or end_id > len(doc_tokens) or start_id >= len(doc_tokens):\n                continue\n\n            new_doc_tokens = """"\n            for idx, token in enumerate(doc_tokens):\n                if idx == start_id:\n                    new_start_id = len(new_doc_tokens)\n                    break\n                new_doc_tokens = new_doc_tokens + token\n\n            new_doc_tokens = """".join(doc_tokens)\n            new_end_id = new_start_id + len(source[\'fake_answers\'][0])\n\n            if source[\'fake_answers\'][0] != """".join(new_doc_tokens[new_start_id:new_end_id]):\n                continue\n\n            if is_training:\n                new_end_id = new_end_id - 1\n                example = {\n                        ""qas_id"":source[\'question_id\'],\n                        ""question_text"":source[\'question\'].strip(),\n                        ""question_type"":question_type,\n                        ""doc_tokens"":new_doc_tokens.strip(),\n                        ""start_position"":new_start_id,\n                        ""end_position"":new_end_id }\n\n                examples.append(example)\n\n    print(""len(examples):"",len(examples))\n    return examples\n\ndef convert_examples_to_features(examples, tokenizer, max_seq_length, max_query_length):\n\n    features = []\n\n    for example in tqdm(examples):\n        query_tokens = list(example[\'question_text\'])\n        question_type = example[\'question_type\']    \n\n        doc_tokens = example[\'doc_tokens\']\n        doc_tokens = doc_tokens.replace(u""\xe2\x80\x9c"", u""\\"""")\n        doc_tokens = doc_tokens.replace(u""\xe2\x80\x9d"", u""\\"""")\n        start_position = example[\'start_position\']\n        end_position = example[\'end_position\']\n\n        if len(query_tokens) > max_query_length:\n            query_tokens = query_tokens[0:max_query_length]\n\n        tokens = []\n        segment_ids = []\n\n        tokens.append(""[CLS]"")\n        segment_ids.append(0)\n        start_position = start_position + 1\n        end_position = end_position + 1\n\n        for token in query_tokens:\n            tokens.append(token)\n            segment_ids.append(0)\n            start_position = start_position + 1\n            end_position = end_position + 1\n\n        tokens.append(""[SEP]"")\n        segment_ids.append(0)\n        start_position = start_position + 1\n        end_position = end_position + 1\n\n        for i in doc_tokens:\n            tokens.append(i)\n            segment_ids.append(1)\n        tokens.append(""[SEP]"")\n        segment_ids.append(1)\n\n        if end_position >= max_seq_length:\n            continue\n\n        if len(tokens) > max_seq_length:\n            tokens[max_seq_length-1] = ""[SEP]""\n            input_ids = tokenizer.convert_tokens_to_ids(tokens[:max_seq_length])      ## !!! SEP\n            segment_ids = segment_ids[:max_seq_length]\n        else:\n            input_ids = tokenizer.convert_tokens_to_ids(tokens)\n\n        input_mask = [1] * len(input_ids)\n        assert len(input_ids) == len(segment_ids)\n\n        features.append(\n                        {""input_ids"":input_ids,\n                         ""input_mask"":input_mask,\n                         ""segment_ids"":segment_ids,\n                         ""start_position"":start_position,\n                         ""end_position"":end_position })\n\n    with open(""./train.data"", \'w\', encoding=""utf-8"") as fout:\n        for feature in features:\n            fout.write(json.dumps(feature, ensure_ascii=False) + \'\\n\')\n    print(""len(features):"",len(features))\n    return features\n\nif __name__ == ""__main__"":\n\n    tokenizer = BertTokenizer.from_pretrained(\'bert-base-chinese\', do_lower_case=True)\n    # \xe7\x94\x9f\xe6\x88\x90\xe8\xae\xad\xe7\xbb\x83\xe6\x95\xb0\xe6\x8d\xae\xef\xbc\x8c train.data\n    examples = read_squad_examples(zhidao_input_file=args.zhidao_input_file,\n                                   search_input_file=args.search_input_file)\n    features = convert_examples_to_features(examples=examples, tokenizer=tokenizer,\n                                            max_seq_length=args.max_seq_length, max_query_length=args.max_query_length)\n\n    # \xe7\x94\x9f\xe6\x88\x90\xe9\xaa\x8c\xe8\xaf\x81\xe6\x95\xb0\xe6\x8d\xae\xef\xbc\x8c dev.data\xe3\x80\x82\xe8\xae\xb0\xe5\xbe\x97\xe6\xb3\xa8\xe9\x87\x8a\xe6\x8e\x89\xe7\x94\x9f\xe6\x88\x90\xe8\xae\xad\xe7\xbb\x83\xe6\x95\xb0\xe6\x8d\xae\xe7\x9a\x84\xe4\xbb\xa3\xe7\xa0\x81\xef\xbc\x8c\xe5\xb9\xb6\xe5\x9c\xa8196\xe8\xa1\x8c\xe5\xb0\x86train.data\xe6\x94\xb9\xe4\xb8\xbadev.data\n   # examples = read_squad_examples(zhidao_input_file=args.dev_zhidao_input_file,\n   #                                search_input_file=args.dev_search_input_file)\n   # features = convert_examples_to_features(examples=examples, tokenizer=tokenizer,\n   #                                         max_seq_length=args.max_seq_length, max_query_length=args.max_query_length)\n'"
Dureader/dataset/tokenization.py,0,"b'from __future__ import absolute_import, division, print_function, unicode_literals\n\nimport collections\nimport logging\nimport os\nimport unicodedata\nfrom io import open\n\nfrom file_utils import cached_path\n\nlogger = logging.getLogger(__name__)\n\nPRETRAINED_VOCAB_ARCHIVE_MAP = {\n    \'bert-base-uncased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt"",\n    \'bert-large-uncased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-vocab.txt"",\n    \'bert-base-cased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt"",\n    \'bert-large-cased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-vocab.txt"",\n    \'bert-base-multilingual-uncased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt"",\n    \'bert-base-multilingual-cased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt"",\n    \'bert-base-chinese\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-vocab.txt"",\n}\nPRETRAINED_VOCAB_POSITIONAL_EMBEDDINGS_SIZE_MAP = {\n    \'bert-base-uncased\': 512,\n    \'bert-large-uncased\': 512,\n    \'bert-base-cased\': 512,\n    \'bert-large-cased\': 512,\n    \'bert-base-multilingual-uncased\': 512,\n    \'bert-base-multilingual-cased\': 512,\n    \'bert-base-chinese\': 512,\n}\nVOCAB_NAME = \'vocab.txt\'\n\n\ndef load_vocab(vocab_file):\n    """"""Loads a vocabulary file into a dictionary.""""""\n    vocab = collections.OrderedDict()\n    index = 0\n    with open(vocab_file, ""r"", encoding=""utf-8"") as reader:\n        while True:\n            token = reader.readline()\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = index\n            index += 1\n    return vocab\n\n\ndef whitespace_tokenize(text):\n    """"""Runs basic whitespace cleaning and splitting on a piece of text.""""""\n    text = text.strip()\n    if not text:\n        return []\n    tokens = text.split()\n    return tokens\n\n\nclass BertTokenizer(object):\n    """"""Runs end-to-end tokenization: punctuation splitting + wordpiece""""""\n\n    def __init__(self, vocab_file, do_lower_case=True, max_len=None, do_basic_tokenize=True,\n                 never_split=(""[UNK]"", ""[SEP]"", ""[PAD]"", ""[CLS]"", ""[MASK]"")):\n        """"""Constructs a BertTokenizer.\n        Args:\n          vocab_file: Path to a one-wordpiece-per-line vocabulary file\n          do_lower_case: Whether to lower case the input\n                         Only has an effect when do_wordpiece_only=False\n          do_basic_tokenize: Whether to do basic tokenization before wordpiece.\n          max_len: An artificial maximum length to truncate tokenized sequences to;\n                         Effective maximum length is always the minimum of this\n                         value (if specified) and the underlying BERT model\'s\n                         sequence length.\n          never_split: List of tokens which will never be split during tokenization.\n                         Only has an effect when do_wordpiece_only=False\n        """"""\n        if not os.path.isfile(vocab_file):\n            raise ValueError(\n                ""Can\'t find a vocabulary file at path \'{}\'. To load the vocabulary from a Google pretrained ""\n                ""model use `tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)`"".format(vocab_file))\n        self.vocab = load_vocab(vocab_file)\n        self.ids_to_tokens = collections.OrderedDict(\n            [(ids, tok) for tok, ids in self.vocab.items()])\n        self.do_basic_tokenize = do_basic_tokenize\n        if do_basic_tokenize:\n          self.basic_tokenizer = BasicTokenizer(do_lower_case=do_lower_case,\n                                                never_split=never_split)\n        self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab)\n        self.max_len = max_len if max_len is not None else int(1e12)\n\n    def tokenize(self, text):\n        if self.do_basic_tokenize:\n          split_tokens = []\n          for token in self.basic_tokenizer.tokenize(text):\n              for sub_token in self.wordpiece_tokenizer.tokenize(token):\n                  split_tokens.append(sub_token)\n        else:\n          split_tokens = self.wordpiece_tokenizer.tokenize(text)\n        return split_tokens\n\n    def convert_tokens_to_ids(self, tokens):\n        """"""Converts a sequence of tokens into ids using the vocab.""""""\n        ids = []\n        for token in tokens:\n            if token != ""[CLS]"" and token != ""[SEP]"":\n                token = token.lower()\n            if token in self.vocab:\n                ids.append(self.vocab[token])\n            else:\n                ids.append(self.vocab[""[UNK]""])\n        if len(ids) > self.max_len:\n            logger.warning(\n                ""Token indices sequence length is longer than the specified maximum ""\n                "" sequence length for this BERT model ({} > {}). Running this""\n                "" sequence through BERT will result in indexing errors"".format(len(ids), self.max_len)\n            )\n        return ids\n\n    def convert_ids_to_tokens(self, ids):\n        """"""Converts a sequence of ids in wordpiece tokens using the vocab.""""""\n        tokens = []\n        for i in ids:\n            tokens.append(self.ids_to_tokens[i])\n        return tokens\n\n    @classmethod\n    def from_pretrained(cls, pretrained_model_name_or_path, cache_dir=None, *inputs, **kwargs):\n        """"""\n        Instantiate a PreTrainedBertModel from a pre-trained model file.\n        Download and cache the pre-trained model file if needed.\n        """"""\n        if pretrained_model_name_or_path in PRETRAINED_VOCAB_ARCHIVE_MAP:\n            vocab_file = PRETRAINED_VOCAB_ARCHIVE_MAP[pretrained_model_name_or_path]\n        else:\n            vocab_file = pretrained_model_name_or_path\n        if os.path.isdir(vocab_file):\n            vocab_file = os.path.join(vocab_file, VOCAB_NAME)\n        # redirect to the cache, if necessary\n        try:\n            resolved_vocab_file = cached_path(vocab_file, cache_dir=cache_dir)\n        except EnvironmentError:\n            logger.error(\n                ""Model name \'{}\' was not found in model name list ({}). ""\n                ""We assumed \'{}\' was a path or url but couldn\'t find any file ""\n                ""associated to this path or url."".format(\n                    pretrained_model_name_or_path,\n                    \', \'.join(PRETRAINED_VOCAB_ARCHIVE_MAP.keys()),\n                    vocab_file))\n            return None\n        if resolved_vocab_file == vocab_file:\n            logger.info(""loading vocabulary file {}"".format(vocab_file))\n        else:\n            logger.info(""loading vocabulary file {} from cache at {}"".format(\n                vocab_file, resolved_vocab_file))\n        if pretrained_model_name_or_path in PRETRAINED_VOCAB_POSITIONAL_EMBEDDINGS_SIZE_MAP:\n            # if we\'re using a pretrained model, ensure the tokenizer wont index sequences longer\n            # than the number of positional embeddings\n            max_len = PRETRAINED_VOCAB_POSITIONAL_EMBEDDINGS_SIZE_MAP[pretrained_model_name_or_path]\n            kwargs[\'max_len\'] = min(kwargs.get(\'max_len\', int(1e12)), max_len)\n        # Instantiate tokenizer.\n        tokenizer = cls(resolved_vocab_file, *inputs, **kwargs)\n        return tokenizer\n\n\nclass BasicTokenizer(object):\n    """"""Runs basic tokenization (punctuation splitting, lower casing, etc.).""""""\n\n    def __init__(self,\n                 do_lower_case=True,\n                 never_split=(""[UNK]"", ""[SEP]"", ""[PAD]"", ""[CLS]"", ""[MASK]"")):\n        """"""Constructs a BasicTokenizer.\n        Args:\n          do_lower_case: Whether to lower case the input.\n        """"""\n        self.do_lower_case = do_lower_case\n        self.never_split = never_split\n\n    def tokenize(self, text):\n        """"""Tokenizes a piece of text.""""""\n        text = self._clean_text(text)\n       \n        text = self._tokenize_chinese_chars(text)\n        orig_tokens = whitespace_tokenize(text)\n        split_tokens = []\n        for token in orig_tokens:\n            if self.do_lower_case and token not in self.never_split:\n                token = token.lower()\n                token = self._run_strip_accents(token)\n            split_tokens.extend(self._run_split_on_punc(token))\n\n        output_tokens = whitespace_tokenize("" "".join(split_tokens))\n        return output_tokens\n\n    def _run_strip_accents(self, text):\n        """"""Strips accents from a piece of text.""""""\n        text = unicodedata.normalize(""NFD"", text)\n        output = []\n        for char in text:\n            cat = unicodedata.category(char)\n            if cat == ""Mn"":\n                continue\n            output.append(char)\n        return """".join(output)\n\n    def _run_split_on_punc(self, text):\n        """"""Splits punctuation on a piece of text.""""""\n        if text in self.never_split:\n            return [text]\n        chars = list(text)\n        i = 0\n        start_new_word = True\n        output = []\n        while i < len(chars):\n            char = chars[i]\n            if _is_punctuation(char):\n                output.append([char])\n                start_new_word = True\n            else:\n                if start_new_word:\n                    output.append([])\n                start_new_word = False\n                output[-1].append(char)\n            i += 1\n\n        return ["""".join(x) for x in output]\n\n    def _tokenize_chinese_chars(self, text):\n        """"""Adds whitespace around any CJK character.""""""\n        output = []\n        for char in text:\n            cp = ord(char)\n            if self._is_chinese_char(cp):\n                output.append("" "")\n                output.append(char)\n                output.append("" "")\n            else:\n                output.append(char)\n        return """".join(output)\n\n    def _is_chinese_char(self, cp):\n        """"""Checks whether CP is the codepoint of a CJK character.""""""\n        # This defines a ""chinese character"" as anything in the CJK Unicode block:\n        #   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)\n        #\n        # Note that the CJK Unicode block is NOT all Japanese and Korean characters,\n        # despite its name. The modern Korean Hangul alphabet is a different block,\n        # as is Japanese Hiragana and Katakana. Those alphabets are used to write\n        # space-separated words, so they are not treated specially and handled\n        # like the all of the other languages.\n        if ((cp >= 0x4E00 and cp <= 0x9FFF) or  #\n                (cp >= 0x3400 and cp <= 0x4DBF) or  #\n                (cp >= 0x20000 and cp <= 0x2A6DF) or  #\n                (cp >= 0x2A700 and cp <= 0x2B73F) or  #\n                (cp >= 0x2B740 and cp <= 0x2B81F) or  #\n                (cp >= 0x2B820 and cp <= 0x2CEAF) or\n                (cp >= 0xF900 and cp <= 0xFAFF) or  #\n                (cp >= 0x2F800 and cp <= 0x2FA1F)):  #\n            return True\n\n        return False\n\n    def _clean_text(self, text):\n        """"""Performs invalid character removal and whitespace cleanup on text.""""""\n        output = []\n        for char in text:\n            cp = ord(char)\n            if cp == 0 or cp == 0xfffd or _is_control(char):\n                continue\n            if _is_whitespace(char):\n                output.append("" "")\n            else:\n                output.append(char)\n        return """".join(output)\n\n\nclass WordpieceTokenizer(object):\n    """"""Runs WordPiece tokenization.""""""\n\n    def __init__(self, vocab, unk_token=""[UNK]"", max_input_chars_per_word=100):\n        self.vocab = vocab\n        self.unk_token = unk_token\n        self.max_input_chars_per_word = max_input_chars_per_word\n\n    def tokenize(self, text):\n        """"""Tokenizes a piece of text into its word pieces.\n        This uses a greedy longest-match-first algorithm to perform tokenization\n        using the given vocabulary.\n        For example:\n          input = ""unaffable""\n          output = [""un"", ""##aff"", ""##able""]\n        Args:\n          text: A single token or whitespace separated tokens. This should have\n            already been passed through `BasicTokenizer`.\n        Returns:\n          A list of wordpiece tokens.\n        """"""\n\n        output_tokens = []\n        for token in whitespace_tokenize(text):\n            chars = list(token)\n            if len(chars) > 100:                            ## !!! \n                output_tokens.append(self.unk_token)\n                continue\n\n            is_bad = False\n            start = 0\n            sub_tokens = []\n            while start < len(chars):\n                end = len(chars)\n                cur_substr = None\n                while start < end:\n                    substr = """".join(chars[start:end])\n                    if start > 0:\n                        substr = ""##"" + substr\n                    if substr in self.vocab:\n                        cur_substr = substr\n                        break\n                    end -= 1\n                if cur_substr is None:\n                    is_bad = True\n                    break\n                sub_tokens.append(cur_substr)\n                start = end\n\n            if is_bad:\n                output_tokens.append(self.unk_token)\n            else:\n                output_tokens.extend(sub_tokens)\n        return output_tokens\n\n\ndef _is_whitespace(char):\n    """"""Checks whether `chars` is a whitespace character.""""""\n    # \\t, \\n, and \\r are technically contorl characters but we treat them\n    # as whitespace since they are generally considered as such.\n    if char == "" "" or char == ""\\t"" or char == ""\\n"" or char == ""\\r"":\n        return True\n    cat = unicodedata.category(char)\n    if cat == ""Zs"":\n        return True\n    return False\n\n\ndef _is_control(char):\n    """"""Checks whether `chars` is a control character.""""""\n    # These are technically control characters but we count them as whitespace\n    # characters.\n    if char == ""\\t"" or char == ""\\n"" or char == ""\\r"":\n        return False\n    cat = unicodedata.category(char)\n    if cat.startswith(""C""):\n        return True\n    return False\n\n\ndef _is_punctuation(char):\n    """"""Checks whether `chars` is a punctuation character.""""""\n    cp = ord(char)\n    # We treat all non-letter/number ASCII as punctuation.\n    # Characters such as ""^"", ""$"", and ""`"" are not in the Unicode\n    # Punctuation class but we treat them as punctuation anyways, for\n    # consistency.\n    if ((cp >= 33 and cp <= 47) or (cp >= 58 and cp <= 64) or\n            (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126)):\n        return True\n    cat = unicodedata.category(char)\n    if cat.startswith(""P""):\n        return True\n    return False\n'"
Dureader/handle_data/__init__.py,0,b''
Dureader/handle_data/paragraph_extraction.py,0,"b'#!/usr/bin/python\n#-*- coding:utf-8 -*-\n\nimport sys\nif sys.version[0] == \'2\':\n    reload(sys)\n    sys.setdefaultencoding(""utf-8"")\nimport json\nimport copy\nfrom preprocess import metric_max_over_ground_truths, f1_score\n\ndef compute_paragraph_score(sample):\n    """"""\n    For each paragraph, compute the f1 score compared with the question\n    Args:\n        sample: a sample in the dataset.\n    Returns:\n        None\n    Raises:\n        None\n    """"""\n    scores = []\n    question = sample[""segmented_question""]\n    for doc in sample[\'documents\']:\n        doc[\'segmented_paragraphs_scores\'] = []\n        for p_idx, para_tokens in enumerate(doc[\'segmented_paragraphs\']):\n            if len(question) > 0:\n                related_score = metric_max_over_ground_truths(f1_score,\n                        para_tokens,\n                        [question])\n            else:\n                related_score = 0.0\n            doc[\'segmented_paragraphs_scores\'].append(related_score)\n\n            scores.append(related_score)\n#    with open(\'scores.txt\', \'a\') as f:\n #       f.write(str(scores) + \'\\n\' + \'\\n\')\ndef dup_remove(doc):\n    """"""\n    For each document, remove the duplicated paragraphs\n    Args:\n        doc: a doc in the sample\n    Returns:\n        bool\n    Raises:\n        None\n    """"""\n    paragraphs_his = {}\n    del_ids = []\n    para_id = None\n    if \'most_related_para\' in doc:\n        para_id = doc[\'most_related_para\']\n    doc[\'paragraphs_length\'] = []\n    for p_idx, (segmented_paragraph, paragraph_score) in \\\n        enumerate(zip(doc[""segmented_paragraphs""], doc[""segmented_paragraphs_scores""])):\n        doc[\'paragraphs_length\'].append(len(segmented_paragraph))\n        paragraph = \'\'.join(segmented_paragraph)\n        if paragraph in paragraphs_his:\n            del_ids.append(p_idx)\n            if p_idx == para_id:\n                para_id = paragraphs_his[paragraph]\n            continue\n        paragraphs_his[paragraph] = p_idx\n    # delete\n    prev_del_num = 0\n    del_num = 0\n    for p_idx in del_ids:\n        if p_idx < para_id: \n            prev_del_num += 1\n        del doc[""segmented_paragraphs""][p_idx - del_num]\n        del doc[""segmented_paragraphs_scores""][p_idx - del_num]\n        del doc[\'paragraphs_length\'][p_idx - del_num]\n        del_num += 1\n    if len(del_ids) != 0:\n        if \'most_related_para\' in doc:\n            doc[\'most_related_para\'] = para_id - prev_del_num\n        doc[\'paragraphs\'] = []\n        for segmented_para in doc[""segmented_paragraphs""]:\n            paragraph = \'\'.join(segmented_para)\n            doc[\'paragraphs\'].append(paragraph)\n        return True\n    else:\n        return False\n\n\ndef paragraph_selection(sample, mode):\n    """"""\n    For each document, select paragraphs that includes as much information as possible\n    Args:\n        sample: a sample in the dataset.\n        mode: string of (""train"", ""dev"", ""test""), indicate the type of dataset to process.\n    Returns:\n        None\n    Raises:\n        None\n    """"""\n    scores = []\n    # predefined maximum length of paragraph\n    MAX_P_LEN = 510\n    # predefined splitter\n  #  splitter = u\'<splitter>\'\n    splitter = \'\xe3\x80\x82\'\n\n    # topN of related paragraph to choose\n    topN = 5\n    doc_id = None\n    if \'answer_docs\' in sample and len(sample[\'answer_docs\']) > 0:\n        doc_id = sample[\'answer_docs\'][0]\n        if doc_id >= len(sample[\'documents\']):\n            # Data error, answer doc ID > number of documents, this sample\n            # will be filtered by dataset.py\n            return\n    for d_idx, doc in enumerate(sample[\'documents\']):\n        if \'segmented_paragraphs_scores\' not in doc:\n            continue\n        status = dup_remove(doc)\n        segmented_title = doc[""segmented_title""]\n        title_len = len(segmented_title)\n        para_id = None\n        if doc_id is not None:\n            para_id = sample[\'documents\'][doc_id][\'most_related_para\']\n        total_len = title_len + sum(doc[\'paragraphs_length\'])\n        # add splitter\n        para_num = len(doc[""segmented_paragraphs""])\n        total_len += para_num\n        if total_len <= MAX_P_LEN:\n            incre_len = title_len\n            total_segmented_content = copy.deepcopy(segmented_title)\n            for p_idx, segmented_para in enumerate(doc[""segmented_paragraphs""]):\n                if doc_id == d_idx and para_id > p_idx:\n                    incre_len += len([splitter] + segmented_para)\n                if doc_id == d_idx and para_id == p_idx:\n                    incre_len += 1\n                total_segmented_content += [splitter] + segmented_para\n            if doc_id == d_idx:\n                answer_start = incre_len + sample[\'answer_spans\'][0][0]\n                answer_end = incre_len + sample[\'answer_spans\'][0][1]\n                sample[\'answer_spans\'][0][0] = answer_start\n                sample[\'answer_spans\'][0][1] = answer_end\n            doc[""segmented_paragraphs""] = [total_segmented_content]\n            doc[""segmented_paragraphs_scores""] = [1.0]\n            doc[\'paragraphs_length\'] = [total_len]\n            doc[\'paragraphs\'] = [\'\'.join(total_segmented_content)]\n            doc[\'most_related_para\'] = 0\n            continue\n        # find topN paragraph id\n        para_infos = []\n        for p_idx, (para_tokens, para_scores) in \\\n                enumerate(zip(doc[\'segmented_paragraphs\'], doc[\'segmented_paragraphs_scores\'])):\n            para_infos.append((para_tokens, para_scores, len(para_tokens), p_idx))\n\n#        para_infos.sort(key=lambda x: (-x[1], x[2]))\n   #     para_infos.sort(key=lambda x: -x[1])\n        topN_idx = []\n\n        for para_info in para_infos[:topN]:\n            topN_idx.append(para_info[-1])\n            scores.append((para_info[1],para_info[2]))\n\n        final_idx = []\n        total_len = title_len\n        if doc_id == d_idx:\n            if mode == ""train"":\n                final_idx.append(para_id)\n                total_len = title_len + 1 + doc[\'paragraphs_length\'][para_id]\n        for id in topN_idx:\n            if total_len > MAX_P_LEN:\n                break\n            if doc_id == d_idx and id == para_id and mode == ""train"":\n                continue\n            total_len += 1 + doc[\'paragraphs_length\'][id] \n            final_idx.append(id)\n        total_segmented_content = copy.deepcopy(segmented_title)\n        final_idx.sort()\n        incre_len = title_len\n        for id in final_idx:\n            if doc_id == d_idx and id < para_id:\n                incre_len += 1 + doc[\'paragraphs_length\'][id]\n            if doc_id == d_idx and id == para_id:\n                incre_len += 1\n            total_segmented_content += [splitter] + doc[\'segmented_paragraphs\'][id]\n        if doc_id == d_idx:\n            answer_start = incre_len + sample[\'answer_spans\'][0][0]\n            answer_end = incre_len + sample[\'answer_spans\'][0][1]\n            sample[\'answer_spans\'][0][0] = answer_start\n            sample[\'answer_spans\'][0][1] = answer_end\n        doc[""segmented_paragraphs""] = [total_segmented_content]\n        doc[""segmented_paragraphs_scores""] = [1.0]\n        doc[\'paragraphs_length\'] = [total_len]\n        doc[\'paragraphs\'] = [\'\'.join(total_segmented_content)]\n        doc[\'most_related_para\'] = 0\n\n\nif __name__ == ""__main__"":\n    # mode=""train""/""dev""/""test""\n    mode = sys.argv[1]\n    for line in sys.stdin:\n        line = line.strip()\n        if line == """":\n            continue\n        try:\n            sample = json.loads(line, encoding=\'utf8\')\n        except:\n            print >>sys.stderr, ""Invalid input json format - \'{}\' will be ignored"".format(line)\n            continue\n        compute_paragraph_score(sample)\n        paragraph_selection(sample, mode)\n        print(json.dumps(sample, encoding=\'utf8\', ensure_ascii=False))\n\n'"
Dureader/handle_data/preprocess.py,0,"b'import sys\nimport json\nfrom collections import Counter\n\ndef precision_recall_f1(prediction, ground_truth):\n    """"""\n    This function calculates and returns the precision, recall and f1-score\n    Args:\n        prediction: prediction string or list to be matched\n        ground_truth: golden string or list reference\n    Returns:\n        floats of (p, r, f1)\n    Raises:\n        None\n    """"""\n    if not isinstance(prediction, list):\n        prediction_tokens = prediction.split()\n    else:\n        prediction_tokens = prediction\n    if not isinstance(ground_truth, list):\n        ground_truth_tokens = ground_truth.split()\n    else:\n        ground_truth_tokens = ground_truth\n    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n    num_same = sum(common.values())\n    if num_same == 0:\n        return 0, 0, 0\n    p = 1.0 * num_same / len(prediction_tokens)\n    r = 1.0 * num_same / len(ground_truth_tokens)\n    f1 = (2 * p * r) / (p + r)\n    return p, r, f1\n\n\ndef recall(prediction, ground_truth):\n    """"""\n    This function calculates and returns the recall\n    Args:\n        prediction: prediction string or list to be matched\n        ground_truth: golden string or list reference\n    Returns:\n        floats of recall\n    Raises:\n        None\n    """"""\n    return precision_recall_f1(prediction, ground_truth)[1]\n\n\ndef f1_score(prediction, ground_truth):\n    """"""\n    This function calculates and returns the f1-score\n    Args:\n        prediction: prediction string or list to be matched\n        ground_truth: golden string or list reference\n    Returns:\n        floats of f1\n    Raises:\n        None\n    """"""\n    return precision_recall_f1(prediction, ground_truth)[2]\n\n\ndef metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n    """"""\n    This function calculates and returns the precision, recall and f1-score\n    Args:\n        metric_fn: metric function pointer which calculates scores according to corresponding logic.\n        prediction: prediction string or list to be matched\n        ground_truth: golden string or list reference\n    Returns:\n        floats of (p, r, f1)\n    Raises:\n        None\n    """"""\n    scores_for_ground_truths = []\n    for ground_truth in ground_truths:\n        score = metric_fn(prediction, ground_truth)\n        scores_for_ground_truths.append(score)\n    return max(scores_for_ground_truths)\n\n\ndef find_best_question_match(doc, question, with_score=False):\n    """"""\n    For each document, find the paragraph that matches best to the question.\n    Args:\n        doc: The document object.\n        question: The question tokens.\n        with_score: If True then the match score will be returned,\n            otherwise False.\n    Returns:\n        The index of the best match paragraph, if with_score=False,\n        otherwise returns a tuple of the index of the best match paragraph\n        and the match score of that paragraph.\n    """"""\n    most_related_para = -1\n    max_related_score = 0\n    most_related_para_len = 0\n    for p_idx, para_tokens in enumerate(doc[\'segmented_paragraphs\']):\n        if len(question) > 0:\n            related_score = metric_max_over_ground_truths(recall,\n                    para_tokens,\n                    question)\n        else:\n            related_score = 0\n\n        if related_score > max_related_score \\\n                or (related_score == max_related_score \\\n                and len(para_tokens) < most_related_para_len):\n            most_related_para = p_idx\n            max_related_score = related_score\n            most_related_para_len = len(para_tokens)\n    if most_related_para == -1:\n        most_related_para = 0\n    if with_score:\n        return most_related_para, max_related_score\n    return most_related_para\n\n\ndef find_fake_answer(sample):\n    """"""\n    For each document, finds the most related paragraph based on recall,\n    then finds a span that maximize the f1_score compared with the gold answers\n    and uses this span as a fake answer span\n    Args:\n        sample: a sample in the dataset\n    Returns:\n        None\n    Raises:\n        None\n    """"""\n    for doc in sample[\'documents\']:\n        most_related_para = -1\n        most_related_para_len = 999999\n        max_related_score = 0\n        for p_idx, para_tokens in enumerate(doc[\'segmented_paragraphs\']):\n            if len(sample[\'segmented_answers\']) > 0:\n                related_score = metric_max_over_ground_truths(recall,\n                                                              para_tokens,\n                                                              sample[\'segmented_answers\'])\n            else:\n                continue\n            if related_score > max_related_score \\\n                    or (related_score == max_related_score\n                        and len(para_tokens) < most_related_para_len):\n                most_related_para = p_idx\n                most_related_para_len = len(para_tokens)\n                max_related_score = related_score\n        doc[\'most_related_para\'] = most_related_para\n\n    sample[\'answer_docs\'] = []\n    sample[\'answer_spans\'] = []\n    sample[\'fake_answers\'] = []\n    sample[\'match_scores\'] = []\n\n    best_match_score = 0\n    best_match_d_idx, best_match_span = -1, [-1, -1]\n    best_fake_answer = None\n    answer_tokens = set()\n    for segmented_answer in sample[\'segmented_answers\']:\n        answer_tokens = answer_tokens | set([token for token in segmented_answer])\n    for d_idx, doc in enumerate(sample[\'documents\']):\n        if not doc[\'is_selected\']:\n            continue\n        if doc[\'most_related_para\'] == -1:\n            doc[\'most_related_para\'] = 0\n        most_related_para_tokens = doc[\'segmented_paragraphs\'][doc[\'most_related_para\']][:1000]\n        for start_tidx in range(len(most_related_para_tokens)):\n            if most_related_para_tokens[start_tidx] not in answer_tokens:\n                continue\n            for end_tidx in range(len(most_related_para_tokens) - 1, start_tidx - 1, -1):\n                span_tokens = most_related_para_tokens[start_tidx: end_tidx + 1]\n                if len(sample[\'segmented_answers\']) > 0:\n                    match_score = metric_max_over_ground_truths(f1_score, span_tokens,\n                                                                sample[\'segmented_answers\'])\n                else:\n                    match_score = 0\n                if match_score == 0:\n                    break\n                if match_score > best_match_score:\n                    best_match_d_idx = d_idx\n                    best_match_span = [start_tidx, end_tidx]\n                    best_match_score = match_score\n                    best_fake_answer = \'\'.join(span_tokens)\n    if best_match_score > 0:\n        sample[\'answer_docs\'].append(best_match_d_idx)\n        sample[\'answer_spans\'].append(best_match_span)\n        sample[\'fake_answers\'].append(best_fake_answer)\n        sample[\'match_scores\'].append(best_match_score)\n\n\nif __name__ == \'__main__\':\n    for line in sys.stdin:\n        sample = json.loads(line)\n        find_fake_answer(sample)\n        print(json.dumps(sample, encoding=\'utf8\', ensure_ascii=False))\n'"
Dureader/metric/bleu.py,0,"b""# coding:utf8\n\nfrom functools import reduce\nimport math\nimport json\nfrom collections import defaultdict\nimport sys \nimport importlib\nimport common\nimportlib.reload(sys)\n\n\nclass BLEU(object):\n    def __init__(self, n_size):\n        self.match_ngram = {}\n        self.candi_ngram = {}\n        self.bp_r = 0\n        self.bp_c = 0\n        self.n_size = n_size\n\n    def add_inst(self, cand, ref_list):\n        for n_size in range(self.n_size):\n            self.count_ngram(cand, ref_list, n_size)\n        self.count_bp(cand, ref_list)\n\n    def count_ngram(self, cand, ref_list, n_size):\n        cand_ngram = common.get_ngram(cand, n_size)\n        refs_ngram = []\n        for ref in ref_list:\n            refs_ngram.append(common.get_ngram(ref, n_size))\n        if n_size not in self.match_ngram:\n            self.match_ngram[n_size] = 0\n            self.candi_ngram[n_size] = 0\n        match_size, cand_size = common.get_match_size(cand_ngram, refs_ngram)\n        self.match_ngram[n_size] += match_size\n        self.candi_ngram[n_size] += cand_size\n\n    def count_bp(self, cand, ref_list):\n        self.bp_c += len(cand)\n        self.bp_r += min([\n            (abs(len(cand) - len(ref)), len(ref))\n            for ref in ref_list]\n            )[1]\n\n    def score(self):\n        prob_list = []\n        for n_size in range(self.n_size):\n            try:\n                if self.candi_ngram[n_size] == 0:\n                    _score = 0.0\n                else:\n                    _score = self.match_ngram[n_size] / float(self.candi_ngram[n_size])\n            except:\n                _score = 0\n            prob_list.append(_score)\n        bleu_list = [prob_list[0]]\n        for n in range(1, self.n_size):\n            bleu_list.append(bleu_list[-1] * prob_list[n])\n        for n in range(self.n_size):\n            bleu_list[n] = bleu_list[n] ** (1./float(n+1))\n        if float(self.bp_c) == 0.0:\n            bp = 0.0\n        else:\n            bp = math.exp(min(1 - self.bp_r / float(self.bp_c), 0))\n        for n in range(self.n_size):\n            bleu_list[n] = bleu_list[n] * bp\n        return bleu_list\n\nclass BLEUWithBonus(BLEU):\n    def __init__(self, n_size, alpha=1.0, beta=1.0):\n        super(BLEUWithBonus, self).__init__(n_size)\n        self.alpha = alpha\n        self.beta = beta\n\n    def add_inst(self,\n            cand,\n            ref_list,\n            yn_label=None, yn_ref=None, entity_ref=None):\n        #super(BLEUWithBonus, self).add_inst(cand, ref_list)\n        BLEU.add_inst(self, cand, ref_list)\n        if yn_label is not None and yn_ref is not None:\n            self.add_yn_bonus(cand, ref_list, yn_label, yn_ref)\n        elif entity_ref is not None:\n            self.add_entity_bonus(cand, entity_ref)\n\n    def add_yn_bonus(self, cand, ref_list, yn_label, yn_ref):\n        for n_size in range(self.n_size):\n            cand_ngram = common.get_ngram(cand, n_size, label=yn_label)\n            ref_ngram = []\n            for ref_id, r in enumerate(yn_ref):\n                ref_ngram.append(common.get_ngram(ref_list[ref_id], n_size, label=r))\n            match_size, cand_size = common.get_match_size(cand_ngram, ref_ngram)\n            self.match_ngram[n_size] += self.alpha * match_size\n            self.candi_ngram[n_size] += self.alpha * match_size\n\n    def add_entity_bonus(self, cand, entity_ref):\n        for n_size in range(self.n_size):\n            cand_ngram = common.get_ngram(cand, n_size, label='ENTITY')\n            ref_ngram = []\n            for reff_id, r in enumerate(entity_ref):\n                ref_ngram.append(common.get_ngram(r, n_size, label='ENTITY'))\n            match_size, cand_size = common.get_match_size(cand_ngram, ref_ngram)\n            self.match_ngram[n_size] += self.beta * match_size\n            self.candi_ngram[n_size] += self.beta * match_size\n"""
Dureader/metric/common.py,0,"b""# coding:utf8\nfrom functools import reduce\nimport math\nimport json\nfrom collections import defaultdict\nimport sys\n\ndef get_match_size(cand_ngram, refs_ngram):\n    ref_set = defaultdict(int)\n    for ref_ngram in refs_ngram:\n        tmp_ref_set = defaultdict(int)\n        for ngram in ref_ngram:\n            tmp_ref_set[ngram] += 1\n        for ngram, count in tmp_ref_set.items():\n            ref_set[ngram] = max(ref_set[ngram], count)\n    cand_set = defaultdict(int)\n    for ngram in cand_ngram:\n        cand_set[ngram] += 1\n    match_size = 0\n    for ngram, count in cand_set.items():\n        match_size += min(count, ref_set.get(ngram, 0))\n    cand_size = len(cand_ngram)\n    return match_size, cand_size\n\ndef get_ngram(sent, n_size, label=None):\n    def _ngram(sent, n_size):\n        ngram_list = []\n        for left in range(len(sent) - n_size):\n            ngram_list.append(sent[left: left + n_size + 1])\n        return ngram_list\n\n    ngram_list = _ngram(sent, n_size)\n    if label is not None:\n        ngram_list = [ngram + '_' + label for ngram in ngram_list]\n    return ngram_list\n\ndef word2char(str_in):\n    str_out = str_in.replace(' ', '')\n    return ''.join(str_out.split())\n"""
Dureader/metric/mrc_eval.py,0,"b'# coding:utf8\n""""""\nThis module computes evaluation metrics for DuReader dataset.\n""""""\n\nimport argparse\nimport itertools\nimport json\nimport sys\nimport importlib\nimport zipfile\n\nfrom collections import Counter\nfrom bleu import BLEUWithBonus\nfrom rouge import RougeL\n\nEMPTY = \'\'\nYESNO_LABELS = set([\'Yes\', \'No\', \'Depends\'])\n\ndef normalize(s):\n    """"""\n    Normalize strings to space joined chars.\n    Args:\n        s: a list of strings.\n    Returns:\n        A list of normalized strings.\n    """"""\n    if not s:\n        return s\n    normalized = []\n    for ss in s:\n        tokens = [c for c in list(ss) if len(c.strip()) != 0]\n        norm_s = \'\'.join(tokens)\n        norm_s = norm_s.replace(u""\xef\xbc\x8c"", u"","")\n        norm_s = norm_s.replace(u""\xe3\x80\x82"", u""."")\n        norm_s = norm_s.replace(u""\xef\xbc\x81"", u""!"")\n        norm_s = norm_s.replace(u""\xef\xbc\x9f"", u""?"")\n        norm_s = norm_s.replace(u""\xef\xbc\x9b"", u"";"")\n        norm_s = norm_s.replace(u""\xef\xbc\x88"", u""("").replace(u""\xef\xbc\x89"", u"")"")\n        norm_s = norm_s.replace(u""\xe3\x80\x90"", u""["").replace(u""\xe3\x80\x91"", u""]"")\n        norm_s = norm_s.replace(u""\xe2\x80\x9c"", u""\\"""").replace(u""\xe2\x80\x9c"", u""\\"""")\n        normalized.append(norm_s)\n    return normalized\n\n\ndef data_check(obj):\n    """"""\n    Check data.\n\n    Raises:\n        Raises AssertionError when data is not legal.\n    """"""\n    assert \'question_id\' in obj, ""Missing \'question_id\' field.""\n    #assert \'question_type\' in obj, \\\n    #        ""Missing \'question_type\' field. question_id: {}"".format(obj[\'question_type\'])\n\n    #assert \'yesno_answers\' in obj, \\\n    #        ""Missing \'yesno_answers\' field. question_id: {}"".format(obj[\'question_id\'])\n    if ""yesno_answers"" in obj:\n        assert isinstance(obj[\'yesno_answers\'], list), \\\n            r""""""\'yesno_answers\' field must be a list, if the \'question_type\' is not\n            \'YES_NO\', then this field should be an empty list.\n            question_id: {}"""""".format(obj[\'question_id\'])\n    else:\n        obj[""yesno_answers""] = []\n\n    if ""entity_answers"" not in obj:\n        obj[""entity_answers""] = []\n\n\ndef read_file(file_name, is_ref=False):\n    """"""\n    Read predict answers or reference answers from file.\n\n    Args:\n        file_name: the name of the file containing predict result or reference\n                   result.\n\n    Returns:\n        A dictionary mapping question_id to the result information. The result\n        information itself is also a dictionary with has four keys:\n        - question_type: type of the query.\n        - yesno_answers: A list of yesno answers corresponding to \'answers\'.\n        - answers: A list of predicted answers.\n        - entity_answers: A list, each element is also a list containing the entities\n                    tagged out from the corresponding answer string.\n    """"""\n    def _open(file_name, mode, zip_obj=None):\n        if zip_obj is not None:\n            return zip_obj.open(file_name, mode)\n        return open(file_name, mode, encoding=\'utf-8\')\n\n    results = {}\n    if is_ref:\n#        keys = [\'source\', \'answers\', \'yesno_answers\', \'entity_answers\', \'question_type\']\n        keys = [\'answers\', \'yesno_answers\', \'entity_answers\', \'question_type\']\n    else:\n        keys = [\'answers\', \'yesno_answers\', \'entity_answers\', \'question_type\'] \n    try:\n        zf = zipfile.ZipFile(file_name, \'r\') if file_name.endswith(\'.zip\') else None\n    except:\n        zf = None\n    file_list = [file_name] if zf is None else zf.namelist()\n\n    for fn in file_list:\n        line_num = 0\n        for line in _open(fn, \'r\', zip_obj=zf):\n            try:\n                line_num += 1\n                obj = json.loads(line.strip())\n            except ValueError:\n                #raise ValueError(""Every line of data should be legal json, in line %s"" % str(line_num))\n                print >> sys.stderr, ValueError(""Every line of data should be legal json, in line %s"" % str(line_num))\n                continue\n            data_check(obj)\n            qid = obj[\'question_id\']\n            assert qid not in results, ""Duplicate question_id: {}"".format(qid)\n            results[qid] = {}\n            for k in keys:\n                if k == \'answers\':\n                    results[qid][k] = normalize(obj[k])\n                else:\n                    results[qid][k] = obj[k]\n            if is_ref:\n                for i, e in enumerate(results[qid][\'entity_answers\']):\n                    results[qid][\'entity_answers\'][i] = normalize(e)\n    return results\n\ndef main(args):\n    err = None\n    metrics = {}\n    bleu4, rouge_l = 0.0, 0.0\n    alpha = args.ab\n    beta = args.ab\n    bleu_eval = BLEUWithBonus(4, alpha=alpha, beta=beta)\n    rouge_eval = RougeL(alpha=alpha, beta=beta, gamma=1.2)\n    try:\n        pred_result = read_file(args.pred_file)\n        ref_result = read_file(args.ref_file, is_ref=True)\n        for qid, results in ref_result.items():\n            cand_result = pred_result.get(qid, {})\n            #pred_answers = cand_result.get(\'answers\', [EMPTY])[0]\n            pred_answers = cand_result.get(\'answers\', [])\n            if not pred_answers:\n                pred_answers = EMPTY\n            else:\n                pred_answers = pred_answers[0]\n            pred_yn_label = None\n            ref_entities = None\n            ref_answers = results.get(\'answers\', [])\n            if not ref_answers:\n                continue\n            if results[\'question_type\'] == \'ENTITY\':\n                ref_entities = set(\n                        itertools.chain(*results.get(\'entity_answers\', [[]])))\n                with open(\'e.data\',\'a\', encoding=\'utf-8\') as f:\n                    f.write(str(ref_entities) + \'\\n\')\n                if not ref_entities:\n                    ref_entities = None\n            if results[\'question_type\'] == \'YES_NO\':\n                cand_yesno = cand_result.get(\'yesno_answers\', [])\n                pred_yn_label = None if len(cand_yesno) == 0 \\\n                        else cand_yesno[0]\n            bleu_eval.add_inst(\n                    pred_answers,\n                    ref_answers,\n                    yn_label=pred_yn_label,\n                    yn_ref=results[\'yesno_answers\'],\n                    entity_ref=ref_entities)\n            rouge_eval.add_inst(\n                    pred_answers,\n                    ref_answers,\n                    yn_label=pred_yn_label,\n                    yn_ref=results[\'yesno_answers\'],\n                    entity_ref=ref_entities)\n        bleu4 = bleu_eval.score()[-1]\n        rouge_l = rouge_eval.score()\n    except ValueError as ve:\n        err = ve\n    except AssertionError as ae:\n        err = ae\n    # too keep compatible to leaderboard evaluation.\n    metrics[\'errorMsg\'] = \'success\' if err is None else err\n    metrics[\'errorCode\'] = 0 if err is None else 1\n    metrics[\'data\'] = [\n            {\'type\': \'BOTH\', \'name\': \'ROUGE-L\', \'value\': round(rouge_l* 100, 2)},\n            {\'type\': \'BOTH\', \'name\': \'BLEU-4\', \'value\': round(bleu4 * 100, 2)},\n            ]\n    print(json.dumps(metrics, ensure_ascii=False).encode(\'utf8\'))\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'pred_file\', help=\'predict file\')\n    parser.add_argument(\'ref_file\', help=\'reference file\')\n    parser.add_argument(\'task\',\n            help=\'task name, only to keep compatible with leaderboard eval\')\n    parser.add_argument(\'--ab\', type=float, default=1.0,\n            help=\'common value of alpha and beta\')\n    args = parser.parse_args()\n    main(args)\n'"
Dureader/metric/rouge.py,0,"b'# coding:utf8\n\nfrom functools import reduce\nimport math\nimport json\nimport numpy as np\nfrom collections import defaultdict\nimport sys\nimport importlib\nimportlib.reload(sys)\n\n\nclass RougeL(object):\n    def __init__(self, alpha=1.0, beta=1.0, gamma=1.2):\n        self.alpha = alpha\n        self.beta = beta\n        self.gamma = gamma\n        self.inst_scores = []\n\n    def lcs(self, string, sub):\n        if len(string) < len(sub):\n            sub, string = string, sub\n        lengths = np.zeros((len(string) + 1, len(sub) + 1))\n        for j in range(1, len(sub) + 1):\n            for i in range(1, len(string) + 1):\n                if string[i - 1] == sub[j - 1]:\n                    lengths[i][j] = lengths[i - 1][j - 1] + 1\n                else:\n                    lengths[i][j] = max(lengths[i - 1][j], lengths[i][j - 1])\n        return lengths[len(string)][len(sub)]\n\n    def add_inst(self,\n            cand,\n            ref_list,\n            yn_label=None, yn_ref=None, entity_ref=None):\n        precs, recalls = [], []\n        for i, ref in enumerate(ref_list):\n            basic_lcs = self.lcs(cand, ref)\n            yn_bonus, entity_bonus = 0.0, 0.0\n            if yn_ref is not None and yn_label is not None:\n                yn_bonus = self.add_yn_bonus(cand, ref, yn_label, yn_ref[i])\n            elif entity_ref is not None:\n                entity_bonus = self.add_entity_bonus(cand, entity_ref)\n            p_denom = len(cand) + self.alpha * yn_bonus + self.beta * entity_bonus\n            r_denom = len(ref) + self.alpha * yn_bonus + self.beta * entity_bonus\n            prec = (basic_lcs + self.alpha * yn_bonus + self.beta * entity_bonus) \\\n                    / p_denom if p_denom > 0. else 0.\n            rec = (basic_lcs + self.alpha * yn_bonus + self.beta * entity_bonus) \\\n                    / r_denom if r_denom > 0. else 0.\n            precs.append(prec)\n            recalls.append(rec)\n\n        prec_max = max(precs)\n        rec_max = max(recalls)\n        if prec_max != 0 and rec_max != 0:\n            score = ((1 + self.gamma**2) * prec_max * rec_max) / \\\n                    float(rec_max + self.gamma**2 * prec_max)\n        else:\n            score = 0.0\n        self.inst_scores.append(score)\n\n    def add_yn_bonus(self, cand, ref, yn_label, yn_ref):\n        if yn_label != yn_ref:\n            return 0.0\n        lcs_ = self.lcs(cand, ref)\n        return lcs_\n\n    def add_entity_bonus(self, cand, entity_ref):\n        lcs_ = 0.0\n        for ent in entity_ref:\n            if ent in cand:\n                lcs_ += len(ent)\n        return lcs_\n\n    def score(self):\n        return 1. * sum(self.inst_scores) / len(self.inst_scores)\n'"
Dureader/metric/test.py,0,"b'import json\n\nwith open(""./ref.json"", \'r\', encoding=\'utf-8\') as f:\n    for line in f.readlines():\n        source = json.loads(line.strip())\nprint(""######################## end #################################"")\n    \n'"
Dureader/model_dir/__init__.py,0,b''
Dureader/model_dir/modeling.py,69,"b'import copy\nimport json\nimport logging\nimport math\nimport os\nimport args\nimport shutil\nimport tarfile\nimport tempfile\nfrom io import open\nimport torch.nn.functional as F\nimport torch\nfrom torch import nn\nfrom torch.nn import CrossEntropyLoss\n\nimport sys\n#sys.path.append(\'../dataset\'); \nfrom dataset.file_utils import cached_path\n\nlogger = logging.getLogger(__name__)\n\nPRETRAINED_MODEL_ARCHIVE_MAP = {\n    \'bert-base-uncased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz"",\n    \'bert-large-uncased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased.tar.gz"",\n    \'bert-base-cased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased.tar.gz"",\n    \'bert-large-cased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased.tar.gz"",\n    \'bert-base-multilingual-uncased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased.tar.gz"",\n    \'bert-base-multilingual-cased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased.tar.gz"",\n    \'bert-base-chinese\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese.tar.gz"",\n}\nCONFIG_NAME = \'bert_config.json\'\nWEIGHTS_NAME = \'pytorch_model.bin\'\nTF_WEIGHTS_NAME = \'model.ckpt\'\n\ndef load_tf_weights_in_bert(model, tf_checkpoint_path):\n    """""" Load tf checkpoints in a pytorch model\n    """"""\n    try:\n        import re\n        import numpy as np\n        import tensorflow as tf\n    except ImportError:\n        print(""Loading a TensorFlow models in PyTorch, requires TensorFlow to be installed. Please see ""\n            ""https://www.tensorflow.org/install/ for installation instructions."")\n        raise\n    tf_path = os.path.abspath(tf_checkpoint_path)\n    print(""Converting TensorFlow checkpoint from {}"".format(tf_path))\n    # Load weights from TF model\n    init_vars = tf.train.list_variables(tf_path)\n    names = []\n    arrays = []\n    for name, shape in init_vars:\n        print(""Loading TF weight {} with shape {}"".format(name, shape))\n        array = tf.train.load_variable(tf_path, name)\n        names.append(name)\n        arrays.append(array)\n\n    for name, array in zip(names, arrays):\n        name = name.split(\'/\')\n        # adam_v and adam_m are variables used in AdamWeightDecayOptimizer to calculated m and v\n        # which are not required for using pretrained model\n        if any(n in [""adam_v"", ""adam_m""] for n in name):\n            print(""Skipping {}"".format(""/"".join(name)))\n            continue\n        pointer = model\n        for m_name in name:\n            if re.fullmatch(r\'[A-Za-z]+_\\d+\', m_name):\n                l = re.split(r\'_(\\d+)\', m_name)\n            else:\n                l = [m_name]\n            if l[0] == \'kernel\' or l[0] == \'gamma\':\n                pointer = getattr(pointer, \'weight\')\n            elif l[0] == \'output_bias\' or l[0] == \'beta\':\n                pointer = getattr(pointer, \'bias\')\n            elif l[0] == \'output_weights\':\n                pointer = getattr(pointer, \'weight\')\n            else:\n                pointer = getattr(pointer, l[0])\n            if len(l) >= 2:\n                num = int(l[1])\n                pointer = pointer[num]\n        if m_name[-11:] == \'_embeddings\':\n            pointer = getattr(pointer, \'weight\')\n        elif m_name == \'kernel\':\n            array = np.transpose(array)\n        try:\n            assert pointer.shape == array.shape\n        except AssertionError as e:\n            e.args += (pointer.shape, array.shape)\n            raise\n        print(""Initialize PyTorch weight {}"".format(name))\n        pointer.data = torch.from_numpy(array)\n    return model\n\n\ndef gelu(x):\n    """"""Implementation of the gelu activation function.\n        For information: OpenAI GPT\'s gelu is slightly different (and gives slightly different results):\n        0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n        Also see https://arxiv.org/abs/1606.08415\n    """"""\n    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n\n\ndef swish(x):\n    return x * torch.sigmoid(x)\n\n\nACT2FN = {""gelu"": gelu, ""relu"": torch.nn.functional.relu, ""swish"": swish}\n\n\nclass BertConfig(object):\n    """"""Configuration class to store the configuration of a `BertModel`.\n    """"""\n    def __init__(self,\n                 vocab_size_or_config_json_file,\n                 hidden_size=768,\n                 num_hidden_layers=12,\n                 num_attention_heads=12,\n                 intermediate_size=3072,\n                 hidden_act=""gelu"",\n                 hidden_dropout_prob=0.1,\n                 attention_probs_dropout_prob=0.1,\n                 max_position_embeddings=512,\n                 type_vocab_size=2,\n                 initializer_range=0.02):\n        """"""Constructs BertConfig.\n        Args:\n            vocab_size_or_config_json_file: Vocabulary size of `inputs_ids` in `BertModel`.\n            hidden_size: Size of the encoder layers and the pooler layer.\n            num_hidden_layers: Number of hidden layers in the Transformer encoder.\n            num_attention_heads: Number of attention heads for each attention layer in\n                the Transformer encoder.\n            intermediate_size: The size of the ""intermediate"" (i.e., feed-forward)\n                layer in the Transformer encoder.\n            hidden_act: The non-linear activation function (function or string) in the\n                encoder and pooler. If string, ""gelu"", ""relu"" and ""swish"" are supported.\n            hidden_dropout_prob: The dropout probabilitiy for all fully connected\n                layers in the embeddings, encoder, and pooler.\n            attention_probs_dropout_prob: The dropout ratio for the attention\n                probabilities.\n            max_position_embeddings: The maximum sequence length that this model might\n                ever be used with. Typically set this to something large just in case\n                (e.g., 512 or 1024 or 2048).\n            type_vocab_size: The vocabulary size of the `token_type_ids` passed into\n                `BertModel`.\n            initializer_range: The sttdev of the truncated_normal_initializer for\n                initializing all weight matrices.\n        """"""\n        if isinstance(vocab_size_or_config_json_file, str) or (sys.version_info[0] == 2\n                        and isinstance(vocab_size_or_config_json_file, unicode)):\n            with open(vocab_size_or_config_json_file, ""r"", encoding=\'utf-8\') as reader:\n                json_config = json.loads(reader.read())\n            for key, value in json_config.items():\n                self.__dict__[key] = value\n        elif isinstance(vocab_size_or_config_json_file, int):\n            self.vocab_size = vocab_size_or_config_json_file\n            self.hidden_size = hidden_size\n            self.num_hidden_layers = num_hidden_layers\n            self.num_attention_heads = num_attention_heads\n            self.hidden_act = hidden_act\n            self.intermediate_size = intermediate_size\n            self.hidden_dropout_prob = hidden_dropout_prob\n            self.attention_probs_dropout_prob = attention_probs_dropout_prob\n            self.max_position_embeddings = max_position_embeddings\n            self.type_vocab_size = type_vocab_size\n            self.initializer_range = initializer_range\n        else:\n            raise ValueError(""First argument must be either a vocabulary size (int)""\n                             ""or the path to a pretrained model config file (str)"")\n\n    @classmethod\n    def from_dict(cls, json_object):\n        """"""Constructs a `BertConfig` from a Python dictionary of parameters.""""""\n        config = BertConfig(vocab_size_or_config_json_file=-1)\n        for key, value in json_object.items():\n            config.__dict__[key] = value\n        return config\n\n    @classmethod\n    def from_json_file(cls, json_file):\n        """"""Constructs a `BertConfig` from a json file of parameters.""""""\n        with open(json_file, ""r"", encoding=\'utf-8\') as reader:\n            text = reader.read()\n        return cls.from_dict(json.loads(text))\n\n    def __repr__(self):\n        return str(self.to_json_string())\n\n    def to_dict(self):\n        """"""Serializes this instance to a Python dictionary.""""""\n        output = copy.deepcopy(self.__dict__)\n        return output\n\n    def to_json_string(self):\n        """"""Serializes this instance to a JSON string.""""""\n        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + ""\\n""\n\ntry:\n    from apex.normalization.fused_layer_norm import FusedLayerNorm as BertLayerNorm\nexcept ImportError:\n    logger.info(""Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex ."")\n    class BertLayerNorm(nn.Module):\n        def __init__(self, hidden_size, eps=1e-12):\n            """"""Construct a layernorm module in the TF style (epsilon inside the square root).\n            """"""\n            super(BertLayerNorm, self).__init__()\n            self.weight = nn.Parameter(torch.ones(hidden_size))\n            self.bias = nn.Parameter(torch.zeros(hidden_size))\n            self.variance_epsilon = eps\n\n        def forward(self, x):\n            u = x.mean(-1, keepdim=True)\n            s = (x - u).pow(2).mean(-1, keepdim=True)\n            x = (x - u) / torch.sqrt(s + self.variance_epsilon)\n            return self.weight * x + self.bias\n\nclass BertEmbeddings(nn.Module):\n    """"""Construct the embeddings from word, position and token_type embeddings.\n    """"""\n    def __init__(self, config):\n        super(BertEmbeddings, self).__init__()\n\n        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=0)\n        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size, padding_idx=0)\n        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size, padding_idx=0)\n\n        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n    def forward(self, input_ids, token_type_ids=None):\n        seq_length = input_ids.size(1)\n       \n        position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)\n        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n        if token_type_ids is None:\n            token_type_ids = torch.zeros_like(input_ids)\n   \n        words_embeddings = self.word_embeddings(input_ids)\n        position_embeddings = self.position_embeddings(position_ids)\n        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n   \n        embeddings = words_embeddings + token_type_embeddings + position_embeddings #wiq_embeddings + position_embeddings\n        embeddings = self.LayerNorm(embeddings)\n        embeddings = self.dropout(embeddings)\n  \n        return embeddings\n\n\nclass BertSelfAttention(nn.Module):\n    def __init__(self, config):\n        super(BertSelfAttention, self).__init__()\n        if config.hidden_size % config.num_attention_heads != 0:\n            raise ValueError(\n                ""The hidden size (%d) is not a multiple of the number of attention ""\n                ""heads (%d)"" % (config.hidden_size, config.num_attention_heads))\n        self.num_attention_heads = config.num_attention_heads\n        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n        self.all_head_size = self.num_attention_heads * self.attention_head_size\n\n        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n\n        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n\n    def transpose_for_scores(self, x):\n        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n        x = x.view(*new_x_shape)\n        return x.permute(0, 2, 1, 3)\n\n    def forward(self, hidden_states, attention_mask):\n        mixed_query_layer = self.query(hidden_states)\n        mixed_key_layer = self.key(hidden_states)\n        mixed_value_layer = self.value(hidden_states)\n\n        query_layer = self.transpose_for_scores(mixed_query_layer)\n        key_layer = self.transpose_for_scores(mixed_key_layer)\n        value_layer = self.transpose_for_scores(mixed_value_layer)\n\n        # Take the dot product between ""query"" and ""key"" to get the raw attention scores.\n        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n        # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n        attention_scores = attention_scores + attention_mask\n\n        # Normalize the attention scores to probabilities.\n        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n\n        # This is actually dropping out entire tokens to attend to, which might\n        # seem a bit unusual, but is taken from the original Transformer paper.\n        attention_probs = self.dropout(attention_probs)\n\n        context_layer = torch.matmul(attention_probs, value_layer)\n        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n        context_layer = context_layer.view(*new_context_layer_shape)\n        return context_layer\n\n\nclass BertSelfOutput(nn.Module):\n    def __init__(self, config):\n        super(BertSelfOutput, self).__init__()\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n    def forward(self, hidden_states, input_tensor):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n        return hidden_states\n\n\nclass BertAttention(nn.Module):\n    def __init__(self, config):\n        super(BertAttention, self).__init__()\n        self.self = BertSelfAttention(config)\n        self.output = BertSelfOutput(config)\n\n    def forward(self, input_tensor, attention_mask):\n        self_output = self.self(input_tensor, attention_mask)\n        attention_output = self.output(self_output, input_tensor)\n        return attention_output\n\n\nclass BertIntermediate(nn.Module):\n    def __init__(self, config):\n        super(BertIntermediate, self).__init__()\n        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n        if isinstance(config.hidden_act, str) or (sys.version_info[0] == 2 and isinstance(config.hidden_act, unicode)):\n            self.intermediate_act_fn = ACT2FN[config.hidden_act]\n        else:\n            self.intermediate_act_fn = config.hidden_act\n\n    def forward(self, hidden_states):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.intermediate_act_fn(hidden_states)\n        return hidden_states\n\n\nclass BertOutput(nn.Module):\n    def __init__(self, config):\n        super(BertOutput, self).__init__()\n        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n    def forward(self, hidden_states, input_tensor):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n        return hidden_states\n\n\nclass BertLayer(nn.Module):\n    def __init__(self, config):\n        super(BertLayer, self).__init__()\n        self.attention = BertAttention(config)\n        self.intermediate = BertIntermediate(config)\n        self.output = BertOutput(config)\n\n    def forward(self, hidden_states, attention_mask):\n        attention_output = self.attention(hidden_states, attention_mask)\n        intermediate_output = self.intermediate(attention_output)\n        layer_output = self.output(intermediate_output, attention_output)\n        return layer_output\n\n\nclass BertEncoder(nn.Module):\n    def __init__(self, config):\n        super(BertEncoder, self).__init__()\n        layer = BertLayer(config)\n        self.layer = nn.ModuleList([copy.deepcopy(layer) for _ in range(config.num_hidden_layers)])\n\n    def forward(self, hidden_states, attention_mask, output_all_encoded_layers=True):\n        all_encoder_layers = []\n        for layer_module in self.layer:\n            hidden_states = layer_module(hidden_states, attention_mask)\n            if output_all_encoded_layers:\n                all_encoder_layers.append(hidden_states)\n        if not output_all_encoded_layers:\n            all_encoder_layers.append(hidden_states)\n        return all_encoder_layers\n\n\nclass BertPooler(nn.Module):\n    def __init__(self, config):\n        super(BertPooler, self).__init__()\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.activation = nn.Tanh()\n\n    def forward(self, hidden_states):\n        # We ""pool"" the model by simply taking the hidden state corresponding\n        # to the first token.\n        first_token_tensor = hidden_states[:, 0]\n        pooled_output = self.dense(first_token_tensor)\n        pooled_output = self.activation(pooled_output)\n        return pooled_output\n\n\nclass BertPredictionHeadTransform(nn.Module):\n    def __init__(self, config):\n        super(BertPredictionHeadTransform, self).__init__()\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        if isinstance(config.hidden_act, str) or (sys.version_info[0] == 2 and isinstance(config.hidden_act, unicode)):\n            self.transform_act_fn = ACT2FN[config.hidden_act]\n        else:\n            self.transform_act_fn = config.hidden_act\n        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\n\n    def forward(self, hidden_states):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.transform_act_fn(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states)\n        return hidden_states\n\n\nclass BertLMPredictionHead(nn.Module):\n    def __init__(self, config, bert_model_embedding_weights):\n        super(BertLMPredictionHead, self).__init__()\n        self.transform = BertPredictionHeadTransform(config)\n\n        # The output weights are the same as the input embeddings, but there is\n        # an output-only bias for each token.\n        self.decoder = nn.Linear(bert_model_embedding_weights.size(1),\n                                 bert_model_embedding_weights.size(0),\n                                 bias=False)\n        self.decoder.weight = bert_model_embedding_weights\n        self.bias = nn.Parameter(torch.zeros(bert_model_embedding_weights.size(0)))\n\n    def forward(self, hidden_states):\n        hidden_states = self.transform(hidden_states)\n        hidden_states = self.decoder(hidden_states) + self.bias\n        return hidden_states\n\n\nclass BertOnlyMLMHead(nn.Module):\n    def __init__(self, config, bert_model_embedding_weights):\n        super(BertOnlyMLMHead, self).__init__()\n        self.predictions = BertLMPredictionHead(config, bert_model_embedding_weights)\n\n    def forward(self, sequence_output):\n        prediction_scores = self.predictions(sequence_output)\n        return prediction_scores\n\n\nclass BertOnlyNSPHead(nn.Module):\n    def __init__(self, config):\n        super(BertOnlyNSPHead, self).__init__()\n        self.seq_relationship = nn.Linear(config.hidden_size, 2)\n\n    def forward(self, pooled_output):\n        seq_relationship_score = self.seq_relationship(pooled_output)\n        return seq_relationship_score\n\n\nclass BertPreTrainingHeads(nn.Module):\n    def __init__(self, config, bert_model_embedding_weights):\n        super(BertPreTrainingHeads, self).__init__()\n        self.predictions = BertLMPredictionHead(config, bert_model_embedding_weights)\n        self.seq_relationship = nn.Linear(config.hidden_size, 2)\n\n    def forward(self, sequence_output, pooled_output):\n        prediction_scores = self.predictions(sequence_output)\n        seq_relationship_score = self.seq_relationship(pooled_output)\n        return prediction_scores, seq_relationship_score\n\n\nclass BertPreTrainedModel(nn.Module):\n    """""" An abstract class to handle weights initialization and\n        a simple interface for dowloading and loading pretrained models.\n    """"""\n    def __init__(self, config, *inputs, **kwargs):\n        super(BertPreTrainedModel, self).__init__()\n        if not isinstance(config, BertConfig):\n            raise ValueError(\n                ""Parameter config in `{}(config)` should be an instance of class `BertConfig`. ""\n                ""To create a model from a Google pretrained model use ""\n                ""`model = {}.from_pretrained(PRETRAINED_MODEL_NAME)`"".format(\n                    self.__class__.__name__, self.__class__.__name__\n                ))\n        self.config = config\n\n    def init_bert_weights(self, module):\n        if isinstance(module, (nn.Linear, nn.Embedding)):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n        elif isinstance(module, BertLayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        if isinstance(module, nn.Linear) and module.bias is not None:\n            module.bias.data.zero_()\n\n    @classmethod\n    def from_pretrained(cls, pretrained_model_name_or_path, state_dict=None, cache_dir=None, from_tf=False, *inputs, **kwargs):\n        """"""\n        Instantiate a BertPreTrainedModel from a pre-trained model file or a pytorch state dict.\n        Download and cache the pre-trained model file if needed.\n        Params:\n            pretrained_model_name_or_path: either:\n                - a str with the name of a pre-trained model to load selected in the list of:\n                    . `bert-base-uncased`\n                    . `bert-large-uncased`\n                    . `bert-base-cased`\n                    . `bert-large-cased`\n                    . `bert-base-multilingual-uncased`\n                    . `bert-base-multilingual-cased`\n                    . `bert-base-chinese`\n                - a path or url to a pretrained model archive containing:\n                    . `bert_config.json` a configuration file for the model\n                    . `pytorch_model.bin` a PyTorch dump of a BertForPreTraining instance\n                - a path or url to a pretrained model archive containing:\n                    . `bert_config.json` a configuration file for the model\n                    . `model.chkpt` a TensorFlow checkpoint\n            from_tf: should we load the weights from a locally saved TensorFlow checkpoint\n            cache_dir: an optional path to a folder in which the pre-trained models will be cached.\n            state_dict: an optional state dictionnary (collections.OrderedDict object) to use instead of Google pre-trained models\n            *inputs, **kwargs: additional input for the specific Bert class\n                (ex: num_labels for BertForSequenceClassification)\n        """"""\n        if pretrained_model_name_or_path in PRETRAINED_MODEL_ARCHIVE_MAP:\n            archive_file = PRETRAINED_MODEL_ARCHIVE_MAP[pretrained_model_name_or_path]\n        else:\n            archive_file = pretrained_model_name_or_path\n        # redirect to the cache, if necessary\n        try:\n            resolved_archive_file = cached_path(archive_file, cache_dir=cache_dir)\n        except EnvironmentError:\n            logger.error(\n                ""Model name \'{}\' was not found in model name list ({}). ""\n                ""We assumed \'{}\' was a path or url but couldn\'t find any file ""\n                ""associated to this path or url."".format(\n                    pretrained_model_name_or_path,\n                    \', \'.join(PRETRAINED_MODEL_ARCHIVE_MAP.keys()),\n                    archive_file))\n            return None\n        if resolved_archive_file == archive_file:\n            print(""loading archive file {}"".format(archive_file))\n        else:\n            print(""loading archive file {} from cache at {}"".format(\n                archive_file, resolved_archive_file))\n        tempdir = None\n        if os.path.isdir(resolved_archive_file) or from_tf:\n            serialization_dir = resolved_archive_file\n        else:\n            # Extract archive to temp dir\n            tempdir = tempfile.mkdtemp()\n            print(""extracting archive file {} to temp dir {}"".format(\n                resolved_archive_file, tempdir))\n            with tarfile.open(resolved_archive_file, \'r:gz\') as archive:\n                archive.extractall(tempdir)\n            serialization_dir = tempdir\n        # Load config\n        config_file = os.path.join(serialization_dir, CONFIG_NAME)\n        config = BertConfig.from_json_file(config_file)\n        logger.info(""Model config {}"".format(config))\n        # Instantiate model.\n        model = cls(config, *inputs, **kwargs)\n        if state_dict is None and not from_tf:\n            weights_path = os.path.join(serialization_dir, WEIGHTS_NAME)\n            print(\'load state_dict\' + str(weights_path))\n            state_dict = torch.load(weights_path, map_location=\'cpu\' if not torch.cuda.is_available() else None)\n        if tempdir:\n            # Clean up temp dir\n            shutil.rmtree(tempdir)\n        if from_tf:\n            # Directly load from a TensorFlow checkpoint\n            weights_path = os.path.join(serialization_dir, TF_WEIGHTS_NAME)\n            return load_tf_weights_in_bert(model, weights_path)\n        # Load from a PyTorch state_dict\n        old_keys = []\n        new_keys = []\n        for key in state_dict.keys():\n            new_key = None\n            if \'gamma\' in key:\n                new_key = key.replace(\'gamma\', \'weight\')\n            if \'beta\' in key:\n                new_key = key.replace(\'beta\', \'bias\')\n            if new_key:\n                old_keys.append(key)\n                new_keys.append(new_key)\n        for old_key, new_key in zip(old_keys, new_keys):\n            state_dict[new_key] = state_dict.pop(old_key)\n\n        missing_keys = []\n        unexpected_keys = []\n        error_msgs = []\n        # copy state_dict so _load_from_state_dict can modify it\n        metadata = getattr(state_dict, \'_metadata\', None)\n        state_dict = state_dict.copy()\n        if metadata is not None:\n            state_dict._metadata = metadata\n\n        def load(module, prefix=\'\'):\n            local_metadata = {} if metadata is None else metadata.get(prefix[:-1], {})\n            module._load_from_state_dict(\n                state_dict, prefix, local_metadata, True, missing_keys, unexpected_keys, error_msgs)\n            for name, child in module._modules.items():\n                if child is not None:\n                    load(child, prefix + name + \'.\')\n        start_prefix = \'\'\n        if not hasattr(model, \'bert\') and any(s.startswith(\'bert.\') for s in state_dict.keys()):\n            start_prefix = \'bert.\'\n        load(model, prefix=start_prefix)\n\n        if len(missing_keys) > 0:\n            logger.info(""Weights of {} not initialized from pretrained model: {}"".format(\n                model.__class__.__name__, missing_keys))\n        if len(unexpected_keys) > 0:\n            logger.info(""Weights from pretrained model not used in {}: {}"".format(\n                model.__class__.__name__, unexpected_keys))\n        if len(error_msgs) > 0:\n            raise RuntimeError(\'Error(s) in loading state_dict for {}:\\n\\t{}\'.format(\n                               model.__class__.__name__, ""\\n\\t"".join(error_msgs)))\n        return model\n\n\nclass BertModel(BertPreTrainedModel):\n    """"""BERT model (""Bidirectional Embedding Representations from a Transformer"").\n    Params:\n        config: a BertConfig class instance with the configuration to build a new model\n    Inputs:\n        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n            a `sentence B` token (see BERT paper for more details).\n        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n            selected in [0, 1]. It\'s a mask to be used if the input sequence length is smaller than the max\n            input sequence length in the current batch. It\'s the mask that we typically use for attention when\n            a batch has varying length sentences.\n        `output_all_encoded_layers`: boolean which controls the content of the `encoded_layers` output as described below. Default: `True`.\n    Outputs: Tuple of (encoded_layers, pooled_output)\n        `encoded_layers`: controled by `output_all_encoded_layers` argument:\n            - `output_all_encoded_layers=True`: outputs a list of the full sequences of encoded-hidden-states at the end\n                of each attention block (i.e. 12 full sequences for BERT-base, 24 for BERT-large), each\n                encoded-hidden-state is a torch.FloatTensor of size [batch_size, sequence_length, hidden_size],\n            - `output_all_encoded_layers=False`: outputs only the full sequence of hidden-states corresponding\n                to the last attention block of shape [batch_size, sequence_length, hidden_size],\n        `pooled_output`: a torch.FloatTensor of size [batch_size, hidden_size] which is the output of a\n            classifier pretrained on top of the hidden state associated to the first character of the\n            input (`CLS`) to train on the Next-Sentence task (see BERT\'s paper).\n    Example usage:\n    ```python\n    # Already been converted into WordPiece token ids\n    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n    config = modeling.BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n    model = modeling.BertModel(config=config)\n    all_encoder_layers, pooled_output = model(input_ids, token_type_ids, input_mask)\n    ```\n    """"""\n    def __init__(self, config):\n        super(BertModel, self).__init__(config)\n        self.embeddings = BertEmbeddings(config)\n        self.encoder = BertEncoder(config)\n        self.pooler = BertPooler(config)\n        self.apply(self.init_bert_weights)\n\n    def forward(self, input_ids, token_type_ids, attention_mask=None, output_all_encoded_layers=True):\n        if attention_mask is None:\n            attention_mask = torch.ones_like(input_ids)\n        if token_type_ids is None:\n            token_type_ids = torch.zeros_like(input_ids)\n\n        # We create a 3D attention mask from a 2D tensor mask.\n        # Sizes are [batch_size, 1, 1, to_seq_length]\n        # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\n        # this attention mask is more simple than the triangular masking of causal attention\n        # used in OpenAI GPT, we just need to prepare the broadcast dimension here.\n        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n\n        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n        # masked positions, this operation will create a tensor which is 0.0 for\n        # positions we want to attend and -10000.0 for masked positions.\n        # Since we are adding it to the raw scores before the softmax, this is\n        # effectively the same as removing these entirely.\n        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype) # fp16 compatibility\n        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n        \n        embedding_output = self.embeddings(input_ids, token_type_ids)\n        \n        encoded_layers = self.encoder(embedding_output,\n                                      extended_attention_mask,\n                                      output_all_encoded_layers=output_all_encoded_layers)\n        \n        sequence_output = encoded_layers[-1]\n      \n        pooled_output = self.pooler(sequence_output)\n       \n        if not output_all_encoded_layers:\n            encoded_layers = encoded_layers[-1]\n      \n        return encoded_layers, pooled_output\n\n\nclass BertForPreTraining(BertPreTrainedModel):\n    """"""BERT model with pre-training heads.\n    This module comprises the BERT model followed by the two pre-training heads:\n        - the masked language modeling head, and\n        - the next sentence classification head.\n    Params:\n        config: a BertConfig class instance with the configuration to build a new model.\n    Inputs:\n        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n            a `sentence B` token (see BERT paper for more details).\n        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n            selected in [0, 1]. It\'s a mask to be used if the input sequence length is smaller than the max\n            input sequence length in the current batch. It\'s the mask that we typically use for attention when\n            a batch has varying length sentences.\n        `masked_lm_labels`: optional masked language modeling labels: torch.LongTensor of shape [batch_size, sequence_length]\n            with indices selected in [-1, 0, ..., vocab_size]. All labels set to -1 are ignored (masked), the loss\n            is only computed for the labels set in [0, ..., vocab_size]\n        `next_sentence_label`: optional next sentence classification loss: torch.LongTensor of shape [batch_size]\n            with indices selected in [0, 1].\n            0 => next sentence is the continuation, 1 => next sentence is a random sentence.\n    Outputs:\n        if `masked_lm_labels` and `next_sentence_label` are not `None`:\n            Outputs the total_loss which is the sum of the masked language modeling loss and the next\n            sentence classification loss.\n        if `masked_lm_labels` or `next_sentence_label` is `None`:\n            Outputs a tuple comprising\n            - the masked language modeling logits of shape [batch_size, sequence_length, vocab_size], and\n            - the next sentence classification logits of shape [batch_size, 2].\n    Example usage:\n    ```python\n    # Already been converted into WordPiece token ids\n    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n    model = BertForPreTraining(config)\n    masked_lm_logits_scores, seq_relationship_logits = model(input_ids, token_type_ids, input_mask)\n    ```\n    """"""\n    def __init__(self, config):\n        super(BertForPreTraining, self).__init__(config)\n        self.bert = BertModel(config)\n        self.cls = BertPreTrainingHeads(config, self.bert.embeddings.word_embeddings.weight)\n        self.apply(self.init_bert_weights)\n\n    def forward(self, input_ids, token_type_ids=None, attention_mask=None, masked_lm_labels=None, next_sentence_label=None):\n        sequence_output, pooled_output = self.bert(input_ids, token_type_ids, attention_mask,\n                                                   output_all_encoded_layers=False)\n        prediction_scores, seq_relationship_score = self.cls(sequence_output, pooled_output)\n\n        if masked_lm_labels is not None and next_sentence_label is not None:\n            loss_fct = CrossEntropyLoss(ignore_index=-1)\n            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), masked_lm_labels.view(-1))\n            next_sentence_loss = loss_fct(seq_relationship_score.view(-1, 2), next_sentence_label.view(-1))\n            total_loss = masked_lm_loss + next_sentence_loss\n            return total_loss\n        else:\n            return prediction_scores, seq_relationship_score\n\n\nclass BertForMaskedLM(BertPreTrainedModel):\n    """"""BERT model with the masked language modeling head.\n    This module comprises the BERT model followed by the masked language modeling head.\n    Params:\n        config: a BertConfig class instance with the configuration to build a new model.\n    Inputs:\n        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n            a `sentence B` token (see BERT paper for more details).\n        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n            selected in [0, 1]. It\'s a mask to be used if the input sequence length is smaller than the max\n            input sequence length in the current batch. It\'s the mask that we typically use for attention when\n            a batch has varying length sentences.\n        `masked_lm_labels`: masked language modeling labels: torch.LongTensor of shape [batch_size, sequence_length]\n            with indices selected in [-1, 0, ..., vocab_size]. All labels set to -1 are ignored (masked), the loss\n            is only computed for the labels set in [0, ..., vocab_size]\n    Outputs:\n        if `masked_lm_labels` is  not `None`:\n            Outputs the masked language modeling loss.\n        if `masked_lm_labels` is `None`:\n            Outputs the masked language modeling logits of shape [batch_size, sequence_length, vocab_size].\n    Example usage:\n    ```python\n    # Already been converted into WordPiece token ids\n    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n    model = BertForMaskedLM(config)\n    masked_lm_logits_scores = model(input_ids, token_type_ids, input_mask)\n    ```\n    """"""\n    def __init__(self, config):\n        super(BertForMaskedLM, self).__init__(config)\n        self.bert = BertModel(config)\n        self.cls = BertOnlyMLMHead(config, self.bert.embeddings.word_embeddings.weight)\n        self.apply(self.init_bert_weights)\n\n    def forward(self, input_ids, token_type_ids=None, attention_mask=None, masked_lm_labels=None):\n        sequence_output, _ = self.bert(input_ids, token_type_ids, attention_mask,\n                                       output_all_encoded_layers=False)\n        prediction_scores = self.cls(sequence_output)\n\n        if masked_lm_labels is not None:\n            loss_fct = CrossEntropyLoss(ignore_index=-1)\n            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), masked_lm_labels.view(-1))\n            return masked_lm_loss\n        else:\n            return prediction_scores\n\n\nclass BertForNextSentencePrediction(BertPreTrainedModel):\n    """"""BERT model with next sentence prediction head.\n    This module comprises the BERT model followed by the next sentence classification head.\n    Params:\n        config: a BertConfig class instance with the configuration to build a new model.\n    Inputs:\n        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n            a `sentence B` token (see BERT paper for more details).\n        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n            selected in [0, 1]. It\'s a mask to be used if the input sequence length is smaller than the max\n            input sequence length in the current batch. It\'s the mask that we typically use for attention when\n            a batch has varying length sentences.\n        `next_sentence_label`: next sentence classification loss: torch.LongTensor of shape [batch_size]\n            with indices selected in [0, 1].\n            0 => next sentence is the continuation, 1 => next sentence is a random sentence.\n    Outputs:\n        if `next_sentence_label` is not `None`:\n            Outputs the total_loss which is the sum of the masked language modeling loss and the next\n            sentence classification loss.\n        if `next_sentence_label` is `None`:\n            Outputs the next sentence classification logits of shape [batch_size, 2].\n    Example usage:\n    ```python\n    # Already been converted into WordPiece token ids\n    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n    model = BertForNextSentencePrediction(config)\n    seq_relationship_logits = model(input_ids, token_type_ids, input_mask)\n    ```\n    """"""\n    def __init__(self, config):\n        super(BertForNextSentencePrediction, self).__init__(config)\n        self.bert = BertModel(config)\n        self.cls = BertOnlyNSPHead(config)\n        self.apply(self.init_bert_weights)\n\n    def forward(self, input_ids, token_type_ids=None, attention_mask=None, next_sentence_label=None):\n        _, pooled_output = self.bert(input_ids, token_type_ids, attention_mask,\n                                     output_all_encoded_layers=False)\n        seq_relationship_score = self.cls( pooled_output)\n\n        if next_sentence_label is not None:\n            loss_fct = CrossEntropyLoss(ignore_index=-1)\n            next_sentence_loss = loss_fct(seq_relationship_score.view(-1, 2), next_sentence_label.view(-1))\n            return next_sentence_loss\n        else:\n            return seq_relationship_score\n\n\nclass BertForSequenceClassification(BertPreTrainedModel):\n    """"""BERT model for classification.\n    This module is composed of the BERT model with a linear layer on top of\n    the pooled output.\n    Params:\n        `config`: a BertConfig class instance with the configuration to build a new model.\n        `num_labels`: the number of classes for the classifier. Default = 2.\n    Inputs:\n        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n            a `sentence B` token (see BERT paper for more details).\n        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n            selected in [0, 1]. It\'s a mask to be used if the input sequence length is smaller than the max\n            input sequence length in the current batch. It\'s the mask that we typically use for attention when\n            a batch has varying length sentences.\n        `labels`: labels for the classification output: torch.LongTensor of shape [batch_size]\n            with indices selected in [0, ..., num_labels].\n    Outputs:\n        if `labels` is not `None`:\n            Outputs the CrossEntropy classification loss of the output with the labels.\n        if `labels` is `None`:\n            Outputs the classification logits of shape [batch_size, num_labels].\n    Example usage:\n    ```python\n    # Already been converted into WordPiece token ids\n    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n    num_labels = 2\n    model = BertForSequenceClassification(config, num_labels)\n    logits = model(input_ids, token_type_ids, input_mask)\n    ```\n    """"""\n    def __init__(self, config, num_labels):\n        super(BertForSequenceClassification, self).__init__(config)\n        self.num_labels = num_labels\n        self.bert = BertModel(config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, num_labels)\n        self.apply(self.init_bert_weights)\n\n    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n        _, pooled_output = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)\n        pooled_output = self.dropout(pooled_output)\n        logits = self.classifier(pooled_output)\n\n        if labels is not None:\n            loss_fct = CrossEntropyLoss()\n            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n            return loss\n        else:\n            return logits\n\n\nclass BertForMultipleChoice(BertPreTrainedModel):\n    """"""BERT model for multiple choice tasks.\n    This module is composed of the BERT model with a linear layer on top of\n    the pooled output.\n    Params:\n        `config`: a BertConfig class instance with the configuration to build a new model.\n        `num_choices`: the number of classes for the classifier. Default = 2.\n    Inputs:\n        `input_ids`: a torch.LongTensor of shape [batch_size, num_choices, sequence_length]\n            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, num_choices, sequence_length]\n            with the token types indices selected in [0, 1]. Type 0 corresponds to a `sentence A`\n            and type 1 corresponds to a `sentence B` token (see BERT paper for more details).\n        `attention_mask`: an optional torch.LongTensor of shape [batch_size, num_choices, sequence_length] with indices\n            selected in [0, 1]. It\'s a mask to be used if the input sequence length is smaller than the max\n            input sequence length in the current batch. It\'s the mask that we typically use for attention when\n            a batch has varying length sentences.\n        `labels`: labels for the classification output: torch.LongTensor of shape [batch_size]\n            with indices selected in [0, ..., num_choices].\n    Outputs:\n        if `labels` is not `None`:\n            Outputs the CrossEntropy classification loss of the output with the labels.\n        if `labels` is `None`:\n            Outputs the classification logits of shape [batch_size, num_labels].\n    Example usage:\n    ```python\n    # Already been converted into WordPiece token ids\n    input_ids = torch.LongTensor([[[31, 51, 99], [15, 5, 0]], [[12, 16, 42], [14, 28, 57]]])\n    input_mask = torch.LongTensor([[[1, 1, 1], [1, 1, 0]],[[1,1,0], [1, 0, 0]]])\n    token_type_ids = torch.LongTensor([[[0, 0, 1], [0, 1, 0]],[[0, 1, 1], [0, 0, 1]]])\n    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n    num_choices = 2\n    model = BertForMultipleChoice(config, num_choices)\n    logits = model(input_ids, token_type_ids, input_mask)\n    ```\n    """"""\n    def __init__(self, config, num_choices):\n        super(BertForMultipleChoice, self).__init__(config)\n        self.num_choices = num_choices\n        self.bert = BertModel(config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, 1)\n        self.apply(self.init_bert_weights)\n\n    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n        flat_input_ids = input_ids.view(-1, input_ids.size(-1))\n        flat_token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1))\n        flat_attention_mask = attention_mask.view(-1, attention_mask.size(-1))\n        _, pooled_output = self.bert(flat_input_ids, flat_token_type_ids, flat_attention_mask, output_all_encoded_layers=False)\n        pooled_output = self.dropout(pooled_output)\n        logits = self.classifier(pooled_output)\n        reshaped_logits = logits.view(-1, self.num_choices)\n\n        if labels is not None:\n            loss_fct = CrossEntropyLoss()\n            loss = loss_fct(reshaped_logits, labels)\n            return loss\n        else:\n            return reshaped_logits\n\n\nclass BertForTokenClassification(BertPreTrainedModel):\n    """"""BERT model for token-level classification.\n    This module is composed of the BERT model with a linear layer on top of\n    the full hidden state of the last layer.\n    Params:\n        `config`: a BertConfig class instance with the configuration to build a new model.\n        `num_labels`: the number of classes for the classifier. Default = 2.\n    Inputs:\n        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n            a `sentence B` token (see BERT paper for more details).\n        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n            selected in [0, 1]. It\'s a mask to be used if the input sequence length is smaller than the max\n            input sequence length in the current batch. It\'s the mask that we typically use for attention when\n            a batch has varying length sentences.\n        `labels`: labels for the classification output: torch.LongTensor of shape [batch_size, sequence_length]\n            with indices selected in [0, ..., num_labels].\n    Outputs:\n        if `labels` is not `None`:\n            Outputs the CrossEntropy classification loss of the output with the labels.\n        if `labels` is `None`:\n            Outputs the classification logits of shape [batch_size, sequence_length, num_labels].\n    Example usage:\n    ```python\n    # Already been converted into WordPiece token ids\n    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n    num_labels = 2\n    model = BertForTokenClassification(config, num_labels)\n    logits = model(input_ids, token_type_ids, input_mask)\n    ```\n    """"""\n    def __init__(self, config, num_labels):\n        super(BertForTokenClassification, self).__init__(config)\n        self.num_labels = num_labels\n        self.bert = BertModel(config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, num_labels)\n        self.apply(self.init_bert_weights)\n\n    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n        sequence_output, _ = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)\n        sequence_output = self.dropout(sequence_output)\n        logits = self.classifier(sequence_output)\n\n        if labels is not None:\n            loss_fct = CrossEntropyLoss()\n            # Only keep active parts of the loss\n            if attention_mask is not None:\n                active_loss = attention_mask.view(-1) == 1\n                active_logits = logits.view(-1, self.num_labels)[active_loss]\n                active_labels = labels.view(-1)[active_loss]\n                loss = loss_fct(active_logits, active_labels)\n            else:\n                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n            return loss\n        else:\n            return logits\n\nclass BertForQuestionAnswering(BertPreTrainedModel):\n\n    def __init__(self, config):\n        super(BertForQuestionAnswering, self).__init__(config)\n        self.bert = BertModel(config)\n        self.qa_outputs =  nn.Linear(768, 2)\n        self.loss_fct = CrossEntropyLoss()\n\n    def forward(self, input_ids, token_type_ids=None, attention_mask=None, start_positions=None, end_positions=None):\n\n        sequence_output, _ = self.bert(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask, output_all_encoded_layers=False)\n\n        logits = self.qa_outputs(sequence_output)\n        \n        start_logits, end_logits = logits.split(1, dim=-1)\n        start_logits = start_logits.squeeze(-1)\n        end_logits = end_logits.squeeze(-1)\n\n        if start_positions is not None and end_positions is not None:\n\n            start_loss = self.loss_fct(start_logits, start_positions)\n            end_loss = self.loss_fct(end_logits, end_positions)\n            total_loss = (start_loss + end_loss) / 2                       # !!!!!!!!!!\n\n            return total_loss, start_logits, end_logits\n        else:\n            return start_logits, end_logits\n'"
Dureader/predict/__init__.py,0,b''
Dureader/predict/args.py,1,"b'import torch\n\nseed = 42\ndevice = torch.device(""cuda"", 0)\ntest_lines = 187818  # \xe5\xa4\x9a\xe5\xb0\x91\xe6\x9d\xa1\xe8\xae\xad\xe7\xbb\x83\xe6\x95\xb0\xe6\x8d\xae\xef\xbc\x8c\xe5\x8d\xb3\xef\xbc\x9alen(features)\n\nsearch_input_file = ""../data/extracted/trainset/search.train.json""\nzhidao_input_file = ""../data/extracted/trainset/zhidao.train.json""\ndev_zhidao_input_file = ""../data/extracted/devset/zhidao.dev.json""\ndev_search_input_file = ""../data/extracted/devset/search.dev.json""\n\nmax_seq_length = 512\nmax_query_length = 60\n\noutput_dir = ""./model_dir""\npredict_example_files=\'predict.data\'\n\nmax_para_num=5  # \xe9\x80\x89\xe6\x8b\xa9\xe5\x87\xa0\xe7\xaf\x87\xe6\x96\x87\xe6\xa1\xa3\xe8\xbf\x9b\xe8\xa1\x8c\xe9\xa2\x84\xe6\xb5\x8b\nlearning_rate = 5e-5\nbatch_size = 4\nnum_train_epochs = 4\ngradient_accumulation_steps = 8   # \xe6\xa2\xaf\xe5\xba\xa6\xe7\xb4\xaf\xe7\xa7\xaf\nnum_train_optimization_steps = int(test_lines / gradient_accumulation_steps / batch_size) * num_train_epochs\nlog_step = int(test_lines / batch_size / 4)  # \xe6\xaf\x8f\xe4\xb8\xaaepoch\xe9\xaa\x8c\xe8\xaf\x81\xe5\x87\xa0\xe6\xac\xa1\xef\xbc\x8c\xe9\xbb\x98\xe8\xae\xa44\xe6\xac\xa1\n'"
Dureader/predict/file_utils.py,0,"b'import json\nimport logging\nimport os\nimport shutil\nimport tempfile\nfrom functools import wraps\nfrom hashlib import sha256\nimport sys\nfrom io import open\n\nimport requests\nfrom tqdm import tqdm\n\nfrom urllib.parse import urlparse\n\ntry:\n    from pathlib import Path\n    PYTORCH_PRETRAINED_BERT_CACHE = Path(os.getenv(\'PYTORCH_PRETRAINED_BERT_CACHE\',\n                                                   Path.home() / \'.pytorch_pretrained_bert\'))\nexcept (AttributeError, ImportError):\n    PYTORCH_PRETRAINED_BERT_CACHE = os.getenv(\'PYTORCH_PRETRAINED_BERT_CACHE\',\n                                              os.path.join(os.path.expanduser(""~""), \'.pytorch_pretrained_bert\'))\n\nlogger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n\ndef url_to_filename(url, etag=None):\n    """"""\n    Convert `url` into a hashed filename in a repeatable way.\n    If `etag` is specified, append its hash to the url\'s, delimited\n    by a period.\n    """"""\n    url_bytes = url.encode(\'utf-8\')\n    url_hash = sha256(url_bytes)\n    filename = url_hash.hexdigest()\n\n    if etag:\n        etag_bytes = etag.encode(\'utf-8\')\n        etag_hash = sha256(etag_bytes)\n        filename += \'.\' + etag_hash.hexdigest()\n\n    return filename\n\n\ndef filename_to_url(filename, cache_dir=None):\n    """"""\n    Return the url and etag (which may be ``None``) stored for `filename`.\n    Raise ``EnvironmentError`` if `filename` or its stored metadata do not exist.\n    """"""\n    if cache_dir is None:\n        cache_dir = PYTORCH_PRETRAINED_BERT_CACHE\n    if sys.version_info[0] == 3 and isinstance(cache_dir, Path):\n        cache_dir = str(cache_dir)\n\n    cache_path = os.path.join(cache_dir, filename)\n    if not os.path.exists(cache_path):\n        raise EnvironmentError(""file {} not found"".format(cache_path))\n\n    meta_path = cache_path + \'.json\'\n    if not os.path.exists(meta_path):\n        raise EnvironmentError(""file {} not found"".format(meta_path))\n\n    with open(meta_path, encoding=""utf-8"") as meta_file:\n        metadata = json.load(meta_file)\n    url = metadata[\'url\']\n    etag = metadata[\'etag\']\n\n    return url, etag\n\ndef cached_path(url_or_filename, cache_dir=None):\n    """"""\n    Given something that might be a URL (or might be a local path),\n    determine which. If it\'s a URL, download the file and cache it, and\n    return the path to the cached file. If it\'s already a local path,\n    make sure the file exists and then return the path.\n    """"""\n    if cache_dir is None:\n        cache_dir = PYTORCH_PRETRAINED_BERT_CACHE\n    if sys.version_info[0] == 3 and isinstance(url_or_filename, Path):\n        url_or_filename = str(url_or_filename)\n    if sys.version_info[0] == 3 and isinstance(cache_dir, Path):\n        cache_dir = str(cache_dir)\n\n    parsed = urlparse(url_or_filename)\n\n    if parsed.scheme in (\'http\', \'https\', \'s3\'):\n        # URL, so get it from the cache (downloading if necessary)\n        return get_from_cache(url_or_filename, cache_dir)\n    elif os.path.exists(url_or_filename):\n        # File, and it exists.\n        return url_or_filename\n    elif parsed.scheme == \'\':\n        # File, but it doesn\'t exist.\n        raise EnvironmentError(""file {} not found"".format(url_or_filename))\n    else:\n        # Something unknown\n        raise ValueError(""unable to parse {} as a URL or as a local path"".format(url_or_filename))\n\n\ndef split_s3_path(url):\n    """"""Split a full s3 path into the bucket name and path.""""""\n    parsed = urlparse(url)\n    if not parsed.netloc or not parsed.path:\n        raise ValueError(""bad s3 path {}"".format(url))\n    bucket_name = parsed.netloc\n    s3_path = parsed.path\n    # Remove \'/\' at beginning of path.\n    if s3_path.startswith(""/""):\n        s3_path = s3_path[1:]\n    return bucket_name, s3_path\n\n\ndef s3_request(func):\n    """"""\n    Wrapper function for s3 requests in order to create more helpful error\n    messages.\n    """"""\n\n    @wraps(func)\n    def wrapper(url, *args, **kwargs):\n        try:\n            return func(url, *args, **kwargs)\n        except ClientError as exc:\n            if int(exc.response[""Error""][""Code""]) == 404:\n                raise EnvironmentError(""file {} not found"".format(url))\n            else:\n                raise\n\n    return wrapper\n\n\n@s3_request\ndef s3_etag(url):\n    """"""Check ETag on S3 object.""""""\n    s3_resource = boto3.resource(""s3"")\n    bucket_name, s3_path = split_s3_path(url)\n    s3_object = s3_resource.Object(bucket_name, s3_path)\n    return s3_object.e_tag\n\n\n@s3_request\ndef s3_get(url, temp_file):\n    """"""Pull a file directly from S3.""""""\n    s3_resource = boto3.resource(""s3"")\n    bucket_name, s3_path = split_s3_path(url)\n    s3_resource.Bucket(bucket_name).download_fileobj(s3_path, temp_file)\n\n\ndef http_get(url, temp_file):\n    req = requests.get(url, stream=True)\n    content_length = req.headers.get(\'Content-Length\')\n    total = int(content_length) if content_length is not None else None\n    progress = tqdm(unit=""B"", total=total)\n    for chunk in req.iter_content(chunk_size=1024):\n        if chunk: # filter out keep-alive new chunks\n            progress.update(len(chunk))\n            temp_file.write(chunk)\n    progress.close()\n\n\ndef get_from_cache(url, cache_dir=None):\n    """"""\n    Given a URL, look for the corresponding dataset in the local cache.\n    If it\'s not there, download it. Then return the path to the cached file.\n    """"""\n    if cache_dir is None:\n        cache_dir = PYTORCH_PRETRAINED_BERT_CACHE\n    if sys.version_info[0] == 3 and isinstance(cache_dir, Path):\n        cache_dir = str(cache_dir)\n\n    if not os.path.exists(cache_dir):\n        os.makedirs(cache_dir)\n\n    # Get eTag to add to filename, if it exists.\n    if url.startswith(""s3://""):\n        etag = s3_etag(url)\n    else:\n        response = requests.head(url, allow_redirects=True)\n        if response.status_code != 200:\n            raise IOError(""HEAD request failed for url {} with status code {}""\n                          .format(url, response.status_code))\n        etag = response.headers.get(""ETag"")\n\n    filename = url_to_filename(url, etag)\n\n    # get cache path to put the file\n    cache_path = os.path.join(cache_dir, filename)\n\n    if not os.path.exists(cache_path):\n        # Download to temporary file, then copy to cache dir once finished.\n        # Otherwise you get corrupt cache entries if the download gets interrupted.\n        with tempfile.NamedTemporaryFile() as temp_file:\n            logger.info(""%s not found in cache, downloading to %s"", url, temp_file.name)\n\n            # GET file object\n            if url.startswith(""s3://""):\n                s3_get(url, temp_file)\n            else:\n                http_get(url, temp_file)\n\n            # we are copying the file before closing it, so flush to avoid truncation\n            temp_file.flush()\n            # shutil.copyfileobj() starts at the current position, so go to the start\n            temp_file.seek(0)\n\n            logger.info(""copying %s to cache at %s"", temp_file.name, cache_path)\n            with open(cache_path, \'wb\') as cache_file:\n                shutil.copyfileobj(temp_file, cache_file)\n\n            logger.info(""creating metadata file for %s"", cache_path)\n            meta = {\'url\': url, \'etag\': etag}\n            meta_path = cache_path + \'.json\'\n            with open(meta_path, \'w\', encoding=""utf-8"") as meta_file:\n                json.dump(meta, meta_file)\n\n            logger.info(""removing temp file %s"", temp_file.name)\n\n    return cache_path\n\n\ndef read_set_from_file(filename):\n    \'\'\'\n    Extract a de-duped collection (set) of text from a file.\n    Expected file format is one item per line.\n    \'\'\'\n    collection = set()\n    with open(filename, \'r\', encoding=\'utf-8\') as file_:\n        for line in file_:\n            collection.add(line.rstrip())\n    return collection\n\n\ndef get_file_extension(path, dot=True, lower=True):\n    ext = os.path.splitext(path)[1]\n    ext = ext if dot else ext[1:]\n    return ext.lower() if lower else ext\n'"
Dureader/predict/modeling.py,69,"b'import copy\nimport json\nimport logging\nimport math\nimport os\nimport args\nimport shutil\nimport tarfile\nimport tempfile\nfrom io import open\nimport torch.nn.functional as F\nimport torch\nfrom torch import nn\nfrom torch.nn import CrossEntropyLoss\n\nimport sys\nsys.path.append(\'../dataset\'); \nfrom file_utils import cached_path\n\nlogger = logging.getLogger(__name__)\n\nPRETRAINED_MODEL_ARCHIVE_MAP = {\n    \'bert-base-uncased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz"",\n    \'bert-large-uncased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased.tar.gz"",\n    \'bert-base-cased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased.tar.gz"",\n    \'bert-large-cased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased.tar.gz"",\n    \'bert-base-multilingual-uncased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased.tar.gz"",\n    \'bert-base-multilingual-cased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased.tar.gz"",\n    \'bert-base-chinese\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese.tar.gz"",\n}\nCONFIG_NAME = \'bert_config.json\'\nWEIGHTS_NAME = \'pytorch_model.bin\'\nTF_WEIGHTS_NAME = \'model.ckpt\'\n\ndef load_tf_weights_in_bert(model, tf_checkpoint_path):\n    """""" Load tf checkpoints in a pytorch model\n    """"""\n    try:\n        import re\n        import numpy as np\n        import tensorflow as tf\n    except ImportError:\n        print(""Loading a TensorFlow models in PyTorch, requires TensorFlow to be installed. Please see ""\n            ""https://www.tensorflow.org/install/ for installation instructions."")\n        raise\n    tf_path = os.path.abspath(tf_checkpoint_path)\n    print(""Converting TensorFlow checkpoint from {}"".format(tf_path))\n    # Load weights from TF model\n    init_vars = tf.train.list_variables(tf_path)\n    names = []\n    arrays = []\n    for name, shape in init_vars:\n        print(""Loading TF weight {} with shape {}"".format(name, shape))\n        array = tf.train.load_variable(tf_path, name)\n        names.append(name)\n        arrays.append(array)\n\n    for name, array in zip(names, arrays):\n        name = name.split(\'/\')\n        # adam_v and adam_m are variables used in AdamWeightDecayOptimizer to calculated m and v\n        # which are not required for using pretrained model\n        if any(n in [""adam_v"", ""adam_m""] for n in name):\n            print(""Skipping {}"".format(""/"".join(name)))\n            continue\n        pointer = model\n        for m_name in name:\n            if re.fullmatch(r\'[A-Za-z]+_\\d+\', m_name):\n                l = re.split(r\'_(\\d+)\', m_name)\n            else:\n                l = [m_name]\n            if l[0] == \'kernel\' or l[0] == \'gamma\':\n                pointer = getattr(pointer, \'weight\')\n            elif l[0] == \'output_bias\' or l[0] == \'beta\':\n                pointer = getattr(pointer, \'bias\')\n            elif l[0] == \'output_weights\':\n                pointer = getattr(pointer, \'weight\')\n            else:\n                pointer = getattr(pointer, l[0])\n            if len(l) >= 2:\n                num = int(l[1])\n                pointer = pointer[num]\n        if m_name[-11:] == \'_embeddings\':\n            pointer = getattr(pointer, \'weight\')\n        elif m_name == \'kernel\':\n            array = np.transpose(array)\n        try:\n            assert pointer.shape == array.shape\n        except AssertionError as e:\n            e.args += (pointer.shape, array.shape)\n            raise\n        print(""Initialize PyTorch weight {}"".format(name))\n        pointer.data = torch.from_numpy(array)\n    return model\n\n\ndef gelu(x):\n    """"""Implementation of the gelu activation function.\n        For information: OpenAI GPT\'s gelu is slightly different (and gives slightly different results):\n        0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n        Also see https://arxiv.org/abs/1606.08415\n    """"""\n    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n\n\ndef swish(x):\n    return x * torch.sigmoid(x)\n\n\nACT2FN = {""gelu"": gelu, ""relu"": torch.nn.functional.relu, ""swish"": swish}\n\n\nclass BertConfig(object):\n    """"""Configuration class to store the configuration of a `BertModel`.\n    """"""\n    def __init__(self,\n                 vocab_size_or_config_json_file,\n                 hidden_size=768,\n                 num_hidden_layers=12,\n                 num_attention_heads=12,\n                 intermediate_size=3072,\n                 hidden_act=""gelu"",\n                 hidden_dropout_prob=0.1,\n                 attention_probs_dropout_prob=0.1,\n                 max_position_embeddings=512,\n                 type_vocab_size=2,\n                 initializer_range=0.02):\n        """"""Constructs BertConfig.\n        Args:\n            vocab_size_or_config_json_file: Vocabulary size of `inputs_ids` in `BertModel`.\n            hidden_size: Size of the encoder layers and the pooler layer.\n            num_hidden_layers: Number of hidden layers in the Transformer encoder.\n            num_attention_heads: Number of attention heads for each attention layer in\n                the Transformer encoder.\n            intermediate_size: The size of the ""intermediate"" (i.e., feed-forward)\n                layer in the Transformer encoder.\n            hidden_act: The non-linear activation function (function or string) in the\n                encoder and pooler. If string, ""gelu"", ""relu"" and ""swish"" are supported.\n            hidden_dropout_prob: The dropout probabilitiy for all fully connected\n                layers in the embeddings, encoder, and pooler.\n            attention_probs_dropout_prob: The dropout ratio for the attention\n                probabilities.\n            max_position_embeddings: The maximum sequence length that this model might\n                ever be used with. Typically set this to something large just in case\n                (e.g., 512 or 1024 or 2048).\n            type_vocab_size: The vocabulary size of the `token_type_ids` passed into\n                `BertModel`.\n            initializer_range: The sttdev of the truncated_normal_initializer for\n                initializing all weight matrices.\n        """"""\n        if isinstance(vocab_size_or_config_json_file, str) or (sys.version_info[0] == 2\n                        and isinstance(vocab_size_or_config_json_file, unicode)):\n            with open(vocab_size_or_config_json_file, ""r"", encoding=\'utf-8\') as reader:\n                json_config = json.loads(reader.read())\n            for key, value in json_config.items():\n                self.__dict__[key] = value\n        elif isinstance(vocab_size_or_config_json_file, int):\n            self.vocab_size = vocab_size_or_config_json_file\n            self.hidden_size = hidden_size\n            self.num_hidden_layers = num_hidden_layers\n            self.num_attention_heads = num_attention_heads\n            self.hidden_act = hidden_act\n            self.intermediate_size = intermediate_size\n            self.hidden_dropout_prob = hidden_dropout_prob\n            self.attention_probs_dropout_prob = attention_probs_dropout_prob\n            self.max_position_embeddings = max_position_embeddings\n            self.type_vocab_size = type_vocab_size\n            self.initializer_range = initializer_range\n        else:\n            raise ValueError(""First argument must be either a vocabulary size (int)""\n                             ""or the path to a pretrained model config file (str)"")\n\n    @classmethod\n    def from_dict(cls, json_object):\n        """"""Constructs a `BertConfig` from a Python dictionary of parameters.""""""\n        config = BertConfig(vocab_size_or_config_json_file=-1)\n        for key, value in json_object.items():\n            config.__dict__[key] = value\n        return config\n\n    @classmethod\n    def from_json_file(cls, json_file):\n        """"""Constructs a `BertConfig` from a json file of parameters.""""""\n        with open(json_file, ""r"", encoding=\'utf-8\') as reader:\n            text = reader.read()\n        return cls.from_dict(json.loads(text))\n\n    def __repr__(self):\n        return str(self.to_json_string())\n\n    def to_dict(self):\n        """"""Serializes this instance to a Python dictionary.""""""\n        output = copy.deepcopy(self.__dict__)\n        return output\n\n    def to_json_string(self):\n        """"""Serializes this instance to a JSON string.""""""\n        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + ""\\n""\n\ntry:\n    from apex.normalization.fused_layer_norm import FusedLayerNorm as BertLayerNorm\nexcept ImportError:\n    logger.info(""Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex ."")\n    class BertLayerNorm(nn.Module):\n        def __init__(self, hidden_size, eps=1e-12):\n            """"""Construct a layernorm module in the TF style (epsilon inside the square root).\n            """"""\n            super(BertLayerNorm, self).__init__()\n            self.weight = nn.Parameter(torch.ones(hidden_size))\n            self.bias = nn.Parameter(torch.zeros(hidden_size))\n            self.variance_epsilon = eps\n\n        def forward(self, x):\n            u = x.mean(-1, keepdim=True)\n            s = (x - u).pow(2).mean(-1, keepdim=True)\n            x = (x - u) / torch.sqrt(s + self.variance_epsilon)\n            return self.weight * x + self.bias\n\nclass BertEmbeddings(nn.Module):\n    """"""Construct the embeddings from word, position and token_type embeddings.\n    """"""\n    def __init__(self, config):\n        super(BertEmbeddings, self).__init__()\n\n        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=0)\n        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size, padding_idx=0)\n        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size, padding_idx=0)\n\n        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n    def forward(self, input_ids, token_type_ids=None):\n        seq_length = input_ids.size(1)\n       \n        position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)\n        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n        if token_type_ids is None:\n            token_type_ids = torch.zeros_like(input_ids)\n   \n        words_embeddings = self.word_embeddings(input_ids)\n        position_embeddings = self.position_embeddings(position_ids)\n        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n   \n        embeddings = words_embeddings + token_type_embeddings + position_embeddings #wiq_embeddings + position_embeddings\n        embeddings = self.LayerNorm(embeddings)\n        embeddings = self.dropout(embeddings)\n  \n        return embeddings\n\n\nclass BertSelfAttention(nn.Module):\n    def __init__(self, config):\n        super(BertSelfAttention, self).__init__()\n        if config.hidden_size % config.num_attention_heads != 0:\n            raise ValueError(\n                ""The hidden size (%d) is not a multiple of the number of attention ""\n                ""heads (%d)"" % (config.hidden_size, config.num_attention_heads))\n        self.num_attention_heads = config.num_attention_heads\n        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n        self.all_head_size = self.num_attention_heads * self.attention_head_size\n\n        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n\n        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n\n    def transpose_for_scores(self, x):\n        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n        x = x.view(*new_x_shape)\n        return x.permute(0, 2, 1, 3)\n\n    def forward(self, hidden_states, attention_mask):\n        mixed_query_layer = self.query(hidden_states)\n        mixed_key_layer = self.key(hidden_states)\n        mixed_value_layer = self.value(hidden_states)\n\n        query_layer = self.transpose_for_scores(mixed_query_layer)\n        key_layer = self.transpose_for_scores(mixed_key_layer)\n        value_layer = self.transpose_for_scores(mixed_value_layer)\n\n        # Take the dot product between ""query"" and ""key"" to get the raw attention scores.\n        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n        # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n        attention_scores = attention_scores + attention_mask\n\n        # Normalize the attention scores to probabilities.\n        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n\n        # This is actually dropping out entire tokens to attend to, which might\n        # seem a bit unusual, but is taken from the original Transformer paper.\n        attention_probs = self.dropout(attention_probs)\n\n        context_layer = torch.matmul(attention_probs, value_layer)\n        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n        context_layer = context_layer.view(*new_context_layer_shape)\n        return context_layer\n\n\nclass BertSelfOutput(nn.Module):\n    def __init__(self, config):\n        super(BertSelfOutput, self).__init__()\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n    def forward(self, hidden_states, input_tensor):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n        return hidden_states\n\n\nclass BertAttention(nn.Module):\n    def __init__(self, config):\n        super(BertAttention, self).__init__()\n        self.self = BertSelfAttention(config)\n        self.output = BertSelfOutput(config)\n\n    def forward(self, input_tensor, attention_mask):\n        self_output = self.self(input_tensor, attention_mask)\n        attention_output = self.output(self_output, input_tensor)\n        return attention_output\n\n\nclass BertIntermediate(nn.Module):\n    def __init__(self, config):\n        super(BertIntermediate, self).__init__()\n        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n        if isinstance(config.hidden_act, str) or (sys.version_info[0] == 2 and isinstance(config.hidden_act, unicode)):\n            self.intermediate_act_fn = ACT2FN[config.hidden_act]\n        else:\n            self.intermediate_act_fn = config.hidden_act\n\n    def forward(self, hidden_states):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.intermediate_act_fn(hidden_states)\n        return hidden_states\n\n\nclass BertOutput(nn.Module):\n    def __init__(self, config):\n        super(BertOutput, self).__init__()\n        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n    def forward(self, hidden_states, input_tensor):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n        return hidden_states\n\n\nclass BertLayer(nn.Module):\n    def __init__(self, config):\n        super(BertLayer, self).__init__()\n        self.attention = BertAttention(config)\n        self.intermediate = BertIntermediate(config)\n        self.output = BertOutput(config)\n\n    def forward(self, hidden_states, attention_mask):\n        attention_output = self.attention(hidden_states, attention_mask)\n        intermediate_output = self.intermediate(attention_output)\n        layer_output = self.output(intermediate_output, attention_output)\n        return layer_output\n\n\nclass BertEncoder(nn.Module):\n    def __init__(self, config):\n        super(BertEncoder, self).__init__()\n        layer = BertLayer(config)\n        self.layer = nn.ModuleList([copy.deepcopy(layer) for _ in range(config.num_hidden_layers)])\n\n    def forward(self, hidden_states, attention_mask, output_all_encoded_layers=True):\n        all_encoder_layers = []\n        for layer_module in self.layer:\n            hidden_states = layer_module(hidden_states, attention_mask)\n            if output_all_encoded_layers:\n                all_encoder_layers.append(hidden_states)\n        if not output_all_encoded_layers:\n            all_encoder_layers.append(hidden_states)\n        return all_encoder_layers\n\n\nclass BertPooler(nn.Module):\n    def __init__(self, config):\n        super(BertPooler, self).__init__()\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.activation = nn.Tanh()\n\n    def forward(self, hidden_states):\n        # We ""pool"" the model by simply taking the hidden state corresponding\n        # to the first token.\n        first_token_tensor = hidden_states[:, 0]\n        pooled_output = self.dense(first_token_tensor)\n        pooled_output = self.activation(pooled_output)\n        return pooled_output\n\n\nclass BertPredictionHeadTransform(nn.Module):\n    def __init__(self, config):\n        super(BertPredictionHeadTransform, self).__init__()\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        if isinstance(config.hidden_act, str) or (sys.version_info[0] == 2 and isinstance(config.hidden_act, unicode)):\n            self.transform_act_fn = ACT2FN[config.hidden_act]\n        else:\n            self.transform_act_fn = config.hidden_act\n        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\n\n    def forward(self, hidden_states):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.transform_act_fn(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states)\n        return hidden_states\n\n\nclass BertLMPredictionHead(nn.Module):\n    def __init__(self, config, bert_model_embedding_weights):\n        super(BertLMPredictionHead, self).__init__()\n        self.transform = BertPredictionHeadTransform(config)\n\n        # The output weights are the same as the input embeddings, but there is\n        # an output-only bias for each token.\n        self.decoder = nn.Linear(bert_model_embedding_weights.size(1),\n                                 bert_model_embedding_weights.size(0),\n                                 bias=False)\n        self.decoder.weight = bert_model_embedding_weights\n        self.bias = nn.Parameter(torch.zeros(bert_model_embedding_weights.size(0)))\n\n    def forward(self, hidden_states):\n        hidden_states = self.transform(hidden_states)\n        hidden_states = self.decoder(hidden_states) + self.bias\n        return hidden_states\n\n\nclass BertOnlyMLMHead(nn.Module):\n    def __init__(self, config, bert_model_embedding_weights):\n        super(BertOnlyMLMHead, self).__init__()\n        self.predictions = BertLMPredictionHead(config, bert_model_embedding_weights)\n\n    def forward(self, sequence_output):\n        prediction_scores = self.predictions(sequence_output)\n        return prediction_scores\n\n\nclass BertOnlyNSPHead(nn.Module):\n    def __init__(self, config):\n        super(BertOnlyNSPHead, self).__init__()\n        self.seq_relationship = nn.Linear(config.hidden_size, 2)\n\n    def forward(self, pooled_output):\n        seq_relationship_score = self.seq_relationship(pooled_output)\n        return seq_relationship_score\n\n\nclass BertPreTrainingHeads(nn.Module):\n    def __init__(self, config, bert_model_embedding_weights):\n        super(BertPreTrainingHeads, self).__init__()\n        self.predictions = BertLMPredictionHead(config, bert_model_embedding_weights)\n        self.seq_relationship = nn.Linear(config.hidden_size, 2)\n\n    def forward(self, sequence_output, pooled_output):\n        prediction_scores = self.predictions(sequence_output)\n        seq_relationship_score = self.seq_relationship(pooled_output)\n        return prediction_scores, seq_relationship_score\n\n\nclass BertPreTrainedModel(nn.Module):\n    """""" An abstract class to handle weights initialization and\n        a simple interface for dowloading and loading pretrained models.\n    """"""\n    def __init__(self, config, *inputs, **kwargs):\n        super(BertPreTrainedModel, self).__init__()\n        if not isinstance(config, BertConfig):\n            raise ValueError(\n                ""Parameter config in `{}(config)` should be an instance of class `BertConfig`. ""\n                ""To create a model from a Google pretrained model use ""\n                ""`model = {}.from_pretrained(PRETRAINED_MODEL_NAME)`"".format(\n                    self.__class__.__name__, self.__class__.__name__\n                ))\n        self.config = config\n\n    def init_bert_weights(self, module):\n        if isinstance(module, (nn.Linear, nn.Embedding)):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n        elif isinstance(module, BertLayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        if isinstance(module, nn.Linear) and module.bias is not None:\n            module.bias.data.zero_()\n\n    @classmethod\n    def from_pretrained(cls, pretrained_model_name_or_path, state_dict=None, cache_dir=None, from_tf=False, *inputs, **kwargs):\n        """"""\n        Instantiate a BertPreTrainedModel from a pre-trained model file or a pytorch state dict.\n        Download and cache the pre-trained model file if needed.\n        Params:\n            pretrained_model_name_or_path: either:\n                - a str with the name of a pre-trained model to load selected in the list of:\n                    . `bert-base-uncased`\n                    . `bert-large-uncased`\n                    . `bert-base-cased`\n                    . `bert-large-cased`\n                    . `bert-base-multilingual-uncased`\n                    . `bert-base-multilingual-cased`\n                    . `bert-base-chinese`\n                - a path or url to a pretrained model archive containing:\n                    . `bert_config.json` a configuration file for the model\n                    . `pytorch_model.bin` a PyTorch dump of a BertForPreTraining instance\n                - a path or url to a pretrained model archive containing:\n                    . `bert_config.json` a configuration file for the model\n                    . `model.chkpt` a TensorFlow checkpoint\n            from_tf: should we load the weights from a locally saved TensorFlow checkpoint\n            cache_dir: an optional path to a folder in which the pre-trained models will be cached.\n            state_dict: an optional state dictionnary (collections.OrderedDict object) to use instead of Google pre-trained models\n            *inputs, **kwargs: additional input for the specific Bert class\n                (ex: num_labels for BertForSequenceClassification)\n        """"""\n        if pretrained_model_name_or_path in PRETRAINED_MODEL_ARCHIVE_MAP:\n            archive_file = PRETRAINED_MODEL_ARCHIVE_MAP[pretrained_model_name_or_path]\n        else:\n            archive_file = pretrained_model_name_or_path\n        # redirect to the cache, if necessary\n        try:\n            resolved_archive_file = cached_path(archive_file, cache_dir=cache_dir)\n        except EnvironmentError:\n            logger.error(\n                ""Model name \'{}\' was not found in model name list ({}). ""\n                ""We assumed \'{}\' was a path or url but couldn\'t find any file ""\n                ""associated to this path or url."".format(\n                    pretrained_model_name_or_path,\n                    \', \'.join(PRETRAINED_MODEL_ARCHIVE_MAP.keys()),\n                    archive_file))\n            return None\n        if resolved_archive_file == archive_file:\n            print(""loading archive file {}"".format(archive_file))\n        else:\n            print(""loading archive file {} from cache at {}"".format(\n                archive_file, resolved_archive_file))\n        tempdir = None\n        if os.path.isdir(resolved_archive_file) or from_tf:\n            serialization_dir = resolved_archive_file\n        else:\n            # Extract archive to temp dir\n            tempdir = tempfile.mkdtemp()\n            print(""extracting archive file {} to temp dir {}"".format(\n                resolved_archive_file, tempdir))\n            with tarfile.open(resolved_archive_file, \'r:gz\') as archive:\n                archive.extractall(tempdir)\n            serialization_dir = tempdir\n        # Load config\n        config_file = os.path.join(serialization_dir, CONFIG_NAME)\n        config = BertConfig.from_json_file(config_file)\n        logger.info(""Model config {}"".format(config))\n        # Instantiate model.\n        model = cls(config, *inputs, **kwargs)\n        if state_dict is None and not from_tf:\n            weights_path = os.path.join(serialization_dir, WEIGHTS_NAME)\n            print(\'load state_dict\' + str(weights_path))\n            state_dict = torch.load(weights_path, map_location=\'cpu\' if not torch.cuda.is_available() else None)\n        if tempdir:\n            # Clean up temp dir\n            shutil.rmtree(tempdir)\n        if from_tf:\n            # Directly load from a TensorFlow checkpoint\n            weights_path = os.path.join(serialization_dir, TF_WEIGHTS_NAME)\n            return load_tf_weights_in_bert(model, weights_path)\n        # Load from a PyTorch state_dict\n        old_keys = []\n        new_keys = []\n        for key in state_dict.keys():\n            new_key = None\n            if \'gamma\' in key:\n                new_key = key.replace(\'gamma\', \'weight\')\n            if \'beta\' in key:\n                new_key = key.replace(\'beta\', \'bias\')\n            if new_key:\n                old_keys.append(key)\n                new_keys.append(new_key)\n        for old_key, new_key in zip(old_keys, new_keys):\n            state_dict[new_key] = state_dict.pop(old_key)\n\n        missing_keys = []\n        unexpected_keys = []\n        error_msgs = []\n        # copy state_dict so _load_from_state_dict can modify it\n        metadata = getattr(state_dict, \'_metadata\', None)\n        state_dict = state_dict.copy()\n        if metadata is not None:\n            state_dict._metadata = metadata\n\n        def load(module, prefix=\'\'):\n            local_metadata = {} if metadata is None else metadata.get(prefix[:-1], {})\n            module._load_from_state_dict(\n                state_dict, prefix, local_metadata, True, missing_keys, unexpected_keys, error_msgs)\n            for name, child in module._modules.items():\n                if child is not None:\n                    load(child, prefix + name + \'.\')\n        start_prefix = \'\'\n        if not hasattr(model, \'bert\') and any(s.startswith(\'bert.\') for s in state_dict.keys()):\n            start_prefix = \'bert.\'\n        load(model, prefix=start_prefix)\n\n        if len(missing_keys) > 0:\n            logger.info(""Weights of {} not initialized from pretrained model: {}"".format(\n                model.__class__.__name__, missing_keys))\n        if len(unexpected_keys) > 0:\n            logger.info(""Weights from pretrained model not used in {}: {}"".format(\n                model.__class__.__name__, unexpected_keys))\n        if len(error_msgs) > 0:\n            raise RuntimeError(\'Error(s) in loading state_dict for {}:\\n\\t{}\'.format(\n                               model.__class__.__name__, ""\\n\\t"".join(error_msgs)))\n        return model\n\n\nclass BertModel(BertPreTrainedModel):\n    """"""BERT model (""Bidirectional Embedding Representations from a Transformer"").\n    Params:\n        config: a BertConfig class instance with the configuration to build a new model\n    Inputs:\n        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n            a `sentence B` token (see BERT paper for more details).\n        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n            selected in [0, 1]. It\'s a mask to be used if the input sequence length is smaller than the max\n            input sequence length in the current batch. It\'s the mask that we typically use for attention when\n            a batch has varying length sentences.\n        `output_all_encoded_layers`: boolean which controls the content of the `encoded_layers` output as described below. Default: `True`.\n    Outputs: Tuple of (encoded_layers, pooled_output)\n        `encoded_layers`: controled by `output_all_encoded_layers` argument:\n            - `output_all_encoded_layers=True`: outputs a list of the full sequences of encoded-hidden-states at the end\n                of each attention block (i.e. 12 full sequences for BERT-base, 24 for BERT-large), each\n                encoded-hidden-state is a torch.FloatTensor of size [batch_size, sequence_length, hidden_size],\n            - `output_all_encoded_layers=False`: outputs only the full sequence of hidden-states corresponding\n                to the last attention block of shape [batch_size, sequence_length, hidden_size],\n        `pooled_output`: a torch.FloatTensor of size [batch_size, hidden_size] which is the output of a\n            classifier pretrained on top of the hidden state associated to the first character of the\n            input (`CLS`) to train on the Next-Sentence task (see BERT\'s paper).\n    Example usage:\n    ```python\n    # Already been converted into WordPiece token ids\n    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n    config = modeling.BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n    model = modeling.BertModel(config=config)\n    all_encoder_layers, pooled_output = model(input_ids, token_type_ids, input_mask)\n    ```\n    """"""\n    def __init__(self, config):\n        super(BertModel, self).__init__(config)\n        self.embeddings = BertEmbeddings(config)\n        self.encoder = BertEncoder(config)\n        self.pooler = BertPooler(config)\n        self.apply(self.init_bert_weights)\n\n    def forward(self, input_ids, token_type_ids, attention_mask=None, output_all_encoded_layers=True):\n        if attention_mask is None:\n            attention_mask = torch.ones_like(input_ids)\n        if token_type_ids is None:\n            token_type_ids = torch.zeros_like(input_ids)\n\n        # We create a 3D attention mask from a 2D tensor mask.\n        # Sizes are [batch_size, 1, 1, to_seq_length]\n        # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\n        # this attention mask is more simple than the triangular masking of causal attention\n        # used in OpenAI GPT, we just need to prepare the broadcast dimension here.\n        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n\n        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n        # masked positions, this operation will create a tensor which is 0.0 for\n        # positions we want to attend and -10000.0 for masked positions.\n        # Since we are adding it to the raw scores before the softmax, this is\n        # effectively the same as removing these entirely.\n        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype) # fp16 compatibility\n        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n        \n        embedding_output = self.embeddings(input_ids, token_type_ids)\n        \n        encoded_layers = self.encoder(embedding_output,\n                                      extended_attention_mask,\n                                      output_all_encoded_layers=output_all_encoded_layers)\n        \n        sequence_output = encoded_layers[-1]\n      \n        pooled_output = self.pooler(sequence_output)\n       \n        if not output_all_encoded_layers:\n            encoded_layers = encoded_layers[-1]\n      \n        return encoded_layers, pooled_output\n\n\nclass BertForPreTraining(BertPreTrainedModel):\n    """"""BERT model with pre-training heads.\n    This module comprises the BERT model followed by the two pre-training heads:\n        - the masked language modeling head, and\n        - the next sentence classification head.\n    Params:\n        config: a BertConfig class instance with the configuration to build a new model.\n    Inputs:\n        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n            a `sentence B` token (see BERT paper for more details).\n        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n            selected in [0, 1]. It\'s a mask to be used if the input sequence length is smaller than the max\n            input sequence length in the current batch. It\'s the mask that we typically use for attention when\n            a batch has varying length sentences.\n        `masked_lm_labels`: optional masked language modeling labels: torch.LongTensor of shape [batch_size, sequence_length]\n            with indices selected in [-1, 0, ..., vocab_size]. All labels set to -1 are ignored (masked), the loss\n            is only computed for the labels set in [0, ..., vocab_size]\n        `next_sentence_label`: optional next sentence classification loss: torch.LongTensor of shape [batch_size]\n            with indices selected in [0, 1].\n            0 => next sentence is the continuation, 1 => next sentence is a random sentence.\n    Outputs:\n        if `masked_lm_labels` and `next_sentence_label` are not `None`:\n            Outputs the total_loss which is the sum of the masked language modeling loss and the next\n            sentence classification loss.\n        if `masked_lm_labels` or `next_sentence_label` is `None`:\n            Outputs a tuple comprising\n            - the masked language modeling logits of shape [batch_size, sequence_length, vocab_size], and\n            - the next sentence classification logits of shape [batch_size, 2].\n    Example usage:\n    ```python\n    # Already been converted into WordPiece token ids\n    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n    model = BertForPreTraining(config)\n    masked_lm_logits_scores, seq_relationship_logits = model(input_ids, token_type_ids, input_mask)\n    ```\n    """"""\n    def __init__(self, config):\n        super(BertForPreTraining, self).__init__(config)\n        self.bert = BertModel(config)\n        self.cls = BertPreTrainingHeads(config, self.bert.embeddings.word_embeddings.weight)\n        self.apply(self.init_bert_weights)\n\n    def forward(self, input_ids, token_type_ids=None, attention_mask=None, masked_lm_labels=None, next_sentence_label=None):\n        sequence_output, pooled_output = self.bert(input_ids, token_type_ids, attention_mask,\n                                                   output_all_encoded_layers=False)\n        prediction_scores, seq_relationship_score = self.cls(sequence_output, pooled_output)\n\n        if masked_lm_labels is not None and next_sentence_label is not None:\n            loss_fct = CrossEntropyLoss(ignore_index=-1)\n            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), masked_lm_labels.view(-1))\n            next_sentence_loss = loss_fct(seq_relationship_score.view(-1, 2), next_sentence_label.view(-1))\n            total_loss = masked_lm_loss + next_sentence_loss\n            return total_loss\n        else:\n            return prediction_scores, seq_relationship_score\n\n\nclass BertForMaskedLM(BertPreTrainedModel):\n    """"""BERT model with the masked language modeling head.\n    This module comprises the BERT model followed by the masked language modeling head.\n    Params:\n        config: a BertConfig class instance with the configuration to build a new model.\n    Inputs:\n        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n            a `sentence B` token (see BERT paper for more details).\n        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n            selected in [0, 1]. It\'s a mask to be used if the input sequence length is smaller than the max\n            input sequence length in the current batch. It\'s the mask that we typically use for attention when\n            a batch has varying length sentences.\n        `masked_lm_labels`: masked language modeling labels: torch.LongTensor of shape [batch_size, sequence_length]\n            with indices selected in [-1, 0, ..., vocab_size]. All labels set to -1 are ignored (masked), the loss\n            is only computed for the labels set in [0, ..., vocab_size]\n    Outputs:\n        if `masked_lm_labels` is  not `None`:\n            Outputs the masked language modeling loss.\n        if `masked_lm_labels` is `None`:\n            Outputs the masked language modeling logits of shape [batch_size, sequence_length, vocab_size].\n    Example usage:\n    ```python\n    # Already been converted into WordPiece token ids\n    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n    model = BertForMaskedLM(config)\n    masked_lm_logits_scores = model(input_ids, token_type_ids, input_mask)\n    ```\n    """"""\n    def __init__(self, config):\n        super(BertForMaskedLM, self).__init__(config)\n        self.bert = BertModel(config)\n        self.cls = BertOnlyMLMHead(config, self.bert.embeddings.word_embeddings.weight)\n        self.apply(self.init_bert_weights)\n\n    def forward(self, input_ids, token_type_ids=None, attention_mask=None, masked_lm_labels=None):\n        sequence_output, _ = self.bert(input_ids, token_type_ids, attention_mask,\n                                       output_all_encoded_layers=False)\n        prediction_scores = self.cls(sequence_output)\n\n        if masked_lm_labels is not None:\n            loss_fct = CrossEntropyLoss(ignore_index=-1)\n            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), masked_lm_labels.view(-1))\n            return masked_lm_loss\n        else:\n            return prediction_scores\n\n\nclass BertForNextSentencePrediction(BertPreTrainedModel):\n    """"""BERT model with next sentence prediction head.\n    This module comprises the BERT model followed by the next sentence classification head.\n    Params:\n        config: a BertConfig class instance with the configuration to build a new model.\n    Inputs:\n        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n            a `sentence B` token (see BERT paper for more details).\n        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n            selected in [0, 1]. It\'s a mask to be used if the input sequence length is smaller than the max\n            input sequence length in the current batch. It\'s the mask that we typically use for attention when\n            a batch has varying length sentences.\n        `next_sentence_label`: next sentence classification loss: torch.LongTensor of shape [batch_size]\n            with indices selected in [0, 1].\n            0 => next sentence is the continuation, 1 => next sentence is a random sentence.\n    Outputs:\n        if `next_sentence_label` is not `None`:\n            Outputs the total_loss which is the sum of the masked language modeling loss and the next\n            sentence classification loss.\n        if `next_sentence_label` is `None`:\n            Outputs the next sentence classification logits of shape [batch_size, 2].\n    Example usage:\n    ```python\n    # Already been converted into WordPiece token ids\n    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n    model = BertForNextSentencePrediction(config)\n    seq_relationship_logits = model(input_ids, token_type_ids, input_mask)\n    ```\n    """"""\n    def __init__(self, config):\n        super(BertForNextSentencePrediction, self).__init__(config)\n        self.bert = BertModel(config)\n        self.cls = BertOnlyNSPHead(config)\n        self.apply(self.init_bert_weights)\n\n    def forward(self, input_ids, token_type_ids=None, attention_mask=None, next_sentence_label=None):\n        _, pooled_output = self.bert(input_ids, token_type_ids, attention_mask,\n                                     output_all_encoded_layers=False)\n        seq_relationship_score = self.cls( pooled_output)\n\n        if next_sentence_label is not None:\n            loss_fct = CrossEntropyLoss(ignore_index=-1)\n            next_sentence_loss = loss_fct(seq_relationship_score.view(-1, 2), next_sentence_label.view(-1))\n            return next_sentence_loss\n        else:\n            return seq_relationship_score\n\n\nclass BertForSequenceClassification(BertPreTrainedModel):\n    """"""BERT model for classification.\n    This module is composed of the BERT model with a linear layer on top of\n    the pooled output.\n    Params:\n        `config`: a BertConfig class instance with the configuration to build a new model.\n        `num_labels`: the number of classes for the classifier. Default = 2.\n    Inputs:\n        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n            a `sentence B` token (see BERT paper for more details).\n        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n            selected in [0, 1]. It\'s a mask to be used if the input sequence length is smaller than the max\n            input sequence length in the current batch. It\'s the mask that we typically use for attention when\n            a batch has varying length sentences.\n        `labels`: labels for the classification output: torch.LongTensor of shape [batch_size]\n            with indices selected in [0, ..., num_labels].\n    Outputs:\n        if `labels` is not `None`:\n            Outputs the CrossEntropy classification loss of the output with the labels.\n        if `labels` is `None`:\n            Outputs the classification logits of shape [batch_size, num_labels].\n    Example usage:\n    ```python\n    # Already been converted into WordPiece token ids\n    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n    num_labels = 2\n    model = BertForSequenceClassification(config, num_labels)\n    logits = model(input_ids, token_type_ids, input_mask)\n    ```\n    """"""\n    def __init__(self, config, num_labels):\n        super(BertForSequenceClassification, self).__init__(config)\n        self.num_labels = num_labels\n        self.bert = BertModel(config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, num_labels)\n        self.apply(self.init_bert_weights)\n\n    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n        _, pooled_output = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)\n        pooled_output = self.dropout(pooled_output)\n        logits = self.classifier(pooled_output)\n\n        if labels is not None:\n            loss_fct = CrossEntropyLoss()\n            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n            return loss\n        else:\n            return logits\n\n\nclass BertForMultipleChoice(BertPreTrainedModel):\n    """"""BERT model for multiple choice tasks.\n    This module is composed of the BERT model with a linear layer on top of\n    the pooled output.\n    Params:\n        `config`: a BertConfig class instance with the configuration to build a new model.\n        `num_choices`: the number of classes for the classifier. Default = 2.\n    Inputs:\n        `input_ids`: a torch.LongTensor of shape [batch_size, num_choices, sequence_length]\n            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, num_choices, sequence_length]\n            with the token types indices selected in [0, 1]. Type 0 corresponds to a `sentence A`\n            and type 1 corresponds to a `sentence B` token (see BERT paper for more details).\n        `attention_mask`: an optional torch.LongTensor of shape [batch_size, num_choices, sequence_length] with indices\n            selected in [0, 1]. It\'s a mask to be used if the input sequence length is smaller than the max\n            input sequence length in the current batch. It\'s the mask that we typically use for attention when\n            a batch has varying length sentences.\n        `labels`: labels for the classification output: torch.LongTensor of shape [batch_size]\n            with indices selected in [0, ..., num_choices].\n    Outputs:\n        if `labels` is not `None`:\n            Outputs the CrossEntropy classification loss of the output with the labels.\n        if `labels` is `None`:\n            Outputs the classification logits of shape [batch_size, num_labels].\n    Example usage:\n    ```python\n    # Already been converted into WordPiece token ids\n    input_ids = torch.LongTensor([[[31, 51, 99], [15, 5, 0]], [[12, 16, 42], [14, 28, 57]]])\n    input_mask = torch.LongTensor([[[1, 1, 1], [1, 1, 0]],[[1,1,0], [1, 0, 0]]])\n    token_type_ids = torch.LongTensor([[[0, 0, 1], [0, 1, 0]],[[0, 1, 1], [0, 0, 1]]])\n    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n    num_choices = 2\n    model = BertForMultipleChoice(config, num_choices)\n    logits = model(input_ids, token_type_ids, input_mask)\n    ```\n    """"""\n    def __init__(self, config, num_choices):\n        super(BertForMultipleChoice, self).__init__(config)\n        self.num_choices = num_choices\n        self.bert = BertModel(config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, 1)\n        self.apply(self.init_bert_weights)\n\n    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n        flat_input_ids = input_ids.view(-1, input_ids.size(-1))\n        flat_token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1))\n        flat_attention_mask = attention_mask.view(-1, attention_mask.size(-1))\n        _, pooled_output = self.bert(flat_input_ids, flat_token_type_ids, flat_attention_mask, output_all_encoded_layers=False)\n        pooled_output = self.dropout(pooled_output)\n        logits = self.classifier(pooled_output)\n        reshaped_logits = logits.view(-1, self.num_choices)\n\n        if labels is not None:\n            loss_fct = CrossEntropyLoss()\n            loss = loss_fct(reshaped_logits, labels)\n            return loss\n        else:\n            return reshaped_logits\n\n\nclass BertForTokenClassification(BertPreTrainedModel):\n    """"""BERT model for token-level classification.\n    This module is composed of the BERT model with a linear layer on top of\n    the full hidden state of the last layer.\n    Params:\n        `config`: a BertConfig class instance with the configuration to build a new model.\n        `num_labels`: the number of classes for the classifier. Default = 2.\n    Inputs:\n        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n            a `sentence B` token (see BERT paper for more details).\n        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n            selected in [0, 1]. It\'s a mask to be used if the input sequence length is smaller than the max\n            input sequence length in the current batch. It\'s the mask that we typically use for attention when\n            a batch has varying length sentences.\n        `labels`: labels for the classification output: torch.LongTensor of shape [batch_size, sequence_length]\n            with indices selected in [0, ..., num_labels].\n    Outputs:\n        if `labels` is not `None`:\n            Outputs the CrossEntropy classification loss of the output with the labels.\n        if `labels` is `None`:\n            Outputs the classification logits of shape [batch_size, sequence_length, num_labels].\n    Example usage:\n    ```python\n    # Already been converted into WordPiece token ids\n    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n    num_labels = 2\n    model = BertForTokenClassification(config, num_labels)\n    logits = model(input_ids, token_type_ids, input_mask)\n    ```\n    """"""\n    def __init__(self, config, num_labels):\n        super(BertForTokenClassification, self).__init__(config)\n        self.num_labels = num_labels\n        self.bert = BertModel(config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, num_labels)\n        self.apply(self.init_bert_weights)\n\n    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n        sequence_output, _ = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)\n        sequence_output = self.dropout(sequence_output)\n        logits = self.classifier(sequence_output)\n\n        if labels is not None:\n            loss_fct = CrossEntropyLoss()\n            # Only keep active parts of the loss\n            if attention_mask is not None:\n                active_loss = attention_mask.view(-1) == 1\n                active_logits = logits.view(-1, self.num_labels)[active_loss]\n                active_labels = labels.view(-1)[active_loss]\n                loss = loss_fct(active_logits, active_labels)\n            else:\n                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n            return loss\n        else:\n            return logits\n\nclass BertForQuestionAnswering(BertPreTrainedModel):\n\n    def __init__(self, config):\n        super(BertForQuestionAnswering, self).__init__(config)\n        self.bert = BertModel(config)\n        self.qa_outputs =  nn.Linear(768, 2)\n        self.loss_fct = CrossEntropyLoss()\n\n    def forward(self, input_ids, token_type_ids=None, attention_mask=None, start_positions=None, end_positions=None):\n\n        sequence_output, _ = self.bert(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask, output_all_encoded_layers=False)\n\n        logits = self.qa_outputs(sequence_output)\n        \n        start_logits, end_logits = logits.split(1, dim=-1)\n        start_logits = start_logits.squeeze(-1)\n        end_logits = end_logits.squeeze(-1)\n\n        if start_positions is not None and end_positions is not None:\n\n            start_loss = self.loss_fct(start_logits, start_positions)\n            end_loss = self.loss_fct(end_logits, end_positions)\n            total_loss = (start_loss + end_loss) / 2                       # !!!!!!!!!!\n\n            return total_loss, start_logits, end_logits\n        else:\n            return start_logits, end_logits\n'"
Dureader/predict/predict_data.py,1,"b'import torch\nimport json\n\ndef predict_data(question_text, doc_tokens, tokenizer, max_seq_length, max_query_length):\n\n    features = []\n    query_tokens = list(question_text)\n\n    if len(query_tokens) > max_query_length:\n        query_tokens = query_tokens[0:max_query_length]\n\n    tokens, segment_ids = [], []\n    tokens.append(""[CLS]"")\n    segment_ids.append(0)\n\n    for token in query_tokens:\n        tokens.append(token)\n        segment_ids.append(0)\n\n    tokens.append(""[SEP]"")\n    segment_ids.append(0)\n\n    for i in doc_tokens:\n            tokens.append(i)\n            segment_ids.append(1)\n\n    tokens.append(""[SEP]"")\n    segment_ids.append(1)\n\n    if len(tokens) > max_seq_length:\n        tokens[max_seq_length-1] = ""[SEP]""\n        input_ids = tokenizer.convert_tokens_to_ids(tokens[:max_seq_length])      ## !!! SEP\n        segment_ids = segment_ids[:max_seq_length]\n    else:\n        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n   \n    input_mask = [1] * len(input_ids)\n\n    assert len(input_ids) == len(segment_ids)\n\n    return (torch.LongTensor(input_ids).unsqueeze(0), torch.LongTensor(input_mask).unsqueeze(0), torch.LongTensor(segment_ids).unsqueeze(0))\n\n'"
Dureader/predict/predicting.py,7,"b'import json\nimport args\nimport torch\nimport pickle\nimport torch.nn as nn\nfrom tqdm import tqdm\n\nimport predict_data\nfrom tokenization import BertTokenizer\nfrom file_utils import PYTORCH_PRETRAINED_BERT_CACHE\nfrom modeling import BertForQuestionAnswering, BertConfig\n\ndef find_best_answer_for_passage(start_probs, end_probs, passage_len, question):\n        best_start, best_end, max_prob = -1, -1, 0\n\n        start_probs, end_probs  = start_probs.unsqueeze(0), end_probs.unsqueeze(0)\n        prob_start, best_start = torch.max(start_probs, 1)\n        prob_end, best_end = torch.max(end_probs, 1)\n        num = 0\n        while True:\n            if num > 3:\n                break\n            if best_end >= best_start:\n                break\n            else:\n                start_probs[0][best_start], end_probs[0][best_end] = 0.0, 0.0\n                prob_start, best_start = torch.max(start_probs, 1)\n                prob_end, best_end = torch.max(end_probs, 1)\n            num += 1\n        max_prob = prob_start * prob_end\n\n        if best_start <= best_end:\n            return (best_start, best_end), max_prob\n        else:\n            return (best_end, best_start), max_prob\n\ndef find_best_answer(sample, start_probs, end_probs, prior_scores=(0.44, 0.23, 0.15, 0.09, 0.07)):\n\n        best_p_idx, best_span, best_score = None, None, 0\n\n        for p_idx, passage in enumerate(sample[\'doc_tokens\'][:args.max_para_num]):\n    \n            passage_len = min(args.max_seq_length, len(passage[\'doc_tokens\']))\n            answer_span, score = find_best_answer_for_passage(start_probs[p_idx], end_probs[p_idx], passage_len, sample[\'question_text\'])\n\n            score *= prior_scores[p_idx]\n\n            answer = ""p"" + sample[\'question_text\'] + ""\xe3\x80\x82"" + sample[\'doc_tokens\'][p_idx][\'doc_tokens\']\n            answer = answer[answer_span[0]: answer_span[1]+1]\n\n            if score > best_score:\n                best_score = score\n                best_p_idx = p_idx\n                best_span = answer_span\n   \n        if best_p_idx is None or best_span is None:\n            best_answer = \'\'\n        else:\n            para = ""p"" + sample[\'question_text\'] + ""\xe3\x80\x82"" + sample[\'doc_tokens\'][best_p_idx][\'doc_tokens\']\n            best_answer = \'\'.join(para[best_span[0]: best_span[1]+1])\n\n        return best_answer, best_p_idx\n\ndef evaluate(model, result_file):\n\n    with open(args.predict_example_files,\'rb\') as f:\n        eval_examples = pickle.load(f)\n\n    with torch.no_grad():\n        tokenizer = BertTokenizer.from_pretrained(\'bert-base-chinese\', do_lower_case=True)\n        model.eval()\n        pred_answers, ref_answers = [], []\n\n        for step, example in enumerate(tqdm(eval_examples)):\n            start_probs, end_probs = [], []\n            question_text = example[\'question_text\']\n   \n            for p_num, doc_tokens in enumerate(example[\'doc_tokens\'][:args.max_para_num]):             \n                (input_ids, input_mask, segment_ids) = \\\n                                         predict_data.predict_data(question_text, doc_tokens[\'doc_tokens\'], tokenizer, args.max_seq_length, args.max_query_length)\n               \n                start_prob, end_prob = model(input_ids, segment_ids, attention_mask=input_mask)     # !!!!!!!!!!\n                start_probs.append(start_prob.squeeze(0))\n                end_probs.append(end_prob.squeeze(0))\n                \n            best_answer, docs_index = find_best_answer(example, start_probs, end_probs)\n\n            pred_answers.append({\'question_id\': example[\'id\'],\n                                 \'question\':example[\'question_text\'],\n                                 \'question_type\': example[\'question_type\'],\n                                 \'answers\': [best_answer],\n                                 \'entity_answers\': [[]],\n                                 \'yesno_answers\': []})\n            if \'answers\' in example:\n                ref_answers.append({\'question_id\': example[\'id\'],\n                                    \'question_type\': example[\'question_type\'],\n                                    \'answers\': example[\'answers\'],\n                                    \'entity_answers\': [[]],\n                                    \'yesno_answers\': []})\n        with open(result_file, \'w\', encoding=\'utf-8\') as fout:\n            for pred_answer in pred_answers:\n                fout.write(json.dumps(pred_answer, ensure_ascii=False) + \'\\n\')\n        with open(""../metric/ref.json"", \'w\', encoding=\'utf-8\') as fout:\n            for pred_answer in ref_answers:\n                fout.write(json.dumps(pred_answer, ensure_ascii=False) + \'\\n\')\n\ndef eval_all():\n   \n    output_model_file = ""../model_dir/best_model""\n    output_config_file = ""../model_dir/bert_config.json""  \n    \n    config = BertConfig(output_config_file)\n    model = BertForQuestionAnswering(config)\n    model.load_state_dict(torch.load(output_model_file)) #, map_location=\'cpu\'))\n    evaluate(model.cpu(),result_file=""../metric/predicts.json"")\n   \neval_all()\n'"
Dureader/predict/tokenization.py,0,"b'from __future__ import absolute_import, division, print_function, unicode_literals\n\nimport collections\nimport logging\nimport os\nimport unicodedata\nfrom io import open\n\nfrom file_utils import cached_path\n\nlogger = logging.getLogger(__name__)\n\nPRETRAINED_VOCAB_ARCHIVE_MAP = {\n    \'bert-base-uncased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt"",\n    \'bert-large-uncased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-vocab.txt"",\n    \'bert-base-cased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt"",\n    \'bert-large-cased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-vocab.txt"",\n    \'bert-base-multilingual-uncased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt"",\n    \'bert-base-multilingual-cased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt"",\n    \'bert-base-chinese\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-vocab.txt"",\n}\nPRETRAINED_VOCAB_POSITIONAL_EMBEDDINGS_SIZE_MAP = {\n    \'bert-base-uncased\': 512,\n    \'bert-large-uncased\': 512,\n    \'bert-base-cased\': 512,\n    \'bert-large-cased\': 512,\n    \'bert-base-multilingual-uncased\': 512,\n    \'bert-base-multilingual-cased\': 512,\n    \'bert-base-chinese\': 512,\n}\nVOCAB_NAME = \'vocab.txt\'\n\n\ndef load_vocab(vocab_file):\n    """"""Loads a vocabulary file into a dictionary.""""""\n    vocab = collections.OrderedDict()\n    index = 0\n    with open(vocab_file, ""r"", encoding=""utf-8"") as reader:\n        while True:\n            token = reader.readline()\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = index\n            index += 1\n    return vocab\n\n\ndef whitespace_tokenize(text):\n    """"""Runs basic whitespace cleaning and splitting on a piece of text.""""""\n    text = text.strip()\n    if not text:\n        return []\n    tokens = text.split()\n    return tokens\n\n\nclass BertTokenizer(object):\n    """"""Runs end-to-end tokenization: punctuation splitting + wordpiece""""""\n\n    def __init__(self, vocab_file, do_lower_case=True, max_len=None, do_basic_tokenize=True,\n                 never_split=(""[UNK]"", ""[SEP]"", ""[PAD]"", ""[CLS]"", ""[MASK]"")):\n        """"""Constructs a BertTokenizer.\n        Args:\n          vocab_file: Path to a one-wordpiece-per-line vocabulary file\n          do_lower_case: Whether to lower case the input\n                         Only has an effect when do_wordpiece_only=False\n          do_basic_tokenize: Whether to do basic tokenization before wordpiece.\n          max_len: An artificial maximum length to truncate tokenized sequences to;\n                         Effective maximum length is always the minimum of this\n                         value (if specified) and the underlying BERT model\'s\n                         sequence length.\n          never_split: List of tokens which will never be split during tokenization.\n                         Only has an effect when do_wordpiece_only=False\n        """"""\n        if not os.path.isfile(vocab_file):\n            raise ValueError(\n                ""Can\'t find a vocabulary file at path \'{}\'. To load the vocabulary from a Google pretrained ""\n                ""model use `tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)`"".format(vocab_file))\n        self.vocab = load_vocab(vocab_file)\n        self.ids_to_tokens = collections.OrderedDict(\n            [(ids, tok) for tok, ids in self.vocab.items()])\n        self.do_basic_tokenize = do_basic_tokenize\n        if do_basic_tokenize:\n          self.basic_tokenizer = BasicTokenizer(do_lower_case=do_lower_case,\n                                                never_split=never_split)\n        self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab)\n        self.max_len = max_len if max_len is not None else int(1e12)\n\n    def tokenize(self, text):\n        if self.do_basic_tokenize:\n          split_tokens = []\n          for token in self.basic_tokenizer.tokenize(text):\n              for sub_token in self.wordpiece_tokenizer.tokenize(token):\n                  split_tokens.append(sub_token)\n        else:\n          split_tokens = self.wordpiece_tokenizer.tokenize(text)\n        return split_tokens\n\n    def convert_tokens_to_ids(self, tokens):\n        """"""Converts a sequence of tokens into ids using the vocab.""""""\n        ids = []\n        for token in tokens:\n            if token != ""[CLS]"" and token != ""[SEP]"":\n                token = token.lower()\n            if token in self.vocab:\n                ids.append(self.vocab[token])\n            else:\n                ids.append(self.vocab[""[UNK]""])\n        if len(ids) > self.max_len:\n            logger.warning(\n                ""Token indices sequence length is longer than the specified maximum ""\n                "" sequence length for this BERT model ({} > {}). Running this""\n                "" sequence through BERT will result in indexing errors"".format(len(ids), self.max_len)\n            )\n        return ids\n\n    def convert_ids_to_tokens(self, ids):\n        """"""Converts a sequence of ids in wordpiece tokens using the vocab.""""""\n        tokens = []\n        for i in ids:\n            tokens.append(self.ids_to_tokens[i])\n        return tokens\n\n    @classmethod\n    def from_pretrained(cls, pretrained_model_name_or_path, cache_dir=None, *inputs, **kwargs):\n        """"""\n        Instantiate a PreTrainedBertModel from a pre-trained model file.\n        Download and cache the pre-trained model file if needed.\n        """"""\n        if pretrained_model_name_or_path in PRETRAINED_VOCAB_ARCHIVE_MAP:\n            vocab_file = PRETRAINED_VOCAB_ARCHIVE_MAP[pretrained_model_name_or_path]\n        else:\n            vocab_file = pretrained_model_name_or_path\n        if os.path.isdir(vocab_file):\n            vocab_file = os.path.join(vocab_file, VOCAB_NAME)\n        # redirect to the cache, if necessary\n        try:\n            resolved_vocab_file = cached_path(vocab_file, cache_dir=cache_dir)\n        except EnvironmentError:\n            logger.error(\n                ""Model name \'{}\' was not found in model name list ({}). ""\n                ""We assumed \'{}\' was a path or url but couldn\'t find any file ""\n                ""associated to this path or url."".format(\n                    pretrained_model_name_or_path,\n                    \', \'.join(PRETRAINED_VOCAB_ARCHIVE_MAP.keys()),\n                    vocab_file))\n            return None\n        if resolved_vocab_file == vocab_file:\n            logger.info(""loading vocabulary file {}"".format(vocab_file))\n        else:\n            logger.info(""loading vocabulary file {} from cache at {}"".format(\n                vocab_file, resolved_vocab_file))\n        if pretrained_model_name_or_path in PRETRAINED_VOCAB_POSITIONAL_EMBEDDINGS_SIZE_MAP:\n            # if we\'re using a pretrained model, ensure the tokenizer wont index sequences longer\n            # than the number of positional embeddings\n            max_len = PRETRAINED_VOCAB_POSITIONAL_EMBEDDINGS_SIZE_MAP[pretrained_model_name_or_path]\n            kwargs[\'max_len\'] = min(kwargs.get(\'max_len\', int(1e12)), max_len)\n        # Instantiate tokenizer.\n        tokenizer = cls(resolved_vocab_file, *inputs, **kwargs)\n        return tokenizer\n\n\nclass BasicTokenizer(object):\n    """"""Runs basic tokenization (punctuation splitting, lower casing, etc.).""""""\n\n    def __init__(self,\n                 do_lower_case=True,\n                 never_split=(""[UNK]"", ""[SEP]"", ""[PAD]"", ""[CLS]"", ""[MASK]"")):\n        """"""Constructs a BasicTokenizer.\n        Args:\n          do_lower_case: Whether to lower case the input.\n        """"""\n        self.do_lower_case = do_lower_case\n        self.never_split = never_split\n\n    def tokenize(self, text):\n        """"""Tokenizes a piece of text.""""""\n        text = self._clean_text(text)\n       \n        text = self._tokenize_chinese_chars(text)\n        orig_tokens = whitespace_tokenize(text)\n        split_tokens = []\n        for token in orig_tokens:\n            if self.do_lower_case and token not in self.never_split:\n                token = token.lower()\n                token = self._run_strip_accents(token)\n            split_tokens.extend(self._run_split_on_punc(token))\n\n        output_tokens = whitespace_tokenize("" "".join(split_tokens))\n        return output_tokens\n\n    def _run_strip_accents(self, text):\n        """"""Strips accents from a piece of text.""""""\n        text = unicodedata.normalize(""NFD"", text)\n        output = []\n        for char in text:\n            cat = unicodedata.category(char)\n            if cat == ""Mn"":\n                continue\n            output.append(char)\n        return """".join(output)\n\n    def _run_split_on_punc(self, text):\n        """"""Splits punctuation on a piece of text.""""""\n        if text in self.never_split:\n            return [text]\n        chars = list(text)\n        i = 0\n        start_new_word = True\n        output = []\n        while i < len(chars):\n            char = chars[i]\n            if _is_punctuation(char):\n                output.append([char])\n                start_new_word = True\n            else:\n                if start_new_word:\n                    output.append([])\n                start_new_word = False\n                output[-1].append(char)\n            i += 1\n\n        return ["""".join(x) for x in output]\n\n    def _tokenize_chinese_chars(self, text):\n        """"""Adds whitespace around any CJK character.""""""\n        output = []\n        for char in text:\n            cp = ord(char)\n            if self._is_chinese_char(cp):\n                output.append("" "")\n                output.append(char)\n                output.append("" "")\n            else:\n                output.append(char)\n        return """".join(output)\n\n    def _is_chinese_char(self, cp):\n        """"""Checks whether CP is the codepoint of a CJK character.""""""\n        # This defines a ""chinese character"" as anything in the CJK Unicode block:\n        #   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)\n        #\n        # Note that the CJK Unicode block is NOT all Japanese and Korean characters,\n        # despite its name. The modern Korean Hangul alphabet is a different block,\n        # as is Japanese Hiragana and Katakana. Those alphabets are used to write\n        # space-separated words, so they are not treated specially and handled\n        # like the all of the other languages.\n        if ((cp >= 0x4E00 and cp <= 0x9FFF) or  #\n                (cp >= 0x3400 and cp <= 0x4DBF) or  #\n                (cp >= 0x20000 and cp <= 0x2A6DF) or  #\n                (cp >= 0x2A700 and cp <= 0x2B73F) or  #\n                (cp >= 0x2B740 and cp <= 0x2B81F) or  #\n                (cp >= 0x2B820 and cp <= 0x2CEAF) or\n                (cp >= 0xF900 and cp <= 0xFAFF) or  #\n                (cp >= 0x2F800 and cp <= 0x2FA1F)):  #\n            return True\n\n        return False\n\n    def _clean_text(self, text):\n        """"""Performs invalid character removal and whitespace cleanup on text.""""""\n        output = []\n        for char in text:\n            cp = ord(char)\n            if cp == 0 or cp == 0xfffd or _is_control(char):\n                continue\n            if _is_whitespace(char):\n                output.append("" "")\n            else:\n                output.append(char)\n        return """".join(output)\n\n\nclass WordpieceTokenizer(object):\n    """"""Runs WordPiece tokenization.""""""\n\n    def __init__(self, vocab, unk_token=""[UNK]"", max_input_chars_per_word=100):\n        self.vocab = vocab\n        self.unk_token = unk_token\n        self.max_input_chars_per_word = max_input_chars_per_word\n\n    def tokenize(self, text):\n        """"""Tokenizes a piece of text into its word pieces.\n        This uses a greedy longest-match-first algorithm to perform tokenization\n        using the given vocabulary.\n        For example:\n          input = ""unaffable""\n          output = [""un"", ""##aff"", ""##able""]\n        Args:\n          text: A single token or whitespace separated tokens. This should have\n            already been passed through `BasicTokenizer`.\n        Returns:\n          A list of wordpiece tokens.\n        """"""\n\n        output_tokens = []\n        for token in whitespace_tokenize(text):\n            chars = list(token)\n            if len(chars) > 100:                            ## !!! \n                output_tokens.append(self.unk_token)\n                continue\n\n            is_bad = False\n            start = 0\n            sub_tokens = []\n            while start < len(chars):\n                end = len(chars)\n                cur_substr = None\n                while start < end:\n                    substr = """".join(chars[start:end])\n                    if start > 0:\n                        substr = ""##"" + substr\n                    if substr in self.vocab:\n                        cur_substr = substr\n                        break\n                    end -= 1\n                if cur_substr is None:\n                    is_bad = True\n                    break\n                sub_tokens.append(cur_substr)\n                start = end\n\n            if is_bad:\n                output_tokens.append(self.unk_token)\n            else:\n                output_tokens.extend(sub_tokens)\n        return output_tokens\n\n\ndef _is_whitespace(char):\n    """"""Checks whether `chars` is a whitespace character.""""""\n    # \\t, \\n, and \\r are technically contorl characters but we treat them\n    # as whitespace since they are generally considered as such.\n    if char == "" "" or char == ""\\t"" or char == ""\\n"" or char == ""\\r"":\n        return True\n    cat = unicodedata.category(char)\n    if cat == ""Zs"":\n        return True\n    return False\n\n\ndef _is_control(char):\n    """"""Checks whether `chars` is a control character.""""""\n    # These are technically control characters but we count them as whitespace\n    # characters.\n    if char == ""\\t"" or char == ""\\n"" or char == ""\\r"":\n        return False\n    cat = unicodedata.category(char)\n    if cat.startswith(""C""):\n        return True\n    return False\n\n\ndef _is_punctuation(char):\n    """"""Checks whether `chars` is a punctuation character.""""""\n    cp = ord(char)\n    # We treat all non-letter/number ASCII as punctuation.\n    # Characters such as ""^"", ""$"", and ""`"" are not in the Unicode\n    # Punctuation class but we treat them as punctuation anyways, for\n    # consistency.\n    if ((cp >= 33 and cp <= 47) or (cp >= 58 and cp <= 64) or\n            (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126)):\n        return True\n    cat = unicodedata.category(char)\n    if cat.startswith(""P""):\n        return True\n    return False\n'"
Dureader/predict/util.py,0,"b'import json\nimport args\nimport torch\nimport pickle\nfrom tqdm import tqdm\n\ndef creat_examples(filename_1, filename_2, result):         \n\n    examples = []\n    with open(filename_1, \'r\', encoding=\'utf-8\') as f:\n        for line in tqdm(f.readlines()):\n\n            source = json.loads(line.strip())\n            source[\'doc_tokens\'] = []\n            for doc in source[\'documents\']:\n                    ques_len = len(doc[\'segmented_title\']) + 1\n                    clean_doc = """".join(doc[\'segmented_paragraphs\'][doc[\'most_related_para\']][ques_len:])\n                    if len(clean_doc) > 4:\n                        source[\'doc_tokens\'].append( {\'doc_tokens\': clean_doc} )\n\n            example = ({\n                        \'id\':source[\'question_id\'],\n                        \'question_text\':source[\'question\'].strip(),\n                        \'question_type\': source[\'question_type\'],\n                        \'doc_tokens\':source[\'doc_tokens\'],\n                        \'answers\':source[\'answers\']})\n            examples.append(example)\n    with open(filename_2, \'r\', encoding=\'utf-8\') as f:\n        for line in tqdm(f.readlines()):\n            source = json.loads(line.strip())\n            source[\'doc_tokens\'] = []\n            for doc in source[\'documents\']:\n                    ques_len = len(doc[\'segmented_title\']) + 1\n                    clean_doc = """".join(doc[\'segmented_paragraphs\'][doc[\'most_related_para\']][ques_len:])\n                    if len(clean_doc) > 4:\n                        source[\'doc_tokens\'].append( {\'doc_tokens\': clean_doc} )\n        \n            if len(source[\'documents\']) == 0:\n                print(""error"")\n                continue\n            example = ({\n                        \'id\':source[\'question_id\'],\n                        \'question_text\':source[\'question\'].strip(),\n                        \'question_type\': source[\'question_type\'],\n                        \'doc_tokens\':source[\'doc_tokens\'],\n                        \'answers\':source[\'answers\'] })\n            examples.append(example)\n\n    print(""{} questions in total"".format(len(examples)))\n    with open(result,\'wb\') as fw:\n        pickle.dump(examples, fw)\n\nif __name__ == ""__main__"":\n    creat_examples(filename_1=args.dev_zhidao_input_file,\n                   filename_2=args.dev_search_input_file,\n                   result=args.predict_example_files     )\n\n'"
