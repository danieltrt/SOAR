file_path,api_count,code
10.Dataset_and_Dataloader 自定义数据读取/Dataset_Dataloader_and_training_examples.py,59,"b'\n#%%\nimport torch\nfrom torchvision import datasets\n\ncifar10 = datasets.CIFAR10(\'data\', train=True, download=True)\n\n\n#%%\ncifar10_val = datasets.CIFAR10(\'data\', train=False, download=True)\n\n\n#%%\ntype(cifar10)\n\n\n#%%\nisinstance(cifar10, torch.utils.data.Dataset)\n\n\n#%%\nlen(cifar10)\n\n\n#%%\nimg, label = cifar10[99]\n\n\n#%%\nprint(img)\n\n\n#%%\nfrom matplotlib import pyplot as plt\n\nplt.imshow(img)\nplt.show()\n\n\n#%%\nlabel\n\n\n#%%\nfrom torchvision import transforms\ndir(transforms)\n\n\n#%%\nfrom torchvision import transforms\n\nto_tensor = transforms.ToTensor()\n\nimg, label = cifar10[99]\n\nimg_t = to_tensor(img)\n\nimg_t.shape\n\n\n#%%\ncifar10 = datasets.CIFAR10(\'data\', train=True, download=True,\n                          transform=transforms.ToTensor())\n\n\n#%%\nimg, _ = cifar10[99]\ntype(img)\n\n\n#%%\nimg.shape\n\n\n#%%\nimg.dtype\n\n\n#%%\nimg.min(), img.max()\n\n\n#%%\nplt.imshow(img.permute(1, 2, 0))\nplt.show()\n\n\n#%%\nimgs = torch.stack([img for img, _ in cifar10], dim=3)\nimgs.shape\n\n\n#%%\nimgs.view(3, -1).mean(dim=1)\n\n\n#%%\nimgs.view(3, -1).std(dim=1)\n\n\n#%%\ntransforms.Normalize((0.4915, 0.4823, 0.4468), (0.2470, 0.2435, 0.2616))\n\n\n#%%\ncifar10 = datasets.CIFAR10(\'data\', train=True, download=True,\n                          transform=transforms.Compose([\n                              transforms.ToTensor(),\n                              transforms.Normalize((0.4915, 0.4823, 0.4468),\n                                                   (0.2470, 0.2435, 0.2616))\n                          ]))\n\n\n#%%\ncifar10_val = datasets.CIFAR10(\'data\', train=False, download=True,\n                          transform=transforms.Compose([\n                              transforms.ToTensor(),\n                              transforms.Normalize((0.4915, 0.4823, 0.4468),\n                                                   (0.2470, 0.2435, 0.2616))\n                          ]))\n\n\n#%%\nimg, _ = cifar10[99]\n\nplt.imshow(img.permute(1, 2, 0))\nplt.show()\n\n\n#%%\nlabel_map = {0: 0, 2: 1}\ncifar2 = [(img, label_map[label]) for img, label in cifar10 if label in [0, 2]]\ncifar2_val = [(img, label_map[label]) for img, label in cifar10_val if label in [0, 2]]\n\n\n#%%\nimport torch.nn as nn\n\nmodel = nn.Sequential(\n            nn.Linear(3072, 512),\n            nn.Tanh(),\n            nn.Linear(512, n_output_features))\n\n\n#%%\ndef softmax(x):\n    return torch.exp(x) / torch.exp(x).sum()\n\n\n#%%\nx = torch.tensor([1.0, 2.0, 3.0])\n\nsoftmax(x)\n\n\n#%%\nsoftmax(x).sum()\n\n\n#%%\nimport torch\nimport torch.nn as nn\n\ntorch.manual_seed(123)\n\nsoftmax = nn.Softmax(dim=1)\n\nx = torch.tensor([[1.0, 2.0, 3.0],\n                  [1.0, 2.0, 3.0]])\n\nsoftmax(x)\n\n\n#%%\nmodel = nn.Sequential(\n            nn.Linear(3072, 512),\n            nn.Tanh(),\n            nn.Linear(512, 2),\n            nn.Softmax(dim=1))\n\n\n#%%\nimg, _ = cifar2[0]\n\nplt.imshow(img.permute(1, 2, 0))\nplt.show()\n\n\n#%%\nimg_batch = img.view(-1).unsqueeze(0)\n\n\n#%%\nout = model(img_batch)\nout\n\n\n#%%\n_, index = torch.max(out, dim=1)\n\nindex\n\n\n#%%\nsoftmax = nn.Softmax(dim=1)\n\nlog_softmax = nn.LogSoftmax(dim=1)\n\nx = torch.tensor([[0.0, 104.0]])\n\nsoftmax(x)\n\n\n#%%\ntorch.log(softmax(x))\n\n\n#%%\nlog_softmax(x)\n\n\n#%%\ntorch.exp(log_softmax(x))\n\n\n#%%\nmodel = nn.Sequential(\n            nn.Linear(3072, 512),\n            nn.Tanh(),\n            nn.Linear(512, 2),\n            nn.LogSoftmax(dim=1))\n\n\n#%%\nloss = nn.NLLLoss()\n\n\n#%%\nimg, label = cifar2[0]\n\nout = model(img.view(-1).unsqueeze(0))\n\nloss(out, torch.tensor([label]))\n\n\n#%%\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nmodel = nn.Sequential(\n            nn.Linear(3072, 512),\n            nn.Tanh(),\n            nn.Linear(512, 2),\n            nn.LogSoftmax(dim=1))\n\nlearning_rate = 1e-4\n\noptimizer = optim.SGD(model.parameters(), lr=learning_rate)\n\nloss_fn = nn.NLLLoss()\n\nnepochs = 100\n\nfor epoch in range(nepochs):\n    for img, label in cifar2:\n        out = model(img.view(-1).unsqueeze(0))\n        loss = loss_fn(out, torch.tensor([label]))\n                \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    print(""Epoch: %d, Loss: %f"" % (epoch, float(loss)))\n\n\n#%%\ntrain_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=True)\n\n\n#%%\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\ntrain_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=True)\n\nmodel = nn.Sequential(\n            nn.Linear(3072, 128),\n            nn.Tanh(),\n            nn.Linear(128, 2),\n            nn.LogSoftmax(dim=1))\n\nlearning_rate = 1e-2\n\noptimizer = optim.SGD(model.parameters(), lr=learning_rate)\n\nloss_fn = nn.NLLLoss()\n\nnepochs = 100\n\nfor epoch in range(nepochs):\n    for imgs, labels in train_loader:\n        outputs = model(imgs.view(imgs.shape[0], -1))\n        loss = loss_fn(outputs, labels)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    print(""Epoch: %d, Loss: %f"" % (epoch, float(loss)))\n\n\n#%%\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\ntrain_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=True)\n\nmodel = nn.Sequential(\n            nn.Linear(3072, 512),\n            nn.Tanh(),\n            nn.Linear(512, 2),\n            nn.LogSoftmax(dim=1))\n\nlearning_rate = 1e-2\n\noptimizer = optim.SGD(model.parameters(), lr=learning_rate)\n\nloss_fn = nn.NLLLoss()\n\nnepochs = 100\n\nfor epoch in range(nepochs):\n    for imgs, labels in train_loader:\n        outputs = model(imgs.view(imgs.shape[0], -1))\n        loss = loss_fn(outputs, labels)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    print(""Epoch: %d, Loss: %f"" % (epoch, float(loss)))\n\n\n#%%\ntrain_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=False)\n\ncorrect = 0\ntotal = 0\n\nwith torch.no_grad():\n    for imgs, labels in train_loader:\n        outputs = model(imgs.view(imgs.shape[0], -1))\n        _, predicted = torch.max(outputs, dim=1)\n        total += labels.shape[0]\n        correct += int((predicted == labels).sum())\n        \nprint(""Accuracy: %f"" % (correct / total))\n\n\n#%%\nval_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64, shuffle=False)\n\ncorrect = 0\ntotal = 0\n\nwith torch.no_grad():\n    for imgs, labels in val_loader:\n        outputs = model(imgs.view(imgs.shape[0], -1))\n        _, predicted = torch.max(outputs, dim=1)\n        total += labels.shape[0]\n        correct += int((predicted == labels).sum())\n        \nprint(""Accuracy: %f"" % (correct / total))\n\n\n#%%\nmodel = nn.Sequential(\n            nn.Linear(3072, 1024),\n            nn.Tanh(),\n            nn.Linear(1024, 512),\n            nn.Tanh(),\n            nn.Linear(512, 128),\n            nn.Tanh(),\n            nn.Linear(128, 2),\n            nn.LogSoftmax(dim=1))\n\n\n#%%\nmodel = nn.Sequential(\n            nn.Linear(3072, 1024),\n            nn.Tanh(),\n            nn.Linear(1024, 512),\n            nn.Tanh(),\n            nn.Linear(512, 128),\n            nn.Tanh(),\n            nn.Linear(128, 2))\n\nloss_fn = nn.CrossEntropyLoss()\n\n\n#%%\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\ntrain_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=True)\n\nmodel = nn.Sequential(\n            nn.Linear(3072, 1024),\n            nn.Tanh(),\n            nn.Linear(1024, 512),\n            nn.Tanh(),\n            nn.Linear(512, 128),\n            nn.Tanh(),\n            nn.Linear(128, 2))\n\nlearning_rate = 1e-2\n\noptimizer = optim.SGD(model.parameters(), lr=learning_rate)\n\nloss_fn = nn.CrossEntropyLoss()\n\nnepochs = 100\n\nfor epoch in range(nepochs):\n    for imgs, labels in train_loader:\n        outputs = model(imgs.view(imgs.shape[0], -1))\n        loss = loss_fn(outputs, labels)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    print(""Epoch: %d, Loss: %f"" % (epoch, float(loss)))\n\n\n#%%\ntrain_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=False)\n\ncorrect = 0\ntotal = 0\n\nwith torch.no_grad():\n    for imgs, labels in train_loader:\n        outputs = model(imgs.view(imgs.shape[0], -1))\n        _, predicted = torch.max(outputs, dim=1)\n        total += labels.shape[0]\n        correct += int((predicted == labels).sum())\n        \nprint(""Accuracy: %f"" % (correct / total))\n\n\n#%%\nval_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64, shuffle=False)\n\ncorrect = 0\ntotal = 0\n\nwith torch.no_grad():\n    for imgs, labels in val_loader:\n        outputs = model(imgs.view(imgs.shape[0], -1))\n        _, predicted = torch.max(outputs, dim=1)\n        total += labels.shape[0]\n        correct += int((predicted == labels).sum())\n        \nprint(""Accuracy: %f"" % (correct / total))\n\n\n#%%\nsum([p.numel() for p in model.parameters()])\n\n\n#%%\nsum([p.numel() for p in model.parameters() if p.requires_grad == True])\n\n\n#%%\nfirst_model = nn.Sequential(\n                nn.Linear(3072, 512),\n                nn.Tanh(),\n                nn.Linear(512, 2),\n                nn.LogSoftmax(dim=1))\n\nsum([p.numel() for p in first_model.parameters()])\n\n\n#%%\nsum([p.numel() for p in nn.Linear(3072, 512).parameters()])\n\n\n#%%\nsum([p.numel() for p in nn.Linear(3072, 1024).parameters()])\n\n\n#%%\nlinear = nn.Linear(3072, 1024)\n\nlinear.weight.shape, linear.bias.shape\n\n\n#%%\nconv = nn.Conv2d(3, 16, kernel_size=3)\n\n\n#%%\nconv.weight.shape\n\n\n#%%\nconv.bias.shape\n\n\n#%%\nimg, _ = cifar2[0]\n\noutput = conv(img.unsqueeze(0))\n\n\n#%%\nimg.unsqueeze(0).shape, output.shape\n\n\n#%%\nplt.imshow(output[0, 0].detach(), cmap=\'gray\')\nplt.show()\n\n\n#%%\noutput.shape\n\n\n#%%\nconv = nn.Conv2d(3, 1, kernel_size=3, padding=1)\n\n\n#%%\noutput = conv(img.unsqueeze(0))\n\noutput.shape\n\n\n#%%\nwith torch.no_grad():\n    conv.bias.zero_()\n\n\n#%%\nwith torch.no_grad():\n    conv.weight.fill_(1.0 / 9.0)\n\n\n#%%\noutput = conv(img.unsqueeze(0))\nplt.imshow(output[0, 0].detach(), cmap=\'gray\')\nplt.show()\n\n\n#%%\nconv = nn.Conv2d(3, 1, kernel_size=3, padding=1)\n\nwith torch.no_grad():\n    conv.weight[:] = torch.tensor([[-1.0, 0.0, 1.0],\n                                   [-1.0, 0.0, 1.0],\n                                   [-1.0, 0.0, 1.0]])\n    conv.bias.zero_()\n\n\n#%%\noutput = conv(img.unsqueeze(0))\nplt.imshow(output[0, 0].detach(), cmap=\'gray\')\nplt.show()\n\n\n#%%\npool = nn.MaxPool2d(2)\n\n\n#%%\noutput = pool(img.unsqueeze(0))\n\noutput.shape\n\n\n#%%\nmodel = nn.Sequential(\n            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n            nn.Tanh(),\n            nn.MaxPool2d(2),\n            nn.Conv2d(32, 16, kernel_size=3, padding=1),\n            nn.Tanh(),\n            nn.MaxPool2d(2),\n            nn.Conv2d(16, 8, kernel_size=3, padding=1),\n            nn.Tanh(),\n            nn.MaxPool2d(2),\n            ...)\n\n\n#%%\nmodel = nn.Sequential(\n            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n            nn.Tanh(),\n            nn.MaxPool2d(2),\n            nn.Conv2d(32, 16, kernel_size=3, padding=1),\n            nn.Tanh(),\n            nn.MaxPool2d(2),\n            nn.Conv2d(16, 8, kernel_size=3, padding=1),\n            nn.Tanh(),\n            nn.MaxPool2d(2),\n            # WARNING: something missing here\n            nn.Linear(128, 32),\n            nn.Tanh(),\n            nn.Linear(32, 2))\n\n\n#%%\nsum([p.numel() for p in model.parameters()])\n\n\n#%%\nmodel(img.unsqueeze(0))\n\n\n#%%\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n        self.act1 = nn.Tanh()\n        self.pool1 = nn.MaxPool2d(2)\n        self.conv2 = nn.Conv2d(32, 16, kernel_size=3, padding=1)\n        self.act2 = nn.Tanh()\n        self.pool2 = nn.MaxPool2d(2)\n        self.fc1 = nn.Linear(8 * 8 * 8, 32)\n        self.act4 = nn.Tanh()\n        self.fc2 = nn.Linear(32, 2)\n\n    def forward(self, x):\n        out = self.pool1(self.act1(self.conv1(x)))\n        out = self.pool2(self.act2(self.conv2(out)))\n        out = out.view(-1, 8 * 8 * 8)\n        out = self.act4(self.fc1(out))\n        out = self.fc2(out)\n        return out\n\n\n#%%\nmodel = Net()\n\nsum([p.numel() for p in model.parameters()])\n\n\n#%%\nimport torch.nn.functional as F\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(32, 16, kernel_size=3, padding=1)\n        self.conv3 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n        self.fc1 = nn.Linear(128, 32)\n        self.fc2 = nn.Linear(32, 2)\n        \n    def forward(self, x):\n        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n        out = F.max_pool2d(torch.tanh(self.conv3(out)), 2)\n        out = out.view(-1, 8 * 4 * 4)\n        out = torch.tanh(self.fc1(out))\n        out = self.fc2(out)\n        return out\n\n\n#%%\nmodel = Net()\nmodel(img.unsqueeze(0))\n\n\n#%%\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ntrain_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=True)\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n        self.fc1 = nn.Linear(8 * 8 * 8, 32)\n        self.fc2 = nn.Linear(32, 2)\n        \n    def forward(self, x):\n        out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n        out = F.max_pool2d(torch.relu(self.conv2(out)), 2)\n        out = out.view(-1, 8 * 8 * 8)\n        out = torch.tanh(self.fc1(out))\n        out = self.fc2(out)\n        return out\n    \nmodel = Net()\n\nlearning_rate = 1e-2\n\noptimizer = optim.SGD(model.parameters(), lr=learning_rate)\n\nloss_fn = nn.CrossEntropyLoss()\n\nnepochs = 100\n\nfor epoch in range(nepochs):\n    for imgs, labels in train_loader:\n        outputs = model(imgs)\n        loss = loss_fn(outputs, labels)\n                \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    print(""Epoch: %d, Loss: %f"" % (epoch, float(loss)))\n\n\n#%%\ntrain_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=False)\n\ncorrect = 0\ntotal = 0\n\nwith torch.no_grad():\n    for imgs, labels in train_loader:\n        outputs = model(imgs)\n        _, predicted = torch.max(outputs, dim=1)\n        total += labels.shape[0]\n        correct += int((predicted == labels).sum())\n        \nprint(""Accuracy: %f"" % (correct / total))\n\n\n#%%\nval_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64, shuffle=False)\n\ncorrect = 0\ntotal = 0\n\nwith torch.no_grad():\n    for imgs, labels in val_loader:\n        outputs = model(imgs)\n        _, predicted = torch.max(outputs, dim=1)\n        total += labels.shape[0]\n        correct += int((predicted == labels).sum())\n        \nprint(""Accuracy: %f"" % (correct / total))\n\n\n'"
10.Dataset_and_Dataloader 自定义数据读取/ImageFolder简单使用.py,0,"b""#%%\nfrom torchvision.datasets import ImageFolder\n\n\n#%%\n# \xe4\xb8\x89\xe4\xb8\xaa\xe6\x96\x87\xe4\xbb\xb6\xe5\xa4\xb9\xef\xbc\x8c\xe6\xaf\x8f\xe4\xb8\xaa\xe6\x96\x87\xe4\xbb\xb6\xe5\xa4\xb9\xe4\xb8\x80\xe5\x85\xb1\xe6\x9c\x89 3 \xe5\xbc\xa0\xe5\x9b\xbe\xe7\x89\x87\xe4\xbd\x9c\xe4\xb8\xba\xe4\xbe\x8b\xe5\xad\x90\nfolder_set = ImageFolder('./example_data/image/')\n\n\n#%%\n# \xe6\x9f\xa5\xe7\x9c\x8b\xe5\x90\x8d\xe7\xa7\xb0\xe5\x92\x8c\xe7\xb1\xbb\xe5\x88\xab\xe4\xb8\x8b\xe6\xa0\x87\xe7\x9a\x84\xe5\xaf\xb9\xe5\xba\x94\nfolder_set.class_to_idx\n\n\n#%%\n# \xe5\xbe\x97\xe5\x88\xb0\xe6\x89\x80\xe6\x9c\x89\xe7\x9a\x84\xe5\x9b\xbe\xe7\x89\x87\xe5\x90\x8d\xe5\xad\x97\xe5\x92\x8c\xe6\xa0\x87\xe7\xad\xbe\nfolder_set.imgs\n\n\n#%%\n# \xe5\x8f\x96\xe5\x87\xba\xe5\x85\xb6\xe4\xb8\xad\xe4\xb8\x80\xe4\xb8\xaa\xe6\x95\xb0\xe6\x8d\xae\nim, label = folder_set[0]\n\n\n#%%\nim\n\n\n#%%\nlabel\n\n\n#%%\nfrom torchvision import transforms as tfs\n\n\n#%%\n# \xe4\xbc\xa0\xe5\x85\xa5\xe6\x95\xb0\xe6\x8d\xae\xe9\xa2\x84\xe5\xa4\x84\xe7\x90\x86\xe6\x96\xb9\xe5\xbc\x8f\ndata_tf = tfs.ToTensor()\n\nfolder_set = ImageFolder('./example_data/image/', transform=data_tf)\n\nim, label = folder_set[0]\n\n\n#%%\nim\n\n\n#%%\nlabel\n\n#%% [markdown]\n# \xe5\x8f\xaf\xe4\xbb\xa5\xe7\x9c\x8b\xe5\x88\xb0\xe9\x80\x9a\xe8\xbf\x87\xe8\xbf\x99\xe7\xa7\x8d\xe6\x96\xb9\xe5\xbc\x8f\xe8\x83\xbd\xe5\xa4\x9f\xe9\x9d\x9e\xe5\xb8\xb8\xe6\x96\xb9\xe4\xbe\xbf\xe7\x9a\x84\xe8\xae\xbf\xe9\x97\xae\xe6\xaf\x8f\xe4\xb8\xaa\xe6\x95\xb0\xe6\x8d\xae\xe7\x82\xb9"""
10.Dataset_and_Dataloader 自定义数据读取/custom_dataset.py,2,"b""#%% [markdown]\n# # \xe7\x81\xb5\xe6\xb4\xbb\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe8\xaf\xbb\xe5\x8f\x96\n\n#%% [markdown]\n# ## Dataset\n\n#%%\nfrom torch.utils.data import Dataset\n\n\n#%%\n# \xe5\xae\x9a\xe4\xb9\x89\xe4\xb8\x80\xe4\xb8\xaa\xe5\xad\x90\xe7\xb1\xbb\xe5\x8f\xab custom_dataset\xef\xbc\x8c\xe7\xbb\xa7\xe6\x89\xbf\xe4\xb8\x8e Dataset\nclass custom_dataset(Dataset):\n    def __init__(self, txt_path, transform=None):\n        self.transform = transform # \xe4\xbc\xa0\xe5\x85\xa5\xe6\x95\xb0\xe6\x8d\xae\xe9\xa2\x84\xe5\xa4\x84\xe7\x90\x86\n        with open(txt_path, 'r') as f:\n            lines = f.readlines()\n        \n        self.img_list = [i.split()[0] for i in lines] # \xe5\xbe\x97\xe5\x88\xb0\xe6\x89\x80\xe6\x9c\x89\xe7\x9a\x84\xe5\x9b\xbe\xe5\x83\x8f\xe5\x90\x8d\xe5\xad\x97\n        self.label_list = [i.split()[1] for i in lines] # \xe5\xbe\x97\xe5\x88\xb0\xe6\x89\x80\xe6\x9c\x89\xe7\x9a\x84 label \n\n    def __getitem__(self, idx): # \xe6\xa0\xb9\xe6\x8d\xae idx \xe5\x8f\x96\xe5\x87\xba\xe5\x85\xb6\xe4\xb8\xad\xe4\xb8\x80\xe4\xb8\xaa\n        img = self.img_list[idx]\n        label = self.label_list[idx]\n        if self.transform is not None:\n            img = self.transform(img)\n        return img, label\n\n    def __len__(self): # \xe6\x80\xbb\xe6\x95\xb0\xe6\x8d\xae\xe7\x9a\x84\xe5\xa4\x9a\xe5\xb0\x91\n        return len(self.label_list)\n\n\n#%%\ntxt_dataset = custom_dataset('./example_data/train.txt') # \xe8\xaf\xbb\xe5\x85\xa5 txt \xe6\x96\x87\xe4\xbb\xb6\n\n\n#%%\n# \xe5\x8f\x96\xe5\xbe\x97\xe5\x85\xb6\xe4\xb8\xad\xe4\xb8\x80\xe4\xb8\xaa\xe6\x95\xb0\xe6\x8d\xae\ndata, label = txt_dataset[0]\nprint(data)\nprint(label)\n\n\n#%%\n# \xe5\x86\x8d\xe5\x8f\x96\xe4\xb8\x80\xe4\xb8\xaa\ndata2, label2 = txt_dataset[34]\nprint(data2)\nprint(label2)\n\n#%% [markdown]\n# \xe6\x89\x80\xe4\xbb\xa5\xe9\x80\x9a\xe8\xbf\x87\xe8\xbf\x99\xe7\xa7\x8d\xe6\x96\xb9\xe5\xbc\x8f\xe6\x88\x91\xe4\xbb\xac\xe4\xb9\x9f\xe8\x83\xbd\xe5\xa4\x9f\xe9\x9d\x9e\xe5\xb8\xb8\xe6\x96\xb9\xe4\xbe\xbf\xe7\x9a\x84\xe5\xae\x9a\xe4\xb9\x89\xe4\xb8\x80\xe4\xb8\xaa\xe6\x95\xb0\xe6\x8d\xae\xe8\xaf\xbb\xe5\x85\xa5\xef\xbc\x8c\xe5\x90\x8c\xe6\x97\xb6\xe4\xb9\x9f\xe8\x83\xbd\xe5\xa4\x9f\xe6\x96\xb9\xe4\xbe\xbf\xe7\x9a\x84\xe5\xae\x9a\xe4\xb9\x89\xe6\x95\xb0\xe6\x8d\xae\xe9\xa2\x84\xe5\xa4\x84\xe7\x90\x86\n#%% [markdown]\n# ## DataLoader\n\n#%%\nfrom torch.utils.data import DataLoader\n\n\n#%%\ntrain_data1 = DataLoader(folder_set, batch_size=2, shuffle=True) # \xe5\xb0\x86 2 \xe4\xb8\xaa\xe6\x95\xb0\xe6\x8d\xae\xe4\xbd\x9c\xe4\xb8\xba\xe4\xb8\x80\xe4\xb8\xaa batch\n\n\n#%%\nfor im, label in train_data1: # \xe8\xae\xbf\xe9\x97\xae\xe8\xbf\xad\xe4\xbb\xa3\xe5\x99\xa8\n    print(label)\n\n#%% [markdown]\n# \xe5\x8f\xaf\xe4\xbb\xa5\xe7\x9c\x8b\xe5\x88\xb0\xef\xbc\x8c\xe9\x80\x9a\xe8\xbf\x87\xe8\xae\xad\xe7\xbb\x83\xe6\x88\x91\xe4\xbb\xac\xe5\x8f\xaf\xe4\xbb\xa5\xe8\xae\xbf\xe9\x97\xae\xe5\x88\xb0\xe6\x89\x80\xe6\x9c\x89\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xef\xbc\x8c\xe8\xbf\x99\xe4\xba\x9b\xe6\x95\xb0\xe6\x8d\xae\xe8\xa2\xab\xe5\x88\x86\xe4\xb8\xba\xe4\xba\x86 5 \xe4\xb8\xaa batch\xef\xbc\x8c\xe5\x89\x8d\xe9\x9d\xa2 4 \xe4\xb8\xaa\xe9\x83\xbd\xe6\x9c\x89\xe4\xb8\xa4\xe4\xb8\xaa\xe6\x95\xb0\xe6\x8d\xae\xef\xbc\x8c\xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x80\xe4\xb8\xaa batch \xe5\x8f\xaa\xe6\x9c\x89\xe4\xb8\x80\xe4\xb8\xaa\xe6\x95\xb0\xe6\x8d\xae\xef\xbc\x8c\xe5\x9b\xa0\xe4\xb8\xba\xe4\xb8\x80\xe5\x85\xb1\xe6\x9c\x89 9 \xe4\xb8\xaa\xe6\x95\xb0\xe6\x8d\xae\xef\xbc\x8c\xe5\x90\x8c\xe6\x97\xb6\xe9\xa1\xba\xe5\xba\x8f\xe4\xb9\x9f\xe8\xa2\xab\xe6\x89\x93\xe4\xb9\xb1\xe4\xba\x86\n#%% [markdown]\n# \xe4\xb8\x8b\xe9\x9d\xa2\xe6\x88\x91\xe4\xbb\xac\xe7\x94\xa8\xe8\x87\xaa\xe5\xae\x9a\xe4\xb9\x89\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe8\xaf\xbb\xe5\x85\xa5\xe4\xb8\xbe\xe4\xbe\x8b\xe5\xad\x90\n\n#%%\ntrain_data2 = DataLoader(txt_dataset, 8, True) # batch size \xe8\xae\xbe\xe7\xbd\xae\xe4\xb8\xba 8\n\n\n#%%\nim, label = next(iter(train_data2)) # \xe4\xbd\xbf\xe7\x94\xa8\xe8\xbf\x99\xe7\xa7\x8d\xe6\x96\xb9\xe5\xbc\x8f\xe8\xae\xbf\xe9\x97\xae\xe8\xbf\xad\xe4\xbb\xa3\xe5\x99\xa8\xe4\xb8\xad\xe7\xac\xac\xe4\xb8\x80\xe4\xb8\xaa batch \xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\n\n\n#%%\nim\n\n\n#%%\nlabel\n\n#%% [markdown]\n# \xe7\x8e\xb0\xe5\x9c\xa8\xe6\x9c\x89\xe4\xb8\x80\xe4\xb8\xaa\xe9\x9c\x80\xe6\xb1\x82\xef\xbc\x8c\xe5\xb8\x8c\xe6\x9c\x9b\xe8\x83\xbd\xe5\xa4\x9f\xe5\xb0\x86\xe4\xb8\x8a\xe9\x9d\xa2\xe4\xb8\x80\xe4\xb8\xaa batch \xe8\xbe\x93\xe5\x87\xba\xe7\x9a\x84 label \xe8\xa1\xa5\xe6\x88\x90\xe7\x9b\xb8\xe5\x90\x8c\xe7\x9a\x84\xe9\x95\xbf\xe5\xba\xa6\xef\xbc\x8c\xe7\x9f\xad\xe7\x9a\x84 label \xe7\x94\xa8 0 \xe5\xa1\xab\xe5\x85\x85\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe5\xb0\xb1\xe9\x9c\x80\xe8\xa6\x81\xe4\xbd\xbf\xe7\x94\xa8 `collate_fn` \xe6\x9d\xa5\xe8\x87\xaa\xe5\xae\x9a\xe4\xb9\x89\xe6\x88\x91\xe4\xbb\xac batch \xe7\x9a\x84\xe5\xa4\x84\xe7\x90\x86\xe6\x96\xb9\xe5\xbc\x8f\xef\xbc\x8c\xe4\xb8\x8b\xe9\x9d\xa2\xe7\x9b\xb4\xe6\x8e\xa5\xe4\xb8\xbe\xe4\xbe\x8b\xe5\xad\x90\n\n#%%\ndef collate_fn(batch):\n    batch.sort(key=lambda x: len(x[1]), reverse=True) # \xe5\xb0\x86\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe6\x8c\x89\xe7\x85\xa7 label \xe7\x9a\x84\xe9\x95\xbf\xe5\xba\xa6\xe4\xbb\x8e\xe5\xa4\xa7\xe5\x88\xb0\xe5\xb0\x8f\xe6\x8e\x92\xe5\xba\x8f\n    img, label = zip(*batch) # \xe5\xb0\x86\xe6\x95\xb0\xe6\x8d\xae\xe5\x92\x8c label \xe9\x85\x8d\xe5\xaf\xb9\xe5\x8f\x96\xe5\x87\xba\n    # \xe5\xa1\xab\xe5\x85\x85\n    pad_label = []\n    lens = []\n    max_len = len(label[0])\n    for i in range(len(label)):\n        temp_label = label[i]\n        temp_label += '0' * (max_len - len(label[i]))\n        pad_label.append(temp_label)\n        lens.append(len(label[i]))\n    pad_label \n    return img, pad_label, lens # \xe8\xbe\x93\xe5\x87\xba label \xe7\x9a\x84\xe7\x9c\x9f\xe5\xae\x9e\xe9\x95\xbf\xe5\xba\xa6\n\n#%% [markdown]\n# \xe4\xbd\xbf\xe7\x94\xa8\xe6\x88\x91\xe4\xbb\xac\xe8\x87\xaa\xe5\xb7\xb1\xe5\xae\x9a\xe4\xb9\x89 collate_fn \xe7\x9c\x8b\xe7\x9c\x8b\xe6\x95\x88\xe6\x9e\x9c\n\n#%%\ntrain_data3 = DataLoader(txt_dataset, 8, True, collate_fn=collate_fn) # batch size \xe8\xae\xbe\xe7\xbd\xae\xe4\xb8\xba 8\n\n\n#%%\nim, label, lens = next(iter(train_data3))\n\n\n#%%\nim\n\n\n#%%\nlabel\n\n\n#%%\nlens\n\n#%% [markdown]\n# \xe5\x8f\xaf\xe4\xbb\xa5\xe7\x9c\x8b\xe5\x88\xb0\xe4\xb8\x80\xe4\xb8\xaa batch \xe4\xb8\xad\xe6\x89\x80\xe6\x9c\x89\xe7\x9a\x84 label \xe9\x83\xbd\xe4\xbb\x8e\xe9\x95\xbf\xe5\x88\xb0\xe7\x9f\xad\xe8\xbf\x9b\xe8\xa1\x8c\xe6\x8e\x92\xe5\x88\x97\xef\xbc\x8c\xe5\x90\x8c\xe6\x97\xb6\xe7\x9f\xad\xe7\x9a\x84 label \xe9\x83\xbd\xe8\xa2\xab\xe8\xa1\xa5\xe9\x95\xbf\xe4\xba\x86\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe4\xbd\xbf\xe7\x94\xa8 collate_fn \xe8\x83\xbd\xe5\xa4\x9f\xe9\x9d\x9e\xe5\xb8\xb8\xe6\x96\xb9\xe4\xbe\xbf\xe7\x9a\x84\xe5\xa4\x84\xe7\x90\x86\xe4\xb8\x80\xe4\xb8\xaa batch \xe4\xb8\xad\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xef\xbc\x8c\xe4\xb8\x80\xe8\x88\xac\xe6\x83\x85\xe5\x86\xb5\xe4\xb8\x8b\xef\xbc\x8c\xe6\xb2\xa1\xe6\x9c\x89\xe7\x89\xb9\xe5\x88\xab\xe7\x9a\x84\xe8\xa6\x81\xe6\xb1\x82\xef\xbc\x8c\xe4\xbd\xbf\xe7\x94\xa8 pytorch \xe4\xb8\xad\xe5\x86\x85\xe7\xbd\xae\xe7\x9a\x84 collate_fn \xe5\xb0\xb1\xe5\x8f\xaf\xe4\xbb\xa5\xe6\xbb\xa1\xe8\xb6\xb3\xe8\xa6\x81\xe6\xb1\x82\xe4\xba\x86\n\n"""
11.Custom_Dataset_Example 定义自己的数据集/dataSet.py,2,"b""#\xe6\x95\xb0\xe6\x8d\xae\xe5\xa4\x84\xe7\x90\x86\nimport os\nimport torch\nfrom torch.utils import data\nfrom PIL import Image\nimport numpy as np\nfrom torchvision import transforms\n\ntransform=transforms.Compose([\n    transforms.Resize(224), #\xe7\xbc\xa9\xe6\x94\xbe\xe5\x9b\xbe\xe7\x89\x87\xef\xbc\x8c\xe4\xbf\x9d\xe6\x8c\x81\xe9\x95\xbf\xe5\xae\xbd\xe6\xaf\x94\xe4\xb8\x8d\xe5\x8f\x98\xef\xbc\x8c\xe6\x9c\x80\xe7\x9f\xad\xe8\xbe\xb9\xe7\x9a\x84\xe9\x95\xbf\xe4\xb8\xba224\xe5\x83\x8f\xe7\xb4\xa0,\n    transforms.CenterCrop(224), #\xe4\xbb\x8e\xe4\xb8\xad\xe9\x97\xb4\xe5\x88\x87\xe5\x87\xba 224*224\xe7\x9a\x84\xe5\x9b\xbe\xe7\x89\x87\n    transforms.ToTensor(), #\xe5\xb0\x86\xe5\x9b\xbe\xe7\x89\x87\xe8\xbd\xac\xe6\x8d\xa2\xe4\xb8\xbaTensor,\xe5\xbd\x92\xe4\xb8\x80\xe5\x8c\x96\xe8\x87\xb3[0,1]\n    transforms.Normalize(mean=[.5,.5,.5],std=[.5,.5,.5]) #\xe6\xa0\x87\xe5\x87\x86\xe5\x8c\x96\xe8\x87\xb3[-1,1]\n])\n\n#\xe5\xae\x9a\xe4\xb9\x89\xe8\x87\xaa\xe5\xb7\xb1\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe5\x90\x88\nclass DogCat(data.Dataset):\n\n    def __init__(self,root,transform):\n        #\xe6\x89\x80\xe6\x9c\x89\xe5\x9b\xbe\xe7\x89\x87\xe7\x9a\x84\xe7\xbb\x9d\xe5\xaf\xb9\xe8\xb7\xaf\xe5\xbe\x84\n        imgs=os.listdir(root)\n\n        self.imgs=[os.path.join(root,k) for k in imgs]\n        self.transforms=transform\n\n    def __getitem__(self, index):\n        img_path=self.imgs[index]\n        #dog-> 1 cat ->0\n        label=1 if 'dog' in img_path.split('/')[-1] else 0\n        pil_img=Image.open(img_path)\n        if self.transforms:\n            data=self.transforms(pil_img)\n        else:\n            pil_img=np.asarray(pil_img)\n            data=torch.from_numpy(pil_img)\n        return data,label\n\n    def __len__(self):\n        return len(self.imgs)\n\ndataSet=DogCat('./data/dogcat',transform=transform)\n\nprint(dataSet[0])\n"""
11.Custom_Dataset_Example 定义自己的数据集/imageFolder.py,2,"b""from torchvision.datasets import ImageFolder\nimport torch\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader\n\n#\xe5\x8a\xa0\xe4\xb8\x8atransforms\nnormalize=transforms.Normalize(mean=[.5,.5,.5],std=[.5,.5,.5])\ntransform=transforms.Compose([\n    transforms.RandomSizedCrop(224),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(), #\xe5\xb0\x86\xe5\x9b\xbe\xe7\x89\x87\xe8\xbd\xac\xe6\x8d\xa2\xe4\xb8\xbaTensor,\xe5\xbd\x92\xe4\xb8\x80\xe5\x8c\x96\xe8\x87\xb3[0,1]\n    normalize\n])\n\ndataset=ImageFolder('data/dogcat_2/',transform=transform)\n\n#dataloader\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa\xe5\x8f\xaf\xe8\xbf\xad\xe4\xbb\xa3\xe7\x9a\x84\xe5\xaf\xb9\xe8\xb1\xa1\xef\xbc\x8c\xe6\x84\x8f\xe5\x91\xb3\xe7\x9d\x80\xe6\x88\x91\xe4\xbb\xac\xe5\x8f\xaf\xe4\xbb\xa5\xe5\x83\x8f\xe4\xbd\xbf\xe7\x94\xa8\xe8\xbf\xad\xe4\xbb\xa3\xe5\x99\xa8\xe4\xb8\x80\xe6\xa0\xb7\xe4\xbd\xbf\xe7\x94\xa8\xe5\xae\x83 \xe6\x88\x96\xe8\x80\x85 or batch_datas, batch_labels in dataloader:\ndataloader = DataLoader(dataset, batch_size=3, shuffle=True, num_workers=0, drop_last=False)\n\ndataiter = iter(dataloader)\nimgs, labels = next(dataiter)\nprint(imgs.size()) # batch_size, channel, height, weight\n#\xe8\xbe\x93\xe5\x87\xba torch.Size([3, 3, 224, 224])\n"""
11.Custom_Dataset_Example 定义自己的数据集/imageLoader.py,3,"b""'''\n\xe5\x9c\xa8\xe6\x95\xb0\xe6\x8d\xae\xe5\xa4\x84\xe7\x90\x86\xe4\xb8\xad\xef\xbc\x8c\xe6\x9c\x89\xe6\x97\xb6\xe4\xbc\x9a\xe5\x87\xba\xe7\x8e\xb0\xe6\x9f\x90\xe4\xb8\xaa\xe6\xa0\xb7\xe6\x9c\xac\xe6\x97\xa0\xe6\xb3\x95\xe8\xaf\xbb\xe5\x8f\x96\xe7\xad\x89\xe9\x97\xae\xe9\xa2\x98\xef\xbc\x8c\xe6\xaf\x94\xe5\xa6\x82\xe6\x9f\x90\xe5\xbc\xa0\xe5\x9b\xbe\xe7\x89\x87\xe6\x8d\x9f\xe5\x9d\x8f\xe3\x80\x82\xe8\xbf\x99\xe6\x97\xb6\xe5\x9c\xa8__getitem__\xe5\x87\xbd\xe6\x95\xb0\xe4\xb8\xad\xe5\xb0\x86\xe5\x87\xba\xe7\x8e\xb0\xe5\xbc\x82\xe5\xb8\xb8\xef\xbc\x8c\xe6\xad\xa4\xe6\x97\xb6\xe6\x9c\x80\xe5\xa5\xbd\xe7\x9a\x84\xe8\xa7\xa3\xe5\x86\xb3\xe6\x96\xb9\xe6\xa1\x88\xe5\x8d\xb3\xe6\x98\xaf\xe5\xb0\x86\xe5\x87\xba\xe9\x94\x99\xe7\x9a\x84\xe6\xa0\xb7\xe6\x9c\xac\xe5\x89\x94\xe9\x99\xa4\xe3\x80\x82\xe5\xa6\x82\xe6\x9e\x9c\xe5\xae\x9e\xe5\x9c\xa8\xe6\x98\xaf\xe9\x81\x87\xe5\x88\xb0\xe8\xbf\x99\xe7\xa7\x8d\xe6\x83\x85\xe5\x86\xb5\xe6\x97\xa0\xe6\xb3\x95\xe5\xa4\x84\xe7\x90\x86\xef\xbc\x8c\xe5\x88\x99\xe5\x8f\xaf\xe4\xbb\xa5\xe8\xbf\x94\xe5\x9b\x9eNone\xe5\xaf\xb9\xe8\xb1\xa1\xef\xbc\x8c\xe7\x84\xb6\xe5\x90\x8e\xe5\x9c\xa8Dataloader\xe4\xb8\xad\xe5\xae\x9e\xe7\x8e\xb0\xe8\x87\xaa\xe5\xae\x9a\xe4\xb9\x89\xe7\x9a\x84collate_fn\xef\xbc\x8c\xe5\xb0\x86\xe7\xa9\xba\xe5\xaf\xb9\xe8\xb1\xa1\xe8\xbf\x87\xe6\xbb\xa4\xe6\x8e\x89\xe3\x80\x82\xe4\xbd\x86\xe8\xa6\x81\xe6\xb3\xa8\xe6\x84\x8f\xef\xbc\x8c\xe5\x9c\xa8\xe8\xbf\x99\xe7\xa7\x8d\xe6\x83\x85\xe5\x86\xb5\xe4\xb8\x8bdataloader\xe8\xbf\x94\xe5\x9b\x9e\xe7\x9a\x84batch\xe6\x95\xb0\xe7\x9b\xae\xe4\xbc\x9a\xe5\xb0\x91\xe4\xba\x8ebatch_size\xe3\x80\x82\n'''\nfrom dataSet import *\nimport random\nclass NewDogCat(DogCat): # \xe7\xbb\xa7\xe6\x89\xbf\xe5\x89\x8d\xe9\x9d\xa2\xe5\xae\x9e\xe7\x8e\xb0\xe7\x9a\x84DogCat\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\n    def __getitem__(self, index):\n        try:\n            # \xe8\xb0\x83\xe7\x94\xa8\xe7\x88\xb6\xe7\xb1\xbb\xe7\x9a\x84\xe8\x8e\xb7\xe5\x8f\x96\xe5\x87\xbd\xe6\x95\xb0\xef\xbc\x8c\xe5\x8d\xb3 DogCat.__getitem__(self, index)\n            return super(NewDogCat,self).__getitem__(index)\n        except:\n            #\xe5\xaf\xb9\xe4\xba\x8e\xe8\xaf\xb8\xe5\xa6\x82\xe6\xa0\xb7\xe6\x9c\xac\xe6\x8d\x9f\xe5\x9d\x8f\xe6\x88\x96\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe5\x8a\xa0\xe8\xbd\xbd\xe5\xbc\x82\xe5\xb8\xb8\xe7\xad\x89\xe6\x83\x85\xe5\x86\xb5\xef\xbc\x8c\xe8\xbf\x98\xe5\x8f\xaf\xe4\xbb\xa5\xe9\x80\x9a\xe8\xbf\x87\xe5\x85\xb6\xe5\xae\x83\xe6\x96\xb9\xe5\xbc\x8f\xe8\xa7\xa3\xe5\x86\xb3\xe3\x80\x82\xe4\xbe\x8b\xe5\xa6\x82\xe4\xbd\x86\xe5\x87\xa1\xe9\x81\x87\xe5\x88\xb0\xe5\xbc\x82\xe5\xb8\xb8\xe6\x83\x85\xe5\x86\xb5\xef\xbc\x8c\xe5\xb0\xb1\xe9\x9a\x8f\xe6\x9c\xba\xe5\x8f\x96\xe4\xb8\x80\xe5\xbc\xa0\xe5\x9b\xbe\xe7\x89\x87\xe4\xbb\xa3\xe6\x9b\xbf\xef\xbc\x9a\n            new_index = random.randint(0, len(self) - 1)\n            return self[new_index]\n\nfrom torch.utils.data.dataloader import default_collate # \xe5\xaf\xbc\xe5\x85\xa5\xe9\xbb\x98\xe8\xae\xa4\xe7\x9a\x84\xe6\x8b\xbc\xe6\x8e\xa5\xe6\x96\xb9\xe5\xbc\x8f\nfrom torch.utils.data import DataLoader\ndef my_collate_fn(batch):\n    '''\n    batch\xe4\xb8\xad\xe6\xaf\x8f\xe4\xb8\xaa\xe5\x85\x83\xe7\xb4\xa0\xe5\xbd\xa2\xe5\xa6\x82(data, label)\n    '''\n    # \xe8\xbf\x87\xe6\xbb\xa4\xe4\xb8\xbaNone\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\n    batch = list(filter(lambda x:x[0] is not None, batch))\n    if len(batch) == 0: return torch.Tensor()\n    return default_collate(batch) # \xe7\x94\xa8\xe9\xbb\x98\xe8\xae\xa4\xe6\x96\xb9\xe5\xbc\x8f\xe6\x8b\xbc\xe6\x8e\xa5\xe8\xbf\x87\xe6\xbb\xa4\xe5\x90\x8e\xe7\x9a\x84batch\xe6\x95\xb0\xe6\x8d\xae\n\n\ntransform=transforms.Compose([\n    transforms.Resize(224), #\xe7\xbc\xa9\xe6\x94\xbe\xe5\x9b\xbe\xe7\x89\x87\xef\xbc\x8c\xe4\xbf\x9d\xe6\x8c\x81\xe9\x95\xbf\xe5\xae\xbd\xe6\xaf\x94\xe4\xb8\x8d\xe5\x8f\x98\xef\xbc\x8c\xe6\x9c\x80\xe7\x9f\xad\xe8\xbe\xb9\xe7\x9a\x84\xe9\x95\xbf\xe4\xb8\xba224\xe5\x83\x8f\xe7\xb4\xa0,\n    transforms.CenterCrop(224), #\xe4\xbb\x8e\xe4\xb8\xad\xe9\x97\xb4\xe5\x88\x87\xe5\x87\xba 224*224\xe7\x9a\x84\xe5\x9b\xbe\xe7\x89\x87\n    transforms.ToTensor(), #\xe5\xb0\x86\xe5\x9b\xbe\xe7\x89\x87\xe8\xbd\xac\xe6\x8d\xa2\xe4\xb8\xbaTensor,\xe5\xbd\x92\xe4\xb8\x80\xe5\x8c\x96\xe8\x87\xb3[0,1]\n    transforms.Normalize(mean=[.5,.5,.5],std=[.5,.5,.5]) #\xe6\xa0\x87\xe5\x87\x86\xe5\x8c\x96\xe8\x87\xb3[-1,1]\n])\n\n\ndataset = NewDogCat(root='data/dogcat_wrong/', transform=transform)\n\n#print(dataSet[11])\ndataloader = DataLoader(dataset, 2, collate_fn=my_collate_fn, num_workers=1,shuffle=True)\nfor batch_datas, batch_labels in dataloader:\n    print(batch_datas.size(),batch_labels.size())"""
11.Custom_Dataset_Example 定义自己的数据集/sampler.py,2,"b""from dataSet import *\ndataset = DogCat('data/dogcat/', transform=transform)\n\nfrom torch.utils.data import DataLoader\n# \xe7\x8b\x97\xe7\x9a\x84\xe5\x9b\xbe\xe7\x89\x87\xe8\xa2\xab\xe5\x8f\x96\xe5\x87\xba\xe7\x9a\x84\xe6\xa6\x82\xe7\x8e\x87\xe6\x98\xaf\xe7\x8c\xab\xe7\x9a\x84\xe6\xa6\x82\xe7\x8e\x87\xe7\x9a\x84\xe4\xb8\xa4\xe5\x80\x8d\n# \xe4\xb8\xa4\xe7\xb1\xbb\xe5\x9b\xbe\xe7\x89\x87\xe8\xa2\xab\xe5\x8f\x96\xe5\x87\xba\xe7\x9a\x84\xe6\xa6\x82\xe7\x8e\x87\xe4\xb8\x8eweights\xe7\x9a\x84\xe7\xbb\x9d\xe5\xaf\xb9\xe5\xa4\xa7\xe5\xb0\x8f\xe6\x97\xa0\xe5\x85\xb3\xef\xbc\x8c\xe5\x8f\xaa\xe5\x92\x8c\xe6\xaf\x94\xe5\x80\xbc\xe6\x9c\x89\xe5\x85\xb3\nweights = [2 if label == 1 else 1 for data, label in dataset]\n\nprint(weights)\n\nfrom torch.utils.data.sampler import  WeightedRandomSampler\nsampler = WeightedRandomSampler(weights,\\\n                                num_samples=9,\\\n                                replacement=True)\ndataloader = DataLoader(dataset,\n                        batch_size=3,\n                        sampler=sampler)\nfor datas, labels in dataloader:\n    print(labels.tolist())"""
11.Custom_Dataset_Example 定义自己的数据集/test2.py,0,"b""import numpy as np\n\n# example 1:\ndata1 = np.ones((3,3))\narr2 = np.array(data1)\narr3 = np.asarray(data1)\ndata1[1]= 2\nprint('data1:\\n', data1)\nprint('arr2:\\n', arr2)\nprint('arr3:\\n', arr3)"""
12.Visdom_Visualization visdom可视化/bar.py,0,"b""from visdom import Visdom\nimport numpy as np\nimport math\n\nvis = Visdom()\n\n# \xe5\x8d\x95\xe4\xb8\xaa\xe6\x9d\xa1\xe5\xbd\xa2\xe5\x9b\xbe\nvis.bar(X=np.random.rand(20))\n\n# \xe5\xa0\x86\xe5\x8f\xa0\xe6\x9d\xa1\xe5\xbd\xa2\xe5\x9b\xbe\nvis.bar(\n    X=np.abs(np.random.rand(5, 3)),\n    opts=dict(\n        stacked=True,\n        legend=['Sina', '163', 'AliBaBa'],\n        rownames=['2013', '2014', '2015', '2016', '2017']\n    )\n)\n\n# \xe5\x88\x86\xe7\xbb\x84\xe6\x9d\xa1\xe5\xbd\xa2\xe5\x9b\xbe\nvis.bar(\n    X=np.random.rand(20, 3),\n    opts=dict(\n        stacked=False,\n        legend=['A', 'B', 'C']\n    )\n)\n"""
12.Visdom_Visualization visdom可视化/boxplot.py,0,"b""from visdom import Visdom\nimport numpy as np\n\nvis = Visdom()\n\n# boxplot\nX = np.random.rand(100, 2)\nX[:, 1] += 2\n\nvis.boxplot(\n    X=X,\n    opts=dict(legend=['Men', 'Women'])\n)\n"""
12.Visdom_Visualization visdom可视化/contour.py,0,"b""from visdom import Visdom\nimport numpy as np\n\nvis = Visdom()\n\nx = np.tile(np.arange(1, 101), (100, 1))\ny = x.transpose()\nX = np.exp((((x - 50) ** 2) + ((y - 50) ** 2)) / -(20.0 ** 2))\n\nvis.contour(X=X, opts=dict(colormap='Viridis'))\n"""
12.Visdom_Visualization visdom可视化/heatmap.py,0,"b""from visdom import Visdom\nimport numpy as np\nimport math\n\nvis = Visdom()\n\nvis.heatmap(\n    X=np.outer(np.arange(1, 6), np.arange(1, 11)),\n    opts=dict(\n        columnnames=['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j'],\n        rownames=['y1', 'y2', 'y3', 'y4', 'y5'],\n        colormap='Electric',\n    )\n)\n"""
12.Visdom_Visualization visdom可视化/histogram.py,0,"b'from visdom import Visdom\nimport numpy as np\n\nvis = Visdom()\n\nvis.histogram(X=np.random.rand(10000), opts=dict(numbins=20))\n'"
12.Visdom_Visualization visdom可视化/img.py,0,"b""from visdom import Visdom\nimport numpy as np\nvis= Visdom()\n\n# \xe6\x98\xbe\xe7\xa4\xba\xe5\x8d\x95\xe5\x9b\xbe\xe7\x89\x87\nvis.image(\n    np.random.rand(3, 256, 256),\n    opts=dict(title='\xe5\x8d\x95\xe5\x9b\xbe\xe7\x89\x87', caption='\xe5\x9b\xbe\xe7\x89\x87\xe6\xa0\x87\xe9\xa2\x981'),\n)\n\n# \xe6\x98\xbe\xe7\xa4\xba\xe7\xbd\x91\xe6\xa0\xbc\xe5\x9b\xbe\xe7\x89\x87\nvis.images(\n    np.random.randn(20, 3, 64, 64),\n    opts=dict(title='\xe7\xbd\x91\xe6\xa0\xbc\xe5\x9b\xbe\xe5\x83\x8f', caption='\xe5\x9b\xbe\xe7\x89\x87\xe6\xa0\x87\xe9\xa2\x982')\n)\n"""
12.Visdom_Visualization visdom可视化/line.py,0,"b'from visdom import Visdom\nimport numpy as np\n\nvis = Visdom()\n\n# line plots\nY = np.linspace(-5, 5, 100)\nvis.line(\n    Y=np.column_stack((Y * Y, np.sqrt(Y + 5))),\n    X=np.column_stack((Y, Y)),\n    opts=dict(markers=False),\n)\n'"
12.Visdom_Visualization visdom可视化/mesh.py,0,"b'from visdom import Visdom\nimport numpy as np\n\nvis = Visdom()\n\n# mesh plot\nx = [0, 0, 1, 1, 0, 0, 1, 1]\ny = [0, 1, 1, 0, 0, 1, 1, 0]\nz = [0, 0, 0, 0, 1, 1, 1, 1]\nX = np.c_[x, y, z]\ni = [7, 0, 0, 0, 4, 4, 6, 6, 4, 0, 3, 2]\nj = [3, 4, 1, 2, 5, 6, 5, 2, 0, 1, 6, 3]\nk = [0, 7, 2, 3, 6, 7, 1, 1, 5, 5, 7, 6]\nY = np.c_[i, j, k]\n\nvis.mesh(X=X, Y=Y, opts=dict(opacity=0.5))\n'"
12.Visdom_Visualization visdom可视化/scatter.py,0,"b""from visdom import Visdom\nimport numpy as np\n\nvis = Visdom()\n\n# 2D scatterplot with custom intensities (red channel)\nvis.scatter(\n    X =  np.random.rand(255, 2),\n    Y = (np.random.randn(255) > 0) + 1 ,\n   opts=dict(\n        markersize=10,\n        markercolor=np.floor(np.random.random((2, 3)) * 255),\n\tlegend=['Men', 'Women']\n    ),\n)\n"""
12.Visdom_Visualization visdom可视化/stem.py,0,"b""from visdom import Visdom\nimport numpy as np\nimport math\n\nvis = Visdom()\n\n# stemplot\nY = np.linspace(0, 2 * math.pi, 70)\nX = np.column_stack((np.sin(Y), np.cos(Y)))\nvis.stem(\n    X=X,\n    Y=Y,\n    opts=dict(legend=['Sine', 'Cosine'])\n)\n"""
12.Visdom_Visualization visdom可视化/surf.py,0,"b""from visdom import Visdom\nimport numpy as np\n\nvis = Visdom()\n\nx = np.tile(np.arange(1, 101), (100, 1))\ny = x.transpose()\nX = np.exp((((x - 50) ** 2) + ((y - 50) ** 2)) / -(20.0 ** 2))\n\nvis.surf(X=X, opts=dict(colormap='Hot'))\n"""
12.Visdom_Visualization visdom可视化/svg.py,0,"b'from visdom import Visdom\nimport numpy as np\n\nvis = Visdom()\n\nsvgstr = """"""\n<svg height=""300"" width=""300"">\n  <ellipse cx=""80"" cy=""80"" rx=""50"" ry=""30""\n   style=""fill:red;stroke:purple;stroke-width:2"" />\n  \xe6\x8a\xb1\xe6\xad\x89\xef\xbc\x8c\xe4\xbd\xa0\xe7\x9a\x84\xe6\xb5\x8f\xe8\xa7\x88\xe5\x99\xa8\xe4\xb8\x8d\xe6\x94\xaf\xe6\x8c\x81\xe5\x9c\xa8\xe7\xba\xbf\xe6\x98\xbe\xe7\xa4\xbaSVG\xe5\xaf\xb9\xe8\xb1\xa1.\n</svg>\n""""""\nvis.svg(\n    svgstr=svgstr,\n    opts=dict(title=\'SVG\xe5\x9b\xbe\xe5\x83\x8f\')\n)\n'"
12.Visdom_Visualization visdom可视化/text.py,0,"b""from visdom import Visdom\nvis= Visdom()\nvis.text('Hello, world !')\n"""
13.Tensorboard_Visualization tensorboard可视化/Tensorboard可视化-oldversion.py,8,"b'#%% [markdown]\n# # TensorBoard \xe5\x8f\xaf\xe8\xa7\x86\xe5\x8c\x96\n# [github](https://github.com/lanpa/tensorboard-pytorch)\n\n#%%\nimport sys\nsys.path.append(\'..\')\n\nimport numpy as np\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom torchvision.datasets import CIFAR10\nfrom utils import resnet\nfrom torchvision import transforms as tfs\nfrom datetime import datetime\nfrom tensorboardX import SummaryWriter\n\n\n#%%\n# \xe4\xbd\xbf\xe7\x94\xa8\xe6\x95\xb0\xe6\x8d\xae\xe5\xa2\x9e\xe5\xbc\xba\ndef train_tf(x):\n    im_aug = tfs.Compose([\n        tfs.Resize(120),\n        tfs.RandomHorizontalFlip(),\n        tfs.RandomCrop(96),\n        tfs.ColorJitter(brightness=0.5, contrast=0.5, hue=0.5),\n        tfs.ToTensor(),\n        tfs.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n    ])\n    x = im_aug(x)\n    return x\n\ndef test_tf(x):\n    im_aug = tfs.Compose([\n        tfs.Resize(96),\n        tfs.ToTensor(),\n        tfs.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n    ])\n    x = im_aug(x)\n    return x\n\ntrain_set = CIFAR10(\'./data\', train=True, transform=train_tf)\ntrain_data = torch.utils.data.DataLoader(train_set, batch_size=256, shuffle=True, num_workers=4)\nvalid_set = CIFAR10(\'./data\', train=False, transform=test_tf)\nvalid_data = torch.utils.data.DataLoader(valid_set, batch_size=256, shuffle=False, num_workers=4)\n\nnet = resnet(3, 10)\noptimizer = torch.optim.SGD(net.parameters(), lr=0.1, weight_decay=1e-4)\ncriterion = nn.CrossEntropyLoss()\n\n\n#%%\nwriter = SummaryWriter()\n\ndef get_acc(output, label):\n    total = output.shape[0]\n    _, pred_label = output.max(1)\n    num_correct = (pred_label == label).sum().data[0]\n    return num_correct / total\n\nif torch.cuda.is_available():\n    net = net.cuda()\nprev_time = datetime.now()\nfor epoch in range(30):\n    train_loss = 0\n    train_acc = 0\n    net = net.train()\n    for im, label in train_data:\n        if torch.cuda.is_available():\n            im = Variable(im.cuda())  # (bs, 3, h, w)\n            label = Variable(label.cuda())  # (bs, h, w)\n        else:\n            im = Variable(im)\n            label = Variable(label)\n        # forward\n        output = net(im)\n        loss = criterion(output, label)\n        # backward\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        train_loss += loss.data[0]\n        train_acc += get_acc(output, label)\n    cur_time = datetime.now()\n    h, remainder = divmod((cur_time - prev_time).seconds, 3600)\n    m, s = divmod(remainder, 60)\n    time_str = ""Time %02d:%02d:%02d"" % (h, m, s)\n    valid_loss = 0\n    valid_acc = 0\n    net = net.eval()\n    for im, label in valid_data:\n        if torch.cuda.is_available():\n            im = Variable(im.cuda(), volatile=True)\n            label = Variable(label.cuda(), volatile=True)\n        else:\n            im = Variable(im, volatile=True)\n            label = Variable(label, volatile=True)\n        output = net(im)\n        loss = criterion(output, label)\n        valid_loss += loss.data[0]\n        valid_acc += get_acc(output, label)\n    epoch_str = (\n                ""Epoch %d. Train Loss: %f, Train Acc: %f, Valid Loss: %f, Valid Acc: %f, ""\n                % (epoch, train_loss / len(train_data),\n                   train_acc / len(train_data), valid_loss / len(valid_data),\n                   valid_acc / len(valid_data)))\n    prev_time = cur_time\n    # ====================== \xe4\xbd\xbf\xe7\x94\xa8 tensorboard ==================\n    writer.add_scalars(\'Loss\', {\'train\': train_loss / len(train_data),\n                                \'valid\': valid_loss / len(valid_data)}, epoch)\n    writer.add_scalars(\'Acc\', {\'train\': train_acc / len(train_data),\n                               \'valid\': valid_acc / len(valid_data)}, epoch)\n    # =========================================================\n    print(epoch_str + time_str)\n\n#%% [markdown]\n# ![](https://ws1.sinaimg.cn/large/006tNc79ly1fms31s3i4yj31gc0qimy6.jpg)\n\n'"
13.Tensorboard_Visualization tensorboard可视化/logger.py,0,"b'# Code referenced from https://gist.github.com/gyglim/1f8dfb1b5c82627ae3efcfbbadb9f514\nimport tensorflow as tf\nimport numpy as np\nimport scipy.misc \ntry:\n    from StringIO import StringIO  # Python 2.7\nexcept ImportError:\n    from io import BytesIO         # Python 3.x\n\n\nclass Logger(object):\n    \n    def __init__(self, log_dir):\n        """"""Create a summary writer logging to log_dir.""""""\n        self.writer = tf.summary.FileWriter(log_dir)\n\n    def scalar_summary(self, tag, value, step):\n        """"""Log a scalar variable.""""""\n        summary = tf.Summary(value=[tf.Summary.Value(tag=tag, simple_value=value)])\n        self.writer.add_summary(summary, step)\n\n    def image_summary(self, tag, images, step):\n        """"""Log a list of images.""""""\n\n        img_summaries = []\n        for i, img in enumerate(images):\n            # Write the image to a string\n            try:\n                s = StringIO()\n            except:\n                s = BytesIO()\n            scipy.misc.toimage(img).save(s, format=""png"")\n\n            # Create an Image object\n            img_sum = tf.Summary.Image(encoded_image_string=s.getvalue(),\n                                       height=img.shape[0],\n                                       width=img.shape[1])\n            # Create a Summary value\n            img_summaries.append(tf.Summary.Value(tag=\'%s/%d\' % (tag, i), image=img_sum))\n\n        # Create and write Summary\n        summary = tf.Summary(value=img_summaries)\n        self.writer.add_summary(summary, step)\n        \n    def histo_summary(self, tag, values, step, bins=1000):\n        """"""Log a histogram of the tensor of values.""""""\n\n        # Create a histogram using numpy\n        counts, bin_edges = np.histogram(values, bins=bins)\n\n        # Fill the fields of the histogram proto\n        hist = tf.HistogramProto()\n        hist.min = float(np.min(values))\n        hist.max = float(np.max(values))\n        hist.num = int(np.prod(values.shape))\n        hist.sum = float(np.sum(values))\n        hist.sum_squares = float(np.sum(values**2))\n\n        # Drop the start of the first bin\n        bin_edges = bin_edges[1:]\n\n        # Add bin edges and counts\n        for edge in bin_edges:\n            hist.bucket_limit.append(edge)\n        for c in counts:\n            hist.bucket.append(c)\n\n        # Create and write Summary\n        summary = tf.Summary(value=[tf.Summary.Value(tag=tag, histo=hist)])\n        self.writer.add_summary(summary, step)\n        self.writer.flush()'"
13.Tensorboard_Visualization tensorboard可视化/main.py,5,"b""import torch\nimport torch.nn as nn\nimport torchvision\nfrom torchvision import transforms\nfrom logger import Logger\n\n\n# Device configuration\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# MNIST dataset \ndataset = torchvision.datasets.MNIST(root='../../data', \n                                     train=True, \n                                     transform=transforms.ToTensor(),  \n                                     download=True)\n\n# Data loader\ndata_loader = torch.utils.data.DataLoader(dataset=dataset, \n                                          batch_size=100, \n                                          shuffle=True)\n\n\n# Fully connected neural network with one hidden layer\nclass NeuralNet(nn.Module):\n    def __init__(self, input_size=784, hidden_size=500, num_classes=10):\n        super(NeuralNet, self).__init__()\n        self.fc1 = nn.Linear(input_size, hidden_size) \n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(hidden_size, num_classes)  \n    \n    def forward(self, x):\n        out = self.fc1(x)\n        out = self.relu(out)\n        out = self.fc2(out)\n        return out\n\nmodel = NeuralNet().to(device)\n\nlogger = Logger('./logs')\n\n# Loss and optimizer\ncriterion = nn.CrossEntropyLoss()  \noptimizer = torch.optim.Adam(model.parameters(), lr=0.00001)  \n\ndata_iter = iter(data_loader)\niter_per_epoch = len(data_loader)\ntotal_step = 50000\n\n# Start training\nfor step in range(total_step):\n    \n    # Reset the data_iter\n    if (step+1) % iter_per_epoch == 0:\n        data_iter = iter(data_loader)\n\n    # Fetch images and labels\n    images, labels = next(data_iter)\n    images, labels = images.view(images.size(0), -1).to(device), labels.to(device)\n    \n    # Forward pass\n    outputs = model(images)\n    loss = criterion(outputs, labels)\n    \n    # Backward and optimize\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    # Compute accuracy\n    _, argmax = torch.max(outputs, 1)\n    accuracy = (labels == argmax.squeeze()).float().mean()\n\n    if (step+1) % 100 == 0:\n        print ('Step [{}/{}], Loss: {:.4f}, Acc: {:.2f}' \n               .format(step+1, total_step, loss.item(), accuracy.item()))\n\n        # ================================================================== #\n        #                        Tensorboard Logging                         #\n        # ================================================================== #\n\n        # 1. Log scalar values (scalar summary)\n        info = { 'loss': loss.item(), 'accuracy': accuracy.item() }\n\n        for tag, value in info.items():\n            logger.scalar_summary(tag, value, step+1)\n\n        # 2. Log values and gradients of the parameters (histogram summary)\n        for tag, value in model.named_parameters():\n            tag = tag.replace('.', '/')\n            logger.histo_summary(tag, value.data.cpu().numpy(), step+1)\n            logger.histo_summary(tag+'/grad', value.grad.data.cpu().numpy(), step+1)\n\n        # 3. Log training images (image summary)\n        info = { 'images': images.view(-1, 28, 28)[:10].cpu().numpy() }\n\n        for tag, images in info.items():\n            logger.image_summary(tag, images, step+1)"""
14.Semantic_Segmentation 语义分割/Basic_blocks.py,5,"b'import torch\nimport torch.nn as nn\nimport torch.utils as utils\nimport torch.nn.init as init\nimport torch.utils.data as data\nimport torchvision.utils as v_utils\nimport torchvision.datasets as dset\nimport torchvision.transforms as transforms\nfrom torch.autograd import Variable\n\n\ndef conv_block(in_dim,out_dim,act_fn):\n    model = nn.Sequential(\n        nn.Conv2d(in_dim,out_dim, kernel_size=3, stride=1, padding=1),\n        nn.BatchNorm2d(out_dim),\n        act_fn,\n    )\n    return model\n\n\ndef conv_trans_block(in_dim,out_dim,act_fn):\n    model = nn.Sequential(\n        nn.ConvTranspose2d(in_dim,out_dim, kernel_size=3, stride=2, padding=1,output_padding=1),\n        nn.BatchNorm2d(out_dim),\n        act_fn,\n    )\n    return model\n\n\ndef maxpool():\n    pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n    return pool\n\n\ndef conv_block_2(in_dim,out_dim,act_fn):\n    model = nn.Sequential(\n        conv_block(in_dim,out_dim,act_fn),\n        nn.Conv2d(out_dim,out_dim, kernel_size=3, stride=1, padding=1),\n        nn.BatchNorm2d(out_dim),\n    )\n    return model    \n\n\ndef conv_block_3(in_dim,out_dim,act_fn):\n    model = nn.Sequential(\n        conv_block(in_dim,out_dim,act_fn),\n        conv_block(out_dim,out_dim,act_fn),\n        nn.Conv2d(out_dim,out_dim, kernel_size=3, stride=1, padding=1),\n        nn.BatchNorm2d(out_dim),\n    )\n    return model'"
14.Semantic_Segmentation 语义分割/FusionNet.py,1,"b'from Basic_blocks import * \n\n\nclass Conv_residual_conv(nn.Module):\n\n    def __init__(self,in_dim,out_dim,act_fn):\n        super(Conv_residual_conv,self).__init__()\n        self.in_dim = in_dim\n        self.out_dim = out_dim\n        act_fn = act_fn\n\n        self.conv_1 = conv_block(self.in_dim,self.out_dim,act_fn)\n        self.conv_2 = conv_block_3(self.out_dim,self.out_dim,act_fn)\n        self.conv_3 = conv_block(self.out_dim,self.out_dim,act_fn)\n\n    def forward(self,input):\n        conv_1 = self.conv_1(input)\n        conv_2 = self.conv_2(conv_1)\n        res = conv_1 + conv_2\n        conv_3 = self.conv_3(res)\n        return conv_3\n\n\nclass FusionGenerator(nn.Module):\n\n    def __init__(self,input_nc, output_nc, ngf):\n        super(FusionGenerator,self).__init__()\n        self.in_dim = input_nc\n        self.out_dim = ngf\n        self.final_out_dim = output_nc\n        act_fn = nn.LeakyReLU(0.2, inplace=True)\n        act_fn_2 = nn.ReLU()\n\n        print(""\\n------Initiating FusionNet------\\n"")\n\n        # encoder\n\n        self.down_1 = Conv_residual_conv(self.in_dim, self.out_dim, act_fn)\n        self.pool_1 = maxpool()\n        self.down_2 = Conv_residual_conv(self.out_dim, self.out_dim * 2, act_fn)\n        self.pool_2 = maxpool()\n        self.down_3 = Conv_residual_conv(self.out_dim * 2, self.out_dim * 4, act_fn)\n        self.pool_3 = maxpool()\n        self.down_4 = Conv_residual_conv(self.out_dim * 4, self.out_dim * 8, act_fn)\n        self.pool_4 = maxpool()\n\n        # bridge\n\n        self.bridge = Conv_residual_conv(self.out_dim * 8, self.out_dim * 16, act_fn)\n\n        # decoder\n\n        self.deconv_1 = conv_trans_block(self.out_dim * 16, self.out_dim * 8, act_fn_2)\n        self.up_1 = Conv_residual_conv(self.out_dim * 8, self.out_dim * 8, act_fn_2)\n        self.deconv_2 = conv_trans_block(self.out_dim * 8, self.out_dim * 4, act_fn_2)\n        self.up_2 = Conv_residual_conv(self.out_dim * 4, self.out_dim * 4, act_fn_2)\n        self.deconv_3 = conv_trans_block(self.out_dim * 4, self.out_dim * 2, act_fn_2)\n        self.up_3 = Conv_residual_conv(self.out_dim * 2, self.out_dim * 2, act_fn_2)\n        self.deconv_4 = conv_trans_block(self.out_dim * 2, self.out_dim, act_fn_2)\n        self.up_4 = Conv_residual_conv(self.out_dim, self.out_dim, act_fn_2)\n\n        # output\n\n        self.out = nn.Conv2d(self.out_dim,self.final_out_dim, kernel_size=3, stride=1, padding=1)\n        self.out_2 = nn.Tanh()\n        \'\'\'\n        self.out = nn.Sequential(\n            nn.Conv2d(self.out_dim,self.final_out_dim, kernel_size=3, stride=1, padding=1),\n            #nn.BatchNorm2d(self.final_out_dim),\n            nn.Tanh(),\n        )\n        \'\'\'\n\n        # initialization\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                m.weight.data.normal_(0.0, 0.02)\n                m.bias.data.fill_(0)\n            \n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.normal_(1.0, 0.02)\n                m.bias.data.fill_(0)\n\n\n    def forward(self,input):\n\n        down_1 = self.down_1(input)\n        pool_1 = self.pool_1(down_1)\n        down_2 = self.down_2(pool_1)\n        pool_2 = self.pool_2(down_2)\n        down_3 = self.down_3(pool_2)\n        pool_3 = self.pool_3(down_3)\n        down_4 = self.down_4(pool_3)\n        pool_4 = self.pool_4(down_4)\n\n        bridge = self.bridge(pool_4)\n\n        deconv_1 = self.deconv_1(bridge)\n        skip_1 = (deconv_1 + down_4)/2\n        up_1 = self.up_1(skip_1)\n        deconv_2 = self.deconv_2(up_1)\n        skip_2 = (deconv_2 + down_3)/2\n        up_2 = self.up_2(skip_2)\n        deconv_3 = self.deconv_3(up_2)\n        skip_3 = (deconv_3 + down_2)/2\n        up_3 = self.up_3(skip_3)\n        deconv_4 = self.deconv_4(up_3)\n        skip_4 = (deconv_4 + down_1)/2\n        up_4 = self.up_4(skip_4)\n\n        out = self.out(up_4)\n        out = self.out_2(out)\n        #out = torch.clamp(out, min=-1, max=1)\n\n        return out'"
14.Semantic_Segmentation 语义分割/UNet.py,4,"b'from Basic_blocks import * \n\nclass UnetGenerator(nn.Module):\n\n\tdef __init__(self,in_dim,out_dim,num_filter):\n\t\tsuper(UnetGenerator,self).__init__()\n\t\tself.in_dim = in_dim\n\t\tself.out_dim = out_dim\n\t\tself.num_filter = num_filter\n\t\tact_fn = nn.LeakyReLU(0.2, inplace=True)\n\n\t\tprint(""\\n------Initiating U-Net------\\n"")\n\n\t\tself.down_1 = conv_block_2(self.in_dim,self.num_filter,act_fn)\n\t\tself.pool_1 = maxpool()\n\t\tself.down_2 = conv_block_2(self.num_filter*1,self.num_filter*2,act_fn)\n\t\tself.pool_2 = maxpool()\n\t\tself.down_3 = conv_block_2(self.num_filter*2,self.num_filter*4,act_fn)\n\t\tself.pool_3 = maxpool()\n\t\tself.down_4 = conv_block_2(self.num_filter*4,self.num_filter*8,act_fn)\n\t\tself.pool_4 = maxpool()\n\n\t\tself.bridge = conv_block_2(self.num_filter*8,self.num_filter*16,act_fn)\n\n\t\tself.trans_1 = conv_trans_block(self.num_filter*16,self.num_filter*8,act_fn)\n\t\tself.up_1 = conv_block_2(self.num_filter*16,self.num_filter*8,act_fn)\n\t\tself.trans_2 = conv_trans_block(self.num_filter*8,self.num_filter*4,act_fn)\n\t\tself.up_2 = conv_block_2(self.num_filter*8,self.num_filter*4,act_fn)\n\t\tself.trans_3 = conv_trans_block(self.num_filter*4,self.num_filter*2,act_fn)\n\t\tself.up_3 = conv_block_2(self.num_filter*4,self.num_filter*2,act_fn)\n\t\tself.trans_4 = conv_trans_block(self.num_filter*2,self.num_filter*1,act_fn)\n\t\tself.up_4 = conv_block_2(self.num_filter*2,self.num_filter*1,act_fn)\n\n\t\tself.out = nn.Sequential(\n\t\t\tnn.Conv2d(self.num_filter,self.out_dim,3,1,1),\n\t\t\tnn.Tanh(),\n\t\t)\n\n\tdef forward(self,input):\n\t\tdown_1 = self.down_1(input)\n\t\tpool_1 = self.pool_1(down_1)\n\t\tdown_2 = self.down_2(pool_1)\n\t\tpool_2 = self.pool_2(down_2)\n\t\tdown_3 = self.down_3(pool_2)\n\t\tpool_3 = self.pool_3(down_3)\n\t\tdown_4 = self.down_4(pool_3)\n\t\tpool_4 = self.pool_4(down_4)\n\n\t\tbridge = self.bridge(pool_4)\n\n\t\ttrans_1 = self.trans_1(bridge)\n\t\tconcat_1 = torch.cat([trans_1,down_4],dim=1)\n\t\tup_1 = self.up_1(concat_1)\n\t\ttrans_2 = self.trans_2(up_1)\n\t\tconcat_2 = torch.cat([trans_2,down_3],dim=1)\n\t\tup_2 = self.up_2(concat_2)\n\t\ttrans_3 = self.trans_3(up_2)\n\t\tconcat_3 = torch.cat([trans_3,down_2],dim=1)\n\t\tup_3 = self.up_3(concat_3)\n\t\ttrans_4 = self.trans_4(up_3)\n\t\tconcat_4 = torch.cat([trans_4,down_1],dim=1)\n\t\tup_4 = self.up_4(concat_4)\n\n\t\tout = self.out(up_4)\n\n\t\treturn out'"
14.Semantic_Segmentation 语义分割/main.py,4,"b'# Semantic Segmentation\n# Code by GunhoChoi\n\nfrom FusionNet import * \nfrom UNet import *\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport argparse\n\nparser = argparse.ArgumentParser()\nparser.add_argument(""--network"",type=str,default=""fusionnet"",help=""choose between fusionnet & unet"")\nparser.add_argument(""--batch_size"",type=int,default=1,help=""batch size"")\nparser.add_argument(""--num_gpu"",type=int,default=1,help=""number of gpus"")\nargs = parser.parse_args()\n\n# hyperparameters\n\nbatch_size = args.batch_size\nimg_size = 256\nlr = 0.0002\nepoch = 100\n\n# input pipeline\n\nimg_dir = ""./maps/""\nimg_data = dset.ImageFolder(root=img_dir, transform = transforms.Compose([\n                                            transforms.Scale(size=img_size),\n                                            transforms.CenterCrop(size=(img_size,img_size*2)),\n                                            transforms.ToTensor(),\n                                            ]))\nimg_batch = data.DataLoader(img_data, batch_size=batch_size,\n                            shuffle=True, num_workers=2)\n\n# initiate Generator\n\nif args.network == ""fusionnet"":\n\tgenerator = nn.DataParallel(FusionGenerator(3,3,64),device_ids=[i for i in range(args.num_gpu)]).cuda()\nelif args.network == ""unet"":\n\tgenerator = nn.DataParallel(UnetGenerator(3,3,64),device_ids=[i for i in range(args.num_gpu)]).cuda()\n\n# load pretrained model\n\ntry:\n    generator = torch.load(\'./model/{}.pkl\'.format(args.network))\n    print(""\\n--------model restored--------\\n"")\nexcept:\n    print(""\\n--------model not restored--------\\n"")\n    pass\n\n# loss function & optimizer\n\nrecon_loss_func = nn.MSELoss()\ngen_optimizer = torch.optim.Adam(generator.parameters(),lr=lr)\n\n# training\n\nfile = open(\'./{}_mse_loss\'.format(args.network), \'w\')\nfor i in range(epoch):\n    for _,(image,label) in enumerate(img_batch):\n        satel_image, map_image = torch.chunk(image, chunks=2, dim=3) \n        \n        gen_optimizer.zero_grad()\n\n        x = Variable(satel_image).cuda(0)\n        y_ = Variable(map_image).cuda(0)\n        y = generator.forward(x)\n        \n        loss = recon_loss_func(y,y_)\n        file.write(str(loss)+""\\n"")\n        loss.backward()\n        gen_optimizer.step()\n\n        if _ % 400 ==0:\n            print(i)\n            print(loss)\n            v_utils.save_image(x.cpu().data,""./result/original_image_{}_{}.png"".format(i,_))\n            v_utils.save_image(y_.cpu().data,""./result/label_image_{}_{}.png"".format(i,_))\n            v_utils.save_image(y.cpu().data,""./result/gen_image_{}_{}.png"".format(i,_))\n            torch.save(generator,\'./model/{}.pkl\'.format(args.network))    \n'"
2.PyTorch _basics PyTorch基础/PyTorch overview.py,12,"b'#%% [markdown]\n# ## 2.2 PyTorch\xe7\xac\xac\xe4\xb8\x80\xe6\xad\xa5\n# \n# PyTorch\xe7\x9a\x84\xe7\xae\x80\xe6\xb4\x81\xe8\xae\xbe\xe8\xae\xa1\xe4\xbd\xbf\xe5\xbe\x97\xe5\xae\x83\xe5\x85\xa5\xe9\x97\xa8\xe5\xbe\x88\xe7\xae\x80\xe5\x8d\x95\xef\xbc\x8c\xe5\x9c\xa8\xe6\xb7\xb1\xe5\x85\xa5\xe4\xbb\x8b\xe7\xbb\x8dPyTorch\xe4\xb9\x8b\xe5\x89\x8d\xef\xbc\x8c\xe6\x9c\xac\xe8\x8a\x82\xe5\xb0\x86\xe5\x85\x88\xe4\xbb\x8b\xe7\xbb\x8d\xe4\xb8\x80\xe4\xba\x9bPyTorch\xe7\x9a\x84\xe5\x9f\xba\xe7\xa1\x80\xe7\x9f\xa5\xe8\xaf\x86\xef\xbc\x8c\xe4\xbd\xbf\xe5\xbe\x97\xe8\xaf\xbb\xe8\x80\x85\xe8\x83\xbd\xe5\xa4\x9f\xe5\xaf\xb9PyTorch\xe6\x9c\x89\xe4\xb8\x80\xe4\xb8\xaa\xe5\xa4\xa7\xe8\x87\xb4\xe7\x9a\x84\xe4\xba\x86\xe8\xa7\xa3\xef\xbc\x8c\xe5\xb9\xb6\xe8\x83\xbd\xe5\xa4\x9f\xe7\x94\xa8PyTorch\xe6\x90\xad\xe5\xbb\xba\xe4\xb8\x80\xe4\xb8\xaa\xe7\xae\x80\xe5\x8d\x95\xe7\x9a\x84\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xe3\x80\x82\xe9\x83\xa8\xe5\x88\x86\xe5\x86\x85\xe5\xae\xb9\xe8\xaf\xbb\xe8\x80\x85\xe5\x8f\xaf\xe8\x83\xbd\xe6\x9a\x82\xe6\x97\xb6\xe4\xb8\x8d\xe5\xa4\xaa\xe7\x90\x86\xe8\xa7\xa3\xef\xbc\x8c\xe5\x8f\xaf\xe5\x85\x88\xe4\xb8\x8d\xe4\xba\x88\xe4\xbb\xa5\xe6\xb7\xb1\xe7\xa9\xb6\xef\xbc\x8c\xe6\x9c\xac\xe4\xb9\xa6\xe7\x9a\x84\xe7\xac\xac3\xe7\xab\xa0\xe5\x92\x8c\xe7\xac\xac4\xe7\xab\xa0\xe5\xb0\x86\xe4\xbc\x9a\xe5\xaf\xb9\xe6\xad\xa4\xe8\xbf\x9b\xe8\xa1\x8c\xe6\xb7\xb1\xe5\x85\xa5\xe8\xae\xb2\xe8\xa7\xa3\xe3\x80\x82\n# \n# \xe6\x9c\xac\xe8\x8a\x82\xe5\x86\x85\xe5\xae\xb9\xe5\x8f\x82\xe8\x80\x83\xe4\xba\x86PyTorch\xe5\xae\x98\xe6\x96\xb9\xe6\x95\x99\xe7\xa8\x8b[^1]\xe5\xb9\xb6\xe5\x81\x9a\xe4\xba\x86\xe7\x9b\xb8\xe5\xba\x94\xe7\x9a\x84\xe5\xa2\x9e\xe5\x88\xa0\xe4\xbf\xae\xe6\x94\xb9\xef\xbc\x8c\xe4\xbd\xbf\xe5\xbe\x97\xe5\x86\x85\xe5\xae\xb9\xe6\x9b\xb4\xe8\xb4\xb4\xe5\x90\x88\xe6\x96\xb0\xe7\x89\x88\xe6\x9c\xac\xe7\x9a\x84PyTorch\xe6\x8e\xa5\xe5\x8f\xa3\xef\xbc\x8c\xe5\x90\x8c\xe6\x97\xb6\xe4\xb9\x9f\xe6\x9b\xb4\xe9\x80\x82\xe5\x90\x88\xe6\x96\xb0\xe6\x89\x8b\xe5\xbf\xab\xe9\x80\x9f\xe5\x85\xa5\xe9\x97\xa8\xe3\x80\x82\xe5\x8f\xa6\xe5\xa4\x96\xe6\x9c\xac\xe4\xb9\xa6\xe9\x9c\x80\xe8\xa6\x81\xe8\xaf\xbb\xe8\x80\x85\xe5\x85\x88\xe6\x8e\x8c\xe6\x8f\xa1\xe5\x9f\xba\xe7\xa1\x80\xe7\x9a\x84Numpy\xe4\xbd\xbf\xe7\x94\xa8\xef\xbc\x8c\xe5\x85\xb6\xe4\xbb\x96\xe7\x9b\xb8\xe5\x85\xb3\xe7\x9f\xa5\xe8\xaf\x86\xe6\x8e\xa8\xe8\x8d\x90\xe8\xaf\xbb\xe8\x80\x85\xe5\x8f\x82\xe8\x80\x83CS231n\xe7\x9a\x84\xe6\x95\x99\xe7\xa8\x8b[^2]\xe3\x80\x82\n# \n# [^1]: http://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html\n# [^2]: http://cs231n.github.io/python-numpy-tutorial/\n#%% [markdown]\n# ### Tensor\n# \n# Tensor\xe6\x98\xafPyTorch\xe4\xb8\xad\xe9\x87\x8d\xe8\xa6\x81\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe7\xbb\x93\xe6\x9e\x84\xef\xbc\x8c\xe5\x8f\xaf\xe8\xae\xa4\xe4\xb8\xba\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa\xe9\xab\x98\xe7\xbb\xb4\xe6\x95\xb0\xe7\xbb\x84\xe3\x80\x82\xe5\xae\x83\xe5\x8f\xaf\xe4\xbb\xa5\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa\xe6\x95\xb0\xef\xbc\x88\xe6\xa0\x87\xe9\x87\x8f\xef\xbc\x89\xe3\x80\x81\xe4\xb8\x80\xe7\xbb\xb4\xe6\x95\xb0\xe7\xbb\x84\xef\xbc\x88\xe5\x90\x91\xe9\x87\x8f\xef\xbc\x89\xe3\x80\x81\xe4\xba\x8c\xe7\xbb\xb4\xe6\x95\xb0\xe7\xbb\x84\xef\xbc\x88\xe7\x9f\xa9\xe9\x98\xb5\xef\xbc\x89\xe4\xbb\xa5\xe5\x8f\x8a\xe6\x9b\xb4\xe9\xab\x98\xe7\xbb\xb4\xe7\x9a\x84\xe6\x95\xb0\xe7\xbb\x84\xe3\x80\x82Tensor\xe5\x92\x8cNumpy\xe7\x9a\x84ndarrays\xe7\xb1\xbb\xe4\xbc\xbc\xef\xbc\x8c\xe4\xbd\x86Tensor\xe5\x8f\xaf\xe4\xbb\xa5\xe4\xbd\xbf\xe7\x94\xa8GPU\xe8\xbf\x9b\xe8\xa1\x8c\xe5\x8a\xa0\xe9\x80\x9f\xe3\x80\x82Tensor\xe7\x9a\x84\xe4\xbd\xbf\xe7\x94\xa8\xe5\x92\x8cNumpy\xe5\x8f\x8aMatlab\xe7\x9a\x84\xe6\x8e\xa5\xe5\x8f\xa3\xe5\x8d\x81\xe5\x88\x86\xe7\x9b\xb8\xe4\xbc\xbc\xef\xbc\x8c\xe4\xb8\x8b\xe9\x9d\xa2\xe9\x80\x9a\xe8\xbf\x87\xe5\x87\xa0\xe4\xb8\xaa\xe4\xbe\x8b\xe5\xad\x90\xe6\x9d\xa5\xe7\x9c\x8b\xe7\x9c\x8bTensor\xe7\x9a\x84\xe5\x9f\xba\xe6\x9c\xac\xe4\xbd\xbf\xe7\x94\xa8\xe3\x80\x82\n\n#%%\nfrom __future__ import print_function\nimport torch as t\nt.__version__\n\n\n#%%\n# \xe6\x9e\x84\xe5\xbb\xba 5x3 \xe7\x9f\xa9\xe9\x98\xb5\xef\xbc\x8c\xe5\x8f\xaa\xe6\x98\xaf\xe5\x88\x86\xe9\x85\x8d\xe4\xba\x86\xe7\xa9\xba\xe9\x97\xb4\xef\xbc\x8c\xe6\x9c\xaa\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\nx = t.Tensor(5, 3)\n\nx = t.Tensor([[1,2],[3,4]])\nx\n\n\n#%%\n# \xe4\xbd\xbf\xe7\x94\xa8[0,1]\xe5\x9d\x87\xe5\x8c\x80\xe5\x88\x86\xe5\xb8\x83\xe9\x9a\x8f\xe6\x9c\xba\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe4\xba\x8c\xe7\xbb\xb4\xe6\x95\xb0\xe7\xbb\x84\nx = t.rand(5, 3)  \nx\n\n\n#%%\nprint(x.size()) # \xe6\x9f\xa5\xe7\x9c\x8bx\xe7\x9a\x84\xe5\xbd\xa2\xe7\x8a\xb6\nx.size()[1], x.size(1) # \xe6\x9f\xa5\xe7\x9c\x8b\xe5\x88\x97\xe7\x9a\x84\xe4\xb8\xaa\xe6\x95\xb0, \xe4\xb8\xa4\xe7\xa7\x8d\xe5\x86\x99\xe6\xb3\x95\xe7\xad\x89\xe4\xbb\xb7\n\n#%% [markdown]\n# `torch.Size` \xe6\x98\xaftuple\xe5\xaf\xb9\xe8\xb1\xa1\xe7\x9a\x84\xe5\xad\x90\xe7\xb1\xbb\xef\xbc\x8c\xe5\x9b\xa0\xe6\xad\xa4\xe5\xae\x83\xe6\x94\xaf\xe6\x8c\x81tuple\xe7\x9a\x84\xe6\x89\x80\xe6\x9c\x89\xe6\x93\x8d\xe4\xbd\x9c\xef\xbc\x8c\xe5\xa6\x82x.size()[0]\n\n#%%\ny = t.rand(5, 3)\n# \xe5\x8a\xa0\xe6\xb3\x95\xe7\x9a\x84\xe7\xac\xac\xe4\xb8\x80\xe7\xa7\x8d\xe5\x86\x99\xe6\xb3\x95\nx + y\n\n\n#%%\n# \xe5\x8a\xa0\xe6\xb3\x95\xe7\x9a\x84\xe7\xac\xac\xe4\xba\x8c\xe7\xa7\x8d\xe5\x86\x99\xe6\xb3\x95\nt.add(x, y)\n\n\n#%%\n# \xe5\x8a\xa0\xe6\xb3\x95\xe7\x9a\x84\xe7\xac\xac\xe4\xb8\x89\xe7\xa7\x8d\xe5\x86\x99\xe6\xb3\x95\xef\xbc\x9a\xe6\x8c\x87\xe5\xae\x9a\xe5\x8a\xa0\xe6\xb3\x95\xe7\xbb\x93\xe6\x9e\x9c\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe7\x9b\xae\xe6\xa0\x87\xe4\xb8\xbaresult\nresult = t.Tensor(5, 3) # \xe9\xa2\x84\xe5\x85\x88\xe5\x88\x86\xe9\x85\x8d\xe7\xa9\xba\xe9\x97\xb4\nt.add(x, y, out=result) # \xe8\xbe\x93\xe5\x85\xa5\xe5\x88\xb0result\nresult\n\n\n#%%\nprint(\'\xe6\x9c\x80\xe5\x88\x9dy\')\nprint(y)\n\nprint(\'\xe7\xac\xac\xe4\xb8\x80\xe7\xa7\x8d\xe5\x8a\xa0\xe6\xb3\x95\xef\xbc\x8cy\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x9c\')\ny.add(x) # \xe6\x99\xae\xe9\x80\x9a\xe5\x8a\xa0\xe6\xb3\x95\xef\xbc\x8c\xe4\xb8\x8d\xe6\x94\xb9\xe5\x8f\x98y\xe7\x9a\x84\xe5\x86\x85\xe5\xae\xb9\nprint(y)\n\nprint(\'\xe7\xac\xac\xe4\xba\x8c\xe7\xa7\x8d\xe5\x8a\xa0\xe6\xb3\x95\xef\xbc\x8cy\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x9c\')\ny.add_(x) # inplace \xe5\x8a\xa0\xe6\xb3\x95\xef\xbc\x8cy\xe5\x8f\x98\xe4\xba\x86\nprint(y)\n\n#%% [markdown]\n# \xe6\xb3\xa8\xe6\x84\x8f\xef\xbc\x8c\xe5\x87\xbd\xe6\x95\xb0\xe5\x90\x8d\xe5\x90\x8e\xe9\x9d\xa2\xe5\xb8\xa6\xe4\xb8\x8b\xe5\x88\x92\xe7\xba\xbf**`_`** \xe7\x9a\x84\xe5\x87\xbd\xe6\x95\xb0\xe4\xbc\x9a\xe4\xbf\xae\xe6\x94\xb9Tensor\xe6\x9c\xac\xe8\xba\xab\xe3\x80\x82\xe4\xbe\x8b\xe5\xa6\x82\xef\xbc\x8c`x.add_(y)`\xe5\x92\x8c`x.t_()`\xe4\xbc\x9a\xe6\x94\xb9\xe5\x8f\x98 `x`\xef\xbc\x8c\xe4\xbd\x86`x.add(y)`\xe5\x92\x8c`x.t()`\xe8\xbf\x94\xe5\x9b\x9e\xe4\xb8\x80\xe4\xb8\xaa\xe6\x96\xb0\xe7\x9a\x84Tensor\xef\xbc\x8c \xe8\x80\x8c`x`\xe4\xb8\x8d\xe5\x8f\x98\xe3\x80\x82\n\n#%%\n# Tensor\xe7\x9a\x84\xe9\x80\x89\xe5\x8f\x96\xe6\x93\x8d\xe4\xbd\x9c\xe4\xb8\x8eNumpy\xe7\xb1\xbb\xe4\xbc\xbc\nx[:, 1]\n\n#%% [markdown]\n# Tensor\xe8\xbf\x98\xe6\x94\xaf\xe6\x8c\x81\xe5\xbe\x88\xe5\xa4\x9a\xe6\x93\x8d\xe4\xbd\x9c\xef\xbc\x8c\xe5\x8c\x85\xe6\x8b\xac\xe6\x95\xb0\xe5\xad\xa6\xe8\xbf\x90\xe7\xae\x97\xe3\x80\x81\xe7\xba\xbf\xe6\x80\xa7\xe4\xbb\xa3\xe6\x95\xb0\xe3\x80\x81\xe9\x80\x89\xe6\x8b\xa9\xe3\x80\x81\xe5\x88\x87\xe7\x89\x87\xe7\xad\x89\xe7\xad\x89\xef\xbc\x8c\xe5\x85\xb6\xe6\x8e\xa5\xe5\x8f\xa3\xe8\xae\xbe\xe8\xae\xa1\xe4\xb8\x8eNumpy\xe6\x9e\x81\xe4\xb8\xba\xe7\x9b\xb8\xe4\xbc\xbc\xe3\x80\x82\xe6\x9b\xb4\xe8\xaf\xa6\xe7\xbb\x86\xe7\x9a\x84\xe4\xbd\xbf\xe7\x94\xa8\xe6\x96\xb9\xe6\xb3\x95\xef\xbc\x8c\xe4\xbc\x9a\xe5\x9c\xa8\xe7\xac\xac\xe4\xb8\x89\xe7\xab\xa0\xe7\xb3\xbb\xe7\xbb\x9f\xe8\xae\xb2\xe8\xa7\xa3\xe3\x80\x82\n# \n# Tensor\xe5\x92\x8cNumpy\xe7\x9a\x84\xe6\x95\xb0\xe7\xbb\x84\xe4\xb9\x8b\xe9\x97\xb4\xe7\x9a\x84\xe4\xba\x92\xe6\x93\x8d\xe4\xbd\x9c\xe9\x9d\x9e\xe5\xb8\xb8\xe5\xae\xb9\xe6\x98\x93\xe4\xb8\x94\xe5\xbf\xab\xe9\x80\x9f\xe3\x80\x82\xe5\xaf\xb9\xe4\xba\x8eTensor\xe4\xb8\x8d\xe6\x94\xaf\xe6\x8c\x81\xe7\x9a\x84\xe6\x93\x8d\xe4\xbd\x9c\xef\xbc\x8c\xe5\x8f\xaf\xe4\xbb\xa5\xe5\x85\x88\xe8\xbd\xac\xe4\xb8\xbaNumpy\xe6\x95\xb0\xe7\xbb\x84\xe5\xa4\x84\xe7\x90\x86\xef\xbc\x8c\xe4\xb9\x8b\xe5\x90\x8e\xe5\x86\x8d\xe8\xbd\xac\xe5\x9b\x9eTensor\xe3\x80\x82c\n# \n# \n\n#%%\na = t.ones(5) # \xe6\x96\xb0\xe5\xbb\xba\xe4\xb8\x80\xe4\xb8\xaa\xe5\x85\xa81\xe7\x9a\x84Tensor\na\n\n\n#%%\nb = a.numpy() # Tensor -> Numpy\nb\n\n\n#%%\nimport numpy as np\na = np.ones(5)\nb = t.from_numpy(a) # Numpy->Tensor\nprint(a)\nprint(b) \n\n#%% [markdown]\n# Tensor\xe5\x92\x8cnumpy\xe5\xaf\xb9\xe8\xb1\xa1\xe5\x85\xb1\xe4\xba\xab\xe5\x86\x85\xe5\xad\x98\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe4\xbb\x96\xe4\xbb\xac\xe4\xb9\x8b\xe9\x97\xb4\xe7\x9a\x84\xe8\xbd\xac\xe6\x8d\xa2\xe5\xbe\x88\xe5\xbf\xab\xef\xbc\x8c\xe8\x80\x8c\xe4\xb8\x94\xe5\x87\xa0\xe4\xb9\x8e\xe4\xb8\x8d\xe4\xbc\x9a\xe6\xb6\x88\xe8\x80\x97\xe4\xbb\x80\xe4\xb9\x88\xe8\xb5\x84\xe6\xba\x90\xe3\x80\x82\xe4\xbd\x86\xe8\xbf\x99\xe4\xb9\x9f\xe6\x84\x8f\xe5\x91\xb3\xe7\x9d\x80\xef\xbc\x8c\xe5\xa6\x82\xe6\x9e\x9c\xe5\x85\xb6\xe4\xb8\xad\xe4\xb8\x80\xe4\xb8\xaa\xe5\x8f\x98\xe4\xba\x86\xef\xbc\x8c\xe5\x8f\xa6\xe5\xa4\x96\xe4\xb8\x80\xe4\xb8\xaa\xe4\xb9\x9f\xe4\xbc\x9a\xe9\x9a\x8f\xe4\xb9\x8b\xe6\x94\xb9\xe5\x8f\x98\xe3\x80\x82\n\n#%%\nb.add_(1) # \xe4\xbb\xa5`_`\xe7\xbb\x93\xe5\xb0\xbe\xe7\x9a\x84\xe5\x87\xbd\xe6\x95\xb0\xe4\xbc\x9a\xe4\xbf\xae\xe6\x94\xb9\xe8\x87\xaa\xe8\xba\xab\nprint(a)\nprint(b) # Tensor\xe5\x92\x8cNumpy\xe5\x85\xb1\xe4\xba\xab\xe5\x86\x85\xe5\xad\x98\n\n#%% [markdown]\n# \xe5\xa6\x82\xe6\x9e\x9c\xe4\xbd\xa0\xe6\x83\xb3\xe8\x8e\xb7\xe5\x8f\x96\xe6\x9f\x90\xe4\xb8\x80\xe4\xb8\xaa\xe5\x85\x83\xe7\xb4\xa0\xe7\x9a\x84\xe5\x80\xbc\xef\xbc\x8c\xe5\x8f\xaf\xe4\xbb\xa5\xe4\xbd\xbf\xe7\x94\xa8`scalar.item`\xe3\x80\x82 \xe7\x9b\xb4\xe6\x8e\xa5`tensor[idx]`\xe5\xbe\x97\xe5\x88\xb0\xe7\x9a\x84\xe8\xbf\x98\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaatensor: \xe4\xb8\x80\xe4\xb8\xaa0-dim \xe7\x9a\x84tensor\xef\xbc\x8c\xe4\xb8\x80\xe8\x88\xac\xe7\xa7\xb0\xe4\xb8\xbascalar.\n\n#%%\nscalar = b[0]\nscalar\n\n\n#%%\nscalar.size() #0-dim\n\n\n#%%\nscalar.item() # \xe4\xbd\xbf\xe7\x94\xa8scalar.item()\xe8\x83\xbd\xe4\xbb\x8e\xe4\xb8\xad\xe5\x8f\x96\xe5\x87\xbapython\xe5\xaf\xb9\xe8\xb1\xa1\xe7\x9a\x84\xe6\x95\xb0\xe5\x80\xbc\n\n\n#%%\ntensor = t.tensor([2]) # \xe6\xb3\xa8\xe6\x84\x8f\xe5\x92\x8cscalar\xe7\x9a\x84\xe5\x8c\xba\xe5\x88\xab\ntensor,scalar\n\n\n#%%\ntensor.size(),scalar.size()\n\n\n#%%\n# \xe5\x8f\xaa\xe6\x9c\x89\xe4\xb8\x80\xe4\xb8\xaa\xe5\x85\x83\xe7\xb4\xa0\xe7\x9a\x84tensor\xe4\xb9\x9f\xe5\x8f\xaf\xe4\xbb\xa5\xe8\xb0\x83\xe7\x94\xa8`tensor.item()`\ntensor.item(), scalar.item()\n\n#%% [markdown]\n# \xe6\xad\xa4\xe5\xa4\x96\xe5\x9c\xa8pytorch\xe4\xb8\xad\xe8\xbf\x98\xe6\x9c\x89\xe4\xb8\x80\xe4\xb8\xaa\xe5\x92\x8c`np.array` \xe5\xbe\x88\xe7\xb1\xbb\xe4\xbc\xbc\xe7\x9a\x84\xe6\x8e\xa5\xe5\x8f\xa3: `torch.tensor`, \xe4\xba\x8c\xe8\x80\x85\xe7\x9a\x84\xe4\xbd\xbf\xe7\x94\xa8\xe5\x8d\x81\xe5\x88\x86\xe7\xb1\xbb\xe4\xbc\xbc\xe3\x80\x82\n\n#%%\ntensor = t.tensor([3,4]) # \xe6\x96\xb0\xe5\xbb\xba\xe4\xb8\x80\xe4\xb8\xaa\xe5\x8c\x85\xe5\x90\xab 3\xef\xbc\x8c4 \xe4\xb8\xa4\xe4\xb8\xaa\xe5\x85\x83\xe7\xb4\xa0\xe7\x9a\x84tensor\n\n\n#%%\nscalar = t.tensor(3)\nscalar\n\n\n#%%\nold_tensor = tensor\nnew_tensor = t.tensor(old_tensor)\nnew_tensor[0] = 1111\nold_tensor, new_tensor\n\n#%% [markdown]\n# \xe9\x9c\x80\xe8\xa6\x81\xe6\xb3\xa8\xe6\x84\x8f\xe7\x9a\x84\xe6\x98\xaf\xef\xbc\x8c`t.tensor()`\xe6\x80\xbb\xe6\x98\xaf\xe4\xbc\x9a\xe8\xbf\x9b\xe8\xa1\x8c\xe6\x95\xb0\xe6\x8d\xae\xe6\x8b\xb7\xe8\xb4\x9d\xef\xbc\x8c\xe6\x96\xb0tensor\xe5\x92\x8c\xe5\x8e\x9f\xe6\x9d\xa5\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe4\xb8\x8d\xe5\x86\x8d\xe5\x85\xb1\xe4\xba\xab\xe5\x86\x85\xe5\xad\x98\xe3\x80\x82\xe6\x89\x80\xe4\xbb\xa5\xe5\xa6\x82\xe6\x9e\x9c\xe4\xbd\xa0\xe6\x83\xb3\xe5\x85\xb1\xe4\xba\xab\xe5\x86\x85\xe5\xad\x98\xe7\x9a\x84\xe8\xaf\x9d\xef\xbc\x8c\xe5\xbb\xba\xe8\xae\xae\xe4\xbd\xbf\xe7\x94\xa8`torch.from_numpy()`\xe6\x88\x96\xe8\x80\x85`tensor.detach()`\xe6\x9d\xa5\xe6\x96\xb0\xe5\xbb\xba\xe4\xb8\x80\xe4\xb8\xaatensor, \xe4\xba\x8c\xe8\x80\x85\xe5\x85\xb1\xe4\xba\xab\xe5\x86\x85\xe5\xad\x98\xe3\x80\x82\n\n#%%\nnew_tensor = old_tensor.detach()\nnew_tensor[0] = 1111\nold_tensor, new_tensor\n\n#%% [markdown]\n# Tensor\xe5\x8f\xaf\xe9\x80\x9a\xe8\xbf\x87`.cuda` \xe6\x96\xb9\xe6\xb3\x95\xe8\xbd\xac\xe4\xb8\xbaGPU\xe7\x9a\x84Tensor\xef\xbc\x8c\xe4\xbb\x8e\xe8\x80\x8c\xe4\xba\xab\xe5\x8f\x97GPU\xe5\xb8\xa6\xe6\x9d\xa5\xe7\x9a\x84\xe5\x8a\xa0\xe9\x80\x9f\xe8\xbf\x90\xe7\xae\x97\xe3\x80\x82\n\n#%%\n# \xe5\x9c\xa8\xe4\xb8\x8d\xe6\x94\xaf\xe6\x8c\x81CUDA\xe7\x9a\x84\xe6\x9c\xba\xe5\x99\xa8\xe4\xb8\x8b\xef\xbc\x8c\xe4\xb8\x8b\xe4\xb8\x80\xe6\xad\xa5\xe8\xbf\x98\xe6\x98\xaf\xe5\x9c\xa8CPU\xe4\xb8\x8a\xe8\xbf\x90\xe8\xa1\x8c\ndevice = t.device(""cuda:0"" if t.cuda.is_available() else ""cpu"")\nx = x.to(device)\ny = y.to(device)\nz = x+y\n\n#%% [markdown]\n# \xe6\xad\xa4\xe5\xa4\x96\xef\xbc\x8c\xe8\xbf\x98\xe5\x8f\xaf\xe4\xbb\xa5\xe4\xbd\xbf\xe7\x94\xa8`tensor.cuda()` \xe7\x9a\x84\xe6\x96\xb9\xe5\xbc\x8f\xe5\xb0\x86tensor\xe6\x8b\xb7\xe8\xb4\x9d\xe5\x88\xb0gpu\xe4\xb8\x8a\xef\xbc\x8c\xe4\xbd\x86\xe6\x98\xaf\xe8\xbf\x99\xe7\xa7\x8d\xe6\x96\xb9\xe5\xbc\x8f\xe4\xb8\x8d\xe5\xa4\xaa\xe6\x8e\xa8\xe8\x8d\x90\xe3\x80\x82\n#%% [markdown]\n# \xe6\xad\xa4\xe5\xa4\x84\xe5\x8f\xaf\xe8\x83\xbd\xe5\x8f\x91\xe7\x8e\xb0GPU\xe8\xbf\x90\xe7\xae\x97\xe7\x9a\x84\xe9\x80\x9f\xe5\xba\xa6\xe5\xb9\xb6\xe6\x9c\xaa\xe6\x8f\x90\xe5\x8d\x87\xe5\xa4\xaa\xe5\xa4\x9a\xef\xbc\x8c\xe8\xbf\x99\xe6\x98\xaf\xe5\x9b\xa0\xe4\xb8\xbax\xe5\x92\x8cy\xe5\xa4\xaa\xe5\xb0\x8f\xe4\xb8\x94\xe8\xbf\x90\xe7\xae\x97\xe4\xb9\x9f\xe8\xbe\x83\xe4\xb8\xba\xe7\xae\x80\xe5\x8d\x95\xef\xbc\x8c\xe8\x80\x8c\xe4\xb8\x94\xe5\xb0\x86\xe6\x95\xb0\xe6\x8d\xae\xe4\xbb\x8e\xe5\x86\x85\xe5\xad\x98\xe8\xbd\xac\xe7\xa7\xbb\xe5\x88\xb0\xe6\x98\xbe\xe5\xad\x98\xe8\xbf\x98\xe9\x9c\x80\xe8\xa6\x81\xe8\x8a\xb1\xe8\xb4\xb9\xe9\xa2\x9d\xe5\xa4\x96\xe7\x9a\x84\xe5\xbc\x80\xe9\x94\x80\xe3\x80\x82GPU\xe7\x9a\x84\xe4\xbc\x98\xe5\x8a\xbf\xe9\x9c\x80\xe5\x9c\xa8\xe5\xa4\xa7\xe8\xa7\x84\xe6\xa8\xa1\xe6\x95\xb0\xe6\x8d\xae\xe5\x92\x8c\xe5\xa4\x8d\xe6\x9d\x82\xe8\xbf\x90\xe7\xae\x97\xe4\xb8\x8b\xe6\x89\x8d\xe8\x83\xbd\xe4\xbd\x93\xe7\x8e\xb0\xe5\x87\xba\xe6\x9d\xa5\xe3\x80\x82\n#%% [markdown]\n# \n# ### autograd: \xe8\x87\xaa\xe5\x8a\xa8\xe5\xbe\xae\xe5\x88\x86\n# \n# \xe6\xb7\xb1\xe5\xba\xa6\xe5\xad\xa6\xe4\xb9\xa0\xe7\x9a\x84\xe7\xae\x97\xe6\xb3\x95\xe6\x9c\xac\xe8\xb4\xa8\xe4\xb8\x8a\xe6\x98\xaf\xe9\x80\x9a\xe8\xbf\x87\xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe6\xb1\x82\xe5\xaf\xbc\xe6\x95\xb0\xef\xbc\x8c\xe8\x80\x8cPyTorch\xe7\x9a\x84**`autograd`**\xe6\xa8\xa1\xe5\x9d\x97\xe5\x88\x99\xe5\xae\x9e\xe7\x8e\xb0\xe4\xba\x86\xe6\xad\xa4\xe5\x8a\x9f\xe8\x83\xbd\xe3\x80\x82\xe5\x9c\xa8Tensor\xe4\xb8\x8a\xe7\x9a\x84\xe6\x89\x80\xe6\x9c\x89\xe6\x93\x8d\xe4\xbd\x9c\xef\xbc\x8cautograd\xe9\x83\xbd\xe8\x83\xbd\xe4\xb8\xba\xe5\xae\x83\xe4\xbb\xac\xe8\x87\xaa\xe5\x8a\xa8\xe6\x8f\x90\xe4\xbe\x9b\xe5\xbe\xae\xe5\x88\x86\xef\xbc\x8c\xe9\x81\xbf\xe5\x85\x8d\xe4\xba\x86\xe6\x89\x8b\xe5\x8a\xa8\xe8\xae\xa1\xe7\xae\x97\xe5\xaf\xbc\xe6\x95\xb0\xe7\x9a\x84\xe5\xa4\x8d\xe6\x9d\x82\xe8\xbf\x87\xe7\xa8\x8b\xe3\x80\x82\n#  \n# ~~`autograd.Variable`\xe6\x98\xafAutograd\xe4\xb8\xad\xe7\x9a\x84\xe6\xa0\xb8\xe5\xbf\x83\xe7\xb1\xbb\xef\xbc\x8c\xe5\xae\x83\xe7\xae\x80\xe5\x8d\x95\xe5\xb0\x81\xe8\xa3\x85\xe4\xba\x86Tensor\xef\xbc\x8c\xe5\xb9\xb6\xe6\x94\xaf\xe6\x8c\x81\xe5\x87\xa0\xe4\xb9\x8e\xe6\x89\x80\xe6\x9c\x89Tensor\xe6\x9c\x89\xe7\x9a\x84\xe6\x93\x8d\xe4\xbd\x9c\xe3\x80\x82Tensor\xe5\x9c\xa8\xe8\xa2\xab\xe5\xb0\x81\xe8\xa3\x85\xe4\xb8\xbaVariable\xe4\xb9\x8b\xe5\x90\x8e\xef\xbc\x8c\xe5\x8f\xaf\xe4\xbb\xa5\xe8\xb0\x83\xe7\x94\xa8\xe5\xae\x83\xe7\x9a\x84`.backward`\xe5\xae\x9e\xe7\x8e\xb0\xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xef\xbc\x8c\xe8\x87\xaa\xe5\x8a\xa8\xe8\xae\xa1\xe7\xae\x97\xe6\x89\x80\xe6\x9c\x89\xe6\xa2\xaf\xe5\xba\xa6~~ ~~Variable\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe7\xbb\x93\xe6\x9e\x84\xe5\xa6\x82\xe5\x9b\xbe2-6\xe6\x89\x80\xe7\xa4\xba\xe3\x80\x82~~\n# \n# \n# ![\xe5\x9b\xbe2-6:Variable\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe7\xbb\x93\xe6\x9e\x84](imgs/autograd_Variable.svg)\n# \n#   *\xe4\xbb\x8e0.4\xe8\xb5\xb7, Variable \xe6\xad\xa3\xe5\xbc\x8f\xe5\x90\x88\xe5\xb9\xb6\xe5\x85\xa5Tensor, Variable \xe6\x9c\xac\xe6\x9d\xa5\xe5\xae\x9e\xe7\x8e\xb0\xe7\x9a\x84\xe8\x87\xaa\xe5\x8a\xa8\xe5\xbe\xae\xe5\x88\x86\xe5\x8a\x9f\xe8\x83\xbd\xef\xbc\x8cTensor\xe5\xb0\xb1\xe8\x83\xbd\xe6\x94\xaf\xe6\x8c\x81\xe3\x80\x82\xe8\xaf\xbb\xe8\x80\x85\xe8\xbf\x98\xe6\x98\xaf\xe5\x8f\xaf\xe4\xbb\xa5\xe4\xbd\xbf\xe7\x94\xa8Variable(tensor), \xe4\xbd\x86\xe6\x98\xaf\xe8\xbf\x99\xe4\xb8\xaa\xe6\x93\x8d\xe4\xbd\x9c\xe5\x85\xb6\xe5\xae\x9e\xe4\xbb\x80\xe4\xb9\x88\xe9\x83\xbd\xe6\xb2\xa1\xe5\x81\x9a\xe3\x80\x82\xe5\xbb\xba\xe8\xae\xae\xe8\xaf\xbb\xe8\x80\x85\xe4\xbb\xa5\xe5\x90\x8e\xe7\x9b\xb4\xe6\x8e\xa5\xe4\xbd\xbf\xe7\x94\xa8tensor*. \n#   \n#   \xe8\xa6\x81\xe6\x83\xb3\xe4\xbd\xbf\xe5\xbe\x97Tensor\xe4\xbd\xbf\xe7\x94\xa8autograd\xe5\x8a\x9f\xe8\x83\xbd\xef\xbc\x8c\xe5\x8f\xaa\xe9\x9c\x80\xe8\xa6\x81\xe8\xae\xbe\xe7\xbd\xae`tensor.requries_grad=True`. \n# \n# \n# ~~Variable\xe4\xb8\xbb\xe8\xa6\x81\xe5\x8c\x85\xe5\x90\xab\xe4\xb8\x89\xe4\xb8\xaa\xe5\xb1\x9e\xe6\x80\xa7\xe3\x80\x82~~\n# ~~- `data`\xef\xbc\x9a\xe4\xbf\x9d\xe5\xad\x98Variable\xe6\x89\x80\xe5\x8c\x85\xe5\x90\xab\xe7\x9a\x84Tensor~~\n# ~~- `grad`\xef\xbc\x9a\xe4\xbf\x9d\xe5\xad\x98`data`\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\xef\xbc\x8c`grad`\xe4\xb9\x9f\xe6\x98\xaf\xe4\xb8\xaaVariable\xef\xbc\x8c\xe8\x80\x8c\xe4\xb8\x8d\xe6\x98\xafTensor\xef\xbc\x8c\xe5\xae\x83\xe5\x92\x8c`data`\xe7\x9a\x84\xe5\xbd\xa2\xe7\x8a\xb6\xe4\xb8\x80\xe6\xa0\xb7\xe3\x80\x82~~\n# ~~- `grad_fn`\xef\xbc\x9a\xe6\x8c\x87\xe5\x90\x91\xe4\xb8\x80\xe4\xb8\xaa`Function`\xe5\xaf\xb9\xe8\xb1\xa1\xef\xbc\x8c\xe8\xbf\x99\xe4\xb8\xaa`Function`\xe7\x94\xa8\xe6\x9d\xa5\xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe8\xae\xa1\xe7\xae\x97\xe8\xbe\x93\xe5\x85\xa5\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\xef\xbc\x8c\xe5\x85\xb7\xe4\xbd\x93\xe7\xbb\x86\xe8\x8a\x82\xe4\xbc\x9a\xe5\x9c\xa8\xe4\xb8\x8b\xe4\xb8\x80\xe7\xab\xa0\xe8\xae\xb2\xe8\xa7\xa3\xe3\x80\x82~~\n\n#%%\n# \xe4\xb8\xbatensor\xe8\xae\xbe\xe7\xbd\xae requires_grad \xe6\xa0\x87\xe8\xaf\x86\xef\xbc\x8c\xe4\xbb\xa3\xe8\xa1\xa8\xe7\x9d\x80\xe9\x9c\x80\xe8\xa6\x81\xe6\xb1\x82\xe5\xaf\xbc\xe6\x95\xb0\n# pytorch \xe4\xbc\x9a\xe8\x87\xaa\xe5\x8a\xa8\xe8\xb0\x83\xe7\x94\xa8autograd \xe8\xae\xb0\xe5\xbd\x95\xe6\x93\x8d\xe4\xbd\x9c\nx = t.ones(2, 2, requires_grad=True)\n\n# \xe4\xb8\x8a\xe4\xb8\x80\xe6\xad\xa5\xe7\xad\x89\xe4\xbb\xb7\xe4\xba\x8e\n# x = t.ones(2,2)\n# x.requires_grad = True\n\nx\n\n\n#%%\ny = x.sum()\ny\n\n\n#%%\ny.grad_fn\n\n\n#%%\ny.backward() # \xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad,\xe8\xae\xa1\xe7\xae\x97\xe6\xa2\xaf\xe5\xba\xa6\n\n\n#%%\n# y = x.sum() = (x[0][0] + x[0][1] + x[1][0] + x[1][1])\n# \xe6\xaf\x8f\xe4\xb8\xaa\xe5\x80\xbc\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\xe9\x83\xbd\xe4\xb8\xba1\nx.grad \n\n#%% [markdown]\n# \xe6\xb3\xa8\xe6\x84\x8f\xef\xbc\x9a`grad`\xe5\x9c\xa8\xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe8\xbf\x87\xe7\xa8\x8b\xe4\xb8\xad\xe6\x98\xaf\xe7\xb4\xaf\xe5\x8a\xa0\xe7\x9a\x84(accumulated)\xef\xbc\x8c\xe8\xbf\x99\xe6\x84\x8f\xe5\x91\xb3\xe7\x9d\x80\xe6\xaf\x8f\xe4\xb8\x80\xe6\xac\xa1\xe8\xbf\x90\xe8\xa1\x8c\xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xef\xbc\x8c\xe6\xa2\xaf\xe5\xba\xa6\xe9\x83\xbd\xe4\xbc\x9a\xe7\xb4\xaf\xe5\x8a\xa0\xe4\xb9\x8b\xe5\x89\x8d\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe4\xb9\x8b\xe5\x89\x8d\xe9\x9c\x80\xe6\x8a\x8a\xe6\xa2\xaf\xe5\xba\xa6\xe6\xb8\x85\xe9\x9b\xb6\xe3\x80\x82\n\n#%%\ny.backward()\nx.grad\n\n\n#%%\ny.backward()\nx.grad\n\n\n#%%\n# \xe4\xbb\xa5\xe4\xb8\x8b\xe5\x88\x92\xe7\xba\xbf\xe7\xbb\x93\xe6\x9d\x9f\xe7\x9a\x84\xe5\x87\xbd\xe6\x95\xb0\xe6\x98\xafinplace\xe6\x93\x8d\xe4\xbd\x9c\xef\xbc\x8c\xe4\xbc\x9a\xe4\xbf\xae\xe6\x94\xb9\xe8\x87\xaa\xe8\xba\xab\xe7\x9a\x84\xe5\x80\xbc\xef\xbc\x8c\xe5\xb0\xb1\xe5\x83\x8fadd_\nx.grad.data.zero_()\n\n\n#%%\ny.backward()\nx.grad\n\n#%% [markdown]\n# ###  \xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\n# \n# Autograd\xe5\xae\x9e\xe7\x8e\xb0\xe4\xba\x86\xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe5\x8a\x9f\xe8\x83\xbd\xef\xbc\x8c\xe4\xbd\x86\xe6\x98\xaf\xe7\x9b\xb4\xe6\x8e\xa5\xe7\x94\xa8\xe6\x9d\xa5\xe5\x86\x99\xe6\xb7\xb1\xe5\xba\xa6\xe5\xad\xa6\xe4\xb9\xa0\xe7\x9a\x84\xe4\xbb\xa3\xe7\xa0\x81\xe5\x9c\xa8\xe5\xbe\x88\xe5\xa4\x9a\xe6\x83\x85\xe5\x86\xb5\xe4\xb8\x8b\xe8\xbf\x98\xe6\x98\xaf\xe7\xa8\x8d\xe6\x98\xbe\xe5\xa4\x8d\xe6\x9d\x82\xef\xbc\x8ctorch.nn\xe6\x98\xaf\xe4\xb8\x93\xe9\x97\xa8\xe4\xb8\xba\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xe8\xae\xbe\xe8\xae\xa1\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9d\x97\xe5\x8c\x96\xe6\x8e\xa5\xe5\x8f\xa3\xe3\x80\x82nn\xe6\x9e\x84\xe5\xbb\xba\xe4\xba\x8e Autograd\xe4\xb9\x8b\xe4\xb8\x8a\xef\xbc\x8c\xe5\x8f\xaf\xe7\x94\xa8\xe6\x9d\xa5\xe5\xae\x9a\xe4\xb9\x89\xe5\x92\x8c\xe8\xbf\x90\xe8\xa1\x8c\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xe3\x80\x82nn.Module\xe6\x98\xafnn\xe4\xb8\xad\xe6\x9c\x80\xe9\x87\x8d\xe8\xa6\x81\xe7\x9a\x84\xe7\xb1\xbb\xef\xbc\x8c\xe5\x8f\xaf\xe6\x8a\x8a\xe5\xae\x83\xe7\x9c\x8b\xe6\x88\x90\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa\xe7\xbd\x91\xe7\xbb\x9c\xe7\x9a\x84\xe5\xb0\x81\xe8\xa3\x85\xef\xbc\x8c\xe5\x8c\x85\xe5\x90\xab\xe7\xbd\x91\xe7\xbb\x9c\xe5\x90\x84\xe5\xb1\x82\xe5\xae\x9a\xe4\xb9\x89\xe4\xbb\xa5\xe5\x8f\x8aforward\xe6\x96\xb9\xe6\xb3\x95\xef\xbc\x8c\xe8\xb0\x83\xe7\x94\xa8forward(input)\xe6\x96\xb9\xe6\xb3\x95\xef\xbc\x8c\xe5\x8f\xaf\xe8\xbf\x94\xe5\x9b\x9e\xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x9c\xe3\x80\x82\xe4\xb8\x8b\xe9\x9d\xa2\xe5\xb0\xb1\xe4\xbb\xa5\xe6\x9c\x80\xe6\x97\xa9\xe7\x9a\x84\xe5\x8d\xb7\xe7\xa7\xaf\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xef\xbc\x9aLeNet\xe4\xb8\xba\xe4\xbe\x8b\xef\xbc\x8c\xe6\x9d\xa5\xe7\x9c\x8b\xe7\x9c\x8b\xe5\xa6\x82\xe4\xbd\x95\xe7\x94\xa8`nn.Module`\xe5\xae\x9e\xe7\x8e\xb0\xe3\x80\x82LeNet\xe7\x9a\x84\xe7\xbd\x91\xe7\xbb\x9c\xe7\xbb\x93\xe6\x9e\x84\xe5\xa6\x82\xe5\x9b\xbe2-7\xe6\x89\x80\xe7\xa4\xba\xe3\x80\x82\n# \n# ![\xe5\x9b\xbe2-7:LeNet\xe7\xbd\x91\xe7\xbb\x9c\xe7\xbb\x93\xe6\x9e\x84](imgs/nn_lenet.png)\n# \n# \xe8\xbf\x99\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa\xe5\x9f\xba\xe7\xa1\x80\xe7\x9a\x84\xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad(feed-forward)\xe7\xbd\x91\xe7\xbb\x9c: \xe6\x8e\xa5\xe6\x94\xb6\xe8\xbe\x93\xe5\x85\xa5\xef\xbc\x8c\xe7\xbb\x8f\xe8\xbf\x87\xe5\xb1\x82\xe5\xb1\x82\xe4\xbc\xa0\xe9\x80\x92\xe8\xbf\x90\xe7\xae\x97\xef\xbc\x8c\xe5\xbe\x97\xe5\x88\xb0\xe8\xbe\x93\xe5\x87\xba\xe3\x80\x82\n# \n# #### \xe5\xae\x9a\xe4\xb9\x89\xe7\xbd\x91\xe7\xbb\x9c\n# \n# \xe5\xae\x9a\xe4\xb9\x89\xe7\xbd\x91\xe7\xbb\x9c\xe6\x97\xb6\xef\xbc\x8c\xe9\x9c\x80\xe8\xa6\x81\xe7\xbb\xa7\xe6\x89\xbf`nn.Module`\xef\xbc\x8c\xe5\xb9\xb6\xe5\xae\x9e\xe7\x8e\xb0\xe5\xae\x83\xe7\x9a\x84forward\xe6\x96\xb9\xe6\xb3\x95\xef\xbc\x8c\xe6\x8a\x8a\xe7\xbd\x91\xe7\xbb\x9c\xe4\xb8\xad\xe5\x85\xb7\xe6\x9c\x89\xe5\x8f\xaf\xe5\xad\xa6\xe4\xb9\xa0\xe5\x8f\x82\xe6\x95\xb0\xe7\x9a\x84\xe5\xb1\x82\xe6\x94\xbe\xe5\x9c\xa8\xe6\x9e\x84\xe9\x80\xa0\xe5\x87\xbd\xe6\x95\xb0`__init__`\xe4\xb8\xad\xe3\x80\x82\xe5\xa6\x82\xe6\x9e\x9c\xe6\x9f\x90\xe4\xb8\x80\xe5\xb1\x82(\xe5\xa6\x82ReLU)\xe4\xb8\x8d\xe5\x85\xb7\xe6\x9c\x89\xe5\x8f\xaf\xe5\xad\xa6\xe4\xb9\xa0\xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0\xef\xbc\x8c\xe5\x88\x99\xe6\x97\xa2\xe5\x8f\xaf\xe4\xbb\xa5\xe6\x94\xbe\xe5\x9c\xa8\xe6\x9e\x84\xe9\x80\xa0\xe5\x87\xbd\xe6\x95\xb0\xe4\xb8\xad\xef\xbc\x8c\xe4\xb9\x9f\xe5\x8f\xaf\xe4\xbb\xa5\xe4\xb8\x8d\xe6\x94\xbe\xef\xbc\x8c\xe4\xbd\x86\xe5\xbb\xba\xe8\xae\xae\xe4\xb8\x8d\xe6\x94\xbe\xe5\x9c\xa8\xe5\x85\xb6\xe4\xb8\xad\xef\xbc\x8c\xe8\x80\x8c\xe5\x9c\xa8forward\xe4\xb8\xad\xe4\xbd\xbf\xe7\x94\xa8`nn.functional`\xe4\xbb\xa3\xe6\x9b\xbf\xe3\x80\x82\n\n#%%\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Net(nn.Module):\n    def __init__(self):\n        # nn.Module\xe5\xad\x90\xe7\xb1\xbb\xe7\x9a\x84\xe5\x87\xbd\xe6\x95\xb0\xe5\xbf\x85\xe9\xa1\xbb\xe5\x9c\xa8\xe6\x9e\x84\xe9\x80\xa0\xe5\x87\xbd\xe6\x95\xb0\xe4\xb8\xad\xe6\x89\xa7\xe8\xa1\x8c\xe7\x88\xb6\xe7\xb1\xbb\xe7\x9a\x84\xe6\x9e\x84\xe9\x80\xa0\xe5\x87\xbd\xe6\x95\xb0\n        # \xe4\xb8\x8b\xe5\xbc\x8f\xe7\xad\x89\xe4\xbb\xb7\xe4\xba\x8enn.Module.__init__(self)\n        super(Net, self).__init__()\n        \n        # \xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82 \'1\'\xe8\xa1\xa8\xe7\xa4\xba\xe8\xbe\x93\xe5\x85\xa5\xe5\x9b\xbe\xe7\x89\x87\xe4\xb8\xba\xe5\x8d\x95\xe9\x80\x9a\xe9\x81\x93, \'6\'\xe8\xa1\xa8\xe7\xa4\xba\xe8\xbe\x93\xe5\x87\xba\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\xef\xbc\x8c\'5\'\xe8\xa1\xa8\xe7\xa4\xba\xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8\xe4\xb8\xba5*5\n        self.conv1 = nn.Conv2d(1, 6, 5) \n        # \xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82\n        self.conv2 = nn.Conv2d(6, 16, 5) \n        # \xe4\xbb\xbf\xe5\xb0\x84\xe5\xb1\x82/\xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe5\xb1\x82\xef\xbc\x8cy = Wx + b\n        self.fc1   = nn.Linear(16*5*5, 120) \n        self.fc2   = nn.Linear(120, 84)\n        self.fc3   = nn.Linear(84, 10)\n\n    def forward(self, x): \n        # \xe5\x8d\xb7\xe7\xa7\xaf -> \xe6\xbf\x80\xe6\xb4\xbb -> \xe6\xb1\xa0\xe5\x8c\x96 \n        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n        x = F.max_pool2d(F.relu(self.conv2(x)), 2) \n        # reshape\xef\xbc\x8c\xe2\x80\x98-1\xe2\x80\x99\xe8\xa1\xa8\xe7\xa4\xba\xe8\x87\xaa\xe9\x80\x82\xe5\xba\x94\n        x = x.view(x.size()[0], -1) \n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)        \n        return x\n\nnet = Net()\nprint(net)\n\n#%% [markdown]\n# \xe5\x8f\xaa\xe8\xa6\x81\xe5\x9c\xa8nn.Module\xe7\x9a\x84\xe5\xad\x90\xe7\xb1\xbb\xe4\xb8\xad\xe5\xae\x9a\xe4\xb9\x89\xe4\xba\x86forward\xe5\x87\xbd\xe6\x95\xb0\xef\xbc\x8cbackward\xe5\x87\xbd\xe6\x95\xb0\xe5\xb0\xb1\xe4\xbc\x9a\xe8\x87\xaa\xe5\x8a\xa8\xe8\xa2\xab\xe5\xae\x9e\xe7\x8e\xb0(\xe5\x88\xa9\xe7\x94\xa8`autograd`)\xe3\x80\x82\xe5\x9c\xa8`forward` \xe5\x87\xbd\xe6\x95\xb0\xe4\xb8\xad\xe5\x8f\xaf\xe4\xbd\xbf\xe7\x94\xa8\xe4\xbb\xbb\xe4\xbd\x95tensor\xe6\x94\xaf\xe6\x8c\x81\xe7\x9a\x84\xe5\x87\xbd\xe6\x95\xb0\xef\xbc\x8c\xe8\xbf\x98\xe5\x8f\xaf\xe4\xbb\xa5\xe4\xbd\xbf\xe7\x94\xa8if\xe3\x80\x81for\xe5\xbe\xaa\xe7\x8e\xaf\xe3\x80\x81print\xe3\x80\x81log\xe7\xad\x89Python\xe8\xaf\xad\xe6\xb3\x95\xef\xbc\x8c\xe5\x86\x99\xe6\xb3\x95\xe5\x92\x8c\xe6\xa0\x87\xe5\x87\x86\xe7\x9a\x84Python\xe5\x86\x99\xe6\xb3\x95\xe4\xb8\x80\xe8\x87\xb4\xe3\x80\x82\n# \n# \xe7\xbd\x91\xe7\xbb\x9c\xe7\x9a\x84\xe5\x8f\xaf\xe5\xad\xa6\xe4\xb9\xa0\xe5\x8f\x82\xe6\x95\xb0\xe9\x80\x9a\xe8\xbf\x87`net.parameters()`\xe8\xbf\x94\xe5\x9b\x9e\xef\xbc\x8c`net.named_parameters`\xe5\x8f\xaf\xe5\x90\x8c\xe6\x97\xb6\xe8\xbf\x94\xe5\x9b\x9e\xe5\x8f\xaf\xe5\xad\xa6\xe4\xb9\xa0\xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0\xe5\x8f\x8a\xe5\x90\x8d\xe7\xa7\xb0\xe3\x80\x82\n\n#%%\nparams = list(net.parameters())\nprint(len(params))\n\n\n#%%\nfor name,parameters in net.named_parameters():\n    print(name,\':\',parameters.size())\n\n#%% [markdown]\n# forward\xe5\x87\xbd\xe6\x95\xb0\xe7\x9a\x84\xe8\xbe\x93\xe5\x85\xa5\xe5\x92\x8c\xe8\xbe\x93\xe5\x87\xba\xe9\x83\xbd\xe6\x98\xafTensor\xe3\x80\x82\n\n#%%\ninput = t.randn(1, 1, 32, 32)\nout = net(input)\nout.size()\n\n\n#%%\nnet.zero_grad() # \xe6\x89\x80\xe6\x9c\x89\xe5\x8f\x82\xe6\x95\xb0\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\xe6\xb8\x85\xe9\x9b\xb6\nout.backward(t.ones(1,10)) # \xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\n\n#%% [markdown]\n# \xe9\x9c\x80\xe8\xa6\x81\xe6\xb3\xa8\xe6\x84\x8f\xe7\x9a\x84\xe6\x98\xaf\xef\xbc\x8ctorch.nn\xe5\x8f\xaa\xe6\x94\xaf\xe6\x8c\x81mini-batches\xef\xbc\x8c\xe4\xb8\x8d\xe6\x94\xaf\xe6\x8c\x81\xe4\xb8\x80\xe6\xac\xa1\xe5\x8f\xaa\xe8\xbe\x93\xe5\x85\xa5\xe4\xb8\x80\xe4\xb8\xaa\xe6\xa0\xb7\xe6\x9c\xac\xef\xbc\x8c\xe5\x8d\xb3\xe4\xb8\x80\xe6\xac\xa1\xe5\xbf\x85\xe9\xa1\xbb\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaabatch\xe3\x80\x82\xe4\xbd\x86\xe5\xa6\x82\xe6\x9e\x9c\xe5\x8f\xaa\xe6\x83\xb3\xe8\xbe\x93\xe5\x85\xa5\xe4\xb8\x80\xe4\xb8\xaa\xe6\xa0\xb7\xe6\x9c\xac\xef\xbc\x8c\xe5\x88\x99\xe7\x94\xa8 `input.unsqueeze(0)`\xe5\xb0\x86batch_size\xe8\xae\xbe\xe4\xb8\xba\xef\xbc\x91\xe3\x80\x82\xe4\xbe\x8b\xe5\xa6\x82 `nn.Conv2d` \xe8\xbe\x93\xe5\x85\xa5\xe5\xbf\x85\xe9\xa1\xbb\xe6\x98\xaf4\xe7\xbb\xb4\xe7\x9a\x84\xef\xbc\x8c\xe5\xbd\xa2\xe5\xa6\x82$nSamples \\times nChannels \\times Height \\times Width$\xe3\x80\x82\xe5\x8f\xaf\xe5\xb0\x86nSample\xe8\xae\xbe\xe4\xb8\xba1\xef\xbc\x8c\xe5\x8d\xb3$1 \\times nChannels \\times Height \\times Width$\xe3\x80\x82\n#%% [markdown]\n# #### \xe6\x8d\x9f\xe5\xa4\xb1\xe5\x87\xbd\xe6\x95\xb0\n# \n# nn\xe5\xae\x9e\xe7\x8e\xb0\xe4\xba\x86\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xe4\xb8\xad\xe5\xa4\xa7\xe5\xa4\x9a\xe6\x95\xb0\xe7\x9a\x84\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x87\xbd\xe6\x95\xb0\xef\xbc\x8c\xe4\xbe\x8b\xe5\xa6\x82nn.MSELoss\xe7\x94\xa8\xe6\x9d\xa5\xe8\xae\xa1\xe7\xae\x97\xe5\x9d\x87\xe6\x96\xb9\xe8\xaf\xaf\xe5\xb7\xae\xef\xbc\x8cnn.CrossEntropyLoss\xe7\x94\xa8\xe6\x9d\xa5\xe8\xae\xa1\xe7\xae\x97\xe4\xba\xa4\xe5\x8f\x89\xe7\x86\xb5\xe6\x8d\x9f\xe5\xa4\xb1\xe3\x80\x82\n\n#%%\noutput = net(input)\ntarget = t.arange(0,10).view(1,10) \ncriterion = nn.MSELoss()\nloss = criterion(output, target)\nloss # loss\xe6\x98\xaf\xe4\xb8\xaascalar\n\n#%% [markdown]\n# \xe5\xa6\x82\xe6\x9e\x9c\xe5\xaf\xb9loss\xe8\xbf\x9b\xe8\xa1\x8c\xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe6\xba\xaf\xe6\xba\x90(\xe4\xbd\xbf\xe7\x94\xa8`gradfn`\xe5\xb1\x9e\xe6\x80\xa7)\xef\xbc\x8c\xe5\x8f\xaf\xe7\x9c\x8b\xe5\x88\xb0\xe5\xae\x83\xe7\x9a\x84\xe8\xae\xa1\xe7\xae\x97\xe5\x9b\xbe\xe5\xa6\x82\xe4\xb8\x8b\xef\xbc\x9a\n# \n# ```\n# input -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d  \n#       -> view -> linear -> relu -> linear -> relu -> linear \n#       -> MSELoss\n#       -> loss\n# ```\n# \n# \xe5\xbd\x93\xe8\xb0\x83\xe7\x94\xa8`loss.backward()`\xe6\x97\xb6\xef\xbc\x8c\xe8\xaf\xa5\xe5\x9b\xbe\xe4\xbc\x9a\xe5\x8a\xa8\xe6\x80\x81\xe7\x94\x9f\xe6\x88\x90\xe5\xb9\xb6\xe8\x87\xaa\xe5\x8a\xa8\xe5\xbe\xae\xe5\x88\x86\xef\xbc\x8c\xe4\xb9\x9f\xe5\x8d\xb3\xe4\xbc\x9a\xe8\x87\xaa\xe5\x8a\xa8\xe8\xae\xa1\xe7\xae\x97\xe5\x9b\xbe\xe4\xb8\xad\xe5\x8f\x82\xe6\x95\xb0(Parameter)\xe7\x9a\x84\xe5\xaf\xbc\xe6\x95\xb0\xe3\x80\x82\n\n#%%\n# \xe8\xbf\x90\xe8\xa1\x8c.backward\xef\xbc\x8c\xe8\xa7\x82\xe5\xaf\x9f\xe8\xb0\x83\xe7\x94\xa8\xe4\xb9\x8b\xe5\x89\x8d\xe5\x92\x8c\xe8\xb0\x83\xe7\x94\xa8\xe4\xb9\x8b\xe5\x90\x8e\xe7\x9a\x84grad\nnet.zero_grad() # \xe6\x8a\x8anet\xe4\xb8\xad\xe6\x89\x80\xe6\x9c\x89\xe5\x8f\xaf\xe5\xad\xa6\xe4\xb9\xa0\xe5\x8f\x82\xe6\x95\xb0\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\xe6\xb8\x85\xe9\x9b\xb6\nprint(\'\xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe4\xb9\x8b\xe5\x89\x8d conv1.bias\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\')\nprint(net.conv1.bias.grad)\nloss.backward()\nprint(\'\xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe4\xb9\x8b\xe5\x90\x8e conv1.bias\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\')\nprint(net.conv1.bias.grad)\n\n#%% [markdown]\n# #### \xe4\xbc\x98\xe5\x8c\x96\xe5\x99\xa8\n#%% [markdown]\n# \xe5\x9c\xa8\xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe8\xae\xa1\xe7\xae\x97\xe5\xae\x8c\xe6\x89\x80\xe6\x9c\x89\xe5\x8f\x82\xe6\x95\xb0\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\xe5\x90\x8e\xef\xbc\x8c\xe8\xbf\x98\xe9\x9c\x80\xe8\xa6\x81\xe4\xbd\xbf\xe7\x94\xa8\xe4\xbc\x98\xe5\x8c\x96\xe6\x96\xb9\xe6\xb3\x95\xe6\x9d\xa5\xe6\x9b\xb4\xe6\x96\xb0\xe7\xbd\x91\xe7\xbb\x9c\xe7\x9a\x84\xe6\x9d\x83\xe9\x87\x8d\xe5\x92\x8c\xe5\x8f\x82\xe6\x95\xb0\xef\xbc\x8c\xe4\xbe\x8b\xe5\xa6\x82\xe9\x9a\x8f\xe6\x9c\xba\xe6\xa2\xaf\xe5\xba\xa6\xe4\xb8\x8b\xe9\x99\x8d\xe6\xb3\x95(SGD)\xe7\x9a\x84\xe6\x9b\xb4\xe6\x96\xb0\xe7\xad\x96\xe7\x95\xa5\xe5\xa6\x82\xe4\xb8\x8b\xef\xbc\x9a\n# ```\n# weight = weight - learning_rate * gradient\n# ```\n# \n# \xe6\x89\x8b\xe5\x8a\xa8\xe5\xae\x9e\xe7\x8e\xb0\xe5\xa6\x82\xe4\xb8\x8b\xef\xbc\x9a\n# \n# ```python\n# learning_rate = 0.01\n# for f in net.parameters():\n#     f.data.sub_(f.grad.data * learning_rate)# inplace \xe5\x87\x8f\xe6\xb3\x95\n# ```\n# \n# `torch.optim`\xe4\xb8\xad\xe5\xae\x9e\xe7\x8e\xb0\xe4\xba\x86\xe6\xb7\xb1\xe5\xba\xa6\xe5\xad\xa6\xe4\xb9\xa0\xe4\xb8\xad\xe7\xbb\x9d\xe5\xa4\xa7\xe5\xa4\x9a\xe6\x95\xb0\xe7\x9a\x84\xe4\xbc\x98\xe5\x8c\x96\xe6\x96\xb9\xe6\xb3\x95\xef\xbc\x8c\xe4\xbe\x8b\xe5\xa6\x82RMSProp\xe3\x80\x81Adam\xe3\x80\x81SGD\xe7\xad\x89\xef\xbc\x8c\xe6\x9b\xb4\xe4\xbe\xbf\xe4\xba\x8e\xe4\xbd\xbf\xe7\x94\xa8\xef\xbc\x8c\xe5\x9b\xa0\xe6\xad\xa4\xe5\xa4\xa7\xe5\xa4\x9a\xe6\x95\xb0\xe6\x97\xb6\xe5\x80\x99\xe5\xb9\xb6\xe4\xb8\x8d\xe9\x9c\x80\xe8\xa6\x81\xe6\x89\x8b\xe5\x8a\xa8\xe5\x86\x99\xe4\xb8\x8a\xe8\xbf\xb0\xe4\xbb\xa3\xe7\xa0\x81\xe3\x80\x82\n\n#%%\nimport torch.optim as optim\n#\xe6\x96\xb0\xe5\xbb\xba\xe4\xb8\x80\xe4\xb8\xaa\xe4\xbc\x98\xe5\x8c\x96\xe5\x99\xa8\xef\xbc\x8c\xe6\x8c\x87\xe5\xae\x9a\xe8\xa6\x81\xe8\xb0\x83\xe6\x95\xb4\xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0\xe5\x92\x8c\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\noptimizer = optim.SGD(net.parameters(), lr = 0.01)\n\n# \xe5\x9c\xa8\xe8\xae\xad\xe7\xbb\x83\xe8\xbf\x87\xe7\xa8\x8b\xe4\xb8\xad\n# \xe5\x85\x88\xe6\xa2\xaf\xe5\xba\xa6\xe6\xb8\x85\xe9\x9b\xb6(\xe4\xb8\x8enet.zero_grad()\xe6\x95\x88\xe6\x9e\x9c\xe4\xb8\x80\xe6\xa0\xb7)\noptimizer.zero_grad() \n\n# \xe8\xae\xa1\xe7\xae\x97\xe6\x8d\x9f\xe5\xa4\xb1\noutput = net(input)\nloss = criterion(output, target)\n\n#\xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\nloss.backward()\n\n#\xe6\x9b\xb4\xe6\x96\xb0\xe5\x8f\x82\xe6\x95\xb0\noptimizer.step()\n\n#%% [markdown]\n# \n# \n# ####  \xe6\x95\xb0\xe6\x8d\xae\xe5\x8a\xa0\xe8\xbd\xbd\xe4\xb8\x8e\xe9\xa2\x84\xe5\xa4\x84\xe7\x90\x86\n# \n# \xe5\x9c\xa8\xe6\xb7\xb1\xe5\xba\xa6\xe5\xad\xa6\xe4\xb9\xa0\xe4\xb8\xad\xe6\x95\xb0\xe6\x8d\xae\xe5\x8a\xa0\xe8\xbd\xbd\xe5\x8f\x8a\xe9\xa2\x84\xe5\xa4\x84\xe7\x90\x86\xe6\x98\xaf\xe9\x9d\x9e\xe5\xb8\xb8\xe5\xa4\x8d\xe6\x9d\x82\xe7\xb9\x81\xe7\x90\x90\xe7\x9a\x84\xef\xbc\x8c\xe4\xbd\x86PyTorch\xe6\x8f\x90\xe4\xbe\x9b\xe4\xba\x86\xe4\xb8\x80\xe4\xba\x9b\xe5\x8f\xaf\xe6\x9e\x81\xe5\xa4\xa7\xe7\xae\x80\xe5\x8c\x96\xe5\x92\x8c\xe5\x8a\xa0\xe5\xbf\xab\xe6\x95\xb0\xe6\x8d\xae\xe5\xa4\x84\xe7\x90\x86\xe6\xb5\x81\xe7\xa8\x8b\xe7\x9a\x84\xe5\xb7\xa5\xe5\x85\xb7\xe3\x80\x82\xe5\x90\x8c\xe6\x97\xb6\xef\xbc\x8c\xe5\xaf\xb9\xe4\xba\x8e\xe5\xb8\xb8\xe7\x94\xa8\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xef\xbc\x8cPyTorch\xe4\xb9\x9f\xe6\x8f\x90\xe4\xbe\x9b\xe4\xba\x86\xe5\xb0\x81\xe8\xa3\x85\xe5\xa5\xbd\xe7\x9a\x84\xe6\x8e\xa5\xe5\x8f\xa3\xe4\xbe\x9b\xe7\x94\xa8\xe6\x88\xb7\xe5\xbf\xab\xe9\x80\x9f\xe8\xb0\x83\xe7\x94\xa8\xef\xbc\x8c\xe8\xbf\x99\xe4\xba\x9b\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe4\xb8\xbb\xe8\xa6\x81\xe4\xbf\x9d\xe5\xad\x98\xe5\x9c\xa8torchvison\xe4\xb8\xad\xe3\x80\x82\n# \n# `torchvision`\xe5\xae\x9e\xe7\x8e\xb0\xe4\xba\x86\xe5\xb8\xb8\xe7\x94\xa8\xe7\x9a\x84\xe5\x9b\xbe\xe5\x83\x8f\xe6\x95\xb0\xe6\x8d\xae\xe5\x8a\xa0\xe8\xbd\xbd\xe5\x8a\x9f\xe8\x83\xbd\xef\xbc\x8c\xe4\xbe\x8b\xe5\xa6\x82Imagenet\xe3\x80\x81CIFAR10\xe3\x80\x81MNIST\xe7\xad\x89\xef\xbc\x8c\xe4\xbb\xa5\xe5\x8f\x8a\xe5\xb8\xb8\xe7\x94\xa8\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe8\xbd\xac\xe6\x8d\xa2\xe6\x93\x8d\xe4\xbd\x9c\xef\xbc\x8c\xe8\xbf\x99\xe6\x9e\x81\xe5\xa4\xa7\xe5\x9c\xb0\xe6\x96\xb9\xe4\xbe\xbf\xe4\xba\x86\xe6\x95\xb0\xe6\x8d\xae\xe5\x8a\xa0\xe8\xbd\xbd\xef\xbc\x8c\xe5\xb9\xb6\xe4\xb8\x94\xe4\xbb\xa3\xe7\xa0\x81\xe5\x85\xb7\xe6\x9c\x89\xe5\x8f\xaf\xe9\x87\x8d\xe7\x94\xa8\xe6\x80\xa7\xe3\x80\x82\n# \n# \n# ### \xe5\xb0\x8f\xe8\xaf\x95\xe7\x89\x9b\xe5\x88\x80\xef\xbc\x9aCIFAR-10\xe5\x88\x86\xe7\xb1\xbb\n# \n# \xe4\xb8\x8b\xe9\x9d\xa2\xe6\x88\x91\xe4\xbb\xac\xe6\x9d\xa5\xe5\xb0\x9d\xe8\xaf\x95\xe5\xae\x9e\xe7\x8e\xb0\xe5\xaf\xb9CIFAR-10\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe7\x9a\x84\xe5\x88\x86\xe7\xb1\xbb\xef\xbc\x8c\xe6\xad\xa5\xe9\xaa\xa4\xe5\xa6\x82\xe4\xb8\x8b: \n# \n# 1. \xe4\xbd\xbf\xe7\x94\xa8torchvision\xe5\x8a\xa0\xe8\xbd\xbd\xe5\xb9\xb6\xe9\xa2\x84\xe5\xa4\x84\xe7\x90\x86CIFAR-10\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\n# 2. \xe5\xae\x9a\xe4\xb9\x89\xe7\xbd\x91\xe7\xbb\x9c\n# 3. \xe5\xae\x9a\xe4\xb9\x89\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x87\xbd\xe6\x95\xb0\xe5\x92\x8c\xe4\xbc\x98\xe5\x8c\x96\xe5\x99\xa8\n# 4. \xe8\xae\xad\xe7\xbb\x83\xe7\xbd\x91\xe7\xbb\x9c\xe5\xb9\xb6\xe6\x9b\xb4\xe6\x96\xb0\xe7\xbd\x91\xe7\xbb\x9c\xe5\x8f\x82\xe6\x95\xb0\n# 5. \xe6\xb5\x8b\xe8\xaf\x95\xe7\xbd\x91\xe7\xbb\x9c\n# \n# ####   CIFAR-10\xe6\x95\xb0\xe6\x8d\xae\xe5\x8a\xa0\xe8\xbd\xbd\xe5\x8f\x8a\xe9\xa2\x84\xe5\xa4\x84\xe7\x90\x86\n# \n# CIFAR-10[^3]\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa\xe5\xb8\xb8\xe7\x94\xa8\xe7\x9a\x84\xe5\xbd\xa9\xe8\x89\xb2\xe5\x9b\xbe\xe7\x89\x87\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xef\xbc\x8c\xe5\xae\x83\xe6\x9c\x8910\xe4\xb8\xaa\xe7\xb1\xbb\xe5\x88\xab: \'airplane\', \'automobile\', \'bird\', \'cat\', \'deer\', \'dog\', \'frog\', \'horse\', \'ship\', \'truck\'\xe3\x80\x82\xe6\xaf\x8f\xe5\xbc\xa0\xe5\x9b\xbe\xe7\x89\x87\xe9\x83\xbd\xe6\x98\xaf$3\\times32\\times32$\xef\xbc\x8c\xe4\xb9\x9f\xe5\x8d\xb33-\xe9\x80\x9a\xe9\x81\x93\xe5\xbd\xa9\xe8\x89\xb2\xe5\x9b\xbe\xe7\x89\x87\xef\xbc\x8c\xe5\x88\x86\xe8\xbe\xa8\xe7\x8e\x87\xe4\xb8\xba$32\\times32$\xe3\x80\x82\n# \n# [^3]: http://www.cs.toronto.edu/~kriz/cifar.html\n\n#%%\nimport torchvision as tv\nimport torchvision.transforms as transforms\nfrom torchvision.transforms import ToPILImage\nshow = ToPILImage() # \xe5\x8f\xaf\xe4\xbb\xa5\xe6\x8a\x8aTensor\xe8\xbd\xac\xe6\x88\x90Image\xef\xbc\x8c\xe6\x96\xb9\xe4\xbe\xbf\xe5\x8f\xaf\xe8\xa7\x86\xe5\x8c\x96\n\n\n#%%\n# \xe7\xac\xac\xe4\xb8\x80\xe6\xac\xa1\xe8\xbf\x90\xe8\xa1\x8c\xe7\xa8\x8b\xe5\xba\x8ftorchvision\xe4\xbc\x9a\xe8\x87\xaa\xe5\x8a\xa8\xe4\xb8\x8b\xe8\xbd\xbdCIFAR-10\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xef\xbc\x8c\n# \xe5\xa4\xa7\xe7\xba\xa6100M\xef\xbc\x8c\xe9\x9c\x80\xe8\x8a\xb1\xe8\xb4\xb9\xe4\xb8\x80\xe5\xae\x9a\xe7\x9a\x84\xe6\x97\xb6\xe9\x97\xb4\xef\xbc\x8c\n# \xe5\xa6\x82\xe6\x9e\x9c\xe5\xb7\xb2\xe7\xbb\x8f\xe4\xb8\x8b\xe8\xbd\xbd\xe6\x9c\x89CIFAR-10\xef\xbc\x8c\xe5\x8f\xaf\xe9\x80\x9a\xe8\xbf\x87root\xe5\x8f\x82\xe6\x95\xb0\xe6\x8c\x87\xe5\xae\x9a\n\n# \xe5\xae\x9a\xe4\xb9\x89\xe5\xaf\xb9\xe6\x95\xb0\xe6\x8d\xae\xe7\x9a\x84\xe9\xa2\x84\xe5\xa4\x84\xe7\x90\x86\ntransform = transforms.Compose([\n        transforms.ToTensor(), # \xe8\xbd\xac\xe4\xb8\xbaTensor\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), # \xe5\xbd\x92\xe4\xb8\x80\xe5\x8c\x96\n                             ])\n\n# \xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\ntrainset = tv.datasets.CIFAR10(\n                    root=\'/home/cy/tmp/data/\', \n                    train=True, \n                    download=True,\n                    transform=transform)\n\ntrainloader = t.utils.data.DataLoader(\n                    trainset, \n                    batch_size=4,\n                    shuffle=True, \n                    num_workers=2)\n\n# \xe6\xb5\x8b\xe8\xaf\x95\xe9\x9b\x86\ntestset = tv.datasets.CIFAR10(\n                    \'/home/cy/tmp/data/\',\n                    train=False, \n                    download=True, \n                    transform=transform)\n\ntestloader = t.utils.data.DataLoader(\n                    testset,\n                    batch_size=4, \n                    shuffle=False,\n                    num_workers=2)\n\nclasses = (\'plane\', \'car\', \'bird\', \'cat\',\n           \'deer\', \'dog\', \'frog\', \'horse\', \'ship\', \'truck\')\n\n#%% [markdown]\n# Dataset\xe5\xaf\xb9\xe8\xb1\xa1\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xef\xbc\x8c\xe5\x8f\xaf\xe4\xbb\xa5\xe6\x8c\x89\xe4\xb8\x8b\xe6\xa0\x87\xe8\xae\xbf\xe9\x97\xae\xef\xbc\x8c\xe8\xbf\x94\xe5\x9b\x9e\xe5\xbd\xa2\xe5\xa6\x82(data, label)\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe3\x80\x82\n\n#%%\n(data, label) = trainset[100]\nprint(classes[label])\n\n# (data + 1) / 2\xe6\x98\xaf\xe4\xb8\xba\xe4\xba\x86\xe8\xbf\x98\xe5\x8e\x9f\xe8\xa2\xab\xe5\xbd\x92\xe4\xb8\x80\xe5\x8c\x96\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\nshow((data + 1) / 2).resize((100, 100))\n\n#%% [markdown]\n# Dataloader\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa\xe5\x8f\xaf\xe8\xbf\xad\xe4\xbb\xa3\xe7\x9a\x84\xe5\xaf\xb9\xe8\xb1\xa1\xef\xbc\x8c\xe5\xae\x83\xe5\xb0\x86dataset\xe8\xbf\x94\xe5\x9b\x9e\xe7\x9a\x84\xe6\xaf\x8f\xe4\xb8\x80\xe6\x9d\xa1\xe6\x95\xb0\xe6\x8d\xae\xe6\x8b\xbc\xe6\x8e\xa5\xe6\x88\x90\xe4\xb8\x80\xe4\xb8\xaabatch\xef\xbc\x8c\xe5\xb9\xb6\xe6\x8f\x90\xe4\xbe\x9b\xe5\xa4\x9a\xe7\xba\xbf\xe7\xa8\x8b\xe5\x8a\xa0\xe9\x80\x9f\xe4\xbc\x98\xe5\x8c\x96\xe5\x92\x8c\xe6\x95\xb0\xe6\x8d\xae\xe6\x89\x93\xe4\xb9\xb1\xe7\xad\x89\xe6\x93\x8d\xe4\xbd\x9c\xe3\x80\x82\xe5\xbd\x93\xe7\xa8\x8b\xe5\xba\x8f\xe5\xaf\xb9dataset\xe7\x9a\x84\xe6\x89\x80\xe6\x9c\x89\xe6\x95\xb0\xe6\x8d\xae\xe9\x81\x8d\xe5\x8e\x86\xe5\xae\x8c\xe4\xb8\x80\xe9\x81\x8d\xe4\xb9\x8b\xe5\x90\x8e\xef\xbc\x8c\xe7\x9b\xb8\xe5\xba\x94\xe7\x9a\x84\xe5\xaf\xb9Dataloader\xe4\xb9\x9f\xe5\xae\x8c\xe6\x88\x90\xe4\xba\x86\xe4\xb8\x80\xe6\xac\xa1\xe8\xbf\xad\xe4\xbb\xa3\xe3\x80\x82\n\n#%%\ndataiter = iter(trainloader)\nimages, labels = dataiter.next() # \xe8\xbf\x94\xe5\x9b\x9e4\xe5\xbc\xa0\xe5\x9b\xbe\xe7\x89\x87\xe5\x8f\x8a\xe6\xa0\x87\xe7\xad\xbe\nprint(\' \'.join(\'%11s\'%classes[labels[j]] for j in range(4)))\nshow(tv.utils.make_grid((images+1)/2)).resize((400,100))\n\n#%% [markdown]\n# ####   \xe5\xae\x9a\xe4\xb9\x89\xe7\xbd\x91\xe7\xbb\x9c\n# \n# \xe6\x8b\xb7\xe8\xb4\x9d\xe4\xb8\x8a\xe9\x9d\xa2\xe7\x9a\x84LeNet\xe7\xbd\x91\xe7\xbb\x9c\xef\xbc\x8c\xe4\xbf\xae\xe6\x94\xb9self.conv1\xe7\xac\xac\xe4\xb8\x80\xe4\xb8\xaa\xe5\x8f\x82\xe6\x95\xb0\xe4\xb8\xba3\xe9\x80\x9a\xe9\x81\x93\xef\xbc\x8c\xe5\x9b\xa0CIFAR-10\xe6\x98\xaf3\xe9\x80\x9a\xe9\x81\x93\xe5\xbd\xa9\xe5\x9b\xbe\xe3\x80\x82\n\n#%%\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5) \n        self.conv2 = nn.Conv2d(6, 16, 5)  \n        self.fc1   = nn.Linear(16*5*5, 120)  \n        self.fc2   = nn.Linear(120, 84)\n        self.fc3   = nn.Linear(84, 10)\n\n    def forward(self, x): \n        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2)) \n        x = F.max_pool2d(F.relu(self.conv2(x)), 2) \n        x = x.view(x.size()[0], -1) \n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)        \n        return x\n\n\nnet = Net()\nprint(net)\n\n#%% [markdown]\n# ####  \xe5\xae\x9a\xe4\xb9\x89\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x87\xbd\xe6\x95\xb0\xe5\x92\x8c\xe4\xbc\x98\xe5\x8c\x96\xe5\x99\xa8(loss\xe5\x92\x8coptimizer)\n\n#%%\nfrom torch import optim\ncriterion = nn.CrossEntropyLoss() # \xe4\xba\xa4\xe5\x8f\x89\xe7\x86\xb5\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x87\xbd\xe6\x95\xb0\noptimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n\n#%% [markdown]\n# ###   \xe8\xae\xad\xe7\xbb\x83\xe7\xbd\x91\xe7\xbb\x9c\n# \n# \xe6\x89\x80\xe6\x9c\x89\xe7\xbd\x91\xe7\xbb\x9c\xe7\x9a\x84\xe8\xae\xad\xe7\xbb\x83\xe6\xb5\x81\xe7\xa8\x8b\xe9\x83\xbd\xe6\x98\xaf\xe7\xb1\xbb\xe4\xbc\xbc\xe7\x9a\x84\xef\xbc\x8c\xe4\xb8\x8d\xe6\x96\xad\xe5\x9c\xb0\xe6\x89\xa7\xe8\xa1\x8c\xe5\xa6\x82\xe4\xb8\x8b\xe6\xb5\x81\xe7\xa8\x8b\xef\xbc\x9a\n# \n# - \xe8\xbe\x93\xe5\x85\xa5\xe6\x95\xb0\xe6\x8d\xae\n# - \xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad+\xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\n# - \xe6\x9b\xb4\xe6\x96\xb0\xe5\x8f\x82\xe6\x95\xb0\n# \n\n#%%\nt.set_num_threads(8)\nfor epoch in range(2):  \n    \n    running_loss = 0.0\n    for i, data in enumerate(trainloader, 0):\n        \n        # \xe8\xbe\x93\xe5\x85\xa5\xe6\x95\xb0\xe6\x8d\xae\n        inputs, labels = data\n        \n        # \xe6\xa2\xaf\xe5\xba\xa6\xe6\xb8\x85\xe9\x9b\xb6\n        optimizer.zero_grad()\n        \n        # forward + backward \n        outputs = net(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()   \n        \n        # \xe6\x9b\xb4\xe6\x96\xb0\xe5\x8f\x82\xe6\x95\xb0 \n        optimizer.step()\n        \n        # \xe6\x89\x93\xe5\x8d\xb0log\xe4\xbf\xa1\xe6\x81\xaf\n        # loss \xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaascalar,\xe9\x9c\x80\xe8\xa6\x81\xe4\xbd\xbf\xe7\x94\xa8loss.item()\xe6\x9d\xa5\xe8\x8e\xb7\xe5\x8f\x96\xe6\x95\xb0\xe5\x80\xbc\xef\xbc\x8c\xe4\xb8\x8d\xe8\x83\xbd\xe4\xbd\xbf\xe7\x94\xa8loss[0]\n        running_loss += loss.item()\n        if i % 2000 == 1999: # \xe6\xaf\x8f2000\xe4\xb8\xaabatch\xe6\x89\x93\xe5\x8d\xb0\xe4\xb8\x80\xe4\xb8\x8b\xe8\xae\xad\xe7\xbb\x83\xe7\x8a\xb6\xe6\x80\x81\n            print(\'[%d, %5d] loss: %.3f\'                   % (epoch+1, i+1, running_loss / 2000))\n            running_loss = 0.0\nprint(\'Finished Training\')\n\n#%% [markdown]\n# \xe6\xad\xa4\xe5\xa4\x84\xe4\xbb\x85\xe8\xae\xad\xe7\xbb\x83\xe4\xba\x862\xe4\xb8\xaaepoch\xef\xbc\x88\xe9\x81\x8d\xe5\x8e\x86\xe5\xae\x8c\xe4\xb8\x80\xe9\x81\x8d\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe7\xa7\xb0\xe4\xb8\xba\xe4\xb8\x80\xe4\xb8\xaaepoch\xef\xbc\x89\xef\xbc\x8c\xe6\x9d\xa5\xe7\x9c\x8b\xe7\x9c\x8b\xe7\xbd\x91\xe7\xbb\x9c\xe6\x9c\x89\xe6\xb2\xa1\xe6\x9c\x89\xe6\x95\x88\xe6\x9e\x9c\xe3\x80\x82\xe5\xb0\x86\xe6\xb5\x8b\xe8\xaf\x95\xe5\x9b\xbe\xe7\x89\x87\xe8\xbe\x93\xe5\x85\xa5\xe5\x88\xb0\xe7\xbd\x91\xe7\xbb\x9c\xe4\xb8\xad\xef\xbc\x8c\xe8\xae\xa1\xe7\xae\x97\xe5\xae\x83\xe7\x9a\x84label\xef\xbc\x8c\xe7\x84\xb6\xe5\x90\x8e\xe4\xb8\x8e\xe5\xae\x9e\xe9\x99\x85\xe7\x9a\x84label\xe8\xbf\x9b\xe8\xa1\x8c\xe6\xaf\x94\xe8\xbe\x83\xe3\x80\x82\n\n#%%\ndataiter = iter(testloader)\nimages, labels = dataiter.next() # \xe4\xb8\x80\xe4\xb8\xaabatch\xe8\xbf\x94\xe5\x9b\x9e4\xe5\xbc\xa0\xe5\x9b\xbe\xe7\x89\x87\nprint(\'\xe5\xae\x9e\xe9\x99\x85\xe7\x9a\x84label: \', \' \'.join(            \'%08s\'%classes[labels[j]] for j in range(4)))\nshow(tv.utils.make_grid(images / 2 - 0.5)).resize((400,100))\n\n#%% [markdown]\n# \xe6\x8e\xa5\xe7\x9d\x80\xe8\xae\xa1\xe7\xae\x97\xe7\xbd\x91\xe7\xbb\x9c\xe9\xa2\x84\xe6\xb5\x8b\xe7\x9a\x84label\xef\xbc\x9a\n\n#%%\n# \xe8\xae\xa1\xe7\xae\x97\xe5\x9b\xbe\xe7\x89\x87\xe5\x9c\xa8\xe6\xaf\x8f\xe4\xb8\xaa\xe7\xb1\xbb\xe5\x88\xab\xe4\xb8\x8a\xe7\x9a\x84\xe5\x88\x86\xe6\x95\xb0\noutputs = net(images)\n# \xe5\xbe\x97\xe5\x88\x86\xe6\x9c\x80\xe9\xab\x98\xe7\x9a\x84\xe9\x82\xa3\xe4\xb8\xaa\xe7\xb1\xbb\n_, predicted = t.max(outputs.data, 1)\n\nprint(\'\xe9\xa2\x84\xe6\xb5\x8b\xe7\xbb\x93\xe6\x9e\x9c: \', \' \'.join(\'%5s\'            % classes[predicted[j]] for j in range(4)))\n\n#%% [markdown]\n# \xe5\xb7\xb2\xe7\xbb\x8f\xe5\x8f\xaf\xe4\xbb\xa5\xe7\x9c\x8b\xe5\x87\xba\xe6\x95\x88\xe6\x9e\x9c\xef\xbc\x8c\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x8750%\xef\xbc\x8c\xe4\xbd\x86\xe8\xbf\x99\xe5\x8f\xaa\xe6\x98\xaf\xe4\xb8\x80\xe9\x83\xa8\xe5\x88\x86\xe7\x9a\x84\xe5\x9b\xbe\xe7\x89\x87\xef\xbc\x8c\xe5\x86\x8d\xe6\x9d\xa5\xe7\x9c\x8b\xe7\x9c\x8b\xe5\x9c\xa8\xe6\x95\xb4\xe4\xb8\xaa\xe6\xb5\x8b\xe8\xaf\x95\xe9\x9b\x86\xe4\xb8\x8a\xe7\x9a\x84\xe6\x95\x88\xe6\x9e\x9c\xe3\x80\x82\n\n#%%\ncorrect = 0 # \xe9\xa2\x84\xe6\xb5\x8b\xe6\xad\xa3\xe7\xa1\xae\xe7\x9a\x84\xe5\x9b\xbe\xe7\x89\x87\xe6\x95\xb0\ntotal = 0 # \xe6\x80\xbb\xe5\x85\xb1\xe7\x9a\x84\xe5\x9b\xbe\xe7\x89\x87\xe6\x95\xb0\n\n\n# \xe7\x94\xb1\xe4\xba\x8e\xe6\xb5\x8b\xe8\xaf\x95\xe7\x9a\x84\xe6\x97\xb6\xe5\x80\x99\xe4\xb8\x8d\xe9\x9c\x80\xe8\xa6\x81\xe6\xb1\x82\xe5\xaf\xbc\xef\xbc\x8c\xe5\x8f\xaf\xe4\xbb\xa5\xe6\x9a\x82\xe6\x97\xb6\xe5\x85\xb3\xe9\x97\xadautograd\xef\xbc\x8c\xe6\x8f\x90\xe9\xab\x98\xe9\x80\x9f\xe5\xba\xa6\xef\xbc\x8c\xe8\x8a\x82\xe7\xba\xa6\xe5\x86\x85\xe5\xad\x98\nwith t.no_grad():\n    for data in testloader:\n        images, labels = data\n        outputs = net(images)\n        _, predicted = t.max(outputs, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum()\n\nprint(\'10000\xe5\xbc\xa0\xe6\xb5\x8b\xe8\xaf\x95\xe9\x9b\x86\xe4\xb8\xad\xe7\x9a\x84\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87\xe4\xb8\xba: %d %%\' % (100 * correct / total))\n\n#%% [markdown]\n# \xe8\xae\xad\xe7\xbb\x83\xe7\x9a\x84\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87\xe8\xbf\x9c\xe6\xaf\x94\xe9\x9a\x8f\xe6\x9c\xba\xe7\x8c\x9c\xe6\xb5\x8b(\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x8710%)\xe5\xa5\xbd\xef\xbc\x8c\xe8\xaf\x81\xe6\x98\x8e\xe7\xbd\x91\xe7\xbb\x9c\xe7\xa1\xae\xe5\xae\x9e\xe5\xad\xa6\xe5\x88\xb0\xe4\xba\x86\xe4\xb8\x9c\xe8\xa5\xbf\xe3\x80\x82\n#%% [markdown]\n# ####  \xe5\x9c\xa8GPU\xe8\xae\xad\xe7\xbb\x83\n# \xe5\xb0\xb1\xe5\x83\x8f\xe4\xb9\x8b\xe5\x89\x8d\xe6\x8a\x8aTensor\xe4\xbb\x8eCPU\xe8\xbd\xac\xe5\x88\xb0GPU\xe4\xb8\x80\xe6\xa0\xb7\xef\xbc\x8c\xe6\xa8\xa1\xe5\x9e\x8b\xe4\xb9\x9f\xe5\x8f\xaf\xe4\xbb\xa5\xe7\xb1\xbb\xe4\xbc\xbc\xe5\x9c\xb0\xe4\xbb\x8eCPU\xe8\xbd\xac\xe5\x88\xb0GPU\xe3\x80\x82\n\n#%%\ndevice = t.device(""cuda:0"" if t.cuda.is_available() else ""cpu"")\n\nnet.to(device)\nimages = images.to(device)\nlabels = labels.to(device)\noutput = net(images)\nloss= criterion(output,labels)\n\nloss\n\n#%% [markdown]\n# \xe5\xa6\x82\xe6\x9e\x9c\xe5\x8f\x91\xe7\x8e\xb0\xe5\x9c\xa8GPU\xe4\xb8\x8a\xe5\xb9\xb6\xe6\xb2\xa1\xe6\x9c\x89\xe6\xaf\x94CPU\xe6\x8f\x90\xe9\x80\x9f\xe5\xbe\x88\xe5\xa4\x9a\xef\xbc\x8c\xe5\xae\x9e\xe9\x99\x85\xe4\xb8\x8a\xe6\x98\xaf\xe5\x9b\xa0\xe4\xb8\xba\xe7\xbd\x91\xe7\xbb\x9c\xe6\xaf\x94\xe8\xbe\x83\xe5\xb0\x8f\xef\xbc\x8cGPU\xe6\xb2\xa1\xe6\x9c\x89\xe5\xae\x8c\xe5\x85\xa8\xe5\x8f\x91\xe6\x8c\xa5\xe8\x87\xaa\xe5\xb7\xb1\xe7\x9a\x84\xe7\x9c\x9f\xe6\xad\xa3\xe5\xae\x9e\xe5\x8a\x9b\xe3\x80\x82\n#%% [markdown]\n# \xe5\xaf\xb9PyTorch\xe7\x9a\x84\xe5\x9f\xba\xe7\xa1\x80\xe4\xbb\x8b\xe7\xbb\x8d\xe8\x87\xb3\xe6\xad\xa4\xe7\xbb\x93\xe6\x9d\x9f\xe3\x80\x82\xe6\x80\xbb\xe7\xbb\x93\xe4\xb8\x80\xe4\xb8\x8b\xef\xbc\x8c\xe6\x9c\xac\xe8\x8a\x82\xe4\xb8\xbb\xe8\xa6\x81\xe5\x8c\x85\xe5\x90\xab\xe4\xbb\xa5\xe4\xb8\x8b\xe5\x86\x85\xe5\xae\xb9\xe3\x80\x82\n# \n# 1. Tensor: \xe7\xb1\xbb\xe4\xbc\xbcNumpy\xe6\x95\xb0\xe7\xbb\x84\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe7\xbb\x93\xe6\x9e\x84\xef\xbc\x8c\xe4\xb8\x8eNumpy\xe6\x8e\xa5\xe5\x8f\xa3\xe7\xb1\xbb\xe4\xbc\xbc\xef\xbc\x8c\xe5\x8f\xaf\xe6\x96\xb9\xe4\xbe\xbf\xe5\x9c\xb0\xe4\xba\x92\xe7\x9b\xb8\xe8\xbd\xac\xe6\x8d\xa2\xe3\x80\x82\n# 2. autograd/: \xe4\xb8\xbatensor\xe6\x8f\x90\xe4\xbe\x9b\xe8\x87\xaa\xe5\x8a\xa8\xe6\xb1\x82\xe5\xaf\xbc\xe5\x8a\x9f\xe8\x83\xbd\xe3\x80\x82\n# 3. nn: \xe4\xb8\x93\xe9\x97\xa8\xe4\xb8\xba\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xe8\xae\xbe\xe8\xae\xa1\xe7\x9a\x84\xe6\x8e\xa5\xe5\x8f\xa3\xef\xbc\x8c\xe6\x8f\x90\xe4\xbe\x9b\xe4\xba\x86\xe5\xbe\x88\xe5\xa4\x9a\xe6\x9c\x89\xe7\x94\xa8\xe7\x9a\x84\xe5\x8a\x9f\xe8\x83\xbd(\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xe5\xb1\x82\xef\xbc\x8c\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x87\xbd\xe6\x95\xb0\xef\xbc\x8c\xe4\xbc\x98\xe5\x8c\x96\xe5\x99\xa8\xe7\xad\x89)\xe3\x80\x82\n# 4. \xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xe8\xae\xad\xe7\xbb\x83: \xe4\xbb\xa5CIFAR-10\xe5\x88\x86\xe7\xb1\xbb\xe4\xb8\xba\xe4\xbe\x8b\xe6\xbc\x94\xe7\xa4\xba\xe4\xba\x86\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xe7\x9a\x84\xe8\xae\xad\xe7\xbb\x83\xe6\xb5\x81\xe7\xa8\x8b\xef\xbc\x8c\xe5\x8c\x85\xe6\x8b\xac\xe6\x95\xb0\xe6\x8d\xae\xe5\x8a\xa0\xe8\xbd\xbd\xe3\x80\x81\xe7\xbd\x91\xe7\xbb\x9c\xe6\x90\xad\xe5\xbb\xba\xe3\x80\x81\xe8\xae\xad\xe7\xbb\x83\xe5\x8f\x8a\xe6\xb5\x8b\xe8\xaf\x95\xe3\x80\x82\n# \n# \xe9\x80\x9a\xe8\xbf\x87\xe6\x9c\xac\xe8\x8a\x82\xe7\x9a\x84\xe5\xad\xa6\xe4\xb9\xa0\xef\xbc\x8c\xe7\x9b\xb8\xe4\xbf\xa1\xe8\xaf\xbb\xe8\x80\x85\xe5\x8f\xaf\xe4\xbb\xa5\xe4\xbd\x93\xe4\xbc\x9a\xe5\x87\xbaPyTorch\xe5\x85\xb7\xe6\x9c\x89\xe6\x8e\xa5\xe5\x8f\xa3\xe7\xae\x80\xe5\x8d\x95\xe3\x80\x81\xe4\xbd\xbf\xe7\x94\xa8\xe7\x81\xb5\xe6\xb4\xbb\xe7\xad\x89\xe7\x89\xb9\xe7\x82\xb9\xe3\x80\x82\xe4\xbb\x8e\xe4\xb8\x8b\xe4\xb8\x80\xe7\xab\xa0\xe5\xbc\x80\xe5\xa7\x8b\xef\xbc\x8c\xe6\x9c\xac\xe4\xb9\xa6\xe5\xb0\x86\xe6\xb7\xb1\xe5\x85\xa5\xe7\xb3\xbb\xe7\xbb\x9f\xe5\x9c\xb0\xe8\xae\xb2\xe8\xa7\xa3PyTorch\xe7\x9a\x84\xe5\x90\x84\xe9\x83\xa8\xe5\x88\x86\xe7\x9f\xa5\xe8\xaf\x86\xe3\x80\x82\n\n'"
2.PyTorch _basics PyTorch基础/PyTorch-tensor-basic-usage-oldversion.py,101,"b'#%% [markdown]\n# # PyTorch Tensor Basic Usage\n# \n# - Create Tensor\n# - Indexing,Joining,Slicing\n# - Initialization\n# - Math Operations\n#%% [markdown]\n# ## 1. Create Tensor\n# ### 1) random numbers\n\n#%%\nimport torch\n\n# torch.rand(sizes) -> [0,1)\nx = torch.rand(2,3)\nx\n\n\n#%%\n# torch.randn(sizes) -> Z(0,1)\nx = torch.randn(2,3)\nx\n\n\n#%%\n# torch.randperm(n) -> permutation of 0~n\nx = torch.randperm(5)\nx\n\n#%% [markdown]\n# ### 2) zeros, ones, arange\n\n#%%\n# torch.zeros(2,3) -> [[0,0,0],[0,0,0]]\nx = torch.zeros(2,3)\nx\n\n\n#%%\n# torch.ones(2,3) -> [[0,0,0],[0,0,0]]\nx = torch.ones(2,3)\nx\n\n\n#%%\n# torch.arange(start,end,step=1) -> [start,end) with step\nx = torch.arange(0,3,step=0.5)\nx\n\n#%% [markdown]\n# ### 3) Tensor Data Type\n\n#%%\n# torch.FloatTensor(size or list)\nx = torch.FloatTensor(2,3)\nx\n\n\n#%%\n# torch.FloatTensor(size or list)\nx = torch.FloatTensor([2,3])\nx\n\n\n#%%\n# tensor.type_as(tensor_type)\nx = x.type_as(torch.IntTensor())\nx\n\n#%% [markdown]\n# ### 4) Numpy to Tensor, Tensor to Numpy\n\n#%%\nimport numpy as np\n\n# torch.from_numpy(ndarray) -> tensor\n\nx1 = np.ndarray(shape=(2,3), dtype=int,buffer=np.array([1,2,3,4,5,6]))\nx2 = torch.from_numpy(x1)\n\nx2\n\n\n#%%\n# tensor.numpy() -> ndarray\nx3 = x2.numpy()\nx3\n\n#%% [markdown]\n# ### 5) Tensor on CPU & GPU\n\n#%%\nx = torch.FloatTensor([[1,2,3],[4,5,6]])\nx\n\n\n#%%\nx_gpu = x.cuda()\nx_gpu\n\n\n#%%\nx_cpu = x_gpu.cpu()\nx_cpu\n\n#%% [markdown]\n# ### 6) Tensor Size\n\n#%%\n# tensor.size() -> indexing also possible\n\nx = torch.FloatTensor(10,12,3,3)\n\nx.size()[:]\n\n#%% [markdown]\n# ## 2. Indexing, Slicing, Joining, Reshaping\n# ### 1) Indexing\n\n#%%\n# torch.index_select(input, dim, index)\n\nx = torch.rand(4,3)\nout = torch.index_select(x,0,torch.LongTensor([0,3]))\n\nx,out\n\n\n#%%\n# pythonic indexing also works\n\nx[:,0],x[0,:],x[0:2,0:2]\n\n\n#%%\n# torch.masked_select(input, mask)\n\nx = torch.randn(2,3)\nmask = torch.ByteTensor([[0,0,1],[0,1,0]])\nout = torch.masked_select(x,mask)\n\nx, mask, out\n\n#%% [markdown]\n# ### 2) Joining\n\n#%%\n# torch.cat(seq, dim=0) -> concatenate tensor along dim\n\nx = torch.FloatTensor([[1,2,3],[4,5,6]])\ny = torch.FloatTensor([[-1,-2,-3],[-4,-5,-6]])\nz1 = torch.cat([x,y],dim=0)\nz2 = torch.cat([x,y],dim=1)\n\nx,y,z1,z2\n\n\n#%%\n# torch.stack(sequence,dim=0) -> stack along new dim\n\nx = torch.FloatTensor([[1,2,3],[4,5,6]])\nx_stack = torch.stack([x,x,x,x],dim=0)\n\nx_stack\n\n#%% [markdown]\n# ### 3) Slicing\n\n#%%\n# torch.chunk(tensor, chunks, dim=0) -> tensor into num chunks\n\nx_1, x_2 = torch.chunk(z1,2,dim=0)\ny_1, y_2, y_3 = torch.chunk(z1,3,dim=1)\n\nz1,x_1,x_2,z1,y_1,y_2,y_3\n\n\n#%%\n# torch.split(tensor,split_size,dim=0) -> split into specific size\n\nx1,x2 = torch.split(z1,2,dim=0)\ny1 = torch.split(z1,2,dim=1) \n\nz1,x1,x2,y1\n\n#%% [markdown]\n# ### 4) squeezing\n\n#%%\n# torch.squeeze(input,dim=None) -> reduce dim by 1\n\nx1 = torch.FloatTensor(10,1,3,1,4)\nx2 = torch.squeeze(x1)\n\nx1.size(),x2.size()\n\n\n#%%\n# torch.unsqueeze(input,dim=None) -> add dim by 1\n\nx1 = torch.FloatTensor(10,3,4)\nx2 = torch.unsqueeze(x1,dim=0)\n\nx1.size(),x2.size()\n\n#%% [markdown]\n# ### 5) Reshaping\n\n#%%\n# tensor.view(size)\n\nx1 = torch.FloatTensor(10,3,4)\nx2 = x1.view(-1)\nx3 = x1.view(5,-1)\nx4 = x1.view(3,10,-1)\n\nx1.size(), x2.size(), x3.size(), x4.size()\n\n#%% [markdown]\n# ## 3. Initialization\n\n#%%\nimport torch.nn.init as init\n\nx1 = init.uniform(torch.FloatTensor(3,4),a=0,b=9) \nx2 = init.normal(torch.FloatTensor(3,4),std=0.2)\nx3 = init.constant(torch.FloatTensor(3,4),3.1415)\n\nx1,x2,x3\n\n#%% [markdown]\n# ## 4. Math Operations\n# ### 1) Arithmetic operations\n\n#%%\n# torch.add()\n\nx1 = torch.FloatTensor([[1,2,3],[4,5,6]])\nx2 = torch.FloatTensor([[1,2,3],[4,5,6]])\nadd = torch.add(x1,x2)\n\nx1,x2,add,x1+x2,x1-x2\n\n\n#%%\n# torch.add() broadcasting\n\nx1 = torch.FloatTensor([[1,2,3],[4,5,6]])\nx2 = torch.add(x1,10)\n\nx1,x2,x1+10,x2-10\n\n\n#%%\n# torch.mul() -> size better match\n\nx1 = torch.FloatTensor([[1,2,3],[4,5,6]])\nx2 = torch.FloatTensor([[1,2,3],[4,5,6]])\nx3 = torch.mul(x1,x2)\n\nx3\n\n\n#%%\n# torch.mul() -> broadcasting\n\nx1 = torch.FloatTensor([[1,2,3],[4,5,6]])\nx2 = x1*10\n\nx2\n\n\n#%%\n# torch.div() -> size better match\n\nx1 = torch.FloatTensor([[1,2,3],[4,5,6]])\nx2 = torch.FloatTensor([[1,2,3],[4,5,6]])\nx3 = torch.div(x1,x2)\n\nx3\n\n\n#%%\n# torch.div() -> broadcasting\n\nx1 = torch.FloatTensor([[1,2,3],[4,5,6]])\n\nx1/5\n\n#%% [markdown]\n# ### 2) Other Math Operations\n\n#%%\n# torch.pow(input,exponent)\n\nx1 = torch.FloatTensor(3,4)\ntorch.pow(x1,2),x1**2\n\n\n#%%\n# torch.exp(tensor,out=None) \n\nx1 = torch.FloatTensor(3,4)\ntorch.exp(x1)\n\n\n#%%\n# torch.log(input, out=None) -> natural logarithm\n\nx1 = torch.FloatTensor(3,4)\ntorch.log(x1)\n\n#%% [markdown]\n# ### 3) Matrix operations\n\n#%%\n# torch.mm(mat1, mat2) -> matrix multiplication\n\nx1 = torch.FloatTensor(3,4)\nx2 = torch.FloatTensor(4,5)\n\ntorch.mm(x1,x2)\n\n\n#%%\n# torch.bmm(batch1, batch2) -> batch matrix multiplication\n\nx1 = torch.FloatTensor(10,3,4)\nx2 = torch.FloatTensor(10,4,5)\n\ntorch.bmm(x1,x2).size()\n\n\n#%%\n# torch.dot(tensor1,tensor2) -> dot product of two tensor\n\nx1 = torch.FloatTensor(3,4)\nx2 = torch.FloatTensor(3,4)\n\ntorch.dot(x1,x2)\n\n\n#%%\n# torch.t(matrix) -> transposed matrix\n\nx1 = torch.FloatTensor(3,4)\n\nx1,x1.t()\n\n\n#%%\n# torch.transpose(input,dim0,dim1) -> transposed matrix\n\nx1 = torch.FloatTensor(10,3,4)\n\nx1.size(), torch.transpose(x1,1,2).size(), x1.transpose(1,2).size()\n\n\n#%%\n# torch.eig(a,eigenvectors=False) -> eigen_value, eigen_vector\n\nx1 = torch.FloatTensor(4,4)\n\nx1,torch.eig(x1,True)\n\n\n'"
2.PyTorch _basics PyTorch基础/Tensor.py,19,"b'#%% [markdown]\n# # \xe7\xac\xac\xe4\xb8\x89\xe7\xab\xa0 PyTorch\xe5\x9f\xba\xe7\xa1\x80\xef\xbc\x9aTensor\xe5\x92\x8cAutograd\n# \n# ## 3.1 Tensor\n# \n# Tensor\xef\xbc\x8c\xe5\x8f\x88\xe5\x90\x8d\xe5\xbc\xa0\xe9\x87\x8f\xef\xbc\x8c\xe8\xaf\xbb\xe8\x80\x85\xe5\x8f\xaf\xe8\x83\xbd\xe5\xaf\xb9\xe8\xbf\x99\xe4\xb8\xaa\xe5\x90\x8d\xe8\xaf\x8d\xe4\xbc\xbc\xe6\x9b\xbe\xe7\x9b\xb8\xe8\xaf\x86\xef\xbc\x8c\xe5\x9b\xa0\xe5\xae\x83\xe4\xb8\x8d\xe4\xbb\x85\xe5\x9c\xa8PyTorch\xe4\xb8\xad\xe5\x87\xba\xe7\x8e\xb0\xe8\xbf\x87\xef\xbc\x8c\xe5\xae\x83\xe4\xb9\x9f\xe6\x98\xafTheano\xe3\x80\x81TensorFlow\xe3\x80\x81\n# Torch\xe5\x92\x8cMxNet\xe4\xb8\xad\xe9\x87\x8d\xe8\xa6\x81\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe7\xbb\x93\xe6\x9e\x84\xe3\x80\x82\xe5\x85\xb3\xe4\xba\x8e\xe5\xbc\xa0\xe9\x87\x8f\xe7\x9a\x84\xe6\x9c\xac\xe8\xb4\xa8\xe4\xb8\x8d\xe4\xb9\x8f\xe6\xb7\xb1\xe5\xba\xa6\xe7\x9a\x84\xe5\x89\x96\xe6\x9e\x90\xef\xbc\x8c\xe4\xbd\x86\xe4\xbb\x8e\xe5\xb7\xa5\xe7\xa8\x8b\xe8\xa7\x92\xe5\xba\xa6\xe6\x9d\xa5\xe8\xae\xb2\xef\xbc\x8c\xe5\x8f\xaf\xe7\xae\x80\xe5\x8d\x95\xe5\x9c\xb0\xe8\xae\xa4\xe4\xb8\xba\xe5\xae\x83\xe5\xb0\xb1\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa\xe6\x95\xb0\xe7\xbb\x84\xef\xbc\x8c\xe4\xb8\x94\xe6\x94\xaf\xe6\x8c\x81\xe9\xab\x98\xe6\x95\x88\xe7\x9a\x84\xe7\xa7\x91\xe5\xad\xa6\xe8\xae\xa1\xe7\xae\x97\xe3\x80\x82\xe5\xae\x83\xe5\x8f\xaf\xe4\xbb\xa5\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa\xe6\x95\xb0\xef\xbc\x88\xe6\xa0\x87\xe9\x87\x8f\xef\xbc\x89\xe3\x80\x81\xe4\xb8\x80\xe7\xbb\xb4\xe6\x95\xb0\xe7\xbb\x84\xef\xbc\x88\xe5\x90\x91\xe9\x87\x8f\xef\xbc\x89\xe3\x80\x81\xe4\xba\x8c\xe7\xbb\xb4\xe6\x95\xb0\xe7\xbb\x84\xef\xbc\x88\xe7\x9f\xa9\xe9\x98\xb5\xef\xbc\x89\xe5\x92\x8c\xe6\x9b\xb4\xe9\xab\x98\xe7\xbb\xb4\xe7\x9a\x84\xe6\x95\xb0\xe7\xbb\x84\xef\xbc\x88\xe9\xab\x98\xe9\x98\xb6\xe6\x95\xb0\xe6\x8d\xae\xef\xbc\x89\xe3\x80\x82Tensor\xe5\x92\x8cNumpy\xe7\x9a\x84ndarrays\xe7\xb1\xbb\xe4\xbc\xbc\xef\xbc\x8c\xe4\xbd\x86PyTorch\xe7\x9a\x84tensor\xe6\x94\xaf\xe6\x8c\x81GPU\xe5\x8a\xa0\xe9\x80\x9f\xe3\x80\x82\n# \n# \xe6\x9c\xac\xe8\x8a\x82\xe5\xb0\x86\xe7\xb3\xbb\xe7\xbb\x9f\xe8\xae\xb2\xe8\xa7\xa3tensor\xe7\x9a\x84\xe4\xbd\xbf\xe7\x94\xa8\xef\xbc\x8c\xe5\x8a\x9b\xe6\xb1\x82\xe9\x9d\xa2\xe9\x9d\xa2\xe4\xbf\xb1\xe5\x88\xb0\xef\xbc\x8c\xe4\xbd\x86\xe4\xb8\x8d\xe4\xbc\x9a\xe6\xb6\x89\xe5\x8f\x8a\xe6\xaf\x8f\xe4\xb8\xaa\xe5\x87\xbd\xe6\x95\xb0\xe3\x80\x82\xe5\xaf\xb9\xe4\xba\x8e\xe6\x9b\xb4\xe5\xa4\x9a\xe5\x87\xbd\xe6\x95\xb0\xe5\x8f\x8a\xe5\x85\xb6\xe7\x94\xa8\xe6\xb3\x95\xef\xbc\x8c\xe8\xaf\xbb\xe8\x80\x85\xe5\x8f\xaf\xe9\x80\x9a\xe8\xbf\x87\xe5\x9c\xa8IPython/Notebook\xe4\xb8\xad\xe4\xbd\xbf\xe7\x94\xa8\xe5\x87\xbd\xe6\x95\xb0\xe5\x90\x8d\xe5\x8a\xa0`?`\xe6\x9f\xa5\xe7\x9c\x8b\xe5\xb8\xae\xe5\x8a\xa9\xe6\x96\x87\xe6\xa1\xa3\xef\xbc\x8c\xe6\x88\x96\xe6\x9f\xa5\xe9\x98\x85PyTorch\xe5\xae\x98\xe6\x96\xb9\xe6\x96\x87\xe6\xa1\xa3[^1]\xe3\x80\x82\n# \n# [^1]: http://docs.pytorch.org\n\n#%%\n# Let\'s begin\nfrom __future__ import print_function\nimport torch  as t\nt.__version__\n\n#%% [markdown]\n# ###  3.1.1 \xe5\x9f\xba\xe7\xa1\x80\xe6\x93\x8d\xe4\xbd\x9c\n# \n# \xe5\xad\xa6\xe4\xb9\xa0\xe8\xbf\x87Numpy\xe7\x9a\x84\xe8\xaf\xbb\xe8\x80\x85\xe4\xbc\x9a\xe5\xaf\xb9\xe6\x9c\xac\xe8\x8a\x82\xe5\x86\x85\xe5\xae\xb9\xe6\x84\x9f\xe5\x88\xb0\xe9\x9d\x9e\xe5\xb8\xb8\xe7\x86\x9f\xe6\x82\x89\xef\xbc\x8c\xe5\x9b\xa0tensor\xe7\x9a\x84\xe6\x8e\xa5\xe5\x8f\xa3\xe6\x9c\x89\xe6\x84\x8f\xe8\xae\xbe\xe8\xae\xa1\xe6\x88\x90\xe4\xb8\x8eNumpy\xe7\xb1\xbb\xe4\xbc\xbc\xef\xbc\x8c\xe4\xbb\xa5\xe6\x96\xb9\xe4\xbe\xbf\xe7\x94\xa8\xe6\x88\xb7\xe4\xbd\xbf\xe7\x94\xa8\xe3\x80\x82\xe4\xbd\x86\xe4\xb8\x8d\xe7\x86\x9f\xe6\x82\x89Numpy\xe4\xb9\x9f\xe6\xb2\xa1\xe5\x85\xb3\xe7\xb3\xbb\xef\xbc\x8c\xe6\x9c\xac\xe8\x8a\x82\xe5\x86\x85\xe5\xae\xb9\xe5\xb9\xb6\xe4\xb8\x8d\xe8\xa6\x81\xe6\xb1\x82\xe5\x85\x88\xe6\x8e\x8c\xe6\x8f\xa1Numpy\xe3\x80\x82\n# \n# \xe4\xbb\x8e\xe6\x8e\xa5\xe5\x8f\xa3\xe7\x9a\x84\xe8\xa7\x92\xe5\xba\xa6\xe6\x9d\xa5\xe8\xae\xb2\xef\xbc\x8c\xe5\xaf\xb9tensor\xe7\x9a\x84\xe6\x93\x8d\xe4\xbd\x9c\xe5\x8f\xaf\xe5\x88\x86\xe4\xb8\xba\xe4\xb8\xa4\xe7\xb1\xbb\xef\xbc\x9a\n# \n# 1. `torch.function`\xef\xbc\x8c\xe5\xa6\x82`torch.save`\xe7\xad\x89\xe3\x80\x82\n# 2. \xe5\x8f\xa6\xe4\xb8\x80\xe7\xb1\xbb\xe6\x98\xaf`tensor.function`\xef\xbc\x8c\xe5\xa6\x82`tensor.view`\xe7\xad\x89\xe3\x80\x82\n# \n# \xe4\xb8\xba\xe6\x96\xb9\xe4\xbe\xbf\xe4\xbd\xbf\xe7\x94\xa8\xef\xbc\x8c\xe5\xaf\xb9tensor\xe7\x9a\x84\xe5\xa4\xa7\xe9\x83\xa8\xe5\x88\x86\xe6\x93\x8d\xe4\xbd\x9c\xe5\x90\x8c\xe6\x97\xb6\xe6\x94\xaf\xe6\x8c\x81\xe8\xbf\x99\xe4\xb8\xa4\xe7\xb1\xbb\xe6\x8e\xa5\xe5\x8f\xa3\xef\xbc\x8c\xe5\x9c\xa8\xe6\x9c\xac\xe4\xb9\xa6\xe4\xb8\xad\xe4\xb8\x8d\xe5\x81\x9a\xe5\x85\xb7\xe4\xbd\x93\xe5\x8c\xba\xe5\x88\x86\xef\xbc\x8c\xe5\xa6\x82`torch.sum (torch.sum(a, b))`\xe4\xb8\x8e`tensor.sum (a.sum(b))`\xe5\x8a\x9f\xe8\x83\xbd\xe7\xad\x89\xe4\xbb\xb7\xe3\x80\x82\n# \n# \xe8\x80\x8c\xe4\xbb\x8e\xe5\xad\x98\xe5\x82\xa8\xe7\x9a\x84\xe8\xa7\x92\xe5\xba\xa6\xe6\x9d\xa5\xe8\xae\xb2\xef\xbc\x8c\xe5\xaf\xb9tensor\xe7\x9a\x84\xe6\x93\x8d\xe4\xbd\x9c\xe5\x8f\x88\xe5\x8f\xaf\xe5\x88\x86\xe4\xb8\xba\xe4\xb8\xa4\xe7\xb1\xbb\xef\xbc\x9a\n# \n# 1. \xe4\xb8\x8d\xe4\xbc\x9a\xe4\xbf\xae\xe6\x94\xb9\xe8\x87\xaa\xe8\xba\xab\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xef\xbc\x8c\xe5\xa6\x82 `a.add(b)`\xef\xbc\x8c \xe5\x8a\xa0\xe6\xb3\x95\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x9c\xe4\xbc\x9a\xe8\xbf\x94\xe5\x9b\x9e\xe4\xb8\x80\xe4\xb8\xaa\xe6\x96\xb0\xe7\x9a\x84tensor\xe3\x80\x82\n# 2. \xe4\xbc\x9a\xe4\xbf\xae\xe6\x94\xb9\xe8\x87\xaa\xe8\xba\xab\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xef\xbc\x8c\xe5\xa6\x82 `a.add_(b)`\xef\xbc\x8c \xe5\x8a\xa0\xe6\xb3\x95\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x9c\xe4\xbb\x8d\xe5\xad\x98\xe5\x82\xa8\xe5\x9c\xa8a\xe4\xb8\xad\xef\xbc\x8ca\xe8\xa2\xab\xe4\xbf\xae\xe6\x94\xb9\xe4\xba\x86\xe3\x80\x82\n# \n# \xe5\x87\xbd\xe6\x95\xb0\xe5\x90\x8d\xe4\xbb\xa5`_`\xe7\xbb\x93\xe5\xb0\xbe\xe7\x9a\x84\xe9\x83\xbd\xe6\x98\xafinplace\xe6\x96\xb9\xe5\xbc\x8f, \xe5\x8d\xb3\xe4\xbc\x9a\xe4\xbf\xae\xe6\x94\xb9\xe8\xb0\x83\xe7\x94\xa8\xe8\x80\x85\xe8\x87\xaa\xe5\xb7\xb1\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xef\xbc\x8c\xe5\x9c\xa8\xe5\xae\x9e\xe9\x99\x85\xe5\xba\x94\xe7\x94\xa8\xe4\xb8\xad\xe9\x9c\x80\xe5\x8a\xa0\xe4\xbb\xa5\xe5\x8c\xba\xe5\x88\x86\xe3\x80\x82\n# \n# #### \xe5\x88\x9b\xe5\xbb\xbaTensor\n# \n# \xe5\x9c\xa8PyTorch\xe4\xb8\xad\xe6\x96\xb0\xe5\xbb\xbatensor\xe7\x9a\x84\xe6\x96\xb9\xe6\xb3\x95\xe6\x9c\x89\xe5\xbe\x88\xe5\xa4\x9a\xef\xbc\x8c\xe5\x85\xb7\xe4\xbd\x93\xe5\xa6\x82\xe8\xa1\xa83-1\xe6\x89\x80\xe7\xa4\xba\xe3\x80\x82\n# \n# \xe8\xa1\xa83-1: \xe5\xb8\xb8\xe8\xa7\x81\xe6\x96\xb0\xe5\xbb\xbatensor\xe7\x9a\x84\xe6\x96\xb9\xe6\xb3\x95\n# \n# |\xe5\x87\xbd\xe6\x95\xb0|\xe5\x8a\x9f\xe8\x83\xbd|\n# |:---:|:---:|\n# |Tensor(\\*sizes)|\xe5\x9f\xba\xe7\xa1\x80\xe6\x9e\x84\xe9\x80\xa0\xe5\x87\xbd\xe6\x95\xb0|\n# |tensor(data,)|\xe7\xb1\xbb\xe4\xbc\xbcnp.array\xe7\x9a\x84\xe6\x9e\x84\xe9\x80\xa0\xe5\x87\xbd\xe6\x95\xb0|\n# |ones(\\*sizes)|\xe5\x85\xa81Tensor|\n# |zeros(\\*sizes)|\xe5\x85\xa80Tensor|\n# |eye(\\*sizes)|\xe5\xaf\xb9\xe8\xa7\x92\xe7\xba\xbf\xe4\xb8\xba1\xef\xbc\x8c\xe5\x85\xb6\xe4\xbb\x96\xe4\xb8\xba0|\n# |arange(s,e,step|\xe4\xbb\x8es\xe5\x88\xb0e\xef\xbc\x8c\xe6\xad\xa5\xe9\x95\xbf\xe4\xb8\xbastep|\n# |linspace(s,e,steps)|\xe4\xbb\x8es\xe5\x88\xb0e\xef\xbc\x8c\xe5\x9d\x87\xe5\x8c\x80\xe5\x88\x87\xe5\x88\x86\xe6\x88\x90steps\xe4\xbb\xbd|\n# |rand/randn(\\*sizes)|\xe5\x9d\x87\xe5\x8c\x80/\xe6\xa0\x87\xe5\x87\x86\xe5\x88\x86\xe5\xb8\x83|\n# |normal(mean,std)/uniform(from,to)|\xe6\xad\xa3\xe6\x80\x81\xe5\x88\x86\xe5\xb8\x83/\xe5\x9d\x87\xe5\x8c\x80\xe5\x88\x86\xe5\xb8\x83|\n# |randperm(m)|\xe9\x9a\x8f\xe6\x9c\xba\xe6\x8e\x92\xe5\x88\x97|\n# \n# \xe8\xbf\x99\xe4\xba\x9b\xe5\x88\x9b\xe5\xbb\xba\xe6\x96\xb9\xe6\xb3\x95\xe9\x83\xbd\xe5\x8f\xaf\xe4\xbb\xa5\xe5\x9c\xa8\xe5\x88\x9b\xe5\xbb\xba\xe7\x9a\x84\xe6\x97\xb6\xe5\x80\x99\xe6\x8c\x87\xe5\xae\x9a\xe6\x95\xb0\xe6\x8d\xae\xe7\xb1\xbb\xe5\x9e\x8bdtype\xe5\x92\x8c\xe5\xad\x98\xe6\x94\xbedevice(cpu/gpu).\n# \n# \n# \xe5\x85\xb6\xe4\xb8\xad\xe4\xbd\xbf\xe7\x94\xa8`Tensor`\xe5\x87\xbd\xe6\x95\xb0\xe6\x96\xb0\xe5\xbb\xbatensor\xe6\x98\xaf\xe6\x9c\x80\xe5\xa4\x8d\xe6\x9d\x82\xe5\xa4\x9a\xe5\x8f\x98\xe7\x9a\x84\xe6\x96\xb9\xe5\xbc\x8f\xef\xbc\x8c\xe5\xae\x83\xe6\x97\xa2\xe5\x8f\xaf\xe4\xbb\xa5\xe6\x8e\xa5\xe6\x94\xb6\xe4\xb8\x80\xe4\xb8\xaalist\xef\xbc\x8c\xe5\xb9\xb6\xe6\xa0\xb9\xe6\x8d\xaelist\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe6\x96\xb0\xe5\xbb\xbatensor\xef\xbc\x8c\xe4\xb9\x9f\xe8\x83\xbd\xe6\xa0\xb9\xe6\x8d\xae\xe6\x8c\x87\xe5\xae\x9a\xe7\x9a\x84\xe5\xbd\xa2\xe7\x8a\xb6\xe6\x96\xb0\xe5\xbb\xbatensor\xef\xbc\x8c\xe8\xbf\x98\xe8\x83\xbd\xe4\xbc\xa0\xe5\x85\xa5\xe5\x85\xb6\xe4\xbb\x96\xe7\x9a\x84tensor\xef\xbc\x8c\xe4\xb8\x8b\xe9\x9d\xa2\xe4\xb8\xbe\xe5\x87\xa0\xe4\xb8\xaa\xe4\xbe\x8b\xe5\xad\x90\xe3\x80\x82\n\n#%%\n# \xe6\x8c\x87\xe5\xae\x9atensor\xe7\x9a\x84\xe5\xbd\xa2\xe7\x8a\xb6\na = t.Tensor(2, 3)\na # \xe6\x95\xb0\xe5\x80\xbc\xe5\x8f\x96\xe5\x86\xb3\xe4\xba\x8e\xe5\x86\x85\xe5\xad\x98\xe7\xa9\xba\xe9\x97\xb4\xe7\x9a\x84\xe7\x8a\xb6\xe6\x80\x81\xef\xbc\x8cprint\xe6\x97\xb6\xe5\x80\x99\xe5\x8f\xaf\xe8\x83\xbdoverflow\n\n\n#%%\n# \xe7\x94\xa8list\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe5\x88\x9b\xe5\xbb\xbatensor\nb = t.Tensor([[1,2,3],[4,5,6]])\nb\n\n\n#%%\nb.tolist() # \xe6\x8a\x8atensor\xe8\xbd\xac\xe4\xb8\xbalist\n\n#%% [markdown]\n# `tensor.size()`\xe8\xbf\x94\xe5\x9b\x9e`torch.Size`\xe5\xaf\xb9\xe8\xb1\xa1\xef\xbc\x8c\xe5\xae\x83\xe6\x98\xaftuple\xe7\x9a\x84\xe5\xad\x90\xe7\xb1\xbb\xef\xbc\x8c\xe4\xbd\x86\xe5\x85\xb6\xe4\xbd\xbf\xe7\x94\xa8\xe6\x96\xb9\xe5\xbc\x8f\xe4\xb8\x8etuple\xe7\x95\xa5\xe6\x9c\x89\xe5\x8c\xba\xe5\x88\xab\n\n#%%\nb_size = b.size()\nb_size\n\n\n#%%\nb.numel() # b\xe4\xb8\xad\xe5\x85\x83\xe7\xb4\xa0\xe6\x80\xbb\xe4\xb8\xaa\xe6\x95\xb0\xef\xbc\x8c2*3\xef\xbc\x8c\xe7\xad\x89\xe4\xbb\xb7\xe4\xba\x8eb.nelement()\n\n\n#%%\n# \xe5\x88\x9b\xe5\xbb\xba\xe4\xb8\x80\xe4\xb8\xaa\xe5\x92\x8cb\xe5\xbd\xa2\xe7\x8a\xb6\xe4\xb8\x80\xe6\xa0\xb7\xe7\x9a\x84tensor\nc = t.Tensor(b_size)\n# \xe5\x88\x9b\xe5\xbb\xba\xe4\xb8\x80\xe4\xb8\xaa\xe5\x85\x83\xe7\xb4\xa0\xe4\xb8\xba2\xe5\x92\x8c3\xe7\x9a\x84tensor\nd = t.Tensor((2, 3))\nc, d\n\n#%% [markdown]\n# \xe9\x99\xa4\xe4\xba\x86`tensor.size()`\xef\xbc\x8c\xe8\xbf\x98\xe5\x8f\xaf\xe4\xbb\xa5\xe5\x88\xa9\xe7\x94\xa8`tensor.shape`\xe7\x9b\xb4\xe6\x8e\xa5\xe6\x9f\xa5\xe7\x9c\x8btensor\xe7\x9a\x84\xe5\xbd\xa2\xe7\x8a\xb6\xef\xbc\x8c`tensor.shape`\xe7\xad\x89\xe4\xbb\xb7\xe4\xba\x8e`tensor.size()`\n\n#%%\nc.shape\n\n#%% [markdown]\n# \xe9\x9c\x80\xe8\xa6\x81\xe6\xb3\xa8\xe6\x84\x8f\xe7\x9a\x84\xe6\x98\xaf\xef\xbc\x8c`t.Tensor(*sizes)`\xe5\x88\x9b\xe5\xbb\xbatensor\xe6\x97\xb6\xef\xbc\x8c\xe7\xb3\xbb\xe7\xbb\x9f\xe4\xb8\x8d\xe4\xbc\x9a\xe9\xa9\xac\xe4\xb8\x8a\xe5\x88\x86\xe9\x85\x8d\xe7\xa9\xba\xe9\x97\xb4\xef\xbc\x8c\xe5\x8f\xaa\xe6\x98\xaf\xe4\xbc\x9a\xe8\xae\xa1\xe7\xae\x97\xe5\x89\xa9\xe4\xbd\x99\xe7\x9a\x84\xe5\x86\x85\xe5\xad\x98\xe6\x98\xaf\xe5\x90\xa6\xe8\xb6\xb3\xe5\xa4\x9f\xe4\xbd\xbf\xe7\x94\xa8\xef\xbc\x8c\xe4\xbd\xbf\xe7\x94\xa8\xe5\x88\xb0tensor\xe6\x97\xb6\xe6\x89\x8d\xe4\xbc\x9a\xe5\x88\x86\xe9\x85\x8d\xef\xbc\x8c\xe8\x80\x8c\xe5\x85\xb6\xe5\xae\x83\xe6\x93\x8d\xe4\xbd\x9c\xe9\x83\xbd\xe6\x98\xaf\xe5\x9c\xa8\xe5\x88\x9b\xe5\xbb\xba\xe5\xae\x8ctensor\xe4\xb9\x8b\xe5\x90\x8e\xe9\xa9\xac\xe4\xb8\x8a\xe8\xbf\x9b\xe8\xa1\x8c\xe7\xa9\xba\xe9\x97\xb4\xe5\x88\x86\xe9\x85\x8d\xe3\x80\x82\xe5\x85\xb6\xe5\xae\x83\xe5\xb8\xb8\xe7\x94\xa8\xe7\x9a\x84\xe5\x88\x9b\xe5\xbb\xbatensor\xe7\x9a\x84\xe6\x96\xb9\xe6\xb3\x95\xe4\xb8\xbe\xe4\xbe\x8b\xe5\xa6\x82\xe4\xb8\x8b\xe3\x80\x82\n\n#%%\nt.ones(2, 3)\n\n\n#%%\nt.zeros(2, 3)\n\n\n#%%\nt.arange(1, 6, 2)\n\n\n#%%\nt.linspace(1, 10, 3)\n\n\n#%%\nt.randn(2, 3, device=t.device(\'cpu\'))\n\n\n#%%\nt.randperm(5) # \xe9\x95\xbf\xe5\xba\xa6\xe4\xb8\xba5\xe7\x9a\x84\xe9\x9a\x8f\xe6\x9c\xba\xe6\x8e\x92\xe5\x88\x97\n\n\n#%%\nt.eye(2, 3, dtype=t.int) # \xe5\xaf\xb9\xe8\xa7\x92\xe7\xba\xbf\xe4\xb8\xba1, \xe4\xb8\x8d\xe8\xa6\x81\xe6\xb1\x82\xe8\xa1\x8c\xe5\x88\x97\xe6\x95\xb0\xe4\xb8\x80\xe8\x87\xb4\n\n#%% [markdown]\n# `torch.tensor`\xe6\x98\xaf\xe5\x9c\xa80.4\xe7\x89\x88\xe6\x9c\xac\xe6\x96\xb0\xe5\xa2\x9e\xe5\x8a\xa0\xe7\x9a\x84\xe4\xb8\x80\xe4\xb8\xaa\xe6\x96\xb0\xe7\x89\x88\xe6\x9c\xac\xe7\x9a\x84\xe5\x88\x9b\xe5\xbb\xbatensor\xe6\x96\xb9\xe6\xb3\x95\xef\xbc\x8c\xe4\xbd\xbf\xe7\x94\xa8\xe7\x9a\x84\xe6\x96\xb9\xe6\xb3\x95\xef\xbc\x8c\xe5\x92\x8c\xe5\x8f\x82\xe6\x95\xb0\xe5\x87\xa0\xe4\xb9\x8e\xe5\x92\x8c`np.array`\xe5\xae\x8c\xe5\x85\xa8\xe4\xb8\x80\xe8\x87\xb4\n\n#%%\nscalar = t.tensor(3.14159) \nprint(\'scalar: %s, shape of sclar: %s\' %(scalar, scalar.shape))\n\n\n#%%\nvector = t.tensor([1, 2])\nprint(\'vector: %s, shape of vector: %s\' %(vector, vector.shape))\n\n\n#%%\ntensor = t.Tensor(1,2) # \xe6\xb3\xa8\xe6\x84\x8f\xe5\x92\x8ct.tensor([1, 2])\xe7\x9a\x84\xe5\x8c\xba\xe5\x88\xab\ntensor.shape\n\n\n#%%\nmatrix = t.tensor([[0.1, 1.2], [2.2, 3.1], [4.9, 5.2]])\nmatrix,matrix.shape\n\n\n#%%\nt.tensor([[0.11111, 0.222222, 0.3333333]],\n                     dtype=t.float64,\n                     device=t.device(\'cpu\'))\n\n\n#%%\nempty_tensor = t.tensor([])\nempty_tensor.shape\n\n#%% [markdown]\n# #### \xe5\xb8\xb8\xe7\x94\xa8Tensor\xe6\x93\x8d\xe4\xbd\x9c\n#%% [markdown]\n# \xe9\x80\x9a\xe8\xbf\x87`tensor.view`\xe6\x96\xb9\xe6\xb3\x95\xe5\x8f\xaf\xe4\xbb\xa5\xe8\xb0\x83\xe6\x95\xb4tensor\xe7\x9a\x84\xe5\xbd\xa2\xe7\x8a\xb6\xef\xbc\x8c\xe4\xbd\x86\xe5\xbf\x85\xe9\xa1\xbb\xe4\xbf\x9d\xe8\xaf\x81\xe8\xb0\x83\xe6\x95\xb4\xe5\x89\x8d\xe5\x90\x8e\xe5\x85\x83\xe7\xb4\xa0\xe6\x80\xbb\xe6\x95\xb0\xe4\xb8\x80\xe8\x87\xb4\xe3\x80\x82`view`\xe4\xb8\x8d\xe4\xbc\x9a\xe4\xbf\xae\xe6\x94\xb9\xe8\x87\xaa\xe8\xba\xab\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xef\xbc\x8c\xe8\xbf\x94\xe5\x9b\x9e\xe7\x9a\x84\xe6\x96\xb0tensor\xe4\xb8\x8e\xe6\xba\x90tensor\xe5\x85\xb1\xe4\xba\xab\xe5\x86\x85\xe5\xad\x98\xef\xbc\x8c\xe4\xb9\x9f\xe5\x8d\xb3\xe6\x9b\xb4\xe6\x94\xb9\xe5\x85\xb6\xe4\xb8\xad\xe7\x9a\x84\xe4\xb8\x80\xe4\xb8\xaa\xef\xbc\x8c\xe5\x8f\xa6\xe5\xa4\x96\xe4\xb8\x80\xe4\xb8\xaa\xe4\xb9\x9f\xe4\xbc\x9a\xe8\xb7\x9f\xe7\x9d\x80\xe6\x94\xb9\xe5\x8f\x98\xe3\x80\x82\xe5\x9c\xa8\xe5\xae\x9e\xe9\x99\x85\xe5\xba\x94\xe7\x94\xa8\xe4\xb8\xad\xe5\x8f\xaf\xe8\x83\xbd\xe7\xbb\x8f\xe5\xb8\xb8\xe9\x9c\x80\xe8\xa6\x81\xe6\xb7\xbb\xe5\x8a\xa0\xe6\x88\x96\xe5\x87\x8f\xe5\xb0\x91\xe6\x9f\x90\xe4\xb8\x80\xe7\xbb\xb4\xe5\xba\xa6\xef\xbc\x8c\xe8\xbf\x99\xe6\x97\xb6\xe5\x80\x99`squeeze`\xe5\x92\x8c`unsqueeze`\xe4\xb8\xa4\xe4\xb8\xaa\xe5\x87\xbd\xe6\x95\xb0\xe5\xb0\xb1\xe6\xb4\xbe\xe4\xb8\x8a\xe7\x94\xa8\xe5\x9c\xba\xe4\xba\x86\xe3\x80\x82\n\n#%%\na = t.arange(0, 6)\na.view(2, 3)\n\n\n#%%\nb = a.view(-1, 3) # \xe5\xbd\x93\xe6\x9f\x90\xe4\xb8\x80\xe7\xbb\xb4\xe4\xb8\xba-1\xe7\x9a\x84\xe6\x97\xb6\xe5\x80\x99\xef\xbc\x8c\xe4\xbc\x9a\xe8\x87\xaa\xe5\x8a\xa8\xe8\xae\xa1\xe7\xae\x97\xe5\xae\x83\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f\nb.shape\n\n\n#%%\nb.unsqueeze(1) # \xe6\xb3\xa8\xe6\x84\x8f\xe5\xbd\xa2\xe7\x8a\xb6\xef\xbc\x8c\xe5\x9c\xa8\xe7\xac\xac1\xe7\xbb\xb4\xef\xbc\x88\xe4\xb8\x8b\xe6\xa0\x87\xe4\xbb\x8e0\xe5\xbc\x80\xe5\xa7\x8b\xef\xbc\x89\xe4\xb8\x8a\xe5\xa2\x9e\xe5\x8a\xa0\xe2\x80\x9c\xef\xbc\x91\xe2\x80\x9d \n#\xe7\xad\x89\xe4\xbb\xb7\xe4\xba\x8e b[:,None]\nb[:, None].shape\n\n\n#%%\nb.unsqueeze(-2) # -2\xe8\xa1\xa8\xe7\xa4\xba\xe5\x80\x92\xe6\x95\xb0\xe7\xac\xac\xe4\xba\x8c\xe4\xb8\xaa\xe7\xbb\xb4\xe5\xba\xa6\n\n\n#%%\nc = b.view(1, 1, 1, 2, 3)\nc.squeeze(0) # \xe5\x8e\x8b\xe7\xbc\xa9\xe7\xac\xac0\xe7\xbb\xb4\xe7\x9a\x84\xe2\x80\x9c\xef\xbc\x91\xe2\x80\x9d\n\n\n#%%\nc.squeeze() # \xe6\x8a\x8a\xe6\x89\x80\xe6\x9c\x89\xe7\xbb\xb4\xe5\xba\xa6\xe4\xb8\xba\xe2\x80\x9c1\xe2\x80\x9d\xe7\x9a\x84\xe5\x8e\x8b\xe7\xbc\xa9\n\n\n#%%\na[1] = 100\nb # a\xe4\xbf\xae\xe6\x94\xb9\xef\xbc\x8cb\xe4\xbd\x9c\xe4\xb8\xbaview\xe4\xb9\x8b\xe5\x90\x8e\xe7\x9a\x84\xef\xbc\x8c\xe4\xb9\x9f\xe4\xbc\x9a\xe8\xb7\x9f\xe7\x9d\x80\xe4\xbf\xae\xe6\x94\xb9\n\n#%% [markdown]\n# `resize`\xe6\x98\xaf\xe5\x8f\xa6\xe4\xb8\x80\xe7\xa7\x8d\xe5\x8f\xaf\xe7\x94\xa8\xe6\x9d\xa5\xe8\xb0\x83\xe6\x95\xb4`size`\xe7\x9a\x84\xe6\x96\xb9\xe6\xb3\x95\xef\xbc\x8c\xe4\xbd\x86\xe4\xb8\x8e`view`\xe4\xb8\x8d\xe5\x90\x8c\xef\xbc\x8c\xe5\xae\x83\xe5\x8f\xaf\xe4\xbb\xa5\xe4\xbf\xae\xe6\x94\xb9tensor\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f\xe3\x80\x82\xe5\xa6\x82\xe6\x9e\x9c\xe6\x96\xb0\xe5\xa4\xa7\xe5\xb0\x8f\xe8\xb6\x85\xe8\xbf\x87\xe4\xba\x86\xe5\x8e\x9f\xe5\xa4\xa7\xe5\xb0\x8f\xef\xbc\x8c\xe4\xbc\x9a\xe8\x87\xaa\xe5\x8a\xa8\xe5\x88\x86\xe9\x85\x8d\xe6\x96\xb0\xe7\x9a\x84\xe5\x86\x85\xe5\xad\x98\xe7\xa9\xba\xe9\x97\xb4\xef\xbc\x8c\xe8\x80\x8c\xe5\xa6\x82\xe6\x9e\x9c\xe6\x96\xb0\xe5\xa4\xa7\xe5\xb0\x8f\xe5\xb0\x8f\xe4\xba\x8e\xe5\x8e\x9f\xe5\xa4\xa7\xe5\xb0\x8f\xef\xbc\x8c\xe5\x88\x99\xe4\xb9\x8b\xe5\x89\x8d\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe4\xbe\x9d\xe6\x97\xa7\xe4\xbc\x9a\xe8\xa2\xab\xe4\xbf\x9d\xe5\xad\x98\xef\xbc\x8c\xe7\x9c\x8b\xe4\xb8\x80\xe4\xb8\xaa\xe4\xbe\x8b\xe5\xad\x90\xe3\x80\x82\n\n#%%\nb.resize_(1, 3)\nb\n\n\n#%%\nb.resize_(3, 3) # \xe6\x97\xa7\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe4\xbe\x9d\xe6\x97\xa7\xe4\xbf\x9d\xe5\xad\x98\xe7\x9d\x80\xef\xbc\x8c\xe5\xa4\x9a\xe5\x87\xba\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f\xe4\xbc\x9a\xe5\x88\x86\xe9\x85\x8d\xe6\x96\xb0\xe7\xa9\xba\xe9\x97\xb4\nb\n\n#%% [markdown]\n# #### \xe7\xb4\xa2\xe5\xbc\x95\xe6\x93\x8d\xe4\xbd\x9c\n# \n# Tensor\xe6\x94\xaf\xe6\x8c\x81\xe4\xb8\x8enumpy.ndarray\xe7\xb1\xbb\xe4\xbc\xbc\xe7\x9a\x84\xe7\xb4\xa2\xe5\xbc\x95\xe6\x93\x8d\xe4\xbd\x9c\xef\xbc\x8c\xe8\xaf\xad\xe6\xb3\x95\xe4\xb8\x8a\xe4\xb9\x9f\xe7\xb1\xbb\xe4\xbc\xbc\xef\xbc\x8c\xe4\xb8\x8b\xe9\x9d\xa2\xe9\x80\x9a\xe8\xbf\x87\xe4\xb8\x80\xe4\xba\x9b\xe4\xbe\x8b\xe5\xad\x90\xef\xbc\x8c\xe8\xae\xb2\xe8\xa7\xa3\xe5\xb8\xb8\xe7\x94\xa8\xe7\x9a\x84\xe7\xb4\xa2\xe5\xbc\x95\xe6\x93\x8d\xe4\xbd\x9c\xe3\x80\x82\xe5\xa6\x82\xe6\x97\xa0\xe7\x89\xb9\xe6\xae\x8a\xe8\xaf\xb4\xe6\x98\x8e\xef\xbc\x8c\xe7\xb4\xa2\xe5\xbc\x95\xe5\x87\xba\xe6\x9d\xa5\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x9c\xe4\xb8\x8e\xe5\x8e\x9ftensor\xe5\x85\xb1\xe4\xba\xab\xe5\x86\x85\xe5\xad\x98\xef\xbc\x8c\xe4\xb9\x9f\xe5\x8d\xb3\xe4\xbf\xae\xe6\x94\xb9\xe4\xb8\x80\xe4\xb8\xaa\xef\xbc\x8c\xe5\x8f\xa6\xe4\xb8\x80\xe4\xb8\xaa\xe4\xbc\x9a\xe8\xb7\x9f\xe7\x9d\x80\xe4\xbf\xae\xe6\x94\xb9\xe3\x80\x82\n\n#%%\na = t.randn(3, 4)\na\n\n\n#%%\na[0] # \xe7\xac\xac0\xe8\xa1\x8c(\xe4\xb8\x8b\xe6\xa0\x87\xe4\xbb\x8e0\xe5\xbc\x80\xe5\xa7\x8b)\n\n\n#%%\na[:, 0] # \xe7\xac\xac0\xe5\x88\x97\n\n\n#%%\na[0][2] # \xe7\xac\xac0\xe8\xa1\x8c\xe7\xac\xac2\xe4\xb8\xaa\xe5\x85\x83\xe7\xb4\xa0\xef\xbc\x8c\xe7\xad\x89\xe4\xbb\xb7\xe4\xba\x8ea[0, 2]\n\n\n#%%\na[0, -1] # \xe7\xac\xac0\xe8\xa1\x8c\xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x80\xe4\xb8\xaa\xe5\x85\x83\xe7\xb4\xa0\n\n\n#%%\na[:2] # \xe5\x89\x8d\xe4\xb8\xa4\xe8\xa1\x8c\n\n\n#%%\na[:2, 0:2] # \xe5\x89\x8d\xe4\xb8\xa4\xe8\xa1\x8c\xef\xbc\x8c\xe7\xac\xac0,1\xe5\x88\x97\n\n\n#%%\nprint(a[0:1, :2]) # \xe7\xac\xac0\xe8\xa1\x8c\xef\xbc\x8c\xe5\x89\x8d\xe4\xb8\xa4\xe5\x88\x97 \nprint(a[0, :2]) # \xe6\xb3\xa8\xe6\x84\x8f\xe4\xb8\xa4\xe8\x80\x85\xe7\x9a\x84\xe5\x8c\xba\xe5\x88\xab\xef\xbc\x9a\xe5\xbd\xa2\xe7\x8a\xb6\xe4\xb8\x8d\xe5\x90\x8c\n\n\n#%%\n# None\xe7\xb1\xbb\xe4\xbc\xbc\xe4\xba\x8enp.newaxis, \xe4\xb8\xbaa\xe6\x96\xb0\xe5\xa2\x9e\xe4\xba\x86\xe4\xb8\x80\xe4\xb8\xaa\xe8\xbd\xb4\n# \xe7\xad\x89\xe4\xbb\xb7\xe4\xba\x8ea.view(1, a.shape[0], a.shape[1])\na[None].shape\n\n\n#%%\na[None].shape # \xe7\xad\x89\xe4\xbb\xb7\xe4\xba\x8ea[None,:,:]\n\n\n#%%\na[:,None,:].shape\n\n\n#%%\na[:,None,:,None,None].shape\n\n\n#%%\na > 1 # \xe8\xbf\x94\xe5\x9b\x9e\xe4\xb8\x80\xe4\xb8\xaaByteTensor\n\n\n#%%\na[a>1] # \xe7\xad\x89\xe4\xbb\xb7\xe4\xba\x8ea.masked_select(a>1)\n# \xe9\x80\x89\xe6\x8b\xa9\xe7\xbb\x93\xe6\x9e\x9c\xe4\xb8\x8e\xe5\x8e\x9ftensor\xe4\xb8\x8d\xe5\x85\xb1\xe4\xba\xab\xe5\x86\x85\xe5\xad\x98\xe7\xa9\xba\xe9\x97\xb4\n\n\n#%%\na[t.LongTensor([0,1])] # \xe7\xac\xac0\xe8\xa1\x8c\xe5\x92\x8c\xe7\xac\xac1\xe8\xa1\x8c\n\n#%% [markdown]\n# \xe5\x85\xb6\xe5\xae\x83\xe5\xb8\xb8\xe7\x94\xa8\xe7\x9a\x84\xe9\x80\x89\xe6\x8b\xa9\xe5\x87\xbd\xe6\x95\xb0\xe5\xa6\x82\xe8\xa1\xa83-2\xe6\x89\x80\xe7\xa4\xba\xe3\x80\x82\n# \n# \xe8\xa1\xa83-2\xe5\xb8\xb8\xe7\x94\xa8\xe7\x9a\x84\xe9\x80\x89\xe6\x8b\xa9\xe5\x87\xbd\xe6\x95\xb0\n# \n# \xe5\x87\xbd\xe6\x95\xb0|\xe5\x8a\x9f\xe8\x83\xbd|\n# :---:|:---:|\n# index_select(input, dim, index)|\xe5\x9c\xa8\xe6\x8c\x87\xe5\xae\x9a\xe7\xbb\xb4\xe5\xba\xa6dim\xe4\xb8\x8a\xe9\x80\x89\xe5\x8f\x96\xef\xbc\x8c\xe6\xaf\x94\xe5\xa6\x82\xe9\x80\x89\xe5\x8f\x96\xe6\x9f\x90\xe4\xba\x9b\xe8\xa1\x8c\xe3\x80\x81\xe6\x9f\x90\xe4\xba\x9b\xe5\x88\x97\n# masked_select(input, mask)|\xe4\xbe\x8b\xe5\xad\x90\xe5\xa6\x82\xe4\xb8\x8a\xef\xbc\x8ca[a>0]\xef\xbc\x8c\xe4\xbd\xbf\xe7\x94\xa8ByteTensor\xe8\xbf\x9b\xe8\xa1\x8c\xe9\x80\x89\xe5\x8f\x96\n# non_zero(input)|\xe9\x9d\x9e0\xe5\x85\x83\xe7\xb4\xa0\xe7\x9a\x84\xe4\xb8\x8b\xe6\xa0\x87\n# gather(input, dim, index)|\xe6\xa0\xb9\xe6\x8d\xaeindex\xef\xbc\x8c\xe5\x9c\xa8dim\xe7\xbb\xb4\xe5\xba\xa6\xe4\xb8\x8a\xe9\x80\x89\xe5\x8f\x96\xe6\x95\xb0\xe6\x8d\xae\xef\xbc\x8c\xe8\xbe\x93\xe5\x87\xba\xe7\x9a\x84size\xe4\xb8\x8eindex\xe4\xb8\x80\xe6\xa0\xb7\n# \n# \n# `gather`\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa\xe6\xaf\x94\xe8\xbe\x83\xe5\xa4\x8d\xe6\x9d\x82\xe7\x9a\x84\xe6\x93\x8d\xe4\xbd\x9c\xef\xbc\x8c\xe5\xaf\xb9\xe4\xb8\x80\xe4\xb8\xaa2\xe7\xbb\xb4tensor\xef\xbc\x8c\xe8\xbe\x93\xe5\x87\xba\xe7\x9a\x84\xe6\xaf\x8f\xe4\xb8\xaa\xe5\x85\x83\xe7\xb4\xa0\xe5\xa6\x82\xe4\xb8\x8b\xef\xbc\x9a\n# \n# ```python\n# out[i][j] = input[index[i][j]][j]  # dim=0\n# out[i][j] = input[i][index[i][j]]  # dim=1\n# ```\n# \xe4\xb8\x89\xe7\xbb\xb4tensor\xe7\x9a\x84`gather`\xe6\x93\x8d\xe4\xbd\x9c\xe5\x90\x8c\xe7\x90\x86\xef\xbc\x8c\xe4\xb8\x8b\xe9\x9d\xa2\xe4\xb8\xbe\xe5\x87\xa0\xe4\xb8\xaa\xe4\xbe\x8b\xe5\xad\x90\xe3\x80\x82\n\n#%%\na = t.arange(0, 16).view(4, 4)\na\n\n\n#%%\n# \xe9\x80\x89\xe5\x8f\x96\xe5\xaf\xb9\xe8\xa7\x92\xe7\xba\xbf\xe7\x9a\x84\xe5\x85\x83\xe7\xb4\xa0\nindex = t.LongTensor([[0,1,2,3]])\na.gather(0, index)\n\n\n#%%\n# \xe9\x80\x89\xe5\x8f\x96\xe5\x8f\x8d\xe5\xaf\xb9\xe8\xa7\x92\xe7\xba\xbf\xe4\xb8\x8a\xe7\x9a\x84\xe5\x85\x83\xe7\xb4\xa0\nindex = t.LongTensor([[3,2,1,0]]).t()\na.gather(1, index)\n\n\n#%%\n# \xe9\x80\x89\xe5\x8f\x96\xe5\x8f\x8d\xe5\xaf\xb9\xe8\xa7\x92\xe7\xba\xbf\xe4\xb8\x8a\xe7\x9a\x84\xe5\x85\x83\xe7\xb4\xa0\xef\xbc\x8c\xe6\xb3\xa8\xe6\x84\x8f\xe4\xb8\x8e\xe4\xb8\x8a\xe9\x9d\xa2\xe7\x9a\x84\xe4\xb8\x8d\xe5\x90\x8c\nindex = t.LongTensor([[3,2,1,0]])\na.gather(0, index)\n\n\n#%%\n# \xe9\x80\x89\xe5\x8f\x96\xe4\xb8\xa4\xe4\xb8\xaa\xe5\xaf\xb9\xe8\xa7\x92\xe7\xba\xbf\xe4\xb8\x8a\xe7\x9a\x84\xe5\x85\x83\xe7\xb4\xa0\nindex = t.LongTensor([[0,1,2,3],[3,2,1,0]]).t()\nb = a.gather(1, index)\nb\n\n#%% [markdown]\n# \xe4\xb8\x8e`gather`\xe7\x9b\xb8\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84\xe9\x80\x86\xe6\x93\x8d\xe4\xbd\x9c\xe6\x98\xaf`scatter_`\xef\xbc\x8c`gather`\xe6\x8a\x8a\xe6\x95\xb0\xe6\x8d\xae\xe4\xbb\x8einput\xe4\xb8\xad\xe6\x8c\x89index\xe5\x8f\x96\xe5\x87\xba\xef\xbc\x8c\xe8\x80\x8c`scatter_`\xe6\x98\xaf\xe6\x8a\x8a\xe5\x8f\x96\xe5\x87\xba\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe5\x86\x8d\xe6\x94\xbe\xe5\x9b\x9e\xe5\x8e\xbb\xe3\x80\x82\xe6\xb3\xa8\xe6\x84\x8f`scatter_`\xe5\x87\xbd\xe6\x95\xb0\xe6\x98\xafinplace\xe6\x93\x8d\xe4\xbd\x9c\xe3\x80\x82\n# \n# ```python\n# out = input.gather(dim, index)\n# -->\xe8\xbf\x91\xe4\xbc\xbc\xe9\x80\x86\xe6\x93\x8d\xe4\xbd\x9c\n# out = Tensor()\n# out.scatter_(dim, index)\n# ```\n\n#%%\n# \xe6\x8a\x8a\xe4\xb8\xa4\xe4\xb8\xaa\xe5\xaf\xb9\xe8\xa7\x92\xe7\xba\xbf\xe5\x85\x83\xe7\xb4\xa0\xe6\x94\xbe\xe5\x9b\x9e\xe5\x8e\xbb\xe5\x88\xb0\xe6\x8c\x87\xe5\xae\x9a\xe4\xbd\x8d\xe7\xbd\xae\nc = t.zeros(4,4)\nc.scatter_(1, index, b)\n\n#%% [markdown]\n# \xe5\xaf\xb9tensor\xe7\x9a\x84\xe4\xbb\xbb\xe4\xbd\x95\xe7\xb4\xa2\xe5\xbc\x95\xe6\x93\x8d\xe4\xbd\x9c\xe4\xbb\x8d\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaatensor\xef\xbc\x8c\xe6\x83\xb3\xe8\xa6\x81\xe8\x8e\xb7\xe5\x8f\x96\xe6\xa0\x87\xe5\x87\x86\xe7\x9a\x84python\xe5\xaf\xb9\xe8\xb1\xa1\xe6\x95\xb0\xe5\x80\xbc\xef\xbc\x8c\xe9\x9c\x80\xe8\xa6\x81\xe8\xb0\x83\xe7\x94\xa8`tensor.item()`, \xe8\xbf\x99\xe4\xb8\xaa\xe6\x96\xb9\xe6\xb3\x95\xe5\x8f\xaa\xe5\xaf\xb9\xe5\x8c\x85\xe5\x90\xab\xe4\xb8\x80\xe4\xb8\xaa\xe5\x85\x83\xe7\xb4\xa0\xe7\x9a\x84tensor\xe9\x80\x82\xe7\x94\xa8\n\n#%%\na[0,0] #\xe4\xbe\x9d\xe6\x97\xa7\xe6\x98\xaftensor\xef\xbc\x89\n\n\n#%%\na[0,0].item() # python float\n\n\n#%%\nd = a[0:1, 0:1, None]\nprint(d.shape)\nd.item() # \xe5\x8f\xaa\xe5\x8c\x85\xe5\x90\xab\xe4\xb8\x80\xe4\xb8\xaa\xe5\x85\x83\xe7\xb4\xa0\xe7\x9a\x84tensor\xe5\x8d\xb3\xe5\x8f\xaf\xe8\xb0\x83\xe7\x94\xa8tensor.item,\xe4\xb8\x8e\xe5\xbd\xa2\xe7\x8a\xb6\xe6\x97\xa0\xe5\x85\xb3\n\n\n#%%\n# a[0].item()  ->\n# raise ValueError: only one element tensors can be converted to Python scalars\n\n#%% [markdown]\n# #### \xe9\xab\x98\xe7\xba\xa7\xe7\xb4\xa2\xe5\xbc\x95\n# PyTorch\xe5\x9c\xa80.2\xe7\x89\x88\xe6\x9c\xac\xe4\xb8\xad\xe5\xae\x8c\xe5\x96\x84\xe4\xba\x86\xe7\xb4\xa2\xe5\xbc\x95\xe6\x93\x8d\xe4\xbd\x9c\xef\xbc\x8c\xe7\x9b\xae\xe5\x89\x8d\xe5\xb7\xb2\xe7\xbb\x8f\xe6\x94\xaf\xe6\x8c\x81\xe7\xbb\x9d\xe5\xa4\xa7\xe5\xa4\x9a\xe6\x95\xb0numpy\xe7\x9a\x84\xe9\xab\x98\xe7\xba\xa7\xe7\xb4\xa2\xe5\xbc\x95[^10]\xe3\x80\x82\xe9\xab\x98\xe7\xba\xa7\xe7\xb4\xa2\xe5\xbc\x95\xe5\x8f\xaf\xe4\xbb\xa5\xe7\x9c\x8b\xe6\x88\x90\xe6\x98\xaf\xe6\x99\xae\xe9\x80\x9a\xe7\xb4\xa2\xe5\xbc\x95\xe6\x93\x8d\xe4\xbd\x9c\xe7\x9a\x84\xe6\x89\xa9\xe5\xb1\x95\xef\xbc\x8c\xe4\xbd\x86\xe6\x98\xaf\xe9\xab\x98\xe7\xba\xa7\xe7\xb4\xa2\xe5\xbc\x95\xe6\x93\x8d\xe4\xbd\x9c\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x9c\xe4\xb8\x80\xe8\x88\xac\xe4\xb8\x8d\xe5\x92\x8c\xe5\x8e\x9f\xe5\xa7\x8b\xe7\x9a\x84Tensor\xe8\xb4\xa1\xe7\x8c\xae\xe5\x86\x85\xe5\x87\xba\xe3\x80\x82 \n# [^10]: https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html#advanced-indexing\n\n#%%\nx = t.arange(0,27).view(3,3,3)\nx\n\n\n#%%\nx[[1, 2], [1, 2], [2, 0]] # x[1,1,2]\xe5\x92\x8cx[2,2,0]\n\n\n#%%\nx[[2, 1, 0], [0], [1]] # x[2,0,1],x[1,0,1],x[0,0,1]\n\n\n#%%\nx[[0, 2], ...] # x[0] \xe5\x92\x8c x[2]\n\n#%% [markdown]\n# #### Tensor\xe7\xb1\xbb\xe5\x9e\x8b\n# \n# Tensor\xe6\x9c\x89\xe4\xb8\x8d\xe5\x90\x8c\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe7\xb1\xbb\xe5\x9e\x8b\xef\xbc\x8c\xe5\xa6\x82\xe8\xa1\xa83-3\xe6\x89\x80\xe7\xa4\xba\xef\xbc\x8c\xe6\xaf\x8f\xe7\xa7\x8d\xe7\xb1\xbb\xe5\x9e\x8b\xe5\x88\x86\xe5\x88\xab\xe5\xaf\xb9\xe5\xba\x94\xe6\x9c\x89CPU\xe5\x92\x8cGPU\xe7\x89\x88\xe6\x9c\xac(HalfTensor\xe9\x99\xa4\xe5\xa4\x96)\xe3\x80\x82\xe9\xbb\x98\xe8\xae\xa4\xe7\x9a\x84tensor\xe6\x98\xafFloatTensor\xef\xbc\x8c\xe5\x8f\xaf\xe9\x80\x9a\xe8\xbf\x87`t.set_default_tensor_type` \xe6\x9d\xa5\xe4\xbf\xae\xe6\x94\xb9\xe9\xbb\x98\xe8\xae\xa4tensor\xe7\xb1\xbb\xe5\x9e\x8b(\xe5\xa6\x82\xe6\x9e\x9c\xe9\xbb\x98\xe8\xae\xa4\xe7\xb1\xbb\xe5\x9e\x8b\xe4\xb8\xbaGPU tensor\xef\xbc\x8c\xe5\x88\x99\xe6\x89\x80\xe6\x9c\x89\xe6\x93\x8d\xe4\xbd\x9c\xe9\x83\xbd\xe5\xb0\x86\xe5\x9c\xa8GPU\xe4\xb8\x8a\xe8\xbf\x9b\xe8\xa1\x8c)\xe3\x80\x82Tensor\xe7\x9a\x84\xe7\xb1\xbb\xe5\x9e\x8b\xe5\xaf\xb9\xe5\x88\x86\xe6\x9e\x90\xe5\x86\x85\xe5\xad\x98\xe5\x8d\xa0\xe7\x94\xa8\xe5\xbe\x88\xe6\x9c\x89\xe5\xb8\xae\xe5\x8a\xa9\xe3\x80\x82\xe4\xbe\x8b\xe5\xa6\x82\xe5\xaf\xb9\xe4\xba\x8e\xe4\xb8\x80\xe4\xb8\xaasize\xe4\xb8\xba(1000, 1000, 1000)\xe7\x9a\x84FloatTensor\xef\xbc\x8c\xe5\xae\x83\xe6\x9c\x89`1000*1000*1000=10^9`\xe4\xb8\xaa\xe5\x85\x83\xe7\xb4\xa0\xef\xbc\x8c\xe6\xaf\x8f\xe4\xb8\xaa\xe5\x85\x83\xe7\xb4\xa0\xe5\x8d\xa032bit/8 = 4Byte\xe5\x86\x85\xe5\xad\x98\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe5\x85\xb1\xe5\x8d\xa0\xe5\xa4\xa7\xe7\xba\xa64GB\xe5\x86\x85\xe5\xad\x98/\xe6\x98\xbe\xe5\xad\x98\xe3\x80\x82HalfTensor\xe6\x98\xaf\xe4\xb8\x93\xe9\x97\xa8\xe4\xb8\xbaGPU\xe7\x89\x88\xe6\x9c\xac\xe8\xae\xbe\xe8\xae\xa1\xe7\x9a\x84\xef\xbc\x8c\xe5\x90\x8c\xe6\xa0\xb7\xe7\x9a\x84\xe5\x85\x83\xe7\xb4\xa0\xe4\xb8\xaa\xe6\x95\xb0\xef\xbc\x8c\xe6\x98\xbe\xe5\xad\x98\xe5\x8d\xa0\xe7\x94\xa8\xe5\x8f\xaa\xe6\x9c\x89FloatTensor\xe7\x9a\x84\xe4\xb8\x80\xe5\x8d\x8a\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe5\x8f\xaf\xe4\xbb\xa5\xe6\x9e\x81\xe5\xa4\xa7\xe7\xbc\x93\xe8\xa7\xa3GPU\xe6\x98\xbe\xe5\xad\x98\xe4\xb8\x8d\xe8\xb6\xb3\xe7\x9a\x84\xe9\x97\xae\xe9\xa2\x98\xef\xbc\x8c\xe4\xbd\x86\xe7\x94\xb1\xe4\xba\x8eHalfTensor\xe6\x89\x80\xe8\x83\xbd\xe8\xa1\xa8\xe7\xa4\xba\xe7\x9a\x84\xe6\x95\xb0\xe5\x80\xbc\xe5\xa4\xa7\xe5\xb0\x8f\xe5\x92\x8c\xe7\xb2\xbe\xe5\xba\xa6\xe6\x9c\x89\xe9\x99\x90[^2]\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe5\x8f\xaf\xe8\x83\xbd\xe5\x87\xba\xe7\x8e\xb0\xe6\xba\xa2\xe5\x87\xba\xe7\xad\x89\xe9\x97\xae\xe9\xa2\x98\xe3\x80\x82\n# \n# [^2]: https://stackoverflow.com/questions/872544/what-range-of-numbers-can-be-represented-in-a-16-32-and-64-bit-ieee-754-syste\n# \n# \xe8\xa1\xa83-3: tensor\xe6\x95\xb0\xe6\x8d\xae\xe7\xb1\xbb\xe5\x9e\x8b\n# \n# | Data type                | dtype                             | CPU tensor                                                   | GPU tensor                |\n# | ------------------------ | --------------------------------- | ------------------------------------------------------------ | ------------------------- |\n# | 32-bit floating point    | `torch.float32` or `torch.float`  | `torch.FloatTensor`                                          | `torch.cuda.FloatTensor`  |\n# | 64-bit floating point    | `torch.float64` or `torch.double` | `torch.DoubleTensor`                                         | `torch.cuda.DoubleTensor` |\n# | 16-bit floating point    | `torch.float16` or `torch.half`   | `torch.HalfTensor`                                           | `torch.cuda.HalfTensor`   |\n# | 8-bit integer (unsigned) | `torch.uint8`                     | [`torch.ByteTensor`](https://pytorch.org/docs/stable/tensors.html#torch.ByteTensor) | `torch.cuda.ByteTensor`   |\n# | 8-bit integer (signed)   | `torch.int8`                      | `torch.CharTensor`                                           | `torch.cuda.CharTensor`   |\n# | 16-bit integer (signed)  | `torch.int16` or `torch.short`    | `torch.ShortTensor`                                          | `torch.cuda.ShortTensor`  |\n# | 32-bit integer (signed)  | `torch.int32` or `torch.int`      | `torch.IntTensor`                                            | `torch.cuda.IntTensor`    |\n# | 64-bit integer (signed)  | `torch.int64` or `torch.long`     | `torch.LongTensor`                                           | `torch.cuda.LongTensor`   |\n# \n#  \n# \n# \xe5\x90\x84\xe6\x95\xb0\xe6\x8d\xae\xe7\xb1\xbb\xe5\x9e\x8b\xe4\xb9\x8b\xe9\x97\xb4\xe5\x8f\xaf\xe4\xbb\xa5\xe4\xba\x92\xe7\x9b\xb8\xe8\xbd\xac\xe6\x8d\xa2\xef\xbc\x8c`type(new_type)`\xe6\x98\xaf\xe9\x80\x9a\xe7\x94\xa8\xe7\x9a\x84\xe5\x81\x9a\xe6\xb3\x95\xef\xbc\x8c\xe5\x90\x8c\xe6\x97\xb6\xe8\xbf\x98\xe6\x9c\x89`float`\xe3\x80\x81`long`\xe3\x80\x81`half`\xe7\xad\x89\xe5\xbf\xab\xe6\x8d\xb7\xe6\x96\xb9\xe6\xb3\x95\xe3\x80\x82CPU tensor\xe4\xb8\x8eGPU tensor\xe4\xb9\x8b\xe9\x97\xb4\xe7\x9a\x84\xe4\xba\x92\xe7\x9b\xb8\xe8\xbd\xac\xe6\x8d\xa2\xe9\x80\x9a\xe8\xbf\x87`tensor.cuda`\xe5\x92\x8c`tensor.cpu`\xe6\x96\xb9\xe6\xb3\x95\xe5\xae\x9e\xe7\x8e\xb0\xef\xbc\x8c\xe6\xad\xa4\xe5\xa4\x96\xe8\xbf\x98\xe5\x8f\xaf\xe4\xbb\xa5\xe4\xbd\xbf\xe7\x94\xa8`tensor.to(device)`\xe3\x80\x82Tensor\xe8\xbf\x98\xe6\x9c\x89\xe4\xb8\x80\xe4\xb8\xaa`new`\xe6\x96\xb9\xe6\xb3\x95\xef\xbc\x8c\xe7\x94\xa8\xe6\xb3\x95\xe4\xb8\x8e`t.Tensor`\xe4\xb8\x80\xe6\xa0\xb7\xef\xbc\x8c\xe4\xbc\x9a\xe8\xb0\x83\xe7\x94\xa8\xe8\xaf\xa5tensor\xe5\xaf\xb9\xe5\xba\x94\xe7\xb1\xbb\xe5\x9e\x8b\xe7\x9a\x84\xe6\x9e\x84\xe9\x80\xa0\xe5\x87\xbd\xe6\x95\xb0\xef\xbc\x8c\xe7\x94\x9f\xe6\x88\x90\xe4\xb8\x8e\xe5\xbd\x93\xe5\x89\x8dtensor\xe7\xb1\xbb\xe5\x9e\x8b\xe4\xb8\x80\xe8\x87\xb4\xe7\x9a\x84tensor\xe3\x80\x82`torch.*_like(tensora)` \xe5\x8f\xaf\xe4\xbb\xa5\xe7\x94\x9f\xe6\x88\x90\xe5\x92\x8c`tensora`\xe6\x8b\xa5\xe6\x9c\x89\xe5\x90\x8c\xe6\xa0\xb7\xe5\xb1\x9e\xe6\x80\xa7(\xe7\xb1\xbb\xe5\x9e\x8b\xef\xbc\x8c\xe5\xbd\xa2\xe7\x8a\xb6\xef\xbc\x8ccpu/gpu)\xe7\x9a\x84\xe6\x96\xb0tensor\xe3\x80\x82 `tensor.new_*(new_shape)` \xe6\x96\xb0\xe5\xbb\xba\xe4\xb8\x80\xe4\xb8\xaa\xe4\xb8\x8d\xe5\x90\x8c\xe5\xbd\xa2\xe7\x8a\xb6\xe7\x9a\x84tensor\xe3\x80\x82\n\n#%%\n# \xe8\xae\xbe\xe7\xbd\xae\xe9\xbb\x98\xe8\xae\xa4tensor\xef\xbc\x8c\xe6\xb3\xa8\xe6\x84\x8f\xe5\x8f\x82\xe6\x95\xb0\xe6\x98\xaf\xe5\xad\x97\xe7\xac\xa6\xe4\xb8\xb2\nt.set_default_tensor_type(\'torch.DoubleTensor\')\n\n\n#%%\na = t.Tensor(2,3)\na.dtype # \xe7\x8e\xb0\xe5\x9c\xa8a\xe6\x98\xafDoubleTensor,dtype\xe6\x98\xaffloat64\n\n\n#%%\n# \xe6\x81\xa2\xe5\xa4\x8d\xe4\xb9\x8b\xe5\x89\x8d\xe7\x9a\x84\xe9\xbb\x98\xe8\xae\xa4\xe8\xae\xbe\xe7\xbd\xae\nt.set_default_tensor_type(\'torch.FloatTensor\')\n\n\n#%%\n# \xe6\x8a\x8aa\xe8\xbd\xac\xe6\x88\x90FloatTensor\xef\xbc\x8c\xe7\xad\x89\xe4\xbb\xb7\xe4\xba\x8eb=a.type(t.FloatTensor)\nb = a.float() \nb.dtype\n\n\n#%%\nc = a.type_as(b)\nc\n\n\n#%%\na.new(2,3) # \xe7\xad\x89\xe4\xbb\xb7\xe4\xba\x8etorch.DoubleTensor(2,3)\xef\xbc\x8c\xe5\xbb\xba\xe8\xae\xae\xe4\xbd\xbf\xe7\x94\xa8a.new_tensor\n\n\n#%%\nt.zeros_like(a) #\xe7\xad\x89\xe4\xbb\xb7\xe4\xba\x8et.zeros(a.shape,dtype=a.dtype,device=a.device)\n\n\n#%%\nt.zeros_like(a, dtype=t.int16) #\xe5\x8f\xaf\xe4\xbb\xa5\xe4\xbf\xae\xe6\x94\xb9\xe6\x9f\x90\xe4\xba\x9b\xe5\xb1\x9e\xe6\x80\xa7\n\n\n#%%\nt.rand_like(a)\n\n\n#%%\na.new_ones(4,5, dtype=t.int)\n\n\n#%%\na.new_tensor([3,4]) # \n\n#%% [markdown]\n# #### \xe9\x80\x90\xe5\x85\x83\xe7\xb4\xa0\xe6\x93\x8d\xe4\xbd\x9c\n# \n# \xe8\xbf\x99\xe9\x83\xa8\xe5\x88\x86\xe6\x93\x8d\xe4\xbd\x9c\xe4\xbc\x9a\xe5\xaf\xb9tensor\xe7\x9a\x84\xe6\xaf\x8f\xe4\xb8\x80\xe4\xb8\xaa\xe5\x85\x83\xe7\xb4\xa0(point-wise\xef\xbc\x8c\xe5\x8f\x88\xe5\x90\x8delement-wise)\xe8\xbf\x9b\xe8\xa1\x8c\xe6\x93\x8d\xe4\xbd\x9c\xef\xbc\x8c\xe6\xad\xa4\xe7\xb1\xbb\xe6\x93\x8d\xe4\xbd\x9c\xe7\x9a\x84\xe8\xbe\x93\xe5\x85\xa5\xe4\xb8\x8e\xe8\xbe\x93\xe5\x87\xba\xe5\xbd\xa2\xe7\x8a\xb6\xe4\xb8\x80\xe8\x87\xb4\xe3\x80\x82\xe5\xb8\xb8\xe7\x94\xa8\xe7\x9a\x84\xe6\x93\x8d\xe4\xbd\x9c\xe5\xa6\x82\xe8\xa1\xa83-4\xe6\x89\x80\xe7\xa4\xba\xe3\x80\x82\n# \n# \xe8\xa1\xa83-4: \xe5\xb8\xb8\xe8\xa7\x81\xe7\x9a\x84\xe9\x80\x90\xe5\x85\x83\xe7\xb4\xa0\xe6\x93\x8d\xe4\xbd\x9c\n# \n# |\xe5\x87\xbd\xe6\x95\xb0|\xe5\x8a\x9f\xe8\x83\xbd|\n# |:--:|:--:|\n# |abs/sqrt/div/exp/fmod/log/pow..|\xe7\xbb\x9d\xe5\xaf\xb9\xe5\x80\xbc/\xe5\xb9\xb3\xe6\x96\xb9\xe6\xa0\xb9/\xe9\x99\xa4\xe6\xb3\x95/\xe6\x8c\x87\xe6\x95\xb0/\xe6\xb1\x82\xe4\xbd\x99/\xe6\xb1\x82\xe5\xb9\x82..|\n# |cos/sin/asin/atan2/cosh..|\xe7\x9b\xb8\xe5\x85\xb3\xe4\xb8\x89\xe8\xa7\x92\xe5\x87\xbd\xe6\x95\xb0|\n# |ceil/round/floor/trunc| \xe4\xb8\x8a\xe5\x8f\x96\xe6\x95\xb4/\xe5\x9b\x9b\xe8\x88\x8d\xe4\xba\x94\xe5\x85\xa5/\xe4\xb8\x8b\xe5\x8f\x96\xe6\x95\xb4/\xe5\x8f\xaa\xe4\xbf\x9d\xe7\x95\x99\xe6\x95\xb4\xe6\x95\xb0\xe9\x83\xa8\xe5\x88\x86|\n# |clamp(input, min, max)|\xe8\xb6\x85\xe8\xbf\x87min\xe5\x92\x8cmax\xe9\x83\xa8\xe5\x88\x86\xe6\x88\xaa\xe6\x96\xad|\n# |sigmod/tanh..|\xe6\xbf\x80\xe6\xb4\xbb\xe5\x87\xbd\xe6\x95\xb0\n# \n# \xe5\xaf\xb9\xe4\xba\x8e\xe5\xbe\x88\xe5\xa4\x9a\xe6\x93\x8d\xe4\xbd\x9c\xef\xbc\x8c\xe4\xbe\x8b\xe5\xa6\x82div\xe3\x80\x81mul\xe3\x80\x81pow\xe3\x80\x81fmod\xe7\xad\x89\xef\xbc\x8cPyTorch\xe9\x83\xbd\xe5\xae\x9e\xe7\x8e\xb0\xe4\xba\x86\xe8\xbf\x90\xe7\xae\x97\xe7\xac\xa6\xe9\x87\x8d\xe8\xbd\xbd\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe5\x8f\xaf\xe4\xbb\xa5\xe7\x9b\xb4\xe6\x8e\xa5\xe4\xbd\xbf\xe7\x94\xa8\xe8\xbf\x90\xe7\xae\x97\xe7\xac\xa6\xe3\x80\x82\xe5\xa6\x82`a ** 2` \xe7\xad\x89\xe4\xbb\xb7\xe4\xba\x8e`torch.pow(a,2)`, `a * 2`\xe7\xad\x89\xe4\xbb\xb7\xe4\xba\x8e`torch.mul(a,2)`\xe3\x80\x82\n# \n# \xe5\x85\xb6\xe4\xb8\xad`clamp(x, min, max)`\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe6\xbb\xa1\xe8\xb6\xb3\xe4\xbb\xa5\xe4\xb8\x8b\xe5\x85\xac\xe5\xbc\x8f\xef\xbc\x9a\n# $$\n# y_i =\n# \\begin{cases}\n# min,  & \\text{if  } x_i \\lt min \\\\\n# x_i,  & \\text{if  } min \\le x_i \\le max  \\\\\n# max,  & \\text{if  } x_i \\gt max\\\\\n# \\end{cases}\n# $$\n# `clamp`\xe5\xb8\xb8\xe7\x94\xa8\xe5\x9c\xa8\xe6\x9f\x90\xe4\xba\x9b\xe9\x9c\x80\xe8\xa6\x81\xe6\xaf\x94\xe8\xbe\x83\xe5\xa4\xa7\xe5\xb0\x8f\xe7\x9a\x84\xe5\x9c\xb0\xe6\x96\xb9\xef\xbc\x8c\xe5\xa6\x82\xe5\x8f\x96\xe4\xb8\x80\xe4\xb8\xaatensor\xe7\x9a\x84\xe6\xaf\x8f\xe4\xb8\xaa\xe5\x85\x83\xe7\xb4\xa0\xe4\xb8\x8e\xe5\x8f\xa6\xe4\xb8\x80\xe4\xb8\xaa\xe6\x95\xb0\xe7\x9a\x84\xe8\xbe\x83\xe5\xa4\xa7\xe5\x80\xbc\xe3\x80\x82\n\n#%%\na = t.arange(0, 6).view(2, 3)\nt.cos(a)\n\n\n#%%\na % 3 # \xe7\xad\x89\xe4\xbb\xb7\xe4\xba\x8et.fmod(a, 3)\n\n\n#%%\na ** 2 # \xe7\xad\x89\xe4\xbb\xb7\xe4\xba\x8et.pow(a, 2)\n\n\n#%%\n# \xe5\x8f\x96a\xe4\xb8\xad\xe7\x9a\x84\xe6\xaf\x8f\xe4\xb8\x80\xe4\xb8\xaa\xe5\x85\x83\xe7\xb4\xa0\xe4\xb8\x8e3\xe7\x9b\xb8\xe6\xaf\x94\xe8\xbe\x83\xe5\xa4\xa7\xe7\x9a\x84\xe4\xb8\x80\xe4\xb8\xaa (\xe5\xb0\x8f\xe4\xba\x8e3\xe7\x9a\x84\xe6\x88\xaa\xe6\x96\xad\xe6\x88\x903)\nprint(a)\nt.clamp(a, min=3)\n\n\n#%%\nb = a.sin_() # \xe6\x95\x88\xe6\x9e\x9c\xe5\x90\x8c a = a.sin();b=a ,\xe4\xbd\x86\xe6\x98\xaf\xe6\x9b\xb4\xe9\xab\x98\xe6\x95\x88\xe8\x8a\x82\xe7\x9c\x81\xe6\x98\xbe\xe5\xad\x98\na\n\n#%% [markdown]\n# ####  \xe5\xbd\x92\xe5\xb9\xb6\xe6\x93\x8d\xe4\xbd\x9c \n# \xe6\xad\xa4\xe7\xb1\xbb\xe6\x93\x8d\xe4\xbd\x9c\xe4\xbc\x9a\xe4\xbd\xbf\xe8\xbe\x93\xe5\x87\xba\xe5\xbd\xa2\xe7\x8a\xb6\xe5\xb0\x8f\xe4\xba\x8e\xe8\xbe\x93\xe5\x85\xa5\xe5\xbd\xa2\xe7\x8a\xb6\xef\xbc\x8c\xe5\xb9\xb6\xe5\x8f\xaf\xe4\xbb\xa5\xe6\xb2\xbf\xe7\x9d\x80\xe6\x9f\x90\xe4\xb8\x80\xe7\xbb\xb4\xe5\xba\xa6\xe8\xbf\x9b\xe8\xa1\x8c\xe6\x8c\x87\xe5\xae\x9a\xe6\x93\x8d\xe4\xbd\x9c\xe3\x80\x82\xe5\xa6\x82\xe5\x8a\xa0\xe6\xb3\x95`sum`\xef\xbc\x8c\xe6\x97\xa2\xe5\x8f\xaf\xe4\xbb\xa5\xe8\xae\xa1\xe7\xae\x97\xe6\x95\xb4\xe4\xb8\xaatensor\xe7\x9a\x84\xe5\x92\x8c\xef\xbc\x8c\xe4\xb9\x9f\xe5\x8f\xaf\xe4\xbb\xa5\xe8\xae\xa1\xe7\xae\x97tensor\xe4\xb8\xad\xe6\xaf\x8f\xe4\xb8\x80\xe8\xa1\x8c\xe6\x88\x96\xe6\xaf\x8f\xe4\xb8\x80\xe5\x88\x97\xe7\x9a\x84\xe5\x92\x8c\xe3\x80\x82\xe5\xb8\xb8\xe7\x94\xa8\xe7\x9a\x84\xe5\xbd\x92\xe5\xb9\xb6\xe6\x93\x8d\xe4\xbd\x9c\xe5\xa6\x82\xe8\xa1\xa83-5\xe6\x89\x80\xe7\xa4\xba\xe3\x80\x82\n# \n# \xe8\xa1\xa83-5: \xe5\xb8\xb8\xe7\x94\xa8\xe5\xbd\x92\xe5\xb9\xb6\xe6\x93\x8d\xe4\xbd\x9c\n# \n# |\xe5\x87\xbd\xe6\x95\xb0|\xe5\x8a\x9f\xe8\x83\xbd|\n# |:---:|:---:|\n# |mean/sum/median/mode|\xe5\x9d\x87\xe5\x80\xbc/\xe5\x92\x8c/\xe4\xb8\xad\xe4\xbd\x8d\xe6\x95\xb0/\xe4\xbc\x97\xe6\x95\xb0|\n# |norm/dist|\xe8\x8c\x83\xe6\x95\xb0/\xe8\xb7\x9d\xe7\xa6\xbb|\n# |std/var|\xe6\xa0\x87\xe5\x87\x86\xe5\xb7\xae/\xe6\x96\xb9\xe5\xb7\xae|\n# |cumsum/cumprod|\xe7\xb4\xaf\xe5\x8a\xa0/\xe7\xb4\xaf\xe4\xb9\x98|\n# \n# \xe4\xbb\xa5\xe4\xb8\x8a\xe5\xa4\xa7\xe5\xa4\x9a\xe6\x95\xb0\xe5\x87\xbd\xe6\x95\xb0\xe9\x83\xbd\xe6\x9c\x89\xe4\xb8\x80\xe4\xb8\xaa\xe5\x8f\x82\xe6\x95\xb0**`dim`**\xef\xbc\x8c\xe7\x94\xa8\xe6\x9d\xa5\xe6\x8c\x87\xe5\xae\x9a\xe8\xbf\x99\xe4\xba\x9b\xe6\x93\x8d\xe4\xbd\x9c\xe6\x98\xaf\xe5\x9c\xa8\xe5\x93\xaa\xe4\xb8\xaa\xe7\xbb\xb4\xe5\xba\xa6\xe4\xb8\x8a\xe6\x89\xa7\xe8\xa1\x8c\xe7\x9a\x84\xe3\x80\x82\xe5\x85\xb3\xe4\xba\x8edim(\xe5\xaf\xb9\xe5\xba\x94\xe4\xba\x8eNumpy\xe4\xb8\xad\xe7\x9a\x84axis)\xe7\x9a\x84\xe8\xa7\xa3\xe9\x87\x8a\xe4\xbc\x97\xe8\xaf\xb4\xe7\xba\xb7\xe7\xba\xad\xef\xbc\x8c\xe8\xbf\x99\xe9\x87\x8c\xe6\x8f\x90\xe4\xbe\x9b\xe4\xb8\x80\xe4\xb8\xaa\xe7\xae\x80\xe5\x8d\x95\xe7\x9a\x84\xe8\xae\xb0\xe5\xbf\x86\xe6\x96\xb9\xe5\xbc\x8f\xef\xbc\x9a\n# \n# \xe5\x81\x87\xe8\xae\xbe\xe8\xbe\x93\xe5\x85\xa5\xe7\x9a\x84\xe5\xbd\xa2\xe7\x8a\xb6\xe6\x98\xaf(m, n, k)\n# \n# - \xe5\xa6\x82\xe6\x9e\x9c\xe6\x8c\x87\xe5\xae\x9adim=0\xef\xbc\x8c\xe8\xbe\x93\xe5\x87\xba\xe7\x9a\x84\xe5\xbd\xa2\xe7\x8a\xb6\xe5\xb0\xb1\xe6\x98\xaf(1, n, k)\xe6\x88\x96\xe8\x80\x85(n, k)\n# - \xe5\xa6\x82\xe6\x9e\x9c\xe6\x8c\x87\xe5\xae\x9adim=1\xef\xbc\x8c\xe8\xbe\x93\xe5\x87\xba\xe7\x9a\x84\xe5\xbd\xa2\xe7\x8a\xb6\xe5\xb0\xb1\xe6\x98\xaf(m, 1, k)\xe6\x88\x96\xe8\x80\x85(m, k)\n# - \xe5\xa6\x82\xe6\x9e\x9c\xe6\x8c\x87\xe5\xae\x9adim=2\xef\xbc\x8c\xe8\xbe\x93\xe5\x87\xba\xe7\x9a\x84\xe5\xbd\xa2\xe7\x8a\xb6\xe5\xb0\xb1\xe6\x98\xaf(m, n, 1)\xe6\x88\x96\xe8\x80\x85(m, n)\n# \n# size\xe4\xb8\xad\xe6\x98\xaf\xe5\x90\xa6\xe6\x9c\x89""1""\xef\xbc\x8c\xe5\x8f\x96\xe5\x86\xb3\xe4\xba\x8e\xe5\x8f\x82\xe6\x95\xb0`keepdim`\xef\xbc\x8c`keepdim=True`\xe4\xbc\x9a\xe4\xbf\x9d\xe7\x95\x99\xe7\xbb\xb4\xe5\xba\xa6`1`\xe3\x80\x82\xe6\xb3\xa8\xe6\x84\x8f\xef\xbc\x8c\xe4\xbb\xa5\xe4\xb8\x8a\xe5\x8f\xaa\xe6\x98\xaf\xe7\xbb\x8f\xe9\xaa\x8c\xe6\x80\xbb\xe7\xbb\x93\xef\xbc\x8c\xe5\xb9\xb6\xe9\x9d\x9e\xe6\x89\x80\xe6\x9c\x89\xe5\x87\xbd\xe6\x95\xb0\xe9\x83\xbd\xe7\xac\xa6\xe5\x90\x88\xe8\xbf\x99\xe7\xa7\x8d\xe5\xbd\xa2\xe7\x8a\xb6\xe5\x8f\x98\xe5\x8c\x96\xe6\x96\xb9\xe5\xbc\x8f\xef\xbc\x8c\xe5\xa6\x82`cumsum`\xe3\x80\x82\n\n#%%\nb = t.ones(2, 3)\nb.sum(dim = 0, keepdim=True)\n\n\n#%%\n# keepdim=False\xef\xbc\x8c\xe4\xb8\x8d\xe4\xbf\x9d\xe7\x95\x99\xe7\xbb\xb4\xe5\xba\xa6""1""\xef\xbc\x8c\xe6\xb3\xa8\xe6\x84\x8f\xe5\xbd\xa2\xe7\x8a\xb6\nb.sum(dim=0, keepdim=False)\n\n\n#%%\nb.sum(dim=1)\n\n\n#%%\na = t.arange(0, 6).view(2, 3)\nprint(a)\na.cumsum(dim=1) # \xe6\xb2\xbf\xe7\x9d\x80\xe8\xa1\x8c\xe7\xb4\xaf\xe5\x8a\xa0\n\n#%% [markdown]\n# #### \xe6\xaf\x94\xe8\xbe\x83\n# \xe6\xaf\x94\xe8\xbe\x83\xe5\x87\xbd\xe6\x95\xb0\xe4\xb8\xad\xe6\x9c\x89\xe4\xb8\x80\xe4\xba\x9b\xe6\x98\xaf\xe9\x80\x90\xe5\x85\x83\xe7\xb4\xa0\xe6\xaf\x94\xe8\xbe\x83\xef\xbc\x8c\xe6\x93\x8d\xe4\xbd\x9c\xe7\xb1\xbb\xe4\xbc\xbc\xe4\xba\x8e\xe9\x80\x90\xe5\x85\x83\xe7\xb4\xa0\xe6\x93\x8d\xe4\xbd\x9c\xef\xbc\x8c\xe8\xbf\x98\xe6\x9c\x89\xe4\xb8\x80\xe4\xba\x9b\xe5\x88\x99\xe7\xb1\xbb\xe4\xbc\xbc\xe4\xba\x8e\xe5\xbd\x92\xe5\xb9\xb6\xe6\x93\x8d\xe4\xbd\x9c\xe3\x80\x82\xe5\xb8\xb8\xe7\x94\xa8\xe6\xaf\x94\xe8\xbe\x83\xe5\x87\xbd\xe6\x95\xb0\xe5\xa6\x82\xe8\xa1\xa83-6\xe6\x89\x80\xe7\xa4\xba\xe3\x80\x82\n# \n# \xe8\xa1\xa83-6: \xe5\xb8\xb8\xe7\x94\xa8\xe6\xaf\x94\xe8\xbe\x83\xe5\x87\xbd\xe6\x95\xb0\n# \n# |\xe5\x87\xbd\xe6\x95\xb0|\xe5\x8a\x9f\xe8\x83\xbd|\n# |:--:|:--:|\n# |gt/lt/ge/le/eq/ne|\xe5\xa4\xa7\xe4\xba\x8e/\xe5\xb0\x8f\xe4\xba\x8e/\xe5\xa4\xa7\xe4\xba\x8e\xe7\xad\x89\xe4\xba\x8e/\xe5\xb0\x8f\xe4\xba\x8e\xe7\xad\x89\xe4\xba\x8e/\xe7\xad\x89\xe4\xba\x8e/\xe4\xb8\x8d\xe7\xad\x89|\n# |topk|\xe6\x9c\x80\xe5\xa4\xa7\xe7\x9a\x84k\xe4\xb8\xaa\xe6\x95\xb0|\n# |sort|\xe6\x8e\x92\xe5\xba\x8f|\n# |max/min|\xe6\xaf\x94\xe8\xbe\x83\xe4\xb8\xa4\xe4\xb8\xaatensor\xe6\x9c\x80\xe5\xa4\xa7\xe6\x9c\x80\xe5\xb0\x8f\xe5\x80\xbc|\n# \n# \xe8\xa1\xa8\xe4\xb8\xad\xe7\xac\xac\xe4\xb8\x80\xe8\xa1\x8c\xe7\x9a\x84\xe6\xaf\x94\xe8\xbe\x83\xe6\x93\x8d\xe4\xbd\x9c\xe5\xb7\xb2\xe7\xbb\x8f\xe5\xae\x9e\xe7\x8e\xb0\xe4\xba\x86\xe8\xbf\x90\xe7\xae\x97\xe7\xac\xa6\xe9\x87\x8d\xe8\xbd\xbd\xef\xbc\x8c\xe5\x9b\xa0\xe6\xad\xa4\xe5\x8f\xaf\xe4\xbb\xa5\xe4\xbd\xbf\xe7\x94\xa8`a>=b`\xe3\x80\x81`a>b`\xe3\x80\x81`a!=b`\xe3\x80\x81`a==b`\xef\xbc\x8c\xe5\x85\xb6\xe8\xbf\x94\xe5\x9b\x9e\xe7\xbb\x93\xe6\x9e\x9c\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa`ByteTensor`\xef\xbc\x8c\xe5\x8f\xaf\xe7\x94\xa8\xe6\x9d\xa5\xe9\x80\x89\xe5\x8f\x96\xe5\x85\x83\xe7\xb4\xa0\xe3\x80\x82max/min\xe8\xbf\x99\xe4\xb8\xa4\xe4\xb8\xaa\xe6\x93\x8d\xe4\xbd\x9c\xe6\xaf\x94\xe8\xbe\x83\xe7\x89\xb9\xe6\xae\x8a\xef\xbc\x8c\xe4\xbb\xa5max\xe6\x9d\xa5\xe8\xaf\xb4\xef\xbc\x8c\xe5\xae\x83\xe6\x9c\x89\xe4\xbb\xa5\xe4\xb8\x8b\xe4\xb8\x89\xe7\xa7\x8d\xe4\xbd\xbf\xe7\x94\xa8\xe6\x83\x85\xe5\x86\xb5\xef\xbc\x9a\n# - t.max(tensor)\xef\xbc\x9a\xe8\xbf\x94\xe5\x9b\x9etensor\xe4\xb8\xad\xe6\x9c\x80\xe5\xa4\xa7\xe7\x9a\x84\xe4\xb8\x80\xe4\xb8\xaa\xe6\x95\xb0\n# - t.max(tensor,dim)\xef\xbc\x9a\xe6\x8c\x87\xe5\xae\x9a\xe7\xbb\xb4\xe4\xb8\x8a\xe6\x9c\x80\xe5\xa4\xa7\xe7\x9a\x84\xe6\x95\xb0\xef\xbc\x8c\xe8\xbf\x94\xe5\x9b\x9etensor\xe5\x92\x8c\xe4\xb8\x8b\xe6\xa0\x87\n# - t.max(tensor1, tensor2): \xe6\xaf\x94\xe8\xbe\x83\xe4\xb8\xa4\xe4\xb8\xaatensor\xe7\x9b\xb8\xe6\xaf\x94\xe8\xbe\x83\xe5\xa4\xa7\xe7\x9a\x84\xe5\x85\x83\xe7\xb4\xa0\n# \n# \xe8\x87\xb3\xe4\xba\x8e\xe6\xaf\x94\xe8\xbe\x83\xe4\xb8\x80\xe4\xb8\xaatensor\xe5\x92\x8c\xe4\xb8\x80\xe4\xb8\xaa\xe6\x95\xb0\xef\xbc\x8c\xe5\x8f\xaf\xe4\xbb\xa5\xe4\xbd\xbf\xe7\x94\xa8clamp\xe5\x87\xbd\xe6\x95\xb0\xe3\x80\x82\xe4\xb8\x8b\xe9\x9d\xa2\xe4\xb8\xbe\xe4\xbe\x8b\xe8\xaf\xb4\xe6\x98\x8e\xe3\x80\x82\n\n#%%\na = t.linspace(0, 15, 6).view(2, 3)\na\n\n\n#%%\nb = t.linspace(15, 0, 6).view(2, 3)\nb\n\n\n#%%\na>b\n\n\n#%%\na[a>b] # a\xe4\xb8\xad\xe5\xa4\xa7\xe4\xba\x8eb\xe7\x9a\x84\xe5\x85\x83\xe7\xb4\xa0\n\n\n#%%\nt.max(a)\n\n\n#%%\nt.max(b, dim=1) \n# \xe7\xac\xac\xe4\xb8\x80\xe4\xb8\xaa\xe8\xbf\x94\xe5\x9b\x9e\xe5\x80\xbc\xe7\x9a\x8415\xe5\x92\x8c6\xe5\x88\x86\xe5\x88\xab\xe8\xa1\xa8\xe7\xa4\xba\xe7\xac\xac0\xe8\xa1\x8c\xe5\x92\x8c\xe7\xac\xac1\xe8\xa1\x8c\xe6\x9c\x80\xe5\xa4\xa7\xe7\x9a\x84\xe5\x85\x83\xe7\xb4\xa0\n# \xe7\xac\xac\xe4\xba\x8c\xe4\xb8\xaa\xe8\xbf\x94\xe5\x9b\x9e\xe5\x80\xbc\xe7\x9a\x840\xe5\x92\x8c0\xe8\xa1\xa8\xe7\xa4\xba\xe4\xb8\x8a\xe8\xbf\xb0\xe6\x9c\x80\xe5\xa4\xa7\xe7\x9a\x84\xe6\x95\xb0\xe6\x98\xaf\xe8\xaf\xa5\xe8\xa1\x8c\xe7\xac\xac0\xe4\xb8\xaa\xe5\x85\x83\xe7\xb4\xa0\n\n\n#%%\nt.max(a,b)\n\n\n#%%\n# \xe6\xaf\x94\xe8\xbe\x83a\xe5\x92\x8c10\xe8\xbe\x83\xe5\xa4\xa7\xe7\x9a\x84\xe5\x85\x83\xe7\xb4\xa0\nt.clamp(a, min=10)\n\n#%% [markdown]\n# #### \xe7\xba\xbf\xe6\x80\xa7\xe4\xbb\xa3\xe6\x95\xb0\n# \n# PyTorch\xe7\x9a\x84\xe7\xba\xbf\xe6\x80\xa7\xe5\x87\xbd\xe6\x95\xb0\xe4\xb8\xbb\xe8\xa6\x81\xe5\xb0\x81\xe8\xa3\x85\xe4\xba\x86Blas\xe5\x92\x8cLapack\xef\xbc\x8c\xe5\x85\xb6\xe7\x94\xa8\xe6\xb3\x95\xe5\x92\x8c\xe6\x8e\xa5\xe5\x8f\xa3\xe9\x83\xbd\xe4\xb8\x8e\xe4\xb9\x8b\xe7\xb1\xbb\xe4\xbc\xbc\xe3\x80\x82\xe5\xb8\xb8\xe7\x94\xa8\xe7\x9a\x84\xe7\xba\xbf\xe6\x80\xa7\xe4\xbb\xa3\xe6\x95\xb0\xe5\x87\xbd\xe6\x95\xb0\xe5\xa6\x82\xe8\xa1\xa83-7\xe6\x89\x80\xe7\xa4\xba\xe3\x80\x82\n# \n# \xe8\xa1\xa83-7: \xe5\xb8\xb8\xe7\x94\xa8\xe7\x9a\x84\xe7\xba\xbf\xe6\x80\xa7\xe4\xbb\xa3\xe6\x95\xb0\xe5\x87\xbd\xe6\x95\xb0\n# \n# |\xe5\x87\xbd\xe6\x95\xb0|\xe5\x8a\x9f\xe8\x83\xbd|\n# |:---:|:---:|\n# |trace|\xe5\xaf\xb9\xe8\xa7\x92\xe7\xba\xbf\xe5\x85\x83\xe7\xb4\xa0\xe4\xb9\x8b\xe5\x92\x8c(\xe7\x9f\xa9\xe9\x98\xb5\xe7\x9a\x84\xe8\xbf\xb9)|\n# |diag|\xe5\xaf\xb9\xe8\xa7\x92\xe7\xba\xbf\xe5\x85\x83\xe7\xb4\xa0|\n# |triu/tril|\xe7\x9f\xa9\xe9\x98\xb5\xe7\x9a\x84\xe4\xb8\x8a\xe4\xb8\x89\xe8\xa7\x92/\xe4\xb8\x8b\xe4\xb8\x89\xe8\xa7\x92\xef\xbc\x8c\xe5\x8f\xaf\xe6\x8c\x87\xe5\xae\x9a\xe5\x81\x8f\xe7\xa7\xbb\xe9\x87\x8f|\n# |mm/bmm|\xe7\x9f\xa9\xe9\x98\xb5\xe4\xb9\x98\xe6\xb3\x95\xef\xbc\x8cbatch\xe7\x9a\x84\xe7\x9f\xa9\xe9\x98\xb5\xe4\xb9\x98\xe6\xb3\x95|\n# |addmm/addbmm/addmv/addr/badbmm..|\xe7\x9f\xa9\xe9\x98\xb5\xe8\xbf\x90\xe7\xae\x97\n# |t|\xe8\xbd\xac\xe7\xbd\xae|\n# |dot/cross|\xe5\x86\x85\xe7\xa7\xaf/\xe5\xa4\x96\xe7\xa7\xaf\n# |inverse|\xe6\xb1\x82\xe9\x80\x86\xe7\x9f\xa9\xe9\x98\xb5\n# |svd|\xe5\xa5\x87\xe5\xbc\x82\xe5\x80\xbc\xe5\x88\x86\xe8\xa7\xa3\n# \n# \xe5\x85\xb7\xe4\xbd\x93\xe4\xbd\xbf\xe7\x94\xa8\xe8\xaf\xb4\xe6\x98\x8e\xe8\xaf\xb7\xe5\x8f\x82\xe8\xa7\x81\xe5\xae\x98\xe6\x96\xb9\xe6\x96\x87\xe6\xa1\xa3[^3]\xef\xbc\x8c\xe9\x9c\x80\xe8\xa6\x81\xe6\xb3\xa8\xe6\x84\x8f\xe7\x9a\x84\xe6\x98\xaf\xef\xbc\x8c\xe7\x9f\xa9\xe9\x98\xb5\xe7\x9a\x84\xe8\xbd\xac\xe7\xbd\xae\xe4\xbc\x9a\xe5\xaf\xbc\xe8\x87\xb4\xe5\xad\x98\xe5\x82\xa8\xe7\xa9\xba\xe9\x97\xb4\xe4\xb8\x8d\xe8\xbf\x9e\xe7\xbb\xad\xef\xbc\x8c\xe9\x9c\x80\xe8\xb0\x83\xe7\x94\xa8\xe5\xae\x83\xe7\x9a\x84`.contiguous`\xe6\x96\xb9\xe6\xb3\x95\xe5\xb0\x86\xe5\x85\xb6\xe8\xbd\xac\xe4\xb8\xba\xe8\xbf\x9e\xe7\xbb\xad\xe3\x80\x82\n# [^3]: http://pytorch.org/docs/torch.html#blas-and-lapack-operations\n\n#%%\nb = a.t()\nb.is_contiguous()\n\n\n#%%\nb.contiguous()\n\n#%% [markdown]\n# ### 3.1.2 Tensor\xe5\x92\x8cNumpy\n# \n# Tensor\xe5\x92\x8cNumpy\xe6\x95\xb0\xe7\xbb\x84\xe4\xb9\x8b\xe9\x97\xb4\xe5\x85\xb7\xe6\x9c\x89\xe5\xbe\x88\xe9\xab\x98\xe7\x9a\x84\xe7\x9b\xb8\xe4\xbc\xbc\xe6\x80\xa7\xef\xbc\x8c\xe5\xbd\xbc\xe6\xad\xa4\xe4\xb9\x8b\xe9\x97\xb4\xe7\x9a\x84\xe4\xba\x92\xe6\x93\x8d\xe4\xbd\x9c\xe4\xb9\x9f\xe9\x9d\x9e\xe5\xb8\xb8\xe7\xae\x80\xe5\x8d\x95\xe9\xab\x98\xe6\x95\x88\xe3\x80\x82\xe9\x9c\x80\xe8\xa6\x81\xe6\xb3\xa8\xe6\x84\x8f\xe7\x9a\x84\xe6\x98\xaf\xef\xbc\x8cNumpy\xe5\x92\x8cTensor\xe5\x85\xb1\xe4\xba\xab\xe5\x86\x85\xe5\xad\x98\xe3\x80\x82\xe7\x94\xb1\xe4\xba\x8eNumpy\xe5\x8e\x86\xe5\x8f\xb2\xe6\x82\xa0\xe4\xb9\x85\xef\xbc\x8c\xe6\x94\xaf\xe6\x8c\x81\xe4\xb8\xb0\xe5\xaf\x8c\xe7\x9a\x84\xe6\x93\x8d\xe4\xbd\x9c\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe5\xbd\x93\xe9\x81\x87\xe5\x88\xb0Tensor\xe4\xb8\x8d\xe6\x94\xaf\xe6\x8c\x81\xe7\x9a\x84\xe6\x93\x8d\xe4\xbd\x9c\xe6\x97\xb6\xef\xbc\x8c\xe5\x8f\xaf\xe5\x85\x88\xe8\xbd\xac\xe6\x88\x90Numpy\xe6\x95\xb0\xe7\xbb\x84\xef\xbc\x8c\xe5\xa4\x84\xe7\x90\x86\xe5\x90\x8e\xe5\x86\x8d\xe8\xbd\xac\xe5\x9b\x9etensor\xef\xbc\x8c\xe5\x85\xb6\xe8\xbd\xac\xe6\x8d\xa2\xe5\xbc\x80\xe9\x94\x80\xe5\xbe\x88\xe5\xb0\x8f\xe3\x80\x82\n\n#%%\nimport numpy as np\na = np.ones([2, 3],dtype=np.float32)\na\n\n\n#%%\nb = t.from_numpy(a)\nb\n\n\n#%%\nb = t.Tensor(a) # \xe4\xb9\x9f\xe5\x8f\xaf\xe4\xbb\xa5\xe7\x9b\xb4\xe6\x8e\xa5\xe5\xb0\x86numpy\xe5\xaf\xb9\xe8\xb1\xa1\xe4\xbc\xa0\xe5\x85\xa5Tensor\nb\n\n\n#%%\na[0, 1]=100\nb\n\n\n#%%\nc = b.numpy() # a, b, c\xe4\xb8\x89\xe4\xb8\xaa\xe5\xaf\xb9\xe8\xb1\xa1\xe5\x85\xb1\xe4\xba\xab\xe5\x86\x85\xe5\xad\x98\nc\n\n#%% [markdown]\n# **\xe6\xb3\xa8\xe6\x84\x8f**\xef\xbc\x9a \xe5\xbd\x93numpy\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe7\xb1\xbb\xe5\x9e\x8b\xe5\x92\x8cTensor\xe7\x9a\x84\xe7\xb1\xbb\xe5\x9e\x8b\xe4\xb8\x8d\xe4\xb8\x80\xe6\xa0\xb7\xe7\x9a\x84\xe6\x97\xb6\xe5\x80\x99\xef\xbc\x8c\xe6\x95\xb0\xe6\x8d\xae\xe4\xbc\x9a\xe8\xa2\xab\xe5\xa4\x8d\xe5\x88\xb6\xef\xbc\x8c\xe4\xb8\x8d\xe4\xbc\x9a\xe5\x85\xb1\xe4\xba\xab\xe5\x86\x85\xe5\xad\x98\xe3\x80\x82\n\n#%%\na = np.ones([2, 3])\n# \xe6\xb3\xa8\xe6\x84\x8f\xe5\x92\x8c\xe4\xb8\x8a\xe9\x9d\xa2\xe7\x9a\x84a\xe7\x9a\x84\xe5\x8c\xba\xe5\x88\xab\xef\xbc\x88dtype\xe4\xb8\x8d\xe6\x98\xaffloat32\xef\xbc\x89\na.dtype\n\n\n#%%\nb = t.Tensor(a) # \xe6\xad\xa4\xe5\xa4\x84\xe8\xbf\x9b\xe8\xa1\x8c\xe6\x8b\xb7\xe8\xb4\x9d\xef\xbc\x8c\xe4\xb8\x8d\xe5\x85\xb1\xe4\xba\xab\xe5\x86\x85\xe5\xad\x98\nb.dtype\n\n\n#%%\nc = t.from_numpy(a) # \xe6\xb3\xa8\xe6\x84\x8fc\xe7\x9a\x84\xe7\xb1\xbb\xe5\x9e\x8b\xef\xbc\x88DoubleTensor\xef\xbc\x89\nc\n\n\n#%%\na[0, 1] = 100\nb # b\xe4\xb8\x8ea\xe4\xb8\x8d\xe5\x85\xb1\xe4\xba\xab\xe5\x86\x85\xe5\xad\x98\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe5\x8d\xb3\xe4\xbd\xbfa\xe6\x94\xb9\xe5\x8f\x98\xe4\xba\x86\xef\xbc\x8cb\xe4\xb9\x9f\xe4\xb8\x8d\xe5\x8f\x98\n\n\n#%%\nc # c\xe4\xb8\x8ea\xe5\x85\xb1\xe4\xba\xab\xe5\x86\x85\xe5\xad\x98\n\n#%% [markdown]\n# **\xe6\xb3\xa8\xe6\x84\x8f\xef\xbc\x9a** \xe4\xb8\x8d\xe8\xae\xba\xe8\xbe\x93\xe5\x85\xa5\xe7\x9a\x84\xe7\xb1\xbb\xe5\x9e\x8b\xe6\x98\xaf\xe4\xbb\x80\xe4\xb9\x88\xef\xbc\x8ct.tensor\xe9\x83\xbd\xe4\xbc\x9a\xe8\xbf\x9b\xe8\xa1\x8c\xe6\x95\xb0\xe6\x8d\xae\xe6\x8b\xb7\xe8\xb4\x9d\xef\xbc\x8c\xe4\xb8\x8d\xe4\xbc\x9a\xe5\x85\xb1\xe4\xba\xab\xe5\x86\x85\xe5\xad\x98\n\n#%%\ntensor = t.tensor(a) \n\n\n#%%\ntensor[0,0]=0\na\n\n#%% [markdown]\n# \xe5\xb9\xbf\xe6\x92\xad\xe6\xb3\x95\xe5\x88\x99(broadcast)\xe6\x98\xaf\xe7\xa7\x91\xe5\xad\xa6\xe8\xbf\x90\xe7\xae\x97\xe4\xb8\xad\xe7\xbb\x8f\xe5\xb8\xb8\xe4\xbd\xbf\xe7\x94\xa8\xe7\x9a\x84\xe4\xb8\x80\xe4\xb8\xaa\xe6\x8a\x80\xe5\xb7\xa7\xef\xbc\x8c\xe5\xae\x83\xe5\x9c\xa8\xe5\xbf\xab\xe9\x80\x9f\xe6\x89\xa7\xe8\xa1\x8c\xe5\x90\x91\xe9\x87\x8f\xe5\x8c\x96\xe7\x9a\x84\xe5\x90\x8c\xe6\x97\xb6\xe4\xb8\x8d\xe4\xbc\x9a\xe5\x8d\xa0\xe7\x94\xa8\xe9\xa2\x9d\xe5\xa4\x96\xe7\x9a\x84\xe5\x86\x85\xe5\xad\x98/\xe6\x98\xbe\xe5\xad\x98\xe3\x80\x82\n# Numpy\xe7\x9a\x84\xe5\xb9\xbf\xe6\x92\xad\xe6\xb3\x95\xe5\x88\x99\xe5\xae\x9a\xe4\xb9\x89\xe5\xa6\x82\xe4\xb8\x8b\xef\xbc\x9a\n# \n# - \xe8\xae\xa9\xe6\x89\x80\xe6\x9c\x89\xe8\xbe\x93\xe5\x85\xa5\xe6\x95\xb0\xe7\xbb\x84\xe9\x83\xbd\xe5\x90\x91\xe5\x85\xb6\xe4\xb8\xadshape\xe6\x9c\x80\xe9\x95\xbf\xe7\x9a\x84\xe6\x95\xb0\xe7\xbb\x84\xe7\x9c\x8b\xe9\xbd\x90\xef\xbc\x8cshape\xe4\xb8\xad\xe4\xb8\x8d\xe8\xb6\xb3\xe7\x9a\x84\xe9\x83\xa8\xe5\x88\x86\xe9\x80\x9a\xe8\xbf\x87\xe5\x9c\xa8\xe5\x89\x8d\xe9\x9d\xa2\xe5\x8a\xa01\xe8\xa1\xa5\xe9\xbd\x90\n# - \xe4\xb8\xa4\xe4\xb8\xaa\xe6\x95\xb0\xe7\xbb\x84\xe8\xa6\x81\xe4\xb9\x88\xe5\x9c\xa8\xe6\x9f\x90\xe4\xb8\x80\xe4\xb8\xaa\xe7\xbb\xb4\xe5\xba\xa6\xe7\x9a\x84\xe9\x95\xbf\xe5\xba\xa6\xe4\xb8\x80\xe8\x87\xb4\xef\xbc\x8c\xe8\xa6\x81\xe4\xb9\x88\xe5\x85\xb6\xe4\xb8\xad\xe4\xb8\x80\xe4\xb8\xaa\xe4\xb8\xba1\xef\xbc\x8c\xe5\x90\xa6\xe5\x88\x99\xe4\xb8\x8d\xe8\x83\xbd\xe8\xae\xa1\xe7\xae\x97 \n# - \xe5\xbd\x93\xe8\xbe\x93\xe5\x85\xa5\xe6\x95\xb0\xe7\xbb\x84\xe7\x9a\x84\xe6\x9f\x90\xe4\xb8\xaa\xe7\xbb\xb4\xe5\xba\xa6\xe7\x9a\x84\xe9\x95\xbf\xe5\xba\xa6\xe4\xb8\xba1\xe6\x97\xb6\xef\xbc\x8c\xe8\xae\xa1\xe7\xae\x97\xe6\x97\xb6\xe6\xb2\xbf\xe6\xad\xa4\xe7\xbb\xb4\xe5\xba\xa6\xe5\xa4\x8d\xe5\x88\xb6\xe6\x89\xa9\xe5\x85\x85\xe6\x88\x90\xe4\xb8\x80\xe6\xa0\xb7\xe7\x9a\x84\xe5\xbd\xa2\xe7\x8a\xb6\n# \n# PyTorch\xe5\xbd\x93\xe5\x89\x8d\xe5\xb7\xb2\xe7\xbb\x8f\xe6\x94\xaf\xe6\x8c\x81\xe4\xba\x86\xe8\x87\xaa\xe5\x8a\xa8\xe5\xb9\xbf\xe6\x92\xad\xe6\xb3\x95\xe5\x88\x99\xef\xbc\x8c\xe4\xbd\x86\xe6\x98\xaf\xe7\xac\x94\xe8\x80\x85\xe8\xbf\x98\xe6\x98\xaf\xe5\xbb\xba\xe8\xae\xae\xe8\xaf\xbb\xe8\x80\x85\xe9\x80\x9a\xe8\xbf\x87\xe4\xbb\xa5\xe4\xb8\x8b\xe4\xb8\xa4\xe4\xb8\xaa\xe5\x87\xbd\xe6\x95\xb0\xe7\x9a\x84\xe7\xbb\x84\xe5\x90\x88\xe6\x89\x8b\xe5\x8a\xa8\xe5\xae\x9e\xe7\x8e\xb0\xe5\xb9\xbf\xe6\x92\xad\xe6\xb3\x95\xe5\x88\x99\xef\xbc\x8c\xe8\xbf\x99\xe6\xa0\xb7\xe6\x9b\xb4\xe7\x9b\xb4\xe8\xa7\x82\xef\xbc\x8c\xe6\x9b\xb4\xe4\xb8\x8d\xe6\x98\x93\xe5\x87\xba\xe9\x94\x99\xef\xbc\x9a\n# \n# - `unsqueeze`\xe6\x88\x96\xe8\x80\x85`view`\xef\xbc\x8c\xe6\x88\x96\xe8\x80\x85tensor[None],\xef\xbc\x9a\xe4\xb8\xba\xe6\x95\xb0\xe6\x8d\xae\xe6\x9f\x90\xe4\xb8\x80\xe7\xbb\xb4\xe7\x9a\x84\xe5\xbd\xa2\xe7\x8a\xb6\xe8\xa1\xa51\xef\xbc\x8c\xe5\xae\x9e\xe7\x8e\xb0\xe6\xb3\x95\xe5\x88\x991\n# - `expand`\xe6\x88\x96\xe8\x80\x85`expand_as`\xef\xbc\x8c\xe9\x87\x8d\xe5\xa4\x8d\xe6\x95\xb0\xe7\xbb\x84\xef\xbc\x8c\xe5\xae\x9e\xe7\x8e\xb0\xe6\xb3\x95\xe5\x88\x993\xef\xbc\x9b\xe8\xaf\xa5\xe6\x93\x8d\xe4\xbd\x9c\xe4\xb8\x8d\xe4\xbc\x9a\xe5\xa4\x8d\xe5\x88\xb6\xe6\x95\xb0\xe7\xbb\x84\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe4\xb8\x8d\xe4\xbc\x9a\xe5\x8d\xa0\xe7\x94\xa8\xe9\xa2\x9d\xe5\xa4\x96\xe7\x9a\x84\xe7\xa9\xba\xe9\x97\xb4\xe3\x80\x82\n# \n# \xe6\xb3\xa8\xe6\x84\x8f\xef\xbc\x8crepeat\xe5\xae\x9e\xe7\x8e\xb0\xe4\xb8\x8eexpand\xe7\x9b\xb8\xe7\xb1\xbb\xe4\xbc\xbc\xe7\x9a\x84\xe5\x8a\x9f\xe8\x83\xbd\xef\xbc\x8c\xe4\xbd\x86\xe6\x98\xafrepeat\xe4\xbc\x9a\xe6\x8a\x8a\xe7\x9b\xb8\xe5\x90\x8c\xe6\x95\xb0\xe6\x8d\xae\xe5\xa4\x8d\xe5\x88\xb6\xe5\xa4\x9a\xe4\xbb\xbd\xef\xbc\x8c\xe5\x9b\xa0\xe6\xad\xa4\xe4\xbc\x9a\xe5\x8d\xa0\xe7\x94\xa8\xe9\xa2\x9d\xe5\xa4\x96\xe7\x9a\x84\xe7\xa9\xba\xe9\x97\xb4\xe3\x80\x82\n\n#%%\na = t.ones(3, 2)\nb = t.zeros(2, 3,1)\n\n\n#%%\n# \xe8\x87\xaa\xe5\x8a\xa8\xe5\xb9\xbf\xe6\x92\xad\xe6\xb3\x95\xe5\x88\x99\n# \xe7\xac\xac\xe4\xb8\x80\xe6\xad\xa5\xef\xbc\x9aa\xe6\x98\xaf2\xe7\xbb\xb4,b\xe6\x98\xaf3\xe7\xbb\xb4\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe5\x85\x88\xe5\x9c\xa8\xe8\xbe\x83\xe5\xb0\x8f\xe7\x9a\x84a\xe5\x89\x8d\xe9\x9d\xa2\xe8\xa1\xa51 \xef\xbc\x8c\n#               \xe5\x8d\xb3\xef\xbc\x9aa.unsqueeze(0)\xef\xbc\x8ca\xe7\x9a\x84\xe5\xbd\xa2\xe7\x8a\xb6\xe5\x8f\x98\xe6\x88\x90\xef\xbc\x881\xef\xbc\x8c3\xef\xbc\x8c2\xef\xbc\x89\xef\xbc\x8cb\xe7\x9a\x84\xe5\xbd\xa2\xe7\x8a\xb6\xe6\x98\xaf\xef\xbc\x882\xef\xbc\x8c3\xef\xbc\x8c1\xef\xbc\x89,\n# \xe7\xac\xac\xe4\xba\x8c\xe6\xad\xa5:   a\xe5\x92\x8cb\xe5\x9c\xa8\xe7\xac\xac\xe4\xb8\x80\xe7\xbb\xb4\xe5\x92\x8c\xe7\xac\xac\xe4\xb8\x89\xe7\xbb\xb4\xe5\xbd\xa2\xe7\x8a\xb6\xe4\xb8\x8d\xe4\xb8\x80\xe6\xa0\xb7\xef\xbc\x8c\xe5\x85\xb6\xe4\xb8\xad\xe4\xb8\x80\xe4\xb8\xaa\xe4\xb8\xba1 \xef\xbc\x8c\n#               \xe5\x8f\xaf\xe4\xbb\xa5\xe5\x88\xa9\xe7\x94\xa8\xe5\xb9\xbf\xe6\x92\xad\xe6\xb3\x95\xe5\x88\x99\xe6\x89\xa9\xe5\xb1\x95\xef\xbc\x8c\xe4\xb8\xa4\xe4\xb8\xaa\xe5\xbd\xa2\xe7\x8a\xb6\xe9\x83\xbd\xe5\x8f\x98\xe6\x88\x90\xe4\xba\x86\xef\xbc\x882\xef\xbc\x8c3\xef\xbc\x8c2\xef\xbc\x89\na+b\n\n\n#%%\n# \xe6\x89\x8b\xe5\x8a\xa8\xe5\xb9\xbf\xe6\x92\xad\xe6\xb3\x95\xe5\x88\x99\n# \xe6\x88\x96\xe8\x80\x85 a.view(1,3,2).expand(2,3,2)+b.expand(2,3,2)\na[None].expand(2, 3, 2) + b.expand(2,3,2)\n\n\n#%%\n# expand\xe4\xb8\x8d\xe4\xbc\x9a\xe5\x8d\xa0\xe7\x94\xa8\xe9\xa2\x9d\xe5\xa4\x96\xe7\xa9\xba\xe9\x97\xb4\xef\xbc\x8c\xe5\x8f\xaa\xe4\xbc\x9a\xe5\x9c\xa8\xe9\x9c\x80\xe8\xa6\x81\xe7\x9a\x84\xe6\x97\xb6\xe5\x80\x99\xe6\x89\x8d\xe6\x89\xa9\xe5\x85\x85\xef\xbc\x8c\xe5\x8f\xaf\xe6\x9e\x81\xe5\xa4\xa7\xe8\x8a\x82\xe7\x9c\x81\xe5\x86\x85\xe5\xad\x98\ne = a.unsqueeze(0).expand(10000000000000, 3,2)\n\n#%% [markdown]\n# ### 3.1.3 \xe5\x86\x85\xe9\x83\xa8\xe7\xbb\x93\xe6\x9e\x84\n# \n# tensor\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe7\xbb\x93\xe6\x9e\x84\xe5\xa6\x82\xe5\x9b\xbe3-1\xe6\x89\x80\xe7\xa4\xba\xe3\x80\x82tensor\xe5\x88\x86\xe4\xb8\xba\xe5\xa4\xb4\xe4\xbf\xa1\xe6\x81\xaf\xe5\x8c\xba(Tensor)\xe5\x92\x8c\xe5\xad\x98\xe5\x82\xa8\xe5\x8c\xba(Storage)\xef\xbc\x8c\xe4\xbf\xa1\xe6\x81\xaf\xe5\x8c\xba\xe4\xb8\xbb\xe8\xa6\x81\xe4\xbf\x9d\xe5\xad\x98\xe7\x9d\x80tensor\xe7\x9a\x84\xe5\xbd\xa2\xe7\x8a\xb6\xef\xbc\x88size\xef\xbc\x89\xe3\x80\x81\xe6\xad\xa5\xe9\x95\xbf\xef\xbc\x88stride\xef\xbc\x89\xe3\x80\x81\xe6\x95\xb0\xe6\x8d\xae\xe7\xb1\xbb\xe5\x9e\x8b\xef\xbc\x88type\xef\xbc\x89\xe7\xad\x89\xe4\xbf\xa1\xe6\x81\xaf\xef\xbc\x8c\xe8\x80\x8c\xe7\x9c\x9f\xe6\xad\xa3\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe5\x88\x99\xe4\xbf\x9d\xe5\xad\x98\xe6\x88\x90\xe8\xbf\x9e\xe7\xbb\xad\xe6\x95\xb0\xe7\xbb\x84\xe3\x80\x82\xe7\x94\xb1\xe4\xba\x8e\xe6\x95\xb0\xe6\x8d\xae\xe5\x8a\xa8\xe8\xbe\x84\xe6\x88\x90\xe5\x8d\x83\xe4\xb8\x8a\xe4\xb8\x87\xef\xbc\x8c\xe5\x9b\xa0\xe6\xad\xa4\xe4\xbf\xa1\xe6\x81\xaf\xe5\x8c\xba\xe5\x85\x83\xe7\xb4\xa0\xe5\x8d\xa0\xe7\x94\xa8\xe5\x86\x85\xe5\xad\x98\xe8\xbe\x83\xe5\xb0\x91\xef\xbc\x8c\xe4\xb8\xbb\xe8\xa6\x81\xe5\x86\x85\xe5\xad\x98\xe5\x8d\xa0\xe7\x94\xa8\xe5\x88\x99\xe5\x8f\x96\xe5\x86\xb3\xe4\xba\x8etensor\xe4\xb8\xad\xe5\x85\x83\xe7\xb4\xa0\xe7\x9a\x84\xe6\x95\xb0\xe7\x9b\xae\xef\xbc\x8c\xe4\xb9\x9f\xe5\x8d\xb3\xe5\xad\x98\xe5\x82\xa8\xe5\x8c\xba\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f\xe3\x80\x82\n# \n# \xe4\xb8\x80\xe8\x88\xac\xe6\x9d\xa5\xe8\xaf\xb4\xe4\xb8\x80\xe4\xb8\xaatensor\xe6\x9c\x89\xe7\x9d\x80\xe4\xb8\x8e\xe4\xb9\x8b\xe7\x9b\xb8\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84storage, storage\xe6\x98\xaf\xe5\x9c\xa8data\xe4\xb9\x8b\xe4\xb8\x8a\xe5\xb0\x81\xe8\xa3\x85\xe7\x9a\x84\xe6\x8e\xa5\xe5\x8f\xa3\xef\xbc\x8c\xe4\xbe\xbf\xe4\xba\x8e\xe4\xbd\xbf\xe7\x94\xa8\xef\xbc\x8c\xe8\x80\x8c\xe4\xb8\x8d\xe5\x90\x8ctensor\xe7\x9a\x84\xe5\xa4\xb4\xe4\xbf\xa1\xe6\x81\xaf\xe4\xb8\x80\xe8\x88\xac\xe4\xb8\x8d\xe5\x90\x8c\xef\xbc\x8c\xe4\xbd\x86\xe5\x8d\xb4\xe5\x8f\xaf\xe8\x83\xbd\xe4\xbd\xbf\xe7\x94\xa8\xe7\x9b\xb8\xe5\x90\x8c\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe3\x80\x82\xe4\xb8\x8b\xe9\x9d\xa2\xe7\x9c\x8b\xe4\xb8\xa4\xe4\xb8\xaa\xe4\xbe\x8b\xe5\xad\x90\xe3\x80\x82\n# \n# ![\xe5\x9b\xbe3-1: Tensor\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe7\xbb\x93\xe6\x9e\x84](imgs/tensor_data_structure.svg)\n\n#%%\na = t.arange(0, 6)\na.storage()\n\n\n#%%\nb = a.view(2, 3)\nb.storage()\n\n\n#%%\n# \xe4\xb8\x80\xe4\xb8\xaa\xe5\xaf\xb9\xe8\xb1\xa1\xe7\x9a\x84id\xe5\x80\xbc\xe5\x8f\xaf\xe4\xbb\xa5\xe7\x9c\x8b\xe4\xbd\x9c\xe5\xae\x83\xe5\x9c\xa8\xe5\x86\x85\xe5\xad\x98\xe4\xb8\xad\xe7\x9a\x84\xe5\x9c\xb0\xe5\x9d\x80\n# storage\xe7\x9a\x84\xe5\x86\x85\xe5\xad\x98\xe5\x9c\xb0\xe5\x9d\x80\xe4\xb8\x80\xe6\xa0\xb7\xef\xbc\x8c\xe5\x8d\xb3\xe6\x98\xaf\xe5\x90\x8c\xe4\xb8\x80\xe4\xb8\xaastorage\nid(b.storage()) == id(a.storage())\n\n\n#%%\n# a\xe6\x94\xb9\xe5\x8f\x98\xef\xbc\x8cb\xe4\xb9\x9f\xe9\x9a\x8f\xe4\xb9\x8b\xe6\x94\xb9\xe5\x8f\x98\xef\xbc\x8c\xe5\x9b\xa0\xe4\xb8\xba\xe4\xbb\x96\xe4\xbb\xac\xe5\x85\xb1\xe4\xba\xabstorage\na[1] = 100\nb\n\n\n#%%\nc = a[2:] \nc.storage()\n\n\n#%%\nc.data_ptr(), a.data_ptr() # data_ptr\xe8\xbf\x94\xe5\x9b\x9etensor\xe9\xa6\x96\xe5\x85\x83\xe7\xb4\xa0\xe7\x9a\x84\xe5\x86\x85\xe5\xad\x98\xe5\x9c\xb0\xe5\x9d\x80\n# \xe5\x8f\xaf\xe4\xbb\xa5\xe7\x9c\x8b\xe5\x87\xba\xe7\x9b\xb8\xe5\xb7\xae8\xef\xbc\x8c\xe8\xbf\x99\xe6\x98\xaf\xe5\x9b\xa0\xe4\xb8\xba2*4=8--\xe7\x9b\xb8\xe5\xb7\xae\xe4\xb8\xa4\xe4\xb8\xaa\xe5\x85\x83\xe7\xb4\xa0\xef\xbc\x8c\xe6\xaf\x8f\xe4\xb8\xaa\xe5\x85\x83\xe7\xb4\xa0\xe5\x8d\xa04\xe4\xb8\xaa\xe5\xad\x97\xe8\x8a\x82(float)\n\n\n#%%\nc[0] = -100 # c[0]\xe7\x9a\x84\xe5\x86\x85\xe5\xad\x98\xe5\x9c\xb0\xe5\x9d\x80\xe5\xaf\xb9\xe5\xba\x94a[2]\xe7\x9a\x84\xe5\x86\x85\xe5\xad\x98\xe5\x9c\xb0\xe5\x9d\x80\na\n\n\n#%%\nd = t.Tensor(c.storage())\nd[0] = 6666\nb\n\n\n#%%\n# \xe4\xb8\x8b\xe9\x9d\xa2\xef\xbc\x94\xe4\xb8\xaatensor\xe5\x85\xb1\xe4\xba\xabstorage\nid(a.storage()) == id(b.storage()) == id(c.storage()) == id(d.storage())\n\n\n#%%\na.storage_offset(), c.storage_offset(), d.storage_offset()\n\n\n#%%\ne = b[::2, ::2] # \xe9\x9a\x942\xe8\xa1\x8c/\xe5\x88\x97\xe5\x8f\x96\xe4\xb8\x80\xe4\xb8\xaa\xe5\x85\x83\xe7\xb4\xa0\nid(e.storage()) == id(a.storage())\n\n\n#%%\nb.stride(), e.stride()\n\n\n#%%\ne.is_contiguous()\n\n#%% [markdown]\n# \xe5\x8f\xaf\xe8\xa7\x81\xe7\xbb\x9d\xe5\xa4\xa7\xe5\xa4\x9a\xe6\x95\xb0\xe6\x93\x8d\xe4\xbd\x9c\xe5\xb9\xb6\xe4\xb8\x8d\xe4\xbf\xae\xe6\x94\xb9tensor\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xef\xbc\x8c\xe8\x80\x8c\xe5\x8f\xaa\xe6\x98\xaf\xe4\xbf\xae\xe6\x94\xb9\xe4\xba\x86tensor\xe7\x9a\x84\xe5\xa4\xb4\xe4\xbf\xa1\xe6\x81\xaf\xe3\x80\x82\xe8\xbf\x99\xe7\xa7\x8d\xe5\x81\x9a\xe6\xb3\x95\xe6\x9b\xb4\xe8\x8a\x82\xe7\x9c\x81\xe5\x86\x85\xe5\xad\x98\xef\xbc\x8c\xe5\x90\x8c\xe6\x97\xb6\xe6\x8f\x90\xe5\x8d\x87\xe4\xba\x86\xe5\xa4\x84\xe7\x90\x86\xe9\x80\x9f\xe5\xba\xa6\xe3\x80\x82\xe5\x9c\xa8\xe4\xbd\xbf\xe7\x94\xa8\xe4\xb8\xad\xe9\x9c\x80\xe8\xa6\x81\xe6\xb3\xa8\xe6\x84\x8f\xe3\x80\x82\n# \xe6\xad\xa4\xe5\xa4\x96\xe6\x9c\x89\xe4\xba\x9b\xe6\x93\x8d\xe4\xbd\x9c\xe4\xbc\x9a\xe5\xaf\xbc\xe8\x87\xb4tensor\xe4\xb8\x8d\xe8\xbf\x9e\xe7\xbb\xad\xef\xbc\x8c\xe8\xbf\x99\xe6\x97\xb6\xe9\x9c\x80\xe8\xb0\x83\xe7\x94\xa8`tensor.contiguous`\xe6\x96\xb9\xe6\xb3\x95\xe5\xb0\x86\xe5\xae\x83\xe4\xbb\xac\xe5\x8f\x98\xe6\x88\x90\xe8\xbf\x9e\xe7\xbb\xad\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xef\xbc\x8c\xe8\xaf\xa5\xe6\x96\xb9\xe6\xb3\x95\xe4\xbc\x9a\xe4\xbd\xbf\xe6\x95\xb0\xe6\x8d\xae\xe5\xa4\x8d\xe5\x88\xb6\xe4\xb8\x80\xe4\xbb\xbd\xef\xbc\x8c\xe4\xb8\x8d\xe5\x86\x8d\xe4\xb8\x8e\xe5\x8e\x9f\xe6\x9d\xa5\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe5\x85\xb1\xe4\xba\xabstorage\xe3\x80\x82\n# \xe5\x8f\xa6\xe5\xa4\x96\xe8\xaf\xbb\xe8\x80\x85\xe5\x8f\xaf\xe4\xbb\xa5\xe6\x80\x9d\xe8\x80\x83\xe4\xb8\x80\xe4\xb8\x8b\xef\xbc\x8c\xe4\xb9\x8b\xe5\x89\x8d\xe8\xaf\xb4\xe8\xbf\x87\xe7\x9a\x84\xe9\xab\x98\xe7\xba\xa7\xe7\xb4\xa2\xe5\xbc\x95\xe4\xb8\x80\xe8\x88\xac\xe4\xb8\x8d\xe5\x85\xb1\xe4\xba\xabstroage\xef\xbc\x8c\xe8\x80\x8c\xe6\x99\xae\xe9\x80\x9a\xe7\xb4\xa2\xe5\xbc\x95\xe5\x85\xb1\xe4\xba\xabstorage\xef\xbc\x8c\xe8\xbf\x99\xe6\x98\xaf\xe4\xb8\xba\xe4\xbb\x80\xe4\xb9\x88\xef\xbc\x9f\xef\xbc\x88\xe6\x8f\x90\xe7\xa4\xba\xef\xbc\x9a\xe6\x99\xae\xe9\x80\x9a\xe7\xb4\xa2\xe5\xbc\x95\xe5\x8f\xaf\xe4\xbb\xa5\xe9\x80\x9a\xe8\xbf\x87\xe5\x8f\xaa\xe4\xbf\xae\xe6\x94\xb9tensor\xe7\x9a\x84offset\xef\xbc\x8cstride\xe5\x92\x8csize\xef\xbc\x8c\xe8\x80\x8c\xe4\xb8\x8d\xe4\xbf\xae\xe6\x94\xb9storage\xe6\x9d\xa5\xe5\xae\x9e\xe7\x8e\xb0\xef\xbc\x89\xe3\x80\x82\n#%% [markdown]\n# ### 3.1.4 \xe5\x85\xb6\xe5\xae\x83\xe6\x9c\x89\xe5\x85\xb3Tensor\xe7\x9a\x84\xe8\xaf\x9d\xe9\xa2\x98\n# \xe8\xbf\x99\xe9\x83\xa8\xe5\x88\x86\xe7\x9a\x84\xe5\x86\x85\xe5\xae\xb9\xe4\xb8\x8d\xe5\xa5\xbd\xe4\xb8\x93\xe9\x97\xa8\xe5\x88\x92\xe5\x88\x86\xe4\xb8\x80\xe5\xb0\x8f\xe8\x8a\x82\xef\xbc\x8c\xe4\xbd\x86\xe6\x98\xaf\xe7\xac\x94\xe8\x80\x85\xe8\xae\xa4\xe4\xb8\xba\xe4\xbb\x8d\xe5\x80\xbc\xe5\xbe\x97\xe8\xaf\xbb\xe8\x80\x85\xe6\xb3\xa8\xe6\x84\x8f\xef\xbc\x8c\xe6\x95\x85\xe8\x80\x8c\xe5\xb0\x86\xe5\x85\xb6\xe6\x94\xbe\xe5\x9c\xa8\xe8\xbf\x99\xe4\xb8\x80\xe5\xb0\x8f\xe8\x8a\x82\xe3\x80\x82\n#%% [markdown]\n# #### GPU/CPU\n# tensor\xe5\x8f\xaf\xe4\xbb\xa5\xe5\xbe\x88\xe9\x9a\x8f\xe6\x84\x8f\xe7\x9a\x84\xe5\x9c\xa8gpu/cpu\xe4\xb8\x8a\xe4\xbc\xa0\xe8\xbe\x93\xe3\x80\x82\xe4\xbd\xbf\xe7\x94\xa8`tensor.cuda(device_id)`\xe6\x88\x96\xe8\x80\x85`tensor.cpu()`\xe3\x80\x82\xe5\x8f\xa6\xe5\xa4\x96\xe4\xb8\x80\xe4\xb8\xaa\xe6\x9b\xb4\xe9\x80\x9a\xe7\x94\xa8\xe7\x9a\x84\xe6\x96\xb9\xe6\xb3\x95\xe6\x98\xaf`tensor.to(device)`\xe3\x80\x82\n\n#%%\na = t.randn(3, 4)\na.device\n\n\n#%%\nif t.cuda.is_available():\n    a = t.randn(3,4, device=t.device(\'cuda:1\'))\n    # \xe7\xad\x89\xe4\xbb\xb7\xe4\xba\x8e\n    # a.t.randn(3,4).cuda(1)\n    # \xe4\xbd\x86\xe6\x98\xaf\xe5\x89\x8d\xe8\x80\x85\xe6\x9b\xb4\xe5\xbf\xab\n    a.device\n\n\n#%%\ndevice = t.device(\'cpu\')\na.to(device)\n\n#%% [markdown]\n# **\xe6\xb3\xa8\xe6\x84\x8f**\n# - \xe5\xb0\xbd\xe9\x87\x8f\xe4\xbd\xbf\xe7\x94\xa8`tensor.to(device)`, \xe5\xb0\x86`device`\xe8\xae\xbe\xe4\xb8\xba\xe4\xb8\x80\xe4\xb8\xaa\xe5\x8f\xaf\xe9\x85\x8d\xe7\xbd\xae\xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0\xef\xbc\x8c\xe8\xbf\x99\xe6\xa0\xb7\xe5\x8f\xaf\xe4\xbb\xa5\xe5\xbe\x88\xe8\xbd\xbb\xe6\x9d\xbe\xe7\x9a\x84\xe4\xbd\xbf\xe7\xa8\x8b\xe5\xba\x8f\xe5\x90\x8c\xe6\x97\xb6\xe5\x85\xbc\xe5\xae\xb9GPU\xe5\x92\x8cCPU\n# - \xe6\x95\xb0\xe6\x8d\xae\xe5\x9c\xa8GPU\xe4\xb9\x8b\xe4\xb8\xad\xe4\xbc\xa0\xe8\xbe\x93\xe7\x9a\x84\xe9\x80\x9f\xe5\xba\xa6\xe8\xa6\x81\xe8\xbf\x9c\xe5\xbf\xab\xe4\xba\x8e\xe5\x86\x85\xe5\xad\x98(CPU)\xe5\x88\xb0\xe6\x98\xbe\xe5\xad\x98(GPU), \xe6\x89\x80\xe4\xbb\xa5\xe5\xb0\xbd\xe9\x87\x8f\xe9\x81\xbf\xe5\x85\x8d\xe9\xa2\x91\xe7\xb9\x81\xe7\x9a\x84\xe5\x9c\xa8\xe5\x86\x85\xe5\xad\x98\xe5\x92\x8c\xe6\x98\xbe\xe5\xad\x98\xe4\xb8\xad\xe4\xbc\xa0\xe8\xbe\x93\xe6\x95\xb0\xe6\x8d\xae\xe3\x80\x82\n#%% [markdown]\n# #### \xe6\x8c\x81\xe4\xb9\x85\xe5\x8c\x96\n# Tensor\xe7\x9a\x84\xe4\xbf\x9d\xe5\xad\x98\xe5\x92\x8c\xe5\x8a\xa0\xe8\xbd\xbd\xe5\x8d\x81\xe5\x88\x86\xe7\x9a\x84\xe7\xae\x80\xe5\x8d\x95\xef\xbc\x8c\xe4\xbd\xbf\xe7\x94\xa8t.save\xe5\x92\x8ct.load\xe5\x8d\xb3\xe5\x8f\xaf\xe5\xae\x8c\xe6\x88\x90\xe7\x9b\xb8\xe5\xba\x94\xe7\x9a\x84\xe5\x8a\x9f\xe8\x83\xbd\xe3\x80\x82\xe5\x9c\xa8save/load\xe6\x97\xb6\xe5\x8f\xaf\xe6\x8c\x87\xe5\xae\x9a\xe4\xbd\xbf\xe7\x94\xa8\xe7\x9a\x84`pickle`\xe6\xa8\xa1\xe5\x9d\x97\xef\xbc\x8c\xe5\x9c\xa8load\xe6\x97\xb6\xe8\xbf\x98\xe5\x8f\xaf\xe5\xb0\x86GPU tensor\xe6\x98\xa0\xe5\xb0\x84\xe5\x88\xb0CPU\xe6\x88\x96\xe5\x85\xb6\xe5\xae\x83GPU\xe4\xb8\x8a\xe3\x80\x82\n\n#%%\nif t.cuda.is_available():\n    a = a.cuda(1) # \xe6\x8a\x8aa\xe8\xbd\xac\xe4\xb8\xbaGPU1\xe4\xb8\x8a\xe7\x9a\x84tensor,\n    t.save(a,\'a.pth\')\n\n    # \xe5\x8a\xa0\xe8\xbd\xbd\xe4\xb8\xbab, \xe5\xad\x98\xe5\x82\xa8\xe4\xba\x8eGPU1\xe4\xb8\x8a(\xe5\x9b\xa0\xe4\xb8\xba\xe4\xbf\x9d\xe5\xad\x98\xe6\x97\xb6tensor\xe5\xb0\xb1\xe5\x9c\xa8GPU1\xe4\xb8\x8a)\n    b = t.load(\'a.pth\')\n    # \xe5\x8a\xa0\xe8\xbd\xbd\xe4\xb8\xbac, \xe5\xad\x98\xe5\x82\xa8\xe4\xba\x8eCPU\n    c = t.load(\'a.pth\', map_location=lambda storage, loc: storage)\n    # \xe5\x8a\xa0\xe8\xbd\xbd\xe4\xb8\xbad, \xe5\xad\x98\xe5\x82\xa8\xe4\xba\x8eGPU0\xe4\xb8\x8a\n    d = t.load(\'a.pth\', map_location={\'cuda:1\':\'cuda:0\'})\n\n#%% [markdown]\n# ####   \xe5\x90\x91\xe9\x87\x8f\xe5\x8c\x96\n#%% [markdown]\n# \xe5\x90\x91\xe9\x87\x8f\xe5\x8c\x96\xe8\xae\xa1\xe7\xae\x97\xe6\x98\xaf\xe4\xb8\x80\xe7\xa7\x8d\xe7\x89\xb9\xe6\xae\x8a\xe7\x9a\x84\xe5\xb9\xb6\xe8\xa1\x8c\xe8\xae\xa1\xe7\xae\x97\xe6\x96\xb9\xe5\xbc\x8f\xef\xbc\x8c\xe7\x9b\xb8\xe5\xaf\xb9\xe4\xba\x8e\xe4\xb8\x80\xe8\x88\xac\xe7\xa8\x8b\xe5\xba\x8f\xe5\x9c\xa8\xe5\x90\x8c\xe4\xb8\x80\xe6\x97\xb6\xe9\x97\xb4\xe5\x8f\xaa\xe6\x89\xa7\xe8\xa1\x8c\xe4\xb8\x80\xe4\xb8\xaa\xe6\x93\x8d\xe4\xbd\x9c\xe7\x9a\x84\xe6\x96\xb9\xe5\xbc\x8f\xef\xbc\x8c\xe5\xae\x83\xe5\x8f\xaf\xe5\x9c\xa8\xe5\x90\x8c\xe4\xb8\x80\xe6\x97\xb6\xe9\x97\xb4\xe6\x89\xa7\xe8\xa1\x8c\xe5\xa4\x9a\xe4\xb8\xaa\xe6\x93\x8d\xe4\xbd\x9c\xef\xbc\x8c\xe9\x80\x9a\xe5\xb8\xb8\xe6\x98\xaf\xe5\xaf\xb9\xe4\xb8\x8d\xe5\x90\x8c\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe6\x89\xa7\xe8\xa1\x8c\xe5\x90\x8c\xe6\xa0\xb7\xe7\x9a\x84\xe4\xb8\x80\xe4\xb8\xaa\xe6\x88\x96\xe4\xb8\x80\xe6\x89\xb9\xe6\x8c\x87\xe4\xbb\xa4\xef\xbc\x8c\xe6\x88\x96\xe8\x80\x85\xe8\xaf\xb4\xe6\x8a\x8a\xe6\x8c\x87\xe4\xbb\xa4\xe5\xba\x94\xe7\x94\xa8\xe4\xba\x8e\xe4\xb8\x80\xe4\xb8\xaa\xe6\x95\xb0\xe7\xbb\x84/\xe5\x90\x91\xe9\x87\x8f\xe4\xb8\x8a\xe3\x80\x82\xe5\x90\x91\xe9\x87\x8f\xe5\x8c\x96\xe5\x8f\xaf\xe6\x9e\x81\xe5\xa4\xa7\xe6\x8f\x90\xe9\xab\x98\xe7\xa7\x91\xe5\xad\xa6\xe8\xbf\x90\xe7\xae\x97\xe7\x9a\x84\xe6\x95\x88\xe7\x8e\x87\xef\xbc\x8cPython\xe6\x9c\xac\xe8\xba\xab\xe6\x98\xaf\xe4\xb8\x80\xe9\x97\xa8\xe9\xab\x98\xe7\xba\xa7\xe8\xaf\xad\xe8\xa8\x80\xef\xbc\x8c\xe4\xbd\xbf\xe7\x94\xa8\xe5\xbe\x88\xe6\x96\xb9\xe4\xbe\xbf\xef\xbc\x8c\xe4\xbd\x86\xe8\xbf\x99\xe4\xb9\x9f\xe6\x84\x8f\xe5\x91\xb3\xe7\x9d\x80\xe5\xbe\x88\xe5\xa4\x9a\xe6\x93\x8d\xe4\xbd\x9c\xe5\xbe\x88\xe4\xbd\x8e\xe6\x95\x88\xef\xbc\x8c\xe5\xb0\xa4\xe5\x85\xb6\xe6\x98\xaf`for`\xe5\xbe\xaa\xe7\x8e\xaf\xe3\x80\x82\xe5\x9c\xa8\xe7\xa7\x91\xe5\xad\xa6\xe8\xae\xa1\xe7\xae\x97\xe7\xa8\x8b\xe5\xba\x8f\xe4\xb8\xad\xe5\xba\x94\xe5\xbd\x93\xe6\x9e\x81\xe5\x8a\x9b\xe9\x81\xbf\xe5\x85\x8d\xe4\xbd\xbf\xe7\x94\xa8Python\xe5\x8e\x9f\xe7\x94\x9f\xe7\x9a\x84`for\xe5\xbe\xaa\xe7\x8e\xaf`\xe3\x80\x82\n\n#%%\ndef for_loop_add(x, y):\n    result = []\n    for i,j in zip(x, y):\n        result.append(i + j)\n    return t.Tensor(result)\n\n\n#%%\nx = t.zeros(100)\ny = t.ones(100)\nget_ipython().run_line_magic(\'timeit\', \'-n 10 for_loop_add(x, y)\')\nget_ipython().run_line_magic(\'timeit\', \'-n 10 x + y\')\n\n#%% [markdown]\n# \xe5\x8f\xaf\xe8\xa7\x81\xe4\xba\x8c\xe8\x80\x85\xe6\x9c\x89\xe8\xb6\x85\xe8\xbf\x87\xe5\x87\xa0\xe5\x8d\x81\xe5\x80\x8d\xe7\x9a\x84\xe9\x80\x9f\xe5\xba\xa6\xe5\xb7\xae\xe8\xb7\x9d\xef\xbc\x8c\xe5\x9b\xa0\xe6\xad\xa4\xe5\x9c\xa8\xe5\xae\x9e\xe9\x99\x85\xe4\xbd\xbf\xe7\x94\xa8\xe4\xb8\xad\xe5\xba\x94\xe5\xb0\xbd\xe9\x87\x8f\xe8\xb0\x83\xe7\x94\xa8\xe5\x86\x85\xe5\xbb\xba\xe5\x87\xbd\xe6\x95\xb0(buildin-function)\xef\xbc\x8c\xe8\xbf\x99\xe4\xba\x9b\xe5\x87\xbd\xe6\x95\xb0\xe5\xba\x95\xe5\xb1\x82\xe7\x94\xb1C/C++\xe5\xae\x9e\xe7\x8e\xb0\xef\xbc\x8c\xe8\x83\xbd\xe9\x80\x9a\xe8\xbf\x87\xe6\x89\xa7\xe8\xa1\x8c\xe5\xba\x95\xe5\xb1\x82\xe4\xbc\x98\xe5\x8c\x96\xe5\xae\x9e\xe7\x8e\xb0\xe9\xab\x98\xe6\x95\x88\xe8\xae\xa1\xe7\xae\x97\xe3\x80\x82\xe5\x9b\xa0\xe6\xad\xa4\xe5\x9c\xa8\xe5\xb9\xb3\xe6\x97\xb6\xe5\x86\x99\xe4\xbb\xa3\xe7\xa0\x81\xe6\x97\xb6\xef\xbc\x8c\xe5\xb0\xb1\xe5\xba\x94\xe5\x85\xbb\xe6\x88\x90\xe5\x90\x91\xe9\x87\x8f\xe5\x8c\x96\xe7\x9a\x84\xe6\x80\x9d\xe7\xbb\xb4\xe4\xb9\xa0\xe6\x83\xaf\xef\xbc\x8c\xe5\x8d\x83\xe4\xb8\x87\xe9\x81\xbf\xe5\x85\x8d\xe5\xaf\xb9\xe8\xbe\x83\xe5\xa4\xa7\xe7\x9a\x84tensor\xe8\xbf\x9b\xe8\xa1\x8c\xe9\x80\x90\xe5\x85\x83\xe7\xb4\xa0\xe9\x81\x8d\xe5\x8e\x86\xe3\x80\x82\n#%% [markdown]\n# \xe6\xad\xa4\xe5\xa4\x96\xe8\xbf\x98\xe6\x9c\x89\xe4\xbb\xa5\xe4\xb8\x8b\xe5\x87\xa0\xe7\x82\xb9\xe9\x9c\x80\xe8\xa6\x81\xe6\xb3\xa8\xe6\x84\x8f\xef\xbc\x9a\n# - \xe5\xa4\xa7\xe5\xa4\x9a\xe6\x95\xb0`t.function`\xe9\x83\xbd\xe6\x9c\x89\xe4\xb8\x80\xe4\xb8\xaa\xe5\x8f\x82\xe6\x95\xb0`out`\xef\xbc\x8c\xe8\xbf\x99\xe6\x97\xb6\xe5\x80\x99\xe4\xba\xa7\xe7\x94\x9f\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x9c\xe5\xb0\x86\xe4\xbf\x9d\xe5\xad\x98\xe5\x9c\xa8out\xe6\x8c\x87\xe5\xae\x9atensor\xe4\xb9\x8b\xe4\xb8\xad\xe3\x80\x82\n# - `t.set_num_threads`\xe5\x8f\xaf\xe4\xbb\xa5\xe8\xae\xbe\xe7\xbd\xaePyTorch\xe8\xbf\x9b\xe8\xa1\x8cCPU\xe5\xa4\x9a\xe7\xba\xbf\xe7\xa8\x8b\xe5\xb9\xb6\xe8\xa1\x8c\xe8\xae\xa1\xe7\xae\x97\xe6\x97\xb6\xe5\x80\x99\xe6\x89\x80\xe5\x8d\xa0\xe7\x94\xa8\xe7\x9a\x84\xe7\xba\xbf\xe7\xa8\x8b\xe6\x95\xb0\xef\xbc\x8c\xe8\xbf\x99\xe4\xb8\xaa\xe5\x8f\xaf\xe4\xbb\xa5\xe7\x94\xa8\xe6\x9d\xa5\xe9\x99\x90\xe5\x88\xb6PyTorch\xe6\x89\x80\xe5\x8d\xa0\xe7\x94\xa8\xe7\x9a\x84CPU\xe6\x95\xb0\xe7\x9b\xae\xe3\x80\x82\n# - `t.set_printoptions`\xe5\x8f\xaf\xe4\xbb\xa5\xe7\x94\xa8\xe6\x9d\xa5\xe8\xae\xbe\xe7\xbd\xae\xe6\x89\x93\xe5\x8d\xb0tensor\xe6\x97\xb6\xe7\x9a\x84\xe6\x95\xb0\xe5\x80\xbc\xe7\xb2\xbe\xe5\xba\xa6\xe5\x92\x8c\xe6\xa0\xbc\xe5\xbc\x8f\xe3\x80\x82\n# \xe4\xb8\x8b\xe9\x9d\xa2\xe4\xb8\xbe\xe4\xbe\x8b\xe8\xaf\xb4\xe6\x98\x8e\xe3\x80\x82\n\n#%%\na = t.arange(0, 20000000)\nprint(a[-1], a[-2]) # 32bit\xe7\x9a\x84IntTensor\xe7\xb2\xbe\xe5\xba\xa6\xe6\x9c\x89\xe9\x99\x90\xe5\xaf\xbc\xe8\x87\xb4\xe6\xba\xa2\xe5\x87\xba\nb = t.LongTensor()\nt.arange(0, 20000000, out=b) # 64bit\xe7\x9a\x84LongTensor\xe4\xb8\x8d\xe4\xbc\x9a\xe6\xba\xa2\xe5\x87\xba\nb[-1],b[-2]\n\n\n#%%\na = t.randn(2,3)\na\n\n\n#%%\nt.set_printoptions(precision=10)\na\n\n#%% [markdown]\n# ### 3.1.5 \xe5\xb0\x8f\xe8\xaf\x95\xe7\x89\x9b\xe5\x88\x80\xef\xbc\x9a\xe7\xba\xbf\xe6\x80\xa7\xe5\x9b\x9e\xe5\xbd\x92\n#%% [markdown]\n# \xe7\xba\xbf\xe6\x80\xa7\xe5\x9b\x9e\xe5\xbd\x92\xe6\x98\xaf\xe6\x9c\xba\xe5\x99\xa8\xe5\xad\xa6\xe4\xb9\xa0\xe5\x85\xa5\xe9\x97\xa8\xe7\x9f\xa5\xe8\xaf\x86\xef\xbc\x8c\xe5\xba\x94\xe7\x94\xa8\xe5\x8d\x81\xe5\x88\x86\xe5\xb9\xbf\xe6\xb3\x9b\xe3\x80\x82\xe7\xba\xbf\xe6\x80\xa7\xe5\x9b\x9e\xe5\xbd\x92\xe5\x88\xa9\xe7\x94\xa8\xe6\x95\xb0\xe7\x90\x86\xe7\xbb\x9f\xe8\xae\xa1\xe4\xb8\xad\xe5\x9b\x9e\xe5\xbd\x92\xe5\x88\x86\xe6\x9e\x90\xef\xbc\x8c\xe6\x9d\xa5\xe7\xa1\xae\xe5\xae\x9a\xe4\xb8\xa4\xe7\xa7\x8d\xe6\x88\x96\xe4\xb8\xa4\xe7\xa7\x8d\xe4\xbb\xa5\xe4\xb8\x8a\xe5\x8f\x98\xe9\x87\x8f\xe9\x97\xb4\xe7\x9b\xb8\xe4\xba\x92\xe4\xbe\x9d\xe8\xb5\x96\xe7\x9a\x84\xe5\xae\x9a\xe9\x87\x8f\xe5\x85\xb3\xe7\xb3\xbb\xe7\x9a\x84\xef\xbc\x8c\xe5\x85\xb6\xe8\xa1\xa8\xe8\xbe\xbe\xe5\xbd\xa2\xe5\xbc\x8f\xe4\xb8\xba$y = wx+b+e$\xef\xbc\x8c$e$\xe4\xb8\xba\xe8\xaf\xaf\xe5\xb7\xae\xe6\x9c\x8d\xe4\xbb\x8e\xe5\x9d\x87\xe5\x80\xbc\xe4\xb8\xba0\xe7\x9a\x84\xe6\xad\xa3\xe6\x80\x81\xe5\x88\x86\xe5\xb8\x83\xe3\x80\x82\xe9\xa6\x96\xe5\x85\x88\xe8\xae\xa9\xe6\x88\x91\xe4\xbb\xac\xe6\x9d\xa5\xe7\xa1\xae\xe8\xae\xa4\xe7\xba\xbf\xe6\x80\xa7\xe5\x9b\x9e\xe5\xbd\x92\xe7\x9a\x84\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x87\xbd\xe6\x95\xb0\xef\xbc\x9a\n# $$\n# loss = \\sum_i^N \\frac 1 2 ({y_i-(wx_i+b)})^2\n# $$\n# \xe7\x84\xb6\xe5\x90\x8e\xe5\x88\xa9\xe7\x94\xa8\xe9\x9a\x8f\xe6\x9c\xba\xe6\xa2\xaf\xe5\xba\xa6\xe4\xb8\x8b\xe9\x99\x8d\xe6\xb3\x95\xe6\x9b\xb4\xe6\x96\xb0\xe5\x8f\x82\xe6\x95\xb0$\\textbf{w}$\xe5\x92\x8c$\\textbf{b}$\xe6\x9d\xa5\xe6\x9c\x80\xe5\xb0\x8f\xe5\x8c\x96\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x87\xbd\xe6\x95\xb0\xef\xbc\x8c\xe6\x9c\x80\xe7\xbb\x88\xe5\xad\xa6\xe5\xbe\x97$\\textbf{w}$\xe5\x92\x8c$\\textbf{b}$\xe7\x9a\x84\xe6\x95\xb0\xe5\x80\xbc\xe3\x80\x82\n\n#%%\nimport torch as t\nget_ipython().run_line_magic(\'matplotlib\', \'inline\')\nfrom matplotlib import pyplot as plt\nfrom IPython import display\n\ndevice = t.device(\'cpu\') #\xe5\xa6\x82\xe6\x9e\x9c\xe4\xbd\xa0\xe6\x83\xb3\xe7\x94\xa8gpu\xef\xbc\x8c\xe6\x94\xb9\xe6\x88\x90t.device(\'cuda:0\')\n\n\n#%%\n# \xe8\xae\xbe\xe7\xbd\xae\xe9\x9a\x8f\xe6\x9c\xba\xe6\x95\xb0\xe7\xa7\x8d\xe5\xad\x90\xef\xbc\x8c\xe4\xbf\x9d\xe8\xaf\x81\xe5\x9c\xa8\xe4\xb8\x8d\xe5\x90\x8c\xe7\x94\xb5\xe8\x84\x91\xe4\xb8\x8a\xe8\xbf\x90\xe8\xa1\x8c\xe6\x97\xb6\xe4\xb8\x8b\xe9\x9d\xa2\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe4\xb8\x80\xe8\x87\xb4\nt.manual_seed(1000) \n\ndef get_fake_data(batch_size=8):\n    \'\'\' \xe4\xba\xa7\xe7\x94\x9f\xe9\x9a\x8f\xe6\x9c\xba\xe6\x95\xb0\xe6\x8d\xae\xef\xbc\x9ay=x*2+3\xef\xbc\x8c\xe5\x8a\xa0\xe4\xb8\x8a\xe4\xba\x86\xe4\xb8\x80\xe4\xba\x9b\xe5\x99\xaa\xe5\xa3\xb0\'\'\'\n    x = t.rand(batch_size, 1, device=device) * 5\n    y = x * 2 + 3 +  t.randn(batch_size, 1, device=device)\n    return x, y\n\n\n#%%\n# \xe6\x9d\xa5\xe7\x9c\x8b\xe7\x9c\x8b\xe4\xba\xa7\xe7\x94\x9f\xe7\x9a\x84x-y\xe5\x88\x86\xe5\xb8\x83\nx, y = get_fake_data(batch_size=16)\nplt.scatter(x.squeeze().cpu().numpy(), y.squeeze().cpu().numpy())\n\n\n#%%\n# \xe9\x9a\x8f\xe6\x9c\xba\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe5\x8f\x82\xe6\x95\xb0\nw = t.rand(1, 1).to(device)\nb = t.zeros(1, 1).to(device)\n\nlr =0.02 # \xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\n\nfor ii in range(500):\n    x, y = get_fake_data(batch_size=4)\n    \n    # forward\xef\xbc\x9a\xe8\xae\xa1\xe7\xae\x97loss\n    y_pred = x.mm(w) + b.expand_as(y) # x@W\xe7\xad\x89\xe4\xbb\xb7\xe4\xba\x8ex.mm(w);for python3 only\n    loss = 0.5 * (y_pred - y) ** 2 # \xe5\x9d\x87\xe6\x96\xb9\xe8\xaf\xaf\xe5\xb7\xae\n    loss = loss.mean()\n    \n    # backward\xef\xbc\x9a\xe6\x89\x8b\xe5\x8a\xa8\xe8\xae\xa1\xe7\xae\x97\xe6\xa2\xaf\xe5\xba\xa6\n    dloss = 1\n    dy_pred = dloss * (y_pred - y)\n    \n    dw = x.t().mm(dy_pred)\n    db = dy_pred.sum()\n    \n    # \xe6\x9b\xb4\xe6\x96\xb0\xe5\x8f\x82\xe6\x95\xb0\n    w.sub_(lr * dw)\n    b.sub_(lr * db)\n    \n    if ii%50 ==0:\n       \n        # \xe7\x94\xbb\xe5\x9b\xbe\n        display.clear_output(wait=True)\n        x = t.arange(0, 6).view(-1, 1)\n        y = x.mm(w) + b.expand_as(x)\n        plt.plot(x.cpu().numpy(), y.cpu().numpy()) # predicted\n        \n        x2, y2 = get_fake_data(batch_size=32) \n        plt.scatter(x2.numpy(), y2.numpy()) # true data\n        \n        plt.xlim(0, 5)\n        plt.ylim(0, 13)\n        plt.show()\n        plt.pause(0.5)\n        \nprint(\'w: \', w.item(), \'b: \', b.item())\n\n#%% [markdown]\n# \xe5\x8f\xaf\xe8\xa7\x81\xe7\xa8\x8b\xe5\xba\x8f\xe5\xb7\xb2\xe7\xbb\x8f\xe5\x9f\xba\xe6\x9c\xac\xe5\xad\xa6\xe5\x87\xbaw=2\xe3\x80\x81b=3\xef\xbc\x8c\xe5\xb9\xb6\xe4\xb8\x94\xe5\x9b\xbe\xe4\xb8\xad\xe7\x9b\xb4\xe7\xba\xbf\xe5\x92\x8c\xe6\x95\xb0\xe6\x8d\xae\xe5\xb7\xb2\xe7\xbb\x8f\xe5\xae\x9e\xe7\x8e\xb0\xe8\xbe\x83\xe5\xa5\xbd\xe7\x9a\x84\xe6\x8b\x9f\xe5\x90\x88\xe3\x80\x82\n#%% [markdown]\n# \xe8\x99\xbd\xe7\x84\xb6\xe4\xb8\x8a\xe9\x9d\xa2\xe6\x8f\x90\xe5\x88\xb0\xe4\xba\x86\xe8\xae\xb8\xe5\xa4\x9a\xe6\x93\x8d\xe4\xbd\x9c\xef\xbc\x8c\xe4\xbd\x86\xe6\x98\xaf\xe5\x8f\xaa\xe8\xa6\x81\xe6\x8e\x8c\xe6\x8f\xa1\xe4\xba\x86\xe8\xbf\x99\xe4\xb8\xaa\xe4\xbe\x8b\xe5\xad\x90\xe5\x9f\xba\xe6\x9c\xac\xe4\xb8\x8a\xe5\xb0\xb1\xe5\x8f\xaf\xe4\xbb\xa5\xe4\xba\x86\xef\xbc\x8c\xe5\x85\xb6\xe4\xbb\x96\xe7\x9a\x84\xe7\x9f\xa5\xe8\xaf\x86\xef\xbc\x8c\xe8\xaf\xbb\xe8\x80\x85\xe6\x97\xa5\xe5\x90\x8e\xe9\x81\x87\xe5\x88\xb0\xe7\x9a\x84\xe6\x97\xb6\xe5\x80\x99\xef\xbc\x8c\xe5\x8f\xaf\xe4\xbb\xa5\xe5\x86\x8d\xe7\x9c\x8b\xe7\x9c\x8b\xe8\xbf\x99\xe9\x83\xa8\xe4\xbb\xbd\xe7\x9a\x84\xe5\x86\x85\xe5\xae\xb9\xe6\x88\x96\xe8\x80\x85\xe6\x9f\xa5\xe6\x89\xbe\xe5\xaf\xb9\xe5\xba\x94\xe6\x96\x87\xe6\xa1\xa3\xe3\x80\x82\n# \n\n'"
2.PyTorch _basics PyTorch基础/auto_grad 自动求导.py,15,"b'#%% [markdown]\n# # \xe8\x87\xaa\xe5\x8a\xa8\xe6\xb1\x82\xe5\xaf\xbc\n# \xe8\xbf\x99\xe6\xac\xa1\xe8\xaf\xbe\xe7\xa8\x8b\xe6\x88\x91\xe4\xbb\xac\xe4\xbc\x9a\xe4\xba\x86\xe8\xa7\xa3 PyTorch \xe4\xb8\xad\xe7\x9a\x84\xe8\x87\xaa\xe5\x8a\xa8\xe6\xb1\x82\xe5\xaf\xbc\xe6\x9c\xba\xe5\x88\xb6\xef\xbc\x8c\xe8\x87\xaa\xe5\x8a\xa8\xe6\xb1\x82\xe5\xaf\xbc\xe6\x98\xaf PyTorch \xe4\xb8\xad\xe9\x9d\x9e\xe5\xb8\xb8\xe9\x87\x8d\xe8\xa6\x81\xe7\x9a\x84\xe7\x89\xb9\xe6\x80\xa7\xef\xbc\x8c\xe8\x83\xbd\xe5\xa4\x9f\xe8\xae\xa9\xe6\x88\x91\xe4\xbb\xac\xe9\x81\xbf\xe5\x85\x8d\xe6\x89\x8b\xe5\x8a\xa8\xe5\x8e\xbb\xe8\xae\xa1\xe7\xae\x97\xe9\x9d\x9e\xe5\xb8\xb8\xe5\xa4\x8d\xe6\x9d\x82\xe7\x9a\x84\xe5\xaf\xbc\xe6\x95\xb0\xef\xbc\x8c\xe8\xbf\x99\xe8\x83\xbd\xe5\xa4\x9f\xe6\x9e\x81\xe5\xa4\xa7\xe5\x9c\xb0\xe5\x87\x8f\xe5\xb0\x91\xe4\xba\x86\xe6\x88\x91\xe4\xbb\xac\xe6\x9e\x84\xe5\xbb\xba\xe6\xa8\xa1\xe5\x9e\x8b\xe7\x9a\x84\xe6\x97\xb6\xe9\x97\xb4\xef\xbc\x8c\xe8\xbf\x99\xe4\xb9\x9f\xe6\x98\xaf\xe5\x85\xb6\xe5\x89\x8d\xe8\xba\xab Torch \xe8\xbf\x99\xe4\xb8\xaa\xe6\xa1\x86\xe6\x9e\xb6\xe6\x89\x80\xe4\xb8\x8d\xe5\x85\xb7\xe5\xa4\x87\xe7\x9a\x84\xe7\x89\xb9\xe6\x80\xa7\xef\xbc\x8c\xe4\xb8\x8b\xe9\x9d\xa2\xe6\x88\x91\xe4\xbb\xac\xe9\x80\x9a\xe8\xbf\x87\xe4\xbe\x8b\xe5\xad\x90\xe7\x9c\x8b\xe7\x9c\x8b PyTorch \xe8\x87\xaa\xe5\x8a\xa8\xe6\xb1\x82\xe5\xaf\xbc\xe7\x9a\x84\xe7\x8b\xac\xe7\x89\xb9\xe9\xad\x85\xe5\x8a\x9b\xe4\xbb\xa5\xe5\x8f\x8a\xe6\x8e\xa2\xe7\xa9\xb6\xe8\x87\xaa\xe5\x8a\xa8\xe6\xb1\x82\xe5\xaf\xbc\xe7\x9a\x84\xe6\x9b\xb4\xe5\xa4\x9a\xe7\x94\xa8\xe6\xb3\x95\xe3\x80\x82\n\n#%%\nimport torch\ntorch.__version__\n# \xe5\xb7\xb2\xe7\xbb\x8f\xe4\xbf\xae\xe6\x94\xb9\xe5\x8f\xaf\xe4\xbb\xa5\xe8\xbf\x90\xe8\xa1\x8c0.4.x\n\n#%% [markdown]\n# ## \xe7\xae\x80\xe5\x8d\x95\xe6\x83\x85\xe5\x86\xb5\xe7\x9a\x84\xe8\x87\xaa\xe5\x8a\xa8\xe6\xb1\x82\xe5\xaf\xbc\n# \xe4\xb8\x8b\xe9\x9d\xa2\xe6\x88\x91\xe4\xbb\xac\xe6\x98\xbe\xe7\xa4\xba\xe4\xb8\x80\xe4\xba\x9b\xe7\xae\x80\xe5\x8d\x95\xe6\x83\x85\xe5\x86\xb5\xe7\x9a\x84\xe8\x87\xaa\xe5\x8a\xa8\xe6\xb1\x82\xe5\xaf\xbc\xef\xbc\x8c""\xe7\xae\x80\xe5\x8d\x95""\xe4\xbd\x93\xe7\x8e\xb0\xe5\x9c\xa8\xe8\xae\xa1\xe7\xae\x97\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x9c\xe9\x83\xbd\xe6\x98\xaf\xe6\xa0\x87\xe9\x87\x8f\xef\xbc\x8c\xe4\xb9\x9f\xe5\xb0\xb1\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa\xe6\x95\xb0\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe5\xaf\xb9\xe8\xbf\x99\xe4\xb8\xaa\xe6\xa0\x87\xe9\x87\x8f\xe8\xbf\x9b\xe8\xa1\x8c\xe8\x87\xaa\xe5\x8a\xa8\xe6\xb1\x82\xe5\xaf\xbc\xe3\x80\x82\n\n#%%\nx = torch.tensor([2.0], requires_grad=True)\ny = x + 2\nz = y ** 2 + 3\nprint(z)\n\n#%% [markdown]\n# \xe9\x80\x9a\xe8\xbf\x87\xe4\xb8\x8a\xe9\x9d\xa2\xe7\x9a\x84\xe4\xb8\x80\xe4\xba\x9b\xe5\x88\x97\xe6\x93\x8d\xe4\xbd\x9c\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe4\xbb\x8e x \xe5\xbe\x97\xe5\x88\xb0\xe4\xba\x86\xe6\x9c\x80\xe5\x90\x8e\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x9cout\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe5\x8f\xaf\xe4\xbb\xa5\xe5\xb0\x86\xe5\x85\xb6\xe8\xa1\xa8\xe7\xa4\xba\xe4\xb8\xba\xe6\x95\xb0\xe5\xad\xa6\xe5\x85\xac\xe5\xbc\x8f\n# \n# $$\n# z = (x + 2)^2 + 3\n# $$\n# \n# \xe9\x82\xa3\xe4\xb9\x88\xe6\x88\x91\xe4\xbb\xac\xe4\xbb\x8e z \xe5\xaf\xb9 x \xe6\xb1\x82\xe5\xaf\xbc\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x9c\xe5\xb0\xb1\xe6\x98\xaf \n# \n# $$\n# \\frac{\\partial z}{\\partial x} = 2 (x + 2) = 2 (2 + 2) = 8\n# $$\n# \xe5\xa6\x82\xe6\x9e\x9c\xe4\xbd\xa0\xe5\xaf\xb9\xe6\xb1\x82\xe5\xaf\xbc\xe4\xb8\x8d\xe7\x86\x9f\xe6\x82\x89\xef\xbc\x8c\xe5\x8f\xaf\xe4\xbb\xa5\xe6\x9f\xa5\xe7\x9c\x8b\xe4\xbb\xa5\xe4\xb8\x8b[\xe7\xbd\x91\xe5\x9d\x80\xe8\xbf\x9b\xe8\xa1\x8c\xe5\xa4\x8d\xe4\xb9\xa0](https://baike.baidu.com/item/%E5%AF%BC%E6%95%B0#1)\n\n#%%\n# \xe4\xbd\xbf\xe7\x94\xa8\xe8\x87\xaa\xe5\x8a\xa8\xe6\xb1\x82\xe5\xaf\xbc\nz.backward()\nprint(x.grad)\n\n#%% [markdown]\n# \xe5\xaf\xb9\xe4\xba\x8e\xe4\xb8\x8a\xe9\x9d\xa2\xe8\xbf\x99\xe6\xa0\xb7\xe4\xb8\x80\xe4\xb8\xaa\xe7\xae\x80\xe5\x8d\x95\xe7\x9a\x84\xe4\xbe\x8b\xe5\xad\x90\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe9\xaa\x8c\xe8\xaf\x81\xe4\xba\x86\xe8\x87\xaa\xe5\x8a\xa8\xe6\xb1\x82\xe5\xaf\xbc\xef\xbc\x8c\xe5\x90\x8c\xe6\x97\xb6\xe5\x8f\xaf\xe4\xbb\xa5\xe5\x8f\x91\xe7\x8e\xb0\xe5\x8f\x91\xe7\x8e\xb0\xe4\xbd\xbf\xe7\x94\xa8\xe8\x87\xaa\xe5\x8a\xa8\xe6\xb1\x82\xe5\xaf\xbc\xe9\x9d\x9e\xe5\xb8\xb8\xe6\x96\xb9\xe4\xbe\xbf\xe3\x80\x82\xe5\xa6\x82\xe6\x9e\x9c\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa\xe6\x9b\xb4\xe5\x8a\xa0\xe5\xa4\x8d\xe6\x9d\x82\xe7\x9a\x84\xe4\xbe\x8b\xe5\xad\x90\xef\xbc\x8c\xe9\x82\xa3\xe4\xb9\x88\xe6\x89\x8b\xe5\x8a\xa8\xe6\xb1\x82\xe5\xaf\xbc\xe5\xb0\xb1\xe4\xbc\x9a\xe6\x98\xbe\xe5\xbe\x97\xe9\x9d\x9e\xe5\xb8\xb8\xe7\x9a\x84\xe9\xba\xbb\xe7\x83\xa6\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe8\x87\xaa\xe5\x8a\xa8\xe6\xb1\x82\xe5\xaf\xbc\xe7\x9a\x84\xe6\x9c\xba\xe5\x88\xb6\xe8\x83\xbd\xe5\xa4\x9f\xe5\xb8\xae\xe5\x8a\xa9\xe6\x88\x91\xe4\xbb\xac\xe7\x9c\x81\xe5\x8e\xbb\xe9\xba\xbb\xe7\x83\xa6\xe7\x9a\x84\xe6\x95\xb0\xe5\xad\xa6\xe8\xae\xa1\xe7\xae\x97\xef\xbc\x8c\xe4\xb8\x8b\xe9\x9d\xa2\xe6\x88\x91\xe4\xbb\xac\xe5\x8f\xaf\xe4\xbb\xa5\xe7\x9c\x8b\xe4\xb8\x80\xe4\xb8\xaa\xe6\x9b\xb4\xe5\x8a\xa0\xe5\xa4\x8d\xe6\x9d\x82\xe7\x9a\x84\xe4\xbe\x8b\xe5\xad\x90\xe3\x80\x82\n\n#%%\nx = torch.randn(10, 20, requires_grad=True)\ny = torch.randn(10, 5, requires_grad=True)\nw = torch.randn(20, 5, requires_grad=True)\n\nout = torch.mean(y - torch.matmul(x, w)) # torch.matmul \xe6\x98\xaf\xe5\x81\x9a\xe7\x9f\xa9\xe9\x98\xb5\xe4\xb9\x98\xe6\xb3\x95\nout.backward()\n\n#%% [markdown]\n# \xe5\xa6\x82\xe6\x9e\x9c\xe4\xbd\xa0\xe5\xaf\xb9\xe7\x9f\xa9\xe9\x98\xb5\xe4\xb9\x98\xe6\xb3\x95\xe4\xb8\x8d\xe7\x86\x9f\xe6\x82\x89\xef\xbc\x8c\xe5\x8f\xaf\xe4\xbb\xa5\xe6\x9f\xa5\xe7\x9c\x8b\xe4\xb8\x8b\xe9\x9d\xa2\xe7\x9a\x84[\xe7\xbd\x91\xe5\x9d\x80\xe8\xbf\x9b\xe8\xa1\x8c\xe5\xa4\x8d\xe4\xb9\xa0](https://baike.baidu.com/item/%E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95/5446029?fr=aladdin)\n\n#%%\n# \xe5\xbe\x97\xe5\x88\xb0 x \xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\nprint(x.grad)\n\n\n#%%\n# \xe5\xbe\x97\xe5\x88\xb0 y \xe7\x9a\x84\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\nprint(y.grad)\n\n\n#%%\n# \xe5\xbe\x97\xe5\x88\xb0 w \xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\nprint(w.grad)\n\n#%% [markdown]\n# \xe4\xb8\x8a\xe9\x9d\xa2\xe6\x95\xb0\xe5\xad\xa6\xe5\x85\xac\xe5\xbc\x8f\xe5\xb0\xb1\xe6\x9b\xb4\xe5\x8a\xa0\xe5\xa4\x8d\xe6\x9d\x82\xef\xbc\x8c\xe7\x9f\xa9\xe9\x98\xb5\xe4\xb9\x98\xe6\xb3\x95\xe4\xb9\x8b\xe5\x90\x8e\xe5\xaf\xb9\xe4\xb8\xa4\xe4\xb8\xaa\xe7\x9f\xa9\xe9\x98\xb5\xe5\xaf\xb9\xe5\xba\x94\xe5\x85\x83\xe7\xb4\xa0\xe7\x9b\xb8\xe4\xb9\x98\xef\xbc\x8c\xe7\x84\xb6\xe5\x90\x8e\xe6\x89\x80\xe6\x9c\x89\xe5\x85\x83\xe7\xb4\xa0\xe6\xb1\x82\xe5\xb9\xb3\xe5\x9d\x87\xef\xbc\x8c\xe6\x9c\x89\xe5\x85\xb4\xe8\xb6\xa3\xe7\x9a\x84\xe5\x90\x8c\xe5\xad\xa6\xe5\x8f\xaf\xe4\xbb\xa5\xe6\x89\x8b\xe5\x8a\xa8\xe5\x8e\xbb\xe8\xae\xa1\xe7\xae\x97\xe4\xb8\x80\xe4\xb8\x8b\xe6\xa2\xaf\xe5\xba\xa6\xef\xbc\x8c\xe4\xbd\xbf\xe7\x94\xa8 PyTorch \xe7\x9a\x84\xe8\x87\xaa\xe5\x8a\xa8\xe6\xb1\x82\xe5\xaf\xbc\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe8\x83\xbd\xe5\xa4\x9f\xe9\x9d\x9e\xe5\xb8\xb8\xe5\xae\xb9\xe6\x98\x93\xe5\xbe\x97\xe5\x88\xb0 x, y \xe5\x92\x8c w \xe7\x9a\x84\xe5\xaf\xbc\xe6\x95\xb0\xef\xbc\x8c\xe5\x9b\xa0\xe4\xb8\xba\xe6\xb7\xb1\xe5\xba\xa6\xe5\xad\xa6\xe4\xb9\xa0\xe4\xb8\xad\xe5\x85\x85\xe6\xbb\xa1\xe5\xa4\xa7\xe9\x87\x8f\xe7\x9a\x84\xe7\x9f\xa9\xe9\x98\xb5\xe8\xbf\x90\xe7\xae\x97\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe6\x88\x91\xe4\xbb\xac\xe6\xb2\xa1\xe6\x9c\x89\xe5\x8a\x9e\xe6\xb3\x95\xe6\x89\x8b\xe5\x8a\xa8\xe5\x8e\xbb\xe6\xb1\x82\xe8\xbf\x99\xe4\xba\x9b\xe5\xaf\xbc\xe6\x95\xb0\xef\xbc\x8c\xe6\x9c\x89\xe4\xba\x86\xe8\x87\xaa\xe5\x8a\xa8\xe6\xb1\x82\xe5\xaf\xbc\xe8\x83\xbd\xe5\xa4\x9f\xe9\x9d\x9e\xe5\xb8\xb8\xe6\x96\xb9\xe4\xbe\xbf\xe5\x9c\xb0\xe8\xa7\xa3\xe5\x86\xb3\xe7\xbd\x91\xe7\xbb\x9c\xe6\x9b\xb4\xe6\x96\xb0\xe7\x9a\x84\xe9\x97\xae\xe9\xa2\x98\xe3\x80\x82\n#%% [markdown]\n# ## \xe5\xa4\x8d\xe6\x9d\x82\xe6\x83\x85\xe5\x86\xb5\xe7\x9a\x84\xe8\x87\xaa\xe5\x8a\xa8\xe6\xb1\x82\xe5\xaf\xbc\n# \xe4\xb8\x8a\xe9\x9d\xa2\xe6\x88\x91\xe4\xbb\xac\xe5\xb1\x95\xe7\xa4\xba\xe4\xba\x86\xe7\xae\x80\xe5\x8d\x95\xe6\x83\x85\xe5\x86\xb5\xe4\xb8\x8b\xe7\x9a\x84\xe8\x87\xaa\xe5\x8a\xa8\xe6\xb1\x82\xe5\xaf\xbc\xef\xbc\x8c\xe9\x83\xbd\xe6\x98\xaf\xe5\xaf\xb9\xe6\xa0\x87\xe9\x87\x8f\xe8\xbf\x9b\xe8\xa1\x8c\xe8\x87\xaa\xe5\x8a\xa8\xe6\xb1\x82\xe5\xaf\xbc\xef\xbc\x8c\xe5\x8f\xaf\xe8\x83\xbd\xe4\xbd\xa0\xe4\xbc\x9a\xe6\x9c\x89\xe4\xb8\x80\xe4\xb8\xaa\xe7\x96\x91\xe9\x97\xae\xef\xbc\x8c\xe5\xa6\x82\xe4\xbd\x95\xe5\xaf\xb9\xe4\xb8\x80\xe4\xb8\xaa\xe5\x90\x91\xe9\x87\x8f\xe6\x88\x96\xe8\x80\x85\xe7\x9f\xa9\xe9\x98\xb5\xe8\x87\xaa\xe5\x8a\xa8\xe6\xb1\x82\xe5\xaf\xbc\xe4\xba\x86\xe5\x91\xa2\xef\xbc\x9f\xe6\x84\x9f\xe5\x85\xb4\xe8\xb6\xa3\xe7\x9a\x84\xe5\x90\x8c\xe5\xad\xa6\xe5\x8f\xaf\xe4\xbb\xa5\xe8\x87\xaa\xe5\xb7\xb1\xe5\x85\x88\xe5\x8e\xbb\xe5\xb0\x9d\xe8\xaf\x95\xe4\xb8\x80\xe4\xb8\x8b\xef\xbc\x8c\xe4\xb8\x8b\xe9\x9d\xa2\xe6\x88\x91\xe4\xbb\xac\xe4\xbc\x9a\xe4\xbb\x8b\xe7\xbb\x8d\xe5\xaf\xb9\xe5\xa4\x9a\xe7\xbb\xb4\xe6\x95\xb0\xe7\xbb\x84\xe7\x9a\x84\xe8\x87\xaa\xe5\x8a\xa8\xe6\xb1\x82\xe5\xaf\xbc\xe6\x9c\xba\xe5\x88\xb6\xe3\x80\x82\n\n#%%\nm = torch.FloatTensor([[2, 3]]) # \xe6\x9e\x84\xe5\xbb\xba\xe4\xb8\x80\xe4\xb8\xaa 1 x 2 \xe7\x9a\x84\xe7\x9f\xa9\xe9\x98\xb5\nm.requires_grad_()\nn = torch.zeros(1, 2) # \xe6\x9e\x84\xe5\xbb\xba\xe4\xb8\x80\xe4\xb8\xaa\xe7\x9b\xb8\xe5\x90\x8c\xe5\xa4\xa7\xe5\xb0\x8f\xe7\x9a\x84 0 \xe7\x9f\xa9\xe9\x98\xb5\nprint(m)\nprint(n)\n\n\n#%%\n# \xe9\x80\x9a\xe8\xbf\x87 m \xe4\xb8\xad\xe7\x9a\x84\xe5\x80\xbc\xe8\xae\xa1\xe7\xae\x97\xe6\x96\xb0\xe7\x9a\x84 n \xe4\xb8\xad\xe7\x9a\x84\xe5\x80\xbc\nn[0, 0] = m[0, 0] ** 2\nn[0, 1] = m[0, 1] ** 3\nprint(n)\n\n#%% [markdown]\n# \xe5\xb0\x86\xe4\xb8\x8a\xe9\x9d\xa2\xe7\x9a\x84\xe5\xbc\x8f\xe5\xad\x90\xe5\x86\x99\xe6\x88\x90\xe6\x95\xb0\xe5\xad\xa6\xe5\x85\xac\xe5\xbc\x8f\xef\xbc\x8c\xe5\x8f\xaf\xe4\xbb\xa5\xe5\xbe\x97\xe5\x88\xb0 \n# $$\n# n = (n_0,\\ n_1) = (m_0^2,\\ m_1^3) = (2^2,\\ 3^3) \n# $$\n#%% [markdown]\n# \xe4\xb8\x8b\xe9\x9d\xa2\xe6\x88\x91\xe4\xbb\xac\xe7\x9b\xb4\xe6\x8e\xa5\xe5\xaf\xb9 n \xe8\xbf\x9b\xe8\xa1\x8c\xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xef\xbc\x8c\xe4\xb9\x9f\xe5\xb0\xb1\xe6\x98\xaf\xe6\xb1\x82 n \xe5\xaf\xb9 m \xe7\x9a\x84\xe5\xaf\xbc\xe6\x95\xb0\xe3\x80\x82\n# \n# \xe8\xbf\x99\xe6\x97\xb6\xe6\x88\x91\xe4\xbb\xac\xe9\x9c\x80\xe8\xa6\x81\xe6\x98\x8e\xe7\xa1\xae\xe8\xbf\x99\xe4\xb8\xaa\xe5\xaf\xbc\xe6\x95\xb0\xe7\x9a\x84\xe5\xae\x9a\xe4\xb9\x89\xef\xbc\x8c\xe5\x8d\xb3\xe5\xa6\x82\xe4\xbd\x95\xe5\xae\x9a\xe4\xb9\x89\n# \n# $$\n# \\frac{\\partial n}{\\partial m} = \\frac{\\partial (n_0,\\ n_1)}{\\partial (m_0,\\ m_1)}\n# $$\n# \n#%% [markdown]\n# \xe5\x9c\xa8 PyTorch \xe4\xb8\xad\xef\xbc\x8c\xe5\xa6\x82\xe6\x9e\x9c\xe8\xa6\x81\xe8\xb0\x83\xe7\x94\xa8\xe8\x87\xaa\xe5\x8a\xa8\xe6\xb1\x82\xe5\xaf\xbc\xef\xbc\x8c\xe9\x9c\x80\xe8\xa6\x81\xe5\xbe\x80`backward()`\xe4\xb8\xad\xe4\xbc\xa0\xe5\x85\xa5\xe4\xb8\x80\xe4\xb8\xaa\xe5\x8f\x82\xe6\x95\xb0\xef\xbc\x8c\xe8\xbf\x99\xe4\xb8\xaa\xe5\x8f\x82\xe6\x95\xb0\xe7\x9a\x84\xe5\xbd\xa2\xe7\x8a\xb6\xe5\x92\x8c n \xe4\xb8\x80\xe6\xa0\xb7\xe5\xa4\xa7\xef\xbc\x8c\xe6\xaf\x94\xe5\xa6\x82\xe6\x98\xaf $(w_0,\\ w_1)$\xef\xbc\x8c\xe9\x82\xa3\xe4\xb9\x88\xe8\x87\xaa\xe5\x8a\xa8\xe6\xb1\x82\xe5\xaf\xbc\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x9c\xe5\xb0\xb1\xe6\x98\xaf\xef\xbc\x9a\n# $$\n# \\frac{\\partial n}{\\partial m_0} = w_0 \\frac{\\partial n_0}{\\partial m_0} + w_1 \\frac{\\partial n_1}{\\partial m_0}\n# $$\n# $$\n# \\frac{\\partial n}{\\partial m_1} = w_0 \\frac{\\partial n_0}{\\partial m_1} + w_1 \\frac{\\partial n_1}{\\partial m_1}\n# $$\n\n#%%\nn.backward(torch.ones_like(n)) # \xe5\xb0\x86 (w0, w1) \xe5\x8f\x96\xe6\x88\x90 (1, 1)\n\n\n#%%\nprint(m.grad)\n\n#%% [markdown]\n# \xe9\x80\x9a\xe8\xbf\x87\xe8\x87\xaa\xe5\x8a\xa8\xe6\xb1\x82\xe5\xaf\xbc\xe6\x88\x91\xe4\xbb\xac\xe5\xbe\x97\xe5\x88\xb0\xe4\xba\x86\xe6\xa2\xaf\xe5\xba\xa6\xe6\x98\xaf 4 \xe5\x92\x8c 27\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe5\x8f\xaf\xe4\xbb\xa5\xe9\xaa\x8c\xe7\xae\x97\xe4\xb8\x80\xe4\xb8\x8b\n# $$\n# \\frac{\\partial n}{\\partial m_0} = w_0 \\frac{\\partial n_0}{\\partial m_0} + w_1 \\frac{\\partial n_1}{\\partial m_0} = 2 m_0 + 0 = 2 \\times 2 = 4\n# $$\n# $$\n# \\frac{\\partial n}{\\partial m_1} = w_0 \\frac{\\partial n_0}{\\partial m_1} + w_1 \\frac{\\partial n_1}{\\partial m_1} = 0 + 3 m_1^2 = 3 \\times 3^2 = 27\n# $$\n# \xe9\x80\x9a\xe8\xbf\x87\xe9\xaa\x8c\xe7\xae\x97\xe6\x88\x91\xe4\xbb\xac\xe5\x8f\xaf\xe4\xbb\xa5\xe5\xbe\x97\xe5\x88\xb0\xe7\x9b\xb8\xe5\x90\x8c\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x9c\n#%% [markdown]\n# ## \xe5\xa4\x9a\xe6\xac\xa1\xe8\x87\xaa\xe5\x8a\xa8\xe6\xb1\x82\xe5\xaf\xbc\n# \xe9\x80\x9a\xe8\xbf\x87\xe8\xb0\x83\xe7\x94\xa8 backward \xe6\x88\x91\xe4\xbb\xac\xe5\x8f\xaf\xe4\xbb\xa5\xe8\xbf\x9b\xe8\xa1\x8c\xe4\xb8\x80\xe6\xac\xa1\xe8\x87\xaa\xe5\x8a\xa8\xe6\xb1\x82\xe5\xaf\xbc\xef\xbc\x8c\xe5\xa6\x82\xe6\x9e\x9c\xe6\x88\x91\xe4\xbb\xac\xe5\x86\x8d\xe8\xb0\x83\xe7\x94\xa8\xe4\xb8\x80\xe6\xac\xa1 backward\xef\xbc\x8c\xe4\xbc\x9a\xe5\x8f\x91\xe7\x8e\xb0\xe7\xa8\x8b\xe5\xba\x8f\xe6\x8a\xa5\xe9\x94\x99\xef\xbc\x8c\xe6\xb2\xa1\xe6\x9c\x89\xe5\x8a\x9e\xe6\xb3\x95\xe5\x86\x8d\xe5\x81\x9a\xe4\xb8\x80\xe6\xac\xa1\xe3\x80\x82\xe8\xbf\x99\xe6\x98\xaf\xe5\x9b\xa0\xe4\xb8\xba PyTorch \xe9\xbb\x98\xe8\xae\xa4\xe5\x81\x9a\xe5\xae\x8c\xe4\xb8\x80\xe6\xac\xa1\xe8\x87\xaa\xe5\x8a\xa8\xe6\xb1\x82\xe5\xaf\xbc\xe4\xb9\x8b\xe5\x90\x8e\xef\xbc\x8c\xe8\xae\xa1\xe7\xae\x97\xe5\x9b\xbe\xe5\xb0\xb1\xe8\xa2\xab\xe4\xb8\xa2\xe5\xbc\x83\xe4\xba\x86\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe4\xb8\xa4\xe6\xac\xa1\xe8\x87\xaa\xe5\x8a\xa8\xe6\xb1\x82\xe5\xaf\xbc\xe9\x9c\x80\xe8\xa6\x81\xe6\x89\x8b\xe5\x8a\xa8\xe8\xae\xbe\xe7\xbd\xae\xe4\xb8\x80\xe4\xb8\xaa\xe4\xb8\x9c\xe8\xa5\xbf\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe9\x80\x9a\xe8\xbf\x87\xe4\xb8\x8b\xe9\x9d\xa2\xe7\x9a\x84\xe5\xb0\x8f\xe4\xbe\x8b\xe5\xad\x90\xe6\x9d\xa5\xe8\xaf\xb4\xe6\x98\x8e\xe3\x80\x82\n\n#%%\nx = torch.tensor([3.], requires_grad=True)\ny = x * 2 + x ** 2 + 3\nprint(y)\n\n\n#%%\ny.backward(retain_graph=True) # \xe8\xae\xbe\xe7\xbd\xae retain_graph \xe4\xb8\xba True \xe6\x9d\xa5\xe4\xbf\x9d\xe7\x95\x99\xe8\xae\xa1\xe7\xae\x97\xe5\x9b\xbe\n\n\n#%%\nprint(x.grad)\n\n\n#%%\ny.backward() # \xe5\x86\x8d\xe5\x81\x9a\xe4\xb8\x80\xe6\xac\xa1\xe8\x87\xaa\xe5\x8a\xa8\xe6\xb1\x82\xe5\xaf\xbc\xef\xbc\x8c\xe8\xbf\x99\xe6\xac\xa1\xe4\xb8\x8d\xe4\xbf\x9d\xe7\x95\x99\xe8\xae\xa1\xe7\xae\x97\xe5\x9b\xbe\n\n\n#%%\nprint(x.grad)\n\n#%% [markdown]\n# \xe5\x8f\xaf\xe4\xbb\xa5\xe5\x8f\x91\xe7\x8e\xb0 x \xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\xe5\x8f\x98\xe6\x88\x90\xe4\xba\x86 16\xef\xbc\x8c\xe5\x9b\xa0\xe4\xb8\xba\xe8\xbf\x99\xe9\x87\x8c\xe5\x81\x9a\xe4\xba\x86\xe4\xb8\xa4\xe6\xac\xa1\xe8\x87\xaa\xe5\x8a\xa8\xe6\xb1\x82\xe5\xaf\xbc\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe8\xae\xb2\xe7\xac\xac\xe4\xb8\x80\xe6\xac\xa1\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6 8 \xe5\x92\x8c\xe7\xac\xac\xe4\xba\x8c\xe6\xac\xa1\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6 8 \xe5\x8a\xa0\xe8\xb5\xb7\xe6\x9d\xa5\xe5\xbe\x97\xe5\x88\xb0\xe4\xba\x86 16 \xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x9c\xe3\x80\x82\n#%% [markdown]\n# **\xe5\xb0\x8f\xe7\xbb\x83\xe4\xb9\xa0**\n# \n# \xe5\xae\x9a\xe4\xb9\x89\n# \n# $$\n# x = \n# \\left[\n# \\begin{matrix}\n# x_0 \\\\\n# x_1\n# \\end{matrix}\n# \\right] = \n# \\left[\n# \\begin{matrix}\n# 2 \\\\\n# 3\n# \\end{matrix}\n# \\right]\n# $$\n# \n# $$\n# k = (k_0,\\ k_1) = (x_0^2 + 3 x_1,\\ 2 x_0 + x_1^2)\n# $$\n# \n# \xe6\x88\x91\xe4\xbb\xac\xe5\xb8\x8c\xe6\x9c\x9b\xe6\xb1\x82\xe5\xbe\x97\n# \n# $$\n# j = \\left[\n# \\begin{matrix}\n# \\frac{\\partial k_0}{\\partial x_0} & \\frac{\\partial k_0}{\\partial x_1} \\\\\n# \\frac{\\partial k_1}{\\partial x_0} & \\frac{\\partial k_1}{\\partial x_1}\n# \\end{matrix}\n# \\right]\n# $$\n# \n# \xe5\x8f\x82\xe8\x80\x83\xe7\xad\x94\xe6\xa1\x88\xef\xbc\x9a\n# \n# $$\n# \\left[\n# \\begin{matrix}\n# 4 & 3 \\\\\n# 2 & 6 \\\\\n# \\end{matrix}\n# \\right]\n# $$\n\n#%%\nx = torch.tensor([2, 3], dtype=torch.float,  requires_grad=True)\nk = torch.zeros(2)\n\nk[0] = x[0] ** 2 + 3 * x[1]\nk[1] = x[1] ** 2 + 2 * x[0]\n\n\n#%%\nprint(k)\n\n\n#%%\nj = torch.zeros(2, 2)\n\nk.backward(torch.FloatTensor([1, 0]), retain_graph=True)\n\n\n#%%\nj[0] = x.grad\n\nx.grad.zero_() # \xe5\xbd\x92\xe9\x9b\xb6\xe4\xb9\x8b\xe5\x89\x8d\xe6\xb1\x82\xe5\xbe\x97\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\n\nk.backward(torch.FloatTensor([0, 1]))\nj[1] = x.grad.data\n\n\n#%%\nprint(j)\n'"
2.PyTorch _basics PyTorch基础/autograd 自动求导.py,3,"b""#%% [markdown]\n# ## 3.2 autograd\n# \n# \xe7\x94\xa8Tensor\xe8\xae\xad\xe7\xbb\x83\xe7\xbd\x91\xe7\xbb\x9c\xe5\xbe\x88\xe6\x96\xb9\xe4\xbe\xbf\xef\xbc\x8c\xe4\xbd\x86\xe4\xbb\x8e\xe4\xb8\x8a\xe4\xb8\x80\xe5\xb0\x8f\xe8\x8a\x82\xe6\x9c\x80\xe5\x90\x8e\xe7\x9a\x84\xe7\xba\xbf\xe6\x80\xa7\xe5\x9b\x9e\xe5\xbd\x92\xe4\xbe\x8b\xe5\xad\x90\xe6\x9d\xa5\xe7\x9c\x8b\xef\xbc\x8c\xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe8\xbf\x87\xe7\xa8\x8b\xe9\x9c\x80\xe8\xa6\x81\xe6\x89\x8b\xe5\x8a\xa8\xe5\xae\x9e\xe7\x8e\xb0\xe3\x80\x82\xe8\xbf\x99\xe5\xaf\xb9\xe4\xba\x8e\xe5\x83\x8f\xe7\xba\xbf\xe6\x80\xa7\xe5\x9b\x9e\xe5\xbd\x92\xe7\xad\x89\xe8\xbe\x83\xe4\xb8\xba\xe7\xae\x80\xe5\x8d\x95\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\xe6\x9d\xa5\xe8\xaf\xb4\xef\xbc\x8c\xe8\xbf\x98\xe5\x8f\xaf\xe4\xbb\xa5\xe5\xba\x94\xe4\xbb\x98\xef\xbc\x8c\xe4\xbd\x86\xe5\xae\x9e\xe9\x99\x85\xe4\xbd\xbf\xe7\x94\xa8\xe4\xb8\xad\xe7\xbb\x8f\xe5\xb8\xb8\xe5\x87\xba\xe7\x8e\xb0\xe9\x9d\x9e\xe5\xb8\xb8\xe5\xa4\x8d\xe6\x9d\x82\xe7\x9a\x84\xe7\xbd\x91\xe7\xbb\x9c\xe7\xbb\x93\xe6\x9e\x84\xef\xbc\x8c\xe6\xad\xa4\xe6\x97\xb6\xe5\xa6\x82\xe6\x9e\x9c\xe6\x89\x8b\xe5\x8a\xa8\xe5\xae\x9e\xe7\x8e\xb0\xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xef\xbc\x8c\xe4\xb8\x8d\xe4\xbb\x85\xe8\xb4\xb9\xe6\x97\xb6\xe8\xb4\xb9\xe5\x8a\x9b\xef\xbc\x8c\xe8\x80\x8c\xe4\xb8\x94\xe5\xae\xb9\xe6\x98\x93\xe5\x87\xba\xe9\x94\x99\xef\xbc\x8c\xe9\x9a\xbe\xe4\xbb\xa5\xe6\xa3\x80\xe6\x9f\xa5\xe3\x80\x82torch.autograd\xe5\xb0\xb1\xe6\x98\xaf\xe4\xb8\xba\xe6\x96\xb9\xe4\xbe\xbf\xe7\x94\xa8\xe6\x88\xb7\xe4\xbd\xbf\xe7\x94\xa8\xef\xbc\x8c\xe8\x80\x8c\xe4\xb8\x93\xe9\x97\xa8\xe5\xbc\x80\xe5\x8f\x91\xe7\x9a\x84\xe4\xb8\x80\xe5\xa5\x97\xe8\x87\xaa\xe5\x8a\xa8\xe6\xb1\x82\xe5\xaf\xbc\xe5\xbc\x95\xe6\x93\x8e\xef\xbc\x8c\xe5\xae\x83\xe8\x83\xbd\xe5\xa4\x9f\xe6\xa0\xb9\xe6\x8d\xae\xe8\xbe\x93\xe5\x85\xa5\xe5\x92\x8c\xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe8\xbf\x87\xe7\xa8\x8b\xe8\x87\xaa\xe5\x8a\xa8\xe6\x9e\x84\xe5\xbb\xba\xe8\xae\xa1\xe7\xae\x97\xe5\x9b\xbe\xef\xbc\x8c\xe5\xb9\xb6\xe6\x89\xa7\xe8\xa1\x8c\xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe3\x80\x82\n# \n# \xe8\xae\xa1\xe7\xae\x97\xe5\x9b\xbe(Computation Graph)\xe6\x98\xaf\xe7\x8e\xb0\xe4\xbb\xa3\xe6\xb7\xb1\xe5\xba\xa6\xe5\xad\xa6\xe4\xb9\xa0\xe6\xa1\x86\xe6\x9e\xb6\xe5\xa6\x82PyTorch\xe5\x92\x8cTensorFlow\xe7\xad\x89\xe7\x9a\x84\xe6\xa0\xb8\xe5\xbf\x83\xef\xbc\x8c\xe5\x85\xb6\xe4\xb8\xba\xe9\xab\x98\xe6\x95\x88\xe8\x87\xaa\xe5\x8a\xa8\xe6\xb1\x82\xe5\xaf\xbc\xe7\xae\x97\xe6\xb3\x95\xe2\x80\x94\xe2\x80\x94\xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad(Back Propogation)\xe6\x8f\x90\xe4\xbe\x9b\xe4\xba\x86\xe7\x90\x86\xe8\xae\xba\xe6\x94\xaf\xe6\x8c\x81\xef\xbc\x8c\xe4\xba\x86\xe8\xa7\xa3\xe8\xae\xa1\xe7\xae\x97\xe5\x9b\xbe\xe5\x9c\xa8\xe5\xae\x9e\xe9\x99\x85\xe5\x86\x99\xe7\xa8\x8b\xe5\xba\x8f\xe8\xbf\x87\xe7\xa8\x8b\xe4\xb8\xad\xe4\xbc\x9a\xe6\x9c\x89\xe6\x9e\x81\xe5\xa4\xa7\xe7\x9a\x84\xe5\xb8\xae\xe5\x8a\xa9\xe3\x80\x82\xe6\x9c\xac\xe8\x8a\x82\xe5\xb0\x86\xe6\xb6\x89\xe5\x8f\x8a\xe4\xb8\x80\xe4\xba\x9b\xe5\x9f\xba\xe7\xa1\x80\xe7\x9a\x84\xe8\xae\xa1\xe7\xae\x97\xe5\x9b\xbe\xe7\x9f\xa5\xe8\xaf\x86\xef\xbc\x8c\xe4\xbd\x86\xe5\xb9\xb6\xe4\xb8\x8d\xe8\xa6\x81\xe6\xb1\x82\xe8\xaf\xbb\xe8\x80\x85\xe4\xba\x8b\xe5\x85\x88\xe5\xaf\xb9\xe6\xad\xa4\xe6\x9c\x89\xe6\xb7\xb1\xe5\x85\xa5\xe7\x9a\x84\xe4\xba\x86\xe8\xa7\xa3\xe3\x80\x82\xe5\x85\xb3\xe4\xba\x8e\xe8\xae\xa1\xe7\xae\x97\xe5\x9b\xbe\xe7\x9a\x84\xe5\x9f\xba\xe7\xa1\x80\xe7\x9f\xa5\xe8\xaf\x86\xe6\x8e\xa8\xe8\x8d\x90\xe9\x98\x85\xe8\xaf\xbbChristopher Olah\xe7\x9a\x84\xe6\x96\x87\xe7\xab\xa0[^1]\xe3\x80\x82\n# \n# [^1]: http://colah.github.io/posts/2015-08-Backprop/\n# \n# \n# ### 3.2.1 requires_grad\n# PyTorch\xe5\x9c\xa8autograd\xe6\xa8\xa1\xe5\x9d\x97\xe4\xb8\xad\xe5\xae\x9e\xe7\x8e\xb0\xe4\xba\x86\xe8\xae\xa1\xe7\xae\x97\xe5\x9b\xbe\xe7\x9a\x84\xe7\x9b\xb8\xe5\x85\xb3\xe5\x8a\x9f\xe8\x83\xbd\xef\xbc\x8cautograd\xe4\xb8\xad\xe7\x9a\x84\xe6\xa0\xb8\xe5\xbf\x83\xe6\x95\xb0\xe6\x8d\xae\xe7\xbb\x93\xe6\x9e\x84\xe6\x98\xafVariable\xe3\x80\x82\xe4\xbb\x8ev0.4\xe7\x89\x88\xe6\x9c\xac\xe8\xb5\xb7\xef\xbc\x8cVariable\xe5\x92\x8cTensor\xe5\x90\x88\xe5\xb9\xb6\xe3\x80\x82\xe6\x88\x91\xe4\xbb\xac\xe5\x8f\xaf\xe4\xbb\xa5\xe8\xae\xa4\xe4\xb8\xba\xe9\x9c\x80\xe8\xa6\x81\xe6\xb1\x82\xe5\xaf\xbc(requires_grad)\xe7\x9a\x84tensor\xe5\x8d\xb3Variable. autograd\xe8\xae\xb0\xe5\xbd\x95\xe5\xaf\xb9tensor\xe7\x9a\x84\xe6\x93\x8d\xe4\xbd\x9c\xe8\xae\xb0\xe5\xbd\x95\xe7\x94\xa8\xe6\x9d\xa5\xe6\x9e\x84\xe5\xbb\xba\xe8\xae\xa1\xe7\xae\x97\xe5\x9b\xbe\xe3\x80\x82\n# \n# Variable\xe6\x8f\x90\xe4\xbe\x9b\xe4\xba\x86\xe5\xa4\xa7\xe9\x83\xa8\xe5\x88\x86tensor\xe6\x94\xaf\xe6\x8c\x81\xe7\x9a\x84\xe5\x87\xbd\xe6\x95\xb0\xef\xbc\x8c\xe4\xbd\x86\xe5\x85\xb6\xe4\xb8\x8d\xe6\x94\xaf\xe6\x8c\x81\xe9\x83\xa8\xe5\x88\x86`inplace`\xe5\x87\xbd\xe6\x95\xb0\xef\xbc\x8c\xe5\x9b\xa0\xe8\xbf\x99\xe4\xba\x9b\xe5\x87\xbd\xe6\x95\xb0\xe4\xbc\x9a\xe4\xbf\xae\xe6\x94\xb9tensor\xe8\x87\xaa\xe8\xba\xab\xef\xbc\x8c\xe8\x80\x8c\xe5\x9c\xa8\xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe4\xb8\xad\xef\xbc\x8cvariable\xe9\x9c\x80\xe8\xa6\x81\xe7\xbc\x93\xe5\xad\x98\xe5\x8e\x9f\xe6\x9d\xa5\xe7\x9a\x84tensor\xe6\x9d\xa5\xe8\xae\xa1\xe7\xae\x97\xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe6\xa2\xaf\xe5\xba\xa6\xe3\x80\x82\xe5\xa6\x82\xe6\x9e\x9c\xe6\x83\xb3\xe8\xa6\x81\xe8\xae\xa1\xe7\xae\x97\xe5\x90\x84\xe4\xb8\xaaVariable\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\xef\xbc\x8c\xe5\x8f\xaa\xe9\x9c\x80\xe8\xb0\x83\xe7\x94\xa8\xe6\xa0\xb9\xe8\x8a\x82\xe7\x82\xb9variable\xe7\x9a\x84`backward`\xe6\x96\xb9\xe6\xb3\x95\xef\xbc\x8cautograd\xe4\xbc\x9a\xe8\x87\xaa\xe5\x8a\xa8\xe6\xb2\xbf\xe7\x9d\x80\xe8\xae\xa1\xe7\xae\x97\xe5\x9b\xbe\xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xef\xbc\x8c\xe8\xae\xa1\xe7\xae\x97\xe6\xaf\x8f\xe4\xb8\x80\xe4\xb8\xaa\xe5\x8f\xb6\xe5\xad\x90\xe8\x8a\x82\xe7\x82\xb9\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\xe3\x80\x82\n# \n# `variable.backward(gradient=None, retain_graph=None, create_graph=None)`\xe4\xb8\xbb\xe8\xa6\x81\xe6\x9c\x89\xe5\xa6\x82\xe4\xb8\x8b\xe5\x8f\x82\xe6\x95\xb0\xef\xbc\x9a\n# \n# - grad_variables\xef\xbc\x9a\xe5\xbd\xa2\xe7\x8a\xb6\xe4\xb8\x8evariable\xe4\xb8\x80\xe8\x87\xb4\xef\xbc\x8c\xe5\xaf\xb9\xe4\xba\x8e`y.backward()`\xef\xbc\x8cgrad_variables\xe7\x9b\xb8\xe5\xbd\x93\xe4\xba\x8e\xe9\x93\xbe\xe5\xbc\x8f\xe6\xb3\x95\xe5\x88\x99${dz \\over dx}={dz \\over dy} \\times {dy \\over dx}$\xe4\xb8\xad\xe7\x9a\x84$\\textbf {dz} \\over \\textbf {dy}$\xe3\x80\x82grad_variables\xe4\xb9\x9f\xe5\x8f\xaf\xe4\xbb\xa5\xe6\x98\xaftensor\xe6\x88\x96\xe5\xba\x8f\xe5\x88\x97\xe3\x80\x82\n# - retain_graph\xef\xbc\x9a\xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe9\x9c\x80\xe8\xa6\x81\xe7\xbc\x93\xe5\xad\x98\xe4\xb8\x80\xe4\xba\x9b\xe4\xb8\xad\xe9\x97\xb4\xe7\xbb\x93\xe6\x9e\x9c\xef\xbc\x8c\xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe4\xb9\x8b\xe5\x90\x8e\xef\xbc\x8c\xe8\xbf\x99\xe4\xba\x9b\xe7\xbc\x93\xe5\xad\x98\xe5\xb0\xb1\xe8\xa2\xab\xe6\xb8\x85\xe7\xa9\xba\xef\xbc\x8c\xe5\x8f\xaf\xe9\x80\x9a\xe8\xbf\x87\xe6\x8c\x87\xe5\xae\x9a\xe8\xbf\x99\xe4\xb8\xaa\xe5\x8f\x82\xe6\x95\xb0\xe4\xb8\x8d\xe6\xb8\x85\xe7\xa9\xba\xe7\xbc\x93\xe5\xad\x98\xef\xbc\x8c\xe7\x94\xa8\xe6\x9d\xa5\xe5\xa4\x9a\xe6\xac\xa1\xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe3\x80\x82\n# - create_graph\xef\xbc\x9a\xe5\xaf\xb9\xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe8\xbf\x87\xe7\xa8\x8b\xe5\x86\x8d\xe6\xac\xa1\xe6\x9e\x84\xe5\xbb\xba\xe8\xae\xa1\xe7\xae\x97\xe5\x9b\xbe\xef\xbc\x8c\xe5\x8f\xaf\xe9\x80\x9a\xe8\xbf\x87`backward of backward`\xe5\xae\x9e\xe7\x8e\xb0\xe6\xb1\x82\xe9\xab\x98\xe9\x98\xb6\xe5\xaf\xbc\xe6\x95\xb0\xe3\x80\x82\n# \n# \xe4\xb8\x8a\xe8\xbf\xb0\xe6\x8f\x8f\xe8\xbf\xb0\xe5\x8f\xaf\xe8\x83\xbd\xe6\xaf\x94\xe8\xbe\x83\xe6\x8a\xbd\xe8\xb1\xa1\xef\xbc\x8c\xe5\xa6\x82\xe6\x9e\x9c\xe6\xb2\xa1\xe6\x9c\x89\xe7\x9c\x8b\xe6\x87\x82\xef\xbc\x8c\xe4\xb8\x8d\xe7\x94\xa8\xe7\x9d\x80\xe6\x80\xa5\xef\xbc\x8c\xe4\xbc\x9a\xe5\x9c\xa8\xe6\x9c\xac\xe8\x8a\x82\xe5\x90\x8e\xe5\x8d\x8a\xe9\x83\xa8\xe5\x88\x86\xe8\xaf\xa6\xe7\xbb\x86\xe4\xbb\x8b\xe7\xbb\x8d\xef\xbc\x8c\xe4\xb8\x8b\xe9\x9d\xa2\xe5\x85\x88\xe7\x9c\x8b\xe5\x87\xa0\xe4\xb8\xaa\xe4\xbe\x8b\xe5\xad\x90\xe3\x80\x82\n\n#%%\nfrom __future__ import print_function\nimport torch as t\n\n\n#%%\n#\xe5\x9c\xa8\xe5\x88\x9b\xe5\xbb\xbatensor\xe7\x9a\x84\xe6\x97\xb6\xe5\x80\x99\xe6\x8c\x87\xe5\xae\x9arequires_grad\na = t.randn(3,4, requires_grad=True)\n# \xe6\x88\x96\xe8\x80\x85\na = t.randn(3,4).requires_grad_()\n# \xe6\x88\x96\xe8\x80\x85\na = t.randn(3,4)\na.requires_grad=True\na\n\n\n#%%\nb = t.zeros(3,4).requires_grad_()\nb\n\n\n#%%\n# \xe4\xb9\x9f\xe5\x8f\xaf\xe5\x86\x99\xe6\x88\x90c = a + b\nc = a.add(b)\nc\n\n\n#%%\nd = c.sum()\nd.backward() # \xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\n\n\n#%%\nd # d\xe8\xbf\x98\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaarequires_grad=True\xe7\x9a\x84tensor,\xe5\xaf\xb9\xe5\xae\x83\xe7\x9a\x84\xe6\x93\x8d\xe4\xbd\x9c\xe9\x9c\x80\xe8\xa6\x81\xe6\x85\x8e\xe9\x87\x8d\nd.requires_grad\n\n\n#%%\na.grad\n\n\n#%%\n# \xe6\xad\xa4\xe5\xa4\x84\xe8\x99\xbd\xe7\x84\xb6\xe6\xb2\xa1\xe6\x9c\x89\xe6\x8c\x87\xe5\xae\x9ac\xe9\x9c\x80\xe8\xa6\x81\xe6\xb1\x82\xe5\xaf\xbc\xef\xbc\x8c\xe4\xbd\x86c\xe4\xbe\x9d\xe8\xb5\x96\xe4\xba\x8ea\xef\xbc\x8c\xe8\x80\x8ca\xe9\x9c\x80\xe8\xa6\x81\xe6\xb1\x82\xe5\xaf\xbc\xef\xbc\x8c\n# \xe5\x9b\xa0\xe6\xad\xa4c\xe7\x9a\x84requires_grad\xe5\xb1\x9e\xe6\x80\xa7\xe4\xbc\x9a\xe8\x87\xaa\xe5\x8a\xa8\xe8\xae\xbe\xe4\xb8\xbaTrue\na.requires_grad, b.requires_grad, c.requires_grad\n\n\n#%%\n# \xe7\x94\xb1\xe7\x94\xa8\xe6\x88\xb7\xe5\x88\x9b\xe5\xbb\xba\xe7\x9a\x84variable\xe5\xb1\x9e\xe4\xba\x8e\xe5\x8f\xb6\xe5\xad\x90\xe8\x8a\x82\xe7\x82\xb9\xef\xbc\x8c\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84grad_fn\xe6\x98\xafNone\na.is_leaf, b.is_leaf, c.is_leaf\n\n\n#%%\n# c.grad\xe6\x98\xafNone, \xe5\x9b\xa0c\xe4\xb8\x8d\xe6\x98\xaf\xe5\x8f\xb6\xe5\xad\x90\xe8\x8a\x82\xe7\x82\xb9\xef\xbc\x8c\xe5\xae\x83\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\xe6\x98\xaf\xe7\x94\xa8\xe6\x9d\xa5\xe8\xae\xa1\xe7\xae\x97a\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\n# \xe6\x89\x80\xe4\xbb\xa5\xe8\x99\xbd\xe7\x84\xb6c.requires_grad = True,\xe4\xbd\x86\xe5\x85\xb6\xe6\xa2\xaf\xe5\xba\xa6\xe8\xae\xa1\xe7\xae\x97\xe5\xae\x8c\xe4\xb9\x8b\xe5\x90\x8e\xe5\x8d\xb3\xe8\xa2\xab\xe9\x87\x8a\xe6\x94\xbe\nc.grad is None\n\n#%% [markdown]\n# \xe8\xae\xa1\xe7\xae\x97\xe4\xb8\x8b\xe9\x9d\xa2\xe8\xbf\x99\xe4\xb8\xaa\xe5\x87\xbd\xe6\x95\xb0\xe7\x9a\x84\xe5\xaf\xbc\xe5\x87\xbd\xe6\x95\xb0\xef\xbc\x9a\n# $$\n# y = x^2\\bullet e^x\n# $$\n# \xe5\xae\x83\xe7\x9a\x84\xe5\xaf\xbc\xe5\x87\xbd\xe6\x95\xb0\xe6\x98\xaf\xef\xbc\x9a\n# $$\n# {dy \\over dx} = 2x\\bullet e^x + x^2 \\bullet e^x\n# $$\n# \xe6\x9d\xa5\xe7\x9c\x8b\xe7\x9c\x8bautograd\xe7\x9a\x84\xe8\xae\xa1\xe7\xae\x97\xe7\xbb\x93\xe6\x9e\x9c\xe4\xb8\x8e\xe6\x89\x8b\xe5\x8a\xa8\xe6\xb1\x82\xe5\xaf\xbc\xe8\xae\xa1\xe7\xae\x97\xe7\xbb\x93\xe6\x9e\x9c\xe7\x9a\x84\xe8\xaf\xaf\xe5\xb7\xae\xe3\x80\x82\n\n#%%\ndef f(x):\n    '''\xe8\xae\xa1\xe7\xae\x97y'''\n    y = x**2 * t.exp(x)\n    return y\n\ndef gradf(x):\n    '''\xe6\x89\x8b\xe5\x8a\xa8\xe6\xb1\x82\xe5\xaf\xbc\xe5\x87\xbd\xe6\x95\xb0'''\n    dx = 2*x*t.exp(x) + x**2*t.exp(x)\n    return dx\n\n\n#%%\nx = t.randn(3,4, requires_grad = True)\ny = f(x)\ny\n\n\n#%%\ny.backward(t.ones(y.size())) # gradient\xe5\xbd\xa2\xe7\x8a\xb6\xe4\xb8\x8ey\xe4\xb8\x80\xe8\x87\xb4\nx.grad\n\n\n#%%\n# autograd\xe7\x9a\x84\xe8\xae\xa1\xe7\xae\x97\xe7\xbb\x93\xe6\x9e\x9c\xe4\xb8\x8e\xe5\x88\xa9\xe7\x94\xa8\xe5\x85\xac\xe5\xbc\x8f\xe6\x89\x8b\xe5\x8a\xa8\xe8\xae\xa1\xe7\xae\x97\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x9c\xe4\xb8\x80\xe8\x87\xb4\ngradf(x) \n\n#%% [markdown]\n# ### 3.2.2 \xe8\xae\xa1\xe7\xae\x97\xe5\x9b\xbe\n# \n# PyTorch\xe4\xb8\xad`autograd`\xe7\x9a\x84\xe5\xba\x95\xe5\xb1\x82\xe9\x87\x87\xe7\x94\xa8\xe4\xba\x86\xe8\xae\xa1\xe7\xae\x97\xe5\x9b\xbe\xef\xbc\x8c\xe8\xae\xa1\xe7\xae\x97\xe5\x9b\xbe\xe6\x98\xaf\xe4\xb8\x80\xe7\xa7\x8d\xe7\x89\xb9\xe6\xae\x8a\xe7\x9a\x84\xe6\x9c\x89\xe5\x90\x91\xe6\x97\xa0\xe7\x8e\xaf\xe5\x9b\xbe\xef\xbc\x88DAG\xef\xbc\x89\xef\xbc\x8c\xe7\x94\xa8\xe4\xba\x8e\xe8\xae\xb0\xe5\xbd\x95\xe7\xae\x97\xe5\xad\x90\xe4\xb8\x8e\xe5\x8f\x98\xe9\x87\x8f\xe4\xb9\x8b\xe9\x97\xb4\xe7\x9a\x84\xe5\x85\xb3\xe7\xb3\xbb\xe3\x80\x82\xe4\xb8\x80\xe8\x88\xac\xe7\x94\xa8\xe7\x9f\xa9\xe5\xbd\xa2\xe8\xa1\xa8\xe7\xa4\xba\xe7\xae\x97\xe5\xad\x90\xef\xbc\x8c\xe6\xa4\xad\xe5\x9c\x86\xe5\xbd\xa2\xe8\xa1\xa8\xe7\xa4\xba\xe5\x8f\x98\xe9\x87\x8f\xe3\x80\x82\xe5\xa6\x82\xe8\xa1\xa8\xe8\xbe\xbe\xe5\xbc\x8f$ \\textbf {z = wx + b}$\xe5\x8f\xaf\xe5\x88\x86\xe8\xa7\xa3\xe4\xb8\xba$\\textbf{y = wx}$\xe5\x92\x8c$\\textbf{z = y + b}$\xef\xbc\x8c\xe5\x85\xb6\xe8\xae\xa1\xe7\xae\x97\xe5\x9b\xbe\xe5\xa6\x82\xe5\x9b\xbe3-3\xe6\x89\x80\xe7\xa4\xba\xef\xbc\x8c\xe5\x9b\xbe\xe4\xb8\xad`MUL`\xef\xbc\x8c`ADD`\xe9\x83\xbd\xe6\x98\xaf\xe7\xae\x97\xe5\xad\x90\xef\xbc\x8c$\\textbf{w}$\xef\xbc\x8c$\\textbf{x}$\xef\xbc\x8c$\\textbf{b}$\xe5\x8d\xb3\xe5\x8f\x98\xe9\x87\x8f\xe3\x80\x82\n# \n# ![\xe5\x9b\xbe3-3:computation graph](imgs/com_graph.svg)\n#%% [markdown]\n# \xe5\xa6\x82\xe4\xb8\x8a\xe6\x9c\x89\xe5\x90\x91\xe6\x97\xa0\xe7\x8e\xaf\xe5\x9b\xbe\xe4\xb8\xad\xef\xbc\x8c$\\textbf{X}$\xe5\x92\x8c$\\textbf{b}$\xe6\x98\xaf\xe5\x8f\xb6\xe5\xad\x90\xe8\x8a\x82\xe7\x82\xb9\xef\xbc\x88leaf node\xef\xbc\x89\xef\xbc\x8c\xe8\xbf\x99\xe4\xba\x9b\xe8\x8a\x82\xe7\x82\xb9\xe9\x80\x9a\xe5\xb8\xb8\xe7\x94\xb1\xe7\x94\xa8\xe6\x88\xb7\xe8\x87\xaa\xe5\xb7\xb1\xe5\x88\x9b\xe5\xbb\xba\xef\xbc\x8c\xe4\xb8\x8d\xe4\xbe\x9d\xe8\xb5\x96\xe4\xba\x8e\xe5\x85\xb6\xe4\xbb\x96\xe5\x8f\x98\xe9\x87\x8f\xe3\x80\x82$\\textbf{z}$\xe7\xa7\xb0\xe4\xb8\xba\xe6\xa0\xb9\xe8\x8a\x82\xe7\x82\xb9\xef\xbc\x8c\xe6\x98\xaf\xe8\xae\xa1\xe7\xae\x97\xe5\x9b\xbe\xe7\x9a\x84\xe6\x9c\x80\xe7\xbb\x88\xe7\x9b\xae\xe6\xa0\x87\xe3\x80\x82\xe5\x88\xa9\xe7\x94\xa8\xe9\x93\xbe\xe5\xbc\x8f\xe6\xb3\x95\xe5\x88\x99\xe5\xbe\x88\xe5\xae\xb9\xe6\x98\x93\xe6\xb1\x82\xe5\xbe\x97\xe5\x90\x84\xe4\xb8\xaa\xe5\x8f\xb6\xe5\xad\x90\xe8\x8a\x82\xe7\x82\xb9\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\xe3\x80\x82\n# $${\\partial z \\over \\partial b} = 1,\\space {\\partial z \\over \\partial y} = 1\\\\\n# {\\partial y \\over \\partial w }= x,{\\partial y \\over \\partial x}= w\\\\\n# {\\partial z \\over \\partial x}= {\\partial z \\over \\partial y} {\\partial y \\over \\partial x}=1 * w\\\\\n# {\\partial z \\over \\partial w}= {\\partial z \\over \\partial y} {\\partial y \\over \\partial w}=1 * x\\\\\n# $$\n# \xe8\x80\x8c\xe6\x9c\x89\xe4\xba\x86\xe8\xae\xa1\xe7\xae\x97\xe5\x9b\xbe\xef\xbc\x8c\xe4\xb8\x8a\xe8\xbf\xb0\xe9\x93\xbe\xe5\xbc\x8f\xe6\xb1\x82\xe5\xaf\xbc\xe5\x8d\xb3\xe5\x8f\xaf\xe5\x88\xa9\xe7\x94\xa8\xe8\xae\xa1\xe7\xae\x97\xe5\x9b\xbe\xe7\x9a\x84\xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe8\x87\xaa\xe5\x8a\xa8\xe5\xae\x8c\xe6\x88\x90\xef\xbc\x8c\xe5\x85\xb6\xe8\xbf\x87\xe7\xa8\x8b\xe5\xa6\x82\xe5\x9b\xbe3-4\xe6\x89\x80\xe7\xa4\xba\xe3\x80\x82\n# \n# ![\xe5\x9b\xbe3-4\xef\xbc\x9a\xe8\xae\xa1\xe7\xae\x97\xe5\x9b\xbe\xe7\x9a\x84\xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad](imgs/com_graph_backward.svg)\n# \n# \n# \xe5\x9c\xa8PyTorch\xe5\xae\x9e\xe7\x8e\xb0\xe4\xb8\xad\xef\xbc\x8cautograd\xe4\xbc\x9a\xe9\x9a\x8f\xe7\x9d\x80\xe7\x94\xa8\xe6\x88\xb7\xe7\x9a\x84\xe6\x93\x8d\xe4\xbd\x9c\xef\xbc\x8c\xe8\xae\xb0\xe5\xbd\x95\xe7\x94\x9f\xe6\x88\x90\xe5\xbd\x93\xe5\x89\x8dvariable\xe7\x9a\x84\xe6\x89\x80\xe6\x9c\x89\xe6\x93\x8d\xe4\xbd\x9c\xef\xbc\x8c\xe5\xb9\xb6\xe7\x94\xb1\xe6\xad\xa4\xe5\xbb\xba\xe7\xab\x8b\xe4\xb8\x80\xe4\xb8\xaa\xe6\x9c\x89\xe5\x90\x91\xe6\x97\xa0\xe7\x8e\xaf\xe5\x9b\xbe\xe3\x80\x82\xe7\x94\xa8\xe6\x88\xb7\xe6\xaf\x8f\xe8\xbf\x9b\xe8\xa1\x8c\xe4\xb8\x80\xe4\xb8\xaa\xe6\x93\x8d\xe4\xbd\x9c\xef\xbc\x8c\xe7\x9b\xb8\xe5\xba\x94\xe7\x9a\x84\xe8\xae\xa1\xe7\xae\x97\xe5\x9b\xbe\xe5\xb0\xb1\xe4\xbc\x9a\xe5\x8f\x91\xe7\x94\x9f\xe6\x94\xb9\xe5\x8f\x98\xe3\x80\x82\xe6\x9b\xb4\xe5\xba\x95\xe5\xb1\x82\xe7\x9a\x84\xe5\xae\x9e\xe7\x8e\xb0\xe4\xb8\xad\xef\xbc\x8c\xe5\x9b\xbe\xe4\xb8\xad\xe8\xae\xb0\xe5\xbd\x95\xe4\xba\x86\xe6\x93\x8d\xe4\xbd\x9c`Function`\xef\xbc\x8c\xe6\xaf\x8f\xe4\xb8\x80\xe4\xb8\xaa\xe5\x8f\x98\xe9\x87\x8f\xe5\x9c\xa8\xe5\x9b\xbe\xe4\xb8\xad\xe7\x9a\x84\xe4\xbd\x8d\xe7\xbd\xae\xe5\x8f\xaf\xe9\x80\x9a\xe8\xbf\x87\xe5\x85\xb6`grad_fn`\xe5\xb1\x9e\xe6\x80\xa7\xe5\x9c\xa8\xe5\x9b\xbe\xe4\xb8\xad\xe7\x9a\x84\xe4\xbd\x8d\xe7\xbd\xae\xe6\x8e\xa8\xe6\xb5\x8b\xe5\xbe\x97\xe5\x88\xb0\xe3\x80\x82\xe5\x9c\xa8\xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe8\xbf\x87\xe7\xa8\x8b\xe4\xb8\xad\xef\xbc\x8cautograd\xe6\xb2\xbf\xe7\x9d\x80\xe8\xbf\x99\xe4\xb8\xaa\xe5\x9b\xbe\xe4\xbb\x8e\xe5\xbd\x93\xe5\x89\x8d\xe5\x8f\x98\xe9\x87\x8f\xef\xbc\x88\xe6\xa0\xb9\xe8\x8a\x82\xe7\x82\xb9$\\textbf{z}$\xef\xbc\x89\xe6\xba\xaf\xe6\xba\x90\xef\xbc\x8c\xe5\x8f\xaf\xe4\xbb\xa5\xe5\x88\xa9\xe7\x94\xa8\xe9\x93\xbe\xe5\xbc\x8f\xe6\xb1\x82\xe5\xaf\xbc\xe6\xb3\x95\xe5\x88\x99\xe8\xae\xa1\xe7\xae\x97\xe6\x89\x80\xe6\x9c\x89\xe5\x8f\xb6\xe5\xad\x90\xe8\x8a\x82\xe7\x82\xb9\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\xe3\x80\x82\xe6\xaf\x8f\xe4\xb8\x80\xe4\xb8\xaa\xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe6\x93\x8d\xe4\xbd\x9c\xe7\x9a\x84\xe5\x87\xbd\xe6\x95\xb0\xe9\x83\xbd\xe6\x9c\x89\xe4\xb8\x8e\xe4\xb9\x8b\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84\xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe5\x87\xbd\xe6\x95\xb0\xe7\x94\xa8\xe6\x9d\xa5\xe8\xae\xa1\xe7\xae\x97\xe8\xbe\x93\xe5\x85\xa5\xe7\x9a\x84\xe5\x90\x84\xe4\xb8\xaavariable\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\xef\xbc\x8c\xe8\xbf\x99\xe4\xba\x9b\xe5\x87\xbd\xe6\x95\xb0\xe7\x9a\x84\xe5\x87\xbd\xe6\x95\xb0\xe5\x90\x8d\xe9\x80\x9a\xe5\xb8\xb8\xe4\xbb\xa5`Backward`\xe7\xbb\x93\xe5\xb0\xbe\xe3\x80\x82\xe4\xb8\x8b\xe9\x9d\xa2\xe7\xbb\x93\xe5\x90\x88\xe4\xbb\xa3\xe7\xa0\x81\xe5\xad\xa6\xe4\xb9\xa0autograd\xe7\x9a\x84\xe5\xae\x9e\xe7\x8e\xb0\xe7\xbb\x86\xe8\x8a\x82\xe3\x80\x82\n\n#%%\nx = t.ones(1)\nb = t.rand(1, requires_grad = True)\nw = t.rand(1, requires_grad = True)\ny = w * x # \xe7\xad\x89\xe4\xbb\xb7\xe4\xba\x8ey=w.mul(x)\nz = y + b # \xe7\xad\x89\xe4\xbb\xb7\xe4\xba\x8ez=y.add(b)\n\n\n#%%\nx.requires_grad, b.requires_grad, w.requires_grad\n\n\n#%%\n# \xe8\x99\xbd\xe7\x84\xb6\xe6\x9c\xaa\xe6\x8c\x87\xe5\xae\x9ay.requires_grad\xe4\xb8\xbaTrue\xef\xbc\x8c\xe4\xbd\x86\xe7\x94\xb1\xe4\xba\x8ey\xe4\xbe\x9d\xe8\xb5\x96\xe4\xba\x8e\xe9\x9c\x80\xe8\xa6\x81\xe6\xb1\x82\xe5\xaf\xbc\xe7\x9a\x84w\n# \xe6\x95\x85\xe8\x80\x8cy.requires_grad\xe4\xb8\xbaTrue\ny.requires_grad\n\n\n#%%\nx.is_leaf, w.is_leaf, b.is_leaf\n\n\n#%%\ny.is_leaf, z.is_leaf\n\n\n#%%\n# grad_fn\xe5\x8f\xaf\xe4\xbb\xa5\xe6\x9f\xa5\xe7\x9c\x8b\xe8\xbf\x99\xe4\xb8\xaavariable\xe7\x9a\x84\xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe5\x87\xbd\xe6\x95\xb0\xef\xbc\x8c\n# z\xe6\x98\xafadd\xe5\x87\xbd\xe6\x95\xb0\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe5\xae\x83\xe7\x9a\x84\xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe5\x87\xbd\xe6\x95\xb0\xe6\x98\xafAddBackward\nz.grad_fn \n\n\n#%%\n# next_functions\xe4\xbf\x9d\xe5\xad\x98grad_fn\xe7\x9a\x84\xe8\xbe\x93\xe5\x85\xa5\xef\xbc\x8c\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaatuple\xef\xbc\x8ctuple\xe7\x9a\x84\xe5\x85\x83\xe7\xb4\xa0\xe4\xb9\x9f\xe6\x98\xafFunction\n# \xe7\xac\xac\xe4\xb8\x80\xe4\xb8\xaa\xe6\x98\xafy\xef\xbc\x8c\xe5\xae\x83\xe6\x98\xaf\xe4\xb9\x98\xe6\xb3\x95(mul)\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84\xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe5\x87\xbd\xe6\x95\xb0y.grad_fn\xe6\x98\xafMulBackward\n# \xe7\xac\xac\xe4\xba\x8c\xe4\xb8\xaa\xe6\x98\xafb\xef\xbc\x8c\xe5\xae\x83\xe6\x98\xaf\xe5\x8f\xb6\xe5\xad\x90\xe8\x8a\x82\xe7\x82\xb9\xef\xbc\x8c\xe7\x94\xb1\xe7\x94\xa8\xe6\x88\xb7\xe5\x88\x9b\xe5\xbb\xba\xef\xbc\x8cgrad_fn\xe4\xb8\xbaNone\xef\xbc\x8c\xe4\xbd\x86\xe6\x98\xaf\xe6\x9c\x89\nz.grad_fn.next_functions \n\n\n#%%\n# variable\xe7\x9a\x84grad_fn\xe5\xaf\xb9\xe5\xba\x94\xe7\x9d\x80\xe5\x92\x8c\xe5\x9b\xbe\xe4\xb8\xad\xe7\x9a\x84function\xe7\x9b\xb8\xe5\xaf\xb9\xe5\xba\x94\nz.grad_fn.next_functions[0][0] == y.grad_fn\n\n\n#%%\n# \xe7\xac\xac\xe4\xb8\x80\xe4\xb8\xaa\xe6\x98\xafw\xef\xbc\x8c\xe5\x8f\xb6\xe5\xad\x90\xe8\x8a\x82\xe7\x82\xb9\xef\xbc\x8c\xe9\x9c\x80\xe8\xa6\x81\xe6\xb1\x82\xe5\xaf\xbc\xef\xbc\x8c\xe6\xa2\xaf\xe5\xba\xa6\xe6\x98\xaf\xe7\xb4\xaf\xe5\x8a\xa0\xe7\x9a\x84\n# \xe7\xac\xac\xe4\xba\x8c\xe4\xb8\xaa\xe6\x98\xafx\xef\xbc\x8c\xe5\x8f\xb6\xe5\xad\x90\xe8\x8a\x82\xe7\x82\xb9\xef\xbc\x8c\xe4\xb8\x8d\xe9\x9c\x80\xe8\xa6\x81\xe6\xb1\x82\xe5\xaf\xbc\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe4\xb8\xbaNone\ny.grad_fn.next_functions\n\n\n#%%\n# \xe5\x8f\xb6\xe5\xad\x90\xe8\x8a\x82\xe7\x82\xb9\xe7\x9a\x84grad_fn\xe6\x98\xafNone\nw.grad_fn,x.grad_fn\n\n#%% [markdown]\n# \xe8\xae\xa1\xe7\xae\x97w\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\xe7\x9a\x84\xe6\x97\xb6\xe5\x80\x99\xef\xbc\x8c\xe9\x9c\x80\xe8\xa6\x81\xe7\x94\xa8\xe5\x88\xb0x\xe7\x9a\x84\xe6\x95\xb0\xe5\x80\xbc(${\\partial y\\over \\partial w} = x $)\xef\xbc\x8c\xe8\xbf\x99\xe4\xba\x9b\xe6\x95\xb0\xe5\x80\xbc\xe5\x9c\xa8\xe5\x89\x8d\xe5\x90\x91\xe8\xbf\x87\xe7\xa8\x8b\xe4\xb8\xad\xe4\xbc\x9a\xe4\xbf\x9d\xe5\xad\x98\xe6\x88\x90buffer\xef\xbc\x8c\xe5\x9c\xa8\xe8\xae\xa1\xe7\xae\x97\xe5\xae\x8c\xe6\xa2\xaf\xe5\xba\xa6\xe4\xb9\x8b\xe5\x90\x8e\xe4\xbc\x9a\xe8\x87\xaa\xe5\x8a\xa8\xe6\xb8\x85\xe7\xa9\xba\xe3\x80\x82\xe4\xb8\xba\xe4\xba\x86\xe8\x83\xbd\xe5\xa4\x9f\xe5\xa4\x9a\xe6\xac\xa1\xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe9\x9c\x80\xe8\xa6\x81\xe6\x8c\x87\xe5\xae\x9a`retain_graph`\xe6\x9d\xa5\xe4\xbf\x9d\xe7\x95\x99\xe8\xbf\x99\xe4\xba\x9bbuffer\xe3\x80\x82\n\n#%%\n# \xe4\xbd\xbf\xe7\x94\xa8retain_graph\xe6\x9d\xa5\xe4\xbf\x9d\xe5\xad\x98buffer\nz.backward(retain_graph=True)\nw.grad\n\n\n#%%\n# \xe5\xa4\x9a\xe6\xac\xa1\xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xef\xbc\x8c\xe6\xa2\xaf\xe5\xba\xa6\xe7\xb4\xaf\xe5\x8a\xa0\xef\xbc\x8c\xe8\xbf\x99\xe4\xb9\x9f\xe5\xb0\xb1\xe6\x98\xafw\xe4\xb8\xadAccumulateGrad\xe6\xa0\x87\xe8\xaf\x86\xe7\x9a\x84\xe5\x90\xab\xe4\xb9\x89\nz.backward()\nw.grad\n\n#%% [markdown]\n# PyTorch\xe4\xbd\xbf\xe7\x94\xa8\xe7\x9a\x84\xe6\x98\xaf\xe5\x8a\xa8\xe6\x80\x81\xe5\x9b\xbe\xef\xbc\x8c\xe5\xae\x83\xe7\x9a\x84\xe8\xae\xa1\xe7\xae\x97\xe5\x9b\xbe\xe5\x9c\xa8\xe6\xaf\x8f\xe6\xac\xa1\xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe6\x97\xb6\xe9\x83\xbd\xe6\x98\xaf\xe4\xbb\x8e\xe5\xa4\xb4\xe5\xbc\x80\xe5\xa7\x8b\xe6\x9e\x84\xe5\xbb\xba\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe5\xae\x83\xe8\x83\xbd\xe5\xa4\x9f\xe4\xbd\xbf\xe7\x94\xa8Python\xe6\x8e\xa7\xe5\x88\xb6\xe8\xaf\xad\xe5\x8f\xa5\xef\xbc\x88\xe5\xa6\x82for\xe3\x80\x81if\xe7\xad\x89\xef\xbc\x89\xe6\xa0\xb9\xe6\x8d\xae\xe9\x9c\x80\xe6\xb1\x82\xe5\x88\x9b\xe5\xbb\xba\xe8\xae\xa1\xe7\xae\x97\xe5\x9b\xbe\xe3\x80\x82\xe8\xbf\x99\xe7\x82\xb9\xe5\x9c\xa8\xe8\x87\xaa\xe7\x84\xb6\xe8\xaf\xad\xe8\xa8\x80\xe5\xa4\x84\xe7\x90\x86\xe9\xa2\x86\xe5\x9f\x9f\xe4\xb8\xad\xe5\xbe\x88\xe6\x9c\x89\xe7\x94\xa8\xef\xbc\x8c\xe5\xae\x83\xe6\x84\x8f\xe5\x91\xb3\xe7\x9d\x80\xe4\xbd\xa0\xe4\xb8\x8d\xe9\x9c\x80\xe8\xa6\x81\xe4\xba\x8b\xe5\x85\x88\xe6\x9e\x84\xe5\xbb\xba\xe6\x89\x80\xe6\x9c\x89\xe5\x8f\xaf\xe8\x83\xbd\xe7\x94\xa8\xe5\x88\xb0\xe7\x9a\x84\xe5\x9b\xbe\xe7\x9a\x84\xe8\xb7\xaf\xe5\xbe\x84\xef\xbc\x8c\xe5\x9b\xbe\xe5\x9c\xa8\xe8\xbf\x90\xe8\xa1\x8c\xe6\x97\xb6\xe6\x89\x8d\xe6\x9e\x84\xe5\xbb\xba\xe3\x80\x82\n\n#%%\ndef abs(x):\n    if x.data[0]>0: return x\n    else: return -x\nx = t.ones(1,requires_grad=True)\ny = abs(x)\ny.backward()\nx.grad\n\n\n#%%\nx = -1*t.ones(1)\nx = x.requires_grad_()\ny = abs(x)\ny.backward()\nprint(x.grad)\n\n\n#%%\ndef f(x):\n    result = 1\n    for ii in x:\n        if ii.item()>0: result=ii*result\n    return result\nx = t.arange(-2,4,requires_grad=True)\ny = f(x) # y = x[3]*x[4]*x[5]\ny.backward()\nx.grad\n\n#%% [markdown]\n# \xe5\x8f\x98\xe9\x87\x8f\xe7\x9a\x84`requires_grad`\xe5\xb1\x9e\xe6\x80\xa7\xe9\xbb\x98\xe8\xae\xa4\xe4\xb8\xbaFalse\xef\xbc\x8c\xe5\xa6\x82\xe6\x9e\x9c\xe6\x9f\x90\xe4\xb8\x80\xe4\xb8\xaa\xe8\x8a\x82\xe7\x82\xb9requires_grad\xe8\xa2\xab\xe8\xae\xbe\xe7\xbd\xae\xe4\xb8\xbaTrue\xef\xbc\x8c\xe9\x82\xa3\xe4\xb9\x88\xe6\x89\x80\xe6\x9c\x89\xe4\xbe\x9d\xe8\xb5\x96\xe5\xae\x83\xe7\x9a\x84\xe8\x8a\x82\xe7\x82\xb9`requires_grad`\xe9\x83\xbd\xe6\x98\xafTrue\xe3\x80\x82\xe8\xbf\x99\xe5\x85\xb6\xe5\xae\x9e\xe5\xbe\x88\xe5\xa5\xbd\xe7\x90\x86\xe8\xa7\xa3\xef\xbc\x8c\xe5\xaf\xb9\xe4\xba\x8e$ \\textbf{x}\\to \\textbf{y} \\to \\textbf{z}$\xef\xbc\x8cx.requires_grad = True\xef\xbc\x8c\xe5\xbd\x93\xe9\x9c\x80\xe8\xa6\x81\xe8\xae\xa1\xe7\xae\x97$\\partial z \\over \\partial x$\xe6\x97\xb6\xef\xbc\x8c\xe6\xa0\xb9\xe6\x8d\xae\xe9\x93\xbe\xe5\xbc\x8f\xe6\xb3\x95\xe5\x88\x99\xef\xbc\x8c$\\frac{\\partial z}{\\partial x} = \\frac{\\partial z}{\\partial y} \\frac{\\partial y}{\\partial x}$\xef\xbc\x8c\xe8\x87\xaa\xe7\x84\xb6\xe4\xb9\x9f\xe9\x9c\x80\xe8\xa6\x81\xe6\xb1\x82$ \\frac{\\partial z}{\\partial y}$\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5y.requires_grad\xe4\xbc\x9a\xe8\xa2\xab\xe8\x87\xaa\xe5\x8a\xa8\xe6\xa0\x87\xe4\xb8\xbaTrue. \n# \n# \n# \n# \xe6\x9c\x89\xe4\xba\x9b\xe6\x97\xb6\xe5\x80\x99\xe6\x88\x91\xe4\xbb\xac\xe5\x8f\xaf\xe8\x83\xbd\xe4\xb8\x8d\xe5\xb8\x8c\xe6\x9c\x9bautograd\xe5\xaf\xb9tensor\xe6\xb1\x82\xe5\xaf\xbc\xe3\x80\x82\xe8\xae\xa4\xe4\xb8\xba\xe6\xb1\x82\xe5\xaf\xbc\xe9\x9c\x80\xe8\xa6\x81\xe7\xbc\x93\xe5\xad\x98\xe8\xae\xb8\xe5\xa4\x9a\xe4\xb8\xad\xe9\x97\xb4\xe7\xbb\x93\xe6\x9e\x84\xef\xbc\x8c\xe5\xa2\x9e\xe5\x8a\xa0\xe9\xa2\x9d\xe5\xa4\x96\xe7\x9a\x84\xe5\x86\x85\xe5\xad\x98/\xe6\x98\xbe\xe5\xad\x98\xe5\xbc\x80\xe9\x94\x80\xef\xbc\x8c\xe9\x82\xa3\xe4\xb9\x88\xe6\x88\x91\xe4\xbb\xac\xe5\x8f\xaf\xe4\xbb\xa5\xe5\x85\xb3\xe9\x97\xad\xe8\x87\xaa\xe5\x8a\xa8\xe6\xb1\x82\xe5\xaf\xbc\xe3\x80\x82\xe5\xaf\xb9\xe4\xba\x8e\xe4\xb8\x8d\xe9\x9c\x80\xe8\xa6\x81\xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe7\x9a\x84\xe6\x83\x85\xe6\x99\xaf\xef\xbc\x88\xe5\xa6\x82inference\xef\xbc\x8c\xe5\x8d\xb3\xe6\xb5\x8b\xe8\xaf\x95\xe6\x8e\xa8\xe7\x90\x86\xe6\x97\xb6\xef\xbc\x89\xef\xbc\x8c\xe5\x85\xb3\xe9\x97\xad\xe8\x87\xaa\xe5\x8a\xa8\xe6\xb1\x82\xe5\xaf\xbc\xe5\x8f\xaf\xe5\xae\x9e\xe7\x8e\xb0\xe4\xb8\x80\xe5\xae\x9a\xe7\xa8\x8b\xe5\xba\xa6\xe7\x9a\x84\xe9\x80\x9f\xe5\xba\xa6\xe6\x8f\x90\xe5\x8d\x87\xef\xbc\x8c\xe5\xb9\xb6\xe8\x8a\x82\xe7\x9c\x81\xe7\xba\xa6\xe4\xb8\x80\xe5\x8d\x8a\xe6\x98\xbe\xe5\xad\x98\xef\xbc\x8c\xe5\x9b\xa0\xe5\x85\xb6\xe4\xb8\x8d\xe9\x9c\x80\xe8\xa6\x81\xe5\x88\x86\xe9\x85\x8d\xe7\xa9\xba\xe9\x97\xb4\xe8\xae\xa1\xe7\xae\x97\xe6\xa2\xaf\xe5\xba\xa6\xe3\x80\x82\n\n#%%\nx = t.ones(1, requires_grad=True)\nw = t.rand(1, requires_grad=True)\ny = x * w\n# y\xe4\xbe\x9d\xe8\xb5\x96\xe4\xba\x8ew\xef\xbc\x8c\xe8\x80\x8cw.requires_grad = True\nx.requires_grad, w.requires_grad, y.requires_grad\n\n\n#%%\nwith t.no_grad():\n    x = t.ones(1)\n    w = t.rand(1, requires_grad = True)\n    y = x * w\n# y\xe4\xbe\x9d\xe8\xb5\x96\xe4\xba\x8ew\xe5\x92\x8cx\xef\xbc\x8c\xe8\x99\xbd\xe7\x84\xb6w.requires_grad = True\xef\xbc\x8c\xe4\xbd\x86\xe6\x98\xafy\xe7\x9a\x84requires_grad\xe4\xbe\x9d\xe6\x97\xa7\xe4\xb8\xbaFalse\nx.requires_grad, w.requires_grad, y.requires_grad\n\n\n#%%\nget_ipython().run_line_magic('pinfo2', 't.no_grad')\n\n\n#%%\nt.set_grad_enabled(False)\nx = t.ones(1)\nw = t.rand(1, requires_grad = True)\ny = x * w\n# y\xe4\xbe\x9d\xe8\xb5\x96\xe4\xba\x8ew\xe5\x92\x8cx\xef\xbc\x8c\xe8\x99\xbd\xe7\x84\xb6w.requires_grad = True\xef\xbc\x8c\xe4\xbd\x86\xe6\x98\xafy\xe7\x9a\x84requires_grad\xe4\xbe\x9d\xe6\x97\xa7\xe4\xb8\xbaFalse\nx.requires_grad, w.requires_grad, y.requires_grad\n\n\n#%%\n# \xe6\x81\xa2\xe5\xa4\x8d\xe9\xbb\x98\xe8\xae\xa4\xe9\x85\x8d\xe7\xbd\xae\nt.set_grad_enabled(True)\n\n#%% [markdown]\n# \xe5\xa6\x82\xe6\x9e\x9c\xe6\x88\x91\xe4\xbb\xac\xe6\x83\xb3\xe8\xa6\x81\xe4\xbf\xae\xe6\x94\xb9tensor\xe7\x9a\x84\xe6\x95\xb0\xe5\x80\xbc\xef\xbc\x8c\xe4\xbd\x86\xe6\x98\xaf\xe5\x8f\x88\xe4\xb8\x8d\xe5\xb8\x8c\xe6\x9c\x9b\xe8\xa2\xabautograd\xe8\xae\xb0\xe5\xbd\x95\xef\xbc\x8c\xe9\x82\xa3\xe4\xb9\x88\xe6\x88\x91\xe4\xb9\x88\xe5\x8f\xaf\xe4\xbb\xa5\xe5\xaf\xb9tensor.data\xe8\xbf\x9b\xe8\xa1\x8c\xe6\x93\x8d\xe4\xbd\x9c\n\n#%%\na = t.ones(3,4,requires_grad=True)\nb = t.ones(3,4,requires_grad=True)\nc = a * b\n\na.data # \xe8\xbf\x98\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaatensor\n\n\n#%%\na.data.requires_grad # \xe4\xbd\x86\xe6\x98\xaf\xe5\xb7\xb2\xe7\xbb\x8f\xe6\x98\xaf\xe7\x8b\xac\xe7\xab\x8b\xe4\xba\x8e\xe8\xae\xa1\xe7\xae\x97\xe5\x9b\xbe\xe4\xb9\x8b\xe5\xa4\x96\n\n\n#%%\nd = a.data.sigmoid_() # sigmoid_ \xe6\x98\xaf\xe4\xb8\xaainplace\xe6\x93\x8d\xe4\xbd\x9c\xef\xbc\x8c\xe4\xbc\x9a\xe4\xbf\xae\xe6\x94\xb9a\xe8\x87\xaa\xe8\xba\xab\xe7\x9a\x84\xe5\x80\xbc\nd.requires_grad\n\n\n#%%\na \n\n#%% [markdown]\n# \xe5\xa6\x82\xe6\x9e\x9c\xe6\x88\x91\xe4\xbb\xac\xe5\xb8\x8c\xe6\x9c\x9b\xe5\xaf\xb9tensor\xef\xbc\x8c\xe4\xbd\x86\xe6\x98\xaf\xe5\x8f\x88\xe4\xb8\x8d\xe5\xb8\x8c\xe6\x9c\x9b\xe8\xa2\xab\xe8\xae\xb0\xe5\xbd\x95, \xe5\x8f\xaf\xe4\xbb\xa5\xe4\xbd\xbf\xe7\x94\xa8tensor.data \xe6\x88\x96\xe8\x80\x85tensor.detach()\n\n#%%\na.requires_grad\n\n\n#%%\n# \xe8\xbf\x91\xe4\xbc\xbc\xe4\xba\x8e tensor=a.data, \xe4\xbd\x86\xe6\x98\xaf\xe5\xa6\x82\xe6\x9e\x9ctensor\xe8\xa2\xab\xe4\xbf\xae\xe6\x94\xb9\xef\xbc\x8cbackward\xe5\x8f\xaf\xe8\x83\xbd\xe4\xbc\x9a\xe6\x8a\xa5\xe9\x94\x99\ntensor = a.detach()\ntensor.requires_grad\n\n\n#%%\n# \xe7\xbb\x9f\xe8\xae\xa1tensor\xe7\x9a\x84\xe4\xb8\x80\xe4\xba\x9b\xe6\x8c\x87\xe6\xa0\x87\xef\xbc\x8c\xe4\xb8\x8d\xe5\xb8\x8c\xe6\x9c\x9b\xe8\xa2\xab\xe8\xae\xb0\xe5\xbd\x95\nmean = tensor.mean()\nstd = tensor.std()\nmaximum = tensor.max()\n\n\n#%%\ntensor[0]=1\n# \xe4\xb8\x8b\xe9\x9d\xa2\xe4\xbc\x9a\xe6\x8a\xa5\xe9\x94\x99\xef\xbc\x9a\xe3\x80\x80RuntimeError: one of the variables needed for gradient\n#             computation has been modified by an inplace operation\n#\xe3\x80\x80\xe5\x9b\xa0\xe4\xb8\xba c=a*b, b\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\xe5\x8f\x96\xe5\x86\xb3\xe4\xba\x8ea\xef\xbc\x8c\xe7\x8e\xb0\xe5\x9c\xa8\xe4\xbf\xae\xe6\x94\xb9\xe4\xba\x86tensor\xef\xbc\x8c\xe5\x85\xb6\xe5\xae\x9e\xe4\xb9\x9f\xe5\xb0\xb1\xe6\x98\xaf\xe4\xbf\xae\xe6\x94\xb9\xe4\xba\x86a\xef\xbc\x8c\xe6\xa2\xaf\xe5\xba\xa6\xe4\xb8\x8d\xe5\x86\x8d\xe5\x87\x86\xe7\xa1\xae\n# c.sum().backward() \n\n#%% [markdown]\n# \xe5\x9c\xa8\xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe8\xbf\x87\xe7\xa8\x8b\xe4\xb8\xad\xe9\x9d\x9e\xe5\x8f\xb6\xe5\xad\x90\xe8\x8a\x82\xe7\x82\xb9\xe7\x9a\x84\xe5\xaf\xbc\xe6\x95\xb0\xe8\xae\xa1\xe7\xae\x97\xe5\xae\x8c\xe4\xb9\x8b\xe5\x90\x8e\xe5\x8d\xb3\xe8\xa2\xab\xe6\xb8\x85\xe7\xa9\xba\xe3\x80\x82\xe8\x8b\xa5\xe6\x83\xb3\xe6\x9f\xa5\xe7\x9c\x8b\xe8\xbf\x99\xe4\xba\x9b\xe5\x8f\x98\xe9\x87\x8f\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\xef\xbc\x8c\xe6\x9c\x89\xe4\xb8\xa4\xe7\xa7\x8d\xe6\x96\xb9\xe6\xb3\x95\xef\xbc\x9a\n# - \xe4\xbd\xbf\xe7\x94\xa8autograd.grad\xe5\x87\xbd\xe6\x95\xb0\n# - \xe4\xbd\xbf\xe7\x94\xa8hook\n# \n# `autograd.grad`\xe5\x92\x8c`hook`\xe6\x96\xb9\xe6\xb3\x95\xe9\x83\xbd\xe6\x98\xaf\xe5\xbe\x88\xe5\xbc\xba\xe5\xa4\xa7\xe7\x9a\x84\xe5\xb7\xa5\xe5\x85\xb7\xef\xbc\x8c\xe6\x9b\xb4\xe8\xaf\xa6\xe7\xbb\x86\xe7\x9a\x84\xe7\x94\xa8\xe6\xb3\x95\xe5\x8f\x82\xe8\x80\x83\xe5\xae\x98\xe6\x96\xb9api\xe6\x96\x87\xe6\xa1\xa3\xef\xbc\x8c\xe8\xbf\x99\xe9\x87\x8c\xe4\xb8\xbe\xe4\xbe\x8b\xe8\xaf\xb4\xe6\x98\x8e\xe5\x9f\xba\xe7\xa1\x80\xe7\x9a\x84\xe4\xbd\xbf\xe7\x94\xa8\xe3\x80\x82\xe6\x8e\xa8\xe8\x8d\x90\xe4\xbd\xbf\xe7\x94\xa8`hook`\xe6\x96\xb9\xe6\xb3\x95\xef\xbc\x8c\xe4\xbd\x86\xe6\x98\xaf\xe5\x9c\xa8\xe5\xae\x9e\xe9\x99\x85\xe4\xbd\xbf\xe7\x94\xa8\xe4\xb8\xad\xe5\xba\x94\xe5\xb0\xbd\xe9\x87\x8f\xe9\x81\xbf\xe5\x85\x8d\xe4\xbf\xae\xe6\x94\xb9grad\xe7\x9a\x84\xe5\x80\xbc\xe3\x80\x82\n\n#%%\nx = t.ones(3, requires_grad=True)\nw = t.rand(3, requires_grad=True)\ny = x * w\n# y\xe4\xbe\x9d\xe8\xb5\x96\xe4\xba\x8ew\xef\xbc\x8c\xe8\x80\x8cw.requires_grad = True\nz = y.sum()\nx.requires_grad, w.requires_grad, y.requires_grad\n\n\n#%%\n# \xe9\x9d\x9e\xe5\x8f\xb6\xe5\xad\x90\xe8\x8a\x82\xe7\x82\xb9grad\xe8\xae\xa1\xe7\xae\x97\xe5\xae\x8c\xe4\xb9\x8b\xe5\x90\x8e\xe8\x87\xaa\xe5\x8a\xa8\xe6\xb8\x85\xe7\xa9\xba\xef\xbc\x8cy.grad\xe6\x98\xafNone\nz.backward()\n(x.grad, w.grad, y.grad)\n\n\n#%%\n# \xe7\xac\xac\xe4\xb8\x80\xe7\xa7\x8d\xe6\x96\xb9\xe6\xb3\x95\xef\xbc\x9a\xe4\xbd\xbf\xe7\x94\xa8grad\xe8\x8e\xb7\xe5\x8f\x96\xe4\xb8\xad\xe9\x97\xb4\xe5\x8f\x98\xe9\x87\x8f\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\nx = t.ones(3, requires_grad=True)\nw = t.rand(3, requires_grad=True)\ny = x * w\nz = y.sum()\n# z\xe5\xaf\xb9y\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\xef\xbc\x8c\xe9\x9a\x90\xe5\xbc\x8f\xe8\xb0\x83\xe7\x94\xa8backward()\nt.autograd.grad(z, y)\n\n\n#%%\n# \xe7\xac\xac\xe4\xba\x8c\xe7\xa7\x8d\xe6\x96\xb9\xe6\xb3\x95\xef\xbc\x9a\xe4\xbd\xbf\xe7\x94\xa8hook\n# hook\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa\xe5\x87\xbd\xe6\x95\xb0\xef\xbc\x8c\xe8\xbe\x93\xe5\x85\xa5\xe6\x98\xaf\xe6\xa2\xaf\xe5\xba\xa6\xef\xbc\x8c\xe4\xb8\x8d\xe5\xba\x94\xe8\xaf\xa5\xe6\x9c\x89\xe8\xbf\x94\xe5\x9b\x9e\xe5\x80\xbc\ndef variable_hook(grad):\n    print('y\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\xef\xbc\x9a',grad)\n\nx = t.ones(3, requires_grad=True)\nw = t.rand(3, requires_grad=True)\ny = x * w\n# \xe6\xb3\xa8\xe5\x86\x8chook\nhook_handle = y.register_hook(variable_hook)\nz = y.sum()\nz.backward()\n\n# \xe9\x99\xa4\xe9\x9d\x9e\xe4\xbd\xa0\xe6\xaf\x8f\xe6\xac\xa1\xe9\x83\xbd\xe8\xa6\x81\xe7\x94\xa8hook\xef\xbc\x8c\xe5\x90\xa6\xe5\x88\x99\xe7\x94\xa8\xe5\xae\x8c\xe4\xb9\x8b\xe5\x90\x8e\xe8\xae\xb0\xe5\xbe\x97\xe7\xa7\xbb\xe9\x99\xa4hook\nhook_handle.remove()\n\n#%% [markdown]\n# \xe6\x9c\x80\xe5\x90\x8e\xe5\x86\x8d\xe6\x9d\xa5\xe7\x9c\x8b\xe7\x9c\x8bvariable\xe4\xb8\xadgrad\xe5\xb1\x9e\xe6\x80\xa7\xe5\x92\x8cbackward\xe5\x87\xbd\xe6\x95\xb0`grad_variables`\xe5\x8f\x82\xe6\x95\xb0\xe7\x9a\x84\xe5\x90\xab\xe4\xb9\x89\xef\xbc\x8c\xe8\xbf\x99\xe9\x87\x8c\xe7\x9b\xb4\xe6\x8e\xa5\xe4\xb8\x8b\xe7\xbb\x93\xe8\xae\xba\xef\xbc\x9a\n# \n# - variable $\\textbf{x}$\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\xe6\x98\xaf\xe7\x9b\xae\xe6\xa0\x87\xe5\x87\xbd\xe6\x95\xb0${f(x)} $\xe5\xaf\xb9$\\textbf{x}$\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\xef\xbc\x8c$\\frac{df(x)}{dx} = (\\frac {df(x)}{dx_0},\\frac {df(x)}{dx_1},...,\\frac {df(x)}{dx_N})$\xef\xbc\x8c\xe5\xbd\xa2\xe7\x8a\xb6\xe5\x92\x8c$\\textbf{x}$\xe4\xb8\x80\xe8\x87\xb4\xe3\x80\x82\n# - \xe5\xaf\xb9\xe4\xba\x8ey.backward(grad_variables)\xe4\xb8\xad\xe7\x9a\x84grad_variables\xe7\x9b\xb8\xe5\xbd\x93\xe4\xba\x8e\xe9\x93\xbe\xe5\xbc\x8f\xe6\xb1\x82\xe5\xaf\xbc\xe6\xb3\x95\xe5\x88\x99\xe4\xb8\xad\xe7\x9a\x84$\\frac{\\partial z}{\\partial x} = \\frac{\\partial z}{\\partial y} \\frac{\\partial y}{\\partial x}$\xe4\xb8\xad\xe7\x9a\x84$\\frac{\\partial z}{\\partial y}$\xe3\x80\x82z\xe6\x98\xaf\xe7\x9b\xae\xe6\xa0\x87\xe5\x87\xbd\xe6\x95\xb0\xef\xbc\x8c\xe4\xb8\x80\xe8\x88\xac\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa\xe6\xa0\x87\xe9\x87\x8f\xef\xbc\x8c\xe6\x95\x85\xe8\x80\x8c$\\frac{\\partial z}{\\partial y}$\xe7\x9a\x84\xe5\xbd\xa2\xe7\x8a\xb6\xe4\xb8\x8evariable $\\textbf{y}$\xe7\x9a\x84\xe5\xbd\xa2\xe7\x8a\xb6\xe4\xb8\x80\xe8\x87\xb4\xe3\x80\x82`z.backward()`\xe5\x9c\xa8\xe4\xb8\x80\xe5\xae\x9a\xe7\xa8\x8b\xe5\xba\xa6\xe4\xb8\x8a\xe7\xad\x89\xe4\xbb\xb7\xe4\xba\x8ey.backward(grad_y)\xe3\x80\x82`z.backward()`\xe7\x9c\x81\xe7\x95\xa5\xe4\xba\x86grad_variables\xe5\x8f\x82\xe6\x95\xb0\xef\xbc\x8c\xe6\x98\xaf\xe5\x9b\xa0\xe4\xb8\xba$z$\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa\xe6\xa0\x87\xe9\x87\x8f\xef\xbc\x8c\xe8\x80\x8c$\\frac{\\partial z}{\\partial z} = 1$\n\n#%%\nx = t.arange(0,3, requires_grad=True)\ny = x**2 + x*2\nz = y.sum()\nz.backward() # \xe4\xbb\x8ez\xe5\xbc\x80\xe5\xa7\x8b\xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\nx.grad\n\n\n#%%\nx = t.arange(0,3, requires_grad=True)\ny = x**2 + x*2\nz = y.sum()\ny_gradient = t.Tensor([1,1,1]) # dz/dy\ny.backward(y_gradient) #\xe4\xbb\x8ey\xe5\xbc\x80\xe5\xa7\x8b\xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\nx.grad\n\n#%% [markdown]\n# \xe5\x8f\xa6\xe5\xa4\x96\xe5\x80\xbc\xe5\xbe\x97\xe6\xb3\xa8\xe6\x84\x8f\xe7\x9a\x84\xe6\x98\xaf\xef\xbc\x8c\xe5\x8f\xaa\xe6\x9c\x89\xe5\xaf\xb9variable\xe7\x9a\x84\xe6\x93\x8d\xe4\xbd\x9c\xe6\x89\x8d\xe8\x83\xbd\xe4\xbd\xbf\xe7\x94\xa8autograd\xef\xbc\x8c\xe5\xa6\x82\xe6\x9e\x9c\xe5\xaf\xb9variable\xe7\x9a\x84data\xe7\x9b\xb4\xe6\x8e\xa5\xe8\xbf\x9b\xe8\xa1\x8c\xe6\x93\x8d\xe4\xbd\x9c\xef\xbc\x8c\xe5\xb0\x86\xe6\x97\xa0\xe6\xb3\x95\xe4\xbd\xbf\xe7\x94\xa8\xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe3\x80\x82\xe9\x99\xa4\xe4\xba\x86\xe5\xaf\xb9\xe5\x8f\x82\xe6\x95\xb0\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xef\xbc\x8c\xe4\xb8\x80\xe8\x88\xac\xe6\x88\x91\xe4\xbb\xac\xe4\xb8\x8d\xe4\xbc\x9a\xe4\xbf\xae\xe6\x94\xb9variable.data\xe7\x9a\x84\xe5\x80\xbc\xe3\x80\x82\n#%% [markdown]\n# \xe5\x9c\xa8PyTorch\xe4\xb8\xad\xe8\xae\xa1\xe7\xae\x97\xe5\x9b\xbe\xe7\x9a\x84\xe7\x89\xb9\xe7\x82\xb9\xe5\x8f\xaf\xe6\x80\xbb\xe7\xbb\x93\xe5\xa6\x82\xe4\xb8\x8b\xef\xbc\x9a\n# \n# - autograd\xe6\xa0\xb9\xe6\x8d\xae\xe7\x94\xa8\xe6\x88\xb7\xe5\xaf\xb9variable\xe7\x9a\x84\xe6\x93\x8d\xe4\xbd\x9c\xe6\x9e\x84\xe5\xbb\xba\xe5\x85\xb6\xe8\xae\xa1\xe7\xae\x97\xe5\x9b\xbe\xe3\x80\x82\xe5\xaf\xb9\xe5\x8f\x98\xe9\x87\x8f\xe7\x9a\x84\xe6\x93\x8d\xe4\xbd\x9c\xe6\x8a\xbd\xe8\xb1\xa1\xe4\xb8\xba`Function`\xe3\x80\x82\n# - \xe5\xaf\xb9\xe4\xba\x8e\xe9\x82\xa3\xe4\xba\x9b\xe4\xb8\x8d\xe6\x98\xaf\xe4\xbb\xbb\xe4\xbd\x95\xe5\x87\xbd\xe6\x95\xb0(Function)\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xef\xbc\x8c\xe7\x94\xb1\xe7\x94\xa8\xe6\x88\xb7\xe5\x88\x9b\xe5\xbb\xba\xe7\x9a\x84\xe8\x8a\x82\xe7\x82\xb9\xe7\xa7\xb0\xe4\xb8\xba\xe5\x8f\xb6\xe5\xad\x90\xe8\x8a\x82\xe7\x82\xb9\xef\xbc\x8c\xe5\x8f\xb6\xe5\xad\x90\xe8\x8a\x82\xe7\x82\xb9\xe7\x9a\x84`grad_fn`\xe4\xb8\xbaNone\xe3\x80\x82\xe5\x8f\xb6\xe5\xad\x90\xe8\x8a\x82\xe7\x82\xb9\xe4\xb8\xad\xe9\x9c\x80\xe8\xa6\x81\xe6\xb1\x82\xe5\xaf\xbc\xe7\x9a\x84variable\xef\xbc\x8c\xe5\x85\xb7\xe6\x9c\x89`AccumulateGrad`\xe6\xa0\x87\xe8\xaf\x86\xef\xbc\x8c\xe5\x9b\xa0\xe5\x85\xb6\xe6\xa2\xaf\xe5\xba\xa6\xe6\x98\xaf\xe7\xb4\xaf\xe5\x8a\xa0\xe7\x9a\x84\xe3\x80\x82\n# - variable\xe9\xbb\x98\xe8\xae\xa4\xe6\x98\xaf\xe4\xb8\x8d\xe9\x9c\x80\xe8\xa6\x81\xe6\xb1\x82\xe5\xaf\xbc\xe7\x9a\x84\xef\xbc\x8c\xe5\x8d\xb3`requires_grad`\xe5\xb1\x9e\xe6\x80\xa7\xe9\xbb\x98\xe8\xae\xa4\xe4\xb8\xbaFalse\xef\xbc\x8c\xe5\xa6\x82\xe6\x9e\x9c\xe6\x9f\x90\xe4\xb8\x80\xe4\xb8\xaa\xe8\x8a\x82\xe7\x82\xb9requires_grad\xe8\xa2\xab\xe8\xae\xbe\xe7\xbd\xae\xe4\xb8\xbaTrue\xef\xbc\x8c\xe9\x82\xa3\xe4\xb9\x88\xe6\x89\x80\xe6\x9c\x89\xe4\xbe\x9d\xe8\xb5\x96\xe5\xae\x83\xe7\x9a\x84\xe8\x8a\x82\xe7\x82\xb9`requires_grad`\xe9\x83\xbd\xe4\xb8\xbaTrue\xe3\x80\x82\n# - variable\xe7\x9a\x84`volatile`\xe5\xb1\x9e\xe6\x80\xa7\xe9\xbb\x98\xe8\xae\xa4\xe4\xb8\xbaFalse\xef\xbc\x8c\xe5\xa6\x82\xe6\x9e\x9c\xe6\x9f\x90\xe4\xb8\x80\xe4\xb8\xaavariable\xe7\x9a\x84`volatile`\xe5\xb1\x9e\xe6\x80\xa7\xe8\xa2\xab\xe8\xae\xbe\xe4\xb8\xbaTrue\xef\xbc\x8c\xe9\x82\xa3\xe4\xb9\x88\xe6\x89\x80\xe6\x9c\x89\xe4\xbe\x9d\xe8\xb5\x96\xe5\xae\x83\xe7\x9a\x84\xe8\x8a\x82\xe7\x82\xb9`volatile`\xe5\xb1\x9e\xe6\x80\xa7\xe9\x83\xbd\xe4\xb8\xbaTrue\xe3\x80\x82volatile\xe5\xb1\x9e\xe6\x80\xa7\xe4\xb8\xbaTrue\xe7\x9a\x84\xe8\x8a\x82\xe7\x82\xb9\xe4\xb8\x8d\xe4\xbc\x9a\xe6\xb1\x82\xe5\xaf\xbc\xef\xbc\x8cvolatile\xe7\x9a\x84\xe4\xbc\x98\xe5\x85\x88\xe7\xba\xa7\xe6\xaf\x94`requires_grad`\xe9\xab\x98\xe3\x80\x82\n# - \xe5\xa4\x9a\xe6\xac\xa1\xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe6\x97\xb6\xef\xbc\x8c\xe6\xa2\xaf\xe5\xba\xa6\xe6\x98\xaf\xe7\xb4\xaf\xe5\x8a\xa0\xe7\x9a\x84\xe3\x80\x82\xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe7\x9a\x84\xe4\xb8\xad\xe9\x97\xb4\xe7\xbc\x93\xe5\xad\x98\xe4\xbc\x9a\xe8\xa2\xab\xe6\xb8\x85\xe7\xa9\xba\xef\xbc\x8c\xe4\xb8\xba\xe8\xbf\x9b\xe8\xa1\x8c\xe5\xa4\x9a\xe6\xac\xa1\xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe9\x9c\x80\xe6\x8c\x87\xe5\xae\x9a`retain_graph`=True\xe6\x9d\xa5\xe4\xbf\x9d\xe5\xad\x98\xe8\xbf\x99\xe4\xba\x9b\xe7\xbc\x93\xe5\xad\x98\xe3\x80\x82\n# - \xe9\x9d\x9e\xe5\x8f\xb6\xe5\xad\x90\xe8\x8a\x82\xe7\x82\xb9\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\xe8\xae\xa1\xe7\xae\x97\xe5\xae\x8c\xe4\xb9\x8b\xe5\x90\x8e\xe5\x8d\xb3\xe8\xa2\xab\xe6\xb8\x85\xe7\xa9\xba\xef\xbc\x8c\xe5\x8f\xaf\xe4\xbb\xa5\xe4\xbd\xbf\xe7\x94\xa8`autograd.grad`\xe6\x88\x96`hook`\xe6\x8a\x80\xe6\x9c\xaf\xe8\x8e\xb7\xe5\x8f\x96\xe9\x9d\x9e\xe5\x8f\xb6\xe5\xad\x90\xe8\x8a\x82\xe7\x82\xb9\xe7\x9a\x84\xe5\x80\xbc\xe3\x80\x82\n# - variable\xe7\x9a\x84grad\xe4\xb8\x8edata\xe5\xbd\xa2\xe7\x8a\xb6\xe4\xb8\x80\xe8\x87\xb4\xef\xbc\x8c\xe5\xba\x94\xe9\x81\xbf\xe5\x85\x8d\xe7\x9b\xb4\xe6\x8e\xa5\xe4\xbf\xae\xe6\x94\xb9variable.data\xef\xbc\x8c\xe5\x9b\xa0\xe4\xb8\xba\xe5\xaf\xb9data\xe7\x9a\x84\xe7\x9b\xb4\xe6\x8e\xa5\xe6\x93\x8d\xe4\xbd\x9c\xe6\x97\xa0\xe6\xb3\x95\xe5\x88\xa9\xe7\x94\xa8autograd\xe8\xbf\x9b\xe8\xa1\x8c\xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\n# - \xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe5\x87\xbd\xe6\x95\xb0`backward`\xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0`grad_variables`\xe5\x8f\xaf\xe4\xbb\xa5\xe7\x9c\x8b\xe6\x88\x90\xe9\x93\xbe\xe5\xbc\x8f\xe6\xb1\x82\xe5\xaf\xbc\xe7\x9a\x84\xe4\xb8\xad\xe9\x97\xb4\xe7\xbb\x93\xe6\x9e\x9c\xef\xbc\x8c\xe5\xa6\x82\xe6\x9e\x9c\xe6\x98\xaf\xe6\xa0\x87\xe9\x87\x8f\xef\xbc\x8c\xe5\x8f\xaf\xe4\xbb\xa5\xe7\x9c\x81\xe7\x95\xa5\xef\xbc\x8c\xe9\xbb\x98\xe8\xae\xa4\xe4\xb8\xba1\n# - PyTorch\xe9\x87\x87\xe7\x94\xa8\xe5\x8a\xa8\xe6\x80\x81\xe5\x9b\xbe\xe8\xae\xbe\xe8\xae\xa1\xef\xbc\x8c\xe5\x8f\xaf\xe4\xbb\xa5\xe5\xbe\x88\xe6\x96\xb9\xe4\xbe\xbf\xe5\x9c\xb0\xe6\x9f\xa5\xe7\x9c\x8b\xe4\xb8\xad\xe9\x97\xb4\xe5\xb1\x82\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xef\xbc\x8c\xe5\x8a\xa8\xe6\x80\x81\xe7\x9a\x84\xe8\xae\xbe\xe8\xae\xa1\xe8\xae\xa1\xe7\xae\x97\xe5\x9b\xbe\xe7\xbb\x93\xe6\x9e\x84\xe3\x80\x82\n# \n# \xe8\xbf\x99\xe4\xba\x9b\xe7\x9f\xa5\xe8\xaf\x86\xe4\xb8\x8d\xe6\x87\x82\xe5\xa4\xa7\xe5\xa4\x9a\xe6\x95\xb0\xe6\x83\x85\xe5\x86\xb5\xe4\xb8\x8b\xe4\xb9\x9f\xe4\xb8\x8d\xe4\xbc\x9a\xe5\xbd\xb1\xe5\x93\x8d\xe5\xaf\xb9pytorch\xe7\x9a\x84\xe4\xbd\xbf\xe7\x94\xa8\xef\xbc\x8c\xe4\xbd\x86\xe6\x98\xaf\xe6\x8e\x8c\xe6\x8f\xa1\xe8\xbf\x99\xe4\xba\x9b\xe7\x9f\xa5\xe8\xaf\x86\xe6\x9c\x89\xe5\x8a\xa9\xe4\xba\x8e\xe6\x9b\xb4\xe5\xa5\xbd\xe7\x9a\x84\xe7\x90\x86\xe8\xa7\xa3pytorch\xef\xbc\x8c\xe5\xb9\xb6\xe6\x9c\x89\xe6\x95\x88\xe7\x9a\x84\xe9\x81\xbf\xe5\xbc\x80\xe5\xbe\x88\xe5\xa4\x9a\xe9\x99\xb7\xe9\x98\xb1\n#%% [markdown]\n# ### 3.2.3 \xe6\x89\xa9\xe5\xb1\x95autograd\n# \n# \n# \xe7\x9b\xae\xe5\x89\x8d\xe7\xbb\x9d\xe5\xa4\xa7\xe5\xa4\x9a\xe6\x95\xb0\xe5\x87\xbd\xe6\x95\xb0\xe9\x83\xbd\xe5\x8f\xaf\xe4\xbb\xa5\xe4\xbd\xbf\xe7\x94\xa8`autograd`\xe5\xae\x9e\xe7\x8e\xb0\xe5\x8f\x8d\xe5\x90\x91\xe6\xb1\x82\xe5\xaf\xbc\xef\xbc\x8c\xe4\xbd\x86\xe5\xa6\x82\xe6\x9e\x9c\xe9\x9c\x80\xe8\xa6\x81\xe8\x87\xaa\xe5\xb7\xb1\xe5\x86\x99\xe4\xb8\x80\xe4\xb8\xaa\xe5\xa4\x8d\xe6\x9d\x82\xe7\x9a\x84\xe5\x87\xbd\xe6\x95\xb0\xef\xbc\x8c\xe4\xb8\x8d\xe6\x94\xaf\xe6\x8c\x81\xe8\x87\xaa\xe5\x8a\xa8\xe5\x8f\x8d\xe5\x90\x91\xe6\xb1\x82\xe5\xaf\xbc\xe6\x80\x8e\xe4\xb9\x88\xe5\x8a\x9e? \xe5\x86\x99\xe4\xb8\x80\xe4\xb8\xaa`Function`\xef\xbc\x8c\xe5\xae\x9e\xe7\x8e\xb0\xe5\xae\x83\xe7\x9a\x84\xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe5\x92\x8c\xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe4\xbb\xa3\xe7\xa0\x81\xef\xbc\x8c`Function`\xe5\xaf\xb9\xe5\xba\x94\xe4\xba\x8e\xe8\xae\xa1\xe7\xae\x97\xe5\x9b\xbe\xe4\xb8\xad\xe7\x9a\x84\xe7\x9f\xa9\xe5\xbd\xa2\xef\xbc\x8c \xe5\xae\x83\xe6\x8e\xa5\xe6\x94\xb6\xe5\x8f\x82\xe6\x95\xb0\xef\xbc\x8c\xe8\xae\xa1\xe7\xae\x97\xe5\xb9\xb6\xe8\xbf\x94\xe5\x9b\x9e\xe7\xbb\x93\xe6\x9e\x9c\xe3\x80\x82\xe4\xb8\x8b\xe9\x9d\xa2\xe7\xbb\x99\xe5\x87\xba\xe4\xb8\x80\xe4\xb8\xaa\xe4\xbe\x8b\xe5\xad\x90\xe3\x80\x82\n# \n# ```python\n# \n# class Mul(Function):\n#                                                             \n#     @staticmethod\n#     def forward(ctx, w, x, b, x_requires_grad = True):\n#         ctx.x_requires_grad = x_requires_grad\n#         ctx.save_for_backward(w,x)\n#         output = w * x + b\n#         return output\n#         \n#     @staticmethod\n#     def backward(ctx, grad_output):\n#         w,x = ctx.saved_tensors\n#         grad_w = grad_output * x\n#         if ctx.x_requires_grad:\n#             grad_x = grad_output * w\n#         else:\n#             grad_x = None\n#         grad_b = grad_output * 1\n#         return grad_w, grad_x, grad_b, None\n# ```\n# \n# \xe5\x88\x86\xe6\x9e\x90\xe5\xa6\x82\xe4\xb8\x8b\xef\xbc\x9a\n# \n# - \xe8\x87\xaa\xe5\xae\x9a\xe4\xb9\x89\xe7\x9a\x84Function\xe9\x9c\x80\xe8\xa6\x81\xe7\xbb\xa7\xe6\x89\xbfautograd.Function\xef\xbc\x8c\xe6\xb2\xa1\xe6\x9c\x89\xe6\x9e\x84\xe9\x80\xa0\xe5\x87\xbd\xe6\x95\xb0`__init__`\xef\xbc\x8cforward\xe5\x92\x8cbackward\xe5\x87\xbd\xe6\x95\xb0\xe9\x83\xbd\xe6\x98\xaf\xe9\x9d\x99\xe6\x80\x81\xe6\x96\xb9\xe6\xb3\x95\n# - backward\xe5\x87\xbd\xe6\x95\xb0\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe5\x92\x8cforward\xe5\x87\xbd\xe6\x95\xb0\xe7\x9a\x84\xe8\xbe\x93\xe5\x85\xa5\xe4\xb8\x80\xe4\xb8\x80\xe5\xaf\xb9\xe5\xba\x94\xef\xbc\x8cbackward\xe5\x87\xbd\xe6\x95\xb0\xe7\x9a\x84\xe8\xbe\x93\xe5\x85\xa5\xe5\x92\x8cforward\xe5\x87\xbd\xe6\x95\xb0\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe4\xb8\x80\xe4\xb8\x80\xe5\xaf\xb9\xe5\xba\x94\n# - backward\xe5\x87\xbd\xe6\x95\xb0\xe7\x9a\x84grad_output\xe5\x8f\x82\xe6\x95\xb0\xe5\x8d\xb3t.autograd.backward\xe4\xb8\xad\xe7\x9a\x84`grad_variables`\n# - \xe5\xa6\x82\xe6\x9e\x9c\xe6\x9f\x90\xe4\xb8\x80\xe4\xb8\xaa\xe8\xbe\x93\xe5\x85\xa5\xe4\xb8\x8d\xe9\x9c\x80\xe8\xa6\x81\xe6\xb1\x82\xe5\xaf\xbc\xef\xbc\x8c\xe7\x9b\xb4\xe6\x8e\xa5\xe8\xbf\x94\xe5\x9b\x9eNone\xef\xbc\x8c\xe5\xa6\x82forward\xe4\xb8\xad\xe7\x9a\x84\xe8\xbe\x93\xe5\x85\xa5\xe5\x8f\x82\xe6\x95\xb0x_requires_grad\xe6\x98\xbe\xe7\x84\xb6\xe6\x97\xa0\xe6\xb3\x95\xe5\xaf\xb9\xe5\xae\x83\xe6\xb1\x82\xe5\xaf\xbc\xef\xbc\x8c\xe7\x9b\xb4\xe6\x8e\xa5\xe8\xbf\x94\xe5\x9b\x9eNone\xe5\x8d\xb3\xe5\x8f\xaf\n# - \xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe5\x8f\xaf\xe8\x83\xbd\xe9\x9c\x80\xe8\xa6\x81\xe5\x88\xa9\xe7\x94\xa8\xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe7\x9a\x84\xe6\x9f\x90\xe4\xba\x9b\xe4\xb8\xad\xe9\x97\xb4\xe7\xbb\x93\xe6\x9e\x9c\xef\xbc\x8c\xe9\x9c\x80\xe8\xa6\x81\xe8\xbf\x9b\xe8\xa1\x8c\xe4\xbf\x9d\xe5\xad\x98\xef\xbc\x8c\xe5\x90\xa6\xe5\x88\x99\xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe7\xbb\x93\xe6\x9d\x9f\xe5\x90\x8e\xe8\xbf\x99\xe4\xba\x9b\xe5\xaf\xb9\xe8\xb1\xa1\xe5\x8d\xb3\xe8\xa2\xab\xe9\x87\x8a\xe6\x94\xbe\n# \n# Function\xe7\x9a\x84\xe4\xbd\xbf\xe7\x94\xa8\xe5\x88\xa9\xe7\x94\xa8Function.apply(variable)\n\n#%%\nfrom torch.autograd import Function\nclass MultiplyAdd(Function):\n                                                            \n    @staticmethod\n    def forward(ctx, w, x, b):                              \n        ctx.save_for_backward(w,x)\n        output = w * x + b\n        return output\n        \n    @staticmethod\n    def backward(ctx, grad_output):                         \n        w,x = ctx.saved_tensors\n        grad_w = grad_output * x\n        grad_x = grad_output * w\n        grad_b = grad_output * 1\n        return grad_w, grad_x, grad_b                       \n\n\n#%%\nx = t.ones(1)\nw = t.rand(1, requires_grad = True)\nb = t.rand(1, requires_grad = True)\n# \xe5\xbc\x80\xe5\xa7\x8b\xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\nz=MultiplyAdd.apply(w, x, b)\n# \xe5\xbc\x80\xe5\xa7\x8b\xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\nz.backward()\n\n# x\xe4\xb8\x8d\xe9\x9c\x80\xe8\xa6\x81\xe6\xb1\x82\xe5\xaf\xbc\xef\xbc\x8c\xe4\xb8\xad\xe9\x97\xb4\xe8\xbf\x87\xe7\xa8\x8b\xe8\xbf\x98\xe6\x98\xaf\xe4\xbc\x9a\xe8\xae\xa1\xe7\xae\x97\xe5\xae\x83\xe7\x9a\x84\xe5\xaf\xbc\xe6\x95\xb0\xef\xbc\x8c\xe4\xbd\x86\xe9\x9a\x8f\xe5\x90\x8e\xe8\xa2\xab\xe6\xb8\x85\xe7\xa9\xba\nx.grad, w.grad, b.grad\n\n\n#%%\nx = t.ones(1)\nw = t.rand(1, requires_grad = True)\nb = t.rand(1, requires_grad = True)\n#print('\xe5\xbc\x80\xe5\xa7\x8b\xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad')\nz=MultiplyAdd.apply(w,x,b)\n#print('\xe5\xbc\x80\xe5\xa7\x8b\xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad')\n\n# \xe8\xb0\x83\xe7\x94\xa8MultiplyAdd.backward\n# \xe8\xbe\x93\xe5\x87\xbagrad_w, grad_x, grad_b\nz.grad_fn.apply(t.ones(1))\n\n#%% [markdown]\n# \xe4\xb9\x8b\xe6\x89\x80\xe4\xbb\xa5forward\xe5\x87\xbd\xe6\x95\xb0\xe7\x9a\x84\xe8\xbe\x93\xe5\x85\xa5\xe6\x98\xaftensor\xef\xbc\x8c\xe8\x80\x8cbackward\xe5\x87\xbd\xe6\x95\xb0\xe7\x9a\x84\xe8\xbe\x93\xe5\x85\xa5\xe6\x98\xafvariable\xef\xbc\x8c\xe6\x98\xaf\xe4\xb8\xba\xe4\xba\x86\xe5\xae\x9e\xe7\x8e\xb0\xe9\xab\x98\xe9\x98\xb6\xe6\xb1\x82\xe5\xaf\xbc\xe3\x80\x82backward\xe5\x87\xbd\xe6\x95\xb0\xe7\x9a\x84\xe8\xbe\x93\xe5\x85\xa5\xe8\xbe\x93\xe5\x87\xba\xe8\x99\xbd\xe7\x84\xb6\xe6\x98\xafvariable\xef\xbc\x8c\xe4\xbd\x86\xe5\x9c\xa8\xe5\xae\x9e\xe9\x99\x85\xe4\xbd\xbf\xe7\x94\xa8\xe6\x97\xb6autograd.Function\xe4\xbc\x9a\xe5\xb0\x86\xe8\xbe\x93\xe5\x85\xa5variable\xe6\x8f\x90\xe5\x8f\x96\xe4\xb8\xbatensor\xef\xbc\x8c\xe5\xb9\xb6\xe5\xb0\x86\xe8\xae\xa1\xe7\xae\x97\xe7\xbb\x93\xe6\x9e\x9c\xe7\x9a\x84tensor\xe5\xb0\x81\xe8\xa3\x85\xe6\x88\x90variable\xe8\xbf\x94\xe5\x9b\x9e\xe3\x80\x82\xe5\x9c\xa8backward\xe5\x87\xbd\xe6\x95\xb0\xe4\xb8\xad\xef\xbc\x8c\xe4\xb9\x8b\xe6\x89\x80\xe4\xbb\xa5\xe4\xb9\x9f\xe8\xa6\x81\xe5\xaf\xb9variable\xe8\xbf\x9b\xe8\xa1\x8c\xe6\x93\x8d\xe4\xbd\x9c\xef\xbc\x8c\xe6\x98\xaf\xe4\xb8\xba\xe4\xba\x86\xe8\x83\xbd\xe5\xa4\x9f\xe8\xae\xa1\xe7\xae\x97\xe6\xa2\xaf\xe5\xba\xa6\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\xef\xbc\x88backward of backward\xef\xbc\x89\xe3\x80\x82\xe4\xb8\x8b\xe9\x9d\xa2\xe4\xb8\xbe\xe4\xbe\x8b\xe8\xaf\xb4\xe6\x98\x8e\xef\xbc\x8c\xe6\x9c\x89\xe5\x85\xb3torch.autograd.grad\xe7\x9a\x84\xe6\x9b\xb4\xe8\xaf\xa6\xe7\xbb\x86\xe4\xbd\xbf\xe7\x94\xa8\xe8\xaf\xb7\xe5\x8f\x82\xe7\x85\xa7\xe6\x96\x87\xe6\xa1\xa3\xe3\x80\x82\n\n#%%\nx = t.tensor([5], requires_grad=True)\ny = x ** 2\ngrad_x = t.autograd.grad(y, x, create_graph=True)\ngrad_x # dy/dx = 2 * x\n\n\n#%%\ngrad_grad_x = t.autograd.grad(grad_x[0],x)\ngrad_grad_x # \xe4\xba\x8c\xe9\x98\xb6\xe5\xaf\xbc\xe6\x95\xb0 d(2x)/dx = 2\n\n#%% [markdown]\n# \xe8\xbf\x99\xe7\xa7\x8d\xe8\xae\xbe\xe8\xae\xa1\xe8\x99\xbd\xe7\x84\xb6\xe8\x83\xbd\xe8\xae\xa9`autograd`\xe5\x85\xb7\xe6\x9c\x89\xe9\xab\x98\xe9\x98\xb6\xe6\xb1\x82\xe5\xaf\xbc\xe5\x8a\x9f\xe8\x83\xbd\xef\xbc\x8c\xe4\xbd\x86\xe5\x85\xb6\xe4\xb9\x9f\xe9\x99\x90\xe5\x88\xb6\xe4\xba\x86Tensor\xe7\x9a\x84\xe4\xbd\xbf\xe7\x94\xa8\xef\xbc\x8c\xe5\x9b\xa0autograd\xe4\xb8\xad\xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe7\x9a\x84\xe5\x87\xbd\xe6\x95\xb0\xe5\x8f\xaa\xe8\x83\xbd\xe5\x88\xa9\xe7\x94\xa8\xe5\xbd\x93\xe5\x89\x8d\xe5\xb7\xb2\xe7\xbb\x8f\xe6\x9c\x89\xe7\x9a\x84Variable\xe6\x93\x8d\xe4\xbd\x9c\xe3\x80\x82\xe8\xbf\x99\xe4\xb8\xaa\xe8\xae\xbe\xe8\xae\xa1\xe6\x98\xaf\xe5\x9c\xa8`0.2`\xe7\x89\x88\xe6\x9c\xac\xe6\x96\xb0\xe5\x8a\xa0\xe5\x85\xa5\xe7\x9a\x84\xef\xbc\x8c\xe4\xb8\xba\xe4\xba\x86\xe6\x9b\xb4\xe5\xa5\xbd\xe7\x9a\x84\xe7\x81\xb5\xe6\xb4\xbb\xe6\x80\xa7\xef\xbc\x8c\xe4\xb9\x9f\xe4\xb8\xba\xe4\xba\x86\xe5\x85\xbc\xe5\xae\xb9\xe6\x97\xa7\xe7\x89\x88\xe6\x9c\xac\xe7\x9a\x84\xe4\xbb\xa3\xe7\xa0\x81\xef\xbc\x8cPyTorch\xe8\xbf\x98\xe6\x8f\x90\xe4\xbe\x9b\xe4\xba\x86\xe5\x8f\xa6\xe5\xa4\x96\xe4\xb8\x80\xe7\xa7\x8d\xe6\x89\xa9\xe5\xb1\x95autograd\xe7\x9a\x84\xe6\x96\xb9\xe6\xb3\x95\xe3\x80\x82PyTorch\xe6\x8f\x90\xe4\xbe\x9b\xe4\xba\x86\xe4\xb8\x80\xe4\xb8\xaa\xe8\xa3\x85\xe9\xa5\xb0\xe5\x99\xa8`@once_differentiable`\xef\xbc\x8c\xe8\x83\xbd\xe5\xa4\x9f\xe5\x9c\xa8backward\xe5\x87\xbd\xe6\x95\xb0\xe4\xb8\xad\xe8\x87\xaa\xe5\x8a\xa8\xe5\xb0\x86\xe8\xbe\x93\xe5\x85\xa5\xe7\x9a\x84variable\xe6\x8f\x90\xe5\x8f\x96\xe6\x88\x90tensor\xef\xbc\x8c\xe6\x8a\x8a\xe8\xae\xa1\xe7\xae\x97\xe7\xbb\x93\xe6\x9e\x9c\xe7\x9a\x84tensor\xe8\x87\xaa\xe5\x8a\xa8\xe5\xb0\x81\xe8\xa3\x85\xe6\x88\x90variable\xe3\x80\x82\xe6\x9c\x89\xe4\xba\x86\xe8\xbf\x99\xe4\xb8\xaa\xe7\x89\xb9\xe6\x80\xa7\xe6\x88\x91\xe4\xbb\xac\xe5\xb0\xb1\xe8\x83\xbd\xe5\xa4\x9f\xe5\xbe\x88\xe6\x96\xb9\xe4\xbe\xbf\xe7\x9a\x84\xe4\xbd\xbf\xe7\x94\xa8numpy/scipy\xe4\xb8\xad\xe7\x9a\x84\xe5\x87\xbd\xe6\x95\xb0\xef\xbc\x8c\xe6\x93\x8d\xe4\xbd\x9c\xe4\xb8\x8d\xe5\x86\x8d\xe5\xb1\x80\xe9\x99\x90\xe4\xba\x8evariable\xe6\x89\x80\xe6\x94\xaf\xe6\x8c\x81\xe7\x9a\x84\xe6\x93\x8d\xe4\xbd\x9c\xe3\x80\x82\xe4\xbd\x86\xe6\x98\xaf\xe8\xbf\x99\xe7\xa7\x8d\xe5\x81\x9a\xe6\xb3\x95\xe6\xad\xa3\xe5\xa6\x82\xe5\x90\x8d\xe5\xad\x97\xe4\xb8\xad\xe6\x89\x80\xe6\x9a\x97\xe7\xa4\xba\xe7\x9a\x84\xe9\x82\xa3\xe6\xa0\xb7\xe5\x8f\xaa\xe8\x83\xbd\xe6\xb1\x82\xe5\xaf\xbc\xe4\xb8\x80\xe6\xac\xa1\xef\xbc\x8c\xe5\xae\x83\xe6\x89\x93\xe6\x96\xad\xe4\xba\x86\xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe5\x9b\xbe\xef\xbc\x8c\xe4\xb8\x8d\xe5\x86\x8d\xe6\x94\xaf\xe6\x8c\x81\xe9\xab\x98\xe9\x98\xb6\xe6\xb1\x82\xe5\xaf\xbc\xe3\x80\x82\n# \n# \n# \xe4\xb8\x8a\xe9\x9d\xa2\xe6\x89\x80\xe6\x8f\x8f\xe8\xbf\xb0\xe7\x9a\x84\xe9\x83\xbd\xe6\x98\xaf\xe6\x96\xb0\xe5\xbc\x8fFunction\xef\xbc\x8c\xe8\xbf\x98\xe6\x9c\x89\xe4\xb8\xaalegacy Function\xef\xbc\x8c\xe5\x8f\xaf\xe4\xbb\xa5\xe5\xb8\xa6\xe6\x9c\x89`__init__`\xe6\x96\xb9\xe6\xb3\x95\xef\xbc\x8c`forward`\xe5\x92\x8c`backwad`\xe5\x87\xbd\xe6\x95\xb0\xe4\xb9\x9f\xe4\xb8\x8d\xe9\x9c\x80\xe8\xa6\x81\xe5\xa3\xb0\xe6\x98\x8e\xe4\xb8\xba`@staticmethod`\xef\xbc\x8c\xe4\xbd\x86\xe9\x9a\x8f\xe7\x9d\x80\xe7\x89\x88\xe6\x9c\xac\xe6\x9b\xb4\xe8\xbf\xad\xef\xbc\x8c\xe6\xad\xa4\xe7\xb1\xbbFunction\xe5\xb0\x86\xe8\xb6\x8a\xe6\x9d\xa5\xe8\xb6\x8a\xe5\xb0\x91\xe9\x81\x87\xe5\x88\xb0\xef\xbc\x8c\xe5\x9c\xa8\xe6\xad\xa4\xe4\xb8\x8d\xe5\x81\x9a\xe6\x9b\xb4\xe5\xa4\x9a\xe4\xbb\x8b\xe7\xbb\x8d\xe3\x80\x82\n# \n# \xe6\xad\xa4\xe5\xa4\x96\xe5\x9c\xa8\xe5\xae\x9e\xe7\x8e\xb0\xe4\xba\x86\xe8\x87\xaa\xe5\xb7\xb1\xe7\x9a\x84Function\xe4\xb9\x8b\xe5\x90\x8e\xef\xbc\x8c\xe8\xbf\x98\xe5\x8f\xaf\xe4\xbb\xa5\xe4\xbd\xbf\xe7\x94\xa8`gradcheck`\xe5\x87\xbd\xe6\x95\xb0\xe6\x9d\xa5\xe6\xa3\x80\xe6\xb5\x8b\xe5\xae\x9e\xe7\x8e\xb0\xe6\x98\xaf\xe5\x90\xa6\xe6\xad\xa3\xe7\xa1\xae\xe3\x80\x82`gradcheck`\xe9\x80\x9a\xe8\xbf\x87\xe6\x95\xb0\xe5\x80\xbc\xe9\x80\xbc\xe8\xbf\x91\xe6\x9d\xa5\xe8\xae\xa1\xe7\xae\x97\xe6\xa2\xaf\xe5\xba\xa6\xef\xbc\x8c\xe5\x8f\xaf\xe8\x83\xbd\xe5\x85\xb7\xe6\x9c\x89\xe4\xb8\x80\xe5\xae\x9a\xe7\x9a\x84\xe8\xaf\xaf\xe5\xb7\xae\xef\xbc\x8c\xe9\x80\x9a\xe8\xbf\x87\xe6\x8e\xa7\xe5\x88\xb6`eps`\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f\xe5\x8f\xaf\xe4\xbb\xa5\xe6\x8e\xa7\xe5\x88\xb6\xe5\xae\xb9\xe5\xbf\x8d\xe7\x9a\x84\xe8\xaf\xaf\xe5\xb7\xae\xe3\x80\x82\n# \xe5\x85\xb3\xe4\xba\x8e\xe8\xbf\x99\xe9\x83\xa8\xe4\xbb\xbd\xe7\x9a\x84\xe5\x86\x85\xe5\xae\xb9\xe5\x8f\xaf\xe4\xbb\xa5\xe5\x8f\x82\xe8\x80\x83github\xe4\xb8\x8a\xe5\xbc\x80\xe5\x8f\x91\xe8\x80\x85\xe4\xbb\xac\xe7\x9a\x84\xe8\xae\xa8\xe8\xae\xba[^3]\xe3\x80\x82\n# \n# [^3]: https://github.com/pytorch/pytorch/pull/1016\n#%% [markdown]\n# \xe4\xb8\x8b\xe9\x9d\xa2\xe4\xb8\xbe\xe4\xbe\x8b\xe8\xaf\xb4\xe6\x98\x8e\xe5\xa6\x82\xe4\xbd\x95\xe5\x88\xa9\xe7\x94\xa8Function\xe5\xae\x9e\xe7\x8e\xb0sigmoid Function\xe3\x80\x82\n\n#%%\nclass Sigmoid(Function):\n                                                             \n    @staticmethod\n    def forward(ctx, x): \n        output = 1 / (1 + t.exp(-x))\n        ctx.save_for_backward(output)\n        return output\n        \n    @staticmethod\n    def backward(ctx, grad_output): \n        output,  = ctx.saved_tensors\n        grad_x = output * (1 - output) * grad_output\n        return grad_x                            \n\n\n#%%\n# \xe9\x87\x87\xe7\x94\xa8\xe6\x95\xb0\xe5\x80\xbc\xe9\x80\xbc\xe8\xbf\x91\xe6\x96\xb9\xe5\xbc\x8f\xe6\xa3\x80\xe9\xaa\x8c\xe8\xae\xa1\xe7\xae\x97\xe6\xa2\xaf\xe5\xba\xa6\xe7\x9a\x84\xe5\x85\xac\xe5\xbc\x8f\xe5\xaf\xb9\xe4\xb8\x8d\xe5\xaf\xb9\ntest_input = t.randn(3,4, requires_grad=True)\nt.autograd.gradcheck(Sigmoid.apply, (test_input,), eps=1e-3)\n\n\n#%%\ndef f_sigmoid(x):\n    y = Sigmoid.apply(x)\n    y.backward(t.ones(x.size()))\n    \ndef f_naive(x):\n    y =  1/(1 + t.exp(-x))\n    y.backward(t.ones(x.size()))\n    \ndef f_th(x):\n    y = t.sigmoid(x)\n    y.backward(t.ones(x.size()))\n    \nx=t.randn(100, 100, requires_grad=True)\nget_ipython().run_line_magic('timeit', '-n 100 f_sigmoid(x)')\nget_ipython().run_line_magic('timeit', '-n 100 f_naive(x)')\nget_ipython().run_line_magic('timeit', '-n 100 f_th(x)')\n\n#%% [markdown]\n# \xe6\x98\xbe\xe7\x84\xb6`f_sigmoid`\xe8\xa6\x81\xe6\xaf\x94\xe5\x8d\x95\xe7\xba\xaf\xe5\x88\xa9\xe7\x94\xa8`autograd`\xe5\x8a\xa0\xe5\x87\x8f\xe5\x92\x8c\xe4\xb9\x98\xe6\x96\xb9\xe6\x93\x8d\xe4\xbd\x9c\xe5\xae\x9e\xe7\x8e\xb0\xe7\x9a\x84\xe5\x87\xbd\xe6\x95\xb0\xe5\xbf\xab\xe4\xb8\x8d\xe5\xb0\x91\xef\xbc\x8c\xe5\x9b\xa0\xe4\xb8\xbaf_sigmoid\xe7\x9a\x84backward\xe4\xbc\x98\xe5\x8c\x96\xe4\xba\x86\xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe7\x9a\x84\xe8\xbf\x87\xe7\xa8\x8b\xe3\x80\x82\xe5\x8f\xa6\xe5\xa4\x96\xe5\x8f\xaf\xe4\xbb\xa5\xe7\x9c\x8b\xe5\x87\xba\xe7\xb3\xbb\xe7\xbb\x9f\xe5\xae\x9e\xe7\x8e\xb0\xe7\x9a\x84built-in\xe6\x8e\xa5\xe5\x8f\xa3(t.sigmoid)\xe6\x9b\xb4\xe5\xbf\xab\xe3\x80\x82\n#%% [markdown]\n# ### 3.2.4 \xe5\xb0\x8f\xe8\xaf\x95\xe7\x89\x9b\xe5\x88\x80: \xe7\x94\xa8Variable\xe5\xae\x9e\xe7\x8e\xb0\xe7\xba\xbf\xe6\x80\xa7\xe5\x9b\x9e\xe5\xbd\x92\n# \xe5\x9c\xa8\xe4\xb8\x8a\xe4\xb8\x80\xe8\x8a\x82\xe4\xb8\xad\xe8\xae\xb2\xe8\xa7\xa3\xe4\xba\x86\xe5\x88\xa9\xe7\x94\xa8tensor\xe5\xae\x9e\xe7\x8e\xb0\xe7\xba\xbf\xe6\x80\xa7\xe5\x9b\x9e\xe5\xbd\x92\xef\xbc\x8c\xe5\x9c\xa8\xe8\xbf\x99\xe4\xb8\x80\xe5\xb0\x8f\xe8\x8a\x82\xe4\xb8\xad\xef\xbc\x8c\xe5\xb0\x86\xe8\xae\xb2\xe8\xa7\xa3\xe5\xa6\x82\xe4\xbd\x95\xe5\x88\xa9\xe7\x94\xa8autograd/Variable\xe5\xae\x9e\xe7\x8e\xb0\xe7\xba\xbf\xe6\x80\xa7\xe5\x9b\x9e\xe5\xbd\x92\xef\xbc\x8c\xe4\xbb\xa5\xe6\xad\xa4\xe6\x84\x9f\xe5\x8f\x97autograd\xe7\x9a\x84\xe4\xbe\xbf\xe6\x8d\xb7\xe4\xb9\x8b\xe5\xa4\x84\xe3\x80\x82\n\n#%%\nimport torch as t\nget_ipython().run_line_magic('matplotlib', 'inline')\nfrom matplotlib import pyplot as plt\nfrom IPython import display \nimport numpy as np\n\n\n#%%\n# \xe8\xae\xbe\xe7\xbd\xae\xe9\x9a\x8f\xe6\x9c\xba\xe6\x95\xb0\xe7\xa7\x8d\xe5\xad\x90\xef\xbc\x8c\xe4\xb8\xba\xe4\xba\x86\xe5\x9c\xa8\xe4\xb8\x8d\xe5\x90\x8c\xe4\xba\xba\xe7\x94\xb5\xe8\x84\x91\xe4\xb8\x8a\xe8\xbf\x90\xe8\xa1\x8c\xe6\x97\xb6\xe4\xb8\x8b\xe9\x9d\xa2\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe4\xb8\x80\xe8\x87\xb4\nt.manual_seed(1000) \n\ndef get_fake_data(batch_size=8):\n    ''' \xe4\xba\xa7\xe7\x94\x9f\xe9\x9a\x8f\xe6\x9c\xba\xe6\x95\xb0\xe6\x8d\xae\xef\xbc\x9ay = x*2 + 3\xef\xbc\x8c\xe5\x8a\xa0\xe4\xb8\x8a\xe4\xba\x86\xe4\xb8\x80\xe4\xba\x9b\xe5\x99\xaa\xe5\xa3\xb0'''\n    x = t.rand(batch_size,1) * 5\n    y = x * 2 + 3 + t.randn(batch_size, 1)\n    return x, y\n\n\n#%%\n# \xe6\x9d\xa5\xe7\x9c\x8b\xe7\x9c\x8b\xe4\xba\xa7\xe7\x94\x9fx-y\xe5\x88\x86\xe5\xb8\x83\xe6\x98\xaf\xe4\xbb\x80\xe4\xb9\x88\xe6\xa0\xb7\xe7\x9a\x84\nx, y = get_fake_data()\nplt.scatter(x.squeeze().numpy(), y.squeeze().numpy())\n\n\n#%%\n# \xe9\x9a\x8f\xe6\x9c\xba\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe5\x8f\x82\xe6\x95\xb0\nw = t.rand(1,1, requires_grad=True)\nb = t.zeros(1,1, requires_grad=True)\nlosses = np.zeros(500)\n\nlr =0.005 # \xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\n\nfor ii in range(500):\n    x, y = get_fake_data(batch_size=32)\n    \n    # forward\xef\xbc\x9a\xe8\xae\xa1\xe7\xae\x97loss\n    y_pred = x.mm(w) + b.expand_as(y)\n    loss = 0.5 * (y_pred - y) ** 2\n    loss = loss.sum()\n    losses[ii] = loss.item()\n    \n    # backward\xef\xbc\x9a\xe6\x89\x8b\xe5\x8a\xa8\xe8\xae\xa1\xe7\xae\x97\xe6\xa2\xaf\xe5\xba\xa6\n    loss.backward()\n    \n    # \xe6\x9b\xb4\xe6\x96\xb0\xe5\x8f\x82\xe6\x95\xb0\n    w.data.sub_(lr * w.grad.data)\n    b.data.sub_(lr * b.grad.data)\n    \n    # \xe6\xa2\xaf\xe5\xba\xa6\xe6\xb8\x85\xe9\x9b\xb6\n    w.grad.data.zero_()\n    b.grad.data.zero_()\n    \n    if ii%50 ==0:\n        # \xe7\x94\xbb\xe5\x9b\xbe\n        display.clear_output(wait=True)\n        x = t.arange(0, 6).view(-1, 1)\n        y = x.mm(w.data) + b.data.expand_as(x)\n        plt.plot(x.numpy(), y.numpy()) # predicted\n        \n        x2, y2 = get_fake_data(batch_size=20) \n        plt.scatter(x2.numpy(), y2.numpy()) # true data\n        \n        plt.xlim(0,5)\n        plt.ylim(0,13)   \n        plt.show()\n        plt.pause(0.5)\n        \nprint(w.item(), b.item())\n\n\n#%%\nplt.plot(losses)\nplt.ylim(5,50)\n\n#%% [markdown]\n# \xe7\x94\xa8autograd\xe5\xae\x9e\xe7\x8e\xb0\xe7\x9a\x84\xe7\xba\xbf\xe6\x80\xa7\xe5\x9b\x9e\xe5\xbd\x92\xe6\x9c\x80\xe5\xa4\xa7\xe7\x9a\x84\xe4\xb8\x8d\xe5\x90\x8c\xe7\x82\xb9\xe5\xb0\xb1\xe5\x9c\xa8\xe4\xba\x8eautograd\xe4\xb8\x8d\xe9\x9c\x80\xe8\xa6\x81\xe8\xae\xa1\xe7\xae\x97\xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xef\xbc\x8c\xe5\x8f\xaf\xe4\xbb\xa5\xe8\x87\xaa\xe5\x8a\xa8\xe8\xae\xa1\xe7\xae\x97\xe5\xbe\xae\xe5\x88\x86\xe3\x80\x82\xe8\xbf\x99\xe7\x82\xb9\xe4\xb8\x8d\xe5\x8d\x95\xe6\x98\xaf\xe5\x9c\xa8\xe6\xb7\xb1\xe5\xba\xa6\xe5\xad\xa6\xe4\xb9\xa0\xef\xbc\x8c\xe5\x9c\xa8\xe8\xae\xb8\xe5\xa4\x9a\xe6\x9c\xba\xe5\x99\xa8\xe5\xad\xa6\xe4\xb9\xa0\xe7\x9a\x84\xe9\x97\xae\xe9\xa2\x98\xe4\xb8\xad\xe9\x83\xbd\xe5\xbe\x88\xe6\x9c\x89\xe7\x94\xa8\xe3\x80\x82\xe5\x8f\xa6\xe5\xa4\x96\xe9\x9c\x80\xe8\xa6\x81\xe6\xb3\xa8\xe6\x84\x8f\xe7\x9a\x84\xe6\x98\xaf\xe5\x9c\xa8\xe6\xaf\x8f\xe6\xac\xa1\xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe4\xb9\x8b\xe5\x89\x8d\xe8\xa6\x81\xe8\xae\xb0\xe5\xbe\x97\xe5\x85\x88\xe6\x8a\x8a\xe6\xa2\xaf\xe5\xba\xa6\xe6\xb8\x85\xe9\x9b\xb6\xe3\x80\x82\n# \n# \xe6\x9c\xac\xe7\xab\xa0\xe4\xb8\xbb\xe8\xa6\x81\xe4\xbb\x8b\xe7\xbb\x8d\xe4\xba\x86PyTorch\xe4\xb8\xad\xe4\xb8\xa4\xe4\xb8\xaa\xe5\x9f\xba\xe7\xa1\x80\xe5\xba\x95\xe5\xb1\x82\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe7\xbb\x93\xe6\x9e\x84\xef\xbc\x9aTensor\xe5\x92\x8cautograd\xe4\xb8\xad\xe7\x9a\x84Variable\xe3\x80\x82Tensor\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa\xe7\xb1\xbb\xe4\xbc\xbcNumpy\xe6\x95\xb0\xe7\xbb\x84\xe7\x9a\x84\xe9\xab\x98\xe6\x95\x88\xe5\xa4\x9a\xe7\xbb\xb4\xe6\x95\xb0\xe5\x80\xbc\xe8\xbf\x90\xe7\xae\x97\xe6\x95\xb0\xe6\x8d\xae\xe7\xbb\x93\xe6\x9e\x84\xef\xbc\x8c\xe6\x9c\x89\xe7\x9d\x80\xe5\x92\x8cNumpy\xe7\x9b\xb8\xe7\xb1\xbb\xe4\xbc\xbc\xe7\x9a\x84\xe6\x8e\xa5\xe5\x8f\xa3\xef\xbc\x8c\xe5\xb9\xb6\xe6\x8f\x90\xe4\xbe\x9b\xe7\xae\x80\xe5\x8d\x95\xe6\x98\x93\xe7\x94\xa8\xe7\x9a\x84GPU\xe5\x8a\xa0\xe9\x80\x9f\xe3\x80\x82Variable\xe6\x98\xafautograd\xe5\xb0\x81\xe8\xa3\x85\xe4\xba\x86Tensor\xe5\xb9\xb6\xe6\x8f\x90\xe4\xbe\x9b\xe8\x87\xaa\xe5\x8a\xa8\xe6\xb1\x82\xe5\xaf\xbc\xe6\x8a\x80\xe6\x9c\xaf\xe7\x9a\x84\xef\xbc\x8c\xe5\x85\xb7\xe6\x9c\x89\xe5\x92\x8cTensor\xe5\x87\xa0\xe4\xb9\x8e\xe4\xb8\x80\xe6\xa0\xb7\xe7\x9a\x84\xe6\x8e\xa5\xe5\x8f\xa3\xe3\x80\x82`autograd`\xe6\x98\xafPyTorch\xe7\x9a\x84\xe8\x87\xaa\xe5\x8a\xa8\xe5\xbe\xae\xe5\x88\x86\xe5\xbc\x95\xe6\x93\x8e\xef\xbc\x8c\xe9\x87\x87\xe7\x94\xa8\xe5\x8a\xa8\xe6\x80\x81\xe8\xae\xa1\xe7\xae\x97\xe5\x9b\xbe\xe6\x8a\x80\xe6\x9c\xaf\xef\xbc\x8c\xe8\x83\xbd\xe5\xa4\x9f\xe5\xbf\xab\xe9\x80\x9f\xe9\xab\x98\xe6\x95\x88\xe7\x9a\x84\xe8\xae\xa1\xe7\xae\x97\xe5\xaf\xbc\xe6\x95\xb0\xe3\x80\x82\n\n"""
2.PyTorch _basics PyTorch基础/grad.py,32,"b""\n#%%\nimport torch\n\nt_c = torch.tensor([0.5, 14.0, 15.0, 28.0, 11.0, 8.0, 3.0, -4.0, 6.0, 13.0, 21.0])\n# t_u = torch.tensor([32.9000, 57.2000, 59.0000, 82.4000, 51.8000, 46.4000, 37.4000, 24.8000, 42.8000, 55.4000, 69.8000])\n# t_u = torch.tensor([30.3057, 56.7230, 57.9168, 79.4823, 52.3995, 47.3290, 37.7892, 25.9138, 43.6609, 56.5834, 71.3901])\nt_u = torch.tensor([35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4])\n\n\n#%%\ndef model(t_u, w, b):\n    return w * t_u + b\n\n\n#%%\ndef loss_fn(t_p, t_c):\n    squared_diffs = (t_p - t_c)**2\n    return squared_diffs.mean()\n\n\n#%%\nw = torch.ones(1)\nb = torch.zeros(1)\n\nt_p = model(t_u, w, b)\nt_p\n\n\n#%%\nloss = loss_fn(t_p, t_c)\nloss\n\n\n#%%\ndelta = 0.1\n\nloss_rate_of_change_w = (loss_fn(model(t_u, w + delta, b), t_c) - loss_fn(model(t_u, w - delta, b), t_c)) / (2.0 * delta)\n\n\n#%%\nlearning_rate = 1e-2\n\nw = w - learning_rate * loss_rate_of_change_w\n\n\n#%%\nloss_rate_of_change_b = (loss_fn(model(t_u, w, b + delta), t_c) - loss_fn(model(t_u, w, b - delta), t_c)) / (2.0 * delta)\n\nb = b - learning_rate * loss_rate_of_change_b\n\n\n#%%\ndef dloss_fn(t_p, t_c):\n    dsq_diffs = 2 * (t_p - t_c)\n    return dsq_diffs\n\n\n#%%\ndef model(t_u, w, b):\n    return w * t_u + b\n\n\n#%%\ndef dmodel_dw(t_u, w, b):\n    return t_u\n\n\n#%%\ndef dmodel_db(t_u, w, b):\n    return 1.0\n\n\n#%%\ndef grad_fn(t_u, t_c, t_p, w, b):\n    dloss_dw = dloss_fn(t_p, t_c) * dmodel_dw(t_u, w, b)\n    dloss_db = dloss_fn(t_p, t_c) * dmodel_db(t_u, w, b)\n    return torch.stack([dloss_dw.mean(), dloss_db.mean()])\n\n\n#%%\nparams = torch.tensor([1.0, 0.0])\n\nnepochs = 100\n\nlearning_rate = 1e-2\n\nfor epoch in range(nepochs):\n    # forward pass\n    w, b = params\n    t_p = model(t_u, w, b)\n\n    loss = loss_fn(t_p, t_c)\n    print('Epoch %d, Loss %f' % (epoch, float(loss)))\n    \n    # backward pass\n    grad = grad_fn(t_u, t_c, t_p, w, b)\n\n    print('Params:', params)\n    print('Grad:', grad)\n    \n    params = params - learning_rate * grad\n    \nparams\n\n\n#%%\nparams = torch.tensor([1.0, 0.0])\n\nnepochs = 100\n\nlearning_rate = 1e-4\n\nfor epoch in range(nepochs):\n    # forward pass\n    w, b = params\n    t_p = model(t_u, w, b)\n\n    loss = loss_fn(t_p, t_c)\n    print('Epoch %d, Loss %f' % (epoch, float(loss)))\n    \n    # backward pass\n    grad = grad_fn(t_u, t_c, t_p, w, b)\n\n    print('Params:', params)\n    print('Grad:', grad)\n    \n    params = params - learning_rate * grad\n    \nparams\n\n\n#%%\nt_un = 0.1 * t_u\n\n\n#%%\nparams = torch.tensor([1.0, 0.0])\n\nnepochs = 100\n\nlearning_rate = 1e-2\n\nfor epoch in range(nepochs):\n    # forward pass\n    w, b = params\n    t_p = model(t_un, w, b)\n\n    loss = loss_fn(t_p, t_c)\n    print('Epoch %d, Loss %f' % (epoch, float(loss)))\n    \n    # backward pass\n    grad = grad_fn(t_un, t_c, t_p, w, b)\n\n    print('Params:', params)\n    print('Grad:', grad)\n    \n    params = params - learning_rate * grad\n    \nparams\n\n\n#%%\nparams = torch.tensor([1.0, 0.0])\n\nnepochs = 5000\n\nlearning_rate = 1e-2\n\nfor epoch in range(nepochs):\n    # forward pass\n    w, b = params\n    t_p = model(t_un, w, b)\n\n    loss = loss_fn(t_p, t_c)\n    print('Epoch %d, Loss %f' % (epoch, float(loss)))\n    \n    # backward pass\n    grad = grad_fn(t_un, t_c, t_p, w, b)\n\n    params = params - learning_rate * grad\n    \nparams\n\n\n#%%\ndef model(t_u, w, b):\n    return w * t_u + b\n\ndef loss_fn(t_p, t_c):\n    sq_diffs = (t_p - t_c)**2\n    return sq_diffs.mean()\n\n\n#%%\nparams = torch.tensor([1.0, 0.0], requires_grad=True)\n\nloss = loss_fn(model(t_u, *params), t_c)\n\n\n#%%\nparams.grad is None\n\n\n#%%\nloss.backward()\n\n\n#%%\nparams.grad\n\n\n#%%\nif params.grad is not None:\n    params.grad.zero_()\n\n\n#%%\ndef model(t_u, w, b):\n    return w * t_u + b\n\ndef loss_fn(t_p, t_c):\n    sq_diffs = (t_p - t_c)**2\n    return sq_diffs.mean()\n\nparams = torch.tensor([1.0, 0.0], requires_grad=True)\n\nnepochs = 5000\n\nlearning_rate = 1e-2\n\nfor epoch in range(nepochs):\n    # forward pass\n    t_p = model(t_un, *params)\n    loss = loss_fn(t_p, t_c)\n\n    print('Epoch %d, Loss %f' % (epoch, float(loss)))\n    \n    # backward pass\n    if params.grad is not None:\n        params.grad.zero_()\n\n    loss.backward()\n\n    #params.grad.clamp_(-1.0, 1.0)\n    #print(params, params.grad)\n\n    params = (params - learning_rate * params.grad).detach().requires_grad_()\n\nparams\n#t_p = model(t_un, *params)\n#t_p\n\n\n#%%\nimport torch.optim as optim\n\ndir(optim)\n\n\n#%%\nparams = torch.tensor([1.0, 0.0], requires_grad=True)\n\nlearning_rate = 1e-5\n\noptimizer = optim.SGD([params], lr=learning_rate)\n\n\n#%%\nt_p = model(t_u, *params)\n\nloss = loss_fn(t_p, t_c)\n\nloss.backward()\n\noptimizer.step()\n\nparams\n\n\n#%%\nparams = torch.tensor([1.0, 0.0], requires_grad=True)\n\nlearning_rate = 1e-2\n\noptimizer = optim.SGD([params], lr=learning_rate)\n\nt_p = model(t_un, *params)\n\nloss = loss_fn(t_p, t_c)\n\noptimizer.zero_grad()\n\nloss.backward()\n\noptimizer.step()\n\nparams\n\n\n#%%\ndef model(t_u, w, b):\n    return w * t_u + b\n\ndef loss_fn(t_p, t_c):\n    sq_diffs = (t_p - t_c)**2\n    return sq_diffs.mean()\n\nparams = torch.tensor([1.0, 0.0], requires_grad=True)\n\nnepochs = 5000\nlearning_rate = 1e-2\n\noptimizer = optim.SGD([params], lr=learning_rate)\n\nfor epoch in range(nepochs):\n    \n    # forward pass\n    t_p = model(t_un, *params)\n    loss = loss_fn(t_p, t_c)\n\n    print('Epoch %d, Loss %f' % (epoch, float(loss)))\n        \n    # backward pass\n    optimizer.zero_grad()\n    loss.backward()    \n    optimizer.step()\n\nt_p = model(t_un, *params)\n\nparams\n\n\n#%%\ndef model(t_u, w, b):\n    return w * t_u + b\n\ndef loss_fn(t_p, t_c):\n    sq_diffs = (t_p - t_c)**2\n    return sq_diffs.mean()\n\nparams = torch.tensor([1.0, 0.0], requires_grad=True)\n\nnepochs = 5000\nlearning_rate = 1e-1\n\noptimizer = optim.Adam([params], lr=learning_rate)\n\nfor epoch in range(nepochs):\n    # forward pass\n    t_p = model(t_u, *params)\n    loss = loss_fn(t_p, t_c)\n\n    print('Epoch %d, Loss %f' % (epoch, float(loss)))\n        \n    # backward pass\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\nt_p = model(t_u, *params)\n\nparams\n\n\n#%%\nfrom matplotlib import pyplot as plt\n\nplt.plot(0.1 * t_u.numpy(), t_p.detach().numpy())\nplt.plot(0.1 * t_u.numpy(), t_c.numpy(), 'o')\n\n\n#%%\nn_samples = t_u.shape[0]\nn_val = int(0.2 * n_samples)\n\nshuffled_indices = torch.randperm(n_samples)\n\ntrain_indices = shuffled_indices[:-n_val]\nval_indices = shuffled_indices[-n_val:]\n\ntrain_indices, val_indices\n\n\n#%%\nt_u_train = t_u[train_indices]\nt_c_train = t_c[train_indices]\n\nt_u_val = t_u[val_indices]\nt_c_val = t_c[val_indices]\n\n\n#%%\ndef model(t_u, w, b):\n    return w * t_u + b\n\ndef loss_fn(t_p, t_c):\n    sq_diffs = (t_p - t_c)**2\n    return sq_diffs.mean()\n\nparams = torch.tensor([1.0, 0.0], requires_grad=True)\n\nnepochs = 5000\nlearning_rate = 1e-2\n\noptimizer = optim.SGD([params], lr=learning_rate)\n\nt_un_train = 0.1 * t_u_train\nt_un_val = 0.1 * t_u_val\n\nfor epoch in range(nepochs):\n    \n    # forward pass\n    t_p_train = model(t_un_train, *params)\n    loss_train = loss_fn(t_p_train, t_c_train)\n\n    t_p_val = model(t_un_val, *params)\n    loss_val = loss_fn(t_p_val, t_c_val)\n\n    print('Epoch %d, Training loss %f, Validation loss %f' % (epoch, float(loss_train), float(loss_val)))\n        \n    # backward pass\n    optimizer.zero_grad()\n    loss_train.backward()    \n    optimizer.step()\n\nt_p = model(t_un, *params)\n\nparams\n\n\n#%%\nfor epoch in range(nepochs):\n    \n    # forward pass\n    t_p_train = model(t_un_train, *params)\n    loss_train = loss_fn(t_p_train, t_c_train)\n\n    with torch.no_grad():\n        t_p_val = model(t_un_val, *params)\n        loss_val = loss_fn(t_p_val, t_c_val)\n\n    print('Epoch %d, Training loss %f, Validation loss %f' % (epoch, float(loss_train), float(loss_val)))\n        \n    # backward pass\n    optimizer.zero_grad()\n    loss_train.backward()    \n    optimizer.step()\n\n\n#%%\nfor epoch in range(nepochs):\n    # ...\n    print(loss_val.requires_grad)  # prints False\n    # ...\n\n\n#%%\ndef forward(t_u, t_c, is_train):\n    with torch.set_grad_enabled(is_train):\n        t_p = model(t_u, *params)\n        loss = loss_fn(t_p, t_c)\n    return loss\n\n\n#%%\nimport torch\nimport torch.nn as nn\n\nmodel = nn.Linear(1, 1) # We'll look into the arguments in a minute\n\n\n#%%\ny = model.forward(x)  # DON'T DO THIS\ny = model(x)          # DO THIS\n\n\n#%%\nmodel.weight\n\n\n#%%\nx = torch.ones(1)\n\nmodel(x)\n\n\n#%%\nx = torch.ones(10, 1)\n\nmodel(x)\n\n\n#%%\nt_u = torch.unsqueeze(t_u, 1)\nt_c = torch.unsqueeze(t_c, 1)\n\n\n#%%\nmodel = nn.Linear(1, 1)\n\n\n#%%\nmodel.parameters()\n\n\n#%%\nlist(model.parameters())\n\n\n#%%\nmodel = nn.Linear(1, 1)\n\nlearning_rate = 1e-2\n\noptimizer = optim.SGD(model.parameters(), lr=learning_rate)\n\n\n#%%\nt_u_train = t_u[train_indices]\nt_c_train = t_c[train_indices]\n\nt_u_val = t_u[val_indices]\nt_c_val = t_c[val_indices]\n\nt_un_train = 0.1 * t_u_train\nt_un_val = 0.1 * t_u_val\n\n\n#%%\nfor epoch in range(nepochs):\n    \n    # forward pass\n    t_p_train = model(t_un_train)\n    loss_train = loss_fn(t_p_train, t_c_train)\n\n    with torch.no_grad():\n        t_p_val = model(t_un_val)\n        loss_val = loss_fn(t_p_val, t_c_val)\n\n    print('Epoch %d, Training loss %f, Validation loss %f' % (epoch, float(loss_train), float(loss_val)))\n        \n    # backward pass\n    optimizer.zero_grad()\n    loss_train.backward()    \n    optimizer.step()\n\n\n#%%\nmodel.weight, model.bias\n\n\n#%%\nloss_fn = nn.MSELoss()\n\n\n#%%\nimport torch\nimport torch.nn as nn\n\nmodel = nn.Linear(1, 1)\n\nlearning_rate = 1e-2\n\noptimizer = optim.SGD(model.parameters(), lr=learning_rate)\n\nloss_fn = nn.MSELoss()\n\nnepochs = 5000\n\nfor epoch in range(nepochs):\n    # forward pass\n    t_p_train = model(t_un_train)\n    loss_train = loss_fn(t_p_train, t_c_train)\n\n    with torch.no_grad():\n        t_p_val = model(t_un_val)\n        loss_val = loss_fn(t_p_val, t_c_val)\n\n    print('Epoch %d, Training loss %f, Validation loss %f' % (epoch, float(loss_train), float(loss_val)))\n        \n    # backward pass\n    optimizer.zero_grad()\n    loss_train.backward()    \n    optimizer.step()\n    \nmodel.weight, model.bias\n\n\n#%%\nmodel = nn.Sequential(\n            nn.Linear(1, 10),\n            nn.Tanh(),\n            nn.Linear(10, 1))\nmodel\n\n\n#%%\n[param.shape for param in model.parameters()]\n\n\n#%%\nfor name, param in model.named_parameters():\n    print(name, param.shape)\n\n\n#%%\nfrom collections import OrderedDict\n\nmodel = nn.Sequential(OrderedDict([\n    ('hidden_linear', nn.Linear(1, 10)),\n    ('hidden_activation', nn.Tanh()),\n    ('output_linear', nn.Linear(10, 1))\n]))\n\nmodel\n\n\n#%%\nfor name, param in model.named_parameters():\n    print(name, param.shape)\n\n\n#%%\nmodel.hidden_linear.weight\n\n\n#%%\nimport torch\nimport torch.nn as nn\n\nmodel = nn.Sequential(OrderedDict([\n    ('hidden_linear', nn.Linear(1, 10)),\n    ('hidden_activation', nn.Tanh()),\n    ('output_linear', nn.Linear(10, 1))\n]))\n\nlearning_rate = 1e-2\n\noptimizer = optim.SGD(model.parameters(), lr=learning_rate)\n\nloss_fn = nn.MSELoss()\n\nnepochs = 5000\n\nfor epoch in range(nepochs):\n    # forward pass\n    t_p_train = model(t_un_train)\n    loss_train = loss_fn(t_p_train, t_c_train)\n\n    with torch.no_grad():\n        t_p_val = model(t_un_val)\n        loss_val = loss_fn(t_p_val, t_c_val)\n\n    print('Epoch %d, Training loss %f, Validation loss %f' % (epoch, float(loss_train), float(loss_val)))\n        \n    # backward pass\n    optimizer.zero_grad()\n    loss_train.backward()    \n    optimizer.step()\n    \nmodel(t_un_val), t_c_val\n\n\n#%%\nfrom matplotlib import pyplot as plt\n\nplt.plot(t_u.numpy(), t_c.numpy(), 'o')\nplt.plot(t_u.numpy(), model(0.1 * t_u).detach().numpy(), 'x')\n\n\n#%%\nfrom matplotlib import pyplot as plt\n\nplt.plot(t_un.numpy(), t_c.numpy(), 'o')\nplt.plot(t_un.numpy(), model(t_un, w, b).numpy(), 'r-')\n\n#plt.plot(t_u.numpy(), model(0.1 * t_u).detach().numpy(), 'x')\n\n\n"""
2.PyTorch _basics PyTorch基础/simple_regression 简单回归.py,7,"b'#%%\nimport torch\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom itertools import count\n\n#%%\nrandom_state = 5000\ntorch.manual_seed(random_state)\nPOLY_DEGREE = 4\nW_target = torch.randn(POLY_DEGREE, 1) * 5\nb_target = torch.randn(1) * 5\n\n#%%\ndef make_features(x):\n    """"""\xe5\x88\x9b\xe5\xbb\xba\xe4\xb8\x80\xe4\xb8\xaa\xe7\x89\xb9\xe5\xbe\x81\xe7\x9f\xa9\xe9\x98\xb5\xe7\xbb\x93\xe6\x9e\x84\xe4\xb8\xba[x, x^2, x^3, x^4].""""""\n    x = x.unsqueeze(1)\n    return torch.cat([x ** i for i in range(1, POLY_DEGREE + 1)], 1)\n\n\ndef f(x):\n    """"""\xe8\xbf\x91\xe4\xbc\xbc\xe5\x87\xbd\xe6\x95\xb0.""""""\n    return x.mm(W_target) + b_target[0]\n\n\ndef poly_desc(W, b):\n    """"""\xe7\x94\x9f\xe6\x88\x90\xe5\xa4\x9a\xe5\x90\x91\xe5\xbc\x8f\xe6\x8f\x8f\xe8\xbf\xb0\xe5\x86\x85\xe5\xae\xb9.""""""\n    result = \'y = \'\n    for i, w in enumerate(W):\n        result += \'{:+.2f} x^{} \'.format(w, len(W) - i)\n    result += \'{:+.2f}\'.format(b[0])\n    return result\n\n\ndef get_batch(batch_size=32):\n    """"""\xe5\x88\x9b\xe5\xbb\xba\xe7\xb1\xbb\xe4\xbc\xbc (x, f(x))\xe7\x9a\x84\xe6\x89\xb9\xe6\x95\xb0\xe6\x8d\xae.""""""\n    random = torch.from_numpy(np.sort(torch.randn(batch_size)))\n    x = make_features(random)\n    y = f(x)\n    return x, y\n\n\n#%% \xe5\xa3\xb0\xe6\x98\x8e\xe6\xa8\xa1\xe5\x9e\x8b\nfc = torch.nn.Linear(W_target.size(0), 1)\n\nfor batch_idx in count(1):\n    # \xe8\x8e\xb7\xe5\x8f\x96\xe6\x95\xb0\xe6\x8d\xae\n    batch_x, batch_y = get_batch()\n\n    # \xe9\x87\x8d\xe7\xbd\xae\xe6\xb1\x82\xe5\xaf\xbc\n    fc.zero_grad()\n\n    # \xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\n    output = F.smooth_l1_loss(fc(batch_x), batch_y)\n    loss = output.item()\n\n    # \xe5\x90\x8e\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\n    output.backward()\n\n    # \xe5\xba\x94\xe7\x94\xa8\xe5\xaf\xbc\xe6\x95\xb0\n    for param in fc.parameters():\n        param.data.add_(-0.1 * param.grad.data)\n\n    # \xe5\x81\x9c\xe6\xad\xa2\xe6\x9d\xa1\xe4\xbb\xb6\n    if loss < 1e-3:\n        plt.cla()\n        plt.scatter(batch_x.data.numpy()[:, 0], batch_y.data.numpy()[:, 0], label=\'real curve\', color=\'b\')\n        plt.plot(batch_x.data.numpy()[:, 0], fc(batch_x).data.numpy()[:, 0], label=\'fitting curve\', color=\'r\')\n        plt.legend()\n        plt.show()\n        break\n\n#%%\nprint(\'Loss: {:.6f} after {} batches\'.format(loss, batch_idx))\nprint(\'==> Learned function:\\t\' + poly_desc(fc.weight.data.view(-1), fc.bias.data))\nprint(\'==> Actual function:\\t\' + poly_desc(W_target.view(-1), b_target))\n'"
2.PyTorch _basics PyTorch基础/tensor 张量的构造和使用.py,35,"b""#%% [markdown]\n# # \xe5\xbc\xa0\xe9\x87\x8f\xe7\x9a\x84\xe6\x9e\x84\xe9\x80\xa0\xe5\x92\x8c\xe4\xbd\xbf\xe7\x94\xa8\n\n#%%\nimport torch\n\n#%% [markdown]\n# \xe6\x9e\x84\xe9\x80\xa0\xe5\xbc\xa0\xe9\x87\x8f\n\n#%%\nt1 = torch.tensor([0., 1., 2.])\nt2 = torch.tensor([[0., 1., 2.], [3., 4., 5.], [6., 7., 8.]])\nt3 = torch.tensor([[[0., 1., 2.], [3., 4., 5.], [6., 7., 8.]],\n        [[9., 10., 11.], [12., 13., 14.], [15., 16., 17.]],\n        [[18., 19., 20.], [21., 22., 23.], [24., 25., 26.]]])\n\n\n#%%\nt1 = torch.empty(2) # \xe6\x9c\xaa\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\nt2 = torch.zeros(2, 2) # \xe5\x90\x84\xe5\x85\x83\xe7\xb4\xa0\xe5\x80\xbc.\nt3 = torch.ones(2, 2, 2) # \xe5\x90\x84\xe5\x85\x83\xe7\xb4\xa0\xe5\x80\xbc\xe4\xb8\xba1.\nt4 = torch.full((2, 2, 2, 2), 3.) # \xe5\x90\x84\xe5\x85\x83\xe7\xb4\xa0\xe5\x80\xbc\xe4\xb8\xba3.\n\n\n#%%\nt2 = torch.empty(2, 2)\nt2[0, 0] = 0.\nt2[0, 1] = 1.\nt2[1, 0] = 2.\nt2[1, 1] = 3.\nprint(t2)\nprint(t2.equal(torch.tensor([[0., 1.], [2., 3.]])))\n\n\n#%%\ntorch.zeros(2, 3, 4)\ntorch.ones(2, 3, 4)\n\n\n#%%\ntorch.ones_like(t2)\n\n\n#%%\ntorch.linspace(0, 3, steps=4)\n\n#%% [markdown]\n# \xe5\xbc\xa0\xe9\x87\x8f\xe7\x9a\x84\xe6\x80\xa7\xe8\xb4\xa8\n\n#%%\nprint('data = {}'.format(t2))\nprint('size = {}'.format(t2.size()))\nprint('dim = {}'.format(t2.dim()))\nprint('numel = {}'.format(t2.numel()))\n\n\n#%%\nt2.dtype\n\n#%% [markdown]\n# \xe6\x94\xb9\xe5\x8f\x98\xe5\xbc\xa0\xe9\x87\x8f\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f\n\n#%%\ntc = torch.arange(12) # \xe5\xbc\xa0\xe9\x87\x8f\xe5\xa4\xa7\xe5\xb0\x8f (12,)\nprint('tc = {}'.format(tc))\nt322 = tc.reshape(3, 2, 2) # \xe5\xbc\xa0\xe9\x87\x8f\xe5\xa4\xa7\xe5\xb0\x8f (3, 2, 2)\nprint('t322 = {}'.format(t322))\nt43 = t322.reshape(4, 3) # \xe5\xbc\xa0\xe9\x87\x8f\xe5\xa4\xa7\xe5\xb0\x8f (4, 3)\nprint('t43 = {}'.format(t43))\n\n\n#%%\nt12 = torch.tensor([[5., -9.],])\nt21 = t12.transpose(0, 1)\nprint('t21 = {}'.format(t21))\nt21 = t12.t()\nprint('t21 = {}'.format(t21))\n\n\n#%%\nt12 = torch.tensor([[5., -9.],])\nprint('t12 = {}'.format(t12))\nt34 = t12.repeat(3, 2)\nprint('t34 = {}'.format(t34))\n\n\n#%%\nt44 = torch.arange(16).reshape(4, 4)\nprint('t44 = {}'.format(t44))\nt23 = t44[1:-1, :3]\nprint('t23 = {}'.format(t23))\n\n#%% [markdown]\n# \xe5\xbc\xa0\xe9\x87\x8f\xe7\x9a\x84\xe6\x95\xb0\xe5\xad\xa6\xe8\xbf\x90\xe7\xae\x97\n\n#%%\ntl = torch.tensor([[1., 2., 3.], [4., 5., 6.]])\ntr = torch.tensor([[7., 8., 9.], [10., 11., 12.]])\nprint(tl + tr) # \xe5\x8a\xa0\xe6\xb3\x95\nprint(tl - tr) # \xe5\x87\x8f\xe6\xb3\x95\nprint(tl * tr) # \xe4\xb9\x98\xe6\xb3\x95\nprint(tl / tr) # \xe9\x99\xa4\xe6\xb3\x95\nprint(tl ** tr) # \xe6\x9c\x89\xe7\x90\x86\xe6\x95\xb0\xe6\xac\xa1\xe4\xb9\x98\xe6\x96\xb9\nprint(tl ** (1 / tr)) # \xe6\x9c\x89\xe7\x90\x86\xe6\x95\xb0\xe6\xac\xa1\xe5\xbc\x80\xe6\x96\xb9\n\n\n#%%\nprint(torch.zeros(3, 4) + 5) # \xe5\xbe\x97\xe5\x88\xb0\xe5\x90\x84\xe5\x85\x83\xe7\xb4\xa0\xe5\x85\xa8\xe4\xb8\xba5\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f\xe4\xb8\xba(3,4)\xe7\x9a\x84\xe5\xbc\xa0\xe9\x87\x8f\nprint(-6 * torch.ones(2)) # \xe5\xbe\x97\xe5\x88\xb0\xe5\x90\x84\xe5\x85\x83\xe7\xb4\xa0\xe5\x85\xa8\xe4\xb8\xba-6\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f\xe4\xb8\xba(2,)\xe7\x9a\x84\xe5\xbc\xa0\xe9\x87\x8f\nprint(torch.ones(2, 3, 4) + torch.ones(4)) # \xe5\xbe\x97\xe5\x88\xb0\xe5\x90\x84\xe5\x85\x83\xe7\xb4\xa0\xe5\x85\xa8\xe4\xb8\xba2\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f\xe4\xb8\xba(2,3,4)\xe7\x9a\x84\xe5\xbc\xa0\xe9\x87\x8f\n\n\n#%%\nt234 = torch.arange(24).reshape(2, 3, 4)\nprint('sqrt = {}'.format(t234.sqrt()))\nprint('sum = {}'.format(t234.sum()))\nprint('prod = {}'.format(t234.prod()))\nprint('norm(2) = {}'.format(t234.norm(2)))\nprint('cumsum = {}'.format(t234.cumsum(dim=0)))\nprint('cumprod = {}'.format(t234.cumprod(dim=1)))\n\n\n#%%\ntp = torch.pow(torch.arange(1, 4), torch.arange(3))\nprint('pow = {}'.format(tp))\nte = torch.exp(torch.tensor([0.1, -0.01]))\nprint('exp = {}'.format(te))\nts = torch.sin(torch.tensor([[3.14 / 4,],]))\nprint('sin = {}'.format(ts))\n\n\n#%%\nt5 = torch.arange(5)\ntf = torch.frac(t5 * 0.3)\nprint('frac = {}'.format(tf))\ntc = torch.clamp(t5, 0.5, 3.5)\nprint('clamp = {}'.format(tc))\n\n#%% [markdown]\n# \xe5\xbc\xa0\xe9\x87\x8f\xe7\x9a\x84\xe6\x8b\xbc\xe6\x8e\xa5\n\n#%%\ntp = torch.arange(12).reshape(3, 4)\ntn = -tp\ntc0 = torch.cat([tp, tn], 0)\nprint('tc0 = {}'.format(tc0))\ntc1 = torch.cat([tp, tp, tn, tn], 1)\nprint('tc1 = {}'.format(tc1))\n\n\n#%%\ntp = torch.arange(12).reshape(3, 4)\ntn = -tp\nts0 = torch.stack([tp, tn], 0)\nprint('ts0 = {}'.format(ts0))\nts1 = torch.stack([tp, tp, tn, tn], 1)\nprint('ts1 = {}'.format(ts1))\n\n\n"""
2.PyTorch _basics PyTorch基础/tensor_only_examples.py,36,"b""\n#%%\na = [1.0, 2.0, 1.0]\n\n\n#%%\na[0]\n\n\n#%%\na[2] = 3.0\na\n\n\n#%%\nimport torch\na = torch.ones(3)\na\n\n\n#%%\na[1]\n\n\n#%%\nfloat(a[1])\n\n\n#%%\na[2] = 2.0\na\n\n\n#%%\npoints = torch.zeros(6) # <1>\npoints[0] = 1.0 # <2>\npoints[1] = 4.0\npoints[2] = 2.0\npoints[3] = 1.0\npoints[4] = 3.0\npoints[5] = 5.0\n\n\n#%%\npoints = torch.tensor([1.0, 4.0, 2.0, 1.0, 3.0, 5.0])\npoints\n\n\n#%%\nfloat(points[0]), float(points[1])\n\n\n#%%\npoints = torch.tensor([[1.0, 4.0], [2.0, 1.0], [3.0, 5.0]])\npoints\n\n\n#%%\npoints.shape\n\n\n#%%\npoints = torch.zeros(3, 2)\npoints\n\n\n#%%\npoints = torch.FloatTensor([[1.0, 4.0], [2.0, 1.0], [3.0, 5.0]])\npoints\n\n\n#%%\npoints[0, 1]\n\n\n#%%\npoints[0]\n\n\n#%%\npoints = torch.tensor([[1.0, 4.0], [2.0, 1.0], [3.0, 5.0]])\npoints.storage()\n\n\n#%%\npoints_storage = points.storage()\npoints_storage[0]\n\n\n#%%\npoints.storage()[1]\n\n\n#%%\npoints = torch.tensor([[1.0, 4.0], [2.0, 1.0], [3.0, 5.0]])\npoints_storage = points.storage()\npoints_storage[0] = 2.0\npoints\n\n\n#%%\npoints = torch.tensor([[1.0, 4.0], [2.0, 1.0], [3.0, 5.0]])\nsecond_point = points[1]\nsecond_point.storage_offset()\n\n\n#%%\nsecond_point.size()\n\n\n#%%\nsecond_point.shape\n\n\n#%%\npoints.stride()\n\n\n#%%\npoints = torch.tensor([[1.0, 4.0], [2.0, 1.0], [3.0, 5.0]])\nsecond_point = points[1]\nsecond_point.size()\n\n\n#%%\nsecond_point.storage_offset()\n\n\n#%%\nsecond_point.stride()\n\n\n#%%\npoints = torch.tensor([[1.0, 4.0], [2.0, 1.0], [3.0, 5.0]])\nsecond_point = points[1]\nsecond_point[0] = 10.0\npoints\n\n\n#%%\npoints = torch.tensor([[1.0, 4.0], [2.0, 1.0], [3.0, 5.0]])\nsecond_point = points[1].clone()\nsecond_point[0] = 10.0\npoints\n\n\n#%%\npoints = torch.tensor([[1.0, 4.0], [2.0, 1.0], [3.0, 5.0]])\npoints\n\n\n#%%\npoints_t = points.t()\npoints_t\n\n\n#%%\nid(points.storage()) == id(points_t.storage())\n\n\n#%%\npoints.stride()\n\n\n#%%\npoints_t.stride()\n\n\n#%%\nsome_tensor = torch.ones(3, 4, 5)\nsome_tensor_t = some_tensor.transpose(0, 2)\nsome_tensor.shape\n\n\n#%%\nsome_tensor_t.shape\n\n\n#%%\nsome_tensor.stride()\n\n\n#%%\nsome_tensor_t.stride()\n\n\n#%%\npoints.is_contiguous()\n\n\n#%%\npoints_t.is_contiguous()\n\n\n#%%\npoints = torch.tensor([[1.0, 4.0], [2.0, 1.0], [3.0, 5.0]])\npoints_t = points.t()\npoints_t\n\n\n#%%\npoints_t.storage()\n\n\n#%%\npoints_t.stride()\n\n\n#%%\npoints_t_cont = points_t.contiguous()\npoints_t_cont\n\n\n#%%\npoints_t_cont.stride()\n\n\n#%%\npoints_t_cont.storage()\n\n\n#%%\ndouble_points = torch.ones(10, 2, dtype=torch.double)\nshort_points = torch.tensor([[1, 2], [3, 4]], dtype=torch.short)\n\n\n#%%\nshort_points.dtype\n\n\n#%%\ndouble_points = torch.zeros(10, 2).double()\nshort_points = torch.ones(10, 2).short()\n\n\n#%%\ndouble_points = torch.zeros(10, 2).to(torch.double)\nshort_points = torch.ones(10, 2).to(dtype=torch.short)\n\n\n#%%\npoints = torch.randn(10, 2)\nshort_points = points.type(torch.short)\n\n\n#%%\npoints = torch.tensor([[1.0, 4.0], [2.0, 1.0], [3.0, 4.0]])\n\n\n#%%\nsome_list = list(range(6))\nsome_list[:]     # all elements in the list\nsome_list[1:4]   # from element 1 inclusive to element 4 exclusive\nsome_list[1:]    # from element 1 inclusive to the end of the list\nsome_list[:4]    # from the start of the list to element 4 exclusive\nsome_list[:-1]   # from the start of the list to one before the last element\nsome_list[1:4:2] # from element 1 inclusive to element 4 exclusive in steps of 2\n\n\n#%%\npoints[1:]       # all rows but first, implicitly all columns\npoints[1:, :]    # all rows but first, all columns\npoints[1:, 0]    # all rows but first, first column\n\n\n#%%\npoints = torch.ones(3, 4)\npoints_np = points.numpy()\npoints_np\n\n\n#%%\npoints = torch.from_numpy(points_np)\n\n\n#%%\ntorch.save(points, 'ourpoints.t')\n\n\n#%%\nwith open('ourpoints.t','wb') as f:\n   torch.save(points, f)\n\n\n#%%\npoints = torch.load('ourpoints.t')\n\n\n#%%\nwith open('ourpoints.t','rb') as f:\n   points = torch.load(f)\n\n\n#%%\nimport h5py\n\nf = h5py.File('ourpoints.hdf5', 'w')\ndset = f.create_dataset('coords', data=points.numpy())\nf.close()\n\n\n#%%\nf = h5py.File('ourpoints.hdf5', 'r')\ndset = f['coords']\nlast_points = dset[1:]\n\n\n#%%\nlast_points = torch.from_numpy(dset[1:])\nf.close()\n\n\n#%%\npoints_gpu = torch.tensor([[1.0, 4.0], [2.0, 1.0], [3.0, 4.0]], device='cuda')\n\n\n#%%\npoints_gpu = points.to(device='cuda')\n\n\n#%%\npoints_gpu = points.to(device='cuda:0')\n\n\n#%%\npoints = 2 * points  # <1>\npoints_gpu = 2 * points.to(device='cuda')  # <2>\n\n\n#%%\npoints_gpu = points_gpu + 4\n\n\n#%%\npoints_cpu = points_gpu.to(device='cpu')\n\n\n#%%\npoints_gpu = points.cuda()  # <1>\npoints_gpu = points.cuda(0)\npoints_cpu = points_gpu.cpu()\n\n\n#%%\na = torch.ones(3, 2)\na_t = torch.transpose(a, 0, 1)\n\n\n#%%\na = torch.ones(3, 2)\na_t = a.transpose(0, 1)\n\n\n#%%\na = torch.ones(3, 2)\n\n\n#%%\na.zero_()\na\n\n\n"""
2.PyTorch _basics PyTorch基础/tensor_tutorial.py,22,"b'""""""\nWhat is PyTorch?\n================\n\nIt\xe2\x80\x99s a Python-based scientific computing package targeted at two sets of\naudiences:\n\n-  A replacement for NumPy to use the power of GPUs\n-  a deep learning research platform that provides maximum flexibility\n   and speed\n\nGetting Started\n---------------\n\nTensors\n^^^^^^^\n\nTensors are similar to NumPy\xe2\x80\x99s ndarrays, with the addition being that\nTensors can also be used on a GPU to accelerate computing.\n""""""\n\nfrom __future__ import print_function\nimport torch\n\n###############################################################\n# Construct a 5x3 matrix, uninitialized:\n\nx = torch.empty(5, 3)\nprint(x)\n\n###############################################################\n# Construct a randomly initialized matrix:\n\nx = torch.rand(5, 3)\nprint(x)\n\n###############################################################\n# Construct a matrix filled zeros and of dtype long:\n\nx = torch.zeros(5, 3, dtype=torch.long)\nprint(x)\n\n###############################################################\n# Construct a tensor directly from data:\n\nx = torch.tensor([5.5, 3])\nprint(x)\n\n###############################################################\n# or create a tensor based on an existing tensor. These methods\n# will reuse properties of the input tensor, e.g. dtype, unless\n# new values are provided by user\n\nx = x.new_ones(5, 3, dtype=torch.double)      # new_* methods take in sizes\nprint(x)\n\nx = torch.randn_like(x, dtype=torch.float)    # override dtype!\nprint(x)                                      # result has the same size\n\n###############################################################\n# Get its size:\n\nprint(x.size())\n\n###############################################################\n# .. note::\n#     ``torch.Size`` is in fact a tuple, so it supports all tuple operations.\n#\n# Operations\n# ^^^^^^^^^^\n# There are multiple syntaxes for operations. In the following\n# example, we will take a look at the addition operation.\n#\n# Addition: syntax 1\ny = torch.rand(5, 3)\nprint(x + y)\n\n###############################################################\n# Addition: syntax 2\n\nprint(torch.add(x, y))\n\n###############################################################\n# Addition: providing an output tensor as argument\nresult = torch.empty(5, 3)\ntorch.add(x, y, out=result)\nprint(result)\n\n###############################################################\n# Addition: in-place\n\n# adds x to y\ny.add_(x)\nprint(y)\n\n###############################################################\n# .. note::\n#     Any operation that mutates a tensor in-place is post-fixed with an ``_``.\n#     For example: ``x.copy_(y)``, ``x.t_()``, will change ``x``.\n#\n# You can use standard NumPy-like indexing with all bells and whistles!\n\nprint(x[:, 1])\n\n###############################################################\n# Resizing: If you want to resize/reshape tensor, you can use ``torch.view``:\nx = torch.randn(4, 4)\ny = x.view(16)\nz = x.view(-1, 8)  # the size -1 is inferred from other dimensions\nprint(x.size(), y.size(), z.size())\n\n###############################################################\n# If you have a one element tensor, use ``.item()`` to get the value as a\n# Python number\nx = torch.randn(1)\nprint(x)\nprint(x.item())\n\n###############################################################\n# **Read later:**\n#\n#\n#   100+ Tensor operations, including transposing, indexing, slicing,\n#   mathematical operations, linear algebra, random numbers, etc.,\n#   are described\n#   `here <http://pytorch.org/docs/torch>`_.\n#\n# NumPy Bridge\n# ------------\n#\n# Converting a Torch Tensor to a NumPy array and vice versa is a breeze.\n#\n# The Torch Tensor and NumPy array will share their underlying memory\n# locations, and changing one will change the other.\n#\n# Converting a Torch Tensor to a NumPy Array\n# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\na = torch.ones(5)\nprint(a)\n\n###############################################################\n#\n\nb = a.numpy()\nprint(b)\n\n###############################################################\n# See how the numpy array changed in value.\n\na.add_(1)\nprint(a)\nprint(b)\n\n###############################################################\n# Converting NumPy Array to Torch Tensor\n# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n# See how changing the np array changed the Torch Tensor automatically\n\nimport numpy as np\na = np.ones(5)\nb = torch.from_numpy(a)\nnp.add(a, 1, out=a)\nprint(a)\nprint(b)\n\n###############################################################\n# All the Tensors on the CPU except a CharTensor support converting to\n# NumPy and back.\n#\n# CUDA Tensors\n# ------------\n#\n# Tensors can be moved onto any device using the ``.to`` method.\n\n# let us run this cell only if CUDA is available\n# We will use ``torch.device`` objects to move tensors in and out of GPU\nif torch.cuda.is_available():\n    device = torch.device(""cuda"")          # a CUDA device object\n    y = torch.ones_like(x, device=device)  # directly create a tensor on GPU\n    x = x.to(device)                       # or just use strings ``.to(""cuda"")``\n    z = x + y\n    print(z)\n    print(z.to(""cpu"", torch.double))       # ``.to`` can also change dtype together!\n'"
3.Linear_regression 线性回归/Linear-Regression-oldversion.py,5,"b'#%% [markdown]\n# # Linear Regression\n# \n# - Linear Data\n# - Linear Model\n#%% [markdown]\n# ## 1. Import Required Libraries\n\n#%%\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.init as init\n\n#%% [markdown]\n# ## 2. Generate Data\n\n#%%\nnum_data = 1000 \nnum_epoch = 1000\n\nnoise = init.normal(torch.FloatTensor(num_data,1),std=0.2)\nx = init.uniform(torch.Tensor(num_data,1),-10,10)\ny = 2*x+3\ny_noise = 2*(x+noise)+3\n\n#%% [markdown]\n# ## 3. Model & Optimizer\n\n#%%\nmodel = nn.Linear(1,1)\noutput = model(Variable(x))\n\nloss_func = nn.L1Loss()\noptimizer = optim.SGD(model.parameters(),lr=0.01)\n\n#%% [markdown]\n# ## 4. Train\n\n#%%\n# train\nloss_arr =[]\nlabel = Variable(y_noise)\nfor i in range(num_epoch):\n    output = model(Variable(x))\n    optimizer.zero_grad()\n\n    loss = loss_func(output,label)\n    loss.backward()\n    optimizer.step()\n    if i % 10 == 0:\n        print(loss)\n    loss_arr.append(loss.data.numpy()[0])\n\n#%% [markdown]\n# ## 5. Check Trained Parameters\n\n#%%\nparam_list = list(model.parameters())\nprint(param_list[0].data,param_list[1].data)\n\n\n'"
3.Linear_regression 线性回归/Linear-Regression-visdom-oldversion.py,6,"b""#%% [markdown]\n# # Linear Regression\n# \n# - Linear Data\n# - Linear Model\n#%% [markdown]\n# ## 1. Import Required Libraries\n\n#%%\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.init as init\n\n\n#%%\n# visdom is a visualization tool from facebook\n\nfrom visdom import Visdom\nviz = Visdom()\n\n#%% [markdown]\n# ## 2. Generate Data\n\n#%%\nnum_data = 1000 \nnum_epoch = 1000\n\nnoise = init.normal(torch.FloatTensor(num_data,1),std=0.2)\nx = init.uniform(torch.Tensor(num_data,1),-10,10)\ny = 2*x+3\ny_noise = 2*(x+noise)+3\n\n\n#%%\n# visualize data with visdom\n\ninput_data = torch.cat([x,y_noise],1)\n\nwin=viz.scatter(\n    X = input_data,\n    opts=dict(\n        xtickmin=-10,\n        xtickmax=10,\n        xtickstep=1,\n        ytickmin=-20,\n        ytickmax=20,\n        ytickstep=1,\n        markersymbol='dot',\n        markersize=5,\n        markercolor=np.random.randint(0, 255, num_data),\n    ),\n)\n\nviz.updateTrace(\n    X = x,\n    Y = y,\n    win=win,\n)\n\n#%% [markdown]\n# ## 3. Model & Optimizer\n\n#%%\nmodel = nn.Linear(1,1)\noutput = model(Variable(x))\n\nloss_func = nn.L1Loss()\noptimizer = optim.SGD(model.parameters(),lr=0.01)\n\n#%% [markdown]\n# ## 4. Train\n\n#%%\n# train\nloss_arr =[]\nlabel = Variable(y_noise)\nfor i in range(num_epoch):\n    output = model(Variable(x))\n    optimizer.zero_grad()\n\n    loss = loss_func(output,label)\n    loss.backward()\n    optimizer.step()\n    if i % 10 == 0:\n        print(loss)\n    loss_arr.append(loss.data.numpy()[0])\n\n#%% [markdown]\n# ## 5. Check Trained Parameters\n\n#%%\nparam_list = list(model.parameters())\nprint(param_list[0].data,param_list[1].data)\n\n#%% [markdown]\n# ## 6. Visualize output\n\n#%%\nwin_2=viz.scatter(\n    X = input_data,\n    opts=dict(\n        xtickmin=-10,\n        xtickmax=10,\n        xtickstep=1,\n        ytickmin=-20,\n        ytickmax=20,\n        ytickstep=1,\n        markersymbol='dot',\n        markercolor=np.random.randint(0, 255, num_data),\n        markersize=5,\n    ),\n)\n\nviz.updateTrace(\n    X = x,\n    Y = output.data,\n    win = win_2,\n    opts=dict(\n        xtickmin=-15,\n        xtickmax=10,\n        xtickstep=1,\n        ytickmin=-300,\n        ytickmax=200,\n        ytickstep=1,\n        markersymbol='dot',\n    ),\n)\n\n#%% [markdown]\n# ## 7. Visualize Loss Graph\n\n#%%\nx = np.reshape([i for i in range(num_epoch)],newshape=[num_epoch,1])\nloss_data = np.reshape(loss_arr,newshape=[num_epoch,1])\n\nwin2=viz.line(\n    X = x,\n    Y = loss_data,\n    opts=dict(\n        xtickmin=0,\n        xtickmax=num_epoch,\n        xtickstep=1,\n        ytickmin=0,\n        ytickmax=20,\n        ytickstep=1,\n        markercolor=np.random.randint(0, 255, num_epoch),\n    ),\n)\n\n\n"""
3.Linear_regression 线性回归/Linear-regression 线性规划.py,39,"b""#%% [markdown]\n# # \xe7\xba\xbf\xe6\x80\xa7\xe8\xa7\x84\xe5\x88\x92\n\n#%%\nimport torch\nimport torch.nn\nimport torch.optim\n\n#%% [markdown]\n# \xe7\x94\xa8 gels() \xe6\x9c\x80\xe5\xb0\x8f\xe4\xba\x8c\xe4\xb9\x98\n\n#%%\nx = torch.tensor([[1., 1., 1.], [2., 3., 1.],\n        [3., 5., 1.], [4., 2., 1.], [5., 4., 1.]])\ny = torch.tensor([-10., 12., 14., 16., 18.])\nwr, _ = torch.gels(y, x)\nw = wr[:3]\nw\n\n\n#%%\nx = torch.tensor([[1., 1., 1.], [2., 3., 1.], \n        [3., 5., 1.], [4., 2., 1.], [5., 4., 1.]])\ny = torch.tensor([[-10., -3.], [12., 14.], [14., 12.], [16., 16.], [18., 16.]])\nwr, _ = torch.gels(y, x)\nw = wr[:3, :]\nw\n\n#%% [markdown]\n# MSE \xe6\x8d\x9f\xe5\xa4\xb1\n\n#%%\ncriterion = torch.nn.MSELoss()\npred = torch.arange(5, requires_grad=True)\ny = torch.ones(5)\nloss = criterion(pred, y)\nloss\n\n#%% [markdown]\n# \xe7\x94\xa8\xe4\xbc\x98\xe5\x8c\x96\xe5\xbc\x95\xe6\x93\x8e\xe6\xb1\x82\xe8\xa7\xa3\n\n#%%\nx = torch.tensor([[1., 1., 1.], [2., 3., 1.], \n        [3., 5., 1.], [4., 2., 1.], [5., 4., 1.]])\ny = torch.tensor([-10., 12., 14., 16., 18.])\nw = torch.zeros(3, requires_grad=True)\n\ncriterion = torch.nn.MSELoss()\noptimizer = torch.optim.Adam([w,],)\n\nfor step in range(30001):\n    if step:\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    \n    pred = torch.mv(x, w)\n    loss = criterion(pred, y)\n    if step % 1000 == 0:\n        print('step = {}, loss = {:g}, W = {}'.format(\n                step, loss, w.tolist()))\n\n\n#%%\nx = torch.tensor([[1., 1.], [2., 3.], [3., 5.], [4., 2.], [5., 4.]])\ny = torch.tensor([-10., 12., 14., 16., 18.]).reshape(-1, 1)\n\nfc = torch.nn.Linear(2, 1)\ncriterion = torch.nn.MSELoss()\noptimizer = torch.optim.Adam(fc.parameters())\nweights, bias = fc.parameters()\n\nfor step in range(30001):\n    if step:\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    \n    pred = fc(x)\n    loss = criterion(pred, y)\n    \n    if step % 1000 == 0:\n        print('step = {}, loss = {:g}, weights = {}, bias={}'.format(\n                step, loss, weights[0, :].tolist(), bias.item()))\n\n#%% [markdown]\n# \xe5\xbd\x92\xe4\xb8\x80\xe5\x8c\x96\n\n#%%\nx = torch.tensor([[1, 1, 1], [2, 3, 1], [3, 5, 1],\n        [4, 2, 1], [5, 4, 1]], dtype=torch.float32)\ny = torch.tensor([-10, 12, 14, 16, 18],\n        dtype=torch.float32)\n\nx_mean, x_std = torch.mean(x, dim=0), torch.std(x, dim=0)\nx_mean[-1], x_std[-1] = 0, 1\nx_norm = (x - x_mean) / x_std\n\n#%% [markdown]\n# \xe6\x97\xa0\xe5\xbd\x92\xe4\xb8\x80\xe5\x8c\x96\xe5\x92\x8c\xe6\x9c\x89\xe5\xbd\x92\xe4\xb8\x80\xe5\x8c\x96\xe7\x9a\x84\xe6\xaf\x94\xe8\xbe\x83\n\n#%%\nx = torch.tensor([[1000000, 0.0001], [2000000, 0.0003],\n        [3000000, 0.0005], [4000000, 0.0002], [5000000, 0.0004]])\ny = torch.tensor([-1000., 1200., 1400., 1600., 1800.]).reshape(-1, 1)\n\nfc = torch.nn.Linear(2, 1)\ncriterion = torch.nn.MSELoss()\noptimizer = torch.optim.Adam(fc.parameters())\n\nfor step in range(10001):\n    if step:\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    pred = fc(x)\n    loss = criterion(pred, y)\n    if step % 1000 == 0:\n        print('step = {}, loss = {:g}'.format(step, loss))\n\n\n#%%\nx = torch.tensor([[1000000, 0.0001], [2000000, 0.0003],\n        [3000000, 0.0005], [4000000, 0.0002], [5000000, 0.0004]])\ny = torch.tensor([-1000., 1200., 1400., 1600., 1800.]).reshape(-1, 1)\n\nx_mean, x_std = torch.mean(x, dim=0), torch.std(x, dim=0)\nx_norm = (x - x_mean) / x_std\ny_mean, y_std = torch.mean(y, dim=0), torch.std(y, dim=0)\ny_norm = (y - y_mean) / y_std\n\nfc = torch.nn.Linear(2, 1)\ncriterion = torch.nn.MSELoss()\noptimizer = torch.optim.Adam(fc.parameters())\n\nfor step in range(10001):\n    if step:\n        optimizer.zero_grad()\n        loss_norm.backward()\n        optimizer.step()\n    pred_norm = fc(x_norm)\n    loss_norm = criterion(pred_norm, y_norm)\n    pred = pred_norm * y_std + y_mean\n    loss = criterion(pred, y)\n    if step % 1000 == 0:\n        print('step = {}, loss = {:g}'.format(step, loss))\n\n\n"""
3.Linear_regression 线性回归/fourth-degree-polynomial-regression.py,6,"b'#%%  Trains a single fully-connected layer to fit a 4th degree polynomial.\nfrom __future__ import print_function\nfrom itertools import count\n\nimport torch\nimport torch.nn.functional as F\n\nPOLY_DEGREE = 4\nW_target = torch.randn(POLY_DEGREE, 1) * 5\nb_target = torch.randn(1) * 5\n\n\ndef make_features(x):\n    """"""Builds features i.e. a matrix with columns [x, x^2, x^3, x^4].""""""\n    x = x.unsqueeze(1)\n    return torch.cat([x ** i for i in range(1, POLY_DEGREE+1)], 1)\n\n\ndef f(x):\n    """"""Approximated function.""""""\n    return x.mm(W_target) + b_target.item()\n\n\ndef poly_desc(W, b):\n    """"""Creates a string description of a polynomial.""""""\n    result = \'y = \'\n    for i, w in enumerate(W):\n        result += \'{:+.2f} x^{} \'.format(w, len(W) - i)\n    result += \'{:+.2f}\'.format(b[0])\n    return result\n\n\ndef get_batch(batch_size=32):\n    """"""Builds a batch i.e. (x, f(x)) pair.""""""\n    random = torch.randn(batch_size)\n    x = make_features(random)\n    y = f(x)\n    return x, y\n\n\n# Define model\nfc = torch.nn.Linear(W_target.size(0), 1)\n\nfor batch_idx in count(1):\n    # Get data\n    batch_x, batch_y = get_batch()\n\n    # Reset gradients\n    fc.zero_grad()\n\n    # Forward pass\n    output = F.smooth_l1_loss(fc(batch_x), batch_y)\n    loss = output.item()\n\n    # Backward pass\n    output.backward()\n\n    # Apply gradients\n    for param in fc.parameters():\n        param.data.add_(-0.1 * param.grad.data)\n\n    # Stop criterion\n    if loss < 1e-3:\n        break\n\nprint(\'Loss: {:.6f} after {} batches\'.format(loss, batch_idx))\nprint(\'==> Learned function:\\t\' + poly_desc(fc.weight.view(-1), fc.bias))\nprint(\'==> Actual function:\\t\' + poly_desc(W_target.view(-1), b_target))\n'"
3.Linear_regression 线性回归/linear-classifier 线性判决.py,14,"b""#%% [markdown]\n# # \xe7\xba\xbf\xe6\x80\xa7\xe5\x88\xa4\xe5\x86\xb3\n\n#%%\nimport torch\nimport torch.nn\nimport torch.optim\n\n#%% [markdown]\n# \xe4\xb8\xa4\xe5\x88\x86\xe7\xb1\xbb\xe5\x88\xa4\xe5\x86\xb3\n\n#%%\nx = torch.tensor([[1., 1., 1.], [2., 3., 1.],\n        [3., 5., 1.], [4., 2., 1.], [5., 4., 1.]])\ny = torch.tensor([0., 1., 1., 0., 1.])\nw = torch.zeros(3, requires_grad=True)\n\ncriterion = torch.nn.BCEWithLogitsLoss()\noptimizer = torch.optim.Adam([w,],)\n\nfor step in range(100001):\n    if step:\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    pred = torch.mv(x, w)\n    loss = criterion(pred, y)\n    if step % 10000 == 0:\n        print('\xe7\xac\xac{}\xe6\xad\xa5\xef\xbc\x9aloss = {:g}, W = {}'.format(step, loss, w.tolist()))\n\n#%% [markdown]\n# \xe5\xa4\x9a\xe5\x88\x86\xe7\xb1\xbb\xe5\x88\xa4\xe5\x86\xb3\n\n#%%\nx = torch.tensor([[1., 1., 1.], [2., 3., 1.],\n        [3., 5., 1.], [4., 2., 1.], [5., 4., 1.]])\ny = torch.tensor([0, 2, 1, 0, 2])\nw = torch.zeros(3, 3, requires_grad=True)\n\ncriterion = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam([w,],)\n\nfor step in range(100001):\n    if step:\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    pred = torch.mm(x, w)\n    loss = criterion(pred, y)\n    if step % 10000 == 0:\n        print('\xe7\xac\xac{}\xe6\xad\xa5\xef\xbc\x9aloss = {:g}, W = {}'.format(step, loss, w))\n\n\n"""
3.Linear_regression 线性回归/mnist-linear MNIST数字识别.py,9,"b""#%% [markdown]\n# # MNIST \xe6\x95\xb0\xe5\xad\x97\xe8\xaf\x86\xe5\x88\xab\n\n#%%\nimport torch\nimport torch.nn\nimport torch.optim\nimport torchvision.datasets\nimport torchvision.transforms\nimport torch.utils.data\n\n#%% [markdown]\n# \xe8\xaf\xbb\xe5\x8f\x96\xe6\x95\xb0\xe6\x8d\xae\n\n#%%\ntrain_dataset = torchvision.datasets.MNIST(root='./data/mnist',\n        train=True, transform=torchvision.transforms.ToTensor(),\n        download=True)\ntest_dataset = torchvision.datasets.MNIST(root='./data/mnist',\n        train=False, transform=torchvision.transforms.ToTensor(),\n        download=True)\n\n\n#%%\nbatch_size = 100\ntrain_loader = torch.utils.data.DataLoader(\n        dataset=train_dataset, batch_size=batch_size)\nprint('len(train_loader) = {}'.format(len(train_loader)))\ntest_loader = torch.utils.data.DataLoader(\n        dataset=test_dataset, batch_size=batch_size)\nprint('len(test_loader) = {}'.format(len(test_loader)))\n\nfor images, labels in train_loader:\n    print ('image.size() = {}'.format(images.size()))\n    print ('labels.size() = {}'.format(labels.size()))\n    break\n\n\n#%%\nget_ipython().run_line_magic('matplotlib', 'inline')\nimport matplotlib.pyplot as plt\nplt.imshow(images[0, 0], cmap='gray')\nplt.title('label = {}'.format(labels[0]))\n\n#%% [markdown]\n# \xe8\xae\xad\xe7\xbb\x83\n\n#%%\nfc = torch.nn.Linear(28 * 28, 10)\ncriterion = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(fc.parameters()) \n\nnum_epochs = 5\nfor epoch in range(num_epochs):\n    for idx, (images, labels) in enumerate(train_loader):\n        x = images.reshape(-1, 28*28)\n        \n        optimizer.zero_grad()\n        preds = fc(x)\n        loss = criterion(preds, labels)\n        loss.backward()\n        optimizer.step()\n        \n        if idx % 100 == 0:\n            print('\xe7\xac\xac{}\xe8\xb6\x9f\xe7\xac\xac{}\xe6\x89\xb9\xef\xbc\x9aloss = {:g}'.format(epoch, idx, loss))\n\n#%% [markdown]\n# \xe6\xb5\x8b\xe8\xaf\x95\n\n#%%\ncorrect = 0\ntotal = 0\nfor images, labels in test_loader:\n    x = images.reshape(-1, 28 * 28)\n    preds = fc(x)\n    predicted = torch.argmax(preds, 1)\n    total += labels.size(0)\n    correct += (predicted == labels).sum().item()\n    \naccuracy = correct / total\nprint('\xe6\xb5\x8b\xe8\xaf\x95\xe9\x9b\x86\xe4\xb8\x8a\xe7\x9a\x84\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87\xef\xbc\x9a{:.1%}'.format(accuracy))\n\n\n"""
3.Linear_regression 线性回归/population-regression 世界人口回归.py,12,"b'#%% [markdown]\n# # \xe4\xb8\x96\xe7\x95\x8c\xe4\xba\xba\xe5\x8f\xa3\xe5\x9b\x9e\xe5\xbd\x92\n\n#%%\nget_ipython().run_line_magic(\'matplotlib\', \'inline\')\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn\nimport torch.optim\n\n#%% [markdown]\n# \xe8\xaf\xbb\xe5\x8f\x96\xe6\x95\xb0\xe6\x8d\xae\n\n#%%\nurl = r\'http://en.wikipedia.org/wiki/World_population_estimates\'\ndf = pd.read_html(url, header=0, attrs={""class"" : ""wikitable""})[2]\ndf\n\n\n#%%\nyears = torch.tensor(df.iloc[:, 0], dtype=torch.float32)\npopulations = torch.tensor(df.iloc[:, 1], dtype=torch.float32)\n\n#%% [markdown]\n# \xe7\xba\xbf\xe6\x80\xa7\xe5\x9b\x9e\xe5\xbd\x92\n\n#%%\nx = torch.stack([years, torch.ones_like(years)], 1)\ny = populations\nwr, _ = torch.gels(y, x)\nslope, intercept = wr[:2, 0]\nresult = \'population = {:.2e} * year + {:.2e}\'.format(slope, intercept)\nprint(\'\xe5\x9b\x9e\xe5\xbd\x92\xe7\xbb\x93\xe6\x9e\x9c\xef\xbc\x9a\' + result)\n\n\n#%%\nx = torch.stack([years, torch.ones_like(years)], 1)\ny = populations\nw = x.t().mm(x).inverse().mm(x.t()).mv(y)\nslope, intercept = w\nresult = \'population = {:.2e} * year + {:.2e}\'.format(slope, intercept)\nprint(\'\xe5\x9b\x9e\xe5\xbd\x92\xe7\xbb\x93\xe6\x9e\x9c\xef\xbc\x9a\' + result)\n\n\n#%%\nplt.scatter(years, populations, s=0.1, label=\'actual\', color=\'k\')\nplt.plot(years.tolist(), (slope * years + intercept).tolist(), label=result, color=\'k\')\nplt.xlabel(\'Year\')\nplt.ylabel(\'Population\')\nplt.legend()\nplt.show();\n\n\n#%%\nx = years.reshape(-1, 1)\ny = populations\n\nx_mean, x_std = torch.mean(x), torch.std(x)\nx_norm = (x - x_mean) / x_std\ny_mean, y_std = torch.mean(y), torch.std(y)\ny_norm = (y - y_mean) / y_std\n\nfc = torch.nn.Linear(1, 1)\ncriterion = torch.nn.MSELoss()\noptimizer = torch.optim.Adam(fc.parameters())\nweight_norm, bias_norm = fc.parameters()\n\nfor step in range(5001):\n    if step:\n        fc.zero_grad()\n        loss_norm.backward()\n        optimizer.step()\n    output_norm = fc(x_norm)\n    pred_norm = output_norm.squeeze()\n    loss_norm = criterion(pred_norm, y_norm)\n    weight = y_std / x_std * weight_norm\n    bias = (weight_norm * (0 - x_mean) / x_std + bias_norm) * y_std + y_mean\n    if step % 1000 == 0:\n        print(\'\xe7\xac\xac{}\xe6\xad\xa5\xef\xbc\x9aweight = {}, bias = {}\'.format(step, weight.item(), bias.item()))\n\nresult = \'population = {:.2e} * year + {:.2e}\'.format(weight.item(), bias.item())\nprint(\'\xe5\x9b\x9e\xe5\xbd\x92\xe7\xbb\x93\xe6\x9e\x9c\xef\xbc\x9a\' + result)\n\n\n'"
3.Linear_regression 线性回归/stock-volume-prediction 股票成交量变化预测.py,7,"b""#%% [markdown]\n# # \xe8\x82\xa1\xe7\xa5\xa8\xe6\x88\x90\xe4\xba\xa4\xe9\x87\x8f\xe5\x8f\x98\xe5\x8c\x96\xe9\xa2\x84\xe6\xb5\x8b\n\n#%%\nfrom pandas_datareader.data import DataReader\n\nimport torch\nimport torch.nn\nimport torch.optim\n\n#%% [markdown]\n# \xe6\x95\xb0\xe6\x8d\xae\xe8\xaf\xbb\xe5\x8f\x96\n\n#%%\ndf = DataReader('FB.US', 'quandl', '2012-01-01', '2018-02-01')\ndf\n\n\n#%%\ntrain_start, train_end = sum(df.index >= '2017'), sum(df.index >= '2013')\ntest_start, test_end = sum(df.index >= '2018'), sum(df.index >= '2017')\nn_total_train = train_end - train_start\nn_total_test = test_end - test_start\ns_mean = df[train_start:train_end].mean()\ns_std = df[train_start:train_end].std()\nn_features = 5\ndf_feature = ((df - s_mean) / s_std).iloc[:, :n_features]\ns_label = (df['Volume'] < df['Volume'].shift(1)).astype(int)\ndf_feature, s_label\n\n#%% [markdown]\n# \xe8\xae\xad\xe7\xbb\x83\xe5\x92\x8c\xe6\xb5\x8b\xe8\xaf\x95\n\n#%%\nfc = torch.nn.Linear(n_features, 1)\nweights, bias = fc.parameters()\ncriterion = torch.nn.BCEWithLogitsLoss()\noptimizer = torch.optim.Adam(fc.parameters())\n\nx = torch.tensor(df_feature.values, dtype=torch.float32)\ny = torch.tensor(s_label.values.reshape(-1, 1), dtype=torch.float32)\n\nn_step = 20001\nfor step in range(n_step):\n    if step:\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    pred = fc(x)\n    loss = criterion(pred[train_start:train_end], y[train_start:train_end])\n    \n    if step % 500 == 0:\n        # print('#{}, \xe6\x8d\x9f\xe5\xa4\xb1 = {:g}, \xe6\x9d\x83\xe9\x87\x8d weights = {}, bias = {:g}'.format(\n        #         step, loss, weights[0, :].tolist(), bias.item()))\n        print('#{}, \xe6\x8d\x9f\xe5\xa4\xb1 = {:g}, '.format(step, loss))\n        \n        output = (pred > 0)\n        correct = (output == y.byte())\n        n_correct_train = correct[train_start:train_end].sum().item()\n        n_correct_test = correct[test_start:test_end].sum().item()\n        accuracy_train = n_correct_train / n_total_train\n        accuracy_test = n_correct_test / n_total_test\n        print('\xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87 = {}, \xe6\xb5\x8b\xe8\xaf\x95\xe9\x9b\x86\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87 = {}'.format(accuracy_train, accuracy_test))\n\n\n"""
3.Linear_regression 线性回归/线性模型和梯度下降.py,16,"b""#%% [markdown]\n# # \xe7\xba\xbf\xe6\x80\xa7\xe6\xa8\xa1\xe5\x9e\x8b\xe5\x92\x8c\xe6\xa2\xaf\xe5\xba\xa6\xe4\xb8\x8b\xe9\x99\x8d\n# \xe8\xbf\x99\xe6\x98\xaf\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xe7\x9a\x84\xe7\xac\xac\xe4\xb8\x80\xe8\xaf\xbe\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe4\xbc\x9a\xe5\xad\xa6\xe4\xb9\xa0\xe4\xb8\x80\xe4\xb8\xaa\xe9\x9d\x9e\xe5\xb8\xb8\xe7\xae\x80\xe5\x8d\x95\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\xef\xbc\x8c\xe7\xba\xbf\xe6\x80\xa7\xe5\x9b\x9e\xe5\xbd\x92\xef\xbc\x8c\xe5\x90\x8c\xe6\x97\xb6\xe4\xb9\x9f\xe4\xbc\x9a\xe5\xad\xa6\xe4\xb9\xa0\xe4\xb8\x80\xe4\xb8\xaa\xe4\xbc\x98\xe5\x8c\x96\xe7\xae\x97\xe6\xb3\x95-\xe6\xa2\xaf\xe5\xba\xa6\xe4\xb8\x8b\xe9\x99\x8d\xe6\xb3\x95\xef\xbc\x8c\xe5\xaf\xb9\xe8\xbf\x99\xe4\xb8\xaa\xe6\xa8\xa1\xe5\x9e\x8b\xe8\xbf\x9b\xe8\xa1\x8c\xe4\xbc\x98\xe5\x8c\x96\xe3\x80\x82\xe7\xba\xbf\xe6\x80\xa7\xe5\x9b\x9e\xe5\xbd\x92\xe6\x98\xaf\xe7\x9b\x91\xe7\x9d\xa3\xe5\xad\xa6\xe4\xb9\xa0\xe9\x87\x8c\xe9\x9d\xa2\xe4\xb8\x80\xe4\xb8\xaa\xe9\x9d\x9e\xe5\xb8\xb8\xe7\xae\x80\xe5\x8d\x95\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\xef\xbc\x8c\xe5\x90\x8c\xe6\x97\xb6\xe6\xa2\xaf\xe5\xba\xa6\xe4\xb8\x8b\xe9\x99\x8d\xe4\xb9\x9f\xe6\x98\xaf\xe6\xb7\xb1\xe5\xba\xa6\xe5\xad\xa6\xe4\xb9\xa0\xe4\xb8\xad\xe5\xba\x94\xe7\x94\xa8\xe6\x9c\x80\xe5\xb9\xbf\xe7\x9a\x84\xe4\xbc\x98\xe5\x8c\x96\xe7\xae\x97\xe6\xb3\x95\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe5\xb0\x86\xe4\xbb\x8e\xe8\xbf\x99\xe9\x87\x8c\xe5\xbc\x80\xe5\xa7\x8b\xe6\x88\x91\xe4\xbb\xac\xe7\x9a\x84\xe6\xb7\xb1\xe5\xba\xa6\xe5\xad\xa6\xe4\xb9\xa0\xe4\xb9\x8b\xe6\x97\x85\n#%% [markdown]\n# ## \xe4\xb8\x80\xe5\x85\x83\xe7\xba\xbf\xe6\x80\xa7\xe5\x9b\x9e\xe5\xbd\x92\n# \xe4\xb8\x80\xe5\x85\x83\xe7\xba\xbf\xe6\x80\xa7\xe6\xa8\xa1\xe5\x9e\x8b\xe9\x9d\x9e\xe5\xb8\xb8\xe7\xae\x80\xe5\x8d\x95\xef\xbc\x8c\xe5\x81\x87\xe8\xae\xbe\xe6\x88\x91\xe4\xbb\xac\xe6\x9c\x89\xe5\x8f\x98\xe9\x87\x8f $x_i$ \xe5\x92\x8c\xe7\x9b\xae\xe6\xa0\x87 $y_i$\xef\xbc\x8c\xe6\xaf\x8f\xe4\xb8\xaa i \xe5\xaf\xb9\xe5\xba\x94\xe4\xba\x8e\xe4\xb8\x80\xe4\xb8\xaa\xe6\x95\xb0\xe6\x8d\xae\xe7\x82\xb9\xef\xbc\x8c\xe5\xb8\x8c\xe6\x9c\x9b\xe5\xbb\xba\xe7\xab\x8b\xe4\xb8\x80\xe4\xb8\xaa\xe6\xa8\xa1\xe5\x9e\x8b\n# \n# $$\n# \\hat{y}_i = w x_i + b\n# $$\n# \n# $\\hat{y}_i$ \xe6\x98\xaf\xe6\x88\x91\xe4\xbb\xac\xe9\xa2\x84\xe6\xb5\x8b\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x9c\xef\xbc\x8c\xe5\xb8\x8c\xe6\x9c\x9b\xe9\x80\x9a\xe8\xbf\x87 $\\hat{y}_i$ \xe6\x9d\xa5\xe6\x8b\x9f\xe5\x90\x88\xe7\x9b\xae\xe6\xa0\x87 $y_i$\xef\xbc\x8c\xe9\x80\x9a\xe4\xbf\x97\xe6\x9d\xa5\xe8\xae\xb2\xe5\xb0\xb1\xe6\x98\xaf\xe6\x89\xbe\xe5\x88\xb0\xe8\xbf\x99\xe4\xb8\xaa\xe5\x87\xbd\xe6\x95\xb0\xe6\x8b\x9f\xe5\x90\x88 $y_i$ \xe4\xbd\xbf\xe5\xbe\x97\xe8\xaf\xaf\xe5\xb7\xae\xe6\x9c\x80\xe5\xb0\x8f\xef\xbc\x8c\xe5\x8d\xb3\xe6\x9c\x80\xe5\xb0\x8f\xe5\x8c\x96\n# \n# $$\n# \\frac{1}{n} \\sum_{i=1}^n(\\hat{y}_i - y_i)^2\n# $$\n#%% [markdown]\n# \xe9\x82\xa3\xe4\xb9\x88\xe5\xa6\x82\xe4\xbd\x95\xe6\x9c\x80\xe5\xb0\x8f\xe5\x8c\x96\xe8\xbf\x99\xe4\xb8\xaa\xe8\xaf\xaf\xe5\xb7\xae\xe5\x91\xa2\xef\xbc\x9f\n# \n# \xe8\xbf\x99\xe9\x87\x8c\xe9\x9c\x80\xe8\xa6\x81\xe7\x94\xa8\xe5\x88\xb0**\xe6\xa2\xaf\xe5\xba\xa6\xe4\xb8\x8b\xe9\x99\x8d**\xef\xbc\x8c\xe8\xbf\x99\xe6\x98\xaf\xe6\x88\x91\xe4\xbb\xac\xe6\x8e\xa5\xe8\xa7\xa6\xe5\x88\xb0\xe7\x9a\x84\xe7\xac\xac\xe4\xb8\x80\xe4\xb8\xaa\xe4\xbc\x98\xe5\x8c\x96\xe7\xae\x97\xe6\xb3\x95\xef\xbc\x8c\xe9\x9d\x9e\xe5\xb8\xb8\xe7\xae\x80\xe5\x8d\x95\xef\xbc\x8c\xe4\xbd\x86\xe6\x98\xaf\xe5\x8d\xb4\xe9\x9d\x9e\xe5\xb8\xb8\xe5\xbc\xba\xe5\xa4\xa7\xef\xbc\x8c\xe5\x9c\xa8\xe6\xb7\xb1\xe5\xba\xa6\xe5\xad\xa6\xe4\xb9\xa0\xe4\xb8\xad\xe8\xa2\xab\xe5\xa4\xa7\xe9\x87\x8f\xe4\xbd\xbf\xe7\x94\xa8\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe8\xae\xa9\xe6\x88\x91\xe4\xbb\xac\xe4\xbb\x8e\xe7\xae\x80\xe5\x8d\x95\xe7\x9a\x84\xe4\xbe\x8b\xe5\xad\x90\xe5\x87\xba\xe5\x8f\x91\xe4\xba\x86\xe8\xa7\xa3\xe6\xa2\xaf\xe5\xba\xa6\xe4\xb8\x8b\xe9\x99\x8d\xe6\xb3\x95\xe7\x9a\x84\xe5\x8e\x9f\xe7\x90\x86\n#%% [markdown]\n# ## \xe6\xa2\xaf\xe5\xba\xa6\xe4\xb8\x8b\xe9\x99\x8d\xe6\xb3\x95\n# \xe5\x9c\xa8\xe6\xa2\xaf\xe5\xba\xa6\xe4\xb8\x8b\xe9\x99\x8d\xe6\xb3\x95\xe4\xb8\xad\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe9\xa6\x96\xe5\x85\x88\xe8\xa6\x81\xe6\x98\x8e\xe7\xa1\xae\xe6\xa2\xaf\xe5\xba\xa6\xe7\x9a\x84\xe6\xa6\x82\xe5\xbf\xb5\xef\xbc\x8c\xe9\x9a\x8f\xe5\x90\x8e\xe6\x88\x91\xe4\xbb\xac\xe5\x86\x8d\xe4\xba\x86\xe8\xa7\xa3\xe5\xa6\x82\xe4\xbd\x95\xe4\xbd\xbf\xe7\x94\xa8\xe6\xa2\xaf\xe5\xba\xa6\xe8\xbf\x9b\xe8\xa1\x8c\xe4\xb8\x8b\xe9\x99\x8d\xe3\x80\x82\n#%% [markdown]\n# ### \xe6\xa2\xaf\xe5\xba\xa6\n# \xe6\xa2\xaf\xe5\xba\xa6\xe5\x9c\xa8\xe6\x95\xb0\xe5\xad\xa6\xe4\xb8\x8a\xe5\xb0\xb1\xe6\x98\xaf\xe5\xaf\xbc\xe6\x95\xb0\xef\xbc\x8c\xe5\xa6\x82\xe6\x9e\x9c\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa\xe5\xa4\x9a\xe5\x85\x83\xe5\x87\xbd\xe6\x95\xb0\xef\xbc\x8c\xe9\x82\xa3\xe4\xb9\x88\xe6\xa2\xaf\xe5\xba\xa6\xe5\xb0\xb1\xe6\x98\xaf\xe5\x81\x8f\xe5\xaf\xbc\xe6\x95\xb0\xe3\x80\x82\xe6\xaf\x94\xe5\xa6\x82\xe4\xb8\x80\xe4\xb8\xaa\xe5\x87\xbd\xe6\x95\xb0f(x, y)\xef\xbc\x8c\xe9\x82\xa3\xe4\xb9\x88 f \xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\xe5\xb0\xb1\xe6\x98\xaf \n# \n# $$\n# (\\frac{\\partial f}{\\partial x},\\ \\frac{\\partial f}{\\partial y})\n# $$\n# \n# \xe5\x8f\xaf\xe4\xbb\xa5\xe7\xa7\xb0\xe4\xb8\xba grad f(x, y) \xe6\x88\x96\xe8\x80\x85 $\\nabla f(x, y)$\xe3\x80\x82\xe5\x85\xb7\xe4\xbd\x93\xe6\x9f\x90\xe4\xb8\x80\xe7\x82\xb9 $(x_0,\\ y_0)$ \xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\xe5\xb0\xb1\xe6\x98\xaf $\\nabla f(x_0,\\ y_0)$\xe3\x80\x82\n# \n# \xe4\xb8\x8b\xe9\x9d\xa2\xe8\xbf\x99\xe4\xb8\xaa\xe5\x9b\xbe\xe7\x89\x87\xe6\x98\xaf $f(x) = x^2$ \xe8\xbf\x99\xe4\xb8\xaa\xe5\x87\xbd\xe6\x95\xb0\xe5\x9c\xa8 x=1 \xe5\xa4\x84\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\n# \n# ![](https://ws3.sinaimg.cn/large/006tNc79ly1fmarbuh2j3j30ba0b80sy.jpg)\n#%% [markdown]\n# \xe6\xa2\xaf\xe5\xba\xa6\xe6\x9c\x89\xe4\xbb\x80\xe4\xb9\x88\xe6\x84\x8f\xe4\xb9\x89\xe5\x91\xa2\xef\xbc\x9f\xe4\xbb\x8e\xe5\x87\xa0\xe4\xbd\x95\xe6\x84\x8f\xe4\xb9\x89\xe6\x9d\xa5\xe8\xae\xb2\xef\xbc\x8c\xe4\xb8\x80\xe4\xb8\xaa\xe7\x82\xb9\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\xe5\x80\xbc\xe6\x98\xaf\xe8\xbf\x99\xe4\xb8\xaa\xe5\x87\xbd\xe6\x95\xb0\xe5\x8f\x98\xe5\x8c\x96\xe6\x9c\x80\xe5\xbf\xab\xe7\x9a\x84\xe5\x9c\xb0\xe6\x96\xb9\xef\xbc\x8c\xe5\x85\xb7\xe4\xbd\x93\xe6\x9d\xa5\xe8\xaf\xb4\xef\xbc\x8c\xe5\xaf\xb9\xe4\xba\x8e\xe5\x87\xbd\xe6\x95\xb0 f(x, y)\xef\xbc\x8c\xe5\x9c\xa8\xe7\x82\xb9 $(x_0, y_0)$ \xe5\xa4\x84\xef\xbc\x8c\xe6\xb2\xbf\xe7\x9d\x80\xe6\xa2\xaf\xe5\xba\xa6 $\\nabla f(x_0,\\ y_0)$ \xe7\x9a\x84\xe6\x96\xb9\xe5\x90\x91\xef\xbc\x8c\xe5\x87\xbd\xe6\x95\xb0\xe5\xa2\x9e\xe5\x8a\xa0\xe6\x9c\x80\xe5\xbf\xab\xef\xbc\x8c\xe4\xb9\x9f\xe5\xb0\xb1\xe6\x98\xaf\xe8\xaf\xb4\xe6\xb2\xbf\xe7\x9d\x80\xe6\xa2\xaf\xe5\xba\xa6\xe7\x9a\x84\xe6\x96\xb9\xe5\x90\x91\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe8\x83\xbd\xe5\xa4\x9f\xe6\x9b\xb4\xe5\xbf\xab\xe5\x9c\xb0\xe6\x89\xbe\xe5\x88\xb0\xe5\x87\xbd\xe6\x95\xb0\xe7\x9a\x84\xe6\x9e\x81\xe5\xa4\xa7\xe5\x80\xbc\xe7\x82\xb9\xef\xbc\x8c\xe6\x88\x96\xe8\x80\x85\xe5\x8f\x8d\xe8\xbf\x87\xe6\x9d\xa5\xe6\xb2\xbf\xe7\x9d\x80\xe6\xa2\xaf\xe5\xba\xa6\xe7\x9a\x84\xe5\x8f\x8d\xe6\x96\xb9\xe5\x90\x91\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe8\x83\xbd\xe5\xa4\x9f\xe6\x9b\xb4\xe5\xbf\xab\xe5\x9c\xb0\xe6\x89\xbe\xe5\x88\xb0\xe5\x87\xbd\xe6\x95\xb0\xe7\x9a\x84\xe6\x9c\x80\xe5\xb0\x8f\xe5\x80\xbc\xe7\x82\xb9\xe3\x80\x82\n#%% [markdown]\n# ### \xe6\xa2\xaf\xe5\xba\xa6\xe4\xb8\x8b\xe9\x99\x8d\xe6\xb3\x95\n# \xe6\x9c\x89\xe4\xba\x86\xe5\xaf\xb9\xe6\xa2\xaf\xe5\xba\xa6\xe7\x9a\x84\xe7\x90\x86\xe8\xa7\xa3\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe5\xb0\xb1\xe8\x83\xbd\xe4\xba\x86\xe8\xa7\xa3\xe6\xa2\xaf\xe5\xba\xa6\xe4\xb8\x8b\xe9\x99\x8d\xe5\x8f\x91\xe7\x9a\x84\xe5\x8e\x9f\xe7\x90\x86\xe4\xba\x86\xe3\x80\x82\xe4\xb8\x8a\xe9\x9d\xa2\xe6\x88\x91\xe4\xbb\xac\xe9\x9c\x80\xe8\xa6\x81\xe6\x9c\x80\xe5\xb0\x8f\xe5\x8c\x96\xe8\xbf\x99\xe4\xb8\xaa\xe8\xaf\xaf\xe5\xb7\xae\xef\xbc\x8c\xe4\xb9\x9f\xe5\xb0\xb1\xe6\x98\xaf\xe9\x9c\x80\xe8\xa6\x81\xe6\x89\xbe\xe5\x88\xb0\xe8\xbf\x99\xe4\xb8\xaa\xe8\xaf\xaf\xe5\xb7\xae\xe7\x9a\x84\xe6\x9c\x80\xe5\xb0\x8f\xe5\x80\xbc\xe7\x82\xb9\xef\xbc\x8c\xe9\x82\xa3\xe4\xb9\x88\xe6\xb2\xbf\xe7\x9d\x80\xe6\xa2\xaf\xe5\xba\xa6\xe7\x9a\x84\xe5\x8f\x8d\xe6\x96\xb9\xe5\x90\x91\xe6\x88\x91\xe4\xbb\xac\xe5\xb0\xb1\xe8\x83\xbd\xe5\xa4\x9f\xe6\x89\xbe\xe5\x88\xb0\xe8\xbf\x99\xe4\xb8\xaa\xe6\x9c\x80\xe5\xb0\x8f\xe5\x80\xbc\xe7\x82\xb9\xe3\x80\x82\n# \n# \xe6\x88\x91\xe4\xbb\xac\xe5\x8f\xaf\xe4\xbb\xa5\xe6\x9d\xa5\xe7\x9c\x8b\xe4\xb8\x80\xe4\xb8\xaa\xe7\x9b\xb4\xe8\xa7\x82\xe7\x9a\x84\xe8\xa7\xa3\xe9\x87\x8a\xe3\x80\x82\xe6\xaf\x94\xe5\xa6\x82\xe6\x88\x91\xe4\xbb\xac\xe5\x9c\xa8\xe4\xb8\x80\xe5\xba\xa7\xe5\xa4\xa7\xe5\xb1\xb1\xe4\xb8\x8a\xe7\x9a\x84\xe6\x9f\x90\xe5\xa4\x84\xe4\xbd\x8d\xe7\xbd\xae\xef\xbc\x8c\xe7\x94\xb1\xe4\xba\x8e\xe6\x88\x91\xe4\xbb\xac\xe4\xb8\x8d\xe7\x9f\xa5\xe9\x81\x93\xe6\x80\x8e\xe4\xb9\x88\xe4\xb8\x8b\xe5\xb1\xb1\xef\xbc\x8c\xe4\xba\x8e\xe6\x98\xaf\xe5\x86\xb3\xe5\xae\x9a\xe8\xb5\xb0\xe4\xb8\x80\xe6\xad\xa5\xe7\xae\x97\xe4\xb8\x80\xe6\xad\xa5\xef\xbc\x8c\xe4\xb9\x9f\xe5\xb0\xb1\xe6\x98\xaf\xe5\x9c\xa8\xe6\xaf\x8f\xe8\xb5\xb0\xe5\x88\xb0\xe4\xb8\x80\xe4\xb8\xaa\xe4\xbd\x8d\xe7\xbd\xae\xe7\x9a\x84\xe6\x97\xb6\xe5\x80\x99\xef\xbc\x8c\xe6\xb1\x82\xe8\xa7\xa3\xe5\xbd\x93\xe5\x89\x8d\xe4\xbd\x8d\xe7\xbd\xae\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\xef\xbc\x8c\xe6\xb2\xbf\xe7\x9d\x80\xe6\xa2\xaf\xe5\xba\xa6\xe7\x9a\x84\xe8\xb4\x9f\xe6\x96\xb9\xe5\x90\x91\xef\xbc\x8c\xe4\xb9\x9f\xe5\xb0\xb1\xe6\x98\xaf\xe5\xbd\x93\xe5\x89\x8d\xe6\x9c\x80\xe9\x99\xa1\xe5\xb3\xad\xe7\x9a\x84\xe4\xbd\x8d\xe7\xbd\xae\xe5\x90\x91\xe4\xb8\x8b\xe8\xb5\xb0\xe4\xb8\x80\xe6\xad\xa5\xef\xbc\x8c\xe7\x84\xb6\xe5\x90\x8e\xe7\xbb\xa7\xe7\xbb\xad\xe6\xb1\x82\xe8\xa7\xa3\xe5\xbd\x93\xe5\x89\x8d\xe4\xbd\x8d\xe7\xbd\xae\xe6\xa2\xaf\xe5\xba\xa6\xef\xbc\x8c\xe5\x90\x91\xe8\xbf\x99\xe4\xb8\x80\xe6\xad\xa5\xe6\x89\x80\xe5\x9c\xa8\xe4\xbd\x8d\xe7\xbd\xae\xe6\xb2\xbf\xe7\x9d\x80\xe6\x9c\x80\xe9\x99\xa1\xe5\xb3\xad\xe6\x9c\x80\xe6\x98\x93\xe4\xb8\x8b\xe5\xb1\xb1\xe7\x9a\x84\xe4\xbd\x8d\xe7\xbd\xae\xe8\xb5\xb0\xe4\xb8\x80\xe6\xad\xa5\xe3\x80\x82\xe8\xbf\x99\xe6\xa0\xb7\xe4\xb8\x80\xe6\xad\xa5\xe6\xad\xa5\xe7\x9a\x84\xe8\xb5\xb0\xe4\xb8\x8b\xe5\x8e\xbb\xef\xbc\x8c\xe4\xb8\x80\xe7\x9b\xb4\xe8\xb5\xb0\xe5\x88\xb0\xe8\xa7\x89\xe5\xbe\x97\xe6\x88\x91\xe4\xbb\xac\xe5\xb7\xb2\xe7\xbb\x8f\xe5\x88\xb0\xe4\xba\x86\xe5\xb1\xb1\xe8\x84\x9a\xe3\x80\x82\xe5\xbd\x93\xe7\x84\xb6\xe8\xbf\x99\xe6\xa0\xb7\xe8\xb5\xb0\xe4\xb8\x8b\xe5\x8e\xbb\xef\xbc\x8c\xe6\x9c\x89\xe5\x8f\xaf\xe8\x83\xbd\xe6\x88\x91\xe4\xbb\xac\xe4\xb8\x8d\xe8\x83\xbd\xe8\xb5\xb0\xe5\x88\xb0\xe5\xb1\xb1\xe8\x84\x9a\xef\xbc\x8c\xe8\x80\x8c\xe6\x98\xaf\xe5\x88\xb0\xe4\xba\x86\xe6\x9f\x90\xe4\xb8\x80\xe4\xb8\xaa\xe5\xb1\x80\xe9\x83\xa8\xe7\x9a\x84\xe5\xb1\xb1\xe5\xb3\xb0\xe4\xbd\x8e\xe5\xa4\x84\xe3\x80\x82\n# \n# \xe7\xb1\xbb\xe6\xaf\x94\xe6\x88\x91\xe4\xbb\xac\xe7\x9a\x84\xe9\x97\xae\xe9\xa2\x98\xef\xbc\x8c\xe5\xb0\xb1\xe6\x98\xaf\xe6\xb2\xbf\xe7\x9d\x80\xe6\xa2\xaf\xe5\xba\xa6\xe7\x9a\x84\xe5\x8f\x8d\xe6\x96\xb9\xe5\x90\x91\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe4\xb8\x8d\xe6\x96\xad\xe6\x94\xb9\xe5\x8f\x98 w \xe5\x92\x8c b \xe7\x9a\x84\xe5\x80\xbc\xef\xbc\x8c\xe6\x9c\x80\xe7\xbb\x88\xe6\x89\xbe\xe5\x88\xb0\xe4\xb8\x80\xe7\xbb\x84\xe6\x9c\x80\xe5\xa5\xbd\xe7\x9a\x84 w \xe5\x92\x8c b \xe4\xbd\xbf\xe5\xbe\x97\xe8\xaf\xaf\xe5\xb7\xae\xe6\x9c\x80\xe5\xb0\x8f\xe3\x80\x82\n# \n# \xe5\x9c\xa8\xe6\x9b\xb4\xe6\x96\xb0\xe7\x9a\x84\xe6\x97\xb6\xe5\x80\x99\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe9\x9c\x80\xe8\xa6\x81\xe5\x86\xb3\xe5\xae\x9a\xe6\xaf\x8f\xe6\xac\xa1\xe6\x9b\xb4\xe6\x96\xb0\xe7\x9a\x84\xe5\xb9\x85\xe5\xba\xa6\xef\xbc\x8c\xe6\xaf\x94\xe5\xa6\x82\xe5\x9c\xa8\xe4\xb8\x8b\xe5\xb1\xb1\xe7\x9a\x84\xe4\xbe\x8b\xe5\xad\x90\xe4\xb8\xad\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe9\x9c\x80\xe8\xa6\x81\xe6\xaf\x8f\xe6\xac\xa1\xe5\xbe\x80\xe4\xb8\x8b\xe8\xb5\xb0\xe7\x9a\x84\xe9\x82\xa3\xe4\xb8\x80\xe6\xad\xa5\xe7\x9a\x84\xe9\x95\xbf\xe5\xba\xa6\xef\xbc\x8c\xe8\xbf\x99\xe4\xb8\xaa\xe9\x95\xbf\xe5\xba\xa6\xe7\xa7\xb0\xe4\xb8\xba\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\xef\xbc\x8c\xe7\x94\xa8 $\\eta$ \xe8\xa1\xa8\xe7\xa4\xba\xef\xbc\x8c\xe8\xbf\x99\xe4\xb8\xaa\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\xe9\x9d\x9e\xe5\xb8\xb8\xe9\x87\x8d\xe8\xa6\x81\xef\xbc\x8c\xe4\xb8\x8d\xe5\x90\x8c\xe7\x9a\x84\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\xe9\x83\xbd\xe4\xbc\x9a\xe5\xaf\xbc\xe8\x87\xb4\xe4\xb8\x8d\xe5\x90\x8c\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x9c\xef\xbc\x8c\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\xe5\xa4\xaa\xe5\xb0\x8f\xe4\xbc\x9a\xe5\xaf\xbc\xe8\x87\xb4\xe4\xb8\x8b\xe9\x99\x8d\xe9\x9d\x9e\xe5\xb8\xb8\xe7\xbc\x93\xe6\x85\xa2\xef\xbc\x8c\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\xe5\xa4\xaa\xe5\xa4\xa7\xe5\x8f\x88\xe4\xbc\x9a\xe5\xaf\xbc\xe8\x87\xb4\xe8\xb7\xb3\xe5\x8a\xa8\xe9\x9d\x9e\xe5\xb8\xb8\xe6\x98\x8e\xe6\x98\xbe\xef\xbc\x8c\xe5\x8f\xaf\xe4\xbb\xa5\xe7\x9c\x8b\xe7\x9c\x8b\xe4\xb8\x8b\xe9\x9d\xa2\xe7\x9a\x84\xe4\xbe\x8b\xe5\xad\x90\n# \n# ![](https://ws2.sinaimg.cn/large/006tNc79ly1fmgn23lnzjg30980gogso.gif)\n# \n# \xe5\x8f\xaf\xe4\xbb\xa5\xe7\x9c\x8b\xe5\x88\xb0\xe4\xb8\x8a\xe9\x9d\xa2\xe7\x9a\x84\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\xe8\xbe\x83\xe4\xb8\xba\xe5\x90\x88\xe9\x80\x82\xef\xbc\x8c\xe8\x80\x8c\xe4\xb8\x8b\xe9\x9d\xa2\xe7\x9a\x84\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\xe5\xa4\xaa\xe5\xa4\xa7\xef\xbc\x8c\xe5\xb0\xb1\xe4\xbc\x9a\xe5\xaf\xbc\xe8\x87\xb4\xe4\xb8\x8d\xe6\x96\xad\xe8\xb7\xb3\xe5\x8a\xa8\n# \n# \xe6\x9c\x80\xe5\x90\x8e\xe6\x88\x91\xe4\xbb\xac\xe7\x9a\x84\xe6\x9b\xb4\xe6\x96\xb0\xe5\x85\xac\xe5\xbc\x8f\xe5\xb0\xb1\xe6\x98\xaf\n# \n# $$\n# w := w - \\eta \\frac{\\partial f(w,\\ b)}{\\partial w} \\\\\n# b := b - \\eta \\frac{\\partial f(w,\\ b)}{\\partial b}\n# $$\n# \n# \xe9\x80\x9a\xe8\xbf\x87\xe4\xb8\x8d\xe6\x96\xad\xe5\x9c\xb0\xe8\xbf\xad\xe4\xbb\xa3\xe6\x9b\xb4\xe6\x96\xb0\xef\xbc\x8c\xe6\x9c\x80\xe7\xbb\x88\xe6\x88\x91\xe4\xbb\xac\xe8\x83\xbd\xe5\xa4\x9f\xe6\x89\xbe\xe5\x88\xb0\xe4\xb8\x80\xe7\xbb\x84\xe6\x9c\x80\xe4\xbc\x98\xe7\x9a\x84 w \xe5\x92\x8c b\xef\xbc\x8c\xe8\xbf\x99\xe5\xb0\xb1\xe6\x98\xaf\xe6\xa2\xaf\xe5\xba\xa6\xe4\xb8\x8b\xe9\x99\x8d\xe6\xb3\x95\xe7\x9a\x84\xe5\x8e\x9f\xe7\x90\x86\xe3\x80\x82\n# \n# \xe6\x9c\x80\xe5\x90\x8e\xe5\x8f\xaf\xe4\xbb\xa5\xe9\x80\x9a\xe8\xbf\x87\xe8\xbf\x99\xe5\xbc\xa0\xe5\x9b\xbe\xe5\xbd\xa2\xe8\xb1\xa1\xe5\x9c\xb0\xe8\xaf\xb4\xe6\x98\x8e\xe4\xb8\x80\xe4\xb8\x8b\xe8\xbf\x99\xe4\xb8\xaa\xe6\x96\xb9\xe6\xb3\x95\n# \n# ![](https://ws3.sinaimg.cn/large/006tNc79ly1fmarxsltfqj30gx091gn4.jpg)\n#%% [markdown]\n# \xe4\xb8\x8a\xe9\x9d\xa2\xe6\x98\xaf\xe5\x8e\x9f\xe7\x90\x86\xe9\x83\xa8\xe5\x88\x86\xef\xbc\x8c\xe4\xb8\x8b\xe9\x9d\xa2\xe9\x80\x9a\xe8\xbf\x87\xe4\xb8\x80\xe4\xb8\xaa\xe4\xbe\x8b\xe5\xad\x90\xe6\x9d\xa5\xe8\xbf\x9b\xe4\xb8\x80\xe6\xad\xa5\xe5\xad\xa6\xe4\xb9\xa0\xe7\xba\xbf\xe6\x80\xa7\xe6\xa8\xa1\xe5\x9e\x8b\n\n#%%\nimport torch\nimport numpy as np\n\ntorch.manual_seed(2017)\n\n\n#%%\n# \xe8\xaf\xbb\xe5\x85\xa5\xe6\x95\xb0\xe6\x8d\xae x \xe5\x92\x8c y\nx_train = np.array([[3.3], [4.4], [5.5], [6.71], [6.93], [4.168],\n                    [9.779], [6.182], [7.59], [2.167], [7.042],\n                    [10.791], [5.313], [7.997], [3.1]], dtype=np.float32)\n\ny_train = np.array([[1.7], [2.76], [2.09], [3.19], [1.694], [1.573],\n                    [3.366], [2.596], [2.53], [1.221], [2.827],\n                    [3.465], [1.65], [2.904], [1.3]], dtype=np.float32)\n\n\n#%%\n# \xe7\x94\xbb\xe5\x87\xba\xe5\x9b\xbe\xe5\x83\x8f\nimport matplotlib.pyplot as plt\nget_ipython().run_line_magic('matplotlib', 'inline')\n\nplt.plot(x_train, y_train, 'bo')\n\n\n#%%\n# \xe8\xbd\xac\xe6\x8d\xa2\xe6\x88\x90 Tensor\nx_train = torch.from_numpy(x_train)\ny_train = torch.from_numpy(y_train)\n\n# \xe5\xae\x9a\xe4\xb9\x89\xe5\x8f\x82\xe6\x95\xb0 w \xe5\x92\x8c b\nw = torch.randn(1, requires_grad=True) # \xe9\x9a\x8f\xe6\x9c\xba\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\nb = torch.zeros(1, requires_grad=True) # \xe4\xbd\xbf\xe7\x94\xa8 0 \xe8\xbf\x9b\xe8\xa1\x8c\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\n\n\n#%%\n# \xe6\x9e\x84\xe5\xbb\xba\xe7\xba\xbf\xe6\x80\xa7\xe5\x9b\x9e\xe5\xbd\x92\xe6\xa8\xa1\xe5\x9e\x8b\n\ndef linear_model(x):\n    return x * w + b\n\n\n#%%\ny_ = linear_model(x_train)\n\n#%% [markdown]\n# \xe7\xbb\x8f\xe8\xbf\x87\xe4\xb8\x8a\xe9\x9d\xa2\xe7\x9a\x84\xe6\xad\xa5\xe9\xaa\xa4\xe6\x88\x91\xe4\xbb\xac\xe5\xb0\xb1\xe5\xae\x9a\xe4\xb9\x89\xe5\xa5\xbd\xe4\xba\x86\xe6\xa8\xa1\xe5\x9e\x8b\xef\xbc\x8c\xe5\x9c\xa8\xe8\xbf\x9b\xe8\xa1\x8c\xe5\x8f\x82\xe6\x95\xb0\xe6\x9b\xb4\xe6\x96\xb0\xe4\xb9\x8b\xe5\x89\x8d\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe5\x8f\xaf\xe4\xbb\xa5\xe5\x85\x88\xe7\x9c\x8b\xe7\x9c\x8b\xe6\xa8\xa1\xe5\x9e\x8b\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe7\xbb\x93\xe6\x9e\x9c\xe9\x95\xbf\xe4\xbb\x80\xe4\xb9\x88\xe6\xa0\xb7\n\n#%%\nplt.plot(x_train.detach().numpy(), y_train.detach().numpy(), 'bo', label='real')\nplt.plot(x_train.detach().numpy(), y_.detach().numpy(), 'ro', label='estimated')\nplt.legend()\n\n#%% [markdown]\n# **\xe6\x80\x9d\xe8\x80\x83\xef\xbc\x9a\xe7\xba\xa2\xe8\x89\xb2\xe7\x9a\x84\xe7\x82\xb9\xe8\xa1\xa8\xe7\xa4\xba\xe9\xa2\x84\xe6\xb5\x8b\xe5\x80\xbc\xef\xbc\x8c\xe4\xbc\xbc\xe4\xb9\x8e\xe6\x8e\x92\xe5\x88\x97\xe6\x88\x90\xe4\xb8\x80\xe6\x9d\xa1\xe7\x9b\xb4\xe7\xba\xbf\xef\xbc\x8c\xe8\xaf\xb7\xe6\x80\x9d\xe8\x80\x83\xe4\xb8\x80\xe4\xb8\x8b\xe8\xbf\x99\xe4\xba\x9b\xe7\x82\xb9\xe6\x98\xaf\xe5\x90\xa6\xe5\x9c\xa8\xe4\xb8\x80\xe6\x9d\xa1\xe7\x9b\xb4\xe7\xba\xbf\xe4\xb8\x8a\xef\xbc\x9f**\n#%% [markdown]\n# \xe8\xbf\x99\xe4\xb8\xaa\xe6\x97\xb6\xe5\x80\x99\xe9\x9c\x80\xe8\xa6\x81\xe8\xae\xa1\xe7\xae\x97\xe6\x88\x91\xe4\xbb\xac\xe7\x9a\x84\xe8\xaf\xaf\xe5\xb7\xae\xe5\x87\xbd\xe6\x95\xb0\xef\xbc\x8c\xe4\xb9\x9f\xe5\xb0\xb1\xe6\x98\xaf\n# \n# $$\n# \\frac{1}{n} \\sum_{i=1}^n(\\hat{y}_i - y_i)^2\n# $$\n\n#%%\n# \xe8\xae\xa1\xe7\xae\x97\xe8\xaf\xaf\xe5\xb7\xae\ndef get_loss(y_, y):\n    return torch.mean((y_ - y_train) ** 2)\n\nloss = get_loss(y_, y_train)\n\n\n#%%\n# \xe6\x89\x93\xe5\x8d\xb0\xe4\xb8\x80\xe4\xb8\x8b\xe7\x9c\x8b\xe7\x9c\x8b loss \xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f\nprint(loss)\n\n#%% [markdown]\n# \xe5\xae\x9a\xe4\xb9\x89\xe5\xa5\xbd\xe4\xba\x86\xe8\xaf\xaf\xe5\xb7\xae\xe5\x87\xbd\xe6\x95\xb0\xef\xbc\x8c\xe6\x8e\xa5\xe4\xb8\x8b\xe6\x9d\xa5\xe6\x88\x91\xe4\xbb\xac\xe9\x9c\x80\xe8\xa6\x81\xe8\xae\xa1\xe7\xae\x97 w \xe5\x92\x8c b \xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\xe4\xba\x86\xef\xbc\x8c\xe8\xbf\x99\xe6\x97\xb6\xe5\xbe\x97\xe7\x9b\x8a\xe4\xba\x8e PyTorch \xe7\x9a\x84\xe8\x87\xaa\xe5\x8a\xa8\xe6\xb1\x82\xe5\xaf\xbc\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe4\xb8\x8d\xe9\x9c\x80\xe8\xa6\x81\xe6\x89\x8b\xe5\x8a\xa8\xe5\x8e\xbb\xe7\xae\x97\xe6\xa2\xaf\xe5\xba\xa6\xef\xbc\x8c\xe6\x9c\x89\xe5\x85\xb4\xe8\xb6\xa3\xe7\x9a\x84\xe5\x90\x8c\xe5\xad\xa6\xe5\x8f\xaf\xe4\xbb\xa5\xe6\x89\x8b\xe5\x8a\xa8\xe8\xae\xa1\xe7\xae\x97\xe4\xb8\x80\xe4\xb8\x8b\xef\xbc\x8cw \xe5\x92\x8c b \xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\xe5\x88\x86\xe5\x88\xab\xe6\x98\xaf\n# \n# $$\n# \\frac{\\partial}{\\partial w} = \\frac{2}{n} \\sum_{i=1}^n x_i(w x_i + b - y_i) \\\\\n# \\frac{\\partial}{\\partial b} = \\frac{2}{n} \\sum_{i=1}^n (w x_i + b - y_i)\n# $$\n\n#%%\n# \xe8\x87\xaa\xe5\x8a\xa8\xe6\xb1\x82\xe5\xaf\xbc\nloss.backward()\n\n\n#%%\n# \xe6\x9f\xa5\xe7\x9c\x8b w \xe5\x92\x8c b \xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\nprint(w.grad)\nprint(b.grad)\n\n\n#%%\n# \xe6\x9b\xb4\xe6\x96\xb0\xe4\xb8\x80\xe6\xac\xa1\xe5\x8f\x82\xe6\x95\xb0\n\nwith torch.no_grad():\n    w -= 1e-2 * w.grad\n    b -= 1e-2 * b.grad\n\n#%% [markdown]\n# \xe6\x9b\xb4\xe6\x96\xb0\xe5\xae\x8c\xe6\x88\x90\xe5\x8f\x82\xe6\x95\xb0\xe4\xb9\x8b\xe5\x90\x8e\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe5\x86\x8d\xe4\xb8\x80\xe6\xac\xa1\xe7\x9c\x8b\xe7\x9c\x8b\xe6\xa8\xa1\xe5\x9e\x8b\xe8\xbe\x93\xe5\x87\xba\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x9c\n\n#%%\ny_ = linear_model(x_train)\nplt.plot(x_train.detach().numpy(), y_train.detach().numpy(), 'bo', label='real')\nplt.plot(x_train.detach().numpy(), y_.detach().numpy(), 'ro', label='estimated')\nplt.legend()\n\n#%% [markdown]\n# \xe4\xbb\x8e\xe4\xb8\x8a\xe9\x9d\xa2\xe7\x9a\x84\xe4\xbe\x8b\xe5\xad\x90\xe5\x8f\xaf\xe4\xbb\xa5\xe7\x9c\x8b\xe5\x88\xb0\xef\xbc\x8c\xe6\x9b\xb4\xe6\x96\xb0\xe4\xb9\x8b\xe5\x90\x8e\xe7\xba\xa2\xe8\x89\xb2\xe7\x9a\x84\xe7\xba\xbf\xe8\xb7\x91\xe5\x88\xb0\xe4\xba\x86\xe8\x93\x9d\xe8\x89\xb2\xe7\x9a\x84\xe7\xba\xbf\xe4\xb8\x8b\xe9\x9d\xa2\xef\xbc\x8c\xe6\xb2\xa1\xe6\x9c\x89\xe7\x89\xb9\xe5\x88\xab\xe5\xa5\xbd\xe7\x9a\x84\xe6\x8b\x9f\xe5\x90\x88\xe8\x93\x9d\xe8\x89\xb2\xe7\x9a\x84\xe7\x9c\x9f\xe5\xae\x9e\xe5\x80\xbc\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe6\x88\x91\xe4\xbb\xac\xe9\x9c\x80\xe8\xa6\x81\xe5\x9c\xa8\xe8\xbf\x9b\xe8\xa1\x8c\xe5\x87\xa0\xe6\xac\xa1\xe6\x9b\xb4\xe6\x96\xb0\n\n#%%\nfor e in range(10): # \xe8\xbf\x9b\xe8\xa1\x8c 10 \xe6\xac\xa1\xe6\x9b\xb4\xe6\x96\xb0\n    y_ = linear_model(x_train)\n    loss = get_loss(y_, y_train)\n    \n    w.grad.zero_() # \xe8\xae\xb0\xe5\xbe\x97\xe5\xbd\x92\xe9\x9b\xb6\xe6\xa2\xaf\xe5\xba\xa6\n    b.grad.zero_() # \xe8\xae\xb0\xe5\xbe\x97\xe5\xbd\x92\xe9\x9b\xb6\xe6\xa2\xaf\xe5\xba\xa6\n    loss.backward()\n    \n    with torch.no_grad():\n        w -= 1e-2 * w.grad\n        b -= 1e-2 * b.grad\n    \n    print('epoch: {}, loss: {}'.format(e, loss.item()))\n\n\n#%%\ny_ = linear_model(x_train)\nplt.plot(x_train.detach().numpy(), y_train.detach().numpy(), 'bo', label='real')\nplt.plot(x_train.detach().numpy(), y_.detach().numpy(), 'ro', label='estimated')\nplt.legend()\n\n#%% [markdown]\n# \xe7\xbb\x8f\xe8\xbf\x87 10 \xe6\xac\xa1\xe6\x9b\xb4\xe6\x96\xb0\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe5\x8f\x91\xe7\x8e\xb0\xe7\xba\xa2\xe8\x89\xb2\xe7\x9a\x84\xe9\xa2\x84\xe6\xb5\x8b\xe7\xbb\x93\xe6\x9e\x9c\xe5\xb7\xb2\xe7\xbb\x8f\xe6\xaf\x94\xe8\xbe\x83\xe5\xa5\xbd\xe7\x9a\x84\xe6\x8b\x9f\xe5\x90\x88\xe4\xba\x86\xe8\x93\x9d\xe8\x89\xb2\xe7\x9a\x84\xe7\x9c\x9f\xe5\xae\x9e\xe5\x80\xbc\xe3\x80\x82\n# \n# \xe7\x8e\xb0\xe5\x9c\xa8\xe4\xbd\xa0\xe5\xb7\xb2\xe7\xbb\x8f\xe5\xad\xa6\xe4\xbc\x9a\xe4\xba\x86\xe4\xbd\xa0\xe7\x9a\x84\xe7\xac\xac\xe4\xb8\x80\xe4\xb8\xaa\xe6\x9c\xba\xe5\x99\xa8\xe5\xad\xa6\xe4\xb9\xa0\xe6\xa8\xa1\xe5\x9e\x8b\xe4\xba\x86\xef\xbc\x8c\xe5\x86\x8d\xe6\x8e\xa5\xe5\x86\x8d\xe5\x8e\x89\xef\xbc\x8c\xe5\xae\x8c\xe6\x88\x90\xe4\xb8\x8b\xe9\x9d\xa2\xe7\x9a\x84\xe5\xb0\x8f\xe7\xbb\x83\xe4\xb9\xa0\xe3\x80\x82\n#%% [markdown]\n# **\xe5\xb0\x8f\xe7\xbb\x83\xe4\xb9\xa0\xef\xbc\x9a**\n# \n# \xe9\x87\x8d\xe5\x90\xaf notebook \xe8\xbf\x90\xe8\xa1\x8c\xe4\xb8\x8a\xe9\x9d\xa2\xe7\x9a\x84\xe7\xba\xbf\xe6\x80\xa7\xe5\x9b\x9e\xe5\xbd\x92\xe6\xa8\xa1\xe5\x9e\x8b\xef\xbc\x8c\xe4\xbd\x86\xe6\x98\xaf\xe6\x94\xb9\xe5\x8f\x98\xe8\xae\xad\xe7\xbb\x83\xe6\xac\xa1\xe6\x95\xb0\xe4\xbb\xa5\xe5\x8f\x8a\xe4\xb8\x8d\xe5\x90\x8c\xe7\x9a\x84\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\xe8\xbf\x9b\xe8\xa1\x8c\xe5\xb0\x9d\xe8\xaf\x95\xe5\xbe\x97\xe5\x88\xb0\xe4\xb8\x8d\xe5\x90\x8c\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x9c\n#%% [markdown]\n# ## \xe5\xa4\x9a\xe9\xa1\xb9\xe5\xbc\x8f\xe5\x9b\x9e\xe5\xbd\x92\xe6\xa8\xa1\xe5\x9e\x8b\n#%% [markdown]\n# \xe4\xb8\x8b\xe9\x9d\xa2\xe6\x88\x91\xe4\xbb\xac\xe6\x9b\xb4\xe8\xbf\x9b\xe4\xb8\x80\xe6\xad\xa5\xef\xbc\x8c\xe8\xae\xb2\xe4\xb8\x80\xe8\xae\xb2\xe5\xa4\x9a\xe9\xa1\xb9\xe5\xbc\x8f\xe5\x9b\x9e\xe5\xbd\x92\xe3\x80\x82\xe4\xbb\x80\xe4\xb9\x88\xe6\x98\xaf\xe5\xa4\x9a\xe9\xa1\xb9\xe5\xbc\x8f\xe5\x9b\x9e\xe5\xbd\x92\xe5\x91\xa2\xef\xbc\x9f\xe9\x9d\x9e\xe5\xb8\xb8\xe7\xae\x80\xe5\x8d\x95\xef\xbc\x8c\xe6\xa0\xb9\xe6\x8d\xae\xe4\xb8\x8a\xe9\x9d\xa2\xe7\x9a\x84\xe7\xba\xbf\xe6\x80\xa7\xe5\x9b\x9e\xe5\xbd\x92\xe6\xa8\xa1\xe5\x9e\x8b\n# \n# $$\n# \\hat{y} = w x + b\n# $$\n# \n# \xe8\xbf\x99\xe9\x87\x8c\xe6\x98\xaf\xe5\x85\xb3\xe4\xba\x8e x \xe7\x9a\x84\xe4\xb8\x80\xe4\xb8\xaa\xe4\xb8\x80\xe6\xac\xa1\xe5\xa4\x9a\xe9\xa1\xb9\xe5\xbc\x8f\xef\xbc\x8c\xe8\xbf\x99\xe4\xb8\xaa\xe6\xa8\xa1\xe5\x9e\x8b\xe6\xaf\x94\xe8\xbe\x83\xe7\xae\x80\xe5\x8d\x95\xef\xbc\x8c\xe6\xb2\xa1\xe6\x9c\x89\xe5\x8a\x9e\xe6\xb3\x95\xe6\x8b\x9f\xe5\x90\x88\xe6\xaf\x94\xe8\xbe\x83\xe5\xa4\x8d\xe6\x9d\x82\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe6\x88\x91\xe4\xbb\xac\xe5\x8f\xaf\xe4\xbb\xa5\xe4\xbd\xbf\xe7\x94\xa8\xe6\x9b\xb4\xe9\xab\x98\xe6\xac\xa1\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\xef\xbc\x8c\xe6\xaf\x94\xe5\xa6\x82\n# \n# $$\n# \\hat{y} = w_0 + w_1 x + w_2 x^2 + w_3 x^3 + \\cdots\n# $$\n# \n# \xe8\xbf\x99\xe6\xa0\xb7\xe5\xb0\xb1\xe8\x83\xbd\xe5\xa4\x9f\xe6\x8b\x9f\xe5\x90\x88\xe6\x9b\xb4\xe5\x8a\xa0\xe5\xa4\x8d\xe6\x9d\x82\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\xef\xbc\x8c\xe8\xbf\x99\xe5\xb0\xb1\xe6\x98\xaf\xe5\xa4\x9a\xe9\xa1\xb9\xe5\xbc\x8f\xe6\xa8\xa1\xe5\x9e\x8b\xef\xbc\x8c\xe8\xbf\x99\xe9\x87\x8c\xe4\xbd\xbf\xe7\x94\xa8\xe4\xba\x86 x \xe7\x9a\x84\xe6\x9b\xb4\xe9\xab\x98\xe6\xac\xa1\xef\xbc\x8c\xe5\x90\x8c\xe7\x90\x86\xe8\xbf\x98\xe6\x9c\x89\xe5\xa4\x9a\xe5\x85\x83\xe5\x9b\x9e\xe5\xbd\x92\xe6\xa8\xa1\xe5\x9e\x8b\xef\xbc\x8c\xe5\xbd\xa2\xe5\xbc\x8f\xe4\xb9\x9f\xe6\x98\xaf\xe4\xb8\x80\xe6\xa0\xb7\xe7\x9a\x84\xef\xbc\x8c\xe5\x8f\xaa\xe6\x98\xaf\xe5\x87\xba\xe4\xba\x86\xe4\xbd\xbf\xe7\x94\xa8 x\xef\xbc\x8c\xe8\xbf\x98\xe6\x98\xaf\xe6\x9b\xb4\xe5\xa4\x9a\xe7\x9a\x84\xe5\x8f\x98\xe9\x87\x8f\xef\xbc\x8c\xe6\xaf\x94\xe5\xa6\x82 y\xe3\x80\x81z \xe7\xad\x89\xe7\xad\x89\xef\xbc\x8c\xe5\x90\x8c\xe6\x97\xb6\xe4\xbb\x96\xe4\xbb\xac\xe7\x9a\x84 loss \xe5\x87\xbd\xe6\x95\xb0\xe5\x92\x8c\xe7\xae\x80\xe5\x8d\x95\xe7\x9a\x84\xe7\xba\xbf\xe6\x80\xa7\xe5\x9b\x9e\xe5\xbd\x92\xe6\xa8\xa1\xe5\x9e\x8b\xe6\x98\xaf\xe4\xb8\x80\xe8\x87\xb4\xe7\x9a\x84\xe3\x80\x82\n#%% [markdown]\n# \xe9\xa6\x96\xe5\x85\x88\xe6\x88\x91\xe4\xbb\xac\xe5\x8f\xaf\xe4\xbb\xa5\xe5\x85\x88\xe5\xae\x9a\xe4\xb9\x89\xe4\xb8\x80\xe4\xb8\xaa\xe9\x9c\x80\xe8\xa6\x81\xe6\x8b\x9f\xe5\x90\x88\xe7\x9a\x84\xe7\x9b\xae\xe6\xa0\x87\xe5\x87\xbd\xe6\x95\xb0\xef\xbc\x8c\xe8\xbf\x99\xe4\xb8\xaa\xe5\x87\xbd\xe6\x95\xb0\xe6\x98\xaf\xe4\xb8\xaa\xe4\xb8\x89\xe6\xac\xa1\xe7\x9a\x84\xe5\xa4\x9a\xe9\xa1\xb9\xe5\xbc\x8f\n\n#%%\n# \xe5\xae\x9a\xe4\xb9\x89\xe4\xb8\x80\xe4\xb8\xaa\xe5\xa4\x9a\xe5\x8f\x98\xe9\x87\x8f\xe5\x87\xbd\xe6\x95\xb0\n\nw_target = np.array([0.5, 3, 2.4]) # \xe5\xae\x9a\xe4\xb9\x89\xe5\x8f\x82\xe6\x95\xb0\nb_target = np.array([0.9]) # \xe5\xae\x9a\xe4\xb9\x89\xe5\x8f\x82\xe6\x95\xb0\n\nf_des = 'y = {:.2f} + {:.2f} * x + {:.2f} * x^2 + {:.2f} * x^3'.format(\n    b_target[0], w_target[0], w_target[1], w_target[2]) # \xe6\x89\x93\xe5\x8d\xb0\xe5\x87\xba\xe5\x87\xbd\xe6\x95\xb0\xe7\x9a\x84\xe5\xbc\x8f\xe5\xad\x90\n\nprint(f_des)\n\n#%% [markdown]\n# \xe6\x88\x91\xe4\xbb\xac\xe5\x8f\xaf\xe4\xbb\xa5\xe5\x85\x88\xe7\x94\xbb\xe5\x87\xba\xe8\xbf\x99\xe4\xb8\xaa\xe5\xa4\x9a\xe9\xa1\xb9\xe5\xbc\x8f\xe7\x9a\x84\xe5\x9b\xbe\xe5\x83\x8f\n\n#%%\n# \xe7\x94\xbb\xe5\x87\xba\xe8\xbf\x99\xe4\xb8\xaa\xe5\x87\xbd\xe6\x95\xb0\xe7\x9a\x84\xe6\x9b\xb2\xe7\xba\xbf\nx_sample = np.arange(-3, 3.1, 0.1)\ny_sample = b_target[0] + w_target[0] * x_sample + w_target[1] * x_sample ** 2 + w_target[2] * x_sample ** 3\n\nplt.plot(x_sample, y_sample, label='real curve')\nplt.legend()\n\n#%% [markdown]\n# \xe6\x8e\xa5\xe7\x9d\x80\xe6\x88\x91\xe4\xbb\xac\xe5\x8f\xaf\xe4\xbb\xa5\xe6\x9e\x84\xe5\xbb\xba\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xef\xbc\x8c\xe9\x9c\x80\xe8\xa6\x81 x \xe5\x92\x8c y\xef\xbc\x8c\xe5\x90\x8c\xe6\x97\xb6\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa\xe4\xb8\x89\xe6\xac\xa1\xe5\xa4\x9a\xe9\xa1\xb9\xe5\xbc\x8f\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe6\x88\x91\xe4\xbb\xac\xe5\x8f\x96\xe4\xba\x86 $x,\\ x^2, x^3$\n\n#%%\n# \xe6\x9e\x84\xe5\xbb\xba\xe6\x95\xb0\xe6\x8d\xae x \xe5\x92\x8c y\n# x \xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa\xe5\xa6\x82\xe4\xb8\x8b\xe7\x9f\xa9\xe9\x98\xb5 [x, x^2, x^3]\n# y \xe6\x98\xaf\xe5\x87\xbd\xe6\x95\xb0\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x9c [y]\n\nx_train = np.stack([x_sample ** i for i in range(1, 4)], axis=1)\nx_train = torch.from_numpy(x_train).float() # \xe8\xbd\xac\xe6\x8d\xa2\xe6\x88\x90 float tensor\n\ny_train = torch.from_numpy(y_sample).float().unsqueeze(1) # \xe8\xbd\xac\xe5\x8c\x96\xe6\x88\x90 float tensor \n\n#%% [markdown]\n# \xe6\x8e\xa5\xe7\x9d\x80\xe6\x88\x91\xe4\xbb\xac\xe5\x8f\xaf\xe4\xbb\xa5\xe5\xae\x9a\xe4\xb9\x89\xe9\x9c\x80\xe8\xa6\x81\xe4\xbc\x98\xe5\x8c\x96\xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0\xef\xbc\x8c\xe5\xb0\xb1\xe6\x98\xaf\xe5\x89\x8d\xe9\x9d\xa2\xe8\xbf\x99\xe4\xb8\xaa\xe5\x87\xbd\xe6\x95\xb0\xe9\x87\x8c\xe9\x9d\xa2\xe7\x9a\x84 $w_i$\n\n#%%\n# \xe5\xae\x9a\xe4\xb9\x89\xe5\x8f\x82\xe6\x95\xb0\xe5\x92\x8c\xe6\xa8\xa1\xe5\x9e\x8b\nw = torch.randn((3, 1), requires_grad=True)\nb = torch.zeros((1), requires_grad=True)\n\ndef multi_linear(x):\n    return torch.mm(x, w) + b\n\n#%% [markdown]\n# \xe6\x88\x91\xe4\xbb\xac\xe5\x8f\xaf\xe4\xbb\xa5\xe7\x94\xbb\xe5\x87\xba\xe6\xb2\xa1\xe6\x9c\x89\xe6\x9b\xb4\xe6\x96\xb0\xe4\xb9\x8b\xe5\x89\x8d\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\xe5\x92\x8c\xe7\x9c\x9f\xe5\xae\x9e\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\xe4\xb9\x8b\xe9\x97\xb4\xe7\x9a\x84\xe5\xaf\xb9\xe6\xaf\x94\n\n#%%\n# \xe7\x94\xbb\xe5\x87\xba\xe6\x9b\xb4\xe6\x96\xb0\xe4\xb9\x8b\xe5\x89\x8d\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\ny_pred = multi_linear(x_train)\n\nplt.plot(x_train.detach().numpy()[:, 0], y_pred.detach().numpy(), label='fitting curve', color='r')\nplt.plot(x_train.detach().numpy()[:, 0], y_sample, label='real curve', color='b')\nplt.legend()\n\n#%% [markdown]\n# \xe5\x8f\xaf\xe4\xbb\xa5\xe5\x8f\x91\xe7\x8e\xb0\xef\xbc\x8c\xe8\xbf\x99\xe4\xb8\xa4\xe6\x9d\xa1\xe6\x9b\xb2\xe7\xba\xbf\xe4\xb9\x8b\xe9\x97\xb4\xe5\xad\x98\xe5\x9c\xa8\xe5\xb7\xae\xe5\xbc\x82\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe8\xae\xa1\xe7\xae\x97\xe4\xb8\x80\xe4\xb8\x8b\xe4\xbb\x96\xe4\xbb\xac\xe4\xb9\x8b\xe9\x97\xb4\xe7\x9a\x84\xe8\xaf\xaf\xe5\xb7\xae\n\n#%%\n# \xe8\xae\xa1\xe7\xae\x97\xe8\xaf\xaf\xe5\xb7\xae\xef\xbc\x8c\xe8\xbf\x99\xe9\x87\x8c\xe7\x9a\x84\xe8\xaf\xaf\xe5\xb7\xae\xe5\x92\x8c\xe4\xb8\x80\xe5\x85\x83\xe7\x9a\x84\xe7\xba\xbf\xe6\x80\xa7\xe6\xa8\xa1\xe5\x9e\x8b\xe7\x9a\x84\xe8\xaf\xaf\xe5\xb7\xae\xe6\x98\xaf\xe7\x9b\xb8\xe5\x90\x8c\xe7\x9a\x84\xef\xbc\x8c\xe5\x89\x8d\xe9\x9d\xa2\xe5\xb7\xb2\xe7\xbb\x8f\xe5\xae\x9a\xe4\xb9\x89\xe8\xbf\x87\xe4\xba\x86 get_loss\nloss = get_loss(y_pred, y_train)\nprint(loss)\n\n\n#%%\n# \xe8\x87\xaa\xe5\x8a\xa8\xe6\xb1\x82\xe5\xaf\xbc\nloss.backward()\n\n\n#%%\n# \xe6\x9f\xa5\xe7\x9c\x8b\xe4\xb8\x80\xe4\xb8\x8b w \xe5\x92\x8c b \xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\nprint(w.grad)\nprint(b.grad)\n\n\n#%%\n# \xe6\x9b\xb4\xe6\x96\xb0\xe4\xb8\x80\xe4\xb8\x8b\xe5\x8f\x82\xe6\x95\xb0\nwith torch.no_grad():\n    w -= 0.001 * w.grad\n    b -= 0.001 * b.grad\n\n\n#%%\n# \xe7\x94\xbb\xe5\x87\xba\xe6\x9b\xb4\xe6\x96\xb0\xe4\xb8\x80\xe6\xac\xa1\xe4\xb9\x8b\xe5\x90\x8e\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\ny_pred = multi_linear(x_train)\n\nplt.plot(x_train.detach().numpy()[:, 0], y_pred.detach().numpy(), label='fitting curve', color='r')\nplt.plot(x_train.detach().numpy()[:, 0], y_sample, label='real curve', color='b')\nplt.legend()\n\n#%% [markdown]\n# \xe5\x9b\xa0\xe4\xb8\xba\xe5\x8f\xaa\xe6\x9b\xb4\xe6\x96\xb0\xe4\xba\x86\xe4\xb8\x80\xe6\xac\xa1\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe4\xb8\xa4\xe6\x9d\xa1\xe6\x9b\xb2\xe7\xba\xbf\xe4\xb9\x8b\xe9\x97\xb4\xe7\x9a\x84\xe5\xb7\xae\xe5\xbc\x82\xe4\xbb\x8d\xe7\x84\xb6\xe5\xad\x98\xe5\x9c\xa8\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe8\xbf\x9b\xe8\xa1\x8c 100 \xe6\xac\xa1\xe8\xbf\xad\xe4\xbb\xa3\n\n#%%\n# \xe8\xbf\x9b\xe8\xa1\x8c 100 \xe6\xac\xa1\xe5\x8f\x82\xe6\x95\xb0\xe6\x9b\xb4\xe6\x96\xb0\nfor e in range(100):\n    y_pred = multi_linear(x_train)\n    loss = get_loss(y_pred, y_train)\n    \n    w.grad.zero_()\n    b.grad.zero_()\n    loss.backward()\n    # \xe6\x9b\xb4\xe6\x96\xb0\xe5\x8f\x82\xe6\x95\xb0\n    with torch.no_grad():\n        w -= 0.001 * w.grad\n        b -= 0.001 * b.grad  \n\n    if (e + 1) % 20 == 0:\n        print('epoch {}, Loss: {:.5f}'.format(e+1, loss.item()))\n\n#%% [markdown]\n# \xe5\x8f\xaf\xe4\xbb\xa5\xe7\x9c\x8b\xe5\x88\xb0\xe6\x9b\xb4\xe6\x96\xb0\xe5\xae\x8c\xe6\x88\x90\xe4\xb9\x8b\xe5\x90\x8e loss \xe5\xb7\xb2\xe7\xbb\x8f\xe9\x9d\x9e\xe5\xb8\xb8\xe5\xb0\x8f\xe4\xba\x86\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe7\x94\xbb\xe5\x87\xba\xe6\x9b\xb4\xe6\x96\xb0\xe4\xb9\x8b\xe5\x90\x8e\xe7\x9a\x84\xe6\x9b\xb2\xe7\xba\xbf\xe5\xaf\xb9\xe6\xaf\x94\n\n#%%\n# \xe7\x94\xbb\xe5\x87\xba\xe6\x9b\xb4\xe6\x96\xb0\xe4\xb9\x8b\xe5\x90\x8e\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x9c\ny_pred = multi_linear(x_train)\n\nplt.plot(x_train.detach().numpy()[:, 0], y_pred.detach().numpy(), label='fitting curve', color='r')\nplt.plot(x_train.detach().numpy()[:, 0], y_sample, label='real curve', color='b')\nplt.legend()\n\n#%% [markdown]\n# \xe5\x8f\xaf\xe4\xbb\xa5\xe7\x9c\x8b\xe5\x88\xb0\xef\xbc\x8c\xe7\xbb\x8f\xe8\xbf\x87 100 \xe6\xac\xa1\xe6\x9b\xb4\xe6\x96\xb0\xe4\xb9\x8b\xe5\x90\x8e\xef\xbc\x8c\xe5\x8f\xaf\xe4\xbb\xa5\xe7\x9c\x8b\xe5\x88\xb0\xe6\x8b\x9f\xe5\x90\x88\xe7\x9a\x84\xe7\xba\xbf\xe5\x92\x8c\xe7\x9c\x9f\xe5\xae\x9e\xe7\x9a\x84\xe7\xba\xbf\xe5\xb7\xb2\xe7\xbb\x8f\xe5\xae\x8c\xe5\x85\xa8\xe9\x87\x8d\xe5\x90\x88\xe4\xba\x86\n#%% [markdown]\n# **\xe5\xb0\x8f\xe7\xbb\x83\xe4\xb9\xa0\xef\xbc\x9a\xe4\xb8\x8a\xe9\x9d\xa2\xe7\x9a\x84\xe4\xbe\x8b\xe5\xad\x90\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa\xe4\xb8\x89\xe6\xac\xa1\xe7\x9a\x84\xe5\xa4\x9a\xe9\xa1\xb9\xe5\xbc\x8f\xef\xbc\x8c\xe5\xb0\x9d\xe8\xaf\x95\xe4\xbd\xbf\xe7\x94\xa8\xe4\xba\x8c\xe6\xac\xa1\xe7\x9a\x84\xe5\xa4\x9a\xe9\xa1\xb9\xe5\xbc\x8f\xe5\x8e\xbb\xe6\x8b\x9f\xe5\x90\x88\xe5\xae\x83\xef\xbc\x8c\xe7\x9c\x8b\xe7\x9c\x8b\xe6\x9c\x80\xe5\x90\x8e\xe8\x83\xbd\xe5\x81\x9a\xe5\x88\xb0\xe5\xa4\x9a\xe5\xa5\xbd**\n# \n# **\xe6\x8f\x90\xe7\xa4\xba\xef\xbc\x9a\xe5\x8f\x82\xe6\x95\xb0 `w = torch.randn(2, 1)`\xef\xbc\x8c\xe5\x90\x8c\xe6\x97\xb6\xe9\x87\x8d\xe6\x96\xb0\xe6\x9e\x84\xe5\xbb\xba x \xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86**\n\n"""
4.Logistic_regression Logistic回归/Logistic 回归模型.py,26,"b'#%% [markdown]\n# # Logistic \xe5\x9b\x9e\xe5\xbd\x92\xe6\xa8\xa1\xe5\x9e\x8b\n#%% [markdown]\n# Logistic \xe5\x9b\x9e\xe5\xbd\x92\xe6\x98\xaf\xe4\xb8\x80\xe7\xa7\x8d\xe5\xb9\xbf\xe4\xb9\x89\xe7\x9a\x84\xe5\x9b\x9e\xe5\xbd\x92\xe6\xa8\xa1\xe5\x9e\x8b\xef\xbc\x8c\xe5\x85\xb6\xe4\xb8\x8e\xe5\xa4\x9a\xe5\x85\x83\xe7\xba\xbf\xe6\x80\xa7\xe5\x9b\x9e\xe5\xbd\x92\xe6\x9c\x89\xe7\x9d\x80\xe5\xbe\x88\xe5\xa4\x9a\xe7\x9b\xb8\xe4\xbc\xbc\xe4\xb9\x8b\xe5\xa4\x84\xef\xbc\x8c\xe6\xa8\xa1\xe5\x9e\x8b\xe7\x9a\x84\xe5\xbd\xa2\xe5\xbc\x8f\xe5\x9f\xba\xe6\x9c\xac\xe7\x9b\xb8\xe5\x90\x8c\xef\xbc\x8c\xe8\x99\xbd\xe7\x84\xb6\xe4\xb9\x9f\xe8\xa2\xab\xe7\xa7\xb0\xe4\xb8\xba\xe5\x9b\x9e\xe5\xbd\x92\xef\xbc\x8c\xe4\xbd\x86\xe6\x98\xaf\xe5\x85\xb6\xe6\x9b\xb4\xe5\xa4\x9a\xe7\x9a\x84\xe6\x83\x85\xe5\x86\xb5\xe4\xbd\xbf\xe7\x94\xa8\xe5\x9c\xa8\xe5\x88\x86\xe7\xb1\xbb\xe9\x97\xae\xe9\xa2\x98\xe4\xb8\x8a\xef\xbc\x8c\xe5\x90\x8c\xe6\x97\xb6\xe5\x8f\x88\xe4\xbb\xa5\xe4\xba\x8c\xe5\x88\x86\xe7\xb1\xbb\xe6\x9b\xb4\xe4\xb8\xba\xe5\xb8\xb8\xe7\x94\xa8\xe3\x80\x82\n#%% [markdown]\n# ## \xe6\xa8\xa1\xe5\x9e\x8b\xe5\xbd\xa2\xe5\xbc\x8f\n# Logistic \xe5\x9b\x9e\xe5\xbd\x92\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\xe5\xbd\xa2\xe5\xbc\x8f\xe5\x92\x8c\xe7\xba\xbf\xe6\x80\xa7\xe5\x9b\x9e\xe5\xbd\x92\xe4\xb8\x80\xe6\xa0\xb7\xef\xbc\x8c\xe9\x83\xbd\xe6\x98\xaf y = wx + b\xef\xbc\x8c\xe5\x85\xb6\xe4\xb8\xad x \xe5\x8f\xaf\xe4\xbb\xa5\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa\xe5\xa4\x9a\xe7\xbb\xb4\xe7\x9a\x84\xe7\x89\xb9\xe5\xbe\x81\xef\xbc\x8c\xe5\x94\xaf\xe4\xb8\x80\xe4\xb8\x8d\xe5\x90\x8c\xe7\x9a\x84\xe5\x9c\xb0\xe6\x96\xb9\xe5\x9c\xa8\xe4\xba\x8e Logistic \xe5\x9b\x9e\xe5\xbd\x92\xe4\xbc\x9a\xe5\xaf\xb9 y \xe4\xbd\x9c\xe7\x94\xa8\xe4\xb8\x80\xe4\xb8\xaa logistic \xe5\x87\xbd\xe6\x95\xb0\xef\xbc\x8c\xe5\xb0\x86\xe5\x85\xb6\xe5\x8f\x98\xe4\xb8\xba\xe4\xb8\x80\xe7\xa7\x8d\xe6\xa6\x82\xe7\x8e\x87\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x9c\xe3\x80\x82 Logistic \xe5\x87\xbd\xe6\x95\xb0\xe4\xbd\x9c\xe4\xb8\xba Logistic \xe5\x9b\x9e\xe5\xbd\x92\xe7\x9a\x84\xe6\xa0\xb8\xe5\xbf\x83\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe4\xb8\x8b\xe9\x9d\xa2\xe8\xae\xb2\xe4\xb8\x80\xe8\xae\xb2 Logistic \xe5\x87\xbd\xe6\x95\xb0\xef\xbc\x8c\xe4\xb9\x9f\xe8\xa2\xab\xe7\xa7\xb0\xe4\xb8\xba Sigmoid \xe5\x87\xbd\xe6\x95\xb0\xe3\x80\x82\n#%% [markdown]\n# ### Sigmoid \xe5\x87\xbd\xe6\x95\xb0\n# Sigmoid \xe5\x87\xbd\xe6\x95\xb0\xe9\x9d\x9e\xe5\xb8\xb8\xe7\xae\x80\xe5\x8d\x95\xef\xbc\x8c\xe5\x85\xb6\xe5\x85\xac\xe5\xbc\x8f\xe5\xa6\x82\xe4\xb8\x8b\n# \n# $$\n# f(x) = \\frac{1}{1 + e^{-x}}\n# $$\n# \n# Sigmoid \xe5\x87\xbd\xe6\x95\xb0\xe7\x9a\x84\xe5\x9b\xbe\xe5\x83\x8f\xe5\xa6\x82\xe4\xb8\x8b\n# \n# ![](https://ws2.sinaimg.cn/large/006tKfTcly1fmd3dde091g30du060mx0.gif)\n# \n# \xe5\x8f\xaf\xe4\xbb\xa5\xe7\x9c\x8b\xe5\x88\xb0 Sigmoid \xe5\x87\xbd\xe6\x95\xb0\xe7\x9a\x84\xe8\x8c\x83\xe5\x9b\xb4\xe6\x98\xaf\xe5\x9c\xa8 0 ~ 1 \xe4\xb9\x8b\xe9\x97\xb4\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe4\xbb\xbb\xe4\xbd\x95\xe4\xb8\x80\xe4\xb8\xaa\xe5\x80\xbc\xe7\xbb\x8f\xe8\xbf\x87\xe4\xba\x86 Sigmoid \xe5\x87\xbd\xe6\x95\xb0\xe7\x9a\x84\xe4\xbd\x9c\xe7\x94\xa8\xef\xbc\x8c\xe9\x83\xbd\xe4\xbc\x9a\xe5\x8f\x98\xe6\x88\x90 0 ~ 1 \xe4\xb9\x8b\xe9\x97\xb4\xe7\x9a\x84\xe4\xb8\x80\xe4\xb8\xaa\xe5\x80\xbc\xef\xbc\x8c\xe8\xbf\x99\xe4\xb8\xaa\xe5\x80\xbc\xe5\x8f\xaf\xe4\xbb\xa5\xe5\xbd\xa2\xe8\xb1\xa1\xe5\x9c\xb0\xe7\x90\x86\xe8\xa7\xa3\xe4\xb8\xba\xe4\xb8\x80\xe4\xb8\xaa\xe6\xa6\x82\xe7\x8e\x87\xef\xbc\x8c\xe6\xaf\x94\xe5\xa6\x82\xe5\xaf\xb9\xe4\xba\x8e\xe4\xba\x8c\xe5\x88\x86\xe7\xb1\xbb\xe9\x97\xae\xe9\xa2\x98\xef\xbc\x8c\xe8\xbf\x99\xe4\xb8\xaa\xe5\x80\xbc\xe8\xb6\x8a\xe5\xb0\x8f\xe5\xb0\xb1\xe8\xa1\xa8\xe7\xa4\xba\xe5\xb1\x9e\xe4\xba\x8e\xe7\xac\xac\xe4\xb8\x80\xe7\xb1\xbb\xef\xbc\x8c\xe8\xbf\x99\xe4\xb8\xaa\xe5\x80\xbc\xe8\xb6\x8a\xe5\xa4\xa7\xe5\xb0\xb1\xe8\xa1\xa8\xe7\xa4\xba\xe5\xb1\x9e\xe4\xba\x8e\xe7\xac\xac\xe4\xba\x8c\xe7\xb1\xbb\xe3\x80\x82\n#%% [markdown]\n# \xe5\x8f\xa6\xe5\xa4\x96\xe4\xb8\x80\xe4\xb8\xaa Logistic \xe5\x9b\x9e\xe5\xbd\x92\xe7\x9a\x84\xe5\x89\x8d\xe6\x8f\x90\xe6\x98\xaf\xe7\xa1\xae\xe4\xbf\x9d\xe4\xbd\xa0\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe5\x85\xb7\xe6\x9c\x89\xe9\x9d\x9e\xe5\xb8\xb8\xe8\x89\xaf\xe5\xa5\xbd\xe7\x9a\x84\xe7\xba\xbf\xe6\x80\xa7\xe5\x8f\xaf\xe5\x88\x86\xe6\x80\xa7\xef\xbc\x8c\xe4\xb9\x9f\xe5\xb0\xb1\xe6\x98\xaf\xe8\xaf\xb4\xef\xbc\x8c\xe4\xbd\xa0\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe8\x83\xbd\xe5\xa4\x9f\xe5\x9c\xa8\xe4\xb8\x80\xe5\xae\x9a\xe7\x9a\x84\xe7\xbb\xb4\xe5\xba\xa6\xe4\xb8\x8a\xe8\xa2\xab\xe5\x88\x86\xe4\xb8\xba\xe4\xb8\xa4\xe4\xb8\xaa\xe9\x83\xa8\xe5\x88\x86\xef\xbc\x8c\xe6\xaf\x94\xe5\xa6\x82\n# \n# ![](https://ws1.sinaimg.cn/large/006tKfTcly1fmd3gwdueoj30aw0aewex.jpg)\n#%% [markdown]\n# \xe5\x8f\xaf\xe4\xbb\xa5\xe7\x9c\x8b\xe5\x88\xb0\xef\xbc\x8c\xe4\xb8\x8a\xe9\x9d\xa2\xe7\xba\xa2\xe8\x89\xb2\xe7\x9a\x84\xe7\x82\xb9\xe5\x92\x8c\xe8\x93\x9d\xe8\x89\xb2\xe7\x9a\x84\xe7\x82\xb9\xe8\x83\xbd\xe5\xa4\x9f\xe5\x87\xa0\xe4\xb9\x8e\xe8\xa2\xab\xe4\xb8\x80\xe4\xb8\xaa\xe7\xbb\xbf\xe8\x89\xb2\xe7\x9a\x84\xe5\xb9\xb3\xe9\x9d\xa2\xe5\x88\x86\xe5\x89\xb2\xe5\xbc\x80\xe6\x9d\xa5\n#%% [markdown]\n# ## \xe5\x9b\x9e\xe5\xbd\x92\xe9\x97\xae\xe9\xa2\x98 vs \xe5\x88\x86\xe7\xb1\xbb\xe9\x97\xae\xe9\xa2\x98\n# Logistic \xe5\x9b\x9e\xe5\xbd\x92\xe5\xa4\x84\xe7\x90\x86\xe7\x9a\x84\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa\xe5\x88\x86\xe7\xb1\xbb\xe9\x97\xae\xe9\xa2\x98\xef\xbc\x8c\xe8\x80\x8c\xe4\xb8\x8a\xe4\xb8\x80\xe4\xb8\xaa\xe6\xa8\xa1\xe5\x9e\x8b\xe6\x98\xaf\xe5\x9b\x9e\xe5\xbd\x92\xe6\xa8\xa1\xe5\x9e\x8b\xef\xbc\x8c\xe9\x82\xa3\xe4\xb9\x88\xe5\x9b\x9e\xe5\xbd\x92\xe9\x97\xae\xe9\xa2\x98\xe5\x92\x8c\xe5\x88\x86\xe7\xb1\xbb\xe9\x97\xae\xe9\xa2\x98\xe7\x9a\x84\xe5\x8c\xba\xe5\x88\xab\xe5\x9c\xa8\xe5\x93\xaa\xe9\x87\x8c\xe5\x91\xa2\xef\xbc\x9f\n# \n# \xe4\xbb\x8e\xe4\xb8\x8a\xe9\x9d\xa2\xe7\x9a\x84\xe5\x9b\xbe\xe5\x8f\xaf\xe4\xbb\xa5\xe7\x9c\x8b\xe5\x87\xba\xef\xbc\x8c\xe5\x88\x86\xe7\xb1\xbb\xe9\x97\xae\xe9\xa2\x98\xe5\xb8\x8c\xe6\x9c\x9b\xe6\x8a\x8a\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe5\x88\x86\xe5\x88\xb0\xe6\x9f\x90\xe4\xb8\x80\xe7\xb1\xbb\xef\xbc\x8c\xe6\xaf\x94\xe5\xa6\x82\xe4\xb8\x80\xe4\xb8\xaa 3 \xe5\x88\x86\xe7\xb1\xbb\xe9\x97\xae\xe9\xa2\x98\xef\xbc\x8c\xe9\x82\xa3\xe4\xb9\x88\xe5\xaf\xb9\xe4\xba\x8e\xe4\xbb\xbb\xe4\xbd\x95\xe4\xb8\x80\xe4\xb8\xaa\xe6\x95\xb0\xe6\x8d\xae\xe7\x82\xb9\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe9\x83\xbd\xe5\xb8\x8c\xe6\x9c\x9b\xe6\x89\xbe\xe5\x88\xb0\xe5\x85\xb6\xe5\x88\xb0\xe5\xba\x95\xe5\xb1\x9e\xe4\xba\x8e\xe5\x93\xaa\xe4\xb8\x80\xe7\xb1\xbb\xef\xbc\x8c\xe6\x9c\x80\xe7\xbb\x88\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x9c\xe5\x8f\xaa\xe6\x9c\x89\xe4\xb8\x89\xe7\xa7\x8d\xe6\x83\x85\xe5\x86\xb5\xef\xbc\x8c{0, 1, 2}\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe8\xbf\x99\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa\xe7\xa6\xbb\xe6\x95\xa3\xe7\x9a\x84\xe9\x97\xae\xe9\xa2\x98\xe3\x80\x82\n# \n# \xe8\x80\x8c\xe5\x9b\x9e\xe5\xbd\x92\xe9\x97\xae\xe9\xa2\x98\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa\xe8\xbf\x9e\xe7\xbb\xad\xe7\x9a\x84\xe9\x97\xae\xe9\xa2\x98\xef\xbc\x8c\xe6\xaf\x94\xe5\xa6\x82\xe6\x9b\xb2\xe7\xba\xbf\xe7\x9a\x84\xe6\x8b\x9f\xe5\x90\x88\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe5\x8f\xaf\xe4\xbb\xa5\xe6\x8b\x9f\xe5\x90\x88\xe4\xbb\xbb\xe6\x84\x8f\xe7\x9a\x84\xe5\x87\xbd\xe6\x95\xb0\xe7\xbb\x93\xe6\x9e\x9c\xef\xbc\x8c\xe8\xbf\x99\xe4\xb8\xaa\xe7\xbb\x93\xe6\x9e\x9c\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa\xe8\xbf\x9e\xe7\xbb\xad\xe7\x9a\x84\xe5\x80\xbc\xe3\x80\x82\n# \n# \xe5\x88\x86\xe7\xb1\xbb\xe9\x97\xae\xe9\xa2\x98\xe5\x92\x8c\xe5\x9b\x9e\xe5\xbd\x92\xe9\x97\xae\xe9\xa2\x98\xe6\x98\xaf\xe6\x9c\xba\xe5\x99\xa8\xe5\xad\xa6\xe4\xb9\xa0\xe5\x92\x8c\xe6\xb7\xb1\xe5\xba\xa6\xe5\xad\xa6\xe4\xb9\xa0\xe7\x9a\x84\xe7\xac\xac\xe4\xb8\x80\xe6\xad\xa5\xef\xbc\x8c\xe6\x8b\xbf\xe5\x88\xb0\xe4\xbb\xbb\xe4\xbd\x95\xe4\xb8\x80\xe4\xb8\xaa\xe9\x97\xae\xe9\xa2\x98\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe9\x83\xbd\xe9\x9c\x80\xe8\xa6\x81\xe5\x85\x88\xe7\xa1\xae\xe5\xae\x9a\xe5\x85\xb6\xe5\x88\xb0\xe5\xba\x95\xe6\x98\xaf\xe5\x88\x86\xe7\xb1\xbb\xe8\xbf\x98\xe6\x98\xaf\xe5\x9b\x9e\xe5\xbd\x92\xef\xbc\x8c\xe7\x84\xb6\xe5\x90\x8e\xe5\x86\x8d\xe8\xbf\x9b\xe8\xa1\x8c\xe7\xae\x97\xe6\xb3\x95\xe8\xae\xbe\xe8\xae\xa1\n#%% [markdown]\n# ## \xe6\x8d\x9f\xe5\xa4\xb1\xe5\x87\xbd\xe6\x95\xb0\n# \xe5\x89\x8d\xe4\xb8\x80\xe8\x8a\x82\xe5\xaf\xb9\xe4\xba\x8e\xe5\x9b\x9e\xe5\xbd\x92\xe9\x97\xae\xe9\xa2\x98\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe6\x9c\x89\xe4\xb8\x80\xe4\xb8\xaa loss \xe5\x8e\xbb\xe8\xa1\xa1\xe9\x87\x8f\xe8\xaf\xaf\xe5\xb7\xae\xef\xbc\x8c\xe9\x82\xa3\xe4\xb9\x88\xe5\xaf\xb9\xe4\xba\x8e\xe5\x88\x86\xe7\xb1\xbb\xe9\x97\xae\xe9\xa2\x98\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe5\xa6\x82\xe4\xbd\x95\xe5\x8e\xbb\xe8\xa1\xa1\xe9\x87\x8f\xe8\xbf\x99\xe4\xb8\xaa\xe8\xaf\xaf\xe5\xb7\xae\xef\xbc\x8c\xe5\xb9\xb6\xe8\xae\xbe\xe8\xae\xa1 loss \xe5\x87\xbd\xe6\x95\xb0\xe5\x91\xa2\xef\xbc\x9f\n# \n# Logistic \xe5\x9b\x9e\xe5\xbd\x92\xe4\xbd\xbf\xe7\x94\xa8\xe4\xba\x86 Sigmoid \xe5\x87\xbd\xe6\x95\xb0\xe5\xb0\x86\xe7\xbb\x93\xe6\x9e\x9c\xe5\x8f\x98\xe5\x88\xb0 0 ~ 1 \xe4\xb9\x8b\xe9\x97\xb4\xef\xbc\x8c\xe5\xaf\xb9\xe4\xba\x8e\xe4\xbb\xbb\xe6\x84\x8f\xe8\xbe\x93\xe5\x85\xa5\xe4\xb8\x80\xe4\xb8\xaa\xe6\x95\xb0\xe6\x8d\xae\xef\xbc\x8c\xe7\xbb\x8f\xe8\xbf\x87 Sigmoid \xe4\xb9\x8b\xe5\x90\x8e\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x9c\xe6\x88\x91\xe4\xbb\xac\xe8\xae\xb0\xe4\xb8\xba $\\hat{y}$\xef\xbc\x8c\xe8\xa1\xa8\xe7\xa4\xba\xe8\xbf\x99\xe4\xb8\xaa\xe6\x95\xb0\xe6\x8d\xae\xe7\x82\xb9\xe5\xb1\x9e\xe4\xba\x8e\xe7\xac\xac\xe4\xba\x8c\xe7\xb1\xbb\xe7\x9a\x84\xe6\xa6\x82\xe7\x8e\x87\xef\xbc\x8c\xe9\x82\xa3\xe4\xb9\x88\xe5\x85\xb6\xe5\xb1\x9e\xe4\xba\x8e\xe7\xac\xac\xe4\xb8\x80\xe7\xb1\xbb\xe7\x9a\x84\xe6\xa6\x82\xe7\x8e\x87\xe5\xb0\xb1\xe6\x98\xaf $1-\\hat{y}$\xe3\x80\x82\xe5\xa6\x82\xe6\x9e\x9c\xe8\xbf\x99\xe4\xb8\xaa\xe6\x95\xb0\xe6\x8d\xae\xe7\x82\xb9\xe5\xb1\x9e\xe4\xba\x8e\xe7\xac\xac\xe4\xba\x8c\xe7\xb1\xbb\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe5\xb8\x8c\xe6\x9c\x9b $\\hat{y}$ \xe8\xb6\x8a\xe5\xa4\xa7\xe8\xb6\x8a\xe5\xa5\xbd\xef\xbc\x8c\xe4\xb9\x9f\xe5\xb0\xb1\xe6\x98\xaf\xe8\xb6\x8a\xe9\x9d\xa0\xe8\xbf\x91 1 \xe8\xb6\x8a\xe5\xa5\xbd\xef\xbc\x8c\xe5\xa6\x82\xe6\x9e\x9c\xe8\xbf\x99\xe4\xb8\xaa\xe6\x95\xb0\xe6\x8d\xae\xe5\xb1\x9e\xe4\xba\x8e\xe7\xac\xac\xe4\xb8\x80\xe7\xb1\xbb\xef\xbc\x8c\xe9\x82\xa3\xe4\xb9\x88\xe6\x88\x91\xe4\xbb\xac\xe5\xb8\x8c\xe6\x9c\x9b $1-\\hat{y}$ \xe8\xb6\x8a\xe5\xa4\xa7\xe8\xb6\x8a\xe5\xa5\xbd\xef\xbc\x8c\xe4\xb9\x9f\xe5\xb0\xb1\xe6\x98\xaf $\\hat{y}$ \xe8\xb6\x8a\xe5\xb0\x8f\xe8\xb6\x8a\xe5\xa5\xbd\xef\xbc\x8c\xe8\xb6\x8a\xe9\x9d\xa0\xe8\xbf\x91 0 \xe8\xb6\x8a\xe5\xa5\xbd\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe6\x88\x91\xe4\xbb\xac\xe5\x8f\xaf\xe4\xbb\xa5\xe8\xbf\x99\xe6\xa0\xb7\xe8\xae\xbe\xe8\xae\xa1\xe6\x88\x91\xe4\xbb\xac\xe7\x9a\x84 loss \xe5\x87\xbd\xe6\x95\xb0\n# \n# $$\n# loss = -(y * log(\\hat{y}) + (1 - y) * log(1 - \\hat{y}))\n# $$\n# \n# \xe5\x85\xb6\xe4\xb8\xad y \xe8\xa1\xa8\xe7\xa4\xba\xe7\x9c\x9f\xe5\xae\x9e\xe7\x9a\x84 label\xef\xbc\x8c\xe5\x8f\xaa\xe8\x83\xbd\xe5\x8f\x96 {0, 1} \xe8\xbf\x99\xe4\xb8\xa4\xe4\xb8\xaa\xe5\x80\xbc\xef\xbc\x8c\xe5\x9b\xa0\xe4\xb8\xba $\\hat{y}$ \xe8\xa1\xa8\xe7\xa4\xba\xe7\xbb\x8f\xe8\xbf\x87 Logistic \xe5\x9b\x9e\xe5\xbd\x92\xe9\xa2\x84\xe6\xb5\x8b\xe4\xb9\x8b\xe5\x90\x8e\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x9c\xef\xbc\x8c\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa 0 ~ 1 \xe4\xb9\x8b\xe9\x97\xb4\xe7\x9a\x84\xe5\xb0\x8f\xe6\x95\xb0\xe3\x80\x82\xe5\xa6\x82\xe6\x9e\x9c y \xe6\x98\xaf 0\xef\xbc\x8c\xe8\xa1\xa8\xe7\xa4\xba\xe8\xaf\xa5\xe6\x95\xb0\xe6\x8d\xae\xe5\xb1\x9e\xe4\xba\x8e\xe7\xac\xac\xe4\xb8\x80\xe7\xb1\xbb\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe5\xb8\x8c\xe6\x9c\x9b $\\hat{y}$ \xe8\xb6\x8a\xe5\xb0\x8f\xe8\xb6\x8a\xe5\xa5\xbd\xef\xbc\x8c\xe4\xb8\x8a\xe9\x9d\xa2\xe7\x9a\x84 loss \xe5\x87\xbd\xe6\x95\xb0\xe5\x8f\x98\xe4\xb8\xba\n# \n# $$\n# loss = - (log(1 - \\hat{y}))\n# $$\n# \n# \xe5\x9c\xa8\xe8\xae\xad\xe7\xbb\x83\xe6\xa8\xa1\xe5\x9e\x8b\xe7\x9a\x84\xe6\x97\xb6\xe5\x80\x99\xe6\x88\x91\xe4\xbb\xac\xe5\xb8\x8c\xe6\x9c\x9b\xe6\x9c\x80\xe5\xb0\x8f\xe5\x8c\x96 loss \xe5\x87\xbd\xe6\x95\xb0\xef\xbc\x8c\xe6\xa0\xb9\xe6\x8d\xae log \xe5\x87\xbd\xe6\x95\xb0\xe7\x9a\x84\xe5\x8d\x95\xe8\xb0\x83\xe6\x80\xa7\xef\xbc\x8c\xe4\xb9\x9f\xe5\xb0\xb1\xe6\x98\xaf\xe6\x9c\x80\xe5\xb0\x8f\xe5\x8c\x96 $\\hat{y}$\xef\xbc\x8c\xe4\xb8\x8e\xe6\x88\x91\xe4\xbb\xac\xe7\x9a\x84\xe8\xa6\x81\xe6\xb1\x82\xe6\x98\xaf\xe4\xb8\x80\xe8\x87\xb4\xe7\x9a\x84\xe3\x80\x82\n# \n# \xe8\x80\x8c\xe5\xa6\x82\xe6\x9e\x9c y \xe6\x98\xaf 1\xef\xbc\x8c\xe8\xa1\xa8\xe7\xa4\xba\xe8\xaf\xa5\xe6\x95\xb0\xe6\x8d\xae\xe5\xb1\x9e\xe4\xba\x8e\xe7\xac\xac\xe4\xba\x8c\xe7\xb1\xbb\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe5\xb8\x8c\xe6\x9c\x9b $\\hat{y}$ \xe8\xb6\x8a\xe5\xa4\xa7\xe8\xb6\x8a\xe5\xa5\xbd\xef\xbc\x8c\xe5\x90\x8c\xe6\x97\xb6\xe4\xb8\x8a\xe9\x9d\xa2\xe7\x9a\x84 loss \xe5\x87\xbd\xe6\x95\xb0\xe5\x8f\x98\xe4\xb8\xba\n# \n# $$\n# loss = -(log(\\hat{y}))\n# $$\n# \n# \xe6\x88\x91\xe4\xbb\xac\xe5\xb8\x8c\xe6\x9c\x9b\xe6\x9c\x80\xe5\xb0\x8f\xe5\x8c\x96 loss \xe5\x87\xbd\xe6\x95\xb0\xe4\xb9\x9f\xe5\xb0\xb1\xe6\x98\xaf\xe6\x9c\x80\xe5\xa4\xa7\xe5\x8c\x96 $\\hat{y}$\xef\xbc\x8c\xe8\xbf\x99\xe4\xb9\x9f\xe4\xb8\x8e\xe6\x88\x91\xe4\xbb\xac\xe7\x9a\x84\xe8\xa6\x81\xe6\xb1\x82\xe4\xb8\x80\xe8\x87\xb4\xe3\x80\x82\n# \n# \xe6\x89\x80\xe4\xbb\xa5\xe9\x80\x9a\xe8\xbf\x87\xe4\xb8\x8a\xe9\x9d\xa2\xe7\x9a\x84\xe8\xae\xba\xe8\xbf\xb0\xef\xbc\x8c\xe8\xaf\xb4\xe6\x98\x8e\xe4\xba\x86\xe8\xbf\x99\xe4\xb9\x88\xe6\x9e\x84\xe5\xbb\xba loss \xe5\x87\xbd\xe6\x95\xb0\xe6\x98\xaf\xe5\x90\x88\xe7\x90\x86\xe7\x9a\x84\xe3\x80\x82\n#%% [markdown]\n# \xe4\xb8\x8b\xe9\x9d\xa2\xe6\x88\x91\xe4\xbb\xac\xe9\x80\x9a\xe8\xbf\x87\xe4\xbe\x8b\xe5\xad\x90\xe6\x9d\xa5\xe5\x85\xb7\xe4\xbd\x93\xe5\xad\xa6\xe4\xb9\xa0 Logistic \xe5\x9b\x9e\xe5\xbd\x92\n\n#%%\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nget_ipython().run_line_magic(\'matplotlib\', \'inline\')\n\n\n#%%\n# \xe8\xae\xbe\xe5\xae\x9a\xe9\x9a\x8f\xe6\x9c\xba\xe7\xa7\x8d\xe5\xad\x90\ntorch.manual_seed(2017)\n\n#%% [markdown]\n# \xe6\x88\x91\xe4\xbb\xac\xe4\xbb\x8e data.txt \xe8\xaf\xbb\xe5\x85\xa5\xe6\x95\xb0\xe6\x8d\xae\xef\xbc\x8c\xe6\x84\x9f\xe5\x85\xb4\xe8\xb6\xa3\xe7\x9a\x84\xe5\x90\x8c\xe5\xad\xa6\xe5\x8f\xaf\xe4\xbb\xa5\xe6\x89\x93\xe5\xbc\x80 data.txt \xe6\x96\x87\xe4\xbb\xb6\xe8\xbf\x9b\xe8\xa1\x8c\xe6\x9f\xa5\xe7\x9c\x8b\n# \n# \xe8\xaf\xbb\xe5\x85\xa5\xe6\x95\xb0\xe6\x8d\xae\xe7\x82\xb9\xe4\xb9\x8b\xe5\x90\x8e\xe6\x88\x91\xe4\xbb\xac\xe6\xa0\xb9\xe6\x8d\xae\xe4\xb8\x8d\xe5\x90\x8c\xe7\x9a\x84 label \xe5\xb0\x86\xe6\x95\xb0\xe6\x8d\xae\xe7\x82\xb9\xe5\x88\x86\xe4\xb8\xba\xe4\xba\x86\xe7\xba\xa2\xe8\x89\xb2\xe5\x92\x8c\xe8\x93\x9d\xe8\x89\xb2\xef\xbc\x8c\xe5\xb9\xb6\xe4\xb8\x94\xe7\x94\xbb\xe5\x9b\xbe\xe5\xb1\x95\xe7\xa4\xba\xe5\x87\xba\xe6\x9d\xa5\xe4\xba\x86\n\n#%%\n# \xe4\xbb\x8e data.txt \xe4\xb8\xad\xe8\xaf\xbb\xe5\x85\xa5\xe7\x82\xb9\nwith open(\'./data.txt\', \'r\') as f:\n    data_list = [i.split(\'\\n\')[0].split(\',\') for i in f.readlines()]\n    data = [(float(i[0]), float(i[1]), float(i[2])) for i in data_list]\n\n# \xe6\xa0\x87\xe5\x87\x86\xe5\x8c\x96\nx0_max = max([i[0] for i in data])\nx1_max = max([i[1] for i in data])\ndata = [(i[0]/x0_max, i[1]/x1_max, i[2]) for i in data]\n\nx0 = list(filter(lambda x: x[-1] == 0.0, data)) # \xe9\x80\x89\xe6\x8b\xa9\xe7\xac\xac\xe4\xb8\x80\xe7\xb1\xbb\xe7\x9a\x84\xe7\x82\xb9\nx1 = list(filter(lambda x: x[-1] == 1.0, data)) # \xe9\x80\x89\xe6\x8b\xa9\xe7\xac\xac\xe4\xba\x8c\xe7\xb1\xbb\xe7\x9a\x84\xe7\x82\xb9\n\nplot_x0 = [i[0] for i in x0]\nplot_y0 = [i[1] for i in x0]\nplot_x1 = [i[0] for i in x1]\nplot_y1 = [i[1] for i in x1]\n\nplt.plot(plot_x0, plot_y0, \'ro\', label=\'x_0\')\nplt.plot(plot_x1, plot_y1, \'bo\', label=\'x_1\')\nplt.legend(loc=\'best\')\n\n#%% [markdown]\n# \xe6\x8e\xa5\xe4\xb8\x8b\xe6\x9d\xa5\xe6\x88\x91\xe4\xbb\xac\xe5\xb0\x86\xe6\x95\xb0\xe6\x8d\xae\xe8\xbd\xac\xe6\x8d\xa2\xe6\x88\x90 NumPy \xe7\x9a\x84\xe7\xb1\xbb\xe5\x9e\x8b\xef\xbc\x8c\xe6\x8e\xa5\xe7\x9d\x80\xe8\xbd\xac\xe6\x8d\xa2\xe5\x88\xb0 Tensor \xe4\xb8\xba\xe4\xb9\x8b\xe5\x90\x8e\xe7\x9a\x84\xe8\xae\xad\xe7\xbb\x83\xe5\x81\x9a\xe5\x87\x86\xe5\xa4\x87\n\n#%%\nnp_data = np.array(data, dtype=\'float32\') # \xe8\xbd\xac\xe6\x8d\xa2\xe6\x88\x90 numpy array\nx_data = torch.from_numpy(np_data[:, 0:2]) # \xe8\xbd\xac\xe6\x8d\xa2\xe6\x88\x90 Tensor, \xe5\xa4\xa7\xe5\xb0\x8f\xe6\x98\xaf [100, 2]\ny_data = torch.from_numpy(np_data[:, -1]).unsqueeze(1) # \xe8\xbd\xac\xe6\x8d\xa2\xe6\x88\x90 Tensor\xef\xbc\x8c\xe5\xa4\xa7\xe5\xb0\x8f\xe6\x98\xaf [100, 1]\n\n#%% [markdown]\n# \xe4\xb8\x8b\xe9\x9d\xa2\xe6\x88\x91\xe4\xbb\xac\xe6\x9d\xa5\xe5\xae\x9e\xe7\x8e\xb0\xe4\xbb\xa5\xe4\xb8\x8b Sigmoid \xe7\x9a\x84\xe5\x87\xbd\xe6\x95\xb0\xef\xbc\x8cSigmoid \xe5\x87\xbd\xe6\x95\xb0\xe7\x9a\x84\xe5\x85\xac\xe5\xbc\x8f\xe4\xb8\xba\n# \n# $$\n# f(x) = \\frac{1}{1 + e^{-x}}\n# $$\n\n#%%\n# \xe5\xae\x9a\xe4\xb9\x89 sigmoid \xe5\x87\xbd\xe6\x95\xb0\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\n#%% [markdown]\n# \xe7\x94\xbb\xe5\x87\xba Sigmoid \xe5\x87\xbd\xe6\x95\xb0\xef\xbc\x8c\xe5\x8f\xaf\xe4\xbb\xa5\xe7\x9c\x8b\xe5\x88\xb0\xe5\x80\xbc\xe8\xb6\x8a\xe5\xa4\xa7\xef\xbc\x8c\xe7\xbb\x8f\xe8\xbf\x87 Sigmoid \xe5\x87\xbd\xe6\x95\xb0\xe4\xb9\x8b\xe5\x90\x8e\xe8\xb6\x8a\xe9\x9d\xa0\xe8\xbf\x91 1\xef\xbc\x8c\xe5\x80\xbc\xe8\xb6\x8a\xe5\xb0\x8f\xef\xbc\x8c\xe8\xb6\x8a\xe9\x9d\xa0\xe8\xbf\x91 0\n\n#%%\n# \xe7\x94\xbb\xe5\x87\xba sigmoid \xe7\x9a\x84\xe5\x9b\xbe\xe5\x83\x8f\n\nplot_x = np.arange(-10, 10.01, 0.01)\nplot_y = sigmoid(plot_x)\n\nplt.plot(plot_x, plot_y, \'r\')\n\n#%% [markdown]\n# \xe5\x9c\xa8 PyTorch \xe5\xbd\x93\xe4\xb8\xad\xef\xbc\x8c\xe4\xb8\x8d\xe9\x9c\x80\xe8\xa6\x81\xe6\x88\x91\xe4\xbb\xac\xe8\x87\xaa\xe5\xb7\xb1\xe5\x86\x99 Sigmoid \xe7\x9a\x84\xe5\x87\xbd\xe6\x95\xb0\xef\xbc\x8cPyTorch \xe5\xb7\xb2\xe7\xbb\x8f\xe7\x94\xa8\xe5\xba\x95\xe5\xb1\x82\xe7\x9a\x84 C++ \xe8\xaf\xad\xe8\xa8\x80\xe4\xb8\xba\xe6\x88\x91\xe4\xbb\xac\xe5\x86\x99\xe5\xa5\xbd\xe4\xba\x86\xe4\xb8\x80\xe4\xba\x9b\xe5\xb8\xb8\xe7\x94\xa8\xe7\x9a\x84\xe5\x87\xbd\xe6\x95\xb0\xef\xbc\x8c\xe4\xb8\x8d\xe4\xbb\x85\xe6\x96\xb9\xe4\xbe\xbf\xe6\x88\x91\xe4\xbb\xac\xe4\xbd\xbf\xe7\x94\xa8\xef\xbc\x8c\xe5\x90\x8c\xe6\x97\xb6\xe9\x80\x9f\xe5\xba\xa6\xe4\xb8\x8a\xe6\xaf\x94\xe6\x88\x91\xe4\xbb\xac\xe8\x87\xaa\xe5\xb7\xb1\xe5\xae\x9e\xe7\x8e\xb0\xe7\x9a\x84\xe6\x9b\xb4\xe5\xbf\xab\xef\xbc\x8c\xe7\xa8\xb3\xe5\xae\x9a\xe6\x80\xa7\xe6\x9b\xb4\xe5\xa5\xbd\n# \n# \xe9\x80\x9a\xe8\xbf\x87\xe5\xaf\xbc\xe5\x85\xa5 `torch.nn.functional` \xe6\x9d\xa5\xe4\xbd\xbf\xe7\x94\xa8\xef\xbc\x8c\xe4\xb8\x8b\xe9\x9d\xa2\xe5\xb0\xb1\xe6\x98\xaf\xe4\xbd\xbf\xe7\x94\xa8\xe6\x96\xb9\xe6\xb3\x95\xef\xbc\x88\xe8\xaf\xa5\xe6\x96\xb9\xe6\xb3\x95\xe5\xb7\xb2deprecated\xef\xbc\x89\n# \n# \xe6\x96\xb0\xe8\xa1\xa5\xe5\x85\x85\xef\xbc\x9a` UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n#   warnings.warn(""nn.functional.sigmoid is deprecated. Use torch.sigmoid instead."")`\n\n#%%\n# import torch.nn.functional as F\n\n\n#%%\n# \xe5\xae\x9a\xe4\xb9\x89 logistic \xe5\x9b\x9e\xe5\xbd\x92\xe6\xa8\xa1\xe5\x9e\x8b\nw = torch.randn(2, 1, requires_grad=True) \nb = torch.zeros(1, requires_grad=True)\n\ndef logistic_regression(x):\n    return torch.sigmoid(torch.mm(x, w) + b)\n\n#%% [markdown]\n# \xe5\x9c\xa8\xe6\x9b\xb4\xe6\x96\xb0\xe4\xb9\x8b\xe5\x89\x8d\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe5\x8f\xaf\xe4\xbb\xa5\xe7\x94\xbb\xe5\x87\xba\xe5\x88\x86\xe7\xb1\xbb\xe7\x9a\x84\xe6\x95\x88\xe6\x9e\x9c\n\n#%%\n# \xe7\x94\xbb\xe5\x87\xba\xe5\x8f\x82\xe6\x95\xb0\xe6\x9b\xb4\xe6\x96\xb0\xe4\xb9\x8b\xe5\x89\x8d\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x9c\nw0 = w[0].item()\nw1 = w[1].item()\nb0 = b.item()\n\nplot_x = np.arange(0.2, 1, 0.01)\nplot_y = (-w0 * plot_x - b0) / w1\n\nplt.plot(plot_x, plot_y, \'g\', label=\'cutting line\')\nplt.plot(plot_x0, plot_y0, \'ro\', label=\'x_0\')\nplt.plot(plot_x1, plot_y1, \'bo\', label=\'x_1\')\nplt.legend(loc=\'best\')\n\n#%% [markdown]\n# \xe5\x8f\xaf\xe4\xbb\xa5\xe7\x9c\x8b\xe5\x88\xb0\xe5\x88\x86\xe7\xb1\xbb\xe6\x95\x88\xe6\x9e\x9c\xe5\x9f\xba\xe6\x9c\xac\xe6\x98\xaf\xe6\xb7\xb7\xe4\xb9\xb1\xe7\x9a\x84\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe6\x9d\xa5\xe8\xae\xa1\xe7\xae\x97\xe4\xb8\x80\xe4\xb8\x8b loss\xef\xbc\x8c\xe5\x85\xac\xe5\xbc\x8f\xe5\xa6\x82\xe4\xb8\x8b\n# \n# $$\n# loss = -(y * log(\\hat{y}) + (1 - y) * log(1 - \\hat{y}))\n# $$\n\n#%%\n# \xe8\xae\xa1\xe7\xae\x97loss\ndef binary_loss(y_pred, y):\n    logits = (y * y_pred.clamp(1e-12).log() + (1 - y) * (1 - y_pred).clamp(1e-12).log()).mean()\n    return -logits\n\n#%% [markdown]\n# \xe6\xb3\xa8\xe6\x84\x8f\xe5\x88\xb0\xe5\x85\xb6\xe4\xb8\xad\xe4\xbd\xbf\xe7\x94\xa8 `.clamp`\xef\xbc\x8c\xe8\xbf\x99\xe6\x98\xaf[\xe6\x96\x87\xe6\xa1\xa3](http://pytorch.org/docs/0.3.0/torch.html?highlight=clamp#torch.clamp)\xe7\x9a\x84\xe5\x86\x85\xe5\xae\xb9\xef\xbc\x8c\xe6\x9f\xa5\xe7\x9c\x8b\xe4\xb8\x80\xe4\xb8\x8b\xef\xbc\x8c\xe5\xb9\xb6\xe4\xb8\x94\xe6\x80\x9d\xe8\x80\x83\xe4\xb8\x80\xe4\xb8\x8b\xe8\xbf\x99\xe9\x87\x8c\xe6\x98\xaf\xe5\x90\xa6\xe4\xb8\x80\xe5\xae\x9a\xe8\xa6\x81\xe4\xbd\xbf\xe7\x94\xa8\xe8\xbf\x99\xe4\xb8\xaa\xe5\x87\xbd\xe6\x95\xb0\xef\xbc\x8c\xe5\xa6\x82\xe6\x9e\x9c\xe4\xb8\x8d\xe4\xbd\xbf\xe7\x94\xa8\xe4\xbc\x9a\xe5\x87\xba\xe7\x8e\xb0\xe4\xbb\x80\xe4\xb9\x88\xe6\xa0\xb7\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x9c\n# \n# **\xe6\x8f\x90\xe7\xa4\xba\xef\xbc\x9a\xe6\x9f\xa5\xe7\x9c\x8b\xe4\xb8\x80\xe4\xb8\xaa log \xe5\x87\xbd\xe6\x95\xb0\xe7\x9a\x84\xe5\x9b\xbe\xe5\x83\x8f**\n\n#%%\ny_pred = logistic_regression(x_data)\nloss = binary_loss(y_pred, y_data)\nprint(loss)\n\n#%% [markdown]\n# \xe5\xbe\x97\xe5\x88\xb0 loss \xe4\xb9\x8b\xe5\x90\x8e\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe8\xbf\x98\xe6\x98\xaf\xe4\xbd\xbf\xe7\x94\xa8\xe6\xa2\xaf\xe5\xba\xa6\xe4\xb8\x8b\xe9\x99\x8d\xe6\xb3\x95\xe6\x9b\xb4\xe6\x96\xb0\xe5\x8f\x82\xe6\x95\xb0\xef\xbc\x8c\xe8\xbf\x99\xe9\x87\x8c\xe5\x8f\xaf\xe4\xbb\xa5\xe4\xbd\xbf\xe7\x94\xa8\xe8\x87\xaa\xe5\x8a\xa8\xe6\xb1\x82\xe5\xaf\xbc\xe6\x9d\xa5\xe7\x9b\xb4\xe6\x8e\xa5\xe5\xbe\x97\xe5\x88\xb0\xe5\x8f\x82\xe6\x95\xb0\xe7\x9a\x84\xe5\xaf\xbc\xe6\x95\xb0\xef\xbc\x8c\xe6\x84\x9f\xe5\x85\xb4\xe8\xb6\xa3\xe7\x9a\x84\xe5\x90\x8c\xe5\xad\xa6\xe5\x8f\xaf\xe4\xbb\xa5\xe5\x8e\xbb\xe6\x89\x8b\xe5\x8a\xa8\xe6\x8e\xa8\xe5\xaf\xbc\xe4\xb8\x80\xe4\xb8\x8b\xe5\xaf\xbc\xe6\x95\xb0\xe7\x9a\x84\xe5\x85\xac\xe5\xbc\x8f\n\n#%%\n# \xe8\x87\xaa\xe5\x8a\xa8\xe6\xb1\x82\xe5\xaf\xbc\xe5\xb9\xb6\xe6\x9b\xb4\xe6\x96\xb0\xe5\x8f\x82\xe6\x95\xb0\nloss.backward()\n\nwith torch.no_grad():\n    w -= 0.1 * w.grad\n    b -= 0.1 * b.grad\n\n# \xe7\xae\x97\xe5\x87\xba\xe4\xb8\x80\xe6\xac\xa1\xe6\x9b\xb4\xe6\x96\xb0\xe4\xb9\x8b\xe5\x90\x8e\xe7\x9a\x84loss\ny_pred = logistic_regression(x_data)\nloss = binary_loss(y_pred, y_data)\nprint(loss)\n\n#%% [markdown]\n# \xe4\xb8\x8a\xe9\x9d\xa2\xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0\xe6\x9b\xb4\xe6\x96\xb0\xe6\x96\xb9\xe5\xbc\x8f\xe5\x85\xb6\xe5\xae\x9e\xe6\x98\xaf\xe7\xb9\x81\xe7\x90\x90\xe7\x9a\x84\xe9\x87\x8d\xe5\xa4\x8d\xe6\x93\x8d\xe4\xbd\x9c\xef\xbc\x8c\xe5\xa6\x82\xe6\x9e\x9c\xe6\x88\x91\xe4\xbb\xac\xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0\xe5\xbe\x88\xe5\xa4\x9a\xef\xbc\x8c\xe6\xaf\x94\xe5\xa6\x82\xe6\x9c\x89 100 \xe4\xb8\xaa\xef\xbc\x8c\xe9\x82\xa3\xe4\xb9\x88\xe6\x88\x91\xe4\xbb\xac\xe9\x9c\x80\xe8\xa6\x81\xe5\x86\x99 100 \xe8\xa1\x8c\xe6\x9d\xa5\xe6\x9b\xb4\xe6\x96\xb0\xe5\x8f\x82\xe6\x95\xb0\xef\xbc\x8c\xe4\xb8\xba\xe4\xba\x86\xe6\x96\xb9\xe4\xbe\xbf\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe5\x8f\xaf\xe4\xbb\xa5\xe5\x86\x99\xe6\x88\x90\xe4\xb8\x80\xe4\xb8\xaa\xe5\x87\xbd\xe6\x95\xb0\xe6\x9d\xa5\xe6\x9b\xb4\xe6\x96\xb0\xef\xbc\x8c\xe5\x85\xb6\xe5\xae\x9e PyTorch \xe5\xb7\xb2\xe7\xbb\x8f\xe4\xb8\xba\xe6\x88\x91\xe4\xbb\xac\xe5\xb0\x81\xe8\xa3\x85\xe4\xba\x86\xe4\xb8\x80\xe4\xb8\xaa\xe5\x87\xbd\xe6\x95\xb0\xe6\x9d\xa5\xe5\x81\x9a\xe8\xbf\x99\xe4\xbb\xb6\xe4\xba\x8b\xef\xbc\x8c\xe8\xbf\x99\xe5\xb0\xb1\xe6\x98\xaf PyTorch \xe4\xb8\xad\xe7\x9a\x84\xe4\xbc\x98\xe5\x8c\x96\xe5\x99\xa8 `torch.optim`\n# \n# \xe4\xbd\xbf\xe7\x94\xa8 `torch.optim` \xe9\x9c\x80\xe8\xa6\x81\xe5\x8f\xa6\xe5\xa4\x96\xe4\xb8\x80\xe4\xb8\xaa\xe6\x95\xb0\xe6\x8d\xae\xe7\xb1\xbb\xe5\x9e\x8b\xef\xbc\x8c\xe5\xb0\xb1\xe6\x98\xaf `nn.Parameter`\xef\xbc\x8c`nn.Parameter` \xe9\xbb\x98\xe8\xae\xa4\xe6\x98\xaf\xe8\xa6\x81\xe6\xb1\x82\xe6\xa2\xaf\xe5\xba\xa6\xe7\x9a\x84\n# \n# \xe4\xbd\xbf\xe7\x94\xa8 `torch.optim.SGD` \xe5\x8f\xaf\xe4\xbb\xa5\xe4\xbd\xbf\xe7\x94\xa8\xe6\xa2\xaf\xe5\xba\xa6\xe4\xb8\x8b\xe9\x99\x8d\xe6\xb3\x95\xe6\x9d\xa5\xe6\x9b\xb4\xe6\x96\xb0\xe5\x8f\x82\xe6\x95\xb0\xef\xbc\x8cPyTorch \xe4\xb8\xad\xe7\x9a\x84\xe4\xbc\x98\xe5\x8c\x96\xe5\x99\xa8\xe6\x9c\x89\xe6\x9b\xb4\xe5\xa4\x9a\xe7\x9a\x84\xe4\xbc\x98\xe5\x8c\x96\xe7\xae\x97\xe6\xb3\x95\xef\xbc\x8c\xe5\x9c\xa8\xe6\x9c\xac\xe7\xab\xa0\xe5\x90\x8e\xe9\x9d\xa2\xe7\x9a\x84\xe8\xaf\xbe\xe7\xa8\x8b\xe6\x88\x91\xe4\xbb\xac\xe4\xbc\x9a\xe6\x9b\xb4\xe5\x8a\xa0\xe8\xaf\xa6\xe7\xbb\x86\xe7\x9a\x84\xe4\xbb\x8b\xe7\xbb\x8d\n# \n# \xe5\xb0\x86\xe5\x8f\x82\xe6\x95\xb0 w \xe5\x92\x8c b \xe6\x94\xbe\xe5\x88\xb0 `torch.optim.SGD` \xe4\xb8\xad\xe4\xb9\x8b\xe5\x90\x8e\xef\xbc\x8c\xe8\xaf\xb4\xe6\x98\x8e\xe4\xb8\x80\xe4\xb8\x8b\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f\xef\xbc\x8c\xe5\xb0\xb1\xe5\x8f\xaf\xe4\xbb\xa5\xe4\xbd\xbf\xe7\x94\xa8 `optimizer.step()` \xe6\x9d\xa5\xe6\x9b\xb4\xe6\x96\xb0\xe5\x8f\x82\xe6\x95\xb0\xe4\xba\x86\xef\xbc\x8c\xe6\xaf\x94\xe5\xa6\x82\xe4\xb8\x8b\xe9\x9d\xa2\xe6\x88\x91\xe4\xbb\xac\xe5\xb0\x86\xe5\x8f\x82\xe6\x95\xb0\xe4\xbc\xa0\xe5\x85\xa5\xe4\xbc\x98\xe5\x8c\x96\xe5\x99\xa8\xef\xbc\x8c\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\xe8\xae\xbe\xe7\xbd\xae\xe4\xb8\xba 1.0\n\n#%%\n# \xe4\xbd\xbf\xe7\x94\xa8 torch.optim \xe6\x9b\xb4\xe6\x96\xb0\xe5\x8f\x82\xe6\x95\xb0\nfrom torch import nn\nw = nn.Parameter(torch.randn(2, 1))\nb = nn.Parameter(torch.zeros(1))\n\ndef logistic_regression(x):\n    return torch.sigmoid(torch.mm(x, w) + b)\n\noptimizer = torch.optim.SGD([w, b], lr=1.)\n\n\n#%%\n# \xe8\xbf\x9b\xe8\xa1\x8c 1000 \xe6\xac\xa1\xe6\x9b\xb4\xe6\x96\xb0\nimport time\n\nstart = time.time()\nfor e in range(1000):\n    # \xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\n    y_pred = logistic_regression(x_data)\n    loss = binary_loss(y_pred, y_data) # \xe8\xae\xa1\xe7\xae\x97 loss\n    # \xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\n    optimizer.zero_grad() # \xe4\xbd\xbf\xe7\x94\xa8\xe4\xbc\x98\xe5\x8c\x96\xe5\x99\xa8\xe5\xb0\x86\xe6\xa2\xaf\xe5\xba\xa6\xe5\xbd\x92 0\n    loss.backward()\n    optimizer.step() # \xe4\xbd\xbf\xe7\x94\xa8\xe4\xbc\x98\xe5\x8c\x96\xe5\x99\xa8\xe6\x9d\xa5\xe6\x9b\xb4\xe6\x96\xb0\xe5\x8f\x82\xe6\x95\xb0\n    # \xe8\xae\xa1\xe7\xae\x97\xe6\xad\xa3\xe7\xa1\xae\xe7\x8e\x87\n    mask = y_pred.ge(0.5).float()\n    acc = (mask == y_data).sum().item() / y_data.shape[0]\n    if (e + 1) % 200 == 0:\n        print(\'epoch: {}, Loss: {:.5f}, Acc: {:.5f}\'.format(e+1, loss.item(), acc))\nduring = time.time() - start\nprint()\nprint(\'During Time: {:.3f} s\'.format(during))\n\n#%% [markdown]\n# \xe5\x8f\xaf\xe4\xbb\xa5\xe7\x9c\x8b\xe5\x88\xb0\xe4\xbd\xbf\xe7\x94\xa8\xe4\xbc\x98\xe5\x8c\x96\xe5\x99\xa8\xe4\xb9\x8b\xe5\x90\x8e\xe6\x9b\xb4\xe6\x96\xb0\xe5\x8f\x82\xe6\x95\xb0\xe9\x9d\x9e\xe5\xb8\xb8\xe7\xae\x80\xe5\x8d\x95\xef\xbc\x8c\xe5\x8f\xaa\xe9\x9c\x80\xe8\xa6\x81\xe5\x9c\xa8\xe8\x87\xaa\xe5\x8a\xa8\xe6\xb1\x82\xe5\xaf\xbc\xe4\xb9\x8b\xe5\x89\x8d\xe4\xbd\xbf\xe7\x94\xa8**`optimizer.zero_grad()`** \xe6\x9d\xa5\xe5\xbd\x92 0 \xe6\xa2\xaf\xe5\xba\xa6\xef\xbc\x8c\xe7\x84\xb6\xe5\x90\x8e\xe4\xbd\xbf\xe7\x94\xa8 **`optimizer.step()`**\xe6\x9d\xa5\xe6\x9b\xb4\xe6\x96\xb0\xe5\x8f\x82\xe6\x95\xb0\xe5\xb0\xb1\xe5\x8f\xaf\xe4\xbb\xa5\xe4\xba\x86\xef\xbc\x8c\xe9\x9d\x9e\xe5\xb8\xb8\xe7\xae\x80\xe4\xbe\xbf\n# \n# \xe5\x90\x8c\xe6\x97\xb6\xe7\xbb\x8f\xe8\xbf\x87\xe4\xba\x86 1000 \xe6\xac\xa1\xe6\x9b\xb4\xe6\x96\xb0\xef\xbc\x8closs \xe4\xb9\x9f\xe9\x99\x8d\xe5\xbe\x97\xe6\xaf\x94\xe8\xbe\x83\xe4\xbd\x8e\xe4\xba\x86\n#%% [markdown]\n# \xe4\xb8\x8b\xe9\x9d\xa2\xe6\x88\x91\xe4\xbb\xac\xe7\x94\xbb\xe5\x87\xba\xe6\x9b\xb4\xe6\x96\xb0\xe4\xb9\x8b\xe5\x90\x8e\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x9c\n\n#%%\n# \xe7\x94\xbb\xe5\x87\xba\xe6\x9b\xb4\xe6\x96\xb0\xe4\xb9\x8b\xe5\x90\x8e\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x9c\nw0 = w[0].item()\nw1 = w[1].item()\nb0 = b.item()\n\nplot_x = np.arange(0.2, 1, 0.01)\nplot_y = (-w0 * plot_x - b0) / w1\n\nplt.plot(plot_x, plot_y, \'g\', label=\'cutting line\')\nplt.plot(plot_x0, plot_y0, \'ro\', label=\'x_0\')\nplt.plot(plot_x1, plot_y1, \'bo\', label=\'x_1\')\nplt.legend(loc=\'best\')\n\n#%% [markdown]\n# \xe5\x8f\xaf\xe4\xbb\xa5\xe7\x9c\x8b\xe5\x88\xb0\xe6\x9b\xb4\xe6\x96\xb0\xe4\xb9\x8b\xe5\x90\x8e\xe6\xa8\xa1\xe5\x9e\x8b\xe5\xb7\xb2\xe7\xbb\x8f\xe8\x83\xbd\xe5\xa4\x9f\xe5\x9f\xba\xe6\x9c\xac\xe5\xb0\x86\xe8\xbf\x99\xe4\xb8\xa4\xe7\xb1\xbb\xe7\x82\xb9\xe5\x88\x86\xe5\xbc\x80\xe4\xba\x86\n#%% [markdown]\n# \xe5\x89\x8d\xe9\x9d\xa2\xe6\x88\x91\xe4\xbb\xac\xe4\xbd\xbf\xe7\x94\xa8\xe4\xba\x86\xe8\x87\xaa\xe5\xb7\xb1\xe5\x86\x99\xe7\x9a\x84 loss\xef\xbc\x8c\xe5\x85\xb6\xe5\xae\x9e PyTorch \xe5\xb7\xb2\xe7\xbb\x8f\xe4\xb8\xba\xe6\x88\x91\xe4\xbb\xac\xe5\x86\x99\xe5\xa5\xbd\xe4\xba\x86\xe4\xb8\x80\xe4\xba\x9b\xe5\xb8\xb8\xe8\xa7\x81\xe7\x9a\x84 loss\xef\xbc\x8c\xe6\xaf\x94\xe5\xa6\x82\xe7\xba\xbf\xe6\x80\xa7\xe5\x9b\x9e\xe5\xbd\x92\xe9\x87\x8c\xe9\x9d\xa2\xe7\x9a\x84 loss \xe6\x98\xaf `nn.MSE()`\xef\xbc\x8c\xe8\x80\x8c Logistic \xe5\x9b\x9e\xe5\xbd\x92\xe7\x9a\x84\xe4\xba\x8c\xe5\x88\x86\xe7\xb1\xbb loss \xe5\x9c\xa8 PyTorch \xe4\xb8\xad\xe6\x98\xaf `nn.BCEWithLogitsLoss()`\xef\xbc\x8c\xe5\x85\xb3\xe4\xba\x8e\xe6\x9b\xb4\xe5\xa4\x9a\xe7\x9a\x84 loss\xef\xbc\x8c\xe5\x8f\xaf\xe4\xbb\xa5\xe6\x9f\xa5\xe7\x9c\x8b[\xe6\x96\x87\xe6\xa1\xa3](http://pytorch.org/docs/0.3.0/nn.html#loss-functions)\n# \n# PyTorch \xe4\xb8\xba\xe6\x88\x91\xe4\xbb\xac\xe5\xae\x9e\xe7\x8e\xb0\xe7\x9a\x84 loss \xe5\x87\xbd\xe6\x95\xb0\xe6\x9c\x89\xe4\xb8\xa4\xe4\xb8\xaa\xe5\xa5\xbd\xe5\xa4\x84\xef\xbc\x8c\xe7\xac\xac\xe4\xb8\x80\xe6\x98\xaf\xe6\x96\xb9\xe4\xbe\xbf\xe6\x88\x91\xe4\xbb\xac\xe4\xbd\xbf\xe7\x94\xa8\xef\xbc\x8c\xe4\xb8\x8d\xe9\x9c\x80\xe8\xa6\x81\xe9\x87\x8d\xe5\xa4\x8d\xe9\x80\xa0\xe8\xbd\xae\xe5\xad\x90\xef\xbc\x8c\xe7\xac\xac\xe4\xba\x8c\xe5\xb0\xb1\xe6\x98\xaf\xe5\x85\xb6\xe5\xae\x9e\xe7\x8e\xb0\xe6\x98\xaf\xe5\x9c\xa8\xe5\xba\x95\xe5\xb1\x82 C++ \xe8\xaf\xad\xe8\xa8\x80\xe4\xb8\x8a\xe7\x9a\x84\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe9\x80\x9f\xe5\xba\xa6\xe4\xb8\x8a\xe5\x92\x8c\xe7\xa8\xb3\xe5\xae\x9a\xe6\x80\xa7\xe4\xb8\x8a\xe9\x83\xbd\xe8\xa6\x81\xe6\xaf\x94\xe6\x88\x91\xe4\xbb\xac\xe8\x87\xaa\xe5\xb7\xb1\xe5\xae\x9e\xe7\x8e\xb0\xe7\x9a\x84\xe8\xa6\x81\xe5\xa5\xbd\n# \n# \xe5\x8f\xa6\xe5\xa4\x96\xef\xbc\x8cPyTorch \xe5\x87\xba\xe4\xba\x8e\xe7\xa8\xb3\xe5\xae\x9a\xe6\x80\xa7\xe8\x80\x83\xe8\x99\x91\xef\xbc\x8c\xe5\xb0\x86\xe6\xa8\xa1\xe5\x9e\x8b\xe7\x9a\x84 Sigmoid \xe6\x93\x8d\xe4\xbd\x9c\xe5\x92\x8c\xe6\x9c\x80\xe5\x90\x8e\xe7\x9a\x84 loss \xe9\x83\xbd\xe5\x90\x88\xe5\x9c\xa8\xe4\xba\x86 `nn.BCEWithLogitsLoss()`\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe6\x88\x91\xe4\xbb\xac\xe4\xbd\xbf\xe7\x94\xa8 PyTorch \xe8\x87\xaa\xe5\xb8\xa6\xe7\x9a\x84 loss \xe5\xb0\xb1\xe4\xb8\x8d\xe9\x9c\x80\xe8\xa6\x81\xe5\x86\x8d\xe5\x8a\xa0\xe4\xb8\x8a Sigmoid \xe6\x93\x8d\xe4\xbd\x9c\xe4\xba\x86\n\n#%%\n# \xe4\xbd\xbf\xe7\x94\xa8\xe8\x87\xaa\xe5\xb8\xa6\xe7\x9a\x84loss\ncriterion = nn.BCEWithLogitsLoss() # \xe5\xb0\x86 sigmoid \xe5\x92\x8c loss \xe5\x86\x99\xe5\x9c\xa8\xe4\xb8\x80\xe5\xb1\x82\xef\xbc\x8c\xe6\x9c\x89\xe6\x9b\xb4\xe5\xbf\xab\xe7\x9a\x84\xe9\x80\x9f\xe5\xba\xa6\xe3\x80\x81\xe6\x9b\xb4\xe5\xa5\xbd\xe7\x9a\x84\xe7\xa8\xb3\xe5\xae\x9a\xe6\x80\xa7\n\nw = nn.Parameter(torch.randn(2, 1))\nb = nn.Parameter(torch.zeros(1))\n\ndef logistic_reg(x):\n    return torch.mm(x, w) + b\n\noptimizer = torch.optim.SGD([w, b], 1.)\n\n\n#%%\ny_pred = logistic_reg(x_data)\nloss = criterion(y_pred, y_data)\nprint(loss.item())\n\n\n#%%\n# \xe5\x90\x8c\xe6\xa0\xb7\xe8\xbf\x9b\xe8\xa1\x8c 1000 \xe6\xac\xa1\xe6\x9b\xb4\xe6\x96\xb0\n\nstart = time.time()\nfor e in range(1000):\n    # \xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\n    y_pred = logistic_reg(x_data)\n    loss = criterion(y_pred, y_data)\n    # \xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    # \xe8\xae\xa1\xe7\xae\x97\xe6\xad\xa3\xe7\xa1\xae\xe7\x8e\x87\n    mask = y_pred.ge(0.5).float()\n    acc = (mask == y_data).sum().item() / y_data.shape[0]\n    if (e + 1) % 200 == 0:\n        print(\'epoch: {}, Loss: {:.5f}, Acc: {:.5f}\'.format(e+1, loss.item(), acc))\n\nduring = time.time() - start\nprint()\nprint(\'During Time: {:.3f} s\'.format(during))\n\n#%% [markdown]\n# \xe5\x8f\xaf\xe4\xbb\xa5\xe7\x9c\x8b\xe5\x88\xb0\xef\xbc\x8c\xe4\xbd\xbf\xe7\x94\xa8\xe4\xba\x86 PyTorch \xe8\x87\xaa\xe5\xb8\xa6\xe7\x9a\x84 loss \xe4\xb9\x8b\xe5\x90\x8e\xef\xbc\x8c\xe9\x80\x9f\xe5\xba\xa6\xe6\x9c\x89\xe4\xba\x86\xe4\xb8\x80\xe5\xae\x9a\xe7\x9a\x84\xe4\xb8\x8a\xe5\x8d\x87\xef\xbc\x8c\xe8\x99\xbd\xe7\x84\xb6\xe7\x9c\x8b\xe4\xb8\x8a\xe5\x8e\xbb\xe9\x80\x9f\xe5\xba\xa6\xe7\x9a\x84\xe6\x8f\x90\xe5\x8d\x87\xe5\xb9\xb6\xe4\xb8\x8d\xe5\xa4\x9a\xef\xbc\x8c\xe4\xbd\x86\xe6\x98\xaf\xe8\xbf\x99\xe5\x8f\xaa\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa\xe5\xb0\x8f\xe7\xbd\x91\xe7\xbb\x9c\xef\xbc\x8c\xe5\xaf\xb9\xe4\xba\x8e\xe5\xa4\xa7\xe7\xbd\x91\xe7\xbb\x9c\xef\xbc\x8c\xe4\xbd\xbf\xe7\x94\xa8\xe8\x87\xaa\xe5\xb8\xa6\xe7\x9a\x84 loss \xe4\xb8\x8d\xe7\xae\xa1\xe5\xaf\xb9\xe4\xba\x8e\xe7\xa8\xb3\xe5\xae\x9a\xe6\x80\xa7\xe8\xbf\x98\xe6\x98\xaf\xe9\x80\x9f\xe5\xba\xa6\xe8\x80\x8c\xe8\xa8\x80\xef\xbc\x8c\xe9\x83\xbd\xe6\x9c\x89\xe8\xb4\xa8\xe7\x9a\x84\xe9\xa3\x9e\xe8\xb7\x83\xef\xbc\x8c\xe5\x90\x8c\xe6\x97\xb6\xe4\xb9\x9f\xe9\x81\xbf\xe5\x85\x8d\xe4\xba\x86\xe9\x87\x8d\xe5\xa4\x8d\xe9\x80\xa0\xe8\xbd\xae\xe5\xad\x90\xe7\x9a\x84\xe5\x9b\xb0\xe6\x89\xb0\n'"
4.Logistic_regression Logistic回归/logistic.py,2,"b'import numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nfrom torch import nn\nfrom torch import optim\n\nfr=open(""data.txt"")\nlines=fr.readlines()\n\nlines=[x.split(\'\\n\')[0] for x in lines]\n#print(lines)\nlines=[x.split(\',\') for x in lines]\nx_data=np.array([[float(i[0]),float(i[1])] for i in lines],dtype=np.float32)\ny_data=np.array([[float(i[2])] for i in lines ],dtype=np.float32)\nx_data=torch.from_numpy(x_data)\ny_data=torch.from_numpy(y_data)\n\n#\xe5\xae\x9a\xe4\xb9\x89Logistic \xe5\x9b\x9e\xe5\xbd\x92\xe6\xa8\xa1\xe5\x9e\x8b\nclass LogisticRegression(nn.Module):\n    def __init__(self):\n        super(LogisticRegression,self).__init__()\n        self.lr=nn.Linear(2,1)\n        self.sm=nn.Sigmoid()\n\n    def forward(self,x):\n        x=self.lr(x)\n        x=self.sm(x)\n        return x\nlogistic_model=LogisticRegression()\n#BCE \xe6\x98\xaf\xe4\xba\x8c\xe5\x88\x86\xe7\xb1\xbb\xe7\x9a\x84\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x87\xbd\xe6\x95\xb0\ncriterion=nn.BCELoss()\noptimizer=optim.SGD(logistic_model.parameters(),lr=1e-3,momentum=0.9)\n\nfor epoch in range(5000):\n    x = x_data\n    y = y_data\n\n    #\xe5\x89\x8d\xe5\x90\x91\n    out=logistic_model(x)\n    loss=criterion(out,y)\n    print_loss=loss.item()\n\n    print(""item:"",loss.item())\n    mask=out.ge(0.5).float() #\xe8\xbe\x93\xe5\x87\xba\xe5\xa4\xa7\xe4\xba\x8e0.5 \xe5\xb0\xb1\xe6\x98\xaf1 \xef\xbc\x8c\xe5\xb0\x8f\xe4\xba\x8e0.5 \xe5\xb0\xb1\xe6\x98\xaf0\n\n    correct=(mask==y).sum()\n    # if epoch + 1 == 1000:\n    #     print(correct)\n    acc=correct.item()/x.size(0)\n\n    #\xe5\x90\x8e\xe5\x90\x91\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    if(epoch+1)%1000==0:\n        print(""*""*10)\n        print(""epoch:{}"".format(epoch+1))\n        print(""loss is {:.4f}"".format(print_loss))\n        print(""acc is{:.4f}"".format(acc))\n'"
5.Optimizer 优化器/Adadelta.py,7,"b""#%% [markdown]\n# # Adadelta\n# Adadelta \xe7\xae\x97\xe6\x98\xaf Adagrad \xe6\xb3\x95\xe7\x9a\x84\xe5\xbb\xb6\xe4\xbc\xb8\xef\xbc\x8c\xe5\xae\x83\xe8\xb7\x9f RMSProp \xe4\xb8\x80\xe6\xa0\xb7\xef\xbc\x8c\xe9\x83\xbd\xe6\x98\xaf\xe4\xb8\xba\xe4\xba\x86\xe8\xa7\xa3\xe5\x86\xb3 Adagrad \xe4\xb8\xad\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\xe4\xb8\x8d\xe6\x96\xad\xe5\x87\x8f\xe5\xb0\x8f\xe7\x9a\x84\xe9\x97\xae\xe9\xa2\x98\xef\xbc\x8cRMSProp \xe6\x98\xaf\xe9\x80\x9a\xe8\xbf\x87\xe7\xa7\xbb\xe5\x8a\xa8\xe5\x8a\xa0\xe6\x9d\x83\xe5\xb9\xb3\xe5\x9d\x87\xe7\x9a\x84\xe6\x96\xb9\xe5\xbc\x8f\xef\xbc\x8c\xe8\x80\x8c Adadelta \xe4\xb9\x9f\xe6\x98\xaf\xe4\xb8\x80\xe7\xa7\x8d\xe6\x96\xb9\xe6\xb3\x95\xef\xbc\x8c\xe6\x9c\x89\xe8\xb6\xa3\xe7\x9a\x84\xe6\x98\xaf\xef\xbc\x8c\xe5\xae\x83\xe5\xb9\xb6\xe4\xb8\x8d\xe9\x9c\x80\xe8\xa6\x81\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\xe8\xbf\x99\xe4\xb8\xaa\xe5\x8f\x82\xe6\x95\xb0\xe3\x80\x82\n# \n# ## Adadelta \xe6\xb3\x95\n# Adadelta \xe8\xb7\x9f RMSProp \xe4\xb8\x80\xe6\xa0\xb7\xef\xbc\x8c\xe5\x85\x88\xe4\xbd\xbf\xe7\x94\xa8\xe7\xa7\xbb\xe5\x8a\xa8\xe5\xb9\xb3\xe5\x9d\x87\xe6\x9d\xa5\xe8\xae\xa1\xe7\xae\x97 s\n# \n# $$\n# s = \\rho s + (1 - \\rho) g^2\n# $$\n# \n# \xe8\xbf\x99\xe9\x87\x8c $\\rho$ \xe5\x92\x8c RMSProp \xe4\xb8\xad\xe7\x9a\x84 $\\alpha$ \xe9\x83\xbd\xe6\x98\xaf\xe7\xa7\xbb\xe5\x8a\xa8\xe5\xb9\xb3\xe5\x9d\x87\xe7\xb3\xbb\xe6\x95\xb0\xef\xbc\x8cg \xe6\x98\xaf\xe5\x8f\x82\xe6\x95\xb0\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\xef\xbc\x8c\xe7\x84\xb6\xe5\x90\x8e\xe6\x88\x91\xe4\xbb\xac\xe4\xbc\x9a\xe8\xae\xa1\xe7\xae\x97\xe9\x9c\x80\xe8\xa6\x81\xe6\x9b\xb4\xe6\x96\xb0\xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0\xe7\x9a\x84\xe5\x8f\x98\xe5\x8c\x96\xe9\x87\x8f\n# \n# $$\n# g' = \\frac{\\sqrt{\\Delta \\theta + \\epsilon}}{\\sqrt{s + \\epsilon}} g\n# $$\n# \n# $\\Delta \\theta$ \xe5\x88\x9d\xe5\xa7\x8b\xe4\xb8\xba 0 \xe5\xbc\xa0\xe9\x87\x8f\xef\xbc\x8c\xe6\xaf\x8f\xe4\xb8\x80\xe6\xad\xa5\xe5\x81\x9a\xe5\xa6\x82\xe4\xb8\x8b\xe7\x9a\x84\xe6\x8c\x87\xe6\x95\xb0\xe5\x8a\xa0\xe6\x9d\x83\xe7\xa7\xbb\xe5\x8a\xa8\xe5\xb9\xb3\xe5\x9d\x87\xe6\x9b\xb4\xe6\x96\xb0\n# \n# $$\n# \\Delta \\theta = \\rho \\Delta \\theta + (1 - \\rho) g'^2\n# $$\n# \n# \xe6\x9c\x80\xe5\x90\x8e\xe5\x8f\x82\xe6\x95\xb0\xe6\x9b\xb4\xe6\x96\xb0\xe5\xa6\x82\xe4\xb8\x8b\n# \n# $$\n# \\theta = \\theta - g'\n# $$\n# \n# \xe4\xb8\x8b\xe9\x9d\xa2\xe6\x88\x91\xe4\xbb\xac\xe5\xae\x9e\xe7\x8e\xb0\xe4\xbb\xa5\xe4\xb8\x8b Adadelta\n\n#%%\ndef adadelta(parameters, sqrs, deltas, rho):\n    eps = 1e-6\n    for param, sqr, delta in zip(parameters, sqrs, deltas):\n        sqr[:] = rho * sqr + (1 - rho) * param.grad.data ** 2\n        cur_delta = torch.sqrt(delta + eps) / torch.sqrt(sqr + eps) * param.grad.data\n        delta[:] = rho * delta + (1 - rho) * cur_delta ** 2\n        param.data = param.data - cur_delta\n\n\n#%%\nimport numpy as np\nimport torch\nfrom torchvision.datasets import MNIST # \xe5\xaf\xbc\xe5\x85\xa5 pytorch \xe5\x86\x85\xe7\xbd\xae\xe7\x9a\x84 mnist \xe6\x95\xb0\xe6\x8d\xae\nfrom torch.utils.data import DataLoader\nfrom torch import nn\nimport time\nimport matplotlib.pyplot as plt\nget_ipython().run_line_magic('matplotlib', 'inline')\n\ndef data_tf(x):\n    x = np.array(x, dtype='float32') / 255\n    x = (x - 0.5) / 0.5 # \xe6\xa0\x87\xe5\x87\x86\xe5\x8c\x96\xef\xbc\x8c\xe8\xbf\x99\xe4\xb8\xaa\xe6\x8a\x80\xe5\xb7\xa7\xe4\xb9\x8b\xe5\x90\x8e\xe4\xbc\x9a\xe8\xae\xb2\xe5\x88\xb0\n    x = x.reshape((-1,)) # \xe6\x8b\x89\xe5\xb9\xb3\n    x = torch.from_numpy(x)\n    return x\n\ntrain_set = MNIST(r'C:/DATASETS', train=True, transform=data_tf, download=True) # \xe8\xbd\xbd\xe5\x85\xa5\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xef\xbc\x8c\xe7\x94\xb3\xe6\x98\x8e\xe5\xae\x9a\xe4\xb9\x89\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe5\x8f\x98\xe6\x8d\xa2\ntest_set = MNIST(r'C:/DATASETS', train=False, transform=data_tf, download=True)\n\n# \xe5\xae\x9a\xe4\xb9\x89 loss \xe5\x87\xbd\xe6\x95\xb0\ncriterion = nn.CrossEntropyLoss()\n\n\n#%%\ntrain_data = DataLoader(train_set, batch_size=64, shuffle=True)\n# \xe4\xbd\xbf\xe7\x94\xa8 Sequential \xe5\xae\x9a\xe4\xb9\x89 3 \xe5\xb1\x82\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\nnet = nn.Sequential(\n    nn.Linear(784, 200),\n    nn.ReLU(),\n    nn.Linear(200, 10),\n)\n\n# \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe6\xa2\xaf\xe5\xba\xa6\xe5\xb9\xb3\xe6\x96\xb9\xe9\xa1\xb9\xe5\x92\x8c delta \xe9\xa1\xb9\nsqrs = []\ndeltas = []\nfor param in net.parameters():\n    sqrs.append(torch.zeros_like(param.data))\n    deltas.append(torch.zeros_like(param.data))\n\n# \xe5\xbc\x80\xe5\xa7\x8b\xe8\xae\xad\xe7\xbb\x83\nlosses = []\nidx = 0\nstart = time.time() # \xe8\xae\xb0\xe6\x97\xb6\xe5\xbc\x80\xe5\xa7\x8b\nfor e in range(5):\n    train_loss = 0\n    for im, label in train_data:\n        # \xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\n        out = net(im)\n        loss = criterion(out, label)\n        # \xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\n        net.zero_grad()\n        loss.backward()\n        adadelta(net.parameters(), sqrs, deltas, 0.9) # rho \xe8\xae\xbe\xe7\xbd\xae\xe4\xb8\xba 0.9\n        # \xe8\xae\xb0\xe5\xbd\x95\xe8\xaf\xaf\xe5\xb7\xae\n        train_loss += loss.item()\n        if idx % 30 == 0:\n            losses.append(loss.item())\n        idx += 1\n    print('epoch: {}, Train Loss: {:.6f}'.format(e, train_loss / len(train_data)))\nend = time.time() # \xe8\xae\xa1\xe6\x97\xb6\xe7\xbb\x93\xe6\x9d\x9f\nprint('\xe4\xbd\xbf\xe7\x94\xa8\xe6\x97\xb6\xe9\x97\xb4: {:.5f} s'.format(end - start))\n\n\n#%%\nx_axis = np.linspace(0, 5, len(losses), endpoint=True)\nplt.semilogy(x_axis, losses, label='rho=0.99')\nplt.legend(loc='best')\n\n#%% [markdown]\n# \xe5\x8f\xaf\xe4\xbb\xa5\xe7\x9c\x8b\xe5\x88\xb0\xe4\xbd\xbf\xe7\x94\xa8 adadelta \xe8\xb7\x91 5 \xe6\xac\xa1\xe8\x83\xbd\xe5\xa4\x9f\xe5\xbe\x97\xe5\x88\xb0\xe6\x9b\xb4\xe5\xb0\x8f\xe7\x9a\x84 loss\n#%% [markdown]\n# **\xe5\xb0\x8f\xe7\xbb\x83\xe4\xb9\xa0\xef\xbc\x9a\xe6\x80\x9d\xe8\x80\x83\xe4\xb8\x80\xe4\xb8\x8b\xe4\xb8\xba\xe4\xbb\x80\xe4\xb9\x88 Adadelta \xe6\xb2\xa1\xe6\x9c\x89\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\xe8\xbf\x99\xe4\xb8\xaa\xe5\x8f\x82\xe6\x95\xb0\xef\xbc\x8c\xe5\xae\x83\xe6\x98\xaf\xe8\xa2\xab\xe4\xbb\x80\xe4\xb9\x88\xe4\xbb\xa3\xe6\x9b\xbf\xe4\xba\x86**\n#%% [markdown]\n# \xe5\xbd\x93\xe7\x84\xb6 pytorch \xe4\xb9\x9f\xe5\x86\x85\xe7\xbd\xae\xe4\xba\x86 adadelta \xe7\x9a\x84\xe6\x96\xb9\xe6\xb3\x95\xef\xbc\x8c\xe9\x9d\x9e\xe5\xb8\xb8\xe7\xae\x80\xe5\x8d\x95\xef\xbc\x8c\xe5\x8f\xaa\xe9\x9c\x80\xe8\xa6\x81\xe8\xb0\x83\xe7\x94\xa8 `torch.optim.Adadelta()` \xe5\xb0\xb1\xe5\x8f\xaf\xe4\xbb\xa5\xe4\xba\x86\xef\xbc\x8c\xe4\xb8\x8b\xe9\x9d\xa2\xe6\x98\xaf\xe4\xbe\x8b\xe5\xad\x90\n\n#%%\ntrain_data = DataLoader(train_set, batch_size=64, shuffle=True)\n# \xe4\xbd\xbf\xe7\x94\xa8 Sequential \xe5\xae\x9a\xe4\xb9\x89 3 \xe5\xb1\x82\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\nnet = nn.Sequential(\n    nn.Linear(784, 200),\n    nn.ReLU(),\n    nn.Linear(200, 10),\n)\n\noptimizer = torch.optim.Adadelta(net.parameters(), rho=0.9)\n\n# \xe5\xbc\x80\xe5\xa7\x8b\xe8\xae\xad\xe7\xbb\x83\nstart = time.time() # \xe8\xae\xb0\xe6\x97\xb6\xe5\xbc\x80\xe5\xa7\x8b\nfor e in range(5):\n    train_loss = 0\n    for im, label in train_data:\n        # \xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\n        out = net(im)\n        loss = criterion(out, label)\n        # \xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        # \xe8\xae\xb0\xe5\xbd\x95\xe8\xaf\xaf\xe5\xb7\xae\n        train_loss += loss.item()\n    print('epoch: {}, Train Loss: {:.6f}'.format(e, train_loss / len(train_data)))\nend = time.time() # \xe8\xae\xa1\xe6\x97\xb6\xe7\xbb\x93\xe6\x9d\x9f\nprint('\xe4\xbd\xbf\xe7\x94\xa8\xe6\x97\xb6\xe9\x97\xb4: {:.5f} s'.format(end - start))\n\n#%% [markdown]\n# **\xe5\xb0\x8f\xe7\xbb\x83\xe4\xb9\xa0\xef\xbc\x9a\xe7\x9c\x8b\xe7\x9c\x8b pytorch \xe4\xb8\xad\xe7\x9a\x84 adadelta\xef\xbc\x8c\xe9\x87\x8c\xe9\x9d\xa2\xe6\x98\xaf\xe6\x9c\x89\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\xe8\xbf\x99\xe4\xb8\xaa\xe5\x8f\x82\xe6\x95\xb0\xef\xbc\x8c\xe4\xbd\x86\xe6\x98\xaf\xe5\x89\x8d\xe9\x9d\xa2\xe6\x88\x91\xe4\xbb\xac\xe8\xae\xb2\xe8\xbf\x87 adadelta \xe4\xb8\x8d\xe7\x94\xa8\xe8\xae\xbe\xe7\xbd\xae\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\xef\xbc\x8c\xe7\x9c\x8b\xe7\x9c\x8b\xe8\xbf\x99\xe4\xb8\xaa\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\xe5\x88\xb0\xe5\xba\x95\xe6\x98\xaf\xe5\xb9\xb2\xe5\x98\x9b\xe7\x9a\x84**\n\n"""
5.Optimizer 优化器/Adagrad.py,6,"b'#%% [markdown]\n# # Adagrad\n# \xe8\xbf\x99\xe4\xb8\xaa\xe4\xbc\x98\xe5\x8c\x96\xe7\xae\x97\xe6\xb3\x95\xe8\xa2\xab\xe7\xa7\xb0\xe4\xb8\xba\xe8\x87\xaa\xe9\x80\x82\xe5\xba\x94\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\xe4\xbc\x98\xe5\x8c\x96\xe7\xae\x97\xe6\xb3\x95\xef\xbc\x8c\xe4\xb9\x8b\xe5\x89\x8d\xe6\x88\x91\xe4\xbb\xac\xe8\xae\xb2\xe7\x9a\x84\xe9\x9a\x8f\xe6\x9c\xba\xe6\xa2\xaf\xe5\xba\xa6\xe4\xb8\x8b\xe9\x99\x8d\xe4\xbb\xa5\xe5\x8f\x8a\xe5\x8a\xa8\xe9\x87\x8f\xe6\xb3\x95\xe5\xaf\xb9\xe6\x89\x80\xe6\x9c\x89\xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0\xe9\x83\xbd\xe4\xbd\xbf\xe7\x94\xa8\xe7\x9a\x84\xe5\x9b\xba\xe5\xae\x9a\xe7\x9a\x84\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\xe8\xbf\x9b\xe8\xa1\x8c\xe5\x8f\x82\xe6\x95\xb0\xe6\x9b\xb4\xe6\x96\xb0\xef\xbc\x8c\xe4\xbd\x86\xe6\x98\xaf\xe4\xb8\x8d\xe5\x90\x8c\xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0\xe6\xa2\xaf\xe5\xba\xa6\xe5\x8f\xaf\xe8\x83\xbd\xe4\xb8\x8d\xe4\xb8\x80\xe6\xa0\xb7\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe9\x9c\x80\xe8\xa6\x81\xe4\xb8\x8d\xe5\x90\x8c\xe7\x9a\x84\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\xe6\x89\x8d\xe8\x83\xbd\xe6\xaf\x94\xe8\xbe\x83\xe5\xa5\xbd\xe7\x9a\x84\xe8\xbf\x9b\xe8\xa1\x8c\xe8\xae\xad\xe7\xbb\x83\xef\xbc\x8c\xe4\xbd\x86\xe6\x98\xaf\xe8\xbf\x99\xe4\xb8\xaa\xe4\xba\x8b\xe6\x83\x85\xe5\x8f\x88\xe4\xb8\x8d\xe8\x83\xbd\xe5\xbe\x88\xe5\xa5\xbd\xe5\x9c\xb0\xe8\xa2\xab\xe4\xba\xba\xe4\xb8\xba\xe6\x93\x8d\xe4\xbd\x9c\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5 Adagrad \xe4\xbe\xbf\xe8\x83\xbd\xe5\xa4\x9f\xe5\xb8\xae\xe5\x8a\xa9\xe6\x88\x91\xe4\xbb\xac\xe5\x81\x9a\xe8\xbf\x99\xe4\xbb\xb6\xe4\xba\x8b\xe3\x80\x82\n# \n# ## Adagrad \xe7\xae\x97\xe6\xb3\x95\n# Adagrad \xe7\x9a\x84\xe6\x83\xb3\xe6\xb3\x95\xe9\x9d\x9e\xe5\xb8\xb8\xe7\xae\x80\xe5\x8d\x95\xef\xbc\x8c\xe5\x9c\xa8\xe6\xaf\x8f\xe6\xac\xa1\xe4\xbd\xbf\xe7\x94\xa8\xe4\xb8\x80\xe4\xb8\xaa batch size \xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe8\xbf\x9b\xe8\xa1\x8c\xe5\x8f\x82\xe6\x95\xb0\xe6\x9b\xb4\xe6\x96\xb0\xe7\x9a\x84\xe6\x97\xb6\xe5\x80\x99\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe9\x9c\x80\xe8\xa6\x81\xe8\xae\xa1\xe7\xae\x97\xe6\x89\x80\xe6\x9c\x89\xe5\x8f\x82\xe6\x95\xb0\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\xef\xbc\x8c\xe9\x82\xa3\xe4\xb9\x88\xe5\x85\xb6\xe6\x83\xb3\xe6\xb3\x95\xe5\xb0\xb1\xe6\x98\xaf\xe5\xaf\xb9\xe4\xba\x8e\xe6\xaf\x8f\xe4\xb8\xaa\xe5\x8f\x82\xe6\x95\xb0\xef\xbc\x8c\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe4\xb8\x80\xe4\xb8\xaa\xe5\x8f\x98\xe9\x87\x8f s \xe4\xb8\xba 0\xef\xbc\x8c\xe7\x84\xb6\xe5\x90\x8e\xe6\xaf\x8f\xe6\xac\xa1\xe5\xb0\x86\xe8\xaf\xa5\xe5\x8f\x82\xe6\x95\xb0\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\xe5\xb9\xb3\xe6\x96\xb9\xe6\xb1\x82\xe5\x92\x8c\xe7\xb4\xaf\xe5\x8a\xa0\xe5\x88\xb0\xe8\xbf\x99\xe4\xb8\xaa\xe5\x8f\x98\xe9\x87\x8f s \xe4\xb8\x8a\xef\xbc\x8c\xe7\x84\xb6\xe5\x90\x8e\xe5\x9c\xa8\xe6\x9b\xb4\xe6\x96\xb0\xe8\xbf\x99\xe4\xb8\xaa\xe5\x8f\x82\xe6\x95\xb0\xe7\x9a\x84\xe6\x97\xb6\xe5\x80\x99\xef\xbc\x8c\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\xe5\xb0\xb1\xe5\x8f\x98\xe4\xb8\xba\n# \n# $$\n# \\frac{\\eta}{\\sqrt{s + \\epsilon}}\n# $$\n# \n# \xe8\xbf\x99\xe9\x87\x8c\xe7\x9a\x84 $\\epsilon$ \xe6\x98\xaf\xe4\xb8\xba\xe4\xba\x86\xe6\x95\xb0\xe5\x80\xbc\xe7\xa8\xb3\xe5\xae\x9a\xe6\x80\xa7\xe8\x80\x8c\xe5\x8a\xa0\xe4\xb8\x8a\xe7\x9a\x84\xef\xbc\x8c\xe5\x9b\xa0\xe4\xb8\xba\xe6\x9c\x89\xe5\x8f\xaf\xe8\x83\xbd s \xe7\x9a\x84\xe5\x80\xbc\xe4\xb8\xba 0\xef\xbc\x8c\xe9\x82\xa3\xe4\xb9\x88 0 \xe5\x87\xba\xe7\x8e\xb0\xe5\x9c\xa8\xe5\x88\x86\xe6\xaf\x8d\xe5\xb0\xb1\xe4\xbc\x9a\xe5\x87\xba\xe7\x8e\xb0\xe6\x97\xa0\xe7\xa9\xb7\xe5\xa4\xa7\xe7\x9a\x84\xe6\x83\x85\xe5\x86\xb5\xef\xbc\x8c\xe9\x80\x9a\xe5\xb8\xb8 $\\epsilon$ \xe5\x8f\x96 $10^{-10}$\xef\xbc\x8c\xe8\xbf\x99\xe6\xa0\xb7\xe4\xb8\x8d\xe5\x90\x8c\xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0\xe7\x94\xb1\xe4\xba\x8e\xe6\xa2\xaf\xe5\xba\xa6\xe4\xb8\x8d\xe5\x90\x8c\xef\xbc\x8c\xe4\xbb\x96\xe4\xbb\xac\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84 s \xe5\xa4\xa7\xe5\xb0\x8f\xe4\xb9\x9f\xe5\xb0\xb1\xe4\xb8\x8d\xe5\x90\x8c\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe4\xb8\x8a\xe9\x9d\xa2\xe7\x9a\x84\xe5\x85\xac\xe5\xbc\x8f\xe5\xbe\x97\xe5\x88\xb0\xe7\x9a\x84\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\xe4\xb9\x9f\xe5\xb0\xb1\xe4\xb8\x8d\xe5\x90\x8c\xef\xbc\x8c\xe8\xbf\x99\xe4\xb9\x9f\xe5\xb0\xb1\xe5\xae\x9e\xe7\x8e\xb0\xe4\xba\x86\xe8\x87\xaa\xe9\x80\x82\xe5\xba\x94\xe7\x9a\x84\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\xe3\x80\x82\n# \n# \n#%% [markdown]\n# Adagrad \xe7\x9a\x84\xe6\xa0\xb8\xe5\xbf\x83\xe6\x83\xb3\xe6\xb3\x95\xe5\xb0\xb1\xe6\x98\xaf\xef\xbc\x8c\xe5\xa6\x82\xe6\x9e\x9c\xe4\xb8\x80\xe4\xb8\xaa\xe5\x8f\x82\xe6\x95\xb0\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\xe4\xb8\x80\xe7\x9b\xb4\xe9\x83\xbd\xe9\x9d\x9e\xe5\xb8\xb8\xe5\xa4\xa7\xef\xbc\x8c\xe9\x82\xa3\xe4\xb9\x88\xe5\x85\xb6\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\xe5\xb0\xb1\xe5\x8f\x98\xe5\xb0\x8f\xe4\xb8\x80\xe7\x82\xb9\xef\xbc\x8c\xe9\x98\xb2\xe6\xad\xa2\xe9\x9c\x87\xe8\x8d\xa1\xef\xbc\x8c\xe8\x80\x8c\xe4\xb8\x80\xe4\xb8\xaa\xe5\x8f\x82\xe6\x95\xb0\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\xe4\xb8\x80\xe7\x9b\xb4\xe9\x83\xbd\xe9\x9d\x9e\xe5\xb8\xb8\xe5\xb0\x8f\xef\xbc\x8c\xe9\x82\xa3\xe4\xb9\x88\xe8\xbf\x99\xe4\xb8\xaa\xe5\x8f\x82\xe6\x95\xb0\xe7\x9a\x84\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\xe5\xb0\xb1\xe5\x8f\x98\xe5\xa4\xa7\xe4\xb8\x80\xe7\x82\xb9\xef\xbc\x8c\xe4\xbd\xbf\xe5\xbe\x97\xe5\x85\xb6\xe8\x83\xbd\xe5\xa4\x9f\xe6\x9b\xb4\xe5\xbf\xab\xe5\x9c\xb0\xe6\x9b\xb4\xe6\x96\xb0\n# \n# Adagrad \xe4\xb9\x9f\xe6\x9c\x89\xe4\xb8\x80\xe4\xba\x9b\xe9\x97\xae\xe9\xa2\x98\xef\xbc\x8c\xe5\x9b\xa0\xe4\xb8\xba s \xe4\xb8\x8d\xe6\x96\xad\xe7\xb4\xaf\xe5\x8a\xa0\xe6\xa2\xaf\xe5\xba\xa6\xe7\x9a\x84\xe5\xb9\xb3\xe6\x96\xb9\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe4\xbc\x9a\xe8\xb6\x8a\xe6\x9d\xa5\xe8\xb6\x8a\xe5\xa4\xa7\xef\xbc\x8c\xe5\xaf\xbc\xe8\x87\xb4\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\xe5\x9c\xa8\xe5\x90\x8e\xe6\x9c\x9f\xe4\xbc\x9a\xe5\x8f\x98\xe5\xbe\x97\xe8\xbe\x83\xe5\xb0\x8f\xef\xbc\x8c\xe5\xaf\xbc\xe8\x87\xb4\xe6\x94\xb6\xe6\x95\x9b\xe4\xb9\x8f\xe5\x8a\x9b\xe7\x9a\x84\xe6\x83\x85\xe5\x86\xb5\xef\xbc\x8c\xe5\x8f\xaf\xe8\x83\xbd\xe6\x97\xa0\xe6\xb3\x95\xe6\x94\xb6\xe6\x95\x9b\xe5\x88\xb0\xe8\xa1\xa8\xe8\xbe\x83\xe5\xa5\xbd\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x9c\xef\xbc\x8c\xe5\xbd\x93\xe7\x84\xb6\xe5\x90\x8e\xe9\x9d\xa2\xe6\x9c\x89\xe4\xb8\x80\xe4\xb8\xaa\xe5\xaf\xb9\xe5\x85\xb6\xe7\x9a\x84\xe6\x94\xb9\xe8\xbf\x9b\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe4\xb9\x8b\xe5\x90\x8e\xe4\xbc\x9a\xe8\xae\xb2\xe5\x88\xb0\n# \n# \xe4\xb8\x8b\xe9\x9d\xa2\xe6\x88\x91\xe4\xbb\xac\xe6\x9d\xa5\xe5\xae\x9e\xe7\x8e\xb0\xe4\xb8\x80\xe4\xb8\x8b Adagrad \xe7\x9a\x84\xe7\xae\x97\xe6\xb3\x95\n\n#%%\ndef sgd_adagrad(parameters, sqrs, lr):\n    eps = 1e-10\n    for param, sqr in zip(parameters, sqrs):\n        sqr[:] = sqr + param.grad.data ** 2\n        div = lr / torch.sqrt(sqr + eps) * param.grad.data\n        param.data = param.data - div\n\n\n#%%\nimport numpy as np\nimport torch\nfrom torchvision.datasets import MNIST # \xe5\xaf\xbc\xe5\x85\xa5 pytorch \xe5\x86\x85\xe7\xbd\xae\xe7\x9a\x84 mnist \xe6\x95\xb0\xe6\x8d\xae\nfrom torch.utils.data import DataLoader\nfrom torch import nn\nimport time\nimport matplotlib.pyplot as plt\nget_ipython().run_line_magic(\'matplotlib\', \'inline\')\n\ndef data_tf(x):\n    x = np.array(x, dtype=\'float32\') / 255\n    x = (x - 0.5) / 0.5 # \xe6\xa0\x87\xe5\x87\x86\xe5\x8c\x96\xef\xbc\x8c\xe8\xbf\x99\xe4\xb8\xaa\xe6\x8a\x80\xe5\xb7\xa7\xe4\xb9\x8b\xe5\x90\x8e\xe4\xbc\x9a\xe8\xae\xb2\xe5\x88\xb0\n    x = x.reshape((-1,)) # \xe6\x8b\x89\xe5\xb9\xb3\n    x = torch.from_numpy(x)\n    return x\n\ntrain_set = MNIST(r""C:/DATASETS"", train=True, transform=data_tf, download=True) # \xe8\xbd\xbd\xe5\x85\xa5\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xef\xbc\x8c\xe7\x94\xb3\xe6\x98\x8e\xe5\xae\x9a\xe4\xb9\x89\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe5\x8f\x98\xe6\x8d\xa2\ntest_set = MNIST(r""C:/DATASETS"", train=False, transform=data_tf, download=True)\n\n# \xe5\xae\x9a\xe4\xb9\x89 loss \xe5\x87\xbd\xe6\x95\xb0\ncriterion = nn.CrossEntropyLoss()\n\n\n#%%\ntrain_data = DataLoader(train_set, batch_size=64, shuffle=True)\n# \xe4\xbd\xbf\xe7\x94\xa8 Sequential \xe5\xae\x9a\xe4\xb9\x89 3 \xe5\xb1\x82\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\nnet = nn.Sequential(\n    nn.Linear(784, 200),\n    nn.ReLU(),\n    nn.Linear(200, 10),\n)\n\n# \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe6\xa2\xaf\xe5\xba\xa6\xe5\xb9\xb3\xe6\x96\xb9\xe9\xa1\xb9\nsqrs = []\nfor param in net.parameters():\n    sqrs.append(torch.zeros_like(param.data))\n    \n# \xe5\xbc\x80\xe5\xa7\x8b\xe8\xae\xad\xe7\xbb\x83\nlosses = []\nidx = 0\nstart = time.time() # \xe8\xae\xb0\xe6\x97\xb6\xe5\xbc\x80\xe5\xa7\x8b\nfor e in range(5):\n    train_loss = 0\n    for im, label in train_data:\n        # \xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\n        out = net(im)\n        loss = criterion(out, label)\n        # \xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\n        net.zero_grad()\n        loss.backward()\n        sgd_adagrad(net.parameters(), sqrs, 1e-2) # \xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\xe8\xae\xbe\xe4\xb8\xba 0.01\n        # \xe8\xae\xb0\xe5\xbd\x95\xe8\xaf\xaf\xe5\xb7\xae\n        train_loss += loss.item()\n        if idx % 30 == 0:\n            losses.append(loss.item())\n        idx += 1\n    print(\'epoch: {}, Train Loss: {:.6f}\'.format(e, train_loss / len(train_data)))\nend = time.time() # \xe8\xae\xa1\xe6\x97\xb6\xe7\xbb\x93\xe6\x9d\x9f\nprint(\'\xe4\xbd\xbf\xe7\x94\xa8\xe6\x97\xb6\xe9\x97\xb4: {:.5f} s\'.format(end - start))\n\n\n#%%\nx_axis = np.linspace(0, 5, len(losses), endpoint=True)\nplt.semilogy(x_axis, losses, label=\'adagrad\')\nplt.legend(loc=\'best\')\n\n#%% [markdown]\n# \xe5\x8f\xaf\xe4\xbb\xa5\xe7\x9c\x8b\xe5\x88\xb0\xef\xbc\x8c\xe4\xbd\xbf\xe7\x94\xa8\xe8\x87\xaa\xe9\x80\x82\xe5\xba\x94\xe7\x9a\x84\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\xe8\xb7\x91 5 \xe4\xb8\xaa epoch \xe5\x8f\xaf\xe4\xbb\xa5\xe5\xbe\x97\xe5\x88\xb0\xe6\xaf\x94\xe9\x9a\x8f\xe6\x9c\xba\xe6\xa2\xaf\xe5\xba\xa6\xe4\xb8\x8b\xe9\x99\x8d\xe5\xbe\x97\xe5\x88\xb0\xe6\x9b\xb4\xe5\xb0\x8f\xe7\x9a\x84 loss\xef\xbc\x8c\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\xe8\x83\xbd\xe5\xa4\x9f\xe8\x87\xaa\xe9\x80\x82\xe5\xba\x94\xe5\x9c\xb0\xe9\x99\x8d\xe4\xbd\x8e\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe8\x83\xbd\xe5\xa4\x9f\xe6\x9c\x89\xe7\x9d\x80\xe6\x9b\xb4\xe5\xa5\xbd\xe7\x9a\x84\xe6\x95\x88\xe6\x9e\x9c\n#%% [markdown]\n# \xe5\xbd\x93\xe7\x84\xb6 pytorch \xe4\xb9\x9f\xe5\x86\x85\xe7\xbd\xae\xe4\xba\x86 adagrad \xe7\x9a\x84\xe4\xbc\x98\xe5\x8c\x96\xe7\xae\x97\xe6\xb3\x95\xef\xbc\x8c\xe5\x8f\xaa\xe9\x9c\x80\xe8\xa6\x81\xe8\xb0\x83\xe7\x94\xa8 `torch.optim.Adagrad()` \xe5\xb0\xb1\xe5\x8f\xaf\xe4\xbb\xa5\xe4\xba\x86\xef\xbc\x8c\xe4\xb8\x8b\xe9\x9d\xa2\xe6\x98\xaf\xe4\xbe\x8b\xe5\xad\x90\n\n#%%\ntrain_data = DataLoader(train_set, batch_size=64, shuffle=True)\n# \xe4\xbd\xbf\xe7\x94\xa8 Sequential \xe5\xae\x9a\xe4\xb9\x89 3 \xe5\xb1\x82\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\nnet = nn.Sequential(\n    nn.Linear(784, 200),\n    nn.ReLU(),\n    nn.Linear(200, 10),\n)\n \noptimizer = torch.optim.Adagrad(net.parameters(), lr=1e-2)\n# \xe5\xbc\x80\xe5\xa7\x8b\xe8\xae\xad\xe7\xbb\x83\n\nstart = time.time() # \xe8\xae\xb0\xe6\x97\xb6\xe5\xbc\x80\xe5\xa7\x8b\nfor e in range(5):\n    train_loss = 0\n    for im, label in train_data:\n        # \xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\n        out = net(im)\n        loss = criterion(out, label)\n        # \xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        # \xe8\xae\xb0\xe5\xbd\x95\xe8\xaf\xaf\xe5\xb7\xae\n        train_loss += loss.item()\n    print(\'epoch: {}, Train Loss: {:.6f}\'.format(e, train_loss / len(train_data)))\nend = time.time() # \xe8\xae\xa1\xe6\x97\xb6\xe7\xbb\x93\xe6\x9d\x9f\nprint(\'\xe4\xbd\xbf\xe7\x94\xa8\xe6\x97\xb6\xe9\x97\xb4: {:.5f} s\'.format(end - start))\n\n\n'"
5.Optimizer 优化器/Adam.py,7,"b""#%% [markdown]\n# # Adam\n# Adam \xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa\xe7\xbb\x93\xe5\x90\x88\xe4\xba\x86\xe5\x8a\xa8\xe9\x87\x8f\xe6\xb3\x95\xe5\x92\x8c RMSProp \xe7\x9a\x84\xe4\xbc\x98\xe5\x8c\x96\xe7\xae\x97\xe6\xb3\x95\xef\xbc\x8c\xe5\x85\xb6\xe7\xbb\x93\xe5\x90\x88\xe4\xba\x86\xe4\xb8\xa4\xe8\x80\x85\xe7\x9a\x84\xe4\xbc\x98\xe7\x82\xb9\xe3\x80\x82\n# \n# ## Adam \xe7\xae\x97\xe6\xb3\x95\n# Adam \xe7\xae\x97\xe6\xb3\x95\xe4\xbc\x9a\xe4\xbd\xbf\xe7\x94\xa8\xe4\xb8\x80\xe4\xb8\xaa\xe5\x8a\xa8\xe9\x87\x8f\xe5\x8f\x98\xe9\x87\x8f v \xe5\x92\x8c\xe4\xb8\x80\xe4\xb8\xaa RMSProp \xe4\xb8\xad\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\xe5\x85\x83\xe7\xb4\xa0\xe5\xb9\xb3\xe6\x96\xb9\xe7\x9a\x84\xe7\xa7\xbb\xe5\x8a\xa8\xe6\x8c\x87\xe6\x95\xb0\xe5\x8a\xa0\xe6\x9d\x83\xe5\xb9\xb3\xe5\x9d\x87 s\xef\xbc\x8c\xe9\xa6\x96\xe5\x85\x88\xe5\xb0\x86\xe4\xbb\x96\xe4\xbb\xac\xe5\x85\xa8\xe9\x83\xa8\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe4\xb8\xba 0\xef\xbc\x8c\xe7\x84\xb6\xe5\x90\x8e\xe5\x9c\xa8\xe6\xaf\x8f\xe6\xac\xa1\xe8\xbf\xad\xe4\xbb\xa3\xe4\xb8\xad\xef\xbc\x8c\xe8\xae\xa1\xe7\xae\x97\xe4\xbb\x96\xe4\xbb\xac\xe7\x9a\x84\xe7\xa7\xbb\xe5\x8a\xa8\xe5\x8a\xa0\xe6\x9d\x83\xe5\xb9\xb3\xe5\x9d\x87\xe8\xbf\x9b\xe8\xa1\x8c\xe6\x9b\xb4\xe6\x96\xb0\n# \n# $$\n# v = \\beta_1 v + (1 - \\beta_1) g \\\\\n# s = \\beta_2 s + (1 - \\beta_2) g^2\n# $$\n# \n# \xe5\x9c\xa8 adam \xe7\xae\x97\xe6\xb3\x95\xe9\x87\x8c\xef\xbc\x8c\xe4\xb8\xba\xe4\xba\x86\xe5\x87\x8f\xe8\xbd\xbb v \xe5\x92\x8c s \xe8\xa2\xab\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe4\xb8\xba 0 \xe7\x9a\x84\xe5\x88\x9d\xe6\x9c\x9f\xe5\xaf\xb9\xe8\xae\xa1\xe7\xae\x97\xe6\x8c\x87\xe6\x95\xb0\xe5\x8a\xa0\xe6\x9d\x83\xe7\xa7\xbb\xe5\x8a\xa8\xe5\xb9\xb3\xe5\x9d\x87\xe7\x9a\x84\xe5\xbd\xb1\xe5\x93\x8d\xef\xbc\x8c\xe6\xaf\x8f\xe6\xac\xa1 v \xe5\x92\x8c s \xe9\x83\xbd\xe5\x81\x9a\xe4\xb8\x8b\xe9\x9d\xa2\xe7\x9a\x84\xe4\xbf\xae\xe6\xad\xa3\n# \n# $$\n# \\hat{v} = \\frac{v}{1 - \\beta_1^t} \\\\\n# \\hat{s} = \\frac{s}{1 - \\beta_2^t}\n# $$\n# \n# \xe8\xbf\x99\xe9\x87\x8c t \xe6\x98\xaf\xe8\xbf\xad\xe4\xbb\xa3\xe6\xac\xa1\xe6\x95\xb0\xef\xbc\x8c\xe5\x8f\xaf\xe4\xbb\xa5\xe7\x9c\x8b\xe5\x88\xb0\xef\xbc\x8c\xe5\xbd\x93 $0 \\leq \\beta_1, \\beta_2 \\leq 1$ \xe7\x9a\x84\xe6\x97\xb6\xe5\x80\x99\xef\xbc\x8c\xe8\xbf\xad\xe4\xbb\xa3\xe5\x88\xb0\xe5\x90\x8e\xe6\x9c\x9f t \xe6\xaf\x94\xe8\xbe\x83\xe5\xa4\xa7\xef\xbc\x8c\xe9\x82\xa3\xe4\xb9\x88 $\\beta_1^t$ \xe5\x92\x8c $\\beta_2^t$ \xe5\xb0\xb1\xe5\x87\xa0\xe4\xb9\x8e\xe4\xb8\xba 0\xef\xbc\x8c\xe5\xb0\xb1\xe4\xb8\x8d\xe4\xbc\x9a\xe5\xaf\xb9 v \xe5\x92\x8c s \xe6\x9c\x89\xe4\xbb\xbb\xe4\xbd\x95\xe5\xbd\xb1\xe5\x93\x8d\xe4\xba\x86\xef\xbc\x8c\xe7\xae\x97\xe6\xb3\x95\xe4\xbd\x9c\xe8\x80\x85\xe5\xbb\xba\xe8\xae\xae$\\beta_1 = 0.9$, $\\beta_2 = 0.999$\xe3\x80\x82\n# \n# \xe6\x9c\x80\xe5\x90\x8e\xe4\xbd\xbf\xe7\x94\xa8\xe4\xbf\xae\xe6\xad\xa3\xe4\xb9\x8b\xe5\x90\x8e\xe7\x9a\x84 $\\hat{v}$ \xe5\x92\x8c $\\hat{s}$ \xe8\xbf\x9b\xe8\xa1\x8c\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\xe7\x9a\x84\xe9\x87\x8d\xe6\x96\xb0\xe8\xae\xa1\xe7\xae\x97\n# \n# $$\n# g' = \\frac{\\eta \\hat{v}}{\\sqrt{\\hat{s} + \\epsilon}}\n# $$\n# \n# \xe8\xbf\x99\xe9\x87\x8c $\\eta$ \xe6\x98\xaf\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\xef\xbc\x8c$epsilon$ \xe4\xbb\x8d\xe7\x84\xb6\xe6\x98\xaf\xe4\xb8\xba\xe4\xba\x86\xe6\x95\xb0\xe5\x80\xbc\xe7\xa8\xb3\xe5\xae\x9a\xe6\x80\xa7\xe8\x80\x8c\xe6\xb7\xbb\xe5\x8a\xa0\xe7\x9a\x84\xe5\xb8\xb8\xe6\x95\xb0\xef\xbc\x8c\xe6\x9c\x80\xe5\x90\x8e\xe5\x8f\x82\xe6\x95\xb0\xe6\x9b\xb4\xe6\x96\xb0\xe6\x9c\x89\n# \n# $$\n# \\theta_i = \\theta_{i-1} - g'\n# $$\n#%% [markdown]\n# \xe4\xb8\x8b\xe9\x9d\xa2\xe6\x88\x91\xe4\xbb\xac\xe6\x9d\xa5\xe5\xae\x9e\xe7\x8e\xb0\xe4\xbb\xa5\xe4\xb8\x8b adam \xe7\xae\x97\xe6\xb3\x95\n\n#%%\ndef adam(parameters, vs, sqrs, lr, t, beta1=0.9, beta2=0.999):\n    eps = 1e-8\n    for param, v, sqr in zip(parameters, vs, sqrs):\n        v[:] = beta1 * v + (1 - beta1) * param.grad.data\n        sqr[:] = beta2 * sqr + (1 - beta2) * param.grad.data ** 2\n        v_hat = v / (1 - beta1 ** t)\n        s_hat = sqr / (1 - beta2 ** t)\n        param.data = param.data - lr * v_hat / torch.sqrt(s_hat + eps)\n\n\n#%%\nimport numpy as np\nimport torch\nfrom torchvision.datasets import MNIST # \xe5\xaf\xbc\xe5\x85\xa5 pytorch \xe5\x86\x85\xe7\xbd\xae\xe7\x9a\x84 mnist \xe6\x95\xb0\xe6\x8d\xae\nfrom torch.utils.data import DataLoader\nfrom torch import nn\nimport time\nimport matplotlib.pyplot as plt\nget_ipython().run_line_magic('matplotlib', 'inline')\n\ndef data_tf(x):\n    x = np.array(x, dtype='float32') / 255\n    x = (x - 0.5) / 0.5 # \xe6\xa0\x87\xe5\x87\x86\xe5\x8c\x96\xef\xbc\x8c\xe8\xbf\x99\xe4\xb8\xaa\xe6\x8a\x80\xe5\xb7\xa7\xe4\xb9\x8b\xe5\x90\x8e\xe4\xbc\x9a\xe8\xae\xb2\xe5\x88\xb0\n    x = x.reshape((-1,)) # \xe6\x8b\x89\xe5\xb9\xb3\n    x = torch.from_numpy(x)\n    return x\n\ntrain_set = MNIST('./data', train=True, transform=data_tf, download=True) # \xe8\xbd\xbd\xe5\x85\xa5\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xef\xbc\x8c\xe7\x94\xb3\xe6\x98\x8e\xe5\xae\x9a\xe4\xb9\x89\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe5\x8f\x98\xe6\x8d\xa2\ntest_set = MNIST('./data', train=False, transform=data_tf, download=True)\n\n# \xe5\xae\x9a\xe4\xb9\x89 loss \xe5\x87\xbd\xe6\x95\xb0\ncriterion = nn.CrossEntropyLoss()\n\n\n#%%\ntrain_data = DataLoader(train_set, batch_size=64, shuffle=True)\n# \xe4\xbd\xbf\xe7\x94\xa8 Sequential \xe5\xae\x9a\xe4\xb9\x89 3 \xe5\xb1\x82\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\nnet = nn.Sequential(\n    nn.Linear(784, 200),\n    nn.ReLU(),\n    nn.Linear(200, 10),\n)\n\n# \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe6\xa2\xaf\xe5\xba\xa6\xe5\xb9\xb3\xe6\x96\xb9\xe9\xa1\xb9\xe5\x92\x8c\xe5\x8a\xa8\xe9\x87\x8f\xe9\xa1\xb9\nsqrs = []\nvs = []\nfor param in net.parameters():\n    sqrs.append(torch.zeros_like(param.data))\n    vs.append(torch.zeros_like(param.data))\nt = 1\n# \xe5\xbc\x80\xe5\xa7\x8b\xe8\xae\xad\xe7\xbb\x83\nlosses = []\nidx = 0\n\nstart = time.time() # \xe8\xae\xb0\xe6\x97\xb6\xe5\xbc\x80\xe5\xa7\x8b\nfor e in range(5):\n    train_loss = 0\n    for im, label in train_data:\n        # \xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\n        out = net(im)\n        loss = criterion(out, label)\n        # \xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\n        net.zero_grad()\n        loss.backward()\n        adam(net.parameters(), vs, sqrs, 1e-3, t) # \xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\xe8\xae\xbe\xe4\xb8\xba 0.001\n        t += 1\n        # \xe8\xae\xb0\xe5\xbd\x95\xe8\xaf\xaf\xe5\xb7\xae\n        train_loss += loss.item()\n        if idx % 30 == 0:\n            losses.append(loss.item())\n        idx += 1\n    print('epoch: {}, Train Loss: {:.6f}'\n          .format(e, train_loss / len(train_data)))\nend = time.time() # \xe8\xae\xa1\xe6\x97\xb6\xe7\xbb\x93\xe6\x9d\x9f\nprint('\xe4\xbd\xbf\xe7\x94\xa8\xe6\x97\xb6\xe9\x97\xb4: {:.5f} s'.format(end - start))\n\n\n#%%\nx_axis = np.linspace(0, 5, len(losses), endpoint=True)\nplt.semilogy(x_axis, losses, label='adam')\nplt.legend(loc='best')\n\n#%% [markdown]\n# \xe5\x8f\xaf\xe4\xbb\xa5\xe7\x9c\x8b\xe5\x88\xb0\xe4\xbd\xbf\xe7\x94\xa8 adam \xe7\xae\x97\xe6\xb3\x95 loss \xe8\x83\xbd\xe5\xa4\x9f\xe6\x9b\xb4\xe5\xbf\xab\xe6\x9b\xb4\xe5\xa5\xbd\xe5\x9c\xb0\xe6\x94\xb6\xe6\x95\x9b\xef\xbc\x8c\xe4\xbd\x86\xe6\x98\xaf\xe4\xb8\x80\xe5\xae\x9a\xe8\xa6\x81\xe5\xb0\x8f\xe5\xbf\x83\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\xe7\x9a\x84\xe8\xae\xbe\xe5\xae\x9a\xef\xbc\x8c\xe4\xbd\xbf\xe7\x94\xa8\xe8\x87\xaa\xe9\x80\x82\xe5\xba\x94\xe7\x9a\x84\xe7\xae\x97\xe6\xb3\x95\xe4\xb8\x80\xe8\x88\xac\xe9\x9c\x80\xe8\xa6\x81\xe6\x9b\xb4\xe5\xb0\x8f\xe7\x9a\x84\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\n# \n# \xe5\xbd\x93\xe7\x84\xb6 pytorch \xe4\xb8\xad\xe4\xb9\x9f\xe5\x86\x85\xe7\xbd\xae\xe4\xba\x86 adam \xe7\x9a\x84\xe5\xae\x9e\xe7\x8e\xb0\xef\xbc\x8c\xe5\x8f\xaa\xe9\x9c\x80\xe8\xa6\x81\xe8\xb0\x83\xe7\x94\xa8 `torch.optim.Adam()`\xef\xbc\x8c\xe4\xb8\x8b\xe9\x9d\xa2\xe6\x98\xaf\xe4\xbe\x8b\xe5\xad\x90\n\n#%%\ntrain_data = DataLoader(train_set, batch_size=64, shuffle=True)\n# \xe4\xbd\xbf\xe7\x94\xa8 Sequential \xe5\xae\x9a\xe4\xb9\x89 3 \xe5\xb1\x82\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\nnet = nn.Sequential(\n    nn.Linear(784, 200),\n    nn.ReLU(),\n    nn.Linear(200, 10),\n)\n\noptimizer = torch.optim.Adam(net.parameters(), lr=1e-3)\n    \n# \xe5\xbc\x80\xe5\xa7\x8b\xe8\xae\xad\xe7\xbb\x83\nstart = time.time() # \xe8\xae\xb0\xe6\x97\xb6\xe5\xbc\x80\xe5\xa7\x8b\nfor e in range(5):\n    train_loss = 0\n    for im, label in train_data:\n        # \xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\n        out = net(im)\n        loss = criterion(out, label)\n        # \xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        # \xe8\xae\xb0\xe5\xbd\x95\xe8\xaf\xaf\xe5\xb7\xae\n        train_loss += loss.item()\n    print('epoch: {}, Train Loss: {:.6f}'.format(e, train_loss / len(train_data)))\nend = time.time() # \xe8\xae\xa1\xe6\x97\xb6\xe7\xbb\x93\xe6\x9d\x9f\nprint('\xe4\xbd\xbf\xe7\x94\xa8\xe6\x97\xb6\xe9\x97\xb4: {:.5f} s'.format(end - start))\n\n#%% [markdown]\n# \xe8\xbf\x99\xe6\x98\xaf\xe6\x88\x91\xe4\xbb\xac\xe8\xae\xb2\xe7\x9a\x84\xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x80\xe4\xb8\xaa\xe4\xbc\x98\xe5\x8c\x96\xe7\xae\x97\xe6\xb3\x95\xef\xbc\x8c\xe4\xb8\x8b\xe9\x9d\xa2\xe6\x94\xbe\xe4\xb8\x80\xe5\xbc\xa0\xe5\x90\x84\xe4\xb8\xaa\xe4\xbc\x98\xe5\x8c\x96\xe7\xae\x97\xe6\xb3\x95\xe7\x9a\x84\xe5\xaf\xb9\xe6\xaf\x94\xe5\x9b\xbe\xe7\xbb\x93\xe6\x9d\x9f\xe8\xbf\x99\xe4\xb8\x80\xe8\x8a\x82\xe7\x9a\x84\xe5\x86\x85\xe5\xae\xb9\n# \n# ![](https://raw.githubusercontent.com/cs231n/cs231n.github.io/master/assets/nn3/opt1.gif)\n# \n# ![](https://raw.githubusercontent.com/cs231n/cs231n.github.io/master/assets/nn3/opt2.gif)\n# \n# \n#%% [markdown]\n# \xe8\xbf\x99\xe4\xb8\xa4\xe5\xbc\xa0\xe5\x9b\xbe\xe7\x94\x9f\xe5\x8a\xa8\xe5\xbd\xa2\xe8\xb1\xa1\xe5\x9c\xb0\xe5\xb1\x95\xe7\xa4\xba\xe4\xba\x86\xe5\x90\x84\xe7\xa7\x8d\xe4\xbc\x98\xe5\x8c\x96\xe7\xae\x97\xe6\xb3\x95\xe7\x9a\x84\xe5\xae\x9e\xe9\x99\x85\xe6\x95\x88\xe6\x9e\x9c\n\n"""
5.Optimizer 优化器/Example-Himmelblau 函数的优化.py,7,"b""#%% [markdown]\n# # Himmelblau \xe5\x87\xbd\xe6\x95\xb0\xe7\x9a\x84\xe4\xbc\x98\xe5\x8c\x96\n\n#%%\nget_ipython().run_line_magic('matplotlib', 'inline')\n\nimport numpy as np\nfrom mpl_toolkits.mplot3d import Axes3D\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\nfrom matplotlib.colors import LinearSegmentedColormap\n\nimport torch\n\n#%% [markdown]\n# \xe5\xae\x9a\xe4\xb9\x89 Himmelblau \xe5\x87\xbd\xe6\x95\xb0\n\n#%%\ndef himmelblau(x):\n    return (x[0] ** 2 + x[1] - 11) ** 2 + (x[0] + x[1] ** 2 - 7) ** 2\n\n#%% [markdown]\n# \xe7\xbb\x98\xe5\x88\xb6 Himmelblau \xe5\x87\xbd\xe6\x95\xb0\n\n#%%\nx = np.arange(-6, 6, 0.1)\ny = np.arange(-6, 6, 0.1)\nX, Y = np.meshgrid(x, y)\nZ = himmelblau([X, Y])\n\nfig = plt.figure()\nax = fig.gca(projection='3d')\nax.plot_surface(X, Y, Z)\nax.view_init(60, -30)\nax.set_xlabel('x[0]')\nax.set_ylabel('x[1]')\nfig.show();\n\n#%% [markdown]\n# \xe6\xb1\x82\xe6\x9c\x80\xe5\xb0\x8f\xe5\x80\xbc\n\n#%%\n\nx = torch.tensor([0., 0.], requires_grad=True) # \xe6\x94\xb6\xe6\x95\x9b\xe5\x88\xb0 (3, 2)\n# x = torch.tensor([-1., 0.], requires_grad=True) # \xe6\x94\xb6\xe6\x95\x9b\xe5\x88\xb0 (-2.81, 3.13)\n# x = torch.tensor([-4., 0..], requires_grad=True) # \xe6\x94\xb6\xe6\x95\x9b\xe5\x88\xb0 (-3.78, -3.28)\n# x = torch.tensor([4., 0.], requires_grad=True) # \xe6\x94\xb6\xe6\x95\x9b\xe5\x88\xb0 (3.58, -1.85)\noptimizer = torch.optim.Adam([x,])\nfor step in range(20001):\n    if step:\n        optimizer.zero_grad()\n        f.backward()\n        optimizer.step()\n    f = himmelblau(x)\n    if step % 1000 == 0:\n        print ('step {}: x = {}, f(x) = {}'.format(step, x.tolist(), f))\n\n#%% [markdown]\n# \xe6\xb1\x82\xe6\x9e\x81\xe5\xa4\xa7\xe5\x80\xbc\n\n#%%\nx = torch.tensor([0., 0.], requires_grad=True)\noptimizer = torch.optim.Adam([x,])\nfor step in range(20001):\n    if step:\n        optimizer.zero_grad()\n        (-f).backward()\n        optimizer.step()\n    f = himmelblau(x)\n    if step % 1000 == 0:\n        print ('step {}: x = {}, f(x) = {}'.format(step, x.tolist(), f))\n\n\n"""
5.Optimizer 优化器/Himmelblau 函数的优化.py,7,"b""#%% [markdown]\n# # Himmelblau \xe5\x87\xbd\xe6\x95\xb0\xe7\x9a\x84\xe4\xbc\x98\xe5\x8c\x96\n\n#%%\nget_ipython().run_line_magic('matplotlib', 'inline')\n\nimport numpy as np\nfrom mpl_toolkits.mplot3d import Axes3D\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\nfrom matplotlib.colors import LinearSegmentedColormap\n\nimport torch\n\n#%% [markdown]\n# \xe5\xae\x9a\xe4\xb9\x89 Himmelblau \xe5\x87\xbd\xe6\x95\xb0\n\n#%%\ndef himmelblau(x):\n    return (x[0] ** 2 + x[1] - 11) ** 2 + (x[0] + x[1] ** 2 - 7) ** 2\n\n#%% [markdown]\n# \xe7\xbb\x98\xe5\x88\xb6 Himmelblau \xe5\x87\xbd\xe6\x95\xb0\n\n#%%\nx = np.arange(-6, 6, 0.1)\ny = np.arange(-6, 6, 0.1)\nX, Y = np.meshgrid(x, y)\nZ = himmelblau([X, Y])\n\nfig = plt.figure()\nax = fig.gca(projection='3d')\nax.plot_surface(X, Y, Z)\nax.view_init(60, -30)\nax.set_xlabel('x[0]')\nax.set_ylabel('x[1]')\nfig.show();\n\n#%% [markdown]\n# \xe6\xb1\x82\xe6\x9c\x80\xe5\xb0\x8f\xe5\x80\xbc\n\n#%%\n\nx = torch.tensor([0., 0.], requires_grad=True) # \xe6\x94\xb6\xe6\x95\x9b\xe5\x88\xb0 (3, 2)\n# x = torch.tensor([-1., 0.], requires_grad=True) # \xe6\x94\xb6\xe6\x95\x9b\xe5\x88\xb0 (-2.81, 3.13)\n# x = torch.tensor([-4., 0..], requires_grad=True) # \xe6\x94\xb6\xe6\x95\x9b\xe5\x88\xb0 (-3.78, -3.28)\n# x = torch.tensor([4., 0.], requires_grad=True) # \xe6\x94\xb6\xe6\x95\x9b\xe5\x88\xb0 (3.58, -1.85)\noptimizer = torch.optim.Adam([x,])\nfor step in range(20001):\n    if step:\n        optimizer.zero_grad()\n        f.backward()\n        optimizer.step()\n    f = himmelblau(x)\n    if step % 1000 == 0:\n        print ('step {}: x = {}, f(x) = {}'.format(step, x.tolist(), f))\n\n#%% [markdown]\n# \xe6\xb1\x82\xe6\x9e\x81\xe5\xa4\xa7\xe5\x80\xbc\n\n#%%\nx = torch.tensor([0., 0.], requires_grad=True)\noptimizer = torch.optim.Adam([x,])\nfor step in range(20001):\n    if step:\n        optimizer.zero_grad()\n        (-f).backward()\n        optimizer.step()\n    f = himmelblau(x)\n    if step % 1000 == 0:\n        print ('step {}: x = {}, f(x) = {}'.format(step, x.tolist(), f))\n\n\n"""
5.Optimizer 优化器/RMSProp.py,7,"b""#%% [markdown]\n# # RMSProp\n# RMSprop \xe6\x98\xaf\xe7\x94\xb1 Geoff Hinton \xe5\x9c\xa8\xe4\xbb\x96 Coursera \xe8\xaf\xbe\xe7\xa8\x8b\xe4\xb8\xad\xe6\x8f\x90\xe5\x87\xba\xe7\x9a\x84\xe4\xb8\x80\xe7\xa7\x8d\xe9\x80\x82\xe5\xba\x94\xe6\x80\xa7\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\xe6\x96\xb9\xe6\xb3\x95\xef\xbc\x8c\xe8\x87\xb3\xe4\xbb\x8a\xe4\xbb\x8d\xe6\x9c\xaa\xe8\xa2\xab\xe5\x85\xac\xe5\xbc\x80\xe5\x8f\x91\xe8\xa1\xa8\xe3\x80\x82\xe5\x89\x8d\xe9\x9d\xa2\xe6\x88\x91\xe4\xbb\xac\xe6\x8f\x90\xe5\x88\xb0\xe4\xba\x86 Adagrad \xe7\xae\x97\xe6\xb3\x95\xe6\x9c\x89\xe4\xb8\x80\xe4\xb8\xaa\xe9\x97\xae\xe9\xa2\x98\xef\xbc\x8c\xe5\xb0\xb1\xe6\x98\xaf\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\xe5\x88\x86\xe6\xaf\x8d\xe4\xb8\x8a\xe7\x9a\x84\xe5\x8f\x98\xe9\x87\x8f s \xe4\xb8\x8d\xe6\x96\xad\xe8\xa2\xab\xe7\xb4\xaf\xe5\x8a\xa0\xe5\xa2\x9e\xe5\xa4\xa7\xef\xbc\x8c\xe6\x9c\x80\xe5\x90\x8e\xe4\xbc\x9a\xe5\xaf\xbc\xe8\x87\xb4\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\xe9\x99\xa4\xe4\xbb\xa5\xe4\xb8\x80\xe4\xb8\xaa\xe6\xaf\x94\xe8\xbe\x83\xe5\xa4\xa7\xe7\x9a\x84\xe6\x95\xb0\xe4\xb9\x8b\xe5\x90\x8e\xe5\x8f\x98\xe5\xbe\x97\xe9\x9d\x9e\xe5\xb8\xb8\xe5\xb0\x8f\xef\xbc\x8c\xe8\xbf\x99\xe4\xb8\x8d\xe5\x88\xa9\xe4\xba\x8e\xe6\x88\x91\xe4\xbb\xac\xe6\x89\xbe\xe5\x88\xb0\xe6\x9c\x80\xe5\x90\x8e\xe7\x9a\x84\xe6\x9c\x80\xe4\xbc\x98\xe8\xa7\xa3\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5 RMSProp \xe7\x9a\x84\xe6\x8f\x90\xe5\x87\xba\xe5\xb0\xb1\xe6\x98\xaf\xe4\xb8\xba\xe4\xba\x86\xe8\xa7\xa3\xe5\x86\xb3\xe8\xbf\x99\xe4\xb8\xaa\xe9\x97\xae\xe9\xa2\x98\xe3\x80\x82\n# \n# ## RMSProp \xe7\xae\x97\xe6\xb3\x95\n# RMSProp \xe4\xbb\x8d\xe7\x84\xb6\xe4\xbc\x9a\xe4\xbd\xbf\xe7\x94\xa8\xe6\xa2\xaf\xe5\xba\xa6\xe7\x9a\x84\xe5\xb9\xb3\xe6\x96\xb9\xe9\x87\x8f\xef\xbc\x8c\xe4\xb8\x8d\xe5\x90\x8c\xe4\xba\x8e Adagrad\xef\xbc\x8c\xe5\x85\xb6\xe4\xbc\x9a\xe4\xbd\xbf\xe7\x94\xa8\xe4\xb8\x80\xe4\xb8\xaa\xe6\x8c\x87\xe6\x95\xb0\xe5\x8a\xa0\xe6\x9d\x83\xe7\xa7\xbb\xe5\x8a\xa8\xe5\xb9\xb3\xe5\x9d\x87\xe6\x9d\xa5\xe8\xae\xa1\xe7\xae\x97\xe8\xbf\x99\xe4\xb8\xaa s\xef\xbc\x8c\xe4\xb9\x9f\xe5\xb0\xb1\xe6\x98\xaf\n# \n# $$\n# s_i = \\alpha s_{i-1} + (1 - \\alpha) \\ g^2\n# $$\n# \n# \xe8\xbf\x99\xe9\x87\x8c g \xe8\xa1\xa8\xe7\xa4\xba\xe5\xbd\x93\xe5\x89\x8d\xe6\xb1\x82\xe5\x87\xba\xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0\xe6\xa2\xaf\xe5\xba\xa6\xef\xbc\x8c\xe7\x84\xb6\xe5\x90\x8e\xe6\x9c\x80\xe7\xbb\x88\xe6\x9b\xb4\xe6\x96\xb0\xe5\x92\x8c Adagrad \xe6\x98\xaf\xe4\xb8\x80\xe6\xa0\xb7\xe7\x9a\x84\xef\xbc\x8c\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\xe5\x8f\x98\xe6\x88\x90\xe4\xba\x86\n# \n# $$\n# \\frac{\\eta}{\\sqrt{s + \\epsilon}}\n# $$\n# \n# \xe8\xbf\x99\xe9\x87\x8c $\\alpha$ \xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa\xe7\xa7\xbb\xe5\x8a\xa8\xe5\xb9\xb3\xe5\x9d\x87\xe7\x9a\x84\xe7\xb3\xbb\xe6\x95\xb0\xef\xbc\x8c\xe4\xb9\x9f\xe6\x98\xaf\xe5\x9b\xa0\xe4\xb8\xba\xe8\xbf\x99\xe4\xb8\xaa\xe7\xb3\xbb\xe6\x95\xb0\xef\xbc\x8c\xe5\xaf\xbc\xe8\x87\xb4\xe4\xba\x86 RMSProp \xe5\x92\x8c Adagrad \xe4\xb8\x8d\xe5\x90\x8c\xe7\x9a\x84\xe5\x9c\xb0\xe6\x96\xb9\xef\xbc\x8c\xe8\xbf\x99\xe4\xb8\xaa\xe7\xb3\xbb\xe6\x95\xb0\xe4\xbd\xbf\xe5\xbe\x97 RMSProp \xe6\x9b\xb4\xe6\x96\xb0\xe5\x88\xb0\xe5\x90\x8e\xe6\x9c\x9f\xe7\xb4\xaf\xe5\x8a\xa0\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\xe5\xb9\xb3\xe6\x96\xb9\xe8\xbe\x83\xe5\xb0\x8f\xef\xbc\x8c\xe4\xbb\x8e\xe8\x80\x8c\xe4\xbf\x9d\xe8\xaf\x81 s \xe4\xb8\x8d\xe4\xbc\x9a\xe5\xa4\xaa\xe5\xa4\xa7\xef\xbc\x8c\xe4\xb9\x9f\xe5\xb0\xb1\xe4\xbd\xbf\xe5\xbe\x97\xe6\xa8\xa1\xe5\x9e\x8b\xe5\x90\x8e\xe6\x9c\x9f\xe4\xbe\x9d\xe7\x84\xb6\xe8\x83\xbd\xe5\xa4\x9f\xe6\x89\xbe\xe5\x88\xb0\xe6\xaf\x94\xe8\xbe\x83\xe4\xbc\x98\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x9c\n# \n# \xe5\xae\x9e\xe7\x8e\xb0\xe4\xb8\x8a\xe5\x92\x8c Adagrad \xe9\x9d\x9e\xe5\xb8\xb8\xe5\x83\x8f\n\n#%%\ndef rmsprop(parameters, sqrs, lr, alpha):\n    eps = 1e-10\n    for param, sqr in zip(parameters, sqrs):\n        sqr[:] = alpha * sqr + (1 - alpha) * param.grad.data ** 2\n        div = lr / torch.sqrt(sqr + eps) * param.grad.data\n        param.data = param.data - div\n\n\n#%%\nimport numpy as np\nimport torch\nfrom torchvision.datasets import MNIST # \xe5\xaf\xbc\xe5\x85\xa5 pytorch \xe5\x86\x85\xe7\xbd\xae\xe7\x9a\x84 mnist \xe6\x95\xb0\xe6\x8d\xae\nfrom torch.utils.data import DataLoader\nfrom torch import nn\nimport time\nimport matplotlib.pyplot as plt\nget_ipython().run_line_magic('matplotlib', 'inline')\n\ndef data_tf(x):\n    x = np.array(x, dtype='float32') / 255\n    x = (x - 0.5) / 0.5 # \xe6\xa0\x87\xe5\x87\x86\xe5\x8c\x96\xef\xbc\x8c\xe8\xbf\x99\xe4\xb8\xaa\xe6\x8a\x80\xe5\xb7\xa7\xe4\xb9\x8b\xe5\x90\x8e\xe4\xbc\x9a\xe8\xae\xb2\xe5\x88\xb0\n    x = x.reshape((-1,)) # \xe6\x8b\x89\xe5\xb9\xb3\n    x = torch.from_numpy(x)\n    return x\n\ntrain_set = MNIST('./data', train=True, transform=data_tf, download=True) # \xe8\xbd\xbd\xe5\x85\xa5\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xef\xbc\x8c\xe7\x94\xb3\xe6\x98\x8e\xe5\xae\x9a\xe4\xb9\x89\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe5\x8f\x98\xe6\x8d\xa2\ntest_set = MNIST('./data', train=False, transform=data_tf, download=True)\n\n# \xe5\xae\x9a\xe4\xb9\x89 loss \xe5\x87\xbd\xe6\x95\xb0\ncriterion = nn.CrossEntropyLoss()\n\n\n#%%\ntrain_data = DataLoader(train_set, batch_size=64, shuffle=True)\n# \xe4\xbd\xbf\xe7\x94\xa8 Sequential \xe5\xae\x9a\xe4\xb9\x89 3 \xe5\xb1\x82\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\nnet = nn.Sequential(\n    nn.Linear(784, 200),\n    nn.ReLU(),\n    nn.Linear(200, 10),\n)\n\n# \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe6\xa2\xaf\xe5\xba\xa6\xe5\xb9\xb3\xe6\x96\xb9\xe9\xa1\xb9\nsqrs = []\nfor param in net.parameters():\n    sqrs.append(torch.zeros_like(param.data))\n    \n# \xe5\xbc\x80\xe5\xa7\x8b\xe8\xae\xad\xe7\xbb\x83\nlosses = []\nidx = 0\nstart = time.time() # \xe8\xae\xb0\xe6\x97\xb6\xe5\xbc\x80\xe5\xa7\x8b\nfor e in range(5):\n    train_loss = 0\n    for im, label in train_data:\n        # \xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\n        out = net(im)\n        loss = criterion(out, label)\n        # \xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\n        net.zero_grad()\n        loss.backward()\n        rmsprop(net.parameters(), sqrs, 1e-3, 0.9) # \xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\xe8\xae\xbe\xe4\xb8\xba 0.001\xef\xbc\x8calpha \xe8\xae\xbe\xe4\xb8\xba 0.9\n        # \xe8\xae\xb0\xe5\xbd\x95\xe8\xaf\xaf\xe5\xb7\xae\n        train_loss += loss.item()\n        if idx % 30 == 0:\n            losses.append(loss.item())\n        idx += 1\n    print('epoch: {}, Train Loss: {:.6f}'\n          .format(e, train_loss / len(train_data)))\nend = time.time() # \xe8\xae\xa1\xe6\x97\xb6\xe7\xbb\x93\xe6\x9d\x9f\nprint('\xe4\xbd\xbf\xe7\x94\xa8\xe6\x97\xb6\xe9\x97\xb4: {:.5f} s'.format(end - start))\n\n\n#%%\nx_axis = np.linspace(0, 5, len(losses), endpoint=True)\nplt.semilogy(x_axis, losses, label='alpha=0.9')\nplt.legend(loc='best')\n\n\n#%%\ntrain_data = DataLoader(train_set, batch_size=64, shuffle=True)\n# \xe4\xbd\xbf\xe7\x94\xa8 Sequential \xe5\xae\x9a\xe4\xb9\x89 3 \xe5\xb1\x82\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\nnet = nn.Sequential(\n    nn.Linear(784, 200),\n    nn.ReLU(),\n    nn.Linear(200, 10),\n)\n\n# \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe6\xa2\xaf\xe5\xba\xa6\xe5\xb9\xb3\xe6\x96\xb9\xe9\xa1\xb9\nsqrs = []\nfor param in net.parameters():\n    sqrs.append(torch.zeros_like(param.data))\n    \n# \xe5\xbc\x80\xe5\xa7\x8b\xe8\xae\xad\xe7\xbb\x83\nlosses = []\nidx = 0\n\nstart = time.time() # \xe8\xae\xb0\xe6\x97\xb6\xe5\xbc\x80\xe5\xa7\x8b\nfor e in range(5):\n    train_loss = 0\n    for im, label in train_data:\n        # \xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\n        out = net(im)\n        loss = criterion(out, label)\n        # \xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\n        net.zero_grad()\n        loss.backward()\n        rmsprop(net.parameters(), sqrs, 1e-3, 0.999) # \xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\xe8\xae\xbe\xe4\xb8\xba 0.001\xef\xbc\x8calpha \xe8\xae\xbe\xe4\xb8\xba 0.999\n        # \xe8\xae\xb0\xe5\xbd\x95\xe8\xaf\xaf\xe5\xb7\xae\n        train_loss += loss.item()\n        if idx % 30 == 0:\n            losses.append(loss.item())\n        idx += 1\n    print('epoch: {}, Train Loss: {:.6f}'\n          .format(e, train_loss / len(train_data)))\nend = time.time() # \xe8\xae\xa1\xe6\x97\xb6\xe7\xbb\x93\xe6\x9d\x9f\nprint('\xe4\xbd\xbf\xe7\x94\xa8\xe6\x97\xb6\xe9\x97\xb4: {:.5f} s'.format(end - start))\n\n\n#%%\nx_axis = np.linspace(0, 5, len(losses), endpoint=True)\nplt.semilogy(x_axis, losses, label='alpha=0.999')\nplt.legend(loc='best')\n\n#%% [markdown]\n# **\xe5\xb0\x8f\xe7\xbb\x83\xe4\xb9\xa0\xef\xbc\x9a\xe5\x8f\xaf\xe4\xbb\xa5\xe7\x9c\x8b\xe5\x88\xb0\xe4\xbd\xbf\xe7\x94\xa8\xe4\xba\x86\xe4\xb8\x8d\xe5\x90\x8c\xe7\x9a\x84 alpha \xe4\xbc\x9a\xe4\xbd\xbf\xe5\xbe\x97 loss \xe5\x9c\xa8\xe4\xb8\x8b\xe9\x99\x8d\xe8\xbf\x87\xe7\xa8\x8b\xe4\xb8\xad\xe7\x9a\x84\xe9\x9c\x87\xe8\x8d\xa1\xe7\xa8\x8b\xe5\xba\xa6\xe4\xb8\x8d\xe5\x90\x8c\xef\xbc\x8c\xe6\x83\xb3\xe6\x83\xb3\xe4\xb8\xba\xe4\xbb\x80\xe4\xb9\x88**\n#%% [markdown]\n# \xe5\xbd\x93\xe7\x84\xb6 pytorch \xe4\xb9\x9f\xe5\x86\x85\xe7\xbd\xae\xe4\xba\x86 rmsprop \xe7\x9a\x84\xe6\x96\xb9\xe6\xb3\x95\xef\xbc\x8c\xe9\x9d\x9e\xe5\xb8\xb8\xe7\xae\x80\xe5\x8d\x95\xef\xbc\x8c\xe5\x8f\xaa\xe9\x9c\x80\xe8\xa6\x81\xe8\xb0\x83\xe7\x94\xa8 `torch.optim.RMSprop()` \xe5\xb0\xb1\xe5\x8f\xaf\xe4\xbb\xa5\xe4\xba\x86\xef\xbc\x8c\xe4\xb8\x8b\xe9\x9d\xa2\xe6\x98\xaf\xe4\xbe\x8b\xe5\xad\x90\n\n#%%\ntrain_data = DataLoader(train_set, batch_size=64, shuffle=True)\n# \xe4\xbd\xbf\xe7\x94\xa8 Sequential \xe5\xae\x9a\xe4\xb9\x89 3 \xe5\xb1\x82\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\nnet = nn.Sequential(\n    nn.Linear(784, 200),\n    nn.ReLU(),\n    nn.Linear(200, 10),\n)\n\noptimizer = torch.optim.RMSprop(net.parameters(), lr=1e-3, alpha=0.9)\n    \n# \xe5\xbc\x80\xe5\xa7\x8b\xe8\xae\xad\xe7\xbb\x83\n\nstart = time.time() # \xe8\xae\xb0\xe6\x97\xb6\xe5\xbc\x80\xe5\xa7\x8b\nfor e in range(5):\n    train_loss = 0\n    for im, label in train_data:\n        # \xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\n        out = net(im)\n        loss = criterion(out, label)\n        # \xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        # \xe8\xae\xb0\xe5\xbd\x95\xe8\xaf\xaf\xe5\xb7\xae\n        train_loss += loss.item()\n    print('epoch: {}, Train Loss: {:.6f}'\n          .format(e, train_loss / len(train_data)))\nend = time.time() # \xe8\xae\xa1\xe6\x97\xb6\xe7\xbb\x93\xe6\x9d\x9f\nprint('\xe4\xbd\xbf\xe7\x94\xa8\xe6\x97\xb6\xe9\x97\xb4: {:.5f} s'.format(end - start))\n\n\n"""
5.Optimizer 优化器/SGD(Stochastic-gradient-descent).py,3,"b""#%% [markdown]\n# # \xe9\x9a\x8f\xe6\x9c\xba\xe6\xa2\xaf\xe5\xba\xa6\xe4\xb8\x8b\xe9\x99\x8d\xe6\xb3\x95\n# \xe5\x89\x8d\xe9\x9d\xa2\xe6\x88\x91\xe4\xbb\xac\xe4\xbb\x8b\xe7\xbb\x8d\xe4\xba\x86\xe6\xa2\xaf\xe5\xba\xa6\xe4\xb8\x8b\xe9\x99\x8d\xe6\xb3\x95\xe7\x9a\x84\xe6\x95\xb0\xe5\xad\xa6\xe5\x8e\x9f\xe7\x90\x86\xef\xbc\x8c\xe4\xb8\x8b\xe9\x9d\xa2\xe6\x88\x91\xe4\xbb\xac\xe9\x80\x9a\xe8\xbf\x87\xe4\xbe\x8b\xe5\xad\x90\xe6\x9d\xa5\xe8\xaf\xb4\xe6\x98\x8e\xe4\xb8\x80\xe4\xb8\x8b\xe9\x9a\x8f\xe6\x9c\xba\xe6\xa2\xaf\xe5\xba\xa6\xe4\xb8\x8b\xe9\x99\x8d\xe6\xb3\x95\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe5\x88\x86\xe5\x88\xab\xe4\xbb\x8e 0 \xe8\x87\xaa\xe5\xb7\xb1\xe5\xae\x9e\xe7\x8e\xb0\xef\xbc\x8c\xe4\xbb\xa5\xe5\x8f\x8a\xe4\xbd\xbf\xe7\x94\xa8 pytorch \xe4\xb8\xad\xe8\x87\xaa\xe5\xb8\xa6\xe7\x9a\x84\xe4\xbc\x98\xe5\x8c\x96\xe5\x99\xa8\n\n#%%\nimport numpy as np\nimport torch\nfrom torchvision.datasets import MNIST # \xe5\xaf\xbc\xe5\x85\xa5 pytorch \xe5\x86\x85\xe7\xbd\xae\xe7\x9a\x84 mnist \xe6\x95\xb0\xe6\x8d\xae\nfrom torch.utils.data import DataLoader\nfrom torch import nn\nimport time\nimport matplotlib.pyplot as plt\nget_ipython().run_line_magic('matplotlib', 'inline')\n\ndef data_tf(x):\n    x = np.array(x, dtype='float32') / 255 # \xe5\xb0\x86\xe6\x95\xb0\xe6\x8d\xae\xe5\x8f\x98\xe5\x88\xb0 0 ~ 1 \xe4\xb9\x8b\xe9\x97\xb4\n    x = (x - 0.5) / 0.5 # \xe6\xa0\x87\xe5\x87\x86\xe5\x8c\x96\xef\xbc\x8c\xe8\xbf\x99\xe4\xb8\xaa\xe6\x8a\x80\xe5\xb7\xa7\xe4\xb9\x8b\xe5\x90\x8e\xe4\xbc\x9a\xe8\xae\xb2\xe5\x88\xb0\n    x = x.reshape((-1,)) # \xe6\x8b\x89\xe5\xb9\xb3\n    x = torch.from_numpy(x)\n    return x\n\ntrain_set = MNIST(r'C:/DATASETS', train=True, transform=data_tf, download=True) # \xe8\xbd\xbd\xe5\x85\xa5\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xef\xbc\x8c\xe7\x94\xb3\xe6\x98\x8e\xe5\xae\x9a\xe4\xb9\x89\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe5\x8f\x98\xe6\x8d\xa2\ntest_set = MNIST(r'C:/DATASETS', train=False, transform=data_tf, download=True)\n\n# \xe5\xae\x9a\xe4\xb9\x89 loss \xe5\x87\xbd\xe6\x95\xb0\ncriterion = nn.CrossEntropyLoss()\n\n#%% [markdown]\n# \xe9\x9a\x8f\xe6\x9c\xba\xe6\xa2\xaf\xe5\xba\xa6\xe4\xb8\x8b\xe9\x99\x8d\xe6\xb3\x95\xe9\x9d\x9e\xe5\xb8\xb8\xe7\xae\x80\xe5\x8d\x95\xef\xbc\x8c\xe5\x85\xac\xe5\xbc\x8f\xe5\xb0\xb1\xe6\x98\xaf\n# $$\n# \\theta_{i+1} = \\theta_i - \\eta \\nabla L(\\theta)\n# $$\n# \xe9\x9d\x9e\xe5\xb8\xb8\xe7\xae\x80\xe5\x8d\x95\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe5\x8f\xaf\xe4\xbb\xa5\xe4\xbb\x8e 0 \xe5\xbc\x80\xe5\xa7\x8b\xe8\x87\xaa\xe5\xb7\xb1\xe5\xae\x9e\xe7\x8e\xb0\n\n#%%\ndef sgd_update(parameters, lr):\n    for param in parameters:\n        param.data = param.data - lr * param.grad.data\n\n#%% [markdown]\n# \xe6\x88\x91\xe4\xbb\xac\xe5\x8f\xaf\xe4\xbb\xa5\xe5\xb0\x86 batch size \xe5\x85\x88\xe8\xae\xbe\xe7\xbd\xae\xe4\xb8\xba 1\xef\xbc\x8c\xe7\x9c\x8b\xe7\x9c\x8b\xe6\x9c\x89\xe4\xbb\x80\xe4\xb9\x88\xe6\x95\x88\xe6\x9e\x9c\n\n#%%\ntrain_data = DataLoader(train_set, batch_size=1, shuffle=True)\n# \xe4\xbd\xbf\xe7\x94\xa8 Sequential \xe5\xae\x9a\xe4\xb9\x89 3 \xe5\xb1\x82\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\nnet = nn.Sequential(\n    nn.Linear(784, 200),\n    nn.ReLU(),\n    nn.Linear(200, 10),\n)\n\n# \xe5\xbc\x80\xe5\xa7\x8b\xe8\xae\xad\xe7\xbb\x83\nlosses1 = []\nidx = 0\n\nstart = time.time() # \xe8\xae\xb0\xe6\x97\xb6\xe5\xbc\x80\xe5\xa7\x8b\nfor e in range(5):\n    train_loss = 0\n    for im, label in train_data:\n        # \xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\n        out = net(im)\n        loss = criterion(out, label)\n        # \xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\n        print(net.grad)\n        net.zero_grad()\n        loss.backward()\n        sgd_update(net.parameters(), 1e-2) # \xe4\xbd\xbf\xe7\x94\xa8 0.01 \xe7\x9a\x84\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\n        # \xe8\xae\xb0\xe5\xbd\x95\xe8\xaf\xaf\xe5\xb7\xae\n        train_loss += loss.item()\n        if idx % 30 == 0:\n            losses1.append(loss.item())\n        idx += 1\n    print('epoch: {}, Train Loss: {:.6f}'.format(e, train_loss / len(train_data)))\nend = time.time() # \xe8\xae\xa1\xe6\x97\xb6\xe7\xbb\x93\xe6\x9d\x9f\nprint('\xe4\xbd\xbf\xe7\x94\xa8\xe6\x97\xb6\xe9\x97\xb4: {:.5f} s'.format(end - start))\n\n\n#%%\nx_axis = np.linspace(0, 5, len(losses1), endpoint=True)\nplt.semilogy(x_axis, losses1, label='batch_size=1')\nplt.legend(loc='best')\n\n#%% [markdown]\n# \xe5\x8f\xaf\xe4\xbb\xa5\xe7\x9c\x8b\xe5\x88\xb0\xef\xbc\x8closs \xe5\x9c\xa8\xe5\x89\xa7\xe7\x83\x88\xe9\x9c\x87\xe8\x8d\xa1\xef\xbc\x8c\xe5\x9b\xa0\xe4\xb8\xba\xe6\xaf\x8f\xe6\xac\xa1\xe9\x83\xbd\xe6\x98\xaf\xe5\x8f\xaa\xe5\xaf\xb9\xe4\xb8\x80\xe4\xb8\xaa\xe6\xa0\xb7\xe6\x9c\xac\xe7\x82\xb9\xe5\x81\x9a\xe8\xae\xa1\xe7\xae\x97\xef\xbc\x8c\xe6\xaf\x8f\xe4\xb8\x80\xe5\xb1\x82\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\xe9\x83\xbd\xe5\x85\xb7\xe6\x9c\x89\xe5\xbe\x88\xe9\xab\x98\xe7\x9a\x84\xe9\x9a\x8f\xe6\x9c\xba\xe6\x80\xa7\xef\xbc\x8c\xe8\x80\x8c\xe4\xb8\x94\xe9\x9c\x80\xe8\xa6\x81\xe8\x80\x97\xe8\xb4\xb9\xe4\xba\x86\xe5\xa4\xa7\xe9\x87\x8f\xe7\x9a\x84\xe6\x97\xb6\xe9\x97\xb4\n\n#%%\ntrain_data = DataLoader(train_set, batch_size=64, shuffle=True)\n# \xe4\xbd\xbf\xe7\x94\xa8 Sequential \xe5\xae\x9a\xe4\xb9\x89 3 \xe5\xb1\x82\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\nnet = nn.Sequential(\n    nn.Linear(784, 200),\n    nn.ReLU(),\n    nn.Linear(200, 10),\n)\n\n# \xe5\xbc\x80\xe5\xa7\x8b\xe8\xae\xad\xe7\xbb\x83\nlosses2 = []\nidx = 0\nstart = time.time() # \xe8\xae\xb0\xe6\x97\xb6\xe5\xbc\x80\xe5\xa7\x8b\nfor e in range(5):\n    train_loss = 0\n    for im, label in train_data:\n        # \xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\n        out = net(im)\n        loss = criterion(out, label)\n        # \xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\n        net.zero_grad()\n        loss.backward()\n        sgd_update(net.parameters(), 1e-2)\n        # \xe8\xae\xb0\xe5\xbd\x95\xe8\xaf\xaf\xe5\xb7\xae\n        train_loss += loss.item()\n        if idx % 30 == 0:\n            losses2.append(loss.item())\n        idx += 1\n    print('epoch: {}, Train Loss: {:.6f}'.format(e, train_loss / len(train_data)))\nend = time.time() # \xe8\xae\xa1\xe6\x97\xb6\xe7\xbb\x93\xe6\x9d\x9f\nprint('\xe4\xbd\xbf\xe7\x94\xa8\xe6\x97\xb6\xe9\x97\xb4: {:.5f} s'.format(end - start))\n\n\n#%%\nx_axis = np.linspace(0, 5, len(losses2), endpoint=True)\nplt.semilogy(x_axis, losses2, label='batch_size=64')\nplt.legend(loc='best')\n\n#%% [markdown]\n# \xe9\x80\x9a\xe8\xbf\x87\xe4\xb8\x8a\xe9\x9d\xa2\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x9c\xe5\x8f\xaf\xe4\xbb\xa5\xe7\x9c\x8b\xe5\x88\xb0 loss \xe6\xb2\xa1\xe6\x9c\x89 batch \xe7\xad\x89\xe4\xba\x8e 1 \xe9\x9c\x87\xe8\x8d\xa1\xe9\x82\xa3\xe4\xb9\x88\xe8\xb7\x9d\xe7\xa6\xbb\xef\xbc\x8c\xe5\x90\x8c\xe6\x97\xb6\xe4\xb9\x9f\xe5\x8f\xaf\xe4\xbb\xa5\xe9\x99\x8d\xe5\x88\xb0\xe4\xb8\x80\xe5\xae\x9a\xe7\x9a\x84\xe7\xa8\x8b\xe5\xba\xa6\xe4\xba\x86\xef\xbc\x8c\xe6\x97\xb6\xe9\x97\xb4\xe4\xb8\x8a\xe4\xb9\x9f\xe6\xaf\x94\xe4\xb9\x8b\xe5\x89\x8d\xe5\xbf\xab\xe4\xba\x86\xe9\x9d\x9e\xe5\xb8\xb8\xe5\xa4\x9a\xef\xbc\x8c\xe5\x9b\xa0\xe4\xb8\xba\xe6\x8c\x89\xe7\x85\xa7 batch \xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe9\x87\x8f\xe8\xae\xa1\xe7\xae\x97\xe4\xb8\x8a\xe6\x9b\xb4\xe5\xbf\xab\xef\xbc\x8c\xe5\x90\x8c\xe6\x97\xb6\xe6\xa2\xaf\xe5\xba\xa6\xe5\xaf\xb9\xe6\xaf\x94\xe4\xba\x8e batch size = 1 \xe7\x9a\x84\xe6\x83\x85\xe5\x86\xb5\xe4\xb9\x9f\xe8\xb7\x9f\xe6\x8e\xa5\xe8\xbf\x91\xe7\x9c\x9f\xe5\xae\x9e\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5 batch size \xe7\x9a\x84\xe5\x80\xbc\xe8\xb6\x8a\xe5\xa4\xa7\xef\xbc\x8c\xe6\xa2\xaf\xe5\xba\xa6\xe4\xb9\x9f\xe5\xb0\xb1\xe8\xb6\x8a\xe7\xa8\xb3\xe5\xae\x9a\xef\xbc\x8c\xe8\x80\x8c batch size \xe8\xb6\x8a\xe5\xb0\x8f\xef\xbc\x8c\xe6\xa2\xaf\xe5\xba\xa6\xe5\x85\xb7\xe6\x9c\x89\xe8\xb6\x8a\xe9\xab\x98\xe7\x9a\x84\xe9\x9a\x8f\xe6\x9c\xba\xe6\x80\xa7\xef\xbc\x8c\xe8\xbf\x99\xe9\x87\x8c batch size \xe4\xb8\xba 64\xef\xbc\x8c\xe5\x8f\xaf\xe4\xbb\xa5\xe7\x9c\x8b\xe5\x88\xb0 loss \xe4\xbb\x8d\xe7\x84\xb6\xe5\xad\x98\xe5\x9c\xa8\xe9\x9c\x87\xe8\x8d\xa1\xef\xbc\x8c\xe4\xbd\x86\xe8\xbf\x99\xe5\xb9\xb6\xe6\xb2\xa1\xe6\x9c\x89\xe5\x85\xb3\xe7\xb3\xbb\xef\xbc\x8c\xe5\xa6\x82\xe6\x9e\x9c batch size \xe5\xa4\xaa\xe5\xa4\xa7\xef\xbc\x8c\xe5\xaf\xb9\xe4\xba\x8e\xe5\x86\x85\xe5\xad\x98\xe7\x9a\x84\xe9\x9c\x80\xe6\xb1\x82\xe5\xb0\xb1\xe6\x9b\xb4\xe9\xab\x98\xef\xbc\x8c\xe5\x90\x8c\xe6\x97\xb6\xe4\xb9\x9f\xe4\xb8\x8d\xe5\x88\xa9\xe4\xba\x8e\xe7\xbd\x91\xe7\xbb\x9c\xe8\xb7\xb3\xe5\x87\xba\xe5\xb1\x80\xe9\x83\xa8\xe6\x9e\x81\xe5\xb0\x8f\xe7\x82\xb9\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe7\x8e\xb0\xe5\x9c\xa8\xe6\x99\xae\xe9\x81\x8d\xe4\xbd\xbf\xe7\x94\xa8\xe5\x9f\xba\xe4\xba\x8e batch \xe7\x9a\x84\xe9\x9a\x8f\xe6\x9c\xba\xe6\xa2\xaf\xe5\xba\xa6\xe4\xb8\x8b\xe9\x99\x8d\xe6\xb3\x95\xef\xbc\x8c\xe8\x80\x8c batch \xe7\x9a\x84\xe5\xa4\x9a\xe5\xb0\x91\xe5\x9f\xba\xe4\xba\x8e\xe5\xae\x9e\xe9\x99\x85\xe6\x83\x85\xe5\x86\xb5\xe8\xbf\x9b\xe8\xa1\x8c\xe8\x80\x83\xe8\x99\x91\n#%% [markdown]\n# \xe4\xb8\x8b\xe9\x9d\xa2\xe6\x88\x91\xe4\xbb\xac\xe8\xb0\x83\xe9\xab\x98\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\xef\xbc\x8c\xe7\x9c\x8b\xe7\x9c\x8b\xe6\x9c\x89\xe4\xbb\x80\xe4\xb9\x88\xe6\xa0\xb7\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x9c\n\n#%%\ntrain_data = DataLoader(train_set, batch_size=64, shuffle=True)\n# \xe4\xbd\xbf\xe7\x94\xa8 Sequential \xe5\xae\x9a\xe4\xb9\x89 3 \xe5\xb1\x82\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\nnet = nn.Sequential(\n    nn.Linear(784, 200),\n    nn.ReLU(),\n    nn.Linear(200, 10),\n)\n\n# \xe5\xbc\x80\xe5\xa7\x8b\xe8\xae\xad\xe7\xbb\x83\nlosses3 = []\nidx = 0\nstart = time.time() # \xe8\xae\xb0\xe6\x97\xb6\xe5\xbc\x80\xe5\xa7\x8b\nfor e in range(5):\n    train_loss = 0\n    for im, label in train_data:\n        # \xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\n        out = net(im)\n        loss = criterion(out, label)\n        # \xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\n        net.zero_grad()\n        loss.backward()\n        sgd_update(net.parameters(), 1) # \xe4\xbd\xbf\xe7\x94\xa8 1.0 \xe7\x9a\x84\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\n        # \xe8\xae\xb0\xe5\xbd\x95\xe8\xaf\xaf\xe5\xb7\xae\n        train_loss += loss.item()\n        if idx % 30 == 0:\n            losses3.append(loss.item())\n        idx += 1\n    print('epoch: {}, Train Loss: {:.6f}'.format(e, train_loss / len(train_data)))\nend = time.time() # \xe8\xae\xa1\xe6\x97\xb6\xe7\xbb\x93\xe6\x9d\x9f\nprint('\xe4\xbd\xbf\xe7\x94\xa8\xe6\x97\xb6\xe9\x97\xb4: {:.5f} s'.format(end - start))\n\n\n#%%\nx_axis = np.linspace(0, 5, len(losses3), endpoint=True)\nplt.semilogy(x_axis, losses3, label='lr = 1')\nplt.legend(loc='best')\n\n#%% [markdown]\n# \xe5\x8f\xaf\xe4\xbb\xa5\xe7\x9c\x8b\xe5\x88\xb0\xef\xbc\x8c\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\xe5\xa4\xaa\xe5\xa4\xa7\xe4\xbc\x9a\xe4\xbd\xbf\xe5\xbe\x97\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x87\xbd\xe6\x95\xb0\xe4\xb8\x8d\xe6\x96\xad\xe5\x9b\x9e\xe8\xb7\xb3\xef\xbc\x8c\xe4\xbb\x8e\xe8\x80\x8c\xe6\x97\xa0\xe6\xb3\x95\xe8\xae\xa9\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x87\xbd\xe6\x95\xb0\xe8\xbe\x83\xe5\xa5\xbd\xe9\x99\x8d\xe4\xbd\x8e\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe6\x88\x91\xe4\xbb\xac\xe4\xb8\x80\xe8\x88\xac\xe9\x83\xbd\xe6\x98\xaf\xe7\x94\xa8\xe4\xb8\x80\xe4\xb8\xaa\xe6\xaf\x94\xe8\xbe\x83\xe5\xb0\x8f\xe7\x9a\x84\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\n#%% [markdown]\n# \xe5\xae\x9e\xe9\x99\x85\xe4\xb8\x8a\xe6\x88\x91\xe4\xbb\xac\xe5\xb9\xb6\xe4\xb8\x8d\xe7\x94\xa8\xe8\x87\xaa\xe5\xb7\xb1\xe9\x80\xa0\xe8\xbd\xae\xe5\xad\x90\xef\xbc\x8c\xe5\x9b\xa0\xe4\xb8\xba pytorch \xe4\xb8\xad\xe5\xb7\xb2\xe7\xbb\x8f\xe4\xb8\xba\xe6\x88\x91\xe4\xbb\xac\xe5\x86\x85\xe7\xbd\xae\xe4\xba\x86\xe9\x9a\x8f\xe6\x9c\xba\xe6\xa2\xaf\xe5\xba\xa6\xe4\xb8\x8b\xe9\x99\x8d\xe5\x8f\x91\xef\xbc\x8c\xe8\x80\x8c\xe4\xb8\x94\xe4\xb9\x8b\xe5\x89\x8d\xe6\x88\x91\xe4\xbb\xac\xe4\xb8\x80\xe7\x9b\xb4\xe5\x9c\xa8\xe4\xbd\xbf\xe7\x94\xa8\xef\xbc\x8c\xe4\xb8\x8b\xe9\x9d\xa2\xe6\x88\x91\xe4\xbb\xac\xe6\x9d\xa5\xe4\xbd\xbf\xe7\x94\xa8 pytorch \xe8\x87\xaa\xe5\xb8\xa6\xe7\x9a\x84\xe4\xbc\x98\xe5\x8c\x96\xe5\x99\xa8\xe6\x9d\xa5\xe5\xae\x9e\xe7\x8e\xb0\xe9\x9a\x8f\xe6\x9c\xba\xe6\xa2\xaf\xe5\xba\xa6\xe4\xb8\x8b\xe9\x99\x8d\n\n#%%\ntrain_data = DataLoader(train_set, batch_size=64, shuffle=True)\n# \xe4\xbd\xbf\xe7\x94\xa8 Sequential \xe5\xae\x9a\xe4\xb9\x89 3 \xe5\xb1\x82\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\nnet = nn.Sequential(\n    nn.Linear(784, 200),\n    nn.ReLU(),\n    nn.Linear(200, 10),\n)\n\noptimzier = torch.optim.SGD(net.parameters(), 1e-2)\n# \xe5\xbc\x80\xe5\xa7\x8b\xe8\xae\xad\xe7\xbb\x83\n\nstart = time.time() # \xe8\xae\xb0\xe6\x97\xb6\xe5\xbc\x80\xe5\xa7\x8b\nfor e in range(5):\n    train_loss = 0\n    for im, label in train_data:\n        # \xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\n        out = net(im)\n        loss = criterion(out, label)\n        # \xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\n        optimzier.zero_grad()\n        loss.backward()\n        optimzier.step()\n        # \xe8\xae\xb0\xe5\xbd\x95\xe8\xaf\xaf\xe5\xb7\xae\n        train_loss += loss.item()\n    print('epoch: {}, Train Loss: {:.6f}'.format(e, train_loss / len(train_data)))\nend = time.time() # \xe8\xae\xa1\xe6\x97\xb6\xe7\xbb\x93\xe6\x9d\x9f\nprint('\xe4\xbd\xbf\xe7\x94\xa8\xe6\x97\xb6\xe9\x97\xb4: {:.5f} s'.format(end - start))\n\n\n"""
5.Optimizer 优化器/momentum.py,6,"b""#%% [markdown]\n# # \xe5\x8a\xa8\xe9\x87\x8f\xe6\xb3\x95\n# \xe4\xbd\xbf\xe7\x94\xa8\xe6\xa2\xaf\xe5\xba\xa6\xe4\xb8\x8b\xe9\x99\x8d\xe6\xb3\x95\xef\xbc\x8c\xe6\xaf\x8f\xe6\xac\xa1\xe9\x83\xbd\xe4\xbc\x9a\xe6\x9c\x9d\xe7\x9d\x80\xe7\x9b\xae\xe6\xa0\x87\xe5\x87\xbd\xe6\x95\xb0\xe4\xb8\x8b\xe9\x99\x8d\xe6\x9c\x80\xe5\xbf\xab\xe7\x9a\x84\xe6\x96\xb9\xe5\x90\x91\xef\xbc\x8c\xe8\xbf\x99\xe4\xb9\x9f\xe7\xa7\xb0\xe4\xb8\xba\xe6\x9c\x80\xe9\x80\x9f\xe4\xb8\x8b\xe9\x99\x8d\xe6\xb3\x95\xe3\x80\x82\xe8\xbf\x99\xe7\xa7\x8d\xe6\x9b\xb4\xe6\x96\xb0\xe6\x96\xb9\xe6\xb3\x95\xe7\x9c\x8b\xe4\xbc\xbc\xe9\x9d\x9e\xe5\xb8\xb8\xe5\xbf\xab\xef\xbc\x8c\xe5\xae\x9e\xe9\x99\x85\xe4\xb8\x8a\xe5\xad\x98\xe5\x9c\xa8\xe4\xb8\x80\xe4\xba\x9b\xe9\x97\xae\xe9\xa2\x98\xe3\x80\x82\n# \n# ## \xe6\xa2\xaf\xe5\xba\xa6\xe4\xb8\x8b\xe9\x99\x8d\xe6\xb3\x95\xe7\x9a\x84\xe9\x97\xae\xe9\xa2\x98\n# \xe8\x80\x83\xe8\x99\x91\xe4\xb8\x80\xe4\xb8\xaa\xe4\xba\x8c\xe7\xbb\xb4\xe8\xbe\x93\xe5\x85\xa5\xef\xbc\x8c$[x_1, x_2]$\xef\xbc\x8c\xe8\xbe\x93\xe5\x87\xba\xe7\x9a\x84\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x87\xbd\xe6\x95\xb0 $L: R^2 \\rightarrow R$\xef\xbc\x8c\xe4\xb8\x8b\xe9\x9d\xa2\xe6\x98\xaf\xe8\xbf\x99\xe4\xb8\xaa\xe5\x87\xbd\xe6\x95\xb0\xe7\x9a\x84\xe7\xad\x89\xe9\xab\x98\xe7\xba\xbf\n# \n# ![](https://ws1.sinaimg.cn/large/006tKfTcly1fmnketw5f4j30az04lq31.jpg)\n# \n# \xe5\x8f\xaf\xe4\xbb\xa5\xe6\x83\xb3\xe8\xb1\xa1\xe6\x88\x90\xe4\xb8\x80\xe4\xb8\xaa\xe5\xbe\x88\xe6\x89\x81\xe7\x9a\x84\xe6\xbc\x8f\xe6\x96\x97\xef\xbc\x8c\xe8\xbf\x99\xe6\xa0\xb7\xe5\x9c\xa8\xe7\xab\x96\xe7\x9b\xb4\xe6\x96\xb9\xe5\x90\x91\xe4\xb8\x8a\xef\xbc\x8c\xe6\xa2\xaf\xe5\xba\xa6\xe5\xb0\xb1\xe9\x9d\x9e\xe5\xb8\xb8\xe5\xa4\xa7\xef\xbc\x8c\xe5\x9c\xa8\xe6\xb0\xb4\xe5\xb9\xb3\xe6\x96\xb9\xe5\x90\x91\xe4\xb8\x8a\xef\xbc\x8c\xe6\xa2\xaf\xe5\xba\xa6\xe5\xb0\xb1\xe7\x9b\xb8\xe5\xaf\xb9\xe8\xbe\x83\xe5\xb0\x8f\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe6\x88\x91\xe4\xbb\xac\xe5\x9c\xa8\xe8\xae\xbe\xe7\xbd\xae\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\xe7\x9a\x84\xe6\x97\xb6\xe5\x80\x99\xe5\xb0\xb1\xe4\xb8\x8d\xe8\x83\xbd\xe8\xae\xbe\xe7\xbd\xae\xe5\xa4\xaa\xe5\xa4\xa7\xef\xbc\x8c\xe4\xb8\xba\xe4\xba\x86\xe9\x98\xb2\xe6\xad\xa2\xe7\xab\x96\xe7\x9b\xb4\xe6\x96\xb9\xe5\x90\x91\xe4\xb8\x8a\xe5\x8f\x82\xe6\x95\xb0\xe6\x9b\xb4\xe6\x96\xb0\xe5\xa4\xaa\xe8\xbf\x87\xe4\xba\x86\xef\xbc\x8c\xe8\xbf\x99\xe6\xa0\xb7\xe4\xb8\x80\xe4\xb8\xaa\xe8\xbe\x83\xe5\xb0\x8f\xe7\x9a\x84\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\xe5\x8f\x88\xe5\xaf\xbc\xe8\x87\xb4\xe4\xba\x86\xe6\xb0\xb4\xe5\xb9\xb3\xe6\x96\xb9\xe5\x90\x91\xe4\xb8\x8a\xe5\x8f\x82\xe6\x95\xb0\xe5\x9c\xa8\xe6\x9b\xb4\xe6\x96\xb0\xe7\x9a\x84\xe6\x97\xb6\xe5\x80\x99\xe5\xa4\xaa\xe8\xbf\x87\xe4\xba\x8e\xe7\xbc\x93\xe6\x85\xa2\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe5\xb0\xb1\xe5\xaf\xbc\xe8\x87\xb4\xe6\x9c\x80\xe7\xbb\x88\xe6\x94\xb6\xe6\x95\x9b\xe8\xb5\xb7\xe6\x9d\xa5\xe9\x9d\x9e\xe5\xb8\xb8\xe6\x85\xa2\xe3\x80\x82\n# \n# ## \xe5\x8a\xa8\xe9\x87\x8f\xe6\xb3\x95\n# \xe5\x8a\xa8\xe9\x87\x8f\xe6\xb3\x95\xe7\x9a\x84\xe6\x8f\x90\xe5\x87\xba\xe5\xb0\xb1\xe6\x98\xaf\xe4\xb8\xba\xe4\xba\x86\xe5\xba\x94\xe5\xaf\xb9\xe8\xbf\x99\xe4\xb8\xaa\xe9\x97\xae\xe9\xa2\x98\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe6\xa2\xaf\xe5\xba\xa6\xe4\xb8\x8b\xe9\x99\x8d\xe6\xb3\x95\xe5\x81\x9a\xe4\xb8\x80\xe4\xb8\xaa\xe4\xbf\xae\xe6\x94\xb9\xe5\xa6\x82\xe4\xb8\x8b\n# \n# $$\n# v_i = \\gamma v_{i-1} + \\eta \\nabla L(\\theta)\n# $$\n# $$\n# \\theta_i = \\theta_{i-1} - v_i\n# $$\n# \n# \xe5\x85\xb6\xe4\xb8\xad $v_i$ \xe6\x98\xaf\xe5\xbd\x93\xe5\x89\x8d\xe9\x80\x9f\xe5\xba\xa6\xef\xbc\x8c$\\gamma$ \xe6\x98\xaf\xe5\x8a\xa8\xe9\x87\x8f\xe5\x8f\x82\xe6\x95\xb0\xef\xbc\x8c\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa\xe5\xb0\x8f\xe4\xba\x8e 1\xe7\x9a\x84\xe6\xad\xa3\xe6\x95\xb0\xef\xbc\x8c$\\eta$ \xe6\x98\xaf\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\n#%% [markdown]\n# \xe7\x9b\xb8\xe5\xbd\x93\xe4\xba\x8e\xe6\xaf\x8f\xe6\xac\xa1\xe5\x9c\xa8\xe8\xbf\x9b\xe8\xa1\x8c\xe5\x8f\x82\xe6\x95\xb0\xe6\x9b\xb4\xe6\x96\xb0\xe7\x9a\x84\xe6\x97\xb6\xe5\x80\x99\xef\xbc\x8c\xe9\x83\xbd\xe4\xbc\x9a\xe5\xb0\x86\xe4\xb9\x8b\xe5\x89\x8d\xe7\x9a\x84\xe9\x80\x9f\xe5\xba\xa6\xe8\x80\x83\xe8\x99\x91\xe8\xbf\x9b\xe6\x9d\xa5\xef\xbc\x8c\xe6\xaf\x8f\xe4\xb8\xaa\xe5\x8f\x82\xe6\x95\xb0\xe5\x9c\xa8\xe5\x90\x84\xe6\x96\xb9\xe5\x90\x91\xe4\xb8\x8a\xe7\x9a\x84\xe7\xa7\xbb\xe5\x8a\xa8\xe5\xb9\x85\xe5\xba\xa6\xe4\xb8\x8d\xe4\xbb\x85\xe5\x8f\x96\xe5\x86\xb3\xe4\xba\x8e\xe5\xbd\x93\xe5\x89\x8d\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\xef\xbc\x8c\xe8\xbf\x98\xe5\x8f\x96\xe5\x86\xb3\xe4\xba\x8e\xe8\xbf\x87\xe5\x8e\xbb\xe5\x90\x84\xe4\xb8\xaa\xe6\xa2\xaf\xe5\xba\xa6\xe5\x9c\xa8\xe5\x90\x84\xe4\xb8\xaa\xe6\x96\xb9\xe5\x90\x91\xe4\xb8\x8a\xe6\x98\xaf\xe5\x90\xa6\xe4\xb8\x80\xe8\x87\xb4\xef\xbc\x8c\xe5\xa6\x82\xe6\x9e\x9c\xe4\xb8\x80\xe4\xb8\xaa\xe6\xa2\xaf\xe5\xba\xa6\xe4\xb8\x80\xe7\x9b\xb4\xe6\xb2\xbf\xe7\x9d\x80\xe5\xbd\x93\xe5\x89\x8d\xe6\x96\xb9\xe5\x90\x91\xe8\xbf\x9b\xe8\xa1\x8c\xe6\x9b\xb4\xe6\x96\xb0\xef\xbc\x8c\xe9\x82\xa3\xe4\xb9\x88\xe6\xaf\x8f\xe6\xac\xa1\xe6\x9b\xb4\xe6\x96\xb0\xe7\x9a\x84\xe5\xb9\x85\xe5\xba\xa6\xe5\xb0\xb1\xe8\xb6\x8a\xe6\x9d\xa5\xe8\xb6\x8a\xe5\xa4\xa7\xef\xbc\x8c\xe5\xa6\x82\xe6\x9e\x9c\xe4\xb8\x80\xe4\xb8\xaa\xe6\xa2\xaf\xe5\xba\xa6\xe5\x9c\xa8\xe4\xb8\x80\xe4\xb8\xaa\xe6\x96\xb9\xe5\x90\x91\xe4\xb8\x8a\xe4\xb8\x8d\xe6\x96\xad\xe5\x8f\x98\xe5\x8c\x96\xef\xbc\x8c\xe9\x82\xa3\xe4\xb9\x88\xe5\x85\xb6\xe6\x9b\xb4\xe6\x96\xb0\xe5\xb9\x85\xe5\xba\xa6\xe5\xb0\xb1\xe4\xbc\x9a\xe8\xa2\xab\xe8\xa1\xb0\xe5\x87\x8f\xef\xbc\x8c\xe8\xbf\x99\xe6\xa0\xb7\xe6\x88\x91\xe4\xbb\xac\xe5\xb0\xb1\xe5\x8f\xaf\xe4\xbb\xa5\xe4\xbd\xbf\xe7\x94\xa8\xe4\xb8\x80\xe4\xb8\xaa\xe8\xbe\x83\xe5\xa4\xa7\xe7\x9a\x84\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\xef\xbc\x8c\xe4\xbd\xbf\xe5\xbe\x97\xe6\x94\xb6\xe6\x95\x9b\xe6\x9b\xb4\xe5\xbf\xab\xef\xbc\x8c\xe5\x90\x8c\xe6\x97\xb6\xe6\xa2\xaf\xe5\xba\xa6\xe6\xaf\x94\xe8\xbe\x83\xe5\xa4\xa7\xe7\x9a\x84\xe6\x96\xb9\xe5\x90\x91\xe5\xb0\xb1\xe4\xbc\x9a\xe5\x9b\xa0\xe4\xb8\xba\xe5\x8a\xa8\xe9\x87\x8f\xe7\x9a\x84\xe5\x85\xb3\xe7\xb3\xbb\xe6\xaf\x8f\xe6\xac\xa1\xe6\x9b\xb4\xe6\x96\xb0\xe7\x9a\x84\xe5\xb9\x85\xe5\xba\xa6\xe5\x87\x8f\xe5\xb0\x91\xef\xbc\x8c\xe5\xa6\x82\xe4\xb8\x8b\xe5\x9b\xbe\n# \n# ![](https://ws1.sinaimg.cn/large/006tNc79gy1fmo5l53o76j30ak04gjrh.jpg)\n# \n# \xe6\xaf\x94\xe5\xa6\x82\xe6\x88\x91\xe4\xbb\xac\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\xe6\xaf\x8f\xe6\xac\xa1\xe9\x83\xbd\xe7\xad\x89\xe4\xba\x8e g\xef\xbc\x8c\xe8\x80\x8c\xe4\xb8\x94\xe6\x96\xb9\xe5\x90\x91\xe9\x83\xbd\xe7\x9b\xb8\xe5\x90\x8c\xef\xbc\x8c\xe9\x82\xa3\xe4\xb9\x88\xe5\x8a\xa8\xe9\x87\x8f\xe6\xb3\x95\xe5\x9c\xa8\xe8\xaf\xa5\xe6\x96\xb9\xe5\x90\x91\xe4\xb8\x8a\xe4\xbd\xbf\xe5\x8f\x82\xe6\x95\xb0\xe5\x8a\xa0\xe9\x80\x9f\xe7\xa7\xbb\xe5\x8a\xa8\xef\xbc\x8c\xe6\x9c\x89\xe4\xb8\x8b\xe9\x9d\xa2\xe7\x9a\x84\xe5\x85\xac\xe5\xbc\x8f\xef\xbc\x9a\n# \n# $$\n# v_0 = 0\n# $$\n# $$\n# v_1 = \\gamma v_0 + \\eta g = \\eta g\n# $$\n# $$\n# v_2 = \\gamma v_1 + \\eta g = (1 + \\gamma) \\eta g\n# $$\n# $$\n# v_3 = \\gamma v_2 + \\eta g = (1 + \\gamma + \\gamma^2) \\eta g\n# $$\n# $$\n# \\cdots\n# $$\n# $$\n# v_{+ \\infty} = (1 + \\gamma + \\gamma^2 + \\gamma^3 + \\cdots) \\eta g = \\frac{1}{1 - \\gamma} \\eta g\n# $$\n# \n# \xe5\xa6\x82\xe6\x9e\x9c\xe6\x88\x91\xe4\xbb\xac\xe6\x8a\x8a $\\gamma$ \xe5\xae\x9a\xe4\xb8\xba 0.9\xef\xbc\x8c\xe9\x82\xa3\xe4\xb9\x88\xe6\x9b\xb4\xe6\x96\xb0\xe5\xb9\x85\xe5\xba\xa6\xe7\x9a\x84\xe5\xb3\xb0\xe5\x80\xbc\xe5\xb0\xb1\xe6\x98\xaf\xe5\x8e\x9f\xe6\x9c\xac\xe6\xa2\xaf\xe5\xba\xa6\xe4\xb9\x98\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\xe7\x9a\x84 10 \xe5\x80\x8d\xe3\x80\x82\n# \n# \xe6\x9c\xac\xe8\xb4\xa8\xe4\xb8\x8a\xe8\xaf\xb4\xef\xbc\x8c\xe5\x8a\xa8\xe9\x87\x8f\xe6\xb3\x95\xe5\xb0\xb1\xe4\xbb\xbf\xe4\xbd\x9b\xe6\x88\x91\xe4\xbb\xac\xe4\xbb\x8e\xe9\xab\x98\xe5\x9d\xa1\xe4\xb8\x8a\xe6\x8e\xa8\xe4\xb8\x80\xe4\xb8\xaa\xe7\x90\x83\xef\xbc\x8c\xe5\xb0\x8f\xe7\x90\x83\xe5\x9c\xa8\xe5\x90\x91\xe4\xb8\x8b\xe6\xbb\x9a\xe5\x8a\xa8\xe7\x9a\x84\xe8\xbf\x87\xe7\xa8\x8b\xe4\xb8\xad\xe7\xa7\xaf\xe7\xb4\xaf\xe4\xba\x86\xe5\x8a\xa8\xe9\x87\x8f\xef\xbc\x8c\xe5\x9c\xa8\xe9\x80\x94\xe4\xb8\xad\xe4\xb9\x9f\xe4\xbc\x9a\xe5\x8f\x98\xe5\xbe\x97\xe8\xb6\x8a\xe6\x9d\xa5\xe8\xb6\x8a\xe5\xbf\xab\xef\xbc\x8c\xe6\x9c\x80\xe5\x90\x8e\xe4\xbc\x9a\xe8\xbe\xbe\xe5\x88\xb0\xe4\xb8\x80\xe4\xb8\xaa\xe5\xb3\xb0\xe5\x80\xbc\xef\xbc\x8c\xe5\xaf\xb9\xe5\xba\x94\xe4\xba\x8e\xe6\x88\x91\xe4\xbb\xac\xe7\x9a\x84\xe7\xae\x97\xe6\xb3\x95\xe4\xb8\xad\xe5\xb0\xb1\xe6\x98\xaf\xef\xbc\x8c\xe5\x8a\xa8\xe9\x87\x8f\xe9\xa1\xb9\xe4\xbc\x9a\xe6\xb2\xbf\xe7\x9d\x80\xe6\xa2\xaf\xe5\xba\xa6\xe6\x8c\x87\xe5\x90\x91\xe6\x96\xb9\xe5\x90\x91\xe7\x9b\xb8\xe5\x90\x8c\xe7\x9a\x84\xe6\x96\xb9\xe5\x90\x91\xe4\xb8\x8d\xe6\x96\xad\xe5\xa2\x9e\xe5\xa4\xa7\xef\xbc\x8c\xe5\xaf\xb9\xe4\xba\x8e\xe6\xa2\xaf\xe5\xba\xa6\xe6\x96\xb9\xe5\x90\x91\xe6\x94\xb9\xe5\x8f\x98\xe7\x9a\x84\xe6\x96\xb9\xe5\x90\x91\xe9\x80\x90\xe6\xb8\x90\xe5\x87\x8f\xe5\xb0\x8f\xef\xbc\x8c\xe5\xbe\x97\xe5\x88\xb0\xe4\xba\x86\xe6\x9b\xb4\xe5\xbf\xab\xe7\x9a\x84\xe6\x94\xb6\xe6\x95\x9b\xe9\x80\x9f\xe5\xba\xa6\xe4\xbb\xa5\xe5\x8f\x8a\xe6\x9b\xb4\xe5\xb0\x8f\xe7\x9a\x84\xe9\x9c\x87\xe8\x8d\xa1\xe3\x80\x82\n# \n# \xe4\xb8\x8b\xe9\x9d\xa2\xe6\x88\x91\xe4\xbb\xac\xe6\x89\x8b\xe5\x8a\xa8\xe5\xae\x9e\xe7\x8e\xb0\xe4\xb8\x80\xe4\xb8\xaa\xe5\x8a\xa8\xe9\x87\x8f\xe6\xb3\x95\xef\xbc\x8c\xe5\x85\xac\xe5\xbc\x8f\xe5\xb7\xb2\xe7\xbb\x8f\xe5\x9c\xa8\xe4\xb8\x8a\xe9\x9d\xa2\xe4\xba\x86\n\n#%%\ndef sgd_momentum(parameters, vs, lr, gamma):\n    for param, v in zip(parameters, vs):\n        v[:] = gamma * v + lr * param.grad.data\n        param.data = param.data - v\n\n\n#%%\nimport numpy as np\nimport torch\nfrom torchvision.datasets import MNIST # \xe5\xaf\xbc\xe5\x85\xa5 pytorch \xe5\x86\x85\xe7\xbd\xae\xe7\x9a\x84 mnist \xe6\x95\xb0\xe6\x8d\xae\nfrom torch.utils.data import DataLoader\nfrom torch import nn\nimport time\nimport matplotlib.pyplot as plt\nget_ipython().run_line_magic('matplotlib', 'inline')\n\ndef data_tf(x):\n    x = np.array(x, dtype='float32') / 255\n    x = (x - 0.5) / 0.5 # \xe6\xa0\x87\xe5\x87\x86\xe5\x8c\x96\xef\xbc\x8c\xe8\xbf\x99\xe4\xb8\xaa\xe6\x8a\x80\xe5\xb7\xa7\xe4\xb9\x8b\xe5\x90\x8e\xe4\xbc\x9a\xe8\xae\xb2\xe5\x88\xb0\n    x = x.reshape((-1,)) # \xe6\x8b\x89\xe5\xb9\xb3\n    x = torch.from_numpy(x)\n    return x\n\ntrain_set = MNIST(r'C:/DATASETS', train=True, transform=data_tf, download=True) # \xe8\xbd\xbd\xe5\x85\xa5\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xef\xbc\x8c\xe7\x94\xb3\xe6\x98\x8e\xe5\xae\x9a\xe4\xb9\x89\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe5\x8f\x98\xe6\x8d\xa2\ntest_set = MNIST(r'C:/DATASETS', train=False, transform=data_tf, download=True)\n\n# \xe5\xae\x9a\xe4\xb9\x89 loss \xe5\x87\xbd\xe6\x95\xb0\ncriterion = nn.CrossEntropyLoss()\n\n\n#%%\ntrain_data = DataLoader(train_set, batch_size=64, shuffle=True)\n# \xe4\xbd\xbf\xe7\x94\xa8 Sequential \xe5\xae\x9a\xe4\xb9\x89 3 \xe5\xb1\x82\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\nnet = nn.Sequential(\n    nn.Linear(784, 200),\n    nn.ReLU(),\n    nn.Linear(200, 10),\n)\n\n# \xe5\xb0\x86\xe9\x80\x9f\xe5\xba\xa6\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe4\xb8\xba\xe5\x92\x8c\xe5\x8f\x82\xe6\x95\xb0\xe5\xbd\xa2\xe7\x8a\xb6\xe7\x9b\xb8\xe5\x90\x8c\xe7\x9a\x84\xe9\x9b\xb6\xe5\xbc\xa0\xe9\x87\x8f\nvs = []\nfor param in net.parameters():\n    vs.append(torch.zeros_like(param.data))\n    \n# \xe5\xbc\x80\xe5\xa7\x8b\xe8\xae\xad\xe7\xbb\x83\nlosses = []\n\nstart = time.time() # \xe8\xae\xb0\xe6\x97\xb6\xe5\xbc\x80\xe5\xa7\x8b\nfor e in range(5):\n    train_loss = 0\n    for im, label in train_data:\n        # \xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\n        out = net(im)\n        loss = criterion(out, label)\n        # \xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\n        net.zero_grad()\n        loss.backward()\n        sgd_momentum(net.parameters(), vs, 1e-2, 0.9) # \xe4\xbd\xbf\xe7\x94\xa8\xe7\x9a\x84\xe5\x8a\xa8\xe9\x87\x8f\xe5\x8f\x82\xe6\x95\xb0\xe4\xb8\xba 0.9\xef\xbc\x8c\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87 0.01\n        # \xe8\xae\xb0\xe5\xbd\x95\xe8\xaf\xaf\xe5\xb7\xae\n        train_loss += loss.item()\n        \n        losses.append(loss.item())\n    print('epoch: {}, Train Loss: {:.6f}'.format(e, train_loss / len(train_data)))\nend = time.time() # \xe8\xae\xa1\xe6\x97\xb6\xe7\xbb\x93\xe6\x9d\x9f\nprint('\xe4\xbd\xbf\xe7\x94\xa8\xe6\x97\xb6\xe9\x97\xb4: {:.5f} s'.format(end - start))\n\n#%% [markdown]\n# \xe5\x8f\xaf\xe4\xbb\xa5\xe7\x9c\x8b\xe5\x88\xb0\xef\xbc\x8c\xe5\x8a\xa0\xe5\xae\x8c\xe5\x8a\xa8\xe9\x87\x8f\xe4\xb9\x8b\xe5\x90\x8e loss \xe8\x83\xbd\xe4\xb8\x8b\xe9\x99\x8d\xe9\x9d\x9e\xe5\xb8\xb8\xe5\xbf\xab\xef\xbc\x8c\xe4\xbd\x86\xe6\x98\xaf\xe4\xb8\x80\xe5\xae\x9a\xe8\xa6\x81\xe5\xb0\x8f\xe5\xbf\x83\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\xe5\x92\x8c\xe5\x8a\xa8\xe9\x87\x8f\xe5\x8f\x82\xe6\x95\xb0\xef\xbc\x8c\xe8\xbf\x99\xe4\xb8\xa4\xe4\xb8\xaa\xe5\x80\xbc\xe4\xbc\x9a\xe7\x9b\xb4\xe6\x8e\xa5\xe5\xbd\xb1\xe5\x93\x8d\xe5\x88\xb0\xe5\x8f\x82\xe6\x95\xb0\xe6\xaf\x8f\xe6\xac\xa1\xe6\x9b\xb4\xe6\x96\xb0\xe7\x9a\x84\xe5\xb9\x85\xe5\xba\xa6\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe5\x8f\xaf\xe4\xbb\xa5\xe5\xa4\x9a\xe8\xaf\x95\xe5\x87\xa0\xe4\xb8\xaa\xe5\x80\xbc\n#%% [markdown]\n# \xe5\xbd\x93\xe7\x84\xb6\xef\xbc\x8cpytorch \xe5\x86\x85\xe7\xbd\xae\xe4\xba\x86\xe5\x8a\xa8\xe9\x87\x8f\xe6\xb3\x95\xe7\x9a\x84\xe5\xae\x9e\xe7\x8e\xb0\xef\xbc\x8c\xe9\x9d\x9e\xe5\xb8\xb8\xe7\xae\x80\xe5\x8d\x95\xef\xbc\x8c\xe7\x9b\xb4\xe6\x8e\xa5\xe5\x9c\xa8 `torch.optim.SGD(momentum=0.9)` \xe5\x8d\xb3\xe5\x8f\xaf\xef\xbc\x8c\xe4\xb8\x8b\xe9\x9d\xa2\xe5\xae\x9e\xe7\x8e\xb0\xe4\xb8\x80\xe4\xb8\x8b\n\n#%%\ntrain_data = DataLoader(train_set, batch_size=64, shuffle=True)\n# \xe4\xbd\xbf\xe7\x94\xa8 Sequential \xe5\xae\x9a\xe4\xb9\x89 3 \xe5\xb1\x82\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\nnet = nn.Sequential(\n    nn.Linear(784, 200),\n    nn.ReLU(),\n    nn.Linear(200, 10),\n)\n\noptimizer = torch.optim.SGD(net.parameters(), lr=1e-2, momentum=0.9) # \xe5\x8a\xa0\xe5\x8a\xa8\xe9\x87\x8f\n# \xe5\xbc\x80\xe5\xa7\x8b\xe8\xae\xad\xe7\xbb\x83\nlosses = []\nidx = 0\nstart = time.time() # \xe8\xae\xb0\xe6\x97\xb6\xe5\xbc\x80\xe5\xa7\x8b\nfor e in range(5):\n    train_loss = 0\n    for im, label in train_data:\n        # \xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\n        out = net(im)\n        loss = criterion(out, label)\n        # \xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        # \xe8\xae\xb0\xe5\xbd\x95\xe8\xaf\xaf\xe5\xb7\xae\n        train_loss += loss.item()\n        if idx % 30 == 0: # 30 \xe6\xad\xa5\xe8\xae\xb0\xe5\xbd\x95\xe4\xb8\x80\xe6\xac\xa1\n            losses.append(loss.item())\n        idx += 1\n    print('epoch: {}, Train Loss: {:.6f}'.format(e, train_loss / len(train_data)))\nend = time.time() # \xe8\xae\xa1\xe6\x97\xb6\xe7\xbb\x93\xe6\x9d\x9f\nprint('\xe4\xbd\xbf\xe7\x94\xa8\xe6\x97\xb6\xe9\x97\xb4: {:.5f} s'.format(end - start))\n\n\n#%%\nx_axis = np.linspace(0, 5, len(losses), endpoint=True)\nplt.semilogy(x_axis, losses, label='momentum: 0.9')\nplt.legend(loc='best')\n\n#%% [markdown]\n# \xe6\x88\x91\xe4\xbb\xac\xe5\x8f\xaf\xe4\xbb\xa5\xe5\xaf\xb9\xe6\xaf\x94\xe4\xb8\x80\xe4\xb8\x8b\xe4\xb8\x8d\xe5\x8a\xa0\xe5\x8a\xa8\xe9\x87\x8f\xe7\x9a\x84\xe9\x9a\x8f\xe6\x9c\xba\xe6\xa2\xaf\xe5\xba\xa6\xe4\xb8\x8b\xe9\x99\x8d\xe6\xb3\x95\n\n#%%\n# \xe4\xbd\xbf\xe7\x94\xa8 Sequential \xe5\xae\x9a\xe4\xb9\x89 3 \xe5\xb1\x82\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\nnet = nn.Sequential(\n    nn.Linear(784, 200),\n    nn.ReLU(),\n    nn.Linear(200, 10),\n)\n\noptimizer = torch.optim.SGD(net.parameters(), lr=1e-2) # \xe4\xb8\x8d\xe5\x8a\xa0\xe5\x8a\xa8\xe9\x87\x8f\n# \xe5\xbc\x80\xe5\xa7\x8b\xe8\xae\xad\xe7\xbb\x83\nlosses1 = []\nidx = 0\nstart = time.time() # \xe8\xae\xb0\xe6\x97\xb6\xe5\xbc\x80\xe5\xa7\x8b\nfor e in range(5):\n    train_loss = 0\n    for im, label in train_data:\n        # \xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\n        out = net(im)\n        loss = criterion(out, label)\n        # \xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        # \xe8\xae\xb0\xe5\xbd\x95\xe8\xaf\xaf\xe5\xb7\xae\n        train_loss += loss.item()\n        if idx % 30 == 0: # 30 \xe6\xad\xa5\xe8\xae\xb0\xe5\xbd\x95\xe4\xb8\x80\xe6\xac\xa1\n            losses1.append(loss.item())\n        idx += 1\n    print('epoch: {}, Train Loss: {:.6f}'.format(e, train_loss / len(train_data)))\nend = time.time() # \xe8\xae\xa1\xe6\x97\xb6\xe7\xbb\x93\xe6\x9d\x9f\nprint('\xe4\xbd\xbf\xe7\x94\xa8\xe6\x97\xb6\xe9\x97\xb4: {:.5f} s'.format(end - start))\n\n\n#%%\nx_axis = np.linspace(0, 5, len(losses), endpoint=True)\nplt.semilogy(x_axis, losses, label='momentum: 0.9')\nplt.semilogy(np.linspace(0, 5, len(losses1), endpoint=True), losses1, label='no momentum')\nplt.legend(loc='best')\n\n#%% [markdown]\n# \xe5\x8f\xaf\xe4\xbb\xa5\xe7\x9c\x8b\xe5\x88\xb0\xe5\x8a\xa0\xe5\xae\x8c\xe5\x8a\xa8\xe9\x87\x8f\xe4\xb9\x8b\xe5\x90\x8e\xe7\x9a\x84 loss \xe4\xb8\x8b\xe9\x99\x8d\xe7\x9a\x84\xe7\xa8\x8b\xe5\xba\xa6\xe6\x9b\xb4\xe4\xbd\x8e\xe4\xba\x86\xef\xbc\x8c\xe5\x8f\xaf\xe4\xbb\xa5\xe5\xb0\x86\xe5\x8a\xa8\xe9\x87\x8f\xe7\x90\x86\xe8\xa7\xa3\xe4\xb8\xba\xe4\xb8\x80\xe7\xa7\x8d\xe6\x83\xaf\xe6\x80\xa7\xe4\xbd\x9c\xe7\x94\xa8\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe6\xaf\x8f\xe6\xac\xa1\xe6\x9b\xb4\xe6\x96\xb0\xe7\x9a\x84\xe5\xb9\x85\xe5\xba\xa6\xe9\x83\xbd\xe4\xbc\x9a\xe6\xaf\x94\xe4\xb8\x8d\xe5\x8a\xa0\xe5\x8a\xa8\xe9\x87\x8f\xe7\x9a\x84\xe6\x83\x85\xe5\x86\xb5\xe6\x9b\xb4\xe5\xa4\x9a\n\n"""
5.Optimizer 优化器/optim-some-exmples.py,14,"b""#%%\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn.functional as F\nimport torch.utils.data as Data\n\n#%%\ntorch.manual_seed(1)  # \xe7\xa1\xae\xe5\xae\x9a\xe9\x9a\x8f\xe6\x9c\xba\xe7\xa7\x8d\xe5\xad\x90\xef\xbc\x8c\xe4\xbf\x9d\xe8\xaf\x81\xe7\xbb\x93\xe6\x9e\x9c\xe5\x8f\xaf\xe9\x87\x8d\xe5\xa4\x8d\n\nLR = 0.01\nBATCH_SIZE = 20\nEPOCH = 10\n\n#%% \xe7\x94\x9f\xe6\x88\x90\xe6\x95\xb0\xe6\x8d\xae\nx = torch.unsqueeze(torch.linspace(-1, 1, 1500), dim=1)\ny = x.pow(3) + 0.1 * torch.normal(torch.zeros(*x.size()))\n\n#%% \xe7\xbb\x98\xe5\x88\xb6\xe6\x95\xb0\xe6\x8d\xae\xe5\x88\x86\xe5\xb8\x83\nplt.scatter(x.numpy(), y.numpy())\nplt.show()\n\n#%% \xe6\x8a\x8a\xe6\x95\xb0\xe6\x8d\xae\xe8\xbd\xac\xe6\x8d\xa2\xe4\xb8\xbatorch\xe7\xb1\xbb\xe5\x9e\x8b\ntorch_dataset = Data.TensorDataset(x, y)\nloader = Data.DataLoader(dataset=torch_dataset, \n                        batch_size=BATCH_SIZE, \n                        shuffle=True, \n                        num_workers=2, )\n\n\n#%% \xe5\xae\x9a\xe4\xb9\x89\xe6\xa8\xa1\xe5\x9e\x8b\nclass Net(torch.nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.hidden = torch.nn.Linear(1, 20)  # \xe9\x9a\x90\xe8\x97\x8f\xe5\xb1\x82\n        self.predict = torch.nn.Linear(20, 1)  # \xe8\xbe\x93\xe5\x87\xba\xe5\xb1\x82\n\n    def forward(self, x):\n        # pdb.set_trace()\n        x = F.relu(self.hidden(x))  # \xe9\x9a\x90\xe8\x97\x8f\xe5\xb1\x82\xe7\x9a\x84\xe6\xbf\x80\xe6\xb4\xbb\xe5\x87\xbd\xe6\x95\xb0\n        x = self.predict(x)  # \xe7\xba\xbf\xe6\x80\xa7\xe8\xbe\x93\xe5\x87\xba\n        return x\n\n\n#%% \xe4\xb8\x8d\xe5\x90\x8c\xe7\x9a\x84\xe7\xbd\x91\xe7\xbb\x9c\xe6\xa8\xa1\xe5\x9e\x8b\nnet_SGD = Net()\nnet_Momentum = Net()\nnet_RMSprop = Net()\nnet_AdaGrad = Net()\nnet_Adam = Net()\n\nnets = [net_SGD, net_Momentum, net_AdaGrad, net_RMSprop, net_Adam]\n#%% \xe4\xb8\x8d\xe5\x90\x8c\xe7\x9a\x84\xe4\xbc\x98\xe5\x8c\x96\xe5\x99\xa8\nopt_SGD = torch.optim.SGD(net_SGD.parameters(), lr=LR)\nopt_Momentum = torch.optim.SGD(net_Momentum.parameters(), lr=LR, momentum=0.8)\n\nopt_AdaGrad = torch.optim.Adagrad(net_AdaGrad.parameters(), lr=LR)\nopt_RMSprop = torch.optim.RMSprop(net_RMSprop.parameters(), lr=LR, alpha=0.9)\nopt_Adam = torch.optim.Adam(net_Adam.parameters(), lr=LR, betas=(0.9, 0.99))\noptimizers = [opt_SGD, opt_Momentum, opt_AdaGrad, opt_RMSprop, opt_Adam]\n#%%\nloss_func = torch.nn.MSELoss()\nlosses_his = [[], [], [], [], []]  # \xe8\xae\xb0\xe5\xbd\x95loss\xe7\x94\xa8\n\n#%% \xe6\xa8\xa1\xe5\x9e\x8b\xe8\xae\xad\xe7\xbb\x83\nfor epoch in range(EPOCH):\n    print('Epoch: ', epoch)\n    for step, (batch_x, batch_y) in enumerate(loader):\n        b_x = batch_x\n        b_y = batch_y\n\n        for net, opt, l_his in zip(nets, optimizers, losses_his):\n            output = net(b_x)  # \xe5\x89\x8d\xe5\x90\x91\xe7\xae\x97\xe6\xb3\x95\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x9c\n            loss = loss_func(output, b_y)  # \xe8\xae\xa1\xe7\xae\x97loss\n            opt.zero_grad()  # \xe6\xa2\xaf\xe5\xba\xa6\xe6\xb8\x85\xe9\x9b\xb6\n            loss.backward()  # \xe5\x90\x8e\xe5\x90\x91\xe7\xae\x97\xe6\xb3\x95\xef\xbc\x8c\xe8\xae\xa1\xe7\xae\x97\xe6\xa2\xaf\xe5\xba\xa6\n            opt.step()  # \xe5\xba\x94\xe7\x94\xa8\xe6\xa2\xaf\xe5\xba\xa6\n            l_his.append(loss.item())  # \xe8\xae\xb0\xe5\xbd\x95loss\n\n#%%\nlabels = ['SGD', 'Momentum', 'AdaGrad', 'RMSprop', 'Adam']\nfor i, l_his in enumerate(losses_his):\n    plt.plot(l_his, label=labels[i])\nplt.legend(loc='best')\nplt.xlabel('Steps')\nplt.ylabel('Loss')\nplt.ylim((0, 0.2))\nplt.show()\n"""
5.Optimizer 优化器/simple-optim.py,5,"b""#%% [markdown]\n# # \xe6\xa2\xaf\xe5\xba\xa6\xe5\x92\x8c\xe4\xbc\x98\xe5\x8c\x96\n\n#%%\nfrom math import pi\nimport torch\nimport torch.optim\n\n#%% [markdown]\n# \xe6\xb1\x82\xe6\xa2\xaf\xe5\xba\xa6\n\n#%%\nx = torch.tensor([pi / 3,  pi / 6], requires_grad=True)\nf = - ((x.cos() ** 2).sum()) ** 2\nprint('\xe5\x87\xbd\xe6\x95\xb0\xe5\x80\xbc = {}'.format(f))\nf.backward()\nprint('\xe6\xa2\xaf\xe5\xba\xa6\xe5\x80\xbc = {}'.format(x.grad))\nref = 2 * (torch.cos(x) ** 2).sum() * torch.sin(2 * x)\nprint('\xe6\xa2\xaf\xe5\xba\xa6\xe5\x80\xbc(\xe5\x8f\x82\xe8\x80\x83) = {}'.format(ref))\n\n#%% [markdown]\n# \xe4\xbc\x98\xe5\x8c\x96\xe9\x97\xae\xe9\xa2\x98\xe6\xb1\x82\xe8\xa7\xa3\n\n#%%\nx = torch.tensor([pi / 3,  pi / 6], requires_grad=True)\noptimizer = torch.optim.SGD([x,], lr=0.1, momentum=0)\nfor step in range(11):\n    if step:\n        optimizer.zero_grad()\n        f.backward()\n        optimizer.step()\n    f = - ((x.cos() ** 2).sum()) ** 2\n    print ('step {}: x = {}, f(x) = {}'.format(step, x.tolist(), f))\n\n\n"""
5.Optimizer 优化器/求梯度.py,5,"b""#%% [markdown]\n# # \xe6\xa2\xaf\xe5\xba\xa6\xe5\x92\x8c\xe4\xbc\x98\xe5\x8c\x96\n\n#%%\nfrom math import pi\nimport torch\nimport torch.optim\n\n#%% [markdown]\n# \xe6\xb1\x82\xe6\xa2\xaf\xe5\xba\xa6\n\n#%%\nx = torch.tensor([pi / 3,  pi / 6], requires_grad=True)\nf = - ((x.cos() ** 2).sum()) ** 2\nprint('\xe5\x87\xbd\xe6\x95\xb0\xe5\x80\xbc = {}'.format(f))\nf.backward()\nprint('\xe6\xa2\xaf\xe5\xba\xa6\xe5\x80\xbc = {}'.format(x.grad))\nref = 2 * (torch.cos(x) ** 2).sum() * torch.sin(2 * x)\nprint('\xe6\xa2\xaf\xe5\xba\xa6\xe5\x80\xbc(\xe5\x8f\x82\xe8\x80\x83) = {}'.format(ref))\n\n#%% [markdown]\n# \xe4\xbc\x98\xe5\x8c\x96\xe9\x97\xae\xe9\xa2\x98\xe6\xb1\x82\xe8\xa7\xa3\n\n#%%\nx = torch.tensor([pi / 3,  pi / 6], requires_grad=True)\noptimizer = torch.optim.SGD([x,], lr=0.1, momentum=0)\nfor step in range(11):\n    if step:\n        optimizer.zero_grad()\n        f.backward()\n        optimizer.step()\n    f = - ((x.cos() ** 2).sum()) ** 2\n    print ('step {}: x = {}, f(x) = {}'.format(step, x.tolist(), f))\n\n\n"""
6.Neural_Network 神经网络/FCNN 全连接网络.py,10,"b'#%% [markdown]\n# # \xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe7\xbd\x91\xe7\xbb\x9c\n\n#%%\nimport torch.nn\n\n#%% [markdown]\n# \xe5\x89\x8d\xe9\xa6\x88\xe7\xbd\x91\xe7\xbb\x9c\xe7\x9a\x84\xe6\x90\xad\xe5\xbb\xba\n\n#%%\nnet = torch.nn.Sequential(\n        torch.nn.Linear(2, 3),\n        torch.nn.ReLU(),\n        torch.nn.Linear(3, 1),\n        torch.nn.ReLU())\nnet\n\n#%% [markdown]\n# \xe9\x9d\x9e\xe7\xba\xbf\xe6\x80\xa7\xe6\xbf\x80\xe6\xb4\xbb\xe5\xb1\x82\xe7\x94\xa8\xe6\xb3\x95\n\n#%%\nac = torch.nn.Softmax(dim=1)\nx = torch.tensor([[1., 2.],[3., 4.]], requires_grad=True)\nac(x)\n\n\n#%%\nac = torch.nn.Softmax2d()\nx = torch.arange(16, requires_grad=True).reshape(2, 2, 2, 2)\nac(x)\n\n\n'"
6.Neural_Network 神经网络/Himmelblau函数的模拟.py,9,"b""#%% [markdown]\n# # Himmelblau \xe5\x87\xbd\xe6\x95\xb0\xe7\x9a\x84\xe6\xa8\xa1\xe6\x8b\x9f\n\n#%%\nimport torch\nimport torch.nn as nn\nimport torch.optim\n\n#%% [markdown]\n# \xe7\x94\x9f\xe6\x88\x90\xe6\x95\xb0\xe6\x8d\xae\n\n#%%\ntorch.manual_seed(seed=0) # \xe5\x9b\xba\xe5\xae\x9a\xe9\x9a\x8f\xe6\x9c\xba\xe6\x95\xb0\xe7\xa7\x8d\xe5\xad\x90,\xe8\xbf\x99\xe6\xa0\xb7\xe7\x94\x9f\xe6\x88\x90\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe6\x98\xaf\xe7\xa1\xae\xe5\xae\x9a\xe7\x9a\x84\nsample_num = 1000 # \xe7\x94\x9f\xe6\x88\x90\xe6\xa0\xb7\xe6\x9c\xac\xe6\x95\xb0\nfeatures = torch.rand(sample_num, 2)  * 12 - 6 # \xe7\x89\xb9\xe5\xbe\x81\xe6\x95\xb0\xe6\x8d\xae\nnoises = torch.randn(sample_num)\ndef himmelblau(x):\n    return (x[:,0] ** 2 + x[:,1] - 11) ** 2 + (x[:,0] + x[:,1] ** 2 - 7) ** 2\nhims = himmelblau(features) * 0.01\nlabels = hims + noises # \xe6\xa0\x87\xe7\xad\xbe\xe6\x95\xb0\xe6\x8d\xae\n\n\n#%%\ntrain_num, validate_num, test_num = 600, 200, 200 # \xe5\x88\x86\xe5\x89\xb2\xe6\x95\xb0\xe6\x8d\xae\ntrain_mse = torch.mean(noises[:train_num] ** 2)\nvalidate_mse = torch.mean(noises[train_num:-test_num] ** 2)\ntest_mse = torch.mean(noises[-test_num:] ** 2)\nprint ('\xe7\x9c\x9f\xe5\xae\x9e:\xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86MSE = {:g}, \xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86MSE = {:g}, \xe6\xb5\x8b\xe8\xaf\x95\xe9\x9b\x86MSE = {:g}'.format(\n        train_mse, validate_mse, test_mse))\n# \xe8\xbe\x93\xe5\x87\xba: \xe7\x9c\x9f\xe5\xae\x9e:\xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86MSE = 0.918333, \xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86MSE = 0.902182, \xe6\xb5\x8b\xe8\xaf\x95\xe9\x9b\x86MSE = 0.978382\n\n#%% [markdown]\n# \xe6\x90\xad\xe5\xbb\xba\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\n\n#%%\nhidden_features = [6, 2] # \xe6\x8c\x87\xe5\xae\x9a\xe9\x9a\x90\xe5\x90\xab\xe5\xb1\x82\xe6\x95\xb0\nlayers = [nn.Linear(2, hidden_features[0]),]\nfor idx, hidden_feature in enumerate(hidden_features):\n    layers.append(nn.Sigmoid())\n    next_hidden_feature = hidden_features[idx + 1]             if idx + 1 < len(hidden_features) else 1\n    layers.append(nn.Linear(hidden_feature, next_hidden_feature))\nnet = nn.Sequential(*layers) # \xe5\x89\x8d\xe9\xa6\x88\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\nprint('\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xe4\xb8\xba {}'.format(net))\n\n#%% [markdown]\n# \xe8\xae\xad\xe7\xbb\x83\xe7\xbd\x91\xe7\xbb\x9c\n\n#%%\noptimizer = torch.optim.Adam(net.parameters())\ncriterion = nn.MSELoss()\n\ntrain_entry_num = 600 # \xe9\x80\x89\xe6\x8b\xa9\xe8\xae\xad\xe7\xbb\x83\xe6\xa0\xb7\xe6\x9c\xac\xe6\x95\xb0\n    \nnIteration = 100000 # 00 # \xe6\x9c\x80\xe5\xa4\xa7\xe8\xbf\xad\xe4\xbb\xa3\xe6\xac\xa1\xe6\x95\xb0\nfor step in range(nIteration):\n    outputs = net(features)\n    preds = outputs[:, 0]\n    \n    loss_train = criterion(preds[:train_entry_num],\n            labels[:train_entry_num])\n    loss_validate = criterion(preds[train_num:-test_num],\n            labels[train_num:-test_num])\n    if step % 10000 == 0:\n        print ('#{} \xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86MSE = {:g}, \xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86MSE = {:g}'.format(\n                step, loss_train, loss_validate))\n\n    optimizer.zero_grad()\n    loss_train.backward()\n    optimizer.step()\n\nprint ('\xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86MSE = {:g}, \xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86MSE = {:g}'.format(loss_train, loss_validate))\n\n\n"""
6.Neural_Network 神经网络/Sequential-and-Module 多层神经网络，Sequential 和 Module.py,33,"b""#%% [markdown]\n# # \xe5\xa4\x9a\xe5\xb1\x82\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xef\xbc\x8cSequential \xe5\x92\x8c Module\n# \xe9\x80\x9a\xe8\xbf\x87\xe5\x89\x8d\xe9\x9d\xa2\xe6\x88\x91\xe4\xbb\xac\xe4\xba\x86\xe8\xa7\xa3\xe5\x88\xb0\xe4\xba\x86\xe6\x9c\xba\xe5\x99\xa8\xe5\xad\xa6\xe4\xb9\xa0\xe9\xa2\x86\xe5\x9f\x9f\xe4\xb8\xad\xe6\x9c\x80\xe5\xb8\xb8\xe8\xa7\x81\xe7\x9a\x84\xe4\xb8\xa4\xe4\xb8\xaa\xe6\xa8\xa1\xe5\x9e\x8b\xef\xbc\x8c\xe7\xba\xbf\xe6\x80\xa7\xe5\x9b\x9e\xe5\xbd\x92\xe6\xa8\xa1\xe5\x9e\x8b\xe5\x92\x8c Logistic \xe5\x9b\x9e\xe5\xbd\x92\xe6\xa8\xa1\xe5\x9e\x8b\xef\xbc\x8c\xe4\xbb\x96\xe4\xbb\xac\xe5\x88\x86\xe5\x88\xab\xe6\x98\xaf\xe5\xa4\x84\xe7\x90\x86\xe6\x9c\xba\xe5\x99\xa8\xe5\xad\xa6\xe4\xb9\xa0\xe4\xb8\xad\xe6\x9c\x80\xe5\xb8\xb8\xe8\xa7\x81\xe7\x9a\x84\xe4\xb8\xa4\xe7\xb1\xbb\xe9\x97\xae\xe9\xa2\x98-\xe5\x9b\x9e\xe5\xbd\x92\xe9\x97\xae\xe9\xa2\x98\xe5\x92\x8c\xe5\x88\x86\xe7\xb1\xbb\xe9\x97\xae\xe9\xa2\x98\xe3\x80\x82\n# \n# \xe4\xb8\x8b\xe9\x9d\xa2\xe6\x88\x91\xe4\xbb\xac\xe4\xbc\x9a\xe8\xae\xb2\xe7\xac\xac\xe4\xb8\x80\xe4\xb8\xaa\xe6\xb7\xb1\xe5\xba\xa6\xe5\xad\xa6\xe4\xb9\xa0\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\xef\xbc\x8c\xe5\xa4\x9a\xe5\xb1\x82\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xe3\x80\x82\n#%% [markdown]\n# ## \xe5\xa4\x9a\xe5\xb1\x82\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\n# \xe5\x9c\xa8\xe5\x89\x8d\xe9\x9d\xa2\xe7\x9a\x84\xe7\xba\xbf\xe6\x80\xa7\xe5\x9b\x9e\xe5\xbd\x92\xe4\xb8\xad\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe7\x9a\x84\xe5\x85\xac\xe5\xbc\x8f\xe6\x98\xaf $y = w x + b$\xef\xbc\x8c\xe8\x80\x8c\xe5\x9c\xa8 Logistic \xe5\x9b\x9e\xe5\xbd\x92\xe4\xb8\xad\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe7\x9a\x84\xe5\x85\xac\xe5\xbc\x8f\xe6\x98\xaf $y = Sigmoid(w x + b)$\xef\xbc\x8c\xe5\x85\xb6\xe5\xae\x9e\xe5\xae\x83\xe4\xbb\xac\xe9\x83\xbd\xe5\x8f\xaf\xe4\xbb\xa5\xe7\x9c\x8b\xe6\x88\x90\xe5\x8d\x95\xe5\xb1\x82\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xef\xbc\x8c\xe5\x85\xb6\xe4\xb8\xad Sigmoid \xe8\xa2\xab\xe7\xa7\xb0\xe4\xb8\xba\xe6\xbf\x80\xe6\xb4\xbb\xe5\x87\xbd\xe6\x95\xb0\xef\xbc\x8c\xe4\xb9\x8b\xe5\x90\x8e\xe6\x88\x91\xe4\xbb\xac\xe4\xbc\x9a\xe8\xaf\xa6\xe7\xbb\x86\xe4\xbb\x8b\xe7\xbb\x8d\xe6\xbf\x80\xe6\xb4\xbb\xe5\x87\xbd\xe6\x95\xb0\xe4\xbb\xa5\xe5\x8f\x8a\xe4\xb8\xba\xe4\xbb\x80\xe4\xb9\x88\xe5\xbf\x85\xe9\xa1\xbb\xe4\xbd\xbf\xe7\x94\xa8\xe6\xbf\x80\xe6\xb4\xbb\xe5\x87\xbd\xe6\x95\xb0\xef\xbc\x8c\xe4\xb8\x8b\xe9\x9d\xa2\xe6\x88\x91\xe4\xbb\xac\xe4\xbb\x8e\xe7\x90\x86\xe8\xa7\xa3\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xe5\x85\xa5\xe6\x89\x8b\xe3\x80\x82\n#%% [markdown]\n# ### \xe7\x90\x86\xe8\xa7\xa3\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\n# \xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xe7\x9a\x84\xe7\x81\xb5\xe6\x84\x9f\xe6\x9d\xa5\xe8\x87\xaa\xe4\xba\x8e\xe4\xba\xba\xe8\x84\x91\xe7\x9a\x84\xe7\xa5\x9e\xe7\xbb\x8f\xe5\x85\x83\xe7\xb3\xbb\xe7\xbb\x9f\xef\xbc\x8c\xe4\xb8\x8b\xe9\x9d\xa2\xe6\x88\x91\xe4\xbb\xac\xe6\x94\xbe\xe4\xb8\x80\xe5\xbc\xa0\xe4\xba\xba\xe8\x84\x91\xe7\x9a\x84\xe7\xa5\x9e\xe7\xbb\x8f\xe5\x85\x83\xe5\x92\x8c\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xe7\x9a\x84\xe5\xaf\xb9\xe6\xaf\x94\xe5\x9b\xbe(\xe6\x9d\xa5\xe8\x87\xaa cs231n)\n# \n# ![](https://ws4.sinaimg.cn/large/006tNc79ly1fmgiz5mqs3j30or0773zg.jpg)\n# \n# \xe5\xb7\xa6\xe8\xbe\xb9\xe6\x98\xaf\xe4\xb8\x80\xe5\xbc\xa0\xe7\xa5\x9e\xe7\xbb\x8f\xe5\x85\x83\xe7\x9a\x84\xe5\x9b\xbe\xe7\x89\x87\xef\xbc\x8c\xe7\xa5\x9e\xe7\xbb\x8f\xe5\x85\x83\xe9\x80\x9a\xe8\xbf\x87\xe7\xaa\x81\xe8\xa7\xa6\xe6\x8e\xa5\xe5\x8f\x97\xe8\xbe\x93\xe5\x85\xa5\xef\xbc\x8c\xe7\x84\xb6\xe5\x90\x8e\xe9\x80\x9a\xe8\xbf\x87**\xe7\xa5\x9e\xe7\xbb\x8f\xe6\xbf\x80\xe6\xb4\xbb**\xe7\x9a\x84\xe6\x96\xb9\xe5\xbc\x8f\xe4\xbc\xa0\xe8\xbe\x93\xe7\xbb\x99\xe5\x90\x8e\xe9\x9d\xa2\xe7\x9a\x84\xe7\xa5\x9e\xe7\xbb\x8f\xe5\x85\x83\xe3\x80\x82\xe8\xbf\x99\xe5\xaf\xb9\xe6\xaf\x94\xe4\xba\x8e\xe5\x8f\xb3\xe8\xbe\xb9\xe7\x9a\x84\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xef\xbc\x8c\xe9\xa6\x96\xe5\x85\x88\xe6\x8e\xa5\xe5\x8f\x97\xe6\x95\xb0\xe6\x8d\xae\xe8\xbe\x93\xe5\x85\xa5\xef\xbc\x8c\xe7\x84\xb6\xe5\x90\x8e\xe9\x80\x9a\xe8\xbf\x87\xe8\xae\xa1\xe7\xae\x97\xe5\xbe\x97\xe5\x88\xb0\xe7\xbb\x93\xe6\x9e\x9c\xef\xbc\x8c\xe6\x8e\xa5\xe7\x9d\x80\xe7\xbb\x8f\xe8\xbf\x87**\xe6\xbf\x80\xe6\xb4\xbb\xe5\x87\xbd\xe6\x95\xb0**\xef\xbc\x8c\xe5\x86\x8d\xe4\xbc\xa0\xe7\xbb\x99\xe7\xac\xac\xe4\xba\x8c\xe5\xb1\x82\xe7\x9a\x84\xe7\xa5\x9e\xe7\xbb\x8f\xe5\x85\x83\xe3\x80\x82\n# \n# \xe6\x89\x80\xe4\xbb\xa5\xe5\x89\x8d\xe9\x9d\xa2\xe8\xae\xb2\xe7\x9a\x84 logistic \xe5\x9b\x9e\xe5\xbd\x92\xe6\xa8\xa1\xe5\x9e\x8b\xe5\x92\x8c\xe7\xba\xbf\xe6\x80\xa7\xe5\x9b\x9e\xe5\xbd\x92\xe6\xa8\xa1\xe5\x9e\x8b\xe9\x83\xbd\xe5\x8f\xaf\xe4\xbb\xa5\xe7\x9c\x8b\xe5\x81\x9a\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa\xe5\x8d\x95\xe5\xb1\x82\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xef\xbc\x8c\xe8\x80\x8c logistic \xe5\x9b\x9e\xe5\xbd\x92\xe4\xb8\xad\xe4\xbd\xbf\xe7\x94\xa8\xe4\xba\x86\xe6\xbf\x80\xe6\xb4\xbb\xe5\x87\xbd\xe6\x95\xb0 sigmoid\xe3\x80\x82\n# \n# \xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xe4\xbd\xbf\xe7\x94\xa8\xe7\x9a\x84\xe6\xbf\x80\xe6\xb4\xbb\xe5\x87\xbd\xe6\x95\xb0\xe9\x83\xbd\xe6\x98\xaf\xe9\x9d\x9e\xe7\xba\xbf\xe6\x80\xa7\xe7\x9a\x84\xef\xbc\x8c\xe6\xaf\x8f\xe4\xb8\xaa\xe6\xbf\x80\xe6\xb4\xbb\xe5\x87\xbd\xe6\x95\xb0\xe9\x83\xbd\xe8\xbe\x93\xe5\x85\xa5\xe4\xb8\x80\xe4\xb8\xaa\xe5\x80\xbc\xef\xbc\x8c\xe7\x84\xb6\xe5\x90\x8e\xe5\x81\x9a\xe4\xb8\x80\xe7\xa7\x8d\xe7\x89\xb9\xe5\xae\x9a\xe7\x9a\x84\xe6\x95\xb0\xe5\xad\xa6\xe8\xbf\x90\xe7\xae\x97\xe5\xbe\x97\xe5\x88\xb0\xe4\xb8\x80\xe4\xb8\xaa\xe7\xbb\x93\xe6\x9e\x9c\xef\xbc\x8c\xe4\xb8\x8b\xe9\x9d\xa2\xe4\xb8\xbe\xe5\x87\xa0\xe4\xb8\xaa\xe4\xbe\x8b\xe5\xad\x90\n# \n# sigmoid \xe6\xbf\x80\xe6\xb4\xbb\xe5\x87\xbd\xe6\x95\xb0\n# \n# $$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n# \n# ![](https://ws1.sinaimg.cn/large/006tNc79ly1fmgj7yto7gj308w05oa9w.jpg)\n# \n# tanh \xe6\xbf\x80\xe6\xb4\xbb\xe5\x87\xbd\xe6\x95\xb0\n# \n# $$tanh(x) = 2 \\sigma(2x) - 1$$\n# \n# ![](https://ws3.sinaimg.cn/large/006tNc79ly1fmgj8yjdnlj308w05mt8j.jpg)\n# \n# ReLU \xe6\xbf\x80\xe6\xb4\xbb\xe5\x87\xbd\xe6\x95\xb0\n# \n# $$ReLU(x) = max(0, x)$$\n# \n# ![](https://ws1.sinaimg.cn/large/006tNc79ly1fmgj94ky2oj308n05uq2r.jpg)\n# \n# \xe6\x88\x91\xe4\xbb\xac\xe4\xb8\x8b\xe9\x9d\xa2\xe9\x87\x8d\xe7\x82\xb9\xe8\xae\xb2\xe4\xb8\x80\xe8\xae\xb2 ReLU \xe6\xbf\x80\xe6\xb4\xbb\xe5\x87\xbd\xe6\x95\xb0\xef\xbc\x8c\xe5\x9b\xa0\xe4\xb8\xba\xe7\x8e\xb0\xe5\x9c\xa8\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xe4\xb8\xad 90% \xe7\x9a\x84\xe6\x83\x85\xe5\x86\xb5\xe9\x83\xbd\xe6\x98\xaf\xe4\xbd\xbf\xe7\x94\xa8\xe8\xbf\x99\xe4\xb8\xaa\xe6\xbf\x80\xe6\xb4\xbb\xe5\x87\xbd\xe6\x95\xb0\xe3\x80\x82\xe4\xb8\x80\xe8\x88\xac\xe4\xb8\x80\xe4\xb8\xaa\xe4\xb8\x80\xe5\xb1\x82\xe7\x9a\x84\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xe7\x9a\x84\xe5\x85\xac\xe5\xbc\x8f\xe5\xb0\xb1\xe6\x98\xaf $y = max(0, w x + b)$\xef\xbc\x8c\xe4\xb8\x80\xe4\xb8\xaa\xe4\xb8\xa4\xe5\xb1\x82\xe7\x9a\x84\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xe5\xb0\xb1\xe6\x98\xaf $y = w_2\\ max(0, w_1 x + b_1) + b_2$\xef\xbc\x8c\xe9\x9d\x9e\xe5\xb8\xb8\xe7\xae\x80\xe5\x8d\x95\xef\xbc\x8c\xe4\xbd\x86\xe6\x98\xaf\xe5\x8d\xb4\xe5\xbe\x88\xe6\x9c\x89\xe6\x95\x88\xef\xbc\x8c\xe4\xbd\xbf\xe7\x94\xa8\xe8\xbf\x99\xe4\xb8\xaa\xe6\xbf\x80\xe6\xb4\xbb\xe5\x87\xbd\xe6\x95\xb0\xe8\x83\xbd\xe5\xa4\x9f\xe5\x8a\xa0\xe5\xbf\xab\xe6\xa2\xaf\xe5\xba\xa6\xe4\xb8\x8b\xe9\x99\x8d\xe6\xb3\x95\xe7\x9a\x84\xe6\x94\xb6\xe6\x95\x9b\xe9\x80\x9f\xe5\xba\xa6\xef\xbc\x8c\xe5\x90\x8c\xe6\x97\xb6\xe5\xaf\xb9\xe6\xaf\x94\xe4\xb8\x8e\xe5\x85\xb6\xe4\xbb\x96\xe7\x9a\x84\xe6\xbf\x80\xe6\xb4\xbb\xe5\x87\xbd\xe6\x95\xb0\xef\xbc\x8c\xe8\xbf\x99\xe4\xb8\xaa\xe6\xbf\x80\xe6\xb4\xbb\xe5\x87\xbd\xe6\x95\xb0\xe8\xae\xa1\xe7\xae\x97\xe6\x9b\xb4\xe5\x8a\xa0\xe7\xae\x80\xe5\x8d\x95\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe7\x8e\xb0\xe5\x9c\xa8\xe5\x8f\x98\xe5\xbe\x97\xe9\x9d\x9e\xe5\xb8\xb8\xe6\xb5\x81\xe8\xa1\x8c\xef\xbc\x8c\xe4\xb9\x8b\xe5\x90\x8e\xe4\xbd\xa0\xe4\xbc\x9a\xe5\x8f\x91\xe7\x8e\xb0\xe6\x88\x91\xe4\xbb\xac\xe6\xbf\x80\xe6\xb4\xbb\xe5\x9c\xa8\xe6\x89\x80\xe6\x9c\x89\xe7\x9a\x84\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xe4\xb8\xad\xe9\x83\xbd\xe4\xbc\x9a\xe4\xbd\xbf\xe7\x94\xa8\xe5\xae\x83\xe3\x80\x82\n#%% [markdown]\n# ## \xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x84\n# \xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xe5\xb0\xb1\xe6\x98\xaf\xe5\xbe\x88\xe5\xa4\x9a\xe4\xb8\xaa\xe7\xa5\x9e\xe7\xbb\x8f\xe5\x85\x83\xe5\xa0\x86\xe5\x9c\xa8\xe4\xb8\x80\xe8\xb5\xb7\xe5\xbd\xa2\xe6\x88\x90\xe4\xb8\x80\xe5\xb1\x82\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xef\xbc\x8c\xe9\x82\xa3\xe4\xb9\x88\xe5\xa4\x9a\xe4\xb8\xaa\xe5\xb1\x82\xe5\xa0\x86\xe5\x8f\xa0\xe5\x9c\xa8\xe4\xb8\x80\xe8\xb5\xb7\xe5\xb0\xb1\xe6\x98\xaf\xe6\xb7\xb1\xe5\xb1\x82\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe5\x8f\xaf\xe4\xbb\xa5\xe9\x80\x9a\xe8\xbf\x87\xe4\xb8\x8b\xe9\x9d\xa2\xe7\x9a\x84\xe5\x9b\xbe\xe5\xb1\x95\xe7\xa4\xba\xe4\xb8\x80\xe4\xb8\xaa\xe4\xb8\xa4\xe5\xb1\x82\xe7\x9a\x84\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xe5\x92\x8c\xe4\xb8\x89\xe5\xb1\x82\xe7\x9a\x84\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\n# \n# ![](https://ws2.sinaimg.cn/large/006tNc79ly1fmgjiafmmjj30nu07075w.jpg)\n# \n# \xe5\x8f\xaf\xe4\xbb\xa5\xe7\x9c\x8b\xe5\x88\xb0\xef\xbc\x8c\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x84\xe5\x85\xb6\xe5\xae\x9e\xe9\x9d\x9e\xe5\xb8\xb8\xe7\xae\x80\xe5\x8d\x95\xef\xbc\x8c\xe4\xb8\xbb\xe8\xa6\x81\xe6\x9c\x89\xe8\xbe\x93\xe5\x85\xa5\xe5\xb1\x82\xef\xbc\x8c\xe9\x9a\x90\xe8\x97\x8f\xe5\xb1\x82\xef\xbc\x8c\xe8\xbe\x93\xe5\x87\xba\xe5\xb1\x82\xe6\x9e\x84\xe6\x88\x90\xef\xbc\x8c\xe8\xbe\x93\xe5\x85\xa5\xe5\xb1\x82\xe9\x9c\x80\xe8\xa6\x81\xe6\xa0\xb9\xe6\x8d\xae\xe7\x89\xb9\xe5\xbe\x81\xe6\x95\xb0\xe7\x9b\xae\xe6\x9d\xa5\xe5\x86\xb3\xe5\xae\x9a\xef\xbc\x8c\xe8\xbe\x93\xe5\x87\xba\xe5\xb1\x82\xe6\xa0\xb9\xe6\x8d\xae\xe8\xa7\xa3\xe5\x86\xb3\xe7\x9a\x84\xe9\x97\xae\xe9\xa2\x98\xe6\x9d\xa5\xe5\x86\xb3\xe5\xae\x9a\xef\xbc\x8c\xe9\x82\xa3\xe4\xb9\x88\xe9\x9a\x90\xe8\x97\x8f\xe5\xb1\x82\xe7\x9a\x84\xe7\xbd\x91\xe8\xb7\xaf\xe5\xb1\x82\xe6\x95\xb0\xe4\xbb\xa5\xe5\x8f\x8a\xe6\xaf\x8f\xe5\xb1\x82\xe7\x9a\x84\xe7\xa5\x9e\xe7\xbb\x8f\xe5\x85\x83\xe6\x95\xb0\xe5\xb0\xb1\xe6\x98\xaf\xe5\x8f\xaf\xe4\xbb\xa5\xe8\xb0\x83\xe8\x8a\x82\xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0\xef\xbc\x8c\xe8\x80\x8c\xe4\xb8\x8d\xe5\x90\x8c\xe7\x9a\x84\xe5\xb1\x82\xe6\x95\xb0\xe5\x92\x8c\xe6\xaf\x8f\xe5\xb1\x82\xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0\xe5\xaf\xb9\xe6\xa8\xa1\xe5\x9e\x8b\xe7\x9a\x84\xe5\xbd\xb1\xe5\x93\x8d\xe9\x9d\x9e\xe5\xb8\xb8\xe5\xa4\xa7\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe7\x9c\x8b\xe7\x9c\x8b\xe8\xbf\x99\xe4\xb8\xaa\xe7\xbd\x91\xe7\xab\x99\xe7\x9a\x84 [demo](http://cs.stanford.edu/people/karpathy/convnetjs/demo/classify2d.html)\n# \n# \xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xe5\x90\x91\xe5\x89\x8d\xe4\xbc\xa0\xe6\x92\xad\xe4\xb9\x9f\xe9\x9d\x9e\xe5\xb8\xb8\xe7\xae\x80\xe5\x8d\x95\xef\xbc\x8c\xe5\xb0\xb1\xe6\x98\xaf\xe4\xb8\x80\xe5\xb1\x82\xe4\xb8\x80\xe5\xb1\x82\xe4\xb8\x8d\xe6\x96\xad\xe5\x81\x9a\xe8\xbf\x90\xe7\xae\x97\xe5\xb0\xb1\xe5\x8f\xaf\xe4\xbb\xa5\xe4\xba\x86\xef\xbc\x8c\xe5\x8f\xaf\xe4\xbb\xa5\xe7\x9c\x8b\xe7\x9c\x8b\xe4\xb8\x8b\xe9\x9d\xa2\xe8\xbf\x99\xe4\xb8\xaa\xe4\xbe\x8b\xe5\xad\x90\n# \n# ![](https://ws2.sinaimg.cn/large/006tNc79ly1fmgj4q1j78g309u0cc4qq.gif)\n#%% [markdown]\n# ## \xe4\xb8\xba\xe4\xbb\x80\xe4\xb9\x88\xe8\xa6\x81\xe4\xbd\xbf\xe7\x94\xa8\xe6\xbf\x80\xe6\xb4\xbb\xe5\x87\xbd\xe6\x95\xb0\n# \xe6\xbf\x80\xe6\xb4\xbb\xe5\x87\xbd\xe6\x95\xb0\xe5\x9c\xa8\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xe4\xb8\xad\xe9\x9d\x9e\xe5\xb8\xb8\xe9\x87\x8d\xe8\xa6\x81\xef\xbc\x8c\xe4\xbd\xbf\xe7\x94\xa8\xe6\xbf\x80\xe6\xb4\xbb\xe5\x87\xbd\xe6\x95\xb0\xe4\xb9\x9f\xe6\x98\xaf\xe9\x9d\x9e\xe5\xb8\xb8\xe5\xbf\x85\xe8\xa6\x81\xe7\x9a\x84\xef\xbc\x8c\xe5\x89\x8d\xe9\x9d\xa2\xe6\x88\x91\xe4\xbb\xac\xe4\xbb\x8e\xe4\xba\xba\xe8\x84\x91\xe7\xa5\x9e\xe7\xbb\x8f\xe5\x85\x83\xe7\x9a\x84\xe8\xa7\x92\xe5\xba\xa6\xe7\x90\x86\xe8\xa7\xa3\xe4\xba\x86\xe6\xbf\x80\xe6\xb4\xbb\xe5\x87\xbd\xe6\x95\xb0\xef\xbc\x8c\xe5\x9b\xa0\xe4\xb8\xba\xe7\xa5\x9e\xe7\xbb\x8f\xe5\x85\x83\xe9\x9c\x80\xe8\xa6\x81\xe9\x80\x9a\xe8\xbf\x87\xe6\xbf\x80\xe6\xb4\xbb\xe6\x89\x8d\xe8\x83\xbd\xe5\xbe\x80\xe5\x90\x8e\xe4\xbc\xa0\xe6\x92\xad\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xe4\xb8\xad\xe9\x9c\x80\xe8\xa6\x81\xe6\xbf\x80\xe6\xb4\xbb\xe5\x87\xbd\xe6\x95\xb0\xef\xbc\x8c\xe4\xb8\x8b\xe9\x9d\xa2\xe6\x88\x91\xe4\xbb\xac\xe4\xbb\x8e\xe6\x95\xb0\xe5\xad\xa6\xe7\x9a\x84\xe8\xa7\x92\xe5\xba\xa6\xe7\x90\x86\xe8\xa7\xa3\xe4\xb8\x80\xe4\xb8\x8b\xe6\xbf\x80\xe6\xb4\xbb\xe5\x87\xbd\xe6\x95\xb0\xe7\x9a\x84\xe5\xbf\x85\xe8\xa6\x81\xe6\x80\xa7\xe3\x80\x82\n# \n# \xe6\xaf\x94\xe5\xa6\x82\xe4\xb8\x80\xe4\xb8\xaa\xe4\xb8\xa4\xe5\xb1\x82\xe7\x9a\x84\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xef\xbc\x8c\xe4\xbd\xbf\xe7\x94\xa8 A \xe8\xa1\xa8\xe7\xa4\xba\xe6\xbf\x80\xe6\xb4\xbb\xe5\x87\xbd\xe6\x95\xb0\xef\xbc\x8c\xe9\x82\xa3\xe4\xb9\x88\n# \n# $$\n# y = w_2 A(w_1 x)\n# $$\n# \n# \xe5\xa6\x82\xe6\x9e\x9c\xe6\x88\x91\xe4\xbb\xac\xe4\xb8\x8d\xe4\xbd\xbf\xe7\x94\xa8\xe6\xbf\x80\xe6\xb4\xbb\xe5\x87\xbd\xe6\x95\xb0\xef\xbc\x8c\xe9\x82\xa3\xe4\xb9\x88\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x9c\xe5\xb0\xb1\xe6\x98\xaf\n# \n# $$\n# y = w_2 (w_1 x) = (w_2 w_1) x = \\bar{w} x\n# $$\n# \n# \xe5\x8f\xaf\xe4\xbb\xa5\xe7\x9c\x8b\xe5\x88\xb0\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe5\xb0\x86\xe4\xb8\xa4\xe5\xb1\x82\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0\xe5\x90\x88\xe5\x9c\xa8\xe4\xb8\x80\xe8\xb5\xb7\xef\xbc\x8c\xe7\x94\xa8 $\\bar{w}$ \xe6\x9d\xa5\xe8\xa1\xa8\xe7\xa4\xba\xef\xbc\x8c\xe4\xb8\xa4\xe5\xb1\x82\xe7\x9a\x84\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xe5\x85\xb6\xe5\xae\x9e\xe5\xb0\xb1\xe5\x8f\x98\xe6\x88\x90\xe4\xba\x86\xe4\xb8\x80\xe5\xb1\x82\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xef\xbc\x8c\xe5\x8f\xaa\xe4\xb8\x8d\xe8\xbf\x87\xe5\x8f\x82\xe6\x95\xb0\xe5\x8f\x98\xe6\x88\x90\xe4\xba\x86\xe6\x96\xb0\xe7\x9a\x84 $\\bar{w}$\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe5\xa6\x82\xe6\x9e\x9c\xe4\xb8\x8d\xe4\xbd\xbf\xe7\x94\xa8\xe6\xbf\x80\xe6\xb4\xbb\xe5\x87\xbd\xe6\x95\xb0\xef\xbc\x8c\xe9\x82\xa3\xe4\xb9\x88\xe4\xb8\x8d\xe7\xae\xa1\xe5\xa4\x9a\xe5\xb0\x91\xe5\xb1\x82\xe7\x9a\x84\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xef\xbc\x8c$y = w_n \\cdots w_2 w_1 x = \\bar{w} x$\xef\xbc\x8c\xe5\xb0\xb1\xe9\x83\xbd\xe5\x8f\x98\xe6\x88\x90\xe4\xba\x86\xe5\x8d\x95\xe5\xb1\x82\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe5\x9c\xa8\xe6\xaf\x8f\xe4\xb8\x80\xe5\xb1\x82\xe6\x88\x91\xe4\xbb\xac\xe9\x83\xbd\xe5\xbf\x85\xe9\xa1\xbb\xe4\xbd\xbf\xe7\x94\xa8\xe6\xbf\x80\xe6\xb4\xbb\xe5\x87\xbd\xe6\x95\xb0\xe3\x80\x82\n# \n# \xe6\x9c\x80\xe5\x90\x8e\xe6\x88\x91\xe4\xbb\xac\xe7\x9c\x8b\xe7\x9c\x8b\xe6\xbf\x80\xe6\xb4\xbb\xe5\x87\xbd\xe6\x95\xb0\xe5\xaf\xb9\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xe7\x9a\x84\xe5\xbd\xb1\xe5\x93\x8d\n# \n# ![](https://ws1.sinaimg.cn/large/006tNc79ly1fmgkeqjr34g306r065diu.gif)\n# \n# \xe5\x8f\xaf\xe4\xbb\xa5\xe7\x9c\x8b\xe5\x88\xb0\xe4\xbd\xbf\xe7\x94\xa8\xe4\xba\x86\xe6\xbf\x80\xe6\xb4\xbb\xe5\x87\xbd\xe6\x95\xb0\xe4\xb9\x8b\xe5\x90\x8e\xef\xbc\x8c\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xe5\x8f\xaf\xe4\xbb\xa5\xe9\x80\x9a\xe8\xbf\x87\xe6\x94\xb9\xe5\x8f\x98\xe6\x9d\x83\xe9\x87\x8d\xe5\xae\x9e\xe7\x8e\xb0\xe4\xbb\xbb\xe6\x84\x8f\xe5\xbd\xa2\xe7\x8a\xb6\xef\xbc\x8c\xe8\xb6\x8a\xe6\x98\xaf\xe5\xa4\x8d\xe6\x9d\x82\xe7\x9a\x84\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xe8\x83\xbd\xe6\x8b\x9f\xe5\x90\x88\xe7\x9a\x84\xe5\xbd\xa2\xe7\x8a\xb6\xe8\xb6\x8a\xe5\xa4\x8d\xe6\x9d\x82\xef\xbc\x8c\xe8\xbf\x99\xe5\xb0\xb1\xe6\x98\xaf\xe8\x91\x97\xe5\x90\x8d\xe7\x9a\x84\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xe4\xb8\x87\xe6\x9c\x89\xe9\x80\xbc\xe8\xbf\x91\xe5\xae\x9a\xe7\x90\x86\xe3\x80\x82\n# \n# \xe4\xb8\x8b\xe9\x9d\xa2\xe6\x88\x91\xe4\xbb\xac\xe9\x80\x9a\xe8\xbf\x87\xe4\xbe\x8b\xe5\xad\x90\xe6\x9d\xa5\xe6\x84\x9f\xe5\x8f\x97\xe4\xb8\x80\xe4\xb8\x8b\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xe7\x9a\x84\xe5\xbc\xba\xe5\xa4\xa7\xe4\xb9\x8b\xe5\xa4\x84\n\n#%%\nimport torch\nimport numpy as np\nfrom torch import nn\nimport torch.nn.functional as F\n\nimport matplotlib.pyplot as plt\nget_ipython().run_line_magic('matplotlib', 'inline')\n\n\n#%%\ndef plot_decision_boundary(model, x, y):\n    # Set min and max values and give it some padding\n    x_min, x_max = x[:, 0].min() - 1, x[:, 0].max() + 1\n    y_min, y_max = x[:, 1].min() - 1, x[:, 1].max() + 1\n    h = 0.01\n    # Generate a grid of points with distance h between them\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n    # Predict the function value for the whole grid\n    Z = model(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    # Plot the contour and training examples\n    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n    plt.ylabel('x2')\n    plt.xlabel('x1')\n    plt.scatter(x[:, 0], x[:, 1], c=y.reshape(-1), s=40, cmap=plt.cm.Spectral)\n\n#%% [markdown]\n# \xe8\xbf\x99\xe6\xac\xa1\xe6\x88\x91\xe4\xbb\xac\xe4\xbb\x8d\xe7\x84\xb6\xe5\xa4\x84\xe7\x90\x86\xe4\xb8\x80\xe4\xb8\xaa\xe4\xba\x8c\xe5\x88\x86\xe7\xb1\xbb\xe9\x97\xae\xe9\xa2\x98\xef\xbc\x8c\xe4\xbd\x86\xe6\x98\xaf\xe6\xaf\x94\xe5\x89\x8d\xe9\x9d\xa2\xe7\x9a\x84 logistic \xe5\x9b\x9e\xe5\xbd\x92\xe6\x9b\xb4\xe5\x8a\xa0\xe5\xa4\x8d\xe6\x9d\x82\n\n#%%\nnp.random.seed(1)\nm = 400 # \xe6\xa0\xb7\xe6\x9c\xac\xe6\x95\xb0\xe9\x87\x8f\nN = int(m/2) # \xe6\xaf\x8f\xe4\xb8\x80\xe7\xb1\xbb\xe7\x9a\x84\xe7\x82\xb9\xe7\x9a\x84\xe4\xb8\xaa\xe6\x95\xb0\nD = 2 # \xe7\xbb\xb4\xe5\xba\xa6\nx = np.zeros((m, D))\ny = np.zeros((m, 1), dtype='uint8') # label \xe5\x90\x91\xe9\x87\x8f\xef\xbc\x8c0 \xe8\xa1\xa8\xe7\xa4\xba\xe7\xba\xa2\xe8\x89\xb2\xef\xbc\x8c1 \xe8\xa1\xa8\xe7\xa4\xba\xe8\x93\x9d\xe8\x89\xb2\na = 4\n\nfor j in range(2):\n    ix = range(N*j,N*(j+1))\n    t = np.linspace(j*3.12,(j+1)*3.12,N) + np.random.randn(N)*0.2 # theta\n    r = a*np.sin(4*t) + np.random.randn(N)*0.2 # radius\n    x[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n    y[ix] = j\n\n\n#%%\nplt.scatter(x[:, 0], x[:, 1], c=y.reshape(-1), s=40, cmap=plt.cm.Spectral)\n\n#%% [markdown]\n# \xe6\x88\x91\xe4\xbb\xac\xe5\x8f\xaf\xe4\xbb\xa5\xe5\x85\x88\xe5\xb0\x9d\xe8\xaf\x95\xe7\x94\xa8 logistic \xe5\x9b\x9e\xe5\xbd\x92\xe6\x9d\xa5\xe8\xa7\xa3\xe5\x86\xb3\xe8\xbf\x99\xe4\xb8\xaa\xe9\x97\xae\xe9\xa2\x98\n\n#%%\nx = torch.from_numpy(x).float()\ny = torch.from_numpy(y).float()\n\n\n#%%\nw = nn.Parameter(torch.randn(2, 1))\nb = nn.Parameter(torch.zeros(1))\n\noptimizer = torch.optim.SGD([w, b], 1e-1)\n\ndef logistic_regression(x):\n    return torch.mm(x, w) + b\n\ncriterion = nn.BCEWithLogitsLoss()\n\n\n#%%\nfor e in range(100):\n    out = logistic_regression(x)\n    loss = criterion(out, y)\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    if (e + 1) % 20 == 0:\n        print('epoch: {}, loss: {}'.format(e+1, loss.item()))\n\n\n#%%\ndef plot_logistic(x):\n    x = torch.from_numpy(x).float()\n    out = torch.sigmoid(logistic_regression(x))\n    out = (out > 0.5) * 1\n    return out.numpy()\n\n\n#%%\nplot_decision_boundary(lambda x: plot_logistic(x), x.numpy(), y.numpy())\nplt.title('logistic regression')\n\n#%% [markdown]\n# \xe5\x8f\xaf\xe4\xbb\xa5\xe7\x9c\x8b\xe5\x88\xb0\xef\xbc\x8clogistic \xe5\x9b\x9e\xe5\xbd\x92\xe5\xb9\xb6\xe4\xb8\x8d\xe8\x83\xbd\xe5\xbe\x88\xe5\xa5\xbd\xe7\x9a\x84\xe5\x8c\xba\xe5\x88\x86\xe5\xbc\x80\xe8\xbf\x99\xe4\xb8\xaa\xe5\xa4\x8d\xe6\x9d\x82\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xef\xbc\x8c\xe5\xa6\x82\xe6\x9e\x9c\xe4\xbd\xa0\xe8\xbf\x98\xe8\xae\xb0\xe5\xbe\x97\xe5\x89\x8d\xe9\x9d\xa2\xe7\x9a\x84\xe5\x86\x85\xe5\xae\xb9\xef\xbc\x8c\xe4\xbd\xa0\xe5\xb0\xb1\xe7\x9f\xa5\xe9\x81\x93 logistic \xe5\x9b\x9e\xe5\xbd\x92\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa\xe7\xba\xbf\xe6\x80\xa7\xe5\x88\x86\xe7\xb1\xbb\xe5\x99\xa8\xef\xbc\x8c\xe8\xbf\x99\xe4\xb8\xaa\xe6\x97\xb6\xe5\x80\x99\xe5\xb0\xb1\xe8\xaf\xa5\xe6\x88\x91\xe4\xbb\xac\xe7\x9a\x84\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xe7\x99\xbb\xe5\x9c\xba\xe4\xba\x86\xef\xbc\x81\n\n#%%\n# \xe5\xae\x9a\xe4\xb9\x89\xe4\xb8\xa4\xe5\xb1\x82\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0\nw1 = nn.Parameter(torch.randn(2, 4) * 0.01) # \xe9\x9a\x90\xe8\x97\x8f\xe5\xb1\x82\xe7\xa5\x9e\xe7\xbb\x8f\xe5\x85\x83\xe4\xb8\xaa\xe6\x95\xb0 2\nb1 = nn.Parameter(torch.zeros(4))\n\nw2 = nn.Parameter(torch.randn(4, 1) * 0.01)\nb2 = nn.Parameter(torch.zeros(1))\n\n# \xe5\xae\x9a\xe4\xb9\x89\xe6\xa8\xa1\xe5\x9e\x8b\ndef two_network(x):\n    x1 = torch.mm(x, w1) + b1\n    x1 = torch.tanh(x1) # \xe4\xbd\xbf\xe7\x94\xa8 PyTorch \xe8\x87\xaa\xe5\xb8\xa6\xe7\x9a\x84 tanh \xe6\xbf\x80\xe6\xb4\xbb\xe5\x87\xbd\xe6\x95\xb0\n    x2 = torch.mm(x1, w2) + b2\n    return x2\n\noptimizer = torch.optim.SGD([w1, w2, b1, b2], 1.)\n\ncriterion = nn.BCEWithLogitsLoss()\n\n\n#%%\n# \xe6\x88\x91\xe4\xbb\xac\xe8\xae\xad\xe7\xbb\x83 10000 \xe6\xac\xa1\nfor e in range(10000):\n    out = two_network(x)\n    loss = criterion(out, y)\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    if (e + 1) % 1000 == 0:\n        print('epoch: {}, loss: {}'.format(e+1, loss.item()))\n\n\n#%%\ndef plot_network(x):\n    x = torch.from_numpy(x).float()\n    x1 = torch.mm(x, w1) + b1\n    x1 = torch.tanh(x1)\n    x2 = torch.mm(x1, w2) + b2\n    out = torch.sigmoid(x2)\n    out = (out > 0.5) * 1\n    return out.numpy()\n\n\n#%%\nplot_decision_boundary(lambda x: plot_network(x), x.numpy(), y.numpy())\nplt.title('2 layer network')\n\n#%% [markdown]\n# \xe5\x8f\xaf\xe4\xbb\xa5\xe7\x9c\x8b\xe5\x88\xb0\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xe8\x83\xbd\xe5\xa4\x9f\xe9\x9d\x9e\xe5\xb8\xb8\xe5\xa5\xbd\xe5\x9c\xb0\xe5\x88\x86\xe7\xb1\xbb\xe8\xbf\x99\xe4\xb8\xaa\xe5\xa4\x8d\xe6\x9d\x82\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xef\xbc\x8c\xe5\x92\x8c\xe5\x89\x8d\xe9\x9d\xa2\xe7\x9a\x84 logistic \xe5\x9b\x9e\xe5\xbd\x92\xe7\x9b\xb8\xe6\xaf\x94\xef\xbc\x8c\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xe5\x9b\xa0\xe4\xb8\xba\xe6\x9c\x89\xe4\xba\x86\xe6\xbf\x80\xe6\xb4\xbb\xe5\x87\xbd\xe6\x95\xb0\xe7\x9a\x84\xe5\xad\x98\xe5\x9c\xa8\xef\xbc\x8c\xe6\x88\x90\xe4\xba\x86\xe4\xb8\x80\xe4\xb8\xaa\xe9\x9d\x9e\xe7\xba\xbf\xe6\x80\xa7\xe5\x88\x86\xe7\xb1\xbb\xe5\x99\xa8\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xe5\x88\x86\xe7\xb1\xbb\xe7\x9a\x84\xe8\xbe\xb9\xe7\x95\x8c\xe6\x9b\xb4\xe5\x8a\xa0\xe5\xa4\x8d\xe6\x9d\x82\xe3\x80\x82\n#%% [markdown]\n# ## Sequential \xe5\x92\x8c Module\n#%% [markdown]\n# \xe6\x88\x91\xe4\xbb\xac\xe8\xae\xb2\xe4\xba\x86\xe6\x95\xb0\xe6\x8d\xae\xe5\xa4\x84\xe7\x90\x86\xef\xbc\x8c\xe6\xa8\xa1\xe5\x9e\x8b\xe6\x9e\x84\xe5\xbb\xba\xef\xbc\x8closs \xe5\x87\xbd\xe6\x95\xb0\xe8\xae\xbe\xe8\xae\xa1\xe7\xad\x89\xe7\xad\x89\xe5\x86\x85\xe5\xae\xb9\xef\xbc\x8c\xe4\xbd\x86\xe6\x98\xaf\xe7\x9b\xae\xe5\x89\x8d\xe4\xb8\xba\xe6\xad\xa2\xe6\x88\x91\xe4\xbb\xac\xe8\xbf\x98\xe6\xb2\xa1\xe6\x9c\x89\xe5\x87\x86\xe5\xa4\x87\xe5\xa5\xbd\xe6\x9e\x84\xe5\xbb\xba\xe4\xb8\x80\xe4\xb8\xaa\xe5\xae\x8c\xe6\x95\xb4\xe7\x9a\x84\xe6\x9c\xba\xe5\x99\xa8\xe5\xad\xa6\xe4\xb9\xa0\xe7\xb3\xbb\xe7\xbb\x9f\xef\xbc\x8c\xe4\xb8\x80\xe4\xb8\xaa\xe5\xae\x8c\xe6\x95\xb4\xe7\x9a\x84\xe6\x9c\xba\xe5\x99\xa8\xe5\xad\xa6\xe4\xb9\xa0\xe7\xb3\xbb\xe7\xbb\x9f\xe9\x9c\x80\xe8\xa6\x81\xe6\x88\x91\xe4\xbb\xac\xe4\xb8\x8d\xe6\x96\xad\xe5\x9c\xb0\xe8\xaf\xbb\xe5\x86\x99\xe6\xa8\xa1\xe5\x9e\x8b\xe3\x80\x82\xe5\x9c\xa8\xe7\x8e\xb0\xe5\xae\x9e\xe5\xba\x94\xe7\x94\xa8\xe4\xb8\xad\xef\xbc\x8c\xe4\xb8\x80\xe8\x88\xac\xe6\x88\x91\xe4\xbb\xac\xe4\xbc\x9a\xe5\xb0\x86\xe6\xa8\xa1\xe5\x9e\x8b\xe5\x9c\xa8\xe6\x9c\xac\xe5\x9c\xb0\xe8\xbf\x9b\xe8\xa1\x8c\xe8\xae\xad\xe7\xbb\x83\xef\xbc\x8c\xe7\x84\xb6\xe5\x90\x8e\xe4\xbf\x9d\xe5\xad\x98\xe6\xa8\xa1\xe5\x9e\x8b\xef\xbc\x8c\xe6\x8e\xa5\xe7\x9d\x80\xe6\x88\x91\xe4\xbb\xac\xe4\xbc\x9a\xe5\xb0\x86\xe6\xa8\xa1\xe5\x9e\x8b\xe9\x83\xa8\xe7\xbd\xb2\xe5\x88\xb0\xe4\xb8\x8d\xe5\x90\x8c\xe7\x9a\x84\xe5\x9c\xb0\xe6\x96\xb9\xe8\xbf\x9b\xe8\xa1\x8c\xe5\xba\x94\xe7\x94\xa8\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe5\x9c\xa8\xe8\xbf\x99\xe8\x8a\x82\xe8\xaf\xbe\xe6\x88\x91\xe4\xbb\xac\xe4\xbc\x9a\xe6\x95\x99\xe5\xa4\xa7\xe5\xae\xb6\xe5\xa6\x82\xe4\xbd\x95\xe4\xbf\x9d\xe5\xad\x98 PyTorch \xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\xe3\x80\x82\n# \n# \xe9\xa6\x96\xe5\x85\x88\xe6\x88\x91\xe4\xbb\xac\xe4\xbc\x9a\xe8\xae\xb2\xe4\xb8\x80\xe4\xb8\x8b PyTorch \xe4\xb8\xad\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9d\x97\xef\xbc\x8cSequential \xe5\x92\x8c Module\xe3\x80\x82\n#%% [markdown]\n# \n# \xe5\xaf\xb9\xe4\xba\x8e\xe5\x89\x8d\xe9\x9d\xa2\xe7\x9a\x84\xe7\xba\xbf\xe6\x80\xa7\xe5\x9b\x9e\xe5\xbd\x92\xe6\xa8\xa1\xe5\x9e\x8b\xe3\x80\x81 Logistic\xe5\x9b\x9e\xe5\xbd\x92\xe6\xa8\xa1\xe5\x9e\x8b\xe5\x92\x8c\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe5\x9c\xa8\xe6\x9e\x84\xe5\xbb\xba\xe7\x9a\x84\xe6\x97\xb6\xe5\x80\x99\xe5\xae\x9a\xe4\xb9\x89\xe4\xba\x86\xe9\x9c\x80\xe8\xa6\x81\xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0\xe3\x80\x82\xe8\xbf\x99\xe5\xaf\xb9\xe4\xba\x8e\xe6\xaf\x94\xe8\xbe\x83\xe5\xb0\x8f\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\xe6\x98\xaf\xe5\x8f\xaf\xe8\xa1\x8c\xe7\x9a\x84\xef\xbc\x8c\xe4\xbd\x86\xe6\x98\xaf\xe5\xaf\xb9\xe4\xba\x8e\xe5\xa4\xa7\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\xef\xbc\x8c\xe6\xaf\x94\xe5\xa6\x82100 \xe5\xb1\x82\xe7\x9a\x84\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xef\xbc\x8c\xe8\xbf\x99\xe4\xb8\xaa\xe6\x97\xb6\xe5\x80\x99\xe5\x86\x8d\xe5\x8e\xbb\xe6\x89\x8b\xe5\x8a\xa8\xe5\xae\x9a\xe4\xb9\x89\xe5\x8f\x82\xe6\x95\xb0\xe5\xb0\xb1\xe6\x98\xbe\xe5\xbe\x97\xe9\x9d\x9e\xe5\xb8\xb8\xe9\xba\xbb\xe7\x83\xa6\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5 PyTorch \xe6\x8f\x90\xe4\xbe\x9b\xe4\xba\x86\xe4\xb8\xa4\xe4\xb8\xaa\xe6\xa8\xa1\xe5\x9d\x97\xe6\x9d\xa5\xe5\xb8\xae\xe5\x8a\xa9\xe6\x88\x91\xe4\xbb\xac\xe6\x9e\x84\xe5\xbb\xba\xe6\xa8\xa1\xe5\x9e\x8b\xef\xbc\x8c\xe4\xb8\x80\xe4\xb8\xaa\xe6\x98\xafSequential\xef\xbc\x8c\xe4\xb8\x80\xe4\xb8\xaa\xe6\x98\xaf Module\xe3\x80\x82\n# \n# Sequential \xe5\x85\x81\xe8\xae\xb8\xe6\x88\x91\xe4\xbb\xac\xe6\x9e\x84\xe5\xbb\xba\xe5\xba\x8f\xe5\x88\x97\xe5\x8c\x96\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9d\x97\xef\xbc\x8c\xe8\x80\x8c Module \xe6\x98\xaf\xe4\xb8\x80\xe7\xa7\x8d\xe6\x9b\xb4\xe5\x8a\xa0\xe7\x81\xb5\xe6\xb4\xbb\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\xe5\xae\x9a\xe4\xb9\x89\xe6\x96\xb9\xe5\xbc\x8f\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe4\xb8\x8b\xe9\x9d\xa2\xe5\x88\x86\xe5\x88\xab\xe7\x94\xa8 Sequential \xe5\x92\x8c Module \xe6\x9d\xa5\xe5\xae\x9a\xe4\xb9\x89\xe4\xb8\x8a\xe9\x9d\xa2\xe7\x9a\x84\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xe3\x80\x82\n\n#%%\n# Sequential\nseq_net = nn.Sequential(\n    nn.Linear(2, 4), # PyTorch \xe4\xb8\xad\xe7\x9a\x84\xe7\xba\xbf\xe6\x80\xa7\xe5\xb1\x82\xef\xbc\x8cwx + b\n    nn.Tanh(),\n    nn.Linear(4, 1)\n)\n\n\n#%%\nseq_net\n\n\n#%%\n# \xe5\xba\x8f\xe5\x88\x97\xe6\xa8\xa1\xe5\x9d\x97\xe5\x8f\xaf\xe4\xbb\xa5\xe9\x80\x9a\xe8\xbf\x87\xe7\xb4\xa2\xe5\xbc\x95\xe8\xae\xbf\xe9\x97\xae\xe6\xaf\x8f\xe4\xb8\x80\xe5\xb1\x82\n\nseq_net[0] # \xe7\xac\xac\xe4\xb8\x80\xe5\xb1\x82\n\n\n#%%\n# \xe6\x89\x93\xe5\x8d\xb0\xe5\x87\xba\xe7\xac\xac\xe4\xb8\x80\xe5\xb1\x82\xe7\x9a\x84\xe6\x9d\x83\xe9\x87\x8d\n\nw0 = seq_net[0].weight\nprint(w0)\n\n\n#%%\n# \xe9\x80\x9a\xe8\xbf\x87 parameters \xe5\x8f\xaf\xe4\xbb\xa5\xe5\x8f\x96\xe5\xbe\x97\xe6\xa8\xa1\xe5\x9e\x8b\xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0\nparam = seq_net.parameters()\n\n# \xe5\xae\x9a\xe4\xb9\x89\xe4\xbc\x98\xe5\x8c\x96\xe5\x99\xa8\noptim = torch.optim.SGD(param, 1.)\n\n\n#%%\n# \xe6\x88\x91\xe4\xbb\xac\xe8\xae\xad\xe7\xbb\x83 10000 \xe6\xac\xa1\nfor e in range(10000):\n    out = seq_net(x)\n    loss = criterion(out, y)\n    optim.zero_grad()\n    loss.backward()\n    optim.step()\n    if (e + 1) % 1000 == 0:\n        print('epoch: {}, loss: {}'.format(e+1, loss.item()))\n\n#%% [markdown]\n# \xe5\x8f\xaf\xe4\xbb\xa5\xe7\x9c\x8b\xe5\x88\xb0\xef\xbc\x8c\xe8\xae\xad\xe7\xbb\x83 10000 \xe6\xac\xa1 loss \xe6\xaf\x94\xe4\xb9\x8b\xe5\x89\x8d\xe7\x9a\x84\xe6\x9b\xb4\xe4\xbd\x8e\xef\xbc\x8c\xe8\xbf\x99\xe6\x98\xaf\xe5\x9b\xa0\xe4\xb8\xba PyTorch \xe8\x87\xaa\xe5\xb8\xa6\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9d\x97\xe6\xaf\x94\xe6\x88\x91\xe4\xbb\xac\xe5\x86\x99\xe7\x9a\x84\xe6\x9b\xb4\xe5\x8a\xa0\xe7\xa8\xb3\xe5\xae\x9a\xef\xbc\x8c\xe5\x90\x8c\xe6\x97\xb6\xe4\xb9\x9f\xe6\x9c\x89\xe4\xb8\x80\xe4\xba\x9b\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe7\x9a\x84\xe9\x97\xae\xe9\xa2\x98\xe5\x9c\xa8\xe9\x87\x8c\xe9\x9d\xa2\xef\xbc\x8c\xe5\x85\xb3\xe4\xba\x8e\xe5\x8f\x82\xe6\x95\xb0\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe4\xbc\x9a\xe5\x9c\xa8\xe5\x90\x8e\xe9\x9d\xa2\xe7\x9a\x84\xe8\xaf\xbe\xe7\xa8\x8b\xe4\xb8\xad\xe8\xae\xb2\xe5\x88\xb0\n\n#%%\ndef plot_seq(x):\n    out = torch.sigmoid(seq_net(torch.from_numpy(x).float())).detach().numpy()\n    out = (out > 0.5) * 1\n    return out\n\n\n#%%\nplot_decision_boundary(lambda x: plot_seq(x), x.numpy(), y.numpy())\nplt.title('sequential')\n\n#%% [markdown]\n# \xe6\x9c\x80\xe5\x90\x8e\xe6\x88\x91\xe4\xbb\xac\xe8\xae\xb2\xe4\xb8\x80\xe8\xae\xb2\xe5\xa6\x82\xe4\xbd\x95\xe4\xbf\x9d\xe5\xad\x98\xe6\xa8\xa1\xe5\x9e\x8b\xef\xbc\x8c\xe4\xbf\x9d\xe5\xad\x98\xe6\xa8\xa1\xe5\x9e\x8b\xe5\x9c\xa8 PyTorch \xe4\xb8\xad\xe6\x9c\x89\xe4\xb8\xa4\xe7\xa7\x8d\xe6\x96\xb9\xe5\xbc\x8f\xef\xbc\x8c\xe4\xb8\x80\xe7\xa7\x8d\xe6\x98\xaf\xe5\xb0\x86\xe6\xa8\xa1\xe5\x9e\x8b\xe7\xbb\x93\xe6\x9e\x84\xe5\x92\x8c\xe5\x8f\x82\xe6\x95\xb0\xe9\x83\xbd\xe4\xbf\x9d\xe5\xad\x98\xe5\x9c\xa8\xe4\xb8\x80\xe8\xb5\xb7\xef\xbc\x8c\xe4\xb8\x80\xe7\xa7\x8d\xe6\x98\xaf\xe5\x8f\xaa\xe5\xb0\x86\xe5\x8f\x82\xe6\x95\xb0\xe4\xbf\x9d\xe5\xad\x98\xe4\xb8\x8b\xe6\x9d\xa5\xef\xbc\x8c\xe4\xb8\x8b\xe9\x9d\xa2\xe6\x88\x91\xe4\xbb\xac\xe4\xb8\x80\xe4\xb8\x80\xe4\xbb\x8b\xe7\xbb\x8d\xe3\x80\x82\n\n#%%\n# \xe5\xb0\x86\xe5\x8f\x82\xe6\x95\xb0\xe5\x92\x8c\xe6\xa8\xa1\xe5\x9e\x8b\xe4\xbf\x9d\xe5\xad\x98\xe5\x9c\xa8\xe4\xb8\x80\xe8\xb5\xb7\ntorch.save(seq_net, 'save_seq_net.pth')\n\n#%% [markdown]\n# \xe4\xb8\x8a\xe9\x9d\xa2\xe5\xb0\xb1\xe6\x98\xaf\xe4\xbf\x9d\xe5\xad\x98\xe6\xa8\xa1\xe5\x9e\x8b\xe7\x9a\x84\xe6\x96\xb9\xe5\xbc\x8f\xef\xbc\x8c`torch.save`\xe9\x87\x8c\xe9\x9d\xa2\xe6\x9c\x89\xe4\xb8\xa4\xe4\xb8\xaa\xe5\x8f\x82\xe6\x95\xb0\xef\xbc\x8c\xe7\xac\xac\xe4\xb8\x80\xe4\xb8\xaa\xe6\x98\xaf\xe8\xa6\x81\xe4\xbf\x9d\xe5\xad\x98\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\xef\xbc\x8c\xe7\xac\xac\xe4\xba\x8c\xe4\xb8\xaa\xe5\x8f\x82\xe6\x95\xb0\xe6\x98\xaf\xe4\xbf\x9d\xe5\xad\x98\xe7\x9a\x84\xe8\xb7\xaf\xe5\xbe\x84\xef\xbc\x8c\xe8\xaf\xbb\xe5\x8f\x96\xe6\xa8\xa1\xe5\x9e\x8b\xe7\x9a\x84\xe6\x96\xb9\xe5\xbc\x8f\xe4\xb9\x9f\xe9\x9d\x9e\xe5\xb8\xb8\xe7\xae\x80\xe5\x8d\x95\n\n#%%\n# \xe8\xaf\xbb\xe5\x8f\x96\xe4\xbf\x9d\xe5\xad\x98\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\nseq_net1 = torch.load('save_seq_net.pth')\n\n\n#%%\nseq_net1\n\n\n#%%\nprint(seq_net1[0].weight)\n\n#%% [markdown]\n# \xe6\x88\x91\xe4\xbb\xac\xe5\x8f\xaf\xe4\xbb\xa5\xe7\x9c\x8b\xe5\x88\xb0\xe6\x88\x91\xe4\xbb\xac\xe9\x87\x8d\xe6\x96\xb0\xe8\xaf\xbb\xe5\x85\xa5\xe4\xba\x86\xe6\xa8\xa1\xe5\x9e\x8b\xef\xbc\x8c\xe5\xb9\xb6\xe4\xb8\x94\xe5\xb0\x86\xe5\x85\xb6\xe5\x91\xbd\xe5\x90\x8d\xe4\xb8\xba seq_net1\xef\xbc\x8c\xe5\xb9\xb6\xe4\xb8\x94\xe6\x89\x93\xe5\x8d\xb0\xe4\xba\x86\xe7\xac\xac\xe4\xb8\x80\xe5\xb1\x82\xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0\n# \n# \xe4\xb8\x8b\xe9\x9d\xa2\xe6\x88\x91\xe4\xbb\xac\xe7\x9c\x8b\xe7\x9c\x8b\xe7\xac\xac\xe4\xba\x8c\xe7\xa7\x8d\xe4\xbf\x9d\xe5\xad\x98\xe6\xa8\xa1\xe5\x9e\x8b\xe7\x9a\x84\xe6\x96\xb9\xe5\xbc\x8f\xef\xbc\x8c\xe5\x8f\xaa\xe4\xbf\x9d\xe5\xad\x98\xe5\x8f\x82\xe6\x95\xb0\xe8\x80\x8c\xe4\xb8\x8d\xe4\xbf\x9d\xe5\xad\x98\xe6\xa8\xa1\xe5\x9e\x8b\xe7\xbb\x93\xe6\x9e\x84\n\n#%%\n# \xe4\xbf\x9d\xe5\xad\x98\xe6\xa8\xa1\xe5\x9e\x8b\xe5\x8f\x82\xe6\x95\xb0\ntorch.save(seq_net.state_dict(), 'save_seq_net_params.pth')\n\n#%% [markdown]\n# \xe9\x80\x9a\xe8\xbf\x87\xe4\xb8\x8a\xe9\x9d\xa2\xe7\x9a\x84\xe6\x96\xb9\xe5\xbc\x8f\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe4\xbf\x9d\xe5\xad\x98\xe4\xba\x86\xe6\xa8\xa1\xe5\x9e\x8b\xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0\xef\xbc\x8c\xe5\xa6\x82\xe6\x9e\x9c\xe8\xa6\x81\xe9\x87\x8d\xe6\x96\xb0\xe8\xaf\xbb\xe5\x85\xa5\xe6\xa8\xa1\xe5\x9e\x8b\xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0\xef\xbc\x8c\xe9\xa6\x96\xe5\x85\x88\xe6\x88\x91\xe4\xbb\xac\xe9\x9c\x80\xe8\xa6\x81\xe9\x87\x8d\xe6\x96\xb0\xe5\xae\x9a\xe4\xb9\x89\xe4\xb8\x80\xe6\xac\xa1\xe6\xa8\xa1\xe5\x9e\x8b\xef\xbc\x8c\xe6\x8e\xa5\xe7\x9d\x80\xe9\x87\x8d\xe6\x96\xb0\xe8\xaf\xbb\xe5\x85\xa5\xe5\x8f\x82\xe6\x95\xb0\n\n#%%\nseq_net2 = nn.Sequential(\n    nn.Linear(2, 4),\n    nn.Tanh(),\n    nn.Linear(4, 1)\n)\n\nseq_net2.load_state_dict(torch.load('save_seq_net_params.pth'))\n\n\n#%%\nseq_net2\n\n\n#%%\nprint(seq_net2[0].weight)\n\n#%% [markdown]\n# \xe9\x80\x9a\xe8\xbf\x87\xe8\xbf\x99\xe7\xa7\x8d\xe6\x96\xb9\xe5\xbc\x8f\xe6\x88\x91\xe4\xbb\xac\xe4\xb9\x9f\xe9\x87\x8d\xe6\x96\xb0\xe8\xaf\xbb\xe5\x85\xa5\xe4\xba\x86\xe7\x9b\xb8\xe5\x90\x8c\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\xef\xbc\x8c\xe6\x89\x93\xe5\x8d\xb0\xe7\xac\xac\xe4\xb8\x80\xe5\xb1\x82\xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0\xe5\xaf\xb9\xe6\xaf\x94\xef\xbc\x8c\xe5\x8f\x91\xe7\x8e\xb0\xe5\x92\x8c\xe5\x89\x8d\xe9\x9d\xa2\xe7\x9a\x84\xe5\x8a\x9e\xe6\xb3\x95\xe6\x98\xaf\xe4\xb8\x80\xe6\xa0\xb7\n#%% [markdown]\n# \xe6\x9c\x89\xe8\xbf\x99\xe4\xb8\xa4\xe7\xa7\x8d\xe4\xbf\x9d\xe5\xad\x98\xe5\x92\x8c\xe8\xaf\xbb\xe5\x8f\x96\xe6\xa8\xa1\xe5\x9e\x8b\xe7\x9a\x84\xe6\x96\xb9\xe6\xb3\x95\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe6\x8e\xa8\xe8\x8d\x90\xe4\xbd\xbf\xe7\x94\xa8**\xe7\xac\xac\xe4\xba\x8c\xe7\xa7\x8d**\xef\xbc\x8c\xe5\x9b\xa0\xe4\xb8\xba\xe7\xac\xac\xe4\xba\x8c\xe7\xa7\x8d\xe5\x8f\xaf\xe7\xa7\xbb\xe6\xa4\x8d\xe6\x80\xa7\xe6\x9b\xb4\xe5\xbc\xba\n#%% [markdown]\n# \xe4\xb8\x8b\xe9\x9d\xa2\xe6\x88\x91\xe4\xbb\xac\xe5\x86\x8d\xe7\x94\xa8 Module \xe5\xae\x9a\xe4\xb9\x89\xe8\xbf\x99\xe4\xb8\xaa\xe6\xa8\xa1\xe5\x9e\x8b\xef\xbc\x8c\xe4\xb8\x8b\xe9\x9d\xa2\xe6\x98\xaf\xe4\xbd\xbf\xe7\x94\xa8 Module \xe7\x9a\x84\xe6\xa8\xa1\xe6\x9d\xbf\n# \n# ```\n# class \xe7\xbd\x91\xe7\xbb\x9c\xe5\x90\x8d\xe5\xad\x97(nn.Module):\n#     def __init__(self, \xe4\xb8\x80\xe4\xba\x9b\xe5\xae\x9a\xe4\xb9\x89\xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0):\n#         super(\xe7\xbd\x91\xe7\xbb\x9c\xe5\x90\x8d\xe5\xad\x97, self).__init__()\n#         self.layer1 = nn.Linear(num_input, num_hidden)\n#         self.layer2 = nn.Sequential(...)\n#         ...\n#         \n#         \xe5\xae\x9a\xe4\xb9\x89\xe9\x9c\x80\xe8\xa6\x81\xe7\x94\xa8\xe7\x9a\x84\xe7\xbd\x91\xe7\xbb\x9c\xe5\xb1\x82\n#         \n#     def forward(self, x): # \xe5\xae\x9a\xe4\xb9\x89\xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\n#         x1 = self.layer1(x)\n#         x2 = self.layer2(x)\n#         x = x1 + x2\n#         ...\n#         return x\n# ```\n# \n# \xe6\xb3\xa8\xe6\x84\x8f\xe7\x9a\x84\xe6\x98\xaf\xef\xbc\x8cModule \xe9\x87\x8c\xe9\x9d\xa2\xe4\xb9\x9f\xe5\x8f\xaf\xe4\xbb\xa5\xe4\xbd\xbf\xe7\x94\xa8 Sequential\xef\xbc\x8c\xe5\x90\x8c\xe6\x97\xb6 Module \xe9\x9d\x9e\xe5\xb8\xb8\xe7\x81\xb5\xe6\xb4\xbb\xef\xbc\x8c\xe5\x85\xb7\xe4\xbd\x93\xe4\xbd\x93\xe7\x8e\xb0\xe5\x9c\xa8 forward \xe4\xb8\xad\xef\xbc\x8c\xe5\xa6\x82\xe4\xbd\x95\xe5\xa4\x8d\xe6\x9d\x82\xe7\x9a\x84\xe6\x93\x8d\xe4\xbd\x9c\xe9\x83\xbd\xe8\x83\xbd\xe7\x9b\xb4\xe8\xa7\x82\xe7\x9a\x84\xe5\x9c\xa8 forward \xe9\x87\x8c\xe9\x9d\xa2\xe6\x89\xa7\xe8\xa1\x8c\n# \n# \xe4\xb8\x8b\xe9\x9d\xa2\xe6\x88\x91\xe4\xbb\xac\xe7\x85\xa7\xe7\x9d\x80\xe6\xa8\xa1\xe6\x9d\xbf\xe5\xae\x9e\xe7\x8e\xb0\xe4\xb8\x80\xe4\xb8\x8b\xe4\xb8\x8a\xe9\x9d\xa2\xe7\x9a\x84\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\n\n#%%\nclass module_net(nn.Module):\n    def __init__(self, num_input, num_hidden, num_output):\n        super(module_net, self).__init__()\n        self.layer1 = nn.Linear(num_input, num_hidden)\n        \n        self.layer2 = nn.Tanh()\n        \n        self.layer3 = nn.Linear(num_hidden, num_output)\n        \n    def forward(self, x):\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        return x\n\n\n#%%\nmo_net = module_net(2, 4, 1)\n\n\n#%%\n# \xe8\xae\xbf\xe9\x97\xae\xe6\xa8\xa1\xe5\x9e\x8b\xe4\xb8\xad\xe7\x9a\x84\xe6\x9f\x90\xe5\xb1\x82\xe5\x8f\xaf\xe4\xbb\xa5\xe7\x9b\xb4\xe6\x8e\xa5\xe9\x80\x9a\xe8\xbf\x87\xe5\x90\x8d\xe5\xad\x97\n\n# \xe7\xac\xac\xe4\xb8\x80\xe5\xb1\x82\nl1 = mo_net.layer1\nprint(l1)\n\n\n#%%\n# \xe6\x89\x93\xe5\x8d\xb0\xe5\x87\xba\xe7\xac\xac\xe4\xb8\x80\xe5\xb1\x82\xe7\x9a\x84\xe6\x9d\x83\xe9\x87\x8d\nprint(l1.weight)\n\n\n#%%\n# \xe5\xae\x9a\xe4\xb9\x89\xe4\xbc\x98\xe5\x8c\x96\xe5\x99\xa8\noptim = torch.optim.SGD(mo_net.parameters(), 1.)\n\n\n#%%\n# \xe6\x88\x91\xe4\xbb\xac\xe8\xae\xad\xe7\xbb\x83 10000 \xe6\xac\xa1\nfor e in range(10000):\n    out = mo_net(x)\n    loss = criterion(out, y)\n    optim.zero_grad()\n    loss.backward()\n    optim.step()\n    if (e + 1) % 1000 == 0:\n        print('epoch: {}, loss: {}'.format(e+1, loss.item()))\n\n\n#%%\n# \xe4\xbf\x9d\xe5\xad\x98\xe6\xa8\xa1\xe5\x9e\x8b\ntorch.save(mo_net.state_dict(), 'module_net.pth')\n\n#%% [markdown]\n# \xe5\x8f\xaf\xe4\xbb\xa5\xe7\x9c\x8b\xe5\x88\xb0\xe6\x88\x91\xe4\xbb\xac\xe5\xbe\x97\xe5\x88\xb0\xe4\xba\x86\xe7\x9b\xb8\xe5\x90\x8c\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x9c\xef\xbc\x8c\xe8\x80\x8c\xe4\xb8\x94\xe4\xbd\xbf\xe7\x94\xa8 Sequential \xe5\x92\x8c Module \xe6\x9d\xa5\xe5\xae\x9a\xe4\xb9\x89\xe6\xa8\xa1\xe5\x9e\x8b\xe6\x9b\xb4\xe5\x8a\xa0\xe6\x96\xb9\xe4\xbe\xbf\n# \n# \xe5\x9c\xa8\xe8\xbf\x99\xe4\xb8\x80\xe8\x8a\x82\xe4\xb8\xad\xe6\x88\x91\xe4\xbb\xac\xe8\xbf\x98\xe6\x98\xaf\xe4\xbd\xbf\xe7\x94\xa8\xe6\xa2\xaf\xe5\xba\xa6\xe4\xb8\x8b\xe9\x99\x8d\xe6\xb3\x95\xe6\x9d\xa5\xe4\xbc\x98\xe5\x8c\x96\xe5\x8f\x82\xe6\x95\xb0\xef\xbc\x8c\xe5\x9c\xa8\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xe4\xb8\xad\xef\xbc\x8c\xe8\xbf\x99\xe7\xa7\x8d\xe4\xbc\x98\xe5\x8c\x96\xe6\x96\xb9\xe6\xb3\x95\xe6\x9c\x89\xe4\xb8\x80\xe4\xb8\xaa\xe7\x89\xb9\xe5\x88\xab\xe7\x9a\x84\xe5\x90\x8d\xe5\xad\x97\xef\xbc\x8c\xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe7\xae\x97\xe6\xb3\x95\xef\xbc\x8c\xe4\xb8\x8b\xe4\xb8\x80\xe6\xac\xa1\xe8\xaf\xbe\xe6\x88\x91\xe4\xbb\xac\xe4\xbc\x9a\xe8\xae\xb2\xe4\xb8\x80\xe8\xae\xb2\xe4\xbb\x80\xe4\xb9\x88\xe6\x98\xaf\xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe7\xae\x97\xe6\xb3\x95\n#%% [markdown]\n# **\xe5\xb0\x8f\xe7\xbb\x83\xe4\xb9\xa0\xef\xbc\x9a\xe6\x94\xb9\xe5\x8f\x98\xe7\xbd\x91\xe7\xbb\x9c\xe7\x9a\x84\xe9\x9a\x90\xe8\x97\x8f\xe5\xb1\x82\xe7\xa5\x9e\xe7\xbb\x8f\xe5\x85\x83\xe6\x95\xb0\xe7\x9b\xae\xef\xbc\x8c\xe6\x88\x96\xe8\x80\x85\xe8\xaf\x95\xe8\xaf\x95\xe5\xae\x9a\xe4\xb9\x89\xe4\xb8\x80\xe4\xb8\xaa 5 \xe5\xb1\x82\xe7\x94\x9a\xe8\x87\xb3\xe6\x9b\xb4\xe6\xb7\xb1\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\xef\xbc\x8c\xe5\xa2\x9e\xe5\x8a\xa0\xe8\xae\xad\xe7\xbb\x83\xe6\xac\xa1\xe6\x95\xb0\xef\xbc\x8c\xe6\x94\xb9\xe5\x8f\x98\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\xef\xbc\x8c\xe7\x9c\x8b\xe7\x9c\x8b\xe7\xbb\x93\xe6\x9e\x9c\xe4\xbc\x9a\xe6\x80\x8e\xe4\xb9\x88\xe6\xa0\xb7**\n#%% [markdown]\n# \xe4\xb8\x8b\xe9\x9d\xa2\xe4\xb8\xbe\xe4\xb8\xaa\xe4\xbe\x8b\xe5\xad\x90\n\n#%%\nnet = nn.Sequential(\n    nn.Linear(2, 10),\n    nn.Tanh(),\n    nn.Linear(10, 10),\n    nn.Tanh(),\n    nn.Linear(10, 10),\n    nn.Tanh(),\n    nn.Linear(10, 1)\n)\n\noptim = torch.optim.SGD(net.parameters(), 0.1)\n\n\n#%%\n# \xe6\x88\x91\xe4\xbb\xac\xe8\xae\xad\xe7\xbb\x83 20000 \xe6\xac\xa1\nfor e in range(20000):\n    out = net(x)\n    loss = criterion(out, y)\n    optim.zero_grad()\n    loss.backward()\n    optim.step()\n    if (e + 1) % 1000 == 0:\n        print('epoch: {}, loss: {}'.format(e+1, loss.item()))\n\n\n#%%\ndef plot_net(x):\n    out = torch.sigmoid(net(torch.from_numpy(x).float())).detach().numpy()\n    out = (out > 0.5) * 1\n    return out\n\nplot_decision_boundary(lambda x: plot_net(x), x.numpy(), y.numpy())\nplt.title('sequential')\n\n\n"""
6.Neural_Network 神经网络/backprop 反向传播算法.py,0,"b'#%% [markdown]\n# # \xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe7\xae\x97\xe6\xb3\x95\n# \n# \xe5\x89\x8d\xe9\x9d\xa2\xe6\x88\x91\xe4\xbb\xac\xe4\xbb\x8b\xe7\xbb\x8d\xe4\xba\x86\xe4\xb8\x89\xe4\xb8\xaa\xe6\xa8\xa1\xe5\x9e\x8b\xef\xbc\x8c\xe6\x95\xb4\xe4\xb8\xaa\xe5\xa4\x84\xe7\x90\x86\xe7\x9a\x84\xe5\x9f\xba\xe6\x9c\xac\xe6\xb5\x81\xe7\xa8\x8b\xe9\x83\xbd\xe6\x98\xaf\xe5\xae\x9a\xe4\xb9\x89\xe6\xa8\xa1\xe5\x9e\x8b\xef\xbc\x8c\xe8\xaf\xbb\xe5\x85\xa5\xe6\x95\xb0\xe6\x8d\xae\xef\xbc\x8c\xe7\xbb\x99\xe5\x87\xba\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x87\xbd\xe6\x95\xb0$f$\xef\xbc\x8c\xe9\x80\x9a\xe8\xbf\x87\xe6\xa2\xaf\xe5\xba\xa6\xe4\xb8\x8b\xe9\x99\x8d\xe6\xb3\x95\xe6\x9b\xb4\xe6\x96\xb0\xe5\x8f\x82\xe6\x95\xb0\xe3\x80\x82PyTorch \xe6\x8f\x90\xe4\xbe\x9b\xe4\xba\x86\xe9\x9d\x9e\xe5\xb8\xb8\xe7\xae\x80\xe5\x8d\x95\xe7\x9a\x84\xe8\x87\xaa\xe5\x8a\xa8\xe6\xb1\x82\xe5\xaf\xbc\xe5\xb8\xae\xe5\x8a\xa9\xe6\x88\x91\xe4\xbb\xac\xe6\xb1\x82\xe8\xa7\xa3\xe5\xaf\xbc\xe6\x95\xb0\xef\xbc\x8c\xe5\xaf\xb9\xe4\xba\x8e\xe6\xaf\x94\xe8\xbe\x83\xe7\xae\x80\xe5\x8d\x95\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe4\xb9\x9f\xe8\x83\xbd\xe6\x89\x8b\xe5\x8a\xa8\xe6\xb1\x82\xe5\x87\xba\xe5\x8f\x82\xe6\x95\xb0\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\xef\xbc\x8c\xe4\xbd\x86\xe6\x98\xaf\xe5\xaf\xb9\xe4\xba\x8e\xe9\x9d\x9e\xe5\xb8\xb8\xe5\xa4\x8d\xe6\x9d\x82\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\xef\xbc\x8c\xe6\xaf\x94\xe5\xa6\x82\xe4\xb8\x80\xe4\xb8\xaa 100 \xe5\xb1\x82\xe7\x9a\x84\xe7\xbd\x91\xe7\xbb\x9c\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe5\xa6\x82\xe4\xbd\x95\xe8\x83\xbd\xe5\xa4\x9f\xe6\x9c\x89\xe6\x95\x88\xe5\x9c\xb0\xe6\x89\x8b\xe5\x8a\xa8\xe6\xb1\x82\xe5\x87\xba\xe8\xbf\x99\xe4\xb8\xaa\xe6\xa2\xaf\xe5\xba\xa6\xe5\x91\xa2\xef\xbc\x9f\xe8\xbf\x99\xe9\x87\x8c\xe5\xb0\xb1\xe9\x9c\x80\xe8\xa6\x81\xe5\xbc\x95\xe5\x85\xa5\xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe7\xae\x97\xe6\xb3\x95\xef\xbc\x8c\xe8\x87\xaa\xe5\x8a\xa8\xe6\xb1\x82\xe5\xaf\xbc\xe6\x9c\xac\xe8\xb4\xa8\xe6\x98\xaf\xe5\xb0\xb1\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa\xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe7\xae\x97\xe6\xb3\x95\xe3\x80\x82\n# \n# \xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe7\xae\x97\xe6\xb3\x95\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa\xe6\x9c\x89\xe6\x95\x88\xe5\x9c\xb0\xe6\xb1\x82\xe8\xa7\xa3\xe6\xa2\xaf\xe5\xba\xa6\xe7\x9a\x84\xe7\xae\x97\xe6\xb3\x95\xef\xbc\x8c\xe6\x9c\xac\xe8\xb4\xa8\xe4\xb8\x8a\xe5\x85\xb6\xe5\xae\x9e\xe5\xb0\xb1\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa\xe9\x93\xbe\xe5\xbc\x8f\xe6\xb1\x82\xe5\xaf\xbc\xe6\xb3\x95\xe5\x88\x99\xe7\x9a\x84\xe5\xba\x94\xe7\x94\xa8\xef\xbc\x8c\xe7\x84\xb6\xe8\x80\x8c\xe8\xbf\x99\xe4\xb8\xaa\xe5\xa6\x82\xe6\xad\xa4\xe7\xae\x80\xe5\x8d\x95\xe8\x80\x8c\xe4\xb8\x94\xe6\x98\xbe\xe8\x80\x8c\xe6\x98\x93\xe8\xa7\x81\xe7\x9a\x84\xe6\x96\xb9\xe6\xb3\x95\xe5\x8d\xb4\xe6\x98\xaf\xe5\x9c\xa8 Roseblatt \xe6\x8f\x90\xe5\x87\xba\xe6\x84\x9f\xe7\x9f\xa5\xe6\x9c\xba\xe7\xae\x97\xe6\xb3\x95\xe5\x90\x8e\xe5\xb0\x86\xe8\xbf\x91 30 \xe5\xb9\xb4\xe6\x89\x8d\xe8\xa2\xab\xe5\x8f\x91\xe6\x98\x8e\xe5\x92\x8c\xe6\x99\xae\xe5\x8f\x8a\xe7\x9a\x84\xef\xbc\x8c\xe5\xaf\xb9\xe6\xad\xa4 Bengio \xe8\xbf\x99\xe6\xa0\xb7\xe8\xaf\xb4\xe9\x81\x93\xef\xbc\x9a\xe2\x80\x9c\xe5\xbe\x88\xe5\xa4\x9a\xe7\x9c\x8b\xe4\xbc\xbc\xe6\x98\xbe\xe8\x80\x8c\xe6\x98\x93\xe8\xa7\x81\xe7\x9a\x84\xe6\x83\xb3\xe6\xb3\x95\xe5\x8f\xaa\xe6\x9c\x89\xe5\x9c\xa8\xe4\xba\x8b\xe5\x90\x8e\xe6\x89\x8d\xe5\x8f\x98\xe5\xbe\x97\xe7\x9a\x84\xe6\x98\xbe\xe8\x80\x8c\xe6\x98\x93\xe8\xa7\x81\xe3\x80\x82\xe2\x80\x9d\n# \n# \xe4\xb8\x8b\xe9\x9d\xa2\xe6\x88\x91\xe4\xbb\xac\xe5\xb0\xb1\xe6\x9d\xa5\xe8\xaf\xa6\xe7\xbb\x86\xe5\xb0\x86\xe4\xb8\x80\xe8\xae\xb2\xe4\xbb\x80\xe4\xb9\x88\xe6\x98\xaf\xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe7\xae\x97\xe6\xb3\x95\xe3\x80\x82\n#%% [markdown]\n# ## \xe9\x93\xbe\xe5\xbc\x8f\xe6\xb3\x95\xe5\x88\x99\n# \n# \xe9\xa6\x96\xe5\x85\x88\xe6\x9d\xa5\xe7\xae\x80\xe5\x8d\x95\xe5\x9c\xb0\xe4\xbb\x8b\xe7\xbb\x8d\xe4\xb8\x80\xe4\xb8\x8b\xe9\x93\xbe\xe5\xbc\x8f\xe6\xb3\x95\xe5\x88\x99\xef\xbc\x8c\xe8\x80\x83\xe8\x99\x91\xe4\xb8\x80\xe4\xb8\xaa\xe7\xae\x80\xe5\x8d\x95\xe7\x9a\x84\xe5\x87\xbd\xe6\x95\xb0\xef\xbc\x8c\xe6\xaf\x94\xe5\xa6\x82\n# $$f(x, y, z) = (x + y)z$$\n# \n# \xe6\x88\x91\xe4\xbb\xac\xe5\xbd\x93\xe7\x84\xb6\xe5\x8f\xaf\xe4\xbb\xa5\xe7\x9b\xb4\xe6\x8e\xa5\xe6\xb1\x82\xe5\x87\xba\xe8\xbf\x99\xe4\xb8\xaa\xe5\x87\xbd\xe6\x95\xb0\xe7\x9a\x84\xe5\xbe\xae\xe5\x88\x86\xef\xbc\x8c\xe4\xbd\x86\xe6\x98\xaf\xe8\xbf\x99\xe9\x87\x8c\xe6\x88\x91\xe4\xbb\xac\xe8\xa6\x81\xe4\xbd\xbf\xe7\x94\xa8\xe9\x93\xbe\xe5\xbc\x8f\xe6\xb3\x95\xe5\x88\x99\xef\xbc\x8c\xe4\xbb\xa4\n# $$q=x+y$$\n# \n# \xe9\x82\xa3\xe4\xb9\x88\n# \n# $$f = qz$$\n# \n# \xe5\xaf\xb9\xe4\xba\x8e\xe8\xbf\x99\xe4\xb8\xa4\xe4\xb8\xaa\xe5\xbc\x8f\xe5\xad\x90\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe5\x8f\xaf\xe4\xbb\xa5\xe5\x88\x86\xe5\x88\xab\xe6\xb1\x82\xe5\x87\xba\xe4\xbb\x96\xe4\xbb\xac\xe7\x9a\x84\xe5\xbe\xae\xe5\x88\x86 \n# \n# $$\\frac{\\partial f}{\\partial q} = z, \\frac{\\partial f}{\\partial z}=q$$\n# \n# \xe5\x90\x8c\xe6\x97\xb6$q$\xe6\x98\xaf$x$\xe5\x92\x8c$y$\xe7\x9a\x84\xe6\xb1\x82\xe5\x92\x8c\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe6\x88\x91\xe4\xbb\xac\xe8\x83\xbd\xe5\xa4\x9f\xe5\xbe\x97\xe5\x88\xb0\n# \n# $$\\frac{\\partial q}{x} = 1, \\frac{\\partial q}{y} = 1$$\n# \n# \xe6\x88\x91\xe4\xbb\xac\xe5\x85\xb3\xe5\xbf\x83\xe7\x9a\x84\xe9\x97\xae\xe9\xa2\x98\xe6\x98\xaf\n# \n# $$\\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y}, \\frac{\\partial f}{\\partial z}$$\n# \n# \xe9\x93\xbe\xe5\xbc\x8f\xe6\xb3\x95\xe5\x88\x99\xe5\x91\x8a\xe8\xaf\x89\xe6\x88\x91\xe4\xbb\xac\xe5\xa6\x82\xe4\xbd\x95\xe6\x9d\xa5\xe8\xae\xa1\xe7\xae\x97\xe5\x87\xba\xe4\xbb\x96\xe4\xbb\xac\xe7\x9a\x84\xe5\x80\xbc\n# \n# $$\n# \\frac{\\partial f}{\\partial x} = \\frac{\\partial f}{\\partial q}\\frac{\\partial q}{\\partial x}\n# $$\n# $$\n# \\frac{\\partial f}{\\partial y} = \\frac{\\partial f}{\\partial q}\\frac{\\partial q}{\\partial y}\n# $$\n# $$\n# \\frac{\\partial f}{\\partial z} = q\n# $$\n# \n# \xe9\x80\x9a\xe8\xbf\x87\xe9\x93\xbe\xe5\xbc\x8f\xe6\xb3\x95\xe5\x88\x99\xe6\x88\x91\xe4\xbb\xac\xe7\x9f\xa5\xe9\x81\x93\xe5\xa6\x82\xe6\x9e\x9c\xe6\x88\x91\xe4\xbb\xac\xe9\x9c\x80\xe8\xa6\x81\xe5\xaf\xb9\xe5\x85\xb6\xe4\xb8\xad\xe7\x9a\x84\xe5\x85\x83\xe7\xb4\xa0\xe6\xb1\x82\xe5\xaf\xbc\xef\xbc\x8c\xe9\x82\xa3\xe4\xb9\x88\xe6\x88\x91\xe4\xbb\xac\xe5\x8f\xaf\xe4\xbb\xa5\xe4\xb8\x80\xe5\xb1\x82\xe4\xb8\x80\xe5\xb1\x82\xe6\xb1\x82\xe5\xaf\xbc\xe7\x84\xb6\xe5\x90\x8e\xe5\xb0\x86\xe7\xbb\x93\xe6\x9e\x9c\xe4\xb9\x98\xe8\xb5\xb7\xe6\x9d\xa5\xef\xbc\x8c\xe8\xbf\x99\xe5\xb0\xb1\xe6\x98\xaf\xe9\x93\xbe\xe5\xbc\x8f\xe6\xb3\x95\xe5\x88\x99\xe7\x9a\x84\xe6\xa0\xb8\xe5\xbf\x83\xef\xbc\x8c\xe4\xb9\x9f\xe6\x98\xaf\xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe7\xae\x97\xe6\xb3\x95\xe7\x9a\x84\xe6\xa0\xb8\xe5\xbf\x83\xef\xbc\x8c\xe6\x9b\xb4\xe5\xa4\x9a\xe5\x85\xb3\xe4\xba\x8e\xe9\x93\xbe\xe5\xbc\x8f\xe6\xb3\x95\xe5\x88\x99\xe7\x9a\x84\xe7\xae\x97\xe6\xb3\x95\xef\xbc\x8c\xe5\x8f\xaf\xe4\xbb\xa5\xe8\xae\xbf\xe9\x97\xae\xe8\xbf\x99\xe4\xb8\xaa[\xe6\x96\x87\xe6\xa1\xa3](https://zh.wikipedia.org/wiki/%E9%93%BE%E5%BC%8F%E6%B3%95%E5%88%99)\n#%% [markdown]\n# ## \xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe7\xae\x97\xe6\xb3\x95\n# \n# \xe4\xba\x86\xe8\xa7\xa3\xe4\xba\x86\xe9\x93\xbe\xe5\xbc\x8f\xe6\xb3\x95\xe5\x88\x99\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe5\xb0\xb1\xe5\x8f\xaf\xe4\xbb\xa5\xe5\xbc\x80\xe5\xa7\x8b\xe4\xbb\x8b\xe7\xbb\x8d\xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe7\xae\x97\xe6\xb3\x95\xe4\xba\x86\xef\xbc\x8c\xe6\x9c\xac\xe8\xb4\xa8\xe4\xb8\x8a\xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe7\xae\x97\xe6\xb3\x95\xe5\x8f\xaa\xe6\x98\xaf\xe9\x93\xbe\xe5\xbc\x8f\xe6\xb3\x95\xe5\x88\x99\xe7\x9a\x84\xe4\xb8\x80\xe4\xb8\xaa\xe5\xba\x94\xe7\x94\xa8\xe3\x80\x82\xe6\x88\x91\xe4\xbb\xac\xe8\xbf\x98\xe6\x98\xaf\xe4\xbd\xbf\xe7\x94\xa8\xe4\xb9\x8b\xe5\x89\x8d\xe9\x82\xa3\xe4\xb8\xaa\xe7\x9b\xb8\xe5\x90\x8c\xe7\x9a\x84\xe4\xbe\x8b\xe5\xad\x90$q=x+y, f=qz$\xef\xbc\x8c\xe9\x80\x9a\xe8\xbf\x87\xe8\xae\xa1\xe7\xae\x97\xe5\x9b\xbe\xe5\x8f\xaf\xe4\xbb\xa5\xe5\xb0\x86\xe8\xbf\x99\xe4\xb8\xaa\xe8\xae\xa1\xe7\xae\x97\xe8\xbf\x87\xe7\xa8\x8b\xe8\xa1\xa8\xe8\xbe\xbe\xe5\x87\xba\xe6\x9d\xa5\n# \n# ![](https://ws1.sinaimg.cn/large/006tNc79ly1fmiozcinyzj30c806vglk.jpg)\n# \n# \xe4\xb8\x8a\xe9\x9d\xa2\xe7\xbb\xbf\xe8\x89\xb2\xe7\x9a\x84\xe6\x95\xb0\xe5\xad\x97\xe8\xa1\xa8\xe7\xa4\xba\xe5\x85\xb6\xe6\x95\xb0\xe5\x80\xbc\xef\xbc\x8c\xe4\xb8\x8b\xe9\x9d\xa2\xe7\xba\xa2\xe8\x89\xb2\xe7\x9a\x84\xe6\x95\xb0\xe5\xad\x97\xe8\xa1\xa8\xe7\xa4\xba\xe6\xb1\x82\xe5\x87\xba\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe5\x8f\xaf\xe4\xbb\xa5\xe4\xb8\x80\xe6\xad\xa5\xe4\xb8\x80\xe6\xad\xa5\xe7\x9c\x8b\xe7\x9c\x8b\xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe7\xae\x97\xe6\xb3\x95\xe7\x9a\x84\xe5\xae\x9e\xe7\x8e\xb0\xe3\x80\x82\xe9\xa6\x96\xe5\x85\x88\xe4\xbb\x8e\xe6\x9c\x80\xe5\x90\x8e\xe5\xbc\x80\xe5\xa7\x8b\xef\xbc\x8c\xe6\xa2\xaf\xe5\xba\xa6\xe5\xbd\x93\xe7\x84\xb6\xe6\x98\xaf1\xef\xbc\x8c\xe7\x84\xb6\xe5\x90\x8e\xe8\xae\xa1\xe7\xae\x97\n# \n# $$\\frac{\\partial f}{\\partial q} = z = -4,\\ \\frac{\\partial f}{\\partial z} = q = 3$$\n# \n# \xe6\x8e\xa5\xe7\x9d\x80\xe6\x88\x91\xe4\xbb\xac\xe8\xae\xa1\xe7\xae\x97\n# $$\\frac{\\partial f}{\\partial x} = \\frac{\\partial f}{\\partial q} \\frac{\\partial q}{\\partial x} = -4 \\times 1 = -4,\\ \\frac{\\partial f}{\\partial y} = \\frac{\\partial f}{\\partial q} \\frac{\\partial q}{\\partial y} = -4 \\times 1 = -4$$\n# \n# \xe8\xbf\x99\xe6\xa0\xb7\xe4\xb8\x80\xe6\xad\xa5\xe4\xb8\x80\xe6\xad\xa5\xe6\x88\x91\xe4\xbb\xac\xe5\xb0\xb1\xe6\xb1\x82\xe5\x87\xba\xe4\xba\x86$\\nabla f(x, y, z)$\xe3\x80\x82\n# \n# \xe7\x9b\xb4\xe8\xa7\x82\xe4\xb8\x8a\xe7\x9c\x8b\xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe7\xae\x97\xe6\xb3\x95\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa\xe4\xbc\x98\xe9\x9b\x85\xe7\x9a\x84\xe5\xb1\x80\xe9\x83\xa8\xe8\xbf\x87\xe7\xa8\x8b\xef\xbc\x8c\xe6\xaf\x8f\xe6\xac\xa1\xe6\xb1\x82\xe5\xaf\xbc\xe5\x8f\xaa\xe6\x98\xaf\xe5\xaf\xb9\xe5\xbd\x93\xe5\x89\x8d\xe7\x9a\x84\xe8\xbf\x90\xe7\xae\x97\xe6\xb1\x82\xe5\xaf\xbc\xef\xbc\x8c\xe6\xb1\x82\xe8\xa7\xa3\xe6\xaf\x8f\xe5\xb1\x82\xe7\xbd\x91\xe7\xbb\x9c\xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0\xe9\x83\xbd\xe6\x98\xaf\xe9\x80\x9a\xe8\xbf\x87\xe9\x93\xbe\xe5\xbc\x8f\xe6\xb3\x95\xe5\x88\x99\xe5\xb0\x86\xe5\x89\x8d\xe9\x9d\xa2\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x9c\xe6\xb1\x82\xe5\x87\xba\xe4\xb8\x8d\xe6\x96\xad\xe8\xbf\xad\xe4\xbb\xa3\xe5\x88\xb0\xe8\xbf\x99\xe4\xb8\x80\xe5\xb1\x82\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe8\xaf\xb4\xe8\xbf\x99\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa\xe4\xbc\xa0\xe6\x92\xad\xe8\xbf\x87\xe7\xa8\x8b\n# \n# ### Sigmoid\xe5\x87\xbd\xe6\x95\xb0\xe4\xb8\xbe\xe4\xbe\x8b\n# \n# \xe4\xb8\x8b\xe9\x9d\xa2\xe6\x88\x91\xe4\xbb\xac\xe9\x80\x9a\xe8\xbf\x87Sigmoid\xe5\x87\xbd\xe6\x95\xb0\xe6\x9d\xa5\xe6\xbc\x94\xe7\xa4\xba\xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe8\xbf\x87\xe7\xa8\x8b\xe5\x9c\xa8\xe4\xb8\x80\xe4\xb8\xaa\xe5\xa4\x8d\xe6\x9d\x82\xe7\x9a\x84\xe5\x87\xbd\xe6\x95\xb0\xe4\xb8\x8a\xe6\x98\xaf\xe5\xa6\x82\xe4\xbd\x95\xe8\xbf\x9b\xe8\xa1\x8c\xe7\x9a\x84\xe3\x80\x82\n# \n# $$\n# f(w, x) = \\frac{1}{1+e^{-(w_0 x_0 + w_1 x_1 + w_2)}}\n# $$\n# \n# \xe6\x88\x91\xe4\xbb\xac\xe9\x9c\x80\xe8\xa6\x81\xe6\xb1\x82\xe8\xa7\xa3\xe5\x87\xba\n# $$\\frac{\\partial f}{\\partial w_0}, \\frac{\\partial f}{\\partial w_1}, \\frac{\\partial f}{\\partial w_2}$$\n# \n# \xe9\xa6\x96\xe5\x85\x88\xe6\x88\x91\xe4\xbb\xac\xe5\xb0\x86\xe8\xbf\x99\xe4\xb8\xaa\xe5\x87\xbd\xe6\x95\xb0\xe6\x8a\xbd\xe8\xb1\xa1\xe6\x88\x90\xe4\xb8\x80\xe4\xb8\xaa\xe8\xae\xa1\xe7\xae\x97\xe5\x9b\xbe\xe6\x9d\xa5\xe8\xa1\xa8\xe7\xa4\xba\xef\xbc\x8c\xe5\x8d\xb3\n# $$\n#    f(x) = \\frac{1}{x} \\\\\n#    f_c(x) = 1 + x \\\\\n#    f_e(x) = e^x \\\\\n#    f_w(x) = -(w_0 x_0 + w_1 x_1 + w_2)\n# $$\n# \n# \xe8\xbf\x99\xe6\xa0\xb7\xe6\x88\x91\xe4\xbb\xac\xe5\xb0\xb1\xe8\x83\xbd\xe5\xa4\x9f\xe7\x94\xbb\xe5\x87\xba\xe4\xb8\x8b\xe9\x9d\xa2\xe7\x9a\x84\xe8\xae\xa1\xe7\xae\x97\xe5\x9b\xbe\n# \n# ![](https://ws1.sinaimg.cn/large/006tNc79ly1fmip1va5qjj30lb08e0t0.jpg)\n# \n# \xe5\x90\x8c\xe6\xa0\xb7\xe4\xb8\x8a\xe9\x9d\xa2\xe7\xbb\xbf\xe8\x89\xb2\xe7\x9a\x84\xe6\x95\xb0\xe5\xad\x90\xe8\xa1\xa8\xe7\xa4\xba\xe6\x95\xb0\xe5\x80\xbc\xef\xbc\x8c\xe4\xb8\x8b\xe9\x9d\xa2\xe7\xba\xa2\xe8\x89\xb2\xe7\x9a\x84\xe6\x95\xb0\xe5\xad\x97\xe8\xa1\xa8\xe7\xa4\xba\xe6\xa2\xaf\xe5\xba\xa6\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe4\xbb\x8e\xe5\x90\x8e\xe5\xbe\x80\xe5\x89\x8d\xe8\xae\xa1\xe7\xae\x97\xe4\xb8\x80\xe4\xb8\x8b\xe5\x90\x84\xe4\xb8\xaa\xe5\x8f\x82\xe6\x95\xb0\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\xe3\x80\x82\xe9\xa6\x96\xe5\x85\x88\xe6\x9c\x80\xe5\x90\x8e\xe9\x9d\xa2\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\xe6\x98\xaf1,\xef\xbc\x8c\xe7\x84\xb6\xe5\x90\x8e\xe7\xbb\x8f\xe8\xbf\x87$\\frac{1}{x}$\xe8\xbf\x99\xe4\xb8\xaa\xe5\x87\xbd\xe6\x95\xb0\xef\xbc\x8c\xe8\xbf\x99\xe4\xb8\xaa\xe5\x87\xbd\xe6\x95\xb0\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\xe6\x98\xaf$-\\frac{1}{x^2}$\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe5\xbe\x80\xe5\x89\x8d\xe4\xbc\xa0\xe6\x92\xad\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\xe6\x98\xaf$1 \\times -\\frac{1}{1.37^2} = -0.53$\xef\xbc\x8c\xe7\x84\xb6\xe5\x90\x8e\xe6\x98\xaf$+1$\xe8\xbf\x99\xe4\xb8\xaa\xe6\x93\x8d\xe4\xbd\x9c\xef\xbc\x8c\xe6\xa2\xaf\xe5\xba\xa6\xe4\xb8\x8d\xe5\x8f\x98\xef\xbc\x8c\xe6\x8e\xa5\xe7\x9d\x80\xe6\x98\xaf$e^x$\xe8\xbf\x99\xe4\xb8\xaa\xe8\xbf\x90\xe7\xae\x97\xef\xbc\x8c\xe5\xae\x83\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\xe5\xb0\xb1\xe6\x98\xaf$-0.53 \\times e^{-1} = -0.2$\xef\xbc\x8c\xe8\xbf\x99\xe6\xa0\xb7\xe4\xb8\x8d\xe6\x96\xad\xe5\xbe\x80\xe5\x90\x8e\xe4\xbc\xa0\xe6\x92\xad\xe5\xb0\xb1\xe8\x83\xbd\xe5\xa4\x9f\xe6\xb1\x82\xe5\xbe\x97\xe6\xaf\x8f\xe4\xb8\xaa\xe5\x8f\x82\xe6\x95\xb0\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\xe3\x80\x82\n\n'"
6.Neural_Network 神经网络/deep-neural-network深层神经网络.py,3,"b""#%% [markdown]\n# # \xe6\xb7\xb1\xe5\xb1\x82\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\n# \xe5\x89\x8d\xe9\x9d\xa2\xe6\x88\x91\xe4\xbb\xac\xe7\xae\x80\xe8\xa6\x81\xe4\xbb\x8b\xe7\xbb\x8d\xe4\xba\x86\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xe7\x9a\x84\xe4\xb8\x80\xe4\xba\x9b\xe5\x9f\xba\xe6\x9c\xac\xe7\x9f\xa5\xe8\xaf\x86\xef\xbc\x8c\xe5\x90\x8c\xe6\x97\xb6\xe4\xb9\x9f\xe6\x98\xaf\xe7\xa4\xba\xe8\x8c\x83\xe4\xba\x86\xe5\xa6\x82\xe4\xbd\x95\xe7\x94\xa8\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xe6\x9e\x84\xe5\xbb\xba\xe4\xb8\x80\xe4\xb8\xaa\xe5\xa4\x8d\xe6\x9d\x82\xe7\x9a\x84\xe9\x9d\x9e\xe7\xba\xbf\xe6\x80\xa7\xe4\xba\x8c\xe5\x88\x86\xe7\xb1\xbb\xe5\x99\xa8\xef\xbc\x8c\xe6\x9b\xb4\xe5\xa4\x9a\xe7\x9a\x84\xe6\x83\x85\xe5\x86\xb5\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xe9\x80\x82\xe5\x90\x88\xe4\xbd\xbf\xe7\x94\xa8\xe5\x9c\xa8\xe6\x9b\xb4\xe5\x8a\xa0\xe5\xa4\x8d\xe6\x9d\x82\xe7\x9a\x84\xe6\x83\x85\xe5\x86\xb5\xef\xbc\x8c\xe6\xaf\x94\xe5\xa6\x82\xe5\x9b\xbe\xe5\x83\x8f\xe5\x88\x86\xe7\xb1\xbb\xe7\x9a\x84\xe9\x97\xae\xe9\xa2\x98\xef\xbc\x8c\xe4\xb8\x8b\xe9\x9d\xa2\xe6\x88\x91\xe4\xbb\xac\xe7\x94\xa8\xe6\xb7\xb1\xe5\xba\xa6\xe5\xad\xa6\xe4\xb9\xa0\xe7\x9a\x84\xe5\x85\xa5\xe9\x97\xa8\xe7\xba\xa7\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86 MNIST \xe6\x89\x8b\xe5\x86\x99\xe4\xbd\x93\xe5\x88\x86\xe7\xb1\xbb\xe6\x9d\xa5\xe8\xaf\xb4\xe6\x98\x8e\xe4\xb8\x80\xe4\xb8\x8b\xe6\x9b\xb4\xe6\xb7\xb1\xe5\xb1\x82\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xe7\x9a\x84\xe4\xbc\x98\xe8\x89\xaf\xe8\xa1\xa8\xe7\x8e\xb0\xe3\x80\x82\n# \n# ## MNIST \xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\n# mnist \xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa\xe9\x9d\x9e\xe5\xb8\xb8\xe5\x87\xba\xe5\x90\x8d\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xef\xbc\x8c\xe5\x9f\xba\xe6\x9c\xac\xe4\xb8\x8a\xe5\xbe\x88\xe5\xa4\x9a\xe7\xbd\x91\xe7\xbb\x9c\xe9\x83\xbd\xe5\xb0\x86\xe5\x85\xb6\xe4\xbd\x9c\xe4\xb8\xba\xe4\xb8\x80\xe4\xb8\xaa\xe6\xb5\x8b\xe8\xaf\x95\xe7\x9a\x84\xe6\xa0\x87\xe5\x87\x86\xef\xbc\x8c\xe5\x85\xb6\xe6\x9d\xa5\xe8\x87\xaa\xe7\xbe\x8e\xe5\x9b\xbd\xe5\x9b\xbd\xe5\xae\xb6\xe6\xa0\x87\xe5\x87\x86\xe4\xb8\x8e\xe6\x8a\x80\xe6\x9c\xaf\xe7\xa0\x94\xe7\xa9\xb6\xe6\x89\x80, National Institute of Standards and Technology (NIST)\xe3\x80\x82 \xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86 (training set) \xe7\x94\xb1\xe6\x9d\xa5\xe8\x87\xaa 250 \xe4\xb8\xaa\xe4\xb8\x8d\xe5\x90\x8c\xe4\xba\xba\xe6\x89\x8b\xe5\x86\x99\xe7\x9a\x84\xe6\x95\xb0\xe5\xad\x97\xe6\x9e\x84\xe6\x88\x90, \xe5\x85\xb6\xe4\xb8\xad 50% \xe6\x98\xaf\xe9\xab\x98\xe4\xb8\xad\xe5\xad\xa6\xe7\x94\x9f, 50% \xe6\x9d\xa5\xe8\x87\xaa\xe4\xba\xba\xe5\x8f\xa3\xe6\x99\xae\xe6\x9f\xa5\xe5\xb1\x80 (the Census Bureau) \xe7\x9a\x84\xe5\xb7\xa5\xe4\xbd\x9c\xe4\xba\xba\xe5\x91\x98\xef\xbc\x8c\xe4\xb8\x80\xe5\x85\xb1\xe6\x9c\x89 60000 \xe5\xbc\xa0\xe5\x9b\xbe\xe7\x89\x87\xe3\x80\x82 \xe6\xb5\x8b\xe8\xaf\x95\xe9\x9b\x86(test set) \xe4\xb9\x9f\xe6\x98\xaf\xe5\x90\x8c\xe6\xa0\xb7\xe6\xaf\x94\xe4\xbe\x8b\xe7\x9a\x84\xe6\x89\x8b\xe5\x86\x99\xe6\x95\xb0\xe5\xad\x97\xe6\x95\xb0\xe6\x8d\xae\xef\xbc\x8c\xe4\xb8\x80\xe5\x85\xb1\xe6\x9c\x89 10000 \xe5\xbc\xa0\xe5\x9b\xbe\xe7\x89\x87\xe3\x80\x82\n# \n# \xe6\xaf\x8f\xe5\xbc\xa0\xe5\x9b\xbe\xe7\x89\x87\xe5\xa4\xa7\xe5\xb0\x8f\xe6\x98\xaf 28 x 28 \xe7\x9a\x84\xe7\x81\xb0\xe5\xba\xa6\xe5\x9b\xbe\xef\xbc\x8c\xe5\xa6\x82\xe4\xb8\x8b\n# \n# ![](https://ws3.sinaimg.cn/large/006tKfTcly1fmlx2wl5tqj30ge0au745.jpg)\n# \n# \xe6\x89\x80\xe4\xbb\xa5\xe6\x88\x91\xe4\xbb\xac\xe7\x9a\x84\xe4\xbb\xbb\xe5\x8a\xa1\xe5\xb0\xb1\xe6\x98\xaf\xe7\xbb\x99\xe5\x87\xba\xe4\xb8\x80\xe5\xbc\xa0\xe5\x9b\xbe\xe7\x89\x87\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe5\xb8\x8c\xe6\x9c\x9b\xe5\x8c\xba\xe5\x88\xab\xe5\x87\xba\xe5\x85\xb6\xe5\x88\xb0\xe5\xba\x95\xe5\xb1\x9e\xe4\xba\x8e 0 \xe5\x88\xb0 9 \xe8\xbf\x99 10 \xe4\xb8\xaa\xe6\x95\xb0\xe5\xad\x97\xe4\xb8\xad\xe7\x9a\x84\xe5\x93\xaa\xe4\xb8\x80\xe4\xb8\xaa\xe3\x80\x82\n# \n# ## \xe5\xa4\x9a\xe5\x88\x86\xe7\xb1\xbb\xe9\x97\xae\xe9\xa2\x98\n# \xe5\x89\x8d\xe9\x9d\xa2\xe6\x88\x91\xe4\xbb\xac\xe8\xae\xb2\xe8\xbf\x87\xe4\xba\x8c\xe5\x88\x86\xe7\xb1\xbb\xe9\x97\xae\xe9\xa2\x98\xef\xbc\x8c\xe7\x8e\xb0\xe5\x9c\xa8\xe5\xa4\x84\xe7\x90\x86\xe7\x9a\x84\xe9\x97\xae\xe9\xa2\x98\xe6\x9b\xb4\xe5\x8a\xa0\xe5\xa4\x8d\xe6\x9d\x82\xef\xbc\x8c\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa 10 \xe5\x88\x86\xe7\xb1\xbb\xe9\x97\xae\xe9\xa2\x98\xef\xbc\x8c\xe7\xbb\x9f\xe7\xa7\xb0\xe4\xb8\xba\xe5\xa4\x9a\xe5\x88\x86\xe7\xb1\xbb\xe9\x97\xae\xe9\xa2\x98\xef\xbc\x8c\xe5\xaf\xb9\xe4\xba\x8e\xe5\xa4\x9a\xe5\x88\x86\xe7\xb1\xbb\xe9\x97\xae\xe9\xa2\x98\xe8\x80\x8c\xe8\xa8\x80\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe7\x9a\x84 loss \xe5\x87\xbd\xe6\x95\xb0\xe4\xbd\xbf\xe7\x94\xa8\xe4\xb8\x80\xe4\xb8\xaa\xe6\x9b\xb4\xe5\x8a\xa0\xe5\xa4\x8d\xe6\x9d\x82\xe7\x9a\x84\xe5\x87\xbd\xe6\x95\xb0\xef\xbc\x8c\xe5\x8f\xab\xe4\xba\xa4\xe5\x8f\x89\xe7\x86\xb5\xe3\x80\x82\n# \n# ### softmax\n# \xe6\x8f\x90\xe5\x88\xb0\xe4\xba\xa4\xe5\x8f\x89\xe7\x86\xb5\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe5\x85\x88\xe8\xae\xb2\xe4\xb8\x80\xe4\xb8\x8b softmax \xe5\x87\xbd\xe6\x95\xb0\xef\xbc\x8c\xe5\x89\x8d\xe9\x9d\xa2\xe6\x88\x91\xe4\xbb\xac\xe8\xa7\x81\xe8\xbf\x87\xe4\xba\x86 sigmoid \xe5\x87\xbd\xe6\x95\xb0\xef\xbc\x8c\xe5\xa6\x82\xe4\xb8\x8b\n# \n# $$s(x) = \\frac{1}{1 + e^{-x}}$$\n# \n# \xe5\x8f\xaf\xe4\xbb\xa5\xe5\xb0\x86\xe4\xbb\xbb\xe4\xbd\x95\xe4\xb8\x80\xe4\xb8\xaa\xe5\x80\xbc\xe8\xbd\xac\xe6\x8d\xa2\xe5\x88\xb0 0 ~ 1 \xe4\xb9\x8b\xe9\x97\xb4\xef\xbc\x8c\xe5\xbd\x93\xe7\x84\xb6\xe5\xaf\xb9\xe4\xba\x8e\xe4\xb8\x80\xe4\xb8\xaa\xe4\xba\x8c\xe5\x88\x86\xe7\xb1\xbb\xe9\x97\xae\xe9\xa2\x98\xef\xbc\x8c\xe8\xbf\x99\xe6\xa0\xb7\xe5\xb0\xb1\xe8\xb6\xb3\xe5\xa4\x9f\xe4\xba\x86\xef\xbc\x8c\xe5\x9b\xa0\xe4\xb8\xba\xe5\xaf\xb9\xe4\xba\x8e\xe4\xba\x8c\xe5\x88\x86\xe7\xb1\xbb\xe9\x97\xae\xe9\xa2\x98\xef\xbc\x8c\xe5\xa6\x82\xe6\x9e\x9c\xe4\xb8\x8d\xe5\xb1\x9e\xe4\xba\x8e\xe7\xac\xac\xe4\xb8\x80\xe7\xb1\xbb\xef\xbc\x8c\xe9\x82\xa3\xe4\xb9\x88\xe5\xbf\x85\xe5\xae\x9a\xe5\xb1\x9e\xe4\xba\x8e\xe7\xac\xac\xe4\xba\x8c\xe7\xb1\xbb\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe5\x8f\xaa\xe9\x9c\x80\xe8\xa6\x81\xe7\x94\xa8\xe4\xb8\x80\xe4\xb8\xaa\xe5\x80\xbc\xe6\x9d\xa5\xe8\xa1\xa8\xe7\xa4\xba\xe5\x85\xb6\xe5\xb1\x9e\xe4\xba\x8e\xe5\x85\xb6\xe4\xb8\xad\xe4\xb8\x80\xe7\xb1\xbb\xe6\xa6\x82\xe7\x8e\x87\xef\xbc\x8c\xe4\xbd\x86\xe6\x98\xaf\xe5\xaf\xb9\xe4\xba\x8e\xe5\xa4\x9a\xe5\x88\x86\xe7\xb1\xbb\xe9\x97\xae\xe9\xa2\x98\xef\xbc\x8c\xe8\xbf\x99\xe6\xa0\xb7\xe5\xb9\xb6\xe4\xb8\x8d\xe8\xa1\x8c\xef\xbc\x8c\xe9\x9c\x80\xe8\xa6\x81\xe7\x9f\xa5\xe9\x81\x93\xe5\x85\xb6\xe5\xb1\x9e\xe4\xba\x8e\xe6\xaf\x8f\xe4\xb8\x80\xe7\xb1\xbb\xe7\x9a\x84\xe6\xa6\x82\xe7\x8e\x87\xef\xbc\x8c\xe8\xbf\x99\xe4\xb8\xaa\xe6\x97\xb6\xe5\x80\x99\xe5\xb0\xb1\xe9\x9c\x80\xe8\xa6\x81 softmax \xe5\x87\xbd\xe6\x95\xb0\xe4\xba\x86\xe3\x80\x82\n# \n# softmax \xe5\x87\xbd\xe6\x95\xb0\xe7\xa4\xba\xe4\xbe\x8b\xe5\xa6\x82\xe4\xb8\x8b\n# \n# ![](https://ws4.sinaimg.cn/large/006tKfTcly1fmlxtnfm4fj30ll0bnq3c.jpg)\n# \n#%% [markdown]\n# \xe5\xaf\xb9\xe4\xba\x8e\xe7\xbd\x91\xe7\xbb\x9c\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba $z_1, z_2, \\cdots z_k$\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe9\xa6\x96\xe5\x85\x88\xe5\xaf\xb9\xe4\xbb\x96\xe4\xbb\xac\xe6\xaf\x8f\xe4\xb8\xaa\xe9\x83\xbd\xe5\x8f\x96\xe6\x8c\x87\xe6\x95\xb0\xe5\x8f\x98\xe6\x88\x90 $e^{z_1}, e^{z_2}, \\cdots, e^{z_k}$\xef\xbc\x8c\xe9\x82\xa3\xe4\xb9\x88\xe6\xaf\x8f\xe4\xb8\x80\xe9\xa1\xb9\xe9\x83\xbd\xe9\x99\xa4\xe4\xbb\xa5\xe4\xbb\x96\xe4\xbb\xac\xe7\x9a\x84\xe6\xb1\x82\xe5\x92\x8c\xef\xbc\x8c\xe4\xb9\x9f\xe5\xb0\xb1\xe6\x98\xaf\n# \n# $$\n# z_i \\rightarrow \\frac{e^{z_i}}{\\sum_{j=1}^{k} e^{z_j}}\n# $$\n# \n# \xe5\xa6\x82\xe6\x9e\x9c\xe5\xaf\xb9\xe7\xbb\x8f\xe8\xbf\x87 softmax \xe5\x87\xbd\xe6\x95\xb0\xe7\x9a\x84\xe6\x89\x80\xe6\x9c\x89\xe9\xa1\xb9\xe6\xb1\x82\xe5\x92\x8c\xe5\xb0\xb1\xe7\xad\x89\xe4\xba\x8e 1\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe4\xbb\x96\xe4\xbb\xac\xe6\xaf\x8f\xe4\xb8\x80\xe9\xa1\xb9\xe9\x83\xbd\xe5\x88\x86\xe5\x88\xab\xe8\xa1\xa8\xe7\xa4\xba\xe5\xb1\x9e\xe4\xba\x8e\xe5\x85\xb6\xe4\xb8\xad\xe6\x9f\x90\xe4\xb8\x80\xe7\xb1\xbb\xe7\x9a\x84\xe6\xa6\x82\xe7\x8e\x87\xe3\x80\x82\n# \n# ## \xe4\xba\xa4\xe5\x8f\x89\xe7\x86\xb5\n# \xe4\xba\xa4\xe5\x8f\x89\xe7\x86\xb5\xe8\xa1\xa1\xe9\x87\x8f\xe4\xb8\xa4\xe4\xb8\xaa\xe5\x88\x86\xe5\xb8\x83\xe7\x9b\xb8\xe4\xbc\xbc\xe6\x80\xa7\xe7\x9a\x84\xe4\xb8\x80\xe7\xa7\x8d\xe5\xba\xa6\xe9\x87\x8f\xe6\x96\xb9\xe5\xbc\x8f\xef\xbc\x8c\xe5\x89\x8d\xe9\x9d\xa2\xe8\xae\xb2\xe7\x9a\x84\xe4\xba\x8c\xe5\x88\x86\xe7\xb1\xbb\xe9\x97\xae\xe9\xa2\x98\xe7\x9a\x84 loss \xe5\x87\xbd\xe6\x95\xb0\xe5\xb0\xb1\xe6\x98\xaf\xe4\xba\xa4\xe5\x8f\x89\xe7\x86\xb5\xe7\x9a\x84\xe4\xb8\x80\xe7\xa7\x8d\xe7\x89\xb9\xe6\xae\x8a\xe6\x83\x85\xe5\x86\xb5\xef\xbc\x8c\xe4\xba\xa4\xe5\x8f\x89\xe7\x86\xb5\xe7\x9a\x84\xe4\xb8\x80\xe8\x88\xac\xe5\x85\xac\xe5\xbc\x8f\xe4\xb8\xba\n# \n# $$\n# cross\\_entropy(p, q) = E_{p}[-\\log q] = - \\frac{1}{m} \\sum_{x} p(x) \\log q(x)\n# $$\n# \n# \xe5\xaf\xb9\xe4\xba\x8e\xe4\xba\x8c\xe5\x88\x86\xe7\xb1\xbb\xe9\x97\xae\xe9\xa2\x98\xe6\x88\x91\xe4\xbb\xac\xe5\x8f\xaf\xe4\xbb\xa5\xe5\x86\x99\xe6\x88\x90\n# \n# $$\n# -\\frac{1}{m} \\sum_{i=1}^m (y^{i} \\log sigmoid(x^{i}) + (1 - y^{i}) \\log (1 - sigmoid(x^{i}))\n# $$\n# \n# \xe8\xbf\x99\xe5\xb0\xb1\xe6\x98\xaf\xe6\x88\x91\xe4\xbb\xac\xe4\xb9\x8b\xe5\x89\x8d\xe8\xae\xb2\xe7\x9a\x84\xe4\xba\x8c\xe5\x88\x86\xe7\xb1\xbb\xe9\x97\xae\xe9\xa2\x98\xe7\x9a\x84 loss\xef\xbc\x8c\xe5\xbd\x93\xe6\x97\xb6\xe6\x88\x91\xe4\xbb\xac\xe5\xb9\xb6\xe6\xb2\xa1\xe6\x9c\x89\xe8\xa7\xa3\xe9\x87\x8a\xe5\x8e\x9f\xe5\x9b\xa0\xef\xbc\x8c\xe5\x8f\xaa\xe6\x98\xaf\xe7\xbb\x99\xe5\x87\xba\xe4\xba\x86\xe5\x85\xac\xe5\xbc\x8f\xef\xbc\x8c\xe7\x84\xb6\xe5\x90\x8e\xe8\xa7\xa3\xe9\x87\x8a\xe4\xba\x86\xe5\x85\xb6\xe5\x90\x88\xe7\x90\x86\xe6\x80\xa7\xef\xbc\x8c\xe7\x8e\xb0\xe5\x9c\xa8\xe6\x88\x91\xe4\xbb\xac\xe7\xbb\x99\xe5\x87\xba\xe4\xba\x86\xe5\x85\xac\xe5\xbc\x8f\xe5\x8e\xbb\xe8\xaf\x81\xe6\x98\x8e\xe8\xbf\x99\xe6\xa0\xb7\xe5\x8f\x96 loss \xe5\x87\xbd\xe6\x95\xb0\xe6\x98\xaf\xe5\x90\x88\xe7\x90\x86\xe7\x9a\x84\n# \n# \xe4\xba\xa4\xe5\x8f\x89\xe7\x86\xb5\xe6\x98\xaf\xe4\xbf\xa1\xe6\x81\xaf\xe7\x90\x86\xe8\xae\xba\xe9\x87\x8c\xe9\x9d\xa2\xe7\x9a\x84\xe5\x86\x85\xe5\xae\xb9\xef\xbc\x8c\xe8\xbf\x99\xe9\x87\x8c\xe4\xb8\x8d\xe5\x86\x8d\xe5\x85\xb7\xe4\xbd\x93\xe5\xb1\x95\xe5\xbc\x80\xef\xbc\x8c\xe6\x9b\xb4\xe5\xa4\x9a\xe7\x9a\x84\xe5\x86\x85\xe5\xae\xb9\xef\xbc\x8c\xe5\x8f\xaf\xe4\xbb\xa5\xe7\x9c\x8b\xe5\x88\xb0\xe4\xb8\x8b\xe9\x9d\xa2\xe7\x9a\x84[\xe9\x93\xbe\xe6\x8e\xa5](http://blog.csdn.net/rtygbwwwerr/article/details/50778098)\n# \n# \xe4\xb8\x8b\xe9\x9d\xa2\xe6\x88\x91\xe4\xbb\xac\xe7\x9b\xb4\xe6\x8e\xa5\xe7\x94\xa8 mnist \xe4\xb8\xbe\xe4\xbe\x8b\xef\xbc\x8c\xe8\xae\xb2\xe4\xb8\x80\xe8\xae\xb2\xe6\xb7\xb1\xe5\xba\xa6\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\n\n#%%\nimport numpy as np\nimport torch\nfrom torchvision.datasets import mnist # \xe5\xaf\xbc\xe5\x85\xa5 pytorch \xe5\x86\x85\xe7\xbd\xae\xe7\x9a\x84 mnist \xe6\x95\xb0\xe6\x8d\xae\n\nfrom torch import nn\n\n\n#%%\n# \xe4\xbd\xbf\xe7\x94\xa8\xe5\x86\x85\xe7\xbd\xae\xe5\x87\xbd\xe6\x95\xb0\xe4\xb8\x8b\xe8\xbd\xbd mnist \xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\ntrain_set = mnist.MNIST(r'C:/DATASETS', train=True, download=True)\ntest_set = mnist.MNIST(r'C:/DATASETS', train=False, download=True)\n\n#%% [markdown]\n# \xe6\x88\x91\xe4\xbb\xac\xe5\x8f\xaf\xe4\xbb\xa5\xe7\x9c\x8b\xe7\x9c\x8b\xe5\x85\xb6\xe4\xb8\xad\xe7\x9a\x84\xe4\xb8\x80\xe4\xb8\xaa\xe6\x95\xb0\xe6\x8d\xae\xe6\x98\xaf\xe4\xbb\x80\xe4\xb9\x88\xe6\xa0\xb7\xe5\xad\x90\xe7\x9a\x84\n\n#%%\na_data, a_label = train_set[0]\n\n\n#%%\na_data\n\n\n#%%\na_label\n\n#%% [markdown]\n# \xe8\xbf\x99\xe9\x87\x8c\xe7\x9a\x84\xe8\xaf\xbb\xe5\x85\xa5\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe6\x98\xaf PIL \xe5\xba\x93\xe4\xb8\xad\xe7\x9a\x84\xe6\xa0\xbc\xe5\xbc\x8f\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe5\x8f\xaf\xe4\xbb\xa5\xe9\x9d\x9e\xe5\xb8\xb8\xe6\x96\xb9\xe4\xbe\xbf\xe5\x9c\xb0\xe5\xb0\x86\xe5\x85\xb6\xe8\xbd\xac\xe6\x8d\xa2\xe4\xb8\xba numpy array\n\n#%%\na_data = np.array(a_data, dtype='float32')\nprint(a_data.shape)\n\n#%% [markdown]\n# \xe8\xbf\x99\xe9\x87\x8c\xe6\x88\x91\xe4\xbb\xac\xe5\x8f\xaf\xe4\xbb\xa5\xe7\x9c\x8b\xe5\x88\xb0\xe8\xbf\x99\xe7\xa7\x8d\xe5\x9b\xbe\xe7\x89\x87\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f\xe6\x98\xaf 28 x 28\n\n#%%\nprint(a_data)\n\n#%% [markdown]\n# \xe6\x88\x91\xe4\xbb\xac\xe5\x8f\xaf\xe4\xbb\xa5\xe5\xb0\x86\xe6\x95\xb0\xe7\xbb\x84\xe5\xb1\x95\xe7\xa4\xba\xe5\x87\xba\xe6\x9d\xa5\xef\xbc\x8c\xe9\x87\x8c\xe9\x9d\xa2\xe7\x9a\x84 0 \xe5\xb0\xb1\xe8\xa1\xa8\xe7\xa4\xba\xe9\xbb\x91\xe8\x89\xb2\xef\xbc\x8c255 \xe8\xa1\xa8\xe7\xa4\xba\xe7\x99\xbd\xe8\x89\xb2\n# \n# \xe5\xaf\xb9\xe4\xba\x8e\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe7\xac\xac\xe4\xb8\x80\xe5\xb1\x82\xe7\x9a\x84\xe8\xbe\x93\xe5\x85\xa5\xe5\xb0\xb1\xe6\x98\xaf 28 x 28 = 784\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe5\xbf\x85\xe9\xa1\xbb\xe5\xb0\x86\xe5\xbe\x97\xe5\x88\xb0\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe6\x88\x91\xe4\xbb\xac\xe5\x81\x9a\xe4\xb8\x80\xe4\xb8\xaa\xe5\x8f\x98\xe6\x8d\xa2\xef\xbc\x8c\xe4\xbd\xbf\xe7\x94\xa8 reshape \xe5\xb0\x86\xe4\xbb\x96\xe4\xbb\xac\xe6\x8b\x89\xe5\xb9\xb3\xe6\x88\x90\xe4\xb8\x80\xe4\xb8\xaa\xe4\xb8\x80\xe7\xbb\xb4\xe5\x90\x91\xe9\x87\x8f\n\n#%%\ndef data_tf(x):\n    x = np.array(x, dtype='float32') / 255\n    x = (x - 0.5) / 0.5 # \xe6\xa0\x87\xe5\x87\x86\xe5\x8c\x96\xef\xbc\x8c\xe8\xbf\x99\xe4\xb8\xaa\xe6\x8a\x80\xe5\xb7\xa7\xe4\xb9\x8b\xe5\x90\x8e\xe4\xbc\x9a\xe8\xae\xb2\xe5\x88\xb0\n    x = x.reshape((-1,)) # \xe6\x8b\x89\xe5\xb9\xb3\n    x = torch.from_numpy(x)\n    return x\n\ntrain_set = mnist.MNIST(r'C:/DATASETS', \n                        train=True, \n                        transform = data_tf, \n                        download = True) # \xe9\x87\x8d\xe6\x96\xb0\xe8\xbd\xbd\xe5\x85\xa5\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xef\xbc\x8c\xe7\x94\xb3\xe6\x98\x8e\xe5\xae\x9a\xe4\xb9\x89\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe5\x8f\x98\xe6\x8d\xa2\ntest_set = mnist.MNIST(r'C:/DATASETS', \n                       train = False, \n                       transform = data_tf, \n                       download = True)\n\n\n#%%\na, a_label = train_set[0]\nprint(a.shape)\nprint(a_label)\n\n\n#%%\nfrom torch.utils.data import DataLoader\n# \xe4\xbd\xbf\xe7\x94\xa8 pytorch \xe8\x87\xaa\xe5\xb8\xa6\xe7\x9a\x84 DataLoader \xe5\xae\x9a\xe4\xb9\x89\xe4\xb8\x80\xe4\xb8\xaa\xe6\x95\xb0\xe6\x8d\xae\xe8\xbf\xad\xe4\xbb\xa3\xe5\x99\xa8\ntrain_data = DataLoader(train_set, batch_size=64, shuffle=True)\ntest_data = DataLoader(test_set, batch_size=128, shuffle=False)\n\n#%% [markdown]\n# \xe4\xbd\xbf\xe7\x94\xa8\xe8\xbf\x99\xe6\xa0\xb7\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe8\xbf\xad\xe4\xbb\xa3\xe5\x99\xa8\xe6\x98\xaf\xe9\x9d\x9e\xe5\xb8\xb8\xe6\x9c\x89\xe5\xbf\x85\xe8\xa6\x81\xe7\x9a\x84\xef\xbc\x8c\xe5\xa6\x82\xe6\x9e\x9c\xe6\x95\xb0\xe6\x8d\xae\xe9\x87\x8f\xe5\xa4\xaa\xe5\xa4\xa7\xef\xbc\x8c\xe5\xb0\xb1\xe6\x97\xa0\xe6\xb3\x95\xe4\xb8\x80\xe6\xac\xa1\xe5\xb0\x86\xe4\xbb\x96\xe4\xbb\xac\xe5\x85\xa8\xe9\x83\xa8\xe8\xaf\xbb\xe5\x85\xa5\xe5\x86\x85\xe5\xad\x98\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe9\x9c\x80\xe8\xa6\x81\xe4\xbd\xbf\xe7\x94\xa8 python \xe8\xbf\xad\xe4\xbb\xa3\xe5\x99\xa8\xef\xbc\x8c\xe6\xaf\x8f\xe6\xac\xa1\xe7\x94\x9f\xe6\x88\x90\xe4\xb8\x80\xe4\xb8\xaa\xe6\x89\xb9\xe6\xac\xa1\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\n\n#%%\na, a_label = next(iter(train_data))\n\n\n#%%\n# \xe6\x89\x93\xe5\x8d\xb0\xe5\x87\xba\xe4\xb8\x80\xe4\xb8\xaa\xe6\x89\xb9\xe6\xac\xa1\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe5\xa4\xa7\xe5\xb0\x8f\nprint(a.shape)\nprint(a_label.shape)\n\n\n#%%\n# \xe4\xbd\xbf\xe7\x94\xa8 Sequential \xe5\xae\x9a\xe4\xb9\x89 4 \xe5\xb1\x82\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\nnet = nn.Sequential(\n    nn.Linear(784, 400),\n    nn.ReLU(),\n    nn.Linear(400, 200),\n    nn.ReLU(),\n    nn.Linear(200, 100),\n    nn.ReLU(),\n    nn.Linear(100, 10)\n)\n\n\n#%%\nnet\n\n#%% [markdown]\n# \xe4\xba\xa4\xe5\x8f\x89\xe7\x86\xb5\xe5\x9c\xa8 pytorch \xe4\xb8\xad\xe5\xb7\xb2\xe7\xbb\x8f\xe5\x86\x85\xe7\xbd\xae\xe4\xba\x86\xef\xbc\x8c\xe4\xba\xa4\xe5\x8f\x89\xe7\x86\xb5\xe7\x9a\x84\xe6\x95\xb0\xe5\x80\xbc\xe7\xa8\xb3\xe5\xae\x9a\xe6\x80\xa7\xe6\x9b\xb4\xe5\xb7\xae\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe5\x86\x85\xe7\xbd\xae\xe7\x9a\x84\xe5\x87\xbd\xe6\x95\xb0\xe5\xb7\xb2\xe7\xbb\x8f\xe5\xb8\xae\xe6\x88\x91\xe4\xbb\xac\xe8\xa7\xa3\xe5\x86\xb3\xe4\xba\x86\xe8\xbf\x99\xe4\xb8\xaa\xe9\x97\xae\xe9\xa2\x98\n\n#%%\n# \xe5\xae\x9a\xe4\xb9\x89 loss \xe5\x87\xbd\xe6\x95\xb0\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(net.parameters(), 1e-1) # \xe4\xbd\xbf\xe7\x94\xa8\xe9\x9a\x8f\xe6\x9c\xba\xe6\xa2\xaf\xe5\xba\xa6\xe4\xb8\x8b\xe9\x99\x8d\xef\xbc\x8c\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87 0.1\n\n\n#%%\n# \xe5\xbc\x80\xe5\xa7\x8b\xe8\xae\xad\xe7\xbb\x83\nlosses = []\nacces = []\neval_losses = []\neval_acces = []\n\nfor e in range(20):\n    train_loss = 0\n    train_acc = 0\n    net.train();\n    for im, label in train_data:\n        # \xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\n        out = net(im)\n        loss = criterion(out, label)\n        # \xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        # \xe8\xae\xb0\xe5\xbd\x95\xe8\xaf\xaf\xe5\xb7\xae\n        train_loss += loss.item()\n        # \xe8\xae\xa1\xe7\xae\x97\xe5\x88\x86\xe7\xb1\xbb\xe7\x9a\x84\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87\n        _, pred = out.max(1)\n        num_correct = (pred == label).sum().item()\n        acc = num_correct / im.shape[0]\n        train_acc += acc\n        \n    losses.append(train_loss / len(train_data))\n    acces.append(train_acc / len(train_data))\n    # \xe5\x9c\xa8\xe6\xb5\x8b\xe8\xaf\x95\xe9\x9b\x86\xe4\xb8\x8a\xe6\xa3\x80\xe9\xaa\x8c\xe6\x95\x88\xe6\x9e\x9c\n    eval_loss = 0\n    eval_acc = 0\n    net.eval(); # \xe5\xb0\x86\xe6\xa8\xa1\xe5\x9e\x8b\xe6\x94\xb9\xe4\xb8\xba\xe9\xa2\x84\xe6\xb5\x8b\xe6\xa8\xa1\xe5\xbc\x8f\n    for im, label in test_data:\n        out = net(im)\n        loss = criterion(out, label)\n        # \xe8\xae\xb0\xe5\xbd\x95\xe8\xaf\xaf\xe5\xb7\xae\n        eval_loss += loss.item()\n        # \xe8\xae\xb0\xe5\xbd\x95\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87\n        _, pred = out.max(1)\n        num_correct = (pred == label).sum().item()\n        acc = num_correct / im.shape[0]\n        eval_acc += acc\n        \n    eval_losses.append(eval_loss / len(test_data))\n    eval_acces.append(eval_acc / len(test_data))\n    print('epoch: {}, Train Loss: {:.6f}, Train Acc: {:.6f}, Eval Loss: {:.6f}, Eval Acc: {:.6f}'\n          .format(e, train_loss / len(train_data), train_acc / len(train_data), \n                     eval_loss / len(test_data), eval_acc / len(test_data)))\n\n#%% [markdown]\n# \xe7\x94\xbb\xe5\x87\xba loss \xe6\x9b\xb2\xe7\xba\xbf\xe5\x92\x8c \xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87\xe6\x9b\xb2\xe7\xba\xbf\n\n#%%\nimport matplotlib.pyplot as plt\nget_ipython().run_line_magic('matplotlib', 'inline')\n\n\n#%%\nplt.title('train loss')\nplt.plot(np.arange(len(losses)), losses)\n\n\n#%%\nplt.plot(np.arange(len(acces)), acces)\nplt.title('train acc')\n\n\n#%%\nplt.plot(np.arange(len(eval_losses)), eval_losses)\nplt.title('test loss')\n\n\n#%%\nplt.plot(np.arange(len(eval_acces)), eval_acces)\nplt.title('test acc')\n\n#%% [markdown]\n# \xe5\x8f\xaf\xe4\xbb\xa5\xe7\x9c\x8b\xe5\x88\xb0\xe6\x88\x91\xe4\xbb\xac\xe7\x9a\x84\xe4\xb8\x89\xe5\xb1\x82\xe7\xbd\x91\xe7\xbb\x9c\xe5\x9c\xa8\xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\xe4\xb8\x8a\xe8\x83\xbd\xe5\xa4\x9f\xe8\xbe\xbe\xe5\x88\xb0 99.9% \xe7\x9a\x84\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87\xef\xbc\x8c\xe6\xb5\x8b\xe8\xaf\x95\xe9\x9b\x86\xe4\xb8\x8a\xe8\x83\xbd\xe5\xa4\x9f\xe8\xbe\xbe\xe5\x88\xb0 98.20% \xe7\x9a\x84\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87\n#%% [markdown]\n# **\xe5\xb0\x8f\xe7\xbb\x83\xe4\xb9\xa0\xef\xbc\x9a\xe7\x9c\x8b\xe4\xb8\x80\xe7\x9c\x8b\xe4\xb8\x8a\xe9\x9d\xa2\xe7\x9a\x84\xe8\xae\xad\xe7\xbb\x83\xe8\xbf\x87\xe7\xa8\x8b\xef\xbc\x8c\xe7\x9c\x8b\xe4\xb8\x80\xe4\xb8\x8b\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87\xe6\x98\xaf\xe6\x80\x8e\xe4\xb9\x88\xe8\xae\xa1\xe7\xae\x97\xe5\x87\xba\xe6\x9d\xa5\xe7\x9a\x84\xef\xbc\x8c\xe7\x89\xb9\xe5\x88\xab\xe6\xb3\xa8\xe6\x84\x8f max \xe8\xbf\x99\xe4\xb8\xaa\xe5\x87\xbd\xe6\x95\xb0**\n# \n# **\xe8\x87\xaa\xe5\xb7\xb1\xe9\x87\x8d\xe6\x96\xb0\xe5\xae\x9e\xe7\x8e\xb0\xe4\xb8\x80\xe4\xb8\xaa\xe6\x96\xb0\xe7\x9a\x84\xe7\xbd\x91\xe7\xbb\x9c\xef\xbc\x8c\xe8\xaf\x95\xe8\xaf\x95\xe6\x94\xb9\xe5\x8f\x98\xe9\x9a\x90\xe8\x97\x8f\xe5\xb1\x82\xe7\x9a\x84\xe6\x95\xb0\xe7\x9b\xae\xe5\x92\x8c\xe6\xbf\x80\xe6\xb4\xbb\xe5\x87\xbd\xe6\x95\xb0\xef\xbc\x8c\xe7\x9c\x8b\xe7\x9c\x8b\xe6\x9c\x89\xe4\xbb\x80\xe4\xb9\x88\xe6\x96\xb0\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x9c**\n\n"""
6.Neural_Network 神经网络/dnn_mnist-another-example.py,10,"b'#%%\nimport torch\nimport torch.nn as nn\nimport torchvision.datasets as dsets\nimport torchvision.transforms as transforms\n\n#%% Hyper Parameters   \xe9\x85\x8d\xe7\xbd\xae\xe5\x8f\x82\xe6\x95\xb0\ntorch.manual_seed(1)  # \xe8\xae\xbe\xe7\xbd\xae\xe9\x9a\x8f\xe6\x9c\xba\xe6\x95\xb0\xe7\xa7\x8d\xe5\xad\x90\xef\xbc\x8c\xe7\xa1\xae\xe4\xbf\x9d\xe7\xbb\x93\xe6\x9e\x9c\xe5\x8f\xaf\xe9\x87\x8d\xe5\xa4\x8d\ninput_size = 784  #\nhidden_size = 500\nnum_classes = 10\nnum_epochs = 20  # \xe8\xae\xad\xe7\xbb\x83\xe6\xac\xa1\xe6\x95\xb0\nbatch_size = 100  # \xe6\x89\xb9\xe5\xa4\x84\xe7\x90\x86\xe5\xa4\xa7\xe5\xb0\x8f\nlearning_rate = 0.001  # \xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\n\ndevice = torch.device(\'cuda\' if torch.cuda.is_available() else \'cpu\')\nroot = r""C:\\DATASETS\\mnist""\n\n#%% MNIST Dataset  \xe4\xb8\x8b\xe8\xbd\xbd\xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86 MNIST \xe6\x89\x8b\xe5\x86\x99\xe6\x95\xb0\xe5\xad\x97\xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\ntrain_dataset = dsets.MNIST(root=root,  # \xe6\x95\xb0\xe6\x8d\xae\xe4\xbf\x9d\xe6\x8c\x81\xe7\x9a\x84\xe4\xbd\x8d\xe7\xbd\xae\n                            train=True,  # \xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\n                            transform=transforms.ToTensor(),  # \xe4\xb8\x80\xe4\xb8\xaa\xe5\x8f\x96\xe5\x80\xbc\xe8\x8c\x83\xe5\x9b\xb4\xe6\x98\xaf[0,255]\xe7\x9a\x84PIL.Image\n                            # \xe8\xbd\xac\xe5\x8c\x96\xe4\xb8\xba\xe5\x8f\x96\xe5\x80\xbc\xe8\x8c\x83\xe5\x9b\xb4\xe6\x98\xaf[0,1.0]\xe7\x9a\x84torch.FloadTensor\n                            download=True)  # \xe4\xb8\x8b\xe8\xbd\xbd\xe6\x95\xb0\xe6\x8d\xae\n\ntest_dataset = dsets.MNIST(root=root,\n                           train=False,  # \xe6\xb5\x8b\xe8\xaf\x95\xe9\x9b\x86\n                           transform=transforms.ToTensor())\n\n#%% Data Loader (Input Pipeline)\n# \xe6\x95\xb0\xe6\x8d\xae\xe7\x9a\x84\xe6\x89\xb9\xe5\xa4\x84\xe7\x90\x86\xef\xbc\x8c\xe5\xb0\xba\xe5\xaf\xb8\xe5\xa4\xa7\xe5\xb0\x8f\xe4\xb8\xbabatch_size,\n# \xe5\x9c\xa8\xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\xe4\xb8\xad\xef\xbc\x8cshuffle \xe5\xbf\x85\xe9\xa1\xbb\xe8\xae\xbe\xe7\xbd\xae\xe4\xb8\xbaTrue, \xe8\xa1\xa8\xe7\xa4\xba\xe6\xac\xa1\xe5\xba\x8f\xe6\x98\xaf\xe9\x9a\x8f\xe6\x9c\xba\xe7\x9a\x84\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n                                           batch_size=batch_size,\n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n                                          batch_size=batch_size,\n                                          shuffle=False)\n\n\n#%% Neural Network Model (1 hidden layer)  \xe5\xae\x9a\xe4\xb9\x89\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xe6\xa8\xa1\xe5\x9e\x8b\nclass Net(nn.Module):\n    def __init__(self, input_size, hidden_size, num_classes):\n        super(Net, self).__init__()\n        self.fc1 = nn.Linear(input_size, hidden_size)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(hidden_size, num_classes)\n\n    def forward(self, x):\n        out = self.fc1(x)\n        out = self.relu(out)\n        out = self.fc2(out)\n        return out\n\n\nnet = Net(input_size, hidden_size, num_classes).to(device)\n#%% \xe6\x89\x93\xe5\x8d\xb0\xe6\xa8\xa1\xe5\x9e\x8b\nprint(net)\n\n#%% Loss and Optimizer  \xe5\xae\x9a\xe4\xb9\x89loss\xe5\x92\x8coptimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n\n#%% Train the Model   \xe5\xbc\x80\xe5\xa7\x8b\xe8\xae\xad\xe7\xbb\x83\nnet.train()\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):  # \xe6\x89\xb9\xe5\xa4\x84\xe7\x90\x86\n        # Convert torch tensor to Variable\n        images = images.view(-1, 28 * 28).to(device)\n        labels = labels.to(device)\n\n        # Forward + Backward + Optimize\n        optimizer.zero_grad()  # zero the gradient buffer  #\xe6\xa2\xaf\xe5\xba\xa6\xe6\xb8\x85\xe9\x9b\xb6\xef\xbc\x8c\xe4\xbb\xa5\xe5\x85\x8d\xe5\xbd\xb1\xe5\x93\x8d\xe5\x85\xb6\xe4\xbb\x96batch\n        outputs = net(images)  # \xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\n\n        # import pdb\n        # pdb.set_trace()\n        loss = criterion(outputs, labels)  # loss \n        loss.backward()  # \xe5\x90\x8e\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xef\xbc\x8c\xe8\xae\xa1\xe7\xae\x97\xe6\xa2\xaf\xe5\xba\xa6\n        optimizer.step()  # \xe6\xa2\xaf\xe5\xba\xa6\xe6\x9b\xb4\xe6\x96\xb0\n\n        if (i + 1) % 100 == 0:\n            print(\'Epoch [%d/%d], Step [%d/%d], Loss: %.4f\'\n                    % (epoch + 1, num_epochs, i + 1, len(train_dataset) // batch_size, loss.item()))\n\n#%% Test the Model\ncorrect = 0\ntotal = 0\nnet.eval()\nwith torch.no_grad():\n    for images, labels in test_loader:  # test set \xe6\x89\xb9\xe5\xa4\x84\xe7\x90\x86\n        images = images.view(-1, 28 * 28).to(device)\n        labels = labels.to(device)\n        outputs = net(images)\n        _, predicted = torch.max(outputs.detach(), 1)  # \xe9\xa2\x84\xe6\xb5\x8b\xe7\xbb\x93\xe6\x9e\x9c\n        total += labels.size(0)  # \xe6\xad\xa3\xe7\xa1\xae\xe7\xbb\x93\xe6\x9e\x9c\n        correct += (predicted == labels).sum()  # \xe6\xad\xa3\xe7\xa1\xae\xe7\xbb\x93\xe6\x9e\x9c\xe6\x80\xbb\xe6\x95\xb0\n\nprint(\'Accuracy of the network on the 10000 test images: %d %%\' % (100 * correct / total))\n\n#%% Save the Model\ntorch.save(net.state_dict(), \'mnist_dnn_model.pkl\')\n\n\n#%%\n'"
6.Neural_Network 神经网络/full-example-mnist 完整例子：多层全连接网络实现MNIST手写数字分类.py,4,"b'#%%\nimport torch\nfrom torch import nn, optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\nimport numpy as np\n\ndevice = torch.device(\'cuda\' if torch.cuda.is_available() else \'cpu\')\n#%%\nclass Simple_Net(nn.Module):\n\n    def __init__(self, in_dim, n_hidden_1, n_hidden_2, out_dim):\n        super(Simple_Net, self).__init__()\n        self.layer1 = nn.Linear(in_dim, n_hidden_1)\n        self.layer2 = nn.Linear(n_hidden_1, n_hidden_2)\n        self.layer3 = nn.Linear(n_hidden_2 ,out_dim)\n\n    def forward(self, x):\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        return x\n\n#%%\nclass Activation_Net(nn.Module):\n\n    def __init__(self, in_dim, n_hidden_1, n_hidden_2, out_dim):\n        super(Activation_Net, self).__init__()\n        self.layer1 = nn.Sequential(\n            nn.Linear(in_dim, n_hidden_1),\n            nn.ReLU(True)\n        )\n        self.layer2 = nn.Sequential(\n            nn.Linear(n_hidden_1, n_hidden_2),\n            nn.ReLU(True)\n        )\n        self.layer3 = nn.Linear(n_hidden_2, out_dim)\n\n    def forward(self, x):\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        return x\n\n\n#%%\nclass Batch_Net(nn.Module):\n\n    def __init__(self, in_dim, n_hidden_1, n_hidden_2, out_dim):\n        super(Batch_Net, self).__init__()\n        self.layer1 = nn.Sequential(\n            nn.Linear(in_dim, n_hidden_1),\n            nn.BatchNorm1d(n_hidden_1),\n            nn.ReLU(True)\n        )\n        self.layer2 = nn.Sequential(\n            nn.Linear(n_hidden_1, n_hidden_2),\n            nn.BatchNorm1d(n_hidden_2),\n            nn.ReLU(True)\n        )\n        self.layer3 = nn.Linear(n_hidden_2, out_dim)\n\n    def forward(self, x):\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        return x\n\n\n#%%\nbatch_size = 64\nlearning_rate = 1e-2\nnum_epoches = 20\n\ndef data_tf(x):\n    x = np.array(x, dtype=\'float32\') / 255\n    x = (x - 0.5) / 0.5 # \xe6\xa0\x87\xe5\x87\x86\xe5\x8c\x96\n    x = x.reshape(-1) # \xe6\x8b\x89\xe5\xb9\xb3\n    x = torch.from_numpy(x)\n    return x\n\n\ndatasets_root = r""C:/DATASETS""\n\ntrain_set = datasets.MNIST(root=datasets_root, train=True, transform=data_tf, download=True)\ntest_set = datasets.MNIST(root=datasets_root, train=False, transform=data_tf, download=True)\n\ntrain_dataset = DataLoader(train_set, batch_size=batch_size, shuffle=True)\ntest_dataset = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n\n#%%\n\nD_in, H1, H2, D_out = 28, 300, 100, 10\n\nmodel = Activation_Net(D_in*D_in, H1, H2, D_out).to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=learning_rate)\n\n#%%\n\nepoch_num = 50\n\nlosses = []\nacces = []\neval_losses = []\neval_acces = []\n\nfor e in range(epoch_num):\n    train_loss = 0\n    train_acc = 0\n    model.train()\n    for im, label in train_dataset:\n        # \xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\n        im = im.to(device)\n        label = label.to(device)\n        \n        out = model(im)\n        loss = criterion(out, label)\n        # \xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        # \xe8\xae\xb0\xe5\xbd\x95\xe8\xaf\xaf\xe5\xb7\xae\n        train_loss += loss.item()\n        # \xe8\xae\xa1\xe7\xae\x97\xe5\x88\x86\xe7\xb1\xbb\xe7\x9a\x84\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87\n        _, pred = out.max(dim = 1)\n#\xe6\x80\x8e\xe4\xb9\x88\xe5\xb0\xb1\xe4\xb8\x80\xe5\xae\x9a\xe4\xbf\x9d\xe8\xaf\x81\xe6\x98\xaf\xe6\x8c\x89\xe7\x85\xa7\xe4\xb8\x8b\xe6\xa0\x87\xe7\x9a\x84\xe5\xbe\xaa\xe5\xba\x8f\xe6\x9d\xa5\xe7\x9a\x84\xef\xbc\x9f\xe5\x9b\xa0\xe4\xb8\xbaloss = criterion(out, label)\xe8\xbf\x99\xe5\x8f\xa5\xe5\xb0\xb1\xe6\x98\xaf\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84\xe5\x85\xb3\xe7\xb3\xbb\xef\xbc\x8c\xe8\xa6\x81\xe6\x98\xaf\xe6\x83\xb3\xe4\xb8\x8d\xe5\xaf\xb9\xe5\xba\x94\xe5\xb0\xb1\xe6\x98\xafloss\xe5\x87\xbd\xe6\x95\xb0\xe6\x80\x8e\xe4\xb9\x88\xe8\xae\xbe\xe7\xbd\xae\xe4\xba\x86\n        num_correct = (pred == label).sum().item()\n        acc = num_correct / im.shape[0]\n        train_acc += acc\n        \n    losses.append(train_loss / len(train_dataset))\n    acces.append(train_acc / len(train_dataset))\n    # \xe5\x9c\xa8\xe6\xb5\x8b\xe8\xaf\x95\xe9\x9b\x86\xe4\xb8\x8a\xe6\xa3\x80\xe9\xaa\x8c\xe6\x95\x88\xe6\x9e\x9c\n    eval_loss = 0\n    eval_acc = 0\n    model.eval() # \xe5\xb0\x86\xe6\xa8\xa1\xe5\x9e\x8b\xe6\x94\xb9\xe4\xb8\xba\xe9\xa2\x84\xe6\xb5\x8b\xe6\xa8\xa1\xe5\xbc\x8f\n    with torch.no_grad():\n        for im, label in test_dataset:\n            \n            im = im.to(device)\n            label = label.to(device)\n            \n            out = model(im)\n            loss = criterion(out, label)\n            # \xe8\xae\xb0\xe5\xbd\x95\xe8\xaf\xaf\xe5\xb7\xae\n            eval_loss += loss.item()\n            # \xe8\xae\xb0\xe5\xbd\x95\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87\n            _, pred = out.max(1)\n\n            num_correct = (pred == label).sum().item()\n            acc = num_correct / im.shape[0]\n            eval_acc += acc\n        \n    eval_losses.append(eval_loss / len(test_dataset))\n    eval_acces.append(eval_acc / len(test_dataset))\n    print(\'epoch: {}, Train Loss: {:.6f}, Train Acc: {:.6f}, Eval Loss: {:.6f}, Eval Acc: {:.6f}\'.format( e, train_loss / len(train_dataset), train_acc / len(train_dataset), eval_loss / len(test_dataset), eval_acc / len(test_dataset)))\n\n\n'"
6.Neural_Network 神经网络/iris-multiclassification 数据集iris的分类.py,8,"b'\'\'\'\nPyTorch \xe5\xae\x9e\xe7\x8e\xb0iris \xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe7\x9a\x84\xe5\x88\x86\xe7\xb1\xbb\xe3\x80\x82\n\'\'\'\n#%%\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom sklearn.datasets import load_iris\nfrom torch.optim import SGD\nimport matplotlib.pyplot as plt\n\n#%%\n# GPU \xe6\x98\xaf\xe5\x90\xa6\xe5\x8f\xaf\xe7\x94\xa8\nuse_cuda = torch.cuda.is_available()\nprint(""use_cuda: "", use_cuda)\n\n#%%\n# \xe5\x8a\xa0\xe8\xbd\xbd\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\niris = load_iris()\nprint(iris.keys())  # dict_keys([\'target_names\', \'data\', \'feature_names\', \'DESCR\', \'target\'])\n\n#%%\nx = iris[\'data\']  # \xe7\x89\xb9\xe5\xbe\x81\xe4\xbf\xa1\xe6\x81\xaf\ny = iris[\'target\']  # \xe7\x9b\xae\xe6\xa0\x87\xe5\x88\x86\xe7\xb1\xbb\nprint(x.shape)  # (150, 4)\nprint(x.shape)  # (150,)\n\nprint(y)\n\n#%%\nx = torch.FloatTensor(x)\ny = torch.LongTensor(y)\n\n#%%\nclass Net(nn.Module):\n    """"""\n    \xe5\xae\x9a\xe4\xb9\x89\xe7\xbd\x91\xe7\xbb\x9c\n    """"""\n\n    def __init__(self, n_feature, n_hidden, n_output):\n        """"""\n        \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe5\x87\xbd\xe6\x95\xb0\xef\xbc\x8c\xe6\x8e\xa5\xe5\x8f\x97\xe8\x87\xaa\xe5\xae\x9a\xe4\xb9\x89\xe8\xbe\x93\xe5\x85\xa5\xe7\x89\xb9\xe5\xbe\x81\xe7\xbb\xb4\xe6\x95\xb0\xef\xbc\x8c\xe9\x9a\x90\xe8\x97\x8f\xe5\xb1\x82\xe7\x89\xb9\xe5\xbe\x81\xe7\xbb\xb4\xe6\x95\xb0\xef\xbc\x8c\xe8\xbe\x93\xe5\x87\xba\xe5\xb1\x82\xe7\x89\xb9\xe5\xbe\x81\xe7\xbb\xb4\xe6\x95\xb0\n        """"""\n        super(Net, self).__init__()\n        self.hidden = torch.nn.Linear(n_feature, n_hidden)  # \xe4\xb8\x80\xe4\xb8\xaa\xe7\xba\xbf\xe6\x80\xa7\xe9\x9a\x90\xe8\x97\x8f\xe5\xb1\x82\n        self.predict = torch.nn.Linear(n_hidden, n_output)  # \xe7\xba\xbf\xe6\x80\xa7\xe8\xbe\x93\xe5\x87\xba\xe5\xb1\x82\n\n    def forward(self, x):\n        """"""\n        \xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe8\xbf\x87\xe7\xa8\x8b\n        """"""\n        x = torch.sigmoid(self.hidden(x))\n        x = self.predict(x)\n        out = F.log_softmax(x, dim=1)\n        return out\n\n\n#%% iris \xe4\xb8\xad\xe8\xbe\x93\xe5\x85\xa5\xe7\x89\xb9\xe5\xbe\x81 4 \xe7\xbb\xb4\xef\xbc\x8c\xe9\x9a\x90\xe8\x97\x8f\xe5\xb1\x82\xe5\x92\x8c\xe8\xbe\x93\xe5\x87\xba\xe5\xb1\x82\xe5\x8f\xaf\xe4\xbb\xa5\xe8\x87\xaa\xe5\xb7\xb1\xe9\x80\x89\xe6\x8b\xa9\nnet = Net(n_feature=4, n_hidden=5, n_output=4)\n\n#%% \xe5\xa6\x82\xe6\x9e\x9cGPU\xe5\x8f\xaf\xe7\x94\xa8 \xe8\xae\xad\xe7\xbb\x83\xe6\x95\xb0\xe6\x8d\xae\xe5\x92\x8c\xe6\xa8\xa1\xe5\x9e\x8b\xe9\x83\xbd\xe6\x94\xbe\xe5\x88\xb0GPU\xe4\xb8\x8a\xef\xbc\x8c\xe6\xb3\xa8\xe6\x84\x8f\xef\xbc\x9a\xe6\x95\xb0\xe6\x8d\xae\xe5\x92\x8c\xe7\xbd\x91\xe7\xbb\x9c\xe6\x98\xaf\xe5\x90\xa6\xe5\x9c\xa8GPU\xe4\xb8\x8a\xe8\xa6\x81\xe5\x90\x8c\xe6\xad\xa5\nif use_cuda:\n    x = x.cuda()\n    y = y.cuda()\n    net = net.cuda()\n\n#%% \xe6\x9f\xa5\xe7\x9c\x8b\xe7\xbd\x91\xe7\xbb\x9c\xe7\xbb\x93\xe6\x9e\x84\nprint(net)\n#%%\noptimizer = SGD(net.parameters(), lr=0.5)\n#%%\niter_num = 1000\npx, py = [], []\n\nplt.rcParams[\'font.sans-serif\'] = [\'STSong\']  # \xe7\x94\xa8\xe6\x9d\xa5\xe6\xad\xa3\xe5\xb8\xb8\xe6\x98\xbe\xe7\xa4\xba\xe4\xb8\xad\xe6\x96\x87\xe6\xa0\x87\xe7\xad\xbe\nplt.rcParams[\'axes.unicode_minus\'] = False  # \xe7\x94\xa8\xe6\x9d\xa5\xe6\xad\xa3\xe5\xb8\xb8\xe6\x98\xbe\xe7\xa4\xba\xe8\xb4\x9f\xe5\x8f\xb7\n#%%\nfor i in range(iter_num):\n    # \xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe4\xbc\xa0\xe5\x85\xa5\xe7\xbd\x91\xe7\xbb\x9c\xe5\x89\x8d\xe5\x90\x91\xe8\xae\xa1\xe7\xae\x97\n    prediction = net(x)\n\n    # \xe8\xae\xa1\xe7\xae\x97loss\n    loss = F.nll_loss(prediction, y)\n    # \xe8\xbf\x99\xe9\x87\x8c\xe4\xb9\x9f\xe5\x8f\xaf\xe7\x94\xa8CrossEntropyLoss\n    # loss = loss_func(prediction, y)\n\n    # \xe6\xb8\x85\xe9\x99\xa4\xe7\xbd\x91\xe7\xbb\x9c\xe7\x8a\xb6\xe6\x80\x81\n    optimizer.zero_grad()\n\n    # loss \xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\n    loss.backward()\n\n    # \xe6\x9b\xb4\xe6\x96\xb0\xe5\x8f\x82\xe6\x95\xb0\n    optimizer.step()\n\n    # \xe6\x89\x93\xe5\x8d\xb0\xe5\xb9\xb6\xe8\xae\xb0\xe5\xbd\x95\xe5\xbd\x93\xe5\x89\x8d\xe7\x9a\x84index \xe5\x92\x8c loss\n    print(i, "" loss: "", loss.item())\n    px.append(i)\n    py.append(loss.item())\n\n    if i % 10 == 0:\n        # \xe5\x8a\xa8\xe6\x80\x81\xe7\x94\xbb\xe5\x87\xbaloss\xe8\xb5\xb0\xe5\x90\x91 \xe7\xbb\x93\xe6\x9e\x9c\xef\xbc\x9aloss.png\n        plt.cla()\n        plt.title(u\'\xe8\xae\xad\xe7\xbb\x83\xe8\xbf\x87\xe7\xa8\x8b\xe7\x9a\x84loss\xe6\x9b\xb2\xe7\xba\xbf\')\n        plt.xlabel(u\'\xe8\xbf\xad\xe4\xbb\xa3\xe6\xac\xa1\xe6\x95\xb0\')\n        plt.ylabel(\'\xe6\x8d\x9f\xe5\xa4\xb1\')\n        plt.plot(px, py, \'r-\', lw=1)\n        plt.text(0, 0, \'Loss=%.4f\' % loss.item(), fontdict={\'size\': 20, \'color\': \'red\'})\n        plt.pause(0.1)\n    if i == iter_num - 1:\n        # \xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x80\xe4\xb8\xaa\xe5\x9b\xbe\xe5\x83\x8f\xe5\xae\x9a\xe6\xa0\xbc\n        plt.show()\n'"
6.Neural_Network 神经网络/parameter-initialize 参数初始化.py,10,"b'#%% [markdown]\n# # \xe5\x8f\x82\xe6\x95\xb0\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\n# \xe5\x8f\x82\xe6\x95\xb0\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe5\xaf\xb9\xe6\xa8\xa1\xe5\x9e\x8b\xe5\x85\xb7\xe6\x9c\x89\xe8\xbe\x83\xe5\xa4\xa7\xe7\x9a\x84\xe5\xbd\xb1\xe5\x93\x8d\xef\xbc\x8c\xe4\xb8\x8d\xe5\x90\x8c\xe7\x9a\x84\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe6\x96\xb9\xe5\xbc\x8f\xe5\x8f\xaf\xe8\x83\xbd\xe4\xbc\x9a\xe5\xaf\xbc\xe8\x87\xb4\xe6\x88\xaa\xe7\x84\xb6\xe4\xb8\x8d\xe5\x90\x8c\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x9c\xef\xbc\x8c\xe6\x89\x80\xe5\xb9\xb8\xe7\x9a\x84\xe6\x98\xaf\xe5\xbe\x88\xe5\xa4\x9a\xe6\xb7\xb1\xe5\xba\xa6\xe5\xad\xa6\xe4\xb9\xa0\xe7\x9a\x84\xe5\x85\x88\xe9\xa9\xb1\xe4\xbb\xac\xe5\xb7\xb2\xe7\xbb\x8f\xe5\xb8\xae\xe6\x88\x91\xe4\xbb\xac\xe6\x8e\xa2\xe7\xb4\xa2\xe4\xba\x86\xe5\x90\x84\xe7\xa7\x8d\xe5\x90\x84\xe6\xa0\xb7\xe7\x9a\x84\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe6\x96\xb9\xe5\xbc\x8f\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe6\x88\x91\xe4\xbb\xac\xe5\x8f\xaa\xe9\x9c\x80\xe8\xa6\x81\xe5\xad\xa6\xe4\xbc\x9a\xe5\xa6\x82\xe4\xbd\x95\xe5\xaf\xb9\xe6\xa8\xa1\xe5\x9e\x8b\xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0\xe8\xbf\x9b\xe8\xa1\x8c\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe7\x9a\x84\xe8\xb5\x8b\xe5\x80\xbc\xe5\x8d\xb3\xe5\x8f\xaf\xe3\x80\x82\n#%% [markdown]\n# PyTorch \xe7\x9a\x84\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe6\x96\xb9\xe5\xbc\x8f\xe5\xb9\xb6\xe6\xb2\xa1\xe6\x9c\x89\xe9\x82\xa3\xe4\xb9\x88\xe6\x98\xbe\xe7\x84\xb6\xef\xbc\x8c\xe5\xa6\x82\xe6\x9e\x9c\xe4\xbd\xa0\xe4\xbd\xbf\xe7\x94\xa8\xe6\x9c\x80\xe5\x8e\x9f\xe5\xa7\x8b\xe7\x9a\x84\xe6\x96\xb9\xe5\xbc\x8f\xe5\x88\x9b\xe5\xbb\xba\xe6\xa8\xa1\xe5\x9e\x8b\xef\xbc\x8c\xe9\x82\xa3\xe4\xb9\x88\xe4\xbd\xa0\xe9\x9c\x80\xe8\xa6\x81\xe5\xae\x9a\xe4\xb9\x89\xe6\xa8\xa1\xe5\x9e\x8b\xe4\xb8\xad\xe7\x9a\x84\xe6\x89\x80\xe6\x9c\x89\xe5\x8f\x82\xe6\x95\xb0\xef\xbc\x8c\xe5\xbd\x93\xe7\x84\xb6\xe8\xbf\x99\xe6\xa0\xb7\xe4\xbd\xa0\xe5\x8f\xaf\xe4\xbb\xa5\xe9\x9d\x9e\xe5\xb8\xb8\xe6\x96\xb9\xe4\xbe\xbf\xe5\x9c\xb0\xe5\xae\x9a\xe4\xb9\x89\xe6\xaf\x8f\xe4\xb8\xaa\xe5\x8f\x98\xe9\x87\x8f\xe7\x9a\x84\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe6\x96\xb9\xe5\xbc\x8f\xef\xbc\x8c\xe4\xbd\x86\xe6\x98\xaf\xe5\xaf\xb9\xe4\xba\x8e\xe5\xa4\x8d\xe6\x9d\x82\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\xef\xbc\x8c\xe8\xbf\x99\xe5\xb9\xb6\xe4\xb8\x8d\xe5\xae\xb9\xe6\x98\x93\xef\xbc\x8c\xe8\x80\x8c\xe4\xb8\x94\xe6\x88\x91\xe4\xbb\xac\xe6\x8e\xa8\xe5\xb4\x87\xe4\xbd\xbf\xe7\x94\xa8 Sequential \xe5\x92\x8c Module \xe6\x9d\xa5\xe5\xae\x9a\xe4\xb9\x89\xe6\xa8\xa1\xe5\x9e\x8b\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe8\xbf\x99\xe4\xb8\xaa\xe6\x97\xb6\xe5\x80\x99\xe6\x88\x91\xe4\xbb\xac\xe5\xb0\xb1\xe9\x9c\x80\xe8\xa6\x81\xe7\x9f\xa5\xe9\x81\x93\xe5\xa6\x82\xe4\xbd\x95\xe6\x9d\xa5\xe8\x87\xaa\xe5\xae\x9a\xe4\xb9\x89\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe6\x96\xb9\xe5\xbc\x8f\n#%% [markdown]\n# ## \xe4\xbd\xbf\xe7\x94\xa8 NumPy \xe6\x9d\xa5\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\n# \xe5\x9b\xa0\xe4\xb8\xba PyTorch \xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa\xe9\x9d\x9e\xe5\xb8\xb8\xe7\x81\xb5\xe6\xb4\xbb\xe7\x9a\x84\xe6\xa1\x86\xe6\x9e\xb6\xef\xbc\x8c\xe7\x90\x86\xe8\xae\xba\xe4\xb8\x8a\xe8\x83\xbd\xe5\xa4\x9f\xe5\xaf\xb9\xe6\x89\x80\xe6\x9c\x89\xe7\x9a\x84 Tensor \xe8\xbf\x9b\xe8\xa1\x8c\xe6\x93\x8d\xe4\xbd\x9c\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe6\x88\x91\xe4\xbb\xac\xe8\x83\xbd\xe5\xa4\x9f\xe9\x80\x9a\xe8\xbf\x87\xe5\xae\x9a\xe4\xb9\x89\xe6\x96\xb0\xe7\x9a\x84 Tensor \xe6\x9d\xa5\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xef\xbc\x8c\xe7\x9b\xb4\xe6\x8e\xa5\xe7\x9c\x8b\xe4\xb8\x8b\xe9\x9d\xa2\xe7\x9a\x84\xe4\xbe\x8b\xe5\xad\x90\n\n#%%\nimport numpy as np\nimport torch\nfrom torch import nn\n\n\n#%%\n# \xe5\xae\x9a\xe4\xb9\x89\xe4\xb8\x80\xe4\xb8\xaa Sequential \xe6\xa8\xa1\xe5\x9e\x8b\nnet1 = nn.Sequential(\n    nn.Linear(30, 40),\n    nn.ReLU(),\n    nn.Linear(40, 50),\n    nn.ReLU(),\n    nn.Linear(50, 10)\n)\n\n\n#%%\n# \xe8\xae\xbf\xe9\x97\xae\xe7\xac\xac\xe4\xb8\x80\xe5\xb1\x82\xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0\nw1 = net1[0].weight\nb1 = net1[0].bias\n\n\n#%%\nprint(w1)\n\n#%% [markdown]\n# \xe6\xb3\xa8\xe6\x84\x8f\xef\xbc\x8c\xe8\xbf\x99\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa Parameter\xef\xbc\x8c\xe4\xb9\x9f\xe5\xb0\xb1\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa\xe7\x89\xb9\xe6\xae\x8a\xe7\x9a\x84 Tensor\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe5\x8f\xaf\xe4\xbb\xa5\xe8\xae\xbf\xe9\x97\xae\xe5\x85\xb6 `.data`\xe5\xb1\x9e\xe6\x80\xa7\xe5\xbe\x97\xe5\x88\xb0\xe5\x85\xb6\xe4\xb8\xad\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xef\xbc\x8c\xe7\x84\xb6\xe5\x90\x8e\xe7\x9b\xb4\xe6\x8e\xa5\xe5\xae\x9a\xe4\xb9\x89\xe4\xb8\x80\xe4\xb8\xaa\xe6\x96\xb0\xe7\x9a\x84 Tensor \xe5\xaf\xb9\xe5\x85\xb6\xe8\xbf\x9b\xe8\xa1\x8c\xe6\x9b\xbf\xe6\x8d\xa2\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe5\x8f\xaf\xe4\xbb\xa5\xe4\xbd\xbf\xe7\x94\xa8 PyTorch \xe4\xb8\xad\xe7\x9a\x84\xe4\xb8\x80\xe4\xba\x9b\xe9\x9a\x8f\xe6\x9c\xba\xe6\x95\xb0\xe6\x8d\xae\xe7\x94\x9f\xe6\x88\x90\xe7\x9a\x84\xe6\x96\xb9\xe5\xbc\x8f\xef\xbc\x8c\xe6\xaf\x94\xe5\xa6\x82 `torch.randn`\xef\xbc\x8c\xe5\xa6\x82\xe6\x9e\x9c\xe8\xa6\x81\xe4\xbd\xbf\xe7\x94\xa8\xe6\x9b\xb4\xe5\xa4\x9a PyTorch \xe4\xb8\xad\xe6\xb2\xa1\xe6\x9c\x89\xe7\x9a\x84\xe9\x9a\x8f\xe6\x9c\xba\xe5\x8c\x96\xe6\x96\xb9\xe5\xbc\x8f\xef\xbc\x8c\xe5\x8f\xaf\xe4\xbb\xa5\xe4\xbd\xbf\xe7\x94\xa8 numpy\n\n#%%\n# \xe5\xae\x9a\xe4\xb9\x89\xe4\xb8\x80\xe4\xb8\xaa Tensor \xe7\x9b\xb4\xe6\x8e\xa5\xe5\xaf\xb9\xe5\x85\xb6\xe8\xbf\x9b\xe8\xa1\x8c\xe6\x9b\xbf\xe6\x8d\xa2\nnet1[0].weight.data = torch.from_numpy(np.random.uniform(3, 5, size=(40, 30)))\n\n\n#%%\nprint(net1[0].weight)\n\n#%% [markdown]\n# \xe5\x8f\xaf\xe4\xbb\xa5\xe7\x9c\x8b\xe5\x88\xb0\xe8\xbf\x99\xe4\xb8\xaa\xe5\x8f\x82\xe6\x95\xb0\xe7\x9a\x84\xe5\x80\xbc\xe5\xb7\xb2\xe7\xbb\x8f\xe8\xa2\xab\xe6\x94\xb9\xe5\x8f\x98\xe4\xba\x86\xef\xbc\x8c\xe4\xb9\x9f\xe5\xb0\xb1\xe6\x98\xaf\xe8\xaf\xb4\xe5\xb7\xb2\xe7\xbb\x8f\xe8\xa2\xab\xe5\xae\x9a\xe4\xb9\x89\xe6\x88\x90\xe4\xba\x86\xe6\x88\x91\xe4\xbb\xac\xe9\x9c\x80\xe8\xa6\x81\xe7\x9a\x84\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe6\x96\xb9\xe5\xbc\x8f\xef\xbc\x8c\xe5\xa6\x82\xe6\x9e\x9c\xe6\xa8\xa1\xe5\x9e\x8b\xe4\xb8\xad\xe6\x9f\x90\xe4\xb8\x80\xe5\xb1\x82\xe9\x9c\x80\xe8\xa6\x81\xe6\x88\x91\xe4\xbb\xac\xe6\x89\x8b\xe5\x8a\xa8\xe5\x8e\xbb\xe4\xbf\xae\xe6\x94\xb9\xef\xbc\x8c\xe9\x82\xa3\xe4\xb9\x88\xe6\x88\x91\xe4\xbb\xac\xe5\x8f\xaf\xe4\xbb\xa5\xe7\x9b\xb4\xe6\x8e\xa5\xe7\x94\xa8\xe8\xbf\x99\xe7\xa7\x8d\xe6\x96\xb9\xe5\xbc\x8f\xe5\x8e\xbb\xe8\xae\xbf\xe9\x97\xae\xef\xbc\x8c\xe4\xbd\x86\xe6\x98\xaf\xe6\x9b\xb4\xe5\xa4\x9a\xe7\x9a\x84\xe6\x97\xb6\xe5\x80\x99\xe6\x98\xaf\xe6\xa8\xa1\xe5\x9e\x8b\xe4\xb8\xad\xe7\x9b\xb8\xe5\x90\x8c\xe7\xb1\xbb\xe5\x9e\x8b\xe7\x9a\x84\xe5\xb1\x82\xe9\x83\xbd\xe9\x9c\x80\xe8\xa6\x81\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe6\x88\x90\xe7\x9b\xb8\xe5\x90\x8c\xe7\x9a\x84\xe6\x96\xb9\xe5\xbc\x8f\xef\xbc\x8c\xe8\xbf\x99\xe4\xb8\xaa\xe6\x97\xb6\xe5\x80\x99\xe4\xb8\x80\xe7\xa7\x8d\xe6\x9b\xb4\xe9\xab\x98\xe6\x95\x88\xe7\x9a\x84\xe6\x96\xb9\xe5\xbc\x8f\xe6\x98\xaf\xe4\xbd\xbf\xe7\x94\xa8\xe5\xbe\xaa\xe7\x8e\xaf\xe5\x8e\xbb\xe8\xae\xbf\xe9\x97\xae\xef\xbc\x8c\xe6\xaf\x94\xe5\xa6\x82\n\n#%%\nfor layer in net1:\n    if isinstance(layer, nn.Linear): # \xe5\x88\xa4\xe6\x96\xad\xe6\x98\xaf\xe5\x90\xa6\xe6\x98\xaf\xe7\xba\xbf\xe6\x80\xa7\xe5\xb1\x82\n        param_shape = layer.weight.shape\n        layer.weight.data = torch.from_numpy(np.random.normal(0, 0.5, size=param_shape)) \n        # \xe5\xae\x9a\xe4\xb9\x89\xe4\xb8\xba\xe5\x9d\x87\xe5\x80\xbc\xe4\xb8\xba 0\xef\xbc\x8c\xe6\x96\xb9\xe5\xb7\xae\xe4\xb8\xba 0.5 \xe7\x9a\x84\xe6\xad\xa3\xe6\x80\x81\xe5\x88\x86\xe5\xb8\x83\n\n#%% [markdown]\n# **\xe5\xb0\x8f\xe7\xbb\x83\xe4\xb9\xa0\xef\xbc\x9a\xe4\xb8\x80\xe7\xa7\x8d\xe9\x9d\x9e\xe5\xb8\xb8\xe6\xb5\x81\xe8\xa1\x8c\xe7\x9a\x84\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe6\x96\xb9\xe5\xbc\x8f\xe5\x8f\xab Xavier\xef\xbc\x8c\xe6\x96\xb9\xe6\xb3\x95\xe6\x9d\xa5\xe6\xba\x90\xe4\xba\x8e 2010 \xe5\xb9\xb4\xe7\x9a\x84\xe4\xb8\x80\xe7\xaf\x87\xe8\xae\xba\xe6\x96\x87 [Understanding the difficulty of training deep feedforward neural networks](http://proceedings.mlr.press/v9/glorot10a.html)\xef\xbc\x8c\xe5\x85\xb6\xe9\x80\x9a\xe8\xbf\x87\xe6\x95\xb0\xe5\xad\xa6\xe7\x9a\x84\xe6\x8e\xa8\xe5\x88\xb0\xef\xbc\x8c\xe8\xaf\x81\xe6\x98\x8e\xe4\xba\x86\xe8\xbf\x99\xe7\xa7\x8d\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe6\x96\xb9\xe5\xbc\x8f\xe5\x8f\xaf\xe4\xbb\xa5\xe4\xbd\xbf\xe5\xbe\x97\xe6\xaf\x8f\xe4\xb8\x80\xe5\xb1\x82\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe6\x96\xb9\xe5\xb7\xae\xe6\x98\xaf\xe5\xb0\xbd\xe5\x8f\xaf\xe8\x83\xbd\xe7\x9b\xb8\xe7\xad\x89\xe7\x9a\x84\xef\xbc\x8c\xe6\x9c\x89\xe5\x85\xb4\xe8\xb6\xa3\xe7\x9a\x84\xe5\x90\x8c\xe5\xad\xa6\xe5\x8f\xaf\xe4\xbb\xa5\xe5\x8e\xbb\xe7\x9c\x8b\xe7\x9c\x8b\xe8\xae\xba\xe6\x96\x87**\n# \n# \xe6\x88\x91\xe4\xbb\xac\xe7\xbb\x99\xe5\x87\xba\xe8\xbf\x99\xe7\xa7\x8d\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe7\x9a\x84\xe5\x85\xac\xe5\xbc\x8f\n# \n# $$\n# w\\ \\sim \\ Uniform[- \\frac{\\sqrt{6}}{\\sqrt{n_j + n_{j+1}}}, \\frac{\\sqrt{6}}{\\sqrt{n_j + n_{j+1}}}]\n# $$\n# \n# \xe5\x85\xb6\xe4\xb8\xad $n_j$ \xe5\x92\x8c $n_{j+1}$ \xe8\xa1\xa8\xe7\xa4\xba\xe8\xaf\xa5\xe5\xb1\x82\xe7\x9a\x84\xe8\xbe\x93\xe5\x85\xa5\xe5\x92\x8c\xe8\xbe\x93\xe5\x87\xba\xe6\x95\xb0\xe7\x9b\xae\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe8\xaf\xb7\xe5\xb0\x9d\xe8\xaf\x95\xe5\xae\x9e\xe7\x8e\xb0\xe4\xbb\xa5\xe4\xb8\x8b\xe8\xbf\x99\xe7\xa7\x8d\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe6\x96\xb9\xe5\xbc\x8f\n#%% [markdown]\n# \xe5\xaf\xb9\xe4\xba\x8e Module \xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xef\xbc\x8c\xe5\x85\xb6\xe5\xae\x9e\xe4\xb9\x9f\xe9\x9d\x9e\xe5\xb8\xb8\xe7\xae\x80\xe5\x8d\x95\xef\xbc\x8c\xe5\xa6\x82\xe6\x9e\x9c\xe6\x83\xb3\xe5\xaf\xb9\xe5\x85\xb6\xe4\xb8\xad\xe7\x9a\x84\xe6\x9f\x90\xe5\xb1\x82\xe8\xbf\x9b\xe8\xa1\x8c\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xef\xbc\x8c\xe5\x8f\xaf\xe4\xbb\xa5\xe7\x9b\xb4\xe6\x8e\xa5\xe5\x83\x8f Sequential \xe4\xb8\x80\xe6\xa0\xb7\xe5\xaf\xb9\xe5\x85\xb6 Tensor \xe8\xbf\x9b\xe8\xa1\x8c\xe9\x87\x8d\xe6\x96\xb0\xe5\xae\x9a\xe4\xb9\x89\xef\xbc\x8c\xe5\x85\xb6\xe5\x94\xaf\xe4\xb8\x80\xe4\xb8\x8d\xe5\x90\x8c\xe7\x9a\x84\xe5\x9c\xb0\xe6\x96\xb9\xe5\x9c\xa8\xe4\xba\x8e\xef\xbc\x8c\xe5\xa6\x82\xe6\x9e\x9c\xe8\xa6\x81\xe7\x94\xa8\xe5\xbe\xaa\xe7\x8e\xaf\xe7\x9a\x84\xe6\x96\xb9\xe5\xbc\x8f\xe8\xae\xbf\xe9\x97\xae\xef\xbc\x8c\xe9\x9c\x80\xe8\xa6\x81\xe4\xbb\x8b\xe7\xbb\x8d\xe4\xb8\xa4\xe4\xb8\xaa\xe5\xb1\x9e\xe6\x80\xa7\xef\xbc\x8cchildren \xe5\x92\x8c modules\xef\xbc\x8c\xe4\xb8\x8b\xe9\x9d\xa2\xe6\x88\x91\xe4\xbb\xac\xe4\xb8\xbe\xe4\xbe\x8b\xe6\x9d\xa5\xe8\xaf\xb4\xe6\x98\x8e\n\n#%%\nclass sim_net(nn.Module):\n    def __init__(self):\n        super(sim_net, self).__init__()\n        self.l1 = nn.Sequential(\n            nn.Linear(30, 40),\n            nn.ReLU()\n        )\n        \n        self.l1[0].weight.data = torch.randn(40, 30) # \xe7\x9b\xb4\xe6\x8e\xa5\xe5\xaf\xb9\xe6\x9f\x90\xe4\xb8\x80\xe5\xb1\x82\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\n        \n        self.l2 = nn.Sequential(\n            nn.Linear(40, 50),\n            nn.ReLU()\n        )\n        \n        self.l3 = nn.Sequential(\n            nn.Linear(50, 10),\n            nn.ReLU()\n        )\n    \n    def forward(self, x):\n        x = self.l1(x)\n        x = self.l2(x)\n        x = self.l3(x)\n        return x\n\n\n#%%\nnet2 = sim_net()\n\n\n#%%\n# \xe8\xae\xbf\xe9\x97\xae children\nfor i in net2.children():\n    print(i)\n\n\n#%%\n# \xe8\xae\xbf\xe9\x97\xae modules\nfor i in net2.modules():\n    print(i)\n\n#%% [markdown]\n# \xe9\x80\x9a\xe8\xbf\x87\xe4\xb8\x8a\xe9\x9d\xa2\xe7\x9a\x84\xe4\xbe\x8b\xe5\xad\x90\xef\xbc\x8c\xe7\x9c\x8b\xe5\x88\xb0\xe5\x8c\xba\xe5\x88\xab\xe4\xba\x86\xe5\x90\x97?\n# \n# children \xe5\x8f\xaa\xe4\xbc\x9a\xe8\xae\xbf\xe9\x97\xae\xe5\x88\xb0\xe6\xa8\xa1\xe5\x9e\x8b\xe5\xae\x9a\xe4\xb9\x89\xe4\xb8\xad\xe7\x9a\x84\xe7\xac\xac\xe4\xb8\x80\xe5\xb1\x82\xef\xbc\x8c\xe5\x9b\xa0\xe4\xb8\xba\xe4\xb8\x8a\xe9\x9d\xa2\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\xe4\xb8\xad\xe5\xae\x9a\xe4\xb9\x89\xe4\xba\x86\xe4\xb8\x89\xe4\xb8\xaa Sequential\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe5\x8f\xaa\xe4\xbc\x9a\xe8\xae\xbf\xe9\x97\xae\xe5\x88\xb0\xe4\xb8\x89\xe4\xb8\xaa Sequential\xef\xbc\x8c\xe8\x80\x8c modules \xe4\xbc\x9a\xe8\xae\xbf\xe9\x97\xae\xe5\x88\xb0\xe6\x9c\x80\xe5\x90\x8e\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x84\xef\xbc\x8c\xe6\xaf\x94\xe5\xa6\x82\xe4\xb8\x8a\xe9\x9d\xa2\xe7\x9a\x84\xe4\xbe\x8b\xe5\xad\x90\xef\xbc\x8cmodules \xe4\xb8\x8d\xe4\xbb\x85\xe8\xae\xbf\xe9\x97\xae\xe5\x88\xb0\xe4\xba\x86 Sequential\xef\xbc\x8c\xe4\xb9\x9f\xe8\xae\xbf\xe9\x97\xae\xe5\x88\xb0\xe4\xba\x86 Sequential \xe9\x87\x8c\xe9\x9d\xa2\xef\xbc\x8c\xe8\xbf\x99\xe5\xb0\xb1\xe5\xaf\xb9\xe6\x88\x91\xe4\xbb\xac\xe5\x81\x9a\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe9\x9d\x9e\xe5\xb8\xb8\xe6\x96\xb9\xe4\xbe\xbf\xef\xbc\x8c\xe6\xaf\x94\xe5\xa6\x82\n\n#%%\nfor layer in net2.modules():\n    if isinstance(layer, nn.Linear):\n        param_shape = layer.weight.shape\n        layer.weight.data = torch.from_numpy(np.random.normal(0, 0.5, size=param_shape)) \n\n#%% [markdown]\n# \xe8\xbf\x99\xe4\xb8\x8a\xe9\x9d\xa2\xe5\xae\x9e\xe7\x8e\xb0\xe4\xba\x86\xe5\x92\x8c Sequential \xe7\x9b\xb8\xe5\x90\x8c\xe7\x9a\x84\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xef\xbc\x8c\xe5\x90\x8c\xe6\xa0\xb7\xe9\x9d\x9e\xe5\xb8\xb8\xe7\xae\x80\xe4\xbe\xbf\n#%% [markdown]\n# ## torch.nn.init\n# \xe5\x9b\xa0\xe4\xb8\xba PyTorch \xe7\x81\xb5\xe6\xb4\xbb\xe7\x9a\x84\xe7\x89\xb9\xe6\x80\xa7\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe5\x8f\xaf\xe4\xbb\xa5\xe7\x9b\xb4\xe6\x8e\xa5\xe5\xaf\xb9 Tensor \xe8\xbf\x9b\xe8\xa1\x8c\xe6\x93\x8d\xe4\xbd\x9c\xe4\xbb\x8e\xe8\x80\x8c\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xef\xbc\x8cPyTorch \xe4\xb9\x9f\xe6\x8f\x90\xe4\xbe\x9b\xe4\xba\x86\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe7\x9a\x84\xe5\x87\xbd\xe6\x95\xb0\xe5\xb8\xae\xe5\x8a\xa9\xe6\x88\x91\xe4\xbb\xac\xe5\xbf\xab\xe9\x80\x9f\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xef\xbc\x8c\xe5\xb0\xb1\xe6\x98\xaf `torch.nn.init`\xef\xbc\x8c\xe5\x85\xb6\xe6\x93\x8d\xe4\xbd\x9c\xe5\xb1\x82\xe9\x9d\xa2\xe4\xbb\x8d\xe7\x84\xb6\xe5\x9c\xa8 Tensor \xe4\xb8\x8a\xef\xbc\x8c\xe4\xb8\x8b\xe9\x9d\xa2\xe6\x88\x91\xe4\xbb\xac\xe4\xb8\xbe\xe4\xbe\x8b\xe8\xaf\xb4\xe6\x98\x8e\n\n#%%\nfrom torch.nn import init\n\n\n#%%\nprint(net1[0].weight)\n\n\n#%%\ninit.xavier_uniform_(net1[0].weight) # \xe8\xbf\x99\xe5\xb0\xb1\xe6\x98\xaf\xe4\xb8\x8a\xe9\x9d\xa2\xe6\x88\x91\xe4\xbb\xac\xe8\xae\xb2\xe8\xbf\x87\xe7\x9a\x84 Xavier \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe6\x96\xb9\xe6\xb3\x95\xef\xbc\x8cPyTorch \xe7\x9b\xb4\xe6\x8e\xa5\xe5\x86\x85\xe7\xbd\xae\xe4\xba\x86\xe5\x85\xb6\xe5\xae\x9e\xe7\x8e\xb0\n\n\n#%%\nprint(net1[0].weight)\n\n#%% [markdown]\n# \xe5\x8f\xaf\xe4\xbb\xa5\xe7\x9c\x8b\xe5\x88\xb0\xe5\x8f\x82\xe6\x95\xb0\xe5\xb7\xb2\xe7\xbb\x8f\xe8\xa2\xab\xe4\xbf\xae\xe6\x94\xb9\xe4\xba\x86\n# \n# `torch.nn.init` \xe4\xb8\xba\xe6\x88\x91\xe4\xbb\xac\xe6\x8f\x90\xe4\xbe\x9b\xe4\xba\x86\xe6\x9b\xb4\xe5\xa4\x9a\xe7\x9a\x84\xe5\x86\x85\xe7\xbd\xae\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe6\x96\xb9\xe5\xbc\x8f\xef\xbc\x8c\xe9\x81\xbf\xe5\x85\x8d\xe4\xba\x86\xe6\x88\x91\xe4\xbb\xac\xe9\x87\x8d\xe5\xa4\x8d\xe5\x8e\xbb\xe5\xae\x9e\xe7\x8e\xb0\xe4\xb8\x80\xe4\xba\x9b\xe7\x9b\xb8\xe5\x90\x8c\xe7\x9a\x84\xe6\x93\x8d\xe4\xbd\x9c\n#%% [markdown]\n# \xe4\xb8\x8a\xe9\x9d\xa2\xe8\xae\xb2\xe4\xba\x86\xe4\xb8\xa4\xe7\xa7\x8d\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe6\x96\xb9\xe5\xbc\x8f\xef\xbc\x8c\xe5\x85\xb6\xe5\xae\x9e\xe5\xae\x83\xe4\xbb\xac\xe7\x9a\x84\xe6\x9c\xac\xe8\xb4\xa8\xe9\x83\xbd\xe6\x98\xaf\xe4\xb8\x80\xe6\xa0\xb7\xe7\x9a\x84\xef\xbc\x8c\xe5\xb0\xb1\xe6\x98\xaf\xe5\x8e\xbb\xe4\xbf\xae\xe6\x94\xb9\xe6\x9f\x90\xe4\xb8\x80\xe5\xb1\x82\xe5\x8f\x82\xe6\x95\xb0\xe7\x9a\x84\xe5\xae\x9e\xe9\x99\x85\xe5\x80\xbc\xef\xbc\x8c\xe8\x80\x8c `torch.nn.init` \xe6\x8f\x90\xe4\xbe\x9b\xe4\xba\x86\xe6\x9b\xb4\xe5\xa4\x9a\xe6\x88\x90\xe7\x86\x9f\xe7\x9a\x84\xe6\xb7\xb1\xe5\xba\xa6\xe5\xad\xa6\xe4\xb9\xa0\xe7\x9b\xb8\xe5\x85\xb3\xe7\x9a\x84\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe6\x96\xb9\xe5\xbc\x8f\xef\xbc\x8c\xe9\x9d\x9e\xe5\xb8\xb8\xe6\x96\xb9\xe4\xbe\xbf\n\n'"
7.Convolutional_Neural_Network(CNN) 卷积神经网络/CNN 卷积神经网络.py,16,"b""#%% [markdown]\n# # \xe5\x8d\xb7\xe7\xa7\xaf\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\n\n#%%\nimport torch\nimport torch.nn as nn\n\n#%% [markdown]\n# \xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82\n\n#%%\nconv = torch.nn.Conv2d(16, 33, kernel_size=(3, 5), \n        stride=(2, 1), padding=(4, 2), dilation=(3, 1))\ninputs = torch.randn(20, 16, 50, 100)\noutputs = conv(inputs)\noutputs.size()\n\n#%% [markdown]\n# \xe6\xb1\xa0\xe5\x8c\x96\xe5\xb1\x82\n\n#%%\npool = nn.MaxPool1d(kernel_size=2, stride=2)\ninputs = torch.randn(20, 16, 100)\noutputs = pool(inputs)\noutputs.size()\n\n\n#%%\ninputs = torch.tensor([[[1, 2, 3, 4, 5]]], dtype=torch.float32)\npool = nn.MaxPool1d(kernel_size=2, stride=2, return_indices=True)\noutputs, indices = pool(inputs)\nunpool = nn.MaxUnpool1d(kernel_size=2, stride=2)\nrecovers = unpool(outputs, indices)\nrecovers\n\n\n#%%\ninputs = torch.tensor([[[1, 2, 3, 4, 5]]], dtype=torch.float32)\npool = nn.MaxPool1d(kernel_size=2, stride=2, return_indices=True)\noutputs, indices = pool(inputs)\nunpool = nn.MaxUnpool1d(kernel_size=2, stride=2)\nrecovers = unpool(outputs, indices, output_size=inputs.size())\nrecovers\n\n\n#%%\npool = nn.MaxPool1d(2, stride=2, return_indices=True)\nunpool = nn.MaxUnpool1d(2, stride=2)\ninputs = torch.tensor([[[1, 2, 3, 4, 5, 6, 7, 8]]], dtype=torch.float32)\noutputs, indices = pool(inputs)\nunpool(outputs, indices)\n\n#%% [markdown]\n# \xe4\xb8\x8a\xe9\x87\x87\xe6\xa0\xb7\xe5\xb1\x82\n\n#%%\ninputs = torch.tensor([[[1, 0, 2]]], dtype=torch.float32)\n# m = nn.Upsample(scale_factor=3, mode='nearest')\nm = nn.Upsample(scale_factor=3, mode='linear')\ninputs, m(inputs)\n\n\n#%%\ninputs = torch.tensor([[[1, 0, 2]]], dtype=torch.float32)\n# m = nn.Upsample(size=9, mode='nearest')\nm = nn.Upsample(size=8, mode='linear')\ninputs, m(inputs), m(inputs) * 7\n\n\n#%%\ninputs = torch.arange(0, 4).reshape(1, 1, 4)\ninputs = torch.tensor([[[1, 0, 2]]], dtype=torch.float32)\n# m = nn.Upsample(scale_factor=2, mode='nearest')\nm = nn.Upsample(scale_factor=2, mode='linear')\ninputs, m(inputs)\n\n\n#%%\ninputs = torch.arange(1, 5).view(1, 1, 2, 2)\nupsample = nn.Upsample(scale_factor=2, mode='nearest')\nupsample(inputs)\n\n#%% [markdown]\n# \xe8\xa1\xa5\xe5\x85\xa8\xe5\xb1\x82\n\n#%%\ninputs = torch.arange(25).view(1, 1, 5, 5)\npad = nn.ConstantPad2d([1, 1, 1, 1], value=-1)\noutputs = pad(inputs)\ninputs, outputs\n\n\n#%%\ninputs = torch.arange(25).view(1, 1, 5, 5)\npad = nn.ReplicationPad2d([1, 1, 1, 1])\noutputs = pad(inputs)\ninputs, outputs\n\n\n#%%\ninputs = torch.arange(25).view(1, 1, 5, 5)\npad = nn.ReflectionPad2d([1, 1, 1, 1])\noutputs = pad(inputs)\ninputs, outputs\n\n\n#%%\ninputs = torch.arange(12).view(1, 1, 3, 4)\npad = nn.ConstantPad2d(padding=[1, 1, 1, 1], value=-1)\nprint ('\xe5\xb8\xb8\xe6\x95\xb0\xe8\xa1\xa5\xe5\x85\xa8 = {}'.format(pad(inputs)))\npad = nn.ReplicationPad2d(padding=[1, 1, 1, 1])\nprint ('\xe9\x87\x8d\xe5\xa4\x8d\xe8\xa1\xa5\xe5\x85\xa8 = {}'.format(pad(inputs)))\npad = nn.ReflectionPad2d(padding=[1, 1, 1, 1])\nprint ('\xe5\x8f\x8d\xe5\xb0\x84\xe8\xa1\xa5\xe5\x85\xa8 = {}'.format(pad(inputs)))\n\n\n"""
7.Convolutional_Neural_Network(CNN) 卷积神经网络/Lenet5-and-ResNet18.py,4,"b""import  torch\nfrom    torch.utils.data import DataLoader\nfrom    torchvision import datasets\nfrom    torchvision import transforms\nfrom    torch import nn, optim\n\nfrom    lenet5 import Lenet5\nfrom    resnet import ResNet18\n\ndef main():\n    batchsz = 32\n\n    cifar_train = datasets.CIFAR10('cifar', True, transform=transforms.Compose([\n        transforms.Resize((32, 32)),\n        transforms.ToTensor()\n    ]), download=True)\n    cifar_train = DataLoader(cifar_train, batch_size=batchsz, shuffle=True)\n\n    cifar_test = datasets.CIFAR10('cifar', False, transform=transforms.Compose([\n        transforms.Resize((32, 32)),\n        transforms.ToTensor()\n    ]), download=True)\n    cifar_test = DataLoader(cifar_test, batch_size=batchsz, shuffle=True)\n\n\n    x, label = iter(cifar_train).next()\n    print('x:', x.shape, 'label:', label.shape)\n\n    device = torch.device('cuda')\n    # model = Lenet5().to(device)\n    model = ResNet18().to(device)\n\n    criteon = nn.CrossEntropyLoss().to(device)\n    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n    print(model)\n\n    for epoch in range(1000):\n\n        model.train()\n        for batchidx, (x, label) in enumerate(cifar_train):\n            # [b, 3, 32, 32]\n            # [b]\n            x, label = x.to(device), label.to(device)\n\n            logits = model(x)\n            # logits: [b, 10]\n            # label:  [b]\n            # loss: tensor scalar\n            loss = criteon(logits, label)\n\n            # backprop\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n\n        #\n        print(epoch, 'loss:', loss.item())\n\n\n        model.eval()\n        with torch.no_grad():\n            # test\n            total_correct = 0\n            total_num = 0\n            for x, label in cifar_test:\n                # [b, 3, 32, 32]\n                # [b]\n                x, label = x.to(device), label.to(device)\n\n                # [b, 10]\n                logits = model(x)\n                # [b]\n                pred = logits.argmax(dim=1)\n                # [b] vs [b] => scalar tensor\n                correct = torch.eq(pred, label).float().sum().item()\n                total_correct += correct\n                total_num += x.size(0)\n                # print(correct)\n\n            acc = total_correct / total_num\n            print(epoch, 'acc:', acc)\n\n\n\nif __name__ == '__main__':\n    main()\n"""
7.Convolutional_Neural_Network(CNN) 卷积神经网络/MNIST-using-CNN.py,20,"b""#%% [markdown]\n# # \xe5\x8d\xb7\xe7\xa7\xaf\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\n\n#%%\nimport torch\nimport torch.utils.data\nimport torch.nn\nimport torch.optim\nimport torchvision.datasets\nimport torchvision.transforms\n\n# \xe6\x95\xb0\xe6\x8d\xae\xe8\xaf\xbb\xe5\x8f\x96\ntrain_dataset = torchvision.datasets.MNIST(root='./data/mnist',\n        train=True, transform=torchvision.transforms.ToTensor(),\n        download=True)\ntest_dataset = torchvision.datasets.MNIST(root='./data/mnist',\n        train=False, transform=torchvision.transforms.ToTensor(),\n        download=True)\n\nbatch_size = 100\ntrain_loader = torch.utils.data.DataLoader(\n        dataset=train_dataset, batch_size=batch_size)\ntest_loader = torch.utils.data.DataLoader(\n        dataset=test_dataset, batch_size=batch_size)\n\n# \xe6\x90\xad\xe5\xbb\xba\xe7\xbd\x91\xe7\xbb\x9c\xe7\xbb\x93\xe6\x9e\x84\nclass Net(torch.nn.Module):\n    \n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = torch.nn.Sequential(\n                torch.nn.Conv2d(1, 64, kernel_size=3, padding=1),\n                torch.nn.ReLU(),\n                torch.nn.Conv2d(64, 128, kernel_size=3, padding=1),\n                torch.nn.ReLU(),\n                torch.nn.MaxPool2d(stride=2, kernel_size=2))\n        self.dense = torch.nn.Sequential(\n                torch.nn.Linear(128 * 14 * 14, 1024),\n                torch.nn.ReLU(),\n                torch.nn.Dropout(p=0.5),\n                torch.nn.Linear(1024, 10))\n        \n    def forward(self, x):\n        x = self.conv1(x)\n        x = x.view(-1, 128 * 14 * 14)\n        x = self.dense(x)\n        return x\n\nnet = Net()\n\ncriterion = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(net.parameters()) \n\n# \xe8\xae\xad\xe7\xbb\x83\nnum_epochs = 5\nfor epoch in range(num_epochs):\n    for idx, (images, labels) in enumerate(train_loader):\n        optimizer.zero_grad()\n        preds = net(images)\n        loss = criterion(preds, labels)\n        loss.backward()\n        optimizer.step()\n        \n        if idx % 100 == 0:\n            print('epoch {}, batch {}, loss = {:g}'.format(\n                    epoch, idx, loss.item()))\n\n# \xe6\xb5\x8b\xe8\xaf\x95\ncorrect = 0\ntotal = 0\nfor images, labels in test_loader:\n    preds = net(images)\n    predicted = torch.argmax(preds, 1)\n    total += labels.size(0)\n    correct += (predicted == labels).sum().item()\n    \naccuracy = correct / total\nprint('test acc: {:.1%}'.format(accuracy))\n\n\n"""
7.Convolutional_Neural_Network(CNN) 卷积神经网络/basic_conv 卷积模块.py,8,"b""#%% [markdown]\n# # \xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa8\xa1\xe5\x9d\x97\xe4\xbb\x8b\xe7\xbb\x8d\n# \xe5\x89\x8d\xe9\x9d\xa2\xe6\x88\x91\xe4\xbb\xac\xe4\xbb\x8b\xe7\xbb\x8d\xe4\xba\x86\xe5\x8d\xb7\xe7\xa7\xaf\xe7\xbd\x91\xe7\xbb\x9c\xe7\x9a\x84\xe5\x9f\xba\xe6\x9c\xac\xe7\x9f\xa5\xe8\xaf\x86\xef\xbc\x8c\xe5\x85\xb6\xe5\x9c\xa8\xe8\xae\xa1\xe7\xae\x97\xe6\x9c\xba\xe8\xa7\x86\xe8\xa7\x89\xe9\xa2\x86\xe5\x9f\x9f\xe8\xa2\xab\xe5\xba\x94\xe7\x94\xa8\xe5\xbe\x97\xe9\x9d\x9e\xe5\xb8\xb8\xe5\xb9\xbf\xe6\xb3\x9b\xef\xbc\x8c\xe9\x82\xa3\xe4\xb9\x88\xe5\xb8\xb8\xe8\xa7\x81\xe7\x9a\x84\xe5\x8d\xb7\xe6\x9c\xba\xe7\xbd\x91\xe7\xbb\x9c\xe4\xb8\xad\xe7\x94\xa8\xe5\x88\xb0\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9d\x97\xe8\x83\xbd\xe5\xa4\x9f\xe4\xbd\xbf\xe7\x94\xa8 pytorch \xe9\x9d\x9e\xe5\xb8\xb8\xe8\xbd\xbb\xe6\x9d\xbe\xe5\x9c\xb0\xe5\xae\x9e\xe7\x8e\xb0\xef\xbc\x8c\xe4\xb8\x8b\xe9\x9d\xa2\xe6\x88\x91\xe4\xbb\xac\xe6\x9d\xa5\xe8\xae\xb2\xe4\xb8\x80\xe4\xb8\x8b pytorch \xe4\xb8\xad\xe7\x9a\x84\xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa8\xa1\xe5\x9d\x97\n#%% [markdown]\n# ## \xe5\x8d\xb7\xe7\xa7\xaf\n# \xe5\x8d\xb7\xe7\xa7\xaf\xe5\x9c\xa8 pytorch \xe4\xb8\xad\xe6\x9c\x89\xe4\xb8\xa4\xe7\xa7\x8d\xe6\x96\xb9\xe5\xbc\x8f\xef\xbc\x8c\xe4\xb8\x80\xe7\xa7\x8d\xe6\x98\xaf `torch.nn.Conv2d()`\xef\xbc\x8c\xe4\xb8\x80\xe7\xa7\x8d\xe6\x98\xaf `torch.nn.functional.conv2d()`\xef\xbc\x8c\xe8\xbf\x99\xe4\xb8\xa4\xe7\xa7\x8d\xe5\xbd\xa2\xe5\xbc\x8f\xe6\x9c\xac\xe8\xb4\xa8\xe9\x83\xbd\xe6\x98\xaf\xe4\xbd\xbf\xe7\x94\xa8\xe4\xb8\x80\xe4\xb8\xaa\xe5\x8d\xb7\xe7\xa7\xaf\xe6\x93\x8d\xe4\xbd\x9c\n# \n# \xe8\xbf\x99\xe4\xb8\xa4\xe7\xa7\x8d\xe5\xbd\xa2\xe5\xbc\x8f\xe7\x9a\x84\xe5\x8d\xb7\xe7\xa7\xaf\xe5\xaf\xb9\xe4\xba\x8e\xe8\xbe\x93\xe5\x85\xa5\xe7\x9a\x84\xe8\xa6\x81\xe6\xb1\x82\xe9\x83\xbd\xe6\x98\xaf\xe4\xb8\x80\xe6\xa0\xb7\xe7\x9a\x84\xef\xbc\x8c\xe9\xa6\x96\xe5\x85\x88\xe9\x9c\x80\xe8\xa6\x81\xe8\xbe\x93\xe5\x85\xa5\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa `torch.autograd.Variable()` \xe7\x9a\x84\xe7\xb1\xbb\xe5\x9e\x8b\xef\xbc\x8c\xe5\xa4\xa7\xe5\xb0\x8f\xe6\x98\xaf (batch, channel, H, W)\xef\xbc\x8c\xe5\x85\xb6\xe4\xb8\xad batch \xe8\xa1\xa8\xe7\xa4\xba\xe8\xbe\x93\xe5\x85\xa5\xe7\x9a\x84\xe4\xb8\x80\xe6\x89\xb9\xe6\x95\xb0\xe6\x8d\xae\xe7\x9a\x84\xe6\x95\xb0\xe7\x9b\xae\xef\xbc\x8c\xe7\xac\xac\xe4\xba\x8c\xe4\xb8\xaa\xe6\x98\xaf\xe8\xbe\x93\xe5\x85\xa5\xe7\x9a\x84\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\xef\xbc\x8c\xe4\xb8\x80\xe8\x88\xac\xe4\xb8\x80\xe5\xbc\xa0\xe5\xbd\xa9\xe8\x89\xb2\xe7\x9a\x84\xe5\x9b\xbe\xe7\x89\x87\xe6\x98\xaf 3\xef\xbc\x8c\xe7\x81\xb0\xe5\xba\xa6\xe5\x9b\xbe\xe6\x98\xaf 1\xef\xbc\x8c\xe8\x80\x8c\xe5\x8d\xb7\xe7\xa7\xaf\xe7\xbd\x91\xe7\xbb\x9c\xe8\xbf\x87\xe7\xa8\x8b\xe4\xb8\xad\xe7\x9a\x84\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\xe6\xaf\x94\xe8\xbe\x83\xe5\xa4\xa7\xef\xbc\x8c\xe4\xbc\x9a\xe5\x87\xba\xe7\x8e\xb0\xe5\x87\xa0\xe5\x8d\x81\xe5\x88\xb0\xe5\x87\xa0\xe7\x99\xbe\xe7\x9a\x84\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\xef\xbc\x8cH \xe5\x92\x8c W \xe8\xa1\xa8\xe7\xa4\xba\xe8\xbe\x93\xe5\x85\xa5\xe5\x9b\xbe\xe7\x89\x87\xe7\x9a\x84\xe9\xab\x98\xe5\xba\xa6\xe5\x92\x8c\xe5\xae\xbd\xe5\xba\xa6\xef\xbc\x8c\xe6\xaf\x94\xe5\xa6\x82\xe4\xb8\x80\xe4\xb8\xaa batch \xe6\x98\xaf 32 \xe5\xbc\xa0\xe5\x9b\xbe\xe7\x89\x87\xef\xbc\x8c\xe6\xaf\x8f\xe5\xbc\xa0\xe5\x9b\xbe\xe7\x89\x87\xe6\x98\xaf 3 \xe9\x80\x9a\xe9\x81\x93\xef\xbc\x8c\xe9\xab\x98\xe5\x92\x8c\xe5\xae\xbd\xe5\x88\x86\xe5\x88\xab\xe6\x98\xaf 50 \xe5\x92\x8c 100\xef\xbc\x8c\xe9\x82\xa3\xe4\xb9\x88\xe8\xbe\x93\xe5\x85\xa5\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f\xe5\xb0\xb1\xe6\x98\xaf (32, 3, 50, 100)\n# \n# \xe4\xb8\x8b\xe9\x9d\xa2\xe4\xb8\xbe\xe4\xbe\x8b\xe6\x9d\xa5\xe8\xaf\xb4\xe6\x98\x8e\xe4\xb8\x80\xe4\xb8\x8b\xe8\xbf\x99\xe4\xb8\xa4\xe7\xa7\x8d\xe5\x8d\xb7\xe7\xa7\xaf\xe6\x96\xb9\xe5\xbc\x8f\n\n#%%\nimport numpy as np\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nget_ipython().run_line_magic('matplotlib', 'inline')\n\n\n#%%\nim = Image.open('./cat.png').convert('L') # \xe8\xaf\xbb\xe5\x85\xa5\xe4\xb8\x80\xe5\xbc\xa0\xe7\x81\xb0\xe5\xba\xa6\xe5\x9b\xbe\xe7\x9a\x84\xe5\x9b\xbe\xe7\x89\x87\nim = np.array(im, dtype='float32') # \xe5\xb0\x86\xe5\x85\xb6\xe8\xbd\xac\xe6\x8d\xa2\xe4\xb8\xba\xe4\xb8\x80\xe4\xb8\xaa\xe7\x9f\xa9\xe9\x98\xb5\n\n\n#%%\n# \xe5\x8f\xaf\xe8\xa7\x86\xe5\x8c\x96\xe5\x9b\xbe\xe7\x89\x87\nplt.imshow(im.astype('uint8'), cmap='gray')\n\n\n#%%\n# \xe5\xb0\x86\xe5\x9b\xbe\xe7\x89\x87\xe7\x9f\xa9\xe9\x98\xb5\xe8\xbd\xac\xe5\x8c\x96\xe4\xb8\xba pytorch tensor\xef\xbc\x8c\xe5\xb9\xb6\xe9\x80\x82\xe9\x85\x8d\xe5\x8d\xb7\xe7\xa7\xaf\xe8\xbe\x93\xe5\x85\xa5\xe7\x9a\x84\xe8\xa6\x81\xe6\xb1\x82\nim = torch.from_numpy(im.reshape((1, 1, im.shape[0], im.shape[1]))) \n\n\n#%%\nim.shape\n\n#%% [markdown]\n# \xe4\xb8\x8b\xe9\x9d\xa2\xe6\x88\x91\xe4\xbb\xac\xe5\xae\x9a\xe4\xb9\x89\xe4\xb8\x80\xe4\xb8\xaa\xe7\xae\x97\xe5\xad\x90\xe5\xaf\xb9\xe5\x85\xb6\xe8\xbf\x9b\xe8\xa1\x8c\xe8\xbd\xae\xe5\xbb\x93\xe6\xa3\x80\xe6\xb5\x8b\n\n#%%\n# \xe4\xbd\xbf\xe7\x94\xa8 nn.Conv2d\nconv1 = nn.Conv2d(1, 1, 3, bias=False) # \xe5\xae\x9a\xe4\xb9\x89\xe5\x8d\xb7\xe7\xa7\xaf\n\nsobel_kernel = np.array([\n                        [-1, -1, -1], \n                        [-1, 8, -1], \n                        [-1, -1, -1]], dtype='float32') # \xe5\xae\x9a\xe4\xb9\x89\xe8\xbd\xae\xe5\xbb\x93\xe6\xa3\x80\xe6\xb5\x8b\xe7\xae\x97\xe5\xad\x90\nsobel_kernel = sobel_kernel.reshape((1, 1, 3, 3)) # \xe9\x80\x82\xe9\x85\x8d\xe5\x8d\xb7\xe7\xa7\xaf\xe7\x9a\x84\xe8\xbe\x93\xe5\x85\xa5\xe8\xbe\x93\xe5\x87\xba\nconv1.weight.data = torch.from_numpy(sobel_kernel) # \xe7\xbb\x99\xe5\x8d\xb7\xe7\xa7\xaf\xe7\x9a\x84 kernel \xe8\xb5\x8b\xe5\x80\xbc\n\nedge1 = conv1(im) # \xe4\xbd\x9c\xe7\x94\xa8\xe5\x9c\xa8\xe5\x9b\xbe\xe7\x89\x87\xe4\xb8\x8a\nprint(edge1)\nedge1 = edge1.data.squeeze().numpy() # \xe5\xb0\x86\xe8\xbe\x93\xe5\x87\xba\xe8\xbd\xac\xe6\x8d\xa2\xe4\xb8\xba\xe5\x9b\xbe\xe7\x89\x87\xe7\x9a\x84\xe6\xa0\xbc\xe5\xbc\x8f\n# edge1 = edge1.detach().squeeze().numpy() # \xe5\xb0\x86\xe8\xbe\x93\xe5\x87\xba\xe8\xbd\xac\xe6\x8d\xa2\xe4\xb8\xba\xe5\x9b\xbe\xe7\x89\x87\xe7\x9a\x84\xe6\xa0\xbc\xe5\xbc\x8f\n\n#%% [markdown]\n# \xe4\xb8\x8b\xe9\x9d\xa2\xe6\x88\x91\xe4\xbb\xac\xe5\x8f\xaf\xe8\xa7\x86\xe5\x8c\x96\xe8\xbe\xb9\xe7\xbc\x98\xe6\xa3\x80\xe6\xb5\x8b\xe4\xb9\x8b\xe5\x90\x8e\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x9c\n\n#%%\nplt.imshow(edge1, cmap='gray')\n\n\n#%%\n# \xe4\xbd\xbf\xe7\x94\xa8 F.conv2d\nsobel_kernel = np.array([\n                        [-1, -1, -1], \n                        [-1, 8, -1], \n                        [-1, -1, -1]], dtype='float32') # \xe5\xae\x9a\xe4\xb9\x89\xe8\xbd\xae\xe5\xbb\x93\xe6\xa3\x80\xe6\xb5\x8b\xe7\xae\x97\xe5\xad\x90\nsobel_kernel = sobel_kernel.reshape((1, 1, 3, 3)) # \xe9\x80\x82\xe9\x85\x8d\xe5\x8d\xb7\xe7\xa7\xaf\xe7\x9a\x84\xe8\xbe\x93\xe5\x85\xa5\xe8\xbe\x93\xe5\x87\xba\nweight = torch.from_numpy(sobel_kernel)\n\nedge2 = F.conv2d(im, weight) # \xe4\xbd\x9c\xe7\x94\xa8\xe5\x9c\xa8\xe5\x9b\xbe\xe7\x89\x87\xe4\xb8\x8a\nedge2 = edge2.data.squeeze().numpy() # \xe5\xb0\x86\xe8\xbe\x93\xe5\x87\xba\xe8\xbd\xac\xe6\x8d\xa2\xe4\xb8\xba\xe5\x9b\xbe\xe7\x89\x87\xe7\x9a\x84\xe6\xa0\xbc\xe5\xbc\x8f\nplt.imshow(edge2, cmap='gray')\n\n#%% [markdown]\n# \xe5\x8f\xaf\xe4\xbb\xa5\xe7\x9c\x8b\xe5\x88\xb0\xe4\xb8\xa4\xe7\xa7\x8d\xe5\xbd\xa2\xe5\xbc\x8f\xe8\x83\xbd\xe5\xa4\x9f\xe5\xbe\x97\xe5\x88\xb0\xe7\x9b\xb8\xe5\x90\x8c\xe7\x9a\x84\xe6\x95\x88\xe6\x9e\x9c\xef\xbc\x8c\xe4\xb8\x8d\xe5\x90\x8c\xe7\x9a\x84\xe5\x9c\xb0\xe6\x96\xb9\xe7\x9b\xb8\xe4\xbf\xa1\xe4\xbd\xa0\xe4\xb9\x9f\xe7\x9c\x8b\xe5\x88\xb0\xe4\xba\x86\xef\xbc\x8c\xe4\xbd\xbf\xe7\x94\xa8 `nn.Conv2d()` \xe7\x9b\xb8\xe5\xbd\x93\xe4\xba\x8e\xe7\x9b\xb4\xe6\x8e\xa5\xe5\xae\x9a\xe4\xb9\x89\xe4\xba\x86\xe4\xb8\x80\xe5\xb1\x82\xe5\x8d\xb7\xe7\xa7\xaf\xe7\xbd\x91\xe7\xbb\x9c\xe7\xbb\x93\xe6\x9e\x84\xef\xbc\x8c\xe8\x80\x8c\xe4\xbd\xbf\xe7\x94\xa8 `torch.nn.functional.conv2d()` \xe7\x9b\xb8\xe5\xbd\x93\xe4\xba\x8e\xe5\xae\x9a\xe4\xb9\x89\xe4\xba\x86\xe4\xb8\x80\xe4\xb8\xaa\xe5\x8d\xb7\xe7\xa7\xaf\xe7\x9a\x84\xe6\x93\x8d\xe4\xbd\x9c\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe4\xbd\xbf\xe7\x94\xa8\xe5\x90\x8e\xe8\x80\x85\xe9\x9c\x80\xe8\xa6\x81\xe5\x86\x8d\xe9\xa2\x9d\xe5\xa4\x96\xe5\x8e\xbb\xe5\xae\x9a\xe4\xb9\x89\xe4\xb8\x80\xe4\xb8\xaa weight\xef\xbc\x8c\xe8\x80\x8c\xe4\xb8\x94\xe8\xbf\x99\xe4\xb8\xaa weight \xe4\xb9\x9f\xe5\xbf\x85\xe9\xa1\xbb\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa Variable\xef\xbc\x8c\xe8\x80\x8c\xe4\xbd\xbf\xe7\x94\xa8 `nn.Conv2d()` \xe5\x88\x99\xe4\xbc\x9a\xe5\xb8\xae\xe6\x88\x91\xe4\xbb\xac\xe9\xbb\x98\xe8\xae\xa4\xe5\xae\x9a\xe4\xb9\x89\xe4\xb8\x80\xe4\xb8\xaa\xe9\x9a\x8f\xe6\x9c\xba\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe7\x9a\x84 weight\xef\xbc\x8c\xe5\xa6\x82\xe6\x9e\x9c\xe6\x88\x91\xe4\xbb\xac\xe9\x9c\x80\xe8\xa6\x81\xe4\xbf\xae\xe6\x94\xb9\xef\xbc\x8c\xe9\x82\xa3\xe4\xb9\x88\xe5\x8f\x96\xe5\x87\xba\xe5\x85\xb6\xe4\xb8\xad\xe7\x9a\x84\xe5\x80\xbc\xe5\xaf\xb9\xe5\x85\xb6\xe4\xbf\xae\xe6\x94\xb9\xef\xbc\x8c\xe5\xa6\x82\xe6\x9e\x9c\xe4\xb8\x8d\xe6\x83\xb3\xe4\xbf\xae\xe6\x94\xb9\xef\xbc\x8c\xe9\x82\xa3\xe4\xb9\x88\xe5\x8f\xaf\xe4\xbb\xa5\xe7\x9b\xb4\xe6\x8e\xa5\xe4\xbd\xbf\xe7\x94\xa8\xe8\xbf\x99\xe4\xb8\xaa\xe9\xbb\x98\xe8\xae\xa4\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe7\x9a\x84\xe5\x80\xbc\xef\xbc\x8c\xe9\x9d\x9e\xe5\xb8\xb8\xe6\x96\xb9\xe4\xbe\xbf\n# \n# **\xe5\xae\x9e\xe9\x99\x85\xe4\xbd\xbf\xe7\x94\xa8\xe4\xb8\xad\xe6\x88\x91\xe4\xbb\xac\xe5\x9f\xba\xe6\x9c\xac\xe9\x83\xbd\xe4\xbd\xbf\xe7\x94\xa8 `nn.Conv2d()` \xe8\xbf\x99\xe7\xa7\x8d\xe5\xbd\xa2\xe5\xbc\x8f**\n#%% [markdown]\n# ## \xe6\xb1\xa0\xe5\x8c\x96\xe5\xb1\x82\n# \xe5\x8d\xb7\xe7\xa7\xaf\xe7\xbd\x91\xe7\xbb\x9c\xe4\xb8\xad\xe5\x8f\xa6\xe5\xa4\x96\xe4\xb8\x80\xe4\xb8\xaa\xe9\x9d\x9e\xe5\xb8\xb8\xe9\x87\x8d\xe8\xa6\x81\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x84\xe5\xb0\xb1\xe6\x98\xaf\xe6\xb1\xa0\xe5\x8c\x96\xef\xbc\x8c\xe8\xbf\x99\xe6\x98\xaf\xe5\x88\xa9\xe7\x94\xa8\xe4\xba\x86\xe5\x9b\xbe\xe7\x89\x87\xe7\x9a\x84\xe4\xb8\x8b\xe9\x87\x87\xe6\xa0\xb7\xe4\xb8\x8d\xe5\x8f\x98\xe6\x80\xa7\xef\xbc\x8c\xe5\x8d\xb3\xe4\xb8\x80\xe5\xbc\xa0\xe5\x9b\xbe\xe7\x89\x87\xe5\x8f\x98\xe5\xb0\x8f\xe4\xba\x86\xe8\xbf\x98\xe6\x98\xaf\xe8\x83\xbd\xe5\xa4\x9f\xe7\x9c\x8b\xe5\x87\xba\xe4\xba\x86\xe8\xbf\x99\xe5\xbc\xa0\xe5\x9b\xbe\xe7\x89\x87\xe7\x9a\x84\xe5\x86\x85\xe5\xae\xb9\xef\xbc\x8c\xe8\x80\x8c\xe4\xbd\xbf\xe7\x94\xa8\xe6\xb1\xa0\xe5\x8c\x96\xe5\xb1\x82\xe8\x83\xbd\xe5\xa4\x9f\xe5\xb0\x86\xe5\x9b\xbe\xe7\x89\x87\xe5\xa4\xa7\xe5\xb0\x8f\xe9\x99\x8d\xe4\xbd\x8e\xef\xbc\x8c\xe9\x9d\x9e\xe5\xb8\xb8\xe5\xa5\xbd\xe5\x9c\xb0\xe6\x8f\x90\xe9\xab\x98\xe4\xba\x86\xe8\xae\xa1\xe7\xae\x97\xe6\x95\x88\xe7\x8e\x87\xef\xbc\x8c\xe5\x90\x8c\xe6\x97\xb6\xe6\xb1\xa0\xe5\x8c\x96\xe5\xb1\x82\xe4\xb9\x9f\xe6\xb2\xa1\xe6\x9c\x89\xe5\x8f\x82\xe6\x95\xb0\xe3\x80\x82\xe6\xb1\xa0\xe5\x8c\x96\xe7\x9a\x84\xe6\x96\xb9\xe5\xbc\x8f\xe6\x9c\x89\xe5\xbe\x88\xe5\xa4\x9a\xe7\xa7\x8d\xef\xbc\x8c\xe6\xaf\x94\xe5\xa6\x82\xe6\x9c\x80\xe5\xa4\xa7\xe5\x80\xbc\xe6\xb1\xa0\xe5\x8c\x96\xef\xbc\x8c\xe5\x9d\x87\xe5\x80\xbc\xe6\xb1\xa0\xe5\x8c\x96\xe7\xad\x89\xe7\xad\x89\xef\xbc\x8c\xe5\x9c\xa8\xe5\x8d\xb7\xe7\xa7\xaf\xe7\xbd\x91\xe7\xbb\x9c\xe4\xb8\xad\xe4\xb8\x80\xe8\x88\xac\xe4\xbd\xbf\xe7\x94\xa8\xe6\x9c\x80\xe5\xa4\xa7\xe5\x80\xbc\xe6\xb1\xa0\xe5\x8c\x96\xe3\x80\x82\n# \n# \xe5\x9c\xa8 pytorch \xe4\xb8\xad\xe6\x9c\x80\xe5\xa4\xa7\xe5\x80\xbc\xe6\xb1\xa0\xe5\x8c\x96\xe7\x9a\x84\xe6\x96\xb9\xe5\xbc\x8f\xe4\xb9\x9f\xe6\x9c\x89\xe4\xb8\xa4\xe7\xa7\x8d\xef\xbc\x8c\xe4\xb8\x80\xe7\xa7\x8d\xe6\x98\xaf `nn.MaxPool2d()`\xef\xbc\x8c\xe4\xb8\x80\xe7\xa7\x8d\xe6\x98\xaf `torch.nn.functional.max_pool2d()`\xef\xbc\x8c\xe4\xbb\x96\xe4\xbb\xac\xe5\xaf\xb9\xe4\xba\x8e\xe5\x9b\xbe\xe7\x89\x87\xe7\x9a\x84\xe8\xbe\x93\xe5\x85\xa5\xe8\xa6\x81\xe6\xb1\x82\xe8\xb7\x9f\xe5\x8d\xb7\xe7\xa7\xaf\xe5\xaf\xb9\xe4\xba\x8e\xe5\x9b\xbe\xe7\x89\x87\xe7\x9a\x84\xe8\xbe\x93\xe5\x85\xa5\xe8\xa6\x81\xe6\xb1\x82\xe6\x98\xaf\xe4\xb8\x80\xe6\xa0\xb7\xe4\xba\x86\xef\xbc\x8c\xe5\xb0\xb1\xe4\xb8\x8d\xe5\x86\x8d\xe8\xb5\x98\xe8\xbf\xb0\xef\xbc\x8c\xe4\xb8\x8b\xe9\x9d\xa2\xe6\x88\x91\xe4\xbb\xac\xe4\xb9\x9f\xe4\xb8\xbe\xe4\xbe\x8b\xe8\xaf\xb4\xe6\x98\x8e\n\n#%%\n# \xe4\xbd\xbf\xe7\x94\xa8 nn.MaxPool2d\npool1 = nn.MaxPool2d(2, 2)\nprint('before max pool, image shape: {} x {}'.format(im.shape[2], im.shape[3]))\nsmall_im1 = pool1(im)\nsmall_im1 = small_im1.data.squeeze().numpy()\nprint('after max pool, image shape: {} x {} '.format(small_im1.shape[0], small_im1.shape[1]))\n\n#%% [markdown]\n# \xe5\x8f\xaf\xe4\xbb\xa5\xe7\x9c\x8b\xe5\x88\xb0\xe5\x9b\xbe\xe7\x89\x87\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f\xe5\x87\x8f\xe5\xb0\x8f\xe4\xba\x86\xe4\xb8\x80\xe5\x8d\x8a\xef\xbc\x8c\xe9\x82\xa3\xe4\xb9\x88\xe5\x9b\xbe\xe7\x89\x87\xe6\x98\xaf\xe4\xb8\x8d\xe6\x98\xaf\xe5\x8f\x98\xe4\xba\x86\xe5\x91\xa2\xef\xbc\x9f\xe6\x88\x91\xe4\xbb\xac\xe5\x8f\xaf\xe4\xbb\xa5\xe5\x8f\xaf\xe8\xa7\x86\xe5\x8c\x96\xe4\xb8\x80\xe4\xb8\x8b\n\n#%%\nplt.imshow(small_im1, cmap='gray')\n\n#%% [markdown]\n# \xe5\x8f\xaf\xe4\xbb\xa5\xe7\x9c\x8b\xe5\x88\xb0\xe5\x9b\xbe\xe7\x89\x87\xe5\x87\xa0\xe4\xb9\x8e\xe6\xb2\xa1\xe6\x9c\x89\xe5\x8f\x98\xe5\x8c\x96\xef\xbc\x8c\xe8\xaf\xb4\xe6\x98\x8e\xe6\xb1\xa0\xe5\x8c\x96\xe5\xb1\x82\xe5\x8f\xaa\xe6\x98\xaf\xe5\x87\x8f\xe5\xb0\x8f\xe4\xba\x86\xe5\x9b\xbe\xe7\x89\x87\xe7\x9a\x84\xe5\xb0\xba\xe5\xaf\xb8\xef\xbc\x8c\xe5\xb9\xb6\xe4\xb8\x8d\xe4\xbc\x9a\xe5\xbd\xb1\xe5\x93\x8d\xe5\x9b\xbe\xe7\x89\x87\xe7\x9a\x84\xe5\x86\x85\xe5\xae\xb9\n\n#%%\n# F.max_pool2d\nprint('before max pool, image shape: {} x {}'.format(im.shape[2], im.shape[3]))\nsmall_im2 = F.max_pool2d(im, 2, 2)\nsmall_im2 = small_im2.data.squeeze().numpy()\nprint('after max pool, image shape: {} x {} '.format(small_im1.shape[0], small_im1.shape[1]))\nplt.imshow(small_im2, cmap='gray')\n\n#%% [markdown]\n# **\xe8\xb7\x9f\xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82\xe4\xb8\x80\xe6\xa0\xb7\xef\xbc\x8c\xe5\xae\x9e\xe9\x99\x85\xe4\xbd\xbf\xe7\x94\xa8\xe4\xb8\xad\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe4\xb8\x80\xe8\x88\xac\xe4\xbd\xbf\xe7\x94\xa8 `nn.MaxPool2d()`**\n#%% [markdown]\n# \xe4\xbb\xa5\xe4\xb8\x8a\xe6\x88\x91\xe4\xbb\xac\xe4\xbb\x8b\xe7\xbb\x8d\xe4\xba\x86\xe5\xa6\x82\xe4\xbd\x95\xe5\x9c\xa8 pytorch \xe4\xb8\xad\xe4\xbd\xbf\xe7\x94\xa8\xe5\x8d\xb7\xe7\xa7\xaf\xe7\xbd\x91\xe7\xbb\x9c\xe4\xb8\xad\xe7\x9a\x84\xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa8\xa1\xe5\x9d\x97\xe5\x92\x8c\xe6\xb1\xa0\xe5\x8c\x96\xe6\xa8\xa1\xe5\x9d\x97\xef\xbc\x8c\xe6\x8e\xa5\xe4\xb8\x8b\xe6\x9d\xa5\xe6\x88\x91\xe4\xbb\xac\xe4\xbc\x9a\xe5\xbc\x80\xe5\xa7\x8b\xe8\xae\xb2\xe5\x8d\xb7\xe7\xa7\xaf\xe7\xbd\x91\xe7\xbb\x9c\xe4\xb8\xad\xe5\x87\xa0\xe4\xb8\xaa\xe9\x9d\x9e\xe5\xb8\xb8\xe8\x91\x97\xe5\x90\x8d\xe7\x9a\x84\xe7\xbd\x91\xe7\xbb\x9c\xe7\xbb\x93\xe6\x9e\x84\n\n"""
7.Convolutional_Neural_Network(CNN) 卷积神经网络/batch-normalization 批标准化 (one problem left).py,22,"b""#%% [markdown]\n# # \xe6\x89\xb9\xe6\xa0\x87\xe5\x87\x86\xe5\x8c\x96\n# \xe5\x9c\xa8\xe6\x88\x91\xe4\xbb\xac\xe6\xad\xa3\xe5\xbc\x8f\xe8\xbf\x9b\xe5\x85\xa5\xe6\xa8\xa1\xe5\x9e\x8b\xe7\x9a\x84\xe6\x9e\x84\xe5\xbb\xba\xe5\x92\x8c\xe8\xae\xad\xe7\xbb\x83\xe4\xb9\x8b\xe5\x89\x8d\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe4\xbc\x9a\xe5\x85\x88\xe8\xae\xb2\xe4\xb8\x80\xe8\xae\xb2\xe6\x95\xb0\xe6\x8d\xae\xe9\xa2\x84\xe5\xa4\x84\xe7\x90\x86\xe5\x92\x8c\xe6\x89\xb9\xe6\xa0\x87\xe5\x87\x86\xe5\x8c\x96\xef\xbc\x8c\xe5\x9b\xa0\xe4\xb8\xba\xe6\xa8\xa1\xe5\x9e\x8b\xe8\xae\xad\xe7\xbb\x83\xe5\xb9\xb6\xe4\xb8\x8d\xe5\xae\xb9\xe6\x98\x93\xef\xbc\x8c\xe7\x89\xb9\xe5\x88\xab\xe6\x98\xaf\xe4\xb8\x80\xe4\xba\x9b\xe9\x9d\x9e\xe5\xb8\xb8\xe5\xa4\x8d\xe6\x9d\x82\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\xef\xbc\x8c\xe5\xb9\xb6\xe4\xb8\x8d\xe8\x83\xbd\xe9\x9d\x9e\xe5\xb8\xb8\xe5\xa5\xbd\xe7\x9a\x84\xe8\xae\xad\xe7\xbb\x83\xe5\xbe\x97\xe5\x88\xb0\xe6\x94\xb6\xe6\x95\x9b\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x9c\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe5\xaf\xb9\xe6\x95\xb0\xe6\x8d\xae\xe5\xa2\x9e\xe5\x8a\xa0\xe4\xb8\x80\xe4\xba\x9b\xe9\xa2\x84\xe5\xa4\x84\xe7\x90\x86\xef\xbc\x8c\xe5\x90\x8c\xe6\x97\xb6\xe4\xbd\xbf\xe7\x94\xa8\xe6\x89\xb9\xe6\xa0\x87\xe5\x87\x86\xe5\x8c\x96\xe8\x83\xbd\xe5\xa4\x9f\xe5\xbe\x97\xe5\x88\xb0\xe9\x9d\x9e\xe5\xb8\xb8\xe5\xa5\xbd\xe7\x9a\x84\xe6\x94\xb6\xe6\x95\x9b\xe7\xbb\x93\xe6\x9e\x9c\xef\xbc\x8c\xe8\xbf\x99\xe4\xb9\x9f\xe6\x98\xaf\xe5\x8d\xb7\xe7\xa7\xaf\xe7\xbd\x91\xe7\xbb\x9c\xe8\x83\xbd\xe5\xa4\x9f\xe8\xae\xad\xe7\xbb\x83\xe5\x88\xb0\xe9\x9d\x9e\xe5\xb8\xb8\xe6\xb7\xb1\xe7\x9a\x84\xe5\xb1\x82\xe7\x9a\x84\xe4\xb8\x80\xe4\xb8\xaa\xe9\x87\x8d\xe8\xa6\x81\xe5\x8e\x9f\xe5\x9b\xa0\xe3\x80\x82\n#%% [markdown]\n# ## \xe6\x95\xb0\xe6\x8d\xae\xe9\xa2\x84\xe5\xa4\x84\xe7\x90\x86\n# \xe7\x9b\xae\xe5\x89\x8d\xe6\x95\xb0\xe6\x8d\xae\xe9\xa2\x84\xe5\xa4\x84\xe7\x90\x86\xe6\x9c\x80\xe5\xb8\xb8\xe8\xa7\x81\xe7\x9a\x84\xe6\x96\xb9\xe6\xb3\x95\xe5\xb0\xb1\xe6\x98\xaf\xe4\xb8\xad\xe5\xbf\x83\xe5\x8c\x96\xe5\x92\x8c\xe6\xa0\x87\xe5\x87\x86\xe5\x8c\x96\xef\xbc\x8c\xe4\xb8\xad\xe5\xbf\x83\xe5\x8c\x96\xe7\x9b\xb8\xe5\xbd\x93\xe4\xba\x8e\xe4\xbf\xae\xe6\xad\xa3\xe6\x95\xb0\xe6\x8d\xae\xe7\x9a\x84\xe4\xb8\xad\xe5\xbf\x83\xe4\xbd\x8d\xe7\xbd\xae\xef\xbc\x8c\xe5\xae\x9e\xe7\x8e\xb0\xe6\x96\xb9\xe6\xb3\x95\xe9\x9d\x9e\xe5\xb8\xb8\xe7\xae\x80\xe5\x8d\x95\xef\xbc\x8c\xe5\xb0\xb1\xe6\x98\xaf\xe5\x9c\xa8\xe6\xaf\x8f\xe4\xb8\xaa\xe7\x89\xb9\xe5\xbe\x81\xe7\xbb\xb4\xe5\xba\xa6\xe4\xb8\x8a\xe5\x87\x8f\xe5\x8e\xbb\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84\xe5\x9d\x87\xe5\x80\xbc\xef\xbc\x8c\xe6\x9c\x80\xe5\x90\x8e\xe5\xbe\x97\xe5\x88\xb0 0 \xe5\x9d\x87\xe5\x80\xbc\xe7\x9a\x84\xe7\x89\xb9\xe5\xbe\x81\xe3\x80\x82\xe6\xa0\x87\xe5\x87\x86\xe5\x8c\x96\xe4\xb9\x9f\xe9\x9d\x9e\xe5\xb8\xb8\xe7\xae\x80\xe5\x8d\x95\xef\xbc\x8c\xe5\x9c\xa8\xe6\x95\xb0\xe6\x8d\xae\xe5\x8f\x98\xe6\x88\x90 0 \xe5\x9d\x87\xe5\x80\xbc\xe4\xb9\x8b\xe5\x90\x8e\xef\xbc\x8c\xe4\xb8\xba\xe4\xba\x86\xe4\xbd\xbf\xe5\xbe\x97\xe4\xb8\x8d\xe5\x90\x8c\xe7\x9a\x84\xe7\x89\xb9\xe5\xbe\x81\xe7\xbb\xb4\xe5\xba\xa6\xe6\x9c\x89\xe7\x9d\x80\xe7\x9b\xb8\xe5\x90\x8c\xe7\x9a\x84\xe8\xa7\x84\xe6\xa8\xa1\xef\xbc\x8c\xe5\x8f\xaf\xe4\xbb\xa5\xe9\x99\xa4\xe4\xbb\xa5\xe6\xa0\x87\xe5\x87\x86\xe5\xb7\xae\xe8\xbf\x91\xe4\xbc\xbc\xe4\xb8\xba\xe4\xb8\x80\xe4\xb8\xaa\xe6\xa0\x87\xe5\x87\x86\xe6\xad\xa3\xe6\x80\x81\xe5\x88\x86\xe5\xb8\x83\xef\xbc\x8c\xe4\xb9\x9f\xe5\x8f\xaf\xe4\xbb\xa5\xe4\xbe\x9d\xe6\x8d\xae\xe6\x9c\x80\xe5\xa4\xa7\xe5\x80\xbc\xe5\x92\x8c\xe6\x9c\x80\xe5\xb0\x8f\xe5\x80\xbc\xe5\xb0\x86\xe5\x85\xb6\xe8\xbd\xac\xe5\x8c\x96\xe4\xb8\xba -1 ~ 1 \xe4\xb9\x8b\xe9\x97\xb4\xef\xbc\x8c\xe4\xb8\x8b\xe9\x9d\xa2\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa\xe7\xae\x80\xe5\x8d\x95\xe7\x9a\x84\xe5\x9b\xbe\xe7\xa4\xba\n# \n# ![](https://ws1.sinaimg.cn/large/006tKfTcly1fmqouzer3xj30ij06n0t8.jpg)\n# \n# \xe8\xbf\x99\xe4\xb8\xa4\xe7\xa7\x8d\xe6\x96\xb9\xe6\xb3\x95\xe9\x9d\x9e\xe5\xb8\xb8\xe7\x9a\x84\xe5\xb8\xb8\xe8\xa7\x81\xef\xbc\x8c\xe5\xa6\x82\xe6\x9e\x9c\xe4\xbd\xa0\xe8\xbf\x98\xe8\xae\xb0\xe5\xbe\x97\xef\xbc\x8c\xe5\x89\x8d\xe9\x9d\xa2\xe6\x88\x91\xe4\xbb\xac\xe5\x9c\xa8\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xe7\x9a\x84\xe9\x83\xa8\xe5\x88\x86\xe5\xb0\xb1\xe5\xb7\xb2\xe7\xbb\x8f\xe4\xbd\xbf\xe7\x94\xa8\xe4\xba\x86\xe8\xbf\x99\xe4\xb8\xaa\xe6\x96\xb9\xe6\xb3\x95\xe5\xae\x9e\xe7\x8e\xb0\xe4\xba\x86\xe6\x95\xb0\xe6\x8d\xae\xe6\xa0\x87\xe5\x87\x86\xe5\x8c\x96\xef\xbc\x8c\xe8\x87\xb3\xe4\xba\x8e\xe5\x8f\xa6\xe5\xa4\x96\xe4\xb8\x80\xe4\xba\x9b\xe6\x96\xb9\xe6\xb3\x95\xef\xbc\x8c\xe6\xaf\x94\xe5\xa6\x82 PCA \xe6\x88\x96\xe8\x80\x85 \xe7\x99\xbd\xe5\x99\xaa\xe5\xa3\xb0\xe5\xb7\xb2\xe7\xbb\x8f\xe7\x94\xa8\xe5\xbe\x97\xe9\x9d\x9e\xe5\xb8\xb8\xe5\xb0\x91\xe4\xba\x86\xe3\x80\x82\n#%% [markdown]\n# ## Batch Normalization\n# \xe5\x89\x8d\xe9\x9d\xa2\xe5\x9c\xa8\xe6\x95\xb0\xe6\x8d\xae\xe9\xa2\x84\xe5\xa4\x84\xe7\x90\x86\xe7\x9a\x84\xe6\x97\xb6\xe5\x80\x99\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe5\xb0\xbd\xe9\x87\x8f\xe8\xbe\x93\xe5\x85\xa5\xe7\x89\xb9\xe5\xbe\x81\xe4\xb8\x8d\xe7\x9b\xb8\xe5\x85\xb3\xe4\xb8\x94\xe6\xbb\xa1\xe8\xb6\xb3\xe4\xb8\x80\xe4\xb8\xaa\xe6\xa0\x87\xe5\x87\x86\xe7\x9a\x84\xe6\xad\xa3\xe6\x80\x81\xe5\x88\x86\xe5\xb8\x83\xef\xbc\x8c\xe8\xbf\x99\xe6\xa0\xb7\xe6\xa8\xa1\xe5\x9e\x8b\xe7\x9a\x84\xe8\xa1\xa8\xe7\x8e\xb0\xe4\xb8\x80\xe8\x88\xac\xe4\xb9\x9f\xe8\xbe\x83\xe5\xa5\xbd\xe3\x80\x82\xe4\xbd\x86\xe6\x98\xaf\xe5\xaf\xb9\xe4\xba\x8e\xe5\xbe\x88\xe6\xb7\xb1\xe7\x9a\x84\xe7\xbd\x91\xe8\xb7\xaf\xe7\xbb\x93\xe6\x9e\x84\xef\xbc\x8c\xe7\xbd\x91\xe8\xb7\xaf\xe7\x9a\x84\xe9\x9d\x9e\xe7\xba\xbf\xe6\x80\xa7\xe5\xb1\x82\xe4\xbc\x9a\xe4\xbd\xbf\xe5\xbe\x97\xe8\xbe\x93\xe5\x87\xba\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x9c\xe5\x8f\x98\xe5\xbe\x97\xe7\x9b\xb8\xe5\x85\xb3\xef\xbc\x8c\xe4\xb8\x94\xe4\xb8\x8d\xe5\x86\x8d\xe6\xbb\xa1\xe8\xb6\xb3\xe4\xb8\x80\xe4\xb8\xaa\xe6\xa0\x87\xe5\x87\x86\xe7\x9a\x84 N(0, 1) \xe7\x9a\x84\xe5\x88\x86\xe5\xb8\x83\xef\xbc\x8c\xe7\x94\x9a\xe8\x87\xb3\xe8\xbe\x93\xe5\x87\xba\xe7\x9a\x84\xe4\xb8\xad\xe5\xbf\x83\xe5\xb7\xb2\xe7\xbb\x8f\xe5\x8f\x91\xe7\x94\x9f\xe4\xba\x86\xe5\x81\x8f\xe7\xa7\xbb\xef\xbc\x8c\xe8\xbf\x99\xe5\xaf\xb9\xe4\xba\x8e\xe6\xa8\xa1\xe5\x9e\x8b\xe7\x9a\x84\xe8\xae\xad\xe7\xbb\x83\xef\xbc\x8c\xe7\x89\xb9\xe5\x88\xab\xe6\x98\xaf\xe6\xb7\xb1\xe5\xb1\x82\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\xe8\xae\xad\xe7\xbb\x83\xe9\x9d\x9e\xe5\xb8\xb8\xe7\x9a\x84\xe5\x9b\xb0\xe9\x9a\xbe\xe3\x80\x82\n# \n# \xe6\x89\x80\xe4\xbb\xa5\xe5\x9c\xa8 2015 \xe5\xb9\xb4\xe4\xb8\x80\xe7\xaf\x87\xe8\xae\xba\xe6\x96\x87\xe6\x8f\x90\xe5\x87\xba\xe4\xba\x86\xe8\xbf\x99\xe4\xb8\xaa\xe6\x96\xb9\xe6\xb3\x95\xef\xbc\x8c\xe6\x89\xb9\xe6\xa0\x87\xe5\x87\x86\xe5\x8c\x96\xef\xbc\x8c\xe7\xae\x80\xe8\x80\x8c\xe8\xa8\x80\xe4\xb9\x8b\xef\xbc\x8c\xe5\xb0\xb1\xe6\x98\xaf\xe5\xaf\xb9\xe4\xba\x8e\xe6\xaf\x8f\xe4\xb8\x80\xe5\xb1\x82\xe7\xbd\x91\xe7\xbb\x9c\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xef\xbc\x8c\xe5\xaf\xb9\xe5\x85\xb6\xe5\x81\x9a\xe4\xb8\x80\xe4\xb8\xaa\xe5\xbd\x92\xe4\xb8\x80\xe5\x8c\x96\xef\xbc\x8c\xe4\xbd\xbf\xe5\x85\xb6\xe6\x9c\x8d\xe4\xbb\x8e\xe6\xa0\x87\xe5\x87\x86\xe7\x9a\x84\xe6\xad\xa3\xe6\x80\x81\xe5\x88\x86\xe5\xb8\x83\xef\xbc\x8c\xe8\xbf\x99\xe6\xa0\xb7\xe5\x90\x8e\xe4\xb8\x80\xe5\xb1\x82\xe7\xbd\x91\xe7\xbb\x9c\xe7\x9a\x84\xe8\xbe\x93\xe5\x85\xa5\xe4\xb9\x9f\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa\xe6\xa0\x87\xe5\x87\x86\xe7\x9a\x84\xe6\xad\xa3\xe6\x80\x81\xe5\x88\x86\xe5\xb8\x83\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe8\x83\xbd\xe5\xa4\x9f\xe6\xaf\x94\xe8\xbe\x83\xe5\xa5\xbd\xe7\x9a\x84\xe8\xbf\x9b\xe8\xa1\x8c\xe8\xae\xad\xe7\xbb\x83\xef\xbc\x8c\xe5\x8a\xa0\xe5\xbf\xab\xe6\x94\xb6\xe6\x95\x9b\xe9\x80\x9f\xe5\xba\xa6\xe3\x80\x82\n#%% [markdown]\n# batch normalization \xe7\x9a\x84\xe5\xae\x9e\xe7\x8e\xb0\xe9\x9d\x9e\xe5\xb8\xb8\xe7\xae\x80\xe5\x8d\x95\xef\xbc\x8c\xe5\xaf\xb9\xe4\xba\x8e\xe7\xbb\x99\xe5\xae\x9a\xe7\x9a\x84\xe4\xb8\x80\xe4\xb8\xaa batch \xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae $B = \\{x_1, x_2, \\cdots, x_m\\}$\xe7\xae\x97\xe6\xb3\x95\xe7\x9a\x84\xe5\x85\xac\xe5\xbc\x8f\xe5\xa6\x82\xe4\xb8\x8b\n# \n# $$\n# \\mu_B = \\frac{1}{m} \\sum_{i=1}^m x_i\n# $$\n# $$\n# \\sigma^2_B = \\frac{1}{m} \\sum_{i=1}^m (x_i - \\mu_B)^2\n# $$\n# $$\n# \\hat{x}_i = \\frac{x_i - \\mu_B}{\\sqrt{\\sigma^2_B + \\epsilon}}\n# $$\n# $$\n# y_i = \\gamma \\hat{x}_i + \\beta\n# $$\n#%% [markdown]\n# \xe7\xac\xac\xe4\xb8\x80\xe8\xa1\x8c\xe5\x92\x8c\xe7\xac\xac\xe4\xba\x8c\xe8\xa1\x8c\xe6\x98\xaf\xe8\xae\xa1\xe7\xae\x97\xe5\x87\xba\xe4\xb8\x80\xe4\xb8\xaa batch \xe4\xb8\xad\xe6\x95\xb0\xe6\x8d\xae\xe7\x9a\x84\xe5\x9d\x87\xe5\x80\xbc\xe5\x92\x8c\xe6\x96\xb9\xe5\xb7\xae\xef\xbc\x8c\xe6\x8e\xa5\xe7\x9d\x80\xe4\xbd\xbf\xe7\x94\xa8\xe7\xac\xac\xe4\xb8\x89\xe4\xb8\xaa\xe5\x85\xac\xe5\xbc\x8f\xe5\xaf\xb9 batch \xe4\xb8\xad\xe7\x9a\x84\xe6\xaf\x8f\xe4\xb8\xaa\xe6\x95\xb0\xe6\x8d\xae\xe7\x82\xb9\xe5\x81\x9a\xe6\xa0\x87\xe5\x87\x86\xe5\x8c\x96\xef\xbc\x8c$\\epsilon$ \xe6\x98\xaf\xe4\xb8\xba\xe4\xba\x86\xe8\xae\xa1\xe7\xae\x97\xe7\xa8\xb3\xe5\xae\x9a\xe5\xbc\x95\xe5\x85\xa5\xe7\x9a\x84\xe4\xb8\x80\xe4\xb8\xaa\xe5\xb0\x8f\xe7\x9a\x84\xe5\xb8\xb8\xe6\x95\xb0\xef\xbc\x8c\xe9\x80\x9a\xe5\xb8\xb8\xe5\x8f\x96 $10^{-5}$\xef\xbc\x8c\xe6\x9c\x80\xe5\x90\x8e\xe5\x88\xa9\xe7\x94\xa8\xe6\x9d\x83\xe9\x87\x8d\xe4\xbf\xae\xe6\xad\xa3\xe5\xbe\x97\xe5\x88\xb0\xe6\x9c\x80\xe5\x90\x8e\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe7\xbb\x93\xe6\x9e\x9c\xef\xbc\x8c\xe9\x9d\x9e\xe5\xb8\xb8\xe7\x9a\x84\xe7\xae\x80\xe5\x8d\x95\xef\xbc\x8c\xe4\xb8\x8b\xe9\x9d\xa2\xe6\x88\x91\xe4\xbb\xac\xe5\x8f\xaf\xe4\xbb\xa5\xe5\xae\x9e\xe7\x8e\xb0\xe4\xb8\x80\xe4\xb8\x8b\xe7\xae\x80\xe5\x8d\x95\xe7\x9a\x84\xe4\xb8\x80\xe7\xbb\xb4\xe7\x9a\x84\xe6\x83\x85\xe5\x86\xb5\xef\xbc\x8c\xe4\xb9\x9f\xe5\xb0\xb1\xe6\x98\xaf\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xe4\xb8\xad\xe7\x9a\x84\xe6\x83\x85\xe5\x86\xb5\n\n#%%\nimport torch\n\n\n#%%\ndef simple_batch_norm_1d(x, gamma, beta):\n    eps = 1e-5\n    x_mean = torch.mean(x, dim=0, keepdim=True) # \xe4\xbf\x9d\xe7\x95\x99\xe7\xbb\xb4\xe5\xba\xa6\xe8\xbf\x9b\xe8\xa1\x8c broadcast\n    x_var = torch.mean((x - x_mean) ** 2, dim=0, keepdim=True)\n    x_hat = (x - x_mean) / torch.sqrt(x_var + eps)\n    return gamma.view_as(x_mean) * x_hat + beta.view_as(x_mean)\n\n#%% [markdown]\n# \xe6\x88\x91\xe4\xbb\xac\xe6\x9d\xa5\xe9\xaa\x8c\xe8\xaf\x81\xe4\xb8\x80\xe4\xb8\x8b\xe6\x98\xaf\xe5\x90\xa6\xe5\xaf\xb9\xe4\xba\x8e\xe4\xbb\xbb\xe6\x84\x8f\xe7\x9a\x84\xe8\xbe\x93\xe5\x85\xa5\xef\xbc\x8c\xe8\xbe\x93\xe5\x87\xba\xe4\xbc\x9a\xe8\xa2\xab\xe6\xa0\x87\xe5\x87\x86\xe5\x8c\x96\n\n#%%\nx = torch.arange(15.0).view(5, 3)\ngamma = torch.ones(x.shape[1])\nbeta = torch.zeros(x.shape[1])\nprint('before bn: ')\nprint(x)\ny = simple_batch_norm_1d(x, gamma, beta)\nprint('after bn: ')\nprint(y)\n\n#%% [markdown]\n# \xe5\x8f\xaf\xe4\xbb\xa5\xe7\x9c\x8b\xe5\x88\xb0\xe8\xbf\x99\xe9\x87\x8c\xe4\xb8\x80\xe5\x85\xb1\xe6\x98\xaf 5 \xe4\xb8\xaa\xe6\x95\xb0\xe6\x8d\xae\xe7\x82\xb9\xef\xbc\x8c\xe4\xb8\x89\xe4\xb8\xaa\xe7\x89\xb9\xe5\xbe\x81\xef\xbc\x8c\xe6\xaf\x8f\xe4\xb8\x80\xe5\x88\x97\xe8\xa1\xa8\xe7\xa4\xba\xe4\xb8\x80\xe4\xb8\xaa\xe7\x89\xb9\xe5\xbe\x81\xe7\x9a\x84\xe4\xb8\x8d\xe5\x90\x8c\xe6\x95\xb0\xe6\x8d\xae\xe7\x82\xb9\xef\xbc\x8c\xe4\xbd\xbf\xe7\x94\xa8\xe6\x89\xb9\xe6\xa0\x87\xe5\x87\x86\xe5\x8c\x96\xe4\xb9\x8b\xe5\x90\x8e\xef\xbc\x8c\xe6\xaf\x8f\xe4\xb8\x80\xe5\x88\x97\xe9\x83\xbd\xe5\x8f\x98\xe6\x88\x90\xe4\xba\x86\xe6\xa0\x87\xe5\x87\x86\xe7\x9a\x84\xe6\xad\xa3\xe6\x80\x81\xe5\x88\x86\xe5\xb8\x83\n# \n# \xe8\xbf\x99\xe4\xb8\xaa\xe6\x97\xb6\xe5\x80\x99\xe4\xbc\x9a\xe5\x87\xba\xe7\x8e\xb0\xe4\xb8\x80\xe4\xb8\xaa\xe9\x97\xae\xe9\xa2\x98\xef\xbc\x8c\xe5\xb0\xb1\xe6\x98\xaf\xe6\xb5\x8b\xe8\xaf\x95\xe7\x9a\x84\xe6\x97\xb6\xe5\x80\x99\xe8\xaf\xa5\xe4\xbd\xbf\xe7\x94\xa8\xe6\x89\xb9\xe6\xa0\x87\xe5\x87\x86\xe5\x8c\x96\xe5\x90\x97\xef\xbc\x9f\n# \n# \xe7\xad\x94\xe6\xa1\x88\xe6\x98\xaf\xe8\x82\xaf\xe5\xae\x9a\xe7\x9a\x84\xef\xbc\x8c\xe5\x9b\xa0\xe4\xb8\xba\xe8\xae\xad\xe7\xbb\x83\xe7\x9a\x84\xe6\x97\xb6\xe5\x80\x99\xe4\xbd\xbf\xe7\x94\xa8\xe4\xba\x86\xef\xbc\x8c\xe8\x80\x8c\xe6\xb5\x8b\xe8\xaf\x95\xe7\x9a\x84\xe6\x97\xb6\xe5\x80\x99\xe4\xb8\x8d\xe4\xbd\xbf\xe7\x94\xa8\xe8\x82\xaf\xe5\xae\x9a\xe4\xbc\x9a\xe5\xaf\xbc\xe8\x87\xb4\xe7\xbb\x93\xe6\x9e\x9c\xe5\x87\xba\xe7\x8e\xb0\xe5\x81\x8f\xe5\xb7\xae\xef\xbc\x8c\xe4\xbd\x86\xe6\x98\xaf\xe6\xb5\x8b\xe8\xaf\x95\xe7\x9a\x84\xe6\x97\xb6\xe5\x80\x99\xe5\xa6\x82\xe6\x9e\x9c\xe5\x8f\xaa\xe6\x9c\x89\xe4\xb8\x80\xe4\xb8\xaa\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xef\xbc\x8c\xe9\x82\xa3\xe4\xb9\x88\xe5\x9d\x87\xe5\x80\xbc\xe4\xb8\x8d\xe5\xb0\xb1\xe6\x98\xaf\xe8\xbf\x99\xe4\xb8\xaa\xe5\x80\xbc\xef\xbc\x8c\xe6\x96\xb9\xe5\xb7\xae\xe4\xb8\xba 0 \xe5\x90\x97\xef\xbc\x9f\xe8\xbf\x99\xe6\x98\xbe\xe7\x84\xb6\xe6\x98\xaf\xe9\x9a\x8f\xe6\x9c\xba\xe7\x9a\x84\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe6\xb5\x8b\xe8\xaf\x95\xe7\x9a\x84\xe6\x97\xb6\xe5\x80\x99\xe4\xb8\x8d\xe8\x83\xbd\xe7\x94\xa8\xe6\xb5\x8b\xe8\xaf\x95\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe5\x8e\xbb\xe7\xae\x97\xe5\x9d\x87\xe5\x80\xbc\xe5\x92\x8c\xe6\x96\xb9\xe5\xb7\xae\xef\xbc\x8c\xe8\x80\x8c\xe6\x98\xaf\xe7\x94\xa8\xe8\xae\xad\xe7\xbb\x83\xe7\x9a\x84\xe6\x97\xb6\xe5\x80\x99\xe7\xae\x97\xe5\x87\xba\xe7\x9a\x84\xe7\xa7\xbb\xe5\x8a\xa8\xe5\xb9\xb3\xe5\x9d\x87\xe5\x9d\x87\xe5\x80\xbc\xe5\x92\x8c\xe6\x96\xb9\xe5\xb7\xae\xe5\x8e\xbb\xe4\xbb\xa3\xe6\x9b\xbf\n# \n# \xe4\xb8\x8b\xe9\x9d\xa2\xe6\x88\x91\xe4\xbb\xac\xe5\xae\x9e\xe7\x8e\xb0\xe4\xbb\xa5\xe4\xb8\x8b\xe8\x83\xbd\xe5\xa4\x9f\xe5\x8c\xba\xe5\x88\x86\xe8\xae\xad\xe7\xbb\x83\xe7\x8a\xb6\xe6\x80\x81\xe5\x92\x8c\xe6\xb5\x8b\xe8\xaf\x95\xe7\x8a\xb6\xe6\x80\x81\xe7\x9a\x84\xe6\x89\xb9\xe6\xa0\x87\xe5\x87\x86\xe5\x8c\x96\xe6\x96\xb9\xe6\xb3\x95\n\n#%%\ndef batch_norm_1d(x, gamma, beta, is_training, moving_mean, moving_var, moving_momentum=0.1):\n    eps = 1e-5\n    x_mean = torch.mean(x, dim=0, keepdim=True) # \xe4\xbf\x9d\xe7\x95\x99\xe7\xbb\xb4\xe5\xba\xa6\xe8\xbf\x9b\xe8\xa1\x8c broadcast\n    x_var = torch.mean((x - x_mean) ** 2, dim=0, keepdim=True)\n    if is_training:\n        x_hat = (x - x_mean) / torch.sqrt(x_var + eps)\n        moving_mean[:] = moving_momentum * moving_mean + (1. - moving_momentum) * x_mean\n        moving_var[:] = moving_momentum * moving_var + (1. - moving_momentum) * x_var\n    else:\n        x_hat = (x - moving_mean) / torch.sqrt(moving_var + eps)\n    return gamma.view_as(x_mean) * x_hat + beta.view_as(x_mean)\n\n#%% [markdown]\n# \xe4\xb8\x8b\xe9\x9d\xa2\xe6\x88\x91\xe4\xbb\xac\xe4\xbd\xbf\xe7\x94\xa8\xe4\xb8\x8a\xe4\xb8\x80\xe8\x8a\x82\xe8\xaf\xbe\xe5\xb0\x86\xe7\x9a\x84\xe6\xb7\xb1\xe5\xba\xa6\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xe5\x88\x86\xe7\xb1\xbb mnist \xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe7\x9a\x84\xe4\xbe\x8b\xe5\xad\x90\xe6\x9d\xa5\xe8\xaf\x95\xe9\xaa\x8c\xe4\xb8\x80\xe4\xb8\x8b\xe6\x89\xb9\xe6\xa0\x87\xe5\x87\x86\xe5\x8c\x96\xe6\x98\xaf\xe5\x90\xa6\xe6\x9c\x89\xe7\x94\xa8\n\n#%%\nimport numpy as np\nfrom torchvision.datasets import mnist # \xe5\xaf\xbc\xe5\x85\xa5 pytorch \xe5\x86\x85\xe7\xbd\xae\xe7\x9a\x84 mnist \xe6\x95\xb0\xe6\x8d\xae\nfrom torch.utils.data import DataLoader\nfrom torch import nn\n\n\n#%%\n# \xe4\xbd\xbf\xe7\x94\xa8\xe5\x86\x85\xe7\xbd\xae\xe5\x87\xbd\xe6\x95\xb0\xe4\xb8\x8b\xe8\xbd\xbd mnist \xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\ntrain_set = mnist.MNIST('C:/DATASETS/mnist/', train=True)\ntest_set = mnist.MNIST('C:/DATASETS/mnist/', train=False)\n\ndef data_tf(x):\n    x = np.array(x, dtype='float32') / 255\n    x = (x - 0.5) / 0.5 # \xe6\x95\xb0\xe6\x8d\xae\xe9\xa2\x84\xe5\xa4\x84\xe7\x90\x86\xef\xbc\x8c\xe6\xa0\x87\xe5\x87\x86\xe5\x8c\x96\n    x = x.reshape((-1,)) # \xe6\x8b\x89\xe5\xb9\xb3\n    x = torch.from_numpy(x)\n    return x\n\ntrain_set = mnist.MNIST('C:/DATASETS/mnist/', train=True, transform=data_tf, download=True) # \xe9\x87\x8d\xe6\x96\xb0\xe8\xbd\xbd\xe5\x85\xa5\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xef\xbc\x8c\xe7\x94\xb3\xe6\x98\x8e\xe5\xae\x9a\xe4\xb9\x89\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe5\x8f\x98\xe6\x8d\xa2\ntest_set = mnist.MNIST('C:/DATASETS/mnist/', train=False, transform=data_tf, download=True)\ntrain_data = DataLoader(train_set, batch_size=64, shuffle=True)\ntest_data = DataLoader(test_set, batch_size=128, shuffle=False)\n\n\n#%%\nclass multi_network(nn.Module):\n    def __init__(self):\n        super(multi_network, self).__init__()\n        self.layer1 = nn.Linear(784, 100)\n        self.relu = nn.ReLU(True)\n        self.layer2 = nn.Linear(100, 10)\n        \n        self.gamma = nn.Parameter(torch.randn(100))\n        self.beta = nn.Parameter(torch.randn(100))\n        \n        self.moving_mean = torch.zeros(100)\n        self.moving_var = torch.zeros(100)\n        \n    def forward(self, x, is_train=True):\n        x = self.layer1(x)\n        x = batch_norm_1d(x, self.gamma, self.beta, is_train, self.moving_mean, self.moving_var)\n        x = self.relu(x)\n        x = self.layer2(x)\n        return x\n\n\n#%%\nnet = multi_network()\n\n\n#%%\nnet.parameters\n\n\n#%%\n# \xe5\xae\x9a\xe4\xb9\x89 loss \xe5\x87\xbd\xe6\x95\xb0\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(net.parameters(), 1e-1) # \xe4\xbd\xbf\xe7\x94\xa8\xe9\x9a\x8f\xe6\x9c\xba\xe6\xa2\xaf\xe5\xba\xa6\xe4\xb8\x8b\xe9\x99\x8d\xef\xbc\x8c\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87 0.1\n\n#%% [markdown]\n# \xe4\xb8\xba\xe4\xba\x86\xe6\x96\xb9\xe4\xbe\xbf\xef\xbc\x8c\xe8\xae\xad\xe7\xbb\x83\xe5\x87\xbd\xe6\x95\xb0\xe5\xb7\xb2\xe7\xbb\x8f\xe5\xae\x9a\xe4\xb9\x89\xe5\x9c\xa8\xe5\xa4\x96\xe9\x9d\xa2\xe7\x9a\x84 utils.py \xe4\xb8\xad\xef\xbc\x8c\xe8\xb7\x9f\xe5\x89\x8d\xe9\x9d\xa2\xe8\xae\xad\xe7\xbb\x83\xe7\xbd\x91\xe7\xbb\x9c\xe7\x9a\x84\xe6\x93\x8d\xe4\xbd\x9c\xe6\x98\xaf\xe4\xb8\x80\xe6\xa0\xb7\xe7\x9a\x84\xef\xbc\x8c\xe6\x84\x9f\xe5\x85\xb4\xe8\xb6\xa3\xe7\x9a\x84\xe5\x90\x8c\xe5\xad\xa6\xe5\x8f\xaf\xe4\xbb\xa5\xe5\x8e\xbb\xe7\x9c\x8b\xe7\x9c\x8b\n\n#%%\nfrom utils import train\ntrain(net, train_data, test_data, 10, optimizer, criterion)\n\n#%% [markdown]\n# \xe8\xbf\x99\xe9\x87\x8c\xe7\x9a\x84 $\\gamma$ \xe5\x92\x8c $\\beta$ \xe9\x83\xbd\xe4\xbd\x9c\xe4\xb8\xba\xe5\x8f\x82\xe6\x95\xb0\xe8\xbf\x9b\xe8\xa1\x8c\xe8\xae\xad\xe7\xbb\x83\xef\xbc\x8c\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe4\xb8\xba\xe9\x9a\x8f\xe6\x9c\xba\xe7\x9a\x84\xe9\xab\x98\xe6\x96\xaf\xe5\x88\x86\xe5\xb8\x83\xef\xbc\x8c`moving_mean` \xe5\x92\x8c `moving_var` \xe9\x83\xbd\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe4\xb8\xba 0\xef\xbc\x8c\xe5\xb9\xb6\xe4\xb8\x8d\xe6\x98\xaf\xe6\x9b\xb4\xe6\x96\xb0\xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0\xef\xbc\x8c\xe8\xae\xad\xe7\xbb\x83\xe5\xae\x8c 10 \xe6\xac\xa1\xe4\xb9\x8b\xe5\x90\x8e\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe5\x8f\xaf\xe4\xbb\xa5\xe7\x9c\x8b\xe7\x9c\x8b\xe7\xa7\xbb\xe5\x8a\xa8\xe5\xb9\xb3\xe5\x9d\x87\xe5\x92\x8c\xe7\xa7\xbb\xe5\x8a\xa8\xe6\x96\xb9\xe5\xb7\xae\xe8\xa2\xab\xe4\xbf\xae\xe6\x94\xb9\xe4\xb8\xba\xe4\xba\x86\xe5\xa4\x9a\xe5\xb0\x91\n\n#%%\n# \xe6\x89\x93\xe5\x87\xba moving_mean \xe7\x9a\x84\xe5\x89\x8d 10 \xe9\xa1\xb9\nprint(net.moving_mean[:10])\n\n#%% [markdown]\n# \xe5\x8f\xaf\xe4\xbb\xa5\xe7\x9c\x8b\xe5\x88\xb0\xef\xbc\x8c\xe8\xbf\x99\xe4\xba\x9b\xe5\x80\xbc\xe5\xb7\xb2\xe7\xbb\x8f\xe5\x9c\xa8\xe8\xae\xad\xe7\xbb\x83\xe7\x9a\x84\xe8\xbf\x87\xe7\xa8\x8b\xe4\xb8\xad\xe8\xbf\x9b\xe8\xa1\x8c\xe4\xba\x86\xe4\xbf\xae\xe6\x94\xb9\xef\xbc\x8c\xe5\x9c\xa8\xe6\xb5\x8b\xe8\xaf\x95\xe8\xbf\x87\xe7\xa8\x8b\xe4\xb8\xad\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe4\xb8\x8d\xe9\x9c\x80\xe8\xa6\x81\xe5\x86\x8d\xe8\xae\xa1\xe7\xae\x97\xe5\x9d\x87\xe5\x80\xbc\xe5\x92\x8c\xe6\x96\xb9\xe5\xb7\xae\xef\xbc\x8c\xe7\x9b\xb4\xe6\x8e\xa5\xe4\xbd\xbf\xe7\x94\xa8\xe7\xa7\xbb\xe5\x8a\xa8\xe5\xb9\xb3\xe5\x9d\x87\xe5\x92\x8c\xe7\xa7\xbb\xe5\x8a\xa8\xe6\x96\xb9\xe5\xb7\xae\xe5\x8d\xb3\xe5\x8f\xaf\n#%% [markdown]\n# \xe4\xbd\x9c\xe4\xb8\xba\xe5\xaf\xb9\xe6\xaf\x94\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe7\x9c\x8b\xe7\x9c\x8b\xe4\xb8\x8d\xe4\xbd\xbf\xe7\x94\xa8\xe6\x89\xb9\xe6\xa0\x87\xe5\x87\x86\xe5\x8c\x96\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x9c\n\n#%%\nno_bn_net = nn.Sequential(\n    nn.Linear(784, 100),\n    nn.ReLU(True),\n    nn.Linear(100, 10)\n)\n\noptimizer = torch.optim.SGD(no_bn_net.parameters(), 1e-1) # \xe4\xbd\xbf\xe7\x94\xa8\xe9\x9a\x8f\xe6\x9c\xba\xe6\xa2\xaf\xe5\xba\xa6\xe4\xb8\x8b\xe9\x99\x8d\xef\xbc\x8c\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87 0.1\ntrain(no_bn_net, train_data, test_data, 10, optimizer, criterion)\n\n#%% [markdown]\n# \xe5\x8f\xaf\xe4\xbb\xa5\xe7\x9c\x8b\xe5\x88\xb0\xe8\x99\xbd\xe7\x84\xb6\xe6\x9c\x80\xe5\x90\x8e\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x9c\xe4\xb8\xa4\xe7\xa7\x8d\xe6\x83\x85\xe5\x86\xb5\xe4\xb8\x80\xe6\xa0\xb7\xef\xbc\x8c\xe4\xbd\x86\xe6\x98\xaf\xe5\xa6\x82\xe6\x9e\x9c\xe6\x88\x91\xe4\xbb\xac\xe7\x9c\x8b\xe5\x89\x8d\xe5\x87\xa0\xe6\xac\xa1\xe7\x9a\x84\xe6\x83\x85\xe5\x86\xb5\xef\xbc\x8c\xe5\x8f\xaf\xe4\xbb\xa5\xe7\x9c\x8b\xe5\x88\xb0\xe4\xbd\xbf\xe7\x94\xa8\xe6\x89\xb9\xe6\xa0\x87\xe5\x87\x86\xe5\x8c\x96\xe7\x9a\x84\xe6\x83\x85\xe5\x86\xb5\xe8\x83\xbd\xe5\xa4\x9f\xe6\x9b\xb4\xe5\xbf\xab\xe7\x9a\x84\xe6\x94\xb6\xe6\x95\x9b\xef\xbc\x8c\xe5\x9b\xa0\xe4\xb8\xba\xe8\xbf\x99\xe5\x8f\xaa\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa\xe5\xb0\x8f\xe7\xbd\x91\xe7\xbb\x9c\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe7\x94\xa8\xe4\xb8\x8d\xe7\x94\xa8\xe6\x89\xb9\xe6\xa0\x87\xe5\x87\x86\xe5\x8c\x96\xe9\x83\xbd\xe8\x83\xbd\xe5\xa4\x9f\xe6\x94\xb6\xe6\x95\x9b\xef\xbc\x8c\xe4\xbd\x86\xe6\x98\xaf\xe5\xaf\xb9\xe4\xba\x8e\xe6\x9b\xb4\xe5\x8a\xa0\xe6\xb7\xb1\xe7\x9a\x84\xe7\xbd\x91\xe7\xbb\x9c\xef\xbc\x8c\xe4\xbd\xbf\xe7\x94\xa8\xe6\x89\xb9\xe6\xa0\x87\xe5\x87\x86\xe5\x8c\x96\xe5\x9c\xa8\xe8\xae\xad\xe7\xbb\x83\xe7\x9a\x84\xe6\x97\xb6\xe5\x80\x99\xe8\x83\xbd\xe5\xa4\x9f\xe5\xbe\x88\xe5\xbf\xab\xe5\x9c\xb0\xe6\x94\xb6\xe6\x95\x9b\n#%% [markdown]\n# \xe4\xbb\x8e\xe4\xb8\x8a\xe9\x9d\xa2\xe5\x8f\xaf\xe4\xbb\xa5\xe7\x9c\x8b\xe5\x88\xb0\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe8\x87\xaa\xe5\xb7\xb1\xe5\xae\x9e\xe7\x8e\xb0\xe4\xba\x86 2 \xe7\xbb\xb4\xe6\x83\x85\xe5\x86\xb5\xe7\x9a\x84\xe6\x89\xb9\xe6\xa0\x87\xe5\x87\x86\xe5\x8c\x96\xef\xbc\x8c\xe5\xaf\xb9\xe5\xba\x94\xe4\xba\x8e\xe5\x8d\xb7\xe7\xa7\xaf\xe7\x9a\x84 4 \xe7\xbb\xb4\xe6\x83\x85\xe5\x86\xb5\xe7\x9a\x84\xe6\xa0\x87\xe5\x87\x86\xe5\x8c\x96\xe6\x98\xaf\xe7\xb1\xbb\xe4\xbc\xbc\xe7\x9a\x84\xef\xbc\x8c\xe5\x8f\xaa\xe9\x9c\x80\xe8\xa6\x81\xe6\xb2\xbf\xe7\x9d\x80\xe9\x80\x9a\xe9\x81\x93\xe7\x9a\x84\xe7\xbb\xb4\xe5\xba\xa6\xe8\xbf\x9b\xe8\xa1\x8c\xe5\x9d\x87\xe5\x80\xbc\xe5\x92\x8c\xe6\x96\xb9\xe5\xb7\xae\xe7\x9a\x84\xe8\xae\xa1\xe7\xae\x97\xef\xbc\x8c\xe4\xbd\x86\xe6\x98\xaf\xe6\x88\x91\xe4\xbb\xac\xe8\x87\xaa\xe5\xb7\xb1\xe5\xae\x9e\xe7\x8e\xb0\xe6\x89\xb9\xe6\xa0\x87\xe5\x87\x86\xe5\x8c\x96\xe6\x98\xaf\xe5\xbe\x88\xe7\xb4\xaf\xe7\x9a\x84\xef\xbc\x8cpytorch \xe5\xbd\x93\xe7\x84\xb6\xe4\xb9\x9f\xe4\xb8\xba\xe6\x88\x91\xe4\xbb\xac\xe5\x86\x85\xe7\xbd\xae\xe4\xba\x86\xe6\x89\xb9\xe6\xa0\x87\xe5\x87\x86\xe5\x8c\x96\xe7\x9a\x84\xe5\x87\xbd\xe6\x95\xb0\xef\xbc\x8c\xe4\xb8\x80\xe7\xbb\xb4\xe5\x92\x8c\xe4\xba\x8c\xe7\xbb\xb4\xe5\x88\x86\xe5\x88\xab\xe6\x98\xaf `torch.nn.BatchNorm1d()` \xe5\x92\x8c `torch.nn.BatchNorm2d()`\xef\xbc\x8c\xe4\xb8\x8d\xe5\x90\x8c\xe4\xba\x8e\xe6\x88\x91\xe4\xbb\xac\xe7\x9a\x84\xe5\xae\x9e\xe7\x8e\xb0\xef\xbc\x8cpytorch \xe4\xb8\x8d\xe4\xbb\x85\xe5\xb0\x86 $\\gamma$ \xe5\x92\x8c $\\beta$ \xe4\xbd\x9c\xe4\xb8\xba\xe8\xae\xad\xe7\xbb\x83\xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0\xef\xbc\x8c\xe4\xb9\x9f\xe5\xb0\x86 `moving_mean` \xe5\x92\x8c `moving_var` \xe4\xb9\x9f\xe4\xbd\x9c\xe4\xb8\xba\xe5\x8f\x82\xe6\x95\xb0\xe8\xbf\x9b\xe8\xa1\x8c\xe8\xae\xad\xe7\xbb\x83\n#%% [markdown]\n# \xe4\xb8\x8b\xe9\x9d\xa2\xe6\x88\x91\xe4\xbb\xac\xe5\x9c\xa8\xe5\x8d\xb7\xe7\xa7\xaf\xe7\xbd\x91\xe7\xbb\x9c\xe4\xb8\x8b\xe8\xaf\x95\xe7\x94\xa8\xe4\xb8\x80\xe4\xb8\x8b\xe6\x89\xb9\xe6\xa0\x87\xe5\x87\x86\xe5\x8c\x96\xe7\x9c\x8b\xe7\x9c\x8b\xe6\x95\x88\xe6\x9e\x9c\n\n#%%\ndef data_tf(x):\n    x = np.array(x, dtype='float32') / 255\n    x = (x - 0.5) / 0.5 # \xe6\x95\xb0\xe6\x8d\xae\xe9\xa2\x84\xe5\xa4\x84\xe7\x90\x86\xef\xbc\x8c\xe6\xa0\x87\xe5\x87\x86\xe5\x8c\x96\n    x = torch.from_numpy(x)\n    x = x.unsqueeze(0)\n    return x\n\ntrain_set = mnist.MNIST('./data', train=True, transform=data_tf, download=True) # \xe9\x87\x8d\xe6\x96\xb0\xe8\xbd\xbd\xe5\x85\xa5\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xef\xbc\x8c\xe7\x94\xb3\xe6\x98\x8e\xe5\xae\x9a\xe4\xb9\x89\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe5\x8f\x98\xe6\x8d\xa2\ntest_set = mnist.MNIST('./data', train=False, transform=data_tf, download=True)\ntrain_data = DataLoader(train_set, batch_size=64, shuffle=True)\ntest_data = DataLoader(test_set, batch_size=128, shuffle=False)\n\n\n#%%\n# \xe4\xbd\xbf\xe7\x94\xa8\xe6\x89\xb9\xe6\xa0\x87\xe5\x87\x86\xe5\x8c\x96\nclass conv_bn_net(nn.Module):\n    def __init__(self):\n        super(conv_bn_net, self).__init__()\n        self.stage1 = nn.Sequential(\n            nn.Conv2d(1, 6, 3, padding=1),\n            nn.BatchNorm2d(6),\n            nn.ReLU(True),\n            nn.MaxPool2d(2, 2),\n            nn.Conv2d(6, 16, 5),\n            nn.BatchNorm2d(16),\n            nn.ReLU(True),\n            nn.MaxPool2d(2, 2)\n        )\n        \n        self.classfy = nn.Linear(400, 10)\n    def forward(self, x):\n        x = self.stage1(x)\n        x = x.view(x.shape[0], -1)\n        x = self.classfy(x)\n        return x\n\nnet = conv_bn_net()\noptimizer = torch.optim.SGD(net.parameters(), 1e-1) # \xe4\xbd\xbf\xe7\x94\xa8\xe9\x9a\x8f\xe6\x9c\xba\xe6\xa2\xaf\xe5\xba\xa6\xe4\xb8\x8b\xe9\x99\x8d\xef\xbc\x8c\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87 0.1\n\n\n#%%\ntrain(net, train_data, test_data, 5, optimizer, criterion)\n\n\n#%%\n# \xe4\xb8\x8d\xe4\xbd\xbf\xe7\x94\xa8\xe6\x89\xb9\xe6\xa0\x87\xe5\x87\x86\xe5\x8c\x96\nclass conv_no_bn_net(nn.Module):\n    def __init__(self):\n        super(conv_no_bn_net, self).__init__()\n        self.stage1 = nn.Sequential(\n            nn.Conv2d(1, 6, 3, padding=1),\n            nn.ReLU(True),\n            nn.MaxPool2d(2, 2),\n            nn.Conv2d(6, 16, 5),\n            nn.ReLU(True),\n            nn.MaxPool2d(2, 2)\n        )\n        \n        self.classfy = nn.Linear(400, 10)\n    def forward(self, x):\n        x = self.stage1(x)\n        x = x.view(x.shape[0], -1)\n        x = self.classfy(x)\n        return x\n\nnet = conv_no_bn_net()\noptimizer = torch.optim.SGD(net.parameters(), 1e-1) # \xe4\xbd\xbf\xe7\x94\xa8\xe9\x9a\x8f\xe6\x9c\xba\xe6\xa2\xaf\xe5\xba\xa6\xe4\xb8\x8b\xe9\x99\x8d\xef\xbc\x8c\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87 0.1    \n\n\n#%%\ntrain(net, train_data, test_data, 5, optimizer, criterion)\n\n#%% [markdown]\n# \xe4\xb9\x8b\xe5\x90\x8e\xe4\xbb\x8b\xe7\xbb\x8d\xe4\xb8\x80\xe4\xba\x9b\xe8\x91\x97\xe5\x90\x8d\xe7\x9a\x84\xe7\xbd\x91\xe7\xbb\x9c\xe7\xbb\x93\xe6\x9e\x84\xe7\x9a\x84\xe6\x97\xb6\xe5\x80\x99\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe4\xbc\x9a\xe6\x85\xa2\xe6\x85\xa2\xe8\xae\xa4\xe8\xaf\x86\xe5\x88\xb0\xe6\x89\xb9\xe6\xa0\x87\xe5\x87\x86\xe5\x8c\x96\xe7\x9a\x84\xe9\x87\x8d\xe8\xa6\x81\xe6\x80\xa7\xef\xbc\x8c\xe4\xbd\xbf\xe7\x94\xa8 pytorch \xe8\x83\xbd\xe5\xa4\x9f\xe9\x9d\x9e\xe5\xb8\xb8\xe6\x96\xb9\xe4\xbe\xbf\xe5\x9c\xb0\xe6\xb7\xbb\xe5\x8a\xa0\xe6\x89\xb9\xe6\xa0\x87\xe5\x87\x86\xe5\x8c\x96\xe5\xb1\x82\n\n"""
7.Convolutional_Neural_Network(CNN) 卷积神经网络/cifar10-oldversion.py,11,"b""'''Train CIFAR10 with PyTorch.'''\nfrom __future__ import print_function\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torch.backends.cudnn as cudnn\n\nimport torchvision\nimport torchvision.transforms as transforms\n\nimport os\nimport argparse\n\nfrom model import *\nfrom utils import progress_bar\nfrom torch.autograd import Variable\n\nparser = argparse.ArgumentParser(description='PyTorch CIFAR10 Training')\nparser.add_argument('--lr', default=0.1, type=float, help='learning rate')\nparser.add_argument('--resume', '-r', action='store_true', help='resume from checkpoint')\nargs = parser.parse_args()\n\n\nbest_acc = 0  # best test accuracy\nstart_epoch = 0  # start from epoch 0 or last checkpoint epoch\n\n# Data\nprint('==> Preparing data..')\ntransform_train = transforms.Compose([\n    transforms.RandomCrop(32, padding=4),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)), # <<-- the original repository is wrong!!!\n])\n\ntransform_test = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)), # <<-- the original repository is wrong!!!\n])\n\ntrainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n\ntestset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n\nclasses = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n\n# Model\nuse_cuda=False\nif args.resume:\n    # Load checkpoint.\n    print('==> Resuming from checkpoint..')\n    assert os.path.isdir('checkpoint'), 'Error: no checkpoint directory found!'\n    checkpoint = torch.load('./checkpoint/ckpt.t7')\n    net = checkpoint['net']\n    best_acc = checkpoint['acc']\n    start_epoch = checkpoint['epoch']\nelse:\n    print('==> Building model..')\n    # net = VGG('VGG19')\n    net = ResNet18()\n    # net = PreActResNet18()\n    # net = GoogLeNet()\n    #net = DenseNet121()\n    # net = ResNeXt29_2x64d()\n    # net = MobileNet()\n    # net = DPN92()\n    # net = ShuffleNetG2()\n    # net = SENet18()\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(net.parameters(), lr=args.lr, momentum=0.9, weight_decay=5e-4)\n\n# Training\ndef train(epoch):\n    print('\\nEpoch: %d' % epoch)\n    net.train()\n    train_loss = 0\n    correct = 0\n    total = 0\n    for batch_idx, (inputs, targets) in enumerate(trainloader):\n        optimizer.zero_grad()\n        inputs, targets = Variable(inputs), Variable(targets)\n        outputs = net(inputs)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n\n        #train_loss += loss.data[0]\n        train_loss+=loss.item()\n        _, predicted = torch.max(outputs.data, 1)\n        total += targets.size(0)\n        correct += predicted.eq(targets.data).cpu().sum()\n\n        progress_bar(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n                     % (train_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n\ndef test(epoch):\n    global best_acc\n    net.eval()\n    test_loss = 0\n    correct = 0\n    total = 0\n    for batch_idx, (inputs, targets) in enumerate(testloader):\n        inputs, targets = Variable(inputs), Variable(targets)\n        outputs = net(inputs)\n        loss = criterion(outputs, targets)\n\n        #test_loss += loss.data[0]\n        test_loss+=loss.item()\n        _, predicted = torch.max(outputs.data, 1)\n        total += targets.size(0)\n        correct += predicted.eq(targets.data).cpu().sum()\n\n        progress_bar(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n                     % (test_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n        # Save checkpoint.\n        acc = 100. * correct / total\n        if acc > best_acc:\n            print('Saving..')\n            state = {\n                'net': net.module if use_cuda else net,\n                'acc': acc,\n                'epoch': epoch,\n            }\n            if not os.path.isdir('checkpoint'):\n                os.mkdir('checkpoint')\n            torch.save(state, './checkpoint/ckpt.t7')\n            best_acc = acc\n\nfor epoch in range(start_epoch, start_epoch+1):\n    train(epoch)\n    test(epoch)"""
7.Convolutional_Neural_Network(CNN) 卷积神经网络/conv_net_py_torch-oldversion.py,7,"b""import torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nimport torchvision.transforms as transforms\nimport torchvision.datasets\nfrom bokeh.plotting import figure\nfrom bokeh.io import show\nfrom bokeh.models import LinearAxis, Range1d\nimport numpy as np\n\n\n# Hyperparameters\nnum_epochs = 6\nnum_classes = 10\nbatch_size = 100\nlearning_rate = 0.001\n\nDATA_PATH = 'C:\\\\Users\\Andy\\PycharmProjects\\MNISTData'\nMODEL_STORE_PATH = 'C:\\\\Users\\Andy\\PycharmProjects\\pytorch_models\\\\'\n\n# transforms to apply to the data\ntrans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n\n# MNIST dataset\ntrain_dataset = torchvision.datasets.MNIST(root=DATA_PATH, train=True, transform=trans, download=True)\ntest_dataset = torchvision.datasets.MNIST(root=DATA_PATH, train=False, transform=trans)\n\n# Data loader\ntrain_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n\n# Convolutional neural network (two convolutional layers)\nclass ConvNet(nn.Module):\n    def __init__(self):\n        super(ConvNet, self).__init__()\n        self.layer1 = nn.Sequential(\n            nn.Conv2d(1, 32, kernel_size=5, stride=1, padding=2),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2))\n        self.layer2 = nn.Sequential(\n            nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2))\n        self.drop_out = nn.Dropout()\n        self.fc1 = nn.Linear(7 * 7 * 64, 1000)\n        self.fc2 = nn.Linear(1000, 10)\n\n    def forward(self, x):\n        out = self.layer1(x)\n        out = self.layer2(out)\n        out = out.reshape(out.size(0), -1)\n        out = self.drop_out(out)\n        out = self.fc1(out)\n        out = self.fc2(out)\n        return out\n\n\nmodel = ConvNet()\n\n# Loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\n# Train the model\ntotal_step = len(train_loader)\nloss_list = []\nacc_list = []\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        # Run the forward pass\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss_list.append(loss.item())\n\n        # Backprop and perform Adam optimisation\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # Track the accuracy\n        total = labels.size(0)\n        _, predicted = torch.max(outputs.data, 1)\n        correct = (predicted == labels).sum().item()\n        acc_list.append(correct / total)\n\n        if (i + 1) % 100 == 0:\n            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'\n                  .format(epoch + 1, num_epochs, i + 1, total_step, loss.item(),\n                          (correct / total) * 100))\n\n# Test the model\nmodel.eval()\nwith torch.no_grad():\n    correct = 0\n    total = 0\n    for images, labels in test_loader:\n        outputs = model(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n    print('Test Accuracy of the model on the 10000 test images: {} %'.format((correct / total) * 100))\n\n# Save the model and plot\ntorch.save(model.state_dict(), MODEL_STORE_PATH + 'conv_net_model.ckpt')\n\np = figure(y_axis_label='Loss', width=850, y_range=(0, 1), title='PyTorch ConvNet results')\np.extra_y_ranges = {'Accuracy': Range1d(start=0, end=100)}\np.add_layout(LinearAxis(y_range_name='Accuracy', axis_label='Accuracy (%)'), 'right')\np.line(np.arange(len(loss_list)), loss_list)\np.line(np.arange(len(loss_list)), np.array(acc_list) * 100, y_range_name='Accuracy', color='red')\nshow(p)\n"""
7.Convolutional_Neural_Network(CNN) 卷积神经网络/data-augumentation 数据增强.py,8,"b""#%% [markdown]\n# # \xe6\x95\xb0\xe6\x8d\xae\xe5\xa2\x9e\xe5\xbc\xba\n# \xe5\x89\x8d\xe9\x9d\xa2\xe6\x88\x91\xe4\xbb\xac\xe5\xb7\xb2\xe7\xbb\x8f\xe8\xae\xb2\xe4\xba\x86\xe5\x87\xa0\xe4\xb8\xaa\xe9\x9d\x9e\xe5\xb8\xb8\xe8\x91\x97\xe5\x90\x8d\xe7\x9a\x84\xe5\x8d\xb7\xe7\xa7\xaf\xe7\xbd\x91\xe7\xbb\x9c\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x84\xef\xbc\x8c\xe4\xbd\x86\xe6\x98\xaf\xe5\x8d\x95\xe5\x8d\x95\xe5\x8f\xaa\xe9\x9d\xa0\xe8\xbf\x99\xe4\xba\x9b\xe7\xbd\x91\xe7\xbb\x9c\xe5\xb9\xb6\xe4\xb8\x8d\xe8\x83\xbd\xe5\x8f\x96\xe5\xbe\x97 state-of-the-art \xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x9c\xef\xbc\x8c\xe7\x8e\xb0\xe5\xae\x9e\xe9\x97\xae\xe9\xa2\x98\xe5\xbe\x80\xe5\xbe\x80\xe6\x9b\xb4\xe5\x8a\xa0\xe5\xa4\x8d\xe6\x9d\x82\xef\xbc\x8c\xe9\x9d\x9e\xe5\xb8\xb8\xe5\xae\xb9\xe6\x98\x93\xe5\x87\xba\xe7\x8e\xb0\xe8\xbf\x87\xe6\x8b\x9f\xe5\x90\x88\xe7\x9a\x84\xe9\x97\xae\xe9\xa2\x98\xef\xbc\x8c\xe8\x80\x8c\xe6\x95\xb0\xe6\x8d\xae\xe5\xa2\x9e\xe5\xbc\xba\xe7\x9a\x84\xe6\x96\xb9\xe6\xb3\x95\xe6\x98\xaf\xe5\xaf\xb9\xe6\x8a\x97\xe8\xbf\x87\xe6\x8b\x9f\xe5\x90\x88\xe9\x97\xae\xe9\xa2\x98\xe7\x9a\x84\xe4\xb8\x80\xe4\xb8\xaa\xe9\x87\x8d\xe8\xa6\x81\xe6\x96\xb9\xe6\xb3\x95\xe3\x80\x82\n# \n# 2012 \xe5\xb9\xb4 AlexNet \xe5\x9c\xa8 ImageNet \xe4\xb8\x8a\xe5\xa4\xa7\xe8\x8e\xb7\xe5\x85\xa8\xe8\x83\x9c\xef\xbc\x8c\xe5\x9b\xbe\xe7\x89\x87\xe5\xa2\x9e\xe5\xbc\xba\xe6\x96\xb9\xe6\xb3\x95\xe5\x8a\x9f\xe4\xb8\x8d\xe5\x8f\xaf\xe6\xb2\xa1\xef\xbc\x8c\xe5\x9b\xa0\xe4\xb8\xba\xe6\x9c\x89\xe4\xba\x86\xe5\x9b\xbe\xe7\x89\x87\xe5\xa2\x9e\xe5\xbc\xba\xef\xbc\x8c\xe4\xbd\xbf\xe5\xbe\x97\xe8\xae\xad\xe7\xbb\x83\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe6\xaf\x94\xe5\xae\x9e\xe9\x99\x85\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe5\xa4\x9a\xe4\xba\x86\xe5\xbe\x88\xe5\xa4\x9a'\xe6\x96\xb0'\xe6\xa0\xb7\xe6\x9c\xac\xef\xbc\x8c\xe5\x87\x8f\xe5\xb0\x91\xe4\xba\x86\xe8\xbf\x87\xe6\x8b\x9f\xe5\x90\x88\xe7\x9a\x84\xe9\x97\xae\xe9\xa2\x98\xef\xbc\x8c\xe4\xb8\x8b\xe9\x9d\xa2\xe6\x88\x91\xe4\xbb\xac\xe6\x9d\xa5\xe5\x85\xb7\xe4\xbd\x93\xe8\xa7\xa3\xe9\x87\x8a\xe4\xb8\x80\xe4\xb8\x8b\xe3\x80\x82\n#%% [markdown]\n# ## \xe5\xb8\xb8\xe7\x94\xa8\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe5\xa2\x9e\xe5\xbc\xba\xe6\x96\xb9\xe6\xb3\x95\n# \xe5\xb8\xb8\xe7\x94\xa8\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe5\xa2\x9e\xe5\xbc\xba\xe6\x96\xb9\xe6\xb3\x95\xe5\xa6\x82\xe4\xb8\x8b\xef\xbc\x9a  \n# 1.\xe5\xaf\xb9\xe5\x9b\xbe\xe7\x89\x87\xe8\xbf\x9b\xe8\xa1\x8c\xe4\xb8\x80\xe5\xae\x9a\xe6\xaf\x94\xe4\xbe\x8b\xe7\xbc\xa9\xe6\x94\xbe  \n# 2.\xe5\xaf\xb9\xe5\x9b\xbe\xe7\x89\x87\xe8\xbf\x9b\xe8\xa1\x8c\xe9\x9a\x8f\xe6\x9c\xba\xe4\xbd\x8d\xe7\xbd\xae\xe7\x9a\x84\xe6\x88\xaa\xe5\x8f\x96   \n# 3.\xe5\xaf\xb9\xe5\x9b\xbe\xe7\x89\x87\xe8\xbf\x9b\xe8\xa1\x8c\xe9\x9a\x8f\xe6\x9c\xba\xe7\x9a\x84\xe6\xb0\xb4\xe5\xb9\xb3\xe5\x92\x8c\xe7\xab\x96\xe7\x9b\xb4\xe7\xbf\xbb\xe8\xbd\xac  \n# 4.\xe5\xaf\xb9\xe5\x9b\xbe\xe7\x89\x87\xe8\xbf\x9b\xe8\xa1\x8c\xe9\x9a\x8f\xe6\x9c\xba\xe8\xa7\x92\xe5\xba\xa6\xe7\x9a\x84\xe6\x97\x8b\xe8\xbd\xac  \n# 5.\xe5\xaf\xb9\xe5\x9b\xbe\xe7\x89\x87\xe8\xbf\x9b\xe8\xa1\x8c\xe4\xba\xae\xe5\xba\xa6\xe3\x80\x81\xe5\xaf\xb9\xe6\xaf\x94\xe5\xba\xa6\xe5\x92\x8c\xe9\xa2\x9c\xe8\x89\xb2\xe7\x9a\x84\xe9\x9a\x8f\xe6\x9c\xba\xe5\x8f\x98\xe5\x8c\x96\n# \n# \xe8\xbf\x99\xe4\xba\x9b\xe6\x96\xb9\xe6\xb3\x95 pytorch \xe9\x83\xbd\xe5\xb7\xb2\xe7\xbb\x8f\xe4\xb8\xba\xe6\x88\x91\xe4\xbb\xac\xe5\x86\x85\xe7\xbd\xae\xe5\x9c\xa8\xe4\xba\x86 torchvision \xe9\x87\x8c\xe9\x9d\xa2\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe5\x9c\xa8\xe5\xae\x89\xe8\xa3\x85 pytorch \xe7\x9a\x84\xe6\x97\xb6\xe5\x80\x99\xe4\xb9\x9f\xe5\xae\x89\xe8\xa3\x85\xe4\xba\x86 torchvision\xef\xbc\x8c\xe4\xb8\x8b\xe9\x9d\xa2\xe6\x88\x91\xe4\xbb\xac\xe6\x9d\xa5\xe4\xbe\x9d\xe6\xac\xa1\xe5\xb1\x95\xe7\xa4\xba\xe4\xb8\x80\xe4\xb8\x8b\xe8\xbf\x99\xe4\xba\x9b\xe6\x95\xb0\xe6\x8d\xae\xe5\xa2\x9e\xe5\xbc\xba\xe6\x96\xb9\xe6\xb3\x95\n\n#%%\nfrom PIL import Image\nfrom torchvision import transforms as tfs\n\n\n#%%\n# \xe8\xaf\xbb\xe5\x85\xa5\xe4\xb8\x80\xe5\xbc\xa0\xe5\x9b\xbe\xe7\x89\x87\nim = Image.open('./cat.png')\nim\n\n#%% [markdown]\n# ### \xe9\x9a\x8f\xe6\x9c\xba\xe6\xaf\x94\xe4\xbe\x8b\xe6\x94\xbe\xe7\xbc\xa9\n# \xe9\x9a\x8f\xe6\x9c\xba\xe6\xaf\x94\xe4\xbe\x8b\xe7\xbc\xa9\xe6\x94\xbe\xe4\xb8\xbb\xe8\xa6\x81\xe4\xbd\xbf\xe7\x94\xa8\xe7\x9a\x84\xe6\x98\xaf `torchvision.transforms.Resize()` \xe8\xbf\x99\xe4\xb8\xaa\xe5\x87\xbd\xe6\x95\xb0\xef\xbc\x8c\xe7\xac\xac\xe4\xb8\x80\xe4\xb8\xaa\xe5\x8f\x82\xe6\x95\xb0\xe5\x8f\xaf\xe4\xbb\xa5\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa\xe6\x95\xb4\xe6\x95\xb0\xef\xbc\x8c\xe9\x82\xa3\xe4\xb9\x88\xe5\x9b\xbe\xe7\x89\x87\xe4\xbc\x9a\xe4\xbf\x9d\xe5\xad\x98\xe7\x8e\xb0\xe5\x9c\xa8\xe7\x9a\x84\xe5\xae\xbd\xe5\x92\x8c\xe9\xab\x98\xe7\x9a\x84\xe6\xaf\x94\xe4\xbe\x8b\xef\xbc\x8c\xe5\xb9\xb6\xe5\xb0\x86\xe6\x9b\xb4\xe7\x9f\xad\xe7\x9a\x84\xe8\xbe\xb9\xe7\xbc\xa9\xe6\x94\xbe\xe5\x88\xb0\xe8\xbf\x99\xe4\xb8\xaa\xe6\x95\xb4\xe6\x95\xb0\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f\xef\xbc\x8c\xe7\xac\xac\xe4\xb8\x80\xe4\xb8\xaa\xe5\x8f\x82\xe6\x95\xb0\xe4\xb9\x9f\xe5\x8f\xaf\xe4\xbb\xa5\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa tuple\xef\xbc\x8c\xe9\x82\xa3\xe4\xb9\x88\xe5\x9b\xbe\xe7\x89\x87\xe4\xbc\x9a\xe7\x9b\xb4\xe6\x8e\xa5\xe6\x8a\x8a\xe5\xae\xbd\xe5\x92\x8c\xe9\xab\x98\xe7\xbc\xa9\xe6\x94\xbe\xe5\x88\xb0\xe8\xbf\x99\xe4\xb8\xaa\xe5\xa4\xa7\xe5\xb0\x8f\xef\xbc\x9b\xe7\xac\xac\xe4\xba\x8c\xe4\xb8\xaa\xe5\x8f\x82\xe6\x95\xb0\xe8\xa1\xa8\xe7\xa4\xba\xe6\x94\xbe\xe7\xbc\xa9\xe5\x9b\xbe\xe7\x89\x87\xe4\xbd\xbf\xe7\x94\xa8\xe7\x9a\x84\xe6\x96\xb9\xe6\xb3\x95\xef\xbc\x8c\xe6\xaf\x94\xe5\xa6\x82\xe6\x9c\x80\xe9\x82\xbb\xe8\xbf\x91\xe6\xb3\x95\xef\xbc\x8c\xe6\x88\x96\xe8\x80\x85\xe5\x8f\x8c\xe7\xba\xbf\xe6\x80\xa7\xe5\xb7\xae\xe5\x80\xbc\xe7\xad\x89\xef\xbc\x8c\xe4\xb8\x80\xe8\x88\xac\xe5\x8f\x8c\xe7\xba\xbf\xe6\x80\xa7\xe5\xb7\xae\xe5\x80\xbc\xe8\x83\xbd\xe5\xa4\x9f\xe4\xbf\x9d\xe7\x95\x99\xe5\x9b\xbe\xe7\x89\x87\xe6\x9b\xb4\xe5\xa4\x9a\xe7\x9a\x84\xe4\xbf\xa1\xe6\x81\xaf\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5 pytorch \xe9\xbb\x98\xe8\xae\xa4\xe4\xbd\xbf\xe7\x94\xa8\xe7\x9a\x84\xe6\x98\xaf\xe5\x8f\x8c\xe7\xba\xbf\xe6\x80\xa7\xe5\xb7\xae\xe5\x80\xbc\xef\xbc\x8c\xe4\xbd\xa0\xe5\x8f\xaf\xe4\xbb\xa5\xe6\x89\x8b\xe5\x8a\xa8\xe5\x8e\xbb\xe6\x94\xb9\xe8\xbf\x99\xe4\xb8\xaa\xe5\x8f\x82\xe6\x95\xb0\xef\xbc\x8c\xe6\x9b\xb4\xe5\xa4\x9a\xe7\x9a\x84\xe4\xbf\xa1\xe6\x81\xaf\xe5\x8f\xaf\xe4\xbb\xa5\xe7\x9c\x8b\xe7\x9c\x8b[\xe6\x96\x87\xe6\xa1\xa3](http://pytorch.org/docs/0.3.0/torchvision/transforms.html)\n\n#%%\n# \xe6\xaf\x94\xe4\xbe\x8b\xe7\xbc\xa9\xe6\x94\xbe\nprint('before scale, shape: {}'.format(im.size))\nnew_im = tfs.Resize((100, 200))(im)\nprint('after scale, shape: {}'.format(new_im.size))\nnew_im\n\n#%% [markdown]\n# ### \xe9\x9a\x8f\xe6\x9c\xba\xe4\xbd\x8d\xe7\xbd\xae\xe6\x88\xaa\xe5\x8f\x96\n# \xe9\x9a\x8f\xe6\x9c\xba\xe4\xbd\x8d\xe7\xbd\xae\xe6\x88\xaa\xe5\x8f\x96\xe8\x83\xbd\xe5\xa4\x9f\xe6\x8f\x90\xe5\x8f\x96\xe5\x87\xba\xe5\x9b\xbe\xe7\x89\x87\xe4\xb8\xad\xe5\xb1\x80\xe9\x83\xa8\xe7\x9a\x84\xe4\xbf\xa1\xe6\x81\xaf\xef\xbc\x8c\xe4\xbd\xbf\xe5\xbe\x97\xe7\xbd\x91\xe7\xbb\x9c\xe6\x8e\xa5\xe5\x8f\x97\xe7\x9a\x84\xe8\xbe\x93\xe5\x85\xa5\xe5\x85\xb7\xe6\x9c\x89\xe5\xa4\x9a\xe5\xb0\xba\xe5\xba\xa6\xe7\x9a\x84\xe7\x89\xb9\xe5\xbe\x81\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe8\x83\xbd\xe5\xa4\x9f\xe6\x9c\x89\xe8\xbe\x83\xe5\xa5\xbd\xe7\x9a\x84\xe6\x95\x88\xe6\x9e\x9c\xe3\x80\x82\xe5\x9c\xa8 torchvision \xe4\xb8\xad\xe4\xb8\xbb\xe8\xa6\x81\xe6\x9c\x89\xe4\xb8\x8b\xe9\x9d\xa2\xe4\xb8\xa4\xe7\xa7\x8d\xe6\x96\xb9\xe5\xbc\x8f\xef\xbc\x8c\xe4\xb8\x80\xe4\xb8\xaa\xe6\x98\xaf `torchvision.transforms.RandomCrop()`\xef\xbc\x8c\xe4\xbc\xa0\xe5\x85\xa5\xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0\xe5\xb0\xb1\xe6\x98\xaf\xe6\x88\xaa\xe5\x8f\x96\xe5\x87\xba\xe7\x9a\x84\xe5\x9b\xbe\xe7\x89\x87\xe7\x9a\x84\xe9\x95\xbf\xe5\x92\x8c\xe5\xae\xbd\xef\xbc\x8c\xe5\xaf\xb9\xe5\x9b\xbe\xe7\x89\x87\xe5\x9c\xa8\xe9\x9a\x8f\xe6\x9c\xba\xe4\xbd\x8d\xe7\xbd\xae\xe8\xbf\x9b\xe8\xa1\x8c\xe6\x88\xaa\xe5\x8f\x96\xef\xbc\x9b\xe7\xac\xac\xe4\xba\x8c\xe4\xb8\xaa\xe6\x98\xaf `torchvision.transforms.CenterCrop()`\xef\xbc\x8c\xe5\x90\x8c\xe6\xa0\xb7\xe4\xbc\xa0\xe5\x85\xa5\xe4\xbb\x8b\xe6\x9b\xb2\xe5\x88\x9d\xe7\x9a\x84\xe5\x9b\xbe\xe7\x89\x87\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f\xe4\xbd\x9c\xe4\xb8\xba\xe5\x8f\x82\xe6\x95\xb0\xef\xbc\x8c\xe4\xbc\x9a\xe5\x9c\xa8\xe5\x9b\xbe\xe7\x89\x87\xe7\x9a\x84\xe4\xb8\xad\xe5\xbf\x83\xe8\xbf\x9b\xe8\xa1\x8c\xe6\x88\xaa\xe5\x8f\x96\n\n#%%\n# \xe9\x9a\x8f\xe6\x9c\xba\xe8\xa3\x81\xe5\x89\xaa\xe5\x87\xba 100 x 100 \xe7\x9a\x84\xe5\x8c\xba\xe5\x9f\x9f\nrandom_im1 = tfs.RandomCrop(100)(im)\nrandom_im1\n\n\n#%%\n# \xe9\x9a\x8f\xe6\x9c\xba\xe8\xa3\x81\xe5\x89\xaa\xe5\x87\xba 150 x 100 \xe7\x9a\x84\xe5\x8c\xba\xe5\x9f\x9f\nrandom_im2 = tfs.RandomCrop((150, 100))(im)\nrandom_im2\n\n\n#%%\n# \xe4\xb8\xad\xe5\xbf\x83\xe8\xa3\x81\xe5\x89\xaa\xe5\x87\xba 100 x 100 \xe7\x9a\x84\xe5\x8c\xba\xe5\x9f\x9f\ncenter_im = tfs.CenterCrop(100)(im)\ncenter_im\n\n#%% [markdown]\n# ### \xe9\x9a\x8f\xe6\x9c\xba\xe7\x9a\x84\xe6\xb0\xb4\xe5\xb9\xb3\xe5\x92\x8c\xe7\xab\x96\xe7\x9b\xb4\xe6\x96\xb9\xe5\x90\x91\xe7\xbf\xbb\xe8\xbd\xac\n# \xe5\xaf\xb9\xe4\xba\x8e\xe4\xb8\x8a\xe9\x9d\xa2\xe8\xbf\x99\xe4\xb8\x80\xe5\xbc\xa0\xe7\x8c\xab\xe7\x9a\x84\xe5\x9b\xbe\xe7\x89\x87\xef\xbc\x8c\xe5\xa6\x82\xe6\x9e\x9c\xe6\x88\x91\xe4\xbb\xac\xe5\xb0\x86\xe5\xae\x83\xe7\xbf\xbb\xe8\xbd\xac\xe4\xb8\x80\xe4\xb8\x8b\xef\xbc\x8c\xe5\xae\x83\xe4\xbb\x8d\xe7\x84\xb6\xe6\x98\xaf\xe4\xb8\x80\xe5\xbc\xa0\xe7\x8c\xab\xef\xbc\x8c\xe4\xbd\x86\xe6\x98\xaf\xe5\x9b\xbe\xe7\x89\x87\xe5\xb0\xb1\xe6\x9c\x89\xe4\xba\x86\xe6\x9b\xb4\xe5\xa4\x9a\xe7\x9a\x84\xe5\xa4\x9a\xe6\xa0\xb7\xe6\x80\xa7\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe9\x9a\x8f\xe6\x9c\xba\xe7\xbf\xbb\xe8\xbd\xac\xe4\xb9\x9f\xe6\x98\xaf\xe4\xb8\x80\xe7\xa7\x8d\xe9\x9d\x9e\xe5\xb8\xb8\xe6\x9c\x89\xe6\x95\x88\xe7\x9a\x84\xe6\x89\x8b\xe6\xae\xb5\xe3\x80\x82\xe5\x9c\xa8 torchvision \xe4\xb8\xad\xef\xbc\x8c\xe9\x9a\x8f\xe6\x9c\xba\xe7\xbf\xbb\xe8\xbd\xac\xe4\xbd\xbf\xe7\x94\xa8\xe7\x9a\x84\xe6\x98\xaf `torchvision.transforms.RandomHorizontalFlip()` \xe5\x92\x8c `torchvision.transforms.RandomVerticalFlip()`\n\n#%%\n# \xe9\x9a\x8f\xe6\x9c\xba\xe6\xb0\xb4\xe5\xb9\xb3\xe7\xbf\xbb\xe8\xbd\xac\nh_filp = tfs.RandomHorizontalFlip(p=1)(im)\nh_filp\n\n\n#%%\n# \xe9\x9a\x8f\xe6\x9c\xba\xe7\xab\x96\xe7\x9b\xb4\xe7\xbf\xbb\xe8\xbd\xac\nv_flip = tfs.RandomVerticalFlip()(im)\nv_flip\n\n#%% [markdown]\n# ### \xe9\x9a\x8f\xe6\x9c\xba\xe8\xa7\x92\xe5\xba\xa6\xe6\x97\x8b\xe8\xbd\xac\n# \xe4\xb8\x80\xe4\xba\x9b\xe8\xa7\x92\xe5\xba\xa6\xe7\x9a\x84\xe6\x97\x8b\xe8\xbd\xac\xe4\xbb\x8d\xe7\x84\xb6\xe6\x98\xaf\xe9\x9d\x9e\xe5\xb8\xb8\xe6\x9c\x89\xe7\x94\xa8\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe5\xa2\x9e\xe5\xbc\xba\xe6\x96\xb9\xe5\xbc\x8f\xef\xbc\x8c\xe5\x9c\xa8 torchvision \xe4\xb8\xad\xef\xbc\x8c\xe4\xbd\xbf\xe7\x94\xa8 `torchvision.transforms.RandomRotation()` \xe6\x9d\xa5\xe5\xae\x9e\xe7\x8e\xb0\xef\xbc\x8c\xe5\x85\xb6\xe4\xb8\xad\xe7\xac\xac\xe4\xb8\x80\xe4\xb8\xaa\xe5\x8f\x82\xe6\x95\xb0\xe5\xb0\xb1\xe6\x98\xaf\xe9\x9a\x8f\xe6\x9c\xba\xe6\x97\x8b\xe8\xbd\xac\xe7\x9a\x84\xe8\xa7\x92\xe5\xba\xa6\xef\xbc\x8c\xe6\xaf\x94\xe5\xa6\x82\xe5\xa1\xab\xe5\x85\xa5 10\xef\xbc\x8c\xe9\x82\xa3\xe4\xb9\x88\xe6\xaf\x8f\xe6\xac\xa1\xe5\x9b\xbe\xe7\x89\x87\xe5\xb0\xb1\xe4\xbc\x9a\xe5\x9c\xa8 -10 ~ 10 \xe5\xba\xa6\xe4\xb9\x8b\xe9\x97\xb4\xe9\x9a\x8f\xe6\x9c\xba\xe6\x97\x8b\xe8\xbd\xac\n\n#%%\nrot_im = tfs.RandomRotation(45)(im)\nrot_im\n\n#%% [markdown]\n# ### \xe4\xba\xae\xe5\xba\xa6\xe3\x80\x81\xe5\xaf\xb9\xe6\xaf\x94\xe5\xba\xa6\xe5\x92\x8c\xe9\xa2\x9c\xe8\x89\xb2\xe7\x9a\x84\xe5\x8f\x98\xe5\x8c\x96\n# \xe9\x99\xa4\xe4\xba\x86\xe5\xbd\xa2\xe7\x8a\xb6\xe5\x8f\x98\xe5\x8c\x96\xe5\xa4\x96\xef\xbc\x8c\xe9\xa2\x9c\xe8\x89\xb2\xe5\x8f\x98\xe5\x8c\x96\xe5\x8f\x88\xe6\x98\xaf\xe5\x8f\xa6\xe5\xa4\x96\xe4\xb8\x80\xe7\xa7\x8d\xe5\xa2\x9e\xe5\xbc\xba\xe6\x96\xb9\xe5\xbc\x8f\xef\xbc\x8c\xe5\x85\xb6\xe4\xb8\xad\xe5\x8f\xaf\xe4\xbb\xa5\xe8\xae\xbe\xe7\xbd\xae\xe4\xba\xae\xe5\xba\xa6\xe5\x8f\x98\xe5\x8c\x96\xef\xbc\x8c\xe5\xaf\xb9\xe6\xaf\x94\xe5\xba\xa6\xe5\x8f\x98\xe5\x8c\x96\xe5\x92\x8c\xe9\xa2\x9c\xe8\x89\xb2\xe5\x8f\x98\xe5\x8c\x96\xe7\xad\x89\xef\xbc\x8c\xe5\x9c\xa8 torchvision \xe4\xb8\xad\xe4\xb8\xbb\xe8\xa6\x81\xe4\xbd\xbf\xe7\x94\xa8 `torchvision.transforms.ColorJitter()` \xe6\x9d\xa5\xe5\xae\x9e\xe7\x8e\xb0\xe7\x9a\x84\xef\xbc\x8c\xe7\xac\xac\xe4\xb8\x80\xe4\xb8\xaa\xe5\x8f\x82\xe6\x95\xb0\xe5\xb0\xb1\xe6\x98\xaf\xe4\xba\xae\xe5\xba\xa6\xe7\x9a\x84\xe6\xaf\x94\xe4\xbe\x8b\xef\xbc\x8c\xe7\xac\xac\xe4\xba\x8c\xe4\xb8\xaa\xe6\x98\xaf\xe5\xaf\xb9\xe6\xaf\x94\xe5\xba\xa6\xef\xbc\x8c\xe7\xac\xac\xe4\xb8\x89\xe4\xb8\xaa\xe6\x98\xaf\xe9\xa5\xb1\xe5\x92\x8c\xe5\xba\xa6\xef\xbc\x8c\xe7\xac\xac\xe5\x9b\x9b\xe4\xb8\xaa\xe6\x98\xaf\xe9\xa2\x9c\xe8\x89\xb2\n\n#%%\n# \xe4\xba\xae\xe5\xba\xa6\nbright_im = tfs.ColorJitter(brightness=1)(im) # \xe9\x9a\x8f\xe6\x9c\xba\xe4\xbb\x8e 0 ~ 2 \xe4\xb9\x8b\xe9\x97\xb4\xe4\xba\xae\xe5\xba\xa6\xe5\x8f\x98\xe5\x8c\x96\xef\xbc\x8c1 \xe8\xa1\xa8\xe7\xa4\xba\xe5\x8e\x9f\xe5\x9b\xbe\nbright_im\n\n\n#%%\n# \xe5\xaf\xb9\xe6\xaf\x94\xe5\xba\xa6\ncontrast_im = tfs.ColorJitter(contrast=1)(im) # \xe9\x9a\x8f\xe6\x9c\xba\xe4\xbb\x8e 0 ~ 2 \xe4\xb9\x8b\xe9\x97\xb4\xe5\xaf\xb9\xe6\xaf\x94\xe5\xba\xa6\xe5\x8f\x98\xe5\x8c\x96\xef\xbc\x8c1 \xe8\xa1\xa8\xe7\xa4\xba\xe5\x8e\x9f\xe5\x9b\xbe\ncontrast_im\n\n\n#%%\n# \xe9\xa2\x9c\xe8\x89\xb2\ncolor_im = tfs.ColorJitter(hue=0.5)(im) # \xe9\x9a\x8f\xe6\x9c\xba\xe4\xbb\x8e -0.5 ~ 0.5 \xe4\xb9\x8b\xe9\x97\xb4\xe5\xaf\xb9\xe9\xa2\x9c\xe8\x89\xb2\xe5\x8f\x98\xe5\x8c\x96\ncolor_im\n\n#%% [markdown]\n# \n# \n# \xe4\xb8\x8a\xe9\x9d\xa2\xe6\x88\x91\xe4\xbb\xac\xe8\xae\xb2\xe4\xba\x86\xe8\xbf\x99\xe4\xb9\x88\xe5\x9b\xbe\xe7\x89\x87\xe5\xa2\x9e\xe5\xbc\xba\xe7\x9a\x84\xe6\x96\xb9\xe6\xb3\x95\xef\xbc\x8c\xe5\x85\xb6\xe5\xae\x9e\xe8\xbf\x99\xe4\xba\x9b\xe6\x96\xb9\xe6\xb3\x95\xe9\x83\xbd\xe4\xb8\x8d\xe6\x98\xaf\xe5\xad\xa4\xe7\xab\x8b\xe8\xb5\xb7\xe6\x9d\xa5\xe7\x94\xa8\xe7\x9a\x84\xef\xbc\x8c\xe5\x8f\xaf\xe4\xbb\xa5\xe8\x81\x94\xe5\x90\x88\xe8\xb5\xb7\xe6\x9d\xa5\xe7\x94\xa8\xef\xbc\x8c\xe6\xaf\x94\xe5\xa6\x82\xe5\x85\x88\xe5\x81\x9a\xe9\x9a\x8f\xe6\x9c\xba\xe7\xbf\xbb\xe8\xbd\xac\xef\xbc\x8c\xe7\x84\xb6\xe5\x90\x8e\xe9\x9a\x8f\xe6\x9c\xba\xe6\x88\xaa\xe5\x8f\x96\xef\xbc\x8c\xe5\x86\x8d\xe5\x81\x9a\xe5\xaf\xb9\xe6\xaf\x94\xe5\xba\xa6\xe5\xa2\x9e\xe5\xbc\xba\xe7\xad\x89\xe7\xad\x89\xef\xbc\x8ctorchvision \xe9\x87\x8c\xe9\x9d\xa2\xe6\x9c\x89\xe4\xb8\xaa\xe9\x9d\x9e\xe5\xb8\xb8\xe6\x96\xb9\xe4\xbe\xbf\xe7\x9a\x84\xe5\x87\xbd\xe6\x95\xb0\xe8\x83\xbd\xe5\xa4\x9f\xe5\xb0\x86\xe8\xbf\x99\xe4\xba\x9b\xe5\x8f\x98\xe5\x8c\x96\xe5\x90\x88\xe8\xb5\xb7\xe6\x9d\xa5\xef\xbc\x8c\xe5\xb0\xb1\xe6\x98\xaf `torchvision.transforms.Compose()`\xef\xbc\x8c\xe4\xb8\x8b\xe9\x9d\xa2\xe6\x88\x91\xe4\xbb\xac\xe4\xb8\xbe\xe4\xb8\xaa\xe4\xbe\x8b\xe5\xad\x90\n\n#%%\nim_aug = tfs.Compose([\n    tfs.Resize(120),\n    tfs.RandomHorizontalFlip(),\n    tfs.RandomCrop(96),\n    tfs.ColorJitter(brightness=0.5, contrast=0.5, hue=0.5)\n])\n\n\n#%%\nimport matplotlib.pyplot as plt\nget_ipython().run_line_magic('matplotlib', 'inline')\n\n\n#%%\nnrows = 3\nncols = 3\nfigsize = (8, 8)\n_, figs = plt.subplots(nrows, ncols, figsize=figsize)\nfor i in range(nrows):\n    for j in range(ncols):\n        figs[i][j].imshow(im_aug(im))\n        figs[i][j].axes.get_xaxis().set_visible(False)\n        figs[i][j].axes.get_yaxis().set_visible(False)\nplt.show()\n\n#%% [markdown]\n# \xe5\x8f\xaf\xe4\xbb\xa5\xe7\x9c\x8b\xe5\x88\xb0\xe6\xaf\x8f\xe6\xac\xa1\xe5\x81\x9a\xe5\xae\x8c\xe5\xa2\x9e\xe5\xbc\xba\xe4\xb9\x8b\xe5\x90\x8e\xe7\x9a\x84\xe5\x9b\xbe\xe7\x89\x87\xe9\x83\xbd\xe6\x9c\x89\xe4\xb8\x80\xe4\xba\x9b\xe5\x8f\x98\xe5\x8c\x96\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe8\xbf\x99\xe5\xb0\xb1\xe6\x98\xaf\xe6\x88\x91\xe4\xbb\xac\xe5\x89\x8d\xe9\x9d\xa2\xe8\xae\xb2\xe7\x9a\x84\xef\xbc\x8c\xe5\xa2\x9e\xe5\x8a\xa0\xe4\xba\x86\xe4\xb8\x80\xe4\xba\x9b'\xe6\x96\xb0'\xe6\x95\xb0\xe6\x8d\xae\n# \n# \xe4\xb8\x8b\xe9\x9d\xa2\xe6\x88\x91\xe4\xbb\xac\xe4\xbd\xbf\xe7\x94\xa8\xe5\x9b\xbe\xe5\x83\x8f\xe5\xa2\x9e\xe5\xbc\xba\xe8\xbf\x9b\xe8\xa1\x8c\xe8\xae\xad\xe7\xbb\x83\xe7\xbd\x91\xe7\xbb\x9c\xef\xbc\x8c\xe7\x9c\x8b\xe7\x9c\x8b\xe5\x85\xb7\xe4\xbd\x93\xe7\x9a\x84\xe6\x8f\x90\xe5\x8d\x87\xe7\xa9\xb6\xe7\xab\x9f\xe5\x9c\xa8\xe4\xbb\x80\xe4\xb9\x88\xe5\x9c\xb0\xe6\x96\xb9\xef\xbc\x8c\xe4\xbd\xbf\xe7\x94\xa8\xe5\x89\x8d\xe9\x9d\xa2\xe8\xae\xb2\xe7\x9a\x84 ResNet \xe8\xbf\x9b\xe8\xa1\x8c\xe8\xae\xad\xe7\xbb\x83 \n\n#%%\nimport numpy as np\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torchvision.datasets import CIFAR10\nfrom utils import train, resnet\nfrom torchvision import transforms as tfs\n\n\n#%%\n# \xe4\xbd\xbf\xe7\x94\xa8\xe6\x95\xb0\xe6\x8d\xae\xe5\xa2\x9e\xe5\xbc\xba\ndef train_tf(x):\n    im_aug = tfs.Compose([\n        tfs.Resize(120),\n        tfs.RandomHorizontalFlip(),\n        tfs.RandomCrop(96),\n        tfs.ColorJitter(brightness=0.5, contrast=0.5, hue=0.5),\n        tfs.ToTensor(),\n        tfs.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n    ])\n    x = im_aug(x)\n    return x\n\ndef test_tf(x):\n    im_aug = tfs.Compose([\n        tfs.Resize(96),\n        tfs.ToTensor(),\n        tfs.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n    ])\n    x = im_aug(x)\n    return x\n\ntrain_set = CIFAR10('C:/DATASETS/cifar10/', train=True, transform=train_tf)\ntrain_data = torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=True)\ntest_set = CIFAR10('C:/DATASETS/cifar10/', train=False, transform=test_tf)\ntest_data = torch.utils.data.DataLoader(test_set, batch_size=128, shuffle=False)\n\nnet = resnet(3, 10)\noptimizer = torch.optim.SGD(net.parameters(), lr=0.01)\ncriterion = nn.CrossEntropyLoss()\n\n\n#%%\ntrain(net, train_data, test_data, 10, optimizer, criterion)\n\n\n#%%\n# \xe4\xb8\x8d\xe4\xbd\xbf\xe7\x94\xa8\xe6\x95\xb0\xe6\x8d\xae\xe5\xa2\x9e\xe5\xbc\xba\ndef data_tf(x):\n    im_aug = tfs.Compose([\n        tfs.Resize(96),\n        tfs.ToTensor(),\n        tfs.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n    ])\n    x = im_aug(x)\n    return x\n\ntrain_set = CIFAR10('C:/DATASETS/cifar10/', train=True, transform=data_tf)\ntrain_data = torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=True)\ntest_set = CIFAR10('C:/DATASETS/cifar10/', train=False, transform=data_tf)\ntest_data = torch.utils.data.DataLoader(test_set, batch_size=128, shuffle=False)\n\nnet = resnet(3, 10)\noptimizer = torch.optim.SGD(net.parameters(), lr=0.01)\ncriterion = nn.CrossEntropyLoss()\n\n\n#%%\ntrain(net, train_data, test_data, 10, optimizer, criterion)\n\n#%% [markdown]\n# \xe4\xbb\x8e\xe4\xb8\x8a\xe9\x9d\xa2\xe5\x8f\xaf\xe4\xbb\xa5\xe7\x9c\x8b\xe5\x87\xba\xef\xbc\x8c\xe5\xaf\xb9\xe4\xba\x8e\xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\xef\xbc\x8c\xe4\xb8\x8d\xe5\x81\x9a\xe6\x95\xb0\xe6\x8d\xae\xe5\xa2\x9e\xe5\xbc\xba\xe8\xb7\x91 10 \xe6\xac\xa1\xef\xbc\x8c\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87\xe5\xb7\xb2\xe7\xbb\x8f\xe5\x88\xb0\xe4\xba\x86 95%\xef\xbc\x8c\xe8\x80\x8c\xe4\xbd\xbf\xe7\x94\xa8\xe4\xba\x86\xe6\x95\xb0\xe6\x8d\xae\xe5\xa2\x9e\xe5\xbc\xba\xef\xbc\x8c\xe8\xb7\x91 10 \xe6\xac\xa1\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87\xe5\x8f\xaa\xe6\x9c\x89 75%\xef\xbc\x8c\xe8\xaf\xb4\xe6\x98\x8e\xe6\x95\xb0\xe6\x8d\xae\xe5\xa2\x9e\xe5\xbc\xba\xe4\xb9\x8b\xe5\x90\x8e\xe5\x8f\x98\xe5\xbe\x97\xe6\x9b\xb4\xe9\x9a\xbe\xe4\xba\x86\xe3\x80\x82\n# \n# \xe8\x80\x8c\xe5\xaf\xb9\xe4\xba\x8e\xe6\xb5\x8b\xe8\xaf\x95\xe9\x9b\x86\xef\xbc\x8c\xe4\xbd\xbf\xe7\x94\xa8\xe6\x95\xb0\xe6\x8d\xae\xe5\xa2\x9e\xe5\xbc\xba\xe8\xbf\x9b\xe8\xa1\x8c\xe8\xae\xad\xe7\xbb\x83\xe7\x9a\x84\xe6\x97\xb6\xe5\x80\x99\xef\xbc\x8c\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87\xe4\xbc\x9a\xe6\xaf\x94\xe4\xb8\x8d\xe4\xbd\xbf\xe7\x94\xa8\xe6\x9b\xb4\xe9\xab\x98\xef\xbc\x8c\xe5\x9b\xa0\xe4\xb8\xba\xe6\x95\xb0\xe6\x8d\xae\xe5\xa2\x9e\xe5\xbc\xba\xe6\x8f\x90\xe9\xab\x98\xe4\xba\x86\xe6\xa8\xa1\xe5\x9e\x8b\xe5\xba\x94\xe5\xaf\xb9\xe4\xba\x8e\xe6\x9b\xb4\xe5\xa4\x9a\xe7\x9a\x84\xe4\xb8\x8d\xe5\x90\x8c\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe7\x9a\x84\xe6\xb3\x9b\xe5\x8c\x96\xe8\x83\xbd\xe5\x8a\x9b\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe6\x9c\x89\xe6\x9b\xb4\xe5\xa5\xbd\xe7\x9a\x84\xe6\x95\x88\xe6\x9e\x9c\xe3\x80\x82\n\n"""
7.Convolutional_Neural_Network(CNN) 卷积神经网络/lenet5.py,3,"b'import  torch\nfrom    torch import nn\nfrom    torch.nn import functional as F\n\n\n\n\nclass Lenet5(nn.Module):\n    """"""\n    for cifar10 dataset.\n    """"""\n    def __init__(self):\n        super(Lenet5, self).__init__()\n\n        self.conv_unit = nn.Sequential(\n            # x: [b, 3, 32, 32] => [b, 6, ]\n            nn.Conv2d(3, 6, kernel_size=5, stride=1, padding=0),\n            nn.AvgPool2d(kernel_size=2, stride=2, padding=0),\n            #\n            nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=0),\n            nn.AvgPool2d(kernel_size=2, stride=2, padding=0),\n            #\n        )\n        # flatten\n        # fc unit\n        self.fc_unit = nn.Sequential(\n            nn.Linear(16*5*5, 32),\n            nn.ReLU(),\n            # nn.Linear(120, 84),\n            # nn.ReLU(),\n            nn.Linear(32, 10)\n        )\n\n\n        # [b, 3, 32, 32]\n        tmp = torch.randn(2, 3, 32, 32)\n        out = self.conv_unit(tmp)\n        # [b, 16, 5, 5]\n        print(\'conv out:\', out.shape)\n\n        # # use Cross Entropy Loss\n        # self.criteon = nn.CrossEntropyLoss()\n\n\n\n    def forward(self, x):\n        """"""\n\n        :param x: [b, 3, 32, 32]\n        :return:\n        """"""\n        batchsz = x.size(0)\n        # [b, 3, 32, 32] => [b, 16, 5, 5]\n        x = self.conv_unit(x)\n        # [b, 16, 5, 5] => [b, 16*5*5]\n        x = x.view(batchsz, 16*5*5)\n        # [b, 16*5*5] => [b, 10]\n        logits = self.fc_unit(x)\n\n        # # [b, 10]\n        # pred = F.softmax(logits, dim=1)\n        # loss = self.criteon(logits, y)\n\n        return logits\n\ndef main():\n\n    net = Lenet5()\n\n    tmp = torch.randn(2, 3, 32, 32)\n    out = net(tmp)\n    print(\'lenet out:\', out.shape)\n\n\n\n\nif __name__ == \'__main__\':\n    main()'"
7.Convolutional_Neural_Network(CNN) 卷积神经网络/lr-decay 学习率衰减.py,8,"b'#%% [markdown]\n# # \xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\xe8\xa1\xb0\xe5\x87\x8f\n# \xe5\xaf\xb9\xe4\xba\x8e\xe5\x9f\xba\xe4\xba\x8e\xe4\xb8\x80\xe9\x98\xb6\xe6\xa2\xaf\xe5\xba\xa6\xe8\xbf\x9b\xe8\xa1\x8c\xe4\xbc\x98\xe5\x8c\x96\xe7\x9a\x84\xe6\x96\xb9\xe6\xb3\x95\xe8\x80\x8c\xe8\xa8\x80\xef\xbc\x8c\xe5\xbc\x80\xe5\xa7\x8b\xe7\x9a\x84\xe6\x97\xb6\xe5\x80\x99\xe6\x9b\xb4\xe6\x96\xb0\xe7\x9a\x84\xe5\xb9\x85\xe5\xba\xa6\xe6\x98\xaf\xe6\xaf\x94\xe8\xbe\x83\xe5\xa4\xa7\xe7\x9a\x84\xef\xbc\x8c\xe4\xb9\x9f\xe5\xb0\xb1\xe6\x98\xaf\xe8\xaf\xb4\xe5\xbc\x80\xe5\xa7\x8b\xe7\x9a\x84\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\xe5\x8f\xaf\xe4\xbb\xa5\xe8\xae\xbe\xe7\xbd\xae\xe5\xa4\xa7\xe4\xb8\x80\xe7\x82\xb9\xef\xbc\x8c\xe4\xbd\x86\xe6\x98\xaf\xe5\xbd\x93\xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\xe7\x9a\x84 loss \xe4\xb8\x8b\xe9\x99\x8d\xe5\x88\xb0\xe4\xb8\x80\xe5\xae\x9a\xe7\xa8\x8b\xe5\xba\xa6\xe4\xb9\x8b\xe5\x90\x8e\xef\xbc\x8c\xef\xbc\x8c\xe4\xbd\xbf\xe7\x94\xa8\xe8\xbf\x99\xe4\xb8\xaa\xe5\xa4\xaa\xe5\xa4\xa7\xe7\x9a\x84\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\xe5\xb0\xb1\xe4\xbc\x9a\xe5\xaf\xbc\xe8\x87\xb4 loss \xe4\xb8\x80\xe7\x9b\xb4\xe6\x9d\xa5\xe5\x9b\x9e\xe9\x9c\x87\xe8\x8d\xa1\xef\xbc\x8c\xe6\xaf\x94\xe5\xa6\x82\n# \n# ![](https://ws4.sinaimg.cn/large/006tNc79ly1fmrvdlncomj30bf0aywet.jpg)\n#%% [markdown]\n# \xe8\xbf\x99\xe4\xb8\xaa\xe6\x97\xb6\xe5\x80\x99\xe5\xb0\xb1\xe9\x9c\x80\xe8\xa6\x81\xe5\xaf\xb9\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\xe8\xbf\x9b\xe8\xa1\x8c\xe8\xa1\xb0\xe5\x87\x8f\xe5\xb7\xb2\xe8\xbe\xbe\xe5\x88\xb0 loss \xe7\x9a\x84\xe5\x85\x85\xe5\x88\x86\xe4\xb8\x8b\xe9\x99\x8d\xef\xbc\x8c\xe8\x80\x8c\xe6\x98\xaf\xe7\x94\xa8\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\xe8\xa1\xb0\xe5\x87\x8f\xe7\x9a\x84\xe5\x8a\x9e\xe6\xb3\x95\xe8\x83\xbd\xe5\xa4\x9f\xe8\xa7\xa3\xe5\x86\xb3\xe8\xbf\x99\xe4\xb8\xaa\xe7\x9f\x9b\xe7\x9b\xbe\xef\xbc\x8c\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\xe8\xa1\xb0\xe5\x87\x8f\xe5\xb0\xb1\xe6\x98\xaf\xe9\x9a\x8f\xe7\x9d\x80\xe8\xae\xad\xe7\xbb\x83\xe7\x9a\x84\xe8\xbf\x9b\xe8\xa1\x8c\xe4\xb8\x8d\xe6\x96\xad\xe7\x9a\x84\xe5\x87\x8f\xe5\xb0\x8f\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\xe3\x80\x82\n# \n# \xe5\x9c\xa8 pytorch \xe4\xb8\xad\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\xe8\xa1\xb0\xe5\x87\x8f\xe9\x9d\x9e\xe5\xb8\xb8\xe6\x96\xb9\xe4\xbe\xbf\xef\xbc\x8c\xe4\xbd\xbf\xe7\x94\xa8 `torch.optim.lr_scheduler`\xef\xbc\x8c\xe6\x9b\xb4\xe5\xa4\x9a\xe7\x9a\x84\xe4\xbf\xa1\xe6\x81\xaf\xe5\x8f\xaf\xe4\xbb\xa5\xe7\x9b\xb4\xe6\x8e\xa5\xe6\x9f\xa5\xe7\x9c\x8b[\xe6\x96\x87\xe6\xa1\xa3](http://pytorch.org/docs/0.3.0/optim.html#how-to-adjust-learning-rate)\n# \n# \xe4\xbd\x86\xe6\x98\xaf\xe6\x88\x91\xe6\x8e\xa8\xe8\x8d\x90\xe5\xa4\xa7\xe5\xae\xb6\xe4\xbd\xbf\xe7\x94\xa8\xe4\xb8\x8b\xe9\x9d\xa2\xe8\xbf\x99\xe7\xa7\x8d\xe6\x96\xb9\xe5\xbc\x8f\xe6\x9d\xa5\xe5\x81\x9a\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\xe8\xa1\xb0\xe5\x87\x8f\xef\xbc\x8c\xe6\x9b\xb4\xe5\x8a\xa0\xe7\x9b\xb4\xe8\xa7\x82\xef\xbc\x8c\xe4\xb8\x8b\xe9\x9d\xa2\xe6\x88\x91\xe4\xbb\xac\xe7\x9b\xb4\xe6\x8e\xa5\xe4\xb8\xbe\xe4\xbe\x8b\xe5\xad\x90\xe6\x9d\xa5\xe8\xaf\xb4\xe6\x98\x8e\n\n#%%\nimport numpy as np\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torchvision.datasets import CIFAR10\nfrom utils import resnet\nfrom torchvision import transforms as tfs\nfrom datetime import datetime\n\n\n#%%\nnet = resnet(3, 10)\noptimizer = torch.optim.SGD(net.parameters(), lr=0.01, weight_decay=1e-4)\n\n#%% [markdown]\n# \xe8\xbf\x99\xe9\x87\x8c\xe6\x88\x91\xe4\xbb\xac\xe5\xae\x9a\xe4\xb9\x89\xe5\xa5\xbd\xe4\xba\x86\xe6\xa8\xa1\xe5\x9e\x8b\xe5\x92\x8c\xe4\xbc\x98\xe5\x8c\x96\xe5\x99\xa8\xef\xbc\x8c\xe5\x8f\xaf\xe4\xbb\xa5\xe9\x80\x9a\xe8\xbf\x87 `optimizer.param_groups` \xe6\x9d\xa5\xe5\xbe\x97\xe5\x88\xb0\xe6\x89\x80\xe6\x9c\x89\xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0\xe7\xbb\x84\xe5\x92\x8c\xe5\x85\xb6\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84\xe5\xb1\x9e\xe6\x80\xa7\xef\xbc\x8c\xe5\x8f\x82\xe6\x95\xb0\xe7\xbb\x84\xe6\x98\xaf\xe4\xbb\x80\xe4\xb9\x88\xe6\x84\x8f\xe6\x80\x9d\xe5\x91\xa2\xef\xbc\x9f\xe5\xb0\xb1\xe6\x98\xaf\xe6\x88\x91\xe4\xbb\xac\xe5\x8f\xaf\xe4\xbb\xa5\xe5\xb0\x86\xe6\xa8\xa1\xe5\x9e\x8b\xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0\xe5\x88\x86\xe6\x88\x90\xe5\x87\xa0\xe4\xb8\xaa\xe7\xbb\x84\xef\xbc\x8c\xe6\xaf\x8f\xe4\xb8\xaa\xe7\xbb\x84\xe5\xae\x9a\xe4\xb9\x89\xe4\xb8\x80\xe4\xb8\xaa\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\xef\xbc\x8c\xe8\xbf\x99\xe9\x87\x8c\xe6\xaf\x94\xe8\xbe\x83\xe5\xa4\x8d\xe6\x9d\x82\xef\xbc\x8c\xe4\xb8\x80\xe8\x88\xac\xe6\x9d\xa5\xe8\xae\xb2\xe5\xa6\x82\xe6\x9e\x9c\xe4\xb8\x8d\xe5\x81\x9a\xe7\x89\xb9\xe5\x88\xab\xe4\xbf\xae\xe6\x94\xb9\xef\xbc\x8c\xe5\xb0\xb1\xe5\x8f\xaa\xe6\x9c\x89\xe4\xb8\x80\xe4\xb8\xaa\xe5\x8f\x82\xe6\x95\xb0\xe7\xbb\x84\n# \n# \xe8\xbf\x99\xe4\xb8\xaa\xe5\x8f\x82\xe6\x95\xb0\xe7\xbb\x84\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa\xe5\xad\x97\xe5\x85\xb8\xef\xbc\x8c\xe9\x87\x8c\xe9\x9d\xa2\xe6\x9c\x89\xe5\xbe\x88\xe5\xa4\x9a\xe5\xb1\x9e\xe6\x80\xa7\xef\xbc\x8c\xe6\xaf\x94\xe5\xa6\x82\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\xef\xbc\x8c\xe6\x9d\x83\xe9\x87\x8d\xe8\xa1\xb0\xe5\x87\x8f\xe7\xad\x89\xe7\xad\x89\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe5\x8f\xaf\xe4\xbb\xa5\xe8\xae\xbf\xe9\x97\xae\xe4\xbb\xa5\xe4\xb8\x8b\n\n#%%\nprint(\'learning rate: {}\'.format(optimizer.param_groups[0][\'lr\']))\nprint(\'weight decay: {}\'.format(optimizer.param_groups[0][\'weight_decay\']))\n\n#%% [markdown]\n# \xe6\x89\x80\xe4\xbb\xa5\xe6\x88\x91\xe4\xbb\xac\xe5\x8f\xaf\xe4\xbb\xa5\xe9\x80\x9a\xe8\xbf\x87\xe4\xbf\xae\xe6\x94\xb9\xe8\xbf\x99\xe4\xb8\xaa\xe5\xb1\x9e\xe6\x80\xa7\xe6\x9d\xa5\xe6\x94\xb9\xe5\x8f\x98\xe6\x88\x91\xe4\xbb\xac\xe8\xae\xad\xe7\xbb\x83\xe8\xbf\x87\xe7\xa8\x8b\xe4\xb8\xad\xe7\x9a\x84\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\xef\xbc\x8c\xe9\x9d\x9e\xe5\xb8\xb8\xe7\xae\x80\xe5\x8d\x95\n\n#%%\noptimizer.param_groups[0][\'lr\'] = 1e-5\n\n#%% [markdown]\n# \xe4\xb8\xba\xe4\xba\x86\xe9\x98\xb2\xe6\xad\xa2\xe6\x9c\x89\xe5\xa4\x9a\xe4\xb8\xaa\xe5\x8f\x82\xe6\x95\xb0\xe7\xbb\x84\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe5\x8f\xaf\xe4\xbb\xa5\xe4\xbd\xbf\xe7\x94\xa8\xe4\xb8\x80\xe4\xb8\xaa\xe5\xbe\xaa\xe7\x8e\xaf\n\n#%%\nfor param_group in optimizer.param_groups:\n    param_group[\'lr\'] = 1e-1\n\n#%% [markdown]\n# \xe6\x96\xb9\xe6\xb3\x95\xe5\xb0\xb1\xe6\x98\xaf\xe8\xbf\x99\xe6\xa0\xb7\xef\xbc\x8c\xe9\x9d\x9e\xe5\xb8\xb8\xe7\xae\x80\xe5\x8d\x95\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe5\x8f\xaf\xe4\xbb\xa5\xe5\x9c\xa8\xe4\xbb\xbb\xe6\x84\x8f\xe7\x9a\x84\xe4\xbd\x8d\xe7\xbd\xae\xe6\x94\xb9\xe5\x8f\x98\xe6\x88\x91\xe4\xbb\xac\xe7\x9a\x84\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\n# \n# \xe4\xb8\x8b\xe9\x9d\xa2\xe6\x88\x91\xe4\xbb\xac\xe5\x85\xb7\xe4\xbd\x93\xe6\x9d\xa5\xe7\x9c\x8b\xe7\x9c\x8b\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\xe8\xa1\xb0\xe5\x87\x8f\xe7\x9a\x84\xe5\xa5\xbd\xe5\xa4\x84\n\n#%%\ndef set_learning_rate(optimizer, lr):\n    for param_group in optimizer.param_groups:\n        param_group[\'lr\'] = lr\n\n\n#%%\n# \xe4\xbd\xbf\xe7\x94\xa8\xe6\x95\xb0\xe6\x8d\xae\xe5\xa2\x9e\xe5\xbc\xba\ndef train_tf(x):\n    im_aug = tfs.Compose([\n        tfs.Resize(120),\n        tfs.RandomHorizontalFlip(),\n        tfs.RandomCrop(96),\n        tfs.ColorJitter(brightness=0.5, contrast=0.5, hue=0.5),\n        tfs.ToTensor(),\n        tfs.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n    ])\n    x = im_aug(x)\n    return x\n\ndef test_tf(x):\n    im_aug = tfs.Compose([\n        tfs.Resize(96),\n        tfs.ToTensor(),\n        tfs.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n    ])\n    x = im_aug(x)\n    return x\n\ntrain_set = CIFAR10(\'C:/DATASETS/cifar10/\', train=True, transform=train_tf)\ntrain_data = torch.utils.data.DataLoader(train_set, batch_size=256, shuffle=True)\nvalid_set = CIFAR10(\'C:/DATASETS/cifar10/\', train=False, transform=test_tf)\nvalid_data = torch.utils.data.DataLoader(valid_set, batch_size=256, shuffle=False)\n\nnet = resnet(3, 10)\noptimizer = torch.optim.SGD(net.parameters(), lr=0.1, weight_decay=1e-4)\ncriterion = nn.CrossEntropyLoss()\n\n\n#%%\ntrain_losses = []\nvalid_losses = []\ndevice = torch.device(\'cuda\' if torch.cuda.is_available() else \'cpu\')\nnet = net.to(device)\nprev_time = datetime.now()\nfor epoch in range(30):\n    if epoch == 20:\n        set_learning_rate(optimizer, 0.01) # 80 \xe6\xac\xa1\xe4\xbf\xae\xe6\x94\xb9\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\xe4\xb8\xba 0.01\n    train_loss = 0\n    net = net.train()\n    for im, label in train_data:\n        im = im.to(device)\n        label = label.to(device)\n\n        # forward\n        output = net(im)\n        loss = criterion(output, label)\n        # backward\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        train_loss += loss.item()\n    cur_time = datetime.now()\n    h, remainder = divmod((cur_time - prev_time).seconds, 3600)\n    m, s = divmod(remainder, 60)\n    time_str = ""Time %02d:%02d:%02d"" % (h, m, s)\n    valid_loss = 0\n    valid_acc = 0\n    net = net.eval()\n    with torch.no_grad():\n        for im, label in valid_data:\n            im = im.to(device)\n            label = label.to(device)\n            output = net(im)\n            loss = criterion(output, label)\n            valid_loss += loss.item()\n    epoch_str = (\n        ""Epoch %d. Train Loss: %f, Valid Loss: %f, ""\n        % (epoch, train_loss / len(train_data), valid_loss / len(valid_data)))\n    prev_time = cur_time\n    \n    train_losses.append(train_loss / len(train_data))\n    valid_losses.append(valid_loss / len(valid_data))\n    print(epoch_str + time_str)\n\n#%% [markdown]\n# \xe4\xb8\x8b\xe9\x9d\xa2\xe6\x88\x91\xe4\xbb\xac\xe7\x94\xbb\xe5\x87\xba loss \xe6\x9b\xb2\xe7\xba\xbf\n\n#%%\nimport matplotlib.pyplot as plt\nget_ipython().run_line_magic(\'matplotlib\', \'inline\')\n\n\n#%%\nplt.plot(train_losses, label=\'train\')\nplt.plot(valid_losses, label=\'valid\')\nplt.xlabel(\'epoch\')\nplt.legend(loc=\'best\')\n\n#%% [markdown]\n# \xe8\xbf\x99\xe9\x87\x8c\xe6\x88\x91\xe4\xbb\xac\xe5\x8f\xaa\xe8\xae\xad\xe7\xbb\x83\xe4\xba\x86 30 \xe6\xac\xa1\xef\xbc\x8c\xe5\x9c\xa8 20 \xe6\xac\xa1\xe7\x9a\x84\xe6\x97\xb6\xe5\x80\x99\xe8\xbf\x9b\xe8\xa1\x8c\xe4\xba\x86\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\xe8\xa1\xb0\xe5\x87\x8f\xef\xbc\x8c\xe5\x8f\xaf\xe4\xbb\xa5\xe7\x9c\x8b loss \xe6\x9b\xb2\xe7\xba\xbf\xe5\x9c\xa8 20 \xe6\xac\xa1\xe7\x9a\x84\xe6\x97\xb6\xe5\x80\x99\xe4\xb8\x8d\xe7\xae\xa1\xe6\x98\xaf train loss \xe8\xbf\x98\xe6\x98\xaf valid loss\xef\xbc\x8c\xe9\x83\xbd\xe6\x9c\x89\xe4\xba\x86\xe4\xb8\x80\xe4\xb8\xaa\xe9\x99\xa1\xe9\x99\x8d\xe3\x80\x82\n# \n# \xe5\xbd\x93\xe7\x84\xb6\xe8\xbf\x99\xe9\x87\x8c\xe6\x88\x91\xe4\xbb\xac\xe5\x8f\xaa\xe6\x98\xaf\xe4\xbd\x9c\xe4\xb8\xba\xe4\xb8\xbe\xe4\xbe\x8b\xef\xbc\x8c\xe5\x9c\xa8\xe5\xae\x9e\xe9\x99\x85\xe5\xba\x94\xe7\x94\xa8\xe4\xb8\xad\xef\xbc\x8c\xe5\x81\x9a\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\xe8\xa1\xb0\xe5\x87\x8f\xe4\xb9\x8b\xe5\x89\x8d\xe5\xba\x94\xe8\xaf\xa5\xe7\xbb\x8f\xe8\xbf\x87\xe5\x85\x85\xe5\x88\x86\xe7\x9a\x84\xe8\xae\xad\xe7\xbb\x83\xef\xbc\x8c\xe6\xaf\x94\xe5\xa6\x82\xe8\xae\xad\xe7\xbb\x83 80 \xe6\xac\xa1\xe6\x88\x96\xe8\x80\x85 100 \xe6\xac\xa1\xef\xbc\x8c\xe7\x84\xb6\xe5\x90\x8e\xe5\x86\x8d\xe5\x81\x9a\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\xe8\xa1\xb0\xe5\x87\x8f\xe5\xbe\x97\xe5\x88\xb0\xe6\x9b\xb4\xe5\xa5\xbd\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x9c\xef\xbc\x8c\xe6\x9c\x89\xe7\x9a\x84\xe6\x97\xb6\xe5\x80\x99\xe7\x94\x9a\xe8\x87\xb3\xe9\x9c\x80\xe8\xa6\x81\xe5\x81\x9a\xe5\xa4\x9a\xe6\xac\xa1\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\xe8\xa1\xb0\xe5\x87\x8f\n\n'"
7.Convolutional_Neural_Network(CNN) 卷积神经网络/mnist-from-another-repo.py,8,"b""import torch\nimport torch.nn as nn\nimport torchvision\nimport torchvision.transforms as transforms\n\n\n# Device configuration\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Hyper-parameters \ninput_size = 784\nhidden_size = 500\nnum_classes = 10\nnum_epochs = 5\nbatch_size = 100\nlearning_rate = 0.001\n\n# MNIST dataset \ntrain_dataset = torchvision.datasets.MNIST(root=r'C:\\DATASETS', \n                                           train=True, \n                                           transform=transforms.ToTensor(),  \n                                           download=True)\n\ntest_dataset = torchvision.datasets.MNIST(root=r'C:\\DATASETS', \n                                          train=False, \n                                          transform=transforms.ToTensor())\n\n# Data loader\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n                                           batch_size=batch_size, \n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n                                          batch_size=batch_size, \n                                          shuffle=False)\n\n# Fully connected neural network with one hidden layer\nclass NeuralNet(nn.Module):\n    def __init__(self, input_size, hidden_size, num_classes):\n        super(NeuralNet, self).__init__()\n        self.fc1 = nn.Linear(input_size, hidden_size) \n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(hidden_size, num_classes)  \n    \n    def forward(self, x):\n        out = self.fc1(x)\n        out = self.relu(out)\n        out = self.fc2(out)\n        return out\n\nmodel = NeuralNet(input_size, hidden_size, num_classes).to(device)\n\n# Loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  \n\n# Train the model\ntotal_step = len(train_loader)\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):  \n        # Move tensors to the configured device\n        images = images.reshape(-1, 28*28).to(device)\n        labels = labels.to(device)\n        \n        # Forward pass\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        \n        # Backward and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        if (i+1) % 100 == 0:\n            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n\n# Test the model\n# In test phase, we don't need to compute gradients (for memory efficiency)\nwith torch.no_grad():\n    correct = 0\n    total = 0\n    for images, labels in test_loader:\n        images = images.reshape(-1, 28*28).to(device)\n        labels = labels.to(device)\n        outputs = model(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n    print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))\n\n# Save the model checkpoint\ntorch.save(model.state_dict(), 'model.ckpt')"""
7.Convolutional_Neural_Network(CNN) 卷积神经网络/mnist.py,7,"b""\n#%%\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\n\n\n#%%\ntorch.manual_seed(4242)\n\n\n#%%\ntrain_loader = torch.utils.data.DataLoader(\n    datasets.MNIST('./data', train=True, download=True,\n                   transform=transforms.Compose([\n                       transforms.ToTensor(),\n                       transforms.Normalize((0.1307,), (0.3081,))\n                   ])),\n    batch_size=64, shuffle=True)\n\n\n#%%\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n        self.conv2_drop = nn.Dropout2d()\n        self.fc1 = nn.Linear(320, 50)\n        self.fc2 = nn.Linear(50, 10)\n\n    def forward(self, x):\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n        x = x.view(-1, 320)\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x, training=self.training)\n        x = self.fc2(x)\n        return F.log_softmax(x, dim=1)\n\n\n#%%\nmodel = Net()\n\n\n#%%\noptimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n\n\n#%%\nfor epoch in range(10):\n    for batch_idx, (data, target) in enumerate(train_loader):\n        optimizer.zero_grad()\n        output = model(data)\n        loss = F.nll_loss(output, target)\n        print('Current loss', float(loss))\n        loss.backward()\n        optimizer.step()\n\n\n#%%\ntorch.save(model.state_dict(), 'mnist.pth')\n\n\n#%%\npretrained_model = Net()\npretrained_model.load_state_dict(torch.load('mnist.pth'))\n\n\n"""
7.Convolutional_Neural_Network(CNN) 卷积神经网络/mycnn.py,7,"b""# \xe9\x85\x8d\xe7\xbd\xae\xe5\xba\x93\nimport torch\nfrom torch import nn, optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets\nfrom torchvision import transforms\n\n# \xe9\x85\x8d\xe7\xbd\xae\xe5\x8f\x82\xe6\x95\xb0\ntorch.manual_seed(\n    1)  # \xe8\xae\xbe\xe7\xbd\xae\xe9\x9a\x8f\xe6\x9c\xba\xe6\x95\xb0\xe7\xa7\x8d\xe5\xad\x90\xef\xbc\x8c\xe7\xa1\xae\xe4\xbf\x9d\xe7\xbb\x93\xe6\x9e\x9c\xe5\x8f\xaf\xe9\x87\x8d\xe5\xa4\x8d\nbatch_size = 128  # \xe6\x89\xb9\xe5\xa4\x84\xe7\x90\x86\xe5\xa4\xa7\xe5\xb0\x8f\nlearning_rate = 1e-2  # \xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\nnum_epoches = 10  # \xe8\xae\xad\xe7\xbb\x83\xe6\xac\xa1\xe6\x95\xb0\n\n# \xe4\xb8\x8b\xe8\xbd\xbd\xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86 MNIST \xe6\x89\x8b\xe5\x86\x99\xe6\x95\xb0\xe5\xad\x97\xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\ntrain_dataset = datasets.MNIST(\n    root=r'C:/DATASETS/mnist/',  # \xe6\x95\xb0\xe6\x8d\xae\xe4\xbf\x9d\xe6\x8c\x81\xe7\x9a\x84\xe4\xbd\x8d\xe7\xbd\xae\n    train=True,  # \xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\n    transform=transforms.ToTensor(),  # \xe4\xb8\x80\xe4\xb8\xaa\xe5\x8f\x96\xe5\x80\xbc\xe8\x8c\x83\xe5\x9b\xb4\xe6\x98\xaf[0,255]\xe7\x9a\x84PIL.Image\n    # \xe8\xbd\xac\xe5\x8c\x96\xe4\xb8\xba\xe5\x8f\x96\xe5\x80\xbc\xe8\x8c\x83\xe5\x9b\xb4\xe6\x98\xaf[0,1.0]\xe7\x9a\x84torch.FloadTensor\n    download=True)  # \xe4\xb8\x8b\xe8\xbd\xbd\xe6\x95\xb0\xe6\x8d\xae\n\ntest_dataset = datasets.MNIST(\n    root=r'C:/DATASETS/mnist/',\n    train=False,  # \xe6\xb5\x8b\xe8\xaf\x95\xe9\x9b\x86\n    transform=transforms.ToTensor())\n\n# \xe6\x95\xb0\xe6\x8d\xae\xe7\x9a\x84\xe6\x89\xb9\xe5\xa4\x84\xe7\x90\x86\xef\xbc\x8c\xe5\xb0\xba\xe5\xaf\xb8\xe5\xa4\xa7\xe5\xb0\x8f\xe4\xb8\xbabatch_size,\n# \xe5\x9c\xa8\xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\xe4\xb8\xad\xef\xbc\x8cshuffle \xe5\xbf\x85\xe9\xa1\xbb\xe8\xae\xbe\xe7\xbd\xae\xe4\xb8\xbaTrue, \xe8\xa1\xa8\xe7\xa4\xba\xe6\xac\xa1\xe5\xba\x8f\xe6\x98\xaf\xe9\x9a\x8f\xe6\x9c\xba\xe7\x9a\x84\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n\n# \xe5\xae\x9a\xe4\xb9\x89\xe5\x8d\xb7\xe7\xa7\xaf\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xe6\xa8\xa1\xe5\x9e\x8b\nclass Cnn(nn.Module):\n    def __init__(self, in_dim, n_class):  # 28x28x1\n        super(Cnn, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_dim, 6, 3, stride=1, padding=1),  # 28 x28\n            nn.ReLU(True),\n            nn.MaxPool2d(2, 2),  # 14 x 14\n            nn.Conv2d(6, 16, 5, stride=1, padding=0),  # 10 * 10*16\n            nn.ReLU(True), nn.MaxPool2d(2, 2))  # 5x5x16\n\n        self.fc = nn.Sequential(\n            nn.Linear(400, 120),  # 400 = 5 * 5 * 16\n            nn.Linear(120, 84),\n            nn.Linear(84, n_class))\n\n    def forward(self, x):\n        out = self.conv(x)\n        out = out.view(out.size(0), 400)  # 400 = 5 * 5 * 16, \n        out = self.fc(out)\n        return out\n\n\n\n\nmodel = Cnn(1, 10)  # \xe5\x9b\xbe\xe7\x89\x87\xe5\xa4\xa7\xe5\xb0\x8f\xe6\x98\xaf28x28, 10\n# \xe6\x89\x93\xe5\x8d\xb0\xe6\xa8\xa1\xe5\x9e\x8b\nprint(model)\n# \xe5\xae\x9a\xe4\xb9\x89loss\xe5\x92\x8coptimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=learning_rate)\n\n# \xe5\xbc\x80\xe5\xa7\x8b\xe8\xae\xad\xe7\xbb\x83\nfor epoch in range(num_epoches):\n    print('epoch {}'.format(epoch + 1))\n    print('*' * 10)\n    running_loss = 0.0\n    running_acc = 0.0\n    for i, data in enumerate(train_loader, 1):  # \xe6\x89\xb9\xe5\xa4\x84\xe7\x90\x86\n        img, label = data\n\n        # \xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad \n        out = model(img)\n        loss = criterion(out, label)  # loss\n        running_loss += loss.item() * label.size(0)  # total loss , \xe7\x94\xb1\xe4\xba\x8eloss \xe6\x98\xafbatch \xe5\x8f\x96\xe5\x9d\x87\xe5\x80\xbc\xe7\x9a\x84\xef\xbc\x8c\xe9\x9c\x80\xe8\xa6\x81\xe6\x8a\x8abatch size \xe4\xb9\x98\xe5\x9b\x9e\xe5\x8e\xbb\n        _, pred = torch.max(out, 1)  # \xe9\xa2\x84\xe6\xb5\x8b\xe7\xbb\x93\xe6\x9e\x9c\n        num_correct = (pred == label).sum()  # \xe6\xad\xa3\xe7\xa1\xae\xe7\xbb\x93\xe6\x9e\x9c\xe7\x9a\x84num\n        # accuracy = (pred == label).float().mean() #\xe6\xad\xa3\xe7\xa1\xae\xe7\x8e\x87\n        running_acc += num_correct.item()  # \xe6\xad\xa3\xe7\xa1\xae\xe7\xbb\x93\xe6\x9e\x9c\xe7\x9a\x84\xe6\x80\xbb\xe6\x95\xb0\n        # \xe5\x90\x8e\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\n        optimizer.zero_grad()  # \xe6\xa2\xaf\xe5\xba\xa6\xe6\xb8\x85\xe9\x9b\xb6\xef\xbc\x8c\xe4\xbb\xa5\xe5\x85\x8d\xe5\xbd\xb1\xe5\x93\x8d\xe5\x85\xb6\xe4\xbb\x96batch\n        loss.backward()  # \xe5\x90\x8e\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xef\xbc\x8c\xe8\xae\xa1\xe7\xae\x97\xe6\xa2\xaf\xe5\xba\xa6\n        optimizer.step()  # \xe6\xa2\xaf\xe5\xba\xa6\xe6\x9b\xb4\xe6\x96\xb0\n\n        # if i % 300 == 0:\n        #    print('[{}/{}] Loss: {:.6f}, Acc: {:.6f}'.format(\n        #        epoch + 1, num_epoches, running_loss / (batch_size * i),\n        #        running_acc / (batch_size * i)))\n    # \xe6\x89\x93\xe5\x8d\xb0\xe4\xb8\x80\xe4\xb8\xaa\xe5\xbe\xaa\xe7\x8e\xaf\xe5\x90\x8e\xef\xbc\x8c\xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\xe5\x90\x88\xe4\xb8\x8a\xe7\x9a\x84loss \xe5\x92\x8c \xe6\xad\xa3\xe7\xa1\xae\xe7\x8e\x87\n    print('Train Finish {} epoch, Loss: {:.6f}, Acc: {:.6f}'.format(\n        epoch + 1, running_loss / (len(train_dataset)), running_acc / (len(\n            train_dataset))))\n\n# \xe6\xa8\xa1\xe5\x9e\x8b\xe6\xb5\x8b\xe8\xaf\x95\xef\xbc\x8c \xe7\x94\xb1\xe4\xba\x8e\xe8\xae\xad\xe7\xbb\x83\xe5\x92\x8c\xe6\xb5\x8b\xe8\xaf\x95 BatchNorm, Dropout\xe9\x85\x8d\xe7\xbd\xae\xe4\xb8\x8d\xe5\x90\x8c\xef\xbc\x8c\xe9\x9c\x80\xe8\xa6\x81\xe8\xaf\xb4\xe6\x98\x8e\xe6\x98\xaf\xe5\x90\xa6\xe6\xa8\xa1\xe5\x9e\x8b\xe6\xb5\x8b\xe8\xaf\x95\nmodel.eval()\nwith torch.no_grad():\n    eval_loss = 0\n    eval_acc = 0\n    for data in test_loader:  # test set \xe6\x89\xb9\xe5\xa4\x84\xe7\x90\x86\n        img, label = data\n\n        out = model(img)  # \xe5\x89\x8d\xe5\x90\x91\xe7\xae\x97\xe6\xb3\x95 \n        loss = criterion(out, label)  # \xe8\xae\xa1\xe7\xae\x97 loss\n        eval_loss += loss.item() * label.size(0)  # total loss\n        _, pred = torch.max(out, 1)  # \xe9\xa2\x84\xe6\xb5\x8b\xe7\xbb\x93\xe6\x9e\x9c\n        num_correct = (pred == label).sum()  # \xe6\xad\xa3\xe7\xa1\xae\xe7\xbb\x93\xe6\x9e\x9c\n        eval_acc += num_correct.item()  # \xe6\xad\xa3\xe7\xa1\xae\xe7\xbb\x93\xe6\x9e\x9c\xe6\x80\xbb\xe6\x95\xb0\n\nprint('Test Loss: {:.6f}, Acc: {:.6f}'.format(eval_loss / (len(\n    test_dataset)), eval_acc * 1.0 / (len(test_dataset))))\n\n# \xe4\xbf\x9d\xe5\xad\x98\xe6\xa8\xa1\xe5\x9e\x8b\ntorch.save(model.state_dict(), './cnn.pth')\n"""
7.Convolutional_Neural_Network(CNN) 卷积神经网络/pytorch_nn--oldversion.py,8,"b'import torch\nfrom torch.autograd import Variable\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\n\n\ndef simple_gradient():\n    # print the gradient of 2x^2 + 5x\n    x = Variable(torch.ones(2, 2) * 2, requires_grad=True)\n    z = 2 * (x * x) + 5 * x\n    # run the backpropagation\n    z.backward(torch.ones(2, 2))\n    print(x.grad)\n\n\ndef create_nn(batch_size=200, learning_rate=0.01, epochs=10,\n              log_interval=10):\n\n    train_loader = torch.utils.data.DataLoader(\n        datasets.MNIST(\'../data\', train=True, download=True,\n                       transform=transforms.Compose([\n                           transforms.ToTensor(),\n                           transforms.Normalize((0.1307,), (0.3081,))\n                       ])),\n        batch_size=batch_size, shuffle=True)\n    test_loader = torch.utils.data.DataLoader(\n        datasets.MNIST(\'../data\', train=False, transform=transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.1307,), (0.3081,))\n        ])),\n        batch_size=batch_size, shuffle=True)\n\n    class Net(nn.Module):\n        def __init__(self):\n            super(Net, self).__init__()\n            self.fc1 = nn.Linear(28 * 28, 200)\n            self.fc2 = nn.Linear(200, 200)\n            self.fc3 = nn.Linear(200, 10)\n\n        def forward(self, x):\n            x = F.relu(self.fc1(x))\n            x = F.relu(self.fc2(x))\n            x = self.fc3(x)\n            return F.log_softmax(x)\n\n    net = Net()\n    print(net)\n\n    # create a stochastic gradient descent optimizer\n    optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)\n    # create a loss function\n    criterion = nn.NLLLoss()\n\n    # run the main training loop\n    for epoch in range(epochs):\n        for batch_idx, (data, target) in enumerate(train_loader):\n            data, target = Variable(data), Variable(target)\n            # resize data from (batch_size, 1, 28, 28) to (batch_size, 28*28)\n            data = data.view(-1, 28*28)\n            optimizer.zero_grad()\n            net_out = net(data)\n            loss = criterion(net_out, target)\n            loss.backward()\n            optimizer.step()\n            if batch_idx % log_interval == 0:\n                print(\'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\'.format(\n                    epoch, batch_idx * len(data), len(train_loader.dataset),\n                           100. * batch_idx / len(train_loader), loss.data[0]))\n\n    # run a test loop\n    test_loss = 0\n    correct = 0\n    for data, target in test_loader:\n        data, target = Variable(data, volatile=True), Variable(target)\n        data = data.view(-1, 28 * 28)\n        net_out = net(data)\n        # sum up batch loss\n        test_loss += criterion(net_out, target).data[0]\n        pred = net_out.data.max(1)[1]  # get the index of the max log-probability\n        correct += pred.eq(target.data).sum()\n\n    test_loss /= len(test_loader.dataset)\n    print(\'\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\'.format(\n        test_loss, correct, len(test_loader.dataset),\n        100. * correct / len(test_loader.dataset)))\n\n\nif __name__ == ""__main__"":\n    run_opt = 2\n    if run_opt == 1:\n        simple_gradient()\n    elif run_opt == 2:\n        create_nn()'"
7.Convolutional_Neural_Network(CNN) 卷积神经网络/regularization 正则化.py,5,"b""#%% [markdown]\n# # \xe6\xad\xa3\xe5\x88\x99\xe5\x8c\x96\n# \xe5\x89\x8d\xe9\x9d\xa2\xe6\x88\x91\xe4\xbb\xac\xe8\xae\xb2\xe4\xba\x86\xe6\x95\xb0\xe6\x8d\xae\xe5\xa2\x9e\xe5\xbc\xba\xe5\x92\x8c dropout\xef\xbc\x8c\xe8\x80\x8c\xe5\x9c\xa8\xe5\xae\x9e\xe9\x99\x85\xe4\xbd\xbf\xe7\x94\xa8\xe4\xb8\xad\xef\xbc\x8c\xe7\x8e\xb0\xe5\x9c\xa8\xe7\x9a\x84\xe7\xbd\x91\xe7\xbb\x9c\xe5\xbe\x80\xe5\xbe\x80\xe4\xb8\x8d\xe4\xbd\xbf\xe7\x94\xa8 dropout\xef\xbc\x8c\xe8\x80\x8c\xe6\x98\xaf\xe7\x94\xa8\xe5\x8f\xa6\xe5\xa4\x96\xe4\xb8\x80\xe4\xb8\xaa\xe6\x8a\x80\xe6\x9c\xaf\xef\xbc\x8c\xe5\x8f\xab\xe6\xad\xa3\xe5\x88\x99\xe5\x8c\x96\xe3\x80\x82\n# \n# \xe6\xad\xa3\xe5\x88\x99\xe5\x8c\x96\xe6\x98\xaf\xe6\x9c\xba\xe5\x99\xa8\xe5\xad\xa6\xe4\xb9\xa0\xe4\xb8\xad\xe6\x8f\x90\xe5\x87\xba\xe6\x9d\xa5\xe7\x9a\x84\xe4\xb8\x80\xe7\xa7\x8d\xe6\x96\xb9\xe6\xb3\x95\xef\xbc\x8c\xe6\x9c\x89 L1 \xe5\x92\x8c L2 \xe6\xad\xa3\xe5\x88\x99\xe5\x8c\x96\xef\xbc\x8c\xe7\x9b\xae\xe5\x89\x8d\xe4\xbd\xbf\xe7\x94\xa8\xe8\xbe\x83\xe5\xa4\x9a\xe7\x9a\x84\xe6\x98\xaf L2 \xe6\xad\xa3\xe5\x88\x99\xe5\x8c\x96\xef\xbc\x8c\xe5\xbc\x95\xe5\x85\xa5\xe6\xad\xa3\xe5\x88\x99\xe5\x8c\x96\xe7\x9b\xb8\xe5\xbd\x93\xe4\xba\x8e\xe5\x9c\xa8 loss \xe5\x87\xbd\xe6\x95\xb0\xe4\xb8\x8a\xe9\x9d\xa2\xe5\x8a\xa0\xe4\xb8\x8a\xe4\xb8\x80\xe9\xa1\xb9\xef\xbc\x8c\xe6\xaf\x94\xe5\xa6\x82\n# \n# $$\n# f = loss + \\lambda \\sum_{p \\in params} ||p||_2^2\n# $$\n# \n# \xe5\xb0\xb1\xe6\x98\xaf\xe5\x9c\xa8 loss \xe7\x9a\x84\xe5\x9f\xba\xe7\xa1\x80\xe4\xb8\x8a\xe5\x8a\xa0\xe4\xb8\x8a\xe4\xba\x86\xe5\x8f\x82\xe6\x95\xb0\xe7\x9a\x84\xe4\xba\x8c\xe8\x8c\x83\xe6\x95\xb0\xe4\xbd\x9c\xe4\xb8\xba\xe4\xb8\x80\xe4\xb8\xaa\xe6\xad\xa3\xe5\x88\x99\xe5\x8c\x96\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe5\x9c\xa8\xe8\xae\xad\xe7\xbb\x83\xe7\xbd\x91\xe7\xbb\x9c\xe7\x9a\x84\xe6\x97\xb6\xe5\x80\x99\xef\xbc\x8c\xe4\xb8\x8d\xe4\xbb\x85\xe8\xa6\x81\xe6\x9c\x80\xe5\xb0\x8f\xe5\x8c\x96 loss \xe5\x87\xbd\xe6\x95\xb0\xef\xbc\x8c\xe5\x90\x8c\xe6\x97\xb6\xe8\xbf\x98\xe8\xa6\x81\xe6\x9c\x80\xe5\xb0\x8f\xe5\x8c\x96\xe5\x8f\x82\xe6\x95\xb0\xe7\x9a\x84\xe4\xba\x8c\xe8\x8c\x83\xe6\x95\xb0\xef\xbc\x8c\xe4\xb9\x9f\xe5\xb0\xb1\xe6\x98\xaf\xe8\xaf\xb4\xe6\x88\x91\xe4\xbb\xac\xe4\xbc\x9a\xe5\xaf\xb9\xe5\x8f\x82\xe6\x95\xb0\xe5\x81\x9a\xe4\xb8\x80\xe4\xba\x9b\xe9\x99\x90\xe5\x88\xb6\xef\xbc\x8c\xe4\xb8\x8d\xe8\xae\xa9\xe5\xae\x83\xe5\x8f\x98\xe5\xbe\x97\xe5\xa4\xaa\xe5\xa4\xa7\xe3\x80\x82\n#%% [markdown]\n# \xe5\xa6\x82\xe6\x9e\x9c\xe6\x88\x91\xe4\xbb\xac\xe5\xaf\xb9\xe6\x96\xb0\xe7\x9a\x84\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x87\xbd\xe6\x95\xb0 f \xe6\xb1\x82\xe5\xaf\xbc\xe8\xbf\x9b\xe8\xa1\x8c\xe6\xa2\xaf\xe5\xba\xa6\xe4\xb8\x8b\xe9\x99\x8d\xef\xbc\x8c\xe5\xb0\xb1\xe6\x9c\x89\n# \n# $$\n# \\frac{\\partial f}{\\partial p_j} = \\frac{\\partial loss}{\\partial p_j} + 2 \\lambda p_j\n# $$\n# \n# \xe9\x82\xa3\xe4\xb9\x88\xe5\x9c\xa8\xe6\x9b\xb4\xe6\x96\xb0\xe5\x8f\x82\xe6\x95\xb0\xe7\x9a\x84\xe6\x97\xb6\xe5\x80\x99\xe5\xb0\xb1\xe6\x9c\x89\n# \n# $$\n# p_j \\rightarrow p_j - \\eta (\\frac{\\partial loss}{\\partial p_j} + 2 \\lambda p_j) = p_j - \\eta \\frac{\\partial loss}{\\partial p_j} - 2 \\eta \\lambda p_j \n# $$\n# \n#%% [markdown]\n# \xe5\x8f\xaf\xe4\xbb\xa5\xe7\x9c\x8b\xe5\x88\xb0 $p_j - \\eta \\frac{\\partial loss}{\\partial p_j}$ \xe5\x92\x8c\xe6\xb2\xa1\xe5\x8a\xa0\xe6\xad\xa3\xe5\x88\x99\xe9\xa1\xb9\xe8\xa6\x81\xe6\x9b\xb4\xe6\x96\xb0\xe7\x9a\x84\xe9\x83\xa8\xe5\x88\x86\xe4\xb8\x80\xe6\xa0\xb7\xef\xbc\x8c\xe8\x80\x8c\xe5\x90\x8e\xe9\x9d\xa2\xe7\x9a\x84 $2\\eta \\lambda p_j$ \xe5\xb0\xb1\xe6\x98\xaf\xe6\xad\xa3\xe5\x88\x99\xe9\xa1\xb9\xe7\x9a\x84\xe5\xbd\xb1\xe5\x93\x8d\xef\xbc\x8c\xe5\x8f\xaf\xe4\xbb\xa5\xe7\x9c\x8b\xe5\x88\xb0\xe5\x8a\xa0\xe5\xae\x8c\xe6\xad\xa3\xe5\x88\x99\xe9\xa1\xb9\xe4\xb9\x8b\xe5\x90\x8e\xe4\xbc\x9a\xe5\xaf\xb9\xe5\x8f\x82\xe6\x95\xb0\xe5\x81\x9a\xe6\x9b\xb4\xe5\xa4\xa7\xe7\xa8\x8b\xe5\xba\xa6\xe7\x9a\x84\xe6\x9b\xb4\xe6\x96\xb0\xef\xbc\x8c\xe8\xbf\x99\xe4\xb9\x9f\xe8\xa2\xab\xe7\xa7\xb0\xe4\xb8\xba\xe6\x9d\x83\xe9\x87\x8d\xe8\xa1\xb0\xe5\x87\x8f(weight decay)\xef\xbc\x8c\xe5\x9c\xa8 pytorch \xe4\xb8\xad\xe6\xad\xa3\xe5\x88\x99\xe9\xa1\xb9\xe5\xb0\xb1\xe6\x98\xaf\xe9\x80\x9a\xe8\xbf\x87\xe8\xbf\x99\xe7\xa7\x8d\xe6\x96\xb9\xe5\xbc\x8f\xe6\x9d\xa5\xe5\x8a\xa0\xe5\x85\xa5\xe7\x9a\x84\xef\xbc\x8c\xe6\xaf\x94\xe5\xa6\x82\xe6\x83\xb3\xe5\x9c\xa8\xe9\x9a\x8f\xe6\x9c\xba\xe6\xa2\xaf\xe5\xba\xa6\xe4\xb8\x8b\xe9\x99\x8d\xe6\xb3\x95\xe4\xb8\xad\xe4\xbd\xbf\xe7\x94\xa8\xe6\xad\xa3\xe5\x88\x99\xe9\xa1\xb9\xef\xbc\x8c\xe6\x88\x96\xe8\x80\x85\xe8\xaf\xb4\xe6\x9d\x83\xe9\x87\x8d\xe8\xa1\xb0\xe5\x87\x8f\xef\xbc\x8c`torch.optim.SGD(net.parameters(), lr=0.1, weight_decay=1e-4)` \xe5\xb0\xb1\xe5\x8f\xaf\xe4\xbb\xa5\xe4\xba\x86\xef\xbc\x8c\xe8\xbf\x99\xe4\xb8\xaa `weight_decay` \xe7\xb3\xbb\xe6\x95\xb0\xe5\xb0\xb1\xe6\x98\xaf\xe4\xb8\x8a\xe9\x9d\xa2\xe5\x85\xac\xe5\xbc\x8f\xe4\xb8\xad\xe7\x9a\x84 $\\lambda$\xef\xbc\x8c\xe9\x9d\x9e\xe5\xb8\xb8\xe6\x96\xb9\xe4\xbe\xbf\n# \n# \xe6\xb3\xa8\xe6\x84\x8f\xe6\xad\xa3\xe5\x88\x99\xe9\xa1\xb9\xe7\x9a\x84\xe7\xb3\xbb\xe6\x95\xb0\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f\xe9\x9d\x9e\xe5\xb8\xb8\xe9\x87\x8d\xe8\xa6\x81\xef\xbc\x8c\xe5\xa6\x82\xe6\x9e\x9c\xe5\xa4\xaa\xe5\xa4\xa7\xef\xbc\x8c\xe4\xbc\x9a\xe6\x9e\x81\xe5\xa4\xa7\xe7\x9a\x84\xe6\x8a\x91\xe5\x88\xb6\xe5\x8f\x82\xe6\x95\xb0\xe7\x9a\x84\xe6\x9b\xb4\xe6\x96\xb0\xef\xbc\x8c\xe5\xaf\xbc\xe8\x87\xb4\xe6\xac\xa0\xe6\x8b\x9f\xe5\x90\x88\xef\xbc\x8c\xe5\xa6\x82\xe6\x9e\x9c\xe5\xa4\xaa\xe5\xb0\x8f\xef\xbc\x8c\xe9\x82\xa3\xe4\xb9\x88\xe6\xad\xa3\xe5\x88\x99\xe9\xa1\xb9\xe8\xbf\x99\xe4\xb8\xaa\xe9\x83\xa8\xe5\x88\x86\xe5\x9f\xba\xe6\x9c\xac\xe6\xb2\xa1\xe6\x9c\x89\xe8\xb4\xa1\xe7\x8c\xae\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe9\x80\x89\xe6\x8b\xa9\xe4\xb8\x80\xe4\xb8\xaa\xe5\x90\x88\xe9\x80\x82\xe7\x9a\x84\xe6\x9d\x83\xe9\x87\x8d\xe8\xa1\xb0\xe5\x87\x8f\xe7\xb3\xbb\xe6\x95\xb0\xe9\x9d\x9e\xe5\xb8\xb8\xe9\x87\x8d\xe8\xa6\x81\xef\xbc\x8c\xe8\xbf\x99\xe4\xb8\xaa\xe9\x9c\x80\xe8\xa6\x81\xe6\xa0\xb9\xe6\x8d\xae\xe5\x85\xb7\xe4\xbd\x93\xe7\x9a\x84\xe6\x83\x85\xe5\x86\xb5\xe5\x8e\xbb\xe5\xb0\x9d\xe8\xaf\x95\xef\xbc\x8c\xe5\x88\x9d\xe6\xad\xa5\xe5\xb0\x9d\xe8\xaf\x95\xe5\x8f\xaf\xe4\xbb\xa5\xe4\xbd\xbf\xe7\x94\xa8 `1e-4` \xe6\x88\x96\xe8\x80\x85 `1e-3` \n# \n# \xe4\xb8\x8b\xe9\x9d\xa2\xe6\x88\x91\xe4\xbb\xac\xe5\x9c\xa8\xe8\xae\xad\xe7\xbb\x83 cifar 10 \xe4\xb8\xad\xe6\xb7\xbb\xe5\x8a\xa0\xe6\xad\xa3\xe5\x88\x99\xe9\xa1\xb9\n\n#%%\nimport sys\nsys.path.append('..')\n\nimport numpy as np\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torchvision.datasets import CIFAR10\nfrom utils import train, resnet\nfrom torchvision import transforms as tfs\n\n\n#%%\ndef data_tf(x):\n    im_aug = tfs.Compose([\n        tfs.Resize(96),\n        tfs.ToTensor(),\n        tfs.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n    ])\n    x = im_aug(x)\n    return x\n\ntrain_set = CIFAR10('./data', train=True, transform=data_tf)\ntrain_data = torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=True, num_workers=4)\ntest_set = CIFAR10('./data', train=False, transform=data_tf)\ntest_data = torch.utils.data.DataLoader(test_set, batch_size=128, shuffle=False, num_workers=4)\n\nnet = resnet(3, 10)\noptimizer = torch.optim.SGD(net.parameters(), lr=0.01, weight_decay=1e-4) # \xe5\xa2\x9e\xe5\x8a\xa0\xe6\xad\xa3\xe5\x88\x99\xe9\xa1\xb9\ncriterion = nn.CrossEntropyLoss()\n\n\n#%%\nfrom utils import train\ntrain(net, train_data, test_data, 20, optimizer, criterion)\n\n\n"""
7.Convolutional_Neural_Network(CNN) 卷积神经网络/resnet.py,3,"b'import  torch\nfrom    torch import  nn\nfrom    torch.nn import functional as F\n\n\n\nclass ResBlk(nn.Module):\n    """"""\n    resnet block\n    """"""\n\n    def __init__(self, ch_in, ch_out):\n        """"""\n\n        :param ch_in:\n        :param ch_out:\n        """"""\n        super(ResBlk, self).__init__()\n\n        self.conv1 = nn.Conv2d(ch_in, ch_out, kernel_size=3, stride=1, padding=1)\n        self.bn1 = nn.BatchNorm2d(ch_out)\n        self.conv2 = nn.Conv2d(ch_out, ch_out, kernel_size=3, stride=1, padding=1)\n        self.bn2 = nn.BatchNorm2d(ch_out)\n\n        self.extra = nn.Sequential()\n        if ch_out != ch_in:\n            # [b, ch_in, h, w] => [b, ch_out, h, w]\n            self.extra = nn.Sequential(\n                nn.Conv2d(ch_in, ch_out, kernel_size=1, stride=1),\n                nn.BatchNorm2d(ch_out)\n            )\n\n\n    def forward(self, x):\n        """"""\n\n        :param x: [b, ch, h, w]\n        :return:\n        """"""\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        # short cut.\n        # extra module: [b, ch_in, h, w] => [b, ch_out, h, w]\n        # element-wise add:\n        out = self.extra(x) + out\n\n        return out\n\n\n\n\nclass ResNet18(nn.Module):\n\n    def __init__(self):\n        super(ResNet18, self).__init__()\n\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(16)\n        )\n        # followed 4 blocks\n        # [b, 64, h, w] => [b, 128, h ,w]\n        self.blk1 = ResBlk(16, 16)\n        # [b, 128, h, w] => [b, 256, h, w]\n        self.blk2 = ResBlk(16, 32)\n        # # [b, 256, h, w] => [b, 512, h, w]\n        # self.blk3 = ResBlk(128, 256)\n        # # [b, 512, h, w] => [b, 1024, h, w]\n        # self.blk4 = ResBlk(256, 512)\n\n        self.outlayer = nn.Linear(32*32*32, 10)\n\n    def forward(self, x):\n        """"""\n\n        :param x:\n        :return:\n        """"""\n        x = F.relu(self.conv1(x))\n\n        # [b, 64, h, w] => [b, 1024, h, w]\n        x = self.blk1(x)\n        x = self.blk2(x)\n        # x = self.blk3(x)\n        # x = self.blk4(x)\n\n        # print(x.shape)\n        x = x.view(x.size(0), -1)\n        x = self.outlayer(x)\n\n\n        return x\n\n\n\ndef main():\n    blk = ResBlk(64, 128)\n    tmp = torch.randn(2, 64,32, 32)\n    out = blk(tmp)\n    print(\'blkk\', out.shape)\n\n\n    model = ResNet18()\n    tmp = torch.randn(2, 3, 32, 32)\n    out = model(tmp)\n    print(\'resnet:\', out.shape)\n\n\nif __name__ == \'__main__\':\n    main()'"
7.Convolutional_Neural_Network(CNN) 卷积神经网络/simpleCNN.py,1,"b""import torch\nfrom torch import nn, optim\n\nclass SimpleCNN(nn.Module) :\n    def __init__(self) :\n        # b, 3, 32, 32\n        super().__init__()\n        layer1 = nn.Sequential()\n        layer1.add_module('conv_1', nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1))\n        # b, 32, 32, 32\n        layer1.add_module('relu_1', nn.ReLU(True))\n        layer1.add_module('pool_1', nn.MaxPool2d(2, 2))  # b, 32, 16, 16\n        self.layer1 = layer1\n\n        layer2 = nn.Sequential()\n        layer2.add_module('conv_2', nn.Conv2d(32, 64, 3, 1, padding=1))\n        # b, 64, 16, 16\n        layer2.add_module('relu_2', nn.ReLU(True))\n        layer2.add_module('pool_2', nn.MaxPool2d(2, 2))  # b, 64, 8, 8\n        self.layer2 = layer2\n\n        layer3 = nn.Sequential()\n        layer3.add_module('conv_3', nn.Conv2d(64, 128, 3, 1, padding=1))\n        # b, 128, 8, 8\n        layer3.add_module('relu_3', nn.ReLU(True))\n        layer3.add_module('pool_3', nn.MaxPool2d(2, 2))  # b, 128, 4, 4\n        self.layer3 = layer3\n\n        layer4 = nn.Sequential()\n        layer4.add_module('fc_1', nn.Linear(2048, 512))\n        layer4.add_module('fc_relu1', nn.ReLU(True))\n        layer4.add_module('fc_2', nn.Linear(512, 64))\n        layer4.add_module('fc_relu2', nn.ReLU(True))\n        layer4.add_module('fc_3', nn.Linear(64, 10))\n        self.layer4 = layer4\n\n    def forward(self, x):\n        conv1 = self.layer1(x)\n        conv2 = self.layer2(conv1)\n        conv3 = self.layer3(conv2)\n        fc_input = conv3.view(conv3.size(0), -1)\n        fc_out = self.layer4(fc_input)\n        return fc_out\n\n\n\nmodel = SimpleCNN()\nprint(model)\n\na = torch.FloatTensor([[1,2,3],[4,5,6]])\nprint(a)\n\n"""
7.Convolutional_Neural_Network(CNN) 卷积神经网络/utils.py,3,"b'from datetime import datetime\n\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\n\n\ndef get_acc(output, label):\n    total = output.shape[0]\n    _, pred_label = output.max(1)\n    num_correct = (pred_label == label).sum().item()\n    return num_correct / total\n\n\ndef train(net, train_data, valid_data, num_epochs, optimizer, criterion):\n\n    device = torch.device(\'cuda\' if torch.cuda.is_available() else \'cpu\')\n    net = net.to(device)\n    prev_time = datetime.now()\n    for epoch in range(num_epochs):\n        train_loss = 0\n        train_acc = 0\n        net = net.train()\n        for im, label in train_data:\n            im = im.to(device)\n            label = label.to(device)\n            # forward\n            output = net(im)\n            loss = criterion(output, label)\n            # backward\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            train_loss += loss.item()\n            train_acc += get_acc(output, label)\n\n        cur_time = datetime.now()\n        h, remainder = divmod((cur_time - prev_time).seconds, 3600)\n        m, s = divmod(remainder, 60)\n        time_str = ""Time %02d:%02d:%02d"" % (h, m, s)\n        if valid_data is not None:\n            valid_loss = 0\n            valid_acc = 0\n            net = net.eval()\n            with torch.no_grad():\n                for im, label in valid_data:\n                    im = im.to(device)\n                    label = label.to(device)\n                    output = net(im)\n                    loss = criterion(output, label)\n                    valid_loss += loss.item()\n                    valid_acc += get_acc(output, label)\n            \n            epoch_str = (\n                ""Epoch %d. Train Loss: %f, Train Acc: %f, Valid Loss: %f, Valid Acc: %f, ""\n                % (epoch, train_loss / len(train_data),\n                   train_acc / len(train_data), valid_loss / len(valid_data),\n                   valid_acc / len(valid_data)))\n        else:\n            epoch_str = (""Epoch %d. Train Loss: %f, Train Acc: %f, "" %\n                         (epoch, train_loss / len(train_data),\n                          train_acc / len(train_data)))\n        prev_time = cur_time\n        print(epoch_str + time_str)\n\n\ndef conv3x3(in_channel, out_channel, stride=1):\n    return nn.Conv2d(\n        in_channel, out_channel, 3, stride=stride, padding=1, bias=False)\n\n\nclass residual_block(nn.Module):\n    def __init__(self, in_channel, out_channel, same_shape=True):\n        super(residual_block, self).__init__()\n        self.same_shape = same_shape\n        stride = 1 if self.same_shape else 2\n\n        self.conv1 = conv3x3(in_channel, out_channel, stride=stride)\n        self.bn1 = nn.BatchNorm2d(out_channel)\n\n        self.conv2 = conv3x3(out_channel, out_channel)\n        self.bn2 = nn.BatchNorm2d(out_channel)\n        if not self.same_shape:\n            self.conv3 = nn.Conv2d(in_channel, out_channel, 1, stride=stride)\n\n    def forward(self, x):\n        out = self.conv1(x)\n        out = F.relu(self.bn1(out), True)\n        out = self.conv2(out)\n        out = F.relu(self.bn2(out), True)\n\n        if not self.same_shape:\n            x = self.conv3(x)\n        return F.relu(x + out, True)\n\n\nclass resnet(nn.Module):\n    def __init__(self, in_channel, num_classes, verbose=False):\n        super(resnet, self).__init__()\n        self.verbose = verbose\n\n        self.block1 = nn.Conv2d(in_channel, 64, 7, 2)\n\n        self.block2 = nn.Sequential(\n            nn.MaxPool2d(3, 2), residual_block(64, 64), residual_block(64, 64))\n\n        self.block3 = nn.Sequential(\n            residual_block(64, 128, False), residual_block(128, 128))\n\n        self.block4 = nn.Sequential(\n            residual_block(128, 256, False), residual_block(256, 256))\n\n        self.block5 = nn.Sequential(\n            residual_block(256, 512, False),\n            residual_block(512, 512), nn.AvgPool2d(3))\n\n        self.classifier = nn.Linear(512, num_classes)\n\n    def forward(self, x):\n        x = self.block1(x)\n        if self.verbose:\n            print(\'block 1 output: {}\'.format(x.shape))\n        x = self.block2(x)\n        if self.verbose:\n            print(\'block 2 output: {}\'.format(x.shape))\n        x = self.block3(x)\n        if self.verbose:\n            print(\'block 3 output: {}\'.format(x.shape))\n        x = self.block4(x)\n        if self.verbose:\n            print(\'block 4 output: {}\'.format(x.shape))\n        x = self.block5(x)\n        if self.verbose:\n            print(\'block 5 output: {}\'.format(x.shape))\n        x = x.view(x.shape[0], -1)\n        x = self.classifier(x)\n        return x\n'"
7.Convolutional_Neural_Network(CNN) 卷积神经网络/多层全连接神经网络.py,1,"b'#\xe5\xa4\x9a\xe5\xb1\x82\xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\nimport torch\nfrom torch import nn,optim\nfrom torch.utils.data import dataloader\n\n\n#\xe7\xac\xac\xe4\xb8\x80\xe7\xa7\x8d \xe7\xae\x80\xe5\x8d\x95\xe7\x9a\x84\xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe7\x9a\x84\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\nclass simpleNet(nn.Module):\n\n    def __init__(self,in_dim,n_hidden_1,n_hidden_2,out_dim):\n        super(simpleNet,self).__init__()\n        self.layer1=nn.Linear(in_dim,n_hidden_1)\n        self.layer2=nn.Linear(n_hidden_1,n_hidden_2)\n        self.layer3=nn.Linear(n_hidden_2,out_dim)\n\n    def forward(self, x):\n        x=self.layer1(x)\n        x=self.layer2(x)\n        x=self.layer3(x)\n        return x\n\n#\xe7\xac\xac\xe4\xba\x8c\xe7\xa7\x8d\xe6\xb7\xbb\xe5\x8a\xa0\xe6\xbf\x80\xe6\xb4\xbb\xe5\x87\xbd\xe6\x95\xb0\xef\xbc\x8c\xe5\xa2\x9e\xe5\x8a\xa0\xe7\xbd\x91\xe7\xbb\x9c\xe7\x9a\x84\xe9\x9d\x9e\xe7\xba\xbf\xe6\x80\xa7\nclass Actication_Net(nn.Module):\n\n    def __init__(self,in_dim,n_hidden_1,n_hidden_2,out_dim):\n        super(Actication_Net,self).__init__()\n        self.layer1 = nn.Sequential(\n            nn.Linear(in_dim,n_hidden_1),\n            nn.ReLU(True)\n        )\n\n        self.layer2 = nn.Sequential(\n            nn.Linear(n_hidden_1,n_hidden_2),\n            nn.ReLU(True)\n        )\n\n        self.layer3 = nn.Sequential(\n            nn.Linear(n_hidden_2,out_dim)\n        )\n\n    def forward(self, x):\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        return x\n\n#\xe7\xac\xac\xe4\xb8\x89\xe7\xa7\x8d \xe6\xb7\xbb\xe5\x8a\xa0\xe6\x89\xb9\xe5\xa4\x84\xe7\x90\x86\xe6\xa0\x87\xe5\x87\x86\xe5\x8c\x96 \xef\xbc\x8c\xe5\x8a\xa0\xe5\xbf\xab\xe6\x94\xb6\xe6\x95\x9b\xe7\x9a\x84\xe9\x80\x9f\xe5\xba\xa6\nclass Batch_Net(nn.Module):\n\n    def __init__(self,in_dim,n_hidden_1,n_hidden_2,out_dim):\n\n        super(Batch_Net,self).__init__()\n        self.layer1 = nn.Sequential(\n            nn.Linear(in_dim,n_hidden_1),\n            nn.BatchNorm1d(n_hidden_1),\n            nn.ReLU(True)\n        )\n\n        self.layer2 = nn.Sequential(\n            nn.Linear(n_hidden_1,n_hidden_2),\n            nn.BatchNorm1d(n_hidden_2),\n            nn.ReLU(True)\n        )\n\n        self.layer3 = nn.Sequential(\n            nn.Linear(n_hidden_2,out_dim)\n        )\n\n    def forward(self, x):\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        return x\n\n\n'"
8.Famous_CNN 经典的CNN网络/DenseNet.py,8,"b""#%% [markdown]\n# # DenseNet\n# \xe5\x9b\xa0\xe4\xb8\xba ResNet \xe6\x8f\x90\xe5\x87\xba\xe4\xba\x86\xe8\xb7\xa8\xe5\xb1\x82\xe9\x93\xbe\xe6\x8e\xa5\xe7\x9a\x84\xe6\x80\x9d\xe6\x83\xb3\xef\xbc\x8c\xe8\xbf\x99\xe7\x9b\xb4\xe6\x8e\xa5\xe5\xbd\xb1\xe5\x93\x8d\xe4\xba\x86\xe9\x9a\x8f\xe5\x90\x8e\xe5\x87\xba\xe7\x8e\xb0\xe7\x9a\x84\xe5\x8d\xb7\xe7\xa7\xaf\xe7\xbd\x91\xe7\xbb\x9c\xe6\x9e\xb6\xe6\x9e\x84\xef\xbc\x8c\xe5\x85\xb6\xe4\xb8\xad\xe6\x9c\x80\xe6\x9c\x89\xe5\x90\x8d\xe7\x9a\x84\xe5\xb0\xb1\xe6\x98\xaf cvpr 2017 \xe7\x9a\x84 best paper\xef\xbc\x8cDenseNet\xe3\x80\x82\n# \n# DenseNet \xe5\x92\x8c ResNet \xe4\xb8\x8d\xe5\x90\x8c\xe5\x9c\xa8\xe4\xba\x8e ResNet \xe6\x98\xaf\xe8\xb7\xa8\xe5\xb1\x82\xe6\xb1\x82\xe5\x92\x8c\xef\xbc\x8c\xe8\x80\x8c DenseNet \xe6\x98\xaf\xe8\xb7\xa8\xe5\xb1\x82\xe5\xb0\x86\xe7\x89\xb9\xe5\xbe\x81\xe5\x9c\xa8\xe9\x80\x9a\xe9\x81\x93\xe7\xbb\xb4\xe5\xba\xa6\xe8\xbf\x9b\xe8\xa1\x8c\xe6\x8b\xbc\xe6\x8e\xa5\xef\xbc\x8c\xe4\xb8\x8b\xe9\x9d\xa2\xe5\x8f\xaf\xe4\xbb\xa5\xe7\x9c\x8b\xe7\x9c\x8b\xe4\xbb\x96\xe4\xbb\xac\xe4\xb8\xa4\xe8\x80\x85\xe7\x9a\x84\xe5\x9b\xbe\xe7\xa4\xba\n# \n# ![](https://ws4.sinaimg.cn/large/006tNc79ly1fmpvj5vkfhj30uw0anq73.jpg)\n# \n# ![](https://ws1.sinaimg.cn/large/006tNc79ly1fmpvj7fxd1j30vb0eyzqf.jpg)\n#%% [markdown]\n# \xe7\xac\xac\xe4\xb8\x80\xe5\xbc\xa0\xe5\x9b\xbe\xe6\x98\xaf ResNet\xef\xbc\x8c\xe7\xac\xac\xe4\xba\x8c\xe5\xbc\xa0\xe5\x9b\xbe\xe6\x98\xaf DenseNet\xef\xbc\x8c\xe5\x9b\xa0\xe4\xb8\xba\xe6\x98\xaf\xe5\x9c\xa8\xe9\x80\x9a\xe9\x81\x93\xe7\xbb\xb4\xe5\xba\xa6\xe8\xbf\x9b\xe8\xa1\x8c\xe7\x89\xb9\xe5\xbe\x81\xe7\x9a\x84\xe6\x8b\xbc\xe6\x8e\xa5\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe5\xba\x95\xe5\xb1\x82\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe4\xbc\x9a\xe4\xbf\x9d\xe7\x95\x99\xe8\xbf\x9b\xe5\x85\xa5\xe6\x89\x80\xe6\x9c\x89\xe5\x90\x8e\xe9\x9d\xa2\xe7\x9a\x84\xe5\xb1\x82\xef\xbc\x8c\xe8\xbf\x99\xe8\x83\xbd\xe5\xa4\x9f\xe6\x9b\xb4\xe5\xa5\xbd\xe7\x9a\x84\xe4\xbf\x9d\xe8\xaf\x81\xe6\xa2\xaf\xe5\xba\xa6\xe7\x9a\x84\xe4\xbc\xa0\xe6\x92\xad\xef\xbc\x8c\xe5\x90\x8c\xe6\x97\xb6\xe8\x83\xbd\xe5\xa4\x9f\xe4\xbd\xbf\xe7\x94\xa8\xe4\xbd\x8e\xe7\xbb\xb4\xe7\x9a\x84\xe7\x89\xb9\xe5\xbe\x81\xe5\x92\x8c\xe9\xab\x98\xe7\xbb\xb4\xe7\x9a\x84\xe7\x89\xb9\xe5\xbe\x81\xe8\xbf\x9b\xe8\xa1\x8c\xe8\x81\x94\xe5\x90\x88\xe8\xae\xad\xe7\xbb\x83\xef\xbc\x8c\xe8\x83\xbd\xe5\xa4\x9f\xe5\xbe\x97\xe5\x88\xb0\xe6\x9b\xb4\xe5\xa5\xbd\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x9c\xe3\x80\x82\n#%% [markdown]\n# DenseNet \xe4\xb8\xbb\xe8\xa6\x81\xe7\x94\xb1 dense block \xe6\x9e\x84\xe6\x88\x90\xef\xbc\x8c\xe4\xb8\x8b\xe9\x9d\xa2\xe6\x88\x91\xe4\xbb\xac\xe6\x9d\xa5\xe5\xae\x9e\xe7\x8e\xb0\xe4\xb8\x80\xe4\xb8\xaa densen block\n\n#%%\nimport sys\nsys.path.append('..')\n\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torchvision.datasets import CIFAR10\n\n#%% [markdown]\n# \xe9\xa6\x96\xe5\x85\x88\xe5\xae\x9a\xe4\xb9\x89\xe4\xb8\x80\xe4\xb8\xaa\xe5\x8d\xb7\xe7\xa7\xaf\xe5\x9d\x97\xef\xbc\x8c\xe8\xbf\x99\xe4\xb8\xaa\xe5\x8d\xb7\xe7\xa7\xaf\xe5\x9d\x97\xe7\x9a\x84\xe9\xa1\xba\xe5\xba\x8f\xe6\x98\xaf bn -> relu -> conv\n\n#%%\ndef conv_block(in_channel, out_channel):\n    layer = nn.Sequential(\n        nn.BatchNorm2d(in_channel),\n        nn.ReLU(True),\n        nn.Conv2d(in_channel, out_channel, 3, padding=1, bias=False)\n    )\n    return layer\n\n#%% [markdown]\n# dense block \xe5\xb0\x86\xe6\xaf\x8f\xe6\xac\xa1\xe7\x9a\x84\xe5\x8d\xb7\xe7\xa7\xaf\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe7\xa7\xb0\xe4\xb8\xba `growth_rate`\xef\xbc\x8c\xe5\x9b\xa0\xe4\xb8\xba\xe5\xa6\x82\xe6\x9e\x9c\xe8\xbe\x93\xe5\x85\xa5\xe6\x98\xaf `in_channel`\xef\xbc\x8c\xe6\x9c\x89 n \xe5\xb1\x82\xef\xbc\x8c\xe9\x82\xa3\xe4\xb9\x88\xe8\xbe\x93\xe5\x87\xba\xe5\xb0\xb1\xe6\x98\xaf `in_channel + n * growh_rate`\n\n#%%\nclass dense_block(nn.Module):\n    def __init__(self, in_channel, growth_rate, num_layers):\n        super(dense_block, self).__init__()\n        block = []\n        channel = in_channel\n        for i in range(num_layers):\n            block.append(conv_block(channel, growth_rate))\n            channel += growth_rate\n            \n        self.net = nn.Sequential(*block)\n        \n    def forward(self, x):\n        for layer in self.net:\n            out = layer(x)\n            x = torch.cat((out, x), dim=1)\n        return x\n\n#%% [markdown]\n# \xe6\x88\x91\xe4\xbb\xac\xe9\xaa\x8c\xe8\xaf\x81\xe4\xb8\x80\xe4\xb8\x8b\xe8\xbe\x93\xe5\x87\xba\xe7\x9a\x84 channel \xe6\x98\xaf\xe5\x90\xa6\xe6\xad\xa3\xe7\xa1\xae\n\n#%%\ntest_net = dense_block(3, 12, 3)\ntest_x = torch.zeros(1, 3, 96, 96)\nprint('input shape: {} x {} x {}'.format(test_x.shape[1], test_x.shape[2], test_x.shape[3]))\ntest_y = test_net(test_x)\nprint('output shape: {} x {} x {}'.format(test_y.shape[1], test_y.shape[2], test_y.shape[3]))\n\n#%% [markdown]\n# \xe9\x99\xa4\xe4\xba\x86 dense block\xef\xbc\x8cDenseNet \xe4\xb8\xad\xe8\xbf\x98\xe6\x9c\x89\xe4\xb8\x80\xe4\xb8\xaa\xe6\xa8\xa1\xe5\x9d\x97\xe5\x8f\xab\xe8\xbf\x87\xe6\xb8\xa1\xe5\xb1\x82\xef\xbc\x88transition block\xef\xbc\x89\xef\xbc\x8c\xe5\x9b\xa0\xe4\xb8\xba DenseNet \xe4\xbc\x9a\xe4\xb8\x8d\xe6\x96\xad\xe5\x9c\xb0\xe5\xaf\xb9\xe7\xbb\xb4\xe5\xba\xa6\xe8\xbf\x9b\xe8\xa1\x8c\xe6\x8b\xbc\xe6\x8e\xa5\xef\xbc\x8c \xe6\x89\x80\xe4\xbb\xa5\xe5\xbd\x93\xe5\xb1\x82\xe6\x95\xb0\xe5\xbe\x88\xe9\xab\x98\xe7\x9a\x84\xe6\x97\xb6\xe5\x80\x99\xef\xbc\x8c\xe8\xbe\x93\xe5\x87\xba\xe7\x9a\x84\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\xe5\xb0\xb1\xe4\xbc\x9a\xe8\xb6\x8a\xe6\x9d\xa5\xe8\xb6\x8a\xe5\xa4\xa7\xef\xbc\x8c\xe5\x8f\x82\xe6\x95\xb0\xe5\x92\x8c\xe8\xae\xa1\xe7\xae\x97\xe9\x87\x8f\xe4\xb9\x9f\xe4\xbc\x9a\xe8\xb6\x8a\xe6\x9d\xa5\xe8\xb6\x8a\xe5\xa4\xa7\xef\xbc\x8c\xe4\xb8\xba\xe4\xba\x86\xe9\x81\xbf\xe5\x85\x8d\xe8\xbf\x99\xe4\xb8\xaa\xe9\x97\xae\xe9\xa2\x98\xef\xbc\x8c\xe9\x9c\x80\xe8\xa6\x81\xe5\xbc\x95\xe5\x85\xa5\xe8\xbf\x87\xe6\xb8\xa1\xe5\xb1\x82\xe5\xb0\x86\xe8\xbe\x93\xe5\x87\xba\xe9\x80\x9a\xe9\x81\x93\xe9\x99\x8d\xe4\xbd\x8e\xe4\xb8\x8b\xe6\x9d\xa5\xef\xbc\x8c\xe5\x90\x8c\xe6\x97\xb6\xe4\xb9\x9f\xe5\xb0\x86\xe8\xbe\x93\xe5\x85\xa5\xe7\x9a\x84\xe9\x95\xbf\xe5\xae\xbd\xe5\x87\x8f\xe5\x8d\x8a\xef\xbc\x8c\xe8\xbf\x99\xe4\xb8\xaa\xe8\xbf\x87\xe6\xb8\xa1\xe5\xb1\x82\xe5\x8f\xaf\xe4\xbb\xa5\xe4\xbd\xbf\xe7\x94\xa8 1 x 1 \xe7\x9a\x84\xe5\x8d\xb7\xe7\xa7\xaf\n\n#%%\ndef transition(in_channel, out_channel):\n    trans_layer = nn.Sequential(\n        nn.BatchNorm2d(in_channel),\n        nn.ReLU(True),\n        nn.Conv2d(in_channel, out_channel, 1),\n        nn.AvgPool2d(2, 2)\n    )\n    return trans_layer\n\n#%% [markdown]\n# \xe9\xaa\x8c\xe8\xaf\x81\xe4\xb8\x80\xe4\xb8\x8b\xe8\xbf\x87\xe6\xb8\xa1\xe5\xb1\x82\xe6\x98\xaf\xe5\x90\xa6\xe6\xad\xa3\xe7\xa1\xae\n\n#%%\ntest_net = transition(3, 12)\ntest_x = torch.zeros(1, 3, 96, 96)\nprint('input shape: {} x {} x {}'.format(test_x.shape[1], test_x.shape[2], test_x.shape[3]))\ntest_y = test_net(test_x)\nprint('output shape: {} x {} x {}'.format(test_y.shape[1], test_y.shape[2], test_y.shape[3]))\n\n#%% [markdown]\n# \xe6\x9c\x80\xe5\x90\x8e\xe6\x88\x91\xe4\xbb\xac\xe5\xae\x9a\xe4\xb9\x89 DenseNet\n\n#%%\nclass densenet(nn.Module):\n    def __init__(self, in_channel, num_classes, growth_rate=32, block_layers=[6, 12, 24, 16]):\n        super(densenet, self).__init__()\n        self.block1 = nn.Sequential(\n            nn.Conv2d(in_channel, 64, 7, 2, 3),\n            nn.BatchNorm2d(64),\n            nn.ReLU(True),\n            nn.MaxPool2d(3, 2, padding=1)\n        )\n        \n        channels = 64\n        block = []\n        for i, layers in enumerate(block_layers):\n            block.append(dense_block(channels, growth_rate, layers))\n            channels += layers * growth_rate\n            if i != len(block_layers) - 1:\n                block.append(transition(channels, channels // 2)) # \xe9\x80\x9a\xe8\xbf\x87 transition \xe5\xb1\x82\xe5\xb0\x86\xe5\xa4\xa7\xe5\xb0\x8f\xe5\x87\x8f\xe5\x8d\x8a\xef\xbc\x8c\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\xe5\x87\x8f\xe5\x8d\x8a\n                channels = channels // 2\n        \n        self.block2 = nn.Sequential(*block)\n        self.block2.add_module('bn', nn.BatchNorm2d(channels))\n        self.block2.add_module('relu', nn.ReLU(True))\n        self.block2.add_module('avg_pool', nn.AvgPool2d(3))\n        \n        self.classifier = nn.Linear(channels, num_classes)\n    \n    def forward(self, x):\n        x = self.block1(x)\n        x = self.block2(x)\n        \n        x = x.view(x.shape[0], -1)\n        x = self.classifier(x)\n        return x\n\n\n#%%\ntest_net = densenet(3, 10)\ntest_x = torch.zeros(1, 3, 96, 96)\ntest_y = test_net(test_x)\nprint('output: {}'.format(test_y.shape))\n\n\n#%%\nfrom utils import train\n\ndef data_tf(x):\n    x = x.resize((96, 96), 2) # \xe5\xb0\x86\xe5\x9b\xbe\xe7\x89\x87\xe6\x94\xbe\xe5\xa4\xa7\xe5\x88\xb0 96 x 96\n    x = np.array(x, dtype='float32') / 255\n    x = (x - 0.5) / 0.5 # \xe6\xa0\x87\xe5\x87\x86\xe5\x8c\x96\xef\xbc\x8c\xe8\xbf\x99\xe4\xb8\xaa\xe6\x8a\x80\xe5\xb7\xa7\xe4\xb9\x8b\xe5\x90\x8e\xe4\xbc\x9a\xe8\xae\xb2\xe5\x88\xb0\n    x = x.transpose((2, 0, 1)) # \xe5\xb0\x86 channel \xe6\x94\xbe\xe5\x88\xb0\xe7\xac\xac\xe4\xb8\x80\xe7\xbb\xb4\xef\xbc\x8c\xe5\x8f\xaa\xe6\x98\xaf pytorch \xe8\xa6\x81\xe6\xb1\x82\xe7\x9a\x84\xe8\xbe\x93\xe5\x85\xa5\xe6\x96\xb9\xe5\xbc\x8f\n    x = torch.from_numpy(x)\n    return x\n     \ntrain_set = CIFAR10('./data', train=True, transform=data_tf)\ntrain_data = torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=True)\ntest_set = CIFAR10('./data', train=False, transform=data_tf)\ntest_data = torch.utils.data.DataLoader(test_set, batch_size=128, shuffle=False)\n\nnet = densenet(3, 10)\noptimizer = torch.optim.SGD(net.parameters(), lr=0.01)\ncriterion = nn.CrossEntropyLoss()\n\n\n#%%\ntrain(net, train_data, test_data, 20, optimizer, criterion)\n\n#%% [markdown]\n# DenseNet \xe5\xb0\x86\xe6\xae\x8b\xe5\xb7\xae\xe8\xbf\x9e\xe6\x8e\xa5\xe6\x94\xb9\xe4\xb8\xba\xe4\xba\x86\xe7\x89\xb9\xe5\xbe\x81\xe6\x8b\xbc\xe6\x8e\xa5\xef\xbc\x8c\xe4\xbd\xbf\xe5\xbe\x97\xe7\xbd\x91\xe7\xbb\x9c\xe6\x9c\x89\xe4\xba\x86\xe6\x9b\xb4\xe7\xa8\xa0\xe5\xaf\x86\xe7\x9a\x84\xe8\xbf\x9e\xe6\x8e\xa5\n\n"""
8.Famous_CNN 经典的CNN网络/GoogLeNet.py,7,"b""#%% [markdown]\n# # GoogLeNet\n# \xe5\x89\x8d\xe9\x9d\xa2\xe6\x88\x91\xe4\xbb\xac\xe8\xae\xb2\xe7\x9a\x84 VGG \xe6\x98\xaf 2014 \xe5\xb9\xb4 ImageNet \xe6\xaf\x94\xe8\xb5\x9b\xe7\x9a\x84\xe4\xba\x9a\xe5\x86\x9b\xef\xbc\x8c\xe9\x82\xa3\xe4\xb9\x88\xe5\x86\xa0\xe5\x86\x9b\xe6\x98\xaf\xe8\xb0\x81\xe5\x91\xa2\xef\xbc\x9f\xe5\xb0\xb1\xe6\x98\xaf\xe6\x88\x91\xe4\xbb\xac\xe9\xa9\xac\xe4\xb8\x8a\xe8\xa6\x81\xe8\xae\xb2\xe7\x9a\x84 GoogLeNet\xef\xbc\x8c\xe8\xbf\x99\xe6\x98\xaf Google \xe7\x9a\x84\xe7\xa0\x94\xe7\xa9\xb6\xe4\xba\xba\xe5\x91\x98\xe6\x8f\x90\xe5\x87\xba\xe7\x9a\x84\xe7\xbd\x91\xe7\xbb\x9c\xe7\xbb\x93\xe6\x9e\x84\xef\xbc\x8c\xe5\x9c\xa8\xe5\xbd\x93\xe6\x97\xb6\xe5\x8f\x96\xe5\xbe\x97\xe4\xba\x86\xe9\x9d\x9e\xe5\xb8\xb8\xe5\xa4\xa7\xe7\x9a\x84\xe5\xbd\xb1\xe5\x93\x8d\xef\xbc\x8c\xe5\x9b\xa0\xe4\xb8\xba\xe7\xbd\x91\xe7\xbb\x9c\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x84\xe5\x8f\x98\xe5\xbe\x97\xe5\x89\x8d\xe6\x89\x80\xe6\x9c\xaa\xe6\x9c\x89\xef\xbc\x8c\xe5\xae\x83\xe9\xa2\xa0\xe8\xa6\x86\xe4\xba\x86\xe5\xa4\xa7\xe5\xae\xb6\xe5\xaf\xb9\xe5\x8d\xb7\xe7\xa7\xaf\xe7\xbd\x91\xe7\xbb\x9c\xe7\x9a\x84\xe4\xb8\xb2\xe8\x81\x94\xe7\x9a\x84\xe5\x8d\xb0\xe8\xb1\xa1\xe5\x92\x8c\xe5\x9b\xba\xe5\xae\x9a\xe5\x81\x9a\xe6\xb3\x95\xef\xbc\x8c\xe9\x87\x87\xe7\x94\xa8\xe4\xba\x86\xe4\xb8\x80\xe7\xa7\x8d\xe9\x9d\x9e\xe5\xb8\xb8\xe6\x9c\x89\xe6\x95\x88\xe7\x9a\x84 inception \xe6\xa8\xa1\xe5\x9d\x97\xef\xbc\x8c\xe5\xbe\x97\xe5\x88\xb0\xe4\xba\x86\xe6\xaf\x94 VGG \xe6\x9b\xb4\xe6\xb7\xb1\xe7\x9a\x84\xe7\xbd\x91\xe7\xbb\x9c\xe7\xbb\x93\xe6\x9e\x84\xef\xbc\x8c\xe4\xbd\x86\xe6\x98\xaf\xe5\x8d\xb4\xe6\xaf\x94 VGG \xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0\xe6\x9b\xb4\xe5\xb0\x91\xef\xbc\x8c\xe5\x9b\xa0\xe4\xb8\xba\xe5\x85\xb6\xe5\x8e\xbb\xe6\x8e\x89\xe4\xba\x86\xe5\x90\x8e\xe9\x9d\xa2\xe7\x9a\x84\xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe5\xb1\x82\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe5\x8f\x82\xe6\x95\xb0\xe5\xa4\xa7\xe5\xa4\xa7\xe5\x87\x8f\xe5\xb0\x91\xef\xbc\x8c\xe5\x90\x8c\xe6\x97\xb6\xe6\x9c\x89\xe4\xba\x86\xe5\xbe\x88\xe9\xab\x98\xe7\x9a\x84\xe8\xae\xa1\xe7\xae\x97\xe6\x95\x88\xe7\x8e\x87\xe3\x80\x82\n# \n# ![](https://ws2.sinaimg.cn/large/006tNc79ly1fmprhdocouj30qb08vac3.jpg)\n# \n# \xe8\xbf\x99\xe6\x98\xaf googlenet \xe7\x9a\x84\xe7\xbd\x91\xe7\xbb\x9c\xe7\xa4\xba\xe6\x84\x8f\xe5\x9b\xbe\xef\xbc\x8c\xe4\xb8\x8b\xe9\x9d\xa2\xe6\x88\x91\xe4\xbb\xac\xe4\xbb\x8b\xe7\xbb\x8d\xe4\xb8\x80\xe4\xb8\x8b\xe5\x85\xb6\xe4\xbd\x9c\xe4\xb8\xba\xe5\x88\x9b\xe6\x96\xb0\xe7\x9a\x84 inception \xe6\xa8\xa1\xe5\x9d\x97\xe3\x80\x82\n#%% [markdown]\n# ## Inception \xe6\xa8\xa1\xe5\x9d\x97\n# \xe5\x9c\xa8\xe4\xb8\x8a\xe9\x9d\xa2\xe7\x9a\x84\xe7\xbd\x91\xe7\xbb\x9c\xe4\xb8\xad\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe7\x9c\x8b\xe5\x88\xb0\xe4\xba\x86\xe5\xa4\x9a\xe4\xb8\xaa\xe5\x9b\x9b\xe4\xb8\xaa\xe5\xb9\xb6\xe8\xa1\x8c\xe5\x8d\xb7\xe7\xa7\xaf\xe7\x9a\x84\xe5\xb1\x82\xef\xbc\x8c\xe8\xbf\x99\xe4\xba\x9b\xe5\x9b\x9b\xe4\xb8\xaa\xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb9\xb6\xe8\xa1\x8c\xe7\x9a\x84\xe5\xb1\x82\xe5\xb0\xb1\xe6\x98\xaf inception \xe6\xa8\xa1\xe5\x9d\x97\xef\xbc\x8c\xe5\x8f\xaf\xe8\xa7\x86\xe5\x8c\x96\xe5\xa6\x82\xe4\xb8\x8b\n# \n# ![](https://ws4.sinaimg.cn/large/006tNc79gy1fmprivb2hxj30dn09dwef.jpg)\n# \n#%% [markdown]\n# \xe4\xb8\x80\xe4\xb8\xaa inception \xe6\xa8\xa1\xe5\x9d\x97\xe7\x9a\x84\xe5\x9b\x9b\xe4\xb8\xaa\xe5\xb9\xb6\xe8\xa1\x8c\xe7\xba\xbf\xe8\xb7\xaf\xe5\xa6\x82\xe4\xb8\x8b\xef\xbc\x9a\n# 1.\xe4\xb8\x80\xe4\xb8\xaa 1 x 1 \xe7\x9a\x84\xe5\x8d\xb7\xe7\xa7\xaf\xef\xbc\x8c\xe4\xb8\x80\xe4\xb8\xaa\xe5\xb0\x8f\xe7\x9a\x84\xe6\x84\x9f\xe5\x8f\x97\xe9\x87\x8e\xe8\xbf\x9b\xe8\xa1\x8c\xe5\x8d\xb7\xe7\xa7\xaf\xe6\x8f\x90\xe5\x8f\x96\xe7\x89\xb9\xe5\xbe\x81\n# 2.\xe4\xb8\x80\xe4\xb8\xaa 1 x 1 \xe7\x9a\x84\xe5\x8d\xb7\xe7\xa7\xaf\xe5\x8a\xa0\xe4\xb8\x8a\xe4\xb8\x80\xe4\xb8\xaa 3 x 3 \xe7\x9a\x84\xe5\x8d\xb7\xe7\xa7\xaf\xef\xbc\x8c1 x 1 \xe7\x9a\x84\xe5\x8d\xb7\xe7\xa7\xaf\xe9\x99\x8d\xe4\xbd\x8e\xe8\xbe\x93\xe5\x85\xa5\xe7\x9a\x84\xe7\x89\xb9\xe5\xbe\x81\xe9\x80\x9a\xe9\x81\x93\xef\xbc\x8c\xe5\x87\x8f\xe5\xb0\x91\xe5\x8f\x82\xe6\x95\xb0\xe8\xae\xa1\xe7\xae\x97\xe9\x87\x8f\xef\xbc\x8c\xe7\x84\xb6\xe5\x90\x8e\xe6\x8e\xa5\xe4\xb8\x80\xe4\xb8\xaa 3 x 3 \xe7\x9a\x84\xe5\x8d\xb7\xe7\xa7\xaf\xe5\x81\x9a\xe4\xb8\x80\xe4\xb8\xaa\xe8\xbe\x83\xe5\xa4\xa7\xe6\x84\x9f\xe5\x8f\x97\xe9\x87\x8e\xe7\x9a\x84\xe5\x8d\xb7\xe7\xa7\xaf\n# 3.\xe4\xb8\x80\xe4\xb8\xaa 1 x 1 \xe7\x9a\x84\xe5\x8d\xb7\xe7\xa7\xaf\xe5\x8a\xa0\xe4\xb8\x8a\xe4\xb8\x80\xe4\xb8\xaa 5 x 5 \xe7\x9a\x84\xe5\x8d\xb7\xe7\xa7\xaf\xef\xbc\x8c\xe4\xbd\x9c\xe7\x94\xa8\xe5\x92\x8c\xe7\xac\xac\xe4\xba\x8c\xe4\xb8\xaa\xe4\xb8\x80\xe6\xa0\xb7\n# 4.\xe4\xb8\x80\xe4\xb8\xaa 3 x 3 \xe7\x9a\x84\xe6\x9c\x80\xe5\xa4\xa7\xe6\xb1\xa0\xe5\x8c\x96\xe5\x8a\xa0\xe4\xb8\x8a 1 x 1 \xe7\x9a\x84\xe5\x8d\xb7\xe7\xa7\xaf\xef\xbc\x8c\xe6\x9c\x80\xe5\xa4\xa7\xe6\xb1\xa0\xe5\x8c\x96\xe6\x94\xb9\xe5\x8f\x98\xe8\xbe\x93\xe5\x85\xa5\xe7\x9a\x84\xe7\x89\xb9\xe5\xbe\x81\xe6\x8e\x92\xe5\x88\x97\xef\xbc\x8c1 x 1 \xe7\x9a\x84\xe5\x8d\xb7\xe7\xa7\xaf\xe8\xbf\x9b\xe8\xa1\x8c\xe7\x89\xb9\xe5\xbe\x81\xe6\x8f\x90\xe5\x8f\x96\n# \n# \xe6\x9c\x80\xe5\x90\x8e\xe5\xb0\x86\xe5\x9b\x9b\xe4\xb8\xaa\xe5\xb9\xb6\xe8\xa1\x8c\xe7\xba\xbf\xe8\xb7\xaf\xe5\xbe\x97\xe5\x88\xb0\xe7\x9a\x84\xe7\x89\xb9\xe5\xbe\x81\xe5\x9c\xa8\xe9\x80\x9a\xe9\x81\x93\xe8\xbf\x99\xe4\xb8\xaa\xe7\xbb\xb4\xe5\xba\xa6\xe4\xb8\x8a\xe6\x8b\xbc\xe6\x8e\xa5\xe5\x9c\xa8\xe4\xb8\x80\xe8\xb5\xb7\xef\xbc\x8c\xe4\xb8\x8b\xe9\x9d\xa2\xe6\x88\x91\xe4\xbb\xac\xe5\x8f\xaf\xe4\xbb\xa5\xe5\xae\x9e\xe7\x8e\xb0\xe4\xb8\x80\xe4\xb8\x8b\n\n#%%\nimport sys\nsys.path.append('..')\n\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torchvision.datasets import CIFAR10\n\n\n#%%\n# \xe5\xae\x9a\xe4\xb9\x89\xe4\xb8\x80\xe4\xb8\xaa\xe5\x8d\xb7\xe7\xa7\xaf\xe5\x8a\xa0\xe4\xb8\x80\xe4\xb8\xaa relu \xe6\xbf\x80\xe6\xb4\xbb\xe5\x87\xbd\xe6\x95\xb0\xe5\x92\x8c\xe4\xb8\x80\xe4\xb8\xaa batchnorm \xe4\xbd\x9c\xe4\xb8\xba\xe4\xb8\x80\xe4\xb8\xaa\xe5\x9f\xba\xe6\x9c\xac\xe7\x9a\x84\xe5\xb1\x82\xe7\xbb\x93\xe6\x9e\x84\ndef conv_relu(in_channel, out_channel, kernel, stride=1, padding=0):\n    layer = nn.Sequential(\n        nn.Conv2d(in_channel, out_channel, kernel, stride, padding),\n        nn.BatchNorm2d(out_channel, eps=1e-3),\n        nn.ReLU(True)\n    )\n    return layer\n\n\n#%%\nclass inception(nn.Module):\n    def __init__(self, in_channel, out1_1, out2_1, out2_3, out3_1, out3_5, out4_1):\n        super(inception, self).__init__()\n        # \xe7\xac\xac\xe4\xb8\x80\xe6\x9d\xa1\xe7\xba\xbf\xe8\xb7\xaf\n        self.branch1x1 = conv_relu(in_channel, out1_1, 1)\n        \n        # \xe7\xac\xac\xe4\xba\x8c\xe6\x9d\xa1\xe7\xba\xbf\xe8\xb7\xaf\n        self.branch3x3 = nn.Sequential( \n            conv_relu(in_channel, out2_1, 1),\n            conv_relu(out2_1, out2_3, 3, padding=1)\n        )\n        \n        # \xe7\xac\xac\xe4\xb8\x89\xe6\x9d\xa1\xe7\xba\xbf\xe8\xb7\xaf\n        self.branch5x5 = nn.Sequential(\n            conv_relu(in_channel, out3_1, 1),\n            conv_relu(out3_1, out3_5, 5, padding=2)\n        )\n        \n        # \xe7\xac\xac\xe5\x9b\x9b\xe6\x9d\xa1\xe7\xba\xbf\xe8\xb7\xaf\n        self.branch_pool = nn.Sequential(\n            nn.MaxPool2d(3, stride=1, padding=1),\n            conv_relu(in_channel, out4_1, 1)\n        )\n        \n    def forward(self, x):\n        f1 = self.branch1x1(x)\n        f2 = self.branch3x3(x)\n        f3 = self.branch5x5(x)\n        f4 = self.branch_pool(x)\n        output = torch.cat((f1, f2, f3, f4), dim=1)\n        return output\n\n\n#%%\ntest_net = inception(3, 64, 48, 64, 64, 96, 32)\ntest_x = torch.zeros(1, 3, 96, 96)\nprint('input shape: {} x {} x {}'.format(test_x.shape[1], test_x.shape[2], test_x.shape[3]))\ntest_y = test_net(test_x)\nprint('output shape: {} x {} x {}'.format(test_y.shape[1], test_y.shape[2], test_y.shape[3]))\n\n#%% [markdown]\n# \xe5\x8f\xaf\xe4\xbb\xa5\xe7\x9c\x8b\xe5\x88\xb0\xe8\xbe\x93\xe5\x85\xa5\xe7\xbb\x8f\xe8\xbf\x87\xe4\xba\x86 inception \xe6\xa8\xa1\xe5\x9d\x97\xe4\xb9\x8b\xe5\x90\x8e\xef\xbc\x8c\xe5\xa4\xa7\xe5\xb0\x8f\xe6\xb2\xa1\xe6\x9c\x89\xe5\x8f\x98\xe5\x8c\x96\xef\xbc\x8c\xe9\x80\x9a\xe9\x81\x93\xe7\x9a\x84\xe7\xbb\xb4\xe5\xba\xa6\xe5\x8f\x98\xe5\xa4\x9a\xe4\xba\x86\n#%% [markdown]\n# \xe4\xb8\x8b\xe9\x9d\xa2\xe6\x88\x91\xe4\xbb\xac\xe5\xae\x9a\xe4\xb9\x89 GoogLeNet\xef\xbc\x8cGoogLeNet \xe5\x8f\xaf\xe4\xbb\xa5\xe7\x9c\x8b\xe4\xbd\x9c\xe6\x98\xaf\xe5\xbe\x88\xe5\xa4\x9a\xe4\xb8\xaa inception \xe6\xa8\xa1\xe5\x9d\x97\xe7\x9a\x84\xe4\xb8\xb2\xe8\x81\x94\xef\xbc\x8c\xe6\xb3\xa8\xe6\x84\x8f\xef\xbc\x8c\xe5\x8e\x9f\xe8\xae\xba\xe6\x96\x87\xe4\xb8\xad\xe4\xbd\xbf\xe7\x94\xa8\xe4\xba\x86\xe5\xa4\x9a\xe4\xb8\xaa\xe8\xbe\x93\xe5\x87\xba\xe6\x9d\xa5\xe8\xa7\xa3\xe5\x86\xb3\xe6\xa2\xaf\xe5\xba\xa6\xe6\xb6\x88\xe5\xa4\xb1\xe7\x9a\x84\xe9\x97\xae\xe9\xa2\x98\xef\xbc\x8c\xe8\xbf\x99\xe9\x87\x8c\xe6\x88\x91\xe4\xbb\xac\xe5\x8f\xaa\xe5\xae\x9a\xe4\xb9\x89\xe4\xb8\x80\xe4\xb8\xaa\xe7\xae\x80\xe5\x8d\x95\xe7\x89\x88\xe6\x9c\xac\xe7\x9a\x84 GoogLeNet\xef\xbc\x8c\xe7\xae\x80\xe5\x8c\x96\xe4\xb8\xba\xe4\xb8\x80\xe4\xb8\xaa\xe8\xbe\x93\xe5\x87\xba\n\n#%%\nclass googlenet(nn.Module):\n    def __init__(self, in_channel, num_classes, verbose=False):\n        super(googlenet, self).__init__()\n        self.verbose = verbose\n        \n        self.block1 = nn.Sequential(\n            conv_relu(in_channel, out_channel=64, kernel=7, stride=2, padding=3),\n            nn.MaxPool2d(3, 2)\n        )\n        \n        self.block2 = nn.Sequential(\n            conv_relu(64, 64, kernel=1),\n            conv_relu(64, 192, kernel=3, padding=1),\n            nn.MaxPool2d(3, 2)\n        )\n        \n        self.block3 = nn.Sequential(\n            inception(192, 64, 96, 128, 16, 32, 32),\n            inception(256, 128, 128, 192, 32, 96, 64),\n            nn.MaxPool2d(3, 2)\n        )\n        \n        self.block4 = nn.Sequential(\n            inception(480, 192, 96, 208, 16, 48, 64),\n            inception(512, 160, 112, 224, 24, 64, 64),\n            inception(512, 128, 128, 256, 24, 64, 64),\n            inception(512, 112, 144, 288, 32, 64, 64),\n            inception(528, 256, 160, 320, 32, 128, 128),\n            nn.MaxPool2d(3, 2)\n        )\n        \n        self.block5 = nn.Sequential(\n            inception(832, 256, 160, 320, 32, 128, 128),\n            inception(832, 384, 182, 384, 48, 128, 128),\n            nn.AvgPool2d(2)\n        )\n        \n        self.classifier = nn.Linear(1024, num_classes)\n        \n    def forward(self, x):\n        x = self.block1(x)\n        if self.verbose:\n            print('block 1 output: {}'.format(x.shape))\n        x = self.block2(x)\n        if self.verbose:\n            print('block 2 output: {}'.format(x.shape))\n        x = self.block3(x)\n        if self.verbose:\n            print('block 3 output: {}'.format(x.shape))\n        x = self.block4(x)\n        if self.verbose:\n            print('block 4 output: {}'.format(x.shape))\n        x = self.block5(x)\n        if self.verbose:\n            print('block 5 output: {}'.format(x.shape))\n        x = x.view(x.shape[0], -1)\n        x = self.classifier(x)\n        return x\n\n\n#%%\ntest_net = googlenet(3, 10, True)\ntest_x = torch.zeros(1, 3, 96, 96)\ntest_y = test_net(test_x)\nprint('output: {}'.format(test_y.shape))\n\n#%% [markdown]\n# \xe5\x8f\xaf\xe4\xbb\xa5\xe7\x9c\x8b\xe5\x88\xb0\xe8\xbe\x93\xe5\x85\xa5\xe7\x9a\x84\xe5\xb0\xba\xe5\xaf\xb8\xe4\xb8\x8d\xe6\x96\xad\xe5\x87\x8f\xe5\xb0\x8f\xef\xbc\x8c\xe9\x80\x9a\xe9\x81\x93\xe7\x9a\x84\xe7\xbb\xb4\xe5\xba\xa6\xe4\xb8\x8d\xe6\x96\xad\xe5\xa2\x9e\xe5\x8a\xa0\n\n#%%\nfrom utils import train\n\ndef data_tf(x):\n    x = x.resize((96, 96), 2) # \xe5\xb0\x86\xe5\x9b\xbe\xe7\x89\x87\xe6\x94\xbe\xe5\xa4\xa7\xe5\x88\xb0 96 x 96\n    x = np.array(x, dtype='float32') / 255\n    x = (x - 0.5) / 0.5 # \xe6\xa0\x87\xe5\x87\x86\xe5\x8c\x96\xef\xbc\x8c\xe8\xbf\x99\xe4\xb8\xaa\xe6\x8a\x80\xe5\xb7\xa7\xe4\xb9\x8b\xe5\x90\x8e\xe4\xbc\x9a\xe8\xae\xb2\xe5\x88\xb0\n    x = x.transpose((2, 0, 1)) # \xe5\xb0\x86 channel \xe6\x94\xbe\xe5\x88\xb0\xe7\xac\xac\xe4\xb8\x80\xe7\xbb\xb4\xef\xbc\x8c\xe5\x8f\xaa\xe6\x98\xaf pytorch \xe8\xa6\x81\xe6\xb1\x82\xe7\x9a\x84\xe8\xbe\x93\xe5\x85\xa5\xe6\x96\xb9\xe5\xbc\x8f\n    x = torch.from_numpy(x)\n    return x\n     \ntrain_set = CIFAR10('./data', train=True, transform=data_tf)\ntrain_data = torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=True)\ntest_set = CIFAR10('./data', train=False, transform=data_tf)\ntest_data = torch.utils.data.DataLoader(test_set, batch_size=128, shuffle=False)\n\nnet = googlenet(3, 10)\noptimizer = torch.optim.SGD(net.parameters(), lr=0.01)\ncriterion = nn.CrossEntropyLoss()\n\n\n#%%\ntrain(net, train_data, test_data, 20, optimizer, criterion)\n\n#%% [markdown]\n# GoogLeNet \xe5\x8a\xa0\xe5\x85\xa5\xe4\xba\x86\xe6\x9b\xb4\xe5\x8a\xa0\xe7\xbb\x93\xe6\x9e\x84\xe5\x8c\x96\xe7\x9a\x84 Inception \xe5\x9d\x97\xe4\xbd\xbf\xe5\xbe\x97\xe6\x88\x91\xe4\xbb\xac\xe8\x83\xbd\xe5\xa4\x9f\xe4\xbd\xbf\xe7\x94\xa8\xe6\x9b\xb4\xe5\xa4\xa7\xe7\x9a\x84\xe9\x80\x9a\xe9\x81\x93\xef\xbc\x8c\xe6\x9b\xb4\xe5\xa4\x9a\xe7\x9a\x84\xe5\xb1\x82\xef\xbc\x8c\xe5\x90\x8c\xe6\x97\xb6\xe4\xb9\x9f\xe6\x8e\xa7\xe5\x88\xb6\xe4\xba\x86\xe8\xae\xa1\xe7\xae\x97\xe9\x87\x8f\xe3\x80\x82\n# \n# **\xe5\xb0\x8f\xe7\xbb\x83\xe4\xb9\xa0\xef\xbc\x9aGoogLeNet \xe6\x9c\x89\xe5\xbe\x88\xe5\xa4\x9a\xe5\x90\x8e\xe7\xbb\xad\xe7\x9a\x84\xe7\x89\x88\xe6\x9c\xac\xef\xbc\x8c\xe5\xb0\x9d\xe8\xaf\x95\xe7\x9c\x8b\xe7\x9c\x8b\xe8\xae\xba\xe6\x96\x87\xef\xbc\x8c\xe7\x9c\x8b\xe7\x9c\x8b\xe6\x9c\x89\xe4\xbb\x80\xe4\xb9\x88\xe4\xb8\x8d\xe5\x90\x8c\xef\xbc\x8c\xe5\xae\x9e\xe7\x8e\xb0\xe4\xb8\x80\xe4\xb8\x8b\xef\xbc\x9a  \n# v1\xef\xbc\x9a\xe6\x9c\x80\xe6\x97\xa9\xe7\x9a\x84\xe7\x89\x88\xe6\x9c\xac  \n# v2\xef\xbc\x9a\xe5\x8a\xa0\xe5\x85\xa5 batch normalization \xe5\x8a\xa0\xe5\xbf\xab\xe8\xae\xad\xe7\xbb\x83  \n# v3\xef\xbc\x9a\xe5\xaf\xb9 inception \xe6\xa8\xa1\xe5\x9d\x97\xe5\x81\x9a\xe4\xba\x86\xe8\xb0\x83\xe6\x95\xb4  \n# v4\xef\xbc\x9a\xe5\x9f\xba\xe4\xba\x8e ResNet \xe5\x8a\xa0\xe5\x85\xa5\xe4\xba\x86 \xe6\xae\x8b\xe5\xb7\xae\xe8\xbf\x9e\xe6\x8e\xa5  **\n\n"""
8.Famous_CNN 经典的CNN网络/ResNet.py,8,"b""#%% [markdown]\n# # ResNet\n# \xe5\xbd\x93\xe5\xa4\xa7\xe5\xae\xb6\xe8\xbf\x98\xe5\x9c\xa8\xe6\x83\x8a\xe5\x8f\xb9 GoogLeNet \xe7\x9a\x84 inception \xe7\xbb\x93\xe6\x9e\x84\xe7\x9a\x84\xe6\x97\xb6\xe5\x80\x99\xef\xbc\x8c\xe5\xbe\xae\xe8\xbd\xaf\xe4\xba\x9a\xe6\xb4\xb2\xe7\xa0\x94\xe7\xa9\xb6\xe9\x99\xa2\xe7\x9a\x84\xe7\xa0\x94\xe7\xa9\xb6\xe5\x91\x98\xe5\xb7\xb2\xe7\xbb\x8f\xe5\x9c\xa8\xe8\xae\xbe\xe8\xae\xa1\xe6\x9b\xb4\xe6\xb7\xb1\xe4\xbd\x86\xe7\xbb\x93\xe6\x9e\x84\xe6\x9b\xb4\xe5\x8a\xa0\xe7\xae\x80\xe5\x8d\x95\xe7\x9a\x84\xe7\xbd\x91\xe7\xbb\x9c ResNet\xef\xbc\x8c\xe5\xb9\xb6\xe4\xb8\x94\xe5\x87\xad\xe5\x80\x9f\xe8\xbf\x99\xe4\xb8\xaa\xe7\xbd\x91\xe7\xbb\x9c\xe5\xad\x90\xe5\x9c\xa8 2015 \xe5\xb9\xb4 ImageNet \xe6\xaf\x94\xe8\xb5\x9b\xe4\xb8\x8a\xe5\xa4\xa7\xe8\x8e\xb7\xe5\x85\xa8\xe8\x83\x9c\xe3\x80\x82\n# \n# ResNet \xe6\x9c\x89\xe6\x95\x88\xe5\x9c\xb0\xe8\xa7\xa3\xe5\x86\xb3\xe4\xba\x86\xe6\xb7\xb1\xe5\xba\xa6\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xe9\x9a\xbe\xe4\xbb\xa5\xe8\xae\xad\xe7\xbb\x83\xe7\x9a\x84\xe9\x97\xae\xe9\xa2\x98\xef\xbc\x8c\xe5\x8f\xaf\xe4\xbb\xa5\xe8\xae\xad\xe7\xbb\x83\xe9\xab\x98\xe8\xbe\xbe 1000 \xe5\xb1\x82\xe7\x9a\x84\xe5\x8d\xb7\xe7\xa7\xaf\xe7\xbd\x91\xe7\xbb\x9c\xe3\x80\x82\xe7\xbd\x91\xe7\xbb\x9c\xe4\xb9\x8b\xe6\x89\x80\xe4\xbb\xa5\xe9\x9a\xbe\xe4\xbb\xa5\xe8\xae\xad\xe7\xbb\x83\xef\xbc\x8c\xe6\x98\xaf\xe5\x9b\xa0\xe4\xb8\xba\xe5\xad\x98\xe5\x9c\xa8\xe7\x9d\x80\xe6\xa2\xaf\xe5\xba\xa6\xe6\xb6\x88\xe5\xa4\xb1\xe7\x9a\x84\xe9\x97\xae\xe9\xa2\x98\xef\xbc\x8c\xe7\xa6\xbb loss \xe5\x87\xbd\xe6\x95\xb0\xe8\xb6\x8a\xe8\xbf\x9c\xe7\x9a\x84\xe5\xb1\x82\xef\xbc\x8c\xe5\x9c\xa8\xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe7\x9a\x84\xe6\x97\xb6\xe5\x80\x99\xef\xbc\x8c\xe6\xa2\xaf\xe5\xba\xa6\xe8\xb6\x8a\xe5\xb0\x8f\xef\xbc\x8c\xe5\xb0\xb1\xe8\xb6\x8a\xe9\x9a\xbe\xe4\xbb\xa5\xe6\x9b\xb4\xe6\x96\xb0\xef\xbc\x8c\xe9\x9a\x8f\xe7\x9d\x80\xe5\xb1\x82\xe6\x95\xb0\xe7\x9a\x84\xe5\xa2\x9e\xe5\x8a\xa0\xef\xbc\x8c\xe8\xbf\x99\xe4\xb8\xaa\xe7\x8e\xb0\xe8\xb1\xa1\xe8\xb6\x8a\xe4\xb8\xa5\xe9\x87\x8d\xe3\x80\x82\xe4\xb9\x8b\xe5\x89\x8d\xe6\x9c\x89\xe4\xb8\xa4\xe7\xa7\x8d\xe5\xb8\xb8\xe8\xa7\x81\xe7\x9a\x84\xe6\x96\xb9\xe6\xa1\x88\xe6\x9d\xa5\xe8\xa7\xa3\xe5\x86\xb3\xe8\xbf\x99\xe4\xb8\xaa\xe9\x97\xae\xe9\xa2\x98\xef\xbc\x9a\n# \n# 1.\xe6\x8c\x89\xe5\xb1\x82\xe8\xae\xad\xe7\xbb\x83\xef\xbc\x8c\xe5\x85\x88\xe8\xae\xad\xe7\xbb\x83\xe6\xaf\x94\xe8\xbe\x83\xe6\xb5\x85\xe7\x9a\x84\xe5\xb1\x82\xef\xbc\x8c\xe7\x84\xb6\xe5\x90\x8e\xe5\x9c\xa8\xe4\xb8\x8d\xe6\x96\xad\xe5\xa2\x9e\xe5\x8a\xa0\xe5\xb1\x82\xe6\x95\xb0\xef\xbc\x8c\xe4\xbd\x86\xe6\x98\xaf\xe8\xbf\x99\xe7\xa7\x8d\xe6\x96\xb9\xe6\xb3\x95\xe6\x95\x88\xe6\x9e\x9c\xe4\xb8\x8d\xe6\x98\xaf\xe7\x89\xb9\xe5\x88\xab\xe5\xa5\xbd\xef\xbc\x8c\xe8\x80\x8c\xe4\xb8\x94\xe6\xaf\x94\xe8\xbe\x83\xe9\xba\xbb\xe7\x83\xa6\n# \n# 2.\xe4\xbd\xbf\xe7\x94\xa8\xe6\x9b\xb4\xe5\xae\xbd\xe7\x9a\x84\xe5\xb1\x82\xef\xbc\x8c\xe6\x88\x96\xe8\x80\x85\xe5\xa2\x9e\xe5\x8a\xa0\xe8\xbe\x93\xe5\x87\xba\xe9\x80\x9a\xe9\x81\x93\xef\xbc\x8c\xe8\x80\x8c\xe4\xb8\x8d\xe5\x8a\xa0\xe6\xb7\xb1\xe7\xbd\x91\xe7\xbb\x9c\xe7\x9a\x84\xe5\xb1\x82\xe6\x95\xb0\xef\xbc\x8c\xe8\xbf\x99\xe7\xa7\x8d\xe7\xbb\x93\xe6\x9e\x84\xe5\xbe\x80\xe5\xbe\x80\xe5\xbe\x97\xe5\x88\xb0\xe7\x9a\x84\xe6\x95\x88\xe6\x9e\x9c\xe5\x8f\x88\xe4\xb8\x8d\xe5\xa5\xbd\n# \n# ResNet \xe9\x80\x9a\xe8\xbf\x87\xe5\xbc\x95\xe5\x85\xa5\xe4\xba\x86\xe8\xb7\xa8\xe5\xb1\x82\xe9\x93\xbe\xe6\x8e\xa5\xe8\xa7\xa3\xe5\x86\xb3\xe4\xba\x86\xe6\xa2\xaf\xe5\xba\xa6\xe5\x9b\x9e\xe4\xbc\xa0\xe6\xb6\x88\xe5\xa4\xb1\xe7\x9a\x84\xe9\x97\xae\xe9\xa2\x98\xe3\x80\x82\n# \n# ![](https://ws1.sinaimg.cn/large/006tNc79ly1fmptq2snv9j30j808t74a.jpg)\n#%% [markdown]\n# \xe8\xbf\x99\xe5\xb0\xb1\xe6\x99\xae\xe9\x80\x9a\xe7\x9a\x84\xe7\xbd\x91\xe7\xbb\x9c\xe8\xbf\x9e\xe6\x8e\xa5\xe8\xb7\x9f\xe8\xb7\xa8\xe5\xb1\x82\xe6\xae\x8b\xe5\xb7\xae\xe8\xbf\x9e\xe6\x8e\xa5\xe7\x9a\x84\xe5\xaf\xb9\xe6\xaf\x94\xe5\x9b\xbe\xef\xbc\x8c\xe4\xbd\xbf\xe7\x94\xa8\xe6\x99\xae\xe9\x80\x9a\xe7\x9a\x84\xe8\xbf\x9e\xe6\x8e\xa5\xef\xbc\x8c\xe4\xb8\x8a\xe5\xb1\x82\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\xe5\xbf\x85\xe9\xa1\xbb\xe8\xa6\x81\xe4\xb8\x80\xe5\xb1\x82\xe4\xb8\x80\xe5\xb1\x82\xe4\xbc\xa0\xe5\x9b\x9e\xe6\x9d\xa5\xef\xbc\x8c\xe8\x80\x8c\xe6\x98\xaf\xe7\x94\xa8\xe6\xae\x8b\xe5\xb7\xae\xe8\xbf\x9e\xe6\x8e\xa5\xef\xbc\x8c\xe7\x9b\xb8\xe5\xbd\x93\xe4\xba\x8e\xe4\xb8\xad\xe9\x97\xb4\xe6\x9c\x89\xe4\xba\x86\xe4\xb8\x80\xe6\x9d\xa1\xe6\x9b\xb4\xe7\x9f\xad\xe7\x9a\x84\xe8\xb7\xaf\xef\xbc\x8c\xe6\xa2\xaf\xe5\xba\xa6\xe8\x83\xbd\xe5\xa4\x9f\xe4\xbb\x8e\xe8\xbf\x99\xe6\x9d\xa1\xe6\x9b\xb4\xe7\x9f\xad\xe7\x9a\x84\xe8\xb7\xaf\xe4\xbc\xa0\xe5\x9b\x9e\xe6\x9d\xa5\xef\xbc\x8c\xe9\x81\xbf\xe5\x85\x8d\xe4\xba\x86\xe6\xa2\xaf\xe5\xba\xa6\xe8\xbf\x87\xe5\xb0\x8f\xe7\x9a\x84\xe6\x83\x85\xe5\x86\xb5\xe3\x80\x82\n# \n# \xe5\x81\x87\xe8\xae\xbe\xe6\x9f\x90\xe5\xb1\x82\xe7\x9a\x84\xe8\xbe\x93\xe5\x85\xa5\xe6\x98\xaf x\xef\xbc\x8c\xe6\x9c\x9f\xe6\x9c\x9b\xe8\xbe\x93\xe5\x87\xba\xe6\x98\xaf H(x)\xef\xbc\x8c \xe5\xa6\x82\xe6\x9e\x9c\xe6\x88\x91\xe4\xbb\xac\xe7\x9b\xb4\xe6\x8e\xa5\xe6\x8a\x8a\xe8\xbe\x93\xe5\x85\xa5 x \xe4\xbc\xa0\xe5\x88\xb0\xe8\xbe\x93\xe5\x87\xba\xe4\xbd\x9c\xe4\xb8\xba\xe5\x88\x9d\xe5\xa7\x8b\xe7\xbb\x93\xe6\x9e\x9c\xef\xbc\x8c\xe8\xbf\x99\xe5\xb0\xb1\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa\xe6\x9b\xb4\xe6\xb5\x85\xe5\xb1\x82\xe7\x9a\x84\xe7\xbd\x91\xe7\xbb\x9c\xef\xbc\x8c\xe6\x9b\xb4\xe5\xae\xb9\xe6\x98\x93\xe8\xae\xad\xe7\xbb\x83\xef\xbc\x8c\xe8\x80\x8c\xe8\xbf\x99\xe4\xb8\xaa\xe7\xbd\x91\xe7\xbb\x9c\xe6\xb2\xa1\xe6\x9c\x89\xe5\xad\xa6\xe4\xbc\x9a\xe7\x9a\x84\xe9\x83\xa8\xe5\x88\x86\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe5\x8f\xaf\xe4\xbb\xa5\xe4\xbd\xbf\xe7\x94\xa8\xe6\x9b\xb4\xe6\xb7\xb1\xe7\x9a\x84\xe7\xbd\x91\xe7\xbb\x9c F(x) \xe5\x8e\xbb\xe8\xae\xad\xe7\xbb\x83\xe5\xae\x83\xef\xbc\x8c\xe4\xbd\xbf\xe5\xbe\x97\xe8\xae\xad\xe7\xbb\x83\xe6\x9b\xb4\xe5\x8a\xa0\xe5\xae\xb9\xe6\x98\x93\xef\xbc\x8c\xe6\x9c\x80\xe5\x90\x8e\xe5\xb8\x8c\xe6\x9c\x9b\xe6\x8b\x9f\xe5\x90\x88\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x9c\xe5\xb0\xb1\xe6\x98\xaf F(x) = H(x) - x\xef\xbc\x8c\xe8\xbf\x99\xe5\xb0\xb1\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa\xe6\xae\x8b\xe5\xb7\xae\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x84\n# \n# \xe6\xae\x8b\xe5\xb7\xae\xe7\xbd\x91\xe7\xbb\x9c\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x84\xe5\xb0\xb1\xe6\x98\xaf\xe4\xb8\x8a\xe9\x9d\xa2\xe8\xbf\x99\xe7\xa7\x8d\xe6\xae\x8b\xe5\xb7\xae\xe5\x9d\x97\xe7\x9a\x84\xe5\xa0\x86\xe5\x8f\xa0\xef\xbc\x8c\xe4\xb8\x8b\xe9\x9d\xa2\xe8\xae\xa9\xe6\x88\x91\xe4\xbb\xac\xe6\x9d\xa5\xe5\xae\x9e\xe7\x8e\xb0\xe4\xb8\x80\xe4\xb8\xaa residual block\n\n#%%\nimport sys\nsys.path.append('..')\n\nimport numpy as np\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torchvision.datasets import CIFAR10\n\n\n#%%\ndef conv3x3(in_channel, out_channel, stride=1):\n    return nn.Conv2d(in_channel, out_channel, 3, stride=stride, padding=1, bias=False)\n\n\n#%%\nclass residual_block(nn.Module):\n    def __init__(self, in_channel, out_channel, same_shape=True):\n        super(residual_block, self).__init__()\n        self.same_shape = same_shape\n        stride=1 if self.same_shape else 2\n        \n        self.conv1 = conv3x3(in_channel, out_channel, stride=stride)\n        self.bn1 = nn.BatchNorm2d(out_channel)\n        \n        self.conv2 = conv3x3(out_channel, out_channel)\n        self.bn2 = nn.BatchNorm2d(out_channel)\n        if not self.same_shape:\n            self.conv3 = nn.Conv2d(in_channel, out_channel, 1, stride=stride)\n        \n    def forward(self, x):\n        out = self.conv1(x)\n        out = F.relu(self.bn1(out), True)\n        out = self.conv2(out)\n        out = F.relu(self.bn2(out), True)\n        \n        if not self.same_shape:\n            x = self.conv3(x)\n        return F.relu(x+out, True)\n\n#%% [markdown]\n# \xe6\x88\x91\xe4\xbb\xac\xe6\xb5\x8b\xe8\xaf\x95\xe4\xb8\x80\xe4\xb8\x8b\xe4\xb8\x80\xe4\xb8\xaa residual block \xe7\x9a\x84\xe8\xbe\x93\xe5\x85\xa5\xe5\x92\x8c\xe8\xbe\x93\xe5\x87\xba\n\n#%%\n# \xe8\xbe\x93\xe5\x85\xa5\xe8\xbe\x93\xe5\x87\xba\xe5\xbd\xa2\xe7\x8a\xb6\xe7\x9b\xb8\xe5\x90\x8c\ntest_net = residual_block(32, 32)\ntest_x = torch.zeros(1, 32, 96, 96)\nprint('input: {}'.format(test_x.shape))\ntest_y = test_net(test_x)\nprint('output: {}'.format(test_y.shape))\n\n\n#%%\n# \xe8\xbe\x93\xe5\x85\xa5\xe8\xbe\x93\xe5\x87\xba\xe5\xbd\xa2\xe7\x8a\xb6\xe4\xb8\x8d\xe5\x90\x8c\ntest_net = residual_block(3, 32, False)\ntest_x = torch.zeros(1, 3, 96, 96)\nprint('input: {}'.format(test_x.shape))\ntest_y = test_net(test_x)\nprint('output: {}'.format(test_y.shape))\n\n#%% [markdown]\n# \xe4\xb8\x8b\xe9\x9d\xa2\xe6\x88\x91\xe4\xbb\xac\xe5\xb0\x9d\xe8\xaf\x95\xe5\xae\x9e\xe7\x8e\xb0\xe4\xb8\x80\xe4\xb8\xaa ResNet\xef\xbc\x8c\xe5\xae\x83\xe5\xb0\xb1\xe6\x98\xaf residual block \xe6\xa8\xa1\xe5\x9d\x97\xe7\x9a\x84\xe5\xa0\x86\xe5\x8f\xa0\n\n#%%\nclass resnet(nn.Module):\n    def __init__(self, in_channel, num_classes, verbose=False):\n        super(resnet, self).__init__()\n        self.verbose = verbose\n        \n        self.block1 = nn.Conv2d(in_channel, 64, 7, 2)\n        \n        self.block2 = nn.Sequential(\n            nn.MaxPool2d(3, 2),\n            residual_block(64, 64),\n            residual_block(64, 64)\n        )\n        \n        self.block3 = nn.Sequential(\n            residual_block(64, 128, False),\n            residual_block(128, 128)\n        )\n        \n        self.block4 = nn.Sequential(\n            residual_block(128, 256, False),\n            residual_block(256, 256)\n        )\n        \n        self.block5 = nn.Sequential(\n            residual_block(256, 512, False),\n            residual_block(512, 512),\n            nn.AvgPool2d(3)\n        )\n        \n        self.classifier = nn.Linear(512, num_classes)\n        \n    def forward(self, x):\n        x = self.block1(x)\n        if self.verbose:\n            print('block 1 output: {}'.format(x.shape))\n        x = self.block2(x)\n        if self.verbose:\n            print('block 2 output: {}'.format(x.shape))\n        x = self.block3(x)\n        if self.verbose:\n            print('block 3 output: {}'.format(x.shape))\n        x = self.block4(x)\n        if self.verbose:\n            print('block 4 output: {}'.format(x.shape))\n        x = self.block5(x)\n        if self.verbose:\n            print('block 5 output: {}'.format(x.shape))\n        x = x.view(x.shape[0], -1)\n        x = self.classifier(x)\n        return x\n\n#%% [markdown]\n# \xe8\xbe\x93\xe5\x87\xba\xe4\xb8\x80\xe4\xb8\x8b\xe6\xaf\x8f\xe4\xb8\xaa block \xe4\xb9\x8b\xe5\x90\x8e\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f\n\n#%%\ntest_net = resnet(3, 10, True)\ntest_x = torch.zeros(1, 3, 96, 96)\ntest_y = test_net(test_x)\nprint('output: {}'.format(test_y.shape))\n\n\n#%%\nfrom utils import train\n\ndef data_tf(x):\n    x = x.resize((96, 96), 2) # \xe5\xb0\x86\xe5\x9b\xbe\xe7\x89\x87\xe6\x94\xbe\xe5\xa4\xa7\xe5\x88\xb0 96 x 96\n    x = np.array(x, dtype='float32') / 255\n    x = (x - 0.5) / 0.5 # \xe6\xa0\x87\xe5\x87\x86\xe5\x8c\x96\xef\xbc\x8c\xe8\xbf\x99\xe4\xb8\xaa\xe6\x8a\x80\xe5\xb7\xa7\xe4\xb9\x8b\xe5\x90\x8e\xe4\xbc\x9a\xe8\xae\xb2\xe5\x88\xb0\n    x = x.transpose((2, 0, 1)) # \xe5\xb0\x86 channel \xe6\x94\xbe\xe5\x88\xb0\xe7\xac\xac\xe4\xb8\x80\xe7\xbb\xb4\xef\xbc\x8c\xe5\x8f\xaa\xe6\x98\xaf pytorch \xe8\xa6\x81\xe6\xb1\x82\xe7\x9a\x84\xe8\xbe\x93\xe5\x85\xa5\xe6\x96\xb9\xe5\xbc\x8f\n    x = torch.from_numpy(x)\n    return x\n     \ntrain_set = CIFAR10('./data', train=True, transform=data_tf)\ntrain_data = torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=True)\ntest_set = CIFAR10('./data', train=False, transform=data_tf)\ntest_data = torch.utils.data.DataLoader(test_set, batch_size=128, shuffle=False)\n\nnet = resnet(3, 10)\noptimizer = torch.optim.SGD(net.parameters(), lr=0.01)\ncriterion = nn.CrossEntropyLoss()\n\n\n#%%\ntrain(net, train_data, test_data, 20, optimizer, criterion)\n\n#%% [markdown]\n# ResNet \xe4\xbd\xbf\xe7\x94\xa8\xe8\xb7\xa8\xe5\xb1\x82\xe9\x80\x9a\xe9\x81\x93\xe4\xbd\xbf\xe5\xbe\x97\xe8\xae\xad\xe7\xbb\x83\xe9\x9d\x9e\xe5\xb8\xb8\xe6\xb7\xb1\xe7\x9a\x84\xe5\x8d\xb7\xe7\xa7\xaf\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xe6\x88\x90\xe4\xb8\xba\xe5\x8f\xaf\xe8\x83\xbd\xe3\x80\x82\xe5\x90\x8c\xe6\xa0\xb7\xe5\xae\x83\xe4\xbd\xbf\xe7\x94\xa8\xe5\xbe\x88\xe7\xae\x80\xe5\x8d\x95\xe7\x9a\x84\xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82\xe9\x85\x8d\xe7\xbd\xae\xef\xbc\x8c\xe4\xbd\xbf\xe5\xbe\x97\xe5\x85\xb6\xe6\x8b\x93\xe5\xb1\x95\xe6\x9b\xb4\xe5\x8a\xa0\xe7\xae\x80\xe5\x8d\x95\xe3\x80\x82\n# \n# **\xe5\xb0\x8f\xe7\xbb\x83\xe4\xb9\xa0\xef\xbc\x9a  \n# 1.\xe5\xb0\x9d\xe8\xaf\x95\xe4\xb8\x80\xe4\xb8\x8b\xe8\xae\xba\xe6\x96\x87\xe4\xb8\xad\xe6\x8f\x90\xe5\x87\xba\xe7\x9a\x84 bottleneck \xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x84   \n# 2.\xe5\xb0\x9d\xe8\xaf\x95\xe6\x94\xb9\xe5\x8f\x98 conv -> bn -> relu \xe7\x9a\x84\xe9\xa1\xba\xe5\xba\x8f\xe4\xb8\xba bn -> relu -> conv\xef\xbc\x8c\xe7\x9c\x8b\xe7\x9c\x8b\xe7\xb2\xbe\xe5\xba\xa6\xe4\xbc\x9a\xe4\xb8\x8d\xe4\xbc\x9a\xe6\x8f\x90\xe9\xab\x98**\n\n"""
8.Famous_CNN 经典的CNN网络/ResNetBlock.py,3,"b'\n#%%\nimport torch\nimport torch.nn as nn\n\nclass ResNetBlock(nn.Module):\n\n    def __init__(self, dim):\n        super(ResNetBlock, self).__init__()\n        self.conv_block = self.build_conv_block(dim)\n\n    def build_conv_block(self, dim):\n        conv_block = []\n\n        conv_block += [nn.ReflectionPad2d(1)]\n\n        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=0, bias=True),\n                       nn.InstanceNorm2d(dim),\n                       nn.ReLU(True)]\n\n        conv_block += [nn.ReflectionPad2d(1)]\n\n        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=0, bias=True),\n                       nn.InstanceNorm2d(dim)]\n\n        return nn.Sequential(*conv_block)\n\n    def forward(self, x):\n        out = x + self.conv_block(x)\n        return out\n\n\nclass ResNetGenerator(nn.Module):\n\n    def __init__(self, input_nc=3, output_nc=3, ngf=64, n_blocks=9):\n\n        assert(n_blocks >= 0)\n        super(ResNetGenerator, self).__init__()\n\n        self.input_nc = input_nc\n        self.output_nc = output_nc\n        self.ngf = ngf\n\n        model = [nn.ReflectionPad2d(3),\n                 nn.Conv2d(input_nc, ngf, kernel_size=7, padding=0, bias=True),\n                 nn.InstanceNorm2d(ngf),\n                 nn.ReLU(True)]\n\n        n_downsampling = 2\n        for i in range(n_downsampling):\n            mult = 2**i\n            model += [nn.Conv2d(ngf * mult, ngf * mult * 2, kernel_size=3,\n                                stride=2, padding=1, bias=True),\n                      nn.InstanceNorm2d(ngf * mult * 2),\n                      nn.ReLU(True)]\n\n        mult = 2**n_downsampling\n        for i in range(n_blocks):\n            model += [ResNetBlock(ngf * mult)]\n\n        for i in range(n_downsampling):\n            mult = 2**(n_downsampling - i)\n            model += [nn.ConvTranspose2d(ngf * mult, int(ngf * mult / 2),\n                                         kernel_size=3, stride=2,\n                                         padding=1, output_padding=1,\n                                         bias=True),\n                      nn.InstanceNorm2d(int(ngf * mult / 2)),\n                      nn.ReLU(True)]\n\n        model += [nn.ReflectionPad2d(3)]\n        model += [nn.Conv2d(ngf, output_nc, kernel_size=7, padding=0)]\n        model += [nn.Tanh()]\n\n        self.model = nn.Sequential(*model)\n\n    def forward(self, input):\n        return self.model(input)\n\n\n#%%\nnetG = ResNetGenerator()\n\n\n#%%\nmodel_path = \'horse2zebra.pth\'\nmodel_data = torch.load(model_path)\nnetG.load_state_dict(model_data)\n\n\n#%%\nnetG.eval()\n\n\n#%%\nfrom PIL import Image\nfrom torchvision import transforms\n\n\n#%%\npreprocess = transforms.Compose([transforms.Resize(256),\n                                 transforms.ToTensor()])\n\n\n#%%\nimg = Image.open(""horse.jpg"")\nimg\n\n\n#%%\nimg_t = preprocess(img)\nbatch_t = torch.unsqueeze(img_t, 0)\n\n\n#%%\nbatch_out = netG(batch_t)\n\n\n#%%\nout_t = (batch_out.data.squeeze() + 1.0) / 2.0\nout_img = transforms.ToPILImage()(out_t)\n# out_img.save(\'zebra.jpg\')\nout_img\n\n\n'"
8.Famous_CNN 经典的CNN网络/VGG-Net.py,6,"b""#%% [markdown]\n# # VGG\n# \xe8\xae\xa1\xe7\xae\x97\xe6\x9c\xba\xe8\xa7\x86\xe8\xa7\x89\xe6\x98\xaf\xe4\xb8\x80\xe7\x9b\xb4\xe6\xb7\xb1\xe5\xba\xa6\xe5\xad\xa6\xe4\xb9\xa0\xe7\x9a\x84\xe4\xb8\xbb\xe6\x88\x98\xe5\x9c\xba\xef\xbc\x8c\xe4\xbb\x8e\xe8\xbf\x99\xe9\x87\x8c\xe6\x88\x91\xe4\xbb\xac\xe5\xb0\x86\xe6\x8e\xa5\xe8\xa7\xa6\xe5\x88\xb0\xe8\xbf\x91\xe5\x87\xa0\xe5\xb9\xb4\xe9\x9d\x9e\xe5\xb8\xb8\xe6\xb5\x81\xe8\xa1\x8c\xe7\x9a\x84\xe5\x8d\xb7\xe7\xa7\xaf\xe7\xbd\x91\xe7\xbb\x9c\xe7\xbb\x93\xe6\x9e\x84\xef\xbc\x8c\xe7\xbd\x91\xe7\xbb\x9c\xe7\xbb\x93\xe6\x9e\x84\xe7\x94\xb1\xe6\xb5\x85\xe5\x8f\x98\xe6\xb7\xb1\xef\xbc\x8c\xe5\x8f\x82\xe6\x95\xb0\xe8\xb6\x8a\xe6\x9d\xa5\xe8\xb6\x8a\xe5\xa4\x9a\xef\xbc\x8c\xe7\xbd\x91\xe7\xbb\x9c\xe6\x9c\x89\xe7\x9d\x80\xe6\x9b\xb4\xe5\xa4\x9a\xe7\x9a\x84\xe8\xb7\xa8\xe5\xb1\x82\xe9\x93\xbe\xe6\x8e\xa5\xef\xbc\x8c\xe9\xa6\x96\xe5\x85\x88\xe6\x88\x91\xe4\xbb\xac\xe5\x85\x88\xe4\xbb\x8b\xe7\xbb\x8d\xe4\xb8\x80\xe4\xb8\xaa\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86 cifar10\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe5\xb0\x86\xe4\xbb\xa5\xe6\xad\xa4\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe4\xb8\xba\xe4\xbe\x8b\xe4\xbb\x8b\xe7\xbb\x8d\xe5\x90\x84\xe7\xa7\x8d\xe5\x8d\xb7\xe7\xa7\xaf\xe7\xbd\x91\xe7\xbb\x9c\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x84\xe3\x80\x82\n# \n# ## CIFAR 10\n# cifar 10 \xe8\xbf\x99\xe4\xb8\xaa\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe4\xb8\x80\xe5\x85\xb1\xe6\x9c\x89 50000 \xe5\xbc\xa0\xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\xef\xbc\x8c10000 \xe5\xbc\xa0\xe6\xb5\x8b\xe8\xaf\x95\xe9\x9b\x86\xef\xbc\x8c\xe4\xb8\xa4\xe4\xb8\xaa\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe9\x87\x8c\xe9\x9d\xa2\xe7\x9a\x84\xe5\x9b\xbe\xe7\x89\x87\xe9\x83\xbd\xe6\x98\xaf png \xe5\xbd\xa9\xe8\x89\xb2\xe5\x9b\xbe\xe7\x89\x87\xef\xbc\x8c\xe5\x9b\xbe\xe7\x89\x87\xe5\xa4\xa7\xe5\xb0\x8f\xe6\x98\xaf 32 x 32 x 3\xef\xbc\x8c\xe4\xb8\x80\xe5\x85\xb1\xe6\x98\xaf 10 \xe5\x88\x86\xe7\xb1\xbb\xe9\x97\xae\xe9\xa2\x98\xef\xbc\x8c\xe5\x88\x86\xe5\x88\xab\xe4\xb8\xba\xe9\xa3\x9e\xe6\x9c\xba\xe3\x80\x81\xe6\xb1\xbd\xe8\xbd\xa6\xe3\x80\x81\xe9\xb8\x9f\xe3\x80\x81\xe7\x8c\xab\xe3\x80\x81\xe9\xb9\xbf\xe3\x80\x81\xe7\x8b\x97\xe3\x80\x81\xe9\x9d\x92\xe8\x9b\x99\xe3\x80\x81\xe9\xa9\xac\xe3\x80\x81\xe8\x88\xb9\xe5\x92\x8c\xe5\x8d\xa1\xe8\xbd\xa6\xe3\x80\x82\xe8\xbf\x99\xe4\xb8\xaa\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe6\x98\xaf\xe5\xaf\xb9\xe7\xbd\x91\xe7\xbb\x9c\xe6\x80\xa7\xe8\x83\xbd\xe6\xb5\x8b\xe8\xaf\x95\xe4\xb8\x80\xe4\xb8\xaa\xe9\x9d\x9e\xe5\xb8\xb8\xe9\x87\x8d\xe8\xa6\x81\xe7\x9a\x84\xe6\x8c\x87\xe6\xa0\x87\xef\xbc\x8c\xe5\x8f\xaf\xe4\xbb\xa5\xe8\xaf\xb4\xe5\xa6\x82\xe6\x9e\x9c\xe4\xb8\x80\xe4\xb8\xaa\xe7\xbd\x91\xe7\xbb\x9c\xe5\x9c\xa8\xe8\xbf\x99\xe4\xb8\xaa\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe4\xb8\x8a\xe8\xb6\x85\xe8\xbf\x87\xe5\x8f\xa6\xe5\xa4\x96\xe4\xb8\x80\xe4\xb8\xaa\xe7\xbd\x91\xe7\xbb\x9c\xef\xbc\x8c\xe9\x82\xa3\xe4\xb9\x88\xe8\xbf\x99\xe4\xb8\xaa\xe7\xbd\x91\xe7\xbb\x9c\xe6\x80\xa7\xe8\x83\xbd\xe4\xb8\x8a\xe4\xb8\x80\xe5\xae\x9a\xe8\xa6\x81\xe6\xaf\x94\xe5\x8f\xa6\xe5\xa4\x96\xe4\xb8\x80\xe4\xb8\xaa\xe7\xbd\x91\xe7\xbb\x9c\xe5\xa5\xbd\xef\xbc\x8c\xe7\x9b\xae\xe5\x89\x8d\xe8\xbf\x99\xe4\xb8\xaa\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe6\x9c\x80\xe5\xa5\xbd\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x9c\xe6\x98\xaf 95% \xe5\xb7\xa6\xe5\x8f\xb3\xe7\x9a\x84\xe6\xb5\x8b\xe8\xaf\x95\xe9\x9b\x86\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87\xe3\x80\x82\n# \n# ![](https://ws1.sinaimg.cn/large/006tNc79ly1fmpjxxq7wcj30db0ae7ag.jpg)\n# \n# \xe4\xbd\xa0\xe8\x83\xbd\xe7\x94\xa8\xe8\x82\x89\xe7\x9c\xbc\xe5\xaf\xb9\xe8\xbf\x99\xe4\xba\x9b\xe5\x9b\xbe\xe7\x89\x87\xe8\xbf\x9b\xe8\xa1\x8c\xe5\x88\x86\xe7\xb1\xbb\xe5\x90\x97\xef\xbc\x9f\n# \n# cifar 10 \xe5\xb7\xb2\xe7\xbb\x8f\xe8\xa2\xab pytorch \xe5\x86\x85\xe7\xbd\xae\xe4\xba\x86\xef\xbc\x8c\xe4\xbd\xbf\xe7\x94\xa8\xe9\x9d\x9e\xe5\xb8\xb8\xe6\x96\xb9\xe4\xbe\xbf\xef\xbc\x8c\xe5\x8f\xaa\xe9\x9c\x80\xe8\xa6\x81\xe8\xb0\x83\xe7\x94\xa8 `torchvision.datasets.CIFAR10` \xe5\xb0\xb1\xe5\x8f\xaf\xe4\xbb\xa5\xe4\xba\x86\n#%% [markdown]\n# ## VGGNet\n# vggNet \xe6\x98\xaf\xe7\xac\xac\xe4\xb8\x80\xe4\xb8\xaa\xe7\x9c\x9f\xe6\xad\xa3\xe6\x84\x8f\xe4\xb9\x89\xe4\xb8\x8a\xe7\x9a\x84\xe6\xb7\xb1\xe5\xb1\x82\xe7\xbd\x91\xe7\xbb\x9c\xe7\xbb\x93\xe6\x9e\x84\xef\xbc\x8c\xe5\x85\xb6\xe6\x98\xaf ImageNet2014\xe5\xb9\xb4\xe7\x9a\x84\xe5\x86\xa0\xe5\x86\x9b\xef\xbc\x8c\xe5\xbe\x97\xe7\x9b\x8a\xe4\xba\x8e python \xe7\x9a\x84\xe5\x87\xbd\xe6\x95\xb0\xe5\x92\x8c\xe5\xbe\xaa\xe7\x8e\xaf\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe8\x83\xbd\xe5\xa4\x9f\xe9\x9d\x9e\xe5\xb8\xb8\xe6\x96\xb9\xe4\xbe\xbf\xe5\x9c\xb0\xe6\x9e\x84\xe5\xbb\xba\xe9\x87\x8d\xe5\xa4\x8d\xe7\xbb\x93\xe6\x9e\x84\xe7\x9a\x84\xe6\xb7\xb1\xe5\xb1\x82\xe7\xbd\x91\xe7\xbb\x9c\xe3\x80\x82\n# \n# vgg \xe7\x9a\x84\xe7\xbd\x91\xe7\xbb\x9c\xe7\xbb\x93\xe6\x9e\x84\xe9\x9d\x9e\xe5\xb8\xb8\xe7\xae\x80\xe5\x8d\x95\xef\xbc\x8c\xe5\xb0\xb1\xe6\x98\xaf\xe4\xb8\x8d\xe6\x96\xad\xe5\x9c\xb0\xe5\xa0\x86\xe5\x8f\xa0\xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82\xe5\x92\x8c\xe6\xb1\xa0\xe5\x8c\x96\xe5\xb1\x82\xef\xbc\x8c\xe4\xb8\x8b\xe9\x9d\xa2\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa\xe7\xae\x80\xe5\x8d\x95\xe7\x9a\x84\xe5\x9b\xbe\xe7\xa4\xba\n# \n# ![](https://ws4.sinaimg.cn/large/006tNc79ly1fmpk5smtidj307n0dx3yv.jpg)\n# \n# vgg \xe5\x87\xa0\xe4\xb9\x8e\xe5\x85\xa8\xe9\x83\xa8\xe4\xbd\xbf\xe7\x94\xa8 3 x 3 \xe7\x9a\x84\xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8\xe4\xbb\xa5\xe5\x8f\x8a 2 x 2 \xe7\x9a\x84\xe6\xb1\xa0\xe5\x8c\x96\xe5\xb1\x82\xef\xbc\x8c\xe4\xbd\xbf\xe7\x94\xa8\xe5\xb0\x8f\xe7\x9a\x84\xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8\xe8\xbf\x9b\xe8\xa1\x8c\xe5\xa4\x9a\xe5\xb1\x82\xe7\x9a\x84\xe5\xa0\x86\xe5\x8f\xa0\xe5\x92\x8c\xe4\xb8\x80\xe4\xb8\xaa\xe5\xa4\xa7\xe7\x9a\x84\xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8\xe7\x9a\x84\xe6\x84\x9f\xe5\x8f\x97\xe9\x87\x8e\xe6\x98\xaf\xe7\x9b\xb8\xe5\x90\x8c\xe7\x9a\x84\xef\xbc\x8c\xe5\x90\x8c\xe6\x97\xb6\xe5\xb0\x8f\xe7\x9a\x84\xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8\xe8\xbf\x98\xe8\x83\xbd\xe5\x87\x8f\xe5\xb0\x91\xe5\x8f\x82\xe6\x95\xb0\xef\xbc\x8c\xe5\x90\x8c\xe6\x97\xb6\xe5\x8f\xaf\xe4\xbb\xa5\xe6\x9c\x89\xe6\x9b\xb4\xe6\xb7\xb1\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x84\xe3\x80\x82\n# \n# vgg \xe7\x9a\x84\xe4\xb8\x80\xe4\xb8\xaa\xe5\x85\xb3\xe9\x94\xae\xe5\xb0\xb1\xe6\x98\xaf\xe4\xbd\xbf\xe7\x94\xa8\xe5\xbe\x88\xe5\xa4\x9a\xe5\xb1\x82 3 x 3 \xe7\x9a\x84\xe5\x8d\xb7\xe7\xa7\xaf\xe7\x84\xb6\xe5\x90\x8e\xe5\x86\x8d\xe4\xbd\xbf\xe7\x94\xa8\xe4\xb8\x80\xe4\xb8\xaa\xe6\x9c\x80\xe5\xa4\xa7\xe6\xb1\xa0\xe5\x8c\x96\xe5\xb1\x82\xef\xbc\x8c\xe8\xbf\x99\xe4\xb8\xaa\xe6\xa8\xa1\xe5\x9d\x97\xe8\xa2\xab\xe4\xbd\xbf\xe7\x94\xa8\xe4\xba\x86\xe5\xbe\x88\xe5\xa4\x9a\xe6\xac\xa1\xef\xbc\x8c\xe4\xb8\x8b\xe9\x9d\xa2\xe6\x88\x91\xe4\xbb\xac\xe7\x85\xa7\xe7\x9d\x80\xe8\xbf\x99\xe4\xb8\xaa\xe7\xbb\x93\xe6\x9e\x84\xe6\x9d\xa5\xe5\x86\x99\xe4\xb8\x80\xe5\x86\x99\n\n#%%\nimport sys\nsys.path.append('..')\n\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torchvision.datasets import CIFAR10\n\n#%% [markdown]\n# \xe6\x88\x91\xe4\xbb\xac\xe5\x8f\xaf\xe4\xbb\xa5\xe5\xae\x9a\xe4\xb9\x89\xe4\xb8\x80\xe4\xb8\xaa vgg \xe7\x9a\x84 block\xef\xbc\x8c\xe4\xbc\xa0\xe5\x85\xa5\xe4\xb8\x89\xe4\xb8\xaa\xe5\x8f\x82\xe6\x95\xb0\xef\xbc\x8c\xe7\xac\xac\xe4\xb8\x80\xe4\xb8\xaa\xe6\x98\xaf\xe6\xa8\xa1\xe5\x9e\x8b\xe5\xb1\x82\xe6\x95\xb0\xef\xbc\x8c\xe7\xac\xac\xe4\xba\x8c\xe4\xb8\xaa\xe6\x98\xaf\xe8\xbe\x93\xe5\x85\xa5\xe7\x9a\x84\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\xef\xbc\x8c\xe7\xac\xac\xe4\xb8\x89\xe4\xb8\xaa\xe6\x98\xaf\xe8\xbe\x93\xe5\x87\xba\xe7\x9a\x84\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\xef\xbc\x8c\xe7\xac\xac\xe4\xb8\x80\xe5\xb1\x82\xe5\x8d\xb7\xe7\xa7\xaf\xe6\x8e\xa5\xe5\x8f\x97\xe7\x9a\x84\xe8\xbe\x93\xe5\x85\xa5\xe9\x80\x9a\xe9\x81\x93\xe5\xb0\xb1\xe6\x98\xaf\xe5\x9b\xbe\xe7\x89\x87\xe8\xbe\x93\xe5\x85\xa5\xe7\x9a\x84\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\xef\xbc\x8c\xe7\x84\xb6\xe5\x90\x8e\xe8\xbe\x93\xe5\x87\xba\xe6\x9c\x80\xe5\x90\x8e\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\xef\xbc\x8c\xe5\x90\x8e\xe9\x9d\xa2\xe7\x9a\x84\xe5\x8d\xb7\xe7\xa7\xaf\xe6\x8e\xa5\xe5\x8f\x97\xe7\x9a\x84\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\xe5\xb0\xb1\xe6\x98\xaf\xe6\x9c\x80\xe5\x90\x8e\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\n\n#%%\ndef vgg_block(num_convs, in_channels, out_channels):\n    net = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1), nn.ReLU(True)] # \xe5\xae\x9a\xe4\xb9\x89\xe7\xac\xac\xe4\xb8\x80\xe5\xb1\x82\n    \n    for i in range(num_convs-1): # \xe5\xae\x9a\xe4\xb9\x89\xe5\x90\x8e\xe9\x9d\xa2\xe7\x9a\x84\xe5\xbe\x88\xe5\xa4\x9a\xe5\xb1\x82\n        net.append(nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1))\n        net.append(nn.ReLU(True))\n        \n    net.append(nn.MaxPool2d(2, 2)) # \xe5\xae\x9a\xe4\xb9\x89\xe6\xb1\xa0\xe5\x8c\x96\xe5\xb1\x82\n    return nn.Sequential(*net)\n\n#%% [markdown]\n# \xe6\x88\x91\xe4\xbb\xac\xe5\x8f\xaf\xe4\xbb\xa5\xe5\xb0\x86\xe6\xa8\xa1\xe5\x9e\x8b\xe6\x89\x93\xe5\x8d\xb0\xe5\x87\xba\xe6\x9d\xa5\xe7\x9c\x8b\xe7\x9c\x8b\xe7\xbb\x93\xe6\x9e\x84\n\n#%%\nblock_demo = vgg_block(3, 64, 128)\nprint(block_demo)\n\n\n#%%\n# \xe9\xa6\x96\xe5\x85\x88\xe5\xae\x9a\xe4\xb9\x89\xe8\xbe\x93\xe5\x85\xa5\xe4\xb8\xba (1, 64, 300, 300)\ninput_demo = torch.zeros(1, 64, 300, 300)\noutput_demo = block_demo(input_demo)\nprint(output_demo.shape)\n\n#%% [markdown]\n# \xe5\x8f\xaf\xe4\xbb\xa5\xe7\x9c\x8b\xe5\x88\xb0\xe8\xbe\x93\xe5\x87\xba\xe5\xb0\xb1\xe5\x8f\x98\xe4\xb8\xba\xe4\xba\x86 (1, 128, 150, 150)\xef\xbc\x8c\xe5\x8f\xaf\xe4\xbb\xa5\xe7\x9c\x8b\xe5\x88\xb0\xe7\xbb\x8f\xe8\xbf\x87\xe4\xba\x86\xe8\xbf\x99\xe4\xb8\x80\xe4\xb8\xaa vgg block\xef\xbc\x8c\xe8\xbe\x93\xe5\x85\xa5\xe5\xa4\xa7\xe5\xb0\x8f\xe8\xa2\xab\xe5\x87\x8f\xe5\x8d\x8a\xef\xbc\x8c\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\xe5\x8f\x98\xe6\x88\x90\xe4\xba\x86 128\n# \n# \xe4\xb8\x8b\xe9\x9d\xa2\xe6\x88\x91\xe4\xbb\xac\xe5\xae\x9a\xe4\xb9\x89\xe4\xb8\x80\xe4\xb8\xaa\xe5\x87\xbd\xe6\x95\xb0\xe5\xaf\xb9\xe8\xbf\x99\xe4\xb8\xaa vgg block \xe8\xbf\x9b\xe8\xa1\x8c\xe5\xa0\x86\xe5\x8f\xa0\n\n#%%\ndef vgg_stack(num_convs, channels):\n    net = []\n    for n, c in zip(num_convs, channels):\n        in_c = c[0]\n        out_c = c[1]\n        net.append(vgg_block(n, in_c, out_c))\n    return nn.Sequential(*net)\n\n#%% [markdown]\n# \xe4\xbd\x9c\xe4\xb8\xba\xe5\xae\x9e\xe4\xbe\x8b\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe5\xae\x9a\xe4\xb9\x89\xe4\xb8\x80\xe4\xb8\xaa\xe7\xa8\x8d\xe5\xbe\xae\xe7\xae\x80\xe5\x8d\x95\xe4\xb8\x80\xe7\x82\xb9\xe7\x9a\x84 vgg \xe7\xbb\x93\xe6\x9e\x84\xef\xbc\x8c\xe5\x85\xb6\xe4\xb8\xad\xe6\x9c\x89 8 \xe4\xb8\xaa\xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82\n\n#%%\nvgg_net = vgg_stack((1, 1, 2, 2, 2), ((3, 64), (64, 128), (128, 256), (256, 512), (512, 512)))\nprint(vgg_net)\n\n#%% [markdown]\n# \xe6\x88\x91\xe4\xbb\xac\xe5\x8f\xaf\xe4\xbb\xa5\xe7\x9c\x8b\xe5\x88\xb0\xe7\xbd\x91\xe7\xbb\x9c\xe7\xbb\x93\xe6\x9e\x84\xe4\xb8\xad\xe6\x9c\x89\xe4\xb8\xaa 5 \xe4\xb8\xaa \xe6\x9c\x80\xe5\xa4\xa7\xe6\xb1\xa0\xe5\x8c\x96\xef\xbc\x8c\xe8\xaf\xb4\xe6\x98\x8e\xe5\x9b\xbe\xe7\x89\x87\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f\xe4\xbc\x9a\xe5\x87\x8f\xe5\xb0\x91 5 \xe5\x80\x8d\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe5\x8f\xaf\xe4\xbb\xa5\xe9\xaa\x8c\xe8\xaf\x81\xe4\xb8\x80\xe4\xb8\x8b\xef\xbc\x8c\xe8\xbe\x93\xe5\x85\xa5\xe4\xb8\x80\xe5\xbc\xa0 256 x 256 \xe7\x9a\x84\xe5\x9b\xbe\xe7\x89\x87\xe7\x9c\x8b\xe7\x9c\x8b\xe7\xbb\x93\xe6\x9e\x9c\xe6\x98\xaf\xe4\xbb\x80\xe4\xb9\x88\n\n#%%\ntest_x = torch.zeros(1, 3, 256, 256)\ntest_y = vgg_net(test_x)\nprint(test_y.shape)\n\n#%% [markdown]\n# \xe5\x8f\xaf\xe4\xbb\xa5\xe7\x9c\x8b\xe5\x88\xb0\xe5\x9b\xbe\xe7\x89\x87\xe5\x87\x8f\xe5\xb0\x8f\xe4\xba\x86 $2^5$ \xe5\x80\x8d\xef\xbc\x8c\xe6\x9c\x80\xe5\x90\x8e\xe5\x86\x8d\xe5\x8a\xa0\xe4\xb8\x8a\xe5\x87\xa0\xe5\xb1\x82\xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xef\xbc\x8c\xe5\xb0\xb1\xe8\x83\xbd\xe5\xa4\x9f\xe5\xbe\x97\xe5\x88\xb0\xe6\x88\x91\xe4\xbb\xac\xe6\x83\xb3\xe8\xa6\x81\xe7\x9a\x84\xe5\x88\x86\xe7\xb1\xbb\xe8\xbe\x93\xe5\x87\xba\n\n#%%\nclass vgg(nn.Module):\n    def __init__(self):\n        super(vgg, self).__init__()\n        self.feature = vgg_net\n        self.fc = nn.Sequential(\n            nn.Linear(512, 100),\n            nn.ReLU(True),\n            nn.Linear(100, 10)\n        )\n    def forward(self, x):\n        x = self.feature(x)\n        x = x.view(x.shape[0], -1)\n        x = self.fc(x)\n        return x\n\n#%% [markdown]\n# \xe7\x84\xb6\xe5\x90\x8e\xe6\x88\x91\xe4\xbb\xac\xe5\x8f\xaf\xe4\xbb\xa5\xe8\xae\xad\xe7\xbb\x83\xe6\x88\x91\xe4\xbb\xac\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\xe7\x9c\x8b\xe7\x9c\x8b\xe5\x9c\xa8 cifar10 \xe4\xb8\x8a\xe7\x9a\x84\xe6\x95\x88\xe6\x9e\x9c\n\n#%%\nfrom utils import train\n\ndef data_tf(x):\n    x = np.array(x, dtype='float32') / 255\n    x = (x - 0.5) / 0.5 # \xe6\xa0\x87\xe5\x87\x86\xe5\x8c\x96\xef\xbc\x8c\xe8\xbf\x99\xe4\xb8\xaa\xe6\x8a\x80\xe5\xb7\xa7\xe4\xb9\x8b\xe5\x90\x8e\xe4\xbc\x9a\xe8\xae\xb2\xe5\x88\xb0\n    x = x.transpose((2, 0, 1)) # \xe5\xb0\x86 channel \xe6\x94\xbe\xe5\x88\xb0\xe7\xac\xac\xe4\xb8\x80\xe7\xbb\xb4\xef\xbc\x8c\xe5\x8f\xaa\xe6\x98\xaf pytorch \xe8\xa6\x81\xe6\xb1\x82\xe7\x9a\x84\xe8\xbe\x93\xe5\x85\xa5\xe6\x96\xb9\xe5\xbc\x8f\n    x = torch.from_numpy(x)\n    return x\n     \ntrain_set = CIFAR10('./data', train=True, transform=data_tf)\ntrain_data = torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=True)\ntest_set = CIFAR10('./data', train=False, transform=data_tf)\ntest_data = torch.utils.data.DataLoader(test_set, batch_size=128, shuffle=False)\n\nnet = vgg()\noptimizer = torch.optim.SGD(net.parameters(), lr=1e-1)\ncriterion = nn.CrossEntropyLoss()\n\n\n#%%\ntrain(net, train_data, test_data, 20, optimizer, criterion)\n\n#%% [markdown]\n# \xe5\x8f\xaf\xe4\xbb\xa5\xe7\x9c\x8b\xe5\x88\xb0\xef\xbc\x8c\xe8\xb7\x91\xe5\xae\x8c 20 \xe6\xac\xa1\xef\xbc\x8cvgg \xe8\x83\xbd\xe5\x9c\xa8 cifar 10 \xe4\xb8\x8a\xe5\x8f\x96\xe5\xbe\x97 76% \xe5\xb7\xa6\xe5\x8f\xb3\xe7\x9a\x84\xe6\xb5\x8b\xe8\xaf\x95\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87\n\n"""
8.Famous_CNN 经典的CNN网络/utils.py,3,"b'from datetime import datetime\n\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\n\n\ndef get_acc(output, label):\n    total = output.shape[0]\n    _, pred_label = output.max(1)\n    num_correct = (pred_label == label).sum().item()\n    return num_correct / total\n\n\ndef train(net, train_data, valid_data, num_epochs, optimizer, criterion):\n\n    device = torch.device(\'cuda\' if torch.cuda.is_available() else \'cpu\')\n    net = net.to(device)\n    prev_time = datetime.now()\n    for epoch in range(num_epochs):\n        train_loss = 0\n        train_acc = 0\n        net = net.train()\n        for im, label in train_data:\n            im = im.to(device)\n            label = label.to(device)\n            # forward\n            output = net(im)\n            loss = criterion(output, label)\n            # backward\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            train_loss += loss.item()\n            train_acc += get_acc(output, label)\n\n        cur_time = datetime.now()\n        h, remainder = divmod((cur_time - prev_time).seconds, 3600)\n        m, s = divmod(remainder, 60)\n        time_str = ""Time %02d:%02d:%02d"" % (h, m, s)\n        if valid_data is not None:\n            valid_loss = 0\n            valid_acc = 0\n            net = net.eval()\n            with torch.no_grad():\n                for im, label in valid_data:\n                    im = im.to(device)\n                    label = label.to(device)\n                    output = net(im)\n                    loss = criterion(output, label)\n                    valid_loss += loss.item()\n                    valid_acc += get_acc(output, label)\n            \n            epoch_str = (\n                ""Epoch %d. Train Loss: %f, Train Acc: %f, Valid Loss: %f, Valid Acc: %f, ""\n                % (epoch, train_loss / len(train_data),\n                   train_acc / len(train_data), valid_loss / len(valid_data),\n                   valid_acc / len(valid_data)))\n        else:\n            epoch_str = (""Epoch %d. Train Loss: %f, Train Acc: %f, "" %\n                         (epoch, train_loss / len(train_data),\n                          train_acc / len(train_data)))\n        prev_time = cur_time\n        print(epoch_str + time_str)\n\n\ndef conv3x3(in_channel, out_channel, stride=1):\n    return nn.Conv2d(\n        in_channel, out_channel, 3, stride=stride, padding=1, bias=False)\n\n\nclass residual_block(nn.Module):\n    def __init__(self, in_channel, out_channel, same_shape=True):\n        super(residual_block, self).__init__()\n        self.same_shape = same_shape\n        stride = 1 if self.same_shape else 2\n\n        self.conv1 = conv3x3(in_channel, out_channel, stride=stride)\n        self.bn1 = nn.BatchNorm2d(out_channel)\n\n        self.conv2 = conv3x3(out_channel, out_channel)\n        self.bn2 = nn.BatchNorm2d(out_channel)\n        if not self.same_shape:\n            self.conv3 = nn.Conv2d(in_channel, out_channel, 1, stride=stride)\n\n    def forward(self, x):\n        out = self.conv1(x)\n        out = F.relu(self.bn1(out), True)\n        out = self.conv2(out)\n        out = F.relu(self.bn2(out), True)\n\n        if not self.same_shape:\n            x = self.conv3(x)\n        return F.relu(x + out, True)\n\n\nclass resnet(nn.Module):\n    def __init__(self, in_channel, num_classes, verbose=False):\n        super(resnet, self).__init__()\n        self.verbose = verbose\n\n        self.block1 = nn.Conv2d(in_channel, 64, 7, 2)\n\n        self.block2 = nn.Sequential(\n            nn.MaxPool2d(3, 2), residual_block(64, 64), residual_block(64, 64))\n\n        self.block3 = nn.Sequential(\n            residual_block(64, 128, False), residual_block(128, 128))\n\n        self.block4 = nn.Sequential(\n            residual_block(128, 256, False), residual_block(256, 256))\n\n        self.block5 = nn.Sequential(\n            residual_block(256, 512, False),\n            residual_block(512, 512), nn.AvgPool2d(3))\n\n        self.classifier = nn.Linear(512, num_classes)\n\n    def forward(self, x):\n        x = self.block1(x)\n        if self.verbose:\n            print(\'block 1 output: {}\'.format(x.shape))\n        x = self.block2(x)\n        if self.verbose:\n            print(\'block 2 output: {}\'.format(x.shape))\n        x = self.block3(x)\n        if self.verbose:\n            print(\'block 3 output: {}\'.format(x.shape))\n        x = self.block4(x)\n        if self.verbose:\n            print(\'block 4 output: {}\'.format(x.shape))\n        x = self.block5(x)\n        if self.verbose:\n            print(\'block 5 output: {}\'.format(x.shape))\n        x = x.view(x.shape[0], -1)\n        x = self.classifier(x)\n        return x\n'"
9.Using_Pretrained_Models 使用预训练的模型/pre_trained_networks.py,3,"b'\n#%%\nfrom torchvision import models\n\n\n#%%\ndir(models)\n\n\n#%%\nalexnet = models.AlexNet()\n\n\n#%%\nresnet = models.resnet101(pretrained=True)\n\n\n#%%\nresnet\n\n\n#%%\nfrom torchvision import transforms\npreprocess = transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize(\n            mean=[0.485, 0.456, 0.406],\n            std=[0.229, 0.224, 0.225]\n        )])\n\n\n#%%\nfrom PIL import Image\nimg = Image.open(""bobby.jpg"")\n\n\n#%%\nimg\n\n\n#%%\nimg_t = preprocess(img)\n\n\n#%%\nimport torch\nbatch_t = torch.unsqueeze(img_t, 0)\n\n\n#%%\nresnet.eval()\n\n\n#%%\nout = resnet(batch_t)\nout\n\n\n#%%\nwith open(\'imagenet_classes.txt\') as f:\n    labels = f.readlines()\n\n\n#%%\n_, index = torch.max(out, 1)\n\n\n#%%\nlabels[index[0]]\n\n\n#%%\n_, indices = torch.sort(out, descending=True)\n[labels[idx] for idx in indices[0]][:5]\n\n\n'"
9.Using_Pretrained_Models 使用预训练的模型/use Pretrained models with PyTorch.py,6,"b'\n#%%\n# How to use Pretrained models with PyTorch\n# Simple Classifier using resnet50\n# code by GunhoChoi\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.utils.data as data\nimport torchvision.models as models\nimport torchvision.datasets as dset\nimport torchvision.transforms as transforms\nfrom torch.autograd import Variable\n\nbatch_size = 3\nlearning_rate =0.0002\nepoch = 50\n\nresnet = models.resnet50(pretrained=True)\n\n\n#%%\n# Input pipeline from a folder containing multiple folders of images\n# we can check the classes, class_to_idx, and filename with idx\n\nimg_dir = ""./images""\nimg_data = dset.ImageFolder(img_dir, transforms.Compose([\n            transforms.RandomSizedCrop(224),\n            transforms.RandomHorizontalFlip(),\n            transforms.ToTensor(),\n            ]))\n\nprint(img_data.classes)\nprint(img_data.class_to_idx)\nprint(img_data.imgs)\n\n\n#%%\n# After we get the list of images, we can turn the list into batches of images\n# with torch.utils.data.DataLoader()\n\nimg_batch = data.DataLoader(img_data, batch_size=batch_size,\n                            shuffle=True, num_workers=2)\n\nfor img,label in img_batch:\n    print(img.size())\n    print(label)\n\n\n#%%\n# test of the result coming from resnet model\n\nimg = Variable(img)\nprint(resnet(img))\n\n\n#%%\n# we have 2 categorical variables so 1000 -> 500 -> 2\n# test the whole process\n\nmodel = nn.Sequential(\n            nn.Linear(1000,500),\n            nn.ReLU(),\n            nn.BatchNorm1d(500),\n            nn.Linear(500,2),\n            nn.ReLU()\n            )\n\nprint(model(resnet(img)))\n\n\n#%%\n# define loss func & optimizer\n\nloss_func = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(),lr=learning_rate)\n\n\n#%%\n# In order to train with GPU, we need to put the model and variables\n# by doing .cuda()\n\nresnet.cuda()\nmodel.cuda()\n\nfor i in range(epoch):\n    for img,label in img_batch:\n        img = Variable(img).cuda()\n        label = Variable(label).cuda()\n\n        optimizer.zero_grad()\n        output = model(resnet(img))\n        loss = loss_func(output,label)\n        loss.backward()\n        optimizer.step()\n\n    if i % 10 ==0:\n        print(loss)\n\n\n#%%\n# Check Accuracy of the trained model\n# Need to get used to using .cuda() and .data \n\nmodel.eval()\ncorrect = 0\ntotal = 0\n\nfor img,label in img_batch:\n    img = Variable(img).cuda()\n    label = Variable(label).cuda()\n    \n    output = model(resnet(img))\n    _, pred = torch.max(output.data,1)\n    \n    total += label.size(0)\n    correct += (pred == label.data).sum()   \n\nprint(""Accuracy: {}"".format(correct/total))\n\n\n'"
Udacity-Deep-Learning-with-PyTorch/Part 1 - Tensors in PyTorch.py,5,"b'#%% [markdown]\n# # Introduction to Deep Learning with PyTorch\n# \n# In this notebook, you\'ll get introduced to [PyTorch](http://pytorch.org/), a framework for building and training neural networks. PyTorch in a lot of ways behaves like the arrays you love from Numpy. These Numpy arrays, after all, are just tensors. PyTorch takes these tensors and makes it simple to move them to GPUs for the faster processing needed when training neural networks. It also provides a module that automatically calculates gradients (for backpropagation!) and another module specifically for building neural networks. All together, PyTorch ends up being more coherent with Python and the Numpy/Scipy stack compared to TensorFlow and other frameworks.\n# \n# \n#%% [markdown]\n# ## Neural Networks\n# \n# Deep Learning is based on artificial neural networks which have been around in some form since the late 1950s. The networks are built from individual parts approximating neurons, typically called units or simply ""neurons."" Each unit has some number of weighted inputs. These weighted inputs are summed together (a linear combination) then passed through an activation function to get the unit\'s output.\n# \n# <img src=""assets/simple_neuron.png"" width=400px>\n# \n# Mathematically this looks like: \n# \n# $$\n# \\begin{align}\n# y &= f(w_1 x_1 + w_2 x_2 + b) \\\\\n# y &= f\\left(\\sum_i w_i x_i \\right)\n# \\end{align}\n# $$\n# \n# With vectors this is the dot/inner product of two vectors:\n# \n# $$\n# h = \\begin{bmatrix}\n# x_1 \\, x_2 \\cdots  x_n\n# \\end{bmatrix}\n# \\cdot \n# \\begin{bmatrix}\n#            w_1 \\\\\n#            w_2 \\\\\n#            \\vdots \\\\\n#            w_n\n# \\end{bmatrix}\n# $$\n#%% [markdown]\n# ### Stack them up!\n# \n# We can assemble these unit neurons into layers and stacks, into a network of neurons. The output of one layer of neurons becomes the input for the next layer. With multiple input units and output units, we now need to express the weights as a matrix.\n# \n# <img src=\'assets/multilayer_diagram_weights.png\' width=450px>\n# \n# We can express this mathematically with matrices again and use matrix multiplication to get linear combinations for each unit in one operation. For example, the hidden layer ($h_1$ and $h_2$ here) can be calculated \n# \n# $$\n# \\vec{h} = [h_1 \\, h_2] = \n# \\begin{bmatrix}\n# x_1 \\, x_2 \\cdots \\, x_n\n# \\end{bmatrix}\n# \\cdot \n# \\begin{bmatrix}\n#            w_{11} & w_{12} \\\\\n#            w_{21} &w_{22} \\\\\n#            \\vdots &\\vdots \\\\\n#            w_{n1} &w_{n2}\n# \\end{bmatrix}\n# $$\n# \n# The output for this small network is found by treating the hidden layer as inputs for the output unit. The network output is expressed simply\n# \n# $$\n# y =  f_2 \\! \\left(\\, f_1 \\! \\left(\\vec{x} \\, \\mathbf{W_1}\\right) \\mathbf{W_2} \\right)\n# $$\n#%% [markdown]\n# ## Tensors\n# \n# It turns out neural network computations are just a bunch of linear algebra operations on *tensors*, a generalization of matrices. A vector is a 1-dimensional tensor, a matrix is a 2-dimensional tensor, an array with three indices is a 3-dimensional tensor (RGB color images for example). The fundamental data structure for neural networks are tensors and PyTorch (as well as pretty much every other deep learning framework) is built around tensors.\n# \n# <img src=""assets/tensor_examples.svg"" width=600px>\n# \n# With the basics covered, it\'s time to explore how we can use PyTorch to build a simple neural network.\n\n#%%\nget_ipython().run_line_magic(\'matplotlib\', \'inline\')\nget_ipython().run_line_magic(\'config\', ""InlineBackend.figure_format = \'retina\'"")\n\nimport numpy as np\nimport torch\n\nimport helper\n\n#%% [markdown]\n# First, let\'s see how we work with PyTorch tensors. These are the fundamental data structures of neural networks and PyTorch, so it\'s imporatant to understand how these work.\n\n#%%\nx = torch.rand(3, 2)\nx\n\n\n#%%\ny = torch.ones(x.size())\ny\n\n\n#%%\nz = x + y\nz\n\n#%% [markdown]\n# In general PyTorch tensors behave similar to Numpy arrays. They are zero indexed and support slicing.\n\n#%%\nz[0]\n\n\n#%%\nz[:, 1:]\n\n#%% [markdown]\n# Tensors typically have two forms of methods, one method that returns another tensor and another method that performs the operation in place. That is, the values in memory for that tensor are changed without creating a new tensor. In-place functions are always followed by an underscore, for example `z.add()` and `z.add_()`.\n\n#%%\n# Return a new tensor z + 1\nz.add(1)\n\n\n#%%\n# z tensor is unchanged\nz\n\n\n#%%\n# Add 1 and update z tensor in-place\nz.add_(1)\n\n\n#%%\n# z has been updated\nz\n\n#%% [markdown]\n# ### Reshaping\n# \n# Reshaping tensors is a really common operation. First to get the size and shape of a tensor use `.size()`. Then, to reshape a tensor, use `.resize_()`. Notice the underscore, reshaping is an in-place operation.\n\n#%%\nz.size()\n\n\n#%%\nz.resize_(2, 3)\n\n\n#%%\nz\n\n#%% [markdown]\n# ## Numpy to Torch and back\n# \n# Converting between Numpy arrays and Torch tensors is super simple and useful. To create a tensor from a Numpy array, use `torch.from_numpy()`. To convert a tensor to a Numpy array, use the `.numpy()` method.\n\n#%%\na = np.random.rand(4,3)\na\n\n\n#%%\nb = torch.from_numpy(a)\nb\n\n\n#%%\nb.numpy()\n\n#%% [markdown]\n# The memory is shared between the Numpy array and Torch tensor, so if you change the values in-place of one object, the other will change as well.\n\n#%%\n# Multiply PyTorch Tensor by 2, in place\nb.mul_(2)\n\n\n#%%\n# Numpy array matches new values from Tensor\na\n\n\n'"
Udacity-Deep-Learning-with-PyTorch/Part 2 - Neural Networks in PyTorch.py,6,"b'#%% [markdown]\n# # Neural networks with PyTorch\n# \n# Next I\'ll show you how to build a neural network with PyTorch.\n\n#%%\n# Import things like usual\n\nget_ipython().run_line_magic(\'matplotlib\', \'inline\')\nget_ipython().run_line_magic(\'config\', ""InlineBackend.figure_format = \'retina\'"")\n\nimport numpy as np\nimport torch\n\nimport helper\n\nimport matplotlib.pyplot as plt\nfrom torchvision import datasets, transforms\n\n#%% [markdown]\n# First up, we need to get our dataset. This is provided through the `torchvision` package. The code below will download the MNIST dataset, then create training and test datasets for us. Don\'t worry too much about the details here, you\'ll learn more about this later.\n\n#%%\n# Define a transform to normalize the data\ntransform = transforms.Compose([transforms.ToTensor(),\n                              transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n                             ])\n# Download and load the training data\ntrainset = datasets.MNIST(\'MNIST_data/\', download=True, train=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n\n# Download and load the test data\ntestset = datasets.MNIST(\'MNIST_data/\', download=True, train=False, transform=transform)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)\n\n\n#%%\ndataiter = iter(trainloader)\nimages, labels = dataiter.next()\n\n#%% [markdown]\n# We have the training data loaded into `trainloader` and we make that an iterator with `iter(trainloader)`. We\'d use this to loop through the dataset for training, but here I\'m just grabbing the first batch so we can check out the data. We can see below that `images` is just a tensor with size (64, 1, 28, 28). So, 64 images per batch, 1 color channel, and 28x28 images.\n\n#%%\nplt.imshow(images[1].numpy().squeeze(), cmap=\'Greys_r\');\n\n#%% [markdown]\n# ## Building networks with PyTorch\n# \n# Here I\'ll use PyTorch to build a simple feedfoward network to classify the MNIST images. That is, the network will receive a digit image as input and predict the digit in the image.\n# \n# <img src=""assets/mlp_mnist.png"" width=600px>\n# \n# To build a neural network with PyTorch, you use the `torch.nn` module. The network itself is a class inheriting from `torch.nn.Module`. You define each of the operations separately, like `nn.Linear(784, 128)` for a fully connected linear layer with 784 inputs and 128 units.\n# \n# The class needs to include a `forward` method that implements the forward pass through the network. In this method, you pass some input tensor `x` through each of the operations you defined earlier. The `torch.nn` module also has functional equivalents for things like ReLUs in `torch.nn.functional`. This module is usually imported as `F`. Then to use a ReLU activation on some layer (which is just a tensor), you\'d do `F.relu(x)`. Below are a few different commonly used activation functions.\n# \n# <img src=""assets/activation.png"" width=700px>\n# \n# So, for this network, I\'ll build it with three fully connected layers, then a softmax output for predicting classes. The softmax function is similar to the sigmoid in that it squashes inputs between 0 and 1, but it\'s also normalized so that all the values sum to one like a proper probability distribution.\n\n#%%\nfrom torch import nn\nfrom torch import optim\nimport torch.nn.functional as F\n\n\n#%%\nclass Network(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # Defining the layers, 128, 64, 10 units each\n        self.fc1 = nn.Linear(784, 128)\n        self.fc2 = nn.Linear(128, 64)\n        # Output layer, 10 units - one for each digit\n        self.fc3 = nn.Linear(64, 10)\n        \n    def forward(self, x):\n        \'\'\' Forward pass through the network, returns the output logits \'\'\'\n        \n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.fc2(x)\n        x = F.relu(x)\n        x = self.fc3(x)\n        x = F.softmax(x, dim=1)\n        \n        return x\n\nmodel = Network()\nmodel\n\n#%% [markdown]\n# ### Initializing weights and biases\n# \n# The weights and such are automatically initialized for you, but it\'s possible to customize how they are initialized. The weights and biases are tensors attached to the layer you defined, you can get them with `model.fc1.weight` for instance.\n\n#%%\nprint(model.fc1.weight)\nprint(model.fc1.bias)\n\n#%% [markdown]\n# For custom initialization, we want to modify these tensors in place. These are actually autograd *Variables*, so we need to get back the actual tensors with `model.fc1.weight.data`. Once we have the tensors, we can fill them with zeros (for biases) or random normal values.\n\n#%%\n# Set biases to all zeros\nmodel.fc1.bias.data.fill_(0)\n\n\n#%%\n# sample from random normal with standard dev = 0.01\nmodel.fc1.weight.data.normal_(std=0.01)\n\n#%% [markdown]\n# ### Forward pass\n# \n# Now that we have a network, let\'s see what happens when we pass in an image. This is called the forward pass. We\'re going to convert the image data into a tensor, then pass it through the operations defined by the network architecture.\n\n#%%\n# Grab some data \ndataiter = iter(trainloader)\nimages, labels = dataiter.next()\n\n# Resize images into a 1D vector, new shape is (batch size, color channels, image pixels) \nimages.resize_(64, 1, 784)\n# or images.resize_(images.shape[0], 1, 784) to not automatically get batch size\n\n# Forward pass through the network\nimg_idx = 0\nps = model.forward(images[img_idx,:])\n\nimg = images[img_idx]\nhelper.view_classify(img.view(1, 28, 28), ps)\n\n#%% [markdown]\n# As you can see above, our network has basically no idea what this digit is. It\'s because we haven\'t trained it yet, all the weights are random!\n# \n# PyTorch provides a convenient way to build networks like this where a tensor is passed sequentially through operations, `nn.Sequential` ([documentation](https://pytorch.org/docs/master/nn.html#torch.nn.Sequential)). Using this to build the equivalent network:\n\n#%%\n# Hyperparameters for our network\ninput_size = 784\nhidden_sizes = [128, 64]\noutput_size = 10\n\n# Build a feed-forward network\nmodel = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n                      nn.ReLU(),\n                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n                      nn.ReLU(),\n                      nn.Linear(hidden_sizes[1], output_size),\n                      nn.Softmax(dim=1))\nprint(model)\n\n# Forward pass through the network and display output\nimages, labels = next(iter(trainloader))\nimages.resize_(images.shape[0], 1, 784)\nps = model.forward(images[0,:])\nhelper.view_classify(images[0].view(1, 28, 28), ps)\n\n#%% [markdown]\n# You can also pass in an `OrderedDict` to name the individual layers and operations. Note that a dictionary keys must be unique, so _each operation must have a different name_.\n\n#%%\nfrom collections import OrderedDict\nmodel = nn.Sequential(OrderedDict([\n                      (\'fc1\', nn.Linear(input_size, hidden_sizes[0])),\n                      (\'relu1\', nn.ReLU()),\n                      (\'fc2\', nn.Linear(hidden_sizes[0], hidden_sizes[1])),\n                      (\'relu2\', nn.ReLU()),\n                      (\'output\', nn.Linear(hidden_sizes[1], output_size)),\n                      (\'softmax\', nn.Softmax(dim=1))]))\nmodel\n\n#%% [markdown]\n# Now it\'s your turn to build a simple network, use any method I\'ve covered so far. In the next notebook, you\'ll learn how to train a network so it can make good predictions.\n# \n# >**Exercise:** Build a network to classify the MNIST images with _three_ hidden layers. Use 400 units in the first hidden layer, 200 units in the second layer, and 100 units in the third layer. Each hidden layer should have a ReLU activation function, and use softmax on the output layer. \n\n#%%\n## TODO: Your network here\n\n\n#%%\n## Run this cell with your model to make sure it works ##\n# Forward pass through the network and display output\nimages, labels = next(iter(trainloader))\nimages.resize_(images.shape[0], 1, 784)\nps = model.forward(images[0,:])\nhelper.view_classify(images[0].view(1, 28, 28), ps)\n\n\n'"
Udacity-Deep-Learning-with-PyTorch/Part 3 - Training Neural Networks.py,10,"b'#%% [markdown]\n# # Training Neural Networks\n# \n# The network we built in the previous part isn\'t so smart, it doesn\'t know anything about our handwritten digits. Neural networks with non-linear activations work like universal function approximators. There is some function that maps your input to the output. For example, images of handwritten digits to class probabilities. The power of neural networks is that we can train them to approximate this function, and basically any function given enough data and compute time.\n# \n# <img src=""assets/function_approx.png"" width=500px>\n# \n# At first the network is naive, it doesn\'t know the function mapping the inputs to the outputs. We train the network by showing it examples of real data, then adjusting the network parameters such that it approximates this function.\n# \n# To find these parameters, we need to know how poorly the network is predicting the real outputs. For this we calculate a **loss function** (also called the cost), a measure of our prediction error. For example, the mean squared loss is often used in regression and binary classification problems\n# \n# $$\n# \\ell = \\frac{1}{2n}\\sum_i^n{\\left(y_i - \\hat{y}_i\\right)^2}\n# $$\n# \n# where $n$ is the number of training examples, $y_i$ are the true labels, and $\\hat{y}_i$ are the predicted labels.\n# \n# By minimizing this loss with respect to the network parameters, we can find configurations where the loss is at a minimum and the network is able to predict the correct labels with high accuracy. We find this minimum using a process called **gradient descent**. The gradient is the slope of the loss function and points in the direction of fastest change. To get to the minimum in the least amount of time, we then want to follow the gradient (downwards). You can think of this like descending a mountain by following the steepest slope to the base.\n# \n# <img src=\'assets/gradient_descent.png\' width=350px>\n#%% [markdown]\n# ## Backpropagation\n# \n# For single layer networks, gradient descent is simple to implement. However, it\'s more complicated for deeper, multilayer neural networks like the one we\'ve built. Complicated enough that it took about 30 years before researchers figured out how to train multilayer networks, although it\'s straightforward once you learn about it. \n# \n# This is done through **backpropagation** which is really just an application of the chain rule from calculus. It\'s easiest to understand if we convert a two layer network into a graph representation.\n# \n# <img src=\'assets/w1_backprop_graph.png\' width=400px>\n# \n# In the forward pass through the network, our data and operations go from right to left here. To train the weights with gradient descent, we propagate the gradient of the cost backwards through the network. Mathematically, this is really just calculating the gradient of the loss with respect to the weights using the chain rule.\n# \n# $$\n# \\frac{\\partial \\ell}{\\partial w_1} = \\frac{\\partial l_1}{\\partial w_1} \\frac{\\partial s}{\\partial l_1} \\frac{\\partial l_2}{\\partial s} \\frac{\\partial \\ell}{\\partial l_2}\n# $$\n# \n# We update our weights using this gradient with some learning rate $\\alpha$. \n# \n# $$\n# w^\\prime = w - \\alpha \\frac{\\partial \\ell}{\\partial w}\n# $$\n# \n# The learning rate is set such that the weight update steps are small enough that the iterative method settles in a minimum.\n# \n# The first thing we need to do for training is define our loss function. In PyTorch, you\'ll usually see this as `criterion`. Here we\'re using softmax output, so we want to use `criterion = nn.CrossEntropyLoss()` as our loss. Later when training, you use `loss = criterion(output, targets)` to calculate the actual loss.\n# \n# We also need to define the optimizer we\'re using, SGD or Adam, or something along those lines. Here I\'ll just use SGD with `torch.optim.SGD`, passing in the network parameters and the learning rate.\n#%% [markdown]\n# ## Autograd\n# \n# Torch provides a module, `autograd`, for automatically calculating the gradient of tensors. It does this by keeping track of operations performed on tensors. To make sure PyTorch keeps track of operations on a tensor and calculates the gradients, you need to set `requires_grad` on a tensor. You can do this at creation with the `requires_grad` keyword, or at any time with `x.requires_grad_(True)`.\n# \n# You can turn off gradients for a block of code with the `torch.no_grad()` content:\n# ```python\n# x = torch.zeros(1, requires_grad=True)\n# >>> with torch.no_grad():\n# ...     y = x * 2\n# >>> y.requires_grad\n# False\n# ```\n# \n# Also, you can turn on or off gradients altogether with `torch.set_grad_enabled(True|False)`.\n# \n# The gradients are computed with respect to some variable `z` with `z.backward()`. This does a backward pass through the operations that created `z`.\n\n#%%\nget_ipython().run_line_magic(\'matplotlib\', \'inline\')\nget_ipython().run_line_magic(\'config\', ""InlineBackend.figure_format = \'retina\'"")\n\nfrom collections import OrderedDict\n\nimport numpy as np\nimport time\n\nimport torch\nfrom torch import nn\nfrom torch import optim\nimport torch.nn.functional as F\n\nimport helper\n\n\n#%%\nx = torch.randn(2,2, requires_grad=True)\nprint(x)\n\n\n#%%\ny = x**2\nprint(y)\n\n#%% [markdown]\n# Below we can see the operation that created `y`, a power operation `PowBackward0`.\n\n#%%\n## grad_fn shows the function that generated this variable\nprint(y.grad_fn)\n\n#%% [markdown]\n# The autgrad module keeps track of these operations and knows how to calculate the gradient for each one. In this way, it\'s able to calculate the gradients for a chain of operations, with respect to any one tensor. Let\'s reduce the tensor `y` to a scalar value, the mean.\n\n#%%\nz = y.mean()\nprint(z)\n\n#%% [markdown]\n# You can check the gradients for `x` and `y` but they are empty currently.\n\n#%%\nprint(x.grad)\n\n#%% [markdown]\n# To calculate the gradients, you need to run the `.backward` method on a Variable, `z` for example. This will calculate the gradient for `z` with respect to `x`\n# \n# $$\n# \\frac{\\partial z}{\\partial x} = \\frac{\\partial}{\\partial x}\\left[\\frac{1}{n}\\sum_i^n x_i^2\\right] = \\frac{x}{2}\n# $$\n\n#%%\nz.backward()\nprint(x.grad)\nprint(x/2)\n\n#%% [markdown]\n# These gradients calculations are particularly useful for neural networks. For training we need the gradients of the weights with respect to the cost. With PyTorch, we run data forward through the network to calculate the cost, then, go backwards to calculate the gradients with respect to the cost. Once we have the gradients we can make a gradient descent step. \n#%% [markdown]\n# ## Get the data and define the network\n# \n# The same as we saw in part 3, we\'ll load the MNIST dataset and define our network.\n\n#%%\nfrom torchvision import datasets, transforms\n\n# Define a transform to normalize the data\ntransform = transforms.Compose([transforms.ToTensor(),\n                              transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n                             ])\n# Download and load the training data\ntrainset = datasets.MNIST(\'MNIST_data/\', download=True, train=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n\n#%% [markdown]\n# I\'ll build a network with `nn.Sequential` here. Only difference from the last part is I\'m not actually using softmax on the output, but instead just using the raw output from the last layer. This is because the output from softmax is a probability distribution. Often, the output will have values really close to zero or really close to one. Due to [inaccuracies with representing numbers as floating points](https://docs.python.org/3/tutorial/floatingpoint.html), computations with a softmax output can lose accuracy and become unstable. To get around this, we\'ll use the raw output, called the **logits**, to calculate the loss.\n\n#%%\n# Hyperparameters for our network\ninput_size = 784\nhidden_sizes = [128, 64]\noutput_size = 10\n\n# Build a feed-forward network\nmodel = nn.Sequential(OrderedDict([\n                      (\'fc1\', nn.Linear(input_size, hidden_sizes[0])),\n                      (\'relu1\', nn.ReLU()),\n                      (\'fc2\', nn.Linear(hidden_sizes[0], hidden_sizes[1])),\n                      (\'relu2\', nn.ReLU()),\n                      (\'logits\', nn.Linear(hidden_sizes[1], output_size))]))\n\n#%% [markdown]\n# ## Training the network!\n# \n# The first thing we need to do for training is define our loss function. In PyTorch, you\'ll usually see this as `criterion`. Here we\'re using softmax output, so we want to use `criterion = nn.CrossEntropyLoss()` as our loss. Later when training, you use `loss = criterion(output, targets)` to calculate the actual loss.\n# \n# We also need to define the optimizer we\'re using, SGD or Adam, or something along those lines. Here I\'ll just use SGD with `torch.optim.SGD`, passing in the network parameters and the learning rate.\n\n#%%\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.01)\n\n#%% [markdown]\n# First, let\'s consider just one learning step before looping through all the data. The general process with PyTorch:\n# \n# * Make a forward pass through the network to get the logits \n# * Use the logits to calculate the loss\n# * Perform a backward pass through the network with `loss.backward()` to calculate the gradients\n# * Take a step with the optimizer to update the weights\n# \n# Below I\'ll go through one training step and print out the weights and gradients so you can see how it changes.\n\n#%%\nprint(\'Initial weights - \', model.fc1.weight)\n\nimages, labels = next(iter(trainloader))\nimages.resize_(64, 784)\n\n# Clear the gradients, do this because gradients are accumulated\noptimizer.zero_grad()\n\n# Forward pass, then backward pass, then update weights\noutput = model.forward(images)\nloss = criterion(output, labels)\nloss.backward()\nprint(\'Gradient -\', model.fc1.weight.grad)\noptimizer.step()\n\n\n#%%\nprint(\'Updated weights - \', model.fc1.weight)\n\n#%% [markdown]\n# ### Training for real\n# \n# Now we\'ll put this algorithm into a loop so we can go through all the images. This is fairly straightforward. We\'ll loop through the mini-batches in our dataset, pass the data through the network to calculate the losses, get the gradients, then run the optimizer.\n\n#%%\noptimizer = optim.SGD(model.parameters(), lr=0.003)\n\n\n#%%\nepochs = 3\nprint_every = 40\nsteps = 0\nfor e in range(epochs):\n    running_loss = 0\n    for images, labels in iter(trainloader):\n        steps += 1\n        # Flatten MNIST images into a 784 long vector\n        images.resize_(images.size()[0], 784)\n        \n        optimizer.zero_grad()\n        \n        # Forward and backward passes\n        output = model.forward(images)\n        loss = criterion(output, labels)\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n        \n        if steps % print_every == 0:\n            print(""Epoch: {}/{}... "".format(e+1, epochs),\n                  ""Loss: {:.4f}"".format(running_loss/print_every))\n            \n            running_loss = 0\n\n#%% [markdown]\n# With the network trained, we can check out it\'s predictions.\n\n#%%\nimages, labels = next(iter(trainloader))\n\nimg = images[0].view(1, 784)\n# Turn off gradients to speed up this part\nwith torch.no_grad():\n    logits = model.forward(img)\n\n# Output of the network are logits, need to take softmax for probabilities\nps = F.softmax(logits, dim=1)\nhelper.view_classify(img.view(1, 28, 28), ps)\n\n#%% [markdown]\n# Now our network is brilliant. It can accurately predict the digits in our images. Next up you\'ll write the code for training a neural network on a more complex dataset.\n\n'"
Udacity-Deep-Learning-with-PyTorch/Part 4 - Fashion-MNIST Exercise.py,4,"b'#%% [markdown]\n# # Classifying Fashion-MNIST\n# \n# Now it\'s your turn to build and train a neural network. You\'ll be using the [Fashion-MNIST dataset](https://github.com/zalandoresearch/fashion-mnist), a drop-in replacement for the MNIST dataset. MNIST is actually quite trivial with neural networks where you can easily achieve better than 97% accuracy. Fashion-MNIST is a set of 28x28 greyscale images of clothes. It\'s more complex than MNIST, so it\'s a better representation of the actual performance of your network, and a better representation of datasets you\'ll use in the real world.\n# \n# <img src=\'assets/fashion-mnist-sprite.png\' width=500px>\n# \n# In this notebook, you\'ll build your own neural network. For the most part, you could just copy and paste the code from Part 3, but you wouldn\'t be learning. It\'s important for you to write the code yourself and get it to work. Feel free to consult the previous notebook though as you work through this.\n# \n# First off, let\'s load the dataset through torchvision.\n\n#%%\nimport torch\nfrom torchvision import datasets, transforms\nimport helper\n\n# Define a transform to normalize the data\ntransform = transforms.Compose([transforms.ToTensor(),\n                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n# Download and load the training data\ntrainset = datasets.FashionMNIST(\'F_MNIST_data/\', download=True, train=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n\n# Download and load the test data\ntestset = datasets.FashionMNIST(\'F_MNIST_data/\', download=True, train=False, transform=transform)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)\n\n#%% [markdown]\n# Here we can see one of the images.\n\n#%%\nimage, label = next(iter(trainloader))\nhelper.imshow(image[0,:]);\n\n#%% [markdown]\n# With the data loaded, it\'s time to import the necessary packages.\n\n#%%\nget_ipython().run_line_magic(\'matplotlib\', \'inline\')\nget_ipython().run_line_magic(\'config\', ""InlineBackend.figure_format = \'retina\'"")\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport time\n\nimport torch\nfrom torch import nn\nfrom torch import optim\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms\n\nimport helper\n\n#%% [markdown]\n# ## Building the network\n# \n# Here you should define your network. As with MNIST, each image is 28x28 which is a total of 784 pixels, and there are 10 classes. You should include at least one hidden layer. We suggest you use ReLU activations for the layers and to return the logits from the forward pass. It\'s up to you how many layers you add and the size of those layers.\n\n#%%\n# TODO: Define your network architecture here\n\n#%% [markdown]\n# # Train the network\n# \n# Now you should create your network and train it. First you\'ll want to define [the criterion](http://pytorch.org/docs/master/nn.html#loss-functions) ( something like `nn.CrossEntropyLoss`) and [the optimizer](http://pytorch.org/docs/master/optim.html) (typically `optim.SGD` or `optim.Adam`).\n# \n# Then write the training code. Remember the training pass is a fairly straightforward process:\n# \n# * Make a forward pass through the network to get the logits \n# * Use the logits to calculate the loss\n# * Perform a backward pass through the network with `loss.backward()` to calculate the gradients\n# * Take a step with the optimizer to update the weights\n# \n# By adjusting the hyperparameters (hidden units, learning rate, etc), you should be able to get the training loss below 0.4.\n\n#%%\n# TODO: Create the network, define the criterion and optimizer\n\n\n#%%\n# TODO: Train the network here\n\n\n#%%\n# Test out your network!\n\ndataiter = iter(testloader)\nimages, labels = dataiter.next()\nimg = images[0]\n# Convert 2D image to 1D vector\nimg = img.resize_(1, 784)\n\n# TODO: Calculate the class probabilities (softmax) for img\nps = \n\n# Plot the image and probabilities\nhelper.view_classify(img.resize_(1, 28, 28), ps, version=\'Fashion\')\n\n#%% [markdown]\n# Now that your network is trained, you\'ll want to save it to disk so you can load it later instead of training it again. Obviously, it\'s impractical to train a network every time you need one. In practice, you\'ll train it once, save the model, then reload it for further training or making predictions. In the next part, I\'ll show you how to save and load trained models.\n\n'"
Udacity-Deep-Learning-with-PyTorch/Part 5 - Inference and Validation.py,12,"b'#%% [markdown]\n# # Inference and Validation\n# \n# Now that you have a trained network, you can use it for making predictions. This is typically called **inference**, a term borrowed from statistics. However, neural networks have a tendency to perform *too well* on the training data and aren\'t able to generalize to data that hasn\'t been seen before. This is called **overfitting** and it impairs inference performance. To test for overfitting while training, we measure the performance on data not in the training set called the **validation** dataset. We avoid overfitting through regularization such as dropout while monitoring the validation performance during training. In this notebook, I\'ll show you how to do this in PyTorch. \n# \n# First off, I\'ll implement my own feedforward network for the exercise you worked on in part 4 using the Fashion-MNIST dataset.\n# \n# As usual, let\'s start by loading the dataset through torchvision. You\'ll learn more about torchvision and loading data in a later part.\n\n#%%\nget_ipython().run_line_magic(\'matplotlib\', \'inline\')\nget_ipython().run_line_magic(\'config\', ""InlineBackend.figure_format = \'retina\'"")\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport time\n\nimport torch\nfrom torch import nn\nfrom torch import optim\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms\n\nimport helper\n\n\n#%%\n# Define a transform to normalize the data\ntransform = transforms.Compose([transforms.ToTensor(),\n                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n# Download and load the training data\ntrainset = datasets.FashionMNIST(\'F_MNIST_data/\', download=True, train=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n\n# Download and load the test data\ntestset = datasets.FashionMNIST(\'F_MNIST_data/\', download=True, train=False, transform=transform)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)\n\n#%% [markdown]\n# ## Building the network\n# \n# As with MNIST, each image in Fashion-MNIST is 28x28 which is a total of 784 pixels, and there are 10 classes. I\'m going to get a bit more advanced here, I want to be able to build a network with an arbitrary number of hidden layers. That is, I want to pass in a parameter like `hidden_layers = [512, 256, 128]` and the network is contructed with three hidden layers have 512, 256, and 128 units respectively. To do this, I\'ll use `nn.ModuleList` to allow for an arbitrary number of hidden layers. Using `nn.ModuleList` works pretty much the same as a normal Python list, except that it registers each hidden layer `Linear` module properly so the model is aware of the layers.\n# \n# The issue here is I need a way to define each `nn.Linear` module with the appropriate layer sizes. Since each `nn.Linear` operation needs an input size and an output size, I need something that looks like this:\n# \n# ```python\n# # Create ModuleList and add input layer\n# hidden_layers = nn.ModuleList([nn.Linear(input_size, hidden_layers[0])])\n# # Add hidden layers to the ModuleList\n# hidden_layers.extend([nn.Linear(h1, h2) for h1, h2 in layer_sizes])\n# ```\n# \n# Getting these pairs of input and output sizes can be done with a handy trick using `zip`.\n# \n# ```python\n# hidden_layers = [512, 256, 128, 64]\n# layer_sizes = zip(hidden_layers[:-1], hidden_layers[1:])\n# for each in layer_sizes:\n#     print(each)\n# \n# >> (512, 256)\n# >> (256, 128)\n# >> (128, 64)\n# ```\n# \n# I also have the `forward` method returning the log-softmax for the output. Since softmax is a probability distibution over the classes, the log-softmax is a log probability which comes with a [lot of benefits](https://en.wikipedia.org/wiki/Log_probability). Using the log probability, computations are often faster and more accurate. To get the class probabilities later, I\'ll need to take the exponential (`torch.exp`) of the output. Algebra refresher... the exponential function is the inverse of the log function:\n# \n# $$ \\large{e^{\\ln{x}} = x }$$\n# \n# We can include dropout in our network with [`nn.Dropout`](http://pytorch.org/docs/master/nn.html#dropout). This works similar to other modules such as `nn.Linear`. It also takes the dropout probability as an input which we can pass as an input to the network.\n\n#%%\nclass Network(nn.Module):\n    def __init__(self, input_size, output_size, hidden_layers, drop_p=0.5):\n        \'\'\' Builds a feedforward network with arbitrary hidden layers.\n        \n            Arguments\n            ---------\n            input_size: integer, size of the input\n            output_size: integer, size of the output layer\n            hidden_layers: list of integers, the sizes of the hidden layers\n            drop_p: float between 0 and 1, dropout probability\n        \'\'\'\n        super().__init__()\n        # Add the first layer, input to a hidden layer\n        self.hidden_layers = nn.ModuleList([nn.Linear(input_size, hidden_layers[0])])\n        \n        # Add a variable number of more hidden layers\n        layer_sizes = zip(hidden_layers[:-1], hidden_layers[1:])\n        self.hidden_layers.extend([nn.Linear(h1, h2) for h1, h2 in layer_sizes])\n        \n        self.output = nn.Linear(hidden_layers[-1], output_size)\n        \n        self.dropout = nn.Dropout(p=drop_p)\n        \n    def forward(self, x):\n        \'\'\' Forward pass through the network, returns the output logits \'\'\'\n        \n        # Forward through each layer in `hidden_layers`, with ReLU activation and dropout\n        for linear in self.hidden_layers:\n            x = F.relu(linear(x))\n            x = self.dropout(x)\n        \n        x = self.output(x)\n        \n        return F.log_softmax(x, dim=1)\n\n#%% [markdown]\n# # Train the network\n# \n# Since the model\'s forward method returns the log-softmax, I used the [negative log loss](http://pytorch.org/docs/master/nn.html#nllloss) as my criterion, `nn.NLLLoss()`. I also chose to use the [Adam optimizer](http://pytorch.org/docs/master/optim.html#torch.optim.Adam). This is a variant of stochastic gradient descent which includes momentum and in general trains faster than your basic SGD.\n# \n# I\'ve also included a block to measure the validation loss and accuracy. Since I\'m using dropout in the network, I need to turn it off during inference. Otherwise, the network will appear to perform poorly because many of the connections are turned off. PyTorch allows you to set a model in ""training"" or ""evaluation"" modes with `model.train()` and `model.eval()`, respectively. In training mode, dropout is turned on, while in evaluation mode, dropout is turned off. This effects other modules as well that should be on during training but off during inference.\n# \n# The validation code consists of a forward pass through the validation set (also split into batches). With the log-softmax output, I calculate the loss on the validation set, as well as the prediction accuracy.\n\n#%%\n# Create the network, define the criterion and optimizer\nmodel = Network(784, 10, [516, 256], drop_p=0.5)\ncriterion = nn.NLLLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n\n#%%\n# Implement a function for the validation pass\ndef validation(model, testloader, criterion):\n    test_loss = 0\n    accuracy = 0\n    for images, labels in testloader:\n\n        images.resize_(images.shape[0], 784)\n\n        output = model.forward(images)\n        test_loss += criterion(output, labels).item()\n\n        ps = torch.exp(output)\n        equality = (labels.data == ps.max(dim=1)[1])\n        accuracy += equality.type(torch.FloatTensor).mean()\n    \n    return test_loss, accuracy\n\n\n#%%\nepochs = 2\nsteps = 0\nrunning_loss = 0\nprint_every = 40\nfor e in range(epochs):\n    model.train()\n    for images, labels in trainloader:\n        steps += 1\n        \n        # Flatten images into a 784 long vector\n        images.resize_(images.size()[0], 784)\n        \n        optimizer.zero_grad()\n        \n        output = model.forward(images)\n        loss = criterion(output, labels)\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n        \n        if steps % print_every == 0:\n            # Make sure network is in eval mode for inference\n            model.eval()\n            \n            # Turn off gradients for validation, saves memory and computations\n            with torch.no_grad():\n                test_loss, accuracy = validation(model, testloader, criterion)\n                \n            print(""Epoch: {}/{}.. "".format(e+1, epochs),\n                  ""Training Loss: {:.3f}.. "".format(running_loss/print_every),\n                  ""Test Loss: {:.3f}.. "".format(test_loss/len(testloader)),\n                  ""Test Accuracy: {:.3f}"".format(accuracy/len(testloader)))\n            \n            running_loss = 0\n            \n            # Make sure training is back on\n            model.train()\n\n#%% [markdown]\n# ## Inference\n# \n# Now that the model is trained, we can use it for inference. We\'ve done this before, but now we need to remember to set the model in inference mode with `model.eval()`. You\'ll also want to turn off autograd with the `torch.no_grad()` context.\n\n#%%\n# Test out your network!\n\nmodel.eval()\n\ndataiter = iter(testloader)\nimages, labels = dataiter.next()\nimg = images[0]\n# Convert 2D image to 1D vector\nimg = img.view(1, 784)\n\n# Calculate the class probabilities (softmax) for img\nwith torch.no_grad():\n    output = model.forward(img)\n\nps = torch.exp(output)\n\n# Plot the image and probabilities\nhelper.view_classify(img.view(1, 28, 28), ps, version=\'Fashion\')\n\n#%% [markdown]\n# ## Next Up!\n# \n# In the next part, I\'ll show you how to save your trained models. In general, you won\'t want to train a model everytime you need it. Instead, you\'ll train once, save it, then load the model when you want to train more or use if for inference.\n\n'"
Udacity-Deep-Learning-with-PyTorch/Part 6 - Saving and Loading Models.py,9,"b'#%% [markdown]\n# # Saving and Loading Models\n# \n# In this notebook, I\'ll show you how to save and load models with PyTorch. This is important because you\'ll often want to load previously trained models to use in making predictions or to continue training on new data.\n\n#%%\nget_ipython().run_line_magic(\'matplotlib\', \'inline\')\nget_ipython().run_line_magic(\'config\', ""InlineBackend.figure_format = \'retina\'"")\n\nimport matplotlib.pyplot as plt\n\nimport torch\nfrom torch import nn\nfrom torch import optim\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms\n\nimport helper\nimport fc_model\n\n\n#%%\n# Define a transform to normalize the data\ntransform = transforms.Compose([transforms.ToTensor(),\n                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n# Download and load the training data\ntrainset = datasets.FashionMNIST(\'F_MNIST_data/\', download=True, train=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n\n# Download and load the test data\ntestset = datasets.FashionMNIST(\'F_MNIST_data/\', download=True, train=False, transform=transform)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)\n\n#%% [markdown]\n# Here we can see one of the images.\n\n#%%\nimage, label = next(iter(trainloader))\nhelper.imshow(image[0,:]);\n\n#%% [markdown]\n# # Train a network\n# \n# To make things more concise here, I moved the model architecture and training code from the last part to a file called `fc_model`. Importing this, we can easily create a fully-connected network with `fc_model.Network`, and train the network using `fc_model.train`. I\'ll use this model (once it\'s trained) to demonstrate how we can save and load models.\n\n#%%\n# Create the network, define the criterion and optimizer\nmodel = fc_model.Network(784, 10, [512, 256, 128])\ncriterion = nn.NLLLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n\n#%%\nfc_model.train(model, trainloader, testloader, criterion, optimizer, epochs=2)\n\n#%% [markdown]\n# ## Saving and loading networks\n# \n# As you can imagine, it\'s impractical to train a network every time you need to use it. Instead, we can save trained networks then load them later to train more or use them for predictions.\n# \n# The parameters for PyTorch networks are stored in a model\'s `state_dict`. We can see the state dict contains the weight and bias matrices for each of our layers.\n\n#%%\nprint(""Our model: \\n\\n"", model, \'\\n\')\nprint(""The state dict keys: \\n\\n"", model.state_dict().keys())\n\n#%% [markdown]\n# The simplest thing to do is simply save the state dict with `torch.save`. For example, we can save it to a file `\'checkpoint.pth\'`.\n\n#%%\ntorch.save(model.state_dict(), \'checkpoint.pth\')\n\n#%% [markdown]\n# Then we can load the state dict with `torch.load`.\n\n#%%\nstate_dict = torch.load(\'checkpoint.pth\')\nprint(state_dict.keys())\n\n#%% [markdown]\n# And to load the state dict in to the network, you do `model.load_state_dict(state_dict)`.\n\n#%%\nmodel.load_state_dict(state_dict)\n\n#%% [markdown]\n# Seems pretty straightforward, but as usual it\'s a bit more complicated. Loading the state dict works only if the model architecture is exactly the same as the checkpoint architecture. If I create a model with a different architecture, this fails.\n\n#%%\n# Try this\nmodel = fc_model.Network(784, 10, [400, 200, 100])\n# This will throw an error because the tensor sizes are wrong!\nmodel.load_state_dict(state_dict)\n\n#%% [markdown]\n# This means we need to rebuild the model exactly as it was when trained. Information about the model architecture needs to be saved in the checkpoint, along with the state dict. To do this, you build a dictionary with all the information you need to compeletely rebuild the model.\n\n#%%\ncheckpoint = {\'input_size\': 784,\n              \'output_size\': 10,\n              \'hidden_layers\': [each.out_features for each in model.hidden_layers],\n              \'state_dict\': model.state_dict()}\n\ntorch.save(checkpoint, \'checkpoint.pth\')\n\n#%% [markdown]\n# Now the checkpoint has all the necessary information to rebuild the trained model. You can easily make that a function if you want. Similarly, we can write a function to load checkpoints. \n\n#%%\ndef load_checkpoint(filepath):\n    checkpoint = torch.load(filepath)\n    model = fc_model.Network(checkpoint[\'input_size\'],\n                             checkpoint[\'output_size\'],\n                             checkpoint[\'hidden_layers\'])\n    model.load_state_dict(checkpoint[\'state_dict\'])\n    \n    return model\n\n\n#%%\nmodel = load_checkpoint(\'checkpoint.pth\')\nprint(model)\n\n\n'"
Udacity-Deep-Learning-with-PyTorch/Part 7 - Loading Image Data.py,8,"b'#%% [markdown]\n# # Loading Image Data\n# \n# So far we\'ve been working with fairly artificial datasets that you wouldn\'t typically be using in real projects. Instead, you\'ll likely be dealing with full-sized images like you\'d get from smart phone cameras. In this notebook, we\'ll look at how to load images and use them to train neural networks.\n# \n# We\'ll be using a [dataset of cat and dog photos](https://www.kaggle.com/c/dogs-vs-cats) available from Kaggle. Here are a couple example images:\n# \n# <img src=\'assets/dog_cat.png\'>\n# \n# We\'ll use this dataset to train a neural network that can differentiate between cats and dogs. These days it doesn\'t seem like a big accomplishment, but five years ago it was a serious challenge for computer vision systems.\n\n#%%\nget_ipython().run_line_magic(\'matplotlib\', \'inline\')\nget_ipython().run_line_magic(\'config\', ""InlineBackend.figure_format = \'retina\'"")\n\nimport matplotlib.pyplot as plt\n\nimport torch\nfrom torchvision import datasets, transforms\n\nimport helper\n\n#%% [markdown]\n# The easiest way to load image data is with `datasets.ImageFolder` from `torchvision` ([documentation](http://pytorch.org/docs/master/torchvision/datasets.html#imagefolder)). In general you\'ll use `ImageFolder` like so:\n# \n# ```python\n# dataset = datasets.ImageFolder(\'path/to/data\', transform=transforms)\n# ```\n# \n# where `\'path/to/data\'` is the file path to the data directory and `transforms` is a list of processing steps built with the [`transforms`](http://pytorch.org/docs/master/torchvision/transforms.html) module from `torchvision`. ImageFolder expects the files and directories to be constructed like so:\n# ```\n# root/dog/xxx.png\n# root/dog/xxy.png\n# root/dog/xxz.png\n# \n# root/cat/123.png\n# root/cat/nsdf3.png\n# root/cat/asd932_.png\n# ```\n# \n# where each class has it\'s own directory (`cat` and `dog`) for the images. The images are then labeled with the class taken from the directory name. So here, the image `123.png` would be loaded with the class label `cat`. You can download the dataset already structured like this [from here](https://s3.amazonaws.com/content.udacity-data.com/nd089/Cat_Dog_data.zip). I\'ve also split it into a training set and test set.\n# \n# ### Transforms\n# \n# When you load in the data with `ImageFolder`, you\'ll need to define some transforms. For example, the images are different sizes but we\'ll need them to all be the same size for training. You can either resize them with `transforms.Resize()` or crop with `transforms.CenterCrop()`, `transforms.RandomResizedCrop()`, etc. We\'ll also need to convert the images to PyTorch tensors with `transforms.ToTensor()`. Typically you\'ll combine these transforms into a pipeline with `transforms.Compose()`, which accepts a list of transforms and runs them in sequence. It looks something like this to scale, then crop, then convert to a tensor:\n# \n# ```python\n# transforms = transforms.Compose([transforms.Resize(255),\n#                                  transforms.CenterCrop(224),\n#                                  transforms.ToTensor()])\n# \n# ```\n# \n# There are plenty of transforms available, I\'ll cover more in a bit and you can read through the [documentation](http://pytorch.org/docs/master/torchvision/transforms.html). \n# \n# ### Data Loaders\n# \n# With the `ImageFolder` loaded, you have to pass it to a [`DataLoader`](http://pytorch.org/docs/master/data.html#torch.utils.data.DataLoader). The `DataLoader` takes a dataset (such as you would get from `ImageFolder`) and returns batches of images and the corresponding labels. You can set various parameters like the batch size and if the data is shuffled after each epoch.\n# \n# ```python\n# dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n# ```\n# \n# Here `dataloader` is a [generator](https://jeffknupp.com/blog/2013/04/07/improve-your-python-yield-and-generators-explained/). To get data out of it, you need to loop through it or convert it to an iterator and call `next()`.\n# \n# ```python\n# # Looping through it, get a batch on each loop \n# for images, labels in dataloader:\n#     pass\n# \n# # Get one batch\n# images, labels = next(iter(dataloader))\n# ```\n#  \n# >**Exercise:** Load images from the `Cat_Dog_data/train` folder, define a few transforms, then build the dataloader.\n\n#%%\ndata_dir = \'Cat_Dog_data/train\'\n\ntransforms = # TODO: compose transforms here\ndataset = # TODO: create the ImageFolder\ndataloader = # TODO: use the ImageFolder dataset to create the DataLoader\n\n\n#%%\n# Run this to test your data loader\nimages, labels = next(iter(dataloader))\nhelper.imshow(images[0], normalize=False)\n\n#%% [markdown]\n# If you loaded the data correctly, you should see something like this (your image will be different):\n# \n# <img src=\'assets/cat_cropped.png\', width=244>\n#%% [markdown]\n# ## Data Augmentation\n# \n# A common strategy for training neural networks is to introduce randomness in the input data itself. For example, you can randomly rotate, mirror, scale, and/or crop your images during training. This will help your network generalize as it\'s seeing the same images but in different locations, with different sizes, in different orientations, etc.\n# \n# To randomly rotate, scale and crop, then flip your images you would define your transforms like this:\n# \n# ```python\n# train_transforms = transforms.Compose([transforms.RandomRotation(30),\n#                                        transforms.RandomResizedCrop(100),\n#                                        transforms.RandomHorizontalFlip(),\n#                                        transforms.ToTensor(),\n#                                        transforms.Normalize([0.5, 0.5, 0.5], \n#                                                             [0.5, 0.5, 0.5])])\n# ```\n# \n# You\'ll also typically want to normalize images with `transforms.Normalize`. You pass in a list of means and list of standard deviations, then the color channels are normalized like so\n# \n# ```input[channel] = (input[channel] - mean[channel]) / std[channel]```\n# \n# Subtracting `mean` centers the data around zero and dividing by `std` squishes the values to be between -1 and 1. Normalizing helps keep the network work weights near zero which in turn makes backpropagation more stable. Without normalization, networks will tend to fail to learn.\n# \n# You can find a list of all [the available transforms here](http://pytorch.org/docs/0.3.0/torchvision/transforms.html). When you\'re testing however, you\'ll want to use images that aren\'t altered (except you\'ll need to normalize the same way). So, for validation/test images, you\'ll typically just resize and crop.\n# \n# >**Exercise:** Define transforms for training data and testing data below.\n\n#%%\ndata_dir = \'Cat_Dog_data\'\n\n# TODO: Define transforms for the training data and testing data\ntrain_transforms = \n\ntest_transforms = \n\n\n# Pass transforms in here, then run the next cell to see how the transforms look\ntrain_data = datasets.ImageFolder(data_dir + \'/train\', transform=train_transforms)\ntest_data = datasets.ImageFolder(data_dir + \'/test\', transform=test_transforms)\n\ntrainloader = torch.utils.data.DataLoader(train_data, batch_size=32)\ntestloader = torch.utils.data.DataLoader(test_data, batch_size=32)\n\n\n#%%\n# change this to the trainloader or testloader \ndata_iter = iter(testloader)\n\nimages, labels = next(data_iter)\nfig, axes = plt.subplots(figsize=(10,4), ncols=4)\nfor ii in range(4):\n    ax = axes[ii]\n    helper.imshow(images[ii], ax=ax)\n\n#%% [markdown]\n# Your transformed images should look something like this.\n# \n# <center>Training examples:</center>\n# <img src=\'assets/train_examples.png\' width=500px>\n# \n# <center>Testing examples:</center>\n# <img src=\'assets/test_examples.png\' width=500px>\n#%% [markdown]\n# At this point you should be able to load data for training and testing. Now, you should try building a network that can classify cats vs dogs. This is quite a bit more complicated than before with the MNIST and Fashion-MNIST datasets. To be honest, you probably won\'t get it to work with a fully-connected network, no matter how deep. These images have three color channels and at a higher resolution (so far you\'ve seen 28x28 images which are tiny).\n# \n# In the next part, I\'ll show you how to use a pre-trained network to build a model that can actually solve this problem.\n\n#%%\n# Optional TODO: Attempt to build a network to classify cats vs dogs from this dataset\n\n\n'"
Udacity-Deep-Learning-with-PyTorch/Part 8 - Transfer Learning.py,6,"b'#%% [markdown]\n# # Transfer Learning\n# \n# In this notebook, you\'ll learn how to use pre-trained networks to solved challenging problems in computer vision. Specifically, you\'ll use networks trained on [ImageNet](http://www.image-net.org/) [available from torchvision](http://pytorch.org/docs/0.3.0/torchvision/models.html). \n# \n# ImageNet is a massive dataset with over 1 million labeled images in 1000 categories. It\'s used to train deep neural networks using an architecture called convolutional layers. I\'m not going to get into the details of convolutional networks here, but if you want to learn more about them, please [watch this](https://www.youtube.com/watch?v=2-Ol7ZB0MmU).\n# \n# Once trained, these models work astonishingly well as feature detectors for images they weren\'t trained on. Using a pre-trained network on images not in the training set is called transfer learning. Here we\'ll use transfer learning to train a network that can classify our cat and dog photos with near perfect accuracy.\n# \n# With `torchvision.models` you can download these pre-trained networks and use them in your applications. We\'ll include `models` in our imports now.\n\n#%%\nget_ipython().run_line_magic(\'matplotlib\', \'inline\')\nget_ipython().run_line_magic(\'config\', ""InlineBackend.figure_format = \'retina\'"")\n\nimport matplotlib.pyplot as plt\n\nimport torch\nfrom torch import nn\nfrom torch import optim\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms, models\n\n#%% [markdown]\n# Most of the pretrained models require the input to be 224x224 images. Also, we\'ll need to match the normalization used when the models were trained. Each color channel was normalized separately, the means are `[0.485, 0.456, 0.406]` and the standard deviations are `[0.229, 0.224, 0.225]`.\n\n#%%\ndata_dir = \'Cat_Dog_data\'\n\n# TODO: Define transforms for the training data and testing data\n\n\n# Pass transforms in here, then run the next cell to see how the transforms look\ntrain_data = datasets.ImageFolder(data_dir + \'/train\', transform=train_transforms)\ntest_data = datasets.ImageFolder(data_dir + \'/test\', transform=test_transforms)\n\ntrainloader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)\ntestloader = torch.utils.data.DataLoader(test_data, batch_size=32)\n\n#%% [markdown]\n# We can load in a model such as [DenseNet](http://pytorch.org/docs/0.3.0/torchvision/models.html#id5). Let\'s print out the model architecture so we can see what\'s going on.\n\n#%%\nmodel = models.densenet121(pretrained=True)\nmodel\n\n#%% [markdown]\n# This model is built out of two main parts, the features and the classifier. The features part is a stack of convolutional layers and overall works as a feature detector that can be fed into a classifier. The classifier part is a single fully-connected layer `(classifier): Linear(in_features=1024, out_features=1000)`. This layer was trained on the ImageNet dataset, so it won\'t work for our specific problem. That means we need to replace the classifier, but the features will work perfectly on their own. In general, I think about pre-trained networks as amazingly good feature detectors that can be used as the input for simple feed-forward classifiers.\n\n#%%\n# Freeze parameters so we don\'t backprop through them\nfor param in model.parameters():\n    param.requires_grad = False\n\nfrom collections import OrderedDict\nclassifier = nn.Sequential(OrderedDict([\n                          (\'fc1\', nn.Linear(1024, 500)),\n                          (\'relu\', nn.ReLU()),\n                          (\'fc2\', nn.Linear(500, 2)),\n                          (\'output\', nn.LogSoftmax(dim=1))\n                          ]))\n    \nmodel.classifier = classifier\n\n#%% [markdown]\n# With our model built, we need to train the classifier. However, now we\'re using a **really deep** neural network. If you try to train this on a CPU like normal, it will take a long, long time. Instead, we\'re going to use the GPU to do the calculations. The linear algebra computations are done in parallel on the GPU leading to 100x increased training speeds. It\'s also possible to train on multiple GPUs, further decreasing training time.\n# \n# PyTorch, along with pretty much every other deep learning framework, uses [CUDA](https://developer.nvidia.com/cuda-zone) to efficiently compute the forward and backwards passes on the GPU. In PyTorch, you move your model parameters and other tensors to the GPU memory using `model.to(\'cuda\')`. You can move them back from the GPU with `model.to(\'cpu\')` which you\'ll commonly do when you need to operate on the network output outside of PyTorch. As a demonstration of the increased speed, I\'ll compare how long it takes to perform a forward and backward pass with and without a GPU.\n\n#%%\nimport time\n\n\n#%%\nfor device in [\'cpu\', \'cuda\']:\n\n    criterion = nn.NLLLoss()\n    # Only train the classifier parameters, feature parameters are frozen\n    optimizer = optim.Adam(model.classifier.parameters(), lr=0.001)\n\n    model.to(device)\n\n    for ii, (inputs, labels) in enumerate(trainloader):\n\n        # Move input and label tensors to the GPU\n        inputs, labels = inputs.to(device), labels.to(device)\n\n        start = time.time()\n\n        outputs = model.forward(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        if ii==3:\n            break\n        \n    print(f""Device = {device}; Time per batch: {(time.time() - start)/3:.3f} seconds"")\n\n#%% [markdown]\n# You can write device agnostic code which will automatically use CUDA if it\'s enabled like so:\n# ```python\n# # at beginning of the script\n# device = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")\n# \n# ...\n# \n# # then whenever you get a new Tensor or Module\n# # this won\'t copy if they are already on the desired device\n# input = data.to(device)\n# model = MyModule(...).to(device)\n# ```\n# \n# From here, I\'ll let you finish training the model. The process is the same as before except now your model is much more powerful. You should get better than 95% accuracy easily.\n# \n# >**Exercise:** Train a pretrained models to classify the cat and dog images. Continue with the DenseNet model, or try ResNet, it\'s also a good model to try out first. Make sure you are only training the classifier and the parameters for the features part are frozen.\n\n#%%\n# TODO: Train a model with a pre-trained network\n\n\n'"
Udacity-Deep-Learning-with-PyTorch/fc_model.py,4,"b'import torch\nfrom torch import nn\nimport torch.nn.functional as F\n\n\nclass Network(nn.Module):\n    def __init__(self, input_size, output_size, hidden_layers, drop_p=0.5):\n        \'\'\' Builds a feedforward network with arbitrary hidden layers.\n        \n            Arguments\n            ---------\n            input_size: integer, size of the input layer\n            output_size: integer, size of the output layer\n            hidden_layers: list of integers, the sizes of the hidden layers\n        \n        \'\'\'\n        super().__init__()\n        # Input to a hidden layer\n        self.hidden_layers = nn.ModuleList([nn.Linear(input_size, hidden_layers[0])])\n        \n        # Add a variable number of more hidden layers\n        layer_sizes = zip(hidden_layers[:-1], hidden_layers[1:])\n        self.hidden_layers.extend([nn.Linear(h1, h2) for h1, h2 in layer_sizes])\n        \n        self.output = nn.Linear(hidden_layers[-1], output_size)\n        \n        self.dropout = nn.Dropout(p=drop_p)\n        \n    def forward(self, x):\n        \'\'\' Forward pass through the network, returns the output logits \'\'\'\n        \n        for each in self.hidden_layers:\n            x = F.relu(each(x))\n            x = self.dropout(x)\n        x = self.output(x)\n        \n        return F.log_softmax(x, dim=1)\n\n\ndef validation(model, testloader, criterion):\n    accuracy = 0\n    test_loss = 0\n    for images, labels in testloader:\n\n        images = images.resize_(images.size()[0], 784)\n\n        output = model.forward(images)\n        test_loss += criterion(output, labels).item()\n\n        ## Calculating the accuracy \n        # Model\'s output is log-softmax, take exponential to get the probabilities\n        ps = torch.exp(output)\n        # Class with highest probability is our predicted class, compare with true label\n        equality = (labels.data == ps.max(1)[1])\n        # Accuracy is number of correct predictions divided by all predictions, just take the mean\n        accuracy += equality.type_as(torch.FloatTensor()).mean()\n\n    return test_loss, accuracy\n\n\ndef train(model, trainloader, testloader, criterion, optimizer, epochs=5, print_every=40):\n    \n    steps = 0\n    running_loss = 0\n    for e in range(epochs):\n        # Model in training mode, dropout is on\n        model.train()\n        for images, labels in trainloader:\n            steps += 1\n            \n            # Flatten images into a 784 long vector\n            images.resize_(images.size()[0], 784)\n            \n            optimizer.zero_grad()\n            \n            output = model.forward(images)\n            loss = criterion(output, labels)\n            loss.backward()\n            optimizer.step()\n            \n            running_loss += loss.item()\n\n            if steps % print_every == 0:\n                # Model in inference mode, dropout is off\n                model.eval()\n                \n                # Turn off gradients for validation, will speed up inference\n                with torch.no_grad():\n                    test_loss, accuracy = validation(model, testloader, criterion)\n                \n                print(""Epoch: {}/{}.. "".format(e+1, epochs),\n                      ""Training Loss: {:.3f}.. "".format(running_loss/print_every),\n                      ""Test Loss: {:.3f}.. "".format(test_loss/len(testloader)),\n                      ""Test Accuracy: {:.3f}"".format(accuracy/len(testloader)))\n                \n                running_loss = 0\n                \n                # Make sure dropout and grads are on for training\n                model.train()'"
Udacity-Deep-Learning-with-PyTorch/helper.py,1,"b'import matplotlib.pyplot as plt\nimport numpy as np\nfrom torch import nn, optim\nfrom torch.autograd import Variable\n\n\ndef test_network(net, trainloader):\n\n    criterion = nn.MSELoss()\n    optimizer = optim.Adam(net.parameters(), lr=0.001)\n\n    dataiter = iter(trainloader)\n    images, labels = dataiter.next()\n\n    # Create Variables for the inputs and targets\n    inputs = Variable(images)\n    targets = Variable(images)\n\n    # Clear the gradients from all Variables\n    optimizer.zero_grad()\n\n    # Forward pass, then backward pass, then update weights\n    output = net.forward(inputs)\n    loss = criterion(output, targets)\n    loss.backward()\n    optimizer.step()\n\n    return True\n\n\ndef imshow(image, ax=None, title=None, normalize=True):\n    """"""Imshow for Tensor.""""""\n    if ax is None:\n        fig, ax = plt.subplots()\n    image = image.numpy().transpose((1, 2, 0))\n\n    if normalize:\n        mean = np.array([0.485, 0.456, 0.406])\n        std = np.array([0.229, 0.224, 0.225])\n        image = std * image + mean\n        image = np.clip(image, 0, 1)\n\n    ax.imshow(image)\n    ax.spines[\'top\'].set_visible(False)\n    ax.spines[\'right\'].set_visible(False)\n    ax.spines[\'left\'].set_visible(False)\n    ax.spines[\'bottom\'].set_visible(False)\n    ax.tick_params(axis=\'both\', length=0)\n    ax.set_xticklabels(\'\')\n    ax.set_yticklabels(\'\')\n\n    return ax\n\n\ndef view_recon(img, recon):\n    \'\'\' Function for displaying an image (as a PyTorch Tensor) and its\n        reconstruction also a PyTorch Tensor\n    \'\'\'\n\n    fig, axes = plt.subplots(ncols=2, sharex=True, sharey=True)\n    axes[0].imshow(img.numpy().squeeze())\n    axes[1].imshow(recon.data.numpy().squeeze())\n    for ax in axes:\n        ax.axis(\'off\')\n        ax.set_adjustable(\'box-forced\')\n\ndef view_classify(img, ps, version=""MNIST""):\n    \'\'\' Function for viewing an image and it\'s predicted classes.\n    \'\'\'\n    ps = ps.data.numpy().squeeze()\n\n    fig, (ax1, ax2) = plt.subplots(figsize=(6,9), ncols=2)\n    ax1.imshow(img.resize_(1, 28, 28).numpy().squeeze())\n    ax1.axis(\'off\')\n    ax2.barh(np.arange(10), ps)\n    ax2.set_aspect(0.1)\n    ax2.set_yticks(np.arange(10))\n    if version == ""MNIST"":\n        ax2.set_yticklabels(np.arange(10))\n    elif version == ""Fashion"":\n        ax2.set_yticklabels([\'T-shirt/top\',\n                            \'Trouser\',\n                            \'Pullover\',\n                            \'Dress\',\n                            \'Coat\',\n                            \'Sandal\',\n                            \'Shirt\',\n                            \'Sneaker\',\n                            \'Bag\',\n                            \'Ankle Boot\'], size=\'small\');\n    ax2.set_title(\'Class Probability\')\n    ax2.set_xlim(0, 1.1)\n\n    plt.tight_layout()\n'"
6.Neural_Network 神经网络/backpropagation/mnist.py,0,"b'import os\nimport gzip\nimport numpy as np\n\nDATA_URL = \'http://yann.lecun.com/exdb/mnist/\'\n\n# Download and import the MNIST dataset from Yann LeCun\'s website.\n# Reserve 10,000 examples from the training set for validation.\n# Each image is an array of 784 (28x28) float values  from 0 (white) to 1 (black).\ndef load_data(one_hot=True, reshape=None, validation_size=10000):\n    x_tr = load_images(\'train-images-idx3-ubyte.gz\')\n    y_tr = load_labels(\'train-labels-idx1-ubyte.gz\')\n    x_te = load_images(\'t10k-images-idx3-ubyte.gz\')\n    y_te = load_labels(\'t10k-labels-idx1-ubyte.gz\')\n\n    x_tr = x_tr[:-validation_size]\n    y_tr = y_tr[:-validation_size]\n\n    if one_hot:\n        y_tr, y_te = [to_one_hot(y) for y in (y_tr, y_te)]\n\n    if reshape:\n        x_tr, x_te = [x.reshape(-1, *reshape) for x in (x_tr, x_te)]\n\n    y_tr, y_te = [y.reshape(-1, 1) for y in (y_tr, y_te)]\n\n    return x_tr, y_tr, x_te, y_te\n\ndef load_images(filename):\n    maybe_download(filename)\n    with gzip.open(filename, \'rb\') as f:\n        data = np.frombuffer(f.read(), np.uint8, offset=16)\n    return data.reshape(-1, 28 * 28) / np.float32(256)\n\ndef load_labels(filename):\n    maybe_download(filename)\n    with gzip.open(filename, \'rb\') as f:\n        data = np.frombuffer(f.read(), np.uint8, offset=8)\n    return data\n\n# Download the file, unless it\'s already here.\ndef maybe_download(filename):\n    if not os.path.exists(filename):\n        from urllib.request import urlretrieve\n        print(""Downloading %s"" % filename)\n        urlretrieve(DATA_URL + filename, filename)\n\n# Convert class labels from scalars to one-hot vectors.\ndef to_one_hot(labels, num_classes=10):\n    return np.eye(num_classes)[labels]'"
6.Neural_Network 神经网络/backpropagation/mnist_backprop_np.py,0,"b'import  random\nimport  numpy as np\nimport  mnist\n\n\n\ndef sigmoid(z):\n    """"""\n    The sigmoid function.\n     [30/10, 1]\n    """"""\n    return 1.0/(1.0+np.exp(-z))\n\ndef sigmoid_prime(z):\n    """"""Derivative of the sigmoid function.""""""\n    return sigmoid(z)*(1-sigmoid(z))\n\n\nclass Network:\n\n    def __init__(self, sizes):\n        """"""\n        The list ``sizes`` contains the number of neurons in the\n        respective layers of the network.  For example, if the list\n        was [2, 3, 1] then it would be a three-layer network, with the\n        first layer containing 2 neurons, the second layer 3 neurons,\n        and the third layer 1 neuron.  The biases and weights for the\n        network are initialized randomly, using a Gaussian\n        distribution with mean 0, and variance 1.  Note that the first\n        layer is assumed to be an input layer, and by convention we\n        won\'t set any biases for those neurons, since biases are only\n        ever used in computing the outputs from later layers.\n        :param sizes: [784, 100, 10]\n        """"""\n        self.num_layers = len(sizes)\n        self.sizes = sizes\n        # [ch_out, 1]\n        self.biases = [np.random.randn(ch_out, 1) for ch_out in sizes[1:]]\n        # [ch_out, ch_in]\n        self.weights = [np.random.randn(ch_out, ch_in)\n                            for ch_in, ch_out in zip(sizes[:-1], sizes[1:])]\n\n    def forward(self, x):\n        """"""\n\n        :param x: [784, 1]\n        :return: [30, 1]\n        """"""\n\n        for b, w in zip(self.biases, self.weights):\n            # [30, 784]@[784, 1] + [30, 1]=> [30, 1]\n            # [10, 30]@[30, 1] + [10, 1]=> [10, 1]\n            x = sigmoid(np.dot(w, x)+b)\n        return x\n\n    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None):\n        """"""\n        Train the neural network using mini-batch stochastic\n        gradient descent.  The ``training_data`` is a list of tuples\n        ``(x, y)`` representing the training inputs and the desired\n        outputs.  The other non-optional parameters are\n        self-explanatory.  If ``test_data`` is provided then the\n        network will be evaluated against the test data after each\n        epoch, and partial progress printed out.  This is useful for\n        tracking progress, but slows things down substantially.\n        """"""\n        if test_data:\n            n_test = len(test_data)\n\n        n = len(training_data)\n        for j in range(epochs):\n            random.shuffle(training_data)\n\n            mini_batches = [\n                training_data[k:k+mini_batch_size]\n                for k in range(0, n, mini_batch_size)]\n\n            # for every (x,y)\n            for mini_batch in mini_batches:\n                loss = self.update_mini_batch(mini_batch, eta)\n            if test_data:\n                print(""Epoch {0}: {1} / {2}, Loss: {3}"".format(\n                    j, self.evaluate(test_data), n_test, loss))\n            else:\n                print(""Epoch {0} complete"".format(j))\n\n    def update_mini_batch(self, mini_batch, eta):\n        """"""\n        Update the network\'s weights and biases by applying\n        gradient descent using backpropagation to a single mini batch.\n        The ``mini_batch`` is a list of tuples ``(x, y)``, and ``eta``\n        is the learning rate.\n        """"""\n        # https://en.wikipedia.org/wiki/Del\n        nabla_b = [np.zeros(b.shape) for b in self.biases]\n        nabla_w = [np.zeros(w.shape) for w in self.weights]\n        loss = 0\n\n        for x, y in mini_batch:\n            delta_nabla_b, delta_nabla_w, loss_ = self.backprop(x, y)\n            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n            loss += loss_\n\n        # tmp1 = [np.linalg.norm(b/len(mini_batch)) for b in nabla_b]\n        # tmp2 = [np.linalg.norm(w/len(mini_batch)) for w in nabla_w]\n        # print(tmp1)\n        # print(tmp2)\n\n        self.weights = [w-(eta/len(mini_batch))*nw\n                        for w, nw in zip(self.weights, nabla_w)]\n        self.biases = [b-(eta/len(mini_batch))*nb\n                       for b, nb in zip(self.biases, nabla_b)]\n        loss = loss / len(mini_batch)\n\n        return loss\n\n    def backprop(self, x, y):\n        """"""\n        Return a tuple ``(nabla_b, nabla_w)`` representing the\n        gradient for the cost function C_x.  ``nabla_b`` and\n        ``nabla_w`` are layer-by-layer lists of numpy arrays, similar\n        to ``self.biases`` and ``self.weights``.\n        """"""\n        nabla_b = [np.zeros(b.shape) for b in self.biases]\n        nabla_w = [np.zeros(w.shape) for w in self.weights]\n\n        # 1. forward\n        activation = x\n        # w*x = z => sigmoid => x/activation\n        zs = [] # list to store all the z vectors, layer by layer\n        activations = [x] # list to store all the activations, layer by layer\n        for b, w in zip(self.biases, self.weights):\n            # https://stackoverflow.com/questions/34142485/difference-between-numpy-dot-and-python-3-5-matrix-multiplication\n            # np.dot vs np.matmul = @ vs element-wise *\n            z = np.dot(w, activation)\n            z = z + b # [256, 784] matmul [784] => [256]\n            # [256] => [256, 1]\n            zs.append(z)\n            activation = sigmoid(z)\n            activations.append(activation)\n\n        loss = np.power(activations[-1]-y, 2).sum()\n\n        # 2. backward\n        # (Ok-tk)*(1-Ok)*Ok\n        # [10] - [10] * [10]\n        delta = self.cost_prime(activations[-1], y) * sigmoid_prime(zs[-1]) # sigmoid(z)*(1-sigmoid(z))\n        # O_j*Delta_k\n        # [10]\n        nabla_b[-1] = delta\n        # deltaj * Oi\n        # [10] @ [30, 1]^T => [10, 30]\n        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n        # Note that the variable l in the loop below is used a little\n        # differently to the notation in Chapter 2 of the book.  Here,\n        # l = 1 means the last layer of neurons, l = 2 is the\n        # second-last layer, and so on.  It\'s a renumbering of the\n        # scheme in the book, used here to take advantage of the fact\n        # that Python can use negative indices in lists.\n        for l in range(2, self.num_layers):\n            # [30, 1]\n            z = zs[-l]\n            sp = sigmoid_prime(z)\n            # sum()\n            # [10, 30] => [30, 10] @ [10, 1] => [30, 1] * [30, 1]\n            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n            nabla_b[-l] = delta\n            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n\n        return nabla_b, nabla_w, loss\n\n    def evaluate(self, test_data):\n        """"""\n        Return the number of test inputs for which the neural\n        network outputs the correct result. Note that the neural\n        network\'s output is assumed to be the index of whichever\n        neuron in the final layer has the highest activation.\n        """"""\n        test_results = [(np.argmax(self.forward(x)), y)\n                        for (x, y) in test_data]\n        return sum(int(x == y) for (x, y) in test_results)\n\n    def cost_prime(self, output_activations, y):\n        """"""\n        Return the vector of partial derivatives \\partial C_x /\n        \\partial a for the output activations.\n        """"""\n        return output_activations-y\n\n\n\n\ndef main():\n\n    x_train, y_train, x_test, y_test = mnist.load_data(reshape=[784,1])\n    # (50000, 784) (50000, 10) (10000, 784) (10000, 10)\n    print(\'x_train, y_train, x_test, y_test:\', x_train.shape, y_train.shape, x_test.shape, y_test.shape)\n\n    np.random.seed(66)\n\n    model = Network([784, 30, 10])\n    data_train = list(zip(x_train, y_train))\n    data_test = list(zip(x_test, y_test))\n    model.SGD(data_train, 10000, 10, 0.1, data_test)\n\n\n\nif __name__ == \'__main__\':\n    main()'"
6.Neural_Network 神经网络/backpropagation/mnist_loader.py,0,"b'""""""\nmnist_loader\n~~~~~~~~~~~~\n\nA library to load the MNIST image data.  For details of the data\nstructures that are returned, see the doc strings for ``load_data``\nand ``load_data_wrapper``.  In practice, ``load_data_wrapper`` is the\nfunction usually called by our neural network code.\n""""""\n\n#### Libraries\n# Standard library\nimport pickle\nimport gzip\n\n# Third-party libraries\nimport numpy as np\n\ndef load_data():\n    """"""Return the MNIST data as a tuple containing the training data,\n    the validation data, and the test data.\n\n    The ``training_data`` is returned as a tuple with two entries.\n    The first entry contains the actual training images.  This is a\n    numpy ndarray with 50,000 entries.  Each entry is, in turn, a\n    numpy ndarray with 784 values, representing the 28 * 28 = 784\n    pixels in a single MNIST image.\n\n    The second entry in the ``training_data`` tuple is a numpy ndarray\n    containing 50,000 entries.  Those entries are just the digit\n    values (0...9) for the corresponding images contained in the first\n    entry of the tuple.\n\n    The ``validation_data`` and ``test_data`` are similar, except\n    each contains only 10,000 images.\n\n    This is a nice data format, but for use in neural networks it\'s\n    helpful to modify the format of the ``training_data`` a little.\n    That\'s done in the wrapper function ``load_data_wrapper()``, see\n    below.\n    """"""\n    f = gzip.open(\'data/mnist.pkl.gz\', \'rb\')\n    training_data, validation_data, test_data = pickle.load(f, encoding=\'latin1\')\n    f.close()\n    return (training_data, validation_data, test_data)\n\ndef load_data_wrapper():\n    """"""Return a tuple containing ``(training_data, validation_data,\n    test_data)``. Based on ``load_data``, but the format is more\n    convenient for use in our implementation of neural networks.\n\n    In particular, ``training_data`` is a list containing 50,000\n    2-tuples ``(x, y)``.  ``x`` is a 784-dimensional numpy.ndarray\n    containing the input image.  ``y`` is a 10-dimensional\n    numpy.ndarray representing the unit vector corresponding to the\n    correct digit for ``x``.\n\n    ``validation_data`` and ``test_data`` are lists containing 10,000\n    2-tuples ``(x, y)``.  In each case, ``x`` is a 784-dimensional\n    numpy.ndarry containing the input image, and ``y`` is the\n    corresponding classification, i.e., the digit values (integers)\n    corresponding to ``x``.\n\n    Obviously, this means we\'re using slightly different formats for\n    the training data and the validation / test data.  These formats\n    turn out to be the most convenient for use in our neural network\n    code.""""""\n    tr_d, va_d, te_d = load_data()\n    training_inputs = [np.reshape(x, (784, 1)) for x in tr_d[0]]\n    training_results = [vectorized_result(y) for y in tr_d[1]]\n    training_data = list(zip(training_inputs, training_results))\n    validation_inputs = [np.reshape(x, (784, 1)) for x in va_d[0]]\n    validation_data = list(zip(validation_inputs, va_d[1]))\n    test_inputs = [np.reshape(x, (784, 1)) for x in te_d[0]]\n    test_data = list(zip(test_inputs, te_d[1]))\n    return (training_data, validation_data, test_data)\n\ndef vectorized_result(j):\n    """"""Return a 10-dimensional unit vector with a 1.0 in the jth\n    position and zeroes elsewhere.  This is used to convert a digit\n    (0...9) into a corresponding desired output from the neural\n    network.""""""\n    e = np.zeros((10, 1))\n    e[j] = 1.0\n    return e\n'"
6.Neural_Network 神经网络/backpropagation/network.py,0,"b'""""""\nnetwork.py\n~~~~~~~~~~\n\nA module to implement the stochastic gradient descent learning\nalgorithm for a feedforward neural network.  Gradients are calculated\nusing backpropagation.  Note that I have focused on making the code\nsimple, easily readable, and easily modifiable.  It is not optimized,\nand omits many desirable features.\n""""""\n\n#### Libraries\n# Standard library\nimport random\n\n# Third-party libraries\nimport numpy as np\n\nclass Network(object):\n\n    def __init__(self, sizes):\n        """"""The list ``sizes`` contains the number of neurons in the\n        respective layers of the network.  For example, if the list\n        was [2, 3, 1] then it would be a three-layer network, with the\n        first layer containing 2 neurons, the second layer 3 neurons,\n        and the third layer 1 neuron.  The biases and weights for the\n        network are initialized randomly, using a Gaussian\n        distribution with mean 0, and variance 1.  Note that the first\n        layer is assumed to be an input layer, and by convention we\n        won\'t set any biases for those neurons, since biases are only\n        ever used in computing the outputs from later layers.""""""\n        self.num_layers = len(sizes)\n        self.sizes = sizes\n        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n        self.weights = [np.random.randn(y, x)\n                        for x, y in zip(sizes[:-1], sizes[1:])]\n\n    def feedforward(self, a):\n        """"""Return the output of the network if ``a`` is input.""""""\n        for b, w in zip(self.biases, self.weights):\n            a = sigmoid(np.dot(w, a)+b)\n        return a\n\n    def SGD(self, training_data, epochs, mini_batch_size, eta,\n            test_data=None):\n        """"""Train the neural network using mini-batch stochastic\n        gradient descent.  The ``training_data`` is a list of tuples\n        ``(x, y)`` representing the training inputs and the desired\n        outputs.  The other non-optional parameters are\n        self-explanatory.  If ``test_data`` is provided then the\n        network will be evaluated against the test data after each\n        epoch, and partial progress printed out.  This is useful for\n        tracking progress, but slows things down substantially.""""""\n        if test_data: n_test = len(test_data)\n        n = len(training_data)\n        for j in range(epochs):\n            random.shuffle(training_data)\n            mini_batches = [\n                training_data[k:k+mini_batch_size]\n                for k in range(0, n, mini_batch_size)]\n            for mini_batch in mini_batches:\n                self.update_mini_batch(mini_batch, eta)\n            if test_data:\n                print(""Epoch {0}: {1} / {2}"".format(\n                    j, self.evaluate(test_data), n_test) )\n            else:\n                print(""Epoch {0} complete"".format(j) )\n\n    def update_mini_batch(self, mini_batch, eta):\n        """"""Update the network\'s weights and biases by applying\n        gradient descent using backpropagation to a single mini batch.\n        The ``mini_batch`` is a list of tuples ``(x, y)``, and ``eta``\n        is the learning rate.""""""\n        nabla_b = [np.zeros(b.shape) for b in self.biases]\n        nabla_w = [np.zeros(w.shape) for w in self.weights]\n        for x, y in mini_batch:\n            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n        self.weights = [w-(eta/len(mini_batch))*nw\n                        for w, nw in zip(self.weights, nabla_w)]\n        self.biases = [b-(eta/len(mini_batch))*nb\n                       for b, nb in zip(self.biases, nabla_b)]\n\n    def backprop(self, x, y):\n        """"""Return a tuple ``(nabla_b, nabla_w)`` representing the\n        gradient for the cost function C_x.  ``nabla_b`` and\n        ``nabla_w`` are layer-by-layer lists of numpy arrays, similar\n        to ``self.biases`` and ``self.weights``.""""""\n        nabla_b = [np.zeros(b.shape) for b in self.biases]\n        nabla_w = [np.zeros(w.shape) for w in self.weights]\n        # feedforward\n        activation = x\n        activations = [x] # list to store all the activations, layer by layer\n        zs = [] # list to store all the z vectors, layer by layer\n        for b, w in zip(self.biases, self.weights):\n            z = np.dot(w, activation)+b\n            zs.append(z)\n            activation = sigmoid(z)\n            activations.append(activation)\n        # backward pass\n        delta = self.cost_derivative(activations[-1], y) * \\\n            sigmoid_prime(zs[-1])\n        nabla_b[-1] = delta\n        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n        # Note that the variable l in the loop below is used a little\n        # differently to the notation in Chapter 2 of the book.  Here,\n        # l = 1 means the last layer of neurons, l = 2 is the\n        # second-last layer, and so on.  It\'s a renumbering of the\n        # scheme in the book, used here to take advantage of the fact\n        # that Python can use negative indices in lists.\n        for l in range(2, self.num_layers):\n            z = zs[-l]\n            sp = sigmoid_prime(z)\n            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n            nabla_b[-l] = delta\n            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n        return (nabla_b, nabla_w)\n\n    def evaluate(self, test_data):\n        """"""Return the number of test inputs for which the neural\n        network outputs the correct result. Note that the neural\n        network\'s output is assumed to be the index of whichever\n        neuron in the final layer has the highest activation.""""""\n        test_results = [(np.argmax(self.feedforward(x)), y)\n                        for (x, y) in test_data]\n        return sum(int(x == y) for (x, y) in test_results)\n\n    def cost_derivative(self, output_activations, y):\n        """"""Return the vector of partial derivatives \\partial C_x /\n        \\partial a for the output activations.""""""\n        return (output_activations-y)\n\n#### Miscellaneous functions\ndef sigmoid(z):\n    """"""The sigmoid function.""""""\n    return 1.0/(1.0+np.exp(-z))\n\ndef sigmoid_prime(z):\n    """"""Derivative of the sigmoid function.""""""\n    return sigmoid(z)*(1-sigmoid(z))\n\n\n\ndef main():\n    import mnist_loader\n    # Loading the MNIST data\n    training_data, validation_data, test_data = mnist_loader.load_data_wrapper()\n    # Set up a Network with 30 hidden neurons\n    net = Network([784, 30, 10])\n    # Use stochastic gradient descent to learn from the MNIST training_data over\n    # 30 epochs, with a mini-batch size of 10, and a learning rate of \xce\xb7 = 3.0\n    net.SGD(training_data, 30, 10, 3.0, test_data=test_data)\n\nif __name__ == \'__main__\':\n    main()'"
6.Neural_Network 神经网络/bike_prediction/单车预测_PyTorchNN.py,54,"b'#%% [markdown]\n# #  ""\xe6\x91\xa9\xe6\x8b\x9c""\xe9\x9c\x80\xe8\xa6\x81\xe6\x88\x91\n# \n# \xe6\x88\x91\xe4\xbb\xac\xe5\xb0\x86\xe8\xae\xbe\xe8\xae\xa1\xe4\xba\xba\xe5\xb7\xa5\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xe5\xaf\xb9\xe6\x9f\x90\xe5\x9c\xb0\xe5\x8c\xba\xe7\xa7\x9f\xe8\xb5\x81\xe5\x8d\x95\xe8\xbd\xa6\xe7\x9a\x84\xe4\xbd\xbf\xe7\x94\xa8\xe6\x83\x85\xe5\x86\xb5\xe8\xbf\x9b\xe8\xa1\x8c\xe9\xa2\x84\xe6\xb5\x8b\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe5\xb0\x86\xe8\xbf\x99\xe4\xb8\xaa\xe9\x97\xae\xe9\xa2\x98\xe5\x88\x86\xe8\xa7\xa3\xe4\xb8\xba\xe4\xb8\x89\xe4\xb8\xaa\xe5\xb0\x8f\xe9\x97\xae\xe9\xa2\x98\xef\xbc\x9a\n# \n# 1. \xe8\xbe\x93\xe5\x85\xa5\xe8\x8a\x82\xe7\x82\xb9\xe4\xb8\xba1\xe4\xb8\xaa\xef\xbc\x8c\xe9\x9a\x90\xe5\x90\xab\xe5\xb1\x82\xe4\xb8\xba10\xe4\xb8\xaa\xef\xbc\x8c\xe8\xbe\x93\xe5\x87\xba\xe8\x8a\x82\xe7\x82\xb9\xe6\x95\xb0\xe4\xb8\xba1\xe7\x9a\x84\xe5\xb0\x8f\xe5\x9e\x8b\xe4\xba\xba\xe5\xb7\xa5\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xef\xbc\x8c\xe7\x94\xa8\xe6\x95\xb0\xe6\x8d\xae\xe7\x9a\x84\xe4\xb8\x8b\xe6\xa0\x87\xe9\xa2\x84\xe6\xb5\x8b\xe5\x8d\x95\xe8\xbd\xa6\xe6\x95\xb0\xe9\x87\x8f\n# 2. \xe8\xbe\x93\xe5\x85\xa5\xe8\x8a\x82\xe7\x82\xb9\xe4\xb8\xba56\xe4\xb8\xaa\xef\xbc\x8c\xe9\x9a\x90\xe5\x90\xab\xe5\xb1\x82\xe4\xb8\xba10\xe4\xb8\xaa\xef\xbc\x8c\xe8\xbe\x93\xe5\x87\xba\xe8\x8a\x82\xe7\x82\xb9\xe6\x95\xb0\xe4\xb8\xba1\xe7\x9a\x84\xe4\xba\xba\xe5\xb7\xa5\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xef\xbc\x8c\xe7\x94\xa8\xe6\x95\xb0\xe6\x8d\xae\xe5\xba\x93\xe4\xb8\xad\xe7\x9a\x84\xe6\x98\x9f\xe6\x9c\x9f\xe5\x87\xa0\xe3\x80\x81\xe6\x98\xaf\xe5\x90\xa6\xe8\x8a\x82\xe5\x81\x87\xe6\x97\xa5\xe3\x80\x81\xe6\xb8\xa9\xe5\xba\xa6\xe3\x80\x81\xe6\xb9\xbf\xe5\xba\xa6\xe7\xad\x89\xe5\xb1\x9e\xe6\x80\xa7\xe9\xa2\x84\xe6\xb5\x8b\xe5\x8d\x95\xe8\xbd\xa6\xe6\x95\xb0\xe9\x87\x8f\n# 3. \xe8\xbe\x93\xe5\x85\xa5\xe8\x8a\x82\xe7\x82\xb9\xe4\xb8\xba56\xe4\xb8\xaa\xef\xbc\x8c\xe9\x9a\x90\xe5\x90\xab\xe5\xb1\x82\xe8\x8a\x82\xe7\x82\xb9\xe6\x95\xb0\xe4\xb8\xba10\xe4\xb8\xaa\xef\xbc\x8c\xe8\xbe\x93\xe5\x87\xba\xe8\x8a\x82\xe7\x82\xb9\xe6\x95\xb0\xe4\xb8\xba2\xe4\xb8\xaa\xe7\x9a\x84\xe4\xba\xba\xe5\xb7\xa5\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xef\xbc\x8c\xe7\x94\xa8\xe6\x95\xb0\xe6\x8d\xae\xe5\xba\x93\xe4\xb8\xad\xe7\x9a\x84\xe6\x98\x9f\xe6\x9c\x9f\xe5\x87\xa0\xe3\x80\x81\xe6\x98\xaf\xe5\x90\xa6\xe8\x8a\x82\xe5\x81\x87\xe6\x97\xa5\xe3\x80\x81\xe6\xb8\xa9\xe5\xba\xa6\xe3\x80\x81\xe6\xb9\xbf\xe5\xba\xa6\xe7\xad\x89\xe5\xb1\x9e\xe6\x80\xa7\xe9\xa2\x84\xe6\xb5\x8b\xe5\x8d\x95\xe8\xbd\xa6\xe6\x95\xb0\xe9\x87\x8f\xe6\x98\xaf\xe5\xa4\xa7\xe4\xba\x8e\xe5\xb9\xb3\xe5\x9d\x87\xe5\x80\xbc\xe8\xbf\x98\xe6\x98\xaf\xe5\xb0\x8f\xe4\xba\x8e\xe5\xb9\xb3\xe5\x9d\x87\xe5\x80\xbc\n# \n\n#%%\n#\xe5\xaf\xbc\xe5\x85\xa5\xe9\x9c\x80\xe8\xa6\x81\xe4\xbd\xbf\xe7\x94\xa8\xe7\x9a\x84\xe5\xba\x93\nimport numpy as np\nimport pandas as pd #\xe8\xaf\xbb\xe5\x8f\x96csv\xe6\x96\x87\xe4\xbb\xb6\xe7\x9a\x84\xe5\xba\x93\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.optim as optim\n\n# \xe8\xae\xa9\xe8\xbe\x93\xe5\x87\xba\xe7\x9a\x84\xe5\x9b\xbe\xe5\xbd\xa2\xe7\x9b\xb4\xe6\x8e\xa5\xe5\x9c\xa8Notebook\xe4\xb8\xad\xe6\x98\xbe\xe7\xa4\xba\nget_ipython().run_line_magic(\'matplotlib\', \'inline\')\n\n#%% [markdown]\n# ## \xe4\xb8\x80\xe3\x80\x81\xe5\x87\x86\xe5\xa4\x87\xe5\xb7\xa5\xe4\xbd\x9c\xef\xbc\x9a\xe8\xaf\xbb\xe5\x85\xa5\xe6\x95\xb0\xe6\x8d\xae\xe6\x96\x87\xe4\xbb\xb6\n# \n# \xe9\xa6\x96\xe5\x85\x88\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe8\xaf\xbb\xe5\x85\xa5\xe6\x95\xb0\xe6\x8d\xae\xef\xbc\x8c\xe7\xbb\x98\xe5\x88\xb6\xe5\x9b\xbe\xe5\xbd\xa2\xef\xbc\x8c\xe7\x9c\x8b\xe7\x9c\x8b\xe6\x95\xb0\xe6\x8d\xae\xe9\x95\xbf\xe6\x88\x90\xe4\xbb\x80\xe4\xb9\x88\xe6\xa0\xb7\xe5\xad\x90\n\n#%%\n#\xe8\xaf\xbb\xe5\x8f\x96\xe6\x95\xb0\xe6\x8d\xae\xe5\x88\xb0\xe5\x86\x85\xe5\xad\x98\xe4\xb8\xad\xef\xbc\x8crides\xe4\xb8\xba\xe4\xb8\x80\xe4\xb8\xaadataframe\xe5\xaf\xb9\xe8\xb1\xa1\ndata_path = \'bike-sharing-dataset/hour.csv\'\nrides = pd.read_csv(data_path)\n\n#\xe7\x9c\x8b\xe7\x9c\x8b\xe6\x95\xb0\xe6\x8d\xae\xe9\x95\xbf\xe4\xbb\x80\xe4\xb9\x88\xe6\xa0\xb7\xe5\xad\x90\nrides.head()\n\n\n#%%\n#\xe6\x88\x91\xe4\xbb\xac\xe5\x8f\x96\xe5\x87\xba\xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x80\xe5\x88\x97\xe7\x9a\x84\xe5\x89\x8d50\xe6\x9d\xa1\xe8\xae\xb0\xe5\xbd\x95\xe6\x9d\xa5\xe8\xbf\x9b\xe8\xa1\x8c\xe9\xa2\x84\xe6\xb5\x8b\ncounts = rides[\'cnt\'][:50]\n\n#\xe8\x8e\xb7\xe5\xbe\x97\xe5\x8f\x98\xe9\x87\x8fx\xef\xbc\x8c\xe5\xae\x83\xe6\x98\xaf1\xef\xbc\x8c2\xef\xbc\x8c\xe2\x80\xa6\xe2\x80\xa6\xef\xbc\x8c50\nx = np.arange(len(counts))\n\n# \xe5\xb0\x86counts\xe8\xbd\xac\xe6\x88\x90\xe9\xa2\x84\xe6\xb5\x8b\xe5\x8f\x98\xe9\x87\x8f\xef\xbc\x88\xe6\xa0\x87\xe7\xad\xbe\xef\xbc\x89\xef\xbc\x9ay\ny = np.array(counts)\n\n# \xe7\xbb\x98\xe5\x88\xb6\xe4\xb8\x80\xe4\xb8\xaa\xe5\x9b\xbe\xe5\xbd\xa2\xef\xbc\x8c\xe5\xb1\x95\xe7\xa4\xba\xe6\x9b\xb2\xe7\xba\xbf\xe9\x95\xbf\xe7\x9a\x84\xe6\xa0\xb7\xe5\xad\x90\nplt.figure(figsize = (10, 7)) #\xe8\xae\xbe\xe5\xae\x9a\xe7\xbb\x98\xe5\x9b\xbe\xe7\xaa\x97\xe5\x8f\xa3\xe5\xa4\xa7\xe5\xb0\x8f\nplt.plot(x, y, \'o-\') # \xe7\xbb\x98\xe5\x88\xb6\xe5\x8e\x9f\xe5\xa7\x8b\xe6\x95\xb0\xe6\x8d\xae\nplt.xlabel(\'X\') #\xe6\x9b\xb4\xe6\x94\xb9\xe5\x9d\x90\xe6\xa0\x87\xe8\xbd\xb4\xe6\xa0\x87\xe6\xb3\xa8\nplt.ylabel(\'Y\') #\xe6\x9b\xb4\xe6\x94\xb9\xe5\x9d\x90\xe6\xa0\x87\xe8\xbd\xb4\xe6\xa0\x87\xe6\xb3\xa8\n\n#%% [markdown]\n# ### \xe7\xba\xbf\xe6\x80\xa7\xe5\x9b\x9e\xe5\xbd\x92\xe5\xb0\x9d\xe8\xaf\x95\n# \n# \xe6\x88\x91\xe4\xbb\xac\xe5\x8f\xaf\xe4\xbb\xa5\xe5\x85\x88\xe5\xb0\x9d\xe8\xaf\x95\xe7\x94\xa8\xe7\xba\xbf\xe6\x80\xa7\xe5\x9b\x9e\xe5\xbd\x92\xe6\x9d\xa5\xe5\xaf\xb9\xe6\x9b\xb2\xe7\xba\xbf\xe8\xbf\x9b\xe8\xa1\x8c\xe6\x8b\x9f\xe5\x90\x88\xef\xbc\x8c\xe5\xa4\x8d\xe4\xb9\xa0\xe4\xb8\x80\xe4\xb8\x8b\xe4\xb8\x8a\xe8\x8a\x82\xe8\xaf\xbe\xe5\xad\xa6\xe8\xbf\x87\xe7\x9a\x84\xe5\x86\x85\xe5\xae\xb9\xef\xbc\x8c\xe5\xb0\xbd\xe7\xae\xa1\xe6\x95\x88\xe6\x9e\x9c\xe5\xbe\x88\xe5\xb7\xae\n\n#%%\n#\xe6\x88\x91\xe4\xbb\xac\xe5\x8f\x96\xe5\x87\xba\xe6\x95\xb0\xe6\x8d\xae\xe5\xba\x93\xe7\x9a\x84\xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x80\xe5\x88\x97\xe7\x9a\x84\xe5\x89\x8d50\xe6\x9d\xa1\xe8\xae\xb0\xe5\xbd\x95\xe6\x9d\xa5\xe8\xbf\x9b\xe8\xa1\x8c\xe9\xa2\x84\xe6\xb5\x8b\ncounts = rides[\'cnt\'][:50]\n\n# \xe5\x88\x9b\xe5\xbb\xba\xe5\x8f\x98\xe9\x87\x8fx\xef\xbc\x8c\xe5\xae\x83\xe6\x98\xaf1\xef\xbc\x8c2\xef\xbc\x8c\xe2\x80\xa6\xe2\x80\xa6\xef\xbc\x8c50\nx = torch.FloatTensor(np.arange(len(counts), dtype = float))\n\n# \xe5\xb0\x86counts\xe8\xbd\xac\xe6\x88\x90\xe9\xa2\x84\xe6\xb5\x8b\xe5\x8f\x98\xe9\x87\x8f\xef\xbc\x88\xe6\xa0\x87\xe7\xad\xbe\xef\xbc\x89\xef\xbc\x9ay\ny = torch.FloatTensor(np.array(counts, dtype = float))\n\na = torch.rand(1, requires_grad = True) #\xe5\x88\x9b\xe5\xbb\xbaa\xe5\x8f\x98\xe9\x87\x8f\xef\xbc\x8c\xe5\xb9\xb6\xe9\x9a\x8f\xe6\x9c\xba\xe8\xb5\x8b\xe5\x80\xbc\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\nb = torch.rand(1, requires_grad = True) #\xe5\x88\x9b\xe5\xbb\xbab\xe5\x8f\x98\xe9\x87\x8f\xef\xbc\x8c\xe5\xb9\xb6\xe9\x9a\x8f\xe6\x9c\xba\xe8\xb5\x8b\xe5\x80\xbc\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\nprint(\'Initial parameters:\', [a, b])\nlearning_rate = 0.00001 #\xe8\xae\xbe\xe7\xbd\xae\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\nfor i in range(1000):\n    ### \xe5\xa2\x9e\xe5\x8a\xa0\xe4\xba\x86\xe8\xbf\x99\xe9\x83\xa8\xe5\x88\x86\xe4\xbb\xa3\xe7\xa0\x81\xef\xbc\x8c\xe6\xb8\x85\xe7\xa9\xba\xe5\xad\x98\xe5\x82\xa8\xe5\x9c\xa8\xe5\x8f\x98\xe9\x87\x8fa\xef\xbc\x8cb\xe4\xb8\xad\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\xe4\xbf\xa1\xe6\x81\xaf\xef\xbc\x8c\xe4\xbb\xa5\xe5\x85\x8d\xe5\x9c\xa8backward\xe7\x9a\x84\xe8\xbf\x87\xe7\xa8\x8b\xe4\xb8\xad\xe4\xbc\x9a\xe5\x8f\x8d\xe5\xa4\x8d\xe4\xb8\x8d\xe5\x81\x9c\xe5\x9c\xb0\xe7\xb4\xaf\xe5\x8a\xa0\n    predictions = a.expand_as(x) * x+ b.expand_as(x)  #\xe8\xae\xa1\xe7\xae\x97\xe5\x9c\xa8\xe5\xbd\x93\xe5\x89\x8da\xe3\x80\x81b\xe6\x9d\xa1\xe4\xbb\xb6\xe4\xb8\x8b\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\xe9\xa2\x84\xe6\xb5\x8b\xe6\x95\xb0\xe5\x80\xbc\n    loss = torch.mean((predictions - y) ** 2) #\xe9\x80\x9a\xe8\xbf\x87\xe4\xb8\x8e\xe6\xa0\x87\xe7\xad\xbe\xe6\x95\xb0\xe6\x8d\xaey\xe6\xaf\x94\xe8\xbe\x83\xef\xbc\x8c\xe8\xae\xa1\xe7\xae\x97\xe8\xaf\xaf\xe5\xb7\xae\n    print(\'loss:\', loss)\n    loss.backward() #\xe5\xaf\xb9\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x87\xbd\xe6\x95\xb0\xe8\xbf\x9b\xe8\xa1\x8c\xe6\xa2\xaf\xe5\xba\xa6\xe5\x8f\x8d\xe4\xbc\xa0\n    a.data.add_(- learning_rate * a.grad.data)  #\xe5\x88\xa9\xe7\x94\xa8\xe4\xb8\x8a\xe4\xb8\x80\xe6\xad\xa5\xe8\xae\xa1\xe7\xae\x97\xe4\xb8\xad\xe5\xbe\x97\xe5\x88\xb0\xe7\x9a\x84a\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\xe4\xbf\xa1\xe6\x81\xaf\xe6\x9b\xb4\xe6\x96\xb0a\xe4\xb8\xad\xe7\x9a\x84data\xe6\x95\xb0\xe5\x80\xbc\n    b.data.add_(- learning_rate * b.grad.data)  #\xe5\x88\xa9\xe7\x94\xa8\xe4\xb8\x8a\xe4\xb8\x80\xe6\xad\xa5\xe8\xae\xa1\xe7\xae\x97\xe4\xb8\xad\xe5\xbe\x97\xe5\x88\xb0\xe7\x9a\x84b\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\xe4\xbf\xa1\xe6\x81\xaf\xe6\x9b\xb4\xe6\x96\xb0b\xe4\xb8\xad\xe7\x9a\x84data\xe6\x95\xb0\xe5\x80\xbc\n    a.grad.data.zero_() #\xe6\xb8\x85\xe7\xa9\xbaa\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\xe6\x95\xb0\xe5\x80\xbc\n    b.grad.data.zero_() #\xe6\xb8\x85\xe7\xa9\xbab\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\xe6\x95\xb0\xe5\x80\xbc\n\n\n#%%\n# \xe7\xbb\x98\xe5\x88\xb6\xe5\x9b\xbe\xe5\xbd\xa2\xef\xbc\x8c\xe5\xb1\x95\xe7\x8e\xb0\xe7\xba\xbf\xe6\x80\xa7\xe5\x9b\x9e\xe5\xbd\x92\xe7\x9a\x84\xe6\x95\x88\xe6\x9e\x9c\xef\xbc\x8c\xe7\xbb\x93\xe6\x9e\x9c\xe6\x83\xa8\xe4\xb8\x8d\xe5\xbf\x8d\xe7\x9d\xb9\n\nx_data = x.data.numpy() # \xe8\x8e\xb7\xe5\xbe\x97x\xe5\x8c\x85\xe8\xa3\xb9\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\nplt.figure(figsize = (10, 7)) #\xe8\xae\xbe\xe5\xae\x9a\xe7\xbb\x98\xe5\x9b\xbe\xe7\xaa\x97\xe5\x8f\xa3\xe5\xa4\xa7\xe5\xb0\x8f\nxplot, = plt.plot(x_data, y.data.numpy(), \'o\') # \xe7\xbb\x98\xe5\x88\xb6\xe5\x8e\x9f\xe5\xa7\x8b\xe6\x95\xb0\xe6\x8d\xae\n\nyplot, = plt.plot(x_data, predictions.data.numpy())  #\xe7\xbb\x98\xe5\x88\xb6\xe6\x8b\x9f\xe5\x90\x88\xe6\x95\xb0\xe6\x8d\xae\nplt.xlabel(\'X\') #\xe6\x9b\xb4\xe6\x94\xb9\xe5\x9d\x90\xe6\xa0\x87\xe8\xbd\xb4\xe6\xa0\x87\xe6\xb3\xa8\nplt.ylabel(\'Y\') #\xe6\x9b\xb4\xe6\x94\xb9\xe5\x9d\x90\xe6\xa0\x87\xe8\xbd\xb4\xe6\xa0\x87\xe6\xb3\xa8\nstr1 = str(a.data.numpy()[0]) + \'x +\' + str(b.data.numpy()[0]) #\xe5\x9b\xbe\xe4\xbe\x8b\xe4\xbf\xa1\xe6\x81\xaf\nplt.legend([xplot, yplot],[\'Data\', str1]) #\xe7\xbb\x98\xe5\x88\xb6\xe5\x9b\xbe\xe4\xbe\x8b\nplt.show()\n\n#%% [markdown]\n# ## \xe4\xba\x8c\xe3\x80\x81\xe7\xac\xac\xe4\xb8\x80\xe4\xb8\xaa\xe4\xba\xba\xe5\xb7\xa5\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xe9\xa2\x84\xe6\xb5\x8b\xe5\x99\xa8\n# \n# \xe6\x88\x91\xe4\xbb\xac\xe6\x9e\x84\xe5\xbb\xba\xe4\xb8\x80\xe4\xb8\xaa\xe5\x8d\x95\xe4\xb8\x80\xe8\xbe\x93\xe5\x85\xa5\xef\xbc\x8c10\xe4\xb8\xaa\xe9\x9a\x90\xe5\x90\xab\xe5\xb1\x82\xe5\x8d\x95\xe5\x85\x83\xef\xbc\x8c1\xe4\xb8\xaa\xe8\xbe\x93\xe5\x87\xba\xe5\x8d\x95\xe5\x85\x83\xe7\x9a\x84\xe4\xba\xba\xe5\xb7\xa5\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xe9\xa2\x84\xe6\xb5\x8b\xe5\x99\xa8\n#%% [markdown]\n# ### 1. \xe6\x85\xa2\xe9\x80\x9f\xe7\x89\x88\xe6\x9c\xac\n\n#%%\n#\xe5\x8f\x96\xe5\x87\xba\xe6\x95\xb0\xe6\x8d\xae\xe5\xba\x93\xe4\xb8\xad\xe7\x9a\x84\xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x80\xe5\x88\x97\xe7\x9a\x84\xe5\x89\x8d50\xe6\x9d\xa1\xe8\xae\xb0\xe5\xbd\x95\xe6\x9d\xa5\xe8\xbf\x9b\xe8\xa1\x8c\xe9\xa2\x84\xe6\xb5\x8b\ncounts = rides[\'cnt\'][:50]\n\n#\xe5\x88\x9b\xe5\xbb\xba\xe5\x8f\x98\xe9\x87\x8fx\xef\xbc\x8c\xe5\xae\x83\xe6\x98\xaf1\xef\xbc\x8c2\xef\xbc\x8c\xe2\x80\xa6\xe2\x80\xa6\xef\xbc\x8c50\nx = torch.FloatTensor(np.arange(len(counts), dtype = float))\n\n# \xe5\xb0\x86counts\xe8\xbd\xac\xe6\x88\x90\xe9\xa2\x84\xe6\xb5\x8b\xe5\x8f\x98\xe9\x87\x8f\xef\xbc\x88\xe6\xa0\x87\xe7\xad\xbe\xef\xbc\x89\xef\xbc\x9ay\ny = torch.FloatTensor(np.array(counts, dtype = float))\n\n# \xe8\xae\xbe\xe7\xbd\xae\xe9\x9a\x90\xe5\x90\xab\xe5\xb1\x82\xe7\xa5\x9e\xe7\xbb\x8f\xe5\x85\x83\xe7\x9a\x84\xe6\x95\xb0\xe9\x87\x8f\nsz = 10\n\n# \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe6\x89\x80\xe6\x9c\x89\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xe7\x9a\x84\xe6\x9d\x83\xe9\x87\x8d\xef\xbc\x88weights\xef\xbc\x89\xe5\x92\x8c\xe9\x98\x88\xe5\x80\xbc\xef\xbc\x88biases\xef\xbc\x89\nweights = torch.randn((1, sz), requires_grad = True) #1*10\xe7\x9a\x84\xe8\xbe\x93\xe5\x85\xa5\xe5\x88\xb0\xe9\x9a\x90\xe5\x90\xab\xe5\xb1\x82\xe7\x9a\x84\xe6\x9d\x83\xe9\x87\x8d\xe7\x9f\xa9\xe9\x98\xb5\nbiases = torch.randn((sz), requires_grad = True) #\xe5\xb0\xba\xe5\xba\xa6\xe4\xb8\xba10\xe7\x9a\x84\xe9\x9a\x90\xe5\x90\xab\xe5\xb1\x82\xe8\x8a\x82\xe7\x82\xb9\xe5\x81\x8f\xe7\xbd\xae\xe5\x90\x91\xe9\x87\x8f\nweights2 = torch.randn((sz, 1), requires_grad = True) #10*1\xe7\x9a\x84\xe9\x9a\x90\xe5\x90\xab\xe5\x88\xb0\xe8\xbe\x93\xe5\x87\xba\xe5\xb1\x82\xe6\x9d\x83\xe9\x87\x8d\xe7\x9f\xa9\xe9\x98\xb5\n\nlearning_rate = 0.0001 #\xe8\xae\xbe\xe7\xbd\xae\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\nlosses = []\nfor i in range(1000000):\n    # \xe4\xbb\x8e\xe8\xbe\x93\xe5\x85\xa5\xe5\xb1\x82\xe5\x88\xb0\xe9\x9a\x90\xe5\x90\xab\xe5\xb1\x82\xe7\x9a\x84\xe8\xae\xa1\xe7\xae\x97\n    hidden = x.expand(sz, len(x)).t() * weights.expand(len(x), sz) + biases.expand(len(x), sz)\n    # \xe5\xb0\x86sigmoid\xe5\x87\xbd\xe6\x95\xb0\xe4\xbd\x9c\xe7\x94\xa8\xe5\x9c\xa8\xe9\x9a\x90\xe5\x90\xab\xe5\xb1\x82\xe7\x9a\x84\xe6\xaf\x8f\xe4\xb8\x80\xe4\xb8\xaa\xe7\xa5\x9e\xe7\xbb\x8f\xe5\x85\x83\xe4\xb8\x8a\n    hidden = torch.sigmoid(hidden)\n    #print(hidden.size())\n    # \xe9\x9a\x90\xe5\x90\xab\xe5\xb1\x82\xe8\xbe\x93\xe5\x87\xba\xe5\x88\xb0\xe8\xbe\x93\xe5\x87\xba\xe5\xb1\x82\xef\xbc\x8c\xe8\xae\xa1\xe7\xae\x97\xe5\xbe\x97\xe5\x88\xb0\xe6\x9c\x80\xe7\xbb\x88\xe9\xa2\x84\xe6\xb5\x8b\n    predictions = hidden.mm(weights2)#\n    #print(predictions.size())\n    # \xe9\x80\x9a\xe8\xbf\x87\xe4\xb8\x8e\xe6\xa0\x87\xe7\xad\xbe\xe6\x95\xb0\xe6\x8d\xaey\xe6\xaf\x94\xe8\xbe\x83\xef\xbc\x8c\xe8\xae\xa1\xe7\xae\x97\xe5\x9d\x87\xe6\x96\xb9\xe8\xaf\xaf\xe5\xb7\xae\n    loss = torch.mean((predictions - y) ** 2) \n    #print(loss.size())\n    losses.append(loss.data.numpy())\n    \n    # \xe6\xaf\x8f\xe9\x9a\x9410000\xe4\xb8\xaa\xe5\x91\xa8\xe6\x9c\x9f\xe6\x89\x93\xe5\x8d\xb0\xe4\xb8\x80\xe4\xb8\x8b\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x87\xbd\xe6\x95\xb0\xe6\x95\xb0\xe5\x80\xbc\n    if i % 10000 == 0:\n        print(\'loss:\', loss)\n        \n    #\xe5\xaf\xb9\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x87\xbd\xe6\x95\xb0\xe8\xbf\x9b\xe8\xa1\x8c\xe6\xa2\xaf\xe5\xba\xa6\xe5\x8f\x8d\xe4\xbc\xa0\n    loss.backward()\n    \n    #\xe5\x88\xa9\xe7\x94\xa8\xe4\xb8\x8a\xe4\xb8\x80\xe6\xad\xa5\xe8\xae\xa1\xe7\xae\x97\xe4\xb8\xad\xe5\xbe\x97\xe5\x88\xb0\xe7\x9a\x84weights\xef\xbc\x8cbiases\xe7\xad\x89\xe6\xa2\xaf\xe5\xba\xa6\xe4\xbf\xa1\xe6\x81\xaf\xe6\x9b\xb4\xe6\x96\xb0weights\xe6\x88\x96biases\xe4\xb8\xad\xe7\x9a\x84data\xe6\x95\xb0\xe5\x80\xbc\n    weights.data.add_(- learning_rate * weights.grad.data)  \n    biases.data.add_(- learning_rate * biases.grad.data)\n    weights2.data.add_(- learning_rate * weights2.grad.data)\n    \n    # \xe6\xb8\x85\xe7\xa9\xba\xe6\x89\x80\xe6\x9c\x89\xe5\x8f\x98\xe9\x87\x8f\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\xe5\x80\xbc\xe3\x80\x82\n    # \xe5\x9b\xa0\xe4\xb8\xbapytorch\xe4\xb8\xadbackward\xe4\xb8\x80\xe6\xac\xa1\xe6\xa2\xaf\xe5\xba\xa6\xe4\xbf\xa1\xe6\x81\xaf\xe4\xbc\x9a\xe8\x87\xaa\xe5\x8a\xa8\xe7\xb4\xaf\xe5\x8a\xa0\xe5\x88\xb0\xe5\x90\x84\xe4\xb8\xaa\xe5\x8f\x98\xe9\x87\x8f\xe4\xb8\x8a\xef\xbc\x8c\xe5\x9b\xa0\xe6\xad\xa4\xe9\x9c\x80\xe8\xa6\x81\xe6\xb8\x85\xe7\xa9\xba\xef\xbc\x8c\xe5\x90\xa6\xe5\x88\x99\xe4\xb8\x8b\xe4\xb8\x80\xe6\xac\xa1\xe8\xbf\xad\xe4\xbb\xa3\xe4\xbc\x9a\xe7\xb4\xaf\xe5\x8a\xa0\xef\xbc\x8c\xe9\x80\xa0\xe6\x88\x90\xe5\xbe\x88\xe5\xa4\xa7\xe7\x9a\x84\xe5\x81\x8f\xe5\xb7\xae\n    weights.grad.data.zero_()\n    biases.grad.data.zero_()\n    weights2.grad.data.zero_()\n\n\n#%%\n# \xe6\x89\x93\xe5\x8d\xb0\xe8\xaf\xaf\xe5\xb7\xae\xe6\x9b\xb2\xe7\xba\xbf\nplt.plot(losses)\nplt.xlabel(\'Epoch\')\nplt.ylabel(\'Loss\')\n\n\n#%%\nx_data = x.data.numpy() # \xe8\x8e\xb7\xe5\xbe\x97x\xe5\x8c\x85\xe8\xa3\xb9\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\nplt.figure(figsize = (10, 7)) #\xe8\xae\xbe\xe5\xae\x9a\xe7\xbb\x98\xe5\x9b\xbe\xe7\xaa\x97\xe5\x8f\xa3\xe5\xa4\xa7\xe5\xb0\x8f\nxplot, = plt.plot(x_data, y.data.numpy(), \'o\') # \xe7\xbb\x98\xe5\x88\xb6\xe5\x8e\x9f\xe5\xa7\x8b\xe6\x95\xb0\xe6\x8d\xae\n\nyplot, = plt.plot(x_data, predictions.data.numpy())  #\xe7\xbb\x98\xe5\x88\xb6\xe6\x8b\x9f\xe5\x90\x88\xe6\x95\xb0\xe6\x8d\xae\nplt.xlabel(\'X\') #\xe6\x9b\xb4\xe6\x94\xb9\xe5\x9d\x90\xe6\xa0\x87\xe8\xbd\xb4\xe6\xa0\x87\xe6\xb3\xa8\nplt.ylabel(\'Y\') #\xe6\x9b\xb4\xe6\x94\xb9\xe5\x9d\x90\xe6\xa0\x87\xe8\xbd\xb4\xe6\xa0\x87\xe6\xb3\xa8\nplt.legend([xplot, yplot],[\'Data\', \'Prediction under 1000000 epochs\']) #\xe7\xbb\x98\xe5\x88\xb6\xe5\x9b\xbe\xe4\xbe\x8b\nplt.show()\n\n#%% [markdown]\n# ### 2. \xe6\x94\xb9\xe8\xbf\x9b\xe7\x89\x88\xe6\x9c\xac\n# \n# \xe4\xb8\x8a\xe9\x9d\xa2\xe7\x9a\x84\xe7\xa8\x8b\xe5\xba\x8f\xe4\xb9\x8b\xe6\x89\x80\xe4\xbb\xa5\xe8\xb7\x91\xe5\xbe\x97\xe5\xbe\x88\xe6\x85\xa2\xef\xbc\x8c\xe6\x98\xaf\xe5\x9b\xa0\xe4\xb8\xbax\xe7\x9a\x84\xe5\x8f\x96\xe5\x80\xbc\xe8\x8c\x83\xe5\x9b\xb41\xef\xbd\x9e50\xe3\x80\x82\n# \xe8\x80\x8c\xe7\x94\xb1\xe4\xba\x8e\xe6\x89\x80\xe6\x9c\x89\xe6\x9d\x83\xe9\x87\x8d\xe5\x92\x8cbiases\xe7\x9a\x84\xe5\x8f\x96\xe5\x80\xbc\xe8\x8c\x83\xe5\x9b\xb4\xe8\xa2\xab\xe8\xae\xbe\xe5\xae\x9a\xe4\xb8\xba-1,1\xe7\x9a\x84\xe6\xad\xa3\xe6\x80\x81\xe5\x88\x86\xe5\xb8\x83\xe9\x9a\x8f\xe6\x9c\xba\xe6\x95\xb0\xef\xbc\x8c\xe8\xbf\x99\xe6\xa0\xb7\xe5\xb0\xb1\xe5\xaf\xbc\xe8\x87\xb4\n# \xe6\x88\x91\xe4\xbb\xac\xe8\xbe\x93\xe5\x85\xa5\xe7\xbb\x99\xe9\x9a\x90\xe5\x90\xab\xe5\xb1\x82\xe8\x8a\x82\xe7\x82\xb9\xe7\x9a\x84\xe6\x95\xb0\xe5\x80\xbc\xe8\x8c\x83\xe5\x9b\xb4\xe4\xb8\xba-50~50\xef\xbc\x8c\n# \xe8\xa6\x81\xe6\x83\xb3\xe5\xb0\x86sigmoid\xe5\x87\xbd\xe6\x95\xb0\xe7\x9a\x84\xe5\xa4\x9a\xe4\xb8\xaa\xe5\xb3\xb0\xe5\x80\xbc\xe8\xb0\x83\xe8\x8a\x82\xe5\x88\xb0\xe6\x88\x91\xe4\xbb\xac\xe6\x9c\x9f\xe6\x9c\x9b\xe7\x9a\x84\xe4\xbd\x8d\xe7\xbd\xae\xe9\x9c\x80\xe8\xa6\x81\xe8\x80\x97\xe8\xb4\xb9\xe5\xbe\x88\xe5\xa4\x9a\xe7\x9a\x84\xe8\xae\xa1\xe7\xae\x97\xe6\x97\xb6\xe9\x97\xb4\n# \n# \xe6\x88\x91\xe4\xbb\xac\xe7\x9a\x84\xe8\xa7\xa3\xe5\x86\xb3\xe6\x96\xb9\xe6\xa1\x88\xe5\xb0\xb1\xe6\x98\xaf\xe5\xb0\x86\xe8\xbe\x93\xe5\x85\xa5\xe5\x8f\x98\xe9\x87\x8f\xe7\x9a\x84\xe8\x8c\x83\xe5\x9b\xb4\xe5\xbd\x92\xe4\xb8\x80\xe5\x8c\x96\n\n#%%\n#\xe5\x8f\x96\xe5\x87\xba\xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x80\xe5\x88\x97\xe7\x9a\x84\xe5\x89\x8d50\xe6\x9d\xa1\xe8\xae\xb0\xe5\xbd\x95\xe6\x9d\xa5\xe8\xbf\x9b\xe8\xa1\x8c\xe9\xa2\x84\xe6\xb5\x8b\ncounts = rides[\'cnt\'][:50]\n\n#\xe5\x88\x9b\xe5\xbb\xba\xe5\xbd\x92\xe4\xb8\x80\xe5\x8c\x96\xe7\x9a\x84\xe5\x8f\x98\xe9\x87\x8fx\xef\xbc\x8c\xe5\xae\x83\xe7\x9a\x84\xe5\x8f\x96\xe5\x80\xbc\xe6\x98\xaf0.02,0.04,...,1\nx = torch.FloatTensor(np.arange(len(counts), dtype = float) / len(counts))\n\n# \xe5\x88\x9b\xe5\xbb\xba\xe5\xbd\x92\xe4\xb8\x80\xe5\x8c\x96\xe7\x9a\x84\xe9\xa2\x84\xe6\xb5\x8b\xe5\x8f\x98\xe9\x87\x8fy\xef\xbc\x8c\xe5\xae\x83\xe7\x9a\x84\xe5\x8f\x96\xe5\x80\xbc\xe8\x8c\x83\xe5\x9b\xb4\xe6\x98\xaf0\xef\xbd\x9e1\ny = torch.FloatTensor(np.array(counts, dtype = float))\n\n# \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe6\x89\x80\xe6\x9c\x89\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xe7\x9a\x84\xe6\x9d\x83\xe9\x87\x8d\xef\xbc\x88weights\xef\xbc\x89\xe5\x92\x8c\xe9\x98\x88\xe5\x80\xbc\xef\xbc\x88biases\xef\xbc\x89\nweights = torch.randn((1, sz), requires_grad = True)#1*10\xe7\x9a\x84\xe8\xbe\x93\xe5\x85\xa5\xe5\x88\xb0\xe9\x9a\x90\xe5\x90\xab\xe5\xb1\x82\xe7\x9a\x84\xe6\x9d\x83\xe9\x87\x8d\xe7\x9f\xa9\xe9\x98\xb5\nbiases = torch.randn((sz), requires_grad = True) #\xe5\xb0\xba\xe5\xba\xa6\xe4\xb8\xba10\xe7\x9a\x84\xe9\x9a\x90\xe5\x90\xab\xe5\xb1\x82\xe8\x8a\x82\xe7\x82\xb9\xe5\x81\x8f\xe7\xbd\xae\xe5\x90\x91\xe9\x87\x8f\nweights2 = torch.randn((sz, 1), requires_grad = True) #10*1\xe7\x9a\x84\xe9\x9a\x90\xe5\x90\xab\xe5\x88\xb0\xe8\xbe\x93\xe5\x87\xba\xe5\xb1\x82\xe6\x9d\x83\xe9\x87\x8d\xe7\x9f\xa9\xe9\x98\xb5\n\nlearning_rate = 0.0001 #\xe8\xae\xbe\xe7\xbd\xae\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\nlosses = []\nfor i in range(2000000):\n    # \xe4\xbb\x8e\xe8\xbe\x93\xe5\x85\xa5\xe5\xb1\x82\xe5\x88\xb0\xe9\x9a\x90\xe5\x90\xab\xe5\xb1\x82\xe7\x9a\x84\xe8\xae\xa1\xe7\xae\x97\n    hidden = x.expand(sz, len(x)).t() * weights.expand(len(x), sz) + biases.expand(len(x), sz)\n    # \xe5\xb0\x86sigmoid\xe5\x87\xbd\xe6\x95\xb0\xe4\xbd\x9c\xe7\x94\xa8\xe5\x9c\xa8\xe9\x9a\x90\xe5\x90\xab\xe5\xb1\x82\xe7\x9a\x84\xe6\xaf\x8f\xe4\xb8\x80\xe4\xb8\xaa\xe7\xa5\x9e\xe7\xbb\x8f\xe5\x85\x83\xe4\xb8\x8a\n    hidden = torch.sigmoid(hidden)\n    # \xe9\x9a\x90\xe5\x90\xab\xe5\xb1\x82\xe8\xbe\x93\xe5\x87\xba\xe5\x88\xb0\xe8\xbe\x93\xe5\x87\xba\xe5\xb1\x82\xef\xbc\x8c\xe8\xae\xa1\xe7\xae\x97\xe5\xbe\x97\xe5\x88\xb0\xe6\x9c\x80\xe7\xbb\x88\xe9\xa2\x84\xe6\xb5\x8b\n    predictions = hidden.mm(weights2)# + biases2.expand_as(y)\n    # \xe9\x80\x9a\xe8\xbf\x87\xe4\xb8\x8e\xe6\xa0\x87\xe7\xad\xbe\xe6\x95\xb0\xe6\x8d\xaey\xe6\xaf\x94\xe8\xbe\x83\xef\xbc\x8c\xe8\xae\xa1\xe7\xae\x97\xe5\x9d\x87\xe6\x96\xb9\xe8\xaf\xaf\xe5\xb7\xae\n    loss = torch.mean((predictions - y) ** 2) \n    losses.append(loss.data.numpy())\n    \n    # \xe6\xaf\x8f\xe9\x9a\x9410000\xe4\xb8\xaa\xe5\x91\xa8\xe6\x9c\x9f\xe6\x89\x93\xe5\x8d\xb0\xe4\xb8\x80\xe4\xb8\x8b\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x87\xbd\xe6\x95\xb0\xe6\x95\xb0\xe5\x80\xbc\n    if i % 10000 == 0:\n        print(\'loss:\', loss)\n        \n    #\xe5\xaf\xb9\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x87\xbd\xe6\x95\xb0\xe8\xbf\x9b\xe8\xa1\x8c\xe6\xa2\xaf\xe5\xba\xa6\xe5\x8f\x8d\xe4\xbc\xa0\n    loss.backward()\n    \n    #\xe5\x88\xa9\xe7\x94\xa8\xe4\xb8\x8a\xe4\xb8\x80\xe6\xad\xa5\xe8\xae\xa1\xe7\xae\x97\xe4\xb8\xad\xe5\xbe\x97\xe5\x88\xb0\xe7\x9a\x84weights\xef\xbc\x8cbiases\xe7\xad\x89\xe6\xa2\xaf\xe5\xba\xa6\xe4\xbf\xa1\xe6\x81\xaf\xe6\x9b\xb4\xe6\x96\xb0weights\xe6\x88\x96biases\xe4\xb8\xad\xe7\x9a\x84data\xe6\x95\xb0\xe5\x80\xbc\n    weights.data.add_(- learning_rate * weights.grad.data)  \n    biases.data.add_(- learning_rate * biases.grad.data)\n    weights2.data.add_(- learning_rate * weights2.grad.data)\n    \n    # \xe6\xb8\x85\xe7\xa9\xba\xe6\x89\x80\xe6\x9c\x89\xe5\x8f\x98\xe9\x87\x8f\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\xe5\x80\xbc\xe3\x80\x82\n    # \xe5\x9b\xa0\xe4\xb8\xbapytorch\xe4\xb8\xadbackward\xe4\xb8\x80\xe6\xac\xa1\xe6\xa2\xaf\xe5\xba\xa6\xe4\xbf\xa1\xe6\x81\xaf\xe4\xbc\x9a\xe8\x87\xaa\xe5\x8a\xa8\xe7\xb4\xaf\xe5\x8a\xa0\xe5\x88\xb0\xe5\x90\x84\xe4\xb8\xaa\xe5\x8f\x98\xe9\x87\x8f\xe4\xb8\x8a\xef\xbc\x8c\xe5\x9b\xa0\xe6\xad\xa4\xe9\x9c\x80\xe8\xa6\x81\xe6\xb8\x85\xe7\xa9\xba\xef\xbc\x8c\xe5\x90\xa6\xe5\x88\x99\xe4\xb8\x8b\xe4\xb8\x80\xe6\xac\xa1\xe8\xbf\xad\xe4\xbb\xa3\xe4\xbc\x9a\xe7\xb4\xaf\xe5\x8a\xa0\xef\xbc\x8c\xe9\x80\xa0\xe6\x88\x90\xe5\xbe\x88\xe5\xa4\xa7\xe7\x9a\x84\xe5\x81\x8f\xe5\xb7\xae\n    weights.grad.data.zero_()\n    biases.grad.data.zero_()\n    weights2.grad.data.zero_()\n\n\n#%%\nplt.semilogy(losses)\nplt.xlabel(\'Epoch\')\nplt.ylabel(\'Loss\')\n\n\n#%%\nx_data = x.data.numpy() # \xe8\x8e\xb7\xe5\xbe\x97x\xe5\x8c\x85\xe8\xa3\xb9\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\nplt.figure(figsize = (10, 7)) #\xe8\xae\xbe\xe5\xae\x9a\xe7\xbb\x98\xe5\x9b\xbe\xe7\xaa\x97\xe5\x8f\xa3\xe5\xa4\xa7\xe5\xb0\x8f\nxplot, = plt.plot(x_data, y.data.numpy(), \'o\') # \xe7\xbb\x98\xe5\x88\xb6\xe5\x8e\x9f\xe5\xa7\x8b\xe6\x95\xb0\xe6\x8d\xae\nyplot, = plt.plot(x_data, predictions.data.numpy())  #\xe7\xbb\x98\xe5\x88\xb6\xe6\x8b\x9f\xe5\x90\x88\xe6\x95\xb0\xe6\x8d\xae\nplt.xlabel(\'X\') #\xe6\x9b\xb4\xe6\x94\xb9\xe5\x9d\x90\xe6\xa0\x87\xe8\xbd\xb4\xe6\xa0\x87\xe6\xb3\xa8\nplt.ylabel(\'Y\') #\xe6\x9b\xb4\xe6\x94\xb9\xe5\x9d\x90\xe6\xa0\x87\xe8\xbd\xb4\xe6\xa0\x87\xe6\xb3\xa8\nplt.legend([xplot, yplot],[\'Data\', \'Prediction\']) #\xe7\xbb\x98\xe5\x88\xb6\xe5\x9b\xbe\xe4\xbe\x8b\nplt.show()\n\n#%% [markdown]\n# ### 3. \xe7\x94\xa8\xe8\xae\xad\xe7\xbb\x83\xe5\xa5\xbd\xe7\x9a\x84\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xe5\x81\x9a\xe9\xa2\x84\xe6\xb5\x8b\n# \n# \xe9\xa2\x84\xe6\xb5\x8b\xe4\xb8\x8b50\xe4\xb8\xaa\xe8\x8a\x82\xe7\x82\xb9\n\n#%%\ncounts_predict = rides[\'cnt\'][50:100] #\xe8\xaf\xbb\xe5\x8f\x96\xe5\xbe\x85\xe9\xa2\x84\xe6\xb5\x8b\xe7\x9a\x84\xe6\x8e\xa5\xe4\xb8\x8b\xe6\x9d\xa5\xe7\x9a\x8450\xe4\xb8\xaa\xe6\x95\xb0\xe6\x8d\xae\xe7\x82\xb9\n\n#\xe9\xa6\x96\xe5\x85\x88\xe5\xaf\xb9\xe6\x8e\xa5\xe4\xb8\x8b\xe6\x9d\xa5\xe7\x9a\x8450\xe4\xb8\xaa\xe6\x95\xb0\xe6\x8d\xae\xe7\x82\xb9\xe8\xbf\x9b\xe8\xa1\x8c\xe9\x80\x89\xe5\x8f\x96\xef\xbc\x8c\xe6\xb3\xa8\xe6\x84\x8fx\xe5\xba\x94\xe8\xaf\xa5\xe5\x8f\x9651\xef\xbc\x8c52\xef\xbc\x8c\xe2\x80\xa6\xe2\x80\xa6\xef\xbc\x8c100\xef\xbc\x8c\xe7\x84\xb6\xe5\x90\x8e\xe5\x86\x8d\xe5\xbd\x92\xe4\xb8\x80\xe5\x8c\x96\nx = torch.FloatTensor((np.arange(len(counts_predict), dtype = float) + len(counts)) / len(counts_predict))\n\n#\xe8\xaf\xbb\xe5\x8f\x96\xe4\xb8\x8b50\xe4\xb8\xaa\xe7\x82\xb9\xe7\x9a\x84y\xe6\x95\xb0\xe5\x80\xbc\xef\xbc\x8c\xe4\xb8\x8d\xe9\x9c\x80\xe8\xa6\x81\xe5\x81\x9a\xe5\xbd\x92\xe4\xb8\x80\xe5\x8c\x96\ny = torch.FloatTensor(np.array(counts_predict, dtype = float))\n\n# \xe4\xbb\x8e\xe8\xbe\x93\xe5\x85\xa5\xe5\xb1\x82\xe5\x88\xb0\xe9\x9a\x90\xe5\x90\xab\xe5\xb1\x82\xe7\x9a\x84\xe8\xae\xa1\xe7\xae\x97\nhidden = x.expand(sz, len(x)).t() * weights.expand(len(x), sz) + biases.expand(len(x), sz)\n\n# \xe5\xb0\x86sigmoid\xe5\x87\xbd\xe6\x95\xb0\xe4\xbd\x9c\xe7\x94\xa8\xe5\x9c\xa8\xe9\x9a\x90\xe5\x90\xab\xe5\xb1\x82\xe7\x9a\x84\xe6\xaf\x8f\xe4\xb8\x80\xe4\xb8\xaa\xe7\xa5\x9e\xe7\xbb\x8f\xe5\x85\x83\xe4\xb8\x8a\nhidden = torch.sigmoid(hidden)\n\n# \xe9\x9a\x90\xe5\x90\xab\xe5\xb1\x82\xe8\xbe\x93\xe5\x87\xba\xe5\x88\xb0\xe8\xbe\x93\xe5\x87\xba\xe5\xb1\x82\xef\xbc\x8c\xe8\xae\xa1\xe7\xae\x97\xe5\xbe\x97\xe5\x88\xb0\xe6\x9c\x80\xe7\xbb\x88\xe9\xa2\x84\xe6\xb5\x8b\npredictions = hidden.mm(weights2)\n\n# \xe8\xae\xa1\xe7\xae\x97\xe9\xa2\x84\xe6\xb5\x8b\xe6\x95\xb0\xe6\x8d\xae\xe4\xb8\x8a\xe7\x9a\x84\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x87\xbd\xe6\x95\xb0\nloss = torch.mean((predictions - y) ** 2) \nprint(loss)\n\n\nx_data = x.data.numpy() # \xe8\x8e\xb7\xe5\xbe\x97x\xe5\x8c\x85\xe8\xa3\xb9\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\nplt.figure(figsize = (10, 7)) #\xe8\xae\xbe\xe5\xae\x9a\xe7\xbb\x98\xe5\x9b\xbe\xe7\xaa\x97\xe5\x8f\xa3\xe5\xa4\xa7\xe5\xb0\x8f\nxplot, = plt.plot(x_data, y.data.numpy(), \'o\') # \xe7\xbb\x98\xe5\x88\xb6\xe5\x8e\x9f\xe5\xa7\x8b\xe6\x95\xb0\xe6\x8d\xae\nyplot, = plt.plot(x_data, predictions.data.numpy())  #\xe7\xbb\x98\xe5\x88\xb6\xe6\x8b\x9f\xe5\x90\x88\xe6\x95\xb0\xe6\x8d\xae\nplt.xlabel(\'X\') #\xe6\x9b\xb4\xe6\x94\xb9\xe5\x9d\x90\xe6\xa0\x87\xe8\xbd\xb4\xe6\xa0\x87\xe6\xb3\xa8\nplt.ylabel(\'Y\') #\xe6\x9b\xb4\xe6\x94\xb9\xe5\x9d\x90\xe6\xa0\x87\xe8\xbd\xb4\xe6\xa0\x87\xe6\xb3\xa8\nplt.legend([xplot, yplot],[\'Data\', \'Prediction\']) #\xe7\xbb\x98\xe5\x88\xb6\xe5\x9b\xbe\xe4\xbe\x8b\nplt.show()\n\n#%% [markdown]\n# \xe7\x84\xb6\xe8\x80\x8c\xef\xbc\x8c\xe9\xa2\x84\xe6\xb5\x8b\xe5\x8f\x91\xe7\x8e\xb0\xe5\xad\x98\xe5\x9c\xa8\xe7\x9d\x80\xe9\x9d\x9e\xe5\xb8\xb8\xe4\xb8\xa5\xe9\x87\x8d\xe7\x9a\x84\xe8\xbf\x87\xe6\x8b\x9f\xe5\x90\x88\xe7\x8e\xb0\xe8\xb1\xa1\xef\xbc\x81\xe5\x8e\x9f\xe5\x9b\xa0\xe6\x98\xafx\xe5\x92\x8cy\xe6\xa0\xb9\xe6\x9c\xac\xe5\xb0\xb1\xe6\xb2\xa1\xe6\x9c\x89\xe5\x85\xb3\xe7\xb3\xbb\xef\xbc\x81\n#%% [markdown]\n# ## \xe4\xb8\x89\xe3\x80\x81\xe4\xba\xba\xe5\xb7\xa5\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9cNeu\n# \n# \xe5\x9c\xa8\xe8\xbf\x99\xe4\xb8\x80\xe5\xb0\x8f\xe8\x8a\x82\xe4\xb8\xad\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe5\xb0\x86\xe5\x86\x8d\xe6\x9e\x84\xe5\xbb\xba\xe4\xb8\x80\xe4\xb8\xaa\xe4\xba\xba\xe5\xb7\xa5\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xef\xbc\x8c\xe5\x88\xa9\xe7\x94\xa8\xe6\x95\xb0\xe6\x8d\xae\xe5\xba\x93\xe4\xb8\xad\xe7\x9a\x84\xe6\x98\x9f\xe6\x9c\x9f\xe5\x87\xa0\xe3\x80\x81\xe8\x8a\x82\xe5\x81\x87\xe6\x97\xa5\xe3\x80\x81\xe6\x97\xb6\xe9\x97\xb4\xe3\x80\x81\xe9\xa3\x8e\xe9\x80\x9f\xe7\xad\x89\xe4\xbf\xa1\xe6\x81\xaf\xe9\xa2\x84\xe6\xb5\x8b\xe5\x85\xb1\xe4\xba\xab\xe5\x8d\x95\xe8\xbd\xa6\xe7\x9a\x84\xe4\xbd\xbf\xe7\x94\xa8\xe6\x95\xb0\xe9\x87\x8f\n# \n# \xe8\xaf\xa5\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xe6\x9c\x8956\xe4\xb8\xaa\xe8\xbe\x93\xe5\x85\xa5\xe5\xb1\x82\xe8\x8a\x82\xe7\x82\xb9\xe3\x80\x8110\xe4\xb8\xaa\xe9\x9a\x90\xe5\x90\xab\xe5\xb1\x82\xe8\x8a\x82\xe7\x82\xb9\xe5\x92\x8c1\xe4\xb8\xaa\xe8\xbe\x93\xe5\x87\xba\xe8\x8a\x82\xe7\x82\xb9\n#%% [markdown]\n# ### 1. \xe6\x95\xb0\xe6\x8d\xae\xe7\x9a\x84\xe9\xa2\x84\xe5\xa4\x84\xe7\x90\x86\xe8\xbf\x87\xe7\xa8\x8b\n# \n# \xe8\xa6\x81\xe8\xaf\xbb\xe5\x85\xa5\xe5\x85\xb6\xe4\xbb\x96\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe5\xb0\xb1\xe8\xa6\x81\xe8\x80\x83\xe8\x99\x91\xe5\x88\xb0\xe8\xbf\x99\xe4\xba\x9b\xe6\x95\xb0\xe6\x8d\xae\xe5\x85\xb7\xe6\x9c\x89\xe4\xb8\x8d\xe5\x90\x8c\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe7\xb1\xbb\xe5\x9e\x8b\xef\xbc\x8c\xe4\xbb\xa5\xe5\x8f\x8a\xe5\x8f\x96\xe5\x80\xbc\xe8\x8c\x83\xe5\x9b\xb4\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe8\xa6\x81\xe5\xaf\xb9\xe5\xae\x83\xe4\xbb\xac\xe8\xbf\x9b\xe8\xa1\x8c\xe9\xa2\x84\xe5\xa4\x84\xe7\x90\x86\n# \n# \xe5\x8f\xa6\xe5\xa4\x96\xef\xbc\x8c\xe7\x94\xb1\xe4\xba\x8e\xe6\x88\x91\xe4\xbb\xac\xe5\x88\xa9\xe7\x94\xa8\xe4\xba\x86\xe5\x85\xa8\xe9\x83\xa8\xe6\x95\xb0\xe6\x8d\xae\xe6\x9d\xa5\xe8\xae\xad\xe7\xbb\x83\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe9\x87\x87\xe7\x94\xa8\xe4\xb9\x8b\xe5\x89\x8d\xe4\xbb\x8b\xe7\xbb\x8d\xe7\x9a\x84\xe4\xb8\x80\xe6\xac\xa1\xe6\x80\xa7\xe5\x9c\xa8\xe5\x85\xa8\xe9\x83\xa8\xe6\x95\xb0\xe6\x8d\xae\xe4\xb8\x8a\xe8\xae\xad\xe7\xbb\x83\xe7\xbd\x91\xe7\xbb\x9c\xe7\x9a\x84\xe6\x96\xb9\xe6\xb3\x95\xe5\xb0\xb1\xe4\xbc\x9a\xe5\xbe\x88\xe6\x85\xa2\xef\xbc\x8c\n# \xe6\x89\x80\xe4\xbb\xa5\xe6\x88\x91\xe4\xbb\xac\xe5\xb0\x86\xe6\x95\xb0\xe6\x8d\xae\xe5\x88\x92\xe5\x88\x86\xe6\x88\x90\xe4\xba\x86\xe4\xb8\x8d\xe5\x90\x8c\xe7\x9a\x84\xe6\x92\xae\xef\xbc\x88batch\xef\xbc\x89\xef\xbc\x8c\xe4\xb8\x80\xe4\xb8\xaa\xe6\x89\xb9\xe6\xac\xa1\xe4\xb8\x80\xe4\xb8\xaa\xe6\x89\xb9\xe6\xac\xa1\xe5\x9c\xb0\xe8\xae\xad\xe7\xbb\x83\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xef\xbc\x8c\xe5\x9b\xa0\xe6\xad\xa4\xe6\x88\x91\xe4\xbb\xac\xe8\xa6\x81\xe5\xaf\xb9\xe6\x95\xb0\xe6\x8d\xae\xe8\xbf\x9b\xe8\xa1\x8c\xe5\x88\x92\xe5\x88\x86\n\n#%%\n#\xe9\xa6\x96\xe5\x85\x88\xef\xbc\x8c\xe8\xae\xa9\xe6\x88\x91\xe4\xbb\xac\xe5\x86\x8d\xe6\x9d\xa5\xe7\x9c\x8b\xe7\x9c\x8b\xe6\x95\xb0\xe6\x8d\xae\xe9\x95\xbf\xe4\xbb\x80\xe4\xb9\x88\xe6\xa0\xb7\xe5\xad\x90\n#\xe8\xaf\xbb\xe5\x8f\x96\xe6\x95\xb0\xe6\x8d\xae\xe5\x88\xb0\xe5\x86\x85\xe5\xad\x98\xe4\xb8\xad\xef\xbc\x8crides\xe4\xb8\xba\xe4\xb8\x80\xe4\xb8\xaadataframe\xe5\xaf\xb9\xe8\xb1\xa1\ndata_path = \'bike-sharing-dataset/hour.csv\'\nrides = pd.read_csv(data_path)\nrides.head()\n\n#%% [markdown]\n# #### a. \xe5\xaf\xb9\xe4\xba\x8e\xe7\xb1\xbb\xe5\x9e\x8b\xe5\x8f\x98\xe9\x87\x8f\xe7\x9a\x84\xe5\xa4\x84\xe7\x90\x86\n# \n# \xe6\x9c\x89\xe5\xbe\x88\xe5\xa4\x9a\xe5\x8f\x98\xe9\x87\x8f\xe9\x83\xbd\xe5\xb1\x9e\xe4\xba\x8e\xe7\xb1\xbb\xe5\x9e\x8b\xe5\x8f\x98\xe9\x87\x8f\xef\xbc\x8c\xe4\xbe\x8b\xe5\xa6\x82season=1,2,3,4\xef\xbc\x8c\xe5\x88\x86\xe5\x9b\x9b\xe5\xad\xa3\xe3\x80\x82\xe6\x88\x91\xe4\xbb\xac\xe4\xb8\x8d\xe8\x83\xbd\xe5\xb0\x86season\xe5\x8f\x98\xe9\x87\x8f\xe7\x9b\xb4\xe6\x8e\xa5\xe8\xbe\x93\xe5\x85\xa5\xe5\x88\xb0\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xef\xbc\x8c\xe8\xbf\x99\xe6\x98\xaf\xe5\x9b\xa0\xe4\xb8\xbaseason\xe6\x95\xb0\xe5\x80\xbc\xe8\xb6\x8a\xe9\xab\x98\xe5\xb9\xb6\xe4\xb8\x8d\xe8\xa1\xa8\xe7\xa4\xba\xe7\x9b\xb8\xe5\xba\x94\xe7\x9a\x84\xe4\xbf\xa1\xe5\x8f\xb7\xe5\xbc\xba\xe5\xba\xa6\xe8\xb6\x8a\xe5\xa4\xa7\xe3\x80\x82\xe6\x88\x91\xe4\xbb\xac\xe7\x9a\x84\xe8\xa7\xa3\xe5\x86\xb3\xe6\x96\xb9\xe6\xa1\x88\xe6\x98\xaf\xe5\xb0\x86\xe7\xb1\xbb\xe5\x9e\x8b\xe5\x8f\x98\xe9\x87\x8f\xe7\x94\xa8\xe4\xb8\x80\xe4\xb8\xaa\xe2\x80\x9c\xe4\xb8\x80\xe4\xbd\x8d\xe7\x83\xad\xe7\xa0\x81\xe2\x80\x9c\xef\xbc\x88one-hot\xef\xbc\x89\xe6\x9d\xa5\xe7\xbc\x96\xe7\xa0\x81\xef\xbc\x8c\xe4\xb9\x9f\xe5\xb0\xb1\xe6\x98\xaf\xef\xbc\x9a\n# \n# $\n# season = 1 \\rightarrow (1, 0, 0 ,0) \\\\\n# season = 2 \\rightarrow (0, 1, 0, 0) \\\\\n# season = 3 \\rightarrow (0, 0, 1, 0) \\\\\n# season = 4 \\rightarrow (0, 0, 0, 1) \\\\\n# $\n# \n# \xe5\x9b\xa0\xe6\xad\xa4\xef\xbc\x8c\xe5\xa6\x82\xe6\x9e\x9c\xe4\xb8\x80\xe4\xb8\xaa\xe7\xb1\xbb\xe5\x9e\x8b\xe5\x8f\x98\xe9\x87\x8f\xe6\x9c\x89n\xe4\xb8\xaa\xe4\xb8\x8d\xe5\x90\x8c\xe5\x8f\x96\xe5\x80\xbc\xef\xbc\x8c\xe9\x82\xa3\xe4\xb9\x88\xe6\x88\x91\xe4\xbb\xac\xe7\x9a\x84\xe2\x80\x9c\xe4\xb8\x80\xe4\xbd\x8d\xe7\x83\xad\xe7\xa0\x81\xe2\x80\x9c\xe6\x89\x80\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84\xe5\x90\x91\xe9\x87\x8f\xe9\x95\xbf\xe5\xba\xa6\xe5\xb0\xb1\xe4\xb8\xban\n\n#%%\n#\xe5\xaf\xb9\xe4\xba\x8e\xe7\xb1\xbb\xe5\x9e\x8b\xe5\x8f\x98\xe9\x87\x8f\xe7\x9a\x84\xe7\x89\xb9\xe6\xae\x8a\xe5\xa4\x84\xe7\x90\x86\n# season=1,2,3,4, weathersi=1,2,3, mnth= 1,2,...,12, hr=0,1, ...,23, weekday=0,1,...,6\n# \xe7\xbb\x8f\xe8\xbf\x87\xe4\xb8\x8b\xe9\x9d\xa2\xe7\x9a\x84\xe5\xa4\x84\xe7\x90\x86\xe5\x90\x8e\xef\xbc\x8c\xe5\xb0\x86\xe4\xbc\x9a\xe5\xa4\x9a\xe5\x87\xba\xe8\x8b\xa5\xe5\xb9\xb2\xe7\x89\xb9\xe5\xbe\x81\xef\xbc\x8c\xe4\xbe\x8b\xe5\xa6\x82\xef\xbc\x8c\xe5\xaf\xb9\xe4\xba\x8eseason\xe5\x8f\x98\xe9\x87\x8f\xe5\xb0\xb1\xe4\xbc\x9a\xe6\x9c\x89 season_1, season_2, season_3, season_4\n# \xe8\xbf\x99\xe5\x9b\x9b\xe7\xa7\x8d\xe4\xb8\x8d\xe5\x90\x8c\xe7\x9a\x84\xe7\x89\xb9\xe5\xbe\x81\xe3\x80\x82\ndummy_fields = [\'season\', \'weathersit\', \'mnth\', \'hr\', \'weekday\']\nfor each in dummy_fields:\n    #\xe5\x88\xa9\xe7\x94\xa8pandas\xe5\xaf\xb9\xe8\xb1\xa1\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe5\x8f\xaf\xe4\xbb\xa5\xe5\xbe\x88\xe6\x96\xb9\xe4\xbe\xbf\xe5\x9c\xb0\xe5\xb0\x86\xe4\xb8\x80\xe4\xb8\xaa\xe7\xb1\xbb\xe5\x9e\x8b\xe5\x8f\x98\xe9\x87\x8f\xe5\xb1\x9e\xe6\x80\xa7\xe8\xbf\x9b\xe8\xa1\x8cone-hot\xe7\xbc\x96\xe7\xa0\x81\xef\xbc\x8c\xe5\x8f\x98\xe6\x88\x90\xe5\xa4\x9a\xe4\xb8\xaa\xe5\xb1\x9e\xe6\x80\xa7\n    dummies = pd.get_dummies(rides[each], prefix=each, drop_first=False)\n    rides = pd.concat([rides, dummies], axis=1)\n\n# \xe6\x8a\x8a\xe5\x8e\x9f\xe6\x9c\x89\xe7\x9a\x84\xe7\xb1\xbb\xe5\x9e\x8b\xe5\x8f\x98\xe9\x87\x8f\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84\xe7\x89\xb9\xe5\xbe\x81\xe5\x8e\xbb\xe6\x8e\x89\xef\xbc\x8c\xe5\xb0\x86\xe4\xb8\x80\xe4\xba\x9b\xe4\xb8\x8d\xe7\x9b\xb8\xe5\x85\xb3\xe7\x9a\x84\xe7\x89\xb9\xe5\xbe\x81\xe5\x8e\xbb\xe6\x8e\x89\nfields_to_drop = [\'instant\', \'dteday\', \'season\', \'weathersit\', \n                  \'weekday\', \'atemp\', \'mnth\', \'workingday\', \'hr\']\ndata = rides.drop(fields_to_drop, axis=1)\ndata.head()\n\n#%% [markdown]\n# #### b. \xe5\xaf\xb9\xe4\xba\x8e\xe6\x95\xb0\xe5\x80\xbc\xe7\xb1\xbb\xe5\x9e\x8b\xe5\x8f\x98\xe9\x87\x8f\xe8\xbf\x9b\xe8\xa1\x8c\xe6\xa0\x87\xe5\x87\x86\xe5\x8c\x96\n# \xe7\x94\xb1\xe4\xba\x8e\xe6\xaf\x8f\xe4\xb8\xaa\xe6\x95\xb0\xe5\x80\xbc\xe5\x9e\x8b\xe5\x8f\x98\xe9\x87\x8f\xe9\x83\xbd\xe6\x98\xaf\xe7\x9b\xb8\xe4\xba\x92\xe7\x8b\xac\xe7\xab\x8b\xe7\x9a\x84\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe5\xae\x83\xe4\xbb\xac\xe7\x9a\x84\xe6\x95\xb0\xe5\x80\xbc\xe7\xbb\x9d\xe5\xaf\xb9\xe5\xa4\xa7\xe5\xb0\x8f\xe4\xb8\x8e\xe9\x97\xae\xe9\xa2\x98\xe6\x9c\xac\xe8\xba\xab\xe6\xb2\xa1\xe6\x9c\x89\xe5\x85\xb3\xe7\xb3\xbb\xef\xbc\x8c\xe4\xb8\xba\xe4\xba\x86\xe6\xb6\x88\xe9\x99\xa4\xe6\x95\xb0\xe5\x80\xbc\xe5\xa4\xa7\xe5\xb0\x8f\xe7\x9a\x84\xe5\xb7\xae\xe5\xbc\x82\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe5\xaf\xb9\xe6\xaf\x8f\xe4\xb8\x80\xe4\xb8\xaa\xe6\x95\xb0\xe5\x80\xbc\xe5\x9e\x8b\xe5\x8f\x98\xe9\x87\x8f\xe8\xbf\x9b\xe8\xa1\x8c\xe6\xa0\x87\xe5\x87\x86\xe5\x8c\x96\xe5\xa4\x84\xe7\x90\x86\xef\xbc\x8c\xe4\xb9\x9f\xe5\xb0\xb1\xe6\x98\xaf\xe8\xae\xa9\xe5\x85\xb6\xe6\x95\xb0\xe5\x80\xbc\xe9\x83\xbd\xe5\x9b\xb4\xe7\xbb\x95\xe7\x9d\x800\xe5\xb7\xa6\xe5\x8f\xb3\xe6\xb3\xa2\xe5\x8a\xa8\xe3\x80\x82\xe6\xaf\x94\xe5\xa6\x82\xef\xbc\x8c\xe5\xaf\xb9\xe4\xba\x8e\xe6\xb8\xa9\xe5\xba\xa6temp\xe8\xbf\x99\xe4\xb8\xaa\xe5\x8f\x98\xe9\x87\x8f\xe6\x9d\xa5\xe8\xaf\xb4\xef\xbc\x8c\xe5\xae\x83\xe5\x9c\xa8\xe6\x95\xb4\xe4\xb8\xaa\xe6\x95\xb0\xe6\x8d\xae\xe5\xba\x93\xe5\x8f\x96\xe5\x80\xbc\xe7\x9a\x84\xe5\xb9\xb3\xe5\x9d\x87\xe7\x9d\x80\xe4\xb8\xbamean(temp), \xe6\x96\xb9\xe5\xb7\xae\xe4\xb8\xbastd(temp)\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xef\xbc\x8c\xe5\xbd\x92\xe4\xb8\x80\xe5\x8c\x96\xe7\x9a\x84\xe6\xb8\xa9\xe5\xba\xa6\xe8\xae\xa1\xe7\xae\x97\xe4\xb8\xba\xef\xbc\x9a\n# \n# $ temp\'=\\frac{temp - mean(temp)}{std(temp)}$\n# \n# \xe8\xbf\x99\xe6\xa0\xb7\xe5\x81\x9a\xe7\x9a\x84\xe5\xa5\xbd\xe5\xa4\x84\xe5\xb0\xb1\xe6\x98\xaf\xe5\x8f\xaf\xe4\xbb\xa5\xe5\xb0\x86\xe4\xb8\x8d\xe5\x90\x8c\xe7\x9a\x84\xe5\x8f\x96\xe5\x80\xbc\xe8\x8c\x83\xe5\x9b\xb4\xe7\x9a\x84\xe5\x8f\x98\xe9\x87\x8f\xe8\xae\xbe\xe7\xbd\xae\xe4\xb8\xba\xe8\xae\xa9\xe5\xae\x83\xe4\xbb\xac\xe5\xa4\x84\xe4\xba\x8e\xe4\xb8\x80\xe4\xb8\xaa\xe5\xb9\xb3\xe7\xad\x89\xe7\x9a\x84\xe5\x9c\xb0\xe4\xbd\x8d\xe3\x80\x82\n\n#%%\n# \xe8\xb0\x83\xe6\x95\xb4\xe6\x89\x80\xe6\x9c\x89\xe7\x9a\x84\xe7\x89\xb9\xe5\xbe\x81\xef\xbc\x8c\xe6\xa0\x87\xe5\x87\x86\xe5\x8c\x96\xe5\xa4\x84\xe7\x90\x86\nquant_features = [\'cnt\', \'temp\', \'hum\', \'windspeed\']\n#quant_features = [\'temp\', \'hum\', \'windspeed\']\n\n# \xe6\x88\x91\xe4\xbb\xac\xe5\xb0\x86\xe6\xaf\x8f\xe4\xb8\x80\xe4\xb8\xaa\xe5\x8f\x98\xe9\x87\x8f\xe7\x9a\x84\xe5\x9d\x87\xe5\x80\xbc\xe5\x92\x8c\xe6\x96\xb9\xe5\xb7\xae\xe9\x83\xbd\xe5\xad\x98\xe5\x82\xa8\xe5\x88\xb0scaled_features\xe5\x8f\x98\xe9\x87\x8f\xe4\xb8\xad\xe3\x80\x82\nscaled_features = {}\nfor each in quant_features:\n    mean, std = data[each].mean(), data[each].std()\n    scaled_features[each] = [mean, std]\n    data.loc[:, each] = (data[each] - mean)/std\n\n#%% [markdown]\n# #### c. \xe5\xb0\x86\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe8\xbf\x9b\xe8\xa1\x8c\xe5\x88\x86\xe5\x89\xb2\n\n#%%\n# \xe5\xb0\x86\xe6\x89\x80\xe6\x9c\x89\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe5\x88\x86\xe4\xb8\xba\xe6\xb5\x8b\xe8\xaf\x95\xe9\x9b\x86\xe5\x92\x8c\xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe4\xbb\xa5\xe5\x90\x8e21\xe5\xa4\xa9\xe6\x95\xb0\xe6\x8d\xae\xe4\xb8\x80\xe5\x85\xb121*24\xe4\xb8\xaa\xe6\x95\xb0\xe6\x8d\xae\xe7\x82\xb9\xe4\xbd\x9c\xe4\xb8\xba\xe6\xb5\x8b\xe8\xaf\x95\xe9\x9b\x86\xef\xbc\x8c\xe5\x85\xb6\xe5\xae\x83\xe6\x98\xaf\xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\ntest_data = data[-21*24:]\ntrain_data = data[:-21*24]\nprint(\'\xe8\xae\xad\xe7\xbb\x83\xe6\x95\xb0\xe6\x8d\xae\xef\xbc\x9a\',len(train_data),\'\xe6\xb5\x8b\xe8\xaf\x95\xe6\x95\xb0\xe6\x8d\xae\xef\xbc\x9a\',len(test_data))\n\n# \xe5\xb0\x86\xe6\x88\x91\xe4\xbb\xac\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe5\x88\x97\xe5\x88\x86\xe4\xb8\xba\xe7\x89\xb9\xe5\xbe\x81\xe5\x88\x97\xe5\x92\x8c\xe7\x9b\xae\xe6\xa0\x87\xe5\x88\x97\n\n#\xe7\x9b\xae\xe6\xa0\x87\xe5\x88\x97\ntarget_fields = [\'cnt\', \'casual\', \'registered\']\nfeatures, targets = train_data.drop(target_fields, axis=1), train_data[target_fields]\ntest_features, test_targets = test_data.drop(target_fields, axis=1), test_data[target_fields]\n\n# \xe5\xb0\x86\xe6\x95\xb0\xe6\x8d\xae\xe4\xbb\x8epandas dataframe\xe8\xbd\xac\xe6\x8d\xa2\xe4\xb8\xbanumpy\nX = features.values\nY = targets[\'cnt\'].values\nY = Y.astype(float)\n\nY = np.reshape(Y, [len(Y),1])\nlosses = []\n\n\n#%%\nfeatures.head()\n\n#%% [markdown]\n# ### 2. \xe6\x9e\x84\xe5\xbb\xba\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xe5\xb9\xb6\xe8\xbf\x9b\xe8\xa1\x8c\xe8\xae\xad\xe7\xbb\x83\n#%% [markdown]\n# #### a. \xe6\x89\x8b\xe5\x8a\xa8\xe7\xbc\x96\xe5\x86\x99\xe7\x94\xa8Tensor\xe8\xbf\x90\xe7\xae\x97\xe7\x9a\x84\xe4\xba\xba\xe5\xb7\xa5\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\n\n#%%\n# \xe5\xae\x9a\xe4\xb9\x89\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xe6\x9e\xb6\xe6\x9e\x84\xef\xbc\x8cfeatures.shape[1]\xe4\xb8\xaa\xe8\xbe\x93\xe5\x85\xa5\xe5\xb1\x82\xe5\x8d\x95\xe5\x85\x83\xef\xbc\x8c10\xe4\xb8\xaa\xe9\x9a\x90\xe5\x90\xab\xe5\xb1\x82\xef\xbc\x8c1\xe4\xb8\xaa\xe8\xbe\x93\xe5\x87\xba\xe5\xb1\x82\ninput_size = features.shape[1] #\xe8\xbe\x93\xe5\x85\xa5\xe5\xb1\x82\xe5\x8d\x95\xe5\x85\x83\xe4\xb8\xaa\xe6\x95\xb0\nhidden_size = 10 #\xe9\x9a\x90\xe5\x90\xab\xe5\xb1\x82\xe5\x8d\x95\xe5\x85\x83\xe4\xb8\xaa\xe6\x95\xb0\noutput_size = 1 #\xe8\xbe\x93\xe5\x87\xba\xe5\xb1\x82\xe5\x8d\x95\xe5\x85\x83\xe4\xb8\xaa\xe6\x95\xb0\nbatch_size = 128 #\xe6\xaf\x8f\xe9\x9a\x94batch\xe7\x9a\x84\xe8\xae\xb0\xe5\xbd\x95\xe6\x95\xb0\nweights1 = torch.randn(([input_size, hidden_size]), requires_grad = True) #\xe7\xac\xac\xe4\xb8\x80\xe5\x88\xb0\xe4\xba\x8c\xe5\xb1\x82\xe6\x9d\x83\xe9\x87\x8d\nbiases1 = torch.randn(([hidden_size]), requires_grad = True) #\xe9\x9a\x90\xe5\x90\xab\xe5\xb1\x82\xe5\x81\x8f\xe7\xbd\xae\nweights2 = torch.randn(([hidden_size, output_size]), requires_grad = True) #\xe9\x9a\x90\xe5\x90\xab\xe5\xb1\x82\xe5\x88\xb0\xe8\xbe\x93\xe5\x87\xba\xe5\xb1\x82\xe6\x9d\x83\xe9\x87\x8d\ndef neu(x):\n    #\xe8\xae\xa1\xe7\xae\x97\xe9\x9a\x90\xe5\x90\xab\xe5\xb1\x82\xe8\xbe\x93\xe5\x87\xba\n    #x\xe4\xb8\xbabatch_size * input_size\xe7\x9a\x84\xe7\x9f\xa9\xe9\x98\xb5\xef\xbc\x8cweights1\xe4\xb8\xbainput_size*hidden_size\xe7\x9f\xa9\xe9\x98\xb5\xef\xbc\x8c\n    #biases\xe4\xb8\xbahidden_size\xe5\x90\x91\xe9\x87\x8f\xef\xbc\x8c\xe8\xbe\x93\xe5\x87\xba\xe4\xb8\xbabatch_size * hidden_size\xe7\x9f\xa9\xe9\x98\xb5    \n    hidden = x.mm(weights1) + biases1.expand(x.size()[0], hidden_size)\n    hidden = torch.sigmoid(hidden)\n    \n    #\xe8\xbe\x93\xe5\x85\xa5batch_size * hidden_size\xe7\x9f\xa9\xe9\x98\xb5\xef\xbc\x8cmm\xe4\xb8\x8aweights2, hidden_size*output_size\xe7\x9f\xa9\xe9\x98\xb5\xef\xbc\x8c\n    #\xe8\xbe\x93\xe5\x87\xbabatch_size*output_size\xe7\x9f\xa9\xe9\x98\xb5\n    output = hidden.mm(weights2)\n    return output\ndef cost(x, y):\n    # \xe8\xae\xa1\xe7\xae\x97\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x87\xbd\xe6\x95\xb0\n    error = torch.mean((x - y)**2)\n    return error\ndef zero_grad():\n    # \xe6\xb8\x85\xe7\xa9\xba\xe6\xaf\x8f\xe4\xb8\xaa\xe5\x8f\x82\xe6\x95\xb0\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\xe4\xbf\xa1\xe6\x81\xaf\n    if weights1.grad is not None and biases1.grad is not None and weights2.grad is not None:\n        weights1.grad.data.zero_()\n        weights2.grad.data.zero_()\n        biases1.grad.data.zero_()\ndef optimizer_step(learning_rate):\n    # \xe6\xa2\xaf\xe5\xba\xa6\xe4\xb8\x8b\xe9\x99\x8d\xe7\xae\x97\xe6\xb3\x95\n    weights1.data.add_(- learning_rate * weights1.grad.data)\n    weights2.data.add_(- learning_rate * weights2.grad.data)\n    biases1.data.add_(- learning_rate * biases1.grad.data)\n\n\n#%%\n# \xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xe8\xae\xad\xe7\xbb\x83\xe5\xbe\xaa\xe7\x8e\xaf\nlosses = []\nfor i in range(1000):\n    # \xe6\xaf\x8f128\xe4\xb8\xaa\xe6\xa0\xb7\xe6\x9c\xac\xe7\x82\xb9\xe8\xa2\xab\xe5\x88\x92\xe5\x88\x86\xe4\xb8\xba\xe4\xb8\x80\xe4\xb8\xaa\xe6\x92\xae\xef\xbc\x8c\xe5\x9c\xa8\xe5\xbe\xaa\xe7\x8e\xaf\xe7\x9a\x84\xe6\x97\xb6\xe5\x80\x99\xe4\xb8\x80\xe6\x89\xb9\xe4\xb8\x80\xe6\x89\xb9\xe5\x9c\xb0\xe8\xaf\xbb\xe5\x8f\x96\n    batch_loss = []\n    # start\xe5\x92\x8cend\xe5\x88\x86\xe5\x88\xab\xe6\x98\xaf\xe6\x8f\x90\xe5\x8f\x96\xe4\xb8\x80\xe4\xb8\xaabatch\xe6\x95\xb0\xe6\x8d\xae\xe7\x9a\x84\xe8\xb5\xb7\xe5\xa7\x8b\xe5\x92\x8c\xe7\xbb\x88\xe6\xad\xa2\xe4\xb8\x8b\xe6\xa0\x87\n    for start in range(0, len(X), batch_size):\n        end = start + batch_size if start + batch_size < len(X) else len(X)\n        xx = torch.FloatTensor(X[start:end])\n        yy = torch.FloatTensor(Y[start:end])\n        predict = neu(xx)\n        loss = cost(predict, yy)\n        zero_grad()\n        loss.backward()\n        optimizer_step(0.01)\n        batch_loss.append(loss.data.numpy())\n    \n    # \xe6\xaf\x8f\xe9\x9a\x94100\xe6\xad\xa5\xe8\xbe\x93\xe5\x87\xba\xe4\xb8\x80\xe4\xb8\x8b\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x80\xbc\xef\xbc\x88loss\xef\xbc\x89\n    if i % 100==0:\n        losses.append(np.mean(batch_loss))\n        print(i, np.mean(batch_loss))\n\n\n#%%\n# \xe6\x89\x93\xe5\x8d\xb0\xe8\xbe\x93\xe5\x87\xba\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x80\xbc\nfig = plt.figure(figsize=(10, 7))\nplt.plot(np.arange(len(losses))*100,losses, \'o-\')\nplt.xlabel(\'epoch\')\nplt.ylabel(\'MSE\')\n\n#%% [markdown]\n# #### b. \xe8\xb0\x83\xe7\x94\xa8PyTorch\xe7\x8e\xb0\xe6\x88\x90\xe7\x9a\x84\xe5\x87\xbd\xe6\x95\xb0\xef\xbc\x8c\xe6\x9e\x84\xe5\xbb\xba\xe5\xba\x8f\xe5\x88\x97\xe5\x8c\x96\xe7\x9a\x84\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\n\n#%%\n# \xe5\xae\x9a\xe4\xb9\x89\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xe6\x9e\xb6\xe6\x9e\x84\xef\xbc\x8cfeatures.shape[1]\xe4\xb8\xaa\xe8\xbe\x93\xe5\x85\xa5\xe5\xb1\x82\xe5\x8d\x95\xe5\x85\x83\xef\xbc\x8c10\xe4\xb8\xaa\xe9\x9a\x90\xe5\x90\xab\xe5\xb1\x82\xef\xbc\x8c1\xe4\xb8\xaa\xe8\xbe\x93\xe5\x87\xba\xe5\xb1\x82\ninput_size = features.shape[1]\nhidden_size = 10\noutput_size = 1\nbatch_size = 128\nneu = torch.nn.Sequential(\n    torch.nn.Linear(input_size, hidden_size),\n    torch.nn.Sigmoid(),\n    torch.nn.Linear(hidden_size, output_size),\n)\ncost = torch.nn.MSELoss()\noptimizer = torch.optim.SGD(neu.parameters(), lr = 0.01)\n\n\n#%%\n# \xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xe8\xae\xad\xe7\xbb\x83\xe5\xbe\xaa\xe7\x8e\xaf\nlosses = []\nfor i in range(1000):\n    # \xe6\xaf\x8f128\xe4\xb8\xaa\xe6\xa0\xb7\xe6\x9c\xac\xe7\x82\xb9\xe8\xa2\xab\xe5\x88\x92\xe5\x88\x86\xe4\xb8\xba\xe4\xb8\x80\xe4\xb8\xaa\xe6\x92\xae\xef\xbc\x8c\xe5\x9c\xa8\xe5\xbe\xaa\xe7\x8e\xaf\xe7\x9a\x84\xe6\x97\xb6\xe5\x80\x99\xe4\xb8\x80\xe6\x89\xb9\xe4\xb8\x80\xe6\x89\xb9\xe5\x9c\xb0\xe8\xaf\xbb\xe5\x8f\x96\n    batch_loss = []\n    # start\xe5\x92\x8cend\xe5\x88\x86\xe5\x88\xab\xe6\x98\xaf\xe6\x8f\x90\xe5\x8f\x96\xe4\xb8\x80\xe4\xb8\xaabatch\xe6\x95\xb0\xe6\x8d\xae\xe7\x9a\x84\xe8\xb5\xb7\xe5\xa7\x8b\xe5\x92\x8c\xe7\xbb\x88\xe6\xad\xa2\xe4\xb8\x8b\xe6\xa0\x87\n    for start in range(0, len(X), batch_size):\n        end = start + batch_size if start + batch_size < len(X) else len(X)\n        xx = torch.FloatTensor(X[start:end])\n        yy = torch.FloatTensor(Y[start:end])\n        predict = neu(xx)\n        loss = cost(predict, yy)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        batch_loss.append(loss.data.numpy())\n    \n    # \xe6\xaf\x8f\xe9\x9a\x94100\xe6\xad\xa5\xe8\xbe\x93\xe5\x87\xba\xe4\xb8\x80\xe4\xb8\x8b\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x80\xbc\xef\xbc\x88loss\xef\xbc\x89\n    if i % 100==0:\n        losses.append(np.mean(batch_loss))\n        print(i, np.mean(batch_loss))\n\n\n#%%\n# \xe6\x89\x93\xe5\x8d\xb0\xe8\xbe\x93\xe5\x87\xba\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x80\xbc\nfig = plt.figure(figsize=(10, 7))\nplt.plot(np.arange(len(losses))*100,losses, \'o-\')\nplt.xlabel(\'epoch\')\nplt.ylabel(\'MSE\')\n\n#%% [markdown]\n# ### 3. \xe6\xb5\x8b\xe8\xaf\x95\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\n\n#%%\n# \xe7\x94\xa8\xe8\xae\xad\xe7\xbb\x83\xe5\xa5\xbd\xe7\x9a\x84\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xe5\x9c\xa8\xe6\xb5\x8b\xe8\xaf\x95\xe9\x9b\x86\xe4\xb8\x8a\xe8\xbf\x9b\xe8\xa1\x8c\xe9\xa2\x84\xe6\xb5\x8b\ntargets = test_targets[\'cnt\'] #\xe8\xaf\xbb\xe5\x8f\x96\xe6\xb5\x8b\xe8\xaf\x95\xe9\x9b\x86\xe7\x9a\x84cnt\xe6\x95\xb0\xe5\x80\xbc\ntargets = targets.values.reshape([len(targets),1]) #\xe5\xb0\x86\xe6\x95\xb0\xe6\x8d\xae\xe8\xbd\xac\xe6\x8d\xa2\xe6\x88\x90\xe5\x90\x88\xe9\x80\x82\xe7\x9a\x84tensor\xe5\xbd\xa2\xe5\xbc\x8f\ntargets = targets.astype(float) #\xe4\xbf\x9d\xe8\xaf\x81\xe6\x95\xb0\xe6\x8d\xae\xe4\xb8\xba\xe5\xae\x9e\xe6\x95\xb0\n\n# \xe5\xb0\x86\xe5\xb1\x9e\xe6\x80\xa7\xe5\x92\x8c\xe9\xa2\x84\xe6\xb5\x8b\xe5\x8f\x98\xe9\x87\x8f\xe5\x8c\x85\xe8\xa3\xb9\xe5\x9c\xa8Variable\xe5\x9e\x8b\xe5\x8f\x98\xe9\x87\x8f\xe4\xb8\xad\nx = torch.FloatTensor(test_features.values)\ny = torch.FloatTensor(targets)\n\n# \xe7\x94\xa8\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xe8\xbf\x9b\xe8\xa1\x8c\xe9\xa2\x84\xe6\xb5\x8b\npredict = neu(x)\npredict = predict.data.numpy()\n\n\n# \xe5\xb0\x86\xe5\x90\x8e21\xe5\xa4\xa9\xe7\x9a\x84\xe9\xa2\x84\xe6\xb5\x8b\xe6\x95\xb0\xe6\x8d\xae\xe4\xb8\x8e\xe7\x9c\x9f\xe5\xae\x9e\xe6\x95\xb0\xe6\x8d\xae\xe7\x94\xbb\xe5\x9c\xa8\xe4\xb8\x80\xe8\xb5\xb7\xe5\xb9\xb6\xe6\xaf\x94\xe8\xbe\x83\n# \xe6\xa8\xaa\xe5\x9d\x90\xe6\xa0\x87\xe8\xbd\xb4\xe6\x98\xaf\xe4\xb8\x8d\xe5\x90\x8c\xe7\x9a\x84\xe6\x97\xa5\xe6\x9c\x9f\xef\xbc\x8c\xe7\xba\xb5\xe5\x9d\x90\xe6\xa0\x87\xe8\xbd\xb4\xe6\x98\xaf\xe9\xa2\x84\xe6\xb5\x8b\xe6\x88\x96\xe8\x80\x85\xe7\x9c\x9f\xe5\xae\x9e\xe6\x95\xb0\xe6\x8d\xae\xe7\x9a\x84\xe5\x80\xbc\nfig, ax = plt.subplots(figsize = (10, 7))\n\nmean, std = scaled_features[\'cnt\']\nax.plot(predict * std + mean, label=\'Prediction\', linestyle = \'--\')\nax.plot(targets * std + mean, label=\'Data\', linestyle = \'-\')\nax.legend()\nax.set_xlabel(\'Date-time\')\nax.set_ylabel(\'Counts\')\n# \xe5\xaf\xb9\xe6\xa8\xaa\xe5\x9d\x90\xe6\xa0\x87\xe8\xbd\xb4\xe8\xbf\x9b\xe8\xa1\x8c\xe6\xa0\x87\xe6\xb3\xa8\ndates = pd.to_datetime(rides.loc[test_data.index][\'dteday\'])\ndates = dates.apply(lambda d: d.strftime(\'%b %d\'))\nax.set_xticks(np.arange(len(dates))[12::24])\n_ = ax.set_xticklabels(dates[12::24], rotation=45)\n\n#%% [markdown]\n# ### 4. \xe8\xaf\x8a\xe6\x96\xad\xe7\xbd\x91\xe7\xbb\x9c*\n# \n# \xe5\x9c\xa8\xe8\xbf\x99\xe4\xb8\x80\xe5\xb0\x8f\xe8\x8a\x82\xe6\x88\x91\xe4\xbb\xac\xe5\xaf\xb9\xe7\xbd\x91\xe7\xbb\x9c\xe5\x87\xba\xe7\x8e\xb0\xe7\x9a\x84\xe9\x97\xae\xe9\xa2\x98\xe8\xbf\x9b\xe8\xa1\x8c\xe8\xaf\x8a\xe6\x96\xad\xef\xbc\x8c\xe7\x9c\x8b\xe7\x9c\x8b\xe5\x93\xaa\xe4\xb8\x80\xe4\xba\x9b\xe7\xa5\x9e\xe7\xbb\x8f\xe5\x85\x83\xe5\xaf\xbc\xe8\x87\xb4\xe4\xba\x86\xe9\xa2\x84\xe6\xb5\x8b\xe5\x81\x8f\xe5\xb7\xae\n\n#%%\n# \xe9\x80\x89\xe5\x87\xba\xe4\xb8\x89\xe5\xa4\xa9\xe9\xa2\x84\xe6\xb5\x8b\xe4\xb8\x8d\xe5\x87\x86\xe7\x9a\x84\xe6\x97\xa5\xe6\x9c\x9f\xef\xbc\x9aDec 22\xef\xbc\x8c23\xef\xbc\x8c24\n# \xe5\xb0\x86\xe8\xbf\x99\xe4\xb8\x89\xe5\xa4\xa9\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe8\x81\x9a\xe9\x9b\x86\xe5\x88\xb0\xe4\xb8\x80\xe8\xb5\xb7\xef\xbc\x8c\xe5\xad\x98\xe5\x85\xa5subset\xe5\x92\x8csubtargets\xe4\xb8\xad\nbool1 = rides[\'dteday\'] == \'2012-12-22\'\nbool2 = rides[\'dteday\'] == \'2012-12-23\'\nbool3 = rides[\'dteday\'] == \'2012-12-24\'\n\n# \xe5\xb0\x86\xe4\xb8\x89\xe4\xb8\xaa\xe5\xb8\x83\xe5\xb0\x94\xe5\x9e\x8b\xe6\x95\xb0\xe7\xbb\x84\xe6\xb1\x82\xe4\xb8\x8e\nbools = [any(tup) for tup in zip(bool1,bool2,bool3) ]\n# \xe5\xb0\x86\xe7\x9b\xb8\xe5\xba\x94\xe7\x9a\x84\xe5\x8f\x98\xe9\x87\x8f\xe5\x8f\x96\xe5\x87\xba\xe6\x9d\xa5\nsubset = test_features.loc[rides[bools].index]\nsubtargets = test_targets.loc[rides[bools].index]\nsubtargets = subtargets[\'cnt\']\nsubtargets = subtargets.values.reshape([len(subtargets),1])\n\ndef feature(X, net):\n    # \xe5\xae\x9a\xe4\xb9\x89\xe4\xba\x86\xe4\xb8\x80\xe4\xb8\xaa\xe5\x87\xbd\xe6\x95\xb0\xe5\x8f\xaf\xe4\xbb\xa5\xe6\x8f\x90\xe5\x8f\x96\xe7\xbd\x91\xe7\xbb\x9c\xe7\x9a\x84\xe6\x9d\x83\xe9\x87\x8d\xe4\xbf\xa1\xe6\x81\xaf\xef\xbc\x8c\xe6\x89\x80\xe6\x9c\x89\xe7\x9a\x84\xe7\xbd\x91\xe7\xbb\x9c\xe5\x8f\x82\xe6\x95\xb0\xe4\xbf\xa1\xe6\x81\xaf\xe5\x85\xa8\xe9\x83\xa8\xe5\xad\x98\xe5\x82\xa8\xe5\x9c\xa8\xe4\xba\x86neu\xe7\x9a\x84named_parameters\xe9\x9b\x86\xe5\x90\x88\xe4\xb8\xad\xe4\xba\x86\n    X = torch.from_numpy(X).type(torch.FloatTensor)\n    dic = dict(net.named_parameters()) #\xe6\x8f\x90\xe5\x8f\x96\xe5\x87\xba\xe6\x9d\xa5\xe8\xbf\x99\xe4\xb8\xaa\xe9\x9b\x86\xe5\x90\x88\n    weights = dic[\'0.weight\'] #\xe5\x8f\xaf\xe4\xbb\xa5\xe6\x8c\x89\xe7\x85\xa7\xe5\xb1\x82\xe6\x95\xb0.\xe5\x90\x8d\xe7\xa7\xb0\xe6\x9d\xa5\xe7\xb4\xa2\xe5\xbc\x95\xe9\x9b\x86\xe5\x90\x88\xe4\xb8\xad\xe7\x9a\x84\xe7\x9b\xb8\xe5\xba\x94\xe5\x8f\x82\xe6\x95\xb0\xe5\x80\xbc\n    biases = dic[\'0.bias\'] #\xe5\x8f\xaf\xe4\xbb\xa5\xe6\x8c\x89\xe7\x85\xa7\xe5\xb1\x82\xe6\x95\xb0.\xe5\x90\x8d\xe7\xa7\xb0\xe6\x9d\xa5\xe7\xb4\xa2\xe5\xbc\x95\xe9\x9b\x86\xe5\x90\x88\xe4\xb8\xad\xe7\x9a\x84\xe7\x9b\xb8\xe5\xba\x94\xe5\x8f\x82\xe6\x95\xb0\xe5\x80\xbc\n    h = torch.sigmoid(X.mm(weights.t()) + biases.expand([len(X), len(biases)])) # \xe9\x9a\x90\xe5\x90\xab\xe5\xb1\x82\xe7\x9a\x84\xe8\xae\xa1\xe7\xae\x97\xe8\xbf\x87\xe7\xa8\x8b\n    return h # \xe8\xbe\x93\xe5\x87\xba\xe5\xb1\x82\xe7\x9a\x84\xe8\xae\xa1\xe7\xae\x97\n\n# \xe5\xb0\x86\xe8\xbf\x99\xe5\x87\xa0\xe5\xa4\xa9\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe8\xbe\x93\xe5\x85\xa5\xe5\x88\xb0\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xe4\xb8\xad\xef\xbc\x8c\xe8\xaf\xbb\xe5\x8f\x96\xe5\x87\xba\xe9\x9a\x90\xe5\x90\xab\xe5\xb1\x82\xe7\xa5\x9e\xe7\xbb\x8f\xe5\x85\x83\xe7\x9a\x84\xe6\xbf\x80\xe6\xb4\xbb\xe6\x95\xb0\xe5\x80\xbc\xef\xbc\x8c\xe5\xad\x98\xe5\x85\xa5results\xe4\xb8\xad\nresults = feature(subset.values, neu).data.numpy()\n# \xe8\xbf\x99\xe4\xba\x9b\xe6\x95\xb0\xe6\x8d\xae\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84\xe9\xa2\x84\xe6\xb5\x8b\xe5\x80\xbc\xef\xbc\x88\xe8\xbe\x93\xe5\x87\xba\xe5\xb1\x82\xef\xbc\x89\npredict = neu(torch.FloatTensor(subset.values)).data.numpy()\n\n#\xe5\xb0\x86\xe9\xa2\x84\xe6\xb5\x8b\xe5\x80\xbc\xe8\xbf\x98\xe5\x8e\x9f\xe6\x88\x90\xe5\x8e\x9f\xe5\xa7\x8b\xe6\x95\xb0\xe6\x8d\xae\xe7\x9a\x84\xe6\x95\xb0\xe5\x80\xbc\xe8\x8c\x83\xe5\x9b\xb4\nmean, std = scaled_features[\'cnt\']\npredict = predict * std + mean\nsubtargets = subtargets * std + mean\n# \xe5\xb0\x86\xe6\x89\x80\xe6\x9c\x89\xe7\x9a\x84\xe7\xa5\x9e\xe7\xbb\x8f\xe5\x85\x83\xe6\xbf\x80\xe6\xb4\xbb\xe6\xb0\xb4\xe5\xb9\xb3\xe7\x94\xbb\xe5\x9c\xa8\xe5\x90\x8c\xe4\xb8\x80\xe5\xbc\xa0\xe5\x9b\xbe\xe4\xb8\x8a\xef\xbc\x8c\xe8\x93\x9d\xe8\x89\xb2\xe7\x9a\x84\xe6\x98\xaf\xe6\xa8\xa1\xe5\x9e\x8b\xe9\xa2\x84\xe6\xb5\x8b\xe7\x9a\x84\xe6\x95\xb0\xe5\x80\xbc\nfig, ax = plt.subplots(figsize = (8, 6))\nax.plot(results[:,:],\'.:\',alpha = 0.3)\nax.plot((predict - min(predict)) / (max(predict) - min(predict)),\'bs-\',label=\'Prediction\')\nax.plot((subtargets - min(predict)) / (max(predict) - min(predict)),\'ro-\',label=\'Real\')\nax.plot(results[:, 3],\':*\',alpha=1, label=\'Neuro 4\')\n\nax.set_xlim(right=len(predict))\nax.legend()\nplt.ylabel(\'Normalized Values\')\n\ndates = pd.to_datetime(rides.loc[subset.index][\'dteday\'])\ndates = dates.apply(lambda d: d.strftime(\'%b %d\'))\nax.set_xticks(np.arange(len(dates))[12::24])\n_ = ax.set_xticklabels(dates[12::24], rotation=45)\n\n\n#%%\n# \xe6\x89\xbe\xe5\x88\xb0\xe4\xba\x86\xe4\xb8\x8e\xe5\xb3\xb0\xe5\x80\xbc\xe7\x9b\xb8\xe5\xba\x94\xe7\x9a\x84\xe7\xa5\x9e\xe7\xbb\x8f\xe5\x85\x83\xef\xbc\x8c\xe6\x8a\x8a\xe5\xae\x83\xe5\x88\xb0\xe8\xbe\x93\xe5\x85\xa5\xe5\xb1\x82\xe7\x9a\x84\xe6\x9d\x83\xe9\x87\x8d\xe8\xbe\x93\xe5\x87\xba\xe5\x87\xba\xe6\x9d\xa5\ndic = dict(neu.named_parameters())\nweights = dic[\'2.weight\']\nplt.plot(weights.data.numpy()[0],\'o-\')\nplt.xlabel(\'Input Neurons\')\nplt.ylabel(\'Weight\')\n\n\n#%%\n# \xe6\x89\xbe\xe5\x88\xb0\xe4\xba\x86\xe4\xb8\x8e\xe5\xb3\xb0\xe5\x80\xbc\xe5\x93\x8d\xe5\xba\x94\xe7\x9a\x84\xe7\xa5\x9e\xe7\xbb\x8f\xe5\x85\x83\xef\xbc\x8c\xe6\x8a\x8a\xe5\xae\x83\xe5\x88\xb0\xe8\xbe\x93\xe5\x85\xa5\xe5\xb1\x82\xe7\x9a\x84\xe6\x9d\x83\xe9\x87\x8d\xe8\xbe\x93\xe5\x87\xba\xe5\x87\xba\xe6\x9d\xa5\ndic = dict(neu.named_parameters())\nweights = dic[\'0.weight\']\nplt.plot(weights.data.numpy(),\'o-\')\nplt.xlabel(\'Input Neurons\')\nplt.ylabel(\'Weight\')\n\n\n#%%\n# \xe5\x88\x97\xe5\x87\xba\xe6\x89\x80\xe6\x9c\x89\xe7\x9a\x84features\xe4\xb8\xad\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe5\x88\x97\xef\xbc\x8c\xe6\x89\xbe\xe5\x88\xb0\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84\xe7\xbc\x96\xe5\x8f\xb7\nfor (i, c) in zip(range(len(features.columns)), features.columns):\n    print(i,c)\n\n\n#%%\n# \xe6\x98\xbe\xe7\xa4\xba\xe5\x9c\xa8\xe4\xb8\x8d\xe5\x90\x8c\xe6\x97\xa5\xe6\x9c\x9f\xef\xbc\x8c\xe6\x8c\x87\xe5\xae\x9a\xe7\x9a\x84\xe7\xac\xac7\xe4\xb8\xaa\xe9\x9a\x90\xe5\x90\xab\xe5\xb1\x82\xe7\xa5\x9e\xe7\xbb\x8f\xe5\x85\x83\xe7\xbb\x86\xe8\x83\x9e\xe7\x9a\x84\xe6\xbf\x80\xe6\xb4\xbb\xe5\x80\xbc\xef\xbc\x8c\xe4\xbb\xa5\xe5\x8f\x8a\xe8\xbe\x93\xe5\x85\xa5\xe5\xb1\x82\xe5\x93\x8d\xe5\xba\x94\nfig, ax = plt.subplots(figsize = (10, 7))\nax.plot(results[:,6],label=\'neuron in hidden\')\nax.plot(subset.values[:,33],label=\'neuron in input at 8am\')\nax.plot(subset.values[:,42],label=\'neuron in input at 5pm\')\nax.set_xlim(right=len(predict))\nax.legend()\n\ndates = pd.to_datetime(rides.loc[subset.index][\'dteday\'])\ndates = dates.apply(lambda d: d.strftime(\'%b %d\'))\nax.set_xticks(np.arange(len(dates))[12::24])\n_ = ax.set_xticklabels(dates[12::24], rotation=45)\n\n#%% [markdown]\n# ## 4. \xe5\x88\x86\xe7\xb1\xbb\xe4\xba\xba\xe5\xb7\xa5\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9cNeuc\n# \n# \xe6\x9c\xac\xe5\xb0\x8f\xe8\x8a\x82\xe4\xb8\xad\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe8\xa7\xa3\xe5\x86\xb3\xe4\xb8\x80\xe4\xb8\xaa\xe5\x88\x86\xe7\xb1\xbb\xe9\x97\xae\xe9\xa2\x98\xef\xbc\x8c\xe5\x8d\xb3\xe5\xb0\x86\xe9\xa2\x84\xe6\xb5\x8b\xe6\x95\xb0\xe5\x80\xbc\xe6\xa0\xb9\xe6\x8d\xae\xe5\xa4\xa7\xe4\xba\x8e\xe6\x88\x96\xe8\x80\x85\xe5\xb0\x8f\xe4\xba\x8e\xe9\xa2\x84\xe6\xb5\x8b\xe6\x95\xb0\xe9\x87\x8f\xe7\x9a\x84\xe5\xb9\xb3\xe5\x9d\x87\xe5\x80\xbc\xe6\x9d\xa5\xe5\x88\x86\xe6\x88\x90\xe4\xb8\xa4\xe7\xb1\xbb\n# \xe6\x88\x91\xe4\xbb\xac\xe5\x8f\xaa\xe9\x9c\x80\xe8\xa6\x81\xe5\xaf\xb9Neuc\xe8\xbf\x9b\xe8\xa1\x8c\xe5\xb0\x8f\xe5\xb0\x8f\xe7\x9a\x84\xe6\x9b\xb4\xe6\x94\xb9\xef\xbc\x8c\xe5\xb0\x86\xe5\x85\xb6\xe8\xbe\x93\xe5\x87\xba\xe5\x8d\x95\xe5\x85\x83\xe6\x95\xb0\xe9\x87\x8f\xe8\xae\xbe\xe7\xbd\xae\xe4\xb8\xba2\xef\xbc\x8c\xe5\xb9\xb6\xe5\x8a\xa0\xe4\xb8\x8aSigmoid\xe5\x87\xbd\xe6\x95\xb0\xe5\xb0\xb1\xe5\x8f\xaf\xe4\xbb\xa5\xe4\xba\x86\n# \n# \xe5\xaf\xb9\xe4\xba\x8eNeuc\xe6\x9d\xa5\xe8\xaf\xb4\xef\xbc\x8c\xe5\xae\x83\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe6\x98\xaf\xe4\xb8\xa4\xe4\xb8\xaa\xe6\x95\xb0\xe5\x80\xbc\xef\xbc\x8c\xe5\x88\x86\xe5\x88\xab\xe8\xa1\xa8\xe7\xa4\xba\xe5\xb1\x9e\xe4\xba\x8e\xe7\xac\xac0\xe7\xb1\xbb\xe5\x92\x8c\xe7\xac\xac1\xe7\xb1\xbb\xe7\x9a\x84\xe6\xa6\x82\xe7\x8e\x87\n\n#%%\n# \xe9\x87\x8d\xe6\x96\xb0\xe6\x9e\x84\xe9\x80\xa0\xe7\x94\xa8\xe4\xba\x8e\xe5\x88\x86\xe7\xb1\xbb\xe7\x9a\x84\xe4\xba\xba\xe5\xb7\xa5\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9cNeuc\n\ninput_size = features.shape[1]\nhidden_size = 10\noutput_size = 2\nbatch_size = 128\nneuc = torch.nn.Sequential(\n    torch.nn.Linear(input_size, hidden_size),\n    torch.nn.Sigmoid(),\n    torch.nn.Linear(hidden_size, output_size),\n    torch.nn.Sigmoid(),\n)\n# \xe5\xb0\x86\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x87\xbd\xe6\x95\xb0\xe5\xae\x9a\xe4\xb9\x89\xe4\xb8\xba\xe4\xba\xa4\xe5\x8f\x89\xe7\x86\xb5\ncost = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(neuc.parameters(), lr = 0.1)\n\n\n#%%\nY_labels = Y > np.mean(Y)\nY_labels = Y_labels.astype(int)\nY_labels = Y_labels.reshape(-1)\nY_labels\n\n\n#%%\n# \xe5\xae\x9a\xe4\xb9\x89\xe4\xb8\x80\xe4\xb8\xaa\xe4\xb8\x93\xe9\x97\xa8\xe8\xae\xa1\xe7\xae\x97\xe5\x88\x86\xe7\xb1\xbb\xe9\x94\x99\xe8\xaf\xaf\xe7\x8e\x87\xe7\x9a\x84\xe5\x87\xbd\xe6\x95\xb0\xef\xbc\x8c\xe5\xae\x83\xe7\x9a\x84\xe5\x9f\xba\xe6\x9c\xac\xe6\x80\x9d\xe6\x83\xb3\xe6\x98\xaf\xef\xbc\x8c\xe5\xaf\xb9\xe4\xba\x8e\xe9\xa2\x84\xe6\xb5\x8b\xe5\x90\x91\xe9\x87\x8fpredictions\xe7\x9a\x84\xe6\xaf\x8f\xe4\xb8\x80\xe8\xa1\x8c\xef\xbc\x8c\n# \xe5\x8f\x96\xe6\x9c\x80\xe5\xa4\xa7\xe7\x9a\x84\xe9\x82\xa3\xe4\xb8\xaa\xe5\x85\x83\xe7\xb4\xa0\xe7\x9a\x84\xe4\xb8\x8b\xe6\xa0\x87\xef\xbc\x8c\xe4\xb8\x8e\xe6\xa0\x87\xe7\xad\xbelabels\xe4\xb8\xad\xe7\x9a\x84\xe5\x85\x83\xe7\xb4\xa0\xe5\x81\x9a\xe6\xaf\x94\xe8\xbe\x83\ndef error_rate(predictions, labels):\n    """"""\xe8\xae\xa1\xe7\xae\x97\xe9\xa2\x84\xe6\xb5\x8b\xe9\x94\x99\xe8\xaf\xaf\xe7\x8e\x87\xe7\x9a\x84\xe5\x87\xbd\xe6\x95\xb0\xef\xbc\x8c\xe5\x85\xb6\xe4\xb8\xadpredictions\xe6\x98\xaf\xe6\xa8\xa1\xe5\x9e\x8b\xe7\xbb\x99\xe5\x87\xba\xe7\x9a\x84\xe4\xb8\x80\xe7\xbb\x84\xe9\xa2\x84\xe6\xb5\x8b\xe7\xbb\x93\xe6\x9e\x9c\xef\xbc\x8clabels\xe6\x98\xaf\xe6\x95\xb0\xe6\x8d\xae\xe4\xb9\x8b\xe4\xb8\xad\xe7\x9a\x84\xe6\xad\xa3\xe7\xa1\xae\xe7\xad\x94\xe6\xa1\x88""""""\n    predictions = np.argmax(predictions, 1)\n    return 100.0 - (\n      100.0 *\n      np.sum( predictions == labels) /\n      predictions.shape[0])\n\n# \xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xe8\xae\xad\xe7\xbb\x83\xe5\xbe\xaa\xe7\x8e\xaf\nlosses = []\nerrors = []\nfor i in range(4000):\n    # \xe6\xaf\x8f128\xe4\xb8\xaa\xe6\xa0\xb7\xe6\x9c\xac\xe7\x82\xb9\xe8\xa2\xab\xe5\x88\x92\xe5\x88\x86\xe4\xb8\xba\xe4\xb8\x80\xe4\xb8\xaa\xe6\x92\xae\n    batch_loss = []\n    batch_errors = []\n    for start, end in zip(range(0, len(X), batch_size), range(batch_size, len(X)+1, batch_size)):\n        xx = torch.FloatTensor(X[start:end])\n        yy = torch.LongTensor(Y_labels[start:end])\n        predict = neuc(xx)\n        loss = cost(predict, yy)\n        err = error_rate(predict.data.numpy(), yy.data.numpy())\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        batch_loss.append(loss.data.numpy())\n        batch_errors.append(err)\n    \n    # \xe6\xaf\x8f\xe9\x9a\x94100\xe6\xad\xa5\xe8\xbe\x93\xe5\x87\xba\xe4\xb8\x80\xe4\xb8\x8b\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x80\xbc\xef\xbc\x88loss\xef\xbc\x89\n    if i % 100==0:\n        losses.append(np.mean(batch_loss))\n        errors.append(np.mean(batch_errors))\n        print(i, np.mean(batch_loss), np.mean(batch_errors))\n\n\n#%%\n# \xe6\x89\x93\xe5\x8d\xb0\xe8\xbe\x93\xe5\x87\xba\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x80\xbc\nplt.plot(np.arange(len(losses))*100,losses, label = \'Cross Entropy\')\nplt.plot(np.arange(len(losses))*100, np.array(errors) / float(100), label = \'Error Rate\')\nplt.xlabel(\'epoch\')\nplt.ylabel(\'Cross Entropy/Error rates\')\nplt.legend()\n\n#%% [markdown]\n# \xe5\xaf\xb9\xe5\x88\x86\xe7\xb1\xbb\xe6\x95\x88\xe6\x9e\x9c\xe8\xbf\x9b\xe8\xa1\x8c\xe6\xb5\x8b\xe8\xaf\x95\n\n#%%\n# \xe8\xaf\xbb\xe5\x8f\x96\xe6\xb5\x8b\xe8\xaf\x95\xe6\x95\xb0\xe6\x8d\xae\ntargets = test_targets[\'cnt\']\ntargets = targets.values.reshape([len(targets), 1])\nY_labels = targets > np.mean(Y)\nY_labels = Y_labels.astype(int)\nY_labels = Y_labels.reshape(-1)\nx = torch.FloatTensor(test_features.values)\n\n# \xe6\x89\x93\xe5\x8d\xb0\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xe9\xa2\x84\xe6\xb5\x8b\xe7\x9a\x84\xe9\x94\x99\xe8\xaf\xaf\xe7\x8e\x87\npredict = neuc(x)\nprint(error_rate(predict.data.numpy(), Y_labels))\n\n# \xe6\x8e\xa5\xe4\xb8\x8b\xe6\x9d\xa5\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe6\x8a\x8a\xe9\xa2\x84\xe6\xb5\x8b\xe6\xad\xa3\xe7\xa1\xae\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe5\x92\x8c\xe9\x94\x99\xe8\xaf\xaf\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe5\x88\x86\xe5\x88\xab\xe7\x94\xbb\xe5\x87\xba\xe6\x9d\xa5\xef\xbc\x8c\xe7\xba\xb5\xe5\x9d\x90\xe6\xa0\x87\xe5\x88\x86\xe5\x88\xab\xe6\x98\xaf\xe9\xa2\x84\xe6\xb5\x8b\xe6\xad\xa3\xe7\xa1\xae\xe7\x9a\x84\xe6\xa6\x82\xe7\x8e\x87\xe5\x92\x8c\xe9\xa2\x84\xe6\xb5\x8b\xe9\x94\x99\xe8\xaf\xaf\xe7\x9a\x84\xe6\xa6\x82\xe7\x8e\x87\nprob = predict.data.numpy()\nrights = np.argmax(prob, 1) == Y_labels\nwrongs = np.argmax(prob, 1) != Y_labels\nright_labels = Y_labels[rights]\nwrong_labels = Y_labels[wrongs]\nprobs = prob[rights, :]\nprobs1 = prob[wrongs, :]\nrightness = [probs[i, right_labels[i]] for i in range(len(right_labels))]\nright_index = np.arange(len(targets))[rights]\nwrongness = [probs1[i, wrong_labels[i]] for i in range(len(wrong_labels))]\nwrong_index = np.arange(len(targets))[wrongs]\nfig, ax = plt.subplots(figsize = (8, 6))\nax.plot(right_index, rightness, \'.\', label=\'Right\')\nax.plot(wrong_index, wrongness,\'o\',label=\'Wrong\')\n\nax.legend()\nplt.ylabel(\'Probabilities\')\n\ndates = pd.to_datetime(rides.loc[test_features.index][\'dteday\'])\ndates = dates.apply(lambda d: d.strftime(\'%b %d\'))\nax.set_xticks(np.arange(len(dates))[12::24])\n_ = ax.set_xticklabels(dates[12::24], rotation=45)\n\n\n'"
7.Convolutional_Neural_Network(CNN) 卷积神经网络/others/MNIST-CNN-oldversion.py,11,"b'\n#%%\n# MNIST CNN classifier \n# Code by GunhoChoi\n\nimport torch\nimport torch.nn as nn\nimport torch.utils as utils\nfrom torch.autograd import Variable\nimport torchvision.datasets as dset\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nget_ipython().run_line_magic(\'matplotlib\', \'inline\')\n\n# Set Hyperparameters\n\nepoch = 100\nbatch_size =16\nlearning_rate = 0.001\n\n# Download Data\n\nmnist_train = dset.MNIST(""./"", train=True, transform=transforms.ToTensor(), target_transform=None, download=True)\nmnist_test  = dset.MNIST(""./"", train=False, transform=transforms.ToTensor(), target_transform=None, download=True)\n\n\n#%%\n# Check the datasets downloaded\n\nprint(mnist_train.__len__())\nprint(mnist_test.__len__())\nimg1,label1 = mnist_train.__getitem__(0)\nimg2,label2 = mnist_test.__getitem__(0)\n\nprint(img1.size(), label1)\nprint(img2.size(), label2)\n\n# Set Data Loader(input pipeline)\n\ntrain_loader = torch.utils.data.DataLoader(dataset=mnist_train,batch_size=batch_size,shuffle=True)\ntest_loader = torch.utils.data.DataLoader(dataset=mnist_test,batch_size=batch_size,shuffle=True)\n\n\n#%%\n# torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, \n#                 padding=0, dilation=1, groups=1, bias=True)\n# torch.nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1,\n#                    return_indices=False, ceil_mode=False)\n# torch.nn.BatchNorm2d(num_features, eps=1e-05, momentum=0.1,affine=True)\n# torch.nn.ReLU()\n# tensor.view(newshape)\n\nclass CNN(nn.Module):\n    def __init__(self):\n        super(CNN,self).__init__()\n        self.layer1 = nn.Sequential(\n                        nn.Conv2d(1,16,5),   # batch x 16 x 24 x 24\n                        nn.ReLU(),\n                        nn.BatchNorm2d(16),\n                        nn.Conv2d(16,32,5),  # batch x 32 x 20 x 20\n                        nn.ReLU(),\n                        nn.BatchNorm2d(32),\n                        nn.MaxPool2d(2,2)   # batch x 32 x 10 x 10\n        )\n        self.layer2 = nn.Sequential(\n                        nn.Conv2d(32,64,5),  # batch x 64 x 6 x 6\n                        nn.ReLU(),\n                        nn.BatchNorm2d(64),\n                        nn.Conv2d(64,128,5),  # batch x 128 x 2 x 2\n                        nn.ReLU()\n        )\n        self.fc = nn.Linear(2*2*128,10)\n        \n    def forward(self,x):\n        out = self.layer1(x)\n        out = self.layer2(out)\n        out = out.view(batch_size, -1)\n        out = self.fc(out)\n        return out\n        \ncnn = CNN()\n\nloss_func = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(cnn.parameters(), lr=learning_rate)\n\n\n#%%\n# Train Model with train data\n# In order to use GPU you need to move all Variables and model by Module.cuda()\n\nfor i in range(epoch):\n    for j,[image,label] in enumerate(train_loader):\n        image = Variable(image)\n        label = Variable(label)\n        \n        optimizer.zero_grad()\n        result = cnn.forward(image)\n        loss = loss_func(result,label)\n        loss.backward()\n        optimizer.step()\n        \n        if j % 100 == 0:\n            print(loss)\n\n\n#%%\n# Test with test data\n# In order test, we need to change model mode to .eval()\n# and get the highest score label for accuracy\n\ncnn.eval()\ncorrect = 0\ntotal = 0\n\nfor image,label in test_loader:\n    image = Variable(image)\n    result = cnn(image)\n    \n    _,pred = torch.max(result.data,1)\n    \n    total += label.size(0)\n    correct += (pred == label).sum()\n    \nprint(""Accuracy of Test Data: {}"".format(correct/total))\n\n\n'"
7.Convolutional_Neural_Network(CNN) 卷积神经网络/others/MNIST_CNN_cpu-oldversion.py,11,"b'# MNIST CNN classifier \n# Code by GunhoChoi\n\nimport torch\nimport torch.nn as nn\nimport torch.utils as utils\nfrom torch.autograd import Variable\nimport torchvision.datasets as dset\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\n#%matplotlib inline\n\n# Set Hyperparameters\n\nepoch = 1\nbatch_size =16\nlearning_rate = 0.001\n\n# Download Data\n\nmnist_train = dset.MNIST(""./"", train=True, transform=transforms.ToTensor(), target_transform=None, download=True)\nmnist_test  = dset.MNIST(""./"", train=False, transform=transforms.ToTensor(), target_transform=None, download=True)\n\n# Check the datasets downloaded\n\nprint(mnist_train.__len__())\nprint(mnist_test.__len__())\nimg1,label1 = mnist_train.__getitem__(0)\nimg2,label2 = mnist_test.__getitem__(0)\n\nprint(img1.size(), label1)\nprint(img2.size(), label2)\n\n# Set Data Loader(input pipeline)\n\ntrain_loader = torch.utils.data.DataLoader(dataset=mnist_train,batch_size=batch_size,shuffle=True)\ntest_loader = torch.utils.data.DataLoader(dataset=mnist_test,batch_size=batch_size,shuffle=True)\n\n# torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, \n#                 padding=0, dilation=1, groups=1, bias=True)\n# torch.nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1,\n#                    return_indices=False, ceil_mode=False)\n# torch.nn.BatchNorm2d(num_features, eps=1e-05, momentum=0.1,affine=True)\n# torch.nn.ReLU()\n# tensor.view(newshape)\n\nclass CNN(nn.Module):\n\n    def __init__(self):\n\n        super(CNN,self).__init__()\n        self.layer1 = nn.Sequential(\n                        nn.Conv2d(1,16,5),   # batch x 16 x 24 x 24\n                        nn.ReLU(),\n                        nn.BatchNorm2d(16),\n                        nn.Conv2d(16,32,5),  # batch x 32 x 20 x 20\n                        nn.ReLU(),\n                        nn.BatchNorm2d(32),\n                        nn.MaxPool2d(2,2)   # batch x 32 x 10 x 10\n        )\n        self.layer2 = nn.Sequential(\n                        nn.Conv2d(32,64,5),  # batch x 64 x 6 x 6\n                        nn.ReLU(),\n                        nn.BatchNorm2d(64),\n                        nn.Conv2d(64,128,5),  # batch x 128 x 2 x 2\n                        nn.ReLU()\n        )\n        self.fc = nn.Linear(2*2*128,10)\n        \n    def forward(self,x):\n        out = self.layer1(x)\n        out = self.layer2(out)\n        out = out.view(batch_size, -1)\n        out = self.fc(out)\n        return out\n        \ncnn = CNN()\n\nloss_func = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(cnn.parameters(), lr=learning_rate)\n\n# Train Model with train data\n# In order to use GPU you need to move all Variables and model by Module\n\nfor i in range(epoch):\n    for j,[image,label] in enumerate(train_loader):\n        image = Variable(image)\n        label = Variable(label)\n        \n        optimizer.zero_grad()\n        result = cnn.forward(image)\n        loss = loss_func(result,label)\n        loss.backward()\n        optimizer.step()\n        \n        if j % 100 == 0:\n            print(loss)\n\n# Test with test data\n# In order test, we need to change model mode to .eval()\n# and get the highest score label for accuracy\n# Change model to \'eval\' mode (BN uses moving mean/var)\n\ncnn.eval()  \ncorrect= 0\ntotal = 0\n\nfor image, label in test_loader:\n    image = Variable(image)\n    result = cnn(image)\n    _, predicted = torch.max(result.data, 1)\n    total += label.size(0)\n    correct += (predicted == label).sum()\n\nprint(\'Test Accuracy: %f %%\' % (100 * correct / total))\n  \n'"
7.Convolutional_Neural_Network(CNN) 卷积神经网络/others/MNIST_CNN_gpu-oldversion.py,11,"b'# MNIST CNN classifier \n# Code by GunhoChoi\n\nimport torch\nimport torch.nn as nn\nimport torch.utils as utils\nfrom torch.autograd import Variable\nimport torchvision.datasets as dset\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\n#%matplotlib inline\n\n# Set Hyperparameters\n\nepoch = 1\nbatch_size =16\nlearning_rate = 0.001\n\n# Download Data\n\nmnist_train = dset.MNIST(""./"", train=True, transform=transforms.ToTensor(), target_transform=None, download=True)\nmnist_test  = dset.MNIST(""./"", train=False, transform=transforms.ToTensor(), target_transform=None, download=True)\n\n# Check the datasets downloaded\n\nprint(mnist_train.__len__())\nprint(mnist_test.__len__())\nimg1,label1 = mnist_train.__getitem__(0)\nimg2,label2 = mnist_test.__getitem__(0)\n\nprint(img1.size(), label1)\nprint(img2.size(), label2)\n\n# Set Data Loader(input pipeline)\n\ntrain_loader = torch.utils.data.DataLoader(dataset=mnist_train,batch_size=batch_size,shuffle=True)\ntest_loader = torch.utils.data.DataLoader(dataset=mnist_test,batch_size=batch_size,shuffle=True)\n\n# torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, \n#                 padding=0, dilation=1, groups=1, bias=True)\n# torch.nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1,\n#                    return_indices=False, ceil_mode=False)\n# torch.nn.BatchNorm2d(num_features, eps=1e-05, momentum=0.1,affine=True)\n# torch.nn.ReLU()\n# tensor.view(newshape)\n\nclass CNN(nn.Module):\n\n    def __init__(self):\n\n        super(CNN,self).__init__()\n        self.layer1 = nn.Sequential(\n                        nn.Conv2d(1,16,5),   # batch x 16 x 24 x 24\n                        nn.ReLU(),\n                        nn.BatchNorm2d(16),\n                        nn.Conv2d(16,32,5),  # batch x 32 x 20 x 20\n                        nn.ReLU(),\n                        nn.BatchNorm2d(32),\n                        nn.MaxPool2d(2,2)   # batch x 32 x 10 x 10\n        )\n        self.layer2 = nn.Sequential(\n                        nn.Conv2d(32,64,5),  # batch x 64 x 6 x 6\n                        nn.ReLU(),\n                        nn.BatchNorm2d(64),\n                        nn.Conv2d(64,128,5),  # batch x 128 x 2 x 2\n                        nn.ReLU()\n        )\n        self.fc = nn.Linear(2*2*128,10)\n        \n    def forward(self,x):\n        out = self.layer1(x)\n        out = self.layer2(out)\n        out = out.view(batch_size, -1)\n        out = self.fc(out)\n        return out\n        \ncnn = CNN()\n\nloss_func = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(cnn.parameters(), lr=learning_rate)\n\n# Train Model with train data\n# In order to use GPU you need to move all Variables and model by Module.cuda()\n\nfor i in range(epoch):\n    for j,[image,label] in enumerate(train_loader):\n        image = Variable(image).cuda()\n        label = Variable(label).cuda()\n        cnn.cuda()\n        \n        optimizer.zero_grad()\n        result = cnn.forward(image)\n        loss = loss_func(result,label)\n        loss.backward()\n        optimizer.step()\n        \n        if j % 100 == 0:\n            print(loss)\n\n# Test with test data\n# In order test, we need to change model mode to .eval()\n# and get the highest score label for accuracy\n# Change model to \'eval\' mode (BN uses moving mean/var)\n# label also has to be on GPU so .cuda() required -> this took an hour for me to figure out\n\ncnn.eval()  \ncorrect= 0\ntotal = 0\n\nfor image, label in test_loader:\n    image = Variable(image).cuda()\n    result = cnn(image).cuda()\n    _, predicted = torch.max(result.data, 1)\n    total += label.size(0)\n    correct += (predicted == label.cuda()).sum()\n\nprint(\'Test Accuracy of the model on the 10000 test images: %f %%\' % (100 * correct / total))\n  \n'"
8.Famous_CNN 经典的CNN网络/some_famous_networks/DenseNet.py,5,"b""'''DenseNet in PyTorch.'''\nimport math\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom torch.autograd import Variable\n\n\nclass Bottleneck(nn.Module):\n    def __init__(self, in_planes, growth_rate):\n        super(Bottleneck, self).__init__()\n        self.bn1 = nn.BatchNorm2d(in_planes)\n        self.conv1 = nn.Conv2d(in_planes, 4*growth_rate, kernel_size=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(4*growth_rate)\n        self.conv2 = nn.Conv2d(4*growth_rate, growth_rate, kernel_size=3, padding=1, bias=False)\n\n    def forward(self, x):\n        out = self.conv1(F.relu(self.bn1(x)))\n        out = self.conv2(F.relu(self.bn2(out)))\n        out = torch.cat([out,x], 1)\n        return out\n\n\nclass Transition(nn.Module):\n    def __init__(self, in_planes, out_planes):\n        super(Transition, self).__init__()\n        self.bn = nn.BatchNorm2d(in_planes)\n        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=1, bias=False)\n\n    def forward(self, x):\n        out = self.conv(F.relu(self.bn(x)))\n        out = F.avg_pool2d(out, 2)\n        return out\n\n\nclass DenseNet(nn.Module):\n    def __init__(self, block, nblocks, growth_rate=12, reduction=0.5, num_classes=10):\n        super(DenseNet, self).__init__()\n        self.growth_rate = growth_rate\n\n        num_planes = 2*growth_rate\n        self.conv1 = nn.Conv2d(3, num_planes, kernel_size=3, padding=1, bias=False)\n\n        self.dense1 = self._make_dense_layers(block, num_planes, nblocks[0])\n        num_planes += nblocks[0]*growth_rate\n        out_planes = int(math.floor(num_planes*reduction))\n        self.trans1 = Transition(num_planes, out_planes)\n        num_planes = out_planes\n\n        self.dense2 = self._make_dense_layers(block, num_planes, nblocks[1])\n        num_planes += nblocks[1]*growth_rate\n        out_planes = int(math.floor(num_planes*reduction))\n        self.trans2 = Transition(num_planes, out_planes)\n        num_planes = out_planes\n\n        self.dense3 = self._make_dense_layers(block, num_planes, nblocks[2])\n        num_planes += nblocks[2]*growth_rate\n        out_planes = int(math.floor(num_planes*reduction))\n        self.trans3 = Transition(num_planes, out_planes)\n        num_planes = out_planes\n\n        self.dense4 = self._make_dense_layers(block, num_planes, nblocks[3])\n        num_planes += nblocks[3]*growth_rate\n\n        self.bn = nn.BatchNorm2d(num_planes)\n        self.linear = nn.Linear(num_planes, num_classes)\n\n    def _make_dense_layers(self, block, in_planes, nblock):\n        layers = []\n        for i in range(nblock):\n            layers.append(block(in_planes, self.growth_rate))\n            in_planes += self.growth_rate\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = self.conv1(x)\n        out = self.trans1(self.dense1(out))\n        out = self.trans2(self.dense2(out))\n        out = self.trans3(self.dense3(out))\n        out = self.dense4(out)\n        out = F.avg_pool2d(F.relu(self.bn(out)), 4)\n        out = out.view(out.size(0), -1)\n        out = self.linear(out)\n        return out\n\ndef DenseNet121():\n    return DenseNet(Bottleneck, [6,12,24,16], growth_rate=32)\n\ndef DenseNet169():\n    return DenseNet(Bottleneck, [6,12,32,32], growth_rate=32)\n\ndef DenseNet201():\n    return DenseNet(Bottleneck, [6,12,48,32], growth_rate=32)\n\ndef DenseNet161():\n    return DenseNet(Bottleneck, [6,12,36,24], growth_rate=48)\n\ndef densenet_cifar():\n    return DenseNet(Bottleneck, [6,12,24,16], growth_rate=12)\n\ndef test_densenet():\n    net = densenet_cifar()\n    x = torch.randn(1,3,32,32)\n    y = net(Variable(x))\n    print(y)\n\n# test_densenet()"""
8.Famous_CNN 经典的CNN网络/some_famous_networks/ResNet.py,4,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom torch.autograd import Variable\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, in_planes, planes, stride=1):\n        super(BasicBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != self.expansion*planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(self.expansion*planes)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out += self.shortcut(x)\n        out = F.relu(out)\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, in_planes, planes, stride=1):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != self.expansion*planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(self.expansion*planes)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = F.relu(self.bn2(self.conv2(out)))\n        out = self.bn3(self.conv3(out))\n        out += self.shortcut(x)\n        out = F.relu(out)\n        return out\n\n\nclass ResNet(nn.Module):\n    def __init__(self, block, num_blocks, num_classes=10):\n        super(ResNet, self).__init__()\n        self.in_planes = 64\n\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n        self.linear = nn.Linear(512*block.expansion, num_classes)\n\n    def _make_layer(self, block, planes, num_blocks, stride):\n        strides = [stride] + [1]*(num_blocks-1)\n        layers = []\n        for stride in strides:\n            layers.append(block(self.in_planes, planes, stride))\n            self.in_planes = planes * block.expansion\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = F.avg_pool2d(out, 4)\n        out = out.view(out.size(0), -1)\n        out = self.linear(out)\n        return out\n\n\ndef ResNet18():\n    return ResNet(BasicBlock, [2,2,2,2])\n\ndef ResNet34():\n    return ResNet(BasicBlock, [3,4,6,3])\n\ndef ResNet50():\n    return ResNet(Bottleneck, [3,4,6,3])\n\ndef ResNet101():\n    return ResNet(Bottleneck, [3,4,23,3])\n\ndef ResNet152():\n    return ResNet(Bottleneck, [3,8,36,3])\n\n\ndef test():\n    net = ResNet18()\n    y = net(Variable(torch.randn(1,3,32,32)))\n    print(y.size())\n\n# test()'"
8.Famous_CNN 经典的CNN网络/some_famous_networks/dpn.py,5,"b""'''Dual Path Networks in PyTorch.'''\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom torch.autograd import Variable\n\n\nclass Bottleneck(nn.Module):\n    def __init__(self, last_planes, in_planes, out_planes, dense_depth, stride, first_layer):\n        super(Bottleneck, self).__init__()\n        self.out_planes = out_planes\n        self.dense_depth = dense_depth\n\n        self.conv1 = nn.Conv2d(last_planes, in_planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(in_planes)\n        self.conv2 = nn.Conv2d(in_planes, in_planes, kernel_size=3, stride=stride, padding=1, groups=32, bias=False)\n        self.bn2 = nn.BatchNorm2d(in_planes)\n        self.conv3 = nn.Conv2d(in_planes, out_planes+dense_depth, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(out_planes+dense_depth)\n\n        self.shortcut = nn.Sequential()\n        if first_layer:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(last_planes, out_planes+dense_depth, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(out_planes+dense_depth)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = F.relu(self.bn2(self.conv2(out)))\n        out = self.bn3(self.conv3(out))\n        x = self.shortcut(x)\n        d = self.out_planes\n        out = torch.cat([x[:,:d,:,:]+out[:,:d,:,:], x[:,d:,:,:], out[:,d:,:,:]], 1)\n        out = F.relu(out)\n        return out\n\n\nclass DPN(nn.Module):\n    def __init__(self, cfg):\n        super(DPN, self).__init__()\n        in_planes, out_planes = cfg['in_planes'], cfg['out_planes']\n        num_blocks, dense_depth = cfg['num_blocks'], cfg['dense_depth']\n\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.last_planes = 64\n        self.layer1 = self._make_layer(in_planes[0], out_planes[0], num_blocks[0], dense_depth[0], stride=1)\n        self.layer2 = self._make_layer(in_planes[1], out_planes[1], num_blocks[1], dense_depth[1], stride=2)\n        self.layer3 = self._make_layer(in_planes[2], out_planes[2], num_blocks[2], dense_depth[2], stride=2)\n        self.layer4 = self._make_layer(in_planes[3], out_planes[3], num_blocks[3], dense_depth[3], stride=2)\n        self.linear = nn.Linear(out_planes[3]+(num_blocks[3]+1)*dense_depth[3], 10)\n\n    def _make_layer(self, in_planes, out_planes, num_blocks, dense_depth, stride):\n        strides = [stride] + [1]*(num_blocks-1)\n        layers = []\n        for i,stride in enumerate(strides):\n            layers.append(Bottleneck(self.last_planes, in_planes, out_planes, dense_depth, stride, i==0))\n            self.last_planes = out_planes + (i+2) * dense_depth\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = F.avg_pool2d(out, 4)\n        out = out.view(out.size(0), -1)\n        out = self.linear(out)\n        return out\n\n\ndef DPN26():\n    cfg = {\n        'in_planes': (96,192,384,768),\n        'out_planes': (256,512,1024,2048),\n        'num_blocks': (2,2,2,2),\n        'dense_depth': (16,32,24,128)\n    }\n    return DPN(cfg)\n\ndef DPN92():\n    cfg = {\n        'in_planes': (96,192,384,768),\n        'out_planes': (256,512,1024,2048),\n        'num_blocks': (3,4,20,3),\n        'dense_depth': (16,32,24,128)\n    }\n    return DPN(cfg)\n\n\ndef test():\n    net = DPN92()\n    x = Variable(torch.randn(1,3,32,32))\n    y = net(x)\n    print(y)\n\n# test()"""
8.Famous_CNN 经典的CNN网络/some_famous_networks/googlenet.py,5,"b""'''GoogLeNet with PyTorch.'''\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom torch.autograd import Variable\n\n\nclass Inception(nn.Module):\n    def __init__(self, in_planes, n1x1, n3x3red, n3x3, n5x5red, n5x5, pool_planes):\n        super(Inception, self).__init__()\n        # 1x1 conv branch\n        self.b1 = nn.Sequential(\n            nn.Conv2d(in_planes, n1x1, kernel_size=1),\n            nn.BatchNorm2d(n1x1),\n            nn.ReLU(True),\n        )\n\n        # 1x1 conv -> 3x3 conv branch\n        self.b2 = nn.Sequential(\n            nn.Conv2d(in_planes, n3x3red, kernel_size=1),\n            nn.BatchNorm2d(n3x3red),\n            nn.ReLU(True),\n            nn.Conv2d(n3x3red, n3x3, kernel_size=3, padding=1),\n            nn.BatchNorm2d(n3x3),\n            nn.ReLU(True),\n        )\n\n        # 1x1 conv -> 5x5 conv branch\n        self.b3 = nn.Sequential(\n            nn.Conv2d(in_planes, n5x5red, kernel_size=1),\n            nn.BatchNorm2d(n5x5red),\n            nn.ReLU(True),\n            nn.Conv2d(n5x5red, n5x5, kernel_size=3, padding=1),\n            nn.BatchNorm2d(n5x5),\n            nn.ReLU(True),\n            nn.Conv2d(n5x5, n5x5, kernel_size=3, padding=1),\n            nn.BatchNorm2d(n5x5),\n            nn.ReLU(True),\n        )\n\n        # 3x3 pool -> 1x1 conv branch\n        self.b4 = nn.Sequential(\n            nn.MaxPool2d(3, stride=1, padding=1),\n            nn.Conv2d(in_planes, pool_planes, kernel_size=1),\n            nn.BatchNorm2d(pool_planes),\n            nn.ReLU(True),\n        )\n\n    def forward(self, x):\n        y1 = self.b1(x)\n        y2 = self.b2(x)\n        y3 = self.b3(x)\n        y4 = self.b4(x)\n        return torch.cat([y1,y2,y3,y4], 1)\n\n\nclass GoogLeNet(nn.Module):\n    def __init__(self):\n        super(GoogLeNet, self).__init__()\n        self.pre_layers = nn.Sequential(\n            nn.Conv2d(3, 192, kernel_size=3, padding=1),\n            nn.BatchNorm2d(192),\n            nn.ReLU(True),\n        )\n\n        self.a3 = Inception(192,  64,  96, 128, 16, 32, 32)\n        self.b3 = Inception(256, 128, 128, 192, 32, 96, 64)\n\n        self.maxpool = nn.MaxPool2d(3, stride=2, padding=1)\n\n        self.a4 = Inception(480, 192,  96, 208, 16,  48,  64)\n        self.b4 = Inception(512, 160, 112, 224, 24,  64,  64)\n        self.c4 = Inception(512, 128, 128, 256, 24,  64,  64)\n        self.d4 = Inception(512, 112, 144, 288, 32,  64,  64)\n        self.e4 = Inception(528, 256, 160, 320, 32, 128, 128)\n\n        self.a5 = Inception(832, 256, 160, 320, 32, 128, 128)\n        self.b5 = Inception(832, 384, 192, 384, 48, 128, 128)\n\n        self.avgpool = nn.AvgPool2d(8, stride=1)\n        self.linear = nn.Linear(1024, 10)\n\n    def forward(self, x):\n        out = self.pre_layers(x)\n        out = self.a3(out)\n        out = self.b3(out)\n        out = self.maxpool(out)\n        out = self.a4(out)\n        out = self.b4(out)\n        out = self.c4(out)\n        out = self.d4(out)\n        out = self.e4(out)\n        out = self.maxpool(out)\n        out = self.a5(out)\n        out = self.b5(out)\n        out = self.avgpool(out)\n        out = out.view(out.size(0), -1)\n        out = self.linear(out)\n        return out\n\n# net = GoogLeNet()\n# x = torch.randn(1,3,32,32)\n# y = net(Variable(x))\n# print(y.size())"""
8.Famous_CNN 经典的CNN网络/some_famous_networks/lenet.py,2,"b""'''LeNet in PyTorch.'''\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass LeNet(nn.Module):\n    def __init__(self):\n        super(LeNet, self).__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1   = nn.Linear(16*5*5, 120)\n        self.fc2   = nn.Linear(120, 84)\n        self.fc3   = nn.Linear(84, 10)\n\n    def forward(self, x):\n        out = F.relu(self.conv1(x))\n        out = F.max_pool2d(out, 2)\n        out = F.relu(self.conv2(out))\n        out = F.max_pool2d(out, 2)\n        out = out.view(out.size(0), -1)\n        out = F.relu(self.fc1(out))\n        out = F.relu(self.fc2(out))\n        out = self.fc3(out)\n        return out"""
8.Famous_CNN 经典的CNN网络/some_famous_networks/mobilenet.py,4,"b'\'\'\'MobileNet in PyTorch.\nSee the paper ""MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications""\nfor more details.\n\'\'\'\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom torch.autograd import Variable\n\n\nclass Block(nn.Module):\n    \'\'\'Depthwise conv + Pointwise conv\'\'\'\n    def __init__(self, in_planes, out_planes, stride=1):\n        super(Block, self).__init__()\n        self.conv1 = nn.Conv2d(in_planes, in_planes, kernel_size=3, stride=stride, padding=1, groups=in_planes, bias=False)\n        self.bn1 = nn.BatchNorm2d(in_planes)\n        self.conv2 = nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=1, padding=0, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_planes)\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = F.relu(self.bn2(self.conv2(out)))\n        return out\n\n\nclass MobileNet(nn.Module):\n    # (128,2) means conv planes=128, conv stride=2, by default conv stride=1\n    cfg = [64, (128,2), 128, (256,2), 256, (512,2), 512, 512, 512, 512, 512, (1024,2), 1024]\n\n    def __init__(self, num_classes=10):\n        super(MobileNet, self).__init__()\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(32)\n        self.layers = self._make_layers(in_planes=32)\n        self.linear = nn.Linear(1024, num_classes)\n\n    def _make_layers(self, in_planes):\n        layers = []\n        for x in self.cfg:\n            out_planes = x if isinstance(x, int) else x[0]\n            stride = 1 if isinstance(x, int) else x[1]\n            layers.append(Block(in_planes, out_planes, stride))\n            in_planes = out_planes\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.layers(out)\n        out = F.avg_pool2d(out, 2)\n        out = out.view(out.size(0), -1)\n        out = self.linear(out)\n        return out\n\n\ndef test():\n    net = MobileNet()\n    x = torch.randn(1,3,32,32)\n    y = net(Variable(x))\n    print(y.size())\n\n# test()'"
8.Famous_CNN 经典的CNN网络/some_famous_networks/preact_resnet.py,4,"b""'''Pre-activation ResNet in PyTorch.\nReference:\n[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n    Identity Mappings in Deep Residual Networks. arXiv:1603.05027\n'''\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom torch.autograd import Variable\n\n\nclass PreActBlock(nn.Module):\n    '''Pre-activation version of the BasicBlock.'''\n    expansion = 1\n\n    def __init__(self, in_planes, planes, stride=1):\n        super(PreActBlock, self).__init__()\n        self.bn1 = nn.BatchNorm2d(in_planes)\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n\n        if stride != 1 or in_planes != self.expansion*planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(x))\n        shortcut = self.shortcut(out) if hasattr(self, 'shortcut') else x\n        out = self.conv1(out)\n        out = self.conv2(F.relu(self.bn2(out)))\n        out += shortcut\n        return out\n\n\nclass PreActBottleneck(nn.Module):\n    '''Pre-activation version of the original Bottleneck module.'''\n    expansion = 4\n\n    def __init__(self, in_planes, planes, stride=1):\n        super(PreActBottleneck, self).__init__()\n        self.bn1 = nn.BatchNorm2d(in_planes)\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n\n        if stride != 1 or in_planes != self.expansion*planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(x))\n        shortcut = self.shortcut(out) if hasattr(self, 'shortcut') else x\n        out = self.conv1(out)\n        out = self.conv2(F.relu(self.bn2(out)))\n        out = self.conv3(F.relu(self.bn3(out)))\n        out += shortcut\n        return out\n\n\nclass PreActResNet(nn.Module):\n    def __init__(self, block, num_blocks, num_classes=10):\n        super(PreActResNet, self).__init__()\n        self.in_planes = 64\n\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n        self.linear = nn.Linear(512*block.expansion, num_classes)\n\n    def _make_layer(self, block, planes, num_blocks, stride):\n        strides = [stride] + [1]*(num_blocks-1)\n        layers = []\n        for stride in strides:\n            layers.append(block(self.in_planes, planes, stride))\n            self.in_planes = planes * block.expansion\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = self.conv1(x)\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = F.avg_pool2d(out, 4)\n        out = out.view(out.size(0), -1)\n        out = self.linear(out)\n        return out\n\n\ndef PreActResNet18():\n    return PreActResNet(PreActBlock, [2,2,2,2])\n\ndef PreActResNet34():\n    return PreActResNet(PreActBlock, [3,4,6,3])\n\ndef PreActResNet50():\n    return PreActResNet(PreActBottleneck, [3,4,6,3])\n\ndef PreActResNet101():\n    return PreActResNet(PreActBottleneck, [3,4,23,3])\n\ndef PreActResNet152():\n    return PreActResNet(PreActBottleneck, [3,8,36,3])\n\n\ndef test():\n    net = PreActResNet18()\n    y = net(Variable(torch.randn(1,3,32,32)))\n    print(y.size())\n\n# test()"""
8.Famous_CNN 经典的CNN网络/some_famous_networks/resnext.py,4,"b'\'\'\'ResNeXt in PyTorch.\nSee the paper ""Aggregated Residual Transformations for Deep Neural Networks"" for more details.\n\'\'\'\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom torch.autograd import Variable\n\n\nclass Block(nn.Module):\n    \'\'\'Grouped convolution block.\'\'\'\n    expansion = 2\n\n    def __init__(self, in_planes, cardinality=32, bottleneck_width=4, stride=1):\n        super(Block, self).__init__()\n        group_width = cardinality * bottleneck_width\n        self.conv1 = nn.Conv2d(in_planes, group_width, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(group_width)\n        self.conv2 = nn.Conv2d(group_width, group_width, kernel_size=3, stride=stride, padding=1, groups=cardinality, bias=False)\n        self.bn2 = nn.BatchNorm2d(group_width)\n        self.conv3 = nn.Conv2d(group_width, self.expansion*group_width, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(self.expansion*group_width)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != self.expansion*group_width:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, self.expansion*group_width, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(self.expansion*group_width)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = F.relu(self.bn2(self.conv2(out)))\n        out = self.bn3(self.conv3(out))\n        out += self.shortcut(x)\n        out = F.relu(out)\n        return out\n\n\nclass ResNeXt(nn.Module):\n    def __init__(self, num_blocks, cardinality, bottleneck_width, num_classes=10):\n        super(ResNeXt, self).__init__()\n        self.cardinality = cardinality\n        self.bottleneck_width = bottleneck_width\n        self.in_planes = 64\n\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.layer1 = self._make_layer(num_blocks[0], 1)\n        self.layer2 = self._make_layer(num_blocks[1], 2)\n        self.layer3 = self._make_layer(num_blocks[2], 2)\n        # self.layer4 = self._make_layer(num_blocks[3], 2)\n        self.linear = nn.Linear(cardinality*bottleneck_width*8, num_classes)\n\n    def _make_layer(self, num_blocks, stride):\n        strides = [stride] + [1]*(num_blocks-1)\n        layers = []\n        for stride in strides:\n            layers.append(Block(self.in_planes, self.cardinality, self.bottleneck_width, stride))\n            self.in_planes = Block.expansion * self.cardinality * self.bottleneck_width\n        # Increase bottleneck_width by 2 after each stage.\n        self.bottleneck_width *= 2\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        # out = self.layer4(out)\n        out = F.avg_pool2d(out, 8)\n        out = out.view(out.size(0), -1)\n        out = self.linear(out)\n        return out\n\n\ndef ResNeXt29_2x64d():\n    return ResNeXt(num_blocks=[3,3,3], cardinality=2, bottleneck_width=64)\n\ndef ResNeXt29_4x64d():\n    return ResNeXt(num_blocks=[3,3,3], cardinality=4, bottleneck_width=64)\n\ndef ResNeXt29_8x64d():\n    return ResNeXt(num_blocks=[3,3,3], cardinality=8, bottleneck_width=64)\n\ndef ResNeXt29_32x4d():\n    return ResNeXt(num_blocks=[3,3,3], cardinality=32, bottleneck_width=4)\n\ndef test_resnext():\n    net = ResNeXt29_2x64d()\n    x = torch.randn(1,3,32,32)\n    y = net(Variable(x))\n    print(y.size())\n\n# test_resnext()'"
8.Famous_CNN 经典的CNN网络/some_famous_networks/senet.py,4,"b""'''SENet in PyTorch.\nSENet is the winner of ImageNet-2017. The paper is not released yet.\n'''\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom torch.autograd import Variable\n\n\nclass BasicBlock(nn.Module):\n    def __init__(self, in_planes, planes, stride=1):\n        super(BasicBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, planes, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes)\n            )\n\n        # SE layers\n        self.fc1 = nn.Conv2d(planes, planes//16, kernel_size=1)  # Use nn.Conv2d instead of nn.Linear\n        self.fc2 = nn.Conv2d(planes//16, planes, kernel_size=1)\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n\n        # Squeeze\n        w = F.avg_pool2d(out, out.size(2))\n        w = F.relu(self.fc1(w))\n        w = F.sigmoid(self.fc2(w))\n        # Excitation\n        out = out * w  # New broadcasting feature from v0.2!\n\n        out += self.shortcut(x)\n        out = F.relu(out)\n        return out\n\n\nclass PreActBlock(nn.Module):\n    def __init__(self, in_planes, planes, stride=1):\n        super(PreActBlock, self).__init__()\n        self.bn1 = nn.BatchNorm2d(in_planes)\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n\n        if stride != 1 or in_planes != planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, planes, kernel_size=1, stride=stride, bias=False)\n            )\n\n        # SE layers\n        self.fc1 = nn.Conv2d(planes, planes//16, kernel_size=1)\n        self.fc2 = nn.Conv2d(planes//16, planes, kernel_size=1)\n\n    def forward(self, x):\n        out = F.relu(self.bn1(x))\n        shortcut = self.shortcut(out) if hasattr(self, 'shortcut') else x\n        out = self.conv1(out)\n        out = self.conv2(F.relu(self.bn2(out)))\n\n        # Squeeze\n        w = F.avg_pool2d(out, out.size(2))\n        w = F.relu(self.fc1(w))\n        w = F.sigmoid(self.fc2(w))\n        # Excitation\n        out = out * w\n\n        out += shortcut\n        return out\n\n\nclass SENet(nn.Module):\n    def __init__(self, block, num_blocks, num_classes=10):\n        super(SENet, self).__init__()\n        self.in_planes = 64\n\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.layer1 = self._make_layer(block,  64, num_blocks[0], stride=1)\n        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n        self.linear = nn.Linear(512, num_classes)\n\n    def _make_layer(self, block, planes, num_blocks, stride):\n        strides = [stride] + [1]*(num_blocks-1)\n        layers = []\n        for stride in strides:\n            layers.append(block(self.in_planes, planes, stride))\n            self.in_planes = planes\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = F.avg_pool2d(out, 4)\n        out = out.view(out.size(0), -1)\n        out = self.linear(out)\n        return out\n\n\ndef SENet18():\n    return SENet(PreActBlock, [2,2,2,2])\n\n\ndef test():\n    net = SENet18()\n    y = net(Variable(torch.randn(1,3,32,32)))\n    print(y.size())\n\n# test()"""
8.Famous_CNN 经典的CNN网络/some_famous_networks/shufflenet.py,5,"b'\'\'\'ShuffleNet in PyTorch.\nSee the paper ""ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices"" for more details.\n\'\'\'\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom torch.autograd import Variable\n\n\nclass ShuffleBlock(nn.Module):\n    def __init__(self, groups):\n        super(ShuffleBlock, self).__init__()\n        self.groups = groups\n\n    def forward(self, x):\n        \'\'\'Channel shuffle: [N,C,H,W] -> [N,g,C/g,H,W] -> [N,C/g,g,H,w] -> [N,C,H,W]\'\'\'\n        N,C,H,W = x.size()\n        g = self.groups\n        return x.view(N,g,C/g,H,W).permute(0,2,1,3,4).contiguous().view(N,C,H,W)\n\n\nclass Bottleneck(nn.Module):\n    def __init__(self, in_planes, out_planes, stride, groups):\n        super(Bottleneck, self).__init__()\n        self.stride = stride\n\n        mid_planes = out_planes/4\n        g = 1 if in_planes==24 else groups\n        self.conv1 = nn.Conv2d(in_planes, mid_planes, kernel_size=1, groups=g, bias=False)\n        self.bn1 = nn.BatchNorm2d(mid_planes)\n        self.shuffle1 = ShuffleBlock(groups=g)\n        self.conv2 = nn.Conv2d(mid_planes, mid_planes, kernel_size=3, stride=stride, padding=1, groups=mid_planes, bias=False)\n        self.bn2 = nn.BatchNorm2d(mid_planes)\n        self.conv3 = nn.Conv2d(mid_planes, out_planes, kernel_size=1, groups=groups, bias=False)\n        self.bn3 = nn.BatchNorm2d(out_planes)\n\n        self.shortcut = nn.Sequential()\n        if stride == 2:\n            self.shortcut = nn.Sequential(nn.AvgPool2d(3, stride=2, padding=1))\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.shuffle1(out)\n        out = F.relu(self.bn2(self.conv2(out)))\n        out = self.bn3(self.conv3(out))\n        res = self.shortcut(x)\n        out = F.relu(torch.cat([out,res], 1)) if self.stride==2 else F.relu(out+res)\n        return out\n\n\nclass ShuffleNet(nn.Module):\n    def __init__(self, cfg):\n        super(ShuffleNet, self).__init__()\n        out_planes = cfg[\'out_planes\']\n        num_blocks = cfg[\'num_blocks\']\n        groups = cfg[\'groups\']\n\n        self.conv1 = nn.Conv2d(3, 24, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(24)\n        self.in_planes = 24\n        self.layer1 = self._make_layer(out_planes[0], num_blocks[0], groups)\n        self.layer2 = self._make_layer(out_planes[1], num_blocks[1], groups)\n        self.layer3 = self._make_layer(out_planes[2], num_blocks[2], groups)\n        self.linear = nn.Linear(out_planes[2], 10)\n\n    def _make_layer(self, out_planes, num_blocks, groups):\n        layers = []\n        for i in range(num_blocks):\n            stride = 2 if i == 0 else 1\n            cat_planes = self.in_planes if i == 0 else 0\n            layers.append(Bottleneck(self.in_planes, out_planes-cat_planes, stride=stride, groups=groups))\n            self.in_planes = out_planes\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = F.avg_pool2d(out, 4)\n        out = out.view(out.size(0), -1)\n        out = self.linear(out)\n        return out\n\n\ndef ShuffleNetG2():\n    cfg = {\n        \'out_planes\': [200,400,800],\n        \'num_blocks\': [4,8,4],\n        \'groups\': 2\n    }\n    return ShuffleNet(cfg)\n\ndef ShuffleNetG3():\n    cfg = {\n        \'out_planes\': [240,480,960],\n        \'num_blocks\': [4,8,4],\n        \'groups\': 3\n    }\n    return ShuffleNet(cfg)\n\n\ndef test():\n    net = ShuffleNetG2()\n    x = Variable(torch.randn(1,3,32,32))\n    y = net(x)\n    print(y)\n\n# test()'"
8.Famous_CNN 经典的CNN网络/some_famous_networks/vgg.py,4,"b""'''VGG11/13/16/19 in Pytorch.'''\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\n\ncfg = {\n    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n}\n\n\nclass VGG(nn.Module):\n    def __init__(self, vgg_name):\n        super(VGG, self).__init__()\n        self.features = self._make_layers(cfg[vgg_name])\n        self.classifier = nn.Linear(512, 10)\n\n    def forward(self, x):\n        out = self.features(x)\n        out = out.view(out.size(0), -1)\n        out = self.classifier(out)\n        return out\n\n    def _make_layers(self, cfg):\n        layers = []\n        in_channels = 3\n        for x in cfg:\n            if x == 'M':\n                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n            else:\n                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n                           nn.BatchNorm2d(x),\n                           nn.ReLU(inplace=True)]\n                in_channels = x\n        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n        return nn.Sequential(*layers)\n\n# net = VGG('VGG11')\n# x = torch.randn(2,3,32,32)\n# print(net(Variable(x)).size())"""
7.Convolutional_Neural_Network(CNN) 卷积神经网络/others/mnist/main.py,9,"b'#%%\nfrom __future__ import print_function\nimport argparse\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n        self.conv2_drop = nn.Dropout2d()\n        self.fc1 = nn.Linear(320, 50)\n        self.fc2 = nn.Linear(50, 10)\n\n    def forward(self, x):\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n        x = x.view(-1, 320)\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x, training=self.training)\n        x = self.fc2(x)\n        return F.log_softmax(x, dim=1)\n\ndef train(args, model, device, train_loader, optimizer, epoch):\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = F.nll_loss(output, target)\n        loss.backward()\n        optimizer.step()\n        if batch_idx % args.log_interval == 0:\n            print(\'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\'.format(\n                epoch, batch_idx * len(data), len(train_loader.dataset),\n                100. * batch_idx / len(train_loader), loss.item()))\n\ndef test(args, model, device, test_loader):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            test_loss += F.nll_loss(output, target, reduction=\'sum\').item() # sum up batch loss\n            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n            correct += pred.eq(target.view_as(pred)).sum().item()\n\n    test_loss /= len(test_loader.dataset)\n    print(\'\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\'.format(\n        test_loss, correct, len(test_loader.dataset),\n        100. * correct / len(test_loader.dataset)))\n\ndef main():\n    # Training settings\n    parser = argparse.ArgumentParser(description=\'PyTorch MNIST Example\')\n    parser.add_argument(\'--batch-size\', type=int, default=64, metavar=\'N\',\n                        help=\'input batch size for training (default: 64)\')\n    parser.add_argument(\'--test-batch-size\', type=int, default=1000, metavar=\'N\',\n                        help=\'input batch size for testing (default: 1000)\')\n    parser.add_argument(\'--epochs\', type=int, default=10, metavar=\'N\',\n                        help=\'number of epochs to train (default: 10)\')\n    parser.add_argument(\'--lr\', type=float, default=0.01, metavar=\'LR\',\n                        help=\'learning rate (default: 0.01)\')\n    parser.add_argument(\'--momentum\', type=float, default=0.5, metavar=\'M\',\n                        help=\'SGD momentum (default: 0.5)\')\n    parser.add_argument(\'--no-cuda\', action=\'store_true\', default=False,\n                        help=\'disables CUDA training\')\n    parser.add_argument(\'--seed\', type=int, default=1, metavar=\'S\',\n                        help=\'random seed (default: 1)\')\n    parser.add_argument(\'--log-interval\', type=int, default=10, metavar=\'N\',\n                        help=\'how many batches to wait before logging training status\')\n    args = parser.parse_args()\n    use_cuda = not args.no_cuda and torch.cuda.is_available()\n\n    torch.manual_seed(args.seed)\n\n    device = torch.device(""cuda"" if use_cuda else ""cpu"")\n\n    kwargs = {\'num_workers\': 1, \'pin_memory\': True} if use_cuda else {}\n    train_loader = torch.utils.data.DataLoader(\n        datasets.MNIST(\'../data\', train=True, download=True,\n                       transform=transforms.Compose([\n                           transforms.ToTensor(),\n                           transforms.Normalize((0.1307,), (0.3081,))\n                       ])),\n        batch_size=args.batch_size, shuffle=True, **kwargs)\n    test_loader = torch.utils.data.DataLoader(\n        datasets.MNIST(\'../data\', train=False, transform=transforms.Compose([\n                           transforms.ToTensor(),\n                           transforms.Normalize((0.1307,), (0.3081,))\n                       ])),\n        batch_size=args.test_batch_size, shuffle=True, **kwargs)\n\n\n    model = Net().to(device)\n    optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n\n    for epoch in range(1, args.epochs + 1):\n        train(args, model, device, train_loader, optimizer, epoch)\n        test(args, model, device, test_loader)\n\n\nif __name__ == \'__main__\':\n    main()'"
7.Convolutional_Neural_Network(CNN) 卷积神经网络/others/mnist_hogwild/main.py,4,"b""from __future__ import print_function\nimport argparse\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.multiprocessing as mp\n\nfrom train import train, test\n\n# Training settings\nparser = argparse.ArgumentParser(description='PyTorch MNIST Example')\nparser.add_argument('--batch-size', type=int, default=64, metavar='N',\n                    help='input batch size for training (default: 64)')\nparser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n                    help='input batch size for testing (default: 1000)')\nparser.add_argument('--epochs', type=int, default=10, metavar='N',\n                    help='number of epochs to train (default: 10)')\nparser.add_argument('--lr', type=float, default=0.01, metavar='LR',\n                    help='learning rate (default: 0.01)')\nparser.add_argument('--momentum', type=float, default=0.5, metavar='M',\n                    help='SGD momentum (default: 0.5)')\nparser.add_argument('--seed', type=int, default=1, metavar='S',\n                    help='random seed (default: 1)')\nparser.add_argument('--log-interval', type=int, default=10, metavar='N',\n                    help='how many batches to wait before logging training status')\nparser.add_argument('--num-processes', type=int, default=2, metavar='N',\n                    help='how many training processes to use (default: 2)')\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n        self.conv2_drop = nn.Dropout2d()\n        self.fc1 = nn.Linear(320, 50)\n        self.fc2 = nn.Linear(50, 10)\n\n    def forward(self, x):\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n        x = x.view(-1, 320)\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x, training=self.training)\n        x = self.fc2(x)\n        return F.log_softmax(x, dim=1)\n\nif __name__ == '__main__':\n    args = parser.parse_args()\n\n    torch.manual_seed(args.seed)\n\n    model = Net()\n    model.share_memory() # gradients are allocated lazily, so they are not shared here\n\n    processes = []\n    for rank in range(args.num_processes):\n        p = mp.Process(target=train, args=(rank, args, model))\n        # We first train the model across `num_processes` processes\n        p.start()\n        processes.append(p)\n    for p in processes:\n        p.join()\n\n    # Once training is complete, we can test the model\n    test(args, model)\n\n\n"""
7.Convolutional_Neural_Network(CNN) 卷积神经网络/others/mnist_hogwild/train.py,7,"b""import os\nimport torch\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms\n\ndef train(rank, args, model):\n    torch.manual_seed(args.seed + rank)\n\n    train_loader = torch.utils.data.DataLoader(\n        datasets.MNIST('../data', train=True, download=True,\n                    transform=transforms.Compose([\n                        transforms.ToTensor(),\n                        transforms.Normalize((0.1307,), (0.3081,))\n                    ])),\n        batch_size=args.batch_size, shuffle=True, num_workers=1)\n\n    optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n    for epoch in range(1, args.epochs + 1):\n        train_epoch(epoch, args, model, train_loader, optimizer)\n\ndef test(args, model):\n    torch.manual_seed(args.seed)\n\n    test_loader = torch.utils.data.DataLoader(\n        datasets.MNIST('../data', train=False, transform=transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.1307,), (0.3081,))\n        ])),\n        batch_size=args.batch_size, shuffle=True, num_workers=1)\n\n    test_epoch(model, test_loader)\n\n\ndef train_epoch(epoch, args, model, data_loader, optimizer):\n    model.train()\n    pid = os.getpid()\n    for batch_idx, (data, target) in enumerate(data_loader):\n        optimizer.zero_grad()\n        output = model(data)\n        loss = F.nll_loss(output, target)\n        loss.backward()\n        optimizer.step()\n        if batch_idx % args.log_interval == 0:\n            print('{}\\tTrain Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                pid, epoch, batch_idx * len(data), len(data_loader.dataset),\n                100. * batch_idx / len(data_loader), loss.item()))\n\n\ndef test_epoch(model, data_loader):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for data, target in data_loader:\n            output = model(data)\n            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n            pred = output.max(1)[1] # get the index of the max log-probability\n            correct += pred.eq(target).sum().item()\n\n    test_loss /= len(data_loader.dataset)\n    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n        test_loss, correct, len(data_loader.dataset),\n        100. * correct / len(data_loader.dataset)))\n"""
