file_path,api_count,code
evaluation/eval_PCKh.py,0,"b'from scipy.io import loadmat\nfrom numpy import transpose\nimport skimage.io as sio\nfrom utils import visualize\nimport numpy as np\nimport os\nimport argparse\n\ndef main(args):\n    detection = loadmat(\'evaluation/data/detections.mat\')\n    det_idxs = detection[\'RELEASE_img_index\']\n    debug = 0\n    threshold = 0.5\n    SC_BIAS = 0.6\n\n    pa = [2, 3, 7, 7, 4, 5, 8, 9, 10, 0, 12, 13, 8, 8, 14, 15]\n\n    dict = loadmat(\'evaluation/data/detections_our_format.mat\')\n    dataset_joints = dict[\'dataset_joints\']\n    jnt_missing = dict[\'jnt_missing\']\n    pos_pred_src = dict[\'pos_pred_src\']\n    pos_gt_src = dict[\'pos_gt_src\']\n    headboxes_src = dict[\'headboxes_src\']\n\n\n\n    #predictions\n    predfile = args.result\n    preds = loadmat(predfile)[\'preds\']\n    pos_pred_src = transpose(preds, [1, 2, 0])\n\n\n    if debug:\n\n        for i in range(len(det_idxs[0])):\n            anno = mat[\'RELEASE\'][\'annolist\'][0, 0][0][det_idxs[0][i] - 1]\n            fn = anno[\'image\'][\'name\'][0, 0][0]\n            imagePath = \'data/mpii/images/\' + fn\n            oriImg = sio.imread(imagePath)\n            pred = pos_pred_src[:, :, i]\n            visualize(oriImg, pred, pa)\n\n\n    head = np.where(dataset_joints == \'head\')[1][0]\n    lsho = np.where(dataset_joints == \'lsho\')[1][0]\n    lelb = np.where(dataset_joints == \'lelb\')[1][0]\n    lwri = np.where(dataset_joints == \'lwri\')[1][0]\n    lhip = np.where(dataset_joints == \'lhip\')[1][0]\n    lkne = np.where(dataset_joints == \'lkne\')[1][0]\n    lank = np.where(dataset_joints == \'lank\')[1][0]\n\n    rsho = np.where(dataset_joints == \'rsho\')[1][0]\n    relb = np.where(dataset_joints == \'relb\')[1][0]\n    rwri = np.where(dataset_joints == \'rwri\')[1][0]\n    rkne = np.where(dataset_joints == \'rkne\')[1][0]\n    rank = np.where(dataset_joints == \'rank\')[1][0]\n    rhip = np.where(dataset_joints == \'rhip\')[1][0]\n\n    jnt_visible = 1 - jnt_missing\n    uv_error = pos_pred_src - pos_gt_src\n    uv_err = np.linalg.norm(uv_error, axis=1)\n    headsizes = headboxes_src[1, :, :] - headboxes_src[0, :, :]\n    headsizes = np.linalg.norm(headsizes, axis=0)\n    headsizes *= SC_BIAS\n    scale = np.multiply(headsizes, np.ones((len(uv_err), 1)))\n    scaled_uv_err = np.divide(uv_err, scale)\n    scaled_uv_err = np.multiply(scaled_uv_err, jnt_visible)\n    jnt_count = np.sum(jnt_visible, axis=1)\n    less_than_threshold = np.multiply((scaled_uv_err < threshold), jnt_visible)\n    PCKh = np.divide(100. * np.sum(less_than_threshold, axis=1), jnt_count)\n\n\n    # save\n    rng = np.arange(0, 0.5, 0.01)\n    pckAll = np.zeros((len(rng), 16))\n\n    for r in range(len(rng)):\n        threshold = rng[r]\n        less_than_threshold = np.multiply(scaled_uv_err < threshold, jnt_visible)\n        pckAll[r, :] = np.divide(100.*np.sum(less_than_threshold, axis=1), jnt_count)\n\n    name = predfile.split(os.sep)[-1]\n    PCKh = np.ma.array(PCKh, mask=False)\n    PCKh.mask[6:8] = True\n\n    print(\'\\nPrediction file: {}\\n\'.format(args.result))\n    print(""Head,   Shoulder, Elbow,  Wrist,   Hip ,     Knee  , Ankle ,  Mean"")\n    print(\'{:.2f}  {:.2f}     {:.2f}  {:.2f}   {:.2f}   {:.2f}   {:.2f}   {:.2f}\'.format(PCKh[head], 0.5 * (PCKh[lsho] + PCKh[rsho])\\\n            , 0.5 * (PCKh[lelb] + PCKh[relb]),0.5 * (PCKh[lwri] + PCKh[rwri]), 0.5 * (PCKh[lhip] + PCKh[rhip]), 0.5 * (PCKh[lkne] + PCKh[rkne]) \\\n            , 0.5 * (PCKh[lank] + PCKh[rank]), np.mean(PCKh)))\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(description=\'MPII PCKh Evaluation\')\n    parser.add_argument(\'-r\', \'--result\', default=\'checkpoint/mpii/hg_s2_b1/preds.mat\',\n                        type=str, metavar=\'PATH\',\n                        help=\'path to result (default: checkpoint/mpii/hg_s2_b1/preds.mat)\')\n    args = parser.parse_args()\n    main(args)\n'"
evaluation/utils.py,0,"b'\ndef visualize(oriImg, points, pa):\n    import matplotlib\n    import cv2 as cv\n    import matplotlib.pyplot as plt\n    import math\n\n    fig = matplotlib.pyplot.gcf()\n    # fig.set_size_inches(12, 12)\n\n    colors = [[255, 0, 0], [255, 85, 0], [255, 170, 0], [255, 255, 0], [170, 255, 0], [85, 255, 0], [0, 255, 0],\n              [0, 255, 85], [0, 255, 170], [0, 255, 255], [0, 170, 255], [0, 85, 255], [0, 0, 255], [85, 0, 255],\n              [170,0,255],[255,0,255]]\n    canvas = oriImg\n    stickwidth = 4\n    x = points[:, 0]\n    y = points[:, 1]\n\n    for n in range(len(x)):\n        for child in range(len(pa)):\n            if pa[child] is 0:\n                continue\n\n            x1 = x[pa[child] - 1]\n            y1 = y[pa[child] - 1]\n            x2 = x[child]\n            y2 = y[child]\n\n            cv.line(canvas, (x1, y1), (x2, y2), colors[child], 8)\n\n\n    plt.imshow(canvas[:, :, [2, 1, 0]])\n    fig = matplotlib.pyplot.gcf()\n    fig.set_size_inches(12, 12)\n\n    from time import gmtime, strftime\n    import os\n    directory = \'data/mpii/result/test_images\'\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n\n    fn = os.path.join(directory, strftime(""%Y-%m-%d-%H_%M_%S"", gmtime()) + \'.jpg\')\n\n    plt.savefig(fn)'"
example/_init_paths.py,0,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os.path as osp\nimport sys\n\n\ndef add_path(path):\n    if path not in sys.path:\n        sys.path.insert(0, path)\n\n\nthis_dir = osp.dirname(__file__)\n\nlib_path = osp.join(this_dir, '..')\nadd_path(lib_path)\n"""
example/convert_checkpoint.py,9,"b'from __future__ import print_function, absolute_import\n\nimport os\nimport argparse\nimport time\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn.parallel\nimport torch.backends.cudnn as cudnn\nimport torch.optim\nimport torchvision.datasets as datasets\n\nfrom pose import Bar\nfrom pose.utils.logger import Logger, savefig\nfrom pose.utils.evaluation import accuracy, AverageMeter, final_preds\nfrom pose.utils.misc import save_checkpoint, save_pred, adjust_learning_rate\nfrom pose.utils.osutils import mkdir_p, isfile, isdir, join\nfrom pose.utils.imutils import batch_with_heatmap\nfrom pose.utils.transforms import fliplr, flip_back\nimport pose.models as models\nimport pose.datasets as datasets\n\n\n# get model names and dataset names\nmodel_names = sorted(name for name in models.__dict__\n    if name.islower() and not name.startswith(""__"")\n    and callable(models.__dict__[name]))\n\n\ndataset_names = sorted(name for name in datasets.__dict__\n    if name.islower() and not name.startswith(""__"")\n    and callable(datasets.__dict__[name]))\n\n\n# init global variables\nbest_acc = 0\nidx = []\n\n# select proper device to run\ndevice = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\ncudnn.benchmark = True  # There is BN issue for early version of PyTorch\n                        # see https://github.com/bearpaw/pytorch-pose/issues/33\n\ndef main(args):\n    global best_acc\n    global idx\n\n    # idx is the index of joints used to compute accuracy\n    if args.dataset in [\'mpii\', \'lsp\']:\n        idx = [1,2,3,4,5,6,11,12,15,16]\n    elif args.dataset == \'mscoco\':\n        idx = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17]\n    else:\n        print(""Unknown dataset: {}"".format(args.dataset))\n        assert False\n\n    # create checkpoint dir\n    if not isdir(args.checkpoint):\n        mkdir_p(args.checkpoint)\n\n    # create model\n    njoints = datasets.__dict__[args.dataset].njoints\n\n    print(""==> creating model \'{}\', stacks={}, blocks={}"".format(args.arch, args.stacks, args.blocks))\n    model = models.__dict__[args.arch](num_stacks=args.stacks,\n                                       num_blocks=args.blocks,\n                                       num_classes=njoints)\n\n    model = torch.nn.DataParallel(model).to(device)\n\n    # define loss function (criterion) and optimizer\n    criterion = torch.nn.MSELoss(reduction=\'mean\').to(device)\n\n    optimizer = torch.optim.RMSprop(model.parameters(),\n                                    lr=args.lr,\n                                    momentum=args.momentum,\n                                    weight_decay=args.weight_decay)\n\n    # optionally resume from a checkpoint\n    title = args.dataset + \' \' + args.arch\n    if args.resume:\n        if isfile(args.resume):\n            print(""=> loading checkpoint \'{}\'"".format(args.resume))\n            checkpoint = torch.load(args.resume)\n            args.start_epoch = checkpoint[\'epoch\']\n            best_acc = checkpoint[\'best_acc\']\n            model.load_state_dict(checkpoint[\'state_dict\'])\n            optimizer.load_state_dict(checkpoint[\'optimizer\'])\n            print(""=> loaded checkpoint \'{}\' (epoch {})""\n                  .format(args.resume, checkpoint[\'epoch\']))\n            logger = Logger(join(args.checkpoint, \'log.txt\'), title=title, resume=True)\n        else:\n            print(""=> no checkpoint found at \'{}\'"".format(args.resume))\n    else:\n        logger = Logger(join(args.checkpoint, \'log.txt\'), title=title)\n        logger.set_names([\'Epoch\', \'LR\', \'Train Loss\', \'Val Loss\',\n                          \'Train Acc\', \'Val Acc\'])\n\n    print(\'    Total params: %.2fM\'\n          % (sum(p.numel() for p in model.parameters())/1000000.0))\n\n\n    # save a cleaned version of model without dict and DataParallel\n    clean_model = checkpoint[\'state_dict\']\n\n    # create new OrderedDict that does not contain `module.`\n    from collections import OrderedDict\n    clean_model = OrderedDict()\n    if any(key.startswith(\'module\') for key in checkpoint[\'state_dict\']):\n        for k, v in checkpoint[\'state_dict\'].items():\n            name = k[7:] # remove `module.`\n            clean_model[name] = v\n    # import pdb; pdb.set_trace()\n    # import pdb; pdb.set_trace()\n    # while any(key.startswith(\'module\') for key in clean_model):\n    #     clean_model = clean_model.module\n    clean_model_path = join(args.checkpoint, \'clean_model.pth.tar\')\n    torch.save(clean_model, clean_model_path)\n    print(\'clean model saved: {}\'.format(clean_model_path))\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(description=\'PyTorch ImageNet Training\')\n    # Dataset setting\n    parser.add_argument(\'--dataset\', metavar=\'DATASET\', default=\'mpii\',\n                        choices=dataset_names,\n                        help=\'Datasets: \' +\n                            \' | \'.join(dataset_names) +\n                            \' (default: mpii)\')\n    parser.add_argument(\'--year\', default=2014, type=int, metavar=\'N\',\n                        help=\'year of coco dataset: 2014 (default) | 2017)\')\n    parser.add_argument(\'--inp-res\', default=256, type=int,\n                        help=\'input resolution (default: 256)\')\n    parser.add_argument(\'--out-res\', default=64, type=int,\n                    help=\'output resolution (default: 64, to gen GT)\')\n\n    # Model structure\n    parser.add_argument(\'--arch\', \'-a\', metavar=\'ARCH\', default=\'hg\',\n                        choices=model_names,\n                        help=\'model architecture: \' +\n                            \' | \'.join(model_names) +\n                            \' (default: hg)\')\n    parser.add_argument(\'-s\', \'--stacks\', default=8, type=int, metavar=\'N\',\n                        help=\'Number of hourglasses to stack\')\n    parser.add_argument(\'--features\', default=256, type=int, metavar=\'N\',\n                        help=\'Number of features in the hourglass\')\n    parser.add_argument(\'-b\', \'--blocks\', default=1, type=int, metavar=\'N\',\n                        help=\'Number of residual modules at each location in the hourglass\')\n    # Training strategy\n    parser.add_argument(\'-j\', \'--workers\', default=1, type=int, metavar=\'N\',\n                        help=\'number of data loading workers (default: 4)\')\n    parser.add_argument(\'--epochs\', default=100, type=int, metavar=\'N\',\n                        help=\'number of total epochs to run\')\n    parser.add_argument(\'--start-epoch\', default=0, type=int, metavar=\'N\',\n                        help=\'manual epoch number (useful on restarts)\')\n    parser.add_argument(\'--train-batch\', default=6, type=int, metavar=\'N\',\n                        help=\'train batchsize\')\n    parser.add_argument(\'--test-batch\', default=6, type=int, metavar=\'N\',\n                        help=\'test batchsize\')\n    parser.add_argument(\'--lr\', \'--learning-rate\', default=2.5e-4, type=float,\n                        metavar=\'LR\', help=\'initial learning rate\')\n    parser.add_argument(\'--momentum\', default=0, type=float, metavar=\'M\',\n                        help=\'momentum\')\n    parser.add_argument(\'--weight-decay\', \'--wd\', default=0, type=float,\n                        metavar=\'W\', help=\'weight decay (default: 0)\')\n    parser.add_argument(\'--schedule\', type=int, nargs=\'+\', default=[60, 90],\n                        help=\'Decrease learning rate at these epochs.\')\n    parser.add_argument(\'--gamma\', type=float, default=0.1,\n                        help=\'LR is multiplied by gamma on schedule.\')\n    # Data processing\n    parser.add_argument(\'-f\', \'--flip\', dest=\'flip\', action=\'store_true\',\n                        help=\'flip the input during validation\')\n    parser.add_argument(\'--sigma\', type=float, default=1,\n                        help=\'Groundtruth Gaussian sigma.\')\n    parser.add_argument(\'--scale-factor\', type=float, default=0.25,\n                        help=\'Scale factor (data aug).\')\n    parser.add_argument(\'--rot-factor\', type=float, default=30,\n                        help=\'Rotation factor (data aug).\')\n    parser.add_argument(\'--sigma-decay\', type=float, default=0,\n                        help=\'Sigma decay rate for each epoch.\')\n    parser.add_argument(\'--label-type\', metavar=\'LABELTYPE\', default=\'Gaussian\',\n                        choices=[\'Gaussian\', \'Cauchy\'],\n                        help=\'Labelmap dist type: (default=Gaussian)\')\n    # Miscs\n    parser.add_argument(\'-c\', \'--checkpoint\', default=\'checkpoint\', type=str, metavar=\'PATH\',\n                        help=\'path to save checkpoint (default: checkpoint)\')\n    parser.add_argument(\'--snapshot\', default=0, type=int,\n                        help=\'save models for every #snapshot epochs (default: 0)\')\n    parser.add_argument(\'--resume\', default=\'\', type=str, metavar=\'PATH\',\n                        help=\'path to latest checkpoint (default: none)\')\n    parser.add_argument(\'-e\', \'--evaluate\', dest=\'evaluate\', action=\'store_true\',\n                        help=\'evaluate model on validation set\')\n    parser.add_argument(\'-d\', \'--debug\', dest=\'debug\', action=\'store_true\',\n                        help=\'show intermediate results\')\n\n\n    main(parser.parse_args())\n'"
example/main.py,13,"b'from __future__ import print_function, absolute_import\n\nimport os\nimport argparse\nimport time\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn.parallel\nimport torch.backends.cudnn as cudnn\nimport torch.optim\nimport torchvision.datasets as datasets\n\nimport _init_paths\nfrom pose import Bar\nfrom pose.utils.logger import Logger, savefig\nfrom pose.utils.evaluation import accuracy, AverageMeter, final_preds\nfrom pose.utils.misc import save_checkpoint, save_pred, adjust_learning_rate\nfrom pose.utils.osutils import mkdir_p, isfile, isdir, join\nfrom pose.utils.imutils import batch_with_heatmap\nfrom pose.utils.transforms import fliplr, flip_back\nimport pose.models as models\nimport pose.datasets as datasets\nimport pose.losses as losses\n\n\n# get model names and dataset names\nmodel_names = sorted(name for name in models.__dict__\n    if name.islower() and not name.startswith(""__"")\n    and callable(models.__dict__[name]))\n\n\ndataset_names = sorted(name for name in datasets.__dict__\n    if name.islower() and not name.startswith(""__"")\n    and callable(datasets.__dict__[name]))\n\n\n# init global variables\nbest_acc = 0\nidx = []\n\n# select proper device to run\ndevice = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\ncudnn.benchmark = True  # There is BN issue for early version of PyTorch\n                        # see https://github.com/bearpaw/pytorch-pose/issues/33\n\ndef main(args):\n    global best_acc\n    global idx\n\n    # idx is the index of joints used to compute accuracy\n    if args.dataset in [\'mpii\', \'lsp\']:\n        idx = [1,2,3,4,5,6,11,12,15,16]\n    elif args.dataset == \'coco\':\n        idx = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17]\n    else:\n        print(""Unknown dataset: {}"".format(args.dataset))\n        assert False\n\n    # create checkpoint dir\n    if not isdir(args.checkpoint):\n        mkdir_p(args.checkpoint)\n\n    # create model\n    njoints = datasets.__dict__[args.dataset].njoints\n\n    print(""==> creating model \'{}\', stacks={}, blocks={}"".format(args.arch, args.stacks, args.blocks))\n    model = models.__dict__[args.arch](num_stacks=args.stacks,\n                                       num_blocks=args.blocks,\n                                       num_classes=njoints,\n                                       resnet_layers=args.resnet_layers)\n\n    model = torch.nn.DataParallel(model).to(device)\n\n    # define loss function (criterion) and optimizer\n    criterion = losses.JointsMSELoss().to(device)\n\n    if args.solver == \'rms\':\n        optimizer = torch.optim.RMSprop(model.parameters(),\n                                        lr=args.lr,\n                                        momentum=args.momentum,\n                                        weight_decay=args.weight_decay)\n    elif args.solver == \'adam\':\n        optimizer = torch.optim.Adam(\n            model.parameters(),\n            lr=args.lr,\n        )\n    else:\n        print(\'Unknown solver: {}\'.format(args.solver))\n        assert False\n\n    # optionally resume from a checkpoint\n    title = args.dataset + \' \' + args.arch\n    if args.resume:\n        if isfile(args.resume):\n            print(""=> loading checkpoint \'{}\'"".format(args.resume))\n            checkpoint = torch.load(args.resume)\n            args.start_epoch = checkpoint[\'epoch\']\n            best_acc = checkpoint[\'best_acc\']\n            model.load_state_dict(checkpoint[\'state_dict\'])\n            optimizer.load_state_dict(checkpoint[\'optimizer\'])\n            print(""=> loaded checkpoint \'{}\' (epoch {})""\n                  .format(args.resume, checkpoint[\'epoch\']))\n            logger = Logger(join(args.checkpoint, \'log.txt\'), title=title, resume=True)\n        else:\n            print(""=> no checkpoint found at \'{}\'"".format(args.resume))\n    else:\n        logger = Logger(join(args.checkpoint, \'log.txt\'), title=title)\n        logger.set_names([\'Epoch\', \'LR\', \'Train Loss\', \'Val Loss\',\n                          \'Train Acc\', \'Val Acc\'])\n\n    print(\'    Total params: %.2fM\'\n          % (sum(p.numel() for p in model.parameters())/1000000.0))\n\n    # create data loader\n    train_dataset = datasets.__dict__[args.dataset](is_train=True, **vars(args))\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        batch_size=args.train_batch, shuffle=True,\n        num_workers=args.workers, pin_memory=True\n    )\n\n    val_dataset = datasets.__dict__[args.dataset](is_train=False, **vars(args))\n    val_loader = torch.utils.data.DataLoader(\n        val_dataset,\n        batch_size=args.test_batch, shuffle=False,\n        num_workers=args.workers, pin_memory=True\n    )\n\n    # evaluation only\n    if args.evaluate:\n        print(\'\\nEvaluation only\')\n        loss, acc, predictions = validate(val_loader, model, criterion, njoints,\n                                          args.debug, args.flip)\n        save_pred(predictions, checkpoint=args.checkpoint)\n        return\n\n    # train and eval\n    lr = args.lr\n    for epoch in range(args.start_epoch, args.epochs):\n        lr = adjust_learning_rate(optimizer, epoch, lr, args.schedule, args.gamma)\n        print(\'\\nEpoch: %d | LR: %.8f\' % (epoch + 1, lr))\n\n        # decay sigma\n        if args.sigma_decay > 0:\n            train_loader.dataset.sigma *=  args.sigma_decay\n            val_loader.dataset.sigma *=  args.sigma_decay\n\n        # train for one epoch\n        train_loss, train_acc = train(train_loader, model, criterion, optimizer,\n                                      args.debug, args.flip)\n\n        # evaluate on validation set\n        valid_loss, valid_acc, predictions = validate(val_loader, model, criterion,\n                                                  njoints, args.debug, args.flip)\n\n        # append logger file\n        logger.append([epoch + 1, lr, train_loss, valid_loss, train_acc, valid_acc])\n\n        # remember best acc and save checkpoint\n        is_best = valid_acc > best_acc\n        best_acc = max(valid_acc, best_acc)\n        save_checkpoint({\n            \'epoch\': epoch + 1,\n            \'arch\': args.arch,\n            \'state_dict\': model.state_dict(),\n            \'best_acc\': best_acc,\n            \'optimizer\' : optimizer.state_dict(),\n        }, predictions, is_best, checkpoint=args.checkpoint, snapshot=args.snapshot)\n\n    logger.close()\n    logger.plot([\'Train Acc\', \'Val Acc\'])\n    savefig(os.path.join(args.checkpoint, \'log.eps\'))\n\n\ndef train(train_loader, model, criterion, optimizer, debug=False, flip=True):\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n    acces = AverageMeter()\n\n    # switch to train mode\n    model.train()\n\n    end = time.time()\n\n    gt_win, pred_win = None, None\n    bar = Bar(\'Train\', max=len(train_loader))\n    for i, (input, target, meta) in enumerate(train_loader):\n        # measure data loading time\n        data_time.update(time.time() - end)\n\n        input, target = input.to(device), target.to(device, non_blocking=True)\n        target_weight = meta[\'target_weight\'].to(device, non_blocking=True)\n\n        # compute output\n        output = model(input)\n        if type(output) == list:  # multiple output\n            loss = 0\n            for o in output:\n                loss += criterion(o, target, target_weight)\n            output = output[-1]\n        else:  # single output\n            loss = criterion(output, target, target_weight)\n        acc = accuracy(output, target, idx)\n\n        if debug: # visualize groundtruth and predictions\n            gt_batch_img = batch_with_heatmap(input, target)\n            pred_batch_img = batch_with_heatmap(input, output)\n            if not gt_win or not pred_win:\n                ax1 = plt.subplot(121)\n                ax1.title.set_text(\'Groundtruth\')\n                gt_win = plt.imshow(gt_batch_img)\n                ax2 = plt.subplot(122)\n                ax2.title.set_text(\'Prediction\')\n                pred_win = plt.imshow(pred_batch_img)\n            else:\n                gt_win.set_data(gt_batch_img)\n                pred_win.set_data(pred_batch_img)\n            plt.pause(.05)\n            plt.draw()\n\n        # measure accuracy and record loss\n        losses.update(loss.item(), input.size(0))\n        acces.update(acc[0], input.size(0))\n\n        # compute gradient and do SGD step\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        # plot progress\n        bar.suffix  = \'({batch}/{size}) Data: {data:.6f}s | Batch: {bt:.3f}s | Total: {total:} | ETA: {eta:} | Loss: {loss:.4f} | Acc: {acc: .4f}\'.format(\n                    batch=i + 1,\n                    size=len(train_loader),\n                    data=data_time.val,\n                    bt=batch_time.val,\n                    total=bar.elapsed_td,\n                    eta=bar.eta_td,\n                    loss=losses.avg,\n                    acc=acces.avg\n                    )\n        bar.next()\n\n    bar.finish()\n    return losses.avg, acces.avg\n\n\ndef validate(val_loader, model, criterion, num_classes, debug=False, flip=True):\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n    acces = AverageMeter()\n\n    # predictions\n    predictions = torch.Tensor(val_loader.dataset.__len__(), num_classes, 2)\n\n    # switch to evaluate mode\n    model.eval()\n\n    gt_win, pred_win = None, None\n    end = time.time()\n    bar = Bar(\'Eval \', max=len(val_loader))\n    with torch.no_grad():\n        for i, (input, target, meta) in enumerate(val_loader):\n            # measure data loading time\n            data_time.update(time.time() - end)\n\n            input = input.to(device, non_blocking=True)\n            target = target.to(device, non_blocking=True)\n            target_weight = meta[\'target_weight\'].to(device, non_blocking=True)\n\n            # compute output\n            output = model(input)\n            score_map = output[-1].cpu() if type(output) == list else output.cpu()\n            if flip:\n                flip_input = torch.from_numpy(fliplr(input.clone().numpy())).float().to(device)\n                flip_output = model(flip_input)\n                flip_output = flip_output[-1].cpu() if type(flip_output) == list else flip_output.cpu()\n                flip_output = flip_back(flip_output)\n                score_map += flip_output\n\n\n\n            if type(output) == list:  # multiple output\n                loss = 0\n                for o in output:\n                    loss += criterion(o, target, target_weight)\n                output = output[-1]\n            else:  # single output\n                loss = criterion(output, target, target_weight)\n\n            acc = accuracy(score_map, target.cpu(), idx)\n\n            # generate predictions\n            preds = final_preds(score_map, meta[\'center\'], meta[\'scale\'], [64, 64])\n            for n in range(score_map.size(0)):\n                predictions[meta[\'index\'][n], :, :] = preds[n, :, :]\n\n\n            if debug:\n                gt_batch_img = batch_with_heatmap(input, target)\n                pred_batch_img = batch_with_heatmap(input, score_map)\n                if not gt_win or not pred_win:\n                    plt.subplot(121)\n                    gt_win = plt.imshow(gt_batch_img)\n                    plt.subplot(122)\n                    pred_win = plt.imshow(pred_batch_img)\n                else:\n                    gt_win.set_data(gt_batch_img)\n                    pred_win.set_data(pred_batch_img)\n                plt.pause(.05)\n                plt.draw()\n\n            # measure accuracy and record loss\n            losses.update(loss.item(), input.size(0))\n            acces.update(acc[0], input.size(0))\n\n            # measure elapsed time\n            batch_time.update(time.time() - end)\n            end = time.time()\n\n            # plot progress\n            bar.suffix  = \'({batch}/{size}) Data: {data:.6f}s | Batch: {bt:.3f}s | Total: {total:} | ETA: {eta:} | Loss: {loss:.4f} | Acc: {acc: .4f}\'.format(\n                        batch=i + 1,\n                        size=len(val_loader),\n                        data=data_time.val,\n                        bt=batch_time.avg,\n                        total=bar.elapsed_td,\n                        eta=bar.eta_td,\n                        loss=losses.avg,\n                        acc=acces.avg\n                        )\n            bar.next()\n\n        bar.finish()\n    return losses.avg, acces.avg, predictions\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(description=\'PyTorch ImageNet Training\')\n    # Dataset setting\n    parser.add_argument(\'--dataset\', metavar=\'DATASET\', default=\'mpii\',\n                        choices=dataset_names,\n                        help=\'Datasets: \' +\n                            \' | \'.join(dataset_names) +\n                            \' (default: mpii)\')\n    parser.add_argument(\'--image-path\', default=\'\', type=str,\n                        help=\'path to images\')\n    parser.add_argument(\'--anno-path\', default=\'\', type=str,\n                        help=\'path to annotation (json)\')\n    parser.add_argument(\'--year\', default=2014, type=int, metavar=\'N\',\n                        help=\'year of coco dataset: 2014 (default) | 2017)\')\n    parser.add_argument(\'--inp-res\', default=256, type=int,\n                        help=\'input resolution (default: 256)\')\n    parser.add_argument(\'--out-res\', default=64, type=int,\n                    help=\'output resolution (default: 64, to gen GT)\')\n\n    # Model structure\n    parser.add_argument(\'--arch\', \'-a\', metavar=\'ARCH\', default=\'hg\',\n                        choices=model_names,\n                        help=\'model architecture: \' +\n                            \' | \'.join(model_names) +\n                            \' (default: hg)\')\n    parser.add_argument(\'-s\', \'--stacks\', default=8, type=int, metavar=\'N\',\n                        help=\'Number of hourglasses to stack\')\n    parser.add_argument(\'--features\', default=256, type=int, metavar=\'N\',\n                        help=\'Number of features in the hourglass\')\n    parser.add_argument(\'--resnet-layers\', default=50, type=int, metavar=\'N\',\n                        help=\'Number of resnet layers\',\n                        choices=[18, 34, 50, 101, 152])\n    parser.add_argument(\'-b\', \'--blocks\', default=1, type=int, metavar=\'N\',\n                        help=\'Number of residual modules at each location in the hourglass\')\n    # Training strategy\n    parser.add_argument(\'--solver\', metavar=\'SOLVER\', default=\'rms\',\n                        choices=[\'rms\', \'adam\'],\n                        help=\'optimizers\')\n    parser.add_argument(\'-j\', \'--workers\', default=1, type=int, metavar=\'N\',\n                        help=\'number of data loading workers (default: 4)\')\n    parser.add_argument(\'--epochs\', default=100, type=int, metavar=\'N\',\n                        help=\'number of total epochs to run\')\n    parser.add_argument(\'--start-epoch\', default=0, type=int, metavar=\'N\',\n                        help=\'manual epoch number (useful on restarts)\')\n    parser.add_argument(\'--train-batch\', default=6, type=int, metavar=\'N\',\n                        help=\'train batchsize\')\n    parser.add_argument(\'--test-batch\', default=6, type=int, metavar=\'N\',\n                        help=\'test batchsize\')\n    parser.add_argument(\'--lr\', \'--learning-rate\', default=2.5e-4, type=float,\n                        metavar=\'LR\', help=\'initial learning rate\')\n    parser.add_argument(\'--momentum\', default=0, type=float, metavar=\'M\',\n                        help=\'momentum\')\n    parser.add_argument(\'--weight-decay\', \'--wd\', default=0, type=float,\n                        metavar=\'W\', help=\'weight decay (default: 0)\')\n    parser.add_argument(\'--schedule\', type=int, nargs=\'+\', default=[60, 90],\n                        help=\'Decrease learning rate at these epochs.\')\n    parser.add_argument(\'--gamma\', type=float, default=0.1,\n                        help=\'LR is multiplied by gamma on schedule.\')\n    parser.add_argument(\'--target-weight\', dest=\'target_weight\',\n                        action=\'store_true\',\n                        help=\'Loss with target_weight\')\n    # Data processing\n    parser.add_argument(\'-f\', \'--flip\', dest=\'flip\', action=\'store_true\',\n                        help=\'flip the input during validation\')\n    parser.add_argument(\'--sigma\', type=float, default=1,\n                        help=\'Groundtruth Gaussian sigma.\')\n    parser.add_argument(\'--scale-factor\', type=float, default=0.25,\n                        help=\'Scale factor (data aug).\')\n    parser.add_argument(\'--rot-factor\', type=float, default=30,\n                        help=\'Rotation factor (data aug).\')\n    parser.add_argument(\'--sigma-decay\', type=float, default=0,\n                        help=\'Sigma decay rate for each epoch.\')\n    parser.add_argument(\'--label-type\', metavar=\'LABELTYPE\', default=\'Gaussian\',\n                        choices=[\'Gaussian\', \'Cauchy\'],\n                        help=\'Labelmap dist type: (default=Gaussian)\')\n    # Miscs\n    parser.add_argument(\'-c\', \'--checkpoint\', default=\'checkpoint\', type=str, metavar=\'PATH\',\n                        help=\'path to save checkpoint (default: checkpoint)\')\n    parser.add_argument(\'--snapshot\', default=0, type=int,\n                        help=\'save models for every #snapshot epochs (default: 0)\')\n    parser.add_argument(\'--resume\', default=\'\', type=str, metavar=\'PATH\',\n                        help=\'path to latest checkpoint (default: none)\')\n    parser.add_argument(\'-e\', \'--evaluate\', dest=\'evaluate\', action=\'store_true\',\n                        help=\'evaluate model on validation set\')\n    parser.add_argument(\'-d\', \'--debug\', dest=\'debug\', action=\'store_true\',\n                        help=\'show intermediate results\')\n\n\n    main(parser.parse_args())\n'"
pose/__init__.py,0,"b'from __future__ import absolute_import\n\nfrom . import datasets\nfrom . import models\nfrom . import utils\nfrom . import losses\n\nimport os, sys\nsys.path.append(os.path.join(os.path.dirname(__file__), ""progress""))\nfrom progress.bar import Bar as Bar\n\n__version__ = \'0.1.0\'\n'"
pose/datasets/__init__.py,0,"b""from .mpii import mpii\nfrom .coco import coco\nfrom .lsp import lsp\n\n__all__ = ('mpii', 'coco', 'lsp')\n"""
pose/datasets/coco.py,12,"b""from __future__ import print_function, absolute_import\n\nimport os\nimport numpy as np\nimport json\nimport random\nimport math\n\nimport torch\nimport torch.utils.data as data\n\nfrom pose.utils.osutils import *\nfrom pose.utils.imutils import *\nfrom pose.utils.transforms import *\n\n\nclass Mscoco(data.Dataset):\n    def __init__(self, is_train=True, **kwargs):\n        self.is_train   = is_train # training set or test set\n        self.inp_res    = kwargs['inp_res']\n        self.out_res    = kwargs['out_res']\n        self.sigma      = kwargs['sigma']\n        self.scale_factor = kwargs['scale_factor']\n        self.rot_factor = kwargs['rot_factor']\n        self.label_type = kwargs['label_type']\n        self.year       = kwargs['year']\n        self.jsonfile   = kwargs['anno_path']\n        img_folder = kwargs['image_path'] # root image folders\n\n        if is_train:\n            self.img_folder = '{}/train{}'.format(img_folder, self.year)    # root image folders\n        else:\n            self.img_folder = '{}/val{}'.format(img_folder, self.year)    # root image folders\n\n        # create train/val split\n        with open(self.jsonfile) as anno_file:\n            self.anno = json.load(anno_file)\n\n        self.train, self.valid = [], []\n        for idx, val in enumerate(self.anno):\n            if val['isValidation'] == True:\n                self.valid.append(idx)\n            else:\n                self.train.append(idx)\n        self.mean, self.std = self._compute_mean()\n\n    def _compute_mean(self):\n        meanstd_file = './data/coco/mean.pth.tar'\n        if isfile(meanstd_file):\n            meanstd = torch.load(meanstd_file)\n        else:\n            print('==> compute mean')\n            mean = torch.zeros(3)\n            std = torch.zeros(3)\n            cnt = 0\n            for index in self.train:\n                cnt += 1\n                print( '{} | {}'.format(cnt, len(self.train)))\n                a = self.anno[index]\n                img_path = os.path.join(self.img_folder, a['img_paths'])\n                img = load_image(img_path) # CxHxW\n                mean += img.view(img.size(0), -1).mean(1)\n                std += img.view(img.size(0), -1).std(1)\n            mean /= len(self.train)\n            std /= len(self.train)\n            meanstd = {\n                'mean': mean,\n                'std': std,\n                }\n            torch.save(meanstd, meanstd_file)\n        if self.is_train:\n            print('    Mean: %.4f, %.4f, %.4f' % (meanstd['mean'][0], meanstd['mean'][1], meanstd['mean'][2]))\n            print('    Std:  %.4f, %.4f, %.4f' % (meanstd['std'][0], meanstd['std'][1], meanstd['std'][2]))\n\n        return meanstd['mean'], meanstd['std']\n\n    def __getitem__(self, index):\n        sf = self.scale_factor\n        rf = self.rot_factor\n        if self.is_train:\n            a = self.anno[self.train[index]]\n        else:\n            a = self.anno[self.valid[index]]\n\n        img_path = os.path.join(self.img_folder, a['img_paths'])\n        pts = torch.Tensor(a['joint_self'])\n        # pts[:, 0:2] -= 1  # Convert pts to zero based\n\n        # c = torch.Tensor(a['objpos']) - 1\n        c = torch.Tensor(a['objpos'])\n        s = a['scale_provided']\n\n        # Adjust center/scale slightly to avoid cropping limbs\n        if c[0] != -1:\n            c[1] = c[1] + 15 * s\n            s = s * 1.25\n\n        # For single-person pose estimation with a centered/scaled figure\n        nparts = pts.size(0)\n        img = load_image(img_path)  # CxHxW\n\n        r = 0\n        if self.is_train:\n            s = s*torch.randn(1).mul_(sf).add_(1).clamp(1-sf, 1+sf)[0]\n            r = torch.randn(1).mul_(rf).clamp(-2*rf, 2*rf)[0] if random.random() <= 0.6 else 0\n\n            # Flip\n            if random.random() <= 0.5:\n                img = torch.from_numpy(fliplr(img.numpy())).float()\n                pts = shufflelr(pts, width=img.size(2), dataset='mpii')\n                c[0] = img.size(2) - c[0]\n\n            # Color\n            img[0, :, :].mul_(random.uniform(0.8, 1.2)).clamp_(0, 1)\n            img[1, :, :].mul_(random.uniform(0.8, 1.2)).clamp_(0, 1)\n            img[2, :, :].mul_(random.uniform(0.8, 1.2)).clamp_(0, 1)\n\n        # Prepare image and groundtruth map\n        inp = crop(img, c, s, [self.inp_res, self.inp_res], rot=r)\n        inp = color_normalize(inp, self.mean, self.std)\n\n        # Generate ground truth\n        tpts = pts.clone()\n        target = torch.zeros(nparts, self.out_res, self.out_res)\n        target_weight = tpts[:, 2].clone().view(nparts, 1)\n        for i in range(nparts):\n            if tpts[i, 2] > 0: # COCO visible: 0-no label, 1-label + invisible, 2-label + visible\n                tpts[i, 0:2] = to_torch(transform(tpts[i, 0:2]+1, c, s, [self.out_res, self.out_res], rot=r))\n                target[i], vis = draw_labelmap(target[i], tpts[i]-1, self.sigma, type=self.label_type)\n                target_weight[i, 0] = vis\n\n        # Meta info\n        meta = {'index' : index, 'center' : c,\n                'scale' : s, 'pts' : pts,\n                'tpts' : tpts, 'img_path' : img_path,\n                'target_weight': target_weight}\n\n        return inp, target, meta\n\n    def __len__(self):\n        if self.is_train:\n            return len(self.train)\n        else:\n            return len(self.valid)\n\ndef coco(**kwargs):\n    return Mscoco(**kwargs)\n\ncoco.njoints = 17  # ugly but works\n"""
pose/datasets/lsp.py,12,"b'from __future__ import print_function, absolute_import\n\nimport os\nimport numpy as np\nimport json\nimport random\nimport math\n\nimport torch\nimport torch.utils.data as data\n\nfrom pose.utils.osutils import *\nfrom pose.utils.imutils import *\nfrom pose.utils.transforms import *\n\n\nclass LSP(data.Dataset):\n    """"""\n    LSP extended dataset (11,000 train, 1000 test)\n    Original datasets contain 14 keypoints. We interpolate mid-hip and mid-shoulder and change the indices to match\n    the MPII dataset (16 keypoints).\n\n    Wei Yang (bearpaw@GitHub)\n    2017-09-28\n    """"""\n    def __init__(self, is_train = True, **kwargs):\n        self.img_folder = \'./data/lsp\'    # root image folders\n        self.jsonfile = \'./data/lsp/LEEDS_annotations.json\'\n        self.is_train = is_train           # training set or test set\n        self.is_train   = is_train # training set or test set\n        self.inp_res    = kwargs[\'inp_res\']\n        self.out_res    = kwargs[\'out_res\']\n        self.sigma      = kwargs[\'sigma\']\n        self.scale_factor = kwargs[\'scale_factor\']\n        self.rot_factor = kwargs[\'rot_factor\']\n        self.label_type = kwargs[\'label_type\']\n\n        # create train/val split\n        with open(self.jsonfile) as anno_file:\n            self.anno = json.load(anno_file)\n\n        self.train, self.valid = [], []\n        for idx, val in enumerate(self.anno):\n            if val[\'isValidation\'] == True:\n                self.valid.append(idx)\n            else:\n                self.train.append(idx)\n        self.mean, self.std = self._compute_mean()\n\n    def _compute_mean(self):\n        meanstd_file = \'./data/lsp/mean.pth.tar\'\n        if isfile(meanstd_file):\n            meanstd = torch.load(meanstd_file)\n        else:\n            mean = torch.zeros(3)\n            std = torch.zeros(3)\n            for index in self.train:\n                a = self.anno[index]\n                img_path = os.path.join(self.img_folder, a[\'img_paths\'])\n                img = load_image(img_path) # CxHxW\n                mean += img.view(img.size(0), -1).mean(1)\n                std += img.view(img.size(0), -1).std(1)\n            mean /= len(self.train)\n            std /= len(self.train)\n            meanstd = {\n                \'mean\': mean,\n                \'std\': std,\n                }\n            torch.save(meanstd, meanstd_file)\n        if self.is_train:\n            print(\'    Mean: %.4f, %.4f, %.4f\' % (meanstd[\'mean\'][0], meanstd[\'mean\'][1], meanstd[\'mean\'][2]))\n            print(\'    Std:  %.4f, %.4f, %.4f\' % (meanstd[\'std\'][0], meanstd[\'std\'][1], meanstd[\'std\'][2]))\n\n        return meanstd[\'mean\'], meanstd[\'std\']\n\n    def __getitem__(self, index):\n        sf = self.scale_factor\n        rf = self.rot_factor\n        if self.is_train:\n            a = self.anno[self.train[index]]\n        else:\n            a = self.anno[self.valid[index]]\n\n        img_path = os.path.join(self.img_folder, a[\'img_paths\'])\n        pts = torch.Tensor(a[\'joint_self\'])\n        # pts[:, 0:2] -= 1  # Convert pts to zero based\n\n        # c = torch.Tensor(a[\'objpos\']) - 1\n        c = torch.Tensor(a[\'objpos\'])\n        s = a[\'scale_provided\']\n\n        # Adjust center/scale slightly to avoid cropping limbs\n        if c[0] != -1:\n            # c[1] = c[1] + 15 * s\n            s = s * 1.4375\n\n        # For single-person pose estimation with a centered/scaled figure\n        nparts = pts.size(0)\n        img = load_image(img_path)  # CxHxW\n\n        r = 0\n        # if self.is_train:\n        #     s = s*torch.randn(1).mul_(sf).add_(1).clamp(1-sf, 1+sf)[0]\n        #     r = torch.randn(1).mul_(rf).clamp(-2*rf, 2*rf)[0] if random.random() <= 0.6 else 0\n        #\n        #     # # Flip\n        #     # if random.random() <= 0.5:\n        #     #     img = torch.from_numpy(fliplr(img.numpy())).float()\n        #     #     pts = shufflelr(pts, width=img.size(2), dataset=\'mpii\')\n        #     #     c[0] = img.size(2) - c[0]\n        #\n        #     # Color\n        #     img[0, :, :].mul_(random.uniform(0.8, 1.2)).clamp_(0, 1)\n        #     img[1, :, :].mul_(random.uniform(0.8, 1.2)).clamp_(0, 1)\n        #     img[2, :, :].mul_(random.uniform(0.8, 1.2)).clamp_(0, 1)\n\n        # Prepare image and groundtruth map\n        inp = crop(img, c, s, [self.inp_res, self.inp_res], rot=r)\n        inp = color_normalize(inp, self.mean, self.std)\n\n        # Generate ground truth\n        tpts = pts.clone()\n        target = torch.zeros(nparts, self.out_res, self.out_res)\n        for i in range(nparts):\n            # if tpts[i, 2] > 0: # This is evil!!\n            if tpts[i, 0] > 0:\n                tpts[i, 0:2] = to_torch(transform(tpts[i, 0:2]+1, c, s, [self.out_res, self.out_res], rot=r))\n                target[i] = draw_labelmap(target[i], tpts[i]-1, self.sigma, type=self.label_type)\n\n        # Meta info\n        meta = {\'index\' : index, \'center\' : c, \'scale\' : s,\n        \'pts\' : pts, \'tpts\' : tpts}\n\n        return inp, target, meta\n\n    def __len__(self):\n        if self.is_train:\n            return len(self.train)\n        else:\n            return len(self.valid)\n\n\ndef lsp(**kwargs):\n    return LSP(**kwargs)\n\nlsp.njoints = 16  # ugly but works\n'"
pose/datasets/mpii.py,12,"b""from __future__ import print_function, absolute_import\n\nimport os\nimport numpy as np\nimport json\nimport random\nimport math\n\nimport torch\nimport torch.utils.data as data\n\nfrom pose.utils.osutils import *\nfrom pose.utils.imutils import *\nfrom pose.utils.transforms import *\n\n\nclass Mpii(data.Dataset):\n    def __init__(self, is_train = True, **kwargs):\n        self.img_folder = kwargs['image_path'] # root image folders\n        self.jsonfile   = kwargs['anno_path']\n        self.is_train   = is_train # training set or test set\n        self.inp_res    = kwargs['inp_res']\n        self.out_res    = kwargs['out_res']\n        self.sigma      = kwargs['sigma']\n        self.scale_factor = kwargs['scale_factor']\n        self.rot_factor = kwargs['rot_factor']\n        self.label_type = kwargs['label_type']\n\n        # create train/val split\n        with open(self.jsonfile) as anno_file:\n            self.anno = json.load(anno_file)\n\n        self.train_list, self.valid_list = [], []\n        for idx, val in enumerate(self.anno):\n            if val['isValidation'] == True:\n                self.valid_list.append(idx)\n            else:\n                self.train_list.append(idx)\n        self.mean, self.std = self._compute_mean()\n\n    def _compute_mean(self):\n        meanstd_file = './data/mpii/mean.pth.tar'\n        if isfile(meanstd_file):\n            meanstd = torch.load(meanstd_file)\n        else:\n            mean = torch.zeros(3)\n            std = torch.zeros(3)\n            for index in self.train_list:\n                a = self.anno[index]\n                img_path = os.path.join(self.img_folder, a['img_paths'])\n                img = load_image(img_path) # CxHxW\n                mean += img.view(img.size(0), -1).mean(1)\n                std += img.view(img.size(0), -1).std(1)\n            mean /= len(self.train_list)\n            std /= len(self.train_list)\n            meanstd = {\n                'mean': mean,\n                'std': std,\n                }\n            torch.save(meanstd, meanstd_file)\n        if self.is_train:\n            print('    Mean: %.4f, %.4f, %.4f' % (meanstd['mean'][0], meanstd['mean'][1], meanstd['mean'][2]))\n            print('    Std:  %.4f, %.4f, %.4f' % (meanstd['std'][0], meanstd['std'][1], meanstd['std'][2]))\n\n        return meanstd['mean'], meanstd['std']\n\n    def __getitem__(self, index):\n        sf = self.scale_factor\n        rf = self.rot_factor\n        if self.is_train:\n            a = self.anno[self.train_list[index]]\n        else:\n            a = self.anno[self.valid_list[index]]\n\n        img_path = os.path.join(self.img_folder, a['img_paths'])\n        pts = torch.Tensor(a['joint_self'])\n        # pts[:, 0:2] -= 1  # Convert pts to zero based\n\n        # c = torch.Tensor(a['objpos']) - 1\n        c = torch.Tensor(a['objpos'])\n        s = a['scale_provided']\n\n        # Adjust center/scale slightly to avoid cropping limbs\n        if c[0] != -1:\n            c[1] = c[1] + 15 * s\n            s = s * 1.25\n\n        # For single-person pose estimation with a centered/scaled figure\n        nparts = pts.size(0)\n        img = load_image(img_path)  # CxHxW\n\n        r = 0\n        if self.is_train:\n            s = s*torch.randn(1).mul_(sf).add_(1).clamp(1-sf, 1+sf)[0]\n            r = torch.randn(1).mul_(rf).clamp(-2*rf, 2*rf)[0] if random.random() <= 0.6 else 0\n\n            # Flip\n            if random.random() <= 0.5:\n                img = torch.from_numpy(fliplr(img.numpy())).float()\n                pts = shufflelr(pts, width=img.size(2), dataset='mpii')\n                c[0] = img.size(2) - c[0]\n\n            # Color\n            img[0, :, :].mul_(random.uniform(0.8, 1.2)).clamp_(0, 1)\n            img[1, :, :].mul_(random.uniform(0.8, 1.2)).clamp_(0, 1)\n            img[2, :, :].mul_(random.uniform(0.8, 1.2)).clamp_(0, 1)\n\n        # Prepare image and groundtruth map\n        inp = crop(img, c, s, [self.inp_res, self.inp_res], rot=r)\n        inp = color_normalize(inp, self.mean, self.std)\n\n        # Generate ground truth\n        tpts = pts.clone()\n        target = torch.zeros(nparts, self.out_res, self.out_res)\n        target_weight = tpts[:, 2].clone().view(nparts, 1)\n\n        for i in range(nparts):\n            # if tpts[i, 2] > 0: # This is evil!!\n            if tpts[i, 1] > 0:\n                tpts[i, 0:2] = to_torch(transform(tpts[i, 0:2]+1, c, s, [self.out_res, self.out_res], rot=r))\n                target[i], vis = draw_labelmap(target[i], tpts[i]-1, self.sigma, type=self.label_type)\n                target_weight[i, 0] *= vis\n\n        # Meta info\n        meta = {'index' : index, 'center' : c, 'scale' : s,\n        'pts' : pts, 'tpts' : tpts, 'target_weight': target_weight}\n\n        return inp, target, meta\n\n    def __len__(self):\n        if self.is_train:\n            return len(self.train_list)\n        else:\n            return len(self.valid_list)\n\n\ndef mpii(**kwargs):\n    return Mpii(**kwargs)\n\nmpii.njoints = 16  # ugly but works\n"""
pose/datasets/mscoco.py,12,"b""from __future__ import print_function, absolute_import\n\nimport os\nimport numpy as np\nimport json\nimport random\nimport math\n\nimport torch\nimport torch.utils.data as data\n\nfrom pose.utils.osutils import *\nfrom pose.utils.imutils import *\nfrom pose.utils.transforms import *\n\n\nclass Mscoco(data.Dataset):\n    def __init__(self, is_train=True, **kwargs):\n        self.is_train   = is_train # training set or test set\n        self.inp_res    = kwargs['inp_res']\n        self.out_res    = kwargs['out_res']\n        self.sigma      = kwargs['sigma']\n        self.scale_factor = kwargs['scale_factor']\n        self.rot_factor = kwargs['rot_factor']\n        self.label_type = kwargs['label_type']\n        self.year       = kwargs['year']\n\n        if is_train:\n            self.img_folder = './data/mscoco/images/train{}'.format(self.year)    # root image folders\n        else:\n            self.img_folder = './data/mscoco/images/val{}'.format(self.year)    # root image folders\n\n        self.jsonfile   = './data/mscoco/coco_annotations_{}.json'.format(self.year)  # anno file\n\n        # create train/val split\n        with open(self.jsonfile) as anno_file:\n            self.anno = json.load(anno_file)\n\n        self.train, self.valid = [], []\n        for idx, val in enumerate(self.anno):\n            if val['isValidation'] == True:\n                self.valid.append(idx)\n            else:\n                self.train.append(idx)\n        self.mean, self.std = self._compute_mean()\n\n    def _compute_mean(self):\n        meanstd_file = './data/mscoco/mean.pth.tar'\n        if isfile(meanstd_file):\n            meanstd = torch.load(meanstd_file)\n        else:\n            print('==> compute mean')\n            mean = torch.zeros(3)\n            std = torch.zeros(3)\n            cnt = 0\n            for index in self.train:\n                cnt += 1\n                print( '{} | {}'.format(cnt, len(self.train)))\n                a = self.anno[index]\n                img_path = os.path.join(self.img_folder, a['img_paths'])\n                img = load_image(img_path) # CxHxW\n                mean += img.view(img.size(0), -1).mean(1)\n                std += img.view(img.size(0), -1).std(1)\n            mean /= len(self.train)\n            std /= len(self.train)\n            meanstd = {\n                'mean': mean,\n                'std': std,\n                }\n            torch.save(meanstd, meanstd_file)\n        if self.is_train:\n            print('    Mean: %.4f, %.4f, %.4f' % (meanstd['mean'][0], meanstd['mean'][1], meanstd['mean'][2]))\n            print('    Std:  %.4f, %.4f, %.4f' % (meanstd['std'][0], meanstd['std'][1], meanstd['std'][2]))\n\n        return meanstd['mean'], meanstd['std']\n\n    def __getitem__(self, index):\n        sf = self.scale_factor\n        rf = self.rot_factor\n        if self.is_train:\n            a = self.anno[self.train[index]]\n        else:\n            a = self.anno[self.valid[index]]\n\n        img_path = os.path.join(self.img_folder, a['img_paths'])\n        pts = torch.Tensor(a['joint_self'])\n        # pts[:, 0:2] -= 1  # Convert pts to zero based\n\n        # c = torch.Tensor(a['objpos']) - 1\n        c = torch.Tensor(a['objpos'])\n        s = a['scale_provided']\n\n        # Adjust center/scale slightly to avoid cropping limbs\n        if c[0] != -1:\n            c[1] = c[1] + 15 * s\n            s = s * 1.25\n\n        # For single-person pose estimation with a centered/scaled figure\n        nparts = pts.size(0)\n        img = load_image(img_path)  # CxHxW\n\n        r = 0\n        if self.is_train:\n            s = s*torch.randn(1).mul_(sf).add_(1).clamp(1-sf, 1+sf)[0]\n            r = torch.randn(1).mul_(rf).clamp(-2*rf, 2*rf)[0] if random.random() <= 0.6 else 0\n\n            # Flip\n            if random.random() <= 0.5:\n                img = torch.from_numpy(fliplr(img.numpy())).float()\n                pts = shufflelr(pts, width=img.size(2), dataset='mpii')\n                c[0] = img.size(2) - c[0]\n\n            # Color\n            img[0, :, :].mul_(random.uniform(0.8, 1.2)).clamp_(0, 1)\n            img[1, :, :].mul_(random.uniform(0.8, 1.2)).clamp_(0, 1)\n            img[2, :, :].mul_(random.uniform(0.8, 1.2)).clamp_(0, 1)\n\n        # Prepare image and groundtruth map\n        inp = crop(img, c, s, [self.inp_res, self.inp_res], rot=r)\n        inp = color_normalize(inp, self.mean, self.std)\n\n        # Generate ground truth\n        tpts = pts.clone()\n        target = torch.zeros(nparts, self.out_res, self.out_res)\n        for i in range(nparts):\n            if tpts[i, 2] > 0: # COCO visible: 0-no label, 1-label + invisible, 2-label + visible\n                tpts[i, 0:2] = to_torch(transform(tpts[i, 0:2]+1, c, s, [self.out_res, self.out_res], rot=r))\n                target[i] = draw_labelmap(target[i], tpts[i]-1, self.sigma, type=self.label_type)\n\n        # Meta info\n        meta = {'index' : index, 'center' : c, 'scale' : s,\n        'pts' : pts, 'tpts' : tpts}\n\n        return inp, target, meta\n\n    def __len__(self):\n        if self.is_train:\n            return len(self.train)\n        else:\n            return len(self.valid)\n\ndef mscoco(**kwargs):\n    return Mscoco(**kwargs)\n\nmscoco.njoints = 17  # ugly but works\n"""
pose/losses/__init__.py,0,b'from .jointsmseloss import JointsMSELoss\n'
pose/losses/jointsmseloss.py,1,"b""# ------------------------------------------------------------------------------\n# Copyright (c) Microsoft\n# Licensed under the MIT License.\n# Written by Bin Xiao (Bin.Xiao@microsoft.com)\n# Modified by Wei Yang (platero.yang@gmail.com)\n# ------------------------------------------------------------------------------\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport torch.nn as nn\n\n\nclass JointsMSELoss(nn.Module):\n    def __init__(self, use_target_weight=True):\n        super(JointsMSELoss, self).__init__()\n        self.criterion = nn.MSELoss(reduction='mean')\n        self.use_target_weight = use_target_weight\n\n    def forward(self, output, target, target_weight):\n        batch_size = output.size(0)\n        num_joints = output.size(1)\n        heatmaps_pred = output.reshape((batch_size, num_joints, -1)).split(1, 1)\n        heatmaps_gt = target.reshape((batch_size, num_joints, -1)).split(1, 1)\n        loss = 0\n\n        for idx in range(num_joints):\n            heatmap_pred = heatmaps_pred[idx].squeeze()\n            heatmap_gt = heatmaps_gt[idx].squeeze()\n            if self.use_target_weight:\n                loss += 0.5 * self.criterion(\n                    heatmap_pred.mul(target_weight[:, idx]),\n                    heatmap_gt.mul(target_weight[:, idx])\n                )\n            else:\n                loss += 0.5 * self.criterion(heatmap_pred, heatmap_gt)\n\n        return loss / num_joints\n"""
pose/models/__init__.py,0,b'from .hourglass import *\nfrom .hourglass_gn import *\nfrom .pose_resnet import *\n'
pose/models/hourglass.py,2,"b""'''\nHourglass network inserted in the pre-activated Resnet\nUse lr=0.01 for current version\n(c) YANG, Wei\n'''\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# from .preresnet import BasicBlock, Bottleneck\n\n\n__all__ = ['HourglassNet', 'hg']\n\nclass Bottleneck(nn.Module):\n    expansion = 2\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n\n        self.bn1 = nn.BatchNorm2d(inplanes)\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=True)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                               padding=1, bias=True)\n        self.bn3 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 2, kernel_size=1, bias=True)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.bn1(x)\n        out = self.relu(out)\n        out = self.conv1(out)\n\n        out = self.bn2(out)\n        out = self.relu(out)\n        out = self.conv2(out)\n\n        out = self.bn3(out)\n        out = self.relu(out)\n        out = self.conv3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n\n        return out\n\n\nclass Hourglass(nn.Module):\n    def __init__(self, block, num_blocks, planes, depth):\n        super(Hourglass, self).__init__()\n        self.depth = depth\n        self.block = block\n        self.hg = self._make_hour_glass(block, num_blocks, planes, depth)\n\n    def _make_residual(self, block, num_blocks, planes):\n        layers = []\n        for i in range(0, num_blocks):\n            layers.append(block(planes*block.expansion, planes))\n        return nn.Sequential(*layers)\n\n    def _make_hour_glass(self, block, num_blocks, planes, depth):\n        hg = []\n        for i in range(depth):\n            res = []\n            for j in range(3):\n                res.append(self._make_residual(block, num_blocks, planes))\n            if i == 0:\n                res.append(self._make_residual(block, num_blocks, planes))\n            hg.append(nn.ModuleList(res))\n        return nn.ModuleList(hg)\n\n    def _hour_glass_forward(self, n, x):\n        up1 = self.hg[n-1][0](x)\n        low1 = F.max_pool2d(x, 2, stride=2)\n        low1 = self.hg[n-1][1](low1)\n\n        if n > 1:\n            low2 = self._hour_glass_forward(n-1, low1)\n        else:\n            low2 = self.hg[n-1][3](low1)\n        low3 = self.hg[n-1][2](low2)\n        up2 = F.interpolate(low3, scale_factor=2)\n        out = up1 + up2\n        return out\n\n    def forward(self, x):\n        return self._hour_glass_forward(self.depth, x)\n\n\nclass HourglassNet(nn.Module):\n    '''Hourglass model from Newell et al ECCV 2016'''\n    def __init__(self, block, num_stacks=2, num_blocks=4, num_classes=16):\n        super(HourglassNet, self).__init__()\n\n        self.inplanes = 64\n        self.num_feats = 128\n        self.num_stacks = num_stacks\n        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\n                               bias=True)\n        self.bn1 = nn.BatchNorm2d(self.inplanes)\n        self.relu = nn.ReLU(inplace=True)\n        self.layer1 = self._make_residual(block, self.inplanes, 1)\n        self.layer2 = self._make_residual(block, self.inplanes, 1)\n        self.layer3 = self._make_residual(block, self.num_feats, 1)\n        self.maxpool = nn.MaxPool2d(2, stride=2)\n\n        # build hourglass modules\n        ch = self.num_feats*block.expansion\n        hg, res, fc, score, fc_, score_ = [], [], [], [], [], []\n        for i in range(num_stacks):\n            hg.append(Hourglass(block, num_blocks, self.num_feats, 4))\n            res.append(self._make_residual(block, self.num_feats, num_blocks))\n            fc.append(self._make_fc(ch, ch))\n            score.append(nn.Conv2d(ch, num_classes, kernel_size=1, bias=True))\n            if i < num_stacks-1:\n                fc_.append(nn.Conv2d(ch, ch, kernel_size=1, bias=True))\n                score_.append(nn.Conv2d(num_classes, ch, kernel_size=1, bias=True))\n        self.hg = nn.ModuleList(hg)\n        self.res = nn.ModuleList(res)\n        self.fc = nn.ModuleList(fc)\n        self.score = nn.ModuleList(score)\n        self.fc_ = nn.ModuleList(fc_)\n        self.score_ = nn.ModuleList(score_)\n\n    def _make_residual(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=True),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def _make_fc(self, inplanes, outplanes):\n        bn = nn.BatchNorm2d(inplanes)\n        conv = nn.Conv2d(inplanes, outplanes, kernel_size=1, bias=True)\n        return nn.Sequential(\n                conv,\n                bn,\n                self.relu,\n            )\n\n    def forward(self, x):\n        out = []\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n\n        x = self.layer1(x)\n        x = self.maxpool(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n\n        for i in range(self.num_stacks):\n            y = self.hg[i](x)\n            y = self.res[i](y)\n            y = self.fc[i](y)\n            score = self.score[i](y)\n            out.append(score)\n            if i < self.num_stacks-1:\n                fc_ = self.fc_[i](y)\n                score_ = self.score_[i](score)\n                x = x + fc_ + score_\n\n        return out\n\n\ndef hg(**kwargs):\n    model = HourglassNet(Bottleneck, num_stacks=kwargs['num_stacks'], num_blocks=kwargs['num_blocks'],\n                         num_classes=kwargs['num_classes'])\n    return model\n"""
pose/models/hourglass_gn.py,2,"b""'''\nHourglass network inserted in the pre-activated Resnet\nUse lr=0.01 for current version\n(c) YANG, Wei\n'''\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# from .preresnet import BasicBlock, Bottleneck\n\n\n__all__ = ['hg_gn']\n\n# hardcode group number\ngn = 32\n\nclass Bottleneck(nn.Module):\n    expansion = 2\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n\n        self.bn1 = nn.GroupNorm(gn, inplanes)\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=True)\n        self.bn2 = nn.GroupNorm(gn, planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                               padding=1, bias=True)\n        self.bn3 = nn.GroupNorm(gn, planes)\n        self.conv3 = nn.Conv2d(planes, planes * 2, kernel_size=1, bias=True)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.bn1(x)\n        out = self.relu(out)\n        out = self.conv1(out)\n\n        out = self.bn2(out)\n        out = self.relu(out)\n        out = self.conv2(out)\n\n        out = self.bn3(out)\n        out = self.relu(out)\n        out = self.conv3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n\n        return out\n\n\nclass Hourglass(nn.Module):\n    def __init__(self, block, num_blocks, planes, depth):\n        super(Hourglass, self).__init__()\n        self.depth = depth\n        self.block = block\n        self.hg = self._make_hour_glass(block, num_blocks, planes, depth)\n\n    def _make_residual(self, block, num_blocks, planes):\n        layers = []\n        for i in range(0, num_blocks):\n            layers.append(block(planes*block.expansion, planes))\n        return nn.Sequential(*layers)\n\n    def _make_hour_glass(self, block, num_blocks, planes, depth):\n        hg = []\n        for i in range(depth):\n            res = []\n            for j in range(3):\n                res.append(self._make_residual(block, num_blocks, planes))\n            if i == 0:\n                res.append(self._make_residual(block, num_blocks, planes))\n            hg.append(nn.ModuleList(res))\n        return nn.ModuleList(hg)\n\n    def _hour_glass_forward(self, n, x):\n        up1 = self.hg[n-1][0](x)\n        low1 = F.max_pool2d(x, 2, stride=2)\n        low1 = self.hg[n-1][1](low1)\n\n        if n > 1:\n            low2 = self._hour_glass_forward(n-1, low1)\n        else:\n            low2 = self.hg[n-1][3](low1)\n        low3 = self.hg[n-1][2](low2)\n        up2 = F.interpolate(low3, scale_factor=2)\n        out = up1 + up2\n        return out\n\n    def forward(self, x):\n        return self._hour_glass_forward(self.depth, x)\n\n\nclass HourglassNet(nn.Module):\n    '''Hourglass model from Newell et al ECCV 2016'''\n    def __init__(self, block, num_stacks=2, num_blocks=4, num_classes=16):\n        super(HourglassNet, self).__init__()\n\n        self.inplanes = 64\n        self.num_feats = 128\n        self.num_stacks = num_stacks\n        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\n                               bias=True)\n        self.bn1 = nn.GroupNorm(gn, self.inplanes)\n        self.relu = nn.ReLU(inplace=True)\n        self.layer1 = self._make_residual(block, self.inplanes, 1)\n        self.layer2 = self._make_residual(block, self.inplanes, 1)\n        self.layer3 = self._make_residual(block, self.num_feats, 1)\n        self.maxpool = nn.MaxPool2d(2, stride=2)\n\n        # build hourglass modules\n        ch = self.num_feats*block.expansion\n        hg, res, fc, score, fc_, score_ = [], [], [], [], [], []\n        for i in range(num_stacks):\n            hg.append(Hourglass(block, num_blocks, self.num_feats, 4))\n            res.append(self._make_residual(block, self.num_feats, num_blocks))\n            fc.append(self._make_fc(ch, ch))\n            score.append(nn.Conv2d(ch, num_classes, kernel_size=1, bias=True))\n            if i < num_stacks-1:\n                fc_.append(nn.Conv2d(ch, ch, kernel_size=1, bias=True))\n                score_.append(nn.Conv2d(num_classes, ch, kernel_size=1, bias=True))\n        self.hg = nn.ModuleList(hg)\n        self.res = nn.ModuleList(res)\n        self.fc = nn.ModuleList(fc)\n        self.score = nn.ModuleList(score)\n        self.fc_ = nn.ModuleList(fc_)\n        self.score_ = nn.ModuleList(score_)\n\n    def _make_residual(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=True),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def _make_fc(self, inplanes, outplanes):\n        bn = nn.GroupNorm(gn, inplanes)\n        conv = nn.Conv2d(inplanes, outplanes, kernel_size=1, bias=True)\n        return nn.Sequential(\n                conv,\n                bn,\n                self.relu,\n            )\n\n    def forward(self, x):\n        out = []\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n\n        x = self.layer1(x)\n        x = self.maxpool(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n\n        for i in range(self.num_stacks):\n            y = self.hg[i](x)\n            y = self.res[i](y)\n            y = self.fc[i](y)\n            score = self.score[i](y)\n            out.append(score)\n            if i < self.num_stacks-1:\n                fc_ = self.fc_[i](y)\n                score_ = self.score_[i](score)\n                x = x + fc_ + score_\n\n        return out\n\n\ndef hg_gn(**kwargs):\n    model = HourglassNet(Bottleneck, num_stacks=kwargs['num_stacks'], num_blocks=kwargs['num_blocks'],\n                         num_classes=kwargs['num_classes'])\n    return model\n"""
pose/models/pose_resnet.py,1,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom easydict import EasyDict as edict\n\nimport torch\nimport torch.nn as nn\nfrom torchvision.models.resnet import model_zoo\nfrom torchvision.models.resnet import model_urls\nfrom torchvision.models.resnet import BasicBlock, Bottleneck\n\n# Specification\nresnet_spec = {\n    18: (BasicBlock, [2, 2, 2, 2], [64, 64, 128, 256, 512], \'resnet18\'),\n    34: (BasicBlock, [3, 4, 6, 3], [64, 64, 128, 256, 512], \'resnet34\'),\n    50: (Bottleneck, [3, 4, 6, 3], [64, 256, 512, 1024, 2048], \'resnet50\'),\n    101: (Bottleneck, [3, 4, 23, 3], [64, 256, 512, 1024, 2048], \'resnet101\'),\n    152: (Bottleneck, [3, 8, 36, 3], [64, 256, 512, 1024, 2048], \'resnet152\')\n}\n\nclass ResNetBackbone(nn.Module):\n\n    def __init__(self, block, layers, in_channel=3):\n        self.inplanes = 64\n        super(ResNetBackbone, self).__init__()\n        self.conv1 = nn.Conv2d(in_channel, 64, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                # nn.init.kaiming_normal_(m.weight, mode=\'fan_out\', nonlinearity=\'relu\')\n                nn.init.normal_(m.weight, mean=0, std=0.001)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        return x\n\n\nclass DeconvHead(nn.Module):\n    def __init__(self, in_channels, num_layers, num_filters, kernel_size, conv_kernel_size, num_joints, depth_dim,\n                 with_bias_end=True):\n        super(DeconvHead, self).__init__()\n\n        conv_num_filters = num_joints * depth_dim\n\n        assert kernel_size == 2 or kernel_size == 3 or kernel_size == 4, \'Only support kenerl 2, 3 and 4\'\n        padding = 1\n        output_padding = 0\n        if kernel_size == 3:\n            output_padding = 1\n        elif kernel_size == 2:\n            padding = 0\n\n        assert conv_kernel_size == 1 or conv_kernel_size == 3, \'Only support kenerl 1 and 3\'\n        if conv_kernel_size == 1:\n            pad = 0\n        elif conv_kernel_size == 3:\n            pad = 1\n\n        self.features = nn.ModuleList()\n        for i in range(num_layers):\n            _in_channels = in_channels if i == 0 else num_filters\n            self.features.append(\n                nn.ConvTranspose2d(_in_channels, num_filters, kernel_size=kernel_size, stride=2, padding=padding,\n                                   output_padding=output_padding, bias=False))\n            self.features.append(nn.BatchNorm2d(num_filters))\n            self.features.append(nn.ReLU(inplace=True))\n\n        if with_bias_end:\n            self.features.append(\n                nn.Conv2d(num_filters, conv_num_filters, kernel_size=conv_kernel_size, padding=pad, bias=True))\n        else:\n            self.features.append(\n                nn.Conv2d(num_filters, conv_num_filters, kernel_size=conv_kernel_size, padding=pad, bias=False))\n            self.features.append(nn.BatchNorm2d(conv_num_filters))\n            self.features.append(nn.ReLU(inplace=True))\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.normal_(m.weight, mean=0, std=0.001)\n                if with_bias_end:\n                    nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.ConvTranspose2d):\n                nn.init.normal_(m.weight, mean=0, std=0.001)\n\n    def forward(self, x):\n        for i, l in enumerate(self.features):\n            x = l(x)\n        return x\n\n\nclass ResPoseNet(nn.Module):\n    def __init__(self, backbone, head):\n        super(ResPoseNet, self).__init__()\n        self.backbone = backbone\n        self.head = head\n\n    def forward(self, x):\n        x = self.backbone(x)\n        x = self.head(x)\n        return x\n\n\n# Helper functions\ndef get_default_network_config():\n    config = edict()\n    config.from_model_zoo = True\n    config.pretrained = \'\'\n    config.num_layers = 50\n    config.num_deconv_layers = 3\n    config.num_deconv_filters = 256\n    config.num_deconv_kernel = 4\n    config.final_conv_kernel = 1\n    config.depth_dim = 1\n    config.input_channel = 3\n    return config\n\ndef init_pose_net(pose_net, name):\n    org_resnet = model_zoo.load_url(model_urls[name])\n    # drop orginal resnet fc layer, add \'None\' in case of no fc layer, that will raise error\n    org_resnet.pop(\'fc.weight\', None)\n    org_resnet.pop(\'fc.bias\', None)\n    pose_net.backbone.load_state_dict(org_resnet)\n    print(""Init Network from model zoo"")\n\n# create network\ndef pose_resnet(**kwargs):\n\n    cfg = get_default_network_config()\n\n    block_type, layers, channels, name = resnet_spec[kwargs[\'resnet_layers\']]\n    backbone_net = ResNetBackbone(block_type, layers)\n    head_net = DeconvHead(\n        channels[-1], cfg.num_deconv_layers,\n        cfg.num_deconv_filters, cfg.num_deconv_kernel,\n        cfg.final_conv_kernel, kwargs[\'num_classes\'], cfg.depth_dim\n    )\n    pose_net = ResPoseNet(backbone_net, head_net)\n    init_pose_net(pose_net, name)\n    return pose_net\n'"
pose/models/preresnet.py,2,"b'\'\'\'Pre-activated Resnet for cifar dataset. \nPorted form https://github.com/facebook/fb.resnet.torch/blob/master/models/preresnet.lua\n(c) YANG, Wei \n\'\'\'\nimport torch.nn as nn\nimport math\nimport torch.utils.model_zoo as model_zoo\n\n\n__all__ = [\'PreResNet\', \'preresnet20\', \'preresnet32\', \'preresnet44\', \'preresnet56\',\n           \'preresnet110\', \'preresnet1202\']\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    ""3x3 convolution with padding""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.bn1 = nn.BatchNorm2d(inplanes)\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.relu = nn.ReLU(inplace=True)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv2 = conv3x3(planes, planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.bn1(x)\n        out = self.relu(out)\n        out = self.conv1(out)\n\n        out = self.bn2(out)\n        out = self.relu(out)\n        out = self.conv2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.bn1 = nn.BatchNorm2d(inplanes)\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                               padding=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.bn1(x)\n        out = self.relu(out)\n        out = self.conv1(out)\n\n        out = self.bn2(out)\n        out = self.relu(out)\n        out = self.conv2(out)\n\n        out = self.bn3(out)\n        out = self.relu(out)\n        out = self.conv3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n\n        return out\n\n\nclass PreResNet(nn.Module):\n\n    def __init__(self, block, layers, num_classes=1000):\n        self.inplanes = 16\n        super(PreResNet, self).__init__()\n        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1,\n                               bias=False)\n        self.layer1 = self._make_layer(block, 16, layers[0])\n        self.layer2 = self._make_layer(block, 32, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 64, layers[2], stride=2)\n        self.bn1 = nn.BatchNorm2d(64*block.expansion)\n        self.relu = nn.ReLU(inplace=True)\n        self.fc1 = nn.Conv2d(64*block.expansion, 64*block.expansion, kernel_size=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(64*block.expansion)\n        self.fc2 = nn.Conv2d(64*block.expansion, num_classes, kernel_size=1)\n        # self.avgpool = nn.AvgPool2d(8)\n        # self.fc = nn.Linear(64*block.expansion, num_classes)\n\n        # for m in self.modules():\n        #     if isinstance(m, nn.Conv2d):\n        #         n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n        #         m.weight.data.normal_(0, math.sqrt(2. / n))\n        #     elif isinstance(m, nn.BatchNorm2d):\n        #         m.weight.data.fill_(1)\n        #         m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                # nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.fc1(self.relu(self.bn1(x)))\n        x = self.fc2(self.relu(self.bn2(x)))\n        # x = self.sigmoid(x)\n        # x = self.avgpool(x)\n        # x = x.view(x.size(0), -1)\n\n        return [x]\n\n\ndef preresnet20(**kwargs):\n    """"""Constructs a PreResNet-20 model.\n    """"""\n    model = PreResNet(BasicBlock, [3, 3, 3], **kwargs)\n    return model\n\n\ndef preresnet32(**kwargs):\n    """"""Constructs a PreResNet-32 model.\n    """"""\n    model = PreResNet(BasicBlock, [5, 5, 5], **kwargs)\n    return model\n\n\ndef preresnet44(**kwargs):\n    """"""Constructs a PreResNet-44 model.\n    """"""\n    model = PreResNet(Bottleneck, [7, 7, 7], **kwargs)\n    return model\n\n\ndef preresnet56(**kwargs):\n    """"""Constructs a PreResNet-56 model.\n    """"""\n    model = PreResNet(Bottleneck, [9, 9, 9], **kwargs)\n    return model\n\n\ndef preresnet110(**kwargs):\n    """"""Constructs a PreResNet-110 model.\n    """"""\n    model = PreResNet(Bottleneck, [18, 18, 18], **kwargs)\n    return model\n\ndef preresnet1202(**kwargs):\n    """"""Constructs a PreResNet-1202 model.\n    """"""\n    model = PreResNet(Bottleneck, [200, 200, 200], **kwargs)\n    return model'"
pose/utils/__init__.py,0,b'from __future__ import absolute_import\n\nfrom .evaluation import *\nfrom .imutils import *\nfrom .logger import *\nfrom .misc import *\nfrom .osutils import *\nfrom .transforms import *\n'
pose/utils/evaluation.py,8,"b'from __future__ import absolute_import\n\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom random import randint\n\nfrom .misc import *\nfrom .transforms import transform, transform_preds\n\n__all__ = [\'accuracy\', \'AverageMeter\']\n\ndef get_preds(scores):\n    \'\'\' get predictions from score maps in torch Tensor\n        return type: torch.LongTensor\n    \'\'\'\n    assert scores.dim() == 4, \'Score maps should be 4-dim\'\n    maxval, idx = torch.max(scores.view(scores.size(0), scores.size(1), -1), 2)\n\n    maxval = maxval.view(scores.size(0), scores.size(1), 1)\n    idx = idx.view(scores.size(0), scores.size(1), 1) + 1\n\n    preds = idx.repeat(1, 1, 2).float()\n\n    preds[:,:,0] = (preds[:,:,0] - 1) % scores.size(3) + 1\n    preds[:,:,1] = torch.floor((preds[:,:,1] - 1) / scores.size(3)) + 1\n\n    pred_mask = maxval.gt(0).repeat(1, 1, 2).float()\n    preds *= pred_mask\n    return preds\n\ndef calc_dists(preds, target, normalize):\n    preds = preds.float()\n    target = target.float()\n    dists = torch.zeros(preds.size(1), preds.size(0))\n    for n in range(preds.size(0)):\n        for c in range(preds.size(1)):\n            if target[n,c,0] > 1 and target[n, c, 1] > 1:\n                dists[c, n] = torch.dist(preds[n,c,:], target[n,c,:])/normalize[n]\n            else:\n                dists[c, n] = -1\n    return dists\n\ndef dist_acc(dist, thr=0.5):\n    \'\'\' Return percentage below threshold while ignoring values with a -1 \'\'\'\n    dist = dist[dist != -1]\n    if len(dist) > 0:\n        return 1.0 * (dist < thr).sum().item() / len(dist)\n    else:\n        return -1\n\ndef accuracy(output, target, idxs, thr=0.5):\n    \'\'\' Calculate accuracy according to PCK, but uses ground truth heatmap rather than x,y locations\n        First value to be returned is average accuracy across \'idxs\', followed by individual accuracies\n    \'\'\'\n    preds   = get_preds(output)\n    gts     = get_preds(target)\n    norm    = torch.ones(preds.size(0))*output.size(3)/10\n    dists   = calc_dists(preds, gts, norm)\n\n    acc = torch.zeros(len(idxs)+1)\n    avg_acc = 0\n    cnt = 0\n\n    for i in range(len(idxs)):\n        acc[i+1] = dist_acc(dists[idxs[i]-1])\n        if acc[i+1] >= 0:\n            avg_acc = avg_acc + acc[i+1]\n            cnt += 1\n\n    if cnt != 0:\n        acc[0] = avg_acc / cnt\n    return acc\n\ndef final_preds(output, center, scale, res):\n    coords = get_preds(output) # float type\n\n    # pose-processing\n    for n in range(coords.size(0)):\n        for p in range(coords.size(1)):\n            hm = output[n][p]\n            px = int(math.floor(coords[n][p][0]))\n            py = int(math.floor(coords[n][p][1]))\n            if px > 1 and px < res[0] and py > 1 and py < res[1]:\n                diff = torch.Tensor([hm[py - 1][px] - hm[py - 1][px - 2], hm[py][px - 1]-hm[py - 2][px - 1]])\n                coords[n][p] += diff.sign() * .25\n    coords += 0.5\n    preds = coords.clone()\n\n    # Transform back\n    for i in range(coords.size(0)):\n        preds[i] = transform_preds(coords[i], center[i], scale[i], res)\n\n    if preds.dim() < 3:\n        preds = preds.view(1, preds.size())\n\n    return preds\n\n\nclass AverageMeter(object):\n    """"""Computes and stores the average and current value""""""\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n'"
pose/utils/imutils.py,3,"b'from __future__ import absolute_import\n\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport scipy.misc\n\nfrom .misc import *\n\ndef im_to_numpy(img):\n    img = to_numpy(img)\n    img = np.transpose(img, (1, 2, 0)) # H*W*C\n    return img\n\ndef im_to_torch(img):\n    img = np.transpose(img, (2, 0, 1)) # C*H*W\n    img = to_torch(img).float()\n    if img.max() > 1:\n        img /= 255\n    return img\n\ndef load_image(img_path):\n    # H x W x C => C x H x W\n    return im_to_torch(scipy.misc.imread(img_path, mode=\'RGB\'))\n\ndef resize(img, owidth, oheight):\n    img = im_to_numpy(img)\n    print(\'%f %f\' % (img.min(), img.max()))\n    img = scipy.misc.imresize(\n            img,\n            (oheight, owidth)\n        )\n    img = im_to_torch(img)\n    print(\'%f %f\' % (img.min(), img.max()))\n    return img\n\n# =============================================================================\n# Helpful functions generating groundtruth labelmap\n# =============================================================================\n\ndef gaussian(shape=(7,7),sigma=1):\n    """"""\n    2D gaussian mask - should give the same result as MATLAB\'s\n    fspecial(\'gaussian\',[shape],[sigma])\n    """"""\n    m,n = [(ss-1.)/2. for ss in shape]\n    y,x = np.ogrid[-m:m+1,-n:n+1]\n    h = np.exp( -(x*x + y*y) / (2.*sigma*sigma) )\n    h[ h < np.finfo(h.dtype).eps*h.max() ] = 0\n    return to_torch(h).float()\n\ndef draw_labelmap(img, pt, sigma, type=\'Gaussian\'):\n    # Draw a 2D gaussian\n    # Adopted from https://github.com/anewell/pose-hg-train/blob/master/src/pypose/draw.py\n    img = to_numpy(img)\n\n    # Check that any part of the gaussian is in-bounds\n    ul = [int(pt[0] - 3 * sigma), int(pt[1] - 3 * sigma)]\n    br = [int(pt[0] + 3 * sigma + 1), int(pt[1] + 3 * sigma + 1)]\n    if (ul[0] >= img.shape[1] or ul[1] >= img.shape[0] or\n            br[0] < 0 or br[1] < 0):\n        # If not, just return the image as is\n        return to_torch(img), 0\n\n    # Generate gaussian\n    size = 6 * sigma + 1\n    x = np.arange(0, size, 1, float)\n    y = x[:, np.newaxis]\n    x0 = y0 = size // 2\n    # The gaussian is not normalized, we want the center value to equal 1\n    if type == \'Gaussian\':\n        g = np.exp(- ((x - x0) ** 2 + (y - y0) ** 2) / (2 * sigma ** 2))\n    elif type == \'Cauchy\':\n        g = sigma / (((x - x0) ** 2 + (y - y0) ** 2 + sigma ** 2) ** 1.5)\n\n\n    # Usable gaussian range\n    g_x = max(0, -ul[0]), min(br[0], img.shape[1]) - ul[0]\n    g_y = max(0, -ul[1]), min(br[1], img.shape[0]) - ul[1]\n    # Image range\n    img_x = max(0, ul[0]), min(br[0], img.shape[1])\n    img_y = max(0, ul[1]), min(br[1], img.shape[0])\n\n    img[img_y[0]:img_y[1], img_x[0]:img_x[1]] = g[g_y[0]:g_y[1], g_x[0]:g_x[1]]\n    return to_torch(img), 1\n\n# =============================================================================\n# Helpful display functions\n# =============================================================================\n\ndef gauss(x, a, b, c, d=0):\n    return a * np.exp(-(x - b)**2 / (2 * c**2)) + d\n\ndef color_heatmap(x):\n    x = to_numpy(x)\n    color = np.zeros((x.shape[0],x.shape[1],3))\n    color[:,:,0] = gauss(x, .5, .6, .2) + gauss(x, 1, .8, .3)\n    color[:,:,1] = gauss(x, 1, .5, .3)\n    color[:,:,2] = gauss(x, 1, .2, .3)\n    color[color > 1] = 1\n    color = (color * 255).astype(np.uint8)\n    return color\n\ndef imshow(img):\n    npimg = im_to_numpy(img*255).astype(np.uint8)\n    plt.imshow(npimg)\n    plt.axis(\'off\')\n\ndef show_joints(img, pts):\n    imshow(img)\n\n    for i in range(pts.size(0)):\n        if pts[i, 2] > 0:\n            plt.plot(pts[i, 0], pts[i, 1], \'yo\')\n    plt.axis(\'off\')\n\ndef show_sample(inputs, target):\n    num_sample = inputs.size(0)\n    num_joints = target.size(1)\n    height = target.size(2)\n    width = target.size(3)\n\n    for n in range(num_sample):\n        inp = resize(inputs[n], width, height)\n        out = inp\n        for p in range(num_joints):\n            tgt = inp*0.5 + color_heatmap(target[n,p,:,:])*0.5\n            out = torch.cat((out, tgt), 2)\n\n        imshow(out)\n        plt.show()\n\ndef sample_with_heatmap(inp, out, num_rows=2, parts_to_show=None):\n    inp = to_numpy(inp * 255)\n    out = to_numpy(out)\n\n    img = np.zeros((inp.shape[1], inp.shape[2], inp.shape[0]))\n    for i in range(3):\n        img[:, :, i] = inp[i, :, :]\n\n    if parts_to_show is None:\n        parts_to_show = np.arange(out.shape[0])\n\n    # Generate a single image to display input/output pair\n    num_cols = int(np.ceil(float(len(parts_to_show)) / num_rows))\n    size = img.shape[0] // num_rows\n\n    full_img = np.zeros((img.shape[0], size * (num_cols + num_rows), 3), np.uint8)\n    full_img[:img.shape[0], :img.shape[1]] = img\n\n    inp_small = scipy.misc.imresize(img, [size, size])\n\n    # Set up heatmap display for each part\n    for i, part in enumerate(parts_to_show):\n        part_idx = part\n        out_resized = scipy.misc.imresize(out[part_idx], [size, size])\n        out_resized = out_resized.astype(float)/255\n        out_img = inp_small.copy() * .3\n        color_hm = color_heatmap(out_resized)\n        out_img += color_hm * .7\n\n        col_offset = (i % num_cols + num_rows) * size\n        row_offset = (i // num_cols) * size\n        full_img[row_offset:row_offset + size, col_offset:col_offset + size] = out_img\n\n    return full_img\n\ndef batch_with_heatmap(inputs, outputs, mean=torch.Tensor([0.5, 0.5, 0.5]).cuda(), num_rows=2, parts_to_show=None):\n    batch_img = []\n    for n in range(min(inputs.size(0), 4)):\n        inp = inputs[n] + mean.view(3, 1, 1).expand_as(inputs[n])\n        batch_img.append(\n            sample_with_heatmap(inp.clamp(0, 1), outputs[n], num_rows=num_rows, parts_to_show=parts_to_show)\n        )\n    return np.concatenate(batch_img)\n'"
pose/utils/logger.py,0,"b'# A simple torch style logger\n# (C) Wei YANG 2017\nfrom __future__ import absolute_import\n\nimport os\nimport sys\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n__all__ = [\'Logger\', \'LoggerMonitor\', \'savefig\']\n\ndef savefig(fname, dpi=None):\n    dpi = 150 if dpi == None else dpi\n    plt.savefig(fname, dpi=dpi)\n    \ndef plot_overlap(logger, names=None):\n    names = logger.names if names == None else names\n    numbers = logger.numbers\n    for _, name in enumerate(names):\n        x = np.arange(len(numbers[name]))\n        plt.plot(x, np.asarray(numbers[name]))\n    return [logger.title + \'(\' + name + \')\' for name in names]\n\nclass Logger(object):\n    \'\'\'Save training process to log file with simple plot function.\'\'\'\n    def __init__(self, fpath, title=None, resume=False): \n        self.file = None\n        self.resume = resume\n        self.title = \'\' if title == None else title\n        if fpath is not None:\n            if resume: \n                self.file = open(fpath, \'r\') \n                name = self.file.readline()\n                self.names = name.rstrip().split(\'\\t\')\n                self.numbers = {}\n                for _, name in enumerate(self.names):\n                    self.numbers[name] = []\n\n                for numbers in self.file:\n                    numbers = numbers.rstrip().split(\'\\t\')\n                    for i in range(0, len(numbers)):\n                        self.numbers[self.names[i]].append(numbers[i])\n                self.file.close()\n                self.file = open(fpath, \'a\')  \n            else:\n                self.file = open(fpath, \'w\')\n\n    def set_names(self, names):\n        if self.resume: \n            pass\n        # initialize numbers as empty list\n        self.numbers = {}\n        self.names = names\n        for _, name in enumerate(self.names):\n            self.file.write(name)\n            self.file.write(\'\\t\')\n            self.numbers[name] = []\n        self.file.write(\'\\n\')\n        self.file.flush()\n\n\n    def append(self, numbers):\n        assert len(self.names) == len(numbers), \'Numbers do not match names\'\n        for index, num in enumerate(numbers):\n            self.file.write(""{0:.6f}"".format(num))\n            self.file.write(\'\\t\')\n            self.numbers[self.names[index]].append(num)\n        self.file.write(\'\\n\')\n        self.file.flush()\n\n    def plot(self, names=None):   \n        names = self.names if names == None else names\n        numbers = self.numbers\n        for _, name in enumerate(names):\n            x = np.arange(len(numbers[name]))\n            plt.plot(x, np.asarray(numbers[name]))\n        plt.legend([self.title + \'(\' + name + \')\' for name in names])\n        plt.grid(True)\n\n    def close(self):\n        if self.file is not None:\n            self.file.close()\n\nclass LoggerMonitor(object):\n    \'\'\'Load and visualize multiple logs.\'\'\'\n    def __init__ (self, paths):\n        \'\'\'paths is a distionary with {name:filepath} pair\'\'\'\n        self.loggers = []\n        for title, path in paths.items():\n            logger = Logger(path, title=title, resume=True)\n            self.loggers.append(logger)\n\n    def plot(self, names=None):\n        plt.figure()\n        plt.subplot(121)\n        legend_text = []\n        for logger in self.loggers:\n            legend_text += plot_overlap(logger, names)\n        plt.legend(legend_text, bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n        plt.grid(True)\n                    \nif __name__ == \'__main__\':\n    # # Example\n    # logger = Logger(\'test.txt\')\n    # logger.set_names([\'Train loss\', \'Valid loss\',\'Test loss\'])\n\n    # length = 100\n    # t = np.arange(length)\n    # train_loss = np.exp(-t / 10.0) + np.random.rand(length) * 0.1\n    # valid_loss = np.exp(-t / 10.0) + np.random.rand(length) * 0.1\n    # test_loss = np.exp(-t / 10.0) + np.random.rand(length) * 0.1\n\n    # for i in range(0, length):\n    #     logger.append([train_loss[i], valid_loss[i], test_loss[i]])\n    # logger.plot()\n\n    # Example: logger monitor\n    paths = {\n    \'resadvnet20\':\'/home/wyang/code/pytorch-classification/checkpoint/cifar10/resadvnet20/log.txt\', \n    \'resadvnet32\':\'/home/wyang/code/pytorch-classification/checkpoint/cifar10/resadvnet32/log.txt\',\n    \'resadvnet44\':\'/home/wyang/code/pytorch-classification/checkpoint/cifar10/resadvnet44/log.txt\',\n    }\n\n    field = [\'Valid Acc.\']\n\n    monitor = LoggerMonitor(paths)\n    monitor.plot(names=field)\n    savefig(\'test.eps\')'"
pose/utils/misc.py,4,"b'from __future__ import absolute_import\n\nimport os\nimport shutil\nimport torch\nimport math\nimport numpy as np\nimport scipy.io\nimport matplotlib.pyplot as plt\n\ndef to_numpy(tensor):\n    if torch.is_tensor(tensor):\n        return tensor.detach().cpu().numpy()\n    elif type(tensor).__module__ != \'numpy\':\n        raise ValueError(""Cannot convert {} to numpy array""\n                         .format(type(tensor)))\n    return tensor\n\n\ndef to_torch(ndarray):\n    if type(ndarray).__module__ == \'numpy\':\n        return torch.from_numpy(ndarray)\n    elif not torch.is_tensor(ndarray):\n        raise ValueError(""Cannot convert {} to torch tensor""\n                         .format(type(ndarray)))\n    return ndarray\n\n\ndef save_checkpoint(state, preds, is_best, checkpoint=\'checkpoint\', filename=\'checkpoint.pth.tar\', snapshot=None):\n    preds = to_numpy(preds)\n    filepath = os.path.join(checkpoint, filename)\n    torch.save(state, filepath)\n    scipy.io.savemat(os.path.join(checkpoint, \'preds.mat\'), mdict={\'preds\' : preds})\n\n    if snapshot and state.epoch % snapshot == 0:\n        shutil.copyfile(filepath, os.path.join(checkpoint, \'checkpoint_{}.pth.tar\'.format(state.epoch)))\n\n    if is_best:\n        shutil.copyfile(filepath, os.path.join(checkpoint, \'model_best.pth.tar\'))\n        scipy.io.savemat(os.path.join(checkpoint, \'preds_best.mat\'), mdict={\'preds\' : preds})\n\n\ndef save_pred(preds, checkpoint=\'checkpoint\', filename=\'preds_valid.mat\'):\n    preds = to_numpy(preds)\n    filepath = os.path.join(checkpoint, filename)\n    scipy.io.savemat(filepath, mdict={\'preds\' : preds})\n\n\ndef adjust_learning_rate(optimizer, epoch, lr, schedule, gamma):\n    """"""Sets the learning rate to the initial LR decayed by schedule""""""\n    if epoch in schedule:\n        lr *= gamma\n        for param_group in optimizer.param_groups:\n            param_group[\'lr\'] = lr\n    return lr\n'"
pose/utils/osutils.py,0,"b'from __future__ import absolute_import\n\nimport os\nimport errno\n\ndef mkdir_p(dir_path):\n    try:\n        os.makedirs(dir_path)\n    except OSError as e:\n        if e.errno != errno.EEXIST:\n            raise\n\ndef isfile(fname):\n    return os.path.isfile(fname) \n\ndef isdir(dirname):\n    return os.path.isdir(dirname)\n\ndef join(path, *paths):\n    return os.path.join(path, *paths)\n'"
pose/utils/transforms.py,3,"b'from __future__ import absolute_import\n\nimport os\nimport numpy as np\nimport scipy.misc\nimport matplotlib.pyplot as plt\nimport torch\n\nfrom .misc import *\nfrom .imutils import *\n\n\ndef color_normalize(x, mean, std):\n    if x.size(0) == 1:\n        x = x.repeat(3, 1, 1)\n\n    for t, m, s in zip(x, mean, std):\n        t.sub_(m)\n    return x\n\n\ndef flip_back(flip_output, dataset=\'mpii\'):\n    """"""\n    flip output map\n    """"""\n    if dataset ==  \'mpii\':\n        matchedParts = (\n            [0,5],   [1,4],   [2,3],\n            [10,15], [11,14], [12,13]\n        )\n    else:\n        print(\'Not supported dataset: \' + dataset)\n\n    # flip output horizontally\n    flip_output = fliplr(flip_output.numpy())\n\n    # Change left-right parts\n    for pair in matchedParts:\n        tmp = np.copy(flip_output[:, pair[0], :, :])\n        flip_output[:, pair[0], :, :] = flip_output[:, pair[1], :, :]\n        flip_output[:, pair[1], :, :] = tmp\n\n    return torch.from_numpy(flip_output).float()\n\n\ndef shufflelr(x, width, dataset=\'mpii\'):\n    """"""\n    flip coords\n    """"""\n    if dataset ==  \'mpii\':\n        matchedParts = (\n            [0,5],   [1,4],   [2,3],\n            [10,15], [11,14], [12,13]\n        )\n    else:\n        print(\'Not supported dataset: \' + dataset)\n\n    # Flip horizontal\n    x[:, 0] = width - x[:, 0]\n\n    # Change left-right parts\n    for pair in matchedParts:\n        tmp = x[pair[0], :].clone()\n        x[pair[0], :] = x[pair[1], :]\n        x[pair[1], :] = tmp\n\n    return x\n\n\ndef fliplr(x):\n    if x.ndim == 3:\n        x = np.transpose(np.fliplr(np.transpose(x, (0, 2, 1))), (0, 2, 1))\n    elif x.ndim == 4:\n        for i in range(x.shape[0]):\n            x[i] = np.transpose(np.fliplr(np.transpose(x[i], (0, 2, 1))), (0, 2, 1))\n    return x.astype(float)\n\n\ndef get_transform(center, scale, res, rot=0):\n    """"""\n    General image processing functions\n    """"""\n    # Generate transformation matrix\n    h = 200 * scale\n    t = np.zeros((3, 3))\n    t[0, 0] = float(res[1]) / h\n    t[1, 1] = float(res[0]) / h\n    t[0, 2] = res[1] * (-float(center[0]) / h + .5)\n    t[1, 2] = res[0] * (-float(center[1]) / h + .5)\n    t[2, 2] = 1\n    if not rot == 0:\n        rot = -rot # To match direction of rotation from cropping\n        rot_mat = np.zeros((3,3))\n        rot_rad = rot * np.pi / 180\n        sn,cs = np.sin(rot_rad), np.cos(rot_rad)\n        rot_mat[0,:2] = [cs, -sn]\n        rot_mat[1,:2] = [sn, cs]\n        rot_mat[2,2] = 1\n        # Need to rotate around center\n        t_mat = np.eye(3)\n        t_mat[0,2] = -res[1]/2\n        t_mat[1,2] = -res[0]/2\n        t_inv = t_mat.copy()\n        t_inv[:2,2] *= -1\n        t = np.dot(t_inv,np.dot(rot_mat,np.dot(t_mat,t)))\n    return t\n\n\ndef transform(pt, center, scale, res, invert=0, rot=0):\n    # Transform pixel location to different reference\n    t = get_transform(center, scale, res, rot=rot)\n    if invert:\n        t = np.linalg.inv(t)\n    new_pt = np.array([pt[0] - 1, pt[1] - 1, 1.]).T\n    new_pt = np.dot(t, new_pt)\n    return new_pt[:2].astype(int) + 1\n\n\ndef transform_preds(coords, center, scale, res):\n    # size = coords.size()\n    # coords = coords.view(-1, coords.size(-1))\n    # print(coords.size())\n    for p in range(coords.size(0)):\n        coords[p, 0:2] = to_torch(transform(coords[p, 0:2], center, scale, res, 1, 0))\n    return coords\n\n\ndef crop(img, center, scale, res, rot=0):\n    img = im_to_numpy(img)\n\n    # Preprocessing for efficient cropping\n    ht, wd = img.shape[0], img.shape[1]\n    sf = scale * 200.0 / res[0]\n    if sf < 2:\n        sf = 1\n    else:\n        new_size = int(np.math.floor(max(ht, wd) / sf))\n        new_ht = int(np.math.floor(ht / sf))\n        new_wd = int(np.math.floor(wd / sf))\n        if new_size < 2:\n            return torch.zeros(res[0], res[1], img.shape[2]) \\\n                        if len(img.shape) > 2 else torch.zeros(res[0], res[1])\n        else:\n            img = scipy.misc.imresize(img, [new_ht, new_wd])\n            center = center * 1.0 / sf\n            scale = scale / sf\n\n    # Upper left point\n    ul = np.array(transform([0, 0], center, scale, res, invert=1))\n    # Bottom right point\n    br = np.array(transform(res, center, scale, res, invert=1))\n\n    # Padding so that when rotated proper amount of context is included\n    pad = int(np.linalg.norm(br - ul) / 2 - float(br[1] - ul[1]) / 2)\n    if not rot == 0:\n        ul -= pad\n        br += pad\n\n    new_shape = [br[1] - ul[1], br[0] - ul[0]]\n    if len(img.shape) > 2:\n        new_shape += [img.shape[2]]\n    new_img = np.zeros(new_shape)\n\n    # Range to fill new array\n    new_x = max(0, -ul[0]), min(br[0], img.shape[1]) - ul[0]\n    new_y = max(0, -ul[1]), min(br[1], img.shape[0]) - ul[1]\n    # Range to sample from original image\n    old_x = max(0, ul[0]), min(img.shape[1], br[0])\n    old_y = max(0, ul[1]), min(img.shape[0], br[1])\n    new_img[new_y[0]:new_y[1], new_x[0]:new_x[1]] = img[old_y[0]:old_y[1], old_x[0]:old_x[1]]\n\n    if not rot == 0:\n        # Remove padding\n        new_img = scipy.misc.imrotate(new_img, rot)\n        new_img = new_img[pad:-pad, pad:-pad]\n\n    new_img = im_to_torch(scipy.misc.imresize(new_img, res))\n    return new_img\n'"
