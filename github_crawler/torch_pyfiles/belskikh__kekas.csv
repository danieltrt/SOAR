file_path,api_count,code
setup.py,0,"b'import setuptools\n\nwith open(""README.md"", ""r"") as fh:\n    long_description = fh.read()\n\nsetuptools.setup(\n    name=""kekas"",\n    version=""0.1.23"",\n    author=""Aleksandr Belskikh"",\n    author_email=""belskikh.aleksandr@gmail.com"",\n    description=""Just another DL library."",\n    long_description=long_description,\n    long_description_content_type=""text/markdown"",\n    url=""https://github.com/belskikh/kekas"",\n    classifiers=[\n        ""Programming Language :: Python :: 3"",\n        ""License :: OSI Approved :: MIT License"",\n        ""Operating System :: OS Independent"",\n    ],\n    license=""MIT"",\n    python_requires="">=3.6.0"",\n    packages=setuptools.find_packages(),\n    install_requires=[\n        ""pandas>=0.22"",\n        ""numpy>=1.16.4"",\n        ""plotly>=4.0"",\n        ""tb-nightly"",\n        ""torch>=1.2"",\n        ""torchvision>=0.4"",\n        ""tqdm>=4.29.1"",\n        ""scikit-learn>=0.20"",\n    ],\n)\n'"
kekas/__init__.py,0,"b'from . import modules\nfrom .data import DataKek, DataOwner\nfrom .keker import Keker\nfrom .transformations import Transformer, normalize, to_torch\n\n__version__ = ""0.1.23""\n\n__all__ = [\n    ""Keker"",\n    ""DataKek"",\n    ""DataOwner"",\n    ""Transformer"",\n    ""to_torch"",\n    ""normalize"",\n    ""modules"",\n]\n'"
kekas/callbacks.py,7,"b'import shutil\nfrom collections import defaultdict\nfrom pathlib import Path\nfrom pdb import set_trace\nfrom typing import Any, Callable, Dict, List, Optional, Tuple, Union\n\nimport numpy as np\nimport torch\nfrom torch.optim import Optimizer\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau, _LRScheduler\nfrom torch.utils.tensorboard import SummaryWriter\n\nfrom .utils import DotDict, exp_weight_average, extend_postfix, get_opt_lr, get_pbar, to_numpy\n\ntry:\n    from apex import amp\nexcept ImportError:\n    pass  # warning message appears in keker.py module, no needs to be here\n\n\nclass Callback:\n    """"""\n    Abstract base class used to build new callbacks.\n    """"""\n\n    def on_batch_begin(self, i: int, state: DotDict) -> None:\n        pass\n\n    def on_batch_end(self, i: int, state: DotDict) -> None:\n        pass\n\n    def on_epoch_begin(self, epoch: int, epochs: int, state: DotDict) -> None:\n        pass\n\n    def on_epoch_end(self, epoch: int, state: DotDict) -> None:\n        pass\n\n    def on_train_begin(self, state: DotDict) -> None:\n        pass\n\n    def on_train_end(self, state: DotDict) -> None:\n        pass\n\n\nclass Callbacks:\n    def __init__(self, callbacks: Union[List, Any]) -> None:\n        if isinstance(callbacks, Callbacks):\n            self.callbacks = callbacks.callbacks\n        if isinstance(callbacks, list):\n            self.callbacks = callbacks\n        else:\n            self.callbacks = []\n\n    def on_batch_begin(self, i: int, state: DotDict) -> None:\n        for cb in self.callbacks:\n            cb.on_batch_begin(i, state)\n\n    def on_batch_end(self, i: int, state: DotDict) -> None:\n        for cb in self.callbacks:\n            cb.on_batch_end(i, state)\n\n    def on_epoch_begin(self, epoch: int, epochs: int, state: DotDict) -> None:\n        for cb in self.callbacks:\n            cb.on_epoch_begin(epoch, epochs, state)\n\n    def on_epoch_end(self, epoch: int, state: DotDict) -> None:\n        for cb in self.callbacks:\n            cb.on_epoch_end(epoch, state)\n\n    def on_train_begin(self, state: DotDict) -> None:\n        for cb in self.callbacks:\n            cb.on_train_begin(state)\n\n    def on_train_end(self, state: DotDict) -> None:\n        for cb in self.callbacks:\n            cb.on_train_end(state)\n\n\nclass LRUpdater(Callback):\n    """"""Basic class that all Lr updaters inherit from""""""\n\n    def __init__(self, init_lr: float) -> None:\n        self.init_lr = init_lr\n\n    def calc_lr(self) -> float:\n        raise NotImplementedError\n\n    def calc_momentum(self) -> float:\n        raise NotImplementedError\n\n    def update_lr(self, optimizer: Optimizer) -> float:\n        new_lr = self.calc_lr()\n        for pg in optimizer.param_groups:\n            pg[""lr""] = new_lr\n        return new_lr\n\n    def update_momentum(self, optimizer: Optimizer) -> float:\n        new_momentum = self.calc_momentum()\n        if ""betas"" in optimizer.param_groups[0]:\n            for pg in optimizer.param_groups:\n                pg[""betas""] = (new_momentum, pg[""betas""][1])\n        else:\n            for pg in optimizer.param_groups:\n                pg[""momentum""] = new_momentum\n        return new_momentum\n\n    def on_batch_begin(self, i: int, state: DotDict) -> None:\n        if state.core.mode == ""train"":\n            self.update_lr(state.core.opt)\n            self.update_momentum(state.core.opt)\n\n\nclass OneCycleLR(LRUpdater):\n    """"""\n    An learning rate updater\n        that implements the CircularLearningRate (CLR) scheme.\n    Learning rate is increased then decreased linearly.\n    Inspired by\n    https://github.com/fastai/fastai/blob/master/fastai/callbacks/one_cycle.py\n    """"""\n\n    def __init__(\n        self,\n        max_lr: float,\n        cycle_len: int,\n        len_loader: int,\n        momentum_range: Tuple[float, float],\n        div_factor: float,\n        increase_fraction: float,\n        annealing_cos: bool = False,\n    ) -> None:\n        super().__init__(max_lr)\n        self.cycle_len = cycle_len\n        self.momentum_range = momentum_range\n        self.div_factor = div_factor\n        self.increase_fraction = increase_fraction\n        self.len_loader = len_loader\n        self.annealing_cos = annealing_cos\n        self.total_iter = None\n        self.cycle_iter = 0\n        # point in iterations for starting lr decreasing\n        self.cut_point = None\n\n    def on_train_begin(self, state: DotDict) -> None:\n        self.total_iter = self.len_loader * self.cycle_len - 1\n        self.cut_point = int(self.total_iter * self.increase_fraction)\n\n    def calc_lr(self) -> float:\n        # calculate percent for learning rate change\n        if self.cycle_iter <= self.cut_point:\n            percent = self.cycle_iter / self.cut_point\n\n        else:\n            percent = 1 - (self.cycle_iter - self.cut_point) / (self.total_iter - self.cut_point)\n\n        if self.annealing_cos:\n            percent = self.calc_annealing_cos(0, 1, percent)\n\n        res = self.init_lr * (1 + percent * (self.div_factor - 1)) / self.div_factor\n\n        return res\n\n    def calc_momentum(self) -> float:\n        if self.cycle_iter <= self.cut_point:\n            percent = 1 - self.cycle_iter / self.cut_point\n\n        else:\n            percent = (self.cycle_iter - self.cut_point) / (self.total_iter - self.cut_point)\n        if self.annealing_cos:\n            percent = self.calc_annealing_cos(0, 1, percent)\n        res = self.momentum_range[1] + percent * (self.momentum_range[0] - self.momentum_range[1])\n        return res\n\n    def on_batch_begin(self, i: int, state: DotDict) -> None:\n        super().on_batch_begin(i, state)\n        if state.core.mode == ""train"":\n            self.cycle_iter += 1\n\n    def calc_annealing_cos(self, start:int, end:int, pct:float) -> float:\n        ""Cosine anneal from `start` to `end` as pct goes from 0.0 to 1.0.""\n        cos_out = np.cos(np.pi * pct) + 1\n        return end + (start - end) / 2 * cos_out\n\n\nclass LRFinder(LRUpdater):\n    """"""\n    Helps you find an optimal learning rate for a model,\n        as per suggetion of 2015 CLR paper.\n    Learning rate is increased in linear or log scale, depending on user input.\n\n    https://sgugger.github.io/how-do-you-find-a-good-learning-rate.html\n    """"""\n\n    def __init__(self, final_lr: float, n_steps: int, init_lr: float = 1e-6) -> None:\n        super().__init__(init_lr)\n        self.final_lr = final_lr\n        self.n_steps = max(1, n_steps - 1)\n        self.n = 0\n\n    def calc_lr(self) -> float:\n        res = self.init_lr * (self.final_lr / self.init_lr) ** (self.n / self.n_steps)\n        self.n += 1\n        return res\n\n    def calc_momentum(self) -> None:\n        pass\n\n    def update_momentum(self, optimizer: torch.optim.Optimizer) -> float:\n        pass\n\n    def on_batch_end(self, i: int, state: DotDict) -> None:\n        super().on_batch_end(i, state)\n        if self.n == (self.n_steps + 1):\n            print(""\\nEnd of LRFinder"")\n            state.core.stop_epoch = True\n\n\nclass TBLogger(Callback):\n    def __init__(self, logdir: Union[str, Path]) -> None:\n        self.logdir = Path(logdir)\n        self.writer = None\n        self.total_iter = 0\n        self.train_iter = 0\n        self.val_iter = 0\n        self.train_batch_iter = 0\n        self.val_batch_iter = 0\n        self.train_writer = SummaryWriter(str(self.logdir / ""train""))\n        self.val_writer = SummaryWriter(str(self.logdir / ""val""))\n\n    def update_total_iter(self, mode: str) -> None:\n        if mode == ""train"":\n            self.train_iter += 1\n            self.train_batch_iter += 1\n        if mode == ""val"":\n            self.val_iter += 1\n            self.val_batch_iter += 1\n        self.total_iter += 1\n\n    def on_train_begin(self, state: DotDict) -> None:\n        self.train_iter = 0\n        self.val_iter = 0\n        self.logdir.mkdir(exist_ok=True)\n        self.train_writer = SummaryWriter(str(self.logdir / ""train""))\n        self.val_writer = SummaryWriter(str(self.logdir / ""val""))\n\n    def on_epoch_begin(self, epoch: int, epochs: int, state: DotDict):\n        self.train_batch_iter = 0\n        self.val_batch_iter = 0\n\n    def on_batch_end(self, i: int, state: DotDict) -> None:\n        if state.core.mode == ""train"":\n            for name, metric in state.core.batch_metrics[""train""].items():\n                self.train_writer.add_scalar(\n                    f""batch/{name}"", float(metric), global_step=self.total_iter\n                )\n            lr = get_opt_lr(state.core.opt)\n            self.train_writer.add_scalar(""batch/lr"", float(lr), global_step=self.train_iter)\n\n            self.update_total_iter(state.core.mode)\n\n        elif state.core.mode == ""val"":\n            for name, metric in state.core.batch_metrics[""val""].items():\n                self.val_writer.add_scalar(\n                    f""batch/{name}"", float(metric), global_step=self.total_iter\n                )\n\n            self.update_total_iter(state.core.mode)\n\n    def on_epoch_end(self, epoch: int, state: DotDict) -> None:\n        for name, metric in state.core.epoch_metrics[""train""].items():\n            self.train_writer.add_scalar(f""epoch/{name}"", float(metric), global_step=epoch)\n\n        for name, metric in state.core.epoch_metrics[""val""].items():\n            self.val_writer.add_scalar(f""epoch/{name}"", float(metric), global_step=epoch)\n\n    def on_train_end(self, state: DotDict) -> None:\n        self.train_writer.close()\n        self.val_writer.close()\n\n\nclass SimpleLossCallback(Callback):\n    def __init__(self, target_key: str, preds_key: str) -> None:\n        self.target_key = target_key\n        self.preds_key = preds_key\n    \n    def on_batch_end(self, i: int, state: DotDict) -> None:\n        if state.core.mode == ""train"":\n            target = state.core.batch[self.target_key]\n            preds = state.core.out[self.preds_key]\n\n            state.core.loss = state.core.criterion(preds, target)\n\n\nclass SimpleOptimizerCallback(Callback):\n    def on_batch_end(self, i: int, state: DotDict) -> None:\n        if state.core.mode == ""train"":\n            state.core.opt.zero_grad()\n            if state.core.use_fp16:\n                with amp.scale_loss(state.core.loss, state.core.opt) as scaled_loss:\n                    scaled_loss.backward()\n            else:\n                state.core.loss.backward()\n            state.core.opt.step()\n\n\nclass SimpleSchedulerCallback(Callback):\n    def __init__(self,\n                 sched: Union[_LRScheduler, ReduceLROnPlateau],\n                 metric: str = None) -> None:\n        self.metric = ""loss"" or metric\n        self.is_reduce = isinstance(sched, ReduceLROnPlateau)\n\n    def on_epoch_end(self, epoch: int, state: DotDict) -> None:\n        if state.core.mode == ""val"":\n            metric = state.core.epoch_metrics[""val""][self.metric] if self.is_reduce else None\n            state.core.sched.step(metric)\n\n\nclass ProgressBarCallback(Callback):\n    def __init__(self) -> None:\n        self.running_loss = None\n\n    def on_epoch_begin(self, epoch: int, epochs: int, state: DotDict) -> None:\n        self.running_loss = None\n\n        loader = state.core.loader\n        if state.core.mode == ""train"":\n            description = f""Epoch {epoch+1}/{epochs}""\n            state.core.pbar = get_pbar(loader, description)\n        elif state.core.mode == ""test"":\n            description = ""Predict""\n            state.core.pbar = get_pbar(loader, description)\n\n    def on_batch_end(self, i: int, state: DotDict) -> None:\n\n        if state.core.mode == ""train"":\n            if not self.running_loss:\n                self.running_loss = to_numpy(state.core.loss)\n\n            self.running_loss = exp_weight_average(state.core.loss, self.running_loss)\n            postfix = {""loss"": f""{self.running_loss:.4f}""}\n            state.core.pbar.set_postfix(postfix)\n            state.core.pbar.update()\n        elif state.core.mode == ""test"":\n            state.core.pbar.update()\n\n    def on_epoch_end(self, epoch: int, state: DotDict) -> None:\n        if state.core.mode == ""val"":\n            metrics = state.core.get(""epoch_metrics"", {}).get(""val"", {}).copy()\n            if metrics:\n                # rename ""loss"" to ""val_loss"" and put it in the 1st place in dct\n                metrics_tmp = {""val_loss"": metrics.pop(""loss"")}\n                metrics = dict(metrics_tmp, **metrics)\n            state.core.pbar.set_postfix_str(extend_postfix(state.core.pbar.postfix, metrics))\n            state.core.pbar.close()\n        elif state.core.mode == ""test"":\n            state.core.pbar.close()\n\n\nclass MetricsCallback(Callback):\n    def __init__(\n        self, target_key: str, preds_key: str, metrics: Optional[Dict[str, Callable]] = None,\n    ) -> None:\n        self.target_key = target_key\n        self.preds_key = preds_key\n\n        self.metrics = metrics or {}\n        self.reset_metrics()\n\n    def reset_metrics(self):\n        self.train_metrics = defaultdict(float)\n\n        self.val_preds = []\n        self.val_target = []\n\n    def on_epoch_begin(self, epoch: int, epochs: int, state: DotDict) -> None:\n        state.core.epoch_metrics = defaultdict(dict)\n\n    def on_batch_end(self, i: int, state: DotDict) -> None:\n        mode = state.core.mode\n        loss = float(to_numpy(state.core.loss))\n        preds = state.core.out[self.preds_key]\n        # dataparallel case\n        if isinstance(preds, list):\n            preds = torch.cat(preds)\n        target = state.core.batch[self.target_key]\n\n        if mode == ""val"":\n            self.val_preds.append(preds.detach().cpu())\n            self.val_target.append(target.detach().cpu())\n\n        # logs\n        if state.core.do_log:\n            state.core.batch_metrics[mode][""loss""] = loss\n            for name, m in self.metrics.items():\n                value = m(preds, target)\n                state.core.batch_metrics[mode][name] = value\n            if mode == ""train"":\n                for name, value in state.core.batch_metrics[""train""].items():\n                    self.train_metrics[name] += value\n\n    def on_epoch_end(self, epoch: int, state: DotDict) -> None:\n        if state.core.do_log:\n            divider = len(state.core.loader)\n            for k in self.train_metrics.keys():\n                self.train_metrics[k] /= divider\n            state.core.epoch_metrics[""train""] = self.train_metrics\n\n        if self.val_preds:\n            val_preds = torch.cat(self.val_preds)\n            val_target = torch.cat(self.val_target)\n\n            total_val_loss = float(to_numpy(state.core.cpu_criterion(val_preds, val_target)))\n            state.core.epoch_metrics[""val""][""loss""] = total_val_loss\n            for name, m in self.metrics.items():\n                value = m(val_preds, val_target)\n                state.core.epoch_metrics[""val""][name] = value\n\n        self.reset_metrics()\n\n\nclass PredictionsSaverCallback(Callback):\n    def __init__(self, savepath: Optional[Union[str, Path]], preds_key: str) -> None:\n        if savepath is not None:\n            self.savepath = Path(savepath)\n            self.savepath.parent.mkdir(exist_ok=True)\n            self.return_array = False\n        else:\n            self.savepath = None\n            self.return_array = True\n        self.preds_key = preds_key\n        self.preds: List = []\n\n    def on_batch_end(self, i: int, state: DotDict) -> None:\n        if state.core.mode == ""test"":\n            out = state.core.out[self.preds_key]\n            # DataParallelModel workaround\n            if isinstance(out, list):\n                out = np.concatenate([to_numpy(o) for o in out])\n            else:\n                out = to_numpy(out)\n            self.preds.append(out)\n\n    def on_epoch_end(self, epoch: int, state: DotDict) -> None:\n        if state.core.mode == ""test"":\n            preds = np.concatenate(self.preds)\n            if self.return_array:\n                state.core.preds = preds\n            else:\n                np.save(self.savepath, preds)\n            self.preds = []\n\n\nclass CheckpointSaverCallback(Callback):\n    def __init__(\n            self,\n            savedir: str,\n            metric: Optional[str] = None,\n            n_best: int = 3,\n            prefix: Optional[str] = None,\n            mode: str = ""min""\n        ) -> None:\n        self.metric = metric or ""loss""\n        self.n_best = n_best\n        self.savedir = Path(savedir)\n        self.prefix = f""{prefix}."" if prefix is not None else ""checkpoint.""\n\n        if mode not in [""min"", ""max""]:\n            raise ValueError(f""mode should be \'min\' or \'max\', got {mode}"")\n        if mode == ""min"":\n            self.maximize = False\n        if mode == ""max"":\n            self.maximize = True\n\n        self.best_scores = []\n\n    def on_epoch_begin(self, epoch: int, epochs: int, state: DotDict) -> None:\n        # trim best scores\n        self.best_scores = self.best_scores[:epochs]\n\n    def on_epoch_end(self, epoch: int, state: DotDict) -> None:\n        if state.core.mode == ""val"":\n            score_val = state.core.epoch_metrics[""val""][self.metric]\n            score_name = f""{self.prefix}{epoch + 1}.h5""\n            score = (score_val, score_name)\n            sorted_scores = sorted(self.best_scores + [score], reverse=self.maximize)\n            self.best_scores = sorted_scores[: self.n_best]\n            if score_name in (s[1] for s in self.best_scores):\n                state.core.checkpoint = f""{self.savedir / score_name}""\n                # remove worst checkpoint\n                if len(sorted_scores) > self.n_best:\n                    Path(f""{self.savedir / sorted_scores[-1][1]}"").unlink()\n\n    def on_train_end(self, state: DotDict) -> None:\n        best_cp = self.savedir / self.best_scores[0][1]\n        shutil.copy(str(best_cp), f""{self.savedir}/{self.prefix}best.h5"")\n        print(f""\\nCheckpoint\\t{self.metric}"")\n        for score in self.best_scores:\n            print(f""{self.savedir/score[1]}\\t{score[0]:.6f}"")\n\n\nclass EarlyStoppingCallback(Callback):\n    def __init__(\n        self, patience: int, metric: Optional[str] = None, mode: str = ""min"", min_delta: int = 0,\n    ) -> None:\n        self.best_score = None\n        self.metric = metric or ""loss""\n        self.patience = patience\n        self.num_bad_epochs = 0\n        self.is_better = None\n\n        if mode not in [""min"", ""max""]:\n            raise ValueError(f""mode should be \'min\' or \'max\', got {mode}"")\n        if mode == ""min"":\n            self.is_better = lambda score, best: score <= (best - min_delta)\n        if mode == ""max"":\n            self.is_better = lambda score, best: score >= (best - min_delta)\n\n    def on_epoch_end(self, epoch: int, state: DotDict) -> None:\n        if state.core.mode == ""val"":\n            score = state.core.epoch_metrics[""val""][self.metric]\n            if self.best_score is None:\n                self.best_score = score\n            if self.is_better(score, self.best_score):\n                self.num_bad_epochs = 0\n                self.best_score = score\n            else:\n                self.num_bad_epochs += 1\n\n            if self.num_bad_epochs >= self.patience:\n                state.core.stop_train = True\n\n\nclass DebuggerCallback(Callback):\n    def __init__(self, when: List[str], modes: List[str]) -> None:\n        self.when = when\n        self.modes = modes\n\n    def on_batch_begin(self, i: int, state: DotDict) -> None:\n        if ""on_batch_begin"" in self.when:\n            if state.core.mode == ""train"" and ""train"" in self.modes:\n                set_trace()\n            if state.core.mode == ""val"" and ""val"" in self.modes:\n                set_trace()\n            if state.core.mode == ""test"" and ""test"" in self.modes:\n                set_trace()\n\n    def on_batch_end(self, i: int, state: DotDict) -> None:\n        if ""on_batch_end"" in self.when:\n            if state.core.mode == ""train"" and ""train"" in self.modes:\n                set_trace()\n            if state.core.mode == ""val"" and ""val"" in self.modes:\n                set_trace()\n            if state.core.mode == ""test"" and ""test"" in self.modes:\n                set_trace()\n\n    def on_epoch_begin(self, epoch: int, epochs: int, state: DotDict) -> None:\n        if ""on_epoch_begin"" in self.when:\n            if state.core.mode == ""train"" and ""train"" in self.modes:\n                set_trace()\n            if state.core.mode == ""val"" and ""val"" in self.modes:\n                set_trace()\n            if state.core.mode == ""test"" and ""test"" in self.modes:\n                set_trace()\n\n    def on_epoch_end(self, epoch: int, state: DotDict) -> None:\n        if ""on_epoch_end"" in self.when:\n            if state.core.mode == ""train"" and ""train"" in self.modes:\n                set_trace()\n            if state.core.mode == ""val"" and ""val"" in self.modes:\n                set_trace()\n            if state.core.mode == ""test"" and ""test"" in self.modes:\n                set_trace()\n\n    def on_train_begin(self, state: DotDict) -> None:\n        if ""on_train_begin"" in self.when:\n            set_trace()\n\n    def on_train_end(self, state: DotDict) -> None:\n        if ""on_train_end"" in self.when:\n            set_trace()\n'"
kekas/data.py,1,"b'from collections import namedtuple\nfrom typing import Callable, Dict, Optional\n\nimport pandas as pd\nfrom torch.utils.data import Dataset\nfrom torchvision.transforms import Compose\n\n\nclass DataKek(Dataset):\n    """"""The easiest way to store the dataset info is pandas DataFrame.\n    So DataKek gets a DataFrame as a source of data""""""\n\n    def __init__(\n        self, df: pd.DataFrame, reader_fn: Callable, transforms: Optional[Compose] = None,\n    ) -> None:\n        # transform df to list of dict records\n        self.data = df\n        self.reader_fn = reader_fn\n        self.transforms = transforms\n\n    def __getitem__(self, ind: int) -> Dict:\n        datum = self.reader_fn(ind, self.data.iloc[ind].to_dict())\n        if self.transforms is not None:\n            datum = self.transforms(datum)\n        return datum\n\n    def __len__(self) -> int:\n        return len(self.data)\n\n\nDataOwner = namedtuple(""DataOwner"", [""train_dl"", ""val_dl"", ""test_dl""])\n'"
kekas/keker.py,28,"b'import warnings\nfrom collections import defaultdict\nfrom pathlib import Path\nfrom typing import Callable, Dict, List, Optional, Tuple, Type, Union\n\nimport numpy as np\nimport torch\nfrom torch.optim import SGD, Optimizer\nfrom torch.utils.data import DataLoader\n\nfrom .callbacks import (\n    Callback,\n    Callbacks,\n    CheckpointSaverCallback,\n    EarlyStoppingCallback,\n    LRFinder,\n    MetricsCallback,\n    OneCycleLR,\n    PredictionsSaverCallback,\n    ProgressBarCallback,\n    SimpleLossCallback,\n    SimpleOptimizerCallback,\n    SimpleSchedulerCallback,\n    TBLogger,\n)\nfrom .data import DataOwner\nfrom .parallel import DataParallelCriterion, DataParallelModel\nfrom .utils import DotDict, freeze, freeze_to, load_state_dict, plot_tensorboard_log, unfreeze\n\ntry:\n    from apex import amp\nexcept ImportError as e:\n    warnings.warn(\n        f\'Error ""{e}"" during importing apex library. To use mixed precison\'\n        "" you should install it from https://github.com/NVIDIA/apex""\n    )\n\n\nclass Keker:\n    """""" The class serving the whole train-val-predict process.\n\n    Args:\n        model: The neural network for train/val/predict.\n        dataowner: The namedtuple container of train/val/test dataloaders.\n        criterion: The loss function or the dict {\'name\': loss function}\n            in case of multiple loss setup. If multiple loss is using,\n            loss_cb should be provided.\n            (ex. : torch.nn.CrossEntropyLoss(),\n            {""ce"": torch.nn.CrossEntropyLoss(), ""bce"": torch.nn.BCE()})\n        tarket_key: The target/label key for batch-dict from dataloader.\n            The dataloader returns batch as a dict on each iteration,\n            that contains input data and target labels. This is key is for\n            access to target labels in this dict.\n        preds_key: The key for dict from self.step() functions.\n            The self.step() function returns dict of predictions on each batch.\n            This key is for access to predictions in this dict.\n            This attribute is optional in default behavior but coulb be used\n            in case of using custom callbacks.\n        metrics: {""name"": metric_function} dict, that contains callable metrics\n            for calculating. The metric takes prediction and target\n            tensors as parameters, and returns float.\n            For examples see kekas.metrics module.\n        opt: pytorch Optimizer class (ex. torch.optim.SGD, torch.optm.Adam, etc)\n            This optimizer will be used as default during training.\n            Default optimizer is torch.optim.SGD.\n        opt_params: The kwargs dict for optimizer initialization.\n            It should contain any optimizer param you want EXCEPT learning rate,\n            learing rate is specified in self.kek* methods.\n        device: The torch.device when you want to put your model\n        step_fn: The function that will be called at every batch of your data.\n            Take a `self` as a parameter. In this function you define\n            what you do with your batch. You get access to batch through\n            self._state.core.batch object. Batch is a dict returned from your\n            dataloader, and its key and values are specified in reader_function\n            for DataKek.\n            Return a dict that should contain batch predictions with access\n            by `preds_key`.\n            For example see self.default_step method and example ipynbs.\n        loss_cb: The Callback for loss calculation. Define how to calculate loss\n            and store loss value in self._state.core.loss attr.\n            Default loss callback is SimpleLossCallback.\n            For examples see kekas.callbacks.\n        opt_cb: The Callback for optimizer applying.\n            Default optimizer callback is SimpleOptimizerCallback.\n            For examples see kekas.callbacks.\n        callbacks: custom Callbacks thet will be applied before the core ones.\n            For examples see example ipynbs.\n    """"""\n\n    def __init__(\n        self,\n        model: torch.nn.Module,\n        dataowner: Optional[DataOwner] = None,\n        criterion: Optional[Union[torch.nn.Module, Dict[str, torch.nn.Module]]] = None,\n        target_key: str = ""label"",\n        preds_key: str = ""preds"",\n        metrics: Optional[Dict[str, Callable]] = None,\n        opt: Optional[torch.optim.Optimizer] = None,\n        opt_params: Optional[Dict] = None,\n        device: Optional[torch.device] = None,\n        step_fn: Optional[Callback] = None,\n        loss_cb: Optional[Callback] = None,\n        opt_cb: Optional[Callback] = None,\n        callbacks: Optional[Union[List, Callbacks]] = None,\n    ) -> None:\n\n        # The state is an object that stores many variables and represents\n        # the state of your train-val-predict pipeline. state passed to every\n        # callback call.\n        # You can use it as a container for your custom variables, but\n        # DO NOT USE the \'core\' attrubute, because all kekers variables are\n        # there\n        #\n        self.state = DotDict()\n        self.state.core = DotDict()\n\n        self.state.core.model = model\n        self.state.core.dataowner = dataowner or DataOwner(None, None, None)\n\n        self.target_key = target_key\n        self.preds_key = preds_key\n\n        self.state.core.criterion = criterion\n        self.state.core.cpu_criterion = criterion  # for cpu validation\n\n        self.state.core.parallel = False\n        if torch.cuda.device_count() > 1:\n            self.state.core.model = DataParallelModel(self.state.core.model)\n            if self.state.core.criterion is not None:\n                if isinstance(self.state.core.criterion, dict):\n                    self.state.core.criterion = {\n                        k: DataParallelCriterion(v) for k, v in self.state.core.criterion.items()\n                    }\n                else:\n                    self.state.core.criterion = DataParallelCriterion(self.state.core.criterion)\n            self.state.core.parallel = True\n\n        self.opt = opt or SGD\n        self.opt_params = opt_params or {}\n        self.device = device or torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n        self.state.core.model.to(self.device)\n\n        self.step_fn = step_fn or self.default_step_fn\n\n        # the core callbacks for train-val-predict are determined here.\n        # the order of callbacks is matters!\n        if loss_cb is not None:\n            loss_cb = loss_cb(target_key, self.preds_key)\n        else:\n            loss_cb = SimpleLossCallback(target_key, self.preds_key)\n\n        opt_cb = opt_cb or SimpleOptimizerCallback()\n        metrics_cb = MetricsCallback(target_key, self.preds_key, metrics)\n\n        callbacks = callbacks or []\n        self.core_callbacks = callbacks + [\n            loss_cb,\n            metrics_cb,\n            opt_cb,\n            ProgressBarCallback(),\n        ]\n        callbacks = self.core_callbacks[:]\n\n        self.callbacks = Callbacks(callbacks)\n\n        self.state.core.checkpoint = """"\n\n        # The number of batch in dataloader for iteration stop,\n        # determined in self.kek* methods.\n        self.state.core.stop_iter = None\n\n        # flag for train stop after batch.\n        self.state.core.stop_epoch = False\n\n        # flag for stop the whole train, used for early stopping.\n        self.state.core.stop_train = False\n\n        # The scheduler attribute. Scheduler is determined in self.kek* methods.\n        self.state.core.sched = None\n\n        # Flag for logger callback\n        self.state.core.do_log = False\n\n        # Predictions variable\n        self.state.core.preds = None\n\n        # FP16 flag\n        self.state.core.use_fp16 = False\n\n    def kek(self,\n            lr: float,\n            epochs: int,\n            skip_val: bool = False,\n            opt: Optional[Optimizer] = None,\n            opt_params: Optional[Dict] = None,\n            sched: Optional[Callable] = None,\n            sched_params: Optional[Dict] = None,\n            reduce_metric: Optional[str] = None,\n            stop_iter: Optional[int] = None,\n            logdir: Optional[Union[str, Path]] = None,\n            cp_saver_params: Optional[Dict] = None,\n            early_stop_params: Optional[Dict] = None\n        ) -> None:\n        """"""Kek your model to the moon!\n\n        Conducts a standard train-val procedure with several options for\n        customization.\n\n        Args:\n            lr: learining rate\n            epochs: number of epochs to train\n            opt: torch optimizer. If specified, than specified optimizer will be\n                used for this train-val procedure, else the default one.\n            opt_params: The kwargs dict for custom optimizer initialization.\n                It should contain any opt params you want EXCEPT learning rate,\n                Can be defined even for default optimizer.\n            sched: optional pytorch scheduler class. If specified, sched_params\n                must be specified too.\n                Ex: torch.optim.lr_scheduler.StepLR.\n            sched_params: kwargs dict parameters for scheduler\n            reduce_metric: metric to track if sched is ReduceLROnPlateau\n            stop_iter: number of batch when you want to end an epoch\n            logdir: If provided, the TBLogger will be created and tensorboard\n                logs will be written in this directory.\n                For more info see kekas.callbacks.TBLogger and example ipynb\'s.\n            cp_saver_params: kwargs dict parameters for CheckpointSaverCallback.\n                If provided, then a CheckpointSaverCallback will be created.\n                For more info see kekas.callbacks.CheckpointSaverCallback\n                and example ipynb\'s.\n            early_stop_params: kwargs dict parameters for EarlyStoppingCallback.\n                If provided, then a EarlyStoppingCallback will be created.\n                For more info see kekas.callbacks.EarlyStoppingCallback\n                and example ipynb\'s.\n        """"""\n\n        # check if criterion exists\n        if self.state.core.criterion is None:\n            raise Exception(""Keker needs criterion. "" ""Reinitialize Keker with one."")\n\n        # check if dataowner exists\n        if self.state.core.dataowner.train_dl is None:\n            raise Exception(""Keker needs Dataowner. "" ""Reinitialize Keker with one."")\n\n        if stop_iter:\n            self.state.core.stop_iter = stop_iter\n\n        # save callbacks\n        callbacks = self.callbacks\n\n        opt = opt or self.opt\n        opt_params = opt_params or self.opt_params\n        params = (p for p in self.state.core.model.parameters() if p.requires_grad)\n        self.state.core.opt = opt(params=params, lr=lr, **opt_params)\n\n        if self.state.core.use_fp16:\n            _, self.state.core.opt = amp.initialize(\n                self.state.core.model, self.state.core.opt, **self.state.core.amp_params\n            )\n\n        if sched:\n            sched_params = sched_params or {}\n            self.state.core.sched = sched(optimizer=self.state.core.opt, **sched_params)\n            sched_cb = SimpleSchedulerCallback(\n                    sched=self.state.core.sched,\n                    metric=reduce_metric\n            )\n            self.callbacks = Callbacks(self.callbacks.callbacks + [sched_cb])\n\n        if logdir:\n            self.state.core.do_log = True\n            self.state.core.batch_metrics = defaultdict(dict)\n            self.state.core.epoch_metrics = defaultdict(dict)\n            tboard_cb = TBLogger(logdir)\n            self.callbacks = Callbacks(self.callbacks.callbacks + [tboard_cb])\n\n        cp_saver_params = cp_saver_params or {}\n        if cp_saver_params:\n            cp_saver_cb = CheckpointSaverCallback(**cp_saver_params)\n            self.callbacks = Callbacks(self.callbacks.callbacks + [cp_saver_cb])\n\n        early_stop_params = early_stop_params or {}\n        if early_stop_params:\n            early_stop_cb = EarlyStoppingCallback(**early_stop_params)\n            self.callbacks = Callbacks(self.callbacks.callbacks + [early_stop_cb])\n\n        # try-finally to properly close progress bar and restore callbacks\n        try:\n            self.callbacks.on_train_begin(self.state)\n\n            for epoch in range(epochs):\n                self.set_mode(""train"")\n                self._run_epoch(epoch, epochs)\n\n                if not skip_val:\n                    self.set_mode(""val"")\n                    self._run_epoch(epoch, epochs)\n\n                if self.state.core.stop_train:\n                    self.state.core.stop_train = False\n                    print(f""Early stopped on {epoch + 1} epoch"")\n                    break\n\n            self.callbacks.on_train_end(self.state)\n        finally:\n            self.state.core.pbar.close()\n            self.state.core.do_log = False\n            self.callbacks = callbacks\n\n    def kek_one_cycle(\n        self,\n        max_lr: float,\n        cycle_len: int,\n        momentum_range: Tuple[float, float] = (0.95, 0.85),\n        div_factor: float = 25,\n        increase_fraction: float = 0.3,\n        annealing_cos: bool = False,\n        opt: Optional[Optimizer] = None,\n        opt_params: Optional[Dict] = None,\n        logdir: Optional[Union[str, Path]] = None,\n        cp_saver_params: Optional[Dict] = None,\n        early_stop_params: Optional[Dict] = None,\n    ) -> None:\n        """"""Kek your model to the moon with One Cycle policy!\n\n        Conducts a one-cycle train-val procedure with several options for\n        customization.\n        For info about One Cycle policy please see:\n        https://arxiv.org/abs/1803.09820\n        https://sgugger.github.io/the-1cycle-policy.html\n\n        Args:\n            max_lr: the maximum learning rate that will be achieved during\n                training process\n            cycle_len: the number of full passes through the training dataset.\n                It is quite similar to epochs number\n            momentum_range: the range of optimizers momentum changes\n            div_factor: is equal to max_lr / min_lr during one-cycle training\n            increase_fraction: the fraction of the whole iterations during which\n                the learning rate will increase\n            opt: torch optimizer. If specified, than specified optimizer will be\n                used for this train-val procedure, else the default one.\n            opt_params: The kwargs dict for custom optimizer initialization.\n                It should contain any opt params you want EXCEPT learning rate,\n                Can be defined even for default optimizer.\n            logdir: If provided, the TBLogger will be created and tensorboard\n                logs will be written in this directory.\n                For more info see kekas.callbacks.TBLogger and example ipynb\'s.\n            cp_saver_params: kwargs dict parameters for CheckpointSaverCallback.\n                If provided, then a CheckpointSaverCallback will be created.\n                For more info see kekas.callbacks.CheckpointSaverCallback\n                and example ipynb\'s.\n            early_stop_params: kwargs dict parameters for EarlyStoppingCallback.\n                If provided, then a EarlyStoppingCallback will be created.\n                For more info see kekas.callbacks.EarlyStoppingCallback\n                and example ipynb\'s.\n        """"""\n\n        callbacks = self.callbacks\n\n        # temporarily add OneCycle callback\n        len_loader = len(self.state.core.dataowner.train_dl)\n        one_cycle_cb = OneCycleLR(\n            max_lr, cycle_len, len_loader, momentum_range, div_factor, increase_fraction, annealing_cos\n        )\n\n        try:\n            self.callbacks = Callbacks(callbacks.callbacks + [one_cycle_cb])\n\n            self.kek(\n                lr=max_lr,\n                epochs=cycle_len,\n                opt=opt,\n                opt_params=opt_params,\n                logdir=logdir,\n                cp_saver_params=cp_saver_params,\n                early_stop_params=early_stop_params,\n            )\n        finally:\n            # set old callbacks without OneCycle\n            self.callbacks = callbacks\n\n    def kek_lr(\n        self,\n        final_lr: float,\n        logdir: Union[str, Path],\n        init_lr: float = 1e-6,\n        n_steps: Optional[int] = None,\n        opt: Optional[Optimizer] = None,\n        opt_params: Optional[Dict] = None,\n    ) -> None:\n        """"""Help you kek your model to the moon by finding ""optimal"" lr!\n\n        Conducts the learning rate find procedure.\n        For info please see:\n        https://arxiv.org/abs/1803.09820\n        https://sgugger.github.io/how-do-you-find-a-good-learning-rate.html\n\n        Args:\n            final_lr: the learning rate at the end of the lr find process\n            logdir: the directory for tensorboard logs, that will allow you\n                analyze the loss dynamic.\n            init_lr: the learning rate at the start of the lr find process\n            n_steps: the number of iterations of lr find process. If provided\n                finding process will stop at this iteration, else lr find\n                process will last one epoch\n            opt: torch optimizer. If specified, than specified optimizer will be\n                used for this train-val procedure, else the default one.\n            opt_params: The kwargs dict for custom optimizer initialization.\n                It should contain any opt params you want EXCEPT learning rate,\n                Can be defined even for default optimizer.\n        """"""\n\n        logdir = Path(logdir)\n        logdir.mkdir(exist_ok=True)\n        tmp_cp = logdir / ""tmp.h5""\n        self.save(tmp_cp)\n\n        len_loader = len(self.state.core.dataowner.train_dl)\n        n_steps = n_steps if n_steps is not None else len_loader\n        n_epochs = max(1, int(np.ceil(n_steps / len_loader)))\n\n        callbacks = self.callbacks\n\n        try:\n            lrfinder_cb = LRFinder(final_lr=final_lr, init_lr=init_lr, n_steps=n_steps)\n\n            self.callbacks = Callbacks(self.core_callbacks + [lrfinder_cb])\n            self.kek(\n                lr=init_lr,\n                epochs=n_epochs,\n                skip_val=True,\n                logdir=logdir,\n                opt=opt,\n                opt_params=opt_params,\n            )\n        finally:\n            self.callbacks = callbacks\n            self.load(tmp_cp)\n            tmp_cp.unlink()\n\n    def _run_epoch(self, epoch: int, epochs: int) -> None:\n        """"""Run one epoch of train-val procedure\n\n        Args:\n            epoch: number of the current epoch\n            epochs: total number of epochs\n        """"""\n        self.callbacks.on_epoch_begin(epoch, epochs, self.state)\n\n        with torch.set_grad_enabled(self.is_train):\n            for i, batch in enumerate(self.state.core.loader):\n                self.callbacks.on_batch_begin(i, self.state)\n\n                self.state.core.batch = self.to_device(batch)\n\n                self.state.core.out = self.step()\n\n                self.callbacks.on_batch_end(i, self.state)\n\n                if (\n                    self.state.core.stop_iter\n                    and self.state.core.mode == ""train""\n                    and i == self.state.core.stop_iter - 1\n                ):\n                    # break only in train mode and if early stop is set\n                    self.state.core.stop_epoch = True\n\n                if self.state.core.stop_epoch:\n                    self.state.core.stop_epoch = False\n                    # st()\n                    break\n\n        self.callbacks.on_epoch_end(epoch, self.state)\n\n        if self.state.core.checkpoint:\n            self.save(self.state.core.checkpoint)\n            self.state.core.checkpoint = """"\n\n    @staticmethod\n    def default_step_fn(model: torch.nn.Module, batch: Dict[str, torch.Tensor]) -> torch.Tensor:\n        """"""Determine what your model will do with your data.\n\n        Args:\n            model: the pytorch module to pass input in\n            batch: the batch of data from the DataLoader\n\n        Returns:\n            The models forward pass results\n        """"""\n        inp = batch[""image""]\n        return model(inp)\n\n    def step(self) -> Dict[str, torch.Tensor]:\n        """"""The step function that calls each iteration.\n        Wraps the self.step_fn to provide a dict of predictions\n\n        Returns:\n            the dict that contains prediction tensor for the batch.\n        """"""\n        preds = self.step_fn(model=self.state.core.model, batch=self.state.core.batch)\n\n        return {self.preds_key: preds}\n\n    def predict(self, savepath: Optional[Union[str, Path]] = None) -> Union[None, np.ndarray]:\n        """"""Infer the model on test dataloader and saves prediction as numpy array\n\n        Args:\n            savepath: the directory to save predictions. If not passed,\n                the method will return a numpy array of predictions.\n        """"""\n        return self.predict_loader(loader=self.state.core.dataowner.test_dl, savepath=savepath)\n\n    def predict_loader(\n        self, loader: DataLoader, savepath: Optional[Union[str, Path]] = None\n    ) -> Union[None, np.ndarray]:\n        """"""Infer the model on dataloader and saves prediction as numpy array\n\n        Args:\n            loader: the dataloader for generating predictions\n            savepath: the directory to save predictions. If not passed,\n                the method will return a numpy array of predictions.\n        """"""\n        callbacks = self.callbacks\n\n        tmp_callbacks = Callbacks(\n            [ProgressBarCallback(), PredictionsSaverCallback(savepath, self.preds_key)]\n        )\n\n        self.callbacks = tmp_callbacks\n\n        self.state.core.mode = ""test""\n        self.state.core.loader = loader\n        self.state.core.model.eval()\n        with torch.set_grad_enabled(False):\n            self._run_epoch(1, 1)\n\n        self.callbacks = callbacks\n\n        preds = self.state.core.preds\n        self.state.core.preds = None\n        return preds\n\n    def predict_tensor(\n        self, tensor: Type[torch.Tensor], to_numpy: bool = False\n    ) -> Union[Type[torch.Tensor], np.ndarray]:\n        """"""Infer the model on one torch Tensor.\n\n        Args:\n            tensor: torch tensor to predict on.\n                Should has [batch_size, *(one_sample_shape)] shape\n            to_numpy: if True, converts predictions to numpy array\n\n        Returns:\n            Predictions on input tensor.\n        """"""\n        tensor = tensor.to(self.device)\n        with torch.set_grad_enabled(False):\n            self.set_mode(""test"")\n            preds = self.state.core.model(tensor)\n\n            # dataparallel workaround\n            if isinstance(preds, list):\n                preds = torch.cat(preds)\n        if to_numpy:\n            preds = preds.cpu().numpy()\n        return preds\n\n    def predict_array(\n        self, array: np.ndarray, to_numpy: bool = False\n    ) -> Union[Type[torch.Tensor], np.ndarray]:\n        """"""Infer the model on one numpy array.\n\n        Args:\n            array: numpy array to predict on.\n                Should has [batch_size, *(one_sample_shape)] shape\n            to_numpy: if True, converts predictions to numpy array\n\n        Returns:\n            Predictions on input tensor.\n        """"""\n        tensor = torch.from_numpy(array)\n        return self.predict_tensor(tensor, to_numpy)\n\n    def TTA(\n        self,\n        loader: DataLoader,\n        tfms: Union[List, Dict],\n        savedir: Union[str, Path],\n        prefix: str = ""preds"",\n    ) -> None:\n        """"""Conduct the test-time augmentations procedure.\n\n        Create predictions for each set of provided transformations and saves\n        each prediction in savedir as a numpy arrays.\n\n        Args:\n            loader: loader to predict\n            tfms: the list with torchvision.transforms or\n                  the dict with {""name"": torchvision.transforms} pairs.\n                  List indexes or dict keys will be used for generating\n                  predictions names.\n            savedir: the directory to save predictions\n            prefix: the prefix for predictions files names\n        """"""\n        if isinstance(tfms, dict):\n            names = [f""{prefix}_{k}.npy"" for k in tfms]\n            tfms = tfms.values()\n        elif isinstance(tfms, list):\n            names = [f""{prefix}_{i}.npy"" for i in range(len(tfms))]\n        else:\n            raise ValueError(f""Transforms should be List or Dict, "" f""got {type(tfms)}"")\n\n        default_tfms = loader.dataset.transforms\n        for name, tfm in zip(names, tfms):\n            loader.dataset.transforms = tfm\n            savepath = Path(savedir) / name\n            self.predict_loader(loader, savepath)\n        loader.dataset.transforms = default_tfms\n\n    def save(self, savepath: Union[str, Path]) -> None:\n        """"""Save models state dict on the specified path.\n\n        Args:\n            savepath: the path to save the state dict.\n        """"""\n        savepath = Path(savepath)\n        savepath.parent.mkdir(exist_ok=True)\n        torch.save(self.state.core.model.state_dict(), savepath)\n\n    def load(self, loadpath: Union[str, Path], skip_wrong_shape: bool = False) -> None:\n        """"""Loads models state dict from the specified path.\n\n        Args:\n            loadpath: the path from which the state dict will be loaded.\n            skip_wrong_shape: If False, will raise an exception if checkpoints\n                weigths shape doesn\'t match models weights shape.\n                If True, will skip unmatched weights and load only matched.\n        """"""\n        loadpath = Path(loadpath)\n        checkpoint = torch.load(loadpath, map_location=lambda storage, loc: storage)\n\n        # workaround DataParallelModel\n        if (\n            not isinstance(self.state.core.model, DataParallelModel)\n            and ""module."" in list(checkpoint.keys())[0]\n        ):\n            # [7:] is to skip \'module.\' in group name\n            checkpoint = {k[7:]: v for k, v in checkpoint.items()}\n\n        load_state_dict(\n            model=self.state.core.model, state_dict=checkpoint, skip_wrong_shape=skip_wrong_shape,\n        )\n\n    def to_device(self, batch: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n        """"""Moves tensors in batch to self.device.\n\n        Args:\n            batch: the batch dict.\n\n        Returns:\n            The batch dict with tensors on self.device.\n        """"""\n        res = {}\n        for k, v in batch.items():\n            if isinstance(v, dict):\n                v = self.to_device(v)\n            else:\n                if hasattr(v, ""to""):\n                    v = v.to(self.device)\n            res[k] = v\n\n        return res\n\n    def to_fp16(self, **amp_params):\n        """"""Use NVIDIA apex library for mixed precision training.\n        After calling this method, all operations will be used in mixed precision.\n\n        Returns:\n            self\n        """"""\n        amp_params = dict({""opt_level"": ""O1"", ""verbosity"": 0}, **amp_params)\n        self.state.core.amp_params = amp_params\n        self.state.core.model = amp.initialize(self.state.core.model, **amp_params)\n        self.state.core.use_fp16 = True\n        return self\n\n    def set_mode(self, mode: str) -> None:\n        """"""Set the model to train or val and switch dataloaders\n\n        Args:\n            mode: \'train\', \'val\' or \'test\', the mode of training procedure.\n        """"""\n        if mode == ""train"":\n            self.state.core.model.train()\n            self.state.core.loader = self.state.core.dataowner.train_dl\n        elif mode == ""val"":\n            self.state.core.model.eval()\n            self.state.core.loader = self.state.core.dataowner.val_dl\n        elif mode == ""test"":\n            self.state.core.model.eval()\n            self.state.core.loader = self.state.core.dataowner.test_dl\n        self.state.core.mode = mode\n\n    def freeze_to(self, n: int, freeze_bn: bool = False, model_attr: Optional[str] = None) -> None:\n        """"""Freeze model or model\'s part till specified layer.\n\n        Args:\n            n: the layer number to freeze to\n            freeze_bn: if True batchnorm layers will be frozen too\n            model_attr: the name of the model attribute if you want to specify\n                when you want to freeze layers.\n                For examples see example ipynb\'s.\n        """"""\n\n        module = self.get_model_attr(model_attr)\n        freeze_to(module, n, freeze_bn)\n\n    def freeze(self, freeze_bn: bool = False, model_attr: Optional[str] = None) -> None:\n        """"""Freeze model or model\'s part till the last layer\n\n        Args:\n            freeze_bn: if True batchnorm layers will be frozen too\n            model_attr: the name of the model attribute if you want to specify\n                when you want to freeze layers.\n                For examples see example ipynb\'s.\n        """"""\n        module = self.get_model_attr(model_attr)\n        freeze(module, freeze_bn)\n\n    def unfreeze(self, model_attr: Optional[str] = None) -> None:\n        """"""Unfreeze all model or model\'s part layers.\n\n        Args:\n            model_attr: the name of the model attribute if you want to specify\n                when you want to freeze layers.\n                For examples see example ipynb\'s.\n        """"""\n        module = self.get_model_attr(model_attr)\n        unfreeze(module)\n\n    def get_model_attr(self, model_attr: Union[str, None]) -> torch.nn.Module:\n        """"""Get models attribute by name or return the model if name is None.\n\n        Args:\n            model_attr: models attribute name to get. If none, than the model\n                will be returned.\n\n        Returns:\n            The models attribute or the model itself.\n        """"""\n        if self.state.core.parallel:\n            model = self.state.core.model.module\n        else:\n            model = self.state.core.model\n\n        if model_attr is not None:\n            module = getattr(model, model_attr)\n        else:\n            module = model\n        return module\n\n    def add_callbacks(self, callbacks: List[Callback]) -> None:\n        """"""Add callbacks to the beginning of self.callbacks""""""\n        self.callbacks = Callbacks(callbacks + self.callbacks.callbacks)\n\n    @staticmethod\n    def plot_kek(\n        logdir: Union[str, Path],\n        step: Optional[str] = ""batch"",\n        metrics: Optional[List[str]] = None,\n        height: Optional[int] = None,\n        width: Optional[int] = None,\n    ) -> None:\n        """"""Plots your keks results in Jupyter Notebook.\n\n        Args:\n            logdir: the logdir that was specified during kek.\n            step: \'batch\' or \'epoch\' - what logs to show: for batches or\n                for epochs\n            metrics: list of metrics to plot. The loss should be specified as\n                \'loss\', learning rate = \'lr\' and other metrics should be\n                specified as names in metrics dict that was specified during kek\n            height: the height of the whole resulting plot\n            width: the width of the whole resulting plot\n\n        """"""\n        assert step in [""batch"", ""epoch""], (\n            f""Step should be either \'batch\' or"" f""\'epoch\', got \'{step}\'""\n        )\n        plot_tensorboard_log(logdir, step, metrics, height, width)\n\n    @staticmethod\n    def plot_kek_lr(\n        logdir: Union[str, Path], height: Optional[int] = None, width: Optional[int] = None,\n    ) -> None:\n        """"""Plots learing rate finding results in Jupyter Notebook\n        Args:\n            logdir: the logdir that was specified during kek_lr.\n            height: the height of the whole resulting plot\n            width: the width of the whole resulting plot\n        """"""\n        step = ""batch""\n        metrics = [""loss"", ""lr""]\n        plot_tensorboard_log(logdir, step, metrics, height, width)\n\n    @property\n    def is_train(self) -> bool:\n        return self.state.core.mode == ""train""\n'"
kekas/loss.py,6,"b'from typing import List, Tuple, Union\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\n\nclass FocalLoss(nn.Module):\n    def __init__(\n        self, alpha=Union[Tuple[float, int], List], gamma: int = 0, size_average: bool = True,\n    ) -> None:\n        super(FocalLoss, self).__init__()\n        self.gamma = gamma\n        self.alpha = alpha\n        if isinstance(alpha, (float, int)):\n            self.alpha = torch.Tensor([alpha, 1 - alpha])\n        if isinstance(alpha, list):\n            self.alpha = torch.Tensor(alpha)\n        self.size_average = size_average\n\n    def forward(self, input: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n        if input.dim() > 2:\n            input = input.view(input.size(0), input.size(1), -1)  # N,C,H,W => N,C,H*W\n            input = input.transpose(1, 2)  # N,C,H*W => N,H*W,C\n            input = input.contiguous().view(-1, input.size(2))  # N,H*W,C => N*H*W,C\n        target = target.view(-1, 1)\n\n        logpt = F.log_softmax(input)\n        logpt = logpt.gather(1, target)\n        logpt = logpt.view(-1)\n        pt = Variable(logpt.data.exp())\n\n        if self.alpha is not None:\n            if self.alpha.type() != input.data.type():\n                self.alpha = self.alpha.type_as(input.data)\n            at = self.alpha.gather(0, target.data.view(-1))\n            logpt = logpt * Variable(at)\n\n        loss = -1 * (1 - pt) ** self.gamma * logpt\n        if self.size_average:\n            return loss.mean()\n        else:\n            return loss.sum()\n\n\n""""""MIT License\n\nCopyright (c) 2017 carwin\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the ""Software""), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.""""""\n'"
kekas/metrics.py,5,"b'import torch\nfrom sklearn.metrics import accuracy_score, roc_auc_score\n\n\ndef accuracy(preds: torch.Tensor, target: torch.Tensor) -> float:\n    target = target.cpu().detach().numpy()\n    preds = preds.cpu().detach().numpy().argmax(axis=1)\n    return accuracy_score(target, preds)\n\n\ndef bce_accuracy(preds: torch.Tensor, target: torch.Tensor, thresh: float = 0.5) -> float:\n    target = target.cpu().detach().numpy()\n    preds = (torch.sigmoid(preds).cpu().detach().numpy() > thresh).astype(int)\n    return accuracy_score(target, preds)\n\n\ndef roc_auc(preds: torch.Tensor, target: torch.Tensor) -> float:\n    target = target.cpu().detach().numpy()\n    preds = torch.sigmoid(preds).cpu().detach().numpy()\n    return roc_auc_score(target, preds)\n'"
kekas/modules.py,2,"b'from typing import Optional\n\nimport torch\nfrom torch import nn\n\n\nclass Flatten(nn.Module):\n    def forward(self, x):\n        return x.view(x.size()[0], -1)\n\n\n# https://github.com/fastai/fastai/blob/e8c855ac70d9d1968413a75c7e9a0f149d28cab3/fastai/layers.py#L171\nclass AdaptiveConcatPool2d(nn.Module):\n    ""Layer that concats `AdaptiveAvgPool2d` and `AdaptiveMaxPool2d`.""\n\n    def __init__(self, size: Optional[int] = None):\n        ""Output will be 2*size or 2 if size is None""\n        super().__init__()\n        size = size or 1\n        self.ap = nn.AdaptiveAvgPool2d(size)\n        self.mp = nn.AdaptiveMaxPool2d(size)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return torch.cat([self.mp(x), self.ap(x)], 1)\n'"
kekas/parallel.py,17,"b'import threading\n\nimport torch\nimport torch.cuda.comm as comm\nfrom torch.autograd import Function\nfrom torch.nn.modules import Module\nfrom torch.nn.parallel._functions import Broadcast\nfrom torch.nn.parallel.parallel_apply import parallel_apply\nfrom torch.nn.parallel.replicate import replicate\nfrom torch.nn.parallel.scatter_gather import scatter_kwargs\n\n__all__ = [""DataParallelModel"", ""DataParallelCriterion""]\n\n\nclass Reduce(Function):\n    @staticmethod\n    def forward(ctx, *inputs):\n        ctx.target_gpus = [inputs[i].get_device() for i in range(len(inputs))]\n        inputs = sorted(inputs, key=lambda i: i.get_device())\n        return comm.reduce_add(inputs)\n\n    @staticmethod\n    def backward(ctx, *gradOutput):\n        return Broadcast.apply(ctx.target_gpus, gradOutput[0])\n\n\nclass DataParallelModel(Module):\n    def __init__(self, module, device_ids=None, output_device=None, dim=0):\n        super(DataParallelModel, self).__init__()\n\n        if not torch.cuda.is_available():\n            self.module = module\n            self.device_ids = []\n            return\n\n        if device_ids is None:\n            device_ids = list(range(torch.cuda.device_count()))\n        if output_device is None:\n            output_device = device_ids[0]\n\n        self.dim = dim\n        self.module = module\n        self.device_ids = device_ids\n        self.output_device = output_device\n\n        if len(self.device_ids) == 1:\n            self.module.cuda(device_ids[0])\n\n    def forward(self, *inputs, **kwargs):\n        if not self.device_ids:\n            return self.module(*inputs, **kwargs)\n        inputs, kwargs = self.scatter(inputs, kwargs, self.device_ids)\n        if len(self.device_ids) == 1:\n            return self.module(*inputs[0], **kwargs[0])\n        replicas = self.replicate(self.module, self.device_ids[: len(inputs)])\n        outputs = self.parallel_apply(replicas, inputs, kwargs)\n        return outputs\n\n    @staticmethod\n    def replicate(module, device_ids):\n        return replicate(module, device_ids)\n\n    def scatter(self, inputs, kwargs, device_ids):\n        return scatter_kwargs(inputs, kwargs, device_ids, dim=self.dim)\n\n    def parallel_apply(self, replicas, inputs, kwargs):\n        return parallel_apply(replicas, inputs, kwargs, self.device_ids[: len(replicas)])\n\n\nclass DataParallelCriterion(Module):\n    def __init__(self, module, device_ids=None, output_device=None, dim=0):\n        super(DataParallelCriterion, self).__init__()\n\n        if not torch.cuda.is_available():\n            self.module = module\n            self.device_ids = []\n            return\n\n        if device_ids is None:\n            device_ids = list(range(torch.cuda.device_count()))\n        if output_device is None:\n            output_device = device_ids[0]\n\n        self.dim = dim\n        self.module = module\n        self.device_ids = device_ids\n        self.output_device = output_device\n\n        if len(self.device_ids) == 1:\n            self.module.cuda(device_ids[0])\n\n    def forward(self, inputs, *targets, **kwargs):\n        # input should be already scattered\n        # scattering the targets instead\n        targets, kwargs = self.scatter(targets, kwargs, self.device_ids)\n        if len(self.device_ids) == 1:\n            return self.module(inputs, *targets[0], **kwargs[0])\n        inputs = [(input,) for input in inputs]\n        replicas = self.replicate(self.module, self.device_ids[: len(inputs)])\n        outputs = self.parallel_apply(replicas, inputs, targets, kwargs)\n        return self.gather(outputs, self.output_device)\n\n    @staticmethod\n    def replicate(module, device_ids):\n        return replicate(module, device_ids)\n\n    def scatter(self, inputs, kwargs, device_ids):\n        return scatter_kwargs(inputs, kwargs, device_ids, dim=self.dim)\n\n    def parallel_apply(self, replicas, inputs, targets, kwargs):\n        return criterion_parallel_apply(\n            replicas, inputs, targets, kwargs, self.device_ids[: len(replicas)]\n        )\n\n    @staticmethod\n    def gather(outputs, output_device):\n        if not len(outputs[0].shape):\n            outputs = [output.unsqueeze(0) for output in outputs]\n        return Reduce.apply(*outputs) / len(outputs)\n\n\ndef criterion_parallel_apply(modules, inputs, targets, kwargs_tup=None, devices=None):\n    assert len(modules) == len(inputs)\n    assert len(targets) == len(inputs)\n    if kwargs_tup is not None:\n        assert len(modules) == len(kwargs_tup)\n    else:\n        kwargs_tup = ({},) * len(modules)\n    if devices is not None:\n        assert len(modules) == len(devices)\n    else:\n        devices = [None] * len(modules)\n\n    lock = threading.Lock()\n    results = {}\n    grad_enabled = torch.is_grad_enabled()\n\n    def _worker(i, module, input, target, kwargs, device=None):\n        torch.set_grad_enabled(grad_enabled)\n        if device is None:\n            device = get_a_var(input).get_device()\n        try:\n            with torch.cuda.device(device):\n                output = module(*input, *target, **kwargs)\n            with lock:\n                results[i] = output\n        except Exception as e:\n            with lock:\n                results[i] = e\n\n    if len(modules) > 1:\n        threads = [\n            threading.Thread(target=_worker, args=(i, module, input, target, kwargs, device),)\n            for i, (module, input, target, kwargs, device) in enumerate(\n                zip(modules, inputs, targets, kwargs_tup, devices)\n            )\n        ]\n\n        for thread in threads:\n            thread.start()\n        for thread in threads:\n            thread.join()\n    else:\n        _worker(0, modules[0], inputs[0], targets[0], kwargs_tup[0], devices[0])\n\n    outputs = []\n    for i in range(len(inputs)):\n        output = results[i]\n        if isinstance(output, Exception):\n            raise output\n        outputs.append(output)\n    return outputs\n\n\ndef get_a_var(obj):\n    if isinstance(obj, torch.tensor):\n        return obj\n\n    if isinstance(obj, list) or isinstance(obj, tuple):\n        for result in map(get_a_var, obj):\n            if isinstance(result, torch.tensor):\n                return result\n    if isinstance(obj, dict):\n        for result in map(get_a_var, obj.items()):\n            if isinstance(result, torch.tensor):\n                return result\n    return None\n'"
kekas/transformations.py,2,"b'from typing import Callable, Dict, Tuple\n\nimport numpy as np\nimport torch\nfrom torchvision.transforms import Normalize\n\nIMAGENET_STATS = (0.485, 0.456, 0.406), (0.229, 0.224, 0.225)\n\n\nclass Transformer:\n    def __init__(self, key: str, transform_fn: Callable) -> None:\n        self.key = key\n        self.transform_fn = transform_fn\n\n    def __call__(self, datum: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n        datum = datum\n        datum[self.key] = self.transform_fn(datum[self.key])\n        return datum\n\n\ndef to_torch(scale_factor: float = 255.0) -> Callable:\n    return lambda x: torch.from_numpy(x.astype(np.float32) / scale_factor).permute(2, 0, 1)\n\n\ndef normalize(stats: Tuple = IMAGENET_STATS) -> Normalize:\n    return Normalize(*stats)\n'"
kekas/utils.py,7,"b'import logging\nimport sys\nfrom functools import reduce\nfrom pathlib import Path\nfrom pdb import set_trace as st\nfrom typing import Any, Dict, Hashable, List, Optional, Union\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\n\nimport plotly.graph_objs as go\nfrom plotly.offline import init_notebook_mode, iplot\nfrom tensorboard.backend.event_processing.event_accumulator import EventAccumulator, ScalarEvent\nfrom tqdm import tqdm\n\nlogging.getLogger(""tensorflow"").addFilter(lambda x: 0)\n\nBN_TYPES = (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d)\n\n\ndef freeze_to(module: nn.Module, n: int, freeze_bn: bool = False) -> None:\n    layers = list(module.children())\n    for l in layers[:n]:\n        for module in flatten_layer(l):\n            if freeze_bn or not isinstance(module, BN_TYPES):\n                set_grad(module, requires_grad=False)\n\n    for l in layers[n:]:\n        for module in flatten_layer(l):\n            set_grad(module, requires_grad=True)\n\n\ndef freeze(module: nn.Module, freeze_bn: bool = False) -> None:\n    freeze_to(module=module, n=-1, freeze_bn=freeze_bn)\n\n\ndef unfreeze(module: nn.Module) -> None:\n    layers = list(module.children())\n    for l in layers:\n        for module in flatten_layer(l):\n            set_grad(module, requires_grad=True)\n\n\ndef set_grad(module: nn.Module, requires_grad: bool) -> None:\n    for param in module.parameters():\n        param.requires_grad = requires_grad\n\n\n# https://github.com/fastai/fastai/blob/6778fd518e95ea8e1ce1e31a2f96590ee254542c/fastai/torch_core.py#L157\nclass ParameterModule(nn.Module):\n    """"""Register a lone parameter `p` in a module.""""""\n\n    def __init__(self, p: nn.Parameter):\n        super().__init__()\n        self.val = p\n\n    def forward(self, x):\n        return x\n\n\n# https://github.com/fastai/fastai/blob/6778fd518e95ea8e1ce1e31a2f96590ee254542c/fastai/torch_core.py#L149\ndef children_and_parameters(m: nn.Module):\n    """"""Return the children of `m` and its direct parameters not registered in modules.""""""\n    children = list(m.children())\n    children_p = sum([[id(p) for p in c.parameters()] for c in m.children()], [])\n    for p in m.parameters():\n        if id(p) not in children_p:\n            st()\n            children.append(ParameterModule(p))\n    return children\n\n\ndef flatten_layer(layer: nn.Module) -> List[nn.Module]:\n    if len(list(layer.children())):\n        layers = []\n        for children in children_and_parameters(layer):\n            layers += flatten_layer(children)\n        return layers\n    else:\n        return [layer]\n\n\ndef to_numpy(data: torch.Tensor) -> np.ndarray:\n    return data.detach().cpu().numpy()\n\n\ndef exp_weight_average(\n    curr_val: Union[float, torch.Tensor], prev_val: float, alpha: float = 0.9\n) -> float:\n    if isinstance(curr_val, torch.Tensor):\n        curr_val = to_numpy(curr_val)\n    return float(alpha * prev_val + (1 - alpha) * curr_val)\n\n\ndef get_pbar(dataloader: DataLoader, description: str) -> tqdm:\n\n    pbar = tqdm(total=len(dataloader), leave=True, ncols=0, desc=description, file=sys.stdout)\n\n    return pbar\n\n\ndef extend_postfix(postfix: str, dct: Dict) -> str:\n    if postfix is None:\n        postfix = """"\n    postfixes = [postfix] + [f""{k}={v:.4f}"" for k, v in dct.items()]\n    return "", "".join(postfixes)\n\n\ndef get_opt_lr(opt: torch.optim.Optimizer) -> float:\n    lrs = [pg[""lr""] for pg in opt.param_groups]\n    res = reduce(lambda x, y: x + y, lrs) / len(lrs)\n    return res\n\n\nclass DotDict(dict):\n    """"""\n    Example:\n    m = Map({\'first_name\': \'Eduardo\'}, last_name=\'Pool\', age=24, sports=[\'Soccer\'])\n    """"""\n\n    def __init__(self, *args, **kwargs) -> None:\n        super().__init__(*args, **kwargs)\n        for arg in args:\n            if isinstance(arg, dict):\n                for k, v in arg.items():\n                    self[k] = v\n\n        if kwargs:\n            for k, v in kwargs.items():\n                self[k] = v\n\n    def __getattr__(self, attr: str) -> Any:\n        return self.get(attr)\n\n    def __setattr__(self, key: Hashable, value: Any) -> Any:\n        self.__setitem__(key, value)\n\n    def __setitem__(self, key: Hashable, value: Any) -> Any:\n        super().__setitem__(key, value)\n        self.__dict__.update({key: value})\n\n    def __delattr__(self, item: str) -> None:\n        self.__delitem__(item)\n\n    def __delitem__(self, key: str) -> None:\n        super().__delitem__(key)\n        del self.__dict__[key]\n\n\ndef load_state_dict(model: torch.nn.Module, state_dict: Dict, skip_wrong_shape: bool = False):\n    model_state_dict = model.state_dict()\n\n    for key in state_dict:\n        if key in model_state_dict:\n            if model_state_dict[key].shape == state_dict[key].shape:\n                model_state_dict[key] = state_dict[key]\n            elif not skip_wrong_shape:\n                m = (\n                    f""Shapes of the \'{key}\' parameters do not match: ""\n                    f""{model_state_dict[key].shape} vs {state_dict[key].shape}""\n                )\n                raise Exception(m)\n\n    model.load_state_dict(model_state_dict)\n\n\ndef get_tensorboard_scalars(\n    logdir: str, metrics: Optional[List[str]], step: str\n) -> Dict[str, List]:\n    event_acc = EventAccumulator(str(logdir))\n    event_acc.Reload()\n\n    if metrics is not None:\n        scalar_names = [\n            n for n in event_acc.Tags()[""scalars""] if step in n and any(m in n for m in metrics)\n        ]\n    else:\n        scalar_names = [n for n in event_acc.Tags()[""scalars""] if step in n]\n\n    scalars = {sn: event_acc.Scalars(sn) for sn in scalar_names}\n    return scalars\n\n\ndef get_scatter(scalars: Dict[str, ScalarEvent], name: str, prefix: str) -> go.Scatter:\n    xs = [s.step for s in scalars[name]]\n    ys = [s.value for s in scalars[name]]\n\n    return go.Scatter(x=xs, y=ys, name=prefix + name)\n\n\ndef plot_tensorboard_log(\n    logdir: Union[str, Path],\n    step: Optional[str] = ""batch"",\n    metrics: Optional[List[str]] = None,\n    height: Optional[int] = None,\n    width: Optional[int] = None,\n) -> None:\n    init_notebook_mode(connected=True)\n    logdir = Path(logdir)\n\n    train_scalars = get_tensorboard_scalars(logdir / ""train"", metrics, step)\n    val_scalars = get_tensorboard_scalars(logdir / ""val"", metrics, step)\n\n    if height is not None:\n        height = height // len(train_scalars)\n\n    for m in train_scalars:\n        tm = get_scatter(train_scalars, m, prefix=""train/"")\n        try:\n            vm = get_scatter(val_scalars, m, prefix=""val/"")\n            data = [tm, vm]\n        except Exception:\n            data = [tm]\n        layout = go.Layout(title=m, height=height, width=width, yaxis=dict(hoverformat="".6f""))\n        fig = go.Figure(data=data, layout=layout)\n        fig.show()\n'"
