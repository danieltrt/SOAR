file_path,api_count,code
src/appnp.py,10,"b'""""""Training an APPNP model.""""""\n\nimport random\nimport torch\nimport numpy as np\nfrom tqdm import trange\nfrom appnp_layer import APPNPModel\n\nclass APPNPTrainer(object):\n    """"""\n    Method to train PPNP/APPNP model.\n    """"""\n    def __init__(self, args, graph, features, target):\n        """"""\n        :param args: Arguments object.\n        :param graph: Networkx graph.\n        :param features: Feature matrix.\n        :param target: Target vector with labels.\n        """"""\n        self.device = torch.device(\'cuda\' if torch.cuda.is_available() else \'cpu\')\n        self.args = args\n        self.graph = graph\n        self.features = features\n        self.target = target\n        self.create_model()\n        self.train_test_split()\n        self.transfer_node_sets()\n        self.process_features()\n        self.transfer_features()\n\n    def create_model(self):\n        """"""\n        Defining a model and transfering it to GPU/CPU.\n        """"""\n        self.node_count = self.graph.number_of_nodes()\n        self.number_of_labels = np.max(self.target)+1\n        self.number_of_features = max([f for _, feats  in self.features.items() for f in feats])+1\n\n        self.model = APPNPModel(self.args,\n                                self.number_of_labels,\n                                self.number_of_features,\n                                self.graph,\n                                self.device)\n\n        self.model = self.model.to(self.device)\n\n    def train_test_split(self):\n        """"""\n        Creating a train/test split.\n        """"""\n        random.seed(self.args.seed)\n        nodes = [node for node in range(self.node_count)]\n        random.shuffle(nodes)\n        self.train_nodes = nodes[0:self.args.train_size]\n        self.test_nodes = nodes[self.args.train_size:self.args.train_size+self.args.test_size]\n        self.validation_nodes = nodes[self.args.train_size+self.args.test_size:]\n\n    def transfer_node_sets(self):\n        """"""\n        Transfering the node sets to the device.\n        """"""\n        self.train_nodes = torch.LongTensor(self.train_nodes).to(self.device)\n        self.test_nodes = torch.LongTensor(self.test_nodes).to(self.device)\n        self.validation_nodes = torch.LongTensor(self.validation_nodes).to(self.device)\n\n    def process_features(self):\n        """"""\n        Creating a sparse feature matrix and a vector for the target labels.\n        """"""\n        index_1 = [node for node in self.graph.nodes() for fet in self.features[node]]\n        index_2 = [fet for node in self.graph.nodes() for fet in self.features[node]]\n        values = [1.0/len(self.features[node]) for node in self.graph.nodes() for fet in self.features[node]]\n        self.feature_indices = torch.LongTensor([index_1, index_2])\n        self.feature_values = torch.FloatTensor(values)\n        self.target = torch.LongTensor(self.target)\n\n    def transfer_features(self):\n        """"""\n        Transfering the features and the target matrix to the device.\n        """"""\n        self.target = self.target.to(self.device)\n        self.feature_indices = self.feature_indices.to(self.device)\n        self.feature_values = self.feature_values.to(self.device)\n\n    def score(self, index_set):\n        """"""\n        Calculating the accuracy for a given node set.\n        :param index_set: Index of nodes to be included in calculation.\n        :parm acc: Accuracy score.\n        """"""\n        self.model.eval()\n        _, pred = self.model(self.feature_indices, self.feature_values).max(dim=1)\n        correct = pred[index_set].eq(self.target[index_set]).sum().item()\n        acc = correct / index_set.size()[0]\n        return acc\n\n    def do_a_step(self):\n        """"""\n        Doing an optimization step.\n        """"""\n        self.model.train()\n        self.optimizer.zero_grad()\n        prediction = self.model(self.feature_indices, self.feature_values)\n        loss = torch.nn.functional.nll_loss(prediction[self.train_nodes],\n                                            self.target[self.train_nodes])\n        loss = loss+(self.args.lambd/2)*(torch.sum(self.model.layer_2.weight_matrix**2))\n        loss.backward()\n        self.optimizer.step()\n\n    def train_neural_network(self):\n        """"""\n        Training a neural network.\n        """"""\n        print(""\\nTraining.\\n"")\n        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.args.learning_rate)\n        self.best_accuracy = 0\n        self.step_counter = 0\n        iterator = trange(self.args.epochs, desc=\'Validation accuracy: \', leave=True)\n        for _ in iterator:\n            self.do_a_step()\n            accuracy = self.score(self.validation_nodes)\n            iterator.set_description(""Validation accuracy: {:.4f}"".format(accuracy))\n            if accuracy >= self.best_accuracy:\n                self.best_accuracy = accuracy\n                self.test_accuracy = self.score(self.test_nodes)\n                self.step_counter = 0\n            else:\n                self.step_counter = self.step_counter + 1\n                if self.step_counter > self.args.early_stopping_rounds:\n                    iterator.close()\n                    break\n\n    def fit(self):\n        """"""\n        Fitting the network and calculating the test accuracy.\n        """"""\n        self.train_neural_network()\n        print(""\\nBreaking from training process because of early stopping.\\n"")\n        print(""Test accuracy: {:.4f}"".format(self.test_accuracy))\n'"
src/appnp_layer.py,19,"b'""""""APPNP and PPNP layers.""""""\n\nimport math\nimport torch\nfrom torch_sparse import spmm\nfrom utils import create_propagator_matrix\n\ndef uniform(size, tensor):\n    """"""\n    Uniform weight initialization.\n    :param size: Size of the tensor.\n    :param tensor: Tensor initialized.\n    """"""\n    stdv = 1.0 / math.sqrt(size)\n    if tensor is not None:\n        tensor.data.uniform_(-stdv, stdv)\n\nclass DenseFullyConnected(torch.nn.Module):\n    """"""\n    Abstract class for PageRank and Approximate PageRank networks.\n    :param in_channels: Number of input channels.\n    :param out_channels: Number of output channels.\n    :param density: Feature matrix structure.\n    """"""\n    def __init__(self, in_channels, out_channels):\n        super(DenseFullyConnected, self).__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.define_parameters()\n        self.init_parameters()\n\n    def define_parameters(self):\n        """"""\n        Defining the weight matrices.\n        """"""\n        self.weight_matrix = torch.nn.Parameter(torch.Tensor(self.in_channels, self.out_channels))\n        self.bias = torch.nn.Parameter(torch.Tensor(self.out_channels))\n\n    def init_parameters(self):\n        """"""\n        Initializing weights.\n        """"""\n        torch.nn.init.xavier_uniform_(self.weight_matrix)\n        uniform(self.out_channels, self.bias)\n\n    def forward(self, features):\n        """"""\n        Doing a forward pass.\n        :param features: Feature matrix.\n        :return filtered_features: Convolved features.\n        """"""\n        filtered_features = torch.mm(features, self.weight_matrix)\n        filtered_features = filtered_features + self.bias\n        return filtered_features\n\nclass SparseFullyConnected(torch.nn.Module):\n    """"""\n    Abstract class for PageRank and Approximate PageRank networks.\n    :param in_channels: Number of input channels.\n    :param out_channels: Number of output channels.\n    :param density: Feature matrix structure.\n    """"""\n    def __init__(self, in_channels, out_channels):\n        super(SparseFullyConnected, self).__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.define_parameters()\n        self.init_parameters()\n\n    def define_parameters(self):\n        """"""\n        Defining the weight matrices.\n        """"""\n        self.weight_matrix = torch.nn.Parameter(torch.Tensor(self.in_channels, self.out_channels))\n        self.bias = torch.nn.Parameter(torch.Tensor(self.out_channels))\n\n    def init_parameters(self):\n        """"""\n        Initializing weights.\n        """"""\n        torch.nn.init.xavier_uniform_(self.weight_matrix)\n        uniform(self.out_channels, self.bias)\n\n    def forward(self, feature_indices, feature_values):\n        """"""\n        Making a forward pass.\n        :param feature_indices: Non zero value indices.\n        :param feature_values: Matrix values.\n        :return filtered_features: Output features.\n        """"""\n        number_of_nodes = torch.max(feature_indices[0]).item()+1\n        number_of_features = torch.max(feature_indices[1]).item()+1\n        filtered_features = spmm(index = feature_indices,\n                                 value = feature_values,\n                                 m = number_of_nodes,\n                                 n = number_of_features,\n                                 matrix = self.weight_matrix)\n        filtered_features = filtered_features + self.bias\n        return filtered_features\n\nclass APPNPModel(torch.nn.Module):\n    """"""\n    APPNP Model Class.\n    :param args: Arguments object.\n    :param number_of_labels: Number of target labels.\n    :param number_of_features: Number of input features.\n    :param graph: NetworkX graph.\n    :param device: CPU or GPU.\n    """"""\n    def __init__(self, args, number_of_labels, number_of_features, graph, device):\n        super(APPNPModel, self).__init__()\n        self.args = args\n        self.number_of_labels = number_of_labels\n        self.number_of_features = number_of_features\n        self.graph = graph\n        self.device = device\n        self.setup_layers()\n        self.setup_propagator()\n\n    def setup_layers(self):\n        """"""\n        Creating layers.\n        """"""\n        self.layer_1 = SparseFullyConnected(self.number_of_features, self.args.layers[0])\n        self.layer_2 = DenseFullyConnected(self.args.layers[1], self.number_of_labels)\n\n    def setup_propagator(self):\n        """"""\n        Defining propagation matrix (Personalized Pagrerank or adjacency).\n        """"""\n        self.propagator = create_propagator_matrix(self.graph, self.args.alpha, self.args.model)\n        if self.args.model == ""exact"":\n            self.propagator = self.propagator.to(self.device)\n        else:\n            self.edge_indices = self.propagator[""indices""].to(self.device)\n            self.edge_weights = self.propagator[""values""].to(self.device)\n\n    def forward(self, feature_indices, feature_values):\n        """"""\n        Making a forward propagation pass.\n        :param feature_indices: Feature indices for feature matrix.\n        :param feature_values: Values in the feature matrix.\n        :return self.predictions: Predicted class label log softmaxes.\n        """"""\n        feature_values = torch.nn.functional.dropout(feature_values,\n                                                     p=self.args.dropout,\n                                                     training=self.training)\n\n        latent_features_1 = self.layer_1(feature_indices, feature_values)\n\n        latent_features_1 = torch.nn.functional.relu(latent_features_1)\n\n        latent_features_1 = torch.nn.functional.dropout(latent_features_1,\n                                                        p=self.args.dropout,\n                                                        training=self.training)\n\n        latent_features_2 = self.layer_2(latent_features_1)\n        if self.args.model == ""exact"":\n            self.predictions = torch.nn.functional.dropout(self.propagator,\n                                                           p=self.args.dropout,\n                                                           training=self.training)\n\n            self.predictions = torch.mm(self.predictions, latent_features_2)\n        else:\n            localized_predictions = latent_features_2\n            edge_weights = torch.nn.functional.dropout(self.edge_weights,\n                                                       p=self.args.dropout,\n                                                       training=self.training)\n\n            for iteration in range(self.args.iterations):\n\n                new_features = spmm(index=self.edge_indices,\n                                    value=edge_weights,\n                                    n=localized_predictions.shape[0],\n                                    m=localized_predictions.shape[0],\n                                    matrix=localized_predictions)\n\n                localized_predictions = (1-self.args.alpha)*new_features\n                localized_predictions = localized_predictions + self.args.alpha*latent_features_2\n            self.predictions = localized_predictions\n        self.predictions = torch.nn.functional.log_softmax(self.predictions, dim=1)\n        return self.predictions\n'"
src/main.py,1,"b'"""""" Running the APPNP model.""""""\n\nimport torch\nfrom appnp import APPNPTrainer\nfrom param_parser import parameter_parser\nfrom utils import tab_printer, graph_reader, feature_reader, target_reader\n\ndef main():\n    """"""\n    Parsing command line parameters, reading data, fitting an APPNP/PPNP and scoring the model.\n    """"""\n    args = parameter_parser()\n    torch.manual_seed(args.seed)\n    tab_printer(args)\n    graph = graph_reader(args.edge_path)\n    features = feature_reader(args.features_path)\n    target = target_reader(args.target_path)\n    trainer = APPNPTrainer(args, graph, features, target)\n    trainer.fit()\n\nif __name__ == ""__main__"":\n    main()\n'"
src/param_parser.py,0,"b'""""""Getting the parameters from the commandline.""""""\n\nimport argparse\n\ndef parameter_parser():\n    """"""\n    A method to parse up command line parameters. By default it trains on the Cora dataset.\n    The default hyperparameters give a good quality representation without grid search.\n    """"""\n    parser = argparse.ArgumentParser(description=""Run PPNP/APPNP."")\n\n    parser.add_argument(""--edge-path"",\n                        nargs=""?"",\n                        default=""./input/cora_edges.csv"",\n\t                help=""Edge list csv."")\n\n    parser.add_argument(""--features-path"",\n                        nargs=""?"",\n                        default=""./input/cora_features.json"",\n\t                help=""Features json."")\n\n    parser.add_argument(""--target-path"",\n                        nargs=""?"",\n                        default=""./input/cora_target.csv"",\n\t                help=""Target classes csv."")\n\n    parser.add_argument(""--model"",\n                        nargs=""?"",\n                        default=""exact"",\n\t                help=""Model type."")\n\n    parser.add_argument(""--epochs"",\n                        type=int,\n                        default=2000,\n\t                help=""Number of training epochs. Default is 2000."")\n\n    parser.add_argument(""--seed"",\n                        type=int,\n                        default=42,\n\t                help=""Random seed for train-test split. Default is 42."")\n\n    parser.add_argument(""--iterations"",\n                        type=int,\n                        default=10,\n\t                help=""Number of Approximate Personalized PageRank iterations. Default is 10."")\n\n    parser.add_argument(""--early-stopping-rounds"",\n                        type=int,\n                        default=500,\n\t                help=""Number of training rounds before early stopping. Default is 10."")\n\n    parser.add_argument(""--train-size"",\n                        type=int,\n                        default=1500,\n\t                help=""Training set size. Default is 1500."")\n\n    parser.add_argument(""--test-size"",\n                        type=int,\n                        default=500,\n\t                help=""Test set size. Default is 500."")\n\n    parser.add_argument(""--dropout"",\n                        type=float,\n                        default=0.5,\n\t                help=""Dropout parameter. Default is 0.5."")\n\n    parser.add_argument(""--alpha"",\n                        type=float,\n                        default=0.1,\n\t                help=""Page rank teleport parameter. Default is 0.1."")\n\n    parser.add_argument(""--learning-rate"",\n                        type=float,\n                        default=0.01,\n\t                help=""Learning rate. Default is 0.01."")\n\n    parser.add_argument(""--lambd"",\n                        type=float,\n                        default=0.005,\n\t                help=""Weight matrix regularization. Default is 0.005."")\n\n    parser.add_argument(""--layers"",\n                        nargs=""+"",\n                        type=int,\n                        help=""Layer dimensions separated by space. E.g. 64 64."")\n\n    parser.set_defaults(layers=[64, 64])\n\n    return parser.parse_args()\n'"
src/utils.py,3,"b'""""""Utilities for data manipulation.""""""\n\nimport json\nimport torch\nimport numpy as np\nimport pandas as pd\nimport networkx as nx\nfrom scipy import sparse\nfrom texttable import Texttable\n\ndef tab_printer(args):\n    """"""\n    Function to print the logs in a nice tabular format.\n    :param args: Parameters used for the model.\n    """"""\n    args = vars(args)\n    keys = sorted(args.keys())\n    t = Texttable()\n    t.add_rows([[""Parameter"", ""Value""]] + [[k.replace(""_"", "" "").capitalize(), args[k]] for k in keys])\n    print(t.draw())\n\ndef graph_reader(path):\n    """"""\n    Function to read the graph from the path.\n    :param path: Path to the edge list.\n    :return graph: NetworkX object returned.\n    """"""\n    graph = nx.from_edgelist(pd.read_csv(path).values.tolist())\n    graph.remove_edges_from(nx.selfloop_edges(graph))\n    return graph\n\ndef feature_reader(path):\n    """"""\n    Reading the feature matrix stored as JSON from the disk.\n    :param path: Path to the JSON file.\n    :return out_features: Dict with index and value tensor.\n    """"""\n    features = json.load(open(path))\n    features = {int(k): [int(val) for val in v] for k, v in features.items()}\n    return features\n\ndef target_reader(path):\n    """"""\n    Reading the target vector from disk.\n    :param path: Path to the target.\n    :return target: Target vector.\n    """"""\n    target = np.array(pd.read_csv(path)[""target""])\n    return target\n\ndef create_adjacency_matrix(graph):\n    """"""\n    Creating a sparse adjacency matrix.\n    :param graph: NetworkX object.\n    :return A: Adjacency matrix.\n    """"""\n    index_1 = [edge[0] for edge in graph.edges()] + [edge[1] for edge in graph.edges()]\n    index_2 = [edge[1] for edge in graph.edges()] + [edge[0] for edge in graph.edges()]\n    values = [1 for edge in index_1]\n    node_count = max(max(index_1)+1, max(index_2)+1)\n    A = sparse.coo_matrix((values, (index_1, index_2)), shape=(node_count, node_count), dtype=np.float32)\n    return A\n\ndef normalize_adjacency_matrix(A, I):\n    """"""\n    Creating a normalized adjacency matrix with self loops.\n    :param A: Sparse adjacency matrix.\n    :param I: Identity matrix.\n    :return A_tile_hat: Normalized adjacency matrix.\n    """"""\n    A_tilde = A + I\n    degrees = A_tilde.sum(axis=0)[0].tolist()\n    D = sparse.diags(degrees, [0])\n    D = D.power(-0.5)\n    A_tilde_hat = D.dot(A_tilde).dot(D)\n    return A_tilde_hat\n\ndef create_propagator_matrix(graph, alpha, model):\n    """"""\n    Creating  apropagation matrix.\n    :param graph: NetworkX graph.\n    :param alpha: Teleport parameter.\n    :param model: Type of model exact or approximate.\n    :return propagator: Propagator matrix Dense torch matrix /\n    dict with indices and values for sparse multiplication.\n    """"""\n    A = create_adjacency_matrix(graph)\n    I = sparse.eye(A.shape[0])\n    A_tilde_hat = normalize_adjacency_matrix(A, I)\n    if model == ""exact"":\n        propagator = (I-(1-alpha)*A_tilde_hat).todense()\n        propagator = alpha*torch.inverse(torch.FloatTensor(propagator))\n    else:\n        propagator = dict()\n        A_tilde_hat = sparse.coo_matrix(A_tilde_hat)\n        indices = np.concatenate([A_tilde_hat.row.reshape(-1, 1), A_tilde_hat.col.reshape(-1, 1)], axis=1).T\n        propagator[""indices""] = torch.LongTensor(indices)\n        propagator[""values""] = torch.FloatTensor(A_tilde_hat.data)\n    return propagator\n'"
