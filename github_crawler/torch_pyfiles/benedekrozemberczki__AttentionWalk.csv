file_path,api_count,code
src/attentionwalk.py,18,"b'""""""AttentionWalk class.""""""\n\nimport torch\nimport numpy as np\nimport pandas as pd\nfrom tqdm import trange\nfrom utils import read_graph, feature_calculator, adjacency_opposite_calculator\n\nclass AttentionWalkLayer(torch.nn.Module):\n    """"""\n    Attention Walk Layer.\n    For details see the paper.\n    """"""\n    def __init__(self, args, shapes):\n        """"""\n        Setting up the layer.\n        :param args: Arguments object.\n        :param shapes: Shape of the target tensor.\n        """"""\n        super(AttentionWalkLayer, self).__init__()\n        self.args = args\n        self.shapes = shapes\n        self.define_weights()\n        self.initialize_weights()\n\n    def define_weights(self):\n        """"""\n        Define the model weights.\n        """"""\n        half_dim = int(self.args.dimensions/2)\n        self.left_factors = torch.nn.Parameter(torch.Tensor(self.shapes[1], half_dim))\n        self.right_factors = torch.nn.Parameter(torch.Tensor(half_dim, self.shapes[1]))\n        self.attention = torch.nn.Parameter(torch.Tensor(self.shapes[0], 1))\n\n    def initialize_weights(self):\n        """"""\n        Initializing the weights.\n        """"""\n        torch.nn.init.uniform_(self.left_factors, -0.01, 0.01)\n        torch.nn.init.uniform_(self.right_factors, -0.01, 0.01)\n        torch.nn.init.uniform_(self.attention, -0.01, 0.01)\n\n    def forward(self, weighted_target_tensor, adjacency_opposite):\n        """"""\n        Doing a forward propagation pass.\n        :param weighted_target_tensor: Target tensor factorized.\n        :param adjacency_opposite: No-edge indicator matrix.\n        :return loss: Loss being minimized.\n        """"""\n        self.attention_probs = torch.nn.functional.softmax(self.attention, dim=0)\n        probs = self.attention_probs.unsqueeze(1).expand_as(weighted_target_tensor)\n        weighted_target_tensor = weighted_target_tensor * probs\n        weighted_tar_mat = torch.sum(weighted_target_tensor, dim=0)\n        weighted_tar_mat = weighted_tar_mat.view(self.shapes[1], self.shapes[2])\n        estimate = torch.mm(self.left_factors, self.right_factors)\n        loss_on_target = - weighted_tar_mat* torch.log(torch.sigmoid(estimate))\n        loss_opposite = -adjacency_opposite * torch.log(1-torch.sigmoid(estimate))\n        loss_on_mat = self.args.num_of_walks*weighted_tar_mat.shape[0]*loss_on_target+loss_opposite\n        abs_loss_on_mat = torch.abs(loss_on_mat)\n        average_loss_on_mat = torch.mean(abs_loss_on_mat)\n        norms = torch.mean(torch.abs(self.left_factors))+torch.mean(torch.abs(self.right_factors))\n        loss_on_regularization = self.args.beta * (self.attention.norm(2)**2)\n        loss = average_loss_on_mat + loss_on_regularization + self.args.gamma*norms\n        return loss\n\nclass AttentionWalkTrainer(object):\n    """"""\n    Class for training the AttentionWalk model.\n    """"""\n    def __init__(self, args):\n        """"""\n        Initializing the training object.\n        :param args: Arguments object.\n        """"""\n        self.args = args\n        self.graph = read_graph(self.args.edge_path)\n        self.initialize_model_and_features()\n\n    def initialize_model_and_features(self):\n        """"""\n        Creating data tensors and factroization model.\n        """"""\n        self.target_tensor = feature_calculator(self.args, self.graph)\n        self.target_tensor = torch.FloatTensor(self.target_tensor)\n        self.adjacency_opposite = adjacency_opposite_calculator(self.graph)\n        self.adjacency_opposite = torch.FloatTensor(self.adjacency_opposite)\n        self.model = AttentionWalkLayer(self.args, self.target_tensor.shape)\n\n    def fit(self):\n        """"""\n        Fitting the model\n        """"""\n        print(""\\nTraining the model.\\n"")\n        self.model.train()\n        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.args.learning_rate)\n        self.epochs = trange(self.args.epochs, desc=""Loss"")\n        for _ in self.epochs:\n            self.optimizer.zero_grad()\n            loss = self.model(self.target_tensor, self.adjacency_opposite)\n            loss.backward()\n            self.optimizer.step()\n            self.epochs.set_description(""Attention Walk (Loss=%g)"" % round(loss.item(), 4))\n\n    def save_model(self):\n        """"""\n        Saving the embedding and attention vector.\n        """"""\n        self.save_embedding()\n        self.save_attention()\n\n    def save_embedding(self):\n        """"""\n        Saving the embedding matrices as one unified embedding.\n        """"""\n        print(""\\nSaving the model.\\n"")\n        left = self.model.left_factors.detach().numpy()\n        right = self.model.right_factors.detach().numpy().T\n        indices = np.array([range(len(self.graph))]).reshape(-1, 1)\n        embedding = np.concatenate([indices, left, right], axis=1)\n        columns = [""id""] + [""x_"" + str(x) for x in range(self.args.dimensions)]\n        embedding = pd.DataFrame(embedding, columns=columns)\n        embedding.to_csv(self.args.embedding_path, index=None)\n\n    def save_attention(self):\n        """"""\n        Saving the attention vector.\n        """"""\n        attention = self.model.attention_probs.detach().numpy()\n        indices = np.array([range(self.args.window_size)]).reshape(-1, 1)\n        attention = np.concatenate([indices, attention], axis=1)\n        attention = pd.DataFrame(attention, columns=[""Order"", ""Weight""])\n        attention.to_csv(self.args.attention_path, index=None)\n'"
src/main.py,0,"b'""""""Run Attention Walk.""""""\n\nfrom param_parser import parameter_parser\nfrom utils import tab_printer\nfrom attentionwalk import AttentionWalkTrainer\n\ndef main():\n    """"""\n    Parsing command lines, creating target matrix.\n    Fitting an Attention Walker and saving the embedding.\n    """"""\n    args = parameter_parser()\n    tab_printer(args)\n    model = AttentionWalkTrainer(args)\n    model.fit()\n    model.save_model()\n\nif __name__ == ""__main__"":\n    main()\n'"
src/param_parser.py,0,"b'""""""Parameter parsing module.""""""\n\nimport argparse\n\ndef parameter_parser():\n    """"""\n    A method to parse up command line parameters.\n    By default it gives an embedding of the Wikipedia Chameleons dataset.\n    The default hyperparameters give a good quality representation without grid search.\n    Representations are sorted by node ID.\n    """"""\n    parser = argparse.ArgumentParser(description=""Run Attention Walk."")\n\n    parser.add_argument(""--edge-path"",\n                        nargs=""?"",\n                        default=""./input/chameleon_edges.csv"",\n\t                help=""Edge list csv."")\n\n    parser.add_argument(""--embedding-path"",\n                        nargs=""?"",\n                        default=""./output/chameleon_AW_embedding.csv"",\n\t                help=""Target embedding csv."")\n\n    parser.add_argument(""--attention-path"",\n                        nargs=""?"",\n                        default=""./output/chameleon_AW_attention.csv"",\n\t                help=""Attention vector csv."")\n\n    parser.add_argument(""--dimensions"",\n                        type=int,\n                        default=128,\n\t                help=""Number of dimensions. Default is 128."")\n\n    parser.add_argument(""--epochs"",\n                        type=int,\n                        default=200,\n\t                help=""Number of gradient descent iterations. Default is 200."")\n\n    parser.add_argument(""--window-size"",\n                        type=int,\n                        default=5,\n\t                help=""Skip-gram window size. Default is 5."")\n\n    parser.add_argument(""--num-of-walks"",\n                        type=int,\n                        default=80,\n\t                help=""Number of random walks. Default is 80."")\n\n    parser.add_argument(""--beta"",\n                        type=float,\n                        default=0.5,\n\t                help=""Regularization parameter. Default is 0.5."")\n\n    parser.add_argument(""--gamma"",\n                        type=float,\n                        default=0.5,\n\t                help=""Regularization parameter. Default is 0.5."")\n\n    parser.add_argument(""--learning-rate"",\n                        type=float,\n                        default=0.01,\n\t                help=""Gradient descent learning rate. Default is 0.01."")\n\n    return parser.parse_args()\n'"
src/utils.py,0,"b'""""""Reading and writing data.""""""\n\nimport numpy as np\nimport pandas as pd\nimport networkx as nx\nfrom tqdm import tqdm\nfrom scipy import sparse\nfrom texttable import Texttable\n\ndef read_graph(graph_path):\n    """"""\n    Method to read graph and create a target matrix with pooled adjacency matrix powers.\n    :param args: Arguments object.\n    :return graph: graph.\n    """"""\n    print(""\\nTarget matrix creation started.\\n"")\n    graph = nx.from_edgelist(pd.read_csv(graph_path).values.tolist())\n    graph.remove_edges_from(nx.selfloop_edges(graph))\n    return graph\n\ndef tab_printer(args):\n    """"""\n    Function to print the logs in a nice tabular format.\n    :param args: Parameters used for the model.\n    """"""\n    args = vars(args)\n    keys = sorted(args.keys())\n    t = Texttable()\n    t.add_rows([[""Parameter"", ""Value""]])\n    t.add_rows([[k.replace(""_"", "" "").capitalize(), args[k]] for k in keys])\n    print(t.draw())\n\ndef feature_calculator(args, graph):\n    """"""\n    Calculating the feature tensor.\n    :param args: Arguments object.\n    :param graph: NetworkX graph.\n    :return target_matrices: Target tensor.\n    """"""\n    index_1 = [edge[0] for edge in graph.edges()] + [edge[1] for edge in graph.edges()]\n    index_2 = [edge[1] for edge in graph.edges()] + [edge[0] for edge in graph.edges()]\n    values = [1 for edge in index_1]\n    node_count = max(max(index_1)+1, max(index_2)+1)\n    adjacency_matrix = sparse.coo_matrix((values, (index_1, index_2)),\n                                         shape=(node_count, node_count),\n                                         dtype=np.float32)\n\n    degrees = adjacency_matrix.sum(axis=0)[0].tolist()\n    degs = sparse.diags(degrees, [0])\n    normalized_adjacency_matrix = degs.dot(adjacency_matrix)\n    target_matrices = [normalized_adjacency_matrix.todense()]\n    powered_A = normalized_adjacency_matrix\n    if args.window_size > 1:\n        for power in tqdm(range(args.window_size-1), desc=""Adjacency matrix powers""):\n            powered_A = powered_A.dot(normalized_adjacency_matrix)\n            to_add = powered_A.todense()\n            target_matrices.append(to_add)\n    target_matrices = np.array(target_matrices)\n    return target_matrices\n\ndef adjacency_opposite_calculator(graph):\n    """"""\n    Creating no edge indicator matrix.\n    :param graph: NetworkX object.\n    :return adjacency_matrix_opposite: Indicator matrix.\n    """"""\n    adjacency_matrix = sparse.csr_matrix(nx.adjacency_matrix(graph), dtype=np.float32).todense()\n    adjacency_matrix_opposite = np.ones(adjacency_matrix.shape) - adjacency_matrix\n    return adjacency_matrix_opposite\n'"
