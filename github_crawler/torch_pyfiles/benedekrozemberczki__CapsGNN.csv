file_path,api_count,code
src/capsgnn.py,18,"b'""""""CapsGNN Trainer.""""""\n\nimport glob\nimport json\nimport random\nimport torch\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm, trange\nfrom torch_geometric.nn import GCNConv\nfrom utils import create_numeric_mapping\nfrom layers import ListModule, PrimaryCapsuleLayer, Attention, SecondaryCapsuleLayer\nfrom layers import margin_loss\n\nclass CapsGNN(torch.nn.Module):\n    """"""\n    An implementation of themodel described in the following paper:\n    https://openreview.net/forum?id=Byl8BnRcYm\n    """"""\n    def __init__(self, args, number_of_features, number_of_targets):\n        super(CapsGNN, self).__init__()\n        """"""\n        :param args: Arguments object.\n        :param number_of_features: Number of vertex features.\n        :param number_of_targets: Number of classes.\n        """"""\n        self.args = args\n        self.number_of_features = number_of_features\n        self.number_of_targets = number_of_targets\n        self._setup_layers()\n\n    def _setup_base_layers(self):\n        """"""\n        Creating GCN layers.\n        """"""\n        self.base_layers = [GCNConv(self.number_of_features, self.args.gcn_filters)]\n        for _ in range(self.args.gcn_layers-1):\n            self.base_layers.append(GCNConv(self.args.gcn_filters, self.args.gcn_filters))\n        self.base_layers = ListModule(*self.base_layers)\n\n    def _setup_primary_capsules(self):\n        """"""\n        Creating primary capsules.\n        """"""\n        self.first_capsule = PrimaryCapsuleLayer(in_units=self.args.gcn_filters,\n                                                 in_channels=self.args.gcn_layers,\n                                                 num_units=self.args.gcn_layers,\n                                                 capsule_dimensions=self.args.capsule_dimensions)\n\n    def _setup_attention(self):\n        """"""\n        Creating attention layer.\n        """"""\n        self.attention = Attention(self.args.gcn_layers*self.args.capsule_dimensions,\n                                   self.args.inner_attention_dimension)\n\n    def _setup_graph_capsules(self):\n        """"""\n        Creating graph capsules.\n        """"""\n        self.graph_capsule = SecondaryCapsuleLayer(self.args.gcn_layers,\n                                                   self.args.capsule_dimensions,\n                                                   self.args.number_of_capsules,\n                                                   self.args.capsule_dimensions)\n\n    def _setup_class_capsule(self):\n        """"""\n        Creating class capsules.\n        """"""\n        self.class_capsule = SecondaryCapsuleLayer(self.args.capsule_dimensions,\n                                                   self.args.number_of_capsules,\n                                                   self.number_of_targets,\n                                                   self.args.capsule_dimensions)\n\n    def _setup_reconstruction_layers(self):\n        """"""\n        Creating histogram reconstruction layers.\n        """"""\n        self.reconstruction_layer_1 = torch.nn.Linear(self.number_of_targets*self.args.capsule_dimensions,\n                                                      int((self.number_of_features*2)/3))\n\n        self.reconstruction_layer_2 = torch.nn.Linear(int((self.number_of_features*2)/3),\n                                                      int((self.number_of_features*3)/2))\n\n        self.reconstruction_layer_3 = torch.nn.Linear(int((self.number_of_features*3)/2),\n                                                      self.number_of_features)\n\n    def _setup_layers(self):\n        """"""\n        Creating layers of model.\n        1. GCN layers.\n        2. Primary capsules.\n        3. Attention\n        4. Graph capsules.\n        5. Class capsules.\n        6. Reconstruction layers.\n        """"""\n        self._setup_base_layers()\n        self._setup_primary_capsules()\n        self._setup_attention()\n        self._setup_graph_capsules()\n        self._setup_class_capsule()\n        self._setup_reconstruction_layers()\n\n    def calculate_reconstruction_loss(self, capsule_input, features):\n        """"""\n        Calculating the reconstruction loss of the model.\n        :param capsule_input: Output of class capsule.\n        :param features: Feature matrix.\n        :return reconstrcution_loss: Loss of reconstruction.\n        """"""\n\n        v_mag = torch.sqrt((capsule_input**2).sum(dim=1))\n        _, v_max_index = v_mag.max(dim=0)\n        v_max_index = v_max_index.data\n\n        capsule_masked = torch.autograd.Variable(torch.zeros(capsule_input.size()))\n        capsule_masked[v_max_index, :] = capsule_input[v_max_index, :]\n        capsule_masked = capsule_masked.view(1, -1)\n\n        feature_counts = features.sum(dim=0)\n        feature_counts = feature_counts/feature_counts.sum()\n\n        reconstruction_output = torch.nn.functional.relu(self.reconstruction_layer_1(capsule_masked))\n        reconstruction_output = torch.nn.functional.relu(self.reconstruction_layer_2(reconstruction_output))\n        reconstruction_output = torch.softmax(self.reconstruction_layer_3(reconstruction_output), dim=1)\n        reconstruction_output = reconstruction_output.view(1, self.number_of_features)\n        reconstruction_loss = torch.sum((features-reconstruction_output)**2)\n        return reconstruction_loss\n\n    def forward(self, data):\n        """"""\n        Forward propagation pass.\n        :param data: Dictionary of tensors with features and edges.\n        :return class_capsule_output: Class capsule outputs.\n        """"""\n        features = data[""features""]\n        edges = data[""edges""]\n        hidden_representations = []\n\n        for layer in self.base_layers:\n            features = torch.nn.functional.relu(layer(features, edges))\n            hidden_representations.append(features)\n\n        hidden_representations = torch.cat(tuple(hidden_representations))\n        hidden_representations = hidden_representations.view(1, self.args.gcn_layers, self.args.gcn_filters, -1)\n        first_capsule_output = self.first_capsule(hidden_representations)\n        first_capsule_output = first_capsule_output.view(-1, self.args.gcn_layers*self.args.capsule_dimensions)\n        rescaled_capsule_output = self.attention(first_capsule_output)\n        rescaled_first_capsule_output = rescaled_capsule_output.view(-1, self.args.gcn_layers,\n                                                                     self.args.capsule_dimensions)\n        graph_capsule_output = self.graph_capsule(rescaled_first_capsule_output)\n        reshaped_graph_capsule_output = graph_capsule_output.view(-1, self.args.capsule_dimensions,\n                                                                  self.args.number_of_capsules)\n        class_capsule_output = self.class_capsule(reshaped_graph_capsule_output)\n        class_capsule_output = class_capsule_output.view(-1, self.number_of_targets*self.args.capsule_dimensions)\n        class_capsule_output = torch.mean(class_capsule_output, dim=0).view(1,\n                                                                            self.number_of_targets,\n                                                                            self.args.capsule_dimensions)\n        recon = class_capsule_output.view(self.number_of_targets, self.args.capsule_dimensions)\n        reconstruction_loss = self.calculate_reconstruction_loss(recon, data[""features""])\n        return class_capsule_output, reconstruction_loss\n\n\nclass CapsGNNTrainer(object):\n    """"""\n    CapsGNN training and scoring.\n    """"""\n    def __init__(self, args):\n        """"""\n        :param args: Arguments object.\n        """"""\n        self.args = args\n        self.setup_model()\n\n    def enumerate_unique_labels_and_targets(self):\n        """"""\n        Enumerating the features and targets in order to setup weights later.\n        """"""\n        print(""\\nEnumerating feature and target values.\\n"")\n        ending = ""*.json""\n\n        self.train_graph_paths = glob.glob(self.args.train_graph_folder+ending)\n        self.test_graph_paths = glob.glob(self.args.test_graph_folder+ending)\n        graph_paths = self.train_graph_paths + self.test_graph_paths\n\n        targets = set()\n        features = set()\n        for path in tqdm(graph_paths):\n            data = json.load(open(path))\n            targets = targets.union(set([data[""target""]]))\n            features = features.union(set(data[""labels""]))\n\n        self.target_map = create_numeric_mapping(targets)\n        self.feature_map = create_numeric_mapping(features)\n\n        self.number_of_features = len(self.feature_map)\n        self.number_of_targets = len(self.target_map)\n\n    def setup_model(self):\n        """"""\n        Enumerating labels and initializing a CapsGNN.\n        """"""\n        self.enumerate_unique_labels_and_targets()\n        self.model = CapsGNN(self.args, self.number_of_features, self.number_of_targets)\n\n    def create_batches(self):\n        """"""\n        Batching the graphs for training.\n        """"""\n        self.batches = []\n        for i in range(0, len(self.train_graph_paths), self.args.batch_size):\n            self.batches.append(self.train_graph_paths[i:i+self.args.batch_size])\n\n    def create_data_dictionary(self, target, edges, features):\n        """"""\n        Creating a data dictionary.\n        :param target: Target vector.\n        :param edges: Edge list tensor.\n        :param features: Feature tensor.\n        """"""\n        to_pass_forward = dict()\n        to_pass_forward[""target""] = target\n        to_pass_forward[""edges""] = edges\n        to_pass_forward[""features""] = features\n        return to_pass_forward\n\n    def create_target(self, data):\n        """"""\n        Target createn based on data dicionary.\n        :param data: Data dictionary.\n        :return : Target vector.\n        """"""\n        return  torch.FloatTensor([0.0 if i != data[""target""] else 1.0 for i in range(self.number_of_targets)])\n\n    def create_edges(self, data):\n        """"""\n        Create an edge matrix.\n        :param data: Data dictionary.\n        :return : Edge matrix.\n        """"""\n        edges = [[edge[0], edge[1]] for edge in data[""edges""]]\n        edges = edges + [[edge[1], edge[0]] for edge in data[""edges""]]\n        return torch.t(torch.LongTensor(edges))\n\n    def create_features(self, data):\n        """"""\n        Create feature matrix.\n        :param data: Data dictionary.\n        :return features: Matrix of features.\n        """"""\n        features = np.zeros((len(data[""labels""]), self.number_of_features))\n        node_indices = [node for node in range(len(data[""labels""]))]\n        feature_indices = [self.feature_map[label] for label in data[""labels""].values()]\n        features[node_indices, feature_indices] = 1.0\n        features = torch.FloatTensor(features)\n        return features\n\n    def create_input_data(self, path):\n        """"""\n        Creating tensors and a data dictionary with Torch tensors.\n        :param path: path to the data JSON.\n        :return to_pass_forward: Data dictionary.\n        """"""\n        data = json.load(open(path))\n        target = self.create_target(data)\n        edges = self.create_edges(data)\n        features = self.create_features(data)\n        to_pass_forward = self.create_data_dictionary(target, edges, features)\n        return to_pass_forward\n\n    def fit(self):\n        """"""\n        Training a model on the training set.\n        """"""\n        print(""\\nTraining started.\\n"")\n        self.model.train()\n        optimizer = torch.optim.Adam(self.model.parameters(),\n                                     lr=self.args.learning_rate,\n                                     weight_decay=self.args.weight_decay)\n\n        for _ in tqdm(range(self.args.epochs), desc=""Epochs: "", leave=True):\n            random.shuffle(self.train_graph_paths)\n            self.create_batches()\n            losses = 0\n            self.steps = trange(len(self.batches), desc=""Loss"")\n            for step in self.steps:\n                accumulated_losses = 0\n                optimizer.zero_grad()\n                batch = self.batches[step]\n                for path in batch:\n                    data = self.create_input_data(path)\n                    prediction, reconstruction_loss = self.model(data)\n                    loss = margin_loss(prediction,\n                                       data[""target""],\n                                       self.args.lambd)\n                    loss = loss+self.args.theta*reconstruction_loss\n                    accumulated_losses = accumulated_losses + loss\n                accumulated_losses = accumulated_losses/len(batch)\n                accumulated_losses.backward()\n                optimizer.step()\n                losses = losses + accumulated_losses.item()\n                average_loss = losses/(step + 1)\n                self.steps.set_description(""CapsGNN (Loss=%g)"" % round(average_loss, 4))\n\n    def score(self):\n        """"""\n        Scoring on the test set.\n        """"""\n        print(""\\n\\nScoring.\\n"")\n        self.model.eval()\n        self.predictions = []\n        self.hits = []\n        for path in tqdm(self.test_graph_paths):\n            data = self.create_input_data(path)\n            prediction, _ = self.model(data)\n            prediction_mag = torch.sqrt((prediction**2).sum(dim=2))\n            _, prediction_max_index = prediction_mag.max(dim=1)\n            prediction = prediction_max_index.data.view(-1).item()\n            self.predictions.append(prediction)\n            self.hits.append(data[""target""][prediction] == 1.0)\n\n        print(""\\nAccuracy: "" + str(round(np.mean(self.hits), 4)))\n\n    def save_predictions(self):\n        """"""\n        Saving the test set predictions.\n        """"""\n        identifiers = [path.split(""/"")[-1].strip("".json"") for path in self.test_graph_paths]\n        out = pd.DataFrame()\n        out[""id""] = identifiers\n        out[""predictions""] = self.predictions\n        out.to_csv(self.args.prediction_path, index=None)\n'"
src/layers.py,30,"b'""""""CapsGNN layers.""""""\n\nimport torch\nfrom torch.autograd import Variable\n\nclass ListModule(torch.nn.Module):\n    """"""\n    Abstract list layer class.\n    """"""\n    def __init__(self, *args):\n        """"""\n        Model initializing.\n        """"""\n        super(ListModule, self).__init__()\n        idx = 0\n        for module in args:\n            self.add_module(str(idx), module)\n            idx += 1\n\n    def __getitem__(self, idx):\n        """"""\n        Getting the indexed layer.\n        """"""\n        if idx < 0 or idx >= len(self._modules):\n            raise IndexError(\'index {} is out of range\'.format(idx))\n        it = iter(self._modules.values())\n        for _ in range(idx):\n            next(it)\n        return next(it)\n\n    def __iter__(self):\n        """"""\n        Iterating on the layers.\n        """"""\n        return iter(self._modules.values())\n\n    def __len__(self):\n        """"""\n        Number of layers.\n        """"""\n        return len(self._modules)\n\nclass PrimaryCapsuleLayer(torch.nn.Module):\n    """"""\n    Primary Convolutional Capsule Layer class based on:\n    https://github.com/timomernick/pytorch-capsule.\n    """"""\n    def __init__(self, in_units, in_channels, num_units, capsule_dimensions):\n        super(PrimaryCapsuleLayer, self).__init__()\n        """"""\n        :param in_units: Number of input units (GCN layers).\n        :param in_channels: Number of channels.\n        :param num_units: Number of capsules.\n        :param capsule_dimensions: Number of neurons in capsule.\n        """"""\n        self.num_units = num_units\n        self.units = []\n        for i in range(self.num_units):\n            unit = torch.nn.Conv1d(in_channels=in_channels,\n                                   out_channels=capsule_dimensions,\n                                   kernel_size=(in_units, 1),\n                                   stride=1,\n                                   bias=True)\n\n            self.add_module(""unit_"" + str(i), unit)\n            self.units.append(unit)\n\n    @staticmethod\n    def squash(s):\n        """"""\n        Squash activations.\n        :param s: Signal.\n        :return s: Activated signal.\n        """"""\n        mag_sq = torch.sum(s**2, dim=2, keepdim=True)\n        mag = torch.sqrt(mag_sq)\n        s = (mag_sq / (1.0 + mag_sq)) * (s / mag)\n        return s\n\n    def forward(self, x):\n        """"""\n        Forward propagation pass.\n        :param x: Input features.\n        :return : Primary capsule features.\n        """"""\n        u = [self.units[i](x) for i in range(self.num_units)]\n        u = torch.stack(u, dim=1)\n        u = u.view(x.size(0), self.num_units, -1)\n        return PrimaryCapsuleLayer.squash(u)\n\nclass SecondaryCapsuleLayer(torch.nn.Module):\n    """"""\n    Secondary Convolutional Capsule Layer class based on this repostory:\n    https://github.com/timomernick/pytorch-capsule\n    """"""\n    def __init__(self, in_units, in_channels, num_units, unit_size):\n        super(SecondaryCapsuleLayer, self).__init__()\n        """"""\n        :param in_units: Number of input units (GCN layers).\n        :param in_channels: Number of channels.\n        :param num_units: Number of capsules.\n        :param capsule_dimensions: Number of neurons in capsule.\n        """"""\n        self.in_units = in_units\n        self.in_channels = in_channels\n        self.num_units = num_units\n        self.W = torch.nn.Parameter(torch.randn(1, in_channels, num_units, unit_size, in_units))\n\n    @staticmethod\n    def squash(s):\n        """"""\n        Squash activations.\n        :param s: Signal.\n        :return s: Activated signal.\n        """"""\n        mag_sq = torch.sum(s**2, dim=2, keepdim=True)\n        mag = torch.sqrt(mag_sq)\n        s = (mag_sq / (1.0 + mag_sq)) * (s / mag)\n        return s\n\n    def forward(self, x):\n        """"""\n        Forward propagation pass.\n        :param x: Input features.\n        :return : Capsule output.\n        """"""\n        batch_size = x.size(0)\n        x = x.transpose(1, 2)\n        x = torch.stack([x] * self.num_units, dim=2).unsqueeze(4)\n        W = torch.cat([self.W] * batch_size, dim=0)\n        u_hat = torch.matmul(W, x)\n        b_ij = Variable(torch.zeros(1, self.in_channels, self.num_units, 1))\n\n        num_iterations = 3\n\n        for _ in range(num_iterations):\n            c_ij = torch.nn.functional.softmax(b_ij, dim=2)\n            c_ij = torch.cat([c_ij] * batch_size, dim=0).unsqueeze(4)\n            s_j = (c_ij * u_hat).sum(dim=1, keepdim=True)\n            v_j = SecondaryCapsuleLayer.squash(s_j)\n            v_j1 = torch.cat([v_j] * self.in_channels, dim=1)\n            u_vj1 = torch.matmul(u_hat.transpose(3, 4), v_j1).squeeze(4).mean(dim=0, keepdim=True)\n            b_max = torch.max(b_ij, dim = 2, keepdim = True)\n            b_ij = b_ij / b_max.values\n        return v_j.squeeze(1)\n\nclass Attention(torch.nn.Module):\n    """"""\n    2 Layer Attention Module.\n    See the CapsGNN paper for details.\n    """"""\n    def __init__(self, attention_size_1, attention_size_2):\n        super(Attention, self).__init__()\n        """"""\n        :param attention_size_1: Number of neurons in 1st attention layer.\n        :param attention_size_2: Number of neurons in 2nd attention layer.\n        """"""\n        self.attention_1 = torch.nn.Linear(attention_size_1, attention_size_2)\n        self.attention_2 = torch.nn.Linear(attention_size_2, attention_size_1)\n\n    def forward(self, x_in):\n        """"""\n        Forward propagation pass.\n        :param x_in: Primary capsule output.\n        :param condensed_x: Attention normalized capsule output.\n        """"""\n        attention_score_base = self.attention_1(x_in)\n        attention_score_base = torch.nn.functional.relu(attention_score_base)\n        attention_score = self.attention_2(attention_score_base)\n        attention_score = torch.nn.functional.softmax(attention_score, dim=0)\n        condensed_x = x_in *attention_score\n        return condensed_x\n\ndef margin_loss(scores, target, loss_lambda):\n    """"""\n    The margin loss from the original paper. Based on:\n    https://github.com/timomernick/pytorch-capsule\n    :param scores: Capsule scores.\n    :param target: Target groundtruth.\n    :param loss_lambda: Regularization parameter.\n    :return L_c: Classification loss.\n    """"""\n    scores = scores.squeeze()\n    v_mag = torch.sqrt((scores**2).sum(dim=1, keepdim=True))\n    zero = Variable(torch.zeros(1))\n    m_plus = 0.9\n    m_minus = 0.1\n    max_l = torch.max(m_plus - v_mag, zero).view(1, -1)**2\n    max_r = torch.max(v_mag - m_minus, zero).view(1, -1)**2\n    T_c = Variable(torch.zeros(v_mag.shape))\n    T_c = target\n    L_c = T_c * max_l + loss_lambda * (1.0 - T_c) * max_r\n    L_c = L_c.sum(dim=1)\n    L_c = L_c.mean()\n    return L_c\n'"
src/main.py,0,"b'""""""Running CapsGNN.""""""\n\nfrom utils import tab_printer\nfrom capsgnn import CapsGNNTrainer\nfrom param_parser import parameter_parser\n\ndef main():\n    """"""\n    Parsing command line parameters, processing graphs, fitting a CapsGNN.\n    """"""\n    args = parameter_parser()\n    tab_printer(args)\n    model = CapsGNNTrainer(args)\n    model.fit()\n    model.score()\n    model.save_predictions()\n\nif __name__ == ""__main__"":\n    main()\n'"
src/param_parser.py,0,"b'import argparse\n\ndef parameter_parser():\n    """"""\n    A method to parse up command line parameters. By default it learns on the Watts-Strogatz dataset.\n    The default hyperparameters give good results without cross-validation.\n    """"""\n    parser = argparse.ArgumentParser(description=""Run CapsGNN."")\n\t\n    parser.add_argument(""--train-graph-folder"",\n                        nargs=""?"",\n                        default=""./input/train/"",\n\t                help=""Training graphs folder."")\n\n    parser.add_argument(""--test-graph-folder"",\n                        nargs=""?"",\n                        default=""./input/test/"",\n\t                help=""Testing graphs folder."")\n\n    parser.add_argument(""--prediction-path"",\n                        nargs=""?"",\n                        default=""./output/watts_predictions.csv"",\n\t                help=""Path to store the predicted graph labels."")\n\n    parser.add_argument(""--epochs"",\n                        type=int,\n                        default=100,\n\t                help=""Number of training epochs. Default is 100."")\n\n    parser.add_argument(""--batch-size"",\n                        type=int,\n                        default=32,\n\t                help=""Number of graphs processed per batch. Default is 32."")\n\n    parser.add_argument(""--gcn-filters"",\n                        type=int,\n                        default=20,\n\t                help=""Number of Graph Convolutional filters. Default is 20."")\n\n    parser.add_argument(""--gcn-layers"",\n                        type=int,\n                        default=2,\n\t                help=""Number of Graph Convolutional Layers. Default is 2."")\n\n    parser.add_argument(""--inner-attention-dimension"",\n                        type=int,\n                        default=20,\n\t                help=""Number of Attention Neurons. Default is 20."")\n\n    parser.add_argument(""--capsule-dimensions"",\n                        type=int,\n                        default=8,\n\t                help=""Capsule dimensions. Default is 8."")\n\n    parser.add_argument(""--number-of-capsules"",\n                        type=int,\n                        default=8,\n\t                help=""Number of capsules per layer. Default is 8."")\n\n    parser.add_argument(""--weight-decay"",\n                        type=float,\n                        default=10**-6,\n\t                help=""Weight decay. Default is 10^-6."")\n\n    parser.add_argument(""--learning-rate"",\n                        type=float,\n                        default=0.01,\n\t                help=""Learning rate. Default is 0.01."")\n\n    parser.add_argument(""--lambd"",\n                        type=float,\n                        default=0.5,\n\t                help=""Loss combination weight. Default is 0.5."")\n\n    parser.add_argument(""--theta"",\n                        type=float,\n                        default=0.1,\n\t                help=""Reconstruction loss weight. Default is 0.1."")\n\n    return parser.parse_args()\n'"
src/utils.py,0,"b'""""""Data reading and printing utils.""""""\n\nfrom texttable import Texttable\n\ndef tab_printer(args):\n    """"""\n    Function to print the logs in a nice tabular format.\n    :param args: Parameters used for the model.\n    """"""\n    args = vars(args)\n    keys = sorted(args.keys())\n    t = Texttable() \n    t.add_rows([[""Parameter"", ""Value""]])\n    t.add_rows([[k.replace(""_"", "" "").capitalize(), args[k]] for k in keys])\n    print(t.draw())\n\ndef create_numeric_mapping(node_properties):\n    """"""\n    Create node feature map.\n    :param node_properties: List of features sorted.\n    :return : Feature numeric map.\n    """"""\n    return {value:i for i, value in enumerate(node_properties)}\n'"
