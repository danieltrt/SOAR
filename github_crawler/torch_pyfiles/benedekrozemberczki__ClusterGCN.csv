file_path,api_count,code
src/clustergcn.py,4,"b'import torch\nimport random\nimport numpy as np\nfrom tqdm import trange, tqdm\nfrom layers import StackedGCN\nfrom torch.autograd import Variable\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import accuracy_score\n\nclass ClusterGCNTrainer(object):\n    """"""\n    Training a ClusterGCN.\n    """"""\n    def __init__(self, args, clustering_machine):\n        """"""\n        :param ags: Arguments object.\n        :param clustering_machine:\n        """"""  \n        self.args = args\n        self.clustering_machine = clustering_machine\n        self.device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n        self.create_model()\n\n    def create_model(self):\n        """"""\n        Creating a StackedGCN and transferring to CPU/GPU.\n        """"""\n        self.model = StackedGCN(self.args, self.clustering_machine.feature_count, self.clustering_machine.class_count)\n        self.model = self.model.to(self.device)\n\n    def do_forward_pass(self, cluster):\n        """"""\n        Making a forward pass with data from a given partition.\n        :param cluster: Cluster index.\n        :return average_loss: Average loss on the cluster.\n        :return node_count: Number of nodes.\n        """"""\n        edges = self.clustering_machine.sg_edges[cluster].to(self.device)\n        macro_nodes = self.clustering_machine.sg_nodes[cluster].to(self.device)\n        train_nodes = self.clustering_machine.sg_train_nodes[cluster].to(self.device)\n        features = self.clustering_machine.sg_features[cluster].to(self.device)\n        target = self.clustering_machine.sg_targets[cluster].to(self.device).squeeze()\n        predictions = self.model(edges, features)\n        average_loss = torch.nn.functional.nll_loss(predictions[train_nodes], target[train_nodes])\n        node_count = train_nodes.shape[0]\n        return average_loss, node_count\n\n    def update_average_loss(self, batch_average_loss, node_count):\n        """"""\n        Updating the average loss in the epoch.\n        :param batch_average_loss: Loss of the cluster. \n        :param node_count: Number of nodes in currently processed cluster.\n        :return average_loss: Average loss in the epoch.\n        """"""\n        self.accumulated_training_loss = self.accumulated_training_loss + batch_average_loss.item()*node_count\n        self.node_count_seen = self.node_count_seen + node_count\n        average_loss = self.accumulated_training_loss/self.node_count_seen\n        return average_loss\n\n    def do_prediction(self, cluster):\n        """"""\n        Scoring a cluster.\n        :param cluster: Cluster index.\n        :return prediction: Prediction matrix with probabilities.\n        :return target: Target vector.\n        """"""\n        edges = self.clustering_machine.sg_edges[cluster].to(self.device)\n        macro_nodes = self.clustering_machine.sg_nodes[cluster].to(self.device)\n        test_nodes = self.clustering_machine.sg_test_nodes[cluster].to(self.device)\n        features = self.clustering_machine.sg_features[cluster].to(self.device)\n        target = self.clustering_machine.sg_targets[cluster].to(self.device).squeeze()\n        target = target[test_nodes]\n        prediction = self.model(edges, features)\n        prediction = prediction[test_nodes,:]\n        return prediction, target\n\n    def train(self):\n        """"""\n        Training a model.\n        """"""\n        print(""Training started.\\n"")\n        epochs = trange(self.args.epochs, desc = ""Train Loss"")\n        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.args.learning_rate)\n        self.model.train()\n        for epoch in epochs:\n            random.shuffle(self.clustering_machine.clusters)\n            self.node_count_seen = 0\n            self.accumulated_training_loss = 0\n            for cluster in self.clustering_machine.clusters:\n                self.optimizer.zero_grad()\n                batch_average_loss, node_count = self.do_forward_pass(cluster)\n                batch_average_loss.backward()\n                self.optimizer.step()\n                average_loss = self.update_average_loss(batch_average_loss, node_count)\n            epochs.set_description(""Train Loss: %g"" % round(average_loss,4))\n\n    def test(self):\n        """"""\n        Scoring the test and printing the F-1 score.\n        """"""\n        self.model.eval()\n        self.predictions = []\n        self.targets = []\n        for cluster in self.clustering_machine.clusters:\n            prediction, target = self.do_prediction(cluster)\n            self.predictions.append(prediction.cpu().detach().numpy())\n            self.targets.append(target.cpu().detach().numpy())\n        self.targets = np.concatenate(self.targets)\n        self.predictions = np.concatenate(self.predictions).argmax(1)\n        score = f1_score(self.targets, self.predictions, average=""micro"")\n        print(""\\nF-1 score: {:.4f}"".format(score))\n'"
src/clustering.py,6,"b'import metis\nimport torch\nimport random\nimport numpy as np\nimport networkx as nx\nfrom sklearn.model_selection import train_test_split\n\nclass ClusteringMachine(object):\n    """"""\n    Clustering the graph, feature set and target.\n    """"""\n    def __init__(self, args, graph, features, target):\n        """"""\n        :param args: Arguments object with parameters.\n        :param graph: Networkx Graph.\n        :param features: Feature matrix (ndarray).\n        :param target: Target vector (ndarray).\n        """"""\n        self.args = args\n        self.graph = graph\n        self.features = features\n        self.target = target\n        self._set_sizes()\n\n    def _set_sizes(self):\n        """"""\n        Setting the feature and class count.\n        """"""\n        self.feature_count = self.features.shape[1] \n        self.class_count = np.max(self.target)+1\n\n    def decompose(self):\n        """"""\n        Decomposing the graph, partitioning the features and target, creating Torch arrays.\n        """"""\n        if self.args.clustering_method == ""metis"":\n            print(""\\nMetis graph clustering started.\\n"")\n            self.metis_clustering()\n        else:\n            print(""\\nRandom graph clustering started.\\n"")\n            self.random_clustering()\n        self.general_data_partitioning()\n        self.transfer_edges_and_nodes()\n\n    def random_clustering(self):\n        """"""\n        Random clustering the nodes.\n        """"""\n        self.clusters = [cluster for cluster in range(self.args.cluster_number)]\n        self.cluster_membership = {node: random.choice(self.clusters) for node in self.graph.nodes()}\n\n    def metis_clustering(self):\n        """"""\n        Clustering the graph with Metis. For details see:\n        """"""\n        (st, parts) = metis.part_graph(self.graph, self.args.cluster_number)\n        self.clusters = list(set(parts))\n        self.cluster_membership = {node: membership for node, membership in enumerate(parts)}\n\n    def general_data_partitioning(self):\n        """"""\n        Creating data partitions and train-test splits.\n        """"""\n        self.sg_nodes = {}\n        self.sg_edges = {}\n        self.sg_train_nodes = {}\n        self.sg_test_nodes = {}\n        self.sg_features = {}\n        self.sg_targets = {}\n        for cluster in self.clusters:\n            subgraph = self.graph.subgraph([node for node in sorted(self.graph.nodes()) if self.cluster_membership[node] == cluster])\n            self.sg_nodes[cluster] = [node for node in sorted(subgraph.nodes())]\n            mapper = {node: i for i, node in enumerate(sorted(self.sg_nodes[cluster]))}\n            self.sg_edges[cluster] = [[mapper[edge[0]], mapper[edge[1]]] for edge in subgraph.edges()] +  [[mapper[edge[1]], mapper[edge[0]]] for edge in subgraph.edges()]\n            self.sg_train_nodes[cluster], self.sg_test_nodes[cluster] = train_test_split(list(mapper.values()), test_size = self.args.test_ratio)\n            self.sg_test_nodes[cluster] = sorted(self.sg_test_nodes[cluster])\n            self.sg_train_nodes[cluster] = sorted(self.sg_train_nodes[cluster])\n            self.sg_features[cluster] = self.features[self.sg_nodes[cluster],:]\n            self.sg_targets[cluster] = self.target[self.sg_nodes[cluster],:]\n\n    def transfer_edges_and_nodes(self):\n        """"""\n        Transfering the data to PyTorch format.\n        """"""\n        for cluster in self.clusters:\n            self.sg_nodes[cluster] = torch.LongTensor(self.sg_nodes[cluster])\n            self.sg_edges[cluster] = torch.LongTensor(self.sg_edges[cluster]).t()\n            self.sg_train_nodes[cluster] = torch.LongTensor(self.sg_train_nodes[cluster])\n            self.sg_test_nodes[cluster] = torch.LongTensor(self.sg_test_nodes[cluster])\n            self.sg_features[cluster] = torch.FloatTensor(self.sg_features[cluster])\n            self.sg_targets[cluster] = torch.LongTensor(self.sg_targets[cluster])\n'"
src/layers.py,5,"b'import torch\nfrom torch_geometric.nn import GCNConv\n\nclass StackedGCN(torch.nn.Module):\n    """"""\n    Multi-layer GCN model.\n    """"""\n    def __init__(self, args, input_channels, output_channels):\n        """"""\n        :param args: Arguments object.\n        :input_channels: Number of features.\n        :output_channels: Number of target features. \n        """"""\n        super(StackedGCN, self).__init__()\n        self.args = args\n        self.input_channels = input_channels\n        self.output_channels = output_channels\n        self.setup_layers()\n\n    def setup_layers(self):\n        """"""\n        Creating the layes based on the args.\n        """"""\n        self.layers = []\n        self.args.layers = [self.input_channels] + self.args.layers + [self.output_channels]\n        for i, _ in enumerate(self.args.layers[:-1]):\n            self.layers.append(GCNConv(self.args.layers[i],self.args.layers[i+1]))\n        self.layers = ListModule(*self.layers)\n\n    def forward(self, edges, features):\n        """"""\n        Making a forward pass.\n        :param edges: Edge list LongTensor.\n        :param features: Feature matrix input FLoatTensor.\n        :return predictions: Prediction matrix output FLoatTensor.\n        """"""\n        for i, _ in enumerate(self.args.layers[:-2]):\n            features = torch.nn.functional.relu(self.layers[i](features, edges))\n            if i>1:\n                features = torch.nn.functional.dropout(features, p = self.args.dropout, training = self.training)\n        features = self.layers[i+1](features, edges)\n        predictions = torch.nn.functional.log_softmax(features, dim=1)\n        return predictions\n\nclass ListModule(torch.nn.Module):\n    """"""\n    Abstract list layer class.\n    """"""\n    def __init__(self, *args):\n        """"""\n        Module initializing.\n        """"""\n        super(ListModule, self).__init__()\n        idx = 0\n        for module in args:\n            self.add_module(str(idx), module)\n            idx += 1\n\n    def __getitem__(self, idx):\n        """"""\n        Getting the indexed layer.\n        """"""\n        if idx < 0 or idx >= len(self._modules):\n            raise IndexError(\'index {} is out of range\'.format(idx))\n        it = iter(self._modules.values())\n        for i in range(idx):\n            next(it)\n        return next(it)\n\n    def __iter__(self):\n        """"""\n        Iterating on the layers.\n        """"""\n        return iter(self._modules.values())\n\n    def __len__(self):\n        """"""\n        Number of layers.\n        """"""\n        return len(self._modules)\n'"
src/main.py,1,"b'import torch\nfrom parser import parameter_parser\nfrom clustering import ClusteringMachine\nfrom clustergcn import ClusterGCNTrainer\nfrom utils import tab_printer, graph_reader, feature_reader, target_reader\n\ndef main():\n    """"""\n    Parsing command line parameters, reading data, graph decomposition, fitting a ClusterGCN and scoring the model.\n    """"""\n    args = parameter_parser()\n    torch.manual_seed(args.seed)\n    tab_printer(args)\n    graph = graph_reader(args.edge_path)\n    features = feature_reader(args.features_path)\n    target = target_reader(args.target_path)\n    clustering_machine = ClusteringMachine(args, graph, features, target)\n    clustering_machine.decompose()\n    gcn_trainer = ClusterGCNTrainer(args, clustering_machine)\n    gcn_trainer.train()\n    gcn_trainer.test()\n\nif __name__ == ""__main__"":\n    main()\n'"
src/parser.py,0,"b'import argparse\n\ndef parameter_parser():\n    """"""\n    A method to parse up command line parameters. By default it trains on the PubMed dataset.\n    The default hyperparameters give a good quality representation without grid search.\n    """"""\n    parser = argparse.ArgumentParser(description = ""Run ."")\n\n    parser.add_argument(""--edge-path"",\n                        nargs = ""?"",\n                        default = ""./input/edges.csv"",\n\t                help = ""Edge list csv."")\n\n    parser.add_argument(""--features-path"",\n                        nargs = ""?"",\n                        default = ""./input/features.csv"",\n\t                help = ""Features json."")\n\n    parser.add_argument(""--target-path"",\n                        nargs = ""?"",\n                        default = ""./input/target.csv"",\n\t                help = ""Target classes csv."")\n\n    parser.add_argument(""--clustering-method"",\n                        nargs = ""?"",\n                        default = ""metis"",\n\t                help = ""Clustering method for graph decomposition. Default is the metis procedure."")\n\n    parser.add_argument(""--epochs"",\n                        type = int,\n                        default = 200,\n\t                help = ""Number of training epochs. Default is 200."")\n\n    parser.add_argument(""--seed"",\n                        type = int,\n                        default = 42,\n\t                help = ""Random seed for train-test split. Default is 42."")\n\n    parser.add_argument(""--dropout"",\n                        type = float,\n                        default = 0.5,\n\t                help = ""Dropout parameter. Default is 0.5."")\n\n    parser.add_argument(""--learning-rate"",\n                        type = float,\n                        default = 0.01,\n\t                help = ""Learning rate. Default is 0.01."")\n\n    parser.add_argument(""--test-ratio"",\n                        type = float,\n                        default = 0.9,\n\t                help = ""Test data ratio. Default is 0.1."")\n\n    parser.add_argument(""--cluster-number"",\n                        type = int,\n                        default = 10,\n                        help = ""Number of clusters extracted. Default is 10."")\n\n    parser.set_defaults(layers = [16, 16, 16])\n    \n    return parser.parse_args()\n'"
src/utils.py,0,"b'import torch\nimport numpy as np\nimport pandas as pd\nimport networkx as nx\nfrom texttable import Texttable\nfrom scipy.sparse import coo_matrix\n\ndef tab_printer(args):\n    """"""\n    Function to print the logs in a nice tabular format.\n    :param args: Parameters used for the model.\n    """"""\n    args = vars(args)\n    keys = sorted(args.keys())\n    t = Texttable() \n    t.add_rows([[""Parameter"", ""Value""]] +  [[k.replace(""_"","" "").capitalize(),args[k]] for k in keys])\n    print(t.draw())\n\ndef graph_reader(path):\n    """"""\n    Function to read the graph from the path.\n    :param path: Path to the edge list.\n    :return graph: NetworkX object returned.\n    """"""\n    graph = nx.from_edgelist(pd.read_csv(path).values.tolist())\n    return graph\n\ndef feature_reader(path):\n    """"""\n    Reading the sparse feature matrix stored as csv from the disk.\n    :param path: Path to the csv file.\n    :return features: Dense matrix of features.\n    """"""\n    features = pd.read_csv(path)\n    node_index = features[""node_id""].values.tolist()\n    feature_index = features[""feature_id""].values.tolist()\n    feature_values = features[""value""].values.tolist()\n    node_count = max(node_index)+1\n    feature_count = max(feature_index)+1\n    features = coo_matrix((feature_values, (node_index, feature_index)), shape=(node_count, feature_count)).toarray()\n    return features\n\ndef target_reader(path):\n    """"""\n    Reading the target vector from disk.\n    :param path: Path to the target.\n    :return target: Target vector.\n    """"""\n    target = np.array(pd.read_csv(path)[""target""]).reshape(-1,1)\n    return target\n'"
