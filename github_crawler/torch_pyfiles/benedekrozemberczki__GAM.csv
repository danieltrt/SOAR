file_path,api_count,code
src/gam.py,31,"b'""""""Graph Attention Mechanism.""""""\n\nimport glob\nimport json\nimport torch\nimport random\nimport numpy as np\nimport pandas as pd\nimport networkx as nx\nfrom tqdm import tqdm, trange\nfrom utils import calculate_reward, calculate_predictive_loss\nfrom utils import read_node_labels, create_logs, create_features, create_batches\n\nclass StepNetworkLayer(torch.nn.Module):\n    """"""\n    Step Network Layer Class for selecting next node to move.\n    """"""\n    def __init__(self, args, identifiers):\n        """"""\n        Initializing the layer.\n        :param args: Arguments object.\n        :param identifiers: Node type -- id hash map.\n        """"""\n        super(StepNetworkLayer, self).__init__()\n        self.identifiers = identifiers\n        self.args = args\n        self.setup_attention()\n        self.create_parameters()\n\n    def setup_attention(self):\n        """"""\n        Initial attention generation with uniform attention scores.\n        """"""\n        self.attention = torch.ones((len(self.identifiers)))/len(self.identifiers)\n\n    def create_parameters(self):\n        """"""\n        Creating trainable weights and initlaizing them.\n        """"""\n        self.theta_step_1 = torch.nn.Parameter(torch.Tensor(len(self.identifiers),\n                                                            self.args.step_dimensions))\n\n        self.theta_step_2 = torch.nn.Parameter(torch.Tensor(len(self.identifiers),\n                                                            self.args.step_dimensions))\n\n        self.theta_step_3 = torch.nn.Parameter(torch.Tensor(2*self.args.step_dimensions,\n                                                            self.args.combined_dimensions))\n\n        torch.nn.init.uniform_(self.theta_step_1, -1, 1)\n        torch.nn.init.uniform_(self.theta_step_2, -1, 1)\n        torch.nn.init.uniform_(self.theta_step_3, -1, 1)\n\n    def sample_node_label(self, orig_neighbors, graph, features):\n        """"""\n        Sampling a label from the neighbourhood.\n        :param original_neighbors: Neighbours of the source node.\n        :param graph: NetworkX graph.\n        :param features: Node feature matrix.\n        :return label: Label sampled from the neighbourhood with attention.\n        """"""\n        neighbor_vector = torch.tensor([1.0 if n in orig_neighbors else 0.0 for n in graph.nodes()])\n        neighbor_features = torch.mm(neighbor_vector.view(1, -1), features)\n        attention_spread = self.attention * neighbor_features\n        normalized_attention_spread = attention_spread / attention_spread.sum()\n        normalized_attention_spread = normalized_attention_spread.detach().numpy().reshape(-1)\n        label = np.random.choice(np.arange(len(self.identifiers)), p=normalized_attention_spread)\n        return label\n\n    def make_step(self, node, graph, features, labels, inverse_labels):\n        """"""\n        :param node: Source node for step.\n        :param graph: NetworkX graph.\n        :param features: Feature matrix.\n        :param labels: Node labels hash table.\n        :param inverse_labels: Inverse node label hash table.\n        """"""\n        orig_neighbors = set(nx.neighbors(graph, node))\n        label = self.sample_node_label(orig_neighbors, graph, features)\n        labels = list(set(orig_neighbors).intersection(set(inverse_labels[str(label)])))\n        new_node = random.choice(labels)\n        new_node_attributes = torch.zeros((len(self.identifiers), 1))\n        new_node_attributes[label, 0] = 1.0\n        attention_score = self.attention[label]\n        return new_node_attributes, new_node, attention_score\n\n    def forward(self, data, graph, features, node):\n        """"""\n        Making a forward propagation step.\n        :param data: Data hash table.\n        :param graph: NetworkX graph object.\n        :param features: Feature matrix of the graph.\n        :param node: Base node where the step is taken from.\n        :return state: State vector.\n        :return node: New node to move to.\n        :return attention_score: Attention score of chosen node.\n        """"""\n        feature_row, node, attention_score = self.make_step(node, graph, features,\n                                                            data[""labels""], data[""inverse_labels""])\n\n        hidden_attention = torch.mm(self.attention.view(1, -1), self.theta_step_1)\n        hidden_node = torch.mm(torch.t(feature_row), self.theta_step_2)\n        combined_hidden_representation = torch.cat((hidden_attention, hidden_node), dim=1)\n        state = torch.mm(combined_hidden_representation, self.theta_step_3)\n        state = state.view(1, 1, self.args.combined_dimensions)\n        return state, node, attention_score\n\nclass DownStreamNetworkLayer(torch.nn.Module):\n    """"""\n    Neural network layer for attention update and node label assignment.\n    """"""\n    def __init__(self, args, target_number, identifiers):\n        """"""\n        :param args:\n        :param target_number:\n        :param identifiers:\n        """"""\n        super(DownStreamNetworkLayer, self).__init__()\n        self.args = args\n        self.target_number = target_number\n        self.identifiers = identifiers\n        self.create_parameters()\n\n    def create_parameters(self):\n        """"""\n        Defining and initializing the classification and attention update weights.\n        """"""\n        self.theta_classification = torch.nn.Parameter(torch.Tensor(self.args.combined_dimensions, self.target_number))\n        self.theta_rank = torch.nn.Parameter(torch.Tensor(self.args.combined_dimensions, len(self.identifiers)))\n        torch.nn.init.xavier_normal_(self.theta_classification)\n        torch.nn.init.xavier_normal_(self.theta_rank)\n\n    def forward(self, hidden_state):\n        """"""\n        Making a forward propagation pass with the input from the LSTM layer.\n        :param hidden_state: LSTM state used for labeling and attention update.\n        """"""\n        predictions = torch.mm(hidden_state.view(1, -1), self.theta_classification)\n        attention = torch.mm(hidden_state.view(1, -1), self.theta_rank)\n        attention = torch.nn.functional.softmax(attention, dim=1)\n        return predictions, attention\n\nclass GAM(torch.nn.Module):\n    """"""\n    Graph Attention Machine class.\n    """"""\n    def __init__(self, args):\n        """"""\n        Initializing the machine.\n        :param args: Arguments object.\n        """"""\n        super(GAM, self).__init__()\n        self.args = args\n        self.identifiers, self.class_number = read_node_labels(self.args)\n        self.step_block = StepNetworkLayer(self.args, self.identifiers)\n        self.recurrent_block = torch.nn.LSTM(self.args.combined_dimensions,\n                                             self.args.combined_dimensions, 1)\n\n        self.down_block = DownStreamNetworkLayer(self.args, self.class_number, self.identifiers)\n        self.reset_attention()\n\n    def reset_attention(self):\n        """"""\n        Resetting the attention and hidden states.\n        """"""\n        self.step_block.attention = torch.ones((len(self.identifiers)))/len(self.identifiers)\n        self.lstm_h_0 = torch.randn(1, 1, self.args.combined_dimensions)\n        self.lstm_c_0 = torch.randn(1, 1, self.args.combined_dimensions)\n\n    def forward(self, data, graph, features, node):\n        """"""\n        Doing a forward pass on a graph from a given node.\n        :param data: Data dictionary.\n        :param graph: NetworkX graph.\n        :param features: Feature tensor.\n        :param node: Source node identifier.\n        :return label_predictions: Label prediction.\n        :return node: New node to move to.\n        :return attention_score: Attention score on selected node.\n        """"""\n        self.state, node, attention_score = self.step_block(data, graph, features, node)\n        lstm_output, (self.h0, self.c0) = self.recurrent_block(self.state,\n                                                               (self.lstm_h_0, self.lstm_c_0))\n        label_predictions, attention = self.down_block(lstm_output)\n        self.step_block.attention = attention.view(-1)\n        label_predictions = torch.nn.functional.log_softmax(label_predictions, dim=1)\n        return label_predictions, node, attention_score\n\nclass GAMTrainer(object):\n    """"""\n    Object to train a GAM model.\n    """"""\n    def __init__(self, args):\n        self.args = args\n        self.model = GAM(args)\n        self.setup_graphs()\n        self.logs = create_logs(self.args)\n\n    def setup_graphs(self):\n        """"""\n        Listing the training and testing graphs in the source folders.\n        """"""\n        self.training_graphs = glob.glob(self.args.train_graph_folder + ""*.json"")\n        self.test_graphs = glob.glob(self.args.test_graph_folder + ""*.json"")\n\n    def process_graph(self, graph_path, batch_loss):\n        """"""\n        Reading a graph and doing a forward pass on a graph with a time budget.\n        :param graph_path: Location of the graph to process.\n        :param batch_loss: Loss on the graphs processed so far in the batch.\n        :return batch_loss: Incremented loss on the current batch being processed.\n        """"""\n        data = json.load(open(graph_path))\n        graph, features = create_features(data, self.model.identifiers)\n        node = random.choice(list(graph.nodes()))\n        attention_loss = 0\n        for t in range(self.args.time):\n            predictions, node, attention_score = self.model(data, graph, features, node)\n            target, prediction_loss = calculate_predictive_loss(data, predictions)\n            batch_loss = batch_loss + prediction_loss\n            if t < self.args.time-2:\n                attention_loss += (self.args.gamma**(self.args.time-t))*torch.log(attention_score)\n        reward = calculate_reward(target, predictions)\n        batch_loss = batch_loss-reward*attention_loss\n        self.model.reset_attention()\n        return batch_loss\n\n    def process_batch(self, batch):\n        """"""\n        Forward and backward propagation on a batch of graphs.\n        :param batch: Batch if graphs.\n        :return loss_value: Value of loss on batch.\n        """"""\n        self.optimizer.zero_grad()\n        batch_loss = 0\n        for graph_path in batch:\n            batch_loss = self.process_graph(graph_path, batch_loss)\n        batch_loss.backward(retain_graph=True)\n        self.optimizer.step()\n        loss_value = batch_loss.item()\n        self.optimizer.zero_grad()\n        return loss_value\n\n    def update_log(self):\n        """"""\n        Adding the end of epoch loss to the log.\n        """"""\n        average_loss = self.epoch_loss/self.nodes_processed\n        self.logs[""losses""].append(average_loss)\n\n    def fit(self):\n        """"""\n        Fitting a model on the training dataset.\n        """"""\n        print(""\\nTraining started.\\n"")\n        self.model.train()\n        self.optimizer = torch.optim.Adam(self.model.parameters(),\n                                          lr=self.args.learning_rate,\n                                          weight_decay=self.args.weight_decay)\n        self.optimizer.zero_grad()\n        epoch_range = trange(self.args.epochs, desc=""Epoch: "", leave=True)\n        for _ in epoch_range:\n            random.shuffle(self.training_graphs)\n            batches = create_batches(self.training_graphs, self.args.batch_size)\n            self.epoch_loss = 0\n            self.nodes_processed = 0\n            batch_range = trange(len(batches))\n            for batch in batch_range:\n                self.epoch_loss = self.epoch_loss + self.process_batch(batches[batch])\n                self.nodes_processed = self.nodes_processed + len(batches[batch])\n                loss_score = round(self.epoch_loss/self.nodes_processed, 4)\n                batch_range.set_description(""(Loss=%g)"" % loss_score)\n            self.update_log()\n\n    def score_graph(self, data, prediction):\n        """"""\n        Scoring the prediction on the graph.\n        :param data: Data hash table of graph.\n        :param prediction: Label prediction.\n        """"""\n        target = data[""target""]\n        is_it_right = (target == prediction)\n        self.predictions.append(is_it_right)\n\n    def score(self):\n        """"""\n        Scoring the test set graphs.\n        """"""\n        print(""\\n"")\n        print(""\\nScoring the test set.\\n"")\n        self.model.eval()\n        self.predictions = []\n        for data in tqdm(self.test_graphs):\n            data = json.load(open(data))\n            graph, features = create_features(data, self.model.identifiers)\n            node_predictions = []\n            for _ in range(self.args.repetitions):\n                node = random.choice(list(graph.nodes()))\n                for _ in range(self.args.time):\n                    prediction, node, _ = self.model(data, graph, features, node)\n                node_predictions.append(np.argmax(prediction.detach()))\n                self.model.reset_attention()\n            prediction = max(set(node_predictions), key=node_predictions.count)\n            self.score_graph(data, prediction)\n        self.accuracy = float(np.mean(self.predictions))\n        print(""\\nThe test set accuracy is: ""+str(round(self.accuracy, 4))+"".\\n"")\n\n    def save_predictions_and_logs(self):\n        """"""\n        Saving the predictions as a csv file and logs as a JSON.\n        """"""\n        self.logs[""test_accuracy""] = self.accuracy\n        with open(self.args.log_path, ""w"") as f:\n            json.dump(self.logs, f)\n        cols = [""graph_id"", ""predicted_label""]\n        predictions = [[self.test_graphs[i], self.predictions[i].item()] for i in range(len(self.test_graphs))]\n        self.output_data = pd.DataFrame(predictions, columns=cols)\n        self.output_data.to_csv(self.args.prediction_path, index=None)\n'"
src/main.py,0,"b'""""""Running the GAM model.""""""\n\nfrom gam import GAMTrainer\nfrom utils import tab_printer\nfrom param_parser import parameter_parser\n\ndef main():\n    """"""\n    Parsing command line parameters, processing graphs, fitting a GAM.\n    """"""\n    args = parameter_parser()\n    tab_printer(args)\n    model = GAMTrainer(args)\n    model.fit()\n    model.score()\n    model.save_predictions_and_logs()\n\nif __name__ == ""__main__"":\n    main()\n'"
src/param_parser.py,0,"b'""""""Parameter parsing.""""""\n\nimport argparse\n\ndef parameter_parser():\n    """"""\n    A method to parse up command line parameters. By default it learns on the Erdos-Renyi dataset.\n    The default hyperparameters give good results without cross-validation.\n    """"""\n    parser = argparse.ArgumentParser(description=""Run GAM."")\n\t\n    parser.add_argument(""--train-graph-folder"",\n                        nargs=""?"",\n                        default=""./input/train/"",\n\t                help=""Training graphs folder."")\n\n    parser.add_argument(""--test-graph-folder"",\n                        nargs=""?"",\n                        default=""./input/test/"",\n\t                help=""Testing graphs folder."")\n\n    parser.add_argument(""--prediction-path"",\n                        nargs=""?"",\n                        default=""./output/erdos_predictions.csv"",\n\t                help=""Path to store the predicted graph labels."")\n\n    parser.add_argument(""--log-path"",\n                        nargs=""?"",\n                        default=""./logs/erdos_gam_logs.json"",\n\t                help=""Log json with parameters and performance."")\n\n    parser.add_argument(""--epochs"",\n                        type=int,\n                        default=10,\n\t                help=""Number of training epochs. Default is 10."")\n\n    parser.add_argument(""--step-dimensions"",\n                        type=int,\n                        default=32,\n\t                help=""Number of neurons in step network. Default is 32."")\n\n    parser.add_argument(""--combined-dimensions"",\n                        type=int,\n                        default=64,\n\t                help=""Number of neurons in the shared layer of the step net. Default is 64."")\n\n    parser.add_argument(""--batch-size"",\n                        type=int,\n                        default=32,\n\t                help=""Number of graphs processed per batch. Default is 32."")\n\n    parser.add_argument(""--time"",\n                        type=int,\n                        default=10,\n\t                help=""Time budget for steps. Default is 20."")\n\n    parser.add_argument(""--repetitions"",\n                        type=int,\n                        default=10,\n\t                help=""Number of predictive repetitions. Default is 10."")\n\n    parser.add_argument(""--gamma"",\n                        type=float,\n                        default=0.99,\n\t                help=""Discount for correct predictions. Default is 0.99."")\n\n    parser.add_argument(""--learning-rate"",\n                        type=float,\n                        default=0.001,\n\t                help=""Learning rate. Default is 0.001."")\n\n    parser.add_argument(""--weight-decay"",\n                        type=float,\n                        default=10**-5,\n\t                help=""Learning rate. Default is 10^-5."")\n\n    return parser.parse_args()\n'"
src/utils.py,4,"b'""""""Data reading utils.""""""\n\nimport json\nimport glob\nimport torch\nimport numpy as np\nimport networkx as nx\nfrom tqdm import tqdm\nfrom texttable import Texttable\n\ndef tab_printer(args):\n    """"""\n    Function to print the logs in a nice tabular format.\n    :param args: Parameters used for the model.\n    """"""\n    args = vars(args)\n    keys = sorted(args.keys())\n    t = Texttable()\n    t.add_rows([[""Parameter"", ""Value""]])\n    t.add_rows([[k.replace(""_"", "" "").capitalize(), args[k]] for k in keys])\n    print(t.draw())\n\ndef read_node_labels(args):\n    """"""\n    Reading the graphs from disk.\n    :param args: Arguments object.\n    :return identifiers: Hash table of unique node labels in the dataset.\n    :return class_number: Number of unique graph classes in the dataset.\n    """"""\n    print(""\\nCollecting unique node labels.\\n"")\n    labels = set()\n    targets = set()\n    graphs = glob.glob(args.train_graph_folder + ""*.json"")\n    try:\n        graphs = graphs + glob.glob(args.test_graph_folder + ""*.json"")\n    except:\n        pass\n    for g in tqdm(graphs):\n        data = json.load(open(g))\n        labels = labels.union(set(list(data[""labels""].values())))\n        targets = targets.union(set([data[""target""]]))\n    identifiers = {label: i for i, label in enumerate(list(labels))}\n    class_number = len(targets)\n    print(""\\n\\nThe number of graph classes is: ""+str(class_number)+"".\\n"")\n    return identifiers, class_number\n\ndef create_logs(args):\n    """"""\n    Creates a dictionary for logging.\n    :param args: Arguments object.\n    :param log: Hash table for logs.\n    """"""\n    log = dict()\n    log[""losses""] = []\n    log[""params""] = vars(args)\n    return log\n\ndef create_features(data, identifiers):\n    """"""\n     Creates a tensor of node features.\n    :param data: Hash table with data.\n    :param identifiers: Node labels mapping.\n    :return graph: NetworkX object.\n    :return features: Feature Tensor (PyTorch).\n    """"""\n    graph = nx.from_edgelist(data[""edges""])\n    features = []\n    for node in graph.nodes():\n        features.append([1.0 if data[""labels""][str(node)] == i else 0.0 for i in range(len(identifiers))])\n    features = np.array(features, dtype=np.float32)\n    features = torch.tensor(features)\n    return graph, features\n\ndef create_batches(graphs, batch_size):\n    """"""\n    Creating batches of graph locations.\n    :param graphs: List of training graphs.\n    :param batch_size: Size of batches.\n    :return batches: List of lists with paths to graphs.\n    """"""\n    batches = [graphs[i:i + batch_size] for i in range(0, len(graphs), batch_size)]\n    return batches\n\ndef calculate_reward(target, prediction):\n    """"""\n    Calculating a reward for a prediction.\n    :param target: True graph label.\n    :param prediction: Predicted graph label.\n    """"""\n    reward = (target == torch.argmax(prediction))\n    reward = 2*(reward.float()-0.5)\n    return reward\n\ndef calculate_predictive_loss(data, predictions):\n    """"""\n    Prediction loss calculation.\n    :param data: Hash with label.\n    :param prediction: Predicted label.\n    :return target: Target tensor.\n    :prediction loss: Loss on sample.\n    """"""\n    target = [data[""target""]]\n    target = torch.tensor(target)\n    prediction_loss = torch.nn.functional.nll_loss(predictions, target)\n    return target, prediction_loss\n'"
