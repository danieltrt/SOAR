file_path,api_count,code
src/layers.py,16,"b'""""""NGCN and DenseNGCN layers.""""""\n\nimport math\nimport torch\nfrom torch_sparse import spmm\n\ndef uniform(size, tensor):\n    """"""\n    Uniform weight initialization.\n    :param size: Size of the tensor.\n    :param tensor: Tensor initialized.\n    """"""\n    stdv = 1.0 / math.sqrt(size)\n    if tensor is not None:\n        tensor.data.uniform_(-stdv, stdv)\n\nclass SparseNGCNLayer(torch.nn.Module):\n    """"""\n    Multi-scale Sparse Feature Matrix GCN layer.\n    :param in_channels: Number of features.\n    :param out_channels: Number of filters.\n    :param iterations: Adjacency matrix power order.\n    :param dropout_rate: Dropout value.\n    """"""\n    def __init__(self, in_channels, out_channels, iterations, dropout_rate):\n        super(SparseNGCNLayer, self).__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.iterations = iterations\n        self.dropout_rate = dropout_rate\n        self.define_parameters()\n        self.init_parameters()\n\n    def define_parameters(self):\n        """"""\n        Defining the weight matrices.\n        """"""\n        self.weight_matrix = torch.nn.Parameter(torch.Tensor(self.in_channels, self.out_channels))\n        self.bias = torch.nn.Parameter(torch.Tensor(1, self.out_channels))\n\n    def init_parameters(self):\n        """"""\n        Initializing weights.\n        """"""\n        torch.nn.init.xavier_uniform_(self.weight_matrix)\n        torch.nn.init.xavier_uniform_(self.bias)\n\n    def forward(self, normalized_adjacency_matrix, features):\n        """"""\n        Doing a forward pass.\n        :param normalized_adjacency_matrix: Normalized adjacency matrix.\n        :param features: Feature matrix.\n        :return base_features: Convolved features.\n        """"""\n        feature_count, _ = torch.max(features[""indices""],dim=1)\n        feature_count = feature_count + 1\n        base_features = spmm(features[""indices""], features[""values""], feature_count[0],\n                             feature_count[1], self.weight_matrix)\n\n        base_features = base_features + self.bias\n\n        base_features = torch.nn.functional.dropout(base_features,\n                                                    p=self.dropout_rate,\n                                                    training=self.training)\n\n        base_features = torch.nn.functional.relu(base_features)\n        for _ in range(self.iterations-1):\n            base_features = spmm(normalized_adjacency_matrix[""indices""],\n                                 normalized_adjacency_matrix[""values""],\n                                 base_features.shape[0],\n                                 base_features.shape[0],\n                                 base_features)\n        return base_features\n\nclass DenseNGCNLayer(torch.nn.Module):\n    """"""\n    Multi-scale Dense Feature Matrix GCN layer.\n    :param in_channels: Number of features.\n    :param out_channels: Number of filters.\n    :param iterations: Adjacency matrix power order.\n    :param dropout_rate: Dropout value.\n    """"""\n    def __init__(self, in_channels, out_channels, iterations, dropout_rate):\n        super(DenseNGCNLayer, self).__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.iterations = iterations\n        self.dropout_rate = dropout_rate\n        self.define_parameters()\n        self.init_parameters()\n\n    def define_parameters(self):\n        """"""\n        Defining the weight matrices.\n        """"""\n        self.weight_matrix = torch.nn.Parameter(torch.Tensor(self.in_channels, self.out_channels))\n        self.bias = torch.nn.Parameter(torch.Tensor(1, self.out_channels))\n\n    def init_parameters(self):\n        """"""\n        Initializing weights.\n        """"""\n        torch.nn.init.xavier_uniform_(self.weight_matrix)\n        torch.nn.init.xavier_uniform_(self.bias)\n\n    def forward(self, normalized_adjacency_matrix, features):\n        """"""\n        Doing a forward pass.\n        :param normalized_adjacency_matrix: Normalized adjacency matrix.\n        :param features: Feature matrix.\n        :return base_features: Convolved features.\n        """"""\n        base_features = torch.mm(features, self.weight_matrix)\n        base_features = torch.nn.functional.dropout(base_features,\n                                                    p=self.dropout_rate,\n                                                    training=self.training)\n        for _ in range(self.iterations-1):\n            base_features = spmm(normalized_adjacency_matrix[""indices""],\n                                 normalized_adjacency_matrix[""values""],\n                                 base_features.shape[0],\n                                 base_features.shape[0],\n                                 base_features)\n        base_features = base_features + self.bias\n        return base_features\n\nclass ListModule(torch.nn.Module):\n    """"""\n    Abstract list layer class.\n    """"""\n    def __init__(self, *args):\n        """"""\n        Module initializing.\n        """"""\n        super(ListModule, self).__init__()\n        idx = 0\n        for module in args:\n            self.add_module(str(idx), module)\n            idx += 1\n\n    def __getitem__(self, idx):\n        """"""\n        Getting the indexed layer.\n        """"""\n        if idx < 0 or idx >= len(self._modules):\n            raise IndexError(\'index {} is out of range\'.format(idx))\n        it = iter(self._modules.values())\n        for i in range(idx):\n            next(it)\n        return next(it)\n\n    def __iter__(self):\n        """"""\n        Iterating on the layers.\n        """"""\n        return iter(self._modules.values())\n\n    def __len__(self):\n        """"""\n        Number of layers.\n        """"""\n        return len(self._modules)\n'"
src/main.py,1,"b'""""""Running MixHop or N-GCN.""""""\n\nimport torch\nfrom param_parser import parameter_parser\nfrom trainer_and_networks import Trainer\nfrom utils import tab_printer, graph_reader, feature_reader, target_reader\n\ndef main():\n    """"""\n    Parsing command line parameters, reading data.\n    Fitting an NGCN and scoring the model.\n    """"""\n    args = parameter_parser()\n    torch.manual_seed(args.seed)\n    tab_printer(args)\n    graph = graph_reader(args.edge_path)\n    features = feature_reader(args.features_path)\n    target = target_reader(args.target_path)\n    trainer = Trainer(args, graph, features, target, True)\n    trainer.fit()\n    if args.model == ""mixhop"":\n        trainer.evaluate_architecture()\n        args = trainer.reset_architecture()\n        trainer = Trainer(args, graph, features, target, False)\n        trainer.fit()\n\nif __name__ == ""__main__"":\n    main()\n'"
src/param_parser.py,0,"b'""""""Parameter parsing.""""""\n\nimport argparse\n\ndef parameter_parser():\n    """"""\n    A method to parse up command line parameters. By default it trains on the Cora dataset.\n    The default hyperparameters give a good quality representation without grid search.\n    """"""\n    parser = argparse.ArgumentParser(description=""Run MixHop/N-GCN."")\n\n    parser.add_argument(""--edge-path"",\n                        nargs=""?"",\n                        default=""./input/cora_edges.csv"",\n\t                help=""Edge list csv."")\n\n    parser.add_argument(""--features-path"",\n                        nargs=""?"",\n                        default=""./input/cora_features.json"",\n\t                help=""Features json."")\n\n    parser.add_argument(""--target-path"",\n                        nargs=""?"",\n                        default=""./input/cora_target.csv"",\n\t                help=""Target classes csv."")\n\n    parser.add_argument(""--model"",\n                        nargs=""?"",\n                        default=""mixhop"",\n\t                help=""Target classes csv."")\n\n    parser.add_argument(""--epochs"",\n                        type=int,\n                        default=2000,\n\t                help=""Number of training epochs. Default is 2000."")\n\n    parser.add_argument(""--seed"",\n                        type=int,\n                        default=42,\n\t                help=""Random seed for train-test split. Default is 42."")\n\n    parser.add_argument(""--early-stopping"",\n                        type=int,\n                        default=10,\n\t                help=""Number of early stopping rounds. Default is 10."")\n\n    parser.add_argument(""--training-size"",\n                        type=int,\n                        default=1500,\n\t                help=""Training set size. Default is 1500."")\n\n    parser.add_argument(""--validation-size"",\n                        type=int,\n                        default=500,\n\t                help=""Validation set size. Default is 500."")\n\n    parser.add_argument(""--dropout"",\n                        type=float,\n                        default=0.5,\n\t                help=""Dropout parameter. Default is 0.5."")\n\n    parser.add_argument(""--learning-rate"",\n                        type=float,\n                        default=0.01,\n\t                help=""Learning rate. Default is 0.01."")\n\n    parser.add_argument(""--cut-off"",\n                        type=float,\n                        default=0.1,\n\t                help=""Weight cut-off. Default is 0.1."")\n\n    parser.add_argument(""--lambd"",\n                        type=float,\n                        default=0.0005,\n\t                help=""L2 regularization coefficient. Default is 0.0005."")\n\n    parser.add_argument(""--layers-1"",\n                        nargs=""+"",\n                        type=int,\n                        help=""Layer dimensions separated by space (top). E.g. 200 20."")\n\n    parser.add_argument(""--layers-2"",\n                        nargs=""+"",\n                        type=int,\n                        help=""Layer dimensions separated by space (bottom). E.g. 200 200."")\n\n    parser.add_argument(""--budget"",\n                        type=int,\n                        default=60,\n                        help=""Architecture neuron allocation budget. Default is 60."")\n\n    parser.set_defaults(layers_1=[200, 200, 200])\n    parser.set_defaults(layers_2=[200, 200, 200])\n    \n    return parser.parse_args()\n'"
src/trainer_and_networks.py,23,"b'import torch\nimport random\nfrom tqdm import trange\nfrom utils import create_propagator_matrix\nfrom layers import SparseNGCNLayer, DenseNGCNLayer, ListModule\n\nclass NGCNNetwork(torch.nn.Module):\n    """"""\n    Higher Order Graph Convolutional Model.\n    :param args: Arguments object.\n    :param feature_number: Feature input number.\n    :param class_number: Target class number.\n    """"""\n    def __init__(self, args, feature_number, class_number):\n        super(NGCNNetwork, self).__init__()\n        self.args = args\n        self.feature_number = feature_number\n        self.class_number = class_number\n        self.order = len(self.args.layers_1)\n        self.setup_layer_structure()\n\n    def setup_layer_structure(self):\n        """"""\n        Creating the layer structure (3 convolutional layers) and dense final.\n        """"""\n        self.main_layers = [SparseNGCNLayer(self.feature_number, self.args.layers_1[i-1], i, self.args.dropout) for i in range(1, self.order+1)]\n        self.main_layers = ListModule(*self.main_layers)\n        self.fully_connected = torch.nn.Linear(sum(self.args.layers_1), self.class_number)\n\n    def forward(self, normalized_adjacency_matrix, features):\n        """"""\n        Forward pass.\n        :param normalized adjacency_matrix: Target matrix as a dict with indices and values.\n        :param features: Feature matrix.\n        :return predictions: Label predictions.\n        """"""\n        abstract_features = [self.main_layers[i](normalized_adjacency_matrix, features) for i in range(self.order)]\n        abstract_features = torch.cat(abstract_features, dim=1)\n        predictions = torch.nn.functional.log_softmax(self.fully_connected(abstract_features), dim=1)\n        return predictions\n\nclass MixHopNetwork(torch.nn.Module):\n    """"""\n    MixHop: Higher-Order Graph Convolutional Architectures via Sparsified Neighborhood Mixing.\n    :param args: Arguments object.\n    :param feature_number: Feature input number.\n    :param class_number: Target class number.\n    """"""\n    def __init__(self, args, feature_number, class_number):\n        super(MixHopNetwork, self).__init__()\n        self.args = args\n        self.feature_number = feature_number\n        self.class_number = class_number\n        self.calculate_layer_sizes()\n        self.setup_layer_structure()\n\n    def calculate_layer_sizes(self):\n        self.abstract_feature_number_1 = sum(self.args.layers_1)\n        self.abstract_feature_number_2 = sum(self.args.layers_2)\n        self.order_1 = len(self.args.layers_1)\n        self.order_2 = len(self.args.layers_2)\n\n    def setup_layer_structure(self):\n        """"""\n        Creating the layer structure (3 convolutional upper layers, 3 bottom layers) and dense final.\n        """"""\n        self.upper_layers = [SparseNGCNLayer(self.feature_number, self.args.layers_1[i-1], i, self.args.dropout) for i in range(1, self.order_1+1)]\n        self.upper_layers = ListModule(*self.upper_layers)\n        self.bottom_layers = [DenseNGCNLayer(self.abstract_feature_number_1, self.args.layers_2[i-1], i, self.args.dropout) for i in range(1, self.order_2+1)]\n        self.bottom_layers = ListModule(*self.bottom_layers)\n        self.fully_connected = torch.nn.Linear(self.abstract_feature_number_2, self.class_number)\n\n    def calculate_group_loss(self):\n        """"""\n        Calculating the column losses.\n        """"""\n        weight_loss = 0\n        for i in range(self.order_1):\n            upper_column_loss = torch.norm(self.upper_layers[i].weight_matrix, dim=0)\n            loss_upper = torch.sum(upper_column_loss)\n            weight_loss = weight_loss + self.args.lambd*loss_upper\n        for i in range(self.order_2):\n            bottom_column_loss = torch.norm(self.bottom_layers[i].weight_matrix, dim=0)\n            loss_bottom = torch.sum(bottom_column_loss)\n            weight_loss = weight_loss + self.args.lambd*loss_bottom\n        return weight_loss\n\n    def calculate_loss(self):\n        """"""\n        Calculating the losses.\n        """"""\n        weight_loss = 0\n        for i in range(self.order_1):\n            loss_upper = torch.norm(self.upper_layers[i].weight_matrix)\n            weight_loss = weight_loss + self.args.lambd*loss_upper\n        for i in range(self.order_2):\n            loss_bottom = torch.norm(self.bottom_layers[i].weight_matrix)\n            weight_loss = weight_loss + self.args.lambd*loss_bottom\n        return weight_loss\n            \n    def forward(self, normalized_adjacency_matrix, features):\n        """"""\n        Forward pass.\n        :param normalized adjacency_matrix: Target matrix as a dict with indices and values.\n        :param features: Feature matrix.\n        :return predictions: Label predictions.\n        """"""\n        abstract_features_1 = torch.cat([self.upper_layers[i](normalized_adjacency_matrix, features) for i in range(self.order_1)], dim=1)\n        abstract_features_2 = torch.cat([self.bottom_layers[i](normalized_adjacency_matrix, abstract_features_1) for i in range(self.order_2)], dim=1)\n        predictions = torch.nn.functional.log_softmax(self.fully_connected(abstract_features_2), dim=1)\n        return predictions\n\nclass Trainer(object):\n    """"""\n    Class for training the neural network.\n    :param args: Arguments object.\n    :param graph: NetworkX graph.\n    :param features: Feature sparse matrix.\n    :param target: Target vector.\n    :param base_run: Loss calculation behavioural flag.\n    """"""\n    def __init__(self, args, graph, features, target, base_run):\n        self.args = args\n        self.graph = graph\n        self.features = features\n        self.target = target\n        self.base_run = base_run\n        self.setup_features()\n        self.train_test_split()\n        self.setup_model()\n\n    def train_test_split(self):\n        """"""\n        Creating a train/test split.\n        """"""\n        random.seed(self.args.seed)\n        nodes = [node for node in range(self.ncount)]\n        random.shuffle(nodes)\n        self.train_nodes = torch.LongTensor(nodes[0:self.args.training_size])\n        self.validation_nodes = torch.LongTensor(nodes[self.args.training_size:self.args.training_size+self.args.validation_size])\n        self.test_nodes = torch.LongTensor(nodes[self.args.training_size+self.args.validation_size:])\n\n    def setup_features(self):\n        """"""\n        Creating a feature matrix, target vector and propagation matrix.\n        """"""\n        self.ncount = self.features[""dimensions""][0]\n        self.feature_number = self.features[""dimensions""][1]\n        self.class_number = torch.max(self.target).item()+1\n        self.propagation_matrix = create_propagator_matrix(self.graph)\n\n    def setup_model(self):\n        """"""\n        Defining a PageRankNetwork.\n        """"""\n        if self.args.model == ""mixhop"":\n            self.model = MixHopNetwork(self.args, self.feature_number, self.class_number)\n        else:\n            self.model = NGCNNetwork(self.args, self.feature_number, self.class_number)\n\n    def fit(self):\n        """"""\n        Fitting a neural network with early stopping.\n        """"""\n        accuracy = 0\n        no_improvement = 0\n        epochs = trange(self.args.epochs, desc=""Accuracy"")\n        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.args.learning_rate)\n        self.model.train()\n        for _ in epochs:\n            self.optimizer.zero_grad()\n            prediction = self.model(self.propagation_matrix, self.features)\n            loss = torch.nn.functional.nll_loss(prediction[self.train_nodes], self.target[self.train_nodes])\n            if self.args.model == ""mixhop"" and self.base_run == True:\n                loss = loss + self.model.calculate_group_loss()\n            elif self.args.model == ""mixhop"" and self.base_run == False:\n                loss = loss + self.model.calculate_loss()\n            loss.backward()\n            self.optimizer.step()\n            new_accuracy = self.score(self.validation_nodes)\n            epochs.set_description(""Validation Accuracy: %g"" % round(new_accuracy, 4))\n            if new_accuracy < accuracy:\n                no_improvement = no_improvement + 1\n                if no_improvement == self.args.early_stopping:\n                    epochs.close()\n                    break\n            else:\n                no_improvement = 0\n                accuracy = new_accuracy\n        acc = self.score(self.test_nodes)\n        print(""\\nTest accuracy: "" + str(round(acc, 4)) +""\\n"")\n\n    def score(self, indices):\n        """"""\n        Scoring a neural network.\n        :param indices: Indices of nodes involved in accuracy calculation.\n        :return acc: Accuracy score.\n        """"""\n        self.model.eval()\n        _, prediction = self.model(self.propagation_matrix, self.features).max(dim=1)\n        correct = prediction[indices].eq(self.target[indices]).sum().item()\n        acc = correct / indices.shape[0]\n        return acc\n\n    def evaluate_architecture(self):\n        """"""\n        Making a choice about the optimal layer sizes.\n        """"""\n        print(""The best architecture is:\\n"")\n        self.layer_sizes = dict()\n\n        self.layer_sizes[""upper""] = []\n\n        for layer in self.model.upper_layers:\n            norms = torch.norm(layer.weight_matrix**2, dim=0)\n            norms = norms[norms < self.args.cut_off]\n            self.layer_sizes[""upper""].append(norms.shape[0])\n\n        self.layer_sizes[""bottom""] = []\n\n        for layer in self.model.bottom_layers:\n            norms = torch.norm(layer.weight_matrix**2, dim=0)\n            norms = norms[norms < self.args.cut_off]\n            self.layer_sizes[""bottom""].append(norms.shape[0])\n\n        self.layer_sizes[""upper""] = [int(self.args.budget*layer_size/sum(self.layer_sizes[""upper""]))  for layer_size in self.layer_sizes[""upper""]]\n        self.layer_sizes[""bottom""] = [int(self.args.budget*layer_size/sum(self.layer_sizes[""bottom""]))  for layer_size in self.layer_sizes[""bottom""]]\n        print(""Layer 1.: ""+str(tuple(self.layer_sizes[""upper""])))\n        print(""Layer 2.: ""+str(tuple(self.layer_sizes[""bottom""])))\n\n    def reset_architecture(self):\n        """"""\n        Changing the layer sizes.\n        """"""\n        print(""\\nResetting the architecture.\\n"")\n        self.args.layers_1 = self.layer_sizes[""upper""]\n        self.args.layers_2 = self.layer_sizes[""bottom""]\n        return self.args\n'"
src/utils.py,5,"b'""""""Data reading tools.""""""\n\nimport json\nimport torch\nimport numpy as np\nimport pandas as pd\nimport networkx as nx\nfrom scipy import sparse\nfrom texttable import Texttable\n\ndef tab_printer(args):\n    """"""\n    Function to print the logs in a nice tabular format.\n    :param args: Parameters used for the model.\n    """"""\n    args = vars(args)\n    keys = sorted(args.keys())\n    t = Texttable()\n    t.add_rows([[""Parameter"", ""Value""]])\n    t.add_rows([[k.replace(""_"", "" "").capitalize(), args[k]] for k in keys])\n    print(t.draw())\n\ndef graph_reader(path):\n    """"""\n    Function to read the graph from the path.\n    :param path: Path to the edge list.\n    :return graph: NetworkX object returned.\n    """"""\n    graph = nx.from_edgelist(pd.read_csv(path).values.tolist())\n    graph.remove_edges_from(list(nx.selfloop_edges(graph)))\n    return graph\n\ndef feature_reader(path):\n    """"""\n    Reading the feature matrix stored as JSON from the disk.\n    :param path: Path to the JSON file.\n    :return out_features: Dict with index and value tensor.\n    """"""\n    features = json.load(open(path))\n    index_1 = [int(k) for k, v in features.items() for fet in v]\n    index_2 = [int(fet) for k, v in features.items() for fet in v]\n    values = [1.0]*len(index_1)\n    nodes = [int(k) for k, v in features.items()]\n    node_count = max(nodes)+1\n    feature_count = max(index_2)+1\n    features = sparse.coo_matrix((values, (index_1, index_2)),\n                                 shape=(node_count, feature_count),\n                                 dtype=np.float32)\n    out_features = dict()\n    ind = np.concatenate([features.row.reshape(-1, 1), features.col.reshape(-1, 1)], axis=1)\n    out_features[""indices""] = torch.LongTensor(ind.T)\n    out_features[""values""] = torch.FloatTensor(features.data)\n    out_features[""dimensions""] = features.shape\n    return out_features\n\ndef target_reader(path):\n    """"""\n    Reading the target vector from disk.\n    :param path: Path to the target.\n    :return target: Target vector.\n    """"""\n    target = torch.LongTensor(np.array(pd.read_csv(path)[""target""]))\n    return target\n\ndef create_adjacency_matrix(graph):\n    """"""\n    Creating a sparse adjacency matrix.\n    :param graph: NetworkX object.\n    :return A: Adjacency matrix.\n    """"""\n    index_1 = [edge[0] for edge in graph.edges()] + [edge[1] for edge in graph.edges()]\n    index_2 = [edge[1] for edge in graph.edges()] + [edge[0] for edge in graph.edges()]\n    values = [1 for index in index_1]\n    node_count = max(max(index_1)+1, max(index_2)+1)\n    A = sparse.coo_matrix((values, (index_1, index_2)),\n                          shape=(node_count, node_count),\n                          dtype=np.float32)\n    return A\n\ndef normalize_adjacency_matrix(A, I):\n    """"""\n    Creating a normalized adjacency matrix with self loops.\n    :param A: Sparse adjacency matrix.\n    :param I: Identity matrix.\n    :return A_tile_hat: Normalized adjacency matrix.\n    """"""\n    A_tilde = A + I\n    degrees = A_tilde.sum(axis=0)[0].tolist()\n    D = sparse.diags(degrees, [0])\n    D = D.power(-0.5)\n    A_tilde_hat = D.dot(A_tilde).dot(D)\n    return A_tilde_hat\n\ndef create_propagator_matrix(graph):\n    """"""\n    Creating a propagator matrix.\n    :param graph: NetworkX graph.\n    :return propagator: Dictionary of matrix indices and values.\n    """"""\n    A = create_adjacency_matrix(graph)\n    I = sparse.eye(A.shape[0])\n    A_tilde_hat = normalize_adjacency_matrix(A, I)\n    propagator = dict()\n    A_tilde_hat = sparse.coo_matrix(A_tilde_hat)\n    ind = np.concatenate([A_tilde_hat.row.reshape(-1, 1), A_tilde_hat.col.reshape(-1, 1)], axis=1)\n    propagator[""indices""] = torch.LongTensor(ind.T)\n    propagator[""values""] = torch.FloatTensor(A_tilde_hat.data)\n    return propagator\n'"
