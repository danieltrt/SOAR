file_path,api_count,code
src/ego_splitting.py,0,"b'""""""Ego-Splitter class""""""\n\nimport community\nimport networkx as nx\nfrom tqdm import tqdm\n\n\nclass EgoNetSplitter(object):\n    """"""An implementation of `""Ego-Splitting"" see:\n    https://www.eecs.yorku.ca/course_archive/2017-18/F/6412/reading/kdd17p145.pdf\n    From the KDD \'17 paper ""Ego-Splitting Framework: from Non-Overlapping to Overlapping Clusters"".\n    The tool first creates the egonets of nodes.\n    A persona-graph is created which is clustered by the Louvain method.\n    The resulting overlapping cluster memberships are stored as a dictionary.\n    Args:\n        resolution (float): Resolution parameter of Python Louvain. Default 1.0.\n    """"""\n    def __init__(self, resolution=1.0):\n        self.resolution = resolution\n\n    def _create_egonet(self, node):\n        """"""\n        Creating an ego net, extracting personas and partitioning it.\n        Args:\n            node: Node ID for egonet (ego node).\n        """"""\n        ego_net_minus_ego = self.graph.subgraph(self.graph.neighbors(node))\n        components = {i: n for i, n in enumerate(nx.connected_components(ego_net_minus_ego))}\n        new_mapping = {}\n        personalities = []\n        for k, v in components.items():\n            personalities.append(self.index)\n            for other_node in v:\n                new_mapping[other_node] = self.index\n            self.index = self.index+1\n        self.components[node] = new_mapping\n        self.personalities[node] = personalities\n\n    def _create_egonets(self):\n        """"""\n        Creating an egonet for each node.\n        """"""\n        self.components = {}\n        self.personalities = {}\n        self.index = 0\n        print(""Creating egonets."")\n        for node in tqdm(self.graph.nodes()):\n            self._create_egonet(node)\n\n    def _map_personalities(self):\n        """"""\n        Mapping the personas to new nodes.\n        """"""\n        self.personality_map = {p: n for n in self.graph.nodes() for p in self.personalities[n]}\n\n    def _get_new_edge_ids(self, edge):\n        """"""\n        Getting the new edge identifiers.\n        Args:\n            edge: Edge being mapped to the new identifiers.\n        """"""\n        return (self.components[edge[0]][edge[1]], self.components[edge[1]][edge[0]])\n\n    def _create_persona_graph(self):\n        """"""\n        Create a persona graph using the egonet components.\n        """"""\n        print(""Creating the persona graph."")\n        self.persona_graph_edges = [self._get_new_edge_ids(e) for e in tqdm(self.graph.edges())]\n        self.persona_graph = nx.from_edgelist(self.persona_graph_edges)\n\n    def _create_partitions(self):\n        """"""\n        Creating a non-overlapping clustering of nodes in the persona graph.\n        """"""\n        print(""Clustering the persona graph."")\n        self.partitions = community.best_partition(self.persona_graph, resolution=self.resolution)\n        self.overlapping_partitions = {node: [] for node in self.graph.nodes()}\n        for node, membership in self.partitions.items():\n            self.overlapping_partitions[self.personality_map[node]].append(membership)\n\n    def fit(self, graph):\n        """"""\n        Fitting an Ego-Splitter clustering model.\n        Arg types:\n            * **graph** *(NetworkX graph)* - The graph to be clustered.\n        """"""\n        self.graph = graph\n        self._create_egonets()\n        self._map_personalities()\n        self._create_persona_graph()\n        self._create_partitions()\n\n    def get_memberships(self):\n        r""""""Getting the cluster membership of nodes.\n        Return types:\n            * **memberships** *(dictionary of lists)* - Cluster memberships.\n        """"""\n        return self.overlapping_partitions\n'"
src/main.py,1,"b'""""""Running the Splitter.""""""\n\nimport torch\nfrom param_parser import parameter_parser\nfrom splitter import SplitterTrainer\nfrom utils import tab_printer, graph_reader\n\ndef main():\n    """"""\n    Parsing command line parameters.\n    Reading data, embedding base graph, creating persona graph and learning a splitter.\n    Saving the persona mapping and the embedding.\n    """"""\n    args = parameter_parser()\n    torch.manual_seed(args.seed)\n    tab_printer(args)\n    graph = graph_reader(args.edge_path)\n    trainer = SplitterTrainer(graph, args)\n    trainer.fit()\n    trainer.save_embedding()\n    trainer.save_persona_graph_mapping()\n\nif __name__ == ""__main__"":\n    main()\n'"
src/param_parser.py,0,"b'""""""Parameter parsing.""""""\n\nimport argparse\n\ndef parameter_parser():\n    """"""\n    A method to parse up command line parameters.\n    By default it trains on the Chameleons dataset.\n    The default hyperparameters give a good quality representation without grid search.\n    """"""\n    parser = argparse.ArgumentParser(description=""Run Splitter."")\n\n    parser.add_argument(""--edge-path"",\n                        nargs=""?"",\n                        default=""./input/chameleon_edges.csv"",\n\t                help=""Edge list csv."")\n\n    parser.add_argument(""--embedding-output-path"",\n                        nargs=""?"",\n                        default=""./output/chameleon_embedding.csv"",\n\t                help=""Embedding output path."")\n\n    parser.add_argument(""--persona-output-path"",\n                        nargs=""?"",\n                        default=""./output/chameleon_personas.json"",\n\t                help=""Persona output path."")\n\n    parser.add_argument(""--number-of-walks"",\n                        type=int,\n                        default=10,\n\t                help=""Number of random walks per source node. Default is 10."")\n\n    parser.add_argument(""--window-size"",\n                        type=int,\n                        default=5,\n\t                help=""Skip-gram window size. Default is 5."")\n\n    parser.add_argument(""--negative-samples"",\n                        type=int,\n                        default=5,\n\t                help=""Negative sample number. Default is 5."")\n\n    parser.add_argument(""--walk-length"",\n                        type=int,\n                        default=40,\n\t                help=""Truncated random walk length. Default is 40."")\n\n    parser.add_argument(""--seed"",\n                        type=int,\n                        default=42,\n\t                help=""Random seed for PyTorch. Default is 42."")\n\n    parser.add_argument(""--learning-rate"",\n                        type=float,\n                        default=0.025,\n\t                help=""Learning rate. Default is 0.025."")\n\n    parser.add_argument(""--lambd"",\n                        type=float,\n                        default=0.1,\n\t                help=""Regularization parameter. Default is 0.1."")\n\n    parser.add_argument(""--dimensions"",\n                        type=int,\n                        default=128,\n\t                help=""Embedding dimensions. Default is 128."")\n\n    parser.add_argument(\'--workers\',\n                        type=int,\n                        default=4,\n\t                help=\'Number of parallel workers. Default is 4.\')\n\n    return parser.parse_args()\n'"
src/splitter.py,24,"b'""""""Splitter Class.""""""\n\nimport json\nimport torch\nimport random\nimport numpy as np\nimport pandas as pd\nfrom tqdm import trange\nfrom walkers import DeepWalker\nfrom ego_splitting import EgoNetSplitter\n\nclass Splitter(torch.nn.Module):\n    """"""\n    An implementation of ""Splitter: Learning Node Representations\n    that Capture Multiple Social Contexts"" (WWW 2019).\n    Paper: http://epasto.org/papers/www2019splitter.pdf\n    """"""\n    def __init__(self, args, base_node_count, node_count):\n        """"""\n        Splitter set up.\n        :param args: Arguments object.\n        :param base_node_count: Number of nodes in the source graph.\n        :param node_count: Number of nodes in the persona graph.\n        """"""\n        super(Splitter, self).__init__()\n        self.args = args\n        self.base_node_count = base_node_count\n        self.node_count = node_count\n\n    def create_weights(self):\n        """"""\n        Creating weights for embedding.\n        """"""\n        self.base_node_embedding = torch.nn.Embedding(self.base_node_count,\n                                                      self.args.dimensions,\n                                                      padding_idx=0)\n\n        self.node_embedding = torch.nn.Embedding(self.node_count,\n                                                 self.args.dimensions,\n                                                 padding_idx=0)\n\n        self.node_noise_embedding = torch.nn.Embedding(self.node_count,\n                                                       self.args.dimensions,\n                                                       padding_idx=0)\n\n    def initialize_weights(self, base_node_embedding, mapping):\n        """"""\n        Using the base embedding and the persona mapping for initializing the embeddings.\n        :param base_node_embedding: Node embedding of the source graph.\n        :param mapping: Mapping of personas to nodes.\n        """"""\n        persona_embedding = np.array([base_node_embedding[n] for _, n in mapping.items()])\n        self.node_embedding.weight.data = torch.nn.Parameter(torch.Tensor(persona_embedding))\n        self.node_noise_embedding.weight.data = torch.nn.Parameter(torch.Tensor(persona_embedding))\n        self.base_node_embedding.weight.data = torch.nn.Parameter(torch.Tensor(base_node_embedding),\n                                                                  requires_grad=False)\n\n    def calculate_main_loss(self, sources, contexts, targets):\n        """"""\n        Calculating the main embedding loss.\n        :param sources: Source node vector.\n        :param contexts: Context node vector.\n        :param targets: Binary target vector.\n        :return main_loss: Loss value.\n        """"""\n        node_f = self.node_embedding(sources)\n        node_f = torch.nn.functional.normalize(node_f, p=2, dim=1)\n        feature_f = self.node_noise_embedding(contexts)\n        feature_f = torch.nn.functional.normalize(feature_f, p=2, dim=1)\n        scores = torch.sum(node_f*feature_f, dim=1)\n        scores = torch.sigmoid(scores)\n        main_loss = targets*torch.log(scores)+(1-targets)*torch.log(1-scores)\n        main_loss = -torch.mean(main_loss)\n        return main_loss\n\n    def calculate_regularization(self, pure_sources, personas):\n        """"""\n        Calculating the regularization loss.\n        :param pure_sources: Source nodes in persona graph.\n        :param personas: Context node vector.\n        :return regularization_loss: Loss value.\n        """"""\n        source_f = self.node_embedding(pure_sources)\n        original_f = self.base_node_embedding(personas)\n        scores = torch.clamp(torch.sum(source_f*original_f, dim=1), -15, 15)\n        scores = torch.sigmoid(scores)\n        regularization_loss = -torch.mean(torch.log(scores))\n        return regularization_loss\n\n    def forward(self, sources, contexts, targets, personas, pure_sources):\n        """"""\n        Doing a forward pass.\n        :param sources: Source node vector.\n        :param contexts: Context node vector.\n        :param targets: Binary target vector.\n        :param pure_sources: Source nodes in persona graph.\n        :param personas: Context node vector.\n        :return loss: Loss value.\n        """"""\n        main_loss = self.calculate_main_loss(sources, contexts, targets)\n        regularization_loss = self.calculate_regularization(pure_sources, personas)\n        loss = main_loss + self.args.lambd*regularization_loss\n        return loss\n\nclass SplitterTrainer(object):\n    """"""\n    Class for training a Splitter.\n    """"""\n    def __init__(self, graph, args):\n        """"""\n        :param graph: NetworkX graph object.\n        :param args: Arguments object.\n        """"""\n        self.graph = graph\n        self.args = args\n        self.device = torch.device(\'cuda\' if torch.cuda.is_available() else \'cpu\')\n\n    def create_noises(self):\n        """"""\n        Creating node noise distribution for negative sampling.\n        """"""\n        self.downsampled_degrees = {}\n        for n in self.egonet_splitter.persona_graph.nodes():\n            self.downsampled_degrees[n] = int(1+self.egonet_splitter.persona_graph.degree(n)**0.75)\n        self.noises = [k for k, v in self.downsampled_degrees.items() for i in range(v)]\n\n    def base_model_fit(self):\n        """"""\n        Fitting DeepWalk on base model.\n        """"""\n        self.base_walker = DeepWalker(self.graph, self.args)\n        print(""\\nDoing base random walks.\\n"")\n        self.base_walker.create_features()\n        print(""\\nLearning the base model.\\n"")\n        self.base_node_embedding = self.base_walker.learn_base_embedding()\n        print(""\\nDeleting the base walker.\\n"")\n        del self.base_walker\n\n    def create_split(self):\n        """"""\n        Creating an EgoNetSplitter.\n        """"""\n        self.egonet_splitter = EgoNetSplitter()\n        self.egonet_splitter.fit(self.graph)\n        self.persona_walker = DeepWalker(self.egonet_splitter.persona_graph, self.args)\n        print(""\\nDoing persona random walks.\\n"")\n        self.persona_walker.create_features()\n        self.create_noises()\n\n    def setup_model(self):\n        """"""\n        Creating a model and doing a transfer to GPU.\n        """"""\n        base_node_count = self.graph.number_of_nodes()\n        persona_node_count = self.egonet_splitter.persona_graph.number_of_nodes()\n        self.model = Splitter(self.args, base_node_count, persona_node_count)\n        self.model.create_weights()\n        self.model.initialize_weights(self.base_node_embedding,\n                                      self.egonet_splitter.personality_map)\n        self.model = self.model.to(self.device)\n\n    def transfer_batch(self, source_nodes, context_nodes, targets, persona_nodes, pure_source_nodes):\n        """"""\n        Transfering the batch to GPU.\n        """"""\n        self.sources = torch.LongTensor(source_nodes).to(self.device)\n        self.contexts = torch.LongTensor(context_nodes).to(self.device)\n        self.targets = torch.FloatTensor(targets).to(self.device)\n        self.personas = torch.LongTensor(persona_nodes).to(self.device)\n        self.pure_sources = torch.LongTensor(pure_source_nodes).to(self.device)\n\n    def optimize(self):\n        """"""\n        Doing a weight update.\n        """"""\n        loss = self.model(self.sources, self.contexts,\n                          self.targets, self.personas, self.pure_sources)\n        loss.backward()\n        self.optimizer.step()\n        self.optimizer.zero_grad()\n        return loss.item()\n\n    def process_walk(self, walk):\n        """"""\n        Process random walk (source, context) pairs.\n        Sample negative instances and create persona node list.\n        :param walk: Random walk sequence.\n        """"""\n        left_nodes = [walk[i] for i in range(len(walk)-self.args.window_size) for j in range(1, self.args.window_size+1)]\n        right_nodes = [walk[i+j] for i in range(len(walk)-self.args.window_size) for j in range(1, self.args.window_size+1)]\n        node_pair_count = len(left_nodes)\n        source_nodes = left_nodes + right_nodes\n        context_nodes = right_nodes + left_nodes\n        persona_nodes = np.array([self.egonet_splitter.personality_map[source_node] for source_node in source_nodes])\n        pure_source_nodes = np.array(source_nodes)\n        source_nodes = np.array((self.args.negative_samples+1)*source_nodes)\n        noises = np.random.choice(self.noises, node_pair_count*2*self.args.negative_samples)\n        context_nodes = np.concatenate((np.array(context_nodes), noises))\n        positives = [1.0 for node in range(node_pair_count*2)]\n        negatives = [0.0 for node in range(node_pair_count*self.args.negative_samples*2)]\n        targets = np.array(positives + negatives)\n        self.transfer_batch(source_nodes, context_nodes, targets, persona_nodes, pure_source_nodes)\n\n    def update_average_loss(self, loss_score):\n        """"""\n        Updating the average loss and the description of the time remains bar.\n        :param loss_score: Loss on the sample.\n        """"""\n        self.cummulative_loss = self.cummulative_loss + loss_score\n        self.steps = self.steps + 1\n        average_loss = self.cummulative_loss/self.steps\n        self.walk_steps.set_description(""Splitter (Loss=%g)"" % round(average_loss, 4))\n\n    def reset_average_loss(self, step):\n        """"""\n        Doing a reset on the average loss.\n        :param step: Current number of walks processed.\n        """"""\n        if step % 100 == 0:\n            self.cummulative_loss = 0\n            self.steps = 0\n\n    def fit(self):\n        """"""\n        Fitting a model.\n        """"""\n        self.base_model_fit()\n        self.create_split()\n        self.setup_model()\n        self.model.train()\n        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.args.learning_rate)\n        self.optimizer.zero_grad()\n        print(""\\nLearning the joint model.\\n"")\n        random.shuffle(self.persona_walker.paths)\n        self.walk_steps = trange(len(self.persona_walker.paths), desc=""Loss"")\n        for step in self.walk_steps:\n            self.reset_average_loss(step)\n            walk = self.persona_walker.paths[step]\n            self.process_walk(walk)\n            loss_score = self.optimize()\n            self.update_average_loss(loss_score)\n\n    def save_embedding(self):\n        """"""\n        Saving the node embedding.\n        """"""\n        print(""\\n\\nSaving the model.\\n"")\n        nodes = [node for node in self.egonet_splitter.persona_graph.nodes()]\n        nodes.sort()\n        nodes = torch.LongTensor(nodes).to(self.device)\n        embedding = self.model.node_embedding(nodes).cpu().detach().numpy()\n        embedding_header = [""id""] + [""x_"" + str(x) for x in range(self.args.dimensions)]\n        embedding = [np.array(range(embedding.shape[0])).reshape(-1, 1), embedding]\n        embedding = np.concatenate(embedding, axis=1)\n        embedding = pd.DataFrame(embedding, columns=embedding_header)\n        embedding.to_csv(self.args.embedding_output_path, index=None)\n\n    def save_persona_graph_mapping(self):\n        """"""\n        Saving the persona map.\n        """"""\n        with open(self.args.persona_output_path, ""w"") as f:\n            json.dump(self.egonet_splitter.personality_map, f)                     \n'"
src/utils.py,0,"b'""""""Data reading and printing utils.""""""\n\nimport pandas as pd\nimport networkx as nx\nfrom texttable import Texttable\n\ndef tab_printer(args):\n    """"""\n    Function to print the logs in a nice tabular format.\n    :param args: Parameters used for the model.\n    """"""\n    args = vars(args)\n    keys = sorted(args.keys())\n    t = Texttable()\n    t.add_rows([[""Parameter"", ""Value""]])\n    t.add_rows([[k.replace(""_"", "" "").capitalize(), args[k]] for k in keys])\n    print(t.draw())\n\ndef graph_reader(path):\n    """"""\n    Function to read the graph from the path.\n    :param path: Path to the edge list.\n    :return graph: NetworkX object returned.\n    """"""\n    graph = nx.from_edgelist(pd.read_csv(path).values.tolist())\n    graph.remove_edges_from(nx.selfloop_edges(graph))\n    return graph\n'"
src/walkers.py,0,"b'""""""DeepWalker class.""""""\n\nimport random\nimport numpy as np\nfrom tqdm import tqdm\nimport networkx as nx\nfrom gensim.models import Word2Vec\n\nclass DeepWalker(object):\n    """"""\n    DeepWalk node embedding learner object.\n    A barebones implementation of ""DeepWalk: Online Learning of Social Representations"".\n    Paper: https://arxiv.org/abs/1403.6652\n    Video: https://www.youtube.com/watch?v=aZNtHJwfIVg\n    """"""\n    def __init__(self, graph, args):\n        """"""\n        :param graph: NetworkX graph.\n        :param args: Arguments object.\n        """"""\n        self.graph = graph\n        self.args = args\n\n    def do_walk(self, node):\n        """"""\n        Doing a single truncated random walk from a source node.\n        :param node: Source node of the truncated random walk.\n        :return walk: A single random walk.\n        """"""\n        walk = [node]\n        while len(walk) < self.args.walk_length:\n            nebs = [n for n in nx.neighbors(self.graph, walk[-1])]\n            if len(nebs) == 0:\n                break\n            walk.append(random.choice(nebs))\n        return walk\n\n    def create_features(self):\n        """"""\n        Creating random walks from each node.\n        """"""\n        self.paths = []\n        for node in tqdm(self.graph.nodes()):\n            for _ in range(self.args.number_of_walks):\n                walk = self.do_walk(node)\n                self.paths.append(walk)\n\n    def learn_base_embedding(self):\n        """"""\n        Learning an embedding of nodes in the base graph.\n        :return self.embedding: Embedding of nodes in the latent space.\n        """"""\n        self.paths = [[str(node) for node in walk] for walk in self.paths]\n\n        model = Word2Vec(self.paths,\n                         size=self.args.dimensions,\n                         window=self.args.window_size,\n                         min_count=1,\n                         sg=1,\n                         workers=self.args.workers,\n                         iter=1)\n\n        self.embedding = np.array([list(model[str(n)]) for n in self.graph.nodes()])\n        return self.embedding\n'"
