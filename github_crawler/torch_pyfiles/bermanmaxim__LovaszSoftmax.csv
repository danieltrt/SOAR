file_path,api_count,code
demo_helpers/demo_utils.py,0,"b'""""""\nSome utils used in all demos\nMaxim Berman 2018 ESAT-PSI KU Leuven (MIT License)\n""""""\n\nfrom __future__ import print_function, division\nimport numpy as np\nfrom PIL import Image, ImageDraw\nimport contextlib\n\n\ndef paletteVOC(N=256, normalized=False, PIL=False):\n    """"""\n    Pascal VOC color map\n    """"""\n    def bitget(byteval, idx):\n        return ((byteval & (1 << idx)) != 0)\n\n    dtype = \'float32\' if normalized else \'uint8\'\n    cmap = np.zeros((N, 3), dtype=dtype)\n    for i in range(N):\n        r = g = b = 0\n        c = i\n        for j in range(8):\n            r = r | (bitget(c, 0) << 7-j)\n            g = g | (bitget(c, 1) << 7-j)\n            b = b | (bitget(c, 2) << 7-j)\n            c = c >> 3\n\n        cmap[i] = np.array([r, g, b])\n\n    cmap = cmap/255 if normalized else cmap\n    if PIL:\n        cmap = [k for l in cmap for k in l]\n    return cmap\n\n\ndef pil(array):\n    im = Image.fromarray(array)\n    im.putpalette(paletteVOC(PIL=True))\n    return im\n\n\ndef pil_grid(images, max_horiz=np.iinfo(int).max, margin=0, background=\'white\'):\n    """"""\n    Grid of images in PIL\n    """"""\n    n_images = len(images)\n    n_horiz = min(n_images, max_horiz)\n    h_sizes, v_sizes = [0] * n_horiz, [0] * (n_images // n_horiz)\n    for i, im in enumerate(images):\n        h, v = i % n_horiz, i // n_horiz\n        h_sizes[h] = max(h_sizes[h], im.size[0]) + margin\n        v_sizes[v] = max(v_sizes[v], im.size[1]) + margin\n    h_sizes, v_sizes = np.cumsum([0] + h_sizes), np.cumsum([0] + v_sizes)\n    im_grid = Image.new(\'RGB\', (h_sizes[-1], v_sizes[-1]), color=background)\n    for i, im in enumerate(images):\n        im_grid.paste(im, (h_sizes[i % n_horiz], v_sizes[i // n_horiz]))\n    return im_grid\n\n\ndef dummy_triangles(w, categories=[0, 255, 1]):\n    """"""\n    Generate random images with desired categories and random triangles\n    """"""\n    im = Image.new(\'P\', (w, w), color=categories[0])\n    im.putpalette(paletteVOC(PIL=True))\n    draw = ImageDraw.Draw(im)\n    for c in categories[1:]:\n        draw.polygon([tuple(p) for p in np.random.randint(w, size=(3, 2))], fill=c, outline=None)\n    return im\n\n\n# https://stackoverflow.com/questions/2891790/how-to-pretty-printing-a-numpy-array-without-scientific-notation-and-with-given\n@contextlib.contextmanager\ndef printoptions(*args, **kwargs):\n    original = np.get_printoptions()\n    np.set_printoptions(*args, **kwargs)\n    try:\n        yield\n    finally:\n        np.set_printoptions(**original)'"
demo_helpers/demo_utils_tf.py,0,"b'""""""\nSome utils used in tf demos\nMaxim Berman 2018 ESAT-PSI KU Leuven (MIT License)\n""""""\n\nfrom __future__ import print_function, division\nimport tensorflow as tf\n\n# https://danijar.com/structuring-your-tensorflow-models/\n\nimport functools\n\ndef doublewrap(function):\n    """"""\n    A decorator decorator, allowing to use the decorator to be used without\n    parentheses if not arguments are provided. All arguments must be optional.\n    """"""\n    @functools.wraps(function)\n    def decorator(*args, **kwargs):\n        if len(args) == 1 and len(kwargs) == 0 and callable(args[0]):\n            return function(args[0])\n        else:\n            return lambda wrapee: function(wrapee, *args, **kwargs)\n    return decorator\n\n\n@doublewrap\ndef define_scope(function, scope=None, *args, **kwargs):\n    """"""\n    A decorator for functions that define TensorFlow operations. The wrapped\n    function will only be executed once. Subsequent calls to it will directly\n    return the result so that operations are added to the graph only once.\n    The operations added by the function live within a tf.variable_scope(). If\n    this decorator is used with arguments, they will be forwarded to the\n    variable scope. The scope name defaults to the name of the wrapped\n    function.\n    """"""\n    attribute = \'_cache_\' + function.__name__\n    name = scope or function.__name__\n    @property\n    @functools.wraps(function)\n    def decorator(self):\n        if not hasattr(self, attribute):\n            with tf.variable_scope(name, *args, **kwargs):\n                setattr(self, attribute, function(self))\n        return getattr(self, attribute)\n    return decorator\n'"
pytorch/lovasz_losses.py,7,"b'""""""\nLovasz-Softmax and Jaccard hinge loss in PyTorch\nMaxim Berman 2018 ESAT-PSI KU Leuven (MIT License)\n""""""\n\nfrom __future__ import print_function, division\n\nimport torch\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nimport numpy as np\ntry:\n    from itertools import  ifilterfalse\nexcept ImportError: # py3k\n    from itertools import  filterfalse as ifilterfalse\n\n\ndef lovasz_grad(gt_sorted):\n    """"""\n    Computes gradient of the Lovasz extension w.r.t sorted errors\n    See Alg. 1 in paper\n    """"""\n    p = len(gt_sorted)\n    gts = gt_sorted.sum()\n    intersection = gts - gt_sorted.float().cumsum(0)\n    union = gts + (1 - gt_sorted).float().cumsum(0)\n    jaccard = 1. - intersection / union\n    if p > 1: # cover 1-pixel case\n        jaccard[1:p] = jaccard[1:p] - jaccard[0:-1]\n    return jaccard\n\n\ndef iou_binary(preds, labels, EMPTY=1., ignore=None, per_image=True):\n    """"""\n    IoU for foreground class\n    binary: 1 foreground, 0 background\n    """"""\n    if not per_image:\n        preds, labels = (preds,), (labels,)\n    ious = []\n    for pred, label in zip(preds, labels):\n        intersection = ((label == 1) & (pred == 1)).sum()\n        union = ((label == 1) | ((pred == 1) & (label != ignore))).sum()\n        if not union:\n            iou = EMPTY\n        else:\n            iou = float(intersection) / float(union)\n        ious.append(iou)\n    iou = mean(ious)    # mean accross images if per_image\n    return 100 * iou\n\n\ndef iou(preds, labels, C, EMPTY=1., ignore=None, per_image=False):\n    """"""\n    Array of IoU for each (non ignored) class\n    """"""\n    if not per_image:\n        preds, labels = (preds,), (labels,)\n    ious = []\n    for pred, label in zip(preds, labels):\n        iou = []    \n        for i in range(C):\n            if i != ignore: # The ignored label is sometimes among predicted classes (ENet - CityScapes)\n                intersection = ((label == i) & (pred == i)).sum()\n                union = ((label == i) | ((pred == i) & (label != ignore))).sum()\n                if not union:\n                    iou.append(EMPTY)\n                else:\n                    iou.append(float(intersection) / float(union))\n        ious.append(iou)\n    ious = [mean(iou) for iou in zip(*ious)] # mean accross images if per_image\n    return 100 * np.array(ious)\n\n\n# --------------------------- BINARY LOSSES ---------------------------\n\n\ndef lovasz_hinge(logits, labels, per_image=True, ignore=None):\n    """"""\n    Binary Lovasz hinge loss\n      logits: [B, H, W] Variable, logits at each pixel (between -\\infty and +\\infty)\n      labels: [B, H, W] Tensor, binary ground truth masks (0 or 1)\n      per_image: compute the loss per image instead of per batch\n      ignore: void class id\n    """"""\n    if per_image:\n        loss = mean(lovasz_hinge_flat(*flatten_binary_scores(log.unsqueeze(0), lab.unsqueeze(0), ignore))\n                          for log, lab in zip(logits, labels))\n    else:\n        loss = lovasz_hinge_flat(*flatten_binary_scores(logits, labels, ignore))\n    return loss\n\n\ndef lovasz_hinge_flat(logits, labels):\n    """"""\n    Binary Lovasz hinge loss\n      logits: [P] Variable, logits at each prediction (between -\\infty and +\\infty)\n      labels: [P] Tensor, binary ground truth labels (0 or 1)\n      ignore: label to ignore\n    """"""\n    if len(labels) == 0:\n        # only void pixels, the gradients should be 0\n        return logits.sum() * 0.\n    signs = 2. * labels.float() - 1.\n    errors = (1. - logits * Variable(signs))\n    errors_sorted, perm = torch.sort(errors, dim=0, descending=True)\n    perm = perm.data\n    gt_sorted = labels[perm]\n    grad = lovasz_grad(gt_sorted)\n    loss = torch.dot(F.relu(errors_sorted), Variable(grad))\n    return loss\n\n\ndef flatten_binary_scores(scores, labels, ignore=None):\n    """"""\n    Flattens predictions in the batch (binary case)\n    Remove labels equal to \'ignore\'\n    """"""\n    scores = scores.view(-1)\n    labels = labels.view(-1)\n    if ignore is None:\n        return scores, labels\n    valid = (labels != ignore)\n    vscores = scores[valid]\n    vlabels = labels[valid]\n    return vscores, vlabels\n\n\nclass StableBCELoss(torch.nn.modules.Module):\n    def __init__(self):\n         super(StableBCELoss, self).__init__()\n    def forward(self, input, target):\n         neg_abs = - input.abs()\n         loss = input.clamp(min=0) - input * target + (1 + neg_abs.exp()).log()\n         return loss.mean()\n\n\ndef binary_xloss(logits, labels, ignore=None):\n    """"""\n    Binary Cross entropy loss\n      logits: [B, H, W] Variable, logits at each pixel (between -\\infty and +\\infty)\n      labels: [B, H, W] Tensor, binary ground truth masks (0 or 1)\n      ignore: void class id\n    """"""\n    logits, labels = flatten_binary_scores(logits, labels, ignore)\n    loss = StableBCELoss()(logits, Variable(labels.float()))\n    return loss\n\n\n# --------------------------- MULTICLASS LOSSES ---------------------------\n\n\ndef lovasz_softmax(probas, labels, classes=\'present\', per_image=False, ignore=None):\n    """"""\n    Multi-class Lovasz-Softmax loss\n      probas: [B, C, H, W] Variable, class probabilities at each prediction (between 0 and 1).\n              Interpreted as binary (sigmoid) output with outputs of size [B, H, W].\n      labels: [B, H, W] Tensor, ground truth labels (between 0 and C - 1)\n      classes: \'all\' for all, \'present\' for classes present in labels, or a list of classes to average.\n      per_image: compute the loss per image instead of per batch\n      ignore: void class labels\n    """"""\n    if per_image:\n        loss = mean(lovasz_softmax_flat(*flatten_probas(prob.unsqueeze(0), lab.unsqueeze(0), ignore), classes=classes)\n                          for prob, lab in zip(probas, labels))\n    else:\n        loss = lovasz_softmax_flat(*flatten_probas(probas, labels, ignore), classes=classes)\n    return loss\n\n\ndef lovasz_softmax_flat(probas, labels, classes=\'present\'):\n    """"""\n    Multi-class Lovasz-Softmax loss\n      probas: [P, C] Variable, class probabilities at each prediction (between 0 and 1)\n      labels: [P] Tensor, ground truth labels (between 0 and C - 1)\n      classes: \'all\' for all, \'present\' for classes present in labels, or a list of classes to average.\n    """"""\n    if probas.numel() == 0:\n        # only void pixels, the gradients should be 0\n        return probas * 0.\n    C = probas.size(1)\n    losses = []\n    class_to_sum = list(range(C)) if classes in [\'all\', \'present\'] else classes\n    for c in class_to_sum:\n        fg = (labels == c).float() # foreground for class c\n        if (classes is \'present\' and fg.sum() == 0):\n            continue\n        if C == 1:\n            if len(classes) > 1:\n                raise ValueError(\'Sigmoid output possible only with 1 class\')\n            class_pred = probas[:, 0]\n        else:\n            class_pred = probas[:, c]\n        errors = (Variable(fg) - class_pred).abs()\n        errors_sorted, perm = torch.sort(errors, 0, descending=True)\n        perm = perm.data\n        fg_sorted = fg[perm]\n        losses.append(torch.dot(errors_sorted, Variable(lovasz_grad(fg_sorted))))\n    return mean(losses)\n\n\ndef flatten_probas(probas, labels, ignore=None):\n    """"""\n    Flattens predictions in the batch\n    """"""\n    if probas.dim() == 3:\n        # assumes output of a sigmoid layer\n        B, H, W = probas.size()\n        probas = probas.view(B, 1, H, W)\n    B, C, H, W = probas.size()\n    probas = probas.permute(0, 2, 3, 1).contiguous().view(-1, C)  # B * H * W, C = P, C\n    labels = labels.view(-1)\n    if ignore is None:\n        return probas, labels\n    valid = (labels != ignore)\n    vprobas = probas[valid.nonzero().squeeze()]\n    vlabels = labels[valid]\n    return vprobas, vlabels\n\ndef xloss(logits, labels, ignore=None):\n    """"""\n    Cross entropy loss\n    """"""\n    return F.cross_entropy(logits, Variable(labels), ignore_index=255)\n\n\n# --------------------------- HELPER FUNCTIONS ---------------------------\ndef isnan(x):\n    return x != x\n    \n    \ndef mean(l, ignore_nan=False, empty=0):\n    """"""\n    nanmean compatible with generators.\n    """"""\n    l = iter(l)\n    if ignore_nan:\n        l = ifilterfalse(isnan, l)\n    try:\n        n = 1\n        acc = next(l)\n    except StopIteration:\n        if empty == \'raise\':\n            raise ValueError(\'Empty mean\')\n        return empty\n    for n, v in enumerate(l, 2):\n        acc += v\n    if n == 1:\n        return acc\n    return acc / n\n'"
tensorflow/lovasz_losses_tf.py,0,"b'""""""\nLovasz-Softmax and Jaccard hinge loss in Tensorflow\nMaxim Berman 2018 ESAT-PSI KU Leuven (MIT License)\n""""""\n\nfrom __future__ import print_function, division\n\nimport tensorflow as tf\nimport numpy as np\n\n\ndef lovasz_grad(gt_sorted):\n    """"""\n    Computes gradient of the Lovasz extension w.r.t sorted errors\n    See Alg. 1 in paper\n    """"""\n    gts = tf.reduce_sum(gt_sorted)\n    intersection = gts - tf.cumsum(gt_sorted)\n    union = gts + tf.cumsum(1. - gt_sorted)\n    jaccard = 1. - intersection / union\n    jaccard = tf.concat((jaccard[0:1], jaccard[1:] - jaccard[:-1]), 0)\n    return jaccard\n\n\n# --------------------------- BINARY LOSSES ---------------------------\n\n\ndef lovasz_hinge(logits, labels, per_image=True, ignore=None):\n    """"""\n    Binary Lovasz hinge loss\n      logits: [B, H, W] Variable, logits at each pixel (between -\\infty and +\\infty)\n      labels: [B, H, W] Tensor, binary ground truth masks (0 or 1)\n      per_image: compute the loss per image instead of per batch\n      ignore: void class id\n    """"""\n    if per_image:\n        def treat_image(log_lab):\n            log, lab = log_lab\n            log, lab = tf.expand_dims(log, 0), tf.expand_dims(lab, 0)\n            log, lab = flatten_binary_scores(log, lab, ignore)\n            return lovasz_hinge_flat(log, lab)\n        losses = tf.map_fn(treat_image, (logits, labels), dtype=tf.float32)\n        loss = tf.reduce_mean(losses)\n    else:\n        loss = lovasz_hinge_flat(*flatten_binary_scores(logits, labels, ignore))\n    return loss\n\n\ndef lovasz_hinge_flat(logits, labels):\n    """"""\n    Binary Lovasz hinge loss\n      logits: [P] Variable, logits at each prediction (between -\\infty and +\\infty)\n      labels: [P] Tensor, binary ground truth labels (0 or 1)\n      ignore: label to ignore\n    """"""\n\n    def compute_loss():\n        labelsf = tf.cast(labels, logits.dtype)\n        signs = 2. * labelsf - 1.\n        errors = 1. - logits * tf.stop_gradient(signs)\n        errors_sorted, perm = tf.nn.top_k(errors, k=tf.shape(errors)[0], name=""descending_sort"")\n        gt_sorted = tf.gather(labelsf, perm)\n        grad = lovasz_grad(gt_sorted)\n        loss = tf.tensordot(tf.nn.relu(errors_sorted), tf.stop_gradient(grad), 1, name=""loss_non_void"")\n        return loss\n\n    # deal with the void prediction case (only void pixels)\n    loss = tf.cond(tf.equal(tf.shape(logits)[0], 0),\n                   lambda: tf.reduce_sum(logits) * 0.,\n                   compute_loss,\n                   strict=True,\n                   name=""loss""\n                   )\n    return loss\n\n\ndef flatten_binary_scores(scores, labels, ignore=None):\n    """"""\n    Flattens predictions in the batch (binary case)\n    Remove labels equal to \'ignore\'\n    """"""\n    scores = tf.reshape(scores, (-1,))\n    labels = tf.reshape(labels, (-1,))\n    if ignore is None:\n        return scores, labels\n    valid = tf.not_equal(labels, ignore)\n    vscores = tf.boolean_mask(scores, valid, name=\'valid_scores\')\n    vlabels = tf.boolean_mask(labels, valid, name=\'valid_labels\')\n    return vscores, vlabels\n\n\n# --------------------------- MULTICLASS LOSSES ---------------------------\n\n\ndef lovasz_softmax(probas, labels, classes=\'present\', per_image=False, ignore=None, order=\'BHWC\'):\n    """"""\n    Multi-class Lovasz-Softmax loss\n      probas: [B, H, W, C] or [B, C, H, W] Variable, class probabilities at each prediction (between 0 and 1)\n              Interpreted as binary (sigmoid) output with outputs of size [B, H, W].\n      labels: [B, H, W] Tensor, ground truth labels (between 0 and C - 1)\n      classes: \'all\' for all, \'present\' for classes present in labels, or a list of classes to average.\n      per_image: compute the loss per image instead of per batch\n      ignore: void class labels\n      order: use BHWC or BCHW\n    """"""\n    if per_image:\n        def treat_image(prob_lab):\n            prob, lab = prob_lab\n            prob, lab = tf.expand_dims(prob, 0), tf.expand_dims(lab, 0)\n            prob, lab = flatten_probas(prob, lab, ignore, order)\n            return lovasz_softmax_flat(prob, lab, classes=classes)\n        losses = tf.map_fn(treat_image, (probas, labels), dtype=tf.float32)\n        loss = tf.reduce_mean(losses)\n    else:\n        loss = lovasz_softmax_flat(*flatten_probas(probas, labels, ignore, order), classes=classes)\n    return loss\n\n\ndef lovasz_softmax_flat(probas, labels, classes=\'present\'):\n    """"""\n    Multi-class Lovasz-Softmax loss\n      probas: [P, C] Variable, class probabilities at each prediction (between 0 and 1)\n      labels: [P] Tensor, ground truth labels (between 0 and C - 1)\n      classes: \'all\' for all, \'present\' for classes present in labels, or a list of classes to average.\n    """"""\n    C = probas.shape[1]\n    losses = []\n    present = []\n    class_to_sum = list(range(C)) if classes in [\'all\', \'present\'] else classes\n    for c in class_to_sum:\n        fg = tf.cast(tf.equal(labels, c), probas.dtype)  # foreground for class c\n        if classes == \'present\':\n            present.append(tf.reduce_sum(fg) > 0)\n        if C == 1:\n            if len(classes) > 1:\n                raise ValueError(\'Sigmoid output possible only with 1 class\')\n            class_pred = probas[:, 0]\n        else:\n            class_pred = probas[:, c]\n        errors = tf.abs(fg - class_pred)\n        errors_sorted, perm = tf.nn.top_k(errors, k=tf.shape(errors)[0], name=""descending_sort_{}"".format(c))\n        fg_sorted = tf.gather(fg, perm)\n        grad = lovasz_grad(fg_sorted)\n        losses.append(\n            tf.tensordot(errors_sorted, tf.stop_gradient(grad), 1, name=""loss_class_{}"".format(c))\n                      )\n    if len(class_to_sum) == 1:  # short-circuit mean when only one class\n        return losses[0]\n    losses_tensor = tf.stack(losses)\n    if classes == \'present\':\n        present = tf.stack(present)\n        losses_tensor = tf.boolean_mask(losses_tensor, present)\n    loss = tf.reduce_mean(losses_tensor)\n    return loss\n\n\ndef flatten_probas(probas, labels, ignore=None, order=\'BHWC\'):\n    """"""\n    Flattens predictions in the batch\n    """"""\n    if len(probas.shape) == 3:\n        probas, order = tf.expand_dims(probas, 3), \'BHWC\'\n    if order == \'BCHW\':\n        probas = tf.transpose(probas, (0, 2, 3, 1), name=""BCHW_to_BHWC"")\n        order = \'BHWC\'\n    if order != \'BHWC\':\n        raise NotImplementedError(\'Order {} unknown\'.format(order))\n    C = probas.shape[3]\n    probas = tf.reshape(probas, (-1, C))\n    labels = tf.reshape(labels, (-1,))\n    if ignore is None:\n        return probas, labels\n    valid = tf.not_equal(labels, ignore)\n    vprobas = tf.boolean_mask(probas, valid, name=\'valid_probas\')\n    vlabels = tf.boolean_mask(labels, valid, name=\'valid_labels\')\n    return vprobas, vlabels\n'"
