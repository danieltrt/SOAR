file_path,api_count,code
datasets/__init__.py,0,b''
datasets/camvid.py,5,"b'import os\nimport torch\nimport torch.utils.data as data\nimport numpy as np\nfrom PIL import Image\nfrom torchvision.datasets.folder import is_image_file, default_loader\n\n\nclasses = [\'Sky\', \'Building\', \'Column-Pole\', \'Road\',\n           \'Sidewalk\', \'Tree\', \'Sign-Symbol\', \'Fence\', \'Car\', \'Pedestrain\',\n           \'Bicyclist\', \'Void\']\n\n# https://github.com/yandex/segnet-torch/blob/master/datasets/camvid-gen.lua\nclass_weight = torch.FloatTensor([\n    0.58872014284134, 0.51052379608154, 2.6966278553009,\n    0.45021694898605, 1.1785038709641, 0.77028578519821, 2.4782588481903,\n    2.5273461341858, 1.0122526884079, 3.2375309467316, 4.1312313079834, 0])\n\nmean = [0.41189489566336, 0.4251328133025, 0.4326707089857]\nstd = [0.27413549931506, 0.28506257482912, 0.28284674400252]\n\nclass_color = [\n    (128, 128, 128),\n    (128, 0, 0),\n    (192, 192, 128),\n    (128, 64, 128),\n    (0, 0, 192),\n    (128, 128, 0),\n    (192, 128, 128),\n    (64, 64, 128),\n    (64, 0, 128),\n    (64, 64, 0),\n    (0, 128, 192),\n    (0, 0, 0),\n]\n\n\ndef _make_dataset(dir):\n    images = []\n    for root, _, fnames in sorted(os.walk(dir)):\n        for fname in fnames:\n            if is_image_file(fname):\n                path = os.path.join(root, fname)\n                item = path\n                images.append(item)\n    return images\n\n\nclass LabelToLongTensor(object):\n    def __call__(self, pic):\n        if isinstance(pic, np.ndarray):\n            # handle numpy array\n            label = torch.from_numpy(pic).long()\n        else:\n            label = torch.ByteTensor(torch.ByteStorage.from_buffer(pic.tobytes()))\n            label = label.view(pic.size[1], pic.size[0], 1)\n            label = label.transpose(0, 1).transpose(0, 2).squeeze().contiguous().long()\n        return label\n\n\nclass LabelTensorToPILImage(object):\n    def __call__(self, label):\n        label = label.unsqueeze(0)\n        colored_label = torch.zeros(3, label.size(1), label.size(2)).byte()\n        for i, color in enumerate(class_color):\n            mask = label.eq(i)\n            for j in range(3):\n                colored_label[j].masked_fill_(mask, color[j])\n        npimg = colored_label.numpy()\n        npimg = np.transpose(npimg, (1, 2, 0))\n        mode = None\n        if npimg.shape[2] == 1:\n            npimg = npimg[:, :, 0]\n            mode = ""L""\n\n        return Image.fromarray(npimg, mode=mode)\n\n\nclass CamVid(data.Dataset):\n\n    def __init__(self, root, split=\'train\', joint_transform=None,\n                 transform=None, target_transform=LabelToLongTensor(),\n                 download=False,\n                 loader=default_loader):\n        self.root = root\n        assert split in (\'train\', \'val\', \'test\')\n        self.split = split\n        self.transform = transform\n        self.target_transform = target_transform\n        self.joint_transform = joint_transform\n        self.loader = loader\n        self.class_weight = class_weight\n        self.classes = classes\n        self.mean = mean\n        self.std = std\n\n        if download:\n            self.download()\n\n        self.imgs = _make_dataset(os.path.join(self.root, self.split))\n\n    def __getitem__(self, index):\n        path = self.imgs[index]\n        img = self.loader(path)\n        target = Image.open(path.replace(self.split, self.split + \'annot\'))\n\n        if self.joint_transform is not None:\n            img, target = self.joint_transform([img, target])\n\n        if self.transform is not None:\n            img = self.transform(img)\n\n        target = self.target_transform(target)\n        return img, target\n\n    def __len__(self):\n        return len(self.imgs)\n\n    def download(self):\n        # TODO: please download the dataset from\n        # https://github.com/alexgkendall/SegNet-Tutorial/tree/master/CamVid\n        raise NotImplementedError\n'"
datasets/joint_transforms.py,0,"b'from __future__ import division\nimport torch\nimport math\nimport random\nfrom PIL import Image, ImageOps\nimport numpy as np\nimport numbers\nimport types\n\n\nclass JointScale(object):\n    """"""Rescales the input PIL.Image to the given \'size\'.\n    \'size\' will be the size of the smaller edge.\n    For example, if height > width, then image will be\n    rescaled to (size * height / width, size)\n    size: size of the smaller edge\n    interpolation: Default: PIL.Image.BILINEAR\n    """"""\n\n    def __init__(self, size, interpolation=Image.BILINEAR):\n        self.size = size\n        self.interpolation = interpolation\n\n    def __call__(self, imgs):\n        w, h = imgs[0].size\n        if (w <= h and w == self.size) or (h <= w and h == self.size):\n            return imgs\n        if w < h:\n            ow = self.size\n            oh = int(self.size * h / w)\n            return [img.resize((ow, oh), self.interpolation) for img in imgs]\n        else:\n            oh = self.size\n            ow = int(self.size * w / h)\n            return [img.resize((ow, oh), self.interpolation) for img in imgs]\n\n\nclass JointCenterCrop(object):\n    """"""Crops the given PIL.Image at the center to have a region of\n    the given size. size can be a tuple (target_height, target_width)\n    or an integer, in which case the target will be of a square shape (size, size)\n    """"""\n\n    def __init__(self, size):\n        if isinstance(size, numbers.Number):\n            self.size = (int(size), int(size))\n        else:\n            self.size = size\n\n    def __call__(self, imgs):\n        w, h = imgs[0].size\n        th, tw = self.size\n        x1 = int(round((w - tw) / 2.))\n        y1 = int(round((h - th) / 2.))\n        return [img.crop((x1, y1, x1 + tw, y1 + th)) for img in imgs]\n\n\nclass JointPad(object):\n    """"""Pads the given PIL.Image on all sides with the given ""pad"" value""""""\n\n    def __init__(self, padding, fill=0):\n        assert isinstance(padding, numbers.Number)\n        assert isinstance(fill, numbers.Number) or isinstance(fill, str) or isinstance(fill, tuple)\n        self.padding = padding\n        self.fill = fill\n\n    def __call__(self, imgs):\n        return [ImageOps.expand(img, border=self.padding, fill=self.fill) for img in imgs]\n\n\nclass JointLambda(object):\n    """"""Applies a lambda as a transform.""""""\n\n    def __init__(self, lambd):\n        assert isinstance(lambd, types.LambdaType)\n        self.lambd = lambd\n\n    def __call__(self, imgs):\n        return [self.lambd(img) for img in imgs]\n\n\nclass JointRandomCrop(object):\n    """"""Crops the given list of PIL.Image at a random location to have a region of\n    the given size. size can be a tuple (target_height, target_width)\n    or an integer, in which case the target will be of a square shape (size, size)\n    """"""\n\n    def __init__(self, size, padding=0):\n        if isinstance(size, numbers.Number):\n            self.size = (int(size), int(size))\n        else:\n            self.size = size\n        self.padding = padding\n\n    def __call__(self, imgs):\n        if self.padding > 0:\n            imgs = [ImageOps.expand(img, border=self.padding, fill=0) for img in imgs]\n\n        w, h = imgs[0].size\n        th, tw = self.size\n        if w == tw and h == th:\n            return imgs\n\n        x1 = random.randint(0, w - tw)\n        y1 = random.randint(0, h - th)\n        return [img.crop((x1, y1, x1 + tw, y1 + th)) for img in imgs]\n\n\nclass JointRandomHorizontalFlip(object):\n    """"""Randomly horizontally flips the given list of PIL.Image with a probability of 0.5\n    """"""\n\n    def __call__(self, imgs):\n        if random.random() < 0.5:\n            return [img.transpose(Image.FLIP_LEFT_RIGHT) for img in imgs]\n        return imgs\n\n\nclass JointRandomSizedCrop(object):\n    """"""Random crop the given list of PIL.Image to a random size of (0.08 to 1.0) of the original size\n    and and a random aspect ratio of 3/4 to 4/3 of the original aspect ratio\n    This is popularly used to train the Inception networks\n    size: size of the smaller edge\n    interpolation: Default: PIL.Image.BILINEAR\n    """"""\n\n    def __init__(self, size, interpolation=Image.BILINEAR):\n        self.size = size\n        self.interpolation = interpolation\n\n    def __call__(self, imgs):\n        for attempt in range(10):\n            area = imgs[0].size[0] * imgs[0].size[1]\n            target_area = random.uniform(0.08, 1.0) * area\n            aspect_ratio = random.uniform(3. / 4, 4. / 3)\n\n            w = int(round(math.sqrt(target_area * aspect_ratio)))\n            h = int(round(math.sqrt(target_area / aspect_ratio)))\n\n            if random.random() < 0.5:\n                w, h = h, w\n\n            if w <= imgs[0].size[0] and h <= imgs[0].size[1]:\n                x1 = random.randint(0, imgs[0].size[0] - w)\n                y1 = random.randint(0, imgs[0].size[1] - h)\n\n                imgs = [img.crop((x1, y1, x1 + w, y1 + h)) for img in imgs]\n                assert(imgs[0].size == (w, h))\n\n                return [img.resize((self.size, self.size), self.interpolation) for img in imgs]\n\n        # Fallback\n        scale = JointScale(self.size, interpolation=self.interpolation)\n        crop = JointCenterCrop(self.size)\n        return crop(scale(imgs))\n'"
models/__init__.py,0,b''
models/layers.py,5,"b""import torch\nimport torch.nn as nn\n\n\nclass DenseLayer(nn.Sequential):\n    def __init__(self, in_channels, growth_rate):\n        super().__init__()\n        self.add_module('norm', nn.BatchNorm2d(in_channels))\n        self.add_module('relu', nn.ReLU(True))\n        self.add_module('conv', nn.Conv2d(in_channels, growth_rate, kernel_size=3,\n                                          stride=1, padding=1, bias=True))\n        self.add_module('drop', nn.Dropout2d(0.2))\n\n    def forward(self, x):\n        return super().forward(x)\n\n\nclass DenseBlock(nn.Module):\n    def __init__(self, in_channels, growth_rate, n_layers, upsample=False):\n        super().__init__()\n        self.upsample = upsample\n        self.layers = nn.ModuleList([DenseLayer(\n            in_channels + i*growth_rate, growth_rate)\n            for i in range(n_layers)])\n\n    def forward(self, x):\n        if self.upsample:\n            new_features = []\n            #we pass all previous activations into each dense layer normally\n            #But we only store each dense layer's output in the new_features array\n            for layer in self.layers:\n                out = layer(x)\n                x = torch.cat([x, out], 1)\n                new_features.append(out)\n            return torch.cat(new_features,1)\n        else:\n            for layer in self.layers:\n                out = layer(x)\n                x = torch.cat([x, out], 1) # 1 = channel axis\n            return x\n\n\nclass TransitionDown(nn.Sequential):\n    def __init__(self, in_channels):\n        super().__init__()\n        self.add_module('norm', nn.BatchNorm2d(num_features=in_channels))\n        self.add_module('relu', nn.ReLU(inplace=True))\n        self.add_module('conv', nn.Conv2d(in_channels, in_channels,\n                                          kernel_size=1, stride=1,\n                                          padding=0, bias=True))\n        self.add_module('drop', nn.Dropout2d(0.2))\n        self.add_module('maxpool', nn.MaxPool2d(2))\n\n    def forward(self, x):\n        return super().forward(x)\n\n\nclass TransitionUp(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.convTrans = nn.ConvTranspose2d(\n            in_channels=in_channels, out_channels=out_channels,\n            kernel_size=3, stride=2, padding=0, bias=True)\n\n    def forward(self, x, skip):\n        out = self.convTrans(x)\n        out = center_crop(out, skip.size(2), skip.size(3))\n        out = torch.cat([out, skip], 1)\n        return out\n\n\nclass Bottleneck(nn.Sequential):\n    def __init__(self, in_channels, growth_rate, n_layers):\n        super().__init__()\n        self.add_module('bottleneck', DenseBlock(\n            in_channels, growth_rate, n_layers, upsample=True))\n\n    def forward(self, x):\n        return super().forward(x)\n\n\ndef center_crop(layer, max_height, max_width):\n    _, _, h, w = layer.size()\n    xy1 = (w - max_width) // 2\n    xy2 = (h - max_height) // 2\n    return layer[:, :, xy2:(xy2 + max_height), xy1:(xy1 + max_width)]\n"""
models/tiramisu.py,1,"b""import torch\nimport torch.nn as nn\n\nfrom .layers import *\n\n\nclass FCDenseNet(nn.Module):\n    def __init__(self, in_channels=3, down_blocks=(5,5,5,5,5),\n                 up_blocks=(5,5,5,5,5), bottleneck_layers=5,\n                 growth_rate=16, out_chans_first_conv=48, n_classes=12):\n        super().__init__()\n        self.down_blocks = down_blocks\n        self.up_blocks = up_blocks\n        cur_channels_count = 0\n        skip_connection_channel_counts = []\n\n        ## First Convolution ##\n\n        self.add_module('firstconv', nn.Conv2d(in_channels=in_channels,\n                  out_channels=out_chans_first_conv, kernel_size=3,\n                  stride=1, padding=1, bias=True))\n        cur_channels_count = out_chans_first_conv\n\n        #####################\n        # Downsampling path #\n        #####################\n\n        self.denseBlocksDown = nn.ModuleList([])\n        self.transDownBlocks = nn.ModuleList([])\n        for i in range(len(down_blocks)):\n            self.denseBlocksDown.append(\n                DenseBlock(cur_channels_count, growth_rate, down_blocks[i]))\n            cur_channels_count += (growth_rate*down_blocks[i])\n            skip_connection_channel_counts.insert(0,cur_channels_count)\n            self.transDownBlocks.append(TransitionDown(cur_channels_count))\n\n        #####################\n        #     Bottleneck    #\n        #####################\n\n        self.add_module('bottleneck',Bottleneck(cur_channels_count,\n                                     growth_rate, bottleneck_layers))\n        prev_block_channels = growth_rate*bottleneck_layers\n        cur_channels_count += prev_block_channels\n\n        #######################\n        #   Upsampling path   #\n        #######################\n\n        self.transUpBlocks = nn.ModuleList([])\n        self.denseBlocksUp = nn.ModuleList([])\n        for i in range(len(up_blocks)-1):\n            self.transUpBlocks.append(TransitionUp(prev_block_channels, prev_block_channels))\n            cur_channels_count = prev_block_channels + skip_connection_channel_counts[i]\n\n            self.denseBlocksUp.append(DenseBlock(\n                cur_channels_count, growth_rate, up_blocks[i],\n                    upsample=True))\n            prev_block_channels = growth_rate*up_blocks[i]\n            cur_channels_count += prev_block_channels\n\n        ## Final DenseBlock ##\n\n        self.transUpBlocks.append(TransitionUp(\n            prev_block_channels, prev_block_channels))\n        cur_channels_count = prev_block_channels + skip_connection_channel_counts[-1]\n\n        self.denseBlocksUp.append(DenseBlock(\n            cur_channels_count, growth_rate, up_blocks[-1],\n                upsample=False))\n        cur_channels_count += growth_rate*up_blocks[-1]\n\n        ## Softmax ##\n\n        self.finalConv = nn.Conv2d(in_channels=cur_channels_count,\n               out_channels=n_classes, kernel_size=1, stride=1,\n                   padding=0, bias=True)\n        self.softmax = nn.LogSoftmax(dim=1)\n\n    def forward(self, x):\n        out = self.firstconv(x)\n\n        skip_connections = []\n        for i in range(len(self.down_blocks)):\n            out = self.denseBlocksDown[i](out)\n            skip_connections.append(out)\n            out = self.transDownBlocks[i](out)\n\n        out = self.bottleneck(out)\n        for i in range(len(self.up_blocks)):\n            skip = skip_connections.pop()\n            out = self.transUpBlocks[i](out, skip)\n            out = self.denseBlocksUp[i](out)\n\n        out = self.finalConv(out)\n        out = self.softmax(out)\n        return out\n\n\ndef FCDenseNet57(n_classes):\n    return FCDenseNet(\n        in_channels=3, down_blocks=(4, 4, 4, 4, 4),\n        up_blocks=(4, 4, 4, 4, 4), bottleneck_layers=4,\n        growth_rate=12, out_chans_first_conv=48, n_classes=n_classes)\n\n\ndef FCDenseNet67(n_classes):\n    return FCDenseNet(\n        in_channels=3, down_blocks=(5, 5, 5, 5, 5),\n        up_blocks=(5, 5, 5, 5, 5), bottleneck_layers=5,\n        growth_rate=16, out_chans_first_conv=48, n_classes=n_classes)\n\n\ndef FCDenseNet103(n_classes):\n    return FCDenseNet(\n        in_channels=3, down_blocks=(4,5,7,10,12),\n        up_blocks=(12,10,7,5,4), bottleneck_layers=15,\n        growth_rate=16, out_chans_first_conv=48, n_classes=n_classes)\n"""
utils/__init__.py,0,b''
utils/imgs.py,0,"b'import numpy as np\nimport matplotlib.pyplot as plt\n\nSky = [128,128,128]\nBuilding = [128,0,0]\nPole = [192,192,128]\nRoad = [128,64,128]\nPavement = [60,40,222]\nTree = [128,128,0]\nSignSymbol = [192,128,128]\nFence = [64,64,128]\nCar = [64,0,128]\nPedestrian = [64,64,0]\nBicyclist = [0,128,192]\nUnlabelled = [0,0,0]\n\nDSET_MEAN = [0.41189489566336, 0.4251328133025, 0.4326707089857]\nDSET_STD = [0.27413549931506, 0.28506257482912, 0.28284674400252]\n\nlabel_colours = np.array([Sky, Building, Pole, Road, Pavement,\n      Tree, SignSymbol, Fence, Car, Pedestrian, Bicyclist, Unlabelled])\n\n\ndef view_annotated(tensor, plot=True):\n    temp = tensor.numpy()\n    r = temp.copy()\n    g = temp.copy()\n    b = temp.copy()\n    for l in range(0,11):\n        r[temp==l]=label_colours[l,0]\n        g[temp==l]=label_colours[l,1]\n        b[temp==l]=label_colours[l,2]\n\n    rgb = np.zeros((temp.shape[0], temp.shape[1], 3))\n    rgb[:,:,0] = (r/255.0)#[:,:,0]\n    rgb[:,:,1] = (g/255.0)#[:,:,1]\n    rgb[:,:,2] = (b/255.0)#[:,:,2]\n    if plot:\n        plt.imshow(rgb)\n        plt.show()\n    else:\n        return rgb\n\ndef decode_image(tensor):\n    inp = tensor.numpy().transpose((1, 2, 0))\n    mean = np.array(DSET_MEAN)\n    std = np.array(DSET_STD)\n    inp = std * inp + mean\n    return inp\n\ndef view_image(tensor):\n    inp = decode_image(tensor)\n    inp = np.clip(inp, 0, 1)\n    plt.imshow(inp)\n    plt.show()\n'"
utils/training.py,5,"b'import os\nimport sys\nimport math\nimport string\nimport random\nimport shutil\n\nimport torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nfrom torchvision.utils import save_image\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\n\nfrom . import imgs as img_utils\n\nRESULTS_PATH = \'.results/\'\nWEIGHTS_PATH = \'.weights/\'\n\n\ndef save_weights(model, epoch, loss, err):\n    weights_fname = \'weights-%d-%.3f-%.3f.pth\' % (epoch, loss, err)\n    weights_fpath = os.path.join(WEIGHTS_PATH, weights_fname)\n    torch.save({\n            \'startEpoch\': epoch,\n            \'loss\':loss,\n            \'error\': err,\n            \'state_dict\': model.state_dict()\n        }, weights_fpath)\n    shutil.copyfile(weights_fpath, WEIGHTS_PATH+\'latest.th\')\n\ndef load_weights(model, fpath):\n    print(""loading weights \'{}\'"".format(fpath))\n    weights = torch.load(fpath)\n    startEpoch = weights[\'startEpoch\']\n    model.load_state_dict(weights[\'state_dict\'])\n    print(""loaded weights (lastEpoch {}, loss {}, error {})""\n          .format(startEpoch-1, weights[\'loss\'], weights[\'error\']))\n    return startEpoch\n\ndef get_predictions(output_batch):\n    bs,c,h,w = output_batch.size()\n    tensor = output_batch.data\n    values, indices = tensor.cpu().max(1)\n    indices = indices.view(bs,h,w)\n    return indices\n\ndef error(preds, targets):\n    assert preds.size() == targets.size()\n    bs,h,w = preds.size()\n    n_pixels = bs*h*w\n    incorrect = preds.ne(targets).cpu().sum()\n    err = incorrect/n_pixels\n    return round(err,5)\n\ndef train(model, trn_loader, optimizer, criterion, epoch):\n    model.train()\n    trn_loss = 0\n    trn_error = 0\n    for idx, data in enumerate(trn_loader):\n        inputs = Variable(data[0].cuda())\n        targets = Variable(data[1].cuda())\n\n        optimizer.zero_grad()\n        output = model(inputs)\n        loss = criterion(output, targets)\n        loss.backward()\n        optimizer.step()\n\n        trn_loss += loss.data[0]\n        pred = get_predictions(output)\n        trn_error += error(pred, targets.data.cpu())\n\n    trn_loss /= len(trn_loader)\n    trn_error /= len(trn_loader)\n    return trn_loss, trn_error\n\ndef test(model, test_loader, criterion, epoch=1):\n    model.eval()\n    test_loss = 0\n    test_error = 0\n    for data, target in test_loader:\n        data = Variable(data.cuda(), volatile=True)\n        target = Variable(target.cuda())\n        output = model(data)\n        test_loss += criterion(output, target).data[0]\n        pred = get_predictions(output)\n        test_error += error(pred, target.data.cpu())\n    test_loss /= len(test_loader)\n    test_error /= len(test_loader)\n    return test_loss, test_error\n\ndef adjust_learning_rate(lr, decay, optimizer, cur_epoch, n_epochs):\n    """"""Sets the learning rate to the initially\n        configured `lr` decayed by `decay` every `n_epochs`""""""\n    new_lr = lr * (decay ** (cur_epoch // n_epochs))\n    for param_group in optimizer.param_groups:\n        param_group[\'lr\'] = new_lr\n\ndef weights_init(m):\n    if isinstance(m, nn.Conv2d):\n        nn.init.kaiming_uniform(m.weight)\n        m.bias.data.zero_()\n\ndef predict(model, input_loader, n_batches=1):\n    input_loader.batch_size = 1\n    predictions = []\n    model.eval()\n    for input, target in input_loader:\n        data = Variable(input.cuda(), volatile=True)\n        label = Variable(target.cuda())\n        output = model(data)\n        pred = get_predictions(output)\n        predictions.append([input,target,pred])\n    return predictions\n\ndef view_sample_predictions(model, loader, n):\n    inputs, targets = next(iter(loader))\n    data = Variable(inputs.cuda(), volatile=True)\n    label = Variable(targets.cuda())\n    output = model(data)\n    pred = get_predictions(output)\n    batch_size = inputs.size(0)\n    for i in range(min(n, batch_size)):\n        img_utils.view_image(inputs[i])\n        img_utils.view_annotated(targets[i])\n        img_utils.view_annotated(pred[i])\n'"
