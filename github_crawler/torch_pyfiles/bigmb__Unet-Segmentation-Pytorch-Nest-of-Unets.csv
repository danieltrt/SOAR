file_path,api_count,code
2d_from_3d.py,0,"b""import cv2\nimport scipy.misc\n\nimport SimpleITK as sitk #reading MR images\n\nimport glob\n\n\nreadfolderT = glob.glob('/home/bat161/Desktop/Thesis/EADC_HHP/*_MNI.nii.gz')\nreadfolderL = glob.glob('/home/bat161/Desktop/Thesis/EADC_HHP/*_HHP_EADC.nii.gz')\n\n\nTrainingImagesList = []\nTrainingLabelsList = []\n\n\nfor i in range(len(readfolderT)):\n    y_folder = readfolderT[i]\n    yread = sitk.ReadImage(y_folder)\n    yimage = sitk.GetArrayFromImage(yread)\n    x = yimage[:184,:232,112:136]\n    x = scipy.rot90(x)\n    x = scipy.rot90(x)\n    for j in range(x.shape[2]):\n        TrainingImagesList.append((x[:184,:224,j]))\n\nfor i in range(len(readfolderL)):\n    y_folder = readfolderL[i]\n    yread = sitk.ReadImage(y_folder)\n    yimage = sitk.GetArrayFromImage(yread)\n    x = yimage[:184,:232,112:136]\n    x = scipy.rot90(x)\n    x = scipy.rot90(x)\n    for j in range(x.shape[2]):\n        TrainingLabelsList.append((x[:184,:224,j]))\n\nfor i in range(len(TrainingImagesList)):\n\n    xchangeL = TrainingImagesList[i]\n    xchangeL = cv2.resize(xchangeL,(128,128))\n    scipy.misc.imsave('/home/bat161/Desktop/Thesis/Image/png_1C_images/'+str(i)+'.png',xchangeL)\n\nfor i in range(len(TrainingLabelsList)):\n\n    xchangeL = TrainingLabelsList[i]\n    xchangeL = cv2.resize(xchangeL,(128,128))\n    scipy.misc.imsave('/home/bat161/Desktop/Thesis/Image/png_1C_labels/'+str(i)+'.png',xchangeL)"""
Data_Loader.py,6,"b'from __future__ import print_function, division\nimport os\nfrom PIL import Image\nimport torch\nimport torch.utils.data\nimport torchvision\nfrom skimage import io\nfrom torch.utils.data import Dataset\nimport random\nimport numpy as np\n\n\nclass Images_Dataset(Dataset):\n    """"""Class for getting data as a Dict\n    Args:\n        images_dir = path of input images\n        labels_dir = path of labeled images\n        transformI = Input Images transformation (default: None)\n        transformM = Input Labels transformation (default: None)\n    Output:\n        sample : Dict of images and labels""""""\n\n    def __init__(self, images_dir, labels_dir, transformI = None, transformM = None):\n\n        self.labels_dir = labels_dir\n        self.images_dir = images_dir\n        self.transformI = transformI\n        self.transformM = transformM\n\n    def __len__(self):\n        return len(self.images_dir)\n\n    def __getitem__(self, idx):\n\n        for i in range(len(self.images_dir)):\n            image = io.imread(self.images_dir[i])\n            label = io.imread(self.labels_dir[i])\n            if self.transformI:\n                image = self.transformI(image)\n            if self.transformM:\n                label = self.transformM(label)\n            sample = {\'images\': image, \'labels\': label}\n\n        return sample\n\n\nclass Images_Dataset_folder(torch.utils.data.Dataset):\n    """"""Class for getting individual transformations and data\n    Args:\n        images_dir = path of input images\n        labels_dir = path of labeled images\n        transformI = Input Images transformation (default: None)\n        transformM = Input Labels transformation (default: None)\n    Output:\n        tx = Transformed images\n        lx = Transformed labels""""""\n\n    def __init__(self, images_dir, labels_dir,transformI = None, transformM = None):\n        self.images = sorted(os.listdir(images_dir))\n        self.labels = sorted(os.listdir(labels_dir))\n        self.images_dir = images_dir\n        self.labels_dir = labels_dir\n        self.transformI = transformI\n        self.transformM = transformM\n\n        if self.transformI:\n            self.tx = self.transformI\n        else:\n            self.tx = torchvision.transforms.Compose([\n              #  torchvision.transforms.Resize((128,128)),\n                torchvision.transforms.CenterCrop(96),\n                torchvision.transforms.RandomRotation((-10,10)),\n               # torchvision.transforms.RandomHorizontalFlip(),\n                torchvision.transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4),\n                torchvision.transforms.ToTensor(),\n                torchvision.transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n            ])\n\n        if self.transformM:\n            self.lx = self.transformM\n        else:\n            self.lx = torchvision.transforms.Compose([\n              #  torchvision.transforms.Resize((128,128)),\n                torchvision.transforms.CenterCrop(96),\n                torchvision.transforms.RandomRotation((-10,10)),\n                torchvision.transforms.Grayscale(),\n                torchvision.transforms.ToTensor(),\n                #torchvision.transforms.Lambda(lambda x: torch.cat([x, 1 - x], dim=0))\n            ])\n\n    def __len__(self):\n\n        return len(self.images)\n\n    def __getitem__(self, i):\n        i1 = Image.open(self.images_dir + self.images[i])\n        l1 = Image.open(self.labels_dir + self.labels[i])\n\n        seed=np.random.randint(0,2**32) # make a seed with numpy generator \n\n        # apply this seed to img tranfsorms\n        random.seed(seed) \n        torch.manual_seed(seed)\n        img = self.tx(i1)\n        \n        # apply this seed to target/label tranfsorms  \n        random.seed(seed) \n        torch.manual_seed(seed)\n        label = self.lx(l1)\n\n        \n\n        return img, label\n\n'"
Metrics.py,0,"b'import numpy as np\nfrom scipy import spatial\n\n\ndef dice_coeff(im1, im2, empty_score=1.0):\n    """"""Calculates the dice coefficient for the images""""""\n\n    im1 = np.asarray(im1).astype(np.bool)\n    im2 = np.asarray(im2).astype(np.bool)\n\n    if im1.shape != im2.shape:\n        raise ValueError(""Shape mismatch: im1 and im2 must have the same shape."")\n\n    im1 = im1 > 0.5\n    im2 = im2 > 0.5\n\n    im_sum = im1.sum() + im2.sum()\n    if im_sum == 0:\n        return empty_score\n\n    # Compute Dice coefficient\n    intersection = np.logical_and(im1, im2)\n    #print(im_sum)\n\n    return 2. * intersection.sum() / im_sum\n\n\ndef numeric_score(prediction, groundtruth):\n    """"""Computes scores:\n    FP = False Positives\n    FN = False Negatives\n    TP = True Positives\n    TN = True Negatives\n    return: FP, FN, TP, TN""""""\n\n    FP = np.float(np.sum((prediction == 1) & (groundtruth == 0)))\n    FN = np.float(np.sum((prediction == 0) & (groundtruth == 1)))\n    TP = np.float(np.sum((prediction == 1) & (groundtruth == 1)))\n    TN = np.float(np.sum((prediction == 0) & (groundtruth == 0)))\n\n    return FP, FN, TP, TN\n\n\ndef accuracy_score(prediction, groundtruth):\n    """"""Getting the accuracy of the model""""""\n\n    FP, FN, TP, TN = numeric_score(prediction, groundtruth)\n    N = FP + FN + TP + TN\n    accuracy = np.divide(TP + TN, N)\n    return accuracy * 100.0'"
Models.py,39,"b'from __future__ import print_function, division\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.data\nimport torch\n\n\nclass conv_block(nn.Module):\n    """"""\n    Convolution Block \n    """"""\n    def __init__(self, in_ch, out_ch):\n        super(conv_block, self).__init__()\n        \n        self.conv = nn.Sequential(\n            nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=True),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=True),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(inplace=True))\n\n    def forward(self, x):\n\n        x = self.conv(x)\n        return x\n\n\nclass up_conv(nn.Module):\n    """"""\n    Up Convolution Block\n    """"""\n    def __init__(self, in_ch, out_ch):\n        super(up_conv, self).__init__()\n        self.up = nn.Sequential(\n            nn.Upsample(scale_factor=2),\n            nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=True),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        x = self.up(x)\n        return x\n\n\nclass U_Net(nn.Module):\n    """"""\n    UNet - Basic Implementation\n    Paper : https://arxiv.org/abs/1505.04597\n    """"""\n    def __init__(self, in_ch=3, out_ch=1):\n        super(U_Net, self).__init__()\n\n        n1 = 64\n        filters = [n1, n1 * 2, n1 * 4, n1 * 8, n1 * 16]\n        \n        self.Maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.Maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.Maxpool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.Maxpool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n\n        self.Conv1 = conv_block(in_ch, filters[0])\n        self.Conv2 = conv_block(filters[0], filters[1])\n        self.Conv3 = conv_block(filters[1], filters[2])\n        self.Conv4 = conv_block(filters[2], filters[3])\n        self.Conv5 = conv_block(filters[3], filters[4])\n\n        self.Up5 = up_conv(filters[4], filters[3])\n        self.Up_conv5 = conv_block(filters[4], filters[3])\n\n        self.Up4 = up_conv(filters[3], filters[2])\n        self.Up_conv4 = conv_block(filters[3], filters[2])\n\n        self.Up3 = up_conv(filters[2], filters[1])\n        self.Up_conv3 = conv_block(filters[2], filters[1])\n\n        self.Up2 = up_conv(filters[1], filters[0])\n        self.Up_conv2 = conv_block(filters[1], filters[0])\n\n        self.Conv = nn.Conv2d(filters[0], out_ch, kernel_size=1, stride=1, padding=0)\n\n       # self.active = torch.nn.Sigmoid()\n\n    def forward(self, x):\n\n        e1 = self.Conv1(x)\n\n        e2 = self.Maxpool1(e1)\n        e2 = self.Conv2(e2)\n\n        e3 = self.Maxpool2(e2)\n        e3 = self.Conv3(e3)\n\n        e4 = self.Maxpool3(e3)\n        e4 = self.Conv4(e4)\n\n        e5 = self.Maxpool4(e4)\n        e5 = self.Conv5(e5)\n\n        d5 = self.Up5(e5)\n        d5 = torch.cat((e4, d5), dim=1)\n\n        d5 = self.Up_conv5(d5)\n\n        d4 = self.Up4(d5)\n        d4 = torch.cat((e3, d4), dim=1)\n        d4 = self.Up_conv4(d4)\n\n        d3 = self.Up3(d4)\n        d3 = torch.cat((e2, d3), dim=1)\n        d3 = self.Up_conv3(d3)\n\n        d2 = self.Up2(d3)\n        d2 = torch.cat((e1, d2), dim=1)\n        d2 = self.Up_conv2(d2)\n\n        out = self.Conv(d2)\n\n        #d1 = self.active(out)\n\n        return out\n\n\nclass Recurrent_block(nn.Module):\n    """"""\n    Recurrent Block for R2Unet_CNN\n    """"""\n    def __init__(self, out_ch, t=2):\n        super(Recurrent_block, self).__init__()\n\n        self.t = t\n        self.out_ch = out_ch\n        self.conv = nn.Sequential(\n            nn.Conv2d(out_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=True),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        for i in range(self.t):\n            if i == 0:\n                x = self.conv(x)\n            out = self.conv(x + x)\n        return out\n\n\nclass RRCNN_block(nn.Module):\n    """"""\n    Recurrent Residual Convolutional Neural Network Block\n    """"""\n    def __init__(self, in_ch, out_ch, t=2):\n        super(RRCNN_block, self).__init__()\n\n        self.RCNN = nn.Sequential(\n            Recurrent_block(out_ch, t=t),\n            Recurrent_block(out_ch, t=t)\n        )\n        self.Conv = nn.Conv2d(in_ch, out_ch, kernel_size=1, stride=1, padding=0)\n\n    def forward(self, x):\n        x1 = self.Conv(x)\n        x2 = self.RCNN(x1)\n        out = x1 + x2\n        return out\n\n\nclass R2U_Net(nn.Module):\n    """"""\n    R2U-Unet implementation\n    Paper: https://arxiv.org/abs/1802.06955\n    """"""\n    def __init__(self, img_ch=3, output_ch=1, t=2):\n        super(R2U_Net, self).__init__()\n\n        n1 = 64\n        filters = [n1, n1 * 2, n1 * 4, n1 * 8, n1 * 16]\n\n        self.Maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.Maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.Maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.Maxpool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n\n        self.Upsample = nn.Upsample(scale_factor=2)\n\n        self.RRCNN1 = RRCNN_block(img_ch, filters[0], t=t)\n\n        self.RRCNN2 = RRCNN_block(filters[0], filters[1], t=t)\n\n        self.RRCNN3 = RRCNN_block(filters[1], filters[2], t=t)\n\n        self.RRCNN4 = RRCNN_block(filters[2], filters[3], t=t)\n\n        self.RRCNN5 = RRCNN_block(filters[3], filters[4], t=t)\n\n        self.Up5 = up_conv(filters[4], filters[3])\n        self.Up_RRCNN5 = RRCNN_block(filters[4], filters[3], t=t)\n\n        self.Up4 = up_conv(filters[3], filters[2])\n        self.Up_RRCNN4 = RRCNN_block(filters[3], filters[2], t=t)\n\n        self.Up3 = up_conv(filters[2], filters[1])\n        self.Up_RRCNN3 = RRCNN_block(filters[2], filters[1], t=t)\n\n        self.Up2 = up_conv(filters[1], filters[0])\n        self.Up_RRCNN2 = RRCNN_block(filters[1], filters[0], t=t)\n\n        self.Conv = nn.Conv2d(filters[0], output_ch, kernel_size=1, stride=1, padding=0)\n\n       # self.active = torch.nn.Sigmoid()\n\n\n    def forward(self, x):\n\n        e1 = self.RRCNN1(x)\n\n        e2 = self.Maxpool(e1)\n        e2 = self.RRCNN2(e2)\n\n        e3 = self.Maxpool1(e2)\n        e3 = self.RRCNN3(e3)\n\n        e4 = self.Maxpool2(e3)\n        e4 = self.RRCNN4(e4)\n\n        e5 = self.Maxpool3(e4)\n        e5 = self.RRCNN5(e5)\n\n        d5 = self.Up5(e5)\n        d5 = torch.cat((e4, d5), dim=1)\n        d5 = self.Up_RRCNN5(d5)\n\n        d4 = self.Up4(d5)\n        d4 = torch.cat((e3, d4), dim=1)\n        d4 = self.Up_RRCNN4(d4)\n\n        d3 = self.Up3(d4)\n        d3 = torch.cat((e2, d3), dim=1)\n        d3 = self.Up_RRCNN3(d3)\n\n        d2 = self.Up2(d3)\n        d2 = torch.cat((e1, d2), dim=1)\n        d2 = self.Up_RRCNN2(d2)\n\n        out = self.Conv(d2)\n\n      # out = self.active(out)\n\n        return out\n\n\nclass Attention_block(nn.Module):\n    """"""\n    Attention Block\n    """"""\n\n    def __init__(self, F_g, F_l, F_int):\n        super(Attention_block, self).__init__()\n\n        self.W_g = nn.Sequential(\n            nn.Conv2d(F_l, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n            nn.BatchNorm2d(F_int)\n        )\n\n        self.W_x = nn.Sequential(\n            nn.Conv2d(F_g, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n            nn.BatchNorm2d(F_int)\n        )\n\n        self.psi = nn.Sequential(\n            nn.Conv2d(F_int, 1, kernel_size=1, stride=1, padding=0, bias=True),\n            nn.BatchNorm2d(1),\n            nn.Sigmoid()\n        )\n\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, g, x):\n        g1 = self.W_g(g)\n        x1 = self.W_x(x)\n        psi = self.relu(g1 + x1)\n        psi = self.psi(psi)\n        out = x * psi\n        return out\n\n\nclass AttU_Net(nn.Module):\n    """"""\n    Attention Unet implementation\n    Paper: https://arxiv.org/abs/1804.03999\n    """"""\n    def __init__(self, img_ch=3, output_ch=1):\n        super(AttU_Net, self).__init__()\n\n        n1 = 64\n        filters = [n1, n1 * 2, n1 * 4, n1 * 8, n1 * 16]\n\n        self.Maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.Maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.Maxpool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.Maxpool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n\n        self.Conv1 = conv_block(img_ch, filters[0])\n        self.Conv2 = conv_block(filters[0], filters[1])\n        self.Conv3 = conv_block(filters[1], filters[2])\n        self.Conv4 = conv_block(filters[2], filters[3])\n        self.Conv5 = conv_block(filters[3], filters[4])\n\n        self.Up5 = up_conv(filters[4], filters[3])\n        self.Att5 = Attention_block(F_g=filters[3], F_l=filters[3], F_int=filters[2])\n        self.Up_conv5 = conv_block(filters[4], filters[3])\n\n        self.Up4 = up_conv(filters[3], filters[2])\n        self.Att4 = Attention_block(F_g=filters[2], F_l=filters[2], F_int=filters[1])\n        self.Up_conv4 = conv_block(filters[3], filters[2])\n\n        self.Up3 = up_conv(filters[2], filters[1])\n        self.Att3 = Attention_block(F_g=filters[1], F_l=filters[1], F_int=filters[0])\n        self.Up_conv3 = conv_block(filters[2], filters[1])\n\n        self.Up2 = up_conv(filters[1], filters[0])\n        self.Att2 = Attention_block(F_g=filters[0], F_l=filters[0], F_int=32)\n        self.Up_conv2 = conv_block(filters[1], filters[0])\n\n        self.Conv = nn.Conv2d(filters[0], output_ch, kernel_size=1, stride=1, padding=0)\n\n        #self.active = torch.nn.Sigmoid()\n\n\n    def forward(self, x):\n\n        e1 = self.Conv1(x)\n\n        e2 = self.Maxpool1(e1)\n        e2 = self.Conv2(e2)\n\n        e3 = self.Maxpool2(e2)\n        e3 = self.Conv3(e3)\n\n        e4 = self.Maxpool3(e3)\n        e4 = self.Conv4(e4)\n\n        e5 = self.Maxpool4(e4)\n        e5 = self.Conv5(e5)\n\n        #print(x5.shape)\n        d5 = self.Up5(e5)\n        #print(d5.shape)\n        x4 = self.Att5(g=d5, x=e4)\n        d5 = torch.cat((x4, d5), dim=1)\n        d5 = self.Up_conv5(d5)\n\n        d4 = self.Up4(d5)\n        x3 = self.Att4(g=d4, x=e3)\n        d4 = torch.cat((x3, d4), dim=1)\n        d4 = self.Up_conv4(d4)\n\n        d3 = self.Up3(d4)\n        x2 = self.Att3(g=d3, x=e2)\n        d3 = torch.cat((x2, d3), dim=1)\n        d3 = self.Up_conv3(d3)\n\n        d2 = self.Up2(d3)\n        x1 = self.Att2(g=d2, x=e1)\n        d2 = torch.cat((x1, d2), dim=1)\n        d2 = self.Up_conv2(d2)\n\n        out = self.Conv(d2)\n\n      #  out = self.active(out)\n\n        return out\n\n\nclass R2AttU_Net(nn.Module):\n    """"""\n    Residual Recuurent Block with attention Unet\n    Implementation : https://github.com/LeeJunHyun/Image_Segmentation\n    """"""\n    def __init__(self, in_ch=3, out_ch=1, t=2):\n        super(R2AttU_Net, self).__init__()\n\n        n1 = 64\n        filters = [n1, n1 * 2, n1 * 4, n1 * 8, n1 * 16]\n\n        self.Maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.Maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.Maxpool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.Maxpool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n\n        self.RRCNN1 = RRCNN_block(in_ch, filters[0], t=t)\n        self.RRCNN2 = RRCNN_block(filters[0], filters[1], t=t)\n        self.RRCNN3 = RRCNN_block(filters[1], filters[2], t=t)\n        self.RRCNN4 = RRCNN_block(filters[2], filters[3], t=t)\n        self.RRCNN5 = RRCNN_block(filters[3], filters[4], t=t)\n\n        self.Up5 = up_conv(filters[4], filters[3])\n        self.Att5 = Attention_block(F_g=filters[3], F_l=filters[3], F_int=filters[2])\n        self.Up_RRCNN5 = RRCNN_block(filters[4], filters[3], t=t)\n\n        self.Up4 = up_conv(filters[3], filters[2])\n        self.Att4 = Attention_block(F_g=filters[2], F_l=filters[2], F_int=filters[1])\n        self.Up_RRCNN4 = RRCNN_block(filters[3], filters[2], t=t)\n\n        self.Up3 = up_conv(filters[2], filters[1])\n        self.Att3 = Attention_block(F_g=filters[1], F_l=filters[1], F_int=filters[0])\n        self.Up_RRCNN3 = RRCNN_block(filters[2], filters[1], t=t)\n\n        self.Up2 = up_conv(filters[1], filters[0])\n        self.Att2 = Attention_block(F_g=filters[0], F_l=filters[0], F_int=32)\n        self.Up_RRCNN2 = RRCNN_block(filters[1], filters[0], t=t)\n\n        self.Conv = nn.Conv2d(filters[0], out_ch, kernel_size=1, stride=1, padding=0)\n\n       # self.active = torch.nn.Sigmoid()\n\n\n    def forward(self, x):\n\n        e1 = self.RRCNN1(x)\n\n        e2 = self.Maxpool1(e1)\n        e2 = self.RRCNN2(e2)\n\n        e3 = self.Maxpool2(e2)\n        e3 = self.RRCNN3(e3)\n\n        e4 = self.Maxpool3(e3)\n        e4 = self.RRCNN4(e4)\n\n        e5 = self.Maxpool4(e4)\n        e5 = self.RRCNN5(e5)\n\n        d5 = self.Up5(e5)\n        e4 = self.Att5(g=d5, x=e4)\n        d5 = torch.cat((e4, d5), dim=1)\n        d5 = self.Up_RRCNN5(d5)\n\n        d4 = self.Up4(d5)\n        e3 = self.Att4(g=d4, x=e3)\n        d4 = torch.cat((e3, d4), dim=1)\n        d4 = self.Up_RRCNN4(d4)\n\n        d3 = self.Up3(d4)\n        e2 = self.Att3(g=d3, x=e2)\n        d3 = torch.cat((e2, d3), dim=1)\n        d3 = self.Up_RRCNN3(d3)\n\n        d2 = self.Up2(d3)\n        e1 = self.Att2(g=d2, x=e1)\n        d2 = torch.cat((e1, d2), dim=1)\n        d2 = self.Up_RRCNN2(d2)\n\n        out = self.Conv(d2)\n\n      #  out = self.active(out)\n\n        return out\n\n#For nested 3 channels are required\n\nclass conv_block_nested(nn.Module):\n    \n    def __init__(self, in_ch, mid_ch, out_ch):\n        super(conv_block_nested, self).__init__()\n        self.activation = nn.ReLU(inplace=True)\n        self.conv1 = nn.Conv2d(in_ch, mid_ch, kernel_size=3, padding=1, bias=True)\n        self.bn1 = nn.BatchNorm2d(mid_ch)\n        self.conv2 = nn.Conv2d(mid_ch, out_ch, kernel_size=3, padding=1, bias=True)\n        self.bn2 = nn.BatchNorm2d(out_ch)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.activation(x)\n        \n        x = self.conv2(x)\n        x = self.bn2(x)\n        output = self.activation(x)\n\n        return output\n    \n#Nested Unet\n\nclass NestedUNet(nn.Module):\n    """"""\n    Implementation of this paper:\n    https://arxiv.org/pdf/1807.10165.pdf\n    """"""\n    def __init__(self, in_ch=3, out_ch=1):\n        super(NestedUNet, self).__init__()\n\n        n1 = 64\n        filters = [n1, n1 * 2, n1 * 4, n1 * 8, n1 * 16]\n\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.Up = nn.Upsample(scale_factor=2, mode=\'bilinear\', align_corners=True)\n\n        self.conv0_0 = conv_block_nested(in_ch, filters[0], filters[0])\n        self.conv1_0 = conv_block_nested(filters[0], filters[1], filters[1])\n        self.conv2_0 = conv_block_nested(filters[1], filters[2], filters[2])\n        self.conv3_0 = conv_block_nested(filters[2], filters[3], filters[3])\n        self.conv4_0 = conv_block_nested(filters[3], filters[4], filters[4])\n\n        self.conv0_1 = conv_block_nested(filters[0] + filters[1], filters[0], filters[0])\n        self.conv1_1 = conv_block_nested(filters[1] + filters[2], filters[1], filters[1])\n        self.conv2_1 = conv_block_nested(filters[2] + filters[3], filters[2], filters[2])\n        self.conv3_1 = conv_block_nested(filters[3] + filters[4], filters[3], filters[3])\n\n        self.conv0_2 = conv_block_nested(filters[0]*2 + filters[1], filters[0], filters[0])\n        self.conv1_2 = conv_block_nested(filters[1]*2 + filters[2], filters[1], filters[1])\n        self.conv2_2 = conv_block_nested(filters[2]*2 + filters[3], filters[2], filters[2])\n\n        self.conv0_3 = conv_block_nested(filters[0]*3 + filters[1], filters[0], filters[0])\n        self.conv1_3 = conv_block_nested(filters[1]*3 + filters[2], filters[1], filters[1])\n\n        self.conv0_4 = conv_block_nested(filters[0]*4 + filters[1], filters[0], filters[0])\n\n        self.final = nn.Conv2d(filters[0], out_ch, kernel_size=1)\n\n\n    def forward(self, x):\n        \n        x0_0 = self.conv0_0(x)\n        x1_0 = self.conv1_0(self.pool(x0_0))\n        x0_1 = self.conv0_1(torch.cat([x0_0, self.Up(x1_0)], 1))\n\n        x2_0 = self.conv2_0(self.pool(x1_0))\n        x1_1 = self.conv1_1(torch.cat([x1_0, self.Up(x2_0)], 1))\n        x0_2 = self.conv0_2(torch.cat([x0_0, x0_1, self.Up(x1_1)], 1))\n\n        x3_0 = self.conv3_0(self.pool(x2_0))\n        x2_1 = self.conv2_1(torch.cat([x2_0, self.Up(x3_0)], 1))\n        x1_2 = self.conv1_2(torch.cat([x1_0, x1_1, self.Up(x2_1)], 1))\n        x0_3 = self.conv0_3(torch.cat([x0_0, x0_1, x0_2, self.Up(x1_2)], 1))\n\n        x4_0 = self.conv4_0(self.pool(x3_0))\n        x3_1 = self.conv3_1(torch.cat([x3_0, self.Up(x4_0)], 1))\n        x2_2 = self.conv2_2(torch.cat([x2_0, x2_1, self.Up(x3_1)], 1))\n        x1_3 = self.conv1_3(torch.cat([x1_0, x1_1, x1_2, self.Up(x2_2)], 1))\n        x0_4 = self.conv0_4(torch.cat([x0_0, x0_1, x0_2, x0_3, self.Up(x1_3)], 1))\n\n        output = self.final(x0_4)\n        return output\n\n#Dictioary Unet\n#if required for getting the filters and model parameters for each step \n\nclass ConvolutionBlock(nn.Module):\n    """"""Convolution block""""""\n\n    def __init__(self, in_filters, out_filters, kernel_size=3, batchnorm=True, last_active=F.relu):\n        super(ConvolutionBlock, self).__init__()\n\n        self.bn = batchnorm\n        self.last_active = last_active\n        self.c1 = nn.Conv2d(in_filters, out_filters, kernel_size, padding=1)\n        self.b1 = nn.BatchNorm2d(out_filters)\n        self.c2 = nn.Conv2d(out_filters, out_filters, kernel_size, padding=1)\n        self.b2 = nn.BatchNorm2d(out_filters)\n\n    def forward(self, x):\n        x = self.c1(x)\n        if self.bn:\n            x = self.b1(x)\n        x = F.relu(x)\n        x = self.c2(x)\n        if self.bn:\n            x = self.b2(x)\n        x = self.last_active(x)\n        return x\n\n\nclass ContractiveBlock(nn.Module):\n    """"""Deconvuling Block""""""\n\n    def __init__(self, in_filters, out_filters, conv_kern=3, pool_kern=2, dropout=0.5, batchnorm=True):\n        super(ContractiveBlock, self).__init__()\n        self.c1 = ConvolutionBlock(in_filters=in_filters, out_filters=out_filters, kernel_size=conv_kern,\n                                   batchnorm=batchnorm)\n        self.p1 = nn.MaxPool2d(kernel_size=pool_kern, ceil_mode=True)\n        self.d1 = nn.Dropout2d(dropout)\n\n    def forward(self, x):\n        c = self.c1(x)\n        return c, self.d1(self.p1(c))\n\n\nclass ExpansiveBlock(nn.Module):\n    """"""Upconvole Block""""""\n\n    def __init__(self, in_filters1, in_filters2, out_filters, tr_kern=3, conv_kern=3, stride=2, dropout=0.5):\n        super(ExpansiveBlock, self).__init__()\n        self.t1 = nn.ConvTranspose2d(in_filters1, out_filters, tr_kern, stride=2, padding=1, output_padding=1)\n        self.d1 = nn.Dropout(dropout)\n        self.c1 = ConvolutionBlock(out_filters + in_filters2, out_filters, conv_kern)\n\n    def forward(self, x, contractive_x):\n        x_ups = self.t1(x)\n        x_concat = torch.cat([x_ups, contractive_x], 1)\n        x_fin = self.c1(self.d1(x_concat))\n        return x_fin\n\n\nclass Unet_dict(nn.Module):\n    """"""Unet which operates with filters dictionary values""""""\n\n    def __init__(self, n_labels, n_filters=32, p_dropout=0.5, batchnorm=True):\n        super(Unet_dict, self).__init__()\n        filters_dict = {}\n        filt_pair = [3, n_filters]\n\n        for i in range(4):\n            self.add_module(\'contractive_\' + str(i), ContractiveBlock(filt_pair[0], filt_pair[1], batchnorm=batchnorm))\n            filters_dict[\'contractive_\' + str(i)] = (filt_pair[0], filt_pair[1])\n            filt_pair[0] = filt_pair[1]\n            filt_pair[1] = filt_pair[1] * 2\n\n        self.bottleneck = ConvolutionBlock(filt_pair[0], filt_pair[1], batchnorm=batchnorm)\n        filters_dict[\'bottleneck\'] = (filt_pair[0], filt_pair[1])\n\n        for i in reversed(range(4)):\n            self.add_module(\'expansive_\' + str(i),\n                            ExpansiveBlock(filt_pair[1], filters_dict[\'contractive_\' + str(i)][1], filt_pair[0]))\n            filters_dict[\'expansive_\' + str(i)] = (filt_pair[1], filt_pair[0])\n            filt_pair[1] = filt_pair[0]\n            filt_pair[0] = filt_pair[0] // 2\n\n        self.output = nn.Conv2d(filt_pair[1], n_labels, kernel_size=1)\n        filters_dict[\'output\'] = (filt_pair[1], n_labels)\n        self.filters_dict = filters_dict\n\n    # final_forward\n    def forward(self, x):\n        c00, c0 = self.contractive_0(x)\n        c11, c1 = self.contractive_1(c0)\n        c22, c2 = self.contractive_2(c1)\n        c33, c3 = self.contractive_3(c2)\n        bottle = self.bottleneck(c3)\n        u3 = F.relu(self.expansive_3(bottle, c33))\n        u2 = F.relu(self.expansive_2(u3, c22))\n        u1 = F.relu(self.expansive_1(u2, c11))\n        u0 = F.relu(self.expansive_0(u1, c00))\n        return F.softmax(self.output(u0), dim=1)\n\n#Need to check why this Unet is not workin properly \n# \n# class Convolution2(nn.Module):\n#     """"""Convolution Block using 2 Conv2D\n#     Args:\n#         in_channels = Input Channels\n#         out_channels = Output Channels\n#         kernal_size = 3\n#         activation = Relu\n#         batchnorm = True\n# \n#     Output:\n#         Sequential Relu output """"""\n# \n#     def __init__(self, in_channels, out_channels, kernal_size=3, activation=\'Relu\', batchnorm=True):\n#         super(Convolution2, self).__init__()\n# \n#         self.in_channels = in_channels\n#         self.out_channels = out_channels\n#         self.kernal_size = kernal_size\n#         self.batchnorm1 = batchnorm\n# \n#         self.batchnorm2 = batchnorm\n#         self.activation = activation\n# \n#         self.conv1 = nn.Conv2d(self.in_channels, self.out_channels, self.kernal_size,  padding=1, bias=True)\n#         self.conv2 = nn.Conv2d(self.out_channels, self.out_channels, self.kernal_size, padding=1, bias=True)\n# \n#         self.b1 = nn.BatchNorm2d(out_channels)\n#         self.b2 = nn.BatchNorm2d(out_channels)\n# \n#         if self.activation == \'LRelu\':\n#             self.a1 = nn.LeakyReLU(inplace=True)\n#         if self.activation == \'Relu\':\n#             self.a1 = nn.ReLU(inplace=True)\n# \n#         if self.activation == \'LRelu\':\n#             self.a2 = nn.LeakyReLU(inplace=True)\n#         if self.activation == \'Relu\':\n#             self.a2 = nn.ReLU(inplace=True)\n# \n#     def forward(self, x):\n#         x1 = self.conv1(x)\n# \n#         if self.batchnorm1:\n#             x1 = self.b1(x1)\n# \n#         x1 = self.a1(x1)\n# \n#         x1 = self.conv2(x1)\n# \n#         if self.batchnorm2:\n#             x1 = self.b1(x1)\n# \n#         x = self.a2(x1)\n# \n#         return x\n# \n# \n# class UNet(nn.Module):\n#     """"""Implementation of U-Net: Convolutional Networks for Biomedical Image Segmentation (Ronneberger et al., 2015)\n#         https://arxiv.org/abs/1505.04597\n#         Args:\n#             n_class = no. of classes""""""\n# \n#     def __init__(self, n_class, dropout=0.4):\n#         super(UNet, self).__init__()\n# \n#         in_ch = 3\n#         n1 = 64\n#         n2 = n1*2\n#         n3 = n2*2\n#         n4 = n3*2\n#         n5 = n4*2\n# \n#         self.dconv_down1 = Convolution2(in_ch, n1)\n#         self.dconv_down2 = Convolution2(n1, n2)\n#         self.dconv_down3 = Convolution2(n2, n3)\n#         self.dconv_down4 = Convolution2(n3, n4)\n#         self.dconv_down5 = Convolution2(n4, n5)\n# \n#         self.maxpool1 = nn.MaxPool2d(2)\n#         self.maxpool2 = nn.MaxPool2d(2)\n#         self.maxpool3 = nn.MaxPool2d(2)\n#         self.maxpool4 = nn.MaxPool2d(2)\n# \n#         self.upsample1 = nn.Upsample(scale_factor=2)#, mode=\'bilinear\', align_corners=True)\n#         self.upsample2 = nn.Upsample(scale_factor=2)#, mode=\'bilinear\', align_corners=True)\n#         self.upsample3 = nn.Upsample(scale_factor=2)#, mode=\'bilinear\', align_corners=True)\n#         self.upsample4 = nn.Upsample(scale_factor=2)#, mode=\'bilinear\', align_corners=True)\n# \n#         self.dropout1 = nn.Dropout(dropout)\n#         self.dropout2 = nn.Dropout(dropout)\n#         self.dropout3 = nn.Dropout(dropout)\n#         self.dropout4 = nn.Dropout(dropout)\n#         self.dropout5 = nn.Dropout(dropout)\n#         self.dropout6 = nn.Dropout(dropout)\n#         self.dropout7 = nn.Dropout(dropout)\n#         self.dropout8 = nn.Dropout(dropout)\n# \n#         self.dconv_up4 = Convolution2(n4 + n5, n4)\n#         self.dconv_up3 = Convolution2(n3 + n4, n3)\n#         self.dconv_up2 = Convolution2(n2 + n3, n2)\n#         self.dconv_up1 = Convolution2(n1 + n2, n1)\n# \n#         self.conv_last = nn.Conv2d(n1, n_class, kernel_size=1, stride=1, padding=0)\n#       #  self.active = torch.nn.Sigmoid()\n# \n# \n# \n#     def forward(self, x):\n#         conv1 = self.dconv_down1(x)\n#         x = self.maxpool1(conv1)\n#        # x = self.dropout1(x)\n# \n#         conv2 = self.dconv_down2(x)\n#         x = self.maxpool2(conv2)\n#        # x = self.dropout2(x)\n# \n#         conv3 = self.dconv_down3(x)\n#         x = self.maxpool3(conv3)\n#        # x = self.dropout3(x)\n# \n#         conv4 = self.dconv_down4(x)\n#         x = self.maxpool4(conv4)\n#         #x = self.dropout4(x)\n# \n#         x = self.dconv_down5(x)\n# \n#         x = self.upsample4(x)\n#         x = torch.cat((x, conv4), dim=1)\n#         #x = self.dropout5(x)\n# \n#         x = self.dconv_up4(x)\n#         x = self.upsample3(x)\n#         x = torch.cat((x, conv3), dim=1)\n#        # x = self.dropout6(x)\n# \n#         x = self.dconv_up3(x)\n#         x = self.upsample2(x)\n#         x = torch.cat((x, conv2), dim=1)\n#         #x = self.dropout7(x)\n# \n#         x = self.dconv_up2(x)\n#         x = self.upsample1(x)\n#         x = torch.cat((x, conv1), dim=1)\n#         #x = self.dropout8(x)\n# \n#         x = self.dconv_up1(x)\n# \n#         x = self.conv_last(x)\n#      #   out = self.active(x)\n# \n#         return x\n'"
losses.py,1,"b'from __future__ import print_function, division\nimport torch.nn.functional as F\n\n\ndef dice_loss(prediction, target):\n    """"""Calculating the dice loss\n    Args:\n        prediction = predicted image\n        target = Targeted image\n    Output:\n        dice_loss""""""\n\n    smooth = 1.0\n\n    i_flat = prediction.view(-1)\n    t_flat = target.view(-1)\n\n    intersection = (i_flat * t_flat).sum()\n\n    return 1 - ((2. * intersection + smooth) / (i_flat.sum() + t_flat.sum() + smooth))\n\n\ndef calc_loss(prediction, target, bce_weight=0.5):\n    """"""Calculating the loss and metrics\n    Args:\n        prediction = predicted image\n        target = Targeted image\n        metrics = Metrics printed\n        bce_weight = 0.5 (default)\n    Output:\n        loss : dice loss of the epoch """"""\n    bce = F.binary_cross_entropy_with_logits(prediction, target)\n    prediction = F.sigmoid(prediction)\n    dice = dice_loss(prediction, target)\n\n    loss = bce * bce_weight + dice * (1 - bce_weight)\n\n    return loss\n\n\ndef threshold_predictions_v(predictions, thr=150):\n    thresholded_preds = predictions[:]\n   # hist = cv2.calcHist([predictions], [0], None, [2], [0, 2])\n   # plt.plot(hist)\n   # plt.xlim([0, 2])\n   # plt.show()\n    low_values_indices = thresholded_preds < thr\n    thresholded_preds[low_values_indices] = 0\n    low_values_indices = thresholded_preds >= thr\n    thresholded_preds[low_values_indices] = 255\n    return thresholded_preds\n\n\ndef threshold_predictions_p(predictions, thr=0.01):\n    thresholded_preds = predictions[:]\n    #hist = cv2.calcHist([predictions], [0], None, [256], [0, 256])\n    low_values_indices = thresholded_preds < thr\n    thresholded_preds[low_values_indices] = 0\n    low_values_indices = thresholded_preds >= thr\n    thresholded_preds[low_values_indices] = 1\n    return thresholded_preds'"
ploting.py,0,"b'import matplotlib.pyplot as plt\nfrom matplotlib.lines import Line2D\nimport numpy as np\nfrom visdom import Visdom\n\n\ndef show_images(images, labels):\n    """"""Show image with label\n    Args:\n        images = input images\n        labels = input labels\n    Output:\n        plt  = concatenated image and label """"""\n\n    plt.imshow(images.permute(1, 2, 0))\n    plt.imshow(labels, alpha=0.7, cmap=\'gray\')\n    plt.figure()\n\n\ndef show_training_dataset(training_dataset):\n    """"""Showing the images in training set for dict images and labels\n    Args:\n        training_dataset = dictionary of images and labels\n    Output:\n        figure = 3 images shown""""""\n\n    if training_dataset:\n        print(len(training_dataset))\n\n    for i in range(len(training_dataset)):\n        sample = training_dataset[i]\n\n        print(i, sample[\'images\'].shape, sample[\'labels\'].shape)\n\n        ax = plt.subplot(1, 4, i + 1)\n        plt.tight_layout()\n        ax.set_title(\'Sample #{}\'.format(i))\n        ax.axis(\'off\')\n        show_images(sample[\'images\'],sample[\'labels\'])\n\n        if i == 3:\n            plt.show()\n            break\n\nclass VisdomLinePlotter(object):\n\n    """"""Plots to Visdom""""""\n\n    def __init__(self, env_name=\'main\'):\n        self.viz = Visdom()\n        self.env = env_name\n        self.plots = {}\n\n    def plot(self, var_name, split_name, title_name, x, y):\n        if var_name not in self.plots:\n            self.plots[var_name] = self.viz.line(X=np.array([x,x]), Y=np.array([y,y]), env=self.env, opts=dict(\n                legend=[split_name],\n                title=title_name,\n                xlabel=\'Epochs\',\n                ylabel=var_name\n            ))\n        else:\n            self.viz.line(X=np.array([x]), Y=np.array([y]), env=self.env, win=self.plots[var_name], name=split_name, update = \'append\')\n\n\ndef input_images(x, y, i, n_iter, k=1):\n    """"""\n\n    :param x: takes input image\n    :param y: take input label\n    :param i: the epoch number\n    :param n_iter:\n    :param k: for keeping it in loop\n    :return: Returns a image and label\n    """"""\n    if k == 1:\n        x1 = x\n        y1 = y\n\n        x2 = x1.to(\'cpu\')\n        y2 = y1.to(\'cpu\')\n        x2 = x2.detach().numpy()\n        y2 = y2.detach().numpy()\n\n        x3 = x2[1, 1, :, :]\n        y3 = y2[1, 0, :, :]\n\n        fig = plt.figure()\n\n        ax1 = fig.add_subplot(1, 2, 1)\n        ax1.imshow(x3)\n        ax1.axis(\'off\')\n        ax1.set_xticklabels([])\n        ax1.set_yticklabels([])\n        ax1 = fig.add_subplot(1, 2, 2)\n        ax1.imshow(y3)\n        ax1.axis(\'off\')\n        ax1.set_xticklabels([])\n        ax1.set_yticklabels([])\n        plt.savefig(\n            \'./model/pred/L_\' + str(n_iter-1) + \'_epoch_\'\n            + str(i))\n\n\ndef plot_kernels(tensor, n_iter, num_cols=5, cmap=""gray""):\n    """"""Plotting the kernals and layers\n    Args:\n        Tensor :Input layer,\n        n_iter : number of interation,\n        num_cols : number of columbs required for figure\n    Output:\n        Gives the figure of the size decided with output layers activation map\n\n    Default : Last layer will be taken into consideration\n        """"""\n    if not len(tensor.shape) == 4:\n        raise Exception(""assumes a 4D tensor"")\n\n    fig = plt.figure()\n    i = 0\n    t = tensor.data.numpy()\n    b = 0\n    a = 1\n\n    for t1 in t:\n        for t2 in t1:\n            i += 1\n\n            ax1 = fig.add_subplot(5, num_cols, i)\n            ax1.imshow(t2, cmap=cmap)\n            ax1.axis(\'off\')\n            ax1.set_xticklabels([])\n            ax1.set_yticklabels([])\n\n            if i == 1:\n                a = 1\n            if a == 10:\n                break\n            a += 1\n        if i % a == 0:\n            a = 0\n        b += 1\n        if b == 20:\n            break\n\n    plt.savefig(\n        \'./model/pred/Kernal_\' + str(n_iter - 1) + \'_epoch_\'\n        + str(i))\n\n\nclass LayerActivations():\n    """"""Getting the hooks on each layer""""""\n\n    features = None\n\n    def __init__(self, layer):\n        self.hook = layer.register_forward_hook(self.hook_fn)\n\n    def hook_fn(self, module, input, output):\n        self.features = output.cpu()\n\n    def remove(self):\n        self.hook.remove()\n\n\n#to get gradient flow\n#From Pytorch-forums\ndef plot_grad_flow(named_parameters,n_iter):\n\n    \'\'\'Plots the gradients flowing through different layers in the net during training.\n    Can be used for checking for possible gradient vanishing / exploding problems.\n\n    Usage: Plug this function in Trainer class after loss.backwards() as\n    ""plot_grad_flow(self.model.named_parameters())"" to visualize the gradient flow\'\'\'\n    ave_grads = []\n    max_grads = []\n    layers = []\n    for n, p in named_parameters:\n        if (p.requires_grad) and (""bias"" not in n):\n            layers.append(n)\n            ave_grads.append(p.grad.abs().mean())\n            max_grads.append(p.grad.abs().max())\n    plt.bar(np.arange(len(max_grads)), max_grads, alpha=0.1, lw=1, color=""c"")\n    plt.bar(np.arange(len(max_grads)), ave_grads, alpha=0.1, lw=1, color=""b"")\n    plt.hlines(0, 0, len(ave_grads) + 1, lw=2, color=""k"")\n    plt.xticks(range(0, len(ave_grads), 1), layers, rotation=""vertical"")\n    plt.xlim(left=0, right=len(ave_grads))\n    plt.ylim(bottom=-0.001, top=0.02)  # zoom in on the lower gradient regions\n    plt.xlabel(""Layers"")\n    plt.ylabel(""average gradient"")\n    plt.title(""Gradient flow"")\n    plt.grid(True)\n    plt.legend([Line2D([0], [0], color=""c"", lw=4),\n                Line2D([0], [0], color=""b"", lw=4),\n                Line2D([0], [0], color=""k"", lw=4)], [\'max-gradient\', \'mean-gradient\', \'zero-gradient\'])\n    #plt.savefig(\'./model/pred/Grad_Flow_\' + str(n_iter - 1))\n'"
pytorch_run.py,22,"b'from __future__ import print_function, division\nimport os\nimport numpy as np\nfrom PIL import Image\nimport glob\n#import SimpleITK as sitk\nfrom torch import optim\nimport torch.utils.data\nimport torch\nimport torch.nn.functional as F\n\nimport torch.nn\nimport torchvision\nimport matplotlib.pyplot as plt\nimport natsort\nfrom torch.utils.data.sampler import SubsetRandomSampler\nfrom Data_Loader import Images_Dataset, Images_Dataset_folder\nimport torchsummary\n#from torch.utils.tensorboard import SummaryWriter\n#from tensorboardX import SummaryWriter\n\nimport shutil\nimport random\nfrom Models import Unet_dict, NestedUNet, U_Net, R2U_Net, AttU_Net, R2AttU_Net\nfrom losses import calc_loss, dice_loss, threshold_predictions_v,threshold_predictions_p\nfrom ploting import plot_kernels, LayerActivations, input_images, plot_grad_flow\nfrom Metrics import dice_coeff, accuracy_score\nimport time\n#from ploting import VisdomLinePlotter\n#from visdom import Visdom\n\n\n#######################################################\n#Checking if GPU is used\n#######################################################\n\ntrain_on_gpu = torch.cuda.is_available()\n\nif not train_on_gpu:\n    print(\'CUDA is not available. Training on CPU\')\nelse:\n    print(\'CUDA is available. Training on GPU\')\n\ndevice = torch.device(""cuda:0"" if train_on_gpu else ""cpu"")\n\n#######################################################\n#Setting the basic paramters of the model\n#######################################################\n\nbatch_size = 4\nprint(\'batch_size = \' + str(batch_size))\n\nvalid_size = 0.15\n\nepoch = 15\nprint(\'epoch = \' + str(epoch))\n\nrandom_seed = random.randint(1, 100)\nprint(\'random_seed = \' + str(random_seed))\n\nshuffle = True\nvalid_loss_min = np.Inf\nnum_workers = 4\nlossT = []\nlossL = []\nlossL.append(np.inf)\nlossT.append(np.inf)\nepoch_valid = epoch-2\nn_iter = 1\ni_valid = 0\n\npin_memory = False\nif train_on_gpu:\n    pin_memory = True\n\n#plotter = VisdomLinePlotter(env_name=\'Tutorial Plots\')\n\n#######################################################\n#Setting up the model\n#######################################################\n\nmodel_Inputs = [U_Net, R2U_Net, AttU_Net, R2AttU_Net, NestedUNet]\n\n\ndef model_unet(model_input, in_channel=3, out_channel=1):\n    model_test = model_input(in_channel, out_channel)\n    return model_test\n\n#passsing this string so that if it\'s AttU_Net or R2ATTU_Net it doesn\'t throw an error at torchSummary\n\n\nmodel_test = model_unet(model_Inputs[0], 3, 1)\n\nmodel_test.to(device)\n\n#######################################################\n#Getting the Summary of Model\n#######################################################\n\ntorchsummary.summary(model_test, input_size=(3, 128, 128))\n\n#######################################################\n#Passing the Dataset of Images and Labels\n#######################################################\n\nt_data = \'/flush1/bat161/segmentation/New_Trails/venv/DATA/new_3C_I_ori/\'\nl_data = \'/flush1/bat161/segmentation/New_Trails/venv/DATA/new_3C_L_ori/\'\ntest_image = \'/flush1/bat161/segmentation/New_Trails/venv/DATA/test_new_3C_I_ori/0131_0009.png\'\ntest_label = \'/flush1/bat161/segmentation/New_Trails/venv/DATA/test_new_3C_L_ori/0131_0009.png\'\ntest_folderP = \'/flush1/bat161/segmentation/New_Trails/venv/DATA/test_new_3C_I_ori/*\'\ntest_folderL = \'/flush1/bat161/segmentation/New_Trails/venv/DATA/test_new_3C_L_ori/*\'\n\nTraining_Data = Images_Dataset_folder(t_data,\n                                      l_data)\n\n#######################################################\n#Giving a transformation for input data\n#######################################################\n\ndata_transform = torchvision.transforms.Compose([\n          #  torchvision.transforms.Resize((128,128)),\n         #   torchvision.transforms.CenterCrop(96),\n            torchvision.transforms.ToTensor(),\n            torchvision.transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n        ])\n\n#######################################################\n#Trainging Validation Split\n#######################################################\n\nnum_train = len(Training_Data)\nindices = list(range(num_train))\nsplit = int(np.floor(valid_size * num_train))\n\nif shuffle:\n    np.random.seed(random_seed)\n    np.random.shuffle(indices)\n\ntrain_idx, valid_idx = indices[split:], indices[:split]\ntrain_sampler = SubsetRandomSampler(train_idx)\nvalid_sampler = SubsetRandomSampler(valid_idx)\n\ntrain_loader = torch.utils.data.DataLoader(Training_Data, batch_size=batch_size, sampler=train_sampler,\n                                           num_workers=num_workers, pin_memory=pin_memory,)\n\nvalid_loader = torch.utils.data.DataLoader(Training_Data, batch_size=batch_size, sampler=valid_sampler,\n                                           num_workers=num_workers, pin_memory=pin_memory,)\n\n#######################################################\n#Using Adam as Optimizer\n#######################################################\n\ninitial_lr = 0.001\nopt = torch.optim.Adam(model_test.parameters(), lr=initial_lr) # try SGD\n#opt = optim.SGD(model_test.parameters(), lr = initial_lr, momentum=0.99)\n\nMAX_STEP = int(1e10)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(opt, MAX_STEP, eta_min=1e-5)\n#scheduler = optim.lr_scheduler.CosineAnnealingLr(opt, epoch, 1)\n\n#######################################################\n#Writing the params to tensorboard\n#######################################################\n\n#writer1 = SummaryWriter()\n#dummy_inp = torch.randn(1, 3, 128, 128)\n#model_test.to(\'cpu\')\n#writer1.add_graph(model_test, model_test(torch.randn(3, 3, 128, 128, requires_grad=True)))\n#model_test.to(device)\n\n#######################################################\n#Creating a Folder for every data of the program\n#######################################################\n\nNew_folder = \'./model\'\n\nif os.path.exists(New_folder) and os.path.isdir(New_folder):\n    shutil.rmtree(New_folder)\n\ntry:\n    os.mkdir(New_folder)\nexcept OSError:\n    print(""Creation of the main directory \'%s\' failed "" % New_folder)\nelse:\n    print(""Successfully created the main directory \'%s\' "" % New_folder)\n\n#######################################################\n#Setting the folder of saving the predictions\n#######################################################\n\nread_pred = \'./model/pred\'\n\n#######################################################\n#Checking if prediction folder exixts\n#######################################################\n\nif os.path.exists(read_pred) and os.path.isdir(read_pred):\n    shutil.rmtree(read_pred)\n\ntry:\n    os.mkdir(read_pred)\nexcept OSError:\n    print(""Creation of the prediction directory \'%s\' failed of dice loss"" % read_pred)\nelse:\n    print(""Successfully created the prediction directory \'%s\' of dice loss"" % read_pred)\n\n#######################################################\n#checking if the model exists and if true then delete\n#######################################################\n\nread_model_path = \'./model/Unet_D_\' + str(epoch) + \'_\' + str(batch_size)\n\nif os.path.exists(read_model_path) and os.path.isdir(read_model_path):\n    shutil.rmtree(read_model_path)\n    print(\'Model folder there, so deleted for newer one\')\n\ntry:\n    os.mkdir(read_model_path)\nexcept OSError:\n    print(""Creation of the model directory \'%s\' failed"" % read_model_path)\nelse:\n    print(""Successfully created the model directory \'%s\' "" % read_model_path)\n\n#######################################################\n#Training loop\n#######################################################\n\nfor i in range(epoch):\n\n    train_loss = 0.0\n    valid_loss = 0.0\n    since = time.time()\n    scheduler.step(i)\n    lr = scheduler.get_lr()\n\n    #######################################################\n    #Training Data\n    #######################################################\n\n    model_test.train()\n    k = 1\n\n    for x, y in train_loader:\n        x, y = x.to(device), y.to(device)\n\n        #If want to get the input images with their Augmentation - To check the data flowing in net\n        input_images(x, y, i, n_iter, k)\n\n       # grid_img = torchvision.utils.make_grid(x)\n        #writer1.add_image(\'images\', grid_img, 0)\n\n       # grid_lab = torchvision.utils.make_grid(y)\n\n        opt.zero_grad()\n\n        y_pred = model_test(x)\n        lossT = calc_loss(y_pred, y)     # Dice_loss Used\n\n        train_loss += lossT.item() * x.size(0)\n        lossT.backward()\n      #  plot_grad_flow(model_test.named_parameters(), n_iter)\n        opt.step()\n        x_size = lossT.item() * x.size(0)\n        k = 2\n\n    #    for name, param in model_test.named_parameters():\n    #        name = name.replace(\'.\', \'/\')\n    #        writer1.add_histogram(name, param.data.cpu().numpy(), i + 1)\n    #        writer1.add_histogram(name + \'/grad\', param.grad.data.cpu().numpy(), i + 1)\n\n\n    #######################################################\n    #Validation Step\n    #######################################################\n\n    model_test.eval()\n    torch.no_grad() #to increase the validation process uses less memory\n\n    for x1, y1 in valid_loader:\n        x1, y1 = x1.to(device), y1.to(device)\n\n        y_pred1 = model_test(x1)\n        lossL = calc_loss(y_pred1, y1)     # Dice_loss Used\n\n        valid_loss += lossL.item() * x1.size(0)\n        x_size1 = lossL.item() * x1.size(0)\n\n    #######################################################\n    #Saving the predictions\n    #######################################################\n\n    im_tb = Image.open(test_image)\n    im_label = Image.open(test_label)\n    s_tb = data_transform(im_tb)\n    s_label = data_transform(im_label)\n    s_label = s_label.detach().numpy()\n\n    pred_tb = model_test(s_tb.unsqueeze(0).to(device)).cpu()\n    pred_tb = F.sigmoid(pred_tb)\n    pred_tb = pred_tb.detach().numpy()\n\n   #pred_tb = threshold_predictions_v(pred_tb)\n\n    x1 = plt.imsave(\n        \'./model/pred/img_iteration_\' + str(n_iter) + \'_epoch_\'\n        + str(i) + \'.png\', pred_tb[0][0])\n\n  #  accuracy = accuracy_score(pred_tb[0][0], s_label)\n\n    #######################################################\n    #To write in Tensorboard\n    #######################################################\n\n    train_loss = train_loss / len(train_idx)\n    valid_loss = valid_loss / len(valid_idx)\n\n    if (i+1) % 1 == 0:\n        print(\'Epoch: {}/{} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}\'.format(i + 1, epoch, train_loss,\n                                                                                      valid_loss))\n #       writer1.add_scalar(\'Train Loss\', train_loss, n_iter)\n  #      writer1.add_scalar(\'Validation Loss\', valid_loss, n_iter)\n        #writer1.add_image(\'Pred\', pred_tb[0]) #try to get output of shape 3\n\n\n    #######################################################\n    #Early Stopping\n    #######################################################\n\n    if valid_loss <= valid_loss_min and epoch_valid >= i: # and i_valid <= 2:\n\n        print(\'Validation loss decreased ({:.6f} --> {:.6f}).  Saving model \'.format(valid_loss_min, valid_loss))\n        torch.save(model_test.state_dict(),\'./model/Unet_D_\' +\n                                              str(epoch) + \'_\' + str(batch_size) + \'/Unet_epoch_\' + str(epoch)\n                                              + \'_batchsize_\' + str(batch_size) + \'.pth\')\n       # print(accuracy)\n        if round(valid_loss, 4) == round(valid_loss_min, 4):\n            print(i_valid)\n            i_valid = i_valid+1\n        valid_loss_min = valid_loss\n        #if i_valid ==3:\n         #   break\n\n    #######################################################\n    # Extracting the intermediate layers\n    #######################################################\n\n    #####################################\n    # for kernals\n    #####################################\n    x1 = torch.nn.ModuleList(model_test.children())\n    # x2 = torch.nn.ModuleList(x1[16].children())\n     #x3 = torch.nn.ModuleList(x2[0].children())\n\n    #To get filters in the layers\n     #plot_kernels(x1.weight.detach().cpu(), 7)\n\n    #####################################\n    # for images\n    #####################################\n    x2 = len(x1)\n    dr = LayerActivations(x1[x2-1]) #Getting the last Conv Layer\n\n    img = Image.open(test_image)\n    s_tb = data_transform(img)\n\n    pred_tb = model_test(s_tb.unsqueeze(0).to(device)).cpu()\n    pred_tb = F.sigmoid(pred_tb)\n    pred_tb = pred_tb.detach().numpy()\n\n    plot_kernels(dr.features, n_iter, 7, cmap=""rainbow"")\n\n    time_elapsed = time.time() - since\n    print(\'{:.0f}m {:.0f}s\'.format(time_elapsed // 60, time_elapsed % 60))\n    n_iter += 1\n\n#######################################################\n#closing the tensorboard writer\n#######################################################\n\n#writer1.close()\n\n#######################################################\n#if using dict\n#######################################################\n\n#model_test.filter_dict\n\n#######################################################\n#Loading the model\n#######################################################\n\ntest1 =model_test.load_state_dict(torch.load(\'./model/Unet_D_\' +\n                   str(epoch) + \'_\' + str(batch_size)+ \'/Unet_epoch_\' + str(epoch)\n                   + \'_batchsize_\' + str(batch_size) + \'.pth\'))\n\n\n#######################################################\n#checking if cuda is available\n#######################################################\n\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n\n#######################################################\n#Loading the model\n#######################################################\n\nmodel_test.load_state_dict(torch.load(\'./model/Unet_D_\' +\n                   str(epoch) + \'_\' + str(batch_size)+ \'/Unet_epoch_\' + str(epoch)\n                   + \'_batchsize_\' + str(batch_size) + \'.pth\'))\n\nmodel_test.eval()\n\n#######################################################\n#opening the test folder and creating a folder for generated images\n#######################################################\n\nread_test_folder = glob.glob(test_folderP)\nx_sort_test = natsort.natsorted(read_test_folder)  # To sort\n\n\nread_test_folder112 = \'./model/gen_images\'\n\n\nif os.path.exists(read_test_folder112) and os.path.isdir(read_test_folder112):\n    shutil.rmtree(read_test_folder112)\n\ntry:\n    os.mkdir(read_test_folder112)\nexcept OSError:\n    print(""Creation of the testing directory %s failed"" % read_test_folder112)\nelse:\n    print(""Successfully created the testing directory %s "" % read_test_folder112)\n\n\n#For Prediction Threshold\n\nread_test_folder_P_Thres = \'./model/pred_threshold\'\n\n\nif os.path.exists(read_test_folder_P_Thres) and os.path.isdir(read_test_folder_P_Thres):\n    shutil.rmtree(read_test_folder_P_Thres)\n\ntry:\n    os.mkdir(read_test_folder_P_Thres)\nexcept OSError:\n    print(""Creation of the testing directory %s failed"" % read_test_folder_P_Thres)\nelse:\n    print(""Successfully created the testing directory %s "" % read_test_folder_P_Thres)\n\n#For Label Threshold\n\nread_test_folder_L_Thres = \'./model/label_threshold\'\n\n\nif os.path.exists(read_test_folder_L_Thres) and os.path.isdir(read_test_folder_L_Thres):\n    shutil.rmtree(read_test_folder_L_Thres)\n\ntry:\n    os.mkdir(read_test_folder_L_Thres)\nexcept OSError:\n    print(""Creation of the testing directory %s failed"" % read_test_folder_L_Thres)\nelse:\n    print(""Successfully created the testing directory %s "" % read_test_folder_L_Thres)\n\n\n\n\n#######################################################\n#saving the images in the files\n#######################################################\n\nimg_test_no = 0\n\nfor i in range(len(read_test_folder)):\n    im = Image.open(x_sort_test[i])\n\n    im1 = im\n    im_n = np.array(im1)\n    im_n_flat = im_n.reshape(-1, 1)\n\n    for j in range(im_n_flat.shape[0]):\n        if im_n_flat[j] != 0:\n            im_n_flat[j] = 255\n\n    s = data_transform(im)\n    pred = model_test(s.unsqueeze(0).cuda()).cpu()\n    pred = F.sigmoid(pred)\n    pred = pred.detach().numpy()\n\n#    pred = threshold_predictions_p(pred) #Value kept 0.01 as max is 1 and noise is very small.\n\n    if i % 24 == 0:\n        img_test_no = img_test_no + 1\n\n    x1 = plt.imsave(\'./model/gen_images/im_epoch_\' + str(epoch) + \'int_\' + str(i)\n                    + \'_img_no_\' + str(img_test_no) + \'.png\', pred[0][0])\n\n\n####################################################\n#Calculating the Dice Score\n####################################################\n\ndata_transform = torchvision.transforms.Compose([\n          #  torchvision.transforms.Resize((128,128)),\n        #    torchvision.transforms.CenterCrop(96),\n             torchvision.transforms.Grayscale(),\n#            torchvision.transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n        ])\n\n\n\nread_test_folderP = glob.glob(\'./model/gen_images/*\')\nx_sort_testP = natsort.natsorted(read_test_folderP)\n\n\nread_test_folderL = glob.glob(test_folderL)\nx_sort_testL = natsort.natsorted(read_test_folderL)  # To sort\n\n\ndice_score123 = 0.0\nx_count = 0\nx_dice = 0\n\nfor i in range(len(read_test_folderP)):\n\n    x = Image.open(x_sort_testP[i])\n    s = data_transform(x)\n    s = np.array(s)\n    s = threshold_predictions_v(s)\n\n    #save the images\n    x1 = plt.imsave(\'./model/pred_threshold/im_epoch_\' + str(epoch) + \'int_\' + str(i)\n                    + \'_img_no_\' + str(img_test_no) + \'.png\', s)\n\n    y = Image.open(x_sort_testL[i])\n    s2 = data_transform(y)\n    s3 = np.array(s2)\n   # s2 =threshold_predictions_v(s2)\n\n    #save the Images\n    y1 = plt.imsave(\'./model/label_threshold/im_epoch_\' + str(epoch) + \'int_\' + str(i)\n                    + \'_img_no_\' + str(img_test_no) + \'.png\', s3)\n\n    total = dice_coeff(s, s3)\n    print(total)\n\n    if total <= 0.3:\n        x_count += 1\n    if total > 0.3:\n        x_dice = x_dice + total\n    dice_score123 = dice_score123 + total\n\n\nprint(\'Dice Score : \' + str(dice_score123/len(read_test_folderP)))\n#print(x_count)\n#print(x_dice)\n#print(\'Dice Score : \' + str(float(x_dice/(len(read_test_folderP)-x_count))))\n\n'"
pytorch_run_old.py,22,"b'from __future__ import print_function, division\nimport os\nimport numpy as np\nfrom PIL import Image\nimport glob\n\nfrom torch import optim\nimport torch.utils.data\nimport torch\nimport torch.nn.functional as F\n\nimport torch.nn\nimport torchvision\nimport matplotlib.pyplot as plt\nimport natsort\nfrom torch.utils.data.sampler import SubsetRandomSampler\nfrom Data_Loader import Images_Dataset, Images_Dataset_folder\nimport torchsummary\n#from torch.utils.tensorboard import SummaryWriter\nfrom tensorboardX import SummaryWriter\n\nimport shutil\nimport random\nfrom Models import Unet_dict, NestedUNet, U_Net, R2U_Net, AttU_Net, R2AttU_Net\nfrom losses import calc_loss, dice_loss, threshold_predictions_v,threshold_predictions_p\nfrom ploting import plot_kernels, LayerActivations, input_images, plot_grad_flow\nfrom Metrics import dice_coeff, accuracy_score\nimport time\n#from ploting import VisdomLinePlotter\n#from visdom import Visdom\n\n\n#######################################################\n#to make sure you want to run the program\n#######################################################\n\nx = input(\'start the model training: \')\nif x == \'yes\':\n    pass\nelse:\n    exit()\n\n#######################################################\n#Checking if GPU is used\n#######################################################\n\ntrain_on_gpu = torch.cuda.is_available()\n\nif not train_on_gpu:\n    print(\'CUDA is not available. Training on CPU\')\nelse:\n    print(\'CUDA is available. Training on GPU\')\n\ndevice = torch.device(""cuda:0"" if train_on_gpu else ""cpu"")\n\n#######################################################\n#Setting the basic paramters of the model\n#######################################################\n\nbatch_size = 4\nprint(\'batch_size = \' + str(batch_size))\n\nvalid_size = 0.15\n\nepoch = 10\nprint(\'epoch = \' + str(epoch))\n\nrandom_seed = random.randint(1, 100)\nprint(\'random_seed = \' + str(random_seed))\n\nshuffle = True\nvalid_loss_min = np.Inf\nnum_workers = 4\nlossT = []\nlossL = []\nlossL.append(np.inf)\nlossT.append(np.inf)\nepoch_valid = epoch-2\nn_iter = 1\ni_valid = 0\n\npin_memory = False\nif train_on_gpu:\n    pin_memory = True\n\n#plotter = VisdomLinePlotter(env_name=\'Tutorial Plots\')\n\n#######################################################\n#Setting up the model\n#######################################################\n\nmodel_Inputs = [U_Net, R2U_Net, AttU_Net, R2AttU_Net, NestedUNet]\n\n\ndef model_unet(model_input, in_channel=3, out_channel=1):\n    model_test = model_input(in_channel, out_channel)\n    return model_test\n\n#passsing this string so that if it\'s AttU_Net or R2ATTU_Net it doesn\'t throw an error at torchSummary\n\n\nmodel_test = model_unet(model_Inputs[0], 3, 1)\n\nmodel_test.to(device)\n\n#######################################################\n#Getting the Summary of Model\n#######################################################\n\ntorchsummary.summary(model_test, input_size=(3, 128, 128))\n\n#######################################################\n#Passing the Dataset of Images and Labels\n#######################################################\n\nTraining_Data = Images_Dataset_folder(\'/home/malav/Desktop/Pytorch_Computer/DATA/new_3C_I_ori_same/\',\n                                      \'/home/malav/Desktop/Pytorch_Computer/DATA/new_3C_L_ori_same/\')\n\n#######################################################\n#Giving a transformation for input data\n#######################################################\n\ndata_transform = torchvision.transforms.Compose([\n          #  torchvision.transforms.Resize((128,128)),\n            torchvision.transforms.CenterCrop(96),\n            torchvision.transforms.ToTensor(),\n            torchvision.transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n        ])\n\n#######################################################\n#Trainging Validation Split\n#######################################################\n\nnum_train = len(Training_Data)\nindices = list(range(num_train))\nsplit = int(np.floor(valid_size * num_train))\n\nif shuffle:\n    np.random.seed(random_seed)\n    np.random.shuffle(indices)\n\ntrain_idx, valid_idx = indices[split:], indices[:split]\ntrain_sampler = SubsetRandomSampler(train_idx)\nvalid_sampler = SubsetRandomSampler(valid_idx)\n\ntrain_loader = torch.utils.data.DataLoader(Training_Data, batch_size=batch_size, sampler=train_sampler,\n                                           num_workers=num_workers, pin_memory=pin_memory,)\n\nvalid_loader = torch.utils.data.DataLoader(Training_Data, batch_size=batch_size, sampler=valid_sampler,\n                                           num_workers=num_workers, pin_memory=pin_memory,)\n\n#######################################################\n#Using Adam as Optimizer\n#######################################################\n\ninitial_lr = 0.001\nopt = torch.optim.Adam(model_test.parameters(), lr=initial_lr)\nMAX_STEP = int(1e10)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(opt, MAX_STEP, eta_min=1e-5)\n#scheduler = optim.lr_scheduler.CosineAnnealingLr(opt, epoch, 1)\n\n#######################################################\n#Writing the params to tensorboard\n#######################################################\n\nwriter1 = SummaryWriter()\ndummy_inp = torch.randn(1, 3, 128, 128)\nmodel_test.to(\'cpu\')\nwriter1.add_graph(model_test, model_test(torch.randn(3, 3, 128, 128, requires_grad=True)))\nmodel_test.to(device)\n\n#######################################################\n#Creating a Folder for every data of the program\n#######################################################\n\nNew_folder = \'./model\'\n\nif os.path.exists(New_folder) and os.path.isdir(New_folder):\n    shutil.rmtree(New_folder)\n\ntry:\n    os.mkdir(New_folder)\nexcept OSError:\n    print(""Creation of the main directory \'%s\' failed "" % New_folder)\nelse:\n    print(""Successfully created the main directory \'%s\' "" % New_folder)\n\n#######################################################\n#Setting the folder of saving the predictions\n#######################################################\n\nread_pred = \'./model/pred\'\n\n#######################################################\n#Checking if prediction folder exixts\n#######################################################\n\nif os.path.exists(read_pred) and os.path.isdir(read_pred):\n    shutil.rmtree(read_pred)\n\ntry:\n    os.mkdir(read_pred)\nexcept OSError:\n    print(""Creation of the prediction directory \'%s\' failed of dice loss"" % read_pred)\nelse:\n    print(""Successfully created the prediction directory \'%s\' of dice loss"" % read_pred)\n\n#######################################################\n#checking if the model exists and if true then delete\n#######################################################\n\nread_model_path = \'./model/Unet_D_\' + str(epoch) + \'_\' + str(batch_size)\n\nif os.path.exists(read_model_path) and os.path.isdir(read_model_path):\n    shutil.rmtree(read_model_path)\n    print(\'Model folder there, so deleted for newer one\')\n\ntry:\n    os.mkdir(read_model_path)\nexcept OSError:\n    print(""Creation of the model directory \'%s\' failed"" % read_model_path)\nelse:\n    print(""Successfully created the model directory \'%s\' "" % read_model_path)\n\n#######################################################\n#Training loop\n#######################################################\n\nfor i in range(epoch):\n\n    train_loss = 0.0\n    valid_loss = 0.0\n    since = time.time()\n    scheduler.step(i)\n    lr = scheduler.get_lr()\n\n    #######################################################\n    #Training Data\n    #######################################################\n\n    model_test.train()\n\n    for x, y in train_loader:\n        x, y = x.to(device), y.to(device)\n\n        #If want to get the input images with their Augmentation - To check the data flowing in net\n        input_images(x, y, i, n_iter)\n\n       # grid_img = torchvision.utils.make_grid(x)\n        #writer1.add_image(\'images\', grid_img, 0)\n\n       # grid_lab = torchvision.utils.make_grid(y)\n\n        opt.zero_grad()\n\n        y_pred = model_test(x)\n        lossT = calc_loss(y_pred, y)     # Dice_loss Used\n\n        train_loss += lossT.item() * x.size(0)\n        lossT.backward()\n      #  plot_grad_flow(model_test.named_parameters(), n_iter)\n        opt.step()\n        x_size = lossT.item() * x.size(0)\n        k = 2\n\n    #    for name, param in model_test.named_parameters():\n    #        name = name.replace(\'.\', \'/\')\n    #        writer1.add_histogram(name, param.data.cpu().numpy(), i + 1)\n    #        writer1.add_histogram(name + \'/grad\', param.grad.data.cpu().numpy(), i + 1)\n\n\n    #######################################################\n    #Validation Step\n    #######################################################\n\n    model_test.eval()\n    torch.no_grad() #to increase the validation process uses less memory\n\n    for x1, y1 in valid_loader:\n        x1, y1 = x1.to(device), y1.to(device)\n\n        y_pred1 = model_test(x1)\n        lossL = calc_loss(y_pred1, y1)     # Dice_loss Used\n\n        valid_loss += lossL.item() * x1.size(0)\n        x_size1 = lossL.item() * x1.size(0)\n\n    #######################################################\n    #Saving the predictions\n    #######################################################\n\n    im_tb = Image.open(\'/home/malav/Desktop/Pytorch_Computer/DATA/test_new_3C_I_ori_same/0131_0009.png\')\n    im_label = Image.open(\'/home/malav/Desktop/Pytorch_Computer/DATA/test_new_3C_L_ori_same/0131_0009.png\')\n    s_tb = data_transform(im_tb)\n    s_label = data_transform(im_label)\n\n    pred_tb = model_test(s_tb.unsqueeze(0).to(device)).cpu()\n    pred_tb = F.sigmoid(pred_tb)\n    pred_tb = pred_tb.detach().numpy()\n\n   #pred_tb = threshold_predictions_v(pred_tb)\n\n    x1 = plt.imsave(\n        \'./model/pred/img_iteration_\' + str(n_iter) + \'_epoch_\'\n        + str(i) + \'.png\', pred_tb[0][0])\n\n    accuracy = accuracy_score(pred_tb[0][0], s_label)\n\n    #######################################################\n    #To write in Tensorboard\n    #######################################################\n\n    train_loss = train_loss / len(train_idx)\n    valid_loss = valid_loss / len(valid_idx)\n\n    if (i+1) % 1 == 0:\n        print(\'Epoch: {}/{} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}\'.format(i + 1, epoch, train_loss,\n                                                                                      valid_loss))\n        writer1.add_scalar(\'Train Loss\', train_loss, n_iter)\n        writer1.add_scalar(\'Validation Loss\', valid_loss, n_iter)\n        #writer1.add_image(\'Pred\', pred_tb[0]) #try to get output of shape 3\n\n\n    #######################################################\n    #Early Stopping\n    #######################################################\n\n    if valid_loss <= valid_loss_min and epoch_valid >= i: # and i_valid <= 2:\n\n        print(\'Validation loss decreased ({:.6f} --> {:.6f}).  Saving model \'.format(valid_loss_min, valid_loss))\n        torch.save(model_test.state_dict(),\'./model/Unet_D_\' +\n                                              str(epoch) + \'_\' + str(batch_size) + \'/Unet_epoch_\' + str(epoch)\n                                              + \'_batchsize_\' + str(batch_size) + \'.pth\')\n        print(accuracy)\n        if round(valid_loss, 4) == round(valid_loss_min, 4):\n            print(i_valid)\n            i_valid = i_valid+1\n        valid_loss_min = valid_loss\n        #if i_valid ==3:\n         #   break\n\n    #######################################################\n    # Extracting the intermediate layers\n    #######################################################\n\n    #####################################\n    # for kernals\n    #####################################\n    x1 = torch.nn.ModuleList(model_test.children())\n    # x2 = torch.nn.ModuleList(x1[16].children())\n    # x3 = torch.nn.ModuleList(x2[0].children())\n\n    #To get filters in the layers\n    # plot_kernels(x3[3].weight.detach().cpu(), 7)\n\n    #####################################\n    # for images\n    #####################################\n    x2 = len(x1)\n    dr = LayerActivations(x1[x2-1]) #Getting the last Conv Layer\n\n    img = Image.open(\'/home/malav/Desktop/Pytorch_Computer/DATA/test_new_3C_I_ori_same/0131_0009.png\')\n    s_tb = data_transform(img)\n\n    pred_tb = model_test(s_tb.unsqueeze(0).to(device)).cpu()\n    pred_tb = F.sigmoid(pred_tb)\n    pred_tb = pred_tb.detach().numpy()\n\n    plot_kernels(dr.features, n_iter, 7, cmap=""rainbow"")\n\n    time_elapsed = time.time() - since\n    print(\'{:.0f}m {:.0f}s\'.format(time_elapsed // 60, time_elapsed % 60))\n    n_iter += 1\n\n#######################################################\n#closing the tensorboard writer\n#######################################################\n\nwriter1.close()\n\n#######################################################\n#if using dict\n#######################################################\n\n#model_test.filter_dict\n\n#######################################################\n#Loading the model\n#######################################################\n\ntest1 =model_test.load_state_dict(torch.load(\'./model/Unet_D_\' +\n                   str(epoch) + \'_\' + str(batch_size)+ \'/Unet_epoch_\' + str(epoch)\n                   + \'_batchsize_\' + str(batch_size) + \'.pth\'))\n\n\n#######################################################\n#checking if cuda is available\n#######################################################\n\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n\n#######################################################\n#Loading the model\n#######################################################\n\nmodel_test.load_state_dict(torch.load(\'./model/Unet_D_\' +\n                   str(epoch) + \'_\' + str(batch_size)+ \'/Unet_epoch_\' + str(epoch)\n                   + \'_batchsize_\' + str(batch_size) + \'.pth\'))\n\nmodel_test.eval()\n\n#######################################################\n#opening the test folder and creating a folder for generated images\n#######################################################\n\nread_test_folder = glob.glob(\'/home/malav/Desktop/Pytorch_Computer/DATA/test_new_3C_I_ori_same/*\')\nx_sort_test = natsort.natsorted(read_test_folder)  # To sort\n\n\nread_test_folder112 = \'./model/gen_images\'\n\n\nif os.path.exists(read_test_folder112) and os.path.isdir(read_test_folder112):\n    shutil.rmtree(read_test_folder112)\n\ntry:\n    os.mkdir(read_test_folder112)\nexcept OSError:\n    print(""Creation of the testing directory %s failed"" % read_test_folder112)\nelse:\n    print(""Successfully created the testing directory %s "" % read_test_folder112)\n\n\n#For Prediction Threshold\n\nread_test_folder_P_Thres = \'./model/pred_threshold\'\n\n\nif os.path.exists(read_test_folder_P_Thres) and os.path.isdir(read_test_folder_P_Thres):\n    shutil.rmtree(read_test_folder_P_Thres)\n\ntry:\n    os.mkdir(read_test_folder_P_Thres)\nexcept OSError:\n    print(""Creation of the testing directory %s failed"" % read_test_folder_P_Thres)\nelse:\n    print(""Successfully created the testing directory %s "" % read_test_folder_P_Thres)\n\n#For Label Threshold\n\nread_test_folder_L_Thres = \'./model/label_threshold\'\n\n\nif os.path.exists(read_test_folder_L_Thres) and os.path.isdir(read_test_folder_L_Thres):\n    shutil.rmtree(read_test_folder_L_Thres)\n\ntry:\n    os.mkdir(read_test_folder_L_Thres)\nexcept OSError:\n    print(""Creation of the testing directory %s failed"" % read_test_folder_L_Thres)\nelse:\n    print(""Successfully created the testing directory %s "" % read_test_folder_L_Thres)\n\n\n\n#######################################################\n#data transform for test Set (same as before)\n#######################################################\n\ndata_transform = torchvision.transforms.Compose([\n       #    torchvision.transforms.Resize((128, 128)),\n        #    torchvision.transforms.Grayscale(),\n            torchvision.transforms.CenterCrop(96),\n            torchvision.transforms.ToTensor(),\n           torchvision.transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n        ])\n\n#######################################################\n#saving the images in the files\n#######################################################\n\nimg_test_no = 0\n\nfor i in range(len(read_test_folder)):\n    im = Image.open(x_sort_test[i])\n\n    im1 = im\n    im_n = np.array(im1)\n    im_n_flat = im_n.reshape(-1,1)\n\n    for j in range(im_n_flat.shape[0]):\n        if im_n_flat[j] != 0:\n            im_n_flat[j] = 255\n\n    s = data_transform(im)\n    pred = model_test(s.unsqueeze(0).cuda()).cpu()\n    pred = F.sigmoid(pred)\n    pred = pred.detach().numpy()\n\n#    pred = threshold_predictions_p(pred) #Value kept 0.01 as max is 1 and noise is very small.\n\n    if i % 24 == 0:\n        img_test_no = img_test_no + 1\n\n    x1 = plt.imsave(\'./model/gen_images/im_epoch_\' + str(epoch) + \'int_\' + str(i)\n                    + \'_img_no_\' + str(img_test_no) + \'.png\', pred[0][0])\n\n####################################################\n#data transform for test Set (same as before)\n####################################################\n\ndata_transform_test = torchvision.transforms.Compose([\n   # torchvision.transforms.Resize((128, 128)),\n    torchvision.transforms.CenterCrop(96),\n    torchvision.transforms.Grayscale(),\n])\n\n####################################################\n#Calculating the Dice Score\n####################################################\n\nread_test_folderP = glob.glob(\'./model/gen_images/*\')\nx_sort_testP = natsort.natsorted(read_test_folderP)\n\n\nread_test_folderL = glob.glob(\'/home/malav/Desktop/Pytorch_Computer/DATA/test_new_3C_L_ori_same/*\')\nx_sort_testL = natsort.natsorted(read_test_folderL)  # To sort\n\n\ndice_score123 = 0.0\nx_count = 0\nx_dice = 0\n\nfor i in range(len(read_test_folderP)):\n\n    x = Image.open(x_sort_testP[i])\n    s = data_transform_test(x)\n    s = np.array(s)\n    s = threshold_predictions_v(s)\n\n    #save the images\n    x1 = plt.imsave(\'./model/pred_threshold/im_epoch_\' + str(epoch) + \'int_\' + str(i)\n                    + \'_img_no_\' + str(img_test_no) + \'.png\', s)\n\n    y = Image.open(x_sort_testL[i])\n    s2 = data_transform_test(y)\n    s3 = np.array(s2)\n   # s2 =threshold_predictions_v(s2)\n\n    #save the Images\n    y1 = plt.imsave(\'./model/label_threshold/im_epoch_\' + str(epoch) + \'int_\' + str(i)\n                    + \'_img_no_\' + str(img_test_no) + \'.png\', s3)\n\n    total = dice_coeff(s, s3)\n    print(total)\n\n    if total <= 0.3:\n        x_count += 1\n    if total > 0.3:\n        x_dice = x_dice + total\n    dice_score123 = dice_score123 + total\n\n\nprint(\'Dice Score : \' + str(dice_score123/len(read_test_folderP)))\nprint(x_count)\nprint(x_dice)\nprint(\'Dice Score : \' + str(float(x_dice/(len(read_test_folderP)-x_count))))\n\n'"
