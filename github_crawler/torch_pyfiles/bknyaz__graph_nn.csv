file_path,api_count,code
graph_unet.py,59,"b'import matplotlib\r\n\r\nmatplotlib.use(\'agg\')\r\nimport matplotlib.pyplot as plt\r\nimport argparse\r\nimport numpy as np\r\nimport os\r\nimport copy\r\nimport time\r\nimport math\r\nimport torch\r\nimport torch.utils\r\nimport torch.utils.data\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nimport torch.optim as optim\r\nimport torch.optim.lr_scheduler as lr_scheduler\r\nfrom torch.nn.parameter import Parameter\r\nfrom torch.utils.data import DataLoader\r\nfrom os.path import join as pjoin\r\n\r\nprint(\'using torch\', torch.__version__)\r\n\r\n# Experiment parameters\r\nparser = argparse.ArgumentParser(description=\'Graph Convolutional Networks\')\r\nparser.add_argument(\'-D\', \'--dataset\', type=str, default=\'PROTEINS\')\r\nparser.add_argument(\'-M\', \'--model\', type=str, default=\'gcn\', choices=[\'gcn\', \'unet\', \'mgcn\'])\r\nparser.add_argument(\'--lr\', type=float, default=0.005, help=\'learning rate\')\r\nparser.add_argument(\'--lr_decay_steps\', type=str, default=\'25,35\', help=\'learning rate\')\r\nparser.add_argument(\'--wd\', type=float, default=1e-4, help=\'weight decay\')\r\nparser.add_argument(\'-d\', \'--dropout\', type=float, default=0.1, help=\'dropout rate\')\r\nparser.add_argument(\'-f\', \'--filters\', type=str, default=\'64,64,64\', help=\'number of filters in each layer\')\r\nparser.add_argument(\'-K\', \'--filter_scale\', type=int, default=1, help=\'filter scale (receptive field size), must be > 0; 1 for GCN, >1 for ChebNet\')\r\nparser.add_argument(\'--n_hidden\', type=int, default=0,\r\n                    help=\'number of hidden units in a fully connected layer after the last conv layer\')\r\nparser.add_argument(\'--n_hidden_edge\', type=int, default=32,\r\n                    help=\'number of hidden units in a fully connected layer of the edge prediction network\')\r\nparser.add_argument(\'--degree\', action=\'store_true\', default=False, help=\'use one-hot node degree features\')\r\nparser.add_argument(\'--epochs\', type=int, default=40, help=\'number of epochs\')\r\nparser.add_argument(\'-b\', \'--batch_size\', type=int, default=32, help=\'batch size\')\r\nparser.add_argument(\'--bn\', action=\'store_true\', default=False, help=\'use BatchNorm layer\')\r\nparser.add_argument(\'--folds\', type=int, default=10, help=\'number of cross-validation folds (1 for COLORS and TRIANGLES and 10 for other datasets)\')\r\nparser.add_argument(\'-t\', \'--threads\', type=int, default=0, help=\'number of threads to load data\')\r\nparser.add_argument(\'--log_interval\', type=int, default=10, help=\'interval (number of batches) of logging\')\r\nparser.add_argument(\'--device\', type=str, default=\'cuda\', choices=[\'cuda\', \'cpu\'])\r\nparser.add_argument(\'--seed\', type=int, default=111, help=\'random seed\')\r\nparser.add_argument(\'--shuffle_nodes\', action=\'store_true\', default=False, help=\'shuffle nodes for debugging\')\r\nparser.add_argument(\'-g\', \'--torch_geom\', action=\'store_true\', default=False, help=\'use PyTorch Geometric\')\r\nparser.add_argument(\'-a\', \'--adj_sq\', action=\'store_true\', default=False,\r\n                    help=\'use A^2 instead of A as an adjacency matrix\')\r\nparser.add_argument(\'-s\', \'--scale_identity\', action=\'store_true\', default=False,\r\n                    help=\'use 2I instead of I for self connections\')\r\nparser.add_argument(\'-v\', \'--visualize\', action=\'store_true\', default=False,\r\n                    help=\'only for unet: save some adjacency matrices and other data as images\')\r\nparser.add_argument(\'-c\', \'--use_cont_node_attr\', action=\'store_true\', default=False,\r\n                    help=\'use continuous node attributes in addition to discrete ones\')\r\n\r\nargs = parser.parse_args()\r\n\r\nif args.torch_geom:\r\n    from torch_geometric.datasets import TUDataset\r\n    import torch_geometric.transforms as T\r\n\r\nargs.filters = list(map(int, args.filters.split(\',\')))\r\nargs.lr_decay_steps = list(map(int, args.lr_decay_steps.split(\',\')))\r\n\r\nfor arg in vars(args):\r\n    print(arg, getattr(args, arg))\r\n\r\nn_folds = args.folds  # train,val,test splits for COLORS and TRIANGLES and 10-fold cross validation for other datasets\r\ntorch.backends.cudnn.deterministic = True\r\ntorch.backends.cudnn.benchmark = True\r\ntorch.manual_seed(args.seed)\r\ntorch.cuda.manual_seed(args.seed)\r\ntorch.cuda.manual_seed_all(args.seed)\r\nrnd_state = np.random.RandomState(args.seed)\r\n\r\n\r\ndef split_ids(ids, folds=10):\r\n\r\n    if args.dataset == \'COLORS-3\':\r\n        assert folds == 1, \'this dataset has train, val and test splits\'\r\n        train_ids = [np.arange(500)]\r\n        val_ids = [np.arange(500, 3000)]\r\n        test_ids = [np.arange(3000, 10500)]\r\n    elif args.dataset == \'TRIANGLES\':\r\n        assert folds == 1, \'this dataset has train, val and test splits\'\r\n        train_ids = [np.arange(30000)]\r\n        val_ids = [np.arange(30000, 35000)]\r\n        test_ids = [np.arange(35000, 45000)]\r\n    else:\r\n        n = len(ids)\r\n        stride = int(np.ceil(n / float(folds)))\r\n        test_ids = [ids[i: i + stride] for i in range(0, n, stride)]\r\n        assert np.all(\r\n            np.unique(np.concatenate(test_ids)) == sorted(ids)), \'some graphs are missing in the test sets\'\r\n        assert len(test_ids) == folds, \'invalid test sets\'\r\n        train_ids = []\r\n        for fold in range(folds):\r\n            train_ids.append(np.array([e for e in ids if e not in test_ids[fold]]))\r\n            assert len(train_ids[fold]) + len(test_ids[fold]) == len(\r\n                np.unique(list(train_ids[fold]) + list(test_ids[fold]))) == n, \'invalid splits\'\r\n\r\n    return train_ids, test_ids\r\n\r\n\r\nif not args.torch_geom:\r\n    # Unversal data loader and reader (can be used for other graph datasets from https://ls11-www.cs.tu-dortmund.de/staff/morris/graphkerneldatasets)\r\n    class GraphData(torch.utils.data.Dataset):\r\n        def __init__(self,\r\n                     datareader,\r\n                     fold_id,\r\n                     split):\r\n            self.fold_id = fold_id\r\n            self.split = split\r\n            self.rnd_state = datareader.rnd_state\r\n            self.set_fold(datareader.data, fold_id)\r\n\r\n        def set_fold(self, data, fold_id):\r\n            self.total = len(data[\'targets\'])\r\n            self.N_nodes_max = data[\'N_nodes_max\']\r\n            self.num_classes = data[\'num_classes\']\r\n            self.num_features = data[\'num_features\']\r\n            self.idx = data[\'splits\'][fold_id][self.split]\r\n            # use deepcopy to make sure we don\'t alter objects in folds\r\n            self.labels = copy.deepcopy([data[\'targets\'][i] for i in self.idx])\r\n            self.adj_list = copy.deepcopy([data[\'adj_list\'][i] for i in self.idx])\r\n            self.features_onehot = copy.deepcopy([data[\'features_onehot\'][i] for i in self.idx])\r\n            print(\'%s: %d/%d\' % (self.split.upper(), len(self.labels), len(data[\'targets\'])))\r\n\r\n        def __len__(self):\r\n            return len(self.labels)\r\n\r\n        def __getitem__(self, index):\r\n            # convert to torch\r\n            return [torch.from_numpy(self.features_onehot[index]).float(),  # node_features\r\n                    torch.from_numpy(self.adj_list[index]).float(),  # adjacency matrix\r\n                    int(self.labels[index])]\r\n\r\n\r\n    class DataReader():\r\n        \'\'\'\r\n        Class to read the txt files containing all data of the dataset.\r\n        Should work for any dataset from https://ls11-www.cs.tu-dortmund.de/staff/morris/graphkerneldatasets\r\n        \'\'\'\r\n\r\n        def __init__(self,\r\n                     data_dir,  # folder with txt files\r\n                     rnd_state=None,\r\n                     use_cont_node_attr=False,\r\n                     # use or not additional float valued node attributes available in some datasets\r\n                     folds=10):\r\n\r\n            self.data_dir = data_dir\r\n            self.rnd_state = np.random.RandomState() if rnd_state is None else rnd_state\r\n            self.use_cont_node_attr = use_cont_node_attr\r\n            files = os.listdir(self.data_dir)\r\n            data = {}\r\n            nodes, graphs = self.read_graph_nodes_relations(\r\n                list(filter(lambda f: f.find(\'graph_indicator\') >= 0, files))[0])\r\n\r\n            data[\'adj_list\'] = self.read_graph_adj(list(filter(lambda f: f.find(\'_A\') >= 0, files))[0], nodes, graphs)\r\n\r\n            node_labels_file = list(filter(lambda f: f.find(\'node_labels\') >= 0, files))\r\n            if len(node_labels_file) == 1:\r\n                data[\'features\'] = self.read_node_features(node_labels_file[0], nodes, graphs, fn=lambda s: int(s.strip()))\r\n            else:\r\n                data[\'features\'] = None\r\n\r\n            data[\'targets\'] = np.array(\r\n                self.parse_txt_file(list(filter(lambda f: f.find(\'graph_labels\') >= 0 or f.find(\'graph_attributes\') >= 0, files))[0],\r\n                                    line_parse_fn=lambda s: int(float(s.strip()))))\r\n\r\n            if self.use_cont_node_attr:\r\n                data[\'attr\'] = self.read_node_features(list(filter(lambda f: f.find(\'node_attributes\') >= 0, files))[0],\r\n                                                       nodes, graphs,\r\n                                                       fn=lambda s: np.array(list(map(float, s.strip().split(\',\')))))\r\n\r\n            features, n_edges, degrees = [], [], []\r\n            for sample_id, adj in enumerate(data[\'adj_list\']):\r\n                N = len(adj)  # number of nodes\r\n                if data[\'features\'] is not None:\r\n                    assert N == len(data[\'features\'][sample_id]), (N, len(data[\'features\'][sample_id]))\r\n                if not np.allclose(adj, adj.T):\r\n                    print(sample_id, \'not symmetric\')\r\n                n = np.sum(adj)  # total sum of edges\r\n                assert n % 2 == 0, n\r\n                n_edges.append(int(n / 2))  # undirected edges, so need to divide by 2\r\n                degrees.extend(list(np.sum(adj, 1)))\r\n                if data[\'features\'] is not None:\r\n                    features.append(np.array(data[\'features\'][sample_id]))\r\n\r\n            # Create features over graphs as one-hot vectors for each node\r\n            if data[\'features\'] is not None:\r\n                features_all = np.concatenate(features)\r\n                features_min = features_all.min()\r\n                num_features = int(features_all.max() - features_min + 1)  # number of possible values\r\n\r\n            max_degree = np.max(degrees)\r\n            features_onehot = []\r\n            for sample_id, adj in enumerate(data[\'adj_list\']):\r\n                N = adj.shape[0]\r\n                if data[\'features\'] is not None:\r\n                    x = data[\'features\'][sample_id]\r\n                    feature_onehot = np.zeros((len(x), num_features))\r\n                    for node, value in enumerate(x):\r\n                        feature_onehot[node, value - features_min] = 1\r\n                else:\r\n                    feature_onehot = np.empty((N, 0))\r\n                if self.use_cont_node_attr:\r\n                    if args.dataset in [\'COLORS-3\', \'TRIANGLES\']:\r\n                        # first column corresponds to node attention and shouldn\'t be used as node features\r\n                        feature_attr = np.array(data[\'attr\'][sample_id])[:, 1:]\r\n                    else:\r\n                        feature_attr = np.array(data[\'attr\'][sample_id])\r\n                else:\r\n                    feature_attr = np.empty((N, 0))\r\n                if args.degree:\r\n                    degree_onehot = np.zeros((N, max_degree + 1))\r\n                    degree_onehot[np.arange(N), np.sum(adj, 1).astype(np.int32)] = 1\r\n                else:\r\n                    degree_onehot = np.empty((N, 0))\r\n\r\n                node_features = np.concatenate((feature_onehot, feature_attr, degree_onehot), axis=1)\r\n                if node_features.shape[1] == 0:\r\n                    # dummy features for datasets without node labels/attributes\r\n                    # node degree features can be used instead\r\n                    node_features = np.ones((N, 1))\r\n                features_onehot.append(node_features)\r\n\r\n            num_features = features_onehot[0].shape[1]\r\n\r\n            shapes = [len(adj) for adj in data[\'adj_list\']]\r\n            labels = data[\'targets\']  # graph class labels\r\n            labels -= np.min(labels)  # to start from 0\r\n\r\n            classes = np.unique(labels)\r\n            num_classes = len(classes)\r\n\r\n            if not np.all(np.diff(classes) == 1):\r\n                print(\'making labels sequential, otherwise pytorch might crash\')\r\n                labels_new = np.zeros(labels.shape, dtype=labels.dtype) - 1\r\n                for lbl in range(num_classes):\r\n                    labels_new[labels == classes[lbl]] = lbl\r\n                labels = labels_new\r\n                classes = np.unique(labels)\r\n                assert len(np.unique(labels)) == num_classes, np.unique(labels)\r\n\r\n            def stats(x):\r\n                return (np.mean(x), np.std(x), np.min(x), np.max(x))\r\n\r\n            print(\'N nodes avg/std/min/max: \\t%.2f/%.2f/%d/%d\' % stats(shapes))\r\n            print(\'N edges avg/std/min/max: \\t%.2f/%.2f/%d/%d\' % stats(n_edges))\r\n            print(\'Node degree avg/std/min/max: \\t%.2f/%.2f/%d/%d\' % stats(degrees))\r\n            print(\'Node features dim: \\t\\t%d\' % num_features)\r\n            print(\'N classes: \\t\\t\\t%d\' % num_classes)\r\n            print(\'Classes: \\t\\t\\t%s\' % str(classes))\r\n            for lbl in classes:\r\n                print(\'Class %d: \\t\\t\\t%d samples\' % (lbl, np.sum(labels == lbl)))\r\n\r\n            if data[\'features\'] is not None:\r\n                for u in np.unique(features_all):\r\n                    print(\'feature {}, count {}/{}\'.format(u, np.count_nonzero(features_all == u), len(features_all)))\r\n\r\n            N_graphs = len(labels)  # number of samples (graphs) in data\r\n            assert N_graphs == len(data[\'adj_list\']) == len(features_onehot), \'invalid data\'\r\n\r\n            # Create train/test sets first\r\n            train_ids, test_ids = split_ids(rnd_state.permutation(N_graphs), folds=folds)\r\n\r\n            # Create train sets\r\n            splits = []\r\n            for fold in range(len(train_ids)):\r\n                splits.append({\'train\': train_ids[fold],\r\n                               \'test\': test_ids[fold]})\r\n\r\n            data[\'features_onehot\'] = features_onehot\r\n            data[\'targets\'] = labels\r\n            data[\'splits\'] = splits\r\n            data[\'N_nodes_max\'] = np.max(shapes)  # max number of nodes\r\n            data[\'num_features\'] = num_features\r\n            data[\'num_classes\'] = num_classes\r\n\r\n            self.data = data\r\n\r\n        def parse_txt_file(self, fpath, line_parse_fn=None):\r\n            with open(pjoin(self.data_dir, fpath), \'r\') as f:\r\n                lines = f.readlines()\r\n            data = [line_parse_fn(s) if line_parse_fn is not None else s for s in lines]\r\n            return data\r\n\r\n        def read_graph_adj(self, fpath, nodes, graphs):\r\n            edges = self.parse_txt_file(fpath, line_parse_fn=lambda s: s.split(\',\'))\r\n            adj_dict = {}\r\n            for edge in edges:\r\n                node1 = int(edge[0].strip()) - 1  # -1 because of zero-indexing in our code\r\n                node2 = int(edge[1].strip()) - 1\r\n                graph_id = nodes[node1]\r\n                assert graph_id == nodes[node2], (\'invalid data\', graph_id, nodes[node2])\r\n                if graph_id not in adj_dict:\r\n                    n = len(graphs[graph_id])\r\n                    adj_dict[graph_id] = np.zeros((n, n))\r\n                ind1 = np.where(graphs[graph_id] == node1)[0]\r\n                ind2 = np.where(graphs[graph_id] == node2)[0]\r\n                assert len(ind1) == len(ind2) == 1, (ind1, ind2)\r\n                adj_dict[graph_id][ind1, ind2] = 1\r\n\r\n            adj_list = [adj_dict[graph_id] for graph_id in sorted(list(graphs.keys()))]\r\n\r\n            return adj_list\r\n\r\n        def read_graph_nodes_relations(self, fpath):\r\n            graph_ids = self.parse_txt_file(fpath, line_parse_fn=lambda s: int(s.rstrip()))\r\n            nodes, graphs = {}, {}\r\n            for node_id, graph_id in enumerate(graph_ids):\r\n                if graph_id not in graphs:\r\n                    graphs[graph_id] = []\r\n                graphs[graph_id].append(node_id)\r\n                nodes[node_id] = graph_id\r\n            graph_ids = np.unique(list(graphs.keys()))\r\n            for graph_id in graph_ids:\r\n                graphs[graph_id] = np.array(graphs[graph_id])\r\n            return nodes, graphs\r\n\r\n        def read_node_features(self, fpath, nodes, graphs, fn):\r\n            node_features_all = self.parse_txt_file(fpath, line_parse_fn=fn)\r\n            node_features = {}\r\n            for node_id, x in enumerate(node_features_all):\r\n                graph_id = nodes[node_id]\r\n                if graph_id not in node_features:\r\n                    node_features[graph_id] = [None] * len(graphs[graph_id])\r\n                ind = np.where(graphs[graph_id] == node_id)[0]\r\n                assert len(ind) == 1, ind\r\n                assert node_features[graph_id][ind[0]] is None, node_features[graph_id][ind[0]]\r\n                node_features[graph_id][ind[0]] = x\r\n            node_features_lst = [node_features[graph_id] for graph_id in sorted(list(graphs.keys()))]\r\n            return node_features_lst\r\n\r\n\r\n# NN layers and models\r\nclass GraphConv(nn.Module):\r\n    \'\'\'\r\n    Graph Convolution Layer according to (T. Kipf and M. Welling, ICLR 2017) if K<=1\r\n    Chebyshev Graph Convolution Layer according to (M. Defferrard, X. Bresson, and P. Vandergheynst, NIPS 2017) if K>1\r\n    Additional tricks (power of adjacency matrix and weighted self connections) as in the Graph U-Net paper\r\n    \'\'\'\r\n\r\n    def __init__(self,\r\n                 in_features,\r\n                 out_features,\r\n                 n_relations=1,  # number of relation types (adjacency matrices)\r\n                 K=1,  # GCN is K<=1, else ChebNet\r\n                 activation=None,\r\n                 bnorm=False,\r\n                 adj_sq=False,\r\n                 scale_identity=False):\r\n        super(GraphConv, self).__init__()\r\n        self.fc = nn.Linear(in_features=in_features * K * n_relations, out_features=out_features)\r\n        self.n_relations = n_relations\r\n        assert K > 0, (\'filter scale must be greater than 0\', K)\r\n        self.K = K\r\n        self.activation = activation\r\n        self.bnorm = bnorm\r\n        if self.bnorm:\r\n            self.bn = nn.BatchNorm1d(out_features)\r\n        self.adj_sq = adj_sq\r\n        self.scale_identity = scale_identity\r\n\r\n    def chebyshev_basis(self, L, X, K):\r\n        if K > 1:\r\n            Xt = [X]\r\n            Xt.append(torch.bmm(L, X))  # B,N,F\r\n            for k in range(2, K):\r\n                Xt.append(2 * torch.bmm(L, Xt[k - 1]) - Xt[k - 2])  # B,N,F\r\n            Xt = torch.cat(Xt, dim=2)  # B,N,K,F\r\n            return Xt\r\n        else:\r\n            # GCN\r\n            assert K == 1, K\r\n            return torch.bmm(L, X)  # B,N,1,F\r\n\r\n    def laplacian_batch(self, A):\r\n        batch, N = A.shape[:2]\r\n        if self.adj_sq:\r\n            A = torch.bmm(A, A)  # use A^2 to increase graph connectivity\r\n        A_hat = A\r\n        if self.K < 2 or self.scale_identity:\r\n            I = torch.eye(N).unsqueeze(0).to(args.device)\r\n            if self.scale_identity:\r\n                I = 2 * I  # increase weight of self connections\r\n            if self.K < 2:\r\n                A_hat = A + I\r\n        D_hat = (torch.sum(A_hat, 1) + 1e-5) ** (-0.5)\r\n        L = D_hat.view(batch, N, 1) * A_hat * D_hat.view(batch, 1, N)\r\n        return L\r\n\r\n    def forward(self, data):\r\n        x, A, mask = data[:3]\r\n        # print(\'in\', x.shape, torch.sum(torch.abs(torch.sum(x, 2)) > 0))\r\n        if len(A.shape) == 3:\r\n            A = A.unsqueeze(3)\r\n        x_hat = []\r\n\r\n        for rel in range(self.n_relations):\r\n            L = self.laplacian_batch(A[:, :, :, rel])\r\n            x_hat.append(self.chebyshev_basis(L, x, self.K))\r\n        x = self.fc(torch.cat(x_hat, 2))\r\n\r\n        if len(mask.shape) == 2:\r\n            mask = mask.unsqueeze(2)\r\n\r\n        x = x * mask  # to make values of dummy nodes zeros again, otherwise the bias is added after applying self.fc which affects node embeddings in the following layers\r\n\r\n        if self.bnorm:\r\n            x = self.bn(x.permute(0, 2, 1)).permute(0, 2, 1)\r\n        if self.activation is not None:\r\n            x = self.activation(x)\r\n        return (x, A, mask)\r\n\r\n\r\nclass GCN(nn.Module):\r\n    \'\'\'\r\n    Baseline Graph Convolutional Network with a stack of Graph Convolution Layers and global pooling over nodes.\r\n    \'\'\'\r\n\r\n    def __init__(self,\r\n                 in_features,\r\n                 out_features,\r\n                 filters=[64, 64, 64],\r\n                 K=1,\r\n                 bnorm=False,\r\n                 n_hidden=0,\r\n                 dropout=0.2,\r\n                 adj_sq=False,\r\n                 scale_identity=False):\r\n        super(GCN, self).__init__()\r\n\r\n        # Graph convolution layers\r\n        self.gconv = nn.Sequential(*([GraphConv(in_features=in_features if layer == 0 else filters[layer - 1],\r\n                                                out_features=f,\r\n                                                K=K,\r\n                                                activation=nn.ReLU(inplace=True),\r\n                                                bnorm=bnorm,\r\n                                                adj_sq=adj_sq,\r\n                                                scale_identity=scale_identity) for layer, f in enumerate(filters)]))\r\n\r\n        # Fully connected layers\r\n        fc = []\r\n        if dropout > 0:\r\n            fc.append(nn.Dropout(p=dropout))\r\n        if n_hidden > 0:\r\n            fc.append(nn.Linear(filters[-1], n_hidden))\r\n            fc.append(nn.ReLU(inplace=True))\r\n            if dropout > 0:\r\n                fc.append(nn.Dropout(p=dropout))\r\n            n_last = n_hidden\r\n        else:\r\n            n_last = filters[-1]\r\n        fc.append(nn.Linear(n_last, out_features))\r\n        self.fc = nn.Sequential(*fc)\r\n\r\n    def forward(self, data):\r\n        x = self.gconv(data)[0]\r\n        x = torch.max(x, dim=1)[0].squeeze()  # max pooling over nodes (usually performs better than average)\r\n        x = self.fc(x)\r\n        return x\r\n\r\n\r\nclass GraphUnet(nn.Module):\r\n    def __init__(self,\r\n                 in_features,\r\n                 out_features,\r\n                 filters=[64, 64, 64],\r\n                 K=1,\r\n                 bnorm=False,\r\n                 n_hidden=0,\r\n                 dropout=0.2,\r\n                 adj_sq=False,\r\n                 scale_identity=False,\r\n                 shuffle_nodes=False,\r\n                 visualize=False,\r\n                 pooling_ratios=[0.8, 0.8]):\r\n        super(GraphUnet, self).__init__()\r\n\r\n        self.shuffle_nodes = shuffle_nodes\r\n        self.visualize = visualize\r\n        self.pooling_ratios = pooling_ratios\r\n        # Graph convolution layers\r\n        self.gconv = nn.ModuleList([GraphConv(in_features=in_features if layer == 0 else filters[layer - 1],\r\n                                              out_features=f,\r\n                                              K=K,\r\n                                              activation=nn.ReLU(inplace=True),\r\n                                              bnorm=bnorm,\r\n                                              adj_sq=adj_sq,\r\n                                              scale_identity=scale_identity) for layer, f in enumerate(filters)])\r\n        # Pooling layers\r\n        self.proj = []\r\n        for layer, f in enumerate(filters[:-1]):\r\n            # Initialize projection vectors similar to weight/bias initialization in nn.Linear\r\n            fan_in = filters[layer]\r\n            p = Parameter(torch.Tensor(fan_in, 1))\r\n            bound = 1 / math.sqrt(fan_in)\r\n            torch.nn.init.uniform_(p, -bound, bound)\r\n            self.proj.append(p)\r\n        self.proj = nn.ParameterList(self.proj)\r\n\r\n        # Fully connected layers\r\n        fc = []\r\n        if dropout > 0:\r\n            fc.append(nn.Dropout(p=dropout))\r\n        if n_hidden > 0:\r\n            fc.append(nn.Linear(filters[-1], n_hidden))\r\n            if dropout > 0:\r\n                fc.append(nn.Dropout(p=dropout))\r\n            n_last = n_hidden\r\n        else:\r\n            n_last = filters[-1]\r\n        fc.append(nn.Linear(n_last, out_features))\r\n        self.fc = nn.Sequential(*fc)\r\n\r\n    def forward(self, data):\r\n        # data: [node_features, A, graph_support, N_nodes, label]\r\n        if self.shuffle_nodes:\r\n            # shuffle nodes to make sure that the model does not adapt to nodes order (happens in some cases)\r\n            N = data[0].shape[1]\r\n            idx = torch.randperm(N)\r\n            data = (data[0][:, idx], data[1][:, idx, :][:, :, idx], data[2][:, idx], data[3])\r\n\r\n        sample_id_vis, N_nodes_vis = -1, -1\r\n        for layer, gconv in enumerate(self.gconv):\r\n            N_nodes = data[3]\r\n\r\n            # TODO: remove dummy or dropped nodes for speeding up forward/backward passes\r\n            # data = (data[0][:, :N_nodes_max], data[1][:, :N_nodes_max, :N_nodes_max], data[2][:, :N_nodes_max], data[3])\r\n\r\n            x, A = data[:2]\r\n\r\n            B, N, _ = x.shape\r\n\r\n            # visualize data\r\n            if self.visualize and layer < len(self.gconv) - 1:\r\n                for b in range(B):\r\n                    if (layer == 0 and N_nodes[b] < 20 and N_nodes[b] > 10) or sample_id_vis > -1:\r\n                        if sample_id_vis > -1 and sample_id_vis != b:\r\n                            continue\r\n                        if N_nodes_vis < 0:\r\n                            N_nodes_vis = N_nodes[b]\r\n                        plt.figure()\r\n                        plt.imshow(A[b][:N_nodes_vis, :N_nodes_vis].data.cpu().numpy())\r\n                        plt.title(\'layer %d, Input adjacency matrix\' % (layer))\r\n                        plt.savefig(\'input_adjacency_%d.png\' % layer)\r\n                        sample_id_vis = b\r\n                        break\r\n\r\n            mask = data[2].clone()  # clone as we are going to make inplace changes\r\n            x = gconv(data)[0]  # graph convolution\r\n            if layer < len(self.gconv) - 1:\r\n                B, N, C = x.shape\r\n                y = torch.mm(x.view(B * N, C), self.proj[layer]).view(B, N)  # project features\r\n                y = y / (torch.sum(self.proj[layer] ** 2).view(1, 1) ** 0.5)  # node scores used for ranking below\r\n                idx = torch.sort(y, dim=1)[1]  # get indices of y values in the ascending order\r\n                N_remove = (N_nodes.float() * (1 - self.pooling_ratios[layer])).long()  # number of removed nodes\r\n\r\n                # sanity checks\r\n                assert torch.all(\r\n                    N_nodes > N_remove), \'the number of removed nodes must be large than the number of nodes\'\r\n                for b in range(B):\r\n                    # check that mask corresponds to the actual (non-dummy) nodes\r\n                    assert torch.sum(mask[b]) == float(N_nodes[b]), (torch.sum(mask[b]), N_nodes[b])\r\n\r\n                N_nodes_prev = N_nodes\r\n                N_nodes = N_nodes - N_remove\r\n\r\n                for b in range(B):\r\n                    idx_b = idx[b, mask[b, idx[b]] == 1]  # take indices of non-dummy nodes for current data example\r\n                    assert len(idx_b) >= N_nodes[b], (\r\n                        len(idx_b), N_nodes[b])  # number of indices must be at least as the number of nodes\r\n                    mask[b, idx_b[:N_remove[b]]] = 0  # set mask values corresponding to the smallest y-values to 0\r\n\r\n                # sanity checks\r\n                for b in range(B):\r\n                    # check that the new mask corresponds to the actual (non-dummy) nodes\r\n                    assert torch.sum(mask[b]) == float(N_nodes[b]), (\r\n                        b, torch.sum(mask[b]), N_nodes[b], N_remove[b], N_nodes_prev[b])\r\n                    # make sure that y-values of selected nodes are larger than of dropped nodes\r\n                    s = torch.sum(y[b] >= torch.min((y * mask.float())[b]))\r\n                    assert s >= float(N_nodes[b]), (s, N_nodes[b], (y * mask.float())[b])\r\n\r\n                mask = mask.unsqueeze(2)\r\n                x = x * torch.tanh(y).unsqueeze(2) * mask  # propagate only part of nodes using the mask\r\n                A = mask * A * mask.view(B, 1, N)\r\n                mask = mask.squeeze()\r\n                data = (x, A, mask, N_nodes)\r\n\r\n                # visualize data\r\n                if self.visualize and sample_id_vis > -1:\r\n                    b = sample_id_vis\r\n                    plt.figure()\r\n                    plt.imshow(y[b].view(N, 1).expand(N, 2)[:N_nodes_vis].data.cpu().numpy())\r\n                    plt.title(\'Node ranking\')\r\n                    plt.colorbar()\r\n                    plt.savefig(\'nodes_ranking_%d.png\' % layer)\r\n                    plt.figure()\r\n                    plt.imshow(mask[b].view(N, 1).expand(N, 2)[:N_nodes_vis].data.cpu().numpy())\r\n                    plt.title(\'Pooled nodes (%d/%d)\' % (mask[b].sum(), N_nodes_prev[b]))\r\n                    plt.savefig(\'pooled_nodes_mask_%d.png\' % layer)\r\n                    plt.figure()\r\n                    plt.imshow(A[b][:N_nodes_vis, :N_nodes_vis].data.cpu().numpy())\r\n                    plt.title(\'Pooled adjacency matrix\')\r\n                    plt.savefig(\'pooled_adjacency_%d.png\' % layer)\r\n                    print(\'layer %d: visualizations saved \' % layer)\r\n\r\n        if self.visualize and sample_id_vis > -1:\r\n            self.visualize = False  # to prevent visualization for the following batches\r\n\r\n        x = torch.max(x, dim=1)[0].squeeze()  # max pooling over nodes\r\n        x = self.fc(x)\r\n        return x\r\n\r\n\r\nclass MGCN(nn.Module):\r\n    \'\'\'\r\n    Multigraph Convolutional Network based on (B. Knyazev et al., ""Spectral Multigraph Networks for Discovering and Fusing Relationships in Molecules"")\r\n    \'\'\'\r\n\r\n    def __init__(self,\r\n                 in_features,\r\n                 out_features,\r\n                 n_relations,\r\n                 filters=[64, 64, 64],\r\n                 K=1,\r\n                 bnorm=False,\r\n                 n_hidden=0,\r\n                 n_hidden_edge=32,\r\n                 dropout=0.2,\r\n                 adj_sq=False,\r\n                 scale_identity=False):\r\n        super(MGCN, self).__init__()\r\n\r\n        # Graph convolution layers\r\n        self.gconv = nn.Sequential(*([GraphConv(in_features=in_features if layer == 0 else filters[layer - 1],\r\n                                                out_features=f,\r\n                                                n_relations=n_relations,\r\n                                                K=K,\r\n                                                activation=nn.ReLU(inplace=True),\r\n                                                bnorm=bnorm,\r\n                                                adj_sq=adj_sq,\r\n                                                scale_identity=scale_identity) for layer, f in enumerate(filters)]))\r\n\r\n        # Edge prediction NN\r\n        self.edge_pred = nn.Sequential(nn.Linear(in_features * 2, n_hidden_edge),\r\n                                       nn.ReLU(inplace=True),\r\n                                       nn.Linear(n_hidden_edge, 1))\r\n\r\n        # Fully connected layers\r\n        fc = []\r\n        if dropout > 0:\r\n            fc.append(nn.Dropout(p=dropout))\r\n        if n_hidden > 0:\r\n            fc.append(nn.Linear(filters[-1], n_hidden))\r\n            if dropout > 0:\r\n                fc.append(nn.Dropout(p=dropout))\r\n            n_last = n_hidden\r\n        else:\r\n            n_last = filters[-1]\r\n        fc.append(nn.Linear(n_last, out_features))\r\n        self.fc = nn.Sequential(*fc)\r\n\r\n    def forward(self, data):\r\n        # data: [node_features, A, graph_support, N_nodes, label]\r\n\r\n        # Predict edges based on features\r\n        x = data[0]\r\n        B, N, C = x.shape\r\n        mask = data[2]\r\n        # find indices of nodes\r\n        x_cat, idx = [], []\r\n        for b in range(B):\r\n            n = int(mask[b].sum())\r\n            node_i = torch.nonzero(mask[b]).repeat(1, n).view(-1, 1)\r\n            node_j = torch.nonzero(mask[b]).repeat(n, 1).view(-1, 1)\r\n            triu = (node_i < node_j).squeeze()  # skip loops and symmetric connections\r\n            x_cat.append(torch.cat((x[b, node_i[triu]], x[b, node_j[triu]]), 2).view(int(torch.sum(triu)), C * 2))\r\n            idx.append((node_i * N + node_j)[triu].squeeze())\r\n\r\n        x_cat = torch.cat(x_cat)\r\n        idx_flip = np.concatenate((np.arange(C, 2 * C), np.arange(C)))\r\n        # predict values and encourage invariance to nodes order\r\n        y = torch.exp(0.5 * (self.edge_pred(x_cat) + self.edge_pred(x_cat[:, idx_flip])).squeeze())\r\n        A_pred = torch.zeros(B, N * N, device=args.device)\r\n        c = 0\r\n        for b in range(B):\r\n            A_pred[b, idx[b]] = y[c:c + idx[b].nelement()]\r\n            c += idx[b].nelement()\r\n        A_pred = A_pred.view(B, N, N)\r\n        A_pred = (A_pred + A_pred.permute(0, 2, 1))  # assume undirected edges\r\n\r\n        # Use both annotated and predicted adjacency matrices to learn a GCN\r\n        data = (x, torch.stack((data[1], A_pred), 3), mask)\r\n        x = self.gconv(data)[0]\r\n        x = torch.max(x, dim=1)[0].squeeze()  # max pooling over nodes\r\n        x = self.fc(x)\r\n        return x\r\n\r\n\r\ndef collate_batch(batch):\r\n    \'\'\'\r\n    Creates a batch of same size graphs by zero-padding node features and adjacency matrices up to\r\n    the maximum number of nodes in the CURRENT batch rather than in the entire dataset.\r\n    Graphs in the batches are usually much smaller than the largest graph in the dataset, so this method is fast.\r\n    :param batch: batch in the PyTorch Geometric format or [node_features*batch_size, A*batch_size, label*batch_size]\r\n    :return: [node_features, A, graph_support, N_nodes, label]\r\n    \'\'\'\r\n    B = len(batch)\r\n    if args.torch_geom:\r\n        N_nodes = [len(batch[b].x) for b in range(B)]\r\n        C = batch[0].x.shape[1]\r\n    else:\r\n        N_nodes = [len(batch[b][1]) for b in range(B)]\r\n        C = batch[0][0].shape[1]\r\n    N_nodes_max = int(np.max(N_nodes))\r\n\r\n    graph_support = torch.zeros(B, N_nodes_max)\r\n    A = torch.zeros(B, N_nodes_max, N_nodes_max)\r\n    x = torch.zeros(B, N_nodes_max, C)\r\n    for b in range(B):\r\n        if args.torch_geom:\r\n            x[b, :N_nodes[b]] = batch[b].x\r\n            A[b].index_put_((batch[b].edge_index[0], batch[b].edge_index[1]), torch.Tensor([1]))\r\n        else:\r\n            x[b, :N_nodes[b]] = batch[b][0]\r\n            A[b, :N_nodes[b], :N_nodes[b]] = batch[b][1]\r\n        graph_support[b][:N_nodes[b]] = 1  # mask with values of 0 for dummy (zero padded) nodes, otherwise 1\r\n\r\n    N_nodes = torch.from_numpy(np.array(N_nodes)).long()\r\n    labels = torch.from_numpy(np.array([batch[b].y if args.torch_geom else batch[b][2] for b in range(B)])).long()\r\n    return [x, A, graph_support, N_nodes, labels]\r\n\r\n\r\nis_regression = args.dataset in [\'COLORS-3\', \'TRIANGLES\']  # other datasets can be for the regression task (see their README.txt)\r\ntransforms = []  # for PyTorch Geometric\r\nif args.dataset in [\'COLORS-3\', \'TRIANGLES\']:\r\n    assert n_folds == 1, \'use train, val and test splits for these datasets\'\r\n    assert args.use_cont_node_attr, \'node attributes should be used for these datasets\'\r\n\r\n    if args.torch_geom:\r\n        # Class to read note attention from DS_node_attributes.txt\r\n        class HandleNodeAttention(object):\r\n            def __call__(self, data):\r\n                if args.dataset == \'COLORS-3\':\r\n                    data.attn = torch.softmax(data.x[:, 0], dim=0)\r\n                    data.x = data.x[:, 1:]\r\n                else:\r\n                    data.attn = torch.softmax(data.x, dim=0)\r\n                    data.x = None\r\n                return data\r\n\r\n        transforms.append(HandleNodeAttention())\r\nelse:\r\n    assert n_folds == 10, \'10-fold cross-validation should be used for other datasets\'\r\n\r\nprint(\'Regression={}\'.format(is_regression))\r\nprint(\'Loading data\')\r\n\r\nif is_regression:\r\n    def loss_fn(output, target, reduction=\'mean\'):\r\n        loss = (target.float().squeeze() - output.squeeze()) ** 2\r\n        return loss.sum() if reduction == \'sum\' else loss.mean()\r\n\r\n    predict_fn = lambda output: output.round().long().detach().cpu()\r\nelse:\r\n    loss_fn = F.cross_entropy\r\n    predict_fn = lambda output: output.max(1, keepdim=True)[1].detach().cpu()\r\n\r\nif args.torch_geom:\r\n    if args.degree:\r\n        if args.dataset == \'TRIANGLES\':\r\n            max_degree = 14\r\n        else:\r\n            raise NotImplementedError(\'max_degree value should be specified in advance. \'\r\n                                      \'Try running without --torch_geom (-g) and look at dataset statistics printed out by our code.\')\r\n\r\n    if args.degree:\r\n        transforms.append(T.OneHotDegree(max_degree=max_degree, cat=False))\r\n\r\n    dataset = TUDataset(\'./data/%s/\' % args.dataset, name=args.dataset,\r\n                        use_node_attr=args.use_cont_node_attr,\r\n                        transform=T.Compose(transforms))\r\n    train_ids, test_ids = split_ids(rnd_state.permutation(len(dataset)), folds=n_folds)\r\n\r\nelse:\r\n    datareader = DataReader(data_dir=\'./data/%s/\' % args.dataset,\r\n                            rnd_state=rnd_state,\r\n                            folds=n_folds,\r\n                            use_cont_node_attr=args.use_cont_node_attr)\r\n\r\nacc_folds = []\r\n\r\nfor fold_id in range(n_folds):\r\n\r\n    loaders = []\r\n    for split in [\'train\', \'test\']:\r\n        if args.torch_geom:\r\n            gdata = dataset[torch.from_numpy((train_ids if split.find(\'train\') >= 0 else test_ids)[fold_id])]\r\n        else:\r\n            gdata = GraphData(fold_id=fold_id,\r\n                              datareader=datareader,\r\n                              split=split)\r\n\r\n        loader = DataLoader(gdata,\r\n                            batch_size=args.batch_size,\r\n                            shuffle=split.find(\'train\') >= 0,\r\n                            num_workers=args.threads,\r\n                            collate_fn=collate_batch)\r\n        loaders.append(loader)\r\n\r\n    print(\'\\nFOLD {}/{}, train {}, test {}\'.format(fold_id + 1, n_folds, len(loaders[0].dataset), len(loaders[1].dataset)))\r\n\r\n    if args.model == \'gcn\':\r\n        model = GCN(in_features=loaders[0].dataset.num_features,\r\n                    out_features=1 if is_regression else loaders[0].dataset.num_classes,\r\n                    n_hidden=args.n_hidden,\r\n                    filters=args.filters,\r\n                    K=args.filter_scale,\r\n                    bnorm=args.bn,\r\n                    dropout=args.dropout,\r\n                    adj_sq=args.adj_sq,\r\n                    scale_identity=args.scale_identity).to(args.device)\r\n    elif args.model == \'unet\':\r\n        model = GraphUnet(in_features=loaders[0].dataset.num_features,\r\n                          out_features=1 if is_regression else loaders[0].dataset.num_classes,\r\n                          n_hidden=args.n_hidden,\r\n                          filters=args.filters,\r\n                          K=args.filter_scale,\r\n                          bnorm=args.bn,\r\n                          dropout=args.dropout,\r\n                          adj_sq=args.adj_sq,\r\n                          scale_identity=args.scale_identity,\r\n                          shuffle_nodes=args.shuffle_nodes,\r\n                          visualize=args.visualize).to(args.device)\r\n    elif args.model == \'mgcn\':\r\n        model = MGCN(in_features=loaders[0].dataset.num_features,\r\n                     out_features=1 if is_regression else loaders[0].dataset.num_classes,\r\n                     n_relations=2,\r\n                     n_hidden=args.n_hidden,\r\n                     n_hidden_edge=args.n_hidden_edge,\r\n                     filters=args.filters,\r\n                     K=args.filter_scale,\r\n                     bnorm=args.bn,\r\n                     dropout=args.dropout,\r\n                     adj_sq=args.adj_sq,\r\n                     scale_identity=args.scale_identity).to(args.device)\r\n\r\n    else:\r\n        raise NotImplementedError(args.model)\r\n\r\n    print(\'\\nInitialize model\')\r\n    print(model)\r\n    train_params = list(filter(lambda p: p.requires_grad, model.parameters()))\r\n    print(\'N trainable parameters:\', np.sum([p.numel() for p in train_params]))\r\n\r\n    optimizer = optim.Adam(train_params, lr=args.lr, weight_decay=args.wd, betas=(0.5, 0.999))\r\n    scheduler = lr_scheduler.MultiStepLR(optimizer, args.lr_decay_steps, gamma=0.1)\r\n\r\n    # Normalization of continuous node features\r\n    # if args.use_cont_node_attr:\r\n    #     x = []\r\n    #     for batch_idx, data in enumerate(loaders[0]):\r\n    #         if args.torch_geom:\r\n    #             node_attr_dim = loaders[0].dataset.props[\'node_attr_dim\']\r\n    #         x.append(data[0][:, :, :node_attr_dim].view(-1, node_attr_dim).data)\r\n    #     x = torch.cat(x)\r\n    #     mn, sd = torch.mean(x, dim=0).to(args.device), torch.std(x, dim=0).to(args.device) + 1e-5\r\n    #     print(mn, sd)\r\n    # else:\r\n    #     mn, sd = 0, 1\r\n\r\n    # def norm_features(x):\r\n    #     x[:, :, :node_attr_dim] = (x[:, :, :node_attr_dim] - mn) / sd\r\n\r\n    def train(train_loader):\r\n        scheduler.step()\r\n        model.train()\r\n        start = time.time()\r\n        train_loss, n_samples = 0, 0\r\n        for batch_idx, data in enumerate(train_loader):\r\n            for i in range(len(data)):\r\n                data[i] = data[i].to(args.device)\r\n            # if args.use_cont_node_attr:\r\n            #     data[0] = norm_features(data[0])\r\n            optimizer.zero_grad()\r\n            output = model(data)\r\n            loss = loss_fn(output, data[4])\r\n            loss.backward()\r\n            optimizer.step()\r\n            time_iter = time.time() - start\r\n            train_loss += loss.item() * len(output)\r\n            n_samples += len(output)\r\n            if batch_idx % args.log_interval == 0 or batch_idx == len(train_loader) - 1:\r\n                print(\'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f} (avg: {:.6f}) \\tsec/iter: {:.4f}\'.format(\r\n                    epoch + 1, n_samples, len(train_loader.dataset),\r\n                    100. * (batch_idx + 1) / len(train_loader), loss.item(), train_loss / n_samples,\r\n                    time_iter / (batch_idx + 1)))\r\n\r\n\r\n    def test(test_loader):\r\n        model.eval()\r\n        start = time.time()\r\n        test_loss, correct, n_samples = 0, 0, 0\r\n        for batch_idx, data in enumerate(test_loader):\r\n            for i in range(len(data)):\r\n                data[i] = data[i].to(args.device)\r\n            # if args.use_cont_node_attr:\r\n            #     data[0] = norm_features(data[0])\r\n            output = model(data)\r\n            loss = loss_fn(output, data[4], reduction=\'sum\')\r\n            test_loss += loss.item()\r\n            n_samples += len(output)\r\n            pred = predict_fn(output)\r\n\r\n            correct += pred.eq(data[4].detach().cpu().view_as(pred)).sum().item()\r\n\r\n        acc = 100. * correct / n_samples\r\n        print(\'Test set (epoch {}): Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%) \\tsec/iter: {:.4f}\\n\'.format(\r\n            epoch + 1,\r\n            test_loss / n_samples,\r\n            correct,\r\n            n_samples,\r\n            acc, (time.time() - start) / len(test_loader)))\r\n        return acc\r\n\r\n    for epoch in range(args.epochs):\r\n        train(loaders[0])  # no need to evaluate after each epoch\r\n    acc = test(loaders[1])\r\n    acc_folds.append(acc)\r\n\r\nprint(acc_folds)\r\nprint(\'{}-fold cross validation avg acc (+- std): {} ({})\'.format(n_folds, np.mean(acc_folds), np.std(acc_folds)))\r\n'"
