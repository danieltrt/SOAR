file_path,api_count,code
CAM_pytorch/__init__.py,0,b''
CAM_pytorch/main.py,4,"b'import torch as t\nfrom utils.config import opt\nimport models\nimport cv2\nimport numpy as np\nimport torch.backends.cudnn as cudnn\nfrom torch.utils.data import DataLoader  #\xe6\x95\xb0\xe6\x8d\xae\xe5\x8a\xa0\xe8\xbd\xbd\xe5\x99\xa8\nfrom data.MyDataSet import MyDataSet\nfrom torch.autograd import Variable\nfrom utils.visualize import Visualizer  #\xe5\x8f\xaf\xe8\xa7\x86\xe5\x8c\x96visdom\nfrom torchnet import meter\nfrom functools import partial\nfrom torch.nn import functional as F\ndef train():\n    print(""\xe5\xbc\x80\xe5\xa7\x8b\xe8\xae\xad\xe7\xbb\x83"")\n    # \xe5\xae\x9a\xe4\xb9\x89\xe4\xb8\x80\xe4\xb8\xaa\xe7\xbd\x91\xe7\xbb\x9c\xe6\xa8\xa1\xe5\x9e\x8b\xe5\xaf\xb9\xe8\xb1\xa1\n    # \xe9\x80\x9a\xe8\xbf\x87config\xe6\x96\x87\xe4\xbb\xb6\xe4\xb8\xad\xe6\xa8\xa1\xe5\x9e\x8b\xe5\x90\x8d\xe7\xa7\xb0\xe6\x9d\xa5\xe5\x8a\xa0\xe8\xbd\xbd\xe6\xa8\xa1\xe5\x9e\x8b\n    netWork = getattr(models, opt.model)()\n    print(\'\xe5\xbd\x93\xe5\x89\x8d\xe4\xbd\xbf\xe7\x94\xa8\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\xe4\xb8\xba\'+opt.model)\n\n    # \xe5\xae\x9a\xe4\xb9\x89\xe5\x8f\xaf\xe8\xa7\x86\xe5\x8c\x96\xe5\xaf\xb9\xe8\xb1\xa1\n    vis = Visualizer(opt.model)\n\n    # \xe5\x85\x88\xe5\xb0\x86\xe6\xa8\xa1\xe5\x9e\x8b\xe5\x8a\xa0\xe8\xbd\xbd\xe5\x88\xb0\xe5\x86\x85\xe5\xad\x98\xe4\xb8\xad\xef\xbc\x8c\xe5\x8d\xb3CPU\xe4\xb8\xad\n    if opt.load_model_path:\n        netWork.load_state_dict(t.load(opt.load_model_path, map_location=lambda storage, loc: storage))\n    # \xe5\xb0\x86\xe6\xa8\xa1\xe5\x9e\x8b\xe8\xbd\xac\xe5\x88\xb0GPU\n    if opt.use_gpu:\n        netWork.cuda()\n        cudnn.benchmark = True\n\n    train_data = MyDataSet(opt.dataset_root, train=True)\n    val_data = MyDataSet(opt.dataset_root, train=False)\n    # \xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe5\x8a\xa0\xe8\xbd\xbd\xe5\x99\xa8\n    train_dataloader = DataLoader(train_data, opt.batch_size, shuffle=True, num_workers=opt.num_workers)\n    val_dataloader = DataLoader(val_data, opt.batch_size, shuffle=True,num_workers=opt.num_workers)\n\n    # criterion \xe6\x8d\x9f\xe5\xa4\xb1\xe5\x87\xbd\xe6\x95\xb0\xe5\x92\x8coptimizer\xe4\xbc\x98\xe5\x8c\x96\xe5\x99\xa8\n    criterion = t.nn.CrossEntropyLoss()\n    lr = opt.lr\n    optimizer = t.optim.SGD(netWork.parameters(), lr=lr, weight_decay=1e-6, momentum=0.5, nesterov=True)\n\n    # \xe5\xae\x9a\xe4\xb9\x89\xe5\x88\x9d\xe5\xa7\x8b\xe7\x9a\x84loss\n    previous_loss = float(\'inf\')\n\n    for epoch in range(opt.max_epoch):\n        # \xe8\xbf\xad\xe4\xbb\xa3\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe5\x8a\xa0\xe8\xbd\xbd\xe5\x99\xa8\n        for ii, (data_origin, label) in enumerate(train_dataloader):\n            # input_img\xe4\xb8\xba\xe6\xa8\xa1\xe5\x9e\x8b\xe8\xbe\x93\xe5\x85\xa5\xe5\x9b\xbe\xe5\x83\x8f\n            input_img = Variable(data_origin)\n            # label_img\xe4\xb8\xba\xe5\xaf\xb9\xe5\xba\x94\xe6\xa0\x87\xe7\xad\xbe\n            label_img = Variable(label)\n            # \xe5\xb0\x86\xe6\x95\xb0\xe6\x8d\xae\xe8\xbd\xac\xe5\x88\xb0GPU\n            if opt.use_gpu:\n                input_img = input_img.cuda()\n                label_img = label_img.cuda()\n            # \xe4\xbc\x98\xe5\x8c\x96\xe5\x99\xa8\xe6\xa2\xaf\xe5\xba\xa6\xe6\xb8\x85\xe9\x9b\xb6\n            optimizer.zero_grad()\n            label_output = netWork(input_img)\n            # \xe6\x8d\x9f\xe5\xa4\xb1\xe4\xb8\xba\xe4\xba\xa4\xe5\x8f\x89\xe7\x86\xb5\n            loss = criterion(label_output, label_img)\n            loss.backward()\n            optimizer.step()\n            current_loss = loss.data[0]\n            # \xe6\xaf\x8f8\xe6\xac\xa1\xe5\x8f\xaf\xe8\xa7\x86\xe5\x8c\x96loss\n            if ii % 8 == 0.0:\n                vis.plot(\'loss\', current_loss)\n\n        #  \xe4\xb8\x80\xe4\xb8\xaaepoch\xe4\xb9\x8b\xe5\x90\x8e\xe4\xbf\x9d\xe5\xad\x98\xe6\xa8\xa1\xe5\x9e\x8b\n        t.save(netWork.state_dict(), opt.checkpoint_root + opt.model + \'_\' + str(epoch) + \'.pth\')\n        print(""\xe7\xac\xac"" + str(epoch) + ""\xe6\xac\xa1epoch\xe5\xae\x8c\xe6\x88\x90=========================="")\n        vis.log(""epoch:{epoch},lr:{lr},loss:{loss}"".format(\n            epoch=epoch, loss=current_loss, lr= lr ))\n\n        # \xe6\x9b\xb4\xe6\x96\xb0\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87  \xe5\xa6\x82\xe6\x9e\x9c\xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\xe6\x8d\x9f\xe5\xa4\xb1\xe5\xbc\x80\xe5\xa7\x8b\xe5\x8d\x87\xe9\xab\x98\xef\xbc\x8c\xe5\x88\x99\xe9\x99\x8d\xe4\xbd\x8e\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\n        if current_loss > previous_loss:\n            lr = lr * opt.lr_decay\n            # \xe4\xb8\x8d\xe4\xbc\x9a\xe6\x9c\x89moment\xe7\xad\x89\xe4\xbf\xa1\xe6\x81\xaf\xe7\x9a\x84\xe4\xb8\xa2\xe5\xa4\xb1\n            for param_group in optimizer.param_groups:\n                param_group[\'lr\'] = lr\n        previous_loss = current_loss\n\n\n        #============\xe9\xaa\x8c\xe8\xaf\x81===================\n        # \xe6\xa8\xa1\xe5\x9e\x8b\xe8\xb0\x83\xe6\x95\xb4\xe4\xb8\xba\xe9\xaa\x8c\xe8\xaf\x81\xe6\xa8\xa1\xe5\xbc\x8f\n        netWork.eval()\n        confusion_matrix = meter.ConfusionMeter(2)\n        for ii, (val_data_origin, val_label) in enumerate(val_dataloader):\n            val_input_img = Variable(val_data_origin, volatile=True)\n            val_label_img = Variable(val_label.type(t.LongTensor), volatile=True)\n            # \xe5\xb0\x86\xe6\x95\xb0\xe6\x8d\xae\xe8\xbd\xac\xe5\x88\xb0GPU\n            if opt.use_gpu:\n                val_input_img = val_input_img.cuda()\n            val_pridict_label = netWork(val_input_img)\n            confusion_matrix.add(val_pridict_label.data.squeeze(), val_label.type(t.LongTensor))\n        # cm_value   \xe6\xb7\xb7\xe6\xb7\x86\xe7\x9f\xa9\xe9\x98\xb5\xe7\x9a\x84\xe5\x80\xbc\n        cm_value = confusion_matrix.value()\n        # \xe9\xa2\x84\xe6\xb5\x8b\xe6\xad\xa3\xe7\xa1\xae\xe7\x9a\x84\xe6\x95\xb0\xe9\x87\x8f\xe9\x99\xa4\xe4\xbb\xa5\xe6\x80\xbb\xe6\x95\xb0\xe9\x87\x8f \xe5\x86\x8d*100  \xe5\xbe\x97\xe5\x88\xb0\xe6\xad\xa3\xe7\xa1\xae\xe7\x8e\x87\n        accuracy = 100. * (cm_value[0][0] + cm_value[1][1]) / (cm_value.sum())\n        vis.plot(\'\xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87\', accuracy)\n        # \xe5\xb0\x86\xe6\xa8\xa1\xe5\x9e\x8b\xe8\xb0\x83\xe6\x95\xb4\xe4\xb8\xba\xe8\xae\xad\xe7\xbb\x83\xe6\xa8\xa1\xe5\xbc\x8f\n        netWork.train()\n    print(""============\xe8\xae\xad\xe7\xbb\x83\xe5\xae\x8c\xe6\xaf\x95============="")\n\ndef visualize_cam():\n    # \xe9\x80\x9a\xe8\xbf\x87config\xe6\x96\x87\xe4\xbb\xb6\xe4\xb8\xad\xe6\xa8\xa1\xe5\x9e\x8b\xe5\x90\x8d\xe7\xa7\xb0\xe6\x9d\xa5\xe5\x8a\xa0\xe8\xbd\xbd\xe6\xa8\xa1\xe5\x9e\x8b\n    netWork = getattr(models, opt.model)()\n    print(\'\xe5\xbd\x93\xe5\x89\x8d\xe4\xbd\xbf\xe7\x94\xa8\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\xe4\xb8\xba\' + opt.model)\n\n    # \xe5\x85\x88\xe5\xb0\x86\xe6\xa8\xa1\xe5\x9e\x8b\xe5\x8a\xa0\xe8\xbd\xbd\xe5\x88\xb0\xe5\x86\x85\xe5\xad\x98\xe4\xb8\xad\xef\xbc\x8c\xe5\x8d\xb3CPU\xe4\xb8\xad\n    if opt.load_model_path:\n        print(\'\xe5\x8a\xa0\xe8\xbd\xbd\xe9\xa2\x84\xe8\xae\xad\xe7\xbb\x83\xe6\xa8\xa1\xe5\x9e\x8b\')\n        netWork.load_state_dict(t.load(opt.load_model_path, map_location=lambda storage, loc: storage))\n\n    netWork.eval()\n    # \xe4\xb8\xba\xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x80\xe4\xb8\xaaconv\xe5\xb1\x82\xe6\xb7\xbb\xe5\x8a\xa0\xe4\xb8\x80\xe4\xb8\xaa\xe9\x92\xa9\xe5\xad\x90\n    feature_blob = []\n    netWork.feature_layer.register_forward_hook(partial(hook, feature_blob = feature_blob))\n\n    # netWork._modules.get(\'feature_layer.29\').register_forward_hook(partial(hook, feature_blob = feature_blob))\n\n    # \xe5\xbe\x97\xe5\x88\xb0 softmax(\xe5\x8d\xb3 \xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x80\xe5\xb1\x82fc\xe5\xb1\x82)\xe6\x9d\x83\xe9\x87\x8d\n    params = list(netWork.parameters())\n    softmax_weight = np.squeeze(params[-2].data.numpy())\n\n    #==============================================================\n    # [3,64, 128]\n    img_origin, img = MyDataSet(opt.dataset_root, train=True).get_test_img()\n\n\n\n    #[1,2]\n    output = netWork(Variable(img).unsqueeze(0))\n    probs = F.softmax(output).data.squeeze()\n    # \xe6\x8c\x89\xe7\x85\xa7 \xe6\xa6\x82\xe7\x8e\x87\xe8\xbf\x9b\xe8\xa1\x8c\xe4\xbb\x8e\xe5\xa4\xa7\xe5\x88\xb0\xe5\xb0\x8f\xe6\x8e\x92\xe5\xba\x8f\n    probs, idx = probs.sort(0, descending = True)\n\n    # generate class activation map for the top-5 prediction\n    # \xe5\xaf\xb9\xe4\xba\x8e top-5 \xe7\x9a\x84\xe9\xa2\x84\xe6\xb5\x8b \xe7\x94\x9f\xe6\x88\x90CAM  \xe7\xb1\xbb\xe6\xbf\x80\xe6\xb4\xbb\xe6\x98\xa0\xe5\xb0\x84\n    # feature_blob[0]  \xe4\xb8\xba\xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x80\xe4\xb8\xaaconv\xe5\xb1\x82\xe8\xbe\x93\xe5\x87\xba\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x9c  [1,2048,7,7]\n    # softmax_weight  \xe4\xb8\xba\xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe7\x9a\x84\xe6\x9d\x83\xe9\x87\x8d [1000,2048]\n    # idx[0: 5]  \xe4\xb8\xba \xe6\x8c\x89\xe7\x85\xa7\xe6\xa6\x82\xe7\x8e\x87\xe4\xbb\x8e\xe5\xa4\xa7\xe5\x88\xb0\xe5\xb0\x8f\xe6\x8e\x92\xe5\xba\x8f\xe5\x90\x8e\xe7\x9a\x84 \xe5\x89\x8d\xe4\xba\x94\xe4\xb8\xaa\xe6\xa6\x82\xe7\x8e\x87\xe7\x9a\x84\xe4\xb8\x8b\xe6\xa0\x87\n    # cams list:5  \xe6\xaf\x8f\xe4\xb8\xaa\xe9\x87\x8c\xe9\x9d\xa2\xe9\x83\xbd\xe6\x98\xaf[224,224]\n\n    id_bobo = []\n    id_bobo.append(idx[0])\n    cams = compute_cam(feature_blob[0], softmax_weight,id_bobo )\n    for i in range(len(cams)):\n        # render cam and original image\n        # \xe5\xb0\x86 cam\xe7\xb1\xbb\xe6\xbf\x80\xe6\xb4\xbb\xe6\x98\xa0\xe5\xb0\x84 \xe5\x92\x8c \xe5\x8e\x9f\xe5\x9b\xbe  \xe7\xbb\x84\xe5\x90\x88\xe5\x9c\xa8\xe4\xb8\x80\xe8\xb5\xb7\n        filename =\'out.jpg\'\n\n        # \xe5\x8e\x9f\xe5\x9b\xbe [216,380,3]\n        h, w, _ = img_origin.shape\n\n        # \xe4\xbb\x8ecams\xe7\xb1\xbb\xe6\xbf\x80\xe6\xb4\xbb\xe6\x98\xa0\xe5\xb0\x84 \xe8\xbd\xac\xe5\x8c\x96\xe7\x9a\x84 \xe7\x83\xad\xe5\x8a\x9b\xe5\x9b\xbe [216,380,3]\n        heatmap = cv2.applyColorMap(cv2.resize(cams[i], (w, h)), cv2.COLORMAP_JET)\n\n        # \xe6\x9c\x80\xe7\xbb\x88\xe7\xbb\x93\xe6\x9e\x9c= 0.3\xe7\x83\xad\xe5\x8a\x9b\xe5\x9b\xbe  +  0.5\xe5\x8e\x9f\xe5\x9b\xbe\n        result = heatmap * 0.3 + img_origin * 0.5\n        cv2.imwrite(filename, result)\n\n\n# a hook to a given layer\ndef hook(module, input, output, feature_blob):\n    feature_blob.append(output.data.numpy())\n\n# compute class activation map\ndef compute_cam(activation, softmax_weight, class_ids):\n    \'\'\'\n     \xe5\xaf\xb9\xe4\xba\x8e top-5 \xe7\x9a\x84\xe9\xa2\x84\xe6\xb5\x8b \xe7\x94\x9f\xe6\x88\x90CAM  \xe7\xb1\xbb\xe6\xbf\x80\xe6\xb4\xbb\xe6\x98\xa0\xe5\xb0\x84\n    :param activation:  \xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x80\xe4\xb8\xaaconv\xe5\xb1\x82\xe8\xbe\x93\xe5\x87\xba\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x9c  [1,2048,7,7]\n    :param softmax_weight: \xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe7\x9a\x84\xe6\x9d\x83\xe9\x87\x8d [1000,2048]\n    :param class_ids: \xe6\x8c\x89\xe7\x85\xa7\xe6\xa6\x82\xe7\x8e\x87\xe4\xbb\x8e\xe5\xa4\xa7\xe5\x88\xb0\xe5\xb0\x8f\xe6\x8e\x92\xe5\xba\x8f\xe5\x90\x8e\xe7\x9a\x84 \xe6\xa6\x82\xe7\x8e\x87\xe7\x9a\x84\xe4\xb8\x8b\xe6\xa0\x87\n    \'\'\'\n    # b:  batch size\n    # c:  channel \xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\n    b, c, h, w = activation.shape\n    cams = []\n    for idx in class_ids:\n        #[2048,49]\n        activation = activation.reshape(c, h * w)\n        # dot \xe8\xae\xa1\xe7\xae\x97\xe4\xb8\xa4\xe4\xb8\xaa\xe5\xbc\xa0\xe9\x87\x8f\xe7\x9a\x84\xe7\x82\xb9\xe4\xb9\x98 (\xe5\x86\x85\xe7\xa7\xaf). \xe7\x9f\xa9\xe9\x98\xb5\xe4\xb9\x98\xe6\xb3\x95\n        # [1.49]\n        cam = softmax_weight[idx].dot(activation)\n        # [7.7]\n        cam = cam.reshape(h, w)\n        # \xe5\xbd\x92\xe4\xb8\x80\xe5\x8c\x96 normalize to [0, 1]\n        cam =  (cam - cam.min()) / (cam.max() - cam.min())\n        # \xe8\xbd\xac\xe5\x8c\x96\xe5\x88\xb00-255\xe4\xb9\x8b\xe9\x97\xb4 conver to [0, 255]\n        cam = np.uint8(255 * cam)\n        # reshape to (224, 224)\n        # \xe4\xbb\x8e[7,7] resize\xe5\x88\xb0[224,224]\n        cams.append(cv2.resize(cam, (64, 128)))\n\n    return cams\n\nif __name__ == \'__main__\':\n    #\xe5\xbc\x80\xe5\xa7\x8b\xe8\xae\xad\xe7\xbb\x83\n    # train()\n\n    # \xe5\x8f\xaf\xe8\xa7\x86\xe5\x8c\x96class_activation_map\n    visualize_cam()\n'"
FPN_pytorch/fpn.py,4,"b'\'\'\'FPN in PyTorch.\n\nSee the paper ""Feature Pyramid Networks for Object Detection"" for more details.\n\'\'\'\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom torch.autograd import Variable\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, in_planes, planes, stride=1):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != self.expansion*planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(self.expansion*planes)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = F.relu(self.bn2(self.conv2(out)))\n        out = self.bn3(self.conv3(out))\n        out += self.shortcut(x)\n        out = F.relu(out)\n        return out\n\n\nclass FPN(nn.Module):\n    \'\'\'\n    \xe7\xbb\xa7\xe6\x89\xbfnn.Module\xef\xbc\x8c\xe5\xae\x9e\xe7\x8e\xb0\xe8\x87\xaa\xe5\xae\x9a\xe4\xb9\x89\xe7\xbd\x91\xe7\xbb\x9c\xe6\xa8\xa1\xe5\x9e\x8b\n    \'\'\'\n    def __init__(self, block, num_blocks):\n        super(FPN, self).__init__()\n        # \xe8\xbe\x93\xe5\x85\xa5\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\n        self.in_planes = 64\n        # nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)\n\n        # \xe5\x8e\x9f\xe8\xae\xba\xe6\x96\x87\xe7\xbd\x91\xe7\xbb\x9c\xe7\xbb\x93\xe6\x9e\x84 in_channels=3  out_channels=64\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        # \xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0BN\xe5\xb1\x82\xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0\xe6\x98\xaf\xe8\xbe\x93\xe5\x87\xba\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0out_channels=64\n        self.bn1 = nn.BatchNorm2d(64)\n\n        # Bottom-up layers\n        # \xe8\x87\xaa\xe5\xba\x95\xe5\x90\x91\xe4\xb8\x8a\xe7\x9a\x84\xe7\xbd\x91\xe7\xbb\x9c   resnet\xe7\xbd\x91\xe7\xbb\x9c\n        self.layer1 = self._make_layer(block,  64, num_blocks[0], stride=1)\n        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n\n        # Top layer \xef\xbc\x88\xe6\x9c\x80\xe9\xa1\xb6\xe5\xb1\x82\xe5\x8f\xaa\xe6\x9c\x89\xe4\xbe\xa7\xe8\xbe\xb9\xe8\xbf\x9e\xe6\x8e\xa5\xef\xbc\x8ckernel_size=1\xe7\x9b\xae\xe7\x9a\x84\xe5\x87\x8f\xe5\xb0\x91\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\xef\xbc\x8c\xe5\xbd\xa2\xe7\x8a\xb6\xe4\xb8\x8d\xe5\x8f\x98\xef\xbc\x89\n        self.toplayer = nn.Conv2d(2048, 256, kernel_size=1, stride=1, padding=0)  # Reduce channels  \xe5\x87\x8f\xe5\xb0\x91\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\n\n        # Smooth layers   \xe5\xb9\xb3\xe6\xbb\x91\xe5\xb1\x82\n        # \xe4\xbd\x9c\xe7\x94\xa8\xef\xbc\x9a\xe5\x9c\xa8\xe8\x9e\x8d\xe5\x90\x88\xe4\xb9\x8b\xe5\x90\x8e\xe8\xbf\x98\xe4\xbc\x9a\xe5\x86\x8d\xe9\x87\x87\xe7\x94\xa83*3\xe7\x9a\x84\xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8\xe5\xaf\xb9\xe6\xaf\x8f\xe4\xb8\xaa\xe8\x9e\x8d\xe5\x90\x88\xe7\xbb\x93\xe6\x9e\x9c\xe8\xbf\x9b\xe8\xa1\x8c\xe5\x8d\xb7\xe7\xa7\xaf\xef\xbc\x8c\xe7\x9b\xae\xe7\x9a\x84\xe6\x98\xaf\xe6\xb6\x88\xe9\x99\xa4\xe4\xb8\x8a\xe9\x87\x87\xe6\xa0\xb7\xe7\x9a\x84\xe6\xb7\xb7\xe5\x8f\xa0\xe6\x95\x88\xe5\xba\x94\n        self.smooth1 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n        self.smooth2 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n        self.smooth3 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n\n        # Lateral layers   \xe4\xbe\xa7\xe8\xbe\xb9\xe5\xb1\x82\n        # \xef\xbc\x881*1\xe7\x9a\x84\xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8\xe7\x9a\x84\xe4\xb8\xbb\xe8\xa6\x81\xe4\xbd\x9c\xe7\x94\xa8\xe6\x98\xaf\xe5\x87\x8f\xe5\xb0\x91\xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8\xe7\x9a\x84\xe4\xb8\xaa\xe6\x95\xb0\xef\xbc\x8c\xe4\xb9\x9f\xe5\xb0\xb1\xe6\x98\xaf\xe5\x87\x8f\xe5\xb0\x91\xe4\xba\x86feature map\xe7\x9a\x84\xe4\xb8\xaa\xe6\x95\xb0\xef\xbc\x8c\xe5\xb9\xb6\xe4\xb8\x8d\xe6\x94\xb9\xe5\x8f\x98feature map\xe7\x9a\x84\xe5\xb0\xba\xe5\xaf\xb8\xe5\xa4\xa7\xe5\xb0\x8f\xe3\x80\x82\xef\xbc\x89\n        self.latlayer1 = nn.Conv2d(1024, 256, kernel_size=1, stride=1, padding=0)\n        self.latlayer2 = nn.Conv2d( 512, 256, kernel_size=1, stride=1, padding=0)\n        self.latlayer3 = nn.Conv2d( 256, 256, kernel_size=1, stride=1, padding=0)\n\n    def _make_layer(self, block, planes, num_blocks, stride):\n        \'\'\'\n        resnet\xe7\xbd\x91\xe7\xbb\x9c\n        \'\'\'\n        strides = [stride] + [1]*(num_blocks-1)\n        layers = []\n        for stride in strides:\n            layers.append(block(self.in_planes, planes, stride))\n            self.in_planes = planes * block.expansion\n        return nn.Sequential(*layers)\n\n    def _upsample_add(self, x, y):\n        \'\'\'\n        Upsample and add two feature maps.\n        \xe4\xb8\x8a\xe9\x87\x87\xe6\xa0\xb7 \xe5\xb9\xb6 \xe5\xb0\x86\xe4\xb8\xa4\xe4\xb8\xaafeature maps\xe6\xb1\x82\xe5\x92\x8c\n        Args:\n          x: (Variable) top feature map to be upsampled.  \xe5\xb0\x86\xe8\xa6\x81\xe4\xb8\x8a\xe9\x87\x87\xe6\xa0\xb7\xe7\x9a\x84 \xe4\xb8\x8a\xe5\xb1\x82feature map\n          y: (Variable) lateral feature map.    \xe4\xbe\xa7\xe8\xbe\xb9\xe7\x9a\x84feature map\n\n        Returns:\n          (Variable) added feature map.\n\n        Note in PyTorch, when input size is odd, the upsampled feature map\n        with `F.upsample(..., scale_factor=2, mode=\'nearest\')`\n        maybe not equal to the lateral feature map size.\n        \xe5\x9c\xa8PyTorch\xe4\xb8\xad\xef\xbc\x8c\xe5\xbd\x93\xe8\xbe\x93\xe5\x85\xa5\xe5\xa4\xa7\xe5\xb0\x8f\xe4\xb8\xba\xe5\xa5\x87\xe6\x95\xb0\xe6\x97\xb6\xef\xbc\x8c\xe8\xaf\xb7\xe6\xb3\xa8\xe6\x84\x8f\xe4\xb8\x8a\xe9\x87\x87\xe6\xa0\xb7\xe7\x9a\x84\xe7\x89\xb9\xe5\xbe\x81\xe6\x98\xa0\xe5\xb0\x84\n\xc2\xa0\xc2\xa0\xc2\xa0\xc2\xa0\xe7\x94\xa8\'F.upsample\xef\xbc\x88...\xef\xbc\x8cscale_factor = 2\xef\xbc\x8cmode =\'nearest\'\xef\xbc\x89`\n\xc2\xa0\xc2\xa0\xc2\xa0\xc2\xa0\xe5\x8f\xaf\xe8\x83\xbd\xe4\xb8\x8d\xe7\xad\x89\xe4\xba\x8e\xe6\xa8\xaa\xe5\x90\x91\xe7\x89\xb9\xe5\xbe\x81\xe5\x9c\xb0\xe5\x9b\xbe\xe5\xb0\xba\xe5\xaf\xb8\xe3\x80\x82\n\n        e.g.\n        original input size: [N,_,15,15] ->\n        conv2d feature map size: [N,_,8,8] ->\n        upsampled feature map size: [N,_,16,16]\n\n        So we choose bilinear upsample which supports arbitrary output sizes.\n        \xe6\x89\x80\xe4\xbb\xa5\xe6\x88\x91\xe4\xbb\xac\xe9\x80\x89\xe6\x8b\xa9\xe6\x94\xaf\xe6\x8c\x81\xe4\xbb\xbb\xe6\x84\x8f\xe8\xbe\x93\xe5\x87\xba\xe5\xa4\xa7\xe5\xb0\x8f\xe7\x9a\x84\xe5\x8f\x8c\xe7\xba\xbf\xe6\x80\xa7\xe4\xb8\x8a\xe9\x87\x87\xe6\xa0\xb7\xe3\x80\x82\n        \'\'\'\n        _,_,H,W = y.size()\n        # \xe4\xbd\xbf\xe7\x94\xa8 \xe5\x8f\x8c\xe7\xba\xbf\xe6\x80\xa7\xe6\x8f\x92\xe5\x80\xbcbilinear\xe5\xaf\xb9x\xe8\xbf\x9b\xe8\xa1\x8c\xe4\xb8\x8a\xe9\x87\x87\xe6\xa0\xb7\xef\xbc\x8c\xe4\xb9\x8b\xe5\x90\x8e\xe4\xb8\x8ey\xe9\x80\x90\xe5\x85\x83\xe7\xb4\xa0\xe7\x9b\xb8\xe5\x8a\xa0\n        return F.upsample(x, size=(H,W), mode=\'bilinear\') + y\n\n    def forward(self, x):\n        # Bottom-up  \xe8\x87\xaa\xe5\xba\x95\xe5\x90\x91\xe4\xb8\x8a   conv -> batchnmorm -> relu  ->maxpool\n        c1 = F.relu(self.bn1(self.conv1(x)))\n        c1 = F.max_pool2d(c1, kernel_size=3, stride=2, padding=1)\n\n        # resnet\xe7\xbd\x91\xe7\xbb\x9c\n        c2 = self.layer1(c1)\n        c3 = self.layer2(c2)\n        c4 = self.layer3(c3)\n        c5 = self.layer4(c4)\n\n        # Top-down  \xe8\x87\xaa\xe9\xa1\xb6\xe5\x90\x91\xe4\xb8\x8b\xe5\xb9\xb6\xe4\xb8\x8e\xe4\xbe\xa7\xe8\xbe\xb9\xe7\x9b\xb8\xe8\xbf\x9e\n        p5 = self.toplayer(c5)  #\xe5\x87\x8f\xe5\xb0\x91\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\n        p4 = self._upsample_add(p5, self.latlayer1(c4))\n        p3 = self._upsample_add(p4, self.latlayer2(c3))\n        p2 = self._upsample_add(p3, self.latlayer3(c2))\n\n        # Smooth  \xe5\xb9\xb3\xe6\xbb\x91\xe5\xb1\x82\xef\xbc\x88\xe5\x9c\xa8\xe8\x9e\x8d\xe5\x90\x88\xe4\xb9\x8b\xe5\x90\x8e\xe8\xbf\x98\xe4\xbc\x9a\xe5\x86\x8d\xe9\x87\x87\xe7\x94\xa83*3\xe7\x9a\x84\xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8\xe5\xaf\xb9\xe6\xaf\x8f\xe4\xb8\xaa\xe8\x9e\x8d\xe5\x90\x88\xe7\xbb\x93\xe6\x9e\x9c\xe8\xbf\x9b\xe8\xa1\x8c\xe5\x8d\xb7\xe7\xa7\xaf\xef\xbc\x8c\xe7\x9b\xae\xe7\x9a\x84\xe6\x98\xaf\xe6\xb6\x88\xe9\x99\xa4\xe4\xb8\x8a\xe9\x87\x87\xe6\xa0\xb7\xe7\x9a\x84\xe6\xb7\xb7\xe5\x8f\xa0\xe6\x95\x88\xe5\xba\x94\xef\xbc\x89\n        p4 = self.smooth1(p4)\n        p3 = self.smooth2(p3)\n        p2 = self.smooth3(p2)\n        return p2, p3, p4, p5\n\n\ndef FPN101():\n    # [2,4,23,3]\xe4\xb8\xbaFPN101\xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0\n    # return FPN(Bottleneck, [2,4,23,3])\n\n    #[2,2,2,2]\xe4\xb8\xbaFPN18\xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0\n    return FPN(Bottleneck, [2,2,2,2])\n\n\ndef test():\n    # \xe6\x96\xb0\xe5\xbb\xbaFPN101\xe7\xbd\x91\xe7\xbb\x9c\n    net = FPN101()\n    print(\'\xe7\xbd\x91\xe7\xbb\x9c\xe7\xbb\x93\xe6\x9e\x84\xe4\xb8\xba\')\n    print(net)\n    # \xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xef\xbc\x8c\xe5\xbe\x97\xe5\x88\xb0\xe7\xbd\x91\xe7\xbb\x9c\xe8\xbe\x93\xe5\x87\xba\xe5\x80\xbc fms\xe5\x8d\xb3\xe4\xb8\xbap2, p3, p4, p5\n    fms = net(Variable(torch.randn(1,3,224,224)))\n    print(\'\xe7\xbd\x91\xe7\xbb\x9c\xe8\xbe\x93\xe5\x87\xba\xe7\x9a\x84\xe5\x86\x85\xe5\xae\xb9\xe4\xb8\xba\')\n    for fm in fms:\n        print(fm.size())\n\ntest()\n'"
FPN_pytorch/retina_fpn.py,4,"b'\'\'\'RetinaFPN in PyTorch.\n\nSee the paper ""Focal Loss for Dense Object Detection"" for more details.\n\'\'\'\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom torch.autograd import Variable\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, in_planes, planes, stride=1):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != self.expansion*planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(self.expansion*planes)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = F.relu(self.bn2(self.conv2(out)))\n        out = self.bn3(self.conv3(out))\n        out += self.shortcut(x)\n        out = F.relu(out)\n        return out\n\n\nclass RetinaFPN(nn.Module):\n    def __init__(self, block, num_blocks):\n        super(RetinaFPN, self).__init__()\n        self.in_planes = 64\n\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n\n        # Bottom-up layers\n        self.layer2 = self._make_layer(block,  64, num_blocks[0], stride=1)\n        self.layer3 = self._make_layer(block, 128, num_blocks[1], stride=2)\n        self.layer4 = self._make_layer(block, 256, num_blocks[2], stride=2)\n        self.layer5 = self._make_layer(block, 512, num_blocks[3], stride=2)\n        self.conv6 = nn.Conv2d(2048, 256, kernel_size=3, stride=2, padding=1)\n        self.conv7 = nn.Conv2d( 256, 256, kernel_size=3, stride=2, padding=1)\n\n        # Top layer\n        self.toplayer = nn.Conv2d(2048, 256, kernel_size=1, stride=1, padding=0)  # Reduce channels\n\n        # Smooth layers\n        self.smooth1 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n        self.smooth2 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n\n        # Lateral layers\n        self.latlayer1 = nn.Conv2d(1024, 256, kernel_size=1, stride=1, padding=0)\n        self.latlayer2 = nn.Conv2d( 512, 256, kernel_size=1, stride=1, padding=0)\n\n    def _make_layer(self, block, planes, num_blocks, stride):\n        strides = [stride] + [1]*(num_blocks-1)\n        layers = []\n        for stride in strides:\n            layers.append(block(self.in_planes, planes, stride))\n            self.in_planes = planes * block.expansion\n        return nn.Sequential(*layers)\n\n    def _upsample_add(self, x, y):\n        \'\'\'Upsample and add two feature maps.\n\n        Args:\n          x: (Variable) top feature map to be upsampled.\n          y: (Variable) lateral feature map.\n\n        Returns:\n          (Variable) added feature map.\n\n        Note in PyTorch, when input size is odd, the upsampled feature map\n        with `F.upsample(..., scale_factor=2, mode=\'nearest\')`\n        maybe not equal to the lateral feature map size.\n\n        e.g.\n        original input size: [N,_,15,15] ->\n        conv2d feature map size: [N,_,8,8] ->\n        upsampled feature map size: [N,_,16,16]\n\n        So we choose bilinear upsample which supports arbitrary output sizes.\n        \'\'\'\n        _,_,H,W = y.size()\n        return F.upsample(x, size=(H,W), mode=\'bilinear\') + y\n\n    def forward(self, x):\n        # Bottom-up\n        c1 = F.relu(self.bn1(self.conv1(x)))\n        c1 = F.max_pool2d(c1, kernel_size=3, stride=2, padding=1)\n        c2 = self.layer2(c1)\n        c3 = self.layer3(c2)\n        c4 = self.layer4(c3)\n        c5 = self.layer5(c4)\n        p6 = self.conv6(c5)\n        p7 = self.conv7(F.relu(p6))\n        # Top-down\n        p5 = self.toplayer(c5)\n        p4 = self._upsample_add(p5, self.latlayer1(c4))\n        p3 = self._upsample_add(p4, self.latlayer2(c3))\n        # Smooth\n        p4 = self.smooth1(p4)\n        p3 = self.smooth2(p3)\n        return p3, p4, p5, p6, p7\n\n\ndef RetinaFPN101():\n    # return RetinaFPN(Bottleneck, [2,4,23,3])\n    return RetinaFPN(Bottleneck, [2,2,2,2])\n\n\ndef test():\n    net = RetinaFPN101()\n    fms = net(Variable(torch.randn(1,3,600,900)))\n    for fm in fms:\n        print(fm.size())\n\ntest()\n'"
FP_SSD_pytorch/ssd.py,11,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom SSD_pytorch.models import *\nfrom SSD_pytorch.utils.config import opt\nimport os\n\n\nclass SSD(nn.Module):\n    """"""Single Shot Multibox Architecture\n    The network is composed of a base VGG network followed by the\n    added multibox conv layers.  Each multibox layer branches into\n        1) conv2d for class conf scores\n        2) conv2d for localization predictions\n        3) associated priorbox layer to produce default bounding\n           boxes specific to the layer\'s feature map size.\n    SSD\xe6\xa8\xa1\xe5\x9e\x8b\xe7\x94\xb1\xe5\x8e\xbb\xe6\x8e\x89\xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe5\xb1\x82\xe7\x9a\x84vgg\xe7\xbd\x91\xe7\xbb\x9c\xe4\xb8\xba\xe5\x9f\xba\xe7\xa1\x80\xe7\xbb\x84\xe6\x88\x90\xe3\x80\x82\xe5\x9c\xa8\xe4\xb9\x8b\xe5\x90\x8e\xe6\xb7\xbb\xe5\x8a\xa0\xe4\xba\x86\xe5\xa4\x9a\xe7\x9b\x92\xe8\xbd\xac\xe5\x8c\x96\xe5\xb1\x82\xe3\x80\x82\n    \xe6\xaf\x8f\xe4\xb8\xaa\xe5\xa4\x9a\xe7\x9b\x92\xe5\xb1\x82\xe5\x88\x86\xe6\x94\xaf\xe6\x98\xaf\xef\xbc\x9a\n        1\xef\xbc\x89conv2d \xe8\x8e\xb7\xe5\x8f\x96\xe5\x88\x86\xe7\xb1\xbb\xe7\xbd\xae\xe4\xbf\xa1\xe5\xba\xa6\n        2\xef\xbc\x89conv2d\xe8\xbf\x9b\xe8\xa1\x8c\xe5\x9d\x90\xe6\xa0\x87\xe4\xbd\x8d\xe7\xbd\xae\xe9\xa2\x84\xe6\xb5\x8b\n        3\xef\xbc\x89\xe7\x9b\xb8\xe5\x85\xb3\xe5\xb1\x82\xe5\x8e\xbb\xe4\xba\xa7\xe7\x94\x9f\xe7\x89\xb9\xe5\xae\x9a\xe4\xba\x8e\xe8\xaf\xa5\xe5\xb1\x82\xe7\x89\xb9\xe5\xbe\x81\xe5\x9b\xbe\xe5\xa4\xa7\xe5\xb0\x8f\xe7\x9a\x84\xe9\xbb\x98\xe8\xae\xa4\xe7\x9a\x84\xe9\xa2\x84\xe6\xb5\x8b\xe6\xa1\x86bounding  boxes\n\n\n\n    See: https://arxiv.org/pdf/1512.02325.pdf for more details.\n\n    Args:\n        phase: (string) Can be ""test"" or ""train""\n        size: input image size  \xe8\xbe\x93\xe5\x85\xa5\xe7\x9a\x84\xe5\x9b\xbe\xe5\x83\x8f\xe5\xb0\xba\xe5\xaf\xb8\n        base: VGG16 layers for input, size of either 300 or 500   \xe7\xbb\x8f\xe8\xbf\x87\xe4\xbf\xae\xe6\x94\xb9\xe7\x9a\x84vgg\xe7\xbd\x91\xe7\xbb\x9c\n        extras: extra layers that feed to multibox loc and conf layers\n                \xe6\x8f\x90\xe4\xbe\x9b\xe5\xa4\x9a\xe7\x9b\x92\xe5\xae\x9a\xe4\xbd\x8d\xe7\x9a\x84\xe6\xa0\xbc\xe5\xa4\x96\xe5\xb1\x82  \xe5\x92\x8c \xe5\x88\x86\xe7\xb1\xbb\xe7\xbd\xae\xe4\xbf\xa1\xe5\xb1\x82\xef\xbc\x88vgg\xe7\xbd\x91\xe7\xbb\x9c\xe5\x90\x8e\xe9\x9d\xa2\xe6\x96\xb0\xe5\xa2\x9e\xe7\x9a\x84\xe9\xa2\x9d\xe5\xa4\x96\xe5\xb1\x82\xef\xbc\x89\n        head: ""multibox head"" consists of loc and conf conv layers\n                \xe7\x94\xb1\xe5\xae\x9a\xe4\xbd\x8d\xe5\x92\x8c\xe5\x88\x86\xe7\xb1\xbb\xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82\xe7\xbb\x84\xe6\x88\x90\xe7\x9a\x84multibox head\n                (loc_layers, conf_layers)     vgg\xe4\xb8\x8eextras\xe4\xb8\xad\xe8\xbf\x9b\xe8\xa1\x8c\xe5\x88\x86\xe7\xb1\xbb\xe5\x92\x8c\xe5\x9b\x9e\xe5\xbd\x92\xe7\x9a\x84\xe5\xb1\x82\n    """"""\n\n    def __init__(self, phase, size, base, extras, head, num_classes):\n        super(SSD, self).__init__()\n        self.phase = phase\n        self.num_classes = num_classes\n        self.cfg = opt.voc\n        # \xe6\x96\xb0\xe5\xae\x9a\xe4\xb9\x89\xe4\xb8\x80\xe4\xb8\xaa\xe7\xb1\xbb\xef\xbc\x8c\xe8\xaf\xa5\xe7\xb1\xbb\xe7\x9a\x84\xe5\x8a\x9f\xe8\x83\xbd\xef\xbc\x9a\xe5\xaf\xb9\xe4\xba\x8e\xe6\xaf\x8f\xe4\xb8\xaafeature map\xef\xbc\x8c\xe7\x94\x9f\xe6\x88\x90\xe9\xa2\x84\xe6\xb5\x8b\xe6\xa1\x86\xef\xbc\x88\xe4\xb8\xad\xe5\xbf\x83\xe5\x9d\x90\xe6\xa0\x87\xe5\x8f\x8a\xe5\x81\x8f\xe7\xa7\xbb\xe9\x87\x8f\xef\xbc\x89\n        self.priorbox = PriorBox(self.cfg)\n        # \xe8\xb0\x83\xe7\x94\xa8forward\xef\xbc\x8c\xe8\xbf\x94\xe5\x9b\x9e\xe7\x94\x9f\xe6\x88\x90\xe7\x9a\x84\xe9\xa2\x84\xe6\xb5\x8b\xe6\xa1\x86\xe7\xbb\x93\xe6\x9e\x9c\n        # \xe5\xaf\xb9\xe4\xba\x8e\xe6\x89\x80\xe6\x9c\x89\xe9\xa2\x84\xe6\xb5\x8b\xe7\x9a\x84feature map\xef\xbc\x8c\xe5\xad\x98\xe5\x82\xa8\xe7\x9d\x80\xe7\x94\x9f\xe6\x88\x90\xe7\x9a\x84\xe4\xb8\x8d\xe5\x90\x8c\xe9\x95\xbf\xe5\xae\xbd\xe6\xaf\x94\xe7\x9a\x84\xe9\xbb\x98\xe8\xae\xa4\xe6\xa1\x86\xef\xbc\x88\xe5\x8f\xaf\xe4\xbb\xa5\xe7\x90\x86\xe8\xa7\xa3\xe4\xb8\xbaanchor\xef\xbc\x89\n        self.priors = Variable(self.priorbox.forward(), volatile=True)\n        #300\n        self.size = size\n\n        # SSD network\xe8\x8c\x83\xe5\x9b\xb4\n        # \xe7\xbb\x8f\xe8\xbf\x87\xe4\xbf\xae\xe6\x94\xb9\xe7\x9a\x84vgg\xe7\xbd\x91\xe7\xbb\x9c\n        self.vgg = nn.ModuleList(base)\n        # Layer learns to scale the l2 normalized features from conv4_3\n        # Layer\xe5\xb1\x82\xe4\xbb\x8econv4_3\xe5\xad\xa6\xe4\xb9\xa0\xe5\x8e\xbb\xe7\xbc\xa9\xe6\x94\xbel2\xe6\xad\xa3\xe5\x88\x99\xe5\x8c\x96\xe7\x89\xb9\xe5\xbe\x81\n        # \xe8\xae\xba\xe6\x96\x87\xe4\xb8\xadconv4_3 \xe7\x9b\xb8\xe6\xaf\x94\xe8\xbe\x83\xe4\xba\x8e\xe5\x85\xb6\xe4\xbb\x96\xe7\x9a\x84layers\xef\xbc\x8c\xe6\x9c\x89\xe7\x9d\x80\xe4\xb8\x8d\xe5\x90\x8c\xe7\x9a\x84 feature scale\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe4\xbd\xbf\xe7\x94\xa8 ParseNet \xe4\xb8\xad\xe7\x9a\x84 L2 normalization \xe6\x8a\x80\xe6\x9c\xaf\n        # \xe5\xb0\x86conv4_3 feature map \xe4\xb8\xad\xe6\xaf\x8f\xe4\xb8\x80\xe4\xb8\xaa\xe4\xbd\x8d\xe7\xbd\xae\xe7\x9a\x84 feature norm scale \xe5\x88\xb0 20\xef\xbc\x8c\xe5\xb9\xb6\xe4\xb8\x94\xe5\x9c\xa8 back-propagation \xe4\xb8\xad\xe5\xad\xa6\xe4\xb9\xa0\xe8\xbf\x99\xe4\xb8\xaa scale\n        self.L2Norm = L2Norm(512, 20)\n        # vgg\xe7\xbd\x91\xe7\xbb\x9c\xe5\x90\x8e\xe9\x9d\xa2\xe6\x96\xb0\xe5\xa2\x9e\xe7\x9a\x84\xe9\xa2\x9d\xe5\xa4\x96\xe5\xb1\x82\n        self.extras = nn.ModuleList(extras)\n        # vgg\xe4\xb8\x8eextras\xe4\xb8\xad\xe8\xbf\x9b\xe8\xa1\x8c\xe5\x88\x86\xe7\xb1\xbb\xe5\x92\x8c\xe5\x9b\x9e\xe5\xbd\x92\xe7\x9a\x84\xe5\xb1\x82\n        self.loc = nn.ModuleList(head[0])\n        self.conf = nn.ModuleList(head[1])\n\n        # \xe5\xa6\x82\xe6\x9e\x9c\xe7\xbd\x91\xe7\xbb\x9c\xe7\x94\xa8\xe4\xba\x8e\xe6\xb5\x8b\xe8\xaf\x95\xef\xbc\x8c\xe5\x88\x99\xe5\x8a\xa0\xe5\x85\xa5softmax\xe5\x92\x8c\xe6\xa3\x80\xe6\xb5\x8b\n        if phase == \'test\':\n            self.softmax = nn.Softmax(dim=-1)\n            self.detect = Detect(num_classes, 0, 200, 0.01, 0.45)\n\n        #=====bobo\xe6\x96\xb0\xe5\xa2\x9e==================\n        # pool2\xe5\x88\xb0conv4_3  \xe6\x89\xa9\xe5\xbc\xa0\xe5\x8d\xb7\xe7\xa7\xaf\xef\xbc\x8c\xe5\xb0\xba\xe5\xba\xa6\xe5\xb0\x91\xe4\xb8\x80\xe5\x8d\x8a\n        self.DilationConv_128_128= nn.Conv2d(in_channels=128,out_channels= 128, kernel_size=3, padding=2, dilation=2,stride=2)\n        # conv4_3\xe5\x88\xb0conv4_3  \xe5\xb0\xba\xe5\xba\xa6\xe4\xb8\x8d\xe5\x8f\x98\n        self.conv_512_256 = nn.Conv2d(in_channels=512, out_channels=256, kernel_size=3, padding=1, stride=1)\n        # fc7 \xe5\x88\xb0 conv4_3    \xe5\x8f\x8d\xe5\x8d\xb7\xe7\xa7\xaf\xe4\xb8\x8a\xe9\x87\x87\xe6\xa0\xb7\xef\xbc\x8c\xe5\xb0\xba\xe5\xba\xa6\xe5\xa4\xa7\xe4\xb8\x80\xe5\x80\x8d\n        self.DeConv_1024_128 = nn.ConvTranspose2d(in_channels=1024,out_channels=128,kernel_size=2,stride=2)\n\n        # conv4_3 \xe5\x88\xb0FC7  \xe6\x89\xa9\xe5\xbc\xa0\xe5\x8d\xb7\xe7\xa7\xaf\xef\xbc\x8c\xe5\xb0\xba\xe5\xba\xa6\xe5\xb0\x91\xe4\xb8\x80\xe5\x8d\x8a\n        self.DilationConv_512_128 = nn.Conv2d(in_channels=512, out_channels=128, kernel_size=3, padding=2, dilation=2,stride=2)\n        # FC7\xe5\x88\xb0FC7 \xe5\xb0\xba\xe5\xba\xa6\xe4\xb8\x8d\xe5\x8f\x98\n        self.conv_1024_256 = nn.Conv2d(in_channels=1024, out_channels=256, kernel_size=3, padding=1, stride=1)\n        # conv8_2 \xe5\x88\xb0 FC7    \xe5\x8f\x8d\xe5\x8d\xb7\xe7\xa7\xaf\xe4\xb8\x8a\xe9\x87\x87\xe6\xa0\xb7\xef\xbc\x8c\xe5\xb0\xba\xe5\xba\xa6\xe5\xa4\xa7\xe4\xb8\x80\xe5\x80\x8d  10->19\n        self.DeConv_512_128 = nn.ConvTranspose2d(in_channels=512, out_channels=128, kernel_size=3, stride=2,padding=1)\n\n\n        # conv5_3\xe5\x88\xb0conv8_2\n        self.DilationConv_512_128_2 = nn.Conv2d(in_channels=512, out_channels=128, kernel_size=3, padding=2, dilation=2, stride=2)\n        # conv8_2\xe5\x88\xb0conv8_2 \xe5\xb0\xba\xe5\xba\xa6\xe4\xb8\x8d\xe5\x8f\x98\n        self.conv_512_256_2 = nn.Conv2d(in_channels=512, out_channels=256, kernel_size=3, padding=1, stride=1)\n        # conv9_2\xe5\x88\xb0conv8_2\n        self.DeConv_256_128_2 = nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=2, stride=2)\n\n        # \xe5\xb9\xb3\xe6\xbb\x91\xe5\xb1\x82\n        self.smooth = nn.Conv2d(512, 512, kernel_size = 3, padding = 1, stride = 1)\n\n        # \xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0BN\xe5\xb1\x82\xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0\xe6\x98\xaf\xe8\xbe\x93\xe5\x87\xba\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0out_channels\n        self.bn = nn.BatchNorm2d(128)\n    def forward(self, x):\n        """"""Applies network layers and ops on input image(s) x.\n        \xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\n\n        Args:\n            x: input image or batch of images. Shape: [batch,3,300,300].\n\n        Return:\n            Depending on phase:\n            test\xe6\xb5\x8b\xe8\xaf\x95\xe9\x9b\x86:\n                Variable(tensor) of output class label predictions,\n                confidence score, and corresponding location predictions for\n                each object detected. Shape: [batch,topk,7]\n\n            train\xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86:\n                list of concat outputs from:\n                    1: \xe5\x88\x86\xe7\xb1\xbb\xe5\xb1\x82confidence layers, Shape: [batch*num_priors,num_classes]\n                    2: \xe5\x9b\x9e\xe5\xbd\x92\xe5\xae\x9a\xe4\xbd\x8d\xe5\xb1\x82localization layers, Shape: [batch,num_priors*4]\n                    3: priorbox layers, Shape: [2,num_priors*4]\n        """"""\n        # sources\xe4\xbf\x9d\xe5\xad\x98 \xe7\xbd\x91\xe7\xbb\x9c\xe7\x94\x9f\xe6\x88\x90\xe7\x9a\x84\xe4\xb8\x8d\xe5\x90\x8c\xe5\xb1\x82feature map\xe7\xbb\x93\xe6\x9e\x9c\xef\xbc\x8c\xe4\xbb\xa5\xe4\xbe\xbf\xe4\xbd\xbf\xe7\x94\xa8\xe8\xbf\x99\xe4\xba\x9bfeature map\xe6\x9d\xa5\xe8\xbf\x9b\xe8\xa1\x8c\xe5\x88\x86\xe7\xb1\xbb\xe4\xb8\x8e\xe5\x9b\x9e\xe5\xbd\x92\n        sources = list()\n        # \xe4\xbf\x9d\xe5\xad\x98\xe9\xa2\x84\xe6\xb5\x8b\xe5\xb1\x82\xe4\xb8\x8d\xe5\x90\x8cfeature map\xe9\x80\x9a\xe8\xbf\x87\xe5\x9b\x9e\xe5\xbd\x92\xe5\x92\x8c\xe5\x88\x86\xe7\xb1\xbb\xe7\xbd\x91\xe7\xbb\x9c\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe7\xbb\x93\xe6\x9e\x9c\n        loc = list()\n        conf = list()\n\n        # \xe5\x8e\x9f\xe8\xae\xba\xe6\x96\x87\xe4\xb8\xadvgg\xe7\x9a\x84conv4_3\xef\xbc\x8crelu\xe4\xb9\x8b\xe5\x90\x8e\xe5\x8a\xa0\xe5\x85\xa5L2 Normalization\xe6\xad\xa3\xe5\x88\x99\xe5\x8c\x96\xef\xbc\x8c\xe7\x84\xb6\xe5\x90\x8e\xe4\xbf\x9d\xe5\xad\x98feature map\n        # apply vgg up to conv4_3 relu\n        # \xe5\xb0\x86vgg\xe5\xb1\x82\xe7\x9a\x84feature map\xe4\xbf\x9d\xe5\xad\x98\n        # k\xe7\x9a\x84\xe8\x8c\x83\xe5\x9b\xb4\xe4\xb8\xba0-22\n\n        #=========\xe5\xbc\x80\xe5\xa7\x8b\xe4\xbf\x9d\xe5\xad\x98 \xe6\x89\x80\xe9\x9c\x80\xe7\x9a\x84\xe6\x89\x80\xe6\x9c\x89\xe4\xb8\xad\xe9\x97\xb4\xe4\xbf\xa1\xe6\x81\xaf\n\n\n        # \xe4\xbf\x9d\xe5\xad\x98pool2\xef\xbc\x88pool\xe4\xb8\x8b\xe6\xa0\x87\xe4\xbb\x8e1\xe5\xbc\x80\xe5\xa7\x8b\xef\xbc\x89\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x9c\n        # \xe7\xbb\x8f\xe8\xbf\x87maxpool\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe4\xb8\x8d\xe9\x9c\x80\xe8\xa6\x81L2Norm\xe6\xad\xa3\xe5\x88\x99\xe5\x8c\x96\n        for k in range(10):\n            x = self.vgg[k](x)\n        sources.append(x)\n\n        # \xe4\xbf\x9d\xe5\xad\x98conv4_3\xe7\xbb\x93\xe6\x9e\x9c\n        for k in range(10,23):\n            x = self.vgg[k](x)\n        s = self.L2Norm(x)\n        sources.append(s)\n\n        # \xe4\xbf\x9d\xe5\xad\x98conv5_3\xe7\xbb\x93\xe6\x9e\x9c  \xe7\xb1\xbb\xe4\xbc\xbcconv4_3\xe5\x8e\x9f\xe4\xbb\x93\xe5\xba\x93\xe4\xb8\x80\xe6\xa0\xb7\xef\xbc\x8c\xe5\x8a\xa0\xe5\x85\xa5L2Norm\n        for k in range(23, 30):\n            x = self.vgg[k](x)\n        s = self.L2Norm(x)\n        sources.append(s)\n\n        # \xe4\xbf\x9d\xe5\xad\x98 \xe5\x8e\x9ffc7\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe7\xbb\x93\xe6\x9e\x9c\n        # apply vgg up to fc7\xef\xbc\x8c\xe5\x8d\xb3\xe5\xb0\x86\xe5\x8e\x9ffc7\xe5\xb1\x82\xe6\x9b\xb4\xe6\x94\xb9\xe4\xb8\xba\xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82\xe8\xbe\x93\xe5\x87\xba\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x9c\xef\xbc\x8c\xe7\xbb\x8f\xe8\xbf\x87relu\xe4\xb9\x8b\xe5\x90\x8e\xe4\xbf\x9d\xe5\xad\x98\xe7\xbb\x93\xe6\x9e\x9c\n        # k\xe7\x9a\x84\xe8\x8c\x83\xe5\x9b\xb4\xe4\xb8\xba23 - \xe7\xbb\x93\xe6\x9d\x9f\n        for k in range(30, len(self.vgg)):\n            x = self.vgg[k](x)\n        sources.append(x)\n\n        # \xe5\xb0\x86\xe6\x96\xb0\xe5\x8a\xa0\xe7\x9a\x84\xe9\xa2\x9d\xe5\xa4\x96\xe5\xb1\x82 conv8_2\xe3\x80\x81conv9_2\xe3\x80\x81conv10_2\xe3\x80\x81conv11_2\xe7\xbb\x93\xe6\x9e\x9c\xe4\xbf\x9d\xe5\xad\x98\n        # apply extra layers and cache source layer outputs\n        # \xe5\xb0\x86\xe6\x96\xb0\xe5\xa2\x9e\xe5\xb1\x82\xe7\x9a\x84feature map\xe4\xbf\x9d\xe5\xad\x98\n        for k, v in enumerate(self.extras):\n            # \xe6\xaf\x8f\xe7\xbb\x8f\xe8\xbf\x87\xe4\xb8\x80\xe4\xb8\xaaconv\xe5\x8d\xb7\xe7\xa7\xaf\xef\xbc\x8c\xe9\x83\xbdrelu\xe4\xb8\x80\xe4\xb8\x8b\n            x = F.relu(v(x), inplace=True)\n            # \xe8\xae\xba\xe6\x96\x87\xe4\xb8\xad\xe9\x9a\x94\xe4\xb8\x80\xe4\xb8\xaaconv\xe4\xbf\x9d\xe5\xad\x98\xe4\xb8\x80\xe4\xb8\xaa\xe7\xbb\x93\xe6\x9e\x9c\n            if k % 2 == 1:\n                sources.append(x)\n\n        # \xe6\xad\xa4\xe6\x97\xb6sources\xe4\xbf\x9d\xe5\xad\x98\xe4\xba\x86\xe6\x89\x80\xe6\x9c\x89\xe4\xb8\xad\xe9\x97\xb4\xe7\xbb\x93\xe6\x9e\x9c\xef\xbc\x8c\xe8\xae\xba\xe6\x96\x87\xe4\xb8\xad\xe7\x9a\x84pool2\xe3\x80\x81conv4_3\xe3\x80\x81conv5_3\xe3\x80\x81fc7\xe3\x80\x81conv8_2\xe3\x80\x81conv9_2\xe3\x80\x81conv10_2\xe3\x80\x81conv11_2\n\n        # sources_final\xe4\xbf\x9d\xe5\xad\x98\xe5\x90\x84\xe5\xb1\x82\xe8\x9e\x8d\xe5\x90\x88\xe4\xb9\x8b\xe5\x90\x8e\xe7\x9a\x84\xe6\x9c\x80\xe7\xbb\x88\xe7\xbb\x93\xe6\x9e\x9c\n        sources_final=list()\n\n        # con4_3\xe5\xb1\x82\xe8\x9e\x8d\xe5\x90\x88\xe7\xbb\x93\xe6\x9e\x9c  self.bn1(self.conv1(x))\n        conv4_fp=torch.cat((F.relu(self.bn(self.DilationConv_128_128(sources[0])),inplace=True), F.relu(self.conv_512_256(sources[1]),inplace=True), F.relu(self.DeConv_1024_128(sources[3]),inplace=True)),1)\n        sources_final.append(F.relu(  self.smooth(conv4_fp) , inplace=True))\n        # FC7\xe5\xb1\x82\xe8\x9e\x8d\xe5\x90\x88\xe7\xbb\x93\xe6\x9e\x9c\n        fc7_fp = torch.cat((F.relu( self.bn(self.DilationConv_512_128(sources[1])) ,inplace=True),F.relu( self.conv_1024_256(sources[3]),inplace=True) ,F.relu(  self.DeConv_512_128(sources[4]),inplace=True)),1)\n        sources_final.append(F.relu( self.smooth(fc7_fp) , inplace=True))\n        # conv8_2\xe5\xb1\x82\xe8\x9e\x8d\xe5\x90\x88\xe7\xbb\x93\xe6\x9e\x9c\n        conv8_fp= torch.cat(( F.relu( self.bn(self.DilationConv_512_128_2(sources[2])),inplace=True) ,F.relu(self.conv_512_256_2(sources[4]) ,inplace=True)  ,F.relu( self.DeConv_256_128_2(sources[5]),inplace=True)  ),1)\n        sources_final.append( F.relu( self.smooth(conv8_fp) , inplace=True) )\n\n\n        # \xe4\xbf\x9d\xe5\xad\x98 conv9_2\xe3\x80\x81conv10_2\xe3\x80\x81conv11_2\n        sources_final.append(sources[5])\n        sources_final.append(sources[6])\n        sources_final.append(sources[7])\n\n\n        # apply multibox head to source layers\n        # permute  \xe5\xb0\x86tensor\xe7\x9a\x84\xe7\xbb\xb4\xe5\xba\xa6\xe6\x8d\xa2\xe4\xbd\x8d  \xe5\x8f\x82\xe6\x95\xb0\xe4\xb8\xba\xe6\x8d\xa2\xe4\xbd\x8d\xe9\xa1\xba\xe5\xba\x8f\n        #contiguous \xe8\xbf\x94\xe5\x9b\x9e\xe4\xb8\x80\xe4\xb8\xaa\xe5\x86\x85\xe5\xad\x98\xe8\xbf\x9e\xe7\xbb\xad\xe7\x9a\x84\xe6\x9c\x89\xe7\x9b\xb8\xe5\x90\x8c\xe6\x95\xb0\xe6\x8d\xae\xe7\x9a\x84tensor\n\n        #source\xe4\xbf\x9d\xe5\xad\x98\xe7\x9a\x84\xe6\x98\xaf\xe6\xaf\x8f\xe4\xb8\xaa\xe9\xa2\x84\xe6\xb5\x8b\xe5\xb1\x82\xe7\x9a\x84\xe7\xbd\x91\xe7\xbb\x9c\xe8\xbe\x93\xe5\x87\xba,\xe5\x8d\xb3feature maps\n        #loc \xe9\x80\x9a\xe8\xbf\x87\xe4\xbd\xbf\xe7\x94\xa8feature map\xe5\x8e\xbb\xe9\xa2\x84\xe6\xb5\x8b\xe5\x9b\x9e\xe5\xbd\x92\n        #conf\xe9\x80\x9a\xe8\xbf\x87\xe4\xbd\xbf\xe7\x94\xa8feature map\xe5\x8e\xbb\xe9\xa2\x84\xe6\xb5\x8b\xe5\x88\x86\xe7\xb1\xbb\n        for (x, l, c) in zip(sources_final, self.loc, self.conf):\n            loc.append(l(x).permute(0, 2, 3, 1).contiguous())\n            conf.append(c(x).permute(0, 2, 3, 1).contiguous())\n        # \xe5\x9c\xa8\xe7\xbb\x99\xe5\xae\x9a\xe7\xbb\xb4\xe5\xba\xa6\xe4\xb8\x8a\xe5\xaf\xb9\xe8\xbe\x93\xe5\x85\xa5\xe7\x9a\x84\xe5\xbc\xa0\xe9\x87\x8f\xe5\xba\x8f\xe5\x88\x97seq \xe8\xbf\x9b\xe8\xa1\x8c\xe8\xbf\x9e\xe6\x8e\xa5\xe6\x93\x8d\xe4\xbd\x9c    dimension=1\xe8\xa1\xa8\xe7\xa4\xba\xe5\x9c\xa8\xe5\x88\x97\xe4\xb8\x8a\xe8\xbf\x9e\xe6\x8e\xa5\n        loc = torch.cat([o.view(o.size(0), -1) for o in loc], 1)\n        conf = torch.cat([o.view(o.size(0), -1) for o in conf], 1)\n        # \xe6\xb5\x8b\xe8\xaf\x95\xe9\x9b\x86\xe4\xb8\x8a\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\n        if self.phase == ""test"":\n            output = self.detect(\n                loc.view(loc.size(0), -1, 4),                   # loc preds  \xe5\xae\x9a\xe4\xbd\x8d\xe7\x9a\x84\xe9\xa2\x84\xe6\xb5\x8b\n                self.softmax(conf.view(conf.size(0), -1,\n                             self.num_classes)),                # conf preds  \xe5\x88\x86\xe7\xb1\xbb\xe7\x9a\x84\xe9\xa2\x84\xe6\xb5\x8b\n                self.priors.type(type(x.data))                  # default boxes  \xe9\xa2\x84\xe6\xb5\x8b\xe6\xa1\x86\n            )\n        else:\n            # \xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\xe4\xb8\x8a\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\n            output = (\n                loc.view(loc.size(0), -1, 4),    # loc preds [32,8732,4] \xe9\x80\x9a\xe8\xbf\x87\xe7\xbd\x91\xe7\xbb\x9c\xe8\xbe\x93\xe5\x87\xba\xe7\x9a\x84\xe5\xae\x9a\xe4\xbd\x8d\xe7\x9a\x84\xe9\xa2\x84\xe6\xb5\x8b\n                conf.view(conf.size(0), -1, self.num_classes),  #conf preds [32,8732,21]  \xe9\x80\x9a\xe8\xbf\x87\xe7\xbd\x91\xe7\xbb\x9c\xe8\xbe\x93\xe5\x87\xba\xe7\x9a\x84\xe5\x88\x86\xe7\xb1\xbb\xe7\x9a\x84\xe9\xa2\x84\xe6\xb5\x8b\n                self.priors   # \xe4\xb8\x8d\xe5\x90\x8cfeature map\xe6\xa0\xb9\xe6\x8d\xae\xe5\x85\xac\xe5\xbc\x8f\xe7\x94\x9f\xe6\x88\x90\xe7\x9a\x84\xe9\x94\x9a\xe7\xbb\x93\xe6\x9e\x9c [8732,4]   \xe5\x86\x85\xe5\xae\xb9\xe4\xb8\xba \xe4\xb8\xad\xe5\xbf\x83\xe7\x82\xb9\xe5\x9d\x90\xe6\xa0\x87\xe5\x92\x8c\xe5\xae\xbd\xe9\xab\x98\n            )\n        return output\n\n\n    def saveSSD(self, name=None):\n        \'\'\'\n        \xe4\xbf\x9d\xe5\xad\x98\xe6\xa8\xa1\xe5\x9e\x8b\n        \'\'\'\n        # \xe4\xbf\x9d\xe5\xad\x98\xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x80\xe6\xac\xa1\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\n        if name is None:\n            prefix = opt.checkpoint_root + \'last_time_SSD\'\n            name=prefix+\'.pth\'\n        # \xe6\xaf\x8f\xe9\x9a\x94\xe4\xb8\x80\xe6\xae\xb5\xe6\x97\xb6\xe9\x97\xb4\xe4\xbf\x9d\xe5\xad\x98\xe4\xb8\x80\xe6\xac\xa1\xe6\xa8\xa1\xe5\x9e\x8b\n        else:\n            prefix =opt.checkpoint_root +\'SSD_iter\'+name\n            name = prefix + \'.pth\'\n        torch.save(self.state_dict(), name)\n        return name\n\n\n\n# This function is derived from torchvision VGG make_layers()\n# \xe6\xad\xa4\xe6\x96\xb9\xe6\xb3\x95\xe6\xba\x90\xe8\x87\xaatorchvision VGG make_layers\xef\xbc\x88\xef\xbc\x89\n# https://github.com/pytorch/vision/blob/master/torchvision/models/vgg.py\ndef vgg(cfg, i, batch_norm=False):\n    \'\'\'\n    vgg\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x84\n    cfg:  vgg\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x84\n     \'300\': [64, 64, \'M\', 128, 128, \'M\', 256, 256, 256, \'C\', 512, 512, 512, \'M\',\n            512, 512, 512],\n    i: 3   \xe8\xbe\x93\xe5\x85\xa5\xe5\x9b\xbe\xe5\x83\x8f\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\n    batch_norm    \xe4\xb8\xbaFalse\xe3\x80\x82\xe8\x8b\xa5\xe4\xb8\xbaTrue\xef\xbc\x8c\xe5\x88\x99\xe7\xbd\x91\xe7\xbb\x9c\xe4\xb8\xad\xe5\x8a\xa0\xe5\x85\xa5batch_norm\n\n    \xe8\xbf\x94\xe5\x9b\x9e\xe6\xb2\xa1\xe6\x9c\x89\xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe5\xb1\x82\xe7\x9a\x84vgg\xe7\xbd\x91\xe7\xbb\x9c\n    \'\'\'\n    #\xe4\xbf\x9d\xe5\xad\x98vgg\xe6\x89\x80\xe6\x9c\x89\xe5\xb1\x82\n    layers = []\n    #\xe8\xbe\x93\xe5\x85\xa5\xe5\x9b\xbe\xe5\x83\x8f\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\n    in_channels = i\n    for v in cfg:   #M\xe4\xb8\x8eC\xe4\xbc\x9a\xe5\xaf\xbc\xe8\x87\xb4\xe7\x94\x9f\xe6\x88\x90\xe7\x9a\x84feature map\xe5\xa4\xa7\xe5\xb0\x8f\xe5\x87\xba\xe7\x8e\xb0\xe5\x8f\x98\xe5\x8c\x96\n        if v == \'M\':  #\xe6\x9c\x80\xe5\xa4\xa7\xe6\xb1\xa0\xe5\x8c\x96\xe5\xb1\x82   \xe9\xbb\x98\xe8\xae\xa4floor\xe6\xa8\xa1\xe5\xbc\x8f\n            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n        elif v == \'C\':  #\xe6\x9c\x80\xe5\xa4\xa7\xe6\xb1\xa0\xe5\x8c\x96\xe5\xb1\x82   ceil\xe6\xa8\xa1\xe5\xbc\x8f   \xe4\xb8\xa4\xe7\xa7\x8d\xe4\xb8\x8d\xe5\x90\x8c\xe7\x9a\x84maxpool\xe6\x96\xb9\xe5\xbc\x8f    \xe5\x8f\x82\xe8\x80\x83https://blog.csdn.net/GZHermit/article/details/79351803\n            layers += [nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)]\n        else:\n            # \xe5\x8d\xb7\xe7\xa7\xaf\n            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n            if batch_norm:\n                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n            else:\n                layers += [conv2d, nn.ReLU(inplace=True)]\n            in_channels = v\n    # \xe8\xae\xba\xe6\x96\x87\xe5\xb0\x86 Pool5 layer \xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0\xef\xbc\x8c\xe4\xbb\x8e \xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb82\xc3\x972\xe6\xad\xa5\xe9\x95\xbf\xe4\xb8\xba2  \xe8\xbd\xac\xe5\x8f\x98\xe6\x88\x90 \xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb83\xc3\x973 \xe6\xad\xa5\xe9\x95\xbf\xe4\xb8\xba1 \xe5\xa4\x96\xe5\x8a\xa0\xe4\xb8\x80\xe4\xb8\xaa pad\n    pool5 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n    # \xe8\xae\xba\xe6\x96\x87\xe4\xb8\xad\xe5\xb0\x86VGG\xe7\x9a\x84FC6 layer\xe3\x80\x81FC7 layer \xe8\xbd\xac\xe6\x88\x90\xe4\xb8\xba \xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82conv6,conv7 \xe5\xb9\xb6\xe4\xbb\x8e\xe6\xa8\xa1\xe5\x9e\x8b\xe7\x9a\x84FC6\xe3\x80\x81FC7 \xe4\xb8\x8a\xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0\xef\xbc\x8c\xe8\xbf\x9b\xe8\xa1\x8c\xe9\x87\x87\xe6\xa0\xb7\xe5\xbe\x97\xe5\x88\xb0\xe8\xbf\x99\xe4\xb8\xa4\xe4\xb8\xaa\xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82\xe7\x9a\x84 \xe5\x8f\x82\xe6\x95\xb0\n    #\xe8\xbe\x93\xe5\x85\xa5\xe9\x80\x9a\xe9\x81\x93512  \xe8\xbe\x93\xe5\x87\xba\xe9\x80\x9a\xe9\x81\x93\xe4\xb8\xba1024  \xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8\xe4\xb8\xba3  padding\xe4\xb8\xba6    dilation\xe4\xb8\xba\xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8\xe4\xb8\xad\xe5\x85\x83\xe7\xb4\xa0\xe4\xb9\x8b\xe9\x97\xb4\xe7\x9a\x84\xe7\xa9\xba\xe6\xb4\x9e\xe5\xa4\xa7\xe5\xb0\x8f\n    # \xe4\xbf\xae\xe6\x94\xb9Pool5 layer\xe5\x8f\x82\xe6\x95\xb0\xef\xbc\x8c\xe5\xaf\xbc\xe8\x87\xb4\xe6\x84\x9f\xe5\x8f\x97\xe9\x87\x8e\xe5\xa4\xa7\xe5\xb0\x8f\xe6\x94\xb9\xe5\x8f\x98\xe3\x80\x82\xe6\x89\x80\xe4\xbb\xa5conv6\xe9\x87\x87\xe7\x94\xa8 atrous \xe7\xae\x97\xe6\xb3\x95\xef\xbc\x8c\xe5\x8d\xb3\xe5\xad\x94\xe5\xa1\xab\xe5\x85\x85\xe7\xae\x97\xe6\xb3\x95\xe3\x80\x82\n    # \xe5\xad\x94\xe5\xa1\xab\xe5\x85\x85\xe7\xae\x97\xe6\xb3\x95\xe5\xb0\x86\xe5\x8d\xb7\xe7\xa7\xaf weights \xe8\x86\xa8\xe8\x83\x80\xe6\x89\xa9\xe5\xa4\xa7\xef\xbc\x8c\xe5\x8d\xb3\xe5\x8e\x9f\xe6\x9d\xa5\xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8\xe6\x98\xaf 3x3\xef\xbc\x8c\xe8\x86\xa8\xe8\x83\x80\xe5\x90\x8e\xef\xbc\x8c\xe5\x8f\xaf\xe8\x83\xbd\xe5\x8f\x98\xe6\x88\x90 7x7 \xe4\xba\x86\xef\xbc\x8c\xe8\xbf\x99\xe6\xa0\xb7 receptive field \xe5\x8f\x98\xe5\xa4\xa7\xe4\xba\x86\xef\xbc\x8c\xe8\x80\x8c score map \xe4\xb9\x9f\xe5\xbe\x88\xe5\xa4\xa7\xef\xbc\x8c\xe5\x8d\xb3\xe8\xbe\x93\xe5\x87\xba\xe5\x8f\x98\xe6\x88\x90 dense\n    #\xe8\xbf\x99\xe4\xb9\x88\xe5\x81\x9a\xe7\x9a\x84\xe5\xa5\xbd\xe5\xa4\x84\xe6\x98\xaf\xef\xbc\x8c\xe8\xbe\x93\xe5\x87\xba\xe7\x9a\x84 score map \xe5\x8f\x98\xe5\xa4\xa7\xe4\xba\x86\xef\xbc\x8c\xe5\x8d\xb3\xe6\x98\xaf dense \xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe4\xba\x86\xef\xbc\x8c\xe8\x80\x8c\xe4\xb8\x94 receptive field \xe4\xb8\x8d\xe4\xbc\x9a\xe5\x8f\x98\xe5\xb0\x8f\xef\xbc\x8c\xe8\x80\x8c\xe4\xb8\x94\xe5\x8f\xaf\xe4\xbb\xa5\xe5\x8f\x98\xe5\xa4\xa7\xe3\x80\x82\xe8\xbf\x99\xe5\xaf\xb9\xe5\x81\x9a\xe5\x88\x86\xe5\x89\xb2\xe3\x80\x81\xe6\xa3\x80\xe6\xb5\x8b\xe7\xad\x89\xe5\xb7\xa5\xe4\xbd\x9c\xe9\x9d\x9e\xe5\xb8\xb8\xe9\x87\x8d\xe8\xa6\x81\xe3\x80\x82\n    conv6 = nn.Conv2d(512, 1024, kernel_size=3, padding=6, dilation=6)\n    #\xe8\xbe\x93\xe5\x85\xa5\xe9\x80\x9a\xe9\x81\x93512  \xe8\xbe\x93\xe5\x87\xba\xe9\x80\x9a\xe9\x81\x93\xe4\xb8\xba1024  \xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8\xe4\xb8\xba3\n    conv7 = nn.Conv2d(1024, 1024, kernel_size=1)\n    #\xe5\xb0\x86 \xe4\xbf\xae\xe6\x94\xb9\xe7\x9a\x84\xe5\xb1\x82\xe4\xb9\x9f\xe5\x8a\xa0\xe5\x85\xa5\xe5\x88\xb0vgg\xe7\xbd\x91\xe7\xbb\x9c\xe4\xb8\xad\n    layers += [pool5, conv6,\n               nn.ReLU(inplace=True), conv7, nn.ReLU(inplace=True)]\n    return layers\n\n\ndef add_extras(cfg, i, batch_norm=False):\n    \'\'\'\n    vgg\xe7\xbd\x91\xe7\xbb\x9c\xe5\x90\x8e\xe9\x9d\xa2\xe6\x96\xb0\xe5\xa2\x9e\xe7\x9a\x84\xe9\xa2\x9d\xe5\xa4\x96\xe5\xb1\x82\n    :param cfg:  \'300\': [256, \'S\', 512, 128, \'S\', 256, 128, 256, 128, 256],\n    :param i:    1024  \xe8\xbe\x93\xe5\x85\xa5\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\n    :param batch_norm:  flase\n    :return:\n    \'\'\'\n    # \xe6\xb7\xbb\xe5\x8a\xa0\xe5\x88\xb0VGG\xe7\x9a\x84\xe9\xa2\x9d\xe5\xa4\x96\xe5\x9b\xbe\xe5\xb1\x82\xe7\x94\xa8\xe4\xba\x8e\xe7\x89\xb9\xe5\xbe\x81\xe7\xbc\xa9\xe6\x94\xbe\n    layers = []\n    #1024  \xe8\xbe\x93\xe5\x85\xa5\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\n    in_channels = i\n    # \xe6\x8e\xa7\xe5\x88\xb6\xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8\xe5\xb0\xba\xe5\xaf\xb8\xef\xbc\x8c\xe4\xb8\x80\xe7\xbb\xb4\xe6\x95\xb0\xe7\xbb\x84\xe9\x80\x89\xe5\x89\x8d\xe4\xb8\x80\xe4\xb8\xaa\xe6\x95\xb0\xe8\xbf\x98\xe6\x98\xaf\xe5\x90\x8e\xe4\xb8\x80\xe4\xb8\xaa\xe6\x95\xb0\xe3\x80\x82\xe5\x9c\xa8\xe6\xaf\x8f\xe6\xac\xa1\xe5\xbe\xaa\xe7\x8e\xaf\xe6\x97\xb6flag\xe9\x83\xbd\xe6\x94\xb9\xe5\x8f\x98\xef\xbc\x8c\xe5\xaf\xbc\xe8\x87\xb4\xe7\xbd\x91\xe7\xbb\x9c\xe7\x9a\x84\xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8\xe5\xb0\xba\xe5\xaf\xb8\xe4\xb8\xba1,3,1,3\xe4\xba\xa4\xe6\x9b\xbf\n    # False \xe4\xb8\xba1\xef\xbc\x8cTrue\xe4\xb8\xba3\n    # SSD\xe7\xbd\x91\xe7\xbb\x9c\xe5\x9b\xbe\xe4\xb8\xads1\xe6\x8c\x87\xe6\xad\xa5\xe9\x95\xbf\xe4\xb8\xba1\xef\xbc\x8cs2\xe6\x8c\x87\xe6\xad\xa5\xe9\x95\xbf\xe4\xb8\xba2\n    # \xe5\x9c\xa8\xe8\xaf\xa5\xe4\xbb\xa3\xe7\xa0\x81\xe4\xb8\xad\xef\xbc\x8cS\xe4\xbb\xa3\xe8\xa1\xa8\xe6\xad\xa5\xe9\x95\xbf\xe4\xb8\xba2\xef\xbc\x8c\xe6\x97\xa0S\xe4\xbb\xa3\xe8\xa1\xa8\xe9\xbb\x98\xe8\xae\xa4\xef\xbc\x8c\xe5\x8d\xb3\xe6\xad\xa5\xe9\x95\xbf\xe4\xb8\xba1\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5cfg\xe4\xb8\x8e\xe8\xae\xba\xe6\x96\x87\xe7\xbd\x91\xe7\xbb\x9c\xe7\xbb\x93\xe6\x9e\x84\xe5\xae\x8c\xe5\x85\xa8\xe5\x8c\xb9\xe9\x85\x8d\n    flag = False\n    # enumerate\xe6\x9e\x9a\xe4\xb8\xbe   k\xe4\xb8\xba\xe4\xb8\x8b\xe6\xa0\x87   v\xe4\xb8\xba\xe5\x80\xbc\n    for k, v in enumerate(cfg):\n        if in_channels != \'S\':\n            if v == \'S\':\n                layers += [nn.Conv2d(in_channels, cfg[k + 1],\n                           kernel_size=(1, 3)[flag], stride=2, padding=1)]\n            else:\n                layers += [nn.Conv2d(in_channels, v, kernel_size=(1, 3)[flag])]\n            flag = not flag\n        in_channels = v\n    return layers\n\n\ndef multibox(vgg, extra_layers, cfg, num_classes):\n    \'\'\'\n\n\n    :param vgg: \xe7\xbb\x8f\xe8\xbf\x87\xe4\xbf\xae\xe6\x94\xb9\xe5\x90\x8e\xe7\x9a\x84vgg\xe7\xbd\x91\xe7\xbb\x9c\xef\xbc\x88\xe5\x8e\xbb\xe6\x8e\x89\xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe5\xb1\x82\xef\xbc\x8c\xe4\xbf\xae\xe6\x94\xb9pool5\xe5\x8f\x82\xe6\x95\xb0\xe5\xb9\xb6\xe6\xb7\xbb\xe5\x8a\xa0\xe6\x96\xb0\xe5\xb1\x82\xef\xbc\x89\n    :param extra_layers: vgg\xe7\xbd\x91\xe7\xbb\x9c\xe5\x90\x8e\xe9\x9d\xa2\xe6\x96\xb0\xe5\xa2\x9e\xe7\x9a\x84\xe9\xa2\x9d\xe5\xa4\x96\xe5\xb1\x82\n    :param cfg: \'300\': [4, 6, 6, 6, 4, 4],  \xe4\xb8\x8d\xe5\x90\x8c\xe9\x83\xa8\xe5\x88\x86\xe7\x9a\x84feature map\xe4\xb8\x8a\xe4\xb8\x80\xe4\xb8\xaa\xe7\xbd\x91\xe6\xa0\xbc\xe9\xa2\x84\xe6\xb5\x8b\xe5\xa4\x9a\xe5\xb0\x91\xe6\xa1\x86\n    :param num_classes: 20\xe5\x88\x86\xe7\xb1\xbb+1\xe8\x83\x8c\xe6\x99\xaf\xef\xbc\x8c\xe5\x85\xb121\xe7\xb1\xbb\n    :return:\n    \'\'\'\n    # \xe4\xbf\x9d\xe5\xad\x98\xe6\x89\x80\xe6\x9c\x89\xe5\x8f\x82\xe4\xb8\x8e\xe9\xa2\x84\xe6\xb5\x8b\xe7\x9a\x84\xe7\xbd\x91\xe7\xbb\x9c\xe5\xb1\x82\n    loc_layers = []\n    conf_layers = []\n    # \xe4\xbc\xa0\xe5\x85\xa5\xe7\x9a\x84\xe4\xbf\xae\xe6\x94\xb9\xe8\xbf\x87\xe7\x9a\x84vgg\xe7\xbd\x91\xe7\xbb\x9c\xe7\x94\xa8\xe4\xba\x8e\xe9\xa2\x84\xe6\xb5\x8b\xe7\x9a\x84\xe7\xbd\x91\xe7\xbb\x9c\xe6\x98\xaf21\xe5\xb1\x82\xe4\xbb\xa5\xe5\x8f\x8a \xe5\x80\x92\xe6\x95\xb0\xe7\xac\xac\xe4\xba\x8c\xe5\xb1\x82\n    vgg_source = [21, -2]\n    for k, v in enumerate(vgg_source):\n        # \xe6\x8c\x89\xe7\x85\xa7fp-ssd\xe8\xae\xba\xe6\x96\x87\xef\xbc\x8c\xe5\xb0\x861024\xe6\x94\xb9\xe4\xb8\xba512\xe9\x80\x9a\xe9\x81\x93\n        if k==1:\n            in_channels=512\n        else:\n            in_channels=vgg[v].out_channels\n        #4\xe6\x98\xaf\xe5\x9b\x9e\xe5\xbd\x92\xe7\x9a\x84\xe5\x9d\x90\xe6\xa0\x87\xe5\x8f\x82\xe6\x95\xb0  cfg\xe4\xbb\xa3\xe8\xa1\xa8\xe8\xaf\xa5\xe5\xb1\x82feature map\xe4\xb8\x8a\xe4\xb8\x80\xe4\xb8\xaa\xe7\xbd\x91\xe6\xa0\xbc\xe9\xa2\x84\xe6\xb5\x8b\xe5\xa4\x9a\xe5\xb0\x91\xe6\xa1\x86\n        loc_layers += [nn.Conv2d(in_channels,\n                                 cfg[k] * 4, kernel_size=3, padding=1)]\n        #num_classes\xe6\x98\xaf\xe7\xb1\xbb\xe5\x88\xab\xe6\x95\xb0 cfg\xe4\xbb\xa3\xe8\xa1\xa8\xe8\xaf\xa5\xe5\xb1\x82feature map\xe4\xb8\x8a\xe4\xb8\x80\xe4\xb8\xaa\xe7\xbd\x91\xe6\xa0\xbc\xe9\xa2\x84\xe6\xb5\x8b\xe5\xa4\x9a\xe5\xb0\x91\xe6\xa1\x86\n        conf_layers += [nn.Conv2d(in_channels,\n                        cfg[k] * num_classes, kernel_size=3, padding=1)]\n    # [x::y] \xe4\xbb\x8e\xe4\xb8\x8b\xe6\xa0\x87x\xe5\xbc\x80\xe5\xa7\x8b\xef\xbc\x8c\xe6\xaf\x8f\xe9\x9a\x94y\xe5\x8f\x96\xe5\x80\xbc\n    #\xe8\xae\xba\xe6\x96\x87\xe4\xb8\xad\xe6\x96\xb0\xe5\xa2\x9e\xe5\xb1\x82\xe4\xb9\x9f\xe6\x98\xaf\xe6\xaf\x8f\xe9\x9a\x94\xe4\xb8\x80\xe4\xb8\xaa\xe5\xb1\x82\xe6\xb7\xbb\xe5\x8a\xa0\xe4\xb8\x80\xe4\xb8\xaa\xe9\xa2\x84\xe6\xb5\x8b\xe5\xb1\x82\n    # \xe5\xb0\x86\xe6\x96\xb0\xe5\xa2\x9e\xe7\x9a\x84\xe9\xa2\x9d\xe5\xa4\x96\xe5\xb1\x82\xe4\xb8\xad\xe7\x9a\x84\xe9\xa2\x84\xe6\xb5\x8b\xe5\xb1\x82\xe4\xb9\x9f\xe6\xb7\xbb\xe5\x8a\xa0\xe4\xb8\x8a   start=2\xef\xbc\x9a\xe4\xb8\x8b\xe6\xa0\x87\xe8\xb5\xb7\xe5\xa7\x8b\xe4\xbd\x8d\xe7\xbd\xae\n    for k, v in enumerate(extra_layers[1::2], 2):\n        loc_layers += [nn.Conv2d(v.out_channels, cfg[k]\n                                 * 4, kernel_size=3, padding=1)]\n        conf_layers += [nn.Conv2d(v.out_channels, cfg[k]\n                                  * num_classes, kernel_size=3, padding=1)]\n    return vgg, extra_layers, (loc_layers, conf_layers)\n\n\nbase = {\n    # \xe6\x95\xb0\xe5\xad\x97\xe4\xb8\xba\xe6\xaf\x8f\xe5\xb1\x82feature map\xe7\x9a\x84\xe5\xb1\x82\xe6\x95\xb0  M\xe4\xbb\xa3\xe8\xa1\xa8\xe6\x9c\x80\xe5\xa4\xa7\xe6\xb1\xa0\xe5\x8c\x96\xe5\xb1\x82\xef\xbc\x88\xe9\xbb\x98\xe8\xae\xa4floor\xe6\xa8\xa1\xe5\xbc\x8f\xef\xbc\x89    C\xe4\xbb\xa3\xe8\xa1\xa8\xe6\x9c\x80\xe5\xa4\xa7\xe6\xb1\xa0\xe5\x8c\x96\xe5\xb1\x82\xef\xbc\x88ceil\xe6\xa8\xa1\xe5\xbc\x8f\xef\xbc\x89  (\xe5\x8e\xbb\xe6\x8e\x89vgg16\xe7\x9a\x84\xe6\x9c\x80\xe5\x90\x8e\xe7\x9a\x84 maxpool\xe3\x80\x81fc\xe3\x80\x81fc\xe3\x80\x81fc\xe3\x80\x81softmax)\n    \'300\': [64, 64, \'M\', 128, 128, \'M\', 256, 256, 256, \'C\', 512, 512, 512, \'M\',\n            512, 512, 512],\n    \'512\': [],\n}\nextras = {\n    # \xe6\xaf\x8f\xe4\xb8\xaa\xe7\x89\xb9\xe5\xbe\x81\xe5\x9b\xbe\xe9\x83\xbd\xe6\x98\xaf\xe7\x94\xb1 \xe4\xb8\xa4\xe4\xb8\xaaconv \xe7\xbb\x84\xe6\x88\x90\xef\xbc\x8c conv1x1 \xe5\x92\x8cconv3x3\n    \'300\': [256, \'S\', 512, 128, \'S\', 256, 128, 256, 128, 256],\n    \'512\': [],\n}\nmbox = {\n    \'300\': [4, 6, 6, 6, 4, 4],  # \xe4\xb8\x8d\xe5\x90\x8c\xe9\x83\xa8\xe5\x88\x86\xe7\x9a\x84feature map\xe4\xb8\x8a\xe4\xb8\x80\xe4\xb8\xaa\xe7\xbd\x91\xe6\xa0\xbc\xe9\xa2\x84\xe6\xb5\x8b\xe5\xa4\x9a\xe5\xb0\x91\xe6\xa1\x86\n    \'512\': [],\n}\n\n\ndef build_ssd(phase, size=300, num_classes=21):\n    \'\'\'\n    \xe6\x96\xb0\xe5\xbb\xbaSSD\xe6\xa8\xa1\xe5\x9e\x8b\n    \'\'\'\n    # \xe8\xae\xad\xe7\xbb\x83\xe6\x88\x96\xe6\xb5\x8b\xe8\xaf\x95\n    if phase != ""test"" and phase != ""train"":\n        print(""ERROR: Phase: "" + phase + "" not recognized"")\n        return\n    #\xe5\xbd\x93\xe5\x89\x8dSSD300\xe5\x8f\xaa\xe6\x94\xaf\xe6\x8c\x81\xe5\xa4\xa7\xe5\xb0\x8f300\xc3\x97300\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe8\xae\xad\xe7\xbb\x83\n    if size != 300:\n        print(""ERROR: You specified size "" + repr(size) + "". However, "" +\n              ""currently only SSD300 (size=300) is supported!"")\n        return\n\n    #base_\xef\xbc\x9a \xe7\xbb\x8f\xe8\xbf\x87\xe4\xbf\xae\xe6\x94\xb9\xe5\x90\x8e\xe7\x9a\x84vgg\xe7\xbd\x91\xe7\xbb\x9c\xef\xbc\x88\xe5\x8e\xbb\xe6\x8e\x89\xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe5\xb1\x82\xef\xbc\x8c\xe4\xbf\xae\xe6\x94\xb9pool5\xe5\x8f\x82\xe6\x95\xb0\xe5\xb9\xb6\xe6\xb7\xbb\xe5\x8a\xa0\xe6\x96\xb0\xe5\xb1\x82\xef\xbc\x89\n    #extras_\xef\xbc\x9a  vgg\xe7\xbd\x91\xe7\xbb\x9c\xe5\x90\x8e\xe9\x9d\xa2\xe6\x96\xb0\xe5\xa2\x9e\xe7\x9a\x84\xe9\xa2\x9d\xe5\xa4\x96\xe5\xb1\x82\n    # head_ :    (loc_layers, conf_layers)   vgg\xe4\xb8\x8eextras\xe4\xb8\xad\xe8\xbf\x9b\xe8\xa1\x8c\xe5\x88\x86\xe7\xb1\xbb\xe5\x92\x8c\xe5\x9b\x9e\xe5\xbd\x92\xe7\x9a\x84\xe5\xb1\x82\n    base_, extras_, head_ = multibox(vgg(base[str(size)], 3),  #vgg\xe6\x96\xb9\xe6\xb3\x95\xe8\xbf\x94\xe5\x9b\x9e \xe7\xbb\x8f\xe8\xbf\x87\xe4\xbf\xae\xe6\x94\xb9\xe5\x90\x8e\xe7\x9a\x84vgg\xe7\xbd\x91\xe7\xbb\x9c\xef\xbc\x88\xe5\x8e\xbb\xe6\x8e\x89\xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe5\xb1\x82\xef\xbc\x8c\xe4\xbf\xae\xe6\x94\xb9pool5\xe5\x8f\x82\xe6\x95\xb0\xe5\xb9\xb6\xe6\xb7\xbb\xe5\x8a\xa0\xe6\x96\xb0\xe5\xb1\x82\xef\xbc\x89\n                                     add_extras(extras[str(size)], 1024), #vgg\xe7\xbd\x91\xe7\xbb\x9c\xe5\x90\x8e\xe9\x9d\xa2\xe6\x96\xb0\xe5\xa2\x9e\xe7\x9a\x84\xe9\xa2\x9d\xe5\xa4\x96\xe5\xb1\x82\n                                     mbox[str(size)],  #mbox\xe6\x8c\x87\xe4\xb8\x8d\xe5\x90\x8c\xe9\x83\xa8\xe5\x88\x86\xe7\x9a\x84feature map\xe4\xb8\x8a\xe4\xb8\x80\xe4\xb8\xaa\xe7\xbd\x91\xe6\xa0\xbc\xe9\xa2\x84\xe6\xb5\x8b\xe5\xa4\x9a\xe5\xb0\x91\xe6\xa1\x86\n                                     num_classes)\n    # phase\xef\xbc\x9a\'train\'    size\xef\xbc\x9a300    num_classes\xef\xbc\x9a 21 \xe7\xb1\xbb\xe5\x88\xab\xe6\x95\xb0\xef\xbc\x8820\xe7\xb1\xbb+1\xe8\x83\x8c\xe6\x99\xaf\xef\xbc\x89\n    return SSD(phase, size, base_, extras_, head_, num_classes)\n'"
FasterRcnn_pytorch/train.py,2,"b'import os\n\nimport ipdb\nimport matplotlib\nfrom tqdm import tqdm\n\nfrom utils.config import opt\nfrom data.dataset import Dataset, TestDataset, inverse_normalize\nfrom model import FasterRCNNVGG16\nfrom torch.autograd import Variable\nfrom torch.utils import data as data_\nfrom trainer import FasterRCNNTrainer\nfrom utils import array_tool as at\nfrom utils.vis_tool import visdom_bbox\nfrom utils.eval_tool import eval_detection_voc\n\n# fix for ulimit\n# https://github.com/pytorch/pytorch/issues/973#issuecomment-346405667\nimport resource\n\n#RLIMIT_NOFILE\xe4\xb8\x80\xe4\xb8\xaa\xe8\xbf\x9b\xe7\xa8\x8b\xe8\x83\xbd\xe6\x89\x93\xe5\xbc\x80\xe7\x9a\x84\xe6\x9c\x80\xe5\xa4\xa7\xe6\x96\x87\xe4\xbb\xb6\xe6\x95\xb0\xef\xbc\x8c\xe5\x86\x85\xe6\xa0\xb8\xe9\xbb\x98\xe8\xae\xa4\xe6\x98\xaf1024\n#rlimit\xe7\x9a\x84\xe4\xb8\xa4\xe4\xb8\xaa\xe5\x8f\x82\xe6\x95\xb0\n#soft limit  \xe6\x8c\x87\xe5\x86\x85\xe6\xa0\xb8\xe6\x89\x80\xe8\x83\xbd\xe6\x94\xaf\xe6\x8c\x81\xe7\x9a\x84\xe8\xb5\x84\xe6\xba\x90\xe4\xb8\x8a\xe9\x99\x90 \xe6\x9c\x80\xe5\xa4\xa7\xe4\xb9\x9f\xe5\x8f\xaa\xe8\x83\xbd\xe8\xbe\xbe\xe5\x88\xb01024\n#hard limit \xe5\xad\xa6\xe6\xa0\xa1\xe6\x9c\xba\xe6\x88\xbf\xe5\x80\xbc\xe4\xb8\xba4096  \xe5\x9c\xa8\xe8\xb5\x84\xe6\xba\x90\xe4\xb8\xad\xe5\x8f\xaa\xe6\x98\xaf\xe4\xbd\x9c\xe4\xb8\xbasoft limit\xe7\x9a\x84\xe4\xb8\x8a\xe9\x99\x90\xe3\x80\x82\xe5\xbd\x93\xe4\xbd\xa0\xe8\xae\xbe\xe7\xbd\xaehard limit\xe5\x90\x8e\xef\xbc\x8c\xe4\xbd\xa0\xe4\xbb\xa5\xe5\x90\x8e\xe8\xae\xbe\xe7\xbd\xae\xe7\x9a\x84soft limit\xe5\x8f\xaa\xe8\x83\xbd\xe5\xb0\x8f\xe4\xba\x8ehard limit\xe3\x80\x82\n\nrlimit = resource.getrlimit(resource.RLIMIT_NOFILE)\nresource.setrlimit(resource.RLIMIT_NOFILE, (4096, rlimit[1]))\n#\xe5\x9b\xbe\xe5\xbd\xa2\xe5\xb9\xb6\xe6\xb2\xa1\xe6\x9c\x89\xe5\x9c\xa8\xe5\xb1\x8f\xe5\xb9\x95\xe4\xb8\x8a\xe6\x98\xbe\xe7\xa4\xba,\xe4\xbd\x86\xe6\x98\xaf\xe5\xb7\xb2\xe4\xbf\x9d\xe5\xad\x98\xe5\x88\xb0\xe6\x96\x87\xe4\xbb\xb6,\xe5\x85\xb3\xe9\x94\xae\xe6\x98\xaf\xe8\xa6\x81\xe8\xae\xbe\xe7\xbd\xae\'Agg\'\xe7\x9a\x84\xe5\xb1\x9e\xe6\x80\xa7\nmatplotlib.use(\'agg\')\n\n\n\ndef eval(dataloader, faster_rcnn, test_num=10000):\n    """"""\n    \xe9\xaa\x8c\xe8\xaf\x81\n    """"""\n    #\xe7\xa9\xba\xe7\x9a\x84list\n    pred_bboxes, pred_labels, pred_scores = list(), list(), list()\n    gt_bboxes, gt_labels, gt_difficults = list(), list(), list()\n    for ii, (imgs, sizes, gt_bboxes_, gt_labels_, gt_difficults_) in tqdm(enumerate(dataloader)):\n        sizes = [sizes[0][0], sizes[1][0]]\n        #\xe9\xa2\x84\xe6\xb5\x8b\xe7\x9b\xae\xe6\xa0\x87\xe6\xa1\x86\xe3\x80\x81\xe7\xb1\xbb\xe5\x88\xab\xe3\x80\x81\xe5\x88\x86\xe6\x95\xb0\xe5\xad\x98\xe5\x85\xa5list\n        pred_bboxes_, pred_labels_, pred_scores_ = faster_rcnn.predict(imgs, [sizes])\n        #\xe5\xb0\x86\xe7\x9c\x9f\xe5\xae\x9e\xe7\x9a\x84\xe7\x9b\xae\xe6\xa0\x87\xe6\xa1\x86\xe3\x80\x81\xe7\xb1\xbb\xe5\x88\xab\xe3\x80\x81difficults\xe5\xad\x98\xe5\x85\xa5list\n        gt_bboxes += list(gt_bboxes_.numpy())\n        gt_labels += list(gt_labels_.numpy())\n        gt_difficults += list(gt_difficults_.numpy())\n        #\xe5\xb0\x86\xe9\xa2\x84\xe6\xb5\x8b\xe7\x9a\x84\xe7\x9b\xae\xe6\xa0\x87\xe6\xa1\x86\xe3\x80\x81\xe7\xb1\xbb\xe5\x88\xab\xe3\x80\x81\xe5\x88\x86\xe6\x95\xb0\xe5\xad\x98\xe5\x85\xa5list\n        pred_bboxes += pred_bboxes_\n        pred_labels += pred_labels_\n        pred_scores += pred_scores_\n        if ii == test_num: break\n    #\xe8\xbf\x94\xe5\x9b\x9edictz\xe5\xad\x97\xe5\x85\xb8\xef\xbc\x8c\xe4\xb8\xa4\xe4\xb8\xaakey\xe5\x80\xbc\xef\xbc\x9aAP\xe3\x80\x81mAP\n    result = eval_detection_voc(\n        pred_bboxes, pred_labels, pred_scores,\n        gt_bboxes, gt_labels, gt_difficults,\n        use_07_metric=True)\n    return result\n\n\ndef train(**kwargs):\n    """"""\n    \xe8\xae\xad\xe7\xbb\x83\n    """"""\n    #\xe8\xa7\xa3\xe6\x9e\x90\xe5\x91\xbd\xe4\xbb\xa4\xe8\xa1\x8c\xe5\x8f\x82\xe6\x95\xb0\xef\xbc\x8c\xe8\xae\xbe\xe7\xbd\xae\xe9\x85\x8d\xe7\xbd\xae\xe6\x96\x87\xe4\xbb\xb6\xe5\x8f\x82\xe6\x95\xb0\n    opt._parse(kwargs)\n    #\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96Dataset\xe5\x8f\x82\xe6\x95\xb0\n    dataset = Dataset(opt)\n    print(\'load data\')\n    #data_ \xe6\x95\xb0\xe6\x8d\xae\xe5\x8a\xa0\xe8\xbd\xbd\xe5\x99\xa8\xef\xbc\x88\xe8\xa2\xab\xe9\x87\x8d\xe5\x91\xbd\xe5\x90\x8d\xef\xbc\x8cpytorch\xe6\x96\xb9\xe6\xb3\x95\xef\xbc\x89\n    dataloader = data_.DataLoader(dataset, \\\n                                  batch_size=1, \\\n                                  shuffle=True, \\\n                                  # pin_memory=True,\n                                  num_workers=opt.num_workers)\n    #\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96TestDataset\xe5\x8f\x82\xe6\x95\xb0\n    testset = TestDataset(opt)\n    test_dataloader = data_.DataLoader(testset,\n                                       batch_size=1,\n                                       num_workers=opt.test_num_workers,\n                                       shuffle=False, \\\n                                       pin_memory=True\n                                       )\n    #\xe6\x96\xb0\xe5\xbb\xba\xe4\xb8\x80\xe4\xb8\xaaFasterRCNNVGG16\n    faster_rcnn = FasterRCNNVGG16()\n    print(\'model construct completed\')\n    #\xe6\x96\xb0\xe5\xbb\xba\xe4\xb8\x80\xe4\xb8\xaatrainer\xef\xbc\x8c\xe5\xb9\xb6\xe5\xb0\x86\xe7\xbd\x91\xe7\xbb\x9c\xe6\xa8\xa1\xe5\x9e\x8b\xe8\xbd\xac\xe7\xa7\xbb\xe5\x88\xb0GPU\xe4\xb8\x8a\n    #\xe5\xb0\x86FasterRCNNVGG16\xe6\xa8\xa1\xe5\x9e\x8b\xe4\xbc\xa0\xe5\x85\xa5\n    trainer = FasterRCNNTrainer(faster_rcnn).cuda()\n    #\xe5\xa6\x82\xe6\x9e\x9c\xe5\xad\x98\xe5\x9c\xa8\xef\xbc\x8c\xe5\x8a\xa0\xe8\xbd\xbd\xe8\xae\xad\xe7\xbb\x83\xe5\xa5\xbd\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\n    if opt.load_path:\n        trainer.load(opt.load_path)\n        print(\'load pretrained model from %s\' % opt.load_path)\n    #\xe5\x8f\xaf\xe8\xa7\x86\xe5\x8c\x96\xe7\xb1\xbb\xe5\x88\xab vis\xe4\xb8\xbavisdom\xe5\x8a\xa0\xe8\xbd\xbd\xe5\x99\xa8\n    trainer.vis.text(dataset.db.label_names, win=\'labels\')\n    #best_map\xe5\xad\x98\xe6\x94\xbe\xe7\x9a\x84\xe6\x98\xaf \xe6\x9c\x80\xe4\xbc\x98\xe7\x9a\x84mAP\xe7\x9a\x84\xe7\xbd\x91\xe7\xbb\x9c\xe5\x8f\x82\xe6\x95\xb0\n    best_map = 0\n    lr_ = opt.lr\n    for epoch in range(opt.epoch):\n        #trainer\xe6\x96\xb9\xe6\xb3\x95 \xe5\xb0\x86\xe5\xb9\xb3\xe5\x9d\x87\xe7\xb2\xbe\xe5\xba\xa6\xe7\x9a\x84\xe5\x85\x83\xe7\xbb\x84 \xe5\x92\x8c \xe6\xb7\xb7\xe6\xb7\x86\xe7\x9f\xa9\xe9\x98\xb5\xe7\x9a\x84\xe5\x80\xbc\xe7\xbd\xae0\n        trainer.reset_meters()\n        for ii, (img, bbox_, label_, scale) in tqdm(enumerate(dataloader)):\n            #\xe8\xb0\x83\xe6\x95\xb4\xe6\x95\xb0\xe6\x8d\xae\xe7\x9a\x84\xe5\xbd\xa2\xe7\x8a\xb6    scale\xef\xbc\x9a\xe7\xbc\xa9\xe6\x94\xbe\xe5\x80\x8d\xe6\x95\xb0\xef\xbc\x88\xe8\xbe\x93\xe5\x85\xa5\xe5\x9b\xbe\xe7\x89\x87\xe5\xb0\xba\xe5\xaf\xb8 \xe6\xaf\x94\xe4\xb8\x8a \xe8\xbe\x93\xe5\x87\xba\xe6\x95\xb0\xe6\x8d\xae\xe7\x9a\x84\xe5\xb0\xba\xe5\xaf\xb8\xef\xbc\x89\n            #1.6\xe5\xb7\xa6\xe5\x8f\xb3 \xe4\xbe\x9b\xe6\xa8\xa1\xe5\x9e\x8b\xe8\xae\xad\xe7\xbb\x83\xe4\xb9\x8b\xe5\x89\x8d\xe5\xb0\x86\xe6\xa8\xa1\xe5\x9e\x8b\xe8\xa7\x84\xe8\x8c\x83\xe5\x8c\x96\n\t\t\tscale = at.scalar(scale)\n            #\xe5\xb0\x86\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe8\xbd\xac\xe5\x85\xa5\xe5\x88\xb0GPU\xe4\xb8\x8a\n\t\t\t#img  1x3x800x600  \xe4\xb8\x80\xe5\xbc\xa0\xe5\x9b\xbe\xe7\x89\x87 \xe4\xb8\x89\xe9\x80\x9a\xe9\x81\x93  \xe5\xa4\xa7\xe5\xb0\x8f800x600\xef\xbc\x88\xe4\xb8\x8d\xe7\xa1\xae\xe5\xae\x9a\xef\xbc\x89\n\t\t\t#bbox 1x1x4\n\t\t\t#label 1x1\n            img, bbox, label = img.cuda().float(), bbox_.cuda(), label_.cuda()\n            #\xe5\xb0\x86\xe6\x95\xb0\xe6\x8d\xae\xe8\xbd\xac\xe4\xb8\xbaV \xe5\x8f\x98\xe9\x87\x8f\xef\xbc\x8c\xe4\xbb\xa5\xe4\xbe\xbf\xe8\xbf\x9b\xe8\xa1\x8c\xe8\x87\xaa\xe5\x8a\xa8\xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\n            img, bbox, label = Variable(img), Variable(bbox), Variable(label)\n            #\xe8\xae\xad\xe7\xbb\x83\xe5\xb9\xb6\xe6\x9b\xb4\xe6\x96\xb0\xe5\x8f\xaf\xe5\xad\xa6\xe4\xb9\xa0\xe5\x8f\x82\xe6\x95\xb0\xef\xbc\x88\xe9\x87\x8d\xe7\x82\xb9*****\xef\xbc\x89  \xe5\x89\x8d\xe5\x90\x91+\xe5\x8f\x8d\xe5\x90\x91\xef\xbc\x8c\xe8\xbf\x94\xe5\x9b\x9elosses\n            trainer.train_step(img, bbox, label, scale)\n            #\xe8\xbf\x9b\xe8\xa1\x8c\xe5\xa4\x9a\xe4\xb8\xaa\xe6\x95\xb0\xe6\x8d\xae\xe7\x9a\x84\xe5\x8f\xaf\xe8\xa7\x86\xe5\x8c\x96\n            if (ii + 1) % opt.plot_every == 0:\n                #\xe8\xbf\x9b\xe5\x85\xa5\xe8\xb0\x83\xe8\xaf\x95\xe6\xa8\xa1\xe5\xbc\x8f\n                if os.path.exists(opt.debug_file):\n                    ipdb.set_trace()\n\n                # plot loss  \xe7\x94\xbb\xe4\xba\x94\xe4\xb8\xaa\xe6\x8d\x9f\xe5\xa4\xb1\n                trainer.vis.plot_many(trainer.get_meter_data())\n\n                # plot groud truth bboxes  img[0]\xef\xbc\x8c\xe6\x98\xaf\xe5\x8e\x8b\xe7\xbc\xa90\xe4\xbd\x8d\xef\xbc\x8c\xe5\xbd\xa2\xe7\x8a\xb6\xe5\x8f\x98\xe4\xb8\xba[3x800x600]\n                #\xe5\x8f\x8d\xe5\x90\x91\xe5\xbd\x92\xe4\xb8\x80\xe5\x8c\x96\xef\xbc\x8c\xe5\xb0\x86img\xe5\x8f\x8d\xe5\x90\x91\xe8\xbf\x98\xe5\x8e\x9f\xe4\xb8\xba\xe5\x8e\x9f\xe5\xa7\x8b\xe5\x9b\xbe\xe5\x83\x8f\xef\xbc\x8c\xe4\xbb\xa5\xe4\xbe\xbf\xe7\x94\xa8\xe4\xba\x8e\xe6\x98\xbe\xe7\xa4\xba\n                ori_img_ = inverse_normalize(at.tonumpy(img[0]))\n                #\xe9\x80\x9a\xe8\xbf\x87\xe5\x8e\x9f\xe5\xa7\x8b\xe5\x9b\xbe\xe5\x83\x8f\xef\xbc\x8c\xe7\x9c\x9f\xe5\xae\x9ebbox\xef\xbc\x8c\xe7\x9c\x9f\xe5\xae\x9e\xe7\xb1\xbb\xe5\x88\xab \xe8\xbf\x9b\xe8\xa1\x8c\xe6\x98\xbe\xe7\xa4\xba\n                gt_img = visdom_bbox(ori_img_,\n                                     at.tonumpy(bbox_[0]),\n                                     at.tonumpy(label_[0]))\n                trainer.vis.img(\'gt_img\', gt_img)\n\n                # plot predicti bboxes\n                #\xe5\xaf\xb9\xe5\x8e\x9f\xe5\x9b\xbe\xe8\xbf\x9b\xe8\xa1\x8c\xe9\xa2\x84\xe6\xb5\x8b\xef\xbc\x8c\xe5\xbe\x97\xe5\x88\xb0\xe9\xa2\x84\xe6\xb5\x8b\xe7\x9a\x84bbox  label  scores\n                _bboxes, _labels, _scores = trainer.faster_rcnn.predict([ori_img_], visualize=True)\n                #\xe9\x80\x9a\xe8\xbf\x87\xe5\x8e\x9f\xe5\xa7\x8b\xe5\x9b\xbe\xe5\x83\x8f\xe3\x80\x81\xe9\xa2\x84\xe6\xb5\x8b\xe7\x9a\x84bbox\xef\xbc\x8c\xe9\xa2\x84\xe6\xb5\x8b\xe7\x9a\x84\xe7\xb1\xbb\xe5\x88\xab   \xe4\xbb\xa5\xe5\x8f\x8a\xe6\xa6\x82\xe7\x8e\x87  \xe8\xbf\x9b\xe8\xa1\x8c\xe6\x98\xbe\xe7\xa4\xba\n                pred_img = visdom_bbox(ori_img_,\n                                       at.tonumpy(_bboxes[0]),\n                                       at.tonumpy(_labels[0]).reshape(-1),\n                                       at.tonumpy(_scores[0]))\n                trainer.vis.img(\'pred_img\', pred_img)\n\n                # rpn confusion matrix(meter)\n                #rpn\xe6\xb7\xb7\xe6\xb7\x86\xe7\x9f\xa9\xe9\x98\xb5\n                trainer.vis.text(str(trainer.rpn_cm.value().tolist()), win=\'rpn_cm\')\n                # roi confusion matrix\n                #roi\xe6\xb7\xb7\xe6\xb7\x86\xe7\x9f\xa9\xe9\x98\xb5\n                trainer.vis.img(\'roi_cm\', at.totensor(trainer.roi_cm.conf, False).float())\n        #\xe4\xbd\xbf\xe7\x94\xa8\xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86\xe5\xaf\xb9\xe5\xbd\x93\xe5\x89\x8d\xe7\x9a\x84\xe7\xbd\x91\xe7\xbb\x9c\xe8\xbf\x9b\xe8\xa1\x8c\xe9\xaa\x8c\xe8\xaf\x81\xef\xbc\x8c\xe8\xbf\x94\xe5\x9b\x9e\xe4\xb8\x80\xe4\xb8\xaa\xe5\xad\x97\xe5\x85\xb8\xef\xbc\x8ckey\xe5\x80\xbc\xe6\x9c\x89AP,mAP\n        eval_result = eval(test_dataloader, faster_rcnn, test_num=opt.test_num)\n        #\xe5\xa6\x82\xe6\x9e\x9c\xe5\xbd\x93\xe5\x89\x8d\xe7\x9a\x84map\xe5\x80\xbc\xe4\xbc\x98\xe4\xba\x8ebest_map\xef\xbc\x8c\xe5\x88\x99\xe5\xb0\x86\xe5\xbd\x93\xe5\x89\x8d\xe5\x80\xbc\xe8\xb5\x8b\xe7\xbb\x99best_map\xe3\x80\x82\xe5\xb0\x86\xe5\xbd\x93\xe5\x89\x8d\xe6\xa8\xa1\xe5\x9e\x8b\xe4\xbf\x9d\xe7\x95\x99\n        if eval_result[\'map\'] > best_map:\n            best_map = eval_result[\'map\']\n            best_path = trainer.save(best_map=best_map)\n        #\xe5\xa6\x82\xe6\x9e\x9cepoch\xe5\x88\xb0\xe8\xbe\xbe9\xe6\x97\xb6\xef\xbc\x8c\xe5\x8a\xa0\xe8\xbd\xbd \xe5\xbd\x93\xe5\x89\x8d\xe7\x9a\x84\xe6\x9c\x80\xe4\xbc\x98\xe6\xa8\xa1\xe5\x9e\x8b\xef\xbc\x8c\xe5\xb9\xb6\xe5\xb0\x86\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\xe6\x8c\x89lr_decay\xe8\xa1\xb0\xe5\x87\x8f\xe8\xb0\x83\xe4\xbd\x8e\n        if epoch == 9:\n            trainer.load(best_path)\n            trainer.faster_rcnn.scale_lr(opt.lr_decay)\n            lr_ = lr_ * opt.lr_decay\n        #\xe5\x8f\xaf\xe8\xa7\x86\xe5\x8c\x96\xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86\xe7\x9a\x84test_map \xe5\x92\x8clog\xe4\xbf\xa1\xe6\x81\xaf\n        trainer.vis.plot(\'test_map\', eval_result[\'map\'])\n        log_info = \'lr:{}, map:{},loss:{}\'.format(str(lr_),\n                                                  str(eval_result[\'map\']),\n                                                  str(trainer.get_meter_data()))\n        trainer.vis.log(log_info)\n        if epoch == 13: \n            break\n\n\nif __name__ == \'__main__\':\n    import fire\n\n    fire.Fire()\n'"
FasterRcnn_pytorch/trainer.py,5,"b'from collections import namedtuple\nimport time\nfrom torch.nn import functional as F\nfrom model.utils.creator_tool import AnchorTargetCreator, ProposalTargetCreator\n\nfrom torch import nn\nimport torch as t\nfrom torch.autograd import Variable\nfrom utils import array_tool as at\nfrom utils.vis_tool import Visualizer\n\nfrom utils.config import opt\nfrom torchnet.meter import ConfusionMeter, AverageValueMeter\n#Tuple\xe5\x85\x83\xe7\xa5\x96\xe3\x80\x82\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84loss\xe5\x90\x8d\xe7\xa7\xb0\nLossTuple = namedtuple(\'LossTuple\',\n                       [\'rpn_loc_loss\',\n                        \'rpn_cls_loss\',\n                        \'roi_loc_loss\',\n                        \'roi_cls_loss\',\n                        \'total_loss\'\n                        ])\n\n\nclass FasterRCNNTrainer(nn.Module):\n    """"""wrapper for conveniently training. return losses\n       wrapper\xe4\xbb\xa5\xe4\xbe\xbf\xe6\x96\xb9\xe4\xbe\xbf\xe8\xae\xad\xe7\xbb\x83\xef\xbc\x8c\xe8\xbf\x94\xe5\x9b\x9elosses\n    The losses include:\n\n    * :obj:`rpn_loc_loss`: The localization loss for  Region Proposal Network (RPN).\n                           RPN\xe5\xae\x9a\xe4\xbd\x8dloss\n    * :obj:`rpn_cls_loss`: The classification loss for RPN.\n                           RPN\xe5\x88\x86\xe7\xb1\xbbloss\n    * :obj:`roi_loc_loss`: The localization loss for the head module.\n                            roi\xe5\xae\x9a\xe4\xbd\x8dloss\n    * :obj:`roi_cls_loss`: The classification loss for the head module.\n                            roi\xe5\x88\x86\xe7\xb1\xbbloss\n    * :obj:`total_loss`: The sum of 4 loss above.\n                          4\xe4\xb8\xaaloss\xe4\xb9\x8b\xe5\x92\x8c\n\n    Args:\n        faster_rcnn (model.FasterRCNN):\n            A Faster R-CNN model that is going to be trained.\n    """"""\n\n    def __init__(self, faster_rcnn):\n        super(FasterRCNNTrainer, self).__init__()\n        #\xe4\xbc\xa0\xe5\x85\xa5\xe7\x9a\x84\xe6\x98\xafFasterRCNNVGG16\xe6\xa8\xa1\xe5\x9e\x8b\xef\xbc\x8c\xe7\xbb\xa7\xe6\x89\xbf\xe4\xba\x86FasterRCNN\xe6\xa8\xa1\xe5\x9e\x8b\xef\xbc\x8c\xe8\x80\x8c\xe5\x8f\x82\xe6\x95\xb0\xe6\xa0\xb9\xe6\x8d\xae\xe8\xaf\xb4\xe6\x98\x8e \xe6\x98\xafFasterRCNN\xe6\xa8\xa1\xe5\x9e\x8b\n        #\xe5\x8d\xb3\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe7\x9a\x84\xe6\x98\xafFasterRCNN\xe6\xa8\xa1\xe5\x9e\x8b\n        #FasterRCNN\xe6\xa8\xa1\xe5\x9e\x8b\xe6\x98\xaf\xe7\x88\xb6\xe7\xb1\xbb   FasterRCNNVGG16\xe6\xa8\xa1\xe5\x9e\x8b\xe6\x98\xaf\xe5\xad\x90\xe7\xb1\xbb\n        self.faster_rcnn = faster_rcnn\n        #sigma for l1_smooth_loss\n        self.rpn_sigma = opt.rpn_sigma\n        self.roi_sigma = opt.roi_sigma\n\n        # target creator create gt_bbox gt_label etc as training targets.\n        #\xe7\x9b\xae\xe6\xa0\x87\xe6\xa1\x86creator \xe7\x9b\xae\xe6\xa0\x87\xe6\x98\xaf\xe4\xba\xa7\xe7\x94\x9f \xe7\x9c\x9f\xe5\xae\x9e\xe7\x9a\x84bbox \xe7\xb1\xbb\xe5\x88\xab\xe6\xa0\x87\xe7\xad\xbe\xe7\xad\x89\n        #\xe5\xb0\x86\xe7\x9c\x9f\xe5\xae\x9e\xe7\x9a\x84bbox\xe5\x88\x86\xe9\x85\x8d\xe7\xbb\x99\xe9\x94\x9a\xe7\x82\xb9\n        self.anchor_target_creator = AnchorTargetCreator()\n        self.proposal_target_creator = ProposalTargetCreator()\n        #\xe5\xbe\x97\xe5\x88\xb0faster\xe7\xbd\x91\xe7\xbb\x9c\xe6\x9d\x83\xe9\x87\x8d\xef\xbc\x8c\xe5\x9d\x87\xe5\x80\xbc \xe5\x92\x8c\xe6\x96\xb9\xe5\xb7\xae\n        self.loc_normalize_mean = faster_rcnn.loc_normalize_mean\n        self.loc_normalize_std = faster_rcnn.loc_normalize_std\n\n        #\xe5\xbe\x97\xe5\x88\xb0faster\xe7\xbd\x91\xe7\xbb\x9c\xe7\x9a\x84\xe4\xbc\x98\xe5\x8c\x96\xe5\x99\xa8\n        self.optimizer = self.faster_rcnn.get_optimizer()\n        # visdom wrapper\n        self.vis = Visualizer(env=opt.env)\n\n        # indicators for training status\n        #\xe8\xae\xad\xe7\xbb\x83\xe7\x8a\xb6\xe6\x80\x81\xe6\x8c\x87\xe6\xa0\x87  \xe4\xb8\xa4\xe4\xb8\xaa\xe6\xb7\xb7\xe6\xb7\x86\xe7\x9f\xa9\xe9\x98\xb5 2\xc3\x972\xef\xbc\x88\xe5\x89\x8d\xe6\x99\xaf\xe5\x90\x8e\xe6\x99\xaf\xef\xbc\x89   21\xc3\x9721\xef\xbc\x8820\xe7\xb1\xbb+\xe8\x83\x8c\xe6\x99\xaf\xef\xbc\x89\n        self.rpn_cm = ConfusionMeter(2)\n        self.roi_cm = ConfusionMeter(21)\n        self.meters = {k: AverageValueMeter() for k in LossTuple._fields}  # average loss \xe5\xb9\xb3\xe5\x9d\x87\xe6\x8d\x9f\xe5\xa4\xb1\n\n    def forward(self, imgs, bboxes, labels, scale):\n        """"""Forward Faster R-CNN and calculate losses.\n        Faster\xe7\xbd\x91\xe7\xbb\x9c\xe7\x9a\x84\xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe3\x80\x81\xe8\xae\xa1\xe7\xae\x97losses*************************\n        Here are notations used.\n\n        * :math:`N` is the batch size. `N`\xe6\x98\xaf\xe6\x89\xb9\xe9\x87\x8f\xe5\xa4\xa7\xe5\xb0\x8f\n        * :math:`R` is the number of bounding boxes per image. `R`\xe6\x98\xaf\xe6\xaf\x8f\xe4\xb8\xaa\xe5\x9b\xbe\xe5\x83\x8f\xe7\x9a\x84\xe8\xbe\xb9\xe7\x95\x8c\xe6\xa1\x86\xe7\x9a\x84\xe6\x95\xb0\xe9\x87\x8f\n\n        Currently, only :math:`N=1` is supported.\n        \xe5\xbd\x93\xe5\x89\x8d\xe6\xa8\xa1\xe5\x9e\x8b\xef\xbc\x8c\xe5\x8f\xaa\xe6\x9c\x89N=1\xe5\x8f\xaf\xe7\x94\xa8\n\n        Args:\n            imgs (~torch.autograd.Variable): A variable with a batch of images.\n                                            batch=1\xe7\x9a\x84\xe5\x9b\xbe\xe7\x89\x87\xe5\x8f\x98\xe9\x87\x8f\n            bboxes (~torch.autograd.Variable): A batch of bounding boxes.\n                Its shape is :math:`(N, R, 4)`.\n                                            \xe7\x9c\x9f\xe5\xae\x9e\xe4\xba\xba\xe5\xb7\xa5\xe6\xa0\x87\xe6\xb3\xa8\xe7\x9a\x84bboxes\xe5\x8f\x98\xe9\x87\x8f\n            labels (~torch.autograd..Variable): A batch of labels.\n                Its shape is :math:`(N, R)`.\n                 The background is excluded from the definition, which means that the range of the value\n                is :math:`[0, L - 1]`. :math:`L` is the number of foreground classes.\n                 \xe8\x83\x8c\xe6\x99\xaf\xe8\xa2\xab\xe6\x8e\x92\xe9\x99\xa4\xe5\x9c\xa8\xe5\xae\x9a\xe4\xb9\x89\xe4\xb9\x8b\xe5\xa4\x96\xef\xbc\x8c\xe8\xbf\x99\xe6\x84\x8f\xe5\x91\xb3\xe7\x9d\x80\xe5\x80\xbc\xe7\x9a\x84\xe8\x8c\x83\xe5\x9b\xb4\xe3\x80\x82`L`\xe6\x98\xaf\xe5\x89\x8d\xe6\x99\xaf\xe7\xb1\xbb\xe7\x9a\x84\xe6\x95\xb0\xe9\x87\x8f\n            scale (float): Amount of scaling applied to\n                the raw image during preprocessing.\n                \xe9\xa2\x84\xe5\xa4\x84\xe7\x90\x86\xe6\x9c\x9f\xe9\x97\xb4\xe5\xba\x94\xe7\x94\xa8\xe4\xba\x8e\xe5\x8e\x9f\xe5\xa7\x8b\xe5\x9b\xbe\xe5\x83\x8f\xe7\x9a\x84\xe7\xbc\xa9\xe6\x94\xbe\xe9\x87\x8f\n\n        Returns:\n            namedtuple of 5 losses\n            \xe4\xba\x94\xe4\xb8\xaa\xe6\x8d\x9f\xe5\xa4\xb1\n        """"""\n\n        n = bboxes.shape[0]\n        #\xe5\x88\xa4\xe6\x96\xad\xef\xbc\x8c\xe5\x8f\xaa\xe6\x94\xaf\xe6\x8c\x81batch\xe4\xb8\xba1\n        if n != 1:\n            raise ValueError(\'Currently only batch size 1 is supported.\')\n        #img_size=\xe5\x8e\x9f\xe5\x9b\xbe\xe5\x83\x8f\xe7\x9a\x84\xe9\xab\x98\xe3\x80\x81\xe5\xae\xbd\n        _, _, H, W = imgs.shape\n        img_size = (H, W)\n        #\xe9\x80\x9a\xe8\xbf\x87\xe6\x8f\x90\xe5\x8f\x96\xe5\x99\xa8\xef\xbc\x88\xe9\xa2\x84\xe8\xae\xad\xe7\xbb\x83\xe5\xa5\xbd\xe7\x9a\x84VGG16\xef\xbc\x89\xe7\xbd\x91\xe7\xbb\x9c\xe6\x8f\x90\xe5\x8f\x96\xe7\x89\xb9\xe5\xbe\x81\n        features = self.faster_rcnn.extractor(imgs)\n        #\xe9\x80\x9a\xe8\xbf\x87rpn\xe7\xbd\x91\xe7\xbb\x9c\xef\xbc\x88\xe5\x8c\xba\xe5\x9f\x9f\xe6\x8f\x90\xe6\xa1\x88\xe7\xbd\x91\xe7\xbb\x9c\xef\xbc\x89\xe5\xbe\x97\xe5\x88\xb0\n        #rpn\xe8\xbf\x99\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa\xe5\x8c\xba\xe5\x9f\x9f\xe6\x8f\x90\xe6\xa1\x88\xe7\xbd\x91\xe7\xbb\x9c\xe3\x80\x82\xe5\xae\x83\xe6\x8f\x90\xe5\x8f\x96\xe5\x9b\xbe\xe5\x83\x8f\xe7\x89\xb9\xe5\xbe\x81\xef\xbc\x8c\xe9\xa2\x84\xe6\xb5\x8b\xe8\xbe\x93\xe5\x87\xbarois\n        #rpn_locs[1,17316,4]   rpn_scores[1,17316,2]   rois[2000,4]   roi_indices[2000,]\xe5\x85\xa8\xe4\xb8\xba0  anchor [17316,4]\n        rpn_locs, rpn_scores, rois, roi_indices, anchor = \\\n            self.faster_rcnn.rpn(features, img_size, scale)\n\n        # Since batch size is one, convert variables to singular form\n        # \xe7\x94\xb1\xe4\xba\x8e\xe6\x89\xb9\xe9\x87\x8f\xe5\xa4\xa7\xe5\xb0\x8f\xe4\xb8\xba1\xef\xbc\x8c\xe5\x9b\xa0\xe6\xad\xa4\xe5\xb0\x86\xe5\x8f\x98\xe9\x87\x8f\xe8\xbd\xac\xe6\x8d\xa2\xe4\xb8\xba\xe5\x8d\x95\xe6\x95\xb0\xe5\xbd\xa2\xe5\xbc\x8f\xef\xbc\x88\xe5\x8d\xb3\xe5\x8e\x8b\xe7\xbc\xa9\xe7\xac\xac\xe4\xb8\x80\xe7\xbb\xb4\xef\xbc\x89\n        #bbox\xe5\x8f\x98\xe4\xb8\xba[1,4]\n        bbox = bboxes[0]\n        label = labels[0]\n        #\xe5\x88\x99rpn_score\xe5\x8f\x98\xe4\xb8\xba[17316,4]  rpn_loc \xe5\x8f\x98\xe4\xb8\xba[17316,2]\n        rpn_score = rpn_scores[0]\n        rpn_loc = rpn_locs[0]\n        #\xe5\xa4\xa7\xe7\xba\xa62000\xe4\xb8\xaarois\n        roi = rois\n\n        # Sample RoIs and forward   \xe7\xae\x80\xe5\x8d\x95\xe7\x9a\x84ROIs\xe5\x92\x8c\xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\n        # it\'s fine to break the computation graph of rois, consider them as constant input\n        #\xe6\x89\x93\xe7\xa0\xb4rois\xe7\x9a\x84\xe8\xae\xa1\xe7\xae\x97\xe5\x9b\xbe\xef\xbc\x8c\xe5\xb0\x86\xe5\xae\x83\xe4\xbd\x9c\xe4\xb8\xba\xe4\xb8\x80\xe4\xb8\xaa\xe5\x9b\xba\xe5\xae\x9a\xe4\xb8\x8d\xe5\x8f\x98\xe7\x9a\x84\xe8\xbe\x93\xe5\x85\xa5\n        #proposal_target_creator  \xe8\xbe\x93\xe5\x85\xa5\xe4\xb8\xbarois\xef\xbc\x882000\xe4\xb8\xaa\xe5\x80\x99\xe9\x80\x89\xe6\xa1\x86\xef\xbc\x8c\xe5\x92\x8c\xe4\xba\xba\xe5\xb7\xa5\xe6\xa0\x87\xe6\xb3\xa8\xe7\x9a\x84bbox\xef\xbc\x89\xe7\x94\xa8\xe4\xba\x8e\xe7\x94\x9f\xe6\x88\x90\xe8\xae\xad\xe7\xbb\x83\xe7\x9b\xae\xe6\xa0\x87\xef\xbc\x8c\xe5\x8f\xaa\xe8\xae\xad\xe7\xbb\x83\xe7\x94\xa8\xe5\x88\xb0\n        #2000\xe4\xb8\xaarois\xe9\x80\x89\xe5\x87\xba128\xe4\xb8\xaa\n        #sample_roi[128,4]     gt_roi_loc[128,4]     gt_roi_label[128,] \xe5\x80\xbc\xe4\xb8\xba0\xe6\x88\x961 \xe8\xa1\xa8\xe7\xa4\xba\xe6\xad\xa3\xe8\xb4\x9f\xe6\xa0\xb7\xe6\x9c\xac\n        sample_roi, gt_roi_loc, gt_roi_label = self.proposal_target_creator(\n            roi,\n            at.tonumpy(bbox),\n            at.tonumpy(label),\n            self.loc_normalize_mean,\n            self.loc_normalize_std)\n        # NOTE it\'s all zero because now it only support for batch=1 now\n        #\xe5\xae\x83\xe5\x85\xa8\xe9\x83\xa8\xe4\xb8\xba\xe9\x9b\xb6\xef\xbc\x8c\xe5\x9b\xa0\xe4\xb8\xba\xe7\x8e\xb0\xe5\x9c\xa8\xe5\xae\x83\xe5\x8f\xaa\xe6\x94\xaf\xe6\x8c\x81batch = 1\n        sample_roi_index = t.zeros(len(sample_roi))\n        #roi head\xe7\xbd\x91\xe7\xbb\x9c\xe8\xbf\x9b\xe8\xa1\x8c\xe9\xa2\x84\xe6\xb5\x8b\xe7\xb1\xbb\xe5\x88\xab\xe5\x92\x8c\xe7\x9b\xae\xe6\xa0\x87\xe6\xa1\x86\n        #RoIHead\xef\xbc\x9a \xe8\xb4\x9f\xe8\xb4\xa3\xe5\xaf\xb9rois\xe5\x88\x86\xe7\xb1\xbb\xe5\x92\x8c\xe5\xbe\xae\xe8\xb0\x83\xe3\x80\x82\xe5\xaf\xb9RPN\xe6\x89\xbe\xe5\x87\xba\xe7\x9a\x84rois\xef\xbc\x8c\xe5\x88\xa4\xe6\x96\xad\xe5\xae\x83\xe6\x98\xaf\xe5\x90\xa6\xe5\x8c\x85\xe5\x90\xab\xe7\x9b\xae\xe6\xa0\x87\xef\xbc\x8c\xe5\xb9\xb6\xe4\xbf\xae\xe6\xad\xa3\xe6\xa1\x86\xe7\x9a\x84\xe4\xbd\x8d\xe7\xbd\xae\xe5\x92\x8c\xe5\xba\xa7\xe6\xa0\x87\n        #\xe4\xbd\xbf\xe7\x94\xa8RoIs\xe6\x8f\x90\xe8\xae\xae\xe7\x9a\x84\xe7\x9a\x84feature maps\xef\xbc\x8c\xe5\xaf\xb9RoI\xe4\xb8\xad\xe7\x9a\x84\xe5\xaf\xb9\xe8\xb1\xa1\xe8\xbf\x9b\xe8\xa1\x8c\xe5\x88\x86\xe7\xb1\xbb\xe5\xb9\xb6\xe6\x8f\x90\xe9\xab\x98\xe7\x9b\xae\xe6\xa0\x87\xe6\xa1\x86\xe5\xae\x9a\xe4\xbd\x8d\n        #roi_cls_loc  roi\xe7\x9a\x84\xe5\x88\x86\xe7\xb1\xbb\xe3\x80\x81\xe5\x9b\x9e\xe5\xbd\x92\n        #\xe4\xbc\xa0\xe5\x85\xa5  \xe7\x89\xb9\xe5\xbe\x81\xe6\x8f\x90\xe5\x8f\x96\xe7\x9a\x84features   \xe5\x92\x8c  128\xe4\xb8\xaaROI\n        #roi_cls_loc [128,84]\xe5\x9b\x9e\xe5\xbd\x92\xe5\xae\x9a\xe4\xbd\x8d    roi_score[128,21]\xe5\x88\x86\xe7\xb1\xbb\xef\xbc\x8820\xe7\xb1\xbb\xe5\x8a\xa0\xe8\x83\x8c\xe6\x99\xaf\xef\xbc\x89\n        roi_cls_loc, roi_score = self.faster_rcnn.head(\n            features,\n            sample_roi,\n            sample_roi_index)\n\n        # ------------------ RPN losses -------------------#\n        #\xe7\x9c\x9f\xe5\xae\x9e\xe6\xa0\x87\xe6\xb3\xa8\xe7\x9a\x84bbox,\xe9\xa2\x84\xe6\xb5\x8b\xe5\x87\xba\xe6\x9d\xa5\xe7\x9a\x84anchor\xe9\x94\x9a\xe7\x82\xb9\n        # \xe5\xb0\x86\xe7\x9c\x9f\xe5\xae\x9e\xe7\x9a\x84bbox\xe5\x88\x86\xe9\x85\x8d\xe7\xbb\x99\xe9\x94\x9a\xe7\x82\xb9\xef\xbc\x8c\xe8\xbf\x94\xe5\x9b\x9e \xe7\xbb\x8f\xe8\xbf\x87rpn\xe5\x90\x8e\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84\xe5\xae\x9a\xe4\xbd\x8d\xe5\x92\x8c\xe6\xa0\x87\xe7\xad\xbe\n        #gt_rpn_loc[17316,4]     gt_rpn_label  [17316,]\n        gt_rpn_loc, gt_rpn_label = self.anchor_target_creator(\n            at.tonumpy(bbox),\n            anchor,\n            img_size)\n        #\xe8\xbd\xac\xe4\xb8\xba\xe5\x8f\x98\xe9\x87\x8fV  \xe8\xbd\xac\xe4\xb8\xbalong\xe5\x9e\x8b\n        gt_rpn_label = at.tovariable(gt_rpn_label).long()\n        gt_rpn_loc = at.tovariable(gt_rpn_loc)\n        #rpn\xe7\x9a\x84\xe5\x9b\x9e\xe5\xbd\x92\xe5\xae\x9a\xe4\xbd\x8d\xe6\x8d\x9f\xe5\xa4\xb1   rpn_loc_loss[1]\n        rpn_loc_loss = _fast_rcnn_loc_loss(\n            rpn_loc,\n            gt_rpn_loc,\n            gt_rpn_label.data,\n            self.rpn_sigma)\n\n        # NOTE: default value of ignore_index is -100 ...\n        #ignore_index\xe7\x9a\x84\xe9\xbb\x98\xe8\xae\xa4\xe5\x80\xbc\xe6\x98\xaf - 100...\n        #F\xef\xbc\x9apytorch\xe7\x9a\x84function\n        #\xe5\x88\x86\xe7\xb1\xbb\xe4\xbd\xbf\xe7\x94\xa8\xe4\xba\xa4\xe5\x8f\x89\xe7\x86\xb5\n        rpn_cls_loss = F.cross_entropy(rpn_score, gt_rpn_label.cuda(), ignore_index=-1)\n\n        _gt_rpn_label = gt_rpn_label[gt_rpn_label > -1]\n        _rpn_score = at.tonumpy(rpn_score)[at.tonumpy(gt_rpn_label) > -1]\n        #\xe6\xb7\xbb\xe5\x8a\xa0\xe8\xbf\x9brpn \xe6\xb7\xb7\xe6\xb7\x86\xe7\x9f\xa9\xe9\x98\xb5\n        self.rpn_cm.add(at.totensor(_rpn_score, False), _gt_rpn_label.data.long())\n\n        # ------------------ ROI losses (fast rcnn loss) -------------------#\n        #roi\xe5\x88\x86\xe7\xb1\xbb\xe5\x92\x8c\xe5\x9b\x9e\xe5\xbd\x92   \xe5\x8e\x8b\xe7\xbc\xa9\xe7\xac\xac\xe4\xb8\x80\xe7\xbb\xb4\n        #n_sample 128\n        n_sample = roi_cls_loc.shape[0]\n        #\xe6\x94\xb9\xe5\x8f\x98\xe5\xbd\xa2\xe7\x8a\xb6\xe4\xb8\xba[ 32,4]\n        roi_cls_loc = roi_cls_loc.view(n_sample, -1, 4)\n        #\xe5\xbe\x97\xe5\x88\xb0roi\xe7\x9a\x84\xe5\x9b\x9e\xe5\xbd\x92\n        roi_loc = roi_cls_loc[t.arange(0, n_sample).long().cuda(), \\\n                              at.totensor(gt_roi_label).long()]\n        # gt_roi_label\xef\xbc\x9a\xe7\x9c\x9f\xe5\xae\x9eroi\xe7\x9a\x84\xe6\xa0\x87\xe7\xad\xbe\n        #gt_roi_loc\xef\xbc\x9a\xe7\x9c\x9f\xe5\xae\x9eroi\xe7\x9a\x84\xe5\x9b\x9e\xe5\xbd\x92\n        gt_roi_label = at.tovariable(gt_roi_label).long()\n        gt_roi_loc = at.tovariable(gt_roi_loc)\n        #roi\xe7\x9a\x84\xe5\x9b\x9e\xe5\xbd\x92\xe6\x8d\x9f\xe5\xa4\xb1  \xe8\xae\xa1\xe7\xae\x97\xe5\x9b\x9e\xe5\xbd\x92\xe5\xae\x9a\xe4\xbd\x8d\xe7\x9a\x84\xe6\x8d\x9f\xe5\xa4\xb1\n        roi_loc_loss = _fast_rcnn_loc_loss(\n            #contiguous\xe4\xbb\x8e\xe4\xb8\x8d\xe8\xbf\x9e\xe7\xbb\xad\xe8\xb0\x83\xe6\x95\xb4\xe4\xb8\xba\xe8\xbf\x9e\xe7\xbb\xad\n            roi_loc.contiguous(),\n            gt_roi_loc,\n            gt_roi_label.data,\n            self.roi_sigma)\n        #roi\xe5\x88\x86\xe7\xb1\xbb\xe6\x8d\x9f\xe5\xa4\xb1\xef\xbc\x88\xe4\xba\xa4\xe5\x8f\x89\xe7\x86\xb5\xef\xbc\x89\n        roi_cls_loss = nn.CrossEntropyLoss()(roi_score, gt_roi_label.cuda())\n        #\xe6\xb7\xbb\xe5\x8a\xa0\xe8\xbf\x9broi \xe6\xb7\xb7\xe6\xb7\x86\xe7\x9f\xa9\xe9\x98\xb5\n        self.roi_cm.add(at.totensor(roi_score, False), gt_roi_label.data.long())\n        #\xe8\xae\xa1\xe7\xae\x97\xe6\x80\xbb\xe6\x8d\x9f\xe5\xa4\xb1\n        losses = [rpn_loc_loss, rpn_cls_loss, roi_loc_loss, roi_cls_loss]\n        losses = losses + [sum(losses)]\n        #\xe8\xbf\x94\xe5\x9b\x9eTuple\xef\xbc\x8c\xe5\x9b\x9b\xe4\xb8\xaa\xe6\x8d\x9f\xe5\xa4\xb1+\xe6\x80\xbb\xe6\x8d\x9f\xe5\xa4\xb1\n        return LossTuple(*losses)\n    #\xe8\xae\xad\xe7\xbb\x83\xe5\xb9\xb6\xe6\x9b\xb4\xe6\x96\xb0\xe5\x8f\xaf\xe5\xad\xa6\xe4\xb9\xa0\xe5\x8f\x82\xe6\x95\xb0\n    def train_step(self, imgs, bboxes, labels, scale):\n        #\xe4\xbc\x98\xe5\x8c\x96\xe5\x99\xa8\xe6\xa2\xaf\xe5\xba\xa6\xe6\xb8\x85\xe9\x9b\xb6\n        self.optimizer.zero_grad()\n        #\xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xef\xbc\x88\xe9\x87\x8d\xe7\x82\xb9*\xef\xbc\x89  \xe8\xbf\x94\xe5\x9b\x9e\xef\xbc\x88\xe6\x80\xbb\xe6\x8d\x9f\xe5\xa4\xb1 \xe5\x92\x8c\xe5\x9b\x9b\xe7\xb1\xbb\xe6\x8d\x9f\xe5\xa4\xb1\xef\xbc\x89\n        losses = self.forward(imgs, bboxes, labels, scale)\n        #\xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xef\xbc\x88\xe9\x87\x8d\xe7\x82\xb9*\xef\xbc\x89\n        #\xe9\x92\x88\xe5\xaf\xb9\xe6\x80\xbb\xe6\x8d\x9f\xe5\xa4\xb1\xe8\xbf\x9b\xe8\xa1\x8c\xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\n        losses.total_loss.backward()\n        # \xe6\x9b\xb4\xe6\x96\xb0\xe5\x8f\xaf\xe5\xad\xa6\xe4\xb9\xa0\xe5\x8f\x82\xe6\x95\xb0\n        self.optimizer.step()\n        #\xe5\xb0\x86losses\xe5\x86\x99\xe5\x85\xa5meter\xe4\xb8\xad\n        self.update_meters(losses)\n        return losses\n\n    def save(self, save_optimizer=False, save_path=None, **kwargs):\n        """"""serialize models include optimizer and other info\n        return path where the model-file is stored.\n\n        Args:\n            save_optimizer (bool): whether save optimizer.state_dict().\n            save_path (string): where to save model, if it\'s None, save_path\n                is generate using time str and info from kwargs.\n        \n        Returns:\n            save_path(str): the path to save models.\n        """"""\n        save_dict = dict()\n\n        save_dict[\'model\'] = self.faster_rcnn.state_dict()\n        save_dict[\'config\'] = opt._state_dict()\n        save_dict[\'other_info\'] = kwargs\n        save_dict[\'vis_info\'] = self.vis.state_dict()\n\n        if save_optimizer:\n            save_dict[\'optimizer\'] = self.optimizer.state_dict()\n\n        if save_path is None:\n            timestr = time.strftime(\'%m%d%H%M\')\n            save_path = \'checkpoints/fasterrcnn_%s\' % timestr\n            for k_, v_ in kwargs.items():\n                save_path += \'_%s\' % v_\n\n        t.save(save_dict, save_path)\n        self.vis.save([self.vis.env])\n        return save_path\n\n    def load(self, path, load_optimizer=True, parse_opt=False, ):\n        state_dict = t.load(path)\n        if \'model\' in state_dict:\n            self.faster_rcnn.load_state_dict(state_dict[\'model\'])\n        else:  # legacy way, for backward compatibility\n            self.faster_rcnn.load_state_dict(state_dict)\n            return self\n        if parse_opt:\n            opt._parse(state_dict[\'config\'])\n        if \'optimizer\' in state_dict and load_optimizer:\n            self.optimizer.load_state_dict(state_dict[\'optimizer\'])\n        return self\n    #\xe6\x9b\xb4\xe6\x96\xb0\xe4\xbb\xaa\xe8\xa1\xa8\xe7\x9b\x98  \xe7\x94\xa8\xe4\xbb\xa5\xe6\x98\xbe\xe7\xa4\xba\n    def update_meters(self, losses):\n        loss_d = {k: at.scalar(v) for k, v in losses._asdict().items()}\n        for key, meter in self.meters.items():\n            meter.add(loss_d[key])\n    #\xe5\xb0\x86\xe5\x80\xbc\xe9\x87\x8d\xe7\xbd\xae\xe5\x88\xb00\n    def reset_meters(self):\n        for key, meter in self.meters.items():\n            meter.reset()\n        #\xe5\xb0\x86\xe4\xb8\xa4\xe4\xb8\xaa\xe6\xb7\xb7\xe6\xb7\x86\xe7\x9f\xa9\xe9\x98\xb5\xe7\x9a\x84\xe5\x86\x85\xe5\xae\xb9\xe4\xb9\x9f\xe7\xbd\xae\xe4\xb8\xba0\n        self.roi_cm.reset()\n        self.rpn_cm.reset()\n\n    def get_meter_data(self):\n        return {k: v.value()[0] for k, v in self.meters.items()}\n\n#\xe8\xae\xa1\xe7\xae\x97smooth_l1\xe6\x8d\x9f\xe5\xa4\xb1\ndef _smooth_l1_loss(x, t, in_weight, sigma):\n    sigma2 = sigma ** 2\n    diff = in_weight * (x - t)\n    abs_diff = diff.abs()\n    flag = (abs_diff.data < (1. / sigma2)).float()\n    flag = Variable(flag)\n    y = (flag * (sigma2 / 2.) * (diff ** 2) +\n         (1 - flag) * (abs_diff - 0.5 / sigma2))\n    return y.sum()\n\n#\xe8\xae\xa1\xe7\xae\x97\xe5\x9b\x9e\xe5\xbd\x92\xe5\xae\x9a\xe4\xbd\x8d\xe7\x9a\x84\xe6\x8d\x9f\xe5\xa4\xb1\ndef _fast_rcnn_loc_loss(pred_loc, gt_loc, gt_label, sigma):\n    in_weight = t.zeros(gt_loc.shape).cuda()\n    # Localization loss is calculated only for positive rois.\n    # NOTE:  unlike origin implementation, \n    # we don\'t need inside_weight and outside_weight, they can calculate by gt_label\n    #\xe5\xae\x9a\xe4\xbd\x8d\xe6\x8d\x9f\xe5\xa4\xb1 \xe5\x8f\xaa\xe8\xae\xa1\xe7\xae\x97 \xe6\xad\xa3\xe6\xa0\xb7\xe6\x9c\xac\xef\xbc\x88\xe5\xad\x98\xe5\x9c\xa8\xe7\x89\xa9\xe4\xbd\x93\xe7\x9a\x84\xe5\x89\x8d\xe6\x99\xaf\xef\xbc\x89\xe7\x9a\x84\xe6\x8d\x9f\xe5\xa4\xb1\n    #\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe5\x85\xa80\xe7\x9a\x84tensor,\xe5\xb0\x86\xe6\xad\xa3\xe6\xa0\xb7\xe6\x9c\xac\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84\xe4\xbd\x8d\xe7\xbd\xae\xe5\x80\xbc\xe4\xb8\xba1\xe3\x80\x82\xe4\xbb\xa5\xe4\xbe\xbf\xe5\x8f\xaa\xe8\xae\xa1\xe7\xae\x97\xe6\xad\xa3\xe6\xa0\xb7\xe6\x9c\xac\xe7\x9a\x84\xe6\x8d\x9f\xe5\xa4\xb1\n    in_weight[(gt_label > 0).view(-1, 1).expand_as(in_weight).cuda()] = 1\n    #\xe4\xbd\xbf\xe7\x94\xa8smooth_l1\xef\xbc\x88\xe5\x9b\x9e\xe5\xbd\x92\xe4\xbd\x8d\xe7\xbd\xae\xe4\xbd\xbf\xe7\x94\xa8smooth_l1\xef\xbc\x89\xe8\xae\xa1\xe7\xae\x97\xe6\x8d\x9f\xe5\xa4\xb1\n    loc_loss = _smooth_l1_loss(pred_loc, gt_loc, Variable(in_weight), sigma)\n    # Normalize by total number of negtive and positive rois.\n    #\xe4\xb8\xba\xe4\xba\x86\xe4\xbe\xbf\xe4\xba\x8e\xe8\xae\xad\xe7\xbb\x83\xef\xbc\x8c\xe5\xaf\xb9\xe9\x80\x89\xe6\x8b\xa9\xe5\x87\xba\xe7\x9a\x84128\xe4\xb8\xaaRoIs\xef\xbc\x8c\xe8\xbf\x98\xe5\xaf\xb9\xe4\xbb\x96\xe4\xbb\xac\xe7\x9a\x84gt_roi_loc \xe8\xbf\x9b\xe8\xa1\x8c\xe6\xa0\x87\xe5\x87\x86\xe5\x8c\x96\xe5\xa4\x84\xe7\x90\x86\xef\xbc\x88\xe5\x87\x8f\xe5\x8e\xbb\xe5\x9d\x87\xe5\x80\xbc\xe9\x99\xa4\xe4\xbb\xa5\xe6\xa0\x87\xe5\x87\x86\xe5\xb7\xae\xef\xbc\x89\n    loc_loss /= (gt_label >= 0).sum()  # ignore gt_label==-1 for rpn_loss\n    return loc_loss\n'"
ImageDenoising_pytorch/__init__.py,0,b''
ImageDenoising_pytorch/config.py,0,"b'# -*- coding:utf-8 -*-\n# power by Mr.Li\n# \xe8\xae\xbe\xe7\xbd\xae\xe9\xbb\x98\xe8\xae\xa4\xe5\x8f\x82\xe6\x95\xb0\nclass DefaultConfig():\n    env = \'default\'  # visdom \xe7\x8e\xaf\xe5\xa2\x83\xe7\x9a\x84\xe5\x90\x8d\xe5\xad\x97\n    model = \'NetWork\'  # \xe4\xbd\xbf\xe7\x94\xa8\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\xef\xbc\x8c\xe5\x90\x8d\xe5\xad\x97\xe5\xbf\x85\xe9\xa1\xbb\xe4\xb8\x8emodels/__init__.py\xe4\xb8\xad\xe7\x9a\x84\xe5\x90\x8d\xe5\xad\x97\xe4\xb8\x80\xe8\x87\xb4\n\n    data_root=\'/home/bobo/data/PapersReproduced/\'   #\xe8\xaf\xa5\xe6\x96\x87\xe4\xbb\xb6\xe5\xa4\xb9\xe4\xb8\x8b\xe5\xad\x98\xe7\x9d\x80\xe4\xb8\xa4\xe4\xb8\xaa\xe6\x96\x87\xe4\xbb\xb6\xe5\xa4\xb9  \xe5\x8e\x9f\xe5\x9b\xbe  \xe8\xb7\x9f  \xe7\x81\xb0\xe5\xba\xa6\xe5\x8a\xa0\xe5\x99\xaa\xe5\x9b\xbe\n    img_lena=\'/home/bobo/data/PapersReproduced/lena.jpg\'  #\xe6\xb5\x8b\xe8\xaf\x95\xe5\x9b\xbe\xe7\x89\x87\xef\xbc\x8c\xe6\x9c\x89\xe5\x8e\x9f\xe5\x9b\xbe \xe8\xb7\x9f \xe7\x81\xb0\xe5\xba\xa6\xe5\x8a\xa0\xe5\x99\xaa\xe5\x9b\xbe\n\n\n    # load_model_path = ""/home/bobo/PycharmProjects/torchProjectss/papersReproduced/checkpoints/NetWork_0517_21:54:05.pth""  # \xe5\x8a\xa0\xe8\xbd\xbd\xe9\xa2\x84\xe8\xae\xad\xe7\xbb\x83\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\xe7\x9a\x84\xe8\xb7\xaf\xe5\xbe\x84\xef\xbc\x8c\xe4\xb8\xbaNone\xe4\xbb\xa3\xe8\xa1\xa8\xe4\xb8\x8d\xe5\x8a\xa0\xe8\xbd\xbd\n    load_model_path =None\n\n    batch_size = 32  # batch size\n    use_gpu = True  # user GPU or not\n    num_workers = 4  # how many workers for loading data  \xe5\x8a\xa0\xe8\xbd\xbd\xe6\x95\xb0\xe6\x8d\xae\xe6\x97\xb6\xe7\x9a\x84\xe7\xba\xbf\xe7\xa8\x8b\n    print_freq = 20  # print info every N batch  \n\n    max_epoch = 100  \n    lr = 0.001  # initial learning rate\n    lr_decay = 0.5  # when val_loss increase, lr = lr*lr_decay\n    weight_decay = 0e-5  # \xe6\x8d\x9f\xe5\xa4\xb1\xe5\x87\xbd\xe6\x95\xb0\xe3\x80\x81\n #\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe8\xaf\xa5\xe7\xb1\xbb\xe7\x9a\x84\xe4\xb8\x80\xe4\xb8\xaa\xe5\xaf\xb9\xe8\xb1\xa1  \nopt=DefaultConfig()'"
ImageDenoising_pytorch/main.py,3,"b'# -*- coding:utf-8 -*-\n# power by Mr.Li\nfrom config import opt\nimport os\nimport torch as t\nimport models.NetWork as NetWork\nfrom data.dataprocessing import DataProcessing\nfrom torch.utils.data import DataLoader  #\xe6\x95\xb0\xe6\x8d\xae\xe5\x8a\xa0\xe8\xbd\xbd\xe5\x99\xa8\nfrom torch.autograd import Variable\nfrom torchnet import meter  #\xe4\xbb\xaa\xe8\xa1\xa8  \xe7\x94\xa8\xe6\x9d\xa5\xe6\x98\xbe\xe7\xa4\xbaloss\xe7\xad\x89\xe5\x9b\xbe\xe5\xbd\xa2\nfrom utils.visualize import Visualizer  #\xe5\x8f\xaf\xe8\xa7\x86\xe5\x8c\x96visdom\nfrom tqdm import tqdm  #\xe6\x98\xbe\xe7\xa4\xba\xe8\xbf\x9b\xe5\xba\xa6\xe6\x9d\xa1\nfrom torch.nn import functional as F\nfrom pylab import *\n\nfrom numpy import *\n# \xe5\xae\x9a\xe4\xb9\x89\xe4\xb8\x80\xe4\xb8\xaadict\xe5\xad\x97\xe5\x85\xb8\xef\xbc\x8c\xe7\x94\xa8\xe4\xba\x8e\xe4\xbf\x9d\xe5\xad\x98\xe6\xb5\x8b\xe8\xaf\x95\xe5\x9b\xbe\xe5\x83\x8flena\xe5\x9c\xa8\xe6\xaf\x8f\xe4\xb8\xaaepoch\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\xe4\xba\xa7\xe7\x94\x9f\xe7\x9a\x84\xe5\x8e\xbb\xe5\x99\xaa\xe5\x9b\xbe\xe5\x83\x8f\xe3\x80\x82\xe4\xbb\xa5\xe5\xaf\xb9\xe6\xaf\x94\xe6\x95\x88\xe6\x9e\x9c\ndict_lena = {}\n\n\ndef train():\n\n    vis=Visualizer(opt.env)\n    # \xe5\xae\x9a\xe4\xb9\x89\xe4\xb8\x80\xe4\xb8\xaa\xe7\xbd\x91\xe7\xbb\x9c\xe6\xa8\xa1\xe5\x9e\x8b\xe5\xaf\xb9\xe8\xb1\xa1\n    netWork=NetWork()\n    # \xe5\x85\x88\xe5\xb0\x86\xe6\xa8\xa1\xe5\x9e\x8b\xe5\x8a\xa0\xe8\xbd\xbd\xe5\x88\xb0\xe5\x86\x85\xe5\xad\x98\xe4\xb8\xad\xef\xbc\x8c\xe5\x8d\xb3CPU\xe4\xb8\xad\n    map_location = lambda storage, loc: storage\n    if opt.load_model_path:\n        netWork.load_state_dict(t.load(opt.load_model_path,map_location=map_location))\n    # \xe5\xb0\x86\xe6\xa8\xa1\xe5\x9e\x8b\xe8\xbd\xac\xe5\x88\xb0GPU   1\xef\xbc\x9a\xe8\xbd\xac\xe5\x88\xb0GPU1\xe4\xb8\x8a\n    if opt.use_gpu:\n        netWork.cuda(1)\n    # step2: \xe5\x8a\xa0\xe8\xbd\xbd\xe6\x95\xb0\xe6\x8d\xae\n    train_data=DataProcessing(opt.data_root,train=True)\n    #train=False  test=False   \xe5\x88\x99\xe4\xb8\xba\xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86\n    val_data=DataProcessing(opt.data_root,train=False)\n    # \xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe5\x8a\xa0\xe8\xbd\xbd\xe5\x99\xa8\n    train_dataloader = DataLoader(train_data, opt.batch_size, shuffle=True, num_workers=opt.num_workers)\n    val_dataloader = DataLoader(val_data, 1, shuffle=False, num_workers=opt.num_workers)\n    # step3: criterion \xe6\x8d\x9f\xe5\xa4\xb1\xe5\x87\xbd\xe6\x95\xb0\xe5\x92\x8coptimizer\xe4\xbc\x98\xe5\x8c\x96\xe5\x99\xa8\n    # \xe6\x8d\x9f\xe5\xa4\xb1\xe5\x87\xbd\xe6\x95\xb0\xe4\xbd\xbf\xe7\x94\xa8\xe5\x9d\x87\xe6\x96\xb9\xe8\xaf\xaf\xe5\xb7\xae   \n    criterion = t.nn.MSELoss()\n    lr=opt.lr\n    # \xe4\xbc\x98\xe5\x8c\x96\xe5\x99\xa8\xe4\xbd\xbf\xe7\x94\xa8Adam\n    optimizer = t.optim.Adam(netWork.parameters(), lr=opt.lr, weight_decay =opt.weight_decay)\n    # step4: \xe7\xbb\x9f\xe8\xae\xa1\xe6\x8c\x87\xe6\xa0\x87meters  \xe4\xbb\xaa\xe8\xa1\xa8 \xe6\x98\xbe\xe7\xa4\xba\xe6\x8d\x9f\xe5\xa4\xb1\xe7\x9a\x84\xe5\x9b\xbe\xe5\xbd\xa2\n    #\xe8\xae\xa1\xe7\xae\x97\xe6\x89\x80\xe6\x9c\x89\xe4\xb9\xa6\xe7\x9a\x84\xe5\xb9\xb3\xe5\x9d\x87\xe6\x95\xb0\xe5\x92\x8c\xe6\xa0\x87\xe5\x87\x86\xe5\xb7\xae\xef\xbc\x8c\xe6\x9d\xa5\xe7\xbb\x9f\xe8\xae\xa1\xe4\xb8\x80\xe4\xb8\xaaepoch\xe4\xb8\xad\xe6\x8d\x9f\xe5\xa4\xb1\xe7\x9a\x84\xe5\xb9\xb3\xe5\x9d\x87\xe5\x80\xbc\n    loss_meter=meter.AverageValueMeter()\n    # \xe5\xae\x9a\xe4\xb9\x89\xe5\x88\x9d\xe8\xaf\x95\xe7\x9a\x84loss\n    previous_loss = 1e100\n    for epoch in range(opt.max_epoch):\n        #\xe6\xb8\x85\xe7\xa9\xba\xe4\xbb\xaa\xe8\xa1\xa8\xe4\xbf\xa1\xe6\x81\xaf\n        loss_meter.reset()\n        # \xe8\xbf\xad\xe4\xbb\xa3\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe5\x8a\xa0\xe8\xbd\xbd\xe5\x99\xa8\n        for ii, (data_origin, data_grayscale) in enumerate(train_dataloader):\n            #\xe8\xae\xad\xe7\xbb\x83\xe6\xa8\xa1\xe5\x9e\x8b\n            #input_img\xe4\xb8\xba\xe6\xa8\xa1\xe5\x9e\x8b\xe8\xbe\x93\xe5\x85\xa5\xe5\x9b\xbe\xe5\x83\x8f\xef\xbc\x8c\xe4\xb8\xba\xe7\x81\xb0\xe5\xba\xa6\xe5\x8a\xa0\xe5\x99\xaa\xe5\x9b\xbe\n            input_img=Variable(data_grayscale)\n            #output_real_img \xe4\xb8\xbatarget\xe5\x9b\xbe\xe5\x83\x8f\xef\xbc\x8c\xe5\x8d\xb3\xe4\xb8\xba \xe5\x8e\x9f\xe5\x9b\xbe\xe7\x9a\x84\xe7\x81\xb0\xe5\xba\xa6\xe5\x9b\xbe\n            output_real_img=Variable(data_origin)\n            #\xe5\xb0\x86\xe6\x95\xb0\xe6\x8d\xae\xe8\xbd\xac\xe5\x88\xb0GPU\n            if opt.use_gpu:\n                input_img=input_img.cuda(1)\n                output_real_img=output_real_img.cuda(1)\n            #\xe4\xbc\x98\xe5\x8c\x96\xe5\x99\xa8\xe6\xa2\xaf\xe5\xba\xa6\xe6\xb8\x85\xe9\x9b\xb6\n            optimizer.zero_grad()\n            #\xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xef\xbc\x8c\xe5\xbe\x97\xe5\x88\xb0\xe7\xbd\x91\xe7\xbb\x9c\xe4\xba\xa7\xe7\x94\x9f\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe5\x9b\xbe\xe5\x83\x8foutput_img\n            output_img=netWork(input_img)\n            # \xe6\x8d\x9f\xe5\xa4\xb1\xe4\xb8\xbaMSE\xe5\x9d\x87\xe6\x96\xb9\xe8\xaf\xaf\xe5\xb7\xae\n            loss=criterion(output_img,output_real_img)\n            # \xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad  \xe8\x87\xaa\xe5\x8a\xa8\xe6\xb1\x82\xe6\xa2\xaf\xe5\xba\xa6         loss\xe8\xbf\x9b\xe8\xa1\x8c\xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\n            loss.backward()\n            # \xe6\x9b\xb4\xe6\x96\xb0\xe4\xbc\x98\xe5\x8c\x96\xe5\x99\xa8\xe7\x9a\x84\xe5\x8f\xaf\xe5\xad\xa6\xe4\xb9\xa0\xe5\x8f\x82\xe6\x95\xb0       optimizer\xe4\xbc\x98\xe5\x8c\x96\xe5\x99\xa8\xe8\xbf\x9b\xe8\xa1\x8c\xe6\x9b\xb4\xe6\x96\xb0\xe5\x8f\x82\xe6\x95\xb0\n            optimizer.step()\n            # \xe6\x9b\xb4\xe6\x96\xb0\xe4\xbb\xaa\xe8\xa1\xa8 \xe5\xb9\xb6\xe5\x8f\xaf\xe8\xa7\x86\xe5\x8c\x96\n            loss_meter.add(loss.data[0])\n            # \xe6\xaf\x8fprint_freq\xe6\xac\xa1\xe5\x8f\xaf\xe8\xa7\x86\xe5\x8c\x96loss\n            if ii % opt.print_freq == opt.print_freq - 1:\n                # plot\xe6\x98\xaf\xe8\x87\xaa\xe5\xae\x9a\xe4\xb9\x89\xe7\x9a\x84\xe6\x96\xb9\xe6\xb3\x95\n                vis.plot(\'loss\', loss_meter.value()[0])\n        # \xe4\xb8\x80\xe4\xb8\xaaepoch\xe4\xb9\x8b\xe5\x90\x8e\xe4\xbf\x9d\xe5\xad\x98\xe6\xa8\xa1\xe5\x9e\x8b\n        netWork.save()\n        # \xe5\x88\xa9\xe7\x94\xa8lena.jpg\xe6\xb5\x8b\xe8\xaf\x95\xe6\xaf\x8f\xe4\xb8\xaaepoch\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\n        add_every_epoch_lena(netWork,epoch)\n        # # \xe4\xbd\xbf\xe7\x94\xa8\xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86\xe5\x92\x8c\xe5\x8f\xaf\xe8\xa7\x86\xe5\x8c\x96\n        # val_output_img= val(netWork, val_dataloader)\n        # vis.img(""val_output_img"",val_output_img.data)\n        \n        # \xe5\xbd\x93\xe5\x89\x8d\xe6\x97\xb6\xe5\x88\xbb\xe7\x9a\x84\xe4\xb8\x80\xe4\xba\x9b\xe4\xbf\xa1\xe6\x81\xaf\n        vis.log(""epoch:{epoch},lr:{lr},loss:{loss}"".format(\n            epoch=epoch, loss=loss_meter.value()[0],lr=lr))\n        # \xe6\x9b\xb4\xe6\x96\xb0\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87  \xe5\xa6\x82\xe6\x9e\x9c\xe6\x8d\x9f\xe5\xa4\xb1\xe5\xbc\x80\xe5\xa7\x8b\xe5\x8d\x87\xe9\xab\x98\xef\xbc\x8c\xe5\x88\x99\xe9\x99\x8d\xe4\xbd\x8e\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\n        if loss_meter.value()[0] > previous_loss:\n            lr=lr*opt.lr_decay\n            # \xe7\xac\xac\xe4\xba\x8c\xe7\xa7\x8d\xe9\x99\x8d\xe4\xbd\x8e\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\xe7\x9a\x84\xe6\x96\xb9\xe6\xb3\x95:\xe4\xb8\x8d\xe4\xbc\x9a\xe6\x9c\x89moment\xe7\xad\x89\xe4\xbf\xa1\xe6\x81\xaf\xe7\x9a\x84\xe4\xb8\xa2\xe5\xa4\xb1\n            for param_group in optimizer.param_groups:\n                param_group[\'lr\'] = lr\n        previous_loss = loss_meter.value()[0]\n        vis.img_many(dict_lena)\n    print(""============\xe8\xae\xad\xe7\xbb\x83\xe5\xae\x8c\xe6\xaf\x95============="")\n\n\n\n\n\ndef val(model,dataloader):\n    \'\'\'\n    \xe8\xae\xa1\xe7\xae\x97\xe6\xa8\xa1\xe5\x9e\x8b\xe5\x9c\xa8\xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86\xe4\xb8\x8a\xe7\x9a\x84\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87\xe7\xad\x89\xe4\xbf\xa1\xe6\x81\xaf\n    \'\'\'\n\n    # \xe5\xb0\x86\xe6\xa8\xa1\xe5\x9e\x8b\xe8\xb0\x83\xe6\x95\xb4\xe4\xb8\xba\xe9\xaa\x8c\xe8\xaf\x81\xe6\xa8\xa1\xe5\xbc\x8f\n    model.eval()\n\n    for ii, data in tqdm(enumerate(dataloader)):\n        data_origin, data_grayscale = data\n        #\xe8\xae\xbe\xe7\xbd\xae\xe4\xb8\xba\xe9\xaa\x8c\xe8\xaf\x81\xe6\xa8\xa1\xe5\xbc\x8f  pytorch\xe4\xb8\x8d\xe4\xbc\x9a\xe8\xae\xb0\xe5\xbd\x95\xe4\xb8\xad\xe9\x97\xb4\xe5\x8f\x98\xe9\x87\x8f\xef\xbc\x8c\xe8\x8a\x82\xe7\x9c\x81\xe5\x86\x85\xe5\xad\x98\xef\xbc\x8c\xe5\x8f\xaf\xe5\x8a\xa0\xe9\x80\x9f\xe8\xbf\x90\xe7\xae\x97\n        val_input_data_grayscale=Variable(data_grayscale,volatile=True)\n        val_lable_data_origin=Variable(data_grayscale,volatile=True)\n        if opt.use_gpu:\n            val_input_data_grayscale = val_input_data_grayscale.cuda(1)\n            val_lable_data_origin = val_lable_data_origin.cuda(1)\n        val_output_img = model(val_input_data_grayscale)\n\n    # \xe5\xb0\x86\xe6\xa8\xa1\xe5\x9e\x8b\xe8\xb0\x83\xe6\x95\xb4\xe4\xb8\xba\xe8\xae\xad\xe7\xbb\x83\xe6\xa8\xa1\xe5\xbc\x8f\n    model.train()\n    return val_output_img\n\n\ndef add_every_epoch_lena(model,epoch):\n    \'\'\'\n     \xe4\xbf\x9d\xe5\xad\x98\xe6\xb5\x8b\xe8\xaf\x95\xe5\x9b\xbe\xe5\x83\x8flena\xe5\x9c\xa8\xe6\xaf\x8f\xe4\xb8\xaaepoch\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\xe4\xba\xa7\xe7\x94\x9f\xe7\x9a\x84\xe5\x8e\xbb\xe5\x99\xaa\xe5\x9b\xbe\xe5\x83\x8f\xe4\xbb\xa5\xe5\xaf\xb9\xe6\xaf\x94\xe6\x95\x88\xe6\x9e\x9c\n    \'\'\'\n\n    # \xe5\xb0\x86\xe6\xa8\xa1\xe5\x9e\x8b\xe8\xb0\x83\xe6\x95\xb4\xe4\xb8\xba\xe9\xaa\x8c\xe8\xaf\x81\xe6\xa8\xa1\xe5\xbc\x8f\n    model.eval()\n\n    input_img,traget_img=DataProcessing(opt.data_root, train=True).get_lena_imgs()\n    #\xe8\xae\xbe\xe7\xbd\xae\xe4\xb8\xba\xe9\xaa\x8c\xe8\xaf\x81\xe6\xa8\xa1\xe5\xbc\x8f  pytorch\xe4\xb8\x8d\xe4\xbc\x9a\xe8\xae\xb0\xe5\xbd\x95\xe4\xb8\xad\xe9\x97\xb4\xe5\x8f\x98\xe9\x87\x8f\xef\xbc\x8c\xe8\x8a\x82\xe7\x9c\x81\xe5\x86\x85\xe5\xad\x98\xef\xbc\x8c\xe5\x8f\xaf\xe5\x8a\xa0\xe9\x80\x9f\xe8\xbf\x90\xe7\xae\x97\n    input_img = Variable(input_img, volatile=True)\n    traget_img = Variable(traget_img, volatile=True)\n    if opt.use_gpu:\n        input_img = input_img.cuda(1)\n        traget_img = traget_img.cuda(1)\n    input_img=input_img.unsqueeze(0)\n    output_img = model(input_img)\n    # \xe5\xb0\x86\xe6\xa8\xa1\xe5\x9e\x8b\xe8\xb0\x83\xe6\x95\xb4\xe4\xb8\xba\xe8\xae\xad\xe7\xbb\x83\xe6\xa8\xa1\xe5\xbc\x8f\n    model.train()\n    \n    dict_lena[""\xe7\xac\xac""+str(epoch)+""\xe6\xac\xa1\xe7\x94\x9f\xe6\x88\x90\xe7\x9a\x84\xe5\x9b\xbe\xe5\x83\x8f""]=output_img.data\n\nif __name__ == \'__main__\':\n    train()\n\n'"
Noise2noise_pytorch/main.py,0,"b""import os\nfrom utils.config import opt_train,opt_test\nfrom data.datasets import load_dataset\nfrom noise2noise import Noise2Noise\ndef train():\n\n    # \xe5\x8a\xa0\xe8\xbd\xbd\xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\xe5\x92\x8c\xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86\n    train_loader = load_dataset(opt_train.train_dir, opt_train.train_size, opt_train, shuffled=True)\n    valid_loader = load_dataset(opt_train.valid_dir, opt_train.valid_size, opt_train, shuffled=False)\n\n    # \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe6\xa8\xa1\xe5\x9e\x8b\xe5\xb9\xb6\xe8\xae\xad\xe7\xbb\x83\n    n2n = Noise2Noise(opt_train, trainable=True)\n    n2n.train(train_loader, valid_loader)\n\ndef test():\n    # \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe6\xa8\xa1\xe5\x9e\x8b\xef\xbc\x8c\xe8\xbf\x9b\xe8\xa1\x8c\xe6\xb5\x8b\xe8\xaf\x95\n    n2n = Noise2Noise(opt_test, trainable=False)\n    opt_test.redux = False\n    test_loader = load_dataset(opt_test.data,3, opt_test, shuffled=False, single=True)  #\xe4\xbf\xae\xe6\x94\xb90\n    n2n.load_model(opt_test.load_ckpt) #\xe5\x8a\xa0\xe8\xbd\xbd\xe9\xa2\x84\xe8\xae\xad\xe7\xbb\x83\xe6\xa8\xa1\xe5\x9e\x8b\n    n2n.test(test_loader)\n\n\n\nif __name__ == '__main__':\n    os.environ['CUDA_VISIBLE_DEVICES'] = '1'  # \xe9\x80\x89\xe6\x8b\xa9\xe5\x93\xaa\xe5\x9d\x97GPU\xe8\xbf\x90\xe8\xa1\x8c '0' or '1' or '0,1'\n\n    #\xe8\xae\xad\xe7\xbb\x83\n    # train()\n\n    # \xe6\xb5\x8b\xe8\xaf\x95\xe5\x8d\x95\xe5\xbc\xa0\xe5\x9b\xbe\xe7\x89\x87\xef\xbc\x8c\xe5\xb0\x86\xe7\xbb\x93\xe6\x9e\x9c\xe4\xbf\x9d\xe5\xad\x98\xe5\x88\xb0\xe6\x96\x87\xe4\xbb\xb6\xe5\xa4\xb9\xe4\xb8\x8b\n    test()\n"""
Noise2noise_pytorch/noise2noise.py,7,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\nimport torch\nimport torch.nn as nn\nfrom torch.optim import Adam, lr_scheduler\n\nfrom models.unet import UNet\n\nfrom utils.utils import *\nimport os\nimport json\n\n\nclass Noise2Noise(object):\n    """"""Implementation of Noise2Noise from Lehtinen et al. (2018).""""""\n\n    def __init__(self, params, trainable):\n        """"""Initializes model.""""""\n\n        self.p = params\n        self.trainable = trainable\n        self._compile()  #\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe6\xa8\xa1\xe5\x9e\x8b\n\n\n    def _compile(self):\n        """"""\n        Compiles model (architecture, loss function, optimizers, etc.).\n        \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96 \xe7\xbd\x91\xe7\xbb\x9c\xe3\x80\x81\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x87\xbd\xe6\x95\xb0\xe3\x80\x81\xe4\xbc\x98\xe5\x8c\x96\xe5\x99\xa8\xe7\xad\x89\n        """"""\n\n        print(\'Noise2Noise: Learning Image Restoration without Clean Data (Lethinen et al., 2018)\')\n\n        # Model (3x3=9 channels for Monte Carlo since it uses 3 HDR buffers)  \xe5\xb7\xb2\xe5\x88\xa0\xe9\x99\xa4\xe8\x92\x99\xe7\x89\xb9\xe5\x8d\xa1\xe6\xb4\x9b\xe7\x9b\xb8\xe5\x85\xb3\xe4\xbb\xa3\xe7\xa0\x81\n        if self.p.noise_type == \'mc\':\n            self.is_mc = True\n            self.model = UNet(in_channels=9)\n        else:\n            self.is_mc = False\n            self.model = UNet(in_channels=3)\n\n        # Set optimizer and loss, if in training mode\n        # \xe5\xa6\x82\xe6\x9e\x9c \xe4\xb8\xba\xe8\xae\xad\xe7\xbb\x83\xef\xbc\x8c\xe5\x88\x99\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe4\xbc\x98\xe5\x8c\x96\xe5\x99\xa8\xe5\x92\x8c\xe6\x8d\x9f\xe5\xa4\xb1\n        if self.trainable:\n            self.optim = Adam(self.model.parameters(),\n                              lr=self.p.learning_rate,\n                              betas=self.p.adam[:2],\n                              eps=self.p.adam[2])\n\n            # Learning rate adjustment\n            self.scheduler = lr_scheduler.ReduceLROnPlateau(self.optim,\n                patience=self.p.nb_epochs/4, factor=0.5, verbose=True)\n\n            # Loss function\n            if self.p.loss == \'hdr\':\n                assert self.is_mc, \'Using HDR loss on non Monte Carlo images\'\n                self.loss = HDRLoss()\n            elif self.p.loss == \'l2\':\n                self.loss = nn.MSELoss()\n            else:\n                self.loss = nn.L1Loss()\n\n        # CUDA support\n        self.use_cuda = torch.cuda.is_available() and self.p.cuda\n        if self.use_cuda:\n            self.model = self.model.cuda()\n            if self.trainable:\n                self.loss = self.loss.cuda()\n\n\n    def _print_params(self):\n        """"""Formats parameters to print when training.""""""\n\n        print(\'Training parameters: \')\n        self.p.cuda = self.use_cuda\n        param_dict = vars(self.p)\n        pretty = lambda x: x.replace(\'_\', \' \').capitalize()\n        print(\'\\n\'.join(\'  {} = {}\'.format(pretty(k), str(v)) for k, v in param_dict.items()))\n        print()\n\n\n    def save_model(self, epoch, stats, first=False):\n        """"""Saves model to files; can be overwritten at every epoch to save disk space.""""""\n\n        # Create directory for model checkpoints, if nonexistent\n        if first:\n            if self.p.clean_targets:\n                ckpt_dir_name = f\'{datetime.now():{self.p.noise_type}-clean-%H%M}\'\n            else:\n                ckpt_dir_name = f\'{datetime.now():{self.p.noise_type}-%H%M}\'\n            if self.p.ckpt_overwrite:\n                if self.p.clean_targets:\n                    ckpt_dir_name = f\'{self.p.noise_type}-clean\'\n                else:\n                    ckpt_dir_name = self.p.noise_type\n\n            self.ckpt_dir = os.path.join(self.p.ckpt_save_path, ckpt_dir_name)\n            if not os.path.isdir(self.p.ckpt_save_path):\n                os.mkdir(self.p.ckpt_save_path)\n            if not os.path.isdir(self.ckpt_dir):\n                os.mkdir(self.ckpt_dir)\n\n        # Save checkpoint dictionary\n        if self.p.ckpt_overwrite:\n            fname_unet = \'{}/n2n-{}.pt\'.format(self.ckpt_dir, self.p.noise_type)\n        else:\n            valid_loss = stats[\'valid_loss\'][epoch]\n            fname_unet = \'{}/n2n-epoch{}-{:>1.5f}.pt\'.format(self.ckpt_dir, epoch + 1, valid_loss)\n        print(\'Saving checkpoint to: {}\\n\'.format(fname_unet))\n        torch.save(self.model.state_dict(), fname_unet)\n\n        # Save stats to JSON\n        fname_dict = \'{}/n2n-stats.json\'.format(self.ckpt_dir)\n        with open(fname_dict, \'w\') as fp:\n            json.dump(stats, fp, indent=2)\n\n\n    def load_model(self, ckpt_fname):\n        """"""Loads model from checkpoint file.""""""\n\n        print(\'Loading checkpoint from: {}\'.format(ckpt_fname))\n        if self.use_cuda:\n            self.model.load_state_dict(torch.load(ckpt_fname))\n        else:\n            self.model.load_state_dict(torch.load(ckpt_fname, map_location=\'cpu\'))\n\n\n    def _on_epoch_end(self, stats, train_loss, epoch, epoch_start, valid_loader):\n        """"""Tracks and saves starts after each epoch.""""""\n\n        # Evaluate model on validation set\n        print(\'\\rTesting model on validation set... \', end=\'\')\n        epoch_time = time_elapsed_since(epoch_start)[0]\n        valid_loss, valid_time, valid_psnr = self.eval(valid_loader)\n        show_on_epoch_end(epoch_time, valid_time, valid_loss, valid_psnr)\n\n        # Decrease learning rate if plateau\n        self.scheduler.step(valid_loss)\n\n        # Save checkpoint\n        stats[\'train_loss\'].append(train_loss)\n        stats[\'valid_loss\'].append(valid_loss)\n        stats[\'valid_psnr\'].append(valid_psnr)\n        self.save_model(epoch, stats, epoch == 0)\n\n\n\n\n    def test(self, test_loader, show=1):\n        """"""Evaluates denoiser on test set.""""""\n\n        self.model.train(False)\n\n        source_imgs = []\n        denoised_imgs = []\n        clean_imgs = []\n\n        # Create directory for denoised images\n        denoised_dir = os.path.dirname(self.p.data)\n        save_path = os.path.join(denoised_dir, \'denoised\')\n        if not os.path.isdir(save_path):\n            os.mkdir(save_path)\n\n        for batch_idx, (source, target) in enumerate(test_loader):\n            # Only do first <show> images\n            if show == 0 or batch_idx >= show:\n                break\n\n            source_imgs.append(source)\n            clean_imgs.append(target)\n\n            if self.use_cuda:\n                source = source.cuda()\n\n            # Denoise\n            denoised_img = self.model(source).detach()\n            denoised_imgs.append(denoised_img)\n\n        # Squeeze tensors\n        source_imgs = [t.squeeze(0) for t in source_imgs]\n        denoised_imgs = [t.squeeze(0) for t in denoised_imgs]\n        clean_imgs = [t.squeeze(0) for t in clean_imgs]\n\n        # Create montage and save images\n        print(\'Saving images and montages to: {}\'.format(save_path))\n        for i in range(len(source_imgs)):\n            img_name = test_loader.dataset.imgs[i]\n            create_montage(img_name, self.p.noise_type, save_path, source_imgs[i], denoised_imgs[i], clean_imgs[i], show)\n\n\n    def eval(self, valid_loader):\n        """"""Evaluates denoiser on validation set.""""""\n\n        self.model.train(False)\n\n        valid_start = datetime.now()\n        loss_meter = AvgMeter()\n        psnr_meter = AvgMeter()\n\n        for batch_idx, (source, target) in enumerate(valid_loader):\n            if self.use_cuda:\n                source = source.cuda()\n                target = target.cuda()\n\n            # Denoise\n            source_denoised = self.model(source)\n\n            # Update loss\n            loss = self.loss(source_denoised, target)\n            loss_meter.update(loss.item())\n\n            # Compute PSRN\n            if self.is_mc:\n                source_denoised = reinhard_tonemap(source_denoised)\n            # TODO: Find a way to offload to GPU, and deal with uneven batch sizes\n            for i in range(self.p.batch_size):\n                source_denoised = source_denoised.cpu()\n                target = target.cpu()\n                psnr_meter.update(psnr(source_denoised[i], target[i]).item())\n\n        valid_loss = loss_meter.avg\n        valid_time = time_elapsed_since(valid_start)[0]\n        psnr_avg = psnr_meter.avg\n\n        return valid_loss, valid_time, psnr_avg\n\n\n    def train(self, train_loader, valid_loader):\n        """"""Trains denoiser on training set.""""""\n\n        self.model.train(True)\n\n        self._print_params()\n        num_batches = len(train_loader)\n        assert num_batches % self.p.report_interval == 0, \'Report interval must divide total number of batches\'\n\n        # Dictionaries of tracked stats\n        stats = {\'noise_type\': self.p.noise_type,\n                 \'noise_param\': self.p.noise_param,\n                 \'train_loss\': [],\n                 \'valid_loss\': [],\n                 \'valid_psnr\': []}\n\n        # Main training loop\n        train_start = datetime.now()\n        for epoch in range(self.p.nb_epochs):\n            print(\'EPOCH {:d} / {:d}\'.format(epoch + 1, self.p.nb_epochs))\n\n            # Some stats trackers\n            epoch_start = datetime.now()\n            train_loss_meter = AvgMeter()\n            loss_meter = AvgMeter()\n            time_meter = AvgMeter()\n\n            # Minibatch SGD\n            for batch_idx, (source, target) in enumerate(train_loader):\n                batch_start = datetime.now()\n                progress_bar(batch_idx, num_batches, self.p.report_interval, loss_meter.val)\n\n                if self.use_cuda:\n                    source = source.cuda()\n                    target = target.cuda()\n\n                # Denoise image\n                source_denoised = self.model(source)\n\n                loss = self.loss(source_denoised, target)\n                loss_meter.update(loss.item())\n\n                # Zero gradients, perform a backward pass, and update the weights\n                self.optim.zero_grad()\n                loss.backward()\n                self.optim.step()\n\n                # Report/update statistics\n                time_meter.update(time_elapsed_since(batch_start)[1])\n                if (batch_idx + 1) % self.p.report_interval == 0 and batch_idx:\n                    show_on_report(batch_idx, num_batches, loss_meter.avg, time_meter.avg)\n                    train_loss_meter.update(loss_meter.avg)\n                    loss_meter.reset()\n                    time_meter.reset()\n\n            # Epoch end, save and reset tracker\n            self._on_epoch_end(stats, train_loss_meter.avg, epoch, epoch_start, valid_loader)\n            train_loss_meter.reset()\n\n        train_elapsed = time_elapsed_since(train_start)[0]\n        print(\'Training done! Total elapsed time: {}\\n\'.format(train_elapsed))\n\n\nclass HDRLoss(nn.Module):\n    """"""High dynamic range loss.""""""\n\n    def __init__(self, eps=0.01):\n        """"""Initializes loss with numerical stability epsilon.""""""\n\n        super(HDRLoss, self).__init__()\n        self._eps = eps\n\n\n    def forward(self, denoised, target):\n        """"""Computes loss by unpacking render buffer.""""""\n\n        loss = ((denoised - target) ** 2) / (denoised + self._eps) ** 2\n        return torch.mean(loss.view(-1))\n'"
SSD_pytorch/main.py,30,"b'from SSD_pytorch.data import *\nfrom SSD_pytorch.utils.augmentations import SSDAugmentation\nfrom SSD_pytorch.models.modules import MultiBoxLoss\nfrom SSD_pytorch.models.ssd import build_ssd\nfrom SSD_pytorch.models.modules.init_weights import weights_init\nfrom SSD_pytorch.data import VOC_CLASSES as VOC_CLASSES\nimport torch\nfrom SSD_pytorch.utils.config import opt\nfrom torch.autograd import Variable\nimport torch.optim as optim\nimport torch.backends.cudnn as cudnn\nimport torch.utils.data as data\nfrom SSD_pytorch.utils.visualize import Visualizer\nfrom SSD_pytorch.utils.timer import Timer\nfrom SSD_pytorch.utils.eval_untils import evaluate_detections\nimport os\nimport time\nimport sys\nimport pickle\nfrom matplotlib import pyplot as plt\nif sys.version_info[0] == 2:\n    import xml.etree.cElementTree as ET\nelse:\n    import xml.etree.ElementTree as ET\n\n\n\n#\xe8\xae\xbe\xe7\xbd\xae\xe5\x88\x9b\xe5\xbb\xbatensor\xe7\x9a\x84\xe9\xbb\x98\xe8\xae\xa4\xe7\xb1\xbb\xe5\x9e\x8b\nif opt.use_gpu:\n    torch.set_default_tensor_type(\'torch.cuda.FloatTensor\')\nelse:\n    torch.set_default_tensor_type(\'torch.FloatTensor\')\n\ndef train():\n\n    #\xe8\xbf\x94\xe5\x9b\x9e\xe4\xb8\x80\xe4\xb8\xaabatch\xe7\x9a\x84\xe4\xbf\xa1\xe6\x81\xaf\xef\xbc\x8c\xe5\x85\xb6\xe4\xb8\xad\xe6\xaf\x8f\xe8\xa1\x8c\xe4\xbb\xa3\xe8\xa1\xa8\xe4\xb8\x80\xe5\xbc\xa0\xe5\x9b\xbe\xe5\x83\x8f \xe5\x8f\x8a \xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84\xe7\x9c\x9f\xe5\x80\xbc\xe6\xa1\x86\xe5\x92\x8c\xe7\xb1\xbb\xe5\x88\xab\n    dataset = VOCDetection(root=opt.voc_data_root,\n                           transform=SSDAugmentation(opt.voc[\'min_dim\'],\n                                                     opt.MEANS))\n    # \xe5\xae\x9a\xe4\xb9\x89\xe5\x8f\xaf\xe8\xa7\x86\xe5\x8c\x96\xe5\xaf\xb9\xe8\xb1\xa1\n    vis = Visualizer(opt.env + opt.model)\n    # \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96ssd\xe6\xa8\xa1\xe5\x9e\x8b\n    #  opt.voc[\'min_dim\']\xef\xbc\x9a300\xc3\x97300\xe5\x9b\xbe\xe5\x83\x8f\n    # opt.voc[\'num_classes\']  21 \xe7\xb1\xbb\xe5\x88\xab\xe6\x95\xb0\xef\xbc\x8820\xe7\xb1\xbb+1\xe8\x83\x8c\xe6\x99\xaf\xef\xbc\x89\n    ssd_net = build_ssd(\'train\', opt.voc[\'min_dim\'], opt.voc[\'num_classes\'])\n    # \xe6\x95\xb0\xe6\x8d\xae\xe5\xb9\xb6\xe8\xa1\x8c\xe6\x98\xaf\xe5\xbd\x93\xe6\x88\x91\xe4\xbb\xac\xe5\xb0\x86\xe5\xb0\x8f\xe6\x89\xb9\xe9\x87\x8f\xe6\xa0\xb7\xe5\x93\x81\xe5\x88\x86\xe6\x88\x90\xe5\xa4\x9a\xe4\xb8\xaa\xe8\xbe\x83\xe5\xb0\x8f\xe7\x9a\x84\xe6\x89\xb9\xe9\x87\x8f\xe6\x89\xb9\xe6\xac\xa1\xef\xbc\x8c\xe5\xb9\xb6\xe4\xb8\x94\xe5\xaf\xb9\xe6\xaf\x8f\xe4\xb8\xaa\xe8\xbe\x83\xe5\xb0\x8f\xe7\x9a\x84\xe5\xb0\x8f\xe6\x89\xb9\xe9\x87\x8f\xe5\xb9\xb6\xe8\xa1\x8c\xe8\xbf\x90\xe8\xa1\x8c\xe8\xae\xa1\xe7\xae\x97\xe3\x80\x82\n    if opt.use_gpu:\n        # GPU\xe5\xb9\xb6\xe8\xa1\x8c\n        net = torch.nn.DataParallel(ssd_net)\n        net = net.cuda()\n        cudnn.benchmark = True\n    # \xe5\x8a\xa0\xe8\xbd\xbd\xe9\xa2\x84\xe8\xae\xad\xe7\xbb\x83\xe5\xa5\xbd\xe7\x9a\x84\xe7\x9a\x84SSD\xe6\xa8\xa1\xe5\x9e\x8b\n    if opt.load_model_path:\n        print(\'\xe5\x8a\xa0\xe8\xbd\xbd\xe5\xb7\xb2\xe8\xae\xad\xe7\xbb\x83\xe5\xa5\xbd\xe7\x9a\x84SSD\xe6\xa8\xa1\xe5\x9e\x8b\')\n        ssd_net.load_state_dict(torch.load(opt.load_model_path,map_location=lambda storage, loc: storage))\n        print(\'\xe5\x8a\xa0\xe8\xbd\xbd\xe6\x9d\x83\xe9\x87\x8d\xe5\xae\x8c\xe6\x88\x90!\')\n    else:\n        # \xe4\xbb\x8e\xe5\xa4\xb4\xe5\xbc\x80\xe5\xa7\x8b\xe8\xae\xad\xe7\xbb\x83\n        # \xe5\x85\x88\xe5\x8a\xa0\xe8\xbd\xbd\xe5\x8e\xbb\xe6\x8e\x89\xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe5\xb1\x82\xe7\x9a\x84vgg\xe7\xbd\x91\xe7\xbb\x9c\xe3\x80\x82\xe6\x9d\x83\xe9\x87\x8d\xe4\xb8\xba\xe9\xa2\x84\xe8\xae\xad\xe7\xbb\x83\xe5\xa5\xbd\xe7\x9a\x84 \xe5\x8e\xbb\xe6\x8e\x89\xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe5\xb1\x82\xe7\x9a\x84vgg\xe7\xbd\x91\xe7\xbb\x9c\xe6\x9d\x83\xe9\x87\x8d\n        vgg_weights = torch.load(opt.basenet)\n        print(\'\xe6\xad\xa3\xe5\x9c\xa8\xe5\x8a\xa0\xe8\xbd\xbdvgg\xe5\x9f\xba\xe7\xa1\x80\xe7\xbd\x91\xe7\xbb\x9c...\')\n        ssd_net.vgg.load_state_dict(vgg_weights)\n\n        print(\'\xe4\xbb\x8e\xe5\xa4\xb4\xe5\xbc\x80\xe5\xa7\x8b\xe8\xae\xad\xe7\xbb\x83\xef\xbc\x8c\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe9\x99\xa4vgg\xe4\xb9\x8b\xe5\xa4\x96\xe7\x9a\x84\xe7\xbd\x91\xe7\xbb\x9c\xe5\xb1\x82\xe6\x9d\x83\xe9\x87\x8d...\')\n        # \xe4\xbd\xbf\xe7\x94\xa8xavier\xe6\x96\xb9\xe6\xb3\x95\xe6\x9d\xa5\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96vgg\xe5\x90\x8e\xe9\x9d\xa2\xe7\x9a\x84\xe6\x96\xb0\xe5\xa2\x9e\xe5\xb1\x82\xe3\x80\x81loc\xe7\x94\xa8\xe4\xba\x8e\xe5\x9b\x9e\xe5\xbd\x92\xe5\xb1\x82\xe3\x80\x81conf\xe7\x94\xa8\xe4\xba\x8e\xe5\x88\x86\xe7\xb1\xbb\xe5\xb1\x82  \xe7\x9a\x84\xe6\x9d\x83\xe9\x87\x8d\n        ssd_net.extras.apply(weights_init)\n        ssd_net.loc.apply(weights_init)\n        ssd_net.conf.apply(weights_init)\n\n    # \xe4\xbd\xbf\xe7\x94\xa8\xe9\x9a\x8f\xe6\x9c\xba\xe6\xa2\xaf\xe5\xba\xa6\xe4\xb8\x8b\xe9\x99\x8dSGD\n    optimizer = optim.SGD(net.parameters(), lr=opt.lr, momentum=opt.momentum,\n                          weight_decay=opt.weight_decay)\n    # SSD\xe7\x9a\x84\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x87\xbd\xe6\x95\xb0\n    criterion = MultiBoxLoss(opt.voc[\'num_classes\'], 0.5, True, 0, True, 3, 0.5,\n                             False, opt.use_gpu)\n    # \xe5\xb0\x86\xe7\xbd\x91\xe7\xbb\x9c\xe7\xbd\xae\xe4\xb8\xba\xe8\xae\xad\xe7\xbb\x83\xe6\xa8\xa1\xe5\xbc\x8f\n    net.train()\n    # loss counters\n    loc_loss = 0\n    conf_loss = 0\n    epoch = 0\n    print(\'\xe5\x8a\xa0\xe8\xbd\xbd\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86...\')\n    # \xe4\xb8\x80\xe4\xb8\xaaepoch\xe9\x9c\x80\xe8\xa6\x81\xe5\x87\xa0\xe6\xac\xa1batch  //\xe9\x99\xa4\xe6\xb3\x95\xe6\x93\x8d\xe4\xbd\x9c\xef\xbc\x88\xe5\x90\x91\xe4\xb8\x8b\xe5\x8f\x96\xe6\x95\xb4\xef\xbc\x89\n    epoch_size = len(dataset) // opt.batch_size\n\n\n    step_index = 0\n    # \xe8\xae\xbe\xe7\xbd\xae\xe5\x8f\xaf\xe8\xa7\x86\xe5\x8c\x96\n    if opt.visdom:\n        # \xe6\xa0\x87\xe9\xa2\x98\n        vis_title = \'SSD.PyTorch on \' + dataset.name\n        # \xe8\xaf\xb4\xe6\x98\x8e\n        vis_legend = [\'Loc Loss\', \'Conf Loss\', \'Total Loss\']\n        # \xe6\xaf\x8f\xe6\xac\xa1\xe8\xbf\xad\xe4\xbb\xa3\xe7\x9a\x84\xe5\x9b\xbe\xe5\xbd\xa2\n        iter_plot = vis.create_vis_plot(\'Iteration\', \'Loss\', vis_title, vis_legend)\n        # \xe6\xaf\x8f\xe4\xb8\xaaepoch\xe7\x9a\x84\xe5\x9b\xbe\xe5\xbd\xa2\n        epoch_plot = vis.create_vis_plot(\'Epoch\', \'Loss\', vis_title, vis_legend)\n    # \xe6\x95\xb0\xe6\x8d\xae\xe5\x8a\xa0\xe8\xbd\xbd\xe5\x99\xa8\n    # collate_fn\xe5\x90\x88\xe5\xb9\xb6\xe6\xa0\xb7\xe6\x9c\xac\xe5\x88\x97\xe8\xa1\xa8\xe4\xbb\xa5\xe5\xbd\xa2\xe6\x88\x90\xe4\xb8\x80\xe4\xb8\xaa mini-batch\n    # pin_memory\xe5\xa6\x82\xe6\x9e\x9c\xe4\xb8\xbaTrue, \xe6\x95\xb0\xe6\x8d\xae\xe5\x8a\xa0\xe8\xbd\xbd\xe5\x99\xa8\xe4\xbc\x9a\xe5\xb0\x86\xe5\xbc\xa0\xe9\x87\x8f\xe5\xa4\x8d\xe5\x88\xb6\xe5\x88\xb0CUDA\xe5\x9b\xba\xe5\xae\x9a\xe5\x86\x85\xe5\xad\x98\xe4\xb8\xad, \xe7\x84\xb6\xe5\x90\x8e\xe5\x86\x8d\xe8\xbf\x94\xe5\x9b\x9e\xe5\xae\x83\xe4\xbb\xac\n    data_loader = data.DataLoader(dataset, opt.batch_size,\n                                  num_workers=opt.num_workers,\n                                  shuffle=True, collate_fn=detection_collate,\n                                  pin_memory=True)\n    # \xe5\xbc\x80\xe5\xa7\x8b\xe8\xbf\xad\xe4\xbb\xa3\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\n    batch_iterator = iter(data_loader)\n    # iteration\xe6\x95\xb0\xe4\xb8\xba\xe6\xaf\x8f\xe4\xb8\xaabatch\xe8\xbf\xad\xe4\xbb\xa3\xe6\x95\xb0\n    #start_iter\xef\xbc\x9a\xe4\xbb\x8e\xe7\xac\xac\xe5\x87\xa0\xe4\xb8\xaaiter\xe5\xbc\x80\xe5\xa7\x8b\xe8\xbf\xad\xe4\xbb\xa3   max_iter\xef\xbc\x9a\xe6\x9c\x80\xe5\xa4\xa7\xe8\xbf\xad\xe4\xbb\xa3\xe6\x95\xb0\n    for iteration in range(opt.start_iter, opt.voc[\'max_iter\']):\n        # \xe5\x8f\xaf\xe8\xa7\x86\xe5\x8c\x96\n        # epoch_size\xef\xbc\x9a\xe4\xb8\x80\xe4\xb8\xaaepoch\xe9\x9c\x80\xe8\xa6\x81\xe7\x9a\x84batch\xe6\x95\xb0\xe3\x80\x82\xe5\x8d\xb3\xe6\xaf\x8f\xe4\xb8\x80\xe4\xb8\xaaepoch\xe5\x8f\xaf\xe8\xa7\x86\xe5\x8c\x96\xe4\xb8\x80\xe4\xb8\x8b\n        if opt.visdom and iteration != 0 and (iteration % epoch_size == 0):\n            vis.update_vis_plot(epoch, loc_loss, conf_loss, epoch_plot, \'append\',\n                            \'append\', epoch_size)\n            # \xe9\x87\x8d\xe7\xbd\xaeepoch\xe6\x8d\x9f\xe5\xa4\xb1\xe8\xae\xa1\xe6\x95\xb0\xe5\x99\xa8\n            loc_loss = 0\n            conf_loss = 0\n            epoch += 1\n        # \xe5\xa6\x82\xe6\x9e\x9c\xe8\xbf\xad\xe4\xbb\xa3\xe7\x9a\x84batch\xe6\x95\xb0\xe5\x88\xb0\xe8\xbe\xbelr_steps\xe7\x9a\x84\xe5\x80\xbc\xe6\x97\xb6\xef\xbc\x8c\xe5\x88\x99\xe8\xb0\x83\xe6\x95\xb4\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\n        if iteration in opt.voc[\'lr_steps\']:\n            step_index += 1\n            lr = opt.lr * (opt.gamma ** (step_index))\n            for param_group in optimizer.param_groups:\n                param_group[\'lr\'] = lr\n\n        # \xe5\x8a\xa0\xe8\xbd\xbd\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\n        try:\n            # batch_iterator \xe8\xbf\x94\xe5\x9b\x9e\xe4\xb8\x80\xe4\xb8\xaabatch\xe7\x9a\x84\xe4\xbf\xa1\xe6\x81\xaf\xef\xbc\x8c\xe5\x85\xb6\xe4\xb8\xad\xe6\xaf\x8f\xe8\xa1\x8c\xe4\xbb\xa3\xe8\xa1\xa8\xe4\xb8\x80\xe5\xbc\xa0\xe5\x9b\xbe\xe5\x83\x8f \xe5\x8f\x8a \xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84\xe7\x9c\x9f\xe5\x80\xbc\xe6\xa1\x86\xe5\x92\x8c\xe7\xb1\xbb\xe5\x88\xab\n            # target\xef\xbc\x9a [32,num_objs,5]   32\xef\xbc\x9abatch\xe5\xa4\xa7\xe5\xb0\x8f\xef\xbc\x8c num_objs \xef\xbc\x9a\xe4\xb8\x80\xe5\xbc\xa0\xe5\x9b\xbe\xe7\x89\x87\xe7\x9a\x84\xe7\x89\xa9\xe4\xbd\x93\xe6\x95\xb0\xef\xbc\x8c5\xef\xbc\x9a\xe5\x89\x8d\xe5\x9b\x9b\xe4\xb8\xaa\xe6\x95\xb0\xe4\xb8\xba\xe5\x9d\x90\xe6\xa0\x87\xef\xbc\x8c\xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x80\xe4\xb8\xaa\xe6\x95\xb0\xe4\xb8\xba\xe7\xb1\xbb\xe5\x88\xab\n            images, targets = next(batch_iterator)\n        # \xe9\x81\x87\xe5\x88\xb0StopIteration\xef\xbc\x8c\xe5\x8d\xb3\xe8\xbf\xad\xe4\xbb\xa3\xe5\xae\x8c\xe4\xb8\x80\xe6\xac\xa1\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xef\xbc\x8c\xe9\x9c\x80\xe8\xa6\x81\xe5\x86\x8d\xe9\x87\x8d\xe5\xa4\xb4\xe5\xbc\x80\xe5\xa7\x8b\xe8\xbf\xad\xe4\xbb\xa3\n        except StopIteration:\n            data_loader = data.DataLoader(dataset, opt.batch_size,\n                                          num_workers=opt.num_workers,\n                                          shuffle=True, collate_fn=detection_collate,\n                                          pin_memory=True)\n            batch_iterator = iter(data_loader)\n            images, targets = next(batch_iterator)\n        # \xe5\xb0\x86\xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\xe8\xbd\xac\xe5\x88\xb0GPU\xe4\xb8\x8a\n        if opt.use_gpu:\n            images = Variable(images.cuda())\n            targets = [Variable(ann.cuda(), volatile=True) for ann in targets]\n        else:\n            images = Variable(images)\n            targets = [Variable(ann, volatile=True) for ann in targets]\n        # forward\xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe5\xbc\x80\xe5\xa7\x8b\n        # t0\xe3\x80\x81t1\xe7\x94\xa8\xe4\xba\x8e\xe8\xae\xa1\xe7\xae\x97\xe6\x96\xb9\xe6\xb3\x95\xe6\x89\xa7\xe8\xa1\x8c\xe6\x97\xb6\xe9\x97\xb4\n        t0 = time.time()\n        out = net(images)\n\n        # \xe4\xbc\x98\xe5\x8c\x96\xe5\x99\xa8\xe6\xa2\xaf\xe5\xba\xa6\xe6\xb8\x85\xe9\x9b\xb6\n        optimizer.zero_grad()\n        # \xe6\x8d\x9f\xe5\xa4\xb1\xe5\x87\xbd\xe6\x95\xb0\xe5\xbe\x97\xe5\x88\xb0\xe5\xae\x9a\xe4\xbd\x8d\xe8\xaf\xaf\xe5\xb7\xae\xe5\x92\x8c\xe5\x88\x86\xe7\xb1\xbb\xe8\xaf\xaf\xe5\xb7\xae\n        loss_l, loss_c = criterion(out, targets)\n        loss = loss_l + loss_c\n        # \xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\n        loss.backward()\n        # \xe6\x9b\xb4\xe6\x96\xb0\xe5\x8f\xaf\xe5\xad\xa6\xe4\xb9\xa0\xe5\x8f\x82\xe6\x95\xb0\n        optimizer.step()\n        t1 = time.time()\n\n        loc_loss += loss_l.data[0]\n        conf_loss += loss_c.data[0]\n        # \xe6\xaf\x8f10\xe4\xb8\xaabatch\xe8\xbf\xad\xe4\xbb\xa3\xe8\xbe\x93\xe5\x87\xba\xe4\xbf\xa1\xe6\x81\xaf\n        if iteration % 10 == 0:\n            print(\'\xe5\x89\x8d\xe5\x90\x91+\xe5\x8f\x8d\xe5\x90\x91\xe6\x80\xbb\xe5\x85\xb1\xe6\x89\x80\xe9\x9c\x80\xe6\x97\xb6\xe9\x97\xb4timer: %.4f sec.\' % (t1 - t0))\n            print(\'iter\xe4\xb8\xba \' + repr(iteration) + \' || \xe6\x80\xbbLoss: %.4f ||\' % (loss.data[0]), end=\' \')\n        # \xe5\x8f\xaf\xe8\xa7\x86\xe5\x8c\x96\n        if opt.visdom:\n            vis.update_vis_plot(iteration, loss_l.data[0], loss_c.data[0],\n                            iter_plot, epoch_plot, \'append\')\n        # \xe6\xaf\x8f5000\xe6\xac\xa1\xe8\xbf\xad\xe4\xbb\xa3\xe4\xbf\x9d\xe5\xad\x98\xe6\xa8\xa1\xe5\x9e\x8b\n        if iteration != 0 and iteration % 5000 == 0:\n            print(\'Saving state, iter:\', iteration)\n            ssd_net.saveSSD(str(iteration))\n\n    # \xe4\xbf\x9d\xe5\xad\x98\xe6\x9c\x80\xe5\x90\x8e\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\n    ssd_net.saveSSD()\n\ndef eval():\n    \'\'\'\n    \xe4\xbd\xbf\xe7\x94\xa8\xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86\xe8\xbf\x9b\xe8\xa1\x8c\xe9\xaa\x8c\xe8\xaf\x81VOC2007\xe6\xb5\x8b\xe8\xaf\x95\xe9\x9b\x86\xef\xbc\x8c\xe8\xae\xa1\xe7\xae\x97AP\xe5\x8f\x8amAP\n    \'\'\'\n\n    # \xe5\x8a\xa0\xe8\xbd\xbd\xe7\xbd\x91\xe7\xbb\x9c\n    num_classes = len(VOC_CLASSES) + 1  # +1 \xe4\xb8\xba\xe8\x83\x8c\xe6\x99\xaf\n    net = build_ssd(\'test\', 300, num_classes)  # \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96SSD\n    # \xe5\x8a\xa0\xe8\xbd\xbd\xe9\xa2\x84\xe8\xae\xad\xe7\xbb\x83\xe5\xa5\xbd\xe7\x9a\x84\xe7\x9a\x84SSD\xe6\xa8\xa1\xe5\x9e\x8b\n    if opt.load_model_path:\n        print(\'\xe5\x8a\xa0\xe8\xbd\xbd\xe5\xb7\xb2\xe8\xae\xad\xe7\xbb\x83\xe5\xa5\xbd\xe7\x9a\x84SSD\xe6\xa8\xa1\xe5\x9e\x8b\')\n        net.load_state_dict(torch.load(opt.load_model_path, map_location=lambda storage, loc: storage))\n        print(\'\xe5\x8a\xa0\xe8\xbd\xbd\xe6\x9d\x83\xe9\x87\x8d\xe5\xae\x8c\xe6\x88\x90!\')\n    #\xe6\xa8\xa1\xe5\x9e\x8b\xe8\xbd\xac\xe4\xb8\xba\xe9\xaa\x8c\xe8\xaf\x81\xe6\xa8\xa1\xe5\xbc\x8f\n    net.eval()\n    # \xe5\x8a\xa0\xe8\xbd\xbd\xe6\x95\xb0\xe6\x8d\xae (\xe4\xbd\xbf\xe7\x94\xa8VOC2007\xe7\x9a\x84\xe6\xb5\x8b\xe8\xaf\x95\xe9\x9b\x86\xe8\xbf\x9b\xe8\xa1\x8c\xe9\xaa\x8c\xe8\xaf\x81)\n    dataset = VOCDetection(opt.voc_data_root, [(\'2007\',  \'test\')],\n                           BaseTransform(300, opt.MEANS),\n                           VOCAnnotationTransform())\n    if opt.use_gpu:\n        net = net.cuda()\n        cudnn.benchmark = True\n    # \xe5\xbc\x80\xe5\xa7\x8b\xe9\xaa\x8c\xe8\xaf\x81\n    num_images = len(dataset)\n    # all detections are collected into:\n    #    all_boxes[cls][image] = N x 5 array of detections in\n    #    (x1, y1, x2, y2, score)\n    all_boxes = [[[] for _ in range(num_images)]\n                 for _ in range(len(VOC_CLASSES) + 1)]\n\n    # \xe8\xae\xa1\xe7\xae\x97\xe9\xa2\x84\xe6\xb5\x8b\xe6\x97\xb6\xe9\x97\xb4\n    _t = {\'im_detect\': Timer(), \'misc\': Timer()}\n    # \xe4\xbf\x9d\xe5\xad\x98\xe9\xa2\x84\xe6\xb5\x8b\xe7\xbb\x93\xe6\x9e\x9c\xe7\x9a\x84\xe4\xb8\xb4\xe6\x97\xb6\xe6\x96\x87\xe4\xbb\xb6\n    det_file = os.path.join(opt.temp, \'detections.pkl\')\n\n    for i in range(num_images):\n        im, gt, h, w = dataset.pull_item(i)\n\n        x = Variable(im.unsqueeze(0))\n        if opt.use_gpu:\n            x = x.cuda()\n        _t[\'im_detect\'].tic()\n        #\xe5\xbe\x97\xe5\x88\xb0\xe9\xa2\x84\xe6\xb5\x8b\xe7\xbb\x93\xe6\x9e\x9c\n        detections = net(x).data\n        detect_time = _t[\'im_detect\'].toc(average=False)\n\n        # \xe8\xb7\xb3\xe8\xbf\x87j=0\xef\xbc\x8c\xe5\x9b\xa0\xe4\xb8\xba\xe5\xae\x83\xe6\x98\xaf\xe8\x83\x8c\xe6\x99\xaf\n        # \xe9\x81\x8d\xe5\x8e\x86\xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86 \xef\xbc\x8c\xe5\xbe\x97\xe5\x88\xb0\xe7\xbd\x91\xe7\xbb\x9c\xe9\xa2\x84\xe6\xb5\x8b\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x9c\n        for j in range(1, detections.size(1)):\n            dets = detections[0, j, :]\n            mask = dets[:, 0].gt(0.).expand(5, dets.size(0)).t()\n            dets = torch.masked_select(dets, mask).view(-1, 5)\n            if dets.dim() == 0:\n                continue\n            boxes = dets[:, 1:]\n            boxes[:, 0] *= w\n            boxes[:, 2] *= w\n            boxes[:, 1] *= h\n            boxes[:, 3] *= h\n            scores = dets[:, 0].cpu().numpy()\n            cls_dets = np.hstack((boxes.cpu().numpy(),\n                                  scores[:, np.newaxis])).astype(np.float32,\n                                                                 copy=False)\n            all_boxes[j][i] = cls_dets\n\n        print(\'\xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86\xe5\x9b\xbe\xe5\x83\x8f\xe6\xa3\x80\xe6\xb5\x8b\xe8\xbf\x9b\xe5\xba\xa6: {:d}/{:d}  \xe5\x8d\x95\xe5\xbc\xa0\xe9\xa2\x84\xe6\xb5\x8b\xe8\x80\x97\xe6\x97\xb6\xef\xbc\x9a{:.3f}s\'.format(i + 1,\n                                                    num_images, detect_time))\n\n    with open(det_file, \'wb\') as f:\n        pickle.dump(all_boxes, f, pickle.HIGHEST_PROTOCOL)\n\n    print(\'\xe5\xbc\x80\xe5\xa7\x8b\xe8\xaf\x84\xe4\xbc\xb0\xe6\xa3\x80\xe6\xb5\x8b\xe7\xbb\x93\xe6\x9e\x9c\')\n    evaluate_detections(all_boxes, opt.temp, dataset)\n\n\ndef test():\n    \'\'\'\n    \xe6\xb5\x8b\xe8\xaf\x95\xe9\x9b\x86\xe4\xbd\xbf\xe7\x94\xa8VOC2007\xe6\xb5\x8b\xe8\xaf\x95\xe9\x9b\x86\xef\xbc\x8c\xe5\xb0\x86\xe7\xbb\x93\xe6\x9e\x9c\xe5\xad\x98\xe5\x82\xa8\xe5\x88\xb0voc2007_test.txt\xe4\xb8\xad\n    \'\'\'\n    num_classes = len(VOC_CLASSES) + 1  # +1 \xe6\x98\xaf\xe5\x9b\xa0\xe4\xb8\xba\xe8\x83\x8c\xe6\x99\xaf\n    net = build_ssd(\'test\', 300, num_classes)  # \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96SSD\n    # \xe5\x8a\xa0\xe8\xbd\xbd\xe9\xa2\x84\xe8\xae\xad\xe7\xbb\x83\xe5\xa5\xbd\xe7\x9a\x84\xe7\x9a\x84SSD\xe6\xa8\xa1\xe5\x9e\x8b\n    if opt.load_model_path:\n        print(\'\xe5\x8a\xa0\xe8\xbd\xbd\xe5\xb7\xb2\xe8\xae\xad\xe7\xbb\x83\xe5\xa5\xbd\xe7\x9a\x84SSD\xe6\xa8\xa1\xe5\x9e\x8b\')\n        net.load_state_dict(torch.load(opt.load_model_path, map_location=lambda storage, loc: storage))\n        print(\'\xe5\x8a\xa0\xe8\xbd\xbd\xe6\x9d\x83\xe9\x87\x8d\xe5\xae\x8c\xe6\x88\x90!\')\n\n    # \xe6\xa8\xa1\xe5\x9e\x8b\xe8\xb0\x83\xe6\x95\xb4\xe4\xb8\xba\xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86\n    net.eval()\n    # \xe5\x8a\xa0\xe8\xbd\xbd\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\n    testset = VOCDetection(opt.voc_data_root, [(\'2007\', \'test\')], None, VOCAnnotationTransform())\n    if opt.use_gpu:\n        net = net.cuda()\n        cudnn.benchmark = True\n\n    # dump predictions and assoc. ground truth to text file for now\n    transform=BaseTransform(net.size, (104, 117, 123))\n    filename = opt.temp_test + \'voc2007_test.txt\'\n    num_images = len(testset)\n    for i in range(num_images):\n        print(\'\xe6\xb5\x8b\xe8\xaf\x95\xe8\xbf\x9b\xe5\xba\xa6\xef\xbc\x9a {:d}/{:d}\'.format(i + 1, num_images))\n        img = testset.pull_image(i)\n        img_id, annotation = testset.pull_anno(i)\n        x = torch.from_numpy(transform(img)[0]).permute(2, 0, 1)\n        x = Variable(x.unsqueeze(0))\n\n        with open(filename, mode=\'a\') as f:\n            f.write(\'\\nGROUND TRUTH FOR: \' + img_id + \'\\n\')\n            for box in annotation:\n                f.write(\'label: \' + \' || \'.join(str(b) for b in box) + \'\\n\')\n        if opt.use_gpu:\n            x = x.cuda()\n\n        y = net(x)  # forward pass\n        detections = y.data\n        # scale each detection back up to the image\n        # \xe5\xb0\x86\xe6\xaf\x8f\xe4\xb8\xaa\xe6\xa3\x80\xe6\xb5\x8b\xe7\xbc\xa9\xe6\x94\xbe\xe5\x9b\x9e\xe5\x9b\xbe\xe5\x83\x8f\n        scale = torch.Tensor([img.shape[1], img.shape[0],\n                              img.shape[1], img.shape[0]])\n        pred_num = 0\n        for i in range(detections.size(1)):\n            j = 0\n            while detections[0, i, j, 0] >= 0.6:\n                if pred_num == 0:\n                    with open(filename, mode=\'a\') as f:\n                        f.write(\'PREDICTIONS: \' + \'\\n\')\n                score = detections[0, i, j, 0]\n                label_name = VOC_CLASSES[i - 1]\n                pt = (detections[0, i, j, 1:] * scale).cpu().numpy()\n                coords = (pt[0], pt[1], pt[2], pt[3])\n                pred_num += 1\n                with open(filename, mode=\'a\') as f:\n                    f.write(str(pred_num) + \' label: \' + label_name + \' score: \' +\n                            str(score) + \' \' + \' || \'.join(str(c) for c in coords) + \'\\n\')\n                j += 1\n\ndef predict():\n    \'\'\'\n    \xe4\xbd\xbf\xe7\x94\xa8SSD\xe8\xbf\x9b\xe8\xa1\x8c\xe5\xaf\xb9\xe8\xb1\xa1\xe6\xa3\x80\xe6\xb5\x8b,\xe5\xb9\xb6\xe8\xbf\x9b\xe8\xa1\x8c\xe5\x8f\xaf\xe8\xa7\x86\xe5\x8c\x96\n    \'\'\'\n    if torch.cuda.is_available():\n        torch.set_default_tensor_type(\'torch.cuda.FloatTensor\')\n\n    net = build_ssd(\'test\', 300, 21)  # \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96 SSD300\xef\xbc\x8c\xe7\xb1\xbb\xe5\x88\xab\xe4\xb8\xba21\xef\xbc\x8820\xe7\xb1\xbb\xe5\x88\xab+1\xe8\x83\x8c\xe6\x99\xaf\xef\xbc\x89\n    if opt.load_model_path:\n        print(\'\xe5\x8a\xa0\xe8\xbd\xbd\xe5\xb7\xb2\xe8\xae\xad\xe7\xbb\x83\xe5\xa5\xbd\xe7\x9a\x84SSD\xe6\xa8\xa1\xe5\x9e\x8b\')\n        net.load_state_dict(torch.load(opt.load_model_path, map_location=lambda storage, loc: storage))\n        print(\'\xe5\x8a\xa0\xe8\xbd\xbd\xe6\x9d\x83\xe9\x87\x8d\xe5\xae\x8c\xe6\x88\x90!\')\n\n    # \xe5\x8a\xa0\xe8\xbd\xbd\xe5\x8e\x9f\xe5\x9b\xbe\xe5\x83\x8f\xe5\xb9\xb6\xe5\x8f\xaf\xe8\xa7\x86\xe5\x8c\x96\n    image = cv2.imread(opt.test_img, cv2.IMREAD_COLOR)\n    #rgb\xe6\xa0\xbc\xe5\xbc\x8f\xe7\x9a\x84\xe5\x8e\x9f\xe5\x9b\xbe\n    rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    # \xe6\x98\xbe\xe7\xa4\xba\xe5\x8e\x9f\xe5\x9b\xbe\n    plt.figure(figsize=(10, 10))\n    plt.imshow(rgb_image)\n    plt.title(""origin img"")\n    plt.show()\n\n    #\xe5\xaf\xb9\xe8\xbe\x93\xe5\x85\xa5\xe5\x9b\xbe\xe5\x83\x8f\xe8\xbf\x9b\xe8\xa1\x8c\xe9\xa2\x84\xe5\xa4\x84\xe7\x90\x86\n    #\xe5\xb0\x86\xe5\x9b\xbe\xe5\x83\x8fresize\xe5\x88\xb0300x300\n    x = cv2.resize(image, (300, 300)).astype(np.float32)\n    #(104.0, 117.0, 123.0)\xe4\xb8\xbaImageNet\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe7\x9a\x84\xe4\xb8\x89\xe9\x80\x9a\xe9\x81\x93\xe5\x9d\x87\xe5\x80\xbc\xef\xbc\x8c\xe6\x9b\xb4\xe7\xac\xa6\xe5\x90\x88\xe5\xa4\xa7\xe8\x87\xaa\xe7\x84\xb6\xe7\x9a\x84\xe5\x88\x86\xe5\xb8\x83\xe8\xa7\x84\xe5\xbe\x8b\n    x -= (104.0, 117.0, 123.0)\n    #\xe7\xb1\xbb\xe5\x9e\x8b\xe8\xbd\xac\xe5\x8c\x96\n    x = x.astype(np.float32)\n    x = x[:, :, ::-1].copy()\n    #\xe6\x98\xbe\xe7\xa4\xba\xe9\xa2\x84\xe5\xa4\x84\xe7\x90\x86\xe4\xb9\x8b\xe5\x90\x8e\xe7\x9a\x84\xe5\x9b\xbe\xe7\x89\x87\n    plt.title(""pre-process img"")\n    plt.imshow(x)\n    plt.show()\n    #  permute \xe6\x8c\x89\xe5\x88\xb6\xe5\xae\x9a\xe7\x9a\x84\xe7\xbb\xb4\xe6\x95\xb0\xe6\x8e\x92\xe5\xba\x8f\n    x = torch.from_numpy(x).permute(2, 0, 1)\n\n    #\xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xef\xbc\x8c\xe5\xbe\x97\xe5\x88\xb0\xe7\xbd\x91\xe7\xbb\x9c\xe9\xa2\x84\xe6\xb5\x8b\n    xx = Variable(x.unsqueeze(0))  # \xe6\x89\xa9\xe5\xb1\x95\xe7\xac\xac0\xe7\xbb\xb4\xef\xbc\x8c\xe5\x9b\xa0\xe4\xb8\xba\xe7\xbd\x91\xe7\xbb\x9c\xe7\x9a\x84\xe8\xbe\x93\xe5\x85\xa5\xe8\xa6\x81\xe6\xb1\x82\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaabatch\n    if torch.cuda.is_available():\n        xx = xx.cuda()\n    y = net(xx)\n\n    #\xe8\xa7\xa3\xe6\x9e\x90\xe7\xbd\x91\xe7\xbb\x9c\xe8\xbe\x93\xe5\x87\xba\xe5\xb9\xb6\xe5\xb0\x86\xe7\xbb\x93\xe6\x9e\x9c\xe5\x8f\xaf\xe8\xa7\x86\xe5\x8c\x96\n    plt.figure(figsize=(10, 10))\n    colors = plt.cm.hsv(np.linspace(0, 1, 21)).tolist()\n    plt.imshow(rgb_image)  # \xe7\x94\xbb\xe5\x87\xbargb\xe6\xa0\xbc\xe5\xbc\x8f\xe7\x9a\x84\xe5\x8e\x9f\xe5\x9b\xbe\n    currentAxis = plt.gca()  #\xe5\xbd\x93\xe5\x89\x8d\xe8\xbd\xb4\n\n    detections = y.data\n    # \xe5\xb0\x86\xe6\xa3\x80\xe6\xb5\x8b\xe7\xbb\x93\xe6\x9e\x9c\xe7\xbc\xa9\xe6\x94\xbe\xe5\x8c\xb9\xe9\x85\x8d\xe5\x88\xb0\xe5\x8e\x9f\xe5\x9b\xbe\xe4\xb8\x8a  repeat\xef\xbc\x9a\xe6\xb2\xbf\xe7\x9d\x80\xe6\x8c\x87\xe5\xae\x9a\xe7\x9a\x84\xe5\xb0\xba\xe5\xaf\xb8\xe9\x87\x8d\xe5\xa4\x8d tensor\n    scale = torch.Tensor(rgb_image.shape[1::-1]).repeat(2)\n    for i in range(detections.size(1)):\n        j = 0\n        #\xe7\xbd\xae\xe4\xbf\xa1\xe5\xba\xa6\xe5\x88\x86\xe6\x95\xb0\xe9\x98\x88\xe5\x80\xbc\xe8\xae\xbe\xe7\xbd\xae\xe4\xb8\xba0.6\n        while detections[0, i, j, 0] >= 0.6:\n            score = detections[0, i, j, 0]\n            label_name = VOC_CLASSES[i - 1]\n            display_txt = \'%s: %.2f\' % (label_name, score)\n            pt = (detections[0, i, j, 1:] * scale).cpu().numpy()\n            coords = (pt[0], pt[1]), pt[2] - pt[0] + 1, pt[3] - pt[1] + 1\n            color = colors[i]\n            currentAxis.add_patch(plt.Rectangle(*coords, fill=False, edgecolor=color, linewidth=2))\n            currentAxis.text(pt[0], pt[1], display_txt, bbox={\'facecolor\': color, \'alpha\': 0.5})\n            j += 1\n    #\xe5\x8f\xaf\xe8\xa7\x86\xe5\x8c\x96\xe6\x9c\x80\xe5\x90\x8e\xe7\xbb\x93\xe6\x9e\x9c\n    plt.title(""predict img"")\n    plt.show()\n\n\n\nif __name__ == \'__main__\':\n    # train()  #\xe4\xbd\xbf\xe7\x94\xa8VOC2007\xe5\x92\x8c2012\xe7\x9a\x84\xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86+\xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86 \xe5\xbc\x80\xe5\xa7\x8b\xe8\xae\xad\xe7\xbb\x83\n    # eval()  # VOC2007\xe6\xb5\x8b\xe8\xaf\x95\xe9\x9b\x86,\xe8\xae\xa1\xe7\xae\x97\xe5\x90\x84\xe7\xb1\xbbAP\xe5\x8f\x8amAP\n    # test()  #VOC2007\xe6\xb5\x8b\xe8\xaf\x95\xe9\x9b\x86\xef\xbc\x8c\xe5\xb0\x86\xe9\xa2\x84\xe6\xb5\x8b\xe7\xbb\x93\xe6\x9e\x9c\xe5\x86\x99\xe5\x85\xa5txt\n    predict()  #\xe5\x8f\xaf\xe8\xa7\x86\xe5\x8c\x96\xe4\xb8\x80\xe5\xbc\xa0\xe9\xa2\x84\xe6\xb5\x8b\xe5\x9b\xbe\xe7\x89\x87'"
UNet_pytorch/dice_loss.py,5,"b'import torch\nfrom torch.autograd import Function, Variable\n\nclass DiceCoeff(Function):\n    """"""Dice coeff for individual examples""""""\n\n    def forward(self, input, target):\n        self.save_for_backward(input, target)\n        eps = 0.0001\n        self.inter = torch.dot(input.view(-1), target.view(-1))\n        self.union = torch.sum(input) + torch.sum(target) + eps\n\n        t = (2 * self.inter.float() + eps) / self.union.float()\n        return t\n\n    # This function has only a single output, so it gets only one gradient\n    def backward(self, grad_output):\n\n        input, target = self.saved_variables\n        grad_input = grad_target = None\n\n        if self.needs_input_grad[0]:\n            grad_input = grad_output * 2 * (target * self.union - self.inter) \\\n                         / self.union * self.union\n        if self.needs_input_grad[1]:\n            grad_target = None\n\n        return grad_input, grad_target\n\n\ndef dice_coeff(input, target):\n    """"""Dice coeff for batches""""""\n    if input.is_cuda:\n        s = torch.FloatTensor(1).cuda().zero_()\n    else:\n        s = torch.FloatTensor(1).zero_()\n\n    for i, c in enumerate(zip(input, target)):\n        s = s + DiceCoeff().forward(c[0], c[1])\n\n    return s / (i + 1)\n'"
UNet_pytorch/eval.py,3,"b'import torch\nimport torch.nn.functional as F\n\nfrom dice_loss import dice_coeff\n\n\ndef eval_net(net, dataset, gpu=False):\n    \'\'\'\n    :param net: \xe8\xae\xad\xe7\xbb\x83\xe7\x9a\x84\xe7\xbd\x91\xe7\xbb\x9c\n    :param dataset: \xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86\n    \'\'\'\n    """"""Evaluation without the densecrf with the dice coefficient""""""\n    tot = 0\n    for i, b in enumerate(dataset):\n        img = b[0]\n        true_mask = b[1]\n\n        img = torch.from_numpy(img).unsqueeze(0)\n        true_mask = torch.from_numpy(true_mask).unsqueeze(0)\n\n        if gpu:\n            img = img.cuda()\n            true_mask = true_mask.cuda()\n\n        mask_pred = net(img)[0]\n        mask_pred = (F.sigmoid(mask_pred) > 0.5).float()\n        # \xe8\xaf\x84\xe4\xbb\xb7\xe5\x87\xbd\xe6\x95\xb0\xef\xbc\x9aDice\xe7\xb3\xbb\xe6\x95\xb0   Dice\xe8\xb7\x9d\xe7\xa6\xbb\xe7\x94\xa8\xe4\xba\x8e\xe5\xba\xa6\xe9\x87\x8f\xe4\xb8\xa4\xe4\xb8\xaa\xe9\x9b\x86\xe5\x90\x88\xe7\x9a\x84\xe7\x9b\xb8\xe4\xbc\xbc\xe6\x80\xa7\n        tot += dice_coeff(mask_pred, true_mask).item()\n    return tot / i\n'"
UNet_pytorch/predict.py,6,"b'import argparse\nimport os\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\n\nfrom PIL import Image\n\nfrom unet import UNet\nfrom utils import resize_and_crop, normalize, split_img_into_squares, hwc_to_chw, merge_masks, dense_crf\nfrom utils import plot_img_and_mask\n\nfrom torchvision import transforms\n\n\ndef predict_img(net,\n                full_img,\n                scale_factor=0.5,\n                out_threshold=0.5,\n                use_dense_crf=True,\n                use_gpu=False):\n    img_height = full_img.size[1]\n    img_width = full_img.size[0]\n\n    img = resize_and_crop(full_img, scale=scale_factor)\n    img = normalize(img)\n\n    left_square, right_square = split_img_into_squares(img)\n\n    left_square = hwc_to_chw(left_square)\n    right_square = hwc_to_chw(right_square)\n\n    X_left = torch.from_numpy(left_square).unsqueeze(0)\n    X_right = torch.from_numpy(right_square).unsqueeze(0)\n\n    if use_gpu:\n        X_left = X_left.cuda()\n        X_right = X_right.cuda()\n\n    with torch.no_grad():\n        output_left = net(X_left)\n        output_right = net(X_right)\n\n        left_probs = F.sigmoid(output_left).squeeze(0)\n        right_probs = F.sigmoid(output_right).squeeze(0)\n\n        tf = transforms.Compose(\n            [\n                transforms.ToPILImage(),\n                transforms.Resize(img_height),\n                transforms.ToTensor()\n            ]\n        )\n\n        left_probs = tf(left_probs.cpu())\n        right_probs = tf(right_probs.cpu())\n\n        left_mask_np = left_probs.squeeze().cpu().numpy()\n        right_mask_np = right_probs.squeeze().cpu().numpy()\n\n    full_mask = merge_masks(left_mask_np, right_mask_np, img_width)\n\n    if use_dense_crf:\n        full_mask = dense_crf(np.array(full_img).astype(np.uint8), full_mask)\n\n    return full_mask > out_threshold\n\n\ndef get_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--model\', \'-m\', default=\'MODEL.pth\',\n                        metavar=\'FILE\',\n                        help=""Specify the file in which is stored the model""\n                             "" (default : \'MODEL.pth\')"")\n    parser.add_argument(\'--input\', \'-i\', metavar=\'INPUT\', nargs=\'+\',\n                        help=\'filenames of input images\', required=True)\n\n    parser.add_argument(\'--output\', \'-o\', metavar=\'INPUT\', nargs=\'+\',\n                        help=\'filenames of ouput images\')\n    parser.add_argument(\'--cpu\', \'-c\', action=\'store_true\',\n                        help=""Do not use the cuda version of the net"",\n                        default=False)\n    parser.add_argument(\'--viz\', \'-v\', action=\'store_true\',\n                        help=""Visualize the images as they are processed"",\n                        default=False)\n    parser.add_argument(\'--no-save\', \'-n\', action=\'store_true\',\n                        help=""Do not save the output masks"",\n                        default=False)\n    parser.add_argument(\'--no-crf\', \'-r\', action=\'store_true\',\n                        help=""Do not use dense CRF postprocessing"",\n                        default=False)\n    parser.add_argument(\'--mask-threshold\', \'-t\', type=float,\n                        help=""Minimum probability value to consider a mask pixel white"",\n                        default=0.5)\n    parser.add_argument(\'--scale\', \'-s\', type=float,\n                        help=""Scale factor for the input images"",\n                        default=0.5)\n\n    return parser.parse_args()\n\n\ndef get_output_filenames(args):\n    in_files = args.input\n    out_files = []\n\n    if not args.output:\n        for f in in_files:\n            pathsplit = os.path.splitext(f)\n            out_files.append(""{}_OUT{}"".format(pathsplit[0], pathsplit[1]))\n    elif len(in_files) != len(args.output):\n        print(""Error : Input files and output files are not of the same length"")\n        raise SystemExit()\n    else:\n        out_files = args.output\n\n    return out_files\n\n\ndef mask_to_image(mask):\n    return Image.fromarray((mask * 255).astype(np.uint8))\n\n\nif __name__ == ""__main__"":\n    args = get_args()\n    in_files = args.input\n    out_files = get_output_filenames(args)\n\n    net = UNet(n_channels=3, n_classes=1)\n\n    print(""Loading model {}"".format(args.model))\n\n    if not args.cpu:\n        print(""Using CUDA version of the net, prepare your GPU !"")\n        net.cuda()\n        net.load_state_dict(torch.load(args.model))\n    else:\n        net.cpu()\n        net.load_state_dict(torch.load(args.model, map_location=\'cpu\'))\n        print(""Using CPU version of the net, this may be very slow"")\n\n    print(""Model loaded !"")\n\n    for i, fn in enumerate(in_files):\n        print(""\\nPredicting image {} ..."".format(fn))\n\n        img = Image.open(fn)\n        if img.size[0] < img.size[1]:\n            print(""Error: image height larger than the width"")\n\n        mask = predict_img(net=net,\n                           full_img=img,\n                           scale_factor=args.scale,\n                           out_threshold=args.mask_threshold,\n                           use_dense_crf=not args.no_crf,\n                           use_gpu=not args.cpu)\n\n        if args.viz:\n            print(""Visualizing results for image {}, close to continue ..."".format(fn))\n            plot_img_and_mask(img, mask)\n\n        if not args.no_save:\n            out_fn = out_files[i]\n            result = mask_to_image(mask)\n            result.save(out_files[i])\n\n            print(""Mask saved to {}"".format(out_files[i]))\n'"
UNet_pytorch/submit.py,1,"b'import os\nfrom PIL import Image\n\nimport torch\n\nfrom predict import predict_img\nfrom utils import rle_encode\nfrom unet import UNet\n\n\ndef submit(net, gpu=False):\n    """"""Used for Kaggle submission: predicts and encode all test images""""""\n    dir = \'data/test/\'\n\n    N = len(list(os.listdir(dir)))\n    with open(\'SUBMISSION.csv\', \'a\') as f:\n        f.write(\'img,rle_mask\\n\')\n        for index, i in enumerate(os.listdir(dir)):\n            print(\'{}/{}\'.format(index, N))\n\n            img = Image.open(dir + i)\n\n            mask = predict_img(net, img, gpu)\n            enc = rle_encode(mask)\n            f.write(\'{},{}\\n\'.format(i, \' \'.join(map(str, enc))))\n\n\nif __name__ == \'__main__\':\n    net = UNet(3, 1).cuda()\n    net.load_state_dict(torch.load(\'MODEL.pth\'))\n    submit(net, True)\n'"
UNet_pytorch/train.py,8,"b""import sys\nimport os\nimport numpy as np\n\nimport torch\nimport torch.backends.cudnn as cudnn\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import optim\n\nfrom eval import eval_net\nfrom unet import UNet\nfrom utils import get_ids, split_ids, split_train_val, get_imgs_and_masks, batch\nfrom utils.config import opt_train\n\ndef train_net(net,\n              epochs=5,\n              batch_size=1,\n              lr=0.1,\n              val_percent=0.05,  # \xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\xef\xbc\x9a\xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86= 0.95\xef\xbc\x9a 0.05\n              save_cp=True,\n              gpu=False,\n              img_scale=0.5):\n\n    dir_img = opt_train.dir_img\n    dir_mask = opt_train.dir_mask\n    dir_checkpoint = opt_train.dir_checkpoint\n\n    # \xe5\xbe\x97\xe5\x88\xb0 \xe5\x9b\xbe\xe7\x89\x87\xe8\xb7\xaf\xe5\xbe\x84\xe5\x88\x97\xe8\xa1\xa8  ids\xe4\xb8\xba \xe5\x9b\xbe\xe7\x89\x87\xe5\x90\x8d\xe7\xa7\xb0\xef\xbc\x88\xe6\x97\xa0\xe5\x90\x8e\xe7\xbc\x80\xe5\x90\x8d\xef\xbc\x89\n    ids = get_ids(dir_img)\n    # \xe5\xbe\x97\xe5\x88\xb0truple\xe5\x85\x83\xe7\xbb\x84  \xef\xbc\x88\xe6\x97\xa0\xe5\x90\x8e\xe7\xbc\x80\xe5\x90\x8d\xe7\x9a\x84 \xe5\x9b\xbe\xe7\x89\x87\xe5\x90\x8d\xe7\xa7\xb0\xef\xbc\x8c\xe5\xba\x8f\xe5\x8f\xb7\xef\xbc\x89\n    # eg:\xe5\xbd\x93n\xe4\xb8\xba2  \xe5\x9b\xbe\xe7\x89\x87\xe5\x90\x8d\xe7\xa7\xb0\xe4\xb8\xbabobo.jpg \xe6\x97\xb6, \xe5\xbe\x97\xe5\x88\xb0\xef\xbc\x88bobo,0\xef\xbc\x89 \xef\xbc\x88bobo,1\xef\xbc\x89\n    # \xe5\xbd\x93\xe5\xba\x8f\xe5\x8f\xb7\xe4\xb8\xba0 \xe6\x97\xb6\xef\xbc\x8c\xe8\xa3\x81\xe5\x89\xaa\xe5\xae\xbd\xe5\xba\xa6\xef\xbc\x8c\xe5\xbe\x97\xe5\x88\xb0\xe5\xb7\xa6\xe8\xbe\xb9\xe9\x83\xa8\xe5\x88\x86\xe5\x9b\xbe\xe7\x89\x87  \xe5\xbd\x93\xe5\xba\x8f\xe5\x8f\xb7\xe4\xb8\xba1 \xe6\x97\xb6\xef\xbc\x8c\xe8\xa3\x81\xe5\x89\xaa\xe5\xae\xbd\xe5\xba\xa6\xef\xbc\x8c\xe5\xbe\x97\xe5\x88\xb0\xe5\x8f\xb3\xe8\xbe\xb9\xe9\x83\xa8\xe5\x88\x86\xe5\x9b\xbe\xe7\x89\x87\n    ids = split_ids(ids)\n    # \xe6\x89\x93\xe4\xb9\xb1\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe5\x90\x8e\xef\xbc\x8c\xe6\x8c\x89\xe7\x85\xa7val_percent\xe7\x9a\x84\xe6\xaf\x94\xe4\xbe\x8b\xe6\x9d\xa5 \xe5\x88\x87\xe5\x88\x86 \xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86 \xe5\x92\x8c \xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86\n    iddataset = split_train_val(ids, val_percent)\n\n\n    print('''\n    \xe5\xbc\x80\xe5\xa7\x8b\xe8\xae\xad\xe7\xbb\x83:\n        Epochs: {}\n        Batch size: {}\n        Learning rate: {}\n        \xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\xe5\xa4\xa7\xe5\xb0\x8f: {}\n        \xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86\xe5\xa4\xa7\xe5\xb0\x8f: {}\n        GPU: {}\n    '''.format(epochs, batch_size, lr, len(iddataset['train']),\n               len(iddataset['val']), str(gpu)))\n\n    #\xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\xe5\xa4\xa7\xe5\xb0\x8f\n    N_train = len(iddataset['train'])\n\n    optimizer = optim.SGD(net.parameters(),\n                          lr=lr,\n                          momentum=0.9,\n                          weight_decay=0.0005)\n\n    #\xe4\xba\x8c\xe8\xbf\x9b\xe5\x88\xb6\xe4\xba\xa4\xe5\x8f\x89\xe7\x86\xb5\n    criterion = nn.BCELoss()\n\n    for epoch in range(epochs):\n        print('Starting epoch {}/{}.'.format(epoch + 1, epochs))\n\n        # reset the generators\n        # \xe6\xaf\x8f\xe8\xbd\xaeepoch\xe5\xbe\x97\xe5\x88\xb0 \xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86  \xe5\x92\x8c \xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86\n        train = get_imgs_and_masks(iddataset['train'], dir_img, dir_mask, img_scale)\n        val = get_imgs_and_masks(iddataset['val'], dir_img, dir_mask, img_scale)\n\n\n\n\n        # \xe9\x87\x8d\xe7\xbd\xaeepoch\xe6\x8d\x9f\xe5\xa4\xb1\xe8\xae\xa1\xe6\x95\xb0\xe5\x99\xa8\n        epoch_loss = 0\n\n        for i, b in enumerate(batch(train, batch_size)):\n            # \xe5\xbe\x97\xe5\x88\xb0 \xe4\xb8\x80\xe4\xb8\xaabatch\xe7\x9a\x84 imgs tensor \xe5\x8f\x8a \xe5\xaf\xb9\xe5\xba\x94\xe7\x9c\x9f\xe5\xae\x9emask\xe5\x80\xbc\n            # \xe5\xbd\x93\xe5\xba\x8f\xe5\x8f\xb7\xe4\xb8\xba0 \xe6\x97\xb6\xef\xbc\x8c\xe8\xa3\x81\xe5\x89\xaa\xe5\xae\xbd\xe5\xba\xa6\xef\xbc\x8c\xe5\xbe\x97\xe5\x88\xb0\xe5\xb7\xa6\xe8\xbe\xb9\xe9\x83\xa8\xe5\x88\x86\xe5\x9b\xbe\xe7\x89\x87[384,384,3]   \xe5\xbd\x93\xe5\xba\x8f\xe5\x8f\xb7\xe4\xb8\xba1 \xe6\x97\xb6\xef\xbc\x8c\xe8\xa3\x81\xe5\x89\xaa\xe5\xae\xbd\xe5\xba\xa6\xef\xbc\x8c\xe5\xbe\x97\xe5\x88\xb0\xe5\x8f\xb3\xe8\xbe\xb9\xe9\x83\xa8\xe5\x88\x86\xe5\x9b\xbe\xe7\x89\x87[384,190,3]\n            imgs = np.array([i[0] for i in b]).astype(np.float32)\n            true_masks = np.array([i[1] for i in b])\n\n            # \xe5\xb0\x86\xe5\x80\xbc\xe8\xbd\xac\xe4\xb8\xba torch tensor\n            imgs = torch.from_numpy(imgs)\n            true_masks = torch.from_numpy(true_masks)\n\n            # \xe8\xae\xad\xe7\xbb\x83\xe6\x95\xb0\xe6\x8d\xae\xe8\xbd\xac\xe5\x88\xb0GPU\xe4\xb8\x8a\n            if gpu:\n                imgs = imgs.cuda()\n                true_masks = true_masks.cuda()\n\n            # \xe5\xbe\x97\xe5\x88\xb0 \xe7\xbd\x91\xe7\xbb\x9c\xe8\xbe\x93\xe5\x87\xba\xe7\x9a\x84\xe9\xa2\x84\xe6\xb5\x8bmask [10,1,384,384]\n            masks_pred = net(imgs)\n            # \xe7\xbb\x8f\xe8\xbf\x87sigmoid\n            masks_probs = F.sigmoid(masks_pred)\n            masks_probs_flat = masks_probs.view(-1)\n\n            true_masks_flat = true_masks.view(-1)\n            # \xe8\xae\xa1\xe7\xae\x97\xe4\xba\x8c\xe8\xbf\x9b\xe5\x88\xb6\xe4\xba\xa4\xe5\x8f\x89\xe7\x86\xb5\xe6\x8d\x9f\xe5\xa4\xb1\n            loss = criterion(masks_probs_flat, true_masks_flat)\n            # \xe7\xbb\x9f\xe8\xae\xa1\xe4\xb8\x80\xe4\xb8\xaaepoch\xe7\x9a\x84\xe6\x89\x80\xe6\x9c\x89batch\xe7\x9a\x84loss\xe4\xb9\x8b\xe5\x92\x8c\xef\xbc\x8c\xe7\x94\xa8\xe4\xbb\xa5\xe8\xae\xa1\xe7\xae\x97 \xe4\xb8\x80\xe4\xb8\xaaepoch\xe7\x9a\x84 loss\xe5\x9d\x87\xe5\x80\xbc\n            epoch_loss += loss.item()\n\n            # \xe8\xbe\x93\xe5\x87\xba \xe5\xbd\x93\xe5\x89\x8depoch\xe7\x9a\x84\xe7\xac\xac\xe5\x87\xa0\xe4\xb8\xaabatch  \xe5\x8f\x8a \xe5\xbd\x93\xe5\x89\x8dbatch\xe7\x9a\x84loss\n            print('{0:.4f} --- loss: {1:.6f}'.format(i * batch_size / N_train, loss.item()))\n\n            # \xe4\xbc\x98\xe5\x8c\x96\xe5\x99\xa8\xe6\xa2\xaf\xe5\xba\xa6\xe6\xb8\x85\xe9\x9b\xb6\n            optimizer.zero_grad()\n            # \xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\n            loss.backward()\n            # \xe6\x9b\xb4\xe6\x96\xb0\xe5\x8f\x82\xe6\x95\xb0\n            optimizer.step()\n\n        # \xe4\xb8\x80\xe8\xbd\xaeepoch\xe7\xbb\x93\xe6\x9d\x9f\xef\xbc\x8c\xe8\xaf\xa5\xe8\xbd\xaeepoch\xe7\x9a\x84 loss\xe5\x9d\x87\xe5\x80\xbc\n        print('Epoch finished ! Loss: {}'.format(epoch_loss / i))\n\n        # \xe6\xaf\x8f\xe8\xbd\xaeepoch\xe4\xb9\x8b\xe5\x90\x8e\xe4\xbd\xbf\xe7\x94\xa8\xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86\xe8\xbf\x9b\xe8\xa1\x8c\xe8\xaf\x84\xe4\xbb\xb7\n        if True:\n            # \xe8\xaf\x84\xe4\xbb\xb7\xe5\x87\xbd\xe6\x95\xb0\xef\xbc\x9aDice\xe7\xb3\xbb\xe6\x95\xb0   Dice\xe8\xb7\x9d\xe7\xa6\xbb\xe7\x94\xa8\xe4\xba\x8e\xe5\xba\xa6\xe9\x87\x8f\xe4\xb8\xa4\xe4\xb8\xaa\xe9\x9b\x86\xe5\x90\x88\xe7\x9a\x84\xe7\x9b\xb8\xe4\xbc\xbc\xe6\x80\xa7\n            val_dice = eval_net(net, val, gpu)\n            print('Validation Dice Coeff: {}'.format(val_dice))\n\n        # \xe4\xbf\x9d\xe5\xad\x98\xe6\xa8\xa1\xe5\x9e\x8b\n        if save_cp:\n            torch.save(net.state_dict(),\n                       dir_checkpoint + 'CP{}.pth'.format(epoch + 1))\n            print('Checkpoint {} saved !'.format(epoch + 1))\n\n\n\n\n\nif __name__ == '__main__':\n\n    # \xe8\x8e\xb7\xe5\x8f\x96\xe8\xae\xad\xe7\xbb\x83\xe5\x8f\x82\xe6\x95\xb0\n    args = opt_train\n    # n_channels\xef\xbc\x9a\xe8\xbe\x93\xe5\x85\xa5\xe5\x9b\xbe\xe5\x83\x8f\xe7\x9a\x84\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0   n_classes\xef\xbc\x9a\xe4\xba\x8c\xe5\x88\x86\xe7\xb1\xbb\n    net = UNet(n_channels=3, n_classes=1)\n\n    # \xe5\x8a\xa0\xe8\xbd\xbd\xe9\xa2\x84\xe8\xae\xad\xe7\xbb\x83\xe6\xa8\xa1\xe5\x9e\x8b\n    if args.load:\n        net.load_state_dict(torch.load(args.load))\n        print('Model loaded from {}'.format(args.load))\n\n    # \xe7\xbd\x91\xe7\xbb\x9c\xe8\xbd\xac\xe7\xa7\xbb\xe5\x88\xb0GPU\xe4\xb8\x8a\n    if args.gpu:\n        net.cuda()\n        cudnn.benchmark = True # \xe9\x80\x9f\xe5\xba\xa6\xe6\x9b\xb4\xe5\xbf\xab\xef\xbc\x8c\xe4\xbd\x86\xe5\x8d\xa0\xe7\x94\xa8\xe5\x86\x85\xe5\xad\x98\xe6\x9b\xb4\xe5\xa4\x9a\n\n    try:\n        train_net(net=net,\n                  epochs=args.epochs,\n                  batch_size=args.batchsize,\n                  lr=args.lr,\n                  gpu=args.gpu,\n                  img_scale=args.scale)\n    except KeyboardInterrupt:\n        # \xe5\xbd\x93\xe8\xbf\x90\xe8\xa1\x8c\xe5\x87\xba\xe9\x94\x99\xe6\x97\xb6\xef\xbc\x8c\xe4\xbf\x9d\xe5\xad\x98\xe6\x9c\x80\xe6\x96\xb0\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\n        torch.save(net.state_dict(), 'INTERRUPTED.pth')\n        print('Saved interrupt')\n        try:\n            sys.exit(0)\n        except SystemExit:\n            os._exit(0)\n"""
Yolov1_pytorch/config.py,0,"b""# -*- coding:utf-8 -*-\n# power by Mr.Li\n# \xe8\xae\xbe\xe7\xbd\xae\xe9\xbb\x98\xe8\xae\xa4\xe5\x8f\x82\xe6\x95\xb0\nclass DefaultConfig():\n    env = 'YOLOv1'  # visdom \xe7\x8e\xaf\xe5\xa2\x83\xe7\x9a\x84\xe5\x90\x8d\xe5\xad\x97\n    # model = 'NetWork'  # \xe4\xbd\xbf\xe7\x94\xa8\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\xef\xbc\x8c\xe5\x90\x8d\xe5\xad\x97\xe5\xbf\x85\xe9\xa1\xbb\xe4\xb8\x8emodels/__init__.py\xe4\xb8\xad\xe7\x9a\x84\xe5\x90\x8d\xe5\xad\x97\xe4\xb8\x80\xe8\x87\xb4\n    file_root = '/home/zhuhui/data/VOCdevkit/VOC2012/JPEGImages/'  #VOC2012\xe7\x9a\x84\xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\n    test_root = '/home/zhuhui/data/VOCdevkit/VOC2007/JPEGImages/'   #VOC2007\xe7\x9a\x84\xe6\xb5\x8b\xe8\xaf\x95\xe9\x9b\x86\n    train_Annotations = '/home/zhuhui/data/VOCdevkit/VOC2012/Annotations/'\n    voc_2007test='/home/bobo/PycharmProjects/torchProjectss/YOLOv1ByBobo/data/voc2007test.txt'\n    voc_2012train='/home/bobo/PycharmProjects/torchProjectss/YOLOv1ByBobo/data/voc2012train.txt'\n\n    test_img_dir='/home/bobo/PycharmProjects/torchProjectss/YOLOv1ByBobo/testImgs/a.jpg'\n    result_img_dir='/home/bobo/PycharmProjects/torchProjectss/YOLOv1ByBobo/testImgs/result_a.jpg'\n\n\n\n    batch_size = 32  # batch size\n    use_gpu = True  # user GPU or not\n    num_workers = 4  # how many workers for loading data  \xe5\x8a\xa0\xe8\xbd\xbd\xe6\x95\xb0\xe6\x8d\xae\xe6\x97\xb6\xe7\x9a\x84\xe7\xba\xbf\xe7\xa8\x8b\n    print_freq = 20  # print info every N batch\n\n    # load_model_path =None  # \xe5\x8a\xa0\xe8\xbd\xbd\xe9\xa2\x84\xe8\xae\xad\xe7\xbb\x83\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\xe7\x9a\x84\xe8\xb7\xaf\xe5\xbe\x84\xef\xbc\x8c\xe4\xb8\xbaNone\xe4\xbb\xa3\xe8\xa1\xa8\xe4\xb8\x8d\xe5\x8a\xa0\xe8\xbd\xbd\n    best_test_loss_model_path= '/home/bobo/PycharmProjects/torchProjectss/YOLOv1ByBobo/checkpoint/yolo_val_best.pth'\n    current_epoch_model_path='/home/bobo/PycharmProjects/torchProjectss/YOLOv1ByBobo/checkpoint/yolo_bobo.pth'\n    load_model_path = None  # \xe5\x8a\xa0\xe8\xbd\xbd\xe9\xa2\x84\xe8\xae\xad\xe7\xbb\x83\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\xe7\x9a\x84\xe8\xb7\xaf\xe5\xbe\x84\xef\xbc\x8c\xe4\xb8\xbaNone\xe4\xbb\xa3\xe8\xa1\xa8\xe4\xb8\x8d\xe5\x8a\xa0\xe8\xbd\xbd\n    num_epochs = 120   #\xe8\xae\xad\xe7\xbb\x83\xe7\x9a\x84epoch\xe6\xac\xa1\xe6\x95\xb0\n    learning_rate = 0.001  # initial learning rate\n    lr_decay = 0.5  # when val_loss increase, lr = lr*lr_decay\n    momentum=0.95\n    weight_decay =5e-4  # \xe6\x8d\x9f\xe5\xa4\xb1\xe5\x87\xbd\xe6\x95\xb0\n    # VOC\xe7\x9a\x84\xe7\xb1\xbb\xe5\x88\xab\n    VOC_CLASSES = (  # always index 0\n        'aeroplane', 'bicycle', 'bird', 'boat',\n        'bottle', 'bus', 'car', 'cat', 'chair',\n        'cow', 'diningtable', 'dog', 'horse',\n        'motorbike', 'person', 'pottedplant',\n        'sheep', 'sofa', 'train', 'tvmonitor')\n\n\n #\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe8\xaf\xa5\xe7\xb1\xbb\xe7\x9a\x84\xe4\xb8\x80\xe4\xb8\xaa\xe5\xaf\xb9\xe8\xb1\xa1\nopt=DefaultConfig()"""
Yolov1_pytorch/main.py,9,"b""from collections import defaultdict\n\nimport cv2\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\nfrom config import opt\nfrom data.dataset import yoloDataset\nfrom models.net import vgg16\nfrom utils.visualize import Visualizer\nfrom utils.yoloLoss import yoloLoss\nfrom utils.predictUtils import predict_result\nfrom utils.predictUtils import voc_eval\nfrom utils.predictUtils import  voc_ap\n\n\ndef train():\n    vis=Visualizer(opt.env)\n    # \xe7\xbd\x91\xe7\xbb\x9c\xe9\x83\xa8\xe5\x88\x86======================================================\xe5\xbc\x80\xe5\xa7\x8b\n    # True\xe5\x88\x99\xe8\xbf\x94\xe5\x9b\x9e\xe9\xa2\x84\xe8\xae\xad\xe7\xbb\x83\xe5\xa5\xbd\xe7\x9a\x84VGG16\xe6\xa8\xa1\xe5\x9e\x8b\n    net = vgg16(pretrained=True)\n    # \xe6\x8f\x90\xe5\x8f\x96\xe7\x89\xb9\xe5\xbe\x81\xe5\xb1\x82\xe4\xb8\x8d\xe5\x8a\xa8\n    # \xe4\xbf\xae\xe6\x94\xb9\xe5\x88\x86\xe7\xb1\xbb\xe5\xb1\x82\xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x89\xe5\xb1\x82\xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe5\xb1\x82\n    # \xe4\xbf\xae\xe6\x94\xb9vgg\xe7\x9a\x84\xe5\x88\x86\xe7\xb1\xbb\xe5\xb1\x82\xe7\xbb\x93\xe6\x9e\x84\n    # \xe4\xbf\xae\xe6\x94\xb9vgg16\xe7\x9a\x84\xe5\x88\x86\xe7\xb1\xbb\xe9\x83\xa8\xe5\x88\x86\n    net.classifier = nn.Sequential(\n        nn.Linear(512 * 7 * 7, 4096),\n        nn.ReLU(True),\n        nn.Dropout(),\n        # \xe5\x8f\x96\xe6\xb6\x88\xe4\xb8\x80\xe5\xb1\x82\xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe5\xb1\x82\n        # nn.Linear(4096, 4096),\n        # nn.ReLU(True),\n        # nn.Dropout(),\n        # \xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x80\xe5\xb1\x82\xe4\xbf\xae\xe6\x94\xb9\xe4\xb8\xba1470   \xe5\x8d\xb3\xe4\xb8\xba1470\xe4\xbb\xa3\xe8\xa1\xa8\xe4\xb8\x80\xe5\xbc\xa0\xe5\x9b\xbe\xe7\x9a\x84\xe4\xbf\xa1\xe6\x81\xaf\xef\xbc\x881470=7x7x30\xef\xbc\x89\n        nn.Linear(4096, 1470),\n    )\n    # \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe7\xbd\x91\xe7\xbb\x9c\xe7\x9a\x84\xe7\xba\xbf\xe6\x80\xa7\xe5\xb1\x82 \xe6\x9d\x83\xe9\x87\x8d\xe5\x8f\x8a\xe5\x81\x8f\xe5\x90\x91\n    for m in net.modules():\n        if isinstance(m, nn.Linear):\n            m.weight.data.normal_(0, 0.01)\n            m.bias.data.zero_()\n    # \xe5\xb0\x86\xe6\xa8\xa1\xe5\x9e\x8b\xe5\x8a\xa0\xe8\xbd\xbd\xe5\x88\xb0\xe5\x86\x85\xe5\xad\x98\xe4\xb8\xad\xef\xbc\x88CPU\xef\xbc\x89\n    if opt.load_model_path:\n        net.load_state_dict(torch.load(opt.load_model_path,map_location=lambda  storage,loc:storage))\n    # \xe5\x86\x8d\xe5\xb0\x86\xe6\xa8\xa1\xe5\x9e\x8b\xe8\xbd\xac\xe7\xa7\xbb\xe5\x88\xb0GPU\xe4\xb8\x8a\n    if opt.use_gpu:\n        net.cuda()\n    # \xe8\xbe\x93\xe5\x87\xba\xe7\xbd\x91\xe7\xbb\x9c\xe7\xbb\x93\xe6\x9e\x84\n    print(net)\n    print('\xe5\x8a\xa0\xe8\xbd\xbd\xe5\xa5\xbd\xe9\xa2\x84\xe5\x85\x88\xe8\xae\xad\xe7\xbb\x83\xe5\xa5\xbd\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b')\n    # \xe5\xb0\x86\xe6\xa8\xa1\xe5\x9e\x8b\xe8\xb0\x83\xe6\x95\xb4\xe4\xb8\xba\xe8\xae\xad\xe7\xbb\x83\xe6\xa8\xa1\xe5\xbc\x8f\n    net.train()\n    # \xe7\xbd\x91\xe7\xbb\x9c\xe9\x83\xa8\xe5\x88\x86======================================================\xe7\xbb\x93\xe6\x9d\x9f\n\n    # \xe5\x8a\xa0\xe8\xbd\xbd\xe6\x95\xb0\xe6\x8d\xae\xe9\x83\xa8\xe5\x88\x86====================================================\xe5\xbc\x80\xe5\xa7\x8b\n    # \xe8\x87\xaa\xe5\xae\x9a\xe4\xb9\x89\xe5\xb0\x81\xe8\xa3\x85\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\n    train_dataset = yoloDataset(root=opt.file_root, list_file=opt.voc_2012train, train=True, transform=[transforms.ToTensor()])\n    # \xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe5\x8a\xa0\xe8\xbd\xbd\xe5\x99\xa8  shuffle\xef\xbc\x9a\xe6\x89\x93\xe4\xb9\xb1\xe9\xa1\xba\xe5\xba\x8f    num_workers\xef\xbc\x9a\xe7\xba\xbf\xe7\xa8\x8b\xe6\x95\xb0\n    train_loader = DataLoader(train_dataset, batch_size=opt.batch_size, shuffle=True, num_workers=4)\n    test_dataset = yoloDataset(root=opt.test_root, list_file=opt.voc_2007test, train=False, transform=[transforms.ToTensor()])\n    test_loader = DataLoader(test_dataset, batch_size=opt.batch_size, shuffle=False, num_workers=4)\n    # \xe5\x8a\xa0\xe8\xbd\xbd\xe6\x95\xb0\xe6\x8d\xae\xe9\x83\xa8\xe5\x88\x86====================================================\xe7\xbb\x93\xe6\x9d\x9f\n\n    #\xe8\x87\xaa\xe5\xae\x9a\xe4\xb9\x89\xe7\x9a\x84\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x87\xbd\xe6\x95\xb0  7\xe4\xbb\xa3\xe8\xa1\xa8\xe5\xb0\x86\xe5\x9b\xbe\xe5\x83\x8f\xe5\x88\x86\xe4\xb8\xba7x7\xe7\x9a\x84\xe7\xbd\x91\xe6\xa0\xbc   2\xe4\xbb\xa3\xe8\xa1\xa8\xe4\xb8\x80\xe4\xb8\xaa\xe7\xbd\x91\xe6\xa0\xbc\xe9\xa2\x84\xe6\xb5\x8b\xe4\xb8\xa4\xe4\xb8\xaa\xe6\xa1\x86   5\xe4\xbb\xa3\xe8\xa1\xa8 \xce\xbbcoord  \xe6\x9b\xb4\xe9\x87\x8d\xe8\xa7\x868\xe7\xbb\xb4\xe7\x9a\x84\xe5\x9d\x90\xe6\xa0\x87\xe9\xa2\x84\xe6\xb5\x8b     0.5\xe4\xbb\xa3\xe8\xa1\xa8\xe6\xb2\xa1\xe6\x9c\x89object\xe7\x9a\x84bbox\xe7\x9a\x84confidence loss\n    criterion = yoloLoss(7, 2, 5, 0.5)\n    # \xe4\xbc\x98\xe5\x8c\x96\xe5\x99\xa8\n    optimizer = torch.optim.SGD(net.parameters(), lr=opt.learning_rate, momentum=opt.momentum, weight_decay=opt.weight_decay)\n\n    print('\xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\xe6\x9c\x89 %d \xe5\xbc\xa0\xe5\x9b\xbe\xe5\x83\x8f' % (len(train_dataset)))\n    print('\xe4\xb8\x80\xe4\xb8\xaabatch\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f\xe4\xb8\xba %d' % (opt.batch_size))\n    # \xe5\xb0\x86\xe8\xae\xad\xe7\xbb\x83\xe8\xbf\x87\xe7\xa8\x8b\xe7\x9a\x84\xe4\xbf\xa1\xe6\x81\xaf\xe5\x86\x99\xe5\x85\xa5log\xe6\x96\x87\xe4\xbb\xb6\xe4\xb8\xad\n    logfile = open('log/log.txt', 'w')\n    # inf\xe4\xb8\xba\xe6\xad\xa3\xe6\x97\xa0\xe7\xa9\xb7\xe5\xa4\xa7\n    best_test_loss = np.inf\n\n    for epoch in range(opt.num_epochs):\n        if epoch == 1:\n            opt.learning_rate = 0.0005\n        if epoch == 2:\n            opt.learning_rate = 0.00075\n        if epoch == 3:\n            opt.learning_rate = 0.001\n        if epoch == 80:\n            opt.learning_rate = 0.0001\n        if epoch == 100:\n            opt.learning_rate = 0.00001\n        for param_group in optimizer.param_groups:\n            param_group['lr'] = opt.learning_rate\n        # \xe7\xac\xac\xe5\x87\xa0\xe6\xac\xa1epoch  \xe5\x8f\x8a \xe5\xbd\x93\xe5\x89\x8depoch\xe7\x9a\x84\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\n        print('\\n\\n\xe5\xbd\x93\xe5\x89\x8d\xe7\x9a\x84epoch\xe4\xb8\xba %d / %d' % (epoch + 1, opt.num_epochs))\n        print('\xe5\xbd\x93\xe5\x89\x8depoch\xe7\x9a\x84\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87: {}'.format(opt.learning_rate))\n\n        # \xe6\xaf\x8f\xe8\xbd\xaeepoch\xe7\x9a\x84\xe6\x80\xbbloss\n        total_loss = 0.\n        # \xe5\xbc\x80\xe5\xa7\x8b\xe8\xae\xad\xe7\xbb\x83\n        for i, (images, target) in enumerate(train_loader):\n            images = Variable(images)\n            target = Variable(target)\n            if opt.use_gpu:\n                images, target = images.cuda(), target.cuda()\n            # \xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xef\xbc\x8c\xe5\xbe\x97\xe5\x88\xb0\xe9\xa2\x84\xe6\xb5\x8b\xe5\x80\xbc\n            pred = net(images)\n            # \xe8\xae\xa1\xe7\xae\x97\xe6\x8d\x9f\xe5\xa4\xb1  yoloLoss\xe7\xbb\xa7\xe6\x89\xbfnn.Module\xef\xbc\x8c\xe8\xb0\x83\xe7\x94\xa8\xe6\x96\xb9\xe6\xb3\x95\xe5\x90\x8d\xe5\x8d\xb3\xe8\x87\xaa\xe5\x8a\xa8\xe8\xbf\x9b\xe8\xa1\x8c\xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xef\xbc\x8c\xe6\x89\xa7\xe8\xa1\x8cforward\xe6\x96\xb9\xe6\xb3\x95\n            loss = criterion(pred, target)\n            total_loss += loss.data[0]\n            # \xe4\xbc\x98\xe5\x8c\x96\xe5\x99\xa8\xe6\xa2\xaf\xe5\xba\xa6\xe6\xb8\x85\xe9\x9b\xb6\n            optimizer.zero_grad()\n            #loss\xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\n            loss.backward()\n            # \xe6\x9b\xb4\xe6\x96\xb0\xe5\x8f\x82\xe6\x95\xb0\n            optimizer.step()\n            if (i + 1) % opt.print_freq == 0:\n                print('\xe5\x9c\xa8\xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\xe4\xb8\x8a\xef\xbc\x9a\xe5\xbd\x93\xe5\x89\x8depoch\xe4\xb8\xba [%d/%d], Iter [%d/%d] \xe5\xbd\x93\xe5\x89\x8dbatch\xe6\x8d\x9f\xe5\xa4\xb1\xe4\xb8\xba: %.4f, \xe5\xbd\x93\xe5\x89\x8depoch\xe5\x88\xb0\xe7\x9b\xae\xe5\x89\x8d\xe4\xb8\xba\xe6\xad\xa2\xe5\xb9\xb3\xe5\x9d\x87\xe6\x8d\x9f\xe5\xa4\xb1\xe4\xb8\xba: %.4f'\n                      % (epoch + 1, opt.num_epochs, i + 1, len(train_loader), loss.data[0], total_loss / (i + 1)))\n                # \xe7\x94\xbb\xe5\x87\xba\xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\xe7\x9a\x84\xe5\xb9\xb3\xe5\x9d\x87\xe6\x8d\x9f\xe5\xa4\xb1\n                vis.plot_train_val(loss_train=total_loss / (i + 1))\n        # \xe4\xbf\x9d\xe5\xad\x98\xe6\x9c\x80\xe6\x96\xb0\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\n        torch.save(net.state_dict(),opt.current_epoch_model_path)\n# =========================================================\xe7\x9c\x8b\xe5\x88\xb0\xe6\xad\xa4\n        # \xe4\xb8\x80\xe6\xac\xa1epoch\xe9\xaa\x8c\xe8\xaf\x81\n        validation_loss = 0.0\n        # \xe6\xa8\xa1\xe5\x9e\x8b\xe8\xb0\x83\xe6\x95\xb4\xe4\xb8\xba\xe9\xaa\x8c\xe8\xaf\x81\xe6\xa8\xa1\xe5\xbc\x8f\n        net.eval()\n        # \xe6\xaf\x8f\xe8\xbd\xaeepoch\xe4\xb9\x8b\xe5\x90\x8e\xe7\x94\xa8VOC2007\xe6\xb5\x8b\xe8\xaf\x95\xe9\x9b\x86\xe8\xbf\x9b\xe8\xa1\x8c\xe9\xaa\x8c\xe8\xaf\x81\n        for i, (images, target) in enumerate(test_loader):\n            images = Variable(images, volatile=True)\n            target = Variable(target, volatile=True)\n            if opt.use_gpu:\n                images, target = images.cuda(), target.cuda()\n            # \xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe5\xbe\x97\xe5\x88\xb0\xe9\xa2\x84\xe6\xb5\x8b\xe5\x80\xbc\n            pred = net(images)\n            # loss\n            loss = criterion(pred, target)\n            validation_loss += loss.data[0]\n        # \xe8\xae\xa1\xe7\xae\x97\xe5\x9c\xa8VOC2007\xe6\xb5\x8b\xe8\xaf\x95\xe9\x9b\x86\xe4\xb8\x8a\xe7\x9a\x84\xe5\xb9\xb3\xe5\x9d\x87\xe6\x8d\x9f\xe5\xa4\xb1\n        validation_loss /= len(test_loader)\n        # \xe7\x94\xbb\xe5\x87\xba\xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86\xe7\x9a\x84\xe5\xb9\xb3\xe5\x9d\x87\xe6\x8d\x9f\xe5\xa4\xb1\n        vis.plot_train_val(loss_val=validation_loss)\n        # \xe8\xae\xad\xe7\xbb\x83\xe6\xa8\xa1\xe5\x9e\x8b\xe7\x9a\x84\xe7\x9b\xae\xe6\xa0\x87\xe6\x98\xaf \xe5\x9c\xa8\xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86\xe4\xb8\x8a\xe7\x9a\x84loss\xe6\x9c\x80\xe5\xb0\x8f\n        # \xe4\xbf\x9d\xe5\xad\x98\xe5\x88\xb0\xe7\x9b\xae\xe5\x89\x8d\xe4\xb8\xba\xe6\xad\xa2 \xe5\x9c\xa8\xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86\xe4\xb8\x8a\xe7\x9a\x84loss\xe6\x9c\x80\xe5\xb0\x8f \xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\n        if best_test_loss > validation_loss:\n            best_test_loss = validation_loss\n            print('\xe5\xbd\x93\xe5\x89\x8d\xe5\xbe\x97\xe5\x88\xb0\xe6\x9c\x80\xe5\xa5\xbd\xe7\x9a\x84\xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86\xe7\x9a\x84\xe5\xb9\xb3\xe5\x9d\x87\xe6\x8d\x9f\xe5\xa4\xb1\xe4\xb8\xba  %.5f' % best_test_loss)\n            torch.save(net.state_dict(),opt.best_test_loss_model_path)\n        # \xe5\xb0\x86\xe5\xbd\x93\xe5\x89\x8depoch\xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0\xe5\x86\x99\xe5\x85\xa5log\xe6\x96\x87\xe4\xbb\xb6\xe4\xb8\xad\n        logfile.writelines(str(epoch) + '\\t' + str(validation_loss) + '\\n')\n        logfile.flush()\n\ndef predict():\n    # fasle \xe8\xbf\x94\xe5\x9b\x9e \xe6\x9c\xaa\xe8\xae\xad\xe7\xbb\x83\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\n    predict_model = vgg16(pretrained=False)\n    # \xe4\xbf\xae\xe6\x94\xb9\xe7\xbd\x91\xe7\xbb\x9c\xe7\xbb\x93\xe6\x9e\x84\n    predict_model.classifier = nn.Sequential(\n                nn.Linear(512 * 7 * 7, 4096),\n                nn.ReLU(True),\n                nn.Dropout(),\n                #nn.Linear(4096, 4096),\n                #nn.ReLU(True),\n                #nn.Dropout(),\n                nn.Linear(4096, 1470),\n            )\n    # \xe5\xb0\x86\xe6\xa8\xa1\xe5\x9e\x8b\xe5\x8a\xa0\xe8\xbd\xbd\xe5\x88\xb0CPU\n    predict_model.load_state_dict(torch.load(opt.load_model_path,map_location=lambda  storage,loc:storage))\n    # \xe6\xa8\xa1\xe5\x9e\x8b\xe6\x94\xb9\xe4\xb8\xba\xe9\xa2\x84\xe6\xb5\x8b\xe6\xa8\xa1\xe5\xbc\x8f\n    predict_model.eval()\n    # \xe5\xa6\x82\xe6\x9e\x9cGPU\xe5\x8f\xaf\xe7\x94\xa8\xef\xbc\x8c\xe5\x8a\xa0\xe8\xbd\xbd\xe5\x88\xb0GPU\n    if opt.use_gpu:\n        predict_model.cuda()\n    # \xe6\xb5\x8b\xe8\xaf\x95\xe9\x9b\x86\xe7\x85\xa7\xe7\x89\x87\xe5\x9c\xb0\xe5\x9d\x80\n    test_img_dir = opt.test_img_dir\n    image = cv2.imread(test_img_dir)\n    # result\xe4\xb8\xad\xe5\x86\x85\xe5\xae\xb9\xe4\xb8\xba  \xe5\xb7\xa6\xe4\xb8\x8a\xe8\xa7\x92\xe5\x9d\x90\xe6\xa0\x87\xe3\x80\x81\xe5\x8f\xb3\xe4\xb8\x8b\xe8\xa7\x92\xe5\x9d\x90\xe6\xa0\x87\xe3\x80\x81\xe7\xb1\xbb\xe5\x88\xab\xe5\x90\x8d\xe3\x80\x81\xe8\xbe\x93\xe5\x85\xa5\xe7\x9a\x84\xe6\xb5\x8b\xe8\xaf\x95\xe5\x9b\xbe\xe5\x9c\xb0\xe5\x9d\x80\xe3\x80\x81\xe9\xa2\x84\xe6\xb5\x8b\xe7\xb1\xbb\xe5\x88\xab\xe7\x9a\x84\xe5\x8f\xaf\xe8\x83\xbd\xe6\x80\xa7\n    result = predict_result(predict_model, test_img_dir)\n    for left_up, right_bottom, class_name, _, prob in result:\n        # \xe5\xb0\x86\xe9\xa2\x84\xe6\xb5\x8b\xe6\xa1\x86\xe6\xb7\xbb\xe5\x8a\xa0\xe5\x88\xb0\xe6\xb5\x8b\xe8\xaf\x95\xe5\x9b\xbe\xe7\x89\x87\xe4\xb8\xad\n        cv2.rectangle(image, left_up, right_bottom, (0, 255, 0), 2)\n        # \xe9\xa2\x84\xe6\xb5\x8b\xe6\xa1\x86\xe7\x9a\x84\xe5\xb7\xa6\xe4\xb8\x8a\xe8\xa7\x92\xe5\x86\x99\xe5\x85\xa5 \xe6\x89\x80\xe5\xb1\x9e\xe7\xb1\xbb\xe5\x88\xab\n        cv2.putText(image, class_name, left_up, cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 1, cv2.LINE_AA)\n        print(prob)\n    # \xe5\xb0\x86\xe6\xb5\x8b\xe8\xaf\x95\xe7\xbb\x93\xe6\x9e\x9c\xe5\x86\x99\xe5\x85\xa5\n    cv2.imwrite(opt.result_img_dir,image)\n\ndef  eval():\n    '''\n    \xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86 \xe4\xbd\xbf\xe7\x94\xa8voc2012\xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\xe5\x8e\xbb\xe9\xaa\x8c\xe8\xaf\x81\n    '''\n    # defaultdict\xe7\xb1\xbb\xe5\xb0\xb1\xe5\xa5\xbd\xe5\x83\x8f\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaadict\xe5\xad\x97\xe5\x85\xb8\xef\xbc\x8c\xe4\xbd\xbf\xe7\x94\xa8list\xe7\xb1\xbb\xe5\x9e\x8b\xe6\x9d\xa5\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\n    target = defaultdict(list)\n    preds = defaultdict(list)\n\n    image_list = []  # image path list\n    # \xe4\xbd\xbf\xe7\x94\xa8voc_2012\xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\xe8\xbf\x9b\xe8\xa1\x8c\xe9\xaa\x8c\xe8\xaf\x81\n    f = open(opt.voc_2012train)\n    lines = f.readlines()\n    file_list = []\n    for line in lines:\n        splited = line.strip().split()\n        file_list.append(splited)\n    f.close()\n    print('---\xe5\x87\x86\xe5\xa4\x87\xe7\x9c\x9f\xe5\xae\x9e\xe6\xa0\x87\xe7\xad\xbe---')\n    for image_file in tqdm(file_list):\n        image_id = image_file[0]\n        image_list.append(image_id)\n        num_obj = int(image_file[1])\n        for i in range(num_obj):\n            x1 = int(image_file[2+5*i])\n            y1 = int(image_file[3+5*i])\n            x2 = int(image_file[4+5*i])\n            y2 = int(image_file[5+5*i])\n            c = int(image_file[6+5*i])\n            class_name = opt.VOC_CLASSES[c]\n            target[(image_id,class_name)].append([x1,y1,x2,y2])\n\n    print('---\xe5\xbc\x80\xe5\xa7\x8b\xe9\xa2\x84\xe6\xb5\x8b---')\n    model = vgg16(pretrained=False)\n    model.classifier = nn.Sequential(\n            nn.Linear(512 * 7 * 7, 4096),\n            nn.ReLU(True),\n            nn.Dropout(),\n            # nn.Linear(4096, 4096),\n            # nn.ReLU(True),\n            # nn.Dropout(),\n            nn.Linear(4096, 1470),\n        )\n    # \xe6\xa8\xa1\xe5\x9e\x8b\xe5\x8a\xa0\xe8\xbd\xbd\xe5\x88\xb0CPU\xe4\xb8\xad\n    model.load_state_dict(torch.load(opt.best_test_loss_model_path,map_location=lambda  storage,loc:storage))\n    # \xe8\xb0\x83\xe6\x95\xb4\xe4\xb8\xba\xe9\xa2\x84\xe6\xb5\x8b\xe6\xa8\xa1\xe5\xbc\x8f\n    model.eval()\n    if opt.use_gpu:\n        model.cuda()\n\n    for image_path in tqdm(image_list):\n        # result\xe4\xb8\xad\xe5\x86\x85\xe5\xae\xb9\xe4\xb8\xba  \xe5\xb7\xa6\xe4\xb8\x8a\xe8\xa7\x92\xe5\x9d\x90\xe6\xa0\x87\xe3\x80\x81\xe5\x8f\xb3\xe4\xb8\x8b\xe8\xa7\x92\xe5\x9d\x90\xe6\xa0\x87\xe3\x80\x81\xe7\xb1\xbb\xe5\x88\xab\xe5\x90\x8d\xe3\x80\x81\xe8\xbe\x93\xe5\x85\xa5\xe5\x9b\xbe\xe5\x83\x8f\xe5\x9c\xb0\xe5\x9d\x80\xe3\x80\x81\xe9\xa2\x84\xe6\xb5\x8b\xe7\xb1\xbb\xe5\x88\xab\xe7\x9a\x84\xe5\x8f\xaf\xe8\x83\xbd\xe6\x80\xa7\n        result = predict_result(model, image_path,root_path=opt.file_root)\n        for (x1, y1), (x2, y2), class_name, image_id, prob in result:  # image_id is actually image_path\n                preds[class_name].append([image_id, prob, x1, y1, x2, y2])\n    print('\\n---\xe5\xbc\x80\xe5\xa7\x8b\xe8\xaf\x84\xe4\xbc\xb0---')\n    voc_eval(preds, target, VOC_CLASSES=opt.VOC_CLASSES)\n\n\n# \xe4\xb8\xbb\xe5\x87\xbd\xe6\x95\xb0\nif __name__ == '__main__':\n    # print()\n    # \xe5\x91\xbd\xe4\xbb\xa4\xe8\xa1\x8c\xe5\xb7\xa5\xe5\x85\xb7\n    import fire\n    fire.Fire()\n\n    # eval()\n    train()\n    # predict()\n\n\n\n"""
Yolov1_pytorch/main_resnet.py,9,"b'from collections import defaultdict\n\nimport cv2\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\nfrom config import opt\nfrom data.dataset import yoloDataset\nfrom models.resnet import  resnet152_bo,resnet152\nfrom utils.visualize import Visualizer\nfrom utils.yoloLoss import yoloLoss\nfrom utils.predictUtils import predict_result\nfrom utils.predictUtils import voc_eval\nfrom utils.predictUtils import  voc_ap\n\n\ndef train():\n    vis=Visualizer(opt.env)\n    # \xe7\xbd\x91\xe7\xbb\x9c\xe9\x83\xa8\xe5\x88\x86======================================================\xe5\xbc\x80\xe5\xa7\x8b\n    # True\xe5\x88\x99\xe8\xbf\x94\xe5\x9b\x9e\xe9\xa2\x84\xe8\xae\xad\xe7\xbb\x83\xe5\xa5\xbd\xe7\x9a\x84resnet152_bo\xe6\xa8\xa1\xe5\x9e\x8b\n    net=resnet152_bo(resnet152(pretrained=True))\n    # \xe5\xb0\x86\xe6\xa8\xa1\xe5\x9e\x8b\xe5\x8a\xa0\xe8\xbd\xbd\xe5\x88\xb0\xe5\x86\x85\xe5\xad\x98\xe4\xb8\xad\xef\xbc\x88CPU\xef\xbc\x89\n    if opt.load_model_path:\n        net.load_state_dict(torch.load(opt.load_model_path,map_location=lambda  storage,loc:storage))\n    # \xe5\x86\x8d\xe5\xb0\x86\xe6\xa8\xa1\xe5\x9e\x8b\xe8\xbd\xac\xe7\xa7\xbb\xe5\x88\xb0GPU\xe4\xb8\x8a\n    if opt.use_gpu:\n        net.cuda()\n    # \xe8\xbe\x93\xe5\x87\xba\xe7\xbd\x91\xe7\xbb\x9c\xe7\xbb\x93\xe6\x9e\x84\n    print(net)\n    print(\'\xe5\x8a\xa0\xe8\xbd\xbd\xe5\xa5\xbd\xe9\xa2\x84\xe5\x85\x88\xe8\xae\xad\xe7\xbb\x83\xe5\xa5\xbd\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\')\n    # \xe5\xb0\x86\xe6\xa8\xa1\xe5\x9e\x8b\xe8\xb0\x83\xe6\x95\xb4\xe4\xb8\xba\xe8\xae\xad\xe7\xbb\x83\xe6\xa8\xa1\xe5\xbc\x8f\n    net.train()\n    # \xe7\xbd\x91\xe7\xbb\x9c\xe9\x83\xa8\xe5\x88\x86======================================================\xe7\xbb\x93\xe6\x9d\x9f\n\n    # \xe5\x8a\xa0\xe8\xbd\xbd\xe6\x95\xb0\xe6\x8d\xae\xe9\x83\xa8\xe5\x88\x86====================================================\xe5\xbc\x80\xe5\xa7\x8b\n    # \xe8\x87\xaa\xe5\xae\x9a\xe4\xb9\x89\xe5\xb0\x81\xe8\xa3\x85\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\n    train_dataset = yoloDataset(root=opt.file_root, list_file=opt.voc_2012train, train=True, transform=[transforms.ToTensor()])\n    # \xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe5\x8a\xa0\xe8\xbd\xbd\xe5\x99\xa8  shuffle\xef\xbc\x9a\xe6\x89\x93\xe4\xb9\xb1\xe9\xa1\xba\xe5\xba\x8f    num_workers\xef\xbc\x9a\xe7\xba\xbf\xe7\xa8\x8b\xe6\x95\xb0\n    train_loader = DataLoader(train_dataset, batch_size=opt.batch_size, shuffle=True, num_workers=4)\n    test_dataset = yoloDataset(root=opt.test_root, list_file=opt.voc_2007test, train=False, transform=[transforms.ToTensor()])\n    test_loader = DataLoader(test_dataset, batch_size=opt.batch_size, shuffle=False, num_workers=4)\n    # \xe5\x8a\xa0\xe8\xbd\xbd\xe6\x95\xb0\xe6\x8d\xae\xe9\x83\xa8\xe5\x88\x86====================================================\xe7\xbb\x93\xe6\x9d\x9f\n\n    #\xe8\x87\xaa\xe5\xae\x9a\xe4\xb9\x89\xe7\x9a\x84\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x87\xbd\xe6\x95\xb0  7\xe4\xbb\xa3\xe8\xa1\xa8\xe5\xb0\x86\xe5\x9b\xbe\xe5\x83\x8f\xe5\x88\x86\xe4\xb8\xba7x7\xe7\x9a\x84\xe7\xbd\x91\xe6\xa0\xbc   2\xe4\xbb\xa3\xe8\xa1\xa8\xe4\xb8\x80\xe4\xb8\xaa\xe7\xbd\x91\xe6\xa0\xbc\xe9\xa2\x84\xe6\xb5\x8b\xe4\xb8\xa4\xe4\xb8\xaa\xe6\xa1\x86   5\xe4\xbb\xa3\xe8\xa1\xa8 \xce\xbbcoord  \xe6\x9b\xb4\xe9\x87\x8d\xe8\xa7\x868\xe7\xbb\xb4\xe7\x9a\x84\xe5\x9d\x90\xe6\xa0\x87\xe9\xa2\x84\xe6\xb5\x8b     0.5\xe4\xbb\xa3\xe8\xa1\xa8\xe6\xb2\xa1\xe6\x9c\x89object\xe7\x9a\x84bbox\xe7\x9a\x84confidence loss\n    criterion = yoloLoss(7, 2, 5, 0.5)\n    learning_rate=opt.learning_rate\n    # \xe4\xbc\x98\xe5\x8c\x96\xe5\x99\xa8\n    optimizer = torch.optim.SGD(net.parameters(), lr=opt.learning_rate, momentum=opt.momentum, weight_decay=opt.weight_decay)\n    #optimizer = torch.optim.Adam(net.parameters(), lr=opt.learning_rate, weight_decay=opt.weight_decay)\n    print(\'\xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\xe6\x9c\x89 %d \xe5\xbc\xa0\xe5\x9b\xbe\xe5\x83\x8f\' % (len(train_dataset)))\n    print(\'\xe4\xb8\x80\xe4\xb8\xaabatch\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f\xe4\xb8\xba %d\' % (opt.batch_size))\n    # \xe5\xb0\x86\xe8\xae\xad\xe7\xbb\x83\xe8\xbf\x87\xe7\xa8\x8b\xe7\x9a\x84\xe4\xbf\xa1\xe6\x81\xaf\xe5\x86\x99\xe5\x85\xa5log\xe6\x96\x87\xe4\xbb\xb6\xe4\xb8\xad\n    logfile = open(\'log/log.txt\', \'w\')\n    # inf\xe4\xb8\xba\xe6\xad\xa3\xe6\x97\xa0\xe7\xa9\xb7\xe5\xa4\xa7\n    best_test_loss = np.inf\n\n    for epoch in range(opt.num_epochs):\n        if epoch == 1:\n            learning_rate = 0.0005\n        if epoch == 2:\n            learning_rate = 0.00075\n        if epoch == 3:\n            learning_rate = 0.001\n        if epoch == 80:\n            learning_rate = 0.0001\n        if epoch == 100:\n            learning_rate = 0.00001\n        for param_group in optimizer.param_groups:\n            param_group[\'lr\'] = learning_rate\n        # \xe7\xac\xac\xe5\x87\xa0\xe6\xac\xa1epoch  \xe5\x8f\x8a \xe5\xbd\x93\xe5\x89\x8depoch\xe7\x9a\x84\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\n        print(\'\\n\\n\xe5\xbd\x93\xe5\x89\x8d\xe7\x9a\x84epoch\xe4\xb8\xba %d / %d\' % (epoch + 1, opt.num_epochs))\n        print(\'\xe5\xbd\x93\xe5\x89\x8depoch\xe7\x9a\x84\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87: {}\'.format(learning_rate))\n\n        # \xe6\xaf\x8f\xe8\xbd\xaeepoch\xe7\x9a\x84\xe6\x80\xbbloss\n        total_loss = 0.\n        # \xe5\xbc\x80\xe5\xa7\x8b\xe8\xae\xad\xe7\xbb\x83\n        for i, (images, target) in enumerate(train_loader):\n            images = Variable(images)\n            target = Variable(target)\n            if opt.use_gpu:\n                images, target = images.cuda(), target.cuda()\n            # \xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xef\xbc\x8c\xe5\xbe\x97\xe5\x88\xb0\xe9\xa2\x84\xe6\xb5\x8b\xe5\x80\xbc\n            pred = net(images)\n            # \xe8\xae\xa1\xe7\xae\x97\xe6\x8d\x9f\xe5\xa4\xb1  yoloLoss\xe7\xbb\xa7\xe6\x89\xbfnn.Module\xef\xbc\x8c\xe8\xb0\x83\xe7\x94\xa8\xe6\x96\xb9\xe6\xb3\x95\xe5\x90\x8d\xe5\x8d\xb3\xe8\x87\xaa\xe5\x8a\xa8\xe8\xbf\x9b\xe8\xa1\x8c\xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xef\xbc\x8c\xe6\x89\xa7\xe8\xa1\x8cforward\xe6\x96\xb9\xe6\xb3\x95\n            loss = criterion(pred, target)\n            total_loss += loss.data[0]\n            # \xe4\xbc\x98\xe5\x8c\x96\xe5\x99\xa8\xe6\xa2\xaf\xe5\xba\xa6\xe6\xb8\x85\xe9\x9b\xb6\n            optimizer.zero_grad()\n            #loss\xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\n            loss.backward()\n            # \xe6\x9b\xb4\xe6\x96\xb0\xe5\x8f\x82\xe6\x95\xb0\n            optimizer.step()\n            if (i + 1) % opt.print_freq == 0:\n                print(\'\xe5\x9c\xa8\xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\xe4\xb8\x8a\xef\xbc\x9a\xe5\xbd\x93\xe5\x89\x8depoch\xe4\xb8\xba [%d/%d], Iter [%d/%d] \xe5\xbd\x93\xe5\x89\x8dbatch\xe6\x8d\x9f\xe5\xa4\xb1\xe4\xb8\xba: %.4f, \xe5\xbd\x93\xe5\x89\x8depoch\xe5\x88\xb0\xe7\x9b\xae\xe5\x89\x8d\xe4\xb8\xba\xe6\xad\xa2\xe5\xb9\xb3\xe5\x9d\x87\xe6\x8d\x9f\xe5\xa4\xb1\xe4\xb8\xba: %.4f\'\n                      % (epoch + 1, opt.num_epochs, i + 1, len(train_loader), loss.data[0], total_loss / (i + 1)))\n                # \xe7\x94\xbb\xe5\x87\xba\xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\xe7\x9a\x84\xe5\xb9\xb3\xe5\x9d\x87\xe6\x8d\x9f\xe5\xa4\xb1\n                vis.plot_train_val(loss_train=total_loss / (i + 1))\n        # \xe4\xbf\x9d\xe5\xad\x98\xe6\x9c\x80\xe6\x96\xb0\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\n        torch.save(net.state_dict(),opt.current_epoch_model_path)\n        vis.log(""epoch:{epoch},lr:{lr}"".format(\n            epoch=epoch, lr=learning_rate))\n\n        # =========================================================\xe7\x9c\x8b\xe5\x88\xb0\xe6\xad\xa4\n        # \xe4\xb8\x80\xe6\xac\xa1epoch\xe9\xaa\x8c\xe8\xaf\x81\n        validation_loss = 0.0\n        # \xe6\xa8\xa1\xe5\x9e\x8b\xe8\xb0\x83\xe6\x95\xb4\xe4\xb8\xba\xe9\xaa\x8c\xe8\xaf\x81\xe6\xa8\xa1\xe5\xbc\x8f\n        net.eval()\n        # \xe6\xaf\x8f\xe8\xbd\xaeepoch\xe4\xb9\x8b\xe5\x90\x8e\xe7\x94\xa8VOC2007\xe6\xb5\x8b\xe8\xaf\x95\xe9\x9b\x86\xe8\xbf\x9b\xe8\xa1\x8c\xe9\xaa\x8c\xe8\xaf\x81\n        for i, (images, target) in enumerate(test_loader):\n            images = Variable(images, volatile=True)\n            target = Variable(target, volatile=True)\n            if opt.use_gpu:\n                images, target = images.cuda(), target.cuda()\n            # \xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe5\xbe\x97\xe5\x88\xb0\xe9\xa2\x84\xe6\xb5\x8b\xe5\x80\xbc\n            pred = net(images)\n            # loss\n            loss = criterion(pred, target)\n            validation_loss += loss.data[0]\n        # \xe8\xae\xa1\xe7\xae\x97\xe5\x9c\xa8VOC2007\xe6\xb5\x8b\xe8\xaf\x95\xe9\x9b\x86\xe4\xb8\x8a\xe7\x9a\x84\xe5\xb9\xb3\xe5\x9d\x87\xe6\x8d\x9f\xe5\xa4\xb1\n        validation_loss /= len(test_loader)\n        # \xe7\x94\xbb\xe5\x87\xba\xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86\xe7\x9a\x84\xe5\xb9\xb3\xe5\x9d\x87\xe6\x8d\x9f\xe5\xa4\xb1\n        vis.plot_train_val(loss_val=validation_loss)\n        # \xe8\xae\xad\xe7\xbb\x83\xe6\xa8\xa1\xe5\x9e\x8b\xe7\x9a\x84\xe7\x9b\xae\xe6\xa0\x87\xe6\x98\xaf \xe5\x9c\xa8\xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86\xe4\xb8\x8a\xe7\x9a\x84loss\xe6\x9c\x80\xe5\xb0\x8f\n        # \xe4\xbf\x9d\xe5\xad\x98\xe5\x88\xb0\xe7\x9b\xae\xe5\x89\x8d\xe4\xb8\xba\xe6\xad\xa2 \xe5\x9c\xa8\xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86\xe4\xb8\x8a\xe7\x9a\x84loss\xe6\x9c\x80\xe5\xb0\x8f \xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\n        if best_test_loss > validation_loss:\n            best_test_loss = validation_loss\n            print(\'\xe5\xbd\x93\xe5\x89\x8d\xe5\xbe\x97\xe5\x88\xb0\xe6\x9c\x80\xe5\xa5\xbd\xe7\x9a\x84\xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86\xe7\x9a\x84\xe5\xb9\xb3\xe5\x9d\x87\xe6\x8d\x9f\xe5\xa4\xb1\xe4\xb8\xba  %.5f\' % best_test_loss)\n            torch.save(net.state_dict(),opt.best_test_loss_model_path)\n        # \xe5\xb0\x86\xe5\xbd\x93\xe5\x89\x8depoch\xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0\xe5\x86\x99\xe5\x85\xa5log\xe6\x96\x87\xe4\xbb\xb6\xe4\xb8\xad\n        logfile.writelines(str(epoch) + \'\\t\' + str(validation_loss) + \'\\n\')\n        logfile.flush()\n\ndef predict():\n    # fasle \xe8\xbf\x94\xe5\x9b\x9e \xe6\x9c\xaa\xe8\xae\xad\xe7\xbb\x83\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\n    predict_model = resnet152_bo(resnet152(pretrained=True))\n\n    predict_model.load_state_dict(torch.load(opt.load_model_path,map_location=lambda  storage,loc:storage))\n    # \xe6\xa8\xa1\xe5\x9e\x8b\xe6\x94\xb9\xe4\xb8\xba\xe9\xa2\x84\xe6\xb5\x8b\xe6\xa8\xa1\xe5\xbc\x8f\n    predict_model.eval()\n    # \xe5\xa6\x82\xe6\x9e\x9cGPU\xe5\x8f\xaf\xe7\x94\xa8\xef\xbc\x8c\xe5\x8a\xa0\xe8\xbd\xbd\xe5\x88\xb0GPU\n    if opt.use_gpu:\n        predict_model.cuda()\n    # \xe6\xb5\x8b\xe8\xaf\x95\xe9\x9b\x86\xe7\x85\xa7\xe7\x89\x87\xe5\x9c\xb0\xe5\x9d\x80\n    test_img_dir = opt.test_img_dir\n    image = cv2.imread(test_img_dir)\n    # result\xe4\xb8\xad\xe5\x86\x85\xe5\xae\xb9\xe4\xb8\xba  \xe5\xb7\xa6\xe4\xb8\x8a\xe8\xa7\x92\xe5\x9d\x90\xe6\xa0\x87\xe3\x80\x81\xe5\x8f\xb3\xe4\xb8\x8b\xe8\xa7\x92\xe5\x9d\x90\xe6\xa0\x87\xe3\x80\x81\xe7\xb1\xbb\xe5\x88\xab\xe5\x90\x8d\xe3\x80\x81\xe8\xbe\x93\xe5\x85\xa5\xe7\x9a\x84\xe6\xb5\x8b\xe8\xaf\x95\xe5\x9b\xbe\xe5\x9c\xb0\xe5\x9d\x80\xe3\x80\x81\xe9\xa2\x84\xe6\xb5\x8b\xe7\xb1\xbb\xe5\x88\xab\xe7\x9a\x84\xe5\x8f\xaf\xe8\x83\xbd\xe6\x80\xa7\n    result = predict_result(predict_model, test_img_dir)\n    for left_up, right_bottom, class_name, _, prob in result:\n        # \xe5\xb0\x86\xe9\xa2\x84\xe6\xb5\x8b\xe6\xa1\x86\xe6\xb7\xbb\xe5\x8a\xa0\xe5\x88\xb0\xe6\xb5\x8b\xe8\xaf\x95\xe5\x9b\xbe\xe7\x89\x87\xe4\xb8\xad\n        cv2.rectangle(image, left_up, right_bottom, (0, 255, 0), 2)\n        # \xe9\xa2\x84\xe6\xb5\x8b\xe6\xa1\x86\xe7\x9a\x84\xe5\xb7\xa6\xe4\xb8\x8a\xe8\xa7\x92\xe5\x86\x99\xe5\x85\xa5 \xe6\x89\x80\xe5\xb1\x9e\xe7\xb1\xbb\xe5\x88\xab\n        cv2.putText(image, class_name, left_up, cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 1, cv2.LINE_AA)\n        print(prob)\n    # \xe5\xb0\x86\xe6\xb5\x8b\xe8\xaf\x95\xe7\xbb\x93\xe6\x9e\x9c\xe5\x86\x99\xe5\x85\xa5\n    cv2.imwrite(opt.result_img_dir,image)\n\n\n\n# \xe4\xb8\xbb\xe5\x87\xbd\xe6\x95\xb0\nif __name__ == \'__main__\':\n\n    # \xe5\x91\xbd\xe4\xbb\xa4\xe8\xa1\x8c\xe5\xb7\xa5\xe5\x85\xb7\n    import fire\n    fire.Fire()\n\n    train()\n    # predict()\n\n\n\n'"
Yolov3_pytorch/main.py,23,"b'from __future__ import division\nfrom torch.utils.data import DataLoader\nimport torch.optim as optim\nfrom models.models import *\nfrom utils.utils import *\nfrom datasets.datasets import *\nimport os\nimport time\nimport datetime\nimport resource\nfrom utils.visualize import Visualizer\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torch.autograd import Variable\nfrom utils.config import opt_train,opt_test,opt_detect\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom matplotlib.ticker import NullLocator\nfrom torchnet import meter  #\xe4\xbb\xaa\xe8\xa1\xa8  \xe7\x94\xa8\xe6\x9d\xa5\xe6\x98\xbe\xe7\xa4\xbaloss\xe7\xad\x89\xe5\x9b\xbe\xe5\xbd\xa2\ndef train():\n    opt = opt_train\n    os.makedirs(\'checkpoints\', exist_ok=True)  #\xe7\x94\xa8\xe4\xba\x8e\xe5\xad\x98\xe5\x82\xa8\xe8\xae\xad\xe7\xbb\x83\xe5\x90\x8e\xe7\x9a\x84\xe7\xbd\x91\xe7\xbb\x9c\xe6\xa8\xa1\xe5\x9e\x8b\n\n    # \xe5\xae\x9a\xe4\xb9\x89\xe5\x8f\xaf\xe8\xa7\x86\xe5\x8c\x96\xe5\xaf\xb9\xe8\xb1\xa1\n    vis = Visualizer(\'YOLO v3 on coco\')\n\n    # Get data configuration\n    # \xe8\x8e\xb7\xe5\x8f\x96dataloader\xe9\x85\x8d\xe7\xbd\xae\n    data_config     = parse_data_config(opt.data_config_path)\n    num_classes = int(data_config[\'classes\'])\n    # \xe6\x8b\xbf\xe5\x88\xb0\xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\n    train_path      = data_config[\'train\']\n\n    #hyperparams \xe5\x8d\xb3cfg\xe4\xb8\xad\xe7\x9a\x84[net]\xe9\x83\xa8\xe5\x88\x86\xef\xbc\x8c\xe7\xbd\x91\xe7\xbb\x9c\xe8\xae\xad\xe7\xbb\x83\xe7\x9a\x84\xe8\xb6\x85\xe5\x8f\x82\xe6\x95\xb0\n    hyperparams     = parse_model_config(opt.model_config_path)[0]   # model_config_path\xef\xbc\x9a\xe6\xa8\xa1\xe5\x9e\x8b\xe7\xbd\x91\xe7\xbb\x9c\xe7\xbb\x93\xe6\x9e\x84cf\xe6\x96\x87\xe4\xbb\xb6\n    learning_rate   = float(hyperparams[\'learning_rate\'])\n    momentum        = float(hyperparams[\'momentum\'])\n    decay           = float(hyperparams[\'decay\'])\n    burn_in         = int(hyperparams[\'burn_in\'])\n\n    # Initiate model\n    # \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe6\xa8\xa1\xe5\x9e\x8b\n    model = Darknet(opt.model_config_path)   # model_config_path\xef\xbc\x9a\xe6\xa8\xa1\xe5\x9e\x8b\xe7\xbd\x91\xe7\xbb\x9c\xe7\xbb\x93\xe6\x9e\x84\n\n   # \xe8\xae\xad\xe7\xbb\x83\xe4\xb8\x8d\xe5\x86\x8d\xe4\xbf\x9d\xe5\xad\x98\xe4\xb8\xba.weights\xe6\xa0\xbc\xe5\xbc\x8f\xe6\xa8\xa1\xe5\x9e\x8b\n    if opt.load_model_path:\n        # \xe5\x8a\xa0\xe8\xbd\xbd\xe9\xa2\x84\xe8\xae\xad\xe7\xbb\x83\xe5\xa5\xbd\xe7\x9a\x84\xe7\x9a\x84yolo v3\xe6\xa8\xa1\xe5\x9e\x8b  \xe5\x92\x8c \xe4\xbc\x98\xe5\x8c\x96\xe5\x99\xa8\xe7\x8a\xb6\xe6\x80\x81\n        print(\'\xe5\x8a\xa0\xe8\xbd\xbd\xe5\xb7\xb2\xe8\xae\xad\xe7\xbb\x83\xe5\xa5\xbd\xe7\x9a\x84yolo v3\xe6\xa8\xa1\xe5\x9e\x8b\')\n        #\xe5\xa4\x9aGPU\n        checkpoint = torch.load(opt.load_model_path, map_location=\'cpu\')\n        model.load_state_dict(checkpoint[\'model\'])\n        # \xe5\xa4\x9aGPU\xe6\x9c\x89\xe9\x97\xae\xe9\xa2\x98\xef\xbc\x8c\xe4\xb8\x8d\xe5\x8f\xaf\xe7\x94\xa8\n        # if torch.cuda.device_count() > 1:\n        #     print(\'Using \', torch.cuda.device_count(), \' GPUs\')\n        #     model = nn.DataParallel(model)\n        model.train() # \xe8\xbd\xac\xe4\xb8\xba\xe8\xae\xad\xe7\xbb\x83\xe6\xa8\xa1\xe5\xbc\x8f\n\n        del checkpoint  # current, saved\n    else:\n        print(\'\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96yolo v3\xe6\xa8\xa1\xe5\x9e\x8b\xe5\x8f\x82\xe6\x95\xb0\')\n        model.apply(weights_init_normal)\n        # \xe5\xa4\x9aGPU\xe6\x9c\x89\xe9\x97\xae\xe9\xa2\x98\xef\xbc\x8c\xe4\xb8\x8d\xe5\x8f\xaf\xe7\x94\xa8\n        # if torch.cuda.device_count() > 1:\n        #     print(\'Using \', torch.cuda.device_count(), \' GPUs\')\n        #     model = nn.DataParallel(model)\n        model.train() # \xe8\xbd\xac\xe4\xb8\xba\xe8\xae\xad\xe7\xbb\x83\xe6\xa8\xa1\xe5\xbc\x8f\n\n\n    if opt.use_cuda:\n        model = model.cuda()\n        torch.backends.cudnn.benchmark = True\n\n    # # \xe6\x89\x93\xe5\x8d\xb0\xe6\xa8\xa1\xe5\x9e\x8b\xe4\xbf\xa1\xe6\x81\xaf\n    # modelinfo(model)\n\n    # Get dataloader\n    dataloader = torch.utils.data.DataLoader(\n        ListDataset(train_path),\n        batch_size=opt.batch_size, shuffle=False, num_workers=opt.n_cpu)\n    # \xe8\xae\xbe\xe7\xbd\xae\xe5\xa5\xbd \xe9\xbb\x98\xe8\xae\xa4\xe6\x96\xb0\xe5\xbb\xba\xe7\x9a\x84tensor\xe7\xb1\xbb\xe5\x9e\x8b\n    Tensor = torch.cuda.FloatTensor if opt.use_cuda else torch.FloatTensor\n    \n    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, dampening=0, weight_decay=decay)\n    \n    # \xe8\xae\xa1\xe7\xae\x97\xe6\x89\x80\xe6\x9c\x89\xe4\xb9\xa6\xe7\x9a\x84\xe5\xb9\xb3\xe5\x9d\x87\xe6\x95\xb0\xe5\x92\x8c\xe6\xa0\x87\xe5\x87\x86\xe5\xb7\xae\xef\xbc\x8c\xe6\x9d\xa5\xe7\xbb\x9f\xe8\xae\xa1\xe4\xb8\x80\xe4\xb8\xaaepoch\xe4\xb8\xad\xe6\x8d\x9f\xe5\xa4\xb1\xe7\x9a\x84\xe5\xb9\xb3\xe5\x9d\x87\xe5\x80\xbc\n    loss_meter = meter.AverageValueMeter()\n    previous_loss = float(\'inf\')  # \xe8\xa1\xa8\xe7\xa4\xba\xe6\xad\xa3\xe6\x97\xa0\xe7\xa9\xb7\n    # \xe5\xbc\x80\xe5\xa7\x8b\xe8\xae\xad\xe7\xbb\x83\n    for epoch in range(opt.epochs):\n       \n        # \xe6\xb8\x85\xe7\xa9\xba\xe4\xbb\xaa\xe8\xa1\xa8\xe4\xbf\xa1\xe6\x81\xaf\xe5\x92\x8c\xe6\xb7\xb7\xe6\xb7\x86\xe7\x9f\xa9\xe9\x98\xb5\xe4\xbf\xa1\xe6\x81\xaf\n        loss_meter.reset()\n        # \xe6\xaf\x8f\xe8\xbd\xaeepoch\n        for batch_i, (_, imgs, targets) in enumerate(dataloader):\n            # imgs :\xe5\xa4\x84\xe7\x90\x86\xe5\x90\x8e\xe7\x9a\x84\xe5\x9b\xbe\xe5\x83\x8ftensor[16,3,416,416]        targets:\xe5\x9d\x90\xe6\xa0\x87\xe8\xa2\xab\xe5\xbd\x92\xe4\xb8\x80\xe5\x8c\x96\xe5\x90\x8e\xe7\x9a\x84\xe7\x9c\x9f\xe5\x80\xbc\xe6\xa1\x86filled_labels[16,50,5] \xe5\x80\xbc\xe5\x9c\xa80-1\xe4\xb9\x8b\xe9\x97\xb4\n            imgs = Variable(imgs.type(Tensor))\n            targets = Variable(targets.type(Tensor), requires_grad=False)\n            # \xe4\xbc\x98\xe5\x8c\x96\xe5\x99\xa8\xe6\xa2\xaf\xe5\xba\xa6\xe6\xb8\x85\xe9\x9b\xb6\n            optimizer.zero_grad()\n            # \xe5\xbe\x97\xe5\x88\xb0\xe7\xbd\x91\xe7\xbb\x9c\xe8\xbe\x93\xe5\x87\xba\xe5\x80\xbc\xef\xbc\x8c\xe4\xbd\x9c\xe4\xb8\xba\xe6\x8d\x9f\xe5\xa4\xb1 (loss :\xe5\xa4\x9a\xe5\xb0\xba\xe5\xba\xa6\xe9\xa2\x84\xe6\xb5\x8b\xe7\x9a\x84\xe6\x80\xbbloss\xe4\xb9\x8b\xe5\x92\x8c)\n            loss = model(imgs, targets)\n            # \xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad  \xe8\x87\xaa\xe5\x8a\xa8\xe6\xb1\x82\xe6\xa2\xaf\xe5\xba\xa6\n            loss.backward()\n            # \xe6\x9b\xb4\xe6\x96\xb0\xe4\xbc\x98\xe5\x8c\x96\xe5\x99\xa8\xe7\x9a\x84\xe5\x8f\xaf\xe5\xad\xa6\xe4\xb9\xa0\xe5\x8f\x82\xe6\x95\xb0\n            optimizer.step()\n            loss_meter.add(loss.item())\n            # \xe6\x80\xbbloss\xe4\xb8\xba loss_x\xe3\x80\x81loss_y\xe3\x80\x81loss_w\xe3\x80\x81loss_h\xe3\x80\x81loss_conf\xe3\x80\x81loss_cls\xe4\xb9\x8b\xe5\x92\x8c\n            print(\'[Epoch %d/%d, Batch %d/%d] [Losses: x %f, y %f, w %f, h %f, conf %f, cls %f, total %f, recall: %.5f]\' %\n                                        (epoch, opt.epochs, batch_i, len(dataloader),\n                                        model.losses[\'x\'], model.losses[\'y\'], model.losses[\'w\'],\n                                        model.losses[\'h\'], model.losses[\'conf\'], model.losses[\'cls\'],\n                                        loss.item(), model.losses[\'recall\']))\n            # \xe6\xaf\x8fprint_freq\xe6\xac\xa1\xe5\x8f\xaf\xe8\xa7\x86\xe5\x8c\x96loss\n            if batch_i % opt.print_freq == opt.print_freq - 1:\n                # plot\xe6\x98\xaf\xe8\x87\xaa\xe5\xae\x9a\xe4\xb9\x89\xe7\x9a\x84\xe6\x96\xb9\xe6\xb3\x95\n                vis.plot(\'batch loss\', loss.item())\n                vis.plot(\'batch loss_x\', model.losses[\'x\'])\n                vis.plot(\'batch loss_y\', model.losses[\'y\'])\n                vis.plot(\'batch loss_w\', model.losses[\'w\'])\n                vis.plot(\'batch loss_h\', model.losses[\'h\'])\n                vis.plot(\'batch loss_conf\', model.losses[\'conf\'])\n                vis.plot(\'batch loss_cls\', model.losses[\'cls\'])\n                vis.plot(\'batch recall\', model.losses[\'recall\'])\n        # \xe6\xaf\x8f\xe9\x9a\x94\xe5\x87\xa0\xe4\xb8\xaa\xe6\xa8\xa1\xe5\x9e\x8b\xe4\xbf\x9d\xe5\xad\x98\xe4\xb8\x80\xe6\xac\xa1\n        if epoch % opt.checkpoint_interval == 0:\n            checkpoint = {\'model\': model.state_dict()}\n            torch.save(checkpoint, opt.checkpoint_dir + \'/\'+str(epoch)+\'yolov3.pt\')\n        print(""\xe7\xac\xac"" + str(epoch) + ""\xe6\xac\xa1epoch\xe5\xae\x8c\xe6\x88\x90=========================="")\n        # \xe5\xbd\x93\xe5\x89\x8d\xe6\x97\xb6\xe5\x88\xbb\xe7\x9a\x84\xe4\xb8\x80\xe4\xba\x9b\xe4\xbf\xa1\xe6\x81\xaf\n        vis.log(""epoch:{epoch},lr:{lr},loss:{loss}"".format(\n            epoch=epoch, loss=str(loss.item()), lr=learning_rate))\n        \n        #\xe4\xbb\xa5\xe4\xb8\x8b\xe4\xb8\xbabobo\xe6\xb7\xbb\xe5\x8a\xa0\xef\xbc\x8c\xe5\x8e\x9f\xe4\xbd\x9c\xe8\x80\x85\xe4\xbb\xa3\xe7\xa0\x81 \xe6\xb2\xa1\xe6\x9c\x89\xe6\x9b\xb4\xe6\x96\xb0\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\xe5\x8f\x82\xe6\x95\xb0\xef\xbc\x8c\xe6\x95\x85\xe6\xb7\xbb\xe5\x8a\xa0\xe3\x80\x82\xef\xbc\x88\xe4\xb9\x9f\xe5\x8f\xaf\xe8\x83\xbd\xe6\x98\xaf\xe5\xbc\x84\xe5\xb7\xa7\xe6\x88\x90\xe6\x8b\x99\xef\xbc\x89\n        #\xe6\x9b\xb4\xe6\x96\xb0\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87  \xe5\xa6\x82\xe6\x9e\x9c\xe6\x8d\x9f\xe5\xa4\xb1\xe5\xbc\x80\xe5\xa7\x8b\xe5\x8d\x87\xe9\xab\x98\xef\xbc\x8c\xe5\x88\x99\xe9\x99\x8d\xe4\xbd\x8e\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\n        if loss_meter.value()[0]>previous_loss:\n            learning_rate = learning_rate * opt.lr_decay\n            # \xe7\xac\xac\xe4\xba\x8c\xe7\xa7\x8d\xe9\x99\x8d\xe4\xbd\x8e\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\xe7\x9a\x84\xe6\x96\xb9\xe6\xb3\x95:\xe4\xb8\x8d\xe4\xbc\x9a\xe6\x9c\x89moment\xe7\xad\x89\xe4\xbf\xa1\xe6\x81\xaf\xe7\x9a\x84\xe4\xb8\xa2\xe5\xa4\xb1\n            for param_group in optimizer.param_groups:\n                param_group[\'lr\'] = learning_rate\n        previous_loss=loss_meter.value()[0]\ndef test():\n\n    opt = opt_test\n    # Get data configuration\n    data_config = parse_data_config(opt.data_config_path)\n    test_path = data_config[\'valid\']\n    num_classes = int(data_config[\'classes\'])\n\n    # Initiate model\n    model = Darknet(opt.model_config_path)\n\n    if opt.load_model_path:\n        # \xe5\x88\xa4\xe6\x96\xad\xe6\x98\xaf\xe8\xae\xad\xe7\xbb\x83\xe6\xa8\xa1\xe5\x9e\x8b.pt \xe8\xbf\x98\xe6\x98\xaf.weights\xe5\xae\x98\xe6\x96\xb9\xe6\xa8\xa1\xe5\x9e\x8b\n        if \'.pt\' in opt.load_model_path:\n            # \xe5\x8a\xa0\xe8\xbd\xbd\xe9\xa2\x84\xe8\xae\xad\xe7\xbb\x83\xe5\xa5\xbd\xe7\x9a\x84\xe7\x9a\x84yolo v3\xe6\xa8\xa1\xe5\x9e\x8b\n            print(\'\xe5\x8a\xa0\xe8\xbd\xbd\xe5\xb7\xb2\xe8\xae\xad\xe7\xbb\x83\xe5\xa5\xbd\xe7\x9a\x84yolo v3\xe8\x87\xaa\xe8\xae\xad\xe7\xbb\x83\xe6\xa8\xa1\xe5\x9e\x8b\')\n            checkpoint = torch.load(opt.load_model_path, map_location=\'cpu\')\n            model.load_state_dict(checkpoint[\'model\'])\n        if \'.weights\' in opt.load_model_path:\n            model.load_weights(opt.load_model_path)\n    else:\n        # \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96Conv\xe3\x80\x81BatchNorm2d\xe6\x9d\x83\xe9\x87\x8d\n        print(\'\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96yolo v3\xe6\xa8\xa1\xe5\x9e\x8b\xe5\x8f\x82\xe6\x95\xb0\')\n        model.apply(weights_init_normal)\n        \n    if opt.use_cuda:\n        model = model.cuda()\n\n    model.eval()\n\n    # Get dataloader\n    dataset = ListDataset(test_path)\n    dataloader = torch.utils.data.DataLoader(dataset,\n                                             batch_size=opt.batch_size, shuffle=False, num_workers=opt.n_cpu)\n\n    Tensor = torch.cuda.FloatTensor if opt.use_cuda else torch.FloatTensor\n\n    n_gt = 0\n    correct = 0\n\n    print(\'Compute mAP...\')\n\n    outputs = []\n    targets = None\n    APs = []\n    for batch_i, (_, imgs, targets) in enumerate(dataloader):\n        imgs = Variable(imgs.type(Tensor))\n        targets = targets.type(Tensor)\n\n        with torch.no_grad():\n            output = model(imgs)\n            output = non_max_suppression(output, 80, conf_thres=opt.conf_thres, nms_thres=opt.nms_thres)\n\n        # Compute average precision for each sample\n        for sample_i in range(targets.size(0)):\n            correct = []\n\n            # Get labels for sample where width is not zero (dummies)\n            annotations = targets[sample_i, targets[sample_i, :, 3] != 0]\n            # Extract detections\n            detections = output[sample_i]\n\n            if detections is None:\n                # If there are no detections but there are annotations mask as zero AP\n                if annotations.size(0) != 0:\n                    APs.append(0)\n                print(\'\xe6\x9c\x89\xe7\x89\xa9\xe4\xbd\x93\xef\xbc\x8c\xe4\xbd\x86\xe6\x98\xaf\xe9\xa2\x84\xe6\xb5\x8b\xe4\xb8\xba\xe7\xa9\xba\xef\xbc\x8c\xe5\x88\x99AP=0\')\n                continue\n\n            # Get detections sorted by decreasing confidence scores\n            detections = detections[np.argsort(-detections[:, 4])]\n\n            # If no annotations add number of detections as incorrect\n            if annotations.size(0) == 0:\n                correct.extend([0 for _ in range(len(detections))])\n            else:\n                # Extract target boxes as (x1, y1, x2, y2)\n                target_boxes = torch.FloatTensor(annotations[:, 1:].shape)\n                target_boxes[:, 0] = (annotations[:, 1] - annotations[:, 3] / 2)\n                target_boxes[:, 1] = (annotations[:, 2] - annotations[:, 4] / 2)\n                target_boxes[:, 2] = (annotations[:, 1] + annotations[:, 3] / 2)\n                target_boxes[:, 3] = (annotations[:, 2] + annotations[:, 4] / 2)\n                target_boxes *= opt.img_size\n\n                detected = []\n                for *pred_bbox, conf, obj_conf, obj_pred in detections:\n\n                    pred_bbox = torch.FloatTensor(pred_bbox).view(1, -1)\n                    # Compute iou with target boxes\n                    iou = bbox_iou(pred_bbox, target_boxes)\n                    # Extract index of largest overlap\n                    best_i = np.argmax(iou)\n                    # If overlap exceeds threshold and classification is correct mark as correct\n                    if iou[best_i] > opt.iou_thres and obj_pred == annotations[best_i, 0] and best_i not in detected:\n                        correct.append(1)\n                        detected.append(best_i)\n                    else:\n                        correct.append(0)\n\n            # Extract true and false positives\n            true_positives = np.array(correct)\n            false_positives = 1 - true_positives\n\n            # Compute cumulative false positives and true positives\n            false_positives = np.cumsum(false_positives)\n            true_positives = np.cumsum(true_positives)\n\n            # Compute recall and precision at all ranks\n            recall = true_positives / annotations.size(0) if annotations.size(0) else true_positives\n            precision = true_positives / np.maximum(true_positives + false_positives, np.finfo(np.float64).eps)\n\n            # Compute average precision\n            AP = compute_ap(recall, precision)\n            APs.append(AP)\n\n            print(""+ Sample [%d/%d] AP: %.4f (%.4f)"" % (len(APs), len(dataset), AP, np.mean(APs)))\n\n    print(""Mean Average Precision: %.4f"" % np.mean(APs))\ndef  detect():\n\n\n    opt = opt_detect\n    print(opt)\n\n    # \xe5\x88\x9b\xe5\xbb\xbaoutput\xe6\x96\x87\xe4\xbb\xb6\xe5\xa4\xb9\xef\xbc\x8c\xe4\xbb\xa5\xe4\xbe\xbf\xe4\xbf\x9d\xe5\xad\x98\xe7\x94\x9f\xe6\x88\x90\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x9c\n    os.makedirs(\'output\', exist_ok=True)\n\n    # Set up model\n    model = Darknet(opt.config_path, img_size=opt.img_size)\n\n    if opt.load_model_path:\n        # \xe5\x88\xa4\xe6\x96\xad\xe6\x98\xaf\xe8\xae\xad\xe7\xbb\x83\xe6\xa8\xa1\xe5\x9e\x8b.pt \xe8\xbf\x98\xe6\x98\xaf.weights\xe5\xae\x98\xe6\x96\xb9\xe6\xa8\xa1\xe5\x9e\x8b\n        if \'.pt\' in opt.load_model_path:\n            # \xe5\x8a\xa0\xe8\xbd\xbd\xe9\xa2\x84\xe8\xae\xad\xe7\xbb\x83\xe5\xa5\xbd\xe7\x9a\x84\xe7\x9a\x84yolo v3\xe6\xa8\xa1\xe5\x9e\x8b\n            print(\'\xe5\x8a\xa0\xe8\xbd\xbd\xe5\xb7\xb2\xe8\xae\xad\xe7\xbb\x83\xe5\xa5\xbd\xe7\x9a\x84yolo v3\xe8\x87\xaa\xe8\xae\xad\xe7\xbb\x83\xe6\xa8\xa1\xe5\x9e\x8b\')\n            checkpoint = torch.load(opt.load_model_path, map_location=\'cpu\')\n            model.load_state_dict(checkpoint[\'model\'])\n        if \'.weights\' in opt.load_model_path:\n            model.load_weights(opt.load_model_path)\n    else:\n        # \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96Conv\xe3\x80\x81BatchNorm2d\xe6\x9d\x83\xe9\x87\x8d\n        print(\'\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96yolo v3\xe6\xa8\xa1\xe5\x9e\x8b\xe5\x8f\x82\xe6\x95\xb0\')\n        model.apply(weights_init_normal)\n\n    # \xe5\xb0\x86\xe6\xa8\xa1\xe5\x9e\x8b\xe8\xbd\xac\xe5\x88\xb0GPU\xe4\xb8\x8a\n    if opt.use_cuda:\n        model.cuda()\n\n    # \xe6\xa8\xa1\xe5\x9e\x8b\xe8\xb0\x83\xe6\x95\xb4\xe4\xb8\xba\xe9\xaa\x8c\xe8\xaf\x81\xe6\xa8\xa1\xe5\xbc\x8f\n    model.eval()\n\n    # \xe5\x8a\xa0\xe8\xbd\xbd\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\n    dataloader = DataLoader(ImageFolder(opt.image_folder, img_size=opt.img_size),\n                            batch_size=opt.batch_size, shuffle=False, num_workers=opt.n_cpu)\n\n    # \xe4\xbb\x8e\xe6\x96\x87\xe4\xbb\xb6\xe4\xb8\xad\xe6\x8f\x90\xe5\x8f\x96\xe7\xb1\xbb\xe6\xa0\x87\xe7\xad\xbe\n    classes = load_classes(opt.class_path)\n\n    # \xe8\xae\xbe\xe7\xbd\xae\xe9\xbb\x98\xe8\xae\xa4Tensor\n    Tensor = torch.cuda.FloatTensor if opt.use_cuda else torch.FloatTensor\n\n    imgs = []  # \xe4\xbf\x9d\xe5\xad\x98\xe6\x96\x87\xe4\xbb\xb6\xe8\xb7\xaf\xe5\xbe\x84\n    img_detections = []  # \xe5\xad\x98\xe5\x82\xa8\xe6\xaf\x8f\xe4\xb8\xaa\xe5\x9b\xbe\xe5\x83\x8f\xe7\xb4\xa2\xe5\xbc\x95\xe7\x9a\x84\xe7\xbd\x91\xe7\xbb\x9c\xe8\xbe\x93\xe5\x87\xba\xe6\xa3\x80\xe6\xb5\x8b\xe7\xbb\x93\xe6\x9e\x9c\n\n    print(\'\\n\xe5\xbc\x80\xe5\xa7\x8b\xe8\xbf\x9b\xe8\xa1\x8c \xe7\x9b\xae\xe6\xa0\x87\xe6\xa3\x80\xe6\xb5\x8b:\')\n    prev_time = time.time()\n    for batch_i, (img_paths, input_imgs) in enumerate(dataloader):\n        # Configure input\n        input_imgs = Variable(input_imgs.type(Tensor))\n\n        # Get detections\n        with torch.no_grad():\n            # \xe5\xbe\x97\xe5\x88\xb0\xe7\xbd\x91\xe7\xbb\x9c\xe8\xbe\x93\xe5\x87\xba\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x9c\n            detections = model(input_imgs)\n            # \xe8\xbf\x9b\xe8\xa1\x8cNMS \xe9\x9d\x9e\xe6\x9e\x81\xe5\xa4\xa7\xe5\x80\xbc\xe6\x8a\x91\xe5\x88\xb6\n            detections = non_max_suppression(detections, 80, opt.conf_thres, opt.nms_thres)\n\n        # \xe8\xae\xb0\xe5\xbd\x95\xe6\x97\xb6\xe9\x97\xb4\n        current_time = time.time()\n        inference_time = datetime.timedelta(seconds=current_time - prev_time)\n        prev_time = current_time\n        print(\'\\t+ Batch %d, Inference Time: %s\' % (batch_i, inference_time))\n\n        # \xe4\xbf\x9d\xe5\xad\x98\xe5\x9b\xbe\xe7\x89\x87\xe5\x92\x8c \xe7\xbd\x91\xe7\xbb\x9c\xe8\xbe\x93\xe5\x87\xba\xe7\x9a\x84\xe6\xa3\x80\xe6\xb5\x8b\xe7\xbb\x93\xe6\x9e\x9c\n        imgs.extend(img_paths)\n        img_detections.extend(detections)\n\n    # Bounding-box colors\n    cmap = plt.get_cmap(\'tab20b\')\n    colors = [cmap(i) for i in np.linspace(0, 1, 20)]\n\n    print(\'\\nSaving images:\')\n    # Iterate through images and save plot of detections\n    for img_i, (path, detections) in enumerate(zip(imgs, img_detections)):\n\n        print(""(%d) Image: \'%s\'"" % (img_i, path))\n\n        # Create plot\n        img = np.array(Image.open(path))\n        plt.switch_backend(\'agg\')\n        plt.figure()\n        fig, ax = plt.subplots(1)\n        ax.imshow(img)\n\n        # The amount of padding that was added\n        pad_x = max(img.shape[0] - img.shape[1], 0) * (opt.img_size / max(img.shape))\n        pad_y = max(img.shape[1] - img.shape[0], 0) * (opt.img_size / max(img.shape))\n        # Image height and width after padding is removed\n        unpad_h = opt.img_size - pad_y\n        unpad_w = opt.img_size - pad_x\n\n        # Draw bounding boxes and labels of detections\n        if detections is not None:\n            unique_labels = detections[:, -1].cpu().unique()\n            n_cls_preds = len(unique_labels)\n            bbox_colors = random.sample(colors, n_cls_preds)\n            for x1, y1, x2, y2, conf, cls_conf, cls_pred in detections:\n                print(\'\\t+ Label: %s, Conf: %.5f\' % (classes[int(cls_pred)], cls_conf.item()))\n\n                # Rescale coordinates to original dimensions\n                box_h = ((y2 - y1) / unpad_h) * img.shape[0]\n                box_w = ((x2 - x1) / unpad_w) * img.shape[1]\n                y1 = ((y1 - pad_y // 2) / unpad_h) * img.shape[0]\n                x1 = ((x1 - pad_x // 2) / unpad_w) * img.shape[1]\n\n                color = bbox_colors[int(np.where(unique_labels == int(cls_pred))[0])]\n                # Create a Rectangle patch\n                bbox = patches.Rectangle((x1, y1), box_w, box_h, linewidth=2,\n                                         edgecolor=color,\n                                         facecolor=\'none\')\n                # Add the bbox to the plot\n                ax.add_patch(bbox)\n                # Add label\n                plt.text(x1, y1, s=classes[int(cls_pred)], color=\'white\', verticalalignment=\'top\',\n                         bbox={\'color\': color, \'pad\': 0})\n\n        # Save generated image with detections\n        plt.axis(\'off\')\n        plt.gca().xaxis.set_major_locator(NullLocator())\n        plt.gca().yaxis.set_major_locator(NullLocator())\n        plt.savefig(\'output/%d.png\' % (img_i), bbox_inches=\'tight\', pad_inches=0.0)\n        plt.close()\n\n\n\n\n\n\nif __name__ == \'__main__\':\n    \'\'\'\n    \xe5\xa2\x9e\xe5\xa4\xa7 \xe8\xbf\x9b\xe7\xa8\x8b\xe6\x89\x93\xe5\xbc\x80\xe7\x9a\x84\xe6\x9c\x80\xe5\xa4\xa7\xe6\x96\x87\xe4\xbb\xb6\xe6\x95\xb0\xef\xbc\x8c\xe5\x86\x85\xe6\xa0\xb8\xe9\xbb\x98\xe8\xae\xa4\xe4\xb8\xba1024\n    RLIMIT_NOFILE\xe4\xb8\x80\xe4\xb8\xaa\xe8\xbf\x9b\xe7\xa8\x8b\xe8\x83\xbd\xe6\x89\x93\xe5\xbc\x80\xe7\x9a\x84\xe6\x9c\x80\xe5\xa4\xa7\xe6\x96\x87\xe4\xbb\xb6\xe6\x95\xb0\xef\xbc\x8c\xe5\x86\x85\xe6\xa0\xb8\xe9\xbb\x98\xe8\xae\xa4\xe6\x98\xaf1024\n    rlimit\xe7\x9a\x84\xe4\xb8\xa4\xe4\xb8\xaa\xe5\x8f\x82\xe6\x95\xb0\n    soft limit  \xe6\x8c\x87\xe5\x86\x85\xe6\xa0\xb8\xe6\x89\x80\xe8\x83\xbd\xe6\x94\xaf\xe6\x8c\x81\xe7\x9a\x84\xe8\xb5\x84\xe6\xba\x90\xe4\xb8\x8a\xe9\x99\x90 \xe6\x9c\x80\xe5\xa4\xa7\xe4\xb9\x9f\xe5\x8f\xaa\xe8\x83\xbd\xe8\xbe\xbe\xe5\x88\xb01024\n    hard limit \xe5\xad\xa6\xe6\xa0\xa1\xe6\x9c\xba\xe6\x88\xbf\xe5\x80\xbc\xe4\xb8\xba4096  \xe5\x9c\xa8\xe8\xb5\x84\xe6\xba\x90\xe4\xb8\xad\xe5\x8f\xaa\xe6\x98\xaf\xe4\xbd\x9c\xe4\xb8\xbasoft limit\xe7\x9a\x84\xe4\xb8\x8a\xe9\x99\x90\xe3\x80\x82\xe5\xbd\x93\xe4\xbd\xa0\xe8\xae\xbe\xe7\xbd\xaehard limit\xe5\x90\x8e\xef\xbc\x8c\xe4\xbd\xa0\xe4\xbb\xa5\xe5\x90\x8e\xe8\xae\xbe\xe7\xbd\xae\xe7\x9a\x84soft limit\xe5\x8f\xaa\xe8\x83\xbd\xe5\xb0\x8f\xe4\xba\x8ehard limit\n    \'\'\'\n    rlimit = resource.getrlimit(resource.RLIMIT_NOFILE)\n    resource.setrlimit(resource.RLIMIT_NOFILE, (4096, rlimit[1]))\n\n    # torch.cuda.set_device(1)\n\n    train()   # \xe5\xbc\x80\xe5\xa7\x8b\xe8\xae\xad\xe7\xbb\x83\n    # test()    # \xe6\xb5\x8b\xe8\xaf\x95\xef\xbc\x8c\xe8\xae\xa1\xe7\xae\x97mAP\n    # detect()    # \xe5\x88\x86\xe5\x89\xb2demo'"
CAM_pytorch/data/MyDataSet.py,1,"b""#!/usr/bin/python\n# -*- coding:utf-8 -*-\n# power by Mr.Li\nimport os\nfrom torch.utils import data\nfrom torchvision import transforms as T\nimport cv2\nimport random\nfrom utils.config import opt\nclass MyDataSet(data.Dataset):\n    '''\n    \xe4\xb8\xbb\xe8\xa6\x81\xe7\x9b\xae\xe6\xa0\x87\xef\xbc\x9a \xe8\x8e\xb7\xe5\x8f\x96\xe6\x89\x80\xe6\x9c\x89\xe5\x9b\xbe\xe7\x89\x87\xe7\x9a\x84\xe5\x9c\xb0\xe5\x9d\x80\xef\xbc\x8c\xe5\xb9\xb6\xe6\xa0\xb9\xe6\x8d\xae\xe8\xae\xad\xe7\xbb\x83\xef\xbc\x8c\xe9\xaa\x8c\xe8\xaf\x81\xef\xbc\x8c\xe6\xb5\x8b\xe8\xaf\x95\xe5\x88\x92\xe5\x88\x86\xe6\x95\xb0\xe6\x8d\xae\n    '''\n    def __init__(self, root, transforms=None, train=True, test=False):\n        self.test = test  #\xe7\x8a\xb6\xe6\x80\x81\n        self.train = train\n        self.root = root  #\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe8\xb7\xaf\xe5\xbe\x84\n\n        # \xe8\xaf\xbb\xe5\x8f\x96\xe6\x96\x87\xe4\xbb\xb6\xe5\xa4\xb9\xe4\xb8\x8b\xe6\x89\x80\xe6\x9c\x89\xe5\x9b\xbe\xe5\x83\x8f\n        if root!='':\n            pos_root=os.path.join(root, 'pos')\n            neg_root = os.path.join(root, 'neg')\n\n            pos_imgs = [os.path.join(pos_root, img) for img in os.listdir(pos_root)]\n            neg_imgs = [os.path.join(neg_root, img) for img in os.listdir(neg_root)]\n\n            imgs = pos_imgs + neg_imgs\n            # \xe6\x89\x93\xe4\xb9\xb1\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\n            random.shuffle(imgs)\n        else:\n            print('\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe4\xb8\xba\xe7\xa9\xba\xef\xbc\x9f\xef\xbc\x9f\xef\xbc\x9f')\n            imgs = []\n\n        imgs_num = len (imgs)\n        # \xe5\x88\x92\xe5\x88\x86\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\n        if train:\n            self.imgs = imgs[:int(0.8 * imgs_num)]\n        else:\n            self.imgs = imgs[int(0.8 * imgs_num):]\n\n\n\n        # \xe5\xaf\xb9\xe5\x9b\xbe\xe5\x83\x8f\xe8\xbf\x9b\xe8\xa1\x8c\xe8\xbd\xac\xe5\x8c\x96(\xe8\x8b\xa5\xe6\x9c\xaa\xe6\x8c\x87\xe5\xae\x9a\xe8\xbd\xac\xe5\x8c\x96\xef\xbc\x8c\xe5\x88\x99\xe6\x89\xa7\xe8\xa1\x8c\xe9\xbb\x98\xe8\xae\xa4\xe6\x93\x8d\xe4\xbd\x9c)\n        if transforms is None:\n            normalize = T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n\n            if self.test or not train:  # \xe6\xb5\x8b\xe8\xaf\x95\xe9\x9b\x86\xe5\x92\x8c\xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86\n                self.transforms = T.Compose([\n                    T.ToTensor(),\n                    normalize\n                ])\n            else:  # \xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\n                self.transforms = T.Compose([\n                    T.ToTensor(),\n                    normalize\n                ])\n\n    def __getitem__(self, index):\n        '''\n        \xe4\xb8\x80\xe6\xac\xa1\xe8\xbf\x94\xe5\x9b\x9e\xe4\xb8\x80\xe5\xbc\xa0\xe5\x9b\xbe\xe7\x89\x87\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\n        '''\n        # \xe5\x9b\xbe\xe7\x89\x87\xe7\x9a\x84\xe5\xae\x8c\xe6\x95\xb4\xe8\xb7\xaf\xe5\xbe\x84\n        img_path = self.imgs[index]\n        # \xe8\xaf\xbb\xe5\x8f\x96\xe5\x9b\xbe\xe5\x83\x8f\n        img = cv2.imread(img_path)\n        img = self.BGR2RGB(img)  # \xe5\x9b\xa0\xe4\xb8\xbapytorch\xe8\x87\xaa\xe8\xba\xab\xe6\x8f\x90\xe4\xbe\x9b\xe7\x9a\x84\xe9\xa2\x84\xe8\xae\xad\xe7\xbb\x83\xe5\xa5\xbd\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\xe6\x9c\x9f\xe6\x9c\x9b\xe7\x9a\x84\xe8\xbe\x93\xe5\x85\xa5\xe6\x98\xafRGB\n        img = cv2.resize(img, (64, 128))\n        # \xe5\xaf\xb9\xe5\x9b\xbe\xe7\x89\x87\xe8\xbf\x9b\xe8\xa1\x8c\xe8\xbd\xac\xe5\x8c\x96\n        img = self.transforms(img)\n        # \xe6\xa0\x87\xe7\xad\xbe\xe7\x9c\x9f\xe5\x80\xbc\n        if 'neg' in img_path:\n            label=0  # \xe6\xb2\xa1\xe6\x9c\x89\xe4\xba\xba\n        else:\n            label=1   # \xe6\x9c\x89\xe4\xba\xba\n\n        return img,label\n\n    def __len__(self):\n        return len(self.imgs)\n\n    def BGR2RGB(self, img):\n        return cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n    def get_test_img(self):\n        # \xe8\xaf\xbb\xe5\x8f\x96\xe5\x9b\xbe\xe5\x83\x8f\n        img_origin = cv2.imread(opt.test_img)\n        img = self.BGR2RGB(img_origin)  # \xe5\x9b\xa0\xe4\xb8\xbapytorch\xe8\x87\xaa\xe8\xba\xab\xe6\x8f\x90\xe4\xbe\x9b\xe7\x9a\x84\xe9\xa2\x84\xe8\xae\xad\xe7\xbb\x83\xe5\xa5\xbd\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\xe6\x9c\x9f\xe6\x9c\x9b\xe7\x9a\x84\xe8\xbe\x93\xe5\x85\xa5\xe6\x98\xafRGB\n        img = cv2.resize(img, (64, 128))\n        # \xe5\xaf\xb9\xe5\x9b\xbe\xe7\x89\x87\xe8\xbf\x9b\xe8\xa1\x8c\xe8\xbd\xac\xe5\x8c\x96\n        img = self.transforms(img)\n        return img_origin,img\n\n"""
CAM_pytorch/data/__init__.py,0,b''
CAM_pytorch/models/VGG_CAM.py,1,"b'# -*- coding:utf-8 -*-\n# power by Mr.Li\nfrom torch import nn\nimport torch as t\nfrom torchvision.models import vgg16\nfrom utils.config import opt\nclass VGG16_CAM(nn.Module):\n    \'\'\'\n    \xe5\xae\x9a\xe4\xb9\x89\xe7\xbd\x91\xe7\xbb\x9c\n    \'\'\'\n    def __init__(self):\n        super(VGG16_CAM, self).__init__()\n        # \xe8\xae\xbe\xe7\xbd\xae\xe7\xbd\x91\xe7\xbb\x9c\xe5\x90\x8d\xe7\xa7\xb0\n        self.moduel_name = str(""VGG16_CAM"")\n        # \xe5\x8e\xbb\xe6\x8e\x89 VGG16 feature\xe5\xb1\x82\xe7\x9a\x84maxpool\xe5\xb1\x82\n        self.feature_layer = nn.Sequential(*list(vgg16(pretrained=True).features.children())[0:-1])\n        # \xe5\x85\xa8\xe5\xb1\x80\xe5\xb9\xb3\xe5\x9d\x87\xe6\xb1\xa0\xe5\x8c\x96\xe5\xb1\x82 GAP\n        self.fc_layer = nn.Linear(512,2)\n\n    def forward(self, x):\n        x = self.feature_layer(x)\n        # GAP \xe5\x85\xa8\xe5\xb1\x80\xe5\xb9\xb3\xe5\x9d\x87\xe6\xb1\xa0\xe5\x8c\x96\n        x = t.mean(x,dim=3)\n        x = t.mean(x,dim=2)\n\n        # \xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe5\xb1\x82+softmax\xe5\xb1\x82\n        x = self.fc_layer(x)\n        # x = F.softmax(x)   #\xe4\xba\xa4\xe5\x8f\x89\xe7\x86\xb5\xe8\x87\xaa\xe5\xb8\xa6softmax\n        return x\n\n\n# def test():\n#     from torch.autograd import Variable\n#     model=VGG16_CAM()\n#     print(model)\n#     img=t.rand(2,3,224,224)\n#     img=Variable(img)\n#     output=model(img)\n#     print(output.size())\n#\n# if __name__ == \'__main__\':\n#     test()'"
CAM_pytorch/models/__init__.py,0,b'from .VGG_CAM import VGG16_CAM'
CAM_pytorch/utils/__init__.py,0,b''
CAM_pytorch/utils/config.py,0,"b""# -*- coding:utf-8 -*-\n# power by Mr.Li\n# \xe8\xae\xbe\xe7\xbd\xae\xe9\xbb\x98\xe8\xae\xa4\xe5\x8f\x82\xe6\x95\xb0\nimport datetime\nimport os\nclass DefaultConfig():\n    # \xe4\xbd\xbf\xe7\x94\xa8\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\xef\xbc\x8c\xe5\x90\x8d\xe5\xad\x97\xe5\xbf\x85\xe9\xa1\xbb\xe4\xb8\x8emodels/__init__.py\xe4\xb8\xad\xe7\x9a\x84\xe5\x90\x8d\xe5\xad\x97\xe4\xb8\x80\xe8\x87\xb4\n    # \xe7\x9b\xae\xe5\x89\x8d\xe6\x94\xaf\xe6\x8c\x81\xe7\x9a\x84\xe7\xbd\x91\xe7\xbb\x9c\n    model = 'VGG16_CAM'\n\n    # \xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe5\x9c\xb0\xe5\x9d\x80\n    dataset_root = '/home/bobo/data/cam_dataset/INRIAPerson/Train'\n\n    # \xe4\xbf\x9d\xe5\xad\x98\xe6\xa8\xa1\xe5\x9e\x8b\n    root = os.path.abspath(os.path.join(os.path.dirname(__file__))) + '/'\n    checkpoint_root = root + '../checkpoint/'  # \xe5\xad\x98\xe5\x82\xa8\xe6\xa8\xa1\xe5\x9e\x8b\xe7\x9a\x84\xe8\xb7\xaf\xe5\xbe\x84\n    # load_model_path = None  # \xe5\x8a\xa0\xe8\xbd\xbd\xe9\xa2\x84\xe8\xae\xad\xe7\xbb\x83\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\xe7\x9a\x84\xe8\xb7\xaf\xe5\xbe\x84\xef\xbc\x8c\xe4\xb8\xbaNone\xe4\xbb\xa3\xe8\xa1\xa8\xe4\xb8\x8d\xe5\x8a\xa0\xe8\xbd\xbd\xef\xbc\x88\xe7\x94\xa8\xe4\xba\x8e\xe8\xae\xad\xe7\xbb\x83\xef\xbc\x89\n    load_model_path = checkpoint_root+'VGG16_CAM_39_99.455.pth'\n\n    use_gpu = True  # user GPU or not\n    batch_size = 32\n    num_workers = 4  #  \xe5\x8a\xa0\xe8\xbd\xbd\xe6\x95\xb0\xe6\x8d\xae\xe6\x97\xb6\xe7\x9a\x84\xe7\xba\xbf\xe7\xa8\x8b\xe6\x95\xb0\n\n    max_epoch = 40\n\n\n    lr = 0.01\n    lr_decay = 0.5\n\n    test_img='/home/bobo/windowsPycharmProject/cam_pytorch/person_and_bike_191.png'  #\xe4\xb8\x80\xe5\xbc\xa0\xe6\xb5\x8b\xe8\xaf\x95\xe5\x9b\xbe\xe7\x89\x87\xe5\x9c\xb0\xe5\x9d\x80\n\n\n\n#\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe8\xaf\xa5\xe7\xb1\xbb\xe7\x9a\x84\xe4\xb8\x80\xe4\xb8\xaa\xe5\xaf\xb9\xe8\xb1\xa1\nopt=DefaultConfig()"""
CAM_pytorch/utils/visualize.py,0,"b""#!/usr/bin/python\n# -*- coding:utf-8 -*-\n# power by Mr.Li\nimport visdom\nimport time\nimport numpy as np\nclass Visualizer(object):\n    '''\n    \xe5\xb0\x81\xe8\xa3\x85\xe4\xba\x86visdom\xe7\x9a\x84\xe5\x9f\xba\xe6\x9c\xac\xe6\x93\x8d\xe4\xbd\x9c\xef\xbc\x8c\xe4\xbd\x86\xe6\x98\xaf\xe4\xbd\xa0\xe4\xbb\x8d\xe7\x84\xb6\xe5\x8f\xaf\xe4\xbb\xa5\xe9\x80\x9a\xe8\xbf\x87`self.vis.function`\n    \xe8\xb0\x83\xe7\x94\xa8\xe5\x8e\x9f\xe7\x94\x9f\xe7\x9a\x84visdom\xe6\x8e\xa5\xe5\x8f\xa3\n    '''\n    def __init__(self, env='default', **kwargs):\n        self.vis = visdom.Visdom(env=env, **kwargs)\n        # \xe7\x94\xbb\xe7\x9a\x84\xe7\xac\xac\xe5\x87\xa0\xe4\xb8\xaa\xe6\x95\xb0\xef\xbc\x8c\xe7\x9b\xb8\xe5\xbd\x93\xe4\xba\x8e\xe6\xa8\xaa\xe5\xba\xa7\xe6\xa0\x87\n        # \xe4\xbf\x9d\xe5\xad\x98\xef\xbc\x88\xe2\x80\x99loss',23\xef\xbc\x89 \xe5\x8d\xb3loss\xe7\x9a\x84\xe7\xac\xac23\xe4\xb8\xaa\xe7\x82\xb9\n        self.index = {}\n        self.log_text = ''\n    def reinit(self,env='default',**kwargs):\n        '''\n        \xe4\xbf\xae\xe6\x94\xb9visdom\xe7\x9a\x84\xe9\x85\x8d\xe7\xbd\xae  \xe9\x87\x8d\xe6\x96\xb0\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\n        '''\n        self.vis = visdom.Visdom(env=env,**kwargs)\n        return self\n    def plot_many(self, d):\n        '''\n        \xe4\xb8\x80\xe6\xac\xa1plot\xe5\xa4\x9a\xe4\xb8\xaa\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x9b\xbe\xe5\xbd\xa2\n        @params d: dict (name,value) i.e. ('loss',0.11)\n        '''\n        for k, v in d.items():\n            self.plot(k, v)\n    def img_many(self, d):\n        '''\n     \xe4\xb8\x80\xe6\xac\xa1\xe7\x94\xbb\xe5\xa4\x9a\xe4\xb8\xaa\xe5\x9b\xbe\xe5\x83\x8f\n        '''\n        for k, v in d.items():\n            self.img(k, v)\n    def plot(self, name, y,**kwargs):\n        '''\n        self.plot('loss',1.00)\n        '''\n        #\xe5\xbe\x97\xe5\x88\xb0\xe4\xb8\x8b\xe6\xa0\x87\xe5\xba\x8f\xe5\x8f\xb7\n        x = self.index.get(name, 0)\n        self.vis.line(Y=np.array([y]), X=np.array([x]),\n                      win=name,#\xe7\xaa\x97\xe5\x8f\xa3\xe5\x90\x8d\n                      opts=dict(title=name),\n                      update=None if x == 0 else 'append', #\xe6\x8c\x89\xe7\x85\xa7append\xe7\x9a\x84\xe7\x94\xbb\xe5\x9b\xbe\xe5\xbd\xa2\n                      **kwargs\n                      )\n        #\xe4\xb8\x8b\xe6\xa0\x87\xe7\xb4\xaf\xe5\x8a\xa01\n        self.index[name] = x + 1\n    def img(self, name, img_,**kwargs):\n        '''\n        self.img('input_img',t.Tensor(64,64))\n        self.img('input_imgs',t.Tensor(3,64,64))\n        self.img('input_imgs',t.Tensor(100,1,64,64))\n        self.img('input_imgs',t.Tensor(100,3,64,64),nrows=10)\n\n        \xef\xbc\x81\xef\xbc\x81\xef\xbc\x81don\xe2\x80\x98t ~~self.img('input_imgs',t.Tensor(100,64,64),nrows=10)~~\xef\xbc\x81\xef\xbc\x81\xef\xbc\x81\n        '''\n        self.vis.images(img_.cpu().numpy(),\n                       win=name,\n                       opts=dict(title=name),\n                       **kwargs\n                       )\n    def log(self,info,win='log_text'):\n        '''\n        self.log({'loss':1,'lr':0.0001})\n        \xe6\x89\x93\xe5\x8d\xb0\xe6\x97\xa5\xe5\xbf\x97\n        '''\n\n        self.log_text += ('[{time}] {info} <br>'.format(\n                            time=time.strftime('%m%d_%H%M%S'),\\\n                            info=info))\n        self.vis.text(self.log_text,win)\n    def __getattr__(self, name):\n        return getattr(self.vis, name)"""
FasterRcnn_pytorch/data/__init__.py,0,b''
FasterRcnn_pytorch/data/dataset.py,0,"b'import torch as t\nfrom .voc_dataset import VOCBboxDataset\nfrom skimage import transform as sktsf\nfrom torchvision import transforms as tvtsf\nfrom . import util\nimport numpy as np\nfrom utils.config import opt\n\n\ndef inverse_normalize(img):\n    """"""\n   \xe5\xb0\x86[-1,1]\xe8\x8c\x83\xe5\x9b\xb4\xe7\x9a\x84\xe5\x9b\xbe\xe5\x83\x8f\xe8\xbf\x91\xe4\xbc\xbc\xe8\xbf\x98\xe5\x8e\x9f\xe5\x9b\x9e[0,255]\xe4\xb9\x8b\xe9\x97\xb4\n    """"""\n    if opt.caffe_pretrain:\n        img = img + (np.array([122.7717, 115.9465, 102.9801]).reshape(3, 1, 1))\n        return img[::-1, :, :]\n    # approximate un-normalize for visualize\n    return (img * 0.225 + 0.45).clip(min=0, max=1) * 255\n\n\ndef pytorch_normalze(img):\n    """"""\n    https://github.com/pytorch/vision/issues/223\n    return appr -1~1 RGB\n    \xe5\xaf\xb9pytorch\xe6\xa0\xbc\xe5\xbc\x8f\xe7\x9a\x84\xe5\x9b\xbe\xe5\x83\x8f\xe8\xbf\x9b\xe8\xa1\x8c\xe8\xa7\x84\xe8\x8c\x83\xe5\x8c\x96\xef\xbc\x8c\xe8\xbf\x94\xe5\x9b\x9e\xe5\x80\xbc\xe8\x8c\x83\xe5\x9b\xb4\xe5\x9c\xa8[-1,1]\xe4\xb9\x8b\xe9\x97\xb4 \xe9\x80\x9a\xe9\x81\x93\xe4\xb8\xbaRGB\n    """"""\n    normalize = tvtsf.Normalize(mean=[0.485, 0.456, 0.406],\n                                std=[0.229, 0.224, 0.225])\n    img = normalize(t.from_numpy(img))\n    return img.numpy()\n\n\ndef caffe_normalize(img):\n    """"""\n    return appr -125-125 BGR\n    \xe5\xaf\xb9caffe\xe6\xa0\xbc\xe5\xbc\x8f\xe7\x9a\x84\xe5\x9b\xbe\xe5\x83\x8f\xe8\xbf\x9b\xe8\xa1\x8c\xe8\xa7\x84\xe8\x8c\x83\xe5\x8c\x96\xef\xbc\x8c\xe8\xbf\x94\xe5\x9b\x9e\xe5\x80\xbc\xe8\x8c\x83\xe5\x9b\xb4\xe5\x9c\xa8[-125,125]\xe4\xb9\x8b\xe9\x97\xb4 \xe9\x80\x9a\xe9\x81\x93\xe4\xb8\xbaBGR\n    """"""\n    img = img[[2, 1, 0], :, :]  # RGB-BGR\n    img = img * 255\n    mean = np.array([122.7717, 115.9465, 102.9801]).reshape(3, 1, 1)\n    img = (img - mean).astype(np.float32, copy=True)\n    return img\n\n\ndef preprocess(img, min_size=600, max_size=1000):\n    """"""Preprocess an image for feature extraction.\n     \n    The length of the shorter edge is scaled to :obj:`self.min_size`.\n    After the scaling, if the length of the longer edge is longer than\n    :param min_size:\n    :obj:`self.max_size`, the image is scaled to fit the longer edge\n    to :obj:`self.max_size`.\n    After resizing the image, the image is subtracted by a mean image value\n    :obj:`self.mean`.\n    \n    \xe9\xa2\x84\xe5\xa4\x84\xe7\x90\x86\xe5\x9b\xbe\xe5\x83\x8f\xe4\xbb\xa5\xe8\xbf\x9b\xe8\xa1\x8c\xe7\x89\xb9\xe5\xbe\x81\xe6\x8f\x90\xe5\x8f\x96\xe3\x80\x82\n    \xe8\xbe\x83\xe7\x9f\xad\xe8\xbe\xb9\xe7\x9a\x84\xe9\x95\xbf\xe5\xba\xa6\xe7\xbc\xa9\xe6\x94\xbe\xe4\xb8\xba\xef\xbc\x9amin_size\xe3\x80\x82\n    \xe7\xbc\xa9\xe6\x94\xbe\xe5\x90\x8e\xef\xbc\x8c\xe5\xa6\x82\xe6\x9e\x9c\xe9\x95\xbf\xe8\xbe\xb9\xe7\x9a\x84\xe9\x95\xbf\xe5\xba\xa6\xe6\xaf\x94min_size\xe6\x88\x96\xe8\x80\x85max_size\xe9\x95\xbf\xef\xbc\x8c\xe5\x88\x99\xe9\x95\xbf\xe8\xbe\xb9\xe7\x9a\x84\xe9\x95\xbf\xe5\xba\xa6\xe8\xa2\xab\xe7\xbc\xa9\xe6\x94\xbe\xe5\x88\xb0max_size\n    \xe8\xb0\x83\xe6\x95\xb4\xe5\x9b\xbe\xe5\x83\x8f\xe5\xa4\xa7\xe5\xb0\x8f\xe5\x90\x8e\xef\xbc\x8c\xe5\x9b\xbe\xe5\x83\x8f\xe5\x87\x8f\xe5\x8e\xbb\xe5\xb9\xb3\xe5\x9d\x87\xe5\x9b\xbe\xe5\x83\x8f\xe5\x80\xbcmean\n    \n    \xe5\x9b\xbe\xe7\x89\x87\xe8\xbf\x9b\xe8\xa1\x8c\xe7\xbc\xa9\xe6\x94\xbe\xef\xbc\x8c\xe4\xbd\xbf\xe5\xbe\x97\xe9\x95\xbf\xe8\xbe\xb9\xe5\xb0\x8f\xe4\xba\x8e\xe7\xad\x89\xe4\xba\x8e1000\xef\xbc\x8c\xe7\x9f\xad\xe8\xbe\xb9\xe5\xb0\x8f\xe4\xba\x8e\xe7\xad\x89\xe4\xba\x8e600\xef\xbc\x88\xe8\x87\xb3\xe5\xb0\x91\xe6\x9c\x89\xe4\xb8\x80\xe4\xb8\xaa\xe7\xad\x89\xe4\xba\x8e\xef\xbc\x89\n\n    Args:\n        img (~numpy.ndarray): An image. This is in CHW and RGB format.\n            The range of its value is :math:`[0, 255]`.\n\n    Returns:\n        ~numpy.ndarray: A preprocessed image.\n\n    """"""\n    C, H, W = img.shape\n    scale1 = min_size / min(H, W)\n    scale2 = max_size / max(H, W)\n    scale = min(scale1, scale2)\n    img = img / 255.\n    img = sktsf.resize(img, (C, H * scale, W * scale), mode=\'reflect\')\n    # both the longer and shorter should be less than\n    # max_size and min_size\n    if opt.caffe_pretrain:\n        normalize = caffe_normalize\n    else:\n        normalize = pytorch_normalze\n    #\xe8\xb0\x83\xe7\x94\xa8\xe4\xb8\x8a\xe8\xbf\xb0\xe6\x96\xb9\xe6\xb3\x95\xe5\xaf\xb9img\xe8\xbf\x9b\xe8\xa1\x8c\xe8\xa7\x84\xe8\x8c\x83\xe5\x8c\x96\n    return normalize(img)\n\n\nclass Transform(object):\n\n    def __init__(self, min_size=600, max_size=1000):\n        self.min_size = min_size\n        self.max_size = max_size\n\n    def __call__(self, in_data):\n        img, bbox, label = in_data\n        _, H, W = img.shape\n        #\xe8\xb0\x83\xe7\x94\xa8\xe4\xb8\x8a\xe8\xbf\xb0\xe6\x96\xb9\xe6\xb3\x95\xe8\xbf\x9b\xe8\xa1\x8c\xe7\xbc\xa9\xe6\x94\xbe\xe5\x9b\xbe\xe5\x83\x8f\n        img = preprocess(img, self.min_size, self.max_size)\n        _, o_H, o_W = img.shape\n        scale = o_H / H\n        #\xe5\xaf\xb9\xe5\x9b\xbe\xe5\x83\x8f\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84bbox\xe4\xb9\x9f\xe8\xbf\x9b\xe8\xa1\x8c\xe5\x90\x8c\xe7\xad\x89\xe5\xb0\xba\xe5\xba\xa6\xe7\x9a\x84\xe7\xbc\xa9\xe6\x94\xbe\n        bbox = util.resize_bbox(bbox, (H, W), (o_H, o_W))\n\n        # horizontally flip \n\t\t#\xe6\xb0\xb4\xe5\xb9\xb3\xe7\xbf\xbb\xe8\xbd\xac\xef\xbc\x88\xe5\xaf\xb9img\xe5\x92\x8c\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84bbox\xe8\xbf\x9b\xe8\xa1\x8c\xe5\x90\x8c\xe7\xad\x89\xe5\xb0\xba\xe5\xba\xa6\xe7\x9a\x84\xe6\xb0\xb4\xe5\xb9\xb3\xe7\xbf\xbb\xe8\xbd\xac\xef\xbc\x89=============================\xe5\x8f\xaa\xe8\xbf\x9b\xe8\xa1\x8c\xe6\xb0\xb4\xe5\xb9\xb3\xe7\xbf\xbb\xe8\xbd\xac\n        img, params = util.random_flip(\n            img, x_random=True, return_param=True)\n        bbox = util.flip_bbox(\n            bbox, (o_H, o_W), x_flip=params[\'x_flip\'])\n\n        return img, bbox, label, scale\n\n\nclass Dataset:\n    def __init__(self, opt):\n        self.opt = opt\n\t\t#\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96VOCBboxDataset\xef\xbc\x8c\xe4\xbc\xa0\xe5\x85\xa5 \xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe5\x9c\xb0\xe5\x9d\x80\n\t\t#eg:  /data/image/voc/VOCdevkit/VOC2007/\n        self.db = VOCBboxDataset(opt.voc_data_dir)\n        #\xe8\xb0\x83\xe7\x94\xa8\xe4\xb8\x8a\xe8\xbf\xb0\xe6\x96\xb9\xe6\xb3\x95Transform\xef\xbc\x88\xe5\x9b\xbe\xe5\x83\x8f\xe8\xbd\xac\xe5\x8c\x96\xe6\x96\xb9\xe5\xbc\x8f\xef\xbc\x89\xef\xbc\x8c\xe8\xbf\x9b\xe8\xa1\x8c\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\n        self.tsf = Transform(opt.min_size, opt.max_size)\n\n    def __getitem__(self, idx):\n\t    #\xe5\xbe\x97\xe5\x88\xb0\xe5\x8e\x9f\xe5\xa7\x8bimg\xef\xbc\x8c\xe6\xa3\x80\xe6\xb5\x8b\xe6\xa1\x86\xe3\x80\x81\xe6\xa0\x87\xe7\xad\xbe\xe3\x80\x81\xe5\x9b\xb0\xe9\x9a\xbe\xe5\xba\xa6\n        ori_img, bbox, label, difficult = self.db.get_example(idx)\n        #\xe8\xb0\x83\xe7\x94\xa8\xe4\xb8\x8a\xe8\xbf\xb0\xe6\x96\xb9\xe6\xb3\x95Transform\xef\xbc\x8c\xe6\x89\xa7\xe8\xa1\x8c__call__\xe6\x96\xb9\xe6\xb3\x95\xe3\x80\x82\xe8\xbf\x94\xe5\x9b\x9e\xe8\xa7\x84\xe8\x8c\x83\xe5\x8c\x96\xe5\x90\x8e\xe7\x9a\x84img, bbox, label, \xe8\xbd\xac\xe5\x8c\x96\xe4\xb9\x8b\xe5\x90\x8e\xe7\x9a\x84\xe6\xaf\x94\xe4\xbe\x8bscale\n        img, bbox, label, scale = self.tsf((ori_img, bbox, label))\n        # TODO: check whose stride is negative to fix this instead copy all\n        # some of the strides of a given numpy array are negative.\n        \n        return img.copy(), bbox.copy(), label.copy(), scale\n\n    def __len__(self):\n        return len(self.db)\n\n\nclass TestDataset:\n    \n    def __init__(self, opt, split=\'test\', use_difficult=True):\n        self.opt = opt\n        self.db = VOCBboxDataset(opt.voc_data_dir, split=split, use_difficult=use_difficult)\n\n    def __getitem__(self, idx):\n        ori_img, bbox, label, difficult = self.db.get_example(idx)\n        img = preprocess(ori_img)\n        return img, ori_img.shape[1:], bbox, label, difficult\n\n    def __len__(self):\n        return len(self.db)\n'"
FasterRcnn_pytorch/data/util.py,0,"b'import numpy as np\nfrom PIL import Image\nimport random\n\n\ndef read_image(path, dtype=np.float32, color=True):\n    """"""Read an image from a file.\n       \xe4\xbb\x8e\xe6\x96\x87\xe4\xbb\xb6\xe4\xb8\xad\xe8\xaf\xbb\xe5\x8f\x96\xe5\x9b\xbe\xe5\x83\x8f\n\n    This function reads an image from given file. The image is CHW format and\n    the range of its value is :math:`[0, 255]`. If :obj:`color = True`, the\n    order of the channels is RGB.\n       \xe8\xaf\xa5\xe5\x8a\x9f\xe8\x83\xbd\xe4\xbb\x8e\xe7\xbb\x99\xe5\xae\x9a\xe6\x96\x87\xe4\xbb\xb6\xe8\xaf\xbb\xe5\x8f\x96\xe5\x9b\xbe\xe5\x83\x8f\xe3\x80\x82 \xe5\x9b\xbe\xe5\x83\x8f\xe6\x98\xafCHW\xe6\xa0\xbc\xe5\xbc\x8f\xe5\x92\x8c\xe5\x80\xbc\xe7\x9a\x84\xe8\x8c\x83\xe5\x9b\xb4\xe6\x98\xaf[0\xef\xbc\x8c255]\xe3\x80\x82 \n       \xe5\xa6\x82\xe6\x9e\x9ccolor = True\xef\xbc\x8c\xe9\x82\xa3\xe4\xb9\x88\xe9\x80\x9a\xe9\x81\x93\xe7\x9a\x84\xe9\xa1\xba\xe5\xba\x8f\xe6\x98\xafRGB\xe3\x80\x82\n\n    Args:\n        path (str): A path of image file.\xe5\x9b\xbe\xe5\x83\x8f\xe6\x96\x87\xe4\xbb\xb6\xe7\x9a\x84\xe8\xb7\xaf\xe5\xbe\x84\n        dtype: The type of array. The default value is :obj:`~numpy.float32`.\n        color (bool): This option determines the number of channels.\n            If :obj:`True`, the number of channels is three. In this case,\n            the order of the channels is RGB. This is the default behaviour.\n            If :obj:`False`, this function returns a grayscale image.\n            True:\xe5\x9c\xa8\xe8\xbf\x99\xe7\xa7\x8d\xe6\x83\x85\xe5\x86\xb5\xe4\xb8\x8b\xef\xbc\x8c\xe9\xa2\x91\xe9\x81\x93\xe7\x9a\x84\xe9\xa1\xba\xe5\xba\x8f\xe6\x98\xafrgb\xe3\x80\x82(\xe9\xbb\x98\xe8\xae\xa4)\n            False:\xe6\xad\xa4\xe5\x87\xbd\xe6\x95\xb0\xe8\xbf\x94\xe5\x9b\x9e\xe7\x81\xb0\xe5\xba\xa6\xe5\x9b\xbe\xe5\x83\x8f\n\n    Returns:\n        ~numpy.ndarray: An image.\n    """"""\n\n    f = Image.open(path)\n    try:\n        if color:\n            img = f.convert(\'RGB\')\n        else:\n            img = f.convert(\'P\')\n        img = np.asarray(img, dtype=dtype)\n    finally:\n        #hasattr() \xe5\x87\xbd\xe6\x95\xb0\xe7\x94\xa8\xe4\xba\x8e\xe5\x88\xa4\xe6\x96\xad\xe5\xaf\xb9\xe8\xb1\xa1\xe6\x98\xaf\xe5\x90\xa6\xe5\x8c\x85\xe5\x90\xab\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84\xe5\xb1\x9e\xe6\x80\xa7\xe3\x80\x82\n        #\xe5\xa6\x82\xe6\x9e\x9c\xe5\xaf\xb9\xe8\xb1\xa1\xe6\x9c\x89\xe8\xaf\xa5\xe5\xb1\x9e\xe6\x80\xa7\xe8\xbf\x94\xe5\x9b\x9e True\xef\xbc\x8c\xe5\x90\xa6\xe5\x88\x99\xe8\xbf\x94\xe5\x9b\x9e False\n        if hasattr(f, \'close\'):\n            f.close()\n\n    if img.ndim == 2:\n        #\xe6\x89\xa9\xe5\xb1\x95\xe7\xbb\xb4\xe5\xba\xa6\xef\xbc\x8c batch\n        # reshape (H, W) -> (1, H, W)\n        return img[np.newaxis]\n    else:\n        # transpose (H, W, C) -> (C, H, W)\n        return img.transpose((2, 0, 1))\n\n\ndef resize_bbox(bbox, in_size, out_size):\n    """"""Resize bounding boxes according to image resize.\n      \xe6\xa0\xb9\xe6\x8d\xae\xe8\xb0\x83\xe6\x95\xb4\xe5\x9b\xbe\xe5\x83\x8f\xe7\x9a\x84\xe6\xaf\x94\xe4\xbe\x8b\xe8\xb0\x83\xe6\x95\xb4\xe8\xbe\xb9\xe7\x95\x8c\xe6\xa1\x86\n\n    The bounding boxes are expected to be packed into a two dimensional\n    tensor of shape :math:`(R, 4)`, where :math:`R` is the number of\n    bounding boxes in the image. The second axis represents attributes of\n    the bounding box. They are :math:`(y_{min}, x_{min}, y_{max}, x_{max})`,\n    where the four attributes are coordinates of the top left and the\n    bottom right vertices.\n    \xc2\xa0 \xe9\xa2\x84\xe8\xae\xa1bboxes\xe5\xb0\x86\xe8\xa2\xab\xe6\x89\x93\xe5\x8c\x85\xe6\x88\x90\xe4\xba\x8c\xe7\xbb\xb4\xe5\xbc\xa0\xe9\x87\x8f\xe5\xbd\xa2\xe7\x8a\xb6\xef\xbc\x9a\xef\xbc\x88R\xef\xbc\x8c4\xef\xbc\x89\n      \xe5\x85\xb6\xe4\xb8\xad\xef\xbc\x9a\xe6\x95\xb0\xe5\xad\xa6\xef\xbc\x9a`R`\xe6\x98\xaf\xe5\x9b\xbe\xe5\x83\x8f\xe4\xb8\xad\xe7\x9a\x84\xe8\xbe\xb9\xe7\x95\x8c\xe6\xa1\x86\xe7\x9a\x84\xe4\xb8\xaa\xe6\x95\xb0\xe3\x80\x82\n      \xe7\xac\xac\xe4\xba\x8c\xe4\xb8\xaa\xe8\xbd\xb4\xe8\xa1\xa8\xe7\xa4\xba\xe5\xb1\x9e\xe6\x80\xa7\xe8\xbe\xb9\xe7\x95\x8c\xe6\xa1\x86\xe3\x80\x82 \xef\xbc\x88y_ {min}\xef\xbc\x8cx_ {min}\xef\xbc\x8cy_ {max}\xef\xbc\x8cx_ {max}\xef\xbc\x89`\xef\xbc\x8c\n\xc2\xa0\xc2\xa0\xc2\xa0\xc2\xa0 \xe5\x85\xb6\xe4\xb8\xad\xe5\x9b\x9b\xe4\xb8\xaa\xe5\xb1\x9e\xe6\x80\xa7\xe6\x98\xaf\xe5\xb7\xa6\xe4\xb8\x8a\xe8\xa7\x92\xe7\x9a\x84\xe5\x9d\x90\xe6\xa0\x87\xe5\x92\x8c\xe5\x8f\xb3\xe4\xb8\x8b\xe8\xa7\x92\xe7\x9a\x84\xe9\xa1\xb6\xe7\x82\xb9\xe3\x80\x82\n\n\n    Args:\n        bbox (~numpy.ndarray): An array whose shape is :math:`(R, 4)`.\n            :math:`R` is the number of bounding boxes.\n        in_size (tuple): A tuple of length 2. The height and the width\n            of the image before resized.\n            \xe5\x9b\xbe\xe5\x83\x8f\xe8\xb0\x83\xe6\x95\xb4\xe4\xb9\x8b\xe5\x89\x8d\xe7\x9a\x84\xe9\xab\x98\xe5\x92\x8c\xe5\xae\xbd\n        out_size (tuple): A tuple of length 2. The height and the width\n            of the image after resized.\n            \xe5\x9b\xbe\xe5\x83\x8f\xe8\xb0\x83\xe6\x95\xb4\xe4\xb9\x8b\xe5\x90\x8e\xe7\x9a\x84\xe9\xab\x98\xe5\x92\x8c\xe5\xae\xbd\n\n    Returns:\n        ~numpy.ndarray:\n        Bounding boxes rescaled according to the given image shapes.\n\n    """"""\n    #\xe5\x8d\xb3copy\xe4\xb8\x80\xe4\xbb\xbd\xe5\x86\x8d\xe7\x94\xa8\xef\xbc\x8c\xe4\xb8\x8d\xe7\x84\xb6\xe4\xbc\x9a\xe5\x9b\xa0\xe4\xb8\xba\xe5\x85\xb1\xe4\xba\xab\xe5\x86\x85\xe5\xad\x98\xef\xbc\x8c\xe6\x93\x8d\xe4\xbd\x9c\xe5\x90\x8c\xe4\xb8\x80\xe4\xb8\xaabbox.\n    bbox = bbox.copy()\n    y_scale = float(out_size[0]) / in_size[0]\n    x_scale = float(out_size[1]) / in_size[1]\n    bbox[:, 0] = y_scale * bbox[:, 0]\n    bbox[:, 2] = y_scale * bbox[:, 2]\n    bbox[:, 1] = x_scale * bbox[:, 1]\n    bbox[:, 3] = x_scale * bbox[:, 3]\n    return bbox\n\n\ndef flip_bbox(bbox, size, y_flip=False, x_flip=False):\n    """"""Flip bounding boxes accordingly.\n      \xe7\x9b\xb8\xe5\xba\x94\xe5\x9c\xb0\xe7\xbf\xbb\xe8\xbd\xacbboxes\n\n    The bounding boxes are expected to be packed into a two dimensional\n    tensor of shape :math:`(R, 4)`, where :math:`R` is the number of\n    bounding boxes in the image. The second axis represents attributes of\n    the bounding box. They are :math:`(y_{min}, x_{min}, y_{max}, x_{max})`,\n    where the four attributes are coordinates of the top left and the\n    bottom right vertices.\n       \xe9\xa2\x84\xe8\xae\xa1bboxes\xe5\xb0\x86\xe8\xa2\xab\xe6\x89\x93\xe5\x8c\x85\xe6\x88\x90\xe4\xba\x8c\xe7\xbb\xb4\xe5\xbc\xa0\xe9\x87\x8f\xe5\xbd\xa2\xe7\x8a\xb6\xef\xbc\x9a\xef\xbc\x88R\xef\xbc\x8c4\xef\xbc\x89\n      \xe5\x85\xb6\xe4\xb8\xad\xef\xbc\x9a\xe6\x95\xb0\xe5\xad\xa6\xef\xbc\x9a`R`\xe6\x98\xaf\xe5\x9b\xbe\xe5\x83\x8f\xe4\xb8\xad\xe7\x9a\x84\xe8\xbe\xb9\xe7\x95\x8c\xe6\xa1\x86\xe7\x9a\x84\xe4\xb8\xaa\xe6\x95\xb0\xe3\x80\x82\n      \xe7\xac\xac\xe4\xba\x8c\xe4\xb8\xaa\xe8\xbd\xb4\xe8\xa1\xa8\xe7\xa4\xba\xe5\xb1\x9e\xe6\x80\xa7\xe8\xbe\xb9\xe7\x95\x8c\xe6\xa1\x86\xe3\x80\x82 \xef\xbc\x88y_ {min}\xef\xbc\x8cx_ {min}\xef\xbc\x8cy_ {max}\xef\xbc\x8cx_ {max}\xef\xbc\x89`\xef\xbc\x8c\n\xc2\xa0\xc2\xa0\xc2\xa0\xc2\xa0 \xe5\x85\xb6\xe4\xb8\xad\xe5\x9b\x9b\xe4\xb8\xaa\xe5\xb1\x9e\xe6\x80\xa7\xe6\x98\xaf\xe5\xb7\xa6\xe4\xb8\x8a\xe8\xa7\x92\xe7\x9a\x84\xe5\x9d\x90\xe6\xa0\x87\xe5\x92\x8c\xe5\x8f\xb3\xe4\xb8\x8b\xe8\xa7\x92\xe7\x9a\x84\xe9\xa1\xb6\xe7\x82\xb9\xe3\x80\x82\n\n    Args:\n        bbox (~numpy.ndarray): An array whose shape is :math:`(R, 4)`.\n            :math:`R` is the number of bounding boxes.\n        size (tuple): A tuple of length 2. The height and the width\n            of the image before resized.\xe9\xab\x98\xe5\xba\xa6\xe5\x92\x8c\xe5\xae\xbd\xe5\xba\xa6\n        y_flip (bool): Flip bounding box according to a vertical flip of\n            an image.\xe6\xa0\xb9\xe6\x8d\xae\xe5\x9e\x82\xe7\x9b\xb4\xe7\xbf\xbb\xe8\xbd\xac\xe7\x9a\x84\xe6\x96\xb9\xe5\xbc\x8f\xe7\xbf\xbb\xe8\xbd\xac\xe8\xbe\xb9\xe7\x95\x8c\xe6\xa1\x86\n        x_flip (bool): Flip bounding box according to a horizontal flip of\n            an image.\xe6\xa0\xb9\xe6\x8d\xae\xe6\xb0\xb4\xe5\xb9\xb3\xe7\xbf\xbb\xe8\xbd\xac\xe6\x9d\xa5\xe7\xbf\xbb\xe8\xbd\xacbboxes\n\n    Returns:\n        ~numpy.ndarray:\n        Bounding boxes flipped according to the given flips.\n\n    """"""\n    H, W = size\n    bbox = bbox.copy()\n    if y_flip:\n        y_max = H - bbox[:, 0]\n        y_min = H - bbox[:, 2]\n        bbox[:, 0] = y_min\n        bbox[:, 2] = y_max\n    if x_flip:\n        x_max = W - bbox[:, 1]\n        x_min = W - bbox[:, 3]\n        bbox[:, 1] = x_min\n        bbox[:, 3] = x_max\n    return bbox\n\n\ndef crop_bbox(\n        bbox, y_slice=None, x_slice=None,\n        allow_outside_center=True, return_param=False):\n    """"""Translate bounding boxes to fit within the cropped area of an image.\n    \xe8\xbd\xac\xe6\x8d\xa2\xe8\xbe\xb9\xe7\x95\x8c\xe6\xa1\x86\xe4\xbb\xa5\xe9\x80\x82\xe5\xba\x94\xe5\x9b\xbe\xe5\x83\x8f\xe7\x9a\x84\xe8\xa3\x81\xe5\x89\xaa\xe5\x8c\xba\xe5\x9f\x9f\n\n    This method is mainly used together with image cropping.\n    This method translates the coordinates of bounding boxes like\n    :func:`data.util.translate_bbox`. In addition,\n    this function truncates the bounding boxes to fit within the cropped area.\n    If a bounding box does not overlap with the cropped area,\n    this bounding box will be removed.\n    \xe6\xad\xa4\xe6\x96\xb9\xe6\xb3\x95\xe4\xb8\xbb\xe8\xa6\x81\xe4\xb8\x8e\xe5\x9b\xbe\xe5\x83\x8f\xe8\xa3\x81\xe5\x89\xaa\xe4\xb8\x80\xe8\xb5\xb7\xe4\xbd\xbf\xe7\x94\xa8\xe3\x80\x82\n    \xe8\xbf\x99\xe4\xb8\xaa\xe6\x96\xb9\xe6\xb3\x95\xe8\xbd\xac\xe6\x8d\xa2\xe4\xba\x86\xe8\xbe\xb9\xe7\x95\x8c\xe6\xa1\x86\xe7\x9a\x84\xe5\x9d\x90\xe6\xa0\x87\xef\xbc\x8c\xe7\xb1\xbb\xe4\xbc\xbc\xe4\xba\x8e\xe6\x9c\xac\xe6\x96\x87\xe4\xb8\xad\xe7\x9a\x84translate_bbox\xe6\x96\xb9\xe6\xb3\x95\xe3\x80\x82\n    \xe6\xad\xa4\xe5\xa4\x96\xef\xbc\x8c\xe6\xad\xa4\xe5\x8a\x9f\xe8\x83\xbd\xe4\xbc\x9a\xe6\x88\xaa\xe6\x96\xad\xe8\xbe\xb9\xe6\xa1\x86\xe4\xbb\xa5\xe9\x80\x82\xe5\xba\x94\xe8\xa3\x81\xe5\x89\xaa\xe5\x8c\xba\xe5\x9f\x9f\xe3\x80\x82\n    \xe5\xa6\x82\xe6\x9e\x9c\xe8\xbe\xb9\xe7\x95\x8c\xe6\xa1\x86\xe4\xb8\x8d\xe4\xb8\x8e\xe8\xa3\x81\xe5\x89\xaa\xe5\x8c\xba\xe5\x9f\x9f\xe9\x87\x8d\xe5\x8f\xa0\xef\xbc\x8c\xe6\xad\xa4\xe8\xbe\xb9\xe7\x95\x8c\xe6\xa1\x86\xe5\xb0\x86\xe8\xa2\xab\xe5\x88\xa0\xe9\x99\xa4\xe3\x80\x82\n\n    The bounding boxes are expected to be packed into a two dimensional\n    tensor of shape :math:`(R, 4)`, where :math:`R` is the number of\n    bounding boxes in the image. The second axis represents attributes of\n    the bounding box. They are :math:`(y_{min}, x_{min}, y_{max}, x_{max})`,\n    where the four attributes are coordinates of the top left and the\n    bottom right vertices.\n      \xe9\xa2\x84\xe8\xae\xa1bboxes\xe5\xb0\x86\xe8\xa2\xab\xe6\x89\x93\xe5\x8c\x85\xe6\x88\x90\xe4\xba\x8c\xe7\xbb\xb4\xe5\xbc\xa0\xe9\x87\x8f\xe5\xbd\xa2\xe7\x8a\xb6\xef\xbc\x9a\xef\xbc\x88R\xef\xbc\x8c4\xef\xbc\x89\n      \xe5\x85\xb6\xe4\xb8\xad\xef\xbc\x9a\xe6\x95\xb0\xe5\xad\xa6\xef\xbc\x9a`R`\xe6\x98\xaf\xe5\x9b\xbe\xe5\x83\x8f\xe4\xb8\xad\xe7\x9a\x84\xe8\xbe\xb9\xe7\x95\x8c\xe6\xa1\x86\xe7\x9a\x84\xe4\xb8\xaa\xe6\x95\xb0\xe3\x80\x82\n      \xe7\xac\xac\xe4\xba\x8c\xe4\xb8\xaa\xe8\xbd\xb4\xe8\xa1\xa8\xe7\xa4\xba\xe5\xb1\x9e\xe6\x80\xa7\xe8\xbe\xb9\xe7\x95\x8c\xe6\xa1\x86\xe3\x80\x82 \xef\xbc\x88y_ {min}\xef\xbc\x8cx_ {min}\xef\xbc\x8cy_ {max}\xef\xbc\x8cx_ {max}\xef\xbc\x89`\xef\xbc\x8c\n\xc2\xa0\xc2\xa0\xc2\xa0\xc2\xa0 \xe5\x85\xb6\xe4\xb8\xad\xe5\x9b\x9b\xe4\xb8\xaa\xe5\xb1\x9e\xe6\x80\xa7\xe6\x98\xaf\xe5\xb7\xa6\xe4\xb8\x8a\xe8\xa7\x92\xe7\x9a\x84\xe5\x9d\x90\xe6\xa0\x87\xe5\x92\x8c\xe5\x8f\xb3\xe4\xb8\x8b\xe8\xa7\x92\xe7\x9a\x84\xe9\xa1\xb6\xe7\x82\xb9\xe3\x80\x82\n\n    Args:\n        bbox (~numpy.ndarray): Bounding boxes to be transformed. The shape is\n            :math:`(R, 4)`. :math:`R` is the number of bounding boxes.\n        y_slice (slice): The slice of y axis.\n        x_slice (slice): The slice of x axis.\n        allow_outside_center (bool): If this argument is :obj:`False`,\n            bounding boxes whose centers are outside of the cropped area\n            are removed. The default value is :obj:`True`.\n        return_param (bool): If :obj:`True`, this function returns\n            indices of kept bounding boxes.\n\n    Returns:\n        ~numpy.ndarray or (~numpy.ndarray, dict):\n\n        If :obj:`return_param = False`, returns an array :obj:`bbox`.\n\n        If :obj:`return_param = True`,\n        returns a tuple whose elements are :obj:`bbox, param`.\n        :obj:`param` is a dictionary of intermediate parameters whose\n        contents are listed below with key, value-type and the description\n        of the value.\n\n        * **index** (*numpy.ndarray*): An array holding indices of used \\\n            bounding boxes.\n\n    """"""\n\n    t, b = _slice_to_bounds(y_slice)\n    l, r = _slice_to_bounds(x_slice)\n    crop_bb = np.array((t, l, b, r))\n\n    if allow_outside_center:\n        mask = np.ones(bbox.shape[0], dtype=bool)\n    else:\n        center = (bbox[:, :2] + bbox[:, 2:]) / 2\n        mask = np.logical_and(crop_bb[:2] <= center, center < crop_bb[2:]) \\\n            .all(axis=1)\n\n    bbox = bbox.copy()\n    bbox[:, :2] = np.maximum(bbox[:, :2], crop_bb[:2])\n    bbox[:, 2:] = np.minimum(bbox[:, 2:], crop_bb[2:])\n    bbox[:, :2] -= crop_bb[:2]\n    bbox[:, 2:] -= crop_bb[:2]\n\n    mask = np.logical_and(mask, (bbox[:, :2] < bbox[:, 2:]).all(axis=1))\n    bbox = bbox[mask]\n\n    if return_param:\n        return bbox, {\'index\': np.flatnonzero(mask)}\n    else:\n        return bbox\n\n\ndef _slice_to_bounds(slice_):\n    if slice_ is None:\n        return 0, np.inf\n\n    if slice_.start is None:\n        l = 0\n    else:\n        l = slice_.start\n\n    if slice_.stop is None:\n        u = np.inf\n    else:\n        u = slice_.stop\n\n    return l, u\n\n\ndef translate_bbox(bbox, y_offset=0, x_offset=0):\n    """"""Translate bounding boxes.\n    \xe8\xbd\xac\xe6\x8d\xa2\xe8\xbe\xb9\xe7\x95\x8c\xe6\xa1\x86\n\n    This method is mainly used together with image transforms, such as padding\n    and cropping, which translates the left top point of the image from\n    coordinate :math:`(0, 0)` to coordinate\n    :math:`(y, x) = (y_{offset}, x_{offset})`.\n    \xe6\xad\xa4\xe6\x96\xb9\xe6\xb3\x95\xe4\xb8\xbb\xe8\xa6\x81\xe4\xb8\x8e\xe5\x9b\xbe\xe5\x83\x8f\xe8\xbd\xac\xe6\x8d\xa2\xe4\xb8\x80\xe8\xb5\xb7\xe4\xbd\xbf\xe7\x94\xa8\xef\xbc\x8c\xe4\xbe\x8b\xe5\xa6\x82\xe5\xa1\xab\xe5\x85\x85\xe5\x92\x8c\xe8\xa3\x81\xe5\x89\xaa\xef\xbc\x8c\xe4\xbb\x8e\xe8\x80\x8c\xe8\xbd\xac\xe6\x8d\xa2\xe5\x9b\xbe\xe5\x83\x8f\xe7\x9a\x84\xe5\xb7\xa6\xe4\xb8\x8a\xe8\xa7\x92\xe5\x9d\x90\xe6\xa0\x87\xe3\x80\x82\n\n    The bounding boxes are expected to be packed into a two dimensional\n    tensor of shape :math:`(R, 4)`, where :math:`R` is the number of\n    bounding boxes in the image. The second axis represents attributes of\n    the bounding box. They are :math:`(y_{min}, x_{min}, y_{max}, x_{max})`,\n    where the four attributes are coordinates of the top left and the\n    bottom right vertices.\n\n    Args:\n        bbox (~numpy.ndarray): Bounding boxes to be transformed. The shape is\n            :math:`(R, 4)`. :math:`R` is the number of bounding boxes.\n        y_offset (int or float): The offset along y axis.\n        x_offset (int or float): The offset along x axis.\n\n    Returns:\n        ~numpy.ndarray:\n        Bounding boxes translated according to the given offsets.\n\n    """"""\n\n    out_bbox = bbox.copy()\n    out_bbox[:, :2] += (y_offset, x_offset)\n    out_bbox[:, 2:] += (y_offset, x_offset)\n\n    return out_bbox\n\n\ndef random_flip(img, y_random=False, x_random=False,\n                return_param=False, copy=False):\n    """"""Randomly flip an image in vertical or horizontal direction.\n    \xe5\x9c\xa8\xe5\x9e\x82\xe7\x9b\xb4\xe6\x88\x96\xe6\xb0\xb4\xe5\xb9\xb3\xe6\x96\xb9\xe5\x90\x91\xe9\x9a\x8f\xe6\x9c\xba\xe7\xbf\xbb\xe8\xbd\xac\xe5\x9b\xbe\xe5\x83\x8f\n\n    Args:\n        img (~numpy.ndarray): An array that gets flipped. This is in\n            CHW format.\n        y_random (bool): Randomly flip in vertical direction.\n        x_random (bool): Randomly flip in horizontal direction.\n        return_param (bool): Returns information of flip.\n        copy (bool): If False, a view of :obj:`img` will be returned.\n\n    Returns:\n        ~numpy.ndarray or (~numpy.ndarray, dict):\n\n        If :obj:`return_param = False`,\n        returns an array :obj:`out_img` that is the result of flipping.\n\n        If :obj:`return_param = True`,\n        returns a tuple whose elements are :obj:`out_img, param`.\n        :obj:`param` is a dictionary of intermediate parameters whose\n        contents are listed below with key, value-type and the description\n        of the value.\n\n        * **y_flip** (*bool*): Whether the image was flipped in the\\\n            vertical direction or not.\n        * **x_flip** (*bool*): Whether the image was flipped in the\\\n            horizontal direction or not.\n\n    """"""\n    y_flip, x_flip = False, False\n    if y_random:\n        y_flip = random.choice([True, False])\n    if x_random:\n        x_flip = random.choice([True, False])\n\n    if y_flip:\n        img = img[:, ::-1, :]\n    if x_flip:\n        img = img[:, :, ::-1]\n\n    if copy:\n        img = img.copy()\n\n    if return_param:\n        return img, {\'y_flip\': y_flip, \'x_flip\': x_flip}\n    else:\n        return img\n'"
FasterRcnn_pytorch/data/voc_dataset.py,0,"b'import os\nimport xml.etree.ElementTree as ET\n\nimport numpy as np\n\nfrom .util import read_image\n\n\nclass VOCBboxDataset:\n    """"""Bounding box dataset for PASCAL `VOC`_.\n    \xe8\xbe\xb9\xe7\x95\x8c\xe6\xa1\x86\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\n\n    .. _`VOC`: http://host.robots.ox.ac.uk/pascal/VOC/voc2012/\n\n    The index corresponds to each image.\n\n    When queried by an index, if :obj:`return_difficult == False`,\n    this dataset returns a corresponding\n    :obj:`img, bbox, label`, a tuple of an image, bounding boxes and labels.\n    This is the default behaviour.\n    If :obj:`return_difficult == True`, this dataset returns corresponding\n    :obj:`img, bbox, label, difficult`. :obj:`difficult` is a boolean array\n    that indicates whether bounding boxes are labeled as difficult or not.\n\n    The bounding boxes are packed into a two dimensional tensor of shape\n    :math:`(R, 4)`, where :math:`R` is the number of bounding boxes in\n    the image. The second axis represents attributes of the bounding box.\n    They are :math:`(y_{min}, x_{min}, y_{max}, x_{max})`, where the\n    four attributes are coordinates of the top left and the bottom right\n    vertices.\n\n    The labels are packed into a one dimensional tensor of shape :math:`(R,)`.\n    :math:`R` is the number of bounding boxes in the image.\n    The class name of the label :math:`l` is :math:`l` th element of\n    :obj:`VOC_BBOX_LABEL_NAMES`.\n\n    The array :obj:`difficult` is a one dimensional boolean array of shape\n    :math:`(R,)`. :math:`R` is the number of bounding boxes in the image.\n    If :obj:`use_difficult` is :obj:`False`, this array is\n    a boolean array with all :obj:`False`.\n\n    The type of the image, the bounding boxes and the labels are as follows.\n\n    * :obj:`img.dtype == numpy.float32`\n    * :obj:`bbox.dtype == numpy.float32`\n    * :obj:`label.dtype == numpy.int32`\n    * :obj:`difficult.dtype == numpy.bool`\n\n    Args:\n        data_dir (string): Path to the root of the training data. \n            i.e. ""/data/image/voc/VOCdevkit/VOC2007/""\n        split ({\'train\', \'val\', \'trainval\', \'test\'}): Select a split of the\n            dataset. :obj:`test` split is only available for\n            2007 dataset.\n        year ({\'2007\', \'2012\'}): Use a dataset prepared for a challenge\n            held in :obj:`year`.\n        use_difficult (bool): If :obj:`True`, use images that are labeled as\n            difficult in the original annotation.\n        return_difficult (bool): If :obj:`True`, this dataset returns\n            a boolean array\n            that indicates whether bounding boxes are labeled as difficult\n            or not. The default value is :obj:`False`.\n\n    """"""\n\n    def __init__(self, data_dir, split=\'trainval\',\n                 use_difficult=False, return_difficult=False,\n                 ):\n\n        # if split not in [\'train\', \'trainval\', \'val\']:\n        #     if not (split == \'test\' and year == \'2007\'):\n        #         warnings.warn(\n        #             \'please pick split from \\\'train\\\', \\\'trainval\\\', \\\'val\\\'\'\n        #             \'for 2012 dataset. For 2007 dataset, you can pick \\\'test\\\'\'\n        #             \' in addition to the above mentioned splits.\'\n        #         )\n        id_list_file = os.path.join(\n            data_dir, \'ImageSets/Main/{0}.txt\'.format(split))\n\n        self.ids = [id_.strip() for id_ in open(id_list_file)]\n        self.data_dir = data_dir\n        self.use_difficult = use_difficult\n        self.return_difficult = return_difficult\n\t\t#\xe5\xb0\x86voc\xe6\xa0\x87\xe7\xad\xbe\xe5\x90\x8d\xe7\xa7\xb0\xe8\xb5\x8b\xe5\x80\xbc\xe7\xbb\x99VOCBboxDataset\xe5\xaf\xb9\xe8\xb1\xa1\n        self.label_names = VOC_BBOX_LABEL_NAMES\n\n    def __len__(self):\n        return len(self.ids)\n    #dataset\xe7\x94\xa8\xe5\x88\xb0\xe8\xaf\xa5\xe6\x96\xb9\xe6\xb3\x95\xe5\xbe\x97\xe5\x88\xb0\xe4\xb8\x80\xe5\xbc\xa0\xe5\x9b\xbe\xe7\x89\x87\xe7\x9a\x84\xe5\x90\x84\xe7\xa7\x8d\xe4\xbf\xa1\xe6\x81\xaf\n    def get_example(self, i):\n        """"""Returns the i-th example.\n\n        Returns a color image and bounding boxes. The image is in CHW format.\n        The returned image is RGB.\n\t\t\xe8\xbf\x94\xe5\x9b\x9e\xe5\xbd\xa9\xe8\x89\xb2\xe5\x9b\xbe\xe5\x83\x8f\xe5\x92\x8cbbox\xe3\x80\x82\xe5\x9b\xbe\xe5\x83\x8f\xe5\xa4\xa7\xe5\xb0\x8f\xe4\xb8\xbaCHW\xef\xbc\x88\xe9\x80\x9a\xe9\x81\x93\xe3\x80\x81\xe9\xab\x98\xe3\x80\x81\xe5\xae\xbd\xef\xbc\x89,\xe8\xbf\x94\xe5\x9b\x9e\xe5\x9b\xbe\xe5\x83\x8f\xe4\xb8\xbaRGB\n\n        Args:\n            i (int): The index of the example.\n\n        Returns:\n            tuple of an image and bounding boxes\n\n        """"""\n        id_ = self.ids[i]\n\t\t#=======================================================================\xe5\x9c\xa8\xe8\xbf\x99\xe9\x87\x8c\xe8\xaf\xbb\xe5\x8f\x96\xe8\xb7\xaf\xe5\xbe\x84\xe5\x90\x8e\xe5\x8f\xaa\xe6\x8b\xbf\xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x80\xe4\xb8\xaa\\\xe5\x90\x8e\xe9\x9d\xa2\xe7\x9a\x84\xe5\x86\x85\xe5\xae\xb9\n        anno = ET.parse(\n            os.path.join(self.data_dir, \'Annotations\', id_ + \'.xml\'))\n        bbox = list()\n        label = list()\n        difficult = list()\n        for obj in anno.findall(\'object\'):\n            # when in not using difficult split, and the object is difficult, skipt it.\n\t\t\t#\xe5\x9c\xa8\xe4\xb8\x8d\xe4\xbd\xbf\xe7\x94\xa8\xe5\x9b\xb0\xe9\x9a\xbe\xe5\x88\x86\xe5\x89\xb2\xe6\x97\xb6\xef\xbc\x8c\xe5\xaf\xb9\xe8\xb1\xa1\xe6\x98\xafdifficult\xef\xbc\x8c\xe8\xb7\xb3\xe8\xbf\x87\xe5\xae\x83\n            if not self.use_difficult and int(obj.find(\'difficult\').text) == 1:\n                continue\n\n            difficult.append(int(obj.find(\'difficult\').text))\n            bndbox_anno = obj.find(\'bndbox\')\n            # subtract 1 to make pixel indexes 0-based\n\t\t\t#\xe5\x87\x8f1 \xe4\xbb\xa5\xe4\xbd\xbf\xe5\x83\x8f\xe7\xb4\xa0\xe7\xb4\xa2\xe5\xbc\x95\xe5\x9f\xba\xe4\xba\x8e0\n            bbox.append([\n                int(bndbox_anno.find(tag).text) - 1\n                for tag in (\'ymin\', \'xmin\', \'ymax\', \'xmax\')])\n            name = obj.find(\'name\').text.lower().strip()\n            label.append(VOC_BBOX_LABEL_NAMES.index(name))\n        bbox = np.stack(bbox).astype(np.float32)\n        label = np.stack(label).astype(np.int32)\n        # When `use_difficult==False`, all elements in `difficult` are False.\n        difficult = np.array(difficult, dtype=np.bool).astype(np.uint8)  # PyTorch don\'t support np.bool\n\n        # Load a image\n        img_file = os.path.join(self.data_dir, \'JPEGImages\', id_ + \'.jpg\')\n        img = read_image(img_file, color=True)\n\n        # if self.return_difficult:\n        #     return img, bbox, label, difficult\n        return img, bbox, label, difficult\n\n    __getitem__ = get_example\n\n#\xe6\xa0\x87\xe7\xad\xbe\xe5\x90\x8d\xe7\xa7\xb0\nVOC_BBOX_LABEL_NAMES = (\n    \'aeroplane\',\n    \'bicycle\',\n    \'bird\',\n    \'boat\',\n    \'bottle\',\n    \'bus\',\n    \'car\',\n    \'cat\',\n    \'chair\',\n    \'cow\',\n    \'diningtable\',\n    \'dog\',\n    \'horse\',\n    \'motorbike\',\n    \'person\',\n    \'pottedplant\',\n    \'sheep\',\n    \'sofa\',\n    \'train\',\n    \'tvmonitor\')\n'"
FasterRcnn_pytorch/misc/convert_caffe_pretrain.py,2,"b'# code from ruotian luo\n# https://github.com/ruotianluo/pytorch-faster-rcnn\nimport torch\nfrom torch.utils.model_zoo import load_url\nfrom torchvision import models\n\nsd = load_url(""https://s3-us-west-2.amazonaws.com/jcjohns-models/vgg16-00b39a1b.pth"")\nsd[\'classifier.0.weight\'] = sd[\'classifier.1.weight\']\nsd[\'classifier.0.bias\'] = sd[\'classifier.1.bias\']\ndel sd[\'classifier.1.weight\']\ndel sd[\'classifier.1.bias\']\n\nsd[\'classifier.3.weight\'] = sd[\'classifier.4.weight\']\nsd[\'classifier.3.bias\'] = sd[\'classifier.4.bias\']\ndel sd[\'classifier.4.weight\']\ndel sd[\'classifier.4.bias\']\n\n\n# speicify the path to save\ntorch.save(sd, ""vgg16_caffe.pth"")'"
FasterRcnn_pytorch/misc/train_fast.py,2,"b""import os\n\nimport ipdb\nimport matplotlib\nfrom tqdm import tqdm\n\nfrom utils.config import opt\nfrom data.dataset import Dataset, TestDataset\nfrom model import FasterRCNNVGG16\nfrom torch.autograd import Variable\nfrom torch.utils import data as data_\nfrom trainer import FasterRCNNTrainer\nfrom utils import array_tool as at\nfrom utils.vis_tool import visdom_bbox\nfrom utils.eval_tool import eval_detection_voc\n\nmatplotlib.use('agg')\n\ndef eval(dataloader, faster_rcnn, test_num=10000):\n    pred_bboxes, pred_labels, pred_scores = list(), list(), list()\n    gt_bboxes, gt_labels, gt_difficults = list(), list(), list()\n    for ii, (imgs, sizes, gt_bboxes_, gt_labels_, gt_difficults_) in tqdm(enumerate(dataloader)):\n        sizes = [sizes[0][0], sizes[1][0]]\n        pred_bboxes_, pred_labels_, pred_scores_ = faster_rcnn.predict(imgs, [sizes])\n        gt_bboxes += list(gt_bboxes_.numpy())\n        gt_labels += list(gt_labels_.numpy())\n        gt_difficults += list(gt_difficults_.numpy())\n        pred_bboxes += pred_bboxes_\n        pred_labels += pred_labels_\n        pred_scores += pred_scores_\n        if ii == test_num: break\n\n    result = eval_detection_voc(\n        pred_bboxes, pred_labels, pred_scores,\n        gt_bboxes, gt_labels, gt_difficults,\n        use_07_metric=True)\n    return result\n\n\ndef train(**kwargs):\n    opt._parse(kwargs)\n\n    dataset = Dataset(opt)\n    print('load data')\n    dataloader = data_.DataLoader(dataset, \\\n                                  batch_size=1, \\\n                                  shuffle=True, \\\n                                  # pin_memory=True,\n                                  num_workers=opt.num_workers)\n    testset = TestDataset(opt)\n    test_dataloader = data_.DataLoader(testset,\n                                       batch_size=1,\n                                       num_workers=2,\n                                       shuffle=False, \\\n                                       # pin_memory=True\n                                       )\n    faster_rcnn = FasterRCNNVGG16()\n    print('model construct completed')\n    trainer = FasterRCNNTrainer(faster_rcnn).cuda()\n    if opt.load_path:\n        trainer.load(opt.load_path)\n        print('load pretrained model from %s' % opt.load_path)\n\n    trainer.vis.text(dataset.db.label_names, win='labels')\n    best_map = 0\n    for epoch in range(7):\n        trainer.reset_meters()\n        for ii, (img, bbox_, label_, scale, ori_img) in tqdm(enumerate(dataloader)):\n            scale = at.scalar(scale)\n            img, bbox, label = img.cuda().float(), bbox_.cuda(), label_.cuda()\n            img, bbox, label = Variable(img), Variable(bbox), Variable(label)\n            losses = trainer.train_step(img, bbox, label, scale)\n\n            if (ii + 1) % opt.plot_every == 0:\n                if os.path.exists(opt.debug_file):\n                    ipdb.set_trace()\n\n                # plot loss\n                trainer.vis.plot_many(trainer.get_meter_data())\n\n                # plot groud truth bboxes\n                ori_img_ = (img * 0.225 + 0.45).clamp(min=0, max=1) * 255\n                gt_img = visdom_bbox(at.tonumpy(ori_img_)[0], \n                                    at.tonumpy(bbox_)[0], \n                                    label_[0].numpy())\n                trainer.vis.img('gt_img', gt_img)\n\n                # plot predicti bboxes\n                _bboxes, _labels, _scores = trainer.faster_rcnn.predict(ori_img,visualize=True)\n                pred_img = visdom_bbox( at.tonumpy(ori_img[0]), \n                                        at.tonumpy(_bboxes[0]),\n                                        at.tonumpy(_labels[0]).reshape(-1), \n                                        at.tonumpy(_scores[0]))\n                trainer.vis.img('pred_img', pred_img)\n\n                # rpn confusion matrix(meter)\n                trainer.vis.text(str(trainer.rpn_cm.value().tolist()), win='rpn_cm')\n                # roi confusion matrix\n                trainer.vis.img('roi_cm', at.totensor(trainer.roi_cm.conf, False).float())\n        if epoch==4:\n            trainer.faster_rcnn.scale_lr(opt.lr_decay)\n\n    eval_result = eval(test_dataloader, faster_rcnn, test_num=1e100)\n    print('eval_result')\n    trainer.save(mAP=eval_result['map'])\n\nif __name__ == '__main__':\n    import fire\n\n    fire.Fire()\n"""
FasterRcnn_pytorch/model/__init__.py,0,b'from .faster_rcnn_vgg16 import FasterRCNNVGG16\n'
FasterRcnn_pytorch/model/faster_rcnn.py,2,"b'from __future__ import division\nimport torch as t\nimport numpy as np\nimport cupy as cp\nfrom utils import array_tool as at\nfrom model.utils.bbox_tools import loc2bbox\nfrom model.utils.nms import non_maximum_suppression\n\nfrom torch import nn\nfrom data.dataset import preprocess\nfrom torch.nn import functional as F\nfrom utils.config import opt\n\n\nclass FasterRCNN(nn.Module):\n    """"""Base class for Faster R-CNN.\n    \xe7\xbb\xa7\xe6\x89\xbfpytorch\xe6\xa8\xa1\xe5\x9e\x8b\n    This is a base class for Faster R-CNN links supporting object detection\n    API [#]_. The following three stages constitute Faster R-CNN.\n\n    1. **Feature extraction**: Images are taken and their \\\n        feature maps are calculated.\n    2. **Region Proposal Networks**: Given the feature maps calculated in \\\n        the previous stage, produce set of RoIs around objects.\n    3. **Localization and Classification Heads**: Using feature maps that \\\n        belong to the proposed RoIs, classify the categories of the objects \\\n        in the RoIs and improve localizations.\n        \xe4\xbd\xbf\xe7\x94\xa8RoIs\xe6\x8f\x90\xe8\xae\xae\xe7\x9a\x84\xe7\x9a\x84feature maps\xef\xbc\x8c\xe5\xaf\xb9RoI\xe4\xb8\xad\xe7\x9a\x84\xe5\xaf\xb9\xe8\xb1\xa1\xe8\xbf\x9b\xe8\xa1\x8c\xe5\x88\x86\xe7\xb1\xbb\xe5\xb9\xb6\xe6\x8f\x90\xe9\xab\x98\xe7\x9b\xae\xe6\xa0\x87\xe6\xa1\x86\xe5\xae\x9a\xe4\xbd\x8d\n\n    Each stage is carried out by one of the callable\n    :class:`torch.nn.Module` objects :obj:`feature`, :obj:`rpn` and :obj:`head`.\n\n    There are two functions :meth:`predict` and :meth:`__call__` to conduct\n    object detection.\n    :meth:`predict` takes images and returns bounding boxes that are converted\n    to image coordinates. This will be useful for a scenario when\n    Faster R-CNN is treated as a black box function, for instance.\n    :meth:`__call__` is provided for a scnerario when intermediate outputs\n    are needed, for instance, for training and debugging.\n\n    Links that support obejct detection API have method :meth:`predict` with\n    the same interface. Please refer to :meth:`predict` for\n    further details.\n\n    .. [#] Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun. \\\n    Faster R-CNN: Towards Real-Time Object Detection with \\\n    Region Proposal Networks. NIPS 2015.\n\n    Args:\n        extractor (nn.Module): A module that takes a BCHW image\n            array and returns feature maps.\n              \xe8\xbe\x93\xe5\x85\xa5\xe4\xb8\x80\xe5\xbc\xa0 BCHW image\xe6\x95\xb0\xe7\xbb\x84\xef\xbc\x8c\xe8\xbf\x94\xe5\x9b\x9e\xe4\xb8\x80\xe4\xb8\xaafeature maps\n        rpn (nn.Module): A module that has the same interface as\n            :class:`model.region_proposal_network.RegionProposalNetwork`.\n            Please refer to the documentation found there.\n             \xe8\xbf\x99\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa\xe5\x8c\xba\xe5\x9f\x9f\xe6\x8f\x90\xe6\xa1\x88\xe7\xbd\x91\xe7\xbb\x9c\xe3\x80\x82\xe5\xae\x83\xe6\x8f\x90\xe5\x8f\x96\xe5\x9b\xbe\xe5\x83\x8f\xe7\x89\xb9\xe5\xbe\x81\xef\xbc\x8c\xe8\xbe\x93\xe5\x87\xba \xe5\x80\x99\xe9\x80\x89\xe6\xa1\x86bboxes\n             Region Proposal Network\xef\xbc\x88RPN\xef\xbc\x89\xe4\xbb\xa3\xe6\x9b\xbf\xe4\xba\x86Selective Search\n        head (nn.Module): A module that takes\n            a BCHW variable, RoIs and batch indices for RoIs. This returns class\n            dependent localization paramters and class scores.\n        loc_normalize_mean (tuple of four floats): Mean values of\n            localization estimates.\n        loc_normalize_std (tupler of four floats): Standard deviation\n            of localization estimates.\n\n    """"""\n\n    def __init__(self, extractor, rpn, head,\n                loc_normalize_mean = (0., 0., 0., 0.),\n                loc_normalize_std = (0.1, 0.1, 0.2, 0.2)\n    ):\n        super(FasterRCNN, self).__init__()\n        self.extractor = extractor\n        #rpn\xe7\xbd\x91\xe7\xbb\x9c\n        self.rpn = rpn\n        #roi head\xe7\xbd\x91\xe7\xbb\x9c\xe8\xbf\x9b\xe8\xa1\x8c\xe9\xa2\x84\xe6\xb5\x8b\xe7\xb1\xbb\xe5\x88\xab\xe5\x92\x8c\xe7\x9b\xae\xe6\xa0\x87\xe6\xa1\x86\n        self.head = head\n\n        # mean and std\n        self.loc_normalize_mean = loc_normalize_mean\n        self.loc_normalize_std = loc_normalize_std\n        self.use_preset(\'evaluate\')\n\n    @property\n    def n_class(self):\n        # Total number of classes including the background.\n        return self.head.n_class\n\n    def forward(self, x, scale=1.):\n        """"""Forward Faster R-CNN.\n\n        Scaling paramter :obj:`scale` is used by RPN to determine the\n        threshold to select small objects, which are going to be\n        rejected irrespective of their confidence scores.\n\n        Here are notations used.\n\n        * :math:`N` is the number of batch size\n        * :math:`R\'` is the total number of RoIs produced across batches. \\\n            Given :math:`R_i` proposed RoIs from the :math:`i` th image, \\\n            :math:`R\' = \\\\sum _{i=1} ^ N R_i`.\n        * :math:`L` is the number of classes excluding the background.\n\n        Classes are ordered by the background, the first class, ..., and\n        the :math:`L` th class.\n\n        Args:\n            x (autograd.Variable): 4D image variable.\n            scale (float): Amount of scaling applied to the raw image\n                during preprocessing.\n\n        Returns:\n            Variable, Variable, array, array:\n            Returns tuple of four values listed below.\n\n            * **roi_cls_locs**: Offsets and scalings for the proposed RoIs. \\\n                Its shape is :math:`(R\', (L + 1) \\\\times 4)`.\n            * **roi_scores**: Class predictions for the proposed RoIs. \\\n                Its shape is :math:`(R\', L + 1)`.\n            * **rois**: RoIs proposed by RPN. Its shape is \\\n                :math:`(R\', 4)`.\n            * **roi_indices**: Batch indices of RoIs. Its shape is \\\n                :math:`(R\',)`.\n\n        """"""\n        img_size = x.shape[2:]\n\n        h = self.extractor(x)\n        rpn_locs, rpn_scores, rois, roi_indices, anchor = \\\n            self.rpn(h, img_size, scale)\n        roi_cls_locs, roi_scores = self.head(\n            h, rois, roi_indices)\n        return roi_cls_locs, roi_scores, rois, roi_indices\n\n    def use_preset(self, preset):\n        """"""Use the given preset during prediction.\n\n        This method changes values of :obj:`self.nms_thresh` and\n        :obj:`self.score_thresh`. These values are a threshold value\n        used for non maximum suppression and a threshold value\n        to discard low confidence proposals in :meth:`predict`,\n        respectively.\n\n        If the attributes need to be changed to something\n        other than the values provided in the presets, please modify\n        them by directly accessing the public attributes.\n\n        Args:\n            preset ({\'visualize\', \'evaluate\'): A string to determine the\n                preset to use.\n\n        """"""\n        if preset == \'visualize\':\n            self.nms_thresh = 0.3\n            self.score_thresh = 0.7\n        elif preset == \'evaluate\':\n            self.nms_thresh = 0.3\n            self.score_thresh = 0.05\n        else:\n            raise ValueError(\'preset must be visualize or evaluate\')\n\n    def _suppress(self, raw_cls_bbox, raw_prob):\n        bbox = list()\n        label = list()\n        score = list()\n        # skip cls_id = 0 because it is the background class\n        for l in range(1, self.n_class):\n            cls_bbox_l = raw_cls_bbox.reshape((-1, self.n_class, 4))[:, l, :]\n            prob_l = raw_prob[:, l]\n            mask = prob_l > self.score_thresh\n            cls_bbox_l = cls_bbox_l[mask]\n            prob_l = prob_l[mask]\n            keep = non_maximum_suppression(\n                cp.array(cls_bbox_l), self.nms_thresh, prob_l)\n            keep = cp.asnumpy(keep)\n            bbox.append(cls_bbox_l[keep])\n            # The labels are in [0, self.n_class - 2].\n            label.append((l - 1) * np.ones((len(keep),)))\n            score.append(prob_l[keep])\n        bbox = np.concatenate(bbox, axis=0).astype(np.float32)\n        label = np.concatenate(label, axis=0).astype(np.int32)\n        score = np.concatenate(score, axis=0).astype(np.float32)\n        return bbox, label, score\n\n    def predict(self, imgs,sizes=None,visualize=False):\n        """"""Detect objects from images.\n        \xe4\xbb\x8e\xe5\x9b\xbe\xe5\x83\x8f\xe4\xb8\xad\xe6\xa3\x80\xe6\xb5\x8b\xe7\x89\xa9\xe4\xbd\x93\n\n        This method predicts objects for each image.\n          \xe6\xad\xa4\xe6\x96\xb9\xe6\xb3\x95\xe9\xa2\x84\xe6\xb5\x8b\xe6\xaf\x8f\xe4\xb8\xaa\xe5\x9b\xbe\xe5\x83\x8f\xe7\x9a\x84\xe5\xaf\xb9\xe8\xb1\xa1\xe3\x80\x82\n        Args:\n            imgs (iterable of numpy.ndarray): Arrays holding images.\n                All images are in CHW and RGB format\n                and the range of their value is :math:`[0, 255]`.\n\n        Returns:\n           tuple of lists:\n           This method returns a tuple of three lists,\n           :obj:`(bboxes, labels, scores)`.\n\n           * **bboxes**: A list of float arrays of shape :math:`(R, 4)`, \\\n               where :math:`R` is the number of bounding boxes in a image. \\\n               Each bouding box is organized by \\\n               :math:`(y_{min}, x_{min}, y_{max}, x_{max})` \\\n               in the second axis.\n           * **labels** : A list of integer arrays of shape :math:`(R,)`. \\\n               Each value indicates the class of the bounding box. \\\n               Values are in range :math:`[0, L - 1]`, where :math:`L` is the \\\n               number of the foreground classes.\n           * **scores** : A list of float arrays of shape :math:`(R,)`. \\\n               Each value indicates how confident the prediction is.\n\n        """"""\n        #\xe5\xb0\x86\xe6\xa8\xa1\xe5\x9d\x97\xe8\xae\xbe\xe7\xbd\xae\xe4\xb8\xba\xe8\xaf\x84\xe4\xbc\xb0\xe6\xa8\xa1\xe5\xbc\x8f\xe3\x80\x82\xe8\xbf\x99\xe5\x8f\xaa\xe5\xaf\xb9\xe8\xaf\xb8\xe5\xa6\x82Dropout\xe6\x88\x96BatchNorm\xe7\xad\x89\xe6\xa8\xa1\xe5\x9d\x97\xe6\x9c\x89\xe4\xbb\xbb\xe4\xbd\x95\xe5\xbd\xb1\xe5\x93\x8d\xe3\x80\x82module\xe4\xb8\xad\xe7\x9a\x84\xe6\x96\xb9\xe6\xb3\x95\n        self.eval()\n        #\xe5\x8f\xaf\xe8\xa7\x86\xe5\x8c\x96\n        if visualize:\n            #\xe8\xae\xbe\xe7\xbd\xae\xe4\xb8\xba\xe5\x8f\xaf\xe8\xa7\x86\xe5\x8c\x96  \xe8\xae\xbe\xe7\xbd\xae self.nms_thresh = 0.3   self.score_thresh = 0.7\n            #\xe8\xaf\x84\xe4\xbc\xb0\xe6\xa8\xa1\xe5\xbc\x8f \xe5\x92\x8c \xe5\x8f\xaf\xe8\xa7\x86\xe5\x8c\x96\xe6\xa8\xa1\xe5\xbc\x8f \xe4\xbd\xbf\xe7\x94\xa8\xe4\xb8\x8d\xe5\x90\x8c\xe7\x9a\x84nms\xe6\x9c\x80\xe5\xa4\xa7\xe5\x8c\x96\xe6\x8a\x91\xe5\x88\xb6 \xe5\x92\x8c\xe9\x98\x88\xe5\x80\xbc\n            self.use_preset(\'visualize\')\n            prepared_imgs = list()\n            sizes = list()\n            for img in imgs:\n                size = img.shape[1:]\n                img = preprocess(at.tonumpy(img))\n                prepared_imgs.append(img)\n                sizes.append(size)\n        else:\n             prepared_imgs = imgs \n        bboxes = list()\n        labels = list()\n        scores = list()\n        #size[600,800]\n        for img, size in zip(prepared_imgs, sizes):\n            #img\xe7\x94\xb1[3,600,800]\xe8\xbd\xac\xe4\xb8\xba[1\xef\xbc\x8c3,600,800]  \xe8\xbd\xac\xe4\xb8\xba\xe5\x8f\x98\xe9\x87\x8f\xef\xbc\x8c\xe6\x89\xa9\xe5\x85\x85\xe4\xb8\x80\xe7\xbb\xb4 \xe5\xb9\xb6\xe8\xae\xbe\xe7\xbd\xae\xe4\xb8\xba \xe9\xa2\x84\xe6\xb5\x8b\xe6\xa8\xa1\xe5\xbc\x8f\n            img = t.autograd.Variable(at.totensor(img).float()[None], volatile=True)\n            #scale \xe4\xb8\xba1\n            scale = img.shape[3] / size[1]\n            roi_cls_loc, roi_scores, rois, _ = self(img, scale=scale)\n            # We are assuming that batch size is 1.\n            roi_score = roi_scores.data\n            roi_cls_loc = roi_cls_loc.data\n            roi = at.totensor(rois) / scale\n\n            # Convert predictions to bounding boxes in image coordinates.\n            # Bounding boxes are scaled to the scale of the input images.\n            mean = t.Tensor(self.loc_normalize_mean).cuda(). \\\n                repeat(self.n_class)[None]\n            std = t.Tensor(self.loc_normalize_std).cuda(). \\\n                repeat(self.n_class)[None]\n\n            roi_cls_loc = (roi_cls_loc * std + mean)\n            roi_cls_loc = roi_cls_loc.view(-1, self.n_class, 4)\n            roi = roi.view(-1, 1, 4).expand_as(roi_cls_loc)\n            cls_bbox = loc2bbox(at.tonumpy(roi).reshape((-1, 4)),\n                                at.tonumpy(roi_cls_loc).reshape((-1, 4)))\n            cls_bbox = at.totensor(cls_bbox)\n            cls_bbox = cls_bbox.view(-1, self.n_class * 4)\n            # clip bounding box\n            cls_bbox[:, 0::2] = (cls_bbox[:, 0::2]).clamp(min=0, max=size[0])\n            cls_bbox[:, 1::2] = (cls_bbox[:, 1::2]).clamp(min=0, max=size[1])\n\n            prob = at.tonumpy(F.softmax(at.tovariable(roi_score), dim=1))\n\n            raw_cls_bbox = at.tonumpy(cls_bbox)\n            raw_prob = at.tonumpy(prob)\n\n            bbox, label, score = self._suppress(raw_cls_bbox, raw_prob)\n            bboxes.append(bbox)\n            labels.append(label)\n            scores.append(score)\n\n        self.use_preset(\'evaluate\')\n        self.train()\n        return bboxes, labels, scores\n\n    def get_optimizer(self):\n        """"""\n        return optimizer, It could be overwriten if you want to specify \n        special optimizer\n        """"""\n        lr = opt.lr\n        params = []\n        for key, value in dict(self.named_parameters()).items():\n            if value.requires_grad:\n                if \'bias\' in key:\n                    params += [{\'params\': [value], \'lr\': lr * 2, \'weight_decay\': 0}]\n                else:\n                    params += [{\'params\': [value], \'lr\': lr, \'weight_decay\': opt.weight_decay}]\n        if opt.use_adam:\n            self.optimizer = t.optim.Adam(params)\n        else:\n            self.optimizer = t.optim.SGD(params, momentum=0.9)\n        return self.optimizer\n\n    def scale_lr(self, decay=0.1):\n        for param_group in self.optimizer.param_groups:\n            param_group[\'lr\'] *= decay\n        return self.optimizer\n\n\n\n\n'"
FasterRcnn_pytorch/model/faster_rcnn_vgg16.py,0,"b'import torch as t\nfrom torch import nn\nfrom torchvision.models import vgg16\nfrom model.region_proposal_network import RegionProposalNetwork\nfrom model.faster_rcnn import FasterRCNN\nfrom model.roi_module import RoIPooling2D\nfrom utils import array_tool as at\nfrom utils.config import opt\n\n\ndef decom_vgg16():\n    #\xe6\x9e\x84\xe5\xbb\xba\xe6\xa8\xa1\xe5\x9e\x8b\n    # the 30th layer of features is relu of conv5_3\n    #\xe7\xac\xac30\xe5\xb1\x82\xe5\x8a\x9f\xe8\x83\xbd\xe6\x98\xafconv5_3\xe7\x9a\x84relu\n    \n    #\xe5\xa6\x82\xe6\x9e\x9c\xe9\xa2\x84\xe8\xae\xad\xe7\xbb\x83\xe7\x9a\x84caffe\xe6\xa8\xa1\xe5\x9e\x8b\xe5\xad\x98\xe5\x9c\xa8\xef\xbc\x8c\xe5\x88\x99\xe5\x8a\xa0\xe8\xbd\xbdcaffe\xe6\xa8\xa1\xe5\x9e\x8b\n    if opt.caffe_pretrain:\n        #vgg16\xe6\x96\xb9\xe6\xb3\x95\xe6\x98\xafpytorch\xe6\x96\xb9\xe6\xb3\x95\n        model = vgg16(pretrained=False)\n        if not opt.load_path:\n            model.load_state_dict(t.load(opt.caffe_pretrain_path))\n    #\xe5\x90\xa6\xe5\x88\x99\xe5\x8a\xa0\xe8\xbd\xbdpytorch\xe8\x87\xaa\xe8\xba\xab\xe7\x9a\x84\xe9\xa2\x84\xe8\xae\xad\xe7\xbb\x83\xe5\xa5\xbd\xe7\x9a\x84vgg16\xe6\xa8\xa1\xe5\x9e\x8b\n    else:\n        model = vgg16(not opt.load_path)\n    \n    #\xe7\xac\xac30\xe5\xb1\x82\xe5\x8a\x9f\xe8\x83\xbd\xe6\x98\xafconv5_3\xe7\x9a\x84relu\n    #\xe5\x8f\x96\xe5\x87\xba\xe7\xac\xac30\xe5\xb1\x82\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xef\xbc\x8c\xe5\x8d\xb3conv5\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe3\x80\x82\xef\xbc\x88\xe5\x9c\xa8pooling\xe4\xb9\x8b\xe5\x89\x8d\xe7\x9a\x84\xe4\xb8\x80\xe5\xb1\x82\xe5\x8f\x96\xe5\x87\xba\xe6\x95\xb0\xe6\x8d\xae\xe4\xbd\x9c\xe4\xb8\xbafeatures\xef\xbc\x89\n    features = list(model.features)[:30]\n    #vgg16\xe7\x9a\x84\xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x89\xe5\xb1\x82\xe5\x88\x86\xe7\xb1\xbb\xe5\xb1\x82\n    #init\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe6\x96\xb9\xe6\xb3\x95\xe9\x80\x9a\xe8\xbf\x87classifier\xe5\x91\xbd\xe5\x90\x8d\xe5\x8c\x85\xe5\x90\xab\xe4\xba\x86\xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x89\xe5\xb1\x82\xe5\x88\x86\xe7\xb1\xbb\xe5\xb1\x82\xef\xbc\x88\xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5+\xe6\xbf\x80\xe6\xb4\xbb\xe5\x87\xbd\xe6\x95\xb0\xef\xbc\x89\n    classifier = model.classifier\n    #\xe5\xb0\x86\xe4\xb8\x89\xe5\xb1\x82\xe5\x88\x86\xe7\xb1\xbb\xe5\xb1\x82\xe6\x94\xbe\xe5\x85\xa5list\xe4\xb8\xad\n    classifier = list(classifier)\n    #\xe5\x88\xa0\xe9\x99\xa4\xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x80\xe4\xb8\xaa\xe5\x88\x86\xe7\xb1\xbb\xe5\xb1\x82\xe7\x9a\x84\xe6\xbf\x80\xe6\xb4\xbb\xe5\x87\xbd\xe6\x95\xb0\n    del classifier[6]\n    #use dropout in RoIHead(config.py)\n    if not opt.use_drop:\n        del classifier[5]\n        del classifier[2]\n    classifier = nn.Sequential(*classifier)\n\n    # freeze top4 conv\n    #\xe5\x86\xbb\xe7\xbb\x93\xe5\x89\x8d\xe5\x9b\x9b\xe5\xb1\x82\xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82\n    #\xe4\xb8\xba\xe4\xba\x86\xe8\x8a\x82\xe7\x9c\x81\xe6\x98\xbe\xe5\xad\x98\xef\xbc\x8c\xe5\x89\x8d\xe5\x9b\x9b\xe5\xb1\x82\xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82\xe7\x9a\x84\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\xe8\xae\xbe\xe4\xb8\xba0\n    for layer in features[:10]:\n        for p in layer.parameters():\n            #false\xe5\x8d\xb3\xe4\xb8\xba\xe4\xb8\x8d\xe9\x9c\x80\xe8\xa6\x81\xe6\xa2\xaf\xe5\xba\xa6\xe6\xb1\x82\xe5\xaf\xbc\xef\xbc\x8c\xe4\xb8\x8d\xe6\x9b\xb4\xe6\x96\xb0\xe5\x8f\x82\xe6\x95\xb0\n            p.requires_grad = False\n    #Sequential\xe5\xae\xb9\xe5\x99\xa8\xef\xbc\x8c\xe6\xa8\xa1\xe5\x9d\x97\xe5\xb0\x86\xe6\x8c\x89\xe7\x85\xa7\xe4\xbc\xa0\xe9\x80\x92\xe7\x9a\x84\xe9\xa1\xba\xe5\xba\x8f\xe6\xb7\xbb\xe5\x8a\xa0\xe5\x88\xb0\xe6\xa8\xa1\xe5\x9d\x97\xe4\xb8\xad\n    #feature:Variable,\xe5\xb7\xb2\xe5\x86\xbb\xe7\xbb\x93\xe5\x89\x8d\xe5\x9b\x9b\xe5\xb1\x82\xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82.\xe5\x80\xbc\xe4\xb8\xba\xe4\xbb\x8e\xe5\xbc\x80\xe5\xa7\x8b\xe5\x88\xb0conv5_3\n    #classifier\xef\xbc\x9avgg16\xe4\xb8\x89\xe5\xb1\x82\xe5\x88\x86\xe7\xb1\xbb\xe5\xb1\x82\xef\xbc\x8clist\xe6\x94\xbe\xe5\x85\xa5Sequential\xe4\xb8\xad\n    return nn.Sequential(*features), classifier\n\n\nclass FasterRCNNVGG16(FasterRCNN):\n\n    """"""Faster R-CNN based on VGG-16.\n     Faster R-CNN\xe5\x9f\xba\xe4\xba\x8eVGG-16\n    FasterRCNNVGG16\xe7\xbb\xa7\xe6\x89\xbf\xe4\xba\x86FasterRCNN\n    For descriptions on the interface of this model, please refer to\n    :class:`model.faster_rcnn.FasterRCNN`.\n\n    Args:\n        n_fg_class (int): The number of classes excluding the background.\n        ratios (list of floats): This is ratios of width to height of\n            the anchors.\n        anchor_scales (list of numbers): This is areas of anchors.\n            Those areas will be the product of the square of an element in\n            :obj:`anchor_scales` and the original area of the reference\n            window.\n\n    """"""\n    #\xe5\x9c\xa8vgg16\xe4\xb8\xad\xe4\xb8\x8b\xe9\x87\x87\xe6\xa0\xb716x\xe8\xbe\x93\xe5\x87\xbaconv5\xef\xbc\x88pooling\xe7\x9a\x84\xe5\x89\x8d\xe4\xb8\x80\xe5\xb1\x82\xef\xbc\x89\n    feat_stride = 16  # downsample 16x for output of conv5 in vgg16\n\n    def __init__(self,\n                 n_fg_class=20,\n                 ratios=[0.5, 1, 2],\n                 anchor_scales=[8, 16, 32]\n                 ):\n        #decom_vgg16\n        #extractor\xef\xbc\x9a\xe7\x89\xb9\xe5\xbe\x81\xe6\x8f\x90\xe5\x8f\x96\xe5\x99\xa8\xe3\x80\x82Variable,\xe5\xb7\xb2\xe5\x86\xbb\xe7\xbb\x93\xe5\x89\x8d\xe5\x9b\x9b\xe5\xb1\x82\xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82.\xe5\x80\xbc\xe4\xb8\xba\xe4\xbb\x8e\xe5\xbc\x80\xe5\xa7\x8b\xe5\x88\xb0conv5_3\n        #classifier\xef\xbc\x9a\xe5\x88\x86\xe7\xb1\xbb\xe5\x99\xa8\xe3\x80\x82vgg16\xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x80\xe5\xb1\x82\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\n        extractor, classifier = decom_vgg16()\n        #\xe6\x96\xb0\xe5\xbb\xba\xe4\xb8\x80\xe4\xb8\xaarpn \xe5\x8c\xba\xe5\x9f\x9f\xe6\x8f\x90\xe6\xa1\x88\xe7\xbd\x91\xe7\xbb\x9c\n        #512:in_channels\xe8\xbe\x93\xe5\x85\xa5\xe7\x9a\x84\xe9\x80\x9a\xe9\x81\x93\xe5\xa4\xa7\xe5\xb0\x8f\n        #512:mid_channels\xe4\xb8\xad\xe9\x97\xb4\xe5\xbc\xa0\xe9\x87\x8f\xe7\x9a\x84\xe9\x80\x9a\xe9\x81\x93\xe5\xa4\xa7\xe5\xb0\x8f\n        rpn = RegionProposalNetwork(\n            512, 512,\n            ratios=ratios,\n            anchor_scales=anchor_scales,\n            feat_stride=self.feat_stride,\n        )\n        #\xe6\x96\xb0\xe5\xbb\xba\xe4\xb8\x80\xe4\xb8\xaaroihead\n        #\xe6\x96\xb9\xe6\xb3\x95\xe5\x9c\xa8\xe6\x9c\xac\xe9\xa1\xb5\n        head = VGG16RoIHead(\n            n_class=n_fg_class + 1,\n            roi_size=7,\n            spatial_scale=(1. / self.feat_stride),\n            classifier=classifier\n        )\n       \n        super(FasterRCNNVGG16, self).__init__(\n            extractor,\n            rpn,\n            head,\n        )\n\n\nclass VGG16RoIHead(nn.Module):\n    """"""Faster R-CNN Head for VGG-16 based implementation.\n    This class is used as a head for Faster R-CNN.\n    This outputs class-wise localizations and classification based on feature\n    maps in the given RoIs.\n       Faster R-CNN Head\xe5\x9f\xba\xe4\xba\x8eVGG-16\xe7\x9a\x84\xe5\xae\x9e\xe7\x8e\xb0\n    \xc2\xa0\xc2\xa0\xe8\xbf\x99\xe4\xb8\xaaclass\xe8\xa2\xab\xe7\x94\xa8\xe4\xbd\x9chead for Faster R-CNN.\n       \xe6\xa0\xb9\xe6\x8d\xae\xe7\xbb\x99\xe5\xae\x9a\xe7\x9a\x84RoI\xe4\xb8\xad\xe7\x9a\x84\xe7\x89\xb9\xe5\xbe\x81  \xe6\x98\xa0\xe5\xb0\x84\xe8\xbe\x93\xe5\x87\xba \xe5\x88\x86\xe7\xb1\xbb\xe5\xae\x9a\xe4\xbd\x8d\xe5\x92\x8c\xe5\x88\x86\xe7\xb1\xbb\n    \n    Args:\n        n_class (int): The number of classes possibly including the background.\n                 \xe5\x8f\xaf\xe8\x83\xbd\xe5\x8c\x85\xe5\x90\xab\xe8\x83\x8c\xe6\x99\xaf\xe7\x9a\x84\xe7\xb1\xbb\xe7\x9a\x84\xe6\x95\xb0\xe9\x87\x8f\n        roi_size (int): Height and width of the feature maps after RoI-pooling.\n                  RoI-pooling\xe4\xb9\x8b\xe5\x90\x8e\xe7\x9a\x84 feature maps\xe7\x9a\x84\xe9\xab\x98\xe5\x92\x8c\xe5\xae\xbd\n        spatial_scale (float): Scale of the roi is resized.\n                  \xe8\xb0\x83\xe6\x95\xb4\xe5\x90\x8e\xe7\x9a\x84roi\xe7\x9a\x84\n        classifier (nn.Module): Two layer Linear ported from vgg16\n\n    """"""\n\n    def __init__(self, n_class, roi_size, spatial_scale,\n                 classifier):\n        # n_class includes the background\n        super(VGG16RoIHead, self).__init__()\n\n        self.classifier = classifier\n        self.cls_loc = nn.Linear(4096, n_class * 4)\n        self.score = nn.Linear(4096, n_class)\n\n        normal_init(self.cls_loc, 0, 0.001)\n        normal_init(self.score, 0, 0.01)\n\n        self.n_class = n_class\n        self.roi_size = roi_size\n        self.spatial_scale = spatial_scale\n        self.roi = RoIPooling2D(self.roi_size, self.roi_size, self.spatial_scale)\n\n    def forward(self, x, rois, roi_indices):\n        """"""Forward the chain.\n\n        We assume that there are :math:`N` batches.\n\n        Args:\n            x (Variable): 4D image variable.\n            rois (Tensor): A bounding box array containing coordinates of\n                proposal boxes.  This is a concatenation of bounding box\n                arrays from multiple images in the batch.\n                Its shape is :math:`(R\', 4)`. Given :math:`R_i` proposed\n                RoIs from the :math:`i` th image,\n                :math:`R\' = \\\\sum _{i=1} ^ N R_i`.\n            roi_indices (Tensor): An array containing indices of images to\n                which bounding boxes correspond to. Its shape is :math:`(R\',)`.\n\n        """"""\n        # in case roi_indices is  ndarray\n        roi_indices = at.totensor(roi_indices).float()\n        rois = at.totensor(rois).float()\n        indices_and_rois = t.cat([roi_indices[:, None], rois], dim=1)\n        # NOTE: important: yx->xy\n        xy_indices_and_rois = indices_and_rois[:, [0, 2, 1, 4, 3]]\n        indices_and_rois = t.autograd.Variable(xy_indices_and_rois.contiguous())\n\n        pool = self.roi(x, indices_and_rois)\n        pool = pool.view(pool.size(0), -1)\n        fc7 = self.classifier(pool)\n        roi_cls_locs = self.cls_loc(fc7)\n        roi_scores = self.score(fc7)\n        return roi_cls_locs, roi_scores\n\n\ndef normal_init(m, mean, stddev, truncated=False):\n    """"""\n    weight initalizer: truncated normal and random normal.\n    \xe6\x9d\x83\xe9\x87\x8d\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96  \xe6\x88\xaa\xe6\x96\xadnormal\xe5\x92\x8c\xe9\x9a\x8f\xe6\x9c\xbanormal\n    """"""\n    # x is a parameter\n    if truncated:\n        m.weight.data.normal_().fmod_(2).mul_(stddev).add_(mean)  # not a perfect approximation\n    else:\n        m.weight.data.normal_(mean, stddev)\n        m.bias.data.zero_()\n'"
FasterRcnn_pytorch/model/region_proposal_network.py,5,"b'import numpy as np\nfrom torch.nn import functional as F\nimport torch as t\nfrom torch import nn\n\nfrom model.utils.bbox_tools import generate_anchor_base\nfrom model.utils.creator_tool import ProposalCreator\n\n\nclass RegionProposalNetwork(nn.Module):\n    """"""Region Proposal Network introduced in Faster R-CNN.\n\n    This is Region Proposal Network introduced in Faster R-CNN [#]_.\n    This takes features extracted from images and propose\n    class agnostic bounding boxes around ""objects"".\n   \xe8\xbf\x99\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa\xe5\x8c\xba\xe5\x9f\x9f\xe6\x8f\x90\xe6\xa1\x88\xe7\xbd\x91\xe7\xbb\x9c\xe3\x80\x82\xe5\xae\x83\xe6\x8f\x90\xe5\x8f\x96\xe5\x9b\xbe\xe5\x83\x8f\xe7\x89\xb9\xe5\xbe\x81\xef\xbc\x8c\xe8\xbe\x93\xe5\x87\xba \xe5\x80\x99\xe9\x80\x89\xe6\xa1\x86bboxes\n\n    .. [#] Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun. \\\n    Faster R-CNN: Towards Real-Time Object Detection with \\\n    Region Proposal Networks. NIPS 2015.\n\n    Args:\n        in_channels (int): The channel size of input.\n        mid_channels (int): The channel size of the intermediate tensor.\n        ratios (list of floats): This is ratios of width to height of\n            the anchors.\n        anchor_scales (list of numbers): This is areas of anchors.\n            Those areas will be the product of the square of an element in\n            :obj:`anchor_scales` and the original area of the reference\n            window.\n        feat_stride (int): Stride size after extracting features from an\n            image.\n        initialW (callable): Initial weight value. If :obj:`None` then this\n            function uses Gaussian distribution scaled by 0.1 to\n            initialize weight.\n            May also be a callable that takes an array and edits its values.\n        proposal_creator_params (dict): Key valued paramters for\n            :class:`model.utils.creator_tools.ProposalCreator`.\n\n    .. seealso::\n        :class:`~model.utils.creator_tools.ProposalCreator`\n\n    """"""\n\n    def __init__(\n            self, in_channels=512, mid_channels=512, ratios=[0.5, 1, 2],\n            anchor_scales=[8, 16, 32], feat_stride=16,\n            proposal_creator_params=dict(),\n    ):\n        super(RegionProposalNetwork, self).__init__()\n        self.anchor_base = generate_anchor_base(\n            anchor_scales=anchor_scales, ratios=ratios)\n        self.feat_stride = feat_stride\n        self.proposal_layer = ProposalCreator(self, **proposal_creator_params)\n        n_anchor = self.anchor_base.shape[0]\n        self.conv1 = nn.Conv2d(in_channels, mid_channels, 3, 1, 1)\n        self.score = nn.Conv2d(mid_channels, n_anchor * 2, 1, 1, 0)\n        self.loc = nn.Conv2d(mid_channels, n_anchor * 4, 1, 1, 0)\n        normal_init(self.conv1, 0, 0.01)\n        normal_init(self.score, 0, 0.01)\n        normal_init(self.loc, 0, 0.01)\n\n    def forward(self, x, img_size, scale=1.):\n        """"""Forward Region Proposal Network.\n\n        Here are notations.\n\n        * :math:`N` is batch size.\n        * :math:`C` channel size of the input.\n        * :math:`H` and :math:`W` are height and witdh of the input feature.\n        * :math:`A` is number of anchors assigned to each pixel.\n\n        Args:\n            x (~torch.autograd.Variable): The Features extracted from images.\n                Its shape is :math:`(N, C, H, W)`.\n            img_size (tuple of ints): A tuple :obj:`height, width`,\n                which contains image size after scaling.\n            scale (float): The amount of scaling done to the input images after\n                reading them from files.\n\n        Returns:\n            (~torch.autograd.Variable, ~torch.autograd.Variable, array, array, array):\n\n            This is a tuple of five following values.\n\n            * **rpn_locs**: Predicted bounding box offsets and scales for \\\n                anchors. Its shape is :math:`(N, H W A, 4)`.\n            * **rpn_scores**:  Predicted foreground scores for \\\n                anchors. Its shape is :math:`(N, H W A, 2)`.\n            * **rois**: A bounding box array containing coordinates of \\\n                proposal boxes.  This is a concatenation of bounding box \\\n                arrays from multiple images in the batch. \\\n                Its shape is :math:`(R\', 4)`. Given :math:`R_i` predicted \\\n                bounding boxes from the :math:`i` th image, \\\n                :math:`R\' = \\\\sum _{i=1} ^ N R_i`.\n            * **roi_indices**: An array containing indices of images to \\\n                which RoIs correspond to. Its shape is :math:`(R\',)`.\n            * **anchor**: Coordinates of enumerated shifted anchors. \\\n                Its shape is :math:`(H W A, 4)`.\n\n        """"""\n        n, _, hh, ww = x.shape\n        anchor = _enumerate_shifted_anchor(\n            np.array(self.anchor_base),\n            self.feat_stride, hh, ww)\n\n        n_anchor = anchor.shape[0] // (hh * ww)\n        h = F.relu(self.conv1(x))\n\n        rpn_locs = self.loc(h)\n        # UNNOTE: check whether need contiguous\n        # A: Yes\n        rpn_locs = rpn_locs.permute(0, 2, 3, 1).contiguous().view(n, -1, 4)\n        rpn_scores = self.score(h)\n        rpn_scores = rpn_scores.permute(0, 2, 3, 1).contiguous()\n        rpn_fg_scores = \\\n            rpn_scores.view(n, hh, ww, n_anchor, 2)[:, :, :, :, 1].contiguous()\n        rpn_fg_scores = rpn_fg_scores.view(n, -1)\n        rpn_scores = rpn_scores.view(n, -1, 2)\n\n        rois = list()\n        roi_indices = list()\n        for i in range(n):\n            roi = self.proposal_layer(\n                rpn_locs[i].cpu().data.numpy(),\n                rpn_fg_scores[i].cpu().data.numpy(),\n                anchor, img_size,\n                scale=scale)\n            batch_index = i * np.ones((len(roi),), dtype=np.int32)\n            rois.append(roi)\n            roi_indices.append(batch_index)\n\n        rois = np.concatenate(rois, axis=0)\n        roi_indices = np.concatenate(roi_indices, axis=0)\n        return rpn_locs, rpn_scores, rois, roi_indices, anchor\n\n\ndef _enumerate_shifted_anchor(anchor_base, feat_stride, height, width):\n    # Enumerate all shifted anchors:\n    #\n    # add A anchors (1, A, 4) to\n    # cell K shifts (K, 1, 4) to get\n    # shift anchors (K, A, 4)\n    # reshape to (K*A, 4) shifted anchors\n    # return (K*A, 4)\n\n    # !TODO: add support for torch.CudaTensor\n    # xp = cuda.get_array_module(anchor_base)\n    # it seems that it can\'t be boosed using GPU\n    import numpy as xp\n    shift_y = xp.arange(0, height * feat_stride, feat_stride)\n    shift_x = xp.arange(0, width * feat_stride, feat_stride)\n    shift_x, shift_y = xp.meshgrid(shift_x, shift_y)\n    shift = xp.stack((shift_y.ravel(), shift_x.ravel(),\n                      shift_y.ravel(), shift_x.ravel()), axis=1)\n\n    A = anchor_base.shape[0]\n    K = shift.shape[0]\n    anchor = anchor_base.reshape((1, A, 4)) + \\\n             shift.reshape((1, K, 4)).transpose((1, 0, 2))\n    anchor = anchor.reshape((K * A, 4)).astype(np.float32)\n    return anchor\n\n\ndef _enumerate_shifted_anchor_torch(anchor_base, feat_stride, height, width):\n    # Enumerate all shifted anchors:\n    #\n    # add A anchors (1, A, 4) to\n    # cell K shifts (K, 1, 4) to get\n    # shift anchors (K, A, 4)\n    # reshape to (K*A, 4) shifted anchors\n    # return (K*A, 4)\n\n    # !TODO: add support for torch.CudaTensor\n    # xp = cuda.get_array_module(anchor_base)\n    import torch as t\n    shift_y = t.arange(0, height * feat_stride, feat_stride)\n    shift_x = t.arange(0, width * feat_stride, feat_stride)\n    shift_x, shift_y = xp.meshgrid(shift_x, shift_y)\n    shift = xp.stack((shift_y.ravel(), shift_x.ravel(),\n                      shift_y.ravel(), shift_x.ravel()), axis=1)\n\n    A = anchor_base.shape[0]\n    K = shift.shape[0]\n    anchor = anchor_base.reshape((1, A, 4)) + \\\n             shift.reshape((1, K, 4)).transpose((1, 0, 2))\n    anchor = anchor.reshape((K * A, 4)).astype(np.float32)\n    return anchor\n\n\ndef normal_init(m, mean, stddev, truncated=False):\n    """"""\n    weight initalizer: truncated normal and random normal.\n    """"""\n    # x is a parameter\n    if truncated:\n        m.weight.data.normal_().fmod_(2).mul_(stddev).add_(mean)  # not a perfect approximation\n    else:\n        m.weight.data.normal_(mean, stddev)\n        m.bias.data.zero_()\n'"
FasterRcnn_pytorch/model/roi_module.py,3,"b'from collections import namedtuple\nfrom string import Template\n\nimport cupy, torch\nimport cupy as cp\nimport torch as t\nfrom torch.autograd import Function\n\nfrom model.utils.roi_cupy import kernel_backward, kernel_forward\n\nStream = namedtuple(\'Stream\', [\'ptr\'])\n\n\n@cupy.util.memoize(for_each_device=True)\ndef load_kernel(kernel_name, code, **kwargs):\n    cp.cuda.runtime.free(0)\n    code = Template(code).substitute(**kwargs)\n    kernel_code = cupy.cuda.compile_with_cache(code)\n    return kernel_code.get_function(kernel_name)\n\n\nCUDA_NUM_THREADS = 1024\n\n\ndef GET_BLOCKS(N, K=CUDA_NUM_THREADS):\n    return (N + K - 1) // K\n\n\nclass RoI(Function):\n    """"""\n    NOTE\xef\xbc\x9aonly CUDA-compatible\n    """"""\n\n    def __init__(self, outh, outw, spatial_scale):\n        self.forward_fn = load_kernel(\'roi_forward\', kernel_forward)\n        self.backward_fn = load_kernel(\'roi_backward\', kernel_backward)\n        self.outh, self.outw, self.spatial_scale = outh, outw, spatial_scale\n\n    def forward(self, x, rois):\n        # NOTE: MAKE SURE input is contiguous too\n        x = x.contiguous()\n        rois = rois.contiguous()\n        self.in_size = B, C, H, W = x.size()\n        self.N = N = rois.size(0)\n        output = t.zeros(N, C, self.outh, self.outw).cuda()\n        self.argmax_data = t.zeros(N, C, self.outh, self.outw).int().cuda()\n        self.rois = rois\n        args = [x.data_ptr(), rois.data_ptr(),\n                output.data_ptr(),\n                self.argmax_data.data_ptr(),\n                self.spatial_scale, C, H, W,\n                self.outh, self.outw,\n                output.numel()]\n        stream = Stream(ptr=torch.cuda.current_stream().cuda_stream)\n        self.forward_fn(args=args,\n                        block=(CUDA_NUM_THREADS, 1, 1),\n                        grid=(GET_BLOCKS(output.numel()), 1, 1),\n                        stream=stream)\n        return output\n\n    def backward(self, grad_output):\n        ##NOTE: IMPORTANT CONTIGUOUS\n        # TODO: input\n        grad_output = grad_output.contiguous()\n        B, C, H, W = self.in_size\n        grad_input = t.zeros(self.in_size).cuda()\n        stream = Stream(ptr=torch.cuda.current_stream().cuda_stream)\n        args = [grad_output.data_ptr(),\n                self.argmax_data.data_ptr(),\n                self.rois.data_ptr(),\n                grad_input.data_ptr(),\n                self.N, self.spatial_scale, C, H, W, self.outh, self.outw,\n                grad_input.numel()]\n        self.backward_fn(args=args,\n                         block=(CUDA_NUM_THREADS, 1, 1),\n                         grid=(GET_BLOCKS(grad_input.numel()), 1, 1),\n                         stream=stream\n                         )\n        return grad_input, None\n\n\nclass RoIPooling2D(t.nn.Module):\n\n    def __init__(self, outh, outw, spatial_scale):\n        super(RoIPooling2D, self).__init__()\n        self.RoI = RoI(outh, outw, spatial_scale)\n\n    def forward(self, x, rois):\n        return self.RoI(x, rois)\n\n\ndef test_roi_module():\n    ## fake data###\n    B, N, C, H, W, PH, PW = 2, 8, 4, 32, 32, 7, 7\n\n    bottom_data = t.randn(B, C, H, W).cuda()\n    bottom_rois = t.randn(N, 5)\n    bottom_rois[:int(N / 2), 0] = 0\n    bottom_rois[int(N / 2):, 0] = 1\n    bottom_rois[:, 1:] = (t.rand(N, 4) * 100).float()\n    bottom_rois = bottom_rois.cuda()\n    spatial_scale = 1. / 16\n    outh, outw = PH, PW\n\n    # pytorch version\n    module = RoIPooling2D(outh, outw, spatial_scale)\n    x = t.autograd.Variable(bottom_data, requires_grad=True)\n    rois = t.autograd.Variable(bottom_rois)\n    output = module(x, rois)\n    output.sum().backward()\n\n    def t2c(variable):\n        npa = variable.data.cpu().numpy()\n        return cp.array(npa)\n\n    def test_eq(variable, array, info):\n        cc = cp.asnumpy(array)\n        neq = (cc != variable.data.cpu().numpy())\n        assert neq.sum() == 0, \'test failed: %s\' % info\n\n    # chainer version,if you\'re going to run this\n    # pip install chainer \n    import chainer.functions as F\n    from chainer import Variable\n    x_cn = Variable(t2c(x))\n\n    o_cn = F.roi_pooling_2d(x_cn, t2c(rois), outh, outw, spatial_scale)\n    test_eq(output, o_cn.array, \'forward\')\n    F.sum(o_cn).backward()\n    test_eq(x.grad, x_cn.grad, \'backward\')\n    print(\'test pass\')\n'"
FasterRcnn_pytorch/utils/__init__.py,0,"b'#    Copyright 2017 cy\n# \n#    Licensed under the Apache License, Version 2.0 (the ""License"");\n#    you may not use this file except in compliance with the License.\n#    You may obtain a copy of the License at\n# \n#        http://www.apache.org/licenses/LICENSE-2.0\n# \n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an ""AS IS"" BASIS,\n#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#    See the License for the specific language governing permissions and\n#    limitations under the License.\n\n'"
FasterRcnn_pytorch/utils/array_tool.py,0,"b'""""""\ntools to convert specified type\n""""""\nimport torch as t\nimport numpy as np\n\n\ndef tonumpy(data):\n    if isinstance(data, np.ndarray):\n        return data\n    if isinstance(data, t._TensorBase):\n        return data.cpu().numpy()\n    if isinstance(data, t.autograd.Variable):\n        return tonumpy(data.data)\n\n\ndef totensor(data, cuda=True):\n    if isinstance(data, np.ndarray):\n        tensor = t.from_numpy(data)\n    if isinstance(data, t._TensorBase):\n        tensor = data\n    if isinstance(data, t.autograd.Variable):\n        tensor = data.data\n    if cuda:\n        tensor = tensor.cuda()\n    return tensor\n\n\ndef tovariable(data):\n    if isinstance(data, np.ndarray):\n        return tovariable(totensor(data))\n    if isinstance(data, t._TensorBase):\n        return t.autograd.Variable(data)\n    if isinstance(data, t.autograd.Variable):\n        return data\n    else:\n        raise ValueError(""UnKnow data type: %s, input should be {np.ndarray,Tensor,Variable}"" %type(data))\n\n\ndef scalar(data):\n    if isinstance(data, np.ndarray):\n        return data.reshape(1)[0]\n    if isinstance(data, t._TensorBase):\n        return data.view(1)[0]\n    if isinstance(data, t.autograd.Variable):\n        return data.data.view(1)[0]\n'"
FasterRcnn_pytorch/utils/config.py,0,"b'from pprint import pprint\n\n\n# Default Configs for training\n# NOTE that, config items could be overwriten by passing argument through command line.\n# e.g. --voc-data-dir=\'./data/\'\n\nclass Config:\n    # data\n    #\xe6\x9b\xb4\xe6\x94\xb9_bobo\n    voc_data_dir = \'/home/bobo/PycharmProjects/torchProjectss/fasterbychenyun/VOCdevkit/Pascal VOC2007/VOCdevkit/VOC2007\'\n    min_size = 600  # image resize\n    max_size = 1000 # image resize\n    num_workers = 8\n    test_num_workers = 8\n\n    # sigma for l1_smooth_loss\n    rpn_sigma = 3.\n    roi_sigma = 1.\n\n    # param for optimizer\n    # 0.0005 in origin paper but 0.0001 in tf-faster-rcnn\n    weight_decay = 0.0005\n    lr_decay = 0.1  # 1e-3 -> 1e-4\n    lr = 1e-3\n\n\n    # visualization\n    env = \'faster-rcnn\'  # visdom env\n    port = 8097\n    plot_every = 40  # vis every N iter\n\n    # preset\n    data = \'voc\'\n    pretrained_model = \'vgg16\'\n\n    # training\n    epoch = 14\n\n\n    use_adam = False # Use Adam optimizer\n    use_chainer = False # try match everything as chainer\n    use_drop = False # use dropout in RoIHead\n    # debug\n    debug_file = \'/tmp/debugf\'\n\n    test_num = 10000\n    # model\n    load_path = None\n\n    #caffe_pretrain = False # use caffe pretrained model instead of torchvision\n    caffe_pretrain = True # use caffe pretrained model instead of torchvision\n    caffe_pretrain_path = \'/home/bobo/PycharmProjects/torchProjectss/fasterbychenyun/simplefasterrcnnpytorchmaster/checkpoints/vgg16_caffe.pth\'\n    \n\n    def _parse(self, kwargs):\n        state_dict = self._state_dict()\n        for k, v in kwargs.items():\n            if k not in state_dict:\n                raise ValueError(\'UnKnown Option: ""--%s""\' % k)\n            setattr(self, k, v)\n\n        print(\'======user config========\')\n        pprint(self._state_dict())\n        print(\'==========end============\')\n\n    def _state_dict(self):\n        return {k: getattr(self, k) for k, _ in Config.__dict__.items() \\\n                if not k.startswith(\'_\')}\n\n\nopt = Config()\n'"
FasterRcnn_pytorch/utils/eval_tool.py,0,"b'from __future__ import division\n\nfrom collections import defaultdict\nimport itertools\nimport numpy as np\nimport six\n\nfrom model.utils.bbox_tools import bbox_iou\n\n\ndef eval_detection_voc(\n        pred_bboxes, pred_labels, pred_scores, gt_bboxes, gt_labels,\n        gt_difficults=None,\n        iou_thresh=0.5, use_07_metric=False):\n    """"""Calculate average precisions based on evaluation code of PASCAL VOC.\n\n    This function evaluates predicted bounding boxes obtained from a dataset\n    which has :math:`N` images by using average precision for each class.\n    The code is based on the evaluation code used in PASCAL VOC Challenge.\n\n    Args:\n        pred_bboxes (iterable of numpy.ndarray): An iterable of :math:`N`\n            sets of bounding boxes.\n            Its index corresponds to an index for the base dataset.\n            Each element of :obj:`pred_bboxes` is a set of coordinates\n            of bounding boxes. This is an array whose shape is :math:`(R, 4)`,\n            where :math:`R` corresponds\n            to the number of bounding boxes, which may vary among boxes.\n            The second axis corresponds to\n            :math:`y_{min}, x_{min}, y_{max}, x_{max}` of a bounding box.\n        pred_labels (iterable of numpy.ndarray): An iterable of labels.\n            Similar to :obj:`pred_bboxes`, its index corresponds to an\n            index for the base dataset. Its length is :math:`N`.\n        pred_scores (iterable of numpy.ndarray): An iterable of confidence\n            scores for predicted bounding boxes. Similar to :obj:`pred_bboxes`,\n            its index corresponds to an index for the base dataset.\n            Its length is :math:`N`.\n        gt_bboxes (iterable of numpy.ndarray): An iterable of ground truth\n            bounding boxes\n            whose length is :math:`N`. An element of :obj:`gt_bboxes` is a\n            bounding box whose shape is :math:`(R, 4)`. Note that the number of\n            bounding boxes in each image does not need to be same as the number\n            of corresponding predicted boxes.\n        gt_labels (iterable of numpy.ndarray): An iterable of ground truth\n            labels which are organized similarly to :obj:`gt_bboxes`.\n        gt_difficults (iterable of numpy.ndarray): An iterable of boolean\n            arrays which is organized similarly to :obj:`gt_bboxes`.\n            This tells whether the\n            corresponding ground truth bounding box is difficult or not.\n            By default, this is :obj:`None`. In that case, this function\n            considers all bounding boxes to be not difficult.\n        iou_thresh (float): A prediction is correct if its Intersection over\n            Union with the ground truth is above this value.\n        use_07_metric (bool): Whether to use PASCAL VOC 2007 evaluation metric\n            for calculating average precision. The default value is\n            :obj:`False`.\n\n    Returns:\n        dict:\n\n        The keys, value-types and the description of the values are listed\n        below.\n\n        * **ap** (*numpy.ndarray*): An array of average precisions. \\\n            The :math:`l`-th value corresponds to the average precision \\\n            for class :math:`l`. If class :math:`l` does not exist in \\\n            either :obj:`pred_labels` or :obj:`gt_labels`, the corresponding \\\n            value is set to :obj:`numpy.nan`.\n        * **map** (*float*): The average of Average Precisions over classes.\n\n    """"""\n\n    prec, rec = calc_detection_voc_prec_rec(\n        pred_bboxes, pred_labels, pred_scores,\n        gt_bboxes, gt_labels, gt_difficults,\n        iou_thresh=iou_thresh)\n\n    ap = calc_detection_voc_ap(prec, rec, use_07_metric=use_07_metric)\n\n    return {\'ap\': ap, \'map\': np.nanmean(ap)}\n\n\ndef calc_detection_voc_prec_rec(\n        pred_bboxes, pred_labels, pred_scores, gt_bboxes, gt_labels,\n        gt_difficults=None,\n        iou_thresh=0.5):\n    """"""Calculate precision and recall based on evaluation code of PASCAL VOC.\n\n    This function calculates precision and recall of\n    predicted bounding boxes obtained from a dataset which has :math:`N`\n    images.\n    The code is based on the evaluation code used in PASCAL VOC Challenge.\n\n    Args:\n        pred_bboxes (iterable of numpy.ndarray): An iterable of :math:`N`\n            sets of bounding boxes.\n            Its index corresponds to an index for the base dataset.\n            Each element of :obj:`pred_bboxes` is a set of coordinates\n            of bounding boxes. This is an array whose shape is :math:`(R, 4)`,\n            where :math:`R` corresponds\n            to the number of bounding boxes, which may vary among boxes.\n            The second axis corresponds to\n            :math:`y_{min}, x_{min}, y_{max}, x_{max}` of a bounding box.\n        pred_labels (iterable of numpy.ndarray): An iterable of labels.\n            Similar to :obj:`pred_bboxes`, its index corresponds to an\n            index for the base dataset. Its length is :math:`N`.\n        pred_scores (iterable of numpy.ndarray): An iterable of confidence\n            scores for predicted bounding boxes. Similar to :obj:`pred_bboxes`,\n            its index corresponds to an index for the base dataset.\n            Its length is :math:`N`.\n        gt_bboxes (iterable of numpy.ndarray): An iterable of ground truth\n            bounding boxes\n            whose length is :math:`N`. An element of :obj:`gt_bboxes` is a\n            bounding box whose shape is :math:`(R, 4)`. Note that the number of\n            bounding boxes in each image does not need to be same as the number\n            of corresponding predicted boxes.\n        gt_labels (iterable of numpy.ndarray): An iterable of ground truth\n            labels which are organized similarly to :obj:`gt_bboxes`.\n        gt_difficults (iterable of numpy.ndarray): An iterable of boolean\n            arrays which is organized similarly to :obj:`gt_bboxes`.\n            This tells whether the\n            corresponding ground truth bounding box is difficult or not.\n            By default, this is :obj:`None`. In that case, this function\n            considers all bounding boxes to be not difficult.\n        iou_thresh (float): A prediction is correct if its Intersection over\n            Union with the ground truth is above this value..\n\n    Returns:\n        tuple of two lists:\n        This function returns two lists: :obj:`prec` and :obj:`rec`.\n\n        * :obj:`prec`: A list of arrays. :obj:`prec[l]` is precision \\\n            for class :math:`l`. If class :math:`l` does not exist in \\\n            either :obj:`pred_labels` or :obj:`gt_labels`, :obj:`prec[l]` is \\\n            set to :obj:`None`.\n        * :obj:`rec`: A list of arrays. :obj:`rec[l]` is recall \\\n            for class :math:`l`. If class :math:`l` that is not marked as \\\n            difficult does not exist in \\\n            :obj:`gt_labels`, :obj:`rec[l]` is \\\n            set to :obj:`None`.\n\n    """"""\n\n    pred_bboxes = iter(pred_bboxes)\n    pred_labels = iter(pred_labels)\n    pred_scores = iter(pred_scores)\n    gt_bboxes = iter(gt_bboxes)\n    gt_labels = iter(gt_labels)\n    if gt_difficults is None:\n        gt_difficults = itertools.repeat(None)\n    else:\n        gt_difficults = iter(gt_difficults)\n\n    n_pos = defaultdict(int)\n    score = defaultdict(list)\n    match = defaultdict(list)\n\n    for pred_bbox, pred_label, pred_score, gt_bbox, gt_label, gt_difficult in \\\n            six.moves.zip(\n                pred_bboxes, pred_labels, pred_scores,\n                gt_bboxes, gt_labels, gt_difficults):\n\n        if gt_difficult is None:\n            gt_difficult = np.zeros(gt_bbox.shape[0], dtype=bool)\n\n        for l in np.unique(np.concatenate((pred_label, gt_label)).astype(int)):\n            pred_mask_l = pred_label == l\n            pred_bbox_l = pred_bbox[pred_mask_l]\n            pred_score_l = pred_score[pred_mask_l]\n            # sort by score\n            order = pred_score_l.argsort()[::-1]\n            pred_bbox_l = pred_bbox_l[order]\n            pred_score_l = pred_score_l[order]\n\n            gt_mask_l = gt_label == l\n            gt_bbox_l = gt_bbox[gt_mask_l]\n            gt_difficult_l = gt_difficult[gt_mask_l]\n\n            n_pos[l] += np.logical_not(gt_difficult_l).sum()\n            score[l].extend(pred_score_l)\n\n            if len(pred_bbox_l) == 0:\n                continue\n            if len(gt_bbox_l) == 0:\n                match[l].extend((0,) * pred_bbox_l.shape[0])\n                continue\n\n            # VOC evaluation follows integer typed bounding boxes.\n            pred_bbox_l = pred_bbox_l.copy()\n            pred_bbox_l[:, 2:] += 1\n            gt_bbox_l = gt_bbox_l.copy()\n            gt_bbox_l[:, 2:] += 1\n\n            iou = bbox_iou(pred_bbox_l, gt_bbox_l)\n            gt_index = iou.argmax(axis=1)\n            # set -1 if there is no matching ground truth\n            gt_index[iou.max(axis=1) < iou_thresh] = -1\n            del iou\n\n            selec = np.zeros(gt_bbox_l.shape[0], dtype=bool)\n            for gt_idx in gt_index:\n                if gt_idx >= 0:\n                    if gt_difficult_l[gt_idx]:\n                        match[l].append(-1)\n                    else:\n                        if not selec[gt_idx]:\n                            match[l].append(1)\n                        else:\n                            match[l].append(0)\n                    selec[gt_idx] = True\n                else:\n                    match[l].append(0)\n\n    for iter_ in (\n            pred_bboxes, pred_labels, pred_scores,\n            gt_bboxes, gt_labels, gt_difficults):\n        if next(iter_, None) is not None:\n            raise ValueError(\'Length of input iterables need to be same.\')\n\n    n_fg_class = max(n_pos.keys()) + 1\n    prec = [None] * n_fg_class\n    rec = [None] * n_fg_class\n\n    for l in n_pos.keys():\n        score_l = np.array(score[l])\n        match_l = np.array(match[l], dtype=np.int8)\n\n        order = score_l.argsort()[::-1]\n        match_l = match_l[order]\n\n        tp = np.cumsum(match_l == 1)\n        fp = np.cumsum(match_l == 0)\n\n        # If an element of fp + tp is 0,\n        # the corresponding element of prec[l] is nan.\n        prec[l] = tp / (fp + tp)\n        # If n_pos[l] is 0, rec[l] is None.\n        if n_pos[l] > 0:\n            rec[l] = tp / n_pos[l]\n\n    return prec, rec\n\n\ndef calc_detection_voc_ap(prec, rec, use_07_metric=False):\n    """"""Calculate average precisions based on evaluation code of PASCAL VOC.\n\n    This function calculates average precisions\n    from given precisions and recalls.\n    The code is based on the evaluation code used in PASCAL VOC Challenge.\n\n    Args:\n        prec (list of numpy.array): A list of arrays.\n            :obj:`prec[l]` indicates precision for class :math:`l`.\n            If :obj:`prec[l]` is :obj:`None`, this function returns\n            :obj:`numpy.nan` for class :math:`l`.\n        rec (list of numpy.array): A list of arrays.\n            :obj:`rec[l]` indicates recall for class :math:`l`.\n            If :obj:`rec[l]` is :obj:`None`, this function returns\n            :obj:`numpy.nan` for class :math:`l`.\n        use_07_metric (bool): Whether to use PASCAL VOC 2007 evaluation metric\n            for calculating average precision. The default value is\n            :obj:`False`.\n\n    Returns:\n        ~numpy.ndarray:\n        This function returns an array of average precisions.\n        The :math:`l`-th value corresponds to the average precision\n        for class :math:`l`. If :obj:`prec[l]` or :obj:`rec[l]` is\n        :obj:`None`, the corresponding value is set to :obj:`numpy.nan`.\n\n    """"""\n\n    n_fg_class = len(prec)\n    ap = np.empty(n_fg_class)\n    for l in six.moves.range(n_fg_class):\n        if prec[l] is None or rec[l] is None:\n            ap[l] = np.nan\n            continue\n\n        if use_07_metric:\n            # 11 point metric\n            ap[l] = 0\n            for t in np.arange(0., 1.1, 0.1):\n                if np.sum(rec[l] >= t) == 0:\n                    p = 0\n                else:\n                    p = np.max(np.nan_to_num(prec[l])[rec[l] >= t])\n                ap[l] += p / 11\n        else:\n            # correct AP calculation\n            # first append sentinel values at the end\n            mpre = np.concatenate(([0], np.nan_to_num(prec[l]), [0]))\n            mrec = np.concatenate(([0], rec[l], [1]))\n\n            mpre = np.maximum.accumulate(mpre[::-1])[::-1]\n\n            # to calculate area under PR curve, look for points\n            # where X axis (recall) changes value\n            i = np.where(mrec[1:] != mrec[:-1])[0]\n\n            # and sum (\\Delta recall) * prec\n            ap[l] = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n\n    return ap\n'"
FasterRcnn_pytorch/utils/vis_tool.py,0,"b'import time\n\nimport numpy as np\nimport matplotlib\nimport torch as t\nimport visdom\n\nmatplotlib.use(\'Agg\')\nfrom matplotlib import pyplot as plot\n\n# from data.voc_dataset import VOC_BBOX_LABEL_NAMES\n\n\nVOC_BBOX_LABEL_NAMES = (\n    \'fly\',\n    \'bike\',\n    \'bird\',\n    \'boat\',\n    \'pin\',\n    \'bus\',\n    \'c\',\n    \'cat\',\n    \'chair\',\n    \'cow\',\n    \'table\',\n    \'dog\',\n    \'horse\',\n    \'moto\',\n    \'p\',\n    \'plant\',\n    \'shep\',\n    \'sofa\',\n    \'train\',\n    \'tv\',\n)\n\n\ndef vis_image(img, ax=None):\n    """"""Visualize a color image.\n\n    Args:\n        img (~numpy.ndarray): An array of shape :math:`(3, height, width)`.\n            This is in RGB format and the range of its value is\n            :math:`[0, 255]`.\n        ax (matplotlib.axes.Axis): The visualization is displayed on this\n            axis. If this is :obj:`None` (default), a new axis is created.\n\n    Returns:\n        ~matploblib.axes.Axes:\n        Returns the Axes object with the plot for further tweaking.\n\n    """"""\n\n    if ax is None:\n        fig = plot.figure()\n        ax = fig.add_subplot(1, 1, 1)\n    # CHW -> HWC\n    img = img.transpose((1, 2, 0))\n    ax.imshow(img.astype(np.uint8))\n    return ax\n\n\ndef vis_bbox(img, bbox, label=None, score=None, ax=None):\n    """"""Visualize bounding boxes inside image.\n\n    Args:\n        img (~numpy.ndarray): An array of shape :math:`(3, height, width)`.\n            This is in RGB format and the range of its value is\n            :math:`[0, 255]`.\n        bbox (~numpy.ndarray): An array of shape :math:`(R, 4)`, where\n            :math:`R` is the number of bounding boxes in the image.\n            Each element is organized\n            by :math:`(y_{min}, x_{min}, y_{max}, x_{max})` in the second axis.\n        label (~numpy.ndarray): An integer array of shape :math:`(R,)`.\n            The values correspond to id for label names stored in\n            :obj:`label_names`. This is optional.\n        score (~numpy.ndarray): A float array of shape :math:`(R,)`.\n             Each value indicates how confident the prediction is.\n             This is optional.\n        label_names (iterable of strings): Name of labels ordered according\n            to label ids. If this is :obj:`None`, labels will be skipped.\n        ax (matplotlib.axes.Axis): The visualization is displayed on this\n            axis. If this is :obj:`None` (default), a new axis is created.\n\n    Returns:\n        ~matploblib.axes.Axes:\n        Returns the Axes object with the plot for further tweaking.\n\n    """"""\n\n    label_names = list(VOC_BBOX_LABEL_NAMES) + [\'bg\']\n    # add for index `-1`\n    if label is not None and not len(bbox) == len(label):\n        raise ValueError(\'The length of label must be same as that of bbox\')\n    if score is not None and not len(bbox) == len(score):\n        raise ValueError(\'The length of score must be same as that of bbox\')\n\n    # Returns newly instantiated matplotlib.axes.Axes object if ax is None\n    ax = vis_image(img, ax=ax)\n\n    # If there is no bounding box to display, visualize the image and exit.\n    if len(bbox) == 0:\n        return ax\n\n    for i, bb in enumerate(bbox):\n        xy = (bb[1], bb[0])\n        height = bb[2] - bb[0]\n        width = bb[3] - bb[1]\n        ax.add_patch(plot.Rectangle(\n            xy, width, height, fill=False, edgecolor=\'red\', linewidth=2))\n\n        caption = list()\n\n        if label is not None and label_names is not None:\n            lb = label[i]\n            if not (-1 <= lb < len(label_names)):  # modfy here to add backgroud\n                raise ValueError(\'No corresponding name is given\')\n            caption.append(label_names[lb])\n        if score is not None:\n            sc = score[i]\n            caption.append(\'{:.2f}\'.format(sc))\n\n        if len(caption) > 0:\n            ax.text(bb[1], bb[0],\n                    \': \'.join(caption),\n                    style=\'italic\',\n                    bbox={\'facecolor\': \'white\', \'alpha\': 0.5, \'pad\': 0})\n    return ax\n\n\ndef fig2data(fig):\n    """"""\n    brief Convert a Matplotlib figure to a 4D numpy array with RGBA \n    channels and return it\n\n    @param fig\xef\xbc\x9a a matplotlib figure\n    @return a numpy 3D array of RGBA values\n    """"""\n    # draw the renderer\n    fig.canvas.draw()\n\n    # Get the RGBA buffer from the figure\n    w, h = fig.canvas.get_width_height()\n    buf = np.fromstring(fig.canvas.tostring_argb(), dtype=np.uint8)\n    buf.shape = (w, h, 4)\n\n    # canvas.tostring_argb give pixmap in ARGB mode. Roll the ALPHA channel to have it in RGBA mode\n    buf = np.roll(buf, 3, axis=2)\n    return buf.reshape(h, w, 4)\n\n\ndef fig4vis(fig):\n    """"""\n    convert figure to ndarray\n    """"""\n    ax = fig.get_figure()\n    img_data = fig2data(ax).astype(np.int32)\n    plot.close()\n    # HWC->CHW\n    return img_data[:, :, :3].transpose((2, 0, 1)) / 255.\n\n\ndef visdom_bbox(*args, **kwargs):\n    fig = vis_bbox(*args, **kwargs)\n    data = fig4vis(fig)\n    return data\n\n\nclass Visualizer(object):\n    """"""\n    wrapper for visdom\n    you can still access naive visdom function by \n    self.line, self.scater,self._send,etc.\n    due to the implementation of `__getattr__`\n    """"""\n\n    def __init__(self, env=\'default\', **kwargs):\n        self.vis = visdom.Visdom(env=env, **kwargs)\n        self._vis_kw = kwargs\n\n        # e.g.\xef\xbc\x88\xe2\x80\x99loss\',23\xef\xbc\x89 the 23th value of loss\n        self.index = {}\n        self.log_text = \'\'\n\n    def reinit(self, env=\'default\', **kwargs):\n        """"""\n        change the config of visdom\n        """"""\n        self.vis = visdom.Visdom(env=env, **kwargs)\n        return self\n\n    def plot_many(self, d):\n        """"""\n        plot multi values\n        @params d: dict (name,value) i.e. (\'loss\',0.11)\n        """"""\n        for k, v in d.items():\n            if v is not None:\n                self.plot(k, v)\n\n    def img_many(self, d):\n        for k, v in d.items():\n            self.img(k, v)\n\n    def plot(self, name, y, **kwargs):\n        """"""\n        self.plot(\'loss\',1.00)\n        """"""\n        x = self.index.get(name, 0)\n        self.vis.line(Y=np.array([y]), X=np.array([x]),\n                      win=name,\n                      opts=dict(title=name),\n                      update=None if x == 0 else \'append\',\n                      **kwargs\n                      )\n        self.index[name] = x + 1\n\n    def img(self, name, img_, **kwargs):\n        """"""\n        self.img(\'input_img\',t.Tensor(64,64))\n        self.img(\'input_imgs\',t.Tensor(3,64,64))\n        self.img(\'input_imgs\',t.Tensor(100,1,64,64))\n        self.img(\'input_imgs\',t.Tensor(100,3,64,64),nrows=10)\n        \xef\xbc\x81\xef\xbc\x81\xef\xbc\x81don\xe2\x80\x98t ~~self.img(\'input_imgs\',t.Tensor(100,64,64),nrows=10)~~\xef\xbc\x81\xef\xbc\x81\xef\xbc\x81\n        """"""\n        self.vis.images(t.Tensor(img_).cpu().numpy(),\n                        win=name,\n                        opts=dict(title=name),\n                        **kwargs\n                        )\n\n    def log(self, info, win=\'log_text\'):\n        """"""\n        self.log({\'loss\':1,\'lr\':0.0001})\n        """"""\n        self.log_text += (\'[{time}] {info} <br>\'.format(\n            time=time.strftime(\'%m%d_%H%M%S\'), \\\n            info=info))\n        self.vis.text(self.log_text, win)\n\n    def __getattr__(self, name):\n        return getattr(self.vis, name)\n\n    def state_dict(self):\n        return {\n            \'index\': self.index,\n            \'vis_kw\': self._vis_kw,\n            \'log_text\': self.log_text,\n            \'env\': self.vis.env\n        }\n\n    def load_state_dict(self, d):\n        self.vis = visdom.Visdom(env=d.get(\'env\', self.vis.env), **(self.d.get(\'vis_kw\')))\n        self.log_text = d.get(\'log_text\', \'\')\n        self.index = d.get(\'index\', dict())\n        return self\n'"
ImageDenoising_pytorch/data/__init__.py,0,b''
ImageDenoising_pytorch/data/dataprocessing.py,1,"b""# -*- coding:utf-8 -*-\n# power by Mr.Li\nimport os\nfrom PIL import  Image\nfrom torch.utils import data\nfrom torchvision import  transforms as T\n# \xe5\xb0\x81\xe8\xa3\x85\xe8\x87\xaa\xe5\xae\x9a\xe4\xb9\x89\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\nclass DataProcessing(data.Dataset):\n    '''\n    \xe4\xb8\xbb\xe8\xa6\x81\xe7\x9b\xae\xe6\xa0\x87\xef\xbc\x9a \xe8\x8e\xb7\xe5\x8f\x96\xe6\x89\x80\xe6\x9c\x89\xe5\x9b\xbe\xe7\x89\x87\xe7\x9a\x84\xe5\x9c\xb0\xe5\x9d\x80\xef\xbc\x8c\xe5\xb9\xb6\xe6\xa0\xb9\xe6\x8d\xae\xe8\xae\xad\xe7\xbb\x83\xef\xbc\x8c\xe9\xaa\x8c\xe8\xaf\x81\xef\xbc\x8c\xe6\xb5\x8b\xe8\xaf\x95\xe5\x88\x92\xe5\x88\x86\xe6\x95\xb0\xe6\x8d\xae\n    '''\n    def __init__(self, root, transforms=None, train=True, test=False):\n        self.test = test\n        self.root=root\n        # \xe8\xaf\xbb\xe5\x8f\x96\xe4\xb8\xa4\xe4\xb8\xaa\xe6\x96\x87\xe4\xbb\xb6\xe5\xa4\xb9\xe4\xb8\x8b\xe6\x89\x80\xe6\x9c\x89\xe5\x9b\xbe\xe5\x83\x8f\n        imgs_origin = [os.path.join(root+'JPEGImages/', img) for img in os.listdir(root+'JPEGImages/')]\n        imgs_grayscale = [os.path.join(root + 'JPEGImages_bo/', img) for img in os.listdir(root + 'JPEGImages_bo/')]\n        # \xe6\x9c\xaa\xe5\x86\x99\xe6\xb5\x8b\xe8\xaf\x95\xe9\x9b\x86\n        if self.test:\n            #=====================================\n            print()\n        else:\n            imgs_origin= sorted(imgs_origin, key=lambda x: int(x.split('.')[-2].split('/')[-1]))\n            imgs_grayscale = sorted(imgs_grayscale, key=lambda x: int(x.split('.')[-2].split('/')[-1]))\n        \n        imgs_num = len(imgs_origin)\n        # 2\xe5\x88\x92\xe5\x88\x86\xe6\x95\xb0\xe6\x8d\xae\n        # \xe5\xa6\x82\xe6\x9e\x9c\xe6\x98\xaf\xe6\xb5\x8b\xe8\xaf\x95\xe9\x9b\x86\xe5\xb0\xb1\xe7\x9b\xb4\xe6\x8e\xa5\xe7\x94\xa8\n        if self.test:\n            self.imgs_origin = imgs_origin\n            self.imgs_grayscale=imgs_grayscale\n\n        elif train:\n            self.imgs_origin=imgs_origin[:int(0.9*imgs_num)]\n            self.imgs_grayscale = imgs_grayscale[:int(0.9 * imgs_num)]\n\n        else:\n            self.imgs_origin=imgs_origin[int(0.9 * imgs_num):]\n            self.imgs_grayscale = imgs_grayscale[int(0.9 * imgs_num):]\n\n\n        # 3 \xe5\xaf\xb9\xe5\x9b\xbe\xe5\x83\x8f\xe8\xbf\x9b\xe8\xa1\x8c\xe8\xbd\xac\xe5\x8c\x96(\xe8\x8b\xa5\xe6\x9c\xaa\xe6\x8c\x87\xe5\xae\x9a\xe8\xbd\xac\xe5\x8c\x96\xef\xbc\x8c\xe5\x88\x99\xe6\x89\xa7\xe8\xa1\x8c\xe9\xbb\x98\xe8\xae\xa4\xe6\x93\x8d\xe4\xbd\x9c)===========\n        if transforms is None:\n            normalize = T.Normalize(mean=[0, 0, 0],\n                                    std=[1, 1, 1])\n\n            if self.test or not train:\n                self.transforms = T.Compose([\n                    T.Scale(224),\n                    T.CenterCrop(224),\n                    T.ToTensor(),\n                    normalize\n                ])\n            else:\n                self.transforms = T.Compose([\n                    # \xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\xe6\x9c\xaa\xe8\xbf\x9b\xe8\xa1\x8c\xe9\x9a\x8f\xe6\x9c\xba\xe8\xae\xbe\xe7\xbd\xae\xef\xbc\x8c\xe6\x98\xaf\xe5\x9b\xa0\xe4\xb8\xba\xe5\x88\x86\xe7\xb1\xbb\xe9\x9c\x80\xe8\xa6\x81\xe9\x9a\x8f\xe6\x9c\xba\xe6\x88\xaa\xe5\x8f\x96\xe5\x9b\xbe\xe5\x83\x8f\xef\xbc\x8c\xe4\xbb\xa5\xe4\xbe\xbf\xe6\x8f\x90\xe9\xab\x98\xe6\xa8\xa1\xe5\x9e\x8b\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87\xe3\x80\x82\n                    # \xe5\xbd\x93\xe5\x89\x8d\xe6\xa8\xa1\xe5\x9e\x8b\xe6\x98\xaf\xe7\x94\xa8\xe4\xba\x8e\xe5\x8e\xbb\xe5\x99\xaa\xef\xbc\x8c\xe6\x95\x85\xe4\xb8\x8d\xe9\x9c\x80\xe8\xa6\x81\n                    # T.Scale(256),\n                    # T.RandomSizedCrop(224),\n                    # T.RandomHorizontalFlip(),\n                    # T.ToTensor(),\n                    # normalize\n                    T.Scale(224),\n                    T.CenterCrop(224),\n                    T.ToTensor(),\n                    normalize\n                ])\n\n    def __getitem__(self, index):\n            '''\n            \xe4\xb8\x80\xe6\xac\xa1\xe8\xbf\x94\xe5\x9b\x9e\xe4\xb8\x80\xe5\xbc\xa0\xe5\x9b\xbe\xe7\x89\x87\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\n            '''\n            img_path_origin=self.imgs_origin[index]\n            imgs_path_grayscale=self.imgs_grayscale[index]\n\n            if self.test:\n                #=================\n                print()\n            else:\n                print()\n            #\xe5\x8a\xa0\xe8\xbd\xbd\xe4\xb8\x80\xe5\xbc\xa0\xe5\x8e\x9f\xe5\x9b\xbe\xe7\x9a\x84\xe7\x81\xb0\xe5\xba\xa6\xe5\x9b\xbe \xe5\x92\x8c \xe4\xb8\x80\xe5\xbc\xa0\xe7\x81\xb0\xe5\xba\xa6\xe5\x8a\xa0\xe5\x99\xaa\xe5\x9b\xbe\n            data_origin=Image.open(img_path_origin).convert('L')\n            data_grayscale=Image.open(imgs_path_grayscale)\n            #\xe5\xaf\xb9\xe5\x9b\xbe\xe7\x89\x87\xe8\xbf\x9b\xe8\xa1\x8c\xe8\xbd\xac\xe5\x8c\x96\n            data_origin=self.transforms(data_origin)\n            data_grayscale=self.transforms(data_grayscale)\n            return data_origin,data_grayscale\n    def __len__(self):\n        return len(self.imgs_origin)\n\n    def get_lena_imgs(self):\n        '''\n        \xe4\xbd\xbf\xe7\x94\xa8lena\xe6\x9d\xa5\xe6\xa3\x80\xe6\xb5\x8b\xe6\xaf\x8f\xe4\xb8\xaaepoch\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\xe6\x95\x88\xe6\x9e\x9c\n        '''\n        traget_img = Image.open('/home/bobo/data/PapersReproduced/lena.jpg').convert('L')\n        input_img=Image.open('/home/bobo/data/PapersReproduced/lena_gray.jpg')\n        # \xe5\xaf\xb9\xe5\x9b\xbe\xe7\x89\x87\xe8\xbf\x9b\xe8\xa1\x8c\xe8\xbd\xac\xe5\x8c\x96\n        traget_img = self.transforms(traget_img)\n        input_img = self.transforms(input_img)\n        return input_img,traget_img\n"""
ImageDenoising_pytorch/data/porductData.py,0,"b'import glob as gb\nfrom PIL import Image\n\nfrom pylab import *\n\nfrom numpy import *\nimport ipdb\nimport random\n#\xe4\xba\xa7\xe7\x94\x9f\xe6\x95\xb0\xe6\x8d\xae\n#\xe8\xaf\xbb\xe5\x8f\x96\xe5\x9b\xbe\xe5\x83\x8f\xef\xbc\x8c\xe8\xbd\xac\xe5\x8c\x96\xe4\xb8\xba\xe7\x81\xb0\xe5\xba\xa6\xe5\x9b\xbe\xef\xbc\x8c\xe5\xb9\xb6\xe6\xb7\xbb\xe5\x8a\xa0 \xe5\x9d\x87\xe5\x80\xbc\xe4\xb8\xba0\xef\xbc\x8c\xe6\x96\xb9\xe5\xb7\xae\xe4\xb8\xba25\xe7\x9a\x84\xe9\xab\x98\xe6\x96\xaf\xe5\x99\xaa\xe5\xa3\xb0\xef\xbc\x8c\xe4\xbf\x9d\xe5\xad\x98\xe5\x9b\xbe\xe5\x83\x8f\nimg_path = gb.glob(""/home/bobo/data/VOCdevkit/Pascal VOC2007/VOCdevkit/VOC2007/JPEGImages/*.jpg"")\n\nii = 0;\nfor path in img_path:\n    # \xe5\x8f\xaa\xe6\x8b\xbf15000\xe5\xbc\xa0\xe7\x85\xa7\xe7\x89\x87  \xe5\x85\xb6\xe5\xae\x9e\xe5\x8f\xaa\xe6\x9c\x895000\xe5\xbc\xa0\xe7\x85\xa7\xe7\x89\x87\n\n    if (ii < 15000):\n        # \xe8\xaf\xbb\xe5\x8f\x96\xe5\x9b\xbe\xe5\x83\x8f\xef\xbc\x8c\xe8\xbd\xac\xe5\x8c\x96\xe4\xb8\xba\xe7\x81\xb0\xe5\xba\xa6\xe5\x9b\xbe\n        im = array(Image.open(path).convert(\'L\'))\n        # \xe8\xae\xbe\xe5\xae\x9a\xe9\xab\x98\xe6\x96\xaf\xe5\x87\xbd\xe6\x95\xb0\xe7\x9a\x84\xe5\x81\x8f\xe7\xa7\xbb\n        means = 0\n        # \xe8\xae\xbe\xe5\xae\x9a\xe9\xab\x98\xe6\x96\xaf\xe5\x87\xbd\xe6\x95\xb0\xe7\x9a\x84\xe6\xa0\x87\xe5\x87\x86\xe5\xb7\xae\n        sigma = 25\n        im_flatten = im[:, :].flatten()\n        # \xe8\xae\xa1\xe7\xae\x97\xe6\x96\xb0\xe7\x9a\x84\xe5\x83\x8f\xe7\xb4\xa0\xe5\x80\xbc\n        for i in range(im.shape[0] * im.shape[1]):\n            pr = int(im_flatten[i]) + random.gauss(0, sigma)\n            #\xe5\xb0\x8f\xe4\xba\x8e0\xe7\xbd\xae\xe4\xb8\xba0\n            if (pr < 0):\n                pr = 0\n            # \xe5\xa4\xa7\xe4\xba\x8e255,\xe7\xbd\xae\xe4\xb8\xba255\n            if (pr > 255):\n                pr = 255\n            im_flatten[i] = pr\n        im[:, :] = im_flatten.reshape([im.shape[0], im.shape[1]])\n        # \xe4\xbf\x9d\xe5\xad\x98\xe7\x81\xb0\xe5\xba\xa6\xe5\x9b\xbe\n        from scipy import misc\n        # \xe6\x8b\xbf\xe5\x88\xb0\xe7\x85\xa7\xe7\x89\x87\xe5\x90\x8d\xe5\xad\x97\n        imgs_name = path.split(\'.\')[-2].split(\'/\')[-1]\n        # \xe4\xbf\x9d\xe5\xad\x98\xe5\x9b\xbe\xe5\x83\x8f\n        misc.imsave(\n            \'/home/bobo/data/VOCdevkit/Pascal VOC2007/VOCdevkit/VOC2007/JPEGImages_Noise_added_grayscale/\' + imgs_name + \'.jpg\',\n            im)\n\n# \xe8\xaf\xbb\xe5\x8f\x96\xe8\xaf\xa5JPEGImages_Noise_added_grayscale\xe7\x9b\xae\xe5\xbd\x95\xe4\xb8\x8b\xe6\x89\x80\xe6\x9c\x89\xe7\x9a\x84jpg\xe6\x96\x87\xe4\xbb\xb6\nimg_path2 = gb.glob(""/home/bobo/data/VOCdevkit/Pascal VOC2007/VOCdevkit/VOC2007/JPEGImages_Noise_added_grayscale/*.jpg"")\nprint(img_path2.__len__())'"
ImageDenoising_pytorch/models/NetWork.py,1,"b'# -*- coding:utf-8 -*-\n# power by Mr.Li\n\nfrom torch import nn\nimport  time\nimport torch as t\nclass NetWork(nn.Module):\n    \'\'\'\n    \xe5\xae\x9a\xe4\xb9\x89\xe5\x8e\xbb\xe5\x99\xaa\xe7\xbd\x91\xe7\xbb\x9c\n    \'\'\'\n    def __init__(self):\n        super(NetWork,self).__init__()\n        # \xe8\xae\xbe\xe7\xbd\xae\xe5\x8e\xbb\n        self.moduel_name=str(""NetWork"")\n        self.main=nn.Sequential(\n\n            #\xe9\x97\xae\xe9\xa2\x98\xef\xbc\x9a\n            #padding  \xe4\xb8\x8a\xe4\xb8\x8b\xe5\xb7\xa6\xe5\x8f\xb3\xe5\xa1\xab\xe5\x85\x850\xe7\x9a\x84\xe6\x95\xb0\xe9\x87\x8f\xe4\xb8\xba \xef\xbc\x88\xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8\xe5\xa4\xa7\xe5\xb0\x8f-1\xef\xbc\x89/2  \n            #\xe6\x97\xa0pool\n            # \xe7\x9b\xb8\xe6\xaf\x94\xe8\xbe\x83Relu\xef\xbc\x8cLeakyReLU\xe6\x9b\xb4\xe4\xbc\x98\xef\xbc\x9f \xe6\x9c\xaa\xe6\xb5\x8b\xe8\xaf\x95\n            # 1\xe4\xbb\xa3\xe8\xa1\xa8\xe8\xbe\x93\xe5\x85\xa5\xe6\x97\xb6\xe7\x9a\x84\xe5\x9b\xbe\xe5\x83\x8f\xe5\xb1\x82\xe6\x95\xb0,32\xe4\xbb\xa3\xe8\xa1\xa8\xe8\xbe\x93\xe5\x87\xba\xe7\x9a\x84\xe5\xb1\x82\xe6\x95\xb0,5\xe4\xbb\xa3\xe8\xa1\xa8\xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8\n            # nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)\n            #\xe5\x8d\xb7\xe7\xa7\xaf\n            nn.Conv2d(in_channels=1,out_channels=32,kernel_size=5,stride=1,padding=2,bias=True),\n            #\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0BN\xe5\xb1\x82\xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0\xe6\x98\xaf\xe8\xbe\x93\xe5\x87\xba\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0out_channels=32\n            nn.BatchNorm2d(32),\n            nn.ReLU(True),\n\n            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1, bias=True),\n            # \xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0BN\xe5\xb1\x82\xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0\xe6\x98\xaf\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0out_channels=64\n            nn.BatchNorm2d(64),\n            nn.ReLU(True),\n\n            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=1, stride=1, padding=0, bias=True),\n            # \xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0BN\xe5\xb1\x82\xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0\xe6\x98\xaf\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0out_channels=64\n            nn.BatchNorm2d(64),\n            nn.ReLU(True),\n\n\n            #\xe5\x8f\x8d\xe5\x8d\xb7\xe7\xa7\xaf\n            # class torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True)\n            nn.ConvTranspose2d(in_channels=64,out_channels=32 ,kernel_size=3, stride=1,padding=1, bias=True),\n            nn.BatchNorm2d(32),\n            nn.ReLU(True),\n\n            nn.ConvTranspose2d(in_channels=32, out_channels=1, kernel_size=5, stride=1, padding=2, bias=True),\n            # nn.Tanh()  # \xe8\xbe\x93\xe5\x87\xba\xe8\x8c\x83\xe5\x9b\xb4 -1~1 \xe6\x95\x85\xe8\x80\x8c\xe9\x87\x87\xe7\x94\xa8Tanh # \xe8\xbe\x93\xe5\x87\xba\xe5\xbd\xa2\xe7\x8a\xb6\xef\xbc\x9a1x \xe9\x95\xbf x \xe5\xae\xbd\n            nn.Sigmoid()  # \xe8\xbe\x93\xe5\x87\xba\xe8\x8c\x83\xe5\x9b\xb4 0~1  \n        )\n    def forward(self,x):\n        # \xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\n        x=self.main(x)\n        return x\n\n    def save(self, name=None):\n        \'\'\'\n        \xe8\xae\xbe\xe7\xbd\xae\xe5\xad\x98\xe5\x82\xa8\xe6\xa8\xa1\xe5\x9e\x8b\xe7\x9a\x84\xe8\xb7\xaf\xe5\xbe\x84\n        \'\'\'\n        if name is None:\n            prefix = \'/home/bobo/PycharmProjects/torchProjectss/papersReproduced/checkpoints/\' + self.moduel_name + ""_""\n            name = time.strftime(prefix + \'%m%d_%H:%M:%S.pth\')\n        t.save(self.state_dict(), name)\n        return name'"
ImageDenoising_pytorch/models/__init__.py,0,b'from .NetWork import NetWork'
ImageDenoising_pytorch/utils/__init__.py,0,b''
ImageDenoising_pytorch/utils/visualize.py,0,"b""# -*- coding:utf-8 -*-\n# power by Mr.Li\nimport visdom\nimport time\nimport numpy as np\nclass Visualizer(object):\n    '''\n    \xe5\xb0\x81\xe8\xa3\x85\xe4\xba\x86visdom\xe7\x9a\x84\xe5\x9f\xba\xe6\x9c\xac\xe6\x93\x8d\xe4\xbd\x9c\xef\xbc\x8c\xe4\xbd\x86\xe6\x98\xaf\xe4\xbd\xa0\xe4\xbb\x8d\xe7\x84\xb6\xe5\x8f\xaf\xe4\xbb\xa5\xe9\x80\x9a\xe8\xbf\x87`self.vis.function`\n    \xe8\xb0\x83\xe7\x94\xa8\xe5\x8e\x9f\xe7\x94\x9f\xe7\x9a\x84visdom\xe6\x8e\xa5\xe5\x8f\xa3\n    '''\n    def __init__(self, env='default', **kwargs):\n        self.vis = visdom.Visdom(env=env, **kwargs)\n        # \xe7\x94\xbb\xe7\x9a\x84\xe7\xac\xac\xe5\x87\xa0\xe4\xb8\xaa\xe6\x95\xb0\xef\xbc\x8c\xe7\x9b\xb8\xe5\xbd\x93\xe4\xba\x8e\xe6\xa8\xaa\xe5\xba\xa7\xe6\xa0\x87\n        # \xe4\xbf\x9d\xe5\xad\x98\xef\xbc\x88\xe2\x80\x99loss',23\xef\xbc\x89 \xe5\x8d\xb3loss\xe7\x9a\x84\xe7\xac\xac23\xe4\xb8\xaa\xe7\x82\xb9\n        self.index = {}\n        self.log_text = ''\n    def reinit(self,env='default',**kwargs):\n        '''\n        \xe4\xbf\xae\xe6\x94\xb9visdom\xe7\x9a\x84\xe9\x85\x8d\xe7\xbd\xae  \xe9\x87\x8d\xe6\x96\xb0\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\n        '''\n        self.vis = visdom.Visdom(env=env,**kwargs)\n        return self\n    def plot_many(self, d):\n        '''\n        \xe4\xb8\x80\xe6\xac\xa1plot\xe5\xa4\x9a\xe4\xb8\xaa\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x9b\xbe\xe5\xbd\xa2\n        @params d: dict (name,value) i.e. ('loss',0.11)\n        '''\n        for k, v in d.items():\n            self.plot(k, v)\n    def img_many(self, d):\n        '''\n        \xe4\xb8\x80\xe6\xac\xa1\xe7\x94\xbb\xe5\xa4\x9a\xe4\xb8\xaa\xe5\x9b\xbe\xe5\x83\x8f  \n        @params d: dict (name,GPU\xe4\xb8\x8a\xe7\x9a\x84Tensor) \xe8\x8b\xa5\xe4\xb8\xbaVariable\xef\xbc\x8c\xe8\xb0\x83\xe7\x94\xa8\xe6\x96\xb9\xe6\xb3\x95\xe5\x89\x8d\xe9\x9c\x80\xe6\x8f\x90\xe5\x89\x8d\xe8\xbd\xac\xe4\xb8\xbaTensor. (V.data\xe5\x8d\xb3\xe5\x8f\xaf)\n        '''\n        for k, v in d.items():\n            self.img(k, v)\n    def plot(self, name, y,**kwargs):\n        '''\n        self.plot('loss',1.00)\n        '''\n        #\xe5\xbe\x97\xe5\x88\xb0\xe4\xb8\x8b\xe6\xa0\x87\xe5\xba\x8f\xe5\x8f\xb7\n        x = self.index.get(name, 0)\n        self.vis.line(Y=np.array([y]), X=np.array([x]),\n                      win=name,#\xe7\xaa\x97\xe5\x8f\xa3\xe5\x90\x8d\n                      opts=dict(title=name),\n                      update=None if x == 0 else 'append', #\xe6\x8c\x89\xe7\x85\xa7append\xe7\x9a\x84\xe7\x94\xbb\xe5\x9b\xbe\xe5\xbd\xa2\n                      **kwargs\n                      )\n        #\xe4\xb8\x8b\xe6\xa0\x87\xe7\xb4\xaf\xe5\x8a\xa01\n        self.index[name] = x + 1\n    def img(self, name, img_,**kwargs):\n        '''\n        self.img('input_img',t.Tensor(64,64))\n        self.img('input_imgs',t.Tensor(3,64,64))\n        self.img('input_imgs',t.Tensor(100,1,64,64))\n        self.img('input_imgs',t.Tensor(100,3,64,64),nrows=10)\n        \xef\xbc\x81\xef\xbc\x81\xef\xbc\x81don\xe2\x80\x98t ~~self.img('input_imgs',t.Tensor(100,64,64),nrows=10)~~\xef\xbc\x81\xef\xbc\x81\xef\xbc\x81\n        '''\n    \n        self.vis.images(img_.cpu().numpy(),\n                       win=name,\n                       opts=dict(title=name),\n                       **kwargs\n                       )\n    def log(self,info,win='log_text'):\n        '''\n        self.log({'loss':1,'lr':0.0001})\n        \xe6\x89\x93\xe5\x8d\xb0\xe6\x97\xa5\xe5\xbf\x97\n        '''\n\n        self.log_text += ('[{time}] {info} <br>'.format(\n                            time=time.strftime('%m%d_%H%M%S'),\\\n                            info=info))\n        self.vis.text(self.log_text,win)\n    def __getattr__(self, name):\n        return getattr(self.vis, name)"""
Noise2noise_pytorch/checkpoints/__init__.py,0,b''
Noise2noise_pytorch/data/datasets.py,1,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\nimport torchvision.transforms.functional as tvF\nfrom torch.utils.data import Dataset, DataLoader\n\nimport os\nfrom sys import platform\nimport numpy as np\nimport random\nfrom string import ascii_letters\nfrom PIL import Image, ImageFont, ImageDraw\n\ndef load_dataset(root_dir, redux, params, shuffled=False, single=False):\n    """"""\n    Loads dataset and returns corresponding data loader.  \xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe5\x8a\xa0\xe8\xbd\xbd\xe5\x99\xa8\n     redux\xe6\x8c\x87 \xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\xe5\xa4\xa7\xe5\xb0\x8f or \xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86\xe5\xa4\xa7\xe5\xb0\x8f\n    """"""\n\n    # Create Torch dataset  \xe6\x96\xb0\xe5\xa2\x9e \xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86   \xe5\x99\xaa\xe5\xa3\xb0\xe7\xb1\xbb\xe5\x9e\x8b\xe5\x92\x8c\xe5\x99\xaa\xe5\xa3\xb0\xe5\x8f\x82\xe6\x95\xb0\n    noise = (params.noise_type, params.noise_param)\n\n    # Instantiate appropriate dataset class  \xe5\xae\x9e\xe4\xbe\x8b\xe5\x8c\x96\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\n    # if params.noise_type == \'mc\':\n    # #mc\xe6\x8c\x87 \xe8\x92\x99\xe7\x89\xb9\xe5\x8d\xa1\xe6\xb4\x9b\n    #     dataset = MonteCarloDataset(root_dir, redux, params.crop_size,\n    #         clean_targets=params.clean_targets)\n    # else:\n    # \xe5\x85\xb6\xe4\xbd\x99\xe5\x99\xaa\xe5\xa3\xb0 \xe7\x94\x9f\xe6\x88\x90\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\n    dataset = NoisyDataset(root_dir, redux, params.crop_size,\n        clean_targets=params.clean_targets, noise_dist=noise, seed=params.seed)\n\n    # Use batch size of 1, if requested (e.g. test set)\n    # \xe5\xbd\x93single=True,\xe5\x88\x99 batch_size=1 \xef\xbc\x88\xe6\xb5\x8b\xe8\xaf\x95\xe9\x9b\x86\xe9\x9c\x80\xe8\xa6\x81\xef\xbc\x89\n    if single:\n        return DataLoader(dataset, batch_size=1, shuffle=shuffled)\n    else:\n        return DataLoader(dataset, batch_size=params.batch_size, shuffle=shuffled)\n\n\nclass AbstractDataset(Dataset):\n    """"""Abstract dataset class for Noise2Noise.  Noise2Noise\xe7\x9a\x84\xe6\x8a\xbd\xe8\xb1\xa1\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe7\xb1\xbb""""""\n\n    def __init__(self, root_dir, redux=0, crop_size=128, clean_targets=False):\n        """"""Initializes abstract dataset.""""""\n\n        super(AbstractDataset, self).__init__()\n\n        self.imgs = []\n        self.root_dir = root_dir\n        self.redux = redux\n        self.crop_size = crop_size\n        self.clean_targets = clean_targets\n\n    def _random_crop(self, img_list):\n        """"""Performs random square crop of fixed size.\n        Works with list so that all items get the same cropped window (e.g. for buffers).\n        """"""\n\n        w, h = img_list[0].size\n        assert w >= self.crop_size and h >= self.crop_size, \\\n            f\'Error: Crop size: {self.crop_size}, Image size: ({w}, {h})\'\n        cropped_imgs = []\n        i = np.random.randint(0, h - self.crop_size + 1)\n        j = np.random.randint(0, w - self.crop_size + 1)\n\n        for img in img_list:\n            # Resize if dimensions are too small\n            if min(w, h) < self.crop_size:\n                img = tvF.resize(img, (self.crop_size, self.crop_size))\n\n            # Random crop\n            cropped_imgs.append(tvF.crop(img, i, j, self.crop_size, self.crop_size))\n\n        return cropped_imgs\n\n\n    def __getitem__(self, index):\n        """"""Retrieves image from data folder.""""""\n\n        raise NotImplementedError(\'Abstract method not implemented!\')\n\n\n    def __len__(self):\n        """"""Returns length of dataset.""""""\n\n        return len(self.imgs)\n\n\nclass NoisyDataset(AbstractDataset):\n    """"""Class for injecting random noise into dataset. \xe5\xb0\x86\xe9\x9a\x8f\xe6\x9c\xba\xe5\x99\xaa\xe5\xa3\xb0\xe6\xb3\xa8\xe5\x85\xa5\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xef\xbc\x8c\xe7\xbb\xa7\xe6\x89\xbfAbstractDataset\xe7\xb1\xbb   """"""\n\n    def __init__(self, root_dir, redux, crop_size, clean_targets=False,\n        noise_dist=(\'gaussian\', 50.), seed=None):\n        """"""Initializes noisy image dataset.  \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96 \xe5\x99\xaa\xe5\xa3\xb0\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86""""""\n\n        super(NoisyDataset, self).__init__(root_dir, redux, crop_size, clean_targets)\n\n        # \xe8\xaf\xbb\xe5\x8f\x96\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\n        self.imgs = os.listdir(root_dir)\n\n        if redux:\n            self.imgs = self.imgs[:redux]\n\n        # Noise parameters (max std for Gaussian, lambda for Poisson, nb of artifacts for text)\n        # \xe5\x99\xaa\xe5\xa3\xb0\xe5\x8f\x82\xe6\x95\xb0\n        self.noise_type = noise_dist[0]\n        self.noise_param = noise_dist[1]\n        self.seed = seed\n        if self.seed:\n            np.random.seed(self.seed)\n\n\n    def _add_noise(self, img):\n        """"""Adds Gaussian or Poisson noise to image.""""""\n\n        w, h = img.size\n        c = len(img.getbands())\n\n        # Poisson distribution\n        # It is unclear how the paper handles this. Poisson noise is not additive,\n        # it is data dependent, meaning that adding sampled valued from a Poisson\n        # will change the image intensity...\n        if self.noise_type == \'poisson\':\n            noise = np.random.poisson(img)\n            noise_img = img + noise\n            noise_img = 255 * (noise_img / np.amax(noise_img))\n\n        # Normal distribution (default)\n        else:\n            if self.seed:\n                std = self.noise_param\n            else:\n                std = np.random.uniform(0, self.noise_param)\n            noise = np.random.normal(0, std, (h, w, c))\n\n            # Add noise and clip\n            noise_img = np.array(img) + noise\n\n        noise_img = np.clip(noise_img, 0, 255).astype(np.uint8)\n        return Image.fromarray(noise_img)\n\n\n    def _add_text_overlay(self, img):\n        """"""Adds text overlay to images. \xe4\xb8\xba\xe5\x9b\xbe\xe5\x83\x8f\xe6\xb7\xbb\xe5\x8a\xa0\xe6\x96\x87\xe6\x9c\xac\xe5\x8f\xa0\xe5\x8a\xa0""""""\n\n        assert self.noise_param < 1, \'Text parameter is an occupancy probability\'\n\n        w, h = img.size\n        c = len(img.getbands())\n\n        # Choose font and get ready to draw\n        if platform == \'linux\':\n            serif = \'/usr/share/fonts/truetype/dejavu/DejaVuSerif.ttf\'\n        else:\n            serif = \'Times New Roman.ttf\'\n        text_img = img.copy()\n        text_draw = ImageDraw.Draw(text_img)\n\n        # Text binary mask to compute occupancy efficiently\n        w, h = img.size\n        mask_img = Image.new(\'1\', (w, h))\n        mask_draw = ImageDraw.Draw(mask_img)\n\n        # Random occupancy in range [0, p]\n        if self.seed:\n            random.seed(self.seed)\n            max_occupancy = self.noise_param\n        else:\n            max_occupancy = np.random.uniform(0, self.noise_param)\n        def get_occupancy(x):\n            y = np.array(x, dtype=np.uint8)\n            return np.sum(y) / y.size\n\n        # Add text overlay by choosing random text, length, color and position\n        while 1:\n            font = ImageFont.truetype(serif, np.random.randint(16, 21))\n            length = np.random.randint(10, 25)\n            chars = \'\'.join(random.choice(ascii_letters) for i in range(length))\n            color = tuple(np.random.randint(0, 255, c))\n            pos = (np.random.randint(0, w), np.random.randint(0, h))\n            text_draw.text(pos, chars, color, font=font)\n\n            # Update mask and check occupancy\n            mask_draw.text(pos, chars, 1, font=font)\n            if get_occupancy(mask_img) > max_occupancy:\n                break\n\n        return text_img\n\n\n    def _corrupt(self, img):\n        """"""Corrupts images (Gaussian, Poisson, or text overlay).""""""\n\n        if self.noise_type in [\'gaussian\', \'poisson\']:\n            return self._add_noise(img)\n        elif self.noise_type == \'text\':\n            return self._add_text_overlay(img)\n        else:\n            raise ValueError(\'Invalid noise type: {}\'.format(self.noise_type))\n\n\n    def __getitem__(self, index):\n        """"""Retrieves image from folder and corrupts it.""""""\n\n        # Load PIL image\n        img_path = os.path.join(self.root_dir, self.imgs[index])\n        img =Image.open(img_path).convert(\'RGB\')\n\n        # Random square crop\n        if self.crop_size != 0:\n            img = self._random_crop([img])[0]\n\n        # Corrupt source image \xe8\xbd\xac\xe5\x8c\x96\xe4\xb8\xba\xe6\x8d\x9f\xe5\x9d\x8f\xe5\x9b\xbe\xe5\x83\x8f\n        source = tvF.to_tensor(self._corrupt(img))\n\n        # Corrupt target image, but not when clean targets are requested\n        # \xe9\x80\x89\xe6\x8b\xa9 \xe6\xa0\x87\xe7\xad\xbe\xe4\xb8\xba \xe5\xb9\xb2\xe5\x87\x80\xe5\x9b\xbe\xe5\x83\x8f or \xe5\x99\xaa\xe5\xa3\xb0\xe5\x9b\xbe\xe5\x83\x8f\n        if self.clean_targets:\n            target = tvF.to_tensor(img)\n        else:\n            target = tvF.to_tensor(self._corrupt(img))\n\n        return source, target\n\n'"
Noise2noise_pytorch/models/__init__.py,0,b'from .unet import UNet'
Noise2noise_pytorch/models/unet.py,6,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\nimport torch\nimport torch.nn as nn\n\n\nclass UNet(nn.Module):\n    """"""Custom U-Net architecture for Noise2Noise (see Appendix, Table 2).""""""\n\n    def __init__(self, in_channels=3, out_channels=3):\n        """"""Initializes U-Net.""""""\n\n        super(UNet, self).__init__()\n\n        # Layers: enc_conv0, enc_conv1, pool1\n        self._block1 = nn.Sequential(\n            nn.Conv2d(in_channels, 48, 3, stride=1, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(48, 48, 3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2))\n\n        # Layers: enc_conv(i), pool(i); i=2..5\n        self._block2 = nn.Sequential(\n            nn.Conv2d(48, 48, 3, stride=1, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2))\n\n        # Layers: enc_conv6, upsample5\n        self._block3 = nn.Sequential(\n            nn.Conv2d(48, 48, 3, stride=1, padding=1),\n            nn.ReLU(inplace=True),\n            nn.ConvTranspose2d(48, 48, 3, stride=2, padding=1, output_padding=1))\n            #nn.Upsample(scale_factor=2, mode=\'nearest\'))\n\n        # Layers: dec_conv5a, dec_conv5b, upsample4\n        self._block4 = nn.Sequential(\n            nn.Conv2d(96, 96, 3, stride=1, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(96, 96, 3, stride=1, padding=1),\n            nn.ReLU(inplace=True),\n            nn.ConvTranspose2d(96, 96, 3, stride=2, padding=1, output_padding=1))\n            #nn.Upsample(scale_factor=2, mode=\'nearest\'))\n\n        # Layers: dec_deconv(i)a, dec_deconv(i)b, upsample(i-1); i=4..2\n        self._block5 = nn.Sequential(\n            nn.Conv2d(144, 96, 3, stride=1, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(96, 96, 3, stride=1, padding=1),\n            nn.ReLU(inplace=True),\n            nn.ConvTranspose2d(96, 96, 3, stride=2, padding=1, output_padding=1))\n            #nn.Upsample(scale_factor=2, mode=\'nearest\'))\n\n        # Layers: dec_conv1a, dec_conv1b, dec_conv1c,\n        self._block6 = nn.Sequential(\n            nn.Conv2d(96 + in_channels, 64, 3, stride=1, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(64, 32, 3, stride=1, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(32, out_channels, 3, stride=1, padding=1),\n            nn.LeakyReLU(0.1))\n\n        # Initialize weights\n        self._init_weights()\n\n\n    def _init_weights(self):\n        """"""Initializes weights using He et al. (2015).""""""\n\n        for m in self.modules():\n            if isinstance(m, nn.ConvTranspose2d) or isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight.data)\n                m.bias.data.zero_()\n\n\n    def forward(self, x):\n        """"""Through encoder, then decoder by adding U-skip connections. """"""\n\n        # Encoder\n        pool1 = self._block1(x)\n        pool2 = self._block2(pool1)\n        pool3 = self._block2(pool2)\n        pool4 = self._block2(pool3)\n        pool5 = self._block2(pool4)\n\n        # Decoder\n        upsample5 = self._block3(pool5)\n        concat5 = torch.cat((upsample5, pool4), dim=1)\n        upsample4 = self._block4(concat5)\n        concat4 = torch.cat((upsample4, pool3), dim=1)\n        upsample3 = self._block5(concat4)\n        concat3 = torch.cat((upsample3, pool2), dim=1)\n        upsample2 = self._block5(concat3)\n        concat2 = torch.cat((upsample2, pool1), dim=1)\n        upsample1 = self._block5(concat2)\n        concat1 = torch.cat((upsample1, x), dim=1)\n\n        # Final activation\n        return self._block6(concat1)\n'"
Noise2noise_pytorch/utils/__init__.py,0,b''
Noise2noise_pytorch/utils/config.py,0,"b""# -*- coding:utf-8 -*-\n# power by Mr.Li\n# \xe8\xae\xbe\xe7\xbd\xae\xe8\xae\xad\xe7\xbb\x83\xe6\x97\xb6\xe7\x9a\x84\xe9\xbb\x98\xe8\xae\xa4\xe5\x8f\x82\xe6\x95\xb0\nclass Config_Train():\n\n\n    # \xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe5\x8f\x82\xe6\x95\xb0\n    train_dir = '/home/bobo/data/noise2noise/train'  # \xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\xe5\x9c\xb0\xe5\x9d\x80\n    valid_dir = '/home/bobo/data/noise2noise/valid'  # \xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86\xe5\x9c\xb0\xe5\x9d\x80\n    ckpt_save_path = './checkpoints'  # \xe6\xa8\xa1\xe5\x9e\x8b\xe4\xbf\x9d\xe5\xad\x98\xe5\x9c\xb0\xe5\x9d\x80\n    report_interval = 250  # \xe6\xaf\x8f\xe5\x87\xa0\xe6\xac\xa1\xe8\xbf\x9b\xe8\xa1\x8c\xe8\xbe\x93\xe5\x87\xba\xe8\xae\xad\xe7\xbb\x83\xe4\xbf\xa1\xe6\x81\xaf\n    train_size = 1000  # \xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\xe5\xa4\xa7\xe5\xb0\x8f\n    valid_size = 200  # \xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86\xe5\xa4\xa7\xe5\xb0\x8f\n    ckpt_overwrite=False\n\n    # \xe8\xb6\x85\xe5\x8f\x82\xe6\x95\xb0\n    learning_rate = 0.001  # \xe5\x88\x9d\xe5\xa7\x8b\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\n    adam = [0.9, 0.99, 1e-8]  # \xe4\xbc\x98\xe5\x8c\x96\xe5\x99\xa8\n    batch_size = 4  # batch-size\xe5\xa4\xa7\xe5\xb0\x8f\n    nb_epochs = 100  # \xe8\xae\xad\xe7\xbb\x83\xe8\xbd\xae\xe6\x95\xb0\n    loss = 'l1'  # \xe6\x8d\x9f\xe5\xa4\xb1\xe5\x87\xbd\xe6\x95\xb0  \xe9\x80\x89L1\xef\xbc\x8c\xe5\x80\xbe\xe5\x90\x91\xe4\xba\x8e\xe5\xad\xa6\xe4\xb9\xa0\xe4\xb8\xad\xe4\xbd\x8d\xe6\x95\xb0  \xe5\x8f\xaf\xe9\x80\x89'l1', 'l2'\n    cuda = True  # \xe4\xbd\xbf\xe7\x94\xa8GPU\n\n    # \xe7\x94\x9f\xe6\x88\x90\xe5\x99\xaa\xe5\xa3\xb0\xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0\n    noise_type = 'text'  # \xe9\x80\x89\xe6\x8b\xa9\xe5\x99\xaa\xe5\xa3\xb0\xe7\xb1\xbb\xe5\x9e\x8b\xef\xbc\x8c\xe9\x80\x89\xe6\xb0\xb4\xe5\x8d\xb0  \xe5\x8f\xaf\xe9\x80\x89'gaussian', 'poisson', 'text'\n    noise_param = 0.5  # text\xe5\x99\xaa\xe5\xa3\xb0\xef\xbc\x8c\xe5\x8d\xb3\xe6\xb0\xb4\xe5\x8d\xb0\xe5\x99\xaa\xe5\xa3\xb0\xe9\x80\x890.5\xef\xbc\x8c\xe9\xab\x98\xe6\x96\xaf\\\xe6\xb3\x8a\xe6\x9d\xbe\xe5\x99\xaa\xe5\xa3\xb0\xe9\x80\x8950\xef\xbc\x8c\n    crop_size = 128  # \xe5\xaf\xb9\xe5\x8e\x9f\xe5\x9b\xbe\xe8\xbf\x9b\xe8\xa1\x8c\xe9\x9a\x8f\xe6\x9c\xba\xe8\xa3\x81\xe5\x89\xaa\xe7\x9a\x84\xe5\xb0\xba\xe5\xaf\xb8\xef\xbc\x8c\xe7\x94\xa8\xe4\xba\x8e\xe8\xbe\x93\xe5\x85\xa5\xe7\xbd\x91\xe7\xbb\x9c\n    clean_targets = False  # \xe6\x98\xaf\xe5\x90\xa6\xe4\xbd\xbf\xe7\x94\xa8 \xe5\xb9\xb2\xe5\x87\x80\xe7\x9b\xae\xe6\xa0\x87\xe4\xbd\x9c\xe4\xb8\xba\xe6\xa0\x87\xe7\xad\xbe \xe8\xbf\x9b\xe8\xa1\x8c\xe8\xae\xad\xe7\xbb\x83\n    seed=False # \xe5\x9b\xba\xe5\xae\x9a\xe9\x9a\x8f\xe6\x9c\xba\xe7\xa7\x8d\xe5\xad\x90\n\n# \xe8\xae\xbe\xe7\xbd\xae\xe6\xb5\x8b\xe8\xaf\x95\xe6\x97\xb6\xe7\x9a\x84\xe9\xbb\x98\xe8\xae\xa4\xe5\x8f\x82\xe6\x95\xb0\nclass Config_Test():\n\n\n    # \xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe5\x8f\x82\xe6\x95\xb0\n    data= '/home/bobo/data/noise2noise/valid'\n    load_ckpt='/home/bobo/windowsPycharmProject/noise2noise-pytorch/checkpoints/text-1142/n2n-epoch49-0.10109.pt'  #\xe5\x8a\xa0\xe8\xbd\xbd\xe6\xa8\xa1\xe5\x9e\x8b\xe7\x9a\x84\xe5\x9c\xb0\xe5\x9d\x80\n    cuda=True # \xe4\xbd\xbf\xe7\x94\xa8GPU\n\n\n\n    # \xe7\x94\x9f\xe6\x88\x90\xe5\x99\xaa\xe5\xa3\xb0\xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0\n    noise_type = 'text'  # \xe9\x80\x89\xe6\x8b\xa9\xe5\x99\xaa\xe5\xa3\xb0\xe7\xb1\xbb\xe5\x9e\x8b\xef\xbc\x8c\xe9\x80\x89\xe6\xb0\xb4\xe5\x8d\xb0  \xe5\x8f\xaf\xe9\x80\x89'gaussian', 'poisson', 'text'\n    noise_param = 0.5  # text\xe5\x99\xaa\xe5\xa3\xb0\xef\xbc\x8c\xe5\x8d\xb3\xe6\xb0\xb4\xe5\x8d\xb0\xe5\x99\xaa\xe5\xa3\xb0\xe9\x80\x890.5, \xe9\xab\x98\xe6\x96\xaf\\\xe6\xb3\x8a\xe6\x9d\xbe\xe5\x99\xaa\xe5\xa3\xb0\xe9\x80\x8950\xef\xbc\x8c\n    seed = False  # \xe5\x9b\xba\xe5\xae\x9a\xe9\x9a\x8f\xe6\x9c\xba\xe7\xa7\x8d\xe5\xad\x90\n    crop_size = 256  # \xe5\xaf\xb9\xe5\x8e\x9f\xe5\x9b\xbe\xe8\xbf\x9b\xe8\xa1\x8c\xe9\x9a\x8f\xe6\x9c\xba\xe8\xa3\x81\xe5\x89\xaa\xe7\x9a\x84\xe5\xb0\xba\xe5\xaf\xb8\xef\xbc\x8c\xe7\x94\xa8\xe4\xba\x8e\xe8\xbe\x93\xe5\x85\xa5\xe7\xbd\x91\xe7\xbb\x9c\n    clean_targets=True # \xe6\xb5\x8b\xe8\xaf\x95\xe6\x97\xb6\xe5\xb0\x86 \xe4\xbd\xbf\xe7\x94\xa8\xe5\xb9\xb2\xe5\x87\x80\xe7\x9a\x84\xe6\xa0\x87\xe7\xad\xbe\xe7\x9c\x9f\xe5\xae\x9e \xe8\xbf\x9b\xe8\xa1\x8c\xe5\xaf\xb9\xe6\xaf\x94\n\n\n# \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe8\xaf\xa5\xe7\xb1\xbb\xe7\x9a\x84\xe4\xb8\x80\xe4\xb8\xaa\xe5\xaf\xb9\xe8\xb1\xa1\nopt_train = Config_Train()\nopt_test = Config_Test()"""
Noise2noise_pytorch/utils/utils.py,4,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\nimport torch\nimport torch.nn.functional as F\nimport torchvision.transforms.functional as tvF\n\nimport os\nimport numpy as np\nfrom datetime import datetime\n\n\nfrom matplotlib import rcParams\nrcParams[\'font.family\'] = \'serif\'\nimport matplotlib\nmatplotlib.use(\'agg\')\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import MaxNLocator\n\n\ndef clear_line():\n    """"""Clears line from any characters.""""""\n\n    print(\'\\r{}\'.format(\' \' * 80), end=\'\\r\')\n\n\ndef progress_bar(batch_idx, num_batches, report_interval, train_loss):\n    """"""Neat progress bar to track training.""""""\n\n    dec = int(np.ceil(np.log10(num_batches)))\n    bar_size = 21 + dec\n    progress = (batch_idx % report_interval) / report_interval\n    fill = int(progress * bar_size) + 1\n    print(\'\\rBatch {:>{dec}d} [{}{}] Train loss: {:>1.5f}\'.format(batch_idx + 1, \'=\' * fill + \'>\', \' \' * (bar_size - fill), train_loss, dec=str(dec)), end=\'\')\n\n\ndef time_elapsed_since(start):\n    """"""Computes elapsed time since start.""""""\n\n    timedelta = datetime.now() - start\n    string = str(timedelta)[:-7]\n    ms = int(timedelta.total_seconds() * 1000)\n\n    return string, ms\n\n\ndef show_on_epoch_end(epoch_time, valid_time, valid_loss, valid_psnr):\n    """"""Formats validation error stats.""""""\n\n    clear_line()\n    print(\'Train time: {} | Valid time: {} | Valid loss: {:>1.5f} | Avg PSNR: {:.2f} dB\'.format(epoch_time, valid_time, valid_loss, valid_psnr))\n\n\ndef show_on_report(batch_idx, num_batches, loss, elapsed):\n    """"""Formats training stats.""""""\n\n    clear_line()\n    dec = int(np.ceil(np.log10(num_batches)))\n    print(\'Batch {:>{dec}d} / {:d} | Avg loss: {:>1.5f} | Avg train time / batch: {:d} ms\'.format(batch_idx + 1, num_batches, loss, int(elapsed), dec=dec))\n\n\n\n\n\n\n\n\ndef reinhard_tonemap(tensor):\n    """"""Reinhard et al. (2002) tone mapping.""""""\n\n    tensor[tensor < 0] = 0\n    return torch.pow(tensor / (1 + tensor), 1 / 2.2)\n\n\ndef psnr(input, target):\n    """"""Computes peak signal-to-noise ratio.""""""\n    \n    return 10 * torch.log10(1 / F.mse_loss(input, target))\n\n\ndef create_montage(img_name, noise_type, save_path, source_t, denoised_t, clean_t, show):\n    """"""Creates montage for easy comparison.""""""\n\n    fig, ax = plt.subplots(1, 3, figsize=(9, 3))\n    fig.canvas.set_window_title(img_name.capitalize()[:-4])\n\n    # Bring tensors to CPU\n    source_t = source_t.cpu().narrow(0, 0, 3)\n    denoised_t = denoised_t.cpu()\n    clean_t = clean_t.cpu()\n    \n    source = tvF.to_pil_image(source_t)\n    denoised = tvF.to_pil_image(torch.clamp(denoised_t, 0, 1))\n    clean = tvF.to_pil_image(clean_t)\n\n    # Build image montage\n    psnr_vals = [psnr(source_t, clean_t), psnr(denoised_t, clean_t)]\n    titles = [\'Input: {:.2f} dB\'.format(psnr_vals[0]),\n              \'Denoised: {:.2f} dB\'.format(psnr_vals[1]),\n              \'Ground truth\']\n    zipped = zip(titles, [source, denoised, clean])\n    for j, (title, img) in enumerate(zipped):\n        ax[j].imshow(img)\n        ax[j].set_title(title)\n        ax[j].axis(\'off\')\n\n    # Open pop up window, if requested\n    if show > 0:\n        plt.show()\n\n    # Save to files\n    fname = os.path.splitext(img_name)[0]\n    source.save(os.path.join(save_path, \'{fname}-{noise_type}-noisy.png\'))\n    denoised.save(os.path.join(save_path, \'{fname}-{noise_type}-denoised.png\'))\n    fig.savefig(os.path.join(save_path, \'{fname}-{noise_type}-montage.png\'), bbox_inches=\'tight\')\n\n\nclass AvgMeter(object):\n    """"""Computes and stores the average and current value.\n    Useful for tracking averages such as elapsed times, minibatch losses, etc.\n    """"""\n\n    def __init__(self):\n        self.reset()\n\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0.\n        self.sum = 0\n        self.count = 0\n\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n'"
SSD_pytorch/data/__init__.py,2,"b'from .voc0712 import VOCDetection, VOCAnnotationTransform, VOC_CLASSES, VOC_ROOT\nimport torch\nimport cv2\nimport numpy as np\n\ndef detection_collate(batch):\n    """"""Custom collate fn for dealing with batches of images that have a different\n    number of associated object annotations (bounding boxes).\n\n    Arguments:\n        batch: (tuple) A tuple of tensor images and lists of annotations\n\n    Return:\n        A tuple containing:\n            1) (tensor) batch of images stacked on their 0 dim\n            2) (list of tensors) annotations for a given image are stacked on\n                                 0 dim\n    """"""\n    targets = []\n    imgs = []\n    for sample in batch:\n        imgs.append(sample[0])\n        targets.append(torch.FloatTensor(sample[1]))\n    return torch.stack(imgs, 0), targets\n\n\ndef base_transform(image, size, mean):\n    x = cv2.resize(image, (size, size)).astype(np.float32)\n    x -= mean\n    x = x.astype(np.float32)\n    return x\n\n\nclass BaseTransform:\n    def __init__(self, size, mean):\n        self.size = size\n        self.mean = np.array(mean, dtype=np.float32)\n\n    def __call__(self, image, boxes=None, labels=None):\n        return base_transform(image, self.size, self.mean), boxes, labels\n'"
SSD_pytorch/data/voc0712.py,5,"b'""""""VOC Dataset Classes\n\nOriginal author: Francisco Massa\nhttps://github.com/fmassa/vision/blob/voc_dataset/torchvision/datasets/voc.py\n\nUpdated by: Ellis Brown, Max deGroot\n""""""\nfrom SSD_pytorch.utils.config import opt\nimport os.path as osp\nimport sys\nimport torch\nimport torch.utils.data as data\nimport cv2\nimport numpy as np\nif sys.version_info[0] == 2:\n    import xml.etree.cElementTree as ET\nelse:\n    import xml.etree.ElementTree as ET\n\nVOC_CLASSES = (  # always index 0\n    \'aeroplane\', \'bicycle\', \'bird\', \'boat\',\n    \'bottle\', \'bus\', \'car\', \'cat\', \'chair\',\n    \'cow\', \'diningtable\', \'dog\', \'horse\',\n    \'motorbike\', \'person\', \'pottedplant\',\n    \'sheep\', \'sofa\', \'train\', \'tvmonitor\')\n\nVOC_ROOT = opt.voc_data_root\n\n\nclass VOCAnnotationTransform(object):\n    """"""Transforms a VOC annotation into a Tensor of bbox coords and label index\n    Initilized with a dictionary lookup of classnames to indexes\n\n    \xe5\xb0\x86VOC\xe6\xb3\xa8\xe9\x87\x8a\xe8\xbd\xac\xe6\x8d\xa2\xe4\xb8\xba\xe7\x9c\x9f\xe5\x80\xbc\xe6\xa1\x86bbox\xe5\x9d\x90\xe6\xa0\x87 \xe5\x92\x8c \xe6\xa0\x87\xe7\xad\xbe\xe7\xb4\xa2\xe5\xbc\x95\xe7\x9a\x84tensor\n\xc2\xa0\xc2\xa0\xe7\x94\xa8\xe7\xb4\xa2\xe5\xbc\x95\xe7\x9a\x84\xe5\xad\x97\xe5\x85\xb8\xe5\x90\x8d\xe7\xa7\xb0\xe7\x9a\x84dict\xe6\x9f\xa5\xe6\x89\xbe\n\n    Arguments:\n        class_to_ind (dict, optional): dictionary lookup of classnames -> indexes\n            (default: alphabetic indexing of VOC\'s 20 classes)\n        keep_difficult (bool, optional): keep difficult instances or not\n            (default: False)\n        height (int): height\n        width (int): width\n    """"""\n\n    def __init__(self, class_to_ind=None, keep_difficult=False):\n        self.class_to_ind = class_to_ind or dict(\n            zip(VOC_CLASSES, range(len(VOC_CLASSES))))\n        self.keep_difficult = keep_difficult\n\n    def __call__(self, target, width, height):\n        """"""\n        Arguments:\n            target (annotation) : the target annotation to be made usable\n                will be an ET.Element\n        Returns:\n            a list containing lists of bounding boxes  [bbox coords, class name]\n            \xe4\xb8\x80\xe4\xb8\xaa\xe5\x8c\x85\xe5\x90\xabbbox\xe5\x9d\x90\xe6\xa0\x87 \xe5\x92\x8c \xe7\xb1\xbb\xe5\x90\x8d\xe7\x9a\x84 list\n        """"""\n        res = []\n        # \xe9\x81\x8d\xe5\x8e\x86\xe8\xbf\x99\xe5\xbc\xa0\xe5\x9b\xbe\xe7\x9a\x84\xe6\x89\x80\xe6\x9c\x89\xe7\x89\xa9\xe4\xbd\x93\n        for obj in target.iter(\'object\'):\n            difficult = int(obj.find(\'difficult\').text) == 1\n            if not self.keep_difficult and difficult:\n                continue\n            name = obj.find(\'name\').text.lower().strip()\n            bbox = obj.find(\'bndbox\')\n\n            pts = [\'xmin\', \'ymin\', \'xmax\', \'ymax\']\n            bndbox = []\n            for i, pt in enumerate(pts):\n                cur_pt = int(bbox.find(pt).text) - 1\n                # scale height or width\n                cur_pt = cur_pt / width if i % 2 == 0 else cur_pt / height\n                bndbox.append(cur_pt)\n            label_idx = self.class_to_ind[name]\n            bndbox.append(label_idx)\n            res += [bndbox]  # [xmin, ymin, xmax, ymax, label_ind]\n            # img_id = target.find(\'filename\').text[:-4]\n\n        return res  # \xe5\xbd\xa2\xe7\x8a\xb6\xe5\xbd\xa2\xe5\xa6\x82[[xmin, ymin, xmax, ymax, label_ind], ... ]\n\n\nclass VOCDetection(data.Dataset):\n    """"""VOC Detection Dataset Object\n    VOC\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xef\xbc\x8c\xe7\xbb\xa7\xe6\x89\xbfdata.Dataset\xef\xbc\x8c\xe9\x9c\x80\xe5\xae\x9e\xe7\x8e\xb0__getitem__\xe3\x80\x81__len__\xe6\x96\xb9\xe6\xb3\x95\n\n    input is image, target is annotation\n\n    Arguments:\n        root (string): filepath to VOCdevkit folder.\n                       \xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe6\xa0\xb9\xe7\x9b\xae\xe5\xbd\x95\n        image_set (string): imageset to use (eg. \'train\', \'val\', \'test\')\n                        \xe8\xae\xad\xe7\xbb\x83or \xe9\xaa\x8c\xe8\xaf\x81or \xe6\xb5\x8b\xe8\xaf\x95\n        transform (callable, optional): transformation to perform on the\n            input image\n                          \xe5\x9b\xbe\xe5\x83\x8f\xe8\xbd\xac\xe5\x8c\x96\xe5\x92\x8c\xe6\x95\xb0\xe6\x8d\xae\xe5\xa2\x9e\xe5\xbc\xba\n        target_transform (callable, optional): transformation to perform on the\n            target `annotation`\n            (eg: take in caption string, return tensor of word indices)\n        dataset_name (string, optional): which dataset to load\n            (default: \'VOC2007\')\n    """"""\n\n    def __init__(self, root,\n                 image_sets=[(\'2007\', \'trainval\'), (\'2012\', \'trainval\')],\n                 transform=None, target_transform=VOCAnnotationTransform(),\n                 dataset_name=\'VOC0712\'):\n        self.root = root\n        self.image_set = image_sets\n        self.transform = transform   #SSDAugmentation(cfg[\'min_dim\'],MEANS))  \xe5\x9b\xbe\xe5\x83\x8f\xe5\xa2\x9e\xe5\xbc\xba\n        self.target_transform = target_transform   #VOCAnnotationTransform()  \xe6\xb3\xa8\xe9\x87\x8a\xe5\x8f\x98\xe6\x8d\xa2\n        self.name = dataset_name\n        self._annopath = osp.join(\'%s\', \'Annotations\', \'%s.xml\')  #\xe8\xaf\xbb\xe5\x8f\x96\xe6\x89\x80\xe6\x9c\x89xml\xe6\x96\x87\xe4\xbb\xb6\n        self._imgpath = osp.join(\'%s\', \'JPEGImages\', \'%s.jpg\')    #\xe8\xaf\xbb\xe5\x8f\x96\xe6\x89\x80\xe6\x9c\x89jpg\xe6\x96\x87\xe4\xbb\xb6\n        self.ids = list()   #\xe5\x9b\xbe\xe5\x83\x8f\xe7\x9a\x84id\xe5\x85\xa8\xe9\x83\xa8\xe4\xbf\x9d\xe5\xad\x98\xe5\x9c\xa8ids\n        # 2007\xe5\x92\x8c2012\xe7\x9a\x84\xe8\xae\xad\xe7\xbb\x83\xe9\xaa\x8c\xe8\xaf\x81\xe9\x9b\x86\n        for (year, name) in image_sets:\n            rootpath = osp.join(self.root, \'VOC\' + year)\n            for line in open(osp.join(rootpath, \'ImageSets\', \'Main\', name + \'.txt\')):\n                self.ids.append((rootpath, line.strip()))\n\n    def __getitem__(self, index):\n        \'\'\'\n        :param index: \xe5\x8f\x96\xe7\xac\xac\xe5\x87\xa0\xe6\x9d\xa1\xe6\x95\xb0\xe6\x8d\xae\n        :return: \xe4\xb8\x80\xe5\xbc\xa0\xe5\x9b\xbe\xe5\x83\x8f\xe5\x8f\x8a\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84\xe7\x9c\x9f\xe5\x80\xbc\xe6\xa1\x86\xe5\x92\x8c\xe7\xb1\xbb\xe5\x88\xab\n        \'\'\'\n        im, gt, h, w = self.pull_item(index)\n        return im, gt\n\n    def __len__(self):\n        return len(self.ids)\n\n    def pull_item(self, index):\n        \'\'\'\n        \xe5\x8f\x96\xe6\x9f\x90\xe6\x9d\xa1\xe6\x95\xb0\xe6\x8d\xae\n        :param index: \xe5\x8f\x96\xe7\xac\xac\xe5\x87\xa0\xe6\x9d\xa1\xe6\x95\xb0\xe6\x8d\xae\n        :return: \xe4\xb8\x80\xe5\xbc\xa0\xe5\x9b\xbe\xe5\x83\x8f\xe3\x80\x81\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84\xe7\x9c\x9f\xe5\x80\xbc\xe6\xa1\x86\xe3\x80\x81\xe9\xab\x98\xe3\x80\x81\xe5\xae\xbd\n        \'\'\'\n        img_id = self.ids[index]\n        # \xe8\xaf\xbb\xe5\x8f\x96\xe5\x9b\xbe\xe5\x83\x8f\xe4\xb8\x8e\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84xml\n        target = ET.parse(self._annopath % img_id).getroot()\n        img = cv2.imread(self._imgpath % img_id)\n        # \xe5\xbe\x97\xe5\x88\xb0\xe5\x9b\xbe\xe5\x83\x8f\xe7\x9a\x84\xe9\xab\x98\xe3\x80\x81\xe5\xae\xbd\xe3\x80\x81\xe9\x80\x9a\xe9\x81\x93\xef\xbc\x88\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe4\xb8\xad\xe9\xab\x98\xe5\xae\xbd\xe4\xb8\x8d\xe4\xb8\x80\xe5\xae\x9a\xef\xbc\x89\n        height, width, channels = img.shape\n        # VOCAnnotationTransform()  \xe6\xb3\xa8\xe9\x87\x8a\xe5\x8f\x98\xe6\x8d\xa2\xef\xbc\x88\xe8\xa7\xa3\xe6\x9e\x90xml\xef\xbc\x8c\xe8\xbf\x94\xe5\x9b\x9e\xe4\xb8\x80\xe4\xb8\xaalist  \xe5\x8c\x85\xe5\x90\xab\xe6\x89\x80\xe6\x9c\x89\xe5\xaf\xb9\xe8\xb1\xa1\xe7\x9a\x84bbox\xe5\x9d\x90\xe6\xa0\x87\xe4\xb8\x8e\xe7\xb1\xbb\xe5\x90\x8d\xef\xbc\x89\n        if self.target_transform is not None:\n            target = self.target_transform(target, width, height)\n        # SSDAugmentation(cfg[\'min_dim\'],MEANS))  \xe5\x9b\xbe\xe5\x83\x8f\xe5\xa2\x9e\xe5\xbc\xba\n        if self.transform is not None:\n            # \xe8\xbd\xac\xe5\x8c\x96\xe4\xb8\xbatensor  \xe5\xbd\xa2\xe7\x8a\xb6\xe4\xb8\xba\xef\xbc\x88x,5\xef\xbc\x89  x:\xe5\x9b\xbe\xe5\x83\x8f\xe4\xb8\xad\xe7\x9a\x84\xe7\x89\xa9\xe4\xbd\x93\xe6\x80\xbb\xe6\x95\xb0   5\xef\xbc\x9abbox\xe5\x9d\x90\xe6\xa0\x87\xe3\x80\x81 \xe7\xb1\xbb\xe5\x88\xab\n            target = np.array(target)\n            # \xe5\x9b\xbe\xe5\x83\x8f\xe5\xa2\x9e\xe5\xbc\xba\n            img, boxes, labels = self.transform(img, target[:, :4], target[:, 4])\n            # to rgb  \xe8\xbd\xac\xe5\x8c\x96\xe4\xb8\xbargb\n            img = img[:, :, (2, 1, 0)]\n            # img = img.transpose(2, 0, 1)\n            # hstack\xe5\x90\x88\xe5\xb9\xb6  axis=1\xe6\x8c\x89\xe7\x85\xa7\xe5\x88\x97\xe5\x90\x88\xe5\xb9\xb6   target\xef\xbc\x9a\xe4\xb8\x80\xe8\xa1\x8c\xe5\x86\x85\xe5\xae\xb9\xe6\x98\xafboxes\xe5\x9d\x90\xe6\xa0\x87+\xe7\xb1\xbb\xe5\x88\xab\n            target = np.hstack((boxes, np.expand_dims(labels, axis=1)))\n        # \xe4\xb8\x80\xe5\xbc\xa0\xe5\x9b\xbe\xe5\x83\x8f\xe3\x80\x81\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84\xe7\x9c\x9f\xe5\x80\xbc\xe6\xa1\x86\xe5\x92\x8c\xe7\xb1\xbb\xe5\x88\xab\xe3\x80\x81\xe9\xab\x98\xe3\x80\x81\xe5\xae\xbd\n        return torch.from_numpy(img).permute(2, 0, 1), target, height, width\n        # return torch.from_numpy(img), target, height, width\n\n    def pull_image(self, index):\n        \'\'\'Returns the original image object at index in PIL form\n\n        Note: not using self.__getitem__(), as any transformations passed in\n        could mess up this functionality.\n\n        Argument:\n            index (int): index of img to show\n        Return:\n            PIL img\n        \'\'\'\n        img_id = self.ids[index]\n        return cv2.imread(self._imgpath % img_id, cv2.IMREAD_COLOR)\n\n    def pull_anno(self, index):\n        \'\'\'Returns the original annotation of image at index\n\n        Note: not using self.__getitem__(), as any transformations passed in\n        could mess up this functionality.\n\n        Argument:\n            index (int): index of img to get annotation of\n        Return:\n            list:  [img_id, [(label, bbox coords),...]]\n                eg: (\'001718\', [(\'dog\', (96, 13, 438, 332))])\n        \'\'\'\n        img_id = self.ids[index]\n        anno = ET.parse(self._annopath % img_id).getroot()\n        gt = self.target_transform(anno, 1, 1)\n        return img_id[1], gt\n\n    def pull_tensor(self, index):\n        \'\'\'Returns the original image at an index in tensor form\n\n        Note: not using self.__getitem__(), as any transformations passed in\n        could mess up this functionality.\n\n        Argument:\n            index (int): index of img to show\n        Return:\n            tensorized version of img, squeezed\n        \'\'\'\n        return torch.Tensor(self.pull_image(index)).unsqueeze_(0)\n'"
SSD_pytorch/models/__init__.py,0,b'from .functions import *\nfrom .modules import *\n'
SSD_pytorch/models/box_utils.py,23,"b'# -*- coding: utf-8 -*-\nimport torch\n\n\ndef point_form(boxes):\n    """""" Convert prior_boxes to (xmin, ymin, xmax, ymax)\n    representation for comparison to point form ground truth data.\n\n    \xe5\xb0\x86\xe9\xa2\x84\xe5\x85\x88\xe7\x94\x9f\xe6\x88\x90\xe7\x9a\x84\xe9\x94\x9a\xe5\x9d\x90\xe6\xa0\x87\xe7\x94\xb1 \xe4\xb8\xad\xe5\xbf\x83\xe7\x82\xb9\xe5\x9d\x90\xe6\xa0\x87\xe5\x92\x8c\xe5\xae\xbd\xe9\xab\x98 \xe8\xbd\xac\xe5\x8c\x96\xe4\xb8\xba (xmin, ymin, xmax, ymax)\xe5\xbd\xa2\xe5\xbc\x8f\n    Args:\n        boxes: (tensor) center-size default boxes from priorbox layers.\n    Return:\n        boxes: (tensor) Converted xmin, ymin, xmax, ymax form of boxes.\n    """"""\n    #\xe6\x8c\x89\xe7\x85\xa7\xe7\xba\xb5\xe5\x9d\x90\xe6\xa0\x87\xe5\xb0\x86\xe7\xbb\x93\xe6\x9e\x9c\xe5\x90\x88\xe5\xb9\xb6.\n    # \xe4\xb8\xad\xe5\xbf\x83\xe7\x82\xb9\xe5\x9d\x90\xe6\xa0\x87-\xef\xbc\x88w,h\xef\xbc\x89*\xef\xbc\x881/2\xef\xbc\x89\xe5\x8d\xb3\xe4\xb8\xba\xe5\xb7\xa6\xe4\xb8\x8a\xe8\xa7\x92\xe5\x9d\x90\xe6\xa0\x87xmin, ymin\n    # \xe4\xb8\xad\xe5\xbf\x83\xe7\x82\xb9\xe5\x9d\x90\xe6\xa0\x87+\xef\xbc\x88w,h\xef\xbc\x89*\xef\xbc\x881/2\xef\xbc\x89\xe5\x8d\xb3\xe4\xb8\xba\xe5\x8f\xb3\xe4\xb8\x8b\xe8\xa7\x92\xe5\x9d\x90\xe6\xa0\x87xmax, ymax\n    return torch.cat((boxes[:, :2] - boxes[:, 2:]/2,     # xmin, ymin\n                     boxes[:, :2] + boxes[:, 2:]/2), 1)  # xmax, ymax\n\n\ndef center_size(boxes):\n    """""" Convert prior_boxes to (cx, cy, w, h)\n    representation for comparison to center-size form ground truth data.\n    Args:\n        boxes: (tensor) point_form boxes\n    Return:\n        boxes: (tensor) Converted xmin, ymin, xmax, ymax form of boxes.\n    """"""\n    return torch.cat((boxes[:, 2:] + boxes[:, :2])/2,  # cx, cy\n                     boxes[:, 2:] - boxes[:, :2], 1)  # w, h\n\n\ndef intersect(box_a, box_b):\n    """""" We resize both tensors to [A,B,2] without new malloc:\n    [A,2] -> [A,1,2] -> [A,B,2]\n    [B,2] -> [1,B,2] -> [A,B,2]\n    Then we compute the area of intersect between box_a and box_b.\n    Args:\n      box_a: (tensor) bounding boxes, Shape: [A,4].\n      box_b: (tensor) bounding boxes, Shape: [B,4].\n    Return:\n      (tensor) intersection area, Shape: [A,B].\n    """"""\n    A = box_a.size(0)\n    B = box_b.size(0)\n    max_xy = torch.min(box_a[:, 2:].unsqueeze(1).expand(A, B, 2),\n                       box_b[:, 2:].unsqueeze(0).expand(A, B, 2))\n    min_xy = torch.max(box_a[:, :2].unsqueeze(1).expand(A, B, 2),\n                       box_b[:, :2].unsqueeze(0).expand(A, B, 2))\n    inter = torch.clamp((max_xy - min_xy), min=0)\n    return inter[:, :, 0] * inter[:, :, 1]\n\n\ndef jaccard(box_a, box_b):\n    """"""Compute the jaccard overlap of two sets of boxes.  The jaccard overlap\n    is simply the intersection over union of two boxes.  Here we operate on\n    ground truth boxes and default boxes.\n\n    \xe8\xae\xa1\xe7\xae\x97\xe4\xb8\xa4\xe7\xbb\x84\xe7\x9b\x92\xe5\xad\x90\xe4\xb8\xa4\xe4\xb8\xa4\xe7\x9a\x84jaccard\xe9\x87\x8d\xe5\x8f\xa0\xef\xbc\x8c\xe5\x8d\xb3IOU\xe9\x87\x8d\xe5\x8f\xa0\xe7\x8e\x87\xe3\x80\x82 jaccard\xe9\x87\x8d\xe5\x8f\xa0\xe5\x8f\xaa\xe6\x98\xaf\xe4\xb8\xa4\xe4\xb8\xaa\xe7\x9b\x92\xe5\xad\x90\xe7\x9a\x84\xe8\x81\x94\xe5\x90\x88\xe4\xba\xa4\xe5\x8f\x89\xe3\x80\x82\n    \xe6\x88\x91\xe4\xbb\xac\xe5\x9c\xa8\xe6\xad\xa4\xe8\xbf\x9b\xe8\xa1\x8c\xe6\x93\x8d\xe4\xbd\x9c\xe5\x9c\xb0\xe9\x9d\xa2\xe5\xae\x9e\xe5\x86\xb5\xe6\xa1\x86\xe5\x92\x8c\xe9\xbb\x98\xe8\xae\xa4\xe6\xa1\x86\n    E.g.:\n        A \xe2\x88\xa9 B / A \xe2\x88\xaa B = A \xe2\x88\xa9 B / (area(A) + area(B) - A \xe2\x88\xa9 B)\n    Args:\n        box_a: (tensor) Ground truth bounding boxes, Shape: [num_objects,4]\n        box_b: (tensor) Prior boxes from priorbox layers, Shape: [num_priors,4]\n    Return:\n        jaccard overlap: (tensor) Shape: [box_a.size(0), box_b.size(0)]\n    """"""\n    inter = intersect(box_a, box_b)\n    area_a = ((box_a[:, 2]-box_a[:, 0]) *\n              (box_a[:, 3]-box_a[:, 1])).unsqueeze(1).expand_as(inter)  # [A,B]\n    area_b = ((box_b[:, 2]-box_b[:, 0]) *\n              (box_b[:, 3]-box_b[:, 1])).unsqueeze(0).expand_as(inter)  # [A,B]\n    union = area_a + area_b - inter\n    return inter / union  # [A,B]\n\n\ndef match(threshold, truths, priors, variances, labels, loc_t, conf_t, idx):\n    """"""Match each prior box with the ground truth box of the highest jaccard\n    overlap, encode the bounding boxes, then return the matched indices\n    corresponding to both confidence and location preds.\n    \xe5\xb0\x86\xe6\xaf\x8f\xe4\xb8\xaa\xe9\xa2\x84\xe6\xb5\x8b\xe6\xa1\x86\xe4\xb8\x8e\xe6\x9c\x80\xe9\xab\x98IOU\xe7\x9a\x84\xe7\x9c\x9f\xe5\xae\x9e\xe6\xa1\x86\xe7\x9b\xb8\xe5\x8c\xb9\xe9\x85\x8d\xef\xbc\x8c\xe5\xaf\xb9\xe8\xbe\xb9\xe7\x95\x8c\xe6\xa1\x86\xe8\xbf\x9b\xe8\xa1\x8c\xe7\xbc\x96\xe7\xa0\x81\xef\xbc\x8c\xe7\x84\xb6\xe5\x90\x8e\xe8\xbf\x94\xe5\x9b\x9e\xe5\x8c\xb9\xe9\x85\x8d\xe7\x9a\x84\xe7\xb4\xa2\xe5\xbc\x95\n\xc2\xa0\xc2\xa0\xe5\xaf\xb9\xe5\xba\x94\xe4\xba\x8e\xe7\xbd\xae\xe4\xbf\xa1\xe5\xba\xa6\xe5\x92\x8c\xe4\xbd\x8d\xe7\xbd\xae\xe9\xa2\x84\xe6\xb5\x8b\xe3\x80\x82\n    Args:\n        threshold: (float) The overlap threshold used when mathing boxes.  0.5\n        truths: (tensor) Ground truth boxes, Shape: [num_obj, num_priors].  \xe7\x9c\x9f\xe5\x80\xbc\xe6\xa1\x86[\xe4\xb8\x80\xe5\xbc\xa0\xe5\x9b\xbe\xe7\x89\x87\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84]\n        priors: (tensor) Prior boxes from priorbox layers, Shape: [n_priors,4].  \xe9\x94\x9a\xe6\xa1\x86\xe5\x9d\x90\xe6\xa0\x87[8732,4]\n        variances: (tensor) Variances corresponding to each prior coord,  \xe5\xaf\xb9\xe5\xba\x94\xe4\xba\x8e\xe6\xaf\x8f\xe4\xb8\xaa\xe9\x94\x9a\xe6\xa1\x86\xe7\x9a\x84\xe6\x96\xb9\xe5\xb7\xae [8732,4]\n            Shape: [num_priors, 4].\n        labels: (tensor) All the class labels for the image, Shape: [num_obj].   \xe5\xaf\xb9\xe4\xba\x8e\xe8\xaf\xa5\xe5\x9b\xbe\xe5\x83\x8f\xe4\xb8\xad\xe7\x89\xa9\xe4\xbd\x93\xe7\x9a\x84\xe7\xb1\xbb\xe5\x88\xab\xe6\xa0\x87\xe7\xad\xbe\xe7\x9c\x9f\xe5\x80\xbc[\xe7\x89\xa9\xe4\xbd\x93\xe6\x95\xb0]\n        loc_t: (tensor) Tensor to be filled w/ endcoded location targets.  \xe4\xbf\x9d\xe5\xad\x98 \xe7\xbc\x96\xe7\xa0\x81\xe5\x90\x8e\xe7\x9a\x84\xe5\x9b\x9e\xe5\xbd\x92\xe7\x9b\xae\xe6\xa0\x87\n        conf_t: (tensor) Tensor to be filled w/ matched indices for conf preds.   \xe4\xbf\x9d\xe5\xad\x98 \xe5\x8c\xb9\xe9\x85\x8d\xe5\xa5\xbd\xe7\x9a\x84\n        idx: (int) current batch index   batch\xe4\xb8\xad\xe5\xbd\x93\xe5\x89\x8d\xe7\xac\xacidx\xe5\xbc\xa0\xe5\x9b\xbe\xe7\x89\x87\n    Return:\n        The matched indices corresponding to 1)location and 2)confidence preds.\n        \xe5\x8c\xb9\xe9\x85\x8d\xe7\x9a\x84\xe7\xb4\xa2\xe5\xbc\x95\xe5\xaf\xb9\xe5\xba\x94\xe4\xba\x8e  1\xef\xbc\x89\xe4\xbd\x8d\xe7\xbd\xae  \xe5\x92\x8c  2\xef\xbc\x89\xe7\xbd\xae\xe4\xbf\xa1\xe5\xba\xa6\xe3\x80\x82\n    """"""\n    # jaccard index\n    #\xe8\x8b\xa5truths[2,4] priors[8732,4]\xef\xbc\x8c\xe5\x88\x99\xe7\xbb\x93\xe6\x9e\x9c\xe4\xb8\xba[2,8732]\n    #\xe8\xae\xa1\xe7\xae\x97\xe7\x9c\x9f\xe5\x80\xbc\xe6\xa1\x86\xe4\xb8\x8e\xe9\x94\x9a\xe4\xb9\x8b\xe9\x97\xb4 \xe4\xb8\xa4\xe4\xb8\xa4\xe7\x9a\x84\xe9\x87\x8d\xe5\x8f\xa0\xe7\x8e\x87IOU\n    overlaps = jaccard(\n        truths,\n        # \xe5\xb0\x86\xe9\xa2\x84\xe5\x85\x88\xe7\x94\x9f\xe6\x88\x90\xe7\x9a\x84\xe9\x94\x9a\xe5\x9d\x90\xe6\xa0\x87\xe7\x94\xb1\xe4\xb8\xad\xe5\xbf\x83\xe7\x82\xb9\xe5\x9d\x90\xe6\xa0\x87\xe5\x92\x8c\xe5\xae\xbd\xe9\xab\x98\xe8\xbd\xac\xe5\x8c\x96\xe4\xb8\xba(xmin, ymin, xmax, ymax)\xe5\xbd\xa2\xe5\xbc\x8f\n        point_form(priors)\n    )\n\n    #\xe5\x8e\x9f\xe8\xae\xba\xe6\x96\x87\xe7\x9a\x84\xe5\x8c\xb9\xe9\x85\x8d\xe6\x96\xb9\xe6\xb3\x95:\n    #1\xe3\x80\x81\xe7\x94\xa8 MultiBox \xe4\xb8\xad\xe7\x9a\x84 best jaccard overlap \xe6\x9d\xa5\xe5\x8c\xb9\xe9\x85\x8d\xe6\xaf\x8f\xe4\xb8\x80\xe4\xb8\xaa ground truth box \xe4\xb8\x8e default box\xef\xbc\x8c\xe8\xbf\x99\xe6\xa0\xb7\xe5\xb0\xb1\xe8\x83\xbd\xe4\xbf\x9d\xe8\xaf\x81\xe6\xaf\x8f\xe4\xb8\x80\xe4\xb8\xaa groundtruth box \xe4\xb8\x8e\xe5\x94\xaf\xe4\xb8\x80\xe7\x9a\x84\xe4\xb8\x80\xe4\xb8\xaa default box \xe5\xaf\xb9\xe5\xba\x94\xe8\xb5\xb7\xe6\x9d\xa5\n    #2\xe3\x80\x81\xe5\x8f\x88\xe4\xb8\x8d\xe5\x90\x8c\xe4\xba\x8e MultiBox \xef\xbc\x8c\xe5\xb0\x86 default box \xe4\xb8\x8e\xe4\xbb\xbb\xe4\xbd\x95\xe7\x9a\x84 groundtruth box \xe9\x85\x8d\xe5\xaf\xb9\xef\xbc\x8c\xe5\x8f\xaa\xe8\xa6\x81\xe4\xb8\xa4\xe8\x80\x85\xe4\xb9\x8b\xe9\x97\xb4\xe7\x9a\x84 jaccard overlap \xe5\xa4\xa7\xe4\xba\x8e\xe4\xb8\x80\xe4\xb8\xaa\xe9\x98\x88\xe5\x80\xbc\xef\xbc\x8c\xe8\xbf\x99\xe9\x87\x8c\xe6\x9c\xac\xe6\x96\x87\xe7\x9a\x84\xe9\x98\x88\xe5\x80\xbc\xe4\xb8\xba 0.5\n\n    # (Bipartite Matching)\xe4\xba\x8c\xe5\x88\x86\xe5\x8c\xb9\xe9\x85\x8d.\n    # [1,num_objects] best prior for each ground truth\n    #\xe5\xaf\xb9\xe4\xba\x8e\xe6\xaf\x8f\xe4\xb8\xaa\xe7\x9c\x9f\xe5\x80\xbc\xe6\xa1\x86\xef\xbc\x8c\xe6\x89\xbe\xe5\x87\xba \xe4\xb8\x8e\xe5\x85\xb6\xe5\x8c\xb9\xe9\x85\x8dIOU\xe6\x9c\x80\xe9\xab\x98\xe7\x9a\x84\xe9\x94\x9a\xe6\xa1\x86  \xef\xbc\x88\xe7\x9b\xb8\xe5\xaf\xb9\xe4\xba\x8eoverlaps\xef\xbc\x8c\xe6\x89\xbe\xe5\x87\xba\xe6\xaf\x8f\xe4\xb8\x80\xe8\xa1\x8c\xe6\x9c\x80\xe5\xa4\xa7\xe7\x9a\x84\xe5\x80\xbc\xef\xbc\x8c\xe8\xbf\x94\xe5\x9b\x9e\xe8\xaf\xa5\xe5\x80\xbc\xe5\x8f\x8a\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84\xe5\xba\x8f\xe5\x8f\xb7\xef\xbc\x89\n    #best_prior_overlap\xe4\xb8\xba \xe4\xb8\x8e\xe7\x9c\x9f\xe5\x80\xbc\xe6\xa1\x86\xe5\x8c\xb9\xe9\x85\x8dIOU\xe6\x9c\x80\xe9\xab\x98\xe7\x9a\x84\xe5\x80\xbc\xe3\x80\x82best_prior_idx\xe4\xb8\xba  \xe5\xaf\xb9\xe5\xba\x94\xe5\xba\x8f\xe5\x8f\xb7\xe3\x80\x82 \xe5\xbd\xa2\xe7\x8a\xb6 [1,num_objects]\n    best_prior_overlap, best_prior_idx = overlaps.max(1, keepdim=True)\n    # [1,num_priors] best ground truth for each prior\n    # \xe5\xaf\xb9\xe4\xba\x8e\xe6\xaf\x8f\xe4\xb8\x80\xe4\xb8\xaa\xe9\x94\x9a\xe6\xa1\x86\xef\xbc\x8c\xe6\x89\xbe\xe5\x87\xba \xe4\xb8\x8e\xe5\x85\xb6\xe5\x8c\xb9\xe9\x85\x8dIOU\xe6\x9c\x80\xe9\xab\x98\xe7\x9a\x84\xe7\x9c\x9f\xe5\x80\xbc\xe6\xa1\x86  \xef\xbc\x88\xe7\x9b\xb8\xe5\xaf\xb9\xe4\xba\x8eoverlaps\xef\xbc\x8c\xe6\x89\xbe\xe5\x87\xba\xe6\xaf\x8f\xe4\xb8\x80\xe5\x88\x97\xe6\x9c\x80\xe5\xa4\xa7\xe7\x9a\x84\xe5\x80\xbc\xef\xbc\x8c\xe8\xbf\x94\xe5\x9b\x9e\xe8\xaf\xa5\xe5\x80\xbc\xe5\x8f\x8a\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84\xe5\xba\x8f\xe5\x8f\xb7\xef\xbc\x89\n    best_truth_overlap, best_truth_idx = overlaps.max(0, keepdim=True)\n    # \xe5\x8e\x8b\xe7\xbc\xa9tensor\n    best_truth_idx.squeeze_(0)\n    best_truth_overlap.squeeze_(0)\n    best_prior_idx.squeeze_(1)\n    best_prior_overlap.squeeze_(1)\n\n    #pytorch\xe6\x96\xb9\xe6\xb3\x95  index_fill_(dim, index, val)\n    # \xe6\x8c\x89\xe5\x8f\x82\xe6\x95\xb0 index \xe7\xbb\x99\xe5\x87\xba\xe7\x9a\x84\xe7\xb4\xa2\xe5\xbc\x95\xe5\xba\x8f\xe5\x88\x97, \xe5\xb0\x86\xe5\x8e\x9f tensor \xe4\xb8\xad\xe7\x9a\x84\xe5\x85\x83\xe7\xb4\xa0\xe7\x94\xa8 val \xe5\xa1\xab\xe5\x85\x85\n    #dim (int) \xe2\x80\x93 \xe7\xb4\xa2\xe5\xbc\x95 index \xe6\x89\x80\xe6\x8c\x87\xe5\x90\x91\xe7\x9a\x84\xe7\xbb\xb4\xe5\xba\xa6\n    #index (LongTensor) \xe2\x80\x93 \xe4\xbb\x8e\xe5\x8f\x82\xe6\x95\xb0 val \xe4\xb8\xad\xe9\x80\x89\xe5\x8f\x96\xe6\x95\xb0\xe6\x8d\xae\xe7\x9a\x84\xe7\xb4\xa2\xe5\xbc\x95\xe5\xba\x8f\xe5\x88\x97\n    #\xe5\x8d\xb3\xe5\xb0\x86best_truth_overlap\xe7\x9f\xa9\xe9\x98\xb5\xe4\xb8\xad \xe6\x8c\x89\xe7\x85\xa7\xe6\xaf\x8f\xe5\x88\x97\xe4\xb8\xad\xef\xbc\x8c\xe5\xba\x8f\xe5\x8f\xb7\xe5\x8c\x85\xe5\x90\xab\xe5\x9c\xa8best_prior_idx\xe4\xb8\xad\xe7\x9a\x84\xe5\x80\xbc\xe9\x83\xbd\xe8\xa2\xab\xe6\x9b\xbf\xe6\x8d\xa2\xe4\xb8\xba2\n    best_truth_overlap.index_fill_(0, best_prior_idx, 2)  # ensure best prior\n    # TODO refactor: index  best_prior_idx with long tensor\n    # ensure every gt matches with its prior of max overlap\n    # \xe7\xa1\xae\xe4\xbf\x9d\xe6\xaf\x8f\xe4\xb8\xaa\xe7\x9c\x9f\xe5\x80\xbc\xe6\xa1\x86 \xe5\x8c\xb9\xe9\x85\x8d\xe5\x88\xb0\xe4\xb8\x8e\xe5\x85\xb6IOU\xe6\x9c\x80\xe5\xa4\xa7\xe7\x9a\x84\xe9\x94\x9a\n    for j in range(best_prior_idx.size(0)):\n        best_truth_idx[best_prior_idx[j]] = j\n    matches = truths[best_truth_idx]          # \xe5\xaf\xb9\xe4\xba\x8e\xe6\xaf\x8f\xe4\xb8\xaa\xe9\x94\x9a\xe6\xa1\x86\xef\xbc\x8c\xe5\x8c\xb9\xe9\x85\x8d\xe5\x88\xb0\xe7\x9a\x84\xe7\x9c\x9f\xe5\x80\xbc\xe6\xa1\x86\xe7\x9a\x84\xe5\x9d\x90\xe6\xa0\x87Shape: [num_priors,4]\n    conf = labels[best_truth_idx] + 1         # Shape: [num_priors]\n    # \xe5\xbe\x97\xe5\x88\xb0\xe4\xb8\x80\xe5\xbc\xa0\xe5\x9b\xbe\xe7\x9a\x84\xe5\x88\x86\xe7\xb1\xbb\xe8\xaf\xaf\xe5\xb7\xae\n    conf[best_truth_overlap < threshold] = 0  # label as background \xe5\xb0\x86IOU\xe5\xb0\x8f\xe4\xba\x8e\xe9\x98\x88\xe5\x80\xbc\xe7\x9a\x84\xe6\xa0\x87\xe7\xad\xbe\xe7\xbd\xae\xe4\xb8\xba0,\xef\xbc\x8c\xe5\x8d\xb3\xe8\x83\x8c\xe6\x99\xaf\n    #\xe5\xbe\x97\xe5\x88\xb0\xe4\xb8\x80\xe5\xbc\xa0\xe5\x9b\xbe\xe7\x9a\x84\xe7\x9c\x9f\xe5\x80\xbc\xe6\xa1\x86\xe4\xb8\x8e\xe9\x94\x9a\xe7\x9a\x84\xe5\x9d\x90\xe6\xa0\x87\xe5\x81\x8f\xe7\xa7\xbb\xe5\x80\xbc[8732, 4]\n    loc = encode(matches, priors, variances)\n    #\xe4\xbf\x9d\xe5\xad\x98\xe4\xb8\x80\xe4\xb8\xaabatch\xe4\xb8\xad\xe7\x9a\x84\xe5\x9d\x90\xe6\xa0\x87\xe5\x81\x8f\xe7\xa7\xbb\xe5\x80\xbc  \xe5\x92\x8c \xe5\x88\x86\xe7\xb1\xbb\xe8\xaf\xaf\xe5\xb7\xae\n    loc_t[idx] = loc    # [num_priors,4] encoded offsets to learn  \xe7\xbc\x96\xe7\xa0\x81\xe5\x8c\xb9\xe9\x85\x8d\xe5\x88\xb0\xe7\x9a\x84\xe7\x9c\x9f\xe5\x80\xbc\xe6\xa1\x86\xe4\xb8\x8e\xe9\x94\x9a\xe7\x9a\x84\xe5\x9d\x90\xe6\xa0\x87\xe5\x81\x8f\xe7\xa7\xbb\xef\xbc\x8c\xe4\xbb\xa5\xe4\xbe\xbf\xe7\xbd\x91\xe7\xbb\x9c\xe5\xad\xa6\xe4\xb9\xa0\n    conf_t[idx] = conf  # [num_priors] top class label for each prior   \xe5\xaf\xb9\xe4\xba\x8e\xe6\xaf\x8f\xe4\xb8\xaa\xe9\x94\x9a\xe6\xa1\x86\xe7\x9a\x84\xe5\x89\x8d\xe5\x87\xa0\xe4\xb8\xaa\xe7\xb1\xbb\xe5\x88\xab\xe6\xa0\x87\xe7\xad\xbe\n\n\ndef encode(matched, priors, variances):\n    """"""Encode the variances from the priorbox layers into the ground truth boxes\n    we have matched (based on jaccard overlap) with the prior boxes.\n\n    \xe5\xb0\x86 \xe6\x9d\xa5\xe8\x87\xaa\xe9\x94\x9a\xe6\xa1\x86\xe5\xb1\x82\xe7\x9a\x84\xe6\x96\xb9\xe5\xb7\xae\xe7\xbc\x96\xe7\xa0\x81\xe5\x88\xb0  \xe4\xb8\x8e\xe8\xbf\x99\xe4\xba\x9b\xe9\x94\x9a\xe6\xa1\x86\xe7\x9b\xb8\xe5\x8c\xb9\xe9\x85\x8d\xe5\x88\xb0\xe7\x9a\x84 \xe7\x9c\x9f\xe5\x80\xbc\xe6\xa1\x86\xe4\xb8\xad\n    Args:\n        matched: (tensor) Coords of ground truth for each prior in point-form \xef\xbc\x88\xe5\xaf\xb9\xe4\xba\x8e\xe6\xaf\x8f\xe4\xb8\x80\xe4\xb8\xaa\xe9\x94\x9a\xe6\xa1\x86\xef\xbc\x8c\xe9\x83\xbd\xe5\x8c\xb9\xe9\x85\x8d\xe4\xb8\x80\xe4\xb8\xaaIOU\xe5\xa4\xa7\xe4\xba\x8e\xe9\x98\x88\xe5\x80\xbc\xe7\x9a\x84\xe7\x9c\x9f\xe5\x80\xbc\xe6\xa1\x86\xef\xbc\x89\n            Shape: [num_priors, 4].\n        priors: (tensor) Prior boxes in center-offset form \xef\xbc\x88\xe5\x80\xbc\xe4\xb8\xba\xe4\xb8\xad\xe5\xbf\x83\xe5\x81\x8f\xe7\xa7\xbb\xe5\xbd\xa2\xe5\xbc\x8f\xe7\x9a\x84\xe9\x94\x9a\xe6\xa1\x86\xef\xbc\x89\n            Shape: [num_priors,4].\n        variances: (list[float]) Variances of priorboxes \xef\xbc\x88\xe9\x94\x9a\xe6\xa1\x86\xe7\x9a\x84\xe6\x96\xb9\xe5\xb7\xae\xef\xbc\x89\n    Return:\n        encoded boxes (tensor), Shape: [num_priors, 4]\n    """"""\n\n    #\xe5\x8e\x9f\xe8\xae\xba\xe6\x96\x87\xe4\xb8\xadLloc \xe5\x8d\xb3\xe5\xae\x9a\xe4\xbd\x8d\xe6\x8d\x9f\xe5\xa4\xb1\xe8\xae\xa1\xe7\xae\x97\xe5\x85\xac\xe5\xbc\x8f\n    # dist b/t match center and prior\'s center\n    #\xe5\x8d\xb3\xe4\xb8\xba \xe5\xaf\xb9\xe4\xba\x8e\xe6\xaf\x8f\xe4\xb8\x80\xe4\xb8\xaa\xe9\x94\x9a\xe6\xa1\x86\xef\xbc\x8c\xe9\x83\xbd\xe5\x8c\xb9\xe9\x85\x8d\xe4\xb8\x80\xe4\xb8\xaaIOU\xe5\xa4\xa7\xe4\xba\x8e\xe9\x98\x88\xe5\x80\xbc\xe7\x9a\x84\xe7\x9c\x9f\xe5\x80\xbc\xe6\xa1\x86\xe4\xb8\xad\xe5\xbf\x83\xe7\x82\xb9\xe5\x9d\x90\xe6\xa0\x87- \xe5\x85\xac\xe5\xbc\x8f\xe7\x94\x9f\xe6\x88\x90\xe7\x9a\x84\xe9\x94\x9a\xe6\xa1\x86\n    g_cxcy = (matched[:, :2] + matched[:, 2:])/2 - priors[:, :2]\n    # encode variance\xe7\xbc\x96\xe7\xa0\x81\xe6\x96\xb9\xe5\xb7\xae\n    g_cxcy /= (variances[0] * priors[:, 2:])\n    # match wh / prior wh  \xe8\xae\xa1\xe7\xae\x97 wh/ \xe9\x94\x9a\xe7\x9a\x84 wh,\xe4\xb9\x8b\xe5\x90\x8e\xe5\x86\x8dlog\n    #xmax-xmin=w   ymax-ymin=h\n    g_wh = (matched[:, 2:] - matched[:, :2]) / priors[:, 2:]\n    g_wh = torch.log(g_wh) / variances[1]\n    # return target for smooth_l1_loss\n    return torch.cat([g_cxcy, g_wh], 1)  # [num_priors,4]\n\n\n# Adapted from https://github.com/Hakuyume/chainer-ssd\ndef decode(loc, priors, variances):\n    """"""Decode locations from predictions using priors to undo\n    the encoding we did for offset regression at train time.\n    Args:\n        loc (tensor): location predictions for loc layers,\n            Shape: [num_priors,4]\n        priors (tensor): Prior boxes in center-offset form.\n            Shape: [num_priors,4].\n        variances: (list[float]) Variances of priorboxes\n    Return:\n        decoded bounding box predictions\n    """"""\n\n    boxes = torch.cat((\n        priors[:, :2] + loc[:, :2] * variances[0] * priors[:, 2:],\n        priors[:, 2:] * torch.exp(loc[:, 2:] * variances[1])), 1)\n    boxes[:, :2] -= boxes[:, 2:] / 2\n    boxes[:, 2:] += boxes[:, :2]\n    return boxes\n\n\ndef log_sum_exp(x):\n    """"""Utility function for computing log_sum_exp while determining\n    This will be used to determine unaveraged confidence loss across\n    all examples in a batch.\n\n    Args:\n        x (Variable(tensor)): conf_preds from conf layers\n    """"""\n    x_max = x.data.max()\n    # 1\xe3\x80\x81\xe7\x9f\xa9\xe9\x98\xb5\xe9\x80\x90\xe5\x85\x83\xe7\xb4\xa0-\xe7\x9f\xa9\xe9\x98\xb5\xe6\x9c\x80\xe5\xa4\xa7\xe5\x80\xbc\n    # 2\xe3\x80\x81\xe7\x9f\xa9\xe9\x98\xb5\xe9\x80\x90\xe5\x85\x83\xe7\xb4\xa0\xe6\xb1\x82exp\n    # 3\xe3\x80\x81\xe6\xaf\x8f\xe8\xa1\x8c\xe7\x9b\xb8\xe5\x8a\xa0\xef\xbc\x8c\xe7\xbb\x93\xe6\x9e\x9c \xe8\xa1\x8c\xe4\xb8\x8d\xe5\x8f\x98\xef\xbc\x8c\xe5\x88\x97\xe5\x8f\x98\xe4\xb8\xba1\n    # 4\xe3\x80\x81\xe9\x80\x90\xe5\x85\x83\xe7\xb4\xa0log\n    # 5\xe3\x80\x81\xe9\x80\x90\xe5\x85\x83\xe7\xb4\xa0+\xe7\x9f\xa9\xe9\x98\xb5\xe6\x9c\x80\xe5\xa4\xa7\xe5\x80\xbc\n    return torch.log(torch.sum(torch.exp(x-x_max), 1, keepdim=True)) + x_max\n\n\n# Original author: Francisco Massa:\n# https://github.com/fmassa/object-detection.torch\n# Ported to PyTorch by Max deGroot (02/01/2017)\ndef nms(boxes, scores, overlap=0.5, top_k=200):\n    """"""Apply non-maximum suppression at test time to avoid detecting too many\n    overlapping bounding boxes for a given object.\n    Args:\n        boxes: (tensor) The location preds for the img, Shape: [num_priors,4].\n        scores: (tensor) The class predscores for the img, Shape:[num_priors].\n        overlap: (float) The overlap thresh for suppressing unnecessary boxes.\n        top_k: (int) The Maximum number of box preds to consider.\n    Return:\n        The indices of the kept boxes with respect to num_priors.\n    """"""\n\n    keep = scores.new(scores.size(0)).zero_().long()\n    if boxes.numel() == 0:\n        return keep\n    x1 = boxes[:, 0]\n    y1 = boxes[:, 1]\n    x2 = boxes[:, 2]\n    y2 = boxes[:, 3]\n    area = torch.mul(x2 - x1, y2 - y1)\n    v, idx = scores.sort(0)  # sort in ascending order\n    # I = I[v >= 0.01]\n    idx = idx[-top_k:]  # indices of the top-k largest vals\n    xx1 = boxes.new()\n    yy1 = boxes.new()\n    xx2 = boxes.new()\n    yy2 = boxes.new()\n    w = boxes.new()\n    h = boxes.new()\n\n    # keep = torch.Tensor()\n    count = 0\n    while idx.numel() > 0:\n        i = idx[-1]  # index of current largest val\n        # keep.append(i)\n        keep[count] = i\n        count += 1\n        if idx.size(0) == 1:\n            break\n        idx = idx[:-1]  # remove kept element from view\n        # load bboxes of next highest vals\n        torch.index_select(x1, 0, idx, out=xx1)\n        torch.index_select(y1, 0, idx, out=yy1)\n        torch.index_select(x2, 0, idx, out=xx2)\n        torch.index_select(y2, 0, idx, out=yy2)\n        # store element-wise max with next highest score\n        xx1 = torch.clamp(xx1, min=x1[i])\n        yy1 = torch.clamp(yy1, min=y1[i])\n        xx2 = torch.clamp(xx2, max=x2[i])\n        yy2 = torch.clamp(yy2, max=y2[i])\n        w.resize_as_(xx2)\n        h.resize_as_(yy2)\n        w = xx2 - xx1\n        h = yy2 - yy1\n        # check sizes of xx1 and xx2.. after each iteration\n        w = torch.clamp(w, min=0.0)\n        h = torch.clamp(h, min=0.0)\n        inter = w*h\n        # IoU = i / (area(a) + area(b) - i)\n        rem_areas = torch.index_select(area, 0, idx)  # load remaining areas)\n        union = (rem_areas - inter) + area[i]\n        IoU = inter/union  # store result in iou\n        # keep only elements with an IoU <= overlap\n        idx = idx[IoU.le(overlap)]\n    return keep, count\n'"
SSD_pytorch/models/ssd.py,8,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom SSD_pytorch.models import *\nfrom SSD_pytorch.utils.config import opt\nimport os\n\n\nclass SSD(nn.Module):\n    """"""Single Shot Multibox Architecture\n    The network is composed of a base VGG network followed by the\n    added multibox conv layers.  Each multibox layer branches into\n        1) conv2d for class conf scores\n        2) conv2d for localization predictions\n        3) associated priorbox layer to produce default bounding\n           boxes specific to the layer\'s feature map size.\n    SSD\xe6\xa8\xa1\xe5\x9e\x8b\xe7\x94\xb1\xe5\x8e\xbb\xe6\x8e\x89\xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe5\xb1\x82\xe7\x9a\x84vgg\xe7\xbd\x91\xe7\xbb\x9c\xe4\xb8\xba\xe5\x9f\xba\xe7\xa1\x80\xe7\xbb\x84\xe6\x88\x90\xe3\x80\x82\xe5\x9c\xa8\xe4\xb9\x8b\xe5\x90\x8e\xe6\xb7\xbb\xe5\x8a\xa0\xe4\xba\x86\xe5\xa4\x9a\xe7\x9b\x92\xe8\xbd\xac\xe5\x8c\x96\xe5\xb1\x82\xe3\x80\x82\n    \xe6\xaf\x8f\xe4\xb8\xaa\xe5\xa4\x9a\xe7\x9b\x92\xe5\xb1\x82\xe5\x88\x86\xe6\x94\xaf\xe6\x98\xaf\xef\xbc\x9a\n        1\xef\xbc\x89conv2d \xe8\x8e\xb7\xe5\x8f\x96\xe5\x88\x86\xe7\xb1\xbb\xe7\xbd\xae\xe4\xbf\xa1\xe5\xba\xa6\n        2\xef\xbc\x89conv2d\xe8\xbf\x9b\xe8\xa1\x8c\xe5\x9d\x90\xe6\xa0\x87\xe4\xbd\x8d\xe7\xbd\xae\xe9\xa2\x84\xe6\xb5\x8b\n        3\xef\xbc\x89\xe7\x9b\xb8\xe5\x85\xb3\xe5\xb1\x82\xe5\x8e\xbb\xe4\xba\xa7\xe7\x94\x9f\xe7\x89\xb9\xe5\xae\x9a\xe4\xba\x8e\xe8\xaf\xa5\xe5\xb1\x82\xe7\x89\xb9\xe5\xbe\x81\xe5\x9b\xbe\xe5\xa4\xa7\xe5\xb0\x8f\xe7\x9a\x84\xe9\xbb\x98\xe8\xae\xa4\xe7\x9a\x84\xe9\xa2\x84\xe6\xb5\x8b\xe6\xa1\x86bounding  boxes\n\n\n\n    See: https://arxiv.org/pdf/1512.02325.pdf for more details.\n\n    Args:\n        phase: (string) Can be ""test"" or ""train""\n        size: input image size  \xe8\xbe\x93\xe5\x85\xa5\xe7\x9a\x84\xe5\x9b\xbe\xe5\x83\x8f\xe5\xb0\xba\xe5\xaf\xb8\n        base: VGG16 layers for input, size of either 300 or 500   \xe7\xbb\x8f\xe8\xbf\x87\xe4\xbf\xae\xe6\x94\xb9\xe7\x9a\x84vgg\xe7\xbd\x91\xe7\xbb\x9c\n        extras: extra layers that feed to multibox loc and conf layers\n                \xe6\x8f\x90\xe4\xbe\x9b\xe5\xa4\x9a\xe7\x9b\x92\xe5\xae\x9a\xe4\xbd\x8d\xe7\x9a\x84\xe6\xa0\xbc\xe5\xa4\x96\xe5\xb1\x82  \xe5\x92\x8c \xe5\x88\x86\xe7\xb1\xbb\xe7\xbd\xae\xe4\xbf\xa1\xe5\xb1\x82\xef\xbc\x88vgg\xe7\xbd\x91\xe7\xbb\x9c\xe5\x90\x8e\xe9\x9d\xa2\xe6\x96\xb0\xe5\xa2\x9e\xe7\x9a\x84\xe9\xa2\x9d\xe5\xa4\x96\xe5\xb1\x82\xef\xbc\x89\n        head: ""multibox head"" consists of loc and conf conv layers\n                \xe7\x94\xb1\xe5\xae\x9a\xe4\xbd\x8d\xe5\x92\x8c\xe5\x88\x86\xe7\xb1\xbb\xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82\xe7\xbb\x84\xe6\x88\x90\xe7\x9a\x84multibox head\n                (loc_layers, conf_layers)     vgg\xe4\xb8\x8eextras\xe4\xb8\xad\xe8\xbf\x9b\xe8\xa1\x8c\xe5\x88\x86\xe7\xb1\xbb\xe5\x92\x8c\xe5\x9b\x9e\xe5\xbd\x92\xe7\x9a\x84\xe5\xb1\x82\n    """"""\n\n    def __init__(self, phase, size, base, extras, head, num_classes):\n        super(SSD, self).__init__()\n        self.phase = phase\n        self.num_classes = num_classes\n        self.cfg = opt.voc\n        # \xe6\x96\xb0\xe5\xae\x9a\xe4\xb9\x89\xe4\xb8\x80\xe4\xb8\xaa\xe7\xb1\xbb\xef\xbc\x8c\xe8\xaf\xa5\xe7\xb1\xbb\xe7\x9a\x84\xe5\x8a\x9f\xe8\x83\xbd\xef\xbc\x9a\xe5\xaf\xb9\xe4\xba\x8e\xe6\xaf\x8f\xe4\xb8\xaafeature map\xef\xbc\x8c\xe7\x94\x9f\xe6\x88\x90\xe9\xa2\x84\xe6\xb5\x8b\xe6\xa1\x86\xef\xbc\x88\xe4\xb8\xad\xe5\xbf\x83\xe5\x9d\x90\xe6\xa0\x87\xe5\x8f\x8a\xe5\x81\x8f\xe7\xa7\xbb\xe9\x87\x8f\xef\xbc\x89\n        self.priorbox = PriorBox(self.cfg)\n        # \xe8\xb0\x83\xe7\x94\xa8forward\xef\xbc\x8c\xe8\xbf\x94\xe5\x9b\x9e\xe7\x94\x9f\xe6\x88\x90\xe7\x9a\x84\xe9\xa2\x84\xe6\xb5\x8b\xe6\xa1\x86\xe7\xbb\x93\xe6\x9e\x9c\n        # \xe5\xaf\xb9\xe4\xba\x8e\xe6\x89\x80\xe6\x9c\x89\xe9\xa2\x84\xe6\xb5\x8b\xe7\x9a\x84feature map\xef\xbc\x8c\xe5\xad\x98\xe5\x82\xa8\xe7\x9d\x80\xe7\x94\x9f\xe6\x88\x90\xe7\x9a\x84\xe4\xb8\x8d\xe5\x90\x8c\xe9\x95\xbf\xe5\xae\xbd\xe6\xaf\x94\xe7\x9a\x84\xe9\xbb\x98\xe8\xae\xa4\xe6\xa1\x86\xef\xbc\x88\xe5\x8f\xaf\xe4\xbb\xa5\xe7\x90\x86\xe8\xa7\xa3\xe4\xb8\xbaanchor\xef\xbc\x89\n        self.priors = Variable(self.priorbox.forward(), volatile=True)\n        #300\n        self.size = size\n\n        # SSD network\xe8\x8c\x83\xe5\x9b\xb4\n        # \xe7\xbb\x8f\xe8\xbf\x87\xe4\xbf\xae\xe6\x94\xb9\xe7\x9a\x84vgg\xe7\xbd\x91\xe7\xbb\x9c\n        self.vgg = nn.ModuleList(base)\n        # Layer learns to scale the l2 normalized features from conv4_3\n        # Layer\xe5\xb1\x82\xe4\xbb\x8econv4_3\xe5\xad\xa6\xe4\xb9\xa0\xe5\x8e\xbb\xe7\xbc\xa9\xe6\x94\xbel2\xe6\xad\xa3\xe5\x88\x99\xe5\x8c\x96\xe7\x89\xb9\xe5\xbe\x81\n        # \xe8\xae\xba\xe6\x96\x87\xe4\xb8\xadconv4_3 \xe7\x9b\xb8\xe6\xaf\x94\xe8\xbe\x83\xe4\xba\x8e\xe5\x85\xb6\xe4\xbb\x96\xe7\x9a\x84layers\xef\xbc\x8c\xe6\x9c\x89\xe7\x9d\x80\xe4\xb8\x8d\xe5\x90\x8c\xe7\x9a\x84 feature scale\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe4\xbd\xbf\xe7\x94\xa8 ParseNet \xe4\xb8\xad\xe7\x9a\x84 L2 normalization \xe6\x8a\x80\xe6\x9c\xaf\n        # \xe5\xb0\x86conv4_3 feature map \xe4\xb8\xad\xe6\xaf\x8f\xe4\xb8\x80\xe4\xb8\xaa\xe4\xbd\x8d\xe7\xbd\xae\xe7\x9a\x84 feature norm scale \xe5\x88\xb0 20\xef\xbc\x8c\xe5\xb9\xb6\xe4\xb8\x94\xe5\x9c\xa8 back-propagation \xe4\xb8\xad\xe5\xad\xa6\xe4\xb9\xa0\xe8\xbf\x99\xe4\xb8\xaa scale\n        self.L2Norm = L2Norm(512, 20)\n        # vgg\xe7\xbd\x91\xe7\xbb\x9c\xe5\x90\x8e\xe9\x9d\xa2\xe6\x96\xb0\xe5\xa2\x9e\xe7\x9a\x84\xe9\xa2\x9d\xe5\xa4\x96\xe5\xb1\x82\n        self.extras = nn.ModuleList(extras)\n        # vgg\xe4\xb8\x8eextras\xe4\xb8\xad\xe8\xbf\x9b\xe8\xa1\x8c\xe5\x88\x86\xe7\xb1\xbb\xe5\x92\x8c\xe5\x9b\x9e\xe5\xbd\x92\xe7\x9a\x84\xe5\xb1\x82\n        self.loc = nn.ModuleList(head[0])\n        self.conf = nn.ModuleList(head[1])\n\n        # \xe5\xa6\x82\xe6\x9e\x9c\xe7\xbd\x91\xe7\xbb\x9c\xe7\x94\xa8\xe4\xba\x8e\xe6\xb5\x8b\xe8\xaf\x95\xef\xbc\x8c\xe5\x88\x99\xe5\x8a\xa0\xe5\x85\xa5softmax\xe5\x92\x8c\xe6\xa3\x80\xe6\xb5\x8b\n        if phase == \'test\':\n            self.softmax = nn.Softmax(dim=-1)\n            self.detect = Detect(num_classes, 0, 200, 0.01, 0.45)\n\n    def forward(self, x):\n        """"""Applies network layers and ops on input image(s) x.\n        \xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\n\n        Args:\n            x: input image or batch of images. Shape: [batch,3,300,300].\n\n        Return:\n            Depending on phase:\n            test\xe6\xb5\x8b\xe8\xaf\x95\xe9\x9b\x86:\n                Variable(tensor) of output class label predictions,\n                confidence score, and corresponding location predictions for\n                each object detected. Shape: [batch,topk,7]\n\n            train\xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86:\n                list of concat outputs from:\n                    1: \xe5\x88\x86\xe7\xb1\xbb\xe5\xb1\x82confidence layers, Shape: [batch*num_priors,num_classes]\n                    2: \xe5\x9b\x9e\xe5\xbd\x92\xe5\xae\x9a\xe4\xbd\x8d\xe5\xb1\x82localization layers, Shape: [batch,num_priors*4]\n                    3: priorbox layers, Shape: [2,num_priors*4]\n        """"""\n        # sources\xe4\xbf\x9d\xe5\xad\x98 \xe7\xbd\x91\xe7\xbb\x9c\xe7\x94\x9f\xe6\x88\x90\xe7\x9a\x84\xe4\xb8\x8d\xe5\x90\x8c\xe5\xb1\x82feature map\xe7\xbb\x93\xe6\x9e\x9c\xef\xbc\x8c\xe4\xbb\xa5\xe4\xbe\xbf\xe4\xbd\xbf\xe7\x94\xa8\xe8\xbf\x99\xe4\xba\x9bfeature map\xe6\x9d\xa5\xe8\xbf\x9b\xe8\xa1\x8c\xe5\x88\x86\xe7\xb1\xbb\xe4\xb8\x8e\xe5\x9b\x9e\xe5\xbd\x92\n        sources = list()\n        # \xe4\xbf\x9d\xe5\xad\x98\xe9\xa2\x84\xe6\xb5\x8b\xe5\xb1\x82\xe4\xb8\x8d\xe5\x90\x8cfeature map\xe9\x80\x9a\xe8\xbf\x87\xe5\x9b\x9e\xe5\xbd\x92\xe5\x92\x8c\xe5\x88\x86\xe7\xb1\xbb\xe7\xbd\x91\xe7\xbb\x9c\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe7\xbb\x93\xe6\x9e\x9c\n        loc = list()\n        conf = list()\n\n        # \xe5\x8e\x9f\xe8\xae\xba\xe6\x96\x87\xe4\xb8\xadvgg\xe7\x9a\x84conv4_3\xef\xbc\x8crelu\xe4\xb9\x8b\xe5\x90\x8e\xe5\x8a\xa0\xe5\x85\xa5L2 Normalization\xe6\xad\xa3\xe5\x88\x99\xe5\x8c\x96\xef\xbc\x8c\xe7\x84\xb6\xe5\x90\x8e\xe4\xbf\x9d\xe5\xad\x98feature map\n        # apply vgg up to conv4_3 relu\n        # \xe5\xb0\x86vgg\xe5\xb1\x82\xe7\x9a\x84feature map\xe4\xbf\x9d\xe5\xad\x98\n        for k in range(23):\n            x = self.vgg[k](x)\n        s = self.L2Norm(x)\n        sources.append(s)\n\n        # apply vgg up to fc7\xef\xbc\x8c\xe5\x8d\xb3\xe5\xb0\x86\xe5\x8e\x9ffc7\xe5\xb1\x82\xe6\x9b\xb4\xe6\x94\xb9\xe4\xb8\xba\xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82\xe8\xbe\x93\xe5\x87\xba\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x9c\xef\xbc\x8c\xe7\xbb\x8f\xe8\xbf\x87relu\xe4\xb9\x8b\xe5\x90\x8e\xe4\xbf\x9d\xe5\xad\x98\xe7\xbb\x93\xe6\x9e\x9c\n        for k in range(23, len(self.vgg)):\n            x = self.vgg[k](x)\n        sources.append(x)\n\n        # apply extra layers and cache source layer outputs\n        # \xe5\xb0\x86\xe6\x96\xb0\xe5\xa2\x9e\xe5\xb1\x82\xe7\x9a\x84feature map\xe4\xbf\x9d\xe5\xad\x98\n        for k, v in enumerate(self.extras):\n            # \xe6\xaf\x8f\xe7\xbb\x8f\xe8\xbf\x87\xe4\xb8\x80\xe4\xb8\xaaconv\xe5\x8d\xb7\xe7\xa7\xaf\xef\xbc\x8c\xe9\x83\xbdrelu\xe4\xb8\x80\xe4\xb8\x8b\n            x = F.relu(v(x), inplace=True)\n            # \xe8\xae\xba\xe6\x96\x87\xe4\xb8\xad\xe9\x9a\x94\xe4\xb8\x80\xe4\xb8\xaaconv\xe4\xbf\x9d\xe5\xad\x98\xe4\xb8\x80\xe4\xb8\xaa\xe7\xbb\x93\xe6\x9e\x9c\n            if k % 2 == 1:\n                sources.append(x)\n\n        # apply multibox head to source layers\n        # permute  \xe5\xb0\x86tensor\xe7\x9a\x84\xe7\xbb\xb4\xe5\xba\xa6\xe6\x8d\xa2\xe4\xbd\x8d  \xe5\x8f\x82\xe6\x95\xb0\xe4\xb8\xba\xe6\x8d\xa2\xe4\xbd\x8d\xe9\xa1\xba\xe5\xba\x8f\n        #contiguous \xe8\xbf\x94\xe5\x9b\x9e\xe4\xb8\x80\xe4\xb8\xaa\xe5\x86\x85\xe5\xad\x98\xe8\xbf\x9e\xe7\xbb\xad\xe7\x9a\x84\xe6\x9c\x89\xe7\x9b\xb8\xe5\x90\x8c\xe6\x95\xb0\xe6\x8d\xae\xe7\x9a\x84tensor\n\n        #source\xe4\xbf\x9d\xe5\xad\x98\xe7\x9a\x84\xe6\x98\xaf\xe6\xaf\x8f\xe4\xb8\xaa\xe9\xa2\x84\xe6\xb5\x8b\xe5\xb1\x82\xe7\x9a\x84\xe7\xbd\x91\xe7\xbb\x9c\xe8\xbe\x93\xe5\x87\xba,\xe5\x8d\xb3feature maps\n        #loc \xe9\x80\x9a\xe8\xbf\x87\xe4\xbd\xbf\xe7\x94\xa8feature map\xe5\x8e\xbb\xe9\xa2\x84\xe6\xb5\x8b\xe5\x9b\x9e\xe5\xbd\x92\n        #conf\xe9\x80\x9a\xe8\xbf\x87\xe4\xbd\xbf\xe7\x94\xa8feature map\xe5\x8e\xbb\xe9\xa2\x84\xe6\xb5\x8b\xe5\x88\x86\xe7\xb1\xbb\n        for (x, l, c) in zip(sources, self.loc, self.conf):\n            loc.append(l(x).permute(0, 2, 3, 1).contiguous())\n            conf.append(c(x).permute(0, 2, 3, 1).contiguous())\n        # \xe5\x9c\xa8\xe7\xbb\x99\xe5\xae\x9a\xe7\xbb\xb4\xe5\xba\xa6\xe4\xb8\x8a\xe5\xaf\xb9\xe8\xbe\x93\xe5\x85\xa5\xe7\x9a\x84\xe5\xbc\xa0\xe9\x87\x8f\xe5\xba\x8f\xe5\x88\x97seq \xe8\xbf\x9b\xe8\xa1\x8c\xe8\xbf\x9e\xe6\x8e\xa5\xe6\x93\x8d\xe4\xbd\x9c    dimension=1\xe8\xa1\xa8\xe7\xa4\xba\xe5\x9c\xa8\xe5\x88\x97\xe4\xb8\x8a\xe8\xbf\x9e\xe6\x8e\xa5\n        loc = torch.cat([o.view(o.size(0), -1) for o in loc], 1)\n        conf = torch.cat([o.view(o.size(0), -1) for o in conf], 1)\n        # \xe6\xb5\x8b\xe8\xaf\x95\xe9\x9b\x86\xe4\xb8\x8a\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\n        if self.phase == ""test"":\n            output = self.detect(\n                loc.view(loc.size(0), -1, 4),                   # loc preds  \xe5\xae\x9a\xe4\xbd\x8d\xe7\x9a\x84\xe9\xa2\x84\xe6\xb5\x8b\n                self.softmax(conf.view(conf.size(0), -1,\n                             self.num_classes)),                # conf preds  \xe5\x88\x86\xe7\xb1\xbb\xe7\x9a\x84\xe9\xa2\x84\xe6\xb5\x8b\n                self.priors.type(type(x.data))                  # default boxes  \xe9\xa2\x84\xe6\xb5\x8b\xe6\xa1\x86\n            )\n        else:\n            # \xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\xe4\xb8\x8a\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\n            output = (\n                loc.view(loc.size(0), -1, 4),    # loc preds [32,8732,4] \xe9\x80\x9a\xe8\xbf\x87\xe7\xbd\x91\xe7\xbb\x9c\xe8\xbe\x93\xe5\x87\xba\xe7\x9a\x84\xe5\xae\x9a\xe4\xbd\x8d\xe7\x9a\x84\xe9\xa2\x84\xe6\xb5\x8b\n                conf.view(conf.size(0), -1, self.num_classes),  #conf preds [32,8732,21]  \xe9\x80\x9a\xe8\xbf\x87\xe7\xbd\x91\xe7\xbb\x9c\xe8\xbe\x93\xe5\x87\xba\xe7\x9a\x84\xe5\x88\x86\xe7\xb1\xbb\xe7\x9a\x84\xe9\xa2\x84\xe6\xb5\x8b\n                self.priors   # \xe4\xb8\x8d\xe5\x90\x8cfeature map\xe6\xa0\xb9\xe6\x8d\xae\xe5\x85\xac\xe5\xbc\x8f\xe7\x94\x9f\xe6\x88\x90\xe7\x9a\x84\xe9\x94\x9a\xe7\xbb\x93\xe6\x9e\x9c [8732,4]   \xe5\x86\x85\xe5\xae\xb9\xe4\xb8\xba \xe4\xb8\xad\xe5\xbf\x83\xe7\x82\xb9\xe5\x9d\x90\xe6\xa0\x87\xe5\x92\x8c\xe5\xae\xbd\xe9\xab\x98\n            )\n        return output\n\n\n    def saveSSD(self, name=None):\n        \'\'\'\n        \xe4\xbf\x9d\xe5\xad\x98\xe6\xa8\xa1\xe5\x9e\x8b\n        \'\'\'\n        # \xe4\xbf\x9d\xe5\xad\x98\xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x80\xe6\xac\xa1\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\n        if name is None:\n            prefix = opt.checkpoint_root + \'last_time_SSD\'\n            name=prefix+\'.pth\'\n        # \xe6\xaf\x8f\xe9\x9a\x94\xe4\xb8\x80\xe6\xae\xb5\xe6\x97\xb6\xe9\x97\xb4\xe4\xbf\x9d\xe5\xad\x98\xe4\xb8\x80\xe6\xac\xa1\xe6\xa8\xa1\xe5\x9e\x8b\n        else:\n            prefix =opt.checkpoint_root +\'SSD_iter\'+name\n            name = prefix + \'.pth\'\n        torch.save(self.state_dict(), name)\n        return name\n\n\n\n# This function is derived from torchvision VGG make_layers()\n# \xe6\xad\xa4\xe6\x96\xb9\xe6\xb3\x95\xe6\xba\x90\xe8\x87\xaatorchvision VGG make_layers\xef\xbc\x88\xef\xbc\x89\n# https://github.com/pytorch/vision/blob/master/torchvision/models/vgg.py\ndef vgg(cfg, i, batch_norm=False):\n    \'\'\'\n    vgg\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x84\n    cfg:  vgg\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x84\n     \'300\': [64, 64, \'M\', 128, 128, \'M\', 256, 256, 256, \'C\', 512, 512, 512, \'M\',\n            512, 512, 512],\n    i: 3   \xe8\xbe\x93\xe5\x85\xa5\xe5\x9b\xbe\xe5\x83\x8f\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\n    batch_norm    \xe4\xb8\xbaFalse\xe3\x80\x82\xe8\x8b\xa5\xe4\xb8\xbaTrue\xef\xbc\x8c\xe5\x88\x99\xe7\xbd\x91\xe7\xbb\x9c\xe4\xb8\xad\xe5\x8a\xa0\xe5\x85\xa5batch_norm\n\n    \xe8\xbf\x94\xe5\x9b\x9e\xe6\xb2\xa1\xe6\x9c\x89\xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe5\xb1\x82\xe7\x9a\x84vgg\xe7\xbd\x91\xe7\xbb\x9c\n    \'\'\'\n    #\xe4\xbf\x9d\xe5\xad\x98vgg\xe6\x89\x80\xe6\x9c\x89\xe5\xb1\x82\n    layers = []\n    #\xe8\xbe\x93\xe5\x85\xa5\xe5\x9b\xbe\xe5\x83\x8f\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\n    in_channels = i\n    for v in cfg:   #M\xe4\xb8\x8eC\xe4\xbc\x9a\xe5\xaf\xbc\xe8\x87\xb4\xe7\x94\x9f\xe6\x88\x90\xe7\x9a\x84feature map\xe5\xa4\xa7\xe5\xb0\x8f\xe5\x87\xba\xe7\x8e\xb0\xe5\x8f\x98\xe5\x8c\x96\n        if v == \'M\':  #\xe6\x9c\x80\xe5\xa4\xa7\xe6\xb1\xa0\xe5\x8c\x96\xe5\xb1\x82   \xe9\xbb\x98\xe8\xae\xa4floor\xe6\xa8\xa1\xe5\xbc\x8f\n            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n        elif v == \'C\':  #\xe6\x9c\x80\xe5\xa4\xa7\xe6\xb1\xa0\xe5\x8c\x96\xe5\xb1\x82   ceil\xe6\xa8\xa1\xe5\xbc\x8f   \xe4\xb8\xa4\xe7\xa7\x8d\xe4\xb8\x8d\xe5\x90\x8c\xe7\x9a\x84maxpool\xe6\x96\xb9\xe5\xbc\x8f    \xe5\x8f\x82\xe8\x80\x83https://blog.csdn.net/GZHermit/article/details/79351803\n            layers += [nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)]\n        else:\n            # \xe5\x8d\xb7\xe7\xa7\xaf\n            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n            if batch_norm:\n                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n            else:\n                layers += [conv2d, nn.ReLU(inplace=True)]\n            in_channels = v\n    # \xe8\xae\xba\xe6\x96\x87\xe5\xb0\x86 Pool5 layer \xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0\xef\xbc\x8c\xe4\xbb\x8e \xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb82\xc3\x972\xe6\xad\xa5\xe9\x95\xbf\xe4\xb8\xba2  \xe8\xbd\xac\xe5\x8f\x98\xe6\x88\x90 \xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb83\xc3\x973 \xe6\xad\xa5\xe9\x95\xbf\xe4\xb8\xba1 \xe5\xa4\x96\xe5\x8a\xa0\xe4\xb8\x80\xe4\xb8\xaa pad\n    pool5 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n    # \xe8\xae\xba\xe6\x96\x87\xe4\xb8\xad\xe5\xb0\x86VGG\xe7\x9a\x84FC6 layer\xe3\x80\x81FC7 layer \xe8\xbd\xac\xe6\x88\x90\xe4\xb8\xba \xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82conv6,conv7 \xe5\xb9\xb6\xe4\xbb\x8e\xe6\xa8\xa1\xe5\x9e\x8b\xe7\x9a\x84FC6\xe3\x80\x81FC7 \xe4\xb8\x8a\xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0\xef\xbc\x8c\xe8\xbf\x9b\xe8\xa1\x8c\xe9\x87\x87\xe6\xa0\xb7\xe5\xbe\x97\xe5\x88\xb0\xe8\xbf\x99\xe4\xb8\xa4\xe4\xb8\xaa\xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82\xe7\x9a\x84 \xe5\x8f\x82\xe6\x95\xb0\n    #\xe8\xbe\x93\xe5\x85\xa5\xe9\x80\x9a\xe9\x81\x93512  \xe8\xbe\x93\xe5\x87\xba\xe9\x80\x9a\xe9\x81\x93\xe4\xb8\xba1024  \xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8\xe4\xb8\xba3  padding\xe4\xb8\xba6    dilation\xe4\xb8\xba\xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8\xe4\xb8\xad\xe5\x85\x83\xe7\xb4\xa0\xe4\xb9\x8b\xe9\x97\xb4\xe7\x9a\x84\xe7\xa9\xba\xe6\xb4\x9e\xe5\xa4\xa7\xe5\xb0\x8f\n    # \xe4\xbf\xae\xe6\x94\xb9Pool5 layer\xe5\x8f\x82\xe6\x95\xb0\xef\xbc\x8c\xe5\xaf\xbc\xe8\x87\xb4\xe6\x84\x9f\xe5\x8f\x97\xe9\x87\x8e\xe5\xa4\xa7\xe5\xb0\x8f\xe6\x94\xb9\xe5\x8f\x98\xe3\x80\x82\xe6\x89\x80\xe4\xbb\xa5conv6\xe9\x87\x87\xe7\x94\xa8 atrous \xe7\xae\x97\xe6\xb3\x95\xef\xbc\x8c\xe5\x8d\xb3\xe5\xad\x94\xe5\xa1\xab\xe5\x85\x85\xe7\xae\x97\xe6\xb3\x95\xe3\x80\x82\n    # \xe5\xad\x94\xe5\xa1\xab\xe5\x85\x85\xe7\xae\x97\xe6\xb3\x95\xe5\xb0\x86\xe5\x8d\xb7\xe7\xa7\xaf weights \xe8\x86\xa8\xe8\x83\x80\xe6\x89\xa9\xe5\xa4\xa7\xef\xbc\x8c\xe5\x8d\xb3\xe5\x8e\x9f\xe6\x9d\xa5\xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8\xe6\x98\xaf 3x3\xef\xbc\x8c\xe8\x86\xa8\xe8\x83\x80\xe5\x90\x8e\xef\xbc\x8c\xe5\x8f\xaf\xe8\x83\xbd\xe5\x8f\x98\xe6\x88\x90 7x7 \xe4\xba\x86\xef\xbc\x8c\xe8\xbf\x99\xe6\xa0\xb7 receptive field \xe5\x8f\x98\xe5\xa4\xa7\xe4\xba\x86\xef\xbc\x8c\xe8\x80\x8c score map \xe4\xb9\x9f\xe5\xbe\x88\xe5\xa4\xa7\xef\xbc\x8c\xe5\x8d\xb3\xe8\xbe\x93\xe5\x87\xba\xe5\x8f\x98\xe6\x88\x90 dense\n    #\xe8\xbf\x99\xe4\xb9\x88\xe5\x81\x9a\xe7\x9a\x84\xe5\xa5\xbd\xe5\xa4\x84\xe6\x98\xaf\xef\xbc\x8c\xe8\xbe\x93\xe5\x87\xba\xe7\x9a\x84 score map \xe5\x8f\x98\xe5\xa4\xa7\xe4\xba\x86\xef\xbc\x8c\xe5\x8d\xb3\xe6\x98\xaf dense \xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe4\xba\x86\xef\xbc\x8c\xe8\x80\x8c\xe4\xb8\x94 receptive field \xe4\xb8\x8d\xe4\xbc\x9a\xe5\x8f\x98\xe5\xb0\x8f\xef\xbc\x8c\xe8\x80\x8c\xe4\xb8\x94\xe5\x8f\xaf\xe4\xbb\xa5\xe5\x8f\x98\xe5\xa4\xa7\xe3\x80\x82\xe8\xbf\x99\xe5\xaf\xb9\xe5\x81\x9a\xe5\x88\x86\xe5\x89\xb2\xe3\x80\x81\xe6\xa3\x80\xe6\xb5\x8b\xe7\xad\x89\xe5\xb7\xa5\xe4\xbd\x9c\xe9\x9d\x9e\xe5\xb8\xb8\xe9\x87\x8d\xe8\xa6\x81\xe3\x80\x82\n    conv6 = nn.Conv2d(512, 1024, kernel_size=3, padding=6, dilation=6)\n    #\xe8\xbe\x93\xe5\x85\xa5\xe9\x80\x9a\xe9\x81\x93512  \xe8\xbe\x93\xe5\x87\xba\xe9\x80\x9a\xe9\x81\x93\xe4\xb8\xba1024  \xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8\xe4\xb8\xba3\n    conv7 = nn.Conv2d(1024, 1024, kernel_size=1)\n    #\xe5\xb0\x86 \xe4\xbf\xae\xe6\x94\xb9\xe7\x9a\x84\xe5\xb1\x82\xe4\xb9\x9f\xe5\x8a\xa0\xe5\x85\xa5\xe5\x88\xb0vgg\xe7\xbd\x91\xe7\xbb\x9c\xe4\xb8\xad\n    layers += [pool5, conv6,\n               nn.ReLU(inplace=True), conv7, nn.ReLU(inplace=True)]\n    return layers\n\n\ndef add_extras(cfg, i, batch_norm=False):\n    \'\'\'\n    vgg\xe7\xbd\x91\xe7\xbb\x9c\xe5\x90\x8e\xe9\x9d\xa2\xe6\x96\xb0\xe5\xa2\x9e\xe7\x9a\x84\xe9\xa2\x9d\xe5\xa4\x96\xe5\xb1\x82\n    :param cfg:  \'300\': [256, \'S\', 512, 128, \'S\', 256, 128, 256, 128, 256],\n    :param i:    1024  \xe8\xbe\x93\xe5\x85\xa5\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\n    :param batch_norm:  flase\n    :return:\n    \'\'\'\n    # \xe6\xb7\xbb\xe5\x8a\xa0\xe5\x88\xb0VGG\xe7\x9a\x84\xe9\xa2\x9d\xe5\xa4\x96\xe5\x9b\xbe\xe5\xb1\x82\xe7\x94\xa8\xe4\xba\x8e\xe7\x89\xb9\xe5\xbe\x81\xe7\xbc\xa9\xe6\x94\xbe\n    layers = []\n    #1024  \xe8\xbe\x93\xe5\x85\xa5\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\n    in_channels = i\n    # \xe6\x8e\xa7\xe5\x88\xb6\xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8\xe5\xb0\xba\xe5\xaf\xb8\xef\xbc\x8c\xe4\xb8\x80\xe7\xbb\xb4\xe6\x95\xb0\xe7\xbb\x84\xe9\x80\x89\xe5\x89\x8d\xe4\xb8\x80\xe4\xb8\xaa\xe6\x95\xb0\xe8\xbf\x98\xe6\x98\xaf\xe5\x90\x8e\xe4\xb8\x80\xe4\xb8\xaa\xe6\x95\xb0\xe3\x80\x82\xe5\x9c\xa8\xe6\xaf\x8f\xe6\xac\xa1\xe5\xbe\xaa\xe7\x8e\xaf\xe6\x97\xb6flag\xe9\x83\xbd\xe6\x94\xb9\xe5\x8f\x98\xef\xbc\x8c\xe5\xaf\xbc\xe8\x87\xb4\xe7\xbd\x91\xe7\xbb\x9c\xe7\x9a\x84\xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8\xe5\xb0\xba\xe5\xaf\xb8\xe4\xb8\xba1,3,1,3\xe4\xba\xa4\xe6\x9b\xbf\n    # False \xe4\xb8\xba1\xef\xbc\x8cTrue\xe4\xb8\xba3\n    # SSD\xe7\xbd\x91\xe7\xbb\x9c\xe5\x9b\xbe\xe4\xb8\xads1\xe6\x8c\x87\xe6\xad\xa5\xe9\x95\xbf\xe4\xb8\xba1\xef\xbc\x8cs2\xe6\x8c\x87\xe6\xad\xa5\xe9\x95\xbf\xe4\xb8\xba2\n    # \xe5\x9c\xa8\xe8\xaf\xa5\xe4\xbb\xa3\xe7\xa0\x81\xe4\xb8\xad\xef\xbc\x8cS\xe4\xbb\xa3\xe8\xa1\xa8\xe6\xad\xa5\xe9\x95\xbf\xe4\xb8\xba2\xef\xbc\x8c\xe6\x97\xa0S\xe4\xbb\xa3\xe8\xa1\xa8\xe9\xbb\x98\xe8\xae\xa4\xef\xbc\x8c\xe5\x8d\xb3\xe6\xad\xa5\xe9\x95\xbf\xe4\xb8\xba1\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5cfg\xe4\xb8\x8e\xe8\xae\xba\xe6\x96\x87\xe7\xbd\x91\xe7\xbb\x9c\xe7\xbb\x93\xe6\x9e\x84\xe5\xae\x8c\xe5\x85\xa8\xe5\x8c\xb9\xe9\x85\x8d\n    flag = False\n    # enumerate\xe6\x9e\x9a\xe4\xb8\xbe   k\xe4\xb8\xba\xe4\xb8\x8b\xe6\xa0\x87   v\xe4\xb8\xba\xe5\x80\xbc\n    for k, v in enumerate(cfg):\n        if in_channels != \'S\':\n            if v == \'S\':\n                layers += [nn.Conv2d(in_channels, cfg[k + 1],\n                           kernel_size=(1, 3)[flag], stride=2, padding=1)]\n            else:\n                layers += [nn.Conv2d(in_channels, v, kernel_size=(1, 3)[flag])]\n            flag = not flag\n        in_channels = v\n    return layers\n\n\ndef multibox(vgg, extra_layers, cfg, num_classes):\n    \'\'\'\n\n    :param vgg: \xe7\xbb\x8f\xe8\xbf\x87\xe4\xbf\xae\xe6\x94\xb9\xe5\x90\x8e\xe7\x9a\x84vgg\xe7\xbd\x91\xe7\xbb\x9c\xef\xbc\x88\xe5\x8e\xbb\xe6\x8e\x89\xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe5\xb1\x82\xef\xbc\x8c\xe4\xbf\xae\xe6\x94\xb9pool5\xe5\x8f\x82\xe6\x95\xb0\xe5\xb9\xb6\xe6\xb7\xbb\xe5\x8a\xa0\xe6\x96\xb0\xe5\xb1\x82\xef\xbc\x89\n    :param extra_layers: vgg\xe7\xbd\x91\xe7\xbb\x9c\xe5\x90\x8e\xe9\x9d\xa2\xe6\x96\xb0\xe5\xa2\x9e\xe7\x9a\x84\xe9\xa2\x9d\xe5\xa4\x96\xe5\xb1\x82\n    :param cfg: \'300\': [4, 6, 6, 6, 4, 4],  \xe4\xb8\x8d\xe5\x90\x8c\xe9\x83\xa8\xe5\x88\x86\xe7\x9a\x84feature map\xe4\xb8\x8a\xe4\xb8\x80\xe4\xb8\xaa\xe7\xbd\x91\xe6\xa0\xbc\xe9\xa2\x84\xe6\xb5\x8b\xe5\xa4\x9a\xe5\xb0\x91\xe6\xa1\x86\n    :param num_classes: 20\xe5\x88\x86\xe7\xb1\xbb+1\xe8\x83\x8c\xe6\x99\xaf\xef\xbc\x8c\xe5\x85\xb121\xe7\xb1\xbb\n    :return:\n    \'\'\'\n    # \xe4\xbf\x9d\xe5\xad\x98\xe6\x89\x80\xe6\x9c\x89\xe5\x8f\x82\xe4\xb8\x8e\xe9\xa2\x84\xe6\xb5\x8b\xe7\x9a\x84\xe7\xbd\x91\xe7\xbb\x9c\xe5\xb1\x82\n    loc_layers = []\n    conf_layers = []\n    # \xe4\xbc\xa0\xe5\x85\xa5\xe7\x9a\x84\xe4\xbf\xae\xe6\x94\xb9\xe8\xbf\x87\xe7\x9a\x84vgg\xe7\xbd\x91\xe7\xbb\x9c\xe7\x94\xa8\xe4\xba\x8e\xe9\xa2\x84\xe6\xb5\x8b\xe7\x9a\x84\xe7\xbd\x91\xe7\xbb\x9c\xe6\x98\xaf21\xe5\xb1\x82\xe4\xbb\xa5\xe5\x8f\x8a \xe5\x80\x92\xe6\x95\xb0\xe7\xac\xac\xe4\xba\x8c\xe5\xb1\x82\n    vgg_source = [21, -2]\n    for k, v in enumerate(vgg_source):\n        #4\xe6\x98\xaf\xe5\x9b\x9e\xe5\xbd\x92\xe7\x9a\x84\xe5\x9d\x90\xe6\xa0\x87\xe5\x8f\x82\xe6\x95\xb0  cfg\xe4\xbb\xa3\xe8\xa1\xa8\xe8\xaf\xa5\xe5\xb1\x82feature map\xe4\xb8\x8a\xe4\xb8\x80\xe4\xb8\xaa\xe7\xbd\x91\xe6\xa0\xbc\xe9\xa2\x84\xe6\xb5\x8b\xe5\xa4\x9a\xe5\xb0\x91\xe6\xa1\x86\n        loc_layers += [nn.Conv2d(vgg[v].out_channels,\n                                 cfg[k] * 4, kernel_size=3, padding=1)]\n        #num_classes\xe6\x98\xaf\xe7\xb1\xbb\xe5\x88\xab\xe6\x95\xb0 cfg\xe4\xbb\xa3\xe8\xa1\xa8\xe8\xaf\xa5\xe5\xb1\x82feature map\xe4\xb8\x8a\xe4\xb8\x80\xe4\xb8\xaa\xe7\xbd\x91\xe6\xa0\xbc\xe9\xa2\x84\xe6\xb5\x8b\xe5\xa4\x9a\xe5\xb0\x91\xe6\xa1\x86\n        conf_layers += [nn.Conv2d(vgg[v].out_channels,\n                        cfg[k] * num_classes, kernel_size=3, padding=1)]\n    # [x::y] \xe4\xbb\x8e\xe4\xb8\x8b\xe6\xa0\x87x\xe5\xbc\x80\xe5\xa7\x8b\xef\xbc\x8c\xe6\xaf\x8f\xe9\x9a\x94y\xe5\x8f\x96\xe5\x80\xbc\n    #\xe8\xae\xba\xe6\x96\x87\xe4\xb8\xad\xe6\x96\xb0\xe5\xa2\x9e\xe5\xb1\x82\xe4\xb9\x9f\xe6\x98\xaf\xe6\xaf\x8f\xe9\x9a\x94\xe4\xb8\x80\xe4\xb8\xaa\xe5\xb1\x82\xe6\xb7\xbb\xe5\x8a\xa0\xe4\xb8\x80\xe4\xb8\xaa\xe9\xa2\x84\xe6\xb5\x8b\xe5\xb1\x82\n    # \xe5\xb0\x86\xe6\x96\xb0\xe5\xa2\x9e\xe7\x9a\x84\xe9\xa2\x9d\xe5\xa4\x96\xe5\xb1\x82\xe4\xb8\xad\xe7\x9a\x84\xe9\xa2\x84\xe6\xb5\x8b\xe5\xb1\x82\xe4\xb9\x9f\xe6\xb7\xbb\xe5\x8a\xa0\xe4\xb8\x8a   start=2\xef\xbc\x9a\xe4\xb8\x8b\xe6\xa0\x87\xe8\xb5\xb7\xe5\xa7\x8b\xe4\xbd\x8d\xe7\xbd\xae\n    for k, v in enumerate(extra_layers[1::2], 2):\n        loc_layers += [nn.Conv2d(v.out_channels, cfg[k]\n                                 * 4, kernel_size=3, padding=1)]\n        conf_layers += [nn.Conv2d(v.out_channels, cfg[k]\n                                  * num_classes, kernel_size=3, padding=1)]\n    return vgg, extra_layers, (loc_layers, conf_layers)\n\n\nbase = {\n    # \xe6\x95\xb0\xe5\xad\x97\xe4\xb8\xba\xe6\xaf\x8f\xe5\xb1\x82feature map\xe7\x9a\x84\xe5\xb1\x82\xe6\x95\xb0  M\xe4\xbb\xa3\xe8\xa1\xa8\xe6\x9c\x80\xe5\xa4\xa7\xe6\xb1\xa0\xe5\x8c\x96\xe5\xb1\x82\xef\xbc\x88\xe9\xbb\x98\xe8\xae\xa4floor\xe6\xa8\xa1\xe5\xbc\x8f\xef\xbc\x89    C\xe4\xbb\xa3\xe8\xa1\xa8\xe6\x9c\x80\xe5\xa4\xa7\xe6\xb1\xa0\xe5\x8c\x96\xe5\xb1\x82\xef\xbc\x88ceil\xe6\xa8\xa1\xe5\xbc\x8f\xef\xbc\x89  (\xe5\x8e\xbb\xe6\x8e\x89vgg16\xe7\x9a\x84\xe6\x9c\x80\xe5\x90\x8e\xe7\x9a\x84 maxpool\xe3\x80\x81fc\xe3\x80\x81fc\xe3\x80\x81fc\xe3\x80\x81softmax)\n    \'300\': [64, 64, \'M\', 128, 128, \'M\', 256, 256, 256, \'C\', 512, 512, 512, \'M\',\n            512, 512, 512],\n    \'512\': [],\n}\nextras = {\n    #\n    \'300\': [256, \'S\', 512, 128, \'S\', 256, 128, 256, 128, 256],\n    \'512\': [],\n}\nmbox = {\n    \'300\': [4, 6, 6, 6, 4, 4],  # \xe4\xb8\x8d\xe5\x90\x8c\xe9\x83\xa8\xe5\x88\x86\xe7\x9a\x84feature map\xe4\xb8\x8a\xe4\xb8\x80\xe4\xb8\xaa\xe7\xbd\x91\xe6\xa0\xbc\xe9\xa2\x84\xe6\xb5\x8b\xe5\xa4\x9a\xe5\xb0\x91\xe6\xa1\x86\n    \'512\': [],\n}\n\n\ndef build_ssd(phase, size=300, num_classes=21):\n    \'\'\'\n    \xe6\x96\xb0\xe5\xbb\xbaSSD\xe6\xa8\xa1\xe5\x9e\x8b\n    \'\'\'\n    # \xe8\xae\xad\xe7\xbb\x83\xe6\x88\x96\xe6\xb5\x8b\xe8\xaf\x95\n    if phase != ""test"" and phase != ""train"":\n        print(""ERROR: Phase: "" + phase + "" not recognized"")\n        return\n    #\xe5\xbd\x93\xe5\x89\x8dSSD300\xe5\x8f\xaa\xe6\x94\xaf\xe6\x8c\x81\xe5\xa4\xa7\xe5\xb0\x8f300\xc3\x97300\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe8\xae\xad\xe7\xbb\x83\n    if size != 300:\n        print(""ERROR: You specified size "" + repr(size) + "". However, "" +\n              ""currently only SSD300 (size=300) is supported!"")\n        return\n\n    #base_\xef\xbc\x9a \xe7\xbb\x8f\xe8\xbf\x87\xe4\xbf\xae\xe6\x94\xb9\xe5\x90\x8e\xe7\x9a\x84vgg\xe7\xbd\x91\xe7\xbb\x9c\xef\xbc\x88\xe5\x8e\xbb\xe6\x8e\x89\xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe5\xb1\x82\xef\xbc\x8c\xe4\xbf\xae\xe6\x94\xb9pool5\xe5\x8f\x82\xe6\x95\xb0\xe5\xb9\xb6\xe6\xb7\xbb\xe5\x8a\xa0\xe6\x96\xb0\xe5\xb1\x82\xef\xbc\x89\n    #extras_\xef\xbc\x9a  vgg\xe7\xbd\x91\xe7\xbb\x9c\xe5\x90\x8e\xe9\x9d\xa2\xe6\x96\xb0\xe5\xa2\x9e\xe7\x9a\x84\xe9\xa2\x9d\xe5\xa4\x96\xe5\xb1\x82\n    # head_ :    (loc_layers, conf_layers)   vgg\xe4\xb8\x8eextras\xe4\xb8\xad\xe8\xbf\x9b\xe8\xa1\x8c\xe5\x88\x86\xe7\xb1\xbb\xe5\x92\x8c\xe5\x9b\x9e\xe5\xbd\x92\xe7\x9a\x84\xe5\xb1\x82\n    base_, extras_, head_ = multibox(vgg(base[str(size)], 3),  #vgg\xe6\x96\xb9\xe6\xb3\x95\xe8\xbf\x94\xe5\x9b\x9e \xe7\xbb\x8f\xe8\xbf\x87\xe4\xbf\xae\xe6\x94\xb9\xe5\x90\x8e\xe7\x9a\x84vgg\xe7\xbd\x91\xe7\xbb\x9c\xef\xbc\x88\xe5\x8e\xbb\xe6\x8e\x89\xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe5\xb1\x82\xef\xbc\x8c\xe4\xbf\xae\xe6\x94\xb9pool5\xe5\x8f\x82\xe6\x95\xb0\xe5\xb9\xb6\xe6\xb7\xbb\xe5\x8a\xa0\xe6\x96\xb0\xe5\xb1\x82\xef\xbc\x89\n                                     add_extras(extras[str(size)], 1024), #vgg\xe7\xbd\x91\xe7\xbb\x9c\xe5\x90\x8e\xe9\x9d\xa2\xe6\x96\xb0\xe5\xa2\x9e\xe7\x9a\x84\xe9\xa2\x9d\xe5\xa4\x96\xe5\xb1\x82\n                                     mbox[str(size)],  #mbox\xe6\x8c\x87\xe4\xb8\x8d\xe5\x90\x8c\xe9\x83\xa8\xe5\x88\x86\xe7\x9a\x84feature map\xe4\xb8\x8a\xe4\xb8\x80\xe4\xb8\xaa\xe7\xbd\x91\xe6\xa0\xbc\xe9\xa2\x84\xe6\xb5\x8b\xe5\xa4\x9a\xe5\xb0\x91\xe6\xa1\x86\n                                     num_classes)\n    # phase\xef\xbc\x9a\'train\'    size\xef\xbc\x9a300    num_classes\xef\xbc\x9a 21 \xe7\xb1\xbb\xe5\x88\xab\xe6\x95\xb0\xef\xbc\x8820\xe7\xb1\xbb+1\xe8\x83\x8c\xe6\x99\xaf\xef\xbc\x89\n    return SSD(phase, size, base_, extras_, head_, num_classes)\n'"
SSD_pytorch/utils/__init__.py,0,b'from .augmentations import SSDAugmentation'
SSD_pytorch/utils/augmentations.py,2,"b'import torch\nfrom torchvision import transforms\nimport cv2\nimport numpy as np\nimport types\nfrom numpy import random\n\n\ndef intersect(box_a, box_b):\n    max_xy = np.minimum(box_a[:, 2:], box_b[2:])\n    min_xy = np.maximum(box_a[:, :2], box_b[:2])\n    inter = np.clip((max_xy - min_xy), a_min=0, a_max=np.inf)\n    return inter[:, 0] * inter[:, 1]\n\n\ndef jaccard_numpy(box_a, box_b):\n    """"""Compute the jaccard overlap of two sets of boxes.  The jaccard overlap\n    is simply the intersection over union of two boxes.\n    E.g.:\n        A \xe2\x88\xa9 B / A \xe2\x88\xaa B = A \xe2\x88\xa9 B / (area(A) + area(B) - A \xe2\x88\xa9 B)\n    Args:\n        box_a: Multiple bounding boxes, Shape: [num_boxes,4]\n        box_b: Single bounding box, Shape: [4]\n    Return:\n        jaccard overlap: Shape: [box_a.shape[0], box_a.shape[1]]\n    """"""\n    inter = intersect(box_a, box_b)\n    area_a = ((box_a[:, 2]-box_a[:, 0]) *\n              (box_a[:, 3]-box_a[:, 1]))  # [A,B]\n    area_b = ((box_b[2]-box_b[0]) *\n              (box_b[3]-box_b[1]))  # [A,B]\n    union = area_a + area_b - inter\n    return inter / union  # [A,B]\n\n\nclass Compose(object):\n    """"""Composes several augmentations together.\n    Args:\n        transforms (List[Transform]): list of transforms to compose.\n    Example:\n        >>> augmentations.Compose([\n        >>>     transforms.CenterCrop(10),\n        >>>     transforms.ToTensor(),\n        >>> ])\n    """"""\n\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, img, boxes=None, labels=None):\n        for t in self.transforms:\n            img, boxes, labels = t(img, boxes, labels)\n        return img, boxes, labels\n\n\nclass Lambda(object):\n    """"""Applies a lambda as a transform.""""""\n\n    def __init__(self, lambd):\n        assert isinstance(lambd, types.LambdaType)\n        self.lambd = lambd\n\n    def __call__(self, img, boxes=None, labels=None):\n        return self.lambd(img, boxes, labels)\n\n\nclass ConvertFromInts(object):\n    def __call__(self, image, boxes=None, labels=None):\n        return image.astype(np.float32), boxes, labels\n\n\nclass SubtractMeans(object):\n    def __init__(self, mean):\n        self.mean = np.array(mean, dtype=np.float32)\n\n    def __call__(self, image, boxes=None, labels=None):\n        image = image.astype(np.float32)\n        image -= self.mean\n        return image.astype(np.float32), boxes, labels\n\n\nclass ToAbsoluteCoords(object):\n    def __call__(self, image, boxes=None, labels=None):\n        height, width, channels = image.shape\n        boxes[:, 0] *= width\n        boxes[:, 2] *= width\n        boxes[:, 1] *= height\n        boxes[:, 3] *= height\n\n        return image, boxes, labels\n\n\nclass ToPercentCoords(object):\n    def __call__(self, image, boxes=None, labels=None):\n        height, width, channels = image.shape\n        boxes[:, 0] /= width\n        boxes[:, 2] /= width\n        boxes[:, 1] /= height\n        boxes[:, 3] /= height\n\n        return image, boxes, labels\n\n\nclass Resize(object):\n    def __init__(self, size=300):\n        self.size = size\n\n    def __call__(self, image, boxes=None, labels=None):\n        image = cv2.resize(image, (self.size,\n                                 self.size))\n        return image, boxes, labels\n\n\nclass RandomSaturation(object):\n    def __init__(self, lower=0.5, upper=1.5):\n        self.lower = lower\n        self.upper = upper\n        assert self.upper >= self.lower, ""contrast upper must be >= lower.""\n        assert self.lower >= 0, ""contrast lower must be non-negative.""\n\n    def __call__(self, image, boxes=None, labels=None):\n        if random.randint(2):\n            image[:, :, 1] *= random.uniform(self.lower, self.upper)\n\n        return image, boxes, labels\n\n\nclass RandomHue(object):\n    def __init__(self, delta=18.0):\n        assert delta >= 0.0 and delta <= 360.0\n        self.delta = delta\n\n    def __call__(self, image, boxes=None, labels=None):\n        if random.randint(2):\n            image[:, :, 0] += random.uniform(-self.delta, self.delta)\n            image[:, :, 0][image[:, :, 0] > 360.0] -= 360.0\n            image[:, :, 0][image[:, :, 0] < 0.0] += 360.0\n        return image, boxes, labels\n\n\nclass RandomLightingNoise(object):\n    def __init__(self):\n        self.perms = ((0, 1, 2), (0, 2, 1),\n                      (1, 0, 2), (1, 2, 0),\n                      (2, 0, 1), (2, 1, 0))\n\n    def __call__(self, image, boxes=None, labels=None):\n        if random.randint(2):\n            swap = self.perms[random.randint(len(self.perms))]\n            shuffle = SwapChannels(swap)  # shuffle channels\n            image = shuffle(image)\n        return image, boxes, labels\n\n\nclass ConvertColor(object):\n    def __init__(self, current=\'BGR\', transform=\'HSV\'):\n        self.transform = transform\n        self.current = current\n\n    def __call__(self, image, boxes=None, labels=None):\n        if self.current == \'BGR\' and self.transform == \'HSV\':\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n        elif self.current == \'HSV\' and self.transform == \'BGR\':\n            image = cv2.cvtColor(image, cv2.COLOR_HSV2BGR)\n        else:\n            raise NotImplementedError\n        return image, boxes, labels\n\n\nclass RandomContrast(object):\n    def __init__(self, lower=0.5, upper=1.5):\n        self.lower = lower\n        self.upper = upper\n        assert self.upper >= self.lower, ""contrast upper must be >= lower.""\n        assert self.lower >= 0, ""contrast lower must be non-negative.""\n\n    # expects float image\n    def __call__(self, image, boxes=None, labels=None):\n        if random.randint(2):\n            alpha = random.uniform(self.lower, self.upper)\n            image *= alpha\n        return image, boxes, labels\n\n\nclass RandomBrightness(object):\n    def __init__(self, delta=32):\n        assert delta >= 0.0\n        assert delta <= 255.0\n        self.delta = delta\n\n    def __call__(self, image, boxes=None, labels=None):\n        if random.randint(2):\n            delta = random.uniform(-self.delta, self.delta)\n            image += delta\n        return image, boxes, labels\n\n\nclass ToCV2Image(object):\n    def __call__(self, tensor, boxes=None, labels=None):\n        return tensor.cpu().numpy().astype(np.float32).transpose((1, 2, 0)), boxes, labels\n\n\nclass ToTensor(object):\n    def __call__(self, cvimage, boxes=None, labels=None):\n        return torch.from_numpy(cvimage.astype(np.float32)).permute(2, 0, 1), boxes, labels\n\n\nclass RandomSampleCrop(object):\n    """"""Crop\n    Arguments:\n        img (Image): the image being input during training\n        boxes (Tensor): the original bounding boxes in pt form\n        labels (Tensor): the class labels for each bbox\n        mode (float tuple): the min and max jaccard overlaps\n    Return:\n        (img, boxes, classes)\n            img (Image): the cropped image\n            boxes (Tensor): the adjusted bounding boxes in pt form\n            labels (Tensor): the class labels for each bbox\n    """"""\n    def __init__(self):\n        self.sample_options = (\n            # using entire original input image\n            None,\n            # sample a patch s.t. MIN jaccard w/ obj in .1,.3,.4,.7,.9\n            (0.1, None),\n            (0.3, None),\n            (0.7, None),\n            (0.9, None),\n            # randomly sample a patch\n            (None, None),\n        )\n\n    def __call__(self, image, boxes=None, labels=None):\n        height, width, _ = image.shape\n        while True:\n            # randomly choose a mode\n            mode = random.choice(self.sample_options)\n            if mode is None:\n                return image, boxes, labels\n\n            min_iou, max_iou = mode\n            if min_iou is None:\n                min_iou = float(\'-inf\')\n            if max_iou is None:\n                max_iou = float(\'inf\')\n\n            # max trails (50)\n            for _ in range(50):\n                current_image = image\n\n                w = random.uniform(0.3 * width, width)\n                h = random.uniform(0.3 * height, height)\n\n                # aspect ratio constraint b/t .5 & 2\n                if h / w < 0.5 or h / w > 2:\n                    continue\n\n                left = random.uniform(width - w)\n                top = random.uniform(height - h)\n\n                # convert to integer rect x1,y1,x2,y2\n                rect = np.array([int(left), int(top), int(left+w), int(top+h)])\n\n                # calculate IoU (jaccard overlap) b/t the cropped and gt boxes\n                overlap = jaccard_numpy(boxes, rect)\n\n                # is min and max overlap constraint satisfied? if not try again\n                if overlap.min() < min_iou and max_iou < overlap.max():\n                    continue\n\n                # cut the crop from the image\n                current_image = current_image[rect[1]:rect[3], rect[0]:rect[2],\n                                              :]\n\n                # keep overlap with gt box IF center in sampled patch\n                centers = (boxes[:, :2] + boxes[:, 2:]) / 2.0\n\n                # mask in all gt boxes that above and to the left of centers\n                m1 = (rect[0] < centers[:, 0]) * (rect[1] < centers[:, 1])\n\n                # mask in all gt boxes that under and to the right of centers\n                m2 = (rect[2] > centers[:, 0]) * (rect[3] > centers[:, 1])\n\n                # mask in that both m1 and m2 are true\n                mask = m1 * m2\n\n                # have any valid boxes? try again if not\n                if not mask.any():\n                    continue\n\n                # take only matching gt boxes\n                current_boxes = boxes[mask, :].copy()\n\n                # take only matching gt labels\n                current_labels = labels[mask]\n\n                # should we use the box left and top corner or the crop\'s\n                current_boxes[:, :2] = np.maximum(current_boxes[:, :2],\n                                                  rect[:2])\n                # adjust to crop (by substracting crop\'s left,top)\n                current_boxes[:, :2] -= rect[:2]\n\n                current_boxes[:, 2:] = np.minimum(current_boxes[:, 2:],\n                                                  rect[2:])\n                # adjust to crop (by substracting crop\'s left,top)\n                current_boxes[:, 2:] -= rect[:2]\n\n                return current_image, current_boxes, current_labels\n\n\nclass Expand(object):\n    def __init__(self, mean):\n        self.mean = mean\n\n    def __call__(self, image, boxes, labels):\n        if random.randint(2):\n            return image, boxes, labels\n\n        height, width, depth = image.shape\n        ratio = random.uniform(1, 4)\n        left = random.uniform(0, width*ratio - width)\n        top = random.uniform(0, height*ratio - height)\n\n        expand_image = np.zeros(\n            (int(height*ratio), int(width*ratio), depth),\n            dtype=image.dtype)\n        expand_image[:, :, :] = self.mean\n        expand_image[int(top):int(top + height),\n                     int(left):int(left + width)] = image\n        image = expand_image\n\n        boxes = boxes.copy()\n        boxes[:, :2] += (int(left), int(top))\n        boxes[:, 2:] += (int(left), int(top))\n\n        return image, boxes, labels\n\n\nclass RandomMirror(object):\n    def __call__(self, image, boxes, classes):\n        _, width, _ = image.shape\n        if random.randint(2):\n            image = image[:, ::-1]\n            boxes = boxes.copy()\n            boxes[:, 0::2] = width - boxes[:, 2::-2]\n        return image, boxes, classes\n\n\nclass SwapChannels(object):\n    """"""Transforms a tensorized image by swapping the channels in the order\n     specified in the swap tuple.\n    Args:\n        swaps (int triple): final order of channels\n            eg: (2, 1, 0)\n    """"""\n\n    def __init__(self, swaps):\n        self.swaps = swaps\n\n    def __call__(self, image):\n        """"""\n        Args:\n            image (Tensor): image tensor to be transformed\n        Return:\n            a tensor with channels swapped according to swap\n        """"""\n        # if torch.is_tensor(image):\n        #     image = image.data.cpu().numpy()\n        # else:\n        #     image = np.array(image)\n        image = image[:, :, self.swaps]\n        return image\n\n\nclass PhotometricDistort(object):\n    def __init__(self):\n        self.pd = [\n            RandomContrast(),\n            ConvertColor(transform=\'HSV\'),\n            RandomSaturation(),\n            RandomHue(),\n            ConvertColor(current=\'HSV\', transform=\'BGR\'),\n            RandomContrast()\n        ]\n        self.rand_brightness = RandomBrightness()\n        self.rand_light_noise = RandomLightingNoise()\n\n    def __call__(self, image, boxes, labels):\n        im = image.copy()\n        im, boxes, labels = self.rand_brightness(im, boxes, labels)\n        if random.randint(2):\n            distort = Compose(self.pd[:-1])\n        else:\n            distort = Compose(self.pd[1:])\n        im, boxes, labels = distort(im, boxes, labels)\n        return self.rand_light_noise(im, boxes, labels)\n\n\nclass SSDAugmentation(object):\n    def __init__(self, size=300, mean=(104, 117, 123)):\n        self.mean = mean\n        self.size = size\n        self.augment = Compose([\n            ConvertFromInts(),\n            ToAbsoluteCoords(),\n            PhotometricDistort(),\n            Expand(self.mean),\n            RandomSampleCrop(),\n            RandomMirror(), # \xe9\x9a\x8f\xe6\x9c\xba\xe9\x95\x9c\xe5\x83\x8f\n            ToPercentCoords(),\n            Resize(self.size),\n            SubtractMeans(self.mean)\n        ])\n\n    def __call__(self, img, boxes, labels):\n        return self.augment(img, boxes, labels)\n'"
SSD_pytorch/utils/config.py,0,"b'# -*- coding:utf-8 -*-\n# power by Mr.Li\n# \xe8\xae\xbe\xe7\xbd\xae\xe9\xbb\x98\xe8\xae\xa4\xe5\x8f\x82\xe6\x95\xb0\nimport os.path\nclass DefaultConfig():\n    env = \'SSD_\'  # visdom \xe7\x8e\xaf\xe5\xa2\x83\xe7\x9a\x84\xe5\x90\x8d\xe5\xad\x97\n    visdom=True # \xe6\x98\xaf\xe5\x90\xa6\xe5\x8f\xaf\xe8\xa7\x86\xe5\x8c\x96\n    # \xe7\x9b\xae\xe5\x89\x8d\xe6\x94\xaf\xe6\x8c\x81\xe7\x9a\x84\xe7\xbd\x91\xe7\xbb\x9c\n    model = \'vgg16\'\n\n\n    voc_data_root=\'/home/bobo/data/VOCdevkit/\'  # VOC\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe6\xa0\xb9\xe7\x9b\xae\xe5\xbd\x95,\xe8\xaf\xa5\xe6\x96\x87\xe4\xbb\xb6\xe5\xa4\xb9\xe4\xb8\x8b\xe6\x9c\x89\xe4\xb8\xa4\xe4\xb8\xaa\xe5\xad\x90\xe6\x96\x87\xe4\xbb\xb6\xe5\xa4\xb9\xe3\x80\x82\xe4\xb8\x80\xe4\xb8\xaa\xe5\x8f\xabVOC2007,\xe4\xb8\x80\xe4\xb8\xaa\xe5\x8f\xabVOC2012\n\n    # \xe5\x9f\xba\xe7\xa1\x80\xe7\xbd\x91\xe7\xbb\x9c\xef\xbc\x8c\xe5\x8d\xb3\xe7\x89\xb9\xe5\xbe\x81\xe6\x8f\x90\xe5\x8f\x96\xe7\xbd\x91\xe7\xbb\x9c\xef\xbc\x88\xe5\x8e\xbb\xe6\x8e\x89\xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe7\x9a\x84\xe9\xa2\x84\xe8\xae\xad\xe7\xbb\x83\xe6\xa8\xa1\xe5\x9e\x8bvgg16\xef\xbc\x89\n    basenet=\'/home/bobo/windowsPycharmProject/SSD_pytorch/checkpoint/vgg16_reducedfc.pth\'  #\xe5\xba\x94\xe4\xb8\xba\xe5\x85\xa8\xe8\xb7\xaf\xe5\xbe\x84 \xe9\xa2\x84\xe8\xae\xad\xe7\xbb\x83\xe5\xa5\xbd\xe7\x9a\x84\xe5\x8e\xbb\xe6\x8e\x89\xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe5\xb1\x82\xe7\x9a\x84vgg16\xe6\xa8\xa1\xe5\x9e\x8b\n    batch_size = 32  # \xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\xe7\x9a\x84batch size\n    start_iter=0  #\xe8\xae\xad\xe7\xbb\x83\xe4\xbb\x8e\xe7\xac\xac\xe5\x87\xa0\xe4\xb8\xaaitem\xe5\xbc\x80\xe5\xa7\x8b\n    num_workers = 4  # \xe5\x8a\xa0\xe8\xbd\xbd\xe6\x95\xb0\xe6\x8d\xae\xe6\x97\xb6\xe7\x9a\x84\xe7\xba\xbf\xe7\xa8\x8b\xe6\x95\xb0\n    use_gpu = True  # user GPU or not\n    lr = 0.001  # \xe5\x88\x9d\xe5\xa7\x8b\xe7\x9a\x84\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\n    momentum=0.9 #\xe4\xbc\x98\xe5\x8c\x96\xe5\x99\xa8\xe7\x9a\x84\xe5\x8a\xa8\xe9\x87\x8f\xe5\x80\xbc\n    weight_decay=5e-4 #\xe9\x9a\x8f\xe6\x9c\xba\xe6\xa2\xaf\xe5\xba\xa6\xe4\xb8\x8b\xe9\x99\x8dSGD\xe7\x9a\x84\xe6\x9d\x83\xe9\x87\x8d\xe8\xa1\xb0\xe5\x87\x8f\n    gamma=0.1  # Gamma update for SGD  \xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\xe8\xb0\x83\xe6\x95\xb4\xe5\x8f\x82\xe6\x95\xb0\n\n    checkpoint_root =\'/home/bobo/windowsPycharmProject/SSD_pytorch/checkpoint/\' #\xe4\xbf\x9d\xe5\xad\x98\xe6\xa8\xa1\xe5\x9e\x8b\xe7\x9a\x84\xe7\x9b\xae\xe5\xbd\x95\n    # load_model_path = None  # \xe5\x8a\xa0\xe8\xbd\xbd\xe9\xa2\x84\xe8\xae\xad\xe7\xbb\x83\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\xe7\x9a\x84\xe8\xb7\xaf\xe5\xbe\x84\xef\xbc\x8c\xe4\xb8\xbaNone\xe4\xbb\xa3\xe8\xa1\xa8\xe4\xb8\x8d\xe5\x8a\xa0\xe8\xbd\xbd\n    load_model_path =\'/home/bobo/windowsPycharmProject/SSD_pytorch/checkpoint/ssd300_COCO_100000.pth\'\n    # load_model_path=\'C:\\\\Users\\\\Administrator\\\\Desktop\\\\ssd300_COCO_10000.pth\'\n\n\n    # gets home dir cross platform\n    HOME = os.path.expanduser(""~"")\n    # \xe4\xbd\xbf\xe8\xbe\xb9\xe7\x95\x8c\xe6\xa1\x86\xe6\xbc\x82\xe4\xba\xae\n    COLORS = ((255, 0, 0, 128), (0, 255, 0, 128), (0, 0, 255, 128),\n              (0, 255, 255, 128), (255, 0, 255, 128), (255, 255, 0, 128))\n    MEANS = (104, 117, 123)\n    # SSD300 \xe9\x85\x8d\xe7\xbd\xae\n    voc = {\n        \'num_classes\': 21,  # \xe5\x88\x86\xe7\xb1\xbb\xe7\xb1\xbb\xe5\x88\xab20+\xe8\x83\x8c\xe6\x99\xaf1\n        \'lr_steps\': (80000, 100000, 120000),\n        \'max_iter\': 120000,  # \xe8\xbf\xad\xe4\xbb\xa3\xe6\xac\xa1\xe6\x95\xb0\n        \'feature_maps\': [38, 19, 10, 5, 3, 1],\n        \'min_dim\': 300,  # \xe5\xbd\x93\xe5\x89\x8dSSD300\xe5\x8f\xaa\xe6\x94\xaf\xe6\x8c\x81\xe5\xa4\xa7\xe5\xb0\x8f300\xc3\x97300\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe8\xae\xad\xe7\xbb\x83\n        \'steps\': [8, 16, 32, 64, 100, 300],  # \xe6\x84\x9f\xe5\x8f\x97\xe9\x87\x8e\xef\xbc\x8c\xe7\x9b\xb8\xe5\xaf\xb9\xe4\xba\x8e\xe5\x8e\x9f\xe5\x9b\xbe\xe7\xbc\xa9\xe5\xb0\x8f\xe7\x9a\x84\xe5\x80\x8d\xe6\x95\xb0\n        \'min_sizes\': [30, 60, 111, 162, 213, 264],\n        \'max_sizes\': [60, 111, 162, 213, 264, 315],\n        \'aspect_ratios\': [[2], [2, 3], [2, 3], [2, 3], [2], [2]],\n        \'variance\': [0.1, 0.2],  # \xe6\x96\xb9\xe5\xb7\xae\n        \'clip\': True,\n        \'name\': \'VOC\',\n    }\n\n    # \xe9\xaa\x8c\xe8\xaf\x81\n    confidence_threshold=0.01   # \xe6\xa3\x80\xe6\xb5\x8b\xe7\xbd\xae\xe4\xbf\xa1\xe5\xba\xa6\xe9\x98\x88\xe5\x80\xbc  or 0.05\n    top_k=5           # \xe8\xbf\x9b\xe4\xb8\x80\xe6\xad\xa5\xe9\x99\x90\xe5\x88\xb6\xe8\xa6\x81\xe8\xa7\xa3\xe6\x9e\x90\xe7\x9a\x84\xe9\xa2\x84\xe6\xb5\x8b\xe6\x95\xb0\xe9\x87\x8f\n    cleanup=True               # \xe6\xb8\x85\xe9\x99\xa4\xe5\xb9\xb6\xe5\x88\xa0\xe9\x99\xa4eval\xe5\x90\x8e\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x9c\xe6\x96\x87\xe4\xbb\xb6\n    temp= \'/home/bobo/windowsPycharmProject/SSD_pytorch/temp\'   #\xe4\xbf\x9d\xe5\xad\x98\xe9\xaa\x8c\xe8\xaf\x81\xe7\x9a\x84\xe4\xb8\xb4\xe6\x97\xb6\xe6\x96\x87\xe4\xbb\xb6\n    annopath = os.path.join(voc_data_root, \'VOC2007\', \'Annotations\', \'%s.xml\')\n    imgpath = os.path.join(voc_data_root, \'VOC2007\', \'JPEGImages\', \'%s.jpg\')\n    imgsetpath = os.path.join(voc_data_root, \'VOC2007\', \'ImageSets\',\n                              \'Main\', \'{:s}.txt\')\n\n    #\xe6\xb5\x8b\xe8\xaf\x95\n    temp_test=\'/home/bobo/windowsPycharmProject/SSD_pytorch/temp/\' # \xe4\xbf\x9d\xe5\xad\x98\xe6\xb5\x8b\xe8\xaf\x95\xe9\x9b\x86\xef\xbc\x88VOC2007\xe6\xb5\x8b\xe8\xaf\x95\xe9\x9b\x86\xef\xbc\x89\xe7\x9a\x84\xe7\xbd\x91\xe7\xbb\x9c\xe9\xa2\x84\xe6\xb5\x8b\xe7\xbb\x93\xe6\x9e\x9c\n\n    #\xe9\xa2\x84\xe6\xb5\x8b\xef\xbc\x8c\xe5\x8f\xaf\xe8\xa7\x86\xe5\x8c\x96\xe4\xb8\x80\xe5\xbc\xa0\xe9\xa2\x84\xe6\xb5\x8b\xe5\x9b\xbe\xe7\x89\x87\n    test_img=\'/home/bobo/windowsPycharmProject/SSD_pytorch/temp/test.png\'\n\n\n#\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe8\xaf\xa5\xe7\xb1\xbb\xe7\x9a\x84\xe4\xb8\x80\xe4\xb8\xaa\xe5\xaf\xb9\xe8\xb1\xa1\nopt=DefaultConfig()'"
SSD_pytorch/utils/eval_untils.py,3,"b'from SSD_pytorch.data import *\nfrom SSD_pytorch.data import VOC_CLASSES as VOC_CLASSES\nfrom SSD_pytorch.utils.config import opt\nimport os\nimport sys\nimport pickle\nif sys.version_info[0] == 2:\n    import xml.etree.cElementTree as ET\nelse:\n    import xml.etree.ElementTree as ET\n\ndef evaluate_detections(box_list, output_dir, dataset):\n    write_voc_results_file(box_list, dataset)\n    do_python_eval(output_dir)\n\n\ndef write_voc_results_file(all_boxes, dataset):\n    for cls_ind, cls in enumerate(VOC_CLASSES):\n        print(\'Writing {:s} VOC results file\'.format(cls))\n        filename = get_voc_results_file_template(\'test\', cls)\n        with open(filename, \'wt\') as f:\n            for im_ind, index in enumerate(dataset.ids):\n                dets = all_boxes[cls_ind+1][im_ind]\n                if dets == []:\n                    continue\n                # the VOCdevkit expects 1-based indices\n                for k in range(dets.shape[0]):\n                    f.write(\'{:s} {:.3f} {:.1f} {:.1f} {:.1f} {:.1f}\\n\'.\n                            format(index[1], dets[k, -1],\n                                   dets[k, 0] + 1, dets[k, 1] + 1,\n                                   dets[k, 2] + 1, dets[k, 3] + 1))\n\ndef get_voc_results_file_template(image_set, cls):\n    # VOCdevkit/VOC2007/results/det_test_aeroplane.txt\n    filename = \'det_\' + image_set + \'_%s.txt\' % (cls)\n    #\xe6\xb5\x8b\xe8\xaf\x95\xe9\x9b\x86\xe4\xb8\xbaVOC2007\xe7\x9a\x84test\n    filedir = os.path.join(opt.voc_data_root+\'/VOC2007\', \'results\')\n    if not os.path.exists(filedir):\n        os.makedirs(filedir)\n    path = os.path.join(filedir, filename)\n    return path\n\ndef do_python_eval(output_dir=\'output\', use_07=True):\n    cachedir = os.path.join(opt.voc_data_root+\'/VOC2007\', \'annotations_cache\')\n    aps = []\n    # The PASCAL VOC metric changed in 2010\n    use_07_metric = use_07\n    print(\'VOC07 metric? \' + (\'Yes\' if use_07_metric else \'No\'))\n    if not os.path.isdir(output_dir):\n        os.mkdir(output_dir)\n    for i, cls in enumerate(VOC_CLASSES):\n        filename = get_voc_results_file_template(\'test\', cls)\n        rec, prec, ap = voc_eval(\n           filename, opt.annopath, opt.imgsetpath.format(\'test\'), cls, cachedir,\n           ovthresh=0.5, use_07_metric=use_07_metric)\n        aps += [ap]\n        print(\'AP for {} = {:.4f}\'.format(cls, ap))\n        with open(os.path.join(output_dir, cls + \'_pr.pkl\'), \'wb\') as f:\n            pickle.dump({\'rec\': rec, \'prec\': prec, \'ap\': ap}, f)\n    print(\'Mean AP = {:.4f}\'.format(np.mean(aps)))\n    print(\'~~~~~~~~\')\n    print(\'Results:\')\n    for ap in aps:\n        print(\'{:.3f}\'.format(ap))\n    print(\'{:.3f}\'.format(np.mean(aps)))\n    print(\'~~~~~~~~\')\n    print(\'\')\n    print(\'--------------------------------------------------------------\')\n    print(\'Results computed with the **unofficial** Python eval code.\')\n    print(\'Results should be very close to the official MATLAB eval code.\')\n    print(\'--------------------------------------------------------------\')\n\ndef voc_eval(detpath,\n             annopath,\n             imagesetfile,\n             classname,\n             cachedir,\n             ovthresh=0.5,\n             use_07_metric=True):\n    """"""rec, prec, ap = voc_eval(detpath,\n                           annopath,\n                           imagesetfile,\n                           classname,\n                           [ovthresh],\n                           [use_07_metric])\nTop level function that does the PASCAL VOC evaluation.\ndetpath: Path to detections\n   detpath.format(classname) should produce the detection results file.\nannopath: Path to annotations\n   annopath.format(imagename) should be the xml annotations file.\nimagesetfile: Text file containing the list of images, one image per line.\nclassname: Category name (duh)\ncachedir: Directory for caching the annotations\n[ovthresh]: Overlap threshold (default = 0.5)\n[use_07_metric]: Whether to use VOC07\'s 11 point AP computation\n   (default True)\n""""""\n# assumes detections are in detpath.format(classname)\n# assumes annotations are in annopath.format(imagename)\n# assumes imagesetfile is a text file with each line an image name\n# cachedir caches the annotations in a pickle file\n# first load gt\n    if not os.path.isdir(cachedir):\n        os.mkdir(cachedir)\n    cachefile = os.path.join(cachedir, \'annots.pkl\')\n    # read list of images\n    with open(imagesetfile, \'r\') as f:\n        lines = f.readlines()\n    imagenames = [x.strip() for x in lines]\n    if not os.path.isfile(cachefile):\n        # load annots\n        recs = {}\n        for i, imagename in enumerate(imagenames):\n            recs[imagename] = parse_rec(annopath % (imagename))\n            if i % 100 == 0:\n                print(\'Reading annotation for {:d}/{:d}\'.format(\n                   i + 1, len(imagenames)))\n        # save\n        print(\'Saving cached annotations to {:s}\'.format(cachefile))\n        with open(cachefile, \'wb\') as f:\n            pickle.dump(recs, f)\n    else:\n        # load\n        with open(cachefile, \'rb\') as f:\n            recs = pickle.load(f)\n\n    # extract gt objects for this class\n    class_recs = {}\n    npos = 0\n    for imagename in imagenames:\n        R = [obj for obj in recs[imagename] if obj[\'name\'] == classname]\n        bbox = np.array([x[\'bbox\'] for x in R])\n        difficult = np.array([x[\'difficult\'] for x in R]).astype(np.bool)\n        det = [False] * len(R)\n        npos = npos + sum(~difficult)\n        class_recs[imagename] = {\'bbox\': bbox,\n                                 \'difficult\': difficult,\n                                 \'det\': det}\n\n    # read dets\n    detfile = detpath.format(classname)\n    with open(detfile, \'r\') as f:\n        lines = f.readlines()\n    if any(lines) == 1:\n\n        splitlines = [x.strip().split(\' \') for x in lines]\n        image_ids = [x[0] for x in splitlines]\n        confidence = np.array([float(x[1]) for x in splitlines])\n        BB = np.array([[float(z) for z in x[2:]] for x in splitlines])\n\n        # sort by confidence\n        sorted_ind = np.argsort(-confidence)\n        sorted_scores = np.sort(-confidence)\n        BB = BB[sorted_ind, :]\n        image_ids = [image_ids[x] for x in sorted_ind]\n\n        # go down dets and mark TPs and FPs\n        nd = len(image_ids)\n        tp = np.zeros(nd)\n        fp = np.zeros(nd)\n        for d in range(nd):\n            R = class_recs[image_ids[d]]\n            bb = BB[d, :].astype(float)\n            ovmax = -np.inf\n            BBGT = R[\'bbox\'].astype(float)\n            if BBGT.size > 0:\n                # compute overlaps\n                # intersection\n                ixmin = np.maximum(BBGT[:, 0], bb[0])\n                iymin = np.maximum(BBGT[:, 1], bb[1])\n                ixmax = np.minimum(BBGT[:, 2], bb[2])\n                iymax = np.minimum(BBGT[:, 3], bb[3])\n                iw = np.maximum(ixmax - ixmin, 0.)\n                ih = np.maximum(iymax - iymin, 0.)\n                inters = iw * ih\n                uni = ((bb[2] - bb[0]) * (bb[3] - bb[1]) +\n                       (BBGT[:, 2] - BBGT[:, 0]) *\n                       (BBGT[:, 3] - BBGT[:, 1]) - inters)\n                overlaps = inters / uni\n                ovmax = np.max(overlaps)\n                jmax = np.argmax(overlaps)\n\n            if ovmax > ovthresh:\n                if not R[\'difficult\'][jmax]:\n                    if not R[\'det\'][jmax]:\n                        tp[d] = 1.\n                        R[\'det\'][jmax] = 1\n                    else:\n                        fp[d] = 1.\n            else:\n                fp[d] = 1.\n\n        # compute precision recall\n        fp = np.cumsum(fp)\n        tp = np.cumsum(tp)\n        rec = tp / float(npos)\n        # avoid divide by zero in case the first detection matches a difficult\n        # ground truth\n        prec = tp / np.maximum(tp + fp, np.finfo(np.float64).eps)\n        ap = voc_ap(rec, prec, use_07_metric)\n    else:\n        rec = -1.\n        prec = -1.\n        ap = -1.\n\n    return rec, prec, ap\n\ndef parse_rec(filename):\n    """""" Parse a PASCAL VOC xml file """"""\n    tree = ET.parse(filename)\n    objects = []\n    for obj in tree.findall(\'object\'):\n        obj_struct = {}\n        obj_struct[\'name\'] = obj.find(\'name\').text\n        obj_struct[\'pose\'] = obj.find(\'pose\').text\n        obj_struct[\'truncated\'] = int(obj.find(\'truncated\').text)\n        obj_struct[\'difficult\'] = int(obj.find(\'difficult\').text)\n        bbox = obj.find(\'bndbox\')\n        obj_struct[\'bbox\'] = [int(bbox.find(\'xmin\').text) - 1,\n                              int(bbox.find(\'ymin\').text) - 1,\n                              int(bbox.find(\'xmax\').text) - 1,\n                              int(bbox.find(\'ymax\').text) - 1]\n        objects.append(obj_struct)\n\n    return objects\n\ndef voc_ap(rec, prec, use_07_metric=True):\n    """""" ap = voc_ap(rec, prec, [use_07_metric])\n    Compute VOC AP given precision and recall.\n    If use_07_metric is true, uses the\n    VOC 07 11 point method (default:True).\n    """"""\n    if use_07_metric:\n        # 11 point metric\n        ap = 0.\n        for t in np.arange(0., 1.1, 0.1):\n            if np.sum(rec >= t) == 0:\n                p = 0\n            else:\n                p = np.max(prec[rec >= t])\n            ap = ap + p / 11.\n    else:\n        # correct AP calculation\n        # first append sentinel values at the end\n        mrec = np.concatenate(([0.], rec, [1.]))\n        mpre = np.concatenate(([0.], prec, [0.]))\n\n        # compute the precision envelope\n        for i in range(mpre.size - 1, 0, -1):\n            mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n\n        # to calculate area under PR curve, look for points\n        # where X axis (recall) changes value\n        i = np.where(mrec[1:] != mrec[:-1])[0]\n\n        # and sum (\\Delta recall) * prec\n        ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n    return ap'"
SSD_pytorch/utils/timer.py,0,"b'import time\nclass Timer(object):\n    """"""A simple timer.""""""\n    def __init__(self):\n        self.total_time = 0.\n        self.calls = 0\n        self.start_time = 0.\n        self.diff = 0.\n        self.average_time = 0.\n\n    def tic(self):\n        # using time.time instead of time.clock because time time.clock\n        # does not normalize for multithreading\n        self.start_time = time.time()\n\n    def toc(self, average=True):\n        self.diff = time.time() - self.start_time\n        self.total_time += self.diff\n        self.calls += 1\n        self.average_time = self.total_time / self.calls\n        if average:\n            return self.average_time\n        else:\n            return self.diff'"
SSD_pytorch/utils/visualize.py,6,"b""#!/usr/bin/python\n# -*- coding:utf-8 -*-\n# power by Mr.Li\nimport visdom\nimport time\nimport numpy as np\nimport torch\nclass Visualizer(object):\n    '''\n    \xe5\xb0\x81\xe8\xa3\x85\xe4\xba\x86visdom\xe7\x9a\x84\xe5\x9f\xba\xe6\x9c\xac\xe6\x93\x8d\xe4\xbd\x9c\xef\xbc\x8c\xe4\xbd\x86\xe6\x98\xaf\xe4\xbd\xa0\xe4\xbb\x8d\xe7\x84\xb6\xe5\x8f\xaf\xe4\xbb\xa5\xe9\x80\x9a\xe8\xbf\x87`self.vis.function`\n    \xe8\xb0\x83\xe7\x94\xa8\xe5\x8e\x9f\xe7\x94\x9f\xe7\x9a\x84visdom\xe6\x8e\xa5\xe5\x8f\xa3\n    '''\n    def __init__(self, env='default', **kwargs):\n        self.vis = visdom.Visdom(env=env, **kwargs)\n        # \xe7\x94\xbb\xe7\x9a\x84\xe7\xac\xac\xe5\x87\xa0\xe4\xb8\xaa\xe6\x95\xb0\xef\xbc\x8c\xe7\x9b\xb8\xe5\xbd\x93\xe4\xba\x8e\xe6\xa8\xaa\xe5\xba\xa7\xe6\xa0\x87\n        # \xe4\xbf\x9d\xe5\xad\x98\xef\xbc\x88\xe2\x80\x99loss',23\xef\xbc\x89 \xe5\x8d\xb3loss\xe7\x9a\x84\xe7\xac\xac23\xe4\xb8\xaa\xe7\x82\xb9\n        self.index = {}\n        self.log_text = ''\n    def reinit(self,env='default',**kwargs):\n        '''\n        \xe4\xbf\xae\xe6\x94\xb9visdom\xe7\x9a\x84\xe9\x85\x8d\xe7\xbd\xae  \xe9\x87\x8d\xe6\x96\xb0\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\n        '''\n        self.vis = visdom.Visdom(env=env,**kwargs)\n        return self\n    def plot_many(self, d):\n        '''\n        \xe4\xb8\x80\xe6\xac\xa1plot\xe5\xa4\x9a\xe4\xb8\xaa\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x9b\xbe\xe5\xbd\xa2\n        @params d: dict (name,value) i.e. ('loss',0.11)\n        '''\n        for k, v in d.items():\n            self.plot(k, v)\n    def img_many(self, d):\n        '''\n     \xe4\xb8\x80\xe6\xac\xa1\xe7\x94\xbb\xe5\xa4\x9a\xe4\xb8\xaa\xe5\x9b\xbe\xe5\x83\x8f\n        '''\n        for k, v in d.items():\n            self.img(k, v)\n    def plot(self, name, y,**kwargs):\n        '''\n        self.plot('loss',1.00)\n        '''\n        #\xe5\xbe\x97\xe5\x88\xb0\xe4\xb8\x8b\xe6\xa0\x87\xe5\xba\x8f\xe5\x8f\xb7\n        x = self.index.get(name, 0)\n        self.vis.line(Y=np.array([y]), X=np.array([x]),\n                      win=name,#\xe7\xaa\x97\xe5\x8f\xa3\xe5\x90\x8d\n                      opts=dict(title=name),\n                      update=None if x == 0 else 'append', #\xe6\x8c\x89\xe7\x85\xa7append\xe7\x9a\x84\xe7\x94\xbb\xe5\x9b\xbe\xe5\xbd\xa2\n                      **kwargs\n                      )\n        #\xe4\xb8\x8b\xe6\xa0\x87\xe7\xb4\xaf\xe5\x8a\xa01\n        self.index[name] = x + 1\n    def img(self, name, img_,**kwargs):\n        '''\n        self.img('input_img',t.Tensor(64,64))\n        self.img('input_imgs',t.Tensor(3,64,64))\n        self.img('input_imgs',t.Tensor(100,1,64,64))\n        self.img('input_imgs',t.Tensor(100,3,64,64),nrows=10)\n\n        \xef\xbc\x81\xef\xbc\x81\xef\xbc\x81don\xe2\x80\x98t ~~self.img('input_imgs',t.Tensor(100,64,64),nrows=10)~~\xef\xbc\x81\xef\xbc\x81\xef\xbc\x81\n        '''\n        self.vis.images(img_.cpu().numpy(),\n                       win=name,\n                       opts=dict(title=name),\n                       **kwargs\n                       )\n    def log(self,info,win='log_text'):\n        '''\n        self.log({'loss':1,'lr':0.0001})\n        \xe6\x89\x93\xe5\x8d\xb0\xe6\x97\xa5\xe5\xbf\x97\n        '''\n\n        self.log_text += ('[{time}] {info} <br>'.format(\n                            time=time.strftime('%m%d_%H%M%S'),\\\n                            info=info))\n        self.vis.text(self.log_text,win)\n    def __getattr__(self, name):\n        return getattr(self.vis, name)\n\n    def create_vis_plot(self,_xlabel, _ylabel, _title, _legend):\n        viz = visdom.Visdom()\n        '''\n        \xe6\x96\xb0\xe5\xa2\x9e\xe5\x8f\xaf\xe8\xa7\x86\xe5\x8c\x96\xe5\x9b\xbe\xe5\xbd\xa2\n        '''\n        return viz.line(\n            X=torch.zeros((1,)).cpu(),\n            Y=torch.zeros((1, 3)).cpu(),\n            opts=dict(\n                xlabel=_xlabel,\n                ylabel=_ylabel,\n                title=_title,\n                legend=_legend\n            )\n        )\n\n    def update_vis_plot(self,iteration, loc, conf, window1, window2, update_type,\n                        epoch_size=1):\n        '''\n        \xe5\x8f\xaf\xe8\xa7\x86\xe5\x8c\x96\xe5\x9b\xbe\xe5\xbd\xa2\xe9\x87\x8c\xe6\x9b\xb4\xe6\x96\xb0\xe6\x95\xb0\xe6\x8d\xae\n        '''\n        viz = visdom.Visdom()\n        viz.line(\n            X=torch.ones((1, 3)).cpu() * iteration,\n            Y=torch.Tensor([loc, conf, loc + conf]).unsqueeze(0).cpu() / epoch_size,\n            win=window1,\n            update=update_type\n        )\n        # initialize epoch plot on first iteration\n\n        if iteration == 0:\n            viz.line(\n                X=torch.zeros((1, 3)).cpu(),\n                Y=torch.Tensor([loc, conf, loc + conf]).unsqueeze(0).cpu(),\n                win=window2,\n                update=True\n            )"""
UNet_pytorch/unet/__init__.py,0,b'from .unet_model import UNet\n'
UNet_pytorch/unet/unet_model.py,0,"b'# full assembly of the sub-parts to form the complete net\n\nfrom .unet_parts import *\n\nclass UNet(nn.Module):\n    def __init__(self, n_channels, n_classes):\n        super(UNet, self).__init__()\n        self.inc = inconv(n_channels, 64)  # \xe8\xbe\x93\xe5\x87\xba\xe5\xb1\x82\xef\xbc\x8cn_channels=3 \xe8\xbe\x93\xe5\x85\xa5\xe5\x9b\xbe\xe5\x83\x8f\xe4\xb8\xba3\xe9\x80\x9a\xe9\x81\x93\n        self.down1 = down(64, 128)\n        self.down2 = down(128, 256)\n        self.down3 = down(256, 512)\n        self.down4 = down(512, 512)\n        self.up1 = up(1024, 256)\n        self.up2 = up(512, 128)\n        self.up3 = up(256, 64)\n        self.up4 = up(128, 64)\n        # \xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x80\xe5\xb1\x82\xe7\x9a\x84\xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8\xe5\xa4\xa7\xe5\xb0\x8f\xe4\xb8\xba1*1\xef\xbc\x8c\xe5\xb0\x8664\xe9\x80\x9a\xe9\x81\x93\xe7\x9a\x84\xe7\x89\xb9\xe5\xbe\x81\xe5\x9b\xbe\xe8\xbd\xac\xe5\x8c\x96\xe4\xb8\xba\xe7\x89\xb9\xe5\xae\x9a\xe6\xb7\xb1\xe5\xba\xa6\xef\xbc\x88\xe5\x88\x86\xe7\xb1\xbb\xe6\x95\xb0\xe9\x87\x8f\xef\xbc\x8c\xe4\xba\x8c\xe5\x88\x86\xe7\xb1\xbb\xe4\xb8\xba2\xef\xbc\x89\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x9c\n        self.outc = outconv(64, n_classes) # \xe8\xbe\x93\xe5\x87\xba\xe5\xb1\x82\xef\xbc\x8cn_classes=1 \xe4\xba\x8c\xe5\x88\x86\xe7\xb1\xbb\n\n    def forward(self, x):\n        x1 = self.inc(x)\n        x2 = self.down1(x1)\n        x3 = self.down2(x2)\n        x4 = self.down3(x3)\n        x5 = self.down4(x4)\n        x = self.up1(x5, x4)\n        x = self.up2(x, x3)\n        x = self.up3(x, x2)\n        x = self.up4(x, x1)\n        x = self.outc(x)\n        return x\n'"
UNet_pytorch/unet/unet_parts.py,3,"b""# sub-parts of the U-Net model\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass double_conv(nn.Module):\n    '''(conv => BN => ReLU) * 2'''\n    def __init__(self, in_ch, out_ch):\n        super(double_conv, self).__init__()\n        self.conv = nn.Sequential(\n            # \xe6\xaf\x8f\xe6\xac\xa1\xe9\x87\x8d\xe5\xa4\x8d\xe4\xb8\xad\xe9\x83\xbd\xe6\x9c\x892\xe4\xb8\xaa\xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82\xef\xbc\x8c\xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8\xe5\xa4\xa7\xe5\xb0\x8f\xe5\x9d\x87\xe4\xb8\xba3*3\n            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        x = self.conv(x)\n        return x\n\n\nclass inconv(nn.Module):\n    def __init__(self, in_ch, out_ch):\n        # \xe8\xbe\x93\xe5\x85\xa5\xe5\xb1\x82\xef\xbc\x8cin_ch=3\xe8\xbe\x93\xe5\x85\xa5\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0, out_ch=64\xe8\xbe\x93\xe5\x87\xba\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\n        super(inconv, self).__init__()\n        self.conv = double_conv(in_ch, out_ch)\n\n    def forward(self, x):\n        x = self.conv(x)\n        return x\n\n\nclass down(nn.Module):\n    def __init__(self, in_ch, out_ch):\n        super(down, self).__init__()\n        # \xe8\xae\xba\xe6\x96\x87\xef\xbc\x9a\xe5\xae\x83\xe7\x9a\x84\xe6\x9e\xb6\xe6\x9e\x84\xe6\x98\xaf\xe4\xb8\x80\xe7\xa7\x8d\xe9\x87\x8d\xe5\xa4\x8d\xe7\xbb\x93\xe6\x9e\x84\xef\xbc\x8c\xe6\xaf\x8f\xe6\xac\xa1\xe9\x87\x8d\xe5\xa4\x8d\xe4\xb8\xad\xe9\x83\xbd\xe6\x9c\x892\xe4\xb8\xaa\xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82\xe5\x92\x8c\xe4\xb8\x80\xe4\xb8\xaapooling\xe5\xb1\x82\xef\xbc\x8c\xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82\xe4\xb8\xad\xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8\xe5\xa4\xa7\xe5\xb0\x8f\xe5\x9d\x87\xe4\xb8\xba3*3\xef\xbc\x8c\xe6\xbf\x80\xe6\xb4\xbb\xe5\x87\xbd\xe6\x95\xb0\xe4\xbd\xbf\xe7\x94\xa8ReLU\n        self.mpconv = nn.Sequential(\n            nn.MaxPool2d(2),\n            double_conv(in_ch, out_ch)\n        )\n\n    def forward(self, x):\n        x = self.mpconv(x)\n        return x\n\n\nclass up(nn.Module):\n    def __init__(self, in_ch, out_ch, bilinear=True):\n        super(up, self).__init__()\n\n        #  would be a nice idea if the upsampling could be learned too,\n        #  but my machine do not have enough memory to handle all those weights\n        # \xe9\xbb\x98\xe8\xae\xa4\xe4\xb8\xba\xe5\x85\xa8\xe9\x83\xa8\xe4\xb8\xba \xe4\xb8\x8a\xe9\x87\x87\xe6\xa0\xb7\xef\xbc\x88\xe4\xbd\x9c\xe8\x80\x85\xe5\x9b\xa0\xe5\x86\x85\xe5\xad\x98\xe4\xb8\x8d\xe8\xb6\xb3\xef\xbc\x89\xe3\x80\x82\n        # bilinear=False \xe6\x94\xb9\xe4\xb8\xba  \xe4\xbd\xbf\xe7\x94\xa8\xe5\x8f\x8d\xe5\x8d\xb7\xe7\xa7\xaf\xef\xbc\x88\xe8\xae\xba\xe6\x96\x87\xe6\x96\xb9\xe6\xb3\x95\xef\xbc\x89\xef\xbc\x8c\xe6\x95\x88\xe6\x9e\x9c\xe4\xbc\x9a\xe6\x9b\xb4\xe5\xa5\xbd\xef\xbc\x9f\n\n        # \xe4\xbd\xbf\xe7\x94\xa8\xe5\x8f\x8c\xe7\xba\xbf\xe6\x80\xa7\xe4\xb8\x8a\xe9\x87\x87\xe6\xa0\xb7\xe6\x9d\xa5\xe6\x94\xbe\xe5\xa4\xa7\xe8\xbe\x93\xe5\x85\xa5\n        if bilinear:  #  batchsize=10   scale=0.3  \xe6\x97\xb6  \xe5\x8d\xa0\xe7\x94\xa89047M\n            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n        # \xef\xbc\x88\xe8\xae\xba\xe6\x96\x87\xe6\x96\xb9\xe6\xb3\x95\xef\xbc\x89\xe4\xba\x8c\xe7\xbb\xb4\xe5\x8f\x8d\xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82 \xe5\x8f\x8d\xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82\xe5\x8f\xaf\xe4\xbb\xa5\xe7\x90\x86\xe8\xa7\xa3\xe4\xb8\xba\xe8\xbe\x93\xe5\x85\xa5\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe5\x92\x8c\xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8\xe7\x9a\x84\xe4\xbd\x8d\xe7\xbd\xae\xe5\x8f\x8d\xe8\xbd\xac\xe7\x9a\x84\xe5\x8d\xb7\xe7\xa7\xaf\xe6\x93\x8d\xe4\xbd\x9c. \xe5\x8f\x8d\xe5\x8d\xb7\xe7\xa7\xaf\xe6\x9c\x89\xe6\x97\xb6\xe5\x80\x99\xe4\xb9\x9f\xe4\xbc\x9a\xe8\xa2\xab\xe7\xbf\xbb\xe8\xaf\x91\xe6\x88\x90\xe8\xa7\xa3\xe5\x8d\xb7\xe7\xa7\xaf.\n        else:  # \xe8\xae\xba\xe6\x96\x87\xe6\x96\xb9\xe6\xb3\x95   batchsize=10   scale=0.3  \xe6\x97\xb6 \xe5\x8d\xa0\xe7\x94\xa89040M\n            self.up = nn.ConvTranspose2d(in_ch//2, in_ch//2, 2, stride=2)\n\n        self.conv = double_conv(in_ch, out_ch)\n\n    def forward(self, x1, x2):\n        x1 = self.up(x1)\n        diffX = x1.size()[2] - x2.size()[2]\n        diffY = x1.size()[3] - x2.size()[3]\n        x2 = F.pad(x2, (diffX // 2, int(diffX / 2),\n                        diffY // 2, int(diffY / 2)))\n        # \xe6\xb7\xb1\xe5\xba\xa6\xe7\x9b\xb8\xe5\x8a\xa0\n        x = torch.cat([x2, x1], dim=1)\n        x = self.conv(x)\n        return x\n\n\nclass outconv(nn.Module):\n    def __init__(self, in_ch, out_ch):\n        super(outconv, self).__init__()\n        # \xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x80\xe5\xb1\x82\xe7\x9a\x84\xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8\xe5\xa4\xa7\xe5\xb0\x8f\xe4\xb8\xba1*1\xef\xbc\x8c\xe5\xb0\x8664\xe9\x80\x9a\xe9\x81\x93\xe7\x9a\x84\xe7\x89\xb9\xe5\xbe\x81\xe5\x9b\xbe\xe8\xbd\xac\xe5\x8c\x96\xe4\xb8\xba\xe7\x89\xb9\xe5\xae\x9a\xe6\xb7\xb1\xe5\xba\xa6\xef\xbc\x88\xe5\x88\x86\xe7\xb1\xbb\xe6\x95\xb0\xe9\x87\x8f\xef\xbc\x8c\xe4\xba\x8c\xe5\x88\x86\xe7\xb1\xbb\xe4\xb8\xba2\xef\xbc\x89\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x9c\n        self.conv = nn.Conv2d(in_ch, out_ch, 1)\n\n    def forward(self, x):\n        x = self.conv(x)\n        return x\n"""
UNet_pytorch/utils/__init__.py,0,b'from .crf import *\nfrom .load import *\nfrom .utils import *\nfrom .data_vis import *\n'
UNet_pytorch/utils/config.py,0,"b""# -*- coding:utf-8 -*-\n# power by Mr.Li\n# \xe8\xae\xbe\xe7\xbd\xae\xe9\xbb\x98\xe8\xae\xa4\xe5\x8f\x82\xe6\x95\xb0\nimport os.path\nclass DefaultConfig_train():\n    epochs=5  #number of epochs\n    batchsize= 10  #batch size\n    lr=0.1  #learning rate\n    gpu=True  #use cudas\n    load=False  #load file model\n    scale=0.3    #downscaling factor of the images  \xe5\x9b\xbe\xe5\x83\x8f\xe8\xae\xad\xe7\xbb\x83\xe6\x97\xb6\xe7\xbc\xa9\xe5\xb0\x8f\xe5\x80\x8d\xe6\x95\xb0       \xe8\xaf\xa5\xe5\x80\xbc\xe5\xaf\xb9\xe5\x86\x85\xe5\xad\x98\xe5\xbd\xb1\xe5\x93\x8d\xe8\xbe\x83\xe5\xa4\xa7\xef\xbc\x88\xe4\xbb\x93\xe5\xba\x93\xe9\xbb\x98\xe8\xae\xa40.5\xef\xbc\x89\n\n    # \xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\n    dir_img = '/home/bobo/data/CarvanaImageMaskingChallenge_UNet/train/'\n    dir_mask = '/home/bobo/data/CarvanaImageMaskingChallenge_UNet/train_masks/'\n    dir_checkpoint = './checkpoints/'  # \xe6\xa8\xa1\xe5\x9e\x8b\xe4\xbf\x9d\xe5\xad\x98\xe4\xbd\x8d\xe7\xbd\xae\n\n    visdom=True   # \xe6\x98\xaf\xe5\x90\xa6\xe5\x8f\xaf\xe8\xa7\x86\xe5\x8c\x96\n\n    env = 'U-Net'  # visdom \xe7\x8e\xaf\xe5\xa2\x83\xe7\x9a\x84\xe5\x90\x8d\xe5\xad\x97\n    visdom = True  # \xe6\x98\xaf\xe5\x90\xa6\xe5\x8f\xaf\xe8\xa7\x86\xe5\x8c\x96\n    datesets_name='Carvana Image Masking Challenge'  # \xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe5\x90\x8d\xe7\xa7\xb0\n\n\n\n\n\n\n\nclass DefaultConfig_predict():\n    input='./intput.jpg'    #filenames of input images\n    output='./output.jpg'   #filenames of ouput images\n    model= './MODEL.pth'    # Specify the file in which is stored the model\n    cpu=False   #Do not use the cuda version of the net\n    scale=0.5    #Scale factor for the input images\n    mask_threshold=0.5   #Minimum probability value to consider a mask pixel white\n    no_crf=False    #Do not use dense CRF postprocessing\n    no_save=False   #Do not save the output masks\n    viz=False    #Visualize the images as they are processed\n#\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe8\xaf\xa5\xe7\xb1\xbb\xe7\x9a\x84\xe4\xb8\x80\xe4\xb8\xaa\xe5\xaf\xb9\xe8\xb1\xa1\nopt_train=DefaultConfig_train()\n\nopt_predict=DefaultConfig_predict()\n\n"""
UNet_pytorch/utils/crf.py,0,"b'import numpy as np\nimport pydensecrf.densecrf as dcrf\n\ndef dense_crf(img, output_probs):\n    h = output_probs.shape[0]\n    w = output_probs.shape[1]\n\n    output_probs = np.expand_dims(output_probs, 0)\n    output_probs = np.append(1 - output_probs, output_probs, axis=0)\n\n    d = dcrf.DenseCRF2D(w, h, 2)\n    U = -np.log(output_probs)\n    U = U.reshape((2, -1))\n    U = np.ascontiguousarray(U)\n    img = np.ascontiguousarray(img)\n\n    d.setUnaryEnergy(U)\n\n    d.addPairwiseGaussian(sxy=20, compat=3)\n    d.addPairwiseBilateral(sxy=30, srgb=20, rgbim=img, compat=10)\n\n    Q = d.inference(5)\n    Q = np.argmax(np.array(Q), axis=0).reshape((h, w))\n\n    return Q\n'"
UNet_pytorch/utils/data_vis.py,0,"b""import matplotlib.pyplot as plt\n\ndef plot_img_and_mask(img, mask):\n    fig = plt.figure()\n    a = fig.add_subplot(1, 2, 1)\n    a.set_title('Input image')\n    plt.imshow(img)\n\n    b = fig.add_subplot(1, 2, 2)\n    b.set_title('Output mask')\n    plt.imshow(mask)\n    plt.show()"""
UNet_pytorch/utils/load.py,0,"b'#\n# load.py : utils on generators / lists of ids to transform from strings to\n#           cropped images and masks\n\nimport os\n\nimport numpy as np\nfrom PIL import Image\n\nfrom .utils import resize_and_crop, get_square, normalize, hwc_to_chw\n\n\ndef get_ids(dir):\n    """"""Returns a list of the ids in the directory""""""\n    # eg\xef\xbc\x9af[:-4]\xe6\x98\xaf\xe4\xb8\xba\xe4\xba\x86\xe5\x8e\xbb\xe6\x8e\x89 .jpg \xe5\x90\x8e\xe7\xbc\x80\xe3\x80\x82\xe7\xbb\x93\xe6\x9e\x9c\xe5\x8f\xaa\xe4\xb8\xba \xe7\x85\xa7\xe7\x89\x87\xe5\x90\x8d\xe7\xa7\xb0\xef\xbc\x8c\xe6\x97\xa0\xe5\x90\x8e\xe7\xbc\x80\xe3\x80\x82\n    return (f[:-4] for f in os.listdir(dir))\n\n\ndef split_ids(ids, n=2):\n    """"""Split each id in n, creating n tuples (id, k) for each id""""""\n    return ((id, i) for i in range(n) for id in ids)\n\n\ndef to_cropped_imgs(ids, dir, suffix, scale):\n    """"""From a list of tuples, returns the correct cropped img""""""\n    #\xe8\xbf\x94\xe5\x9b\x9e  tuples\xef\xbc\x8c\xef\xbc\x88img\xe7\x9a\x84resize\xe5\x90\x8e\xe7\x9a\x84tensor,\xe5\xba\x8f\xe5\x8f\xb7\xef\xbc\x89\n    for id, pos in ids:\n        im = resize_and_crop(Image.open(dir + id + suffix), scale=scale)\n        # get_square: \xe5\xbd\x93pos\xe4\xb8\xba0 \xe6\x97\xb6\xef\xbc\x8c\xe8\xa3\x81\xe5\x89\xaa\xe5\xae\xbd\xe5\xba\xa6\xef\xbc\x8c\xe5\xbe\x97\xe5\x88\xb0\xe5\xb7\xa6\xe8\xbe\xb9\xe9\x83\xa8\xe5\x88\x86\xe5\x9b\xbe\xe7\x89\x87[384,384,3]   \xe5\xbd\x93pos\xe4\xb8\xba1 \xe6\x97\xb6\xef\xbc\x8c\xe8\xa3\x81\xe5\x89\xaa\xe5\xae\xbd\xe5\xba\xa6\xef\xbc\x8c\xe5\xbe\x97\xe5\x88\xb0\xe5\x8f\xb3\xe8\xbe\xb9\xe9\x83\xa8\xe5\x88\x86\xe5\x9b\xbe\xe7\x89\x87[384,190,3]\n        yield get_square(im, pos)\n\ndef get_imgs_and_masks(ids, dir_img, dir_mask, scale):\n    \'\'\'\n    :param ids:\n    :param dir_img: \xe5\x9b\xbe\xe7\x89\x87\xe8\xb7\xaf\xe5\xbe\x84\n    :param dir_mask: mask\xe5\x9b\xbe\xe7\x89\x87\xe8\xb7\xaf\xe5\xbe\x84\n    :param scale: \xe5\x9b\xbe\xe5\x83\x8f\xe8\xae\xad\xe7\xbb\x83\xe6\x97\xb6\xe7\xbc\xa9\xe5\xb0\x8f\xe5\x80\x8d\xe6\x95\xb0\n    :return:all the couples (img, mask)\n    \'\'\'\n    """"""Return all the couples (img, mask)""""""\n\n    # \xe8\xaf\xbb\xe5\x8f\x96\xe5\x9b\xbe\xe7\x89\x87\xef\xbc\x8c\xe5\xb9\xb6\xe6\x8c\x89\xe7\x85\xa7scale\xe8\xbf\x9b\xe8\xa1\x8cresize\n    imgs = to_cropped_imgs(ids, dir_img, \'.jpg\', scale)\n\n    # need to transform from HWC to CHW  \xe8\xbd\xac\xe5\x8c\x96\xef\xbc\x88\xe9\xab\x98H\xe3\x80\x81\xe5\xae\xbdW\xe3\x80\x81\xe9\x80\x9a\xe9\x81\x93C\xef\xbc\x89\xe4\xb8\xba\xef\xbc\x88\xe9\x80\x9a\xe9\x81\x93C\xe3\x80\x81\xe9\xab\x98H\xe3\x80\x81\xe5\xae\xbdW\xef\xbc\x89\n    imgs_switched = map(hwc_to_chw, imgs)\n    # \xe5\xbd\x92\xe4\xb8\x80\xe5\x8c\x96\xef\xbc\x88\xe5\x80\xbc\xe8\xbd\xac\xe5\x8c\x96\xe5\x88\xb00-1\xe4\xb9\x8b\xe9\x97\xb4\xef\xbc\x89\n    imgs_normalized = map(normalize, imgs_switched)\n\n    masks = to_cropped_imgs(ids, dir_mask, \'_mask.gif\', scale)\n    # list( rezise\xe4\xb8\x94\xe7\xbb\x8f\xe8\xbf\x87\xe8\xbd\xac\xe5\x8c\x96\xe5\x92\x8c\xe5\xbd\x92\xe4\xb8\x80\xe5\x8c\x96\xe5\x90\x8e\xe7\x9a\x84\xe5\x9b\xbe\xe5\x83\x8ftensor,resize\xe5\x90\x8e\xe7\x9a\x84mask\xe5\x9b\xbe\xe5\x83\x8ftensor)\n    return zip(imgs_normalized, masks)\n\n\ndef get_full_img_and_mask(id, dir_img, dir_mask):\n    im = Image.open(dir_img + id + \'.jpg\')\n    mask = Image.open(dir_mask + id + \'_mask.gif\')\n    return np.array(im), np.array(mask)\n'"
UNet_pytorch/utils/utils.py,0,"b'import random\nimport numpy as np\n\n\ndef get_square(img, pos):\n    """"""Extract a left or a right square from ndarray shape : (H, W, C))""""""\n    h = img.shape[0]\n    if pos == 0:\n        return img[:, :h]\n    else:\n        return img[:, -h:]\n\ndef split_img_into_squares(img):\n    return get_square(img, 0), get_square(img, 1)\n\ndef hwc_to_chw(img):\n    return np.transpose(img, axes=[2, 0, 1])\n\ndef resize_and_crop(pilimg, scale=0.5, final_height=None):\n    w = pilimg.size[0]\n    h = pilimg.size[1]\n    newW = int(w * scale)\n    newH = int(h * scale)\n\n    if not final_height:\n        diff = 0\n    else:\n        diff = newH - final_height\n\n    img = pilimg.resize((newW, newH))\n    # crop \xe4\xbb\x8e\xe5\x9b\xbe\xe5\x83\x8f\xe4\xb8\xad\xe6\x8f\x90\xe5\x8f\x96\xe5\x87\xba\xe6\x9f\x90\xe4\xb8\xaa\xe7\x9f\xa9\xe5\xbd\xa2\xe5\xa4\xa7\xe5\xb0\x8f\xe7\x9a\x84\xe5\x9b\xbe\xe5\x83\x8f\xe3\x80\x82\xe5\xae\x83\xe6\x8e\xa5\xe6\x94\xb6\xe4\xb8\x80\xe4\xb8\xaa\xe5\x9b\x9b\xe5\x85\x83\xe7\xb4\xa0\xe7\x9a\x84\xe5\x85\x83\xe7\xbb\x84\xe4\xbd\x9c\xe4\xb8\xba\xe5\x8f\x82\xe6\x95\xb0\xef\xbc\x8c(\xe8\xb5\xb7\xe5\xa7\x8b\xe7\x82\xb9\xe7\x9a\x84\xe6\xa8\xaa\xe5\x9d\x90\xe6\xa0\x87\xef\xbc\x8c\xe8\xb5\xb7\xe5\xa7\x8b\xe7\x82\xb9\xe7\x9a\x84\xe7\xba\xb5\xe5\x9d\x90\xe6\xa0\x87\xef\xbc\x8c\xe5\xae\xbd\xe5\xba\xa6\xef\xbc\x8c\xe9\xab\x98\xe5\xba\xa6\xef\xbc\x89\xef\xbc\x8c\xe5\x9d\x90\xe6\xa0\x87\xe7\xb3\xbb\xe7\xbb\x9f\xe7\x9a\x84\xe5\x8e\x9f\xe7\x82\xb9\xef\xbc\x880, 0\xef\xbc\x89\xe6\x98\xaf\xe5\xb7\xa6\xe4\xb8\x8a\xe8\xa7\x92\xe3\x80\x82\n    img = img.crop((0, diff // 2, newW, newH - diff // 2))\n    return np.array(img, dtype=np.float32)\n\ndef batch(iterable, batch_size):\n    """"""Yields lists by batch""""""\n    b = []\n    for i, t in enumerate(iterable):\n        b.append(t)\n        if (i + 1) % batch_size == 0:\n            yield b\n            b = []\n\n    if len(b) > 0:\n        yield b\n\ndef split_train_val(dataset, val_percent=0.05):\n    dataset = list(dataset)\n    length = len(dataset)\n    n = int(length * val_percent)\n    random.shuffle(dataset)\n    return {\'train\': dataset[:-n], \'val\': dataset[-n:]}\n\n\ndef normalize(x):\n    return x / 255\n\ndef merge_masks(img1, img2, full_w):\n    h = img1.shape[0]\n\n    new = np.zeros((h, full_w), np.float32)\n    new[:, :full_w // 2 + 1] = img1[:, :full_w // 2 + 1]\n    new[:, full_w // 2 + 1:] = img2[:, -(full_w // 2 - 1):]\n\n    return new\n\n\n# credits to https://stackoverflow.com/users/6076729/manuel-lagunas\ndef rle_encode(mask_image):\n    pixels = mask_image.flatten()\n    # We avoid issues with \'1\' at the start or end (at the corners of\n    # the original image) by setting those pixels to \'0\' explicitly.\n    # We do not expect these to be non-zero for an accurate mask,\n    # so this should not harm the score.\n    pixels[0] = 0\n    pixels[-1] = 0\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 2\n    runs[1::2] = runs[1::2] - runs[:-1:2]\n    return runs\n'"
Yolov1_pytorch/data/__init__.py,0,b''
Yolov1_pytorch/data/dataset.py,10,"b""#encoding:utf-8\n#\n'''\ntxt\xe6\x8f\x8f\xe8\xbf\xb0\xe6\x96\x87\xe4\xbb\xb6 image_name.jpg   num  x y w h  c    x y w h c \xe8\xbf\x99\xe6\xa0\xb7\xe5\xb0\xb1\xe6\x98\xaf\xe8\xaf\xb4\xe4\xb8\x80\xe5\xbc\xa0\xe5\x9b\xbe\xe7\x89\x87\xe4\xb8\xad\xe6\x9c\x89\xe4\xb8\xa4\xe4\xb8\xaa\xe7\x9b\xae\xe6\xa0\x87\n'''\nimport os\nimport sys\nimport os.path\n\nimport random\nimport numpy as np\n\nimport torch\nimport torch.utils.data as data\nimport torchvision.transforms as transforms\n\nimport cv2\n\nclass yoloDataset(data.Dataset):\n    '''\n    \xe8\x87\xaa\xe5\xae\x9a\xe4\xb9\x89\xe5\xb0\x81\xe8\xa3\x85\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86    \n    '''\n    image_size = 224\n    def __init__(self,root,list_file,train,transform):\n        print('\xe6\x95\xb0\xe6\x8d\xae\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96')\n        self.root=root\n        self.train = train\n        self.transform=transform    #\xe5\xaf\xb9\xe5\x9b\xbe\xe5\x83\x8f\xe8\xbd\xac\xe5\x8c\x96\n        self.fnames = []            #\xe5\x9b\xbe\xe5\x83\x8f\xe5\x90\x8d\xe5\xad\x97\n        self.boxes = []\n        self.labels = []\n        self.mean = (123,117,104)   #RGB\xe5\x9d\x87\xe5\x80\xbc\n\n        with open(list_file) as f:\n            lines  = f.readlines()\n\n        # \xe9\x81\x8d\xe5\x8e\x86voc2012train.txt\xe6\xaf\x8f\xe4\xb8\x80\xe8\xa1\x8c\n        for line in lines:\n            splited = line.strip().split()\n            # \xe8\xb5\x8b\xe5\x80\xbc\xe5\x9b\xbe\xe5\x83\x8f\xe5\x90\x8d\xe5\xad\x97\n            self.fnames.append(splited[0])\n            # \xe8\xb5\x8b\xe5\x80\xbc\xe4\xb8\x80\xe5\xbc\xa0\xe5\x9b\xbe\xe7\x9a\x84\xe7\x89\xa9\xe4\xbd\x93\xe6\x80\xbb\xe6\x95\xb0\n            num_faces = int(splited[1])\n            box=[]\n            label=[]\n            # \xe9\x81\x8d\xe5\x8e\x86\xe4\xb8\x80\xe5\xbc\xa0\xe5\x9b\xbe\xe7\x9a\x84\xe6\x89\x80\xe6\x9c\x89\xe7\x89\xa9\xe4\xbd\x93\n            #  bbox\xe5\x9d\x90\xe6\xa0\x87\xef\xbc\x884\xe4\xb8\xaa\xe5\x80\xbc\xef\xbc\x89   \xe7\x89\xa9\xe4\xbd\x93\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84\xe7\xb1\xbb\xe7\x9a\x84\xe5\xba\x8f\xe5\x8f\xb7\xef\xbc\x881\xe4\xb8\xaa\xe5\x80\xbc\xef\xbc\x89  \xe6\x89\x80\xe4\xbb\xa5\xe8\xa6\x81\xe5\x8a\xa05*i\n            for i in range(num_faces):\n                x = float(splited[2+5*i])\n                y = float(splited[3+5*i])\n                x2 = float(splited[4+5*i])\n                y2 = float(splited[5+5*i])\n                c = splited[6+5*i]\n                box.append([x,y,x2,y2])\n                label.append(int(c)+1)\n            # bbox  \xe5\x86\x99\xe5\x85\xa5\xe6\x89\x80\xe6\x9c\x89\xe7\x89\xa9\xe4\xbd\x93\xe7\x9a\x84\xe5\x9d\x90\xe6\xa0\x87\xe5\x80\xbc\n            self.boxes.append(torch.Tensor(box))\n            # label \xe5\x86\x99\xe5\x85\xa5\xe6\xa0\x87\xe7\xad\xbe\n            self.labels.append(torch.LongTensor(label))\n        # \xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe4\xb8\xad\xe5\x9b\xbe\xe5\x83\x8f\xe6\x80\xbb\xe6\x95\xb0\n        self.num_samples = len(self.boxes)\n\n    def __getitem__(self,idx):\n        '''\n        \xe7\xbb\xa7\xe6\x89\xbfDataset\xef\xbc\x8c\xe9\x9c\x80\xe5\xae\x9e\xe7\x8e\xb0\xe8\xaf\xa5\xe6\x96\xb9\xe6\xb3\x95\xef\xbc\x8c\xe5\xbe\x97\xe5\x88\xb0\xe4\xb8\x80\xe4\xb8\xaaitem\n        '''\n        fname = self.fnames[idx]\n        # \xe8\xaf\xbb\xe5\x8f\x96\xe5\x9b\xbe\xe5\x83\x8f\n        img = cv2.imread(os.path.join(self.root+fname))\n        # clone \xe6\xb7\xb1\xe5\xa4\x8d\xe5\x88\xb6\xef\xbc\x8c\xe4\xb8\x8d\xe5\x85\xb1\xe4\xba\xab\xe5\x86\x85\xe5\xad\x98\n        # \xe6\x8b\xbf\xe5\x87\xba\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84bbox\xe5\x8f\x8a \xe6\xa0\x87\xe7\xad\xbe\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84\xe5\xba\x8f\xe5\x8f\xb7\n        boxes = self.boxes[idx].clone()\n        labels = self.labels[idx].clone()\n\n        # \xe5\xa6\x82\xe6\x9e\x9c\xe4\xb8\xba\xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86,\xe8\xbf\x9b\xe8\xa1\x8c\xe6\x95\xb0\xe6\x8d\xae\xe5\xa2\x9e\xe5\xbc\xba\n        if self.train:\n            # \xe9\x9a\x8f\xe6\x9c\xba\xe7\xbf\xbb\xe8\xbd\xac\n            img, boxes = self.random_flip(img, boxes)\n            #\xe5\x9b\xba\xe5\xae\x9a\xe4\xbd\x8f\xe9\xab\x98\xe5\xba\xa6\xef\xbc\x8c\xe4\xbb\xa50.6-1.4\xe4\xbc\xb8\xe7\xbc\xa9\xe5\xae\xbd\xe5\xba\xa6\xef\xbc\x8c\xe5\x81\x9a\xe5\x9b\xbe\xe5\x83\x8f\xe5\xbd\xa2\xe5\x8f\x98\n            img,boxes = self.randomScale(img,boxes)\n            # \xe9\x9a\x8f\xe6\x9c\xba\xe6\xa8\xa1\xe7\xb3\x8a\n            img = self.randomBlur(img)\n            # \xe9\x9a\x8f\xe6\x9c\xba\xe4\xba\xae\xe5\xba\xa6\n            img = self.RandomBrightness(img)\n            # \xe9\x9a\x8f\xe6\x9c\xba\xe8\x89\xb2\xe8\xb0\x83\n            img = self.RandomHue(img)\n            # \xe9\x9a\x8f\xe6\x9c\xba\xe9\xa5\xb1\xe5\x92\x8c\xe5\xba\xa6\n            img = self.RandomSaturation(img)\n            # \xe9\x9a\x8f\xe6\x9c\xba\xe8\xbd\xac\xe6\x8d\xa2\n            img,boxes,labels = self.randomShift(img,boxes,labels)\n\n        h,w,_ = img.shape\n        boxes /= torch.Tensor([w,h,w,h]).expand_as(boxes)\n        img = self.BGR2RGB(img) #\xe5\x9b\xa0\xe4\xb8\xbapytorch\xe8\x87\xaa\xe8\xba\xab\xe6\x8f\x90\xe4\xbe\x9b\xe7\x9a\x84\xe9\xa2\x84\xe8\xae\xad\xe7\xbb\x83\xe5\xa5\xbd\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\xe6\x9c\x9f\xe6\x9c\x9b\xe7\x9a\x84\xe8\xbe\x93\xe5\x85\xa5\xe6\x98\xafRGB\n        img = self.subMean(img,self.mean) #\xe5\x87\x8f\xe5\x8e\xbb\xe5\x9d\x87\xe5\x80\xbc\n        img = cv2.resize(img,(self.image_size,self.image_size))   #\xe6\x94\xb9\xe5\x8f\x98\xe5\xbd\xa2\xe7\x8a\xb6\xe5\x88\xb0\xef\xbc\x88224,224\xef\xbc\x89\n        # \xe6\x8b\xbf\xe5\x88\xb0\xe5\x9b\xbe\xe5\x83\x8f\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84\xe7\x9c\x9f\xe5\x80\xbc\xef\xbc\x8c\xe4\xbb\xa5\xe4\xbe\xbf\xe8\xae\xa1\xe7\xae\x97loss\n        target = self.encoder(boxes,labels)# 7x7x30    \xe4\xb8\x80\xe5\xbc\xa0\xe5\x9b\xbe\xe8\xa2\xab\xe5\x88\x86\xe4\xb8\xba7x7\xe7\x9a\x84\xe7\xbd\x91\xe6\xa0\xbc;30=\xef\xbc\x882x5+20\xef\xbc\x89 \xe4\xb8\x80\xe4\xb8\xaa\xe7\xbd\x91\xe6\xa0\xbc\xe9\xa2\x84\xe6\xb5\x8b\xe4\xb8\xa4\xe4\xb8\xaa\xe6\xa1\x86   \xe4\xb8\x80\xe4\xb8\xaa\xe7\xbd\x91\xe6\xa0\xbc\xe9\xa2\x84\xe6\xb5\x8b\xe6\x89\x80\xe6\x9c\x89\xe5\x88\x86\xe7\xb1\xbb\xe6\xa6\x82\xe7\x8e\x87\xef\xbc\x8cVOC\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe5\x88\x86\xe7\xb1\xbb\xe4\xb8\xba20\xe7\xb1\xbb\n        # \xe5\x9b\xbe\xe5\x83\x8f\xe8\xbd\xac\xe5\x8c\x96\n        for t in self.transform:\n            img = t(img)\n        #\xe8\xbf\x94\xe5\x9b\x9e \xe6\x9c\x80\xe7\xbb\x88\xe5\xa4\x84\xe7\x90\x86\xe5\xa5\xbd\xe7\x9a\x84img \xe4\xbb\xa5\xe5\x8f\x8a \xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84 \xe7\x9c\x9f\xe5\x80\xbctarget\xef\xbc\x88\xe5\xbd\xa2\xe7\x8a\xb6\xe4\xb8\xba\xe7\xbd\x91\xe7\xbb\x9c\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe7\xbb\x93\xe6\x9e\x9c\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f\xef\xbc\x89\n        return img,target\n    def __len__(self):\n        '''\n        \xe7\xbb\xa7\xe6\x89\xbfDataset\xef\xbc\x8c\xe9\x9c\x80\xe5\xae\x9e\xe7\x8e\xb0\xe8\xaf\xa5\xe6\x96\xb9\xe6\xb3\x95\xef\xbc\x8c\xe5\xbe\x97\xe5\x88\xb0\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe4\xb8\xad\xe5\x9b\xbe\xe5\x83\x8f\xe6\x80\xbb\xe6\x95\xb0\n        '''\n        return self.num_samples\n\n    def encoder(self,boxes,labels):\n        '''\n        boxes (tensor) [[x1,y1,x2,y2],[x1,y1,x2,y2],[]]\n        labels (tensor) [...]\n        return 7x7x30\n        '''\n        target = torch.zeros((7,7,30))\n        cell_size = 1./7\n        # boxes[:, 2:]\xe4\xbb\xa3\xe8\xa1\xa8  2: \xe4\xbb\xa3\xe8\xa1\xa8xmax,ymax\n        # boxes[:, :2]\xe4\xbb\xa3\xe8\xa1\xa8  \xef\xbc\x9a2  \xe4\xbb\xa3\xe8\xa1\xa8xmin,ymin\n        # wh\xe4\xbb\xa3\xe8\xa1\xa8  bbox\xe7\x9a\x84\xe5\xae\xbd\xef\xbc\x88xmax-xmin\xef\xbc\x89\xe5\x92\x8c\xe9\xab\x98\xef\xbc\x88ymax-ymin\xef\xbc\x89\n        wh = boxes[:,2:]-boxes[:,:2]\n        # bbox\xe7\x9a\x84\xe4\xb8\xad\xe5\xbf\x83\xe7\x82\xb9\xe5\x9d\x90\xe6\xa0\x87\n        cxcy = (boxes[:,2:]+boxes[:,:2])/2\n        # cxcy.size()[0]\xe4\xbb\xa3\xe8\xa1\xa8 \xe4\xb8\x80\xe5\xbc\xa0\xe5\x9b\xbe\xe5\x83\x8f\xe7\x9a\x84\xe7\x89\xa9\xe4\xbd\x93\xe6\x80\xbb\xe6\x95\xb0\n        # \xe9\x81\x8d\xe5\x8e\x86\xe4\xb8\x80\xe5\xbc\xa0\xe5\x9b\xbe\xe5\x83\x8f\xe7\x9a\x84\xe7\x89\xa9\xe4\xbd\x93\xe6\x80\xbb\xe6\x95\xb0\n        for i in range(cxcy.size()[0]):\n            # \xe6\x8b\xbf\xe5\x88\xb0\xe7\xac\xaci\xe8\xa1\x8c\xe6\x95\xb0\xe6\x8d\xae\xef\xbc\x8c\xe5\x8d\xb3\xe7\xac\xaci\xe4\xb8\xaabbox\xe7\x9a\x84\xe4\xb8\xad\xe5\xbf\x83\xe7\x82\xb9\xe5\x9d\x90\xe6\xa0\x87\xef\xbc\x88\xe7\x9b\xb8\xe5\xaf\xb9\xe4\xba\x8e\xe6\x95\xb4\xe5\xbc\xa0\xe5\x9b\xbe\xef\xbc\x8c\xe5\x8f\x96\xe5\x80\xbc\xe5\x9c\xa80-1\xe4\xb9\x8b\xe9\x97\xb4\xef\xbc\x89\n            cxcy_sample = cxcy[i]\n            # ceil\xe8\xbf\x94\xe5\x9b\x9e\xe6\x95\xb0\xe5\xad\x97\xe7\x9a\x84\xe4\xb8\x8a\xe5\x85\xa5\xe6\x95\xb4\xe6\x95\xb0\n            # cxcy_sample\xe4\xb8\xba\xe4\xb8\x80\xe4\xb8\xaa\xe7\x89\xa9\xe4\xbd\x93\xe7\x9a\x84\xe4\xb8\xad\xe5\xbf\x83\xe7\x82\xb9\xe5\x9d\x90\xe6\xa0\x87\xef\xbc\x8c\xe6\xb1\x82\xe8\xaf\xa5\xe5\x9d\x90\xe6\xa0\x87\xe4\xbd\x8d\xe4\xba\x8e7x7\xe7\xbd\x91\xe6\xa0\xbc\xe7\x9a\x84\xe5\x93\xaa\xe4\xb8\xaa\xe7\xbd\x91\xe6\xa0\xbc\n            # cxcy_sample\xe5\x9d\x90\xe6\xa0\x87\xe5\x9c\xa80-1\xe4\xb9\x8b\xe9\x97\xb4  \xe7\x8e\xb0\xe5\x9c\xa8\xe6\xb1\x82\xe5\xae\x83\xe5\x86\x8d0-7\xe4\xb9\x8b\xe9\x97\xb4\xe7\x9a\x84\xe5\x80\xbc\xef\xbc\x8c\xe6\x95\x85\xe4\xb9\x98\xe4\xbb\xa57\n            # ij\xe9\x95\xbf\xe5\xba\xa6\xe4\xb8\xba2\xef\xbc\x8c\xe4\xbb\xa3\xe8\xa1\xa87x7\xe6\xa1\x86\xe7\x9a\x84\xe6\x9f\x90\xe4\xb8\x80\xe4\xb8\xaa\xe6\xa1\x86 \xe8\xb4\x9f\xe8\xb4\xa3\xe9\xa2\x84\xe6\xb5\x8b\xe4\xb8\x80\xe4\xb8\xaa\xe7\x89\xa9\xe4\xbd\x93\n            ij = (cxcy_sample/cell_size).ceil()-1\n            # \xe6\xaf\x8f\xe8\xa1\x8c\xe7\x9a\x84\xe7\xac\xac4\xe5\x92\x8c\xe7\xac\xac9\xe7\x9a\x84\xe5\x80\xbc\xe8\xae\xbe\xe7\xbd\xae\xe4\xb8\xba1\xef\xbc\x8c\xe5\x8d\xb3\xe6\xaf\x8f\xe4\xb8\xaa\xe7\xbd\x91\xe6\xa0\xbc\xe6\x8f\x90\xe4\xbe\x9b\xe7\x9a\x84\xe4\xb8\xa4\xe4\xb8\xaa\xe7\x9c\x9f\xe5\xae\x9e\xe5\x80\x99\xe9\x80\x89\xe6\xa1\x86 \xe6\xa1\x86\xe4\xbd\x8f\xe7\x89\xa9\xe4\xbd\x93\xe7\x9a\x84\xe6\xa6\x82\xe7\x8e\x87\xe6\x98\xaf1.\n            #xml\xe4\xb8\xad\xe5\x9d\x90\xe6\xa0\x87\xe7\x90\x86\xe8\xa7\xa3\xef\xbc\x9a\xe5\x8e\x9f\xe5\x9b\xbe\xe5\x83\x8f\xe5\xb7\xa6\xe4\xb8\x8a\xe8\xa7\x92\xe4\xb8\xba\xe5\x8e\x9f\xe7\x82\xb9\xef\xbc\x8c\xe5\x8f\xb3\xe8\xbe\xb9\xe4\xb8\xbax\xe8\xbd\xb4\xef\xbc\x8c\xe4\xb8\x8b\xe8\xbe\xb9\xe4\xb8\xbay\xe8\xbd\xb4\xe3\x80\x82\n            # \xe8\x80\x8c\xe4\xba\x8c\xe7\xbb\xb4\xe7\x9f\xa9\xe9\x98\xb5\xef\xbc\x88x\xef\xbc\x8cy\xef\xbc\x89  x\xe4\xbb\xa3\xe8\xa1\xa8\xe7\xac\xac\xe5\x87\xa0\xe8\xa1\x8c\xef\xbc\x8cy\xe4\xbb\xa3\xe8\xa1\xa8\xe7\xac\xac\xe5\x87\xa0\xe5\x88\x97\n            # \xe5\x81\x87\xe8\xae\xbeij\xe4\xb8\xba\xef\xbc\x881,2\xef\xbc\x89 \xe4\xbb\xa3\xe8\xa1\xa8x\xe8\xbd\xb4\xe6\x96\xb9\xe5\x90\x91\xe9\x95\xbf\xe5\xba\xa6\xe4\xb8\xba1\xef\xbc\x8cy\xe8\xbd\xb4\xe6\x96\xb9\xe5\x90\x91\xe9\x95\xbf\xe5\xba\xa6\xe4\xb8\xba2\n            # \xe4\xba\x8c\xe7\xbb\xb4\xe7\x9f\xa9\xe9\x98\xb5\xe5\x8f\x96\xef\xbc\x882,1\xef\xbc\x89 \xe4\xbb\x8e0\xe5\xbc\x80\xe5\xa7\x8b\xef\xbc\x8c\xe4\xbb\xa3\xe8\xa1\xa8\xe7\xac\xac2\xe8\xa1\x8c\xef\xbc\x8c\xe7\xac\xac1\xe5\x88\x97\xe7\x9a\x84\xe5\x80\xbc\n            # \xe7\x94\xbb\xe4\xb8\x80\xe4\xb8\x8b\xe5\x9b\xbe\xe5\xb0\xb1\xe6\x98\x8e\xe7\x99\xbd\xe4\xba\x86\n            target[int(ij[1]),int(ij[0]),4] = 1\n            target[int(ij[1]),int(ij[0]),9] = 1\n            # \xe5\x8a\xa09\xe6\x98\xaf\xe5\x9b\xa0\xe4\xb8\xba\xe5\x89\x8d0-9\xe4\xb8\xba\xe4\xb8\xa4\xe4\xb8\xaa\xe7\x9c\x9f\xe5\xae\x9e\xe5\x80\x99\xe9\x80\x89\xe6\xac\xbe\xe7\x9a\x84\xe5\x80\xbc\xe3\x80\x82\xe5\x90\x8e10-20\xe4\xb8\xba20\xe5\x88\x86\xe7\xb1\xbb   \xe5\xb0\x86\xe5\xaf\xb9\xe5\xba\x94\xe5\x88\x86\xe7\xb1\xbb\xe6\xa0\x87\xe4\xb8\xba1\n            target[int(ij[1]),int(ij[0]),int(labels[i])+9] = 1\n            # \xe5\x8c\xb9\xe9\x85\x8d\xe5\x88\xb0\xe7\x9a\x84\xe7\xbd\x91\xe6\xa0\xbc\xe7\x9a\x84\xe5\xb7\xa6\xe4\xb8\x8a\xe8\xa7\x92\xe7\x9a\x84\xe5\x9d\x90\xe6\xa0\x87\xef\xbc\x88\xe5\x8f\x96\xe5\x80\xbc\xe5\x9c\xa80-1\xe4\xb9\x8b\xe9\x97\xb4\xef\xbc\x89\xef\xbc\x88\xe5\x8e\x9f\xe4\xbd\x9c\xe8\x80\x85\xef\xbc\x89\n            # \xe6\xa0\xb9\xe6\x8d\xae\xe4\xba\x8c\xe7\xbb\xb4\xe7\x9f\xa9\xe9\x98\xb5\xe7\x9a\x84\xe6\x80\xa7\xe8\xb4\xa8\xef\xbc\x8c\xe4\xbb\x8e\xe4\xb8\x8a\xe5\x88\xb0\xe4\xb8\x8b  \xe4\xbb\x8e\xe5\xb7\xa6\xe5\x88\xb0\xe5\x8f\xb3\n            xy = ij*cell_size\n            #cxcy_sample\xef\xbc\x9a\xe7\xac\xaci\xe4\xb8\xaabbox\xe7\x9a\x84\xe4\xb8\xad\xe5\xbf\x83\xe7\x82\xb9\xe5\x9d\x90\xe6\xa0\x87     xy\xef\xbc\x9a\xe5\x8c\xb9\xe9\x85\x8d\xe5\x88\xb0\xe7\x9a\x84\xe7\xbd\x91\xe6\xa0\xbc\xe7\x9a\x84\xe5\xb7\xa6\xe4\xb8\x8a\xe8\xa7\x92\xe7\x9b\xb8\xe5\xaf\xb9\xe5\x9d\x90\xe6\xa0\x87\n            # delta_xy\xef\xbc\x9a\xe7\x9c\x9f\xe5\xae\x9e\xe6\xa1\x86\xe7\x9a\x84\xe4\xb8\xad\xe5\xbf\x83\xe7\x82\xb9\xe5\x9d\x90\xe6\xa0\x87\xe7\x9b\xb8\xe5\xaf\xb9\xe4\xba\x8e  \xe4\xbd\x8d\xe4\xba\x8e\xe8\xaf\xa5\xe4\xb8\xad\xe5\xbf\x83\xe7\x82\xb9\xe6\x89\x80\xe5\x9c\xa8\xe7\xbd\x91\xe6\xa0\xbc\xe7\x9a\x84\xe5\xb7\xa6\xe4\xb8\x8a\xe8\xa7\x92   \xe7\x9a\x84\xe7\x9b\xb8\xe5\xaf\xb9\xe5\x9d\x90\xe6\xa0\x87\xef\xbc\x8c\xe6\xad\xa4\xe6\x97\xb6\xe5\x8f\xaf\xe4\xbb\xa5\xe5\xb0\x86\xe7\xbd\x91\xe6\xa0\xbc\xe7\x9a\x84\xe5\xb7\xa6\xe4\xb8\x8a\xe8\xa7\x92\xe7\x9c\x8b\xe5\x81\x9a\xe5\x8e\x9f\xe7\x82\xb9\xef\xbc\x8c\xe4\xbd\xa0\xe8\xbf\x99\xe7\x82\xb9\xe7\x9b\xb8\xe5\xaf\xb9\xe4\xba\x8e\xe5\x8e\x9f\xe7\x82\xb9\xe7\x9a\x84\xe4\xbd\x8d\xe7\xbd\xae\xe3\x80\x82\xe5\x8f\x96\xe5\x80\xbc\xe5\x9c\xa80-1\xef\xbc\x8c\xe4\xbd\x86\xe6\x98\xaf\xe6\xaf\x941/7\xe5\xb0\x8f\n            delta_xy = (cxcy_sample -xy)/cell_size\n            # x,y\xe4\xbb\xa3\xe8\xa1\xa8\xe4\xba\x86\xe6\xa3\x80\xe6\xb5\x8b\xe6\xa1\x86\xe4\xb8\xad\xe5\xbf\x83\xe7\x9b\xb8\xe5\xaf\xb9\xe4\xba\x8e\xe7\xbd\x91\xe6\xa0\xbc\xe8\xbe\xb9\xe6\xa1\x86\xe7\x9a\x84\xe5\x9d\x90\xe6\xa0\x87\xe3\x80\x82w,h\xe7\x9a\x84\xe5\x8f\x96\xe5\x80\xbc\xe7\x9b\xb8\xe5\xaf\xb9\xe4\xba\x8e\xe6\x95\xb4\xe5\xb9\x85\xe5\x9b\xbe\xe5\x83\x8f\xe7\x9a\x84\xe5\xb0\xba\xe5\xaf\xb8\n            # \xe5\x86\x99\xe5\x85\xa5\xe4\xb8\x80\xe4\xb8\xaa\xe7\xbd\x91\xe6\xa0\xbc\xe5\xaf\xb9\xe5\xba\x94\xe4\xb8\xa4\xe4\xb8\xaa\xe6\xa1\x86\xe7\x9a\x84x,y,   wh\xef\xbc\x9abbox\xe7\x9a\x84\xe5\xae\xbd\xef\xbc\x88xmax-xmin\xef\xbc\x89\xe5\x92\x8c\xe9\xab\x98\xef\xbc\x88ymax-ymin\xef\xbc\x89\xef\xbc\x88\xe5\x8f\x96\xe5\x80\xbc\xe5\x9c\xa80-1\xe4\xb9\x8b\xe9\x97\xb4\xef\xbc\x89\n            target[int(ij[1]),int(ij[0]),2:4] = wh[i]\n            target[int(ij[1]),int(ij[0]),:2] = delta_xy\n            target[int(ij[1]),int(ij[0]),7:9] = wh[i]\n            target[int(ij[1]),int(ij[0]),5:7] = delta_xy\n        return target\n    def BGR2RGB(self,img):\n        return cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n    def BGR2HSV(self,img):\n        return cv2.cvtColor(img,cv2.COLOR_BGR2HSV)\n    def HSV2BGR(self,img):\n        return cv2.cvtColor(img,cv2.COLOR_HSV2BGR)\n    \n    def RandomBrightness(self,bgr):\n        if random.random() < 0.5:\n            hsv = self.BGR2HSV(bgr)\n            h,s,v = cv2.split(hsv)\n            adjust = random.choice([0.5,1.5])\n            v = v*adjust\n            v = np.clip(v, 0, 255).astype(hsv.dtype)\n            hsv = cv2.merge((h,s,v))\n            bgr = self.HSV2BGR(hsv)\n        return bgr\n    def RandomSaturation(self,bgr):\n        if random.random() < 0.5:\n            hsv = self.BGR2HSV(bgr)\n            h,s,v = cv2.split(hsv)\n            adjust = random.choice([0.5,1.5])\n            s = s*adjust\n            s = np.clip(s, 0, 255).astype(hsv.dtype)\n            hsv = cv2.merge((h,s,v))\n            bgr = self.HSV2BGR(hsv)\n        return bgr\n    def RandomHue(self,bgr):\n        if random.random() < 0.5:\n            hsv = self.BGR2HSV(bgr)\n            h,s,v = cv2.split(hsv)\n            adjust = random.choice([0.5,1.5])\n            h = h*adjust\n            h = np.clip(h, 0, 255).astype(hsv.dtype)\n            hsv = cv2.merge((h,s,v))\n            bgr = self.HSV2BGR(hsv)\n        return bgr\n\n    def randomBlur(self,bgr):\n        '''\n         \xe9\x9a\x8f\xe6\x9c\xba\xe6\xa8\xa1\xe7\xb3\x8a\n        '''\n        if random.random()<0.5:\n            bgr = cv2.blur(bgr,(5,5))\n        return bgr\n\n    def randomShift(self,bgr,boxes,labels):\n        #\xe5\xb9\xb3\xe7\xa7\xbb\xe5\x8f\x98\xe6\x8d\xa2\n        center = (boxes[:,2:]+boxes[:,:2])/2\n        if random.random() <0.5:\n            height,width,c = bgr.shape\n            after_shfit_image = np.zeros((height,width,c),dtype=bgr.dtype)\n            after_shfit_image[:,:,:] = (104,117,123) #bgr\n            shift_x = random.uniform(-width*0.2,width*0.2)\n            shift_y = random.uniform(-height*0.2,height*0.2)\n            #print(bgr.shape,shift_x,shift_y)\n            #\xe5\x8e\x9f\xe5\x9b\xbe\xe5\x83\x8f\xe7\x9a\x84\xe5\xb9\xb3\xe7\xa7\xbb\n            if shift_x>=0 and shift_y>=0:\n                after_shfit_image[int(shift_y):,int(shift_x):,:] = bgr[:height-int(shift_y),:width-int(shift_x),:]\n            elif shift_x>=0 and shift_y<0:\n                after_shfit_image[:height+int(shift_y),int(shift_x):,:] = bgr[-int(shift_y):,:width-int(shift_x),:]\n            elif shift_x <0 and shift_y >=0:\n                after_shfit_image[int(shift_y):,:width+int(shift_x),:] = bgr[:height-int(shift_y),-int(shift_x):,:]\n            elif shift_x<0 and shift_y<0:\n                after_shfit_image[:height+int(shift_y),:width+int(shift_x),:] = bgr[-int(shift_y):,-int(shift_x):,:]\n\n            shift_xy = torch.FloatTensor([[int(shift_x),int(shift_y)]]).expand_as(center)\n            center = center + shift_xy\n            mask1 = (center[:,0] >0) & (center[:,0] < width)\n            mask2 = (center[:,1] >0) & (center[:,1] < height)\n            mask = (mask1 & mask2).view(-1,1)\n            boxes_in = boxes[mask.expand_as(boxes)].view(-1,4)\n            if len(boxes_in) == 0:\n                return bgr,boxes,labels\n            box_shift = torch.FloatTensor([[int(shift_x),int(shift_y),int(shift_x),int(shift_y)]]).expand_as(boxes_in)\n            boxes_in = boxes_in+box_shift\n            labels_in = labels[mask.view(-1)]\n            return after_shfit_image,boxes_in,labels_in\n        return bgr,boxes,labels\n\n    def randomScale(self,bgr,boxes):\n        #\xe5\x9b\xba\xe5\xae\x9a\xe4\xbd\x8f\xe9\xab\x98\xe5\xba\xa6\xef\xbc\x8c\xe4\xbb\xa50.6-1.4\xe4\xbc\xb8\xe7\xbc\xa9\xe5\xae\xbd\xe5\xba\xa6\xef\xbc\x8c\xe5\x81\x9a\xe5\x9b\xbe\xe5\x83\x8f\xe5\xbd\xa2\xe5\x8f\x98\n        if random.random() < 0.5:\n            scale = random.uniform(0.6,1.4)\n            height,width,c = bgr.shape\n            bgr = cv2.resize(bgr,(int(width*scale),height))\n            scale_tensor = torch.FloatTensor([[scale,1,scale,1]]).expand_as(boxes)\n            boxes = boxes * scale_tensor\n            return bgr,boxes\n        return bgr,boxes\n\n    def randomCrop(self,bgr,boxes,labels):\n        if random.random() < 0.5:\n            center = (boxes[:,2:]+boxes[:,:2])/2\n            height,width,c = bgr.shape\n            h = random.uniform(0.6*height,height)\n            w = random.uniform(0.6*width,width)\n            x = random.uniform(0,width-w)\n            y = random.uniform(0,height-h)\n            x,y,h,w = int(x),int(y),int(h),int(w)\n\n            center = center - torch.FloatTensor([[x,y]]).expand_as(center)\n            mask1 = (center[:,0]>0) & (center[:,0]<w)\n            mask2 = (center[:,1]>0) & (center[:,1]<h)\n            mask = (mask1 & mask2).view(-1,1)\n\n            boxes_in = boxes[mask.expand_as(boxes)].view(-1,4)\n            if(len(boxes_in)==0):\n                return bgr,boxes,labels\n            box_shift = torch.FloatTensor([[x,y,x,y]]).expand_as(boxes_in)\n\n            boxes_in = boxes_in - box_shift\n            labels_in = labels[mask.view(-1)]\n            img_croped = bgr[y:y+h,x:x+w,:]\n            return img_croped,boxes_in,labels_in\n        return bgr,boxes,labels\n\n\n\n\n    def subMean(self,bgr,mean):\n        mean = np.array(mean, dtype=np.float32)\n        bgr = bgr - mean\n        return bgr\n\n    def random_flip(self, im, boxes):\n        '''\n        \xe9\x9a\x8f\xe6\x9c\xba\xe7\xbf\xbb\xe8\xbd\xac\n        '''\n        if random.random() < 0.5:\n            im_lr = np.fliplr(im).copy()\n            h,w,_ = im.shape\n            xmin = w - boxes[:,2]\n            xmax = w - boxes[:,0]\n            boxes[:,0] = xmin\n            boxes[:,2] = xmax\n            return im_lr, boxes\n        return im, boxes\n    def random_bright(self, im, delta=16):\n        alpha = random.random()\n        if alpha > 0.3:\n            im = im * alpha + random.randrange(-delta,delta)\n            im = im.clip(min=0,max=255).astype(np.uint8)\n        return im\n\n\n\n\n"""
Yolov1_pytorch/data/xml_2_txt.py,0,"b'import xml.etree.ElementTree as ET\nimport os\nfrom config import opt\n\ndef parse_rec(filename):\n    """"""\n    Parse a PASCAL VOC xml file\n    \xe8\xa7\xa3\xe6\x9e\x90\xe4\xb8\x80\xe4\xb8\xaa PASCAL VOC xml file\n    \xe5\xb0\x86\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe4\xbb\x8exml\xe8\xa7\xa3\xe6\x9e\x90\xe4\xb8\xbatxt  \xe7\x94\xa8\xe4\xba\x8e\xe7\x94\x9f\xe6\x88\x90voc2007test.txt\xe7\xad\x89\n    """"""\n    tree = ET.parse(filename)\n    objects = []\n    # \xe9\x81\x8d\xe5\x8e\x86\xe4\xb8\x80\xe5\xbc\xa0\xe5\x9b\xbe\xe4\xb8\xad\xe7\x9a\x84\xe6\x89\x80\xe6\x9c\x89\xe7\x89\xa9\xe4\xbd\x93\n    for obj in tree.findall(\'object\'):\n        obj_struct = {}\n        obj_struct[\'name\'] = obj.find(\'name\').text\n        #obj_struct[\'pose\'] = obj.find(\'pose\').text\n        #obj_struct[\'truncated\'] = int(obj.find(\'truncated\').text)\n        #obj_struct[\'difficult\'] = int(obj.find(\'difficult\').text)\n        bbox = obj.find(\'bndbox\')\n        # \xe4\xbb\x8e\xe5\x8e\x9f\xe5\x9b\xbe\xe5\xb7\xa6\xe4\xb8\x8a\xe8\xa7\x92\xe5\xbc\x80\xe5\xa7\x8b\xe4\xb8\xba\xe5\x8e\x9f\xe7\x82\xb9\xef\xbc\x8c\xe5\x90\x91\xe5\x8f\xb3\xe4\xb8\xbax\xe8\xbd\xb4\xef\xbc\x8c\xe5\x90\x91\xe4\xb8\x8b\xe4\xb8\xbay\xe8\xbd\xb4\xe3\x80\x82\xe5\xb7\xa6\xe4\xb8\x8a\xe8\xa7\x92\xef\xbc\x88xmin\xef\xbc\x8cymin\xef\xbc\x89\xe5\x92\x8c\xe5\x8f\xb3\xe4\xb8\x8b\xe8\xa7\x92(xmax,ymax)\n        obj_struct[\'bbox\'] = [int(float(bbox.find(\'xmin\').text)),\n                              int(float(bbox.find(\'ymin\').text)),\n                              int(float(bbox.find(\'xmax\').text)),\n                              int(float(bbox.find(\'ymax\').text))]\n        objects.append(obj_struct)\n\n    return objects\n\n# \xe6\x96\xb0\xe5\xbb\xba\xe4\xb8\x80\xe4\xb8\xaa\xe5\x90\x8d\xe4\xb8\xbavoc2012train\xe7\x9a\x84txt\xe6\x96\x87\xe4\xbb\xb6\xef\xbc\x8c\xe5\x87\x86\xe5\xa4\x87\xe5\x86\x99\xe5\x85\xa5\xe6\x95\xb0\xe6\x8d\xae\ntxt_file = open(\'data/voc2012train.txt\',\'w\')\nAnnotations = opt.train_Annotations\nxml_files = os.listdir(Annotations)\n\n# \xe9\x81\x8d\xe5\x8e\x86\xe6\x89\x80\xe6\x9c\x89\xe7\x9a\x84xml\nfor xml_file in xml_files:\n    image_path = xml_file.split(\'.\')[0] + \'.jpg\'\n    # txt \xe5\x86\x99\xe5\x85\xa5\xe5\x9b\xbe\xe5\x83\x8f\xe5\x90\x8d\xe5\xad\x97   \xe9\x9d\x9e\xe5\xae\x8c\xe6\x95\xb4\xe8\xb7\xaf\xe5\xbe\x84\n    txt_file.write(image_path+\' \')\n    results = parse_rec(Annotations + xml_file)\n    num_obj = len(results)\n    # txt\xe5\x86\x99\xe5\x85\xa5  \xe4\xb8\x80\xe5\xbc\xa0\xe5\x9b\xbe\xe4\xb8\xad\xe7\x9a\x84\xe7\x89\xa9\xe4\xbd\x93\xe6\x80\xbb\xe6\x95\xb0\n    txt_file.write(str(num_obj)+\' \')\n    # \xe9\x81\x8d\xe5\x8e\x86\xe4\xb8\x80\xe5\xbc\xa0\xe5\x9b\xbe\xe7\x89\x87\xe4\xb8\xad\xe7\x9a\x84\xe6\x89\x80\xe6\x9c\x89\xe7\x89\xa9\xe4\xbd\x93\n    for result in results:\n        class_name = result[\'name\']\n        bbox = result[\'bbox\']\n        class_name = opt.VOC_CLASSES.index(class_name)\n        # txt\xe5\x86\x99\xe5\x85\xa5  bbox\xe7\x9a\x84\xe5\x9d\x90\xe6\xa0\x87 \xe4\xbb\xa5\xe5\x8f\x8a  \xe6\xaf\x8f\xe4\xb8\xaa\xe7\x89\xa9\xe4\xbd\x93\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84\xe7\xb1\xbb\xe7\x9a\x84\xe5\xba\x8f\xe5\x8f\xb7\n        txt_file.write(str(bbox[0])+\' \'+str(bbox[1])+\' \'+str(bbox[2])+\' \'+str(bbox[3])+\' \'+str(class_name)+\' \')\n    txt_file.write(\'\\n\')\n    #\xe6\x9c\x80\xe5\x90\x8e\xe6\xa0\xbc\xe5\xbc\x8f:\xe5\x9b\xbe\xe5\x83\x8f\xe5\x90\x8d\xef\xbc\x881\xe4\xb8\xaa\xe5\x80\xbc\xef\xbc\x89   \xe7\x89\xa9\xe4\xbd\x93\xe6\x80\xbb\xe6\x95\xb0\xef\xbc\x881\xe4\xb8\xaa\xe5\x80\xbc\xef\xbc\x89    bbox\xe5\x9d\x90\xe6\xa0\x87\xef\xbc\x884\xe4\xb8\xaa\xe5\x80\xbc\xef\xbc\x89   \xe7\x89\xa9\xe4\xbd\x93\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84\xe7\xb1\xbb\xe7\x9a\x84\xe5\xba\x8f\xe5\x8f\xb7\xef\xbc\x881\xe4\xb8\xaa\xe5\x80\xbc\xef\xbc\x89\n\ntxt_file.close()'"
Yolov1_pytorch/models/__init__.py,0,b''
Yolov1_pytorch/models/net.py,13,"b'#encoding:utf-8\nimport torch.nn as nn\nimport torch.utils.model_zoo as model_zoo\nimport math\nimport torch.nn.functional as F\n\'\'\'\n\xe5\x8f\xaa\xe7\x94\xa8\xe4\xba\x86vgg16\xe6\x96\xb9\xe6\xb3\x95\n\'\'\'\n\n__all__ = [\n    \'VGG\', \'vgg11\', \'vgg11_bn\', \'vgg13\', \'vgg13_bn\', \'vgg16\', \'vgg16_bn\',\n    \'vgg19_bn\', \'vgg19\',\n]\n\n\nmodel_urls = {\n    \'vgg11\': \'https://download.pytorch.org/models/vgg11-bbd30ac9.pth\',\n    \'vgg13\': \'https://download.pytorch.org/models/vgg13-c768596a.pth\',\n    \'vgg16\': \'https://download.pytorch.org/models/vgg16-397923af.pth\',\n    \'vgg19\': \'https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\',\n    \'vgg11_bn\': \'https://download.pytorch.org/models/vgg11_bn-6002323d.pth\',\n    \'vgg13_bn\': \'https://download.pytorch.org/models/vgg13_bn-abd245e5.pth\',\n    \'vgg16_bn\': \'https://download.pytorch.org/models/vgg16_bn-6c64b313.pth\',\n    \'vgg19_bn\': \'https://download.pytorch.org/models/vgg19_bn-c79401a0.pth\',\n}\n\n\nclass VGG(nn.Module):\n\n    def __init__(self, features, num_classes=1000):\n        super(VGG, self).__init__()\n        self.features = features\n        self.classifier = nn.Sequential(\n            nn.Linear(512 * 7 * 7, 4096),\n            nn.ReLU(True),\n            nn.Dropout(),\n            nn.Linear(4096, 4096),\n            nn.ReLU(True),\n            nn.Dropout(),\n            nn.Linear(4096, num_classes),\n        )\n        self._initialize_weights()\n\n    def forward(self, x):\n        x = self.features(x)\n        x = x.view(x.size(0), -1)\n        x = self.classifier(x)\n        # \xe4\xbb\x8evgg16\xe5\xbe\x97\xe5\x88\xb0\xe8\xbe\x93\xe5\x87\xba\xef\xbc\x8c\xe7\xbb\x8f\xe8\xbf\x87sigmoid \xe5\xbd\x92\xe4\xb8\x80\xe5\x8c\x96\xe5\x88\xb00-1\xe4\xb9\x8b\xe9\x97\xb4\n        x = F.sigmoid(x)\n        # \xe5\x86\x8d\xe6\x94\xb9\xe5\x8f\x98\xe5\xbd\xa2\xe7\x8a\xb6\xef\xbc\x8c\xe8\xbf\x94\xe5\x9b\x9e\xef\xbc\x88xxx,7,7,30\xef\xbc\x89  xxx\xe4\xbb\xa3\xe8\xa1\xa8\xe5\x87\xa0\xe5\xbc\xa0\xe7\x85\xa7\xe7\x89\x87\xef\xbc\x8c\xef\xbc\x887,7,30\xef\xbc\x89\xe4\xbb\xa3\xe8\xa1\xa8\xe4\xb8\x80\xe5\xbc\xa0\xe7\x85\xa7\xe7\x89\x87\xe7\x9a\x84\xe4\xbf\xa1\xe6\x81\xaf\n        x = x.view(-1,7,7,30)\n        return x\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n                if m.bias is not None:\n                    m.bias.data.zero_()\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.Linear):\n                m.weight.data.normal_(0, 0.01)\n                m.bias.data.zero_()\n\n\ndef make_layers(cfg, batch_norm=False):\n    layers = []\n    in_channels = 3\n    for v in cfg:\n        if v == \'M\':\n            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n        else:\n            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n            if batch_norm:\n                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n            else:\n                layers += [conv2d, nn.ReLU(inplace=True)]\n            in_channels = v\n    return nn.Sequential(*layers)\n\n\ncfg = {\n    \'A\': [64, \'M\', 128, \'M\', 256, 256, \'M\', 512, 512, \'M\', 512, 512, \'M\'],\n    \'B\': [64, 64, \'M\', 128, 128, \'M\', 256, 256, \'M\', 512, 512, \'M\', 512, 512, \'M\'],\n    \'D\': [64, 64, \'M\', 128, 128, \'M\', 256, 256, 256, \'M\', 512, 512, 512, \'M\', 512, 512, 512, \'M\'],\n    \'E\': [64, 64, \'M\', 128, 128, \'M\', 256, 256, 256, 256, \'M\', 512, 512, 512, 512, \'M\', 512, 512, 512, 512, \'M\'],\n}\n\n\ndef vgg11(pretrained=False, **kwargs):\n    """"""VGG 11-layer model (configuration ""A"")\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = VGG(make_layers(cfg[\'A\']), **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'vgg11\']))\n    return model\n\n\ndef vgg11_bn(pretrained=False, **kwargs):\n    """"""VGG 11-layer model (configuration ""A"") with batch normalization\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = VGG(make_layers(cfg[\'A\'], batch_norm=True), **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'vgg11_bn\']))\n    return model\n\n\ndef vgg13(pretrained=False, **kwargs):\n    """"""VGG 13-layer model (configuration ""B"")\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = VGG(make_layers(cfg[\'B\']), **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'vgg13\']))\n    return model\n\n\ndef vgg13_bn(pretrained=False, **kwargs):\n    """"""VGG 13-layer model (configuration ""B"") with batch normalization\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = VGG(make_layers(cfg[\'B\'], batch_norm=True), **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'vgg13_bn\']))\n    return model\n\n\ndef vgg16(pretrained=False, **kwargs):\n    """"""VGG 16-layer model (configuration ""D"")\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = VGG(make_layers(cfg[\'D\']), **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'vgg16\']))\n    return model\n\n\ndef vgg16_bn(pretrained=False, **kwargs):\n    """"""VGG 16-layer model (configuration ""D"") with batch normalization\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = VGG(make_layers(cfg[\'D\'], batch_norm=True), **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'vgg16_bn\']))\n    return model\n\n\ndef vgg19(pretrained=False, **kwargs):\n    """"""VGG 19-layer model (configuration ""E"")\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = VGG(make_layers(cfg[\'E\']), **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'vgg19\']))\n    return model\n\n\ndef vgg19_bn(pretrained=False, **kwargs):\n    """"""VGG 19-layer model (configuration \'E\') with batch normalization\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = VGG(make_layers(cfg[\'E\'], batch_norm=True), **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'vgg19_bn\']))\n    return model\n\ndef test():\n    import torch\n    from torch.autograd import Variable\n    model = vgg16()\n    # \xe6\x8f\x90\xe5\x8f\x96\xe7\x89\xb9\xe5\xbe\x81\xe5\xb1\x82\xe4\xb8\x8d\xe5\x8a\xa8\n    # \xe4\xbf\xae\xe6\x94\xb9\xe5\x88\x86\xe7\xb1\xbb\xe5\xb1\x82\xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x89\xe5\xb1\x82\xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe5\xb1\x82\n    # \xe4\xbf\xae\xe6\x94\xb9vgg\xe7\x9a\x84\xe5\x88\x86\xe7\xb1\xbb\xe5\xb1\x82\xe7\xbb\x93\xe6\x9e\x84\n    model.classifier = nn.Sequential(\n            nn.Linear(512 * 7 * 7, 4096),\n            nn.ReLU(True),\n            nn.Dropout(),\n            nn.Linear(4096, 4096),\n            nn.ReLU(True),\n            nn.Dropout(),\n            # \xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x80\xe5\xb1\x82\xe4\xbf\xae\xe6\x94\xb9\xe4\xb8\xba1470   \xe5\x8d\xb3\xe4\xb8\xba1470\xe4\xbb\xa3\xe8\xa1\xa8\xe4\xb8\x80\xe5\xbc\xa0\xe5\x9b\xbe\xe7\x9a\x84\xe4\xbf\xa1\xe6\x81\xaf\xef\xbc\x881470=7x7x30\xef\xbc\x89\n            nn.Linear(4096, 1470),\n        )\n    print(model.classifier[6])\n    print(\'=============================\')\n    print(model)\n    print(\'=============================\')\n    img = torch.rand(2,3,224,224)\n    img = Variable(img)\n    output = model(img)\n    print(output.size())\n\nif __name__ == \'__main__\':\n    test()'"
Yolov1_pytorch/models/resnet.py,5,"b""import torch.nn as nn\nimport torch.utils.model_zoo as model_zoo\nimport math\nimport torch.nn.functional as F\nfrom  torchvision.models  import resnet152\n\nclass resnet152_bo(nn.Module):\n\n    def __init__(self, features, num_classes=1000):\n        super(resnet152_bo, self).__init__()\n        model=resnet152()\n        # \xe5\x85\x88\xe4\xb8\x8d\xe4\xbf\xae\xe6\x94\xb9\xe7\xbd\x91\xe7\xbb\x9c\xe7\xbb\x93\xe6\x9e\x84\xef\xbc\x8c\xe5\x85\x88\xe8\xaf\x95\xe8\xaf\x95\xe5\x8f\xaa\xe4\xbf\xae\xe6\x94\xb9\xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x80\xe4\xb8\xaa\xe8\xbe\x93\xe5\x87\xba\xe5\xb1\x82\xe5\x8f\x82\xe6\x95\xb0\n        # #\xe5\x8f\x96\xe6\x8e\x89model\xe7\x9a\x84\xe5\x90\x8e\xe4\xb8\xa4\xe5\xb1\x82(\xe5\x8e\xbb\xe6\x8e\x89\xe6\x9c\x80\xe5\x90\x8e\xe7\x9a\x84\xe6\x9c\x80\xe5\xa4\xa7\xe6\xb1\xa0\xe5\x8c\x96\xe5\xb1\x82\xe5\x92\x8c\xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe5\xb1\x82)\n        self.features=nn.Sequential(*list(model.children())[:-2])\n        self.classifier=nn.Sequential(\n            nn.Linear(2048 * 7 * 7, 4096),\n            nn.ReLU(True),\n            nn.Dropout(),\n            # \xe5\x8f\x96\xe6\xb6\x88\xe4\xb8\x80\xe5\xb1\x82\xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe5\xb1\x82\n            # nn.Linear(4096, 4096),\n            # nn.ReLU(True),\n            # nn.Dropout(),\n            # \xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x80\xe5\xb1\x82\xe4\xbf\xae\xe6\x94\xb9\xe4\xb8\xba1470   \xe5\x8d\xb3\xe4\xb8\xba1470\xe4\xbb\xa3\xe8\xa1\xa8\xe4\xb8\x80\xe5\xbc\xa0\xe5\x9b\xbe\xe7\x9a\x84\xe4\xbf\xa1\xe6\x81\xaf\xef\xbc\x881470=7x7x30\xef\xbc\x89\n            nn.Linear(4096, 1470),\n        )\n        # model.fc = nn.Linear(2048, 1470)\n        # self.resnet152_bo=model\n        # \xe5\x8f\xaa\xe4\xbf\xae\xe6\x94\xb9\xe4\xba\x86\xe7\xba\xbf\xe5\xbd\xa2\xe5\xb1\x82  \xe6\x89\x80\xe4\xbb\xa5\xe5\x8f\xaa\xe7\xbb\x99\xe7\xba\xbf\xe5\xbd\xa2\xe5\xb1\x82\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe6\x9d\x83\xe9\x87\x8d\n        self._initialize_weights()\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            # \xe5\x8f\xaa\xe4\xbf\xae\xe6\x94\xb9\xe4\xba\x86\xe7\xba\xbf\xe5\xbd\xa2\xe5\xb1\x82  \xe6\x89\x80\xe4\xbb\xa5\xe5\x8f\xaa\xe7\xbb\x99\xe7\xba\xbf\xe5\xbd\xa2\xe5\xb1\x82\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe6\x9d\x83\xe9\x87\x8d\n            if isinstance(m, nn.Linear):\n                m.weight.data.normal_(0, 0.01)\n                m.bias.data.zero_()\n    def forward(self, x):\n        x = self.features(x)\n        x = x.view(x.size(0), -1)\n        x = self.classifier(x)\n        # \xe5\xbe\x97\xe5\x88\xb0\xe8\xbe\x93\xe5\x87\xba\xef\xbc\x8c\xe7\xbb\x8f\xe8\xbf\x87sigmoid \xe5\xbd\x92\xe4\xb8\x80\xe5\x8c\x96\xe5\x88\xb00-1\xe4\xb9\x8b\xe9\x97\xb4\n        x = F.sigmoid(x)\n        # \xe5\x86\x8d\xe6\x94\xb9\xe5\x8f\x98\xe5\xbd\xa2\xe7\x8a\xb6\xef\xbc\x8c\xe8\xbf\x94\xe5\x9b\x9e\xef\xbc\x88xxx,7,7,30\xef\xbc\x89  xxx\xe4\xbb\xa3\xe8\xa1\xa8\xe5\x87\xa0\xe5\xbc\xa0\xe7\x85\xa7\xe7\x89\x87\xef\xbc\x8c\xef\xbc\x887,7,30\xef\xbc\x89\xe4\xbb\xa3\xe8\xa1\xa8\xe4\xb8\x80\xe5\xbc\xa0\xe7\x85\xa7\xe7\x89\x87\xe7\x9a\x84\xe4\xbf\xa1\xe6\x81\xaf\n        x = x.view(-1,7,7,30)\n        return x\n\n\n\ndef test():\n    '''\n    \xe6\xb5\x8b\xe8\xaf\x95\xe4\xbd\xbf\xe7\x94\xa8\n    '''\n    import torch\n    from torch.autograd import Variable\n\n    model = resnet152_bo(resnet152(pretrained=True))\n    img = torch.rand(2,3,224,224)\n    img = Variable(img)\n    output = model(img)\n    output = output.view(-1, 7, 7, 30)\n    print(output.size())\n\nif __name__ == '__main__':\n    test()"""
Yolov1_pytorch/utils/__init__.py,0,b''
Yolov1_pytorch/utils/predictUtils.py,11,"b""import cv2\nimport torch\nimport numpy as np\nimport torchvision.transforms as transforms\nfrom torch.autograd import Variable\nfrom config import opt\ndef predict_result(model,image_name,root_path=''):\n    '''\n    \xe9\xa2\x84\xe6\xb5\x8b\xe4\xb8\x80\xe5\xbc\xa0\xe6\xb5\x8b\xe8\xaf\x95\xe7\x85\xa7\xe7\x89\x87\n    '''\n    result = []\n    image = cv2.imread(root_path+image_name)\n    h,w,_ = image.shape\n    # \xe5\xb0\x86\xe5\x9b\xbe\xe5\x83\x8f\xe8\xa7\x84\xe8\x8c\x83\xe5\x8c\x96\xe5\x88\xb0\xef\xbc\x88224,224\xef\xbc\x89\n    img = cv2.resize(image,(224,224))\n    # \xe8\xbd\xac\xe6\x8d\xa2\xe4\xb8\xbaRGB\n    img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n    mean = (123,117,104)#RGB\n    # \xe5\x87\x8f\xe5\x8e\xbb\xe5\x9d\x87\xe5\x80\xbc\n    img = img - np.array(mean,dtype=np.float32)\n    #\xe5\xaf\xb9\xe5\x9b\xbe\xe5\x83\x8f\xe8\xbf\x9b\xe8\xa1\x8c\xe8\xbd\xac\xe5\x8c\x96\n    transform = transforms.Compose([transforms.ToTensor(),])\n    img = transform(img)\n    # volatile\xe7\x9b\xb8\xe5\xbd\x93\xe4\xba\x8erequires_grad=False\xef\xbc\x8c\xe4\xb8\x8d\xe4\xbf\x9d\xe5\xad\x98\xe4\xb8\xad\xe9\x97\xb4\xe5\x8f\x98\xe9\x87\x8f\xe3\x80\x82\xe4\xbb\x85\xe7\x94\xa8\xe4\xba\x8e\xe7\xba\xaf\xe6\x8e\xa8\xe6\x96\xad\n    img = Variable(img[None,:,:,:],volatile=True)\n    if opt.use_gpu:\n        img = img.cuda()\n    pred = model(img) #1x7x7x30\n    pred = pred.cpu()\n    # \xe5\xb0\x86\xe7\xbd\x91\xe7\xbb\x9c\xe8\xbe\x93\xe5\x87\xba\xe7\xbb\x93\xe6\x9e\x9c\xe8\xbd\xac\xe5\x8c\x96\xe4\xb8\xba  \xe5\x8f\xaf\xe8\xa7\x86\xe5\x8c\x96\xe6\xa0\xbc\xe5\xbc\x8f\n    boxes,cls_indexs,probs = decoder(pred)\n    # \xe9\x81\x8d\xe5\x8e\x86\xe4\xb8\x80\xe5\xbc\xa0\xe5\x9b\xbe\xe5\x83\x8f\xe4\xb8\x8a\xe6\x89\x80\xe6\x9c\x89\xe7\x9a\x84\xe9\xa2\x84\xe6\xb5\x8b\xe5\x80\x99\xe9\x80\x89\xe6\xa1\x86\n    for i,box in enumerate(boxes):\n        x1 = int(box[0]*w)\n        x2 = int(box[2]*w)\n        y1 = int(box[1]*h)\n        y2 = int(box[3]*h)\n        cls_index = cls_indexs[i]\n        cls_index = int(cls_index) # convert LongTensor to int\n        prob = probs[i]\n        prob = float(prob)\n        result.append([(x1,y1),(x2,y2),opt.VOC_CLASSES[cls_index],image_name,prob])\n    return result\ndef decoder(pred):\n    '''\n    \xe8\xa7\xa3\xe7\xa0\x81\n    pred (tensor) 1x7x7x30\n    return (tensor) box[[x1,y1,x2,y2]] label[...]\n    '''\n    boxes=[]\n    cls_indexs=[]\n    probs = []\n    cell_size = 1./7\n    pred = pred.data\n    pred = pred.squeeze(0) #7x7x30\n    contain1 = pred[:,:,4].unsqueeze(2)\n    contain2 = pred[:,:,9].unsqueeze(2)\n    contain = torch.cat((contain1,contain2),2)\n    mask1 = contain > 0.9 #\xe5\xa4\xa7\xe4\xba\x8e\xe9\x98\x88\xe5\x80\xbc\n    mask2 = (contain==contain.max()) #we always select the best contain_prob what ever it>0.9\n    mask = (mask1+mask2).gt(0)\n    min_score,min_index = torch.min(mask,2) #\xe6\xaf\x8f\xe4\xb8\xaacell\xe5\x8f\xaa\xe9\x80\x89\xe6\x9c\x80\xe5\xa4\xa7\xe6\xa6\x82\xe7\x8e\x87\xe7\x9a\x84\xe9\x82\xa3\xe4\xb8\xaa\xe9\xa2\x84\xe6\xb5\x8b\xe6\xa1\x86\n    for i in range(7):\n        for j in range(7):\n            for b in range(2):\n                index = min_index[i,j]\n                mask[i,j,index] = 0\n                if mask[i,j,b] == 1:\n                    #print(i,j,b)\n                    box = pred[i,j,b*5:b*5+4]\n                    contain_prob = torch.FloatTensor([pred[i,j,b*5+4]])\n                    xy = torch.FloatTensor([j,i])*cell_size #cell\xe5\xb7\xa6\xe4\xb8\x8a\xe8\xa7\x92  up left of cell\n                    box[:2] = box[:2]*cell_size + xy # return cxcy relative to image\n                    box_xy = torch.FloatTensor(box.size())#\xe8\xbd\xac\xe6\x8d\xa2\xe6\x88\x90xy\xe5\xbd\xa2\xe5\xbc\x8f    convert[cx,cy,w,h] to [x1,xy1,x2,y2]\n                    box_xy[:2] = box[:2] - 0.5*box[2:]\n                    box_xy[2:] = box[:2] + 0.5*box[2:]\n                    max_prob,cls_index = torch.max(pred[i,j,10:],0)\n                    boxes.append(box_xy.view(1,4))\n                    cls_indexs.append(cls_index)\n                    probs.append(contain_prob)\n    boxes = torch.cat(boxes,0) #(n,4)\n    probs = torch.cat(probs,0) #(n,)\n    cls_indexs = torch.cat(cls_indexs,0) #(n,)\n    keep = nms(boxes,probs)\n    return boxes[keep],cls_indexs[keep],probs[keep]\n\ndef nms(bboxes,scores,threshold=0.5):\n    '''\n    bboxes(tensor) [N,4]\n    scores(tensor) [N,]\n    '''\n    x1 = bboxes[:,0]\n    y1 = bboxes[:,1]\n    x2 = bboxes[:,2]\n    y2 = bboxes[:,3]\n    areas = (x2-x1) * (y2-y1)\n\n    _,order = scores.sort(0,descending=True)\n    keep = []\n    while order.numel() > 0:\n        i = order[0]\n        keep.append(i)\n\n        if order.numel() == 1:\n            break\n\n        xx1 = x1[order[1:]].clamp(min=x1[i])\n        yy1 = y1[order[1:]].clamp(min=y1[i])\n        xx2 = x2[order[1:]].clamp(max=x2[i])\n        yy2 = y2[order[1:]].clamp(max=y2[i])\n\n        w = (xx2-xx1).clamp(min=0)\n        h = (yy2-yy1).clamp(min=0)\n        inter = w*h\n\n        ovr = inter / (areas[i] + areas[order[1:]] - inter)\n        ids = (ovr<=threshold).nonzero().squeeze()\n        if ids.numel() == 0:\n            break\n        order = order[ids+1]\n    return torch.LongTensor(keep)\n\n\ndef voc_ap(rec, prec, use_07_metric=False):\n    if use_07_metric:\n        # 11 point metric\n        ap = 0.\n        for t in np.arange(0., 1.1, 0.1):\n            if np.sum(rec >= t) == 0:\n                p = 0\n            else:\n                p = np.max(prec[rec >= t])\n            ap = ap + p / 11.\n\n    else:\n        # correct ap caculation\n        mrec = np.concatenate(([0.], rec, [1.]))\n        mpre = np.concatenate(([0.], prec, [0.]))\n\n        for i in range(mpre.size - 1, 0, -1):\n            mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n\n        i = np.where(mrec[1:] != mrec[:-1])[0]\n\n        ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n\n    return ap\n\n\ndef voc_eval(preds, target, VOC_CLASSES=opt.VOC_CLASSES, threshold=0.5, use_07_metric=False, ):\n    '''\n    preds {'cat':[[image_id,confidence,x1,y1,x2,y2],...],'dog':[[],...]}\n    target {(image_id,class):[[],]}\n\n    \xe4\xb8\xbe\xe4\xbe\x8b\xef\xbc\x9a\n    preds = {\n        'cat': [['image01', 0.9, 20, 20, 40, 40], ['image01', 0.8, 20, 20, 50, 50], ['image02', 0.8, 30, 30, 50, 50]],\n        'dog': [['image01', 0.78, 60, 60, 90, 90]]}\n    target = {('image01', 'cat'): [[20, 20, 41, 41]], ('image01', 'dog'): [[60, 60, 91, 91]],\n              ('image02', 'cat'): [[30, 30, 51, 51]]}\n    '''\n    aps = []\n    # \xe9\x81\x8d\xe5\x8e\x86\xe6\x89\x80\xe6\x9c\x89\xe7\x9a\x84\xe7\xb1\xbb\xe5\x88\xab\n    for i, class_ in enumerate(VOC_CLASSES):\n        pred = preds[class_]  # [[image_id,confidence,x1,y1,x2,y2],...]\n        if len(pred) == 0:  # \xe5\xa6\x82\xe6\x9e\x9c\xe8\xbf\x99\xe4\xb8\xaa\xe7\xb1\xbb\xe5\x88\xab\xe4\xb8\x80\xe4\xb8\xaa\xe9\x83\xbd\xe6\xb2\xa1\xe6\x9c\x89\xe6\xa3\x80\xe6\xb5\x8b\xe5\x88\xb0\xe7\x9a\x84\xe5\xbc\x82\xe5\xb8\xb8\xe6\x83\x85\xe5\x86\xb5\n            ap = -1\n            print('---class {} ap {}---'.format(class_, ap))\n            aps += [ap]\n            break\n        # print(pred)\n        image_ids = [x[0] for x in pred]\n        confidence = np.array([float(x[1]) for x in pred])\n        BB = np.array([x[2:] for x in pred])\n        # sort by confidence\n        sorted_ind = np.argsort(-confidence)\n        sorted_scores = np.sort(-confidence)\n        BB = BB[sorted_ind, :]\n        image_ids = [image_ids[x] for x in sorted_ind]\n\n        # go down dets and mark TPs and FPs\n        npos = 0.\n        for (key1, key2) in target:\n            if key2 == class_:\n                npos += len(target[(key1, key2)])  # \xe7\xbb\x9f\xe8\xae\xa1\xe8\xbf\x99\xe4\xb8\xaa\xe7\xb1\xbb\xe5\x88\xab\xe7\x9a\x84\xe6\xad\xa3\xe6\xa0\xb7\xe6\x9c\xac\xef\xbc\x8c\xe5\x9c\xa8\xe8\xbf\x99\xe9\x87\x8c\xe7\xbb\x9f\xe8\xae\xa1\xe6\x89\x8d\xe4\xb8\x8d\xe4\xbc\x9a\xe9\x81\x97\xe6\xbc\x8f\n        nd = len(image_ids)\n        tp = np.zeros(nd)\n        fp = np.zeros(nd)\n        for d, image_id in enumerate(image_ids):\n            bb = BB[d]  # \xe9\xa2\x84\xe6\xb5\x8b\xe6\xa1\x86\n            if (image_id, class_) in target:\n                BBGT = target[(image_id, class_)]  # [[],]\n                for bbgt in BBGT:\n                    # compute overlaps\n                    # intersection\n                    ixmin = np.maximum(bbgt[0], bb[0])\n                    iymin = np.maximum(bbgt[1], bb[1])\n                    ixmax = np.minimum(bbgt[2], bb[2])\n                    iymax = np.minimum(bbgt[3], bb[3])\n                    iw = np.maximum(ixmax - ixmin + 1., 0.)\n                    ih = np.maximum(iymax - iymin + 1., 0.)\n                    inters = iw * ih\n\n                    union = (bb[2] - bb[0] + 1.) * (bb[3] - bb[1] + 1.) + (bbgt[2] - bbgt[0] + 1.) * (\n                                bbgt[3] - bbgt[1] + 1.) - inters\n                    if union == 0:\n                        print(bb, bbgt)\n\n                    overlaps = inters / union\n                    if overlaps > threshold:\n                        tp[d] = 1\n                        BBGT.remove(bbgt)  # \xe8\xbf\x99\xe4\xb8\xaa\xe6\xa1\x86\xe5\xb7\xb2\xe7\xbb\x8f\xe5\x8c\xb9\xe9\x85\x8d\xe5\x88\xb0\xe4\xba\x86\xef\xbc\x8c\xe4\xb8\x8d\xe8\x83\xbd\xe5\x86\x8d\xe5\x8c\xb9\xe9\x85\x8d\n                        if len(BBGT) == 0:\n                            del target[(image_id, class_)]  # \xe5\x88\xa0\xe9\x99\xa4\xe6\xb2\xa1\xe6\x9c\x89box\xe7\x9a\x84\xe9\x94\xae\xe5\x80\xbc\n                        break\n                fp[d] = 1 - tp[d]\n            else:\n                fp[d] = 1\n        fp = np.cumsum(fp)\n        tp = np.cumsum(tp)\n        rec = tp / float(npos)\n        prec = tp / np.maximum(tp + fp, np.finfo(np.float64).eps)\n        # print(rec,prec)\n        ap = voc_ap(rec, prec, use_07_metric)\n        print('---class {} ap {}---'.format(class_, ap))\n        aps += [ap]\n    print('---map {}---'.format(np.mean(aps)))"""
Yolov1_pytorch/utils/visualize.py,0,"b""import visdom \nimport numpy as np\n\nclass Visualizer():\n    def __init__(self, env='main', **kwargs):\n        '''\n        **kwargs, dict option\n        '''\n        self.vis = visdom.Visdom(env=env)\n        self.index = {}  # x, dict\n        self.log_text = ''\n        self.env = env\n    \n    def plot_train_val(self, loss_train=None, loss_val=None):\n        '''\n        plot val loss and train loss in one figure\n        '''\n        x = self.index.get('train_val', 0)\n\n        if x == 0:\n            loss = loss_train if loss_train else loss_val\n            win_y = np.column_stack((loss, loss))\n            win_x = np.column_stack((x, x))\n            self.win = self.vis.line(Y=win_y, X=win_x, \n                                env=self.env)\n                                # opts=dict(\n                                #     title='train_test_loss',\n                                # ))\n            self.index['train_val'] = x + 1\n            return \n\n        if loss_train != None:\n            self.vis.line(Y=np.array([loss_train]), X=np.array([x]),\n                        win=self.win,\n                        name='1',\n                        update='append',\n                        env=self.env)\n            self.index['train_val'] = x + 5\n        else:\n            self.vis.line(Y=np.array([loss_val]), X=np.array([x]),\n                        win=self.win,\n                        name='2',\n                        update='append',\n                        env=self.env)\n\n    def plot_many(self, d):\n        '''\n        d: dict {name, value}\n        '''\n        for k, v in d.iteritems():\n            self.plot(k, v)\n\n    def plot(self, name, y, **kwargs):\n        '''\n        plot('loss', 1.00)\n        '''\n        x = self.index.get(name, 0) # if none, return 0\n        self.vis.line(Y=np.array([y]), X=np.array([x]),\n                    win=name,\n                    opts=dict(title=name),\n                    update=None if x== 0 else 'append',\n                    **kwargs)\n        self.index[name] = x + 1\n    \n    def log(self, info, win='log_text'):\n        '''\n        show text in box not write into txt?\n        '''\n        pass\n"""
Yolov1_pytorch/utils/yoloLoss.py,13,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nclass yoloLoss(nn.Module):\n    '''\n    \xe5\xae\x9a\xe4\xb9\x89\xe4\xb8\x80\xe4\xb8\xaatorch.nn\xe4\xb8\xad\xe5\xb9\xb6\xe6\x9c\xaa\xe5\xae\x9e\xe7\x8e\xb0\xe7\x9a\x84\xe7\xbd\x91\xe7\xbb\x9c\xe5\xb1\x82\xef\xbc\x8c\xe4\xbb\xa5\xe4\xbd\xbf\xe5\xbe\x97\xe4\xbb\xa3\xe7\xa0\x81\xe6\x9b\xb4\xe5\x8a\xa0\xe6\xa8\xa1\xe5\x9d\x97\xe5\x8c\x96\n    torch.nn.Modules\xe7\x9b\xb8\xe5\xbd\x93\xe4\xba\x8e\xe6\x98\xaf\xe5\xaf\xb9\xe7\xbd\x91\xe7\xbb\x9c\xe6\x9f\x90\xe7\xa7\x8d\xe5\xb1\x82\xe7\x9a\x84\xe5\xb0\x81\xe8\xa3\x85\xef\xbc\x8c\xe5\x8c\x85\xe6\x8b\xac\xe7\xbd\x91\xe7\xbb\x9c\xe7\xbb\x93\xe6\x9e\x84\xe4\xbb\xa5\xe5\x8f\x8a\xe7\xbd\x91\xe7\xbb\x9c\xe5\x8f\x82\xe6\x95\xb0\xef\xbc\x8c\xe5\x92\x8c\xe5\x85\xb6\xe4\xbb\x96\xe6\x9c\x89\xe7\x94\xa8\xe7\x9a\x84\xe6\x93\x8d\xe4\xbd\x9c\xe5\xa6\x82\xe8\xbe\x93\xe5\x87\xba\xe5\x8f\x82\xe6\x95\xb0\n    \xe7\xbb\xa7\xe6\x89\xbfModules\xe7\xb1\xbb\xef\xbc\x8c\xe9\x9c\x80\xe5\xae\x9e\xe7\x8e\xb0__init__()\xe6\x96\xb9\xe6\xb3\x95\xef\xbc\x8c\xe4\xbb\xa5\xe5\x8f\x8aforward()\xe6\x96\xb9\xe6\xb3\x95\n    '''\n    def __init__(self,S,B,l_coord,l_noobj):\n        super(yoloLoss,self).__init__()\n        self.S = S    #7\xe4\xbb\xa3\xe8\xa1\xa8\xe5\xb0\x86\xe5\x9b\xbe\xe5\x83\x8f\xe5\x88\x86\xe4\xb8\xba7x7\xe7\x9a\x84\xe7\xbd\x91\xe6\xa0\xbc\n        self.B = B    #2\xe4\xbb\xa3\xe8\xa1\xa8\xe4\xb8\x80\xe4\xb8\xaa\xe7\xbd\x91\xe6\xa0\xbc\xe9\xa2\x84\xe6\xb5\x8b\xe4\xb8\xa4\xe4\xb8\xaa\xe6\xa1\x86\n        self.l_coord = l_coord   #5\xe4\xbb\xa3\xe8\xa1\xa8 \xce\xbbcoord  \xe6\x9b\xb4\xe9\x87\x8d\xe8\xa7\x868\xe7\xbb\xb4\xe7\x9a\x84\xe5\x9d\x90\xe6\xa0\x87\xe9\xa2\x84\xe6\xb5\x8b\n        self.l_noobj = l_noobj   #0.5\xe4\xbb\xa3\xe8\xa1\xa8\xe6\xb2\xa1\xe6\x9c\x89object\xe7\x9a\x84bbox\xe7\x9a\x84confidence loss\n\n    def compute_iou(self, box1, box2):\n        '''\n        \xe8\xae\xa1\xe7\xae\x97\xe4\xb8\xa4\xe4\xb8\xaa\xe6\xa1\x86\xe7\x9a\x84\xe9\x87\x8d\xe5\x8f\xa0\xe7\x8e\x87IOU\n        \xe9\x80\x9a\xe8\xbf\x87\xe4\xb8\xa4\xe7\xbb\x84\xe6\xa1\x86\xe7\x9a\x84\xe8\x81\x94\xe5\x90\x88\xe8\xae\xa1\xe7\xae\x97\xe4\xba\xa4\xe9\x9b\x86\xef\xbc\x8c\xe6\xaf\x8f\xe4\xb8\xaa\xe6\xa1\x86\xe4\xb8\xba[x1\xef\xbc\x8cy1\xef\xbc\x8cx2\xef\xbc\x8cy2]\xe3\x80\x82\n        Compute the intersection over union of two set of boxes, each box is [x1,y1,x2,y2].\n        Args:\n          box1: (tensor) bounding boxes, sized [N,4].\n          box2: (tensor) bounding boxes, sized [M,4].\n        Return:\n          (tensor) iou, sized [N,M].\n        '''\n        N = box1.size(0)\n        M = box2.size(0)\n\n        lt = torch.max(\n            box1[:,:2].unsqueeze(1).expand(N,M,2),  # [N,2] -> [N,1,2] -> [N,M,2]\n            box2[:,:2].unsqueeze(0).expand(N,M,2),  # [M,2] -> [1,M,2] -> [N,M,2]\n        )\n\n        rb = torch.min(\n            box1[:,2:].unsqueeze(1).expand(N,M,2),  # [N,2] -> [N,1,2] -> [N,M,2]\n            box2[:,2:].unsqueeze(0).expand(N,M,2),  # [M,2] -> [1,M,2] -> [N,M,2]\n        )\n\n        wh = rb - lt  # [N,M,2]\n        # wh(wh<0)= 0  # clip at 0\n        wh= (wh < 0).float()\n        inter = wh[:,:,0] * wh[:,:,1]  # [N,M]\n\n        area1 = (box1[:,2]-box1[:,0]) * (box1[:,3]-box1[:,1])  # [N,]\n        area2 = (box2[:,2]-box2[:,0]) * (box2[:,3]-box2[:,1])  # [M,]\n        area1 = area1.unsqueeze(1).expand_as(inter)  # [N,] -> [N,1] -> [N,M]\n        area2 = area2.unsqueeze(0).expand_as(inter)  # [M,] -> [1,M] -> [N,M]\n\n        iou = inter / (area1 + area2 - inter)\n        return iou\n\n    def forward(self,pred_tensor,target_tensor):\n        '''\n        pred_tensor: (tensor) size(batchsize,S,S,Bx5+20=30) [x,y,w,h,c]\n        target_tensor: (tensor) size(batchsize,S,S,30)\n\n        Mr.Li\xe4\xb8\xaa\xe4\xba\xba\xe8\xa7\x81\xe8\xa7\xa3\xef\xbc\x9a\n        \xe6\x9c\xac\xe6\x9d\xa5\xe6\x9c\x89\xef\xbc\x8c\xe9\xa2\x84\xe6\xb5\x8b\xe6\x97\xa0--\xe3\x80\x8b\xe8\xae\xa1\xe7\xae\x97response loss\xe5\x93\x8d\xe5\xba\x94\xe6\x8d\x9f\xe5\xa4\xb1\n        \xe6\x9c\xac\xe6\x9d\xa5\xe6\x9c\x89\xef\xbc\x8c\xe9\xa2\x84\xe6\xb5\x8b\xe6\x9c\x89--\xe3\x80\x8b\xe8\xae\xa1\xe7\xae\x97not response loss \xe6\x9c\xaa\xe5\x93\x8d\xe5\xba\x94\xe6\x8d\x9f\xe5\xa4\xb1\n        \xe6\x9c\xac\xe6\x9d\xa5\xe6\x97\xa0\xef\xbc\x8c\xe9\xa2\x84\xe6\xb5\x8b\xe6\x97\xa0--\xe3\x80\x8b\xe6\x97\xa0\xe6\x8d\x9f\xe5\xa4\xb1(\xe4\xb8\x8d\xe8\xae\xa1\xe7\xae\x97)\n        \xe6\x9c\xac\xe6\x9d\xa5\xe6\x97\xa0\xef\xbc\x8c\xe9\xa2\x84\xe6\xb5\x8b\xe6\x9c\x89--\xe3\x80\x8b\xe8\xae\xa1\xe7\xae\x97\xe4\xb8\x8d\xe5\x8c\x85\xe5\x90\xabobj\xe6\x8d\x9f\xe5\xa4\xb1  \xe5\x8f\xaa\xe8\xae\xa1\xe7\xae\x97\xe7\xac\xac4,9\xe4\xbd\x8d\xe7\x9a\x84\xe6\x9c\x89\xe6\x97\xa0\xe7\x89\xa9\xe4\xbd\x93\xe6\xa6\x82\xe7\x8e\x87\xe7\x9a\x84loss\n        '''\n        # N\xe4\xb8\xbabatchsize\n        N = pred_tensor.size()[0]\n        # \xe5\x9d\x90\xe6\xa0\x87mask    4\xef\xbc\x9a\xe6\x98\xaf\xe7\x89\xa9\xe4\xbd\x93\xe6\x88\x96\xe8\x80\x85\xe8\x83\x8c\xe6\x99\xaf\xe7\x9a\x84confidence    >0 \xe6\x8b\xbf\xe5\x88\xb0\xe6\x9c\x89\xe7\x89\xa9\xe4\xbd\x93\xe7\x9a\x84\xe8\xae\xb0\xe5\xbd\x95\n        coo_mask = target_tensor[:,:,:,4] > 0\n        # \xe6\xb2\xa1\xe6\x9c\x89\xe7\x89\xa9\xe4\xbd\x93mask                                 ==0  \xe6\x8b\xbf\xe5\x88\xb0\xe6\x97\xa0\xe7\x89\xa9\xe4\xbd\x93\xe7\x9a\x84\xe8\xae\xb0\xe5\xbd\x95\n        noo_mask = target_tensor[:,:,:,4] == 0\n        # unsqueeze(-1) \xe6\x89\xa9\xe5\xb1\x95\xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x80\xe7\xbb\xb4\xef\xbc\x8c\xe7\x94\xa80\xe5\xa1\xab\xe5\x85\x85\xef\xbc\x8c\xe4\xbd\xbf\xe5\xbe\x97\xe5\xbd\xa2\xe7\x8a\xb6\xe4\xb8\x8etarget_tensor\xe4\xb8\x80\xe6\xa0\xb7\n        # coo_mask\xe3\x80\x81noo_mask\xe5\xbd\xa2\xe7\x8a\xb6\xe6\x89\xa9\xe5\x85\x85\xe5\x88\xb0[32,7,7,30]\n        # coo_mask \xe5\xa4\xa7\xe9\x83\xa8\xe5\x88\x86\xe4\xb8\xba0   \xe8\xae\xb0\xe5\xbd\x95\xe4\xb8\xba1\xe4\xbb\xa3\xe8\xa1\xa8\xe7\x9c\x9f\xe5\xae\x9e\xe6\x9c\x89\xe7\x89\xa9\xe4\xbd\x93\xe7\x9a\x84\xe7\xbd\x91\xe6\xa0\xbc\n        # noo_mask  \xe5\xa4\xa7\xe9\x83\xa8\xe5\x88\x86\xe4\xb8\xba1  \xe8\xae\xb0\xe5\xbd\x95\xe4\xb8\xba1\xe4\xbb\xa3\xe8\xa1\xa8\xe7\x9c\x9f\xe5\xae\x9e\xe6\x97\xa0\xe7\x89\xa9\xe4\xbd\x93\xe7\x9a\x84\xe7\xbd\x91\xe6\xa0\xbc\n        coo_mask = coo_mask.unsqueeze(-1).expand_as(target_tensor)\n        noo_mask = noo_mask.unsqueeze(-1).expand_as(target_tensor)\n        # coo_pred \xe5\x8f\x96\xe5\x87\xba\xe9\xa2\x84\xe6\xb5\x8b\xe7\xbb\x93\xe6\x9e\x9c\xe4\xb8\xad\xe6\x9c\x89\xe7\x89\xa9\xe4\xbd\x93\xe7\x9a\x84\xe7\xbd\x91\xe6\xa0\xbc\xef\xbc\x8c\xe5\xb9\xb6\xe6\x94\xb9\xe5\x8f\x98\xe5\xbd\xa2\xe7\x8a\xb6\xe4\xb8\xba\xef\xbc\x88xxx,30\xef\xbc\x89  xxx\xe4\xbb\xa3\xe8\xa1\xa8\xe4\xb8\x80\xe4\xb8\xaabatch\xe7\x9a\x84\xe5\x9b\xbe\xe7\x89\x87\xe4\xb8\x8a\xe7\x9a\x84\xe5\xad\x98\xe5\x9c\xa8\xe7\x89\xa9\xe4\xbd\x93\xe7\x9a\x84\xe7\xbd\x91\xe6\xa0\xbc\xe6\x80\xbb\xe6\x95\xb0    30\xe4\xbb\xa3\xe8\xa1\xa82*5+20   \xe4\xbe\x8b\xe5\xa6\x82\xef\xbc\x9acoo_pred[72,30]\n        coo_pred = pred_tensor[coo_mask].view(-1,30)\n        # \xe4\xb8\x80\xe4\xb8\xaa\xe7\xbd\x91\xe6\xa0\xbc\xe9\xa2\x84\xe6\xb5\x8b\xe7\x9a\x84\xe4\xb8\xa4\xe4\xb8\xaabox  30\xe7\x9a\x84\xe5\x89\x8d10\xe5\x8d\xb3\xe4\xb8\xba2\xe4\xb8\xaax,y,w,h,c\xef\xbc\x8c\xe5\xb9\xb6\xe8\xb0\x83\xe6\x95\xb4\xe4\xb8\xba\xef\xbc\x88xxx,5\xef\xbc\x89 xxx\xe4\xb8\xba\xe6\x89\x80\xe6\x9c\x89\xe7\x9c\x9f\xe5\xae\x9e\xe5\xad\x98\xe5\x9c\xa8\xe7\x89\xa9\xe4\xbd\x93\xe7\x9a\x84\xe9\xa2\x84\xe6\xb5\x8b\xe6\xa1\x86\xef\xbc\x8c\xe8\x80\x8c\xe9\x9d\x9e\xe6\x89\x80\xe6\x9c\x89\xe7\x9c\x9f\xe5\xae\x9e\xe5\xad\x98\xe5\x9c\xa8\xe7\x89\xa9\xe4\xbd\x93\xe7\x9a\x84\xe7\xbd\x91\xe6\xa0\xbc     \xe4\xbe\x8b\xe5\xa6\x82\xef\xbc\x9abox_pred[144,5]\n        # contiguous\xe5\xb0\x86\xe4\xb8\x8d\xe8\xbf\x9e\xe7\xbb\xad\xe7\x9a\x84\xe6\x95\xb0\xe7\xbb\x84\xe8\xb0\x83\xe6\x95\xb4\xe4\xb8\xba\xe8\xbf\x9e\xe7\xbb\xad\xe7\x9a\x84\xe6\x95\xb0\xe7\xbb\x84\n        box_pred = coo_pred[:,:10].contiguous().view(-1,5) #box[x1,y1,w1,h1,c1]\n                                                            # #[x2,y2,w2,h2,c2]\n        # \xe6\xaf\x8f\xe4\xb8\xaa\xe7\xbd\x91\xe6\xa0\xbc\xe9\xa2\x84\xe6\xb5\x8b\xe7\x9a\x84\xe7\xb1\xbb\xe5\x88\xab  \xe5\x90\x8e20\n        class_pred = coo_pred[:,10:]\n\n        # \xe5\xaf\xb9\xe7\x9c\x9f\xe5\xae\x9e\xe6\xa0\x87\xe7\xad\xbe\xe5\x81\x9a\xe5\x90\x8c\xe6\xa0\xb7\xe6\x93\x8d\xe4\xbd\x9c\n        coo_target = target_tensor[coo_mask].view(-1,30)\n        box_target = coo_target[:,:10].contiguous().view(-1,5)\n        class_target = coo_target[:,10:]\n\n        # \xe8\xae\xa1\xe7\xae\x97\xe4\xb8\x8d\xe5\x8c\x85\xe5\x90\xabobj\xe6\x8d\x9f\xe5\xa4\xb1  \xe5\x8d\xb3\xe6\x9c\xac\xe6\x9d\xa5\xe6\x97\xa0\xef\xbc\x8c\xe9\xa2\x84\xe6\xb5\x8b\xe6\x9c\x89\n        # \xe5\x9c\xa8\xe9\xa2\x84\xe6\xb5\x8b\xe7\xbb\x93\xe6\x9e\x9c\xe4\xb8\xad\xe6\x8b\xbf\xe5\x88\xb0\xe7\x9c\x9f\xe5\xae\x9e\xe6\x97\xa0\xe7\x89\xa9\xe4\xbd\x93\xe7\x9a\x84\xe7\xbd\x91\xe6\xa0\xbc\xef\xbc\x8c\xe5\xb9\xb6\xe6\x94\xb9\xe5\x8f\x98\xe5\xbd\xa2\xe7\x8a\xb6\xe4\xb8\xba\xef\xbc\x88xxx,30\xef\xbc\x89  xxx\xe4\xbb\xa3\xe8\xa1\xa8\xe4\xb8\x80\xe4\xb8\xaabatch\xe7\x9a\x84\xe5\x9b\xbe\xe7\x89\x87\xe4\xb8\x8a\xe7\x9a\x84\xe4\xb8\x8d\xe5\xad\x98\xe5\x9c\xa8\xe7\x89\xa9\xe4\xbd\x93\xe7\x9a\x84\xe7\xbd\x91\xe6\xa0\xbc\xe6\x80\xbb\xe6\x95\xb0    30\xe4\xbb\xa3\xe8\xa1\xa82*5+20   \xe4\xbe\x8b\xe5\xa6\x82\xef\xbc\x9a[1496,30]\n        noo_pred = pred_tensor[noo_mask].view(-1,30)\n        noo_target = target_tensor[noo_mask].view(-1,30)      # \xe4\xbe\x8b\xe5\xa6\x82\xef\xbc\x9a[1496,30]\n        # ByteTensor\xef\xbc\x9a8-bit integer (unsigned)\n        noo_pred_mask = torch.cuda.ByteTensor(noo_pred.size())   # \xe4\xbe\x8b\xe5\xa6\x82\xef\xbc\x9a[1496,30]\n        noo_pred_mask.zero_()   #\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe5\x85\xa8\xe4\xb8\xba0\n        # \xe5\xb0\x86\xe7\xac\xac4\xe3\x80\x819  \xe5\x8d\xb3\xe6\x9c\x89\xe7\x89\xa9\xe4\xbd\x93\xe7\x9a\x84confidence\xe7\xbd\xae\xe4\xb8\xba1\n        noo_pred_mask[:,4]=1;noo_pred_mask[:,9]=1\n        # \xe6\x8b\xbf\xe5\x88\xb0\xe7\xac\xac4\xe5\x88\x97\xe5\x92\x8c\xe7\xac\xac9\xe5\x88\x97\xe9\x87\x8c\xe9\x9d\xa2\xe7\x9a\x84\xe5\x80\xbc\xef\xbc\x88\xe5\x8d\xb3\xe6\x8b\xbf\xe5\x88\xb0\xe7\x9c\x9f\xe5\xae\x9e\xe6\x97\xa0\xe7\x89\xa9\xe4\xbd\x93\xe7\x9a\x84\xe7\xbd\x91\xe6\xa0\xbc\xe4\xb8\xad\xef\xbc\x8c\xe7\xbd\x91\xe7\xbb\x9c\xe9\xa2\x84\xe6\xb5\x8b\xe8\xbf\x99\xe4\xba\x9b\xe7\xbd\x91\xe6\xa0\xbc\xe6\x9c\x89\xe7\x89\xa9\xe4\xbd\x93\xe7\x9a\x84\xe6\xa6\x82\xe7\x8e\x87\xe5\x80\xbc\xef\xbc\x89    \xe4\xb8\x80\xe8\xa1\x8c\xe6\x9c\x89\xe4\xb8\xa4\xe4\xb8\xaa\xe5\x80\xbc\xef\xbc\x88\xe7\xac\xac4\xe5\x92\x8c\xe7\xac\xac9\xe4\xbd\x8d\xef\xbc\x89                           \xe4\xbe\x8b\xe5\xa6\x82noo_pred_c\xef\xbc\x9a2992        noo_target_c\xef\xbc\x9a2992\n        # noo pred\xe5\x8f\xaa\xe9\x9c\x80\xe8\xa6\x81\xe8\xae\xa1\xe7\xae\x97\xe7\xb1\xbb\xe5\x88\xabc\xe7\x9a\x84\xe6\x8d\x9f\xe5\xa4\xb1\n        noo_pred_c = noo_pred[noo_pred_mask]\n        # \xe6\x8b\xbf\xe5\x88\xb0\xe7\xac\xac4\xe5\x88\x97\xe5\x92\x8c\xe7\xac\xac9\xe5\x88\x97\xe9\x87\x8c\xe9\x9d\xa2\xe7\x9a\x84\xe5\x80\xbc  \xe7\x9c\x9f\xe5\x80\xbc\xe4\xb8\xba0\xef\xbc\x8c\xe7\x9c\x9f\xe5\xae\x9e\xe6\x97\xa0\xe7\x89\xa9\xe4\xbd\x93\xef\xbc\x88\xe5\x8d\xb3\xe6\x8b\xbf\xe5\x88\xb0\xe7\x9c\x9f\xe5\xae\x9e\xe6\x97\xa0\xe7\x89\xa9\xe4\xbd\x93\xe7\x9a\x84\xe7\xbd\x91\xe6\xa0\xbc\xe4\xb8\xad\xef\xbc\x8c\xe8\xbf\x99\xe4\xba\x9b\xe7\xbd\x91\xe6\xa0\xbc\xe6\x9c\x89\xe7\x89\xa9\xe4\xbd\x93\xe7\x9a\x84\xe6\xa6\x82\xe7\x8e\x87\xe5\x80\xbc\xef\xbc\x8c\xe4\xb8\xba0\xef\xbc\x89\n        noo_target_c = noo_target[noo_pred_mask]\n        # \xe5\x9d\x87\xe6\x96\xb9\xe8\xaf\xaf\xe5\xb7\xae    \xe5\xa6\x82\xe6\x9e\x9c size_average = True\xef\xbc\x8c\xe8\xbf\x94\xe5\x9b\x9e loss.mean()\xe3\x80\x82    \xe4\xbe\x8b\xe5\xa6\x82noo_pred_c\xef\xbc\x9a2992        noo_target_c\xef\xbc\x9a2992\n        # nooobj_loss \xe4\xb8\x80\xe4\xb8\xaa\xe6\xa0\x87\xe9\x87\x8f\n        nooobj_loss = F.mse_loss(noo_pred_c,noo_target_c,size_average=False)\n\n\n        #\xe8\xae\xa1\xe7\xae\x97\xe5\x8c\x85\xe5\x90\xabobj\xe6\x8d\x9f\xe5\xa4\xb1  \xe5\x8d\xb3\xe6\x9c\xac\xe6\x9d\xa5\xe6\x9c\x89\xef\xbc\x8c\xe9\xa2\x84\xe6\xb5\x8b\xe6\x9c\x89  \xe5\x92\x8c  \xe6\x9c\xac\xe6\x9d\xa5\xe6\x9c\x89\xef\xbc\x8c\xe9\xa2\x84\xe6\xb5\x8b\xe6\x97\xa0\n        coo_response_mask = torch.cuda.ByteTensor(box_target.size())\n        coo_response_mask.zero_()\n        coo_not_response_mask = torch.cuda.ByteTensor(box_target.size())\n        coo_not_response_mask.zero_()\n        # \xe9\x80\x89\xe6\x8b\xa9\xe6\x9c\x80\xe5\xa5\xbd\xe7\x9a\x84IOU\n        for i in range(0,box_target.size()[0],2):\n            box1 = box_pred[i:i+2]\n            box1_xyxy = Variable(torch.FloatTensor(box1.size()))\n            box1_xyxy[:,:2] = box1[:,:2] -0.5*box1[:,2:4]\n            box1_xyxy[:,2:4] = box1[:,:2] +0.5*box1[:,2:4]\n            box2 = box_target[i].view(-1,5)\n            box2_xyxy = Variable(torch.FloatTensor(box2.size()))\n            box2_xyxy[:,:2] = box2[:,:2] -0.5*box2[:,2:4]\n            box2_xyxy[:,2:4] = box2[:,:2] +0.5*box2[:,2:4]\n            iou = self.compute_iou(box1_xyxy[:,:4],box2_xyxy[:,:4]) #[2,1]\n            max_iou,max_index = iou.max(0)\n            max_index = max_index.data.cuda()\n            coo_response_mask[i+max_index]=1\n            coo_not_response_mask[i+1-max_index]=1\n        # 1.response loss\xe5\x93\x8d\xe5\xba\x94\xe6\x8d\x9f\xe5\xa4\xb1\xef\xbc\x8c\xe5\x8d\xb3\xe6\x9c\xac\xe6\x9d\xa5\xe6\x9c\x89\xef\xbc\x8c\xe9\xa2\x84\xe6\xb5\x8b\xe6\x9c\x89   \xe6\x9c\x89\xe7\x9b\xb8\xe5\xba\x94 \xe5\x9d\x90\xe6\xa0\x87\xe9\xa2\x84\xe6\xb5\x8b\xe7\x9a\x84loss  \xef\xbc\x88x,y,w\xe5\xbc\x80\xe6\x96\xb9\xef\xbc\x8ch\xe5\xbc\x80\xe6\x96\xb9\xef\xbc\x89\xe5\x8f\x82\xe8\x80\x83\xe8\xae\xba\xe6\x96\x87loss\xe5\x85\xac\xe5\xbc\x8f\n        # box_pred [144,5]   coo_response_mask[144,5]   box_pred_response:[72,5]\n        # \xe9\x80\x89\xe6\x8b\xa9IOU\xe6\x9c\x80\xe5\xa5\xbd\xe7\x9a\x84box\xe6\x9d\xa5\xe8\xbf\x9b\xe8\xa1\x8c\xe8\xb0\x83\xe6\x95\xb4  \xe8\xb4\x9f\xe8\xb4\xa3\xe6\xa3\x80\xe6\xb5\x8b\xe5\x87\xba\xe6\x9f\x90\xe7\x89\xa9\xe4\xbd\x93\n        box_pred_response = box_pred[coo_response_mask].view(-1,5)\n        box_target_response = box_target[coo_response_mask].view(-1,5)\n        # box_pred_response:[72,5]     \xe8\xae\xa1\xe7\xae\x97\xe9\xa2\x84\xe6\xb5\x8b \xe6\x9c\x89\xe7\x89\xa9\xe4\xbd\x93\xe7\x9a\x84\xe6\xa6\x82\xe7\x8e\x87\xe8\xaf\xaf\xe5\xb7\xae\xef\xbc\x8c\xe8\xbf\x94\xe5\x9b\x9e\xe4\xb8\x80\xe4\xb8\xaa\xe6\x95\xb0\n        contain_loss = F.mse_loss(box_pred_response[:,4],box_target_response[:,4],size_average=False)\n        # \xe8\xae\xa1\xe7\xae\x97\xef\xbc\x88x,y,w\xe5\xbc\x80\xe6\x96\xb9\xef\xbc\x8ch\xe5\xbc\x80\xe6\x96\xb9\xef\xbc\x89\xe5\x8f\x82\xe8\x80\x83\xe8\xae\xba\xe6\x96\x87loss\xe5\x85\xac\xe5\xbc\x8f\n        loc_loss = F.mse_loss(box_pred_response[:,:2],box_target_response[:,:2],size_average=False) + F.mse_loss(torch.sqrt(box_pred_response[:,2:4]),torch.sqrt(box_target_response[:,2:4]),size_average=False)\n\n        # 2.not response loss \xe6\x9c\xaa\xe5\x93\x8d\xe5\xba\x94\xe6\x8d\x9f\xe5\xa4\xb1\xef\xbc\x8c\xe5\x8d\xb3\xe6\x9c\xac\xe6\x9d\xa5\xe6\x9c\x89\xef\xbc\x8c\xe9\xa2\x84\xe6\xb5\x8b\xe6\x97\xa0   \xe6\x9c\xaa\xe5\x93\x8d\xe5\xba\x94\n        # box_pred_not_response = box_pred[coo_not_response_mask].view(-1,5)\n        # box_target_not_response = box_target[coo_not_response_mask].view(-1,5)\n        # box_target_not_response[:,4]= 0\n        # box_pred_response:[72,5]\n        # \xe8\xae\xa1\xe7\xae\x97c  \xe6\x9c\x89\xe7\x89\xa9\xe4\xbd\x93\xe7\x9a\x84\xe6\xa6\x82\xe7\x8e\x87\xe7\x9a\x84loss\n        not_contain_loss = F.mse_loss(box_pred_response[:,4],box_target_response[:,4],size_average=False)\n        # 3.class loss  \xe8\xae\xa1\xe7\xae\x97\xe4\xbc\xa0\xe5\x85\xa5\xe7\x9a\x84\xe7\x9c\x9f\xe5\xae\x9e\xe6\x9c\x89\xe7\x89\xa9\xe4\xbd\x93\xe7\x9a\x84\xe7\xbd\x91\xe6\xa0\xbc  \xe5\x88\x86\xe7\xb1\xbb\xe7\x9a\x84\xe7\xb1\xbb\xe5\x88\xab\xe6\x8d\x9f\xe5\xa4\xb1 \n        class_loss = F.mse_loss(class_pred,class_target,size_average=False)\n        # \xe9\x99\xa4\xe4\xbb\xa5N  \xe5\x8d\xb3\xe5\xb9\xb3\xe5\x9d\x87\xe4\xb8\x80\xe5\xbc\xa0\xe5\x9b\xbe\xe7\x9a\x84\xe6\x80\xbb\xe6\x8d\x9f\xe5\xa4\xb1\n        return (self.l_coord*loc_loss + contain_loss + not_contain_loss + self.l_noobj*nooobj_loss + class_loss)/N\n\n\n\n\n"""
Yolov3_pytorch/datasets/datasets.py,5,"b""import glob\nimport random\nimport os\nimport numpy as np\n\nimport torch\n\nfrom torch.utils.data import Dataset\nfrom PIL import Image\nimport torchvision.transforms as transforms\n\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\nfrom skimage.transform import resize\n\nimport sys\n\n\nimport glob\nimport random\nimport os\nimport numpy as np\n\nimport torch\n\nfrom torch.utils.data import Dataset\nfrom PIL import Image\nimport torchvision.transforms as transforms\n\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\nfrom skimage.transform import resize\n\nimport sys\n\nclass ImageFolder(Dataset):\n    '''\n    \xe4\xbb\x85detect.py\xe7\x94\xa8\xe5\x88\xb0\xef\xbc\x8c\xe7\x94\xa8\xe4\xba\x8e\xe6\xb5\x8b\xe8\xaf\x95\xe4\xbe\x8b\xe5\xad\x90\n    '''\n    def __init__(self, folder_path, img_size=416):\n        self.files = sorted(glob.glob('%s/*.*' % folder_path))\n        self.img_shape = (img_size, img_size)\n\n    def __getitem__(self, index):\n        img_path = self.files[index % len(self.files)]\n        # Extract image\n        img = np.array(Image.open(img_path))\n        h, w, _ = img.shape\n        dim_diff = np.abs(h - w)\n        # Upper (left) and lower (right) padding\n        pad1, pad2 = dim_diff // 2, dim_diff - dim_diff // 2\n        # Determine padding\n        pad = ((pad1, pad2), (0, 0), (0, 0)) if h <= w else ((0, 0), (pad1, pad2), (0, 0))\n        # Add padding\n        input_img = np.pad(img, pad, 'constant', constant_values=127.5) / 255.\n        # Resize and normalize\n        input_img = resize(input_img, (*self.img_shape, 3), mode='reflect')\n        # Channels-first\n        input_img = np.transpose(input_img, (2, 0, 1))\n        # As pytorch tensor\n        input_img = torch.from_numpy(input_img).float()\n\n        # \xe8\xbf\x94\xe5\x9b\x9e\xe5\x9b\xbe\xe7\x89\x87\xe8\xb7\xaf\xe5\xbe\x84\xe3\x80\x81\xe7\xbb\x8f\xe8\xbf\x87\xe5\xa4\x84\xe7\x90\x86\xe5\x90\x8e\xe7\x9a\x84\xe5\x9b\xbe\xe5\x83\x8ftensor\n        return img_path, input_img\n\n    def __len__(self):\n        return len(self.files)\n\n\nclass ListDataset(Dataset):\n    '''\n    \xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe5\x8a\xa0\xe8\xbd\xbd\xe5\x99\xa8\n    '''\n    def __init__(self, list_path, img_size=416):\n        # \xe8\xaf\xbb\xe5\x8f\x96  \xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe4\xb8\xad\xe5\x88\x86\xe9\x85\x8d\xe4\xb8\xba\xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\xe7\x9a\x84txt\xe6\x96\x87\xe6\x9c\xac\xef\xbc\x8c\xe4\xbb\xa5list\xe5\xbd\xa2\xe5\xbc\x8f\xe4\xbf\x9d\xe5\xad\x98\n        with open(list_path, 'r') as file:\n            self.img_files = file.readlines()\n        # \xe8\xaf\xbb\xe5\x8f\x96  \xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe4\xb8\xad\xe5\x88\x86\xe9\x85\x8d\xe4\xb8\xba\xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\xe7\x9a\x84txt\xe6\x96\x87\xe6\x9c\xac\xef\xbc\x88\xe5\x8d\xb3\xe6\xa0\x87\xe7\xad\xbe\xef\xbc\x8ccoco\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe4\xbb\xa5txt\xe4\xbf\x9d\xe5\xad\x98 \xe6\xa1\x86\xe7\x9c\x9f\xe5\x80\xbc\xef\xbc\x89\xef\xbc\x8c\xe4\xbb\xa5list\xe5\xbd\xa2\xe5\xbc\x8f\xe4\xbf\x9d\xe5\xad\x98\n        self.label_files = [path.replace('images', 'labels').replace('.png', '.txt').replace('.jpg', '.txt') for path in self.img_files]\n        # \xe8\xbe\x93\xe5\x85\xa5\xe8\xae\xad\xe7\xbb\x83\xe5\x9b\xbe\xe5\x83\x8f\xe5\xa4\xa7\xe5\xb0\x8f\n        self.img_shape = (img_size, img_size)\n        self.max_objects = 50  # \xe8\xae\xbe\xe5\xae\x9a\xe4\xb8\x80\xe5\xbc\xa0\xe5\x9b\xbe\xe5\x83\x8f\xe6\x9c\x80\xe5\xa4\x9a\xe7\x9c\x9f\xe5\xae\x9e\xe5\xad\x98\xe5\x9c\xa850\xe4\xb8\xaa\xe7\x89\xa9\xe4\xbd\x93\xef\xbc\x88\xe5\xb0\x81\xe8\xa3\x85 \xe5\x9b\xbe\xe5\x83\x8f\xe7\x9c\x9f\xe5\x80\xbc\xe6\xa1\x86\xe6\x97\xb6\xe4\xbd\xbf\xe7\x94\xa8\xe5\x88\xb0\xef\xbc\x89\n\n    def __getitem__(self, index):\n\n        '''\n        \xe8\xae\xad\xe7\xbb\x83\xe6\x97\xb6\xe8\x8e\xb7\xe5\x8f\x96\xe5\x8d\x95\xe5\xbc\xa0\xe5\x9b\xbe\xe5\x83\x8f\xe5\x8f\x8a\xe7\x9c\x9f\xe5\x80\xbc\n        '''\n\n        # \xe8\xaf\xbb\xe5\x8f\x96\xe5\x9b\xbe\xe5\x83\x8f\xe4\xb8\xbatensor\n        img_path = self.img_files[index % len(self.img_files)].rstrip()\n\n        copy_img=Image.open(img_path).copy()\n        img = np.array(copy_img)\n\n        # Handles images with less than three channels\n        # \xe5\xa4\x84\xe7\x90\x86 \xe5\x9b\xbe\xe5\x83\x8f\xe7\x9a\x84\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\xe4\xb8\x8d\xe4\xb8\xba3 \xe6\x97\xb6(\xe5\x8d\xb3\xe8\xaf\xa5\xe5\x9b\xbe\xe5\x83\x8f\xe6\x8d\x9f\xe5\x9d\x8f)\xef\xbc\x8c\xe5\x88\x99 \xe8\xaf\xbb\xe5\x8f\x96\xe4\xb8\x8b\xe4\xb8\x80\xe5\xbc\xa0\xe5\x9b\xbe\xe7\x89\x87\n        while len(img.shape) != 3:\n            index += 1\n            img_path = self.img_files[index % len(self.img_files)].rstrip()\n            img = np.array(Image.open(img_path))\n\n        # \xe5\xaf\xb9\xe5\x9b\xbe\xe5\x83\x8ftensor\xe8\xbf\x9b\xe8\xa1\x8c\xe5\xa4\x84\xe7\x90\x86\xef\xbc\x88\xe6\x95\xb0\xe6\x8d\xae\xe5\xa2\x9e\xe5\xbc\xba\xe3\x80\x81\xe8\xa7\x84\xe8\x8c\x83\xe5\x8c\x96\xef\xbc\x89\n\n        # w,h\xe6\x8c\x89\xe7\x85\xa7\xe8\xbe\x83\xe5\xa4\xa7\xe5\x80\xbc\xe5\xa1\xab\xe5\x85\x85\xe6\x88\x90\xe6\xad\xa3\xe6\x96\xb9\xe5\xbd\xa2\n        h, w, _ = img.shape\n        # np.abs \xe7\xbb\x9d\xe5\xaf\xb9\xe5\x80\xbc\n        dim_diff = np.abs(h - w)\n        # Upper (left) and lower (right) padding\n        # \xe4\xb8\x8a\xef\xbc\x88\xe5\xb7\xa6\xef\xbc\x89\xe5\x92\x8c\xe4\xb8\x8b\xef\xbc\x88\xe5\x8f\xb3\xef\xbc\x89\xe5\xa1\xab\xe5\x85\x85\n        pad1, pad2 = dim_diff // 2, dim_diff - dim_diff // 2\n        # Determine padding \xe7\xa1\xae\xe5\xae\x9a\xe5\xa1\xab\xe5\x85\x85\n        pad = ((pad1, pad2), (0, 0), (0, 0)) if h <= w else ((0, 0), (pad1, pad2), (0, 0))\n        # Add padding \xe6\xb7\xbb\xe5\x8a\xa0\xe5\xa1\xab\xe5\x85\x85\n        input_img = np.pad(img, pad, 'constant', constant_values=128) / 255.\n\n        # \xe5\xa1\xab\xe5\x85\x85\xe6\x88\x90\xe6\xad\xa3\xe6\x96\xb9\xe5\xbd\xa2\xe5\x90\x8e resize\xe5\x88\xb0 \xe6\x8c\x87\xe5\xae\x9a\xe5\xbd\xa2\xe7\x8a\xb6\xef\xbc\x88\xe4\xb8\x80\xe8\x88\xac\xe4\xb8\xba416x416\xef\xbc\x89\n        padded_h, padded_w, _ = input_img.shape\n        # Resize and normalize  resize\xe5\xb9\xb6\xe8\xa7\x84\xe8\x8c\x83\xe5\x8c\x96\n        input_img = resize(input_img, (*self.img_shape, 3), mode='reflect')\n\n        # Channels-first  \xe8\xbd\xac\xe6\x8d\xa2\xe9\x80\x9a\xe9\x81\x93\n        input_img = np.transpose(input_img, (2, 0, 1))\n        # As pytorch tensor  \xe8\xbd\xac\xe4\xb8\xbapytorch tensor\n        input_img = torch.from_numpy(input_img).float()\n\n        #---------\n        #  \xe7\x9c\x9f\xe5\x80\xbc\xe6\xa0\x87\xe7\xad\xbe\xe5\xa4\x84\xe7\x90\x86\n        #---------\n        label_path = self.label_files[index % len(self.img_files)].rstrip()\n        labels = None\n        if os.path.exists(label_path):\n            # eg\xef\xbc\x9a[8,5]   8\xef\xbc\x9a\xe8\xaf\xa5\xe5\x9b\xbe\xe5\x83\x8f\xe6\x9c\x898\xe4\xb8\xaabbox   5: 0\xe4\xbb\xa3\xe8\xa1\xa8\xe7\xb1\xbb\xe5\x88\xab\xe5\xaf\xb9\xe5\xba\x94\xe5\xba\x8f\xe5\x8f\xb7 1~4\xe4\xbb\xa3\xe8\xa1\xa8\xe5\x9d\x90\xe6\xa0\x87\xef\xbc\x88\xe5\x80\xbc\xe5\x9c\xa80~1\xe4\xb9\x8b\xe9\x97\xb4\xef\xbc\x89\n            labels = np.loadtxt(label_path).reshape(-1, 5)\n            # Extract coordinates for unpadded + unscaled image\n            # \xe6\x8f\x90\xe5\x8f\x96\xe6\x9c\xaa\xe5\xa1\xab\xe5\x85\x85+\xe6\x9c\xaa\xe7\xbc\xa9\xe6\x94\xbe\xe5\x9b\xbe\xe5\x83\x8f\xe7\x9a\x84\xe5\x9d\x90\xe6\xa0\x87\n            x1 = w * (labels[:, 1] - labels[:, 3]/2)\n            y1 = h * (labels[:, 2] - labels[:, 4]/2)\n            x2 = w * (labels[:, 1] + labels[:, 3]/2)\n            y2 = h * (labels[:, 2] + labels[:, 4]/2)\n            # Adjust for added padding\n            # \xe6\xb7\xbb\xe5\x8a\xa0\xe5\xa1\xab\xe5\x85\x85\xef\xbc\x8c\xe4\xbb\xa5\xe4\xbe\xbf\xe4\xba\x8e \xe5\x9b\xbe\xe5\x83\x8f\xe8\xb0\x83\xe6\x95\xb4\xe4\xb8\x80\xe8\x87\xb4\n            x1 += pad[1][0]\n            y1 += pad[0][0]\n            x2 += pad[1][0]\n            y2 += pad[0][0]\n            # Calculate ratios from coordinates\n            # \xe4\xbb\x8e\xe5\x9d\x90\xe6\xa0\x87\xe8\xae\xa1\xe7\xae\x97\xe6\xaf\x94\xe7\x8e\x87\n            labels[:, 1] = ((x1 + x2) / 2) / padded_w\n            labels[:, 2] = ((y1 + y2) / 2) / padded_h\n            labels[:, 3] *= w / padded_w\n            labels[:, 4] *= h / padded_h\n        # Fill matrix\n        # \xe5\xa1\xab\xe5\x85\x85\xe7\x9f\xa9\xe9\x98\xb5\xef\xbc\x88\xe5\xb0\x86 txt\xe9\x87\x8c\xe7\x9a\x84\xe5\x86\x85\xe5\xae\xb9\xef\xbc\x8c\xe5\x8d\xb3\xe6\xaf\x8f\xe5\xbc\xa0\xe5\x9b\xbe\xe5\x83\x8f\xe7\x9a\x84\xe6\x89\x80\xe6\x9c\x89\xe7\x89\xa9\xe4\xbd\x93\xe5\xa1\xab\xe5\x85\xa5\xef\xbc\x8c\xe6\x9c\x80\xe5\xa4\x9a\xe6\xb7\xbb\xe5\x8a\xa050\xe4\xb8\xaa\xe7\x89\xa9\xe4\xbd\x93\xef\xbc\x89\n        filled_labels = np.zeros((self.max_objects, 5))\n        if labels is not None:\n            filled_labels[range(len(labels))[:self.max_objects]] = labels[:self.max_objects]\n        filled_labels = torch.from_numpy(filled_labels)\n        # \xe8\xbf\x94\xe5\x9b\x9e \xe5\x9b\xbe\xe5\x83\x8f\xe8\xb7\xaf\xe5\xbe\x84\xe3\x80\x81\xe5\xa4\x84\xe7\x90\x86\xe5\x90\x8e\xe7\x9a\x84\xe5\x9b\xbe\xe5\x83\x8ftensor\xe3\x80\x81\xe5\x9d\x90\xe6\xa0\x87\xe8\xa2\xab\xe5\xbd\x92\xe4\xb8\x80\xe5\x8c\x96\xe5\x90\x8e\xe7\x9a\x84\xe7\x9c\x9f\xe5\x80\xbc\xe6\xa1\x86filled_labels[50,5] \xe5\x80\xbc\xe5\x9c\xa80-1\xe4\xb9\x8b\xe9\x97\xb4\n        return img_path, input_img, filled_labels\n\n    def __len__(self):\n        return len(self.img_files)\n"""
Yolov3_pytorch/models/models.py,24,"b'from __future__ import division\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport numpy as np\n\nfrom PIL import Image\n\nfrom utils.parse_config import *\nfrom utils.utils import build_targets\nfrom collections import defaultdict\n\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\ndef create_modules(module_defs):\n    """"""\n    Constructs module list of layer blocks from module configuration in module_defs\n    \xe6\xa0\xb9\xe6\x8d\xaemodule_defs\xef\xbc\x88list\xe5\xbd\xa2\xe5\xbc\x8f\xef\xbc\x89\xe4\xb8\xad\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9d\x97\xe9\x85\x8d\xe7\xbd\xae \xe6\x9d\xa5\xe6\x9e\x84\xe9\x80\xa0 \xe7\xbd\x91\xe7\xbb\x9c\xe6\xa8\xa1\xe5\x9d\x97list\n    """"""\n    # \xe7\xac\xac\xe4\xb8\x80\xe8\xa1\x8c\xe5\xad\x98\xe6\x94\xbe\xe7\x9a\x84\xe6\x98\xaf \xe8\xb6\x85\xe5\x8f\x82\xe6\x95\xb0\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe9\x9c\x80\xe8\xa6\x81pop\xe5\x87\xba\xe6\x9d\xa5\n    hyperparams = module_defs.pop(0)\n    # \xe8\xbe\x93\xe5\x85\xa5\xe5\x9b\xbe\xe5\x83\x8f\xe7\x9a\x84\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\xe4\xb8\xba3\n    output_filters = [int(hyperparams[\'channels\'])]\n    # \xe4\xbf\x9d\xe5\xad\x98yolov3\xe7\xbd\x91\xe7\xbb\x9c\xe6\xa8\xa1\xe5\x9e\x8b\n    module_list = nn.ModuleList()\n    for i, module_def in enumerate(module_defs):\n        modules = nn.Sequential()\n        # \xe8\xa7\xa3\xe6\x9e\x90cfg\xe7\xbd\x91\xe7\xbb\x9c\xe7\xbb\x93\xe6\x9e\x84\xef\xbc\x8c\xe8\xbd\xac\xe5\x8c\x96\xe4\xb8\xbapytorch\xe7\xbd\x91\xe7\xbb\x9c\xe7\xbb\x93\xe6\x9e\x84\n        if module_def[\'type\'] == \'convolutional\':\n            \'\'\'\n            \xe6\xaf\x8f\xe4\xb8\xaa\xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82\xe5\x90\x8e\xe9\x83\xbd\xe4\xbc\x9a\xe8\xb7\x9f\xe4\xb8\x80\xe4\xb8\xaaBN\xe5\xb1\x82\xe5\x92\x8c\xe4\xb8\x80\xe4\xb8\xaaLeakyReLU\xef\xbc\x8c\xe7\xae\x97\xe4\xbd\x9clist\xe4\xb8\xad\xe7\x9a\x84\xe4\xb8\x80\xe8\xa1\x8c\n            pad = 1 \xe8\xa1\xa8\xe7\xa4\xba \xe4\xbd\xbf\xe7\x94\xa8pad,\xe4\xbd\x86\xe6\x98\xaf\xe5\x85\xb7\xe4\xbd\x93pad\xe5\x80\xbc\xe6\x97\xb6\xe6\x8c\x89\xe7\x85\xa7kernel_size\xe8\xae\xa1\xe7\xae\x97\xe7\x9a\x84\n            bn=1 \xe4\xb9\x9f\xe8\xa1\xa8\xe7\xa4\xba \xe4\xbd\xbf\xe7\x94\xa8bn,\xe5\x85\xb7\xe4\xbd\x93\xe5\x80\xbc\xe4\xb8\xba \xe8\xbe\x93\xe5\x87\xba\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\n            \'\'\'\n            bn = int(module_def[\'batch_normalize\'])\n            filters = int(module_def[\'filters\'])\n            kernel_size = int(module_def[\'size\'])\n            # // \xe8\xa1\xa8\xe7\xa4\xba\xe5\x85\x88\xe5\x81\x9a\xe9\x99\xa4\xe6\xb3\x95\xef\xbc\x8c\xe7\x84\xb6\xe5\x90\x8e\xe5\x90\x91\xe4\xb8\x8b\xe5\x8f\x96\xe6\x95\xb4\n            pad = (kernel_size - 1) // 2 if int(module_def[\'pad\']) else 0\n            modules.add_module(\'conv_%d\' % i, nn.Conv2d(in_channels=output_filters[-1],\n                                                        out_channels=filters,\n                                                        kernel_size=kernel_size,\n                                                        stride=int(module_def[\'stride\']),\n                                                        padding=pad,\n                                                        bias=not bn))\n            if bn:\n                # \xe5\x80\xbc\xe4\xb8\xba \xe8\xbe\x93\xe5\x87\xba\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\n                modules.add_module(\'batch_norm_%d\' % i, nn.BatchNorm2d(filters))\n            if module_def[\'activation\'] == \'leaky\':\n                # \xe6\xbf\x80\xe6\xb4\xbb\xe5\x87\xbd\xe6\x95\xb0\n                modules.add_module(\'leaky_%d\' % i, nn.LeakyReLU(0.1))\n\n        elif module_def[\'type\'] == \'upsample\':\n            \'\'\'\n            \xe4\xb8\x8a\xe9\x87\x87\xe6\xa0\xb7\xe4\xb8\x8erount\xe6\x90\xad\xe9\x85\x8d\xe4\xbd\xbf\xe7\x94\xa8\n            \n            \xe4\xb8\x8a\xe9\x87\x87\xe6\xa0\xb7\xe5\xb0\x86feature map\xe5\x8f\x98\xe5\xa4\xa7\xef\xbc\x8c\xe7\x84\xb6\xe5\x90\x8e\xe4\xb8\x8e \xe4\xb9\x8b\xe5\x89\x8d\xe7\x9a\x84\xe8\xbe\x83\xe5\xa4\xa7feature map\xe5\x9c\xa8\xe6\xb7\xb1\xe5\xba\xa6\xe4\xb8\x8a\xe5\x90\x88\xe5\xb9\xb6\n            \'\'\'\n\n            # nearest \xe4\xbd\xbf\xe7\x94\xa8\xe6\x9c\x80\xe9\x82\xbb\xe8\xbf\x91 nrighbours \xe5\xaf\xb9\xe8\xbe\x93\xe5\x85\xa5\xe8\xbf\x9b\xe8\xa1\x8c\xe9\x87\x87\xe6\xa0\xb7 \xe5\x83\x8f\xe7\xb4\xa0\xe5\x80\xbc.\n            upsample = nn.Upsample( scale_factor=int(module_def[\'stride\']),\n                                    mode=\'nearest\')\n            modules.add_module(\'upsample_%d\' % i, upsample)\n\n        elif module_def[\'type\'] == \'route\':\n            \'\'\'\n            route \xe6\x8c\x87 \xe6\x8c\x89\xe7\x85\xa7\xe5\x88\x97\xe6\x9d\xa5\xe5\x90\x88\xe5\xb9\xb6tensor,\xe5\x8d\xb3\xe6\x89\xa9\xe5\xb1\x95\xe6\xb7\xb1\xe5\xba\xa6\n            filters\xe4\xb8\xba\xe8\xaf\xa5\xe5\xb1\x82\xe8\xbe\x93\xe5\x87\xba\xef\xbc\x8c\xe4\xbf\x9d\xe5\xad\x98\xe5\x88\xb0output_filters\n            \'\'\'\n            layers = [int(x) for x in module_def[""layers""].split(\',\')]\n            filters = sum([output_filters[layer_i] for layer_i in layers])\n            modules.add_module(\'route_%d\' % i, EmptyLayer())\n\n        elif module_def[\'type\'] == \'shortcut\':\n            \'\'\'\n            shortcut \xe6\x8c\x87  \xe6\xae\x8b\xe5\xb7\xae\xe7\xbb\x93\xe6\x9e\x84\xef\xbc\x8c\xe5\x8d\xb7\xe7\xa7\xaf\xe7\x9a\x84\xe8\xb7\xa8\xe5\xb1\x82\xe8\xbf\x9e\xe6\x8e\xa5\xef\xbc\x8c\xe5\x8d\xb3 \xe5\xb0\x86\xe4\xb8\x8d\xe5\x90\x8c\xe4\xb8\xa4\xe5\xb1\x82\xe8\xbe\x93\xe5\x87\xba\xef\xbc\x88\xe5\x8d\xb3\xe8\xbe\x93\xe5\x87\xba+\xe6\xae\x8b\xe5\xb7\xae\xe5\x9d\x97\xef\xbc\x89\xe7\x9b\xb8\xe5\x8a\xa0 \xe4\xb8\xba \xe6\x9c\x80\xe5\x90\x8e\xe7\xbb\x93\xe6\x9e\x9c\n            filters\xe4\xb8\xba\xe8\xaf\xa5\xe5\xb1\x82\xe8\xbe\x93\xe5\x87\xba\xef\xbc\x8c\xe4\xbf\x9d\xe5\xad\x98\xe5\x88\xb0output_filters\n            \'\'\'\n            filters = output_filters[int(module_def[\'from\'])]\n            modules.add_module(""shortcut_%d"" % i, EmptyLayer())\n\n        elif module_def[""type""] == ""yolo"":\n            \'\'\'\n            \xe5\xaf\xb9\xe4\xba\x8eYOLOLayer\xe5\xb1\x82\xef\xbc\x9a\n            \xe8\xae\xad\xe7\xbb\x83\xe9\x98\xb6\xe6\xae\xb5\xe8\xbf\x94\xe5\x9b\x9e \xe5\x90\x84loss\n            \xe9\xa2\x84\xe6\xb5\x8b\xe9\x98\xb6\xe6\xae\xb5\xe8\xbf\x94\xe5\x9b\x9e  \xe9\xa2\x84\xe6\xb5\x8b\xe7\xbb\x93\xe6\x9e\x9c\n            \'\'\'\n            # mask\xe4\xb8\xba \xe5\x8d\xb3\xe4\xbb\x8e anchor\xe9\x9b\x86\xe5\x90\x88\xe4\xb8\xad\xe9\x80\x89\xe7\x94\xa8\xe5\x93\xaa\xe5\x87\xa0\xe4\xb8\xaaanchor\n            anchor_idxs = [int(x) for x in module_def[""mask""].split("","")]\n            # Extract anchors  \xe6\x8f\x90\xe5\x8f\x96anchor\n            anchors = [int(x) for x in module_def[""anchors""].split("","")]\n            anchors = [(anchors[i], anchors[i+1]) for i in range(0, len(anchors),2)]\n            # \xe5\x8f\xaa\xe6\x8b\xbf \xe8\xaf\xa5\xe5\xb1\x82\xe6\x8c\x91\xe9\x80\x89\xe4\xb9\x8b\xe5\x90\x8e\xe7\x9a\x84anchor\n            anchors = [anchors[i] for i in anchor_idxs]\n            # \xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe5\x85\xb1\xe5\xa4\x9a\xe5\xb0\x91\xe7\xb1\xbb\xe5\x88\xab\xe3\x80\x82coco\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x8680\xe7\xb1\xbb\xe5\x88\xab\n            num_classes = int(module_def[\'classes\'])\n            # \xe8\xbe\x93\xe5\x85\xa5\xe7\x9a\x84\xe8\xae\xad\xe7\xbb\x83\xe5\x9b\xbe\xe5\x83\x8f\xe5\xa4\xa7\xe5\xb0\x8f416\n            img_height = int(hyperparams[\'height\'])\n            # Define detection layer \xe5\xae\x9a\xe4\xb9\x89\xe6\xa3\x80\xe6\xb5\x8b\xe5\xb1\x82\n            yolo_layer = YOLOLayer(anchors, num_classes, img_height)\n            modules.add_module(\'yolo_%d\' % i, yolo_layer)\n        # Register module list and number of output filters\n        # \xe6\xb3\xa8\xe5\x86\x8c\xe6\xa8\xa1\xe5\x9d\x97\xe5\x88\x97\xe8\xa1\xa8\xe5\x92\x8c\xe8\xbe\x93\xe5\x87\xba\xe8\xbf\x87\xe6\xbb\xa4\xe5\x99\xa8\xe7\x9a\x84\xe6\x95\xb0\xe9\x87\x8f\n        #\xe4\xbf\x9d\xe5\xad\x98 \xe6\xa8\xa1\xe5\x9e\x8b\xe7\xbb\x93\xe6\x9e\x84list\n        module_list.append(modules)\n        # \xe4\xbf\x9d\xe5\xad\x98\xe6\xaf\x8f\xe5\xb1\x82\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe7\xbb\x93\xe6\x9e\x9clist\n        output_filters.append(filters)\n\n    return hyperparams, module_list\n\nclass EmptyLayer(nn.Module):\n    """"""Placeholder for \'route\' and \'shortcut\' layers""""""\n    \'\'\'\n    \xe2\x80\x9croute\xe2\x80\x9d\xe5\x92\x8c\xe2\x80\x9cshortcut\xe2\x80\x9d\xe5\xb1\x82\xe7\x9a\x84\xe5\x8d\xa0\xe4\xbd\x8d\xe7\xac\xa6\n    \'\'\'\n    def __init__(self):\n        super(EmptyLayer, self).__init__()\n\nclass YOLOLayer(nn.Module):\n    """"""Detection layer""""""\n    \'\'\'\n    \xe6\xa3\x80\xe6\xb5\x8b\xe5\xb1\x82\n    \xe8\xae\xad\xe7\xbb\x83\xe6\x97\xb6\xe8\xae\xa1\xe7\xae\x97\xe6\x8d\x9f\xe5\xa4\xb1\n    \xe9\xa2\x84\xe6\xb5\x8b\xe6\x97\xb6\xe8\xbe\x93\xe5\x87\xba\xe9\xa2\x84\xe6\xb5\x8b\xe7\xbb\x93\xe6\x9e\x9c\n    \'\'\'\n    def __init__(self, anchors, num_classes, img_dim):\n        \'\'\'\n        :param anchors: \xe8\xaf\xa5\xe6\xa3\x80\xe6\xb5\x8b\xe5\xb1\x82 \xe6\x8c\x91\xe9\x80\x89\xe7\x9a\x84\xe5\x87\xa0\xe4\xb8\xaaanchor\n        :param num_classes: \xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe7\xb1\xbb\xe5\x88\xab\xef\xbc\x8ccoco\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe5\x85\xb180\xe7\xb1\xbb\n        :param img_dim: \xe8\xbe\x93\xe5\x85\xa5\xe5\x9b\xbe\xe5\x83\x8f\xe5\xa4\xa7\xe5\xb0\x8f416\n        \'\'\'\n        super(YOLOLayer, self).__init__()\n        self.anchors = anchors    #\xe8\xaf\xa5\xe6\xa3\x80\xe6\xb5\x8b\xe5\xb1\x82 \xe6\x8c\x91\xe9\x80\x89\xe7\x9a\x84\xe5\x87\xa0\xe4\xb8\xaaanchor\n        self.num_anchors = len(anchors)\n        self.num_classes = num_classes  #\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe7\xb1\xbb\xe5\x88\xab\xef\xbc\x8ccoco\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe5\x85\xb180\xe7\xb1\xbb\n        self.bbox_attrs = 5 + num_classes  #\xe4\xb8\x80\xe4\xb8\xaa \xe7\xbd\x91\xe6\xa0\xbc\xe9\x9c\x80\xe8\xa6\x81\xe9\xa2\x84\xe6\xb5\x8b\xe7\x9a\x84\xe5\x80\xbc\xe4\xb8\xaa\xe6\x95\xb0\n        self.img_dim = img_dim   # \xe8\xbe\x93\xe5\x85\xa5\xe8\xae\xad\xe7\xbb\x83\xe5\x9b\xbe\xe5\x83\x8f\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f\n        self.ignore_thres = 0.5  #  \xe6\x98\xaf\xe5\x90\xa6\xe4\xb8\xba\xe7\x89\xa9\xe4\xbd\x93\xe7\x9a\x84\xe9\x98\x88\xe5\x80\xbc\xef\xbc\x88 \xe9\xa2\x84\xe6\xb5\x8b\xe7\xbb\x93\xe6\x9e\x9c\xef\xbc\x8c\xe5\x8d\xb3\xe7\x89\xa9\xe4\xbd\x93\xe7\xbd\xae\xe4\xbf\xa1\xe5\xba\xa6\xe5\xb0\x8f\xe4\xba\x8e\xe8\xaf\xa5\xe9\x98\x88\xe5\x80\xbc\xef\xbc\x8c\xe5\x88\x99\xe8\xae\xa4\xe4\xb8\xba\xe8\xaf\xa5\xe5\xa4\x84\xe6\xb2\xa1\xe6\x9c\x89\xe9\xa2\x84\xe6\xb5\x8b\xe5\x88\xb0\xe7\x89\xa9\xe4\xbd\x93\xef\xbc\x89\n        self.lambda_coord = 1  #\xe8\xae\xa1\xe7\xae\x97\xe6\x8d\x9f\xe5\xa4\xb1\xe6\x97\xb6\xe7\x9a\x84lambda\xef\xbc\x8c\xe4\xb8\x80\xe8\x88\xac\xe9\xbb\x98\xe8\xae\xa4\xe4\xb8\xba1\xef\xbc\x88\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x85\xac\xe5\xbc\x8f\xe4\xb8\xad\xef\xbc\x8c\xe7\x94\xa8\xe4\xba\x8e\xe8\xb0\x83\xe8\x8a\x82 \xe5\x88\x86\xe7\xb1\xbb  \xe5\x92\x8c \xe6\xa3\x80\xe6\xb5\x8b  \xe7\x9a\x84\xe6\xaf\x94\xe9\x87\x8d\xef\xbc\x89\n\n        self.mse_loss = nn.MSELoss()   #\xe5\x9d\x87\xe6\x96\xb9\xe8\xaf\xaf\xe5\xb7\xae \xe6\x8d\x9f\xe5\xa4\xb1\xe5\x87\xbd\xe6\x95\xb0\xef\xbc\x8c\xe8\xae\xa1\xe7\xae\x97 \xe6\xa3\x80\xe6\xb5\x8b\xe6\x97\xb6\xe7\x9a\x84\xe5\x9d\x90\xe6\xa0\x87\xe6\x8d\x9f\xe5\xa4\xb1\n        self.bce_loss = nn.BCELoss()  #\xe8\xae\xa1\xe7\xae\x97\xe7\x9b\xae\xe6\xa0\x87\xe5\x92\x8c\xe8\xbe\x93\xe5\x87\xba\xe4\xb9\x8b\xe9\x97\xb4\xe7\x9a\x84\xe4\xba\x8c\xe8\xbf\x9b\xe5\x88\xb6\xe4\xba\xa4\xe5\x8f\x89\xe7\x86\xb5  \xe6\x8d\x9f\xe5\xa4\xb1\xe5\x87\xbd\xe6\x95\xb0\xef\xbc\x8c\xe8\xae\xa1\xe7\xae\x97  \xe5\xa4\x9a\xe7\xb1\xbb\xe5\x88\xab\xe7\x9a\x84\xe5\x88\x86\xe7\xb1\xbb\xe6\x8d\x9f\xe5\xa4\xb1\n\n    def forward(self, x, targets=None):\n        # yolo\xe6\x9c\x893\xe4\xb8\xaa\xe6\xa3\x80\xe6\xb5\x8b\xe5\xb1\x8213x13,26x26,52x52\xef\xbc\x8c\xe8\xbf\x99\xe9\x87\x8c\xe4\xbb\xa5 \xe7\xac\xac\xe4\xb8\x80\xe4\xb8\xaa\xe6\xa3\x80\xe6\xb5\x8b\xe5\xb1\x8213x13\xe4\xb8\xba\xe4\xbe\x8b\n        # x [16,255,13,13]  16:batch\xe6\x95\xb0    255\xef\xbc\x9a\xe6\xb7\xb1\xe5\xba\xa6   13x13\xef\xbc\x9afeature map\xe5\xa4\xa7\xe5\xb0\x8f\n        bs = x.size(0)\n        g_dim = x.size(2)  # feature map\xe5\xa4\xa7\xe5\xb0\x8f\n        stride =  self.img_dim / g_dim   # feature\xe7\x9b\xb8\xe5\xaf\xb9\xe4\xba\x8e\xe5\x8e\x9f\xe5\x9b\xbe416\xe7\x9a\x84\xe7\xbc\xa9\xe6\x94\xbe\xe5\x80\x8d\xe6\x95\xb0   32\n        # Tensors for cuda support   \xe8\xae\xbe\xe7\xbd\xae\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96tensor\xe7\x9a\x84\xe9\xbb\x98\xe8\xae\xa4\xe7\xb1\xbb\xe5\x9e\x8b\n        FloatTensor = torch.cuda.FloatTensor if x.is_cuda else torch.FloatTensor\n        LongTensor = torch.cuda.LongTensor if x.is_cuda else torch.LongTensor\n\n        # [16,3,13,13,85]     contiguous\xe8\xbf\x94\xe5\x9b\x9e\xe4\xb8\x80\xe4\xb8\xaa\xe5\x86\x85\xe5\xad\x98\xe8\xbf\x9e\xe7\xbb\xad\xe7\x9a\x84\xe6\x9c\x89\xe7\x9b\xb8\xe5\x90\x8c\xe6\x95\xb0\xe6\x8d\xae\xe7\x9a\x84 tensor\n        prediction = x.view(bs,  self.num_anchors, self.bbox_attrs, g_dim, g_dim).permute(0, 1, 3, 4, 2).contiguous()\n\n        # Get outputs    85\xe4\xb8\xad0-3 \xe4\xb8\xba\xe9\xa2\x84\xe6\xb5\x8b\xe7\x9a\x84\xe6\xa1\x86\xe5\x81\x8f\xe7\xa7\xbb\xef\xbc\x8c4\xe4\xb8\xba \xe7\x89\xa9\xe4\xbd\x93\xe7\xbd\xae\xe4\xbf\xa1\xe5\xba\xa6\xef\xbc\x88\xe6\x98\xaf\xe5\x90\xa6\xe6\x9c\x89\xe7\x89\xa9\xe4\xbd\x93\xef\xbc\x89  5\xef\xbc\x9a \xe4\xb8\xba\xe5\xa4\x9a\xe7\xb1\xbb\xe5\x88\xab\xe7\x9a\x84\xe5\x88\x86\xe7\xb1\xbb\xe6\xa6\x82\xe7\x8e\x87\n        x = torch.sigmoid(prediction[..., 0])          # Center x  [16,3,13,13]\n        y = torch.sigmoid(prediction[..., 1])          # Center y  [16,3,13,13]\n        w = prediction[..., 2]                         # Width     [16,3,13,13]\n        h = prediction[..., 3]                         # Height    [16,3,13,13]\n        conf = torch.sigmoid(prediction[..., 4])       # Conf      [16,3,13,13]\n        pred_cls = torch.sigmoid(prediction[..., 5:])  # Cls pred. [16,3,13,13,80]\n\n        # Calculate offsets for each grid \xe8\xae\xa1\xe7\xae\x97\xe6\xaf\x8f\xe4\xb8\xaa\xe7\xbd\x91\xe6\xa0\xbc\xe7\x9a\x84\xe5\x81\x8f\xe7\xa7\xbb\xe9\x87\x8f\n        # torch.linspace\xe8\xbf\x94\xe5\x9b\x9e start \xe5\x92\x8c end \xe4\xb9\x8b\xe9\x97\xb4\xe7\xad\x89\xe9\x97\xb4\xe9\x9a\x94 steps \xe7\x82\xb9\xe7\x9a\x84\xe4\xb8\x80\xe7\xbb\xb4 Tensor\n        # repeat\xe6\xb2\xbf\xe7\x9d\x80\xe6\x8c\x87\xe5\xae\x9a\xe7\x9a\x84\xe5\xb0\xba\xe5\xaf\xb8\xe9\x87\x8d\xe5\xa4\x8d tensor\n        # \xe8\xbf\x87\xe7\xa8\x8b\xef\xbc\x9a\n        #      torch.linspace(0, g_dim-1, g_dim)  ->  [1,13]\xe7\x9a\x84tensor\n        #      repeat(g_dim,1)                    ->  [13,13]\xe7\x9a\x84tensor \xe6\xaf\x8f\xe8\xa1\x8c\xe5\x86\x85\xe5\xae\xb9\xe4\xb8\xba0-12,\xe5\x85\xb113\xe8\xa1\x8c\n        #      repeat(bs*self.num_anchors, 1, 1)  ->  [48,13,13]\xe7\x9a\x84tensor   [13,13]\xe5\x86\x85\xe5\xae\xb9\xe4\xb8\x8d\xe5\x8f\x98\xef\xbc\x8c\xe5\x9c\xa8\xe6\x89\xa9\xe5\xb1\x95\xe7\x9a\x84\xe4\xb8\x80\xe7\xbb\xb4\xe4\xb8\x8a\xe9\x87\x8d\xe5\xa4\x8d48\xe6\xac\xa1\n        #      view(x.shape)                      ->  resize\xe6\x88\x90[16.3.13.13]\xe7\x9a\x84tensor\n        # grid_x\xe3\x80\x81grid_y\xe7\x94\xa8\xe4\xba\x8e \xe5\xae\x9a\xe4\xbd\x8d feature map\xe7\x9a\x84\xe7\xbd\x91\xe6\xa0\xbc\xe5\xb7\xa6\xe4\xb8\x8a\xe8\xa7\x92\xe5\x9d\x90\xe6\xa0\x87\n        grid_x = torch.linspace(0, g_dim-1, g_dim).repeat(g_dim,1).repeat(bs*self.num_anchors, 1, 1).view(x.shape).type(FloatTensor)    # [16.3.13.13]  \xe6\xaf\x8f\xe8\xa1\x8c\xe5\x86\x85\xe5\xae\xb9\xe4\xb8\xba0-12,\xe5\x85\xb113\xe8\xa1\x8c\n        grid_y = torch.linspace(0, g_dim-1, g_dim).repeat(g_dim,1).t().repeat(bs*self.num_anchors, 1, 1).view(y.shape).type(FloatTensor)  # [16.3.13.13]  \xe6\xaf\x8f\xe5\x88\x97\xe5\x86\x85\xe5\xae\xb9\xe4\xb8\xba0-12,\xe5\x85\xb113\xe5\x88\x97\xef\xbc\x88\xe5\x9b\xa0\xe4\xb8\xba\xe4\xbd\xbf\xe7\x94\xa8\xe8\xbd\xac\xe7\xbd\xaeT\xef\xbc\x89\n        scaled_anchors = [(a_w / stride, a_h / stride) for a_w, a_h in self.anchors]  #\xe5\xb0\x86 \xe5\x8e\x9f\xe5\x9b\xbe\xe5\xb0\xba\xe5\xba\xa6\xe7\x9a\x84\xe9\x94\x9a\xe6\xa1\x86\xe4\xb9\x9f\xe7\xbc\xa9\xe6\x94\xbe\xe5\x88\xb0\xe7\xbb\x9f\xe4\xb8\x80\xe5\xb0\xba\xe5\xba\xa6\xe4\xb8\x8b\n        anchor_w = FloatTensor(scaled_anchors).index_select(1, LongTensor([0]))  #[3,1]  3\xe4\xb8\xaa\xe9\x94\x9a\xe6\xa1\x86\xe7\x9a\x84w\xe5\x80\xbc\n        anchor_h = FloatTensor(scaled_anchors).index_select(1, LongTensor([1]))  #[3,1]  3\xe4\xb8\xaa\xe9\x94\x9a\xe6\xa1\x86\xe7\x9a\x84h\xe5\x80\xbc\n        anchor_w = anchor_w.repeat(bs, 1).repeat(1, 1, g_dim*g_dim).view(w.shape) #[16,3,13,13]\n        anchor_h = anchor_h.repeat(bs, 1).repeat(1, 1, g_dim*g_dim).view(h.shape) #[16,3,13,13]\n\n        # Add offset and scale with anchors  \xe7\xbb\x99\xe9\x94\x9a\xe6\xa1\x86\xe6\xb7\xbb\xe5\x8a\xa0\xe5\x81\x8f\xe7\xa7\xbb\xe9\x87\x8f\xe5\x92\x8c\xe6\xaf\x94\xe4\xbe\x8b\n        pred_boxes = FloatTensor(prediction[..., :4].shape)  #\xe6\x96\xb0\xe5\xbb\xba\xe4\xb8\x80\xe4\xb8\xaatensor[16,3,13,13,4]\n        # pred_boxes\xe4\xb8\xba \xe5\x9c\xa813x13\xe7\x9a\x84feature map\xe5\xb0\xba\xe5\xba\xa6\xe4\xb8\x8a\xe7\x9a\x84\xe9\xa2\x84\xe6\xb5\x8b\xe6\xa1\x86\n        # x,y\xe4\xb8\xba\xe9\xa2\x84\xe6\xb5\x8b\xe5\x80\xbc\xef\xbc\x88\xe7\xbd\x91\xe6\xa0\xbc\xe5\x86\x85\xe7\x9a\x84\xe5\x9d\x90\xe6\xa0\x87\xef\xbc\x8c\xe7\xbb\x8f\xe8\xbf\x87sigmoid\xe4\xb9\x8b\xe5\x90\x8e\xe5\x80\xbc\xe4\xb8\xba0-1\xe4\xb9\x8b\xe9\x97\xb4\xef\xbc\x89 grid_x\xef\xbc\x8cgrid_y\xe5\xae\x9a\xe4\xbd\x8d\xe7\xbd\x91\xe6\xa0\xbc\xe5\xb7\xa6\xe4\xb8\x8a\xe8\xa7\x92\xe5\x81\x8f\xe7\xa7\xbb\xe5\x9d\x90\xe6\xa0\x87\xef\xbc\x88\xe5\x80\xbc\xe5\x9c\xa80-12\xe4\xb9\x8b\xe9\x97\xb4\xef\xbc\x89\n        pred_boxes[..., 0] = x.data + grid_x\n        pred_boxes[..., 1] = y.data + grid_y\n        # w\xef\xbc\x8ch\xe4\xb8\xba \xe9\xa2\x84\xe6\xb5\x8b\xe5\x80\xbc\xef\xbc\x8c\xe5\x8d\xb3\xe7\x9b\xb8\xe5\xaf\xb9\xe4\xba\x8e\xe5\x8e\x9f\xe9\x94\x9a\xe6\xa1\x86\xe7\x9a\x84\xe5\x81\x8f\xe7\xa7\xbb\xe5\x80\xbc    anchor_w\xef\xbc\x8canchor_h\xe4\xb8\xba \xe7\xbd\x91\xe6\xa0\xbc\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x843\xe4\xb8\xaa\xe9\x94\x9a\xe6\xa1\x86\n        pred_boxes[..., 2] = torch.exp(w.data) * anchor_w\n        pred_boxes[..., 3] = torch.exp(h.data) * anchor_h\n\n        # Training \xe8\xae\xad\xe7\xbb\x83\xe9\x98\xb6\xe6\xae\xb5\n        if targets is not None:\n\n            # \xe5\xb0\x86\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x87\xbd\xe6\x95\xb0\xe8\xbd\xac\xe5\x88\xb0GPU\xe4\xb8\x8a\xef\xbc\x8c\xe7\xac\xac\xe4\xb8\x80\xe6\xac\xa1\xe8\xa7\x81...\n            if x.is_cuda:\n                self.mse_loss = self.mse_loss.cuda()\n                self.bce_loss = self.bce_loss.cuda()\n            # nGT \xe7\xbb\x9f\xe8\xae\xa1\xe4\xb8\x80\xe4\xb8\xaabatch\xe4\xb8\xad\xe7\x9a\x84\xe7\x9c\x9f\xe5\x80\xbc\xe6\xa1\x86\xe4\xb8\xaa\xe6\x95\xb0\n            # nCorrect \xe7\xbb\x9f\xe8\xae\xa1 \xe4\xb8\x80\xe4\xb8\xaabatch\xe9\xa2\x84\xe6\xb5\x8b\xe5\x87\xba\xe6\x9c\x89\xe7\x89\xa9\xe4\xbd\x93\xe7\x9a\x84\xe4\xb8\xaa\xe6\x95\xb0\n            # mask   [16,3,13,13]\xe5\x85\xa80   \xe5\x9c\xa83\xe4\xb8\xaa\xe5\x8e\x9f\xe5\xa7\x8b\xe9\x94\x9a\xe6\xa1\x86\xe4\xb8\x8e \xe7\x9c\x9f\xe5\x80\xbc\xe6\xa1\x86 iou\xe6\x9c\x80\xe5\xa4\xa7\xe7\x9a\x84\xe9\x82\xa3\xe4\xb8\xaa\xe9\x94\x9a\xe6\xa1\x86  \xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84\xe9\xa2\x84\xe6\xb5\x8b\xe6\xa1\x86\xe4\xbd\x8d\xe7\xbd\xae\xe7\xbd\xae\xe4\xb8\xba1 \xef\xbc\x8c\xe5\x8d\xb3  \xe8\xb4\x9f\xe8\xb4\xa3\xe6\xa3\x80\xe6\xb5\x8b\xe7\x89\xa9\xe4\xbd\x93\xe7\x9a\x84\xe4\xbd\x8d\xe7\xbd\xae\xe4\xb8\xba1\n            # conf_mask  [16,3,13,13]  \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe5\x85\xa81\xef\xbc\x8c\xe4\xb9\x8b\xe5\x90\x8e\xe7\x9a\x84\xe6\x93\x8d\xe4\xbd\x9c\xef\xbc\x9a\xe8\xb4\x9f\xe8\xb4\xa3\xe9\xa2\x84\xe6\xb5\x8b\xe7\x89\xa9\xe4\xbd\x93\xe7\x9a\x84\xe7\xbd\x91\xe6\xa0\xbc\xe7\xbd\xae\xe4\xb8\xba1\xef\xbc\x8c\xe5\xae\x83\xe5\x91\xa8\xe5\x9b\xb4\xe7\xbd\x91\xe6\xa0\xbc\xe7\xbd\xae\xe4\xb8\xba0\n            # tx, ty [16,3,13,13] \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe5\x85\xa8\xe4\xb8\xba0\xef\xbc\x8c\xe5\x9c\xa8\xe6\x9c\x89\xe7\x9c\x9f\xe5\x80\xbc\xe6\xa1\x86\xe7\x9a\x84\xe7\xbd\x91\xe6\xa0\xbc\xe4\xbd\x8d\xe7\xbd\xae\xe5\x86\x99\xe5\x85\xa5 \xe7\x9c\x9f\xe5\xae\x9e\xe7\x9a\x84\xe7\x89\xa9\xe4\xbd\x93\xe4\xb8\xad\xe5\xbf\x83\xe7\x82\xb9\xe5\x9d\x90\xe6\xa0\x87\n            # tw, th  [16,3,13,13] \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe5\x85\xa8\xe4\xb8\xba0\xef\xbc\x8c\xe8\xaf\xa5\xe5\x80\xbc\xe4\xb8\xba \xe7\x9c\x9f\xe5\x80\xbc\xe6\xa1\x86\xe7\x9a\x84w\xe3\x80\x81h \xe6\x8c\x89\xe7\x85\xa7\xe5\x85\xac\xe5\xbc\x8f\xe8\xbd\xac\xe5\x8c\x96\xe4\xb8\xba \xe7\xbd\x91\xe7\xbb\x9c\xe8\xbe\x93\xe5\x87\xba\xe6\x97\xb6\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84\xe7\x9c\x9f\xe5\x80\xbc\xef\xbc\x88\xe8\xaf\xa5\xe5\x80\xbc\xe5\xaf\xb9\xe5\xba\x94\xe4\xba\x8e \xe7\xbd\x91\xe7\xbb\x9c\xe8\xbe\x93\xe5\x87\xba\xe7\x9a\x84\xe7\x9c\x9f\xe5\x80\xbc\xef\xbc\x89\n            # tconf [16,3,13,13]   \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe5\x85\xa80\xef\xbc\x8c\xe6\x9c\x89\xe7\x9c\x9f\xe5\x80\xbc\xe6\xa1\x86\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84\xe7\xbd\x91\xe6\xa0\xbc\xe4\xbd\x8d\xe7\xbd\xae\xe4\xb8\xba1  \xe6\xa0\x87\xe6\x98\x8e \xe7\x89\xa9\xe4\xbd\x93\xe4\xb8\xad\xe5\xbf\x83\xe7\x82\xb9\xe8\x90\xbd\xe5\x9c\xa8\xe8\xaf\xa5\xe7\xbd\x91\xe6\xa0\xbc\xe4\xb8\xad\xef\xbc\x8c\xe8\xaf\xa5\xe7\xbd\x91\xe6\xa0\xbc\xe5\x8e\xbb\xe8\xb4\x9f\xe8\xb4\xa3\xe9\xa2\x84\xe6\xb5\x8b\xe7\x89\xa9\xe4\xbd\x93\n            # tcls    #[16,3,13,13,80]  \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe5\x85\xa80\xef\xbc\x8c\xe7\xbb\x8f\xe8\xbf\x87one-hot\xe7\xbc\x96\xe7\xa0\x81\xe5\x90\x8e  \xe5\x9c\xa8\xe7\x9c\x9f\xe5\xae\x9e\xe7\xb1\xbb\xe5\x88\xab\xe5\xa4\x84\xe5\x80\xbc\xe4\xb8\xba1\n            nGT, nCorrect, mask, conf_mask, tx, ty, tw, th, tconf, tcls = build_targets(pred_boxes.cpu().data,   #\xe5\x9c\xa813x13\xe5\xb0\xba\xe5\xba\xa6\xe4\xb8\x8a\xe7\x9a\x84\xe9\xa2\x84\xe6\xb5\x8b\xe6\xa1\x86  [16,3,13,13,4]\n                                                                            targets.cpu().data,                  #\xe5\x9d\x90\xe6\xa0\x87\xe8\xa2\xab\xe5\xbd\x92\xe4\xb8\x80\xe5\x8c\x96\xe5\x90\x8e\xe7\x9a\x84\xe7\x9c\x9f\xe5\x80\xbc\xe6\xa1\x86filled_labels[16,50,5] \xe5\x80\xbc\xe5\x9c\xa80-1\xe4\xb9\x8b\xe9\x97\xb4\n                                                                            scaled_anchors,                      #\xe7\xbc\xa9\xe6\x94\xbe\xe5\x88\xb013x13\xe5\xb0\xba\xe5\xba\xa6\xe4\xb8\x8b\xe7\x9a\x843\xe4\xb8\xaa\xe9\x94\x9a\xe6\xa1\x86\n                                                                            self.num_anchors,                    #\xe9\x94\x9a\xe6\xa1\x86\xe4\xb8\xaa\xe6\x95\xb03\n                                                                            self.num_classes,                    #\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe7\xb1\xbb\xe5\x88\xab\xe6\x95\xb0  coco\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x8680\n                                                                            g_dim,                               #feature map\xe7\x9b\xb8\xe5\xaf\xb9\xe4\xba\x8e\xe5\x8e\x9f\xe5\x9b\xbe\xe7\x9a\x84\xe7\xbc\xa9\xe6\x94\xbe\xe5\x80\x8d\xe6\x95\xb013\n                                                                            self.ignore_thres,                   # \xe9\x98\x88\xe5\x80\xbc\xef\xbc\x88\xe7\x94\xa8\xe4\xba\x8e\xe5\x88\xa4\xe6\x96\xad  \xe7\x9c\x9f\xe5\x80\xbc\xe6\xa1\x86 \xe4\xb8\x8e 3\xe4\xb8\xaa\xe5\x8e\x9f\xe5\xa7\x8b\xe9\x94\x9a\xe6\xa1\x86\xe7\x9a\x84iou > \xe9\x98\x88\xe5\x80\xbc\xef\xbc\x89\n                                                                            self.img_dim)                        #\xe7\xbd\x91\xe7\xbb\x9c\xe8\xbe\x93\xe5\x85\xa5\xe5\x9b\xbe\xe5\x83\x8f\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f 416\n            #  conf[16,3,13,13] \xe4\xb8\xba\xe7\xbd\x91\xe7\xbb\x9c\xe8\xbe\x93\xe5\x87\xba\xe5\x80\xbc\xef\xbc\x8c\xe7\x89\xa9\xe4\xbd\x93\xe7\xbd\xae\xe4\xbf\xa1\xe5\xba\xa6\xef\xbc\x88\xe6\x98\xaf\xe5\x90\xa6\xe6\x9c\x89\xe7\x89\xa9\xe4\xbd\x93\xef\xbc\x89\n            nProposals = int((conf > 0.25).sum().item())\n            # \xe5\x8f\xac\xe5\x9b\x9e\xe7\x8e\x87recall = \xe9\xa2\x84\xe6\xb5\x8b\xe5\x87\xba\xe6\x9c\x89\xe7\x89\xa9\xe4\xbd\x93 / \xe7\x9c\x9f\xe5\x80\xbc\xe6\xa1\x86\xe6\x80\xbb\xe6\x95\xb0\n            recall = float(nCorrect / nGT) if nGT else 1\n\n            # Handle masks\n            mask = Variable(mask.type(FloatTensor))\n            cls_mask = Variable(mask.unsqueeze(-1).repeat(1, 1, 1, 1, self.num_classes).type(FloatTensor))\n            conf_mask = Variable(conf_mask.type(FloatTensor))\n\n            # Handle target variables\n            tx    = Variable(tx.type(FloatTensor), requires_grad=False)\n            ty    = Variable(ty.type(FloatTensor), requires_grad=False)\n            tw    = Variable(tw.type(FloatTensor), requires_grad=False)\n            th    = Variable(th.type(FloatTensor), requires_grad=False)\n            tconf = Variable(tconf.type(FloatTensor), requires_grad=False)\n            tcls  = Variable(tcls.type(FloatTensor), requires_grad=False)\n\n            # Mask outputs to ignore non-existing objects  \xe9\x80\x9a\xe8\xbf\x87\xe6\x8e\xa9\xe7\xa0\x81\xe6\x9d\xa5\xe5\xbf\xbd\xe7\x95\xa5 \xe4\xb8\x8d\xe5\xad\x98\xe5\x9c\xa8\xe7\x89\xa9\xe4\xbd\x93\n            # mask \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe5\x85\xa8\xe4\xb8\xba0\xef\xbc\x8c\xe5\x8f\xaa\xe6\x9c\x89  \xe5\x9c\xa83\xe4\xb8\xaa\xe5\x8e\x9f\xe5\xa7\x8b\xe9\x94\x9a\xe6\xa1\x86\xe4\xb8\x8e \xe7\x9c\x9f\xe5\x80\xbc\xe6\xa1\x86 iou\xe6\x9c\x80\xe5\xa4\xa7\xe7\x9a\x84\xe9\x82\xa3\xe4\xb8\xaa\xe9\x94\x9a\xe6\xa1\x86  \xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84\xe9\xa2\x84\xe6\xb5\x8b\xe6\xa1\x86\xe4\xbd\x8d\xe7\xbd\xae\xe7\xbd\xae\xe4\xb8\xba1\xef\xbc\x8c\xe5\x8d\xb3  \xe8\xb4\x9f\xe8\xb4\xa3\xe6\xa3\x80\xe6\xb5\x8b\xe7\x89\xa9\xe4\xbd\x93\xe7\x9a\x84\xe4\xbd\x8d\xe7\xbd\xae\xe4\xb8\xba1\n            loss_x = self.lambda_coord * self.bce_loss(x * mask, tx * mask)\n            loss_y = self.lambda_coord * self.bce_loss(y * mask, ty * mask)\n            loss_w = self.lambda_coord * self.mse_loss(w * mask, tw * mask) / 2   # \xe4\xb8\xba\xe4\xbd\x95 /2 ?\n            loss_h = self.lambda_coord * self.mse_loss(h * mask, th * mask) / 2\n            # \xe6\x9c\x89\xe6\x97\xa0\xe7\x89\xa9\xe4\xbd\x93\xe6\x8d\x9f\xe5\xa4\xb1  conf_mask  [16,3,13,13]  \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe5\x85\xa81\xef\xbc\x8c\xe4\xb9\x8b\xe5\x90\x8e\xe7\x9a\x84\xe6\x93\x8d\xe4\xbd\x9c\xef\xbc\x9a\xe8\xb4\x9f\xe8\xb4\xa3\xe9\xa2\x84\xe6\xb5\x8b\xe7\x89\xa9\xe4\xbd\x93\xe7\x9a\x84\xe7\xbd\x91\xe6\xa0\xbc\xe7\xbd\xae\xe4\xb8\xba1\xef\xbc\x8c\xe5\xae\x83\xe5\x91\xa8\xe5\x9b\xb4\xe7\xbd\x91\xe6\xa0\xbc\xe7\xbd\xae\xe4\xb8\xba0\n            loss_conf = self.bce_loss(conf * conf_mask, tconf * conf_mask)\n            # \xe5\xa4\x9a\xe5\x88\x86\xe7\xb1\xbb\xe6\x8d\x9f\xe5\xa4\xb1\n            loss_cls = self.bce_loss(pred_cls * cls_mask, tcls * cls_mask)\n            loss = loss_x + loss_y + loss_w + loss_h + loss_conf + loss_cls\n            # \xe6\x80\xbbloss\xe4\xb8\xba loss_x\xe3\x80\x81loss_y\xe3\x80\x81loss_w\xe3\x80\x81loss_h\xe3\x80\x81loss_conf\xe3\x80\x81loss_cls\xe4\xb9\x8b\xe5\x92\x8c\n            return loss, loss_x.item(), loss_y.item(), loss_w.item(), loss_h.item(), loss_conf.item(), loss_cls.item(), recall\n\n        else:\n            # If not in training phase return predictions\n            # \xe9\xa2\x84\xe6\xb5\x8b\xe9\x98\xb6\xe6\xae\xb5\xef\xbc\x8c\xe8\xbf\x94\xe5\x9b\x9e \xe9\xa2\x84\xe6\xb5\x8b\xe7\xbb\x93\xe6\x9e\x9c\n            output = torch.cat((pred_boxes.view(bs, -1, 4) * stride, conf.view(bs, -1, 1), pred_cls.view(bs, -1, self.num_classes)), -1)\n            return output.data\n\n\nclass Darknet(nn.Module):\n    """"""YOLOv3 object detection model""""""\n    \'\'\'\n    YOLOv3\xe7\x89\xa9\xe4\xbd\x93\xe6\xa3\x80\xe6\xb5\x8b\xe6\xa8\xa1\xe5\x9e\x8b\n    \'\'\'\n    def __init__(self, config_path, img_size=416):\n        \'\'\'\n        \xe8\xbe\x93\xe5\x85\xa5\xe9\x80\x9a\xe5\xb8\xb8\xe4\xb8\xba416\xef\xbc\x8832\xe7\x9a\x84\xe5\x80\x8d\xe6\x95\xb0\xef\xbc\x89\n        \xe7\x90\x86\xe7\x94\xb1\xef\xbc\x9a\xe5\x8f\x82\xe4\xb8\x8e\xe9\xa2\x84\xe6\xb5\x8b\xe5\xb1\x82\xe7\x9a\x84\xe6\x9c\x80\xe5\xb0\x8f\xe7\x89\xb9\xe5\xbe\x81\xe5\x9b\xbe\xe4\xb8\xba13x13,\xe4\xb8\xba\xe5\x8e\x9f\xe5\x9b\xbe\xe7\xbc\xa9\xe5\xb0\x8f32\xe5\x80\x8d\n        \'\'\'\n        super(Darknet, self).__init__()\n        # \xe5\xb0\x86cfg\xe9\x85\x8d\xe7\xbd\xae\xe6\x96\x87\xe4\xbb\xb6\xe8\xbd\xac\xe5\x8c\x96\xe4\xb8\xbalist,\xe6\xaf\x8f\xe4\xb8\x80\xe8\xa1\x8c \xe4\xb8\xba\xe7\xbd\x91\xe7\xbb\x9c\xe7\x9a\x84\xe4\xb8\x80\xe9\x83\xa8\xe5\x88\x86\n        self.module_defs = parse_model_config(config_path)\n        # \xe8\xa7\xa3\xe6\x9e\x90list\xef\xbc\x8c\xe8\xbf\x94\xe5\x9b\x9e pytorch\xe6\xa8\xa1\xe5\x9e\x8b\xe7\xbb\x93\xe6\x9e\x84\n        self.hyperparams, self.module_list = create_modules(self.module_defs)\n        self.img_size = img_size\n        self.loss_names = [\'x\', \'y\', \'w\', \'h\', \'conf\', \'cls\', \'recall\']\n\n    def forward(self, x, targets=None):\n        # True: \xe8\xae\xad\xe7\xbb\x83\xe9\x98\xb6\xe6\xae\xb5    False:\xe9\xa2\x84\xe6\xb5\x8b\xe9\x98\xb6\xe6\xae\xb5\n        is_training = targets is not None\n        output = []\n        self.losses = defaultdict(float)\n        # \xe4\xbf\x9d\xe5\xad\x98\xe6\xaf\x8f\xe4\xb8\x80\xe5\xb1\x82\xe7\x9a\x84\xe7\xbd\x91\xe7\xbb\x9c\xe8\xbe\x93\xe5\x87\xba\xe5\x80\xbc\n        layer_outputs = []\n        for i, (module_def, module) in enumerate(zip(self.module_defs, self.module_list)):\n            if module_def[\'type\'] in [\'convolutional\', \'upsample\']:\n                x = module(x)\n            elif module_def[\'type\'] == \'route\':\n                \'\'\'\n                route \xe6\x8c\x87 \xe6\x8c\x89\xe7\x85\xa7\xe5\x88\x97\xe6\x9d\xa5\xe5\x90\x88\xe5\xb9\xb6tensor,\xe5\x8d\xb3\xe6\x89\xa9\xe5\xb1\x95\xe6\xb7\xb1\xe5\xba\xa6\n                \n                \xe5\xbd\x93\xe5\xb1\x9e\xe6\x80\xa7\xe5\x8f\xaa\xe6\x9c\x89\xe4\xb8\x80\xe4\xb8\xaa\xe5\x80\xbc\xe6\x97\xb6\xef\xbc\x8c\xe5\xae\x83\xe4\xbc\x9a\xe8\xbe\x93\xe5\x87\xba\xe7\x94\xb1\xe8\xaf\xa5\xe5\x80\xbc\xe7\xb4\xa2\xe5\xbc\x95\xe7\x9a\x84\xe7\xbd\x91\xe7\xbb\x9c\xe5\xb1\x82\xe7\x9a\x84\xe7\x89\xb9\xe5\xbe\x81\xe5\x9b\xbe\xe3\x80\x82\n                \xe5\x9c\xa8\xe6\x88\x91\xe4\xbb\xac\xe7\x9a\x84\xe7\xa4\xba\xe4\xbe\x8b\xe4\xb8\xad\xef\xbc\x8c\xe5\xae\x83\xe6\x98\xaf\xe2\x88\x924\xef\xbc\x8c\xe5\x9b\xa0\xe6\xad\xa4\xe8\xbf\x99\xe4\xb8\xaa\xe5\xb1\x82\xe5\xb0\x86\xe4\xbb\x8eRoute\xe5\xb1\x82\xe5\xbc\x80\xe5\xa7\x8b\xe5\x80\x92\xe6\x95\xb0\xe7\xac\xac4\xe5\xb1\x82\xe7\x9a\x84\xe7\x89\xb9\xe5\xbe\x81\xe5\x9b\xbe\xe3\x80\x82\n                \xe7\x9b\xae\xe7\x9a\x84\xef\xbc\x9a\xe6\x8b\xbf\xe5\x88\xb0\xe4\xb8\xad\xe9\x97\xb4\xe7\xbb\x93\xe6\x9e\x9c\xef\xbc\x8c\xe7\xbb\xa7\xe7\xbb\xad\xe7\xbd\x91\xe7\xbb\x9c\n\n                \xe5\xbd\x93\xe5\x9b\xbe\xe5\xb1\x82\xe6\x9c\x89\xe4\xb8\xa4\xe4\xb8\xaa\xe5\x80\xbc\xe6\x97\xb6\xef\xbc\x8c\xe5\xae\x83\xe4\xbc\x9a\xe8\xbf\x94\xe5\x9b\x9e\xe7\x94\xb1\xe5\x85\xb6\xe5\x80\xbc\xe6\x89\x80\xe7\xb4\xa2\xe5\xbc\x95\xe7\x9a\x84\xe5\x9b\xbe\xe5\xb1\x82\xe7\x9a\x84\xe8\xbf\x9e\xe6\x8e\xa5\xe7\x89\xb9\xe5\xbe\x81\xe5\x9b\xbe\xe3\x80\x82 \n                \xe5\x9c\xa8\xe6\x88\x91\xe4\xbb\xac\xe7\x9a\x84\xe4\xbe\x8b\xe5\xad\x90\xe4\xb8\xad\xef\xbc\x8c\xe5\xae\x83\xe6\x98\xaf\xe2\x88\x921,61\xef\xbc\x8c\xe5\xb9\xb6\xe4\xb8\x94\xe8\xaf\xa5\xe5\x9b\xbe\xe5\xb1\x82\xe5\xb0\x86\xe8\xbe\x93\xe5\x87\xba\xe6\x9d\xa5\xe8\x87\xaa\xe4\xb8\x8a\xe4\xb8\x80\xe5\xb1\x82\xef\xbc\x88-1\xef\xbc\x89\xe5\x92\x8c\xe7\xac\xac61\xe5\xb1\x82\xe7\x9a\x84\xe7\x89\xb9\xe5\xbe\x81\xe5\x9b\xbe\xef\xbc\x8c\xe5\xb9\xb6\xe6\xb2\xbf\xe6\xb7\xb1\xe5\xba\xa6\xe7\x9a\x84\xe7\xbb\xb4\xe5\xba\xa6\xe8\xbf\x9e\xe6\x8e\xa5\xe3\x80\x82\n                \xe7\x9b\xae\xe7\x9a\x84\xef\xbc\x9a\xe6\x8c\x89\xe7\x85\xa7\xe6\xb7\xb1\xe5\xba\xa6\xe7\x9a\x84\xe7\xbb\xb4\xe5\xba\xa6\xe8\xbf\x9e\xe6\x8e\xa5 \xe4\xb8\xa4\xe4\xb8\xaa\xe4\xb8\x8d\xe5\x90\x8c\xe5\xb1\x82\xe7\x9a\x84\xe7\x89\xb9\xe5\xbe\x81\xe5\x9b\xbe\xef\xbc\x8c\xe5\x87\x86\xe5\xa4\x87\xe5\x8e\xbb\xe9\xa2\x84\xe6\xb5\x8b\n                \n                \'\'\'\n                layer_i = [int(x) for x in module_def[\'layers\'].split(\',\')]\n                x = torch.cat([layer_outputs[i] for i in layer_i], 1)\n            elif module_def[\'type\'] == \'shortcut\':\n                \'\'\'\n                shortcut \xe6\x8c\x87  \xe6\xae\x8b\xe5\xb7\xae\xe7\xbb\x93\xe6\x9e\x84\xef\xbc\x8c\xe5\x8d\xb7\xe7\xa7\xaf\xe7\x9a\x84\xe8\xb7\xa8\xe5\xb1\x82\xe8\xbf\x9e\xe6\x8e\xa5\xef\xbc\x8c\xe5\x8d\xb3 \xe5\xb0\x86\xe4\xb8\x8d\xe5\x90\x8c\xe4\xb8\xa4\xe5\xb1\x82\xe8\xbe\x93\xe5\x87\xba\xef\xbc\x88\xe5\x8d\xb3\xe8\xbe\x93\xe5\x87\xba+\xe6\xae\x8b\xe5\xb7\xae\xe5\x9d\x97\xef\xbc\x89\xe7\x9b\xb8\xe5\x8a\xa0 \xe4\xb8\xba \xe6\x9c\x80\xe5\x90\x8e\xe7\xbb\x93\xe6\x9e\x9c\n                \xe5\x8f\x82\xe6\x95\xb0from\xe6\x98\xaf\xe2\x88\x923\xef\xbc\x8c\xe6\x84\x8f\xe6\x80\x9d\xe6\x98\xafshortcut\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe6\x98\xaf\xe9\x80\x9a\xe8\xbf\x87\xe4\xb8\x8e\xe5\x85\x88\xe5\x89\x8d\xe7\x9a\x84\xe5\x80\x92\xe6\x95\xb0\xe7\xac\xac\xe4\xb8\x89\xe5\xb1\x82\xe7\xbd\x91\xe7\xbb\x9c\xe7\x9b\xb8\xe5\x8a\xa0\xe8\x80\x8c\xe5\xbe\x97\xe5\x88\xb0\xe3\x80\x82\n                \'\'\'\n                layer_i = int(module_def[\'from\'])\n                x = layer_outputs[-1] + layer_outputs[layer_i]\n            elif module_def[\'type\'] == \'yolo\':\n                # Train phase: get loss\n                # \xe8\xae\xad\xe7\xbb\x83\xe9\x98\xb6\xe6\xae\xb5\xef\xbc\x9a\xe8\x8e\xb7\xe5\xbe\x97\xe6\x8d\x9f\xe5\xa4\xb1\n                if is_training:\n                    # x\xe4\xb8\xba\xe6\x80\xbbloss, *losses\xe4\xb8\xba\xe5\x90\x84\xe7\xa7\x8dloss\n                    x, *losses = module[0](x, targets)\n                    for name, loss in zip(self.loss_names, losses):\n                        self.losses[name] += loss\n                # Test phase: Get detections\n                # \xe6\xb5\x8b\xe8\xaf\x95\xe9\x98\xb6\xe6\xae\xb5\xef\xbc\x9a\xe8\x8e\xb7\xe5\x8f\x96\xe6\xa3\x80\xe6\xb5\x8b\n                else:\n                    x = module(x)\n                output.append(x)\n            layer_outputs.append(x)\n\n        self.losses[\'recall\'] /= 3\n        # \xe8\xae\xad\xe7\xbb\x83\xe9\x98\xb6\xe6\xae\xb5\xef\xbc\x9a\xe8\xbf\x94\xe5\x9b\x9e\xe6\x80\xbbloss \xe7\x94\xa8\xe4\xba\x8e\xe6\xa2\xaf\xe5\xba\xa6\xe6\x9b\xb4\xe6\x96\xb0\n        # \xe9\xa2\x84\xe6\xb5\x8b\xe9\x98\xb6\xe6\xae\xb5\xef\xbc\x9a\xe8\xbf\x94\xe5\x9b\x9e  \xe9\xa2\x84\xe6\xb5\x8b\xe7\xbb\x93\xe6\x9e\x9c\n        return sum(output) if is_training else torch.cat(output, 1)\n\n\n    def load_weights(self, weights_path):\n        """"""Parses and loads the weights stored in \'weights_path\'""""""\n        \'\'\'\n        \xe8\xa7\xa3\xe6\x9e\x90\xe5\xb9\xb6\xe5\x8a\xa0\xe8\xbd\xbd\xe5\xad\x98\xe5\x82\xa8\xe5\x9c\xa8\'weights_path\xe4\xb8\xad\xe7\x9a\x84\xe6\x9d\x83\xe9\x87\x8d.\n        \xe4\xb8\xba\xe5\x85\xbc\xe5\xae\xb9 \xe5\xae\x98\xe6\x96\xb9yolov3.weight\xe6\x9d\x83\xe9\x87\x8d\xe8\xae\xbe\xe8\xae\xa1\xef\xbc\x8c\xe8\xaf\xa5\xe4\xbb\x93\xe5\xba\x93\xe4\xb8\x8d\xe5\x86\x8d\xe4\xbd\xbf\xe7\x94\xa8\n        \'\'\'\n\n        #Open the weights file\n        fp = open(weights_path, ""rb"")\n        # First five are header values  \xe5\x89\x8d\xe4\xba\x94\xe4\xb8\xaa\xe4\xb8\xba\xe6\xa0\x87\xe9\xa2\x98\xe4\xbf\xa1\xe6\x81\xaf\n        # 1. Major version number\n        # 2. Minor Version Number\n        # 3. Subversion number\n        # 4,5. Images seen by the network (during training)\n        header = np.fromfile(fp, dtype=np.int32, count=5)\n\n        # Needed to write header when saving weights\n        # \xe4\xbf\x9d\xe5\xad\x98\xe6\x9d\x83\xe9\x87\x8d\xe6\x97\xb6\xe9\x9c\x80\xe8\xa6\x81\xe5\x86\x99\xe5\xa4\xb4\n        self.header_info = header\n\n        self.seen = header[3]\n        weights = np.fromfile(fp, dtype=np.float32)         # The rest are weights\n        fp.close()\n\n        ptr = 0\n        for i, (module_def, module) in enumerate(zip(self.module_defs, self.module_list)):\n            if module_def[\'type\'] == \'convolutional\':\n                conv_layer = module[0]\n                if module_def[\'batch_normalize\']:\n                    # Load BN bias, weights, running mean and running variance\n                    bn_layer = module[1]\n                    num_b = bn_layer.bias.numel() # Number of biases\n                    # Bias\n                    bn_b = torch.from_numpy(weights[ptr:ptr + num_b]).view_as(bn_layer.bias)\n                    bn_layer.bias.data.copy_(bn_b)\n                    ptr += num_b\n                    # Weight\n                    bn_w = torch.from_numpy(weights[ptr:ptr + num_b]).view_as(bn_layer.weight)\n                    bn_layer.weight.data.copy_(bn_w)\n                    ptr += num_b\n                    # Running Mean\n                    bn_rm = torch.from_numpy(weights[ptr:ptr + num_b]).view_as(bn_layer.running_mean)\n                    bn_layer.running_mean.data.copy_(bn_rm)\n                    ptr += num_b\n                    # Running Var\n                    bn_rv = torch.from_numpy(weights[ptr:ptr + num_b]).view_as(bn_layer.running_var)\n                    bn_layer.running_var.data.copy_(bn_rv)\n                    ptr += num_b\n                else:\n                    # Load conv. bias\n                    # \xe5\xa6\x82\xe6\x9e\x9c\xe8\xae\xbe\xe7\xbd\xae\xe7\x9a\x84\xe6\x98\xafFalse\xef\xbc\x8c\xe5\x8f\xaa\xe9\x9c\x80\xe5\x8a\xa0\xe8\xbd\xbd\xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82\xe7\x9a\x84\xe5\x81\x8f\xe7\xbd\xae\xe5\x8d\xb3\xe5\x8f\xaf\n                    num_b = conv_layer.bias.numel()\n                    conv_b = torch.from_numpy(weights[ptr:ptr + num_b]).view_as(conv_layer.bias)\n                    conv_layer.bias.data.copy_(conv_b)\n                    ptr += num_b\n                # Load conv. weights\n                # \xe6\x9c\x80\xe7\xbb\x88\xef\xbc\x8c\xe5\x8a\xa0\xe8\xbd\xbd\xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82\xe5\x8f\x82\xe6\x95\xb0\xef\xbc\x9a\n                num_w = conv_layer.weight.numel()\n                conv_w = torch.from_numpy(weights[ptr:ptr + num_w]).view_as(conv_layer.weight)\n                conv_layer.weight.data.copy_(conv_w)\n                ptr += num_w\n\n\n    def save_weights(self, path, cutoff=-1):\n        """"""\n            \xe4\xbf\x9d\xe5\xad\x98\xe6\xa8\xa1\xe5\x9e\x8b\xe6\x9d\x83\xe9\x87\x8d(\xe4\xbb\x85\xe4\xbf\x9d\xe5\xad\x98\xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82conv\xe3\x80\x81BN\xe5\xb1\x82batch_normalize\xe7\x9a\x84\xe6\x9d\x83\xe9\x87\x8d\xe5\x8f\x82\xe6\x95\xb0\xe4\xbf\xa1\xe6\x81\xaf\xef\xbc\x8c\xe5\x85\xb6\xe4\xbd\x99\xe5\x8f\x82\xe6\x95\xb0\xe5\xa6\x82shortcut\xe3\x80\x81rount\xe7\xad\x89\xe4\xb8\xba\xe5\xae\x9a\xe5\x80\xbc\xef\xbc\x8c\xe6\x97\xa0\xe9\x9c\x80\xe4\xbf\x9d\xe5\xad\x98)\n            \xe6\x9d\x83\xe9\x87\x8d\xe6\x96\x87\xe4\xbb\xb6\xe6\x98\xaf\xe5\x8c\x85\xe5\x90\xab\xe4\xbb\xa5\xe4\xb8\xb2\xe8\xa1\x8c\xe6\x96\xb9\xe5\xbc\x8f\xe5\xad\x98\xe5\x82\xa8\xe7\x9a\x84\xe6\x9d\x83\xe9\x87\x8d\xe7\x9a\x84\xe4\xba\x8c\xe8\xbf\x9b\xe5\x88\xb6\xe6\x96\x87\xe4\xbb\xb6\n            \xe5\xbd\x93BN\xe5\xb1\x82\xe5\x87\xba\xe7\x8e\xb0\xe5\x9c\xa8\xe5\x8d\xb7\xe7\xa7\xaf\xe5\x9d\x97\xe4\xb8\xad\xe6\x97\xb6\xef\xbc\x8c\xe4\xb8\x8d\xe5\xad\x98\xe5\x9c\xa8\xe5\x81\x8f\xe5\xb7\xae\xe3\x80\x82 \xe4\xbd\x86\xe6\x98\xaf\xef\xbc\x8c\xe5\xbd\x93\xe6\xb2\xa1\xe6\x9c\x89BN layer \xe6\x97\xb6\xef\xbc\x8c\xe5\x81\x8f\xe5\xb7\xae\xe2\x80\x9c\xe6\x9d\x83\xe9\x87\x8d\xe2\x80\x9d\xe5\xbf\x85\xe9\xa1\xbb\xe4\xbb\x8e\xe6\x96\x87\xe4\xbb\xb6\xe4\xb8\xad\xe8\xaf\xbb\xe5\x8f\x96\n\n            @:param path    - path of the new weights file  \xef\xbc\x88\xe4\xbf\x9d\xe5\xad\x98\xe8\xb7\xaf\xe5\xbe\x84\xef\xbc\x89\n            @:param cutoff  - save layers between 0 and cutoff (cutoff = -1 -> all are saved)\n            \xe5\xbd\x93cutoff=-1\xe6\x97\xb6\xef\xbc\x9a\xe4\xbf\x9d\xe5\xad\x98\xe5\x85\xa8\xe9\x83\xa8\xe7\xbd\x91\xe7\xbb\x9c\xe5\x8f\x82\xe6\x95\xb0\n            \xe5\xbd\x93cutoff\xe4\xb8\x8d\xe4\xb8\xba-1\xe6\x97\xb6\xef\xbc\x8c\xe4\xbf\x9d\xe5\xad\x98\xe6\x8c\x87\xe5\xae\x9a\xe7\x9a\x84\xe9\x83\xa8\xe5\x88\x86\xe7\xbd\x91\xe7\xbb\x9c\xe5\x8f\x82\xe6\x95\xb0\n            \xe4\xb8\xba\xe5\x85\xbc\xe5\xae\xb9 \xe5\xae\x98\xe6\x96\xb9yolov3.weight\xe6\x9d\x83\xe9\x87\x8d\xe8\xae\xbe\xe8\xae\xa1\xef\xbc\x8c\xe8\xaf\xa5\xe4\xbb\x93\xe5\xba\x93\xe4\xb8\x8d\xe5\x86\x8d\xe4\xbd\xbf\xe7\x94\xa8\n        """"""\n        fp = open(path, \'wb\')\n        self.header_info[3] = self.seen\n        # tofile \xe5\xb0\x86\xe6\x95\xb0\xe7\xbb\x84\xe4\xb8\xad\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe4\xbb\xa5\xe4\xba\x8c\xe8\xbf\x9b\xe5\x88\xb6\xe6\xa0\xbc\xe5\xbc\x8f\xe5\x86\x99\xe8\xbf\x9b\xe6\x96\x87\xe4\xbb\xb6\xe3\x80\x82\xe6\x96\x87\xe4\xbb\xb6\xe8\xb7\xaf\xe5\xbe\x84\xe4\xb8\xbafp\n        self.header_info.tofile(fp)\n\n        # Iterate through layers \xe9\x81\x8d\xe5\x8e\x86\xe7\xbd\x91\xe7\xbb\x9c\xe5\xb1\x82\n        for i, (module_def, module) in enumerate(zip(self.module_defs[:cutoff], self.module_list[:cutoff])):\n            if module_def[\'type\'] == \'convolutional\':\n                conv_layer = module[0]\n                # If batch norm, load bn first\n                if module_def[\'batch_normalize\']:\n                    bn_layer = module[1]\n                    bn_layer.bias.data.cpu().numpy().tofile(fp)\n                    bn_layer.weight.data.cpu().numpy().tofile(fp)\n                    bn_layer.running_mean.data.cpu().numpy().tofile(fp)\n                    bn_layer.running_var.data.cpu().numpy().tofile(fp)\n                # Load conv bias\n                else:\n                    conv_layer.bias.data.cpu().numpy().tofile(fp)\n                # Load conv weights\n                conv_layer.weight.data.cpu().numpy().tofile(fp)\n\n        fp.close()\n'"
Yolov3_pytorch/utils/__init__.py,0,b''
Yolov3_pytorch/utils/config.py,0,"b""# -*- coding:utf-8 -*-\n# power by Mr.Li\n# \xe8\xae\xbe\xe7\xbd\xae\xe9\xbb\x98\xe8\xae\xa4\xe5\x8f\x82\xe6\x95\xb0\nimport os.path\nclass DefaultConfig_train():\n    epochs=30    # \xe8\xae\xad\xe7\xbb\x83\xe8\xbd\xae\xe6\x95\xb0\n    image_folder='data/samples'   #\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe5\x9c\xb0\xe5\x9d\x80\n    batch_size=16    #batch\xe5\xa4\xa7\xe5\xb0\x8f\n    model_config_path='config/yolov3.cfg'   # \xe6\xa8\xa1\xe5\x9e\x8b\xe7\xbd\x91\xe7\xbb\x9c\xe7\xbb\x93\xe6\x9e\x84\n    data_config_path='config/coco.data'    # \xe9\x85\x8d\xe7\xbd\xae\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe7\x9a\x84\xe4\xbd\xbf\xe7\x94\xa8\xe6\x83\x85\xe5\x86\xb5\n    class_path='data/coco.names'            #coco\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe7\xb1\xbb\xe5\x88\xab\xe6\xa0\x87\xe7\xad\xbe\n    conf_thres=0.8                          # \xe7\x89\xa9\xe4\xbd\x93\xe7\xbd\xae\xe4\xbf\xa1\xe5\xba\xa6\xe9\x98\x88\xe5\x80\xbc\n    nms_thres=  0.4            # iou for nms\xe7\x9a\x84\xe9\x98\x88\xe5\x80\xbc\n    n_cpu=0                 # \xe6\x89\xb9\xe7\x94\x9f\xe6\x88\x90\xe6\x9c\x9f\xe9\x97\xb4\xe8\xa6\x81\xe4\xbd\xbf\xe7\x94\xa8\xe7\x9a\x84cpu\xe7\xba\xbf\xe7\xa8\x8b\xe6\x95\xb0\n    img_size=416    # \xe8\xbe\x93\xe5\x85\xa5\xe5\x9b\xbe\xe5\x83\x8f\xe5\xb0\xba\xe5\xaf\xb8\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f\n    use_cuda=True     # \xe6\x98\xaf\xe5\x90\xa6\xe4\xbd\xbf\xe7\x94\xa8GPU\n    visdom=True  # \xe6\x98\xaf\xe5\x90\xa6\xe4\xbd\xbf\xe7\x94\xa8visdom\xe6\x9d\xa5\xe5\x8f\xaf\xe8\xa7\x86\xe5\x8c\x96loss\n    print_freq = 8  # \xe8\xae\xad\xe7\xbb\x83\xe6\x97\xb6\xef\xbc\x8c\xe6\xaf\x8fN\xe4\xb8\xaabatch\xe6\x98\xbe\xe7\xa4\xba\n    lr_decay = 0.1  # 1e-3 -> 1e-4\n\n    checkpoint_interval=1   # \xe6\xaf\x8f\xe9\x9a\x94\xe5\x87\xa0\xe4\xb8\xaa\xe6\xa8\xa1\xe5\x9e\x8b\xe4\xbf\x9d\xe5\xad\x98\xe4\xb8\x80\xe6\xac\xa1\n    checkpoint_dir='./checkpoints'   # \xe4\xbf\x9d\xe5\xad\x98\xe7\x94\x9f\xe6\x88\x90\xe6\xa8\xa1\xe5\x9e\x8b\xe7\x9a\x84\xe8\xb7\xaf\xe5\xbe\x84\n\n    load_model_path=None   # \xe5\x8a\xa0\xe8\xbd\xbd\xe9\xa2\x84\xe8\xae\xad\xe7\xbb\x83\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\xe7\x9a\x84\xe8\xb7\xaf\xe5\xbe\x84\xef\xbc\x8c\xe4\xb8\xbaNone\xe4\xbb\xa3\xe8\xa1\xa8\xe4\xb8\x8d\xe5\x8a\xa0\xe8\xbd\xbd\n    # load_model_path=checkpoint_dir+'/latestbobo.pt'  # \xe9\xa2\x84\xe8\xae\xad\xe7\xbb\x83\xe6\x9d\x83\xe9\x87\x8d   (\xe4\xbb\x85.pt)\n\nclass DefaultConfig_test():\n    epochs=200   #number of epochs\n    batch_size=16   #size of each image batch\n    model_config_path='config/yolov3.cfg'  #'path to model config file'\n    data_config_path='config/coco.data'   #'path to data config file'\n\n    checkpoint_dir = './checkpoints'  # \xe4\xbf\x9d\xe5\xad\x98\xe7\x94\x9f\xe6\x88\x90\xe6\xa8\xa1\xe5\x9e\x8b\xe7\x9a\x84\xe8\xb7\xaf\xe5\xbe\x84\n    # load_model_path=None   # \xe5\x8a\xa0\xe8\xbd\xbd\xe9\xa2\x84\xe8\xae\xad\xe7\xbb\x83\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\xe7\x9a\x84\xe8\xb7\xaf\xe5\xbe\x84\xef\xbc\x8c\xe4\xb8\xbaNone\xe4\xbb\xa3\xe8\xa1\xa8\xe4\xb8\x8d\xe5\x8a\xa0\xe8\xbd\xbd\n    load_model_path=checkpoint_dir+'/8yolov3.pt'  # \xe9\xa2\x84\xe8\xae\xad\xe7\xbb\x83\xe6\x9d\x83\xe9\x87\x8d     (.weights\xe6\x88\x96\xe8\x80\x85.pt)\n\n    class_path='data/coco.names'   #'path to class label file'\n    iou_thres=0.5  #'iou threshold required to qualify as detected'\n    conf_thres=0.5 #'object confidence threshold'\n    nms_thres=0.45  #'iou thresshold for non-maximum suppression'\n    n_cpu=0   #'number of cpu threads to use during batch generation'\n    img_size=416  #size of each image dimension\n    use_cuda=True  #'whether to use cuda if available'\n\n\nclass DefaultConfig_detect():\n    image_folder= 'data/samples'  #path to dataset\n    config_path='config/yolov3.cfg'  #path to model config file\n\n\n    checkpoint_dir='./checkpoints'   # \xe4\xbf\x9d\xe5\xad\x98\xe7\x94\x9f\xe6\x88\x90\xe6\xa8\xa1\xe5\x9e\x8b\xe7\x9a\x84\xe8\xb7\xaf\xe5\xbe\x84\n    # load_model_path=None   # \xe5\x8a\xa0\xe8\xbd\xbd\xe9\xa2\x84\xe8\xae\xad\xe7\xbb\x83\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\xe7\x9a\x84\xe8\xb7\xaf\xe5\xbe\x84\xef\xbc\x8c\xe4\xb8\xbaNone\xe4\xbb\xa3\xe8\xa1\xa8\xe4\xb8\x8d\xe5\x8a\xa0\xe8\xbd\xbd\n    load_model_path = checkpoint_dir + '/yolov3.weights'  # \xe9\xa2\x84\xe8\xae\xad\xe7\xbb\x83\xe6\x9d\x83\xe9\x87\x8d  (.weights\xe6\x88\x96\xe8\x80\x85.pt)\n\n\n    class_path='data/coco.names'    #path to class label file\n    conf_thres=0.8    #object confidence threshold\n    nms_thres=0.4    #iou thresshold for non-maximum suppression\n    batch_size=1   #size of the batches\n    n_cpu=8   #number of cpu threads to use during batch generation\n    img_size=416   #size of each image dimension\n    use_cuda=True   #whether to use cuda if available\n\n\n\n#\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe8\xaf\xa5\xe7\xb1\xbb\xe7\x9a\x84\xe4\xb8\x80\xe4\xb8\xaa\xe5\xaf\xb9\xe8\xb1\xa1\nopt_train=DefaultConfig_train()\nopt_test=DefaultConfig_test()\nopt_detect=DefaultConfig_detect()\n"""
Yolov3_pytorch/utils/parse_config.py,0,"b'\n\ndef parse_model_config(path):\n    """"""Parses the yolo-v3 layer configuration file and returns module definitions""""""\n    \'\'\'\n    \xe8\xa7\xa3\xe6\x9e\x90yolo-v3\xe5\xb1\x82\xe9\x85\x8d\xe7\xbd\xae\xe6\x96\x87\xe4\xbb\xb6\xe5\xb9\xb6\xe8\xbf\x94\xe5\x9b\x9e\xe6\xa8\xa1\xe5\x9d\x97\xe5\xae\x9a\xe4\xb9\x89\n    \xe8\xbf\x94\xe5\x9b\x9e\xe7\xbb\x93\xe6\x9e\x9c \xe4\xb8\xba  \xe6\xaf\x8f\xe9\x83\xa8\xe5\x88\x86\xe5\x86\x99\xe4\xb8\xba\xe4\xb8\x80\xe8\xa1\x8c\n    path\xef\xbc\x9a yolov3.cfg\xe7\x9a\x84\xe8\xb7\xaf\xe5\xbe\x84\n    \'\'\'\n    file = open(path, \'r\')\n    # \xe6\x8c\x89\xe8\xa1\x8c\xe8\xaf\xbb\xe5\x8f\x96\xef\xbc\x8c\xe5\xad\x98\xe4\xb8\xbalist\n    lines = file.read().split(\'\\n\')\n    # \xe8\xbf\x87\xe6\xbb\xa4\xe6\x8e\x89 ""#""\xe5\xbc\x80\xe5\xa4\xb4\xe7\x9a\x84\xe5\x86\x85\xe5\xae\xb9\xef\xbc\x8c\xe5\x8d\xb3\xe6\xb3\xa8\xe9\x87\x8a\xe4\xbf\xa1\xe6\x81\xaf\n    lines = [x for x in lines if x and not x.startswith(\'#\')]\n    # lstrip\xe5\x8e\xbb\xe6\x8e\x89\xe5\xb7\xa6\xe8\xbe\xb9\xe7\x9a\x84(\xe5\xa4\xb4\xe9\x83\xa8)\xef\xbc\x8crstrip\xe5\x8e\xbb\xe6\x8e\x89\xe5\x8f\xb3\xe8\xbe\xb9\xe7\x9a\x84(\xe5\xb0\xbe\xe9\x83\xa8)  \xe9\xbb\x98\xe8\xae\xa4\xe5\x88\xa0\xe9\x99\xa4\xe5\xad\x97\xe7\xac\xa6\xe4\xb8\xb2\xe5\xa4\xb4\xe5\x92\x8c\xe5\xb0\xbe\xe7\x9a\x84\xe7\xa9\xba\xe7\x99\xbd\xe5\xad\x97\xe7\xac\xa6(\xe5\x8c\x85\xe6\x8b\xac\\n\xef\xbc\x8c\\r\xef\xbc\x8c\\t\xe8\xbf\x99\xe4\xba\x9b)\n    lines = [x.rstrip().lstrip() for x in lines] # \xe5\x8e\xbb\xe9\x99\xa4\xe8\xbe\xb9\xe7\xbc\x98\xe7\xa9\xba\xe7\x99\xbd\xef\xbc\x8c\xe5\x8d\xb3\xe5\x8e\xbb\xe6\x8e\x89\xe5\xb7\xa6\xe5\x8f\xb3\xe4\xb8\xa4\xe4\xbe\xa7\xe7\x9a\x84\xe7\xa9\xba\xe6\xa0\xbc\xe7\xad\x89\xe5\xad\x97\xe7\xac\xa6\n    module_defs = []\n    for line in lines:\n        # \xe6\xa3\x80\xe6\x9f\xa5\xe5\xad\x97\xe7\xac\xa6\xe4\xb8\xb2\xe6\x98\xaf\xe5\x90\xa6\xe6\x98\xaf\xe4\xbb\xa5\xe6\x8c\x87\xe5\xae\x9a\xe5\xad\x90\xe5\xad\x97\xe7\xac\xa6\xe4\xb8\xb2 [ \xe5\xbc\x80\xe5\xa4\xb4\xef\xbc\x8c\xe5\xa6\x82\xe6\x9e\x9c\xe6\x98\xaf\xe5\x88\x99\xe8\xbf\x94\xe5\x9b\x9e True\xef\xbc\x8c\xe5\x90\xa6\xe5\x88\x99\xe8\xbf\x94\xe5\x9b\x9e False\n        if line.startswith(\'[\'): # This marks the start of a new block  \xe6\xa0\x87\xe5\xbf\x97\xe7\x9d\x80\xe4\xb8\x80\xe4\xb8\xaa\xe6\x96\xb0\xe5\x8c\xba\xe5\x9d\x97\xe7\x9a\x84\xe5\xbc\x80\xe5\xa7\x8b\n            module_defs.append({})\n            module_defs[-1][\'type\'] = line[1:-1].rstrip()\n            if module_defs[-1][\'type\'] == \'convolutional\':\n                module_defs[-1][\'batch_normalize\'] = 0\n        else:\n            key, value = line.split(""="")\n            value = value.strip()\n            module_defs[-1][key.rstrip()] = value.strip()\n\n    return module_defs\n\ndef parse_data_config(path):\n    """"""Parses the dataloader configuration file""""""\n    \'\'\'\n    \xe8\xa7\xa3\xe6\x9e\x90dataloader\xe9\x85\x8d\xe7\xbd\xae\xe6\x96\x87\xe4\xbb\xb6\n    \'\'\'\n    options = dict()\n    # \xe9\xbb\x98\xe8\xae\xa4GPU\xe6\x9c\x894\xe4\xb8\xaa\n    options[\'gpus\'] = \'0,1,2,3\'\n    # \xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe5\x8a\xa0\xe8\xbd\xbd\xe5\x99\xa8\xe5\x8a\xa0\xe8\xbd\xbd\xe6\x95\xb0\xe6\x8d\xae\xe6\x97\xb6\xe4\xbd\xbf\xe7\x94\xa8\xe7\xba\xbf\xe7\xa8\x8b\xe6\x95\xb0\n    options[\'num_workers\'] = \'10\'\n    with open(path, \'r\') as fp:\n        lines = fp.readlines()\n    for line in lines:\n        line = line.strip()\n        if line == \'\' or line.startswith(\'#\'):\n            continue\n        key, value = line.split(\'=\')\n        options[key.strip()] = value.strip()\n    return options\n'"
Yolov3_pytorch/utils/utils.py,29,"b'from __future__ import division\nimport math\nimport time\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\ndef modelinfo(model):\n    #\xe6\x89\x93\xe5\x8d\xb0\xe6\xa8\xa1\xe5\x9e\x8b\xe4\xbf\xa1\xe6\x81\xaf  Plots a line-by-line description of a PyTorch model\n    nparams = sum(x.numel() for x in model.parameters())\n    ngradients = sum(x.numel() for x in model.parameters() if x.requires_grad)\n    print(\'\\n%4s %70s %9s %12s %20s %12s %12s\' % (\'\', \'name\', \'gradient\', \'parameters\', \'shape\', \'mu\', \'sigma\'))\n    for i, (name, p) in enumerate(model.named_parameters()):\n        name = name.replace(\'module_list.\', \'\')\n        print(\'%4g %70s %9s %12g %20s %12g %12g\' % (\n            i, name, p.requires_grad, p.numel(), list(p.shape), p.mean(), p.std()))\n    print(\'\\n%g layers, %g parameters, %g gradients\' % (i + 1, nparams, ngradients))\n\ndef load_classes(path):\n    """"""\n    Loads class labels at \'path\'\n    \xe5\x8a\xa0\xe8\xbd\xbd\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe7\xb1\xbb\xe5\x88\xab\xe6\xa0\x87\xe7\xad\xbe\n    """"""\n    fp = open(path, ""r"")\n    names = fp.read().split(""\\n"")[:-1]\n    return names\n\ndef weights_init_normal(m):\n    \'\'\'\n    \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe6\x9d\x83\xe9\x87\x8d\n    \'\'\'\n    classname = m.__class__.__name__\n    if classname.find(\'Conv\') != -1:\n        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find(\'BatchNorm2d\') != -1:\n        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n        torch.nn.init.constant_(m.bias.data, 0.0)\n\ndef compute_ap(recall, precision):\n    """""" Compute the average precision, given the recall and precision curves.\n    Code originally from https://github.com/rbgirshick/py-faster-rcnn.\n\n    # Arguments\n        recall:    The recall curve (list).\n        precision: The precision curve (list).\n    # Returns\n        The average precision as computed in py-faster-rcnn.\n    """"""\n    # correct AP calculation\n    # first append sentinel values at the end\n    mrec = np.concatenate(([0.], recall, [1.]))\n    mpre = np.concatenate(([0.], precision, [0.]))\n\n    # compute the precision envelope\n    for i in range(mpre.size - 1, 0, -1):\n        mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n\n    # to calculate area under PR curve, look for points\n    # where X axis (recall) changes value\n    i = np.where(mrec[1:] != mrec[:-1])[0]\n\n    # and sum (\\Delta recall) * prec\n    ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n    return ap\n\ndef bbox_iou(box1, box2, x1y1x2y2=True):\n    """"""\n    Returns the IoU of two bounding boxes\n    """"""\n    if not x1y1x2y2:\n        # Transform from center and width to exact coordinates\n        b1_x1, b1_x2 = box1[:, 0] - box1[:, 2] / 2, box1[:, 0] + box1[:, 2] / 2\n        b1_y1, b1_y2 = box1[:, 1] - box1[:, 3] / 2, box1[:, 1] + box1[:, 3] / 2\n        b2_x1, b2_x2 = box2[:, 0] - box2[:, 2] / 2, box2[:, 0] + box2[:, 2] / 2\n        b2_y1, b2_y2 = box2[:, 1] - box2[:, 3] / 2, box2[:, 1] + box2[:, 3] / 2\n    else:\n        # Get the coordinates of bounding boxes\n        b1_x1, b1_y1, b1_x2, b1_y2 = box1[:,0], box1[:,1], box1[:,2], box1[:,3]\n        b2_x1, b2_y1, b2_x2, b2_y2 = box2[:,0], box2[:,1], box2[:,2], box2[:,3]\n\n    # get the corrdinates of the intersection rectangle\n    inter_rect_x1 =  torch.max(b1_x1, b2_x1)\n    inter_rect_y1 =  torch.max(b1_y1, b2_y1)\n    inter_rect_x2 =  torch.min(b1_x2, b2_x2)\n    inter_rect_y2 =  torch.min(b1_y2, b2_y2)\n    # Intersection area\n    inter_area =    torch.clamp(inter_rect_x2 - inter_rect_x1 + 1, min=0) * \\\n                    torch.clamp(inter_rect_y2 - inter_rect_y1 + 1, min=0)\n    # Union Area\n    b1_area = (b1_x2 - b1_x1 + 1) * (b1_y2 - b1_y1 + 1)\n    b2_area = (b2_x2 - b2_x1 + 1) * (b2_y2 - b2_y1 + 1)\n\n    iou = inter_area / (b1_area + b2_area - inter_area + 1e-16)\n\n    return iou\n\n\ndef non_max_suppression(prediction, num_classes, conf_thres=0.5, nms_thres=0.4):\n    """"""\n    Removes detections with lower object confidence score than \'conf_thres\' and performs\n    Non-Maximum Suppression to further filter detections.\n    Returns detections with shape:\n        (x1, y1, x2, y2, object_conf, class_score, class_pred)\n    """"""\n\n    # From (center x, center y, width, height) to (x1, y1, x2, y2)\n    box_corner = prediction.new(prediction.shape)\n    box_corner[:, :, 0] = prediction[:, :, 0] - prediction[:, :, 2] / 2\n    box_corner[:, :, 1] = prediction[:, :, 1] - prediction[:, :, 3] / 2\n    box_corner[:, :, 2] = prediction[:, :, 0] + prediction[:, :, 2] / 2\n    box_corner[:, :, 3] = prediction[:, :, 1] + prediction[:, :, 3] / 2\n    prediction[:, :, :4] = box_corner[:, :, :4]\n\n    output = [None for _ in range(len(prediction))]\n    for image_i, image_pred in enumerate(prediction):\n        # Filter out confidence scores below threshold\n        conf_mask = (image_pred[:, 4] >= conf_thres).squeeze()\n        image_pred = image_pred[conf_mask]\n        # If none are remaining => process next image\n        if not image_pred.size(0):\n            continue\n        # Get score and class with highest confidence\n        class_conf, class_pred = torch.max(image_pred[:, 5:5 + num_classes], 1,  keepdim=True)\n        # Detections ordered as (x1, y1, x2, y2, obj_conf, class_conf, class_pred)\n        detections = torch.cat((image_pred[:, :5], class_conf.float(), class_pred.float()), 1)\n        # Iterate through all predicted classes\n        unique_labels = detections[:, -1].cpu().unique()\n        if prediction.is_cuda:\n            unique_labels = unique_labels.cuda()\n        for c in unique_labels:\n            # Get the detections with the particular class\n            detections_class = detections[detections[:, -1] == c]\n            # Sort the detections by maximum objectness confidence\n            _, conf_sort_index = torch.sort(detections_class[:, 4], descending=True)\n            detections_class = detections_class[conf_sort_index]\n            # Perform non-maximum suppression\n            max_detections = []\n            while detections_class.size(0):\n                # Get detection with highest confidence and save as max detection\n                max_detections.append(detections_class[0].unsqueeze(0))\n                # Stop if we\'re at the last detection\n                if len(detections_class) == 1:\n                    break\n                # Get the IOUs for all boxes with lower confidence\n                ious = bbox_iou(max_detections[-1], detections_class[1:])\n                # Remove detections with IoU >= NMS threshold\n                detections_class = detections_class[1:][ious < nms_thres]\n\n            max_detections = torch.cat(max_detections).data\n            # Add max detections to outputs\n            output[image_i] = max_detections if output[image_i] is None else torch.cat((output[image_i], max_detections))\n    return output\n\ndef build_targets(pred_boxes, target, anchors, num_anchors, num_classes, dim, ignore_thres, img_dim):\n    nB = target.size(0)  #batch\xe4\xb8\xaa\xe6\x95\xb0  16\n    nA = num_anchors     #\xe9\x94\x9a\xe6\xa1\x86\xe4\xb8\xaa\xe6\x95\xb0   3\n    nC = num_classes     #\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe7\xb1\xbb\xe5\x88\xab\xe6\x95\xb0  80\n    dim = dim            #feature map\xe7\x9b\xb8\xe5\xaf\xb9\xe4\xba\x8e\xe5\x8e\x9f\xe5\x9b\xbe\xe7\x9a\x84\xe7\xbc\xa9\xe6\x94\xbe\xe5\x80\x8d\xe6\x95\xb013\n\n    # \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe5\x8f\x82\xe6\x95\xb0\n    mask        = torch.zeros(nB, nA, dim, dim)     #[16,3,13,13]   \xe5\x85\xa80\n    conf_mask   = torch.ones(nB, nA, dim, dim)      #[16,3,13,13]   \xe5\x85\xa81\n    tx          = torch.zeros(nB, nA, dim, dim)     #[16,3,13,13]   \xe5\x85\xa80\n    ty          = torch.zeros(nB, nA, dim, dim)     #[16,3,13,13]   \xe5\x85\xa80\n    tw          = torch.zeros(nB, nA, dim, dim)     #[16,3,13,13]   \xe5\x85\xa80\n    th          = torch.zeros(nB, nA, dim, dim)     #[16,3,13,13]   \xe5\x85\xa80\n    tconf       = torch.zeros(nB, nA, dim, dim)     #[16,3,13,13]   \xe5\x85\xa80\n    tcls        = torch.zeros(nB, nA, dim, dim, num_classes)    #[16,3,13,13,80]  \xe5\x85\xa80\n\n    # \xe4\xb8\xba\xe4\xba\x86\xe8\xae\xa1\xe7\xae\x97\xe4\xb8\x80\xe4\xb8\xaabatch\xe4\xb8\xad\xe7\x9a\x84recall\xe5\x8f\xac\xe5\x9b\x9e\xe7\x8e\x87\n    nGT = 0  # \xe7\xbb\x9f\xe8\xae\xa1 \xe7\x9c\x9f\xe5\x80\xbc\xe6\xa1\x86\xe4\xb8\xaa\xe6\x95\xb0 GT ground truth\n    nCorrect = 0  # \xe7\xbb\x9f\xe8\xae\xa1 \xe9\xa2\x84\xe6\xb5\x8b\xe5\x87\xba\xe6\x9c\x89\xe7\x89\xa9\xe4\xbd\x93\xe7\x9a\x84\xe4\xb8\xaa\xe6\x95\xb0 \xef\xbc\x88\xe5\x8d\xb3 \xe7\x9c\x9f\xe5\x80\xbc\xe6\xa1\x86 \xe4\xb8\x8e 3\xe4\xb8\xaa\xe5\x8e\x9f\xe5\xa7\x8b\xe9\x94\x9a\xe6\xa1\x86\xe4\xb8\x8e\xe7\x9c\x9f\xe5\x80\xbc\xe6\xa1\x86iou\xe6\x9c\x80\xe5\xa4\xa7\xe7\x9a\x84\xe9\x82\xa3\xe4\xb8\xaa\xe9\x94\x9a\xe6\xa1\x86\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84\xe9\xa2\x84\xe6\xb5\x8b\xe6\xa1\x86  \xe4\xb9\x8b\xe9\x97\xb4\xe7\x9a\x84iou > 0.5 \xe4\xb8\xba\xe9\xa2\x84\xe6\xb5\x8b\xe6\xad\xa3\xe7\xa1\xae\xef\xbc\x89\n\n    # \xe9\x81\x8d\xe5\x8e\x86\xe6\xaf\x8f\xe4\xb8\x80\xe5\xbc\xa0\xe5\x9b\xbe\xe7\x89\x87\n    for b in range(nB):\n        #\xe9\x81\x8d\xe5\x8e\x86\xe4\xb8\x80\xe5\xbc\xa0\xe5\x9b\xbe\xe7\x89\x87\xe7\x9a\x84\xe6\x89\x80\xe6\x9c\x89\xe7\x89\xa9\xe4\xbd\x93\n        for t in range(target.shape[1]):\n            if target[b, t].sum() == 0:\n                # \xe5\x8d\xb3\xe4\xbb\xa3\xe8\xa1\xa8\xe9\x81\x8d\xe5\x8e\x86\xe5\xae\x8c\xe6\x89\x80\xe6\x9c\x89\xe7\x89\xa9\xe4\xbd\x93\xef\xbc\x8ccontinue\xe7\x9b\xb4\xe6\x8e\xa5\xe5\xbc\x80\xe5\xa7\x8b\xe4\xb8\x8b\xe4\xb8\x80\xe6\xac\xa1for\xe5\xbe\xaa\xe7\x8e\xaf(\xe8\xaf\x91\xe8\x80\x85\xef\xbc\x9a\xe4\xbd\xbf\xe7\x94\xa8break\xe7\x9b\xb4\xe6\x8e\xa5\xe7\xbb\x93\xe6\x9d\x9ffor\xe5\xbe\xaa\xe7\x8e\xaf\xe6\x9b\xb4\xe5\xa5\xbd)\n                continue\n            nGT += 1\n            # Convert to position relative to box\n            # target\xe7\x9c\x9f\xe5\x80\xbc\xe6\xa1\x86 \xe5\x9d\x90\xe6\xa0\x87\xe8\xa2\xab\xe5\xbd\x92\xe4\xb8\x80\xe5\x8c\x96\xe5\x90\x8e[16,50,5] \xe5\x80\xbc\xe5\x9c\xa80-1\xe4\xb9\x8b\xe9\x97\xb4\xe3\x80\x82\xe6\x95\x85\xe4\xb9\x98\xe4\xbb\xa5 dim  \xe5\xb0\x86\xe5\xb0\xba\xe5\xba\xa6\xe8\xbd\xac\xe5\x8c\x96\xe4\xb8\xba  13x13\xe5\xb0\xba\xe5\xba\xa6\xe4\xb8\x8b\xe7\x9a\x84\xe7\x9c\x9f\xe5\x80\xbc\xe6\xa1\x86\n            gx = target[b, t, 1] * dim\n            gy = target[b, t, 2] * dim\n            gw = target[b, t, 3] * dim\n            gh = target[b, t, 4] * dim\n            # Get grid box indices \xe5\x90\x91\xe4\xb8\x8b\xe5\x8f\x96\xe6\x95\xb4\xef\xbc\x8c\xe8\x8e\xb7\xe5\x8f\x96\xe7\xbd\x91\xe6\xa0\xbc\xe6\xa1\x86\xe7\xb4\xa2\xe5\xbc\x95\xef\xbc\x8c\xe5\x8d\xb3\xe5\xb7\xa6\xe4\xb8\x8a\xe8\xa7\x92\xe5\x81\x8f\xe7\xa7\xbb\xe5\x9d\x90\xe6\xa0\x87\n            gi = int(gx)\n            gj = int(gy)\n            # Get shape of gt box [1,4]\n            gt_box = torch.FloatTensor(np.array([0, 0, gw, gh])).unsqueeze(0)\n            # Get shape of anchor box [3,4]   \xe5\x89\x8d\xe4\xb8\xa4\xe5\x88\x97\xe5\x85\xa8\xe4\xb8\xba0  \xe5\x90\x8e\xe4\xb8\xa4\xe5\x88\x97\xe4\xb8\xba \xe4\xb8\x89\xe4\xb8\xaaanchor\xe7\x9a\x84w\xe3\x80\x81h\n            anchor_shapes = torch.FloatTensor(np.concatenate((np.zeros((len(anchors), 2)), np.array(anchors)), 1))\n            # Calculate iou between gt and anchor shapes\n            # \xe8\xae\xa1\xe7\xae\x97 \xe4\xb8\x80\xe4\xb8\xaa\xe7\x9c\x9f\xe5\x80\xbc\xe6\xa1\x86 \xe4\xb8\x8e  \xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x843\xe4\xb8\xaa\xe5\x8e\x9f\xe5\xa7\x8b\xe9\x94\x9a\xe6\xa1\x86  \xe4\xb9\x8b\xe9\x97\xb4\xe7\x9a\x84iou\n            anch_ious = bbox_iou(gt_box, anchor_shapes)\n            # Where the overlap is larger than threshold set mask to zero (ignore)   \xe5\xbd\x93iou\xe9\x87\x8d\xe5\x8f\xa0\xe7\x8e\x87>\xe9\x98\x88\xe5\x80\xbc\xef\xbc\x8c\xe5\x88\x99\xe7\xbd\xae\xe4\xb8\xba0\n            # conf_mask\xe5\x85\xa8\xe4\xb8\xba1 [16,3,13,13]  \xe5\xbd\x93\xe4\xb8\x80\xe4\xb8\xaa\xe7\x9c\x9f\xe5\x80\xbc\xe6\xa1\x86 \xe4\xb8\x8e  \xe4\xb8\x80\xe4\xb8\xaa\xe5\x8e\x9f\xe5\xa7\x8b\xe9\x94\x9a\xe6\xa1\x86  \xe4\xb9\x8b\xe9\x97\xb4\xe7\x9a\x84iou > \xe9\x98\x88\xe5\x80\xbc\xe6\x97\xb6\xef\xbc\x8c\xe5\x88\x99\xe7\xbd\xae\xe4\xb8\xba0\xe3\x80\x82\n            # \xe5\x8d\xb3 \xe5\xb0\x86 \xe8\xb4\x9f\xe8\xb4\xa3\xe9\xa2\x84\xe6\xb5\x8b\xe7\x89\xa9\xe4\xbd\x93\xe7\x9a\x84\xe7\xbd\x91\xe6\xa0\xbc\xe5\x8f\x8a \xe5\xae\x83\xe5\x91\xa8\xe5\x9b\xb4\xe7\x9a\x84\xe7\xbd\x91\xe6\xa0\xbc \xe9\x83\xbd\xe7\xbd\xae\xe4\xb8\xba0 \xe4\xb8\x8d\xe5\x8f\x82\xe4\xb8\x8e\xe8\xae\xad\xe7\xbb\x83\xef\xbc\x8c\xe5\x90\x8e\xe9\x9d\xa2\xe7\x9a\x84\xe4\xbb\xa3\xe7\xa0\x81\xe4\xbc\x9a \xe5\xb0\x86\xe8\xb4\x9f\xe8\xb4\xa3\xe9\xa2\x84\xe6\xb5\x8b\xe7\x89\xa9\xe4\xbd\x93\xe7\x9a\x84\xe7\xbd\x91\xe6\xa0\xbc\xe5\x86\x8d\xe7\xbd\xae\xe4\xb8\xba1\xe3\x80\x82\n            conf_mask[b, anch_ious > ignore_thres] = 0\n            # Find the best matching anchor box  \xe6\x89\xbe\xe5\x88\xb0 \xe4\xb8\x80\xe4\xb8\xaa\xe7\x9c\x9f\xe5\x80\xbc\xe6\xa1\x86 \xe4\xb8\x8e  \xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x843\xe4\xb8\xaa\xe5\x8e\x9f\xe5\xa7\x8b\xe9\x94\x9a\xe6\xa1\x86  \xe4\xb9\x8b\xe9\x97\xb4\xe7\x9a\x84iou\xe6\x9c\x80\xe5\xa4\xa7\xe7\x9a\x84  \xe4\xb8\x8b\xe6\xa0\x87\xe5\x80\xbc\n            best_n = np.argmax(anch_ious)\n            # Get ground truth box [1,4]\n            gt_box = torch.FloatTensor(np.array([gx, gy, gw, gh])).unsqueeze(0)\n            # Get the best prediction  [1,4]\n            # pred_boxes:\xe5\x9c\xa813x13\xe5\xb0\xba\xe5\xba\xa6\xe4\xb8\x8a\xe7\x9a\x84\xe9\xa2\x84\xe6\xb5\x8b\xe6\xa1\x86\n            # pred_box\xef\xbc\x9a\xe5\x8f\x96\xe5\x87\xba  3\xe4\xb8\xaa\xe5\x8e\x9f\xe5\xa7\x8b\xe9\x94\x9a\xe6\xa1\x86\xe4\xb8\x8e \xe7\x9c\x9f\xe5\x80\xbc\xe6\xa1\x86 iou\xe6\x9c\x80\xe5\xa4\xa7\xe7\x9a\x84\xe9\x82\xa3\xe4\xb8\xaa\xe9\x94\x9a\xe6\xa1\x86  \xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84\xe9\xa2\x84\xe6\xb5\x8b\xe6\xa1\x86\n            pred_box = pred_boxes[b, best_n, gj, gi].unsqueeze(0)\n            # Masks   [16,3,13,13]   \xe5\x85\xa80      \xe5\x9c\xa83\xe4\xb8\xaa\xe5\x8e\x9f\xe5\xa7\x8b\xe9\x94\x9a\xe6\xa1\x86\xe4\xb8\x8e \xe7\x9c\x9f\xe5\x80\xbc\xe6\xa1\x86 iou\xe6\x9c\x80\xe5\xa4\xa7\xe7\x9a\x84\xe9\x82\xa3\xe4\xb8\xaa\xe9\x94\x9a\xe6\xa1\x86  \xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84\xe9\xa2\x84\xe6\xb5\x8b\xe6\xa1\x86\xe4\xbd\x8d\xef\xbc\x8c\xe5\x8d\xb3 \xe8\xb4\x9f\xe8\xb4\xa3\xe9\xa2\x84\xe6\xb5\x8b\xe7\x89\xa9\xe4\xbd\x93\xe7\x9a\x84\xe7\xbd\x91\xe6\xa0\xbc\xe7\xbd\xae\xe4\xb8\xba1 \xef\xbc\x88\xe6\xad\xa4\xe6\x97\xb6\xe5\xae\x83\xe5\x91\xa8\xe5\x9b\xb4\xe7\xbd\x91\xe6\xa0\xbc\xe4\xb8\xba0\xef\xbc\x8c\xe6\x80\x9d\xe6\x83\xb3\xe7\xb1\xbb\xe4\xbc\xbcnms\xef\xbc\x89\n            mask[b, best_n, gj, gi] = 1\n            #  [16,3,13,13]   \xe5\x85\xa81 \xe7\x84\xb6\xe5\x90\x8e\xe5\xb0\x86 \xe8\xb4\x9f\xe8\xb4\xa3\xe9\xa2\x84\xe6\xb5\x8b\xe7\x89\xa9\xe4\xbd\x93\xe7\x9a\x84\xe7\xbd\x91\xe6\xa0\xbc\xe5\x8f\x8a \xe5\xae\x83\xe5\x91\xa8\xe5\x9b\xb4\xe7\x9a\x84\xe7\xbd\x91\xe6\xa0\xbc \xe9\x83\xbd\xe7\xbd\xae\xe4\xb8\xba0 \xe4\xb8\x8d\xe5\x8f\x82\xe4\xb8\x8e\xe8\xae\xad\xe7\xbb\x83 \xef\xbc\x8c\xe7\x84\xb6\xe5\x90\x8e  \xe5\xb0\x86\xe8\xb4\x9f\xe8\xb4\xa3\xe9\xa2\x84\xe6\xb5\x8b\xe7\x89\xa9\xe4\xbd\x93\xe7\x9a\x84\xe7\xbd\x91\xe6\xa0\xbc\xe5\x86\x8d\xe6\xac\xa1\xe7\xbd\xae\xe4\xb8\xba1\xe3\x80\x82\n            #  \xe5\x8d\xb3\xe6\x80\xbb\xe4\xbd\x93\xe6\x80\x9d\xe6\x83\xb3\xe4\xb8\xba\xef\xbc\x9a \xe8\xb4\x9f\xe8\xb4\xa3\xe9\xa2\x84\xe6\xb5\x8b\xe7\x89\xa9\xe4\xbd\x93\xe7\x9a\x84\xe7\xbd\x91\xe6\xa0\xbc \xe4\xbd\x8d\xe7\xbd\xae\xe7\xbd\xae\xe4\xb8\xba1\xef\xbc\x8c\xe5\xae\x83\xe5\x91\xa8\xe5\x9b\xb4\xe7\x9a\x84\xe7\xbd\x91\xe6\xa0\xbc\xe7\xbd\xae\xe4\xb8\xba0\xe3\x80\x82\xe7\xb1\xbb\xe4\xbc\xbcNMS \xe9\x9d\x9e\xe6\x9e\x81\xe5\xa4\xa7\xe5\x80\xbc\xe6\x8a\x91\xe5\x88\xb6\n            conf_mask[b, best_n, gj, gi] = 1\n            # Coordinates \xe5\x9d\x90\xe6\xa0\x87     gi= gx\xe7\x9a\x84\xe5\x90\x91\xe4\xb8\x8b\xe5\x8f\x96\xe6\x95\xb4\xe3\x80\x82  gx-gi\xe3\x80\x81gy-gj \xe4\xb8\xba \xe7\xbd\x91\xe6\xa0\xbc\xe5\x86\x85\xe7\x9a\x84 \xe7\x89\xa9\xe4\xbd\x93\xe4\xb8\xad\xe5\xbf\x83\xe7\x82\xb9\xe5\x9d\x90\xe6\xa0\x87\xef\xbc\x880-1\xe4\xb9\x8b\xe9\x97\xb4\xef\xbc\x89\n            # tx  ty\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe5\x85\xa8\xe4\xb8\xba0\xef\xbc\x8c\xe5\x9c\xa8\xe6\x9c\x89\xe7\x9c\x9f\xe5\x80\xbc\xe6\xa1\x86\xe7\x9a\x84\xe7\xbd\x91\xe6\xa0\xbc\xe4\xbd\x8d\xe7\xbd\xae\xe5\x86\x99\xe5\x85\xa5 \xe7\x9c\x9f\xe5\xae\x9e\xe7\x9a\x84\xe7\x89\xa9\xe4\xbd\x93\xe4\xb8\xad\xe5\xbf\x83\xe7\x82\xb9\xe5\x9d\x90\xe6\xa0\x87\n            tx[b, best_n, gj, gi] = gx - gi\n            ty[b, best_n, gj, gi] = gy - gj\n            # Width and height\n            #  \xe8\xae\xba\xe6\x96\x87\xe4\xb8\xad 13x13\xe5\xb0\xba\xe5\xba\xa6\xe4\xb8\x8b\xe7\x9c\x9f\xe5\x80\xbc\xe6\xa1\x86=\xe5\x8e\x9f\xe5\xa7\x8b\xe9\x94\x9a\xe6\xa1\x86 x \xe4\xbb\xa5e\xe4\xb8\xba\xe5\xba\x95\xe7\x9a\x84 \xe9\xa2\x84\xe6\xb5\x8b\xe5\x80\xbc\xe3\x80\x82\xe6\x95\x85\xe9\xa2\x84\xe6\xb5\x8b\xe5\x80\xbc= log(13x13\xe5\xb0\xba\xe5\xba\xa6\xe4\xb8\x8b\xe7\x9c\x9f\xe5\x80\xbc\xe6\xa1\x86  / \xe5\x8e\x9f\xe5\xa7\x8b\xe9\x94\x9a\xe6\xa1\x86  +  1e-16 )\n            tw[b, best_n, gj, gi] = math.log(gw/anchors[best_n][0] + 1e-16)\n            th[b, best_n, gj, gi] = math.log(gh/anchors[best_n][1] + 1e-16)\n            # One-hot encoding of label\n            tcls[b, best_n, gj, gi, int(target[b, t, 0])] = 1\n            # Calculate iou between ground truth and best matching prediction \xe8\xae\xa1\xe7\xae\x97\xe7\x9c\x9f\xe5\x80\xbc\xe6\xa1\x86 \xe4\xb8\x8e   3\xe4\xb8\xaa\xe5\x8e\x9f\xe5\xa7\x8b\xe9\x94\x9a\xe6\xa1\x86\xe4\xb8\x8e\xe7\x9c\x9f\xe5\x80\xbc\xe6\xa1\x86iou\xe6\x9c\x80\xe5\xa4\xa7\xe7\x9a\x84\xe9\x82\xa3\xe4\xb8\xaa\xe9\x94\x9a\xe6\xa1\x86\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84\xe9\xa2\x84\xe6\xb5\x8b\xe6\xa1\x86    \xe4\xb9\x8b\xe9\x97\xb4\xe7\x9a\x84iou\n            iou = bbox_iou(gt_box, pred_box, x1y1x2y2=False)\n            # [16,3,13,13]   \xe5\x85\xa80\xef\xbc\x8c\xe6\x9c\x89\xe7\x9c\x9f\xe5\x80\xbc\xe6\xa1\x86\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84\xe7\xbd\x91\xe6\xa0\xbc\xe4\xbd\x8d\xe7\xbd\xae\xe4\xb8\xba1  \xe6\xa0\x87\xe6\x98\x8e \xe7\x89\xa9\xe4\xbd\x93\xe4\xb8\xad\xe5\xbf\x83\xe7\x82\xb9\xe8\x90\xbd\xe5\x9c\xa8\xe8\xaf\xa5\xe7\xbd\x91\xe6\xa0\xbc\xe4\xb8\xad\xef\xbc\x8c\xe8\xaf\xa5\xe7\xbd\x91\xe6\xa0\xbc\xe5\x8e\xbb\xe8\xb4\x9f\xe8\xb4\xa3\xe9\xa2\x84\xe6\xb5\x8b\xe7\x89\xa9\xe4\xbd\x93\n            tconf[b, best_n, gj, gi] = 1\n\n            if iou > 0.5:\n                nCorrect += 1\n    # nGT \xe7\xbb\x9f\xe8\xae\xa1\xe4\xb8\x80\xe4\xb8\xaabatch\xe4\xb8\xad\xe7\x9a\x84\xe7\x9c\x9f\xe5\x80\xbc\xe6\xa1\x86\xe4\xb8\xaa\xe6\x95\xb0\n    # nCorrect \xe7\xbb\x9f\xe8\xae\xa1 \xe4\xb8\x80\xe4\xb8\xaabatch\xe9\xa2\x84\xe6\xb5\x8b\xe5\x87\xba\xe6\x9c\x89\xe7\x89\xa9\xe4\xbd\x93\xe7\x9a\x84\xe4\xb8\xaa\xe6\x95\xb0\n    # mask   [16,3,13,13] \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe5\x85\xa80   \xe5\x9c\xa83\xe4\xb8\xaa\xe5\x8e\x9f\xe5\xa7\x8b\xe9\x94\x9a\xe6\xa1\x86\xe4\xb8\x8e \xe7\x9c\x9f\xe5\x80\xbc\xe6\xa1\x86 iou\xe6\x9c\x80\xe5\xa4\xa7\xe7\x9a\x84\xe9\x82\xa3\xe4\xb8\xaa\xe9\x94\x9a\xe6\xa1\x86  \xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84\xe9\xa2\x84\xe6\xb5\x8b\xe6\xa1\x86\xe4\xbd\x8d\xe7\xbd\xae\xe7\xbd\xae\xe4\xb8\xba1\n    # conf_mask  [16,3,13,13]  \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe5\x85\xa81\xef\xbc\x8c\xe4\xb9\x8b\xe5\x90\x8e\xe7\x9a\x84\xe6\x93\x8d\xe4\xbd\x9c\xef\xbc\x9a\xe8\xb4\x9f\xe8\xb4\xa3\xe9\xa2\x84\xe6\xb5\x8b\xe7\x89\xa9\xe4\xbd\x93\xe7\x9a\x84\xe7\xbd\x91\xe6\xa0\xbc\xe7\xbd\xae\xe4\xb8\xba1\xef\xbc\x8c\xe5\xae\x83\xe5\x91\xa8\xe5\x9b\xb4\xe7\xbd\x91\xe6\xa0\xbc\xe7\xbd\xae\xe4\xb8\xba0\n    # tx, ty [16,3,13,13] \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe5\x85\xa8\xe4\xb8\xba0\xef\xbc\x8c\xe5\x9c\xa8\xe6\x9c\x89\xe7\x9c\x9f\xe5\x80\xbc\xe6\xa1\x86\xe7\x9a\x84\xe7\xbd\x91\xe6\xa0\xbc\xe4\xbd\x8d\xe7\xbd\xae\xe5\x86\x99\xe5\x85\xa5 \xe7\x9c\x9f\xe5\xae\x9e\xe7\x9a\x84\xe7\x89\xa9\xe4\xbd\x93\xe4\xb8\xad\xe5\xbf\x83\xe7\x82\xb9\xe5\x9d\x90\xe6\xa0\x87\n    # tw, th  [16,3,13,13] \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe5\x85\xa8\xe4\xb8\xba0\xef\xbc\x8c\xe8\xaf\xa5\xe5\x80\xbc\xe4\xb8\xba \xe7\x9c\x9f\xe5\x80\xbc\xe6\xa1\x86\xe7\x9a\x84w\xe3\x80\x81h \xe6\x8c\x89\xe7\x85\xa7\xe5\x85\xac\xe5\xbc\x8f\xe8\xbd\xac\xe5\x8c\x96\xe4\xb8\xba \xe7\xbd\x91\xe7\xbb\x9c\xe8\xbe\x93\xe5\x87\xba\xe6\x97\xb6\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84\xe7\x9c\x9f\xe5\x80\xbc\xef\xbc\x88\xe8\xaf\xa5\xe5\x80\xbc\xe5\xaf\xb9\xe5\xba\x94\xe4\xba\x8e \xe7\xbd\x91\xe7\xbb\x9c\xe8\xbe\x93\xe5\x87\xba\xe7\x9a\x84\xe7\x9c\x9f\xe5\x80\xbc\xef\xbc\x89\n    # tconf [16,3,13,13]   \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe5\x85\xa80\xef\xbc\x8c\xe6\x9c\x89\xe7\x9c\x9f\xe5\x80\xbc\xe6\xa1\x86\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84\xe7\xbd\x91\xe6\xa0\xbc\xe4\xbd\x8d\xe7\xbd\xae\xe4\xb8\xba1  \xe6\xa0\x87\xe6\x98\x8e \xe7\x89\xa9\xe4\xbd\x93\xe4\xb8\xad\xe5\xbf\x83\xe7\x82\xb9\xe8\x90\xbd\xe5\x9c\xa8\xe8\xaf\xa5\xe7\xbd\x91\xe6\xa0\xbc\xe4\xb8\xad\xef\xbc\x8c\xe8\xaf\xa5\xe7\xbd\x91\xe6\xa0\xbc\xe5\x8e\xbb\xe8\xb4\x9f\xe8\xb4\xa3\xe9\xa2\x84\xe6\xb5\x8b\xe7\x89\xa9\xe4\xbd\x93\n    # tcls    #[16,3,13,13,80]  \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe5\x85\xa80\xef\xbc\x8c\xe7\xbb\x8f\xe8\xbf\x87one-hot\xe7\xbc\x96\xe7\xa0\x81\xe5\x90\x8e  \xe5\x9c\xa8\xe7\x9c\x9f\xe5\xae\x9e\xe7\xb1\xbb\xe5\x88\xab\xe5\xa4\x84\xe5\x80\xbc\xe4\xb8\xba1\n    return nGT, nCorrect, mask, conf_mask, tx, ty, tw, th, tconf, tcls\n\ndef to_categorical(y, num_classes):\n    """""" 1-hot encodes a tensor """"""\n    return torch.from_numpy(np.eye(num_classes, dtype=\'uint8\')[y])\n'"
Yolov3_pytorch/utils/visualize.py,2,"b""#!/usr/bin/python\n# -*- coding:utf-8 -*-\n# power by Mr.Li\nimport visdom\nimport time\nimport numpy as np\nimport torch\nclass Visualizer(object):\n    '''\n    \xe5\xb0\x81\xe8\xa3\x85\xe4\xba\x86visdom\xe7\x9a\x84\xe5\x9f\xba\xe6\x9c\xac\xe6\x93\x8d\xe4\xbd\x9c\xef\xbc\x8c\xe4\xbd\x86\xe6\x98\xaf\xe4\xbd\xa0\xe4\xbb\x8d\xe7\x84\xb6\xe5\x8f\xaf\xe4\xbb\xa5\xe9\x80\x9a\xe8\xbf\x87`self.vis.function`\n    \xe8\xb0\x83\xe7\x94\xa8\xe5\x8e\x9f\xe7\x94\x9f\xe7\x9a\x84visdom\xe6\x8e\xa5\xe5\x8f\xa3\n    '''\n    def __init__(self, env='default', **kwargs):\n        self.vis = visdom.Visdom(env=env, **kwargs)\n        # \xe7\x94\xbb\xe7\x9a\x84\xe7\xac\xac\xe5\x87\xa0\xe4\xb8\xaa\xe6\x95\xb0\xef\xbc\x8c\xe7\x9b\xb8\xe5\xbd\x93\xe4\xba\x8e\xe6\xa8\xaa\xe5\xba\xa7\xe6\xa0\x87\n        # \xe4\xbf\x9d\xe5\xad\x98\xef\xbc\x88\xe2\x80\x99loss',23\xef\xbc\x89 \xe5\x8d\xb3loss\xe7\x9a\x84\xe7\xac\xac23\xe4\xb8\xaa\xe7\x82\xb9\n        self.index = {}\n        self.log_text = ''\n    def reinit(self,env='default',**kwargs):\n        '''\n        \xe4\xbf\xae\xe6\x94\xb9visdom\xe7\x9a\x84\xe9\x85\x8d\xe7\xbd\xae  \xe9\x87\x8d\xe6\x96\xb0\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\n        '''\n        self.vis = visdom.Visdom(env=env,**kwargs)\n        return self\n    def plot_many(self, d):\n        '''\n        \xe4\xb8\x80\xe6\xac\xa1plot\xe5\xa4\x9a\xe4\xb8\xaa\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x9b\xbe\xe5\xbd\xa2\n        @params d: dict (name,value) i.e. ('loss',0.11)\n        '''\n        for k, v in d.items():\n            self.plot(k, v)\n    def img_many(self, d):\n        '''\n     \xe4\xb8\x80\xe6\xac\xa1\xe7\x94\xbb\xe5\xa4\x9a\xe4\xb8\xaa\xe5\x9b\xbe\xe5\x83\x8f\n        '''\n        for k, v in d.items():\n            self.img(k, v)\n    def plot(self, name, y,**kwargs):\n        '''\n        self.plot('loss',1.00)\n        '''\n        #\xe5\xbe\x97\xe5\x88\xb0\xe4\xb8\x8b\xe6\xa0\x87\xe5\xba\x8f\xe5\x8f\xb7\n        x = self.index.get(name, 0)\n        self.vis.line(Y=np.array([y]), X=np.array([x]),\n                      win=name,#\xe7\xaa\x97\xe5\x8f\xa3\xe5\x90\x8d\n                      opts=dict(title=name),\n                      update=None if x == 0 else 'append', #\xe6\x8c\x89\xe7\x85\xa7append\xe7\x9a\x84\xe7\x94\xbb\xe5\x9b\xbe\xe5\xbd\xa2\n                      **kwargs\n                      )\n        #\xe4\xb8\x8b\xe6\xa0\x87\xe7\xb4\xaf\xe5\x8a\xa01\n        self.index[name] = x + 1\n    def img(self, name, img_,**kwargs):\n        '''\n        self.img('input_img',t.Tensor(64,64))\n        self.img('input_imgs',t.Tensor(3,64,64))\n        self.img('input_imgs',t.Tensor(100,1,64,64))\n        self.img('input_imgs',t.Tensor(100,3,64,64),nrows=10)\n\n        \xef\xbc\x81\xef\xbc\x81\xef\xbc\x81don\xe2\x80\x98t ~~self.img('input_imgs',t.Tensor(100,64,64),nrows=10)~~\xef\xbc\x81\xef\xbc\x81\xef\xbc\x81\n        '''\n        self.vis.images(img_.cpu().numpy(),\n                       win=name,\n                       opts=dict(title=name),\n                       **kwargs\n                       )\n    def log(self,info,win='log_text'):\n        '''\n        self.log({'loss':1,'lr':0.0001})\n        \xe6\x89\x93\xe5\x8d\xb0\xe6\x97\xa5\xe5\xbf\x97\n        '''\n\n        self.log_text += ('[{time}] {info} <br>'.format(\n                            time=time.strftime('%m%d_%H%M%S'),\\\n                            info=info))\n        self.vis.text(self.log_text,win)\n    def __getattr__(self, name):\n        return getattr(self.vis, name)\n\n    def create_vis_plot(self,_xlabel, _ylabel, _title, _legend):\n        viz = visdom.Visdom()\n        '''\n        \xe6\x96\xb0\xe5\xa2\x9e\xe5\x8f\xaf\xe8\xa7\x86\xe5\x8c\x96\xe5\x9b\xbe\xe5\xbd\xa2\n        '''\n        return viz.line(\n            X=torch.zeros((1,)).cpu(),\n            Y=torch.zeros((1, 3)).cpu(),\n            opts=dict(\n                xlabel=_xlabel,\n                ylabel=_ylabel,\n                title=_title,\n                legend=_legend\n            )\n        )\n\n"""
FasterRcnn_pytorch/model/utils/__init__.py,0,b''
FasterRcnn_pytorch/model/utils/bbox_tools.py,0,"b'import numpy as np\nimport numpy as xp\n\nimport six\nfrom six import __init__\n\n\ndef loc2bbox(src_bbox, loc):\n    """"""Decode bounding boxes from bounding box offsets and scales.\n\n    Given bounding box offsets and scales computed by\n    :meth:`bbox2loc`, this function decodes the representation to\n    coordinates in 2D image coordinates.\n\n    Given scales and offsets :math:`t_y, t_x, t_h, t_w` and a bounding\n    box whose center is :math:`(y, x) = p_y, p_x` and size :math:`p_h, p_w`,\n    the decoded bounding box\'s center :math:`\\\\hat{g}_y`, :math:`\\\\hat{g}_x`\n    and size :math:`\\\\hat{g}_h`, :math:`\\\\hat{g}_w` are calculated\n    by the following formulas.\n\n    * :math:`\\\\hat{g}_y = p_h t_y + p_y`\n    * :math:`\\\\hat{g}_x = p_w t_x + p_x`\n    * :math:`\\\\hat{g}_h = p_h \\\\exp(t_h)`\n    * :math:`\\\\hat{g}_w = p_w \\\\exp(t_w)`\n\n    The decoding formulas are used in works such as R-CNN [#]_.\n\n    The output is same type as the type of the inputs.\n\n    .. [#] Ross Girshick, Jeff Donahue, Trevor Darrell, Jitendra Malik. \\\n    Rich feature hierarchies for accurate object detection and semantic \\\n    segmentation. CVPR 2014.\n\n    Args:\n        src_bbox (array): A coordinates of bounding boxes.\n            Its shape is :math:`(R, 4)`. These coordinates are\n            :math:`p_{ymin}, p_{xmin}, p_{ymax}, p_{xmax}`.\n        loc (array): An array with offsets and scales.\n            The shapes of :obj:`src_bbox` and :obj:`loc` should be same.\n            This contains values :math:`t_y, t_x, t_h, t_w`.\n\n    Returns:\n        array:\n        Decoded bounding box coordinates. Its shape is :math:`(R, 4)`. \\\n        The second axis contains four values \\\n        :math:`\\\\hat{g}_{ymin}, \\\\hat{g}_{xmin},\n        \\\\hat{g}_{ymax}, \\\\hat{g}_{xmax}`.\n\n    """"""\n\n    if src_bbox.shape[0] == 0:\n        return xp.zeros((0, 4), dtype=loc.dtype)\n\n    src_bbox = src_bbox.astype(src_bbox.dtype, copy=False)\n\n    src_height = src_bbox[:, 2] - src_bbox[:, 0]\n    src_width = src_bbox[:, 3] - src_bbox[:, 1]\n    src_ctr_y = src_bbox[:, 0] + 0.5 * src_height\n    src_ctr_x = src_bbox[:, 1] + 0.5 * src_width\n\n    dy = loc[:, 0::4]\n    dx = loc[:, 1::4]\n    dh = loc[:, 2::4]\n    dw = loc[:, 3::4]\n\n    ctr_y = dy * src_height[:, xp.newaxis] + src_ctr_y[:, xp.newaxis]\n    ctr_x = dx * src_width[:, xp.newaxis] + src_ctr_x[:, xp.newaxis]\n    h = xp.exp(dh) * src_height[:, xp.newaxis]\n    w = xp.exp(dw) * src_width[:, xp.newaxis]\n\n    dst_bbox = xp.zeros(loc.shape, dtype=loc.dtype)\n    dst_bbox[:, 0::4] = ctr_y - 0.5 * h\n    dst_bbox[:, 1::4] = ctr_x - 0.5 * w\n    dst_bbox[:, 2::4] = ctr_y + 0.5 * h\n    dst_bbox[:, 3::4] = ctr_x + 0.5 * w\n\n    return dst_bbox\n\n\ndef bbox2loc(src_bbox, dst_bbox):\n    """"""Encodes the source and the destination bounding boxes to ""loc"".\n\n    Given bounding boxes, this function computes offsets and scales\n    to match the source bounding boxes to the target bounding boxes.\n    Mathematcially, given a bounding box whose center is\n    :math:`(y, x) = p_y, p_x` and\n    size :math:`p_h, p_w` and the target bounding box whose center is\n    :math:`g_y, g_x` and size :math:`g_h, g_w`, the offsets and scales\n    :math:`t_y, t_x, t_h, t_w` can be computed by the following formulas.\n\n    * :math:`t_y = \\\\frac{(g_y - p_y)} {p_h}`\n    * :math:`t_x = \\\\frac{(g_x - p_x)} {p_w}`\n    * :math:`t_h = \\\\log(\\\\frac{g_h} {p_h})`\n    * :math:`t_w = \\\\log(\\\\frac{g_w} {p_w})`\n\n    The output is same type as the type of the inputs.\n    The encoding formulas are used in works such as R-CNN [#]_.\n\n    .. [#] Ross Girshick, Jeff Donahue, Trevor Darrell, Jitendra Malik. \\\n    Rich feature hierarchies for accurate object detection and semantic \\\n    segmentation. CVPR 2014.\n\n    Args:\n        src_bbox (array): An image coordinate array whose shape is\n            :math:`(R, 4)`. :math:`R` is the number of bounding boxes.\n            These coordinates are\n            :math:`p_{ymin}, p_{xmin}, p_{ymax}, p_{xmax}`.\n        dst_bbox (array): An image coordinate array whose shape is\n            :math:`(R, 4)`.\n            These coordinates are\n            :math:`g_{ymin}, g_{xmin}, g_{ymax}, g_{xmax}`.\n\n    Returns:\n        array:\n        Bounding box offsets and scales from :obj:`src_bbox` \\\n        to :obj:`dst_bbox`. \\\n        This has shape :math:`(R, 4)`.\n        The second axis contains four values :math:`t_y, t_x, t_h, t_w`.\n\n    """"""\n\n    height = src_bbox[:, 2] - src_bbox[:, 0]\n    width = src_bbox[:, 3] - src_bbox[:, 1]\n    ctr_y = src_bbox[:, 0] + 0.5 * height\n    ctr_x = src_bbox[:, 1] + 0.5 * width\n\n    base_height = dst_bbox[:, 2] - dst_bbox[:, 0]\n    base_width = dst_bbox[:, 3] - dst_bbox[:, 1]\n    base_ctr_y = dst_bbox[:, 0] + 0.5 * base_height\n    base_ctr_x = dst_bbox[:, 1] + 0.5 * base_width\n\n    eps = xp.finfo(height.dtype).eps\n    height = xp.maximum(height, eps)\n    width = xp.maximum(width, eps)\n\n    dy = (base_ctr_y - ctr_y) / height\n    dx = (base_ctr_x - ctr_x) / width\n    dh = xp.log(base_height / height)\n    dw = xp.log(base_width / width)\n\n    loc = xp.vstack((dy, dx, dh, dw)).transpose()\n    return loc\n\n\ndef bbox_iou(bbox_a, bbox_b):\n    """"""Calculate the Intersection of Unions (IoUs) between bounding boxes.\n\n    IoU is calculated as a ratio of area of the intersection\n    and area of the union.\n\n    This function accepts both :obj:`numpy.ndarray` and :obj:`cupy.ndarray` as\n    inputs. Please note that both :obj:`bbox_a` and :obj:`bbox_b` need to be\n    same type.\n    The output is same type as the type of the inputs.\n\n    Args:\n        bbox_a (array): An array whose shape is :math:`(N, 4)`.\n            :math:`N` is the number of bounding boxes.\n            The dtype should be :obj:`numpy.float32`.\n        bbox_b (array): An array similar to :obj:`bbox_a`,\n            whose shape is :math:`(K, 4)`.\n            The dtype should be :obj:`numpy.float32`.\n\n    Returns:\n        array:\n        An array whose shape is :math:`(N, K)`. \\\n        An element at index :math:`(n, k)` contains IoUs between \\\n        :math:`n` th bounding box in :obj:`bbox_a` and :math:`k` th bounding \\\n        box in :obj:`bbox_b`.\n\n    """"""\n    if bbox_a.shape[1] != 4 or bbox_b.shape[1] != 4:\n        raise IndexError\n\n    # top left\n    tl = xp.maximum(bbox_a[:, None, :2], bbox_b[:, :2])\n    # bottom right\n    br = xp.minimum(bbox_a[:, None, 2:], bbox_b[:, 2:])\n\n    area_i = xp.prod(br - tl, axis=2) * (tl < br).all(axis=2)\n    area_a = xp.prod(bbox_a[:, 2:] - bbox_a[:, :2], axis=1)\n    area_b = xp.prod(bbox_b[:, 2:] - bbox_b[:, :2], axis=1)\n    return area_i / (area_a[:, None] + area_b - area_i)\n\n\ndef __test():\n    pass\n\n\nif __name__ == \'__main__\':\n    __test()\n\n\ndef generate_anchor_base(base_size=16, ratios=[0.5, 1, 2],\n                         anchor_scales=[8, 16, 32]):\n    """"""Generate anchor base windows by enumerating aspect ratio and scales.\n\n    Generate anchors that are scaled and modified to the given aspect ratios.\n    Area of a scaled anchor is preserved when modifying to the given aspect\n    ratio.\n\n    :obj:`R = len(ratios) * len(anchor_scales)` anchors are generated by this\n    function.\n    The :obj:`i * len(anchor_scales) + j` th anchor corresponds to an anchor\n    generated by :obj:`ratios[i]` and :obj:`anchor_scales[j]`.\n\n    For example, if the scale is :math:`8` and the ratio is :math:`0.25`,\n    the width and the height of the base window will be stretched by :math:`8`.\n    For modifying the anchor to the given aspect ratio,\n    the height is halved and the width is doubled.\n\n    Args:\n        base_size (number): The width and the height of the reference window.\n        ratios (list of floats): This is ratios of width to height of\n            the anchors.\n        anchor_scales (list of numbers): This is areas of anchors.\n            Those areas will be the product of the square of an element in\n            :obj:`anchor_scales` and the original area of the reference\n            window.\n\n    Returns:\n        ~numpy.ndarray:\n        An array of shape :math:`(R, 4)`.\n        Each element is a set of coordinates of a bounding box.\n        The second axis corresponds to\n        :math:`(y_{min}, x_{min}, y_{max}, x_{max})` of a bounding box.\n\n    """"""\n    py = base_size / 2.\n    px = base_size / 2.\n\n    anchor_base = np.zeros((len(ratios) * len(anchor_scales), 4),\n                           dtype=np.float32)\n    for i in six.moves.range(len(ratios)):\n        for j in six.moves.range(len(anchor_scales)):\n            h = base_size * anchor_scales[j] * np.sqrt(ratios[i])\n            w = base_size * anchor_scales[j] * np.sqrt(1. / ratios[i])\n\n            index = i * len(anchor_scales) + j\n            anchor_base[index, 0] = py - h / 2.\n            anchor_base[index, 1] = px - w / 2.\n            anchor_base[index, 2] = py + h / 2.\n            anchor_base[index, 3] = px + w / 2.\n    return anchor_base\n'"
FasterRcnn_pytorch/model/utils/creator_tool.py,0,"b'import numpy as np\nimport cupy as cp\n\nfrom model.utils.bbox_tools import bbox2loc, bbox_iou, loc2bbox\nfrom model.utils.nms import non_maximum_suppression\n\n\nclass ProposalTargetCreator(object):\n    """"""Assign ground truth bounding boxes to given RoIs.\n\n    The :meth:`__call__` of this class generates training targets\n    for each object proposal.\n    This is used to train Faster RCNN [#]_.\n\n    .. [#] Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun. \\\n    Faster R-CNN: Towards Real-Time Object Detection with \\\n    Region Proposal Networks. NIPS 2015.\n\n    Args:\n        n_sample (int): The number of sampled regions.\n        pos_ratio (float): Fraction of regions that is labeled as a\n            foreground.\n        pos_iou_thresh (float): IoU threshold for a RoI to be considered as a\n            foreground.\n        neg_iou_thresh_hi (float): RoI is considered to be the background\n            if IoU is in\n            [:obj:`neg_iou_thresh_hi`, :obj:`neg_iou_thresh_hi`).\n        neg_iou_thresh_lo (float): See above.\n\n    """"""\n\n    def __init__(self,\n                 n_sample=128,\n                 pos_ratio=0.25, pos_iou_thresh=0.5,\n                 neg_iou_thresh_hi=0.5, neg_iou_thresh_lo=0.0\n                 ):\n        self.n_sample = n_sample\n        self.pos_ratio = pos_ratio\n        self.pos_iou_thresh = pos_iou_thresh\n        self.neg_iou_thresh_hi = neg_iou_thresh_hi\n        self.neg_iou_thresh_lo = neg_iou_thresh_lo  # NOTE: py-faster-rcnn\xe9\xbb\x98\xe8\xae\xa4\xe7\x9a\x84\xe5\x80\xbc\xe6\x98\xaf0.1\n\n    def __call__(self, roi, bbox, label,\n                 loc_normalize_mean=(0., 0., 0., 0.),\n                 loc_normalize_std=(0.1, 0.1, 0.2, 0.2)):\n        """"""Assigns ground truth to sampled proposals.\n\n        This function samples total of :obj:`self.n_sample` RoIs\n        from the combination of :obj:`roi` and :obj:`bbox`.\n        The RoIs are assigned with the ground truth class labels as well as\n        bounding box offsets and scales to match the ground truth bounding\n        boxes. As many as :obj:`pos_ratio * self.n_sample` RoIs are\n        sampled as foregrounds.\n\n        Offsets and scales of bounding boxes are calculated using\n        :func:`model.utils.bbox_tools.bbox2loc`.\n        Also, types of input arrays and output arrays are same.\n\n        Here are notations.\n\n        * :math:`S` is the total number of sampled RoIs, which equals \\\n            :obj:`self.n_sample`.\n        * :math:`L` is number of object classes possibly including the \\\n            background.\n\n        Args:\n            roi (array): Region of Interests (RoIs) from which we sample.\n                Its shape is :math:`(R, 4)`\n            bbox (array): The coordinates of ground truth bounding boxes.\n                Its shape is :math:`(R\', 4)`.\n            label (array): Ground truth bounding box labels. Its shape\n                is :math:`(R\',)`. Its range is :math:`[0, L - 1]`, where\n                :math:`L` is the number of foreground classes.\n            loc_normalize_mean (tuple of four floats): Mean values to normalize\n                coordinates of bouding boxes.\n            loc_normalize_std (tupler of four floats): Standard deviation of\n                the coordinates of bounding boxes.\n\n        Returns:\n            (array, array, array):\n\n            * **sample_roi**: Regions of interests that are sampled. \\\n                Its shape is :math:`(S, 4)`.\n            * **gt_roi_loc**: Offsets and scales to match \\\n                the sampled RoIs to the ground truth bounding boxes. \\\n                Its shape is :math:`(S, 4)`.\n            * **gt_roi_label**: Labels assigned to sampled RoIs. Its shape is \\\n                :math:`(S,)`. Its range is :math:`[0, L]`. The label with \\\n                value 0 is the background.\n\n        """"""\n        n_bbox, _ = bbox.shape\n\n        roi = np.concatenate((roi, bbox), axis=0)\n\n        pos_roi_per_image = np.round(self.n_sample * self.pos_ratio)\n        iou = bbox_iou(roi, bbox)\n        gt_assignment = iou.argmax(axis=1)\n        max_iou = iou.max(axis=1)\n        # Offset range of classes from [0, n_fg_class - 1] to [1, n_fg_class].\n        # The label with value 0 is the background.\n        gt_roi_label = label[gt_assignment] + 1\n\n        # Select foreground RoIs as those with >= pos_iou_thresh IoU.\n        pos_index = np.where(max_iou >= self.pos_iou_thresh)[0]\n        pos_roi_per_this_image = int(min(pos_roi_per_image, pos_index.size))\n        if pos_index.size > 0:\n            pos_index = np.random.choice(\n                pos_index, size=pos_roi_per_this_image, replace=False)\n\n        # Select background RoIs as those within\n        # [neg_iou_thresh_lo, neg_iou_thresh_hi).\n        neg_index = np.where((max_iou < self.neg_iou_thresh_hi) &\n                             (max_iou >= self.neg_iou_thresh_lo))[0]\n        neg_roi_per_this_image = self.n_sample - pos_roi_per_this_image\n        neg_roi_per_this_image = int(min(neg_roi_per_this_image,\n                                         neg_index.size))\n        if neg_index.size > 0:\n            neg_index = np.random.choice(\n                neg_index, size=neg_roi_per_this_image, replace=False)\n\n        # The indices that we\'re selecting (both positive and negative).\n        keep_index = np.append(pos_index, neg_index)\n        gt_roi_label = gt_roi_label[keep_index]\n        gt_roi_label[pos_roi_per_this_image:] = 0  # negative labels --> 0\n        sample_roi = roi[keep_index]\n\n        # Compute offsets and scales to match sampled RoIs to the GTs.\n        gt_roi_loc = bbox2loc(sample_roi, bbox[gt_assignment[keep_index]])\n        gt_roi_loc = ((gt_roi_loc - np.array(loc_normalize_mean, np.float32)\n                       ) / np.array(loc_normalize_std, np.float32))\n\n        return sample_roi, gt_roi_loc, gt_roi_label\n\n\nclass AnchorTargetCreator(object):\n    """"""Assign the ground truth bounding boxes to anchors.\n\n    Assigns the ground truth bounding boxes to anchors for training Region\n    Proposal Networks introduced in Faster R-CNN [#]_.\n    \xe5\xb0\x86\xe7\x9c\x9f\xe5\xae\x9e\xe7\x9a\x84bbox\xe5\x88\x86\xe9\x85\x8d\xe7\xbb\x99\xe9\x94\x9a\xe7\x82\xb9\n    Offsets and scales to match anchors to the ground truth are\n    calculated using the encoding scheme of\n    :func:`model.utils.bbox_tools.bbox2loc`.\n\n    .. [#] Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun. \\\n    Faster R-CNN: Towards Real-Time Object Detection with \\\n    Region Proposal Networks. NIPS 2015.\n\n    Args:\n        n_sample (int): The number of regions to produce.\n        pos_iou_thresh (float): Anchors with IoU above this\n            threshold will be assigned as positive.\n        neg_iou_thresh (float): Anchors with IoU below this\n            threshold will be assigned as negative.\n        pos_ratio (float): Ratio of positive regions in the\n            sampled regions.\n\n    """"""\n\n    def __init__(self,\n                 n_sample=256,\n                 pos_iou_thresh=0.7, neg_iou_thresh=0.3,\n                 pos_ratio=0.5):\n        self.n_sample = n_sample\n        self.pos_iou_thresh = pos_iou_thresh\n        self.neg_iou_thresh = neg_iou_thresh\n        self.pos_ratio = pos_ratio\n\n    def __call__(self, bbox, anchor, img_size):\n        """"""Assign ground truth supervision to sampled subset of anchors.\n\n        Types of input arrays and output arrays are same.\n\n        Here are notations.\n\n        * :math:`S` is the number of anchors.\n        * :math:`R` is the number of bounding boxes.\n\n        Args:\n            bbox (array): Coordinates of bounding boxes. Its shape is\n                :math:`(R, 4)`.\n            anchor (array): Coordinates of anchors. Its shape is\n                :math:`(S, 4)`.\n            img_size (tuple of ints): A tuple :obj:`H, W`, which\n                is a tuple of height and width of an image.\n\n        Returns:\n            (array, array):\n\n            #NOTE: it\'s scale not only  offset\n            * **loc**: Offsets and scales to match the anchors to \\\n                the ground truth bounding boxes. Its shape is :math:`(S, 4)`.\n            * **label**: Labels of anchors with values \\\n                :obj:`(1=positive, 0=negative, -1=ignore)`. Its shape \\\n                is :math:`(S,)`.\n\n        """"""\n\n        img_H, img_W = img_size\n\n        n_anchor = len(anchor)\n        inside_index = _get_inside_index(anchor, img_H, img_W)\n        anchor = anchor[inside_index]\n        argmax_ious, label = self._create_label(\n            inside_index, anchor, bbox)\n\n        # compute bounding box regression targets\n        loc = bbox2loc(anchor, bbox[argmax_ious])\n\n        # map up to original set of anchors\n        label = _unmap(label, n_anchor, inside_index, fill=-1)\n        loc = _unmap(loc, n_anchor, inside_index, fill=0)\n\n        return loc, label\n\n    def _create_label(self, inside_index, anchor, bbox):\n        # label: 1 is positive, 0 is negative, -1 is dont care\n        label = np.empty((len(inside_index),), dtype=np.int32)\n        label.fill(-1)\n\n        argmax_ious, max_ious, gt_argmax_ious = \\\n            self._calc_ious(anchor, bbox, inside_index)\n\n        # assign negative labels first so that positive labels can clobber them\n        label[max_ious < self.neg_iou_thresh] = 0\n\n        # positive label: for each gt, anchor with highest iou\n        label[gt_argmax_ious] = 1\n\n        # positive label: above threshold IOU\n        label[max_ious >= self.pos_iou_thresh] = 1\n\n        # subsample positive labels if we have too many\n        n_pos = int(self.pos_ratio * self.n_sample)\n        pos_index = np.where(label == 1)[0]\n        if len(pos_index) > n_pos:\n            disable_index = np.random.choice(\n                pos_index, size=(len(pos_index) - n_pos), replace=False)\n            label[disable_index] = -1\n\n        # subsample negative labels if we have too many\n        n_neg = self.n_sample - np.sum(label == 1)\n        neg_index = np.where(label == 0)[0]\n        if len(neg_index) > n_neg:\n            disable_index = np.random.choice(\n                neg_index, size=(len(neg_index) - n_neg), replace=False)\n            label[disable_index] = -1\n\n        return argmax_ious, label\n\n    def _calc_ious(self, anchor, bbox, inside_index):\n        # ious between the anchors and the gt boxes\n        ious = bbox_iou(anchor, bbox)\n        argmax_ious = ious.argmax(axis=1)\n        max_ious = ious[np.arange(len(inside_index)), argmax_ious]\n        gt_argmax_ious = ious.argmax(axis=0)\n        gt_max_ious = ious[gt_argmax_ious, np.arange(ious.shape[1])]\n        gt_argmax_ious = np.where(ious == gt_max_ious)[0]\n\n        return argmax_ious, max_ious, gt_argmax_ious\n\n\ndef _unmap(data, count, index, fill=0):\n    # Unmap a subset of item (data) back to the original set of items (of\n    # size count)\n\n    if len(data.shape) == 1:\n        ret = np.empty((count,), dtype=data.dtype)\n        ret.fill(fill)\n        ret[index] = data\n    else:\n        ret = np.empty((count,) + data.shape[1:], dtype=data.dtype)\n        ret.fill(fill)\n        ret[index, :] = data\n    return ret\n\n\ndef _get_inside_index(anchor, H, W):\n    # Calc indicies of anchors which are located completely inside of the image\n    # whose size is speficied.\n    index_inside = np.where(\n        (anchor[:, 0] >= 0) &\n        (anchor[:, 1] >= 0) &\n        (anchor[:, 2] <= H) &\n        (anchor[:, 3] <= W)\n    )[0]\n    return index_inside\n\n\nclass ProposalCreator:\n    # unNOTE: I\'ll make it undifferential\n    # unTODO: make sure it\'s ok\n    # It\'s ok\n    """"""Proposal regions are generated by calling this object.\n\n    The :meth:`__call__` of this object outputs object detection proposals by\n    applying estimated bounding box offsets\n    to a set of anchors.\n\n    This class takes parameters to control number of bounding boxes to\n    pass to NMS and keep after NMS.\n    If the paramters are negative, it uses all the bounding boxes supplied\n    or keep all the bounding boxes returned by NMS.\n\n    This class is used for Region Proposal Networks introduced in\n    Faster R-CNN [#]_.\n\n    .. [#] Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun. \\\n    Faster R-CNN: Towards Real-Time Object Detection with \\\n    Region Proposal Networks. NIPS 2015.\n\n    Args:\n        nms_thresh (float): Threshold value used when calling NMS.\n        n_train_pre_nms (int): Number of top scored bounding boxes\n            to keep before passing to NMS in train mode.\n        n_train_post_nms (int): Number of top scored bounding boxes\n            to keep after passing to NMS in train mode.\n        n_test_pre_nms (int): Number of top scored bounding boxes\n            to keep before passing to NMS in test mode.\n        n_test_post_nms (int): Number of top scored bounding boxes\n            to keep after passing to NMS in test mode.\n        force_cpu_nms (bool): If this is :obj:`True`,\n            always use NMS in CPU mode. If :obj:`False`,\n            the NMS mode is selected based on the type of inputs.\n        min_size (int): A paramter to determine the threshold on\n            discarding bounding boxes based on their sizes.\n\n    """"""\n\n    def __init__(self,\n                 parent_model,\n                 nms_thresh=0.7,\n                 n_train_pre_nms=12000,\n                 n_train_post_nms=2000,\n                 n_test_pre_nms=6000,\n                 n_test_post_nms=300,\n                 min_size=16\n                 ):\n        self.parent_model = parent_model\n        self.nms_thresh = nms_thresh\n        self.n_train_pre_nms = n_train_pre_nms\n        self.n_train_post_nms = n_train_post_nms\n        self.n_test_pre_nms = n_test_pre_nms\n        self.n_test_post_nms = n_test_post_nms\n        self.min_size = min_size\n\n    def __call__(self, loc, score,\n                 anchor, img_size, scale=1.):\n        """"""input should  be ndarray\n        Propose RoIs.\n\n        Inputs :obj:`loc, score, anchor` refer to the same anchor when indexed\n        by the same index.\n\n        On notations, :math:`R` is the total number of anchors. This is equal\n        to product of the height and the width of an image and the number of\n        anchor bases per pixel.\n\n        Type of the output is same as the inputs.\n\n        Args:\n            loc (array): Predicted offsets and scaling to anchors.\n                Its shape is :math:`(R, 4)`.\n            score (array): Predicted foreground probability for anchors.\n                Its shape is :math:`(R,)`.\n            anchor (array): Coordinates of anchors. Its shape is\n                :math:`(R, 4)`.\n            img_size (tuple of ints): A tuple :obj:`height, width`,\n                which contains image size after scaling.\n            scale (float): The scaling factor used to scale an image after\n                reading it from a file.\n\n        Returns:\n            array:\n            An array of coordinates of proposal boxes.\n            Its shape is :math:`(S, 4)`. :math:`S` is less than\n            :obj:`self.n_test_post_nms` in test time and less than\n            :obj:`self.n_train_post_nms` in train time. :math:`S` depends on\n            the size of the predicted bounding boxes and the number of\n            bounding boxes discarded by NMS.\n\n        """"""\n        # NOTE: when test, remember\n        # faster_rcnn.eval()\n        # to set self.traing = False\n        if self.parent_model.training:\n            n_pre_nms = self.n_train_pre_nms\n            n_post_nms = self.n_train_post_nms\n        else:\n            n_pre_nms = self.n_test_pre_nms\n            n_post_nms = self.n_test_post_nms\n\n        # Convert anchors into proposal via bbox transformations.\n        # roi = loc2bbox(anchor, loc)\n        roi = loc2bbox(anchor, loc)\n\n        # Clip predicted boxes to image.\n        roi[:, slice(0, 4, 2)] = np.clip(\n            roi[:, slice(0, 4, 2)], 0, img_size[0])\n        roi[:, slice(1, 4, 2)] = np.clip(\n            roi[:, slice(1, 4, 2)], 0, img_size[1])\n\n        # Remove predicted boxes with either height or width < threshold.\n        min_size = self.min_size * scale\n        hs = roi[:, 2] - roi[:, 0]\n        ws = roi[:, 3] - roi[:, 1]\n        keep = np.where((hs >= min_size) & (ws >= min_size))[0]\n        roi = roi[keep, :]\n        score = score[keep]\n\n        # Sort all (proposal, score) pairs by score from highest to lowest.\n        # Take top pre_nms_topN (e.g. 6000).\n        order = score.ravel().argsort()[::-1]\n        if n_pre_nms > 0:\n            order = order[:n_pre_nms]\n        roi = roi[order, :]\n\n        # Apply nms (e.g. threshold = 0.7).\n        # Take after_nms_topN (e.g. 300).\n\n        # unNOTE: somthing is wrong here!\n        # TODO: remove cuda.to_gpu\n        keep = non_maximum_suppression(\n            cp.ascontiguousarray(cp.asarray(roi)),\n            thresh=self.nms_thresh)\n        if n_post_nms > 0:\n            keep = keep[:n_post_nms]\n        roi = roi[keep]\n        return roi\n'"
FasterRcnn_pytorch/model/utils/roi_cupy.py,0,"b'kernel_forward = \'\'\'\n    extern ""C""\n    __global__ void roi_forward(const float* const bottom_data,const float* const bottom_rois,\n                float* top_data, int* argmax_data,\n                const double spatial_scale,const int channels,const int height, \n                const int width, const int pooled_height, \n                const int pooled_width,const int NN\n    ){\n        \n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if(idx>=NN)\n        return;\n    const int pw = idx % pooled_width;\n    const int ph = (idx / pooled_width) % pooled_height;\n    const int c = (idx / pooled_width / pooled_height) % channels;\n    int num = idx / pooled_width / pooled_height / channels;\n    const int roi_batch_ind = bottom_rois[num * 5 + 0];\n    const int roi_start_w = round(bottom_rois[num * 5 + 1] * spatial_scale);\n    const int roi_start_h = round(bottom_rois[num * 5 + 2] * spatial_scale);\n    const int roi_end_w = round(bottom_rois[num * 5 + 3] * spatial_scale);\n    const int roi_end_h = round(bottom_rois[num * 5 + 4] * spatial_scale);\n    // Force malformed ROIs to be 1x1\n    const int roi_width = max(roi_end_w - roi_start_w + 1, 1);\n    const int roi_height = max(roi_end_h - roi_start_h + 1, 1);\n    const float bin_size_h = static_cast<float>(roi_height)\n                    / static_cast<float>(pooled_height);\n    const float bin_size_w = static_cast<float>(roi_width)\n                    / static_cast<float>(pooled_width);\n\n    int hstart = static_cast<int>(floor(static_cast<float>(ph)\n                                    * bin_size_h));\n        int wstart = static_cast<int>(floor(static_cast<float>(pw)\n                                    * bin_size_w));\n    int hend = static_cast<int>(ceil(static_cast<float>(ph + 1)\n                                * bin_size_h));\n        int wend = static_cast<int>(ceil(static_cast<float>(pw + 1)\n                                * bin_size_w));\n\n    // Add roi offsets and clip to input boundaries\n    hstart = min(max(hstart + roi_start_h, 0), height);\n    hend = min(max(hend + roi_start_h, 0), height);\n    wstart = min(max(wstart + roi_start_w, 0), width);\n    wend = min(max(wend + roi_start_w, 0), width);\n    bool is_empty = (hend <= hstart) || (wend <= wstart);\n\n    // Define an empty pooling region to be zero\n    float maxval = is_empty ? 0 : -1E+37;\n    // If nothing is pooled, argmax=-1 causes nothing to be backprop\'d\n    int maxidx = -1;\n    const int data_offset = (roi_batch_ind * channels + c) * height * width;\n    for (int h = hstart; h < hend; ++h) {\n        for (int w = wstart; w < wend; ++w) {\n            int bottom_index = h * width + w;\n            if (bottom_data[data_offset + bottom_index] > maxval) {\n                maxval = bottom_data[data_offset + bottom_index];\n                maxidx = bottom_index;\n            }\n        }\n    }\n    top_data[idx]=maxval;\n    argmax_data[idx]=maxidx;\n    }\n\'\'\'\nkernel_backward = \'\'\'\n    extern ""C""\n    __global__ void roi_backward(const float* const top_diff,\n         const int* const argmax_data,const float* const bottom_rois,\n         float* bottom_diff, const int num_rois,\n         const double spatial_scale, int channels,\n         int height, int width, int pooled_height,\n          int pooled_width,const int NN)\n    {\n\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    ////Importtan >= instead of >\n    if(idx>=NN)\n        return;\n    int w = idx % width;\n    int h = (idx / width) % height;\n    int c = (idx/ (width * height)) % channels;\n    int num = idx / (width * height * channels);\n\n    float gradient = 0;\n    // Accumulate gradient over all ROIs that pooled this element\n    for (int roi_n = 0; roi_n < num_rois; ++roi_n) {\n        // Skip if ROI\'s batch index doesn\'t match num\n        if (num != static_cast<int>(bottom_rois[roi_n * 5])) {\n            continue;\n        }\n\n        int roi_start_w = round(bottom_rois[roi_n * 5 + 1]\n                                * spatial_scale);\n        int roi_start_h = round(bottom_rois[roi_n * 5 + 2]\n                                * spatial_scale);\n        int roi_end_w = round(bottom_rois[roi_n * 5 + 3]\n                                * spatial_scale);\n        int roi_end_h = round(bottom_rois[roi_n * 5 + 4]\n                                * spatial_scale);\n\n        // Skip if ROI doesn\'t include (h, w)\n        const bool in_roi = (w >= roi_start_w && w <= roi_end_w &&\n                                h >= roi_start_h && h <= roi_end_h);\n        if (!in_roi) {\n            continue;\n        }\n\n        int offset = (roi_n * channels + c) * pooled_height\n                        * pooled_width;\n\n        // Compute feasible set of pooled units that could have pooled\n        // this bottom unit\n\n        // Force malformed ROIs to be 1x1\n        int roi_width = max(roi_end_w - roi_start_w + 1, 1);\n        int roi_height = max(roi_end_h - roi_start_h + 1, 1);\n\n        float bin_size_h = static_cast<float>(roi_height)\n                        / static_cast<float>(pooled_height);\n        float bin_size_w = static_cast<float>(roi_width)\n                        / static_cast<float>(pooled_width);\n\n        int phstart = floor(static_cast<float>(h - roi_start_h)\n                            / bin_size_h);\n        int phend = ceil(static_cast<float>(h - roi_start_h + 1)\n                            / bin_size_h);\n        int pwstart = floor(static_cast<float>(w - roi_start_w)\n                            / bin_size_w);\n        int pwend = ceil(static_cast<float>(w - roi_start_w + 1)\n                            / bin_size_w);\n\n        phstart = min(max(phstart, 0), pooled_height);\n        phend = min(max(phend, 0), pooled_height);\n        pwstart = min(max(pwstart, 0), pooled_width);\n        pwend = min(max(pwend, 0), pooled_width);\n        for (int ph = phstart; ph < phend; ++ph) {\n            for (int pw = pwstart; pw < pwend; ++pw) {\n                int index_ = ph * pooled_width + pw + offset;\n                if (argmax_data[index_] == (h * width + w)) {\n                    gradient += top_diff[index_];\n                }\n            }\n        }\n    }\n    bottom_diff[idx] = gradient;\n    }\n\'\'\'\n'"
SSD_pytorch/models/functions/__init__.py,0,"b""from .detection import Detect\nfrom .prior_box import PriorBox\n\n\n__all__ = ['Detect', 'PriorBox']\n"""
SSD_pytorch/models/functions/detection.py,4,"b'import torch\nfrom torch.autograd import Function\nfrom ..box_utils import decode, nms\nfrom SSD_pytorch.utils.config import opt\n\n\nclass Detect(Function):\n    """"""At test time, Detect is the final layer of SSD.  Decode location preds,\n    apply non-maximum suppression to location predictions based on conf\n    scores and threshold to a top_k number of output predictions for both\n    confidence score and locations.\n    """"""\n    def __init__(self, num_classes, bkg_label, top_k, conf_thresh, nms_thresh):\n        self.num_classes = num_classes\n        self.background_label = bkg_label\n        self.top_k = top_k\n        # Parameters used in nms.\n        self.nms_thresh = nms_thresh\n        if nms_thresh <= 0:\n            raise ValueError(\'nms_threshold must be non negative.\')\n        self.conf_thresh = conf_thresh\n        self.variance = opt.voc[\'variance\']\n\n    def forward(self, loc_data, conf_data, prior_data):\n        """"""\n        Args:\n            loc_data: (tensor) Loc preds from loc layers\n                Shape: [batch,num_priors*4]\n            conf_data: (tensor) Shape: Conf preds from conf layers\n                Shape: [batch*num_priors,num_classes]\n            prior_data: (tensor) Prior boxes and variances from priorbox layers\n                Shape: [1,num_priors,4]\n        """"""\n        num = loc_data.size(0)  # batch size\n        num_priors = prior_data.size(0)\n        output = torch.zeros(num, self.num_classes, self.top_k, 5)\n        conf_preds = conf_data.view(num, num_priors,\n                                    self.num_classes).transpose(2, 1)\n\n        # Decode predictions into bboxes.\n        for i in range(num):\n            decoded_boxes = decode(loc_data[i], prior_data, self.variance)\n            # For each class, perform nms\n            conf_scores = conf_preds[i].clone()\n\n            for cl in range(1, self.num_classes):\n                c_mask = conf_scores[cl].gt(self.conf_thresh)\n                scores = conf_scores[cl][c_mask]\n                if scores.dim() == 0:\n                    continue\n                l_mask = c_mask.unsqueeze(1).expand_as(decoded_boxes)\n                boxes = decoded_boxes[l_mask].view(-1, 4)\n                # idx of highest scoring and non-overlapping boxes per class\n                ids, count = nms(boxes, scores, self.nms_thresh, self.top_k)\n                output[i, cl, :count] = \\\n                    torch.cat((scores[ids[:count]].unsqueeze(1),\n                               boxes[ids[:count]]), 1)\n        flt = output.contiguous().view(num, -1, 5)\n        _, idx = flt[:, :, 0].sort(1, descending=True)\n        _, rank = idx.sort(1)\n        flt[(rank < self.top_k).unsqueeze(-1).expand_as(flt)].fill_(0)\n        return output\n'"
SSD_pytorch/models/functions/prior_box.py,1,"b'from __future__ import division\nfrom math import sqrt as sqrt\nfrom itertools import product as product\nimport torch\n\n\nclass PriorBox(object):\n    """"""Compute priorbox coordinates in center-offset form for each source\n    feature map.\n    \xe5\xaf\xb9\xe4\xba\x8e\xe6\xaf\x8f\xe4\xb8\xaafeature map\xef\xbc\x8c\xe7\x94\x9f\xe6\x88\x90\xe9\xa2\x84\xe6\xb5\x8b\xe6\xa1\x86\xef\xbc\x88\xe4\xb8\xad\xe5\xbf\x83\xe5\x9d\x90\xe6\xa0\x87\xe5\x8f\x8a\xe5\x81\x8f\xe7\xa7\xbb\xe9\x87\x8f\xef\xbc\x89\n    """"""\n    def __init__(self, cfg):\n        super(PriorBox, self).__init__()\n        # 300\n        self.image_size = cfg[\'min_dim\']\n        # number of priors for feature map location (either 4 or 6)\n        # \xe6\xaf\x8f\xe4\xb8\xaa\xe7\xbd\x91\xe6\xa0\xbc\xe7\x9a\x84\xe9\xa2\x84\xe6\xb5\x8b\xe6\xa1\x86\xe6\x95\xb0\xe7\x9b\xae \xef\xbc\x884 or 6\xef\xbc\x89\n        self.num_priors = len(cfg[\'aspect_ratios\'])\n        #\xe6\x96\xb9\xe5\xb7\xae\n        self.variance = cfg[\'variance\'] or [0.1]\n        # \xe5\x80\xbc\xe4\xb8\xba[38, 19, 10, 5, 3, 1]  \xe5\x8d\xb3feature map\xe7\x9a\x84\xe5\xb0\xba\xe5\xaf\xb8\xe5\xa4\xa7\xe5\xb0\x8f\n        self.feature_maps = cfg[\'feature_maps\']\n        #  s_k \xe8\xa1\xa8\xe7\xa4\xba\xe5\x85\x88\xe9\xaa\x8c\xe6\xa1\x86\xe5\xa4\xa7\xe5\xb0\x8f\xe7\x9b\xb8\xe5\xaf\xb9\xe4\xba\x8e\xe5\x9b\xbe\xe7\x89\x87\xe7\x9a\x84\xe6\xaf\x94\xe4\xbe\x8b\xef\xbc\x8c\xe8\x80\x8c s_{min} \xe5\x92\x8c s_{max} \xe8\xa1\xa8\xe7\xa4\xba\xe6\xaf\x94\xe4\xbe\x8b\xe7\x9a\x84\xe6\x9c\x80\xe5\xb0\x8f\xe5\x80\xbc\xe4\xb8\x8e\xe6\x9c\x80\xe5\xa4\xa7\xe5\x80\xbc\n        # min_sizes\xe5\x92\x8cmax_sizes\xe7\x94\xa8\xe6\x9d\xa5\xe8\xae\xa1\xe7\xae\x97s_k,s_k_prime,\xe4\xbb\xa5\xe4\xbe\xbf\xe8\xae\xa1\xe7\xae\x97 \xe9\x95\xbf\xe5\xae\xbd\xe6\xaf\x94\xe4\xb8\xba1\xe6\x97\xb6\xe7\x9a\x84\xe4\xb8\xa4\xe4\xb8\xaaw.h\n        # \xe5\x90\x84\xe4\xb8\xaa\xe7\x89\xb9\xe5\xbe\x81\xe5\x9b\xbe\xe7\x9a\x84\xe5\x85\x88\xe9\xaa\x8c\xe6\xa1\x86\xe5\xb0\xba\xe5\xba\xa6 [30, 60, 111, 162, 213, 264]\n        self.min_sizes = cfg[\'min_sizes\']\n        # [60, 111, 162, 213, 264, 315]\n        self.max_sizes = cfg[\'max_sizes\']\n        # \xe6\x84\x9f\xe5\x8f\x97\xe9\x87\x8e\xe5\xa4\xa7\xe5\xb0\x8f\xef\xbc\x8c\xe5\x8d\xb3\xe7\x9b\xb8\xe5\xaf\xb9\xe4\xba\x8e\xe5\x8e\x9f\xe5\x9b\xbe\xe7\x9a\x84\xe7\xbc\xa9\xe5\xb0\x8f\xe5\x80\x8d\xe6\x95\xb0\n        self.steps = cfg[\'steps\']\n        # \xe7\xba\xb5\xe6\xa8\xaa\xe6\xaf\x94[[2], [2, 3], [2, 3], [2, 3], [2], [2]]\n        self.aspect_ratios = cfg[\'aspect_ratios\']\n        # True\n        self.clip = cfg[\'clip\']\n        # VOC\n        self.version = cfg[\'name\']\n        for v in self.variance:\n            if v <= 0:\n                raise ValueError(\'Variances must be greater than 0\')\n\n    def forward(self):\n        # mean \xe6\x98\xaf\xe4\xbf\x9d\xe5\xad\x98\xe9\xa2\x84\xe6\xb5\x8b\xe6\xa1\x86\xe7\x9a\x84\xe5\x88\x97\xe8\xa1\xa8\n        mean = []\n        # \xe9\x81\x8d\xe5\x8e\x86\xe4\xb8\x8d\xe5\x90\x8cfeature map\xe7\x9a\x84\xe5\xb0\xba\xe5\xaf\xb8\xe5\xa4\xa7\xe5\xb0\x8f\n        for k, f in enumerate(self.feature_maps):\n            # product\xe7\x94\xa8\xe4\xba\x8e\xe6\xb1\x82\xe5\xa4\x9a\xe4\xb8\xaa\xe5\x8f\xaf\xe8\xbf\xad\xe4\xbb\xa3\xe5\xaf\xb9\xe8\xb1\xa1\xe7\x9a\x84\xe7\xac\x9b\xe5\x8d\xa1\xe5\xb0\x94\xe7\xa7\xaf\xef\xbc\x8c\xe5\xae\x83\xe8\xb7\x9f\xe5\xb5\x8c\xe5\xa5\x97\xe7\x9a\x84 for \xe5\xbe\xaa\xe7\x8e\xaf\xe7\xad\x89\xe4\xbb\xb7\n            # repeat\xe7\x94\xa8\xe4\xba\x8e\xe6\x8c\x87\xe5\xae\x9a\xe9\x87\x8d\xe5\xa4\x8d\xe7\x94\x9f\xe6\x88\x90\xe5\xba\x8f\xe5\x88\x97\xe7\x9a\x84\xe6\xac\xa1\xe6\x95\xb0\xe3\x80\x82\n            # \xe5\x8f\x82\xe8\x80\x83\xef\xbc\x9ahttp://funhacks.net/2017/02/13/itertools/\n            # \xe5\x8d\xb3\xe8\x8b\xa5f\xe4\xb8\xba2\xef\xbc\x8c\xe5\x88\x99i,j\xe5\x8f\x96\xe5\x80\xbc\xe4\xb8\xba00,01,10,11\xe3\x80\x82\xe5\x8d\xb3\xe9\x81\x8d\xe5\x8e\x86\xe6\xaf\x8f\xe4\xb8\x80\xe4\xb8\xaa\xe5\x8f\xaf\xe8\x83\xbd\n\n            # \xe5\xbd\x93k=0,f=38\xe6\x97\xb6\xef\xbc\x8crange(f)\xe7\x9a\x84\xe5\x80\xbc\xe4\xb8\xba\xef\xbc\x880,1\xef\xbc\x8c...,37\xef\xbc\x89\xe5\x88\x99product(range(f), repeat=2)\xe7\x9a\x84\xe6\x89\x80\xe6\x9c\x89\xe5\x8f\x96\xe5\x80\xbc\xe4\xb8\xba\xef\xbc\x880,0\xef\xbc\x89\xef\xbc\x880,1\xef\xbc\x89...\xe7\x9b\xb4\xe5\x88\xb0\xef\xbc\x8837,0\xef\xbc\x89,,,\xef\xbc\x8837,37\xef\xbc\x89\n            # \xe9\x81\x8d\xe5\x8e\x86\xe4\xb8\x80\xe4\xb8\xaafeature map\xe4\xb8\x8a\xe7\x9a\x84\xe6\xaf\x8f\xe4\xb8\x80\xe4\xb8\xaa\xe7\xbd\x91\xe6\xa0\xbc\n            for i, j in product(range(f), repeat=2):\n                # fk \xe6\x98\xaf\xe7\xac\xac k \xe4\xb8\xaa feature map \xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f\n                #image_size=300  steps\xe4\xb8\xba\xe6\xaf\x8f\xe5\xb1\x82feature maps\xe7\x9a\x84\xe6\x84\x9f\xe5\x8f\x97\xe9\x87\x8e\n                f_k = self.image_size / self.steps[k]\n                # \xe5\x8d\x95\xe4\xbd\x8d\xe4\xb8\xad\xe5\xbf\x83unit center x,y\n                # \xe6\xaf\x8f\xe4\xb8\x80\xe4\xb8\xaa\xe7\xbd\x91\xe6\xa0\xbc\xe7\x9a\x84\xe4\xb8\xad\xe5\xbf\x83\xef\xbc\x8c\xe8\xae\xbe\xe7\xbd\xae\xe4\xb8\xba\xef\xbc\x9a(i+0.5|fk|,j+0.5|fk|)\xef\xbc\x8c\xe5\x85\xb6\xe4\xb8\xad\xef\xbc\x8c|fk| \xe6\x98\xaf\xe7\xac\xac k \xe4\xb8\xaa feature map \xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f\xef\xbc\x8c\xe5\x90\x8c\xe6\x97\xb6\xef\xbc\x8ci,j\xe2\x88\x88[0,|fk|)\n                cx = (j + 0.5) / f_k\n                cy = (i + 0.5) / f_k\n\n\n                # \xe6\x80\xbb\xe4\xbd\x93\xe4\xb8\x8a\xef\xbc\x9a\xe5\x85\x88\xe6\xb7\xbb\xe5\x8a\xa0\xe9\x95\xbf\xe5\xae\xbd\xe6\xaf\x94\xe4\xb8\xba1\xe7\x9a\x84\xe4\xb8\xa4\xe4\xb8\xaaw\xe3\x80\x81h\xef\xbc\x88\xe6\xaf\x94\xe8\xbe\x83\xe7\x89\xb9\xe6\xae\x8a\xef\xbc\x89\xef\xbc\x8c\xe5\x86\x8d\xe9\x80\x9a\xe8\xbf\x87\xe5\xbe\xaa\xe7\x8e\xaf\xe6\xb7\xbb\xe5\x8a\xa0\xe5\x85\xb6\xe4\xbb\x96\xe9\x95\xbf\xe5\xae\xbd\xe6\xaf\x94\n                # \xe9\x95\xbf\xe5\xae\xbd\xe6\xaf\x94aspect_ratio: 1\n                # \xe7\x9c\x9f\xe5\xae\x9e\xe5\xa4\xa7\xe5\xb0\x8frel size: min_size\n                # \xe5\x85\x88\xe9\xaa\x8c\xe6\xa1\x86\xe5\xa4\xa7\xe5\xb0\x8f\xe7\x9b\xb8\xe5\xaf\xb9\xe4\xba\x8e\xe5\x9b\xbe\xe7\x89\x87\xe7\x9a\x84\xe6\xaf\x94\xe4\xbe\x8b\n                #\xe8\xae\xa1\xe7\xae\x97s_k \xe6\x98\xaf\xe4\xb8\xba\xe4\xba\x86\xe6\xb1\x82\xe8\xa7\xa3w\xe3\x80\x81h\n                s_k = self.min_sizes[k]/self.image_size\n                # \xe7\x94\xb1\xe4\xba\x8e\xe9\x95\xbf\xe5\xae\xbd\xe6\xaf\x94\xe4\xb8\xba1\xef\xbc\x8c\xe5\x88\x99w=s_k  h=s_k\n                mean += [cx, cy, s_k, s_k]\n\n                # \xe5\xaf\xb9\xe4\xba\x8e aspect ratio \xe4\xb8\xba 1 \xe6\x97\xb6\xef\xbc\x8c\xe8\xbf\x98\xe5\xa2\x9e\xe5\x8a\xa0\xe4\xba\x86\xe4\xb8\x80\xe4\xb8\xaa default box\xe9\x95\xbf\xe5\xae\xbd\xe6\xaf\x94aspect_ratio: 1\n                # rel size: sqrt(s_k * s_(k+1))\n                s_k_prime = sqrt(s_k * (self.max_sizes[k]/self.image_size))\n                # \xe7\x94\xb1\xe4\xba\x8e\xe9\x95\xbf\xe5\xae\xbd\xe6\xaf\x94\xe4\xb8\xba1\xef\xbc\x8c\xe5\x88\x99w=s_k_prime  h=s_k_prime\n                mean += [cx, cy, s_k_prime, s_k_prime]\n\n                # \xe5\x85\xb6\xe4\xbd\x99\xe7\x9a\x84\xe9\x95\xbf\xe5\xae\xbd\xe6\xaf\x94\n                for ar in self.aspect_ratios[k]:\n                    mean += [cx, cy, s_k*sqrt(ar), s_k/sqrt(ar)]\n                    mean += [cx, cy, s_k/sqrt(ar), s_k*sqrt(ar)]\n        # \xe5\xb0\x86mean\xe7\x9a\x84list\xe8\xbd\xac\xe5\x8c\x96\xe4\xb8\xbatensor\n        output = torch.Tensor(mean).view(-1, 4)\n\n        # clip:True \xe5\xb0\x86\xe8\xbe\x93\xe5\x85\xa5input\xe5\xbc\xa0\xe9\x87\x8f\xe6\xaf\x8f\xe4\xb8\xaa\xe5\x85\x83\xe7\xb4\xa0\xe7\x9a\x84\xe5\xa4\xb9\xe7\xb4\xa7\xe5\x88\xb0\xe5\x8c\xba\xe9\x97\xb4 [min,max]\xef\xbc\x8c\xe5\xb9\xb6\xe8\xbf\x94\xe5\x9b\x9e\xe7\xbb\x93\xe6\x9e\x9c\xe5\x88\xb0\xe4\xb8\x80\xe4\xb8\xaa\xe6\x96\xb0\xe5\xbc\xa0\xe9\x87\x8f\n        # \xe6\x93\x8d\xe4\xbd\x9c\xe4\xb8\xba  \xe5\xa6\x82\xe6\x9e\x9c\xe5\x85\x83\xe7\xb4\xa0>max\xef\xbc\x8c\xe5\x88\x99\xe7\xbd\xae\xe4\xb8\xbamax\xe3\x80\x82min\xe7\xb1\xbb\xe4\xbc\xbc\n        if self.clip:\n            output.clamp_(max=1, min=0)\n        return output\n'"
SSD_pytorch/models/modules/__init__.py,0,"b""from .l2norm import L2Norm\nfrom .multibox_loss import MultiBoxLoss\n\n__all__ = ['L2Norm', 'MultiBoxLoss']\n"""
SSD_pytorch/models/modules/init_weights.py,2,"b""import torch.nn as nn\nimport torch.nn.init as init\n'''\n \xe4\xbd\xbf\xe7\x94\xa8xavier\xe6\x96\xb9\xe6\xb3\x95\xe6\x9d\xa5\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96vgg\xe5\x90\x8e\xe9\x9d\xa2\xe7\x9a\x84\xe6\x96\xb0\xe5\xa2\x9e\xe5\xb1\x82\xe3\x80\x81loc\xe7\x94\xa8\xe4\xba\x8e\xe5\x9b\x9e\xe5\xbd\x92\xe5\xb1\x82\xe3\x80\x81conf\xe7\x94\xa8\xe4\xba\x8e\xe5\x88\x86\xe7\xb1\xbb\xe5\xb1\x82  \xe7\x9a\x84\xe6\x9d\x83\xe9\x87\x8d\n'''\ndef xavier(param):\n    '''\n    \xe4\xbd\xbf\xe7\x94\xa8xavier\xe7\xae\x97\xe6\xb3\x95\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe6\x96\xb0\xe5\xa2\x9e\xe5\xb1\x82\xe7\x9a\x84\xe6\x9d\x83\xe9\x87\x8d\n    '''\n    init.xavier_uniform(param)\n\n\ndef weights_init(m):\n    if isinstance(m, nn.Conv2d):\n        xavier(m.weight.data)\n        m.bias.data.zero_()"""
SSD_pytorch/models/modules/l2norm.py,6,"b'import torch\nimport torch.nn as nn\nfrom torch.autograd import Function\nfrom torch.autograd import Variable\nimport torch.nn.init as init\n\nclass L2Norm(nn.Module):\n    def __init__(self,n_channels, scale):\n        super(L2Norm,self).__init__()\n        self.n_channels = n_channels\n        self.gamma = scale or None\n        self.eps = 1e-10\n        self.weight = nn.Parameter(torch.Tensor(self.n_channels))\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        init.constant(self.weight,self.gamma)\n\n    def forward(self, x):\n        norm = x.pow(2).sum(dim=1, keepdim=True).sqrt()+self.eps\n        #x /= norm\n        x = torch.div(x,norm)\n        out = self.weight.unsqueeze(0).unsqueeze(2).unsqueeze(3).expand_as(x) * x\n        return out\n'"
SSD_pytorch/models/modules/multibox_loss.py,11,"b'# -*- coding: utf-8 -*-\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom SSD_pytorch.utils.config import opt\nfrom ..box_utils import match, log_sum_exp\n\n\nclass MultiBoxLoss(nn.Module):\n    """"""SSD Weighted Loss Function\n    SSD\xe7\x9a\x84\xe6\x8d\x9f\xe5\xa4\xb1\xe5\x87\xbd\xe6\x95\xb0\xef\xbc\x8c\xe7\xbb\xa7\xe6\x89\xbfnn.Module\xef\xbc\x8c\xe5\xae\x9a\xe4\xb9\x89\xe4\xb8\xba\xe4\xb8\x80\xe4\xb8\xaa\xe7\xbd\x91\xe7\xbb\x9c\xe6\xa8\xa1\xe5\x9e\x8b\n    Compute Targets:\n    \xe8\xae\xa1\xe7\xae\x97\xe6\xa0\x87\xe5\x87\x86\n        1) Produce Confidence Target Indices by matching  ground truth boxes\n           with (default) \'priorboxes\' that have jaccard index > threshold parameter\n           (default threshold: 0.5).\n           \xe9\x80\x9a\xe8\xbf\x87\xe5\x8c\xb9\xe9\x85\x8d \xe7\x9c\x9f\xe5\x80\xbc\xe6\xa1\x86 \xe4\xb8\x8e \xe9\xa2\x84\xe6\xb5\x8b\xe6\xa1\x86 \xe7\x9a\x84IOU\xe9\x87\x8d\xe5\x8f\xa0\xe7\x8e\x87 \xe6\x9d\xa5\xe4\xba\xa7\xe7\x94\x9f\xe5\x88\x86\xe7\xb1\xbb\xe8\xaf\xaf\xe5\xb7\xae\n           \xe9\xbb\x98\xe8\xae\xa4IOU>0.5\xe5\x8d\xb3\xe4\xb8\xba\xe6\xad\xa3\xe6\xa0\xb7\xe6\x9c\xac\n        2) Produce localization target by \'encoding\' variance into offsets of ground\n           truth boxes and their matched  \'priorboxes\'.\n           \xe9\x80\x9a\xe8\xbf\x87 \xe7\xbc\x96\xe7\xa0\x81 \xe7\x9c\x9f\xe5\x80\xbc\xe6\xa1\x86\xe4\xb8\x8e \xe5\xaf\xb9\xe5\xba\x94\xe5\x8c\xb9\xe9\x85\x8d\xe7\x9a\x84\xe9\xa2\x84\xe6\xb5\x8b\xe6\xa1\x86\xe4\xb9\x8b\xe9\x97\xb4\xe5\x81\x8f\xe7\xa7\xbb\xe7\x9a\x84\xe6\x96\xb9\xe5\xb7\xae  \xe6\x9d\xa5\xe4\xba\xa7\xe7\x94\x9f\xe5\xae\x9a\xe4\xbd\x8d\xe5\x9b\x9e\xe5\xbd\x92\xe8\xaf\xaf\xe5\xb7\xae\n        3) Hard negative mining to filter the excessive number of negative examples\n           that comes with using a large number of default bounding boxes.\n           (default negative:positive ratio 3:1)\n           \xe7\xa1\xac\xe6\x80\xa7\xe8\xb4\x9f\xe5\xbc\x80\xe9\x87\x87   \xe9\xbb\x98\xe8\xae\xa4\xe8\xb4\x9f\xe6\xa0\xb7\xe6\x9c\xac\xef\xbc\x9a\xe6\xad\xa3\xe6\xa0\xb7\xe6\x9c\xac\xe4\xb8\xba3:1 \xe5\x8f\x82\xe8\x80\x83:https://blog.csdn.net/u012285175/article/details/77866878\n\n    Objective Loss:\n        \xe6\x80\xbb\xe6\x8d\x9f\xe5\xa4\xb1\n        L(x,c,l,g) = (Lconf(x, c) + \xce\xb1Lloc(x,l,g)) / N\n        Where, Lconf is the CrossEntropy Loss and Lloc is the SmoothL1 Loss\n        weighted by \xce\xb1 which is set to 1 by cross val.\n        Lconf\xe6\x98\xaf\xe9\x80\x9a\xe8\xbf\x87\xe4\xba\xa4\xe5\x8f\x89\xe7\x86\xb5\xe8\xae\xa1\xe7\xae\x97\xe3\x80\x82Lloc\xe6\x98\xafSmoothL1\xe6\x8d\x9f\xe5\xa4\xb1\xe3\x80\x82 \xce\xb1\xe4\xb8\x80\xe8\x88\xac\xe4\xb8\xba1\n        Args:\n            c: class confidences,\xe5\x88\x86\xe7\xb1\xbb\xe7\xbd\xae\xe4\xbf\xa1\xe5\xba\xa6\n            l: predicted boxes,\xe9\xa2\x84\xe6\xb5\x8b\xe6\xa1\x86\n            g: ground truth boxes \xe7\x9c\x9f\xe5\x80\xbc\xe6\xa1\x86\n            N: number of matched default boxes \xe5\x8c\xb9\xe9\x85\x8d\xe5\x88\xb0\xe7\x9c\x9f\xe5\x80\xbc\xe6\xa1\x86\xe7\x9a\x84\xe6\xad\xa3\xe6\xa0\xb7\xe6\x9c\xac\xe9\xa2\x84\xe6\xb5\x8b\xe6\xa1\x86\xe6\x80\xbb\xe6\x95\xb0\n        See: https://arxiv.org/pdf/1512.02325.pdf for more details.\n    """"""\n\n    def __init__(self, num_classes, overlap_thresh, prior_for_matching,\n                 bkg_label, neg_mining, neg_pos, neg_overlap, encode_target,\n                 use_gpu=True):\n        super(MultiBoxLoss, self).__init__()\n        self.use_gpu = use_gpu  #true\n        self.num_classes = num_classes  #  \xe5\x88\x86\xe7\xb1\xbb\xe6\x95\xb0  20\xe7\xb1\xbb+1\xe8\x83\x8c\xe6\x99\xaf\n        self.threshold = overlap_thresh  #IOU\xe9\x98\x88\xe5\x80\xbc0.5 \xef\xbc\x88>0.5\xe5\x8d\xb3\xe4\xb8\xba\xe6\xad\xa3\xe6\xa0\xb7\xe6\x9c\xac\xef\xbc\x89\n        self.background_label = bkg_label  # 0\xef\xbc\x9a\xe8\x83\x8c\xe6\x99\xaf\xe6\xa0\x87\xe7\xad\xbe\n        self.encode_target = encode_target  #false  \xe7\xbc\x96\xe7\xa0\x81\xe6\xa0\x87\xe7\xad\xbe\n        self.use_prior_for_matching = prior_for_matching   #true  \xe4\xbd\xbf\xe7\x94\xa8\xe5\x85\x88\xe9\xaa\x8c\xe5\x8c\xb9\xe9\x85\x8d\n        self.do_neg_mining = neg_mining  #true \xe4\xbd\xbf\xe7\x94\xa8\xe7\xa1\xac\xe6\x80\xa7\xe8\xb4\x9f\xe5\xbc\x80\xe9\x87\x87\n        self.negpos_ratio = neg_pos  # 3  \xe8\xb4\x9f\xe6\xaf\x94\xe7\x8e\x87\xef\xbc\x8c\xe5\x8d\xb3\xe7\xa1\xac\xe6\x80\xa7\xe8\xb4\x9f\xe5\xbc\x80\xe9\x87\x87\xe4\xb8\xad \xe8\xb4\x9f\xe6\xa0\xb7\xe6\x9c\xac\xef\xbc\x9a\xe6\xad\xa3\xe6\xa0\xb7\xe6\x9c\xac\xe4\xb8\xba3:1\n        self.neg_overlap = neg_overlap  # 0.5\n        self.variance = opt.voc[\'variance\']   #\xe6\x96\xb9\xe5\xb7\xae\n\n    def forward(self, predictions, targets):\n        """"""Multibox Loss\n        Args:\n            predictions (tuple): A tuple containing loc preds, conf preds,\n            and prior boxes from SSD net.\n                conf shape: torch.size(batch_size,num_priors,num_classes)\n                loc shape: torch.size(batch_size,num_priors,4)\n                priors shape: torch.size(num_priors,4)\n             \xe9\xa2\x84\xe6\xb5\x8b\xe6\xa1\x86\xef\xbc\x88\xe5\x85\x83\xe7\xbb\x84\xef\xbc\x89\xef\xbc\x9a\xe5\x8c\x85\xe5\x90\xab \xe5\xae\x9a\xe4\xbd\x8d\xe9\xa2\x84\xe6\xb5\x8b\xe3\x80\x81\xe5\x88\x86\xe7\xb1\xbb\xe9\xa2\x84\xe6\xb5\x8b \xe5\x92\x8c  priors boxes\xef\xbc\x88\xe4\xb8\x8d\xe5\x90\x8cfeature map\xe7\x94\x9f\xe6\x88\x90\xe7\x9a\x84\xe9\x94\x9a\xe7\xbb\x93\xe6\x9e\x9c\xef\xbc\x89\n                   \xe5\x86\x85\xe5\xae\xb9\xe5\x8c\x85\xe5\x90\xab\xef\xbc\x9a\n                   loc\xe5\xae\x9a\xe4\xbd\x8d  (batch_size,num_priors,4)\n                   conf\xe5\x88\x86\xe7\xb1\xbb  (batch_size,num_priors,num_classes)\n                   priors boxes   (num_priors,4)\n\n                    # loc_data  \xe9\x80\x9a\xe8\xbf\x87\xe7\xbd\x91\xe7\xbb\x9c\xe8\xbe\x93\xe5\x87\xba\xe7\x9a\x84\xe5\xae\x9a\xe4\xbd\x8d\xe7\x9a\x84\xe9\xa2\x84\xe6\xb5\x8b [32,8732,4]\n                    # conf_data  \xe9\x80\x9a\xe8\xbf\x87\xe7\xbd\x91\xe7\xbb\x9c\xe8\xbe\x93\xe5\x87\xba\xe7\x9a\x84\xe5\x88\x86\xe7\xb1\xbb\xe7\x9a\x84\xe9\xa2\x84\xe6\xb5\x8b [32,8732,21]\n                    # priors \xe4\xb8\x8d\xe5\x90\x8cfeature map\xe6\xa0\xb9\xe6\x8d\xae\xe5\x85\xac\xe5\xbc\x8f\xe7\x94\x9f\xe6\x88\x90\xe7\x9a\x84\xe9\x94\x9a\xe7\xbb\x93\xe6\x9e\x9c  [8732,4]  \xe4\xb8\x80\xe5\xbc\xa0\xe5\x9b\xbe\xe7\x89\x87\xe6\x80\xbb\xe5\x85\xb1\xe4\xba\xa7\xe7\x94\x9f8732\xe4\xb8\xaa\xe6\xa1\x86\n                    #\xe4\xb9\x8b\xe6\x89\x80\xe4\xbb\xa5\xe7\xa7\xb0\xe4\xb8\xba\xe9\x94\x9a\xef\xbc\x8c\xe8\x80\x8c\xe4\xb8\x8d\xe5\x8f\xab\xe9\xa2\x84\xe6\xb5\x8b\xe6\xa1\x86\xe3\x80\x82\xe6\x98\xaf\xe5\x9b\xa0\xe4\xb8\xba\xe9\x94\x9a\xe6\x98\xaf\xe9\x80\x9a\xe8\xbf\x87\xe5\x85\xac\xe5\xbc\x8f\xe7\x94\x9f\xe6\x88\x90\xe7\x9a\x84\xef\xbc\x8c\xe8\x80\x8c\xe4\xb8\x8d\xe6\x98\xaf\xe9\x80\x9a\xe8\xbf\x87\xe7\xbd\x91\xe7\xbb\x9c\xe9\xa2\x84\xe6\xb5\x8b\xe5\x87\xba\xe6\x9d\xa5\xe7\x9a\x84\xe3\x80\x82\n\n            targets (tensor): Ground truth boxes and labels for a batch,\n                shape: [batch_size,num_objs,5] (last idx is the label).\n            \xe7\x9b\xae\xe6\xa0\x87\xe7\x9c\x9f\xe5\x80\xbc\xef\xbc\x9a\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe4\xb8\xad\xe5\xaf\xb9\xe4\xba\x8e\xe4\xb8\x80\xe4\xb8\xaabatch\xe7\x9a\x84\xe7\x9c\x9f\xe5\x80\xbc\xe6\xa1\x86\xe5\x92\x8c\xe7\xb1\xbb\xe5\x88\xab\n                      [32,num_objs,5]   32\xef\xbc\x9abatch\xe5\xa4\xa7\xe5\xb0\x8f\xef\xbc\x8c num_objs \xef\xbc\x9a\xe4\xb8\x80\xe5\xbc\xa0\xe5\x9b\xbe\xe7\x89\x87\xe7\x9a\x84\xe7\x89\xa9\xe4\xbd\x93\xe6\x95\xb0\xef\xbc\x8c5\xef\xbc\x9a\xe5\x89\x8d\xe5\x9b\x9b\xe4\xb8\xaa\xe6\x95\xb0\xe4\xb8\xba\xe5\x9d\x90\xe6\xa0\x87\xef\xbc\x8c\xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x80\xe4\xb8\xaa\xe6\x95\xb0\xe4\xb8\xba\xe7\xb1\xbb\xe5\x88\xab\n        """"""\n\n        # \xe6\x8d\x9f\xe5\xa4\xb1\xe7\x9a\x84\xe8\xae\xa1\xe7\xae\x97  \xe6\x98\xaf\xe9\x80\x9a\xe8\xbf\x87 \xe7\xbd\x91\xe7\xbb\x9c\xe8\xbe\x93\xe5\x87\xba\xe5\x80\xbc\xe4\xb8\x8e \xe4\xba\x8b\xe5\x85\x88\xe7\x94\x9f\xe6\x88\x90\xe9\x94\x9a\xe7\x9a\x84\xe7\xbb\x93\xe6\x9e\x9c \xe8\xbf\x9b\xe8\xa1\x8c\xe8\xae\xa1\xe7\xae\x97\n        loc_data, conf_data, priors = predictions\n        # batch\xe6\x95\xb0  32\n        num = loc_data.size(0)\n        # [8732,4] \xe4\xb8\x8d\xe5\x90\x8cfeature map\xe6\xa0\xb9\xe6\x8d\xae\xe5\x85\xac\xe5\xbc\x8f\xe7\x94\x9f\xe6\x88\x90\xe7\x9a\x84\xe9\x94\x9a\xe7\xbb\x93\xe6\x9e\x9c\n        priors = priors[:loc_data.size(1), :]\n        # 8732  \xe6\x80\xbb\xe5\x85\xb1\xe7\x9a\x84\xe9\xa2\x84\xe6\xb5\x8b\xe6\xa1\x86\xe6\x95\xb0\xe7\x9b\xae\n        num_priors = (priors.size(0))\n        num_classes = self.num_classes\n\n        # \xe5\x8c\xb9\xe9\x85\x8d \xe4\xb8\x8d\xe5\x90\x8cfeature map\xe7\x94\x9f\xe6\x88\x90\xe7\x9a\x84\xe9\x94\x9a\xe7\xbb\x93\xe6\x9e\x9c  \xe4\xb8\x8e  \xe7\x9c\x9f\xe5\x80\xbc\xe6\xa1\x86,\xe5\xb0\x86\xe7\xbb\x93\xe6\x9e\x9c\xe4\xbf\x9d\xe5\xad\x98\n        # \xe6\x96\xb0\xe5\xbb\xbatensor  \xe5\xae\x9a\xe4\xbd\x8d[32,8732,4]\n        loc_t = torch.Tensor(num, num_priors, 4)\n        # \xe6\x96\xb0\xe5\xbb\xbatensor  \xe5\x88\x86\xe7\xb1\xbb[32,8732]\n        conf_t = torch.LongTensor(num, num_priors)\n\n        # \xe9\x81\x8d\xe5\x8e\x86batch\xe4\xb8\xad\xe7\x9a\x84\xe6\xaf\x8f\xe4\xb8\x80\xe5\xbc\xa0\xe7\x85\xa7\xe7\x89\x87  num=32\n        # \xe4\xbd\xbf\xe7\x94\xa8 \xe7\x9c\x9f\xe5\x80\xbc\xe6\xa1\x86 \xe4\xb8\x8e \xe6\x89\x80\xe6\x9c\x89feature map\xe7\x9a\x84\xe6\xaf\x8f\xe4\xb8\xaa\xe7\xbd\x91\xe6\xa0\xbc\xe7\x94\x9f\xe6\x88\x90\xe7\x9a\x84\xe5\x85\xb18732\xe4\xb8\xaa\xe9\x94\x9a \xe8\xbf\x9b\xe8\xa1\x8c\xe5\x8c\xb9\xe9\x85\x8d\n        for idx in range(num):\n            #[2,4] \xe8\xaf\xa5\xe5\x9b\xbe\xe7\x89\x87\xe6\x9c\x89\xe4\xb8\xa4\xe4\xb8\xaa\xe7\x89\xa9\xe4\xbd\x93\xef\xbc\x8c\xe5\x80\xbc\xe4\xb8\xba\xe6\xaf\x8f\xe4\xb8\xaa\xe7\x89\xa9\xe4\xbd\x93\xe7\x9a\x844\xe4\xb8\xaa\xe5\x9d\x90\xe6\xa0\x87\n            #truths\xe4\xb8\xba\xe4\xb8\x80\xe5\xbc\xa0\xe5\x9b\xbe\xe7\x89\x87\xe4\xb8\xad \xe4\xb8\xa4\xe4\xb8\xaa\xe7\x89\xa9\xe4\xbd\x93\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84\xe7\x9c\x9f\xe5\x80\xbc\xe6\xa1\x86\xe5\x9d\x90\xe6\xa0\x87\n            truths = targets[idx][:, :-1].data\n            # 2  \xe8\xaf\xa5\xe5\x9b\xbe\xe7\x89\x87\xe6\x9c\x89\xe4\xb8\xa4\xe4\xb8\xaa\xe7\x89\xa9\xe4\xbd\x93\xef\xbc\x8c\xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x80\xe4\xb8\xaa\xe5\x80\xbc\xe4\xb8\xba\xe6\xaf\x8f\xe4\xb8\xaa\xe7\x89\xa9\xe4\xbd\x93\xe7\x9a\x84\xe7\xb1\xbb\xe5\x88\xab\n            #labels\xe4\xb8\xba\xe4\xb8\x80\xe5\xbc\xa0\xe5\x9b\xbe\xe7\x89\x87\xe4\xb8\xad \xe4\xb8\xa4\xe4\xb8\xaa\xe7\x89\xa9\xe4\xbd\x93\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84\xe7\xb1\xbb\xe5\x88\xab\xe7\x9c\x9f\xe5\x80\xbc\n            labels = targets[idx][:, -1].data\n            # [8732,4]  defaults  \xe4\xb8\x8d\xe5\x90\x8cfeature map\xe6\xa0\xb9\xe6\x8d\xae\xe5\x85\xac\xe5\xbc\x8f\xe7\x94\x9f\xe6\x88\x90\xe7\x9a\x84\xe9\x94\x9a\xe7\xbb\x93\xe6\x9e\x9c\n            defaults = priors.data\n            # \xe9\x80\x9a\xe8\xbf\x87\xe5\x8c\xb9\xe9\x85\x8d \xe7\x9c\x9f\xe5\x80\xbc\xe6\xa1\x86 \xe4\xb8\x8e \xe9\x94\x9a \xe7\x9a\x84IOU\xe9\x87\x8d\xe5\x8f\xa0\xe7\x8e\x87\xe6\x9d\xa5\xe8\xbf\x9b\xe8\xa1\x8c\xe9\x80\x89\xe6\x8b\xa9\xe3\x80\x82\xe5\xb0\x86\xe7\xbb\x93\xe6\x9e\x9c\xe5\xad\x98\xe5\x85\xa5\xe6\x96\xb0\xe5\xbb\xba\xe7\x9a\x84tensor  loc_t\xe3\x80\x81conf_t\xe4\xb8\xad\n            # threshold \xe9\x98\x88\xe5\x80\xbc\n            # loc_t[8732,4], conf_t[8732]\xe4\xbf\x9d\xe5\xad\x98\xe4\xb8\x80\xe4\xb8\xaabatch\xe4\xb8\xad\xe7\x9a\x84\xe5\x9d\x90\xe6\xa0\x87\xe5\x81\x8f\xe7\xa7\xbb\xe5\x80\xbc  \xe5\x92\x8c \xe5\x88\x86\xe7\xb1\xbb\xe8\xaf\xaf\xe5\xb7\xae\n            match(self.threshold, truths, defaults, self.variance, labels,\n                  loc_t, conf_t, idx)\n        if self.use_gpu:\n            loc_t = loc_t.cuda()\n            conf_t = conf_t.cuda()\n\n        # loc_t, conf_t\xe4\xbf\x9d\xe5\xad\x98\xe4\xb8\x80\xe4\xb8\xaabatch\xe4\xb8\xad\xe7\x9a\x84\xe5\x9d\x90\xe6\xa0\x87\xe5\x81\x8f\xe7\xa7\xbb\xe5\x80\xbc  \xe5\x92\x8c \xe5\x88\x86\xe7\xb1\xbb\xe8\xaf\xaf\xe5\xb7\xae\n        loc_t = Variable(loc_t, requires_grad=False)\n        conf_t = Variable(conf_t, requires_grad=False)\n        # pos [32,8732]  \xe7\x94\xb10,1\xe7\xbb\x84\xe6\x88\x90\xef\xbc\x8c\xe5\x8c\xb9\xe9\x85\x8d\xe5\x88\xb0\xe7\x9c\x9f\xe5\xae\x9e\xe6\xa1\x86\xe7\x9a\x84\xe4\xbd\x8d\xe7\xbd\xae\xe7\xbd\xae\xe4\xb8\xba1\n        pos = conf_t > 0\n        # [32,1] \xe6\xaf\x8f\xe4\xb8\xaa\xe6\x95\xb0\xe4\xbb\xa3\xe8\xa1\xa8 \xe4\xb8\x80\xe5\xbc\xa0\xe5\x9b\xbe\xe7\x89\x87\xe4\xb8\x8a\xe5\x8c\xb9\xe9\x85\x8d\xe5\x88\xb0\xe7\x9a\x84\xe9\x94\x9a\xe6\x95\xb0\xe7\x9b\xae\n        num_pos = pos.sum(dim=1, keepdim=True)\n\n        # Localization Loss (Smooth L1)\n        # Shape: [batch,num_priors,4]\n        # pos_idx [32,8732,4]   \xe7\x94\xb10,1\xe7\xbb\x84\xe6\x88\x90  \xe5\x8c\xb9\xe9\x85\x8d\xe5\x88\xb0\xe7\x9c\x9f\xe5\xae\x9e\xe6\xa1\x86\xe7\x9a\x84\xe4\xbd\x8d\xe7\xbd\xae\xe7\xbd\xae\xe4\xb8\xba1\n        pos_idx = pos.unsqueeze(pos.dim()).expand_as(loc_data)\n        # loc_p [461,4]   \xe4\xbb\x8e\xe7\xbd\x91\xe7\xbb\x9c\xe8\xbe\x93\xe5\x87\xba\xe7\x9a\x84\xe5\x9b\x9e\xe5\xbd\x92\xe6\xa1\x86\xe4\xb8\xad \xe6\x8b\xbf\xe5\x88\xb0 \xe5\x8c\xb9\xe9\x85\x8d\xe5\x88\xb0\xe7\x9c\x9f\xe5\x80\xbc\xe6\xa1\x86 \xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84\xe5\xae\x9a\xe4\xbd\x8d\xe5\x9d\x90\xe6\xa0\x87\n        loc_p = loc_data[pos_idx].view(-1, 4)\n        # loc_t [461,4]\n        loc_t = loc_t[pos_idx].view(-1, 4)\n        # \xe8\xae\xa1\xe7\xae\x97\xe7\xae\x97\xe5\x9b\x9e\xe5\xbd\x92\xe5\xae\x9a\xe4\xbd\x8d\xe6\x8d\x9f\xe5\xa4\xb1   loc_p \xe9\x80\x9a\xe8\xbf\x87\xe7\xbd\x91\xe7\xbb\x9c\xe8\xbe\x93\xe5\x87\xba\xe7\x9a\x84\xe9\xa2\x84\xe6\xb5\x8b\xe6\xa1\x86   loc_t \xe9\x80\x9a\xe8\xbf\x87\xe4\xb8\x8e\xe7\x9c\x9f\xe5\x80\xbc\xe6\xa1\x86\xe5\x8c\xb9\xe9\x85\x8d\xe7\x9a\x84\xe9\x94\x9a\n        loss_l = F.smooth_l1_loss(loc_p, loc_t, size_average=False)\n\n\n\n        #\xe8\xae\xba\xe6\x96\x87\xe4\xb8\xad\xe5\x85\x88\xe5\xb0\x86\xe6\xaf\x8f\xe4\xb8\x80\xe4\xb8\xaa\xe7\x89\xa9\xe4\xbd\x93\xe4\xbd\x8d\xe7\xbd\xae\xe4\xb8\x8a\xe5\xaf\xb9\xe5\xba\x94 predictions\xef\xbc\x88default boxes\xef\xbc\x89\xe6\x98\xaf negative \xe7\x9a\x84 boxes \xe8\xbf\x9b\xe8\xa1\x8c\xe6\x8e\x92\xe5\xba\x8f\xef\xbc\x8c\n        # \xe6\x8c\x89\xe7\x85\xa7 default boxes \xe7\x9a\x84 confidence \xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f\xe3\x80\x82 \xe9\x80\x89\xe6\x8b\xa9\xe6\x9c\x80\xe9\xab\x98\xe7\x9a\x84\xe5\x87\xa0\xe4\xb8\xaa\xef\xbc\x8c\xe4\xbf\x9d\xe8\xaf\x81\xe6\x9c\x80\xe5\x90\x8e negatives\xe3\x80\x81positives \xe7\x9a\x84\xe6\xaf\x94\xe4\xbe\x8b\xe5\x9c\xa8 3:1\n\n        # \xe5\xaf\xb9\xe4\xba\x8e\xe7\xa1\xac\xe6\x80\xa7\xe8\xb4\x9f\xe5\xbc\x80\xe9\x87\x87 \xe5\x9c\xa8batch\xe8\xae\xa1\xe7\xae\x97\xe8\xb4\x9f\xe6\xa0\xb7\xe6\x9c\xac\xe7\x9a\x84\xe6\x9c\x80\xe5\xa4\xa7\xe7\xbd\xae\xe4\xbf\xa1\xe5\xba\xa6\n        # conf_data:[32,9732,21]    batch_conf:[279424,21]  \xe8\xa1\x8c\xe4\xb8\xba\xe7\xbd\x91\xe7\xbb\x9c\xe9\xa2\x84\xe6\xb5\x8b\xe7\x9a\x84\xe6\x89\x80\xe6\x9c\x89\xe5\x88\x86\xe7\xb1\xbb  \xe5\x88\x97\xe4\xb8\xba\xe7\xbd\x91\xe7\xbb\x9c\xe9\xa2\x84\xe6\xb5\x8b\xe7\x9a\x84\xe7\xb1\xbb\xe5\x88\xab\xe6\x95\xb0\n        batch_conf = conf_data.view(-1, self.num_classes)\n        # batch_conf.gather \xe5\x8f\x96\xe5\x87\xbabatch_conf\xe4\xb8\xad \xe5\x8c\xb9\xe9\x85\x8d\xe7\x9a\x84\xe9\x94\x9a\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84\xe9\xa2\x84\xe6\xb5\x8b\xe5\x88\x86\xe7\xb1\xbb\xe5\x80\xbc\n        loss_c = log_sum_exp(batch_conf) - batch_conf.gather(1, conf_t.view(-1, 1))\n\n        # Hard Negative Mining\n        # \xe7\xa1\xac\xe6\x80\xa7\xe8\xb4\x9f\xe5\xbc\x80\xe9\x87\x87\n        loss_c[pos] = 0  # filter out pos boxes for now  \xe8\xbf\x87\xe6\xbb\xa4\xe6\x8e\x89\xe6\xad\xa3\xe6\xa0\xb7\xe6\x9c\xacpos box\n        loss_c = loss_c.view(num, -1)\n        # \xe6\x8c\x89\xe6\xaf\x8f\xe8\xa1\x8c\xe9\x99\x8d\xe5\xba\x8f\xe6\x8e\x92\xe5\xba\x8f  loss_idx\xe4\xb8\xba\xe6\x8e\x92\xe5\xba\x8f\xe5\x90\x8e\xe7\x9a\x84\xe5\xba\x8f\xe5\x8f\xb7\n        _, loss_idx = loss_c.sort(1, descending=True)\n        _, idx_rank = loss_idx.sort(1)\n        # \xe7\xbb\x9f\xe8\xae\xa1 \xe6\xad\xa3\xe6\xa0\xb7\xe6\x9c\xac\xe6\x95\xb0\xe7\x9b\xae\xef\xbc\x88\xe5\x8c\xb9\xe9\x85\x8d\xe5\x88\xb0\xe7\x9c\x9f\xe5\x80\xbc\xe6\xa1\x86\xe7\x9a\x84\xe9\xa2\x84\xe6\xb5\x8b\xe6\xa1\x86\xe6\x80\xbb\xe6\x95\xb0  \xe5\x8f\xaa\xe6\x9c\x89\xe5\x8c\xb9\xe9\x85\x8d\xe5\x88\xb0\xe6\x89\x8d\xe8\x83\xbd\xe8\xae\xa1\xe7\xae\x97\xe6\x8d\x9f\xe5\xa4\xb1\xef\xbc\x89\n        num_pos = pos.long().sum(1, keepdim=True)\n        # \xe7\xbb\x9f\xe8\xae\xa1 \xe8\xb4\x9f\xe6\xa0\xb7\xe6\x9c\xac\xe6\x95\xb0\xe7\x9b\xae\n        # torch.clamp(input, min, max, out=None)  \xe5\xb0\x86\xe8\xbe\x93\xe5\x85\xa5\xe5\xbc\xa0\xe9\x87\x8f input \xe6\x89\x80\xe6\x9c\x89\xe5\x85\x83\xe7\xb4\xa0\xe9\x99\x90\xe5\x88\xb6\xe5\x9c\xa8\xe5\x8c\xba\xe9\x97\xb4 [min, max] \xe4\xb8\xad\xe5\xb9\xb6\xe8\xbf\x94\xe5\x9b\x9e\xe4\xb8\x80\xe4\xb8\xaa\xe7\xbb\x93\xe6\x9e\x9c\xe5\xbc\xa0\xe9\x87\x8f.\n        num_neg = torch.clamp(self.negpos_ratio*num_pos, max=pos.size(1)-1)\n        neg = idx_rank < num_neg.expand_as(idx_rank)\n\n        # Confidence Loss Including Positive and Negative Examples\n        # \xe5\x88\x86\xe7\xb1\xbb\xe6\x8d\x9f\xe5\xa4\xb1 \xe5\x8c\x85\xe5\x90\xab\xe6\xad\xa3\xe6\xa0\xb7\xe6\x9c\xac\xe5\x92\x8c\xe8\xb4\x9f\xe6\xa0\xb7\xe6\x9c\xac   conf_data\xef\xbc\x9a\xe9\x80\x9a\xe8\xbf\x87\xe7\xbd\x91\xe7\xbb\x9c\xe8\xbe\x93\xe5\x87\xba\xe7\x9a\x84\xe5\x88\x86\xe7\xb1\xbb\xe7\x9a\x84\xe9\xa2\x84\xe6\xb5\x8b [32,8732,21]\n        pos_idx = pos.unsqueeze(2).expand_as(conf_data)\n        neg_idx = neg.unsqueeze(2).expand_as(conf_data)\n        conf_p = conf_data[(pos_idx+neg_idx).gt(0)].view(-1, self.num_classes)\n        # conf_t \xe4\xbf\x9d\xe5\xad\x98\xe5\x88\x86\xe7\xb1\xbb\xe8\xaf\xaf\xe5\xb7\xae\n        targets_weighted = conf_t[(pos+neg).gt(0)]\n        # \xe5\x88\x86\xe7\xb1\xbb\xe6\x8d\x9f\xe5\xa4\xb1\xe4\xbd\xbf\xe7\x94\xa8\xe4\xba\xa4\xe5\x8f\x89\xe7\x86\xb5    size_average \xef\xbc\x9a\xe5\xa6\x82\xe6\x9e\x9c\xe4\xb8\xbaTRUE\xef\xbc\x8closs\xe5\x88\x99\xe6\x98\xaf\xe5\xb9\xb3\xe5\x9d\x87\xe5\x80\xbc\xef\xbc\x8c\xe9\x9c\x80\xe8\xa6\x81\xe9\x99\xa4\xe4\xbb\xa5\xe8\xbe\x93\xe5\x85\xa5 tensor \xe4\xb8\xad element \xe7\x9a\x84\xe6\x95\xb0\xe7\x9b\xae\n        loss_c = F.cross_entropy(conf_p, targets_weighted, size_average=False)\n\n        # Sum of losses: L(x,c,l,g) = (Lconf(x, c) + \xce\xb1Lloc(x,l,g)) / N\n        # \xe4\xb8\x8e\xe7\x9c\x9f\xe5\x80\xbc\xe6\xa1\x86\xe5\x8c\xb9\xe9\x85\x8d\xe5\x88\xb0\xe7\x9a\x84\xe9\xa2\x84\xe6\xb5\x8b\xe6\xa1\x86\xe6\x80\xbb\xe6\x95\xb0\n        N = num_pos.data.sum()\n        loss_l /= N\n        loss_c /= N\n        # \xe5\xae\x9a\xe4\xbd\x8d\xe6\x8d\x9f\xe5\xa4\xb1\xe3\x80\x81\xe5\x88\x86\xe7\xb1\xbb\xe6\x8d\x9f\xe5\xa4\xb1\n        return loss_l, loss_c\n'"
Yolov1_pytorch/utils/testImgs/__init__.py,0,b''
FasterRcnn_pytorch/model/utils/nms/__init__.py,0,b'from model.utils.nms.non_maximum_suppression import non_maximum_suppression'
FasterRcnn_pytorch/model/utils/nms/_nms_gpu_post_py.py,0,"b'\nimport numpy as np\n\ndef _nms_gpu_post( mask,\n                  n_bbox,\n                   threads_per_block,\n                   col_blocks\n                  ):\n    n_selection = 0\n    one_ull = np.array([1],dtype=np.uint64)\n    selection = np.zeros((n_bbox,), dtype=np.int32)\n    remv = np.zeros((col_blocks,), dtype=np.uint64)\n\n    for i in range(n_bbox):\n        nblock = i // threads_per_block\n        inblock = i % threads_per_block\n\n        if not (remv[nblock] & one_ull << inblock):\n            selection[n_selection] = i\n            n_selection += 1\n\n            index = i * col_blocks\n            for j in range(nblock, col_blocks):\n                remv[j] |= mask[index + j]\n    return selection, n_selection\n'"
FasterRcnn_pytorch/model/utils/nms/build.py,0,"b'from distutils.core import setup\nfrom distutils.extension import Extension\nfrom Cython.Distutils import build_ext\n\next_modules = [Extension(""_nms_gpu_post"", [""_nms_gpu_post.pyx""])]\nsetup(\n    name=""Hello pyx"",\n    cmdclass={\'build_ext\': build_ext},\n    ext_modules=ext_modules\n)\n'"
FasterRcnn_pytorch/model/utils/nms/non_maximum_suppression.py,0,"b'from __future__ import division\nimport numpy as np\nimport cupy as cp\nimport torch as t\ntry:\n    from ._nms_gpu_post import _nms_gpu_post\nexcept:\n    import warnings\n    warnings.warn(\'\'\'\n    the python code for non_maximum_suppression is about 2x slow\n    It is strongly recommended to build cython code: \n    `cd model/utils/nms/; python3 build.py build_ext --inplace\'\'\')\n    from ._nms_gpu_post_py import _nms_gpu_post\n\n\n@cp.util.memoize(for_each_device=True)\ndef _load_kernel(kernel_name, code, options=()):\n    cp.cuda.runtime.free(0)\n    assert isinstance(options, tuple)\n    kernel_code = cp.cuda.compile_with_cache(code, options=options)\n    return kernel_code.get_function(kernel_name)\n\n\ndef non_maximum_suppression(bbox, thresh, score=None,\n                            limit=None):\n    """"""Suppress bounding boxes according to their IoUs.\n\n    This method checks each bounding box sequentially and selects the bounding\n    box if the Intersection over Unions (IoUs) between the bounding box and the\n    previously selected bounding boxes is less than :obj:`thresh`. This method\n    is mainly used as postprocessing of object detection.\n    The bounding boxes are selected from ones with higher scores.\n    If :obj:`score` is not provided as an argument, the bounding box\n    is ordered by its index in ascending order.\n\n    The bounding boxes are expected to be packed into a two dimensional\n    tensor of shape :math:`(R, 4)`, where :math:`R` is the number of\n    bounding boxes in the image. The second axis represents attributes of\n    the bounding box. They are :math:`(y_{min}, x_{min}, y_{max}, x_{max})`,\n    where the four attributes are coordinates of the top left and the\n    bottom right vertices.\n\n    :obj:`score` is a float array of shape :math:`(R,)`. Each score indicates\n    confidence of prediction.\n\n    This function accepts both :obj:`numpy.ndarray` and :obj:`cupy.ndarray` as\n    an input. Please note that both :obj:`bbox` and :obj:`score` need to be\n    the same type.\n    The type of the output is the same as the input.\n\n    Args:\n        bbox (array): Bounding boxes to be transformed. The shape is\n            :math:`(R, 4)`. :math:`R` is the number of bounding boxes.\n        thresh (float): Threshold of IoUs.\n        score (array): An array of confidences whose shape is :math:`(R,)`.\n        limit (int): The upper bound of the number of the output bounding\n            boxes. If it is not specified, this method selects as many\n            bounding boxes as possible.\n\n    Returns:\n        array:\n        An array with indices of bounding boxes that are selected. \\\n        They are sorted by the scores of bounding boxes in descending \\\n        order. \\\n        The shape of this array is :math:`(K,)` and its dtype is\\\n        :obj:`numpy.int32`. Note that :math:`K \\\\leq R`.\n\n    """"""\n\n    return _non_maximum_suppression_gpu(bbox, thresh, score, limit)\n\n\ndef _non_maximum_suppression_gpu(bbox, thresh, score=None, limit=None):\n    if len(bbox) == 0:\n        return cp.zeros((0,), dtype=np.int32)\n\n    n_bbox = bbox.shape[0]\n\n    if score is not None:\n        order = score.argsort()[::-1].astype(np.int32)\n    else:\n        order = cp.arange(n_bbox, dtype=np.int32)\n\n    sorted_bbox = bbox[order, :]\n    selec, n_selec = _call_nms_kernel(\n        sorted_bbox, thresh)\n    selec = selec[:n_selec]\n    selec = order[selec]\n    if limit is not None:\n        selec = selec[:limit]\n    return cp.asnumpy(selec)\n\n\n_nms_gpu_code = \'\'\'\n#define DIVUP(m,n) ((m) / (n) + ((m) % (n) > 0))\nint const threadsPerBlock = sizeof(unsigned long long) * 8;\n\n__device__\ninline float devIoU(float const *const bbox_a, float const *const bbox_b) {\n  float top = max(bbox_a[0], bbox_b[0]);\n  float bottom = min(bbox_a[2], bbox_b[2]);\n  float left = max(bbox_a[1], bbox_b[1]);\n  float right = min(bbox_a[3], bbox_b[3]);\n  float height = max(bottom - top, 0.f);\n  float width = max(right - left, 0.f);\n  float area_i = height * width;\n  float area_a = (bbox_a[2] - bbox_a[0]) * (bbox_a[3] - bbox_a[1]);\n  float area_b = (bbox_b[2] - bbox_b[0]) * (bbox_b[3] - bbox_b[1]);\n  return area_i / (area_a + area_b - area_i);\n}\n\nextern ""C""\n__global__\nvoid nms_kernel(const int n_bbox, const float thresh,\n                const float *dev_bbox,\n                unsigned long long *dev_mask) {\n  const int row_start = blockIdx.y;\n  const int col_start = blockIdx.x;\n\n  const int row_size =\n        min(n_bbox - row_start * threadsPerBlock, threadsPerBlock);\n  const int col_size =\n        min(n_bbox - col_start * threadsPerBlock, threadsPerBlock);\n\n  __shared__ float block_bbox[threadsPerBlock * 4];\n  if (threadIdx.x < col_size) {\n    block_bbox[threadIdx.x * 4 + 0] =\n        dev_bbox[(threadsPerBlock * col_start + threadIdx.x) * 4 + 0];\n    block_bbox[threadIdx.x * 4 + 1] =\n        dev_bbox[(threadsPerBlock * col_start + threadIdx.x) * 4 + 1];\n    block_bbox[threadIdx.x * 4 + 2] =\n        dev_bbox[(threadsPerBlock * col_start + threadIdx.x) * 4 + 2];\n    block_bbox[threadIdx.x * 4 + 3] =\n        dev_bbox[(threadsPerBlock * col_start + threadIdx.x) * 4 + 3];\n  }\n  __syncthreads();\n\n  if (threadIdx.x < row_size) {\n    const int cur_box_idx = threadsPerBlock * row_start + threadIdx.x;\n    const float *cur_box = dev_bbox + cur_box_idx * 4;\n    int i = 0;\n    unsigned long long t = 0;\n    int start = 0;\n    if (row_start == col_start) {\n      start = threadIdx.x + 1;\n    }\n    for (i = start; i < col_size; i++) {\n      if (devIoU(cur_box, block_bbox + i * 4) >= thresh) {\n        t |= 1ULL << i;\n      }\n    }\n    const int col_blocks = DIVUP(n_bbox, threadsPerBlock);\n    dev_mask[cur_box_idx * col_blocks + col_start] = t;\n  }\n}\n\'\'\'\n\n\ndef _call_nms_kernel(bbox, thresh):\n    # PyTorch does not support unsigned long Tensor.\n    # Doesn\'t matter,since it returns ndarray finally.\n    # So I\'ll keep it unmodified.\n    n_bbox = bbox.shape[0]\n    threads_per_block = 64\n    col_blocks = np.ceil(n_bbox / threads_per_block).astype(np.int32)\n    blocks = (col_blocks, col_blocks, 1)\n    threads = (threads_per_block, 1, 1)\n\n    mask_dev = cp.zeros((n_bbox * col_blocks,), dtype=np.uint64)\n    bbox = cp.ascontiguousarray(bbox, dtype=np.float32)  # NOTE: \xe5\x8f\x98\xe6\x88\x90\xe8\xbf\x9e\xe7\xbb\xad\xe7\x9a\x84\n    kern = _load_kernel(\'nms_kernel\', _nms_gpu_code)\n    kern(blocks, threads, args=(cp.int32(n_bbox), cp.float32(thresh),\n                                bbox, mask_dev))\n\n    mask_host = mask_dev.get()\n    selection, n_selec = _nms_gpu_post(\n        mask_host, n_bbox, threads_per_block, col_blocks)\n    return selection, n_selec\n'"
