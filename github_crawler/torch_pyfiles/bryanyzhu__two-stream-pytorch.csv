file_path,api_count,code
main.py,15,"b'import os\nimport time\nimport argparse\nimport shutil\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.parallel\nimport torch.backends.cudnn as cudnn\nimport torch.optim\nimport torch.utils.data\n\nimport video_transforms\nimport models\nimport datasets\n\nmodel_names = sorted(name for name in models.__dict__\n    if name.islower() and not name.startswith(""__"")\n    and callable(models.__dict__[name]))\n\ndataset_names = sorted(name for name in datasets.__all__)\n\nparser = argparse.ArgumentParser(description=\'PyTorch Two-Stream Action Recognition\')\nparser.add_argument(\'data\', metavar=\'DIR\',\n                    help=\'path to dataset\')\nparser.add_argument(\'--settings\', metavar=\'DIR\', default=\'./datasets/settings\',\n                    help=\'path to datset setting files\')\nparser.add_argument(\'--modality\', \'-m\', metavar=\'MODALITY\', default=\'rgb\',\n                    choices=[""rgb"", ""flow""],\n                    help=\'modality: rgb | flow\')\nparser.add_argument(\'--dataset\', \'-d\', default=\'ucf101\',\n                    choices=[""ucf101"", ""hmdb51""],\n                    help=\'dataset: ucf101 | hmdb51\')\nparser.add_argument(\'--arch\', \'-a\', metavar=\'ARCH\', default=\'vgg16\',\n                    choices=model_names,\n                    help=\'model architecture: \' +\n                        \' | \'.join(model_names) +\n                        \' (default: vgg16)\')\nparser.add_argument(\'-s\', \'--split\', default=1, type=int, metavar=\'S\',\n                    help=\'which split of data to work on (default: 1)\')\nparser.add_argument(\'-j\', \'--workers\', default=8, type=int, metavar=\'N\',\n                    help=\'number of data loading workers (default: 4)\')\nparser.add_argument(\'--epochs\', default=400, type=int, metavar=\'N\',\n                    help=\'number of total epochs to run\')\nparser.add_argument(\'--start-epoch\', default=0, type=int, metavar=\'N\',\n                    help=\'manual epoch number (useful on restarts)\')\nparser.add_argument(\'-b\', \'--batch-size\', default=32, type=int,\n                    metavar=\'N\', help=\'mini-batch size (default: 50)\')\nparser.add_argument(\'--iter-size\', default=4, type=int,\n                    metavar=\'I\', help=\'iter size as in Caffe to reduce memory usage (default: 8)\')\nparser.add_argument(\'--new_length\', default=1, type=int,\n                    metavar=\'N\', help=\'length of sampled video frames (default: 1)\')\nparser.add_argument(\'--lr\', \'--learning-rate\', default=0.001, type=float,\n                    metavar=\'LR\', help=\'initial learning rate\')\nparser.add_argument(\'--momentum\', default=0.9, type=float, metavar=\'M\',\n                    help=\'momentum\')\nparser.add_argument(\'--weight-decay\', \'--wd\', default=1e-4, type=float,\n                    metavar=\'W\', help=\'weight decay (default: 1e-4)\')\nparser.add_argument(\'--print-freq\', default=20, type=int,\n                    metavar=\'N\', help=\'print frequency (default: 20)\')\nparser.add_argument(\'--save-freq\', default=1, type=int,\n                    metavar=\'N\', help=\'save frequency (default: 20)\')\nparser.add_argument(\'--resume\', default=\'./checkpoints\', type=str, metavar=\'PATH\',\n                    help=\'path to latest checkpoint (default: none)\')\nparser.add_argument(\'-e\', \'--evaluate\', dest=\'evaluate\', action=\'store_true\',\n                    help=\'evaluate model on validation set\')\n\nbest_prec1 = 0\n\ndef main():\n    global args, best_prec1\n    args = parser.parse_args()\n\n    # create model\n    print(""Building model ... "")\n    model = build_model()\n    print(""Model %s is loaded. "" % (args.modality + ""_"" + args.arch))\n\n    if not os.path.exists(args.resume):\n        os.makedirs(args.resume)\n    print(""Saving everything to directory %s."" % (args.resume))\n\n    # define loss function (criterion) and optimizer\n    criterion = nn.CrossEntropyLoss().cuda()\n\n    optimizer = torch.optim.SGD(model.parameters(), args.lr,\n                                momentum=args.momentum,\n                                weight_decay=args.weight_decay)\n\n    cudnn.benchmark = True\n\n    # Data transforming\n    clip_mean = [0.485, 0.456, 0.406] * args.new_length\n    clip_std = [0.229, 0.224, 0.225] * args.new_length\n    normalize = video_transforms.Normalize(mean=clip_mean,\n                                     std=clip_std)\n\n    if args.modality == ""rgb"":\n        scale_ratios = [1.0, 0.875, 0.75, 0.66]\n    elif args.modality == ""flow"": \n        scale_ratios = [1.0, 0.875, 0.75]\n    else:\n        print(""No such modality. Only rgb and flow supported."")\n\n    train_transform = video_transforms.Compose([\n            video_transforms.Scale((256)),\n            video_transforms.MultiScaleCrop((224, 224), scale_ratios),\n            video_transforms.RandomHorizontalFlip(),\n            video_transforms.ToTensor(),\n            normalize,\n        ])\n\n    val_transform = video_transforms.Compose([\n            video_transforms.Scale((256)),\n            video_transforms.CenterCrop((224)),\n            video_transforms.ToTensor(),\n            normalize,\n        ])\n    \n    # data loading \n    train_setting_file = ""train_%s_split%d.txt"" % (args.modality, args.split)\n    train_split_file = os.path.join(args.settings, args.dataset, train_setting_file)\n    val_setting_file = ""val_%s_split%d.txt"" % (args.modality, args.split)\n    val_split_file = os.path.join(args.settings, args.dataset, val_setting_file)\n    if not os.path.exists(train_split_file) or not os.path.exists(val_split_file):\n        print(""No split file exists in %s directory. Preprocess the dataset first"" % (args.settings))\n\n    train_dataset = datasets.__dict__[args.dataset](args.data, train_split_file, ""train"", args.new_length, video_transform=train_transform)\n    val_dataset = datasets.__dict__[args.dataset](args.data, val_split_file, ""val"", args.new_length, video_transform=val_transform)\n\n    print(\'{} samples found, {} train samples and {} test samples.\'.format(len(val_dataset)+len(train_dataset),\n                                                                           len(train_dataset),\n                                                                           len(val_dataset)))\n\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        batch_size=args.batch_size, shuffle=True,\n        num_workers=args.workers, pin_memory=True)\n    val_loader = torch.utils.data.DataLoader(\n        val_dataset,\n        batch_size=args.batch_size, shuffle=True,\n        num_workers=args.workers, pin_memory=True)\n\n    if args.evaluate:\n        validate(val_loader, model, criterion)\n        return\n\n    for epoch in range(args.start_epoch, args.epochs):\n        adjust_learning_rate(optimizer, epoch)\n\n        # train for one epoch\n        train(train_loader, model, criterion, optimizer, epoch)\n\n        # evaluate on validation set\n        prec1 = validate(val_loader, model, criterion)\n\n        # remember best prec@1 and save checkpoint\n        is_best = prec1 > best_prec1\n        best_prec1 = max(prec1, best_prec1)\n\n        if (epoch + 1) % args.save_freq == 0:\n            checkpoint_name = ""%03d_%s"" % (epoch + 1, ""checkpoint.pth.tar"")\n            save_checkpoint({\n                \'epoch\': epoch + 1,\n                \'arch\': args.arch,\n                \'state_dict\': model.state_dict(),\n                \'best_prec1\': best_prec1,\n                \'optimizer\' : optimizer.state_dict(),\n            }, is_best, checkpoint_name, args.resume)\n\ndef build_model():\n\n    model_name = args.modality + ""_"" + args.arch\n    model = models.__dict__[model_name](pretrained=True, num_classes=101)\n    if args.arch.startswith(\'alexnet\') or args.arch.startswith(\'vgg\'):\n        model.features = torch.nn.DataParallel(model.features)\n        model.cuda()\n    else:\n        model = torch.nn.DataParallel(model).cuda()\n    return model\n\ndef train(train_loader, model, criterion, optimizer, epoch):\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n    top1 = AverageMeter()\n    top3 = AverageMeter()\n\n    # switch to train mode\n    model.train()\n\n    end = time.time()\n    for i, (input, target) in enumerate(train_loader):\n        # measure data loading time\n        data_time.update(time.time() - end)\n\n        input = input.float().cuda(async=True)\n        target = target.cuda(async=True)\n        input_var = torch.autograd.Variable(input)\n        target_var = torch.autograd.Variable(target)\n\n        output = model(input_var)\n        loss = criterion(output, target_var)\n\n        # measure accuracy and record loss\n        prec1, prec3 = accuracy(output.data, target, topk=(1, 3))\n        losses.update(loss.data[0], input.size(0))\n        top1.update(prec1[0], input.size(0))\n        top3.update(prec3[0], input.size(0))\n\n        # compute gradient and do SGD step\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        if i % args.print_freq == 0:\n            print(\'Epoch: [{0}][{1}/{2}]\\t\'\n                  \'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t\'\n                  \'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t\'\n                  \'Loss {loss.val:.4f} ({loss.avg:.4f})\\t\'\n                  \'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t\'\n                  \'Prec@3 {top3.val:.3f} ({top3.avg:.3f})\'.format(\n                   epoch, i, len(train_loader), batch_time=batch_time,\n                   data_time=data_time, loss=losses, top1=top1, top3=top3))\n\n\ndef validate(val_loader, model, criterion):\n    batch_time = AverageMeter()\n    losses = AverageMeter()\n    top1 = AverageMeter()\n    top3 = AverageMeter()\n\n    # switch to evaluate mode\n    model.eval()\n\n    end = time.time()\n    for i, (input, target) in enumerate(val_loader):\n        input = input.float().cuda(async=True)\n        target = target.cuda(async=True)\n        input_var = torch.autograd.Variable(input, volatile=True)\n        target_var = torch.autograd.Variable(target, volatile=True)\n\n        # compute output\n        output = model(input_var)\n        loss = criterion(output, target_var)\n\n        # measure accuracy and record loss\n        prec1, prec3 = accuracy(output.data, target, topk=(1, 3))\n        losses.update(loss.data[0], input.size(0))\n        top1.update(prec1[0], input.size(0))\n        top3.update(prec3[0], input.size(0))\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        if i % args.print_freq == 0:\n            print(\'Test: [{0}/{1}]\\t\'\n                  \'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t\'\n                  \'Loss {loss.val:.4f} ({loss.avg:.4f})\\t\'\n                  \'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t\'\n                  \'Prec@3 {top3.val:.3f} ({top3.avg:.3f})\'.format(\n                   i, len(val_loader), batch_time=batch_time, loss=losses,\n                   top1=top1, top3=top3))\n\n    print(\' * Prec@1 {top1.avg:.3f} Prec@3 {top3.avg:.3f}\'\n          .format(top1=top1, top3=top3))\n\n    return top1.avg\n\n\ndef save_checkpoint(state, is_best, filename, resume_path):\n    cur_path = os.path.join(resume_path, filename)\n    best_path = os.path.join(resume_path, \'model_best.pth.tar\')\n    torch.save(state, cur_path)\n    if is_best:\n        shutil.copyfile(cur_path, best_path)\n\n\nclass AverageMeter(object):\n    """"""Computes and stores the average and current value""""""\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\ndef adjust_learning_rate(optimizer, epoch):\n    """"""Sets the learning rate to the initial LR decayed by 10 every 150 epochs""""""\n    lr = args.lr * (0.1 ** (epoch // 150))\n    for param_group in optimizer.param_groups:\n        param_group[\'lr\'] = lr\n        # param_group[\'lr\'] = param_group[\'lr\']/2\n\ndef accuracy(output, target, topk=(1,)):\n    """"""Computes the precision@k for the specified values of k""""""\n    maxk = max(topk)\n    batch_size = target.size(0)\n\n    _, pred = output.topk(maxk, 1, True, True)\n    pred = pred.t()\n    correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n    res = []\n    for k in topk:\n        correct_k = correct[:k].view(-1).float().sum(0)\n        res.append(correct_k.mul_(100.0 / batch_size))\n    return res\n\nif __name__ == \'__main__\':\n    main()\n'"
main_single_gpu.py,13,"b'import os\nimport time\nimport argparse\nimport shutil\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.parallel\nimport torch.backends.cudnn as cudnn\nimport torch.optim\nimport torch.utils.data\n\nimport video_transforms\nimport models\nimport datasets\n\nmodel_names = sorted(name for name in models.__dict__\n    if name.islower() and not name.startswith(""__"")\n    and callable(models.__dict__[name]))\n\ndataset_names = sorted(name for name in datasets.__all__)\n\nparser = argparse.ArgumentParser(description=\'PyTorch Two-Stream Action Recognition\')\nparser.add_argument(\'data\', metavar=\'DIR\',\n                    help=\'path to dataset\')\nparser.add_argument(\'--settings\', metavar=\'DIR\', default=\'./datasets/settings\',\n                    help=\'path to datset setting files\')\nparser.add_argument(\'--modality\', \'-m\', metavar=\'MODALITY\', default=\'rgb\',\n                    choices=[""rgb"", ""flow""],\n                    help=\'modality: rgb | flow\')\nparser.add_argument(\'--dataset\', \'-d\', default=\'ucf101\',\n                    choices=[""ucf101"", ""hmdb51""],\n                    help=\'dataset: ucf101 | hmdb51\')\nparser.add_argument(\'--arch\', \'-a\', metavar=\'ARCH\', default=\'rgb_resnet152\',\n                    choices=model_names,\n                    help=\'model architecture: \' +\n                        \' | \'.join(model_names) +\n                        \' (default: rgb_vgg16)\')\nparser.add_argument(\'-s\', \'--split\', default=1, type=int, metavar=\'S\',\n                    help=\'which split of data to work on (default: 1)\')\nparser.add_argument(\'-j\', \'--workers\', default=4, type=int, metavar=\'N\',\n                    help=\'number of data loading workers (default: 4)\')\nparser.add_argument(\'--epochs\', default=250, type=int, metavar=\'N\',\n                    help=\'number of total epochs to run\')\nparser.add_argument(\'--start-epoch\', default=0, type=int, metavar=\'N\',\n                    help=\'manual epoch number (useful on restarts)\')\nparser.add_argument(\'-b\', \'--batch-size\', default=25, type=int,\n                    metavar=\'N\', help=\'mini-batch size (default: 50)\')\nparser.add_argument(\'--iter-size\', default=5, type=int,\n                    metavar=\'I\', help=\'iter size as in Caffe to reduce memory usage (default: 5)\')\nparser.add_argument(\'--new_length\', default=1, type=int,\n                    metavar=\'N\', help=\'length of sampled video frames (default: 1)\')\nparser.add_argument(\'--new_width\', default=340, type=int,\n                    metavar=\'N\', help=\'resize width (default: 340)\')\nparser.add_argument(\'--new_height\', default=256, type=int,\n                    metavar=\'N\', help=\'resize height (default: 256)\')\nparser.add_argument(\'--lr\', \'--learning-rate\', default=0.001, type=float,\n                    metavar=\'LR\', help=\'initial learning rate\')\nparser.add_argument(\'--lr_steps\', default=[100, 200], type=float, nargs=""+"",\n                    metavar=\'LRSteps\', help=\'epochs to decay learning rate by 10\')\nparser.add_argument(\'--momentum\', default=0.9, type=float, metavar=\'M\',\n                    help=\'momentum\')\nparser.add_argument(\'--weight-decay\', \'--wd\', default=5e-4, type=float,\n                    metavar=\'W\', help=\'weight decay (default: 5e-4)\')\nparser.add_argument(\'--print-freq\', default=50, type=int,\n                    metavar=\'N\', help=\'print frequency (default: 50)\')\nparser.add_argument(\'--save-freq\', default=25, type=int,\n                    metavar=\'N\', help=\'save frequency (default: 25)\')\nparser.add_argument(\'--resume\', default=\'./checkpoints\', type=str, metavar=\'PATH\',\n                    help=\'path to latest checkpoint (default: none)\')\nparser.add_argument(\'-e\', \'--evaluate\', dest=\'evaluate\', action=\'store_true\',\n                    help=\'evaluate model on validation set\')\n\nbest_prec1 = 0\nos.environ[""CUDA_DEVICE_ORDER""]=""PCI_BUS_ID""   \nos.environ[""CUDA_VISIBLE_DEVICES""]=""0""\n\ndef main():\n    global args, best_prec1\n    args = parser.parse_args()\n\n    # create model\n    print(""Building model ... "")\n    model = build_model()\n    print(""Model %s is loaded. "" % (args.arch))\n\n    # define loss function (criterion) and optimizer\n    criterion = nn.CrossEntropyLoss().cuda()\n\n    optimizer = torch.optim.SGD(model.parameters(), args.lr,\n                                momentum=args.momentum,\n                                weight_decay=args.weight_decay)\n\n    if not os.path.exists(args.resume):\n        os.makedirs(args.resume)\n    print(""Saving everything to directory %s."" % (args.resume))\n\n    cudnn.benchmark = True\n\n    # Data transforming\n    if args.modality == ""rgb"":\n        is_color = True\n        scale_ratios = [1.0, 0.875, 0.75, 0.66]\n        clip_mean = [0.485, 0.456, 0.406] * args.new_length\n        clip_std = [0.229, 0.224, 0.225] * args.new_length\n    elif args.modality == ""flow"":\n        is_color = False\n        scale_ratios = [1.0, 0.875, 0.75]\n        clip_mean = [0.5, 0.5] * args.new_length\n        clip_std = [0.226, 0.226] * args.new_length\n    else:\n        print(""No such modality. Only rgb and flow supported."")\n\n    normalize = video_transforms.Normalize(mean=clip_mean,\n                                           std=clip_std)\n    train_transform = video_transforms.Compose([\n            # video_transforms.Scale((256)),\n            video_transforms.MultiScaleCrop((224, 224), scale_ratios),\n            video_transforms.RandomHorizontalFlip(),\n            video_transforms.ToTensor(),\n            normalize,\n        ])\n\n    val_transform = video_transforms.Compose([\n            # video_transforms.Scale((256)),\n            video_transforms.CenterCrop((224)),\n            video_transforms.ToTensor(),\n            normalize,\n        ])\n\n    # data loading\n    train_setting_file = ""train_%s_split%d.txt"" % (args.modality, args.split)\n    train_split_file = os.path.join(args.settings, args.dataset, train_setting_file)\n    val_setting_file = ""val_%s_split%d.txt"" % (args.modality, args.split)\n    val_split_file = os.path.join(args.settings, args.dataset, val_setting_file)\n    if not os.path.exists(train_split_file) or not os.path.exists(val_split_file):\n        print(""No split file exists in %s directory. Preprocess the dataset first"" % (args.settings))\n\n    train_dataset = datasets.__dict__[args.dataset](root=args.data,\n                                                    source=train_split_file,\n                                                    phase=""train"",\n                                                    modality=args.modality,\n                                                    is_color=is_color,\n                                                    new_length=args.new_length,\n                                                    new_width=args.new_width,\n                                                    new_height=args.new_height,\n                                                    video_transform=train_transform)\n    val_dataset = datasets.__dict__[args.dataset](root=args.data,\n                                                  source=val_split_file,\n                                                  phase=""val"",\n                                                  modality=args.modality,\n                                                  is_color=is_color,\n                                                  new_length=args.new_length,\n                                                  new_width=args.new_width,\n                                                  new_height=args.new_height,\n                                                  video_transform=val_transform)\n\n    print(\'{} samples found, {} train samples and {} test samples.\'.format(len(val_dataset)+len(train_dataset),\n                                                                           len(train_dataset),\n                                                                           len(val_dataset)))\n\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        batch_size=args.batch_size, shuffle=True,\n        num_workers=args.workers, pin_memory=True)\n    val_loader = torch.utils.data.DataLoader(\n        val_dataset,\n        batch_size=args.batch_size, shuffle=True,\n        num_workers=args.workers, pin_memory=True)\n\n    if args.evaluate:\n        validate(val_loader, model, criterion)\n        return\n\n    for epoch in range(args.start_epoch, args.epochs):\n        adjust_learning_rate(optimizer, epoch)\n\n        # train for one epoch\n        train(train_loader, model, criterion, optimizer, epoch)\n\n        # evaluate on validation set\n        prec1 = 0.0\n        if (epoch + 1) % args.save_freq == 0:\n            prec1 = validate(val_loader, model, criterion)\n\n        # remember best prec@1 and save checkpoint\n        is_best = prec1 > best_prec1\n        best_prec1 = max(prec1, best_prec1)\n\n        if (epoch + 1) % args.save_freq == 0:\n            checkpoint_name = ""%03d_%s"" % (epoch + 1, ""checkpoint.pth.tar"")\n            save_checkpoint({\n                \'epoch\': epoch + 1,\n                \'arch\': args.arch,\n                \'state_dict\': model.state_dict(),\n                \'best_prec1\': best_prec1,\n                \'optimizer\' : optimizer.state_dict(),\n            }, is_best, checkpoint_name, args.resume)\n\ndef build_model():\n\n    model = models.__dict__[args.arch](pretrained=True, num_classes=101)\n    model.cuda()\n    return model\n\ndef train(train_loader, model, criterion, optimizer, epoch):\n    batch_time = AverageMeter()\n    losses = AverageMeter()\n    top1 = AverageMeter()\n\n    # switch to train mode\n    model.train()\n\n    end = time.time()\n    optimizer.zero_grad()\n    loss_mini_batch = 0.0\n    acc_mini_batch = 0.0\n\n    for i, (input, target) in enumerate(train_loader):\n\n        input = input.float().cuda(async=True)\n        target = target.cuda(async=True)\n        input_var = torch.autograd.Variable(input)\n        target_var = torch.autograd.Variable(target)\n\n        output = model(input_var)\n\n        # measure accuracy and record loss\n        prec1, prec3 = accuracy(output.data, target, topk=(1, 3))\n        acc_mini_batch += prec1[0]\n        loss = criterion(output, target_var)\n        loss = loss / args.iter_size\n        loss_mini_batch += loss.data[0]\n        loss.backward()\n\n        if (i+1) % args.iter_size == 0:\n            # compute gradient and do SGD step\n            optimizer.step()\n            optimizer.zero_grad()\n\n            # losses.update(loss_mini_batch/args.iter_size, input.size(0))\n            # top1.update(acc_mini_batch/args.iter_size, input.size(0))\n            losses.update(loss_mini_batch, input.size(0))\n            top1.update(acc_mini_batch/args.iter_size, input.size(0))\n            batch_time.update(time.time() - end)\n            end = time.time()\n            loss_mini_batch = 0\n            acc_mini_batch = 0\n\n            if (i+1) % args.print_freq == 0:\n\n                print(\'Epoch: [{0}][{1}/{2}]\\t\'\n                      \'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t\'\n                      \'Loss {loss.val:.4f} ({loss.avg:.4f})\\t\'\n                      \'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\'.format(\n                       epoch, i+1, len(train_loader)+1, batch_time=batch_time, loss=losses, top1=top1))\n\ndef validate(val_loader, model, criterion):\n    batch_time = AverageMeter()\n    losses = AverageMeter()\n    top1 = AverageMeter()\n    top3 = AverageMeter()\n\n    # switch to evaluate mode\n    model.eval()\n\n    end = time.time()\n    for i, (input, target) in enumerate(val_loader):\n        input = input.float().cuda(async=True)\n        target = target.cuda(async=True)\n        input_var = torch.autograd.Variable(input, volatile=True)\n        target_var = torch.autograd.Variable(target, volatile=True)\n\n        # compute output\n        output = model(input_var)\n        loss = criterion(output, target_var)\n\n        # measure accuracy and record loss\n        prec1, prec3 = accuracy(output.data, target, topk=(1, 3))\n        losses.update(loss.data[0], input.size(0))\n        top1.update(prec1[0], input.size(0))\n        top3.update(prec3[0], input.size(0))\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        if i % args.print_freq == 0:\n            print(\'Test: [{0}/{1}]\\t\'\n                  \'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t\'\n                  \'Loss {loss.val:.4f} ({loss.avg:.4f})\\t\'\n                  \'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t\'\n                  \'Prec@3 {top3.val:.3f} ({top3.avg:.3f})\'.format(\n                   i, len(val_loader), batch_time=batch_time, loss=losses,\n                   top1=top1, top3=top3))\n\n    print(\' * Prec@1 {top1.avg:.3f} Prec@3 {top3.avg:.3f}\'\n          .format(top1=top1, top3=top3))\n\n    return top1.avg\n\ndef save_checkpoint(state, is_best, filename, resume_path):\n    cur_path = os.path.join(resume_path, filename)\n    best_path = os.path.join(resume_path, \'model_best.pth.tar\')\n    torch.save(state, cur_path)\n    if is_best:\n        shutil.copyfile(cur_path, best_path)\n\nclass AverageMeter(object):\n    """"""Computes and stores the average and current value""""""\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\ndef adjust_learning_rate(optimizer, epoch):\n    """"""Sets the learning rate to the initial LR decayed by 10 every 150 epochs""""""\n\n    decay = 0.1 ** (sum(epoch >= np.array(args.lr_steps)))\n    lr = args.lr * decay\n    print(""Current learning rate is %4.6f:"" % lr)\n    for param_group in optimizer.param_groups:\n        param_group[\'lr\'] = lr\n\ndef accuracy(output, target, topk=(1,)):\n    """"""Computes the precision@k for the specified values of k""""""\n    maxk = max(topk)\n    batch_size = target.size(0)\n\n    _, pred = output.topk(maxk, 1, True, True)\n    pred = pred.t()\n    correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n    res = []\n    for k in topk:\n        correct_k = correct[:k].view(-1).float().sum(0)\n        res.append(correct_k.mul_(100.0 / batch_size))\n    return res\n\nif __name__ == \'__main__\':\n    main()\n'"
video_transforms.py,3,"b'from __future__ import division\nimport torch\nimport random\nimport numpy as np\nimport numbers\nimport types\nimport cv2\nimport math\nimport os, sys\nimport collections\n\nclass Compose(object):\n    """"""Composes several video_transforms together.\n\n    Args:\n        transforms (List[Transform]): list of transforms to compose.\n\n    Example:\n        >>> video_transforms.Compose([\n        >>>     video_transforms.CenterCrop(10),\n        >>>     video_transforms.ToTensor(),\n        >>> ])\n    """"""\n\n    def __init__(self, video_transforms):\n        self.video_transforms = video_transforms\n\n    def __call__(self, clips):\n        for t in self.video_transforms:\n            clips = t(clips)\n        return clips\n\nclass Lambda(object):\n    """"""Applies a lambda as a transform""""""\n    def __init__(self, lambd):\n        assert type(lambd) is types.LambdaType\n        self.lambd = lambd\n\n    def __call__(self, clips):\n        return self.lambd(clips)\n\nclass ToTensor(object):\n    """"""Converts a numpy.ndarray (H x W x C) in the range\n    [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0].\n    """"""\n\n    def __call__(self, clips):\n        if isinstance(clips, np.ndarray):\n            # handle numpy array\n            clips = torch.from_numpy(clips.transpose((2, 0, 1)))\n            # backward compatibility\n            return clips.float().div(255.0)\n\nclass Normalize(object):\n    """"""Given mean: (R, G, B) and std: (R, G, B),\n    will normalize each channel of the torch.*Tensor, i.e.\n    channel = (channel - mean) / std\n    Here, the input is a clip, not a single image. (multi-channel data)\n    The dimension of mean and std depends on parameter: new_length\n    If new_length = 1, it falls back to single image case (3 channel)\n    """"""\n\n    def __init__(self, mean, std):\n        self.mean = mean\n        self.std = std\n\n    def __call__(self, tensor):\n        # TODO: make efficient\n        for t, m, s in zip(tensor, self.mean, self.std):\n            t.sub_(m).div_(s)\n        return tensor\n\nclass Scale(object):\n    """""" Rescales the input numpy array to the given \'size\'.\n    \'size\' will be the size of the smaller edge.\n    For example, if height > width, then image will be\n    rescaled to (size * height / width, size)\n    size: size of the smaller edge\n    interpolation: Default: cv2.INTER_LINEAR\n    """"""\n    def __init__(self, size, interpolation=cv2.INTER_LINEAR):\n        self.size = size\n        self.interpolation = interpolation\n\n    def __call__(self, clips):\n\n        h, w, c = clips.shape\n        new_w = 0\n        new_h = 0\n        if isinstance(self.size, int):\n            if (w <= h and w == self.size) or (h <= w and h == self.size):\n                return clips\n            if w < h:\n                new_w = self.size\n                new_h = int(self.size * h / w)\n            else:\n                new_w = int(self.size * w / h)\n                new_h = self.size\n        else:\n            new_w = self.size[0]\n            new_h = self.size[1]\n\n        is_color = False\n        if c % 3 == 0:\n            is_color = True\n\n        if is_color:\n            num_imgs = int(c / 3)\n            scaled_clips = np.zeros((new_h,new_w,c))\n            for frame_id in range(num_imgs):\n                cur_img = clips[:,:,frame_id*3:frame_id*3+3]\n                scaled_clips[:,:,frame_id*3:frame_id*3+3] = cv2.resize(cur_img, (new_w, new_h), self.interpolation)\n        else:\n            num_imgs = int(c / 1)\n            scaled_clips = np.zeros((new_h,new_w,c))\n            for frame_id in range(num_imgs):\n                cur_img = clips[:,:,frame_id:frame_id+1]\n                scaled_clips[:,:,frame_id:frame_id+1] = cv2.resize(cur_img, (new_w, new_h), self.interpolation)\n        return scaled_clips\n\n\nclass CenterCrop(object):\n    """"""Crops the given numpy array at the center to have a region of\n    the given size. size can be a tuple (target_height, target_width)\n    or an integer, in which case the target will be of a square shape (size, size)\n    """"""\n\n    def __init__(self, size):\n        if isinstance(size, numbers.Number):\n            self.size = (int(size), int(size))\n        else:\n            self.size = size\n\n    def __call__(self, clips):\n        h, w, c = clips.shape\n        th, tw = self.size\n        x1 = int(round((w - tw) / 2.))\n        y1 = int(round((h - th) / 2.))\n\n        is_color = False\n        if c % 3 == 0:\n            is_color = True\n\n        if is_color:\n            num_imgs = int(c / 3)\n            scaled_clips = np.zeros((th,tw,c))\n            for frame_id in range(num_imgs):\n                cur_img = clips[:,:,frame_id*3:frame_id*3+3]\n                crop_img = cur_img[y1:y1+th, x1:x1+tw, :]\n                assert(crop_img.shape == (th, tw, 3))\n                scaled_clips[:,:,frame_id*3:frame_id*3+3] = crop_img\n            return scaled_clips\n        else:\n            num_imgs = int(c / 1)\n            scaled_clips = np.zeros((th,tw,c))\n            for frame_id in range(num_imgs):\n                cur_img = clips[:,:,frame_id:frame_id+1]\n                crop_img = cur_img[y1:y1+th, x1:x1+tw, :]\n                assert(crop_img.shape == (th, tw, 1))\n                scaled_clips[:,:,frame_id:frame_id+1] = crop_img\n            return scaled_clips\n\nclass RandomHorizontalFlip(object):\n    """"""Randomly horizontally flips the given numpy array with a probability of 0.5\n    """"""\n    def __call__(self, clips):\n        if random.random() < 0.5:\n            clips = np.fliplr(clips)\n            clips = np.ascontiguousarray(clips)\n        return clips\n\nclass RandomVerticalFlip(object):\n    """"""Randomly vertically flips the given numpy array with a probability of 0.5\n    """"""\n    def __call__(self, clips):\n        if random.random() < 0.5:\n            clips = np.flipud(clips)\n            clips = np.ascontiguousarray(clips)\n        return clips\n\n\nclass RandomSizedCrop(object):\n    """"""Random crop the given numpy array to a random size of (0.08 to 1.0) of the original size\n    and and a random aspect ratio of 3/4 to 4/3 of the original aspect ratio\n    This is popularly used to train the Inception networks\n    size: size of the smaller edge\n    interpolation: Default: cv2.INTER_LINEAR\n    """"""\n\n    def __init__(self, size, interpolation=cv2.INTER_LINEAR):\n        self.size = size\n        self.interpolation = interpolation\n\n    def __call__(self, clips):\n        h, w, c = clips.shape\n        is_color = False\n        if c % 3 == 0:\n            is_color = True\n\n        for attempt in range(10):\n            area = w * h\n            target_area = random.uniform(0.08, 1.0) * area\n            aspect_ratio = random.uniform(3. / 4, 4. / 3)\n\n            new_w = int(round(math.sqrt(target_area * aspect_ratio)))\n            new_h = int(round(math.sqrt(target_area / aspect_ratio)))\n\n            if random.random() < 0.5:\n                new_w, new_h = new_h, new_w\n\n            if new_w <= w and new_h <= h:\n                x1 = random.randint(0, w - new_w)\n                y1 = random.randint(0, h - new_h)\n\n                scaled_clips = np.zeros((self.size,self.size,c))\n                if is_color:\n                    num_imgs = int(c / 3)\n                    for frame_id in range(num_imgs):\n                        cur_img = clips[:,:,frame_id*3:frame_id*3+3]\n                        crop_img = cur_img[y1:y1+new_h, x1:x1+new_w, :]\n                        assert(crop_img.shape == (new_h, new_w, 3))\n                        scaled_clips[:,:,frame_id*3:frame_id*3+3] = cv2.resize(crop_img, (self.size, self.size), self.interpolation)\n                    return scaled_clips\n                else:\n                    num_imgs = int(c / 1)\n                    for frame_id in range(num_imgs):\n                        cur_img = clips[:,:,frame_id:frame_id+1]\n                        crop_img = cur_img[y1:y1+new_h, x1:x1+new_w, :]\n                        assert(crop_img.shape == (new_h, new_w, 1))\n                        scaled_clips[:,:,frame_id:frame_id+1] = cv2.resize(crop_img, (self.size, self.size), self.interpolation)\n                    return scaled_clips\n\n        # Fallback\n        scale = Scale(self.size, interpolation=self.interpolation)\n        crop = CenterCrop(self.size)\n        return crop(scale(clips))\n\nclass MultiScaleCrop(object):\n    """"""\n    Description: Corner cropping and multi-scale cropping. Two data augmentation techniques introduced in:\n        Towards Good Practices for Very Deep Two-Stream ConvNets,\n        http://arxiv.org/abs/1507.02159\n        Limin Wang, Yuanjun Xiong, Zhe Wang and Yu Qiao\n\n    Parameters:\n        size: height and width required by network input, e.g., (224, 224)\n        scale_ratios: efficient scale jittering, e.g., [1.0, 0.875, 0.75, 0.66]\n        fix_crop: use corner cropping or not. Default: True\n        more_fix_crop: use more corners or not. Default: True\n        max_distort: maximum distortion. Default: 1\n        interpolation: Default: cv2.INTER_LINEAR\n    """"""\n\n    def __init__(self, size, scale_ratios, fix_crop=True, more_fix_crop=True, max_distort=1, interpolation=cv2.INTER_LINEAR):\n        self.height = size[0]\n        self.width = size[1]\n        self.scale_ratios = scale_ratios\n        self.fix_crop = fix_crop\n        self.more_fix_crop = more_fix_crop\n        self.max_distort = max_distort\n        self.interpolation = interpolation\n\n    def fillFixOffset(self, datum_height, datum_width):\n        h_off = int((datum_height - self.height) / 4)\n        w_off = int((datum_width - self.width) / 4)\n\n        offsets = []\n        offsets.append((0, 0))          # upper left\n        offsets.append((0, 4*w_off))    # upper right\n        offsets.append((4*h_off, 0))    # lower left\n        offsets.append((4*h_off, 4*w_off))  # lower right\n        offsets.append((2*h_off, 2*w_off))  # center\n\n        if self.more_fix_crop:\n            offsets.append((0, 2*w_off))        # top center\n            offsets.append((4*h_off, 2*w_off))  # bottom center\n            offsets.append((2*h_off, 0))        # left center\n            offsets.append((2*h_off, 4*w_off))  # right center\n\n            offsets.append((1*h_off, 1*w_off))  # upper left quarter\n            offsets.append((1*h_off, 3*w_off))  # upper right quarter\n            offsets.append((3*h_off, 1*w_off))  # lower left quarter\n            offsets.append((3*h_off, 3*w_off))  # lower right quarter\n\n        return offsets\n\n    def fillCropSize(self, input_height, input_width):\n        crop_sizes = []\n        base_size = np.min((input_height, input_width))\n        scale_rates = self.scale_ratios\n        for h in range(len(scale_rates)):\n            crop_h = int(base_size * scale_rates[h])\n            for w in range(len(scale_rates)):\n                crop_w = int(base_size * scale_rates[w])\n                # append this cropping size into the list\n                if (np.absolute(h-w) <= self.max_distort):\n                    crop_sizes.append((crop_h, crop_w))\n\n        return crop_sizes\n\n    def __call__(self, clips):\n        h, w, c = clips.shape\n        is_color = False\n        if c % 3 == 0:\n            is_color = True\n\n        crop_size_pairs = self.fillCropSize(h, w)\n        size_sel = random.randint(0, len(crop_size_pairs)-1)\n        crop_height = crop_size_pairs[size_sel][0]\n        crop_width = crop_size_pairs[size_sel][1]\n\n        if self.fix_crop:\n            offsets = self.fillFixOffset(h, w)\n            off_sel = random.randint(0, len(offsets)-1)\n            h_off = offsets[off_sel][0]\n            w_off = offsets[off_sel][1]\n        else:\n            h_off = random.randint(0, h - self.height)\n            w_off = random.randint(0, w - self.width)\n\n        scaled_clips = np.zeros((self.height,self.width,c))\n        if is_color:\n            num_imgs = int(c / 3)\n            for frame_id in range(num_imgs):\n                cur_img = clips[:,:,frame_id*3:frame_id*3+3]\n                crop_img = cur_img[h_off:h_off+crop_height, w_off:w_off+crop_width, :]\n                scaled_clips[:,:,frame_id*3:frame_id*3+3] = cv2.resize(crop_img, (self.width, self.height), self.interpolation)\n            return scaled_clips\n        else:\n            num_imgs = int(c / 1)\n            for frame_id in range(num_imgs):\n                cur_img = clips[:,:,frame_id:frame_id+1]\n                crop_img = cur_img[h_off:h_off+crop_height, w_off:w_off+crop_width, :]\n                scaled_clips[:,:,frame_id:frame_id+1] = np.expand_dims(cv2.resize(crop_img, (self.width, self.height), self.interpolation), axis=2)\n            return scaled_clips\n\n\n\n\n\n\n'"
datasets/__init__.py,0,"b""from .ucf101 import ucf101\n\n__all__ = ('ucf101','hmdb51')\n"""
datasets/build_file_list.py,0,"b'\nimport argparse\nimport os\nimport glob\nimport random\nimport fnmatch\n\ndef parse_directory(path, rgb_prefix=\'img_\', flow_x_prefix=\'flow_x_\', flow_y_prefix=\'flow_y_\'):\n    """"""\n    Parse directories holding extracted frames from standard benchmarks\n    """"""\n    print(\'parse frames under folder {}\'.format(path))\n    frame_folders = glob.glob(os.path.join(path, \'*\'))\n\n    def count_files(directory, prefix_list):\n        lst = os.listdir(directory)\n        cnt_list = [len(fnmatch.filter(lst, x+\'*\')) for x in prefix_list]\n        return cnt_list\n\n    rgb_counts = {}\n    flow_counts = {}\n    for i,f in enumerate(frame_folders):\n        all_cnt = count_files(f, (rgb_prefix, flow_x_prefix, flow_y_prefix))\n        k = f.split(\'/\')[-1]\n        rgb_counts[k] = all_cnt[0]\n\n        x_cnt = all_cnt[1]\n        y_cnt = all_cnt[2]\n        if x_cnt != y_cnt:\n            raise ValueError(\'x and y direction have different number of flow images. video: \'+f)\n        flow_counts[k] = x_cnt\n        if i % 200 == 0:\n            print(\'{} videos parsed\'.format(i))\n\n    print(\'frame folder analysis done\')\n    return rgb_counts, flow_counts\n\n\ndef build_split_list(split_tuple, frame_info, split_idx, shuffle=False):\n    split = split_tuple[split_idx]\n\n    def build_set_list(set_list):\n        rgb_list, flow_list = list(), list()\n        for item in set_list:\n            rgb_cnt = frame_info[0][item[0]]\n            flow_cnt = frame_info[1][item[0]]\n            rgb_list.append(\'{} {} {}\\n\'.format(item[0], rgb_cnt, item[1]))\n            flow_list.append(\'{} {} {}\\n\'.format(item[0], flow_cnt, item[1]))\n        if shuffle:\n            random.shuffle(rgb_list)\n            random.shuffle(flow_list)\n        return rgb_list, flow_list\n\n    train_rgb_list, train_flow_list = build_set_list(split[0])\n    test_rgb_list, test_flow_list = build_set_list(split[1])\n    return (train_rgb_list, test_rgb_list), (train_flow_list, test_flow_list)\n\n\ndef parse_ucf101_splits():\n    class_ind = [x.strip().split() for x in open(\'ucf101_splits/classInd.txt\')]\n    class_mapping = {x[1]:int(x[0])-1 for x in class_ind}\n\n    def line2rec(line):\n        items = line.strip().split(\'/\')\n        label = class_mapping[items[0]]\n        vid = items[1].split(\'.\')[0]\n        return vid, label\n\n    splits = []\n    for i in xrange(1, 4):\n        train_list = [line2rec(x) for x in open(\'ucf101_splits/trainlist{:02d}.txt\'.format(i))]\n        test_list = [line2rec(x) for x in open(\'ucf101_splits/testlist{:02d}.txt\'.format(i))]\n        splits.append((train_list, test_list))\n    return splits\n\n\ndef parse_hmdb51_splits():\n    # load split file\n    class_files = glob.glob(\'hmdb51_splits/*split*.txt\')\n\n    # load class list\n    class_list = [x.strip() for x in open(\'hmdb51_splits/class_list.txt\')]\n    class_dict = {x: i for i, x in enumerate(class_list)}\n\n    def parse_class_file(filename):\n        # parse filename parts\n        filename_parts = filename.split(\'/\')[-1][:-4].split(\'_\')\n        split_id = int(filename_parts[-1][-1])\n        class_name = \'_\'.join(filename_parts[:-2])\n\n        # parse class file contents\n        contents = [x.strip().split() for x in open(filename).readlines()]\n        train_videos = [ln[0][:-4] for ln in contents if ln[1] == \'1\']\n        test_videos = [ln[0][:-4] for ln in contents if ln[1] == \'2\']\n\n        return class_name, split_id, train_videos, test_videos\n\n    class_info_list = map(parse_class_file, class_files)\n\n    splits = []\n    for i in xrange(1, 4):\n        train_list = [\n            (vid, class_dict[cls[0]]) for cls in class_info_list for vid in cls[2] if cls[1] == i\n        ]\n        test_list = [\n            (vid, class_dict[cls[0]]) for cls in class_info_list for vid in cls[3] if cls[1] == i\n        ]\n        splits.append((train_list, test_list))\n    return splits\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--dataset\', type=str, default=\'ucf101\', choices=[\'ucf101\', \'hmdb51\'])\n    parser.add_argument(\'--frame_path\', type=str, default=\'./ucf101_frames\',\n                        help=""root directory holding the frames"")\n    parser.add_argument(\'--out_list_path\', type=str, default=\'./settings\')\n\n    parser.add_argument(\'--rgb_prefix\', type=str, default=\'img_\',\n                        help=""prefix of RGB frames"")\n    parser.add_argument(\'--flow_x_prefix\', type=str, default=\'flow_x\',\n                        help=""prefix of x direction flow images"")\n    parser.add_argument(\'--flow_y_prefix\', type=str, default=\'flow_y\',\n                        help=""prefix of y direction flow images"", )\n\n    parser.add_argument(\'--num_split\', type=int, default=3,\n                        help=""number of split building file list"")\n    parser.add_argument(\'--shuffle\', action=\'store_true\', default=False)\n\n    args = parser.parse_args()\n\n    dataset = args.dataset\n    frame_path = args.frame_path\n    rgb_p = args.rgb_prefix\n    flow_x_p = args.flow_x_prefix\n    flow_y_p = args.flow_y_prefix\n    num_split = args.num_split\n    out_path = args.out_list_path\n    shuffle = args.shuffle\n\n    out_path = os.path.join(out_path,dataset)\n    if not os.path.isdir(out_path):\n        print(""creating folder: ""+out_path)\n        os.makedirs(out_path)\n\n    # operation\n    print(\'processing dataset {}\'.format(dataset))\n    if dataset==\'ucf101\':\n        split_tp = parse_ucf101_splits()\n    else:\n        split_tp = parse_hmdb51_splits()\n    f_info = parse_directory(frame_path, rgb_p, flow_x_p, flow_y_p)\n\n    print(\'writing list files for training/testing\')\n    for i in xrange(max(num_split, len(split_tp))):\n        lists = build_split_list(split_tp, f_info, i, shuffle)\n        open(os.path.join(out_path, \'train_rgb_split{}.txt\'.format(i + 1)), \'w\').writelines(lists[0][0])\n        open(os.path.join(out_path, \'val_rgb_split{}.txt\'.format(i + 1)), \'w\').writelines(lists[0][1])\n        open(os.path.join(out_path, \'train_flow_split{}.txt\'.format(i + 1)), \'w\').writelines(lists[1][0])\n        open(os.path.join(out_path, \'val_flow_split{}.txt\'.format(i + 1)), \'w\').writelines(lists[1][1])\n\n'"
datasets/build_of.py,0,"b'\nfrom __future__ import print_function\n\nimport os\nimport sys\nimport glob\nimport argparse\nfrom pipes import quote\nfrom multiprocessing import Pool, current_process\n\n\ndef run_optical_flow(vid_item):\n    vid_path = vid_item[0]\n    vid_id = vid_item[1]\n    vid_name = vid_path.split(\'/\')[-1].split(\'.\')[0]\n    out_full_path = os.path.join(out_path, vid_name)\n    try:\n        os.mkdir(out_full_path)\n    except OSError:\n        pass\n\n    current = current_process()\n    dev_id = (int(current._identity[0]) - 1) % NUM_GPU\n    image_path = \'{}/img\'.format(out_full_path)\n    flow_x_path = \'{}/flow_x\'.format(out_full_path)\n    flow_y_path = \'{}/flow_y\'.format(out_full_path)\n\n    cmd = os.path.join(df_path + \'build/extract_gpu\')+\' -f {} -x {} -y {} -i {} -b 20 -t 1 -d {} -s 1 -o {} -w {} -h {}\'.format(\n        quote(vid_path), quote(flow_x_path), quote(flow_y_path), quote(image_path), dev_id, out_format, new_size[0], new_size[1])\n\n    os.system(cmd)\n    print(\'{} {} done\'.format(vid_id, vid_name))\n    sys.stdout.flush()\n    return True\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(description=""extract optical flows"")\n    parser.add_argument(""--src_dir"", type=str, default=\'./UCF-101\',\n                        help=\'path to the video data\')\n    parser.add_argument(""--out_dir"", type=str, default=\'./ucf101_frames\',\n                        help=\'path to store frames and optical flow\')\n    parser.add_argument(""--df_path"", type=str, default=\'./dense_flow/\',\n                        help=\'path to the dense_flow toolbox\')\n\n    parser.add_argument(""--new_width"", type=int, default=0, help=\'resize image width\')\n    parser.add_argument(""--new_height"", type=int, default=0, help=\'resize image height\')\n\n    parser.add_argument(""--num_worker"", type=int, default=8)\n    parser.add_argument(""--num_gpu"", type=int, default=2, help=\'number of GPU\')\n    parser.add_argument(""--out_format"", type=str, default=\'dir\', choices=[\'dir\',\'zip\'],\n                        help=\'path to the dense_flow toolbox\')\n    parser.add_argument(""--ext"", type=str, default=\'avi\', choices=[\'avi\',\'mp4\'],\n                        help=\'video file extensions\')\n\n    args = parser.parse_args()\n\n    out_path = args.out_dir\n    src_path = args.src_dir\n    num_worker = args.num_worker\n    df_path = args.df_path\n    out_format = args.out_format\n    ext = args.ext\n    new_size = (args.new_width, args.new_height)\n    NUM_GPU = args.num_gpu\n\n    if not os.path.isdir(out_path):\n        print(""creating folder: ""+out_path)\n        os.makedirs(out_path)\n\n    vid_list = glob.glob(src_path+\'/*/*.\'+ext)\n    print(len(vid_list))\n    pool = Pool(num_worker)\n    pool.map(run_optical_flow, zip(vid_list, xrange(len(vid_list))))\n\n'"
datasets/ucf101.py,1,"b'import torch.utils.data as data\n\nimport os\nimport sys\nimport random\nimport numpy as np\nimport cv2\n\n\ndef find_classes(dir):\n    classes = [d for d in os.listdir(dir) if os.path.isdir(os.path.join(dir, d))]\n    classes.sort()\n    class_to_idx = {classes[i]: i for i in range(len(classes))}\n    return classes, class_to_idx\n\ndef make_dataset(root, source):\n\n    if not os.path.exists(source):\n        print(""Setting file %s for ucf101 dataset doesn\'t exist."" % (source))\n        sys.exit()\n    else:\n        clips = []\n        with open(source) as split_f:\n            data = split_f.readlines()\n            for line in data:\n                line_info = line.split()\n                clip_path = os.path.join(root, line_info[0])\n                duration = int(line_info[1])\n                target = int(line_info[2])\n                item = (clip_path, duration, target)\n                clips.append(item)\n    return clips\n\ndef ReadSegmentRGB(path, offsets, new_height, new_width, new_length, is_color, name_pattern):\n    if is_color:\n        cv_read_flag = cv2.IMREAD_COLOR         # > 0\n    else:\n        cv_read_flag = cv2.IMREAD_GRAYSCALE     # = 0\n    interpolation = cv2.INTER_LINEAR\n\n    sampled_list = []\n    for offset_id in range(len(offsets)):\n        offset = offsets[offset_id]\n        for length_id in range(1, new_length+1):\n            frame_name = name_pattern % (length_id + offset)\n            frame_path = path + ""/"" + frame_name\n            cv_img_origin = cv2.imread(frame_path, cv_read_flag)\n            if cv_img_origin is None:\n               print(""Could not load file %s"" % (frame_path))\n               sys.exit()\n               # TODO: error handling here\n            if new_width > 0 and new_height > 0:\n                # use OpenCV3, use OpenCV2.4.13 may have error\n                cv_img = cv2.resize(cv_img_origin, (new_width, new_height), interpolation)\n            else:\n                cv_img = cv_img_origin\n            cv_img = cv2.cvtColor(cv_img, cv2.COLOR_BGR2RGB)\n            sampled_list.append(cv_img)\n    clip_input = np.concatenate(sampled_list, axis=2)\n    return clip_input\n\ndef ReadSegmentFlow(path, offsets, new_height, new_width, new_length, is_color, name_pattern):\n    if is_color:\n        cv_read_flag = cv2.IMREAD_COLOR         # > 0\n    else:\n        cv_read_flag = cv2.IMREAD_GRAYSCALE     # = 0\n    interpolation = cv2.INTER_LINEAR\n\n    sampled_list = []\n    for offset_id in range(len(offsets)):\n        offset = offsets[offset_id]\n        for length_id in range(1, new_length+1):\n            frame_name_x = name_pattern % (""x"", length_id + offset)\n            frame_path_x = path + ""/"" + frame_name_x\n            cv_img_origin_x = cv2.imread(frame_path_x, cv_read_flag)\n            frame_name_y = name_pattern % (""y"", length_id + offset)\n            frame_path_y = path + ""/"" + frame_name_y\n            cv_img_origin_y = cv2.imread(frame_path_y, cv_read_flag)\n            if cv_img_origin_x is None or cv_img_origin_y is None:\n               print(""Could not load file %s or %s"" % (frame_path_x, frame_path_y))\n               sys.exit()\n               # TODO: error handling here\n            if new_width > 0 and new_height > 0:\n                cv_img_x = cv2.resize(cv_img_origin_x, (new_width, new_height), interpolation)\n                cv_img_y = cv2.resize(cv_img_origin_y, (new_width, new_height), interpolation)\n            else:\n                cv_img_x = cv_img_origin_x\n                cv_img_y = cv_img_origin_y\n            sampled_list.append(np.expand_dims(cv_img_x, 2))\n            sampled_list.append(np.expand_dims(cv_img_y, 2))\n\n    clip_input = np.concatenate(sampled_list, axis=2)\n    return clip_input\n\n\nclass ucf101(data.Dataset):\n\n    def __init__(self,\n                 root,\n                 source,\n                 phase,\n                 modality,\n                 name_pattern=None,\n                 is_color=True,\n                 num_segments=1,\n                 new_length=1,\n                 new_width=0,\n                 new_height=0,\n                 transform=None,\n                 target_transform=None,\n                 video_transform=None):\n\n        classes, class_to_idx = find_classes(root)\n        clips = make_dataset(root, source)\n\n        if len(clips) == 0:\n            raise(RuntimeError(""Found 0 video clips in subfolders of: "" + root + ""\\n""\n                               ""Check your data directory.""))\n\n        self.root = root\n        self.source = source\n        self.phase = phase\n        self.modality = modality\n\n        self.classes = classes\n        self.class_to_idx = class_to_idx\n        self.clips = clips\n\n        if name_pattern:\n            self.name_pattern = name_pattern\n        else:\n            if self.modality == ""rgb"":\n                self.name_pattern = ""img_%05d.jpg""\n            elif self.modality == ""flow"":\n                self.name_pattern = ""flow_%s_%05d.jpg""\n\n        self.is_color = is_color\n        self.num_segments = num_segments\n        self.new_length = new_length\n        self.new_width = new_width\n        self.new_height = new_height\n\n        self.transform = transform\n        self.target_transform = target_transform\n        self.video_transform = video_transform\n\n    def __getitem__(self, index):\n        path, duration, target = self.clips[index]\n        average_duration = int(duration / self.num_segments)\n        offsets = []\n        for seg_id in range(self.num_segments):\n            if self.phase == ""train"":\n                if average_duration >= self.new_length:\n                    offset = random.randint(0, average_duration - self.new_length)\n                    # No +1 because randint(a,b) return a random integer N such that a <= N <= b.\n                    offsets.append(offset + seg_id * average_duration)\n                else:\n                    offsets.append(0)\n            elif self.phase == ""val"":\n                if average_duration >= self.new_length:\n                    offsets.append(int((average_duration - self.new_length + 1)/2 + seg_id * average_duration))\n                else:\n                    offsets.append(0)\n            else:\n                print(""Only phase train and val are supported."")\n\n\n        if self.modality == ""rgb"":\n            clip_input = ReadSegmentRGB(path,\n                                        offsets,\n                                        self.new_height,\n                                        self.new_width,\n                                        self.new_length,\n                                        self.is_color,\n                                        self.name_pattern\n                                        )\n        elif self.modality == ""flow"":\n            clip_input = ReadSegmentFlow(path,\n                                        offsets,\n                                        self.new_height,\n                                        self.new_width,\n                                        self.new_length,\n                                        self.is_color,\n                                        self.name_pattern\n                                        )\n        else:\n            print(""No such modality %s"" % (self.modality))\n\n        if self.transform is not None:\n            clip_input = self.transform(clip_input)\n        if self.target_transform is not None:\n            target = self.target_transform(target)\n        if self.video_transform is not None:\n            clip_input = self.video_transform(clip_input)\n\n        return clip_input, target\n\n\n    def __len__(self):\n        return len(self.clips)\n'"
models/__init__.py,0,b'from .rgb_vgg16 import *\nfrom .flow_vgg16 import *\nfrom .rgb_resnet import *\nfrom .flow_resnet import *'
models/flow_resnet.py,11,"b'import torch.nn as nn\nimport torch\nimport math\nimport collections\nimport numpy as np\n\nimport torch.utils.model_zoo as model_zoo\n\n__all__ = [\'ResNet\', \'flow_resnet18\', \'flow_resnet34\', \'flow_resnet50\', \'flow_resnet50_aux\', \'flow_resnet101\',\n           \'flow_resnet152\']\n\n\nmodel_urls = {\n    \'resnet18\': \'https://download.pytorch.org/models/resnet18-5c106cde.pth\',\n    \'resnet34\': \'https://download.pytorch.org/models/resnet34-333f7ec4.pth\',\n    \'resnet50\': \'https://download.pytorch.org/models/resnet50-19c8e357.pth\',\n    \'resnet101\': \'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\',\n    \'resnet152\': \'https://download.pytorch.org/models/resnet152-b121ed2d.pth\',\n}\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    ""3x3 convolution with padding""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                               padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNet(nn.Module):\n\n    def __init__(self, block, layers, num_classes=1000):\n        self.inplanes = 64\n        super(ResNet, self).__init__()\n        self.conv1 = nn.Conv2d(20, 64, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n        self.avgpool = nn.AvgPool2d(7)\n        # self.fc_aux = nn.Linear(512 * block.expansion, 101)\n        self.dp = nn.Dropout(p=0.7)\n        self.fc_action = nn.Linear(512 * block.expansion, num_classes)\n        # self.bn_final = nn.BatchNorm1d(num_classes)\n        # self.fc2 = nn.Linear(num_classes, num_classes)\n        # self.fc_final = nn.Linear(num_classes, 101)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.dp(x)\n        x = self.fc_action(x)\n        # x = self.bn_final(x)\n        # x = self.fc2(x)\n        # x = self.fc_final(x)\n\n        return x\n\ndef change_key_names(old_params, in_channels):\n    new_params = collections.OrderedDict()\n    layer_count = 0\n    allKeyList = old_params.keys()\n    for layer_key in allKeyList:\n        if layer_count >= len(allKeyList)-2:\n            # exclude fc layers\n            continue\n        else:\n            if layer_count == 0:\n                rgb_weight = old_params[layer_key]\n                # print(type(rgb_weight))\n                rgb_weight_mean = torch.mean(rgb_weight, dim=1)\n                # TODO: ugly fix here, why torch.mean() turn tensor to Variable\n                # print(type(rgb_weight_mean))\n                flow_weight = rgb_weight_mean.unsqueeze(1).repeat(1,in_channels,1,1)\n                new_params[layer_key] = flow_weight\n                layer_count += 1\n                # print(layer_key, new_params[layer_key].size(), type(new_params[layer_key]))\n            else:\n                new_params[layer_key] = old_params[layer_key]\n                layer_count += 1\n                # print(layer_key, new_params[layer_key].size(), type(new_params[layer_key]))\n    \n    return new_params\n\ndef flow_resnet18(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-18 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n    if pretrained:\n        # model.load_state_dict(model_zoo.load_url(model_urls[\'resnet18\']))\n        in_channels = 20\n        pretrained_dict = model_zoo.load_url(model_urls[\'resnet18\'])\n        model_dict = model.state_dict()\n\n        new_pretrained_dict = change_key_names(pretrained_dict, in_channels)\n        # 1. filter out unnecessary keys\n        new_pretrained_dict = {k: v for k, v in new_pretrained_dict.items() if k in model_dict}\n        # 2. overwrite entries in the existing state dict\n        model_dict.update(new_pretrained_dict) \n        # 3. load the new state dict\n        model.load_state_dict(model_dict)\n\n    return model\n\ndef flow_resnet34(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-34 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet34\']))\n    return model\n\ndef flow_resnet50(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-50 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n    if pretrained:\n        # model.load_state_dict(model_zoo.load_url(model_urls[\'resnet50\']))\n        pretrained_dict = model_zoo.load_url(model_urls[\'resnet50\'])\n\n        model_dict = model.state_dict()\n\n        # 1. filter out unnecessary keys\n        pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n        # 2. overwrite entries in the existing state dict\n        model_dict.update(pretrained_dict) \n        # 3. load the new state dict\n        model.load_state_dict(model_dict)\n\n    return model\n\ndef flow_resnet50_aux(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-50 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n    if pretrained:\n        # model.load_state_dict(model_zoo.load_url(model_urls[\'resnet50\']))\n        pretrained_dict = model_zoo.load_url(model_urls[\'resnet50\'])\n\n        model_dict = model.state_dict()\n        fc_origin_weight = pretrained_dict[""fc.weight""].data.numpy()\n        fc_origin_bias = pretrained_dict[""fc.bias""].data.numpy()\n\n        # 1. filter out unnecessary keys\n        pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n        # 2. overwrite entries in the existing state dict\n        model_dict.update(pretrained_dict) \n        # print(model_dict)\n        fc_new_weight = model_dict[""fc_aux.weight""].numpy() \n        fc_new_bias = model_dict[""fc_aux.bias""].numpy() \n\n        fc_new_weight[:1000, :] = fc_origin_weight\n        fc_new_bias[:1000] = fc_origin_bias\n\n        model_dict[""fc_aux.weight""] = torch.from_numpy(fc_new_weight)\n        model_dict[""fc_aux.bias""] = torch.from_numpy(fc_new_bias)\n\n        # 3. load the new state dict\n        model.load_state_dict(model_dict)\n\n    return model\n\ndef flow_resnet101(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-101 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet101\']))\n    return model\n\ndef flow_resnet152(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-152 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n    if pretrained:\n        in_channels = 20\n        # model.load_state_dict(model_zoo.load_url(model_urls[\'resnet152\']))\n        pretrained_dict = model_zoo.load_url(model_urls[\'resnet152\'])\n        model_dict = model.state_dict()\n\n        new_pretrained_dict = change_key_names(pretrained_dict, in_channels)\n        # 1. filter out unnecessary keys\n        new_pretrained_dict = {k: v for k, v in new_pretrained_dict.items() if k in model_dict}\n        # 2. overwrite entries in the existing state dict\n        model_dict.update(new_pretrained_dict) \n        # 3. load the new state dict\n        model.load_state_dict(model_dict)\n\n    return model'"
models/flow_vgg16.py,4,"b'import torch.nn as nn\nimport torch.utils.model_zoo as model_zoo\n\nimport math\nimport collections\nimport numpy as np\nimport torch\n\n__all__ = [\'VGG\', \'flow_vgg16\']\n\n\nmodel_urls = {\'vgg16\': \'https://download.pytorch.org/models/vgg16-397923af.pth\'}\n\n\nclass VGG(nn.Module):\n\n    def __init__(self, features, num_classes=1000):\n        super(VGG, self).__init__()\n        self.features = features\n        self.classifier = nn.Sequential(\n            nn.Linear(512 * 7 * 7, 4096),\n            nn.ReLU(True),\n            nn.Dropout(p=0.9),\n            nn.Linear(4096, 4096),\n            nn.ReLU(True),\n            nn.Dropout(p=0.8),\n        )\n        # Change the dropout value to 0.9 and 0.8 for flow model\n        self.fc_action = nn.Linear(4096, num_classes)\n        self._initialize_weights()\n\n    def forward(self, x):\n        x = self.features(x)\n        x = x.view(x.size(0), -1)\n        x = self.classifier(x)\n        x = self.fc_action(x)\n        return x\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n                if m.bias is not None:\n                    m.bias.data.zero_()\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.Linear):\n                n = m.weight.size(1)\n                m.weight.data.normal_(0, 0.01)\n                m.bias.data.zero_()\n\n\ndef make_layers(cfg, batch_norm=False):\n    layers = []\n    in_channels = 20\n    for v in cfg:\n        if v == \'M\':\n            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n        else:\n            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n            if batch_norm:\n                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n            else:\n                layers += [conv2d, nn.ReLU(inplace=True)]\n            in_channels = v\n    return nn.Sequential(*layers)\n\n\ncfg = {\n    \'A\': [64, \'M\', 128, \'M\', 256, 256, \'M\', 512, 512, \'M\', 512, 512, \'M\'],\n    \'B\': [64, 64, \'M\', 128, 128, \'M\', 256, 256, \'M\', 512, 512, \'M\', 512, 512, \'M\'],\n    \'D\': [64, 64, \'M\', 128, 128, \'M\', 256, 256, 256, \'M\', 512, 512, 512, \'M\', 512, 512, 512, \'M\'],\n    \'E\': [64, 64, \'M\', 128, 128, \'M\', 256, 256, 256, 256, \'M\', 512, 512, 512, 512, \'M\', 512, 512, 512, 512, \'M\'],\n}\n\ndef change_key_names(old_params, in_channels):\n    new_params = collections.OrderedDict()\n    layer_count = 0\n    for layer_key in old_params.keys():\n        if layer_count < 26:\n            if layer_count == 0:\n                rgb_weight = old_params[layer_key]\n                rgb_weight_mean = torch.mean(rgb_weight, dim=1)\n                flow_weight = rgb_weight_mean.repeat(1,in_channels,1,1)\n                new_params[layer_key] = flow_weight\n                layer_count += 1\n                # print(layer_key, new_params[layer_key].size())\n            else:\n                new_params[layer_key] = old_params[layer_key]\n                layer_count += 1\n                # print(layer_key, new_params[layer_key].size())\n\n    return new_params\n\ndef flow_vgg16(pretrained=False, **kwargs):\n    """"""VGG 16-layer model (configuration ""D"")\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = VGG(make_layers(cfg[\'D\']), **kwargs)\n    # TODO: hardcoded for now for 10 optical flow images, set it as an argument later \n    in_channels = 20            \n    if pretrained:\n        # model.load_state_dict(model_zoo.load_url(model_urls[\'vgg16\']))\n        pretrained_dict = model_zoo.load_url(model_urls[\'vgg16\'])\n        model_dict = model.state_dict()\n\n        new_pretrained_dict = change_key_names(pretrained_dict, in_channels)\n        # 1. filter out unnecessary keys\n        new_pretrained_dict = {k: v for k, v in new_pretrained_dict.items() if k in model_dict}\n        # 2. overwrite entries in the existing state dict\n        model_dict.update(new_pretrained_dict)\n        # 3. load the new state dict\n        model.load_state_dict(model_dict)\n\n    return model\n\n\n'"
models/rgb_resnet.py,9,"b'import torch.nn as nn\nimport torch\nimport math\nimport torch.utils.model_zoo as model_zoo\n\n\n__all__ = [\'ResNet\', \'rgb_resnet18\', \'rgb_resnet34\', \'rgb_resnet50\', \'rgb_resnet50_aux\', \'rgb_resnet101\',\n           \'rgb_resnet152\']\n\n\nmodel_urls = {\n    \'resnet18\': \'https://download.pytorch.org/models/resnet18-5c106cde.pth\',\n    \'resnet34\': \'https://download.pytorch.org/models/resnet34-333f7ec4.pth\',\n    \'resnet50\': \'https://download.pytorch.org/models/resnet50-19c8e357.pth\',\n    \'resnet101\': \'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\',\n    \'resnet152\': \'https://download.pytorch.org/models/resnet152-b121ed2d.pth\',\n}\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    ""3x3 convolution with padding""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                               padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNet(nn.Module):\n\n    def __init__(self, block, layers, num_classes=1000):\n        self.inplanes = 64\n        super(ResNet, self).__init__()\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n        self.avgpool = nn.AvgPool2d(7)\n        # self.fc_aux = nn.Linear(512 * block.expansion, 101)\n        self.dp = nn.Dropout(p=0.8)\n        self.fc_action = nn.Linear(512 * block.expansion, num_classes)\n        # self.bn_final = nn.BatchNorm1d(num_classes)\n        # self.fc2 = nn.Linear(num_classes, num_classes)\n        # self.fc_final = nn.Linear(num_classes, 101)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.dp(x)\n        x = self.fc_action(x)\n        # x = self.bn_final(x)\n        # x = self.fc2(x)\n        # x = self.fc_final(x)\n\n        return x\n\n\ndef rgb_resnet18(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-18 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet18\']))\n    return model\n\n\ndef rgb_resnet34(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-34 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet34\']))\n    return model\n\n\ndef rgb_resnet50(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-50 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n    if pretrained:\n        # model.load_state_dict(model_zoo.load_url(model_urls[\'resnet50\']))\n        pretrained_dict = model_zoo.load_url(model_urls[\'resnet50\'])\n\n        model_dict = model.state_dict()\n\n        # 1. filter out unnecessary keys\n        pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n        # 2. overwrite entries in the existing state dict\n        model_dict.update(pretrained_dict) \n        # 3. load the new state dict\n        model.load_state_dict(model_dict)\n\n    return model\n\ndef rgb_resnet50_aux(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-50 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n    if pretrained:\n        # model.load_state_dict(model_zoo.load_url(model_urls[\'resnet50\']))\n        pretrained_dict = model_zoo.load_url(model_urls[\'resnet50\'])\n\n        model_dict = model.state_dict()\n        fc_origin_weight = pretrained_dict[""fc.weight""].data.numpy()\n        fc_origin_bias = pretrained_dict[""fc.bias""].data.numpy()\n\n        # 1. filter out unnecessary keys\n        pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n        # 2. overwrite entries in the existing state dict\n        model_dict.update(pretrained_dict) \n        # print(model_dict)\n        fc_new_weight = model_dict[""fc_aux.weight""].numpy() \n        fc_new_bias = model_dict[""fc_aux.bias""].numpy() \n\n        fc_new_weight[:1000, :] = fc_origin_weight\n        fc_new_bias[:1000] = fc_origin_bias\n\n        model_dict[""fc_aux.weight""] = torch.from_numpy(fc_new_weight)\n        model_dict[""fc_aux.bias""] = torch.from_numpy(fc_new_bias)\n\n        # 3. load the new state dict\n        model.load_state_dict(model_dict)\n\n    return model\n\ndef rgb_resnet101(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-101 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet101\']))\n    return model\n\n\ndef rgb_resnet152(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-152 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n    if pretrained:\n        # model.load_state_dict(model_zoo.load_url(model_urls[\'resnet152\']))\n        pretrained_dict = model_zoo.load_url(model_urls[\'resnet152\'])\n        model_dict = model.state_dict()\n\n        # 1. filter out unnecessary keys\n        pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n        # 2. overwrite entries in the existing state dict\n        model_dict.update(pretrained_dict) \n        # 3. load the new state dict\n        model.load_state_dict(model_dict)\n\n    return model'"
models/rgb_vgg16.py,3,"b'import torch.nn as nn\nimport torch.utils.model_zoo as model_zoo\nimport math\n\n\n__all__ = [\'VGG\', \'rgb_vgg16\', \'rgb_vgg16_bn\']\n\n\nmodel_urls = {\'vgg16\': \'https://download.pytorch.org/models/vgg16-397923af.pth\'}\n\n\nclass VGG(nn.Module):\n\n    def __init__(self, features, num_classes=1000):\n        super(VGG, self).__init__()\n        self.features = features\n        self.classifier = nn.Sequential(\n            nn.Linear(512 * 7 * 7, 4096),\n            nn.ReLU(True),\n            nn.Dropout(p=0.9),\n            nn.Linear(4096, 4096),\n            nn.ReLU(True),\n            nn.Dropout(p=0.9),\n        )\n        # Change the dropout value to 0.9 and 0.9 for rgb model\n        self.fc_action = nn.Linear(4096, num_classes)\n        self._initialize_weights()\n\n    def forward(self, x):\n        x = self.features(x)\n        x = x.view(x.size(0), -1)\n        x = self.classifier(x)\n        x = self.fc_action(x)\n        return x\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n                if m.bias is not None:\n                    m.bias.data.zero_()\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.Linear):\n                n = m.weight.size(1)\n                m.weight.data.normal_(0, 0.01)\n                m.bias.data.zero_()\n\n\ndef make_layers(cfg, batch_norm=False):\n    layers = []\n    in_channels = 3\n    for v in cfg:\n        if v == \'M\':\n            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n        else:\n            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n            if batch_norm:\n                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n            else:\n                layers += [conv2d, nn.ReLU(inplace=True)]\n            in_channels = v\n    return nn.Sequential(*layers)\n\n\ncfg = {\n    \'A\': [64, \'M\', 128, \'M\', 256, 256, \'M\', 512, 512, \'M\', 512, 512, \'M\'],\n    \'B\': [64, 64, \'M\', 128, 128, \'M\', 256, 256, \'M\', 512, 512, \'M\', 512, 512, \'M\'],\n    \'D\': [64, 64, \'M\', 128, 128, \'M\', 256, 256, 256, \'M\', 512, 512, 512, \'M\', 512, 512, 512, \'M\'],\n    \'E\': [64, 64, \'M\', 128, 128, \'M\', 256, 256, 256, 256, \'M\', 512, 512, 512, 512, \'M\', 512, 512, 512, 512, \'M\'],\n}\n\n\ndef rgb_vgg16(pretrained=False, **kwargs):\n    """"""VGG 16-layer model (configuration ""D"")\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = VGG(make_layers(cfg[\'D\']), **kwargs)\n    if pretrained:\n        # model.load_state_dict(model_zoo.load_url(model_urls[\'vgg16\']))\n        pretrained_dict = model_zoo.load_url(model_urls[\'vgg16\'])\n        model_dict = model.state_dict()\n\n        # 1. filter out unnecessary keys\n        pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n        # 2. overwrite entries in the existing state dict\n        model_dict.update(pretrained_dict) \n        # 3. load the new state dict\n        model.load_state_dict(model_dict)\n\n    return model\n\n\ndef rgb_vgg16_bn(**kwargs):\n    """"""VGG 16-layer model (configuration ""D"") with batch normalization\n       No pretrained model available.\n    """"""\n    return VGG(make_layers(cfg[\'D\'], batch_norm=True), **kwargs)\n\n'"
scripts/eval_ucf101_pytorch/VideoSpatialPrediction.py,7,"b'\'\'\'\nA sample function for classification using spatial network\nCustomize as needed:\ne.g. num_categories, layer for feature extraction, batch_size\n\'\'\'\n\nimport os\nimport sys\nimport numpy as np\nimport math\nimport cv2\nimport scipy.io as sio\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.parallel\nimport torch.backends.cudnn as cudnn\nimport torch.optim\nimport torch.utils.data\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\nimport torchvision.models as models\n\nsys.path.insert(0, ""../../"")\nimport video_transforms\n\ndef VideoSpatialPrediction(\n        vid_name,\n        net,\n        num_categories,\n        start_frame=0,\n        num_frames=0,\n        num_samples=25\n        ):\n\n    if num_frames == 0:\n        imglist = os.listdir(vid_name)\n        duration = len(imglist)\n        # print(duration)\n    else:\n        duration = num_frames\n\n    clip_mean = [0.485, 0.456, 0.406]\n    clip_std = [0.229, 0.224, 0.225]\n    normalize = video_transforms.Normalize(mean=clip_mean,\n                                     std=clip_std)\n    val_transform = video_transforms.Compose([\n            video_transforms.ToTensor(),\n            normalize,\n        ])\n\n    # selection\n    step = int(math.floor((duration-1)/(num_samples-1)))\n    dims = (256,340,3,num_samples)\n    rgb = np.zeros(shape=dims, dtype=np.float64)\n    rgb_flip = np.zeros(shape=dims, dtype=np.float64)\n\n    for i in range(num_samples):\n        img_file = os.path.join(vid_name, \'image_{0:04d}.jpg\'.format(i*step+1))\n        img = cv2.imread(img_file, cv2.IMREAD_UNCHANGED)\n        img = cv2.resize(img, dims[1::-1])\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        rgb[:,:,:,i] = img\n        rgb_flip[:,:,:,i] = img[:,::-1,:]\n\n    # crop\n    rgb_1 = rgb[:224, :224, :,:]\n    rgb_2 = rgb[:224, -224:, :,:]\n    rgb_3 = rgb[16:240, 60:284, :,:]\n    rgb_4 = rgb[-224:, :224, :,:]\n    rgb_5 = rgb[-224:, -224:, :,:]\n    rgb_f_1 = rgb_flip[:224, :224, :,:]\n    rgb_f_2 = rgb_flip[:224, -224:, :,:]\n    rgb_f_3 = rgb_flip[16:240, 60:284, :,:]\n    rgb_f_4 = rgb_flip[-224:, :224, :,:]\n    rgb_f_5 = rgb_flip[-224:, -224:, :,:]\n\n    rgb = np.concatenate((rgb_1,rgb_2,rgb_3,rgb_4,rgb_5,rgb_f_1,rgb_f_2,rgb_f_3,rgb_f_4,rgb_f_5), axis=3)\n\n    _, _, _, c = rgb.shape\n    rgb_list = []\n    for c_index in range(c):\n        cur_img = rgb[:,:,:,c_index].squeeze()\n        cur_img_tensor = val_transform(cur_img)\n        rgb_list.append(np.expand_dims(cur_img_tensor.numpy(), 0))\n        \n    rgb_np = np.concatenate(rgb_list,axis=0)\n    # print(rgb_np.shape)\n    batch_size = 25\n    prediction = np.zeros((num_categories,rgb.shape[3]))\n    num_batches = int(math.ceil(float(rgb.shape[3])/batch_size))\n\n    for bb in range(num_batches):\n        span = range(batch_size*bb, min(rgb.shape[3],batch_size*(bb+1)))\n        input_data = rgb_np[span,:,:,:]\n        imgDataTensor = torch.from_numpy(input_data).type(torch.FloatTensor).cuda()\n        imgDataVar = torch.autograd.Variable(imgDataTensor)\n        output = net(imgDataVar)\n        result = output.data.cpu().numpy()\n        prediction[:, span] = np.transpose(result)\n\n    return prediction\n'"
scripts/eval_ucf101_pytorch/VideoTemporalPrediction.py,7,"b'\'\'\'\nA sample function for classification using temporal network\nCustomize as needed:\ne.g. num_categories, layer for feature extraction, batch_size\n\'\'\'\n\nimport glob\nimport os\nimport sys\nimport numpy as np\nimport math\nimport cv2\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.parallel\nimport torch.backends.cudnn as cudnn\nimport torch.optim\nimport torch.utils.data\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\nimport torchvision.models as models\n\nsys.path.insert(0, ""../../"")\nimport video_transforms\n\ndef VideoTemporalPrediction(\n        vid_name,\n        net,\n        num_categories,\n        start_frame=0,\n        num_frames=0,\n        num_samples=25,\n        optical_flow_frames=10\n        ):\n\n    if num_frames == 0:\n        # print(vid_name)\n        imglist = glob.glob(os.path.join(vid_name, \'*flow_x*.jpg\'))\n        duration = len(imglist)\n    else:\n        duration = num_frames\n\n    clip_mean = [0.5] * 20\n    clip_std = [0.226] * 20\n    normalize = video_transforms.Normalize(mean=clip_mean,\n                                     std=clip_std)\n    val_transform = video_transforms.Compose([\n            video_transforms.ToTensor(),\n            normalize,\n        ])\n\n    # selection\n    step = int(math.floor((duration-optical_flow_frames+1)/num_samples))\n    dims = (256,340,optical_flow_frames*2,num_samples)\n    flow = np.zeros(shape=dims, dtype=np.float64)\n    flow_flip = np.zeros(shape=dims, dtype=np.float64)\n\n    for i in range(num_samples):\n        for j in range(optical_flow_frames):\n            flow_x_file = os.path.join(vid_name, \'flow_x_{0:04d}.jpg\'.format(i*step+j+1 + start_frame))\n            flow_y_file = os.path.join(vid_name, \'flow_y_{0:04d}.jpg\'.format(i*step+j+1 + start_frame))\n            img_x = cv2.imread(flow_x_file, cv2.IMREAD_GRAYSCALE)\n            img_y = cv2.imread(flow_y_file, cv2.IMREAD_GRAYSCALE)\n            img_x = cv2.resize(img_x, dims[1::-1])\n            img_y = cv2.resize(img_y, dims[1::-1])\n\n            flow[:,:,j*2  ,i] = img_x\n            flow[:,:,j*2+1,i] = img_y\n\n            flow_flip[:,:,j*2  ,i] = 255 - img_x[:, ::-1]\n            flow_flip[:,:,j*2+1,i] = img_y[:, ::-1]\n\n    # crop\n    flow_1 = flow[:224, :224, :,:]\n    flow_2 = flow[:224, -224:, :,:]\n    flow_3 = flow[16:240, 60:284, :,:]\n    flow_4 = flow[-224:, :224, :,:]\n    flow_5 = flow[-224:, -224:, :,:]\n    flow_f_1 = flow_flip[:224, :224, :,:]\n    flow_f_2 = flow_flip[:224, -224:, :,:]\n    flow_f_3 = flow_flip[16:240, 60:284, :,:]\n    flow_f_4 = flow_flip[-224:, :224, :,:]\n    flow_f_5 = flow_flip[-224:, -224:, :,:]\n\n    flow = np.concatenate((flow_1,flow_2,flow_3,flow_4,flow_5,flow_f_1,flow_f_2,flow_f_3,flow_f_4,flow_f_5), axis=3)\n    \n    _, _, _, c = flow.shape\n    flow_list = []\n    for c_index in range(c):\n        cur_img = flow[:,:,:,c_index].squeeze()\n        cur_img_tensor = val_transform(cur_img)\n        flow_list.append(np.expand_dims(cur_img_tensor.numpy(), 0))\n        \n    flow_np = np.concatenate(flow_list,axis=0)\n\n    batch_size = 25\n    prediction = np.zeros((num_categories,flow.shape[3]))\n    num_batches = int(math.ceil(float(flow.shape[3])/batch_size))\n\n    for bb in range(num_batches):\n        span = range(batch_size*bb, min(flow.shape[3],batch_size*(bb+1)))\n\n        input_data = flow_np[span,:,:,:]\n        imgDataTensor = torch.from_numpy(input_data).type(torch.FloatTensor).cuda()\n        imgDataVar = torch.autograd.Variable(imgDataTensor)\n        output = net(imgDataVar)\n        result = output.data.cpu().numpy()\n        prediction[:, span] = np.transpose(result)\n\n    return prediction\n'"
scripts/eval_ucf101_pytorch/spatial_demo.py,6,"b'#!/usr/bin/env python\n\nimport os, sys\nimport collections\nimport numpy as np\nimport cv2\nimport math\nimport random\nimport time\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.parallel\nimport torch.backends.cudnn as cudnn\nimport torch.optim\nimport torch.utils.data\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\n\nsys.path.insert(0, ""../../"")\nimport models\nfrom VideoSpatialPrediction import VideoSpatialPrediction\n\nos.environ[""CUDA_DEVICE_ORDER""]=""PCI_BUS_ID""   \nos.environ[""CUDA_VISIBLE_DEVICES""]=""0""\n\ndef softmax(x):\n    y = [math.exp(k) for k in x]\n    sum_y = math.fsum(y)\n    z = [k/sum_y for k in y]\n\n    return z\n\ndef main():\n\n    model_path = \'../../checkpoints/model_best.pth.tar\'\n    data_dir = ""~/UCF101/frames""\n    start_frame = 0\n    num_categories = 101\n\n    model_start_time = time.time()\n    params = torch.load(model_path)\n\n    spatial_net = models.rgb_resnet152(pretrained=False, num_classes=101)\n    spatial_net.load_state_dict(params[\'state_dict\'])\n    spatial_net.cuda()\n    spatial_net.eval()\n    model_end_time = time.time()\n    model_time = model_end_time - model_start_time\n    print(""Action recognition model is loaded in %4.4f seconds."" % (model_time))\n\n    val_file = ""./testlist01_with_labels.txt""\n    f_val = open(val_file, ""r"")\n    val_list = f_val.readlines()\n    print(""we got %d test videos"" % len(val_list))\n\n    line_id = 1\n    match_count = 0\n    result_list = []\n    for line in val_list:\n        line_info = line.split("" "")\n        clip_path = line_info[0]\n        input_video_label = int(line_info[1]) - 1\n\n        spatial_prediction = VideoSpatialPrediction(\n                clip_path,\n                spatial_net,\n                num_categories,\n                start_frame)\n\n        avg_spatial_pred_fc8 = np.mean(spatial_prediction, axis=1)\n        # print(avg_spatial_pred_fc8.shape)\n        result_list.append(avg_spatial_pred_fc8)\n        # avg_spatial_pred = softmax(avg_spatial_pred_fc8)\n\n        pred_index = np.argmax(avg_spatial_pred_fc8)\n        print(""Sample %d/%d: GT: %d, Prediction: %d"" % (line_id, len(val_list), input_video_label, pred_index))\n\n        if pred_index == input_video_label:\n            match_count += 1\n        line_id += 1\n\n    print(match_count)\n    print(len(val_list))\n    print(""Accuracy is %4.4f"" % (float(match_count)/len(val_list)))\n    np.save(""ucf101_s1_rgb_resnet152.npy"", np.array(result_list))\n\nif __name__ == ""__main__"":\n    main()\n\n\n\n\n    # # spatial net prediction\n    # class_list = os.listdir(data_dir)\n    # class_list.sort()\n    # print(class_list)\n\n    # class_index = 0\n    # match_count = 0\n    # total_clip = 1\n    # result_list = []\n\n    # for each_class in class_list:\n    #     class_path = os.path.join(data_dir, each_class)\n\n    #     clip_list = os.listdir(class_path)\n    #     clip_list.sort()\n\n    #     for each_clip in clip_list:\n            # clip_path = os.path.join(class_path, each_clip)\n            # spatial_prediction = VideoSpatialPrediction(\n            #         clip_path,\n            #         spatial_net,\n            #         num_categories,\n            #         start_frame)\n\n            # avg_spatial_pred_fc8 = np.mean(spatial_prediction, axis=1)\n            # # print(avg_spatial_pred_fc8.shape)\n            # result_list.append(avg_spatial_pred_fc8)\n            # # avg_spatial_pred = softmax(avg_spatial_pred_fc8)\n\n            # pred_index = np.argmax(avg_spatial_pred_fc8)\n            # print(""GT: %d, Prediction: %d"" % (class_index, pred_index))\n\n            # if pred_index == class_index:\n            #     match_count += 1\n#             total_clip += 1\n\n#         class_index += 1\n\n#     print(""Accuracy is %4.4f"" % (float(match_count)/total_clip))\n#     np.save(""ucf101_split1_resnet_rgb.npy"", np.array(result_list))\n\n# if __name__ == ""__main__"":\n#     main()\n'"
scripts/eval_ucf101_pytorch/temporal_demo.py,6,"b'#!/usr/bin/env python\n\nimport os\nimport sys\nimport collections\nimport numpy as np\nimport cv2\nimport math\nimport random\nimport time\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.parallel\nimport torch.backends.cudnn as cudnn\nimport torch.optim\nimport torch.utils.data\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\n\nsys.path.insert(0, ""../../"")\nimport models\n\nfrom VideoTemporalPrediction import VideoTemporalPrediction\n\nos.environ[""CUDA_DEVICE_ORDER""]=""PCI_BUS_ID""   \nos.environ[""CUDA_VISIBLE_DEVICES""]=""0""\n\ndef softmax(x):\n    y = [math.exp(k) for k in x]\n    sum_y = math.fsum(y)\n    z = [k/sum_y for k in y]\n\n    return z\n\ndef main():\n\n    model_path = \'../../checkpoints/model_best.pth.tar\'\n    data_dir = ""~/UCF101/ucf101_flow_img_tvl1_gpu""\n    start_frame = 0\n    num_categories = 101\n\n    model_start_time = time.time()\n    params = torch.load(model_path)\n    temporal_net = models.flow_resnet152(pretrained=False, num_classes=101)\n    temporal_net.load_state_dict(params[\'state_dict\'])\n    temporal_net.cuda()\n    temporal_net.eval()\n    model_end_time = time.time()\n    model_time = model_end_time - model_start_time\n    print(""Action recognition temporal model is loaded in %4.4f seconds."" % (model_time))\n\n    val_file = ""./temporal_testlist01_with_labels.txt""\n    f_val = open(val_file, ""r"")\n    val_list = f_val.readlines()\n    print(""we got %d test videos"" % len(val_list))\n\n    line_id = 1\n    match_count = 0\n    result_list = []\n\n    for line in val_list:\n        line_info = line.split("" "")\n        clip_path = line_info[0]\n        input_video_label = int(line_info[1]) - 1\n\n        spatial_prediction = VideoTemporalPrediction(\n                clip_path,\n                temporal_net,\n                num_categories,\n                start_frame)\n\n        avg_spatial_pred_fc8 = np.mean(spatial_prediction, axis=1)\n        # print(avg_spatial_pred_fc8.shape)\n        result_list.append(avg_spatial_pred_fc8)\n        # avg_spatial_pred = softmax(avg_spatial_pred_fc8)\n\n        pred_index = np.argmax(avg_spatial_pred_fc8)\n        print(""Sample %d/%d: GT: %d, Prediction: %d"" % (line_id, len(val_list), input_video_label, pred_index))\n\n        if pred_index == input_video_label:\n            match_count += 1\n        line_id += 1\n\n    print(match_count)\n    print(len(val_list))\n    print(""Accuracy is %4.4f"" % (float(match_count)/len(val_list)))\n    np.save(""ucf101_s1_flow_resnet152.npy"", np.array(result_list))\n\nif __name__ == ""__main__"":\n    main()\n\n\n\n\n    # # spatial net prediction\n    # class_list = os.listdir(data_dir)\n    # class_list.sort()\n    # print(class_list)\n\n    # class_index = 0\n    # match_count = 0\n    # total_clip = 1\n    # result_list = []\n\n    # for each_class in class_list:\n    #     class_path = os.path.join(data_dir, each_class)\n\n    #     clip_list = os.listdir(class_path)\n    #     clip_list.sort()\n\n    #     for each_clip in clip_list:\n            # clip_path = os.path.join(class_path, each_clip)\n            # spatial_prediction = VideoSpatialPrediction(\n            #         clip_path,\n            #         spatial_net,\n            #         num_categories,\n            #         start_frame)\n\n            # avg_spatial_pred_fc8 = np.mean(spatial_prediction, axis=1)\n            # # print(avg_spatial_pred_fc8.shape)\n            # result_list.append(avg_spatial_pred_fc8)\n            # # avg_spatial_pred = softmax(avg_spatial_pred_fc8)\n\n            # pred_index = np.argmax(avg_spatial_pred_fc8)\n            # print(""GT: %d, Prediction: %d"" % (class_index, pred_index))\n\n            # if pred_index == class_index:\n            #     match_count += 1\n#             total_clip += 1\n\n#         class_index += 1\n\n#     print(""Accuracy is %4.4f"" % (float(match_count)/total_clip))\n#     np.save(""ucf101_split1_resnet_rgb.npy"", np.array(result_list))\n\n# if __name__ == ""__main__"":\n#     main()\n'"
