file_path,api_count,code
pytorch/BiLSTM_CRF.py,16,"b'import torch\nimport torch.autograd as autograd\nimport torch.nn as nn\nimport torch.optim as optim\n\ntorch.manual_seed(1)\nSTART_TAG = ""<START>""\nSTOP_TAG = ""<STOP>""\ndef argmax(vec):\n    # return the argmax as a python int\n    _, idx = torch.max(vec, 1)\n    return idx.item()\n\n\ndef prepare_sequence(seq, to_ix):\n    idxs = [to_ix[w] for w in seq]\n    return torch.tensor(idxs, dtype=torch.long)\n\n\n# Compute log sum exp in a numerically stable way for the forward algorithm\ndef log_sum_exp(vec):\n    max_score = vec[0, argmax(vec)]\n    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n    return max_score + \\\n        torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\n\nclass BiLSTM_CRF(nn.Module):\n\n    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim):\n        super(BiLSTM_CRF, self).__init__()\n        self.embedding_dim = embedding_dim\n        self.hidden_dim = hidden_dim\n        self.vocab_size = vocab_size\n        self.tag_to_ix = tag_to_ix\n        self.tagset_size = len(tag_to_ix)\n\n        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n                            num_layers=1, bidirectional=True)\n\n        # Maps the output of the LSTM into tag space.\n        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n\n        # Matrix of transition parameters.  Entry i,j is the score of\n        # transitioning *to* i *from* j.\n        self.transitions = nn.Parameter(\n            torch.randn(self.tagset_size, self.tagset_size))\n\n        # These two statements enforce the constraint that we never transfer\n        # to the start tag and we never transfer from the stop tag\n        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n\n        self.hidden = self.init_hidden()\n\n    def init_hidden(self):\n        return (torch.randn(2, 1, self.hidden_dim // 2),\n                torch.randn(2, 1, self.hidden_dim // 2))\n\n    def _forward_alg(self, feats):\n        # Do the forward algorithm to compute the partition function\n        init_alphas = torch.full((1, self.tagset_size), -10000.)\n        # START_TAG has all of the score.\n        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n\n        # Wrap in a variable so that we will get automatic backprop\n        forward_var = init_alphas\n\n        # Iterate through the sentence\n        for feat in feats:\n            alphas_t = []  # The forward tensors at this timestep\n            for next_tag in range(self.tagset_size):\n                # broadcast the emission score: it is the same regardless of\n                # the previous tag\n                emit_score = feat[next_tag].view(\n                    1, -1).expand(1, self.tagset_size)\n                # the ith entry of trans_score is the score of transitioning to\n                # next_tag from i\n                trans_score = self.transitions[next_tag].view(1, -1)\n                # The ith entry of next_tag_var is the value for the\n                # edge (i -> next_tag) before we do log-sum-exp\n                next_tag_var = forward_var + trans_score + emit_score\n                # The forward variable for this tag is log-sum-exp of all the\n                # scores.\n                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n            forward_var = torch.cat(alphas_t).view(1, -1)\n        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n        alpha = log_sum_exp(terminal_var)\n        return alpha\n\n    def _get_lstm_features(self, sentence):\n        self.hidden = self.init_hidden()\n        embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)\n        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n        lstm_feats = self.hidden2tag(lstm_out)\n        return lstm_feats\n\n    def _score_sentence(self, feats, tags):\n        # Gives the score of a provided tag sequence\n        score = torch.zeros(1)\n        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long), tags])\n        for i, feat in enumerate(feats):\n            score = score + \\\n                self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n        return score\n\n    def _viterbi_decode(self, feats):\n        backpointers = []\n\n        # Initialize the viterbi variables in log space\n        init_vvars = torch.full((1, self.tagset_size), -10000.)\n        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n\n        # forward_var at step i holds the viterbi variables for step i-1\n        forward_var = init_vvars\n        for feat in feats:\n            bptrs_t = []  # holds the backpointers for this step\n            viterbivars_t = []  # holds the viterbi variables for this step\n\n            for next_tag in range(self.tagset_size):\n                # next_tag_var[i] holds the viterbi variable for tag i at the\n                # previous step, plus the score of transitioning\n                # from tag i to next_tag.\n                # We don\'t include the emission scores here because the max\n                # does not depend on them (we add them in below)\n                next_tag_var = forward_var + self.transitions[next_tag]\n                best_tag_id = argmax(next_tag_var)\n                bptrs_t.append(best_tag_id)\n                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n            # Now add in the emission scores, and assign forward_var to the set\n            # of viterbi variables we just computed\n            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n            backpointers.append(bptrs_t)\n\n        # Transition to STOP_TAG\n        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n        best_tag_id = argmax(terminal_var)\n        path_score = terminal_var[0][best_tag_id]\n\n        # Follow the back pointers to decode the best path.\n        best_path = [best_tag_id]\n        for bptrs_t in reversed(backpointers):\n            best_tag_id = bptrs_t[best_tag_id]\n            best_path.append(best_tag_id)\n        # Pop off the start tag (we dont want to return that to the caller)\n        start = best_path.pop()\n        assert start == self.tag_to_ix[START_TAG]  # Sanity check\n        best_path.reverse()\n        return path_score, best_path\n\n    def neg_log_likelihood(self, sentence, tags):\n        feats = self._get_lstm_features(sentence)\n        forward_score = self._forward_alg(feats)\n        gold_score = self._score_sentence(feats, tags)\n        return forward_score - gold_score\n\n    def forward(self, sentence):  # dont confuse this with _forward_alg above.\n        # Get the emission scores from the BiLSTM\n        lstm_feats = self._get_lstm_features(sentence)\n\n        # Find the best path, given the features.\n        score, tag_seq = self._viterbi_decode(lstm_feats)\n        return score, tag_seq\n'"
pytorch/resultCal.py,0,"b'# coding=utf-8\nimport codecs\ndef calculate(x,y,id2word,id2tag,res=[]):\n    entity=[]\n    for j in range(len(x)):\n        if x[j]==0 or y[j]==0:\n            continue\n        if id2tag[y[j]][0]==\'B\':\n            entity=[id2word[x[j]]+\'/\'+id2tag[y[j]]]\n        elif id2tag[y[j]][0]==\'M\' and len(entity)!=0 and entity[-1].split(\'/\')[1][1:]==id2tag[y[j]][1:]:\n            entity.append(id2word[x[j]]+\'/\'+id2tag[y[j]])\n        elif id2tag[y[j]][0]==\'E\' and len(entity)!=0 and entity[-1].split(\'/\')[1][1:]==id2tag[y[j]][1:]:\n            entity.append(id2word[x[j]]+\'/\'+id2tag[y[j]])\n            entity.append(str(j))\n            res.append(entity)\n            entity=[]\n        else:\n            entity=[]\n    return res\n    \n    \ndef calculate3(x,y,id2word,id2tag,res=[]):\n    \'\'\'\n    \xe4\xbd\xbf\xe7\x94\xa8\xe8\xbf\x99\xe4\xb8\xaa\xe5\x87\xbd\xe6\x95\xb0\xe5\x8f\xaf\xe4\xbb\xa5\xe6\x8a\x8a\xe6\x8a\xbd\xe5\x8f\x96\xe5\x87\xba\xe7\x9a\x84\xe5\xae\x9e\xe4\xbd\x93\xe5\x86\x99\xe5\x88\xb0res.txt\xe6\x96\x87\xe4\xbb\xb6\xe4\xb8\xad\xef\xbc\x8c\xe4\xbe\x9b\xe6\x88\x91\xe4\xbb\xac\xe6\x9f\xa5\xe7\x9c\x8b\xe3\x80\x82\n    \xe6\xb3\xa8\xe6\x84\x8f\xef\xbc\x8c\xe8\xbf\x99\xe4\xb8\xaa\xe5\x87\xbd\xe6\x95\xb0\xe6\xaf\x8f\xe6\xac\xa1\xe4\xbd\xbf\xe7\x94\xa8\xe6\x98\xaf\xe5\x9c\xa8\xe6\x96\x87\xe6\xa1\xa3\xe7\x9a\x84\xe6\x9c\x80\xe5\x90\x8e\xe6\xb7\xbb\xe5\x8a\xa0\xe6\x96\xb0\xe4\xbf\xa1\xe6\x81\xaf\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe4\xbd\xbf\xe7\x94\xa8\xe6\x97\xb6\xe5\xb0\xbd\xe9\x87\x8f\xe5\x88\xa0\xe9\x99\xa4res\xe6\x96\x87\xe4\xbb\xb6\xe5\x90\x8e\xe4\xbd\xbf\xe7\x94\xa8\xe3\x80\x82\n    \'\'\'\n    with codecs.open(\'./res.txt\',\'a\',\'utf-8\') as outp:\n        entity=[]\n        for j in range(len(x)): #for every word\n            if x[j]==0 or y[j]==0:\n                continue\n            if id2tag[y[j]][0]==\'B\':\n                entity=[id2word[x[j]]+\'/\'+id2tag[y[j]]]\n            elif id2tag[y[j]][0]==\'M\' and len(entity)!=0 and entity[-1].split(\'/\')[1][1:]==id2tag[y[j]][1:]:\n                entity.append(id2word[x[j]]+\'/\'+id2tag[y[j]])\n            elif id2tag[y[j]][0]==\'E\' and len(entity)!=0 and entity[-1].split(\'/\')[1][1:]==id2tag[y[j]][1:]:\n                entity.append(id2word[x[j]]+\'/\'+id2tag[y[j]])\n                entity.append(str(j))\n                res.append(entity)\n                st = """"\n                for s in entity:\n                    st += s+\' \'\n                #print st\n                outp.write(st+\'\\n\')\n                entity=[]\n            else:\n                entity=[]\n    return res\n'"
pytorch/train.py,7,"b'# coding=utf-8\nimport pickle\nimport pdb\nwith open(\'../data/Bosondata.pkl\', \'rb\') as inp:\n\tword2id = pickle.load(inp)\n\tid2word = pickle.load(inp)\n\ttag2id = pickle.load(inp)\n\tid2tag = pickle.load(inp)\n\tx_train = pickle.load(inp)\n\ty_train = pickle.load(inp)\n\tx_test = pickle.load(inp)\n\ty_test = pickle.load(inp)\n\tx_valid = pickle.load(inp)\n\ty_valid = pickle.load(inp)\nprint ""train len:"",len(x_train)\nprint ""test len:"",len(x_test)\nprint ""valid len"", len(x_valid)\n\nimport torch\nimport torch.autograd as autograd\nimport torch.nn as nn\nimport torch.optim as optim\nimport codecs \nfrom BiLSTM_CRF import BiLSTM_CRF\nfrom resultCal import calculate\n\n#############\nSTART_TAG = ""<START>""\nSTOP_TAG = ""<STOP>""\nEMBEDDING_DIM = 100\nHIDDEN_DIM = 200\nEPOCHS = 5\n\ntag2id[START_TAG]=len(tag2id)\ntag2id[STOP_TAG]=len(tag2id)\n\n\nmodel = BiLSTM_CRF(len(word2id)+1, tag2id, EMBEDDING_DIM, HIDDEN_DIM)\n\noptimizer = optim.SGD(model.parameters(), lr=0.005, weight_decay=1e-4)\n\n    \n    \nfor epoch in range(EPOCHS):\n    index=0\n    for sentence, tags in zip(x_train,y_train):\n        index+=1\n        model.zero_grad()\n\n        sentence=torch.tensor(sentence, dtype=torch.long)\n        tags = torch.tensor([tag2id[t] for t in tags], dtype=torch.long)\n\n        loss = model.neg_log_likelihood(sentence, tags)\n\n        loss.backward()\n        optimizer.step()\n        if index%300==0:\n            print ""epoch"",epoch,""index"",index\n    entityres=[]\n    entityall=[]\n    for sentence, tags in zip(x_test,y_test):\n        sentence=torch.tensor(sentence, dtype=torch.long)\n        score,predict = model(sentence)\n        entityres = calculate(sentence,predict,id2word,id2tag,entityres)\n        entityall = calculate(sentence,tags,id2word,id2tag,entityall)\n    jiaoji = [i for i in entityres if i in entityall]\n    if len(jiaoji)!=0:\n        zhun = float(len(jiaoji))/len(entityres)\n        zhao = float(len(jiaoji))/len(entityall)\n        print ""test:""\n        print ""zhun:"", zhun\n        print ""zhao:"", zhao\n        print ""f:"", (2*zhun*zhao)/(zhun+zhao)\n    else:\n        print ""zhun:"",0\n    \n    path_name = ""./model/model""+str(epoch)+"".pkl""\n    print path_name\n    torch.save(model, path_name)\n    print ""model has been saved""\n\n'"
tensorflow/Batch.py,0,"b'import numpy as np\nclass BatchGenerator(object):\n    """""" Construct a Data generator. The input X, y should be ndarray or list like type.\n    \n    Example:\n        Data_train = BatchGenerator(X=X_train_all, y=y_train_all, shuffle=False)\n        Data_test = BatchGenerator(X=X_test_all, y=y_test_all, shuffle=False)\n        X = Data_train.X\n        y = Data_train.y\n        or:\n        X_batch, y_batch = Data_train.next_batch(batch_size)\n     """""" \n    \n    def __init__(self, X, y, shuffle=False):\n        if type(X) != np.ndarray:\n            X = np.asarray(X)\n        if type(y) != np.ndarray:\n            y = np.asarray(y)\n        self._X = X\n        self._y = y\n        self._epochs_completed = 0\n        self._index_in_epoch = 0\n        self._number_examples = self._X.shape[0]\n        self._shuffle = shuffle\n        if self._shuffle:\n            new_index = np.random.permutation(self._number_examples)\n            self._X = self._X[new_index]\n            self._y = self._y[new_index]\n                \n    @property\n    def x(self):\n        return self._X\n    \n    @property\n    def y(self):\n        return self._y\n    \n    @property\n    def num_examples(self):\n        return self._number_examples\n    \n    @property\n    def epochs_completed(self):\n        return self._epochs_completed\n    \n    def next_batch(self, batch_size):\n        """""" Return the next \'batch_size\' examples from this data set.""""""\n        start = self._index_in_epoch\n        self._index_in_epoch += batch_size\n        if self._index_in_epoch > self._number_examples:\n            # finished epoch\n            self._epochs_completed += 1\n            # Shuffle the data \n            if self._shuffle:\n                new_index = np.random.permutation(self._number_examples)\n                self._X = self._X[new_index]\n                self._y = self._y[new_index]\n            start = 0\n            self._index_in_epoch = batch_size\n            assert batch_size <= self._number_examples\n        end = self._index_in_epoch\n        return self._X[start:end], self._y[start:end]\n\n'"
tensorflow/bilstm_crf.py,0,"b'# -*- coding: utf-8 -*\nimport numpy as np\nimport tensorflow as tf\n\nclass Model:\n    def __init__(self,config,embedding_pretrained,dropout_keep=1):\n        self.lr = config[""lr""]\n        self.batch_size = config[""batch_size""]\n        self.embedding_size = config[""embedding_size""]\n        self.embedding_dim = config[""embedding_dim""] \n        self.sen_len = config[""sen_len""]\n        self.tag_size = config[""tag_size""]\n        self.pretrained = config[""pretrained""]\n        self.dropout_keep = dropout_keep\n        self.embedding_pretrained = embedding_pretrained\n        self.input_data = tf.placeholder(tf.int32, shape=[self.batch_size,self.sen_len], name=""input_data"") \n        self.labels = tf.placeholder(tf.int32,shape=[self.batch_size,self.sen_len], name=""labels"")\n        self.embedding_placeholder = tf.placeholder(tf.float32,shape=[self.embedding_size,self.embedding_dim], name=""embedding_placeholder"")\n        with tf.variable_scope(""bilstm_crf"") as scope:\n            self._build_net()\n    def _build_net(self):\n        word_embeddings = tf.get_variable(""word_embeddings"",[self.embedding_size, self.embedding_dim])\n        if self.pretrained:\n            embeddings_init = word_embeddings.assign(self.embedding_pretrained)\n\n        input_embedded = tf.nn.embedding_lookup(word_embeddings, self.input_data)\n        input_embedded = tf.nn.dropout(input_embedded,self.dropout_keep)\n\n        lstm_fw_cell = tf.nn.rnn_cell.LSTMCell(self.embedding_dim, forget_bias=1.0, state_is_tuple=True)\n        lstm_bw_cell = tf.nn.rnn_cell.LSTMCell(self.embedding_dim, forget_bias=1.0, state_is_tuple=True)\n        (output_fw, output_bw), states = tf.nn.bidirectional_dynamic_rnn(lstm_fw_cell, \n                                                                         lstm_bw_cell, \n                                                                         input_embedded,\n                                                                         dtype=tf.float32,\n                                                                         time_major=False,\n                                                                         scope=None)\n\n        bilstm_out = tf.concat([output_fw, output_bw], axis=2)\n\n\n        # Fully connected layer.\n        W = tf.get_variable(name=""W"", shape=[self.batch_size,2 * self.embedding_dim, self.tag_size],\n                        dtype=tf.float32)\n\n        b = tf.get_variable(name=""b"", shape=[self.batch_size, self.sen_len, self.tag_size], dtype=tf.float32,\n                        initializer=tf.zeros_initializer())\n\n        bilstm_out = tf.tanh(tf.matmul(bilstm_out, W) + b)\n\n        # Linear-CRF.\n        log_likelihood, self.transition_params = tf.contrib.crf.crf_log_likelihood(bilstm_out, self.labels, tf.tile(np.array([self.sen_len]),np.array([self.batch_size])))\n\n        loss = tf.reduce_mean(-log_likelihood)\n\n        # Compute the viterbi sequence and score (used for prediction and test time).\n        self.viterbi_sequence, viterbi_score = tf.contrib.crf.crf_decode(bilstm_out, self.transition_params,tf.tile(np.array([self.sen_len]),np.array([self.batch_size])))\n\n        # Training ops.\n        optimizer = tf.train.AdamOptimizer(self.lr)\n        self.train_op = optimizer.minimize(loss)\n\n       \n'"
tensorflow/train.py,0,"b'# -*- coding: utf-8 -*\nimport pickle\nimport pdb\nimport codecs\nimport re\nimport sys\nimport math\n\nimport numpy as np\n\nimport tensorflow as tf\nfrom Batch import BatchGenerator\nfrom bilstm_crf import Model\nfrom utils import *\n\n\nwith open(\'../data/renmindata.pkl\', \'rb\') as inp:\n\tword2id = pickle.load(inp)\n\tid2word = pickle.load(inp)\n\ttag2id = pickle.load(inp)\n\tid2tag = pickle.load(inp)\n\tx_train = pickle.load(inp)\n\ty_train = pickle.load(inp)\n\tx_test = pickle.load(inp)\n\ty_test = pickle.load(inp)\n\tx_valid = pickle.load(inp)\n\ty_valid = pickle.load(inp)\nprint ""train len:"",len(x_train)\nprint ""test len:"",len(x_test)\nprint ""word2id len"", len(word2id)\nprint \'Creating the data generator ...\'\ndata_train = BatchGenerator(x_train, y_train, shuffle=True)\ndata_valid = BatchGenerator(x_valid, y_valid, shuffle=False)\ndata_test = BatchGenerator(x_test, y_test, shuffle=False)\nprint \'Finished creating the data generator.\'\n\n\nepochs = 31\nbatch_size = 32\n\nconfig = {}\nconfig[""lr""] = 0.001\nconfig[""embedding_dim""] = 100\nconfig[""sen_len""] = len(x_train[0])\nconfig[""batch_size""] = batch_size\nconfig[""embedding_size""] = len(word2id)+1\nconfig[""tag_size""] = len(tag2id)\nconfig[""pretrained""]=False\n\n\nembedding_pre = []\nif len(sys.argv)==2 and sys.argv[1]==""pretrained"":\n    print ""use pretrained embedding""\n    config[""pretrained""]=True\n    word2vec = {}\n    with codecs.open(\'vec.txt\',\'r\',\'utf-8\') as input_data:   \n        for line in input_data.readlines():\n            word2vec[line.split()[0]] = map(eval,line.split()[1:])\n\n    unknow_pre = []\n    unknow_pre.extend([1]*100)\n    embedding_pre.append(unknow_pre) #wordvec id 0\n    for word in word2id:\n        if word2vec.has_key(word):\n            embedding_pre.append(word2vec[word])\n        else:\n            embedding_pre.append(unknow_pre)\n\n    embedding_pre = np.asarray(embedding_pre)\n\n    \n    \n    \nif len(sys.argv)==2 and sys.argv[1]==""test"":\n    print ""begin to test...""\n    model = Model(config,embedding_pre,dropout_keep=1)\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        saver = tf.train.Saver()  \n        ckpt = tf.train.get_checkpoint_state(\'./model\')\n        if ckpt is None:\n            print \'Model not found, please train your model first\'\n        else:    \n            path = ckpt.model_checkpoint_path\n            print \'loading pre-trained model from %s.....\' % path\n            saver.restore(sess, path)\n            test_input(model,sess,word2id,id2tag,batch_size)\n            \n            \nelif len(sys.argv)==3:\n    print ""begin to extraction...""\n    model = Model(config,embedding_pre,dropout_keep=1)\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        saver = tf.train.Saver()  \n        ckpt = tf.train.get_checkpoint_state(\'./model\')\n        if ckpt is None:\n            print \'Model not found, please train your model first\'\n        else:    \n            path = ckpt.model_checkpoint_path\n            print \'loading pre-trained model from %s.....\' % path\n            saver.restore(sess, path)\n            extraction(sys.argv[1],sys.argv[2],model,sess,word2id,id2tag,batch_size)\n\nelse:\n    print ""begin to train...""\n    model = Model(config,embedding_pre,dropout_keep=0.5)\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        saver = tf.train.Saver()  \n        train(model,sess,saver,epochs,batch_size,data_train,data_test,id2word,id2tag)\n        \n     \n\n'"
tensorflow/utils.py,0,"b'# coding=utf-8\nimport codecs\nimport re\nimport numpy as np\n   \ndef calculate(x,y,id2word,id2tag,res=[]):\n    entity=[]\n    for i in range(len(x)): #for every sen\n        for j in range(len(x[0])): #for every word\n            if x[i][j]==0 or y[i][j]==0:\n                continue\n            if id2tag[y[i][j]][0]==\'B\':\n                entity=[id2word[x[i][j]]+\'/\'+id2tag[y[i][j]]]\n            elif id2tag[y[i][j]][0]==\'M\' and len(entity)!=0 and entity[-1].split(\'/\')[1][1:]==id2tag[y[i][j]][1:]:\n                entity.append(id2word[x[i][j]]+\'/\'+id2tag[y[i][j]])\n            elif id2tag[y[i][j]][0]==\'E\' and len(entity)!=0 and entity[-1].split(\'/\')[1][1:]==id2tag[y[i][j]][1:]:\n                entity.append(id2word[x[i][j]]+\'/\'+id2tag[y[i][j]])\n                entity.append(str(i))\n                entity.append(str(j))\n                res.append(entity)\n                entity=[]\n            else:\n                entity=[]\n    return res\n    \ndef get_entity(x,y,id2tag):\n    entity=""""\n    res=[]\n    for i in range(len(x)): #for every sen\n        for j in range(len(x[0])): #for every word\n            if y[i][j]==0:\n                continue\n            if id2tag[y[i][j]][0]==\'B\':\n                entity=id2tag[y[i][j]][1:]+\':\'+x[i][j]\n            elif id2tag[y[i][j]][0]==\'M\' and len(entity)!=0 :\n                entity+=x[i][j]\n            elif id2tag[y[i][j]][0]==\'E\' and len(entity)!=0 :\n                entity+=x[i][j]\n                res.append(entity)\n                entity=[]\n            else:\n                entity=[]\n    return res    \n    \ndef write_entity(outp,x,y,id2tag):\n    \'\'\'\n    \xe6\xb3\xa8\xe6\x84\x8f\xef\xbc\x8c\xe8\xbf\x99\xe4\xb8\xaa\xe5\x87\xbd\xe6\x95\xb0\xe6\xaf\x8f\xe6\xac\xa1\xe4\xbd\xbf\xe7\x94\xa8\xe6\x98\xaf\xe5\x9c\xa8\xe6\x96\x87\xe6\xa1\xa3\xe7\x9a\x84\xe6\x9c\x80\xe5\x90\x8e\xe6\xb7\xbb\xe5\x8a\xa0\xe6\x96\xb0\xe4\xbf\xa1\xe6\x81\xaf\xe3\x80\x82\n    \'\'\'\n    entity=\'\'\n    for i in range(len(x)): \n        if y[i]==0:\n            continue\n        if id2tag[y[i]][0]==\'B\':\n            entity=id2tag[y[i]][2:]+\':\'+x[i] \n        elif id2tag[y[i]][0]==\'M\' and len(entity)!=0:\n            entity+=x[i]\n        elif id2tag[y[i]][0]==\'E\' and len(entity)!=0: \n            entity+=x[i] \n            outp.write(entity+\' \')\n            entity=\'\'\n        else:\n            entity=\'\'       \n    return\n\n    \n\n\ndef train(model,sess,saver,epochs,batch_size,data_train,data_test,id2word,id2tag):    \n    batch_num = int(data_train.y.shape[0] / batch_size)  \n    batch_num_test = int(data_test.y.shape[0] / batch_size) \n    for epoch in range(epochs):\n        for batch in range(batch_num): \n            x_batch, y_batch = data_train.next_batch(batch_size)\n            # print x_batch.shape\n            feed_dict = {model.input_data:x_batch, model.labels:y_batch}\n            pre,_ = sess.run([model.viterbi_sequence,model.train_op], feed_dict)\n            acc = 0\n            if batch%200==0:\n                for i in range(len(y_batch)):\n                    for j in range(len(y_batch[0])):\n                        if y_batch[i][j]==pre[i][j]:\n                            acc+=1\n                print float(acc)/(len(y_batch)*len(y_batch[0]))\n        path_name = ""./model/model""+str(epoch)+"".ckpt""\n        print path_name\n        if epoch%3==0:\n            saver.save(sess, path_name)\n            print ""model has been saved""\n            entityres=[]\n            entityall=[]\n            for batch in range(batch_num): \n                x_batch, y_batch = data_train.next_batch(batch_size)\n                feed_dict = {model.input_data:x_batch, model.labels:y_batch}\n                pre = sess.run([model.viterbi_sequence], feed_dict)\n                pre = pre[0]\n                entityres = calculate(x_batch,pre,id2word,id2tag,entityres)\n                entityall = calculate(x_batch,y_batch,id2word,id2tag,entityall)\n            jiaoji = [i for i in entityres if i in entityall]\n            if len(jiaoji)!=0:\n                zhun = float(len(jiaoji))/len(entityres)\n                zhao = float(len(jiaoji))/len(entityall)\n                print ""train""\n                print ""zhun:"", zhun\n                print ""zhao:"", zhao\n                print ""f:"", (2*zhun*zhao)/(zhun+zhao)\n            else:\n                print ""zhun:"",0\n\n            entityres=[]\n            entityall=[]\n            for batch in range(batch_num_test): \n                x_batch, y_batch = data_test.next_batch(batch_size)\n                feed_dict = {model.input_data:x_batch, model.labels:y_batch}\n                pre = sess.run([model.viterbi_sequence], feed_dict)\n                pre = pre[0]\n                entityres = calculate(x_batch,pre,id2word,id2tag,entityres)\n                entityall = calculate(x_batch,y_batch,id2word,id2tag,entityall)\n            jiaoji = [i for i in entityres if i in entityall]\n            if len(jiaoji)!=0:\n                zhun = float(len(jiaoji))/len(entityres)\n                zhao = float(len(jiaoji))/len(entityall)\n                print ""test""\n                print ""zhun:"", zhun\n                print ""zhao:"", zhao\n                print ""f:"", (2*zhun*zhao)/(zhun+zhao)\n            else:\n                print ""zhun:"",0\n                \n                \nmax_len = 60\ndef padding(ids):\n    if len(ids) >= max_len:  \n        return ids[:max_len]\n    else:\n        ids.extend([0]*(max_len-len(ids)))\n        return ids      \ndef padding_word(sen):\n    if len(sen) >= max_len:\n        return sen[:max_len]\n    else:\n        return sen           \n                  \ndef test_input(model,sess,word2id,id2tag,batch_size):\n    while True:\n        text = raw_input(""Enter your input: "").decode(\'utf-8\');\n        text = re.split(u\'[\xef\xbc\x8c\xe3\x80\x82\xef\xbc\x81\xef\xbc\x9f\xe3\x80\x81\xe2\x80\x98\xe2\x80\x99\xe2\x80\x9c\xe2\x80\x9d\xef\xbc\x88\xef\xbc\x89]\', text) \n        text_id=[]\n        for sen in text:\n            word_id=[]\n            for word in sen:\n                if word in word2id:\n                    word_id.append(word2id[word])\n                else:\n                    word_id.append(word2id[""unknow""])\n            text_id.append(padding(word_id))\n        zero_padding=[]\n        zero_padding.extend([0]*max_len)\n        text_id.extend([zero_padding]*(batch_size-len(text_id)))    \n        feed_dict = {model.input_data:text_id}\n        pre = sess.run([model.viterbi_sequence], feed_dict)\n        entity = get_entity(text,pre[0],id2tag)\n        print \'result:\'\n        for i in entity:\n            print i\n            \n\n            \ndef extraction(input_path,output_path,model,sess,word2id,id2tag,batch_size):\n    text_id=[]\n    text = []\n    with codecs.open(input_path,\'rb\',\'utf8\') as inp:\n        for line in inp.readlines():\n            line = re.split(\'[\xef\xbc\x8c\xe3\x80\x82\xef\xbc\x81\xef\xbc\x9f\xe3\x80\x81\xe2\x80\x98\xe2\x80\x99\xe2\x80\x9c\xe2\x80\x9d\xef\xbc\x88\xef\xbc\x89]\'.decode(\'utf-8\'), line.strip())\n            for sentence in line:\n                if sentence==\'\' or sentence==\' \':\n                    continue\n                word_id=[]\n                for word in sentence:\n                    if word in word2id:\n                        word_id.append(word2id[word])\n                    else:\n                        word_id.append(word2id[""unknow""])\n                text_id.append(padding(word_id))\n                text.append(padding_word(sentence))\n    zero_padding=[]\n    zero_padding.extend([0]*max_len)\n    text_id.extend([zero_padding]*(batch_size-len(text_id)%batch_size)) \n    text_id = np.asarray(text_id)\n    text_id = text_id.reshape(-1,batch_size,max_len)\n    predict = []\n    for index in range(len(text_id)):\n        feed_dict = {model.input_data:text_id[index]}\n        pre = sess.run([model.viterbi_sequence], feed_dict)\n        predict.append(pre[0])\n    predict = np.asarray(predict).reshape(-1,max_len)\n    \n    with codecs.open(output_path,\'a\',\'utf-8\') as outp:\n        for index in range(len(text)):    \n            outp.write(text[index]+""   "") \n            write_entity(outp,text[index],predict[index],id2tag)\n            outp.write(\'\\n\')\n        \n\n\n'"
data/MSRA/train2pkl.py,0,"b'#coding:utf-8\nimport codecs\nimport re\nimport pandas as pd\nimport numpy as np\n\ndef wordtag():\n    input_data = codecs.open(\'train1.txt\',\'r\',\'utf-8\')\n    output_data = codecs.open(\'wordtag.txt\',\'w\',\'utf-8\')\n    for line in input_data.readlines():\n        #line=re.split(\'[\xef\xbc\x8c\xe3\x80\x82\xef\xbc\x9b\xef\xbc\x81\xef\xbc\x9a\xef\xbc\x9f\xe3\x80\x81\xe2\x80\x98\xe2\x80\x99\xe2\x80\x9c\xe2\x80\x9d]/[o]\'.decode(\'utf-8\'),line.strip())\n        line = line.strip().split()\n        \n        if len(line)==0:\n            continue\n        for word in line:\n            word = word.split(\'/\')\n            if word[1]!=\'o\':\n                if len(word[0])==1:\n                    output_data.write(word[0]+""/B_""+word[1]+"" "")\n                elif len(word[0])==2:\n                    output_data.write(word[0][0]+""/B_""+word[1]+"" "")\n                    output_data.write(word[0][1]+""/E_""+word[1]+"" "")\n                else:\n                    output_data.write(word[0][0]+""/B_""+word[1]+"" "")\n                    for j in word[0][1:len(word[0])-1]:\n                        output_data.write(j+""/M_""+word[1]+"" "")\n                    output_data.write(word[0][-1]+""/E_""+word[1]+"" "")\n            else:\n                for j in word[0]:\n                    output_data.write(j+""/o""+"" "")\n        output_data.write(\'\\n\')\n        \n            \n    input_data.close()\n    output_data.close()\n\nwordtag()\ndatas = list()\nlabels = list()\nlinedata=list()\nlinelabel=list()\n\ntag2id = {\'\' :0,\n\'B_ns\' :1,\n\'B_nr\' :2,\n\'B_nt\' :3,\n\'M_nt\' :4,\n\'M_nr\' :5,\n\'M_ns\' :6,\n\'E_nt\' :7,\n\'E_nr\' :8,\n\'E_ns\' :9,\n\'o\': 0}\n\nid2tag = {0:\'\' ,\n1:\'B_ns\' ,\n2:\'B_nr\' ,\n3:\'B_nt\' ,\n4:\'M_nt\' ,\n5:\'M_nr\' ,\n6:\'M_ns\' ,\n7:\'E_nt\' ,\n8:\'E_nr\' ,\n9:\'E_ns\' ,\n10: \'o\'}\n\n\ninput_data = codecs.open(\'wordtag.txt\',\'r\',\'utf-8\')\nfor line in input_data.readlines():\n    line=re.split(\'[\xef\xbc\x8c\xe3\x80\x82\xef\xbc\x9b\xef\xbc\x81\xef\xbc\x9a\xef\xbc\x9f\xe3\x80\x81\xe2\x80\x98\xe2\x80\x99\xe2\x80\x9c\xe2\x80\x9d]/[o]\'.decode(\'utf-8\'),line.strip())\n    for sen in line:\n        sen = sen.strip().split()\n        if len(sen)==0:\n            continue\n        linedata=[]\n        linelabel=[]\n        num_not_o=0\n        for word in sen:\n            word = word.split(\'/\')\n            linedata.append(word[0])\n            linelabel.append(tag2id[word[1]])\n\n            if word[1]!=\'o\':\n                num_not_o+=1\n        if num_not_o!=0:\n            datas.append(linedata)\n            labels.append(linelabel)\n            \ninput_data.close()    \nprint len(datas)\nprint len(labels)\n    \nfrom compiler.ast import flatten\nall_words = flatten(datas)\nsr_allwords = pd.Series(all_words)\nsr_allwords = sr_allwords.value_counts()\nset_words = sr_allwords.index\nset_ids = range(1, len(set_words)+1)    \nword2id = pd.Series(set_ids, index=set_words)\nid2word = pd.Series(set_words, index=set_ids)\n\nword2id[""unknow""] = len(word2id)+1\n\n\nmax_len = 50\ndef X_padding(words):\n    """"""\xe6\x8a\x8a words \xe8\xbd\xac\xe4\xb8\xba id \xe5\xbd\xa2\xe5\xbc\x8f\xef\xbc\x8c\xe5\xb9\xb6\xe8\x87\xaa\xe5\x8a\xa8\xe8\xa1\xa5\xe5\x85\xa8\xe4\xbd\x8d max_len \xe9\x95\xbf\xe5\xba\xa6\xe3\x80\x82""""""\n    ids = list(word2id[words])\n    if len(ids) >= max_len:  # \xe9\x95\xbf\xe5\x88\x99\xe5\xbc\x83\xe6\x8e\x89\n        return ids[:max_len]\n    ids.extend([0]*(max_len-len(ids))) # \xe7\x9f\xad\xe5\x88\x99\xe8\xa1\xa5\xe5\x85\xa8\n    return ids\n\ndef y_padding(ids):\n    """"""\xe6\x8a\x8a tags \xe8\xbd\xac\xe4\xb8\xba id \xe5\xbd\xa2\xe5\xbc\x8f\xef\xbc\x8c \xe5\xb9\xb6\xe8\x87\xaa\xe5\x8a\xa8\xe8\xa1\xa5\xe5\x85\xa8\xe4\xbd\x8d max_len \xe9\x95\xbf\xe5\xba\xa6\xe3\x80\x82""""""\n    if len(ids) >= max_len:  # \xe9\x95\xbf\xe5\x88\x99\xe5\xbc\x83\xe6\x8e\x89\n        return ids[:max_len]\n    ids.extend([0]*(max_len-len(ids))) # \xe7\x9f\xad\xe5\x88\x99\xe8\xa1\xa5\xe5\x85\xa8\n    return ids\n\ndf_data = pd.DataFrame({\'words\': datas, \'tags\': labels}, index=range(len(datas)))\ndf_data[\'x\'] = df_data[\'words\'].apply(X_padding)\ndf_data[\'y\'] = df_data[\'tags\'].apply(y_padding)\nx = np.asarray(list(df_data[\'x\'].values))\ny = np.asarray(list(df_data[\'y\'].values))\n\nfrom sklearn.model_selection import train_test_split\nx_train,x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=43)\nx_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train,  test_size=0.2, random_state=43)\n\n\nprint \'Finished creating the data generator.\'\nimport pickle\nimport os\nwith open(\'../dataMSRA.pkl\', \'wb\') as outp:\n\tpickle.dump(word2id, outp)\n\tpickle.dump(id2word, outp)\n\tpickle.dump(tag2id, outp)\n\tpickle.dump(id2tag, outp)\n\tpickle.dump(x_train, outp)\n\tpickle.dump(y_train, outp)\n\tpickle.dump(x_test, outp)\n\tpickle.dump(y_test, outp)\n\tpickle.dump(x_valid, outp)\n\tpickle.dump(y_valid, outp)\nprint \'** Finished saving the data.\'\n\n\n'"
data/boson/data_util.py,0,"b'#!/usr/bin/python\n# -*- coding: UTF-8 -*-\n\nimport codecs\nimport pandas as pd\nimport numpy as np\nimport re\n\ndef data2pkl():\n    datas = list()\n    labels = list()\n    linedata=list()\n    linelabel=list()\n    tags = set()\n\n    input_data = codecs.open(\'./wordtagsplit.txt\',\'r\',\'utf-8\')\n    for line in input_data.readlines():\n        line = line.split()\n        linedata=[]\n        linelabel=[]\n        numNotO=0\n        for word in line:\n            word = word.split(\'/\')\n            linedata.append(word[0])\n            linelabel.append(word[1])\n            tags.add(word[1])\n            if word[1]!=\'O\':\n                numNotO+=1\n        if numNotO!=0:\n            datas.append(linedata)\n            labels.append(linelabel)\n\n    input_data.close()\n    print len(datas),tags\n    print len(labels)\n    from compiler.ast import flatten\n    all_words = flatten(datas)\n    sr_allwords = pd.Series(all_words)\n    sr_allwords = sr_allwords.value_counts()\n    set_words = sr_allwords.index\n    set_ids = range(1, len(set_words)+1)\n\n\n    tags = [i for i in tags]\n    tag_ids = range(len(tags))\n    word2id = pd.Series(set_ids, index=set_words)\n    id2word = pd.Series(set_words, index=set_ids)\n    tag2id = pd.Series(tag_ids, index=tags)\n    id2tag = pd.Series(tags, index=tag_ids)\n\n    word2id[""unknow""] = len(word2id)+1\n    print word2id\n    max_len = 60\n    def X_padding(words):\n        ids = list(word2id[words])\n        if len(ids) >= max_len:  \n            return ids[:max_len]\n        ids.extend([0]*(max_len-len(ids))) \n        return ids\n\n    def y_padding(tags):\n        ids = list(tag2id[tags])\n        if len(ids) >= max_len: \n            return ids[:max_len]\n        ids.extend([0]*(max_len-len(ids))) \n        return ids\n    df_data = pd.DataFrame({\'words\': datas, \'tags\': labels}, index=range(len(datas)))\n    df_data[\'x\'] = df_data[\'words\'].apply(X_padding)\n    df_data[\'y\'] = df_data[\'tags\'].apply(y_padding)\n    x = np.asarray(list(df_data[\'x\'].values))\n    y = np.asarray(list(df_data[\'y\'].values))\n\n    from sklearn.model_selection import train_test_split\n    x_train,x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=43)\n    x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train,  test_size=0.2, random_state=43)\n\n    \n    import pickle\n    import os\n    with open(\'../Bosondata.pkl\', \'wb\') as outp:\n\t    pickle.dump(word2id, outp)\n\t    pickle.dump(id2word, outp)\n\t    pickle.dump(tag2id, outp)\n\t    pickle.dump(id2tag, outp)\n\t    pickle.dump(x_train, outp)\n\t    pickle.dump(y_train, outp)\n\t    pickle.dump(x_test, outp)\n\t    pickle.dump(y_test, outp)\n\t    pickle.dump(x_valid, outp)\n\t    pickle.dump(y_valid, outp)\n    print \'** Finished saving the data.\'\n    \n    \n    \ndef origin2tag():\n    input_data = codecs.open(\'./origindata.txt\',\'r\',\'utf-8\')\n    output_data = codecs.open(\'./wordtag.txt\',\'w\',\'utf-8\')\n    for line in input_data.readlines():\n        line=line.strip()\n        i=0\n        while i <len(line):\n\t        if line[i] == \'{\':\n\t\t        i+=2\n\t\t        temp=""""\n\t\t        while line[i]!=\'}\':\n\t\t\t        temp+=line[i]\n\t\t\t        i+=1\n\t\t        i+=2\n\t\t        word=temp.split(\':\')\n\t\t        sen = word[1]\n\t\t        output_data.write(sen[0]+""/B_""+word[0]+"" "")\n\t\t        for j in sen[1:len(sen)-1]:\n\t\t\t        output_data.write(j+""/M_""+word[0]+"" "")\n\t\t        output_data.write(sen[-1]+""/E_""+word[0]+"" "")\n\t        else:\n\t\t        output_data.write(line[i]+""/O "")\n\t\t        i+=1\n        output_data.write(\'\\n\')\n    input_data.close()\n    output_data.close()\n\n\ndef tagsplit():\n    with open(\'./wordtag.txt\',\'rb\') as inp:\n\t    texts = inp.read().decode(\'utf-8\')\n    sentences = re.split(\'[\xef\xbc\x8c\xe3\x80\x82\xef\xbc\x81\xef\xbc\x9f\xe3\x80\x81\xe2\x80\x98\xe2\x80\x99\xe2\x80\x9c\xe2\x80\x9d\xef\xbc\x88\xef\xbc\x89]/[O]\'.decode(\'utf-8\'), texts)\n    output_data = codecs.open(\'./wordtagsplit.txt\',\'w\',\'utf-8\')\n    for sentence in sentences:\n\t    if sentence != "" "":\n\t\t    output_data.write(sentence.strip()+\'\\n\')\n    output_data.close()\n\n\norigin2tag()\ntagsplit()\ndata2pkl()\n'"
data/renMinRiBao/data_renmin_word.py,0,"b'# -*- coding: UTF-8 -*-\n\nimport codecs\nimport re\nimport pdb\nimport pandas as pd\nimport numpy as np\nimport collections\ndef originHandle():\n    with open(\'./renmin.txt\',\'r\') as inp,open(\'./renmin2.txt\',\'w\') as outp:\n        for line in inp.readlines():\n            line = line.split(\'  \')\n            i = 1\n            while i<len(line)-1:\n                if line[i][0]==\'[\':\n                    outp.write(line[i].split(\'/\')[0][1:])\n                    i+=1\n                    while i<len(line)-1 and line[i].find(\']\')==-1:\n                        if line[i]!=\'\':\n                            outp.write(line[i].split(\'/\')[0])\n                        i+=1\n                    outp.write(line[i].split(\'/\')[0].strip()+\'/\'+line[i].split(\'/\')[1][-2:]+\' \')\n                elif line[i].split(\'/\')[1]==\'nr\':\n                    word = line[i].split(\'/\')[0] \n                    i+=1\n                    if i<len(line)-1 and line[i].split(\'/\')[1]==\'nr\':\n                        outp.write(word+line[i].split(\'/\')[0]+\'/nr \')           \n                    else:\n                        outp.write(word+\'/nr \')\n                        continue\n                else:\n                    outp.write(line[i]+\' \')\n                i+=1\n            outp.write(\'\\n\')\ndef originHandle2():\n    with codecs.open(\'./renmin2.txt\',\'r\',\'utf-8\') as inp,codecs.open(\'./renmin3.txt\',\'w\',\'utf-8\') as outp:\n        for line in inp.readlines():\n            line = line.split(\' \')\n            i = 0\n            while i<len(line)-1:\n                if line[i]==\'\':\n                    i+=1\n                    continue\n                word = line[i].split(\'/\')[0]\n                tag = line[i].split(\'/\')[1]\n                if tag==\'nr\' or tag==\'ns\' or tag==\'nt\':\n                    outp.write(word[0]+""/B_""+tag+"" "")\n                    for j in word[1:len(word)-1]:\n                        if j!=\' \':\n                            outp.write(j+""/M_""+tag+"" "")\n                    outp.write(word[-1]+""/E_""+tag+"" "")\n                else:\n                    for wor in word:\n                        outp.write(wor+\'/O \')\n                i+=1\n            outp.write(\'\\n\')\ndef sentence2split():\n    with open(\'./renmin3.txt\',\'r\') as inp,codecs.open(\'./renmin4.txt\',\'w\',\'utf-8\') as outp:\n        texts = inp.read().decode(\'utf-8\')\n        sentences = re.split(\'[\xef\xbc\x8c\xe3\x80\x82\xef\xbc\x81\xef\xbc\x9f\xe3\x80\x81\xe2\x80\x98\xe2\x80\x99\xe2\x80\x9c\xe2\x80\x9d:]/[O]\'.decode(\'utf-8\'), texts)\n        for sentence in sentences:\n\t        if sentence != "" "":\n\t\t        outp.write(sentence.strip()+\'\\n\')     \n\ndef data2pkl():\n    datas = list()\n    labels = list()\n    linedata=list()\n    linelabel=list()\n    tags = set()\n    tags.add(\'\')\n    input_data = codecs.open(\'renmin4.txt\',\'r\',\'utf-8\')\n    for line in input_data.readlines():\n        line = line.split()\n        linedata=[]\n        linelabel=[]\n        numNotO=0\n        for word in line:\n            word = word.split(\'/\')\n            linedata.append(word[0])\n            linelabel.append(word[1])\n            tags.add(word[1])\n            if word[1]!=\'O\':\n                numNotO+=1\n        if numNotO!=0:\n            datas.append(linedata)\n            labels.append(linelabel)\n\n    input_data.close()\n    print len(datas)\n    print len(labels)\n    from compiler.ast import flatten\n    all_words = flatten(datas)\n    sr_allwords = pd.Series(all_words)\n    sr_allwords = sr_allwords.value_counts()\n    set_words = sr_allwords.index\n    set_ids = range(1, len(set_words)+1)\n\n    \n    tags = [i for i in tags]\n    tag_ids = range(len(tags))\n    word2id = pd.Series(set_ids, index=set_words)\n    id2word = pd.Series(set_words, index=set_ids)\n    tag2id = pd.Series(tag_ids, index=tags)\n    id2tag = pd.Series(tags, index=tag_ids)\n    word2id[""unknow""]=len(word2id)+1\n    id2word[len(word2id)]=""unknow""\n    print tag2id\n    max_len = 60\n    def X_padding(words):\n        ids = list(word2id[words])\n        if len(ids) >= max_len:  \n            return ids[:max_len]\n        ids.extend([0]*(max_len-len(ids))) \n        return ids\n\n    def y_padding(tags):\n        ids = list(tag2id[tags])\n        if len(ids) >= max_len: \n            return ids[:max_len]\n        ids.extend([0]*(max_len-len(ids))) \n        return ids\n    df_data = pd.DataFrame({\'words\': datas, \'tags\': labels}, index=range(len(datas)))\n    df_data[\'x\'] = df_data[\'words\'].apply(X_padding)\n    df_data[\'y\'] = df_data[\'tags\'].apply(y_padding)\n    x = np.asarray(list(df_data[\'x\'].values))\n    y = np.asarray(list(df_data[\'y\'].values))\n\n    from sklearn.model_selection import train_test_split\n    x_train,x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=43)\n    x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train,  test_size=0.2, random_state=43)\n\n\n    import pickle\n    import os\n    with open(\'../renmindata.pkl\', \'wb\') as outp:\n\t    pickle.dump(word2id, outp)\n\t    pickle.dump(id2word, outp)\n\t    pickle.dump(tag2id, outp)\n\t    pickle.dump(id2tag, outp)\n\t    pickle.dump(x_train, outp)\n\t    pickle.dump(y_train, outp)\n\t    pickle.dump(x_test, outp)\n\t    pickle.dump(y_test, outp)\n\t    pickle.dump(x_valid, outp)\n\t    pickle.dump(y_valid, outp)\n    print \'** Finished saving the data.\'\n    \n\n\noriginHandle()\noriginHandle2()\nsentence2split()\ndata2pkl()\n'"
