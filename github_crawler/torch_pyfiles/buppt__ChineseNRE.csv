file_path,api_count,code
BiLSTM_ATT.py,19,"b""#coding:utf8\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\ntorch.manual_seed(1)\n\nclass BiLSTM_ATT(nn.Module):\n    def __init__(self,config,embedding_pre):\n        super(BiLSTM_ATT,self).__init__()\n        self.batch = config['BATCH']\n        \n        self.embedding_size = config['EMBEDDING_SIZE']\n        self.embedding_dim = config['EMBEDDING_DIM']\n        \n        self.hidden_dim = config['HIDDEN_DIM']\n        self.tag_size = config['TAG_SIZE']\n        \n        self.pos_size = config['POS_SIZE']\n        self.pos_dim = config['POS_DIM']\n        \n        self.pretrained = config['pretrained']\n        if self.pretrained:\n            #self.word_embeds.weight.data.copy_(torch.from_numpy(embedding_pre))\n            self.word_embeds = nn.Embedding.from_pretrained(torch.FloatTensor(embedding_pre),freeze=False)\n        else:\n            self.word_embeds = nn.Embedding(self.embedding_size,self.embedding_dim)\n        \n        self.pos1_embeds = nn.Embedding(self.pos_size,self.pos_dim)\n        self.pos2_embeds = nn.Embedding(self.pos_size,self.pos_dim)\n        self.relation_embeds = nn.Embedding(self.tag_size,self.hidden_dim)\n        \n        self.lstm = nn.LSTM(input_size=self.embedding_dim+self.pos_dim*2,hidden_size=self.hidden_dim//2,num_layers=1, bidirectional=True)\n        self.hidden2tag = nn.Linear(self.hidden_dim,self.tag_size)\n        \n        self.dropout_emb=nn.Dropout(p=0.5)\n        self.dropout_lstm=nn.Dropout(p=0.5)\n        self.dropout_att=nn.Dropout(p=0.5)\n        \n        self.hidden = self.init_hidden()\n        \n        self.att_weight = nn.Parameter(torch.randn(self.batch,1,self.hidden_dim))\n        self.relation_bias = nn.Parameter(torch.randn(self.batch,self.tag_size,1))\n        \n    def init_hidden(self):\n        return torch.randn(2, self.batch, self.hidden_dim // 2)\n        \n    def init_hidden_lstm(self):\n        return (torch.randn(2, self.batch, self.hidden_dim // 2),\n                torch.randn(2, self.batch, self.hidden_dim // 2))\n                \n    def attention(self,H):\n        M = F.tanh(H)\n        a = F.softmax(torch.bmm(self.att_weight,M),2)\n        a = torch.transpose(a,1,2)\n        return torch.bmm(H,a)\n        \n    \n                \n    def forward(self,sentence,pos1,pos2):\n\n        self.hidden = self.init_hidden_lstm()\n\n        embeds = torch.cat((self.word_embeds(sentence),self.pos1_embeds(pos1),self.pos2_embeds(pos2)),2)\n        \n        embeds = torch.transpose(embeds,0,1)\n\n        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n        \n        lstm_out = torch.transpose(lstm_out,0,1)\n        lstm_out = torch.transpose(lstm_out,1,2)\n        \n        lstm_out = self.dropout_lstm(lstm_out)\n        att_out = F.tanh(self.attention(lstm_out))\n        #att_out = self.dropout_att(att_out)\n        \n        relation = torch.tensor([i for i in range(self.tag_size)],dtype = torch.long).repeat(self.batch, 1)\n\n        relation = self.relation_embeds(relation)\n        \n        res = torch.add(torch.bmm(relation,att_out),self.relation_bias)\n        \n        res = F.softmax(res,1)\n\n        \n        return res.view(self.batch,-1)\n"""
train.py,15,"b'#coding:utf8\nimport numpy as np\nimport pickle\nimport sys\nimport codecs\n\n#with open(\'./data/engdata_train.pkl\', \'rb\') as inp:\nwith open(\'./data/people_relation_train.pkl\', \'rb\') as inp:\n    word2id = pickle.load(inp)\n    id2word = pickle.load(inp)\n    relation2id = pickle.load(inp)\n    train = pickle.load(inp)\n    labels = pickle.load(inp)\n    position1 = pickle.load(inp)\n    position2 = pickle.load(inp)\n\n#with open(\'./data/engdata_test.pkl\', \'rb\') as inp: \nwith open(\'./data/people_relation_test.pkl\', \'rb\') as inp:\n    test = pickle.load(inp)\n    labels_t = pickle.load(inp)\n    position1_t = pickle.load(inp)\n    position2_t = pickle.load(inp)\n\n    \n\n   \nprint ""train len"", len(train)     \nprint ""test len"", len(test)   \nprint ""word2id len"",len(word2id)\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nimport torch.utils.data as D\nfrom torch.autograd import Variable\nfrom BiLSTM_ATT import BiLSTM_ATT\n        \n\nEMBEDDING_SIZE = len(word2id)+1        \nEMBEDDING_DIM = 100\n\nPOS_SIZE = 82  #\xe4\xb8\x8d\xe5\x90\x8c\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe8\xbf\x99\xe9\x87\x8c\xe5\x8f\xaf\xe8\x83\xbd\xe4\xbc\x9a\xe6\x8a\xa5\xe9\x94\x99\xe3\x80\x82\nPOS_DIM = 25\n\nHIDDEN_DIM = 200\n\nTAG_SIZE = len(relation2id)\n\nBATCH = 128\nEPOCHS = 100\n\nconfig={}\nconfig[\'EMBEDDING_SIZE\'] = EMBEDDING_SIZE\nconfig[\'EMBEDDING_DIM\'] = EMBEDDING_DIM\nconfig[\'POS_SIZE\'] = POS_SIZE\nconfig[\'POS_DIM\'] = POS_DIM\nconfig[\'HIDDEN_DIM\'] = HIDDEN_DIM\nconfig[\'TAG_SIZE\'] = TAG_SIZE\nconfig[\'BATCH\'] = BATCH\nconfig[""pretrained""]=False\n\nlearning_rate = 0.0005\n\n\nembedding_pre = []\nif len(sys.argv)==2 and sys.argv[1]==""pretrained"":\n    print ""use pretrained embedding""\n    config[""pretrained""]=True\n    word2vec = {}\n    with codecs.open(\'vec.txt\',\'r\',\'utf-8\') as input_data:   \n        for line in input_data.readlines():\n            word2vec[line.split()[0]] = map(eval,line.split()[1:])\n\n    unknow_pre = []\n    unknow_pre.extend([1]*100)\n    embedding_pre.append(unknow_pre) #wordvec id 0\n    for word in word2id:\n        if word2vec.has_key(word):\n            embedding_pre.append(word2vec[word])\n        else:\n            embedding_pre.append(unknow_pre)\n\n    embedding_pre = np.asarray(embedding_pre)\n    print embedding_pre.shape\n\nmodel = BiLSTM_ATT(config,embedding_pre)\n#model = torch.load(\'model/model_epoch20.pkl\')\noptimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\ncriterion = nn.CrossEntropyLoss(size_average=True)\n\n\n\ntrain = torch.LongTensor(train[:len(train)-len(train)%BATCH])\nposition1 = torch.LongTensor(position1[:len(train)-len(train)%BATCH])\nposition2 = torch.LongTensor(position2[:len(train)-len(train)%BATCH])\nlabels = torch.LongTensor(labels[:len(train)-len(train)%BATCH])\ntrain_datasets = D.TensorDataset(train,position1,position2,labels)\ntrain_dataloader = D.DataLoader(train_datasets,BATCH,True,num_workers=2)\n\n\ntest = torch.LongTensor(test[:len(test)-len(test)%BATCH])\nposition1_t = torch.LongTensor(position1_t[:len(test)-len(test)%BATCH])\nposition2_t = torch.LongTensor(position2_t[:len(test)-len(test)%BATCH])\nlabels_t = torch.LongTensor(labels_t[:len(test)-len(test)%BATCH])\ntest_datasets = D.TensorDataset(test,position1_t,position2_t,labels_t)\ntest_dataloader = D.DataLoader(test_datasets,BATCH,True,num_workers=2)\n\n\nfor epoch in range(EPOCHS):\n    print ""epoch:"",epoch\n    acc=0\n    total=0\n    \n    for sentence,pos1,pos2,tag in train_dataloader:\n        sentence = Variable(sentence)\n        pos1 = Variable(pos1)\n        pos2 = Variable(pos2)\n        y = model(sentence,pos1,pos2)  \n        tags = Variable(tag)\n        loss = criterion(y, tags)      \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()    \n       \n        y = np.argmax(y.data.numpy(),axis=1)\n\n        for y1,y2 in zip(y,tag):\n            if y1==y2:\n                acc+=1\n            total+=1\n        \n    print ""train:"",100*float(acc)/total,""%""\n      \n    acc_t=0\n    total_t=0\n    count_predict = [0,0,0,0,0,0,0,0,0,0,0,0]\n    count_total = [0,0,0,0,0,0,0,0,0,0,0,0]\n    count_right = [0,0,0,0,0,0,0,0,0,0,0,0]\n    for sentence,pos1,pos2,tag in test_dataloader:\n        sentence = Variable(sentence)\n        pos1 = Variable(pos1)\n        pos2 = Variable(pos2)\n        y = model(sentence,pos1,pos2)\n        y = np.argmax(y.data.numpy(),axis=1)\n        for y1,y2 in zip(y,tag):\n            count_predict[y1]+=1\n            count_total[y2]+=1\n            if y1==y2:\n                count_right[y1]+=1\n\n    \n    precision = [0,0,0,0,0,0,0,0,0,0,0,0]\n    recall = [0,0,0,0,0,0,0,0,0,0,0,0]\n    for i in range(len(count_predict)):\n        if count_predict[i]!=0 :\n            precision[i] = float(count_right[i])/count_predict[i]\n            \n        if count_total[i]!=0:\n            recall[i] = float(count_right[i])/count_total[i]\n    \n\n    precision = sum(precision)/len(relation2id)\n    recall = sum(recall)/len(relation2id)    \n    print ""\xe5\x87\x86\xe7\xa1\xae\xe7\x8e\x87\xef\xbc\x9a"",precision\n    print ""\xe5\x8f\xac\xe5\x9b\x9e\xe7\x8e\x87\xef\xbc\x9a"",recall\n    print ""f\xef\xbc\x9a"", (2*precision*recall)/(precision+recall)\n\n    if epoch%20==0:\n        model_name = ""./model/model_epoch""+str(epoch)+"".pkl""\n        torch.save(model, model_name)\n        print model_name,""has been saved""\n\n\ntorch.save(model, ""./model/model_01.pkl"")\nprint ""model has been saved""\n\n\n'"
data/SemEval2010_task8_all_data/data_util.py,0,"b'# coding=utf-8\nimport codecs\nimport sys\nimport re\nimport pandas as pd\nimport numpy as np\n\nfrom collections import deque  \n\nrelation2id = {\n""Other"": 0,\n""Cause-Effect"": 1,\n""Instrument-Agency"":2,\n""Product-Producer"":3,\n""Content-Container"":4,\n""Entity-Origin"":5,\n""Entity-Destination"":6,\n""Component-Whole"":7,\n""Member-Collection"":8,\n""Message-Topic"":9\n}\ndatas = deque()\nlabels = deque()\nentity1 = deque()\nentity2 = deque()\nwith codecs.open(\'TRAIN_FILE.TXT\',\'r\',\'utf-8\') as tra:\n    linenum = 0\n    for line in tra:\n        linenum+=1\n        if linenum%4==1:\n            line = line.split(\'\\t\')[1]\n            \n            word_arr = line[1:-4].split()\n            for index in range(len(word_arr)):\n                if ""<e1>"" in word_arr[index]:\n                    entity1.append(index)\n                elif ""<e2>"" in word_arr[index]:\n                    entity2.append(index)\n\n            line = line.replace(""<e1>"","""")\n            line = line.replace(""</e1>"","""")\n            line = line.replace(""<e2>"","""")\n            line = line.replace(""</e2>"","""")\n            line = re.sub(r\'[^\\w\\s]\',\'\',line)\n\n            datas.append(line[1:-2].split())\n        elif linenum%4==2:\n            if line==""Other\\r\\n"":\n                labels.append(0)\n            else:\n                line = line.split(\'(\')\n                labels.append(relation2id[line[0]])\n            \n        else:\n            continue\n            \nprint len(datas),len(labels),len(entity1),len(entity2)\nprint datas[0],labels[0],entity1[0],entity2[0]\n\n    \nfrom compiler.ast import flatten\nall_words = flatten(datas)\nsr_allwords = pd.Series(all_words)\nsr_allwords = sr_allwords.value_counts()\n#print sr_allwords\nset_words = sr_allwords.index\nset_ids = range(1, len(set_words)+1)\nword2id = pd.Series(set_ids, index=set_words)\nid2word = pd.Series(set_words, index=set_ids)\n\nword2id[""BLANK""]=len(word2id)+1\nword2id[""UNKNOW""]=len(word2id)+1\nprint word2id\nmax_len = 70\nsenssslen = 0\n\ndef X_padding(words):\n    ids = []\n    for word in words:\n        if word in word2id:\n            ids.append(word2id[word])\n        else:\n            ids.append(word2id[""UNKNOW""])\n\n    if len(ids) >= max_len: \n        return ids[:max_len]\n    ids.extend([word2id[""BLANK""]]*(max_len-len(ids))) \n    return ids\n\ndef pos_padding(index):\n    ids=[]    \n    for i in range(max_len):\n        ids.append(i-index+max_len)\n    if max_len-index<0:\n        print index,ids\n    return ids\n\n\nx = deque()\npos_e1 = deque()\npos_e2 = deque()\nfor index in range(len(datas)):\n    x.append(X_padding(datas[index]))\n    pos_e1.append(pos_padding(entity1[index]))\n    pos_e2.append(pos_padding(entity2[index]))\n\nx = np.asarray(x)\ny = np.asarray(labels)\npos_e1 = np.asarray(pos_e1)\npos_e2 = np.asarray(pos_e2)\n\nprint x.shape,y.shape,pos_e1.shape,pos_e2.shape\n\nimport pickle\nwith open(\'../engdata_train.pkl\', \'wb\') as outp:\n\tpickle.dump(word2id, outp)\n\tpickle.dump(id2word, outp)\n\tpickle.dump(relation2id, outp)\n\tpickle.dump(x, outp)\n\tpickle.dump(y, outp)\n\tpickle.dump(pos_e1, outp)\n\tpickle.dump(pos_e2, outp)\nprint \'** Finished saving train data.\'\n\n\ndatas = deque()\nlabels = deque()\nentity1 = deque()\nentity2 = deque()\nwith codecs.open(\'TEST_FILE_FULL.TXT\',\'r\',\'utf-8\') as tra:\n    linenum = 0\n    for line in tra:\n        linenum+=1\n        if linenum%4==1:\n            line = line.split(\'\\t\')[1]\n            \n            word_arr = line[1:-4].split()\n            for index in range(len(word_arr)):\n                if ""<e1>"" in word_arr[index]:\n                    entity1.append(index)\n                elif ""<e2>"" in word_arr[index]:\n                    entity2.append(index)\n\n            line = line.replace(""<e1>"","""")\n            line = line.replace(""</e1>"","""")\n            line = line.replace(""<e2>"","""")\n            line = line.replace(""</e2>"","""")\n            line = re.sub(r\'[^\\w\\s]\',\'\',line)\n\n            datas.append(line[1:-2].split())\n        elif linenum%4==2:\n            if line==""Other\\r\\n"":\n                labels.append(0)\n            else:\n                line = line.split(\'(\')\n                labels.append(relation2id[line[0]])\n            \n        else:\n            continue\n\nx = deque()\npos_e1 = deque()\npos_e2 = deque()\nfor index in range(len(datas)):\n    x.append(X_padding(datas[index]))\n    pos_e1.append(pos_padding(entity1[index]))\n    pos_e2.append(pos_padding(entity2[index]))\n    \nx = np.asarray(x)\ny = np.asarray(labels)\npos_e1 = np.asarray(pos_e1)\npos_e2 = np.asarray(pos_e2)\n\nprint x.shape,y.shape,pos_e1.shape,pos_e2.shape\n\nimport pickle\nwith open(\'../engdata_test.pkl\', \'wb\') as outp:\n\tpickle.dump(x, outp)\n\tpickle.dump(y, outp)\n\tpickle.dump(pos_e1, outp)\n\tpickle.dump(pos_e2, outp)\nprint \'** Finished saving train data.\'\n\n'"
data/people-relation/data_util.py,0,"b'#coding:utf8\nimport codecs\nimport sys\nimport pandas as pd\nimport numpy as np\nfrom collections import deque  \nimport pdb\n\nrelation2id = {}\nwith codecs.open(\'relation2id.txt\',\'r\',\'utf-8\') as input_data:\n    for line in input_data.readlines():\n        relation2id[line.split()[0]] = int(line.split()[1])\n    input_data.close()\n    #print relation2id\n\n\n\ndatas = deque()\nlabels = deque()\npositionE1 = deque()\npositionE2 = deque()\ncount = [0,0,0,0,0,0,0,0,0,0,0,0]\ntotal_data=0\nwith codecs.open(\'train.txt\',\'r\',\'utf-8\') as tfc: \n    for lines in tfc:\n        line = lines.split()\n        if count[relation2id[line[2]]] <1500:\n            sentence = []\n            index1 = line[3].index(line[0])\n            position1 = []\n            index2 = line[3].index(line[1])\n            position2 = []\n\n            for i,word in enumerate(line[3]):\n                sentence.append(word)\n                position1.append(i-index1)\n                position2.append(i-index2)\n                i+=1\n            datas.append(sentence)\n            labels.append(relation2id[line[2]])\n            positionE1.append(position1)\n            positionE2.append(position2)\n        count[relation2id[line[2]]]+=1\n        total_data+=1\n        \nprint total_data,len(datas)\n\nfrom compiler.ast import flatten\nall_words = flatten(datas)\nsr_allwords = pd.Series(all_words)\nsr_allwords = sr_allwords.value_counts()\n\nset_words = sr_allwords.index\nset_ids = range(1, len(set_words)+1)\nword2id = pd.Series(set_ids, index=set_words)\nid2word = pd.Series(set_words, index=set_ids)\n\nword2id[""BLANK""]=len(word2id)+1\nword2id[""UNKNOW""]=len(word2id)+1\nid2word[len(id2word)+1]=""BLANK""\nid2word[len(id2word)+1]=""UNKNOW""\n#print ""word2id"",id2word\n\nmax_len = 50\ndef X_padding(words):\n    """"""\xe6\x8a\x8a words \xe8\xbd\xac\xe4\xb8\xba id \xe5\xbd\xa2\xe5\xbc\x8f\xef\xbc\x8c\xe5\xb9\xb6\xe8\x87\xaa\xe5\x8a\xa8\xe8\xa1\xa5\xe5\x85\xa8\xe4\xbd\x8d max_len \xe9\x95\xbf\xe5\xba\xa6\xe3\x80\x82""""""\n    ids = []\n    for i in words:\n        if i in word2id:\n            ids.append(word2id[i])\n        else:\n            ids.append(word2id[""UNKNOW""])\n    if len(ids) >= max_len: \n        return ids[:max_len]\n    ids.extend([word2id[""BLANK""]]*(max_len-len(ids))) \n\n    return ids\n    \n    \ndef pos(num):\n    if num<-40:\n        return 0\n    if num>=-40 and num<=40:\n        return num+40\n    if num>40:\n        return 80\ndef position_padding(words):\n    words = [pos(i) for i in words]\n    if len(words) >= max_len:  \n        return words[:max_len]\n    words.extend([81]*(max_len-len(words))) \n    return words\n\n\n\ndf_data = pd.DataFrame({\'words\': datas, \'tags\': labels,\'positionE1\':positionE1,\'positionE2\':positionE2}, index=range(len(datas)))\ndf_data[\'words\'] = df_data[\'words\'].apply(X_padding)\ndf_data[\'tags\'] = df_data[\'tags\']\ndf_data[\'positionE1\'] = df_data[\'positionE1\'].apply(position_padding)\ndf_data[\'positionE2\'] = df_data[\'positionE2\'].apply(position_padding)\n\ndatas = np.asarray(list(df_data[\'words\'].values))\nlabels = np.asarray(list(df_data[\'tags\'].values))\npositionE1 = np.asarray(list(df_data[\'positionE1\'].values))\npositionE2 = np.asarray(list(df_data[\'positionE2\'].values))\n\nprint datas.shape\nprint labels.shape\nprint positionE1.shape\nprint positionE2.shape\n\n\nimport pickle\nwith open(\'../people_relation_train.pkl\', \'wb\') as outp:\n\tpickle.dump(word2id, outp)\n\tpickle.dump(id2word, outp)\n\tpickle.dump(relation2id, outp)\n\tpickle.dump(datas, outp)\n\tpickle.dump(labels, outp)\n\tpickle.dump(positionE1, outp)\n\tpickle.dump(positionE2, outp)\nprint \'** Finished saving the data.\'\n\n\n\ndatas = deque()\nlabels = deque()\npositionE1 = deque()\npositionE2 = deque()\ncount = [0,0,0,0,0,0,0,0,0,0,0,0]\nwith codecs.open(\'train.txt\',\'r\',\'utf-8\') as tfc: \n    for lines in tfc:\n        line = lines.split()\n        if count[relation2id[line[2]]] >1500 and count[relation2id[line[2]]]<=1800:\n        #if count[relation2id[line[2]]] <=1500:\n            sentence = []\n            index1 = line[3].index(line[0])\n            position1 = []\n            index2 = line[3].index(line[1])\n            position2 = []\n\n            for i,word in enumerate(line[3]):\n                sentence.append(word)\n                position1.append(i-3-index1)\n                position2.append(i-3-index2)\n                i+=1\n            datas.append(sentence)\n            labels.append(relation2id[line[2]])\n            positionE1.append(position1)\n            positionE2.append(position2)\n        count[relation2id[line[2]]]+=1\n        \n        \n        \ndf_data = pd.DataFrame({\'words\': datas, \'tags\': labels,\'positionE1\':positionE1,\'positionE2\':positionE2}, index=range(len(datas)))\ndf_data[\'words\'] = df_data[\'words\'].apply(X_padding)\ndf_data[\'tags\'] = df_data[\'tags\']\ndf_data[\'positionE1\'] = df_data[\'positionE1\'].apply(position_padding)\ndf_data[\'positionE2\'] = df_data[\'positionE2\'].apply(position_padding)\n\ndatas = np.asarray(list(df_data[\'words\'].values))\nlabels = np.asarray(list(df_data[\'tags\'].values))\npositionE1 = np.asarray(list(df_data[\'positionE1\'].values))\npositionE2 = np.asarray(list(df_data[\'positionE2\'].values))\n\n\n\nimport pickle\nwith open(\'../people_relation_test.pkl\', \'wb\') as outp:\n\tpickle.dump(datas, outp)\n\tpickle.dump(labels, outp)\n\tpickle.dump(positionE1, outp)\n\tpickle.dump(positionE2, outp)\nprint \'** Finished saving the data.\'        \n        \n\n'"
