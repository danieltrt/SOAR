file_path,api_count,code
datasets.py,2,"b'import os\nimport torch\n\nimport torch.utils.data as data\n\nfrom PIL import Image\n\n\nclass CocoDetection(data.Dataset):\n    """"""`MS Coco Captions <http://mscoco.org/dataset/#detections-challenge2016>`_ Dataset.\n    Args:\n        root (string): Root directory where images are downloaded to.\n        annFile (string): Path to json annotation file.\n        transform (callable, optional): A function/transform that  takes in an PIL image\n            and returns a transformed version. E.g, ``transforms.ToTensor``\n        target_transform (callable, optional): A function/transform that takes in the\n            target and transforms it.\n    """"""\n\n    def __init__(self, root, annFile, transform=None, target_transform=None):\n\n        from pycocotools.coco import COCO\n\n        self.root = root\n        self.coco = COCO(annFile)\n        self.ids = list(self.coco.imgs.keys())\n        self.transform = transform\n        self.target_transform = target_transform\n\n    def __getitem__(self, index):\n        """"""\n        Args:\n            index (int): Index\n        Returns:\n            tuple: Tuple (image, target). target is the object returned by ``coco.loadAnns``.\n        """"""\n        coco = self.coco\n        img_id = self.ids[index]\n        ann_ids = coco.getAnnIds(imgIds=img_id)\n        target = coco.loadAnns(ann_ids)\n\n        target = torch.unsqueeze(torch.Tensor(target[0][\'bbox\']), -1)\n\n        path = coco.loadImgs(img_id)[0][\'file_name\']\n\n        img = Image.open(os.path.join(self.root, path)).convert(\'RGB\')\n\n        if self.transform is not None:\n            img = self.transform(img)\n\n        if self.target_transform is not None:\n            target = self.target_transform(target)\n\n        return img, target\n\n    def __len__(self):\n        return len(self.ids)\n'"
eval.py,0,"b""def eval():\n    pass\n\n# make sure to set valital=False and use benchmark.\n\n# Batching might be faster\n\nif __name__ == '__main__':\n    pass\n"""
focal_loss.py,7,"b'import torch\n\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom torch.autograd import Variable\n\n\nclass FocalLoss(nn.Module):\n\n    def __init__(self, focusing_param=2, balance_param=0.25):\n        super(FocalLoss, self).__init__()\n\n        self.focusing_param = focusing_param\n        self.balance_param = balance_param\n\n    def forward(self, output, target):\n\n        cross_entropy = F.cross_entropy(output, target)\n        cross_entropy_log = torch.log(cross_entropy)\n        logpt = - F.cross_entropy(output, target)\n        pt    = torch.exp(logpt)\n\n        focal_loss = -((1 - pt) ** self.focusing_param) * logpt\n\n        balanced_focal_loss = self.balance_param * focal_loss\n\n        return balanced_focal_loss\n\n\ndef test_focal_loss():\n    loss = FocalLoss()\n\n    input = Variable(torch.randn(3, 5), requires_grad=True)\n    target = Variable(torch.LongTensor(3).random_(5))\n\n    print(input)\n    print(target)\n\n    output = loss(input, target)\n    print(output)\n    output.backward()\n'"
resnet_features.py,6,"b""import torch.utils.model_zoo as model_zoo\n\nfrom torchvision.models.resnet import BasicBlock, Bottleneck, ResNet\n\n\nclass BasicBlockFeatures(BasicBlock):\n    '''\n    BasicBlock that returns its last conv layer features.\n    '''\n\n    def forward(self, x):\n\n        if isinstance(x, tuple):\n            x = x[0]\n\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        conv2_rep = out\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out, conv2_rep\n\n\nclass BottleneckFeatures(Bottleneck):\n    '''\n    Bottleneck that returns its last conv layer features.\n    '''\n\n    def forward(self, x):\n\n        if isinstance(x, tuple):\n            x = x[0]\n\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        conv3_rep = out\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out, conv3_rep\n\n\nclass ResNetFeatures(ResNet):\n    '''\n    A ResNet that returns features instead of classification.\n    '''\n\n    def forward(self, x):\n\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x, c2 = self.layer1(x)\n        x, c3 = self.layer2(x)\n        x, c4 = self.layer3(x)\n        x, c5 = self.layer4(x)\n\n        return c2, c3, c4, c5\n\n\nmodel_urls = {\n    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n}\n\n\ndef resnet18_features(pretrained=False, **kwargs):\n    '''Constructs a ResNet-18 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    '''\n    model = ResNetFeatures(BasicBlockFeatures, [2, 2, 2, 2], **kwargs)\n\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls['resnet18']))\n\n    return model\n\n\ndef resnet34_features(pretrained=False, **kwargs):\n    '''Constructs a ResNet-34 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    '''\n    model = ResNetFeatures(BasicBlockFeatures, [3, 4, 6, 3], **kwargs)\n\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls['resnet34']))\n\n    return model\n\n\ndef resnet50_features(pretrained=False, **kwargs):\n    '''Constructs a ResNet-50 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    '''\n    model = ResNetFeatures(BottleneckFeatures, [3, 4, 6, 3], **kwargs)\n\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls['resnet50']))\n\n    return model\n\n\ndef resnet101_features(pretrained=False, **kwargs):\n    '''Constructs a ResNet-101 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    '''\n    model = ResNetFeatures(BottleneckFeatures, [3, 4, 23, 3], **kwargs)\n\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls['resnet101']))\n\n    return model\n\n\ndef resnet152_features(pretrained=False, **kwargs):\n    '''Constructs a ResNet-152 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    '''\n    model = ResNetFeatures(BottleneckFeatures, [3, 8, 36, 3], **kwargs)\n\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls['resnet152']))\n\n    return model\n"""
retinanet.py,7,"b""import math\nimport torch\n\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom torch.autograd import Variable\n\nfrom resnet_features import resnet50_features\nfrom utilities.layers import conv1x1, conv3x3\n\n\nclass FeaturePyramid(nn.Module):\n    def __init__(self, resnet):\n        super(FeaturePyramid, self).__init__()\n\n        self.resnet = resnet\n\n        # applied in a pyramid\n        self.pyramid_transformation_3 = conv1x1(512, 256)\n        self.pyramid_transformation_4 = conv1x1(1024, 256)\n        self.pyramid_transformation_5 = conv1x1(2048, 256)\n\n        # both based around resnet_feature_5\n        self.pyramid_transformation_6 = conv3x3(2048, 256, padding=1, stride=2)\n        self.pyramid_transformation_7 = conv3x3(256, 256, padding=1, stride=2)\n\n        # applied after upsampling\n        self.upsample_transform_1 = conv3x3(256, 256, padding=1)\n        self.upsample_transform_2 = conv3x3(256, 256, padding=1)\n\n    def _upsample(self, original_feature, scaled_feature, scale_factor=2):\n        # is this correct? You do lose information on the upscale...\n        height, width = scaled_feature.size()[2:]\n        return F.upsample(original_feature, scale_factor=scale_factor)[:, :, :height, :width]\n\n    def forward(self, x):\n\n        # don't need resnet_feature_2 as it is too large\n        _, resnet_feature_3, resnet_feature_4, resnet_feature_5 = self.resnet(x)\n\n        pyramid_feature_6 = self.pyramid_transformation_6(resnet_feature_5)\n        pyramid_feature_7 = self.pyramid_transformation_7(F.relu(pyramid_feature_6))\n\n        pyramid_feature_5 = self.pyramid_transformation_5(resnet_feature_5)\n        pyramid_feature_4 = self.pyramid_transformation_4(resnet_feature_4)\n        upsampled_feature_5 = self._upsample(pyramid_feature_5, pyramid_feature_4)\n\n        pyramid_feature_4 = self.upsample_transform_1(\n            torch.add(upsampled_feature_5, pyramid_feature_4)\n        )\n\n        pyramid_feature_3 = self.pyramid_transformation_3(resnet_feature_3)\n        upsampled_feature_4 = self._upsample(pyramid_feature_4, pyramid_feature_3)\n\n        pyramid_feature_3 = self.upsample_transform_2(\n            torch.add(upsampled_feature_4, pyramid_feature_3)\n        )\n\n        return (pyramid_feature_3,\n                pyramid_feature_4,\n                pyramid_feature_5,\n                pyramid_feature_6,\n                pyramid_feature_7)\n\n\nclass SubNet(nn.Module):\n\n    def __init__(self, mode, anchors=9, classes=80, depth=4,\n                 base_activation=F.relu,\n                 output_activation=F.sigmoid):\n        super(SubNet, self).__init__()\n        self.anchors = anchors\n        self.classes = classes\n        self.depth = depth\n        self.base_activation = base_activation\n        self.output_activation = output_activation\n\n        self.subnet_base = nn.ModuleList([conv3x3(256, 256, padding=1)\n                                          for _ in range(depth)])\n\n        if mode == 'boxes':\n            self.subnet_output = conv3x3(256, 4 * self.anchors, padding=1)\n        elif mode == 'classes':\n            # add an extra dim for confidence\n            self.subnet_output = conv3x3(256, (1 + self.classes) * self.anchors, padding=1)\n\n        self._output_layer_init(self.subnet_output.bias.data)\n\n    def _output_layer_init(self, tensor, pi=0.01):\n        fill_constant = - math.log((1 - pi) / pi)\n\n        if isinstance(tensor, Variable):\n            self._output_layer_init(tensor.data)\n\n        return tensor.fill_(fill_constant)\n\n    def forward(self, x):\n        for layer in self.subnet_base:\n            x = self.base_activation(layer(x))\n\n        x = self.subnet_output(x)\n        x = x.permute(0, 2, 3, 1).contiguous().view(x.size(0),\n                                                    x.size(2) * x.size(3) * self.anchors, -1)\n\n        return x\n\n\nclass RetinaNet(nn.Module):\n\n    def __init__(self, classes):\n        super(RetinaNet, self).__init__()\n        self.classes = classes\n\n        _resnet = resnet50_features(pretrained=True)\n        self.feature_pyramid = FeaturePyramid(_resnet)\n\n        self.subnet_boxes = SubNet(mode='boxes')\n        self.subnet_classes = SubNet(mode='classes')\n\n    def forward(self, x):\n\n        boxes = []\n        classes = []\n\n        features = self.feature_pyramid(x)\n\n        # how faster to do one loop\n        boxes = [self.subnet_boxes(feature) for feature in features]\n        classes = [self.subnet_classes(feature) for feature in features]\n\n        return torch.cat(boxes, 1), torch.cat(classes, 1)\n\n\nif __name__ == '__main__':\n    import time\n    net = RetinaNet(classes=80)\n    x = Variable(torch.rand(1, 3, 500, 500), volatile=True)\n\n    now = time.time()\n    predictions = net(x)\n    later = time.time()\n\n    print(later - now)\n\n    for prediction in predictions:\n        print(prediction.size())\n"""
train_coco.py,4,"b'import torch\nimport argparse\n\nimport torch.nn as nn\n\nfrom tqdm import tqdm\nfrom torch import optim\nfrom torchvision import transforms\nfrom torch.autograd import Variable\nfrom torch.optim import lr_scheduler\n\nfrom loss import FocalLoss\nfrom retinanet import RetinaNet\nfrom datasets import CocoDetection\n\n\ntrain_loader = torch.utils.data.DataLoader(\n    CocoDetection(root=""./datasets/COCO/train2017"",\n                  annFile=""./datasets/COCO/annotations/instances_train2017.json"",\n                  transform=transforms.Compose([\n                      transforms.ToTensor(),\n                      # normalized because of the pretrained imagenet\n                      transforms.Normalize((0.1307,), (0.3081,))\n                  ])),\n    # batch size should be 16\n    batch_size=1, shuffle=True)\n\nmodel = RetinaNet(classes=80)\nmodel.eval()\n\noptimizer = optim.SGD(model.parameters(),\n                      lr=0.01,\n                      momentum=0.9,\n                      weight_decay=0.0001)\n\nscheduler = lr_scheduler.MultiStepLR(optimizer,\n                                     # Milestones are set assuming batch size is 16:\n                                     # 60000 / batch_size = 3750\n                                     # 80000 / batch_size = 5000\n                                     milestones=[3750, 5000],\n                                     gamma=0.1)\n\n\ncriterion = FocalLoss(80)\n\n\ndef train(model, cuda=False):\n\n    average_loss = 0\n\n    if cuda:\n        model.cuda()\n        model = nn.DataParallel(model)\n\n    for current_batch, (images, box_targets, class_targets) in enumerate(\n            tqdm(train_loader, desc=\'Training on COCO\', unit=\'epoch\')):\n\n        scheduler.step()\n\n        optimizer.zero_grad()\n\n        if cuda:\n            images.cuda()\n            box_targets.cuda()\n            class_targets.cuda()\n\n        images = Variable(images)\n        # box_predictions = Variable(box_targets)\n        # class_predictions = Variable(class_targets)\n        box_predictions, classes_predictions = model(images)\n        \n        loss = criterion(box_predictions, box_targets, class_predictions, class_targets)\n        # loss.backwards()\n        loss.backward()\n\n        average_loss += loss[0]\n\n        # boxes, classes = model(images)\n\n        optimizer.step()\n\n        print(f\'Batch: {current_batch}, Loss: {loss[0]}, Average Loss: {average_loss / current_batch + 1}\')\n\n\nif __name__ == \'__main__\':\n    train(model)\n'"
utilities/__init__.py,0,b''
utilities/images.py,0,"b'from torchvision.transforms import Compose, Normalize, Scale, ToTensor\n\nfrom PIL import Image\n\nimg = Image.open(""../../Documents/2017_07/test/21094803716_da3cea21b8_o.jpg"")\n\nready_image = Compose([\n    Scale([224, 224]),\n    ToTensor(),\n    Normalize(mean=[0.485, 0.456, 0.406],\n              std=[0.229, 0.224, 0.225]),\n\n])\n\n\n'"
utilities/layers.py,1,"b""import torch.nn as nn\n\n\ndef init_conv_weights(layer, weights_std=0.01,  bias=0):\n    '''\n    RetinaNet's layer initialization\n\n    :layer\n    :\n\n    '''\n    nn.init.normal(layer.weight.data, std=weights_std)\n    nn.init.constant(layer.bias.data, val=bias)\n    return layer\n\n\ndef conv1x1(in_channels, out_channels, **kwargs):\n    '''Return a 1x1 convolutional layer with RetinaNet's weight and bias initialization'''\n\n    layer = nn.Conv2d(in_channels, out_channels, kernel_size=1, **kwargs)\n    layer = init_conv_weights(layer)\n\n    return layer\n\n\ndef conv3x3(in_channels, out_channels, **kwargs):\n    '''Return a 3x3 convolutional layer with RetinaNet's weight and bias initialization'''\n\n    layer = nn.Conv2d(in_channels, out_channels, kernel_size=3, **kwargs)\n    layer = init_conv_weights(layer)\n\n    return layer\n\n\n"""
