file_path,api_count,code
src/__init__.py,0,b''
src/configs.py,0,"b'best_bpsp = float(""inf"")\nn_feats = 64\nscale = 3\nresblocks = 3\nK = 10\nplot = """"\nlog_likelihood = True\ncollect_probs = False\n'"
src/data.py,19,"b'import io\nimport os\nfrom typing import Iterable, List, Tuple\n\nimport numpy as np\nimport PIL.Image as Image\nimport torch\nimport torch.utils.data as data\nfrom torch.nn import functional as F\n\nfrom src import configs, util\n\n\nclass PreemptiveRandomSampler(data.Sampler):\n    def __init__(self, indices: List[int], index: int) -> None:\n        super().__init__(""None"")\n        self.indices: List[int]\n        self.indices = indices\n        self.index = index\n\n    def __iter__(self) -> Iterable[int]:  # type: ignore\n        size = len(self.indices)\n        assert 0 <= self.index < size, \\\n            f""0 <= {self.index} < {size} violated""\n        while self.index < size:\n            yield self.indices[self.index]\n            self.index += 1\n        self.index = 0\n        self.indices = torch.randperm(size).tolist()\n\n    def __len__(self) -> int:\n        return len(self.indices)\n\n\ndef average_downsamples(x: torch.Tensor) -> List[torch.Tensor]:\n    downsampled = []\n    for _ in range(configs.scale):\n        downsampled.append(x.detach())\n        x = F.avg_pool2d(pad_to_even(util.tensor_round(x)), 2)\n    downsampled.append(x.detach())\n    return downsampled\n\n\nclass ImageFolder(data.Dataset):\n    """""" Generic Dataset class for a directory full of images given \n        a list of image filenames. Can be used for any unsupervised \n        learning task without labels.\n    """"""\n\n    def __init__(self,\n                 dir_path: str,\n                 filenames: List[str],\n                 scale: int,\n                 transforms,  # torchvision Transform\n                 ) -> None:\n        """""" param dir_path: Path to image directory\n            param filenames: List of image filenames in the directory.\n            param transforms: a torchvision Transform that can be applied to \n                the image.\n        """"""\n        self.scale = scale\n        self.dir_path = dir_path\n        self.filenames = filenames\n        assert filenames, f""{filenames} is empty""\n        self.transforms = transforms\n\n    def to_tensor_not_normalized(self, pic: Image) -> torch.Tensor:\n        """""" copied from PyTorch functional.to_tensor.\n            removed final .div(255.) """"""\n        if isinstance(pic, np.ndarray):\n            # handle numpy array\n            img = torch.from_numpy(pic.transpose((2, 0, 1)))\n            return img\n\n        # handle PIL Image\n        if pic.mode == \'I\':\n            img = torch.from_numpy(np.array(pic, np.int32, copy=False))\n        elif pic.mode == \'I;16\':\n            img = torch.from_numpy(np.array(pic, np.int16, copy=False))\n        elif pic.mode == \'F\':\n            img = torch.from_numpy(np.array(pic, np.float32, copy=False))\n        elif pic.mode == \'1\':\n            img = 255 * torch.from_numpy(np.array(pic, np.uint8, copy=False))\n        else:\n            img = torch.ByteTensor(  # type: ignore\n                torch.ByteStorage.from_buffer(pic.tobytes()))  # type: ignore\n        # PIL image mode: L, P, I, F, RGB, YCbCr, RGBA, CMYK\n        if pic.mode == \'YCbCr\':\n            nchannel = 3\n        elif pic.mode == \'I;16\':\n            nchannel = 1\n        else:\n            nchannel = len(pic.mode)\n        img = img.view(pic.size[1], pic.size[0], nchannel)\n        # put it from HWC to CHW format\n        # yikes, this transpose takes 80% of the loading time/CPU\n        img = img.transpose(0, 1).transpose(0, 2).contiguous()\n        return img.float()\n\n    def load(self,\n             fp: io.BytesIO\n             ) -> torch.Tensor:\n        # x: 3HW\n        img = Image.open(fp)\n        x = self.transforms(img)\n        return self.to_tensor_not_normalized(x)\n\n    def read(self, filename: str) -> bytes:\n        with open(os.path.join(self.dir_path, filename), ""rb"") as f:\n            return f.read()\n\n    def __getitem__(\n            self,\n            idx: int\n    ) -> Tuple[str, torch.Tensor]:  # type: ignore\n        filename = self.filenames[idx]\n        file_bytes = self.read(filename)\n        img_data = self.load(io.BytesIO(file_bytes))\n        return filename, img_data\n\n    def __len__(self) -> int:\n        return len(self.filenames)\n\n\ndef pad_to_even(x: torch.Tensor) -> torch.Tensor:\n    _, _, h, w = x.size()\n    pad_right = w % 2 == 1\n    pad_bottom = h % 2 == 1\n    padding = [0, 1 if pad_right else 0, 0, 1 if pad_bottom else 0]\n    x = F.pad(x, padding, mode=""replicate"")\n    return x\n\n\ndef pad(x: torch.Tensor, H: int, W: int) -> torch.Tensor:\n    _, _, xH, xW = x.size()\n    padding = [0, W - xW, 0, H - xH]\n    return F.pad(x, padding, mode=""replicate"")\n\n\ndef join_2x2(padded_slices: List[torch.Tensor],\n             shape: Tuple[int, int]\n             ) -> torch.Tensor:\n    assert len(padded_slices) == 4, len(padded_slices)\n    # 4 N 3 H W\n    x = torch.stack(padded_slices)\n    # N 3 4 H W\n    x = x.permute(1, 2, 0, 3, 4)\n    N, _, _, H, W = x.size()\n    # N 12 H W\n    x = x.contiguous().view(N, -1, H, W)\n    # N 3 2H 2W\n    x = F.pixel_shuffle(x, upscale_factor=2)\n    # return x[..., :unpad_h, :unpad_w]\n    return x[..., :shape[-2], :shape[-1]]\n\n\ndef get_shapes(H: int, W: int) -> List[Tuple[int, int]]:\n    shapes = [(H, W)]\n    h = H\n    w = W\n    for _ in range(3):\n        h = (h + 1) // 2\n        w = (w + 1) // 2\n        shapes.append((h, w))\n    return shapes\n\n\ndef get_2x2_shapes(\n        H: int, W: int\n) -> Tuple[Tuple[int, int], Tuple[int, int], Tuple[int, int], Tuple[int, int]]:\n    top_H = (H + 1) // 2\n    left_W = (W + 1) // 2\n    bottom_H = H - top_H\n    right_W = W - left_W\n    return (\n        (top_H, left_W), (top_H, right_W),\n        (bottom_H, left_W), (bottom_H, right_W))\n'"
src/decode.py,1,"b'import os\nimport sys\n\nimport click\nimport numpy as np\nimport torch\nimport torchvision.transforms as T\nfrom PIL import ImageFile\n\nfrom src import configs, network\nfrom src.l3c import bitcoding, timer\n\n\n@click.command()\n@click.option(""--path"", type=click.Path(exists=True),\n              help=""Directory of images."")\n@click.option(""--file"", type=click.File(""r""),\n              help=""File for image names."")\n@click.option(""--resblocks"", type=int, default=3, show_default=True,\n              help=""Number of resblocks to use."")\n@click.option(""--n-feats"", type=int, default=64, show_default=True,\n              help=""Size of n_feats vector used."")\n@click.option(""--scale"", type=int, default=3, show_default=True,\n              help=""Scale of downsampling"")\n@click.option(""--load"", type=click.Path(exists=True),\n              help=""Path to load model"")\n@click.option(""--K"", type=int, default=10,\n              help=""Number of clusters in logistic mixture model."")\n@click.option(""--save-path"", type=str,\n              help=""Save directory for images."")\ndef main(\n        path: str, file, resblocks: int, n_feats: int, scale: int,\n        load: str, k: int, save_path: str,\n) -> None:\n    ImageFile.LOAD_TRUNCATED_IMAGES = True\n\n    configs.n_feats = n_feats\n    configs.resblocks = resblocks\n    configs.K = k\n    configs.scale = scale\n    configs.collect_probs = False\n\n    print(sys.argv)\n\n    checkpoint = torch.load(load)\n    print(f""Loaded model from {load}."")\n    print(""Epoch:"", checkpoint[""epoch""])\n\n    compressor = network.Compressor()\n    compressor.nets.load_state_dict(checkpoint[""nets""])\n    compressor = compressor.cuda()\n    print(compressor.nets)\n\n    filenames = [filename.strip() for filename in file]\n    print(f""Loaded directory with {len(filenames)} images"")\n\n    os.makedirs(save_path, exist_ok=True)\n\n    coder = bitcoding.Bitcoding(compressor)\n    decoder_time_accumulator = timer.TimeAccumulator()\n    total_num_bytes = 0\n    total_num_subpixels = 0\n\n    for filename in filenames:\n        assert filename.endswith("".srec""), (\n            f""{filename} is not a .srec file"")\n        filepath = os.path.join(path, filename)\n        with decoder_time_accumulator.execute():\n            x = coder.decode(filepath)\n            x = x.byte().squeeze(0).cpu()\n        img = T.functional.to_pil_image(x)\n        img.save(os.path.join(save_path, f""{filename[:-5]}.png""))\n        print(\n            ""Decomp: ""\n            f""{decoder_time_accumulator.mean_time_spent():.3f};\\t""\n            ""Decomp Time By Scale: "",\n            end="""")\n        decomp_scale_times = coder.decomp_scale_times()\n        print(\n            "", "".join(f""{scale_time:.3f}"" for scale_time in decomp_scale_times),\n            end=""; "")\n\n        total_num_bytes += os.path.getsize(filepath)\n        total_num_subpixels += np.prod(x.size())\n\n        print(\n            f""Bpsp: {total_num_bytes*8/total_num_subpixels:.3f}"", end=""\\r"")\n    print()\n\n\nif __name__ == ""__main__"":\n    main()\n'"
src/encode.py,3,"b'import os\nimport sys\n\nimport click\nimport numpy as np\nimport torch\nimport torchvision.transforms as T\nfrom torch.utils import data\n\nfrom src import configs\nfrom src import data as lc_data\nfrom src import network\nfrom src.l3c import bitcoding, timer\n\n\n@click.command()\n@click.option(""--path"", type=click.Path(exists=True),\n              help=""Directory of images."")\n@click.option(""--file"", type=click.File(""r""),\n              help=""File for image names."")\n@click.option(""--resblocks"", type=int, default=3, show_default=True,\n              help=""Number of resblocks to use."")\n@click.option(""--n-feats"", type=int, default=64, show_default=True,\n              help=""Size of n_feats vector used."")\n@click.option(""--scale"", type=int, default=3, show_default=True,\n              help=""Scale of downsampling"")\n@click.option(""--load"", type=click.Path(exists=True),\n              help=""Path to load model"")\n@click.option(""--K"", type=int, default=10,\n              help=""Number of clusters in logistic mixture model."")\n@click.option(""--crop"", type=int, default=0,\n              help=""Size of image crops in training. 0 means no crop."")\n@click.option(""--log-likelihood"", is_flag=True, default=False,\n              help=""Turn on log-likelihood calculations."")\n@click.option(""--decode"", is_flag=True, default=False,\n              help=""Turn on decoding to verify coding correctness."")\n@click.option(""--save-path"", type=str,\n              help=""Save directory for images."")\ndef main(\n        path: str, file, resblocks: int, n_feats: int, scale: int,\n        load: str, k: int, crop: int,\n        log_likelihood: bool, decode: bool, save_path: str,\n) -> None:\n\n    configs.n_feats = n_feats\n    configs.resblocks = resblocks\n    configs.K = k\n    configs.scale = scale\n    configs.log_likelihood = log_likelihood\n    configs.collect_probs = True\n\n    print(sys.argv)\n\n    checkpoint = torch.load(load)\n    print(f""Loaded model from {load}."")\n    print(""Epoch:"", checkpoint[""epoch""])\n\n    compressor = network.Compressor()\n    compressor.nets.load_state_dict(checkpoint[""nets""])\n    compressor = compressor.cuda()\n\n    print(compressor.nets)\n\n    transforms = []  # type: ignore\n    if crop > 0:\n        transforms.insert(0, T.CenterCrop(crop))\n\n    dataset = lc_data.ImageFolder(\n        path, [filename.strip() for filename in file],\n        scale, T.Compose(transforms)\n    )\n    loader = data.DataLoader(\n        dataset, batch_size=1, shuffle=False,\n        num_workers=0, drop_last=False,\n    )\n    print(f""Loaded directory with {len(dataset)} images"")\n\n    os.makedirs(save_path, exist_ok=True)\n\n    coder = bitcoding.Bitcoding(compressor)\n    encoder_time_accumulator = timer.TimeAccumulator()\n    decoder_time_accumulator = timer.TimeAccumulator()\n    total_file_bytes = 0\n    total_num_subpixels = 0\n    total_entropy_coding_bytes: np.ndarray = 0  # type: ignore\n    total_log_likelihood_bits = network.Bits()\n\n    for i, (filenames, x) in enumerate(loader):\n        assert len(filenames) == 1, filenames\n        filename = filenames[0]\n        file_id = filename.split(""."")[0]\n        filepath = os.path.join(save_path, f""{file_id}.srec"")\n\n        with encoder_time_accumulator.execute():\n            log_likelihood_bits, entropy_coding_bytes = coder.encode(\n                x, filepath)\n\n        total_file_bytes += os.path.getsize(filepath)\n        total_entropy_coding_bytes += np.array(entropy_coding_bytes)\n        total_num_subpixels += np.prod(x.size())\n        if configs.log_likelihood:\n            total_log_likelihood_bits.add_bits(log_likelihood_bits)\n\n        if decode:\n            with decoder_time_accumulator.execute():\n                y = coder.decode(filepath)\n                y = y.cpu()\n            assert torch.all(x == y), (x[x != y], y[x != y])\n\n        if configs.log_likelihood:\n            theoretical_bpsp = total_log_likelihood_bits.get_total_bpsp(\n                total_num_subpixels).item()\n            print(\n                f""Theoretical Bpsp: {theoretical_bpsp:.3f};\\t"", end="""")\n        print(\n            f""Bpsp: {total_file_bytes*8/total_num_subpixels:.3f};\\t""\n            f""Images: {i + 1};\\t""\n            f""Comp: {encoder_time_accumulator.mean_time_spent():.3f};\\t"",\n            end="""")\n        if decode:\n            print(\n                ""Decomp: ""\n                f""{decoder_time_accumulator.mean_time_spent():.3f}"",\n                end="""")\n        print(end=""\\r"")\n    print()\n\n    if decode:\n        print(""Decomp Time By Scale: "", end="""")\n        print("", "".join(\n            f""{scale_time:.3f}""\n            for scale_time in coder.decomp_scale_times()))\n    else:\n        print(""Scale Bpsps: "", end="""")\n        print("", "".join(\n            f""{scale_bpsp:.3f}""\n            for scale_bpsp in total_entropy_coding_bytes*8/total_num_subpixels))\n\n\nif __name__ == ""__main__"":\n    main()\n'"
src/eval.py,3,"b'import sys\nfrom typing import Tuple\n\nimport click\nimport numpy as np\nimport torch\nimport torchvision.transforms as T\nfrom torch import nn\nfrom torch.utils import data\n\nfrom src import configs\nfrom src import data as lc_data\nfrom src import network\nfrom src.l3c import timer\n\n\ndef run_eval(\n        loader: data.DataLoader, compressor: nn.Module,\n) -> Tuple[network.Bits, int]:\n    """""" Runs entire eval epoch. """"""\n    time_accumulator = timer.TimeAccumulator()\n    compressor.eval()\n    cur_agg_size = 0\n\n    with torch.no_grad():\n        # BitsKeeper is used to aggregates bits from all eval iterations.\n        bits_keeper = network.Bits()\n        for i, (_, x) in enumerate(loader):\n            cur_agg_size += np.prod(x.size())\n            with time_accumulator.execute():\n                x = x.cuda()\n                bits = compressor(x)\n            bits_keeper.add_bits(bits)\n\n            bpsp = bits_keeper.get_total_bpsp(cur_agg_size)\n            print(\n                f""Bpsp: {bpsp.item():.3f}; Number of Images: {i + 1}; ""\n                f""Batch Time: {time_accumulator.mean_time_spent()}"",\n                end=""\\r"")\n\n        print()\n    return bits_keeper, cur_agg_size\n\n\ndef count_params(model: nn.Module) -> int:\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n\n@click.command()\n@click.option(""--path"", type=click.Path(exists=True),\n              help=""path to directory of eval images."")\n@click.option(""--file"", type=click.File(""r""),\n              help=""file for eval image names."")\n@click.option(""--workers"", type=int, default=2,\n              help=""Number of worker threads to use in dataloader."")\n@click.option(""--resblocks"", type=int, default=3, show_default=True,\n              help=""Number of resblocks to use."")\n@click.option(""--n-feats"", type=int, default=64, show_default=True,\n              help=""Size of n_feats vector used."")\n@click.option(""--scale"", type=int, default=3, show_default=True,\n              help=""Scale of downsampling"")\n@click.option(""--load"", type=click.Path(exists=True), default=""/dev/null"",\n              help=""Path to load model"")\n@click.option(""--K"", type=int, default=10,\n              help=""Number of clusters in logistic mixture model."")\n@click.option(""--crop"", type=int, default=0,\n              help=""Size of image crops in training. 0 means no crop."")\ndef main(\n        path: str, file, workers: int, resblocks: int, n_feats: int,\n        scale: int, load: str, k: int, crop: int,\n) -> None:\n\n    configs.n_feats = n_feats\n    configs.resblocks = resblocks\n    configs.K = k\n    configs.scale = scale\n\n    print(sys.argv)\n    print([item for item in dir(configs) if not item.startswith(""__"")])\n\n    if load != ""/dev/null"":\n        checkpoint = torch.load(load)\n        print(f""Loaded model from {load}."")\n        print(""Epoch:"", checkpoint[""epoch""])\n    else:\n        checkpoint = {}\n\n    compressor = network.Compressor()\n    if checkpoint:\n        compressor.nets.load_state_dict(checkpoint[""nets""])\n    compressor = compressor.cuda()\n\n    print(f""Number of parameters: {count_params(compressor.nets)}"")\n    print(compressor.nets)\n\n    transforms = []  # type: ignore\n    if crop > 0:\n        transforms.insert(0, T.CenterCrop(crop))\n\n    dataset = lc_data.ImageFolder(\n        path, [filename.strip() for filename in file],\n        scale, T.Compose(transforms)\n    )\n    loader = data.DataLoader(\n        dataset, batch_size=1, shuffle=False,\n        num_workers=workers, drop_last=False,\n    )\n    print(f""Loaded dataset with {len(dataset)} images"")\n\n    bits, inp_size = run_eval(loader, compressor)\n    for key in bits.get_keys():\n        print(f""{key}:"", bits.get_scaled_bpsp(key, inp_size))\n\n\nif __name__ == ""__main__"":\n    main()\n'"
src/network.py,37,"b'from collections import defaultdict\nfrom typing import (DefaultDict, Generator, KeysView, List, NamedTuple,\n                    Optional, Tuple)\n\nimport numpy as np\nimport torch\nfrom torch import nn\n\nfrom src import configs, data, util\nfrom src.l3c import edsr\nfrom src.l3c import logistic_mixture as lm\nfrom src.l3c import prob_clf, quantizer\n\n\nclass LogisticMixtureProbability(NamedTuple):\n    name: str\n    pixel_index: int\n    probs: torch.Tensor\n    lower: torch.Tensor\n    upper: torch.Tensor\n\n\nProbs = Tuple[torch.Tensor, Optional[LogisticMixtureProbability], int]\n\n\nclass Bits:\n    """"""\n    Tracks bpsps from different parts of the pipeline for one forward pass.\n    """"""\n\n    def __init__(self) -> None:\n        assert configs.collect_probs or configs.log_likelihood, (\n            configs.collect_probs, configs.log_likelihood)\n        self.key_to_bits: DefaultDict[\n            str, torch.Tensor] = defaultdict(float)  # type: ignore\n        self.key_to_sizes: DefaultDict[str, int] = defaultdict(int)\n        self.probs: List[Probs] = []\n\n    def add_with_size(\n            self, key: str, nll_sum: torch.Tensor, size: int,\n    ) -> None:\n        if configs.log_likelihood:\n            assert key not in self.key_to_bits, f""{key} already exists""\n            # Divide by np.log(2) to convert from natural log to log base 2\n            self.key_to_bits[key] = nll_sum / np.log(2)\n            self.key_to_sizes[key] = size\n\n    def add(self, key: str, nll: torch.Tensor) -> None:\n        self.add_with_size(\n            key, nll.sum(), np.prod(nll.size()))\n\n    def add_lm(\n            self, y_i: torch.Tensor,\n            lm_probs: LogisticMixtureProbability,\n            loss_fn: lm.DiscretizedMixLogisticLoss) -> None:\n        assert lm_probs.probs.shape[-2:] == y_i.shape[-2:], (\n            lm_probs.probs.shape, y_i.shape)\n        if configs.log_likelihood:\n            nll = loss_fn(y_i, lm_probs.probs)\n            self.add(lm_probs.name, nll)\n        if configs.collect_probs:\n            self.probs.append((y_i, lm_probs, -1))\n\n    def add_uniform(\n            self,\n            key: str,\n            y_i: torch.Tensor,\n            levels: int = 256) -> None:\n        if configs.log_likelihood:\n            size = np.prod(y_i.size())\n            nll_sum = np.log(levels) * size\n            self.add_with_size(key, nll_sum, size)\n        if configs.collect_probs:\n            self.probs.append((y_i, None, levels))\n\n    def get_bits(self, key: str) -> torch.Tensor:\n        return self.key_to_bits[key]\n\n    def get_size(self, key: str) -> int:\n        return self.key_to_sizes[key]\n\n    def get_keys(self) -> KeysView:\n        return self.key_to_bits.keys()\n\n    def get_self_bpsp(self, key: str) -> torch.Tensor:\n        return self.key_to_bits[key]/self.key_to_sizes[key]\n\n    def get_scaled_bpsp(self, key: str, inp_size: int) -> torch.Tensor:\n        return self.key_to_bits[key]/inp_size\n\n    def get_total_bpsp(self, inp_size: int) -> torch.Tensor:\n        return sum(self.key_to_bits.values()) / inp_size  # type: ignore\n\n    def update(self, other: ""Bits"") -> ""Bits"":\n        # Used by Compressor to aggregate bits from decoder.\n        assert len(self.get_keys() & other.get_keys()) == 0, \\\n            f""{self.get_keys()} and {other.get_keys()} intersect.""\n        self.key_to_bits.update(other.key_to_bits)\n        self.key_to_sizes.update(other.key_to_sizes)\n        self.probs += other.probs\n        return self\n\n    def add_bits(self, other: ""Bits"") -> ""Bits"":\n        keys = other.get_keys()\n        assert keys == self.get_keys() or len(self.get_keys()) == 0, (\n            f""{self.get_keys()} != {keys}"")\n\n        for key in keys:\n            self.key_to_bits[key] += other.get_bits(key)\n            self.key_to_sizes[key] += other.get_size(key)\n            # Don\'t do anything with self.key_to_probs at the moment.\n        return self\n\n\nclass PixDecoder(nn.Module):\n    """""" Super-resolution based decoder for pixel-based factorization. """"""\n\n    def __init__(self, scale: int) -> None:\n        super().__init__()\n        self.loss_fn = lm.DiscretizedMixLogisticLoss(rgb_scale=True)\n        self.scale = scale\n\n    def forward_probs(\n            self,\n            x: torch.Tensor,\n            ctx: torch.Tensor\n    ) -> Generator[LogisticMixtureProbability, torch.Tensor,\n                   Tuple[torch.Tensor, torch.Tensor]]:\n        raise NotImplementedError\n\n    def forward(self,  # type: ignore\n                x: torch.Tensor,\n                y: torch.Tensor,\n                ctx: torch.Tensor,\n                ) -> Tuple[Bits, torch.Tensor]:\n        bits = Bits()\n\n        # Check y are filled with integers.\n        # y.long().float() == y\n        if __debug__:\n            not_int = y.long().float() != y\n            assert not torch.any(not_int), y[not_int]\n\n        # mode is used to key tensorboard loggings\n        mode = ""train"" if self.training else ""eval""\n        deltas = x - util.tensor_round(x)\n        bits.add_uniform(\n            f""{mode}/{self.scale}_rounding"",\n            quantizer.to_sym(deltas, x_min=-0.25, x_max=0.5, L=4),\n            levels=4)\n\n        _, _, x_h, x_w = x.size()\n        if not isinstance(ctx, float):\n            ctx = ctx[..., :x_h, :x_w]\n\n        # Divide pixels of y into 2x2 grids and slice y by pixels on\n        # different util of grid.\n        # y: N 3 H W -> N 4 3 H/2 W/2\n        y_slices = group_2x2(y)\n\n        gen = self.forward_probs(x, ctx)\n        try:\n            for i, y_slice in enumerate(y_slices):\n                if i == 0:\n                    lm_probs = next(gen)\n                else:\n                    lm_probs = gen.send(y_slices[i-1])\n                _, _, h, w = y_slice.size()\n                lm_probs = LogisticMixtureProbability(\n                    name=lm_probs.name,\n                    pixel_index=lm_probs.pixel_index,\n                    probs=lm_probs.probs[..., :h, :w],\n                    lower=lm_probs.lower[..., :h, :w],\n                    upper=lm_probs.upper[..., :h, :w])\n                bits.add_lm(y_slice, lm_probs, self.loss_fn)\n        except StopIteration as e:\n            last_pixels, ctx = e.value\n            last_slice = y_slices[-1]\n            _, _, last_h, last_w = last_slice.size()\n            last_pixels = last_pixels[..., : last_h, : last_w]\n            assert torch.all(last_pixels == last_slice), (\n                last_pixels[last_pixels != last_slice],\n                last_slice[last_pixels != last_slice])\n\n        return bits, ctx\n\n\nclass StrongPixDecoder(PixDecoder):\n    def __init__(self, scale: int) -> None:\n        super().__init__(scale)\n        # Input: N 3 H W\n        # Output: N C H W\n        self.rgb_decs = nn.ModuleList([\n            edsr.EDSRDec(\n                3*i, configs.n_feats,\n                resblocks=configs.resblocks, tail=""conv"")\n            for i in range(1, 4)\n        ])\n        self.mix_logits_prob_clf = nn.ModuleList([\n            prob_clf.AtrousProbabilityClassifier(\n                configs.n_feats, C=3, K=configs.K,\n                num_params=self.loss_fn._num_params)\n            for _ in range(1, 4)\n        ])\n        self.feat_convs = nn.ModuleList([\n            util.conv(configs.n_feats, configs.n_feats, 3) for _ in range(1, 4)\n        ])\n        assert (len(self.rgb_decs) == len(self.mix_logits_prob_clf) ==\n                len(self.feat_convs)), (\n                    f""{len(self.rgb_decs)}, ""\n                    f""{len(self.mix_logits_prob_clf)}, {len(self.feat_convs)}""\n        )\n\n    def forward_probs(\n            self,\n            x: torch.Tensor,\n            ctx: torch.Tensor,\n    ) -> Generator[LogisticMixtureProbability, torch.Tensor,\n                   Tuple[torch.Tensor, torch.Tensor]]:\n        # mode is used to key tensorboard loggings\n        mode = ""train"" if self.training else ""eval""\n        # x: N 3 H W, [0, 255]\n        # pix_sum: N 3 H W, [0, 1020]\n        pix_sum = x * 4\n        xy_normalized = x / 127.5 - 1\n        y_i = torch.tensor([], device=x.device)\n        z: torch.Tensor = 0.  # type: ignore\n\n        for i, (rgb_dec, clf, feat_conv) in enumerate(\n                zip(self.rgb_decs,  # type: ignore\n                    self.mix_logits_prob_clf, self.feat_convs)):\n            xy_normalized = torch.cat((xy_normalized, y_i / 127.5 - 1), dim=1)\n            z = rgb_dec(xy_normalized, ctx)\n            ctx = feat_conv(z)\n\n            probs = clf(z)\n            lower = torch.max(\n                pix_sum - (3 - i) * 255, torch.tensor(0., device=x.device))\n            upper = torch.min(\n                pix_sum, torch.tensor(255., device=x.device))\n\n            y_i = yield LogisticMixtureProbability(\n                f""{mode}/{self.scale}_{i}"", i, probs, lower, upper)\n            y_i = data.pad(y_i, x.shape[-2], x.shape[-1])\n            pix_sum -= y_i\n\n        # Last pixel in 2x2 grid should be <= 255 and >= 0\n        return pix_sum, ctx\n\n\ndef group_2x2(\n        x: torch.Tensor\n) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n    """""" Group 2x2 patches of x on its own channel\n        param x: N C H W\n        returns: Tuple[N 4 C H/2 W/2]\n    """"""\n    _, _, h, w = x.size()\n    # assert h % 2 == 0, f""{x.shape} does not satisfy h % 2 == 0""\n    # assert w % 2 == 0, f""{x.shape} does not satisfy w % 2 == 0""\n    x_even_height = x[:, :, 0:h:2, :]\n    x_odd_height = x[:, :, 1:h:2, :]\n    return (\n        x_even_height[:, :, :, 0:w:2],\n        x_even_height[:, :, :, 1:w:2],\n        x_odd_height[:, :, :, 0:w:2],\n        x_odd_height[:, :, :, 1:w:2])\n\n\nclass Compressor(nn.Module):\n    def __init__(self) -> None:\n        super().__init__()\n        assert configs.scale >= 0, configs.scale\n\n        self.loss_fn = lm.DiscretizedMixLogisticLoss(rgb_scale=True)\n        self.ctx_upsamplers = nn.ModuleList([\n            nn.Identity(),  # type: ignore\n            *[edsr.Upsampler(scale=2, n_feats=configs.n_feats)\n              for _ in range(configs.scale-1)]\n        ] if configs.scale > 0 else [])\n        self.decs = nn.ModuleList([\n            StrongPixDecoder(i) for i in range(configs.scale)\n        ])\n        assert len(self.ctx_upsamplers) == len(self.decs), \\\n            f""{len(self.ctx_upsamplers)}, {len(self.decs)}""\n        self.nets = nn.ModuleList([\n            self.ctx_upsamplers, self.decs,\n        ])\n\n    def forward(self,  # type: ignore\n                x: torch.Tensor\n                ) -> Bits:\n        downsampled = data.average_downsamples(x)\n        assert len(downsampled)-1 == len(self.decs), (\n            f""{len(downsampled)-1}, {len(self.decs)}"")\n\n        mode = ""train"" if self.training else ""eval""\n        bits = Bits()\n        bits.add_uniform(f""{mode}/codes_0"", util.tensor_round(downsampled[-1]))\n\n        ctx = 0.\n        for dec, ctx_upsampler, x, y, in zip(  # type: ignore\n                self.decs, self.ctx_upsamplers,\n                downsampled[::-1], downsampled[-2::-1]):\n            ctx = ctx_upsampler(ctx)\n            dec_bits, ctx = dec(x, util.tensor_round(y), ctx)\n            bits.update(dec_bits)\n        return bits\n'"
src/train.py,7,"b'import os\nimport sys\nfrom typing import List\n\nimport click\nimport numpy as np\nimport torch\nimport torchvision.transforms as T\nfrom PIL import ImageFile\nfrom torch import nn, optim\nfrom torch.utils import data, tensorboard\n\nfrom src import configs\nfrom src import data as lc_data\nfrom src import network\nfrom src.l3c import timer\n\n\ndef plot_bpsp(\n        plotter: tensorboard.SummaryWriter, bits: network.Bits,\n        inp_size: int, train_iter: int\n) -> None:\n    """""" Plot bpsps for all keys on tensorboard.\n        bpsp: bits per subpixel/bits per dimension\n        There are 2 bpsps per key:\n        self_bpsp: bpsp based on dimension of log-likelihood tensor.\n            Measures bits if log-likelihood tensor is final scale.\n        scaled_bpsp: bpsp based on dimension of original image.\n            Measures how many bits we contribute to the total bpsp.\n\n        param plotter: tensorboard logger\n        param bits: bpsp aggregator\n        param inp_size: product of dims of original image\n        param train_iter: current training iteration\n        returns: None\n    """"""\n    for key in bits.get_keys():\n        plotter.add_scalar(\n            f""{key}_self_bpsp"", bits.get_self_bpsp(key).item(), train_iter)\n        plotter.add_scalar(\n            f""{key}_scaled_bpsp"",\n            bits.get_scaled_bpsp(key, inp_size).item(), train_iter)\n\n\ndef train_loop(\n        x: torch.Tensor, compressor: nn.Module,\n        optimizer: optim.Optimizer,  # type: ignore\n        train_iter: int, plotter: tensorboard.SummaryWriter,\n        plot_iters: int, clip: float,\n) -> None:\n    """""" Training loop for one batch. Computes grads and runs optimizer.\n    """"""\n    compressor.train()\n    optimizer.zero_grad()\n    inp_size = np.prod(x.size())\n    x = x.cuda()\n    bits = compressor(x)\n    total_loss = bits.get_total_bpsp(inp_size)\n    total_loss.backward()\n    grad_norm = nn.utils.clip_grad_norm_(compressor.parameters(), clip)\n    optimizer.step()\n\n    if train_iter % plot_iters == 0:\n        plotter.add_scalar(\n            ""train/bpsp"", total_loss.item(), train_iter)\n        # Plots gradident norm pre-clipping.\n        plotter.add_scalar(""train/grad_norm"", grad_norm, train_iter)\n        plot_bpsp(plotter, bits, inp_size, train_iter)\n\n\ndef run_eval(\n        eval_loader: data.DataLoader, compressor: nn.Module,\n        train_iter: int, plotter: tensorboard.SummaryWriter,\n        epoch: int,\n) -> None:\n    """""" Runs entire eval epoch. """"""\n    time_accumulator = timer.TimeAccumulator()\n    compressor.eval()\n    inp_size = 0\n\n    with torch.no_grad():\n        # BitsKeeper is used to aggregates bits from all eval iterations.\n        bits_keeper = network.Bits()\n        for _, x in eval_loader:\n            inp_size += np.prod(x.size())\n            with time_accumulator.execute():\n                x = x.cuda()\n                bits = compressor(x)\n            bits_keeper.add_bits(bits)\n\n        total_bpsp = bits_keeper.get_total_bpsp(inp_size)\n\n        eval_bpsp = total_bpsp.item()\n        print(f""Iteration {train_iter} bpsp: {total_bpsp}"")\n        plotter.add_scalar(\n            ""eval/bpsp"", eval_bpsp, train_iter)\n        plotter.add_scalar(\n            ""eval/batch_time"", time_accumulator.mean_time_spent(), train_iter)\n        plot_bpsp(plotter, bits_keeper, inp_size, train_iter)\n\n        if configs.best_bpsp > eval_bpsp:\n            configs.best_bpsp = eval_bpsp\n            torch.save(\n                {""nets"": compressor.nets.state_dict(),  # type: ignore\n                 ""best_bpsp"": configs.best_bpsp,\n                 ""epoch"": epoch},\n                os.path.join(configs.plot, ""best.pth""))\n\n\ndef save(compressor: network.Compressor,\n         sampler_indices: List[int],\n         index: int,\n         epoch: int,\n         train_iter: int,\n         plot: str,\n         filename: str) -> None:\n    """""" Checkpoints training such that the entire training state\n        can be restored. Reason we need this is because condor\n        cluster can preempt jobs.\n\n        param compressor: Contains all of our networks.\n        param sampler_indices: Random indices of dataset\n            produced by our Sampler, which prevents us from having\n            unbalanced sampling of our dataset when restoring. Important \n            because our number of epochs is low.\n        param index: Current index of indices in Sampler. Tells which \n            part of dataset is sampled and which part is not.\n        param train_iter: Train iteration at which model is last trained.\n        param epoch: Current training epoch. \n        param plot: Directory to store checkpoint.\n        param filename: Checkpoint filename.\n    """"""\n    torch.save({\n        ""nets"": compressor.nets.state_dict(),\n        ""sampler_indices"": sampler_indices,\n        ""index"": index,\n        ""epoch"": epoch,\n        ""train_iter"": train_iter,\n        ""best_bpsp"": configs.best_bpsp,\n    }, os.path.join(plot, filename))\n\n\n@click.command()\n@click.option(""--train-path"", type=click.Path(exists=True),\n              help=""path to directory of training images."")\n@click.option(""--eval-path"", type=click.Path(exists=True),\n              help=""path to directory of eval images."")\n@click.option(""--train-file"", type=click.File(""r""),\n              help=""file for training image names."")\n@click.option(""--eval-file"", type=click.File(""r""),\n              help=""file for eval image names."")\n@click.option(""--batch"", type=int, help=""Batch size for training."")\n@click.option(""--workers"", type=int, default=1,\n              help=""Number of worker threads to use in dataloader."")\n@click.option(""--plot"", type=str,\n              help=""path to store tensorboard run data/plots."")\n@click.option(""--epochs"", type=int, default=50, show_default=True,\n              help=""Number of epochs to run."")\n@click.option(""--resblocks"", type=int, default=5, show_default=True,\n              help=""Number of resblocks to use."")\n@click.option(""--n-feats"", type=int, default=64, show_default=True,\n              help=""Size of feature vector/channel width."")\n@click.option(""--scale"", type=int, default=3, show_default=True,\n              help=""Scale of downsampling"")\n@click.option(""--load"", type=click.Path(exists=True), default=""/dev/null"",\n              help=""Path to load model"")\n@click.option(""--lr"", type=float, default=1e-4, help=""Learning rate"")\n@click.option(""--eval-iters"", type=int, default=0,\n              help=""Number of train iterations per evaluation. ""\n                   ""If 0, then evaluate at the end of every epoch."")\n@click.option(""--lr-epochs"", type=int, default=1,\n              help=""Number of epochs before multiplying learning rate by 0.75"")\n@click.option(""--plot-iters"", type=int, default=1000,\n              help=""Number of train iterations before plotting data"")\n@click.option(""--K"", type=int, default=10,\n              help=""Number of clusters in logistic mixture model."")\n@click.option(""--clip"", type=float, default=0.5,\n              help=""Norm to clip by for gradient clipping."")\n@click.option(""--crop"", type=int, default=128,\n              help=""Size of image crops in training."")\n@click.option(""--gd"", type=click.Choice([""sgd"", ""adam"", ""rmsprop""]), default=""adam"",\n              help=""Type of gd to use."")\ndef main(\n        train_path: str, eval_path: str, train_file, eval_file,\n        batch: int, workers: int, plot: str, epochs: int,\n        resblocks: int, n_feats: int, scale: int, load: str,\n        lr: float, eval_iters: int, lr_epochs: int,\n        plot_iters: int, k: int, clip: float,\n        crop: int, gd: str,\n) -> None:\n    ImageFile.LOAD_TRUNCATED_IMAGES = True\n\n    configs.n_feats = n_feats\n    configs.scale = scale\n    configs.resblocks = resblocks\n    configs.K = k\n    configs.plot = plot\n\n    print(sys.argv)\n\n    os.makedirs(plot, exist_ok=True)\n    model_load = os.path.join(plot, ""train.pth"")\n    if os.path.isfile(model_load):\n        load = model_load\n    if os.path.isfile(load) and load != ""/dev/null"":\n        checkpoint = torch.load(load)\n        print(f""Loaded model from {load}."")\n        print(""Epoch:"", checkpoint[""epoch""])\n        if checkpoint.get(""best_bpsp"") is None:\n            print(""Warning: best_bpsp not found!"")\n        else:\n            configs.best_bpsp = checkpoint[""best_bpsp""]\n            print(""Best bpsp:"", configs.best_bpsp)\n    else:\n        checkpoint = {}\n\n    compressor = network.Compressor()\n    if checkpoint:\n        compressor.nets.load_state_dict(checkpoint[""nets""])\n    compressor = compressor.cuda()\n\n    optimizer: optim.Optimizer  # type: ignore\n    if gd == ""adam"":\n        optimizer = optim.Adam(compressor.parameters(), lr=lr, weight_decay=0)\n    elif gd == ""sgd"":\n        optimizer = optim.SGD(compressor.parameters(), lr=lr,\n                              momentum=0.9, nesterov=True)\n    elif gd == ""rmsprop"":\n        optimizer = optim.RMSprop(  # type: ignore\n            compressor.parameters(), lr=lr)\n    else:\n        raise NotImplementedError(gd)\n\n    starting_epoch = checkpoint.get(""epoch"") or 0\n\n    print(compressor)\n\n    train_dataset = lc_data.ImageFolder(\n        train_path,\n        [filename.strip() for filename in train_file],\n        scale,\n        T.Compose([\n            T.RandomHorizontalFlip(),\n            T.RandomCrop(crop),\n        ]),\n    )\n    dataset_index = checkpoint.get(""index"") or 0\n    train_sampler = lc_data.PreemptiveRandomSampler(\n        checkpoint.get(""sampler_indices"") or torch.randperm(\n            len(train_dataset)).tolist(),\n        dataset_index,\n    )\n    train_loader = data.DataLoader(\n        train_dataset, batch_size=batch, sampler=train_sampler,\n        num_workers=workers, drop_last=True,\n    )\n    print(f""Loaded training dataset with {len(train_loader)} batches ""\n          f""and {len(train_loader.dataset)} images"")\n    eval_dataset = lc_data.ImageFolder(\n        eval_path, [filename.strip() for filename in eval_file],\n        scale,\n        T.Lambda(lambda x: x),\n    )\n    eval_loader = data.DataLoader(\n        eval_dataset, batch_size=1, shuffle=False,\n        num_workers=workers, drop_last=False,\n    )\n    print(f""Loaded eval dataset with {len(eval_loader)} batches ""\n          f""and {len(eval_dataset)} images"")\n\n    lr_scheduler = optim.lr_scheduler.StepLR(\n        optimizer, lr_epochs, gamma=0.75)\n\n    for _ in range(starting_epoch):\n        lr_scheduler.step()  # type: ignore\n\n    train_iter = checkpoint.get(""train_iter"") or 0\n    if eval_iters == 0:\n        eval_iters = len(train_loader)\n\n    for epoch in range(starting_epoch, epochs):\n        with tensorboard.SummaryWriter(plot) as plotter:\n            # input: List[Tensor], downsampled images.\n            # sizes: N scale 4\n            for _, inputs in train_loader:\n                train_iter += 1\n                batch_size = inputs[0].shape[0]\n\n                train_loop(inputs, compressor, optimizer, train_iter,\n                           plotter, plot_iters, clip)\n                # Increment dataset_index before checkpointing because\n                # dataset_index is starting index of index of the FIRST\n                # unseen piece of data.\n                dataset_index += batch_size\n\n                if train_iter % plot_iters == 0:\n                    plotter.add_scalar(\n                        ""train/lr"",\n                        lr_scheduler.get_lr()[0],  # type: ignore\n                        train_iter)\n                    save(compressor, train_sampler.indices, dataset_index,\n                         epoch, train_iter, plot, ""train.pth"")\n\n                if train_iter % eval_iters == 0:\n                    run_eval(\n                        eval_loader, compressor, train_iter,\n                        plotter, epoch)\n\n            lr_scheduler.step()  # type: ignore\n            dataset_index = 0\n\n    with tensorboard.SummaryWriter(plot) as plotter:\n        run_eval(eval_loader, compressor, train_iter,\n                 plotter, epochs)\n    save(compressor, train_sampler.indices, train_sampler.index,\n         epochs, train_iter, plot, ""train.pth"")\n    print(""training done"")\n\n\nif __name__ == ""__main__"":\n    main()\n'"
src/util.py,3,"b'import torch\nimport torch.nn as nn\n\n\ndef conv(in_channels: int,\n         out_channels: int,\n         kernel_size: int,\n         bias: bool = True,\n         rate: int = 1,\n         stride: int = 1) -> nn.Conv2d:\n    padding = kernel_size // 2 if rate == 1 else rate\n    return nn.Conv2d(\n        in_channels, out_channels, kernel_size, stride=stride, dilation=rate,\n        padding=padding, bias=bias)\n\n\ndef tensor_round(x: torch.Tensor) -> torch.Tensor:\n    return torch.round(x - 0.001)\n'"
src/l3c/__init__.py,0,b''
src/l3c/bitcoding.py,19,"b'""""""\nCopyright 2019, ETH Zurich\n\nThis file is part of L3C-PyTorch.\n\nL3C-PyTorch is free software: you can redistribute it and/or modify\nit under the terms of the GNU General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or\nany later version.\n\nL3C-PyTorch is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with L3C-PyTorch.  If not, see <https://www.gnu.org/licenses/>.\n""""""\nimport os\nfrom typing import List, Tuple, Union\n\nimport numpy as np\nimport torch\nfrom torch.nn import functional as F\n\nfrom src import configs\nfrom src import data as lc_data\nfrom src import network\nfrom src.l3c import coders, coders_helpers\nfrom src.l3c import logistic_mixture as lm\nfrom src.l3c import quantizer, timer\n\n\nclass Bitcoding(object):\n    """"""\n    Class to encode an image to a file and decode it. Saves timings of \n    individual steps to `times`.\n    If `compare_with_theory = True`, also compares actual bitstream \n    size to size predicted by cross entropy. Note\n    that this is slower because we need to evaluate the loss.\n    """"""\n\n    def __init__(\n            self,\n            compressor: network.Compressor,\n    ) -> None:\n        self.compressor = compressor\n        self.total_num_bytes = 0\n        self.total_num_subpixels = 0\n        self.log_likelihood_bits = network.Bits()\n        self.file_sizes: np.ndarray = 0.  # type: ignore\n        self.scale_timers = [\n            timer.TimeAccumulator() for _ in range(configs.scale + 1)]\n\n    def encode(\n            self, x: torch.Tensor, pout: str\n    ) -> Tuple[network.Bits, List[int]]:\n        """"""\n        Encode image to disk at path `p`.\n        :param img: uint8 tensor of shape CHW or 1CHW\n        :param pout: path\n        """"""\n        assert not os.path.isfile(pout), f""{pout} already exists""\n        assert x.dtype == torch.float32\n        x = x.cuda()\n        self.compressor.eval()\n\n        with torch.no_grad():\n            bits = self.compressor(x)\n\n        entropy_coding_bytes = []  # bytes used by different scales\n\n        with open(pout, ""wb"") as fout:\n            write_shape(x.shape, fout)\n\n            for _, (y_i, lm_probs, levels) in enumerate(bits.probs):\n                # with self.times.prefix_scope(f""[{key}]""):\n                if lm_probs is None:\n                    assert levels != -1, levels\n                    self.encode_uniform(y_i, levels, fout)\n                else:\n                    assert levels == -1, levels\n                    y_i = y_i.contiguous()\n                    entropy_coding_bytes.append(\n                        self.encode_scale(\n                            y_i,\n                            lm_probs.probs.contiguous(),\n                            y_i, self.compressor.loss_fn, fout))\n        return bits, entropy_coding_bytes\n\n    def decomp_scale_times(self) -> List[float]:\n        return [acc.mean_time_spent() for acc in self.scale_timers]\n\n    def decode(self, pin) -> torch.Tensor:\n        """"""\n        :param pin:  Path where image is stored\n        :return: Decoded image, as 1CHW, long\n        """"""\n        with torch.no_grad(), open(pin, ""rb"") as fin:\n            with self.scale_timers[0].execute():\n                H, W = read_shapes(fin)\n                shapes = lc_data.get_shapes(H, W)\n                x = self.decode_uniform(256, fin, shapes[-1])\n\n            ctx: torch.Tensor = 0.  # type: ignore\n            for dec, ctx_upsampler, prev_shape, \\\n                shape, scale_timer in zip(  # type: ignore\n                    self.compressor.decs, self.compressor.ctx_upsamplers,\n                    shapes[::-1], shapes[-2::-1],\n                    self.scale_timers[1:]):\n\n                with scale_timer.execute():\n                    deltas = self.decode_uniform(4, fin, prev_shape)\n                    deltas = quantizer.to_bn(\n                        deltas, x_min=-0.25, x_max=0.5, L=4)\n                    y = x + deltas\n\n                with scale_timer.execute():\n                    ctx = ctx_upsampler(ctx)\n                    if not isinstance(ctx, float):\n                        ctx = ctx[..., :prev_shape[-2], :prev_shape[-1]]\n                    gen = dec.forward_probs(y, ctx)\n                    x_slices: List[torch.Tensor] = []\n                    try:\n                        for i, (h, w) in enumerate(\n                                lc_data.get_2x2_shapes(*shape)):\n                            if i == 0:\n                                lm_probs = next(gen)\n                            else:\n                                lm_probs = gen.send(x_slices[i-1])\n                            x_i = self.decode_scale(\n                                self.compressor.loss_fn, lm_probs.probs,\n                                fin, (h, w)).float()\n                            x_slices.append(lc_data.pad(\n                                x_i, y.shape[-2], y.shape[-1]))\n                    except StopIteration as e:\n                        last_pixels, ctx = e.value\n                        x_slices.append(last_pixels)\n\n                    x = lc_data.join_2x2(x_slices, shape)\n\n        assert x is not None  # assert decoding worked\n        return x\n\n    def encode_uniform(self, S: torch.Tensor, levels: int, fout) -> int:\n        """""" encode coarsest scale, for which we assume a uniform prior. """"""\n        # Because our model only stores RGB values and rounding bits as uniform,\n        # levels is either 4 or 256.\n        # This means we can apply simple bit manipulations to store these as uniform.\n        assert levels == 4 or levels == 256, levels\n\n        if levels == 4:\n            S = S.reshape(-1)\n            S = F.pad(S, [0, S.shape[0] % 4])\n            assert S.shape[0] % 4 == 0\n            N = S.shape[0]//4\n\n        S = S.type(torch.cuda.ByteTensor)  # type: ignore\n        N = S.shape[0]//4 if levels == 4 else np.prod(S.size())\n\n        if levels == 4:\n            S = (S[:N] * 64 +\n                 S[N:2*N] * 16 +\n                 S[2*N:3*N] * 4 +\n                 S[3*N:])\n\n        S_np = S.cpu().numpy()\n        S_buffer = S_np.tobytes()\n        fout.write(S_buffer)\n        return N\n\n    def decode_uniform(\n            self, levels: int, fin, shape: Tuple[int, int]\n    ) -> torch.Tensor:\n        """""" decode coarsest scale, for which we assume a uniform prior. """"""\n        # Because our model only stores RGB values and rounding bits as uniform,\n        # levels is either 4 or 256.\n        assert levels == 4 or levels == 256, levels\n\n        num_elements = np.prod(shape) * 3\n        buffer_size = (num_elements+3)//4 if levels == 4 else num_elements\n        buffer = fin.read(buffer_size)\n        x_np = np.frombuffer(buffer, dtype=np.uint8)\n        x = torch.from_numpy(x_np)\n        if torch.cuda.is_available():\n            x = x.cuda()\n\n        if levels == 4:\n            x = torch.stack([\n                (x & 0b11000000) // 64,\n                (x & 0b00110000) // 16,\n                (x & 0b00001100) // 4,\n                (x & 0b00000011)]).reshape(-1)\n            padding = num_elements % 4\n            assert x.shape[0] == num_elements + padding\n            x = x[:num_elements]\n\n        x = x.reshape(-1, 3, shape[0], shape[1])\n        x = x.float()\n        return x\n\n    def encode_scale(self, S, l, bn, dmll, fout):\n        """""" Encode scale `scale`. """"""\n\n        r = coders.ArithmeticCoder(dmll.L)\n\n        # We encode channel by channel, because that\'s what\'s needed for the RGB scale. For s > 0, this could be done\n        # in parallel for all channels\n        def encoder(c, C_cur):\n            S_c = S[:, c, ...].to(torch.int16)\n            encoded = r.range_encode(S_c, cdf=C_cur)\n            write_num_bytes_encoded(len(encoded), fout)\n            fout.write(encoded)\n            # yielding always bottleneck and extra_info\n            return bn[:, c, ...], len(encoded)\n\n        _, entropy_coding_bytes_per_c = self.code_with_cdf(\n            l, bn.shape, encoder, dmll)\n\n        return sum(entropy_coding_bytes_per_c)\n\n    def decode_scale(self,\n                     dmll: lm.DiscretizedMixLogisticLoss,\n                     l: torch.Tensor,\n                     fin,\n                     shape: Tuple[int, int],\n                     ) -> torch.Tensor:\n        H, W = shape\n        C = 3\n        l = l[..., :H, :W].contiguous()\n        r = coders.ArithmeticCoder(dmll.L)\n\n        # We decode channel by channel, see `encode_scale`.\n        def decoder(_, C_cur):\n            num_bytes = read_num_bytes_encoded(fin)\n            encoded = fin.read(num_bytes)\n            S_c = r.range_decode(\n                encoded, cdf=C_cur).reshape(1, H, W)\n            # TODO: do directly in the extension\n            S_c = S_c.to(l.device, non_blocking=True)\n            bn_c = dmll.to_bn(S_c)\n            # yielding always bottleneck and extra_info (=None here)\n            return bn_c, None\n\n        bn, _ = self.code_with_cdf(l, (1, C, H, W), decoder, dmll)\n\n        return bn\n\n    def code_with_cdf(self, l, bn_shape, bn_coder, dmll):\n        """"""\n        :param l: predicted distribution, i.e., NKpHW, see DiscretizedMixLogisticLoss\n        :param bn_shape: shape of the bottleneck to encode/decode\n        :param bn_coder: function with signature (c: int, C_cur: CDFOut) -> (bottleneck[c], extra_info_c). This is\n        called for every channel of the bottleneck, with C_cur == CDF to use to encode/decode the channel. It shoud\n        return the bottleneck[c].\n        :param dmll: instance of DiscretizedMixLogisticLoss\n        :return: decoded bottleneck, list of all extra info produced by `bn_coder`.\n        """"""\n        N, C, H, W = bn_shape\n        coding = coders_helpers.CodingCDFNonshared(\n            l, total_C=C, dmll=dmll)\n\n        # needed also while encoding to get next C\n        decoded_bn = torch.zeros(N, C, H, W, dtype=torch.float32).to(l.device)\n        extra_info = []\n\n        for c in range(C):\n            C_cond_cur = coding.get_next_C(decoded_bn)\n            decoded_bn[:, c, ...], extra_info_c = bn_coder(\n                c, C_cond_cur)\n            extra_info.append(extra_info_c)\n\n        return decoded_bn, extra_info\n\n\ndef write_shape(\n        shape: Union[torch.Size, Tuple[int, int, int, int]],\n        fout\n) -> int:\n    """"""\n    Write tuple (C,H,W) to file, given shape 1CHW.\n    :return number of bytes written\n    """"""\n    assert len(shape) == 4 and shape[0] == 1 and shape[1] == 3, shape\n    assert shape\n    assert shape[2] < 2**16, shape\n    assert shape[3] < 2**16, shape\n    write_bytes(fout, [np.uint16, np.uint16], shape[2:])\n    return 4\n\n\ndef read_shapes(fin) -> Tuple[int, int]:\n    shape = tuple(map(int, read_bytes(fin, [np.uint16, np.uint16])))\n    assert len(shape) == 2, shape\n    return shape  # type: ignore\n\n\ndef write_num_bytes_encoded(num_bytes, fout):\n    assert num_bytes < 2**32\n    write_bytes(fout, [np.uint32], [num_bytes])\n    return 2  # number of bytes written\n\n\ndef read_num_bytes_encoded(fin):\n    return int(list(read_bytes(fin, [np.uint32]))[0])\n\n\ndef write_bytes(f, ts, xs):\n    for t, x in zip(ts, xs):\n        f.write(t(x).tobytes())\n\n\ndef read_bytes(f, ts):\n    for t in ts:\n        num_bytes_to_read = t().itemsize\n        yield np.frombuffer(f.read(num_bytes_to_read), t, count=1)\n'"
src/l3c/coders.py,2,"b'""""""\nCopyright 2019, ETH Zurich\n\nThis file is part of L3C-PyTorch.\n\nL3C-PyTorch is free software: you can redistribute it and/or modify\nit under the terms of the GNU General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or\nany later version.\n\nL3C-PyTorch is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with L3C-PyTorch.  If not, see <https://www.gnu.org/licenses/>.\n\n--------------------------------------------------------------------------------\n\nVery thin wrapper around torchac, for arithmetic coding.\n\n""""""\nimport torch\n\nfrom src.l3c import logistic_mixture as lm\nfrom src.torchac import torchac\n\n\nclass ArithmeticCoder(object):\n    def __init__(self, L):\n        self.L = L\n        self._cached_cdf = None\n\n    def range_encode(self, data: torch.Tensor, cdf):\n        """"""\n        :param data: data to encode\n        :param cdf: cdf to use, either a NHWLp matrix or instance of CDFOut\n        :return: data encode to a bytes string\n        """"""\n        assert len(data.shape) == 3, data.shape\n\n        data = data.to(\'cpu\', non_blocking=True)\n        assert data.dtype == torch.int16, \'Wrong dtype: {}\'.format(data.dtype)\n        data = data.reshape(-1).contiguous()\n\n        if isinstance(cdf, lm.CDFOut):\n            logit_probs_c_sm, means_c, log_scales_c, _, targets = cdf\n            out_bytes = torchac.encode_logistic_mixture(\n                targets, means_c, log_scales_c, logit_probs_c_sm, data)\n        else:\n            _, _, _, Lp = cdf.shape\n            assert Lp == self.L + 1, (Lp, self.L)\n            out_bytes = torchac.encode_cdf(cdf, data)\n\n        return out_bytes\n\n    def range_decode(self, encoded_bytes, cdf):\n        """"""\n        :param encoded_bytes: bytes encoded by range_encode\n        :param cdf: cdf to use, either a NHWLp matrix or instance of CDFOut\n        :return: decoded matrix as np.int16, NHW\n        """"""\n        if isinstance(cdf, lm.CDFOut):\n            logit_probs_c_sm, means_c, log_scales_c, _, targets = cdf\n\n            N, _, H, W = means_c.shape\n\n            decoded = torchac.decode_logistic_mixture(\n                targets, means_c, log_scales_c, logit_probs_c_sm,\n                encoded_bytes)\n\n        else:\n            N, H, W, Lp = cdf.shape\n            assert Lp == self.L + 1, (Lp, self.L)\n            decoded = torchac.decode_cdf(cdf, encoded_bytes)\n\n        return decoded.reshape(N, H, W)\n'"
src/l3c/coders_helpers.py,2,"b'""""""\nCopyright 2019, ETH Zurich\n\nThis file is part of L3C-PyTorch.\n\nL3C-PyTorch is free software: you can redistribute it and/or modify\nit under the terms of the GNU General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or\nany later version.\n\nL3C-PyTorch is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with L3C-PyTorch.  If not, see <https://www.gnu.org/licenses/>.\n\n--------------------------------------------------------------------------------\n\nVery thin wrapper around DiscretizedMixLogisticLoss.cdf_step_non_shared that keeps track of targets, which are the\nsame for all channels of the bottleneck, as well as the current channel index.\n\n""""""\n\nimport torch\n\nfrom src.l3c.logistic_mixture import CDFOut, DiscretizedMixLogisticLoss\n\n\nclass CodingCDFNonshared(object):\n    def __init__(self, l, total_C, dmll: DiscretizedMixLogisticLoss):\n        """"""\n        :param l: predicted distribution, i.e., NKpHW, see DiscretizedMixLogisticLoss\n        :param total_C:\n        :param dmll:\n        """"""\n        self.l = l\n        self.dmll = dmll\n\n        # Lp = L+1\n        self.targets = torch.linspace(dmll.x_min - dmll.bin_width / 2,\n                                      dmll.x_max + dmll.bin_width / 2,\n                                      dmll.L + 1, dtype=torch.float32, device=l.device)\n        self.total_C = total_C\n        self.c_cur = 0\n\n    def get_next_C(self, decoded_x) -> CDFOut:\n        """"""\n        Get CDF to encode/decode next channel\n        :param decoded_x: NCHW\n        :return: C_cond_cur, NHWL\'\n        """"""\n        C_Cur = self.dmll.cdf_step_non_shared(\n            self.l, self.targets, self.c_cur, self.total_C, decoded_x)\n        self.c_cur += 1\n        return C_Cur\n'"
src/l3c/edsr.py,5,"b'""""""\nCopyright 2019, ETH Zurich\n\nThis file is part of L3C-PyTorch.\n\nL3C-PyTorch is free software: you can redistribute it and/or modify\nit under the terms of the GNU General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or\nany later version.\n\nL3C-PyTorch is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with L3C-PyTorch.  If not, see <https://www.gnu.org/licenses/>.\n""""""\nimport math\nfrom typing import List\n\nimport torch\nimport torch.nn as nn\n\nfrom src import util\n\n\ndef get_act(act: str, n_feats: int = 0) -> nn.Module:\n    """""" param act: Name of activation used.\n        n_feats: channel size.\n        returns the respective activation module, or raise\n            NotImplementedError if act is not implememted.\n    """"""\n    if act == ""relu"":\n        return nn.ReLU(inplace=True)\n    elif act == ""prelu"":\n        return nn.PReLU(n_feats)\n    elif act == ""leaky_relu"":\n        return nn.LeakyReLU(inplace=True)\n    elif act == ""none"":\n        return nn.Identity()  # type: ignore\n    raise NotImplementedError(f""{act} is not implemented"")\n\n\nclass ResBlock(nn.Module):\n    """""" Implementation for ResNet block. """"""\n\n    def __init__(self,\n                 n_feats: int,\n                 kernel_size: int,\n                 act: str = ""leaky_relu"",\n                 atrous: int = 1,\n                 bn: bool = False) -> None:\n        """""" param n_feats: Channel size.\n            param kernel_size: kernel size.\n            param act: string of activation to use.\n            param atrous: controls amount of dilation to use in final conv.\n            param bn: Turns on batch norm. \n        """"""\n        super().__init__()\n\n        m: List[nn.Module] = []\n        _repr = []\n        for i in range(2):\n            atrous_rate = 1 if i == 0 else atrous\n            conv_filter = util.conv(\n                n_feats, n_feats, kernel_size, rate=atrous_rate, bias=True)\n            m.append(conv_filter)\n            _repr.append(f""Conv({n_feats}x{kernel_size}"" +\n                         (f"";A*{atrous_rate})"" if atrous_rate != 1 else """") +\n                         "")"")\n\n            if bn:\n                m.append(nn.BatchNorm2d(n_feats))\n                _repr.append(f""BN({n_feats})"")\n\n            if i == 0:\n                m.append(get_act(act))\n                _repr.append(""Act"")\n        self.body = nn.Sequential(*m)\n\n        self._repr = ""/"".join(_repr)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:  # type: ignore\n        res = self.body(x)\n        res += x\n        return res\n\n    def __repr__(self) -> str:\n        return f""ResBlock({self._repr})""\n\n\nclass Upsampler(nn.Sequential):\n    def __init__(self,\n                 scale: int,\n                 n_feats: int,\n                 bn: bool = False,\n                 act: str = ""none"",\n                 bias: bool = True) -> None:\n        m: List[nn.Module] = []\n        if (scale & (scale - 1)) == 0:  # Is scale = 2^n?\n            for _ in range(int(math.log(scale, 2))):\n                m.append(util.conv(n_feats, 4 * n_feats, 3, bias))\n                m.append(nn.PixelShuffle(2))\n                if bn:\n                    m.append(nn.BatchNorm2d(n_feats))\n                m.append(get_act(act))\n\n        elif scale == 3:\n            m.append(util.conv(n_feats, 9 * n_feats, 3, bias))\n            m.append(nn.PixelShuffle(3))\n            if bn:\n                m.append(nn.BatchNorm2d(n_feats))\n            m.append(get_act(act))\n        else:\n            raise NotImplementedError\n\n        super(Upsampler, self).__init__(*m)\n\n\nclass EDSRDec(nn.Module):\n    def __init__(self,\n                 in_ch: int,\n                 out_ch: int,\n                 resblocks: int = 8,\n                 kernel_size: int = 3,\n                 tail: str = ""none"",\n                 channel_attention: bool = False) -> None:\n        super().__init__()\n        self.head = util.conv(in_ch, out_ch, 1)\n        m_body: List[nn.Module] = [\n            ResBlock(out_ch, kernel_size) for _ in range(resblocks)]\n\n        m_body.append(util.conv(out_ch, out_ch, kernel_size))\n        self.body = nn.Sequential(*m_body)\n\n        self.tail: nn.Module\n        if tail == ""conv"":\n            self.tail = util.conv(out_ch, out_ch, 1)\n        elif tail == ""none"":\n            self.tail = nn.Identity()  # type: ignore\n        elif tail == ""upsample"":\n            self.tail = Upsampler(scale=2, n_feats=out_ch)\n        else:\n            raise NotImplementedError(f""{tail} is not implemented."")\n\n    def forward(self,  # type: ignore\n                x: torch.Tensor,\n                features_to_fuse: torch.Tensor = 0.,  # type: ignore\n                ) -> torch.Tensor:\n        """"""\n        :param x: N C H W\n        :return: N C"" H W\n        """"""\n        x = self.head(x)\n        x = x + features_to_fuse\n        x = self.body(x) + x\n        x = self.tail(x)\n        return x\n'"
src/l3c/logistic_mixture.py,37,"b'""""""\nCopyright 2019, ETH Zurich\n\nThis file is part of L3C-PyTorch.\n\nL3C-PyTorch is free software: you can redistribute it and/or modify\nit under the terms of the GNU General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or\nany later version.\n\nL3C-PyTorch is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with L3C-PyTorch.  If not, see <https://www.gnu.org/licenses/>.\n\n--------------------------------------------------------------------------------\n\nThis class is based on the TensorFlow code of PixelCNN++:\n    https://github.com/openai/pixel-cnn/blob/master/pixel_cnn_pp/nn.py\nIn contrast to that code, we predict mixture weights pi for each channel, i.e., mixture weights are ""non-shared"".\nAlso, x_min, x_max and L are parameters, and we implement a function to get the CDF of a channel.\n\n# ------\n# Naming\n# ------\n\nNote that we use the following names through the code, following the code PixelCNN++:\n    - x: targets, e.g., the RGB image for scale 0\n    - l: for the output of the network;\n      In Fig. 2 in our paper, l is the final output, denoted with p(z^(s-1) | f^(s)), i.e., it contains the parameters\n      for the mixture weights.\n""""""\n\n# from collections import namedtuple\nfrom typing import NamedTuple, Optional, Tuple\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nfrom src import configs\nfrom src.l3c import quantizer\n\n_NUM_PARAMS_RGB = 4  # mu, sigma, pi, lambda\n_NUM_PARAMS_OTHER = 3  # mu, sigma, pi\n_LOG_SCALES_MIN = -7.\n\n\nclass CDFOut(NamedTuple):\n    logit_probs_c_sm: torch.Tensor\n    means_c: torch.Tensor\n    log_scales_c: torch.Tensor\n    K: int\n    targets: torch.Tensor\n\n\ndef non_shared_get_Kp(K, C, num_params):\n    """""" Get Kp=number of channels to predict. \n        See note where we define _NUM_PARAMS_RGB above """"""\n    return num_params * C * K\n\n\ndef non_shared_get_K(Kp: int, C: int, num_params: int) -> int:\n    """""" Inverse of non_shared_get_Kp, get back K=number of mixtures """"""\n    return Kp // (num_params * C)\n\n\n# --------------------------------------------------------------------------------\nclass DiscretizedMixLogisticLoss(nn.Module):\n    def __init__(self, rgb_scale: bool, x_min=0, x_max=255, L=256):\n        """"""\n        :param rgb_scale: Whether this is the loss for the RGB scale. In that case,\n            use_coeffs=True\n            _num_params=_NUM_PARAMS_RGB == 4, since we predict coefficients lambda. See note above.\n        :param x_min: minimum value in targets x\n        :param x_max: maximum value in targets x\n        :param L: number of symbols\n        """"""\n        super(DiscretizedMixLogisticLoss, self).__init__()\n        self.rgb_scale = rgb_scale\n        self.x_min = x_min\n        self.x_max = x_max\n        self.L = L\n        # whether to use coefficients lambda to weight\n        # means depending on previously outputed means.\n        self.use_coeffs = rgb_scale\n        # P means number of different variables contained\n        # in l, l means output of network\n        self._num_params = (\n            _NUM_PARAMS_RGB if rgb_scale else\n            _NUM_PARAMS_OTHER)\n\n        # NOTE: in contrast to the original code,\n        # we use a sigmoid (instead of a tanh)\n        # The optimizer seems to not care,\n        # but it would probably be more principaled to use a tanh\n        # Compare with L55 here:\n        # https://github.com/openai/pixel-cnn/blob/master/pixel_cnn_pp/nn.py#L55\n        self._nonshared_coeffs_act = torch.sigmoid\n\n        # Adapted bounds for our case.\n        self.bin_width = (x_max - x_min) / (L-1)\n        self.x_lower_bound = x_min + 0.001\n        self.x_upper_bound = x_max - 0.001\n\n        self._extra_repr = \'DMLL: x={}, L={}, coeffs={}, P={}, bin_width={}\'.format(\n            (self.x_min, self.x_max), self.L, self.use_coeffs, self._num_params, self.bin_width)\n\n    def extra_repr(self):\n        return self._extra_repr\n\n    @staticmethod\n    def to_per_pixel(entropy, C):\n        N, H, W = entropy.shape\n        return entropy.sum() / (N*C*H*W)  # NHW -> scalar\n\n    def to_sym(self, x):\n        return quantizer.to_sym(x, self.x_min, self.x_max, self.L)\n\n    def to_bn(self, S):\n        return quantizer.to_bn(S, self.x_min, self.x_max, self.L)\n\n    def cdf_step_non_shared(self, l, targets, c_cur, C, x_c=None) -> CDFOut:\n        assert c_cur < C\n\n        # NKHW         NKHW     NKHW\n        logit_probs_c, means_c, log_scales_c, K = self._extract_non_shared_c(\n            c_cur, C, l, x_c)\n\n        logit_probs_c_softmax = F.softmax(logit_probs_c, dim=1)  # NKHW, pi_k\n        return CDFOut(\n            logit_probs_c_softmax, means_c,\n            log_scales_c, K, targets.to(l.device))\n\n    def sample(self, l, C):\n        return self._non_shared_sample(l, C)\n\n    def log_cdf(self, lo, hi, means, log_scales):\n        assert torch.all(lo <= hi), f""{lo[lo > hi]} > {hi[lo > hi]}""\n        assert lo.min() >= self.x_min and hi.max() <= self.x_max, \\\n            \'{},{} not in {},{}\'.format(\n                lo.min(), hi.max(), self.x_min, self.x_max)\n\n        centered_lo = lo - means  # NCKHW\n        centered_hi = hi - means\n\n        # Calc cdf_delta\n        # all of the following is NCKHW\n        # <= exp(7), is exp(-sigma), inverse std. deviation, i.e., sigma\'\n        inv_stdv = torch.exp(-log_scales)\n        # sigma\' * (x - mu + 0.5)\n        # S(sigma\' * (x - mu - 1/255)) = 1 / (1 + exp(sigma\' * (x - mu - 1/255))\n        normalized_lo = inv_stdv * (\n            centered_lo - self.bin_width/2)  # sigma\' * (x - mu - 1/255)\n        lo_cond = (lo >= self.x_lower_bound).float()\n        # log probability for edge case of 0\n        cdf_lo = lo_cond * torch.sigmoid(normalized_lo)\n        normalized_hi = inv_stdv * (centered_hi + self.bin_width/2)\n        hi_cond = (hi <= self.x_upper_bound).float()\n        cdf_hi = hi_cond * torch.sigmoid(normalized_hi) + (1 - hi_cond)  # * 1.\n        # S(sigma\' * (x - mu + 1/255))\n        # NCKHW, cdf^k(c)\n        cdf_delta = cdf_hi - cdf_lo\n        log_cdf_delta = torch.log(torch.clamp(cdf_delta, min=1e-12))\n\n        assert not torch.any(\n            log_cdf_delta > 1e-6\n        ), f""{log_cdf_delta[log_cdf_delta > 1e-6]}""\n        return log_cdf_delta\n\n    def forward(  # type: ignore\n            self, x: torch.Tensor, l: torch.Tensor,\n    ) -> torch.Tensor:\n        """"""\n        :param x: labels, i.e., NCHW, float\n        :param l: predicted distribution, i.e., NKpHW, see above\n        :return: log-likelihood, as NHW if shared, NCHW if non_shared pis\n        """"""\n        assert x.min() >= self.x_min and x.max() <= self.x_max, \\\n            f\'{x.min()},{x.max()} not in {self.x_min},{self.x_max}\'\n\n        # Extract ---\n        #  NC1HW     NCKHW      NCKHW  NCKHW\n        x, logit_pis, means, log_scales, _ = self._extract_non_shared(x, l)\n\n        log_probs = self.log_cdf(x, x, means, log_scales)\n\n        # combine with pi, NCKHW, (-inf, 0]\n        log_weights = F.log_softmax(logit_pis, dim=2)\n        log_probs_weighted = log_weights + log_probs\n\n        # final log(P), NCHW\n        nll = -torch.logsumexp(log_probs_weighted, dim=2)\n        return nll\n\n    def _extract_non_shared(self, x, l):\n        """"""\n        :param x: targets, NCHW\n        :param l: output of net, NKpHW, see above\n        :return:\n            x NC1HW,\n            logit_probs NCKHW (probabilites of scales, i.e., \\pi_k)\n            means NCKHW,\n            log_scales NCKHW (variances),\n            K (number of mixtures)\n        """"""\n        N, C, H, W = x.shape\n        Kp = l.shape[1]\n\n        K = non_shared_get_K(Kp, C, self._num_params)\n\n        # we have, for each channel: K pi / K mu / K sigma / [K coeffs]\n        # note that this only holds for C=3 as for other channels,\n        # there would be more than 3*K coeffs\n        # but non_shared only holds for the C=3 case\n        l = l.reshape(N, self._num_params, C, K, H, W)\n\n        logit_probs = l[:, 0, ...]  # NCKHW\n        means = l[:, 1, ...]  # NCKHW\n        log_scales = torch.clamp(\n            l[:, 2, ...], min=_LOG_SCALES_MIN)  # NCKHW, is >= -7\n        x = x.reshape(N, C, 1, H, W)\n\n        if self.use_coeffs:\n            # Coefficients only supported for multiples of 3,\n            # see note where we define\n            # _NUM_PARAMS_RGB NCKHW, basically coeffs_g_r, coeffs_b_r, coeffs_b_g\n            assert C == 3, C\n            # Each NCKHW\n            coeffs = self._nonshared_coeffs_act(l[:, 3, ...])\n            # each NKHW\n            coeffs_g_r = coeffs[:, 0, ...]\n            coeffs_b_r = coeffs[:, 1, ...]\n            coeffs_b_g = coeffs[:, 2, ...]\n            # NCKHW\n            means = torch.stack(\n                (means[:, 0, ...],\n                 means[:, 1, ...] + coeffs_g_r * x[:, 0, ...],\n                 means[:, 2, ...] + coeffs_b_r * x[:, 0, ...]\n                                  + coeffs_b_g * x[:, 1, ...]),\n                dim=1)\n\n        means = torch.clamp(means, min=self.x_min, max=self.x_max)\n        assert means.shape == (N, C, K, H, W), (means.shape, (N, C, K, H, W))\n        return x, logit_probs, means, log_scales, K\n\n    def _extract_non_shared_c(\n            self, c: int, C: int, l: torch.Tensor,\n            x: Optional[torch.Tensor] = None\n    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, int]:\n        """"""\n        Same as _extract_non_shared but only for c-th channel, used to get CDF\n        """"""\n        assert c < C, f\'{c} >= {C}\'\n\n        N, Kp, H, W = l.shape\n        K = non_shared_get_K(Kp, C, self._num_params)\n\n        l = l.reshape(N, self._num_params, C, K, H, W)\n        logit_probs_c = l[:, 0, c, ...]  # NKHW\n        means_c = l[:, 1, c, ...]  # NKHW\n        log_scales_c = torch.clamp(\n            l[:, 2, c, ...], min=_LOG_SCALES_MIN)  # NKHW, is >= -7\n\n        if self.use_coeffs and c != 0:\n            # N C K H W, coeffs_g_r, coeffs_b_r, coeffs_b_g\n            unscaled_coeffs = l[:, 3, ...]\n            if c == 1:\n                assert x is not None\n                coeffs_g_r = self._nonshared_coeffs_act(\n                    unscaled_coeffs[:, 0, ...])  # NKHW\n                means_c += coeffs_g_r * x[:, 0, ...]\n            elif c == 2:\n                assert x is not None\n                coeffs_b_r = self._nonshared_coeffs_act(\n                    unscaled_coeffs[:, 1, ...])  # NKHW\n                coeffs_b_g = self._nonshared_coeffs_act(\n                    unscaled_coeffs[:, 2, ...])  # NKHW\n                means_c += coeffs_b_r * x[:, 0, ...] + coeffs_b_g * x[:, 1, ...]\n\n        #      NKHW           NKHW     NKHW\n        return logit_probs_c, means_c, log_scales_c, K\n\n    def _non_shared_sample(self, l, C):\n        """""" sample from model """"""\n        N, Kp, H, W = l.shape\n        K = non_shared_get_K(Kp, C, self._num_params)\n        l = l.reshape(N, self._num_params, C, K, H, W)\n\n        logit_probs = l[:, 0, ...]  # NCKHW\n\n        # sample mixture indicator from softmax\n        u = torch.zeros_like(logit_probs).uniform_(1e-5, 1. - 1e-5)  # NCKHW\n        # argmax over K, results in NCHW,\n        # specifies for each c: which of the K mixtures to take\n        sel = torch.argmax(\n            logit_probs - torch.log(-torch.log(u)),  # gumbel sampling\n            dim=2)\n        assert sel.shape == (N, C, H, W), (sel.shape, (N, C, H, W))\n\n        sel = sel.unsqueeze(2)  # NC1HW\n\n        means = torch.gather(l[:, 1, ...], 2, sel).squeeze(2)\n        log_scales = torch.clamp(torch.gather(\n            l[:, 2, ...], 2, sel).squeeze(2), min=_LOG_SCALES_MIN)\n\n        # sample from the resulting logistic,\n        # which now has essentially 1 mixture component only.\n        # We use inverse transform sampling.\n        # i.e. X~logistic; generate u ~ Unfirom; x = CDF^-1(u),\n        #  where CDF^-1 for the logistic is CDF^-1(y) = \\mu + \\sigma * log(y / (1-y))\n        u = torch.zeros_like(means).uniform_(1e-5, 1. - 1e-5)  # NCHW\n        x = means + torch.exp(log_scales) * \\\n            (torch.log(u) - torch.log(1. - u))  # NCHW\n\n        if self.use_coeffs:\n            assert C == 3\n\n            def clamp(x_):\n                return torch.clamp(x_, 0, 255.)\n\n            # Be careful about coefficients!\n            # We need to use the correct selection mask, namely the one for the G and\n            #  B channels, as we update the G and B means!\n            # Doing torch.gather(l[:, 3, ...], 2, sel) would be completly\n            #  wrong.\n            coeffs = torch.sigmoid(l[:, 3, ...])\n            sel_g, sel_b = sel[:, 1, ...], sel[:, 2, ...]\n            coeffs_g_r = torch.gather(coeffs[:, 0, ...], 1, sel_g).squeeze(1)\n            coeffs_b_r = torch.gather(coeffs[:, 1, ...], 1, sel_b).squeeze(1)\n            coeffs_b_g = torch.gather(coeffs[:, 2, ...], 1, sel_b).squeeze(1)\n\n            # Note: In theory, we should go step by step over the channels\n            # and update means with previously sampled\n            # xs. But because of the math above (x = means + ...),\n            # we can just update the means here and it\'s all good.\n            x0 = clamp(x[:, 0, ...])\n            x1 = clamp(x[:, 1, ...] + coeffs_g_r * x0)\n            x2 = clamp(x[:, 2, ...] + coeffs_b_r * x0 + coeffs_b_g * x1)\n            x = torch.stack((x0, x1, x2), dim=1)\n        return x\n'"
src/l3c/prob_clf.py,3,"b'""""""\nCopyright 2019, ETH Zurich\n\nThis file is part of L3C-PyTorch.\n\nL3C-PyTorch is free software: you can redistribute it and/or modify\nit under the terms of the GNU General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or\nany later version.\n\nL3C-PyTorch is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with L3C-PyTorch.  If not, see <https://www.gnu.org/licenses/>.\n""""""\nfrom typing import List, Union\n\nimport torch\nfrom torch import nn\n\nfrom src import util\nfrom src.l3c.logistic_mixture import non_shared_get_Kp\n\n\nclass AtrousProbabilityClassifier(nn.Module):\n    def __init__(self,\n                 in_ch: int,\n                 C: int,\n                 num_params: int,\n                 K: int = 10,\n                 kernel_size: int = 3,\n                 atrous_rates_str: str = \'1,2,4\') -> None:\n        super(AtrousProbabilityClassifier, self).__init__()\n\n        Kp = non_shared_get_Kp(K, C, num_params)\n\n        self.atrous = StackedAtrousConvs(atrous_rates_str, in_ch, Kp,\n                                         kernel_size=kernel_size)\n        self._repr = f\'C={C}; K={K}; Kp={Kp}; rates={atrous_rates_str}\'\n\n    def __repr__(self) -> str:\n        return f\'AtrousProbabilityClassifier({self._repr})\'\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:  # type: ignore\n        """"""\n        :param x: N C H W\n        :return: N Kp H W\n        """"""\n        return self.atrous(x)\n\n\nclass StackedAtrousConvs(nn.Module):\n    def __init__(self,\n                 atrous_rates_str: Union[str, int],\n                 Cin: int,\n                 Cout: int,\n                 bias: bool = True,\n                 kernel_size: int = 3) -> None:\n        super(StackedAtrousConvs, self).__init__()\n        atrous_rates = self._parse_atrous_rates_str(atrous_rates_str)\n        self.atrous = nn.ModuleList(\n            [util.conv(Cin, Cin, kernel_size, rate=rate)\n             for rate in atrous_rates])\n        self.lin = util.conv(len(atrous_rates) * Cin, Cout, 1, bias=bias)\n        self._extra_repr = \'rates={}\'.format(atrous_rates)\n\n    @staticmethod\n    def _parse_atrous_rates_str(atrous_rates_str: Union[str, int]) -> List[int]:\n        # expected to either be an int or a comma-separated string 1,2,4\n        if isinstance(atrous_rates_str, int):\n            return [atrous_rates_str]\n        else:\n            return list(map(int, atrous_rates_str.split(\',\')))\n\n    def extra_repr(self) -> str:\n        return self._extra_repr\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:  # type: ignore\n        x = torch.cat([atrous(x)\n                       for atrous in self.atrous], dim=1)  # type: ignore\n        x = self.lin(x)\n        return x\n'"
src/l3c/quantizer.py,0,"b'""""""\nCopyright 2019, ETH Zurich\n\nThis file is part of L3C-PyTorch.\n\nL3C-PyTorch is free software: you can redistribute it and/or modify\nit under the terms of the GNU General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or\nany later version.\n\nL3C-PyTorch is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with L3C-PyTorch.  If not, see <https://www.gnu.org/licenses/>.\n\n--------------------------------------------------------------------------------\n\nBased on our TensorFlow implementation for the CVPR 2018 paper\n\n""Conditional Probability Models for Deep Image Compression""\n\nThis is a PyTorch implementation of that quantization layer.\n\nhttps://github.com/fab-jul/imgcomp-cvpr/blob/master/code/quantizer.py\n\n""""""\n\n\ndef to_sym(x, x_min, x_max, L):\n    sym_range = x_max - x_min\n    bin_size = sym_range / (L-1)\n    return x.clamp(x_min, x_max).sub(x_min).div(bin_size).round()\n\n\ndef to_bn(S, x_min, x_max, L):\n    sym_range = x_max - x_min\n    bin_size = sym_range / (L-1)\n    return S.float().mul(bin_size).add(x_min)\n'"
src/l3c/timer.py,0,"b'import subprocess\nimport time\nfrom contextlib import contextmanager\nfrom typing import List\n\n\nclass TimeAccumulator(object):\n    """"""\n    Usage:\n    t = TimeAccumulator()\n    for i, x in enumerate(it):\n        with t.execute():\n            expensive_operation()\n        if i % 50 == 0:\n            print(\'Average: {}\'.format(t.mean_time_spent()))\n    """"""\n    _EPS = 1e-8\n\n    def __init__(self) -> None:\n        self.times: List[float] = []\n\n    @contextmanager\n    def execute(self):\n        prev = time.time()\n        try:\n            yield\n            self.times.append(time.time() - prev)\n        except subprocess.CalledProcessError:\n            raise\n\n    def mean_time_spent(self) -> float:\n        """""" :returns mean time spent and resets cached times. """"""\n        total_time_spent = sum(self.times)\n        count = float(len(self.times))\n        if count == 0:\n            count += self._EPS\n        # self.times = []\n        # prevent div by zero errors\n        return total_time_spent / count\n'"
src/torchac/setup.py,3,"b'""""""\nCopyright 2019, ETH Zurich\n\nThis file is part of L3C-PyTorch.\n\nL3C-PyTorch is free software: you can redistribute it and/or modify\nit under the terms of the GNU General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or\nany later version.\n\nL3C-PyTorch is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with L3C-PyTorch.  If not, see <https://www.gnu.org/licenses/>.\n\n--------------------------------------------------------------------------------\n\nNOTE: Needs PyTorch 1.0 or newer, as the C++ code relies on that API!\n\nDepending on the environment variable COMPILE_CUDA, compiles the torchac_backend with or\nwithout support for CUDA, into a module called torchac_backend_gpu or torchac_backend_cpu.\n\nCOMPILE_CUDA = auto is equal to yes if one of the supported combinations of nvcc and gcc is found (see\n_supported_compilers_available).\nCOMPILE_CUDA = force means compile with CUDA, even if it is not one of the supported combinations\nCOMPILE_CUDA = no means no CUDA.\n\nThe only difference between the CUDA and non-CUDA versions is: With CUDA, _get_uint16_cdf from torchac is done with a\nsimple/non-optimized CUDA kernel (torchac_kernel.cu), which has one benefit: we can directly write into shared memory!\nThis saves an expensive copying step from GPU to CPU.\n\nFlags read by this script:\n    COMPILE_CUDA=[auto|force|no]\n    \n""""""\n\nimport sys\nimport re\nimport subprocess\nfrom setuptools import setup\nfrom distutils.version import LooseVersion\nfrom torch.utils.cpp_extension import CppExtension, BuildExtension, CUDAExtension\nimport os\n\n\nMODULE_BASE_NAME = \'torchac_backend\'\n\n\ndef prefixed(prefix, l):\n    ps = [os.path.join(prefix, el) for el in l]\n    for p in ps:\n        if not os.path.isfile(p):\n            raise FileNotFoundError(p)\n    return ps\n\n\ndef compile_ext(cuda_support):\n    print(\'Compiling, cuda_support={}\'.format(cuda_support))\n    ext_module = get_extension(cuda_support)\n\n    setup(name=ext_module.name,\n          version=\'1.0.0\',\n          ext_modules=[ext_module],\n          extra_compile_args=[\'-mmacosx-version-min=10.9\'],\n          cmdclass={\'build_ext\': BuildExtension})\n\n\ndef get_extension(cuda_support):\n    # dir of this file\n    setup_dir = os.path.dirname(os.path.realpath(__file__))\n    # Where the cpp and cu files are\n    prefix = os.path.join(setup_dir, MODULE_BASE_NAME)\n    if not os.path.isdir(prefix):\n        raise ValueError(\'Did not find backend foler: {}\'.format(prefix))\n    if cuda_support:\n        nvcc_avaible, nvcc_version = supported_nvcc_available()\n        if not nvcc_avaible:\n            print(_bold_warn_str(\'***WARN\') + \': Found untested nvcc {}\'.format(nvcc_version))\n\n        return CUDAExtension(\n                MODULE_BASE_NAME + \'_gpu\',\n                prefixed(prefix, [\'torchac.cpp\', \'torchac_kernel.cu\']),\n                define_macros=[(\'COMPILE_CUDA\', \'1\')])\n    else:\n        return CppExtension(\n                MODULE_BASE_NAME + \'_cpu\',\n                prefixed(prefix, [\'torchac.cpp\']))\n\n\n# TODO:\n# Add further supported version as specified in readme\n\n\n\ndef _supported_compilers_available():\n    """"""\n    To see an up-to-date list of tested combinations of GCC and NVCC, see the README\n    """"""\n    return _supported_gcc_available()[0] and supported_nvcc_available()[0]\n\n\ndef _supported_gcc_available():\n    v = _get_version([\'gcc\', \'-v\'], r\'version (.*?)\\s+\')\n    return LooseVersion(\'6.0\') > LooseVersion(v) >= LooseVersion(\'5.0\'), v\n\n\ndef supported_nvcc_available():\n    v = _get_version([\'nvcc\', \'-V\'], \'release (.*?),\')\n    if v is None:\n        return False, \'nvcc unavailable!\'\n    return LooseVersion(v) >= LooseVersion(\'9.0\'), v\n\n\ndef _get_version(cmd, regex):\n    try:\n        otp = subprocess.check_output(cmd, stderr=subprocess.STDOUT).decode()\n        if len(otp.strip()) == 0:\n            raise ValueError(\'No output\')\n        m = re.search(regex, otp)\n        if not m:\n            raise ValueError(\'Regex does not match output:\\n{}\'.format(otp))\n        return m.group(1)\n    except FileNotFoundError:\n        return None\n\n\ndef _bold_warn_str(s):\n    return \'\\x1b[91m\\x1b[1m\' + s + \'\\x1b[0m\'\n\n\ndef _assert_torch_version_sufficient():\n    import torch\n    if LooseVersion(torch.__version__) >= LooseVersion(\'1.0\'):\n        return\n    print(_bold_warn_str(\'Error:\'), \'Need PyTorch version >= 1.0, found {}\'.format(torch.__version__))\n    sys.exit(1)\n\n\ndef main():\n    _assert_torch_version_sufficient()\n\n    cuda_flag = os.environ.get(\'COMPILE_CUDA\', \'no\')\n\n    if cuda_flag == \'auto\':\n        cuda_support = _supported_compilers_available()\n        print(\'Found CUDA supported:\', cuda_support)\n    elif cuda_flag == \'force\':\n        cuda_support = True\n    elif cuda_flag == \'no\':\n        cuda_support = False\n    else:\n        raise ValueError(\'COMPILE_CUDA must be in (auto, force, no), got {}\'.format(cuda_flag))\n\n    compile_ext(cuda_support)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
src/torchac/torchac.py,4,"b'""""""\nCopyright 2019, ETH Zurich\n\nThis file is part of L3C-PyTorch.\n\nL3C-PyTorch is free software: you can redistribute it and/or modify\nit under the terms of the GNU General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or\nany later version.\n\nL3C-PyTorch is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with L3C-PyTorch.  If not, see <https://www.gnu.org/licenses/>.\n""""""\n\n# TODO some comments needed about [..., -1] == 0\n\nimport torch\n\n# torchac can be built with and without CUDA support.\n# Here, we try to import both torchac_backend_gpu and torchac_backend_cpu.\n# If both fail, an exception is thrown here already.\n#\n# The right version is then picked in the functions below.\n#\n# NOTE:\n# Without a clean build, multiple versions might be installed. You may use python seutp.py clean --all to prevent this.\n# But it should not be an issue.\n\n\nimport_errors = []\n\n\ntry:\n    import torchac_backend_gpu\n    CUDA_SUPPORTED = True\nexcept ImportError as e:\n    CUDA_SUPPORTED = False\n    import_errors.append(e)\n\ntry:\n    import torchac_backend_cpu\n    CPU_SUPPORTED = True\nexcept ImportError as e:\n    CPU_SUPPORTED = False\n    import_errors.append(e)\n\n\nprint(f""Using torchac: {CUDA_SUPPORTED}"")\n\n\nimported_at_least_one = CUDA_SUPPORTED or CPU_SUPPORTED\n\n\n# if import_errors:\n#     import_errors_str = \'\\n\'.join(map(str, import_errors))\n#     print(f\'*** Import errors:\\n{import_errors_str}\')\n\n\nif not imported_at_least_one:\n    raise ImportError(\'*** Failed to import any torchac_backend! Make sure to install torchac with torchac/setup.py. \'\n                      \'See the README for details.\')\n\n\nany_backend = torchac_backend_cpu if CPU_SUPPORTED else torchac_backend_gpu\n\n\n# print(f\'*** torchac: GPU support: {CUDA_SUPPORTED} // CPU support: {CPU_SUPPORTED}\')\n\n\ndef _get_gpu_backend():\n    if not CUDA_SUPPORTED:\n        raise ValueError(\'Got CUDA tensor, but torchac_backend_gpu is not available. \'\n                         \'Compile torchac with CUDA support, or use CPU mode (see README).\')\n    return torchac_backend_gpu\n\n\ndef _get_cpu_backend():\n    if not CPU_SUPPORTED:\n        raise ValueError(\'Got CPU tensor, but torchac_backend_cpu is not available. \'\n                         \'Compile torchac without CUDA support, or use GPU mode (see README).\')\n    return torchac_backend_cpu\n\n\ndef encode_cdf(cdf, sym):\n    """"""\n    :param cdf: CDF as 1HWLp, as int16, on CPU!\n    :param sym: the symbols to encode, as int16, on CPU\n    :return: byte-string, encoding `sym`\n    """"""\n    if cdf.is_cuda or sym.is_cuda:\n        raise ValueError(\'CDF and symbols must be on CPU for `encode_cdf`\')\n    # encode_cdf is defined in both backends, so doesn\'t matter which one we use!\n    return any_backend.encode_cdf(cdf, sym)\n\n\ndef decode_cdf(cdf, input_string):\n    """"""\n    :param cdf: CDF as 1HWLp, as int16, on CPU\n    :param input_string: byte-string, encoding some symbols `sym`.\n    :return: decoded `sym`.\n    """"""\n    if cdf.is_cuda:\n        raise ValueError(\'CDF must be on CPU for `decode_cdf`\')\n    # encode_cdf is defined in both backends, so doesn\'t matter which one we use!\n    return any_backend.decode_cdf(cdf, input_string)\n\n\ndef encode_logistic_mixture(\n        targets, means, log_scales, logit_probs_softmax,  # CDF\n        sym):\n    """"""\n    NOTE: This function uses either the CUDA or CPU backend, depending on the device of the input tensors.\n    NOTE: targets, means, log_scales, logit_probs_softmax must all be on the same device (CPU or GPU)\n    In the following, we use\n        Lp: Lp = L+1, where L = number of symbols.\n        K: number of mixtures\n    :param targets: values of symbols, tensor of length Lp, float32\n    :param means: means of mixtures, tensor of shape 1KHW, float32\n    :param log_scales: log(scales) of mixtures, tensor of shape 1KHW, float32\n    :param logit_probs_softmax: weights of the mixtures (PI), tensorf of shape 1KHW, float32\n    :param sym: the symbols to encode. MUST be on CPU!!\n    :return: byte-string, encoding `sym`.\n    """"""\n    if not (targets.is_cuda == means.is_cuda == log_scales.is_cuda == logit_probs_softmax.is_cuda):\n        raise ValueError(\'targets, means, log_scales, logit_probs_softmax must all be on the same device! Got \'\n                         f\'{targets.device}, {means.device}, {log_scales.device}, {logit_probs_softmax.device}.\')\n    if sym.is_cuda:\n        raise ValueError(\'sym must be on CPU!\')\n\n    if targets.is_cuda:\n        return _get_gpu_backend().encode_logistic_mixture(\n            targets, means, log_scales, logit_probs_softmax, sym)\n    else:\n        cdf = _get_uint16_cdf(logit_probs_softmax, targets, means, log_scales)\n        return encode_cdf(cdf, sym)\n\n\ndef decode_logistic_mixture(\n        targets, means, log_scales, logit_probs_softmax,  # CDF\n        input_string):\n    """"""\n    NOTE: This function uses either the CUDA or CPU backend, depending on the device of the input tensors.\n    NOTE: targets, means, log_scales, logit_probs_softmax must all be on the same device (CPU or GPU)\n    In the following, we use\n        Lp: Lp = L+1, where L = number of symbols.\n        K: number of mixtures\n    :param targets: values of symbols, tensor of length Lp, float32\n    :param means: means of mixtures, tensor of shape 1KHW, float32\n    :param log_scales: log(scales) of mixtures, tensor of shape 1KHW, float32\n    :param logit_probs_softmax: weights of the mixtures (PI), tensorf of shape 1KHW, float32\n    :param input_string: byte-string, encoding some symbols `sym`.\n    :return: decoded `sym`.\n    """"""\n    if not (targets.is_cuda == means.is_cuda == log_scales.is_cuda == logit_probs_softmax.is_cuda):\n        raise ValueError(\'targets, means, log_scales, logit_probs_softmax must all be on the same device! Got \'\n                         f\'{targets.device}, {means.device}, {log_scales.device}, {logit_probs_softmax.device}.\')\n\n    if targets.is_cuda:\n        return _get_gpu_backend().decode_logistic_mixture(\n            targets, means, log_scales, logit_probs_softmax, input_string)\n    else:\n        cdf = _get_uint16_cdf(logit_probs_softmax, targets, means, log_scales)\n        return decode_cdf(cdf, input_string)\n\n\n# ------------------------------------------------------------------------------\n\n# The following code is invoced for when the CDF is not on GPU, and we cannot use torchac/torchac_kernel.cu\n# This basically replicates that kernel in pure PyTorch.\n\ndef _get_uint16_cdf(logit_probs_softmax, targets, means, log_scales):\n    cdf_float = _get_C_cur_weighted(\n        logit_probs_softmax, targets, means, log_scales)\n    cdf = _renorm_cast_cdf_(cdf_float, precision=16)\n    cdf = cdf.cpu()\n    return cdf\n\n\ndef _get_C_cur_weighted(logit_probs_softmax_c, targets, means_c, log_scales_c):\n    C_cur = _get_C_cur(targets, means_c, log_scales_c)  # NKHWL\n    C_cur = C_cur.mul(logit_probs_softmax_c.unsqueeze(-1)).sum(1)  # NHWL\n    return C_cur\n\n\ndef _get_C_cur(targets, means_c, log_scales_c):  # NKHWL\n    """"""\n    :param targets: Lp floats\n    :param means_c: NKHW\n    :param log_scales_c: NKHW\n    :return:\n    """"""\n    # NKHW1\n    inv_stdv = torch.exp(-log_scales_c).unsqueeze(-1)\n    # NKHWL\'\n    centered_targets = (targets - means_c.unsqueeze(-1))\n    # NKHWL\'\n    cdf = centered_targets.mul(inv_stdv).sigmoid()  # sigma\' * (x - mu)\n    return cdf\n\n\ndef _renorm_cast_cdf_(cdf, precision):\n    Lp = cdf.shape[-1]\n    finals = 1  # NHW1\n    # RENORMALIZATION_FACTOR in cuda\n    f = torch.tensor(2, dtype=torch.float32, device=cdf.device).pow_(precision)\n    cdf = cdf.mul((f - (Lp - 1)) / finals)  # TODO\n    cdf = cdf.round()\n    cdf = cdf.to(dtype=torch.int16, non_blocking=True)\n    r = torch.arange(Lp, dtype=torch.int16, device=cdf.device)\n    cdf.add_(r)\n    return cdf\n'"
