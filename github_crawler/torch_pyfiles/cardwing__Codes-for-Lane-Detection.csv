file_path,api_count,code
ENet-TuSimple-Torch/pred_json.py,0,"b'import json\nimport cv2\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport os\nimport numpy as np\nimport math\n\ndef show(raw_file, pred_lanes):\n    img = cv2.imread(raw_file)\n    y_samples = range(160,720,10)\n    pred_lanes_vis = [[(x, y) for (x, y) in zip(lane, y_samples) if x >= 0] for lane in pred_lanes]\n    img_vis = img.copy()\n\n    for lane in pred_lanes_vis:\n        cv2.polylines(img_vis, np.int32([lane]), isClosed=False, color=(0,0,255), thickness=2)\n\n    plt.imshow(img_vis)\n    plt.show()\n\ndef isShort(lane, thr):\n    start = [ i for i, x in enumerate(lane) if x>0 ]\n    if not start:\n        return 1\n    start = start[0]\n    end = [ i for i, x in reversed(list(enumerate(lane))) if x>0 ][0]\n    x_s = lane[start]\n    x_e = lane[end]\n    length = math.sqrt((x_s-x_e)*(x_s-x_e)+(start-end)*(start-end)*100)\n    if length <= thr:\n        return 1\n    else:\n        return 0\n\ndef rmShort(lanes, thr):\n    Lanes = [lane for lane in lanes if not isShort(lane, thr)]\n    return Lanes\n\ndef connect(lanes):\n    Lanes = []\n    isAdd = [0 for i in range(len(lanes))]\n    for i in range(len(lanes)-1):\n        for j in range(i+1, len(lanes)):\n            lanea = lanes[i]\n            laneb = lanes[j]\n            starta = [ k for k, x in enumerate(lanea) if x>0 ][0]\n            startb = [ k for k, x in enumerate(laneb) if x>0 ][0]\n            enda = [ k for k, x in reversed(list(enumerate(lanea))) if x>0 ][0]\n            endb = [ k for k, x in reversed(list(enumerate(laneb))) if x>0 ][0]\n            if enda > startb or endb > starta:\n                if enda < startb:\n                    lane1 = lanea[:]\n                    lane2 = laneb[:]\n                    start1 = starta\n                    start2 = startb\n                    end1 = enda\n                    end2 = endb\n                else:\n                    lane1 = laneb[:]\n                    lane2 = lanea[:]\n                    start1 = startb\n                    start2 = starta\n                    end1 = endb\n                    end2 = enda\n                id1 = max(start1, end1-5)\n                k1 = float(lane1[end1]-lane1[id1])/(10*(end1-id1))\n                id2 = min(end2, start2+5)\n                k2 = float(lane2[id2]-lane2[start2])/(10*(id2-start2))\n                extend = lane1[end1] + k1 * (start2 - end1) * 10\n                if abs(k1-k2) < 0.2 and abs(extend-lane2[start2]) < 50:\n                    newlane = [-2 for k in range(56)]\n                    newlane[start1:end1+1] = lane1[start1:end1+1]\n                    newlane[start2:end2+1] = lane2[start2:end2+1]\n                    Lanes.append(newlane)\n                    isAdd[i] = 1\n                    isAdd[j] = 1\n    for i, lane in enumerate(lanes):\n        if isAdd[i] == 0:\n            Lanes.append(lane)\n    for lane in Lanes:\n        lane = fixGap(lane)\n    return Lanes\n\ndef cutMax(lanes):\n    start = []\n    for lane in lanes:\n        s = [ k for k, x in enumerate(lane) if x>0 ][0]\n        start.append(s)\n    m = min(start)\n    if m < 350:\n        index1 = start.index(m)\n        reverse = start[:]\n        reverse.reverse()\n        index2 = reverse.index(m)\n        if index1 + index2 == len(start) - 1:\n            lanes[index1][m] = -2\n    return lanes\n\ndef maxArea(probs, h, w):\n    Probs = []\n    binary = np.zeros((h, w), np.uint8)\n    binary[probs > 40] = 1   # 40 60\n    # print(sum(sum(binary)))\n    contours0, hierarchy = cv2.findContours( binary.copy(), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE) # _, contours0, hierarchy\n    areas = [cv2.contourArea(contour) for contour in contours0]\n    if not areas:\n        return [probs]\n    #print(areas)\n    max_area = max(areas)\n    max_id = [i for i, area in enumerate(areas) if area == max_area][0]\n    mask = np.zeros((h, w), np.uint8)\n    cv2.drawContours( mask, contours0, max_id, 1, -1, 8, hierarchy, 0)\n    probs1 = np.multiply(probs, mask)\n    Probs.append(probs1)\n    if len(areas) > 1:\n        areas2 = areas[:]\n        areas2.remove(max_area)\n        max_area2 = max(areas2)\n        if max_area2 > 800:  # 800 1500\n            max_id2 = [i for i, area in enumerate(areas) if area == max_area2][0]\n            mask2 = np.zeros((h, w), np.uint8)\n            cv2.drawContours( mask2, contours0, max_id2, 1, -1, 8, hierarchy, 0)\n            probs2 = np.multiply(probs, mask2)\n            Probs.append(probs2)\n    return Probs\n\ndef fixGap(coordinate):\n    if any(x > 0 for x in coordinate):\n        start = [ i for i, x in enumerate(coordinate) if x>0 ][0]\n        end = [ i for i, x in reversed(list(enumerate(coordinate))) if x>0 ][0]\n        lane = coordinate[start:end+1]\n        if any(x < 0 for x in lane):\n            print(\'gap detected!\')\n            gap_start = [ i for i, x in enumerate(lane[:-1]) if x>0 and lane[i+1]<0 ]\n            gap_end = [ i+1 for i, x in enumerate(lane[:-1]) if x<0 and lane[i+1]>0 ]\n            gap_id = [ i for i, x in enumerate(lane) if x<0 ]\n            for id in gap_id:\n                for i in range(len(gap_start)):\n                    if id > gap_start[i] and id < gap_end[i]:\n                        gap_width = float(gap_end[i] - gap_start[i])\n                        lane[id] = int((id - gap_start[i]) / gap_width * lane[gap_end[i]] + (gap_end[i] - id) / gap_width * lane[gap_start[i]])\n            assert all(x > 0 for x in lane), ""Gaps still exist!""\n            coordinate[start:end+1] = lane\n    return coordinate\n\ndef getLane(probs, thr, h, w):\n    Coordinate = []\n    Probs = maxArea(probs, h, w)\n    for probs in Probs:\n        coordinate = [-2 for i in range(56)]\n        probs = cv2.resize(probs, (1280, 720), interpolation = cv2.INTER_LINEAR)\n        for i, y in enumerate(range(160, 720, 10)):\n            row = probs[y, :]\n            m = max(row)\n            x = row.argmax()\n            if float(m)/255 > thr:\n                coordinate[i] = x\n        coordinate = fixGap(coordinate)\n        Coordinate.append(coordinate)\n    \'\'\'start = [ i for i, y in enumerate(coordinate) if y>0]\n    if start:\n        start = start[0]\n        if start <= 10:\n            coordinate[start] = -2\'\'\'\n    return Coordinate\n\ntest = \'test\'\nh = 368 # 720 # 368\nw = 640 # 1280 # 640\n#exp = \'r101_SCNN_w8_6_ft_all\'\nif test == \'val\':\n    exp = \'r50_6_ft\'\n    List = open(\'list/val.txt\', \'r\')\n    pred = open(\'pred_val_r50_6_03_03.json\', \'w\')\nelse:\n    exp = \'predicts/ENet_new/r101_SCNN_w8_6_full_all_ft\'\n    #exp = \'r101_DUC_ft\'\n    List = open(\'list/list_test.txt\', \'r\')\n    pred = open(\'pred_ENet_test.json\', \'w\')\n\nthr = 0.2\nLines = List.readlines()\n\ndef default(o):\n    if isinstance(o, np.int64): return int(o)  \n    raise TypeError\n\nfor n in range(0,2782):\n    print(n)\n    line = Lines[n]\n    img_path = line.split()[0]\n    exist_path = exp + img_path[:-3] + \'exist.txt\'\n    exist = open(exist_path, \'r\').readline().split()\n    time = int(float(exist[6])*1000)\n    exist = [int(e) for e in exist[:6]]\n    lanes = []\n    for i in range(len(exist)):\n        if exist[i] == 1:\n            prob_path = exp + img_path[:-4] + \'_\' + str(i+1) + \'_avg.png\'\n            probs = cv2.imread(prob_path, 0)\n            Coordinate = getLane(probs, thr, h, w)\n            for coordinate in Coordinate:\n                lanes.append(coordinate)\n    lanes = rmShort(lanes, 20)\n    lanes = connect(lanes)\n    lanes = rmShort(lanes, 70)\n    lanes = cutMax(lanes)\n    data = {\'raw_file\' : img_path[1:], \'lanes\' : lanes, \'run_time\' : time}\n    js = json.dumps(data, default=default)\n    #show(img_path[1:], lanes)\n    pred.write(js)\n    pred.write(\'\\n\')\n'"
ERFNet-CULane-PyTorch/erf_settings.py,0,b'# Constants:\nIN_IMAGE_W = 1640 # Input and final output image will be this size\nIN_IMAGE_H = 590 # Input and final output image will be this size\nVERTICAL_CROP_SIZE = 240 # We will crop this number of fixels from the image top\nTRAIN_IMG_W = 976 # We will train model with this width\nTRAIN_IMG_H = 208 # We will train model with this hights\nPOINTS_COUNT = 18 # Points count in the estimated curve\nLANES_COUNT = 4 # Possible lanes count\n\nIN_IMAGE_H_AFTER_CROP = IN_IMAGE_H - VERTICAL_CROP_SIZE # 350\n'
ERFNet-CULane-PyTorch/import_images.py,0,"b'import csv\nimport pandas as pd\nimport json\nimport os\nfrom pathlib import Path\nimport numpy as np\nfrom PIL import Image, ImageFont, ImageDraw, ImageEnhance\nfrom erf_settings import *\n\n# Imports images from external source. In this case https://labelbox.com/docs/exporting-data/export-format-detail.\n# It resizes images and converts points to new dimentions.\n\ncsvPath = r\'/home/brans/Downloads/export-2019-10-09T07_33_27.212Z.csv\'\nbaseDsDir = r\'/home/brans/datasets/lanes/\'\ndf = pd.read_csv(csvPath)\n\n\ndef getImgsList(path):\n    valid_images = ["".jpg"", "".JPG"", "".gif"", "".png"", "".tga"", "".jpeg"", "".JPEG""]\n    imgs = []\n    for ext in (valid_images):\n        curpath = Path(path).glob(\'**/*\' + ext)\n        imgs.extend(curpath)\n\n    imgs = list(map(str, imgs))\n    return imgs\n\ndef add_points(rec, laneId, existance, lines, img):\n    xScale = IN_IMAGE_W / img.width\n    yScale = IN_IMAGE_H / img.height\n\n    if (len(existance) > 0):\n        existance += \' \'\n\n    if (laneId in rec and laneId != \'Lane_1_1\'):\n        existance += \'1\'\n        points = rec[laneId][0][\'geometry\']\n\n        pointsStr = \'\'\n        kps = []\n        for point in points:\n            kps.append((point[\'x\'], point[\'y\']))\n\n        kps = np.array(kps).astype(float)\n        kps[:, 0] *= xScale\n        kps[:, 1] *= yScale\n        kps = np.round(kps).astype(int)\n\n        for point in kps:\n            if (len(pointsStr) > 0):\n                pointsStr += \' \'\n            pointsStr += str(point[0]) + \' \' + str(point[1])\n\n        lines += pointsStr + os.linesep\n    else:\n        existance += \'0\'\n\n    return existance, lines\n\n\nfor ind, row in df.iterrows():\n    existance = \'\'\n    lines = \'\'\n    filesList = \'\'\n    dsName = str(row[\'Dataset Name\'])\n    datasetDir = os.path.join(baseDsDir, dsName)\n\n    lable = row[\'Label\']\n    if (lable == \'Skip\'):\n        continue\n\n    imageId = row[\'External ID\']\n    cPath = os.path.join(\'\', dsName, \'data\', imageId)\n    fullPath = os.path.join(baseDsDir, dsName, \'data\', imageId)\n    img = Image.open(fullPath)\n\n    filesList += (cPath + os.linesep)\n\n    imageName = Path(imageId).stem\n    curJson = json.loads(lable)\n\n    img = img.resize((IN_IMAGE_H, IN_IMAGE_W), Image.BILINEAR)\n    img.save(os.path.join(baseDsDir, dsName, \'data\', imageId), \'PNG\')\n\n    existance, lines = add_points(curJson, \'Lane_1_1\', existance, lines, img)\n    existance, lines = add_points(curJson, \'Lane_2_1\', existance, lines, img)\n    existance, lines = add_points(curJson, \'Lane_2_2\', existance, lines, img)\n    existance, lines = add_points(curJson, \'Lane_3_1\', existance, lines, img)\n\n    with open(os.path.join(datasetDir, imageName + \'.exist.txt\'), \'a\') as the_file:\n        the_file.write(existance)\n\n    with open(os.path.join(datasetDir, \'data\', imageName + \'.lines.txt\'), \'a\') as the_file:\n        the_file.write(lines)\n\n    with open(os.path.join(baseDsDir, \'filesList.txt\'), \'a\') as the_file:\n        the_file.write(filesList)\n\n\n'"
ERFNet-CULane-PyTorch/prob_to_lines.py,0,"b""from PIL import Image,ImageDraw, ImageFilter\nfrom erf_settings import *\nimport numpy as np\n\n# Alternative to matlab script that converts probability maps to lines\n\ndef GetLane(score, thr = 0.3):\n\n    coordinate = np.zeros(POINTS_COUNT)\n    for i in range (POINTS_COUNT):\n        lineId = int(TRAIN_IMG_H - i * 20 / IN_IMAGE_H_AFTER_CROP * TRAIN_IMG_H - 1)\n        line = score[lineId, :]\n        max_id = np.argmax(line)\n        max_values = line[max_id]\n        if max_values / 255.0 > thr:\n            coordinate[i] = max_id\n\n    coordSum = np.sum(coordinate > 0)\n    if coordSum < 2:\n        coordinate = np.zeros(POINTS_COUNT)\n\n    return coordinate, coordSum\n\n\ndef GetLines(existArray, scoreMaps, thr = 0.3):\n    coordinates = []\n\n    for l in range(len(scoreMaps)):\n        if (existArray[l] or l == 0):\n            coordinate, coordSum = GetLane(scoreMaps[l], thr)\n\n            if (coordSum > 1):\n                xs = coordinate * (IN_IMAGE_W / TRAIN_IMG_W)\n                xs = np.round(xs).astype(np.int)\n                pos = xs > 0\n                curY = YS[pos]\n                curX = xs[pos]\n                curX += 1\n                coordinates.append(list(zip(curX, curY)))\n            else:\n                coordinates.append([])\n        else:\n            coordinates.append([])\n\n    return coordinates\n\ndef AddMask(img, mask, color, threshold = 0.3):\n    back = Image.new('RGB', (img.size[0], img.size[1]), color=color)\n\n    alpha = np.array(mask).astype(float) / 255\n    alpha[alpha > threshold] = 1.0\n    alpha[alpha <= threshold] = 0.0\n    alpha *= 255\n    alpha = alpha.astype(np.uint8)\n    mask = Image.fromarray(np.array(alpha), 'L')\n    mask_blur = mask.filter(ImageFilter.GaussianBlur(3))\n\n    res = Image.composite(back, img, mask_blur)\n    return res\n"""
ERFNet-CULane-PyTorch/test_erfnet.py,12,"b'import os\nimport time\nimport shutil\nimport torch\nimport torchvision\nimport torch.nn.parallel\nimport torch.backends.cudnn as cudnn\nimport torch.optim\nimport cv2\nimport utils.transforms as tf\nimport numpy as np\nimport models\nfrom models import sync_bn\nimport dataset as ds\nfrom options.options import parser\nimport torch.nn.functional as F\n\nbest_mIoU = 0\n\n\ndef main():\n    global args, best_mIoU\n    args = parser.parse_args()\n\n    os.environ[""CUDA_VISIBLE_DEVICES""] = \',\'.join(str(gpu) for gpu in args.gpus)\n    args.gpus = len(args.gpus)\n\n    if args.dataset == \'VOCAug\' or args.dataset == \'VOC2012\' or args.dataset == \'COCO\':\n        num_class = 21\n        ignore_label = 255\n        scale_series = [10, 20, 30, 60]\n    elif args.dataset == \'Cityscapes\':\n        num_class = 19\n        ignore_label = 255 \n        scale_series = [15, 30, 45, 90]\n    elif args.dataset == \'ApolloScape\':\n        num_class = 37 \n        ignore_label = 255 \n    elif args.dataset == \'CULane\':\n        num_class = 5\n        ignore_label = 255\n    else:\n        raise ValueError(\'Unknown dataset \' + args.dataset)\n\n    model = models.ERFNet(num_class)\n    input_mean = model.input_mean\n    input_std = model.input_std\n    policies = model.get_optim_policies()\n    model = torch.nn.DataParallel(model, device_ids=range(args.gpus)).cuda()\n\n    if args.resume:\n        if os.path.isfile(args.resume):\n            print((""=> loading checkpoint \'{}\'"".format(args.resume)))\n            checkpoint = torch.load(args.resume)\n            args.start_epoch = checkpoint[\'epoch\']\n            best_mIoU = checkpoint[\'best_mIoU\']\n            torch.nn.Module.load_state_dict(model, checkpoint[\'state_dict\'])\n            print((""=> loaded checkpoint \'{}\' (epoch {})"".format(args.evaluate, checkpoint[\'epoch\'])))\n        else:\n            print((""=> no checkpoint found at \'{}\'"".format(args.resume)))\n\n\n    cudnn.benchmark = True\n    cudnn.fastest = True\n\n    # Data loading code\n\n    test_loader = torch.utils.data.DataLoader(\n        getattr(ds, args.dataset.replace(""CULane"", ""VOCAug"") + \'DataSet\')(data_list=args.val_list, transform=torchvision.transforms.Compose([\n            tf.GroupRandomScaleNew(size=(args.img_width, args.img_height), interpolation=(cv2.INTER_LINEAR, cv2.INTER_NEAREST)),\n            tf.GroupNormalize(mean=(input_mean, (0, )), std=(input_std, (1, ))),\n        ])), batch_size=args.batch_size, shuffle=False, num_workers=args.workers, pin_memory=False)\n\n    # define loss function (criterion) optimizer and evaluator\n    weights = [1.0 for _ in range(5)]\n    weights[0] = 0.4\n    class_weights = torch.FloatTensor(weights).cuda()\n    criterion = torch.nn.NLLLoss(ignore_index=ignore_label, weight=class_weights).cuda()\n    for group in policies:\n        print((\'group: {} has {} params, lr_mult: {}, decay_mult: {}\'.format(group[\'name\'], len(group[\'params\']), group[\'lr_mult\'], group[\'decay_mult\'])))\n    optimizer = torch.optim.SGD(policies, args.lr, momentum=args.momentum, weight_decay=args.weight_decay)\n    evaluator = EvalSegmentation(num_class, ignore_label)\n\n    ### evaluate ###\n    validate(test_loader, model, criterion, 0, evaluator)\n    return\n\n\ndef validate(val_loader, model, criterion, iter, evaluator, logger=None):\n\n    batch_time = AverageMeter()\n    losses = AverageMeter()\n    IoU = AverageMeter()\n    mIoU = 0\n\n    # switch to evaluate mode\n    model.eval()\n\n    end = time.time()\n    for i, (input, target, img_name) in enumerate(val_loader):\n\n        input_var = torch.autograd.Variable(input, volatile=True)\n\n        # compute output\n        output, output_exist = model(input_var)\n\n        # measure accuracy and record loss\n\n        output = F.softmax(output, dim=1)\n\n        pred = output.data.cpu().numpy() # BxCxHxW\n        pred_exist = output_exist.data.cpu().numpy() # BxO\n\n        for cnt in range(len(img_name)):\n            directory = \'predicts/vgg_SCNN_DULR_w9\' + img_name[cnt][:-10]\n            if not os.path.exists(directory):\n                os.makedirs(directory)\n            file_exist = open(\'predicts/vgg_SCNN_DULR_w9\'+img_name[cnt].replace(\'.jpg\', \'.exist.txt\'), \'w\')\n            for num in range(4):\n                prob_map = (pred[cnt][num+1]*255).astype(int)\n                save_img = cv2.blur(prob_map,(9,9))\n                cv2.imwrite(\'predicts/vgg_SCNN_DULR_w9\'+img_name[cnt].replace(\'.jpg\', \'_\'+str(num+1)+\'_avg.png\'), save_img)\n                if pred_exist[cnt][num] > 0.5:\n                    file_exist.write(\'1 \')\n                else:\n                    file_exist.write(\'0 \')\n            file_exist.close()\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        if (i + 1) % args.print_freq == 0:\n            print((\'Test: [{0}/{1}]\\t\' \'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t\'.format(i, len(val_loader), batch_time=batch_time)))\n\n    print(\'finished, #test:{}\'.format(i) )\n\n    return mIoU\n\n\nclass AverageMeter(object):\n    """"""Computes and stores the average and current value""""""\n\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = None\n        self.avg = None\n        self.sum = None\n        self.count = None\n\n    def update(self, val, n=1):\n        if self.val is None:\n            self.val = val\n            self.sum = val * n\n            self.count = n\n            self.avg = self.sum / self.count\n        else:\n            self.val = val\n            self.sum += val * n\n            self.count += n\n            self.avg = self.sum / self.count\n\n\nclass EvalSegmentation(object):\n    def __init__(self, num_class, ignore_label=None):\n        self.num_class = num_class\n        self.ignore_label = ignore_label\n\n    def __call__(self, pred, gt):\n        assert (pred.shape == gt.shape)\n        gt = gt.flatten().astype(int)\n        pred = pred.flatten().astype(int)\n        locs = (gt != self.ignore_label)\n        sumim = gt + pred * self.num_class\n        hs = np.bincount(sumim[locs], minlength=self.num_class**2).reshape(self.num_class, self.num_class)\n        return hs\n\n\ndef adjust_learning_rate(optimizer, epoch, lr_steps):\n    """"""Sets the learning rate to the initial LR decayed by 10 every 30 epochs""""""\n    # decay = 0.1**(sum(epoch >= np.array(lr_steps)))\n    decay = ((1 - float(epoch) / args.epochs)**(0.9))\n    lr = args.lr * decay\n    decay = args.weight_decay\n    for param_group in optimizer.param_groups:\n        param_group[\'lr\'] = lr * param_group[\'lr_mult\']\n        param_group[\'weight_decay\'] = decay * param_group[\'decay_mult\']\n\n\nif __name__ == \'__main__\':\n    main()\n'"
ERFNet-CULane-PyTorch/train_erfnet.py,21,"b'import os\nimport time\nimport shutil\nimport torch\nimport torchvision\nimport torch.nn.parallel\nimport torch.backends.cudnn as cudnn\nimport torch.optim\nimport cv2\nimport utils.transforms as tf\nimport numpy as np\nimport models\nimport dataset as ds\nfrom options.options import parser\n\nbest_mIoU = 0\n\n\ndef main():\n    global args, best_mIoU\n    args = parser.parse_args()\n\n    os.environ[""CUDA_VISIBLE_DEVICES""] = \',\'.join(str(gpu) for gpu in args.gpus)\n    args.gpus = len(args.gpus)\n\n    if args.dataset == \'VOCAug\' or args.dataset == \'VOC2012\' or args.dataset == \'COCO\':\n        num_class = 21\n        ignore_label = 255\n        scale_series = [10, 20, 30, 60]\n    elif args.dataset == \'Cityscapes\':\n        num_class = 19\n        ignore_label = 255  # 0\n        scale_series = [15, 30, 45, 90]\n    elif args.dataset == \'ApolloScape\':\n        num_class = 37  # merge the noise and ignore labels\n        ignore_label = 255\n    elif args.dataset == \'CULane\':\n        num_class = 5\n        ignore_label = 255\n    else:\n        raise ValueError(\'Unknown dataset \' + args.dataset)\n\n    model = models.ERFNet(num_class)\n    input_mean = model.input_mean\n    input_std = model.input_std\n    model = torch.nn.DataParallel(model, device_ids=range(args.gpus)).cuda()\n\n    def load_my_state_dict(model, state_dict):  # custom function to load model when not all dict elements\n        own_state = model.state_dict()\n        ckpt_name = []\n        cnt = 0\n        for name, param in state_dict.items():\n            if name not in list(own_state.keys()) or \'output_conv\' in name:\n                 ckpt_name.append(name)\n                 continue\n            own_state[name].copy_(param)\n            cnt += 1\n        print(\'#reused param: {}\'.format(cnt))\n        return model\n\n    if args.resume:\n        if os.path.isfile(args.resume):\n            print((""=> loading checkpoint \'{}\'"".format(args.resume)))\n            checkpoint = torch.load(args.resume)\n            args.start_epoch = checkpoint[\'epoch\']\n            model = load_my_state_dict(model, checkpoint[\'state_dict\'])\n            # torch.nn.Module.load_state_dict(model, checkpoint[\'state_dict\'])\n            print((""=> loaded checkpoint \'{}\' (epoch {})"".format(args.evaluate, checkpoint[\'epoch\'])))\n        else:\n            print((""=> no checkpoint found at \'{}\'"".format(args.resume)))\n\n    cudnn.benchmark = True\n    cudnn.fastest = True\n\n    # Data loading code\n    train_loader = torch.utils.data.DataLoader(\n        getattr(ds, args.dataset.replace(""CULane"", ""VOCAug"") + \'DataSet\')(data_list=args.train_list, transform=torchvision.transforms.Compose([\n            tf.GroupRandomScale(size=(0.595, 0.621), interpolation=(cv2.INTER_LINEAR, cv2.INTER_NEAREST)),\n            tf.GroupRandomCropRatio(size=(args.img_width, args.img_height)),\n            tf.GroupRandomRotation(degree=(-1, 1), interpolation=(cv2.INTER_LINEAR, cv2.INTER_NEAREST), padding=(input_mean, (ignore_label, ))),\n            tf.GroupNormalize(mean=(input_mean, (0, )), std=(input_std, (1, ))),\n        ])), batch_size=args.batch_size, shuffle=True, num_workers=args.workers, pin_memory=False, drop_last=True)\n\n    val_loader = torch.utils.data.DataLoader(\n        getattr(ds, args.dataset.replace(""CULane"", ""VOCAug"") + \'DataSet\')(data_list=args.val_list, transform=torchvision.transforms.Compose([\n            tf.GroupRandomScale(size=(0.595, 0.621), interpolation=(cv2.INTER_LINEAR, cv2.INTER_NEAREST)),\n            tf.GroupRandomCropRatio(size=(args.img_width, args.img_height)),\n            tf.GroupNormalize(mean=(input_mean, (0, )), std=(input_std, (1, ))),\n        ])), batch_size=args.batch_size, shuffle=False, num_workers=args.workers, pin_memory=False)\n\n    # define loss function (criterion) optimizer and evaluator\n    weights = [1.0 for _ in range(5)]\n    weights[0] = 0.4\n    class_weights = torch.FloatTensor(weights).cuda()\n    criterion = torch.nn.NLLLoss(ignore_index=ignore_label, weight=class_weights).cuda()\n    criterion_exist = torch.nn.BCEWithLogitsLoss().cuda()\n    optimizer = torch.optim.SGD(model.parameters(), args.lr, momentum=args.momentum, weight_decay=args.weight_decay)\n    evaluator = EvalSegmentation(num_class, ignore_label)\n\n    args.evaluate = True\n\n    if args.evaluate:\n        validate(val_loader, model, criterion, 0, evaluator)\n        return\n\n    for epoch in range(args.epochs):  # args.start_epoch\n        adjust_learning_rate(optimizer, epoch, args.lr_steps)\n\n        # train for one epoch\n        train(train_loader, model, criterion, criterion_exist, optimizer, epoch)\n\n        # evaluate on validation set\n        if (epoch + 1) % args.eval_freq == 0 or epoch == args.epochs - 1:\n            mIoU = validate(val_loader, model, criterion, (epoch + 1) * len(train_loader), evaluator)\n            # remember best mIoU and save checkpoint\n            is_best = mIoU > best_mIoU\n            best_mIoU = max(mIoU, best_mIoU)\n            save_checkpoint({\n                \'epoch\': epoch + 1,\n                \'arch\': args.arch,\n                \'state_dict\': model.state_dict(),\n                \'best_mIoU\': best_mIoU,\n            }, is_best)\n\n\ndef train(train_loader, model, criterion, criterion_exist, optimizer, epoch):\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n    losses_exist = AverageMeter()\n\n    # switch to train mode\n    model.train()\n\n    end = time.time()\n    for i, (input, target, target_exist) in enumerate(train_loader):\n        # measure data loading time\n        data_time.update(time.time() - end)\n\n        target = target.cuda()\n        target_exist = target_exist.float().cuda()\n        input_var = torch.autograd.Variable(input)\n        target_var = torch.autograd.Variable(target)\n        target_exist_var = torch.autograd.Variable(target_exist)\n\n        # compute output\n        output, output_exist = model(input_var)  # output_mid\n        loss = criterion(torch.nn.functional.log_softmax(output, dim=1), target_var)\n        # print(output_exist.data.cpu().numpy().shape)\n        loss_exist = criterion_exist(output_exist, target_exist_var)\n        loss_tot = loss + loss_exist * 0.1\n\n        # measure accuracy and record loss\n        losses.update(loss.data.item(), input.size(0))\n        losses_exist.update(loss_exist.item(), input.size(0))\n\n        # compute gradient and do SGD step\n        optimizer.zero_grad()\n        loss_tot.backward()\n        optimizer.step()\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        if (i + 1) % args.print_freq == 0:\n            print((\n                      \'Epoch: [{0}][{1}/{2}], lr: {lr:.5f}\\t\' \'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t\' \'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t\' \'Loss {loss.val:.4f} ({loss.avg:.4f})\\t\' \'Loss_exist {loss_exist.val:.4f} ({loss_exist.avg:.4f})\\t\'.format(\n                          epoch, i, len(train_loader), batch_time=batch_time, data_time=data_time, loss=losses,\n                          loss_exist=losses_exist, lr=optimizer.param_groups[-1][\'lr\'])))\n            batch_time.reset()\n            data_time.reset()\n            losses.reset()\n\n\ndef flip(x, dim):\n    xsize = x.size()\n    dim = x.dim() + dim if dim < 0 else dim\n    x = x.view(-1, *xsize[dim:])\n    x = x.view(x.size(0), x.size(1), -1)[:,\n        getattr(torch.arange(x.size(1) - 1, -1, -1), (\'cpu\', \'cuda\')[x.is_cuda])().long(), :]\n    return x.view(xsize)\n\n\ndef validate(val_loader, model, criterion, iter, evaluator, logger=None):\n    batch_time = AverageMeter()\n    losses = AverageMeter()\n    IoU = AverageMeter()\n\n    # switch to evaluate mode\n    model.eval()\n\n    end = time.time()\n    for i, (input, target, target_exist) in enumerate(val_loader):\n        target = target.cuda()\n        input_var = torch.autograd.Variable(input, volatile=True)\n        target_var = torch.autograd.Variable(target)\n\n        # compute output\n        output, _ = model(input_var)\n        loss = criterion(torch.nn.functional.log_softmax(output, dim=1), target_var)\n\n        # measure accuracy and record loss\n\n        pred = output.data.cpu().numpy().transpose(0, 2, 3, 1)\n        pred = np.argmax(pred, axis=3).astype(np.uint8)\n        IoU.update(evaluator(pred, target.cpu().numpy()))\n        losses.update(loss.data.item(), input.size(0))\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        if (i + 1) % args.print_freq == 0:\n            acc = np.sum(np.diag(IoU.sum)) / float(np.sum(IoU.sum))\n            mIoU = np.diag(IoU.sum) / (1e-20 + IoU.sum.sum(1) + IoU.sum.sum(0) - np.diag(IoU.sum))\n            mIoU = np.sum(mIoU) / len(mIoU)\n            print((\'Test: [{0}/{1}]\\t\' \'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t\' \'Loss {loss.val:.4f} ({loss.avg:.4f})\\t\' \'Pixels Acc {acc:.3f}\\t\' \'mIoU {mIoU:.3f}\'.format(i, len(val_loader), batch_time=batch_time, loss=losses, acc=acc, mIoU=mIoU)))\n\n    acc = np.sum(np.diag(IoU.sum)) / float(np.sum(IoU.sum))\n    mIoU = np.diag(IoU.sum) / (1e-20 + IoU.sum.sum(1) + IoU.sum.sum(0) - np.diag(IoU.sum))\n    mIoU = np.sum(mIoU) / len(mIoU)\n    print((\'Testing Results: Pixels Acc {acc:.3f}\\tmIoU {mIoU:.3f} ({bestmIoU:.4f})\\tLoss {loss.avg:.5f}\'.format(acc=acc, mIoU=mIoU, bestmIoU=max(mIoU, best_mIoU), loss=losses)))\n\n    return mIoU\n\n\ndef save_checkpoint(state, is_best, filename=\'checkpoint.pth.tar\'):\n    if not os.path.exists(\'trained\'):\n        os.makedirs(\'trained\')\n    filename = os.path.join(\'trained\', \'_\'.join((args.snapshot_pref, args.method.lower(), filename)))\n    torch.save(state, filename)\n    if is_best:\n        best_name = os.path.join(\'trained\', \'_\'.join((args.snapshot_pref, args.method.lower(), \'model_best.pth.tar\')))\n        shutil.copyfile(filename, best_name)\n\n\nclass AverageMeter(object):\n    """"""Computes and stores the average and current value""""""\n\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = None\n        self.avg = None\n        self.sum = None\n        self.count = None\n\n    def update(self, val, n=1):\n        if self.val is None:\n            self.val = val\n            self.sum = val * n\n            self.count = n\n            self.avg = self.sum / self.count\n        else:\n            self.val = val\n            self.sum += val * n\n            self.count += n\n            self.avg = self.sum / self.count\n\n\nclass EvalSegmentation(object):\n    def __init__(self, num_class, ignore_label=None):\n        self.num_class = num_class\n        self.ignore_label = ignore_label\n\n    def __call__(self, pred, gt):\n        assert (pred.shape == gt.shape)\n        gt = gt.flatten().astype(int)\n        pred = pred.flatten().astype(int)\n        locs = (gt != self.ignore_label)\n        sumim = gt + pred * self.num_class\n        hs = np.bincount(sumim[locs], minlength=self.num_class ** 2).reshape(self.num_class, self.num_class)\n        return hs\n\n\ndef adjust_learning_rate(optimizer, epoch, lr_steps):\n    """"""Sets the learning rate to the initial LR decayed by 10 every 30 epochs""""""\n    # decay = 0.1**(sum(epoch >= np.array(lr_steps)))\n    decay = ((1 - float(epoch) / args.epochs)**(0.9))\n    lr = args.lr * decay\n    decay = args.weight_decay\n    \n    for param_group in optimizer.param_groups:\n        param_group[\'lr\'] = lr \n        param_group[\'weight_decay\'] = decay\n\n\nif __name__ == \'__main__\':\n    main()\n'"
ENet-TuSimple-Torch/evaluate/lane.py,0,"b""import numpy as np\nfrom sklearn.linear_model import LinearRegression\nimport ujson as json\n\n\nclass LaneEval(object):\n    lr = LinearRegression()\n    pixel_thresh = 20\n    pt_thresh = 0.85\n\n    @staticmethod\n    def get_angle(xs, y_samples):\n        xs, ys = xs[xs >= 0], y_samples[xs >= 0]\n        if len(xs) > 1:\n            LaneEval.lr.fit(ys[:, None], xs)\n            k = LaneEval.lr.coef_[0]\n            theta = np.arctan(k)\n        else:\n            theta = 0\n        return theta\n\n    @staticmethod\n    def line_accuracy(pred, gt, thresh):\n        pred = np.array([p if p >= 0 else -100 for p in pred])\n        gt = np.array([g if g >= 0 else -100 for g in gt])\n        return np.sum(np.where(np.abs(pred - gt) < thresh, 1., 0.)) / len(gt)\n\n    @staticmethod\n    def bench(pred, gt, y_samples, running_time):\n        if any(len(p) != len(y_samples) for p in pred):\n            raise Exception('Format of lanes error.')\n        if running_time > 200 or len(gt) + 2 < len(pred):\n            return 0., 0., 1.\n        angles = [LaneEval.get_angle(np.array(x_gts), np.array(y_samples)) for x_gts in gt]\n        threshs = [LaneEval.pixel_thresh / np.cos(angle) for angle in angles]\n        line_accs = []\n        fp, fn = 0., 0.\n        matched = 0.\n        for x_gts, thresh in zip(gt, threshs):\n            accs = [LaneEval.line_accuracy(np.array(x_preds), np.array(x_gts), thresh) for x_preds in pred]\n            max_acc = np.max(accs) if len(accs) > 0 else 0.\n            if max_acc < LaneEval.pt_thresh:\n                fn += 1\n            else:\n                matched += 1\n            line_accs.append(max_acc)\n        fp = len(pred) - matched\n        if len(gt) > 4 and fn > 0:\n            fn -= 1\n        s = sum(line_accs)\n        if len(gt) > 4:\n            s -= min(line_accs)\n        return s / max(min(4.0, len(gt)), 1.), fp / len(pred) if len(pred) > 0 else 0., fn / max(min(len(gt), 4.) , 1.)\n\n    @staticmethod\n    def bench_one_submit(pred_file, gt_file):\n        try:\n            json_pred = [json.loads(line) for line in open(pred_file).readlines()]\n        except BaseException as e:\n            raise Exception('Fail to load json file of the prediction.')\n        json_gt = [json.loads(line) for line in open(gt_file).readlines()]\n        if len(json_gt) != len(json_pred):\n            raise Exception('We do not get the predictions of all the test tasks')\n        gts = {l['raw_file']: l for l in json_gt}\n        accuracy, fp, fn = 0., 0., 0.\n        for pred in json_pred:\n            if 'raw_file' not in pred or 'lanes' not in pred or 'run_time' not in pred:\n                raise Exception('raw_file or lanes or run_time not in some predictions.')\n            raw_file = pred['raw_file']\n            pred_lanes = pred['lanes']\n            run_time = pred['run_time']\n            if raw_file not in gts:\n                raise Exception('Some raw_file from your predictions do not exist in the test tasks.')\n            gt = gts[raw_file]\n            gt_lanes = gt['lanes']\n            y_samples = gt['h_samples']\n            try:\n                a, p, n = LaneEval.bench(pred_lanes, gt_lanes, y_samples, run_time)\n            except BaseException as e:\n                raise Exception('Format of lanes error.')\n            accuracy += a\n            fp += p\n            fn += n\n        num = len(gts)\n        # the first return parameter is the default ranking parameter\n        return json.dumps([\n            {'name': 'Accuracy', 'value': accuracy / num, 'order': 'desc'},\n            {'name': 'FP', 'value': fp / num, 'order': 'asc'},\n            {'name': 'FN', 'value': fn / num, 'order': 'asc'}\n        ])\n\n\nif __name__ == '__main__':\n    import sys\n    try:\n        if len(sys.argv) != 3:\n            raise Exception('Invalid input arguments')\n        print( LaneEval.bench_one_submit(sys.argv[1], sys.argv[2]) )\n    except Exception as e:\n        print( e.message )\n        sys.exit(e.message)\n"""
ERFNet-CULane-PyTorch/dataset/__init__.py,0,b'from .voc_aug import VOCAugDataSet\n'
ERFNet-CULane-PyTorch/dataset/voc_aug.py,3,"b'import os\nimport numpy as np\nimport cv2\nimport torch\nfrom torch.utils.data import Dataset\n\n\nclass VOCAugDataSet(Dataset):\n    def __init__(self, dataset_path=\'/home/houyuenan/remote/ApolloScapes/Codes-for-Lane-Detection/ENet-SAD-Pytorch/data/CULane/list\', data_list=\'train\', transform=None):\n\n        with open(os.path.join(dataset_path, data_list + \'.txt\')) as f:\n            self.img_list = []\n            self.img = []\n            self.label_list = []\n            self.exist_list = []\n            for line in f:\n                self.img.append(line.strip().split("" "")[0])\n                self.img_list.append(dataset_path.replace(\'/list\', \'\') + line.strip().split("" "")[0])\n                self.label_list.append(dataset_path.replace(\'/list\', \'\') + line.strip().split("" "")[1])\n                self.exist_list.append(np.array([int(line.strip().split("" "")[2]), int(line.strip().split("" "")[3]), int(line.strip().split("" "")[4]), int(line.strip().split("" "")[5])]))\n\n        self.img_path = dataset_path\n        self.gt_path = dataset_path\n        self.transform = transform\n        self.is_testing = data_list == \'test_img\' # \'val\'\n\n    def __len__(self):\n        return len(self.img_list)\n\n    def __getitem__(self, idx):\n        image = cv2.imread(os.path.join(self.img_path, self.img_list[idx])).astype(np.float32)\n        label = cv2.imread(os.path.join(self.gt_path, self.label_list[idx]), cv2.IMREAD_UNCHANGED)\n        exist = self.exist_list[idx]\n        image = image[240:, :, :]\n        label = label[240:, :]\n        label = label.squeeze()\n        if self.transform:\n            image, label = self.transform((image, label))\n            image = torch.from_numpy(image).permute(2, 0, 1).contiguous().float()\n            label = torch.from_numpy(label).contiguous().long()\n        if self.is_testing:\n            return image, label, self.img[idx]\n        else:\n            return image, label, exist\n'"
ERFNet-CULane-PyTorch/models/__init__.py,0,b'from .erfnet import ERFNet \n'
ERFNet-CULane-PyTorch/models/erfnet.py,4,"b'# ERFNET full network definition for Pytorch\n# Sept 2017\n# Eduardo Romera\n#######################\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.init as init\nimport torch.nn.functional as F\n\n\nclass DownsamplerBlock(nn.Module):\n    def __init__(self, ninput, noutput):\n        super().__init__()\n\n        self.conv = nn.Conv2d(ninput, noutput - ninput, (3, 3), stride=2, padding=1, bias=True)\n        self.pool = nn.MaxPool2d(2, stride=2)\n        self.bn = nn.BatchNorm2d(noutput, eps=1e-3)\n\n    def forward(self, input):\n        output = torch.cat([self.conv(input), self.pool(input)], 1)\n        output = self.bn(output)\n        return F.relu(output)\n\n\nclass non_bottleneck_1d(nn.Module):\n    def __init__(self, chann, dropprob, dilated):\n        super().__init__()\n\n        self.conv3x1_1 = nn.Conv2d(chann, chann, (3, 1), stride=1, padding=(1, 0), bias=True)\n\n        self.conv1x3_1 = nn.Conv2d(chann, chann, (1, 3), stride=1, padding=(0, 1), bias=True)\n\n        self.bn1 = nn.BatchNorm2d(chann, eps=1e-03)\n\n        self.conv3x1_2 = nn.Conv2d(chann, chann, (3, 1), stride=1, padding=(1 * dilated, 0), bias=True,\n                                   dilation=(dilated, 1))\n\n        self.conv1x3_2 = nn.Conv2d(chann, chann, (1, 3), stride=1, padding=(0, 1 * dilated), bias=True,\n                                   dilation=(1, dilated))\n\n        self.bn2 = nn.BatchNorm2d(chann, eps=1e-03)\n\n        self.dropout = nn.Dropout2d(dropprob)\n\n    def forward(self, input):\n        output = self.conv3x1_1(input)\n        output = F.relu(output)\n        output = self.conv1x3_1(output)\n        output = self.bn1(output)\n        output = F.relu(output)\n\n        output = self.conv3x1_2(output)\n        output = F.relu(output)\n        output = self.conv1x3_2(output)\n        output = self.bn2(output)\n\n        if (self.dropout.p != 0):\n            output = self.dropout(output)\n\n        return F.relu(output + input)  # +input = identity (residual connection)\n\n\nclass Encoder(nn.Module):\n    def __init__(self, num_classes):\n        super().__init__()\n        self.initial_block = DownsamplerBlock(3, 16)\n\n        self.layers = nn.ModuleList()\n\n        self.layers.append(DownsamplerBlock(16, 64))\n\n        for x in range(0, 5):  # 5 times\n            self.layers.append(non_bottleneck_1d(64, 0.1, 1))\n\n        self.layers.append(DownsamplerBlock(64, 128))\n\n        for x in range(0, 2):  # 2 times\n            self.layers.append(non_bottleneck_1d(128, 0.1, 2))\n            self.layers.append(non_bottleneck_1d(128, 0.1, 4))\n            self.layers.append(non_bottleneck_1d(128, 0.1, 8))\n            self.layers.append(non_bottleneck_1d(128, 0.1, 16))\n\n        # only for encoder mode:\n        self.output_conv = nn.Conv2d(128, num_classes, 1, stride=1, padding=0, bias=True)\n\n    def forward(self, input, predict=False):\n        output = self.initial_block(input)\n\n        for layer in self.layers:\n            output = layer(output)\n\n        if predict:\n            output = self.output_conv(output)\n\n        return output\n\n\nclass UpsamplerBlock(nn.Module):\n    def __init__(self, ninput, noutput):\n        super().__init__()\n        self.conv = nn.ConvTranspose2d(ninput, noutput, 3, stride=2, padding=1, output_padding=1, bias=True)\n        self.bn = nn.BatchNorm2d(noutput, eps=1e-3, track_running_stats=True)\n\n    def forward(self, input):\n        output = self.conv(input)\n        output = self.bn(output)\n        return F.relu(output)\n\n\nclass Decoder(nn.Module):\n    def __init__(self, num_classes):\n        super().__init__()\n\n        self.layers = nn.ModuleList()\n\n        self.layers.append(UpsamplerBlock(128, 64))\n        self.layers.append(non_bottleneck_1d(64, 0, 1))\n        self.layers.append(non_bottleneck_1d(64, 0, 1))\n\n        self.layers.append(UpsamplerBlock(64, 16))\n        self.layers.append(non_bottleneck_1d(16, 0, 1))\n        self.layers.append(non_bottleneck_1d(16, 0, 1))\n\n        self.output_conv = nn.ConvTranspose2d(16, num_classes, 2, stride=2, padding=0, output_padding=0, bias=True)\n\n    def forward(self, input):\n        output = input\n\n        for layer in self.layers:\n            output = layer(output)\n\n        output = self.output_conv(output)\n\n        return output\n\n\nclass Lane_exist(nn.Module):\n    def __init__(self, num_output):\n        super().__init__()\n\n        self.layers = nn.ModuleList()\n\n        self.layers.append(nn.Conv2d(128, 32, (3, 3), stride=1, padding=(4, 4), bias=False, dilation=(4, 4)))\n        self.layers.append(nn.BatchNorm2d(32, eps=1e-03))\n\n        self.layers_final = nn.ModuleList()\n\n        self.layers_final.append(nn.Dropout2d(0.1))\n        self.layers_final.append(nn.Conv2d(32, 5, (1, 1), stride=1, padding=(0, 0), bias=True))\n\n        self.maxpool = nn.MaxPool2d(2, stride=2)\n        self.linear1 = nn.Linear(3965, 128)\n        self.linear2 = nn.Linear(128, 4)\n\n    def forward(self, input):\n        output = input\n\n        for layer in self.layers:\n            output = layer(output)\n\n        output = F.relu(output)\n\n        for layer in self.layers_final:\n            output = layer(output)\n\n        output = F.softmax(output, dim=1)\n        output = self.maxpool(output)\n        # print(output.shape)\n        output = output.view(-1, 3965)\n        output = self.linear1(output)\n        output = F.relu(output)\n        output = self.linear2(output)\n        output = F.sigmoid(output)\n\n        return output\n\n\nclass ERFNet(nn.Module):\n    def __init__(self, num_classes, encoder=None):  # use encoder to pass pretrained encoder\n        super().__init__()\n\n        if (encoder == None):\n            self.encoder = Encoder(num_classes)\n        else:\n            self.encoder = encoder\n        self.decoder = Decoder(num_classes)\n        self.lane_exist = Lane_exist(4)  # num_output\n        self.input_mean = [103.939, 116.779, 123.68]  # [0, 0, 0]\n        self.input_std = [1, 1, 1]\n\n    def train(self, mode=True):\n        """"""\n        Override the default train() to freeze the BN parameters\n        :return:\n        """"""\n        super(ERFNet, self).train(mode)\n\n    def get_optim_policies(self):\n        base_weight = []\n        base_bias = []\n        base_bn = []\n\n        addtional_weight = []\n        addtional_bias = []\n        addtional_bn = []\n\n        # print(self.modules())\n\n        for m in self.encoder.modules():  # self.base_model.modules()\n            if isinstance(m, nn.Conv2d):\n                # print(1)\n                ps = list(m.parameters())\n                base_weight.append(ps[0])\n                if len(ps) == 2:\n                    base_bias.append(ps[1])\n            elif isinstance(m, nn.BatchNorm2d):\n                # print(2)\n                base_bn.extend(list(m.parameters()))\n\n        for m in self.decoder.modules():  # self.base_model.modules()\n            if isinstance(m, nn.Conv2d):\n                # print(1)\n                ps = list(m.parameters())\n                base_weight.append(ps[0])\n                if len(ps) == 2:\n                    base_bias.append(ps[1])\n            elif isinstance(m, nn.BatchNorm2d):\n                # print(2)\n                base_bn.extend(list(m.parameters()))\n\n        return [\n            {\n                \'params\': addtional_weight,\n                \'lr_mult\': 10,\n                \'decay_mult\': 1,\n                \'name\': ""addtional weight""\n            },\n            {\n                \'params\': addtional_bias,\n                \'lr_mult\': 20,\n                \'decay_mult\': 1,\n                \'name\': ""addtional bias""\n            },\n            {\n                \'params\': addtional_bn,\n                \'lr_mult\': 10,\n                \'decay_mult\': 0,\n                \'name\': ""addtional BN scale/shift""\n            },\n            {\n                \'params\': base_weight,\n                \'lr_mult\': 1,\n                \'decay_mult\': 1,\n                \'name\': ""base weight""\n            },\n            {\n                \'params\': base_bias,\n                \'lr_mult\': 2,\n                \'decay_mult\': 0,\n                \'name\': ""base bias""\n            },\n            {\n                \'params\': base_bn,\n                \'lr_mult\': 1,\n                \'decay_mult\': 0,\n                \'name\': ""base BN scale/shift""\n            },\n        ]\n\n    def forward(self, input, only_encode=False):\n        \'\'\'if only_encode:\n            return self.encoder.forward(input, predict=True)\n        else:\'\'\'\n        output = self.encoder(input)  # predict=False by default\n        return self.decoder.forward(output), self.lane_exist(output)\n\n'"
ERFNet-CULane-PyTorch/options/__init__.py,0,b'\n'
ERFNet-CULane-PyTorch/options/options.py,0,"b'import argparse\nparser = argparse.ArgumentParser(description=""PyTorch implementation of Semantic Segmentation"")\n\nparser.add_argument(\'dataset\', type=str, choices=[\'VOCAug\', \'VOC2012\', \'COCO\', \'Cityscapes\', \'ApolloScape\', \'CULane\'])\nparser.add_argument(\'method\', type=str, choices=[\'FCN\', \'DeepLab\', \'DeepLab3\', \'PSPNet\', \'ERFNet\'])\nparser.add_argument(\'train_list\', type=str)\nparser.add_argument(\'val_list\', type=str)\n\n# ========================= Model Configs ==========================\nparser.add_argument(\'--arch\', type=str, default=""resnet101"")\nparser.add_argument(\'--dropout\', \'--do\', default=0.1, type=float, metavar=\'DO\', help=\'dropout ratio (default: 0.1)\')\nparser.add_argument(\'--train_size\', default=840, type=int, metavar=\'L\', help=\'size of training patches (default: 473)\')\nparser.add_argument(\'--test_size\', default=840, type=int, metavar=\'L\', help=\'size of testing patches (default: 513)\')\nparser.add_argument(\'--img_height\', default=208, type=int, metavar=\'L\', help=\'height of input images (default: 208)\')\nparser.add_argument(\'--img_width\', default=976, type=int, metavar=\'L\', help=\'width of input images (default: 976)\')\nparser.add_argument(\'--local_rank\', type=int, default=0) # distributed data parallel\n\n# ========================= Learning Configs ==========================\nparser.add_argument(\'--epochs\', default=24, type=int, metavar=\'N\', help=\'number of total epochs to run\')\nparser.add_argument(\'-b\', \'--batch-size\', default=8, type=int, metavar=\'N\', help=\'mini-batch size (default: 256)\')\nparser.add_argument(\'--lr\', \'--learning-rate\', default=0.001, type=float, metavar=\'LR\', help=\'initial learning rate\')\nparser.add_argument(\'--lr_steps\', default=[10, 20], type=float, nargs=""+"", metavar=\'LRSteps\', help=\'epochs to decay learning rate by 10\')\nparser.add_argument(\'--momentum\', default=0.9, type=float, metavar=\'M\', help=\'momentum\')\nparser.add_argument(\'--weight-decay\', \'--wd\', default=1e-4, type=float, metavar=\'W\', help=\'weight decay (default: 5e-4)\')\n\n# ========================= Monitor Configs ==========================\nparser.add_argument(\'--print-freq\', \'-p\', default=1, type=int, metavar=\'N\', help=\'print frequency (default: 10)\')\nparser.add_argument(\'--eval-freq\', \'-ef\', default=1, type=int, metavar=\'N\', help=\'evaluation frequency (default: 5)\')\n\n# ========================= Runtime Configs ==========================\nparser.add_argument(\'-j\', \'--workers\', default=16, type=int, metavar=\'N\', help=\'number of data loading workers (default: 16)\')\nparser.add_argument(\'--resume\', default=\'\', type=str, metavar=\'PATH\', help=\'path to latest checkpoint (default: none)\')\nparser.add_argument(\'--weight\', default=\'\', type=str, metavar=\'PATH\', help=\'path to initial weight (default: none)\')\nparser.add_argument(\'-e\', \'--evaluate\', dest=\'evaluate\', action=\'store_true\', help=\'evaluate model on validation set\') # true\nparser.add_argument(\'--snapshot_pref\', type=str, default="""")\nparser.add_argument(\'--start-epoch\', default=0, type=int, metavar=\'N\', help=\'manual epoch number (useful on restarts)\')\nparser.add_argument(\'--gpus\', nargs=\'+\', type=int, default=None)\n'"
ERFNet-CULane-PyTorch/utils/transforms.py,3,"b'import random\nimport cv2\nimport numpy as np\nimport numbers\n\n__all__ = [\'GroupRandomCrop\', \'GroupCenterCrop\', \'GroupRandomPad\', \'GroupCenterPad\', \'GroupRandomScale\', \'GroupRandomHorizontalFlip\', \'GroupNormalize\']\n\n\nclass GroupRandomCrop(object):\n    def __init__(self, size):\n        if isinstance(size, numbers.Number):\n            self.size = (int(size), int(size))\n        else:\n            self.size = size\n\n    def __call__(self, img_group):\n        h, w = img_group[0].shape[0:2]\n        th, tw = self.size\n\n        out_images = list()\n        h1 = random.randint(0, max(0, h - th))\n        w1 = random.randint(0, max(0, w - tw))\n        h2 = min(h1 + th, h)\n        w2 = min(w1 + tw, w)\n\n        for img in img_group:\n            assert (img.shape[0] == h and img.shape[1] == w)\n            out_images.append(img[h1:h2, w1:w2, ...])\n        return out_images\n\nclass GroupRandomCropRatio(object):\n    def __init__(self, size):\n        if isinstance(size, numbers.Number):\n            self.size = (int(size), int(size))\n        else:\n            self.size = size\n\n    def __call__(self, img_group):\n        h, w = img_group[0].shape[0:2]\n        tw, th = self.size\n\n        out_images = list()\n        h1 = random.randint(0, max(0, h - th))\n        w1 = random.randint(0, max(0, w - tw))\n        h2 = min(h1 + th, h)\n        w2 = min(w1 + tw, w)\n\n        for img in img_group:\n            assert (img.shape[0] == h and img.shape[1] == w)\n            out_images.append(img[h1:h2, w1:w2, ...])\n        return out_images\n\n\nclass GroupCenterCrop(object):\n    def __init__(self, size):\n        if isinstance(size, numbers.Number):\n            self.size = (int(size), int(size))\n        else:\n            self.size = size\n\n    def __call__(self, img_group):\n        h, w = img_group[0].shape[0:2]\n        th, tw = self.size\n\n        out_images = list()\n        h1 = max(0, int((h - th) / 2))\n        w1 = max(0, int((w - tw) / 2))\n        h2 = min(h1 + th, h)\n        w2 = min(w1 + tw, w)\n\n        for img in img_group:\n            assert (img.shape[0] == h and img.shape[1] == w)\n            out_images.append(img[h1:h2, w1:w2, ...])\n        return out_images\n\n\nclass GroupRandomPad(object):\n    def __init__(self, size, padding):\n        if isinstance(size, numbers.Number):\n            self.size = (int(size), int(size))\n        else:\n            self.size = size\n        self.padding = padding\n\n    def __call__(self, img_group):\n        assert (len(self.padding) == len(img_group))\n        h, w = img_group[0].shape[0:2]\n        th, tw = self.size\n\n        out_images = list()\n        h1 = random.randint(0, max(0, th - h))\n        w1 = random.randint(0, max(0, tw - w))\n        h2 = max(th - h - h1, 0)\n        w2 = max(tw - w - w1, 0)\n\n        for img, padding in zip(img_group, self.padding):\n            assert (img.shape[0] == h and img.shape[1] == w)\n            out_images.append(cv2.copyMakeBorder(img, h1, h2, w1, w2, cv2.BORDER_CONSTANT, value=padding))\n            if len(img.shape) > len(out_images[-1].shape):\n                out_images[-1] = out_images[-1][..., np.newaxis]  # single channel image\n        return out_images\n\n\nclass GroupCenterPad(object):\n    def __init__(self, size, padding):\n        if isinstance(size, numbers.Number):\n            self.size = (int(size), int(size))\n        else:\n            self.size = size\n        self.padding = padding\n\n    def __call__(self, img_group):\n        assert (len(self.padding) == len(img_group))\n        h, w = img_group[0].shape[0:2]\n        th, tw = self.size\n\n        out_images = list()\n        h1 = max(0, int((th - h) / 2))\n        w1 = max(0, int((tw - w) / 2))\n        h2 = max(th - h - h1, 0)\n        w2 = max(tw - w - w1, 0)\n\n        for img, padding in zip(img_group, self.padding):\n            assert (img.shape[0] == h and img.shape[1] == w)\n            out_images.append(cv2.copyMakeBorder(img, h1, h2, w1, w2, cv2.BORDER_CONSTANT, value=padding))\n            if len(img.shape) > len(out_images[-1].shape):\n                out_images[-1] = out_images[-1][..., np.newaxis]  # single channel image\n        return out_images\n\n\nclass GroupConcerPad(object):\n    def __init__(self, size, padding):\n        if isinstance(size, numbers.Number):\n            self.size = (int(size), int(size))\n        else:\n            self.size = size\n        self.padding = padding\n\n    def __call__(self, img_group):\n        assert (len(self.padding) == len(img_group))\n        h, w = img_group[0].shape[0:2]\n        th, tw = self.size\n\n        out_images = list()\n        h1 = 0\n        w1 = 0\n        h2 = max(th - h - h1, 0)\n        w2 = max(tw - w - w1, 0)\n\n        for img, padding in zip(img_group, self.padding):\n            assert (img.shape[0] == h and img.shape[1] == w)\n            out_images.append(cv2.copyMakeBorder(img, h1, h2, w1, w2, cv2.BORDER_CONSTANT, value=padding))\n            if len(img.shape) > len(out_images[-1].shape):\n                out_images[-1] = out_images[-1][..., np.newaxis]  # single channel image\n        return out_images\n\nclass GroupRandomScaleNew(object):\n    def __init__(self, size=(976, 208), interpolation=(cv2.INTER_LINEAR, cv2.INTER_NEAREST)):\n        self.size = size\n        self.interpolation = interpolation\n\n    def __call__(self, img_group):\n        assert (len(self.interpolation) == len(img_group))\n        scale_w, scale_h = self.size[0] * 1.0 / 1640, self.size[1] * 1.0 / 350\n        out_images = list()\n        for img, interpolation in zip(img_group, self.interpolation):\n            out_images.append(cv2.resize(img, None, fx=scale_w, fy=scale_h, interpolation=interpolation))\n            if len(img.shape) > len(out_images[-1].shape):\n                out_images[-1] = out_images[-1][..., np.newaxis]  # single channel image\n        return out_images\n\n\nclass GroupRandomScale(object):\n    def __init__(self, size=(0.5, 1.5), interpolation=(cv2.INTER_LINEAR, cv2.INTER_NEAREST)):\n        self.size = size\n        self.interpolation = interpolation\n\n    def __call__(self, img_group):\n        assert (len(self.interpolation) == len(img_group))\n        scale = random.uniform(self.size[0], self.size[1])\n        out_images = list()\n        for img, interpolation in zip(img_group, self.interpolation):\n            out_images.append(cv2.resize(img, None, fx=scale, fy=scale, interpolation=interpolation))\n            if len(img.shape) > len(out_images[-1].shape):\n                out_images[-1] = out_images[-1][..., np.newaxis]  # single channel image\n        return out_images\n\nclass GroupRandomMultiScale(object):\n    def __init__(self, size=(0.5, 1.5), interpolation=(cv2.INTER_LINEAR, cv2.INTER_NEAREST)):\n        self.size = size\n        self.interpolation = interpolation\n\n    def __call__(self, img_group):\n        assert (len(self.interpolation) == len(img_group))\n        scales = [0.5, 1.0, 1.5] # random.uniform(self.size[0], self.size[1])\n        out_images = list()\n        for scale in scales:\n            for img, interpolation in zip(img_group, self.interpolation):\n                out_images.append(cv2.resize(img, None, fx=scale, fy=scale, interpolation=interpolation))\n                if len(img.shape) > len(out_images[-1].shape):\n                    out_images[-1] = out_images[-1][..., np.newaxis]  # single channel image\n        return out_images\n\nclass GroupRandomScaleRatio(object):\n    def __init__(self, size=(680, 762, 562, 592), interpolation=(cv2.INTER_LINEAR, cv2.INTER_NEAREST)):\n        self.size = size\n        self.interpolation = interpolation\n        self.origin_id = [0, 1360, 580, 768, 255, 300, 680, 710, 312, 1509, 800, 1377, 880, 910, 1188, 128, 960, 1784, 1414, 1150, 512, 1162, 950, 750, 1575, 708, 2111, 1848, 1071, 1204, 892, 639, 2040, 1524, 832, 1122, 1224, 2295]\n\n    def __call__(self, img_group):\n        assert (len(self.interpolation) == len(img_group))\n        w_scale = random.randint(self.size[0], self.size[1])\n        h_scale = random.randint(self.size[2], self.size[3])\n        h, w, _ = img_group[0].shape\n        out_images = list()\n        out_images.append(cv2.resize(img_group[0], None, fx=w_scale*1.0/w, fy=h_scale*1.0/h, interpolation=self.interpolation[0])) # fx=w_scale*1.0/w, fy=h_scale*1.0/h\n        ### process label map ###\n        origin_label = cv2.resize(img_group[1], None, fx=w_scale*1.0/w, fy=h_scale*1.0/h, interpolation=self.interpolation[1])\n        origin_label = origin_label.astype(int)\n        label = origin_label[:, :, 0] * 5 + origin_label[:, :, 1] * 3 + origin_label[:, :, 2]\n        new_label = np.ones(label.shape) * 100\n        new_label = new_label.astype(int)\n        for cnt in range(37):\n            new_label = (label == self.origin_id[cnt]) * (cnt - 100) + new_label\n        new_label = (label == self.origin_id[37]) * (36 - 100) + new_label\n        assert(100 not in np.unique(new_label))\n        out_images.append(new_label)\n        return out_images\n\nclass GroupRandomRotation(object):\n    def __init__(self, degree=(-10, 10), interpolation=(cv2.INTER_LINEAR, cv2.INTER_NEAREST), padding=None):\n        self.degree = degree\n        self.interpolation = interpolation\n        self.padding = padding\n\n    def __call__(self, img_group):\n        assert (len(self.interpolation) == len(img_group))\n        v = random.random()\n        if v < 0.5:\n            degree = random.uniform(self.degree[0], self.degree[1])\n            h, w = img_group[0].shape[0:2]\n            center = (w / 2, h / 2)\n            map_matrix = cv2.getRotationMatrix2D(center, degree, 1.0)\n            out_images = list()\n            for img, interpolation, padding in zip(img_group, self.interpolation, self.padding):\n                out_images.append(cv2.warpAffine(img, map_matrix, (w, h), flags=interpolation, borderMode=cv2.BORDER_CONSTANT, borderValue=padding))\n                if len(img.shape) > len(out_images[-1].shape):\n                    out_images[-1] = out_images[-1][..., np.newaxis]  # single channel image\n            return out_images\n        else:\n            return img_group\n\n\nclass GroupRandomBlur(object):\n    def __init__(self, applied):\n        self.applied = applied\n\n    def __call__(self, img_group):\n        assert (len(self.applied) == len(img_group))\n        v = random.random()\n        if v < 0.5:\n            out_images = []\n            for img, a in zip(img_group, self.applied):\n                if a:\n                    img = cv2.GaussianBlur(img, (5, 5), random.uniform(1e-6, 0.6))\n                out_images.append(img)\n                if len(img.shape) > len(out_images[-1].shape):\n                    out_images[-1] = out_images[-1][..., np.newaxis]  # single channel image\n            return out_images\n        else:\n            return img_group\n\n\nclass GroupRandomHorizontalFlip(object):\n    """"""Randomly horizontally flips the given numpy Image with a probability of 0.5\n    """"""\n\n    def __init__(self, is_flow=False):\n        self.is_flow = is_flow\n\n    def __call__(self, img_group, is_flow=False):\n        v = random.random()\n        if v < 0.5:\n            out_images = [np.fliplr(img) for img in img_group]\n            if self.is_flow:\n                for i in range(0, len(out_images), 2):\n                    out_images[i] = -out_images[i]  # invert flow pixel values when flipping\n            return out_images\n        else:\n            return img_group\n\n\nclass GroupNormalize(object):\n    def __init__(self, mean, std):\n        self.mean = mean\n        self.std = std\n\n    def __call__(self, img_group):\n        out_images = list()\n        for img, m, s in zip(img_group, self.mean, self.std):\n            if len(m) == 1:\n                img = img - np.array(m)  # single channel image\n                img = img / np.array(s)\n            else:\n                img = img - np.array(m)[np.newaxis, np.newaxis, ...]\n                img = img / np.array(s)[np.newaxis, np.newaxis, ...]\n            out_images.append(img)\n\n        # cv2.imshow(\'img\', (out_images[0] + np.array(self.mean[0])[np.newaxis, np.newaxis, ...]).astype(np.uint8))\n        # cv2.imshow(\'label\', (out_images[1] * 100).astype(np.uint8))\n        # print(np.unique(out_images[1]))\n        # cv2.waitKey()\n        return out_images\n\n\n# class ToTorchFormatTensor(object):\n#     """""" Converts a PIL.Image (RGB) or numpy.ndarray (H x W x C) in the range [0, 255]\n#     to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0] """"""\n\n#     def __init__(self, data_type):\n#         self.data_type = data_type\n\n#     def __call__(self, pic):\n#         if isinstance(pic, np.ndarray):\n#             # handle numpy array\n#             img = torch.from_numpy(pic).permute(2, 0, 1).contiguous()\n#         else:\n#             # handle PIL Image\n#             img = torch.ByteTensor(torch.ByteStorage.from_buffer(pic.tobytes()))\n#             img = img.view(pic.size[1], pic.size[0], len(pic.mode))\n#             # put it from HWC to CHW format\n#             # yikes, this transpose takes 80% of the loading time/CPU\n#             img = img.transpose(0, 1).transpose(0, 2).contiguous()\n#         return img.float()\n'"
SCNN-Tensorflow/lane-detection-model/config/global_config.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n# @Time    : 18-1-31 \xe4\xb8\x8a\xe5\x8d\x8811:21\n# @Author  : Luo Yao\n# @Site    : http://icode.baidu.com/repos/baidu/personal-code/Luoyao\n# @File    : global_config.py\n# @IDE: PyCharm Community Edition\n""""""\n\xe8\xae\xbe\xe7\xbd\xae\xe5\x85\xa8\xe5\xb1\x80\xe5\x8f\x98\xe9\x87\x8f\n""""""\nfrom easydict import EasyDict as edict\n\n__C = edict()\n# Consumers can get config by: from config import cfg\n\ncfg = __C\n\n# Train options\n__C.TRAIN = edict()\n\n# Set the shadownet training epochs\n__C.TRAIN.EPOCHS = 90100  # 200010\n# Set the display step\n__C.TRAIN.DISPLAY_STEP = 1\n# Set the test display step during training process\n__C.TRAIN.TEST_DISPLAY_STEP = 1000\n# Set the momentum parameter of the optimizer\n__C.TRAIN.MOMENTUM = 0.9\n# Set the initial learning rate\n__C.TRAIN.LEARNING_RATE = 0.01  # 0.0005\n# Set the GPU resource used during training process\n__C.TRAIN.GPU_MEMORY_FRACTION = 0.85\n# Set the GPU allow growth parameter during tensorflow training process\n__C.TRAIN.TF_ALLOW_GROWTH = True\n# Set the shadownet training batch size\n__C.TRAIN.BATCH_SIZE = 8  # 4\n# Set the shadownet validation batch size\n__C.TRAIN.VAL_BATCH_SIZE = 8  # 4\n# Set the learning rate decay steps\n__C.TRAIN.LR_DECAY_STEPS = 210000\n# Set the learning rate decay rate\n__C.TRAIN.LR_DECAY_RATE = 0.1\n# Set the class numbers\n__C.TRAIN.CLASSES_NUMS = 2\n# Set the image height\n__C.TRAIN.IMG_HEIGHT = 288  # 256\n# Set the image width\n__C.TRAIN.IMG_WIDTH = 800  # 512\n# Set GPU number\n__C.TRAIN.GPU_NUM = 4   # 8\n# Set CPU thread number\n__C.TRAIN.CPU_NUM = 4   #\n\n# Test options\n__C.TEST = edict()\n\n# Set the GPU resource used during testing process\n__C.TEST.GPU_MEMORY_FRACTION = 0.8\n# Set the GPU allow growth parameter during tensorflow testing process\n__C.TEST.TF_ALLOW_GROWTH = True\n# Set the test batch size\n__C.TEST.BATCH_SIZE = 8\n# Set the test CPU thread number\n__C.TEST.CPU_NUM = 8\n'"
SCNN-Tensorflow/lane-detection-model/data_provider/lanenet_data_processor.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n# @Time    : 18-5-11 \xe4\xb8\x8b\xe5\x8d\x884:58\n# @Author  : Luo Yao\n# @Site    : http://icode.baidu.com/repos/baidu/personal-code/Luoyao\n# @File    : lanenet_data_processor.py\n# @IDE: PyCharm Community Edition\n""""""\n\xe5\xae\x9e\xe7\x8e\xb0LaneNet\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe8\xa7\xa3\xe6\x9e\x90\xe7\xb1\xbb\n""""""\nimport tensorflow as tf\n\nfrom config import global_config\n\nCFG = global_config.cfg\nVGG_MEAN = [123.68, 116.779, 103.939]\n\n\nclass DataSet(object):\n    """"""\n    \xe5\xae\x9e\xe7\x8e\xb0\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe7\xb1\xbb\n    """"""\n\n    def __init__(self, dataset_info_file):\n        """"""\n        :param dataset_info_file:\n        """"""\n        self._len = 0\n        self.dataset_info_file = dataset_info_file\n        self._img, self._label_instance, self._label_existence = self._init_dataset()\n\n    def __len__(self):\n        return self._len\n\n    @staticmethod\n    def process_img(img_queue):\n        img_raw = tf.read_file(img_queue)\n        img_decoded = tf.image.decode_jpeg(img_raw, channels=3)\n        img_resized = tf.image.resize_images(img_decoded, [CFG.TRAIN.IMG_HEIGHT, CFG.TRAIN.IMG_WIDTH],\n                                             method=tf.image.ResizeMethod.BICUBIC)\n        img_casted = tf.cast(img_resized, tf.float32)\n        return tf.subtract(img_casted, VGG_MEAN)\n\n    @staticmethod\n    def process_label_instance(label_instance_queue):\n        label_instance_raw = tf.read_file(label_instance_queue)\n        label_instance_decoded = tf.image.decode_png(label_instance_raw, channels=1)\n        label_instance_resized = tf.image.resize_images(label_instance_decoded,\n                                                        [CFG.TRAIN.IMG_HEIGHT, CFG.TRAIN.IMG_WIDTH],\n                                                        method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n        label_instance_resized = tf.reshape(label_instance_resized, [CFG.TRAIN.IMG_HEIGHT, CFG.TRAIN.IMG_WIDTH])\n        return tf.cast(label_instance_resized, tf.int32)\n\n    @staticmethod\n    def process_label_existence(label_existence_queue):\n        return tf.cast(label_existence_queue, tf.float32)\n\n    def _init_dataset(self):\n        """"""\n        :return:\n        """"""\n        if not tf.gfile.Exists(self.dataset_info_file):\n            raise ValueError(\'Failed to find file: \' + self.dataset_info_file)\n\n        img_list = []\n        label_instance_list = []\n        label_existence_list = []\n\n        with open(self.dataset_info_file, \'r\') as file:\n            for _info in file:\n                info_tmp = _info.strip(\' \').split()\n\n                img_list.append(info_tmp[0][1:])\n                label_instance_list.append(info_tmp[1][1:])\n                label_existence_list.append([int(info_tmp[2]), int(info_tmp[3]), int(info_tmp[4]), int(info_tmp[5])])\n\n        self._len = len(img_list)\n        # img_queue = tf.train.string_input_producer(img_list)\n        # label_instance_queue = tf.train.string_input_producer(label_instance_list)\n        with tf.name_scope(\'data_augmentation\'):\n            image_tensor = tf.convert_to_tensor(img_list)\n            label_instance_tensor = tf.convert_to_tensor(label_instance_list)\n            label_existence_tensor = tf.convert_to_tensor(label_existence_list)\n            input_queue = tf.train.slice_input_producer([image_tensor, label_instance_tensor, label_existence_tensor])\n            img = self.process_img(input_queue[0])\n            label_instance = self.process_label_instance(input_queue[1])\n            label_existence = self.process_label_existence(input_queue[2])\n\n        return img, label_instance, label_existence\n\n    def next_batch(self, batch_size):\n        return tf.train.batch([self._img, self._label_instance, self._label_existence], batch_size=batch_size,\n                              num_threads=CFG.TRAIN.CPU_NUM)\n'"
SCNN-Tensorflow/lane-detection-model/data_provider/lanenet_data_processor_test.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n# @Time    : 18-5-11 \xe4\xb8\x8b\xe5\x8d\x884:58\n# @Author  : Luo Yao\n# @Site    : http://icode.baidu.com/repos/baidu/personal-code/Luoyao\n# @File    : lanenet_data_processor.py\n# @IDE: PyCharm Community Edition\n""""""\n\xe5\xae\x9e\xe7\x8e\xb0LaneNet\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe8\xa7\xa3\xe6\x9e\x90\xe7\xb1\xbb\n""""""\nimport tensorflow as tf\n\nfrom config import global_config\n\nCFG = global_config.cfg\nVGG_MEAN = [123.68, 116.779, 103.939]\n\n\nclass DataSet(object):\n    """"""\n    \xe5\xae\x9e\xe7\x8e\xb0\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe7\xb1\xbb\n    """"""\n\n    def __init__(self, dataset_info_file, batch_size):\n        """"""\n\n        :param dataset_info_file:\n        """"""\n        self._dataset_info_file = dataset_info_file\n        self._batch_size = batch_size\n        self._img_list = self._init_dataset()\n        self._next_batch_loop_count = 0\n\n    def __len__(self):\n        return self._len\n\n    def _init_dataset(self):\n        """"""\n        :return:\n        """"""\n        img_list = []\n\n        if not tf.gfile.Exists(self._dataset_info_file):\n            raise ValueError(\'Failed to find file: \' + self._dataset_info_file)\n\n        with open(self._dataset_info_file, \'r\') as file:\n            for _info in file:\n                info_tmp = _info.strip(\' \').split()\n                img_list.append(info_tmp[0][1:])\n\n        self._len = len(img_list)\n\n        return img_list\n\n    @staticmethod\n    def process_img(img_path):\n        img_raw = tf.read_file(img_path)\n        img_decoded = tf.image.decode_jpeg(img_raw, channels=3)\n        img_resized = tf.image.resize_images(img_decoded, [CFG.TRAIN.IMG_HEIGHT, CFG.TRAIN.IMG_WIDTH],\n                                             method=tf.image.ResizeMethod.BICUBIC)\n        img_casted = tf.cast(img_resized, tf.float32)\n        return tf.subtract(img_casted, VGG_MEAN)\n\n    def next_batch(self):\n        """"""\n        :return:\n        """"""\n\n        idx_start = self._batch_size * self._next_batch_loop_count\n        idx_end = self._batch_size * self._next_batch_loop_count + self._batch_size\n\n        if idx_end > len(self):\n            idx_end = len(self)\n\n        img_list = self._img_list[idx_start:idx_end]\n        self._next_batch_loop_count += 1\n        return img_list\n'"
SCNN-Tensorflow/lane-detection-model/encoder_decoder_model/cnn_basenet.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# @Time    : 17-9-18 \xe4\xb8\x8b\xe5\x8d\x883:59\n# @Author  : Luo Yao\n# @Site    : http://github.com/TJCVRS\n# @File    : cnn_basenet.py\n# @IDE: PyCharm Community Edition\n""""""\nThe base convolution neural networks mainly implement some useful cnn functions\n""""""\nimport tensorflow as tf\nimport tensorflow.contrib.layers as tf_layer\nfrom tensorflow.contrib.layers.python.layers import initializers\nimport numpy as np\n\nslim = tf.contrib.slim\n\nclass CNNBaseModel(object):\n    """"""\n    Base model for other specific cnn ctpn_models\n    """"""\n\n    def __init__(self):\n        pass\n\n    @staticmethod\n    def conv2d(inputdata, out_channel, kernel_size, padding=\'SAME\',\n               stride=1, w_init=None, b_init=None,\n               split=1, use_bias=True, data_format=\'NHWC\', name=None):\n        """"""\n        Packing the tensorflow conv2d function.\n        :param name: op name\n        :param inputdata: A 4D tensorflow tensor which ust have known number of channels, but can have other\n        unknown dimensions.\n        :param out_channel: number of output channel.\n        :param kernel_size: int so only support square kernel convolution\n        :param padding: \'VALID\' or \'SAME\'\n        :param stride: int so only support square stride\n        :param w_init: initializer for convolution weights\n        :param b_init: initializer for bias\n        :param split: split channels as used in Alexnet mainly group for GPU memory save.\n        :param use_bias:  whether to use bias.\n        :param data_format: default set to NHWC according tensorflow\n        :return: tf.Tensor named ``output``\n        """"""\n        with tf.variable_scope(name):\n            in_shape = inputdata.get_shape().as_list()\n            channel_axis = 3 if data_format == \'NHWC\' else 1\n            in_channel = in_shape[channel_axis]\n            assert in_channel is not None, ""[Conv2D] Input cannot have unknown channel!""\n            assert in_channel % split == 0\n            assert out_channel % split == 0\n\n            padding = padding.upper()\n\n            if isinstance(kernel_size, list):\n                filter_shape = [kernel_size[0], kernel_size[1]] + [in_channel / split, out_channel]\n            else:\n                filter_shape = [kernel_size, kernel_size] + [in_channel / split, out_channel]\n\n            if isinstance(stride, list):\n                strides = [1, stride[0], stride[1], 1] if data_format == \'NHWC\' \\\n                    else [1, 1, stride[0], stride[1]]\n            else:\n                strides = [1, stride, stride, 1] if data_format == \'NHWC\' \\\n                    else [1, 1, stride, stride]\n\n            if w_init is None:\n                w_init = tf.contrib.layers.variance_scaling_initializer()\n            if b_init is None:\n                b_init = tf.constant_initializer()\n\n            w = tf.get_variable(\'W\', filter_shape, initializer=w_init)\n            b = None\n\n            if use_bias:\n                b = tf.get_variable(\'b\', [out_channel], initializer=b_init)\n\n            if split == 1:\n                conv = tf.nn.conv2d(inputdata, w, strides, padding, data_format=data_format)\n            else:\n                inputs = tf.split(inputdata, split, channel_axis)\n                kernels = tf.split(w, split, 3)\n                outputs = [tf.nn.conv2d(i, k, strides, padding, data_format=data_format)\n                           for i, k in zip(inputs, kernels)]\n                conv = tf.concat(outputs, channel_axis)\n\n            ret = tf.identity(tf.nn.bias_add(conv, b, data_format=data_format)\n                              if use_bias else conv, name=name)\n\n        return ret\n\n    @staticmethod\n    def relu(inputdata, name=None):\n        """"""\n\n        :param name:\n        :param inputdata:\n        :return:\n        """"""\n        return tf.nn.relu(features=inputdata, name=name)\n\n    @staticmethod\n    def sigmoid(inputdata, name=None):\n        """"""\n\n        :param name:\n        :param inputdata:\n        :return:\n        """"""\n        return tf.nn.sigmoid(x=inputdata, name=name)\n\n    @staticmethod\n    def maxpooling(inputdata, kernel_size, stride=None, padding=\'VALID\',\n                   data_format=\'NHWC\', name=None):\n        """"""\n\n        :param name:\n        :param inputdata:\n        :param kernel_size:\n        :param stride:\n        :param padding:\n        :param data_format:\n        :return:\n        """"""\n        padding = padding.upper()\n\n        if stride is None:\n            stride = kernel_size\n\n        if isinstance(kernel_size, list):\n            kernel = [1, kernel_size[0], kernel_size[1], 1] if data_format == \'NHWC\' else \\\n                [1, 1, kernel_size[0], kernel_size[1]]\n        else:\n            kernel = [1, kernel_size, kernel_size, 1] if data_format == \'NHWC\' \\\n                else [1, 1, kernel_size, kernel_size]\n\n        if isinstance(stride, list):\n            strides = [1, stride[0], stride[1], 1] if data_format == \'NHWC\' \\\n                else [1, 1, stride[0], stride[1]]\n        else:\n            strides = [1, stride, stride, 1] if data_format == \'NHWC\' \\\n                else [1, 1, stride, stride]\n\n        return tf.nn.max_pool(value=inputdata, ksize=kernel, strides=strides, padding=padding,\n                              data_format=data_format, name=name)\n\n    @staticmethod\n    def avgpooling(inputdata, kernel_size, stride=None, padding=\'VALID\',\n                   data_format=\'NHWC\', name=None):\n        """"""\n\n        :param name:\n        :param inputdata:\n        :param kernel_size:\n        :param stride:\n        :param padding:\n        :param data_format:\n        :return:\n        """"""\n        if stride is None:\n            stride = kernel_size\n\n        kernel = [1, kernel_size, kernel_size, 1] if data_format == \'NHWC\' \\\n            else [1, 1, kernel_size, kernel_size]\n\n        strides = [1, stride, stride, 1] if data_format == \'NHWC\' else [1, 1, stride, stride]\n\n        return tf.nn.avg_pool(value=inputdata, ksize=kernel, strides=strides, padding=padding,\n                              data_format=data_format, name=name)\n\n    @staticmethod\n    def globalavgpooling(inputdata, data_format=\'NHWC\', name=None):\n        """"""\n\n        :param name:\n        :param inputdata:\n        :param data_format:\n        :return:\n        """"""\n        assert inputdata.shape.ndims == 4\n        assert data_format in [\'NHWC\', \'NCHW\']\n\n        axis = [1, 2] if data_format == \'NHWC\' else [2, 3]\n\n        return tf.reduce_mean(input_tensor=inputdata, axis=axis, name=name)\n\n    @staticmethod\n    def layernorm(inputdata, epsilon=1e-5, use_bias=True, use_scale=True,\n                  data_format=\'NHWC\', name=None):\n        """"""\n        :param name:\n        :param inputdata:\n        :param epsilon: epsilon to avoid divide-by-zero.\n        :param use_bias: whether to use the extra affine transformation or not.\n        :param use_scale: whether to use the extra affine transformation or not.\n        :param data_format:\n        :return:\n        """"""\n        shape = inputdata.get_shape().as_list()\n        ndims = len(shape)\n        assert ndims in [2, 4]\n\n        mean, var = tf.nn.moments(inputdata, list(range(1, len(shape))), keep_dims=True)\n\n        if data_format == \'NCHW\':\n            channnel = shape[1]\n            new_shape = [1, channnel, 1, 1]\n        else:\n            channnel = shape[-1]\n            new_shape = [1, 1, 1, channnel]\n        if ndims == 2:\n            new_shape = [1, channnel]\n\n        if use_bias:\n            beta = tf.get_variable(\'beta\', [channnel], initializer=tf.constant_initializer())\n            beta = tf.reshape(beta, new_shape)\n        else:\n            beta = tf.zeros([1] * ndims, name=\'beta\')\n        if use_scale:\n            gamma = tf.get_variable(\'gamma\', [channnel], initializer=tf.constant_initializer(1.0))\n            gamma = tf.reshape(gamma, new_shape)\n        else:\n            gamma = tf.ones([1] * ndims, name=\'gamma\')\n\n        return tf.nn.batch_normalization(inputdata, mean, var, beta, gamma, epsilon, name=name)\n\n    @staticmethod\n    def instancenorm(inputdata, epsilon=1e-5, data_format=\'NHWC\', use_affine=True, name=None):\n        """"""\n\n        :param name:\n        :param inputdata:\n        :param epsilon:\n        :param data_format:\n        :param use_affine:\n        :return:\n        """"""\n        shape = inputdata.get_shape().as_list()\n        if len(shape) != 4:\n            raise ValueError(""Input data of instancebn layer has to be 4D tensor"")\n\n        if data_format == \'NHWC\':\n            axis = [1, 2]\n            ch = shape[3]\n            new_shape = [1, 1, 1, ch]\n        else:\n            axis = [2, 3]\n            ch = shape[1]\n            new_shape = [1, ch, 1, 1]\n        if ch is None:\n            raise ValueError(""Input of instancebn require known channel!"")\n\n        mean, var = tf.nn.moments(inputdata, axis, keep_dims=True)\n\n        if not use_affine:\n            return tf.divide(inputdata - mean, tf.sqrt(var + epsilon), name=\'output\')\n\n        beta = tf.get_variable(\'beta\', [ch], initializer=tf.constant_initializer())\n        beta = tf.reshape(beta, new_shape)\n        gamma = tf.get_variable(\'gamma\', [ch], initializer=tf.constant_initializer(1.0))\n        gamma = tf.reshape(gamma, new_shape)\n        return tf.nn.batch_normalization(inputdata, mean, var, beta, gamma, epsilon, name=name)\n\n    @staticmethod\n    def dropout(inputdata, keep_prob, is_training=None, noise_shape=None, name=None):\n        """"""\n\n        :param name:\n        :param inputdata:\n        :param keep_prob:\n        :param noise_shape:\n        :return:\n        """"""\n        def f1():\n            """"""\n\n            :return:\n            """"""\n            # print(\'batch_normalization: train phase\')\n            return tf.nn.dropout(inputdata, keep_prob, noise_shape, name=name)\n\n        def f2():\n            """"""\n\n            :return:\n            """"""\n            # print(\'batch_normalization: test phase\')\n            return inputdata\n\n        output = tf.cond(is_training, f1, f2)\n        # output = tf.nn.dropout(inputdata, keep_prob, noise_shape, name=name)\n\n        return output\n\n        # return tf.nn.dropout(inputdata, keep_prob=keep_prob, noise_shape=noise_shape, name=name)\n\n    @staticmethod\n    def fullyconnect(inputdata, out_dim, w_init=None, b_init=None,\n                     use_bias=True, name=None):\n        """"""\n        Fully-Connected layer, takes a N>1D tensor and returns a 2D tensor.\n        It is an equivalent of `tf.layers.dense` except for naming conventions.\n\n        :param inputdata:  a tensor to be flattened except for the first dimension.\n        :param out_dim: output dimension\n        :param w_init: initializer for w. Defaults to `variance_scaling_initializer`.\n        :param b_init: initializer for b. Defaults to zero\n        :param use_bias: whether to use bias.\n        :param name:\n        :return: tf.Tensor: a NC tensor named ``output`` with attribute `variables`.\n        """"""\n        shape = inputdata.get_shape().as_list()[1:]\n        if None not in shape:\n            inputdata = tf.reshape(inputdata, [-1, int(np.prod(shape))])\n        else:\n            inputdata = tf.reshape(inputdata, tf.stack([tf.shape(inputdata)[0], -1]))\n\n        if w_init is None:\n            w_init = tf.contrib.layers.variance_scaling_initializer()\n        if b_init is None:\n            b_init = tf.constant_initializer()\n\n        ret = tf.layers.dense(inputs=inputdata, activation=lambda x: tf.identity(x, name=\'output\'),\n                              use_bias=use_bias, name=name,\n                              kernel_initializer=w_init, bias_initializer=b_init,\n                              trainable=True, units=out_dim)\n        return ret\n\n    @staticmethod\n    def layerbn(inputdata, is_training, name):\n        """"""\n\n        :param inputdata:\n        :param is_training:\n        :param name:\n        :return:\n        """"""\n        def f1():\n            """"""\n\n            :return:\n            """"""\n            # print(\'batch_normalization: train phase\')\n            return tf_layer.batch_norm(\n                             inputdata, is_training=True,\n                             center=True, scale=True, updates_collections=None,\n                             scope=name, reuse=False)\n\n        def f2():\n            """"""\n\n            :return:\n            """"""\n            # print(\'batch_normalization: test phase\')\n            return tf_layer.batch_norm(\n                             inputdata, is_training=False,\n                             center=True, scale=True, updates_collections=None,\n                             scope=name, reuse=True)\n\n        output = tf.cond(is_training, f1, f2)\n\n        return output\n\n    @staticmethod\n    def squeeze(inputdata, axis=None, name=None):\n        """"""\n\n        :param inputdata:\n        :param axis:\n        :param name:\n        :return:\n        """"""\n        return tf.squeeze(input=inputdata, axis=axis, name=name)\n\n    @staticmethod\n    def deconv2d(inputdata, out_channel, kernel_size, padding=\'SAME\',\n                 stride=1, w_init=None, b_init=None,\n                 use_bias=True, activation=None, data_format=\'channels_last\',\n                 trainable=True, name=None):\n        """"""\n        Packing the tensorflow conv2d function.\n        :param name: op name\n        :param inputdata: A 4D tensorflow tensor which ust have known number of channels, but can have other\n        unknown dimensions.\n        :param out_channel: number of output channel.\n        :param kernel_size: int so only support square kernel convolution\n        :param padding: \'VALID\' or \'SAME\'\n        :param stride: int so only support square stride\n        :param w_init: initializer for convolution weights\n        :param b_init: initializer for bias\n        :param activation: whether to apply a activation func to deconv result\n        :param use_bias:  whether to use bias.\n        :param data_format: default set to NHWC according tensorflow\n        :return: tf.Tensor named ``output``\n        """"""\n        with tf.variable_scope(name):\n            in_shape = inputdata.get_shape().as_list()\n            channel_axis = 3 if data_format == \'channels_last\' else 1\n            in_channel = in_shape[channel_axis]\n            assert in_channel is not None, ""[Deconv2D] Input cannot have unknown channel!""\n\n            padding = padding.upper()\n\n            if w_init is None:\n                w_init = tf.contrib.layers.variance_scaling_initializer()\n            if b_init is None:\n                b_init = tf.constant_initializer()\n\n            ret = tf.layers.conv2d_transpose(inputs=inputdata, filters=out_channel,\n                                             kernel_size=kernel_size,\n                                             strides=stride, padding=padding,\n                                             data_format=data_format,\n                                             activation=activation, use_bias=use_bias,\n                                             kernel_initializer=w_init,\n                                             bias_initializer=b_init, trainable=trainable,\n                                             name=name)\n        return ret\n\n    @staticmethod\n    def dilation_conv(input_tensor, k_size, out_dims, rate, padding=\'SAME\',\n                      w_init=None, b_init=None, use_bias=False, name=None):\n        """"""\n\n        :param input_tensor:\n        :param k_size:\n        :param out_dims:\n        :param rate:\n        :param padding:\n        :param w_init:\n        :param b_init:\n        :param use_bias:\n        :param name:\n        :return:\n        """"""\n        with tf.variable_scope(name):\n            in_shape = input_tensor.get_shape().as_list()\n            in_channel = in_shape[3]\n            assert in_channel is not None, ""[Conv2D] Input cannot have unknown channel!""\n\n            padding = padding.upper()\n\n            if isinstance(k_size, list):\n                filter_shape = [k_size[0], k_size[1]] + [in_channel, out_dims]\n            else:\n                filter_shape = [k_size, k_size] + [in_channel, out_dims]\n\n            if w_init is None:\n                w_init = tf.contrib.layers.variance_scaling_initializer()\n            if b_init is None:\n                b_init = tf.constant_initializer()\n\n            w = tf.get_variable(\'W\', filter_shape, initializer=w_init)\n            b = None\n\n            if use_bias:\n                b = tf.get_variable(\'b\', [out_dims], initializer=b_init)\n\n            conv = tf.nn.atrous_conv2d(value=input_tensor, filters=w, rate=rate,\n                                       padding=padding, name=\'dilation_conv\')\n\n            if use_bias:\n                ret = tf.add(conv, b)\n            else:\n                ret = conv\n\n        return ret\n'"
SCNN-Tensorflow/lane-detection-model/encoder_decoder_model/vgg_encoder.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n# @Time    : 18-1-29 \xe4\xb8\x8b\xe5\x8d\x882:04\n# @Author  : Luo Yao\n# @Site    : http://icode.baidu.com/repos/baidu/personal-code/Luoyao\n# @File    : dilation_encoder.py\n# @IDE: PyCharm Community Edition\n""""""\n\xe5\xae\x9e\xe7\x8e\xb0\xe4\xb8\x80\xe4\xb8\xaa\xe5\x9f\xba\xe4\xba\x8eVGG16\xe7\x9a\x84\xe7\x89\xb9\xe5\xbe\x81\xe7\xbc\x96\xe7\xa0\x81\xe7\xb1\xbb\n""""""\nfrom collections import OrderedDict\nimport math\n\nimport tensorflow as tf\n\nfrom encoder_decoder_model import cnn_basenet\nfrom config import global_config\n\nCFG = global_config.cfg\n\n\nclass VGG16Encoder(cnn_basenet.CNNBaseModel):\n    """"""\n    \xe5\xae\x9e\xe7\x8e\xb0\xe4\xba\x86\xe4\xb8\x80\xe4\xb8\xaa\xe5\x9f\xba\xe4\xba\x8evgg16\xe7\x9a\x84\xe7\x89\xb9\xe5\xbe\x81\xe7\xbc\x96\xe7\xa0\x81\xe7\xb1\xbb\n    """"""\n\n    def __init__(self, phase):\n        """"""\n\n        :param phase:\n        """"""\n        super(VGG16Encoder, self).__init__()\n        self._train_phase = tf.constant(\'train\', dtype=tf.string)\n        self._test_phase = tf.constant(\'test\', dtype=tf.string)\n        self._phase = phase\n        self._is_training = self._init_phase()\n\n    def _init_phase(self):\n        """"""\n\n        :return:\n        """"""\n        return tf.equal(self._phase, self._train_phase)\n\n    def _conv_stage(self, input_tensor, k_size, out_dims, name,\n                    stride=1, pad=\'SAME\'):\n        """"""\n        \xe5\xb0\x86\xe5\x8d\xb7\xe7\xa7\xaf\xe5\x92\x8c\xe6\xbf\x80\xe6\xb4\xbb\xe5\xb0\x81\xe8\xa3\x85\xe5\x9c\xa8\xe4\xb8\x80\xe8\xb5\xb7\n        :param input_tensor:\n        :param k_size:\n        :param out_dims:\n        :param name:\n        :param stride:\n        :param pad:\n        :return:\n        """"""\n        with tf.variable_scope(name):\n            conv = self.conv2d(inputdata=input_tensor, out_channel=out_dims,\n                               kernel_size=k_size, stride=stride,\n                               use_bias=False, padding=pad, name=\'conv\')\n\n            bn = self.layerbn(inputdata=conv, is_training=self._is_training, name=\'bn\')\n\n            relu = self.relu(inputdata=bn, name=\'relu\')\n\n        return relu\n\n    def _conv_dilated_stage(self, input_tensor, k_size, out_dims, name,\n                            dilation=1, pad=\'SAME\'):\n        """"""\n        \xe5\xb0\x86\xe5\x8d\xb7\xe7\xa7\xaf\xe5\x92\x8c\xe6\xbf\x80\xe6\xb4\xbb\xe5\xb0\x81\xe8\xa3\x85\xe5\x9c\xa8\xe4\xb8\x80\xe8\xb5\xb7\n        :param input_tensor:\n        :param k_size:\n        :param out_dims:\n        :param name:\n        :param stride:\n        :param pad:\n        :return:\n        """"""\n        with tf.variable_scope(name):\n            conv = self.dilation_conv(input_tensor=input_tensor, out_dims=out_dims,\n                                      k_size=k_size, rate=dilation,\n                                      use_bias=False, padding=pad, name=\'conv\')\n\n            bn = self.layerbn(inputdata=conv, is_training=self._is_training, name=\'bn\')\n\n            relu = self.relu(inputdata=bn, name=\'relu\')\n\n        return relu\n\n    def _fc_stage(self, input_tensor, out_dims, name, use_bias=False):\n        """"""\n\n        :param input_tensor:\n        :param out_dims:\n        :param name:\n        :param use_bias:\n        :return:\n        """"""\n        with tf.variable_scope(name):\n            fc = self.fullyconnect(inputdata=input_tensor, out_dim=out_dims, use_bias=use_bias,\n                                   name=\'fc\')\n\n            bn = self.layerbn(inputdata=fc, is_training=self._is_training, name=\'bn\')\n\n            relu = self.relu(inputdata=bn, name=\'relu\')\n\n        return relu\n\n    def encode(self, input_tensor, name):\n        """"""\n        \xe6\xa0\xb9\xe6\x8d\xaevgg16\xe6\xa1\x86\xe6\x9e\xb6\xe5\xaf\xb9\xe8\xbe\x93\xe5\x85\xa5\xe7\x9a\x84tensor\xe8\xbf\x9b\xe8\xa1\x8c\xe7\xbc\x96\xe7\xa0\x81\n        :param input_tensor:\n        :param name:\n        :return: \xe8\xbe\x93\xe5\x87\xbavgg16\xe7\xbc\x96\xe7\xa0\x81\xe7\x89\xb9\xe5\xbe\x81\n        """"""\n        ret = OrderedDict()\n        with tf.variable_scope(name):\n            # conv stage 1_1\n            conv_1_1 = self._conv_stage(input_tensor=input_tensor, k_size=3,\n                                        out_dims=64, name=\'conv1_1\')\n\n            # conv stage 1_2\n            conv_1_2 = self._conv_stage(input_tensor=conv_1_1, k_size=3,\n                                        out_dims=64, name=\'conv1_2\')\n\n            # pool stage 1\n            pool1 = self.maxpooling(inputdata=conv_1_2, kernel_size=2,\n                                    stride=2, name=\'pool1\')\n\n            # conv stage 2_1\n            conv_2_1 = self._conv_stage(input_tensor=pool1, k_size=3,\n                                        out_dims=128, name=\'conv2_1\')\n\n            # conv stage 2_2\n            conv_2_2 = self._conv_stage(input_tensor=conv_2_1, k_size=3,\n                                        out_dims=128, name=\'conv2_2\')\n\n            # pool stage 2\n            pool2 = self.maxpooling(inputdata=conv_2_2, kernel_size=2,\n                                    stride=2, name=\'pool2\')\n\n            # conv stage 3_1\n            conv_3_1 = self._conv_stage(input_tensor=pool2, k_size=3,\n                                        out_dims=256, name=\'conv3_1\')\n\n            # conv_stage 3_2\n            conv_3_2 = self._conv_stage(input_tensor=conv_3_1, k_size=3,\n                                        out_dims=256, name=\'conv3_2\')\n\n            # conv stage 3_3\n            conv_3_3 = self._conv_stage(input_tensor=conv_3_2, k_size=3,\n                                        out_dims=256, name=\'conv3_3\')\n\n            # pool stage 3\n            pool3 = self.maxpooling(inputdata=conv_3_3, kernel_size=2,\n                                    stride=2, name=\'pool3\')\n\n            # conv stage 4_1\n            conv_4_1 = self._conv_stage(input_tensor=pool3, k_size=3,\n                                        out_dims=512, name=\'conv4_1\')\n\n            # conv stage 4_2\n            conv_4_2 = self._conv_stage(input_tensor=conv_4_1, k_size=3,\n                                        out_dims=512, name=\'conv4_2\')\n\n            # conv stage 4_3\n            conv_4_3 = self._conv_stage(input_tensor=conv_4_2, k_size=3,\n                                        out_dims=512, name=\'conv4_3\')\n\n            ### add dilated convolution ###\n\n            # conv stage 5_1\n            conv_5_1 = self._conv_dilated_stage(input_tensor=conv_4_3, k_size=3,\n                                                out_dims=512, dilation=2, name=\'conv5_1\')\n\n            # conv stage 5_2\n            conv_5_2 = self._conv_dilated_stage(input_tensor=conv_5_1, k_size=3,\n                                                out_dims=512, dilation=2, name=\'conv5_2\')\n\n            # conv stage 5_3\n            conv_5_3 = self._conv_dilated_stage(input_tensor=conv_5_2, k_size=3,\n                                                out_dims=512, dilation=2, name=\'conv5_3\')\n\n            # added part of SCNN #\n\n            # conv stage 5_4\n            conv_5_4 = self._conv_dilated_stage(input_tensor=conv_5_3, k_size=3,\n                                                out_dims=1024, dilation=4, name=\'conv5_4\')\n\n            # conv stage 5_5\n            conv_5_5 = self._conv_stage(input_tensor=conv_5_4, k_size=1,\n                                        out_dims=128, name=\'conv5_5\')  # 8 x 36 x 100 x 128\n\n            # add message passing #\n\n            # top to down #\n\n            feature_list_old = []\n            feature_list_new = []\n            for cnt in range(conv_5_5.get_shape().as_list()[1]):\n                feature_list_old.append(tf.expand_dims(conv_5_5[:, cnt, :, :], axis=1))\n            feature_list_new.append(tf.expand_dims(conv_5_5[:, 0, :, :], axis=1))\n\n            w1 = tf.get_variable(\'W1\', [1, 9, 128, 128],\n                                 initializer=tf.random_normal_initializer(0, math.sqrt(2.0 / (9 * 128 * 128 * 5))))\n            with tf.variable_scope(""convs_6_1""):\n                conv_6_1 = tf.add(tf.nn.relu(tf.nn.conv2d(feature_list_old[0], w1, [1, 1, 1, 1], \'SAME\')),\n                                  feature_list_old[1])\n                feature_list_new.append(conv_6_1)\n\n            for cnt in range(2, conv_5_5.get_shape().as_list()[1]):\n                with tf.variable_scope(""convs_6_1"", reuse=True):\n                    conv_6_1 = tf.add(tf.nn.relu(tf.nn.conv2d(feature_list_new[cnt - 1], w1, [1, 1, 1, 1], \'SAME\')),\n                                      feature_list_old[cnt])\n                    feature_list_new.append(conv_6_1)\n\n            # down to top #\n            feature_list_old = feature_list_new\n            feature_list_new = []\n            length = int(CFG.TRAIN.IMG_HEIGHT / 8) - 1\n            feature_list_new.append(feature_list_old[length])\n\n            w2 = tf.get_variable(\'W2\', [1, 9, 128, 128],\n                                 initializer=tf.random_normal_initializer(0, math.sqrt(2.0 / (9 * 128 * 128 * 5))))\n            with tf.variable_scope(""convs_6_2""):\n                conv_6_2 = tf.add(tf.nn.relu(tf.nn.conv2d(feature_list_old[length], w2, [1, 1, 1, 1], \'SAME\')),\n                                  feature_list_old[length - 1])\n                feature_list_new.append(conv_6_2)\n\n            for cnt in range(2, conv_5_5.get_shape().as_list()[1]):\n                with tf.variable_scope(""convs_6_2"", reuse=True):\n                    conv_6_2 = tf.add(tf.nn.relu(tf.nn.conv2d(feature_list_new[cnt - 1], w2, [1, 1, 1, 1], \'SAME\')),\n                                      feature_list_old[length - cnt])\n                    feature_list_new.append(conv_6_2)\n\n            feature_list_new.reverse()\n\n            processed_feature = tf.stack(feature_list_new, axis=1)\n            processed_feature = tf.squeeze(processed_feature, axis=2)\n\n            # left to right #\n\n            feature_list_old = []\n            feature_list_new = []\n            for cnt in range(processed_feature.get_shape().as_list()[2]):\n                feature_list_old.append(tf.expand_dims(processed_feature[:, :, cnt, :], axis=2))\n            feature_list_new.append(tf.expand_dims(processed_feature[:, :, 0, :], axis=2))\n\n            w3 = tf.get_variable(\'W3\', [9, 1, 128, 128],\n                                 initializer=tf.random_normal_initializer(0, math.sqrt(2.0 / (9 * 128 * 128 * 5))))\n            with tf.variable_scope(""convs_6_3""):\n                conv_6_3 = tf.add(tf.nn.relu(tf.nn.conv2d(feature_list_old[0], w3, [1, 1, 1, 1], \'SAME\')),\n                                  feature_list_old[1])\n                feature_list_new.append(conv_6_3)\n\n            for cnt in range(2, processed_feature.get_shape().as_list()[2]):\n                with tf.variable_scope(""convs_6_3"", reuse=True):\n                    conv_6_3 = tf.add(tf.nn.relu(tf.nn.conv2d(feature_list_new[cnt - 1], w3, [1, 1, 1, 1], \'SAME\')),\n                                      feature_list_old[cnt])\n                    feature_list_new.append(conv_6_3)\n\n            # right to left #\n\n            feature_list_old = feature_list_new\n            feature_list_new = []\n            length = int(CFG.TRAIN.IMG_WIDTH / 8) - 1\n            feature_list_new.append(feature_list_old[length])\n\n            w4 = tf.get_variable(\'W4\', [9, 1, 128, 128],\n                                 initializer=tf.random_normal_initializer(0, math.sqrt(2.0 / (9 * 128 * 128 * 5))))\n            with tf.variable_scope(""convs_6_4""):\n                conv_6_4 = tf.add(tf.nn.relu(tf.nn.conv2d(feature_list_old[length], w4, [1, 1, 1, 1], \'SAME\')),\n                                  feature_list_old[length - 1])\n                feature_list_new.append(conv_6_4)\n\n            for cnt in range(2, processed_feature.get_shape().as_list()[2]):\n                with tf.variable_scope(""convs_6_4"", reuse=True):\n                    conv_6_4 = tf.add(tf.nn.relu(tf.nn.conv2d(feature_list_new[cnt - 1], w4, [1, 1, 1, 1], \'SAME\')),\n                                      feature_list_old[length - cnt])\n                    feature_list_new.append(conv_6_4)\n\n            feature_list_new.reverse()\n            processed_feature = tf.stack(feature_list_new, axis=2)\n            processed_feature = tf.squeeze(processed_feature, axis=3)\n\n            #######################\n\n            dropout_output = self.dropout(processed_feature, 0.9, is_training=self._is_training,\n                                          name=\'dropout\')  # 0.9 denotes the probability of being kept\n\n            conv_output = self.conv2d(inputdata=dropout_output, out_channel=5,\n                                      kernel_size=1, use_bias=True, name=\'conv_6\')\n\n            ret[\'prob_output\'] = tf.image.resize_images(conv_output, [CFG.TRAIN.IMG_HEIGHT, CFG.TRAIN.IMG_WIDTH])\n\n            ### add lane existence prediction branch ###\n\n            # spatial softmax #\n            features = conv_output  # N x H x W x C\n            softmax = tf.nn.softmax(features)\n\n            avg_pool = self.avgpooling(softmax, kernel_size=2, stride=2)\n            _, H, W, C = avg_pool.get_shape().as_list()\n            reshape_output = tf.reshape(avg_pool, [-1, H * W * C])\n            fc_output = self.fullyconnect(reshape_output, 128)\n            relu_output = self.relu(inputdata=fc_output, name=\'relu6\')\n            fc_output = self.fullyconnect(relu_output, 4)\n            existence_output = fc_output\n\n            ret[\'existence_output\'] = existence_output\n\n        return ret\n\n\nif __name__ == \'__main__\':\n    a = tf.placeholder(dtype=tf.float32, shape=[CFG.TRAIN.BATCH_SIZE, CFG.TRAIN.IMG_HEIGHT, CFG.TRAIN.IMG_WIDTH, 3],\n                       name=\'input\')\n    encoder = VGG16Encoder(phase=tf.constant(\'train\', dtype=tf.string))\n    ret = encoder.encode(a, name=\'encode\')\n    print(ret)\n'"
SCNN-Tensorflow/lane-detection-model/lanenet_model/lanenet_merge_model.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n# @Time    : 18-5-11 \xe4\xb8\x8b\xe5\x8d\x885:28\n# @Author  : Luo Yao\n# @Site    : http://icode.baidu.com/repos/baidu/personal-code/Luoyao\n# @File    : lanenet_merge_model.py\n# @IDE: PyCharm Community Edition\n""""""\nBuild Lane detection model\n""""""\nimport tensorflow as tf\n\nfrom encoder_decoder_model import vgg_encoder\nfrom encoder_decoder_model import cnn_basenet\nfrom config import global_config\n\nCFG = global_config.cfg\n\n\nclass LaneNet(cnn_basenet.CNNBaseModel):\n    """"""\n    Lane detection model\n    """"""\n\n    @staticmethod\n    def inference(input_tensor, phase, name):\n        """"""\n        feed forward\n        :param name:\n        :param input_tensor:\n        :param phase:\n        :return:\n        """"""\n        with tf.variable_scope(name):\n            with tf.variable_scope(\'inference\'):\n                encoder = vgg_encoder.VGG16Encoder(phase=phase)\n                encode_ret = encoder.encode(input_tensor=input_tensor, name=\'encode\')\n\n            return encode_ret\n\n    @staticmethod\n    def test_inference(input_tensor, phase, name):\n        inference_ret = LaneNet.inference(input_tensor, phase, name)\n        with tf.variable_scope(name):\n            # feed forward to obtain logits\n            # Compute loss\n\n            decode_logits = inference_ret[\'prob_output\']\n            binary_seg_ret = tf.nn.softmax(logits=decode_logits)\n            prob_list = []\n            kernel = tf.get_variable(\'kernel\', [9, 9, 1, 1], initializer=tf.constant_initializer(1.0 / 81),\n                                     trainable=False)\n\n            with tf.variable_scope(""convs_smooth""):\n                prob_smooth = tf.nn.conv2d(tf.cast(tf.expand_dims(binary_seg_ret[:, :, :, 0], axis=3), tf.float32),\n                                           kernel, [1, 1, 1, 1], \'SAME\')\n                prob_list.append(prob_smooth)\n\n            for cnt in range(1, binary_seg_ret.get_shape().as_list()[3]):\n                with tf.variable_scope(""convs_smooth"", reuse=True):\n                    prob_smooth = tf.nn.conv2d(\n                        tf.cast(tf.expand_dims(binary_seg_ret[:, :, :, cnt], axis=3), tf.float32), kernel, [1, 1, 1, 1],\n                        \'SAME\')\n                    prob_list.append(prob_smooth)\n            processed_prob = tf.stack(prob_list, axis=4)\n            processed_prob = tf.squeeze(processed_prob, axis=3)\n            binary_seg_ret = processed_prob\n\n            # Predict lane existence:\n            existence_logit = inference_ret[\'existence_output\']\n            existence_output = tf.nn.sigmoid(existence_logit)\n\n            return binary_seg_ret, existence_output\n\n    @staticmethod\n    def loss(inference, binary_label, existence_label, name):\n        """"""\n        :param name:\n        :param inference:\n        :param existence_label:\n        :param binary_label:\n        :return:\n        """"""\n        # feed forward to obtain logits\n\n        with tf.variable_scope(name):\n\n            inference_ret = inference\n\n            # Compute the segmentation loss\n\n            decode_logits = inference_ret[\'prob_output\']\n            decode_logits_reshape = tf.reshape(\n                decode_logits,\n                shape=[decode_logits.get_shape().as_list()[0],\n                       decode_logits.get_shape().as_list()[1] * decode_logits.get_shape().as_list()[2],\n                       decode_logits.get_shape().as_list()[3]])\n\n            binary_label_reshape = tf.reshape(\n                binary_label,\n                shape=[binary_label.get_shape().as_list()[0],\n                       binary_label.get_shape().as_list()[1] * binary_label.get_shape().as_list()[2]])\n            binary_label_reshape = tf.one_hot(binary_label_reshape, depth=5)\n            class_weights = tf.constant([[0.4, 1.0, 1.0, 1.0, 1.0]])\n            weights_loss = tf.reduce_sum(tf.multiply(binary_label_reshape, class_weights), 2)\n            binary_segmentation_loss = tf.losses.softmax_cross_entropy(onehot_labels=binary_label_reshape,\n                                                                       logits=decode_logits_reshape,\n                                                                       weights=weights_loss)\n            binary_segmentation_loss = tf.reduce_mean(binary_segmentation_loss)\n\n            # Compute the sigmoid loss\n\n            existence_logits = inference_ret[\'existence_output\']\n            existence_loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=existence_label, logits=existence_logits)\n            existence_loss = tf.reduce_mean(existence_loss)\n\n        # Compute the overall loss\n\n        total_loss = binary_segmentation_loss + 0.1 * existence_loss\n        ret = {\n            \'total_loss\': total_loss,\n            \'instance_seg_logits\': decode_logits,\n            \'instance_seg_loss\': binary_segmentation_loss,\n            \'existence_logits\': existence_logits,\n            \'existence_pre_loss\': existence_loss\n        }\n\n        tf.add_to_collection(\'total_loss\', total_loss)\n        tf.add_to_collection(\'instance_seg_logits\', decode_logits)\n        tf.add_to_collection(\'instance_seg_loss\', binary_segmentation_loss)\n        tf.add_to_collection(\'existence_logits\', existence_logits)\n        tf.add_to_collection(\'existence_pre_loss\', existence_loss)\n\n        return ret\n'"
SCNN-Tensorflow/lane-detection-model/tools/test_lanenet.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n# @Time    : 18-5-23 \xe4\xb8\x8a\xe5\x8d\x8811:33\n# @Author  : Luo Yao\n# @Site    : http://icode.baidu.com/repos/baidu/personal-code/Luoyao\n# @File    : test_lanenet.py\n# @IDE: PyCharm Community Edition\n""""""\n\xe6\xb5\x8b\xe8\xaf\x95LaneNet\xe6\xa8\xa1\xe5\x9e\x8b\n""""""\nimport os\nimport os.path as ops\nimport argparse\nimport math\nimport tensorflow as tf\nimport glog as log\nimport cv2\ntry:\n    from cv2 import cv2\nexcept ImportError:\n    pass\n\nfrom lanenet_model import lanenet_merge_model\nfrom config import global_config\nfrom data_provider import lanenet_data_processor_test\n\n\nCFG = global_config.cfg\nVGG_MEAN = [103.939, 116.779, 123.68]\n\n\ndef init_args():\n    """"""\n\n    :return:\n    """"""\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--image_path\', type=str, help=\'The image path or the src image save dir\')\n    parser.add_argument(\'--weights_path\', type=str, help=\'The model weights path\')\n    parser.add_argument(\'--is_batch\', type=str, help=\'If test a batch of images\', default=\'false\')\n    parser.add_argument(\'--batch_size\', type=int, help=\'The batch size of the test images\', default=8)\n    parser.add_argument(\'--save_dir\', type=str, help=\'Test result image save dir\', default=None)\n    parser.add_argument(\'--use_gpu\', type=int, help=\'If use gpu set 1 or 0 instead\', default=1)\n\n    return parser.parse_args()\n\n\n\ndef test_lanenet(image_path, weights_path, use_gpu, image_list, batch_size, save_dir):\n\n    """"""\n    :param image_path:\n    :param weights_path:\n    :param use_gpu:\n    :return:\n    """"""\n    \n    test_dataset = lanenet_data_processor_test.DataSet(image_path, batch_size)\n    input_tensor = tf.placeholder(dtype=tf.string, shape=[None], name=\'input_tensor\')\n    imgs = tf.map_fn(test_dataset.process_img, input_tensor, dtype=tf.float32)\n    phase_tensor = tf.constant(\'test\', tf.string)\n\n    net = lanenet_merge_model.LaneNet()\n    binary_seg_ret, instance_seg_ret = net.test_inference(imgs, phase_tensor, \'lanenet_loss\')\n    initial_var = tf.global_variables()\n    final_var = initial_var[:-1]\n    saver = tf.train.Saver(final_var)\n    # Set sess configuration\n    if use_gpu:\n        sess_config = tf.ConfigProto(device_count={\'GPU\': 1})\n    else:\n        sess_config = tf.ConfigProto(device_count={\'GPU\': 0})\n    sess_config.gpu_options.per_process_gpu_memory_fraction = CFG.TEST.GPU_MEMORY_FRACTION\n    sess_config.gpu_options.allow_growth = CFG.TRAIN.TF_ALLOW_GROWTH\n    sess_config.gpu_options.allocator_type = \'BFC\'\n    sess = tf.Session(config=sess_config)\n    with sess.as_default():\n        sess.run(tf.global_variables_initializer())\n        saver.restore(sess=sess, save_path=weights_path)\n        for i in range(math.ceil(len(image_list) / batch_size)):\n            print(i)\n            paths = test_dataset.next_batch()\n            instance_seg_image, existence_output = sess.run([binary_seg_ret, instance_seg_ret],\n                                                            feed_dict={input_tensor: paths})\n            for cnt, image_name in enumerate(paths):\n                print(image_name)\n                parent_path = os.path.dirname(image_name)\n                directory = os.path.join(save_dir, \'vgg_SCNN_DULR_w9\', parent_path)\n                if not os.path.exists(directory):\n                    os.makedirs(directory)\n                file_exist = open(os.path.join(directory, os.path.basename(image_name)[:-3] + \'exist.txt\'), \'w\')\n                for cnt_img in range(4):\n                    cv2.imwrite(os.path.join(directory, os.path.basename(image_name)[:-4] + \'_\' + str(cnt_img + 1) + \'_avg.png\'),\n                            (instance_seg_image[cnt, :, :, cnt_img + 1] * 255).astype(int))\n                    if existence_output[cnt, cnt_img] > 0.5:\n                        file_exist.write(\'1 \')\n                    else:\n                        file_exist.write(\'0 \')\n                file_exist.close()\n    sess.close()\n    return\n\n\nif __name__ == \'__main__\':\n    # init args\n    args = init_args()\n\n    if args.save_dir is not None and not ops.exists(args.save_dir):\n        log.error(\'{:s} not exist and has been made\'.format(args.save_dir))\n        os.makedirs(args.save_dir)\n\n    save_dir = os.path.join(args.image_path, \'predicts\')\n    if args.save_dir is not None:\n        save_dir = args.save_dir\n\n    img_name = []\n    with open(str(args.image_path), \'r\') as g:\n        for line in g.readlines():\n            img_name.append(line.strip())\n\n    test_lanenet(args.image_path, args.weights_path, args.use_gpu, img_name, args.batch_size, save_dir)'"
SCNN-Tensorflow/lane-detection-model/tools/train_lanenet.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n# @Time    : 18-5-18 \xe4\xb8\x8b\xe5\x8d\x887:31\n# @Author  : Luo Yao\n# @Site    : http://icode.baidu.com/repos/baidu/personal-code/Luoyao\n# @File    : train_lanenet.py\n# @IDE: PyCharm Community Edition\n""""""\n\xe8\xae\xad\xe7\xbb\x83lanenet\xe6\xa8\xa1\xe5\x9e\x8b\n""""""\nimport argparse\nimport os\nimport os.path as ops\nimport time\n\nimport glog as log\nimport numpy as np\nimport tensorflow as tf\n\nimport sys\n\nfrom config import global_config\nfrom lanenet_model import lanenet_merge_model\nfrom data_provider import lanenet_data_processor\n\nCFG = global_config.cfg\n\n\ndef init_args():\n    """"""\n\n    :return:\n    """"""\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\'--dataset_dir\', type=str, help=\'The training dataset dir path\')\n    parser.add_argument(\'--net\', type=str, help=\'Which base net work to use\', default=\'vgg\')\n    parser.add_argument(\'--weights_path\', type=str, help=\'The pretrained weights path\')\n\n    return parser.parse_args()\n\n\ndef average_gradients(tower_grads):\n    """"""Calculate the average gradient for each shared variable across all towers.\n    Note that this function provides a synchronization point across all towers.\n    Args:\n      tower_grads: List of lists of (gradient, variable) tuples. The outer list\n        is over individual gradients. The inner list is over the gradient\n        calculation for each tower.\n    Returns:\n       List of pairs of (gradient, variable) where the gradient has been averaged\n       across all towers.\n    """"""\n    average_grads = []\n    for grad_and_vars in zip(*tower_grads):\n        # Note that each grad_and_vars looks like the following:\n        #   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\n        grads = []\n        for g, _ in grad_and_vars:\n            # Add 0 dimension to the gradients to represent the tower.\n            expanded_g = tf.expand_dims(g, 0)\n\n            # Append on a \'tower\' dimension which we will average over below.\n            grads.append(expanded_g)\n\n        # Average over the \'tower\' dimension.\n        grad = tf.concat(grads, 0)\n        grad = tf.reduce_mean(grad, 0)\n\n        # Keep in mind that the Variables are redundant because they are shared\n        # across towers. So .. we will just return the first tower\'s pointer to\n        # the Variable.\n        v = grad_and_vars[0][1]\n        grad_and_var = (grad, v)\n        average_grads.append(grad_and_var)\n    return average_grads\n\n\ndef forward(batch_queue, net, phase, scope, optimizer=None):\n    img_batch, label_instance_batch, label_existence_batch = batch_queue.dequeue()\n    inference = net.inference(img_batch, phase, \'lanenet_loss\')\n    _ = net.loss(inference, label_instance_batch, label_existence_batch, \'lanenet_loss\')\n    total_loss = tf.add_n(tf.get_collection(\'total_loss\', scope))\n    instance_loss = tf.add_n(tf.get_collection(\'instance_seg_loss\', scope))\n    existence_loss = tf.add_n(tf.get_collection(\'existence_pre_loss\', scope))\n\n    out_logits = tf.add_n(tf.get_collection(\'instance_seg_logits\', scope))\n    # calculate the accuracy\n    out_logits = tf.nn.softmax(logits=out_logits)\n    out_logits_out = tf.argmax(out_logits, axis=-1)\n\n    pred_0 = tf.count_nonzero(tf.multiply(tf.cast(tf.equal(label_instance_batch, 0), tf.int32),\n                                          tf.cast(tf.equal(out_logits_out, 0), tf.int32)),\n                              dtype=tf.int32)\n    pred_1 = tf.count_nonzero(tf.multiply(tf.cast(tf.equal(label_instance_batch, 1), tf.int32),\n                                          tf.cast(tf.equal(out_logits_out, 1), tf.int32)),\n                              dtype=tf.int32)\n    pred_2 = tf.count_nonzero(tf.multiply(tf.cast(tf.equal(label_instance_batch, 2), tf.int32),\n                                          tf.cast(tf.equal(out_logits_out, 2), tf.int32)),\n                              dtype=tf.int32)\n    pred_3 = tf.count_nonzero(tf.multiply(tf.cast(tf.equal(label_instance_batch, 3), tf.int32),\n                                          tf.cast(tf.equal(out_logits_out, 3), tf.int32)),\n                              dtype=tf.int32)\n    pred_4 = tf.count_nonzero(tf.multiply(tf.cast(tf.equal(label_instance_batch, 4), tf.int32),\n                                          tf.cast(tf.equal(out_logits_out, 4), tf.int32)),\n                              dtype=tf.int32)\n    gt_all = tf.count_nonzero(tf.cast(tf.greater(label_instance_batch, 0), tf.int32), dtype=tf.int32)\n    gt_back = tf.count_nonzero(tf.cast(tf.equal(label_instance_batch, 0), tf.int32), dtype=tf.int32)\n\n    pred_all = tf.add(tf.add(tf.add(pred_1, pred_2), pred_3), pred_4)\n\n    accuracy = tf.divide(tf.cast(pred_all, tf.float32), tf.cast(gt_all, tf.float32))\n    accuracy_back = tf.divide(tf.cast(pred_0, tf.float32), tf.cast(gt_back, tf.float32))\n\n    # Compute mIoU of Lanes\n    overlap_1 = pred_1\n    union_1 = tf.add(tf.count_nonzero(tf.cast(tf.equal(label_instance_batch, 1),\n                                              tf.int32), dtype=tf.int32),\n                     tf.count_nonzero(tf.cast(tf.equal(out_logits_out, 1),\n                                              tf.int32), dtype=tf.int32))\n    union_1 = tf.subtract(union_1, overlap_1)\n    IoU_1 = tf.divide(tf.cast(overlap_1, tf.float32), tf.cast(union_1, tf.float32))\n\n    overlap_2 = pred_2\n    union_2 = tf.add(tf.count_nonzero(tf.cast(tf.equal(label_instance_batch, 2),\n                                              tf.int32), dtype=tf.int32),\n                     tf.count_nonzero(tf.cast(tf.equal(out_logits_out, 2),\n                                              tf.int32), dtype=tf.int32))\n    union_2 = tf.subtract(union_2, overlap_2)\n    IoU_2 = tf.divide(tf.cast(overlap_2, tf.float32), tf.cast(union_2, tf.float32))\n\n    overlap_3 = pred_3\n    union_3 = tf.add(tf.count_nonzero(tf.cast(tf.equal(label_instance_batch, 3),\n                                              tf.int32), dtype=tf.int32),\n                     tf.count_nonzero(tf.cast(tf.equal(out_logits_out, 3),\n                                              tf.int32), dtype=tf.int32))\n    union_3 = tf.subtract(union_3, overlap_3)\n    IoU_3 = tf.divide(tf.cast(overlap_3, tf.float32), tf.cast(union_3, tf.float32))\n\n    overlap_4 = pred_4\n    union_4 = tf.add(tf.count_nonzero(tf.cast(tf.equal(label_instance_batch, 4),\n                                              tf.int64), dtype=tf.int32),\n                     tf.count_nonzero(tf.cast(tf.equal(out_logits_out, 4),\n                                              tf.int64), dtype=tf.int32))\n    union_4 = tf.subtract(union_4, overlap_4)\n    IoU_4 = tf.divide(tf.cast(overlap_4, tf.float32), tf.cast(union_4, tf.float32))\n\n    IoU = tf.reduce_mean(tf.stack([IoU_1, IoU_2, IoU_3, IoU_4]))\n\n    tf.get_variable_scope().reuse_variables()\n\n    if optimizer is not None:\n        grads = optimizer.compute_gradients(total_loss)\n    else:\n        grads = None\n    return total_loss, instance_loss, existence_loss, accuracy, accuracy_back, IoU, out_logits_out, grads\n\n\ndef train_net(dataset_dir, weights_path=None, net_flag=\'vgg\'):\n    train_dataset_file = ops.join(dataset_dir, \'train_gt.txt\')\n    val_dataset_file = ops.join(dataset_dir, \'val_gt.txt\')\n\n    assert ops.exists(train_dataset_file)\n\n    phase = tf.placeholder(dtype=tf.string, shape=None, name=\'net_phase\')\n\n    train_dataset = lanenet_data_processor.DataSet(train_dataset_file)\n    val_dataset = lanenet_data_processor.DataSet(val_dataset_file)\n\n    net = lanenet_merge_model.LaneNet()\n\n    tower_grads = []\n\n    global_step = tf.Variable(0, trainable=False)\n\n    learning_rate = tf.train.polynomial_decay(CFG.TRAIN.LEARNING_RATE, global_step,\n                                              CFG.TRAIN.EPOCHS, power=0.9)\n\n    optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.9)\n    img, label_instance, label_existence = train_dataset.next_batch(CFG.TRAIN.BATCH_SIZE)\n    batch_queue = tf.contrib.slim.prefetch_queue.prefetch_queue(\n        [img, label_instance, label_existence], capacity=2 * CFG.TRAIN.GPU_NUM, num_threads=CFG.TRAIN.CPU_NUM)\n\n    val_img, val_label_instance, val_label_existence = val_dataset.next_batch(CFG.TRAIN.VAL_BATCH_SIZE)\n    val_batch_queue = tf.contrib.slim.prefetch_queue.prefetch_queue(\n        [val_img, val_label_instance, val_label_existence], capacity=2 * CFG.TRAIN.GPU_NUM,\n        num_threads=CFG.TRAIN.CPU_NUM)\n    with tf.variable_scope(tf.get_variable_scope()):\n        for i in range(CFG.TRAIN.GPU_NUM):\n            with tf.device(\'/gpu:%d\' % i):\n                with tf.name_scope(\'tower_%d\' % i) as scope:\n                    total_loss, instance_loss, existence_loss, accuracy, accuracy_back, _, out_logits_out, \\\n                        grad = forward(batch_queue, net, phase, scope, optimizer)\n                    tower_grads.append(grad)\n                with tf.name_scope(\'test_%d\' % i) as scope:\n                    val_op_total_loss, val_op_instance_loss, val_op_existence_loss, val_op_accuracy, \\\n                        val_op_accuracy_back, val_op_IoU, _, _ = forward(val_batch_queue, net, phase, scope)\n\n    grads = average_gradients(tower_grads)\n\n    train_op = optimizer.apply_gradients(grads, global_step=global_step)\n\n    train_cost_time_mean = []\n    train_instance_loss_mean = []\n    train_existence_loss_mean = []\n    train_accuracy_mean = []\n    train_accuracy_back_mean = []\n\n    saver = tf.train.Saver()\n    model_save_dir = \'model/culane_lanenet/culane_scnn\'\n    if not ops.exists(model_save_dir):\n        os.makedirs(model_save_dir)\n    train_start_time = time.strftime(\'%Y-%m-%d-%H-%M-%S\', time.localtime(time.time()))\n    model_name = \'culane_lanenet_{:s}_{:s}.ckpt\'.format(net_flag, str(train_start_time))\n    model_save_path = ops.join(model_save_dir, model_name)\n\n    sess_config = tf.ConfigProto(device_count={\'GPU\': CFG.TRAIN.GPU_NUM}, allow_soft_placement=True)\n    sess_config.gpu_options.per_process_gpu_memory_fraction = CFG.TRAIN.GPU_MEMORY_FRACTION\n    sess_config.gpu_options.allow_growth = CFG.TRAIN.TF_ALLOW_GROWTH\n    sess_config.gpu_options.allocator_type = \'BFC\'\n\n    with tf.Session(config=sess_config) as sess:\n        with sess.as_default():\n\n            if weights_path is None:\n                log.info(\'Training from scratch\')\n                init = tf.global_variables_initializer()\n                sess.run(init)\n            else:\n                log.info(\'Restore model from last model checkpoint {:s}\'.format(weights_path))\n                saver.restore(sess=sess, save_path=weights_path)\n\n            # \xe5\x8a\xa0\xe8\xbd\xbd\xe9\xa2\x84\xe8\xae\xad\xe7\xbb\x83\xe5\x8f\x82\xe6\x95\xb0\n            if net_flag == \'vgg\' and weights_path is None:\n                pretrained_weights = np.load(\n                    \'./data/vgg16.npy\',\n                    encoding=\'latin1\').item()\n\n                for vv in tf.trainable_variables():\n                    weights = vv.name.split(\'/\')\n                    if len(weights) >= 3 and weights[-3] in pretrained_weights:\n                        try:\n                            weights_key = weights[-3]\n                            weights = pretrained_weights[weights_key][0]\n                            _op = tf.assign(vv, weights)\n                            sess.run(_op)\n                        except Exception as e:\n                            continue\n        tf.train.start_queue_runners(sess=sess)\n        for epoch in range(CFG.TRAIN.EPOCHS):\n            t_start = time.time()\n\n            _, c, train_accuracy, train_accuracy_back, train_instance_loss, train_existence_loss, _ = \\\n                sess.run([train_op, total_loss, accuracy, accuracy_back, instance_loss, existence_loss, out_logits_out],\n                         feed_dict={phase: \'train\'})\n\n            cost_time = time.time() - t_start\n            train_cost_time_mean.append(cost_time)\n            train_instance_loss_mean.append(train_instance_loss)\n            train_existence_loss_mean.append(train_existence_loss)\n            train_accuracy_mean.append(train_accuracy)\n            train_accuracy_back_mean.append(train_accuracy_back)\n\n            if epoch % CFG.TRAIN.DISPLAY_STEP == 0:\n                print(\n                    \'Epoch: {:d} loss_ins= {:6f} ({:6f}) loss_ext= {:6f} ({:6f}) accuracy= {:6f} ({:6f}) \'\n                    \'accuracy_back= {:6f} ({:6f}) mean_time= {:5f}s \'.format(epoch + 1, train_instance_loss,\n                                                                             np.mean(train_instance_loss_mean),\n                                                                             train_existence_loss,\n                                                                             np.mean(train_existence_loss_mean),\n                                                                             train_accuracy,\n                                                                             np.mean(train_accuracy_mean),\n                                                                             train_accuracy_back,\n                                                                             np.mean(train_accuracy_back_mean),\n                                                                             np.mean(train_cost_time_mean)))\n\n            if epoch % 500 == 0:\n                train_cost_time_mean.clear()\n                train_instance_loss_mean.clear()\n                train_existence_loss_mean.clear()\n                train_accuracy_mean.clear()\n                train_accuracy_back_mean.clear()\n\n            if epoch % 1000 == 0:\n                saver.save(sess=sess, save_path=model_save_path, global_step=epoch)\n\n            if epoch % 10000 != 0 or epoch == 0:\n                continue\n\n            val_cost_time_mean = []\n            val_instance_loss_mean = []\n            val_existence_loss_mean = []\n            val_accuracy_mean = []\n            val_accuracy_back_mean = []\n            val_IoU_mean = []\n\n            for epoch_val in range(int(len(val_dataset) / CFG.TRAIN.VAL_BATCH_SIZE / CFG.TRAIN.GPU_NUM)):\n                t_start_val = time.time()\n                c_val, val_accuracy, val_accuracy_back, val_IoU, val_instance_loss, val_existence_loss = \\\n                    sess.run(\n                        [val_op_total_loss, val_op_accuracy, val_op_accuracy_back,\n                         val_op_IoU, val_op_instance_loss, val_op_existence_loss],\n                        feed_dict={phase: \'test\'})\n\n                cost_time_val = time.time() - t_start_val\n                val_cost_time_mean.append(cost_time_val)\n                val_instance_loss_mean.append(val_instance_loss)\n                val_existence_loss_mean.append(val_existence_loss)\n                val_accuracy_mean.append(val_accuracy)\n                val_accuracy_back_mean.append(val_accuracy_back)\n                val_IoU_mean.append(val_IoU)\n\n                if epoch_val % 1 == 0:\n                    print(\'Epoch_Val: {:d} loss_ins= {:6f} ({:6f}) \'\n                          \'loss_ext= {:6f} ({:6f}) accuracy= {:6f} ({:6f}) accuracy_back= {:6f} ({:6f}) \'\n                          \'mIoU= {:6f} ({:6f}) mean_time= {:5f}s \'.\n                          format(epoch_val + 1, val_instance_loss, np.mean(val_instance_loss_mean), val_existence_loss,\n                                 np.mean(val_existence_loss_mean), val_accuracy, np.mean(val_accuracy_mean),\n                                 val_accuracy_back, np.mean(val_accuracy_back_mean), val_IoU, np.mean(val_IoU_mean),\n                                 np.mean(val_cost_time_mean)))\n\n            val_cost_time_mean.clear()\n            val_instance_loss_mean.clear()\n            val_existence_loss_mean.clear()\n            val_accuracy_mean.clear()\n            val_accuracy_back_mean.clear()\n            val_IoU_mean.clear()\n\n\nif __name__ == \'__main__\':\n    # init args\n    args = init_args()\n\n    # train lanenet\n    train_net(args.dataset_dir, args.weights_path, net_flag=args.net)\n'"
