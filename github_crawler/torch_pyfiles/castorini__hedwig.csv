file_path,api_count,code
__init__.py,0,"b""from models import reg_lstm, kim_cnn, han, char_cnn, xml_cnn\n\n__all__ = ['reg_lstm', 'kim_cnn', 'char_cnn', 'xml_cnn', 'han']\n"""
setup.py,0,"b""from setuptools import setup\n\nsetup(name='hedwig',\n      version='1.0.0',\n      description='PyTorch deep learning models for document classification',\n      packages=['char_cnn', 'han', 'kim_cnn', 'reg_lstm', 'xml_cnn'],\n      )\n"""
common/__init__.py,0,b'from .evaluators import *\nfrom .trainers import *'
common/constants.py,0,"b""import os\n\n# Model categories\nBERT_MODELS = ['BERT-Base', 'BERT-Large', 'HBERT-Base', 'HBERT-Large']\n\n# String templates for logging results\nLOG_HEADER = 'Split  Dev/Acc.  Dev/Pr.  Dev/Re.   Dev/F1   Dev/Loss'\nLOG_TEMPLATE = ' '.join('{:>5s},{:>9.4f},{:>8.4f},{:8.4f},{:8.4f},{:10.4f}'.split(','))\n\n# Path to pretrained model and vocab files\nMODEL_DATA_DIR = os.path.join(os.pardir, 'hedwig-data', 'models')\nPRETRAINED_MODEL_ARCHIVE_MAP = {\n    'bert-base-uncased': os.path.join(MODEL_DATA_DIR, 'bert_pretrained', 'bert-base-uncased'),\n    'bert-large-uncased': os.path.join(MODEL_DATA_DIR, 'bert_pretrained', 'bert-large-uncased'),\n    'bert-base-cased': os.path.join(MODEL_DATA_DIR, 'bert_pretrained', 'bert-base-cased'),\n    'bert-large-cased': os.path.join(MODEL_DATA_DIR, 'bert_pretrained', 'bert-large-cased'),\n    'bert-base-multilingual-uncased': os.path.join(MODEL_DATA_DIR, 'bert_pretrained', 'bert-base-multilingual-uncased'),\n    'bert-base-multilingual-cased': os.path.join(MODEL_DATA_DIR, 'bert_pretrained', 'bert-base-multilingual-cased')\n}\nPRETRAINED_VOCAB_ARCHIVE_MAP = {\n    'bert-base-uncased': os.path.join(MODEL_DATA_DIR, 'bert_pretrained', 'bert-base-uncased-vocab.txt'),\n    'bert-large-uncased': os.path.join(MODEL_DATA_DIR, 'bert_pretrained', 'bert-large-uncased-vocab.txt'),\n    'bert-base-cased': os.path.join(MODEL_DATA_DIR, 'bert_pretrained', 'bert-base-cased-vocab.txt'),\n    'bert-large-cased': os.path.join(MODEL_DATA_DIR, 'bert_pretrained', 'bert-large-cased-vocab.txt'),\n    'bert-base-multilingual-uncased': os.path.join(MODEL_DATA_DIR, 'bert_pretrained', 'bert-base-multilingual-uncased-vocab.txt'),\n    'bert-base-multilingual-cased': os.path.join(MODEL_DATA_DIR, 'bert_pretrained', 'bert-base-multilingual-cased-vocab.txt')\n}\n"""
common/evaluate.py,0,"b'from common.evaluators.classification_evaluator import ClassificationEvaluator\nfrom common.evaluators.relevance_transfer_evaluator import RelevanceTransferEvaluator\n\n\nclass EvaluatorFactory(object):\n    """"""\n    Get the corresponding Evaluator class for a particular dataset.\n    """"""\n    evaluator_map = {\n        \'Reuters\': ClassificationEvaluator,\n        \'AAPD\': ClassificationEvaluator,\n        \'IMDB\': ClassificationEvaluator,\n        \'Yelp2014\': ClassificationEvaluator,\n        \'Robust04\': RelevanceTransferEvaluator,\n        \'Robust05\': RelevanceTransferEvaluator,\n        \'Robust45\': RelevanceTransferEvaluator\n    }\n\n    @staticmethod\n    def get_evaluator(dataset_cls, model, embedding, data_loader, batch_size, device, keep_results=False):\n        if data_loader is None:\n            return None\n\n        if not hasattr(dataset_cls, \'NAME\'):\n            raise ValueError(\'Invalid dataset. Dataset should have NAME attribute.\')\n\n        if dataset_cls.NAME not in EvaluatorFactory.evaluator_map:\n            raise ValueError(\'{} is not implemented.\'.format(dataset_cls))\n\n        return EvaluatorFactory.evaluator_map[dataset_cls.NAME](\n            dataset_cls, model, embedding, data_loader, batch_size, device, keep_results\n        )\n'"
common/train.py,0,"b'from common.trainers.classification_trainer import ClassificationTrainer\nfrom common.trainers.relevance_transfer_trainer import RelevanceTransferTrainer\n\n\nclass TrainerFactory(object):\n    """"""\n    Get the corresponding Trainer class for a particular dataset.\n    """"""\n    trainer_map = {\n        \'Reuters\': ClassificationTrainer,\n        \'AAPD\': ClassificationTrainer,\n        \'IMDB\': ClassificationTrainer,\n        \'Yelp2014\': ClassificationTrainer,\n        \'Robust04\': RelevanceTransferTrainer,\n        \'Robust05\': RelevanceTransferTrainer,\n        \'Robust45\': RelevanceTransferTrainer,\n    }\n\n    @staticmethod\n    def get_trainer(dataset_name, model, embedding, train_loader, trainer_config, train_evaluator, test_evaluator, dev_evaluator=None):\n\n        if dataset_name not in TrainerFactory.trainer_map:\n            raise ValueError(\'{} is not implemented.\'.format(dataset_name))\n\n        return TrainerFactory.trainer_map[dataset_name](\n            model, embedding, train_loader, trainer_config, train_evaluator, test_evaluator, dev_evaluator\n        )\n'"
datasets/__init__.py,0,b''
datasets/aapd.py,2,"b'import os\n\nimport numpy as np\nimport torch\nfrom torchtext.data import NestedField, Field, TabularDataset\nfrom torchtext.data.iterator import BucketIterator\nfrom torchtext.vocab import Vectors\n\nfrom datasets.reuters import clean_string, split_sents\n\n\ndef char_quantize(string, max_length=1000):\n    identity = np.identity(len(AAPDCharQuantized.ALPHABET))\n    quantized_string = np.array([identity[AAPDCharQuantized.ALPHABET[char]] for char in list(string.lower()) if char in AAPDCharQuantized.ALPHABET], dtype=np.float32)\n    if len(quantized_string) > max_length:\n        return quantized_string[:max_length]\n    else:\n        return np.concatenate((quantized_string, np.zeros((max_length - len(quantized_string), len(AAPDCharQuantized.ALPHABET)), dtype=np.float32)))\n\n\ndef process_labels(string):\n    """"""\n    Returns the label string as a list of integers\n    :param string:\n    :return:\n    """"""\n    return [float(x) for x in string]\n\n\nclass AAPD(TabularDataset):\n    NAME = \'AAPD\'\n    NUM_CLASSES = 54\n    IS_MULTILABEL = True\n\n    TEXT_FIELD = Field(batch_first=True, tokenize=clean_string, include_lengths=True)\n    LABEL_FIELD = Field(sequential=False, use_vocab=False, batch_first=True, preprocessing=process_labels)\n\n    @staticmethod\n    def sort_key(ex):\n        return len(ex.text)\n\n    @classmethod\n    def splits(cls, path, train=os.path.join(\'AAPD\', \'train.tsv\'),\n               validation=os.path.join(\'AAPD\', \'dev.tsv\'),\n               test=os.path.join(\'AAPD\', \'test.tsv\'), **kwargs):\n        return super(AAPD, cls).splits(\n            path, train=train, validation=validation, test=test,\n            format=\'tsv\', fields=[(\'label\', cls.LABEL_FIELD), (\'text\', cls.TEXT_FIELD)]\n        )\n\n    @classmethod\n    def iters(cls, path, vectors_name, vectors_cache, batch_size=64, shuffle=True, device=0, vectors=None,\n              unk_init=torch.Tensor.zero_):\n        """"""\n        :param path: directory containing train, test, dev files\n        :param vectors_name: name of word vectors file\n        :param vectors_cache: path to directory containing word vectors file\n        :param batch_size: batch size\n        :param device: GPU device\n        :param vectors: custom vectors - either predefined torchtext vectors or your own custom Vector classes\n        :param unk_init: function used to generate vector for OOV words\n        :return:\n        """"""\n        if vectors is None:\n            vectors = Vectors(name=vectors_name, cache=vectors_cache, unk_init=unk_init)\n\n        train, val, test = cls.splits(path)\n        cls.TEXT_FIELD.build_vocab(train, val, test, vectors=vectors)\n        return BucketIterator.splits((train, val, test), batch_size=batch_size, repeat=False, shuffle=shuffle,\n                                     sort_within_batch=True, device=device)\n\n\nclass AAPDCharQuantized(AAPD):\n    ALPHABET = dict(map(lambda t: (t[1], t[0]), enumerate(list(""""""abcdefghijklmnopqrstuvwxyz0123456789,;.!?:\'\\""/\\\\|_@#$%^&*~`+-=<>()[]{}""""""))))\n    TEXT_FIELD = Field(sequential=False, use_vocab=False, batch_first=True, preprocessing=char_quantize)\n\n    @classmethod\n    def iters(cls, path, vectors_name, vectors_cache, batch_size=64, shuffle=True, device=0, vectors=None,\n              unk_init=torch.Tensor.zero_):\n        """"""\n        :param path: directory containing train, test, dev files\n        :param batch_size: batch size\n        :param device: GPU device\n        :return:\n        """"""\n        train, val, test = cls.splits(path)\n        return BucketIterator.splits((train, val, test), batch_size=batch_size, repeat=False, shuffle=shuffle, device=device)\n\n\nclass AAPDHierarchical(AAPD):\n    NESTING_FIELD = Field(batch_first=True, tokenize=clean_string)\n    TEXT_FIELD = NestedField(NESTING_FIELD, tokenize=split_sents)\n'"
datasets/imdb.py,2,"b'import os\n\nimport numpy as np\nimport torch\nfrom torchtext.data import NestedField, Field, TabularDataset\nfrom torchtext.data.iterator import BucketIterator\nfrom torchtext.vocab import Vectors\n\nfrom datasets.reuters import clean_string, split_sents, process_labels, generate_ngrams\n\n\ndef char_quantize(string, max_length=500):\n    identity = np.identity(len(IMDBCharQuantized.ALPHABET))\n    quantized_string = np.array([identity[IMDBCharQuantized.ALPHABET[char]] for char in list(string.lower()) if char in IMDBCharQuantized.ALPHABET], dtype=np.float32)\n    if len(quantized_string) > max_length:\n        return quantized_string[:max_length]\n    else:\n        return np.concatenate((quantized_string, np.zeros((max_length - len(quantized_string), len(IMDBCharQuantized.ALPHABET)), dtype=np.float32)))\n\n\nclass IMDB(TabularDataset):\n    NAME = \'IMDB\'\n    NUM_CLASSES = 10\n    IS_MULTILABEL = False\n\n    TEXT_FIELD = Field(batch_first=True, tokenize=clean_string, include_lengths=True)\n    LABEL_FIELD = Field(sequential=False, use_vocab=False, batch_first=True, preprocessing=process_labels)\n\n    @staticmethod\n    def sort_key(ex):\n        return len(ex.text)\n\n    @classmethod\n    def splits(cls, path, train=os.path.join(\'IMDB\', \'train.tsv\'),\n               validation=os.path.join(\'IMDB\', \'dev.tsv\'),\n               test=os.path.join(\'IMDB\', \'test.tsv\'), **kwargs):\n        return super(IMDB, cls).splits(\n            path, train=train, validation=validation, test=test,\n            format=\'tsv\', fields=[(\'label\', cls.LABEL_FIELD), (\'text\', cls.TEXT_FIELD)]\n        )\n\n    @classmethod\n    def iters(cls, path, vectors_name, vectors_cache, batch_size=64, shuffle=True, device=0, vectors=None,\n              unk_init=torch.Tensor.zero_):\n        """"""\n        :param path: directory containing train, test, dev files\n        :param vectors_name: name of word vectors file\n        :param vectors_cache: path to directory containing word vectors file\n        :param batch_size: batch size\n        :param device: GPU device\n        :param vectors: custom vectors - either predefined torchtext vectors or your own custom Vector classes\n        :param unk_init: function used to generate vector for OOV words\n        :return:\n        """"""\n        if vectors is None:\n            vectors = Vectors(name=vectors_name, cache=vectors_cache, unk_init=unk_init)\n\n        train, val, test = cls.splits(path)\n        cls.TEXT_FIELD.build_vocab(train, val, test, vectors=vectors)\n        return BucketIterator.splits((train, val, test), batch_size=batch_size, repeat=False, shuffle=shuffle,\n                                     sort_within_batch=True, device=device)\n\n\nclass IMDBHierarchical(IMDB):\n    NESTING_FIELD = Field(batch_first=True, tokenize=clean_string)\n    TEXT_FIELD = NestedField(NESTING_FIELD, tokenize=split_sents)\n\n\nclass IMDBCharQuantized(IMDB):\n    ALPHABET = dict(map(lambda t: (t[1], t[0]), enumerate(list(""""""abcdefghijklmnopqrstuvwxyz0123456789,;.!?:\'\\""/\\\\|_@#$%^&*~`+-=<>()[]{}""""""))))\n    TEXT_FIELD = Field(sequential=False, use_vocab=False, batch_first=True, preprocessing=char_quantize)\n\n    @classmethod\n    def iters(cls, path, vectors_name, vectors_cache, batch_size=64, shuffle=True, device=0, vectors=None,\n              unk_init=torch.Tensor.zero_):\n        """"""\n        :param path: directory containing train, test, dev files\n        :param batch_size: batch size\n        :param device: GPU device\n        :return:\n        """"""\n        train, val, test = cls.splits(path)\n        return BucketIterator.splits((train, val, test), batch_size=batch_size, repeat=False, shuffle=shuffle, device=device)\n'"
datasets/reuters.py,2,"b'import json\nimport os\nimport re\nimport sys\nimport csv\n\nimport numpy as np\nimport torch\nfrom torchtext.data import NestedField, Field, TabularDataset\nfrom torchtext.data.iterator import BucketIterator\nfrom torchtext.vocab import Vectors\n\ncsv.field_size_limit(sys.maxsize)\n\n\ndef clean_string(string):\n    """"""\n    Performs tokenization and string cleaning for the Reuters dataset\n    """"""\n    string = re.sub(r""[^A-Za-z0-9(),!?\\\'`]"", "" "", string)\n    string = re.sub(r""\\s{2,}"", "" "", string)\n    return string.lower().strip().split()\n\n\ndef split_sents(string):\n    string = re.sub(r""[!?]"","" "", string)\n    return string.strip().split(\'.\')\n\n\ndef generate_ngrams(tokens, n=2):\n    n_grams = zip(*[tokens[i:] for i in range(n)])\n    tokens.extend([\'-\'.join(x) for x in n_grams])\n    return tokens\n\n\ndef load_json(string):\n    split_val = json.loads(string)\n    return np.asarray(split_val, dtype=np.float32)\n\n\ndef char_quantize(string, max_length=1000):\n    identity = np.identity(len(ReutersCharQuantized.ALPHABET))\n    quantized_string = np.array([identity[ReutersCharQuantized.ALPHABET[char]] for char in list(string.lower()) if char in ReutersCharQuantized.ALPHABET], dtype=np.float32)\n    if len(quantized_string) > max_length:\n        return quantized_string[:max_length]\n    else:\n        return np.concatenate((quantized_string, np.zeros((max_length - len(quantized_string), len(ReutersCharQuantized.ALPHABET)), dtype=np.float32)))\n\n\ndef process_labels(string):\n    """"""\n    Returns the label string as a list of integers\n    """"""\n    return [float(x) for x in string]\n\n\nclass Reuters(TabularDataset):\n    NAME = \'Reuters\'\n    NUM_CLASSES = 90\n    IS_MULTILABEL = True\n\n    TEXT_FIELD = Field(batch_first=True, tokenize=clean_string, include_lengths=True)\n    LABEL_FIELD = Field(sequential=False, use_vocab=False, batch_first=True, preprocessing=process_labels)\n\n    @staticmethod\n    def sort_key(ex):\n        return len(ex.text)\n\n    @classmethod\n    def splits(cls, path, train=os.path.join(\'Reuters\', \'train.tsv\'),\n               validation=os.path.join(\'Reuters\', \'dev.tsv\'),\n               test=os.path.join(\'Reuters\', \'test.tsv\'), **kwargs):\n        return super(Reuters, cls).splits(\n            path, train=train, validation=validation, test=test,\n            format=\'tsv\', fields=[(\'label\', cls.LABEL_FIELD), (\'text\', cls.TEXT_FIELD)]\n        )\n\n    @classmethod\n    def iters(cls, path, vectors_name, vectors_cache, batch_size=64, shuffle=True, device=0, vectors=None,\n              unk_init=torch.Tensor.zero_):\n        """"""\n        :param path: directory containing train, test, dev files\n        :param vectors_name: name of word vectors file\n        :param vectors_cache: path to directory containing word vectors file\n        :param batch_size: batch size\n        :param device: GPU device\n        :param vectors: custom vectors - either predefined torchtext vectors or your own custom Vector classes\n        :param unk_init: function used to generate vector for OOV words\n        :return:\n        """"""\n        if vectors is None:\n            vectors = Vectors(name=vectors_name, cache=vectors_cache, unk_init=unk_init)\n\n        train, val, test = cls.splits(path)\n        cls.TEXT_FIELD.build_vocab(train, val, test, vectors=vectors)\n        return BucketIterator.splits((train, val, test), batch_size=batch_size, repeat=False, shuffle=shuffle,\n                                     sort_within_batch=True, device=device)\n\n\nclass ReutersBOW(Reuters):\n    TEXT_FIELD = Field(batch_first=True, tokenize=clean_string, preprocessing=generate_ngrams, include_lengths=True)\n\n\nclass ReutersHierarchical(Reuters):\n    NESTING_FIELD = Field(batch_first=True, tokenize=clean_string)\n    TEXT_FIELD = NestedField(NESTING_FIELD, tokenize=split_sents)\n\n\nclass ReutersCharQuantized(Reuters):\n    ALPHABET = dict(map(lambda t: (t[1], t[0]), enumerate(list(""""""abcdefghijklmnopqrstuvwxyz0123456789,;.!?:\'\\""/\\\\|_@#$%^&*~`+-=<>()[]{}""""""))))\n    TEXT_FIELD = Field(sequential=False, use_vocab=False, batch_first=True, preprocessing=char_quantize)\n\n    @classmethod\n    def iters(cls, path, vectors_name, vectors_cache, batch_size=64, shuffle=True, device=0, vectors=None,\n              unk_init=torch.Tensor.zero_):\n        """"""\n        :param path: directory containing train, test, dev files\n        :param batch_size: batch size\n        :param device: GPU device\n        :return:\n        """"""\n        train, val, test = cls.splits(path)\n        return BucketIterator.splits((train, val, test), batch_size=batch_size, repeat=False, shuffle=shuffle, device=device)\n'"
datasets/robust04.py,1,"b'import csv\nimport os\nimport sys\n\nimport torch\nfrom torchtext.data import NestedField, Field, TabularDataset\nfrom torchtext.data.iterator import BucketIterator\nfrom torchtext.vocab import Vectors\n\nfrom datasets.robust45 import clean_string, split_sents, process_docids, process_labels\n\ncsv.field_size_limit(sys.maxsize)\n\n\nclass Robust04(TabularDataset):\n    NAME = \'Robust04\'\n    NUM_CLASSES = 2\n    TEXT_FIELD = Field(batch_first=True, tokenize=clean_string, include_lengths=True)\n    LABEL_FIELD = Field(sequential=False, use_vocab=False, batch_first=True, preprocessing=process_labels)\n    DOCID_FIELD = Field(sequential=False, use_vocab=False, batch_first=True, preprocessing=process_docids)\n    TOPICS = [\'307\', \'310\', \'321\', \'325\', \'330\', \'336\', \'341\', \'344\', \'345\', \'347\', \'350\', \'353\', \'354\', \'355\', \'356\',\n              \'362\', \'363\', \'367\', \'372\', \'375\', \'378\', \'379\', \'389\', \'393\', \'394\', \'397\', \'399\', \'400\', \'404\', \'408\',\n              \'414\', \'416\', \'419\', \'422\', \'423\', \'426\', \'427\', \'433\', \'435\', \'436\', \'439\', \'442\', \'443\', \'445\', \'614\',\n              \'620\', \'626\', \'646\', \'677\', \'690\']\n\n    @staticmethod\n    def sort_key(ex):\n        return len(ex.text)\n\n    @classmethod\n    def splits(cls, path, train, validation, test, **kwargs):\n        return super(Robust04, cls).splits(\n            path, train=train, validation=validation, test=test,\n            format=\'tsv\', fields=[(\'label\', cls.LABEL_FIELD), (\'docid\', cls.DOCID_FIELD), (\'text\', cls.TEXT_FIELD)]\n        )\n\n    @classmethod\n    def iters(cls, path, vectors_name, vectors_cache, topic, batch_size=64, shuffle=True, device=0,\n              vectors=None, unk_init=torch.Tensor.zero_):\n        """"""\n        :param path: directory containing train, test, dev files\n        :param vectors_name: name of word vectors file\n        :param vectors_cache: path to directory containing word vectors file\n        :param topic: topic from which articles should be fetched\n        :param batch_size: batch size\n        :param device: GPU device\n        :param vectors: custom vectors - either predefined torchtext vectors or your own custom Vector classes\n        :param unk_init: function used to generate vector for OOV words\n        :return:\n        """"""\n        if vectors is None:\n            vectors = Vectors(name=vectors_name, cache=vectors_cache, unk_init=unk_init)\n\n        train_path = os.path.join(\'TREC\', \'robust04_train_%s.tsv\' % topic)\n        dev_path = os.path.join(\'TREC\', \'robust04_dev_%s.tsv\' % topic)\n        test_path = os.path.join(\'TREC\', \'core17_10k_%s.tsv\' % topic)\n        train, val, test = cls.splits(path, train=train_path, validation=dev_path, test=test_path)\n        cls.TEXT_FIELD.build_vocab(train, val, test, vectors=vectors)\n        return BucketIterator.splits((train, val, test), batch_size=batch_size, repeat=False, shuffle=shuffle,\n                                     sort_within_batch=True, device=device)\n\n\nclass Robust04Hierarchical(Robust04):\n    NESTING_FIELD = Field(batch_first=True, tokenize=clean_string)\n    TEXT_FIELD = NestedField(NESTING_FIELD, tokenize=split_sents)'"
datasets/robust05.py,1,"b'import csv\nimport os\nimport sys\n\nimport torch\nfrom torchtext.data import NestedField, Field, TabularDataset\nfrom torchtext.data.iterator import BucketIterator\nfrom torchtext.vocab import Vectors\n\nfrom datasets.robust45 import clean_string, split_sents, process_docids, process_labels\n\ncsv.field_size_limit(sys.maxsize)\n\n\nclass Robust05(TabularDataset):\n    NAME = \'Robust05\'\n    NUM_CLASSES = 2\n    TEXT_FIELD = Field(batch_first=True, tokenize=clean_string, include_lengths=True)\n    LABEL_FIELD = Field(sequential=False, use_vocab=False, batch_first=True, preprocessing=process_labels)\n    DOCID_FIELD = Field(sequential=False, use_vocab=False, batch_first=True, preprocessing=process_docids)\n    TOPICS = [\'307\', \'310\', \'325\', \'330\', \'336\', \'341\', \'344\', \'345\', \'347\', \'353\', \'354\', \'362\', \'363\', \'367\', \'372\',\n              \'375\', \'378\', \'389\', \'393\', \'394\', \'397\', \'399\', \'404\', \'408\', \'416\', \'419\', \'426\', \'427\', \'433\', \'435\',\n              \'436\', \'439\', \'443\']\n\n    @staticmethod\n    def sort_key(ex):\n        return len(ex.text)\n\n    @classmethod\n    def splits(cls, path, train, validation, test, **kwargs):\n        return super(Robust05, cls).splits(\n            path, train=train, validation=validation, test=test,\n            format=\'tsv\', fields=[(\'label\', cls.LABEL_FIELD), (\'docid\', cls.DOCID_FIELD), (\'text\', cls.TEXT_FIELD)]\n        )\n\n    @classmethod\n    def iters(cls, path, vectors_name, vectors_cache, topic, batch_size=64, shuffle=True, device=0,\n              vectors=None, unk_init=torch.Tensor.zero_):\n        """"""\n        :param path: directory containing train, test, dev files\n        :param vectors_name: name of word vectors file\n        :param vectors_cache: path to directory containing word vectors file\n        :param topic: topic from which articles should be fetched\n        :param batch_size: batch size\n        :param device: GPU device\n        :param vectors: custom vectors - either predefined torchtext vectors or your own custom Vector classes\n        :param unk_init: function used to generate vector for OOV words\n        :return:\n        """"""\n        if vectors is None:\n            vectors = Vectors(name=vectors_name, cache=vectors_cache, unk_init=unk_init)\n\n        train_path = os.path.join(\'TREC\', \'robust05_train_%s.tsv\' % topic)\n        dev_path = os.path.join(\'TREC\', \'robust05_dev_%s.tsv\' % topic)\n        test_path = os.path.join(\'TREC\', \'core17_%s.tsv\' % topic)\n        train, val, test = cls.splits(path, train=train_path, validation=dev_path, test=test_path)\n        cls.TEXT_FIELD.build_vocab(train, val, test, vectors=vectors)\n        return BucketIterator.splits((train, val, test), batch_size=batch_size, repeat=False, shuffle=shuffle,\n                                     sort_within_batch=True, device=device)\n\n\nclass Robust05Hierarchical(Robust05):\n    NESTING_FIELD = Field(batch_first=True, tokenize=clean_string)\n    TEXT_FIELD = NestedField(NESTING_FIELD, tokenize=split_sents)'"
datasets/robust45.py,1,"b'import csv\nimport os\nimport random\nimport re\nimport sys\n\nimport torch\nfrom nltk import tokenize\nfrom torchtext.data import NestedField, Field, TabularDataset\nfrom torchtext.data.iterator import BucketIterator\nfrom torchtext.vocab import Vectors\n\ncsv.field_size_limit(sys.maxsize)\n\n\ndef clean_string(string, sentence_droprate=0, max_length=5000):\n    """"""\n    Performs tokenization and string cleaning\n    """"""\n    if sentence_droprate > 0:\n        lines = [x for x in tokenize.sent_tokenize(string) if len(x) > 1]\n        lines_drop = [x for x in lines if random.randint(0, 100) > 100 * sentence_droprate]\n        string = \' \'.join(lines_drop if len(lines_drop) > 0 else lines)\n\n    string = re.sub(r\'[^A-Za-z0-9]\', \' \', string)\n    string = re.sub(r\'\\s{2,}\', \' \', string)\n    tokenized_string = string.lower().strip().split()\n    return tokenized_string[:min(max_length, len(tokenized_string))]\n\n\ndef split_sents(string, max_length=50):\n    tokenized_string = [x for x in tokenize.sent_tokenize(string) if len(x) > 1]\n    return tokenized_string[:min(max_length, len(tokenized_string))]\n\n\ndef process_labels(string):\n    """"""\n    Returns the label string as a list of integers\n    :param string:\n    :return:\n    """"""\n    return 0 if string == \'01\' else 1\n\n\ndef process_docids(string):\n    """"""\n    Returns the docid as an integer\n    :param string:\n    :return:\n    """"""\n    try:\n        docid = int(string)\n    except ValueError:\n        # print(""Error converting docid to integer:"", string)\n        docid = 0\n    return docid\n\n\nclass Robust45(TabularDataset):\n    NAME = \'Robust45\'\n    NUM_CLASSES = 2\n    TEXT_FIELD = Field(batch_first=True, tokenize=clean_string, include_lengths=True)\n    LABEL_FIELD = Field(sequential=False, use_vocab=False, batch_first=True, preprocessing=process_labels)\n    DOCID_FIELD = Field(sequential=False, use_vocab=False, batch_first=True, preprocessing=process_docids)\n    TOPICS = [\'307\', \'310\', \'321\', \'325\', \'330\', \'336\', \'341\', \'344\', \'345\', \'347\', \'350\', \'353\', \'354\', \'355\', \'356\',\n              \'362\', \'363\', \'367\', \'372\', \'375\', \'378\', \'379\', \'389\', \'393\', \'394\', \'397\', \'399\', \'400\', \'404\', \'408\',\n              \'414\', \'416\', \'419\', \'422\', \'423\', \'426\', \'427\', \'433\', \'435\', \'436\', \'439\', \'442\', \'443\', \'445\', \'614\',\n              \'620\', \'626\', \'646\', \'677\', \'690\']\n\n    @staticmethod\n    def sort_key(ex):\n        return len(ex.text)\n\n    @classmethod\n    def splits(cls, path, train, validation, test, **kwargs):\n        return super(Robust45, cls).splits(\n            path, train=train, validation=validation, test=test,\n            format=\'tsv\', fields=[(\'label\', cls.LABEL_FIELD), (\'docid\', cls.DOCID_FIELD), (\'text\', cls.TEXT_FIELD)]\n        )\n\n    @classmethod\n    def iters(cls, path, vectors_name, vectors_cache, topic, batch_size=64, shuffle=True, device=0,\n              vectors=None, unk_init=torch.Tensor.zero_):\n        """"""\n        :param path: directory containing train, test, dev files\n        :param vectors_name: name of word vectors file\n        :param vectors_cache: path to directory containing word vectors file\n        :param topic: topic from which articles should be fetched\n        :param batch_size: batch size\n        :param device: GPU device\n        :param vectors: custom vectors - either predefined torchtext vectors or your own custom Vector classes\n        :param unk_init: function used to generate vector for OOV words\n        :return:\n        """"""\n        if vectors is None:\n            vectors = Vectors(name=vectors_name, cache=vectors_cache, unk_init=unk_init)\n\n        train_path = os.path.join(\'TREC\', \'robust45_aug_train_%s.tsv\' % topic)\n        dev_path = os.path.join(\'TREC\', \'robust45_dev_%s.tsv\' % topic)\n        test_path = os.path.join(\'TREC\', \'core17_10k_%s.tsv\' % topic)\n        train, val, test = cls.splits(path, train=train_path, validation=dev_path, test=test_path)\n        cls.TEXT_FIELD.build_vocab(train, val, test, vectors=vectors)\n        return BucketIterator.splits((train, val, test), batch_size=batch_size, repeat=False, shuffle=shuffle,\n                                     sort_within_batch=True, device=device)\n\n\nclass Robust45Hierarchical(Robust45):\n    @staticmethod\n    def clean_sentence(string):\n        return clean_string(string, sentence_droprate=0, max_length=100)\n\n    NESTING_FIELD = Field(batch_first=True, tokenize=clean_string)\n    TEXT_FIELD = NestedField(NESTING_FIELD, tokenize=split_sents)'"
datasets/sst.py,2,"b'import os\n\nimport numpy as np\nimport torch\nfrom torchtext.data import NestedField, Field, TabularDataset\nfrom torchtext.data.iterator import BucketIterator\nfrom torchtext.vocab import Vectors\n\nfrom datasets.reuters import clean_string, split_sents\n\n\ndef char_quantize(string, max_length=500):\n    identity = np.identity(len(SSTCharQuantized.ALPHABET))\n    quantized_string = np.array([identity[SSTCharQuantized.ALPHABET[char]] for char in list(string.lower()) if char in SSTCharQuantized.ALPHABET], dtype=np.float32)\n    if len(quantized_string) > max_length:\n        return quantized_string[:max_length]\n    else:\n        return np.concatenate((quantized_string, np.zeros((max_length - len(quantized_string), len(SSTCharQuantized.ALPHABET)), dtype=np.float32)))\n\n\ndef process_labels(string):\n    """"""\n    Returns the label string as a list of integers\n    :param string:\n    :return:\n    """"""\n    return [float(x) for x in string]\n\n\nclass SST(TabularDataset):\n    NAME = \'SST-2\'\n    NUM_CLASSES = 2\n    IS_MULTILABEL = False\n\n    TEXT_FIELD = Field(batch_first=True, tokenize=clean_string, include_lengths=True)\n    LABEL_FIELD = Field(sequential=False, use_vocab=False, batch_first=True, preprocessing=process_labels)\n\n    @staticmethod\n    def sort_key(ex):\n        return len(ex.text)\n\n    @classmethod\n    def splits(cls, path, train=os.path.join(\'SST-2\', \'train.tsv\'),\n               validation=os.path.join(\'SST-2\', \'dev.tsv\'),\n               test=os.path.join(\'SST-2\', \'test.tsv\'), **kwargs):\n        return super(SST, cls).splits(\n            path, train=train, validation=validation, test=test,\n            format=\'tsv\', fields=[(\'label\', cls.LABEL_FIELD), (\'text\', cls.TEXT_FIELD)]\n        )\n\n    @classmethod\n    def iters(cls, path, vectors_name, vectors_cache, batch_size=64, shuffle=True, device=0, vectors=None,\n              unk_init=torch.Tensor.zero_):\n        """"""\n        :param path: directory containing train, test, dev files\n        :param vectors_name: name of word vectors file\n        :param vectors_cache: path to directory containing word vectors file\n        :param batch_size: batch size\n        :param device: GPU device\n        :param vectors: custom vectors - either predefined torchtext vectors or your own custom Vector classes\n        :param unk_init: function used to generate vector for OOV words\n        :return:\n        """"""\n        if vectors is None:\n            vectors = Vectors(name=vectors_name, cache=vectors_cache, unk_init=unk_init)\n\n        train, val, test = cls.splits(path)\n        cls.TEXT_FIELD.build_vocab(train, val, test, vectors=vectors)\n        return BucketIterator.splits((train, val, test), batch_size=batch_size, repeat=False, shuffle=shuffle,\n                                     sort_within_batch=True, device=device)\n\n\nclass SSTCharQuantized(SST):\n    ALPHABET = dict(map(lambda t: (t[1], t[0]), enumerate(list(""""""abcdefghijklmnopqrstuvwxyz0123456789,;.!?:\'\\""/\\\\|_@#$%^&*~`+-=<>()[]{}""""""))))\n    TEXT_FIELD = Field(sequential=False, use_vocab=False, batch_first=True, preprocessing=char_quantize)\n\n    @classmethod\n    def iters(cls, path, vectors_name, vectors_cache, batch_size=64, shuffle=True, device=0, vectors=None,\n              unk_init=torch.Tensor.zero_):\n        """"""\n        :param path: directory containing train, test, dev files\n        :param batch_size: batch size\n        :param device: GPU device\n        :return:\n        """"""\n        train, val, test = cls.splits(path)\n        return BucketIterator.splits((train, val, test), batch_size=batch_size, repeat=False, shuffle=shuffle, device=device)\n\n\nclass SSTHierarchical(SST):\n    NESTING_FIELD = Field(batch_first=True, tokenize=clean_string)\n    TEXT_FIELD = NestedField(NESTING_FIELD, tokenize=split_sents)\n'"
datasets/yelp2014.py,2,"b'import os\nimport re\n\nimport numpy as np\nimport torch\nfrom torchtext.data import NestedField, Field, TabularDataset\nfrom torchtext.data.iterator import BucketIterator\nfrom torchtext.vocab import Vectors\n\nfrom datasets.reuters import clean_string, split_sents\n\n\ndef char_quantize(string, max_length=1000):\n    identity = np.identity(len(Yelp2014CharQuantized.ALPHABET))\n    quantized_string = np.array([identity[Yelp2014CharQuantized.ALPHABET[char]] for char in list(string.lower()) if char in Yelp2014CharQuantized.ALPHABET], dtype=np.float32)\n    if len(quantized_string) > max_length:\n        return quantized_string[:max_length]\n    else:\n        return np.concatenate((quantized_string, np.zeros((max_length - len(quantized_string), len(Yelp2014CharQuantized.ALPHABET)), dtype=np.float32)))\n\n\ndef process_labels(string):\n    """"""\n    Returns the label string as a list of integers\n    :param string:\n    :return:\n    """"""\n    return [float(x) for x in string]\n\n\nclass Yelp2014(TabularDataset):\n    NAME = \'Yelp2014\'\n    NUM_CLASSES = 5\n    IS_MULTILABEL = False\n\n    TEXT_FIELD = Field(batch_first=True, tokenize=clean_string, include_lengths=True)\n    LABEL_FIELD = Field(sequential=False, use_vocab=False, batch_first=True, preprocessing=process_labels)\n\n    @staticmethod\n    def sort_key(ex):\n        return len(ex.text)\n\n    @classmethod\n    def splits(cls, path, train=os.path.join(\'Yelp2014\', \'train.tsv\'),\n               validation=os.path.join(\'Yelp2014\', \'dev.tsv\'),\n               test=os.path.join(\'Yelp2014\', \'test.tsv\'), **kwargs):\n        return super(Yelp2014, cls).splits(\n            path, train=train, validation=validation, test=test,\n            format=\'tsv\', fields=[(\'label\', cls.LABEL_FIELD), (\'text\', cls.TEXT_FIELD)]\n        )\n\n    @classmethod\n    def iters(cls, path, vectors_name, vectors_cache, batch_size=64, shuffle=True, device=0, vectors=None,\n              unk_init=torch.Tensor.zero_):\n        """"""\n        :param path: directory containing train, test, dev files\n        :param vectors_name: name of word vectors file\n        :param vectors_cache: path to directory containing word vectors file\n        :param batch_size: batch size\n        :param device: GPU device\n        :param vectors: custom vectors - either predefined torchtext vectors or your own custom Vector classes\n        :param unk_init: function used to generate vector for OOV words\n        :return:\n        """"""\n        if vectors is None:\n            vectors = Vectors(name=vectors_name, cache=vectors_cache, unk_init=unk_init)\n\n        train, val, test = cls.splits(path)\n        cls.TEXT_FIELD.build_vocab(train, val, test, vectors=vectors)\n        return BucketIterator.splits((train, val, test), batch_size=batch_size, repeat=False, shuffle=shuffle,\n                                     sort_within_batch=True, device=device)\n\n\nclass Yelp2014CharQuantized(Yelp2014):\n    ALPHABET = dict(map(lambda t: (t[1], t[0]), enumerate(list(""""""abcdefghijklmnopqrstuvwxyz0123456789,;.!?:\'\\""/\\\\|_@#$%^&*~`+-=<>()[]{}""""""))))\n    TEXT_FIELD = Field(sequential=False, use_vocab=False, batch_first=True, preprocessing=char_quantize)\n\n    @classmethod\n    def iters(cls, path, vectors_name, vectors_cache, batch_size=64, shuffle=True, device=0, vectors=None,\n              unk_init=torch.Tensor.zero_):\n        """"""\n        :param path: directory containing train, test, dev files\n        :param batch_size: batch size\n        :param device: GPU device\n        :return:\n        """"""\n        train, val, test = cls.splits(path)\n        return BucketIterator.splits((train, val, test), batch_size=batch_size, repeat=False, shuffle=shuffle, device=device)\n\n\nclass Yelp2014Hierarchical(Yelp2014):\n    NESTING_FIELD = Field(batch_first=True, tokenize=clean_string)\n    TEXT_FIELD = NestedField(NESTING_FIELD, tokenize=split_sents)\n'"
models/__init__.py,0,b''
models/args.py,0,"b'import os\nfrom argparse import ArgumentParser\n\n\ndef get_args():\n    parser = ArgumentParser(description=""PyTorch deep learning models for document classification"")\n\n    parser.add_argument(\'--no-cuda\', action=\'store_false\', dest=\'cuda\')\n    parser.add_argument(\'--gpu\', type=int, default=0)\n    parser.add_argument(\'--epochs\', type=int, default=50)\n    parser.add_argument(\'--batch-size\', type=int, default=32)\n    parser.add_argument(\'--lr\', type=float, default=0.001)\n    parser.add_argument(\'--seed\', type=int, default=3435)\n    parser.add_argument(\'--patience\', type=int, default=5)\n    parser.add_argument(\'--log-every\', type=int, default=10)\n    parser.add_argument(\'--data-dir\', default=os.path.join(os.pardir, \'hedwig-data\', \'datasets\'))\n\n    return parser\n'"
tasks/__init__.py,0,b''
utils/__init__.py,0,b''
utils/optimization.py,1,"b'# coding=utf-8\n# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""PyTorch optimization for BERT model.""""""\n\nimport math\n\nimport torch\n\n\ndef warmup_cosine(x, warmup=0.002):\n    if x < warmup:\n        return x/warmup\n    return 0.5 * (1.0 + torch.cos(math.pi * x))\n\n\ndef warmup_constant(x, warmup=0.002):\n    """""" Linearly increases learning rate over `warmup`*`t_total` (as provided to BertAdam) training steps.\n        Learning rate is 1. afterwards. """"""\n    if x < warmup:\n        return x/warmup\n    return 1.0\n\n\ndef warmup_linear(x, warmup=0.002):\n    """""" Specifies a triangular learning rate schedule where peak is reached at `warmup`*`t_total`-th (as provided to BertAdam) training step.\n        After `t_total`-th training step, learning rate is zero. """"""\n    if x < warmup:\n        return x/warmup\n    return max((x-1.)/(warmup-1.), 0)\n'"
utils/preprocessing.py,0,"b'import numpy as np\n\n\ndef pad_input_matrix(unpadded_matrix, max_doc_length):\n    """"""\n    Returns a zero-padded matrix for a given jagged list\n    :param unpadded_matrix: jagged list to be padded\n    :return: zero-padded matrix\n    """"""\n    max_doc_length = min(max_doc_length, max(len(x) for x in unpadded_matrix))\n    zero_padding_array = [0 for i0 in range(len(unpadded_matrix[0][0]))]\n\n    for i0 in range(len(unpadded_matrix)):\n        if len(unpadded_matrix[i0]) < max_doc_length:\n            unpadded_matrix[i0] += [zero_padding_array for i1 in range(max_doc_length - len(unpadded_matrix[i0]))]\n        elif len(unpadded_matrix[i0]) > max_doc_length:\n            unpadded_matrix[i0] = unpadded_matrix[i0][:max_doc_length]\n'"
common/evaluators/__init__.py,0,b''
common/evaluators/bert_evaluator.py,10,"b'import warnings\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom sklearn import metrics\nfrom torch.utils.data import DataLoader, SequentialSampler, TensorDataset\nfrom tqdm import tqdm\n\nfrom datasets.bert_processors.abstract_processor import convert_examples_to_features, \\\n    convert_examples_to_hierarchical_features\nfrom utils.preprocessing import pad_input_matrix\n\n# Suppress warnings from sklearn.metrics\nwarnings.filterwarnings(\'ignore\')\n\n\nclass BertEvaluator(object):\n    def __init__(self, model, processor, tokenizer, args, split=\'dev\'):\n        self.args = args\n        self.model = model\n        self.processor = processor\n        self.tokenizer = tokenizer\n\n        if split == \'test\':\n            self.eval_examples = self.processor.get_test_examples(args.data_dir)\n        else:\n            self.eval_examples = self.processor.get_dev_examples(args.data_dir)\n\n    def get_scores(self, silent=False):\n        if self.args.is_hierarchical:\n            eval_features = convert_examples_to_hierarchical_features(\n                self.eval_examples, self.args.max_seq_length, self.tokenizer)\n        else:\n            eval_features = convert_examples_to_features(\n                self.eval_examples, self.args.max_seq_length, self.tokenizer)\n\n        unpadded_input_ids = [f.input_ids for f in eval_features]\n        unpadded_input_mask = [f.input_mask for f in eval_features]\n        unpadded_segment_ids = [f.segment_ids for f in eval_features]\n\n        if self.args.is_hierarchical:\n            pad_input_matrix(unpadded_input_ids, self.args.max_doc_length)\n            pad_input_matrix(unpadded_input_mask, self.args.max_doc_length)\n            pad_input_matrix(unpadded_segment_ids, self.args.max_doc_length)\n\n        padded_input_ids = torch.tensor(unpadded_input_ids, dtype=torch.long)\n        padded_input_mask = torch.tensor(unpadded_input_mask, dtype=torch.long)\n        padded_segment_ids = torch.tensor(unpadded_segment_ids, dtype=torch.long)\n        label_ids = torch.tensor([f.label_id for f in eval_features], dtype=torch.long)\n\n        eval_data = TensorDataset(padded_input_ids, padded_input_mask, padded_segment_ids, label_ids)\n        eval_sampler = SequentialSampler(eval_data)\n        eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=self.args.batch_size)\n\n        self.model.eval()\n\n        total_loss = 0\n        nb_eval_steps, nb_eval_examples = 0, 0\n        predicted_labels, target_labels = list(), list()\n\n        for input_ids, input_mask, segment_ids, label_ids in tqdm(eval_dataloader, desc=""Evaluating"", disable=silent):\n            input_ids = input_ids.to(self.args.device)\n            input_mask = input_mask.to(self.args.device)\n            segment_ids = segment_ids.to(self.args.device)\n            label_ids = label_ids.to(self.args.device)\n\n            with torch.no_grad():\n                logits = self.model(input_ids, input_mask, segment_ids)[0]\n\n            if self.args.is_multilabel:\n                predicted_labels.extend(F.sigmoid(logits).round().long().cpu().detach().numpy())\n                target_labels.extend(label_ids.cpu().detach().numpy())\n                loss = F.binary_cross_entropy_with_logits(logits, label_ids.float(), size_average=False)\n            else:\n                predicted_labels.extend(torch.argmax(logits, dim=1).cpu().detach().numpy())\n                target_labels.extend(torch.argmax(label_ids, dim=1).cpu().detach().numpy())\n                loss = F.cross_entropy(logits, torch.argmax(label_ids, dim=1))\n\n            if self.args.n_gpu > 1:\n                loss = loss.mean()\n            if self.args.gradient_accumulation_steps > 1:\n                loss = loss / self.args.gradient_accumulation_steps\n            total_loss += loss.item()\n\n            nb_eval_examples += input_ids.size(0)\n            nb_eval_steps += 1\n\n        predicted_labels, target_labels = np.array(predicted_labels), np.array(target_labels)\n        accuracy = metrics.accuracy_score(target_labels, predicted_labels)\n        precision = metrics.precision_score(target_labels, predicted_labels, average=\'micro\')\n        recall = metrics.recall_score(target_labels, predicted_labels, average=\'micro\')\n        f1 = metrics.f1_score(target_labels, predicted_labels, average=\'micro\')\n        avg_loss = total_loss / nb_eval_steps\n\n        return [accuracy, precision, recall, f1, avg_loss], [\'accuracy\', \'precision\', \'recall\', \'f1\', \'avg_loss\']\n'"
common/evaluators/bow_evaluator.py,6,"b'import warnings\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom sklearn import metrics\nfrom torch.utils.data import DataLoader, SequentialSampler, TensorDataset\nfrom tqdm import tqdm\n\nfrom datasets.bow_processors.abstract_processor import StreamingSparseDataset\n\n# Suppress warnings from sklearn.metrics\nwarnings.filterwarnings(\'ignore\')\n\n\nclass BagOfWordsEvaluator(object):\n    def __init__(self, model, vectorizer, processor, args, split=\'dev\'):\n        self.args = args\n        self.model = model\n        self.processor = processor\n        self.vectorizer = vectorizer\n\n        if split == \'test\':\n            eval_examples = self.processor.get_test_examples(args.data_dir)\n        else:\n            eval_examples = self.processor.get_dev_examples(args.data_dir)\n\n        self.eval_features = vectorizer.transform([x.text for x in eval_examples])\n        self.eval_labels = [[float(x) for x in document.label] for document in eval_examples]\n\n    def get_scores(self, silent=False):\n        self.model.eval()\n        eval_data = StreamingSparseDataset(self.eval_features, self.eval_labels)\n        eval_dataloader = DataLoader(eval_data, shuffle=True, batch_size=self.args.batch_size)\n\n        total_loss = 0\n        nb_eval_steps = 0\n        target_labels = list()\n        predicted_labels = list()\n\n        for features, labels in tqdm(eval_dataloader, desc=""Evaluating"", disable=silent):\n            features = features.to(self.args.device)\n            labels = labels.to(self.args.device)\n\n            with torch.no_grad():\n                logits = self.model(features)\n\n            if self.args.n_gpu > 1:\n                logits = logits.view(labels.size())\n\n            if self.args.is_multilabel:\n                predicted_labels.extend(F.sigmoid(logits).round().long().cpu().detach().numpy())\n                target_labels.extend(labels.cpu().detach().numpy())\n                loss = F.binary_cross_entropy_with_logits(logits, labels.float(), size_average=False)\n            else:\n                predicted_labels.extend(torch.argmax(logits, dim=1).cpu().detach().numpy())\n                target_labels.extend(torch.argmax(labels, dim=1).cpu().detach().numpy())\n                loss = F.cross_entropy(logits, torch.argmax(labels, dim=1))\n\n            if self.args.n_gpu > 1:\n                loss = loss.mean()\n\n            total_loss += loss.item()\n            nb_eval_steps += 1\n\n        predicted_labels, target_labels = np.array(predicted_labels), np.array(target_labels)\n        accuracy = metrics.accuracy_score(target_labels, predicted_labels)\n        precision = metrics.precision_score(target_labels, predicted_labels, average=\'micro\')\n        recall = metrics.recall_score(target_labels, predicted_labels, average=\'micro\')\n        f1 = metrics.f1_score(target_labels, predicted_labels, average=\'micro\')\n        avg_loss = total_loss / nb_eval_steps\n\n        return [accuracy, precision, recall, f1, avg_loss], [\'accuracy\', \'precision\', \'recall\', \'f1\', \'avg_loss\']\n'"
common/evaluators/classification_evaluator.py,4,"b""import numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom sklearn import metrics\n\nfrom common.evaluators.evaluator import Evaluator\n\n\nclass ClassificationEvaluator(Evaluator):\n\n    def __init__(self, dataset_cls, model, embedding, data_loader, batch_size, device, keep_results=False):\n        super().__init__(dataset_cls, model, embedding, data_loader, batch_size, device, keep_results)\n        self.ignore_lengths = False\n        self.is_multilabel = False\n\n    def get_scores(self):\n        self.model.eval()\n        self.data_loader.init_epoch()\n        total_loss = 0\n\n        if hasattr(self.model, 'beta_ema') and self.model.beta_ema > 0:\n            # Temporal averaging\n            old_params = self.model.get_params()\n            self.model.load_ema_params()\n\n        predicted_labels, target_labels = list(), list()\n        for batch_idx, batch in enumerate(self.data_loader):\n            if hasattr(self.model, 'tar') and self.model.tar:\n                if self.ignore_lengths:\n                    scores, rnn_outs = self.model(batch.text)\n                else:\n                    scores, rnn_outs = self.model(batch.text[0], lengths=batch.text[1])\n            else:\n                if self.ignore_lengths:\n                    scores = self.model(batch.text)\n                else:\n                    scores = self.model(batch.text[0], lengths=batch.text[1])\n\n            if self.is_multilabel:\n                scores_rounded = F.sigmoid(scores).round().long()\n                predicted_labels.extend(scores_rounded.cpu().detach().numpy())\n                target_labels.extend(batch.label.cpu().detach().numpy())\n                total_loss += F.binary_cross_entropy_with_logits(scores, batch.label.float(), size_average=False).item()\n            else:\n                predicted_labels.extend(torch.argmax(scores, dim=1).cpu().detach().numpy())\n                target_labels.extend(torch.argmax(batch.label, dim=1).cpu().detach().numpy())\n                total_loss += F.cross_entropy(scores, torch.argmax(batch.label, dim=1), size_average=False).item()\n\n            if hasattr(self.model, 'tar') and self.model.tar:\n                # Temporal activation regularization\n                total_loss += (rnn_outs[1:] - rnn_outs[:-1]).pow(2).mean()\n\n        predicted_labels = np.array(predicted_labels)\n        target_labels = np.array(target_labels)\n        accuracy = metrics.accuracy_score(target_labels, predicted_labels)\n        precision = metrics.precision_score(target_labels, predicted_labels, average='micro')\n        recall = metrics.recall_score(target_labels, predicted_labels, average='micro')\n        f1 = metrics.f1_score(target_labels, predicted_labels, average='micro')\n        avg_loss = total_loss / len(self.data_loader.dataset.examples)\n\n        if hasattr(self.model, 'beta_ema') and self.model.beta_ema > 0:\n            # Temporal averaging\n            self.model.load_params(old_params)\n\n        return [accuracy, precision, recall, f1, avg_loss], ['accuracy', 'precision', 'recall', 'f1', 'cross_entropy_loss']\n"""
common/evaluators/evaluator.py,0,"b'class Evaluator(object):\n    """"""\n    Evaluates a model on a Dataset, using metrics specific to the Dataset.\n    """"""\n\n    def __init__(self, dataset_cls, model, embedding, data_loader, batch_size, device, keep_results=False):\n        self.dataset_cls = dataset_cls\n        self.model = model\n        self.embedding = embedding\n        self.data_loader = data_loader\n        self.batch_size = batch_size\n        self.device = device\n        self.keep_results = keep_results\n\n    def get_sentence_embeddings(self, batch):\n        sent1 = self.embedding(batch.sentence_1).transpose(1, 2)\n        sent2 = self.embedding(batch.sentence_2).transpose(1, 2)\n        return sent1, sent2\n\n    def get_scores(self):\n        """"""\n        Get the scores used to evaluate the model.\n        Should return ([score1, score2, ..], [score1_name, score2_name, ...]).\n        The first score is the primary score used to determine if the model has improved.\n        """"""\n        raise NotImplementedError(\'Evaluator subclass needs to implement get_score\')\n'"
common/evaluators/relevance_transfer_evaluator.py,13,"b'import warnings\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom sklearn import metrics\nfrom torch.utils.data import TensorDataset, SequentialSampler, DataLoader\nfrom tqdm import tqdm\n\nfrom common.constants import BERT_MODELS\nfrom datasets.bert_processors.robust45_processor import convert_examples_to_features\n\n# Suppress warnings from sklearn.metrics\nwarnings.filterwarnings(\'ignore\')\n\n\nclass RelevanceTransferEvaluator(object):\n    def __init__(self, model, config, **kwargs):\n        if config[\'model\'] in BERT_MODELS:\n            self.processor = kwargs[\'processor\']\n            self.tokenizer = kwargs[\'tokenizer\']\n            if config[\'split\'] == \'test\':\n                self.eval_examples = self.processor.get_test_examples(config[\'data_dir\'], topic=config[\'topic\'])\n            else:\n                self.eval_examples = self.processor.get_dev_examples(config[\'data_dir\'], topic=config[\'topic\'])\n        else:\n            self.data_loader = kwargs[\'data_loader\']\n\n        self.model = model\n        self.config = config\n        self.ignore_lengths = config[\'ignore_lengths\']\n        self.y_target = None\n        self.y_pred = None\n        self.docid = None\n\n    def get_scores(self, silent=False):\n        self.model.eval()\n        self.y_target = list()\n        self.y_pred = list()\n        self.docid = list()\n        total_loss = 0\n\n        if self.config[\'model\'] in {\'BERT-Base\', \'BERT-Large\', \'HBERT-Base\', \'HBERT-Large\'}:\n            eval_features = convert_examples_to_features(\n                self.eval_examples,\n                self.config[\'max_seq_length\'],\n                self.tokenizer)\n\n            unpadded_input_ids = [f.input_ids for f in eval_features]\n            unpadded_input_mask = [f.input_mask for f in eval_features]\n            unpadded_segment_ids = [f.segment_ids for f in eval_features]\n\n            padded_input_ids = torch.tensor(unpadded_input_ids, dtype=torch.long)\n            padded_input_mask = torch.tensor(unpadded_input_mask, dtype=torch.long)\n            padded_segment_ids = torch.tensor(unpadded_segment_ids, dtype=torch.long)\n            label_ids = torch.tensor([f.label_id for f in eval_features], dtype=torch.long)\n            document_ids = torch.tensor([f.guid for f in eval_features], dtype=torch.long)\n\n            eval_data = TensorDataset(padded_input_ids, padded_input_mask, padded_segment_ids, label_ids, document_ids)\n            eval_sampler = SequentialSampler(eval_data)\n            eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=self.config[\'batch_size\'])\n\n            for input_ids, input_mask, segment_ids, label_ids, document_ids in tqdm(eval_dataloader, desc=""Evaluating"", disable=silent):\n                input_ids = input_ids.to(self.config[\'device\'])\n                input_mask = input_mask.to(self.config[\'device\'])\n                segment_ids = segment_ids.to(self.config[\'device\'])\n                label_ids = label_ids.to(self.config[\'device\'])\n\n                with torch.no_grad():\n                    logits = torch.sigmoid(self.model(input_ids, input_mask, segment_ids)[0]).squeeze(dim=1)\n\n                # Computing loss and storing predictions\n                self.docid.extend(document_ids.cpu().detach().numpy())\n                self.y_pred.extend(logits.cpu().detach().numpy())\n                self.y_target.extend(label_ids.cpu().detach().numpy())\n                loss = F.binary_cross_entropy(logits, label_ids.float())\n\n                if self.config[\'n_gpu\'] > 1:\n                    loss = loss.mean()\n                if self.config[\'gradient_accumulation_steps\'] > 1:\n                    loss = loss / self.config[\'gradient_accumulation_steps\']\n                total_loss += loss.item()\n\n        else:\n            self.data_loader.init_epoch()\n\n            if hasattr(self.model, \'beta_ema\') and self.model.beta_ema > 0:\n                # Temporal averaging\n                old_params = self.model.get_params()\n                self.model.load_ema_params()\n\n            for batch in tqdm(self.data_loader, desc=""Evaluating"", disable=silent):\n                if hasattr(self.model, \'tar\') and self.model.tar:\n                    if self.ignore_lengths:\n                        logits, rnn_outs = torch.sigmoid(self.model(batch.text)).squeeze(dim=1)\n                    else:\n                        logits, rnn_outs = torch.sigmoid(self.model(batch.text[0], lengths=batch.text[1])).squeeze(dim=1)\n                else:\n                    if self.ignore_lengths:\n                        logits = torch.sigmoid(self.model(batch.text)).squeeze(dim=1)\n                    else:\n                        logits = torch.sigmoid(self.model(batch.text[0], lengths=batch.text[1])).squeeze(dim=1)\n\n                total_loss += F.binary_cross_entropy(logits, batch.label.float()).item()\n                if hasattr(self.model, \'tar\') and self.model.tar:\n                    # Temporal activation regularization\n                    total_loss += (rnn_outs[1:] - rnn_outs[:-1]).pow(2).mean()\n\n                self.docid.extend(batch.docid.cpu().detach().numpy())\n                self.y_pred.extend(logits.cpu().detach().numpy())\n                self.y_target.extend(batch.label.cpu().detach().numpy())\n\n            if hasattr(self.model, \'beta_ema\') and self.model.beta_ema > 0:\n                # Temporal averaging\n                self.model.load_params(old_params)\n\n        predicted_labels = np.around(np.array(self.y_pred))\n        target_labels = np.array(self.y_target)\n        accuracy = metrics.accuracy_score(target_labels, predicted_labels)\n        average_precision = metrics.average_precision_score(target_labels, predicted_labels, average=None)\n        f1 = metrics.f1_score(target_labels, predicted_labels, average=\'macro\')\n        avg_loss = total_loss / len(predicted_labels)\n\n        try:\n            precision = metrics.precision_score(target_labels, predicted_labels, average=None)[1]\n        except IndexError:\n            # Handle cases without positive labels\n            precision = 0\n\n        return [accuracy, precision, average_precision, f1, avg_loss], \\\n               [\'accuracy\', \'precision\', \'average_precision\', \'f1\', \'cross_entropy_loss\']\n'"
common/trainers/__init__.py,0,b''
common/trainers/bert_trainer.py,8,"b'import datetime\nimport os\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, RandomSampler, TensorDataset\nfrom tqdm import tqdm\nfrom tqdm import trange\n\nfrom common.evaluators.bert_evaluator import BertEvaluator\nfrom datasets.bert_processors.abstract_processor import convert_examples_to_features\nfrom datasets.bert_processors.abstract_processor import convert_examples_to_hierarchical_features\nfrom utils.optimization import warmup_linear\nfrom utils.preprocessing import pad_input_matrix\n\n\nclass BertTrainer(object):\n    def __init__(self, model, optimizer, processor, scheduler, tokenizer, args):\n        self.args = args\n        self.model = model\n        self.optimizer = optimizer\n        self.processor = processor\n        self.scheduler = scheduler\n        self.tokenizer = tokenizer\n        self.train_examples = self.processor.get_train_examples(args.data_dir)\n\n        timestamp = datetime.datetime.now().strftime(""%Y-%m-%d_%H-%M-%S"")\n        self.snapshot_path = os.path.join(self.args.save_path, self.processor.NAME, \'%s.pt\' % timestamp)\n\n        self.num_train_optimization_steps = int(\n            len(self.train_examples) / args.batch_size / args.gradient_accumulation_steps) * args.epochs\n\n        self.log_header = \'Epoch Iteration Progress   Dev/Acc.  Dev/Pr.  Dev/Re.   Dev/F1   Dev/Loss\'\n        self.log_template = \' \'.join(\'{:>5.0f},{:>9.0f},{:>6.0f}/{:<5.0f} {:>6.4f},{:>8.4f},{:8.4f},{:8.4f},{:10.4f}\'.split(\',\'))\n\n        self.iterations, self.nb_tr_steps, self.tr_loss = 0, 0, 0\n        self.best_dev_f1, self.unimproved_iters = 0, 0\n        self.early_stop = False\n\n    def train_epoch(self, train_dataloader):\n        for step, batch in enumerate(tqdm(train_dataloader, desc=""Training"")):\n            self.model.train()\n            batch = tuple(t.to(self.args.device) for t in batch)\n            input_ids, input_mask, segment_ids, label_ids = batch\n            logits = self.model(input_ids, input_mask, segment_ids)[0]\n\n            if self.args.is_multilabel:\n                loss = F.binary_cross_entropy_with_logits(logits, label_ids.float())\n            else:\n                loss = F.cross_entropy(logits, torch.argmax(label_ids, dim=1))\n\n            if self.args.n_gpu > 1:\n                loss = loss.mean()\n            if self.args.gradient_accumulation_steps > 1:\n                loss = loss / self.args.gradient_accumulation_steps\n\n            if self.args.fp16:\n                self.optimizer.backward(loss)\n            else:\n                loss.backward()\n\n            self.tr_loss += loss.item()\n            self.nb_tr_steps += 1\n            if (step + 1) % self.args.gradient_accumulation_steps == 0:\n                if self.args.fp16:\n                    lr_this_step = self.args.learning_rate * warmup_linear(self.iterations / self.num_train_optimization_steps, self.args.warmup_proportion)\n                    for param_group in self.optimizer.param_groups:\n                        param_group[\'lr\'] = lr_this_step\n                self.optimizer.step()\n                self.scheduler.step()\n                self.optimizer.zero_grad()\n                self.iterations += 1\n\n    def train(self):\n        if self.args.is_hierarchical:\n            train_features = convert_examples_to_hierarchical_features(\n                self.train_examples, self.args.max_seq_length, self.tokenizer)\n        else:\n            train_features = convert_examples_to_features(\n                self.train_examples, self.args.max_seq_length, self.tokenizer)\n\n        unpadded_input_ids = [f.input_ids for f in train_features]\n        unpadded_input_mask = [f.input_mask for f in train_features]\n        unpadded_segment_ids = [f.segment_ids for f in train_features]\n\n        if self.args.is_hierarchical:\n            pad_input_matrix(unpadded_input_ids, self.args.max_doc_length)\n            pad_input_matrix(unpadded_input_mask, self.args.max_doc_length)\n            pad_input_matrix(unpadded_segment_ids, self.args.max_doc_length)\n\n        print(""Number of examples: "", len(self.train_examples))\n        print(""Batch size:"", self.args.batch_size)\n        print(""Num of steps:"", self.num_train_optimization_steps)\n\n        padded_input_ids = torch.tensor(unpadded_input_ids, dtype=torch.long)\n        padded_input_mask = torch.tensor(unpadded_input_mask, dtype=torch.long)\n        padded_segment_ids = torch.tensor(unpadded_segment_ids, dtype=torch.long)\n        label_ids = torch.tensor([f.label_id for f in train_features], dtype=torch.long)\n\n        train_data = TensorDataset(padded_input_ids, padded_input_mask, padded_segment_ids, label_ids)\n\n        train_sampler = RandomSampler(train_data)\n        train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=self.args.batch_size)\n\n        for epoch in trange(int(self.args.epochs), desc=""Epoch""):\n            self.train_epoch(train_dataloader)\n            dev_evaluator = BertEvaluator(self.model, self.processor, self.tokenizer, self.args, split=\'dev\')\n            dev_acc, dev_precision, dev_recall, dev_f1, dev_loss = dev_evaluator.get_scores()[0]\n\n            # Print validation results\n            tqdm.write(self.log_header)\n            tqdm.write(self.log_template.format(epoch + 1, self.iterations, epoch + 1, self.args.epochs,\n                                                dev_acc, dev_precision, dev_recall, dev_f1, dev_loss))\n\n            # Update validation results\n            if dev_f1 > self.best_dev_f1:\n                self.unimproved_iters = 0\n                self.best_dev_f1 = dev_f1\n                torch.save(self.model, self.snapshot_path)\n\n            else:\n                self.unimproved_iters += 1\n                if self.unimproved_iters >= self.args.patience:\n                    self.early_stop = True\n                    tqdm.write(""Early Stopping. Epoch: {}, Best Dev F1: {}"".format(epoch, self.best_dev_f1))\n                    break\n'"
common/trainers/bow_trainer.py,4,"b'import datetime\nimport os\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm, trange\n\nfrom common.evaluators.bow_evaluator import BagOfWordsEvaluator\nfrom datasets.bow_processors.abstract_processor import StreamingSparseDataset\n\n\nclass BagOfWordsTrainer(object):\n    def __init__(self, model, vectorizer, optimizer, processor, args):\n        self.args = args\n        self.model = model\n        self.processor = processor\n        self.optimizer = optimizer\n        self.vectorizer = vectorizer\n\n        train_examples = self.processor.get_train_examples(args.data_dir)\n        self.train_features = vectorizer.fit_transform([x.text for x in train_examples])\n        self.train_labels = [[float(x) for x in document.label] for document in train_examples]\n\n        timestamp = datetime.datetime.now().strftime(""%Y-%m-%d_%H-%M-%S"")\n        self.snapshot_path = os.path.join(self.args.save_path, self.processor.NAME, \'%s.pt\' % timestamp)\n\n        self.log_header = \'Epoch Iteration Progress   Dev/Acc.  Dev/Pr.  Dev/Re.   Dev/F1   Dev/Loss\'\n        self.log_template = \' \'.join(\'{:>5.0f},{:>9.0f},{:>6.0f}/{:<5.0f} {:>6.4f},{:>8.4f},{:8.4f},{:8.4f},{:10.4f}\'.split(\',\'))\n\n        self.train_loss = 0\n        self.best_dev_f1 = 0\n        self.nb_train_steps = 0\n        self.unimproved_iters = 0\n        self.early_stop = False\n\n    def train_epoch(self, train_dataloader):\n        for step, batch in enumerate(tqdm(train_dataloader, desc=""Training"")):\n            self.model.train()\n            self.optimizer.zero_grad()\n\n            features, labels = tuple(t.to(self.args.device) for t in batch)\n            logits = self.model(features)\n\n            if self.args.n_gpu > 1:\n                logits = logits.view(labels.size())\n\n            if self.args.is_multilabel:\n                loss = F.binary_cross_entropy_with_logits(logits, labels.float())\n            else:\n                loss = F.cross_entropy(logits, torch.argmax(labels, dim=1))\n\n            if self.args.n_gpu > 1:\n                loss = loss.mean()\n\n            loss.backward()\n            self.optimizer.step()\n            self.train_loss += loss.item()\n            self.nb_train_steps += 1\n\n    def train(self):\n        train_data = StreamingSparseDataset(self.train_features, self.train_labels)\n        train_dataloader = DataLoader(train_data, shuffle=True, batch_size=self.args.batch_size)\n\n        print(""Number of examples: "", len(self.train_labels))\n        print(""Batch size:"", self.args.batch_size)\n\n        for epoch in trange(int(self.args.epochs), desc=""Epoch""):\n            self.train_epoch(train_dataloader)\n            dev_evaluator = BagOfWordsEvaluator(self.model, self.vectorizer, self.processor, self.args, split=\'dev\')\n            dev_acc, dev_precision, dev_recall, dev_f1, dev_loss = dev_evaluator.get_scores()[0]\n\n            # Print validation results\n            tqdm.write(self.log_header)\n            tqdm.write(self.log_template.format(epoch + 1, self.nb_train_steps, epoch + 1, self.args.epochs,\n                                                dev_acc, dev_precision, dev_recall, dev_f1, dev_loss))\n\n            # Update validation results\n            if dev_f1 > self.best_dev_f1:\n                self.unimproved_iters = 0\n                self.best_dev_f1 = dev_f1\n                torch.save(self.model, self.snapshot_path)\n            else:\n                self.unimproved_iters += 1\n                if self.unimproved_iters >= self.args.patience:\n                    self.early_stop = True\n                    tqdm.write(""Early Stopping. Epoch: {}, Best Dev F1: {}"".format(epoch, self.best_dev_f1))\n                    break\n'"
common/trainers/classification_trainer.py,4,"b'import datetime\nimport os\nimport time\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\n\nfrom common.trainers.trainer import Trainer\n\n\nclass ClassificationTrainer(Trainer):\n\n    def __init__(self, model, embedding, train_loader, trainer_config, train_evaluator, test_evaluator, dev_evaluator):\n        super().__init__(model, embedding, train_loader, trainer_config, train_evaluator, test_evaluator, dev_evaluator)\n        self.config = trainer_config\n        self.early_stop = False\n        self.best_dev_f1 = 0\n        self.iterations = 0\n        self.iters_not_improved = 0\n        self.start = None\n        self.log_template = \' \'.join(\n            \'{:>6.0f},{:>5.0f},{:>9.0f},{:>5.0f}/{:<5.0f} {:>7.0f}%,{:>8.6f},{:12.4f}\'.split(\',\'))\n        self.dev_log_template = \' \'.join(\n            \'{:>6.0f},{:>5.0f},{:>9.0f},{:>5.0f}/{:<5.0f} {:>7.4f},{:>8.4f},{:8.4f},{:12.4f},{:12.4f}\'.split(\',\'))\n\n        timestamp = datetime.datetime.now().strftime(""%Y-%m-%d_%H-%M-%S"")\n        self.snapshot_path = os.path.join(self.model_outfile, self.train_loader.dataset.NAME, \'%s.pt\' % timestamp)\n\n    def train_epoch(self, epoch):\n        self.train_loader.init_epoch()\n        n_correct, n_total = 0, 0\n        for batch_idx, batch in enumerate(self.train_loader):\n            self.iterations += 1\n            self.model.train()\n            self.optimizer.zero_grad()\n            if hasattr(self.model, \'tar\') and self.model.tar:\n                if \'ignore_lengths\' in self.config and self.config[\'ignore_lengths\']:\n                    scores, rnn_outs = self.model(batch.text)\n                else:\n                    scores, rnn_outs = self.model(batch.text[0], lengths=batch.text[1])\n            else:\n                if \'ignore_lengths\' in self.config and self.config[\'ignore_lengths\']:\n                    scores = self.model(batch.text)\n                else:\n                    scores = self.model(batch.text[0], lengths=batch.text[1])\n\n            if \'is_multilabel\' in self.config and self.config[\'is_multilabel\']:\n                predictions = F.sigmoid(scores).round().long()\n                for tensor1, tensor2 in zip(predictions, batch.label):\n                    if np.array_equal(tensor1, tensor2):\n                        n_correct += 1\n                loss = F.binary_cross_entropy_with_logits(scores, batch.label.float())\n            else:\n                for tensor1, tensor2 in zip(torch.argmax(scores, dim=1), torch.argmax(batch.label.data, dim=1)):\n                    if np.array_equal(tensor1, tensor2):\n                        n_correct += 1\n                loss = F.cross_entropy(scores, torch.argmax(batch.label.data, dim=1))\n\n            if hasattr(self.model, \'tar\') and self.model.tar:\n                loss = loss + self.model.tar * (rnn_outs[1:] - rnn_outs[:-1]).pow(2).mean()\n            if hasattr(self.model, \'ar\') and self.model.ar:\n                loss = loss + self.model.ar * (rnn_outs[:]).pow(2).mean()\n\n            n_total += batch.batch_size\n            train_acc = 100. * n_correct / n_total\n            loss.backward()\n            self.optimizer.step()\n\n            if hasattr(self.model, \'beta_ema\') and self.model.beta_ema > 0:\n                # Temporal averaging\n                self.model.update_ema()\n\n            if self.iterations % self.log_interval == 1:\n                niter = epoch * len(self.train_loader) + batch_idx\n                print(self.log_template.format(time.time() - self.start, epoch, self.iterations, 1 + batch_idx,\n                                               len(self.train_loader), 100.0 * (1 + batch_idx) / len(self.train_loader),\n                                               loss.item(), train_acc))\n\n    def train(self, epochs):\n        self.start = time.time()\n        header = \'  Time Epoch Iteration Progress    (%Epoch)   Loss     Accuracy\'\n        dev_header = \'  Time Epoch Iteration Progress     Dev/Acc. Dev/Pr.  Dev/Recall   Dev/F1       Dev/Loss\'\n        os.makedirs(self.model_outfile, exist_ok=True)\n        os.makedirs(os.path.join(self.model_outfile, self.train_loader.dataset.NAME), exist_ok=True)\n\n        for epoch in range(1, epochs + 1):\n            print(\'\\n\' + header)\n            self.train_epoch(epoch)\n\n            # Evaluate performance on validation set\n            dev_acc, dev_precision, dev_recall, dev_f1, dev_loss = self.dev_evaluator.get_scores()[0]\n\n            # Print validation results\n            print(\'\\n\' + dev_header)\n            print(self.dev_log_template.format(time.time() - self.start, epoch, self.iterations, epoch, epochs,\n                                               dev_acc, dev_precision, dev_recall, dev_f1, dev_loss))\n\n            # Update validation results\n            if dev_f1 > self.best_dev_f1:\n                self.iters_not_improved = 0\n                self.best_dev_f1 = dev_f1\n                torch.save(self.model, self.snapshot_path)\n            else:\n                self.iters_not_improved += 1\n                if self.iters_not_improved >= self.patience:\n                    self.early_stop = True\n                    print(""Early Stopping. Epoch: {}, Best Dev F1: {}"".format(epoch, self.best_dev_f1))\n                    break\n'"
common/trainers/relevance_transfer_trainer.py,13,"b'import datetime\nimport os\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.utils.data import TensorDataset, RandomSampler, DataLoader\nfrom tqdm import trange, tqdm\n\nfrom common.constants import BERT_MODELS\nfrom datasets.bert_processors.robust45_processor import convert_examples_to_features\nfrom tasks.relevance_transfer.resample import ImbalancedDatasetSampler\n\n\nclass RelevanceTransferTrainer(object):\n    def __init__(self, model, config, **kwargs):\n        if config[\'model\'] in BERT_MODELS:\n            self.processor = kwargs[\'processor\']\n            self.scheduler = kwargs[\'scheduler\']\n            self.tokenizer = kwargs[\'tokenizer\']\n            self.train_examples = self.processor.get_train_examples(config[\'data_dir\'], topic=config[\'topic\'])\n        else:\n            self.train_loader = kwargs[\'train_loader\']\n\n        self.model = model\n        self.config = config\n        self.early_stop = False\n        self.best_dev_ap = 0\n        self.iterations = 0\n        self.unimproved_iters = 0\n        self.optimizer = kwargs[\'optimizer\']\n        self.dev_evaluator = kwargs[\'dev_evaluator\']\n\n        self.log_header = \'Epoch Iteration Progress   Dev/Acc.  Dev/Pr.  Dev/AP.   Dev/F1   Dev/Loss\'\n        self.log_template = \' \'.join(\'{:>5.0f},{:>9.0f},{:>6.0f}/{:<5.0f} {:>6.4f},{:>8.4f},{:8.4f},{:8.4f},{:10.4f}\'.split(\',\'))\n\n        timestamp = datetime.datetime.now().strftime(""%Y-%m-%d_%H-%M-%S"")\n        self.snapshot_path = os.path.join(config[\'save_path\'], config[\'dataset\'].NAME, \'%s.pt\' % timestamp)\n\n    def train_epoch(self):\n        for step, batch in enumerate(tqdm(self.train_loader, desc=""Training"")):\n            self.model.train()\n\n            if self.config[\'model\'] in BERT_MODELS:\n                batch = tuple(t.to(self.config[\'device\']) for t in batch)\n                input_ids, input_mask, segment_ids, label_ids = batch\n                logits = torch.sigmoid(self.model(input_ids, input_mask, segment_ids)[0]).squeeze(dim=1)\n                loss = F.binary_cross_entropy(logits, label_ids.float())\n\n                if self.config[\'n_gpu\'] > 1:\n                    loss = loss.mean()\n                if self.config[\'gradient_accumulation_steps\'] > 1:\n                    loss = loss / self.config[\'gradient_accumulation_steps\']\n\n                loss.backward()\n\n                if (step + 1) % self.config[\'gradient_accumulation_steps\'] == 0:\n                    self.optimizer.step()\n                    self.scheduler.step()\n                    self.optimizer.zero_grad()\n                    self.iterations += 1\n\n            else:\n                # Clip gradients to address exploding gradients in LSTM\n                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 25.0)\n\n                # Randomly sample equal number of positive and negative documents\n                self.train_loader.init_epoch()\n                if \'ignore_lengths\' in self.config and self.config[\'ignore_lengths\']:\n                    if \'resample\' in self.config and self.config[\'resample\']:\n                        indices = ImbalancedDatasetSampler(batch.text, batch.label).get_indices()\n                        batch_text = batch.text[indices]\n                        batch_label = batch.label[indices]\n                    else:\n                        batch_text = batch.text\n                        batch_label = batch.label\n                else:\n                    if \'resample\' in self.config and self.config[\'resample\']:\n                        indices = ImbalancedDatasetSampler(batch.text[0], batch.label).get_indices()\n                        batch_text = batch.text[0][indices]\n                        batch_lengths = batch.text[1][indices]\n                        batch_label = batch.label\n                    else:\n                        batch_text = batch.text[0]\n                        batch_lengths = batch.text[1]\n                        batch_label = batch.label\n\n                if hasattr(self.model, \'tar\') and self.model.tar:\n                    if \'ignore_lengths\' in self.config and self.config[\'ignore_lengths\']:\n                        logits, rnn_outs = torch.sigmoid(self.model(batch_text)).squeeze(dim=1)\n                    else:\n                        logits, rnn_outs = torch.sigmoid(self.model(batch_text, lengths=batch_lengths)).squeeze(dim=1)\n                else:\n                    if \'ignore_lengths\' in self.config and self.config[\'ignore_lengths\']:\n                        logits = torch.sigmoid(self.model(batch_text)).squeeze(dim=1)\n                    else:\n                        logits = torch.sigmoid(self.model(batch_text, lengths=batch_lengths)).squeeze(dim=1)\n\n                loss = F.binary_cross_entropy(logits, batch_label.float())\n                if hasattr(self.model, \'tar\') and self.model.tar:\n                    loss = loss + (rnn_outs[1:] - rnn_outs[:-1]).pow(2).mean()\n\n                loss.backward()\n                self.optimizer.step()\n                self.iterations += 1\n                self.optimizer.zero_grad()\n\n                if hasattr(self.model, \'beta_ema\') and self.model.beta_ema > 0:\n                    # Temporal averaging\n                    self.model.update_ema()\n\n    def train(self, epochs):\n        if self.config[\'model\'] in BERT_MODELS:\n            train_features = convert_examples_to_features(\n                self.train_examples,\n                self.config[\'max_seq_length\'],\n                self.tokenizer\n            )\n\n            unpadded_input_ids = [f.input_ids for f in train_features]\n            unpadded_input_mask = [f.input_mask for f in train_features]\n            unpadded_segment_ids = [f.segment_ids for f in train_features]\n\n            padded_input_ids = torch.tensor(unpadded_input_ids, dtype=torch.long)\n            padded_input_mask = torch.tensor(unpadded_input_mask, dtype=torch.long)\n            padded_segment_ids = torch.tensor(unpadded_segment_ids, dtype=torch.long)\n            label_ids = torch.tensor([f.label_id for f in train_features], dtype=torch.long)\n\n            train_data = TensorDataset(padded_input_ids, padded_input_mask, padded_segment_ids, label_ids)\n            train_sampler = RandomSampler(train_data)\n            self.train_loader = DataLoader(train_data, sampler=train_sampler, batch_size=self.config[\'batch_size\'])\n\n        with trange(1, epochs + 1, desc=""Epoch"") as t_epochs:\n            for epoch in t_epochs:\n                self.train_epoch()\n\n                # Evaluate performance on validation set\n                dev_acc, dev_precision, dev_ap, dev_f1, dev_loss = self.dev_evaluator.get_scores()[0]\n                tqdm.write(self.log_header)\n                tqdm.write(self.log_template.format(epoch, self.iterations, epoch, epochs,\n                                                    dev_acc, dev_precision, dev_ap, dev_f1, dev_loss))\n\n                # Update validation results\n                if dev_f1 > self.best_dev_ap:\n                    self.unimproved_iters = 0\n                    self.best_dev_ap = dev_f1\n                    torch.save(self.model, self.snapshot_path)\n                else:\n                    self.unimproved_iters += 1\n                    if self.unimproved_iters >= self.config[\'patience\']:\n                        self.early_stop = True\n                        tqdm.write(""Early Stopping. Epoch: {}, Best Dev F1: {}"".format(epoch, self.best_dev_ap))\n                        t_epochs.close()\n                        break\n'"
common/trainers/trainer.py,0,"b'class Trainer(object):\n\n    """"""\n    Abstraction for training a model on a Dataset.\n    """"""\n\n    def __init__(self, model, embedding, train_loader, trainer_config, train_evaluator, test_evaluator, dev_evaluator=None):\n        self.model = model\n        self.embedding = embedding\n        self.optimizer = trainer_config.get(\'optimizer\')\n        self.train_loader = train_loader\n        self.batch_size = trainer_config.get(\'batch_size\')\n        self.log_interval = trainer_config.get(\'log_interval\')\n        self.model_outfile = trainer_config.get(\'model_outfile\')\n        self.lr_reduce_factor = trainer_config.get(\'lr_reduce_factor\')\n        self.patience = trainer_config.get(\'patience\')\n        self.clip_norm = trainer_config.get(\'clip_norm\')\n\n        self.logger = trainer_config.get(\'logger\')\n\n        self.train_evaluator = train_evaluator\n        self.test_evaluator = test_evaluator\n        self.dev_evaluator = dev_evaluator\n\n    def evaluate(self, evaluator, dataset_name):\n        scores, metric_names = evaluator.get_scores()\n        if self.logger is not None:\n            self.logger.info(\'Evaluation metrics for {}:\'.format(dataset_name))\n            self.logger.info(\'\\t\'.join([\' \'] + metric_names))\n            self.logger.info(\'\\t\'.join([dataset_name] + list(map(str, scores))))\n        return scores\n\n    def get_sentence_embeddings(self, batch):\n        sent1 = self.embedding(batch.sentence_1).transpose(1, 2)\n        sent2 = self.embedding(batch.sentence_2).transpose(1, 2)\n        return sent1, sent2\n\n    def train_epoch(self, epoch):\n        raise NotImplementedError()\n\n    def train(self, epochs):\n        raise NotImplementedError()\n'"
datasets/bert_processors/__init__.py,0,b''
datasets/bert_processors/aapd_processor.py,0,"b'import os\n\nfrom datasets.bert_processors.abstract_processor import BertProcessor, InputExample\n\n\nclass AAPDProcessor(BertProcessor):\n    NAME = \'AAPD\'\n    NUM_CLASSES = 54\n    IS_MULTILABEL = True\n\n    def get_train_examples(self, data_dir):\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir,\'AAPD\', \'train.tsv\')), \'train\')\n\n    def get_dev_examples(self, data_dir):\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, \'AAPD\', \'dev.tsv\')), \'dev\')\n\n    def get_test_examples(self, data_dir):\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, \'AAPD\', \'test.tsv\')), \'test\')\n\n    def _create_examples(self, lines, set_type):\n        examples = []\n        for (i, line) in enumerate(lines):\n            if i == 0:\n                continue\n            guid = ""%s-%s"" % (set_type, i)\n            text_a = line[1]\n            label = line[0]\n            examples.append(\n                InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n        return examples\n'"
datasets/bert_processors/abstract_processor.py,0,"b'import csv\n\nimport sys\nimport numpy as np\nfrom nltk.tokenize import sent_tokenize\n\n\nclass InputExample(object):\n    """"""A single training/test example for simple sequence classification.""""""\n\n    def __init__(self, guid, text_a, text_b=None, label=None):\n        """"""Constructs a InputExample.\n\n        Args:\n            guid: Unique id for the example.\n            text_a: string. The untokenized text of the first sequence. For single\n            sequence tasks, only this sequence must be specified.\n            text_b: (Optional) string. The untokenized text of the second sequence.\n            Only must be specified for sequence pair tasks.\n            label: (Optional) string. The label of the example. This should be\n            specified for train and dev examples, but not for test examples.\n        """"""\n        self.guid = guid\n        self.text_a = text_a\n        self.text_b = text_b\n        self.label = label\n\n\nclass InputFeatures(object):\n    """"""A single set of features of data.""""""\n\n    def __init__(self, input_ids, input_mask, segment_ids, label_id):\n        self.input_ids = input_ids\n        self.input_mask = input_mask\n        self.segment_ids = segment_ids\n        self.label_id = label_id\n\n\nclass BertProcessor(object):\n    """"""Base class for data converters for sequence classification data sets.""""""\n\n    def get_train_examples(self, data_dir):\n        """"""\n        Gets a collection of `InputExample`s for the train set\n        :param data_dir:\n        :return:\n        """"""\n        raise NotImplementedError()\n\n    def get_dev_examples(self, data_dir):\n        """"""\n        Gets a collection of `InputExample`s for the dev set\n        :param data_dir:\n        :return:\n        """"""\n        raise NotImplementedError()\n\n    def get_test_examples(self, data_dir):\n        """"""\n        Gets a collection of `InputExample`s for the test set\n        :param data_dir:\n        :return:\n        """"""\n        raise NotImplementedError()\n\n    def get_labels(self):\n        """"""\n        Gets a list of possible labels in the dataset\n        :return:\n        """"""\n        raise NotImplementedError()\n\n    @classmethod\n    def _read_tsv(cls, input_file, quotechar=None):\n        """"""\n        Reads a Tab Separated Values (TSV) file\n        :param input_file:\n        :param quotechar:\n        :return:\n        """"""\n        with open(input_file, ""r"") as f:\n            reader = csv.reader(f, delimiter=""\\t"", quotechar=quotechar)\n            lines = []\n            for line in reader:\n                if sys.version_info[0] == 2:\n                    line = list(str(cell, \'utf-8\') for cell in line)\n                lines.append(line)\n            return lines\n\n\ndef convert_examples_to_features(examples, max_seq_length, tokenizer, print_examples=False):\n    """"""\n    Loads a data file into a list of InputBatch objects\n    :param examples:\n    :param max_seq_length:\n    :param tokenizer:\n    :param print_examples:\n    :return: a list of InputBatch objects\n    """"""\n\n    features = []\n    for (ex_index, example) in enumerate(examples):\n        tokens_a = tokenizer.tokenize(example.text_a)\n\n        tokens_b = None\n        if example.text_b:\n            tokens_b = tokenizer.tokenize(example.text_b)\n            # Modifies `tokens_a` and `tokens_b` in place so that the total\n            # length is less than the specified length.\n            # Account for [CLS], [SEP], [SEP] with ""- 3""\n            _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n        else:\n            # Account for [CLS] and [SEP] with ""- 2""\n            if len(tokens_a) > max_seq_length - 2:\n                tokens_a = tokens_a[:(max_seq_length - 2)]\n\n        # The convention in BERT is:\n        # (a) For sequence pairs:\n        #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n        #  type_ids: 0   0  0    0    0     0       0 0    1  1  1  1   1 1\n        # (b) For single sequences:\n        #  tokens:   [CLS] the dog is hairy . [SEP]\n        #  type_ids: 0   0   0   0  0     0 0\n        #\n        # Where ""type_ids"" are used to indicate whether this is the first\n        # sequence or the second sequence. The embedding vectors for `type=0` and\n        # `type=1` were learned during pre-training and are added to the wordpiece\n        # embedding vector (and position vector). This is not *strictly* necessary\n        # since the [SEP] token unambigiously separates the sequences, but it makes\n        # it easier for the model to learn the concept of sequences.\n        #\n        # For classification tasks, the first vector (corresponding to [CLS]) is\n        # used as as the ""sentence vector"". Note that this only makes sense because\n        # the entire model is fine-tuned.\n        tokens = [""[CLS]""] + tokens_a + [""[SEP]""]\n        segment_ids = [0] * len(tokens)\n\n        if tokens_b:\n            tokens += tokens_b + [""[SEP]""]\n            segment_ids += [1] * (len(tokens_b) + 1)\n\n        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n\n        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n        # tokens are attended to.\n        input_mask = [1] * len(input_ids)\n\n        # Zero-pad up to the sequence length.\n        padding = [0] * (max_seq_length - len(input_ids))\n        input_ids += padding\n        input_mask += padding\n        segment_ids += padding\n\n        assert len(input_ids) == max_seq_length\n        assert len(input_mask) == max_seq_length\n        assert len(segment_ids) == max_seq_length\n\n        label_id = [float(x) for x in example.label]\n\n        if print_examples and ex_index < 5:\n            print(""tokens: %s"" % "" "".join([str(x) for x in tokens]))\n            print(""input_ids: %s"" % "" "".join([str(x) for x in input_ids]))\n            print(""input_mask: %s"" % "" "".join([str(x) for x in input_mask]))\n            print(""segment_ids: %s"" % "" "".join([str(x) for x in segment_ids]))\n            print(""label: %s"" % example.label)\n\n        features.append(InputFeatures(input_ids=input_ids,\n                                      input_mask=input_mask,\n                                      segment_ids=segment_ids,\n                                      label_id=label_id))\n    return features\n\n\ndef convert_examples_to_hierarchical_features(examples, max_seq_length, tokenizer, print_examples=False):\n    """"""\n    Loads a data file into a list of InputBatch objects\n    :param examples:\n    :param max_seq_length:\n    :param tokenizer:\n    :param print_examples:\n    :return: a list of InputBatch objects\n    """"""\n\n    features = []\n    for (ex_index, example) in enumerate(examples):\n        tokens_a = [tokenizer.tokenize(line) for line in sent_tokenize(example.text_a)]\n        tokens_b = None\n\n        if example.text_b:\n            tokens_b = [tokenizer.tokenize(line) for line in sent_tokenize(example.text_b)]\n            # Modifies `tokens_a` and `tokens_b` in place so that the total length is less than the specified length\n            # Account for [CLS], [SEP], [SEP]\n            _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n        else:\n            # Account for [CLS] and [SEP]\n            for i0 in range(len(tokens_a)):\n                if len(tokens_a[i0]) > max_seq_length - 2:\n                    tokens_a[i0] = tokens_a[i0][:(max_seq_length - 2)]\n\n        tokens = [[""[CLS]""] + line + [""[SEP]""] for line in tokens_a]\n        segment_ids = [[0] * len(line) for line in tokens]\n\n        if tokens_b:\n            tokens += tokens_b + [""[SEP]""]\n            segment_ids += [1] * (len(tokens_b) + 1)\n\n        input_ids = list()\n        for line in tokens:\n            input_ids.append(tokenizer.convert_tokens_to_ids(line))\n\n        # Input mask has 1 for real tokens and 0 for padding tokens\n        input_mask = [[1] * len(line_ids) for line_ids in input_ids]\n\n        # Zero-pad up to the sequence length.\n        padding = [[0] * (max_seq_length - len(line_ids)) for line_ids in input_ids]\n        for i0 in range(len(input_ids)):\n            input_ids[i0] += padding[i0]\n            input_mask[i0] += padding[i0]\n            segment_ids[i0] += padding[i0]\n\n        label_id = [float(x) for x in example.label]\n\n        if print_examples and ex_index < 5:\n            print(""tokens: %s"" % "" "".join([str(x) for x in tokens]))\n            print(""input_ids: %s"" % "" "".join([str(x) for x in input_ids]))\n            print(""input_mask: %s"" % "" "".join([str(x) for x in input_mask]))\n            print(""segment_ids: %s"" % "" "".join([str(x) for x in segment_ids]))\n            print(""label: %s"" % example.label)\n\n        features.append(InputFeatures(input_ids=input_ids,\n                                      input_mask=input_mask,\n                                      segment_ids=segment_ids,\n                                      label_id=label_id))\n    return features\n\n\ndef _truncate_seq_pair(tokens_a, tokens_b, max_length):\n    """"""\n    Truncates a sequence pair in place to the maximum length\n    :param tokens_a:\n    :param tokens_b:\n    :param max_length:\n    :return:\n    """"""\n\n    # This is a simple heuristic which will always truncate the longer sequence\n    # one token at a time. This makes more sense than truncating an equal percent\n    # of tokens from each, since if one sequence is very short then each token\n    # that\'s truncated likely contains more information than a longer sequence.\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_length:\n            break\n        if len(tokens_a) > len(tokens_b):\n            tokens_a.pop()\n        else:\n            tokens_b.pop()\n'"
datasets/bert_processors/agnews_processor.py,0,"b'import os\n\nfrom datasets.bert_processors.abstract_processor import BertProcessor, InputExample\n\n\nclass AGNewsProcessor(BertProcessor):\n    NAME = \'AGNews\'\n    NUM_CLASSES = 4\n    IS_MULTILABEL = False\n\n    def get_train_examples(self, data_dir):\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, \'AGNews\', \'train.tsv\')), \'train\')\n\n    def get_dev_examples(self, data_dir):\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, \'AGNews\', \'dev.tsv\')), \'dev\')\n\n    def get_test_examples(self, data_dir):\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, \'AGNews\', \'test.tsv\')), \'test\')\n\n    def _create_examples(self, lines, set_type):\n        """"""Creates examples for the training and dev sets.""""""\n        examples = []\n        for (i, line) in enumerate(lines):\n            if i == 0:\n                continue\n            guid = ""%s-%s"" % (set_type, i)\n            text_a = line[1]\n            label = line[0]\n            examples.append(\n                InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n        return examples\n'"
datasets/bert_processors/imdb_processor.py,0,"b'import os\n\nfrom datasets.bert_processors.abstract_processor import BertProcessor, InputExample\n\n\nclass IMDBProcessor(BertProcessor):\n    NAME = \'IMDB\'\n    NUM_CLASSES = 10\n    IS_MULTILABEL = False\n\n    def get_train_examples(self, data_dir):\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, \'IMDB\', \'train.tsv\')), \'train\')\n\n    def get_dev_examples(self, data_dir):\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, \'IMDB\', \'dev.tsv\')), \'dev\')\n\n    def get_test_examples(self, data_dir):\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, \'IMDB\', \'test.tsv\')), \'test\')\n\n    def _create_examples(self, lines, set_type):\n        """"""Creates examples for the training and dev sets.""""""\n        examples = []\n        for (i, line) in enumerate(lines):\n            if i == 0:\n                continue\n            guid = ""%s-%s"" % (set_type, i)\n            text_a = line[1]\n            label = line[0]\n            examples.append(\n                InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n        return examples'"
datasets/bert_processors/reuters_processor.py,0,"b""import os\n\nfrom datasets.bert_processors.abstract_processor import BertProcessor, InputExample\n\n\nclass ReutersProcessor(BertProcessor):\n    NAME = 'Reuters'\n    NUM_CLASSES = 90\n    IS_MULTILABEL = True\n    \n    def get_train_examples(self, data_dir):\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, 'Reuters', 'train.tsv')), 'train')\n\n    def get_dev_examples(self, data_dir):\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, 'Reuters', 'dev.tsv')), 'dev')\n\n    def get_test_examples(self, data_dir):\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, 'Reuters', 'test.tsv')), 'test')\n\n    def _create_examples(self, lines, set_type):\n        examples = []\n        for (i, line) in enumerate(lines):\n            if i == 0:\n                continue\n            guid = '%s-%s' % (set_type, i)\n            text_a = line[1]\n            label = line[0]\n            examples.append(\n                InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n        return examples"""
datasets/bert_processors/robust45_processor.py,0,"b'import os\n\nfrom datasets.bert_processors.abstract_processor import BertProcessor, InputExample, InputFeatures\n\n\nclass RelevanceFeatures(InputFeatures):\n    """"""A single set of features for relevance tasks.""""""\n\n    def __init__(self, input_ids, input_mask, segment_ids, label_id, guid):\n        super().__init__(input_ids, input_mask, segment_ids, label_id)\n        self.guid = guid\n\n\nclass Robust45Processor(BertProcessor):\n    NAME = \'Robust45\'\n    NUM_CLASSES = 2\n    TOPICS = [\'307\', \'310\', \'321\', \'325\', \'330\', \'336\', \'341\', \'344\', \'345\', \'347\', \'350\', \'353\', \'354\', \'355\', \'356\',\n              \'362\', \'363\', \'367\', \'372\', \'375\', \'378\', \'379\', \'389\', \'393\', \'394\', \'397\', \'399\', \'400\', \'404\', \'408\',\n              \'414\', \'416\', \'419\', \'422\', \'423\', \'426\', \'427\', \'433\', \'435\', \'436\', \'439\', \'442\', \'443\', \'445\', \'614\',\n              \'620\', \'626\', \'646\', \'677\', \'690\']\n\n    def get_train_examples(self, data_dir, **kwargs):\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, \'TREC\', \'robust45_aug_train_%s.tsv\' % kwargs[\'topic\'])), \'train\')\n\n    def get_dev_examples(self, data_dir, **kwargs):\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, \'TREC\', \'robust45_dev_%s.tsv\' % kwargs[\'topic\'])), \'dev\')\n\n    def get_test_examples(self, data_dir, **kwargs):\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, \'TREC\', \'core17_10k_%s.tsv\' % kwargs[\'topic\'])), \'test\')\n\n    @staticmethod\n    def _create_examples(lines, split):\n        """"""Creates examples for the training and dev sets.""""""\n        examples = []\n        for (i, line) in enumerate(lines):\n            if i == 0:\n                continue\n            text_a = line[2]\n            guid = line[1]\n            label = line[0]\n            examples.append(\n                InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n        return examples\n\n\ndef convert_examples_to_features(examples, max_seq_length, tokenizer):\n    """"""\n    Loads a data file into a list of InputBatch objects\n    :param examples:\n    :param max_seq_length:\n    :param tokenizer:\n    :return: a list of InputBatch objects\n    """"""\n\n    features = []\n    for (ex_index, example) in enumerate(examples):\n        tokens_a = tokenizer.tokenize(example.text_a)\n\n        # Account for [CLS] and [SEP] with ""- 2""\n        if len(tokens_a) > max_seq_length - 2:\n            tokens_a = tokens_a[:(max_seq_length - 2)]\n\n        tokens = [""[CLS]""] + tokens_a + [""[SEP]""]\n        segment_ids = [0] * len(tokens)\n\n        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n\n        # The mask has 1 for real tokens and 0 for padding tokens\n        input_mask = [1] * len(input_ids)\n\n        # Zero-pad up to the sequence length\n        padding = [0] * (max_seq_length - len(input_ids))\n        input_ids += padding\n        input_mask += padding\n        segment_ids += padding\n\n        try:\n            docid = int(example.guid)\n        except ValueError:\n            # print(""Error converting docid to integer:"", string)\n            docid = 0\n\n        features.append(RelevanceFeatures(input_ids=input_ids,\n                                          input_mask=input_mask,\n                                          segment_ids=segment_ids,\n                                          label_id=0 if example.label == \'01\' else 1,\n                                          guid=docid))\n    return features\n'"
datasets/bert_processors/sogou_processor.py,0,"b'import os\n\nfrom datasets.bert_processors.abstract_processor import BertProcessor, InputExample\n\n\nclass SogouProcessor(BertProcessor):\n    NAME = \'Sogou\'\n    NUM_CLASSES = 5\n    IS_MULTILABEL = False\n\n    def get_train_examples(self, data_dir):\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, \'Sogou\', \'train.tsv\')), \'train\')\n\n    def get_dev_examples(self, data_dir):\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, \'Sogou\', \'dev.tsv\')), \'dev\')\n\n    def get_test_examples(self, data_dir):\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, \'Sogou\', \'test.tsv\')), \'test\')\n\n    def _create_examples(self, lines, set_type):\n        """"""Creates examples for the training and dev sets.""""""\n        examples = []\n        for (i, line) in enumerate(lines):\n            if i == 0:\n                continue\n            guid = ""%s-%s"" % (set_type, i)\n            text_a = line[1]\n            label = line[0]\n            examples.append(\n                InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n        return examples\n'"
datasets/bert_processors/sst_processor.py,0,"b'import os\n\nfrom datasets.bert_processors.abstract_processor import BertProcessor, InputExample\n\n\nclass SST2Processor(BertProcessor):\n    NAME = \'SST-2\'\n    NUM_CLASSES = 2\n    IS_MULTILABEL = False\n\n    def get_train_examples(self, data_dir):\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, \'SST-2\', \'train.tsv\')), \'train\')\n\n    def get_dev_examples(self, data_dir):\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, \'SST-2\', \'dev.tsv\')), \'dev\')\n\n    def get_test_examples(self, data_dir):\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, \'SST-2\', \'test.tsv\')), \'test\')\n\n    @staticmethod\n    def _create_examples(lines, set_type):\n        """"""\n        Creates examples for the training and dev sets\n        :param lines:\n        :param set_type:\n        :return:\n        """"""\n        examples = []\n        for (i, line) in enumerate(lines):\n            if i == 0:\n                continue\n            guid = \'%s-%s\' % (set_type, i)\n            label = line[0]\n            text = line[1]\n            examples.append(InputExample(guid=guid, text_a=text, text_b=None, label=label))\n        return examples\n'"
datasets/bert_processors/yelp2014_processor.py,0,"b'import os\n\nfrom datasets.bert_processors.abstract_processor import BertProcessor, InputExample\n\n\nclass Yelp2014Processor(BertProcessor):\n    NAME = \'Yelp2014\'\n    NUM_CLASSES = 5\n    IS_MULTILABEL = False\n\n    def get_train_examples(self, data_dir):\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, \'Yelp2014\', \'train.tsv\')), \'train\')\n\n    def get_dev_examples(self, data_dir):\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, \'Yelp2014\', \'dev.tsv\')), \'dev\')\n\n    def get_test_examples(self, data_dir):\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, \'Yelp2014\', \'test.tsv\')), \'test\')\n\n    def _create_examples(self, lines, set_type):\n        """"""Creates examples for the training and dev sets.""""""\n        examples = []\n        for (i, line) in enumerate(lines):\n            if i == 0:\n                continue\n            guid = ""%s-%s"" % (set_type, i)\n            text_a = line[1]\n            label = line[0]\n            examples.append(\n                InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n        return examples\n'"
datasets/bow_processors/__init__.py,0,b''
datasets/bow_processors/aapd_processor.py,0,"b""import os\n\nfrom datasets.bow_processors.abstract_processor import BagOfWordsProcessor, InputExample\n\n\nclass AAPDProcessor(BagOfWordsProcessor):\n    NAME = 'AAPD'\n    NUM_CLASSES = 54\n    VOCAB_SIZE = 66192\n    IS_MULTILABEL = True\n\n    def get_train_examples(self, data_dir):\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir,'AAPD', 'train.tsv')), 'train')\n\n    def get_dev_examples(self, data_dir):\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, 'AAPD', 'dev.tsv')), 'dev')\n\n    def get_test_examples(self, data_dir):\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, 'AAPD', 'test.tsv')), 'test')\n\n    def _create_examples(self, lines, set_type):\n        examples = []\n        for (i, line) in enumerate(lines):\n            guid = '%s-%s' % (set_type, i)\n            examples.append(InputExample(guid=guid, text=line[1], label=line[0]))\n        return examples"""
datasets/bow_processors/abstract_processor.py,3,"b'import csv\nimport sys\n\nimport torch\nfrom torch import tensor\nfrom torch.utils.data import Dataset\n\n\nclass InputExample(object):\n    def __init__(self, guid, text, label=None):\n        self.guid = guid\n        self.text = text\n        self.label = label\n\n\nclass BagOfWordsProcessor(object):\n    def get_train_examples(self, data_dir):\n        """"""\n        Gets a collection of InputExamples for the train set\n        :param data_dir:\n        :return:\n        """"""\n        raise NotImplementedError()\n\n    def get_dev_examples(self, data_dir):\n        """"""\n        Gets a collection of InputExamples for the dev set\n        :param data_dir:\n        :return:\n        """"""\n        raise NotImplementedError()\n\n    def get_test_examples(self, data_dir):\n        """"""\n        Gets a collection of InputExamples for the test set\n        :param data_dir:\n        :return:\n        """"""\n        raise NotImplementedError()\n\n    @classmethod\n    def _read_tsv(cls, input_file, quotechar=None):\n        """"""\n        Reads a Tab Separated Values (TSV) file\n        :param input_file:\n        :param quotechar:\n        :return:\n        """"""\n        with open(input_file, ""r"") as f:\n            reader = csv.reader(f, delimiter=""\\t"", quotechar=quotechar)\n            lines = []\n            for line in reader:\n                if sys.version_info[0] == 2:\n                    line = list(str(cell, \'utf-8\') for cell in line)\n                lines.append(line)\n            return lines\n\n\nclass StreamingSparseDataset(Dataset):\n    def __init__(self, features, labels):\n        self.features = features\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        feature_tensor = tensor(self.features.getrow(idx).toarray(), dtype=torch.float)\n        label_tensor = tensor(self.labels[idx], dtype=torch.long)\n        return feature_tensor, label_tensor\n\n    def __len__(self):\n        return len(self.labels)\n'"
datasets/bow_processors/imdb_processor.py,0,"b""import os\n\nfrom datasets.bow_processors.abstract_processor import BagOfWordsProcessor, InputExample\n\n\nclass IMDBProcessor(BagOfWordsProcessor):\n    NAME = 'IMDB'\n    NUM_CLASSES = 10\n    VOCAB_SIZE = 395495\n    IS_MULTILABEL = False\n\n    def get_train_examples(self, data_dir):\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, 'IMDB', 'train.tsv')), 'train')\n\n    def get_dev_examples(self, data_dir):\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, 'IMDB', 'dev.tsv')), 'dev')\n\n    def get_test_examples(self, data_dir):\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, 'IMDB', 'test.tsv')), 'test')\n\n    def _create_examples(self, lines, set_type):\n        examples = []\n        for (i, line) in enumerate(lines):\n            guid = '%s-%s' % (set_type, i)\n            examples.append(InputExample(guid=guid, text=line[1], label=line[0]))\n        return examples"""
datasets/bow_processors/reuters_processor.py,0,"b""import os\n\nfrom datasets.bow_processors.abstract_processor import BagOfWordsProcessor, InputExample\n\n\nclass ReutersProcessor(BagOfWordsProcessor):\n    NAME = 'Reuters'\n    NUM_CLASSES = 90\n    VOCAB_SIZE = 36308\n    IS_MULTILABEL = True\n\n    def get_train_examples(self, data_dir):\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, 'Reuters', 'train.tsv')), 'train')\n\n    def get_dev_examples(self, data_dir):\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, 'Reuters', 'dev.tsv')), 'dev')\n\n    def get_test_examples(self, data_dir):\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, 'Reuters', 'test.tsv')), 'test')\n\n    def _create_examples(self, lines, set_type):\n        examples = []\n        for (i, line) in enumerate(lines):\n            guid = '%s-%s' % (set_type, i)\n            examples.append(InputExample(guid=guid, text=line[1], label=line[0]))\n        return examples"""
datasets/bow_processors/yelp2014_processor.py,0,"b""import os\n\nfrom datasets.bow_processors.abstract_processor import BagOfWordsProcessor, InputExample\n\n\nclass Yelp2014Processor(BagOfWordsProcessor):\n    NAME = 'Yelp2014'\n    NUM_CLASSES = 5\n    VOCAB_SIZE = 490166\n    IS_MULTILABEL = False\n\n    def get_train_examples(self, data_dir):\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, 'Yelp2014', 'train.tsv')), 'train')\n\n    def get_dev_examples(self, data_dir):\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, 'Yelp2014', 'dev.tsv')), 'dev')\n\n    def get_test_examples(self, data_dir):\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, 'Yelp2014', 'test.tsv')), 'test')\n\n    def _create_examples(self, lines, set_type):\n        examples = []\n        for (i, line) in enumerate(lines):\n            guid = '%s-%s' % (set_type, i)\n            examples.append(InputExample(guid=guid, text=line[1], label=line[0]))\n        return examples"""
models/bert/__init__.py,0,b''
models/bert/__main__.py,7,"b'import random\nimport time\n\nimport numpy as np\nimport torch\nfrom transformers import AdamW, BertForSequenceClassification, BertTokenizer, WarmupLinearSchedule\n\nfrom common.constants import *\nfrom common.evaluators.bert_evaluator import BertEvaluator\nfrom common.trainers.bert_trainer import BertTrainer\nfrom datasets.bert_processors.aapd_processor import AAPDProcessor\nfrom datasets.bert_processors.agnews_processor import AGNewsProcessor\nfrom datasets.bert_processors.imdb_processor import IMDBProcessor\nfrom datasets.bert_processors.reuters_processor import ReutersProcessor\nfrom datasets.bert_processors.sogou_processor import SogouProcessor\nfrom datasets.bert_processors.sst_processor import SST2Processor\nfrom datasets.bert_processors.yelp2014_processor import Yelp2014Processor\nfrom models.bert.args import get_args\n\n\ndef evaluate_split(model, processor, tokenizer, args, split=\'dev\'):\n    evaluator = BertEvaluator(model, processor, tokenizer, args, split)\n    accuracy, precision, recall, f1, avg_loss = evaluator.get_scores(silent=True)[0]\n    print(\'\\n\' + LOG_HEADER)\n    print(LOG_TEMPLATE.format(split.upper(), accuracy, precision, recall, f1, avg_loss))\n\n\nif __name__ == \'__main__\':\n    # Set default configuration in args.py\n    args = get_args()\n    device = torch.device(""cuda"" if torch.cuda.is_available() and args.cuda else ""cpu"")\n    n_gpu = torch.cuda.device_count()\n\n    print(\'Device:\', str(device).upper())\n    print(\'Number of GPUs:\', n_gpu)\n    print(\'FP16:\', args.fp16)\n\n    # Set random seed for reproducibility\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    if n_gpu > 0:\n        torch.cuda.manual_seed_all(args.seed)\n\n    dataset_map = {\n        \'SST-2\': SST2Processor,\n        \'Reuters\': ReutersProcessor,\n        \'IMDB\': IMDBProcessor,\n        \'AAPD\': AAPDProcessor,\n        \'AGNews\': AGNewsProcessor,\n        \'Yelp2014\': Yelp2014Processor,\n        \'Sogou\': SogouProcessor\n    }\n\n    if args.gradient_accumulation_steps < 1:\n        raise ValueError(""Invalid gradient_accumulation_steps parameter: {}, should be >= 1"".format(\n                            args.gradient_accumulation_steps))\n\n    if args.dataset not in dataset_map:\n        raise ValueError(\'Unrecognized dataset\')\n\n    args.batch_size = args.batch_size // args.gradient_accumulation_steps\n    args.device = device\n    args.n_gpu = n_gpu\n    args.num_labels = dataset_map[args.dataset].NUM_CLASSES\n    args.is_multilabel = dataset_map[args.dataset].IS_MULTILABEL\n\n    if not args.trained_model:\n        save_path = os.path.join(args.save_path, dataset_map[args.dataset].NAME)\n        os.makedirs(save_path, exist_ok=True)\n\n    args.is_hierarchical = False\n    processor = dataset_map[args.dataset]()\n    pretrained_vocab_path = PRETRAINED_VOCAB_ARCHIVE_MAP[args.model]\n    tokenizer = BertTokenizer.from_pretrained(pretrained_vocab_path)\n\n    train_examples = None\n    num_train_optimization_steps = None\n    if not args.trained_model:\n        train_examples = processor.get_train_examples(args.data_dir)\n        num_train_optimization_steps = int(\n            len(train_examples) / args.batch_size / args.gradient_accumulation_steps) * args.epochs\n\n    pretrained_model_path = args.model if os.path.isfile(args.model) else PRETRAINED_MODEL_ARCHIVE_MAP[args.model]\n    model = BertForSequenceClassification.from_pretrained(pretrained_model_path, num_labels=args.num_labels)\n\n    if args.fp16:\n        model.half()\n    model.to(device)\n\n    if n_gpu > 1:\n        model = torch.nn.DataParallel(model)\n\n    # Prepare optimizer\n    param_optimizer = list(model.named_parameters())\n    no_decay = [\'bias\', \'LayerNorm.bias\', \'LayerNorm.weight\']\n    optimizer_grouped_parameters = [\n        {\'params\': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], \'weight_decay\': 0.01},\n        {\'params\': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], \'weight_decay\': 0.0}]\n\n    if args.fp16:\n        try:\n            from apex.optimizers import FP16_Optimizer\n            from apex.optimizers import FusedAdam\n        except ImportError:\n            raise ImportError(""Please install NVIDIA Apex for FP16 training"")\n\n        optimizer = FusedAdam(optimizer_grouped_parameters,\n                              lr=args.lr,\n                              bias_correction=False,\n                              max_grad_norm=1.0)\n        if args.loss_scale == 0:\n            optimizer = FP16_Optimizer(optimizer, dynamic_loss_scale=True)\n        else:\n            optimizer = FP16_Optimizer(optimizer, static_loss_scale=args.loss_scale)\n\n    else:\n        optimizer = AdamW(optimizer_grouped_parameters, lr=args.lr, weight_decay=0.01, correct_bias=False)\n        scheduler = WarmupLinearSchedule(optimizer, t_total=num_train_optimization_steps,\n                                         warmup_steps=args.warmup_proportion * num_train_optimization_steps)\n\n    trainer = BertTrainer(model, optimizer, processor, scheduler, tokenizer, args)\n\n    if not args.trained_model:\n        trainer.train()\n        model = torch.load(trainer.snapshot_path)\n\n    else:\n        model = BertForSequenceClassification.from_pretrained(pretrained_model_path, num_labels=args.num_labels)\n        model_ = torch.load(args.trained_model, map_location=lambda storage, loc: storage)\n        state = {}\n        for key in model_.state_dict().keys():\n            new_key = key.replace(""module."", """")\n            state[new_key] = model_.state_dict()[key]\n        model.load_state_dict(state)\n        model = model.to(device)\n\n    evaluate_split(model, processor, tokenizer, args, split=\'dev\')\n    evaluate_split(model, processor, tokenizer, args, split=\'test\')\n\n'"
models/bert/args.py,0,"b""import os\n\nimport models.args\n\n\ndef get_args():\n    parser = models.args.get_args()\n\n    parser.add_argument('--model', default=None, type=str, required=True)\n    parser.add_argument('--dataset', type=str, default='SST-2', choices=['SST-2', 'AGNews', 'Reuters', 'AAPD', 'IMDB', 'Yelp2014'])\n    parser.add_argument('--save-path', type=str, default=os.path.join('model_checkpoints', 'bert'))\n    parser.add_argument('--cache-dir', default='cache', type=str)\n    parser.add_argument('--trained-model', default=None, type=str)\n    parser.add_argument('--fp16', action='store_true', help='use 16-bit floating point precision')\n\n    parser.add_argument('--max-seq-length',\n                        default=128,\n                        type=int,\n                        help='The maximum total input sequence length after WordPiece tokenization. \\n'\n                             'Sequences longer than this will be truncated, and sequences shorter \\n'\n                             'than this will be padded.')\n\n    parser.add_argument('--warmup-proportion',\n                        default=0.1,\n                        type=float,\n                        help='Proportion of training to perform linear learning rate warmup for')\n\n    parser.add_argument('--gradient-accumulation-steps',\n                        type=int,\n                        default=1,\n                        help='Number of updates steps to accumulate before performing a backward/update pass')\n\n    parser.add_argument('--loss-scale',\n                        type=float,\n                        default=0,\n                        help='Loss scaling to improve fp16 numeric stability. Only used when fp16 set to True.\\n'\n                             '0 (default value): dynamic loss scaling.\\n'\n                             'Positive power of 2: static loss scaling value.\\n')\n\n    args = parser.parse_args()\n    return args\n"""
models/char_cnn/__init__.py,0,b''
models/char_cnn/__main__.py,13,"b'import logging\nimport os\nimport random\nfrom copy import deepcopy\n\nimport numpy as np\nimport torch\n\nfrom common.evaluate import EvaluatorFactory\nfrom common.train import TrainerFactory\nfrom datasets.aapd import AAPDCharQuantized as AAPD\nfrom datasets.imdb import IMDBCharQuantized as IMDB\nfrom datasets.reuters import ReutersCharQuantized as Reuters\nfrom datasets.yelp2014 import Yelp2014CharQuantized as Yelp2014\nfrom models.char_cnn.args import get_args\nfrom models.char_cnn.model import CharCNN\n\n\nclass UnknownWordVecCache(object):\n    """"""\n    Caches the first randomly generated word vector for a certain size to make it is reused.\n    """"""\n    cache = {}\n\n    @classmethod\n    def unk(cls, tensor):\n        size_tup = tuple(tensor.size())\n        if size_tup not in cls.cache:\n            cls.cache[size_tup] = torch.Tensor(tensor.size())\n            cls.cache[size_tup].uniform_(-0.25, 0.25)\n        return cls.cache[size_tup]\n\n\ndef get_logger():\n    logger = logging.getLogger(__name__)\n    logger.setLevel(logging.INFO)\n\n    ch = logging.StreamHandler()\n    ch.setLevel(logging.DEBUG)\n    formatter = logging.Formatter(\'%(levelname)s - %(message)s\')\n    ch.setFormatter(formatter)\n    logger.addHandler(ch)\n\n    return logger\n\n\ndef evaluate_dataset(split_name, dataset_cls, model, embedding, loader, batch_size, device, is_multilabel):\n    saved_model_evaluator = EvaluatorFactory.get_evaluator(dataset_cls, model, embedding, loader, batch_size, device)\n    if hasattr(saved_model_evaluator, \'is_multilabel\'):\n        saved_model_evaluator.is_multilabel = is_multilabel\n    if hasattr(saved_model_evaluator, \'ignore_lengths\'):\n        saved_model_evaluator.ignore_lengths = True\n\n    scores, metric_names = saved_model_evaluator.get_scores()\n    print(\'Evaluation metrics for\', split_name)\n    print(metric_names)\n    print(scores)\n\n\nif __name__ == \'__main__\':\n    # Set default configuration in args.py\n    args = get_args()\n    logger = get_logger()\n\n    # Set random seed for reproducibility\n    torch.manual_seed(args.seed)\n    torch.backends.cudnn.deterministic = True\n    np.random.seed(args.seed)\n    random.seed(args.seed)\n\n    if not args.cuda:\n        args.gpu = -1\n    if torch.cuda.is_available() and args.cuda:\n        print(\'Note: You are using GPU for training\')\n        torch.cuda.set_device(args.gpu)\n        torch.cuda.manual_seed(args.seed)\n    if torch.cuda.is_available() and not args.cuda:\n        print(\'Warning: Using CPU for training\')\n\n    dataset_map = {\n        \'Reuters\': Reuters,\n        \'AAPD\': AAPD,\n        \'IMDB\': IMDB,\n        \'Yelp2014\': Yelp2014\n    }\n\n    if args.dataset not in dataset_map:\n        raise ValueError(\'Unrecognized dataset\')\n\n    else:\n        dataset_class = dataset_map[args.dataset]\n        train_iter, dev_iter, test_iter = dataset_class.iters(args.data_dir,\n                                                              args.word_vectors_file,\n                                                              args.word_vectors_dir,\n                                                              batch_size=args.batch_size,\n                                                              device=args.gpu,\n                                                              unk_init=UnknownWordVecCache.unk)\n\n    config = deepcopy(args)\n    config.dataset = train_iter.dataset\n    config.target_class = train_iter.dataset.NUM_CLASSES\n\n    print(\'Dataset:\', args.dataset)\n    print(\'No. of target classes:\', train_iter.dataset.NUM_CLASSES)\n    print(\'No. of train instances\', len(train_iter.dataset))\n    print(\'No. of dev instances\', len(dev_iter.dataset))\n    print(\'No. of test instances\', len(test_iter.dataset))\n\n    if args.resume_snapshot:\n        if args.cuda:\n            model = torch.load(args.resume_snapshot, map_location=lambda storage, location: storage.cuda(args.gpu))\n        else:\n            model = torch.load(args.resume_snapshot, map_location=lambda storage, location: storage)\n    else:\n        model = CharCNN(config)\n        if args.cuda:\n            model.cuda()\n\n    if not args.trained_model:\n        save_path = os.path.join(args.save_path, dataset_map[args.dataset].NAME)\n        os.makedirs(save_path, exist_ok=True)\n\n    parameter = filter(lambda p: p.requires_grad, model.parameters())\n    optimizer = torch.optim.Adam(parameter, lr=args.lr, weight_decay=args.weight_decay)\n\n    train_evaluator = EvaluatorFactory.get_evaluator(dataset_class, model, None, train_iter, args.batch_size, args.gpu)\n    test_evaluator = EvaluatorFactory.get_evaluator(dataset_class, model, None, test_iter, args.batch_size, args.gpu)\n    dev_evaluator = EvaluatorFactory.get_evaluator(dataset_class, model, None, dev_iter, args.batch_size, args.gpu)\n\n    if hasattr(train_evaluator, \'is_multilabel\'):\n        train_evaluator.is_multilabel = dataset_class.IS_MULTILABEL\n    if hasattr(dev_evaluator, \'is_multilabel\'):\n        dev_evaluator.is_multilabel = dataset_class.IS_MULTILABEL\n    if hasattr(dev_evaluator, \'ignore_lengths\'):\n        dev_evaluator.ignore_lengths = True\n    if hasattr(test_evaluator, \'is_multilabel\'):\n        test_evaluator.is_multilabel = dataset_class.IS_MULTILABEL\n    if hasattr(test_evaluator, \'ignore_lengths\'):\n        test_evaluator.ignore_lengths = True\n\n    trainer_config = {\n        \'optimizer\': optimizer,\n        \'batch_size\': args.batch_size,\n        \'log_interval\': args.log_every,\n        \'patience\': args.patience,\n        \'model_outfile\': args.save_path,\n        \'logger\': logger,\n        \'is_multilabel\': dataset_class.IS_MULTILABEL,\n        \'ignore_lengths\': True\n    }\n\n    trainer = TrainerFactory.get_trainer(args.dataset, model, None, train_iter, trainer_config, train_evaluator, test_evaluator, dev_evaluator)\n\n    if not args.trained_model:\n        trainer.train(args.epochs)\n    else:\n        if args.cuda:\n            model = torch.load(args.trained_model, map_location=lambda storage, location: storage.cuda(args.gpu))\n        else:\n            model = torch.load(args.trained_model, map_location=lambda storage, location: storage)\n\n    # Calculate dev and test metrics\n    if hasattr(trainer, \'snapshot_path\'):\n        model = torch.load(trainer.snapshot_path)\n\n    evaluate_dataset(\'dev\', dataset_map[args.dataset], model, None, dev_iter, args.batch_size,\n                     is_multilabel=dataset_class.IS_MULTILABEL,\n                     device=args.gpu)\n    evaluate_dataset(\'test\', dataset_map[args.dataset], model, None, test_iter, args.batch_size,\n                     is_multilabel=dataset_class.IS_MULTILABEL,\n                     device=args.gpu)\n'"
models/char_cnn/args.py,0,"b""import os\n\nimport models.args\n\n\ndef get_args():\n    parser = models.args.get_args()\n\n    parser.add_argument('--dataset', type=str, default='Reuters', choices=['Reuters', 'AAPD', 'IMDB', 'Yelp2014'])\n    parser.add_argument('--num-conv-filters', type=int, default=256)\n    parser.add_argument('--num-affine-neurons', type=int, default=1024)\n    parser.add_argument('--output-channel', type=int, default=256)\n    parser.add_argument('--dropout', type=float, default=0.5)\n    parser.add_argument('--epoch-decay', type=int, default=15)\n    parser.add_argument('--weight-decay', type=float, default=0)\n\n    parser.add_argument('--word-vectors-dir', default=os.path.join(os.pardir, 'hedwig-data', 'embeddings', 'word2vec'))\n    parser.add_argument('--word-vectors-file', default='GoogleNews-vectors-negative300.txt')\n    parser.add_argument('--save-path', type=str, default=os.path.join('model_checkpoints', 'char_cnn'))\n    parser.add_argument('--resume-snapshot', type=str)\n    parser.add_argument('--trained-model', type=str)\n\n    args = parser.parse_args()\n    return args\n"""
models/char_cnn/model.py,5,"b'import torch\nimport torch.nn as nn\n\nimport torch.nn.functional as F\n\n\nclass CharCNN(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        self.is_cuda_enabled = config.cuda\n\n        num_conv_filters = config.num_conv_filters\n        output_channel = config.output_channel\n        num_affine_neurons = config.num_affine_neurons\n        target_class = config.target_class\n        input_channel = 68\n\n        self.conv1 = nn.Conv1d(input_channel, num_conv_filters, kernel_size=7)\n        self.conv2 = nn.Conv1d(num_conv_filters, num_conv_filters, kernel_size=7)\n        self.conv3 = nn.Conv1d(num_conv_filters, num_conv_filters, kernel_size=3)\n        self.conv4 = nn.Conv1d(num_conv_filters, num_conv_filters, kernel_size=3)\n        self.conv5 = nn.Conv1d(num_conv_filters, num_conv_filters, kernel_size=3)\n        self.conv6 = nn.Conv1d(num_conv_filters, output_channel, kernel_size=3)\n\n        self.dropout = nn.Dropout(config.dropout)\n        self.fc1 = nn.Linear(output_channel, num_affine_neurons)\n        self.fc2 = nn.Linear(num_affine_neurons, num_affine_neurons)\n        self.fc3 = nn.Linear(num_affine_neurons, target_class)\n\n    def forward(self, x, **kwargs):\n        if torch.cuda.is_available() and self.is_cuda_enabled:\n            x = x.transpose(1, 2).type(torch.cuda.FloatTensor)\n        else:\n            x = x.transpose(1, 2).type(torch.FloatTensor)\n\n        x = F.max_pool1d(F.relu(self.conv1(x)), 3)\n        x = F.max_pool1d(F.relu(self.conv2(x)), 3)\n        x = F.relu(self.conv3(x))\n        x = F.relu(self.conv4(x))\n        x = F.relu(self.conv5(x))\n        x = F.relu(self.conv6(x))\n\n        x = F.max_pool1d(x, x.size(2)).squeeze(2)\n        x = F.relu(self.fc1(x.view(x.size(0), -1)))\n        x = self.dropout(x)\n        x = F.relu(self.fc2(x))\n        x = self.dropout(x)\n        return self.fc3(x)\n'"
models/fasttext/__init__.py,0,b''
models/fasttext/__main__.py,15,"b'import os\nimport random\nfrom copy import deepcopy\n\nimport numpy as np\nimport torch\nimport torch.onnx\n\nfrom common.evaluate import EvaluatorFactory\nfrom common.train import TrainerFactory\nfrom datasets.aapd import AAPD\nfrom datasets.imdb import IMDB\nfrom datasets.reuters import ReutersBOW\nfrom datasets.yelp2014 import Yelp2014\nfrom models.fasttext.args import get_args\nfrom models.fasttext.model import FastText\n\n\nclass UnknownWordVecCache(object):\n    """"""\n    Caches the first randomly generated word vector for a certain size to make it is reused.\n    """"""\n    cache = {}\n\n    @classmethod\n    def unk(cls, tensor):\n        size_tup = tuple(tensor.size())\n        if size_tup not in cls.cache:\n            cls.cache[size_tup] = torch.Tensor(tensor.size())\n            cls.cache[size_tup].uniform_(-0.25, 0.25)\n        return cls.cache[size_tup]\n\n\ndef evaluate_dataset(split_name, dataset_cls, model, embedding, loader, batch_size, device, is_multilabel):\n    saved_model_evaluator = EvaluatorFactory.get_evaluator(dataset_cls, model, embedding, loader, batch_size, device)\n    if hasattr(saved_model_evaluator, \'is_multilabel\'):\n        saved_model_evaluator.is_multilabel = is_multilabel\n\n    scores, metric_names = saved_model_evaluator.get_scores()\n    print(\'Evaluation metrics for\', split_name)\n    print(metric_names)\n    print(scores)\n\n\nif __name__ == \'__main__\':\n    # Set default configuration in args.py\n    args = get_args()\n\n    # Set random seed for reproducibility\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    torch.backends.cudnn.deterministic = True\n\n    if not args.cuda:\n        args.gpu = -1\n\n    if torch.cuda.is_available() and args.cuda:\n        print(\'Note: You are using GPU for training\')\n        torch.cuda.set_device(args.gpu)\n        torch.cuda.manual_seed(args.seed)\n        args.gpu = torch.device(\'cuda:%d\' % args.gpu)\n\n    if torch.cuda.is_available() and not args.cuda:\n        print(\'Warning: Using CPU for training\')\n\n    dataset_map = {\n        \'Reuters\': ReutersBOW,\n        \'AAPD\': AAPD,\n        \'IMDB\': IMDB,\n        \'Yelp2014\': Yelp2014\n    }\n\n    if args.dataset not in dataset_map:\n        raise ValueError(\'Unrecognized dataset\')\n    else:\n        dataset_class = dataset_map[args.dataset]\n        train_iter, dev_iter, test_iter = dataset_map[args.dataset].iters(args.data_dir, args.word_vectors_file,\n                                                                          args.word_vectors_dir,\n                                                                          batch_size=args.batch_size, device=args.gpu,\n                                                                          unk_init=UnknownWordVecCache.unk)\n\n    config = deepcopy(args)\n    config.dataset = train_iter.dataset\n    config.target_class = train_iter.dataset.NUM_CLASSES\n    config.words_num = len(train_iter.dataset.TEXT_FIELD.vocab)\n\n    print(\'Dataset:\', args.dataset)\n    print(\'No. of target classes:\', train_iter.dataset.NUM_CLASSES)\n    print(\'No. of train instances\', len(train_iter.dataset))\n    print(\'No. of dev instances\', len(dev_iter.dataset))\n    print(\'No. of test instances\', len(test_iter.dataset))\n\n    if args.resume_snapshot:\n        if args.cuda:\n            model = torch.load(args.resume_snapshot, map_location=lambda storage, location: storage.cuda(args.gpu))\n        else:\n            model = torch.load(args.resume_snapshot, map_location=lambda storage, location: storage)\n    else:\n        model = FastText(config)\n        if args.cuda:\n            model.cuda()\n\n    if not args.trained_model:\n        save_path = os.path.join(args.save_path, dataset_map[args.dataset].NAME)\n        os.makedirs(save_path, exist_ok=True)\n\n    parameter = filter(lambda p: p.requires_grad, model.parameters())\n    optimizer = torch.optim.Adam(parameter, lr=args.lr, weight_decay=args.weight_decay)\n\n    train_evaluator = EvaluatorFactory.get_evaluator(dataset_map[args.dataset], model, None, train_iter, args.batch_size, args.gpu)\n    test_evaluator = EvaluatorFactory.get_evaluator(dataset_map[args.dataset], model, None, test_iter, args.batch_size, args.gpu)\n    dev_evaluator = EvaluatorFactory.get_evaluator(dataset_map[args.dataset], model, None, dev_iter, args.batch_size, args.gpu)\n\n    if hasattr(train_evaluator, \'is_multilabel\'):\n        train_evaluator.is_multilabel = dataset_class.IS_MULTILABEL\n    if hasattr(test_evaluator, \'is_multilabel\'):\n        test_evaluator.is_multilabel = dataset_class.IS_MULTILABEL\n    if hasattr(dev_evaluator, \'is_multilabel\'):\n        dev_evaluator.is_multilabel = dataset_class.IS_MULTILABEL\n\n    trainer_config = {\n        \'optimizer\': optimizer,\n        \'batch_size\': args.batch_size,\n        \'log_interval\': args.log_every,\n        \'patience\': args.patience,\n        \'model_outfile\': args.save_path,\n        \'is_multilabel\': dataset_class.IS_MULTILABEL\n    }\n\n    trainer = TrainerFactory.get_trainer(args.dataset, model, None, train_iter, trainer_config, train_evaluator, test_evaluator, dev_evaluator)\n\n    if not args.trained_model:\n        trainer.train(args.epochs)\n    else:\n        if args.cuda:\n            model = torch.load(args.trained_model, map_location=lambda storage, location: storage.cuda(args.gpu))\n        else:\n            model = torch.load(args.trained_model, map_location=lambda storage, location: storage)\n\n    # Calculate dev and test metrics\n    if hasattr(trainer, \'snapshot_path\'):\n        model = torch.load(trainer.snapshot_path)\n\n    evaluate_dataset(\'dev\', dataset_map[args.dataset], model, None, dev_iter, args.batch_size,\n                     is_multilabel=dataset_class.IS_MULTILABEL,\n                     device=args.gpu)\n    evaluate_dataset(\'test\', dataset_map[args.dataset], model, None, test_iter, args.batch_size,\n                     is_multilabel=dataset_class.IS_MULTILABEL,\n                     device=args.gpu)\n'"
models/fasttext/args.py,0,"b""import os\n\nimport models.args\n\n\ndef get_args():\n    parser = models.args.get_args()\n\n    parser.add_argument('--dataset', type=str, default='Reuters', choices=['Reuters', 'AAPD', 'IMDB', 'Yelp2014'])\n    parser.add_argument('--mode', type=str, default='rand', choices=['rand', 'static', 'non-static'])\n    parser.add_argument('--words-dim', type=int, default=300)\n    parser.add_argument('--dropout', type=float, default=0.5)\n    parser.add_argument('--epoch-decay', type=int, default=15)\n    parser.add_argument('--weight-decay', type=float, default=0)\n\n    parser.add_argument('--word-vectors-dir', default=os.path.join(os.pardir, 'hedwig-data', 'embeddings', 'word2vec'))\n    parser.add_argument('--word-vectors-file', default='GoogleNews-vectors-negative300.txt')\n    parser.add_argument('--save-path', type=str, default=os.path.join('model_checkpoints', 'kim_cnn'))\n    parser.add_argument('--resume-snapshot', type=str)\n    parser.add_argument('--trained-model', type=str)\n\n    args = parser.parse_args()\n    return args\n"""
models/fasttext/model.py,3,"b'import torch\nimport torch.nn as nn\n\nimport torch.nn.functional as F\n\n\nclass FastText(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        dataset = config.dataset\n        target_class = config.target_class\n        words_num = config.words_num\n        words_dim = config.words_dim\n        self.mode = config.mode\n\n        if config.mode == \'rand\':\n            rand_embed_init = torch.Tensor(words_num, words_dim).uniform_(-0.25, 0.25)\n            self.embed = nn.Embedding.from_pretrained(rand_embed_init, freeze=False)\n        elif config.mode == \'static\':\n            self.static_embed = nn.Embedding.from_pretrained(dataset.TEXT_FIELD.vocab.vectors, freeze=True)\n        elif config.mode == \'non-static\':\n            self.non_static_embed = nn.Embedding.from_pretrained(dataset.TEXT_FIELD.vocab.vectors, freeze=False)\n        else:\n            print(""Unsupported Mode"")\n            exit()\n\n        self.dropout = nn.Dropout(config.dropout)\n        self.fc1 = nn.Linear(words_dim, target_class)\n\n    def forward(self, x, **kwargs):\n        if self.mode == \'rand\':\n            x = self.embed(x)  # (batch, sent_len, embed_dim)\n        elif self.mode == \'static\':\n            x = self.static_embed(x)  # (batch, sent_len, embed_dim)\n        elif self.mode == \'non-static\':\n            x = self.non_static_embed(x)  # (batch, sent_len, embed_dim)\n\n        x = F.avg_pool2d(x, (x.shape[1], 1)).squeeze(1)  # (batch, embed_dim)\n\n        logit = self.fc1(x)  # (batch, target_size)\n        return logit\n\n\n'"
models/han/__init__.py,0,b''
models/han/__main__.py,14,"b'import logging\nimport os\nimport random\nfrom copy import deepcopy\n\nimport numpy as np\nimport torch\nimport torch.onnx\n\nfrom common.evaluate import EvaluatorFactory\nfrom common.train import TrainerFactory\nfrom datasets.aapd import AAPDHierarchical as AAPD\nfrom datasets.imdb import IMDBHierarchical as IMDB\nfrom datasets.reuters import ReutersHierarchical as Reuters\nfrom datasets.yelp2014 import Yelp2014Hierarchical as Yelp2014\nfrom models.han.args import get_args\nfrom models.han.model import HAN\n\n\nclass UnknownWordVecCache(object):\n    """"""\n    Caches the first randomly generated word vector for a certain size to make it is reused.\n    """"""\n    cache = {}\n\n    @classmethod\n    def unk(cls, tensor):\n        size_tup = tuple(tensor.size())\n        if size_tup not in cls.cache:\n            cls.cache[size_tup] = torch.Tensor(tensor.size())\n            cls.cache[size_tup].uniform_(-0.25, 0.25)\n        return cls.cache[size_tup]\n\n\ndef get_logger():\n    logger = logging.getLogger(__name__)\n    logger.setLevel(logging.INFO)\n\n    ch = logging.StreamHandler()\n    ch.setLevel(logging.DEBUG)\n    formatter = logging.Formatter(\'%(levelname)s - %(message)s\')\n    ch.setFormatter(formatter)\n    logger.addHandler(ch)\n\n    return logger\n\n\ndef evaluate_dataset(split_name, dataset_cls, model, embedding, loader, batch_size, device, is_multilabel):\n    saved_model_evaluator = EvaluatorFactory.get_evaluator(dataset_cls, model, embedding, loader, batch_size, device)\n    if hasattr(saved_model_evaluator, \'is_multilabel\'):\n        saved_model_evaluator.is_multilabel = is_multilabel\n    if hasattr(saved_model_evaluator, \'ignore_lengths\'):\n        saved_model_evaluator.ignore_lengths = True\n\n    scores, metric_names = saved_model_evaluator.get_scores()\n    print(\'Evaluation metrics for\', split_name)\n    print(metric_names)\n    print(scores)\n\n\nif __name__ == \'__main__\':\n    # Set default configuration in args.py\n    args = get_args()\n    logger = get_logger()\n\n    # Set random seed for reproducibility\n    torch.manual_seed(args.seed)\n    torch.backends.cudnn.deterministic = True\n    np.random.seed(args.seed)\n    random.seed(args.seed)\n\n    if not args.cuda:\n        args.gpu = -1\n    if torch.cuda.is_available() and args.cuda:\n        print(\'Note: You are using GPU for training\')\n        torch.cuda.set_device(args.gpu)\n        torch.cuda.manual_seed(args.seed)\n    if torch.cuda.is_available() and not args.cuda:\n        print(\'Warning: Using CPU for training\')\n\n    dataset_map = {\n        \'Reuters\': Reuters,\n        \'AAPD\': AAPD,\n        \'IMDB\': IMDB,\n        \'Yelp2014\': Yelp2014\n    }\n\n    if args.dataset not in dataset_map:\n        raise ValueError(\'Unrecognized dataset\')\n\n    else:\n        dataset_class = dataset_map[args.dataset]\n        train_iter, dev_iter, test_iter = dataset_class.iters(args.data_dir,\n                                                              args.word_vectors_file,\n                                                              args.word_vectors_dir,\n                                                              batch_size=args.batch_size,\n                                                              device=args.gpu,\n                                                              unk_init=UnknownWordVecCache.unk)\n\n    config = deepcopy(args)\n    config.dataset = train_iter.dataset\n    config.target_class = train_iter.dataset.NUM_CLASSES\n    config.words_num = len(train_iter.dataset.TEXT_FIELD.vocab)\n\n    print(\'Dataset:\', args.dataset)\n    print(\'No. of target classes:\', train_iter.dataset.NUM_CLASSES)\n    print(\'No. of train instances\', len(train_iter.dataset))\n    print(\'No. of dev instances\', len(dev_iter.dataset))\n    print(\'No. of test instances\', len(test_iter.dataset))\n\n    if args.resume_snapshot:\n        if args.cuda:\n            model = torch.load(args.resume_snapshot, map_location=lambda storage, location: storage.cuda(args.gpu))\n        else:\n            model = torch.load(args.resume_snapshot, map_location=lambda storage, location: storage)\n    else:\n        model = HAN(config)\n        if args.cuda:\n            model.cuda()\n\n    if not args.trained_model:\n        save_path = os.path.join(args.save_path, dataset_map[args.dataset].NAME)\n        os.makedirs(save_path, exist_ok=True)\n\n    parameter = filter(lambda p: p.requires_grad, model.parameters())\n    optimizer = torch.optim.Adam(parameter, lr=args.lr, weight_decay=args.weight_decay)\n    \n    train_evaluator = EvaluatorFactory.get_evaluator(dataset_class, model, None, train_iter, args.batch_size, args.gpu)\n    test_evaluator = EvaluatorFactory.get_evaluator(dataset_class, model, None, test_iter, args.batch_size, args.gpu)\n    dev_evaluator = EvaluatorFactory.get_evaluator(dataset_class, model, None, dev_iter, args.batch_size, args.gpu)\n\n    if hasattr(train_evaluator, \'is_multilabel\'):\n        train_evaluator.is_multilabel = dataset_class.IS_MULTILABEL\n    if hasattr(dev_evaluator, \'is_multilabel\'):\n        dev_evaluator.is_multilabel = dataset_class.IS_MULTILABEL\n    if hasattr(dev_evaluator, \'ignore_lengths\'):\n        dev_evaluator.ignore_lengths = True\n    if hasattr(test_evaluator, \'is_multilabel\'):\n        test_evaluator.is_multilabel = dataset_class.IS_MULTILABEL\n    if hasattr(test_evaluator, \'ignore_lengths\'):\n        test_evaluator.ignore_lengths = True\n\n    trainer_config = {\n        \'optimizer\': optimizer,\n        \'batch_size\': args.batch_size,\n        \'log_interval\': args.log_every,\n        \'patience\': args.patience,\n        \'model_outfile\': args.save_path,\n        \'logger\': logger,\n        \'is_multilabel\': dataset_class.IS_MULTILABEL,\n        \'ignore_lengths\': True\n    }\n\n    trainer = TrainerFactory.get_trainer(args.dataset, model, None, train_iter, trainer_config, train_evaluator, test_evaluator, dev_evaluator)\n\n    if not args.trained_model:\n        trainer.train(args.epochs)\n    else:\n        if args.cuda:\n            model = torch.load(args.trained_model, map_location=lambda storage, location: storage.cuda(args.gpu))\n        else:\n            model = torch.load(args.trained_model, map_location=lambda storage, location: storage)\n\n    # Calculate dev and test metrics\n    if hasattr(trainer, \'snapshot_path\'):\n        model = torch.load(trainer.snapshot_path)\n\n    evaluate_dataset(\'dev\', dataset_map[args.dataset], model, None, dev_iter, args.batch_size,\n                     is_multilabel=dataset_class.IS_MULTILABEL,\n                     device=args.gpu)\n    evaluate_dataset(\'test\', dataset_map[args.dataset], model, None, test_iter, args.batch_size,\n                     is_multilabel=dataset_class.IS_MULTILABEL,\n                     device=args.gpu)\n'"
models/han/args.py,0,"b""import os\n\nimport models.args\n\n\ndef get_args():\n    parser = models.args.get_args()\n\n    parser.add_argument('--mode', type=str, default='static', choices=['rand', 'static', 'non-static'])\n    parser.add_argument('--dataset', type=str, default='Reuters', choices=['Reuters', 'AAPD', 'IMDB', 'Yelp2014'])\n    parser.add_argument('--output-channel', type=int, default=100)\n    parser.add_argument('--words-dim', type=int, default=300)\n    parser.add_argument('--embed-dim', type=int, default=300)\n    parser.add_argument('--dropout', type=float, default=0.5)\n    parser.add_argument('--epoch-decay', type=int, default=15)\n    parser.add_argument('--weight-decay', type=float, default=0)\n    parser.add_argument('--word-num-hidden', type=int, default=50)\n    parser.add_argument('--sentence-num-hidden', type=int, default=50)\n\n    parser.add_argument('--word-vectors-dir', default=os.path.join(os.pardir, 'hedwig-data', 'embeddings', 'word2vec'))\n    parser.add_argument('--word-vectors-file', default='GoogleNews-vectors-negative300.txt')\n    parser.add_argument('--save-path', type=str, default=os.path.join('model_checkpoints', 'han'))\n    parser.add_argument('--resume-snapshot', type=str)\n    parser.add_argument('--trained-model', type=str)\n\n    args = parser.parse_args()\n    return args\n"""
models/han/model.py,2,"b'import torch\nimport torch.nn as nn\n\nfrom models.han.sent_level_rnn import SentLevelRNN\nfrom models.han.word_level_rnn import WordLevelRNN\n\n\nclass HAN(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        self.mode = config.mode\n        self.word_attention_rnn = WordLevelRNN(config)\n        self.sentence_attention_rnn = SentLevelRNN(config)\n\n    def forward(self, x,  **kwargs):\n        x = x.permute(1, 2, 0) # Expected : # sentences, # words, batch size\n        num_sentences = x.size(0)\n        word_attentions = None\n        for i in range(num_sentences):\n            word_attn = self.word_attention_rnn(x[i, :, :])\n            if word_attentions is None:\n                word_attentions = word_attn\n            else:\n                word_attentions = torch.cat((word_attentions, word_attn), 0)\n        return self.sentence_attention_rnn(word_attentions)\n\n'"
models/han/sent_level_rnn.py,6,"b'import torch\nimport torch.nn as nn\n\n\nclass SentLevelRNN(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        sentence_num_hidden = config.sentence_num_hidden\n        word_num_hidden = config.word_num_hidden\n        target_class = config.target_class\n        self.sentence_context_weights = nn.Parameter(torch.rand(2 * sentence_num_hidden, 1))\n        self.sentence_context_weights.data.uniform_(-0.1, 0.1)\n        self.sentence_gru = nn.GRU(2 * word_num_hidden, sentence_num_hidden, bidirectional=True)\n        self.sentence_linear = nn.Linear(2 * sentence_num_hidden, 2 * sentence_num_hidden, bias=True)\n        self.fc = nn.Linear(2 * sentence_num_hidden , target_class)\n        self.soft_sent = nn.Softmax()\n\n    def forward(self,x):\n        sentence_h,_ = self.sentence_gru(x)\n        x = torch.tanh(self.sentence_linear(sentence_h))\n        x = torch.matmul(x, self.sentence_context_weights)\n        x = x.squeeze(dim=2)\n        x = self.soft_sent(x.transpose(1,0))\n        x = torch.mul(sentence_h.permute(2, 0, 1), x.transpose(1, 0))\n        x = torch.sum(x, dim=1).transpose(1, 0).unsqueeze(0)\n        x = self.fc(x.squeeze(0))\n        return x\n'"
models/han/word_level_rnn.py,7,"b'import torch\nimport torch.nn as nn\n\n\nclass WordLevelRNN(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        dataset = config.dataset\n        word_num_hidden = config.word_num_hidden\n        words_num = config.words_num\n        words_dim = config.words_dim\n        self.mode = config.mode\n        if self.mode == \'rand\':\n            rand_embed_init = torch.Tensor(words_num, words_dim).uniform(-0.25, 0.25)\n            self.embed = nn.Embedding.from_pretrained(rand_embed_init, freeze=False)\n        elif self.mode == \'static\':\n            self.static_embed = nn.Embedding.from_pretrained(dataset.TEXT_FIELD.vocab.vectors, freeze=True)\n        elif self.mode == \'non-static\':\n            self.non_static_embed = nn.Embedding.from_pretrained(dataset.TEXT_FIELD.vocab.vectors, freeze=False)\n        else:\n            print(""Unsupported order"")\n            exit()\n        self.word_context_weights = nn.Parameter(torch.rand(2 * word_num_hidden, 1))\n        self.GRU = nn.GRU(words_dim, word_num_hidden, bidirectional=True)\n        self.linear = nn.Linear(2 * word_num_hidden, 2 * word_num_hidden, bias=True)\n        self.word_context_weights.data.uniform_(-0.25, 0.25)\n        self.soft_word = nn.Softmax()\n\n    def forward(self, x):\n        # x expected to be of dimensions--> (num_words, batch_size)\n        if self.mode == \'rand\':\n            x = self.embed(x)\n        elif self.mode == \'static\':\n            x = self.static_embed(x)\n        elif self.mode == \'non-static\':\n            x = self.non_static_embed(x)\n        else :\n            print(""Unsupported mode"")\n            exit()\n        h, _ = self.GRU(x)\n        x = torch.tanh(self.linear(h))\n        x = torch.matmul(x, self.word_context_weights)\n        x = x.squeeze(dim=2)\n        x = self.soft_word(x.transpose(1, 0))\n        x = torch.mul(h.permute(2, 0, 1), x.transpose(1, 0))\n        x = torch.sum(x, dim=1).transpose(1, 0).unsqueeze(0)\n        return x\n'"
models/hbert/__init__.py,0,b''
models/hbert/__main__.py,7,"b'import random\nimport time\n\nimport numpy as np\nimport torch\nfrom transformers import AdamW, BertTokenizer, WarmupLinearSchedule\n\nfrom common.constants import *\nfrom common.evaluators.bert_evaluator import BertEvaluator\nfrom common.trainers.bert_trainer import BertTrainer\nfrom datasets.bert_processors.aapd_processor import AAPDProcessor\nfrom datasets.bert_processors.agnews_processor import AGNewsProcessor\nfrom datasets.bert_processors.imdb_processor import IMDBProcessor\nfrom datasets.bert_processors.reuters_processor import ReutersProcessor\nfrom datasets.bert_processors.sogou_processor import SogouProcessor\nfrom datasets.bert_processors.sst_processor import SST2Processor\nfrom datasets.bert_processors.yelp2014_processor import Yelp2014Processor\nfrom models.hbert.args import get_args\nfrom models.hbert.model import HierarchicalBert\n\n\ndef evaluate_split(model, processor, tokenizer, args, split=\'dev\'):\n    evaluator = BertEvaluator(model, processor, tokenizer, args, split)\n    accuracy, precision, recall, f1, avg_loss = evaluator.get_scores(silent=True)[0]\n    print(\'\\n\' + LOG_HEADER)\n    print(LOG_TEMPLATE.format(split.upper(), accuracy, precision, recall, f1, avg_loss))\n\n\nif __name__ == \'__main__\':\n    # Set default configuration in args.py\n    args = get_args()\n    device = torch.device(""cuda"" if torch.cuda.is_available() and args.cuda else ""cpu"")\n    n_gpu = torch.cuda.device_count()\n\n    print(\'Device:\', str(device).upper())\n    print(\'Number of GPUs:\', n_gpu)\n    print(\'FP16:\', args.fp16)\n\n    # Set random seed for reproducibility\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    if n_gpu > 0:\n        torch.cuda.manual_seed_all(args.seed)\n\n    dataset_map = {\n        \'SST-2\': SST2Processor,\n        \'Reuters\': ReutersProcessor,\n        \'IMDB\': IMDBProcessor,\n        \'AAPD\': AAPDProcessor,\n        \'AGNews\': AGNewsProcessor,\n        \'Yelp2014\': Yelp2014Processor,\n        \'Sogou\': SogouProcessor\n    }\n\n    if args.gradient_accumulation_steps < 1:\n        raise ValueError(""Invalid gradient_accumulation_steps parameter: {}, should be >= 1"".format(\n                            args.gradient_accumulation_steps))\n\n    if args.dataset not in dataset_map:\n        raise ValueError(\'Unrecognized dataset\')\n\n    args.batch_size = args.batch_size // args.gradient_accumulation_steps\n    args.device = device\n    args.n_gpu = n_gpu\n    args.num_labels = dataset_map[args.dataset].NUM_CLASSES\n    args.is_multilabel = dataset_map[args.dataset].IS_MULTILABEL\n    args.pretrained_model_path = args.model if os.path.isfile(args.model) else PRETRAINED_MODEL_ARCHIVE_MAP[args.model]\n\n    if not args.trained_model:\n        save_path = os.path.join(args.save_path, dataset_map[args.dataset].NAME)\n        os.makedirs(save_path, exist_ok=True)\n\n    args.is_hierarchical = True\n    args.output_hidden_states = True\n    processor = dataset_map[args.dataset]()\n    pretrained_vocab_path = PRETRAINED_VOCAB_ARCHIVE_MAP[args.model]\n    tokenizer = BertTokenizer.from_pretrained(pretrained_vocab_path)\n\n    train_examples = None\n    num_train_optimization_steps = None\n    if not args.trained_model:\n        train_examples = processor.get_train_examples(args.data_dir)\n        num_train_optimization_steps = int(\n            len(train_examples) / args.batch_size / args.gradient_accumulation_steps) * args.epochs\n\n    model = HierarchicalBert(args)\n\n    if args.fp16:\n        model.half()\n    model.to(device)\n\n    if n_gpu > 1:\n        model = torch.nn.DataParallel(model)\n\n    # Prepare optimizer\n    param_optimizer = list(model.named_parameters())\n\n    no_decay = [\'bias\', \'LayerNorm.bias\', \'LayerNorm.weight\']\n    optimizer_grouped_parameters = [\n        {\'params\': [p for n, p in param_optimizer if \'sentence_encoder\' not in n],\n         \'lr\': args.lr * args.lr_mult, \'weight_decay\': 0.0},\n        {\'params\': [p for n, p in param_optimizer if \'sentence_encoder\' in n and not any(nd in n for nd in no_decay)],\n         \'weight_decay\': 0.01},\n        {\'params\': [p for n, p in param_optimizer if \'sentence_encoder\' in n and any(nd in n for nd in no_decay)],\n         \'weight_decay\': 0.0}]\n\n    if args.fp16:\n        try:\n            from apex.optimizers import FP16_Optimizer\n            from apex.optimizers import FusedAdam\n        except ImportError:\n            raise ImportError(""Please install NVIDIA Apex for distributed and FP16 training"")\n\n        optimizer = FusedAdam(optimizer_grouped_parameters,\n                              lr=args.lr,\n                              bias_correction=False,\n                              max_grad_norm=1.0)\n        if args.loss_scale == 0:\n            optimizer = FP16_Optimizer(optimizer, dynamic_loss_scale=True)\n        else:\n            optimizer = FP16_Optimizer(optimizer, static_loss_scale=args.loss_scale)\n\n    else:\n        optimizer = AdamW(optimizer_grouped_parameters, lr=args.lr, weight_decay=0.01, correct_bias=False)\n        scheduler = WarmupLinearSchedule(optimizer, t_total=num_train_optimization_steps,\n                                         warmup_steps=args.warmup_proportion * num_train_optimization_steps)\n\n    trainer = BertTrainer(model, optimizer, processor, scheduler, tokenizer, args)\n\n    if not args.trained_model:\n        trainer.train()\n        model = torch.load(trainer.snapshot_path)\n    else:\n        model = model = HierarchicalBert(args.model)\n        model_ = torch.load(args, map_location=lambda storage, loc: storage)\n        state = {}\n        for key in model_.state_dict().keys():\n            new_key = key.replace(""module."", """")\n            state[new_key] = model_.state_dict()[key]\n        model.load_state_dict(state)\n        model = model.to(device)\n\n    evaluate_split(model, processor, tokenizer, args, split=\'dev\')\n    evaluate_split(model, processor, tokenizer, args, split=\'test\')\n\n'"
models/hbert/args.py,0,"b""import os\n\nimport models.args\n\n\ndef get_args():\n    parser = models.args.get_args()\n\n    parser.add_argument('--model', default=None, type=str, required=True)\n    parser.add_argument('--dataset', type=str, default='SST-2', choices=['SST-2', 'AGNews', 'Reuters', 'AAPD', 'IMDB', 'Yelp2014'])\n    parser.add_argument('--save-path', type=str, default=os.path.join('model_checkpoints', 'bert'))\n    parser.add_argument('--cache-dir', default='cache', type=str)\n    parser.add_argument('--trained-model', default=None, type=str)\n    parser.add_argument('--local-rank', type=int, default=-1, help='local rank for distributed training')\n    parser.add_argument('--fp16', action='store_true', help='enable 16-bit floating point precision')\n    parser.add_argument('--loss-scale', type=float, default=0, help='loss scaling to improve fp16 numeric stability')\n\n    parser.add_argument('--lr-mult', type=float, default=1)\n    parser.add_argument('--dropout', type=float, default=0.5)\n    parser.add_argument('--dropblock', type=float, default=0.0)\n    parser.add_argument('--dropblock-size', type=int, default=7)\n    parser.add_argument('--beta-ema', type=float, default=0)\n    parser.add_argument('--embed-droprate', type=float, default=0.0)\n    parser.add_argument('--dynamic-pool', action='store_true')\n    parser.add_argument('--dynamic-pool-length', type=int, default=8)\n    parser.add_argument('--output-channel', type=int, default=100)\n\n    parser.add_argument('--max-seq-length', default=128, type=int,\n                        help='maximum total input sequence length after tokenization')\n\n    parser.add_argument('--max-doc-length', default=16, type=int,\n                        help='maximum number of lines processed in one document')\n\n    parser.add_argument('--warmup-proportion', default=0.1, type=float,\n                        help='proportion of training to perform linear learning rate warmup for')\n\n    parser.add_argument('--gradient-accumulation-steps', type=int, default=1,\n                        help='number of updates steps to accumulate before performing a backward/update pass')\n\n    args = parser.parse_args()\n    return args\n"""
models/hbert/model.py,4,"b'import torch\nimport torch.nn.functional as F\nfrom torch import nn\n\nfrom models.hbert.sentence_encoder import BertSentenceEncoder\n\n\nclass HierarchicalBert(nn.Module):\n\n    def __init__(self, args, **kwargs):\n        super().__init__()\n        self.args = args\n        input_channels = 1\n        ks = 3\n\n        self.sentence_encoder = BertSentenceEncoder.from_pretrained(\n            args.pretrained_model_path, num_labels=args.num_labels)\n\n        self.conv1 = nn.Conv2d(input_channels,\n                               args.output_channel,\n                               (3, self.sentence_encoder.config.hidden_size),\n                               padding=(2, 0))\n        self.conv2 = nn.Conv2d(input_channels,\n                               args.output_channel,\n                               (4, self.sentence_encoder.config.hidden_size),\n                               padding=(3, 0))\n        self.conv3 = nn.Conv2d(input_channels,\n                               args.output_channel,\n                               (5, self.sentence_encoder.config.hidden_size),\n                               padding=(4, 0))\n\n        self.dropout = nn.Dropout(args.dropout)\n        self.fc1 = nn.Linear(ks * args.output_channel, args.num_labels)\n\n    def forward(self, input_ids, segment_ids=None, input_mask=None):\n        """"""\n        a batch is a tensor of shape [batch_size, #file_in_commit, #line_in_file]\n        and each element is a line, i.e., a bert_batch,\n        which consists of input_ids, input_mask, segment_ids, label_ids\n        """"""\n        input_ids = input_ids.permute(1, 0, 2)  # (sentences, batch_size, words)\n        segment_ids = segment_ids.permute(1, 0, 2)\n        input_mask = input_mask.permute(1, 0, 2)\n\n        x_encoded = []\n        for i0 in range(len(input_ids)):\n            x_encoded.append(self.sentence_encoder(input_ids[i0], input_mask[i0], segment_ids[i0]))\n\n        x = torch.stack(x_encoded)  # (sentences, batch_size, hidden_size)\n        x = x.permute(1, 0, 2)  # (batch_size, sentences, hidden_size)\n        x = x.unsqueeze(1)  # (batch_size, input_channels, sentences, hidden_size)\n\n        x = [F.relu(self.conv1(x)).squeeze(3),\n             F.relu(self.conv2(x)).squeeze(3),\n             F.relu(self.conv3(x)).squeeze(3)]\n\n        if self.args.dynamic_pool:\n            x = [self.dynamic_pool(i).squeeze(2) for i in x]  # (batch_size, output_channels) * ks\n            x = torch.cat(x, 1)  # (batch_size, output_channels * ks)\n            x = x.view(-1, self.filter_widths * self.output_channel * self.dynamic_pool_length)\n        else:\n            x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]  # (batch_size, output_channels, num_sentences) * ks\n            x = torch.cat(x, 1)  # (batch_size, channel_output * ks)\n\n        x = self.dropout(x)\n        logits = self.fc1(x)  # (batch_size, num_labels)\n\n        return logits, x\n'"
models/hbert/sentence_encoder.py,0,"b'from torch import nn\nfrom transformers import BertPreTrainedModel, BertModel\n\n\nclass BertSentenceEncoder(BertPreTrainedModel):\n    def __init__(self, config):\n        super().__init__(config, num_labels=config.num_labels)\n        self.bert = BertModel(config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.init_weights()\n\n    def forward(self, input_ids, token_type_ids=None, attention_mask=None):\n        pooled_output = self.bert(input_ids, attention_mask, token_type_ids)[1]\n        pooled_output = self.dropout(pooled_output)\n        return pooled_output\n'"
models/kim_cnn/__init__.py,0,b''
models/kim_cnn/__main__.py,14,"b'import logging\nimport os\nimport random\nfrom copy import deepcopy\n\nimport numpy as np\nimport torch\nimport torch.onnx\n\nfrom common.evaluate import EvaluatorFactory\nfrom common.train import TrainerFactory\nfrom datasets.aapd import AAPD\nfrom datasets.imdb import IMDB\nfrom datasets.reuters import Reuters\nfrom datasets.yelp2014 import Yelp2014\nfrom models.kim_cnn.args import get_args\nfrom models.kim_cnn.model import KimCNN\n\n\nclass UnknownWordVecCache(object):\n    """"""\n    Caches the first randomly generated word vector for a certain size to make it is reused.\n    """"""\n    cache = {}\n\n    @classmethod\n    def unk(cls, tensor):\n        size_tup = tuple(tensor.size())\n        if size_tup not in cls.cache:\n            cls.cache[size_tup] = torch.Tensor(tensor.size())\n            cls.cache[size_tup].uniform_(-0.25, 0.25)\n        return cls.cache[size_tup]\n\n\ndef get_logger():\n    logger = logging.getLogger(__name__)\n    logger.setLevel(logging.INFO)\n\n    ch = logging.StreamHandler()\n    ch.setLevel(logging.DEBUG)\n    formatter = logging.Formatter(\'%(levelname)s - %(message)s\')\n    ch.setFormatter(formatter)\n    logger.addHandler(ch)\n\n    return logger\n\n\ndef evaluate_dataset(split_name, dataset_cls, model, embedding, loader, batch_size, device, is_multilabel):\n    saved_model_evaluator = EvaluatorFactory.get_evaluator(dataset_cls, model, embedding, loader, batch_size, device)\n    if hasattr(saved_model_evaluator, \'is_multilabel\'):\n        saved_model_evaluator.is_multilabel = is_multilabel\n\n    scores, metric_names = saved_model_evaluator.get_scores()\n    print(\'Evaluation metrics for\', split_name)\n    print(metric_names)\n    print(scores)\n\n\nif __name__ == \'__main__\':\n    # Set default configuration in args.py\n    args = get_args()\n\n    # Set random seed for reproducibility\n    torch.manual_seed(args.seed)\n    torch.backends.cudnn.deterministic = True\n    if not args.cuda:\n        args.gpu = -1\n    if torch.cuda.is_available() and args.cuda:\n        print(\'Note: You are using GPU for training\')\n        torch.cuda.set_device(args.gpu)\n        torch.cuda.manual_seed(args.seed)\n    if torch.cuda.is_available() and not args.cuda:\n        print(\'Warning: Using CPU for training\')\n    np.random.seed(args.seed)\n    random.seed(args.seed)\n    logger = get_logger()\n\n    dataset_map = {\n        \'Reuters\': Reuters,\n        \'AAPD\': AAPD,\n        \'IMDB\': IMDB,\n        \'Yelp2014\': Yelp2014\n    }\n\n    if args.dataset not in dataset_map:\n        raise ValueError(\'Unrecognized dataset\')\n    else:\n        dataset_class = dataset_map[args.dataset]\n        train_iter, dev_iter, test_iter = dataset_map[args.dataset].iters(args.data_dir, args.word_vectors_file,\n                                                                          args.word_vectors_dir,\n                                                                          batch_size=args.batch_size, device=args.gpu,\n                                                                          unk_init=UnknownWordVecCache.unk)\n\n    config = deepcopy(args)\n    config.dataset = train_iter.dataset\n    config.target_class = train_iter.dataset.NUM_CLASSES\n    config.words_num = len(train_iter.dataset.TEXT_FIELD.vocab)\n\n    print(\'Dataset:\', args.dataset)\n    print(\'No. of target classes:\', train_iter.dataset.NUM_CLASSES)\n    print(\'No. of train instances\', len(train_iter.dataset))\n    print(\'No. of dev instances\', len(dev_iter.dataset))\n    print(\'No. of test instances\', len(test_iter.dataset))\n\n    if args.resume_snapshot:\n        if args.cuda:\n            model = torch.load(args.resume_snapshot, map_location=lambda storage, location: storage.cuda(args.gpu))\n        else:\n            model = torch.load(args.resume_snapshot, map_location=lambda storage, location: storage)\n    else:\n        model = KimCNN(config)\n        if args.cuda:\n            model.cuda()\n\n    if not args.trained_model:\n        save_path = os.path.join(args.save_path, dataset_map[args.dataset].NAME)\n        os.makedirs(save_path, exist_ok=True)\n\n    parameter = filter(lambda p: p.requires_grad, model.parameters())\n    optimizer = torch.optim.Adam(parameter, lr=args.lr, weight_decay=args.weight_decay)\n\n    train_evaluator = EvaluatorFactory.get_evaluator(dataset_map[args.dataset], model, None, train_iter, args.batch_size, args.gpu)\n    test_evaluator = EvaluatorFactory.get_evaluator(dataset_map[args.dataset], model, None, test_iter, args.batch_size, args.gpu)\n    dev_evaluator = EvaluatorFactory.get_evaluator(dataset_map[args.dataset], model, None, dev_iter, args.batch_size, args.gpu)\n\n    if hasattr(train_evaluator, \'is_multilabel\'):\n        train_evaluator.is_multilabel = dataset_class.IS_MULTILABEL\n    if hasattr(test_evaluator, \'is_multilabel\'):\n        test_evaluator.is_multilabel = dataset_class.IS_MULTILABEL\n    if hasattr(dev_evaluator, \'is_multilabel\'):\n        dev_evaluator.is_multilabel = dataset_class.IS_MULTILABEL\n\n    trainer_config = {\n        \'optimizer\': optimizer,\n        \'batch_size\': args.batch_size,\n        \'log_interval\': args.log_every,\n        \'patience\': args.patience,\n        \'model_outfile\': args.save_path,\n        \'logger\': logger,\n        \'is_multilabel\': dataset_class.IS_MULTILABEL\n    }\n\n    trainer = TrainerFactory.get_trainer(args.dataset, model, None, train_iter, trainer_config, train_evaluator, test_evaluator, dev_evaluator)\n\n    if not args.trained_model:\n        trainer.train(args.epochs)\n    else:\n        if args.cuda:\n            model = torch.load(args.trained_model, map_location=lambda storage, location: storage.cuda(args.gpu))\n        else:\n            model = torch.load(args.trained_model, map_location=lambda storage, location: storage)\n\n    # Calculate dev and test metrics\n    if hasattr(trainer, \'snapshot_path\'):\n        model = torch.load(trainer.snapshot_path)\n\n    evaluate_dataset(\'dev\', dataset_map[args.dataset], model, None, dev_iter, args.batch_size,\n                     is_multilabel=dataset_class.IS_MULTILABEL,\n                     device=args.gpu)\n    evaluate_dataset(\'test\', dataset_map[args.dataset], model, None, test_iter, args.batch_size,\n                     is_multilabel=dataset_class.IS_MULTILABEL,\n                     device=args.gpu)\n'"
models/kim_cnn/args.py,0,"b""import os\n\nimport models.args\n\n\ndef get_args():\n    parser = models.args.get_args()\n\n    parser.add_argument('--dataset', type=str, default='Reuters', choices=['Reuters', 'AAPD', 'IMDB', 'Yelp2014'])\n    parser.add_argument('--mode', type=str, default='multichannel', choices=['rand', 'static', 'non-static', 'multichannel'])\n    parser.add_argument('--output-channel', type=int, default=100)\n    parser.add_argument('--words-dim', type=int, default=300)\n    parser.add_argument('--embed-dim', type=int, default=300)\n    parser.add_argument('--dropout', type=float, default=0.5)\n    parser.add_argument('--epoch-decay', type=int, default=15)\n    parser.add_argument('--weight-decay', type=float, default=0)\n\n    parser.add_argument('--word-vectors-dir', default=os.path.join(os.pardir, 'hedwig-data', 'embeddings', 'word2vec'))\n    parser.add_argument('--word-vectors-file', default='GoogleNews-vectors-negative300.txt')\n    parser.add_argument('--save-path', type=str, default=os.path.join('model_checkpoints', 'kim_cnn'))\n    parser.add_argument('--resume-snapshot', type=str)\n    parser.add_argument('--trained-model', type=str)\n\n    args = parser.parse_args()\n    return args\n"""
models/kim_cnn/model.py,5,"b'import torch\nimport torch.nn as nn\n\nimport torch.nn.functional as F\n\n\nclass KimCNN(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        dataset = config.dataset\n        output_channel = config.output_channel\n        target_class = config.target_class\n        words_num = config.words_num\n        words_dim = config.words_dim\n        self.mode = config.mode\n        ks = 3 # There are three conv nets here\n\n        input_channel = 1\n        if config.mode == \'rand\':\n            rand_embed_init = torch.Tensor(words_num, words_dim).uniform_(-0.25, 0.25)\n            self.embed = nn.Embedding.from_pretrained(rand_embed_init, freeze=False)\n        elif config.mode == \'static\':\n            self.static_embed = nn.Embedding.from_pretrained(dataset.TEXT_FIELD.vocab.vectors, freeze=True)\n        elif config.mode == \'non-static\':\n            self.non_static_embed = nn.Embedding.from_pretrained(dataset.TEXT_FIELD.vocab.vectors, freeze=False)\n        elif config.mode == \'multichannel\':\n            self.static_embed = nn.Embedding.from_pretrained(dataset.TEXT_FIELD.vocab.vectors, freeze=True)\n            self.non_static_embed = nn.Embedding.from_pretrained(dataset.TEXT_FIELD.vocab.vectors, freeze=False)\n            input_channel = 2\n        else:\n            print(""Unsupported Mode"")\n            exit()\n\n        self.conv1 = nn.Conv2d(input_channel, output_channel, (3, words_dim), padding=(2,0))\n        self.conv2 = nn.Conv2d(input_channel, output_channel, (4, words_dim), padding=(3,0))\n        self.conv3 = nn.Conv2d(input_channel, output_channel, (5, words_dim), padding=(4,0))\n\n        self.dropout = nn.Dropout(config.dropout)\n        self.fc1 = nn.Linear(ks * output_channel, target_class)\n\n    def forward(self, x, **kwargs):\n        if self.mode == \'rand\':\n            word_input = self.embed(x) # (batch, sent_len, embed_dim)\n            x = word_input.unsqueeze(1) # (batch, channel_input, sent_len, embed_dim)\n        elif self.mode == \'static\':\n            static_input = self.static_embed(x)\n            x = static_input.unsqueeze(1) # (batch, channel_input, sent_len, embed_dim)\n        elif self.mode == \'non-static\':\n            non_static_input = self.non_static_embed(x)\n            x = non_static_input.unsqueeze(1) # (batch, channel_input, sent_len, embed_dim)\n        elif self.mode == \'multichannel\':\n            non_static_input = self.non_static_embed(x)\n            static_input = self.static_embed(x)\n            x = torch.stack([non_static_input, static_input], dim=1) # (batch, channel_input=2, sent_len, embed_dim)\n        else:\n            print(""Unsupported Mode"")\n            exit()\n        x = [F.relu(self.conv1(x)).squeeze(3), F.relu(self.conv2(x)).squeeze(3), F.relu(self.conv3(x)).squeeze(3)]\n        # (batch, channel_output, ~=sent_len) * ks\n        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x] # max-over-time pooling\n        # (batch, channel_output) * ks\n        x = torch.cat(x, 1) # (batch, channel_output * ks)\n        x = self.dropout(x)\n        logit = self.fc1(x) # (batch, target_size)\n        return logit\n'"
models/lr/__init__.py,0,b''
models/lr/__main__.py,9,"b'import os\nimport random\n\nimport numpy as np\nimport torch.onnx\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nfrom common.evaluators.bow_evaluator import BagOfWordsEvaluator\nfrom common.trainers.bow_trainer import BagOfWordsTrainer\nfrom datasets.bow_processors.aapd_processor import AAPDProcessor\nfrom datasets.bow_processors.imdb_processor import IMDBProcessor\nfrom datasets.bow_processors.reuters_processor import ReutersProcessor\nfrom datasets.bow_processors.yelp2014_processor import Yelp2014Processor\nfrom models.lr.args import get_args\nfrom models.lr.model import LogisticRegression\n\n# String templates for logging results\nLOG_HEADER = \'Split  Dev/Acc.  Dev/Pr.  Dev/Re.   Dev/F1   Dev/Loss\'\nLOG_TEMPLATE = \' \'.join(\'{:>5s},{:>9.4f},{:>8.4f},{:8.4f},{:8.4f},{:10.4f}\'.split(\',\'))\n\n\ndef evaluate_split(model, vectorizer, processor, args, split=\'dev\'):\n    evaluator = BagOfWordsEvaluator(model, vectorizer, processor, args, split)\n    accuracy, precision, recall, f1, avg_loss = evaluator.get_scores(silent=True)[0]\n    print(\'\\n\' + LOG_HEADER)\n    print(LOG_TEMPLATE.format(split.upper(), accuracy, precision, recall, f1, avg_loss))\n\n\nif __name__ == \'__main__\':\n    # Set default configuration in args.py\n    args = get_args()\n    n_gpu = torch.cuda.device_count()\n    device = torch.device(""cuda"" if torch.cuda.is_available() and args.cuda else ""cpu"")\n\n    print(\'Number of GPUs:\', n_gpu)\n    print(\'Device:\', str(device).upper())\n\n    # Set random seed for reproducibility\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    if n_gpu > 0:\n        torch.cuda.manual_seed_all(args.seed)\n\n    dataset_map = {\n        \'Reuters\': ReutersProcessor,\n        \'AAPD\': AAPDProcessor,\n        \'IMDB\': IMDBProcessor,\n        \'Yelp2014\': Yelp2014Processor\n    }\n\n    if args.dataset not in dataset_map:\n        raise ValueError(\'Unrecognized dataset\')\n\n    args.device = device\n    args.n_gpu = n_gpu\n    args.num_labels = dataset_map[args.dataset].NUM_CLASSES\n    args.is_multilabel = dataset_map[args.dataset].IS_MULTILABEL\n    args.vocab_size = min(args.max_vocab_size, dataset_map[args.dataset].VOCAB_SIZE)\n\n    train_examples = None\n    processor = dataset_map[args.dataset]()\n    vectorizer = TfidfVectorizer(stop_words=stopwords.words(""english""),\n                                 # max_features=args.max_vocab_size,\n                                 tokenizer=word_tokenize)\n\n    if not args.trained_model:\n        train_examples = processor.get_train_examples(args.data_dir)\n        save_path = os.path.join(args.save_path, dataset_map[args.dataset].NAME)\n        os.makedirs(save_path, exist_ok=True)\n\n    model = LogisticRegression(args)\n    model.to(device)\n\n    if n_gpu > 1:\n        model = torch.nn.DataParallel(model)\n\n    parameter = filter(lambda p: p.requires_grad, model.parameters())\n    optimizer = torch.optim.Adam(parameter, lr=args.lr, weight_decay=args.weight_decay)\n    trainer = BagOfWordsTrainer(model, vectorizer, optimizer, processor, args)\n\n    if not args.trained_model:\n        trainer.train()\n        model = torch.load(trainer.snapshot_path)\n    else:\n        model = torch.load(args.trained_model, map_location=lambda storage, location: storage)\n        model = model.to(device)\n\n    evaluate_split(model, vectorizer, processor, args, split=\'dev\')\n    evaluate_split(model, vectorizer, processor, args, split=\'test\')\n'"
models/lr/args.py,0,"b""import os\n\nimport models.args\n\n\ndef get_args():\n    parser = models.args.get_args()\n\n    parser.add_argument('--dataset', type=str, default='Reuters', choices=['Reuters', 'AAPD', 'IMDB', 'Yelp2014'])\n    parser.add_argument('--max-vocab-size', type=int, default=500000)\n    parser.add_argument('--dropout', type=float, default=0)\n    parser.add_argument('--epoch-decay', type=int, default=15)\n    parser.add_argument('--weight-decay', type=float, default=0)\n\n    parser.add_argument('--save-path', type=str, default=os.path.join('model_checkpoints', 'lr'))\n    parser.add_argument('--resume-snapshot', type=str)\n    parser.add_argument('--trained-model', type=str)\n\n    args = parser.parse_args()\n    return args\n"""
models/lr/model.py,2,"b'import torch\nimport torch.nn as nn\n\n\nclass LogisticRegression(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        self.dropout = nn.Dropout(config.dropout)\n        self.fc1 = nn.Linear(config.vocab_size, config.num_labels)\n\n    def forward(self, x, **kwargs):\n        x = torch.squeeze(x)  # (batch, vocab_size)\n        x = self.dropout(x)\n        logit = self.fc1(x)  # (batch, target_size)\n        return logit\n'"
models/reg_lstm/__init__.py,0,b''
models/reg_lstm/__main__.py,13,"b'import logging\nimport os\nimport random\nfrom copy import deepcopy\n\nimport numpy as np\nimport torch\n\nfrom common.evaluate import EvaluatorFactory\nfrom common.train import TrainerFactory\nfrom datasets.aapd import AAPD\nfrom datasets.imdb import IMDB\nfrom datasets.reuters import Reuters\nfrom datasets.yelp2014 import Yelp2014\nfrom models.reg_lstm.args import get_args\nfrom models.reg_lstm.model import RegLSTM\n\n\nclass UnknownWordVecCache(object):\n    """"""\n    Caches the first randomly generated word vector for a certain size to make it is reused.\n    """"""\n    cache = {}\n\n    @classmethod\n    def unk(cls, tensor):\n        size_tup = tuple(tensor.size())\n        if size_tup not in cls.cache:\n            cls.cache[size_tup] = torch.Tensor(tensor.size())\n            cls.cache[size_tup].uniform_(-0.25, 0.25)\n        return cls.cache[size_tup]\n\n\ndef get_logger():\n    logger = logging.getLogger(__name__)\n    logger.setLevel(logging.INFO)\n\n    ch = logging.StreamHandler()\n    ch.setLevel(logging.DEBUG)\n    formatter = logging.Formatter(\'%(levelname)s - %(message)s\')\n    ch.setFormatter(formatter)\n    logger.addHandler(ch)\n\n    return logger\n\n\ndef evaluate_dataset(split_name, dataset_cls, model, embedding, loader, batch_size, device, is_multilabel):\n    saved_model_evaluator = EvaluatorFactory.get_evaluator(dataset_cls, model, embedding, loader, batch_size, device)\n    if hasattr(saved_model_evaluator, \'is_multilabel\'):\n        saved_model_evaluator.is_multilabel = is_multilabel\n\n    scores, metric_names = saved_model_evaluator.get_scores()\n    print(\'Evaluation metrics for\', split_name)\n    print(metric_names)\n    print(scores)\n\n\nif __name__ == \'__main__\':\n    # Set default configuration in args.py\n    args = get_args()\n    logger = get_logger()\n\n    # Set random seed for reproducibility\n    torch.manual_seed(args.seed)\n    torch.backends.cudnn.deterministic = True\n    np.random.seed(args.seed)\n    random.seed(args.seed)\n\n    if not args.cuda:\n        args.gpu = -1\n    if torch.cuda.is_available() and args.cuda:\n        print(\'Note: You are using GPU for training\')\n        torch.cuda.set_device(args.gpu)\n        torch.cuda.manual_seed(args.seed)\n    if torch.cuda.is_available() and not args.cuda:\n        print(\'Warning: Using CPU for training\')\n\n    dataset_map = {\n        \'Reuters\': Reuters,\n        \'AAPD\': AAPD,\n        \'IMDB\': IMDB,\n        \'Yelp2014\': Yelp2014\n    }\n\n    if args.dataset not in dataset_map:\n        raise ValueError(\'Unrecognized dataset\')\n\n    else:\n        dataset_class = dataset_map[args.dataset]\n        train_iter, dev_iter, test_iter = dataset_class.iters(args.data_dir,\n                                                              args.word_vectors_file,\n                                                              args.word_vectors_dir,\n                                                              batch_size=args.batch_size,\n                                                              device=args.gpu,\n                                                              unk_init=UnknownWordVecCache.unk)\n\n    config = deepcopy(args)\n    config.dataset = train_iter.dataset\n    config.target_class = train_iter.dataset.NUM_CLASSES\n    config.words_num = len(train_iter.dataset.TEXT_FIELD.vocab)\n\n    print(\'Dataset:\', args.dataset)\n    print(\'No. of target classes:\', train_iter.dataset.NUM_CLASSES)\n    print(\'No. of train instances\', len(train_iter.dataset))\n    print(\'No. of dev instances\', len(dev_iter.dataset))\n    print(\'No. of test instances\', len(test_iter.dataset))\n\n    if args.resume_snapshot:\n        if args.cuda:\n            model = torch.load(args.resume_snapshot, map_location=lambda storage, location: storage.cuda(args.gpu))\n        else:\n            model = torch.load(args.resume_snapshot, map_location=lambda storage, location: storage)\n    else:\n        model = RegLSTM(config)\n        if args.cuda:\n            model.cuda()\n\n    if not args.trained_model:\n        save_path = os.path.join(args.save_path, dataset_map[args.dataset].NAME)\n        os.makedirs(save_path, exist_ok=True)\n\n    parameter = filter(lambda p: p.requires_grad, model.parameters())\n    optimizer = torch.optim.Adam(parameter, lr=args.lr, weight_decay=args.weight_decay)\n\n    train_evaluator = EvaluatorFactory.get_evaluator(dataset_class, model, None, train_iter, args.batch_size, args.gpu)\n    test_evaluator = EvaluatorFactory.get_evaluator(dataset_class, model, None, test_iter, args.batch_size, args.gpu)\n    dev_evaluator = EvaluatorFactory.get_evaluator(dataset_class, model, None, dev_iter, args.batch_size, args.gpu)\n\n    if hasattr(train_evaluator, \'is_multilabel\'):\n        train_evaluator.is_multilabel = dataset_class.IS_MULTILABEL\n    if hasattr(test_evaluator, \'is_multilabel\'):\n        test_evaluator.is_multilabel = dataset_class.IS_MULTILABEL\n    if hasattr(dev_evaluator, \'is_multilabel\'):\n        dev_evaluator.is_multilabel = dataset_class.IS_MULTILABEL\n\n    trainer_config = {\n        \'optimizer\': optimizer,\n        \'batch_size\': args.batch_size,\n        \'log_interval\': args.log_every,\n        \'patience\': args.patience,\n        \'model_outfile\': args.save_path,\n        \'logger\': logger,\n        \'is_multilabel\': dataset_class.IS_MULTILABEL\n    }\n\n    trainer = TrainerFactory.get_trainer(args.dataset, model, None, train_iter, trainer_config, train_evaluator, test_evaluator, dev_evaluator)\n\n    if not args.trained_model:\n        trainer.train(args.epochs)\n    else:\n        if args.cuda:\n            model = torch.load(args.trained_model, map_location=lambda storage, location: storage.cuda(args.gpu))\n        else:\n            model = torch.load(args.trained_model, map_location=lambda storage, location: storage)\n\n    model = torch.load(trainer.snapshot_path)\n\n    if model.beta_ema > 0:\n        old_params = model.get_params()\n        model.load_ema_params()\n\n    # Calculate dev and test metrics\n    evaluate_dataset(\'dev\', dataset_class, model, None, dev_iter, args.batch_size,\n                     is_multilabel=dataset_class.IS_MULTILABEL,\n                     device=args.gpu)\n    evaluate_dataset(\'test\', dataset_class, model, None, test_iter, args.batch_size,\n                     is_multilabel=dataset_class.IS_MULTILABEL,\n                     device=args.gpu)\n\n    if model.beta_ema > 0:\n        model.load_params(old_params)\n'"
models/reg_lstm/args.py,0,"b'import os\n\nimport models.args\n\n\ndef get_args():\n    parser = models.args.get_args()\n\n    parser.add_argument(\'--bidirectional\', action=\'store_true\')\n    parser.add_argument(\'--bottleneck-layer\', action=\'store_true\')\n    parser.add_argument(\'--num-layers\', type=int, default=2)\n    parser.add_argument(\'--hidden-dim\', type=int, default=256)\n    parser.add_argument(\'--mode\', type=str, default=\'static\', choices=[\'rand\', \'static\', \'non-static\'])\n    parser.add_argument(\'--dataset\', type=str, default=\'Reuters\', choices=[\'Reuters\', \'AAPD\', \'IMDB\', \'Yelp2014\'])\n    parser.add_argument(\'--words-dim\', type=int, default=300)\n    parser.add_argument(\'--embed-dim\', type=int, default=300)\n    parser.add_argument(\'--epoch-decay\', type=int, default=15)\n    parser.add_argument(\'--weight-decay\', type=float, default=0)\n\n    parser.add_argument(\'--dropout\', type=float, default=0.5)\n    parser.add_argument(\'--wdrop\', type=float, default=0.0, help=""weight drop"")\n    parser.add_argument(\'--beta-ema\', type=float, default=0, help=""temporal averaging"")\n    parser.add_argument(\'--embed-droprate\', type=float, default=0.0, help=""embedding dropout"")\n    parser.add_argument(\'--tar\', type=float, default=0.0, help=""temporal activation regularization"")\n    parser.add_argument(\'--ar\', type=float, default=0.0, help=""activation regularization"")\n\n    parser.add_argument(\'--word-vectors-dir\', default=os.path.join(os.pardir, \'hedwig-data\', \'embeddings\', \'word2vec\'))\n    parser.add_argument(\'--word-vectors-file\', default=\'GoogleNews-vectors-negative300.txt\')\n    parser.add_argument(\'--save-path\', type=str, default=os.path.join(\'model_checkpoints\', \'reg_lstm\'))\n    parser.add_argument(\'--resume-snapshot\', type=str)\n    parser.add_argument(\'--trained-model\', type=str)\n\n    args = parser.parse_args()\n    return args\n'"
models/reg_lstm/embed_regularize.py,1,"b'""""""\nBSD 3-Clause License\n\nCopyright (c) 2017, \nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met:\n\n* Redistributions of source code must retain the above copyright notice, this\n  list of conditions and the following disclaimer.\n\n* Redistributions in binary form must reproduce the above copyright notice,\n  this list of conditions and the following disclaimer in the documentation\n  and/or other materials provided with the distribution.\n\n* Neither the name of the copyright holder nor the names of its\n  contributors may be used to endorse or promote products derived from\n  this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n""""""\nimport numpy as np\n\nimport torch\n\ndef embedded_dropout(embed, words, dropout=0.1, scale=None):\n  if dropout:\n    mask = embed.weight.data.new().resize_((embed.weight.size(0), 1)).bernoulli_(1 - dropout).expand_as(embed.weight) / (1 - dropout)\n    masked_embed_weight = mask * embed.weight\n  else:\n    masked_embed_weight = embed.weight\n  if scale:\n    masked_embed_weight = scale.expand_as(masked_embed_weight) * masked_embed_weight\n\n  padding_idx = embed.padding_idx\n  if padding_idx is None:\n      padding_idx = -1\n\n  X = torch.nn.functional.embedding(words, masked_embed_weight,\n    padding_idx, embed.max_norm, embed.norm_type,\n    embed.scale_grad_by_freq, embed.sparse\n  )\n  return X\n'"
models/reg_lstm/locked_dropout.py,1,"b'import torch\nimport torch.nn as nn\n\n\nclass LockedDropout(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x, dropout=0.5):\n        if not self.training or not dropout:\n            return x\n        m = x.data.new(1, x.size(1), x.size(2)).bernoulli_(1 - dropout)\n        mask = m / (1 - dropout)\n        mask = mask.expand_as(x)\n        return mask * x\n'"
models/reg_lstm/model.py,8,"b'from copy import deepcopy\nimport torch\nimport torch.nn as nn\n\nimport torch.nn.functional as F\n\nfrom models.reg_lstm.weight_drop import WeightDrop\nfrom models.reg_lstm.embed_regularize import embedded_dropout\n\n\nclass RegLSTM(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        dataset = config.dataset\n        target_class = config.target_class\n        self.is_bidirectional = config.bidirectional\n        self.has_bottleneck_layer = config.bottleneck_layer\n        self.mode = config.mode\n        self.tar = config.tar\n        self.ar = config.ar\n        self.beta_ema = config.beta_ema  # Temporal averaging\n        self.wdrop = config.wdrop  # Weight dropping\n        self.embed_droprate = config.embed_droprate  # Embedding dropout\n\n        if config.mode == \'rand\':\n            rand_embed_init = torch.Tensor(config.words_num, config.words_dim).uniform_(-0.25, 0.25)\n            self.embed = nn.Embedding.from_pretrained(rand_embed_init, freeze=False)\n        elif config.mode == \'static\':\n            self.static_embed = nn.Embedding.from_pretrained(dataset.TEXT_FIELD.vocab.vectors, freeze=True)\n        elif config.mode == \'non-static\':\n            self.non_static_embed = nn.Embedding.from_pretrained(dataset.TEXT_FIELD.vocab.vectors, freeze=False)\n        else:\n            print(""Unsupported Mode"")\n            exit()\n\n        self.lstm = nn.LSTM(config.words_dim, config.hidden_dim, dropout=config.dropout, num_layers=config.num_layers,\n                            bidirectional=self.is_bidirectional, batch_first=True)\n\n        if self.wdrop:\n            self.lstm = WeightDrop(self.lstm, [\'weight_hh_l0\'], dropout=self.wdrop)\n        self.dropout = nn.Dropout(config.dropout)\n\n        if self.has_bottleneck_layer:\n            if self.is_bidirectional:\n                self.fc1 = nn.Linear(2 * config.hidden_dim, config.hidden_dim)  # Hidden Bottleneck Layer\n                self.fc2 = nn.Linear(config.hidden_dim, target_class)\n            else:\n                self.fc1 = nn.Linear(config.hidden_dim, config.hidden_dim//2)   # Hidden Bottleneck Layer\n                self.fc2 = nn.Linear(config.hidden_dim//2, target_class)\n        else:\n            if self.is_bidirectional:\n                self.fc1 = nn.Linear(2 * config.hidden_dim, target_class)\n            else:\n                self.fc1 = nn.Linear(config.hidden_dim, target_class)\n        \n        if self.beta_ema>0:\n            self.avg_param = deepcopy(list(p.data for p in self.parameters()))\n            if torch.cuda.is_available():\n                self.avg_param = [a.cuda() for a in self.avg_param]\n            self.steps_ema = 0.\n\n    def forward(self, x, lengths=None):\n        if self.mode == \'rand\':\n            x = embedded_dropout(self.embed, x, dropout=self.embed_droprate if self.training else 0) if self.embed_droprate else self.embed(x)\n        elif self.mode == \'static\':\n            x = embedded_dropout(self.static_embed, x, dropout=self.embed_droprate if self.training else 0) if self.embed_droprate else self.static_embed(x)\n        elif self.mode == \'non-static\':\n            x = embedded_dropout(self.non_static_embed, x, dropout=self.embed_droprate if self.training else 0) if self.embed_droprate else self.non_static_embed(x)\n        else:\n            print(""Unsupported Mode"")\n            exit()\n        if lengths is not None:\n            x = torch.nn.utils.rnn.pack_padded_sequence(x, lengths, batch_first=True)\n        rnn_outs, _ = self.lstm(x)\n        rnn_outs_temp = rnn_outs\n\n        if lengths is not None:\n            rnn_outs,_ = torch.nn.utils.rnn.pad_packed_sequence(rnn_outs, batch_first=True)\n            rnn_outs_temp, _ = torch.nn.utils.rnn.pad_packed_sequence(rnn_outs_temp, batch_first=True)\n\n        x = F.relu(torch.transpose(rnn_outs_temp, 1, 2))\n        x = F.max_pool1d(x, x.size(2)).squeeze(2)\n        x = self.dropout(x)\n        if self.has_bottleneck_layer:\n            x = F.relu(self.fc1(x))\n            # x = self.dropout(x)\n            if self.tar or self.ar:\n                return self.fc2(x), rnn_outs.permute(1,0,2)\n            return self.fc2(x)\n        else:\n            if self.tar or self.ar:\n                return self.fc1(x), rnn_outs.permute(1,0,2)\n            return self.fc1(x)\n\n    def update_ema(self):\n        self.steps_ema += 1\n        for p, avg_p in zip(self.parameters(), self.avg_param):\n            avg_p.mul_(self.beta_ema).add_((1-self.beta_ema)*p.data)\n    \n    def load_ema_params(self):\n        for p, avg_p in zip(self.parameters(), self.avg_param):\n            p.data.copy_(avg_p/(1-self.beta_ema**self.steps_ema))\n\n    def load_params(self, params):\n        for p,avg_p in zip(self.parameters(), params):\n            p.data.copy_(avg_p)\n\n    def get_params(self):\n        params = deepcopy(list(p.data for p in self.parameters()))\n        return params\n'"
models/reg_lstm/weight_drop.py,6,"b'""""""\nBSD 3-Clause License\n\nCopyright (c) 2017, \nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met:\n\n* Redistributions of source code must retain the above copyright notice, this\n  list of conditions and the following disclaimer.\n\n* Redistributions in binary form must reproduce the above copyright notice,\n  this list of conditions and the following disclaimer in the documentation\n  and/or other materials provided with the distribution.\n\n* Neither the name of the copyright holder nor the names of its\n  contributors may be used to endorse or promote products derived from\n  this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n""""""\nimport torch\nfrom torch.nn import Parameter\nfrom functools import wraps\n\n\nclass WeightDrop(torch.nn.Module):\n\n    def __init__(self, module, weights, dropout=0, variational=False):\n        super().__init__()\n        self.module = module\n        self.weights = weights\n        self.dropout = dropout\n        self.variational = variational\n        self._setup()\n\n    def null_function(*args, **kwargs):\n        # We need to replace flatten_parameters with a nothing function\n        return\n\n    def _setup(self):\n        # Terrible temporary solution to an issue regarding compacting weights re: CUDNN RNN\n        if issubclass(type(self.module), torch.nn.RNNBase):\n            self.module.flatten_parameters = self.null_function\n\n        for name_w in self.weights:\n            print(\'Applying weight drop of {} to {}\'.format(self.dropout, name_w))\n            w = getattr(self.module, name_w)\n            del self.module._parameters[name_w]\n            self.module.register_parameter(name_w + \'_raw\', Parameter(w.data))\n\n    def _setweights(self):\n        for name_w in self.weights:\n            raw_w = getattr(self.module, name_w + \'_raw\')\n            w = None\n            if self.variational:\n                mask = torch.autograd.Variable(torch.ones(raw_w.size(0), 1))\n                if raw_w.is_cuda: mask = mask.cuda()\n                mask = torch.nn.functional.dropout(mask, p=self.dropout, training=True)\n                w = mask.expand_as(raw_w) * raw_w\n            else:\n                w = torch.nn.functional.dropout(raw_w, p=self.dropout, training=self.training)\n            setattr(self.module, name_w, w)\n\n    def forward(self, *args):\n        self._setweights()\n        return self.module.forward(*args)\n'"
models/xml_cnn/__init__.py,0,b''
models/xml_cnn/__main__.py,14,"b'import logging\nimport os\nimport random\nfrom copy import deepcopy\n\nimport numpy as np\nimport torch\nimport torch.onnx\n\nfrom common.evaluate import EvaluatorFactory\nfrom common.train import TrainerFactory\nfrom datasets.aapd import AAPD\nfrom datasets.imdb import IMDB\nfrom datasets.reuters import Reuters\nfrom datasets.yelp2014 import Yelp2014\nfrom models.xml_cnn.args import get_args\nfrom models.xml_cnn.model import XmlCNN\n\n\nclass UnknownWordVecCache(object):\n    """"""\n    Caches the first randomly generated word vector for a certain size to make it is reused.\n    """"""\n    cache = {}\n\n    @classmethod\n    def unk(cls, tensor):\n        size_tup = tuple(tensor.size())\n        if size_tup not in cls.cache:\n            cls.cache[size_tup] = torch.Tensor(tensor.size())\n            cls.cache[size_tup].uniform_(-0.25, 0.25)\n        return cls.cache[size_tup]\n\n\ndef get_logger():\n    logger = logging.getLogger(__name__)\n    logger.setLevel(logging.INFO)\n\n    ch = logging.StreamHandler()\n    ch.setLevel(logging.DEBUG)\n    formatter = logging.Formatter(\'%(levelname)s - %(message)s\')\n    ch.setFormatter(formatter)\n    logger.addHandler(ch)\n\n    return logger\n\n\ndef evaluate_dataset(split_name, dataset_cls, model, embedding, loader, batch_size, device, is_multilabel):\n    saved_model_evaluator = EvaluatorFactory.get_evaluator(dataset_cls, model, embedding, loader, batch_size, device)\n    if hasattr(saved_model_evaluator, \'is_multilabel\'):\n        saved_model_evaluator.is_multilabel = is_multilabel\n\n    scores, metric_names = saved_model_evaluator.get_scores()\n    print(\'Evaluation metrics for\', split_name)\n    print(metric_names)\n    print(scores)\n\n\nif __name__ == \'__main__\':\n    # Set default configuration in args.py\n    args = get_args()\n\n    # Set random seed for reproducibility\n    torch.manual_seed(args.seed)\n    torch.backends.cudnn.deterministic = True\n    if not args.cuda:\n        args.gpu = -1\n    if torch.cuda.is_available() and args.cuda:\n        print(\'Note: You are using GPU for training\')\n        torch.cuda.set_device(args.gpu)\n        torch.cuda.manual_seed(args.seed)\n    if torch.cuda.is_available() and not args.cuda:\n        print(\'Warning: Using CPU for training\')\n    np.random.seed(args.seed)\n    random.seed(args.seed)\n    logger = get_logger()\n\n    dataset_map = {\n        \'Reuters\': Reuters,\n        \'AAPD\': AAPD,\n        \'IMDB\': IMDB,\n        \'Yelp2014\': Yelp2014\n    }\n\n    if args.dataset not in dataset_map:\n        raise ValueError(\'Unrecognized dataset\')\n    else:\n        dataset_class = dataset_map[args.dataset]\n        train_iter, dev_iter, test_iter = dataset_map[args.dataset].iters(args.data_dir, args.word_vectors_file,\n                                                                          args.word_vectors_dir,\n                                                                          batch_size=args.batch_size, device=args.gpu,\n                                                                          unk_init=UnknownWordVecCache.unk)\n\n    config = deepcopy(args)\n    config.dataset = train_iter.dataset\n    config.target_class = train_iter.dataset.NUM_CLASSES\n    config.words_num = len(train_iter.dataset.TEXT_FIELD.vocab)\n\n    print(\'Dataset:\', args.dataset)\n    print(\'No. of target classes:\', train_iter.dataset.NUM_CLASSES)\n    print(\'No. of train instances\', len(train_iter.dataset))\n    print(\'No. of dev instances\', len(dev_iter.dataset))\n    print(\'No. of test instances\', len(test_iter.dataset))\n\n    if args.resume_snapshot:\n        if args.cuda:\n            model = torch.load(args.resume_snapshot, map_location=lambda storage, location: storage.cuda(args.gpu))\n        else:\n            model = torch.load(args.resume_snapshot, map_location=lambda storage, location: storage)\n    else:\n        model = XmlCNN(config)\n        if args.cuda:\n            model.cuda()\n\n    if not args.trained_model:\n        save_path = os.path.join(args.save_path, dataset_map[args.dataset].NAME)\n        os.makedirs(save_path, exist_ok=True)\n\n    parameter = filter(lambda p: p.requires_grad, model.parameters())\n    optimizer = torch.optim.Adam(parameter, lr=args.lr, weight_decay=args.weight_decay)\n\n    train_evaluator = EvaluatorFactory.get_evaluator(dataset_map[args.dataset], model, None, train_iter, args.batch_size, args.gpu)\n    test_evaluator = EvaluatorFactory.get_evaluator(dataset_map[args.dataset], model, None, test_iter, args.batch_size, args.gpu)\n    dev_evaluator = EvaluatorFactory.get_evaluator(dataset_map[args.dataset], model, None, dev_iter, args.batch_size, args.gpu)\n\n    if hasattr(train_evaluator, \'is_multilabel\'):\n        train_evaluator.is_multilabel = dataset_class.IS_MULTILABEL\n    if hasattr(test_evaluator, \'is_multilabel\'):\n        test_evaluator.is_multilabel = dataset_class.IS_MULTILABEL\n    if hasattr(dev_evaluator, \'is_multilabel\'):\n        dev_evaluator.is_multilabel = dataset_class.IS_MULTILABEL\n\n    trainer_config = {\n        \'optimizer\': optimizer,\n        \'batch_size\': args.batch_size,\n        \'log_interval\': args.log_every,\n        \'patience\': args.patience,\n        \'model_outfile\': args.save_path,\n        \'logger\': logger,\n        \'is_multilabel\': dataset_class.IS_MULTILABEL\n    }\n\n    trainer = TrainerFactory.get_trainer(args.dataset, model, None, train_iter, trainer_config, train_evaluator, test_evaluator, dev_evaluator)\n\n    if not args.trained_model:\n        trainer.train(args.epochs)\n    else:\n        if args.cuda:\n            model = torch.load(args.trained_model, map_location=lambda storage, location: storage.cuda(args.gpu))\n        else:\n            model = torch.load(args.trained_model, map_location=lambda storage, location: storage)\n\n    # Calculate dev and test metrics\n    if hasattr(trainer, \'snapshot_path\'):\n        model = torch.load(trainer.snapshot_path)\n\n    evaluate_dataset(\'dev\', dataset_map[args.dataset], model, None, dev_iter, args.batch_size,\n                     is_multilabel=dataset_class.IS_MULTILABEL,\n                     device=args.gpu)\n    evaluate_dataset(\'test\', dataset_map[args.dataset], model, None, test_iter, args.batch_size,\n                     is_multilabel=dataset_class.IS_MULTILABEL,\n                     device=args.gpu)\n'"
models/xml_cnn/args.py,0,"b""import os\nimport models.args\n\n\ndef get_args():\n    parser = models.args.get_args()\n\n    parser.add_argument('--mode', type=str, default='multichannel', choices=['rand', 'static', 'non-static', 'multichannel'])\n    parser.add_argument('--dataset', type=str, default='Reuters', choices=['Reuters', 'AAPD', 'IMDB', 'Yelp2014'])\n    parser.add_argument('--dev-every', type=int, default=30)\n    parser.add_argument('--output-channel', type=int, default=100)\n    parser.add_argument('--words-dim', type=int, default=300)\n    parser.add_argument('--embed-dim', type=int, default=300)\n    parser.add_argument('--dropout', type=float, default=0.5)\n    parser.add_argument('--epoch-decay', type=int, default=15)\n    parser.add_argument('--weight-decay', type=float, default=0)\n\n    parser.add_argument('--num-bottleneck-hidden', type=int, default=512)\n    parser.add_argument('--dynamic-pool-length', type=int, default=32)\n\n    parser.add_argument('--word-vectors-dir', default=os.path.join(os.pardir, 'hedwig-data', 'embeddings', 'word2vec'))\n    parser.add_argument('--word-vectors-file', default='GoogleNews-vectors-negative300.txt')\n    parser.add_argument('--save-path', type=str, default=os.path.join('model_checkpoints', 'xml_cnn'))\n    parser.add_argument('--resume-snapshot', type=str)\n    parser.add_argument('--trained-model', type=str)\n\n    args = parser.parse_args()\n    return args\n"""
models/xml_cnn/model.py,5,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass XmlCNN(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        dataset = config.dataset\n        self.output_channel = config.output_channel\n        target_class = config.target_class\n        words_num = config.words_num\n        words_dim = config.words_dim\n        self.mode = config.mode\n        self.num_bottleneck_hidden = config.num_bottleneck_hidden\n        self.dynamic_pool_length = config.dynamic_pool_length\n        self.ks = 3 # There are three conv nets here\n\n        input_channel = 1\n        if config.mode == \'rand\':\n            rand_embed_init = torch.Tensor(words_num, words_dim).uniform_(-0.25, 0.25)\n            self.embed = nn.Embedding.from_pretrained(rand_embed_init, freeze=False)\n        elif config.mode == \'static\':\n            self.static_embed = nn.Embedding.from_pretrained(dataset.TEXT_FIELD.vocab.vectors, freeze=True)\n        elif config.mode == \'non-static\':\n            self.non_static_embed = nn.Embedding.from_pretrained(dataset.TEXT_FIELD.vocab.vectors, freeze=False)\n        elif config.mode == \'multichannel\':\n            self.static_embed = nn.Embedding.from_pretrained(dataset.TEXT_FIELD.vocab.vectors, freeze=True)\n            self.non_static_embed = nn.Embedding.from_pretrained(dataset.TEXT_FIELD.vocab.vectors, freeze=False)\n            input_channel = 2\n        else:\n            print(""Unsupported Mode"")\n            exit()\n\n        ## Different filter sizes in xml_cnn than kim_cnn\n        self.conv1 = nn.Conv2d(input_channel, self.output_channel, (2, words_dim), padding=(1,0))\n        self.conv2 = nn.Conv2d(input_channel, self.output_channel, (4, words_dim), padding=(3,0))\n        self.conv3 = nn.Conv2d(input_channel, self.output_channel, (8, words_dim), padding=(7,0))\n\n        self.dropout = nn.Dropout(config.dropout)\n        self.bottleneck = nn.Linear(self.ks * self.output_channel * self.dynamic_pool_length, self.num_bottleneck_hidden)\n        self.fc1 = nn.Linear(self.num_bottleneck_hidden, target_class)\n\n        self.pool = nn.AdaptiveMaxPool1d(self.dynamic_pool_length) #Adaptive pooling\n\n    def forward(self, x, **kwargs):\n        if self.mode == \'rand\':\n            word_input = self.embed(x) # (batch, sent_len, embed_dim)\n            x = word_input.unsqueeze(1) # (batch, channel_input, sent_len, embed_dim)\n        elif self.mode == \'static\':\n            static_input = self.static_embed(x)\n            x = static_input.unsqueeze(1) # (batch, channel_input, sent_len, embed_dim)\n        elif self.mode == \'non-static\':\n            non_static_input = self.non_static_embed(x)\n            x = non_static_input.unsqueeze(1) # (batch, channel_input, sent_len, embed_dim)\n        elif self.mode == \'multichannel\':\n            non_static_input = self.non_static_embed(x)\n            static_input = self.static_embed(x)\n            x = torch.stack([non_static_input, static_input], dim=1) # (batch, channel_input=2, sent_len, embed_dim)\n        else:\n            print(""Unsupported Mode"")\n            exit()\n        x = [F.relu(self.conv1(x)).squeeze(3), F.relu(self.conv2(x)).squeeze(3), F.relu(self.conv3(x)).squeeze(3)]\n        x = [self.pool(i).squeeze(2) for i in x]\n\n        # (batch, channel_output) * ks\n        x = torch.cat(x, 1) # (batch, channel_output * ks)\n        x = F.relu(self.bottleneck(x.view(-1, self.ks * self.output_channel * self.dynamic_pool_length)))\n        x = self.dropout(x)\n        logit = self.fc1(x) # (batch, target_size)\n        return logit\n'"
tasks/relevance_transfer/__init__.py,0,b''
tasks/relevance_transfer/__main__.py,13,"b'import json\nimport os\nimport pickle\nimport random\nfrom collections import defaultdict\nfrom copy import deepcopy\n\nimport numpy as np\nimport torch\nfrom tqdm import tqdm\nfrom transformers import BertForSequenceClassification as Bert, AdamW, WarmupLinearSchedule, BertTokenizer\n\nfrom common.constants import BERT_MODELS, PRETRAINED_MODEL_ARCHIVE_MAP, PRETRAINED_VOCAB_ARCHIVE_MAP\nfrom common.constants import LOG_HEADER, LOG_TEMPLATE\nfrom common.evaluators.relevance_transfer_evaluator import RelevanceTransferEvaluator\nfrom common.trainers.relevance_transfer_trainer import RelevanceTransferTrainer\nfrom datasets.bert_processors.robust45_processor import Robust45Processor\nfrom datasets.robust04 import Robust04, Robust04Hierarchical\nfrom datasets.robust05 import Robust05, Robust05Hierarchical\nfrom datasets.robust45 import Robust45, Robust45Hierarchical\nfrom models.han.model import HAN\nfrom models.kim_cnn.model import KimCNN\nfrom models.reg_lstm.model import RegLSTM\nfrom models.xml_cnn.model import XmlCNN\nfrom tasks.relevance_transfer.args import get_args\nfrom tasks.relevance_transfer.rerank import rerank\n\n\nclass UnknownWordVecCache(object):\n    """"""\n    Caches the first randomly generated word vector for a certain size to make it is reused.\n    """"""\n    cache = {}\n\n    @classmethod\n    def unk(cls, tensor):\n        size_tup = tuple(tensor.size())\n        if size_tup not in cls.cache:\n            cls.cache[size_tup] = torch.Tensor(tensor.size())\n            cls.cache[size_tup].uniform_(-0.25, 0.25)\n        return cls.cache[size_tup]\n\n\ndef evaluate_split(model, topic, split, config, **kwargs):\n    evaluator_config = {\n        \'model\': config.model,\n        \'topic\': topic,\n        \'split\': split,\n        \'dataset\': kwargs[\'dataset\'],\n        \'batch_size\': config.batch_size,\n        \'ignore_lengths\': False,\n        \'is_lowercase\': True,\n        \'gradient_accumulation_steps\': config.gradient_accumulation_steps,\n        \'max_seq_length\': config.max_seq_length,\n        \'max_doc_length\': args.max_doc_length,\n        \'data_dir\': config.data_dir,\n        \'n_gpu\': n_gpu,\n        \'device\': config.device\n    }\n\n    if config.model in {\'HAN\', \'HR-CNN\'}:\n        trainer_config[\'ignore_lengths\'] = True\n        evaluator_config[\'ignore_lengths\'] = True\n\n    evaluator = RelevanceTransferEvaluator(model, evaluator_config,\n                                           processor=kwargs[\'processor\'],\n                                           tokenizer=kwargs[\'tokenizer\'],\n                                           data_loader=kwargs[\'loader\'],\n                                           dataset=kwargs[\'dataset\'])\n\n    accuracy, precision, recall, f1, avg_loss = evaluator.get_scores()[0]\n\n    if split == \'test\':\n        pred_scores[topic] = (evaluator.y_pred, evaluator.docid)\n    else:\n        print(\'\\n\' + LOG_HEADER)\n        print(LOG_TEMPLATE.format(topic, accuracy, precision, recall, f1, avg_loss) + \'\\n\')\n\n    return evaluator.y_pred\n\n\ndef save_ranks(pred_scores, output_path):\n    with open(output_path, \'w\') as output_file:\n        for topic in tqdm(pred_scores, desc=\'Saving\'):\n            scores, docid = pred_scores[topic]\n            max_scores = defaultdict(list)\n            for score, docid in zip(scores, docid):\n                max_scores[docid].append(score)\n            sorted_score = sorted(((sum(scores) / len(scores), docid) for docid, scores in max_scores.items()),\n                                  reverse=True)\n            rank = 1  # Reset rank counter to one\n            for score, docid in sorted_score:\n                output_file.write(f\'{topic} Q0 {docid} {rank} {score} Castor\\n\')\n                rank += 1\n\n\nif __name__ == \'__main__\':\n    # Set default configuration in args.py\n    args = get_args()\n\n    if torch.cuda.is_available() and not args.cuda:\n        print(\'Warning: Using CPU for training\')\n\n    device = torch.device(""cuda"" if torch.cuda.is_available() and args.cuda else ""cpu"")\n    n_gpu = torch.cuda.device_count()\n    args.device = device\n    args.n_gpu = n_gpu\n    args.num_labels = 1\n\n    print(\'Device:\', str(device).upper())\n    print(\'Number of GPUs:\', n_gpu)\n\n    # Set random seed for reproducibility\n    torch.manual_seed(args.seed)\n    torch.backends.cudnn.deterministic = True\n    np.random.seed(args.seed)\n    random.seed(args.seed)\n\n    dataset_map = {\n        \'Robust04\': Robust04,\n        \'Robust45\': Robust45,\n        \'Robust05\': Robust05\n    }\n\n    dataset_map_hier = {\n        \'Robust04\': Robust04Hierarchical,\n        \'Robust45\': Robust45Hierarchical,\n        \'Robust05\': Robust05Hierarchical\n    }\n\n    dataset_map_bert = {\n        \'Robust45\': Robust45Processor,\n        \'Robust04\': None,\n        \'Robust05\': None\n    }\n\n    model_map = {\n        \'RegLSTM\': RegLSTM,\n        \'KimCNN\': KimCNN,\n        \'HAN\': HAN,\n        \'XML-CNN\': XmlCNN,\n        \'BERT-Base\': Bert,\n        \'BERT-Large\': Bert\n    }\n\n    if args.dataset not in dataset_map:\n        raise ValueError(\'Unrecognized dataset\')\n    else:\n        print(\'Dataset:\', args.dataset)\n\n        if args.model in {\'HAN\', \'HR-CNN\'}:\n            dataset = dataset_map_hier[args.dataset]\n        elif args.model in BERT_MODELS:\n            dataset = dataset_map_bert[args.dataset]\n        else:\n            dataset = dataset_map[args.dataset]\n\n    if args.rerank:\n        rerank(args, dataset)\n        exit(0)\n\n    topic_iter = 0\n    cache_path = os.path.splitext(args.output_path)[0] + \'.pkl\'\n    save_path = os.path.join(args.save_path, dataset_map[args.dataset].NAME)\n    os.makedirs(save_path, exist_ok=True)\n\n    if args.resume_snapshot:\n        # Load previous cached run\n        with open(cache_path, \'rb\') as cache_file:\n            pred_scores = pickle.load(cache_file)\n    else:\n        pred_scores = dict()\n\n    if args.model in BERT_MODELS:\n        if args.gradient_accumulation_steps < 1:\n            raise ValueError(""Invalid gradient_accumulation_steps parameter:"", args.gradient_accumulation_steps)\n\n        processor = dataset_map_bert[args.dataset]()\n        args.is_lowercase = \'uncased\' in args.model\n        variant = \'bert-large-uncased\' if args.model == \'BERT-Large\' else \'bert-base-uncased\'\n\n        for topic in dataset.TOPICS:\n            topic_iter += 1\n            # Skip topics that have already been predicted\n            if args.resume_snapshot and topic in pred_scores:\n                continue\n\n            print(""Training on topic %d of %d..."" % (topic_iter, len(dataset.TOPICS)))\n            args.batch_size = args.batch_size // args.gradient_accumulation_steps\n            train_examples = processor.get_train_examples(args.data_dir, topic=topic)\n            num_train_optimization_steps = int(\n                len(train_examples) / args.batch_size / args.gradient_accumulation_steps) * args.epochs\n\n            if args.model in BERT_MODELS:\n                pretrained_model_path = PRETRAINED_MODEL_ARCHIVE_MAP[variant]\n                model = model_map[args.model].from_pretrained(pretrained_model_path, num_labels=1)\n            else:\n                model = model_map[args.model](args)\n\n            model.to(device)\n            if n_gpu > 1:\n                model = torch.nn.DataParallel(model)\n\n            # Prepare optimizer\n            param_optimizer = list(model.named_parameters())\n            no_decay = [\'bias\', \'LayerNorm.bias\', \'LayerNorm.weight\']\n            optimizer_grouped_parameters = [\n                {\'params\': [p for n, p in param_optimizer if\n                            \'sentence_encoder\' not in n],\n                 \'lr\': args.lr * args.lr_mult, \'weight_decay\': 0.0},\n                {\'params\': [p for n, p in param_optimizer if\n                            \'sentence_encoder\' in n and not any(nd in n for nd in no_decay)],\n                 \'weight_decay\': 0.01},\n                {\'params\': [p for n, p in param_optimizer if\n                            \'sentence_encoder\' in n and any(nd in n for nd in no_decay)],\n                 \'weight_decay\': 0.0}]\n\n            pretrained_vocab_path = PRETRAINED_VOCAB_ARCHIVE_MAP[variant]\n            tokenizer = BertTokenizer.from_pretrained(pretrained_vocab_path)\n            optimizer = AdamW(optimizer_grouped_parameters, lr=args.lr, weight_decay=0.01, correct_bias=False)\n            scheduler = WarmupLinearSchedule(optimizer, t_total=num_train_optimization_steps,\n                                             warmup_steps=args.warmup_proportion * num_train_optimization_steps)\n\n            trainer_config = {\n                \'model\': args.model,\n                \'topic\': topic,\n                \'dataset\': dataset,\n                \'batch_size\': args.batch_size,\n                \'patience\': args.patience,\n                \'epochs\': args.epochs,\n                \'is_lowercase\': True,\n                \'gradient_accumulation_steps\': args.gradient_accumulation_steps,\n                \'max_seq_length\': args.max_seq_length,\n                \'max_doc_length\': args.max_doc_length,\n                \'data_dir\': args.data_dir,\n                \'save_path\': args.save_path,\n                \'n_gpu\': n_gpu,\n                \'device\': args.device\n            }\n\n            evaluator_config = {\n                \'model\': args.model,\n                \'topic\': topic,\n                \'dataset\': dataset,\n                \'split\': \'dev\',\n                \'batch_size\': args.batch_size,\n                \'ignore_lengths\': True,\n                \'is_lowercase\': True,\n                \'gradient_accumulation_steps\': args.gradient_accumulation_steps,\n                \'max_seq_length\': args.max_seq_length,\n                \'max_doc_length\': args.max_doc_length,\n                \'data_dir\': args.data_dir,\n                \'n_gpu\': n_gpu,\n                \'device\': args.device\n            }\n\n            dev_evaluator = RelevanceTransferEvaluator(model, evaluator_config, dataset=dataset, processor=processor,\n                                                       tokenizer=tokenizer)\n            trainer = RelevanceTransferTrainer(model, trainer_config, optimizer=optimizer, processor=processor,\n                                               tokenizer=tokenizer, scheduler=scheduler, dev_evaluator=dev_evaluator)\n\n            trainer.train(args.epochs)\n            model = torch.load(trainer.snapshot_path)\n\n            # Calculate dev and test metrics\n            evaluate_split(model, topic, \'dev\', args, dataset=dataset, processor=processor, tokenizer=tokenizer, loader=None)\n            evaluate_split(model, topic, \'test\', args, dataset=dataset, processor=processor, tokenizer=tokenizer, loader=None)\n\n            with open(cache_path, \'wb\') as cache_file:\n                pickle.dump(pred_scores, cache_file)\n\n    else:\n        if not args.cuda:\n            args.gpu = -1\n        if torch.cuda.is_available() and args.cuda:\n            torch.cuda.set_device(args.gpu)\n            torch.cuda.manual_seed(args.seed)\n\n        with open(os.path.join(\'tasks\', \'relevance_transfer\', \'config.json\'), \'r\') as config_file:\n            topic_configs = json.load(config_file)\n\n        for topic in dataset.TOPICS:\n            topic_iter += 1\n            # Skip topics that have already been predicted\n            if args.resume_snapshot and topic in pred_scores:\n                continue\n\n            print(""Training on topic %d of %d..."" % (topic_iter, len(dataset.TOPICS)))\n            train_iter, dev_iter, test_iter = dataset.iters(args.data_dir, args.word_vectors_file,\n                                                            args.word_vectors_dir, topic,\n                                                            batch_size=args.batch_size, device=device,\n                                                            unk_init=UnknownWordVecCache.unk)\n\n            print(\'Vocabulary size:\', len(train_iter.dataset.TEXT_FIELD.vocab))\n            print(\'Target Classes:\', train_iter.dataset.NUM_CLASSES)\n            print(\'Train Instances:\', len(train_iter.dataset))\n            print(\'Dev Instances:\', len(dev_iter.dataset))\n            print(\'Test Instances:\', len(test_iter.dataset))\n\n            config = deepcopy(args)\n            config.target_class = 1\n            config.dataset = train_iter.dataset\n            config.words_num = len(train_iter.dataset.TEXT_FIELD.vocab)\n\n            if args.variable_dynamic_pool:\n                # Set dynamic pool length based on topic configs\n                if args.model in topic_configs and topic in topic_configs[args.model]:\n                    print(""Setting dynamic_pool to"", topic_configs[args.model][topic][""dynamic_pool""])\n                    config.dynamic_pool = topic_configs[args.model][topic][""dynamic_pool""]\n                    if config.dynamic_pool:\n                        print(""Setting dynamic_pool_length to"",\n                              topic_configs[args.model][topic][""dynamic_pool_length""])\n                        config.dynamic_pool_length = topic_configs[args.model][topic][""dynamic_pool_length""]\n\n            model = model_map[args.model](config)\n\n            if args.cuda:\n                model.cuda()\n                print(\'Shifting model to GPU...\')\n\n            parameter = filter(lambda p: p.requires_grad, model.parameters())\n            optimizer = torch.optim.Adam(parameter, lr=args.lr, weight_decay=args.weight_decay)\n\n            trainer_config = {\n                \'model\': args.model,\n                \'dataset\': dataset,\n                \'batch_size\': args.batch_size,\n                \'patience\': args.patience,\n                \'resample\': args.resample,\n                \'epochs\': args.epochs,\n                \'is_lowercase\': True,\n                \'gradient_accumulation_steps\': args.gradient_accumulation_steps,\n                \'data_dir\': args.data_dir,\n                \'save_path\': args.save_path,\n                \'device\': args.gpu\n            }\n\n            evaluator_config = {\n                \'topic\': topic,\n                \'model\': args.model,\n                \'dataset\': dataset,\n                \'batch_size\': args.batch_size,\n                \'ignore_lengths\': False,\n                \'data_dir\': args.data_dir,\n                \'device\': args.gpu\n            }\n\n            if args.model in {\'HAN\', \'HR-CNN\'}:\n                trainer_config[\'ignore_lengths\'] = True\n                evaluator_config[\'ignore_lengths\'] = True\n\n            test_evaluator = RelevanceTransferEvaluator(model, evaluator_config, dataset=dataset, data_loader=test_iter)\n            dev_evaluator = RelevanceTransferEvaluator(model, evaluator_config, dataset=dataset, data_loader=dev_iter)\n            trainer = RelevanceTransferTrainer(model, trainer_config, train_loader=train_iter, optimizer=optimizer,\n                                               test_evaluator=test_evaluator, dev_evaluator=dev_evaluator)\n\n            trainer.train(args.epochs)\n            model = torch.load(trainer.snapshot_path)\n\n            if hasattr(model, \'beta_ema\') and model.beta_ema > 0:\n                old_params = model.get_params()\n                model.load_ema_params()\n\n            # Calculate dev and test metrics model, topic, split, config\n            evaluate_split(model, topic, \'dev\', args, dataset=dataset, loader=dev_iter, processor=None, tokenizer=None)\n            evaluate_split(model, topic, \'test\', args, dataset=dataset, loader=test_iter, processor=None, tokenizer=None)\n\n            if hasattr(model, \'beta_ema\') and model.beta_ema > 0:\n                model.load_params(old_params)\n\n            with open(cache_path, \'wb\') as cache_file:\n                pickle.dump(pred_scores, cache_file)\n\n    save_ranks(pred_scores, args.output_path)\n'"
tasks/relevance_transfer/args.py,0,"b'import os\n\nfrom argparse import ArgumentParser\n\n\ndef get_args():\n    parser = ArgumentParser(description=""Deep learning models for relevance transfer"")\n    parser.add_argument(\'--no-cuda\', action=\'store_false\', dest=\'cuda\')\n    parser.add_argument(\'--gpu\', type=int, default=0)\n    parser.add_argument(\'--epochs\', type=int, default=50)\n    parser.add_argument(\'--batch-size\', type=int, default=1024)\n    parser.add_argument(\'--mode\', type=str, default=\'static\', choices=[\'rand\', \'static\', \'non-static\', \'multichannel\'])\n    parser.add_argument(\'--lr\', type=float, default=0.001)\n    parser.add_argument(\'--lr-mult\', type=float, default=1)\n    parser.add_argument(\'--seed\', type=int, default=3435)\n    parser.add_argument(\'--dataset\', type=str, default=\'Robust04\', choices=[\'Robust04\', \'Robust05\', \'Robust45\'])\n    parser.add_argument(\'--model\', type=str, default=\'KimCNN\', choices=[\'RegLSTM\', \'KimCNN\', \'HAN\', \'XML-CNN\', \'BERT-Base\',\n                                                                        \'BERT-Large\', \'HBERT-Base\', \'HBERT-Large\'])\n\n    parser.add_argument(\'--dev_every\', type=int, default=30)\n    parser.add_argument(\'--log_every\', type=int, default=10)\n    parser.add_argument(\'--patience\', type=int, default=5)\n    parser.add_argument(\'--save_path\', type=str, default=os.path.join(\'model_checkpoints\', \'relevance_transfer\'))\n    parser.add_argument(\'--words_dim\', type=int, default=300)\n    parser.add_argument(\'--embed_dim\', type=int, default=300)\n    parser.add_argument(\'--dropout\', type=float, default=0.5)\n    parser.add_argument(\'--epoch_decay\', type=int, default=15)\n    parser.add_argument(\'--data_dir\', default=os.path.join(os.pardir, \'hedwig-data\', \'datasets\'))\n    parser.add_argument(\'--word_vectors_dir\', default=os.path.join(os.pardir, \'hedwig-data\', \'embeddings\', \'word2vec\'))\n    parser.add_argument(\'--word_vectors_file\', help=\'word vectors filename\', default=\'GoogleNews-vectors-negative300.txt\')\n    parser.add_argument(""--output-path"", type=str, default=""run.core17.lstm.topics.robust00.txt"")\n    parser.add_argument(\'--resume-snapshot\', action=\'store_true\')\n    parser.add_argument(\'--resample\', action=\'store_true\')\n\n    # RegLSTM parameters\n    parser.add_argument(\'--num-layers\', type=int, default=2)\n    parser.add_argument(\'--hidden-dim\', type=int, default=256)\n    parser.add_argument(\'--bidirectional\', action=\'store_true\')\n    parser.add_argument(\'--tar\', action=\'store_true\')\n    parser.add_argument(\'--weight-decay\', type=float, default=0)\n    parser.add_argument(\'--beta-ema\', type=float, default=0, help=""for temporal averaging"")\n    parser.add_argument(\'--wdrop\', type=float, default=0.0, help=""for weight-drop"")\n    parser.add_argument(\'--embed-droprate\', type=float, default=0.0, help=""for embedded dropout"")\n\n    # KimCNN parameters\n    parser.add_argument(\'--dropblock\', type=float, default=0.0)\n    parser.add_argument(\'--dropblock-size\', type=int, default=7)\n    parser.add_argument(\'--batchnorm\', action=\'store_true\')\n    parser.add_argument(\'--attention\', action=\'store_true\')\n    parser.add_argument(\'--output-channel\', type=int, default=100)\n\n    # HAN parameters\n    parser.add_argument(\'--word-num-hidden\', type=int, default=50)\n    parser.add_argument(\'--sentence-num-hidden\', type=int, default=50)\n\n    # XML-CNN parameters\n    parser.add_argument(\'--bottleneck-layer\', action=\'store_true\')\n    parser.add_argument(\'--dynamic-pool\', action=\'store_true\')\n    parser.add_argument(\'--variable-dynamic-pool\', action=\'store_true\')\n    parser.add_argument(\'--bottleneck-units\', type=int, default=100)\n    parser.add_argument(\'--dynamic-pool-length\', type=int, default=8)\n\n    # HR-CNN parameters\n    parser.add_argument(\'--sentence-channel\', type=int, default=100)\n\n    # BERT parameters\n    parser.add_argument(\'--cache-dir\', default=\'cache\', type=str)\n    parser.add_argument(\'--variant\', type=str, choices=[\'bert-base-uncased\', \'bert-large-uncased\', \'bert-base-cased\', \'bert-large-cased\'])\n    parser.add_argument(\'--max-seq-length\', default=128, type=int)\n    parser.add_argument(\'--max-doc-length\', default=16, type=int)\n    parser.add_argument(\'--warmup-proportion\', default=0.1, type=float)\n    parser.add_argument(\'--gradient-accumulation-steps\', type=int, default=1)\n    parser.add_argument(\'--loss-scale\', type=float, default=0)\n\n    # Re-ranking parameters\n    parser.add_argument(\'--rerank\', action=\'store_true\')\n    parser.add_argument(""--ret-ranks"", type=str, help=\'retrieval rank file\', default=""run.core17.bm25+rm3.wcro0405.hits10000.txt"")\n    parser.add_argument(""--clf-ranks"", type=str, help=\'classification rank file\', default=""run.core17.lstm.topics.robust45.txt"")\n\n    args = parser.parse_args()\n    return args\n'"
tasks/relevance_transfer/rerank.py,0,"b'import os\n\nimport numpy as np\n\n\ndef load_ranks(rank_file):\n    score_dict = {}\n    with open(rank_file, \'r\') as f:\n        for line in f:\n            topic, _, docid, _, score, _ = line.split()\n            if topic not in score_dict:\n                score_dict[topic] = dict()\n            score_dict[topic.strip()][docid.strip()] = float(score)\n    return score_dict\n\n\ndef merge_ranks(old_ranks, new_ranks, topics):\n    doc_ranks = dict()\n    for topic in topics:\n        missing_docids = list()\n        old_scores = old_ranks[topic]\n        new_scores = new_ranks[topic]\n        if topic not in doc_ranks:\n            doc_ranks[topic] = list(), list(), list()\n        print(""Processing documents in topic"", topic)\n        for docid, old_score in old_scores.items():\n            try:\n                new_score = new_scores[docid]\n                doc_ranks[topic][0].append(docid)\n                doc_ranks[topic][1].append(old_score)\n                doc_ranks[topic][2].append(new_score)\n            except KeyError:\n                missing_docids.append(docid)\n        print(""Number of missing documents in topic %s: %d"" % (topic, len(missing_docids)))\n    return doc_ranks\n\n\ndef interpolate(old_scores, new_scores, alpha):\n    s_min, s_max = min(old_scores), max(old_scores)\n    old_score = (old_scores - s_min) / (s_max - s_min)\n    s_min, s_max = min(new_scores), max(new_scores)\n    new_score = (new_scores - s_min) / (s_max - s_min)\n    score = old_score * (1 - alpha) + new_score * alpha\n    return score\n\n\ndef rerank_alpha(doc_ranks, alpha, limit, filename, tag):\n    filename = \'%s_rerank_%0.1f.txt\' % (filename, alpha)\n    with open(os.path.join(filename), \'w\') as f:\n        print(\'Writing output for alpha\', alpha)\n        for topic in doc_ranks:\n            docids, old_scores, new_scores = doc_ranks[topic]\n            score = interpolate(np.array(old_scores), np.array(new_scores), alpha)\n            sorted_score = sorted(list(zip(docids, score)), key=lambda x: -x[1])\n\n            rank = 1\n            for docids, score in sorted_score:\n                f.write(f\'{topic} Q0 {docids} {rank} {score} castor_{tag}\\n\')\n                rank += 1\n                if rank > limit:\n                    break\n\n\ndef rerank(args, dataset):\n    ret_ranks = load_ranks(args.ret_ranks)\n    clf_ranks = load_ranks(args.clf_ranks)\n    doc_ranks = merge_ranks(ret_ranks, clf_ranks, topics=dataset.TOPICS)\n\n    filename = os.path.splitext(args.clf_ranks)[0]\n    for alpha in [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]:\n        rerank_alpha(doc_ranks, alpha, 10000, filename, tag=""achyudh"")'"
tasks/relevance_transfer/resample.py,3,"b'# MIT License\n#\n# Copyright (c) 2018 Ming\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n#\n# Modified by: Achyudh Keshav Ram\n# On: 7th Feb 2019\n\nimport torch\nimport torch.utils.data\n\n\nclass ImbalancedDatasetSampler:\n    """"""Samples elements randomly from a given list of indices for imbalanced dataset\n    Arguments:\n        indices (list, optional): a list of indices\n        num_samples (int, optional): number of samples to draw\n    """"""\n\n    def __init__(self, dataset, labels, indices=None, num_samples=None):\n\n        # All elements in the dataset will be considered if indices is None\n        self.indices = list(range(len(dataset))) if indices is None else indices\n        self.num_samples = len(self.indices) if num_samples is None else num_samples\n\n        # Compute distribution of classes in the dataset\n        self.labels = labels\n        label_to_count = dict()\n        for idx in self.indices:\n            label = self.labels[idx].item()\n            if label in label_to_count:\n                label_to_count[label] += 1\n            else:\n                label_to_count[label] = 1\n\n        # Compute weight for each sample\n        weights = [1.0 / label_to_count[self.labels[idx].item()]\n                   for idx in self.indices]\n        self.weights = torch.DoubleTensor(weights)\n\n    def get_indices(self):\n        return list(self.indices[i] for i in torch.multinomial(\n            self.weights, self.num_samples, replacement=True))\n'"
