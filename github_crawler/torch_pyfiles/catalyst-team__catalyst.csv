file_path,api_count,code
setup.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n# Note: To use the ""upload"" functionality of this file, you must:\n#   $ pip install twine\n\nimport io\nimport os\nfrom shutil import rmtree\nimport sys\n\nfrom setuptools import Command, find_packages, setup\n\n# Package meta-data.\nNAME = ""catalyst""\nDESCRIPTION = ""Catalyst. PyTorch framework for DL research and development.""\nURL = ""https://github.com/catalyst-team/catalyst""\nEMAIL = ""scitator@gmail.com""\nAUTHOR = ""Sergey Kolesnikov""\nREQUIRES_PYTHON = "">=3.6.0""\n\nPROJECT_ROOT = os.path.abspath(os.path.dirname(__file__))\n\n\ndef load_requirements(filename):\n    """"""\n    @TODO: Docs. Contribution is welcome\n    """"""\n    with open(os.path.join(PROJECT_ROOT, filename), ""r"") as f:\n        return f.read().splitlines()\n\n\ndef load_readme():\n    """"""\n    @TODO: Docs. Contribution is welcome\n    """"""\n    readme_path = os.path.join(PROJECT_ROOT, ""README.md"")\n    with io.open(readme_path, encoding=""utf-8"") as f:\n        return ""\\n"" + f.read()\n\n\ndef load_version():\n    """"""\n    @TODO: Docs. Contribution is welcome\n    """"""\n    context = {}\n    with open(os.path.join(PROJECT_ROOT, ""catalyst"", ""__version__.py"")) as f:\n        exec(f.read(), context)\n    return context[""__version__""]\n\n\nclass UploadCommand(Command):\n    """"""Support setup.py upload.""""""\n\n    description = ""Build and publish the package.""\n    user_options = []\n\n    @staticmethod\n    def status(s):\n        """"""Prints things in bold.""""""\n        print(""\\033[1m{0}\\033[0m"".format(s))\n\n    def initialize_options(self):\n        """"""\n        @TODO: Docs. Contribution is welcome\n        """"""\n        pass\n\n    def finalize_options(self):\n        """"""\n        @TODO: Docs. Contribution is welcome\n        """"""\n        pass\n\n    def run(self):\n        """"""\n        @TODO: Docs. Contribution is welcome\n        """"""\n        try:\n            self.status(""Removing previous builds\xe2\x80\xa6"")\n            rmtree(os.path.join(PROJECT_ROOT, ""dist""))\n        except OSError:\n            pass\n\n        self.status(""Building Source and Wheel (universal) distribution\xe2\x80\xa6"")\n        os.system(\n            ""{0} setup.py sdist bdist_wheel --universal"".format(sys.executable)\n        )\n\n        self.status(""Uploading the package to PyPI via Twine\xe2\x80\xa6"")\n        os.system(""twine upload dist/*"")\n\n        self.status(""Pushing git tags\xe2\x80\xa6"")\n        os.system(""git tag v{0}"".format(load_version()))\n        os.system(""git push --tags"")\n\n        sys.exit()\n\n\n# Specific dependencies.\nextras = {\n    ""contrib"": load_requirements(""requirements/requirements-contrib.txt""),\n    ""cv"": load_requirements(""requirements/requirements-cv.txt""),\n    # ""dev"": load_requirements(""requirements/requirements-dev.txt""),\n    ""ecosystem"": load_requirements(""requirements/requirements-ecosystem.txt""),\n    ""ml"": load_requirements(""requirements/requirements-ml.txt""),\n    ""nlp"": load_requirements(""requirements/requirements-nlp.txt""),\n}\nextras[""contrib""] += extras[""ecosystem""] + extras[""cv""] + extras[""nlp""]\n\n# Meta dependency groups.\nall_deps = []\nfor group_name in extras:\n    all_deps += extras[group_name]\nextras[""all""] = all_deps\n\nsetup(\n    name=NAME,\n    version=load_version(),\n    description=DESCRIPTION,\n    long_description=load_readme(),\n    long_description_content_type=""text/markdown"",\n    keywords=[\n        ""Machine Learning"",\n        ""Distributed Computing"",\n        ""Deep Learning"",\n        # ""Reinforcement Learning"",\n        ""Computer Vision"",\n        ""Natural Language Processing"",\n        ""Recommendation Systems"",\n        ""Information Retrieval"",\n        ""PyTorch"",\n    ],\n    author=AUTHOR,\n    author_email=EMAIL,\n    python_requires=REQUIRES_PYTHON,\n    url=URL,\n    download_url=URL,\n    project_urls={\n        ""Bug Tracker"": ""https://github.com/catalyst-team/catalyst/issues"",\n        ""Documentation"": ""https://catalyst-team.github.io/catalyst"",\n        ""Source Code"": ""https://github.com/catalyst-team/catalyst"",\n    },\n    packages=find_packages(exclude=(""tests"",)),\n    entry_points={\n        ""console_scripts"": [\n            ""catalyst-dl=catalyst.dl.__main__:main"",\n            ""catalyst-contrib=catalyst.contrib.__main__:main"",\n            ""catalyst-data=catalyst.data.__main__:main"",\n        ],\n    },\n    scripts=[\n        ""bin/scripts/catalyst-parallel-run"",\n        ""bin/scripts/download-gdrive"",\n        ""bin/scripts/extract-archive"",\n    ],\n    install_requires=load_requirements(""requirements/requirements.txt""),\n    extras_require=extras,\n    include_package_data=True,\n    license=""Apache License 2.0"",\n    classifiers=[\n        ""Environment :: Console"",\n        ""Natural Language :: English"",\n        ""Development Status :: 4 - Beta"",\n        ""Operating System :: OS Independent"",\n        ""License :: OSI Approved :: Apache Software License"",\n        # Audience\n        ""Intended Audience :: Developers"",\n        ""Intended Audience :: Science/Research"",\n        # Topics\n        ""Topic :: Scientific/Engineering :: Artificial Intelligence"",\n        ""Topic :: Scientific/Engineering :: Image Recognition"",\n        ""Topic :: Scientific/Engineering :: Information Analysis"",\n        # Programming\n        ""Programming Language :: Python"",\n        ""Programming Language :: Python :: 3.6"",\n        ""Programming Language :: Python :: 3.7"",\n        ""Programming Language :: Python :: Implementation :: CPython"",\n    ],\n    # $ setup.py publish support.\n    cmdclass={""upload"": UploadCommand},\n)\n'"
catalyst/__init__.py,0,b'from .__version__ import __version__  # noqa: F401\n'
catalyst/__version__.py,0,"b'__version__ = ""20.06""\n'"
docs/conf.py,0,"b'# -*- coding: utf-8 -*-\n#\n# Configuration file for the Sphinx documentation builder.\n#\n# This file does only contain a selection of the most common options. For a\n# full list see the documentation:\n# http://www.sphinx-doc.org/en/master/config\n\n# -- Path setup --------------------------------------------------------------\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\nimport datetime\nimport os\nimport re\nimport sys\n\ncatalyst_root_path = ""../""\nsys.path.insert(0, os.path.abspath(catalyst_root_path))\n\n# -- Project information -----------------------------------------------------\n\nproject = ""Catalyst""\ncopyright = ""{}, Scitator"".format(datetime.datetime.now().year)\nauthor = ""Sergey Kolesnikov""\n\ndocs_repo = ""catalyst""\ndocs_user = ""catalyst-team""\n\nreleases_github_path = ""catalyst-team/catalyst""\n\n\ndef get_version(mode: str = ""full"") -> str:\n    """"""\n    @TODO: Docs. Contribution is welcome\n    """"""\n    current_dir = os.path.abspath(os.path.dirname(__file__))\n    root = os.path.dirname(current_dir)\n    version_file = os.path.join(root, ""catalyst"", ""__version__.py"")\n    if not os.path.exists(version_file):\n        version_file = os.path.join(root, ""__version__.py"")\n\n    version_ = ""1.0""\n    try:\n        with open(version_file) as f:\n            version_ = re.search(\n                r\'^__version__ = [\\\'""]([^\\\'""]*)[\\\'""]\', f.read(), re.M\n            ).group(1)\n    except Exception:\n        pass\n\n    if mode == ""short"":\n        try:\n            version_ = re.search(r""^(\\d+\\.\\d+)"", version_, re.M).group(1)\n        except Exception:\n            pass\n\n    return version_\n\n\n# The short X.Y version\nversion = get_version(""short"")\n# The full version, including alpha/beta/rc tags\nrelease = get_version(""full"")\n\n# -- General configuration ---------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = ""1.0""\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named ""sphinx.ext.*"") or your custom\n# ones.\nextensions = [\n    ""sphinx.ext.autodoc"",\n    ""sphinx.ext.todo"",\n    ""sphinx.ext.coverage"",\n    ""sphinx.ext.mathjax"",\n    ""sphinx.ext.viewcode"",\n    ""sphinx.ext.githubpages"",\n    ""sphinx.ext.napoleon"",\n    # ""releases"",\n]\n\nautodoc_inherit_docstrings = False\nnapoleon_google_docstring = True\nnapoleon_include_init_with_doc = True\nnapoleon_numpy_docstring = False\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [""_templates""]\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\n# source_suffix = ["".rst"", "".md""]\nsource_suffix = "".rst""\n\n# The master toctree document.\nmaster_doc = ""index""\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This pattern also affects html_static_path and html_extra_path.\nexclude_patterns = [""_build"", ""Thumbs.db"", "".DS_Store""]\n\n# Ignoring Third-party packages\n\nautodoc_mock_imports = [\n    ""alchemy"",\n    ""neptune"",\n    ""wandb"",\n    ""gym"",\n    ""gridfs"",\n    ""pymongo"",\n    ""redis"",\n]\n\n# autodoc_default_flags = [\n#     ""members"", ""undoc-members"", ""private-members"",\n#     ""special-members"", ""inherited-members"", ""show-inheritance""\n# ]\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = None\n\n# -- Options for HTML output -------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = ""catalyst_sphinx_theme""\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#\nhtml_theme_options = {}\n# html_theme_options = {\n#     ""display_version"": True,\n#     ""prev_next_buttons_location"": ""bottom"",\n#     ""collapse_navigation"": False,\n#     ""sticky_navigation"": True,\n#     ""navigation_depth"": 4,\n# }\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\n# html_static_path = [""_static""]\n\nhtml_short_title = ""Catalyst DL R&D""\n\n# Custom sidebar templates, must be a dictionary that maps document names\n# to template names.\n#\n# The default sidebars (for documents that don""t match any pattern) are\n# defined by theme itself.  Builtin themes are using these templates by\n# default: ``[""localtoc.html"", ""relations.html"", ""sourcelink.html"",\n# ""searchbox.html""]``.\n#\n# html_sidebars = {}\n\nhtml_context = {\n    ""display_github"": True,\n    ""source_url_prefix"": (\n        f""https://github.com/{docs_user}/{docs_repo}/tree/master/docs""\n    ),\n    ""github_host"": ""github.com"",\n    ""github_user"": docs_user,\n    ""github_repo"": docs_repo,\n    ""github_version"": ""master"",\n    ""conf_py_path"": ""/docs/"",\n    ""source_suffix"": "".rst"",\n}\n\n# -- Options for HTMLHelp output ---------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = ""Catalystdoc""\n\n# -- Options for LaTeX output ------------------------------------------------\n\nlatex_elements = {\n    # The paper size (""letterpaper"" or ""a4paper"").\n    #\n    # ""papersize"": ""letterpaper"",\n    # The font size (""10pt"", ""11pt"" or ""12pt"").\n    #\n    # ""pointsize"": ""10pt"",\n    # Additional stuff for the LaTeX preamble.\n    #\n    # ""preamble"": """",\n    # Latex figure (float) alignment\n    #\n    # ""figure_align"": ""htbp"",\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (\n        master_doc,\n        ""Catalyst.tex"",\n        ""Catalyst Documentation"",\n        ""Scitator"",\n        ""manual"",\n    ),\n]\n\n# -- Options for manual page output ------------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [(master_doc, ""catalyst"", ""Catalyst Documentation"", [author], 1)]\n\n# -- Options for Texinfo output ----------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (\n        master_doc,\n        ""Catalyst"",\n        ""Catalyst Documentation"",\n        author,\n        ""Catalyst"",\n        ""One line description of project."",\n        ""Miscellaneous"",\n    ),\n]\n\n# -- Options for Epub output -------------------------------------------------\n\n# Bibliographic Dublin Core info.\nepub_title = project\n\n# The unique identifier of the text. This can be a ISBN number\n# or the project homepage.\n#\n# epub_identifier = """"\n\n# A unique identification for the text.\n#\n# epub_uid = """"\n\n# A list of files that should not be packed into the epub file.\nepub_exclude_files = [""search.html""]\n\n# -- Extension configuration -------------------------------------------------\n\n# -- Options for todo extension ----------------------------------------------\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = True\n'"
tests/__init__.py,0,b''
catalyst/contrib/__init__.py,0,b''
catalyst/contrib/__main__.py,0,"b'from argparse import ArgumentParser, RawTextHelpFormatter\nfrom collections import OrderedDict\nimport logging\n\nfrom catalyst.contrib.scripts import collect_env, find_thresholds\nfrom catalyst.tools import settings\n\nlogger = logging.getLogger(__name__)\n\nCOMMANDS = OrderedDict(\n    [(""collect-env"", collect_env), (""find-thresholds"", find_thresholds)]\n)\n\ntry:\n    import nmslib  # noqa: F401\n    from catalyst.contrib.scripts import check_index_model, create_index_model\n\n    COMMANDS[""check-index-model""] = check_index_model\n    COMMANDS[""create-index-model""] = create_index_model\nexcept ImportError as ex:\n    if settings.nmslib_required:\n        logger.warning(\n            ""nmslib not available, to install nmslib,""\n            "" run `pip install nmslib`.""\n        )\n        raise ex\n\n\ndef build_parser() -> ArgumentParser:\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    parser = ArgumentParser(\n        ""catalyst-contrib"", formatter_class=RawTextHelpFormatter\n    )\n    all_commands = "", \\n"".join(map(lambda x: f""    {x}"", COMMANDS.keys()))\n\n    subparsers = parser.add_subparsers(\n        metavar=""{command}"",\n        dest=""command"",\n        help=f""available commands: \\n{all_commands}"",\n    )\n    subparsers.required = True\n\n    for key, value in COMMANDS.items():\n        value.build_args(subparsers.add_parser(key))\n\n    return parser\n\n\ndef main():\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    parser = build_parser()\n\n    args, uargs = parser.parse_known_args()\n\n    COMMANDS[args.command].main(args, uargs)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
catalyst/contrib/registry.py,2,"b'""""""\ncatalyst subpackage registries\n""""""\nimport logging\n\nfrom catalyst.tools import settings\nfrom catalyst.tools.registry import Registry\n\nlogger = logging.getLogger(__name__)\n\n\ndef _transforms_loader(r: Registry):\n    try:\n        import albumentations as m\n\n        r.add_from_module(m, prefix=[""A."", ""albu."", ""albumentations.""])\n\n        from albumentations import pytorch as p\n\n        r.add_from_module(p, prefix=[""A."", ""albu."", ""albumentations.""])\n\n        from catalyst.contrib.data.cv import transforms as t\n\n        r.add_from_module(t, prefix=[""catalyst."", ""C.""])\n    except ImportError as ex:\n        if settings.albumentations_required:\n            logger.warning(\n                ""albumentations not available, to install albumentations, ""\n                ""run `pip install albumentations`.""\n            )\n            raise ex\n\n\nTRANSFORMS = Registry(""transform"")\nTRANSFORMS.late_add(_transforms_loader)\nTransform = TRANSFORMS.add\n\n\ndef _samplers_loader(r: Registry):\n    from torch.utils.data import sampler as s\n\n    factories = {\n        k: v\n        for k, v in s.__dict__.items()\n        if ""Sampler"" in k and k != ""Sampler""\n    }\n    r.add(**factories)\n    from catalyst.data import sampler\n\n    r.add_from_module(sampler)\n\n\nSAMPLERS = Registry(""sampler"")\nSAMPLERS.late_add(_samplers_loader)\nSampler = SAMPLERS.add\n\n\nclass _GradClipperWrap:\n    def __init__(self, fn, args, kwargs):\n        self.fn = fn\n        self.args = args\n        self.kwargs = kwargs\n\n    def __call__(self, x):\n        self.fn(x, *self.args, **self.kwargs)\n\n\ndef _grad_clip_loader(r: Registry):\n    from torch.nn.utils import clip_grad as m\n\n    r.add_from_module(m)\n\n\nGRAD_CLIPPERS = Registry(""func"", default_meta_factory=_GradClipperWrap)\nGRAD_CLIPPERS.late_add(_grad_clip_loader)\n\n\ndef _modules_loader(r: Registry):\n    from catalyst.contrib.nn import modules as m\n\n    r.add_from_module(m)\n\n\nMODULES = Registry(""module"")\nMODULES.late_add(_modules_loader)\nModule = MODULES.add\n\n\ndef _model_loader(r: Registry):\n    from catalyst.contrib import models as m\n\n    r.add_from_module(m)\n\n    try:\n        import segmentation_models_pytorch as smp\n\n        r.add_from_module(smp, prefix=""smp."")\n    except ImportError as ex:\n        if settings.segmentation_models_required:\n            logger.warning(\n                ""segmentation_models_pytorch not available,""\n                "" to install segmentation_models_pytorch,""\n                "" run `pip install segmentation-models-pytorch`.""\n            )\n            raise ex\n\n\nMODELS = Registry(""model"")\nMODELS.late_add(_model_loader)\nModel = MODELS.add\n\n\ndef _criterion_loader(r: Registry):\n    from catalyst.contrib.nn import criterion as m\n\n    r.add_from_module(m)\n\n\nCRITERIONS = Registry(""criterion"")\nCRITERIONS.late_add(_criterion_loader)\nCriterion = CRITERIONS.add\n\n\ndef _optimizers_loader(r: Registry):\n    from catalyst.contrib.nn import optimizers as m\n\n    r.add_from_module(m)\n\n\nOPTIMIZERS = Registry(""optimizer"")\nOPTIMIZERS.late_add(_optimizers_loader)\nOptimizer = OPTIMIZERS.add\n\n\ndef _schedulers_loader(r: Registry):\n    from catalyst.contrib.nn import schedulers as m\n\n    r.add_from_module(m)\n\n\nSCHEDULERS = Registry(""scheduler"")\nSCHEDULERS.late_add(_schedulers_loader)\nScheduler = SCHEDULERS.add\n\n\nEXPERIMENTS = Registry(""experiment"")\nExperiment = EXPERIMENTS.add\n\n\n__all__ = [\n    ""Criterion"",\n    ""Optimizer"",\n    ""Scheduler"",\n    ""Module"",\n    ""Model"",\n    ""Sampler"",\n    ""Transform"",\n    ""Experiment"",\n    ""CRITERIONS"",\n    ""GRAD_CLIPPERS"",\n    ""MODELS"",\n    ""MODULES"",\n    ""OPTIMIZERS"",\n    ""SAMPLERS"",\n    ""SCHEDULERS"",\n    ""TRANSFORMS"",\n    ""EXPERIMENTS"",\n]\n'"
catalyst/core/__init__.py,0,"b'# flake8: noqa\n# isort:skip_file\n# import order:\n# callback\n# callbacks\n# experiment\n# runner\n\nfrom .experiment import IExperiment\nfrom .runner import IRunner, IStageBasedRunner\nfrom .callback import Callback, CallbackOrder, CallbackNode, CallbackScope\nfrom .callbacks import *\nfrom .state import State\n'"
catalyst/core/callback.py,0,"b'from typing import TYPE_CHECKING\nfrom enum import IntFlag\n\nif TYPE_CHECKING:\n    from catalyst.core.runner import IRunner\n\n\nclass CallbackNode(IntFlag):\n    """"""Callback node usage flag during distributed training.\n\n    - All (0) - use on all nodes, botch master and worker.\n    - Master (1) - use only on master node.\n    - Worker (2) - use only in worker nodes.\n    """"""\n\n    All = 0\n    Master = 1\n    Worker = 2\n\n\nclass CallbackOrder(IntFlag):\n    """"""Callback usage order during training.\n\n    Catalyst executes Callbacks with low `CallbackOrder`\n    **before** Callbacks with high `CallbackOrder`.\n\n    Predefined orders:\n\n    - **Internal** (0) - some Catalyst Extras,\n      like PhaseCallbacks (used in GANs).\n    - **Metric** (20) - Callbacks with metrics and losses computation.\n    - **MetricAggregation** (40) - metrics aggregation callbacks,\n      like sum different losses into one.\n    - **Optimizer** (60) - optimizer step,\n      requires computed metrics for optimization.\n    - **Validation** (80) - validation step,\n      computes validation metrics subset based on all metrics.\n    - **Scheduler** (100) - scheduler step,\n      in `ReduceLROnPlateau` case\n      requires computed validation metrics for optimizer schedule.\n    - **Logging** (120) - logging step,\n      logs metrics to Console/Tensorboard/Alchemy_,\n      requires computed metrics.\n    - **External** (200) - additional callbacks with custom logic,\n      like InferenceCallbacks\n\n    Nevertheless, you always can create CustomCallback with any order,\n    for example::\n\n        >>> class MyCustomCallback(Callback):\n        >>>     def __init__(self):\n        >>>         super().__init__(order=42)\n        >>>     ...\n        # MyCustomCallback will be executed after all `Metric`-Callbacks\n        # but before all `MetricAggregation`-Callbacks.\n\n    .. _Alchemy: https://alchemy.host\n    """"""\n\n    Internal = 0  # pytorch\n    Metric = 20  # pytorch\n    MetricAggregation = 40  # pytorch\n    Optimizer = 60  # pytorch\n    Validation = 80  # numpy\n    Scheduler = 100  # numpy\n    Logging = 120  # numpy\n    External = 200  # numpy\n\n\nclass CallbackScope(IntFlag):\n    """"""Callback scope usage flag during training.\n\n    - Stage (0) - use Callback only during one experiment stage.\n    - Experiment (1) - use Callback during whole experiment run.\n    """"""\n\n    Stage = 0\n    Experiment = 1\n\n\nclass Callback:\n    """"""\n    An abstraction that lets you customize your experiment run logic.\n    To give users maximum flexibility and extensibility Catalyst supports\n    callback execution anywhere in the training loop:\n\n    .. code:: bash\n\n        -- stage start\n        ---- epoch start\n        ------ loader start\n        -------- batch start\n        ---------- batch handler (Runner logic)\n        -------- batch end\n        ------ loader end\n        ---- epoch end\n        -- stage end\n\n        exception \xe2\x80\x93 if an Exception was raised\n\n    All callbacks have\n        - ``order`` from ``CallbackOrder``\n        - ``node`` from ``CallbackNode``\n        - ``scope`` from ``CallbackScope``\n\n    .. note::\n        To learn more about Catalyst Core concepts, please check out\n\n            - :py:mod:`catalyst.core.experiment.IExperiment`\n            - :py:mod:`catalyst.core.runner.IRunner`\n            - :py:mod:`catalyst.core.callback.Callback`\n\n    Abstraction, please check out the implementations:\n\n        - :py:mod:`catalyst.core.callbacks.criterion.CriterionCallback`\n        - :py:mod:`catalyst.core.callbacks.optimizer.OptimizerCallback`\n        - :py:mod:`catalyst.core.callbacks.scheduler.SchedulerCallback`\n        - :py:mod:`catalyst.core.callbacks.logging.TensorboardLogger`\n        - :py:mod:`catalyst.core.callbacks.checkpoint.CheckpointCallback`\n    """"""\n\n    def __init__(\n        self,\n        order: int,\n        node: int = CallbackNode.All,\n        scope: int = CallbackScope.Stage,\n    ):\n        """"""Callback initializer.\n\n        Args:\n            order: flag from ``CallbackOrder``\n            node: flag from ``CallbackNode``\n            scope: flag from ``CallbackScope``\n        """"""\n        self.node = node\n        self.order = order\n        self.scope = scope\n\n    def on_stage_start(self, runner: ""IRunner""):\n        """"""Event handler for stage start.\n\n        Args:\n            runner (""IRunner""): IRunner instance.\n        """"""\n        pass\n\n    def on_stage_end(self, runner: ""IRunner""):\n        """"""Event handler for stage end.\n\n        Args:\n            runner (""IRunner""): IRunner instance.\n        """"""\n        pass\n\n    def on_epoch_start(self, runner: ""IRunner""):\n        """"""Event handler for epoch start.\n\n        Args:\n            runner (""IRunner""): IRunner instance.\n        """"""\n        pass\n\n    def on_epoch_end(self, runner: ""IRunner""):\n        """"""Event handler for epoch end.\n\n        Args:\n            runner (""IRunner""): IRunner instance.\n        """"""\n        pass\n\n    def on_loader_start(self, runner: ""IRunner""):\n        """"""Event handler for loader start.\n\n        Args:\n            runner (""IRunner""): IRunner instance.\n        """"""\n        pass\n\n    def on_loader_end(self, runner: ""IRunner""):\n        """"""Event handler for loader end.\n\n        Args:\n            runner (""IRunner""): IRunner instance.\n        """"""\n        pass\n\n    def on_batch_start(self, runner: ""IRunner""):\n        """"""Event handler for batch start.\n\n        Args:\n            runner (""IRunner""): IRunner instance.\n        """"""\n        pass\n\n    def on_batch_end(self, runner: ""IRunner""):\n        """"""Event handler for batch end.\n\n        Args:\n            runner (""IRunner""): IRunner instance.\n        """"""\n        pass\n\n    def on_exception(self, runner: ""IRunner""):\n        """"""Event handler for exception case.\n\n        Args:\n            runner (""IRunner""): IRunner instance.\n        """"""\n        pass\n\n\n__all__ = [\n    ""Callback"",\n    ""CallbackNode"",\n    ""CallbackOrder"",\n    ""CallbackScope"",\n]\n'"
catalyst/core/experiment.py,4,"b'from typing import Any, Dict, Iterable, Mapping, Tuple\nfrom abc import ABC, abstractmethod\nfrom collections import OrderedDict\n\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\n\nfrom catalyst.core.callback import Callback\nfrom catalyst.tools.typing import Criterion, Model, Optimizer, Scheduler\n\n\nclass IExperiment(ABC):\n    """"""\n    An abstraction that contains information about the experiment \xe2\x80\x93\n    a model, a criterion, an optimizer, a scheduler, and their hyperparameters.\n    It also contains information about the data and transformations used.\n    In general, the Experiment knows **what** you would like to run.\n\n    .. note::\n        To learn more about Catalyst Core concepts, please check out\n\n            - :py:mod:`catalyst.core.experiment.IExperiment`\n            - :py:mod:`catalyst.core.runner.IRunner`\n            - :py:mod:`catalyst.core.callback.Callback`\n\n    Abstraction, please check out the implementations:\n\n        - :py:mod:`catalyst.dl.experiment.base.BaseExperiment`\n        - :py:mod:`catalyst.dl.experiment.config.ConfigExperiment`\n        - :py:mod:`catalyst.dl.experiment.supervised.SupervisedExperiment`\n    """"""\n\n    @property\n    @abstractmethod\n    def initial_seed(self) -> int:\n        """"""\n        Experiment\'s initial seed, used to setup `global seed`\n        at the beginning of each stage.\n        Additionally, Catalyst Runner setups\n        `experiment.initial_seed + runner.global_epoch + 1`\n        as `global seed` each epoch.\n        Used for experiment reproducibility.\n\n        Example::\n\n            >>> experiment.initial_seed\n            42\n        """"""\n        pass\n\n    @property\n    @abstractmethod\n    def logdir(self) -> str:\n        """"""Path to the directory where the experiment logs would be saved.\n\n        Example::\n\n            >>> experiment.logdir\n            ./path/to/my/experiment/logs\n        """"""\n        pass\n\n    @property\n    @abstractmethod\n    def stages(self) -> Iterable[str]:\n        """"""Experiment\'s stage names.\n\n        Example::\n\n            >>> experiment.stages\n            [""pretraining"", ""training"", ""finetuning""]\n\n        .. note::\n            To understand stages concept, please follow Catalyst documentation,\n            for example, :py:mod:`catalyst.core.callback.Callback`\n        """"""\n        pass\n\n    @property\n    @abstractmethod\n    def distributed_params(self) -> Dict:\n        """"""\n        Dictionary with the parameters for distributed\n        and half-precision training.\n\n        Used in :py:mod:`catalyst.utils.distributed.process_components`\n        to setup `Nvidia Apex`_ or `PyTorch distributed`_.\n\n        .. _`Nvidia Apex`: https://github.com/NVIDIA/apex\n        .. _`PyTorch distributed`:\n            https://pytorch.org/docs/stable/distributed.html\n\n        Example::\n\n            >>> experiment.distributed_params\n            {""opt_level"": ""O1"", ""syncbn"": True}  # Apex variant\n        """"""\n        pass\n\n    @abstractmethod\n    def get_stage_params(self, stage: str) -> Mapping[str, Any]:\n        """"""Returns extra stage parameters for a given stage.\n\n        Example::\n\n            >>> experiment.get_stage_params(stage=""training"")\n            {\n                ""logdir"": ""./logs/training"",\n                ""num_epochs"": 42,\n                ""valid_loader"": ""valid"",\n                ""main_metric"": ""loss"",\n                ""minimize_metric"": True,\n                ""checkpoint_data"": {\n                    ""comment"": ""break the cycle - use the Catalyst""\n                }\n            }\n\n        Args:\n            stage (str): stage name of interest\n                like ""pretrain"" / ""train"" / ""finetune"" / etc\n\n        Returns:\n            dict: parameters for a given stage.\n        """"""\n        pass\n\n    @abstractmethod\n    def get_model(self, stage: str) -> Model:\n        """"""Returns the model for a given stage.\n\n        Example::\n\n            # suppose we have typical MNIST model, like\n            # nn.Sequential(nn.Linear(28*28, 128), nn.Linear(128, 10))\n            >>> experiment.get_model(stage=""training"")\n            Sequential(\n              (0): Linear(in_features=784, out_features=128, bias=True)\n              (1): Linear(in_features=128, out_features=10, bias=True)\n            )\n\n        Args:\n            stage (str): stage name of interest\n                like ""pretrain"" / ""train"" / ""finetune"" / etc\n\n        Returns:\n            Model: model for a given stage.\n        """"""\n        pass\n\n    @abstractmethod\n    def get_criterion(self, stage: str) -> Criterion:\n        """"""Returns the criterion for a given stage.\n\n        Example::\n\n            # for typical classification task\n            >>> experiment.get_criterion(stage=""training"")\n            nn.CrossEntropyLoss()\n\n        Args:\n            stage (str): stage name of interest\n                like ""pretrain"" / ""train"" / ""finetune"" / etc\n\n        Returns:\n            Criterion: criterion for a given stage.\n        """"""\n        pass\n\n    @abstractmethod\n    def get_optimizer(self, stage: str, model: Model) -> Optimizer:\n        """"""Returns the optimizer for a given stage and model.\n\n        Example::\n\n            >>> experiment.get_optimizer(stage=""training"", model=model)\n            torch.optim.Adam(model.parameters())\n\n        Args:\n            stage (str): stage name of interest\n                like ""pretrain"" / ""train"" / ""finetune"" / etc\n            model (Model): model to optimize with stage optimizer\n\n        Returns:\n            Optimizer: optimizer for a given stage and model.\n        """"""\n        pass\n\n    @abstractmethod\n    def get_scheduler(self, stage: str, optimizer: Optimizer) -> Scheduler:\n        """"""Returns the scheduler for a given stage and optimizer.\n\n        Example::\n            >>> experiment.get_scheduler(stage=""training"", optimizer=optimizer)\n            torch.optim.lr_scheduler.StepLR(optimizer)\n\n        Args:\n            stage (str): stage name of interest\n                like ""pretrain"" / ""train"" / ""finetune"" / etc\n            optimizer (Optimizer): optimizer to schedule with stage scheduler\n\n        Returns:\n            Scheduler: scheduler for a given stage and optimizer.\n        """"""\n        pass\n\n    def get_experiment_components(\n        self, stage: str, model: nn.Module = None,\n    ) -> Tuple[Model, Criterion, Optimizer, Scheduler]:\n        """"""\n        Returns the tuple containing criterion, optimizer and scheduler by\n        giving model and stage.\n\n        Aggregation method, based on,\n\n        - :py:mod:`catalyst.core.experiment.IExperiment.get_model`\n        - :py:mod:`catalyst.core.experiment.IExperiment.get_criterion`\n        - :py:mod:`catalyst.core.experiment.IExperiment.get_optimizer`\n        - :py:mod:`catalyst.core.experiment.IExperiment.get_scheduler`\n\n        Args:\n            stage (str): stage name of interest,\n                like ""pretrain"" / ""train"" / ""finetune"" / etc\n            model (Model): model to optimize with stage optimizer\n\n        Returns:\n            tuple: model, criterion, optimizer, scheduler\n                for a given stage and model\n        """"""\n        if model is None:\n            model = self.get_model(stage)\n        criterion = self.get_criterion(stage)\n        optimizer = self.get_optimizer(stage, model)\n        scheduler = self.get_scheduler(stage, optimizer)\n        return model, criterion, optimizer, scheduler\n\n    def get_transforms(self, stage: str = None, dataset: str = None):\n        """"""Returns the data transforms for a given stage and dataset.\n\n        Args:\n            stage (str): stage name of interest,\n                like ""pretrain"" / ""train"" / ""finetune"" / etc\n            dataset (str): dataset name of interest,\n                like ""train"" / ""valid"" / ""infer""\n\n        .. note::\n            For datasets/loaders nameing please follow\n            :py:mod:`catalyst.core.runner` documentation.\n\n        Returns:\n            Data transformations to use for specified dataset.\n\n        """"""\n        raise NotImplementedError\n\n    def get_datasets(\n        self, stage: str, epoch: int = None, **kwargs,\n    ) -> ""OrderedDict[str, Dataset]"":\n        """"""Returns the datasets for a given stage and epoch.\n\n        .. note::\n            For Deep Learning cases you have the same dataset\n            during whole stage.\n\n            For Reinforcement Learning it common to change the dataset\n            (experiment) every training epoch.\n\n        Args:\n            stage (str): stage name of interest,\n                like ""pretrain"" / ""train"" / ""finetune"" / etc\n            epoch (int): epoch index\n            **kwargs (dict): additional parameters to use during\n                dataset creation\n\n        Returns:\n            OrderedDict[str, Dataset]: Ordered dictionary\n                with datasets for current stage and epoch.\n\n        .. note::\n            We need ordered dictionary to guarantee the correct dataflow\n            and order of our training datasets.\n            For example, to run through train data before validation one :)\n\n        Example::\n\n            >>> experiment.get_datasets(\n            >>>     stage=""training"",\n            >>>     in_csv_train=""path/to/train/csv"",\n            >>>     in_csv_valid=""path/to/valid/csv"",\n            >>> )\n            OrderedDict({\n                ""train"": CsvDataset(in_csv=in_csv_train, ...),\n                ""valid"": CsvDataset(in_csv=in_csv_valid, ...),\n            })\n\n\n        """"""\n        raise NotImplementedError\n\n    @abstractmethod\n    def get_loaders(\n        self, stage: str, epoch: int = None,\n    ) -> ""OrderedDict[str, DataLoader]"":\n        """"""Returns the loaders for a given stage.\n\n        .. note::\n            Wrapper for\n            :py:mod:`catalyst.core.experiment.IExperiment.get_datasets`.\n            For most of your experiments you need to rewrite `get_datasets`\n            method only.\n\n        Args:\n            stage (str): stage name of interest,\n                like ""pretrain"" / ""train"" / ""finetune"" / etc\n            epoch (int): epoch index\n            **kwargs (dict): additional parameters to use during\n                dataset creation\n\n        Returns:\n            OrderedDict[str, DataLoader]: Ordered dictionary\n                with loaders for current stage and epoch.\n\n        """"""\n        raise NotImplementedError\n\n    @abstractmethod\n    def get_callbacks(self, stage: str) -> ""OrderedDict[str, Callback]"":\n        """"""Returns callbacks for a given stage.\n\n        .. note::\n            To learn more about Catalyst Callbacks mechanism, please follow\n            :py:mod:`catalyst.core.callback.Callback` documentation.\n\n        .. note::\n            We need ordered dictionary to guarantee the correct dataflow\n            and order of metrics optimization.\n            For example, to compute loss before optimization,\n            or to compute all the metrics before logging :)\n\n        Args:\n            stage (str): stage name of interest\n                like ""pretrain"" / ""train"" / ""finetune"" / etc\n\n        Returns:\n            OrderedDict[str, Callback]: Ordered dictionary\n            with callbacks for current stage.\n\n        .. note::\n            To learn more about Catalyst Core concepts, please check out\n\n                - :py:mod:`catalyst.core.experiment.IExperiment`\n                - :py:mod:`catalyst.core.runner.IRunner`\n                - :py:mod:`catalyst.core.callback.Callback`\n        """"""\n        pass\n\n\n__all__ = [""IExperiment""]\n'"
catalyst/core/legacy.py,0,"b'import warnings\n\n\nclass IRunnerLegacy:\n    """"""\n    Special class to encapsulate all `catalyst.core.runner.IRunner`\n    and `catalyst.core.runner.State` legacy into one place.\n    Used to make `catalyst.core.runner.IRunner` cleaner\n    and easier to understand.\n\n    Saved for backward compatibility. Should be removed someday.\n    """"""\n\n    @property\n    def batch_in(self):\n        """"""Alias for `runner.input`.\n\n        .. warning::\n            Deprecated, saved for backward compatibility.\n            Please use `runner.input` instead.\n        """"""\n        warnings.warn(\n            ""`runner.batch_in` was deprecated, ""\n            ""please use `runner.input` instead"",\n            DeprecationWarning,\n        )\n        return self.input\n\n    @property\n    def batch_out(self):\n        """"""Alias for `runner.output`.\n\n        .. warning::\n            Deprecated, saved for backward compatibility.\n            Please use `runner.output` instead.\n        """"""\n        warnings.warn(\n            ""`runner.batch_out` was deprecated, ""\n            ""please use `runner.output` instead"",\n            DeprecationWarning,\n        )\n        return self.output\n\n    @property\n    def need_backward_pass(self):\n        """"""Alias for `runner.is_train_loader`.\n\n        .. warning::\n            Deprecated, saved for backward compatibility.\n            Please use `runner.is_train_loader` instead.\n        """"""\n        warnings.warn(\n            ""`need_backward_pass` was deprecated, ""\n            ""please use `is_train_loader` instead"",\n            DeprecationWarning,\n        )\n        return self.is_train_loader\n\n    @property\n    def loader_step(self):\n        """"""Alias for `runner.loader_batch_step`.\n\n        .. warning::\n            Deprecated, saved for backward compatibility.\n            Please use `runner.loader_batch_step` instead.\n        """"""\n        warnings.warn(\n            ""`loader_step` was deprecated, ""\n            ""please use `loader_batch_step` instead"",\n            DeprecationWarning,\n        )\n        return self.loader_batch_step\n\n    @property\n    def state(self):\n        """"""Alias for `runner`.\n\n        .. warning::\n            Deprecated, saved for backward compatibility.\n            Please use `runner` instead.\n        """"""\n        warnings.warn(\n            ""`runner.state` was deprecated, "" ""please use `runner` instead"",\n            DeprecationWarning,\n        )\n        return self\n'"
catalyst/core/registry.py,0,"b'from catalyst.contrib.registry import (\n    Criterion,\n    CRITERIONS,\n    GRAD_CLIPPERS,\n    Model,\n    MODELS,\n    Module,\n    MODULES,\n    Optimizer,\n    OPTIMIZERS,\n    Sampler,\n    SAMPLERS,\n    Scheduler,\n    SCHEDULERS,\n    Transform,\n    TRANSFORMS,\n)\nfrom catalyst.tools.registry import Registry\n\n\ndef _callbacks_loader(r: Registry):\n    from catalyst.core import callbacks as m\n\n    r.add_from_module(m)\n\n\nCALLBACKS = Registry(""callback"")\nCALLBACKS.late_add(_callbacks_loader)\nCallback = CALLBACKS.add\n\n__all__ = [\n    ""Callback"",\n    ""Criterion"",\n    ""Optimizer"",\n    ""Scheduler"",\n    ""Module"",\n    ""Model"",\n    ""Sampler"",\n    ""Transform"",\n    ""CALLBACKS"",\n    ""CRITERIONS"",\n    ""GRAD_CLIPPERS"",\n    ""MODELS"",\n    ""MODULES"",\n    ""OPTIMIZERS"",\n    ""SAMPLERS"",\n    ""SCHEDULERS"",\n    ""TRANSFORMS"",\n]\n'"
catalyst/core/runner.py,20,"b'from typing import Any, Callable, Dict, Mapping, Optional, Tuple, Union\nfrom abc import ABC, abstractmethod\nfrom collections import defaultdict, OrderedDict\nfrom pathlib import Path\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, DistributedSampler\n\nfrom catalyst.core import utils\nfrom catalyst.core.callback import Callback, CallbackScope\nfrom catalyst.core.experiment import IExperiment\nfrom catalyst.tools import settings\nfrom catalyst.tools.frozen_class import FrozenClass\nfrom catalyst.tools.typing import (\n    Criterion,\n    Device,\n    Model,\n    Optimizer,\n    RunnerCriterion,\n    RunnerModel,\n    RunnerOptimizer,\n    RunnerScheduler,\n    Scheduler,\n)\n\nfrom .legacy import IRunnerLegacy\n\n\nclass IRunner(ABC, IRunnerLegacy, FrozenClass):\n    """"""\n    An abstraction that knows how to run an experiment.\n    It contains all the logic of **how** to run the experiment,\n    stages, epoch and batches.\n\n    .. note::\n        To learn more about Catalyst Core concepts, please check out\n\n            - :py:mod:`catalyst.core.experiment.IExperiment`\n            - :py:mod:`catalyst.core.runner.IRunner`\n            - :py:mod:`catalyst.core.callback.Callback`\n\n    Abstraction, please check out the implementations:\n\n        - :py:mod:`catalyst.dl.runner.runner.Runner`\n        - :py:mod:`catalyst.dl.runner.supervised.SupervisedRunner`\n\n    Runner also contains full information about experiment runner.\n\n\n    Runner section\n\n\n    **runner.model** - an instance of torch.nn.Module class, \\\n    (should implement ``forward`` method); \\\n    for example,\n    ::\n\n        runner.model = torch.nn.Linear(10, 10)\n\n    **runner.device** - an instance of torch.device (CPU, GPU, TPU); \\\n    for example,\n    ::\n\n        runner.device = torch.device(""cpu"")\n\n\n    Experiment section\n\n\n    **runner.criterion** - an instance of torch.nn.Module class\\\n    or torch.nn.modules.loss._Loss (should implement ``forward`` method); \\\n    for example,\n    ::\n\n        runner.criterion = torch.nn.CrossEntropyLoss()\n\n    **runner.optimizer** - an instance of torch.optim.optimizer.Optimizer\\\n    (should implement ``step`` method); \\\n    for example,\n    ::\n\n        runner.optimizer = torch.optim.Adam()\n\n    **runner.scheduler** -\n    an instance of torch.optim.lr_scheduler._LRScheduler\\\n    (should implement ``step`` method); \\\n    for example,\n    ::\n\n        runner.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau()\n\n    **runner.callbacks** -\n    ordered dictionary with Catalyst.Callback instances;\\\n    for example,\n    ::\n\n        runner.callbacks = {\n            ""accuracy"": AccuracyCallback(),\n            ""criterion"": CriterionCallback(),\n            ""optim"": OptimizerCallback(),\n            ""saver"": CheckpointCallback()\n        }\n\n\n    Dataflow section\n\n\n    **runner.loaders** - ordered dictionary with torch.DataLoaders; \\\n    for example,\n    ::\n\n        runner.loaders = {\n            ""train"": MnistTrainLoader(),\n            ""valid"": MnistValidLoader()\n        }\n\n    .. note::\n        - ""*train*"" prefix is used for training loaders - \\\n          metrics computations, backward pass, optimization\n        - ""*valid*"" prefix is used for validation loaders - \\\n          metrics computations only\n        - ""*infer*"" prefix is used for inference loaders - \\\n          dataset prediction\n\n    **runner.input** - dictionary, \\\n    containing batch of data from currents DataLoader; \\\n    for example,\n    ::\n\n        runner.input = {\n            ""images"": np.ndarray(batch_size, c, h, w),\n            ""targets"": np.ndarray(batch_size, 1),\n        }\n\n    **runner.output** - dictionary, \\\n    containing model output for current batch; \\\n    for example,\n    ::\n\n        runner.output = {""logits"": torch.Tensor(batch_size, num_classes)}\n\n\n    Metrics section\n\n\n    **runner.batch_metrics** - dictionary, flatten storage for batch metrics; \\\n    for example,\n    ::\n\n        runner.batch_metrics = {""loss"": ..., ""accuracy"": ..., ""iou"": ...}\n\n    **runner.loader_metrics** - dictionary with aggregated batch statistics \\\n    for loader (mean over all batches) and global loader metrics, like AUC; \\\n    for example,\n    ::\n\n        runner.loader_metrics = {""loss"": ..., ""accuracy"": ..., ""auc"": ...}\n\n    **runner.epoch_metrics** - dictionary with summarized metrics \\\n    for different loaders and global epoch metrics, like lr, momentum; \\\n    for example,\n    ::\n\n        runner.epoch_metrics = {\n            ""train_loss"": ..., ""train_auc"": ..., ""valid_loss"": ...,\n            ""lr"": ..., ""momentum"": ...,\n        }\n\n\n    Validation metrics section\n\n\n    **runner.main_metric** - string, containing name of metric of interest \\\n    for optimization, validation and checkpointing during training\n\n    **runner.minimize_metric** - bool, indicator flag\n\n        - ``True`` if we need to minimize metric during training,\\\n          like `Cross Entropy loss`\n        - ``False`` if we need to maximize metric during training, \\\n          like `Accuracy` or `Intersection over Union`\n\n\n    Validation section\n\n\n    **runner.valid_loader** - string, name of validation loader \\\n    for metric selection, validation and model checkpoining\n\n    **runner.valid_metrics** - dictionary with validation metrics\\\n    for currect epoch; \\\n    for example,\n    ::\n\n        runner.valid_metrics = {""loss"": ..., ""accuracy"": ..., ""auc"": ...}\n\n    .. note::\n        subdictionary of epoch_metrics\n\n    **runner.is_best_valid** - bool, indicator flag\n\n        - ``True`` if this training epoch is best over all epochs\n        - ``False`` if not\n\n    **runner.best_valid_metrics** - dictionary with best validation metrics \\\n    during whole training process\n\n\n    Distributed section\n\n\n    **runner.distributed_rank** - distributed rank of current worker\n\n    **runner.is_distributed_master** - bool, indicator flag\n\n        - ``True`` if is master node (runner.distributed_rank == 0)\n        - ``False`` if is worker node (runner.distributed_rank != 0)\n\n    **runner.is_distributed_worker** - bool, indicator flag\n\n        - ``True`` if is worker node (runner.distributed_rank > 0)\n        - ``False`` if is master node (runner.distributed_rank <= 0)\n\n\n    Experiment info section\n\n\n    **runner.global_sample_step** - int, numerical indicator, counter for all\\\n    individual samples, that passes through our model during training,\\\n    validation and inference stages\n\n    **runner.global_batch_step** - int, numerical indicator, counter for all\n    batches, that passes through our model during training, validation and\\\n    inference stages\n\n    **runner.global_epoch** - int, numerical indicator,\n    counter for all epochs,\\\n    that have passed during model training, validation and\\\n    inference stages\n\n    **runner.verbose** - bool, indicator flag\n\n    **runner.is_check_run** - bool, indicator flag\n\n        - ``True`` if you want to check you pipeline and \\\n          run only 2 batches per loader and 2 epochs per stage\n        - ``False`` (default) if you want to just the pipeline\n\n    **runner.need_early_stop** - bool, indicator flag \\\n    used for EarlyStopping and CheckRun Callbacks\n\n        - ``True`` if we need to stop the training\n        - ``False`` (default) otherwise\n\n    **runner.need_exception_reraise** - bool, indicator flag\n\n        - ``True`` (default) if you want to show exception \\\n          during pipeline and stop the training process\n        - ``False`` otherwise\n\n\n    Stage info section\n\n\n    **runner.stage_name** - string, current stage name,\\\n    for example,\n    ::\n\n        runner.stage_name = ""pretraining"" / ""training"" / ""finetuning"" / etc\n\n    **runner.num_epochs** - int, maximum number of epochs, \\\n    required for this stage\n\n    **runner.is_infer_stage** - bool, indicator flag\n\n        - ``True`` for inference stages\n        - ``False`` otherwise\n\n\n    Epoch info section\n\n\n    **runner.epoch** - int, numerical indicator for current stage epoch\n\n\n    Loader info section\n\n\n    **runner.loader_sample_step** - int, numerical indicator \\\n    for number of samples passed through our model in current loader\n\n    **runner.loader_batch_step** - int, numerical indicator \\\n    for batch index in current loader\n\n\n    **runner.loader_name** - string, current loader name\\\n    for example,\n    ::\n\n        runner.loader_name = ""train_dataset1"" / ""valid_data2"" / ""infer_golden""\n\n    **runner.loader_len** - int, maximum number of batches in current loader\n\n    **runner.loader_batch_size** - int, batch size parameter in current loader\n\n    **runner.is_train_loader** - bool, indicator flag\n\n        - ``True`` for training loaders\n        - ``False`` otherwise\n\n    **runner.is_valid_loader** - bool, indicator flag\n\n        - ``True`` for validation loaders\n        - ``False`` otherwise\n\n    **runner.is_infer_loader** - bool, indicator flag\n\n        - ``True`` for inference loaders\n        - ``False`` otherwise\n\n\n    Batch info section\n\n\n    **runner.batch_size** - int, length of the current batch\n\n    Logging section\n\n\n    **runner.logdir** - string, path to logging directory to save\\\n    all logs, metrics, checkpoints and artifacts\n\n    **runner.checkpoint_data** - dictionary\\\n    with all extra data for experiment tracking\n\n    Extra section\n\n\n    **runner.exception** - python Exception instance to raise (or not ;) )\n\n    """"""\n\n    _experiment_fn: Callable = IExperiment\n\n    def __init__(\n        self, model: RunnerModel = None, device: Device = None, **kwargs,\n    ):\n        """"""\n        Args:\n            model (RunnerModel): Torch model object\n            device (Device): Torch device\n        """"""\n        self._device = None\n        self._model = None\n        self.device: Device = device\n        self.model: RunnerModel = model\n        self._init(**kwargs)\n        self._freeze()\n\n    def _prepare_inner_state(\n        self,\n        stage: str = settings.stage_infer_prefix,  # @TODO: wtf?\n        device: Device = None,\n        model: RunnerModel = None,\n        criterion: RunnerCriterion = None,\n        optimizer: RunnerOptimizer = None,\n        scheduler: RunnerScheduler = None,\n        callbacks: Dict[str, ""Callback""] = None,\n        logdir: str = None,\n        num_epochs: int = 1,\n        main_metric: str = ""loss"",\n        minimize_metric: bool = True,\n        valid_loader: str = settings.loader_valid_prefix,\n        checkpoint_data: Dict = None,\n        is_check_run: bool = False,\n        verbose: bool = False,\n        **kwargs,\n    ):\n        self._unfreeze()\n\n        # main runner components: model and device to run\n        self.device: Device = device\n        self.model: RunnerModel = model\n\n        # extra experiment components,\n        # use `catalyst.core.IExperiment` to setup them\n        self.criterion: RunnerCriterion = criterion\n        self.optimizer: RunnerOptimizer = optimizer\n        self.scheduler: RunnerScheduler = scheduler\n        # and callbacks\n        self.callbacks: Dict[str, ""Callback""] = callbacks or {}\n\n        # the data\n        self.loaders: OrderedDict[str, DataLoader] = None\n        # and the dataflow - model input, model output\n        self.input = None\n        self.output = None\n\n        # metrics flow - batch, loader, epoch metrics\n        # let\'s use flatten storage for batch metrics\n        # batch_metrics = {\'loss\': ..., \'accuracy\': ..., \'iou\': ...}\n        self.batch_metrics: Dict = defaultdict(None)\n        # just aggregated (aka mean over all batches)\n        # batch statistics for loader\n        # and global loader metrics, like AUC\n        # loader_metrics = {\'loss\': ..., \'accuracy\': ..., `auc`: ...}\n        self.loader_metrics: Dict = defaultdict(None)\n        # summarized metrics for different loaders\n        # and global epoch metrics, like lr, momentum\n        # epoch_metrics = {\n        # \'train_loss\': ..., \'train_auc\': ..., \'valid_loss\': ...,\n        # \'lr\': ..., \'momentum\': ...,\n        # }\n        self.epoch_metrics: Dict = defaultdict(None)\n\n        # metrics & validation\n        self.main_metric: str = main_metric\n        self.minimize_metric: bool = minimize_metric\n\n        # validation\n        self.valid_loader: str = valid_loader\n        self.valid_metrics: Dict = defaultdict(None)\n        self.is_best_valid: bool = False\n        self.best_valid_metrics: Dict = defaultdict(None)\n\n        # distributed info\n        self.distributed_rank: int = utils.get_rank()\n        self.is_distributed_master: bool = ~(self.distributed_rank > 0)\n        self.is_distributed_worker: bool = self.distributed_rank > 0\n        # experiment info\n        self.global_sample_step: int = 0\n        self.global_batch_step: int = 0\n        self.global_epoch: int = 1\n        self.verbose: bool = verbose\n        self.is_check_run: bool = is_check_run\n        self.need_early_stop: bool = False\n        self.need_exception_reraise: bool = True\n        # stage info\n        self.num_epochs: int = num_epochs\n        self.stage_name: str = stage\n        self.is_infer_stage: bool = self.stage_name.startswith(\n            settings.stage_infer_prefix\n        )\n        # epoch info\n        self.epoch: int = 1\n        # loader info\n        self.loader_sample_step: int = 0\n        self.loader_batch_step: int = 0\n        self.loader_name: str = None\n        self.loader_len: int = 0\n        self.loader_batch_size = 0\n        self.is_train_loader: bool = False\n        self.is_valid_loader: bool = False\n        self.is_infer_loader: bool = False\n        # batch info\n        self.batch_size: int = 0\n\n        # logging\n        self.expdir: Path = None\n        self.logdir: Path = Path(logdir) if logdir is not None else None\n        # extra checkpoint data for saving in checkpoint files\n        self.checkpoint_data: Dict = checkpoint_data or {}\n\n        # extra\n        self.exception: Optional[Exception] = None\n\n        # kwargs\n        for key, value in kwargs.items():\n            setattr(self, key, value)\n\n        self._freeze()\n\n    def _init(self, **kwargs) -> None:\n        """"""\n        Inner method for children\'s classes\n        to specify type for Runners\' Experiment.\n        """"""\n        self.experiment: IExperiment = None\n\n    @property\n    def model(self) -> Model:\n        """"""Returns the runner\'s model instance.""""""\n        return self._model\n\n    @model.setter\n    def model(self, value: Union[Model, Dict[str, Model]]):\n        """"""\n        Setter for the runner\'s model, useful for experiment tracing.\n\n        Args:\n            value (Union[Model, Dict[str, Model]]): new model.\n        """"""\n        if isinstance(value, nn.Module):\n            model = value\n        elif isinstance(value, dict):\n            values_are_models = all(\n                isinstance(v, nn.Module) for v in value.values()\n            )\n            if not values_are_models:\n                raise TypeError(\n                    ""Invalid dict value type, must be `torch.nn.Module`""\n                )\n\n            model = value\n        elif isinstance(value, type(None)):\n            model = None\n        else:\n            raise TypeError(\n                f""Invalid value type ""\n                f""must be `torch.nn.Module` or `Dict[str, torch.nn.Module]` ""\n                f""got \'{type(value)}\'""\n            )\n\n        if model is not None and self._device is not None:\n            model: Model = utils.maybe_recursive_call(\n                model, ""to"", device=self._device\n            )\n\n        self._model = model\n\n    @property\n    def device(self) -> Device:\n        """"""Returns the runner\'s device instance.""""""\n        return self._device\n\n    @device.setter\n    def device(self, value: Device):\n        """"""\n        Setter for the runner\'s device.\n\n        Args:\n            value (Device): new torch device.\n        """"""\n        if isinstance(value, torch.device):\n            self._device = value\n        elif isinstance(value, str):\n            self._device = torch.device(value)\n        elif isinstance(value, type(None)):\n            self._device = None\n        else:\n            raise TypeError(\n                f""Invalid value type ""\n                f""must be `str` or `torch.device` ""\n                f""got \'{type(value)}\'""\n            )\n\n        if self._model is not None:\n            self._model = utils.maybe_recursive_call(\n                self._model, ""to"", device=self._device or ""cpu""\n            )\n\n    @staticmethod\n    def _get_experiment_components(\n        experiment: IExperiment, stage: str = None, device: Device = None,\n    ) -> Tuple[Model, Criterion, Optimizer, Scheduler, Device]:\n        """"""\n        Inner method for `Experiment` components preparation.\n\n        Check available torch device, takes model from the experiment\n        and creates stage-specified criterion, optimizer, scheduler for it.\n\n        Args:\n            stage (str): experiment stage name of interest\n                like ""pretrain"" / ""train"" / ""finetune"" / etc\n\n        Returns:\n            tuple: model, criterion, optimizer,\n                scheduler and device for a given stage and model\n        """"""\n        (\n            model,\n            criterion,\n            optimizer,\n            scheduler,\n        ) = experiment.get_experiment_components(stage)\n        (\n            model,\n            criterion,\n            optimizer,\n            scheduler,\n            device,\n        ) = utils.process_components(\n            model=model,\n            criterion=criterion,\n            optimizer=optimizer,\n            scheduler=scheduler,\n            distributed_params=experiment.distributed_params,\n            device=device,\n        )\n        return model, criterion, optimizer, scheduler, device\n\n    @staticmethod\n    def _get_experiment_callbacks(\n        experiment: IExperiment, stage: str,\n    ) -> Dict[str, Callback]:\n        """"""Inner method for `Callbacks` preparation.\n\n        Takes callbacks from the Experiment\n        and filters them for distributed master/worker cases.\n\n        Args:\n            stage (str): stage name of interest,\n                like ""pretrain"" / ""train"" / ""finetune"" / etc\n\n        Returns:\n            OrderedDict[str, Callback]: Ordered dictionary\n                with callbacks for current experiment stage.\n        """"""\n        callbacks = experiment.get_callbacks(stage)\n        callbacks = utils.filter_callbacks_by_node(callbacks)\n        callbacks = utils.sort_callbacks_by_order(callbacks)\n        return callbacks\n\n    def get_attr(self, key: str, inner_key: str = None) -> Any:\n        """"""\n        Alias for python `getattr` method. Useful for Callbacks preparation\n        and cases with multi-criterion, multi-optimizer setup.\n        For example, when you would like to train multi-task classification.\n\n        Used to get a named attribute from a `IRunner` by `key` keyword;\n        for example\\\n        ::\n\n            # example 1\n            runner.get_attr(""criterion"")\n            # is equivalent to\n            runner.criterion\n\n            # example 2\n            runner.get_attr(""optimizer"")\n            # is equivalent to\n            runner.optimizer\n\n            # example 3\n            runner.get_attr(""scheduler"")\n            # is equivalent to\n            runner.scheduler\n\n        With `inner_key` usage, it suppose to find a dictionary under `key`\\\n        and would get `inner_key` from this dict; for example,\n        ::\n\n            # example 1\n            runner.get_attr(""criterion"", ""bce"")\n            # is equivalent to\n            runner.criterion[""bce""]\n\n            # example 2\n            runner.get_attr(""optimizer"", ""adam"")\n            # is equivalent to\n            runner.optimizer[""adam""]\n\n            # example 3\n            runner.get_attr(""scheduler"", ""adam"")\n            # is equivalent to\n            runner.scheduler[""adam""]\n\n        Args:\n            key (str): name for attribute of interest,\n                like `criterion`, `optimizer`, `scheduler`\n            inner_key (str): name of inner dictionary key\n        """"""\n        if inner_key is None:\n            return getattr(self, key)\n        else:\n            return getattr(self, key)[inner_key]\n\n    def _prepare_for_stage(self, stage: str) -> None:\n        """"""\n        Inner method to prepare `Runner` for the specified stage.\n\n        Sets `Experiment` initial seed.\n        Prepares experiment components with `self._get_experiment_components`.\n        Prepares callbacks with `self._get_experiment_callbacks`.\n        Prepares inner state with `self._prepare_inner_state`\n\n        Args:\n            stage (str): stage name of interest,\n                like ""pretrain"" / ""train"" / ""finetune"" / etc\n        """"""\n        utils.set_global_seed(self.experiment.initial_seed)\n        (\n            model,\n            criterion,\n            optimizer,\n            scheduler,\n            device,\n        ) = self._get_experiment_components(\n            experiment=self.experiment, stage=stage,\n        )\n\n        utils.set_global_seed(self.experiment.initial_seed)\n        callbacks = self._get_experiment_callbacks(\n            experiment=self.experiment, stage=stage\n        )\n\n        migrating_params = dict(**self.experiment.get_stage_params(stage))\n        migrate_from_previous_stage = migrating_params.get(\n            ""migrate_from_previous_stage"", True\n        )\n        if (\n            migrate_from_previous_stage\n            and getattr(self, ""callbacks"", None) is not None\n        ):\n            for key, value in self.callbacks.items():\n                if value.scope == CallbackScope.Experiment:\n                    callbacks[key] = value\n\n        callbacks = utils.sort_callbacks_by_order(callbacks)\n\n        self._prepare_inner_state(\n            stage=stage,\n            model=model,\n            device=device,\n            criterion=criterion,\n            optimizer=optimizer,\n            scheduler=scheduler,\n            callbacks=callbacks,\n            **migrating_params,\n        )\n\n    def _prepare_for_epoch(self, stage: str, epoch: int) -> None:\n        """"""\n        Inner method to prepare `Runner` for the specified stage and epoch.\n\n        Args:\n            stage (str): stage name of interest,\n                like ""pretrain"" / ""train"" / ""finetune"" / etc\n            epoch (int): epoch index\n        """"""\n        pass\n\n    def _run_event(self, event: str) -> None:\n        """"""Inner method to run specified event on Runners\' callbacks.\n\n        Args:\n            event(str): event name to run on callbacks.\n\n        .. note::\n            To learn more about Catalyst Callbacks mechanism, please follow\n            :py:mod:`catalyst.core.callback.Callback` documentation.\n\n        """"""\n        for callback in self.callbacks.values():\n            getattr(callback, event)(self)\n\n    def _batch2device(\n        self, batch: Mapping[str, Any], device: Device,\n    ) -> Mapping[str, Any]:\n        """"""\n        Inner method to transfer incoming data batches to Runners\' device.\n\n        Args:\n            batch (Mapping[str, Any]): dictionary with data batches\n                from DataLoader.\n            device (Device): torch device\n\n        Returns:\n            Mapping[str, Any]: same structure as value,\n                but all tensors and np.arrays moved to device\n        """"""\n        output = utils.any2device(batch, device)\n        return output\n\n    @abstractmethod\n    def _handle_batch(self, batch: Mapping[str, Any]) -> None:\n        """"""\n        Inner method to handle specified data batch.\n        Used to make a train/valid/infer stage during Experiment run.\n\n        Args:\n            batch (Mapping[str, Any]): dictionary with data batches\n                from DataLoader.\n        """"""\n        pass\n\n    def _run_batch(self, batch: Mapping[str, Any]) -> None:\n        """"""\n        Inner method to run train step on specified data batch,\n        with batch callbacks events.\n\n        Args:\n            batch (Mapping[str, Any]): dictionary with data batches\n                from DataLoader.\n        """"""\n        if isinstance(batch, dict):\n            self.batch_size = next(iter(batch.values())).shape[0]\n        else:\n            self.batch_size = len(batch[0])\n        self.global_sample_step += self.batch_size\n        self.loader_sample_step += self.batch_size\n        batch = self._batch2device(batch, self.device)\n        self.input = batch\n\n        self._run_event(""on_batch_start"")\n        self._handle_batch(batch=batch)\n        self._run_event(""on_batch_end"")\n\n    def _run_loader(self, loader: DataLoader) -> None:\n        """"""\n        Inner method to pass whole DataLoader through Runner,\n        with loader callbacks events.\n\n        Args:\n            loader (DataLoader): dataloader to iterate\n        """"""\n        self.loader_batch_size = (\n            loader.batch_sampler.batch_size\n            if loader.batch_sampler is not None\n            else loader.batch_size\n        )\n\n        self.loader_sample_step = 0\n        for i, batch in enumerate(loader):\n            self.global_batch_step += 1\n            self.loader_batch_step = i + 1\n            self._run_batch(batch)\n            if self.need_early_stop:\n                self.need_early_stop = False\n                break\n\n    def _run_epoch(self, stage: str, epoch: int) -> None:\n        """"""\n        Inner method to run epoch on Runner,\n        with epoch callbacks events.\n\n        Args:\n            stage (str): stage name of interest,\n                like ""pretrain"" / ""train"" / ""finetune"" / etc\n            epoch (int): epoch index\n        """"""\n        self._prepare_for_epoch(stage=stage, epoch=epoch)\n        assert self.loaders is not None\n\n        # @TODO: better solution with train/inference handling ?\n        self.is_infer_stage = self.stage_name.startswith(""infer"")\n        if not self.is_infer_stage:\n            assert self.valid_loader in self.loaders.keys(), (\n                f""\'{self.valid_loader}\' ""\n                f""should be in provided loaders: {list(self.loaders.keys())}""\n            )\n        else:\n            # @TODO: add check for non distributed run for inference\n            assert not any(\n                x.startswith(settings.loader_train_prefix)\n                for x in self.loaders.keys()\n            ), ""for inference no train loader should be passed""\n\n        for loader_name, loader in self.loaders.items():\n            self.loader_name = loader_name\n            self.loader_len = len(loader)\n            self.is_train_loader = loader_name.startswith(\n                settings.loader_train_prefix\n            )\n            self.is_valid_loader = loader_name.startswith(\n                settings.loader_valid_prefix\n            )\n            self.is_infer_loader = loader_name.startswith(\n                settings.loader_infer_prefix\n            )\n            utils.maybe_recursive_call(\n                self.model, ""train"", mode=self.is_train_loader,\n            )\n\n            if (\n                isinstance(loader.sampler, DistributedSampler)\n                and not self.is_infer_stage\n            ):\n                loader.sampler.set_epoch(self.epoch)\n\n            utils.set_global_seed(\n                self.experiment.initial_seed + self.global_epoch + 1\n            )\n            self._run_event(""on_loader_start"")\n            with torch.set_grad_enabled(self.is_train_loader):\n                self._run_loader(loader)\n            self._run_event(""on_loader_end"")\n\n    def _run_stage(self, stage: str) -> None:\n        """"""\n        Inner method to run stage on Runner,\n        with stage callbacks events.\n\n        Args:\n            stage (str): stage name of interest,\n                like ""pretrain"" / ""train"" / ""finetune"" / etc\n\n        """"""\n        self._prepare_for_stage(stage)\n\n        self._run_event(""on_stage_start"")\n        while self.epoch < self.num_epochs + 1:\n            utils.set_global_seed(\n                self.experiment.initial_seed + self.global_epoch + 1\n            )\n            self._run_event(""on_epoch_start"")\n            self._run_epoch(stage=stage, epoch=self.epoch)\n            self._run_event(""on_epoch_end"")\n\n            if self.need_early_stop:\n                self.need_early_stop = False\n                break\n\n            self.global_epoch += 1\n            self.epoch += 1\n        self._run_event(""on_stage_end"")\n\n    def run_experiment(self, experiment: IExperiment = None) -> ""IRunner"":\n        """"""\n        Starts the experiment.\n\n        Args:\n            experiment (IExperiment): Experiment instance to use for Runner.\n\n        """"""\n        self.experiment = experiment or self.experiment\n        assert self.experiment is not None\n\n        try:\n            for stage in self.experiment.stages:\n                self._run_stage(stage)\n        except (Exception, KeyboardInterrupt) as ex:\n            from catalyst.core.callbacks.exception import ExceptionCallback\n\n            def _exception_handler_check(callbacks: Union[OrderedDict, Dict]):\n                return callbacks is not None and any(\n                    issubclass(x.__class__, ExceptionCallback)\n                    for x in callbacks.values()\n                )\n\n            if _exception_handler_check(getattr(self, ""callbacks"", None)):\n                self.exception = ex\n                self._run_event(""on_exception"")\n            else:\n                raise ex\n\n        return self\n\n\nclass IStageBasedRunner(IRunner):\n    """"""\n    Runner abstraction that suppose to have constant\n    datasources per stage.\n    """"""\n\n    def _prepare_for_stage(self, stage: str):\n        """"""\n        Inner method to prepare `Runner` for the specified stage.\n\n        Sets `Experiment` initial seed.\n        Prepares experiment components with `self._get_experiment_components`.\n        Prepares callbacks with `self._get_experiment_callbacks`.\n        Prepares inner state with `self._prepare_inner_state`\n        Additionally sets `Experiment` datasources for specified stage.\n\n        Args:\n            stage (str): stage name of interest,\n                like ""pretrain"" / ""train"" / ""finetune"" / etc\n        """"""\n        super()._prepare_for_stage(stage=stage)\n\n        utils.set_global_seed(self.experiment.initial_seed)\n        loaders = self.experiment.get_loaders(stage=stage)\n        loaders = utils.validate_loaders(loaders)\n        self.loaders = loaders\n\n\n__all__ = [""IRunner"", ""IStageBasedRunner""]\n'"
catalyst/core/state.py,0,b'from catalyst.core.runner import IRunner as State  # noqa: F401\n'
catalyst/data/__init__.py,0,"b'# flake8: noqa\n\nfrom .augmentor import Augmentor, AugmentorCompose, AugmentorKeys\nfrom .collate_fn import FilteringCollateFn\nfrom .dataset import (\n    DatasetFromSampler,\n    ListDataset,\n    MergeDataset,\n    NumpyDataset,\n    PathsDataset,\n)\nfrom .reader import LambdaReader, ReaderCompose, ReaderSpec, ScalarReader\nfrom .sampler import (\n    BalanceClassSampler,\n    DistributedSamplerWrapper,\n    DynamicLenBatchSampler,\n    MiniEpochSampler,\n)\n'"
catalyst/data/__main__.py,0,"b'# -*- coding: utf-8 -*-\nr""""""Catalyst-data scripts.\n\nExamples:\n    1.  **process-images** reads raw data and outputs\n    preprocessed resized images\n\n    .. code:: bash\n\n        $ catalyst-data process-images \\\\\n            --in-dir /path/to/raw/data/ \\\\\n            --out-dir=./data/dataset \\\\\n            --num-workers=6 \\\\\n            --max-size=224 \\\\\n            --extension=png \\\\\n            --clear-exif \\\\\n            --grayscale \\\\\n            --expand-dims\n\n    2. **tag2label** prepares a dataset to json like\n    `{""class_id"":  class_column_from_dataset}`\n\n    .. code:: bash\n\n        $ catalyst-data tag2label \\\\\n            --in-dir=./data/dataset \\\\\n            --out-dataset=./data/dataset_raw.csv \\\\\n            --out-labeling=./data/tag2cls.json\n\n    3. **check-images** checks images in your data\n    to be non-broken and writes a flag:\n    `true` if image opened without an error and `false` otherwise\n\n    .. code:: bash\n\n        $ catalyst-data check-images \\\\\n            --in-csv=./data/dataset_raw.csv \\\\\n            --img-rootpath=./data/dataset \\\\\n            --img-col=""tag"" \\\\\n            --out-csv=./data/dataset_checked.csv \\\\\n            --n-cpu=4\n\n    4. **split-dataframe** split your dataset into train/valid folds\n\n    .. code:: bash\n\n        $ catalyst-data split-dataframe \\\\\n            --in-csv=./data/dataset_raw.csv \\\\\n            --tag2class=./data/tag2cls.json \\\\\n            --tag-column=tag \\\\\n            --class-column=class \\\\\n            --n-folds=5 \\\\\n            --train-folds=0,1,2,3 \\\\\n            --out-csv=./data/dataset.csv\n\n    5. **image2embedding** embeds images from your csv\n    or image directory with specified neural net architecture\n\n    .. code:: bash\n\n        $ catalyst-data image2embedding \\\\\n            --in-csv=./data/input.csv \\\\\n            --img-col=""filename"" \\\\\n            --img-size=64 \\\\\n            --out-npy=./embeddings.npy \\\\\n            --arch=resnet34 \\\\\n            --pooling=GlobalMaxPool2d \\\\\n            --batch-size=8 \\\\\n            --num-workers=16 \\\\\n            --verbose\n""""""\n\nfrom argparse import ArgumentParser, RawTextHelpFormatter\nfrom collections import OrderedDict\nimport logging\n\nfrom catalyst.__version__ import __version__\nfrom catalyst.data.scripts import split_dataframe, tag2label\nfrom catalyst.tools import settings\n\nlogger = logging.getLogger(__name__)\n\nCOMMANDS = OrderedDict(\n    [(""tag2label"", tag2label), (""split-dataframe"", split_dataframe)]\n)\n\ntry:\n    import imageio  # noqa: F401\n    from catalyst.data.scripts import (\n        image2embedding,\n        process_images,\n        project_embeddings,\n    )\n\n    COMMANDS[""process-images""] = process_images\n    COMMANDS[""image2embedding""] = image2embedding\n    COMMANDS[""project-embeddings""] = project_embeddings\nexcept ImportError as ex:\n    if settings.cv_required:\n        logger.warning(\n            ""some of catalyst-cv dependencies not available,""\n            "" to install dependencies, run `pip install catalyst[cv]`.""\n        )\n        raise ex\n\ntry:\n    import transformers  # noqa: F401\n    from catalyst.data.scripts import text2embedding\n\n    COMMANDS[""text2embedding""] = text2embedding\nexcept ImportError as ex:\n    if settings.transformers_required:\n        logger.warning(\n            ""transformers not available, to install transformers,""\n            "" run `pip install transformers`.""\n        )\n        raise ex\n\n\ndef build_parser() -> ArgumentParser:\n    """"""\n    @TODO: Docs. Contribution is welcome\n    """"""\n    parser = ArgumentParser(\n        ""catalyst-data"", formatter_class=RawTextHelpFormatter\n    )\n    parser.add_argument(\n        ""-v"", ""--version"", action=""version"", version=f""%(prog)s {__version__}""\n    )\n    all_commands = "", \\n"".join(map(lambda x: f""    {x}"", COMMANDS.keys()))\n\n    subparsers = parser.add_subparsers(\n        metavar=""{command}"",\n        dest=""command"",\n        help=f""available commands: \\n{all_commands}"",\n    )\n    subparsers.required = True\n\n    for key, value in COMMANDS.items():\n        value.build_args(subparsers.add_parser(key))\n\n    return parser\n\n\ndef main():\n    """"""\n    @TODO: Docs. Contribution is welcome\n    """"""\n    parser = build_parser()\n\n    args, uargs = parser.parse_known_args()\n\n    COMMANDS[args.command].main(args, uargs)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
catalyst/data/augmentor.py,0,"b'from typing import Callable, Dict, List, Union\n\n\nclass Augmentor:\n    """"""Augmentation abstraction to use with data dictionaries.""""""\n\n    def __init__(\n        self,\n        dict_key: str,\n        augment_fn: Callable,\n        input_key: str = None,\n        output_key: str = None,\n        **kwargs,\n    ):\n        """"""\n        Args:\n            dict_key (str): key to transform\n            augment_fn (Callable): augmentation function to use\n            input_key (str): ``augment_fn`` input key\n            output_key (str): ``augment_fn`` output key\n            **kwargs: default kwargs for augmentations function\n        """"""\n        self.dict_key = dict_key\n        self.augment_fn = augment_fn\n        self.input_key = input_key\n        self.output_key = output_key\n        self.kwargs = kwargs\n\n    def __call__(self, dict_: dict):\n        """"""Applies the augmentation.""""""\n        if self.input_key is not None:\n            output = self.augment_fn(\n                **{self.input_key: dict_[self.dict_key]}, **self.kwargs\n            )\n        else:\n            output = self.augment_fn(dict_[self.dict_key], **self.kwargs)\n\n        if self.output_key is not None:\n            dict_[self.dict_key] = output[self.output_key]\n        else:\n            dict_[self.dict_key] = output\n        return dict_\n\n\nclass AugmentorCompose:\n    """"""Compose augmentors.""""""\n\n    def __init__(self, key2augment_fn: Dict[str, Callable]):\n        """"""\n        Args:\n            key2augment_fn (Dict[str, Callable]): mapping from input key\n                to augmentation function to apply\n        """"""\n        self.key2augment_fn = key2augment_fn\n\n    def __call__(self, dictionary: dict) -> dict:\n        """"""\n        Args:\n            dictionary (dict): item from dataset\n\n        Returns:\n            dict: dictionaty with augmented data\n        """"""\n        results = {}\n        for key, augment_fn in self.key2augment_fn.items():\n            results = {**results, **augment_fn({key: dictionary[key]})}\n\n        return {**dictionary, **results}\n\n\nclass AugmentorKeys:\n    """"""Augmentation abstraction to match input and augmentations keys.""""""\n\n    def __init__(\n        self,\n        dict2fn_dict: Union[Dict[str, str], List[str]],\n        augment_fn: Callable,\n    ):\n        """"""\n        Args:\n            dict2fn_dict (Dict[str, str]): keys matching dict\n                ``{input_key: augment_fn_key}``. For example:\n                ``{""image"": ""image"", ""mask"": ""mask""}``\n            augment_fn: augmentation function\n        """"""\n        if isinstance(dict2fn_dict, list):\n            dict2fn_dict = {key: key for key in dict2fn_dict}\n\n        self.dict2fn_dict = dict2fn_dict\n        self.augment_fn = augment_fn\n\n    def __call__(self, dictionary: dict) -> dict:\n        """"""\n        Args:\n            dictionary (dict): item from dataset\n\n        Returns:\n            dict: dictionaty with augmented data\n        """"""\n        # link keys from dict_ with augment_fn keys\n        data = {\n            fn_key: dictionary[dict_key]\n            for dict_key, fn_key in self.dict2fn_dict.items()\n        }\n\n        augmented = self.augment_fn(**data)\n\n        # link keys from augment_fn back to dict_ keys\n        results = {\n            dict_key: augmented[fn_key]\n            for dict_key, fn_key in self.dict2fn_dict.items()\n        }\n\n        return {**dictionary, **results}\n\n\n__all__ = [""Augmentor"", ""AugmentorCompose"", ""AugmentorKeys""]\n'"
catalyst/data/collate_fn.py,2,"b'import collections\n\nfrom torch.utils.data.dataloader import default_collate\n\n\nclass FilteringCollateFn:\n    """"""\n    Callable object doing job of ``collate_fn`` like ``default_collate``,\n    but does not cast batch items with specified key to :class:`torch.Tensor`.\n\n    Only adds them to list.\n    Supports only key-value format batches\n    """"""\n\n    def __init__(self, *keys):\n        """"""\n        Args:\n            keys: Keys for values that will not be\n                converted to tensor and stacked\n        """"""\n        self.keys = keys\n\n    def __call__(self, batch):\n        """"""\n        Args:\n            batch: current batch\n\n        Returns:\n            batch values filtered by `keys`\n        """"""\n        if isinstance(batch[0], collections.Mapping):\n            result = {}\n            for key in batch[0]:\n                items = [d[key] for d in batch]\n                if key not in self.keys:\n                    items = default_collate(items)\n                result[key] = items\n            return result\n        else:\n            return default_collate(batch)\n\n\n__all__ = [""FilteringCollateFn""]\n'"
catalyst/data/dataset.py,1,"b'from typing import Any, Callable, Dict, List, Union\nfrom pathlib import Path\n\nimport numpy as np\n\nfrom torch.utils.data import Dataset, Sampler\n\nfrom catalyst.utils import merge_dicts\n\n_Path = Union[str, Path]\n\n\nclass ListDataset(Dataset):\n    """"""General purpose dataset class with several data sources `list_data`.""""""\n\n    def __init__(\n        self,\n        list_data: List[Dict],\n        open_fn: Callable,\n        dict_transform: Callable = None,\n    ):\n        """"""\n        Args:\n            list_data (List[Dict]): list of dicts, that stores\n                you data annotations,\n                (for example path to images, labels, bboxes, etc.)\n            open_fn (callable): function, that can open your\n                annotations dict and\n                transfer it to data, needed by your network\n                (for example open image by path, or tokenize read string.)\n            dict_transform (callable): transforms to use on dict.\n                (for example normalize image, add blur, crop/resize/etc)\n        """"""\n        self.data = list_data\n        self.open_fn = open_fn\n        self.dict_transform = (\n            dict_transform if dict_transform is not None else lambda x: x\n        )\n\n    def __getitem__(self, index: int) -> Any:\n        """"""Gets element of the dataset.\n\n        Args:\n            index (int): index of the element in the dataset\n\n        Returns:\n            Single element by index\n        """"""\n        item = self.data[index]\n        dict_ = self.open_fn(item)\n        dict_ = self.dict_transform(dict_)\n\n        return dict_\n\n    def __len__(self) -> int:\n        """"""\n        Returns:\n            int: length of the dataset\n        """"""\n        return len(self.data)\n\n\nclass MergeDataset(Dataset):\n    """"""Abstraction to merge several datasets into one dataset.""""""\n\n    def __init__(self, *datasets: Dataset, dict_transform: Callable = None):\n        """"""\n        Args:\n            datasets (List[Dataset]): params count of datasets to merge\n            dict_transform (callable): transforms common for all datasets.\n                (for example normalize image, add blur, crop/resize/etc)\n        """"""\n        self.len = len(datasets[0])\n        assert all(len(x) == self.len for x in datasets)\n        self.datasets = datasets\n        self.dict_transform = dict_transform\n\n    def __getitem__(self, index: int) -> Any:\n        """"""Get item from all datasets.\n\n        Args:\n            index (int): index to value from all datasets\n\n        Returns:\n            list: list of value in every dataset\n        """"""\n        dcts = [x[index] for x in self.datasets]\n        dct = merge_dicts(*dcts)\n\n        if self.dict_transform is not None:\n            dct = self.dict_transform(dct)\n\n        return dct\n\n    def __len__(self) -> int:\n        """"""\n        Returns:\n            int: length of the dataset\n        """"""\n        return self.len\n\n\nclass NumpyDataset(Dataset):\n    """"""General purpose dataset class to use with `numpy_data`.""""""\n\n    def __init__(\n        self,\n        numpy_data: np.ndarray,\n        numpy_key: str = ""features"",\n        dict_transform: Callable = None,\n    ):\n        """"""\n        Args:\n            numpy_data (np.ndarray): numpy data\n                (for example path to embeddings, features, etc.)\n            numpy_key (str): key to use for output dictionary\n            dict_transform (callable): transforms to use on dict.\n                (for example normalize vector, etc)\n        """"""\n        super().__init__()\n        self.data = numpy_data\n        self.key = numpy_key\n        self.dict_transform = (\n            dict_transform if dict_transform is not None else lambda x: x\n        )\n\n    def __getitem__(self, index: int) -> Any:\n        """"""Gets element of the dataset.\n\n        Args:\n            index (int): index of the element in the dataset\n\n        Returns:\n            Single element by index\n        """"""\n        dict_ = {self.key: np.copy(self.data[index])}\n        dict_ = self.dict_transform(dict_)\n        return dict_\n\n    def __len__(self) -> int:\n        """"""\n        Returns:\n            int: length of the dataset\n        """"""\n        return len(self.data)\n\n\nclass PathsDataset(ListDataset):\n    """"""\n    Dataset that derives features and targets from samples filesystem paths.\n\n    Examples:\n        >>> label_fn = lambda x: x.split(""_"")[0]\n        >>> dataset = PathsDataset(\n        >>>     filenames=Path(""/path/to/images/"").glob(""*.jpg""),\n        >>>     label_fn=label_fn,\n        >>>     open_fn=open_fn,\n        >>> )\n    """"""\n\n    def __init__(\n        self,\n        filenames: List[_Path],\n        open_fn: Callable[[dict], dict],\n        label_fn: Callable[[_Path], Any],\n        **list_dataset_params\n    ):\n        """"""\n        Args:\n            filenames (List[str]): list of file paths that store information\n                about your dataset samples; it could be images, texts or\n                any other files in general.\n            open_fn (callable): function, that can open your\n                annotations dict and\n                transfer it to data, needed by your network\n                (for example open image by path, or tokenize read string)\n            label_fn (callable): function, that can extract target\n                value from sample path\n                (for example, your sample could be an image file like\n                ``/path/to/your/image_1.png`` where the target is encoded as\n                a part of file path)\n            list_dataset_params (dict): base class initialization\n                parameters.\n        """"""\n        list_data = [\n            {""features"": filename, ""targets"": label_fn(filename)}\n            for filename in filenames\n        ]\n\n        super().__init__(\n            list_data=list_data, open_fn=open_fn, **list_dataset_params\n        )\n\n\nclass DatasetFromSampler(Dataset):\n    """"""Dataset of indexes from `Sampler`.""""""\n\n    def __init__(self, sampler: Sampler):\n        """"""\n        Args:\n            sampler (Sampler): @TODO: Docs. Contribution is welcome\n        """"""\n        self.sampler = sampler\n        self.sampler_list = None\n\n    def __getitem__(self, index: int):\n        """"""Gets element of the dataset.\n\n        Args:\n            index (int): index of the element in the dataset\n\n        Returns:\n            Single element by index\n        """"""\n        if self.sampler_list is None:\n            self.sampler_list = list(self.sampler)\n        return self.sampler_list[index]\n\n    def __len__(self) -> int:\n        """"""\n        Returns:\n            int: length of the dataset\n        """"""\n        return len(self.sampler)\n\n\n__all__ = [\n    ""ListDataset"",\n    ""MergeDataset"",\n    ""NumpyDataset"",\n    ""PathsDataset"",\n    ""DatasetFromSampler"",\n]\n'"
catalyst/data/reader.py,0,"b'from typing import Callable, List, Type\nimport functools\n\nimport numpy as np\n\nfrom catalyst.utils import get_one_hot\n\n\nclass ReaderSpec:\n    """"""Reader abstraction for all Readers.\n\n    Applies a function to an element of your data.\n    For example to a row from csv, or to an image, etc.\n\n    All inherited classes have to implement `__call__`.\n    """"""\n\n    def __init__(self, input_key: str, output_key: str):\n        """"""\n        Args:\n            input_key (str): input key to use from annotation dict\n            output_key (str): output key to use to store the result\n        """"""\n        self.input_key = input_key\n        self.output_key = output_key\n\n    def __call__(self, element):\n        """"""\n        Reads a row from your annotations dict and transfer it to data,\n        needed by your network for example open image by path,\n        or read string and tokenize it.\n\n        Args:\n            element: elem in your dataset\n\n        Returns:\n            Data object used for your neural network\n        """"""\n        raise NotImplementedError(\n            ""You cannot apply a transformation using `BaseReader`""\n        )\n\n\nclass ScalarReader(ReaderSpec):\n    """"""\n    Numeric data reader abstraction.\n    Reads a single float, int, str or other from data\n    """"""\n\n    def __init__(\n        self,\n        input_key: str,\n        output_key: str,\n        dtype: Type = np.float32,\n        default_value: float = None,\n        one_hot_classes: int = None,\n        smoothing: float = None,\n    ):\n        """"""\n        Args:\n            input_key (str): input key to use from annotation dict\n            output_key (str): output key to use to store the result\n            dtype (type): datatype of scalar values to use\n            default_value: default value to use if something goes wrong\n            one_hot_classes (int): number of one-hot classes\n            smoothing (float, optional): if specified applies label smoothing\n                to one_hot classes\n        """"""\n        super().__init__(input_key, output_key)\n        self.dtype = dtype\n        self.default_value = default_value\n        self.one_hot_classes = one_hot_classes\n        self.smoothing = smoothing\n        if self.one_hot_classes is not None and self.smoothing is not None:\n            assert 0.0 < smoothing < 1.0, (\n                f""If smoothing is specified it must be in (0; 1), ""\n                f""got {smoothing}""\n            )\n\n    def __call__(self, element):\n        """"""\n        Reads a row from your annotations dict and\n        transfer it to a single value\n\n        Args:\n            element: elem in your dataset\n\n        Returns:\n            dtype: Scalar value\n        """"""\n        scalar = self.dtype(element.get(self.input_key, self.default_value))\n        if self.one_hot_classes is not None:\n            scalar = get_one_hot(\n                scalar, self.one_hot_classes, smoothing=self.smoothing\n            )\n        output = {self.output_key: scalar}\n        return output\n\n\nclass LambdaReader(ReaderSpec):\n    """"""\n    Reader abstraction with an lambda encoders.\n    Can read an elem from dataset and apply `encode_fn` function to it.\n    """"""\n\n    def __init__(\n        self,\n        input_key: str,\n        output_key: str,\n        lambda_fn: Callable = lambda x: x,\n        **kwargs,\n    ):\n        """"""\n        Args:\n            input_key (str): input key to use from annotation dict\n            output_key (str): output key to use to store the result\n            lambda_fn (callable): encode function to use to prepare your data\n                (for example convert chars/words/tokens to indices, etc)\n            kwargs: kwargs for encode function\n        """"""\n        super().__init__(input_key, output_key)\n        self.lambda_fn = functools.partial(lambda_fn, **kwargs)\n\n    def __call__(self, element):\n        """"""\n        Reads a row from your annotations dict\n        and applies `encode_fn` function.\n\n        Args:\n            element: elem in your dataset.\n\n        Returns:\n            Value after applying `lambda_fn` function\n        """"""\n        if self.input_key is not None:\n            element = element[self.input_key]\n        output = self.lambda_fn(element)\n        if self.output_key is not None:\n            output = {self.output_key: output}\n        return output\n\n\nclass ReaderCompose(object):\n    """"""Abstraction to compose several readers into one open function.""""""\n\n    def __init__(self, readers: List[ReaderSpec], mixins: list = None):\n        """"""\n        Args:\n            readers (List[ReaderSpec]): list of reader to compose\n            mixins (list): list of mixins to use\n        """"""\n        self.readers = readers\n        self.mixins = mixins or []\n\n    def __call__(self, element):\n        """"""\n        Reads a row from your annotations dict\n        and applies all readers and mixins\n\n        Args:\n            element: elem in your dataset.\n\n        Returns:\n            Value after applying all readers and mixins\n        """"""\n        result = {}\n        for fn in self.readers:\n            result = {**result, **fn(element)}\n        for fn in self.mixins:\n            result = {**result, **fn(result)}\n        return result\n\n\ntry:\n    # @TODO: remove hotfix\n    from catalyst.contrib.data.cv.reader import (  # noqa: F401\n        ImageReader,\n        MaskReader,\n    )\n\n    __all__ = [\n        ""ReaderSpec"",\n        ""ScalarReader"",\n        ""LambdaReader"",\n        ""ReaderCompose"",\n        ""ImageReader"",\n        ""MaskReader"",\n    ]\nexcept ImportError:\n    __all__ = [\n        ""ReaderSpec"",\n        ""ScalarReader"",\n        ""LambdaReader"",\n        ""ReaderCompose"",\n    ]\n'"
catalyst/data/sampler.py,6,"b'from typing import Iterator, List, Optional, Union\nfrom collections import Counter\nfrom operator import itemgetter\nfrom random import choices, sample\n\nimport numpy as np\n\nimport torch\nfrom torch.utils.data import DistributedSampler\nfrom torch.utils.data.sampler import BatchSampler, Sampler\n\nfrom catalyst.contrib.utils.misc import find_value_ids\nfrom catalyst.data import DatasetFromSampler\n\n\nclass BalanceClassSampler(Sampler):\n    """"""Abstraction over data sampler.\n\n    Allows you to create stratified sample on unbalanced classes.\n    """"""\n\n    def __init__(\n        self, labels: List[int], mode: Union[str, int] = ""downsampling""\n    ):\n        """"""\n        Args:\n            labels (List[int]): list of class label\n                for each elem in the dataset\n            mode (str): Strategy to balance classes.\n                Must be one of [downsampling, upsampling]\n        """"""\n        super().__init__(labels)\n\n        labels = np.array(labels)\n        samples_per_class = {\n            label: (labels == label).sum() for label in set(labels)\n        }\n\n        self.lbl2idx = {\n            label: np.arange(len(labels))[labels == label].tolist()\n            for label in set(labels)\n        }\n\n        if isinstance(mode, str):\n            assert mode in [""downsampling"", ""upsampling""]\n\n        if isinstance(mode, int) or mode == ""upsampling"":\n            samples_per_class = (\n                mode\n                if isinstance(mode, int)\n                else max(samples_per_class.values())\n            )\n        else:\n            samples_per_class = min(samples_per_class.values())\n\n        self.labels = labels\n        self.samples_per_class = samples_per_class\n        self.length = self.samples_per_class * len(set(labels))\n\n    def __iter__(self) -> Iterator[int]:\n        """"""\n        Yields:\n            indices of stratified sample\n        """"""\n        indices = []\n        for key in sorted(self.lbl2idx):\n            replace_ = self.samples_per_class > len(self.lbl2idx[key])\n            indices += np.random.choice(\n                self.lbl2idx[key], self.samples_per_class, replace=replace_\n            ).tolist()\n        assert len(indices) == self.length\n        np.random.shuffle(indices)\n\n        return iter(indices)\n\n    def __len__(self) -> int:\n        """"""\n        Returns:\n             length of result sample\n        """"""\n        return self.length\n\n\nclass BalanceBatchSampler(Sampler):\n    """"""\n    This kind of sampler can be used for both metric learning and\n    classification task.\n\n    Sampler with the given strategy for the C unique classes dataset:\n    - Selection P of C classes for the 1st batch\n    - Selection K instances for each class for the 1st batch\n    - Selection P of C - P remaining classes for 2nd batch\n    - Selection K instances for each class for the 2nd batch\n    - ...\n    The epoch ends when there are no classes left.\n    So, the batch sise is P * K except the last one.\n\n    Thus, in each epoch, all the classes will be selected once, but this\n    does not mean that all the instances will be selected during the epoch.\n\n    One of the purposes of this sampler is to be used for\n    forming triplets and pos/neg pairs inside the batch.\n    To guarante existance of these pairs in the batch,\n    P and K should be > 1. (1)\n\n    Behavior in corner cases:\n    - If a class does not contain K instances,\n    a choice will be made with repetition.\n    - If C % P == 1 then one of the classes should be dropped\n    otherwise statement (1) will not be met.\n\n    This type of sampling can be found in the classical paper of Person Re-Id:\n    ""In Defense of the Triplet Loss for Person Re-Identification"",\n    where P equals 32 and K equals 4.\n    """"""\n\n    def __init__(self, labels: List[int], p: int, k: int):\n        """"""\n        Args:\n            labels: list of classes labeles for each elem in the dataset\n            p: number of classes in a batch, should be > 1\n            k: number of instances of each class in a batch, should be > 1\n        """"""\n        super().__init__(self)\n        classes = set(labels)\n\n        assert isinstance(p, int) and isinstance(k, int)\n        assert (1 < p <= len(classes)) and (1 < k)\n        assert all(\n            n > 1 for n in Counter(labels).values()\n        ), ""Each class shoud contain at least 2 instances to fit (1)""\n\n        self._labels = tuple(labels)  # to make it immutable\n        self._p = p\n        self._k = k\n\n        self._batch_size = self._p * self._k\n        self._classes = classes\n\n        # to satisfy statement (1)\n        num_classes = len(self._classes)\n        if num_classes % self._p == 1:\n            self._num_epoch_classes = num_classes - 1\n        else:\n            self._num_epoch_classes = num_classes\n\n    @property\n    def batch_size(self) -> int:\n        """"""\n        Returns: this value should be used in DataLoader as batch size\n        """"""\n        return self._batch_size\n\n    @property\n    def batches_in_epoch(self) -> int:\n        """"""\n        Returns: number of batches in an epoch\n        """"""\n        return int(np.ceil(self._num_epoch_classes / self._p))\n\n    def __len__(self) -> int:\n        """"""\n        Returns: number of samples in an epoch\n        """"""\n        return self._num_epoch_classes * self._k\n\n    def __iter__(self) -> Iterator[int]:\n        """"""\n        Returns: indeces for sampling dataset elems during an epoch\n        """"""\n        inds = []\n\n        for cls in sample(self._classes, self._num_epoch_classes):\n            all_cls_inds = find_value_ids(self._labels, cls)\n\n            # we\'ve checked in __init__ that this value must be > 1\n            num_samples_exists = len(all_cls_inds)\n\n            if num_samples_exists < self._k:\n                selected_inds = sample(\n                    all_cls_inds, k=num_samples_exists\n                ) + choices(all_cls_inds, k=self._k - num_samples_exists)\n            else:\n                selected_inds = sample(all_cls_inds, k=self._k)\n\n            inds.extend(selected_inds)\n\n        return iter(inds)\n\n\nclass MiniEpochSampler(Sampler):\n    """"""\n    Sampler iterates mini epochs from the dataset used by ``mini_epoch_len``.\n\n    Example:\n        >>> MiniEpochSampler(len(dataset), mini_epoch_len=100)\n        >>> MiniEpochSampler(len(dataset), mini_epoch_len=100,\n        >>>     drop_last=True)\n        >>> MiniEpochSampler(len(dataset), mini_epoch_len=100,\n        >>>     shuffle=""per_epoch"")\n    """"""\n\n    def __init__(\n        self,\n        data_len: int,\n        mini_epoch_len: int,\n        drop_last: bool = False,\n        shuffle: str = None,\n    ):\n        """"""\n        Args:\n            data_len (int): Size of the dataset\n            mini_epoch_len (int): Num samples from the dataset used in one\n                mini epoch.\n            drop_last (bool): If ``True``, sampler will drop the last batches\n                if its size would be less than ``batches_per_epoch``\n            shuffle (str): one of  ``""always""``, ``""real_epoch""``, or `None``.\n                The sampler will shuffle indices\n                > ""per_mini_epoch"" - every mini epoch (every ``__iter__`` call)\n                > ""per_epoch"" -- every real epoch\n                > None -- don\'t shuffle\n        """"""\n        super().__init__(None)\n\n        self.data_len = int(data_len)\n        self.mini_epoch_len = int(mini_epoch_len)\n\n        self.steps = int(data_len / self.mini_epoch_len)\n        self.state_i = 0\n\n        has_reminder = data_len - self.steps * mini_epoch_len > 0\n        if self.steps == 0:\n            self.divider = 1\n        elif has_reminder and not drop_last:\n            self.divider = self.steps + 1\n        else:\n            self.divider = self.steps\n\n        self._indices = np.arange(self.data_len)\n        self.indices = self._indices\n        self.end_pointer = max(self.data_len, self.mini_epoch_len)\n\n        if not (shuffle is None or shuffle in [""per_mini_epoch"", ""per_epoch""]):\n            raise ValueError(\n                f""Shuffle must be one of [\'per_mini_epoch\', \'per_epoch\']. ""\n                f""Got {shuffle}""\n            )\n        self.shuffle_type = shuffle\n\n    def shuffle(self) -> None:\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        if self.shuffle_type == ""per_mini_epoch"" or (\n            self.shuffle_type == ""per_epoch"" and self.state_i == 0\n        ):\n            if self.data_len >= self.mini_epoch_len:\n                self.indices = self._indices\n                np.random.shuffle(self.indices)\n            else:\n                self.indices = np.random.choice(\n                    self._indices, self.mini_epoch_len, replace=True\n                )\n\n    def __iter__(self) -> Iterator[int]:\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        self.state_i = self.state_i % self.divider\n        self.shuffle()\n\n        start = self.state_i * self.mini_epoch_len\n        stop = (\n            self.end_pointer\n            if (self.state_i == self.steps)\n            else (self.state_i + 1) * self.mini_epoch_len\n        )\n        indices = self.indices[start:stop].tolist()\n\n        self.state_i += 1\n        return iter(indices)\n\n    def __len__(self) -> int:\n        """"""\n        Returns:\n            int: length of the mini-epoch\n        """"""\n        return self.mini_epoch_len\n\n\nclass DynamicLenBatchSampler(BatchSampler):\n    """"""\n    A dynamic batch length data sampler.\n    Should be used with `catalyst.utils.trim_tensors`.\n\n    Adapted from ""Dynamic minibatch trimming to improve BERT training speed""\n    https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/discussion/94779\n\n    Args:\n        sampler (torch.utils.data.Sampler): Base sampler.\n        batch_size (int): Size of minibatch.\n        drop_last (bool): If ``True``, the sampler will drop the last batch\n        if its size would be less than ``batch_size``.\n\n    Usage example:\n\n        >>> from torch.utils import data\n        >>> from catalyst.data import DynamicLenBatchSampler\n        >>> from catalyst import utils\n\n        >>> dataset = data.TensorDataset(\n        >>>     input_ids, input_mask, segment_ids, labels\n        >>> )\n\n        >>> sampler_ = data.RandomSampler(dataset)\n        >>> sampler = DynamicLenBatchSampler(\n        >>>     sampler_, batch_size=16, drop_last=False\n        >>> )\n        >>> loader = data.DataLoader(dataset, batch_sampler=sampler)\n\n        >>> for batch in loader:\n        >>>     tensors = utils.trim_tensors(batch)\n        >>>     b_input_ids, b_input_mask, b_segment_ids, b_labels = \\\n        >>>         tuple(t.to(device) for t in tensors)\n    """"""\n\n    def __iter__(self):\n        """"""\n        Iteration over BatchSampler.\n        """"""\n        buckets = [[]] * 100\n        yielded = 0\n\n        for idx in self.sampler:\n            count_zeros = torch.sum(self.sampler.data_source[idx][0] == 0)\n            count_zeros = int(count_zeros / 64)\n            if len(buckets[count_zeros]) == 0:\n                buckets[count_zeros] = []\n\n            buckets[count_zeros].append(idx)\n\n            if len(buckets[count_zeros]) == self.batch_size:\n                batch = list(buckets[count_zeros])\n                yield batch\n                yielded += 1\n                buckets[count_zeros] = []\n\n        batch = []\n        leftover = [idx for bucket in buckets for idx in bucket]\n\n        for idx in leftover:\n            batch.append(idx)\n            if len(batch) == self.batch_size:\n                yielded += 1\n                yield batch\n                batch = []\n\n        if len(batch) > 0 and not self.drop_last:\n            yielded += 1\n            yield batch\n\n        assert len(self) == yielded, (\n            ""produced an inccorect number of batches. ""\n            ""expected %i, but yielded %i"" % (len(self), yielded)\n        )\n\n\nclass DistributedSamplerWrapper(DistributedSampler):\n    """"""\n    Wrapper over `Sampler` for distributed training.\n    Allows you to use any sampler in distributed mode.\n\n    It is especially useful in conjunction with\n    :class:`torch.nn.parallel.DistributedDataParallel`. In such case, each\n    process can pass a DistributedSamplerWrapper instance as a DataLoader\n    sampler, and load a subset of subsampled data of the original dataset\n    that is exclusive to it.\n\n    .. note::\n        Sampler is assumed to be of constant size.\n    """"""\n\n    def __init__(\n        self,\n        sampler,\n        num_replicas: Optional[int] = None,\n        rank: Optional[int] = None,\n        shuffle: bool = True,\n    ):\n        """"""\n        Args:\n            sampler: Sampler used for subsampling\n            num_replicas (int, optional): Number of processes participating in\n                distributed training\n            rank (int, optional): Rank of the current process\n                within ``num_replicas``\n            shuffle (bool, optional): If true (default),\n                sampler will shuffle the indices\n        """"""\n        super(DistributedSamplerWrapper, self).__init__(\n            DatasetFromSampler(sampler),\n            num_replicas=num_replicas,\n            rank=rank,\n            shuffle=shuffle,\n        )\n        self.sampler = sampler\n\n    def __iter__(self):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        self.dataset = DatasetFromSampler(self.sampler)\n        indexes_of_indexes = super().__iter__()\n        subsampler_indexes = self.dataset\n        return iter(itemgetter(*indexes_of_indexes)(subsampler_indexes))\n\n\n__all__ = [\n    ""BalanceClassSampler"",\n    ""BalanceBatchSampler"",\n    ""MiniEpochSampler"",\n    ""DistributedSamplerWrapper"",\n    ""DynamicLenBatchSampler"",\n]\n'"
catalyst/dl/__init__.py,0,b'# flake8: noqa\n# isort:skip_file\n\nfrom catalyst.core import *\n\nfrom .callbacks import *\nfrom .experiment import *\nfrom .runner import *\n'
catalyst/dl/__main__.py,0,"b'from argparse import ArgumentParser, RawTextHelpFormatter\nfrom collections import OrderedDict\n\nfrom catalyst.__version__ import __version__\nfrom catalyst.dl.scripts import init, run, trace\n\nCOMMANDS = OrderedDict([(""init"", init), (""run"", run), (""trace"", trace)])\n\n\ndef build_parser() -> ArgumentParser:\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    parser = ArgumentParser(\n        ""catalyst-dl"", formatter_class=RawTextHelpFormatter\n    )\n    parser.add_argument(\n        ""-v"", ""--version"", action=""version"", version=f""%(prog)s {__version__}""\n    )\n    all_commands = "", \\n"".join(map(lambda x: f""    {x}"", COMMANDS.keys()))\n\n    subparsers = parser.add_subparsers(\n        metavar=""{command}"",\n        dest=""command"",\n        help=f""available commands: \\n{all_commands}"",\n    )\n    subparsers.required = True\n\n    for key, value in COMMANDS.items():\n        value.build_args(subparsers.add_parser(key))\n\n    return parser\n\n\ndef main():\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    parser = build_parser()\n\n    args, uargs = parser.parse_known_args()\n\n    COMMANDS[args.command].main(args, uargs)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
catalyst/dl/registry.py,0,"b'from catalyst.contrib.registry import (\n    Criterion,\n    CRITERIONS,\n    Experiment,\n    EXPERIMENTS,\n    GRAD_CLIPPERS,\n    Model,\n    MODELS,\n    Module,\n    MODULES,\n    Optimizer,\n    OPTIMIZERS,\n    Sampler,\n    SAMPLERS,\n    Scheduler,\n    SCHEDULERS,\n    Transform,\n    TRANSFORMS,\n)\nfrom catalyst.core.registry import Callback, CALLBACKS\nfrom catalyst.tools.registry import Registry\n\n\ndef _callbacks_loader(r: Registry):\n    from catalyst.dl import callbacks as m\n\n    r.add_from_module(m)\n\n\nCALLBACKS.late_add(_callbacks_loader)\n\n__all__ = [\n    ""Callback"",\n    ""Criterion"",\n    ""Optimizer"",\n    ""Scheduler"",\n    ""Module"",\n    ""Model"",\n    ""Sampler"",\n    ""Transform"",\n    ""Experiment"",\n    ""CALLBACKS"",\n    ""CRITERIONS"",\n    ""GRAD_CLIPPERS"",\n    ""MODELS"",\n    ""MODULES"",\n    ""OPTIMIZERS"",\n    ""SAMPLERS"",\n    ""SCHEDULERS"",\n    ""TRANSFORMS"",\n    ""EXPERIMENTS"",\n]\n'"
catalyst/tools/__init__.py,0,"b'# flake8: noqa\nfrom .frozen_class import FrozenClass\nfrom .registry import Registry, RegistryException\nfrom .settings import settings\nfrom .time_manager import TimeManager\n'"
catalyst/tools/frozen_class.py,0,"b'""""""\nFrozen class.\nExample of usage can be found in :py:class:`catalyst.core.runner.IRunner`.\n""""""\n\n\nclass FrozenClass:\n    """"""Class which prohibit ``__setattr__`` on existing attributes.\n\n    Examples:\n        >>> class IRunner(FrozenClass):\n    """"""\n\n    __is_frozen = False\n\n    def __setattr__(self, key, value):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        if self.__is_frozen and not hasattr(self, key):\n            raise TypeError(""%r is a frozen class for key %s"" % (self, key))\n        object.__setattr__(self, key, value)\n\n    def _freeze(self):\n        self.__is_frozen = True\n\n    def _unfreeze(self):\n        self.__is_frozen = False\n'"
catalyst/tools/registry.py,0,"b'""""""\nRegistry.\n.. todo:: Representative docstring for this module\n""""""\nfrom typing import (\n    Any,\n    Callable,\n    Dict,\n    Iterator,\n    List,\n    Mapping,\n    Optional,\n    Tuple,\n    Type,\n    Union,\n)\nimport collections\nimport inspect\nimport warnings\n\nFactory = Union[Type, Callable[..., Any]]\nLateAddCallbak = Callable[[""Registry""], None]\nMetaFactory = Callable[[Factory, Tuple, Mapping], Any]\n\n\ndef _default_meta_factory(factory: Factory, args: Tuple, kwargs: Mapping):\n    return factory(*args, **kwargs)\n\n\nclass RegistryException(Exception):\n    """"""Exception class for all registry errors.""""""\n\n    def __init__(self, message):\n        """"""\n        Init.\n\n        Args:\n            message: exception message\n        """"""\n        super().__init__(message)\n\n\nclass Registry(collections.MutableMapping):\n    """"""\n    Universal class allowing to add and access various factories by name.\n    """"""\n\n    def __init__(\n        self,\n        default_name_key: str,\n        default_meta_factory: MetaFactory = _default_meta_factory,\n    ):\n        """"""\n        Args:\n            default_name_key (str): Default key containing factory name when\n                creating from config\n            default_meta_factory (MetaFactory): default object\n                that calls factory. Optional. Default just calls factory.\n        """"""\n        self.meta_factory = default_meta_factory\n        self._name_key = default_name_key\n        self._factories: Dict[str, Factory] = {}\n        self._late_add_callbacks: List[LateAddCallbak] = []\n\n    @staticmethod\n    def _get_factory_name(f, provided_name=None) -> str:\n        if not provided_name:\n            provided_name = getattr(f, ""__name__"", None)\n            if not provided_name:\n                raise RegistryException(\n                    f""Factory {f} has no __name__ and no "" f""name was provided""\n                )\n            if provided_name == ""<lambda>"":\n                raise RegistryException(\n                    ""Name for lambda factories must be provided""\n                )\n        return provided_name\n\n    def _do_late_add(self):\n        if self._late_add_callbacks:\n            for cb in self._late_add_callbacks:\n                cb(self)\n            self._late_add_callbacks = []\n\n    def add(\n        self,\n        factory: Factory = None,\n        *factories: Factory,\n        name: str = None,\n        **named_factories: Factory,\n    ) -> Factory:\n        """"""\n        Adds factory to registry with it\'s ``__name__`` attribute or provided\n        name. Signature is flexible.\n\n        Args:\n            factory: Factory instance\n            factories: More instances\n            name: Provided name for first instance. Use only when pass\n                single instance.\n            named_factories: Factory and their names as kwargs\n\n        Returns:\n            (Factory): First factory passed\n        """"""\n        if len(factories) > 0 and name is not None:\n            raise RegistryException(\n                ""Multiple factories with single name are not allowed""\n            )\n\n        if factory is not None:\n            named_factories[self._get_factory_name(factory, name)] = factory\n\n        if len(factories) > 0:\n            new = {self._get_factory_name(f): f for f in factories}\n            named_factories.update(new)\n\n        if len(named_factories) == 0:\n            warnings.warn(""No factories were provided!"")\n\n        for name, f in named_factories.items():\n            # self._factories[name] != f is a workaround for\n            # https://github.com/catalyst-team/catalyst/issues/135\n            if name in self._factories and self._factories[name] != f:\n                raise RegistryException(\n                    f""Factory with name \'{name}\' is already present\\n""\n                    f""Already registered: \'{self._factories[name]}\'\\n""\n                    f""New: \'{f}\'""\n                )\n\n        self._factories.update(named_factories)\n\n        return factory\n\n    def late_add(self, cb: LateAddCallbak):\n        """"""\n        Allows to prevent cycle imports by delaying some imports till next\n        registry query.\n\n        Args:\n            cb: Callback receives registry and must call it\'s methods to\n                register factories\n        """"""\n        self._late_add_callbacks.append(cb)\n\n    def add_from_module(\n        self, module, prefix: Union[str, List[str]] = None\n    ) -> None:\n        """"""\n        Adds all factories present in module.\n        If ``__all__`` attribute is present, takes ony what mentioned in it.\n\n        Args:\n            module: module to scan\n            prefix (Union[str, List[str]]): prefix string for all the module\'s\n                factories. If prefix is a list, all values will be treated\n                as aliases.\n        """"""\n        factories = {\n            k: v\n            for k, v in module.__dict__.items()\n            if inspect.isclass(v) or inspect.isfunction(v)\n        }\n\n        # Filter by __all__ if present\n        names_to_add = getattr(module, ""__all__"", list(factories.keys()))\n\n        if prefix is None:\n            prefix = [""""]\n        elif isinstance(prefix, str):\n            prefix = [prefix]\n        elif isinstance(prefix, list):\n            if any((not isinstance(p, str)) for p in prefix):\n                raise TypeError(f""All prefix in list must be strings."")\n        else:\n            raise TypeError(\n                f""Prefix must be a list or a string, got {type(prefix)}.""\n            )\n\n        to_add = {\n            f""{p}{name}"": factories[name]\n            for p in prefix\n            for name in names_to_add\n        }\n        self.add(**to_add)\n\n    def get(self, name: str) -> Optional[Factory]:\n        """"""\n        Retrieves factory, without creating any objects with it\n        or raises error.\n\n        Args:\n            name: factory name\n\n        Returns:\n            Factory: factory by name\n        """"""\n        self._do_late_add()\n\n        if name is None:\n            return None\n\n        res = self._factories.get(name, None)\n\n        if not res:\n            raise RegistryException(\n                f""No factory with name \'{name}\' was registered""\n            )\n\n        return res\n\n    def get_if_str(self, obj: Union[str, Factory]):\n        """"""\n        Returns object from the registry if ``obj`` type is string.\n        """"""\n        if type(obj) is str:\n            return self.get(obj)\n        return obj\n\n    def get_instance(self, name: str, *args, meta_factory=None, **kwargs):\n        """"""\n        Creates instance by calling specified factory\n        with ``instantiate_fn``.\n\n        Args:\n            name: factory name\n            meta_factory: Function that calls factory the right way.\n                If not provided, default is used\n            args: args to pass to the factory\n            **kwargs: kwargs to pass to the factory\n\n        Returns:\n            created instance\n        """"""\n        meta_factory = meta_factory or self.meta_factory\n        f = self.get(name)\n\n        try:\n            if hasattr(f, ""get_from_params""):\n                return f.get_from_params(*args, **kwargs)\n            return meta_factory(f, args, kwargs)\n        except Exception as e:\n            raise RegistryException(\n                f""Factory \'{name}\' call failed: args={args} kwargs={kwargs}""\n            ) from e\n\n    def get_from_params(\n        self, *, meta_factory=None, **kwargs\n    ) -> Union[Any, Tuple[Any, Mapping[str, Any]]]:\n        """"""\n        Creates instance based in configuration dict with ``instantiation_fn``.\n        If ``config[name_key]`` is None, None is returned.\n\n        Args:\n            meta_factory: Function that calls factory the right way.\n                If not provided, default is used.\n            **kwargs: additional kwargs for factory\n\n        Returns:\n            result of calling ``instantiate_fn(factory, **config)``\n        """"""\n        name = kwargs.pop(self._name_key, None)\n        if name:\n            return self.get_instance(name, meta_factory=meta_factory, **kwargs)\n\n    def all(self) -> List[str]:\n        """"""\n        Returns:\n            list of names of registered items\n        """"""\n        self._do_late_add()\n        result = list(self._factories.keys())\n\n        return result\n\n    def len(self) -> int:\n        """"""\n        Returns:\n            length of registered items\n        """"""\n        return len(self._factories)\n\n    def __str__(self) -> str:\n        """"""Returns a string of registered items.""""""\n        return self.all().__str__()\n\n    def __repr__(self) -> str:\n        """"""Returns a string representation of registered items.""""""\n        return self.all().__str__()\n\n    # mapping methods\n    def __len__(self) -> int:\n        """"""Returns length of registered items.""""""\n        self._do_late_add()\n        return self.len()\n\n    def __getitem__(self, name: str) -> Optional[Factory]:\n        """"""Returns a value from the registry by name.""""""\n        return self.get(name)\n\n    def __iter__(self) -> Iterator[str]:\n        """"""Iterates over all registered items.""""""\n        self._do_late_add()\n        return self._factories.__iter__()\n\n    def __contains__(self, name: str):\n        """"""Check if a particular name was registered.""""""\n        self._do_late_add()\n        return self._factories.__contains__(name)\n\n    def __setitem__(self, name: str, factory: Factory) -> None:\n        """"""Add a new factory by giving name.""""""\n        self.add(factory, name=name)\n\n    def __delitem__(self, name: str) -> None:\n        """"""Removes a factory by giving name.""""""\n        self._factories.pop(name)\n\n\n__all__ = [""Registry"", ""RegistryException""]\n'"
catalyst/tools/settings.py,0,"b'from typing import Any, Dict, List, Optional, Tuple\nimport configparser\nimport logging\nimport os\n\nfrom catalyst.tools.frozen_class import FrozenClass\n\nlogger = logging.getLogger(__name__)\n\n\nclass Settings(FrozenClass):\n    def __init__(\n        self,\n        contrib_required: bool = False,\n        cv_required: bool = False,\n        ml_required: bool = False,\n        nlp_required: bool = False,\n        alchemy_logger_required: Optional[bool] = None,\n        neptune_logger_required: Optional[bool] = None,\n        visdom_logger_required: Optional[bool] = None,\n        wandb_logger_required: Optional[bool] = None,\n        plotly_required: Optional[bool] = None,\n        telegram_logger_token: Optional[str] = None,\n        telegram_logger_chat_id: Optional[str] = None,\n        use_lz4: bool = False,\n        use_pyarrow: bool = False,\n        albumentations_required: Optional[bool] = None,\n        segmentation_models_required: Optional[bool] = None,\n        use_libjpeg_turbo: bool = False,\n        nmslib_required: Optional[bool] = None,\n        transformers_required: Optional[bool] = None,\n    ):\n        # [catalyst]\n        self.contrib_required: bool = contrib_required\n        self.cv_required: bool = cv_required\n        self.ml_required: bool = ml_required\n        self.nlp_required: bool = nlp_required\n\n        # stages\n        self.stage_train_prefix: str = ""train""\n        self.stage_valid_prefix: str = ""valid""\n        self.stage_infer_prefix: str = ""infer""\n\n        # loader\n        self.loader_train_prefix: str = ""train""\n        self.loader_valid_prefix: str = ""valid""\n        self.loader_infer_prefix: str = ""infer""\n\n        # [catalyst-contrib]\n        self.alchemy_logger_required: bool = self._optional_value(\n            alchemy_logger_required, default=contrib_required\n        )\n        self.neptune_logger_required: bool = self._optional_value(\n            neptune_logger_required, default=contrib_required\n        )\n        self.visdom_logger_required: bool = self._optional_value(\n            visdom_logger_required, default=contrib_required\n        )\n        self.wandb_logger_required: bool = self._optional_value(\n            wandb_logger_required, default=contrib_required\n        )\n        self.plotly_required: bool = self._optional_value(\n            plotly_required, default=contrib_required\n        )\n        self.telegram_logger_token: str = telegram_logger_token\n        self.telegram_logger_chat_id: str = telegram_logger_chat_id\n        self.use_lz4: bool = use_lz4\n        self.use_pyarrow: bool = use_pyarrow\n\n        # [catalyst-cv]\n        self.albumentations_required: bool = self._optional_value(\n            albumentations_required, default=cv_required\n        )\n        self.segmentation_models_required: bool = self._optional_value(\n            segmentation_models_required, default=cv_required\n        )\n        self.use_libjpeg_turbo: bool = use_libjpeg_turbo\n\n        # [catalyst-ml]\n        self.nmslib_required: bool = self._optional_value(\n            nmslib_required, default=ml_required\n        )\n\n        # [catalyst-nlp]\n        self.transformers_required: bool = self._optional_value(\n            transformers_required, default=nlp_required\n        )\n\n    @staticmethod\n    def _optional_value(value, default):\n        return value if value is not None else default\n\n    def type_hint(self, key: str):\n        # return get_type_hints(self).get(key, None)\n        return type(getattr(self, key, None))\n\n    @staticmethod\n    def parse() -> ""Settings"":\n        kwargrs = MergedConfigParser(ConfigFileFinder(""catalyst"")).parse()\n        return Settings(**kwargrs)\n\n\ndefault_settings = Settings()\n\n\nclass ConfigFileFinder:\n    """"""Encapsulate the logic for finding and reading config files.\n\n    Adapted from:\n\n    - https://gitlab.com/pwoolvett/flake8 (MIT License)\n    - https://github.com/python/mypy (MIT License)\n    """"""\n\n    def __init__(self, program_name: str) -> None:\n        """"""Initialize object to find config files.\n\n        Args:\n            program_name (str): Name of the current program (e.g., catalyst).\n        """"""\n        # user configuration file\n        self.program_name = program_name\n        self.user_config_file = self._user_config_file(program_name)\n\n        # list of filenames to find in the local/project directory\n        self.project_filenames = (""setup.cfg"", ""tox.ini"", f"".{program_name}"")\n\n        self.local_directory = os.path.abspath(os.curdir)\n\n    @staticmethod\n    def _user_config_file(program_name: str) -> str:\n        if os.name == ""nt"":  # if running on Windows\n            home_dir = os.path.expanduser(""~"")\n            config_file_basename = f"".{program_name}""\n        else:\n            home_dir = os.environ.get(\n                ""XDG_CONFIG_HOME"", os.path.expanduser(""~/.config"")\n            )\n            config_file_basename = program_name\n\n        return os.path.join(home_dir, config_file_basename)\n\n    @staticmethod\n    def _read_config(\n        *files: str,\n    ) -> Tuple[configparser.RawConfigParser, List[str]]:\n        config = configparser.RawConfigParser()\n\n        found_files: List[str] = []\n        for filename in files:\n            try:\n                found_files.extend(config.read(filename))\n            except UnicodeDecodeError:\n                logger.exception(\n                    f""There was an error decoding a config file.""\n                    f"" The file with a problem was {filename}.""\n                )\n            except configparser.ParsingError:\n                logger.exception(\n                    f""There was an error trying to parse a config file.""\n                    f"" The file with a problem was {filename}.""\n                )\n\n        return config, found_files\n\n    def local_config_files(self) -> List[str]:  # noqa: D202\n        """"""\n        Find all local config files which actually exist.\n\n        Returns:\n            List[str]: List of files that exist that are\n            local project config  files with extra config files\n            appended to that list (which also exist).\n        """"""\n\n        def generate_possible_local_files():\n            """"""Find and generate all local config files.""""""\n            parent = tail = os.getcwd()\n            found_config_files = False\n            while tail and not found_config_files:\n                for project_filename in self.project_filenames:\n                    filename = os.path.abspath(\n                        os.path.join(parent, project_filename)\n                    )\n                    if os.path.exists(filename):\n                        yield filename\n                        found_config_files = True\n                        self.local_directory = parent\n                (parent, tail) = os.path.split(parent)\n\n        return list(generate_possible_local_files())\n\n    def local_configs(self):\n        """"""Parse all local config files into one config object.""""""\n        config, found_files = self._read_config(*self.local_config_files())\n        if found_files:\n            logger.debug(f""Found local configuration files: {found_files}"")\n        return config\n\n    def user_config(self):\n        """"""Parse the user config file into a config object.""""""\n        config, found_files = self._read_config(self.user_config_file)\n        if found_files:\n            logger.debug(f""Found user configuration files: {found_files}"")\n        return config\n\n\nclass MergedConfigParser:\n    """"""Encapsulate merging different types of configuration files.\n\n    This parses out the options registered that were specified in the\n    configuration files, handles extra configuration files, and returns\n    dictionaries with the parsed values.\n\n    Adapted from:\n\n    - https://gitlab.com/pwoolvett/flake8 (MIT License)\n    - https://github.com/python/mypy (MIT License)\n    """"""\n\n    #: Set of actions that should use the\n    #: :meth:`~configparser.RawConfigParser.getbool` method.\n    GETBOOL_ACTIONS = {""store_true"", ""store_false""}\n\n    def __init__(self, config_finder: ConfigFileFinder):\n        """"""Initialize the MergedConfigParser instance.\n\n        Args:\n            config_finder (ConfigFileFinder): Initialized ConfigFileFinder.\n        """"""\n        self.program_name = config_finder.program_name\n        self.config_finder = config_finder\n\n    def _normalize_value(self, option, value):\n        final_value = option.normalize(\n            value, self.config_finder.local_directory\n        )\n        logger.debug(\n            f""{value} has been normalized to {final_value}""\n            f"" for option \'{option.config_name}\'"",\n        )\n        return final_value\n\n    def _parse_config(self, config_parser):\n        type2method = {\n            bool: config_parser.getboolean,\n            int: config_parser.getint,\n        }\n\n        config_dict: Dict[str, Any] = {}\n        if config_parser.has_section(self.program_name):\n            for option_name in config_parser.options(self.program_name):\n                type_ = default_settings.type_hint(option_name)\n                method = type2method.get(type_, config_parser.get)\n                config_dict[option_name] = method(\n                    self.program_name, option_name\n                )\n\n        return config_dict\n\n    def parse(self) -> dict:\n        """"""Parse and return the local and user config files.\n\n        First this copies over the parsed local configuration and then\n        iterates over the options in the user configuration and sets them if\n        they were not set by the local configuration file.\n\n        Returns:\n            dict: Dictionary of the parsed and merged configuration options.\n        """"""\n        user_config = self._parse_config(self.config_finder.user_config())\n        config = self._parse_config(self.config_finder.local_configs())\n\n        for option, value in user_config.items():\n            config.setdefault(option, value)\n\n        return config\n\n\nsettings = Settings.parse()\n\n__all__ = [""settings""]\n'"
catalyst/tools/time_manager.py,0,"b'""""""\nSimple timer.\n""""""\nfrom time import time\n\n\nclass TimeManager(object):\n    """"""@TODO: Docs. Contribution is welcome.""""""\n\n    def __init__(self):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        self._starts = {}\n        self.elapsed = {}\n\n    def start(self, name: str) -> None:\n        """"""Starts timer ``name``.\n\n        Args:\n            name (str): name of a timer\n        """"""\n        self._starts[name] = time()\n\n    def stop(self, name: str) -> None:\n        """"""Stops timer ``name``.\n\n        Args:\n            name (str): name of a timer\n        """"""\n        assert name in self._starts, f""Timer \'{name}\' wasn\'t started""\n\n        self.elapsed[name] = time() - self._starts[name]\n        del self._starts[name]\n\n    def reset(self) -> None:\n        """"""Reset all previous timers.""""""\n        self.elapsed = {}\n        self._starts = {}\n\n\n__all__ = [""TimeManager""]\n'"
catalyst/tools/typing.py,3,"b'""""""\nAll Catalyst custom types are defined in this module.\n""""""\nfrom typing import Dict, Union\n\nimport torch\nfrom torch import nn, optim\nfrom torch.optim import lr_scheduler\nfrom torch.utils import data\n\nModel = nn.Module\nCriterion = nn.Module\nOptimizer = optim.Optimizer\nScheduler = lr_scheduler._LRScheduler  # noinspection PyProtectedMember\nDataset = data.Dataset\nDevice = Union[str, torch.device]\n\nRunnerModel = Union[Model, Dict[str, Model]]\nRunnerCriterion = Union[Criterion, Dict[str, Criterion]]\nRunnerOptimizer = Union[Optimizer, Dict[str, Optimizer]]\nRunnerScheduler = Union[Scheduler, Dict[str, Scheduler]]\n\n__all__ = [\n    ""Model"",\n    ""Criterion"",\n    ""Optimizer"",\n    ""Scheduler"",\n    ""Dataset"",\n    ""Device"",\n    ""RunnerModel"",\n    ""RunnerCriterion"",\n    ""RunnerOptimizer"",\n    ""RunnerScheduler"",\n]\n'"
catalyst/utils/__init__.py,0,"b'# flake8: noqa\n# isort:skip_file\n""""""\nAll utils are gathered in :py:mod:`catalyst.utils` for easier access.\n\n.. note::\n    Everything from :py:mod:`catalyst.contrib.utils` is included in :py:mod:`catalyst.utils`\n""""""\n\nfrom catalyst.contrib.utils import *\n\nfrom .checkpoint import (\n    load_checkpoint,\n    pack_checkpoint,\n    save_checkpoint,\n    unpack_checkpoint,\n)\nfrom .components import process_components\nfrom .config import load_config, save_config\nfrom .dict import (\n    get_key_str,\n    get_key_none,\n    get_key_list,\n    get_key_dict,\n    get_key_all,\n    get_dictkey_auto_fn,\n    merge_dicts,\n    flatten_dict,\n    split_dict_to_subdicts,\n)\nfrom .distributed import (\n    get_nn_from_ddp_module,\n    get_slurm_params,\n    get_distributed_params,\n    get_distributed_env,\n    get_rank,\n    get_distributed_mean,\n    check_ddp_wrapped,\n    check_torch_distributed_initialized,\n    check_slurm_available,\n    check_apex_available,\n    initialize_apex,\n    assert_fp16_available,\n    is_wrapped_with_ddp,\n    is_torch_distributed_initialized,\n    is_slurm_available,\n    is_apex_available,\n)\nfrom .hash import get_hash, get_short_hash\nfrom .initialization import get_optimal_inner_init, outer_init\nfrom .loader import (\n    get_native_batch_from_loader,\n    get_native_batch_from_loaders,\n)\nfrom .misc import (\n    copy_directory,\n    format_metric,\n    get_fn_default_params,\n    get_fn_argsnames,\n    get_utcnow_time,\n    is_exception,\n    maybe_recursive_call,\n    fn_ends_with_pass,\n)\nfrom .numpy import get_one_hot\nfrom .parser import parse_config_args, parse_args_uargs\nfrom .pipelines import clone_pipeline\nfrom .scripts import (\n    import_module,\n    dump_code,\n    dump_python_files,\n    import_experiment_and_runner,\n    dump_base_experiment_code,\n    distributed_cmd_run,\n)\nfrom .seed import set_global_seed\nfrom .sys import (\n    get_environment_vars,\n    list_conda_packages,\n    list_pip_packages,\n    dump_environment,\n)\nfrom .torch import (\n    any2device,\n    get_activation_fn,\n    get_available_gpus,\n    get_device,\n    get_optimizable_params,\n    get_optimizer_momentum,\n    prepare_cudnn,\n    process_model_params,\n    set_optimizer_momentum,\n    get_requires_grad,\n    set_requires_grad,\n    get_network_output,\n    detach,\n    trim_tensors,\n)\n'"
catalyst/utils/checkpoint.py,2,"b'from typing import Dict, Union\nfrom collections import OrderedDict\nimport os\nfrom pathlib import Path\nimport shutil\n\nimport torch\n\nfrom .distributed import get_nn_from_ddp_module\nfrom .misc import maybe_recursive_call\n\n\ndef pack_checkpoint(\n    model=None, criterion=None, optimizer=None, scheduler=None, **kwargs\n):\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    checkpoint = kwargs\n\n    if isinstance(model, OrderedDict):\n        raise NotImplementedError()\n    else:\n        model_ = get_nn_from_ddp_module(model)\n        checkpoint[""model_state_dict""] = maybe_recursive_call(\n            model_, ""state_dict""\n        )\n\n    for dict2save, name2save in zip(\n        [criterion, optimizer, scheduler],\n        [""criterion"", ""optimizer"", ""scheduler""],\n    ):\n        if dict2save is None:\n            continue\n        # @TODO refactor with maybe_recursive_call\n        if isinstance(dict2save, dict):\n            for key, value in dict2save.items():\n                if value is not None:\n                    name2save_ = name2save + ""_"" + str(key)\n                    # checkpoint[name2save_] = value\n                    name2save_ = name2save_ + ""_state_dict""\n                    checkpoint[name2save_] = value.state_dict()\n        else:\n            # checkpoint[name2save] = dict2save\n            name2save = name2save + ""_state_dict""\n            checkpoint[name2save] = dict2save.state_dict()\n\n    return checkpoint\n\n\ndef unpack_checkpoint(\n    checkpoint, model=None, criterion=None, optimizer=None, scheduler=None\n):\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    if model is not None:\n        model = get_nn_from_ddp_module(model)\n        maybe_recursive_call(\n            model,\n            ""load_state_dict"",\n            recursive_args=checkpoint[""model_state_dict""],\n        )\n\n    for dict2load, name2load in zip(\n        [criterion, optimizer, scheduler],\n        [""criterion"", ""optimizer"", ""scheduler""],\n    ):\n        if dict2load is None:\n            continue\n\n        if isinstance(dict2load, dict):\n            for key, value in dict2load.items():\n                if value is not None:\n                    name2load_ = f""{name2load}_{key}_state_dict""\n                    value.load_state_dict(checkpoint[name2load_])\n        else:\n            name2load = f""{name2load}_state_dict""\n            dict2load.load_state_dict(checkpoint[name2load])\n\n\ndef save_checkpoint(\n    checkpoint: Dict,\n    logdir: Union[Path, str],\n    suffix: str,\n    is_best: bool = False,\n    is_last: bool = False,\n    special_suffix: str = """",\n):\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    os.makedirs(logdir, exist_ok=True)\n    filename = f""{logdir}/{suffix}.pth""\n    torch.save(checkpoint, filename)\n    if is_best:\n        shutil.copyfile(filename, f""{logdir}/best{special_suffix}.pth"")\n    if is_last:\n        shutil.copyfile(filename, f""{logdir}/last{special_suffix}.pth"")\n    return filename\n\n\ndef load_checkpoint(filepath):\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    checkpoint = torch.load(\n        filepath, map_location=lambda storage, loc: storage\n    )\n    return checkpoint\n\n\n__all__ = [\n    ""pack_checkpoint"",\n    ""unpack_checkpoint"",\n    ""save_checkpoint"",\n    ""load_checkpoint"",\n]\n'"
catalyst/utils/components.py,5,"b'from typing import Dict, Tuple\nimport copy\n\nimport torch\nfrom torch import nn\nimport torch.distributed\n\nfrom catalyst.tools.typing import (\n    Criterion,\n    Device,\n    Model,\n    Optimizer,\n    Scheduler,\n)\nfrom catalyst.utils.distributed import (\n    check_apex_available,\n    check_ddp_wrapped,\n    get_distributed_params,\n    get_rank,\n    initialize_apex,\n)\nfrom catalyst.utils.misc import maybe_recursive_call\nfrom catalyst.utils.torch import get_device\n\n\ndef process_components(\n    model: Model,\n    criterion: Criterion = None,\n    optimizer: Optimizer = None,\n    scheduler: Scheduler = None,\n    distributed_params: Dict = None,\n    device: Device = None,\n) -> Tuple[Model, Criterion, Optimizer, Scheduler, Device]:\n    """"""\n    Returns the processed model, criterion, optimizer, scheduler and device.\n\n    Args:\n        model (Model): torch model\n        criterion (Criterion): criterion function\n        optimizer (Optimizer): optimizer\n        scheduler (Scheduler): scheduler\n        distributed_params (dict, optional): dict with the parameters\n            for distributed and FP16 method\n        device (Device, optional): device\n    """"""\n    distributed_params = distributed_params or {}\n    distributed_params = copy.deepcopy(distributed_params)\n    distributed_params.update(get_distributed_params())\n\n    if device is None:\n        device = get_device()\n    elif isinstance(device, str):\n        device = torch.device(device)\n\n    is_apex_available = (\n        distributed_params.pop(""apex"", True) and check_apex_available()\n    )\n\n    model: Model = maybe_recursive_call(model, ""to"", device=device)\n\n    if check_ddp_wrapped(model):\n        pass\n    # distributed data parallel run (ddp) (with apex support)\n    elif get_rank() >= 0:\n        assert isinstance(\n            model, nn.Module\n        ), ""Distributed training is not available for KV model""\n\n        local_rank = distributed_params.pop(""local_rank"", 0) or 0\n        device = f""cuda:{local_rank}""\n        model = maybe_recursive_call(model, ""to"", device=device)\n\n        syncbn = distributed_params.pop(""syncbn"", False)\n\n        if is_apex_available:\n            import apex\n\n            model, optimizer = initialize_apex(\n                model, optimizer, **distributed_params\n            )\n            model = apex.parallel.DistributedDataParallel(model)\n\n            if syncbn:\n                model = apex.parallel.convert_syncbn_model(model)\n        else:\n            model = nn.parallel.DistributedDataParallel(\n                model, device_ids=[local_rank], output_device=local_rank\n            )\n    # data parallel run (dp) (with apex support)\n    else:\n        # apex issue https://github.com/deepset-ai/FARM/issues/210\n        use_apex = (is_apex_available and torch.cuda.device_count() == 1) or (\n            is_apex_available\n            and torch.cuda.device_count() > 1\n            and distributed_params.get(""opt_level"", ""O0"") == ""O1""\n        )\n\n        if use_apex:\n            assert isinstance(\n                model, nn.Module\n            ), ""Apex training is not available for KV model""\n\n            model, optimizer = initialize_apex(\n                model, optimizer, **distributed_params\n            )\n\n        if (\n            torch.cuda.device_count() > 1\n            and device.type != ""cpu""\n            and device.index is None\n        ):\n            if isinstance(model, nn.Module):\n                model = nn.DataParallel(model)\n            elif isinstance(model, dict):\n                model = {k: nn.DataParallel(v) for k, v in model.items()}\n            else:\n                raise NotImplementedError()\n\n    model: Model = maybe_recursive_call(model, ""to"", device=device)\n\n    return model, criterion, optimizer, scheduler, device\n\n\n__all__ = [""process_components""]\n'"
catalyst/utils/config.py,0,"b'from typing import Dict, List, Union\nfrom collections import OrderedDict\nimport json\nfrom logging import getLogger\nfrom pathlib import Path\nimport re\n\nimport yaml\n\nLOG = getLogger(__name__)\n\n\nclass OrderedLoader(yaml.Loader):\n    pass\n\n\ndef construct_mapping(loader, node):\n    loader.flatten_mapping(node)\n    return OrderedDict(loader.construct_pairs(node))\n\n\nOrderedLoader.add_constructor(\n    yaml.resolver.BaseResolver.DEFAULT_MAPPING_TAG, construct_mapping\n)\nOrderedLoader.add_implicit_resolver(\n    ""tag:yaml.org,2002:float"",\n    re.compile(\n        """"""^(?:\n        [-+]?(?:[0-9][0-9_]*)\\\\.[0-9_]*(?:[eE][-+]?[0-9]+)?\n        |[-+]?(?:[0-9][0-9_]*)(?:[eE][-+]?[0-9]+)\n        |\\\\.[0-9_]+(?:[eE][-+][0-9]+)?\n        |[-+]?[0-9][0-9_]*(?::[0-5]?[0-9])+\\\\.[0-9_]*\n        |[-+]?\\\\.(?:inf|Inf|INF)\n        |\\\\.(?:nan|NaN|NAN))$"""""",\n        re.X,\n    ),\n    list(""-+0123456789.""),\n)\n\n\ndef _load_ordered_yaml(stream,):\n    return yaml.load(stream, OrderedLoader)\n\n\ndef load_config(\n    path: Union[str, Path],\n    ordered: bool = False,\n    data_format: str = None,\n    encoding: str = ""utf-8"",\n) -> Union[Dict, List]:\n    """"""\n    Loads config by giving path. Supports YAML and JSON files.\n\n    Examples:\n        >>> load(path=""./config.yml"", ordered=True)\n\n    Args:\n        path (str): path to config file (YAML or JSON)\n        ordered (bool): if true the config will be loaded as ``OrderedDict``\n        data_format (str): ``yaml``, ``yml`` or ``json``.\n        encoding (str): encoding to read the config\n\n    Returns:\n        (Union[Dict, List]): config\n\n    Raises:\n        Exception: if path ``path`` doesn\'t exists\n            or file format is not YAML or JSON\n\n    Adapted from\n    https://github.com/TezRomacH/safitty/blob/v1.2.0/safitty/parser.py#L63\n    which was adapted from\n    https://github.com/catalyst-team/catalyst/blob/v19.03/catalyst/utils/config.py#L10\n    """"""\n    path = Path(path)\n\n    if not path.exists():\n        raise Exception(f""Path \'{path}\' doesn\'t exist!"")\n\n    if data_format is not None:\n        suffix = data_format.lower()\n        if not suffix.startswith("".""):\n            suffix = f"".{suffix}""\n    else:\n        suffix = path.suffix\n\n    assert suffix in [\n        "".json"",\n        "".yml"",\n        "".yaml"",\n    ], f""Unknown file format \'{suffix}\'""\n\n    config = None\n    with path.open(encoding=encoding) as stream:\n        if suffix == "".json"":\n            object_pairs_hook = OrderedDict if ordered else None\n            file = ""\\n"".join(stream.readlines())\n            if file != """":\n                config = json.loads(file, object_pairs_hook=object_pairs_hook)\n\n        elif suffix in ["".yml"", "".yaml""]:\n            loader = OrderedLoader if ordered else yaml.Loader\n            config = yaml.load(stream, loader)\n\n    if config is None:\n        return {}\n\n    return config\n\n\ndef save_config(\n    config: Union[Dict, List],\n    path: Union[str, Path],\n    data_format: str = None,\n    encoding: str = ""utf-8"",\n    ensure_ascii: bool = False,\n    indent: int = 2,\n) -> None:\n    """"""\n    Saves config to file. Path must be either YAML or JSON.\n\n    Args:\n        config (Union[Dict, List]): config to save\n        path (Union[str, Path]): path to save\n        data_format (str): ``yaml``, ``yml`` or ``json``.\n        encoding (str): Encoding to write file. Default is ``utf-8``\n        ensure_ascii (bool): Used for JSON, if True non-ASCII\n        characters are escaped in JSON strings.\n        indent (int): Used for JSON\n\n    Adapted from\n    https://github.com/TezRomacH/safitty/blob/v1.2.0/safitty/parser.py#L110\n    which was adapted from\n    https://github.com/catalyst-team/catalyst/blob/v19.03/catalyst/utils/config.py#L38\n    """"""\n    path = Path(path)\n\n    if data_format is not None:\n        suffix = data_format\n    else:\n        suffix = path.suffix\n\n    assert suffix in [\n        "".json"",\n        "".yml"",\n        "".yaml"",\n    ], f""Unknown file format \'{suffix}\'""\n\n    with path.open(encoding=encoding, mode=""w"") as stream:\n        if suffix == "".json"":\n            json.dump(config, stream, indent=indent, ensure_ascii=ensure_ascii)\n        elif suffix in ["".yml"", "".yaml""]:\n            yaml.dump(config, stream)\n\n\n__all__ = [\n    ""load_config"",\n    ""save_config"",\n]\n'"
catalyst/utils/dict.py,0,"b'from typing import Any, Callable, Dict, List, Optional, Union\nimport collections\nimport copy\n\n\ndef get_key_str(\n    dictionary: dict, key: Optional[Union[str, List[str]]],\n) -> Any:\n    """"""Returns value from dict by key.\n\n    Args:\n        dictionary: dict\n        key: key\n\n    Returns:\n        value\n    """"""\n    return dictionary[key]\n\n\ndef get_key_list(\n    dictionary: dict, key: Optional[Union[str, List[str]]],\n) -> Dict:\n    """"""Returns sub-dict from dict by list of keys.\n\n    Args:\n        dictionary: dict\n        key: list of keys\n\n    Returns:\n        sub-dict\n    """"""\n    result = {key_: dictionary[key_] for key_ in key}\n    return result\n\n\ndef get_key_dict(\n    dictionary: dict, key: Optional[Union[str, List[str]]],\n) -> Dict:\n    """"""Returns sub-dict from dict by dict-mapping of keys.\n\n    Args:\n        dictionary: dict\n        key: dict-mapping of keys\n\n    Returns:\n        sub-dict\n    """"""\n    result = {key_out: dictionary[key_in] for key_in, key_out in key.items()}\n    return result\n\n\ndef get_key_none(\n    dictionary: dict, key: Optional[Union[str, List[str]]],\n) -> Dict:\n    """"""Returns empty dict.\n\n    Args:\n        dictionary: dict\n        key: none\n\n    Returns:\n        dict\n    """"""\n    return {}\n\n\ndef get_key_all(\n    dictionary: dict, key: Optional[Union[str, List[str]]],\n) -> Dict:\n    """"""Returns whole dict.\n\n    Args:\n        dictionary: dict\n        key: none\n\n    Returns:\n        dict\n    """"""\n    return dictionary\n\n\ndef get_dictkey_auto_fn(key: Optional[Union[str, List[str]]]) -> Callable:\n    """"""Function generator for sub-dict preparation from dict\n    based on predefined keys.\n\n    Args:\n        key: keys\n\n    Returns:\n        function\n    """"""\n    if isinstance(key, str):\n        if key == ""__all__"":\n            return get_key_all\n        else:\n            return get_key_str\n    elif isinstance(key, (list, tuple)):\n        return get_key_list\n    elif isinstance(key, dict):\n        return get_key_dict\n    elif key is None:\n        return get_key_none\n    else:\n        raise NotImplementedError()\n\n\ndef merge_dicts(*dicts: dict) -> dict:\n    """"""Recursive dict merge.\n    Instead of updating only top-level keys,\n    ``merge_dicts`` recurses down into dicts nested\n    to an arbitrary depth, updating keys.\n\n    Args:\n        *dicts: several dictionaries to merge\n\n    Returns:\n        dict: deep-merged dictionary\n    """"""\n    assert len(dicts) > 1\n\n    dict_ = copy.deepcopy(dicts[0])\n\n    for merge_dict in dicts[1:]:\n        merge_dict = merge_dict or {}\n        for k in merge_dict:\n            if (\n                k in dict_\n                and isinstance(dict_[k], dict)\n                and isinstance(merge_dict[k], collections.Mapping)\n            ):\n                dict_[k] = merge_dicts(dict_[k], merge_dict[k])\n            else:\n                dict_[k] = merge_dict[k]\n\n    return dict_\n\n\ndef flatten_dict(\n    dictionary: Dict[str, Any], parent_key: str = """", separator: str = ""/""\n) -> ""collections.OrderedDict"":\n    """"""Make the given dictionary flatten.\n\n    Args:\n        dictionary (dict): giving dictionary\n        parent_key (str, optional): prefix nested keys with\n            string ``parent_key``\n        separator (str, optional): delimiter between\n            ``parent_key`` and ``key`` to use\n\n    Returns:\n        collections.OrderedDict: ordered dictionary with flatten keys\n    """"""\n    items = []\n    for key, value in dictionary.items():\n        new_key = parent_key + separator + key if parent_key else key\n        if isinstance(value, collections.MutableMapping):\n            items.extend(\n                flatten_dict(value, new_key, separator=separator).items()\n            )\n        else:\n            items.append((new_key, value))\n    return collections.OrderedDict(items)\n\n\ndef split_dict_to_subdicts(dct: Dict, prefixes: List, extra_key: str):\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    subdicts = {}\n    extra_subdict = {\n        k: v\n        for k, v in dct.items()\n        if all(not k.startswith(prefix) for prefix in prefixes)\n    }\n    if len(extra_subdict) > 0:\n        subdicts[extra_key] = extra_subdict\n    for prefix in prefixes:\n        subdicts[prefix] = {\n            k.replace(f""{prefix}_"", """"): v\n            for k, v in dct.items()\n            if k.startswith(prefix)\n        }\n    return subdicts\n\n\n__all__ = [\n    ""get_dictkey_auto_fn"",\n    ""get_key_all"",\n    ""get_key_dict"",\n    ""get_key_list"",\n    ""get_key_none"",\n    ""get_key_str"",\n    ""merge_dicts"",\n    ""flatten_dict"",\n    ""split_dict_to_subdicts"",\n]\n'"
catalyst/utils/distributed.py,16,"b'from typing import Union\nfrom collections import OrderedDict\nimport os\nimport random\nimport socket\nimport subprocess\nimport warnings\n\nimport deprecation\n\nimport torch\nfrom torch import nn\nimport torch.distributed\n\nfrom catalyst import __version__\n\nfrom .misc import get_fn_default_params\nfrom .torch import get_available_gpus\n\nwarnings.simplefilter(""once"")\nwarnings.filterwarnings(""once"")\n\n\ndef check_ddp_wrapped(model: nn.Module) -> bool:\n    """"""\n    Checks whether model is wrapped with DataParallel/DistributedDataParallel.\n    """"""\n    parallel_wrappers = nn.DataParallel, nn.parallel.DistributedDataParallel\n\n    # Check whether Apex is installed and if it is,\n    # add Apex\'s DistributedDataParallel to list of checked types\n    try:\n        from apex.parallel import DistributedDataParallel as apex_DDP\n\n        parallel_wrappers = parallel_wrappers + (apex_DDP,)\n    except ImportError:\n        pass\n\n    return isinstance(model, parallel_wrappers)\n\n\ndef check_apex_available() -> bool:\n    """"""Checks if apex is available.""""""\n    env_apex = os.getenv(""USE_APEX"", ""1"") == ""1""\n    try:\n        import apex  # noqa: F401\n        from apex import amp  # noqa: F401\n\n        return True and env_apex\n    except ImportError:\n        return False and env_apex\n\n\ndef check_torch_distributed_initialized() -> bool:\n    """"""Checks if torch.distributed is available and initialized.""""""\n    return (\n        torch.distributed.is_available() and torch.distributed.is_initialized()\n    )\n\n\ndef check_slurm_available():\n    """"""Checks if slurm is available.""""""\n    return ""SLURM_JOB_NUM_NODES"" in os.environ and ""SLURM_NODEID"" in os.environ\n\n\ndef assert_fp16_available() -> None:\n    """"""Asserts for installed and available Apex FP16.""""""\n    assert (\n        torch.backends.cudnn.enabled\n    ), ""fp16 mode requires cudnn backend to be enabled.""\n\n    assert check_apex_available(), (\n        ""NVidia Apex package must be installed. ""\n        ""See https://github.com/NVIDIA/apex.""\n    )\n\n\ndef initialize_apex(model, optimizer=None, **distributed_params):\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    import apex\n\n    amp_params = get_fn_default_params(\n        apex.amp.initialize, [""models"", ""optimizers""]\n    )\n    amp_params[""opt_level""] = ""O0""\n    for dp in distributed_params:\n        if dp in amp_params:\n            amp_params[dp] = distributed_params[dp]\n\n    amp_result = apex.amp.initialize(model, optimizer, **amp_params)\n    if optimizer is not None:\n        model, optimizer = amp_result\n    else:\n        model = amp_result\n    return model, optimizer\n\n\ndef get_nn_from_ddp_module(model: nn.Module) -> nn.Module:\n    """"""\n    Return a real model from a torch.nn.DataParallel,\n    torch.nn.parallel.DistributedDataParallel, or\n    apex.parallel.DistributedDataParallel.\n\n    Args:\n        model: A model, or DataParallel wrapper.\n\n    Returns:\n        A model\n    """"""\n    if check_ddp_wrapped(model):\n        model = model.module\n    return model\n\n\ndef get_rank() -> int:\n    """"""\n    Returns the rank of the current worker.\n\n    Returns:\n        (int): ``rank`` if torch.distributed is initialized,\n        otherwise ``-1``\n    """"""\n    if check_torch_distributed_initialized():\n        return torch.distributed.get_rank()\n    else:\n        return -1\n\n\ndef get_distributed_mean(value: Union[float, torch.Tensor]):\n    """"""Computes distributed mean among all nodes.""""""\n    if check_torch_distributed_initialized():\n        value = torch.tensor(\n            value,\n            dtype=torch.float,\n            device=f""cuda:{torch.cuda.current_device()}"",\n            requires_grad=False,\n        )\n        torch.distributed.all_reduce(value)\n        value = float(value.item() / torch.distributed.get_world_size())\n    return value\n\n\ndef get_slurm_params():\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    cmd = ""scontrol show hostnames \'%s\'"" % os.environ[""SLURM_JOB_NODELIST""]\n    nodes = subprocess.getoutput(cmd).split()\n    num_nodes = int(os.environ[""SLURM_JOB_NUM_NODES""])\n    current_node = os.environ[""SLURMD_NODENAME""]\n    master_node = socket.gethostbyname(nodes[0])\n    cur_node_idx = nodes.index(current_node)\n    job_id = os.environ[""SLURM_JOB_ID""]\n    master_port = str(5 * 10 ** 4 + int(job_id) % 10 ** 4)\n    return cur_node_idx, num_nodes, master_node, master_port\n\n\ndef get_distributed_params():\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    master_port = str(random.randint(5 * 10 ** 4, 6 * 10 ** 4))\n    master_addr = ""127.0.0.1""\n    cur_node, num_nodes = 0, 1\n    if check_slurm_available():\n        cur_node, num_nodes, master_addr, master_port = get_slurm_params()\n\n    os.environ[""MASTER_ADDR""] = os.getenv(""MASTER_ADDR"", master_addr)\n    os.environ[""MASTER_PORT""] = os.getenv(""MASTER_PORT"", master_port)\n\n    workers_per_node = torch.cuda.device_count()\n    start_rank = cur_node * workers_per_node\n    world_size = num_nodes * workers_per_node\n\n    local_rank = os.getenv(""LOCAL_RANK"", None)\n    rank = os.getenv(""RANK"", None)\n    local_rank, rank = [v and int(v) for v in [local_rank, rank]]\n    world_size = int(os.getenv(""WORLD_SIZE"", world_size))\n\n    output = OrderedDict(\n        local_rank=local_rank,\n        start_rank=start_rank,\n        rank=rank,\n        world_size=world_size,\n        master_addr=os.environ[""MASTER_ADDR""],\n        master_port=os.environ[""MASTER_PORT""],\n    )\n\n    return output\n\n\ndef get_distributed_env(\n    local_rank, rank, world_size, use_cuda_visible_devices=True\n):\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    env = os.environ.copy()\n    env[""RANK""] = str(rank)\n    env[""WORLD_SIZE""] = str(world_size)\n    env[""LOCAL_RANK""] = str(local_rank)\n    if use_cuda_visible_devices:\n        available_gpus = get_available_gpus()\n        env[""LOCAL_RANK""] = ""0""\n        env[""CUDA_VISIBLE_DEVICES""] = str(available_gpus[local_rank])\n    return env\n\n\n@deprecation.deprecated(\n    deprecated_in=""20.05"",\n    removed_in=""20.06"",\n    current_version=__version__,\n    details=""Use check_ddp_wrapped instead."",\n)\ndef is_wrapped_with_ddp(model: nn.Module) -> bool:\n    """"""\n    Checks whether model is wrapped with DataParallel/DistributedDataParallel.\n    """"""\n    return check_ddp_wrapped(model)\n\n\n@deprecation.deprecated(\n    deprecated_in=""20.05"",\n    removed_in=""20.06"",\n    current_version=__version__,\n    details=""Use check_torch_distributed_initialized instead."",\n)\ndef is_torch_distributed_initialized() -> bool:\n    """"""Checks if torch.distributed is available and initialized.""""""\n    return check_torch_distributed_initialized()\n\n\n@deprecation.deprecated(\n    deprecated_in=""20.05"",\n    removed_in=""20.06"",\n    current_version=__version__,\n    details=""Use check_slurm_available instead."",\n)\ndef is_slurm_available() -> bool:\n    """"""Checks if slurm is available.""""""\n    return check_slurm_available()\n\n\n@deprecation.deprecated(\n    deprecated_in=""20.05"",\n    removed_in=""20.06"",\n    current_version=__version__,\n    details=""Use check_apex_available instead."",\n)\ndef is_apex_available() -> bool:\n    """"""Checks if apex is available.""""""\n    return check_apex_available()\n\n\n__all__ = [\n    ""check_ddp_wrapped"",\n    ""check_apex_available"",\n    ""check_torch_distributed_initialized"",\n    ""check_slurm_available"",\n    ""assert_fp16_available"",\n    ""initialize_apex"",\n    ""get_nn_from_ddp_module"",\n    ""get_rank"",\n    ""get_distributed_mean"",\n    ""get_distributed_env"",\n    ""get_distributed_params"",\n    ""get_slurm_params"",\n    ""is_wrapped_with_ddp"",\n    ""is_torch_distributed_initialized"",\n    ""is_slurm_available"",\n    ""is_apex_available"",\n]\n'"
catalyst/utils/hash.py,0,"b'from typing import Any\nfrom base64 import urlsafe_b64encode\nfrom hashlib import sha256\n\n\ndef _make_hashable(o):\n    if isinstance(o, (tuple, list)):\n        return tuple(((type(o).__name__, _make_hashable(e)) for e in o))\n    if isinstance(o, dict):\n        return tuple(\n            sorted(\n                (type(o).__name__, k, _make_hashable(v)) for k, v in o.items()\n            )\n        )\n    if isinstance(o, (set, frozenset)):\n        return tuple(sorted((type(o).__name__, _make_hashable(e)) for e in o))\n    return o\n\n\ndef get_hash(obj: Any) -> str:\n    """"""\n    Creates unique hash from object following way:\n    - Represent obj as sting recursively\n    - Hash this string with sha256 hash function\n    - encode hash with url-safe base64 encoding\n\n    Args:\n        obj: object to hash\n\n    Returns:\n        base64-encoded string\n    """"""\n    bytes_to_hash = repr(_make_hashable(obj)).encode()\n    hash_bytes = sha256(bytes_to_hash).digest()\n    return urlsafe_b64encode(hash_bytes).decode()\n\n\ndef get_short_hash(o) -> str:\n    """"""\n    @TODO: Docs. Contribution is welcome.\n    """"""\n    hash = get_hash(o)[:6]\n    return hash\n\n\n__all__ = [""get_hash"", ""get_short_hash""]\n'"
catalyst/utils/initialization.py,0,"b'from typing import Callable\n\nfrom torch import nn\n\nACTIVATIONS = {\n    None: ""sigmoid"",\n    nn.Sigmoid: ""sigmoid"",\n    nn.Tanh: ""tanh"",\n    nn.ReLU: ""relu"",\n    nn.LeakyReLU: ""leaky_relu"",\n    nn.ELU: ""relu"",\n}\n\n\ndef _nonlinearity2name(nonlinearity):\n    if isinstance(nonlinearity, nn.Module):\n        nonlinearity = nonlinearity.__class__\n    nonlinearity = ACTIVATIONS.get(nonlinearity, nonlinearity)\n    nonlinearity = nonlinearity.lower()\n    return nonlinearity\n\n\ndef get_optimal_inner_init(\n    nonlinearity: nn.Module, **kwargs\n) -> Callable[[nn.Module], None]:\n    """"""\n    Create initializer for inner layers\n    based on their activation function (nonlinearity).\n\n    Args:\n        nonlinearity: non-linear activation\n    """"""\n    nonlinearity: str = _nonlinearity2name(nonlinearity)\n    assert isinstance(nonlinearity, str)\n\n    if nonlinearity in [""sigmoid"", ""tanh""]:\n        weignt_init_fn = nn.init.xavier_uniform_\n        init_args = kwargs\n    elif nonlinearity in [""relu"", ""leaky_relu""]:\n        weignt_init_fn = nn.init.kaiming_normal_\n        init_args = {**{""nonlinearity"": nonlinearity}, **kwargs}\n    else:\n        raise NotImplementedError\n\n    def inner_init(layer):\n        if isinstance(layer, (nn.Linear, nn.Conv1d, nn.Conv2d)):\n            weignt_init_fn(layer.weight.data, **init_args)\n            if layer.bias is not None:\n                nn.init.zeros_(layer.bias.data)\n\n    return inner_init\n\n\ndef outer_init(layer: nn.Module) -> None:\n    """"""\n    Initialization for output layers of policy and value networks typically\n    used in deep reinforcement learning literature.\n    """"""\n    if isinstance(layer, (nn.Linear, nn.Conv1d, nn.Conv2d)):\n        v = 3e-3\n        nn.init.uniform_(layer.weight.data, -v, v)\n        if layer.bias is not None:\n            nn.init.uniform_(layer.bias.data, -v, v)\n\n\n__all__ = [""get_optimal_inner_init"", ""outer_init""]\n'"
catalyst/utils/loader.py,1,"b'from typing import Dict, Union  # isort:skip\n\nfrom torch.utils.data import DataLoader\n\n\ndef get_native_batch_from_loader(loader: DataLoader, batch_index: int = 0):\n    """"""\n    Returns a batch from experiment loader\n\n    Args:\n        loader (DataLoader): Loader to get batch from\n        batch_index (int): Index of batch to take from dataset of the loader\n    """"""\n    dataset = loader.dataset\n    collate_fn = loader.collate_fn\n    return collate_fn([dataset[batch_index]])\n\n\ndef get_native_batch_from_loaders(\n    loaders: Dict[str, DataLoader],\n    loader: Union[str, int] = 0,\n    batch_index: int = 0,\n):\n    """"""\n    Returns a batch from experiment loaders by its index or name.\n\n    Args:\n        loaders (Dict[str, DataLoader]): Loaders list to get loader from\n        loader (Union[str, int]): Loader name or its index, default is zero\n        batch_index (int): Index of batch to take from dataset of the loader\n    """"""\n    if isinstance(loader, str):\n        _loader = loaders[loader]\n    elif isinstance(loader, int):\n        _loader = list(loaders.values())[loader]\n    else:\n        raise TypeError(""Loader parameter must be a string or an integer"")\n\n    return get_native_batch_from_loader(\n        loader=_loader, batch_index=batch_index\n    )\n\n\n__all__ = [\n    ""get_native_batch_from_loader"",\n    ""get_native_batch_from_loaders"",\n]\n'"
catalyst/utils/misc.py,0,"b'from typing import Any, Callable, List\nfrom datetime import datetime\nimport inspect\nfrom pathlib import Path\nimport shutil\n\n\ndef maybe_recursive_call(\n    object_or_dict,\n    method: str,\n    recursive_args=None,\n    recursive_kwargs=None,\n    **kwargs,\n):\n    """"""Calls the ``method`` recursively for the ``object_or_dict``.\n\n    Args:\n        object_or_dict (Any): some object or a dictionary of objects\n        method (str): method name to call\n        recursive_args: list of arguments to pass to the ``method``\n        recursive_kwargs: list of key-arguments to pass to the ``method``\n        **kwargs: Arbitrary keyword arguments\n    """"""\n    if isinstance(object_or_dict, dict):\n        result = type(object_or_dict)()\n        for k, v in object_or_dict.items():\n            r_args = None if recursive_args is None else recursive_args[k]\n            r_kwargs = (\n                None if recursive_kwargs is None else recursive_kwargs[k]\n            )\n            result[k] = maybe_recursive_call(\n                v,\n                method,\n                recursive_args=r_args,\n                recursive_kwargs=r_kwargs,\n                **kwargs,\n            )\n        return result\n\n    r_args = recursive_args or []\n    if not isinstance(r_args, (list, tuple)):\n        r_args = [r_args]\n    r_kwargs = recursive_kwargs or {}\n    return getattr(object_or_dict, method)(*r_args, **r_kwargs, **kwargs)\n\n\ndef is_exception(ex: Any) -> bool:\n    """"""Check if the argument is of ``Exception`` type.""""""\n    result = (ex is not None) and isinstance(ex, BaseException)\n    return result\n\n\ndef copy_directory(input_dir: Path, output_dir: Path) -> None:\n    """"""Recursively copies the input directory.\n\n    Args:\n        input_dir (Path): input directory\n        output_dir (Path): output directory\n    """"""\n    output_dir.mkdir(exist_ok=True, parents=True)\n    for path in input_dir.iterdir():\n        if path.is_dir():\n            path_name = path.name\n            copy_directory(path, output_dir / path_name)\n        else:\n            shutil.copy2(path, output_dir)\n\n\ndef get_utcnow_time(format: str = None) -> str:\n    """"""Return string with current utc time in chosen format.\n\n    Args:\n        format (str): format string. if None ""%y%m%d.%H%M%S"" will be used.\n\n    Returns:\n        str: formatted utc time string\n    """"""\n    if format is None:\n        format = ""%y%m%d.%H%M%S""\n    result = datetime.utcnow().strftime(format)\n    return result\n\n\ndef format_metric(name: str, value: float) -> str:\n    """"""Format metric.\n\n    Metric will be returned in the scientific format if 4\n    decimal chars are not enough (metric value lower than 1e-4).\n\n    Args:\n        name (str): metric name\n        value (float): value of metric\n    """"""\n    if value < 1e-4:\n        return f""{name}={value:1.3e}""\n    return f""{name}={value:.4f}""\n\n\ndef get_fn_default_params(fn: Callable[..., Any], exclude: List[str] = None):\n    """"""Return default parameters of Callable.\n\n    Args:\n        fn (Callable[..., Any]): target Callable\n        exclude (List[str]): exclude list of parameters\n\n    Returns:\n        dict: contains default parameters of `fn`\n    """"""\n    argspec = inspect.getfullargspec(fn)\n    default_params = zip(\n        argspec.args[-len(argspec.defaults) :], argspec.defaults\n    )\n    if exclude is not None:\n        default_params = filter(lambda x: x[0] not in exclude, default_params)\n    default_params = dict(default_params)\n    return default_params\n\n\ndef get_fn_argsnames(fn: Callable[..., Any], exclude: List[str] = None):\n    """"""Return parameter names of Callable.\n\n    Args:\n        fn (Callable[..., Any]): target Callable\n        exclude (List[str]): exclude list of parameters\n\n    Returns:\n        list: contains parameter names of `fn`\n    """"""\n    argspec = inspect.getfullargspec(fn)\n    params = argspec.args + argspec.kwonlyargs\n    if exclude is not None:\n        params = list(filter(lambda x: x not in exclude, params))\n    return params\n\n\ndef fn_ends_with_pass(fn: Callable[..., Any]):\n    """"""\n    Check that function end with pass statement\n    (probably does nothing in any way).\n    Mainly used to filter callbacks with empty on_{event} methods.\n\n    Args:\n        fn (Callable[..., Any]): target Callable\n\n    Returns:\n        bool: True if there is pass in the first indentation level of fn\n        and nothing happens before it, False in any other case.\n    """"""\n    source_lines = inspect.getsourcelines(fn)[0]\n    if source_lines[-1].strip() == ""pass"":\n        return True\n    return False\n\n\n__all__ = [\n    ""copy_directory"",\n    ""format_metric"",\n    ""get_fn_default_params"",\n    ""get_fn_argsnames"",\n    ""get_utcnow_time"",\n    ""is_exception"",\n    ""maybe_recursive_call"",\n    ""fn_ends_with_pass"",\n]\n'"
catalyst/utils/numpy.py,0,"b'import numpy as np\n\n\ndef get_one_hot(\n    label: int, num_classes: int, smoothing: float = None\n) -> np.ndarray:\n    """"""\n    Applies OneHot vectorization to a giving scalar, optional with\n    label smoothing as described in `Bag of Tricks for Image Classification\n    with Convolutional Neural Networks`_.\n\n    Args:\n        label (int): scalar value to be vectorized\n        num_classes (int): total number of classes\n        smoothing (float, optional): if specified applies label smoothing\n            from ``Bag of Tricks for Image Classification\n            with Convolutional Neural Networks`` paper\n\n    Returns:\n        np.ndarray: a one-hot vector with shape ``(num_classes,)``\n\n    .. _Bag of Tricks for Image Classification with\n        Convolutional Neural Networks: https://arxiv.org/abs/1812.01187\n    """"""\n    assert (\n        num_classes is not None and num_classes > 0\n    ), f""Expect num_classes to be > 0, got {num_classes}""\n\n    assert (\n        label is not None and 0 <= label < num_classes\n    ), f""Expect label to be in [0; {num_classes}), got {label}""\n\n    if smoothing is not None:\n        assert (\n            0.0 < smoothing < 1.0\n        ), f""If smoothing is specified it must be in (0; 1), got {smoothing}""\n\n        smoothed = smoothing / float(num_classes - 1)\n        result = np.full((num_classes,), smoothed, dtype=np.float32)\n        result[label] = 1.0 - smoothing\n\n        return result\n\n    result = np.zeros(num_classes, dtype=np.float32)\n    result[label] = 1.0\n\n    return result\n\n\n__all__ = [""get_one_hot""]\n'"
catalyst/utils/parser.py,0,"b'import copy\nfrom pathlib import Path\n\nfrom .config import load_config\nfrom .dict import merge_dicts\n\n\ndef parse_config_args(*, config, args, unknown_args):\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    for arg in unknown_args:\n        arg_name, value = arg.split(""="")\n        arg_name = arg_name.lstrip(""-"").strip(""/"")\n\n        value_content, value_type = value.rsplit("":"", 1)\n\n        if ""/"" in arg_name:\n            arg_names = arg_name.split(""/"")\n            if value_type == ""str"":\n                arg_value = value_content\n\n                if arg_value.lower() == ""none"":\n                    arg_value = None\n            else:\n                arg_value = eval(""%s(%s)"" % (value_type, value_content))\n\n            config_ = config\n            for arg_name in arg_names[:-1]:\n                if arg_name not in config_:\n                    config_[arg_name] = {}\n\n                config_ = config_[arg_name]\n\n            config_[arg_names[-1]] = arg_value\n        else:\n            if value_type == ""str"":\n                arg_value = value_content\n            else:\n                arg_value = eval(""%s(%s)"" % (value_type, value_content))\n            args.__setattr__(arg_name, arg_value)\n\n    args_exists_ = config.get(""args"", None)\n    if args_exists_ is None:\n        config[""args""] = {}\n\n    for key, value in args._get_kwargs():\n        if value is not None:\n            if key in [""logdir"", ""baselogdir""] and value == """":\n                continue\n            config[""args""][key] = value\n\n    autoresume = config[""args""].get(""autoresume"", None)\n    logdir = config[""args""].get(""logdir"", None)\n    resume = config[""args""].get(""resume"", None)\n    if autoresume is not None and logdir is not None and resume is None:\n        logdir = Path(logdir)\n        checkpoint_filename = logdir / ""checkpoints"" / f""{autoresume}_full.pth""\n        if checkpoint_filename.is_file():\n            config[""args""][""resume""] = str(checkpoint_filename)\n    return config, args\n\n\ndef parse_args_uargs(args, unknown_args):\n    """"""Function for parsing configuration files.\n\n    Args:\n        args: recognized arguments\n        unknown_args: unrecognized arguments\n\n    Returns:\n        tuple: updated arguments, dict with config\n    """"""\n    args_ = copy.deepcopy(args)\n\n    # load params\n    config = {}\n    for config_path in args_.configs:\n        config_ = load_config(config_path, ordered=True)\n        config = merge_dicts(config, config_)\n\n    config, args_ = parse_config_args(\n        config=config, args=args_, unknown_args=unknown_args\n    )\n\n    # hack with argparse in config\n    config_args = config.get(""args"", None)\n    if config_args is not None:\n        for key, value in config_args.items():\n            arg_value = getattr(args_, key, None)\n            if arg_value is None or (\n                key in [""logdir"", ""baselogdir""] and arg_value == """"\n            ):\n                arg_value = value\n            setattr(args_, key, arg_value)\n\n    return args_, config\n\n\n__all__ = [""parse_config_args"", ""parse_args_uargs""]\n'"
catalyst/utils/pipelines.py,0,"b'from pathlib import Path\nimport shutil\n\nfrom git import Repo as repo\n\nfrom .misc import copy_directory\n\nURLS = {\n    ""classification"": ""https://github.com/catalyst-team/classification/"",\n    ""segmentation"": ""https://github.com/catalyst-team/segmentation/"",\n    ""detection"": ""https://github.com/catalyst-team/detection/"",\n}\n\nCATALYST_ROOT = Path(__file__).resolve().parents[3]\nPATH_TO_TEMPLATE = CATALYST_ROOT / ""examples"" / ""_empty""\n\n\ndef clone_pipeline(template: str, out_dir: Path,) -> None:\n    """"""Clones pipeline from empty pipeline template or from demo pipelines\n    available in Git repos of Catalyst Team.\n\n    Args:\n        template (str): type of pipeline you want to clone.\n            empty/classification/segmentation\n        out_dir (pathlib.Path): path where pipeline directory should be cloned\n    """"""\n    if template == ""empty"" or template is None:\n        copy_directory(PATH_TO_TEMPLATE, out_dir)\n    else:\n        url = URLS[template]\n        repo.clone_from(url, out_dir / ""__git_temp"")\n        shutil.rmtree(out_dir / ""__git_temp"" / "".git"")\n        if (out_dir / ""__git_temp"" / "".gitignore"").exists():\n            (out_dir / ""__git_temp"" / "".gitignore"").unlink()\n\n        copy_directory(out_dir / ""__git_temp"", out_dir)\n        shutil.rmtree(out_dir / ""__git_temp"")\n\n\n__all__ = [""clone_pipeline""]\n'"
catalyst/utils/scripts.py,6,"b'from typing import Callable\nfrom importlib.util import module_from_spec, spec_from_file_location\nimport os\nimport pathlib\nimport shutil\nimport subprocess\nimport sys\nimport warnings\n\nimport torch\nimport torch.distributed\n\nfrom .distributed import get_distributed_env, get_distributed_params\nfrom .misc import get_utcnow_time\n\nwarnings.simplefilter(""once"")\nwarnings.filterwarnings(""once"")\n\n\ndef import_module(expdir: pathlib.Path):\n    """"""\n    @TODO: Docs. Contribution is welcome\n    """"""\n    # @TODO: better PYTHONPATH handling\n    if not isinstance(expdir, pathlib.Path):\n        expdir = pathlib.Path(expdir)\n    sys.path.insert(0, str(expdir.absolute()))\n    sys.path.insert(0, os.path.dirname(str(expdir.absolute())))\n    s = spec_from_file_location(\n        expdir.name,\n        str(expdir.absolute() / ""__init__.py""),\n        submodule_search_locations=[expdir.absolute()],\n    )\n    m = module_from_spec(s)\n    s.loader.exec_module(m)\n    sys.modules[expdir.name] = m\n    return m\n\n\ndef _tricky_dir_copy(dir_from, dir_to):\n    os.makedirs(dir_to, exist_ok=True)\n    shutil.rmtree(dir_to)\n    shutil.copytree(dir_from, dir_to)\n\n\ndef dump_code(expdir, logdir):\n    """"""\n    @TODO: Docs. Contribution is welcome\n    """"""\n    expdir = expdir[:-1] if expdir.endswith(""/"") else expdir\n    new_src_dir = f""code""\n\n    # @TODO: hardcoded\n    old_pro_dir = os.path.dirname(os.path.abspath(__file__)) + ""/../""\n    new_pro_dir = os.path.join(logdir, new_src_dir, ""catalyst"")\n    _tricky_dir_copy(old_pro_dir, new_pro_dir)\n\n    old_expdir = os.path.abspath(expdir)\n    expdir_ = os.path.basename(old_expdir)\n    new_expdir = os.path.join(logdir, new_src_dir, expdir_)\n    _tricky_dir_copy(old_expdir, new_expdir)\n\n\ndef dump_python_files(src, dst):\n    """"""\n    @TODO: Docs. Contribution is welcome\n    """"""\n    py_files = list(src.glob(""*.py""))\n    ipynb_files = list(src.glob(""*.ipynb""))\n\n    py_files += ipynb_files\n    py_files = list(set(py_files))\n    for py_file in py_files:\n        shutil.copy2(f""{str(py_file.absolute())}"", f""{dst}/{py_file.name}"")\n\n\ndef import_experiment_and_runner(expdir: pathlib.Path):\n    """"""\n    @TODO: Docs. Contribution is welcome\n    """"""\n    if not isinstance(expdir, pathlib.Path):\n        expdir = pathlib.Path(expdir)\n    m = import_module(expdir)\n    Runner = m.Runner\n    if hasattr(m, ""Experiment""):\n        Experiment = m.Experiment\n    else:\n        Experiment = None\n    return Experiment, Runner\n\n\ndef dump_base_experiment_code(src: pathlib.Path, dst: pathlib.Path):\n    """"""\n    @TODO: Docs. Contribution is welcome\n    """"""\n    utcnow = get_utcnow_time()\n    dst_ = dst.joinpath(""code"")\n    dst = dst.joinpath(f""code-{utcnow}"") if dst_.exists() else dst_\n    os.makedirs(dst, exist_ok=True)\n    dump_python_files(src, dst)\n\n\ndef distributed_cmd_run(\n    worker_fn: Callable, distributed: bool = True, *args, **kwargs\n) -> None:\n    """"""\n    Distributed run\n\n    Args:\n        worker_fn (Callable):\n        distributed (bool):\n        args: additional parameters for worker_fn\n        kwargs: additional key-value parameters for worker_fn\n    """"""\n    distributed_params = get_distributed_params()\n    local_rank = distributed_params[""local_rank""]\n    world_size = distributed_params[""world_size""]\n\n    if distributed and torch.distributed.is_initialized():\n        warnings.warn(\n            ""Looks like you are trying to call distributed setup twice, ""\n            ""switching to normal run for correct distributed training.""\n        )\n\n    if (\n        not distributed\n        or torch.distributed.is_initialized()\n        or world_size <= 1\n    ):\n        worker_fn(*args, **kwargs)\n    elif local_rank is not None:\n        torch.cuda.set_device(int(local_rank))\n\n        torch.distributed.init_process_group(\n            backend=""nccl"", init_method=""env://""\n        )\n        worker_fn(*args, **kwargs)\n    else:\n        workers = []\n        try:\n            for local_rank in range(torch.cuda.device_count()):\n                rank = distributed_params[""start_rank""] + local_rank\n                env = get_distributed_env(local_rank, rank, world_size)\n                cmd = [sys.executable] + sys.argv.copy()\n                workers.append(subprocess.Popen(cmd, env=env))\n            for worker in workers:\n                worker.wait()\n        finally:\n            for worker in workers:\n                worker.kill()\n\n\n__all__ = [\n    ""import_module"",\n    ""dump_code"",\n    ""dump_python_files"",\n    ""import_experiment_and_runner"",\n    ""dump_base_experiment_code"",\n    ""distributed_cmd_run"",\n]\n'"
catalyst/utils/seed.py,2,"b'import random\n\nimport numpy as np\nfrom packaging.version import parse, Version\n\n\ndef set_global_seed(seed: int) -> None:\n    """"""Sets random seed into PyTorch, TensorFlow, Numpy and Random.\n\n    Args:\n        seed: random seed\n    """"""\n    try:\n        import torch\n    except ImportError:\n        pass\n    else:\n        torch.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    try:\n        import tensorflow as tf\n    except ImportError:\n        pass\n    else:\n        if parse(tf.__version__) >= Version(""2.0.0""):\n            tf.random.set_seed(seed)\n        elif parse(tf.__version__) <= Version(""1.13.2""):\n            tf.set_random_seed(seed)\n        else:\n            tf.compat.v1.set_random_seed(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n\n\n__all__ = [""set_global_seed""]\n'"
catalyst/utils/sys.py,0,"b'from typing import Any, Dict, List, Union\nimport json\nimport os\nfrom pathlib import Path\nimport platform\nimport shutil\nimport subprocess\nimport sys\nimport warnings\n\nfrom catalyst.contrib.tools.tensorboard import SummaryWriter\n\nfrom .config import save_config\nfrom .misc import get_utcnow_time\n\n\ndef _decode_dict(dictionary: Dict[str, Union[bytes, str]]) -> Dict[str, str]:\n    """"""\n    Decode bytes values in the dictionary to UTF-8.\n\n    Args:\n        dictionary: a dict\n\n    Returns:\n        dict: decoded dict\n    """"""\n    result = {\n        k: v.decode(""UTF-8"") if type(v) == bytes else v\n        for k, v in dictionary.items()\n    }\n    return result\n\n\ndef get_environment_vars() -> Dict[str, Any]:\n    """"""\n    Creates a dictionary with environment variables.\n\n    Returns:\n        dict: environment variables\n    """"""\n    result = {\n        ""python_version"": sys.version,\n        ""conda_environment"": os.environ.get(""CONDA_DEFAULT_ENV"", """"),\n        ""creation_time"": get_utcnow_time(),\n        ""sysname"": platform.uname()[0],\n        ""nodename"": platform.uname()[1],\n        ""release"": platform.uname()[2],\n        ""version"": platform.uname()[3],\n        ""architecture"": platform.uname()[4],\n        ""user"": os.environ.get(""USER"", """"),\n        ""path"": os.environ.get(""PWD"", """"),\n    }\n\n    with open(os.devnull, ""w"") as devnull:\n        try:\n            git_branch = (\n                subprocess.check_output(\n                    ""git rev-parse --abbrev-ref HEAD"".split(),\n                    shell=True,\n                    stderr=devnull,\n                )\n                .strip()\n                .decode(""UTF-8"")\n            )\n            git_local_commit = subprocess.check_output(\n                ""git rev-parse HEAD"".split(), shell=True, stderr=devnull\n            )\n            git_origin_commit = subprocess.check_output(\n                f""git rev-parse origin/{git_branch}"".split(),\n                shell=True,\n                stderr=devnull,\n            )\n\n            git = {\n                ""branch"": git_branch,\n                ""local_commit"": git_local_commit,\n                ""origin_commit"": git_origin_commit,\n            }\n            result[""git""] = _decode_dict(git)\n        except (subprocess.CalledProcessError, FileNotFoundError):\n            pass\n\n    result = _decode_dict(result)\n    return result\n\n\ndef list_pip_packages() -> str:\n    """"""\n    @TODO: Docs. Contribution is welcome\n    @TODO: When catching exception, e has no attribute \'output\'\n    """"""\n    result = """"\n    with open(os.devnull, ""w"") as devnull:\n        try:\n            result = (\n                subprocess.check_output(""pip freeze"".split(), stderr=devnull)\n                .strip()\n                .decode(""UTF-8"")\n            )\n        except Exception:\n            warnings.warn(\n                f""Failed to freeze pip packages. ""\n                # f""Pip Output: ```{e.output}```.""\n                f""Continue experiment without pip packages dumping.""\n            )\n            pass\n        # except FileNotFoundError:\n        #     pass\n        # except subprocess.CalledProcessError as e:\n        #     raise Exception(""Failed to list packages"") from e\n\n    return result\n\n\ndef list_conda_packages() -> str:\n    """"""\n    @TODO: Docs. Contribution is welcome\n    @TODO: When catching exception, e has no attribute \'output\'\n    """"""\n    result = """"\n    conda_meta_path = Path(sys.prefix) / ""conda-meta""\n    if conda_meta_path.exists():\n        # We are currently in conda virtual env\n        with open(os.devnull, ""w"") as devnull:\n            try:\n                result = (\n                    subprocess.check_output(\n                        ""conda list --export"".split(), stderr=devnull\n                    )\n                    .strip()\n                    .decode(""UTF-8"")\n                )\n            except Exception:\n                warnings.warn(\n                    f""Running from conda env, ""\n                    f""but failed to list conda packages. ""\n                    # f""Conda Output: ```{e.output}```.""\n                    f""Continue experiment without conda packages dumping.""\n                )\n                pass\n            # except FileNotFoundError:\n            #     pass\n            # except subprocess.CalledProcessError as e:\n            #     raise Exception(\n            #         f""Running from conda env, ""\n            #         f""but failed to list conda packages. ""\n            #         f""Conda Output: {e.output}""\n            #     ) from e\n    return result\n\n\ndef dump_environment(\n    experiment_config: Dict, logdir: str, configs_path: List[str] = None,\n) -> None:\n    """"""\n    Saves config, environment variables and package list in JSON into logdir.\n\n    Args:\n        experiment_config (dict): experiment config\n        logdir (str): path to logdir\n        configs_path: path(s) to config\n    """"""\n    configs_path = configs_path or []\n    configs_path = [\n        Path(path) for path in configs_path if isinstance(path, str)\n    ]\n    config_dir = Path(logdir) / ""configs""\n    config_dir.mkdir(exist_ok=True, parents=True)\n\n    environment = get_environment_vars()\n\n    save_config(experiment_config, config_dir / ""_config.json"")\n    save_config(environment, config_dir / ""_environment.json"")\n\n    pip_pkg = list_pip_packages()\n    (config_dir / ""pip-packages.txt"").write_text(pip_pkg)\n    conda_pkg = list_conda_packages()\n    if conda_pkg:\n        (config_dir / ""conda-packages.txt"").write_text(conda_pkg)\n\n    for path in configs_path:\n        name: str = path.name\n        outpath = config_dir / name\n        shutil.copyfile(path, outpath)\n\n    config_str = json.dumps(experiment_config, indent=2, ensure_ascii=False)\n    config_str = config_str.replace(""\\n"", ""\\n\\n"")\n\n    environment_str = json.dumps(environment, indent=2, ensure_ascii=False)\n    environment_str = environment_str.replace(""\\n"", ""\\n\\n"")\n\n    pip_pkg = pip_pkg.replace(""\\n"", ""\\n\\n"")\n    conda_pkg = conda_pkg.replace(""\\n"", ""\\n\\n"")\n    with SummaryWriter(config_dir) as writer:\n        writer.add_text(""_config"", config_str, 0)\n        writer.add_text(""_environment"", environment_str, 0)\n        writer.add_text(""pip-packages"", pip_pkg, 0)\n        if conda_pkg:\n            writer.add_text(""conda-packages"", conda_pkg, 0)\n\n\n__all__ = [\n    ""get_environment_vars"",\n    ""list_conda_packages"",\n    ""list_pip_packages"",\n    ""dump_environment"",\n]\n'"
catalyst/utils/torch.py,27,"b'from typing import Dict, Iterable, List, Union\nimport collections\nimport os\nimport re\n\nimport numpy as np\n\nimport torch\nfrom torch import nn\nimport torch.backends\nfrom torch.backends import cudnn\n\nfrom catalyst.tools.typing import Device, Model, Optimizer\n\nfrom .dict import merge_dicts\n\n\ndef get_optimizable_params(model_or_params):\n    """"""Returns all the parameters that requires gradients.""""""\n    params: Iterable[torch.Tensor] = model_or_params\n    if isinstance(model_or_params, nn.Module):\n        params = model_or_params.parameters()\n\n    master_params = [p for p in params if p.requires_grad]\n    return master_params\n\n\ndef get_optimizer_momentum(optimizer: Optimizer) -> float:\n    """"""Get momentum of current optimizer.\n\n    Args:\n        optimizer: PyTorch optimizer\n\n    Returns:\n        float: momentum at first param group\n    """"""\n    betas = optimizer.param_groups[0].get(""betas"", None)\n    momentum = optimizer.param_groups[0].get(""momentum"", None)\n    return betas[0] if betas is not None else momentum\n\n\ndef set_optimizer_momentum(optimizer: Optimizer, value: float, index: int = 0):\n    """"""Set momentum of ``index`` \'th param group of optimizer to ``value``.\n\n    Args:\n        optimizer: PyTorch optimizer\n        value (float): new value of momentum\n        index (int, optional): integer index of optimizer\'s param groups,\n            default is 0\n    """"""\n    betas = optimizer.param_groups[0].get(""betas"", None)\n    momentum = optimizer.param_groups[0].get(""momentum"", None)\n    if betas is not None:\n        _, beta = betas\n        optimizer.param_groups[index][""betas""] = (value, beta)\n    elif momentum is not None:\n        optimizer.param_groups[index][""momentum""] = value\n\n\ndef get_device() -> torch.device:\n    """"""Simple returning the best available device (GPU or CPU).""""""\n    return torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n\n\ndef get_available_gpus():\n    """"""Array of available GPU ids.\n\n    Examples:\n        >>> os.environ[""CUDA_VISIBLE_DEVICES""] = ""0,2""\n        >>> get_available_gpus()\n        [0, 2]\n\n        >>> os.environ[""CUDA_VISIBLE_DEVICES""] = ""0,-1,1""\n        >>> get_available_gpus()\n        [0]\n\n        >>> os.environ[""CUDA_VISIBLE_DEVICES""] = """"\n        >>> get_available_gpus()\n        []\n\n        >>> os.environ[""CUDA_VISIBLE_DEVICES""] = ""-1""\n        >>> get_available_gpus()\n        []\n\n    Returns:\n        iterable: available GPU ids\n    """"""\n    if ""CUDA_VISIBLE_DEVICES"" in os.environ:\n        result = os.environ[""CUDA_VISIBLE_DEVICES""].split("","")\n        result = [id_ for id_ in result if id_ != """"]\n        # invisible GPUs\n        # https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#env-vars\n        if -1 in result:\n            index = result.index(-1)\n            result = result[:index]\n    elif torch.cuda.is_available():\n        result = list(range(torch.cuda.device_count()))\n    else:\n        result = []\n    return result\n\n\ndef get_activation_fn(activation: str = None):\n    """"""Returns the activation function from ``torch.nn`` by its name.""""""\n    if activation is None or activation.lower() == ""none"":\n        activation_fn = lambda x: x  # noqa: E731\n    else:\n        activation_fn = torch.nn.__dict__[activation]()\n    return activation_fn\n\n\ndef any2device(value, device: Device):\n    """"""\n    Move tensor, list of tensors, list of list of tensors,\n    dict of tensors, tuple of tensors to target device.\n\n    Args:\n        value: Object to be moved\n        device (Device): target device ids\n\n    Returns:\n        Same structure as value, but all tensors and np.arrays moved to device\n    """"""\n    if isinstance(value, dict):\n        return {k: any2device(v, device) for k, v in value.items()}\n    elif isinstance(value, (tuple, list)):\n        return [any2device(v, device) for v in value]\n    elif torch.is_tensor(value):\n        return value.to(device, non_blocking=True)\n    elif (\n        isinstance(value, (np.ndarray, np.void))\n        and value.dtype.fields is not None\n    ):\n        return {\n            k: any2device(value[k], device) for k in value.dtype.fields.keys()\n        }\n    elif isinstance(value, np.ndarray):\n        return torch.Tensor(value).to(device)\n    return value\n\n\ndef prepare_cudnn(deterministic: bool = None, benchmark: bool = None) -> None:\n    """"""\n    Prepares CuDNN benchmark and sets CuDNN\n    to be deterministic/non-deterministic mode\n\n    Args:\n        deterministic (bool): deterministic mode if running in CuDNN backend.\n        benchmark (bool): If ``True`` use CuDNN heuristics to figure out\n            which algorithm will be most performant\n            for your model architecture and input.\n            Setting it to ``False`` may slow down your training.\n    """"""\n    if torch.cuda.is_available():\n        # CuDNN reproducibility\n        # https://pytorch.org/docs/stable/notes/randomness.html#cudnn\n        if deterministic is None:\n            deterministic = (\n                os.environ.get(""CUDNN_DETERMINISTIC"", ""True"") == ""True""\n            )\n        cudnn.deterministic = deterministic\n\n        # https://discuss.pytorch.org/t/how-should-i-disable-using-cudnn-in-my-code/38053/4\n        if benchmark is None:\n            benchmark = os.environ.get(""CUDNN_BENCHMARK"", ""True"") == ""True""\n        cudnn.benchmark = benchmark\n\n\ndef process_model_params(\n    model: Model,\n    layerwise_params: Dict[str, dict] = None,\n    no_bias_weight_decay: bool = True,\n    lr_scaling: float = 1.0,\n) -> List[Union[torch.nn.Parameter, dict]]:\n    """"""Gains model parameters for ``torch.optim.Optimizer``.\n\n    Args:\n        model (torch.nn.Module): Model to process\n        layerwise_params (Dict): Order-sensitive dict where\n            each key is regex pattern and values are layer-wise options\n            for layers matching with a pattern\n        no_bias_weight_decay (bool): If true, removes weight_decay\n            for all ``bias`` parameters in the model\n        lr_scaling (float): layer-wise learning rate scaling,\n            if 1.0, learning rates will not be scaled\n\n    Returns:\n        iterable: parameters for an optimizer\n\n    Example::\n\n        >>> model = catalyst.contrib.models.segmentation.ResnetUnet()\n        >>> layerwise_params = collections.OrderedDict([\n        >>>     (""conv1.*"", dict(lr=0.001, weight_decay=0.0003)),\n        >>>     (""conv.*"", dict(lr=0.002))\n        >>> ])\n        >>> params = process_model_params(model, layerwise_params)\n        >>> optimizer = torch.optim.Adam(params, lr=0.0003)\n\n    """"""\n    params = list(model.named_parameters())\n    layerwise_params = layerwise_params or collections.OrderedDict()\n\n    model_params = []\n    for name, parameters in params:\n        options = {}\n        for pattern, options_ in layerwise_params.items():\n            if re.match(pattern, name) is not None:\n                # all new LR rules write on top of the old ones\n                options = merge_dicts(options, options_)\n\n        # no bias decay from https://arxiv.org/abs/1812.01187\n        if no_bias_weight_decay and name.endswith(""bias""):\n            options[""weight_decay""] = 0.0\n\n        # lr linear scaling from https://arxiv.org/pdf/1706.02677.pdf\n        if ""lr"" in options:\n            options[""lr""] *= lr_scaling\n\n        model_params.append({""params"": parameters, **options})\n\n    return model_params\n\n\ndef get_requires_grad(model: Model):\n    """"""Gets the ``requires_grad`` value for all model parameters.\n\n    Example::\n\n        >>> model = SimpleModel()\n        >>> requires_grad = get_requires_grad(model)\n\n    Args:\n        model (torch.nn.Module): model\n\n    Returns:\n        requires_grad (Dict[str, bool]): value\n    """"""\n    requires_grad = {}\n    for name, param in model.named_parameters():\n        requires_grad[name] = param.requires_grad\n    return requires_grad\n\n\ndef set_requires_grad(\n    model: Model, requires_grad: Union[bool, Dict[str, bool]]\n):\n    """"""Sets the ``requires_grad`` value for all model parameters.\n\n    Example::\n\n        >>> model = SimpleModel()\n        >>> set_requires_grad(model, requires_grad=True)\n        >>> # or\n        >>> model = SimpleModel()\n        >>> set_requires_grad(model, requires_grad={""""})\n\n    Args:\n        model (torch.nn.Module): model\n        requires_grad (Union[bool, Dict[str, bool]]): value\n    """"""\n    if isinstance(requires_grad, dict):\n        for name, param in model.named_parameters():\n            assert (\n                name in requires_grad\n            ), f""Parameter `{name}` does not exist in requires_grad""\n            param.requires_grad = requires_grad[name]\n    else:\n        requires_grad = bool(requires_grad)\n        for param in model.parameters():\n            param.requires_grad = requires_grad\n\n\ndef get_network_output(net: Model, *input_shapes_args, **input_shapes_kwargs):\n    """"""# noqa: D202\n    For each input shape returns an output tensor\n\n    Args:\n        net (Model): the model\n        *input_shapes_args: variable length argument list of shapes\n        **input_shapes_kwargs:\n\n    Examples:\n        >>> net = nn.Linear(10, 5)\n        >>> utils.get_network_output(net, (1, 10))\n        tensor([[[-0.2665,  0.5792,  0.9757, -0.5782,  0.1530]]])\n    """"""\n\n    def _rand_sample(\n        input_shape,\n    ) -> Union[torch.Tensor, Dict[str, torch.Tensor]]:\n        if isinstance(input_shape, dict):\n            input_t = {\n                key: torch.Tensor(torch.randn((1,) + input_shape_))\n                for key, input_shape_ in input_shape.items()\n            }\n        else:\n            input_t = torch.Tensor(torch.randn((1,) + input_shape))\n        return input_t\n\n    input_args = [\n        _rand_sample(input_shape) for input_shape in input_shapes_args\n    ]\n    input_kwargs = {\n        key: _rand_sample(input_shape)\n        for key, input_shape in input_shapes_kwargs.items()\n    }\n\n    output_t = net(*input_args, **input_kwargs)\n    return output_t\n\n\ndef detach(tensor: torch.Tensor) -> np.ndarray:\n    """"""Detach a pytorch tensor from graph and\n    convert it to numpy array\n\n    Args:\n        tensor: PyTorch tensor\n\n    Returns:\n        numpy ndarray\n    """"""\n    return tensor.cpu().detach().numpy()\n\n\ndef trim_tensors(tensors):\n    """"""\n    Trim padding off of a batch of tensors to the smallest possible length.\n    Should be used with `catalyst.data.DynamicLenBatchSampler`.\n\n    Adapted from ""Dynamic minibatch trimming to improve BERT training speed""\n    https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/discussion/94779\n\n    Args:\n        tensors ([torch.tensor]): list of tensors to trim.\n\n    Returns:\n        ([torch.tensor]): list of trimmed tensors.\n    """"""\n    max_len = torch.max(torch.sum((tensors[0] != 0), 1))\n    if max_len > 2:\n        tensors = [tsr[:, :max_len] for tsr in tensors]\n    return tensors\n\n\n__all__ = [\n    ""get_optimizable_params"",\n    ""get_optimizer_momentum"",\n    ""set_optimizer_momentum"",\n    ""get_device"",\n    ""get_available_gpus"",\n    ""get_activation_fn"",\n    ""any2device"",\n    ""prepare_cudnn"",\n    ""process_model_params"",\n    ""get_requires_grad"",\n    ""set_requires_grad"",\n    ""get_network_output"",\n    ""detach"",\n    ""trim_tensors"",\n]\n'"
examples/cifar_simple/__init__.py,0,"b'# flake8: noqa\n\nimport experiments as exp\n\nfrom catalyst.dl import registry, SupervisedRunner as Runner\n\nfrom .experiment import Experiment\nfrom .model import SimpleNet\n\nregistry.EXPERIMENTS.add_from_module(exp)\n'"
examples/cifar_simple/experiment.py,0,"b'from collections import OrderedDict\n\nimport torchvision\nfrom torchvision import transforms\n\nfrom catalyst.dl import ConfigExperiment\n\n\nclass Experiment(ConfigExperiment):\n    """"""\n    @TODO: Docs. Contribution is welcome\n    """"""\n\n    @staticmethod\n    def get_transforms(stage: str = None, mode: str = None):\n        """"""\n        @TODO: Docs. Contribution is welcome\n        """"""\n        return transforms.Compose(\n            [\n                transforms.ToTensor(),\n                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n            ]\n        )\n\n    def get_datasets(self, stage: str, **kwargs):\n        """"""\n        @TODO: Docs. Contribution is welcome\n        """"""\n        datasets = OrderedDict()\n\n        trainset = torchvision.datasets.CIFAR10(\n            root=""./data"",\n            train=True,\n            download=True,\n            transform=Experiment.get_transforms(stage=stage, mode=""train""),\n        )\n        testset = torchvision.datasets.CIFAR10(\n            root=""./data"",\n            train=False,\n            download=True,\n            transform=Experiment.get_transforms(stage=stage, mode=""valid""),\n        )\n\n        datasets[""train""] = trainset\n        datasets[""valid""] = testset\n\n        return datasets\n'"
examples/cifar_simple/model.py,1,"b'from torch import nn\nfrom torch.nn import functional as F\n\nfrom catalyst.contrib import registry\n\n\n@registry.Model\nclass SimpleNet(nn.Module):\n    """"""\n    @TODO: Docs. Contribution is welcome\n    """"""\n\n    def __init__(self):\n        """"""\n        @TODO: Docs. Contribution is welcome\n        """"""\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        """"""\n        @TODO: Docs. Contribution is welcome\n        """"""\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(-1, 16 * 5 * 5)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n'"
examples/cifar_stages/__init__.py,0,"b'# flake8: noqa\n\nfrom catalyst.dl import registry, SupervisedRunner as Runner\n\nfrom .experiment import Experiment\nfrom .model import SimpleNet\n\nregistry.Model(SimpleNet)\n'"
examples/cifar_stages/experiment.py,1,"b'from collections import OrderedDict\n\nimport torch\nimport torchvision\n\nfrom catalyst import utils\nfrom catalyst.dl import ConfigExperiment\n\n\nclass CIFAR10(torchvision.datasets.CIFAR10):\n    """"""`CIFAR10 <https://www.cs.toronto.edu/~kriz/cifar.html>`_ Dataset.""""""\n\n    def __getitem__(self, index: int):\n        """"""Fetch a data sample for a given index.\n\n        Args:\n            index (int): index of the element in the dataset\n\n        Returns:\n            Single element by index\n        """"""\n        image, target = self.data[index], self.targets[index]\n\n        if self.transform is not None:\n            image = self.transform({""image"": image})[""image""]\n\n        return image, target\n\n\nclass Experiment(ConfigExperiment):\n    """"""``ConfigExperiment`` with CIFAR10 dataset.""""""\n\n    def get_model(self, stage: str):\n        """"""\n        Model specification for currect stage\n        Args:\n            stage: current stage name\n\n        Returns:\n            model\n        """"""\n        model = super().get_model(stage=stage)\n        if isinstance(model, torch.nn.DataParallel):\n            model = model.module\n\n        if stage == ""stage2"":\n            for key in [""conv1"", ""pool"", ""conv2""]:\n                layer = getattr(model, key)\n                utils.set_requires_grad(layer, requires_grad=False)\n        return model\n\n    def get_datasets(self, stage: str, **kwargs):\n        """"""Provides train/validation subsets from CIFAR10 dataset.\n\n        Args:\n            stage (str): stage name e.g. ``\'stage1\'`` or ``\'infer\'``\n        """"""\n        datasets = OrderedDict()\n        for mode in (""train"", ""valid""):\n            datasets[mode] = CIFAR10(\n                root=""./data"",\n                train=(mode == ""train""),\n                download=True,\n                transform=self.get_transforms(stage=stage, dataset=mode),\n            )\n\n        return datasets\n'"
examples/cifar_stages/model.py,1,"b'from torch import nn\nfrom torch.nn import functional as F\n\n\nclass SimpleNet(nn.Module):\n    """"""\n    @TODO: Docs. Contribution is welcome\n    """"""\n\n    def __init__(self):\n        """"""\n        @TODO: Docs. Contribution is welcome\n        """"""\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        """"""\n        @TODO: Docs. Contribution is welcome\n        """"""\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(-1, 16 * 5 * 5)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n'"
examples/distilbert_text_classification/__init__.py,0,"b'# flake8: noqa\n# pylint: disable=unused-import\n\nfrom catalyst.contrib.models.nlp import BertClassifier\nfrom catalyst.dl import registry, SupervisedRunner as Runner\n\nfrom .experiment import Experiment\n\nregistry.Model(BertClassifier)\n'"
examples/distilbert_text_classification/experiment.py,0,"b'from typing import Dict\nfrom collections import OrderedDict\nfrom pathlib import Path\n\nimport pandas as pd\n\nfrom catalyst.contrib.data.nlp.dataset import TextClassificationDataset\nfrom catalyst.dl import ConfigExperiment\n\n\nclass Experiment(ConfigExperiment):\n    """"""\n    @TODO: Docs. Contribution is welcome\n    """"""\n\n    def __init__(self, config: Dict):\n        """"""\n        @TODO: Docs. Contribution is welcome\n        """"""\n        super().__init__(config)\n        self.config = config\n\n    def get_transforms(self, stage: str = None, mode: str = None):\n        """"""\n        @TODO: Docs. Contribution is welcome\n        """"""\n        return []\n\n    # noinspection PyMethodOverriding\n    def get_datasets(\n        self,\n        stage: str,\n        path_to_data: str,\n        train_filename: str,\n        valid_filename: str,\n        max_sequence_length: int,\n        **kwargs\n    ):\n        """"""\n        @TODO: Docs. Contribution is welcome\n        """"""\n        datasets = OrderedDict()\n\n        path_to_data = Path(path_to_data)\n\n        train_df = pd.read_csv(path_to_data / train_filename)\n        valid_df = pd.read_csv(path_to_data / valid_filename)\n\n        train_dataset = TextClassificationDataset(\n            texts=train_df[""text""],\n            labels=train_df[""label""],\n            label_dict=None,\n            max_seq_length=max_sequence_length,\n        )\n\n        valid_dataset = TextClassificationDataset(\n            texts=valid_df[""text""],\n            labels=valid_df[""label""],\n            label_dict=train_dataset.label_dict,\n            max_seq_length=max_sequence_length,\n        )\n\n        datasets[""train""] = train_dataset\n        datasets[""valid""] = valid_dataset\n\n        return datasets\n'"
tests/_tests_contrib_dl_callbacks/__init__.py,0,"b'# flake8: noqa\nfrom catalyst.dl import registry, SupervisedRunner as Runner\n\nfrom .experiment import Experiment\nfrom .model import SimpleNet\n\nregistry.Model(SimpleNet)\n'"
tests/_tests_contrib_dl_callbacks/experiment.py,1,"b'from collections import OrderedDict\n\nfrom torch.utils.data import Subset\n\nfrom catalyst.contrib.data.transforms import Compose, Normalize, ToTensor\nfrom catalyst.contrib.datasets import MNIST\nfrom catalyst.dl import ConfigExperiment\n\n\nclass Experiment(ConfigExperiment):\n    """"""\n    @TODO: Docs. Contribution is welcome\n    """"""\n\n    @staticmethod\n    def get_transforms(stage: str = None, mode: str = None):\n        """"""\n        @TODO: Docs. Contribution is welcome\n        """"""\n        return Compose([ToTensor(), Normalize((0.1307,), (0.3081,))])\n\n    def get_datasets(\n        self,\n        stage: str,\n        n_samples: int = 320,\n        duplicate_loaders: bool = False,\n        **kwargs\n    ):\n        """"""\n        @TODO: Docs. Contribution is welcome\n        """"""\n        datasets = OrderedDict()\n\n        if stage != ""infer"":\n            trainset = MNIST(\n                ""./data"",\n                train=False,\n                download=True,\n                transform=Experiment.get_transforms(stage=stage, mode=""train""),\n            )\n            testset = MNIST(\n                ""./data"",\n                train=False,\n                download=True,\n                transform=Experiment.get_transforms(stage=stage, mode=""valid""),\n            )\n            if n_samples > 0:\n                trainset = Subset(trainset, list(range(n_samples)))\n                testset = Subset(testset, list(range(n_samples)))\n            datasets[""train""] = trainset\n            datasets[""valid""] = testset\n            if duplicate_loaders:\n                datasets[""train_additional""] = trainset\n                datasets[""valid_additional""] = testset\n        else:\n            testset = MNIST(\n                ""./data"",\n                train=False,\n                download=True,\n                transform=Experiment.get_transforms(stage=stage, mode=""valid""),\n            )\n            if n_samples > 0:\n                testset = Subset(testset, list(range(n_samples)))\n            datasets[""infer""] = testset\n\n        return datasets\n'"
tests/_tests_contrib_dl_callbacks/model.py,1,"b'from torch import nn\nfrom torch.nn import functional as F\n\n\nclass SimpleNet(nn.Module):\n    """"""\n    @TODO: Docs. Contribution is welcome\n    """"""\n\n    def __init__(self):\n        """"""\n        @TODO: Docs. Contribution is welcome\n        """"""\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n        self.fc1 = nn.Linear(4 * 4 * 50, 500)\n        self.fc2 = nn.Linear(500, 10)\n\n    def forward(self, x):\n        """"""\n        @TODO: Docs. Contribution is welcome\n        """"""\n        x = F.relu(self.conv1(x))\n        x = F.max_pool2d(x, 2, 2)\n        x = F.relu(self.conv2(x))\n        x = F.max_pool2d(x, 2, 2)\n        x = x.view(-1, 4 * 4 * 50)\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n'"
tests/_tests_cv_classification/__init__.py,0,"b'# flake8: noqa\nfrom catalyst.dl import registry, SupervisedRunner as Runner\n\nfrom .experiment import Experiment\nfrom .model import SimpleNet\n\nregistry.Model(SimpleNet)\n'"
tests/_tests_cv_classification/experiment.py,0,"b'from collections import OrderedDict\n\nfrom catalyst.contrib.data.transforms import Compose, Normalize, ToTensor\nfrom catalyst.contrib.datasets import MNIST\nfrom catalyst.dl import ConfigExperiment\n\n\nclass Experiment(ConfigExperiment):\n    """"""\n    @TODO: Docs. Contribution is welcome\n    """"""\n\n    @staticmethod\n    def get_transforms(stage: str = None, mode: str = None):\n        """"""\n        @TODO: Docs. Contribution is welcome\n        """"""\n        return Compose([ToTensor(), Normalize((0.1307,), (0.3081,))])\n\n    def get_datasets(self, stage: str, **kwargs):\n        """"""\n        @TODO: Docs. Contribution is welcome\n        """"""\n        datasets = OrderedDict()\n\n        if stage != ""infer"":\n            trainset = MNIST(\n                ""./data"",\n                train=False,\n                download=True,\n                transform=Experiment.get_transforms(stage=stage, mode=""train""),\n            )\n            testset = MNIST(\n                ""./data"",\n                train=False,\n                download=True,\n                transform=Experiment.get_transforms(stage=stage, mode=""valid""),\n            )\n\n            datasets[""train""] = trainset\n            datasets[""valid""] = testset\n        else:\n            testset = MNIST(\n                ""./data"",\n                train=False,\n                download=True,\n                transform=Experiment.get_transforms(stage=stage, mode=""valid""),\n            )\n            datasets[""infer""] = testset\n\n        return datasets\n'"
tests/_tests_cv_classification/model.py,1,"b'from torch import nn\nfrom torch.nn import functional as F\n\n\nclass SimpleNet(nn.Module):\n    """"""\n    @TODO: Docs. Contribution is welcome\n    """"""\n\n    def __init__(self):\n        """"""\n        @TODO: Docs. Contribution is welcome\n        """"""\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n        self.fc1 = nn.Linear(4 * 4 * 50, 500)\n        self.fc2 = nn.Linear(500, 10)\n\n    def forward(self, x):\n        """"""\n        @TODO: Docs. Contribution is welcome\n        """"""\n        x = F.relu(self.conv1(x))\n        x = F.max_pool2d(x, 2, 2)\n        x = F.relu(self.conv2(x))\n        x = F.max_pool2d(x, 2, 2)\n        x = x.view(-1, 4 * 4 * 50)\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n'"
tests/_tests_cv_classification_transforms/__init__.py,0,"b'# flake8: noqa\nfrom catalyst.dl import registry, SupervisedRunner as Runner\n\nfrom .experiment import Experiment\nfrom .model import SimpleNet\n\nregistry.Model(SimpleNet)\n'"
tests/_tests_cv_classification_transforms/experiment.py,0,"b'from typing import Tuple\nfrom collections import OrderedDict\n\nfrom catalyst.contrib.datasets import MNIST as _MNIST\nfrom catalyst.dl.experiment import ConfigExperiment\n\n\nclass MNIST(_MNIST):\n    """"""`MNIST <http://yann.lecun.com/exdb/mnist/>`_ Dataset.""""""\n\n    def __getitem__(self, index: int) -> Tuple:\n        """"""Fetches a sample for a given index from MNIST dataset.\n\n        Args:\n            index (int): index of the element in the dataset\n\n        Returns:\n            tuple: (image, target) where target is index of the target class\n        """"""\n        image, target = self.data[index], self.targets[index]\n\n        if self.transform is not None:\n            image = self.transform({""image"": image})[""image""]\n\n        return image, target\n\n\nclass Experiment(ConfigExperiment):\n    """"""``ConfigExperiment`` with MNIST dataset.""""""\n\n    def get_datasets(self, stage: str, **kwargs):\n        """"""Provides train/validation subsets from MNIST dataset.\n\n        Args:\n            stage (str): stage name e.g. ``\'stage1\'`` or ``\'infer\'``\n        """"""\n        datasets = OrderedDict()\n        for mode in (""train"", ""valid""):\n            datasets[mode] = MNIST(\n                ""./data"",\n                train=False,\n                download=True,\n                transform=self.get_transforms(stage=stage, dataset=mode),\n            )\n\n        return datasets\n'"
tests/_tests_cv_classification_transforms/model.py,1,"b'from torch import nn\nfrom torch.nn import functional as F\n\n\nclass SimpleNet(nn.Module):\n    """"""\n    @TODO: Docs. Contribution is welcome\n    """"""\n\n    def __init__(self):\n        """"""\n        @TODO: Docs. Contribution is welcome\n        """"""\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n        self.fc1 = nn.Linear(4 * 4 * 50, 500)\n        self.fc2 = nn.Linear(500, 10)\n\n    def forward(self, x):\n        """"""\n        @TODO: Docs. Contribution is welcome\n        """"""\n        x = F.relu(self.conv1(x))\n        x = F.max_pool2d(x, 2, 2)\n        x = F.relu(self.conv2(x))\n        x = F.max_pool2d(x, 2, 2)\n        x = x.view(-1, 4 * 4 * 50)\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n'"
tests/_tests_cv_segmentation/__init__.py,0,b'# flake8: noqa\nfrom catalyst.dl import SupervisedRunner as Runner\n\nfrom .experiment import Experiment\n'
tests/_tests_cv_segmentation/dataset.py,1,"b'from typing import List\nfrom pathlib import Path\n\nfrom skimage.io import imread as gif_imread\n\nfrom torch.utils.data import Dataset\n\nfrom catalyst import utils\n\n\nclass SegmentationDataset(Dataset):\n    """"""Dataset for segmentation tasks\n    Returns a dict with ``image``, ``mask`` and ``filename`` keys\n    """"""\n\n    def __init__(\n        self, images: List[Path], masks: List[Path] = None, transforms=None\n    ) -> None:\n        """"""\n        Args:\n            images (List[Path]): list of paths to the images\n            masks (List[Path]): list of paths to the masks\n                (names must be the same as in images)\n            transforms (optional): dict transforms\n        """"""\n        self.images = images\n        self.masks = masks\n        self.transforms = transforms\n\n    def __len__(self) -> int:\n        """"""\n        Returns:\n            int: length of the dataset\n        """"""\n        return len(self.images)\n\n    def __getitem__(self, idx: int) -> dict:\n        """"""Fetch a data sample for a given index.\n\n        Args:\n            index (int): index of the element in the dataset\n\n        Returns:\n            Single element by index\n        """"""\n        image_path = self.images[idx]\n        image = utils.imread(image_path)\n\n        result = {""image"": image}\n\n        if self.masks is not None:\n            mask = gif_imread(self.masks[idx])\n            result[""mask""] = mask\n\n        if self.transforms is not None:\n            result = self.transforms(result)\n\n        result[""filename""] = image_path.name\n\n        return result\n'"
tests/_tests_cv_segmentation/experiment.py,0,"b'from collections import OrderedDict\nfrom pathlib import Path\n\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\nfrom catalyst.dl import ConfigExperiment\n\nfrom .dataset import SegmentationDataset\n\n\nclass Experiment(ConfigExperiment):\n    """"""\n    @TODO: Docs. Contribution is welcome\n    """"""\n\n    def get_datasets(\n        self,\n        stage: str,\n        image_path: str,\n        mask_path: str,\n        valid_size: float,\n        **kwargs\n    ):\n        """"""\n        @TODO: Docs. Contribution is welcome\n        """"""\n        _images = np.array(sorted(Path(image_path).glob(""*.jpg"")))\n        _masks = np.array(sorted(Path(mask_path).glob(""*.gif"")))\n\n        _indices = np.arange(len(_images))\n\n        train_indices, valid_indices = train_test_split(\n            _indices,\n            test_size=valid_size,\n            random_state=self.initial_seed,\n            shuffle=True,\n        )\n\n        datasets = OrderedDict()\n        for mode, indices in zip(\n            [""train"", ""valid""], [train_indices, valid_indices]\n        ):\n            datasets[mode] = SegmentationDataset(\n                images=_images[indices].tolist(),\n                masks=_masks[indices].tolist(),\n                transforms=self.get_transforms(stage=stage, dataset=mode),\n            )\n\n        return datasets\n'"
tests/_tests_dl_callbacks/__init__.py,0,"b'# flake8: noqa\nfrom catalyst.dl import registry, SupervisedRunner as Runner\n\nfrom .experiment import Experiment\nfrom .model import SimpleNet\n\nregistry.Model(SimpleNet)\n'"
tests/_tests_dl_callbacks/experiment.py,1,"b'from collections import OrderedDict\n\nfrom torch.utils.data import Subset\n\nfrom catalyst.contrib.data.transforms import Compose, Normalize, ToTensor\nfrom catalyst.contrib.datasets import MNIST\nfrom catalyst.dl import ConfigExperiment\n\n\nclass Experiment(ConfigExperiment):\n    """"""\n    @TODO: Docs. Contribution is welcome\n    """"""\n\n    @staticmethod\n    def get_transforms(stage: str = None, mode: str = None):\n        """"""\n        @TODO: Docs. Contribution is welcome\n        """"""\n        return Compose([ToTensor(), Normalize((0.1307,), (0.3081,))])\n\n    def get_datasets(self, stage: str, n_samples: int = 320, **kwargs):\n        """"""\n        @TODO: Docs. Contribution is welcome\n        """"""\n        datasets = OrderedDict()\n\n        if stage != ""infer"":\n            trainset = MNIST(\n                ""./data"",\n                train=False,\n                download=True,\n                transform=Experiment.get_transforms(stage=stage, mode=""train""),\n            )\n            testset = MNIST(\n                ""./data"",\n                train=False,\n                download=True,\n                transform=Experiment.get_transforms(stage=stage, mode=""valid""),\n            )\n            if n_samples > 0:\n                trainset = Subset(trainset, list(range(n_samples)))\n                testset = Subset(testset, list(range(n_samples)))\n            datasets[""train""] = trainset\n            datasets[""valid""] = testset\n        else:\n            testset = MNIST(\n                ""./data"",\n                train=False,\n                download=True,\n                transform=Experiment.get_transforms(stage=stage, mode=""valid""),\n            )\n            if n_samples > 0:\n                testset = Subset(testset, list(range(n_samples)))\n            datasets[""infer""] = testset\n\n        return datasets\n'"
tests/_tests_dl_callbacks/model.py,1,"b'from torch import nn\nfrom torch.nn import functional as F\n\n\nclass SimpleNet(nn.Module):\n    """"""\n    @TODO: Docs. Contribution is welcome\n    """"""\n\n    def __init__(self):\n        """"""\n        @TODO: Docs. Contribution is welcome\n        """"""\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n        self.fc1 = nn.Linear(4 * 4 * 50, 500)\n        self.fc2 = nn.Linear(500, 10)\n\n    def forward(self, x):\n        """"""\n        @TODO: Docs. Contribution is welcome\n        """"""\n        x = F.relu(self.conv1(x))\n        x = F.max_pool2d(x, 2, 2)\n        x = F.relu(self.conv2(x))\n        x = F.max_pool2d(x, 2, 2)\n        x = x.view(-1, 4 * 4 * 50)\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n'"
tests/_tests_scripts/__init__.py,0,b''
tests/_tests_scripts/core.py,0,b'# flake8: noqa\nfrom catalyst.core import *\n'
tests/_tests_scripts/core_contrib_data.py,0,b'# flake8: noqa\nfrom catalyst.contrib.data import *\n'
tests/_tests_scripts/core_contrib_dl.py,0,b'# flake8: noqa\nfrom catalyst.contrib.dl import *\n'
tests/_tests_scripts/core_contrib_models.py,0,b'# flake8: noqa\nfrom catalyst.contrib.models import *\n'
tests/_tests_scripts/core_contrib_nn.py,0,b'# flake8: noqa\nfrom catalyst.contrib.nn import *\n'
tests/_tests_scripts/core_data.py,0,b'# flake8: noqa\nfrom catalyst.data import *\n'
tests/_tests_scripts/cv_contrib_models.py,0,b'# flake8: noqa\nfrom catalyst.contrib.models.cv import *\n'
tests/_tests_scripts/cv_z_segmentation.py,12,"b'#!/usr/bin/env python\n# coding: utf-8\n# flake8: noqa\n# isort:skip_file\nimport os\nimport sys\n\n\nif os.getenv(""USE_DDP"", ""0"") != ""0"":\n    sys.exit()\n\n\n# # Data\n\n# In[ ]:\n\nimport matplotlib\n\nmatplotlib.use(""Agg"")\nimport matplotlib.pyplot as plt\n\nplt.ioff()\n\n# ! pip install tifffile\n\n# In[ ]:\n\nimport tifffile as tiff\n\nimages = tiff.imread(""./data/isbi/train-volume.tif"")\nmasks = tiff.imread(""./data/isbi/train-labels.tif"")\n\ndata = list(zip(images, masks))\n\ntrain_data = data[:2]\nvalid_data = data[:2]\n\n# In[ ]:\n\nimport collections\nimport numpy as np\nimport torch\nfrom catalyst.contrib.data.transforms import Compose, Normalize\nfrom catalyst.data import Augmentor\nfrom catalyst.dl import utils\nfrom catalyst.contrib.nn.criterion import (\n    LovaszLossBinary,\n    LovaszLossMultiLabel,\n    LovaszLossMultiClass,\n)\n\nbs = 1\nnum_workers = 0\n\n\ndef get_loaders(transform):\n    open_fn = lambda x: {""features"": x[0], ""targets"": x[1]}\n\n    loaders = collections.OrderedDict()\n\n    train_loader = utils.get_loader(\n        train_data,\n        open_fn=open_fn,\n        dict_transform=transform,\n        batch_size=bs,\n        num_workers=num_workers,\n        shuffle=True,\n    )\n\n    valid_loader = utils.get_loader(\n        valid_data,\n        open_fn=open_fn,\n        dict_transform=transform,\n        batch_size=bs,\n        num_workers=num_workers,\n        shuffle=False,\n    )\n\n    loaders[""train""] = train_loader\n    loaders[""valid""] = valid_loader\n\n    return loaders\n\n\ndata_transform = Compose(\n    [\n        Augmentor(\n            dict_key=""features"",\n            augment_fn=lambda x: torch.from_numpy(\n                x.copy().astype(np.float32) / 255.0\n            ).unsqueeze_(0),\n        ),\n        Augmentor(dict_key=""features"", augment_fn=Normalize((0.5,), (0.5,)),),\n        Augmentor(\n            dict_key=""targets"",\n            augment_fn=lambda x: torch.from_numpy(\n                x.copy().astype(np.float32) / 255.0\n            ).unsqueeze_(0),\n        ),\n    ]\n)\n\nloaders = get_loaders(data_transform)\n\n# # Model\n\n# In[ ]:\n\nfrom catalyst.contrib.models.cv import Unet\n\n# # Train\n\n# In[ ]:\n\nimport torch\nimport torch.nn as nn\nfrom catalyst.dl.runner import SupervisedRunner\n\n# experiment setup\nnum_epochs = 2\nlogdir = ""./logs/segmentation_notebook""\n\n# model, criterion, optimizer\nmodel = Unet(num_classes=1, in_channels=1, num_channels=32, num_blocks=2)\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\nscheduler = torch.optim.lr_scheduler.MultiStepLR(\n    optimizer, milestones=[10, 20, 40], gamma=0.3\n)\n\n# model runner\nrunner = SupervisedRunner()\n\n# model training\nrunner.train(\n    model=model,\n    criterion=criterion,\n    optimizer=optimizer,\n    loaders=loaders,\n    logdir=logdir,\n    num_epochs=num_epochs,\n    check=True,\n    load_best_on_end=True,\n)\n\n# # Inference\n\n# In[ ]:\n\nrunner_out = runner.predict_loader(loader=loaders[""valid""])\n\n# # Predictions visualization\n\n# In[ ]:\n\nimport matplotlib.pyplot as plt\n\nplt.style.use(""ggplot"")\n\n# In[ ]:\n\nsigmoid = lambda x: 1 / (1 + np.exp(-x))\n\nfor i, (input, output) in enumerate(zip(valid_data, runner_out)):\n    image, mask = input\n\n    threshold = 0.5\n\n    plt.figure(figsize=(10, 8))\n\n    plt.subplot(1, 3, 1)\n    plt.imshow(image, ""gray"")\n\n    plt.subplot(1, 3, 2)\n    output = output[""logits""].cpu().numpy()\n    output = sigmoid(output[0, 0].copy())  # [bs; ch; h; w] -> [h; w]\n    output = (output > threshold).astype(np.uint8)\n    plt.imshow(output, ""gray"")\n\n    plt.subplot(1, 3, 3)\n    plt.imshow(mask, ""gray"")\n\n    plt.show()\n\n# lovasz LovaszLossBinary criterion\n\nmodel = Unet(num_classes=1, in_channels=1, num_channels=32, num_blocks=2)\ncriterion = LovaszLossBinary()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\nrunner.train(\n    model=model,\n    criterion=criterion,\n    optimizer=optimizer,\n    loaders=loaders,\n    logdir=logdir,\n    num_epochs=num_epochs,\n    check=True,\n)\n\n# Multiclasses checks\n# lovasz LovaszLossMultiClass criterion\n\ndata_transform = Compose(\n    [\n        Augmentor(\n            dict_key=""features"",\n            augment_fn=lambda x: torch.from_numpy(\n                x.copy().astype(np.float32) / 255.0\n            ).unsqueeze_(0),\n        ),\n        Augmentor(dict_key=""features"", augment_fn=Normalize((0.5,), (0.5,)),),\n        Augmentor(\n            dict_key=""targets"",\n            augment_fn=lambda x: torch.from_numpy(\n                x.copy().astype(np.float32) / 255.0\n            ),\n        ),\n    ]\n)\n\nloaders = get_loaders(data_transform)\n\nmodel = Unet(num_classes=2, in_channels=1, num_channels=32, num_blocks=2)\ncriterion = LovaszLossMultiClass()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\nrunner.train(\n    model=model,\n    criterion=criterion,\n    optimizer=optimizer,\n    loaders=loaders,\n    logdir=logdir,\n    num_epochs=num_epochs,\n    check=True,\n)\n\n# lovasz LovaszLossMultiLabel criterion\n\n\ndef transform_targets(x):\n    x1 = x.copy().astype(np.float32)[None]\n    x2 = 255 - x.copy().astype(np.float32)[None]\n    return np.vstack([x1, x2]) / 255.0\n\n\ndata_transform = Compose(\n    [\n        Augmentor(\n            dict_key=""features"",\n            augment_fn=lambda x: torch.from_numpy(\n                x.copy().astype(np.float32) / 255.0\n            ).unsqueeze_(0),\n        ),\n        Augmentor(dict_key=""features"", augment_fn=Normalize((0.5,), (0.5,)),),\n        Augmentor(\n            dict_key=""targets"",\n            augment_fn=lambda x: torch.from_numpy(transform_targets(x)),\n        ),\n    ]\n)\n\nloaders = get_loaders(data_transform)\n\nmodel = Unet(num_classes=2, in_channels=1, num_channels=32, num_blocks=2)\ncriterion = LovaszLossMultiLabel()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\nrunner.train(\n    model=model,\n    criterion=criterion,\n    optimizer=optimizer,\n    loaders=loaders,\n    logdir=logdir,\n    num_epochs=num_epochs,\n    check=True,\n)\n'"
tests/_tests_scripts/cv_z_unets.py,2,"b'#!/usr/bin/env python\n# coding: utf-8\n# flake8: noqa\n# isort:skip_file\nimport os\nimport sys\n\n\nif os.getenv(""USE_DDP"", ""0"") != ""0"":\n    sys.exit()\n\n\n# # main check\n\n# In[ ]:\n\nimport torch\n\n\ndef check_unet(net_fn):\n    net = net_fn()\n    # print(net)\n    # print(""-""*80)\n    in_tensor = torch.Tensor(4, 3, 256, 256)\n    out_tensor_ = torch.Tensor(4, 1, 256, 256)\n    # print(in_tensor.shape)\n    # print(""-""*80)\n    out_tensor = net(in_tensor)\n    # print(out_tensor.shape)\n    # print(""-""*80)\n    # print(sum(p.numel() for p in net.parameters()))\n    assert out_tensor.shape == out_tensor_.shape, f""{net_fn} feels bad""\n    print(f""{net_fn} feels good"")\n    return net\n\n\n# ----\n\n# # no pretrain\n\n# In[ ]:\n\nfrom catalyst.contrib.models.cv import Unet\n\nnet = check_unet(Unet)\n\n# In[ ]:\n\nfrom catalyst.contrib.models.cv import Linknet\n\nnet = check_unet(Linknet)\n\n# In[ ]:\n\nfrom catalyst.contrib.models.cv import FPNUnet\n\nnet = check_unet(FPNUnet)\n\n# In[ ]:\n\nfrom catalyst.contrib.models.cv import PSPnet\n\nnet = check_unet(PSPnet)\n\n# ---\n\n# # resnet pretrained\n\n# In[ ]:\n\nfrom catalyst.contrib.models.cv import ResnetUnet\n\nnet = check_unet(ResnetUnet)\n\n# In[ ]:\n\nfrom catalyst.contrib.models.cv import ResnetLinknet\n\nnet = check_unet(ResnetLinknet)\n\n# In[ ]:\n\nfrom catalyst.contrib.models.cv import ResnetFPNUnet\n\nnet = check_unet(ResnetFPNUnet)\n\n# In[ ]:\n\nfrom catalyst.contrib.models.cv import ResnetPSPnet\n\nnet = check_unet(ResnetPSPnet)\n'"
tests/_tests_scripts/dl_callbacks.py,0,b'# flake8: noqa\nfrom catalyst.dl.callbacks import *\n'
tests/_tests_scripts/dl_experiment.py,0,b'# flake8: noqa\nfrom catalyst.dl.experiment import *\n'
tests/_tests_scripts/dl_runner.py,0,b'# flake8: noqa\nfrom catalyst.dl.runner import *\n'
tests/_tests_scripts/dl_utils.py,0,b'# flake8: noqa\nfrom catalyst.dl.utils import *\n'
tests/_tests_scripts/dl_z_classification.py,20,"b'#!/usr/bin/env python\n# coding: utf-8\n# flake8: noqa\n# isort:skip_file\nimport os\nimport sys\n\n\nif os.getenv(""USE_DDP"", ""0"") != ""0"":\n    sys.exit()\n\n\n# # Data\n\n# In[ ]:\n\nimport collections\nimport torch\n\nfrom catalyst.contrib.datasets import MNIST\nfrom catalyst.contrib.data.transforms import ToTensor, Compose, Normalize\n\nbs = 32\nnum_workers = 0\n\ndata_transform = Compose([ToTensor(), Normalize((0.1307,), (0.3081,))])\n\nloaders = collections.OrderedDict()\n\ntrainset = MNIST(\n    ""./data"", train=False, download=True, transform=data_transform\n)\ntrainloader = torch.utils.data.DataLoader(\n    trainset, batch_size=bs, shuffle=True, num_workers=num_workers\n)\n\ntestset = MNIST(""./data"", train=False, download=True, transform=data_transform)\ntestloader = torch.utils.data.DataLoader(\n    testset, batch_size=bs, shuffle=False, num_workers=num_workers\n)\n\nloaders[""train""] = trainloader\nloaders[""valid""] = testloader\n\n# # Model\n\n# In[ ]:\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Net(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n        self.fc1 = nn.Linear(4 * 4 * 50, 500)\n        self.fc2 = nn.Linear(500, 10)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.max_pool2d(x, 2, 2)\n        x = F.relu(self.conv2(x))\n        x = F.max_pool2d(x, 2, 2)\n        x = x.view(-1, 4 * 4 * 50)\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\n\n# # Intro\n# @TODO\n\n# In[ ]:\n\n# for graphs use `tensorboard --logdir=./logs`\n\n# In[ ]:\n\nfrom catalyst.dl import utils\n\n# In[ ]:\n\nNUM_EPOCHS = 2\n\n# # Setup 1 - typical training\n\n# In[ ]:\n\nfrom catalyst.dl.runner import SupervisedRunner\n\n# experiment setup\nnum_epochs = NUM_EPOCHS\nlogdir = ""./logs/cifar_simple_notebook_1""\n\n# model, criterion, optimizer\nmodel = Net()\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters())\n\n# model runner\nrunner = SupervisedRunner()\n\n# model training\nrunner.train(\n    model=model,\n    criterion=criterion,\n    optimizer=optimizer,\n    loaders=loaders,\n    logdir=logdir,\n    num_epochs=num_epochs,\n    check=True,\n)\n\n# In[ ]:\n\n# you can use plotly and tensorboard to plot metrics inside jupyter\n# by default it only plots loss\n# utils.plot_metrics(logdir=logdir)\n\n# # Setup 2 - training with scheduler\n\n# In[ ]:\n\nfrom catalyst.dl.runner import SupervisedRunner\n\n# experiment setup\nnum_epochs = NUM_EPOCHS\nlogdir = ""./logs/cifar_simple_notebook_2""\n\n# model, criterion, optimizer\nmodel = Net()\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters())\n\n# any Pytorch scheduler supported\n# scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[3, 8], gamma=0.3)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer, factor=0.5, patience=2\n)\n\n# model runner\nrunner = SupervisedRunner()\n\n# model training\nrunner.train(\n    model=model,\n    criterion=criterion,\n    optimizer=optimizer,\n    scheduler=scheduler,\n    loaders=loaders,\n    logdir=logdir,\n    num_epochs=num_epochs,\n    check=True,\n)\n\n# # Setup 3 - training with early stop\n\n# In[ ]:\n\nfrom catalyst.dl.runner import SupervisedRunner\nfrom catalyst.dl.callbacks import EarlyStoppingCallback\n\n# experiment setup\nnum_epochs = NUM_EPOCHS\nlogdir = ""./logs/cifar_simple_notebook_3""\n\n# model, criterion, optimizer, scheduler\nmodel = Net()\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters())\nscheduler = torch.optim.lr_scheduler.MultiStepLR(\n    optimizer, milestones=[3, 8], gamma=0.3\n)\n\n# model runner\nrunner = SupervisedRunner()\n\n# model training\nrunner.train(\n    model=model,\n    criterion=criterion,\n    optimizer=optimizer,\n    scheduler=scheduler,\n    loaders=loaders,\n    callbacks=[EarlyStoppingCallback(patience=2, min_delta=0.01)],\n    logdir=logdir,\n    num_epochs=num_epochs,\n    check=True,\n)\n\n# In[ ]:\n\n# utils.plot_metrics(logdir=logdir, metrics=[""loss"", ""_base/lr""])\n\n# # Setup 4 - training with additional metrics\n\n# In[ ]:\n\nfrom catalyst.dl.runner import SupervisedRunner\nfrom catalyst.dl.callbacks import EarlyStoppingCallback, AccuracyCallback\n\n# experiment setup\nnum_epochs = NUM_EPOCHS\nlogdir = ""./logs/cifar_simple_notebook_4""\n\n# model, criterion, optimizer, scheduler\nmodel = Net()\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters())\nscheduler = torch.optim.lr_scheduler.MultiStepLR(\n    optimizer, milestones=[3, 8], gamma=0.3\n)\n\n# model runner\nrunner = SupervisedRunner()\n\n# model training\nrunner.train(\n    model=model,\n    criterion=criterion,\n    optimizer=optimizer,\n    scheduler=scheduler,\n    loaders=loaders,\n    callbacks=[\n        AccuracyCallback(accuracy_args=[1, 3, 5]),\n        EarlyStoppingCallback(patience=2, min_delta=0.01),\n    ],\n    logdir=logdir,\n    num_epochs=num_epochs,\n    check=True,\n)\n\n# In[ ]:\n\n# utils.plot_metrics(\n#     logdir=logdir,\n#     metrics=[""loss"", ""accuracy01"", ""accuracy03"", ""_base/lr""])\n\n# # Setup 5 - training with 1cycle\n\n# In[ ]:\n\nfrom catalyst.dl.runner import SupervisedRunner\nfrom catalyst.dl.callbacks import EarlyStoppingCallback, AccuracyCallback\nfrom catalyst.contrib.nn.schedulers import OneCycleLRWithWarmup\n\n# experiment setup\nnum_epochs = NUM_EPOCHS\nlogdir = ""./logs/cifar_simple_notebook_5""\n\n# model, criterion, optimizer, scheduler\nmodel = Net()\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters())\nscheduler = OneCycleLRWithWarmup(\n    optimizer,\n    num_steps=num_epochs,\n    lr_range=(0.005, 0.00005),\n    warmup_steps=2,\n    momentum_range=(0.85, 0.95),\n)\n\n# model runner\nrunner = SupervisedRunner()\n\n# model training\nrunner.train(\n    model=model,\n    criterion=criterion,\n    optimizer=optimizer,\n    scheduler=scheduler,\n    loaders=loaders,\n    callbacks=[\n        AccuracyCallback(accuracy_args=[1, 3, 5]),\n        EarlyStoppingCallback(patience=2, min_delta=0.01),\n    ],\n    logdir=logdir,\n    num_epochs=num_epochs,\n    check=True,\n)\n\n# In[ ]:\n\n# utils.plot_metrics(\n#     logdir=logdir,\n#     step=""batch"",\n#     metrics=[""loss"", ""accuracy01"", ""_base/lr"", ""_base/momentum""])\n\n# # Setup 6 - training without validation\n\n# In[ ]:\n\nfrom catalyst.dl.runner import SupervisedRunner\nfrom catalyst.dl.callbacks import EarlyStoppingCallback, AccuracyCallback\n\n# experiment setup\nnum_epochs = NUM_EPOCHS\nlogdir = ""./logs/cifar_simple_notebook_6""\n\n# model, criterion, optimizer, scheduler\nmodel = Net()\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters())\nscheduler = torch.optim.lr_scheduler.MultiStepLR(\n    optimizer, milestones=[3, 8], gamma=0.3\n)\n\n# model runner\nrunner = SupervisedRunner()\n\n# model training\nrunner.train(\n    model=model,\n    criterion=criterion,\n    optimizer=optimizer,\n    scheduler=scheduler,\n    loaders={""train"": loaders[""train""]},\n    valid_loader=""train"",\n    callbacks=[AccuracyCallback(accuracy_args=[1, 3, 5]),],\n    logdir=logdir,\n    num_epochs=num_epochs,\n    check=True,\n)\n\n# In[ ]:\n\n# utils.plot_metrics(logdir=logdir, step=""epoch"", metrics=[""loss"", ""accuracy01""])\n\n# # Setup 7 - pipeline check\n\n# In[ ]:\n\nfrom catalyst.dl.runner import SupervisedRunner\n\n# experiment setup\nnum_epochs = NUM_EPOCHS\nlogdir = ""./logs/cifar_simple_notebook_7""\n\n# model, criterion, optimizer, scheduler\nmodel = Net()\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters())\nscheduler = torch.optim.lr_scheduler.MultiStepLR(\n    optimizer, milestones=[3, 8], gamma=0.3\n)\n\n# model runner\nrunner = SupervisedRunner()\n\n# model training\nrunner.train(\n    model=model,\n    criterion=criterion,\n    optimizer=optimizer,\n    scheduler=scheduler,\n    loaders=loaders,\n    logdir=logdir,\n    num_epochs=num_epochs,\n    check=True,  # here is the trick\n)\n\n# # Setup 8 - multi-stage training\n\n# In[ ]:\n\nfrom catalyst.dl.runner import SupervisedRunner\nfrom catalyst.dl.callbacks import EarlyStoppingCallback, AccuracyCallback\n\n# experiment setup\nnum_epochs = NUM_EPOCHS\nlogdir = ""./logs/cifar_simple_notebook_8""\n\n# model, criterion, optimizer, scheduler\nmodel = Net()\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters())\nscheduler = torch.optim.lr_scheduler.MultiStepLR(\n    optimizer, milestones=[3, 8], gamma=0.3\n)\n\n# model runner\nrunner = SupervisedRunner()\n\n# model training - 1\nrunner.train(\n    model=model,\n    criterion=criterion,\n    optimizer=optimizer,\n    scheduler=scheduler,\n    loaders=loaders,\n    callbacks=[\n        AccuracyCallback(accuracy_args=[1, 3, 5]),\n        EarlyStoppingCallback(patience=2, min_delta=0.01),\n    ],\n    logdir=logdir,\n    num_epochs=num_epochs,\n    check=True,\n)\n\n# model training - 2\nnum_epochs = NUM_EPOCHS\nlogdir = ""./logs/cifar_simple_notebook_8""\noptimizer = torch.optim.SGD(model.parameters(), lr=0.05)\n\nrunner.train(\n    model=model,\n    criterion=criterion,\n    optimizer=optimizer,\n    loaders=loaders,\n    logdir=logdir,\n    num_epochs=num_epochs,\n    load_best_on_end=True,\n)\n\n# # Setup 9 - predict_loader\n\n# In[ ]:\n\nrunner_out = runner.predict_loader(model=model, loader=loaders[""valid""],)\n\n# In[ ]:\n\nnext(runner_out)[runner.output_key].shape\n\n# # Setup 10 - predict batch\n\n# In[ ]:\n\nfeatures, targets = next(iter(loaders[""valid""]))\n\n# In[ ]:\n\nfeatures.shape\n\n# In[ ]:\n\nrunner_in = {runner.input_key: features}\nrunner_out = runner.predict_batch(runner_in)\n\n# In[ ]:\n\nrunner_out[runner.output_key].shape\n\n# In[ ]:\n'"
tests/_tests_scripts/dl_z_contirb_functional.py,0,"b'#!/usr/bin/env python\n# coding: utf-8\n# flake8: noqa\nimport os\nimport sys\n\nfrom catalyst.contrib.models import get_convolution_net, get_linear_net\n\nif os.getenv(""USE_APEX"", ""0"") != ""0"" or os.getenv(""USE_DDP"", ""0"") != ""0"":\n    sys.exit()\n\n\nnet = get_linear_net(\n    in_features=32,\n    features=[128, 64, 64],\n    use_bias=[True, False, False],\n    normalization=[None, ""BatchNorm1d"", ""LayerNorm""],\n    dropout_rate=[None, 0.5, 0.8],\n    activation=[None, ""ReLU"", {""module"": ""ELU"", ""alpha"": 0.5}],\n    residual=""soft"",\n)\n\nprint(net)\n\nnet = get_convolution_net(\n    in_channels=3,\n    channels=[128, 64, 64],\n    kernel_sizes=[8, 4, 3],\n    strides=[4, 2, 1],\n    groups=[1, 2, 2],\n    use_bias=[True, False, False],\n    normalization=[None, ""BatchNorm2d"", ""BatchNorm2d""],\n    dropout_rate=[None, 0.5, 0.8],\n    activation=[None, ""ReLU"", {""module"": ""ELU"", ""alpha"": 0.5}],\n    residual=""soft"",\n)\n\nprint(net)\n'"
tests/_tests_scripts/dl_z_distributed_01.py,6,"b'# flake8: noqa\nimport os\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\n\nfrom catalyst import dl\n\n\nclass Projector(nn.Module):\n    def __init__(self, input_size: int):\n        super().__init__()\n        self.linear = nn.Linear(input_size, 1)\n\n    def forward(self, X: torch.Tensor) -> torch.Tensor:\n        return self.linear(X).squeeze(-1)\n\n\ndef main():\n    X = torch.rand(int(1e4), 10)\n    y = torch.rand(X.shape[0])\n    model = Projector(X.shape[1])\n    dataset = TensorDataset(X, y)\n    loader = DataLoader(dataset, batch_size=32, num_workers=1)\n\n    # example  1 - typical training\n    runner = dl.SupervisedRunner()\n    runner.train(\n        model=model,\n        loaders={""train"": loader, ""valid"": loader},\n        # datasets={\n        #     ""batch_size"": 32,\n        #     ""num_workers"": 1,\n        #     ""train"": dataset,\n        #     ""valid"": dataset,\n        # },\n        criterion=nn.MSELoss(),\n        optimizer=optim.Adam(model.parameters()),\n        logdir=""logs/log_example_01"",\n        num_epochs=10,\n        verbose=True,\n        check=True,\n        fp16=False,\n        distributed=False,\n    )\n\n\nif __name__ == ""__main__"":\n    if os.getenv(""USE_APEX"", ""0"") == ""0"" and os.getenv(""USE_DDP"", ""0"") == ""0"":\n        main()\n'"
tests/_tests_scripts/dl_z_distributed_02.py,6,"b'# flake8: noqa\nimport os\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import TensorDataset\n\nfrom catalyst import dl\n\n\nclass Projector(nn.Module):\n    def __init__(self, input_size: int):\n        super().__init__()\n        self.linear = nn.Linear(input_size, 1)\n\n    def forward(self, X: torch.Tensor) -> torch.Tensor:\n        return self.linear(X).squeeze(-1)\n\n\ndef main():\n    X = torch.rand(int(1e4), 10)\n    y = torch.rand(X.shape[0])\n    model = Projector(X.shape[1])\n    dataset = TensorDataset(X, y)\n    # loader = DataLoader(dataset, batch_size=32, num_workers=1)\n\n    # example  2 - typical training with datasets preparation\n    runner = dl.SupervisedRunner()\n    runner.train(\n        model=model,\n        # loaders={""train"": loader, ""valid"": loader},\n        datasets={\n            ""batch_size"": 32,\n            ""num_workers"": 1,\n            ""train"": dataset,\n            ""valid"": dataset,\n        },\n        criterion=nn.MSELoss(),\n        optimizer=optim.Adam(model.parameters()),\n        logdir=""logs/log_example_02"",\n        num_epochs=10,\n        verbose=True,\n        check=True,\n        fp16=False,\n        distributed=False,\n    )\n\n\nif __name__ == ""__main__"":\n    if os.getenv(""USE_APEX"", ""0"") == ""0"" and os.getenv(""USE_DDP"", ""0"") == ""0"":\n        main()\n'"
tests/_tests_scripts/dl_z_distributed_03.py,6,"b'# flake8: noqa\nimport os\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\n\nfrom catalyst import dl\n\n\nclass Projector(nn.Module):\n    def __init__(self, input_size: int):\n        super().__init__()\n        self.linear = nn.Linear(input_size, 1)\n\n    def forward(self, X: torch.Tensor) -> torch.Tensor:\n        return self.linear(X).squeeze(-1)\n\n\ndef main():\n    X = torch.rand(int(1e4), 10)\n    y = torch.rand(X.shape[0])\n    model = Projector(X.shape[1])\n    dataset = TensorDataset(X, y)\n    loader = DataLoader(dataset, batch_size=32, num_workers=1)\n\n    # example  3 - distibuted training with backward loaders compatibility\n    runner = dl.SupervisedRunner()\n    runner.train(\n        model=model,\n        loaders={""train"": loader, ""valid"": loader},\n        # datasets={\n        #     ""batch_size"": 32,\n        #     ""num_workers"": 1,\n        #     ""train"": dataset,\n        #     ""valid"": dataset,\n        # },\n        criterion=nn.MSELoss(),\n        optimizer=optim.Adam(model.parameters()),\n        logdir=""logs/log_example_03"",\n        num_epochs=10,\n        verbose=True,\n        check=True,\n        fp16=False,\n        distributed=True,\n    )\n\n\nif __name__ == ""__main__"":\n    if os.getenv(""USE_APEX"", ""0"") == ""0"" and os.getenv(""USE_DDP"", ""0"") == ""1"":\n        main()\n'"
tests/_tests_scripts/dl_z_distributed_04.py,6,"b'# flake8: noqa\nimport os\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import TensorDataset\n\nfrom catalyst import dl\n\n\nclass Projector(nn.Module):\n    def __init__(self, input_size: int):\n        super().__init__()\n        self.linear = nn.Linear(input_size, 1)\n\n    def forward(self, X: torch.Tensor) -> torch.Tensor:\n        return self.linear(X).squeeze(-1)\n\n\ndef main():\n    X = torch.rand(int(1e4), 10)\n    y = torch.rand(X.shape[0])\n    model = Projector(X.shape[1])\n    dataset = TensorDataset(X, y)\n    # loader = DataLoader(dataset, batch_size=32, num_workers=1)\n\n    # example  4 - distibuted training with datasets preparation\n    runner = dl.SupervisedRunner()\n    runner.train(\n        model=model,\n        # loaders={""train"": loader, ""valid"": loader},\n        datasets={\n            ""batch_size"": 32,\n            ""num_workers"": 1,\n            ""train"": dataset,\n            ""valid"": dataset,\n        },\n        criterion=nn.MSELoss(),\n        optimizer=optim.Adam(model.parameters()),\n        logdir=""logs/log_example_04"",\n        num_epochs=10,\n        verbose=True,\n        check=True,\n        fp16=False,\n        distributed=True,\n    )\n\n\nif __name__ == ""__main__"":\n    if os.getenv(""USE_APEX"", ""0"") == ""0"" and os.getenv(""USE_DDP"", ""0"") == ""1"":\n        main()\n'"
tests/_tests_scripts/dl_z_distributed_05.py,6,"b'# flake8: noqa\nimport os\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\n\nfrom catalyst import dl\n\n\nclass Projector(nn.Module):\n    def __init__(self, input_size: int):\n        super().__init__()\n        self.linear = nn.Linear(input_size, 1)\n\n    def forward(self, X: torch.Tensor) -> torch.Tensor:\n        return self.linear(X).squeeze(-1)\n\n\ndef main():\n    X = torch.rand(int(1e4), 10)\n    y = torch.rand(X.shape[0])\n    model = Projector(X.shape[1])\n    dataset = TensorDataset(X, y)\n    loader = DataLoader(dataset, batch_size=32, num_workers=1)\n\n    # example  5 - distibuted training with backward loaders compatibility\n    # and fp16 support\n    runner = dl.SupervisedRunner()\n    runner.train(\n        model=model,\n        loaders={""train"": loader, ""valid"": loader},\n        # datasets={\n        #     ""batch_size"": 32,\n        #     ""num_workers"": 1,\n        #     ""train"": dataset,\n        #     ""valid"": dataset,\n        # },\n        criterion=nn.MSELoss(),\n        optimizer=optim.Adam(model.parameters()),\n        logdir=""logs/log_example_05"",\n        num_epochs=10,\n        verbose=True,\n        check=True,\n        fp16=True,\n        distributed=True,\n    )\n\n\nif __name__ == ""__main__"":\n    if os.getenv(""USE_APEX"", ""0"") == ""1"" and os.getenv(""USE_DDP"", ""0"") == ""1"":\n        main()\n'"
tests/_tests_scripts/dl_z_distributed_06.py,6,"b'# flake8: noqa\nimport os\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import TensorDataset\n\nfrom catalyst import dl\n\n\nclass Projector(nn.Module):\n    def __init__(self, input_size: int):\n        super().__init__()\n        self.linear = nn.Linear(input_size, 1)\n\n    def forward(self, X: torch.Tensor) -> torch.Tensor:\n        return self.linear(X).squeeze(-1)\n\n\ndef main():\n    X = torch.rand(int(1e4), 10)\n    y = torch.rand(X.shape[0])\n    model = Projector(X.shape[1])\n    dataset = TensorDataset(X, y)\n    # loader = DataLoader(dataset, batch_size=32, num_workers=1)\n\n    # example  6 - distibuted training with datasets preparation\n    # and fp16 support\n    runner = dl.SupervisedRunner()\n    runner.train(\n        model=model,\n        # loaders={""train"": loader, ""valid"": loader},\n        datasets={\n            ""batch_size"": 32,\n            ""num_workers"": 1,\n            ""train"": dataset,\n            ""valid"": dataset,\n        },\n        criterion=nn.MSELoss(),\n        optimizer=optim.Adam(model.parameters()),\n        logdir=""logs/log_example_06"",\n        num_epochs=10,\n        verbose=True,\n        check=True,\n        fp16=True,\n        distributed=True,\n    )\n\n\nif __name__ == ""__main__"":\n    if os.getenv(""USE_APEX"", ""0"") == ""1"" and os.getenv(""USE_DDP"", ""0"") == ""1"":\n        main()\n'"
tests/_tests_scripts/dl_z_distributed_07.py,6,"b'# flake8: noqa\nimport os\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\n\nfrom catalyst import dl, utils\n\n\nclass Projector(nn.Module):\n    def __init__(self, input_size: int):\n        super().__init__()\n        self.linear = nn.Linear(input_size, 1)\n\n    def forward(self, X: torch.Tensor) -> torch.Tensor:\n        return self.linear(X).squeeze(-1)\n\n\n# example  7 - distibuted training with backward loaders compatibility\n# and utils.distributed_cmd_run\ndef train():\n    X = torch.rand(int(1e4), 10)\n    y = torch.rand(X.shape[0])\n    model = Projector(X.shape[1])\n    dataset = TensorDataset(X, y)\n    loader = DataLoader(dataset, batch_size=32, num_workers=1)\n\n    runner = dl.SupervisedRunner()\n    runner.train(\n        model=model,\n        loaders={""train"": loader, ""valid"": loader},\n        # datasets={\n        #     ""batch_size"": 32,\n        #     ""num_workers"": 1,\n        #     ""train"": dataset,\n        #     ""valid"": dataset,\n        # },\n        criterion=nn.MSELoss(),\n        optimizer=optim.Adam(model.parameters()),\n        logdir=""logs/log_example_07"",\n        num_epochs=10,\n        verbose=True,\n        check=True,\n        fp16=False,\n        distributed=False,\n    )\n\n\ndef main():\n    utils.distributed_cmd_run(train)\n\n\nif __name__ == ""__main__"":\n    if os.getenv(""USE_APEX"", ""0"") == ""0"" and os.getenv(""USE_DDP"", ""0"") == ""1"":\n        main()\n'"
tests/_tests_scripts/dl_z_distributed_08.py,6,"b'# flake8: noqa\nimport os\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import TensorDataset\n\nfrom catalyst import dl, utils\n\n\nclass Projector(nn.Module):\n    def __init__(self, input_size: int):\n        super().__init__()\n        self.linear = nn.Linear(input_size, 1)\n\n    def forward(self, X: torch.Tensor) -> torch.Tensor:\n        return self.linear(X).squeeze(-1)\n\n\n# example  7 - distibuted training  datasets preparation\n# and utils.distributed_cmd_run\ndef train():\n    X = torch.rand(int(1e4), 10)\n    y = torch.rand(X.shape[0])\n    model = Projector(X.shape[1])\n    dataset = TensorDataset(X, y)\n    # loader = DataLoader(dataset, batch_size=32, num_workers=1)\n\n    runner = dl.SupervisedRunner()\n    runner.train(\n        model=model,\n        # loaders={""train"": loader, ""valid"": loader},\n        datasets={\n            ""batch_size"": 32,\n            ""num_workers"": 1,\n            ""train"": dataset,\n            ""valid"": dataset,\n        },\n        criterion=nn.MSELoss(),\n        optimizer=optim.Adam(model.parameters()),\n        logdir=""logs/log_example_08"",\n        num_epochs=10,\n        verbose=True,\n        check=True,\n        fp16=False,\n        distributed=False,\n    )\n\n\ndef main():\n    utils.distributed_cmd_run(train)\n\n\nif __name__ == ""__main__"":\n    if os.getenv(""USE_APEX"", ""0"") == ""0"" and os.getenv(""USE_DDP"", ""0"") == ""1"":\n        main()\n'"
tests/_tests_scripts/dl_z_distributed_09.py,6,"b'# flake8: noqa\nimport os\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\n\nfrom catalyst import dl, utils\n\n\nclass Projector(nn.Module):\n    def __init__(self, input_size: int):\n        super().__init__()\n        self.linear = nn.Linear(input_size, 1)\n\n    def forward(self, X: torch.Tensor) -> torch.Tensor:\n        return self.linear(X).squeeze(-1)\n\n\n# example  9 - distibuted training with backward loaders compatibility\n# and fp16 support\n# and utils.distributed_cmd_run\ndef train():\n    X = torch.rand(int(1e4), 10)\n    y = torch.rand(X.shape[0])\n    model = Projector(X.shape[1])\n    dataset = TensorDataset(X, y)\n    loader = DataLoader(dataset, batch_size=32, num_workers=1)\n\n    runner = dl.SupervisedRunner()\n    runner.train(\n        model=model,\n        loaders={""train"": loader, ""valid"": loader},\n        # datasets={\n        #     ""batch_size"": 32,\n        #     ""num_workers"": 1,\n        #     ""train"": dataset,\n        #     ""valid"": dataset,\n        # },\n        criterion=nn.MSELoss(),\n        optimizer=optim.Adam(model.parameters()),\n        logdir=""logs/log_example_09"",\n        num_epochs=10,\n        verbose=True,\n        check=True,\n        fp16=True,\n        distributed=False,\n    )\n\n\ndef main():\n    utils.distributed_cmd_run(train)\n\n\nif __name__ == ""__main__"":\n    if os.getenv(""USE_APEX"", ""0"") == ""1"" and os.getenv(""USE_DDP"", ""0"") == ""1"":\n        main()\n'"
tests/_tests_scripts/dl_z_distributed_10.py,6,"b'# flake8: noqa\nimport os\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import TensorDataset\n\nfrom catalyst import dl, utils\n\n\nclass Projector(nn.Module):\n    def __init__(self, input_size: int):\n        super().__init__()\n        self.linear = nn.Linear(input_size, 1)\n\n    def forward(self, X: torch.Tensor) -> torch.Tensor:\n        return self.linear(X).squeeze(-1)\n\n\n# example  10 - distibuted training with datasets preparation\n# and fp16 support\n# and utils.distributed_cmd_run\ndef train():\n    X = torch.rand(int(1e4), 10)\n    y = torch.rand(X.shape[0])\n    model = Projector(X.shape[1])\n    dataset = TensorDataset(X, y)\n    # loader = DataLoader(dataset, batch_size=32, num_workers=1)\n\n    runner = dl.SupervisedRunner()\n    runner.train(\n        model=model,\n        # loaders={""train"": loader, ""valid"": loader},\n        datasets={\n            ""batch_size"": 32,\n            ""num_workers"": 1,\n            ""train"": dataset,\n            ""valid"": dataset,\n        },\n        criterion=nn.MSELoss(),\n        optimizer=optim.Adam(model.parameters()),\n        logdir=""logs/log_example_10"",\n        num_epochs=10,\n        verbose=True,\n        check=True,\n        fp16=True,\n        distributed=False,\n    )\n\n\ndef main():\n    utils.distributed_cmd_run(train)\n\n\nif __name__ == ""__main__"":\n    if os.getenv(""USE_APEX"", ""0"") == ""1"" and os.getenv(""USE_DDP"", ""0"") == ""1"":\n        main()\n'"
tests/_tests_scripts/dl_z_distributed_11.py,6,"b'# flake8: noqa\nimport os\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import TensorDataset\n\nfrom catalyst import dl\n\n\nclass Projector(nn.Module):\n    def __init__(self, input_size: int):\n        super().__init__()\n        self.linear = nn.Linear(input_size, 1)\n\n    def forward(self, X: torch.Tensor) -> torch.Tensor:\n        return self.linear(X).squeeze(-1)\n\n\ndef datasets_fn(num_features: int):\n    """"""\n    Datasets closure.\n\n    Args:\n        num_features: number of features for dataset creation.\n    """"""\n    X = torch.rand(int(1e4), num_features)\n    y = torch.rand(X.shape[0])\n    dataset = TensorDataset(X, y)\n    return {""train"": dataset, ""valid"": dataset}\n\n\ndef main():\n    num_features = 10\n    model = Projector(num_features)\n\n    # example 11 - typical training with datasets closure\n    runner = dl.SupervisedRunner()\n    runner.train(\n        model=model,\n        # loaders={""train"": loader, ""valid"": loader},\n        datasets={\n            ""batch_size"": 32,\n            ""num_workers"": 1,\n            ""get_datasets_fn"": datasets_fn,\n            ""num_features"": num_features,\n        },\n        criterion=nn.MSELoss(),\n        optimizer=optim.Adam(model.parameters()),\n        logdir=""logs/log_example_11"",\n        num_epochs=10,\n        verbose=True,\n        check=True,\n        fp16=False,\n        distributed=False,\n    )\n\n\nif __name__ == ""__main__"":\n    if os.getenv(""USE_APEX"", ""0"") == ""0"" and os.getenv(""USE_DDP"", ""0"") == ""0"":\n        main()\n'"
tests/_tests_scripts/dl_z_distributed_12.py,6,"b'# flake8: noqa\nimport os\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import TensorDataset\n\nfrom catalyst import dl\n\n\nclass Projector(nn.Module):\n    def __init__(self, input_size: int):\n        super().__init__()\n        self.linear = nn.Linear(input_size, 1)\n\n    def forward(self, X: torch.Tensor) -> torch.Tensor:\n        return self.linear(X).squeeze(-1)\n\n\ndef datasets_fn(num_features: int):\n    """"""\n    Datasets closure.\n\n    Args:\n        num_features: number of features for dataset creation.\n    """"""\n    X = torch.rand(int(1e4), num_features)\n    y = torch.rand(X.shape[0])\n    dataset = TensorDataset(X, y)\n    return {""train"": dataset, ""valid"": dataset}\n\n\ndef main():\n    num_features = 10\n    model = Projector(num_features)\n\n    # example  12 - distibuted training with datasets closure\n    runner = dl.SupervisedRunner()\n    runner.train(\n        model=model,\n        # loaders={""train"": loader, ""valid"": loader},\n        datasets={\n            ""batch_size"": 32,\n            ""num_workers"": 1,\n            ""get_datasets_fn"": datasets_fn,\n            ""num_features"": num_features,\n        },\n        criterion=nn.MSELoss(),\n        optimizer=optim.Adam(model.parameters()),\n        logdir=""logs/log_example_12"",\n        num_epochs=10,\n        verbose=True,\n        check=True,\n        fp16=False,\n        distributed=True,\n    )\n\n\nif __name__ == ""__main__"":\n    if os.getenv(""USE_APEX"", ""0"") == ""0"" and os.getenv(""USE_DDP"", ""0"") == ""1"":\n        main()\n'"
tests/_tests_scripts/dl_z_distributed_13.py,6,"b'# flake8: noqa\nimport os\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import TensorDataset\n\nfrom catalyst import dl\n\n\nclass Projector(nn.Module):\n    def __init__(self, input_size: int):\n        super().__init__()\n        self.linear = nn.Linear(input_size, 1)\n\n    def forward(self, X: torch.Tensor) -> torch.Tensor:\n        return self.linear(X).squeeze(-1)\n\n\ndef datasets_fn(num_features: int):\n    """"""\n    Datasets closure.\n\n    Args:\n        num_features: number of features for dataset creation.\n    """"""\n    X = torch.rand(int(1e4), num_features)\n    y = torch.rand(X.shape[0])\n    dataset = TensorDataset(X, y)\n    return {""train"": dataset, ""valid"": dataset}\n\n\ndef main():\n    num_features = 10\n    model = Projector(num_features)\n\n    # example 13 - distibuted training with datasets closure\n    # and fp16 support\n    runner = dl.SupervisedRunner()\n    runner.train(\n        model=model,\n        # loaders={""train"": loader, ""valid"": loader},\n        datasets={\n            ""batch_size"": 32,\n            ""num_workers"": 1,\n            ""get_datasets_fn"": datasets_fn,\n            ""num_features"": num_features,\n        },\n        criterion=nn.MSELoss(),\n        optimizer=optim.Adam(model.parameters()),\n        logdir=""logs/log_example_13"",\n        num_epochs=10,\n        verbose=True,\n        check=True,\n        fp16=True,\n        distributed=True,\n    )\n\n\nif __name__ == ""__main__"":\n    if os.getenv(""USE_APEX"", ""0"") == ""1"" and os.getenv(""USE_DDP"", ""0"") == ""1"":\n        main()\n'"
tests/_tests_scripts/dl_z_distributed_14.py,6,"b'# flake8: noqa\nimport os\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import TensorDataset\n\nfrom catalyst import dl, utils\n\n\nclass Projector(nn.Module):\n    def __init__(self, input_size: int):\n        super().__init__()\n        self.linear = nn.Linear(input_size, 1)\n\n    def forward(self, X: torch.Tensor) -> torch.Tensor:\n        return self.linear(X).squeeze(-1)\n\n\ndef datasets_fn(num_features: int):\n    """"""\n    Datasets closure.\n\n    Args:\n        num_features: number of features for dataset creation.\n    """"""\n    X = torch.rand(int(1e4), num_features)\n    y = torch.rand(X.shape[0])\n    dataset = TensorDataset(X, y)\n    return {""train"": dataset, ""valid"": dataset}\n\n\n# example 14 - distibuted training with datasets closure\n# and utils.distributed_cmd_run\ndef train():\n    num_features = 10\n    model = Projector(num_features)\n\n    runner = dl.SupervisedRunner()\n    runner.train(\n        model=model,\n        # loaders={""train"": loader, ""valid"": loader},\n        datasets={\n            ""batch_size"": 32,\n            ""num_workers"": 1,\n            ""get_datasets_fn"": datasets_fn,\n            ""num_features"": num_features,\n        },\n        criterion=nn.MSELoss(),\n        optimizer=optim.Adam(model.parameters()),\n        logdir=""logs/log_example_14"",\n        num_epochs=10,\n        verbose=True,\n        check=True,\n        fp16=False,\n        distributed=False,\n    )\n\n\ndef main():\n    utils.distributed_cmd_run(train)\n\n\nif __name__ == ""__main__"":\n    if os.getenv(""USE_APEX"", ""0"") == ""0"" and os.getenv(""USE_DDP"", ""0"") == ""1"":\n        main()\n'"
tests/_tests_scripts/dl_z_distributed_15.py,6,"b'# flake8: noqa\nimport os\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import TensorDataset\n\nfrom catalyst import dl, utils\n\n\nclass Projector(nn.Module):\n    def __init__(self, input_size: int):\n        super().__init__()\n        self.linear = nn.Linear(input_size, 1)\n\n    def forward(self, X: torch.Tensor) -> torch.Tensor:\n        return self.linear(X).squeeze(-1)\n\n\ndef datasets_fn(num_features: int):\n    """"""\n    Datasets closure.\n\n    Args:\n        num_features: number of features for dataset creation.\n    """"""\n    X = torch.rand(int(1e4), num_features)\n    y = torch.rand(X.shape[0])\n    dataset = TensorDataset(X, y)\n    return {""train"": dataset, ""valid"": dataset}\n\n\n# example 14 - distibuted training with datasets closure\n# and fp16 support\n# and utils.distributed_cmd_run\ndef train():\n    num_features = 10\n    model = Projector(num_features)\n\n    runner = dl.SupervisedRunner()\n    runner.train(\n        model=model,\n        # loaders={""train"": loader, ""valid"": loader},\n        datasets={\n            ""batch_size"": 32,\n            ""num_workers"": 1,\n            ""get_datasets_fn"": datasets_fn,\n            ""num_features"": num_features,\n        },\n        criterion=nn.MSELoss(),\n        optimizer=optim.Adam(model.parameters()),\n        logdir=""logs/log_example_15"",\n        num_epochs=10,\n        verbose=True,\n        check=True,\n        fp16=True,\n        distributed=False,\n    )\n\n\ndef main():\n    utils.distributed_cmd_run(train)\n\n\nif __name__ == ""__main__"":\n    if os.getenv(""USE_APEX"", ""0"") == ""1"" and os.getenv(""USE_DDP"", ""0"") == ""1"":\n        main()\n'"
tests/_tests_scripts/dl_z_distributed_16.py,6,"b'# flake8: noqa\nimport os\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import TensorDataset\n\nfrom catalyst import dl, utils\n\n\nclass Projector(nn.Module):\n    def __init__(self, input_size: int):\n        super().__init__()\n        self.linear = nn.Linear(input_size, 1)\n\n    def forward(self, X: torch.Tensor) -> torch.Tensor:\n        return self.linear(X).squeeze(-1)\n\n\ndef datasets_fn(num_features: int):\n    """"""\n    Datasets closure.\n\n    Args:\n        num_features: number of features for dataset creation.\n    """"""\n    X = torch.rand(int(1e4), num_features)\n    y = torch.rand(X.shape[0])\n    dataset = TensorDataset(X, y)\n    return {""train"": dataset, ""valid"": dataset}\n\n\n# example 14 - distibuted training with datasets closure\n# and utils.distributed_cmd_run\ndef train():\n    num_features = 10\n    model = Projector(num_features)\n\n    runner = dl.SupervisedRunner()\n    runner.train(\n        model=model,\n        # loaders={""train"": loader, ""valid"": loader},\n        datasets={\n            ""batch_size"": 32,\n            ""num_workers"": 1,\n            ""get_datasets_fn"": datasets_fn,\n            ""num_features"": num_features,\n        },\n        criterion=nn.MSELoss(),\n        optimizer=optim.Adam(model.parameters()),\n        logdir=""logs/log_example_16"",\n        num_epochs=10,\n        verbose=True,\n        check=True,\n        fp16=False,\n        distributed=True,  # we should not get an error here, only warning\n    )\n\n\ndef main():\n    utils.distributed_cmd_run(train)\n\n\nif __name__ == ""__main__"":\n    if os.getenv(""USE_APEX"", ""0"") == ""0"" and os.getenv(""USE_DDP"", ""0"") == ""1"":\n        main()\n'"
tests/_tests_scripts/dl_z_distributed_17.py,6,"b'# flake8: noqa\nimport os\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import TensorDataset\n\nfrom catalyst import dl, utils\n\n\nclass Projector(nn.Module):\n    def __init__(self, input_size: int):\n        super().__init__()\n        self.linear = nn.Linear(input_size, 1)\n\n    def forward(self, X: torch.Tensor) -> torch.Tensor:\n        return self.linear(X).squeeze(-1)\n\n\ndef datasets_fn(num_features: int):\n    """"""\n    Datasets closure.\n\n    Args:\n        num_features: number of features for dataset creation.\n    """"""\n    X = torch.rand(int(1e4), num_features)\n    y = torch.rand(X.shape[0])\n    dataset = TensorDataset(X, y)\n    return {""train"": dataset, ""valid"": dataset}\n\n\n# example 14 - distibuted training with datasets closure\n# and fp16 support\n# and utils.distributed_cmd_run\ndef train():\n    num_features = 10\n    model = Projector(num_features)\n\n    runner = dl.SupervisedRunner()\n    runner.train(\n        model=model,\n        # loaders={""train"": loader, ""valid"": loader},\n        datasets={\n            ""batch_size"": 32,\n            ""num_workers"": 1,\n            ""get_datasets_fn"": datasets_fn,\n            ""num_features"": num_features,\n        },\n        criterion=nn.MSELoss(),\n        optimizer=optim.Adam(model.parameters()),\n        logdir=""logs/log_example_17"",\n        num_epochs=10,\n        verbose=True,\n        check=True,\n        fp16=True,\n        distributed=True,  # we should not get an error here, only warning\n    )\n\n\ndef main():\n    utils.distributed_cmd_run(train)\n\n\nif __name__ == ""__main__"":\n    if os.getenv(""USE_APEX"", ""0"") == ""1"" and os.getenv(""USE_DDP"", ""0"") == ""1"":\n        main()\n'"
tests/_tests_scripts/dl_z_docs_distributed_0.py,6,"b'# alias for https://catalyst-team.github.io/catalyst/info/distributed.html#prepare-your-script  # noqa: E501 W505\n# flake8: noqa\n# isort:skip_file\nimport os\nimport sys\n\n\nif os.getenv(""USE_APEX"", ""0"") != ""0"" or os.getenv(""USE_DDP"", ""0"") != ""0"":\n    sys.exit()\n\n\nimport torch\nfrom torch.utils.data import DataLoader, TensorDataset\n\nfrom catalyst.dl import SupervisedRunner\n\n# experiment setup\nlogdir = ""./logdir""\nnum_epochs = 8\n\n# data\nnum_samples, num_features = int(1e4), int(1e1)\nX, y = torch.rand(num_samples, num_features), torch.rand(num_samples)\ndataset = TensorDataset(X, y)\nloader = DataLoader(dataset, batch_size=32, num_workers=1)\nloaders = {""train"": loader, ""valid"": loader}\n\n# model, criterion, optimizer, scheduler\nmodel = torch.nn.Linear(num_features, 1)\ncriterion = torch.nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters())\nscheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, [3, 6])\n\n# model training\nrunner = SupervisedRunner()\nrunner.train(\n    model=model,\n    criterion=criterion,\n    optimizer=optimizer,\n    scheduler=scheduler,\n    loaders=loaders,\n    logdir=logdir,\n    num_epochs=num_epochs,\n    verbose=True,\n    check=True,\n)\n'"
tests/_tests_scripts/dl_z_docs_distributed_1.py,6,"b'# alias for https://catalyst-team.github.io/catalyst/info/distributed.html#stage-1-i-just-want-distributed # noqa: E501 W505\n# flake8: noqa\n# isort:skip_file\nimport os\nimport sys\n\n\nif os.getenv(""USE_APEX"", ""0"") != ""0"" or os.getenv(""USE_DDP"", ""0"") != ""1"":\n    sys.exit()\n\n\nimport torch\nfrom torch.utils.data import DataLoader, TensorDataset\n\nfrom catalyst.dl import SupervisedRunner\n\n# data\nnum_samples, num_features = int(1e4), int(1e1)\nX, y = torch.rand(num_samples, num_features), torch.rand(num_samples)\ndataset = TensorDataset(X, y)\nloader = DataLoader(dataset, batch_size=32, num_workers=1)\nloaders = {""train"": loader, ""valid"": loader}\n\n# model, criterion, optimizer, scheduler\nmodel = torch.nn.Linear(num_features, 1)\ncriterion = torch.nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters())\nscheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, [3, 6])\n\n# model training\nrunner = SupervisedRunner()\nrunner.train(\n    model=model,\n    criterion=criterion,\n    optimizer=optimizer,\n    scheduler=scheduler,\n    loaders=loaders,\n    logdir=""./logs/example_1"",\n    num_epochs=8,\n    verbose=True,\n    distributed=True,\n)\n'"
tests/_tests_scripts/dl_z_docs_distributed_2.py,7,"b'# alias for https://catalyst-team.github.io/catalyst/info/distributed.html#case-2-we-are-going-deeper # noqa: E501 W505\n# flake8: noqa\n# isort:skip_file\nimport os\nimport sys\n\n\nif os.getenv(""USE_APEX"", ""0"") != ""0"" or os.getenv(""USE_DDP"", ""0"") != ""1"":\n    sys.exit()\n\n\nimport torch\nfrom torch.utils.data import TensorDataset\n\nfrom catalyst.dl import SupervisedRunner\n\n# data\nnum_samples, num_features = int(1e4), int(1e1)\nX = torch.rand(int(1e4), num_features)\ny = torch.rand(X.shape[0])\ndataset = TensorDataset(X, y)\n\n# model, criterion, optimizer, scheduler\nmodel = torch.nn.Linear(num_features, 1)\ncriterion = torch.nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters())\nscheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, [3, 6])\n\nrunner = SupervisedRunner()\nrunner.train(\n    model=model,\n    datasets={\n        ""batch_size"": 32,\n        ""num_workers"": 1,\n        ""train"": dataset,\n        ""valid"": dataset,\n    },\n    criterion=criterion,\n    optimizer=optimizer,\n    logdir=""./logs/example_2"",\n    num_epochs=8,\n    verbose=True,\n    distributed=True,\n    check=True,\n)\n'"
tests/_tests_scripts/dl_z_docs_distributed_3.py,7,"b'# alias for https://catalyst-team.github.io/catalyst/info/distributed.html#case-3-best-practices-for-distributed-training # noqa: E501 W505\n# flake8: noqa\n# isort:skip_file\nimport os\nimport sys\n\n\nif os.getenv(""USE_APEX"", ""0"") != ""0"" or os.getenv(""USE_DDP"", ""0"") != ""1"":\n    sys.exit()\n\n\nimport torch\nfrom torch.utils.data import TensorDataset\n\nfrom catalyst.dl import SupervisedRunner, utils\n\n\ndef datasets_fn(num_features: int):\n    """"""\n    Docs.\n    """"""\n    X = torch.rand(int(1e4), num_features)\n    y = torch.rand(X.shape[0])\n    dataset = TensorDataset(X, y)\n    return {""train"": dataset, ""valid"": dataset}\n\n\ndef train():\n    """"""\n    Docs.\n    """"""\n    num_features = int(1e1)\n    # model, criterion, optimizer, scheduler\n    model = torch.nn.Linear(num_features, 1)\n    criterion = torch.nn.MSELoss()\n    optimizer = torch.optim.Adam(model.parameters())\n    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, [3, 6])\n\n    runner = SupervisedRunner()\n    runner.train(\n        model=model,\n        datasets={\n            ""batch_size"": 32,\n            ""num_workers"": 1,\n            ""get_datasets_fn"": datasets_fn,\n            ""num_features"": num_features,\n        },\n        criterion=criterion,\n        optimizer=optimizer,\n        scheduler=scheduler,\n        logdir=""./logs/example_3"",\n        num_epochs=8,\n        verbose=True,\n        distributed=False,\n        check=True,\n    )\n\n\nutils.distributed_cmd_run(train)\n'"
tests/_tests_scripts/dl_z_mvp_distributed_mnist_ae.py,2,"b'# flake8: noqa\nimport os\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nfrom catalyst import dl, utils\nfrom catalyst.contrib.data.transforms import ToTensor\nfrom catalyst.contrib.datasets import MNIST\nfrom catalyst.utils import metrics\n\n\nclass ClassifyAE(nn.Module):\n    """"""\n    Docs.\n    """"""\n\n    def __init__(self, in_features, hid_features, out_features):\n        """"""\n        Docs.\n        """"""\n        super().__init__()\n        self.encoder = nn.Sequential(\n            nn.Linear(in_features, hid_features), nn.Tanh()\n        )\n        self.decoder = nn.Sequential(\n            nn.Linear(hid_features, in_features), nn.Sigmoid()\n        )\n        self.clf = nn.Linear(hid_features, out_features)\n\n    def forward(self, x):\n        """"""\n        Docs.\n        """"""\n        z = self.encoder(x)\n        y_hat = self.clf(z)\n        x_ = self.decoder(z)\n        return y_hat, x_\n\n\nclass CustomRunner(dl.Runner):\n    """"""\n    Docs.\n    """"""\n\n    def _handle_batch(self, batch):\n        """"""\n        Docs.\n        """"""\n        x, y = batch\n        x = x.view(x.size(0), -1)\n        y_hat, x_ = self.model(x)\n        loss_clf = F.cross_entropy(y_hat, y)\n        loss_ae = F.mse_loss(x_, x)\n        loss = loss_clf + loss_ae\n        accuracy01, accuracy03, accuracy05 = metrics.accuracy(\n            y_hat, y, topk=(1, 3, 5)\n        )\n\n        self.batch_metrics = {\n            ""loss_clf"": loss_clf,\n            ""loss_ae"": loss_ae,\n            ""loss"": loss,\n            ""accuracy01"": accuracy01,\n            ""accuracy03"": accuracy03,\n            ""accuracy05"": accuracy05,\n        }\n\n        if self.is_train_loader:\n            loss.backward()\n            self.optimizer.step()\n            self.optimizer.zero_grad()\n\n\ndef datasets_fn():\n    """"""\n    Docs.\n    """"""\n    dataset = MNIST(\n        os.getcwd(), train=False, download=True, transform=ToTensor(),\n    )\n    return {""train"": dataset, ""valid"": dataset}\n\n\ndef train():\n    """"""\n    Docs.\n    """"""\n    # model, criterion, optimizer, scheduler\n    model = ClassifyAE(28 * 28, 128, 10)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.02)\n\n    runner = CustomRunner()\n    runner.train(\n        model=model,\n        optimizer=optimizer,\n        datasets={\n            ""batch_size"": 32,\n            ""num_workers"": 1,\n            ""get_datasets_fn"": datasets_fn,\n        },\n        logdir=""./logs/distributed_ae"",\n        num_epochs=8,\n        verbose=True,\n        check=True,\n    )\n\n\ndef main():\n    utils.distributed_cmd_run(train)\n\n\nif __name__ == ""__main__"":\n    if os.getenv(""USE_APEX"", ""0"") == ""0"" and os.getenv(""USE_DDP"", ""0"") == ""1"":\n        main()\n'"
tests/_tests_scripts/dl_z_mvp_mnist.py,4,"b'# flake8: noqa\nimport os\n\nimport torch\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader\n\nfrom catalyst import dl\nfrom catalyst.contrib.data.transforms import ToTensor\nfrom catalyst.contrib.datasets import MNIST\nfrom catalyst.utils import metrics\n\nmodel = torch.nn.Linear(28 * 28, 10)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.02)\n\nloaders = {\n    ""train"": DataLoader(\n        MNIST(os.getcwd(), train=True, download=True, transform=ToTensor()),\n        batch_size=32,\n    ),\n}\n\n\nclass CustomRunner(dl.Runner):\n    def predict_batch(self, batch):\n        # model inference step\n        return self.model(batch[0].to(self.device).view(batch[0].size(0), -1))\n\n    def _handle_batch(self, batch):\n        # model train/valid step\n        x, y = batch\n        y_hat = self.model(x.view(x.size(0), -1))\n\n        loss = F.cross_entropy(y_hat, y)\n        accuracy01, accuracy03 = metrics.accuracy(y_hat, y, topk=(1, 3))\n        self.batch_metrics.update(\n            {""loss"": loss, ""accuracy01"": accuracy01, ""accuracy03"": accuracy03}\n        )\n\n        if self.is_train_loader:\n            loss.backward()\n            self.optimizer.step()\n            self.optimizer.zero_grad()\n\n\ndef main():\n    runner = CustomRunner()\n    # model training\n    runner.train(\n        model=model,\n        optimizer=optimizer,\n        loaders=loaders,\n        logdir=""./logs"",\n        num_epochs=5,\n        verbose=True,\n        load_best_on_end=True,\n        check=True,\n    )\n    # model inference\n    for prediction in runner.predict_loader(loader=loaders[""train""]):\n        assert prediction.detach().cpu().numpy().shape[-1] == 10\n    # model tracing\n    traced_model = runner.trace(loader=loaders[""train""])\n\n\nif __name__ == ""__main__"":\n    if os.getenv(""USE_APEX"", ""0"") == ""0"" and os.getenv(""USE_DDP"", ""0"") == ""0"":\n        main()\n'"
tests/_tests_scripts/dl_z_mvp_mnist_ae.py,4,"b'# flake8: noqa\nimport os\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader\n\nfrom catalyst import dl\nfrom catalyst.contrib.data.transforms import ToTensor\nfrom catalyst.contrib.datasets import MNIST\nfrom catalyst.utils import metrics\n\n\nclass ClassifyAE(torch.nn.Module):\n    def __init__(self, in_features, hid_features, out_features):\n        super().__init__()\n        self.encoder = nn.Sequential(\n            nn.Linear(in_features, hid_features), nn.Tanh()\n        )\n        self.decoder = nn.Sequential(\n            nn.Linear(hid_features, in_features), nn.Sigmoid()\n        )\n        self.clf = nn.Linear(hid_features, out_features)\n\n    def forward(self, x):\n        z = self.encoder(x)\n        y_hat = self.clf(z)\n        x_ = self.decoder(z)\n        return y_hat, x_\n\n\nclass CustomRunner(dl.Runner):\n    def _handle_batch(self, batch):\n        x, y = batch\n        x = x.view(x.size(0), -1)\n        y_hat, x_ = self.model(x)\n        loss_clf = F.cross_entropy(y_hat, y)\n        loss_ae = F.mse_loss(x_, x)\n        loss = loss_clf + loss_ae\n        accuracy01, accuracy03, accuracy05 = metrics.accuracy(\n            y_hat, y, topk=(1, 3, 5)\n        )\n\n        self.batch_metrics = {\n            ""loss_clf"": loss_clf,\n            ""loss_ae"": loss_ae,\n            ""loss"": loss,\n            ""accuracy01"": accuracy01,\n            ""accuracy03"": accuracy03,\n            ""accuracy05"": accuracy05,\n        }\n\n        if self.is_train_loader:\n            loss.backward()\n            self.optimizer.step()\n            self.optimizer.zero_grad()\n\n\ndef main():\n    model = ClassifyAE(28 * 28, 128, 10)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.02)\n\n    loaders = {\n        ""train"": DataLoader(\n            MNIST(\n                os.getcwd(), train=False, download=True, transform=ToTensor(),\n            ),\n            batch_size=32,\n        ),\n        ""valid"": DataLoader(\n            MNIST(\n                os.getcwd(), train=False, download=True, transform=ToTensor(),\n            ),\n            batch_size=32,\n        ),\n    }\n\n    runner = CustomRunner()\n    runner.train(\n        model=model,\n        optimizer=optimizer,\n        loaders=loaders,\n        verbose=True,\n        check=True,\n    )\n\n\nif __name__ == ""__main__"":\n    if os.getenv(""USE_APEX"", ""0"") == ""0"" and os.getenv(""USE_DDP"", ""0"") == ""0"":\n        main()\n'"
tests/_tests_scripts/dl_z_mvp_mnist_gan.py,11,"b'# flake8: noqa\nimport os\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader\n\nfrom catalyst import dl\nfrom catalyst.contrib.data.transforms import ToTensor\nfrom catalyst.contrib.datasets import MNIST\nfrom catalyst.contrib.nn.modules import Flatten, GlobalMaxPool2d, Lambda\n\nLATENT_DIM = 128\n\n\nclass CustomRunner(dl.Runner):\n    def _handle_batch(self, batch):\n        real_images, _ = batch\n        batch_metrics = {}\n\n        # Sample random points in the latent space\n        batch_size = real_images.shape[0]\n        random_latent_vectors = torch.randn(batch_size, LATENT_DIM).to(\n            self.device\n        )\n\n        # Decode them to fake images\n        generated_images = self.model[""generator""](\n            random_latent_vectors\n        ).detach()\n        # Combine them with real images\n        combined_images = torch.cat([generated_images, real_images])\n\n        # Assemble labels discriminating real from fake images\n        labels = torch.cat(\n            [torch.ones((batch_size, 1)), torch.zeros((batch_size, 1))]\n        ).to(self.device)\n        # Add random noise to the labels - important trick!\n        labels += 0.05 * torch.rand(labels.shape).to(self.device)\n\n        # Train the discriminator\n        predictions = self.model[""discriminator""](combined_images)\n        batch_metrics[\n            ""loss_discriminator""\n        ] = F.binary_cross_entropy_with_logits(predictions, labels)\n\n        # Sample random points in the latent space\n        random_latent_vectors = torch.randn(batch_size, LATENT_DIM).to(\n            self.device\n        )\n        # Assemble labels that say ""all real images""\n        misleading_labels = torch.zeros((batch_size, 1)).to(self.device)\n\n        # Train the generator\n        generated_images = self.model[""generator""](random_latent_vectors)\n        predictions = self.model[""discriminator""](generated_images)\n        batch_metrics[""loss_generator""] = F.binary_cross_entropy_with_logits(\n            predictions, misleading_labels\n        )\n\n        self.batch_metrics.update(**batch_metrics)\n\n\ndef main():\n    generator = nn.Sequential(\n        # We want to generate 128 coefficients to reshape into a 7x7x128 map\n        nn.Linear(128, 128 * 7 * 7),\n        nn.LeakyReLU(0.2, inplace=True),\n        Lambda(lambda x: x.view(x.size(0), 128, 7, 7)),\n        nn.ConvTranspose2d(128, 128, (4, 4), stride=(2, 2), padding=1),\n        nn.LeakyReLU(0.2, inplace=True),\n        nn.ConvTranspose2d(128, 128, (4, 4), stride=(2, 2), padding=1),\n        nn.LeakyReLU(0.2, inplace=True),\n        nn.Conv2d(128, 1, (7, 7), padding=3),\n        nn.Sigmoid(),\n    )\n    discriminator = nn.Sequential(\n        nn.Conv2d(1, 64, (3, 3), stride=(2, 2), padding=1),\n        nn.LeakyReLU(0.2, inplace=True),\n        nn.Conv2d(64, 128, (3, 3), stride=(2, 2), padding=1),\n        nn.LeakyReLU(0.2, inplace=True),\n        GlobalMaxPool2d(),\n        Flatten(),\n        nn.Linear(128, 1),\n    )\n\n    model = {""generator"": generator, ""discriminator"": discriminator}\n    optimizer = {\n        ""generator"": torch.optim.Adam(\n            generator.parameters(), lr=0.0003, betas=(0.5, 0.999)\n        ),\n        ""discriminator"": torch.optim.Adam(\n            discriminator.parameters(), lr=0.0003, betas=(0.5, 0.999)\n        ),\n    }\n    loaders = {\n        ""train"": DataLoader(\n            MNIST(\n                os.getcwd(), train=True, download=True, transform=ToTensor(),\n            ),\n            batch_size=32,\n        ),\n    }\n\n    runner = CustomRunner()\n    runner.train(\n        model=model,\n        optimizer=optimizer,\n        loaders=loaders,\n        callbacks=[\n            dl.OptimizerCallback(\n                optimizer_key=""generator"", metric_key=""loss_generator""\n            ),\n            dl.OptimizerCallback(\n                optimizer_key=""discriminator"", metric_key=""loss_discriminator""\n            ),\n        ],\n        main_metric=""loss_generator"",\n        num_epochs=20,\n        verbose=True,\n        logdir=""./logs_gan"",\n        check=True,\n    )\n\n\nif __name__ == ""__main__"":\n    if os.getenv(""USE_APEX"", ""0"") == ""0"" and os.getenv(""USE_DDP"", ""0"") == ""0"":\n        main()\n'"
tests/_tests_scripts/dl_z_mvp_mnist_unet.py,4,"b'# flake8: noqa\nimport os\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader\n\nfrom catalyst import dl\nfrom catalyst.contrib.data.transforms import ToTensor\nfrom catalyst.contrib.datasets import MNIST\nfrom catalyst.utils import metrics\n\n\nclass ClassifyUnet(nn.Module):\n    def __init__(self, in_channels, in_hw, out_features):\n        super().__init__()\n        self.encoder = nn.Sequential(\n            nn.Conv2d(in_channels, in_channels, 3, 1, 1), nn.Tanh()\n        )\n        self.decoder = nn.Conv2d(in_channels, in_channels, 3, 1, 1)\n        self.clf = nn.Linear(in_channels * in_hw * in_hw, out_features)\n\n    def forward(self, x):\n        z = self.encoder(x)\n        z_ = z.view(z.size(0), -1)\n        y_hat = self.clf(z_)\n        x_ = self.decoder(z)\n        return y_hat, x_\n\n\nclass CustomRunner(dl.Runner):\n    def _handle_batch(self, batch):\n        x, y = batch\n        x_noise = (x + torch.rand_like(x)).clamp_(0, 1)\n        y_hat, x_ = self.model(x_noise)\n\n        loss_clf = F.cross_entropy(y_hat, y)\n        iou = metrics.iou(x_, x)\n        loss_iou = 1 - iou\n        loss = loss_clf + loss_iou\n        accuracy01, accuracy03, accuracy05 = metrics.accuracy(\n            y_hat, y, topk=(1, 3, 5)\n        )\n\n        self.batch_metrics = {\n            ""loss_clf"": loss_clf,\n            ""loss_iou"": loss_iou,\n            ""loss"": loss,\n            ""iou"": iou,\n            ""accuracy01"": accuracy01,\n            ""accuracy03"": accuracy03,\n            ""accuracy05"": accuracy05,\n        }\n\n        if self.is_train_loader:\n            loss.backward()\n            self.optimizer.step()\n            self.optimizer.zero_grad()\n\n\ndef main():\n    model = ClassifyUnet(1, 28, 10)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.02)\n\n    loaders = {\n        ""train"": DataLoader(\n            MNIST(\n                os.getcwd(), train=False, download=True, transform=ToTensor(),\n            ),\n            batch_size=32,\n        ),\n        ""valid"": DataLoader(\n            MNIST(\n                os.getcwd(), train=False, download=True, transform=ToTensor(),\n            ),\n            batch_size=32,\n        ),\n    }\n\n    runner = CustomRunner()\n    runner.train(\n        model=model,\n        optimizer=optimizer,\n        loaders=loaders,\n        verbose=True,\n        check=True,\n    )\n\n\nif __name__ == ""__main__"":\n    if os.getenv(""USE_APEX"", ""0"") == ""0"" and os.getenv(""USE_DDP"", ""0"") == ""0"":\n        main()\n'"
tests/_tests_scripts/dl_z_mvp_mnist_vae.py,11,"b'# flake8: noqa\nimport os\n\nimport numpy as np\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader\n\nfrom catalyst import dl\nfrom catalyst.contrib.data.transforms import ToTensor\nfrom catalyst.contrib.datasets import MNIST\nfrom catalyst.utils import metrics\n\nLOG_SCALE_MAX = 2\nLOG_SCALE_MIN = -10\n\n\ndef normal_sample(mu, sigma):\n    return mu + sigma * torch.randn_like(sigma)\n\n\ndef normal_logprob(mu, sigma, z):\n    normalization_constant = -sigma.log() - 0.5 * np.log(2 * np.pi)\n    square_term = -0.5 * ((z - mu) / sigma) ** 2\n    logprob_vec = normalization_constant + square_term\n    logprob = logprob_vec.sum(1)\n    return logprob\n\n\nclass ClassifyVAE(torch.nn.Module):\n    def __init__(self, in_features, hid_features, out_features):\n        super().__init__()\n        self.encoder = torch.nn.Linear(in_features, hid_features * 2)\n        self.decoder = nn.Sequential(\n            nn.Linear(hid_features, in_features), nn.Sigmoid()\n        )\n        self.clf = torch.nn.Linear(hid_features, out_features)\n\n    def forward(self, x, deterministic=False):\n        z = self.encoder(x)\n        bs, z_dim = z.shape\n\n        loc, log_scale = z[:, : z_dim // 2], z[:, z_dim // 2 :]\n        log_scale = torch.clamp(log_scale, LOG_SCALE_MIN, LOG_SCALE_MAX)\n        scale = torch.exp(log_scale)\n        z_ = loc if deterministic else normal_sample(loc, scale)\n        z_logprob = normal_logprob(loc, scale, z_)\n        z_ = z_.view(bs, -1)\n        x_ = self.decoder(z_)\n        y_hat = self.clf(z_)\n\n        return y_hat, x_, z_logprob, loc, log_scale\n\n\nclass CustomRunner(dl.Runner):\n    """"""\n    Docs.\n    """"""\n\n    def _handle_batch(self, batch):\n        """"""\n        Docs.\n        """"""\n        x, y = batch\n        x = x.view(x.size(0), -1)\n        y_hat, x_, z_logprob, loc, log_scale = self.model(x)\n\n        loss_clf = F.cross_entropy(y_hat, y)\n        loss_ae = F.mse_loss(x_, x)\n        loss_kld = (\n            -0.5\n            * torch.mean(1 + log_scale - loc.pow(2) - log_scale.exp())\n            * 0.1\n        )\n        loss_logprob = torch.mean(z_logprob) * 0.01\n        loss = loss_clf + loss_ae + loss_kld + loss_logprob\n        accuracy01, accuracy03, accuracy05 = metrics.accuracy(\n            y_hat, y, topk=(1, 3, 5)\n        )\n\n        self.batch_metrics = {\n            ""loss_clf"": loss_clf,\n            ""loss_ae"": loss_ae,\n            ""loss_kld"": loss_kld,\n            ""loss_logprob"": loss_logprob,\n            ""loss"": loss,\n            ""accuracy01"": accuracy01,\n            ""accuracy03"": accuracy03,\n            ""accuracy05"": accuracy05,\n        }\n\n        if self.is_train_loader:\n            loss.backward()\n            self.optimizer.step()\n            self.optimizer.zero_grad()\n\n\ndef main():\n    model = ClassifyVAE(28 * 28, 64, 10)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.02)\n\n    loaders = {\n        ""train"": DataLoader(\n            MNIST(\n                os.getcwd(), train=False, download=True, transform=ToTensor(),\n            ),\n            batch_size=32,\n        ),\n        ""valid"": DataLoader(\n            MNIST(\n                os.getcwd(), train=False, download=True, transform=ToTensor(),\n            ),\n            batch_size=32,\n        ),\n    }\n\n    runner = CustomRunner()\n    runner.train(\n        model=model,\n        optimizer=optimizer,\n        loaders=loaders,\n        verbose=True,\n        check=True,\n    )\n\n\nif __name__ == ""__main__"":\n    if os.getenv(""USE_APEX"", ""0"") == ""0"" and os.getenv(""USE_DDP"", ""0"") == ""0"":\n        main()\n'"
tests/_tests_scripts/dl_z_mvp_projector.py,6,"b'# flake8: noqa\nimport os\nimport sys\n\nimport torch\nfrom torch.utils.data import DataLoader, TensorDataset\n\nfrom catalyst.dl import SupervisedRunner\n\nif os.getenv(""USE_APEX"", ""0"") != ""0"" or os.getenv(""USE_DDP"", ""0"") != ""0"":\n    sys.exit()\n\n\n# data\nnum_samples, num_features = int(32e3), int(1e1)\nX, y = torch.rand(num_samples, num_features), torch.rand(num_samples)\ndataset = TensorDataset(X, y)\nloader = DataLoader(dataset, batch_size=32, num_workers=1)\nloaders = {""train"": loader, ""valid"": loader}\n\n# model, criterion, optimizer, scheduler\nmodel = torch.nn.Linear(num_features, 1)\ncriterion = torch.nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters())\nscheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, [3, 6])\n\nrunner = SupervisedRunner()\n# model training\nrunner.train(\n    model=model,\n    criterion=criterion,\n    optimizer=optimizer,\n    scheduler=scheduler,\n    loaders=loaders,\n    logdir=""./logdir"",\n    num_epochs=8,\n    verbose=True,\n    check=True,\n    load_best_on_end=True,\n)\n# model inference\nfor prediction in runner.predict_loader(loader=loader):\n    assert prediction[""logits""].cpu().detach().numpy().shape == (32, 1)\n# model tracing\ntraced_model = runner.trace(loader=loader)\n'"
tests/_tests_scripts/nlp_contrib_models.py,0,b'# flake8: noqa\nfrom catalyst.contrib.models.nlp import *\n'
catalyst/contrib/data/__init__.py,0,"b'# flake8: noqa\n\nfrom .transforms import Compose, Normalize, normalize, to_tensor, ToTensor\n'"
catalyst/contrib/data/transforms.py,8,"b'""""""\nThis subpackage was borrowed from\ntorchvision(https://github.com/pytorch/vision).\n""""""\n\nimport numpy as np\n\nimport torch\n\n\ndef to_tensor(pic: np.ndarray) -> torch.Tensor:\n    """"""Convert ``numpy.ndarray`` to tensor.\n\n    Args:\n        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\n\n    Returns:\n        Tensor: Converted image.\n    """"""\n    if not isinstance(pic, np.ndarray):\n        raise TypeError(f""pic should be ndarray. Got {type(pic)}"")\n    if pic.ndim not in {2, 3}:\n        raise ValueError(\n            f""pic should be 2/3 dimensional. Got {pic.ndim} dimensions.""\n        )\n\n    if pic.ndim == 2:\n        pic = pic[:, :, None]\n\n    img = torch.from_numpy(pic.transpose((2, 0, 1)))\n    # backward compatibility\n    if isinstance(img, torch.ByteTensor):\n        return img.float().div(255)\n    return img\n\n\ndef normalize(tensor, mean, std, inplace=False):\n    """"""Normalize a tensor image with mean and standard deviation.\n\n    .. note::\n        This transform acts out of place by default, i.e.,\n            it does not mutates the input tensor.\n\n    Args:\n        tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n        mean (sequence): Sequence of means for each channel.\n        std (sequence): Sequence of standard deviations for each channel.\n        inplace(bool,optional): Bool to make this operation inplace.\n\n    Returns:\n        Tensor: Normalized Tensor image.\n    """"""\n    if not (torch.is_tensor(tensor) and tensor.ndimension() == 3):\n        raise TypeError(""tensor is not a torch image."")\n\n    if not inplace:\n        tensor = tensor.clone()\n\n    dtype = tensor.dtype\n    mean = torch.as_tensor(mean, dtype=dtype, device=tensor.device)\n    std = torch.as_tensor(std, dtype=dtype, device=tensor.device)\n    tensor.sub_(mean[:, None, None]).div_(std[:, None, None])\n    return tensor\n\n\nclass Compose:\n    """"""Composes several transforms together.""""""\n\n    def __init__(self, transforms):\n        """"""\n        Args:\n            transforms (List): list of transforms to compose.\n\n        Example:\n            >>> Compose([ToTensor(), Normalize()])\n        """"""\n        self.transforms = transforms\n\n    def __call__(self, img):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        for t in self.transforms:\n            img = t(img)\n        return img\n\n    def __repr__(self):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        format_string = self.__class__.__name__ + ""(""\n        for t in self.transforms:\n            format_string += ""\\n""\n            format_string += ""    {0}"".format(t)\n        format_string += ""\\n)""\n        return format_string\n\n\nclass ToTensor(object):\n    """"""Convert a ``numpy.ndarray`` to tensor.\n    Converts numpy.ndarray (H x W x C) in the range [0, 255] to a\n    torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0]\n    if the numpy.ndarray has dtype = np.uint8\n    In the other cases, tensors are returned without scaling.\n    """"""\n\n    def __call__(self, pic):\n        """"""\n        Args:\n            pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\n\n        Returns:\n            Tensor: Converted image.\n        """"""\n        return to_tensor(pic)\n\n    def __repr__(self):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        return self.__class__.__name__ + ""()""\n\n\nclass Normalize(object):\n    """"""Normalize a tensor image with mean and standard deviation.\n\n    Given mean: ``(mean[1],...,mean[n])`` and std: ``(std[1],..,std[n])``\n    for ``n`` channels, this transform will normalize each channel of the input\n    ``torch.*Tensor`` i.e.,\n    ``output[channel] = (input[channel] - mean[channel]) / std[channel]``\n\n    .. note::\n        This transform acts out of place, i.e.,\n            it does not mutate the input tensor.\n    """"""\n\n    def __init__(self, mean, std, inplace=False):\n        """"""\n        Args:\n            mean (sequence): Sequence of means for each channel.\n            std (sequence): Sequence of standard deviations for each channel.\n            inplace(bool,optional): Bool to make this operation in-place.\n        """"""\n        self.mean = mean\n        self.std = std\n        self.inplace = inplace\n\n    def __call__(self, tensor):\n        """"""\n        Args:\n            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n\n        Returns:\n            Tensor: Normalized Tensor image.\n        """"""\n        return normalize(tensor, self.mean, self.std, self.inplace)\n\n    def __repr__(self):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        return self.__class__.__name__ + ""(mean={0}, std={1})"".format(\n            self.mean, self.std\n        )\n\n\n__all__ = [""Compose"", ""Normalize"", ""ToTensor"", ""normalize"", ""to_tensor""]\n'"
catalyst/contrib/datasets/__init__.py,0,b'# flake8: noqa\n\nfrom catalyst.contrib.datasets.mnist import MNIST\n'
catalyst/contrib/datasets/mnist.py,15,"b'import codecs\nimport os\n\nimport numpy as np\n\nimport torch\nfrom torch.utils.data import Dataset\n\nfrom catalyst.contrib.datasets.utils import download_and_extract_archive\n\n\nclass MNIST(Dataset):\n    """"""`MNIST <http://yann.lecun.com/exdb/mnist/>`_ Dataset.""""""\n\n    _repr_indent = 4\n\n    resources = [\n        (\n            ""http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz"",\n            ""f68b3c2dcbeaaa9fbdd348bbdeb94873"",\n        ),\n        (\n            ""http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz"",\n            ""d53e105ee54ea40749a09fcbcd1e9432"",\n        ),\n        (\n            ""http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz"",\n            ""9fb629c4189551a2d022fa330f9573f3"",\n        ),\n        (\n            ""http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz"",\n            ""ec29112dd5afa0611ce80d1b7f02629c"",\n        ),\n    ]\n\n    training_file = ""training.pt""\n    test_file = ""test.pt""\n    classes = [\n        ""0 - zero"",\n        ""1 - one"",\n        ""2 - two"",\n        ""3 - three"",\n        ""4 - four"",\n        ""5 - five"",\n        ""6 - six"",\n        ""7 - seven"",\n        ""8 - eight"",\n        ""9 - nine"",\n    ]\n\n    def __init__(\n        self,\n        root,\n        train=True,\n        transform=None,\n        target_transform=None,\n        download=False,\n    ):\n        """"""\n        Args:\n            root (string): Root directory of dataset where\n                ``MNIST/processed/training.pt``\n                and  ``MNIST/processed/test.pt`` exist.\n            train (bool, optional): If True, creates dataset from\n                ``training.pt``, otherwise from ``test.pt``.\n            download (bool, optional): If true, downloads the dataset from\n                the internet and puts it in root directory. If dataset\n                is already downloaded, it is not downloaded again.\n            transform (callable, optional): A function/transform that\n                takes in an image and returns a transformed version.\n            target_transform (callable, optional): A function/transform\n                that takes in the target and transforms it.\n        """"""\n        if isinstance(root, torch._six.string_classes):\n            root = os.path.expanduser(root)\n        self.root = root\n        self.train = train  # training set or test set\n        self.transform = transform\n        self.target_transform = target_transform\n\n        if download:\n            self.download()\n\n        if not self._check_exists():\n            raise RuntimeError(\n                ""Dataset not found. You can use download=True to download it""\n            )\n\n        if self.train:\n            data_file = self.training_file\n        else:\n            data_file = self.test_file\n        self.data, self.targets = torch.load(\n            os.path.join(self.processed_folder, data_file)\n        )\n\n    def __getitem__(self, index):\n        """"""\n        Args:\n            index (int): Index\n\n        Returns:\n            tuple: (image, target) where target is index of the target class.\n        """"""\n        img, target = self.data[index].numpy(), int(self.targets[index])\n\n        if self.transform is not None:\n            img = self.transform(img)\n\n        if self.target_transform is not None:\n            target = self.target_transform(target)\n\n        return img, target\n\n    def __len__(self):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        return len(self.data)\n\n    def __repr__(self):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        head = ""Dataset "" + self.__class__.__name__\n        body = [""Number of datapoints: {}"".format(self.__len__())]\n        if self.root is not None:\n            body.append(""Root location: {}"".format(self.root))\n        body += self.extra_repr().splitlines()\n        if hasattr(self, ""transforms"") and self.transforms is not None:\n            body += [repr(self.transforms)]\n        lines = [head] + ["" "" * self._repr_indent + line for line in body]\n        return ""\\n"".join(lines)\n\n    @property\n    def raw_folder(self):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        return os.path.join(self.root, self.__class__.__name__, ""raw"")\n\n    @property\n    def processed_folder(self):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        return os.path.join(self.root, self.__class__.__name__, ""processed"")\n\n    @property\n    def class_to_idx(self):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        return {_class: i for i, _class in enumerate(self.classes)}\n\n    def _check_exists(self):\n        return os.path.exists(\n            os.path.join(self.processed_folder, self.training_file)\n        ) and os.path.exists(\n            os.path.join(self.processed_folder, self.test_file)\n        )\n\n    def download(self):\n        """"""Download the MNIST data if it doesn\'t exist in processed_folder.""""""\n        if self._check_exists():\n            return\n\n        os.makedirs(self.raw_folder, exist_ok=True)\n        os.makedirs(self.processed_folder, exist_ok=True)\n\n        # download files\n        for url, md5 in self.resources:\n            filename = url.rpartition(""/"")[2]\n            download_and_extract_archive(\n                url, download_root=self.raw_folder, filename=filename, md5=md5\n            )\n\n        # process and save as torch files\n        print(""Processing..."")\n\n        training_set = (\n            read_image_file(\n                os.path.join(self.raw_folder, ""train-images-idx3-ubyte"")\n            ),\n            read_label_file(\n                os.path.join(self.raw_folder, ""train-labels-idx1-ubyte"")\n            ),\n        )\n        test_set = (\n            read_image_file(\n                os.path.join(self.raw_folder, ""t10k-images-idx3-ubyte"")\n            ),\n            read_label_file(\n                os.path.join(self.raw_folder, ""t10k-labels-idx1-ubyte"")\n            ),\n        )\n        with open(\n            os.path.join(self.processed_folder, self.training_file), ""wb""\n        ) as f:\n            torch.save(training_set, f)\n        with open(\n            os.path.join(self.processed_folder, self.test_file), ""wb""\n        ) as f:\n            torch.save(test_set, f)\n\n        print(""Done!"")\n\n    def extra_repr(self):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        return ""Split: {}"".format(""Train"" if self.train is True else ""Test"")\n\n\ndef get_int(b):\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    return int(codecs.encode(b, ""hex""), 16)\n\n\ndef open_maybe_compressed_file(path):\n    """"""Return a file object that possibly decompresses \'path\' on the fly.\n    Decompression occurs when argument `path` is a string\n    and ends with \'.gz\' or \'.xz\'.\n    """"""\n    if not isinstance(path, torch._six.string_classes):\n        return path\n    if path.endswith("".gz""):\n        import gzip\n\n        return gzip.open(path, ""rb"")\n    if path.endswith("".xz""):\n        import lzma\n\n        return lzma.open(path, ""rb"")\n    return open(path, ""rb"")\n\n\ndef read_sn3_pascalvincent_tensor(path, strict=True):\n    """"""Read a SN3 file in ""Pascal Vincent"" format.\n    Argument may be a filename, compressed filename, or file object.\n    """"""\n    # typemap\n    if not hasattr(read_sn3_pascalvincent_tensor, ""typemap""):\n        read_sn3_pascalvincent_tensor.typemap = {\n            8: (torch.uint8, np.uint8, np.uint8),\n            9: (torch.int8, np.int8, np.int8),\n            11: (torch.int16, np.dtype("">i2""), ""i2""),\n            12: (torch.int32, np.dtype("">i4""), ""i4""),\n            13: (torch.float32, np.dtype("">f4""), ""f4""),\n            14: (torch.float64, np.dtype("">f8""), ""f8""),\n        }\n    # read\n    with open_maybe_compressed_file(path) as f:\n        data = f.read()\n    # parse\n    magic = get_int(data[0:4])\n    nd = magic % 256\n    ty = magic // 256\n    assert nd >= 1 and nd <= 3\n    assert ty >= 8 and ty <= 14\n    m = read_sn3_pascalvincent_tensor.typemap[ty]\n    s = [get_int(data[4 * (i + 1) : 4 * (i + 2)]) for i in range(nd)]\n    parsed = np.frombuffer(data, dtype=m[1], offset=(4 * (nd + 1)))\n    assert parsed.shape[0] == np.prod(s) or not strict\n    return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n\n\ndef read_label_file(path):\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    with open(path, ""rb"") as f:\n        x = read_sn3_pascalvincent_tensor(f, strict=False)\n    assert x.dtype == torch.uint8\n    assert x.ndimension() == 1\n    return x.long()\n\n\ndef read_image_file(path):\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    with open(path, ""rb"") as f:\n        x = read_sn3_pascalvincent_tensor(f, strict=False)\n    assert x.dtype == torch.uint8\n    assert x.ndimension() == 3\n    return x\n'"
catalyst/contrib/datasets/utils.py,1,"b'import gzip\nimport hashlib\nimport os\nimport tarfile\nimport zipfile\n\nfrom torch.utils.model_zoo import tqdm\n\n\ndef gen_bar_updater():\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    pbar = tqdm(total=None)\n\n    def bar_update(count, block_size, total_size):\n        if pbar.total is None and total_size:\n            pbar.total = total_size\n        progress_bytes = count * block_size\n        pbar.update(progress_bytes - pbar.n)\n\n    return bar_update\n\n\ndef calculate_md5(fpath, chunk_size=1024 * 1024):\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    md5 = hashlib.md5()\n    with open(fpath, ""rb"") as f:\n        for chunk in iter(lambda: f.read(chunk_size), b""""):\n            md5.update(chunk)\n    return md5.hexdigest()\n\n\ndef check_md5(fpath, md5, **kwargs):\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    return md5 == calculate_md5(fpath, **kwargs)\n\n\ndef check_integrity(fpath, md5=None):\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    if not os.path.isfile(fpath):\n        return False\n    if md5 is None:\n        return True\n    return check_md5(fpath, md5)\n\n\ndef download_url(url, root, filename=None, md5=None):\n    """"""Download a file from a url and place it in root.\n\n    Args:\n        url (str): URL to download file from\n        root (str): Directory to place downloaded file in\n        filename (str, optional): Name to save the file under.\n            If None, use the basename of the URL\n        md5 (str, optional): MD5 checksum of the download.\n            If None, do not check\n    """"""\n    import urllib\n\n    root = os.path.expanduser(root)\n    if not filename:\n        filename = os.path.basename(url)\n    fpath = os.path.join(root, filename)\n\n    os.makedirs(root, exist_ok=True)\n\n    # check if file is already present locally\n    if check_integrity(fpath, md5):\n        print(""Using downloaded and verified file: "" + fpath)\n    else:  # download the file\n        try:\n            print(""Downloading "" + url + "" to "" + fpath)\n            urllib.request.urlretrieve(\n                url, fpath, reporthook=gen_bar_updater()\n            )\n        except (urllib.error.URLError, IOError) as e:\n            if url[:5] == ""https"":\n                url = url.replace(""https:"", ""http:"")\n                print(\n                    ""Failed download. Trying https -> http instead.""\n                    "" Downloading "" + url + "" to "" + fpath\n                )\n                urllib.request.urlretrieve(\n                    url, fpath, reporthook=gen_bar_updater()\n                )\n            else:\n                raise e\n        # check integrity of downloaded file\n        if not check_integrity(fpath, md5):\n            raise RuntimeError(""File not found or corrupted."")\n\n\ndef extract_archive(from_path, to_path=None, remove_finished=False):\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    if to_path is None:\n        to_path = os.path.dirname(from_path)\n\n    if from_path.endswith("".tar""):\n        with tarfile.open(from_path, ""r"") as tar:\n            tar.extractall(path=to_path)\n    elif from_path.endswith("".tar.gz"") or from_path.endswith("".tgz""):\n        with tarfile.open(from_path, ""r:gz"") as tar:\n            tar.extractall(path=to_path)\n    elif from_path.endswith("".tar.xz""):\n        with tarfile.open(from_path, ""r:xz"") as tar:\n            tar.extractall(path=to_path)\n    elif from_path.endswith("".gz""):\n        root, _ = os.path.splitext(os.path.basename(from_path))\n        to_path = os.path.join(to_path, root)\n        with open(to_path, ""wb"") as out_f, gzip.GzipFile(from_path) as zip_f:\n            out_f.write(zip_f.read())\n    elif from_path.endswith("".zip""):\n        with zipfile.ZipFile(from_path, ""r"") as z:\n            z.extractall(to_path)\n    else:\n        raise ValueError(f""Extraction of {from_path} not supported"")\n\n    if remove_finished:\n        os.remove(from_path)\n\n\ndef download_and_extract_archive(\n    url,\n    download_root,\n    extract_root=None,\n    filename=None,\n    md5=None,\n    remove_finished=False,\n):\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    download_root = os.path.expanduser(download_root)\n    if extract_root is None:\n        extract_root = download_root\n    if not filename:\n        filename = os.path.basename(url)\n\n    download_url(url, download_root, filename, md5)\n\n    archive = os.path.join(download_root, filename)\n    print(f""Extracting {archive} to {extract_root}"")\n    extract_archive(archive, extract_root, remove_finished)\n'"
catalyst/contrib/dl/__init__.py,0,b'# flake8: noqa\n# isort:skip_file\n\nfrom .callbacks import *\n'
catalyst/contrib/models/__init__.py,0,"b'# flake8: noqa\n\nfrom .functional import get_convolution_net, get_linear_net\nfrom .hydra import Hydra\nfrom .sequential import (\n    _process_additional_params,\n    ResidualWrapper,\n    SequentialNet,\n)\n'"
catalyst/contrib/models/functional.py,0,"b'from typing import List, Union\n\nfrom torch import nn\n\nfrom .sequential import _process_additional_params, SequentialNet\n\n\ndef get_convolution_net(\n    in_channels: int,\n    history_len: int = 1,\n    channels: List = None,\n    kernel_sizes: List = None,\n    strides: List = None,\n    groups: List = None,\n    use_bias: Union[bool, List] = False,\n    normalization: Union[str, List] = None,\n    dropout_rate: Union[float, List] = None,\n    activation: Union[str, List] = None,\n    residual: Union[bool, str] = False,\n    layer_order: List = None,\n) -> nn.Module:\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    channels = channels or [32, 64, 64]\n    kernel_sizes = kernel_sizes or [8, 4, 3]\n    strides = strides or [4, 2, 1]\n    groups = groups or [1, 1, 1]\n    assert len(channels) == len(kernel_sizes) == len(strides) == len(groups)\n    use_bias = _process_additional_params(use_bias, channels)\n\n    layer_fn = [\n        {\n            ""module"": nn.Conv2d,\n            ""bias"": bias,\n            ""kernel_size"": kernel_size,\n            ""stride"": stride,\n            ""groups"": group,\n        }\n        for bias, kernel_size, stride, group in zip(\n            use_bias, kernel_sizes, strides, groups\n        )\n    ]\n\n    if dropout_rate is not None:\n        dropout_fn = (\n            {""module"": nn.Dropout2d, ""p"": dropout_rate}\n            if isinstance(dropout_rate, float)\n            else [\n                {""module"": nn.Dropout2d, ""p"": p} if p is not None else None\n                for p in dropout_rate\n            ]\n        )\n    else:\n        dropout_fn = None\n\n    channels.insert(0, history_len * in_channels)\n    net = SequentialNet(\n        hiddens=channels,\n        layer_fn=layer_fn,\n        norm_fn=normalization,\n        dropout_fn=dropout_fn,\n        activation_fn=activation,\n        residual=residual,\n        layer_order=layer_order,\n    )\n    return net\n\n\ndef get_linear_net(\n    in_features: int,\n    history_len: int = 1,\n    features: List = None,\n    use_bias: Union[bool, List] = False,\n    normalization: Union[str, List] = None,\n    dropout_rate: Union[float, List] = None,\n    activation: Union[str, List] = None,\n    residual: Union[bool, str] = False,\n    layer_order: List = None,\n) -> nn.Module:\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    features = features or [64, 128, 64]\n\n    layer_fn = (\n        {""module"": nn.Linear, ""bias"": use_bias}\n        if isinstance(use_bias, bool)\n        else [{""module"": nn.Linear, ""bias"": bias} for bias in use_bias]\n    )\n    if dropout_rate is not None:\n        dropout_fn = (\n            {""module"": nn.Dropout, ""p"": dropout_rate}\n            if isinstance(dropout_rate, float)\n            else [\n                {""module"": nn.Dropout, ""p"": p} if p is not None else None\n                for p in dropout_rate\n            ]\n        )\n    else:\n        dropout_fn = None\n\n    features.insert(0, history_len * in_features)\n    net = SequentialNet(\n        hiddens=features,\n        layer_fn=layer_fn,\n        norm_fn=normalization,\n        dropout_fn=dropout_fn,\n        activation_fn=activation,\n        residual=residual,\n        layer_order=layer_order,\n    )\n\n    return net\n'"
catalyst/contrib/models/hydra.py,2,"b'# Author: Sergey Kolesnikov, scitator@gmail.com\n\nfrom typing import Dict, Union\nfrom collections import OrderedDict\nfrom copy import deepcopy\n\nimport torch\nfrom torch import nn\n\nfrom catalyst import utils\nfrom catalyst.contrib.nn.modules import Normalize\n\nfrom .sequential import SequentialNet\n\n\nclass Hydra(nn.Module):\n    """"""Hydra - one model to predict them all.\n\n    @TODO: Docs. Contribution is welcome.\n    """"""\n\n    _parent_keyword = ""_""\n    _hidden_keyword = ""_hidden""\n    _normalize_keyword = ""normalize_output""\n\n    def __init__(\n        self,\n        heads: nn.ModuleDict,\n        encoder: nn.Module = None,\n        embedders: nn.ModuleDict = None,\n    ):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        super().__init__()\n        self.encoder = encoder or nn.Sequential()\n        self.heads = heads\n        self.embedders = embedders or {}\n\n    @staticmethod\n    def parse_head_params(\n        head_params: Dict, in_features: int, is_leaf: bool = False,\n    ) -> Union[nn.Module, nn.ModuleDict]:\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        if is_leaf:\n            if isinstance(head_params, int):\n                head_params = {""hiddens"": [head_params]}\n            normalize = head_params.pop(Hydra._normalize_keyword, False)\n            head_params[""hiddens""].insert(0, in_features)\n\n            output = [(""net"", SequentialNet(**head_params))]\n            if normalize:\n                output.append((""normalize"", Normalize()))\n\n            output = OrderedDict(output)\n            output = nn.Sequential(output)\n        else:\n            output = {}\n\n            hidden_params = head_params.pop(Hydra._hidden_keyword, None)\n            if hidden_params is not None:\n                in_features = (\n                    hidden_params\n                    if isinstance(hidden_params, int)\n                    else hidden_params[""hiddens""][-1]\n                )\n                output[Hydra._hidden_keyword] = Hydra.parse_head_params(\n                    head_params=hidden_params,\n                    in_features=in_features,\n                    is_leaf=True,\n                )\n\n            for head_name_, head_params_ in head_params.items():\n                output[head_name_] = Hydra.parse_head_params(\n                    head_params=head_params_,\n                    in_features=in_features,\n                    is_leaf=not head_name_.startswith(Hydra._parent_keyword),\n                )\n            output = nn.ModuleDict(output)\n        return output\n\n    @staticmethod\n    def forward_head(input_, head):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        if isinstance(head, nn.ModuleDict):\n            output = {}\n\n            net_ = getattr(head, Hydra._hidden_keyword, None)\n            if net_ is not None:\n                input_ = net_(input_)\n                output[""""] = input_\n\n            for head_name, head_layer in head.items():\n                if head_name == Hydra._hidden_keyword:\n                    continue\n                output[head_name] = Hydra.forward_head(input_, head_layer)\n        elif isinstance(head, nn.Module):\n            output = head(input_)\n        else:\n            raise NotImplementedError()\n        return output\n\n    def forward(\n        self, features: torch.Tensor, **targets_kwargs,\n    ):\n        """"""Forward call.""""""\n        embeddings = self.encoder(features)\n\n        heads_output = self.forward_head(embeddings, self.heads)\n        output = {\n            ""features"": features,\n            ""embeddings"": embeddings,\n            **utils.flatten_dict(heads_output),\n        }\n\n        for key, value in targets_kwargs.items():\n            output[f""{key}_embeddings""] = self.embedders[key](value)\n\n        return output\n\n    def forward_tuple(self, features: torch.Tensor):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        output_kv = self.forward(features)\n        output = [\n            output_kv[""features""],\n            output_kv[""embeddings""],\n        ]\n\n        # let\'s remove all hidden parts from prediction\n        output.extend(\n            [\n                value\n                for key, value in output_kv.items()\n                if not key.endswith(""/"")\n                and key not in [""features"", ""embeddings""]\n            ]\n        )\n        output = tuple(output)\n        return output\n\n    @classmethod\n    def get_from_params(\n        cls,\n        heads_params: Dict,\n        encoder_params: Dict = None,\n        embedders_params: Dict = None,\n        in_features: int = None,\n    ) -> ""Hydra"":\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        heads_params_ = deepcopy(heads_params)\n        encoder_params_ = deepcopy(encoder_params)\n        embedders_params_ = deepcopy(embedders_params)\n\n        def _get_normalization_keyword(dct: Dict):\n            return (\n                dct.pop(Hydra._normalize_keyword, False)\n                if dct is not None\n                else False\n            )\n\n        if encoder_params_ is not None:\n            normalize_embeddings: bool = _get_normalization_keyword(\n                encoder_params_\n            )\n\n            encoder = SequentialNet(**encoder_params_)\n            in_features = encoder_params_[""hiddens""][-1]\n\n            if normalize_embeddings:\n                encoder = nn.Sequential(encoder, Normalize())\n        else:\n            assert in_features is not None\n            encoder = None\n\n        heads = Hydra.parse_head_params(\n            head_params=heads_params_, in_features=in_features\n        )\n        assert isinstance(heads, nn.ModuleDict)\n\n        embedders = {}\n        if embedders_params_ is not None:\n            for key, head_params in embedders_params_.items():\n                if isinstance(head_params, int):\n                    head_params = {""num_embeddings"": head_params}\n                normalize_ = head_params.pop(Hydra._normalize_keyword, False)\n                block = [\n                    (\n                        ""embedding"",\n                        nn.Embedding(\n                            embedding_dim=in_features, **head_params,\n                        ),\n                    )\n                ]\n                if normalize_:\n                    block.append((""normalize"", Normalize()))\n\n                block = OrderedDict(block)\n                block = nn.Sequential(block)\n                embedders[key] = block\n            embedders = nn.ModuleDict(embedders)\n\n        net = cls(heads=heads, encoder=encoder, embedders=embedders,)\n\n        return net\n'"
catalyst/contrib/models/sequential.py,2,"b'from typing import Dict, List, Union\nfrom collections import OrderedDict\nfrom copy import deepcopy\n\nimport torch\nfrom torch import nn\n\nfrom catalyst import utils\nfrom catalyst.contrib.registry import MODULES\n\n\ndef _process_additional_params(params, layers):\n    if isinstance(params, List):\n        assert len(params) == len(layers)\n    else:\n        params = [params] * len(layers)\n    return params\n\n\nclass ResidualWrapper(nn.Module):\n    """"""@TODO: Docs. Contribution is welcome.""""""\n\n    def __init__(self, net):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        super().__init__()\n        self.net = net\n\n    def forward(self, x):\n        """"""Forward call.""""""\n        return x + self.net(x)\n\n\nclass SequentialNet(nn.Module):\n    """"""@TODO: Docs. Contribution is welcome.""""""\n\n    def __init__(\n        self,\n        hiddens,\n        layer_fn: Union[str, Dict, List],\n        norm_fn: Union[str, Dict, List] = None,\n        dropout_fn: Union[str, Dict, List] = None,\n        activation_fn: Union[str, Dict, List] = None,\n        residual: Union[bool, str] = False,\n        layer_order: List = None,\n    ):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        super().__init__()\n        assert len(hiddens) > 1, ""No sequence found""\n\n        # layer params\n        layer_fn = _process_additional_params(layer_fn, hiddens[1:])\n        # normalization params\n        norm_fn = _process_additional_params(norm_fn, hiddens[1:])\n        # dropout params\n        dropout_fn = _process_additional_params(dropout_fn, hiddens[1:])\n        # activation params\n        activation_fn = _process_additional_params(activation_fn, hiddens[1:])\n\n        if isinstance(residual, bool) and residual:\n            residual = ""hard""\n            residual = _process_additional_params(residual, hiddens[1:])\n\n        layer_order = layer_order or [""layer"", ""norm"", ""drop"", ""act""]\n\n        def _layer_fn(layer_fn, f_in, f_out, **kwargs):\n            layer_fn = MODULES.get_if_str(layer_fn)\n            layer_fn = layer_fn(f_in, f_out, **kwargs)\n            return layer_fn\n\n        def _normalization_fn(normalization_fn, f_in, f_out, **kwargs):\n            normalization_fn = MODULES.get_if_str(normalization_fn)\n            normalization_fn = (\n                normalization_fn(f_out, **kwargs)\n                if normalization_fn is not None\n                else None\n            )\n            return normalization_fn\n\n        def _dropout_fn(dropout_fn, f_in, f_out, **kwargs):\n            dropout_fn = MODULES.get_if_str(dropout_fn)\n            dropout_fn = (\n                dropout_fn(**kwargs) if dropout_fn is not None else None\n            )\n            return dropout_fn\n\n        def _activation_fn(activation_fn, f_in, f_out, **kwargs):\n            activation_fn = MODULES.get_if_str(activation_fn)\n            activation_fn = (\n                activation_fn(**kwargs) if activation_fn is not None else None\n            )\n            return activation_fn\n\n        name2fn = {\n            ""layer"": _layer_fn,\n            ""norm"": _normalization_fn,\n            ""drop"": _dropout_fn,\n            ""act"": _activation_fn,\n        }\n        name2params = {\n            ""layer"": layer_fn,\n            ""norm"": norm_fn,\n            ""drop"": dropout_fn,\n            ""act"": activation_fn,\n        }\n\n        net = []\n        for i, (f_in, f_out) in enumerate(utils.pairwise(hiddens)):\n            block = []\n            for key in layer_order:\n                sub_fn = name2fn[key]\n                sub_params = deepcopy(name2params[key][i])\n\n                if isinstance(sub_params, Dict):\n                    sub_module = sub_params.pop(""module"")\n                else:\n                    sub_module = sub_params\n                    sub_params = {}\n\n                sub_block = sub_fn(sub_module, f_in, f_out, **sub_params)\n                if sub_block is not None:\n                    block.append((f""{key}"", sub_block))\n\n            block_ = OrderedDict(block)\n            block = torch.nn.Sequential(block_)\n\n            if block_.get(""act"", None) is not None:\n                activation = block_[""act""]\n                activation_init = utils.get_optimal_inner_init(\n                    nonlinearity=activation\n                )\n                block.apply(activation_init)\n\n            if residual == ""hard"" or (residual == ""soft"" and f_in == f_out):\n                block = ResidualWrapper(net=block)\n            net.append((f""block_{i}"", block))\n\n        self.net = torch.nn.Sequential(OrderedDict(net))\n\n    def forward(self, x):\n        """"""Forward call.""""""\n        x = self.net.forward(x)\n        return x\n\n\n__all__ = [""ResidualWrapper"", ""SequentialNet""]\n'"
catalyst/contrib/nn/__init__.py,0,b'# flake8: noqa\n\nfrom .criterion import *\nfrom .modules import *\nfrom .optimizers import *\nfrom .schedulers import *\n'
catalyst/contrib/scripts/__init__.py,0,b''
catalyst/contrib/scripts/check_index_model.py,0,"b'import argparse\nimport collections\n\nimport nmslib\nimport numpy as np\nimport pandas as pd\nimport tqdm\n\n\ndef build_args(parser):\n    """"""Constructs the command-line arguments.""""""\n    parser.add_argument(""--in-csv"", type=str, default=None)\n    parser.add_argument(""--in-knn"", type=str, default=None)\n\n    parser.add_argument(""--in-csv-test"", type=str, default=None)\n    parser.add_argument(""--in-npy-test"", type=str, default=None)\n    parser.add_argument(""--label-column"", type=str, default=None)\n\n    parser.add_argument(\n        ""--knn-metric"",\n        type=str,\n        default=""l2"",\n        choices=[""l2"", ""angulardist"", ""cosinesimil""],\n    )\n    parser.add_argument(\n        ""-b"",\n        ""--batch-size"",\n        default=128,\n        type=int,\n        metavar=""N"",\n        help=""mini-batch size "",\n    )\n    parser.add_argument(""-k"", ""--recall-at"", default=""1,3,5,10"", type=str)\n\n    return parser\n\n\ndef parse_args():\n    """"""Parses the command line arguments for the main method.""""""\n    parser = argparse.ArgumentParser()\n    build_args(parser)\n    args = parser.parse_args()\n    return args\n\n\ndef main(args, _=None):\n    """"""Run ``catalyst-contrib check-index-model`` script.""""""\n    print(""[==       Loading features       ==]"")\n    test_features = np.load(args.in_npy_test, mmap_mode=""r"")\n    test_df = pd.read_csv(args.in_csv_test)\n\n    print(""[==        Loading index         ==]"")\n    index = nmslib.init(\n        method=""hnsw"",\n        space=args.knn_metric,\n        data_type=nmslib.DataType.DENSE_VECTOR,\n    )\n    index.loadIndex(args.in_knn)\n    knn_df = pd.read_csv(args.in_csv)\n\n    recalls = list(map(int, args.recall_at.split("","")))\n\n    res = collections.defaultdict(lambda: [])\n    for i in tqdm.tqdm(range(0, len(test_features), args.batch_size)):\n        features_ = test_features[i : i + args.batch_size, :]\n        pred_ind_dist = index.knnQueryBatch(features_, k=max(recalls))\n        pred_inds = [x[0] for x in pred_ind_dist]\n        pred_labels = [\n            [knn_df.iloc[x_i][args.label_column] for x_i in x]\n            for x in pred_inds\n        ]\n        pred_labels = np.array(pred_labels)\n        true_labels = test_df[args.label_column].values[\n            i : i + args.batch_size, None\n        ]\n        for r_ in recalls:\n            res_ = pred_labels[:, :r_] == true_labels\n            res_ = (res_.sum(axis=1) > 0).astype(np.int32).tolist()\n            res[r_].extend(res_)\n\n    for r_ in recalls:\n        res_ = sum(res[r_]) / len(res[r_]) * 100.0\n        print(\n            ""[==      Recall@{recall_at:2}: {ratio:.4}%      ==]"".format(\n                recall_at=r_, ratio=res_\n            )\n        )\n\n\nif __name__ == ""__main__"":\n    args = parse_args()\n    main(args)\n'"
catalyst/contrib/scripts/collect_env.py,7,"b'""""""\nThis script outputs relevant system environment info.\nDiagnose your system and show basic information.\nUsed to get detail info for better bug reporting.\n\nSource:\nhttps://github.com/pytorch/pytorch/blob/master/torch/utils/collect_env.py\n""""""\n\nimport argparse\nfrom collections import namedtuple\nimport locale\nimport os\nimport re\nimport subprocess\nimport sys\n\ntry:\n    import torch\n\n    TORCH_AVAILABLE = True\nexcept (ImportError, NameError, AttributeError):\n    TORCH_AVAILABLE = False\n\ntry:\n    import catalyst\n\n    CATALYST_AVAILABLE = True\nexcept (ImportError, NameError, AttributeError):\n    CATALYST_AVAILABLE = False\n\ntry:\n    import tensorflow\n\n    TENSORFLOW_AVAILABLE = True\nexcept (ImportError, NameError, AttributeError):\n    TENSORFLOW_AVAILABLE = False\n\ntry:\n    import tensorboard\n\n    TENSORBOARD_AVAILABLE = True\nexcept (ImportError, NameError, AttributeError):\n    TENSORBOARD_AVAILABLE = False\n\n# System Environment Information\nSystemEnv = namedtuple(\n    ""SystemEnv"",\n    [\n        ""catalyst_version"",\n        ""torch_version"",\n        ""is_debug_build"",\n        ""tensorflow_version"",\n        ""tensorboard_version"",\n        ""cuda_compiled_version"",\n        ""gcc_version"",\n        ""cmake_version"",\n        ""os"",\n        ""python_version"",\n        ""is_cuda_available"",\n        ""cuda_runtime_version"",\n        ""nvidia_driver_version"",\n        ""nvidia_gpu_models"",\n        ""cudnn_version"",\n        ""pip_version"",  # ""pip"" or ""pip3""\n        ""pip_packages"",\n        ""conda_packages"",\n    ],\n)\n\n\ndef build_args(parser):\n    """"""Constructs the command-line arguments.""""""\n    return parser\n\n\ndef parse_args():\n    """"""Parses the command line arguments for the main method.""""""\n    parser = argparse.ArgumentParser()\n    build_args(parser)\n    args = parser.parse_args()\n    return args\n\n\ndef run(command):\n    """"""Returns (return-code, stdout, stderr)""""""\n    p = subprocess.Popen(\n        command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True\n    )\n    output, err = p.communicate()\n    rc = p.returncode\n    enc = locale.getpreferredencoding()\n    output = output.decode(enc)\n    err = err.decode(enc)\n    return rc, output.strip(), err.strip()\n\n\ndef run_and_read_all(run_lambda, command):\n    """"""\n    Runs command using run_lambda; reads and returns entire output if rc is 0\n    """"""\n    rc, out, _ = run_lambda(command)\n    if rc != 0:\n        return None\n    return out\n\n\ndef run_and_parse_first_match(run_lambda, command, regex):\n    """"""\n    Runs command using run_lambda, returns the first regex match if it exists\n    """"""\n    rc, out, _ = run_lambda(command)\n    if rc != 0:\n        return None\n    match = re.search(regex, out)\n    if match is None:\n        return None\n    return match.group(1)\n\n\ndef get_conda_packages(run_lambda):\n    """"""Returns conda packages""""""\n    if get_platform() == ""win32"":\n        grep_cmd = (\n            r""findstr /R \'torch numpy cudatoolkit soumith mkl magma ""\n            r""catalyst tensorflow tensorboard\'""\n        )\n    else:\n        grep_cmd = (\n            r""grep \'torch\\|numpy\\|cudatoolkit\\|soumith\\|mkl\\|magma\\|""\n            r""catalyst\\|tensorflow\\|tensorboard\'""\n        )\n    conda = os.environ.get(""CONDA_EXE"", ""conda"")\n    out = run_and_read_all(run_lambda, conda + "" list | "" + grep_cmd)\n    if out is None:\n        return out\n    # Comment starting at beginning of line\n    comment_regex = re.compile(r""^#.*\\n"")\n    return re.sub(comment_regex, """", out)\n\n\ndef get_gcc_version(run_lambda):\n    """"""Returns GCC version""""""\n    return run_and_parse_first_match(run_lambda, ""gcc --version"", r""gcc (.*)"")\n\n\ndef get_cmake_version(run_lambda):\n    """"""Returns cmake version""""""\n    return run_and_parse_first_match(\n        run_lambda, ""cmake --version"", r""cmake (.*)""\n    )\n\n\ndef get_nvidia_driver_version(run_lambda):\n    """"""Returns nvidia driver version""""""\n    if get_platform() == ""darwin"":\n        cmd = ""kextstat | grep -i cuda""\n        return run_and_parse_first_match(\n            run_lambda, cmd, r""com[.]nvidia[.]CUDA [(](.*?)[)]""\n        )\n    smi = get_nvidia_smi()\n    return run_and_parse_first_match(\n        run_lambda, smi, r""Driver Version: (.*?) ""\n    )\n\n\ndef get_gpu_info(run_lambda):\n    """"""Returns GPU info""""""\n    if get_platform() == ""darwin"":\n        if TORCH_AVAILABLE and torch.cuda.is_available():\n            return torch.cuda.get_device_name(None)\n        return None\n    smi = get_nvidia_smi()\n    uuid_regex = re.compile(r"" \\(UUID: .+?\\)"")\n    rc, out, _ = run_lambda(smi + "" -L"")\n    if rc != 0:\n        return None\n    # Anonymize GPUs by removing their UUID\n    return re.sub(uuid_regex, """", out)\n\n\ndef get_running_cuda_version(run_lambda):\n    """"""Returns CUDA version""""""\n    return run_and_parse_first_match(run_lambda, ""nvcc --version"", r""V(.*)$"")\n\n\ndef get_cudnn_version(run_lambda):\n    """"""\n    This will return a list of libcudnn.so;\n    it""s hard to tell which one is being used\n    """"""\n    if get_platform() == ""win32"":\n        cudnn_cmd = ""where /R \'%CUDA_PATH%\\\\bin\' cudnn*.dll""\n    elif get_platform() == ""darwin"":\n        # CUDA libraries and drivers can be found in /usr/local/cuda/. See\n        # https://docs.nvidia.com/cuda/cuda-installation-guide-mac-os-x/index.html#install\n        # https://docs.nvidia.com/deeplearning/sdk/cudnn-install/index.html#installmac\n        # Use CUDNN_LIBRARY when cudnn library is installed elsewhere.\n        cudnn_cmd = ""ls /usr/local/cuda/lib/libcudnn*""\n    else:\n        cudnn_cmd = ""ldconfig -p | grep libcudnn | rev | cut -d"" "" -f1 | rev""\n    rc, out, _ = run_lambda(cudnn_cmd)\n    # find will return 1 if there are permission errors or if not found\n    if len(out) == 0 or (rc != 1 and rc != 0):\n        lib = os.environ.get(""CUDNN_LIBRARY"")\n        if lib is not None and os.path.isfile(lib):\n            return os.path.realpath(lib)\n        return None\n    files = set()\n    for fn in out.split(""\\n""):\n        fn = os.path.realpath(fn)  # eliminate symbolic links\n        if os.path.isfile(fn):\n            files.add(fn)\n    if not files:\n        return None\n    # Alphabetize the result because the order is non-deterministic otherwise\n    files = sorted(files)\n    if len(files) == 1:\n        return files[0]\n    result = ""\\n"".join(files)\n    return ""Probably one of the following:\\n{}"".format(result)\n\n\ndef get_nvidia_smi():\n    """"""Returns nvidia-smi""""""\n    # Note: nvidia-smi is currently available only on Windows and Linux\n    smi = ""nvidia-smi""\n    if get_platform() == ""win32"":\n        smi = ""\'C:\\\\Program Files\\\\NVIDIA Corporation\\\\NVSMI\\\\%s\'"" % smi\n    return smi\n\n\ndef get_platform():\n    """"""Returns platform info""""""\n    if sys.platform.startswith(""linux""):\n        return ""linux""\n    elif sys.platform.startswith(""win32""):\n        return ""win32""\n    elif sys.platform.startswith(""cygwin""):\n        return ""cygwin""\n    elif sys.platform.startswith(""darwin""):\n        return ""darwin""\n    else:\n        return sys.platform\n\n\ndef get_mac_version(run_lambda):\n    """"""Returns Mac version""""""\n    return run_and_parse_first_match(\n        run_lambda, ""sw_vers -productVersion"", r""(.*)""\n    )\n\n\ndef get_windows_version(run_lambda):\n    """"""Returns windows version""""""\n    return run_and_read_all(\n        run_lambda, ""wmic os get Caption | findstr /v Caption""\n    )\n\n\ndef get_lsb_version(run_lambda):\n    """"""Returns lsb version""""""\n    return run_and_parse_first_match(\n        run_lambda, ""lsb_release -a"", r""Description:\\t(.*)""\n    )\n\n\ndef check_release_file(run_lambda):\n    """"""Checks release file""""""\n    return run_and_parse_first_match(\n        run_lambda, ""cat /etc/*-release"", r""PRETTY_NAME=\'(.*)\'""\n    )\n\n\ndef get_os(run_lambda):\n    """"""\n    Returns OS info.\n    """"""\n    platform = get_platform()\n\n    if platform == ""win32"" or platform == ""cygwin"":\n        return get_windows_version(run_lambda)\n\n    if platform == ""darwin"":\n        version = get_mac_version(run_lambda)\n        if version is None:\n            return None\n        return ""Mac OSX {}"".format(version)\n\n    if platform == ""linux"":\n        # Ubuntu/Debian based\n        desc = get_lsb_version(run_lambda)\n        if desc is not None:\n            return desc\n\n        # Try reading /etc/*-release\n        desc = check_release_file(run_lambda)\n        if desc is not None:\n            return desc\n\n        return platform\n\n    # Unknown platform\n    return platform\n\n\ndef get_pip_packages(run_lambda):\n    """"""\n    Returns `pip list` output. Note: will also find conda-installed pytorch\n    and numpy packages.\n    """"""\n    # People generally have `pip` as `pip` or `pip3`\n    def run_with_pip(pip):\n        if get_platform() == ""win32"":\n            grep_cmd = (\n                r""findstr /R \'numpy torch catalyst tensorflow tensorboard\'""\n            )\n        else:\n            grep_cmd = (\n                r""grep \'torch\\|numpy\\|catalyst\\|tensorflow\\|tensorboard\'""\n            )\n        return run_and_read_all(\n            run_lambda, pip + "" list --format=freeze | "" + grep_cmd\n        )\n\n    # Try to figure out if the user is running pip or pip3.\n    out2 = run_with_pip(""pip"")\n    out3 = run_with_pip(""pip3"")\n\n    num_pips = len([x for x in [out2, out3] if x is not None])\n    if num_pips == 0:\n        return ""pip"", out2\n\n    if num_pips == 1:\n        if out2 is not None:\n            return ""pip"", out2\n        return ""pip3"", out3\n\n    # num_pips is 2. Return pip3 by default b/c that most likely\n    # is the one associated with Python 3\n    return ""pip3"", out3\n\n\ndef get_env_info():\n    """"""Returns main SystemEnv info""""""\n    run_lambda = run\n    pip_version, pip_list_output = get_pip_packages(run_lambda)\n\n    if TORCH_AVAILABLE:\n        version_str = torch.__version__\n        debug_mode_str = torch.version.debug\n        cuda_available_str = torch.cuda.is_available()\n        cuda_version_str = torch.version.cuda\n    else:\n        version_str = (\n            debug_mode_str\n        ) = cuda_available_str = cuda_version_str = ""N/A""\n\n    catalyst_str = catalyst.__version__ if CATALYST_AVAILABLE else ""N/A""\n    tensorflow_str = tensorflow.__version__ if TENSORFLOW_AVAILABLE else ""N/A""\n    tensorboard_str = (\n        tensorboard.__version__ if TENSORBOARD_AVAILABLE else ""N/A""\n    )\n\n    return SystemEnv(\n        catalyst_version=catalyst_str,\n        torch_version=version_str,\n        is_debug_build=debug_mode_str,\n        tensorflow_version=tensorflow_str,\n        tensorboard_version=tensorboard_str,\n        python_version=""{}.{}"".format(\n            sys.version_info[0], sys.version_info[1]\n        ),\n        is_cuda_available=cuda_available_str,\n        cuda_compiled_version=cuda_version_str,\n        cuda_runtime_version=get_running_cuda_version(run_lambda),\n        nvidia_gpu_models=get_gpu_info(run_lambda),\n        nvidia_driver_version=get_nvidia_driver_version(run_lambda),\n        cudnn_version=get_cudnn_version(run_lambda),\n        pip_version=pip_version,\n        pip_packages=pip_list_output,\n        conda_packages=get_conda_packages(run_lambda),\n        os=get_os(run_lambda),\n        gcc_version=get_gcc_version(run_lambda),\n        cmake_version=get_cmake_version(run_lambda),\n    )\n\n\nenv_info_fmt = """"""\nCatalyst version: {catalyst_version}\nPyTorch version: {torch_version}\nIs debug build: {is_debug_build}\nCUDA used to build PyTorch: {cuda_compiled_version}\nTensorFlow version: {tensorflow_version}\nTensorBoard version: {tensorboard_version}\n\nOS: {os}\nGCC version: {gcc_version}\nCMake version: {cmake_version}\n\nPython version: {python_version}\nIs CUDA available: {is_cuda_available}\nCUDA runtime version: {cuda_runtime_version}\nGPU models and configuration: {nvidia_gpu_models}\nNvidia driver version: {nvidia_driver_version}\ncuDNN version: {cudnn_version}\n\nVersions of relevant libraries:\n{pip_packages}\n{conda_packages}\n"""""".strip()\n\n\ndef pretty_str(envinfo):  # noqa: C901\n    """"""Pretty formatting for `env_info_fmt` string""""""  # noqa: D202\n\n    def replace_nones(dct, replacement=""Could not collect""):\n        for key in dct.keys():\n            if dct[key] is not None:\n                continue\n            dct[key] = replacement\n        return dct\n\n    def replace_bools(dct, true=""Yes"", false=""No""):\n        for key in dct.keys():\n            if dct[key] is True:\n                dct[key] = true\n            elif dct[key] is False:\n                dct[key] = false\n        return dct\n\n    def prepend(text, tag=""[prepend]""):\n        lines = text.split(""\\n"")\n        updated_lines = [tag + line for line in lines]\n        return ""\\n"".join(updated_lines)\n\n    def replace_if_empty(text, replacement=""No relevant packages""):\n        if text is not None and len(text) == 0:\n            return replacement\n        return text\n\n    def maybe_start_on_next_line(string):\n        # If `string` is multiline, prepend a \\n to it.\n        if string is not None and len(string.split(""\\n"")) > 1:\n            return ""\\n{}\\n"".format(string)\n        return string\n\n    mutable_dict = envinfo._asdict()\n\n    # If nvidia_gpu_models is multiline, start on the next line\n    mutable_dict[""nvidia_gpu_models""] = maybe_start_on_next_line(\n        envinfo.nvidia_gpu_models\n    )\n\n    # If the machine doesn""t have CUDA, report some fields as ""No CUDA""\n    dynamic_cuda_fields = [\n        ""cuda_runtime_version"",\n        ""nvidia_gpu_models"",\n        ""nvidia_driver_version"",\n    ]\n    all_cuda_fields = dynamic_cuda_fields + [""cudnn_version""]\n    all_dynamic_cuda_fields_missing = all(\n        mutable_dict[field] is None for field in dynamic_cuda_fields\n    )\n    if (\n        TORCH_AVAILABLE\n        and not torch.cuda.is_available()\n        and all_dynamic_cuda_fields_missing\n    ):\n        for field in all_cuda_fields:\n            mutable_dict[field] = ""No CUDA""\n        if envinfo.cuda_compiled_version is None:\n            mutable_dict[""cuda_compiled_version""] = ""None""\n\n    # Replace True with Yes, False with No\n    mutable_dict = replace_bools(mutable_dict)\n\n    # Replace all None objects with ""Could not collect""\n    mutable_dict = replace_nones(mutable_dict)\n\n    # If either of these are """", replace with ""No relevant packages""\n    mutable_dict[""pip_packages""] = replace_if_empty(\n        mutable_dict[""pip_packages""]\n    )\n    mutable_dict[""conda_packages""] = replace_if_empty(\n        mutable_dict[""conda_packages""]\n    )\n\n    # Tag conda and pip packages with a prefix\n    # If they were previously None,\n    # they""ll show up as ie ""[conda] Could not collect""\n    if mutable_dict[""pip_packages""]:\n        mutable_dict[""pip_packages""] = prepend(\n            mutable_dict[""pip_packages""], ""[{}] "".format(envinfo.pip_version)\n        )\n    if mutable_dict[""conda_packages""]:\n        mutable_dict[""conda_packages""] = prepend(\n            mutable_dict[""conda_packages""], ""[conda] ""\n        )\n    return env_info_fmt.format(**mutable_dict)\n\n\ndef get_pretty_env_info():\n    """"""Pretty formatting for env info""""""\n    return pretty_str(get_env_info())\n\n\ndef main(args, _=None):\n    """"""Run ``catalyst-contrib collect-env`` script.""""""\n    print(""Collecting environment information..."")\n    output = get_pretty_env_info()\n    print(output)\n\n\nif __name__ == ""__main__"":\n    args = parse_args()\n    main(args)\n'"
catalyst/contrib/scripts/create_index_model.py,0,"b'import argparse\nimport pickle\n\nimport nmslib\nimport numpy as np\nfrom sklearn.decomposition import PCA\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import Normalizer, StandardScaler\n\n\ndef build_args(parser):\n    """"""Constructs the command-line arguments.""""""\n    parser.add_argument(""--in-npy"", type=str, default=None)\n\n    parser.add_argument(""--n-hidden"", type=int, default=None)\n    parser.add_argument(\n        ""--knn-metric"",\n        type=str,\n        default=""l2"",\n        choices=[""l2"", ""angulardist"", ""cosinesimil""],\n    )\n\n    parser.add_argument(""--out-npy"", type=str, default=None)\n    parser.add_argument(""--out-pipeline"", type=str, default=None)\n    parser.add_argument(""--out-knn"", type=str, default=None)\n\n    parser.add_argument(""--in-npy-test"", type=str, default=None)\n    parser.add_argument(""--out-npy-test"", type=str, default=None)\n\n    return parser\n\n\ndef parse_args():\n    """"""Parses the command line arguments for the main method.""""""\n    parser = argparse.ArgumentParser()\n    build_args(parser)\n    args = parser.parse_args()\n    return args\n\n\ndef main(args, _=None):\n    """"""Run ``catalyst-contrib create-index-model`` script.""""""\n    print(""[==       Loading features       ==]"")\n    features = None\n    for in_npy in args.in_npy.split("",""):\n        features_ = np.load(in_npy, mmap_mode=""r"")\n        if features is None:\n            features = features_\n        else:\n            features = np.concatenate((features, features_), axis=0)\n\n    if args.n_hidden is not None:\n        pipeline = Pipeline(\n            [\n                (""scale"", StandardScaler()),\n                (""pca"", PCA(n_components=args.n_hidden, random_state=42)),\n                (""normalize"", Normalizer()),\n            ]\n        )\n\n        print(""[==     Transforming features    ==]"")\n        features = pipeline.fit_transform(features)\n        np.save(args.out_npy, features)\n\n        print(\n            ""[ Explained variance ratio: {ratio:.4} ]"".format(\n                ratio=pipeline.named_steps[\n                    ""pca""\n                ].explained_variance_ratio_.sum()\n            )\n        )\n\n        print(""[==        Saving pipeline       ==]"")\n        pickle.dump(pipeline, open(args.out_pipeline, ""wb""))\n\n    index = nmslib.init(\n        method=""hnsw"",\n        space=args.knn_metric,\n        data_type=nmslib.DataType.DENSE_VECTOR,\n    )\n    print(""[==  Adding features to indexer  ==]"")\n    index.addDataPointBatch(features)\n\n    print(""[==        Creating index        ==]"")\n    index.createIndex({""post"": 1}, print_progress=True)\n    print("""")\n    print(""[==         Saving index         ==]"")\n    index.saveIndex(args.out_knn)\n\n    if args.in_npy_test is not None:\n        test_features = np.load(args.in_npy_test, mmap_mode=""r"")\n        test_features = pipeline.transform(test_features)\n        np.save(args.out_npy_test, test_features)\n\n\nif __name__ == ""__main__"":\n    args = parse_args()\n    main(args)\n'"
catalyst/contrib/scripts/find_thresholds.py,0,"b'from typing import Any, Callable, Dict, List, Tuple\nimport argparse\nfrom itertools import repeat\nimport json\nfrom pathlib import Path\nfrom pprint import pprint\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.special import expit\nfrom sklearn import metrics\nfrom sklearn.model_selection import RepeatedStratifiedKFold\n\nfrom catalyst import utils\n\nBINARY_PER_CLASS_METRICS = [\n    ""accuracy_score"",\n    ""precision_score"",\n    ""recall_score"",\n    ""f1_score"",\n    ""roc_auc_score"",\n]\n\nRANK_METRICS = [\n    ""ndcg_score"",\n    ""coverage_error"",\n    ""label_ranking_loss"",\n    ""label_ranking_average_precision_score"",\n]\n\n\ndef build_args(parser):\n    """"""Constructs the command-line arguments.""""""\n    parser.add_argument(\n        ""--in-csv"",\n        type=Path,\n        help=""Path to .csv with labels column"",\n        required=True,\n    )\n    parser.add_argument(\n        ""--in-label-column"",\n        type=str,\n        help=""Column to get labels"",\n        required=False,\n        default=""labels"",\n    )\n    parser.add_argument(\n        ""--in-npy"",\n        type=Path,\n        help=""Path to .npy with class logits"",\n        required=True,\n    )\n    parser.add_argument(\n        ""--out-thresholds"",\n        type=Path,\n        help=""Path to save .json with thresholds"",\n        required=True,\n    )\n\n    parser.add_argument(\n        ""--metric"",\n        type=str,\n        help=""Metric to use"",\n        required=False,\n        choices=BINARY_PER_CLASS_METRICS,\n        default=""roc_auc_score"",\n    )\n    # parser.add_argument(\n    #     ""--ignore-label"", type=int,\n    #     required=False,\n    #     default=None\n    # )\n    parser.add_argument(\n        ""--num-splits"", type=int, help=""NUM_SPLITS"", required=False, default=5\n    )\n    parser.add_argument(\n        ""--num-repeats"",\n        type=int,\n        help=""NUM_REPEATS"",\n        required=False,\n        default=1,\n    )\n    parser.add_argument(\n        ""--num-workers"",\n        type=int,\n        help=""CPU pool size"",\n        required=False,\n        default=1,\n    )\n\n    utils.boolean_flag(parser, ""verbose"", default=False)\n    utils.boolean_flag(parser, ""sigmoid"", default=False)\n\n    return parser\n\n\ndef parse_args():\n    """"""Parses the command line arguments for the main method.""""""\n    parser = argparse.ArgumentParser()\n    build_args(parser)\n    args = parser.parse_args()\n    return args\n\n\ndef get_binary_labels(labels: np.array, label: int, ignore_label: int = None):\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    binary_labels = labels == label\n    if ignore_label is not None:\n        binary_labels[labels == ignore_label] = 0\n    return (binary_labels).astype(int)\n\n\ndef find_best_split_threshold(\n    y_pred: np.array, y_true: np.array, metric: Callable,\n):\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    thresholds = np.linspace(0.0, 1.0, num=100)\n    metric_values = []\n    for t in thresholds:\n        predictions = (y_pred >= t).astype(int)\n        if sum(predictions) > 0:\n            metric_values.append(metric(y_true, predictions))\n        else:\n            metric_values.append(0.0)\n\n    best_threshold = thresholds[np.argmax(metric_values)]\n    return best_threshold\n\n\ndef find_best_threshold(\n    y_pred: np.ndarray,\n    y_true: np.ndarray,\n    metric_fn: Callable = metrics.roc_auc_score,\n    num_splits: int = 5,\n    num_repeats: int = 1,\n    random_state: int = 42,\n):\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    rkf = RepeatedStratifiedKFold(\n        n_splits=num_splits, n_repeats=num_repeats, random_state=random_state\n    )\n    fold_thresholds = []\n    fold_metrics = {k: [] for k in BINARY_PER_CLASS_METRICS}\n\n    for train_index, test_index in rkf.split(y_true, y_true):\n        y_pred_train, y_pred_test = y_pred[train_index], y_pred[test_index]\n        y_true_train, y_true_test = y_true[train_index], y_true[test_index]\n\n        best_threshold = find_best_split_threshold(\n            y_pred_train, y_true_train, metric=metric_fn\n        )\n        best_predictions = (y_pred_test >= best_threshold).astype(int)\n\n        for metric_name in BINARY_PER_CLASS_METRICS:\n            try:\n                metric_value = metrics.__dict__[metric_name](\n                    y_true_test, best_predictions\n                )\n            except ValueError:\n                metric_value = 0.0\n\n            fold_metrics[metric_name].append(metric_value)\n        fold_thresholds.append(best_threshold)\n\n    fold_best_threshold = np.mean(fold_thresholds)\n    for metric_name in fold_metrics:\n        fold_metrics[metric_name] = np.mean(fold_metrics[metric_name])\n\n    return fold_best_threshold, fold_metrics\n\n\ndef wrap_find_best_threshold(args: Tuple[Any]):\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    class_id, function_args = args[0], args[1:]\n    threshold, metrics = find_best_threshold(*function_args)\n    return class_id, threshold, metrics\n\n\ndef optimize_thresholds(\n    predictions: np.ndarray,\n    labels: np.ndarray,\n    classes: List[int],\n    metric_fn: Callable = metrics.roc_auc_score,\n    num_splits: int = 5,\n    num_repeats: int = 1,\n    num_workers: int = 0,\n    ignore_label: int = None,\n) -> Tuple[Dict, Dict]:\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    pool = utils.get_pool(num_workers)\n\n    predictions_ = predictions.copy()\n\n    predictions_list, labels_list = [], []\n    for cls in classes:\n        predictions_list.append(predictions_[:, cls])\n        labels_list.append(\n            get_binary_labels(labels, cls, ignore_label=ignore_label)\n        )\n\n    results = utils.tqdm_parallel_imap(\n        wrap_find_best_threshold,\n        zip(\n            classes,\n            predictions_list,\n            labels_list,\n            repeat(metric_fn),\n            repeat(num_splits),\n            repeat(num_repeats),\n        ),\n        pool,\n    )\n    results = [(r[1], r[2]) for r in sorted(results, key=lambda x: x[0])]\n\n    result_thresholds = [r[0] for r in results]\n    result_metrics = [r[1] for r in results]\n    class_thresholds = {c: t for (c, t) in zip(classes, result_thresholds)}\n    class_metrics = {c: m for (c, m) in zip(classes, result_metrics)}\n    return class_thresholds, class_metrics\n\n\ndef get_model_confidences(\n    confidences: np.ndarray,\n    thresholds: Dict[int, float] = None,\n    classes: List[int] = None,\n):\n    """"""\n    @TODO: Docs (add description). Contribution is welcome\n\n    Args:\n        confidences (np.ndarray): model predictions of shape\n            [dataset_len; class_confidences]\n        thresholds (Dict[int, float]): thresholds for each class\n        classes (List[int]): classes of interest for evaluation\n    """"""\n    if classes is not None:\n        classes = np.array(classes)\n        confidences = confidences[:, classes]\n\n    confidences_th = confidences.copy()\n    if thresholds is not None:\n        assert confidences.shape[1] == len(thresholds)\n        thresholds = np.array(list(thresholds.values()))\n        confidences_th = confidences - thresholds\n\n    return confidences_th\n\n\ndef score_model_coverage(confidences: np.ndarray, labels: np.ndarray):\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    candidates = np.argsort(-confidences, axis=1)\n    confidences = -np.sort(-confidences, axis=1)\n    candidates[confidences < 0] = -1\n    labels = labels[:, None]\n\n    coverage_metrics = {}\n\n    for top_k in [1, 3, 5]:\n        metric = (candidates[:, :top_k] == labels).sum(axis=1).mean()\n        coverage_metrics[f""Recall@{top_k:02d}""] = metric\n\n    return coverage_metrics\n\n\ndef _sort_dict_by_keys(disordered: Dict):\n    key = lambda item: item[0]\n    sorted_dict = {k: v for k, v in sorted(disordered.items(), key=key)}\n    return sorted_dict\n\n\ndef _save_json(dct: Dict, outpath: Path, suffix: str = None):\n    outpath = str(outpath)\n    if suffix is not None:\n        outpath = outpath.replace("".json"", f""{suffix}.json"")\n    dct = _sort_dict_by_keys({str(k): v for k, v in dct.copy().items()})\n    with open(outpath, ""w"") as fout:\n        json.dump(dct, fout, ensure_ascii=False, indent=4)\n\n\ndef main(args, _=None):\n    """"""Run ``catalyst-contrib find-thresholds`` script.""""""\n    predictions = expit(np.load(args.in_npy))\n    if args.sigmoid:\n        predictions = expit(predictions)\n    labels = pd.read_csv(args.in_csv)[args.in_label_column].values\n    classes = list(set(labels))  # - set([args.ignore_label]))\n\n    assert args.metric in metrics.__dict__.keys()\n    metric_fn = metrics.__dict__[args.metric]\n\n    class_thresholds, class_metrics = optimize_thresholds(\n        predictions=predictions,\n        labels=labels,\n        classes=classes,\n        metric_fn=metric_fn,\n        num_splits=args.num_splits,\n        num_repeats=args.num_repeats,\n        ignore_label=None,  # args.ignore_label,\n        num_workers=args.num_workers,\n    )\n    _save_json(class_thresholds, outpath=args.out_thresholds)\n\n    class_metrics[""_mean""] = {\n        key_metric: np.mean(\n            [\n                class_metrics[key_class][key_metric]\n                for key_class in class_metrics.keys()\n            ]\n        )\n        for key_metric in BINARY_PER_CLASS_METRICS\n    }\n\n    _save_json(class_metrics, args.out_thresholds, suffix="".class.metrics"")\n\n    if args.verbose:\n        print(""CLASS METRICS"")\n        pprint(class_metrics)\n        print(""CLASS THRESHOLDS"")\n        pprint(class_thresholds)\n\n    labels_scores = np.zeros(predictions.shape)\n    labels_scores[:, labels] = 1.0\n    for class_thresholds_ in [None, class_thresholds]:\n        thresholds_used = class_thresholds_ is not None\n\n        confidences = get_model_confidences(\n            confidences=predictions,\n            thresholds=class_thresholds_,\n            classes=classes,\n        )\n\n        rank_metrics = {\n            key: metrics.__dict__[key](labels_scores, confidences)\n            for key in RANK_METRICS\n        }\n        postfix = (\n            "".rank.metrics""\n            if not thresholds_used\n            else "".rank.metrics.thresholds""\n        )\n        _save_json(rank_metrics, args.out_thresholds, suffix=postfix)\n\n        coverage_metrics = score_model_coverage(confidences, labels)\n        postfix = (\n            "".coverage.metrics.json""\n            if not thresholds_used\n            else "".coverage.metrics.thresholds.json""\n        )\n        _save_json(coverage_metrics, args.out_thresholds, suffix=postfix)\n\n        if args.verbose:\n            print(\n                ""RANK METRICS""\n                if not thresholds_used\n                else ""RANK METRICS WITH THRESHOLD""\n            )\n            pprint(rank_metrics)\n            print(\n                ""COVERAGE METRICS""\n                if not thresholds_used\n                else ""COVERAGE METRICS WITH THRESHOLD""\n            )\n            pprint(coverage_metrics)\n\n\nif __name__ == ""__main__"":\n    args = parse_args()\n    main(args)\n'"
catalyst/contrib/tools/__init__.py,0,b''
catalyst/contrib/tools/tensorboard.py,1,"b'# isort:skip_file\n""""""\nTensorboard readers:\n    * :py:class:`EventsFileReader`\n    * :py:class:`SummaryReader`\n""""""\nfrom typing import BinaryIO, Optional, Union\nfrom collections import namedtuple\nfrom collections.abc import Iterable\nimport os\nfrom pathlib import Path\nimport struct\n\nif os.environ.get(""CRC32C_SW_MODE"", None) is None:\n    os.environ[""CRC32C_SW_MODE""] = ""auto""\nfrom crc32c import crc32 as crc32c  # noqa: E402\n\nimport numpy as np  # noqa: E402\n\n# Native tensorboard support from 1.2.0 version of PyTorch\nfrom torch import __version__ as torch_version  # noqa: E402\nfrom packaging import version  # noqa: E402\n\nif version.parse(torch_version) < version.parse(""1.2.0""):\n    from tensorboardX import SummaryWriter as tensorboardX_SummaryWriter\n\n    SummaryWriter = tensorboardX_SummaryWriter\nelse:\n    from torch.utils.tensorboard import SummaryWriter as torch_SummaryWriter\n\n    SummaryWriter = torch_SummaryWriter\nif version.parse(torch_version) < version.parse(""1.2.0""):\n    from tensorboardX.proto.event_pb2 import Event\nelse:\n    from tensorboard.compat.proto.event_pb2 import Event\n\n\ndef _u32(x):\n    return x & 0xFFFFFFFF\n\n\ndef _masked_crc32c(data):\n    x = _u32(crc32c(data))\n    return _u32(((x >> 15) | _u32(x << 17)) + 0xA282EAD8)\n\n\nclass EventReadingException(Exception):\n    """"""An exception that correspond to an event file reading error.""""""\n\n\nclass EventsFileReader(Iterable):\n    """"""An iterator over a Tensorboard events file.""""""\n\n    def __init__(self, events_file: BinaryIO):\n        """"""Initialize an iterator over an events file.\n\n        Args:\n            events_file: An opened file-like object.\n        """"""\n        self._events_file = events_file\n\n    def _read(self, size: int) -> Optional[bytes]:\n        """"""Read exactly next `size` bytes from the current stream.\n\n        Args:\n            size: A size in bytes to be read.\n\n        Returns:\n            A `bytes` object with read data or `None` on EOF.\n\n        """"""\n        data = self._events_file.read(size)\n        if data is None:\n            raise NotImplementedError(\n                ""Reading of a stream in non-blocking mode""\n            )\n        if 0 < len(data) < size:\n            raise EventReadingException(\n                ""The size of read data is less than requested size""\n            )\n        if len(data) == 0:\n            return None\n        return data\n\n    def _read_and_check(self, size: int) -> Optional[bytes]:\n        """"""Read and check data described by a format string.\n\n        Args:\n            size:  A size in bytes to be read.\n\n        Returns:\n             A decoded number.\n        """"""\n        data = self._read(size)\n        if data is None:\n            return None\n        checksum_size = struct.calcsize(""I"")\n        checksum = struct.unpack(""I"", self._read(checksum_size))[0]\n        checksum_computed = _masked_crc32c(data)\n        if checksum != checksum_computed:\n            raise EventReadingException(\n                ""Invalid checksum. {checksum} != {crc32}"".format(\n                    checksum=checksum, crc32=checksum_computed\n                )\n            )\n        return data\n\n    def __iter__(self) -> Event:\n        """"""Iterates over events in the current events file.\n\n        Returns:\n            An Event object\n        """"""\n        while True:\n            header_size = struct.calcsize(""Q"")\n            header = self._read_and_check(header_size)\n            if header is None:\n                break\n            event_size = struct.unpack(""Q"", header)[0]\n            event_raw = self._read_and_check(event_size)\n            if event_raw is None:\n                raise EventReadingException(""Unexpected end of events file"")\n            event = Event()\n            event.ParseFromString(event_raw)\n            yield event\n\n\nSummaryItem = namedtuple(\n    ""SummaryItem"", [""tag"", ""step"", ""wall_time"", ""value"", ""type""]\n)\n\n\ndef _get_scalar(value) -> Optional[np.ndarray]:\n    """"""Decode an scalar event.\n\n    Args:\n        value: A value field of an event\n\n    Returns:\n        Decoded scalar\n    """"""\n    if value.HasField(""simple_value""):\n        return value.simple_value\n    return None\n\n\nclass SummaryReader(Iterable):\n    """"""Iterates over events in all the files in the current logdir.\n\n    .. note::\n        Only scalars are supported at the moment.\n    """"""\n\n    _DECODERS = {\n        ""scalar"": _get_scalar,\n    }\n\n    def __init__(\n        self,\n        logdir: Union[str, Path],\n        tag_filter: Optional[Iterable] = None,\n        types: Iterable = (""scalar"",),\n    ):\n        """"""Initalize new summary reader.\n\n        Args:\n            logdir: A directory with Tensorboard summary data\n            tag_filter: A list of tags to leave (`None` for all)\n            types: A list of types to get.\n            Only ""scalar"" and ""image"" types are allowed at the moment.\n        """"""\n        self._logdir = Path(logdir)\n\n        self._tag_filter = set(tag_filter) if tag_filter is not None else None\n        self._types = set(types)\n        self._check_type_names()\n\n    def _check_type_names(self):\n        if self._types is None:\n            return\n        if not all(\n            type_name in self._DECODERS.keys() for type_name in self._types\n        ):\n            raise ValueError(""Invalid type name"")\n\n    def _decode_events(self, events: Iterable) -> Optional[SummaryItem]:\n        """"""Convert events to `SummaryItem` instances.\n\n        Args:\n            events: An iterable with events objects\n\n        Returns:\n            A generator with decoded events\n            or `None`s if an event can""t be decoded\n        """"""\n        for event in events:\n            if not event.HasField(""summary""):\n                yield None\n            step = event.step\n            wall_time = event.wall_time\n            for value in event.summary.value:\n                tag = value.tag\n                for value_type in self._types:\n                    decoder = self._DECODERS[value_type]\n                    data = decoder(value)\n                    if data is not None:\n                        yield SummaryItem(\n                            tag=tag,\n                            step=step,\n                            wall_time=wall_time,\n                            value=data,\n                            type=value_type,\n                        )\n                else:\n                    yield None\n\n    def _check_tag(self, tag: str) -> bool:\n        """"""Check if a tag matches the current tag filter.\n\n        Args:\n            tag: A string with tag\n\n        Returns:\n            A boolean value.\n        """"""\n        return self._tag_filter is None or tag in self._tag_filter\n\n    def __iter__(self) -> SummaryItem:\n        """"""Iterate over events in all the files in the current logdir.\n\n        Returns:\n            A generator with `SummaryItem` objects\n        """"""\n        log_files = sorted(f for f in self._logdir.glob(""*"") if f.is_file())\n        for file_path in log_files:\n            with open(file_path, ""rb"") as f:\n                reader = EventsFileReader(f)\n                yield from (\n                    item\n                    for item in self._decode_events(reader)\n                    if item is not None\n                    and self._check_tag(item.tag)\n                    and item.type in self._types\n                )\n'"
catalyst/contrib/utils/__init__.py,0,"b'# flake8: noqa\n# isort:skip_file\n\nimport logging\nimport os\n\nlogger = logging.getLogger(__name__)\n\nfrom catalyst.tools import settings\n\nfrom .argparse import boolean_flag\nfrom .compression import pack, pack_if_needed, unpack, unpack_if_needed\nfrom .confusion_matrix import (\n    calculate_tp_fp_fn,\n    calculate_confusion_matrix_from_arrays,\n    calculate_confusion_matrix_from_tensors,\n)\nfrom .cv import *\nfrom .dataset import create_dataset, split_dataset_train_test, create_dataframe\nfrom .misc import (\n    args_are_not_none,\n    make_tuple,\n    pairwise,\n)\nfrom .nlp import *\nfrom .pandas import (\n    dataframe_to_list,\n    folds_to_list,\n    split_dataframe_train_test,\n    split_dataframe_on_folds,\n    split_dataframe_on_stratified_folds,\n    split_dataframe_on_column_folds,\n    map_dataframe,\n    separate_tags,\n    get_dataset_labeling,\n    split_dataframe,\n    merge_multiple_fold_csv,\n    read_multiple_dataframes,\n    read_csv_data,\n    balance_classes,\n)\nfrom .parallel import parallel_imap, tqdm_parallel_imap, get_pool\nfrom .serialization import deserialize, serialize\n\ntry:\n    import plotly  # noqa: F401\n    from .plotly import plot_tensorboard_log, plot_metrics\nexcept ImportError as ex:\n    if settings.plotly_required:\n        logger.warning(\n            ""plotly not available, to install plotly,""\n            "" run `pip install plotly`.""\n        )\n        raise ex\n\nfrom .visualization import (\n    plot_confusion_matrix,\n    render_figure_to_tensor,\n)\n'"
catalyst/contrib/utils/argparse.py,0,"b'from typing import Optional\nimport argparse\n\n\ndef boolean_flag(\n    parser: argparse.ArgumentParser,\n    name: str,\n    default: Optional[bool] = False,\n    help: str = None,\n    shorthand: str = None,\n) -> None:\n    """"""Add a boolean flag to a parser inplace.\n\n    Examples:\n        >>> parser = argparse.ArgumentParser()\n        >>> boolean_flag(\n        >>>     parser, ""flag"", default=False, help=""some flag"", shorthand=""f""\n        >>> )\n\n    Args:\n        parser (argparse.ArgumentParser): parser to add the flag to\n        name (str): argument name\n            --<name> will enable the flag,\n            while --no-<name> will disable it\n        default (bool, optional): default value of the flag\n        help (str): help string for the flag\n        shorthand (str): shorthand string for the argument\n    """"""\n    dest = name.replace(""-"", ""_"")\n    names = [""--"" + name]\n    if shorthand is not None:\n        names.append(""-"" + shorthand)\n    parser.add_argument(\n        *names, action=""store_true"", default=default, dest=dest, help=help\n    )\n    parser.add_argument(""--no-"" + name, action=""store_false"", dest=dest)\n\n\n__all__ = [""boolean_flag""]\n'"
catalyst/contrib/utils/compression.py,0,"b'import base64\nimport logging\n\nimport numpy as np\nfrom six import string_types\n\nfrom catalyst.tools import settings\n\nfrom .serialization import deserialize, serialize\n\nlogger = logging.getLogger(__name__)\n\nif settings.use_lz4:\n    try:\n        import lz4.frame\n    except ImportError as ex:\n        logger.warning(\n            ""lz4 not available, to install lz4, run `pip install lz4`.""\n        )\n        raise ex\n\n\ndef is_compressed(data):\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    return isinstance(data, (bytes, string_types))\n\n\ndef compress(data):\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    if settings.use_lz4:\n        data = serialize(data)\n        data = lz4.frame.compress(data)\n        data = base64.b64encode(data).decode(""ascii"")\n    return data\n\n\ndef compress_if_needed(data):\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    if isinstance(data, np.ndarray):\n        data = compress(data)\n    return data\n\n\ndef decompress(data):\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    if settings.use_lz4:\n        data = base64.b64decode(data)\n        data = lz4.frame.decompress(data)\n        data = deserialize(data)\n    return data\n\n\ndef decompress_if_needed(data):\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    if is_compressed(data):\n        data = decompress(data)\n    return data\n\n\nif settings.use_lz4:\n    pack = compress\n    pack_if_needed = compress_if_needed\n    unpack = decompress\n    unpack_if_needed = decompress_if_needed\nelse:\n    pack = serialize\n    pack_if_needed = serialize\n    unpack = deserialize\n    unpack_if_needed = deserialize\n\n__all__ = [""pack"", ""pack_if_needed"", ""unpack"", ""unpack_if_needed""]\n'"
catalyst/contrib/utils/confusion_matrix.py,2,"b'import numpy as np\n\nimport torch\n\n\ndef calculate_tp_fp_fn(confusion_matrix: np.ndarray) -> np.ndarray:\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    true_positives = np.diag(confusion_matrix)\n    false_positives = confusion_matrix.sum(axis=0) - true_positives\n    false_negatives = confusion_matrix.sum(axis=1) - true_positives\n    return {\n        ""true_positives"": true_positives,\n        ""false_positives"": false_positives,\n        ""false_negatives"": false_negatives,\n    }\n\n\ndef calculate_confusion_matrix_from_arrays(\n    ground_truth: np.ndarray, prediction: np.ndarray, num_classes: int\n) -> np.ndarray:\n    """"""Calculate confusion matrix for a given set of classes.\n    If GT value is outside of the [0, num_classes) it is excluded.\n\n    Args:\n        ground_truth (np.ndarray):\n        prediction (np.ndarray):\n        num_classes (int):\n\n    @TODO: Docs . Contribution is welcome\n    """"""\n    # a long 2xn array with each column being a pixel pair\n    replace_indices = np.vstack((ground_truth.flatten(), prediction.flatten()))\n\n    valid_index = replace_indices[0, :] < num_classes\n    replace_indices = replace_indices[:, valid_index].T\n\n    # add up confusion matrix\n    confusion_matrix, _ = np.histogramdd(\n        replace_indices,\n        bins=(num_classes, num_classes),\n        range=[(0, num_classes), (0, num_classes)],\n    )\n    return confusion_matrix.astype(np.uint64)\n\n\ndef calculate_confusion_matrix_from_tensors(\n    y_pred_logits: torch.Tensor, y_true: torch.Tensor\n) -> np.ndarray:\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    num_classes = y_pred_logits.shape[1]\n    y_pred = torch.argmax(y_pred_logits, dim=1)\n    ground_truth = y_true.cpu().numpy()\n    prediction = y_pred.cpu().numpy()\n\n    return calculate_confusion_matrix_from_arrays(\n        ground_truth, prediction, num_classes\n    )\n\n\n__all__ = [\n    ""calculate_tp_fp_fn"",\n    ""calculate_confusion_matrix_from_arrays"",\n    ""calculate_confusion_matrix_from_tensors"",\n]\n'"
catalyst/contrib/utils/dataset.py,0,"b'from typing import Callable, Dict, Tuple\nfrom collections import defaultdict\nimport glob\nimport itertools\nimport os\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\nDictDataset = Dict[str, object]\n\n\ndef create_dataset(\n    dirs: str,\n    extension: str = None,\n    process_fn: Callable[[str], object] = None,\n    recursive: bool = False,\n) -> DictDataset:\n    """"""\n    Create dataset (dict like `{key: [values]}`) from vctk-like dataset::\n\n        dataset/\n            cat/\n                *.ext\n            dog/\n                *.ext\n\n    Args:\n        dirs (str): path to dirs, for example /home/user/data/**\n        extension (str): data extension you are looking for\n        process_fn (Callable[[str], object]): function(path_to_file) -> object\n            process function for found files, by default\n        recursive (bool): enables recursive globbing\n\n    Returns:\n        dict: dataset\n    """"""\n    extension = extension or ""*""\n    dataset = defaultdict(list)\n\n    dirs = [os.path.expanduser(k) for k in dirs.strip().split()]\n    dirs = itertools.chain(*(glob.glob(d) for d in dirs))\n    dirs = [d for d in dirs if os.path.isdir(d)]\n\n    for d in sorted(dirs):\n        label = os.path.basename(d.rstrip(""/""))\n        pathname = d + (""/**/"" if recursive else ""/"") + extension\n        files = glob.iglob(pathname, recursive=recursive)\n        files = sorted(filter(os.path.isfile, files))\n        if process_fn is None:\n            dataset[label].extend(files)\n        else:\n            dataset[label].extend(list(map(lambda x: process_fn(x), files)))\n\n    return dataset\n\n\ndef split_dataset_train_test(\n    dataset: pd.DataFrame, **train_test_split_args\n) -> Tuple[DictDataset, DictDataset]:\n    """"""Split dataset in train and test parts.\n\n    Args:\n        dataset: dict like dataset\n        **train_test_split_args:\n            test_size : float, int, or None (default is None)\n                If float, should be between 0.0 and 1.0 and represent the\n                proportion of the dataset to include in the test split. If\n                int, represents the absolute number of test samples. If None,\n                the value is automatically set\n                to the complement of the train size.\n                If train size is also None, test size is set to 0.25.\n\n            train_size : float, int, or None (default is None)\n                If float, should be between 0.0 and 1.0 and represent the\n                proportion of the dataset to include in the train split. If\n                int, represents the absolute number of train samples. If None,\n                the value is automatically set\n                to the complement of the test size.\n\n            random_state : int or RandomState\n                Pseudo-random number generator state used for random sampling.\n\n            stratify : array-like or None (default is None)\n                If not None, data is split in a stratified fashion,\n                using this as the class labels.\n\n    Returns:\n        train and test dicts\n    """"""\n    train_dataset = defaultdict(list)\n    test_dataset = defaultdict(list)\n    for key, value in dataset.items():\n        train_ids, test_ids = train_test_split(\n            range(len(value)), **train_test_split_args\n        )\n        train_dataset[key].extend([value[i] for i in train_ids])\n        test_dataset[key].extend([value[i] for i in test_ids])\n    return train_dataset, test_dataset\n\n\ndef create_dataframe(dataset: DictDataset, **dataframe_args) -> pd.DataFrame:\n    """"""Create pd.DataFrame from dict like `{key: [values]}`.\n\n    Args:\n        dataset: dict like `{key: [values]}`\n        **dataframe_args:\n            index : Index or array-like\n                Index to use for resulting frame.\n                Will default to np.arange(n) if no indexing information\n                part of input data and no index provided\n            columns : Index or array-like\n                Column labels to use for resulting frame. Will default to\n                np.arange(n) if no column labels are provided\n            dtype : dtype, default None\n                Data type to force, otherwise infer\n\n    Returns:\n        pd.DataFrame: dataframe from giving dataset\n    """"""\n    data = [\n        (key, value) for key, values in dataset.items() for value in values\n    ]\n    df = pd.DataFrame(data, **dataframe_args)\n    return df\n\n\n__all__ = [""create_dataset"", ""create_dataframe"", ""split_dataset_train_test""]\n'"
catalyst/contrib/utils/misc.py,0,"b'from typing import Any, Iterable, List, Optional\nfrom itertools import tee\n\n\ndef pairwise(iterable: Iterable[Any]) -> Iterable[Any]:\n    """"""Iterate sequences by pairs.\n\n    Examples:\n        >>> for i in pairwise([1, 2, 5, -3]):\n        >>>     print(i)\n        (1, 2)\n        (2, 5)\n        (5, -3)\n\n    Args:\n        iterable: Any iterable sequence\n\n    Returns:\n        pairwise iterator\n    """"""\n    a, b = tee(iterable)\n    next(b, None)\n    return zip(a, b)\n\n\ndef make_tuple(tuple_like):\n    """"""Creates a tuple if given ``tuple_like`` value isn\'t list or tuple.\n\n    Returns:\n        tuple or list\n    """"""\n    tuple_like = (\n        tuple_like\n        if isinstance(tuple_like, (list, tuple))\n        else (tuple_like, tuple_like)\n    )\n    return tuple_like\n\n\ndef args_are_not_none(*args: Optional[Any]) -> bool:\n    """"""Check that all arguments are not ``None``.\n\n    Args:\n        *args (Any): values\n\n    Returns:\n         bool: True if all value were not None, False otherwise\n    """"""\n    if args is None:\n        return False\n\n    for arg in args:\n        if arg is None:\n            return False\n\n    return True\n\n\ndef find_value_ids(it: Iterable[Any], value: Any) -> List[int]:\n    """"""\n    Args:\n        it: list of any\n        value: query element\n\n    Returns: indices of the all elements equal x0\n    """"""\n    inds = [i for i, el in enumerate(it) if el == value]\n    return inds\n\n\n__all__ = [""args_are_not_none"", ""make_tuple"", ""pairwise"", ""find_value_ids""]\n'"
catalyst/contrib/utils/pandas.py,0,"b'from typing import Dict, List, Optional, Tuple, Union\nfrom collections import defaultdict\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\nfrom sklearn.utils import shuffle\nfrom tqdm.auto import tqdm\n\nfrom .misc import args_are_not_none\n\ntqdm.pandas()\n\n\ndef dataframe_to_list(dataframe: pd.DataFrame) -> List[dict]:\n    """"""Converts dataframe to a list of rows (without indexes).\n\n    Args:\n        dataframe (DataFrame): input dataframe\n\n    Returns:\n        (List[dict]): list of rows\n    """"""\n    result = list(dataframe.to_dict(orient=""index"").values())\n    return result\n\n\ndef folds_to_list(folds: Union[list, str, pd.Series]) -> List[int]:\n    """"""This function formats string or either list of numbers\n    into a list of unique int.\n\n    Examples:\n        >>> folds_to_list(""1,2,1,3,4,2,4,6"")\n        [1, 2, 3, 4, 6]\n        >>> folds_to_list([1, 2, 3.0, 5])\n        [1, 2, 3, 5]\n\n    Args:\n        folds (Union[list, str, pd.Series]): Either list of numbers or\n            one string with numbers separated by commas or\n            pandas series\n\n    Returns:\n        List[int]: list of unique ints\n\n    Raises:\n        ValueError: if value in string or array cannot be casted to int\n    """"""\n    if isinstance(folds, str):\n        folds = folds.split("","")\n    elif isinstance(folds, pd.Series):\n        folds = sorted(folds.unique())\n\n    return sorted({int(x) for x in folds})\n\n\ndef split_dataframe_train_test(\n    dataframe: pd.DataFrame, **train_test_split_args\n) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    """"""Split dataframe in train and test part.\n\n    Args:\n        dataframe: pd.DataFrame to split\n        **train_test_split_args:\n            test_size : float, int, or None (default is None)\n                If float, should be between 0.0 and 1.0 and represent the\n                proportion of the dataset to include in the test split. If\n                int, represents the absolute number of test samples. If None,\n                the value is automatically set\n                to the complement of the train size.\n                If train size is also None, test size is set to 0.25.\n\n            train_size : float, int, or None (default is None)\n                If float, should be between 0.0 and 1.0 and represent the\n                proportion of the dataset to include in the train split. If\n                int, represents the absolute number of train samples. If None,\n                the value is automatically set\n                to the complement of the test size.\n\n            random_state : int or RandomState\n                Pseudo-random number generator state used for random sampling.\n\n            stratify : array-like or None (default is None)\n                If not None, data is split in a stratified fashion,\n                using this as the class labels.\n\n    Returns:\n        train and test DataFrames\n\n    .. note::\n        It exist cause sklearn `split` is overcomplicated.\n    """"""\n    df_train, df_test = train_test_split(dataframe, **train_test_split_args)\n    return df_train, df_test\n\n\ndef split_dataframe_on_folds(\n    dataframe: pd.DataFrame, random_state: int = 42, n_folds: int = 5\n) -> pd.DataFrame:\n    """"""Splits DataFrame into `N` folds.\n\n    Args:\n        dataframe: a dataset\n        random_state: seed for random shuffle\n        n_folds: number of result folds\n\n    Returns:\n        pd.DataFrame: new dataframe with `fold` column\n    """"""\n    dataframe = shuffle(dataframe, random_state=random_state)\n\n    df_tmp = []\n    for i, df_el in enumerate(np.array_split(dataframe, n_folds)):\n        df_el[""fold""] = i\n        df_tmp.append(df_el)\n    dataframe = pd.concat(df_tmp)\n    return dataframe\n\n\ndef split_dataframe_on_stratified_folds(\n    dataframe: pd.DataFrame,\n    class_column: str,\n    random_state: int = 42,\n    n_folds: int = 5,\n) -> pd.DataFrame:\n    """"""Splits DataFrame into `N` stratified folds.\n\n    Also see :class:`catalyst.data.sampler.BalanceClassSampler`\n\n    Args:\n        dataframe: a dataset\n        class_column: which column to use for split\n        random_state: seed for random shuffle\n        n_folds: number of result folds\n\n    Returns:\n        pd.DataFrame: new dataframe with `fold` column\n    """"""\n    skf = StratifiedKFold(\n        n_splits=n_folds, shuffle=True, random_state=random_state\n    )\n    fold_column = np.zeros(len(dataframe), dtype=int)\n    for i, (_, test_index) in enumerate(\n        skf.split(range(len(dataframe)), dataframe[class_column])\n    ):\n        fold_column[test_index] = i\n    dataframe[""fold""] = fold_column\n    return dataframe\n\n\ndef split_dataframe_on_column_folds(\n    dataframe: pd.DataFrame,\n    column: str,\n    random_state: int = 42,\n    n_folds: int = 5,\n) -> pd.DataFrame:\n    """"""Splits DataFrame into `N` folds.\n\n    Args:\n        dataframe: a dataset\n        column: which column to use\n        random_state: seed for random shuffle\n        n_folds: number of result folds\n\n    Returns:\n        pd.DataFrame: new dataframe with `fold` column\n    """"""\n    df_tmp = []\n    labels = shuffle(\n        sorted(dataframe[column].unique()), random_state=random_state\n    )\n    for i, fold_labels in enumerate(np.array_split(labels, n_folds)):\n        df_label = dataframe[dataframe[column].isin(fold_labels)]\n        df_label[""fold""] = i\n        df_tmp.append(df_label)\n    dataframe = pd.concat(df_tmp)\n    return dataframe\n\n\ndef map_dataframe(\n    dataframe: pd.DataFrame,\n    tag_column: str,\n    class_column: str,\n    tag2class: Dict[str, int],\n    verbose: bool = False,\n) -> pd.DataFrame:\n    """"""This function maps tags from ``tag_column`` to ints into\n    ``class_column`` using ``tag2class`` dictionary.\n\n    Args:\n        dataframe (pd.DataFrame): input dataframe\n        tag_column (str): column with tags\n        class_column (str) output column with classes\n        tag2class (Dict[str, int]): mapping from tags to class labels\n        verbose: flag if true, uses tqdm\n\n    Returns:\n        pd.DataFrame: updated dataframe with ``class_column``\n    """"""\n    dataframe: pd.DataFrame = dataframe.copy()\n\n    def map_label(x):\n        return tag2class[str(x)]\n\n    if verbose:\n        series: pd.Series = dataframe[tag_column].progress_apply(map_label)\n    else:\n        series: pd.Series = dataframe[tag_column].apply(map_label)\n\n    dataframe.loc[series.index, class_column] = series\n    return dataframe\n\n\ndef separate_tags(\n    dataframe: pd.DataFrame, tag_column: str = ""tag"", tag_delim: str = "",""\n) -> pd.DataFrame:\n    """"""Separates values in ``class_column`` column.\n\n    Args:\n        dataframe: a dataset\n        tag_column: column name to separate values\n        tag_delim: delimiter to separate values\n\n    Returns:\n        pd.DataFrame: new dataframe\n    """"""\n    df_new = []\n    for _, row in dataframe.iterrows():\n        for class_name in row[tag_column].split(tag_delim):\n            df_new.append({**row, **{tag_column: class_name}})\n    df_new = pd.DataFrame(df_new)\n    return df_new\n\n\ndef get_dataset_labeling(\n    dataframe: pd.DataFrame, tag_column: str\n) -> Dict[str, int]:\n    """"""Prepares a mapping using unique values from ``tag_column``.\n\n    .. code-block:: javascript\n\n        {\n            ""class_name_0"": 0,\n            ""class_name_1"": 1,\n            ...\n            ""class_name_N"": N\n        }\n\n    Args:\n        dataframe: a dataset\n        tag_column: which column to use\n\n    Returns:\n        Dict[str, int]: mapping from tag to labels\n    """"""\n    tag_to_labels = {\n        str(class_name): label\n        for label, class_name in enumerate(\n            sorted(dataframe[tag_column].unique())\n        )\n    }\n    return tag_to_labels\n\n\ndef split_dataframe(\n    dataframe: pd.DataFrame,\n    train_folds: List[int],\n    valid_folds: Optional[List[int]] = None,\n    infer_folds: Optional[List[int]] = None,\n    tag2class: Optional[Dict[str, int]] = None,\n    tag_column: str = None,\n    class_column: str = None,\n    seed: int = 42,\n    n_folds: int = 5,\n) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n    """"""Split a Pandas DataFrame into folds.\n\n    Args:\n        dataframe (pd.DataFrame): input dataframe\n        train_folds (List[int]): train folds\n        valid_folds (List[int], optional): valid folds.\n            If none takes all folds not included in ``train_folds``\n        infer_folds (List[int], optional): infer folds.\n            If none takes all folds not included in ``train_folds``\n            and ``valid_folds``\n        tag2class (Dict[str, int], optional): mapping from label names into int\n        tag_column (str, optional): column with label names\n        class_column (str, optional): column to use for split\n        seed (int): seed for split\n        n_folds (int): number of folds\n\n    Returns:\n        (tuple): tuple with 4 dataframes\n            whole dataframe, train part, valid part and infer part\n    """"""\n    if args_are_not_none(tag2class, tag_column, class_column):\n        dataframe = map_dataframe(\n            dataframe, tag_column, class_column, tag2class\n        )\n\n    if class_column is not None:\n        result_dataframe = split_dataframe_on_stratified_folds(\n            dataframe,\n            class_column=class_column,\n            random_state=seed,\n            n_folds=n_folds,\n        )\n    else:\n        result_dataframe = split_dataframe_on_folds(\n            dataframe, random_state=seed, n_folds=n_folds\n        )\n\n    fold_series = result_dataframe[""fold""]\n\n    train_folds = folds_to_list(train_folds)\n    df_train = result_dataframe[fold_series.isin(train_folds)]\n\n    if valid_folds is None:\n        mask = ~fold_series.isin(train_folds)\n        valid_folds = result_dataframe[mask][""fold""]\n\n    valid_folds = folds_to_list(valid_folds)\n    df_valid = result_dataframe[fold_series.isin(valid_folds)]\n\n    infer_folds = folds_to_list(infer_folds or [])\n    df_infer = result_dataframe[fold_series.isin(infer_folds)]\n\n    return result_dataframe, df_train, df_valid, df_infer\n\n\ndef merge_multiple_fold_csv(\n    fold_name: str, paths: Optional[str]\n) -> pd.DataFrame:\n    """"""Reads csv into one DataFrame with column ``fold``.\n\n    Args:\n        fold_name (str): current fold name\n        paths (str): paths to csv separated by commas\n\n    Returns:\n         pd.DataFrame: merged dataframes with column ``fold`` == ``fold_name``\n    """"""\n    result = pd.DataFrame()\n    if paths is not None:\n        for csv_path in paths.split("",""):\n            dataframe = pd.read_csv(csv_path)\n            dataframe[""fold""] = fold_name\n            result = result.append(dataframe, ignore_index=True)\n\n    return result\n\n\ndef read_multiple_dataframes(\n    in_csv_train: str = None,\n    in_csv_valid: str = None,\n    in_csv_infer: str = None,\n    tag2class: Optional[Dict[str, int]] = None,\n    class_column: str = None,\n    tag_column: str = None,\n) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n    """"""This function reads train/valid/infer dataframes from giving paths.\n\n    Args:\n        in_csv_train (str): paths to train csv separated by commas\n        in_csv_valid (str): paths to valid csv separated by commas\n        in_csv_infer (str): paths to infer csv separated by commas\n        tag2class (Dict[str, int], optional): mapping from label names into int\n        tag_column (str, optional): column with label names\n        class_column (str, optional): column to use for split\n\n    Returns:\n        (tuple): tuple with 4 dataframes\n            whole dataframe, train part, valid part and infer part\n    """"""\n    assert any(\n        x is not None for x in (in_csv_train, in_csv_valid, in_csv_infer)\n    )\n\n    result_df = None\n    fold_dfs = {}\n    for fold_df, fold_name in zip(\n        (in_csv_train, in_csv_valid, in_csv_infer), (""train"", ""valid"", ""infer"")\n    ):\n        if fold_df is not None:\n            fold_df = merge_multiple_fold_csv(\n                fold_name=fold_name, paths=fold_df\n            )\n            if args_are_not_none(tag2class, tag_column, class_column):\n                fold_df = map_dataframe(\n                    fold_df, tag_column, class_column, tag2class\n                )\n            fold_dfs[fold_name] = fold_df\n\n            result_df = (\n                fold_df\n                if result_df is None\n                else result_df.append(fold_df, ignore_index=True)\n            )\n\n    output = (\n        result_df,\n        fold_dfs.get(""train"", None),\n        fold_dfs.get(""valid"", None),\n        fold_dfs.get(""infer"", None),\n    )\n\n    return output\n\n\ndef read_csv_data(\n    in_csv: str = None,\n    train_folds: Optional[List[int]] = None,\n    valid_folds: Optional[List[int]] = None,\n    infer_folds: Optional[List[int]] = None,\n    seed: int = 42,\n    n_folds: int = 5,\n    in_csv_train: str = None,\n    in_csv_valid: str = None,\n    in_csv_infer: str = None,\n    tag2class: Optional[Dict[str, int]] = None,\n    class_column: str = None,\n    tag_column: str = None,\n) -> Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]:\n    """"""\n    From giving path ``in_csv`` reads a dataframe\n    and split it to train/valid/infer folds\n    or from several paths ``in_csv_train``, ``in_csv_valid``, ``in_csv_infer``\n    reads independent folds.\n\n    .. note::\n       This function can be used with different combinations of params.\n        First block is used to get dataset from one `csv`:\n            in_csv, train_folds, valid_folds, infer_folds, seed, n_folds\n        Second includes paths to different csv for train/valid and infer parts:\n            in_csv_train, in_csv_valid, in_csv_infer\n        The other params (tag2class, tag_column, class_column) are optional\n            for any previous block\n\n    Args:\n        in_csv (str): paths to whole dataset\n        train_folds (List[int]): train folds\n        valid_folds (List[int], optional): valid folds.\n            If none takes all folds not included in ``train_folds``\n        infer_folds (List[int], optional): infer folds.\n            If none takes all folds not included in ``train_folds``\n            and ``valid_folds``\n        seed (int): seed for split\n        n_folds (int): number of folds\n\n        in_csv_train (str): paths to train csv separated by commas\n        in_csv_valid (str): paths to valid csv separated by commas\n        in_csv_infer (str): paths to infer csv separated by commas\n\n        tag2class (Dict[str, int]): mapping from label names into ints\n        tag_column (str): column with label names\n        class_column (str): column to use for split\n\n    Returns:\n        (Tuple[pd.DataFrame, List[dict], List[dict], List[dict]]):\n            tuple with 4 elements\n            (whole dataframe,\n            list with train data,\n            list with valid data\n            and list with infer data)\n    """"""\n    from_one_df: bool = in_csv is not None\n    from_multiple_df: bool = (\n        in_csv_train is not None\n        or in_csv_valid is not None\n        or in_csv_infer is not None\n    )\n\n    if from_one_df == from_multiple_df:\n        raise ValueError(\n            ""You should pass `in_csv` ""\n            ""or `in_csv_train` with `in_csv_valid` but not both!""\n        )\n\n    if from_one_df:\n        dataframe: pd.DataFrame = pd.read_csv(in_csv)\n        dataframe, df_train, df_valid, df_infer = split_dataframe(\n            dataframe,\n            train_folds=train_folds,\n            valid_folds=valid_folds,\n            infer_folds=infer_folds,\n            tag2class=tag2class,\n            class_column=class_column,\n            tag_column=tag_column,\n            seed=seed,\n            n_folds=n_folds,\n        )\n    else:\n        dataframe, df_train, df_valid, df_infer = read_multiple_dataframes(\n            in_csv_train=in_csv_train,\n            in_csv_valid=in_csv_valid,\n            in_csv_infer=in_csv_infer,\n            tag2class=tag2class,\n            class_column=class_column,\n            tag_column=tag_column,\n        )\n\n    for data in [df_train, df_valid, df_infer]:\n        if data is not None and ""fold"" in data.columns:\n            del data[""fold""]\n\n    result = (\n        dataframe,\n        dataframe_to_list(df_train) if df_train is not None else None,\n        dataframe_to_list(df_valid) if df_valid is not None else None,\n        dataframe_to_list(df_infer) if df_infer is not None else None,\n    )\n\n    return result\n\n\ndef balance_classes(\n    dataframe: pd.DataFrame,\n    class_column: str = ""label"",\n    random_state: int = 42,\n    how: str = ""downsampling"",\n) -> pd.DataFrame:\n    """"""Balance classes in dataframe by ``class_column``.\n\n    See also :class:`catalyst.data.sampler.BalanceClassSampler`.\n\n    Args:\n        dataframe: a dataset\n        class_column: which column to use for split\n        random_state: seed for random shuffle\n        how: strategy to sample\n            must be one on [""downsampling"", ""upsampling""]\n\n    Returns:\n        pd.DataFrame: new dataframe with balanced ``class_column``\n    """"""\n    cnt = defaultdict(lambda: 0.0)\n    for label in sorted(dataframe[class_column].unique()):\n        cnt[label] = len(dataframe[dataframe[class_column] == label])\n\n    if isinstance(how, int) or how == ""upsampling"":\n        samples_per_class = how if isinstance(how, int) else max(cnt.values())\n\n        balanced_dfs = {}\n        for label in sorted(dataframe[class_column].unique()):\n            df_class_column = dataframe[dataframe[class_column] == label]\n            if samples_per_class <= len(df_class_column):\n                balanced_dfs[label] = df_class_column.sample(\n                    samples_per_class, replace=True, random_state=random_state\n                )\n            else:\n                df_class_column_additional = df_class_column.sample(\n                    samples_per_class - len(df_class_column),\n                    replace=True,\n                    random_state=random_state,\n                )\n                balanced_dfs[label] = pd.concat(\n                    (df_class_column, df_class_column_additional)\n                )\n    elif how == ""downsampling"":\n        samples_per_class = min(cnt.values())\n\n        balanced_dfs = {}\n        for label in sorted(dataframe[class_column].unique()):\n            balanced_dfs[label] = dataframe[\n                dataframe[class_column] == label\n            ].sample(\n                samples_per_class, replace=False, random_state=random_state\n            )\n    else:\n        raise NotImplementedError()\n\n    balanced_df = pd.concat(balanced_dfs.values())\n\n    return balanced_df\n\n\n__all__ = [\n    ""dataframe_to_list"",\n    ""folds_to_list"",\n    ""split_dataframe"",\n    ""split_dataframe_on_column_folds"",\n    ""split_dataframe_on_folds"",\n    ""split_dataframe_on_stratified_folds"",\n    ""split_dataframe_train_test"",\n    ""separate_tags"",\n    ""read_multiple_dataframes"",\n    ""map_dataframe"",\n    ""get_dataset_labeling"",\n    ""merge_multiple_fold_csv"",\n    ""read_csv_data"",\n    ""balance_classes"",\n]\n'"
catalyst/contrib/utils/parallel.py,0,"b'# description     :Multiprocessing and tqdm wrapper for easy paralelizing\n# author          :Vsevolod Poletaev\n# author_email    :poletaev.va@gmail.com\n# date            :20190822\n# version         :19.08.7\n# ==============================================================================\n\nfrom typing import List, TypeVar, Union\nfrom multiprocessing.pool import Pool\n\nfrom tqdm import tqdm\n\nT = TypeVar(""T"")\n\n\nclass DumbPool:\n    """"""@TODO: Docs. Contribution is welcome.""""""\n\n    def imap_unordered(self, func, args):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        return map(func, args)\n\n    def __enter__(self):\n        """"""Enter the runtime context related to ``DumbPool`` object.""""""\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        """"""Exit the runtime context related to ``DumbPool`` object.""""""\n        return self\n\n\ndef parallel_imap(func, args, pool: Union[Pool, DumbPool],) -> List[T]:\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    result = list(pool.imap_unordered(func, args))\n    return result\n\n\ndef tqdm_parallel_imap(\n    func, args, pool: Union[Pool, DumbPool], total: int = None, pbar=tqdm,\n) -> List[T]:\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    if total is None and hasattr(args, ""__len__""):\n        total = len(args)\n\n    if pbar is None:\n        result = parallel_imap(func, args, pool)\n    else:\n        result = list(pbar(pool.imap_unordered(func, args), total=total))\n\n    return result\n\n\ndef get_pool(workers: int) -> Union[Pool, DumbPool]:\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    pool = Pool(workers) if workers > 0 and workers is not None else DumbPool()\n    return pool\n\n\n__all__ = [""parallel_imap"", ""tqdm_parallel_imap"", ""get_pool""]\n'"
catalyst/contrib/utils/plotly.py,0,"b'from typing import Dict, List, Optional, Union\nfrom collections import defaultdict\nfrom pathlib import Path\n\nimport plotly.graph_objs as go\nfrom plotly.offline import init_notebook_mode, iplot\n\nfrom catalyst.contrib.tools.tensorboard import SummaryItem, SummaryReader\n\n\ndef _get_tensorboard_scalars(\n    logdir: Union[str, Path], metrics: Optional[List[str]], step: str\n) -> Dict[str, List]:\n    summary_reader = SummaryReader(logdir, types=[""scalar""])\n\n    items = defaultdict(list)\n    for item in summary_reader:\n        if step in item.tag and (\n            metrics is None or any(m in item.tag for m in metrics)\n        ):\n            items[item.tag].append(item)\n    return items\n\n\ndef _get_scatter(scalars: List[SummaryItem], name: str) -> go.Scatter:\n    xs = [s.step for s in scalars]\n    ys = [s.value for s in scalars]\n    return go.Scatter(x=xs, y=ys, name=name)\n\n\ndef plot_tensorboard_log(\n    logdir: Union[str, Path],\n    step: Optional[str] = ""batch"",\n    metrics: Optional[List[str]] = None,\n    height: Optional[int] = None,\n    width: Optional[int] = None,\n) -> None:\n    """"""\n    @TODO: Docs. Contribution is welcome.\n\n    Adapted from\n    https://github.com/belskikh/kekas/blob/v0.1.23/kekas/utils.py#L193\n    """"""\n    init_notebook_mode()\n    logdir = Path(logdir)\n\n    logdirs = {\n        x.name.replace(""_log"", """"): x\n        for x in logdir.glob(""**/*"")\n        if x.is_dir() and str(x).endswith(""_log"")\n    }\n\n    scalars_per_loader = {\n        key: _get_tensorboard_scalars(inner_logdir, metrics, step)\n        for key, inner_logdir in logdirs.items()\n    }\n\n    scalars_per_metric = defaultdict(dict)\n    for key, value in scalars_per_loader.items():\n        for key2, value2 in value.items():\n            scalars_per_metric[key2][key] = value2\n\n    for metric_name, metric_logs in scalars_per_metric.items():\n        metric_data = []\n        for key, value in metric_logs.items():\n            try:\n                data_ = _get_scatter(value, f""{key}/{metric_name}"")\n                metric_data.append(data_)\n            except:  # noqa: E722\n                pass\n\n        layout = go.Layout(\n            title=metric_name,\n            height=height,\n            width=width,\n            yaxis={""hoverformat"": "".5f""},\n        )\n        iplot(go.Figure(data=metric_data, layout=layout))\n\n\ndef plot_metrics(\n    logdir: Union[str, Path],\n    step: Optional[str] = ""epoch"",\n    metrics: Optional[List[str]] = None,\n    height: Optional[int] = None,\n    width: Optional[int] = None,\n) -> None:\n    """"""\n    Plots your learning results.\n\n    Args:\n        logdir: the logdir that was specified during training.\n        step: \'batch\' or \'epoch\' - what logs to show: for batches or\n            for epochs\n        metrics: list of metrics to plot. The loss should be specified as\n            \'loss\', learning rate = \'_base/lr\' and other metrics should be\n            specified as names in metrics dict\n            that was specified during training\n        height: the height of the whole resulting plot\n        width: the width of the whole resulting plot\n\n    """"""\n    assert step in [\n        ""batch"",\n        ""epoch"",\n    ], f""Step should be either \'batch\' or \'epoch\', got \'{step}\'""\n    metrics = metrics or [""loss""]\n    plot_tensorboard_log(logdir, step, metrics, height, width)\n\n\n__all__ = [""plot_tensorboard_log"", ""plot_metrics""]\n'"
catalyst/contrib/utils/serialization.py,0,"b'import logging\nimport pickle\n\nfrom catalyst.tools import settings\n\nlogger = logging.getLogger(__name__)\n\nif settings.use_pyarrow:\n    try:\n        import pyarrow\n    except ImportError as ex:\n        logger.warning(\n            ""pyarrow not available, switching to pickle. ""\n            ""To install pyarrow, run `pip install pyarrow`.""\n        )\n        raise ex\n\n\ndef pyarrow_serialize(data):\n    """"""Serialize the data into bytes using pyarrow.\n\n    Args:\n        data: a value\n\n    Returns:\n        Returns a bytes object serialized with pyarrow data.\n    """"""\n    return pyarrow.serialize(data).to_buffer().to_pybytes()\n\n\ndef pyarrow_deserialize(data):\n    """"""Deserialize bytes into an object using pyarrow.\n\n    Args:\n        bytes: a bytes object containing serialized with pyarrow data.\n\n    Returns:\n        Returns a value deserialized from the bytes-like object.\n    """"""\n    return pyarrow.deserialize(data)\n\n\ndef pickle_serialize(data):\n    """"""Serialize the data into bytes using pickle.\n\n    Args:\n        data: a value\n\n    Returns:\n        Returns a bytes object serialized with pickle data.\n    """"""\n    return pickle.dumps(data)\n\n\ndef pickle_deserialize(data):\n    """"""Deserialize bytes into an object using pickle.\n\n    Args:\n        bytes: a bytes object containing serialized with pickle data.\n\n    Returns:\n        Returns a value deserialized from the bytes-like object.\n    """"""\n    return pickle.loads(data)\n\n\nif settings.use_pyarrow:\n    serialize = pyarrow_serialize\n    deserialize = pyarrow_deserialize\nelse:\n    serialize = pickle_serialize\n    deserialize = pickle_deserialize\n\n__all__ = [""serialize"", ""deserialize""]\n'"
catalyst/contrib/utils/visualization.py,0,"b'import itertools\n\nimport numpy as np\n\nfrom .cv import tensor_from_rgb_image\n\n\ndef plot_confusion_matrix(\n    cm,\n    class_names=None,\n    normalize=False,\n    title=""confusion matrix"",\n    fname=None,\n    show=True,\n    figsize=12,\n    fontsize=32,\n    colormap=""Blues"",\n):\n    """"""\n    Render the confusion matrix and return matplotlib""s figure with it.\n    Normalization can be applied by setting `normalize=True`.\n    """"""\n    import matplotlib\n\n    matplotlib.use(""Agg"")\n    import matplotlib.pyplot as plt\n\n    plt.ioff()\n\n    cmap = plt.cm.__dict__[colormap]\n\n    if class_names is None:\n        class_names = [str(i) for i in range(len(np.diag(cm)))]\n\n    if normalize:\n        cm = cm.astype(np.float32) / cm.sum(axis=1)[:, np.newaxis]\n\n    plt.rcParams.update(\n        {""font.size"": int(fontsize / np.log2(len(class_names)))}\n    )\n\n    f = plt.figure(figsize=(figsize, figsize))\n    plt.title(title)\n    plt.imshow(cm, interpolation=""nearest"", cmap=cmap)\n    plt.colorbar()\n\n    tick_marks = np.arange(len(class_names))\n    plt.xticks(tick_marks, class_names, rotation=45, ha=""right"")\n\n    plt.yticks(tick_marks, class_names)\n\n    fmt = "".2f"" if normalize else ""d""\n    thresh = cm.max() / 2.0\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(\n            j,\n            i,\n            format(cm[i, j], fmt),\n            horizontalalignment=""center"",\n            color=""white"" if cm[i, j] > thresh else ""black"",\n        )\n\n    plt.tight_layout()\n    plt.ylabel(""True label"")\n    plt.xlabel(""Predicted label"")\n\n    if fname is not None:\n        plt.savefig(fname=fname)\n\n    if show:\n        plt.show()\n\n    return f\n\n\ndef render_figure_to_tensor(figure):\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    import matplotlib\n\n    matplotlib.use(""Agg"")\n    import matplotlib.pyplot as plt\n\n    plt.ioff()\n\n    figure.canvas.draw()\n\n    image = np.array(figure.canvas.renderer._renderer)\n    plt.close(figure)\n    del figure\n\n    image = tensor_from_rgb_image(image)\n    return image\n\n\n__all__ = [""plot_confusion_matrix"", ""render_figure_to_tensor""]\n'"
catalyst/core/callbacks/__init__.py,0,"b'# flake8: noqa\n\nfrom .checkpoint import CheckpointCallback, IterationCheckpointCallback\nfrom .criterion import CriterionCallback\nfrom .early_stop import CheckRunCallback, EarlyStoppingCallback\nfrom .exception import ExceptionCallback\nfrom .logging import ConsoleLogger, TensorboardLogger, VerboseLogger\nfrom .metrics import (\n    MetricAggregationCallback,\n    MetricCallback,\n    MetricManagerCallback,\n    MultiMetricCallback,\n)\nfrom .optimizer import OptimizerCallback\nfrom .scheduler import LRUpdater, SchedulerCallback\nfrom .timer import TimerCallback\nfrom .validation import ValidationManagerCallback\n'"
catalyst/core/callbacks/checkpoint.py,0,"b'from typing import Dict, Tuple, Union\nfrom collections import OrderedDict\nimport os\nfrom pathlib import Path\n\nfrom catalyst.core import utils\nfrom catalyst.core.callback import Callback, CallbackNode, CallbackOrder\nfrom catalyst.core.runner import IRunner\n\n\ndef _pack_runner(runner: IRunner):\n    checkpoint = utils.pack_checkpoint(\n        model=runner.model,\n        criterion=runner.criterion,\n        optimizer=runner.optimizer,\n        scheduler=runner.scheduler,\n        epoch_metrics=dict(runner.epoch_metrics),\n        valid_metrics=dict(runner.valid_metrics),\n        stage_name=runner.stage_name,\n        epoch=runner.epoch,\n        loader_name=runner.loader_name,\n        loader_step=runner.loader_batch_step,\n        global_epoch=runner.global_epoch,\n        checkpoint_data=runner.checkpoint_data,\n        main_metric=runner.main_metric,\n        minimize_metric=runner.minimize_metric,\n        valid_loader=runner.valid_loader,\n    )\n    return checkpoint\n\n\ndef _load_checkpoint(\n    *, filename, runner: IRunner, load_full: bool = True\n) -> None:\n    """"""\n    Load checkpoint from a file.\n\n    Arguments:\n        filename (str): path to checkpoint\n        runner (IRunner): current runner\n        load_full (bool): if true (default) then will be performed\n            loading states for criterion, optimizer and scheduler.\n            File should contain keys required for\n            loading model (``\'model_state_dict\'``),\n            criterion (``\'criterion_state_dict\'``) (only for full load),\n            optimizer (``\'optimizer_state_dict\'``),\n            scheduler (``\'scheduler_state_dict\'``).\n\n    Raises:\n        FileNotFoundError: when file specified in ``filename``\n            is not exist.\n    """"""\n    if not os.path.isfile(filename):\n        raise FileNotFoundError(f""No checkpoint found at {filename}!"")\n\n    print(f""=> Loading checkpoint {filename}"")\n    checkpoint = utils.load_checkpoint(filename)\n\n    if not runner.stage_name.startswith(""infer"") and load_full:\n        runner.stage_name = checkpoint[""stage_name""]\n        runner.epoch = checkpoint[""epoch""]\n        runner.global_epoch = checkpoint[""global_epoch""]\n        # @TODO: should we also load,\n        # checkpoint_data, main_metric, minimize_metric, valid_loader ?\n        # epoch_metrics, valid_metrics ?\n\n    if load_full:\n        utils.unpack_checkpoint(\n            checkpoint,\n            model=runner.model,\n            criterion=runner.criterion,\n            optimizer=runner.optimizer,\n            scheduler=runner.scheduler,\n        )\n\n        print(\n            f""loaded state checkpoint {filename} ""\n            f""(global epoch {checkpoint[\'global_epoch\']}, ""\n            f""epoch {checkpoint[\'epoch\']}, ""\n            f""stage {checkpoint[\'stage_name\']})""\n        )\n    else:\n        utils.unpack_checkpoint(\n            checkpoint, model=runner.model,\n        )\n\n        print(f""loaded model checkpoint {filename}"")\n\n\ndef _required_files(logdir: str, load_map: Dict[str, str]) -> Dict[str, str]:\n    """"""\n    Generate required files for load model, criterion,\n    scheduler, optimizer specified in ``load_map``.\n\n    Expected that ``load_map`` contains keys:\n    ``""model""``, ``""criterion""``, ``""optimizer""``, ``""scheduler""``.\n    Otherwise an empty dict will be generated.\n\n    Arguments:\n        logdir (str): directory with logs\n        load_map (Dict[str, str]): dict with specification\n            what should be loaded\n\n    Returns:\n        Mapping from file to parts required from this file.\n    """"""\n    if load_map is None:\n        return OrderedDict()\n\n    default_states = {""best"", ""best_full"", ""last"", ""last_full""}\n    required_full_checkpoint = [""criterion"", ""optimizer"", ""scheduler""]\n    experiment_parts = [""model""] + required_full_checkpoint\n\n    # keep required parts\n    experiment_parts = list(\n        filter(lambda part: part in load_map, experiment_parts)\n    )\n\n    # avoid unnecessary loading\n    if ""model"" in experiment_parts and len(experiment_parts) > 1:\n        required_full_checkpoint.append(""model"")\n\n    # mapping - <filename>: <list of parts to load from this file>\n    required_files = OrderedDict()\n    for part in experiment_parts:\n        fname = load_map[part]\n        required_full = fname.endswith(""_full"")\n        # specified default state\n        if fname in default_states:\n            if part in required_full_checkpoint and not required_full:\n                fname = fname + ""_full""\n            fname = f""{logdir}/checkpoints/{fname}.pth""\n        # in other case specified path to checkpoint\n        required_files[fname] = required_files.get(fname, []) + [part]\n    return required_files\n\n\ndef _load_states_from_file_map(\n    *, runner: IRunner, load_map: Dict[str, str]\n) -> None:\n    """"""\n    Load state of a model, criterion, optimizer, scheduler\n    from files specified in ``load_map``.\n\n    Arguments:\n        runner (IRunner): current runner\n        load_map (Dict[str, str]): dict with mappings to load.\n            Expected keys - ``\'model\'``, ``\'criterion\'``\n            ``\'optimizer\'``, ``\'scheduler\'``, other keys will be\n            ignored.\n            Expected that values will be states (``\'best\'``,\n            ``""best_full""``, ``""last""``, ``""last_full""``) or\n            path to checkpoint.\n            **NOTE:** for successful load criterion, optimizer,\n            scheduler states required a full checkpoint.\n\n    Raises:\n        FileNotFoundError: when file/state specified in ``load_map``\n            is not exist.\n    """"""\n    required_files = _required_files(runner.logdir, load_map)\n\n    for filename in required_files.keys():\n        if not os.path.isfile(filename):\n            raise FileNotFoundError(f""No checkpoint found at {filename}!"")\n\n    # extracting parts from files\n    for filename, parts_to_load in required_files.items():\n        print(f""=> Loading {\', \'.join(parts_to_load)} from {filename}"")\n        checkpoint = utils.load_checkpoint(filename)\n        to_unpack = {part: getattr(runner, part) for part in parts_to_load}\n        utils.unpack_checkpoint(checkpoint, **to_unpack)\n        print(f""   loaded: {\', \'.join(parts_to_load)}"")\n\n\nclass BaseCheckpointCallback(Callback):\n    """"""Base class for all checkpoint callbacks.""""""\n\n    def __init__(self, metrics_filename: str = ""_metrics.json""):\n        """"""\n        Args:\n            metrics_filename (str): filename to save metrics\n                in checkpoint folder. Must ends on ``.json`` or ``.yml``\n        """"""\n        super().__init__(order=CallbackOrder.External, node=CallbackNode.All)\n        self.metrics_filename = metrics_filename\n        self.metrics: dict = {}\n\n    def get_checkpoint_suffix(self, checkpoint: dict) -> str:\n        return ""checkpoint""\n\n    def save_metric(self, logdir: Union[str, Path], metrics: Dict) -> None:\n        utils.save_config(\n            metrics, f""{logdir}/checkpoints/{self.metrics_filename}""\n        )\n\n    def on_exception(self, runner: IRunner):\n        exception = runner.exception\n        if not utils.is_exception(exception):\n            return\n\n        try:\n            checkpoint = _pack_runner(runner)\n            suffix = self.get_checkpoint_suffix(checkpoint)\n            suffix = f""{suffix}.exception_{exception.__class__.__name__}""\n            utils.save_checkpoint(\n                logdir=Path(f""{runner.logdir}/checkpoints/""),\n                checkpoint=checkpoint,\n                suffix=suffix,\n                is_best=False,\n                is_last=False,\n            )\n            metrics = self.metrics\n            metrics[suffix] = runner.valid_metrics\n            self.save_metric(runner.logdir, metrics)\n        except Exception:\n            pass\n\n\nclass CheckpointCallback(BaseCheckpointCallback):\n    """"""\n    Checkpoint callback to save/restore your\n    model/criterion/optimizer/scheduler.\n    """"""\n\n    def __init__(\n        self,\n        save_n_best: int = 1,\n        resume: str = None,\n        resume_dir: str = None,\n        metrics_filename: str = ""_metrics.json"",\n        load_on_stage_start: Union[str, Dict[str, str]] = None,\n        load_on_stage_end: Union[str, Dict[str, str]] = None,\n    ):\n        """"""\n        Args:\n            save_n_best (int): number of best checkpoint to keep,\n                if ``0`` then  store only last state of model and\n                ``load_on_stage_end`` should be one of\n                ``last`` or ``last_full``.\n            resume (str): path to checkpoint to load\n                and initialize runner state\n            resume_dir (str): directory with checkpoints,\n                if specified in combination with ``resume``\n                than resume checkpoint will be loaded from ``resume_dir``\n            metrics_filename (str): filename to save metrics\n                in checkpoint folder.\n                Must ends on ``.json`` or ``.yml``\n            load_on_stage_start (str or Dict[str, str]): load specified\n                state/model at stage start.\n\n                If passed **string** then will be performed initialization from\n                specified state (``best``/``best_full``/``last``/``last_full``)\n                or checkpoint file.\n\n                If passed **dict** then will be performed initialization only\n                for specified parts - model, criterion, optimizer, scheduler.\n\n                Example:\n\n                    >>> # possible checkpoints to use:\n                    >>> #   ""best""/""best_full""/""last""/""last_full""\n                    >>> #   or path to specific checkpoint\n                    >>> to_load = {\n                    >>>    ""model"": ""path/to/checkpoint.pth"",\n                    >>>    ""criterion"": ""best"",\n                    >>>    ""optimizer"": ""last_full"",\n                    >>>    ""scheduler"": ""best_full"",\n                    >>> }\n                    >>> CheckpointCallback(load_on_stage_start=to_load)\n\n                All other keys instead of ``""model""``, ``""criterion""``,\n                ``""optimizer""`` and ``""scheduler""`` will be ignored.\n\n                If ``None`` or an empty dict (or dict without mentioned\n                above keys) then no action is required at stage start and:\n\n                - Config API - will be used best state of model\n                - Notebook API - no action will be performed (will be\n                  used the last state)\n\n                **NOTE:** Loading will be performed on all stages except first.\n\n                **NOTE:** Criterion, optimizer and scheduler are optional keys\n                and should be loaded from full checkpoint.\n\n                Model state can be loaded from any checkpoint.\n\n                When dict contains keys for model and some other part\n                (for example ``{""model"": ""last"", ""optimizer"": ""last""}``)\n                and they match in prefix (``""best""`` and\n                ``""best_full""``) then will be loaded full checkpoint\n                because it contains required states.\n            load_on_stage_end (str or Dict[str, str]): load specified\n                state/model at stage end.\n\n                If passed **string** then will be performed initialization from\n                specified state (``best``/``best_full``/``last``/``last_full``)\n                or checkpoint file.\n\n                If passed **dict** then will be performed initialization only\n                for specified parts - model, criterion, optimizer, scheduler.\n                Logic for dict is the same as for ``load_on_stage_start``.\n\n                If ``None`` then no action is required at stage end\n                and will be used the last runner.\n\n                **NOTE:** Loading will be performed always at stage end.\n        """"""\n        super().__init__(metrics_filename)\n        _possible_states = {\n            None,\n            ""best"",\n            ""last"",\n            ""best_full"",\n            ""last_full"",\n        }\n        assert save_n_best >= 0\n        if save_n_best == 0:\n            assert load_on_stage_end in (None, ""last"", ""last_full"")\n        if isinstance(load_on_stage_start, str):\n            assert load_on_stage_start in _possible_states\n        if isinstance(load_on_stage_end, str):\n            assert load_on_stage_end in _possible_states\n        if resume_dir is not None:\n            assert resume is not None\n\n        self.save_n_best = save_n_best\n        self.resume = resume\n        self.resume_dir = resume_dir\n        self.load_on_stage_start = load_on_stage_start\n        self.load_on_stage_end = load_on_stage_end\n\n        self.top_best_metrics = []\n        self.metrics_history = []\n\n        self._keys_from_state = [""resume"", ""resume_dir""]\n\n    def get_checkpoint_suffix(self, checkpoint: dict) -> str:\n        """"""\n        Create checkpoint filename suffix based on checkpoint data.\n\n        Args:\n            checkpoint (dict): checkpoint dict,\n                should contain ``stage_name`` and ``epoch`` keys.\n        """"""\n        result = f""{checkpoint[\'stage_name\']}.{checkpoint[\'epoch\']}""\n        return result\n\n    def process_metrics(self, last_valid_metrics: Dict[str, float]) -> Dict:\n        """"""\n        Add last validation metrics to list of previous validation metrics\n        and keep ``save_n_best`` metrics.\n\n        Args:\n            last_valid_metrics (dict): dict with metrics\n                from last validation step.\n        """"""\n        top_best_checkpoints = [\n            (Path(filepath).stem, valid_metric)\n            for (filepath, _, valid_metric) in self.top_best_metrics\n        ]\n        all_epochs_metrics = [\n            (f""epoch_{order_index}"", valid_metric)\n            for (order_index, valid_metric) in enumerate(self.metrics_history)\n        ]\n        metrics = []\n        if self.save_n_best > 0:\n            best_valid_metrics = top_best_checkpoints[0][1]\n            metrics = (\n                [(""best"", best_valid_metrics), (""last"", last_valid_metrics)]\n                + top_best_checkpoints\n                + all_epochs_metrics\n            )\n        else:\n            metrics = [(""last"", last_valid_metrics)]\n        self.metrics = OrderedDict(metrics)\n        return self.metrics\n\n    def truncate_checkpoints(self, minimize_metric: bool) -> None:\n        """"""\n        Keep ``save_n_best`` checkpoints based on main metric.\n\n        Args:\n            minimize_metric (bool): if ``True`` then keep\n                ``save_n_best`` checkpoints with the lowest/highest values\n                of the main metric.\n        """"""\n        self.top_best_metrics = sorted(\n            self.top_best_metrics,\n            key=lambda x: x[1],\n            reverse=not minimize_metric,\n        )\n        if len(self.top_best_metrics) > self.save_n_best:\n            last_item = self.top_best_metrics.pop(-1)\n            last_filepath = Path(last_item[0])\n            last_filepaths = last_filepath.parent.glob(\n                last_filepath.name.replace("".pth"", ""*"")\n            )\n            for filepath in last_filepaths:\n                os.remove(filepath)\n\n    def _save_checkpoint(\n        self,\n        logdir: Union[str, Path],\n        suffix: str,\n        checkpoint: Dict,\n        is_best: bool,\n        is_last: bool,\n    ) -> Tuple[str, str]:\n        """"""\n        Save checkpoint (simple and full).\n\n        Args:\n            logdir (str or Path object): directory for storing checkpoints\n            suffix (str): checkpoint suffix\n            checkpoint (dict): dict with checkpoint data\n            is_best (bool): indicator to save best checkpoint,\n                if true then will be saved two additional checkpoints -\n                ``best`` and ``best_full``.\n            is_last (bool): indicator to save the last checkpoint,\n                if true then will be saved two additional checkpoints -\n                ``last`` and ``last_full``.\n        """"""\n        full_checkpoint_path = utils.save_checkpoint(\n            logdir=Path(f""{logdir}/checkpoints/""),\n            checkpoint=checkpoint,\n            suffix=f""{suffix}_full"",\n            is_best=is_best,\n            is_last=is_last,\n            special_suffix=""_full"",\n        )\n        exclude = [""criterion"", ""optimizer"", ""scheduler""]\n        checkpoint_path = utils.save_checkpoint(\n            checkpoint={\n                key: value\n                for key, value in checkpoint.items()\n                if all(z not in key for z in exclude)\n            },\n            logdir=Path(f""{logdir}/checkpoints/""),\n            suffix=suffix,\n            is_best=is_best,\n            is_last=is_last,\n        )\n        return (full_checkpoint_path, checkpoint_path)\n\n    def process_checkpoint(\n        self,\n        logdir: Union[str, Path],\n        checkpoint: Dict,\n        is_best: bool,\n        main_metric: str = ""loss"",\n        minimize_metric: bool = True,\n    ) -> None:\n        """"""\n        Save checkpoint and metrics.\n\n        Args:\n            logdir (str or Path object): directory for storing checkpoints\n            checkpoint (dict): dict with checkpoint data\n            is_best (bool): indicator to save best checkpoint,\n                if true then will be saved two additional checkpoints -\n                ``best`` and ``best_full``.\n            main_metric (str): metric to use for selecting the best model\n            minimize_metric (bool): indicator for selecting best metric,\n                if true then best metric will be the metric with\n                the lowest value, otherwise with the greatest value.\n        """"""\n        _, filepath = self._save_checkpoint(\n            logdir=logdir,\n            checkpoint=checkpoint,\n            suffix=self.get_checkpoint_suffix(checkpoint),\n            is_best=is_best,\n            is_last=True,\n        )\n        valid_metrics = checkpoint[""valid_metrics""]\n        checkpoint_metric = valid_metrics[main_metric]\n        metrics_record = (filepath, checkpoint_metric, valid_metrics)\n        self.top_best_metrics.append(metrics_record)\n        self.metrics_history.append(metrics_record)\n        self.truncate_checkpoints(minimize_metric=minimize_metric)\n        metrics = self.process_metrics(valid_metrics)\n        self.save_metric(logdir, metrics)\n\n    @staticmethod\n    def _load_runner(\n        runner: IRunner,\n        mapping: Union[str, Dict[str, str]],\n        load_full: bool = False,\n    ) -> None:\n        """"""\n        Selects a loading method based on type of mapping.\n\n        Args:\n            runner (IRunner): current runner\n            mapping (str or dict): mapping to use for loading\n            load_full (bool): load a full model, used only\n                when mapping type is string\n\n        """"""\n        if isinstance(mapping, str):\n            if mapping in {""best"", ""best_full"", ""last"", ""last_full""}:\n                checkpoint = f""{runner.logdir}/checkpoints/{mapping}.pth""\n            else:\n                checkpoint = mapping\n            _load_checkpoint(\n                filename=checkpoint, runner=runner, load_full=load_full,\n            )\n        elif isinstance(mapping, dict):\n            _load_states_from_file_map(\n                runner=runner, load_map=mapping,\n            )\n\n    def on_stage_start(self, runner: IRunner) -> None:\n        """"""\n        Setup model for stage.\n\n        **NOTE:** If CheckpointCallback initialized with ``resume``\n        (as path to checkpoint file) or ``resume`` (as filename)\n        and ``resume_dir`` (as directory with file)\n        then will be performed loading checkpoint.\n\n        Args:\n            runner (IRunner): current runner\n        """"""\n        for key in self._keys_from_state:\n            value = getattr(runner, key, None)\n            if value is not None:\n                setattr(self, key, value)\n\n        if self.resume_dir is not None:\n            self.resume = str(self.resume_dir) + ""/"" + str(self.resume)\n\n        if self.resume is not None:\n            self._load_runner(runner, mapping=self.resume, load_full=True)\n            self.resume = None\n        else:\n            _exists_checkpoint = False\n            _load_full = False\n            if isinstance(self.load_on_stage_start, str):\n                _exists_checkpoint = os.path.isfile(\n                    ""{}/checkpoints/{}.pth"".format(\n                        runner.logdir, self.load_on_stage_start\n                    )\n                )\n                _load_full = self.load_on_stage_start.endswith(""full"")\n            elif isinstance(self.load_on_stage_start, dict):\n                required_files = _required_files(\n                    runner.logdir, self.load_on_stage_start\n                ).keys()\n                _exists_checkpoint = all(\n                    os.path.isfile(file) for file in required_files\n                )\n\n            if self.load_on_stage_start is not None and _exists_checkpoint:\n                self._load_runner(\n                    runner,\n                    mapping=self.load_on_stage_start,\n                    load_full=_load_full,\n                )\n\n    def on_epoch_end(self, runner: IRunner) -> None:\n        """"""\n        Collect and save checkpoint after epoch.\n\n        Args:\n            runner (IRunner): current runner\n        """"""\n        if (\n            runner.stage_name.startswith(""infer"")\n            or runner.is_distributed_worker\n        ):\n            return\n\n        if self.save_n_best > 0:\n            checkpoint = _pack_runner(runner)\n            self.process_checkpoint(\n                logdir=runner.logdir,\n                checkpoint=checkpoint,\n                is_best=runner.is_best_valid,\n                main_metric=runner.main_metric,\n                minimize_metric=runner.minimize_metric,\n            )\n\n    def on_stage_end(self, runner: IRunner) -> None:\n        """"""\n        Show information about best checkpoints during the stage and\n        load model specified in ``load_on_stage_end``.\n\n        Args:\n            runner (IRunner): current runner\n        """"""\n        if (\n            runner.stage_name.startswith(""infer"")\n            or runner.is_distributed_worker\n        ):\n            return\n        log_message = ""Top best models:\\n""\n        # store latest state\n        if self.save_n_best == 0:\n            checkpoint = _pack_runner(runner)\n            _, filepath = self._save_checkpoint(\n                logdir=runner.logdir,\n                checkpoint=checkpoint,\n                suffix=""last"",\n                is_best=True,  # will duplicate current (last) as best\n                is_last=False,  # don\'t need that because current state is last\n            )\n            metrics = self.process_metrics(checkpoint[""valid_metrics""])\n            self.save_metric(runner.logdir, metrics)\n            main_metric_value = metrics[""last""][runner.main_metric]\n            log_message += ""{filepath}\\t{metric:3.4f}"".format(\n                filepath=filepath, metric=main_metric_value\n            )\n        else:\n            log_message += ""\\n"".join(\n                [\n                    ""{filepath}\\t{metric:3.4f}"".format(\n                        filepath=filepath, metric=checkpoint_metric\n                    )\n                    for filepath, checkpoint_metric, _ in self.top_best_metrics\n                ]\n            )\n        print(log_message)\n        _not_required_load_states = {""last"", ""last_full""}\n        if (\n            isinstance(self.load_on_stage_end, str)\n            and self.load_on_stage_end not in _not_required_load_states\n            and self.save_n_best > 0\n        ):\n            _load_full = (\n                self.load_on_stage_end.endswith(""full"")\n                if isinstance(self.load_on_stage_end, str)\n                else False\n            )\n            self._load_runner(\n                runner, mapping=self.load_on_stage_end, load_full=_load_full,\n            )\n        elif isinstance(self.load_on_stage_end, dict) and self.save_n_best > 0:\n            to_load = {\n                k: v\n                for k, v in self.load_on_stage_end.items()\n                if v not in _not_required_load_states\n            }\n            self._load_runner(runner, mapping=to_load)\n\n\nclass IterationCheckpointCallback(BaseCheckpointCallback):\n    """"""Iteration checkpoint callback to save your model/criterion/optimizer.""""""\n\n    def __init__(\n        self,\n        save_n_last: int = 1,\n        period: int = 100,\n        stage_restart: bool = True,\n        metrics_filename: str = ""_metrics_iter.json"",\n        load_on_stage_end: str = ""best_full"",\n    ):\n        """"""\n        Args:\n            save_n_last (int): number of last checkpoint to keep\n            period (int): save the checkpoint every `period`\n            stage_restart (bool): restart counter every stage or not\n            metrics_filename (str): filename to save metrics\n                in checkpoint folder. Must ends on ``.json`` or ``.yml``\n            load_on_stage_end (str): name of the model to load\n                at the end of the stage.\n                You can use ``best``, ``best_full`` (default)\n                to load the best model according to validation metrics,\n                or ``last`` ``last_full`` to use just the last one.\n        """"""\n        super().__init__(metrics_filename)\n        self.save_n_last = save_n_last\n        self.period = period\n        self.stage_restart = stage_restart\n        self._iteration_counter = 0\n        self.last_checkpoints = []\n        self.metrics_history = []\n        self.load_on_stage_end = load_on_stage_end\n\n    def get_checkpoint_suffix(self, checkpoint: dict) -> str:\n        """"""\n        Create checkpoint filename suffix based on checkpoint data.\n\n        Args:\n            checkpoint (dict): checkpoint dict,\n                should contain ``stage_name`` and ``epoch`` keys.\n        """"""\n        result = (\n            f""{checkpoint[\'stage_name\']}.""\n            f""epoch.{checkpoint[\'epoch\']}.""\n            f""iter.{self._iteration_counter}""\n        )\n\n        return result\n\n    def process_metrics(self) -> Dict:\n        """"""\n        Update metrics with last ``save_n_last`` checkpoints.\n        """"""\n        n_last_checkpoints = [\n            (Path(filepath).stem, batch_values)\n            for (filepath, batch_values) in self.last_checkpoints\n        ]\n        all_epochs_metrics = [\n            (f""epoch_{order_index}"", valid_metric)\n            for (order_index, valid_metric) in enumerate(self.metrics_history)\n        ]\n\n        metrics = OrderedDict(n_last_checkpoints + all_epochs_metrics)\n        self.metrics = metrics\n        return self.metrics\n\n    def truncate_checkpoints(self, **kwargs) -> None:\n        """"""\n        Keep ``save_n_best`` checkpoints based on main metric.\n        """"""\n        if len(self.last_checkpoints) > self.save_n_last:\n            item = self.last_checkpoints.pop(0)\n            top_filepath = item[0]\n            os.remove(top_filepath)\n\n    def process_checkpoint(\n        self,\n        logdir: Union[str, Path],\n        checkpoint: Dict,\n        batch_metrics: Dict[str, float],\n    ):\n        """"""\n        Save checkpoint and metrics.\n\n        Args:\n            logdir (str or Path object): directory for storing checkpoints\n            checkpoint (dict): dict with checkpoint data\n            batch_metrics (dict): dict with metrics based on a few batches\n        """"""\n        filepath = utils.save_checkpoint(\n            logdir=Path(f""{logdir}/checkpoints/""),\n            checkpoint=checkpoint,\n            suffix=self.get_checkpoint_suffix(checkpoint),\n            is_best=False,\n            is_last=False,\n        )\n\n        self.last_checkpoints.append((filepath, batch_metrics))\n        self.truncate_checkpoints()\n\n        self.metrics_history.append(batch_metrics)\n\n        metrics = self.process_metrics()\n        self.save_metric(logdir, metrics)\n        print(f""\\nSaved checkpoint at {filepath}"")\n\n    def on_stage_start(self, runner: IRunner):\n        """"""\n        Reset iterations counter.\n\n        Args:\n            runner (IRunner): current runner\n        """"""\n        if self.stage_restart:\n            self._iteration_counter = 0\n\n    def on_batch_end(self, runner: IRunner):\n        """"""\n        Save checkpoint based on batches count.\n\n        Args:\n            runner (IRunner): current runner\n        """"""\n        self._iteration_counter += 1\n        if self._iteration_counter % self.period == 0:\n            checkpoint = _pack_runner(runner)\n            self.process_checkpoint(\n                logdir=runner.logdir,\n                checkpoint=checkpoint,\n                batch_metrics=runner.batch_metrics,\n            )\n\n    def on_stage_end(self, runner: IRunner):\n        """"""\n        Load model specified in ``load_on_stage_end``.\n\n        Args:\n            runner (IRunner): current runner\n        """"""\n        if self.load_on_stage_end in [""best"", ""best_full""]:\n            resume = (\n                f""{runner.logdir}/checkpoints/{self.load_on_stage_end}.pth""\n            )\n            print(f""Loading {self.load_on_stage_end} model from {resume}"")\n            _load_checkpoint(\n                filename=resume,\n                runner=runner,\n                load_full=self.load_on_stage_end.endswith(""full""),\n            )\n\n\n__all__ = [""CheckpointCallback"", ""IterationCheckpointCallback""]\n'"
catalyst/core/callbacks/criterion.py,0,"b'from typing import Dict, List, Union\n\nfrom catalyst.core.runner import IRunner\n\nfrom .metrics import _MetricCallback\n\n\nclass CriterionCallback(_MetricCallback):\n    """"""Callback for that measures loss with specified criterion.""""""\n\n    def __init__(\n        self,\n        input_key: Union[str, List[str], Dict[str, str]] = ""targets"",\n        output_key: Union[str, List[str], Dict[str, str]] = ""logits"",\n        prefix: str = ""loss"",\n        criterion_key: str = None,\n        multiplier: float = 1.0,\n        **metric_kwargs,\n    ):\n        """"""\n        Args:\n            input_key (Union[str, List[str], Dict[str, str]]): key/list/dict\n                of keys that takes values from the input dictionary\n                If \'__all__\', the whole input will be passed to the criterion\n                If None, empty dict will be passed to the criterion.\n            output_key (Union[str, List[str], Dict[str, str]]): key/list/dict\n                of keys that takes values from the input dictionary\n                If \'__all__\', the whole output will be passed to the criterion\n                If None, empty dict will be passed to the criterion.\n            prefix (str): prefix for metrics and output key for loss\n                in ``runner.batch_metrics`` dictionary\n            criterion_key (str): A key to take a criterion in case\n                there are several of them and they are in a dictionary format.\n            multiplier (float): scale factor for the output loss.\n        """"""\n        super().__init__(\n            prefix=prefix,\n            input_key=input_key,\n            output_key=output_key,\n            multiplier=multiplier,\n            **metric_kwargs,\n        )\n        self.criterion_key = criterion_key\n        self._criterion = None\n\n    @property\n    def metric_fn(self):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        return self._criterion\n\n    def on_stage_start(self, runner: IRunner):\n        """"""Checks that the current stage has correct criterion.""""""\n        criterion = runner.get_attr(\n            key=""criterion"", inner_key=self.criterion_key\n        )\n        assert criterion is not None\n        self._criterion = criterion\n\n\n__all__ = [\n    ""CriterionCallback"",\n]\n'"
catalyst/core/callbacks/early_stop.py,0,"b'from catalyst.core.callback import Callback, CallbackNode, CallbackOrder\nfrom catalyst.core.runner import IRunner\n\n\nclass CheckRunCallback(Callback):\n    """"""@TODO: Docs. Contribution is welcome.""""""\n\n    def __init__(self, num_batch_steps: int = 3, num_epoch_steps: int = 2):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        super().__init__(order=CallbackOrder.External, node=CallbackNode.All)\n        self.num_batch_steps = num_batch_steps\n        self.num_epoch_steps = num_epoch_steps\n\n    def on_epoch_end(self, runner: IRunner):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        if runner.epoch >= self.num_epoch_steps:\n            runner.need_early_stop = True\n\n    def on_batch_end(self, runner: IRunner):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        if runner.loader_batch_step >= self.num_batch_steps:\n            runner.need_early_stop = True\n\n\nclass EarlyStoppingCallback(Callback):\n    """"""@TODO: Docs. Contribution is welcome.""""""\n\n    def __init__(\n        self,\n        patience: int,\n        metric: str = ""loss"",\n        minimize: bool = True,\n        min_delta: float = 1e-6,\n    ):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        super().__init__(order=CallbackOrder.External, node=CallbackNode.All)\n        self.best_score = None\n        self.metric = metric\n        self.patience = patience\n        self.num_bad_epochs = 0\n        self.is_better = None\n\n        if minimize:\n            self.is_better = lambda score, best: score <= (best - min_delta)\n        else:\n            self.is_better = lambda score, best: score >= (best + min_delta)\n\n    def on_epoch_end(self, runner: IRunner) -> None:\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        if runner.stage_name.startswith(""infer""):\n            return\n\n        score = runner.valid_metrics[self.metric]\n        if self.best_score is None:\n            self.best_score = score\n        if self.is_better(score, self.best_score):\n            self.num_bad_epochs = 0\n            self.best_score = score\n        else:\n            self.num_bad_epochs += 1\n\n        if self.num_bad_epochs >= self.patience:\n            print(f""Early stop at {runner.epoch} epoch"")\n            runner.need_early_stop = True\n'"
catalyst/core/callbacks/exception.py,0,"b'from catalyst.core import utils\nfrom catalyst.core.callback import Callback, CallbackNode, CallbackOrder\nfrom catalyst.core.runner import IRunner\n\n\nclass ExceptionCallback(Callback):\n    """"""@TODO: Docs. Contribution is welcome.""""""\n\n    def __init__(self):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        super().__init__(\n            order=CallbackOrder.External + 1, node=CallbackNode.All\n        )\n\n    def on_exception(self, runner: IRunner):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        exception = runner.exception\n        if not utils.is_exception(exception):\n            return\n\n        if runner.need_exception_reraise:\n            raise exception\n'"
catalyst/core/callbacks/formatters.py,0,"b'from typing import Dict\nfrom abc import ABC, abstractmethod\nimport logging\n\nfrom catalyst.core import utils\nfrom catalyst.core.runner import IRunner\n\n\nclass MetricsFormatter(ABC, logging.Formatter):\n    """"""Abstract metrics formatter.""""""\n\n    def __init__(self, message_prefix):\n        """"""\n        Args:\n            message_prefix: logging format string\n                that will be prepended to message\n        """"""\n        super().__init__(f""{message_prefix}{{message}}"", style=""{"")\n\n    @abstractmethod\n    def _format_message(self, runner: IRunner):\n        pass\n\n    def format(self, record: logging.LogRecord):\n        """"""Format message string.""""""\n        # noinspection PyUnresolvedReferences\n        runner = record.runner\n\n        record.msg = self._format_message(runner)\n\n        return super().format(record)\n\n\nclass TxtMetricsFormatter(MetricsFormatter):\n    """"""Translate batch metrics in human-readable format.\n\n    This class is used by ``logging.Logger`` to make a string from record.\n    For details refer to official docs for \'logging\' module.\n\n    .. warning::\n        This is inner class used by Logger callback,\n        no need to use it directly!\n    """"""\n\n    def __init__(self):\n        """"""\n        Initializes the ``TxtMetricsFormatter``\n        """"""\n        super().__init__(""[{asctime}] "")\n\n    def _format_metrics(self, metrics: Dict[str, Dict[str, float]]):\n        metrics_formatted = {}\n        for key, value in metrics.items():\n            metrics_formatted_ = [\n                utils.format_metric(m_name, m_value)\n                for m_name, m_value in sorted(value.items())\n            ]\n            metrics_formatted_ = "" | "".join(metrics_formatted_)\n            metrics_formatted[key] = metrics_formatted_\n\n        return metrics_formatted\n\n    def _format_message(self, runner: IRunner):\n        message = [""""]\n        mode_metrics = utils.split_dict_to_subdicts(\n            dct=runner.epoch_metrics,\n            prefixes=list(runner.loaders.keys()),\n            extra_key=""_base"",\n        )\n        metrics = self._format_metrics(mode_metrics)\n        for key, value in metrics.items():\n            message.append(\n                f""{runner.epoch}/{runner.num_epochs} ""\n                f""* Epoch {runner.global_epoch} ({key}): {value}""\n            )\n        message = ""\\n"".join(message)\n        return message\n\n\n__all__ = [""MetricsFormatter"", ""TxtMetricsFormatter""]\n'"
catalyst/core/callbacks/logging.py,0,"b'from typing import Dict, List\nimport logging\nimport os\nimport sys\n\nfrom tqdm import tqdm\n\nfrom catalyst.contrib.tools.tensorboard import SummaryWriter\nfrom catalyst.core import utils\nfrom catalyst.core.callback import Callback, CallbackNode, CallbackOrder\nfrom catalyst.core.runner import IRunner\n\nfrom . import formatters\n\n\nclass VerboseLogger(Callback):\n    """"""Logs the params into console.""""""\n\n    def __init__(\n        self, always_show: List[str] = None, never_show: List[str] = None,\n    ):\n        """"""\n        Args:\n            always_show (List[str]): list of metrics to always show\n                if None default is ``[""_timer/_fps""]``\n                to remove always_show metrics set it to an empty list ``[]``\n            never_show (List[str]): list of metrics which will not be shown\n        """"""\n        super().__init__(order=CallbackOrder.Logging, node=CallbackNode.Master)\n        self.tqdm: tqdm = None\n        self.step = 0\n        self.always_show = (\n            always_show if always_show is not None else [""_timer/_fps""]\n        )\n        self.never_show = never_show if never_show is not None else []\n\n        intersection = set(self.always_show) & set(self.never_show)\n\n        _error_message = (\n            f""Intersection of always_show and ""\n            f""never_show has common values: {intersection}""\n        )\n        if bool(intersection):\n            raise ValueError(_error_message)\n\n    def _need_show(self, key: str):\n        not_is_never_shown: bool = key not in self.never_show\n        is_always_shown: bool = key in self.always_show\n        not_basic = not (key.startswith(""_base"") or key.startswith(""_timer""))\n\n        result = not_is_never_shown and (is_always_shown or not_basic)\n\n        return result\n\n    def on_loader_start(self, runner: IRunner):\n        """"""Init tqdm progress bar.""""""\n        self.step = 0\n        self.tqdm = tqdm(\n            total=runner.loader_len,\n            desc=f""{runner.epoch}/{runner.num_epochs}""\n            f"" * Epoch ({runner.loader_name})"",\n            leave=True,\n            ncols=0,\n            file=sys.stdout,\n        )\n\n    def on_loader_end(self, runner: IRunner):\n        """"""Cleanup and close tqdm progress bar.""""""\n        # self.tqdm.visible = False\n        # self.tqdm.leave = True\n        # self.tqdm.disable = True\n        self.tqdm.clear()\n        self.tqdm.close()\n        self.tqdm = None\n        self.step = 0\n\n    def on_batch_end(self, runner: IRunner):\n        """"""Update tqdm progress bar at the end of each batch.""""""\n        self.tqdm.set_postfix(\n            **{\n                k: ""{:3.3f}"".format(v) if v > 1e-3 else ""{:1.3e}"".format(v)\n                for k, v in sorted(runner.batch_metrics.items())\n                if self._need_show(k)\n            }\n        )\n        self.tqdm.update()\n\n    def on_exception(self, runner: IRunner):\n        """"""Called if an Exception was raised.""""""\n        exception = runner.exception\n        if not utils.is_exception(exception):\n            return\n\n        if isinstance(exception, KeyboardInterrupt):\n            self.tqdm.write(""Early exiting"")\n            runner.need_exception_reraise = False\n\n\nclass ConsoleLogger(Callback):\n    """"""Logger callback,\n    translates ``runner.*_metrics`` to console and text file.\n    """"""\n\n    def __init__(self):\n        """"""Init ``ConsoleLogger``.""""""\n        super().__init__(order=CallbackOrder.Logging, node=CallbackNode.Master)\n        self.logger = None\n\n    @staticmethod\n    def _get_logger(logdir):\n        logger = logging.getLogger(""metrics_logger"")\n        logger.setLevel(logging.INFO)\n\n        ch = logging.StreamHandler(sys.stdout)\n        ch.setLevel(logging.INFO)\n\n        txt_formatter = formatters.TxtMetricsFormatter()\n        ch.setFormatter(txt_formatter)\n\n        # add the handlers to the logger\n        logger.addHandler(ch)\n\n        if logdir:\n            fh = logging.FileHandler(f""{logdir}/log.txt"")\n            fh.setLevel(logging.INFO)\n            fh.setFormatter(txt_formatter)\n            logger.addHandler(fh)\n\n        # logger.addHandler(jh)\n        return logger\n\n    def on_stage_start(self, runner: IRunner):\n        """"""Prepare ``runner.logdir`` for the current stage.""""""\n        if runner.logdir:\n            runner.logdir.mkdir(parents=True, exist_ok=True)\n        self.logger = self._get_logger(runner.logdir)\n\n    def on_stage_end(self, runner: IRunner):\n        """"""Called at the end of each stage.""""""\n        for handler in self.logger.handlers:\n            handler.close()\n        self.logger.handlers = []\n\n    def on_epoch_end(self, runner: IRunner):\n        """"""\n        Translate ``runner.metric_manager`` to console and text file\n        at the end of an epoch.\n        """"""\n        self.logger.info("""", extra={""runner"": runner})\n\n\nclass TensorboardLogger(Callback):\n    """"""Logger callback, translates ``runner.metric_manager`` to tensorboard.""""""\n\n    def __init__(\n        self,\n        metric_names: List[str] = None,\n        log_on_batch_end: bool = True,\n        log_on_epoch_end: bool = True,\n    ):\n        """"""\n        Args:\n            metric_names (List[str]): list of metric names to log,\n                if none - logs everything\n            log_on_batch_end (bool): logs per-batch metrics if set True\n            log_on_epoch_end (bool): logs per-epoch metrics if set True\n        """"""\n        super().__init__(order=CallbackOrder.Logging, node=CallbackNode.Master)\n        self.metrics_to_log = metric_names\n        self.log_on_batch_end = log_on_batch_end\n        self.log_on_epoch_end = log_on_epoch_end\n\n        if not (self.log_on_batch_end or self.log_on_epoch_end):\n            raise ValueError(""You have to log something!"")\n\n        self.loggers = {}\n\n    def _log_metrics(\n        self, metrics: Dict[str, float], step: int, mode: str, suffix=""""\n    ):\n        if self.metrics_to_log is None:\n            metrics_to_log = sorted(metrics.keys())\n        else:\n            metrics_to_log = self.metrics_to_log\n\n        for name in metrics_to_log:\n            if name in metrics:\n                self.loggers[mode].add_scalar(\n                    f""{name}{suffix}"", metrics[name], step\n                )\n\n    def on_stage_start(self, runner: IRunner):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        assert runner.logdir is not None\n\n        extra_mode = ""_base""\n        log_dir = os.path.join(runner.logdir, f""{extra_mode}_log"")\n        self.loggers[extra_mode] = SummaryWriter(log_dir)\n\n    def on_loader_start(self, runner: IRunner):\n        """"""Prepare tensorboard writers for the current stage.""""""\n        if runner.loader_name not in self.loggers:\n            log_dir = os.path.join(runner.logdir, f""{runner.loader_name}_log"")\n            self.loggers[runner.loader_name] = SummaryWriter(log_dir)\n\n    def on_batch_end(self, runner: IRunner):\n        """"""Translate batch metrics to tensorboard.""""""\n        if runner.logdir is None:\n            return\n\n        if self.log_on_batch_end:\n            mode = runner.loader_name\n            metrics_ = runner.batch_metrics\n            self._log_metrics(\n                metrics=metrics_,\n                step=runner.global_sample_step,\n                mode=mode,\n                suffix=""/batch"",\n            )\n\n    def on_epoch_end(self, runner: IRunner):\n        """"""Translate epoch metrics to tensorboard.""""""\n        if runner.logdir is None:\n            return\n\n        if self.log_on_epoch_end:\n            per_mode_metrics = utils.split_dict_to_subdicts(\n                dct=runner.epoch_metrics,\n                prefixes=list(runner.loaders.keys()),\n                extra_key=""_base"",\n            )\n\n            for mode, metrics in per_mode_metrics.items():\n                # suffix = """" if mode == ""_base"" else ""/epoch""\n                self._log_metrics(\n                    metrics=metrics,\n                    step=runner.global_epoch,\n                    mode=mode,\n                    suffix=""/epoch"",\n                )\n\n        for logger in self.loggers.values():\n            logger.flush()\n\n    def on_stage_end(self, runner: IRunner):\n        """"""Close opened tensorboard writers.""""""\n        if runner.logdir is None:\n            return\n\n        for logger in self.loggers.values():\n            logger.close()\n\n\n__all__ = [\n    ""ConsoleLogger"",\n    ""TensorboardLogger"",\n    ""VerboseLogger"",\n]\n'"
catalyst/core/callbacks/metrics.py,2,"b'from typing import Any, Callable, Dict, List, Union\nfrom abc import ABC, abstractmethod\nfrom collections import defaultdict\nimport logging\n\nimport torch\n\nfrom catalyst.core import utils\nfrom catalyst.core.callback import Callback, CallbackNode, CallbackOrder\nfrom catalyst.core.runner import IRunner\nfrom catalyst.tools import meters\n\nlogger = logging.getLogger(__name__)\n\n\nclass _MetricCallback(ABC, Callback):\n    """"""@TODO: Docs. Contribution is welcome.""""""\n\n    def __init__(\n        self,\n        prefix: str,\n        input_key: Union[str, List[str], Dict[str, str]] = ""targets"",\n        output_key: Union[str, List[str], Dict[str, str]] = ""logits"",\n        multiplier: float = 1.0,\n        **metrics_kwargs,\n    ):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        super().__init__(order=CallbackOrder.Metric, node=CallbackNode.All)\n        self.prefix = prefix\n        # self.metric_fn = partial(metric_fn, **metrics_kwargs)\n        self.input_key = input_key\n        self.output_key = output_key\n        self.multiplier = multiplier\n        self.metrics_kwargs = metrics_kwargs\n\n        self._get_input = utils.get_dictkey_auto_fn(self.input_key)\n        self._get_output = utils.get_dictkey_auto_fn(self.output_key)\n        kv_types = (dict, tuple, list, type(None))\n\n        is_value_input = (\n            isinstance(self.input_key, str) and self.input_key != ""__all__""\n        )\n        is_value_output = (\n            isinstance(self.output_key, str) and self.output_key != ""__all__""\n        )\n        is_kv_input = (\n            isinstance(self.input_key, kv_types) or self.input_key == ""__all__""\n        )\n        is_kv_output = (\n            isinstance(self.output_key, kv_types)\n            or self.output_key == ""__all__""\n        )\n\n        # @TODO: fix to only KV usage\n        if hasattr(self, ""_compute_metric""):\n            pass  # overridden in descendants\n        elif is_value_input and is_value_output:\n            self._compute_metric = self._compute_metric_value\n        elif is_kv_input and is_kv_output:\n            self._compute_metric = self._compute_metric_key_value\n        else:\n            raise NotImplementedError()\n\n    @property\n    @abstractmethod\n    def metric_fn(self):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        pass\n\n    def _compute_metric_value(self, runner: IRunner):\n        output = self._get_output(runner.output, self.output_key)\n        input = self._get_input(runner.input, self.input_key)\n\n        metric = self.metric_fn(output, input, **self.metrics_kwargs)\n        return metric\n\n    def _compute_metric_key_value(self, runner: IRunner):\n        output = self._get_output(runner.output, self.output_key)\n        input = self._get_input(runner.input, self.input_key)\n\n        metric = self.metric_fn(**output, **input, **self.metrics_kwargs)\n        return metric\n\n    def on_batch_end(self, runner: IRunner) -> None:\n        """"""Computes the metric and add it to batch metrics.""""""\n        metric = self._compute_metric(runner) * self.multiplier\n        runner.batch_metrics[self.prefix] = metric\n\n\nclass MetricCallback(_MetricCallback):\n    """"""A callback that returns single metric on `runner.on_batch_end`.""""""\n\n    def __init__(\n        self,\n        prefix: str,\n        metric_fn: Callable,\n        input_key: Union[str, List[str], Dict[str, str]] = ""targets"",\n        output_key: Union[str, List[str], Dict[str, str]] = ""logits"",\n        multiplier: float = 1.0,\n        **metric_kwargs,\n    ):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        super().__init__(\n            prefix=prefix,\n            input_key=input_key,\n            output_key=output_key,\n            multiplier=multiplier,\n            **metric_kwargs,\n        )\n        self.metric = metric_fn\n\n    @property\n    def metric_fn(self):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        return self.metric\n\n\nclass MultiMetricCallback(MetricCallback):\n    """"""A callback that returns multiple metrics on `runner.on_batch_end`.""""""\n\n    def __init__(\n        self,\n        prefix: str,\n        metric_fn: Callable,\n        list_args: List,\n        input_key: Union[str, List[str], Dict[str, str]] = ""targets"",\n        output_key: Union[str, List[str], Dict[str, str]] = ""logits"",\n        multiplier: float = 1.0,\n        **metrics_kwargs,\n    ):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        super().__init__(\n            prefix=prefix,\n            metric_fn=metric_fn,\n            input_key=input_key,\n            output_key=output_key,\n            multiplier=multiplier,\n            **metrics_kwargs,\n        )\n        self.list_args = list_args\n\n    def on_batch_end(self, runner: IRunner) -> None:\n        """"""Batch end hook.\n\n        Args:\n            runner (IRunner): current runner\n        """"""\n        metrics_ = self._compute_metric(runner)\n\n        for arg, metric in zip(self.list_args, metrics_):\n            if isinstance(arg, int):\n                key = f""{self.prefix}{arg:02}""\n            else:\n                key = f""{self.prefix}_{arg}""\n            runner.batch_metrics[key] = metric * self.multiplier\n\n\nclass MetricAggregationCallback(Callback):\n    """"""A callback to aggregate several metrics in one value.""""""\n\n    def __init__(\n        self,\n        prefix: str,\n        metrics: Union[str, List[str], Dict[str, float]] = None,\n        mode: str = ""mean"",\n        multiplier: float = 1.0,\n    ) -> None:\n        """"""\n        Args:\n            prefix (str): new key for aggregated metric.\n            metrics (Union[str, List[str], Dict[str, float]]): If not None,\n                it aggregates only the values from the metric by these keys.\n                for ``weighted_sum`` aggregation it must be a Dict[str, float].\n            mode (str): function for aggregation.\n                Must be either ``sum``, ``mean`` or ``weighted_sum``.\n            multiplier (float): scale factor for the aggregated metric.\n        """"""\n        super().__init__(\n            order=CallbackOrder.MetricAggregation, node=CallbackNode.All\n        )\n\n        if prefix is None or not isinstance(prefix, str):\n            raise ValueError(""prefix must be str"")\n\n        if mode in (""sum"", ""mean""):\n            if metrics is not None and not isinstance(metrics, list):\n                raise ValueError(\n                    ""For `sum` or `mean` mode the metrics must be ""\n                    ""None or list or str (not dict)""\n                )\n        elif mode in (""weighted_sum"", ""weighted_mean""):\n            if metrics is None or not isinstance(metrics, dict):\n                raise ValueError(\n                    ""For `weighted_sum` or `weighted_mean` mode ""\n                    ""the metrics must be specified ""\n                    ""and must be a dict""\n                )\n        else:\n            raise NotImplementedError(\n                ""mode must be `sum`, `mean` ""\n                ""or `weighted_sum` or `weighted_mean`""\n            )\n\n        if isinstance(metrics, str):\n            metrics = [metrics]\n\n        self.prefix = prefix\n        self.metrics = metrics\n        self.mode = mode\n        self.multiplier = multiplier\n\n        if mode in (""sum"", ""weighted_sum"", ""weighted_mean""):\n            self.aggregation_fn = (\n                lambda x: torch.sum(torch.stack(x)) * multiplier\n            )\n            if mode == ""weighted_mean"":\n                weights_sum = sum(metrics.items())\n                self.metrics = {\n                    key: weight / weights_sum\n                    for key, weight in metrics.items()\n                }\n        elif mode == ""mean"":\n            self.aggregation_fn = (\n                lambda x: torch.mean(torch.stack(x)) * multiplier\n            )\n\n    def _preprocess(self, metrics: Any) -> List[float]:\n        if self.metrics is not None:\n            if self.mode == ""weighted_sum"":\n                result = [\n                    metrics[key] * value for key, value in self.metrics.items()\n                ]\n            else:\n                result = [metrics[key] for key in self.metrics]\n        else:\n            result = list(metrics.values())\n        return result\n\n    def on_batch_end(self, runner: IRunner) -> None:\n        """"""Computes the metric and add it to the metrics.\n\n        Args:\n            runner (IRunner): current runner\n        """"""\n        metrics = self._preprocess(runner.batch_metrics)\n        metric = self.aggregation_fn(metrics)\n        runner.batch_metrics[self.prefix] = metric\n\n\nclass MetricManagerCallback(Callback):\n    """"""\n    Prepares metrics for logging, transferring values from PyTorch to numpy.\n    """"""\n\n    def __init__(self):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        super().__init__(\n            order=CallbackOrder.Logging - 1, node=CallbackNode.All,\n        )\n        self.meters: Dict[str, meters.AverageValueMeter] = None\n\n    @staticmethod\n    def _to_single_value(value: Any) -> float:\n        if hasattr(value, ""item""):\n            value = value.item()\n\n        value = float(value)\n        return value\n\n    @staticmethod\n    def _process_metrics(metrics: Dict[str, Any]):\n        output = {}\n        for key, value in metrics.items():\n            value = utils.get_distributed_mean(value)\n            value = MetricManagerCallback._to_single_value(value)\n            output[key] = value\n        return output\n\n    def on_epoch_start(self, runner: IRunner) -> None:\n        """"""Epoch start hook.\n\n        Args:\n            runner (IRunner): current runner\n        """"""\n        runner.epoch_metrics = defaultdict(None)\n\n    def on_loader_start(self, runner: IRunner) -> None:\n        """"""Loader start hook.\n\n        Args:\n            runner (IRunner): current runner\n        """"""\n        runner.loader_metrics = defaultdict(None)\n        self.meters = defaultdict(meters.AverageValueMeter)\n\n    def on_loader_end(self, runner: IRunner) -> None:\n        """"""Loader end hook.\n\n        Args:\n            runner (IRunner): current runner\n        """"""\n        for key, value in self.meters.items():\n            value = value.mean\n            runner.loader_metrics[key] = value\n        for key, value in runner.loader_metrics.items():\n            runner.epoch_metrics[f""{runner.loader_name}_{key}""] = value\n\n    def on_batch_start(self, runner: IRunner) -> None:\n        """"""Batch start hook.\n\n        Args:\n            runner (IRunner): current runner\n        """"""\n        runner.batch_metrics = defaultdict(None)\n\n    def on_batch_end(self, runner: IRunner) -> None:\n        """"""Batch end hook.\n\n        Args:\n            runner (IRunner): current runner\n        """"""\n        runner.batch_metrics = self._process_metrics(runner.batch_metrics)\n        for key, value in runner.batch_metrics.items():\n            self.meters[key].add(value, runner.batch_size)\n\n\n__all__ = [\n    ""_MetricCallback"",\n    ""MetricCallback"",\n    ""MultiMetricCallback"",\n    ""MetricAggregationCallback"",\n    ""MetricManagerCallback"",\n]\n'"
catalyst/core/callbacks/optimizer.py,0,"b'from typing import Callable, Dict, List\nimport logging\nimport warnings\n\nfrom catalyst.core import registry, utils\nfrom catalyst.core.callback import Callback, CallbackNode, CallbackOrder\nfrom catalyst.core.runner import IRunner\nfrom catalyst.tools.typing import Optimizer\n\nlogger = logging.getLogger(__name__)\n\n\nclass OptimizerCallback(Callback):\n    """"""Optimizer callback, abstraction over optimizer step.""""""\n\n    def __init__(\n        self,\n        metric_key: str = None,\n        optimizer_key: str = None,\n        accumulation_steps: int = 1,\n        grad_clip_params: Dict = None,\n        decouple_weight_decay: bool = True,\n        loss_key: str = None,\n    ):\n        """"""\n        Args:\n            loss_key (str): key to get loss from ``runner.batch_metrics``\n            optimizer_key (str): A key to take a optimizer in case\n                there are several of them and they are in a dictionary format.\n            accumulation_steps (int): number of steps before\n                ``model.zero_grad()``\n            grad_clip_params (dict): params for gradient clipping\n            decouple_weight_decay (bool): If True - decouple weight decay\n                regularization.\n        """"""\n        super().__init__(order=CallbackOrder.Optimizer, node=CallbackNode.All)\n        assert metric_key is None or loss_key is None\n        if loss_key is not None:\n            warnings.warn(\n                ""OptimizerCallback: ""\n                ""`loss_key` is now deprecated in favor `metric_key`"",\n                stacklevel=2,\n            )\n        self.metric_key: str = metric_key or loss_key or ""loss""\n        self.optimizer_key: str = optimizer_key\n\n        self.accumulation_steps: int = accumulation_steps\n        self._accumulation_counter: int = 0\n\n        grad_clip_params: dict = grad_clip_params or {}\n        self.grad_clip_fn = registry.GRAD_CLIPPERS.get_from_params(\n            **grad_clip_params\n        )\n\n        self.decouple_weight_decay = decouple_weight_decay\n        self._optimizer_wd: List[float] = [0.0]\n\n    @staticmethod\n    def grad_step(\n        *,\n        optimizer: Optimizer,\n        optimizer_wds: List[float] = 0,\n        grad_clip_fn: Callable = None,\n    ) -> None:\n        """"""Makes a gradient step for a given optimizer.\n\n        Args:\n            optimizer (Optimizer): the optimizer\n            optimizer_wds (List[float]): list of weight decay parameters\n                for each param group\n            grad_clip_fn (Callable): function for gradient clipping\n        """"""\n        for group, wd in zip(optimizer.param_groups, optimizer_wds):\n            if wd > 0:\n                for param in group[""params""]:\n                    param.data = param.data.add(-wd * group[""lr""], param.data)\n            if grad_clip_fn is not None:\n                grad_clip_fn(group[""params""])\n        optimizer.step()\n\n    def on_stage_start(self, runner: IRunner) -> None:\n        """"""Checks that the current stage has correct optimizer.""""""\n        self._optimizer = runner.get_attr(\n            key=""optimizer"", inner_key=self.optimizer_key\n        )\n        assert self._optimizer is not None\n\n    def on_epoch_start(self, runner: IRunner) -> None:\n        """"""On epoch start event.\n\n        Args:\n            runner (IRunner): current runner\n        """"""\n        if self.decouple_weight_decay:\n            self._optimizer_wd = [\n                group.get(""weight_decay"", 0.0)\n                for group in self._optimizer.param_groups\n            ]\n            for i in range(len(self._optimizer.param_groups)):\n                self._optimizer.param_groups[i][""weight_decay""] = 0.0\n        else:\n            self._optimizer_wd = [0.0] * len(self._optimizer.param_groups)\n\n    def on_epoch_end(self, runner: IRunner) -> None:\n        """"""On epoch end event.\n\n        Args:\n            runner (IRunner): current runner\n        """"""\n        if self.decouple_weight_decay:\n            for i, wd in enumerate(self._optimizer_wd):\n                self._optimizer.param_groups[i][""weight_decay""] = wd\n\n        lr = self._optimizer.param_groups[0][""lr""]\n        lr_name = (\n            f""lr/{self.optimizer_key}""\n            if self.optimizer_key is not None\n            else ""lr""\n        )\n        runner.epoch_metrics[lr_name] = lr\n\n        momentum = utils.get_optimizer_momentum(self._optimizer)\n        if momentum is not None:\n            momentum_name = (\n                f""momentum/{self.optimizer_key}""\n                if self.optimizer_key is not None\n                else ""momentum""\n            )\n            runner.epoch_metrics[momentum_name] = momentum\n\n    def on_batch_end(self, runner: IRunner) -> None:\n        """"""On batch end event\n\n        Args:\n            runner (IRunner): current runner\n        """"""\n        if not runner.is_train_loader:\n            return\n\n        loss = runner.batch_metrics[self.metric_key]\n\n        self._accumulation_counter += 1\n        need_gradient_step = (\n            self._accumulation_counter % self.accumulation_steps == 0\n        )\n\n        # This is very hacky check whether we have AMP optimizer and this may\n        # change in future.\n        # But alternative solution is to have AmpOptimizerCallback.\n        # or expose another c\'tor argument.\n        if hasattr(self._optimizer, ""_amp_stash""):\n            from apex import amp\n\n            # Need to set ``delay_unscale``\n            # according to\n            # https://nvidia.github.io/apex/advanced.html#gradient-accumulation-across-iterations\n            delay_unscale = not need_gradient_step\n            with amp.scale_loss(\n                loss, self._optimizer, delay_unscale=delay_unscale\n            ) as scaled_loss:\n                scaled_loss.backward()\n        else:\n            loss.backward()\n\n        if need_gradient_step:\n            self.grad_step(\n                optimizer=self._optimizer,\n                optimizer_wds=self._optimizer_wd,\n                grad_clip_fn=self.grad_clip_fn,\n            )\n\n            utils.maybe_recursive_call(self._optimizer, ""zero_grad"")\n            self._accumulation_counter = 0\n\n\n__all__ = [""OptimizerCallback""]\n'"
catalyst/core/callbacks/scheduler.py,1,"b'from typing import Tuple\n\nimport torch\n\nfrom catalyst.contrib.nn.schedulers import BatchScheduler, OneCycleLRWithWarmup\nfrom catalyst.core import utils\nfrom catalyst.core.callback import Callback, CallbackNode, CallbackOrder\nfrom catalyst.core.runner import IRunner\n\n\nclass SchedulerCallback(Callback):\n    """"""@TODO: Docs. Contribution is welcome.""""""\n\n    def __init__(\n        self,\n        scheduler_key: str = None,\n        mode: str = None,\n        reduced_metric: str = None,\n    ):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        super().__init__(order=CallbackOrder.Scheduler, node=CallbackNode.All)\n        self.scheduler_key = scheduler_key\n        self.mode = mode\n        self.reduced_metric = reduced_metric\n\n    @staticmethod\n    def _scheduler_step(\n        scheduler, reduced_metric=None,\n    ):\n        if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n            scheduler.step(reduced_metric)\n            lr = scheduler.optimizer.param_groups[0][""lr""]\n        else:\n            scheduler.step()\n            lr = scheduler.get_lr()[0]\n\n        momentum = utils.get_optimizer_momentum(scheduler.optimizer)\n\n        return lr, momentum\n\n    def step_batch(self, runner: IRunner) -> None:\n        """"""@TODO: Docs. Contribution is welcome.\n\n        Args:\n            runner (IRunner): current runner\n        """"""\n        lr, momentum = self._scheduler_step(scheduler=self._scheduler)\n\n        if self.scheduler_key is not None:\n            runner.batch_metrics[f""lr/{self.scheduler_key}""] = lr\n            if momentum is not None:\n                runner.batch_metrics[\n                    f""momentum/{self.scheduler_key}""\n                ] = momentum\n        else:\n            runner.batch_metrics[""lr""] = lr\n            if momentum is not None:\n                runner.batch_metrics[""momentum""] = momentum\n\n    def step_epoch(self, runner: IRunner) -> None:\n        """"""@TODO: Docs. Contribution is welcome.\n\n        Args:\n            runner (IRunner): current runner\n        """"""\n        reduced_metric = runner.valid_metrics[self.reduced_metric]\n        lr, momentum = self._scheduler_step(\n            scheduler=self._scheduler, reduced_metric=reduced_metric\n        )\n\n        if self.scheduler_key is not None:\n            runner.epoch_metrics[f""lr/{self.scheduler_key}""] = lr\n            if momentum is not None:\n                runner.epoch_metrics[\n                    f""momentum/{self.scheduler_key}""\n                ] = momentum\n        else:\n            runner.epoch_metrics[""lr""] = lr\n            if momentum is not None:\n                runner.epoch_metrics[""momentum""] = momentum\n\n    def on_stage_start(self, runner: IRunner) -> None:\n        """"""Stage start hook.\n\n        Args:\n            runner (IRunner): current runner\n        """"""\n        self.reduced_metric = self.reduced_metric or runner.main_metric\n\n        scheduler = runner.get_attr(\n            key=""scheduler"", inner_key=self.scheduler_key\n        )\n        assert scheduler is not None\n        self._scheduler = scheduler\n\n        if self.mode is None:\n            if isinstance(scheduler, BatchScheduler):\n                self.mode = ""batch""\n            else:\n                self.mode = ""epoch""\n\n        if (\n            isinstance(scheduler, OneCycleLRWithWarmup)\n            and self.mode == ""batch""\n        ):\n            scheduler.reset()\n        assert self.mode is not None\n\n    def on_loader_start(self, runner: IRunner) -> None:\n        """"""Loader start hook.\n\n        Args:\n            runner (IRunner): current runner\n        """"""\n        if (\n            runner.is_train_loader\n            and isinstance(self._scheduler, OneCycleLRWithWarmup)\n            and self.mode == ""batch""\n        ):\n            self._scheduler.recalculate(\n                loader_len=runner.loader_len, current_step=runner.epoch\n            )\n\n    def on_batch_end(self, runner: IRunner) -> None:\n        """"""Batch end hook.\n\n        Args:\n            runner (IRunner): current runner\n        """"""\n        if runner.is_train_loader and self.mode == ""batch"":\n            self.step_batch(runner=runner)\n\n    def on_epoch_end(self, runner: IRunner) -> None:\n        """"""Epoch end hook.\n\n        Args:\n            runner (IRunner): current runner\n        """"""\n        if self.mode == ""epoch"":\n            self.step_epoch(runner=runner)\n\n\nclass LRUpdater(Callback):\n    """"""Basic class that all Lr updaters inherit from.""""""\n\n    def __init__(self, optimizer_key: str = None):\n        """"""\n        Args:\n            optimizer_key (str): which optimizer key to use\n                for learning rate scheduling\n        """"""\n        super().__init__(order=CallbackOrder.Scheduler, node=CallbackNode.All)\n        self.init_lr = 0\n        self.optimizer_key = optimizer_key\n\n    def calc_lr(self):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        return None\n\n    def calc_momentum(self):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        return None\n\n    @staticmethod\n    def _update_lr(optimizer, new_lr) -> None:\n        for pg in optimizer.param_groups:\n            pg[""lr""] = new_lr\n\n    @staticmethod\n    def _update_momentum(optimizer, new_momentum) -> None:\n        if ""betas"" in optimizer.param_groups[0]:\n            for pg in optimizer.param_groups:\n                pg[""betas""] = (new_momentum, pg[""betas""][1])\n        else:\n            for pg in optimizer.param_groups:\n                pg[""momentum""] = new_momentum\n\n    def _update_optimizer(self, optimizer) -> Tuple[float, float]:\n        new_lr = self.calc_lr()\n        if new_lr is not None:\n            self._update_lr(optimizer, new_lr)\n\n        new_momentum = self.calc_momentum()\n        if new_momentum is not None:\n            self._update_momentum(optimizer, new_momentum)\n        else:\n            new_momentum = utils.get_optimizer_momentum(optimizer)\n\n        return new_lr, new_momentum\n\n    def update_optimizer(self, runner: IRunner) -> None:\n        """"""@TODO: Docs. Contribution is welcome.\n\n        Args:\n            runner (IRunner): current runner\n        """"""\n        lr, momentum = self._update_optimizer(optimizer=self._optimizer)\n\n        if self.optimizer_key is not None:\n            runner.batch_metrics[f""lr_{self.optimizer_key}""] = lr\n            runner.batch_metrics[f""momentum_{self.optimizer_key}""] = momentum\n        else:\n            runner.batch_metrics[""lr""] = lr\n            runner.batch_metrics[""momentum""] = momentum\n\n    def on_stage_start(self, runner: IRunner) -> None:\n        """"""Stage start hook.\n\n        Args:\n            runner (IRunner): current runner\n        """"""\n        optimizer = runner.get_attr(\n            key=""optimizer"", inner_key=self.optimizer_key\n        )\n        assert optimizer is not None\n        self._optimizer = optimizer\n        self.init_lr = optimizer.defaults[""lr""]\n\n    def on_loader_start(self, runner: IRunner) -> None:\n        """"""Loader start hook.\n\n        Args:\n            runner (IRunner): current runner\n        """"""\n        if runner.is_train_loader:\n            self.update_optimizer(runner=runner)\n\n    def on_batch_end(self, runner: IRunner) -> None:\n        """"""Batch end hook.\n\n        Args:\n            runner (IRunner): current runner\n        """"""\n        if runner.is_train_loader:\n            self.update_optimizer(runner=runner)\n\n\n__all__ = [""SchedulerCallback"", ""LRUpdater""]\n'"
catalyst/core/callbacks/timer.py,0,"b'from catalyst.core.callback import Callback, CallbackNode, CallbackOrder\nfrom catalyst.core.runner import IRunner\nfrom catalyst.tools.time_manager import TimeManager\n\nEPS = 1e-8\n\n\nclass TimerCallback(Callback):\n    """"""Logs pipeline execution time.""""""\n\n    def __init__(self):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        super().__init__(order=CallbackOrder.Metric + 1, node=CallbackNode.All)\n        self.timer = TimeManager()\n\n    def on_loader_start(self, runner: IRunner) -> None:\n        """"""Loader start hook.\n\n        Args:\n            runner (IRunner): current runner\n        """"""\n        self.timer.reset()\n        self.timer.start(""_timer/batch_time"")\n        self.timer.start(""_timer/data_time"")\n\n    def on_loader_end(self, runner: IRunner) -> None:\n        """"""Loader end hook.\n\n        Args:\n            runner (IRunner): current runner\n        """"""\n        self.timer.reset()\n\n    def on_batch_start(self, runner: IRunner) -> None:\n        """"""Batch start hook.\n\n        Args:\n            runner (IRunner): current runner\n        """"""\n        self.timer.stop(""_timer/data_time"")\n        self.timer.start(""_timer/model_time"")\n\n    def on_batch_end(self, runner: IRunner) -> None:\n        """"""Batch end hook.\n\n        Args:\n            runner (IRunner): current runner\n        """"""\n        self.timer.stop(""_timer/model_time"")\n        self.timer.stop(""_timer/batch_time"")\n\n        # @TODO: just a trick\n        self.timer.elapsed[""_timer/_fps""] = runner.batch_size / (\n            self.timer.elapsed[""_timer/batch_time""] + EPS\n        )\n        for key, value in self.timer.elapsed.items():\n            runner.batch_metrics[key] = value\n\n        self.timer.reset()\n        self.timer.start(""_timer/batch_time"")\n        self.timer.start(""_timer/data_time"")\n\n\n__all__ = [""TimerCallback""]\n'"
catalyst/core/callbacks/validation.py,0,"b'from collections import defaultdict\n\nfrom catalyst.core.callback import Callback, CallbackNode, CallbackOrder\nfrom catalyst.core.runner import IRunner\n\n\nclass ValidationManagerCallback(Callback):\n    """"""\n    A callback to aggregate runner.valid_metrics from runner.epoch_metrics.\n    """"""\n\n    def __init__(self):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        super().__init__(\n            order=CallbackOrder.Validation, node=CallbackNode.All,\n        )\n\n    def on_epoch_start(self, runner: IRunner) -> None:\n        """"""Epoch start hook.\n\n        Args:\n            runner (IRunner): current runner\n        """"""\n        runner.valid_metrics = defaultdict(None)\n        runner.is_best_valid = False\n\n    def on_epoch_end(self, runner: IRunner) -> None:\n        """"""Epoch end hook.\n\n        Args:\n            runner (IRunner): current runner\n        """"""\n        if runner.stage_name.startswith(""infer""):\n            return\n\n        runner.valid_metrics = {\n            k.replace(f""{runner.valid_loader}_"", """"): v\n            for k, v in runner.epoch_metrics.items()\n            if k.startswith(runner.valid_loader)\n        }\n        assert (\n            runner.main_metric in runner.valid_metrics\n        ), f""{runner.main_metric} value is not available by the epoch end""\n\n        current_valid_metric = runner.valid_metrics[runner.main_metric]\n        if runner.minimize_metric:\n            best_valid_metric = runner.best_valid_metrics.get(\n                runner.main_metric, float(""+inf"")\n            )\n            is_best = current_valid_metric < best_valid_metric\n        else:\n            best_valid_metric = runner.best_valid_metrics.get(\n                runner.main_metric, float(""-inf"")\n            )\n            is_best = current_valid_metric > best_valid_metric\n\n        if is_best:\n            runner.is_best_valid = True\n            runner.best_valid_metrics = runner.valid_metrics.copy()\n\n\n__all__ = [""ValidationManagerCallback""]\n'"
catalyst/core/utils/__init__.py,0,"b'# flake8: noqa\n\nfrom catalyst.contrib.utils import *\nfrom catalyst.utils import *\n\nfrom .callbacks import filter_callbacks_by_node, sort_callbacks_by_order\nfrom .data import get_loaders_from_params, validate_loaders\n'"
catalyst/core/utils/callbacks.py,0,"b'from typing import Dict, List, Union\nfrom collections import OrderedDict\n\nfrom catalyst.core.callback import CallbackNode\nfrom catalyst.core.utils import get_rank\n\n\ndef sort_callbacks_by_order(\n    callbacks: Union[List, Dict, OrderedDict]\n) -> OrderedDict:\n    """"""Creates an sequence of callbacks and sort them.\n\n    Args:\n        callbacks: either list of callbacks or ordered dict\n\n    Returns:\n        sequence of callbacks sorted by ``callback order``\n    """"""\n    if callbacks is None:\n        output = OrderedDict()\n    elif isinstance(callbacks, (dict, OrderedDict)):\n        output = [(k, v) for k, v in callbacks.items()]\n        output = sorted(output, key=lambda x: x[1].order)\n        output = OrderedDict(output)\n    elif isinstance(callbacks, list):\n        output = sorted(callbacks, key=lambda x: x.order)\n        output = OrderedDict([(i, value) for i, value in enumerate(output)])\n    else:\n        raise TypeError(\n            f""Callbacks must be either Dict/OrderedDict or list, ""\n            f""got {type(callbacks)}""\n        )\n\n    return output\n\n\ndef filter_callbacks_by_node(\n    callbacks: Union[Dict, OrderedDict]\n) -> Union[Dict, OrderedDict]:\n    """"""\n    Filters callbacks based on running node.\n    Deletes worker-only callbacks from ``CallbackNode.Master``\n    and master-only callbacks from ``CallbackNode.Worker``.\n\n    Args:\n        callbacks (Union[Dict, OrderedDict]): callbacks\n\n    Returns:\n        Union[Dict, OrderedDict]: filtered callbacks dictionary.\n    """"""\n    # distributed run setting\n    output = callbacks.copy()\n    rank = get_rank()\n    if rank == 0:  # master node\n        # remove worker-only callbacks on master node\n        for k in list(\n            filter(lambda c: output[c].node == CallbackNode.Worker, output,)\n        ):\n            del output[k]\n    elif rank > 0:  # worker node\n        # remove master-only callbacks on worker nodes\n        for k in list(\n            filter(lambda c: output[c].node == CallbackNode.Master, output,)\n        ):\n            del output[k]\n    return output\n\n\n__all__ = [""sort_callbacks_by_order"", ""filter_callbacks_by_node""]\n'"
catalyst/core/utils/data.py,10,"b'from typing import Any, Callable, Dict\nfrom collections import OrderedDict\nfrom copy import copy\nimport warnings\n\nimport torch\nfrom torch.utils.data import DataLoader, Dataset, DistributedSampler\n\nfrom catalyst.contrib.registry import SAMPLERS\nfrom catalyst.data import DistributedSamplerWrapper\nfrom catalyst.utils import get_rank, merge_dicts, set_global_seed\n\n\ndef _force_make_distributed_loader(loader: DataLoader) -> DataLoader:\n    """"""\n    Transfers loader to distributed mode. Experimental feature.\n\n    Args:\n        loader (DataLoader): pytorch dataloder\n\n    Returns:\n        (DataLoader): pytorch dataloder with distributed sampler.\n    """"""\n    sampler = (\n        DistributedSampler(dataset=loader.dataset)\n        if getattr(loader, ""sampler"", None) is not None\n        else DistributedSamplerWrapper(sampler=loader.sampler)\n    )\n    loader = DataLoader(\n        dataset=copy(loader.dataset),\n        batch_size=loader.batch_size,\n        # shuffle=loader.shuffle,\n        sampler=sampler,\n        # batch_sampler=loader.batch_sampler,\n        num_workers=loader.num_workers,\n        # collate_fn=loader.collate_fn,\n        pin_memory=loader.pin_memory,\n        drop_last=loader.drop_last,\n    )\n    return loader\n\n\ndef validate_loaders(loaders: Dict[str, DataLoader]) -> Dict[str, DataLoader]:\n    """"""\n    Check pytorch dataloaders for distributed setup.\n    Transfers them to distirbuted mode if necessary.\n    (Experimental feature)\n\n    Args:\n        loaders (Dict[str, DataLoader]): dictionery with pytorch dataloaders\n\n    Returns:\n        (Dict[str, DataLoader]): dictionery\n            with pytorch dataloaders (with distributed samplers if necessary)\n    """"""\n    rank = get_rank()\n    if rank >= 0:\n        for key, value in loaders.items():\n            if not isinstance(\n                value.sampler, (DistributedSampler, DistributedSamplerWrapper)\n            ):\n                warnings.warn(\n                    ""With distributed training setup, ""\n                    ""you need ``DistributedSampler`` for your ``DataLoader``.""\n                    ""Transferring to distributed mode. (Experimental feature)""\n                )\n                loaders[key] = _force_make_distributed_loader(value)\n    return loaders\n\n\ndef get_loaders_from_params(\n    batch_size: int = 1,\n    num_workers: int = 0,\n    drop_last: bool = False,\n    per_gpu_scaling: bool = False,\n    loaders_params: Dict[str, Any] = None,\n    samplers_params: Dict[str, Any] = None,\n    initial_seed: int = 42,\n    get_datasets_fn: Callable = None,\n    **data_params,\n) -> ""OrderedDict[str, DataLoader]"":\n    """"""\n    Creates pytorch dataloaders from datasets and additional parameters.\n\n    Args:\n        batch_size (int): ``batch_size`` parameter\n            from ``torch.utils.data.DataLoader``\n        num_workers (int): ``num_workers`` parameter\n            from ``torch.utils.data.DataLoader``\n        drop_last (bool): ``drop_last`` parameter\n            from ``torch.utils.data.DataLoader``\n        per_gpu_scaling (bool): boolean flag,\n            if ``True``, uses ``batch_size=batch_size*num_available_gpus``\n        loaders_params (Dict[str, Any]): additional loaders parameters\n        samplers_params (Dict[str, Any]): additional sampler parameters\n        initial_seed (int): initial seed for ``torch.utils.data.DataLoader``\n            workers\n        get_datasets_fn(Callable): callable function to get dictionary with\n            ``torch.utils.data.Datasets``\n        **data_params: additional data parameters\n            or dictionary with ``torch.utils.data.Datasets`` to use for\n            pytorch dataloaders creation\n\n    Returns:\n        OrderedDict[str, DataLoader]: dictionary with\n            ``torch.utils.data.DataLoader``\n    """"""\n    default_batch_size = batch_size\n    default_num_workers = num_workers\n    loaders_params = loaders_params or {}\n    assert isinstance(loaders_params, dict), (\n        f""`loaders_params` should be a Dict. "" f""Got: {loaders_params}""\n    )\n    samplers_params = samplers_params or {}\n    assert isinstance(\n        samplers_params, dict\n    ), f""`samplers_params` should be a Dict. Got: {samplers_params}""\n\n    distributed_rank = get_rank()\n    distributed = distributed_rank > -1\n\n    if get_datasets_fn is not None:\n        datasets = get_datasets_fn(**data_params)\n    else:\n        datasets = dict(**data_params)\n\n    loaders = OrderedDict()\n    for name, datasource in datasets.items():\n        assert isinstance(\n            datasource, (Dataset, dict)\n        ), f""{datasource} should be Dataset or Dict. Got: {datasource}""\n\n        loader_params = loaders_params.pop(name, {})\n        assert isinstance(\n            loader_params, dict\n        ), f""{loader_params} should be Dict""\n\n        sampler_params = samplers_params.pop(name, None)\n        if sampler_params is None:\n            if isinstance(datasource, dict) and ""sampler"" in datasource:\n                sampler = datasource.pop(""sampler"", None)\n            else:\n                sampler = None\n        else:\n            sampler = SAMPLERS.get_from_params(**sampler_params)\n            if isinstance(datasource, dict) and ""sampler"" in datasource:\n                datasource.pop(""sampler"", None)\n\n        batch_size = loader_params.pop(""batch_size"", default_batch_size)\n        num_workers = loader_params.pop(""num_workers"", default_num_workers)\n\n        if per_gpu_scaling and not distributed:\n            num_gpus = max(1, torch.cuda.device_count())\n            batch_size *= num_gpus\n            num_workers *= num_gpus\n\n        loader_params = {\n            ""batch_size"": batch_size,\n            ""num_workers"": num_workers,\n            ""pin_memory"": torch.cuda.is_available(),\n            ""drop_last"": drop_last,\n            **loader_params,\n        }\n\n        if isinstance(datasource, Dataset):\n            loader_params[""dataset""] = datasource\n        elif isinstance(datasource, dict):\n            assert (\n                ""dataset"" in datasource\n            ), ""You need to specify dataset for dataloader""\n            loader_params = merge_dicts(datasource, loader_params)\n        else:\n            raise NotImplementedError\n\n        if distributed:\n            if sampler is not None:\n                if not isinstance(sampler, DistributedSampler):\n                    sampler = DistributedSamplerWrapper(sampler=sampler)\n            else:\n                sampler = DistributedSampler(dataset=loader_params[""dataset""])\n\n        loader_params[""shuffle""] = name.startswith(""train"") and sampler is None\n\n        loader_params[""sampler""] = sampler\n\n        if ""batch_sampler"" in loader_params:\n            if distributed:\n                raise ValueError(\n                    ""batch_sampler option is mutually ""\n                    ""exclusive with distributed""\n                )\n\n            for k in (""batch_size"", ""shuffle"", ""sampler"", ""drop_last""):\n                loader_params.pop(k, None)\n\n        if ""worker_init_fn"" not in loader_params:\n            loader_params[""worker_init_fn""] = lambda x: set_global_seed(\n                initial_seed + x\n            )\n\n        loaders[name] = DataLoader(**loader_params)\n\n    return loaders\n\n\n__all__ = [""get_loaders_from_params"", ""validate_loaders""]\n'"
catalyst/data/scripts/__init__.py,0,b''
catalyst/data/scripts/image2embedding.py,8,"b'from typing import Sequence\nimport argparse\nfrom pathlib import Path\n\nimport cv2\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\nimport torch\n\nfrom catalyst.contrib.data.cv import ImageReader\nfrom catalyst.contrib.models.cv import ResnetEncoder\nfrom catalyst.dl import utils\n\ncv2.setNumThreads(0)\ncv2.ocl.setUseOpenCL(False)\n\nIMG_SIZE = (224, 224)\n\n\ndef normalize(\n    tensor: torch.Tensor,\n    mean: Sequence[float] = (0.485, 0.456, 0.406),\n    std: Sequence[float] = (0.229, 0.224, 0.225),\n):\n    """"""Normalize a tensor image with mean and standard deviation.\n\n    Args:\n        tensor (torch.Tensor): Tensor image of size (C, H, W) to be normalized\n        mean (Sequence[float]): Sequence of means for each channel\n        std (Sequence[float]): Sequence of standard deviations for each channel\n\n    Returns:\n        torch.Tensor: Normalized Tensor image\n    """"""\n    dtype = tensor.dtype\n    mean = torch.as_tensor(mean, dtype=dtype, device=tensor.device)\n    std = torch.as_tensor(std, dtype=dtype, device=tensor.device)\n\n    if mean.ndim == 1:\n        mean = mean[:, None, None]\n    if std.ndim == 1:\n        std = std[:, None, None]\n\n    tensor.sub_(mean).div_(std)\n    return tensor\n\n\ndef dict_transformer(sample):\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    image = sample[""image""]\n\n    # image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n    # image = np.concatenate([np.expand_dims(image, -1)] * 3, axis=-1)\n    image = cv2.resize(image, IMG_SIZE, interpolation=cv2.INTER_NEAREST)\n    image = torch.from_numpy(image.astype(np.float32) / 255.0).permute(2, 0, 1)\n    image = normalize(image)\n\n    sample[""image""] = image\n    return sample\n\n\ndef build_args(parser):\n    """"""Constructs the command-line arguments for\n    ``catalyst-data image2embeddings``.\n    """"""\n    parser.add_argument(\n        ""--in-csv"", type=str, dest=""in_csv"", help=""Path to csv with photos""\n    )\n    parser.add_argument(\n        ""--img-rootpath"",\n        type=str,\n        dest=""rootpath"",\n        help=""Path to photos directory"",\n    )\n    parser.add_argument(\n        ""--img-col"",\n        type=str,\n        dest=""img_col"",\n        help=""Column in table that contain image path"",\n    )\n    parser.add_argument(\n        ""--img-size"",\n        type=int,\n        dest=""img_size"",\n        default=224,\n        help=""Target size of images"",\n    )\n    parser.add_argument(\n        ""--out-npy"",\n        type=str,\n        dest=""out_npy"",\n        required=True,\n        help=""Path to output `.npy` file with embedded features"",\n    )\n    parser.add_argument(\n        ""--arch"",\n        type=str,\n        dest=""arch"",\n        default=""resnet18"",\n        help=""Neural network architecture"",\n    )\n    parser.add_argument(\n        ""--pooling"",\n        type=str,\n        dest=""pooling"",\n        default=""GlobalAvgPool2d"",\n        help=""Type of pooling to use"",\n    )\n    parser.add_argument(\n        ""--traced-model"",\n        type=Path,\n        dest=""traced_model"",\n        default=None,\n        help=""Path to pytorch traced model"",\n    )\n    parser.add_argument(\n        ""--num-workers"",\n        type=int,\n        dest=""num_workers"",\n        help=""Count of workers for dataloader"",\n        default=0,\n    )\n    parser.add_argument(\n        ""--batch-size"",\n        type=int,\n        dest=""batch_size"",\n        help=""Dataloader batch size"",\n        default=32,\n    )\n    parser.add_argument(\n        ""--verbose"",\n        dest=""verbose"",\n        action=""store_true"",\n        default=False,\n        help=""Print additional information"",\n    )\n    parser.add_argument(""--seed"", type=int, default=42)\n    utils.boolean_flag(\n        parser,\n        ""deterministic"",\n        default=None,\n        help=""Deterministic mode if running in CuDNN backend"",\n    )\n    utils.boolean_flag(\n        parser, ""benchmark"", default=None, help=""Use CuDNN benchmark""\n    )\n\n    return parser\n\n\ndef parse_args():\n    """"""Parses the command line arguments for the main method.""""""\n    parser = argparse.ArgumentParser()\n    build_args(parser)\n    args = parser.parse_args()\n    return args\n\n\ndef main(args, _=None):\n    """"""Run the ``catalyst-data image2embeddings`` script.""""""\n    global IMG_SIZE\n\n    utils.set_global_seed(args.seed)\n    utils.prepare_cudnn(args.deterministic, args.benchmark)\n\n    IMG_SIZE = (args.img_size, args.img_size)\n\n    if args.traced_model is not None:\n        device = utils.get_device()\n        model = torch.jit.load(str(args.traced_model), map_location=device)\n    else:\n        model = ResnetEncoder(arch=args.arch, pooling=args.pooling)\n        model = model.eval()\n        model, _, _, _, device = utils.process_components(model=model)\n\n    df = pd.read_csv(args.in_csv)\n    df = df.reset_index().drop(""index"", axis=1)\n    df = list(df.to_dict(""index"").values())\n\n    open_fn = ImageReader(\n        input_key=args.img_col, output_key=""image"", rootpath=args.rootpath\n    )\n\n    dataloader = utils.get_loader(\n        df,\n        open_fn,\n        batch_size=args.batch_size,\n        num_workers=args.num_workers,\n        dict_transform=dict_transformer,\n    )\n\n    features = []\n    dataloader = tqdm(dataloader) if args.verbose else dataloader\n    with torch.no_grad():\n        for batch in dataloader:\n            features_ = model(batch[""image""].to(device))\n            features_ = features_.cpu().detach().numpy()\n            features.append(features_)\n\n    features = np.concatenate(features, axis=0)\n    np.save(args.out_npy, features)\n\n\nif __name__ == ""__main__"":\n    args = parse_args()\n    main(args)\n'"
catalyst/data/scripts/process_images.py,0,"b'#!/usr/bin/env python\n# usage:\n# catalyst-data process-images \\\n#   --in-dir ./data_in \\\n#   --out-dir ./data_out \\\n#   --num-workers 4 \\\n#   --max-size 224 \\\n#   --clear-exif \\\n#   --grayscale\n\nfrom typing import List\nimport argparse\nfrom functools import wraps\nfrom multiprocessing.pool import Pool\nimport os\nfrom pathlib import Path\n\nimport cv2\nimport numpy as np\n\nfrom catalyst import utils\n\n# Limit cv2\'s processor usage\n# cv2.setNumThreads() doesn\'t work\nos.environ[""OMP_NUM_THREADS""] = ""1""\nos.environ[""MKL_NUM_THREADS""] = ""1""\ncv2.ocl.setUseOpenCL(False)\n\n\ndef build_args(parser):\n    """"""\n    Constructs the command-line arguments for ``catalyst-data process-images``.\n    """"""\n    parser.add_argument(\n        ""--in-dir"", required=True, type=Path, help=""Raw data folder path""\n    )\n\n    parser.add_argument(\n        ""--out-dir"",\n        required=True,\n        type=Path,\n        help=""Processed images folder path"",\n    )\n\n    parser.add_argument(\n        ""--num-workers"",\n        ""-j"",\n        default=1,\n        type=int,\n        help=""Number of workers to parallel the processing"",\n    )\n\n    parser.add_argument(\n        ""--max-size"",\n        default=None,\n        required=False,\n        type=int,\n        help=""Output images size. E.g. 224, 448"",\n    )\n\n    utils.boolean_flag(\n        parser, ""clear-exif"", default=True, help=""Clear EXIF data""\n    )\n\n    utils.boolean_flag(\n        parser, ""grayscale"", default=False, help=""Read images in grayscale""\n    )\n\n    utils.boolean_flag(\n        parser,\n        ""expand-dims"",\n        default=True,\n        help=""Expand array shape for grayscale images"",\n    )\n\n    return parser\n\n\ndef parse_args():\n    """"""Parses the command line arguments for the main method.""""""\n    parser = argparse.ArgumentParser()\n    build_args(parser)\n    args = parser.parse_args()\n    return args\n\n\n# <--- taken from albumentations - https://github.com/albu/albumentations --->\n\n\ndef py3round(number):\n    """"""Unified rounding in all python versions.""""""\n    if abs(round(number) - number) == 0.5:\n        return int(2.0 * round(number / 2.0))\n\n    return int(round(number))\n\n\ndef preserve_channel_dim(func):\n    """"""Preserve dummy channel dim.""""""  # noqa: D202\n\n    @wraps(func)\n    def wrapped_function(img, *args, **kwargs):\n        shape = img.shape\n        result = func(img, *args, **kwargs)\n        if len(shape) == 3 and shape[-1] == 1 and len(result.shape) == 2:\n            result = np.expand_dims(result, axis=-1)\n        return result\n\n    return wrapped_function\n\n\ndef _func_max_size(img, max_size, interpolation, func):\n    height, width = img.shape[:2]\n\n    scale = max_size / float(func(width, height))\n\n    if scale != 1.0:\n        out_size = tuple(py3round(dim * scale) for dim in (width, height))\n        img = cv2.resize(img, out_size, interpolation=interpolation)\n    return img\n\n\n@preserve_channel_dim\ndef longest_max_size(img, max_size, interpolation):\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    return _func_max_size(img, max_size, interpolation, max)\n\n\n# <--- taken from albumentations - https://github.com/albu/albumentations --->\n\n\nclass Preprocessor:\n    """"""@TODO: Docs. Contribution is welcome.""""""\n\n    def __init__(\n        self,\n        in_dir: Path,\n        out_dir: Path,\n        max_size: int = None,\n        clear_exif: bool = True,\n        grayscale: bool = False,\n        expand_dims: bool = True,\n        interpolation=cv2.INTER_LANCZOS4,\n    ):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        self.in_dir = in_dir\n        self.out_dir = out_dir\n        self.grayscale = grayscale\n        self.expand_dims = expand_dims\n        self.max_size = max_size\n        self.clear_exif = clear_exif\n        self.interpolation = interpolation\n\n    def preprocess(self, image_path: Path):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        try:\n            _, extension = os.path.splitext(image_path)\n            kwargs = {\n                ""grayscale"": self.grayscale,\n                ""expand_dims"": self.expand_dims,\n            }\n            if extension.lower() in {""jpg"", ""jpeg""}:\n                # imread does not have exifrotate for non-jpeg type\n                kwargs[""exifrotate""] = not self.clear_exif\n\n            image = np.array(utils.imread(uri=image_path, **kwargs))\n        except Exception as e:\n            print(f""Cannot read file {image_path}, exception: {e}"")\n            return\n\n        if self.max_size is not None:\n            image = longest_max_size(image, self.max_size, self.interpolation)\n\n        target_path = self.out_dir / image_path.relative_to(self.in_dir)\n        target_path.parent.mkdir(parents=True, exist_ok=True)\n\n        image = image.clip(0, 255).round().astype(np.uint8)\n        utils.imwrite(target_path, image)\n\n    def process_all(self, pool: Pool):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        images: List[Path] = []\n        for root, _, files in os.walk(self.in_dir):\n            root = Path(root)\n            images.extend(\n                [\n                    root / filename\n                    for filename in files\n                    if utils.has_image_extension(filename)\n                ]\n            )\n\n        utils.tqdm_parallel_imap(self.preprocess, images, pool)\n\n\ndef main(args, _=None):\n    """"""Run the ``catalyst-data process-images`` script.""""""\n    args = args.__dict__\n    args.pop(""command"", None)\n    num_workers = args.pop(""num_workers"")\n\n    with utils.get_pool(num_workers) as p:\n        Preprocessor(**args).process_all(p)\n\n\nif __name__ == ""__main__"":\n    args = parse_args()\n    main(args)\n'"
catalyst/data/scripts/project_embeddings.py,1,"b'import argparse\nimport os\nfrom os import path\n\nimport cv2\nimport numpy as np\nimport pandas as pd\n\nimport torch\n\nfrom catalyst.contrib.tools.tensorboard import SummaryWriter\n\n\ndef build_args(parser):\n    """"""Constructs the command-line arguments.""""""\n    parser.add_argument(\n        ""--in-npy"",\n        type=str,\n        help=""path to npy with project embeddings"",\n        required=True,\n    )\n    parser.add_argument(\n        ""--in-csv"", type=str, help=""path to csv with photos"", required=True\n    )\n    parser.add_argument(\n        ""--out-dir"",\n        type=str,\n        default=None,\n        help=""directory to output files"",\n        required=True,\n    )\n    parser.add_argument(\n        ""--out-prefix"",\n        type=str,\n        default=None,\n        help=""additional prefix to saved files"",\n    )\n    parser.add_argument(\n        ""--img-col"",\n        type=str,\n        default=None,\n        help=""column in the table that contains image paths"",\n    )\n    parser.add_argument(\n        ""--img-rootpath"", type=str, help=""path to photos directory""\n    )\n    parser.add_argument(\n        ""--img-size"",\n        type=int,\n        default=16,\n        help=""if --img-col is defined, ""\n        ""then images will be resized to (img-size, img-size, 3)"",\n    )\n    parser.add_argument(\n        ""--num-rows"",\n        type=int,\n        default=None,\n        help=""count of rows to use in csv ""\n        ""(if not defined then it will use whole data)"",\n    )\n    parser.add_argument(\n        ""--meta-cols"",\n        type=str,\n        default=None,\n        help=""columns in the table to save, separated by commas"",\n    )\n\n    return parser\n\n\ndef parse_args():\n    """"""Parses the command line arguments for the main method.""""""\n    parser = argparse.ArgumentParser()\n    build_args(parser)\n    args = parser.parse_args()\n    return args\n\n\ndef load_image(filename, size):\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    image = cv2.imread(filename)[..., ::-1]\n    image = cv2.resize(image, (size, size), interpolation=cv2.INTER_NEAREST)\n    return image\n\n\ndef main(args, _=None):\n    """"""Run ``catalyst-data project-embeddings`` script.""""""\n    df = pd.read_csv(args.in_csv)\n    os.makedirs(args.out_dir, exist_ok=True)\n\n    if args.meta_cols is not None:\n        meta_header = args.meta_cols.split("","")\n    else:\n        raise ValueError(""meta-cols must not be None"")\n\n    features = np.load(args.in_npy, mmap_mode=""r"")\n\n    if args.num_rows is not None:\n        df = df.sample(n=args.num_rows)\n\n    if args.img_col is not None:\n        image_names = [\n            path.join(args.img_rootpath, name)\n            for name in df[args.img_col].values\n        ]\n        img_data = np.stack(\n            [load_image(name, args.img_size) for name in image_names], axis=0\n        )\n        img_data = (img_data.transpose((0, 3, 1, 2)) / 255.0).astype(\n            np.float32\n        )\n        img_data = torch.from_numpy(img_data)\n    else:\n        img_data = None\n\n    summary_writer = SummaryWriter(args.out_dir)\n    summary_writer.add_embedding(\n        features,\n        metadata=df[meta_header].astype(str).values,\n        label_img=img_data,\n        metadata_header=meta_header,\n    )\n    summary_writer.close()\n\n    print(\n        f""Done. Run `tensorboard --logdir={args.out_dir}` ""\n        f""to view in Tensorboard""\n    )\n\n\nif __name__ == ""__main__"":\n    args = parse_args()\n    main(args)\n'"
catalyst/data/scripts/split_dataframe.py,0,"b'import argparse\nimport json\nfrom pathlib import Path\n\nimport pandas as pd\n\nfrom catalyst import utils\n\n\ndef build_args(parser):\n    """"""Constructs the command-line arguments for\n    ``catalyst-data split-dataframe``.\n    """"""\n    parser.add_argument(\n        ""--in-csv"",\n        type=Path,\n        dest=""in_csv"",\n        help=""Path to the csv to split"",\n        required=True,\n    )\n    parser.add_argument(\n        ""-n"", ""--num-folds"", type=int, default=5, help=""Number of result folds""\n    )\n    parser.add_argument(\n        ""-t"",\n        ""--train-folds"",\n        type=str,\n        dest=""train_folds"",\n        help=""Numbers separated by commas. They represent train folds"",\n        required=True,\n    )\n    parser.add_argument(\n        ""-v"",\n        ""--valid-folds"",\n        type=str,\n        dest=""valid_folds"",\n        default=None,\n        help=""Numbers separated by commas. They represent valid folds"",\n    )\n    parser.add_argument(\n        ""-i"",\n        ""--infer-folds"",\n        type=str,\n        dest=""infer_folds"",\n        default=None,\n        help=""Numbers separated by commas. They represent infer folds"",\n    )\n\n    parser.add_argument(\n        ""--out-csv"",\n        type=str,\n        help=""Output CSV path for train and valid parts"",\n        required=True,\n    )\n\n    parser.add_argument(\n        ""--tag2class"",\n        type=str,\n        default=None,\n        help=""Path to YAML or JSON of label mappings"",\n    )\n    parser.add_argument(\n        ""--tag-column"",\n        type=str,\n        default=None,\n        dest=""tag_column"",\n        help=""Column of labels (works in pair with `--tag2class` flag)"",\n    )\n    parser.add_argument(\n        ""--class-column"",\n        type=str,\n        default=None,\n        dest=""class_column"",\n        help=""Column of classes"",\n    )\n\n    parser.add_argument(\n        ""--seed"", type=int, default=42, help=""Random seed for split folds""\n    )\n\n    return parser\n\n\ndef parse_args():\n    """"""Parses the command line arguments for the main method.""""""\n    parser = argparse.ArgumentParser()\n    build_args(parser)\n    args, uargs = parser.parse_known_args()\n    return args, uargs\n\n\ndef main(args, uargs=None):\n    """"""Run the ``catalyst-data split-dataframe`` script.""""""\n    dataframe = pd.read_csv(args.in_csv)\n\n    train_folds = (\n        utils.folds_to_list(args.train_folds)\n        if args.train_folds is not None\n        else None\n    )\n    valid_folds = (\n        utils.folds_to_list(args.valid_folds)\n        if args.valid_folds is not None\n        else None\n    )\n    infer_folds = (\n        utils.folds_to_list(args.infer_folds)\n        if args.infer_folds is not None\n        else None\n    )\n\n    tag2class = (\n        json.load(open(args.tag2class)) if args.tag2class is not None else None\n    )\n\n    df_all, train, valid, infer = utils.split_dataframe(\n        dataframe,\n        train_folds=train_folds,\n        valid_folds=valid_folds,\n        infer_folds=infer_folds,\n        tag2class=tag2class,\n        tag_column=args.tag_column,\n        class_column=args.class_column,\n        seed=args.seed,\n        n_folds=args.num_folds,\n    )\n\n    out_csv: str = args.out_csv\n    if out_csv.endswith("".csv""):\n        out_csv = out_csv[:-4]\n\n    df_all.to_csv(f""{out_csv}.csv"", index=False)\n    train.to_csv(f""{out_csv}_train.csv"", index=False)\n    valid.to_csv(f""{out_csv}_valid.csv"", index=False)\n    infer.to_csv(f""{out_csv}_infer.csv"", index=False)\n\n\nif __name__ == ""__main__"":\n    args, uargs = parse_args()\n    main(args, uargs)\n'"
catalyst/data/scripts/tag2label.py,0,"b'import argparse\nimport json\n\nimport pandas as pd\n\nfrom catalyst import utils\n\n\ndef build_args(parser):\n    """"""\n    Constructs the command-line arguments for ``catalyst-data tag2label``.\n    """"""\n    parser.add_argument(\n        ""--in-csv"", type=str, default=None, help=""Path to data in `.csv`.""\n    )\n    parser.add_argument(\n        ""--in-dir"",\n        type=str,\n        default=None,\n        help=""Path to directory with dataset""\n        ""or paths separated by commas for several datasets"",\n    )\n\n    parser.add_argument(\n        ""--out-dataset"",\n        type=str,\n        default=None,\n        required=True,\n        help=""Path to output dataframe"",\n    )\n    parser.add_argument(\n        ""--out-labeling"",\n        type=str,\n        default=None,\n        required=True,\n        help=""Path to output JSON"",\n    )\n\n    parser.add_argument(\n        ""--tag-column"", type=str, default=""tag"", help=""Target column name""\n    )\n    parser.add_argument(\n        ""--tag-delim"",\n        type=str,\n        default=None,\n        help=""Separator if you want to use several target columns"",\n    )\n    utils.boolean_flag(\n        parser, ""recursive"", default=False, help=""Include subdirs in dataset"",\n    )\n\n    return parser\n\n\ndef parse_args():\n    """"""Parses the command line arguments for the main method.""""""\n    parser = argparse.ArgumentParser()\n    build_args(parser)\n    args = parser.parse_args()\n    return args\n\n\ndef _prepare_df_from_dirs(in_dirs, tag_column_name, recursive: bool = False):\n    dfs = []\n    splitted_dirs = in_dirs.strip("","").split("","")\n\n    def process_fn(x):\n        if len(splitted_dirs) == 1:\n            # remove all in_dir part from path\n            return x.replace(f""{in_dir}"", """")\n        else:\n            # leaves last part of in_dir path,\n            #  which identifies separate in_dir\n            return x.replace(f""{in_dir}"", f""{in_dir.split(\'/\')[-2]}/"")\n\n    for in_dir in splitted_dirs:\n        if not in_dir.endswith(""/""):\n            in_dir = f""{in_dir}/""\n\n        dataset = utils.create_dataset(\n            f""{in_dir}/**"", process_fn=process_fn, recursive=recursive\n        )\n\n        dfs.append(\n            utils.create_dataframe(\n                dataset, columns=[tag_column_name, ""filepath""]\n            )\n        )\n\n    df = pd.concat(dfs).reset_index(drop=True)\n    return df\n\n\ndef main(args, _=None):\n    """"""Run the ``catalyst-data tag2label`` script.""""""\n    if args.in_csv is not None:\n        df = pd.read_csv(args.in_csv)\n    elif args.in_dir is not None:\n        df = _prepare_df_from_dirs(\n            args.in_dir, args.tag_column, recursive=args.recursive\n        )\n    else:\n        raise Exception\n\n    if args.tag_delim is not None:\n        df = utils.separate_tags(\n            df, tag_column=args.tag_column, tag_delim=args.tag_delim\n        )\n\n    tag2lbl = utils.get_dataset_labeling(df, args.tag_column)\n    print(""Num classes: "", len(tag2lbl))\n\n    with open(args.out_labeling, ""w"") as fout:\n        json.dump(tag2lbl, fout, indent=4)\n\n    if args.out_dataset is not None:\n        df.to_csv(args.out_dataset, index=False)\n\n\nif __name__ == ""__main__"":\n    args = parse_args()\n    main(args)\n'"
catalyst/data/scripts/text2embedding.py,2,"b'import argparse\nfrom functools import partial\nfrom pathlib import Path\nimport sys\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\nimport torch\nfrom transformers import BertConfig, BertModel, BertTokenizer\n\nfrom catalyst.contrib.utils import process_bert_output, tokenize_text\nfrom catalyst.data import LambdaReader\nfrom catalyst.dl import utils\n\n\ndef build_args(parser):\n    """"""Constructs the command-line arguments for\n    ``catalyst-data text2embeddings``.\n    """"""\n    parser.add_argument(\n        ""--in-csv"", type=str, help=""Path to csv with text"", required=True\n    )\n    parser.add_argument(\n        ""--txt-col"", type=str, help=""Column in table that contain text""\n    )\n    parser.add_argument(\n        ""--in-huggingface"",\n        type=str,\n        required=False,\n        help=""model from huggingface hub"",\n    )\n    required_path_to_model = True\n\n    for arg in sys.argv:\n        if ""--in-huggingface"" in arg:\n            required_path_to_model = False\n\n    if required_path_to_model:\n        parser.add_argument(\n            ""--in-config"", type=Path, required=required_path_to_model\n        )\n        parser.add_argument(\n            ""--in-model"", type=Path, required=required_path_to_model\n        )\n        parser.add_argument(\n            ""--in-vocab"", type=Path, required=required_path_to_model\n        )\n\n    parser.add_argument(\n        ""--out-prefix"", type=str, required=True,\n    )\n    parser.add_argument(""--max-length"", type=int, default=512)\n    utils.boolean_flag(parser, ""mask-for-max-length"", default=False)\n    utils.boolean_flag(parser, ""output-hidden-states"", default=False)\n    utils.boolean_flag(parser, ""strip"", default=True)\n    utils.boolean_flag(parser, ""lowercase"", default=True)\n    utils.boolean_flag(parser, ""remove-punctuation"", default=True)\n    parser.add_argument(""--pooling"", type=str, default=""avg"")\n    parser.add_argument(\n        ""--num-workers"",\n        type=int,\n        dest=""num_workers"",\n        help=""Count of workers for dataloader"",\n        default=0,\n    )\n    parser.add_argument(\n        ""--batch-size"",\n        type=int,\n        dest=""batch_size"",\n        help=""Dataloader batch size"",\n        default=32,\n    )\n    parser.add_argument(\n        ""--verbose"",\n        dest=""verbose"",\n        action=""store_true"",\n        default=False,\n        help=""Print additional information"",\n    )\n    parser.add_argument(""--seed"", type=int, default=42)\n    utils.boolean_flag(\n        parser,\n        ""deterministic"",\n        default=None,\n        help=""Deterministic mode if running in CuDNN backend"",\n    )\n    utils.boolean_flag(\n        parser, ""benchmark"", default=None, help=""Use CuDNN benchmark""\n    )\n\n    return parser\n\n\ndef parse_args():\n    """"""Parses the command line arguments for the main method.""""""\n    parser = argparse.ArgumentParser()\n    build_args(parser)\n    args = parser.parse_args()\n    return args\n\n\ndef _detach(tensor):\n    return tensor.cpu().detach().numpy()\n\n\n@torch.no_grad()\ndef main(args, _=None):\n    """"""Run the ``catalyst-data text2embeddings`` script.""""""\n    batch_size = args.batch_size\n    num_workers = args.num_workers\n    max_length = args.max_length\n    pooling_groups = args.pooling.split("","")\n\n    utils.set_global_seed(args.seed)\n    utils.prepare_cudnn(args.deterministic, args.benchmark)\n\n    if hasattr(args, ""in_huggingface""):\n        model_config = BertConfig.from_pretrained(args.in_huggingface)\n        model_config.output_hidden_states = args.output_hidden_states\n        model = BertModel.from_pretrained(\n            args.in_huggingface, config=model_config\n        )\n        tokenizer = BertTokenizer.from_pretrained(args.in_huggingface)\n    else:\n        model_config = BertConfig.from_pretrained(args.in_config)\n        model_config.output_hidden_states = args.output_hidden_states\n        model = BertModel(config=model_config)\n        tokenizer = BertTokenizer.from_pretrained(args.in_vocab)\n    if hasattr(args, ""in_model""):\n        checkpoint = utils.load_checkpoint(args.in_model)\n        checkpoint = {""model_state_dict"": checkpoint}\n        utils.unpack_checkpoint(checkpoint=checkpoint, model=model)\n\n    model = model.eval()\n    model, _, _, _, device = utils.process_components(model=model)\n\n    df = pd.read_csv(args.in_csv)\n    df = df.dropna(subset=[args.txt_col])\n    df.to_csv(f""{args.out_prefix}.df.csv"", index=False)\n    df = df.reset_index().drop(""index"", axis=1)\n    df = list(df.to_dict(""index"").values())\n    num_samples = len(df)\n\n    open_fn = LambdaReader(\n        input_key=args.txt_col,\n        output_key=None,\n        lambda_fn=partial(\n            tokenize_text,\n            strip=args.strip,\n            lowercase=args.lowercase,\n            remove_punctuation=args.remove_punctuation,\n        ),\n        tokenizer=tokenizer,\n        max_length=max_length,\n    )\n\n    dataloader = utils.get_loader(\n        df, open_fn, batch_size=batch_size, num_workers=num_workers,\n    )\n\n    features = {}\n    dataloader = tqdm(dataloader) if args.verbose else dataloader\n    with torch.no_grad():\n        for idx, batch in enumerate(dataloader):\n            batch = utils.any2device(batch, device)\n            bert_output = model(**batch)\n            mask = (\n                batch[""attention_mask""].unsqueeze(-1)\n                if args.mask_for_max_length\n                else None\n            )\n\n            if utils.check_ddp_wrapped(model):\n                # using several gpu\n                hidden_size = model.module.config.hidden_size\n                hidden_states = model.module.config.output_hidden_states\n\n            else:\n                # using cpu or one gpu\n                hidden_size = model.config.hidden_size\n                hidden_states = model.config.output_hidden_states\n\n            features_ = process_bert_output(\n                bert_output=bert_output,\n                hidden_size=hidden_size,\n                output_hidden_states=hidden_states,\n                pooling_groups=pooling_groups,\n                mask=mask,\n            )\n\n            # create storage based on network output\n            if idx == 0:\n                for key, value in features_.items():\n                    name_ = key if isinstance(key, str) else f""{key:02d}""\n                    _, embedding_size = value.shape\n                    features[name_] = np.memmap(\n                        f""{args.out_prefix}.{name_}.npy"",\n                        dtype=np.float32,\n                        mode=""w+"",\n                        shape=(num_samples, embedding_size),\n                    )\n\n            indices = np.arange(\n                idx * batch_size, min((idx + 1) * batch_size, num_samples)\n            )\n            for key, value in features_.items():\n                name_ = key if isinstance(key, str) else f""{key:02d}""\n                features[name_][indices] = _detach(value)\n\n\nif __name__ == ""__main__"":\n    args = parse_args()\n    main(args)\n'"
catalyst/data/tests/__init__.py,0,b''
catalyst/data/tests/test_sampler.py,0,"b'from typing import List, Tuple\nfrom collections import Counter\nfrom operator import itemgetter\nfrom random import randint, shuffle\n\nimport pytest\n\nfrom catalyst.data.sampler import BalanceBatchSampler\n\n\n@pytest.fixture()\ndef input_balance_batch_sampler() -> List[Tuple[List[int], int, int]]:\n    """"""\n    Returns: test data for sampler in the following order: (labels, p, k)\n    """"""\n    input_cases = [\n        # ideal case\n        ([0, 1, 2, 3, 0, 1, 2, 3], 2, 2),\n        # repetation sampling is needed for class #3\n        ([0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2], 2, 3),\n        # check last batch behaviour:\n        # last batch includes less than p classes (2 < 3)\n        ([0, 1, 2, 3, 4, 0, 1, 2, 3, 4], 3, 2),\n        # we need to drop 1 class during the epoch because\n        # number of classes in data % p = 1\n        ([0, 1, 2, 3, 0, 1, 2, 3], 3, 2),\n        # several random cases\n        ([0, 1, 2, 2, 1, 0, 1, 0, 2, 0, 1, 2], 3, 5),\n        ([0, 1, 2, 2, 1, 0, 1, 0, 2, 0, 1, 2], 2, 3),\n        ([0, 1, 2, 2, 1, 0, 1, 0, 2, 0, 1, 2], 3, 2),\n    ]\n\n    num_random_cases = 0\n    # (alekseysh) It was checked once with N = 100_000 before doing the PR\n    for _ in range(num_random_cases):\n        # code below generates same valid inputs for sampler\n        p, k = randint(2, 12), randint(2, 12)\n        labels_ = [[label] * randint(2, 12) for label in range(p + 1)]\n        labels = [el for sublist in labels_ for el in sublist]\n        shuffle(labels)\n        input_cases.append((labels, p, k))\n\n    return input_cases\n\n\ndef single_check_balance_batch_sampler(\n    labels: List[int], p: int, k: int\n) -> None:\n    """"""\n    Args:\n        labels: list of classes labels\n        p: number of classes in a batch\n        k: number of instances for each class in a batch\n\n    Returns: None\n    """"""\n    sampler = BalanceBatchSampler(labels=labels, p=p, k=k)\n    sampled_ids = list(sampler)\n\n    sampled_classes = []\n    # emulating of 1 epoch\n    for i in range(sampler.batches_in_epoch):\n        i_batch_start = i * sampler.batch_size\n        i_batch_end = min((i + 1) * sampler.batch_size, len(sampler) + 1)\n        batch_ids = sampled_ids[i_batch_start:i_batch_end]\n        batch_labels = itemgetter(*batch_ids)(labels)\n\n        labels_counter = Counter(batch_labels)\n        num_batch_classes = len(labels_counter)\n        num_batch_instances = list(labels_counter.values())\n        cur_batch_size = len(batch_labels)\n        sampled_classes.extend(list(labels_counter.keys()))\n\n        # batch-level invariants\n        assert 4 <= len(set(batch_ids))\n\n        is_last_batch = i == sampler.batches_in_epoch - 1\n        if is_last_batch:\n            assert 1 < num_batch_classes <= p\n            assert all(1 < el <= k for el in num_batch_instances)\n            assert 2 * 2 <= cur_batch_size <= p * k\n        else:\n            assert num_batch_classes == p\n            assert all(el == k for el in num_batch_instances)\n            assert cur_batch_size == p * k\n\n    # epoch-level invariants\n    num_classes_in_data = len(set(labels))\n    num_classes_in_epoch = len(set(sampled_classes))\n    assert (num_classes_in_data == num_classes_in_epoch) or (\n        num_classes_in_data == num_classes_in_epoch + 1\n    )\n\n    assert max(sampled_ids) <= len(labels) - 1\n\n\ndef test_balance_batch_sampler(input_balance_batch_sampler) -> None:\n    """"""\n    Args:\n        input_balance_batch_sampler: pytest fixture\n\n    Returns: None\n    """"""\n    for labels, p, k in input_balance_batch_sampler:\n        single_check_balance_batch_sampler(labels, p, k)\n'"
catalyst/dl/callbacks/__init__.py,0,"b'# flake8: noqa\n\nfrom catalyst.core.callback import *\nfrom catalyst.core.callbacks import *\n\nfrom .confusion_matrix import ConfusionMatrixCallback\nfrom .inference import InferCallback\nfrom .meter import MeterMetricsCallback\nfrom .metrics import (\n    AccuracyCallback,\n    AUCCallback,\n    ClasswiseIouCallback,\n    ClasswiseJaccardCallback,\n    DiceCallback,\n    F1ScoreCallback,\n    IouCallback,\n    JaccardCallback,\n    MapKCallback,\n    MulticlassDiceMetricCallback,\n    PrecisionRecallF1ScoreCallback,\n)\nfrom .mixup import MixupCallback\nfrom .scheduler import LRFinder\n\nfrom catalyst.contrib.dl.callbacks import *  # isort:skip\n'"
catalyst/dl/callbacks/confusion_matrix.py,3,"b'from typing import Dict, List\n\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix as confusion_matrix_fn\n\nimport torch\nimport torch.distributed\n\nfrom catalyst.core import Callback, CallbackNode, CallbackOrder, IRunner\nfrom catalyst.dl import utils\nfrom catalyst.tools import meters\n\n\nclass ConfusionMatrixCallback(Callback):\n    """"""@TODO: Docs. Contribution is welcome.""""""\n\n    def __init__(\n        self,\n        input_key: str = ""targets"",\n        output_key: str = ""logits"",\n        prefix: str = ""confusion_matrix"",\n        version: str = ""tnt"",\n        class_names: List[str] = None,\n        num_classes: int = None,\n        plot_params: Dict = None,\n        tensorboard_callback_name: str = ""_tensorboard"",\n    ):\n        """"""\n        Args:\n            @TODO: Docs. Contribution is welcome\n        """"""\n        super().__init__(CallbackOrder.Metric, CallbackNode.All)\n        self.prefix = prefix\n        self.output_key = output_key\n        self.input_key = input_key\n        self.tensorboard_callback_name = tensorboard_callback_name\n\n        assert version in [""tnt"", ""sklearn""]\n        self._version = version\n        self._plot_params = plot_params or {}\n\n        self.class_names = class_names\n        self.num_classes = (\n            num_classes if class_names is None else len(class_names)\n        )\n\n        assert self.num_classes is not None\n        self._reset_stats()\n\n    def _reset_stats(self):\n        if self._version == ""tnt"":\n            self.confusion_matrix = meters.ConfusionMeter(self.num_classes)\n        elif self._version == ""sklearn"":\n            self.outputs = []\n            self.targets = []\n\n    def _add_to_stats(self, outputs, targets):\n        if self._version == ""tnt"":\n            self.confusion_matrix.add(predicted=outputs, target=targets)\n        elif self._version == ""sklearn"":\n            outputs = outputs.cpu().numpy()\n            targets = targets.cpu().numpy()\n\n            outputs = np.argmax(outputs, axis=1)\n\n            self.outputs.extend(outputs)\n            self.targets.extend(targets)\n\n    def _compute_confusion_matrix(self):\n        if self._version == ""tnt"":\n            confusion_matrix = self.confusion_matrix.value()\n        elif self._version == ""sklearn"":\n            confusion_matrix = confusion_matrix_fn(\n                y_true=self.targets, y_pred=self.outputs\n            )\n        else:\n            raise NotImplementedError()\n        return confusion_matrix\n\n    def _plot_confusion_matrix(\n        self, logger, epoch, confusion_matrix, class_names=None\n    ):\n        fig = utils.plot_confusion_matrix(\n            confusion_matrix,\n            class_names=class_names,\n            normalize=True,\n            show=False,\n            **self._plot_params,\n        )\n        fig = utils.render_figure_to_tensor(fig)\n        logger.add_image(f""{self.prefix}/epoch"", fig, global_step=epoch)\n\n    def on_loader_start(self, runner: IRunner):\n        """"""Loader start hook.\n\n        Args:\n            runner (IRunner): current runner\n        """"""\n        self._reset_stats()\n\n    def on_batch_end(self, runner: IRunner):\n        """"""Batch end hook.\n\n        Args:\n            runner (IRunner): current runner\n        """"""\n        self._add_to_stats(\n            runner.output[self.output_key].detach(),\n            runner.input[self.input_key].detach(),\n        )\n\n    def on_loader_end(self, runner: IRunner):\n        """"""Loader end hook.\n\n        Args:\n            runner (IRunner): current runner\n        """"""\n        class_names = self.class_names or [\n            str(i) for i in range(self.num_classes)\n        ]\n        confusion_matrix = self._compute_confusion_matrix()\n\n        if runner.distributed_rank >= 0:\n            confusion_matrix = torch.from_numpy(confusion_matrix)\n            confusion_matrix = confusion_matrix.to(utils.get_device())\n            torch.distributed.reduce(confusion_matrix, 0)\n            confusion_matrix = confusion_matrix.cpu().numpy()\n\n        if runner.distributed_rank <= 0:\n            tb_callback = runner.callbacks[self.tensorboard_callback_name]\n            self._plot_confusion_matrix(\n                logger=tb_callback.loggers[runner.loader_name],\n                epoch=runner.global_epoch,\n                confusion_matrix=confusion_matrix,\n                class_names=class_names,\n            )\n\n\n__all__ = [""ConfusionMatrixCallback""]\n'"
catalyst/dl/callbacks/inference.py,0,"b'from collections import defaultdict\nimport os\n\nimport numpy as np\n\nfrom catalyst.core import Callback, CallbackOrder, IRunner\n\n\n# @TODO: refactor\nclass InferCallback(Callback):\n    """"""@TODO: Docs. Contribution is welcome.""""""\n\n    def __init__(self, out_dir=None, out_prefix=None):\n        """"""\n        Args:\n            @TODO: Docs. Contribution is welcome\n        """"""\n        super().__init__(CallbackOrder.Internal)\n        self.out_dir = out_dir\n        self.out_prefix = out_prefix\n        self.predictions = defaultdict(lambda: [])\n        self._keys_from_runner = [""out_dir"", ""out_prefix""]\n\n    def on_stage_start(self, runner: IRunner):\n        """"""Stage start hook.\n\n        Args:\n            runner (IRunner): current runner\n        """"""\n        for key in self._keys_from_runner:\n            value = getattr(runner, key, None)\n            if value is not None:\n                setattr(self, key, value)\n        # assert self.out_prefix is not None\n        if self.out_dir is not None:\n            self.out_prefix = str(self.out_dir) + ""/"" + str(self.out_prefix)\n        if self.out_prefix is not None:\n            os.makedirs(os.path.dirname(self.out_prefix), exist_ok=True)\n\n    def on_loader_start(self, runner: IRunner):\n        """"""Loader start hook.\n\n        Args:\n            runner (IRunner): current runner\n        """"""\n        self.predictions = defaultdict(lambda: [])\n\n    def on_batch_end(self, runner: IRunner):\n        """"""Batch end hook.\n\n        Args:\n            runner (IRunner): current runner\n        """"""\n        dct = runner.output\n        dct = {key: value.detach().cpu().numpy() for key, value in dct.items()}\n        for key, value in dct.items():\n            self.predictions[key].append(value)\n\n    def on_loader_end(self, runner: IRunner):\n        """"""Loader end hook.\n\n        Args:\n            runner (IRunner): current runner\n        """"""\n        self.predictions = {\n            key: np.concatenate(value, axis=0)\n            for key, value in self.predictions.items()\n        }\n        if self.out_prefix is not None:\n            for key, value in self.predictions.items():\n                suffix = ""."".join([runner.loader_name, key])\n                np.save(f""{self.out_prefix}/{suffix}.npy"", value)\n\n\n__all__ = [""InferCallback""]\n'"
catalyst/dl/callbacks/meter.py,1,"b'from typing import List\nfrom collections import defaultdict\n\nimport numpy as np\n\nfrom catalyst.core import Callback, CallbackOrder, IRunner\nfrom catalyst.dl.utils import get_activation_fn\n\n\nclass MeterMetricsCallback(Callback):\n    """"""\n    A callback that tracks metrics through meters and prints metrics for\n    each class on `runner.on_loader_end`.\n\n    .. note::\n        This callback works for both single metric and multi-metric meters.\n    """"""\n\n    def __init__(\n        self,\n        metric_names: List[str],\n        meter_list: List,\n        input_key: str = ""targets"",\n        output_key: str = ""logits"",\n        class_names: List[str] = None,\n        num_classes: int = 2,\n        activation: str = ""Sigmoid"",\n    ):\n        """"""\n        Args:\n            metric_names (List[str]): of metrics to print\n                Make sure that they are in the same order that metrics\n                are outputted by the meters in `meter_list`\n            meter_list (list-like): List of meters.meter.Meter instances\n                len(meter_list) == num_classes\n            input_key (str): input key to use for metric calculation\n                specifies our ``y_true``.\n            output_key (str): output key to use for metric calculation;\n                specifies our ``y_pred``\n            class_names (List[str]): class names to display in the logs.\n                If None, defaults to indices for each class, starting from 0.\n            num_classes (int): Number of classes; must be > 1\n            activation (str): An torch.nn activation applied to the logits.\n                Must be one of [\'none\', \'Sigmoid\', \'Softmax2d\']\n        """"""\n        super().__init__(CallbackOrder.Metric)\n        self.metric_names = metric_names\n        self.meters = meter_list\n        self.input_key = input_key\n        self.output_key = output_key\n        self.class_names = class_names\n        self.num_classes = num_classes\n        self.activation = activation\n        self.activation_fn = get_activation_fn(self.activation)\n\n    def _reset_stats(self):\n        for meter in self.meters:\n            meter.reset()\n\n    def on_loader_start(self, runner: IRunner):\n        """"""Loader start hook.\n\n        Args:\n            runner (IRunner): current runner\n        """"""\n        self._reset_stats()\n\n    def on_batch_end(self, runner: IRunner):\n        """"""Batch end hook. Computes batch metrics.\n\n        Args:\n            runner (IRunner): current runner\n        """"""\n        logits = runner.output[self.output_key].detach().float()\n        targets = runner.input[self.input_key].detach().float()\n        probabilities = self.activation_fn(logits)\n\n        for i in range(self.num_classes):\n            self.meters[i].add(probabilities[:, i], targets[:, i])\n\n    def on_loader_end(self, runner: IRunner):\n        """"""Loader end hook. Computes loader metrics.\n\n        Args:\n            runner (IRunner): current runner\n        """"""\n        metrics_tracker = defaultdict(list)\n        loader_values = runner.loader_metrics\n        # Computing metrics for each class\n        for i, meter in enumerate(self.meters):\n            metrics = meter.value()\n            postfix = (\n                self.class_names[i] if self.class_names is not None else str(i)\n            )\n            for prefix, metric_ in zip(self.metric_names, metrics):\n                # appending the per-class values\n                metrics_tracker[prefix].append(metric_)\n                metric_name = f""{prefix}/class_{postfix}""\n                loader_values[metric_name] = metric_\n        # averaging the per-class values for each metric\n        for prefix in self.metric_names:\n            mean_value = float(np.mean(metrics_tracker[prefix]))\n            metric_name = f""{prefix}/_mean""\n            loader_values[metric_name] = mean_value\n\n        self._reset_stats()\n\n\n__all__ = [""MeterMetricsCallback""]\n'"
catalyst/dl/callbacks/mixup.py,1,"b'from typing import List\n\nimport numpy as np\n\nimport torch\n\nfrom catalyst.core import IRunner\nfrom catalyst.dl import CriterionCallback\n\n\nclass MixupCallback(CriterionCallback):\n    """"""Callback to do mixup augmentation.\n\n    More details about mixin can be found in the paper\n    `mixup: Beyond Empirical Risk Minimization`_.\n\n    .. warning::\n        :class:`catalyst.dl.callbacks.MixupCallback` is inherited from\n        :class:`catalyst.dl.CriterionCallback` and does its work.\n        You may not use them together.\n\n    .. _mixup\\: Beyond Empirical Risk Minimization:\n        https://arxiv.org/abs/1710.09412\n    """"""\n\n    def __init__(\n        self,\n        input_key: str = ""targets"",\n        output_key: str = ""logits"",\n        fields: List[str] = (""features"",),\n        alpha=1.0,\n        on_train_only=True,\n        **kwargs\n    ):\n        """"""\n        Args:\n            fields (List[str]): list of features which must be affected.\n            alpha (float): beta distribution a=b parameters.\n                Must be >=0. The more alpha closer to zero\n                the less effect of the mixup.\n            on_train_only (bool): Apply to train only.\n                As the mixup use the proxy inputs, the targets are also proxy.\n                We are not interested in them, are we?\n                So, if on_train_only is True, use a standard output/metric\n                for validation.\n        """"""\n        assert isinstance(input_key, str) and isinstance(output_key, str)\n        assert (\n            len(fields) > 0\n        ), ""At least one field for MixupCallback is required""\n        assert alpha >= 0, ""alpha must be>=0""\n\n        super().__init__(input_key=input_key, output_key=output_key, **kwargs)\n\n        self.on_train_only = on_train_only\n        self.fields = fields\n        self.alpha = alpha\n        self.lam = 1\n        self.index = None\n        self.is_needed = True\n\n    def _compute_loss_value(self, runner: IRunner, criterion):\n        if not self.is_needed:\n            return super()._compute_loss_value(runner, criterion)\n\n        pred = runner.output[self.output_key]\n        y_a = runner.input[self.input_key]\n        y_b = runner.input[self.input_key][self.index]\n\n        loss = self.lam * criterion(pred, y_a) + (1 - self.lam) * criterion(\n            pred, y_b\n        )\n        return loss\n\n    def on_loader_start(self, runner: IRunner):\n        """"""Loader start hook.\n\n        Args:\n            runner (IRunner): current runner\n        """"""\n        self.is_needed = not self.on_train_only or runner.is_train_loader\n\n    def on_batch_start(self, runner: IRunner):\n        """"""Batch start hook.\n\n        Args:\n            runner (IRunner): current runner\n        """"""\n        if not self.is_needed:\n            return\n\n        if self.alpha > 0:\n            self.lam = np.random.beta(self.alpha, self.alpha)\n        else:\n            self.lam = 1\n\n        self.index = torch.randperm(runner.input[self.fields[0]].shape[0])\n        self.index.to(runner.device)\n\n        for f in self.fields:\n            runner.input[f] = (\n                self.lam * runner.input[f]\n                + (1 - self.lam) * runner.input[f][self.index]\n            )\n\n\n__all__ = [""MixupCallback""]\n'"
catalyst/dl/callbacks/scheduler.py,0,"b'from typing import Optional\n\nfrom catalyst.core import IRunner\nfrom catalyst.core.callbacks import LRUpdater\n\n\nclass LRFinder(LRUpdater):\n    """"""\n    Helps you find an optimal learning rate for a model, as per suggestion of\n    `Cyclical Learning Rates for Training Neural Networks`_ paper.\n    Learning rate is increased in linear or log scale, depending on user input.\n\n    See `How Do You Find A Good Learning Rate`_ article for details.\n\n    .. _Cyclical Learning Rates for Training Neural Networks:\n        https://arxiv.org/abs/1506.01186\n    .. _How Do You Find A Good Learning Rate:\n        https://sgugger.github.io/how-do-you-find-a-good-learning-rate.html\n    """"""\n\n    def __init__(\n        self,\n        final_lr,\n        scale: str = ""log"",\n        num_steps: Optional[int] = None,\n        optimizer_key: str = None,\n    ):\n        """"""\n        Args:\n            final_lr: final learning rate to try with\n            scale (str): learning rate increasing scale (""log"" or ""linear"")\n            num_steps (Optional[int]):  number of batches to try;\n                if None - whole loader would be used.\n            optimizer_key (str): which optimizer key to use\n                for learning rate scheduling\n        """"""\n        super().__init__(optimizer_key=optimizer_key)\n\n        self.final_lr = final_lr\n        self.scale = scale\n        self.num_steps = num_steps\n        self.multiplier = 0\n        self.lr_step = 0\n        self.find_iter = 0\n\n        self._calc_lr = None\n        if scale == ""log"":\n            self._calc_lr = self._calc_lr_log\n        elif scale == ""linear"":\n            self._calc_lr = self._calc_lr_linear\n        else:\n            raise Exception(""Not supported"")\n\n    def _calc_lr_log(self):\n        return self.init_lr * self.multiplier ** self.find_iter\n\n    def _calc_lr_linear(self):\n        return self.init_lr + self.lr_step * self.find_iter\n\n    def calc_lr(self):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        res = self._calc_lr()\n        self.find_iter += 1\n        return res\n\n    def on_loader_start(self, runner: IRunner):\n        """"""@TODO: Docs. Contribution is welcome.\n\n        Args:\n            runner (IRunner): current runner\n        """"""\n        if runner.is_train_loader:\n            lr_ = self.final_lr / self.init_lr\n            self.num_steps = self.num_steps or runner.loader_len\n            self.multiplier = lr_ ** (1 / self.num_steps)\n            self.lr_step = (self.final_lr - self.init_lr) / self.num_steps\n\n        super().on_loader_start(runner=runner)\n\n    def on_batch_end(self, runner: IRunner):\n        """"""@TODO: Docs. Contribution is welcome.\n\n        Args:\n            runner (IRunner): current runner\n        """"""\n        super().on_batch_end(runner=runner)\n        if self.find_iter > self.num_steps:\n            raise NotImplementedError(""End of LRFinder"")\n\n\n__all__ = [""LRFinder""]\n'"
catalyst/dl/experiment/__init__.py,0,b'# flake8: noqa\n\nfrom .config import ConfigExperiment\nfrom .experiment import Experiment\nfrom .supervised import SupervisedExperiment\n'
catalyst/dl/experiment/config.py,3,"b'from typing import Any, Callable, Dict, List, Mapping, Union\nfrom collections import OrderedDict\nfrom copy import deepcopy\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader  # noqa F401\n\nfrom catalyst.core import IExperiment\nfrom catalyst.data import Augmentor, AugmentorCompose\nfrom catalyst.dl import (\n    Callback,\n    CheckpointCallback,\n    CheckRunCallback,\n    ConsoleLogger,\n    CriterionCallback,\n    ExceptionCallback,\n    MetricManagerCallback,\n    OptimizerCallback,\n    SchedulerCallback,\n    TensorboardLogger,\n    TimerCallback,\n    utils,\n    ValidationManagerCallback,\n    VerboseLogger,\n)\nfrom catalyst.dl.registry import (\n    CALLBACKS,\n    CRITERIONS,\n    MODELS,\n    OPTIMIZERS,\n    SCHEDULERS,\n    TRANSFORMS,\n)\nfrom catalyst.tools.typing import Criterion, Model, Optimizer, Scheduler\n\n\nclass ConfigExperiment(IExperiment):\n    """"""\n    Experiment created from a configuration file.\n    """"""\n\n    STAGE_KEYWORDS = [\n        ""criterion_params"",\n        ""optimizer_params"",\n        ""scheduler_params"",\n        ""data_params"",\n        ""transform_params"",\n        ""stage_params"",\n        ""callbacks_params"",\n    ]\n\n    def __init__(self, config: Dict):\n        """"""\n        Args:\n            config (dict): dictionary of parameters\n        """"""\n        self._config: Dict = deepcopy(config)\n        self._initial_seed: int = self._config.get(""args"", {}).get(""seed"", 42)\n        self._verbose: bool = self._config.get(""args"", {}).get(\n            ""verbose"", False\n        )\n        self._check_run: bool = self._config.get(""args"", {}).get(\n            ""check"", False\n        )\n        self._check_time: bool = self._config.get(""args"", {}).get(\n            ""timeit"", False\n        )\n        self.__prepare_logdir()\n\n        self._config[""stages""][""stage_params""] = utils.merge_dicts(\n            deepcopy(\n                self._config[""stages""].get(""state_params"", {})\n            ),  # saved for backward compatibility\n            deepcopy(self._config[""stages""].get(""stage_params"", {})),\n            deepcopy(self._config.get(""args"", {})),\n            {""logdir"": self._logdir},\n        )\n        self.stages_config: Dict = self._get_stages_config(\n            self._config[""stages""]\n        )\n\n    def __prepare_logdir(self):\n        EXCLUDE_TAG = ""none""\n\n        logdir = self._config.get(""args"", {}).get(""logdir"", None)\n        baselogdir = self._config.get(""args"", {}).get(""baselogdir"", None)\n\n        if logdir is not None and logdir.lower() != EXCLUDE_TAG:\n            self._logdir = logdir\n        elif baselogdir is not None and baselogdir.lower() != EXCLUDE_TAG:\n            logdir_postfix = self._get_logdir(self._config)\n            self._logdir = f""{baselogdir}/{logdir_postfix}""\n        else:\n            self._logdir = None\n\n    def _get_stages_config(self, stages_config: Dict):\n        stages_defaults = {}\n        stages_config_out = OrderedDict()\n        for key in self.STAGE_KEYWORDS:\n            if key == ""stage_params"":\n                # backward compatibility\n                stages_defaults[key] = utils.merge_dicts(\n                    deepcopy(stages_config.get(""state_params"", {})),\n                    deepcopy(stages_config.get(key, {})),\n                )\n            else:\n                stages_defaults[key] = deepcopy(stages_config.get(key, {}))\n        for stage in stages_config:\n            if (\n                stage in self.STAGE_KEYWORDS\n                or stage == ""state_params""\n                or stages_config.get(stage) is None\n            ):\n                continue\n            stages_config_out[stage] = {}\n            for key in self.STAGE_KEYWORDS:\n                if key == ""stage_params"":\n                    # backward compatibility\n                    stages_config_out[stage][key] = utils.merge_dicts(\n                        deepcopy(stages_defaults.get(""state_params"", {})),\n                        deepcopy(stages_defaults.get(key, {})),\n                        deepcopy(stages_config[stage].get(""state_params"", {})),\n                        deepcopy(stages_config[stage].get(key, {})),\n                    )\n                else:\n                    stages_config_out[stage][key] = utils.merge_dicts(\n                        deepcopy(stages_defaults.get(key, {})),\n                        deepcopy(stages_config[stage].get(key, {})),\n                    )\n\n        return stages_config_out\n\n    def _get_logdir(self, config: Dict) -> str:\n        timestamp = utils.get_utcnow_time()\n        config_hash = utils.get_short_hash(config)\n        logdir = f""{timestamp}.{config_hash}""\n        return logdir\n\n    @property\n    def initial_seed(self) -> int:\n        """"""Experiment\'s initial seed value.""""""\n        return self._initial_seed\n\n    @property\n    def logdir(self):\n        """"""Path to the directory where the experiment logs.""""""\n        return self._logdir\n\n    @property\n    def stages(self) -> List[str]:\n        """"""Experiment\'s stage names.""""""\n        stages_keys = list(self.stages_config.keys())\n        return stages_keys\n\n    @property\n    def distributed_params(self) -> Dict:\n        """"""Dict with the parameters for distributed and FP16 methond.""""""\n        return self._config.get(""distributed_params"", {})\n\n    def get_stage_params(self, stage: str) -> Mapping[str, Any]:\n        """"""Returns the state parameters for a given stage.""""""\n        return self.stages_config[stage].get(""stage_params"", {})\n\n    @staticmethod\n    def _get_model(**params):\n        key_value_flag = params.pop(""_key_value"", False)\n\n        if key_value_flag:\n            model = {}\n            for key, params_ in params.items():\n                model[key] = ConfigExperiment._get_model(**params_)\n            model = nn.ModuleDict(model)\n        else:\n            model = MODELS.get_from_params(**params)\n        return model\n\n    def get_model(self, stage: str):\n        """"""Returns the model for a given stage.""""""\n        model_params = self._config[""model_params""]\n        model = self._get_model(**model_params)\n        return model\n\n    @staticmethod\n    def _get_criterion(**params):\n        key_value_flag = params.pop(""_key_value"", False)\n\n        if key_value_flag:\n            criterion = {}\n            for key, params_ in params.items():\n                criterion[key] = ConfigExperiment._get_criterion(**params_)\n        else:\n            criterion = CRITERIONS.get_from_params(**params)\n            if criterion is not None and torch.cuda.is_available():\n                criterion = criterion.cuda()\n        return criterion\n\n    def get_criterion(self, stage: str) -> Criterion:\n        """"""Returns the criterion for a given stage.""""""\n        criterion_params = self.stages_config[stage].get(\n            ""criterion_params"", {}\n        )\n        criterion = self._get_criterion(**criterion_params)\n        return criterion\n\n    def _get_optimizer(\n        self, stage: str, model: Union[Model, Dict[str, Model]], **params\n    ) -> Optimizer:\n        # @TODO 1: refactoring; this method is too long\n        # @TODO 2: load state dicts for schedulers & criterion\n        layerwise_params = params.pop(""layerwise_params"", OrderedDict())\n        no_bias_weight_decay = params.pop(""no_bias_weight_decay"", True)\n\n        # linear scaling rule from https://arxiv.org/pdf/1706.02677.pdf\n        lr_scaling_params = params.pop(""lr_linear_scaling"", None)\n        if lr_scaling_params:\n            data_params = dict(self.stages_config[stage][""data_params""])\n            batch_size = data_params.get(""batch_size"")\n            per_gpu_scaling = data_params.get(""per_gpu_scaling"", False)\n            distributed_rank = utils.get_rank()\n            distributed = distributed_rank > -1\n            if per_gpu_scaling and not distributed:\n                num_gpus = max(1, torch.cuda.device_count())\n                batch_size *= num_gpus\n\n            base_lr = lr_scaling_params.get(""lr"")\n            base_batch_size = lr_scaling_params.get(""base_batch_size"", 256)\n            lr_scaling = batch_size / base_batch_size\n            params[""lr""] = base_lr * lr_scaling  # scale default lr\n        else:\n            lr_scaling = 1.0\n\n        # getting model parameters\n        model_key = params.pop(""_model"", None)\n        if model_key is None:\n            assert isinstance(\n                model, nn.Module\n            ), ""model is key-value, but optimizer has no specified model""\n            model_params = utils.process_model_params(\n                model, layerwise_params, no_bias_weight_decay, lr_scaling\n            )\n        elif isinstance(model_key, str):\n            model_params = utils.process_model_params(\n                model[model_key],\n                layerwise_params,\n                no_bias_weight_decay,\n                lr_scaling,\n            )\n        elif isinstance(model_key, (list, tuple)):\n            model_params = []\n            for model_key_ in model_key:\n                model_params_ = utils.process_model_params(\n                    model[model_key_],\n                    layerwise_params,\n                    no_bias_weight_decay,\n                    lr_scaling,\n                )\n                model_params.extend(model_params_)\n        else:\n            raise ValueError(""unknown type of model_params"")\n\n        load_from_previous_stage = params.pop(\n            ""load_from_previous_stage"", False\n        )\n        optimizer_key = params.pop(""optimizer_key"", None)\n        optimizer = OPTIMIZERS.get_from_params(**params, params=model_params)\n\n        if load_from_previous_stage and self.stages.index(stage) != 0:\n            checkpoint_path = f""{self.logdir}/checkpoints/best_full.pth""\n            checkpoint = utils.load_checkpoint(checkpoint_path)\n\n            dict2load = optimizer\n            if optimizer_key is not None:\n                dict2load = {optimizer_key: optimizer}\n            utils.unpack_checkpoint(checkpoint, optimizer=dict2load)\n\n            # move optimizer to device\n            device = utils.get_device()\n            for param in model_params:\n                param = param[""params""][0]\n                optimizer_state = optimizer.state[param]\n                for key, value in optimizer_state.items():\n                    optimizer_state[key] = utils.any2device(value, device)\n\n            # update optimizer params\n            for key, value in params.items():\n                for pg in optimizer.param_groups:\n                    pg[key] = value\n\n        return optimizer\n\n    def get_optimizer(\n        self, stage: str, model: Union[Model, Dict[str, Model]]\n    ) -> Union[Optimizer, Dict[str, Optimizer]]:\n        """"""Returns the optimizer for a given stage.\n\n        Args:\n            stage (str): stage name\n            model (Union[Model, Dict[str, Model]]): model or a dict of models\n        """"""\n        optimizer_params = self.stages_config[stage].get(\n            ""optimizer_params"", {}\n        )\n        key_value_flag = optimizer_params.pop(""_key_value"", False)\n\n        if key_value_flag:\n            optimizer = {}\n            for key, params_ in optimizer_params.items():\n                # load specified optimizer from checkpoint\n                optimizer_key = ""optimizer_key""\n                assert optimizer_key not in params_, ""keyword reserved""\n                params_[optimizer_key] = key\n\n                optimizer[key] = self._get_optimizer(stage, model, **params_)\n        else:\n            optimizer = self._get_optimizer(stage, model, **optimizer_params)\n\n        return optimizer\n\n    @staticmethod\n    def _get_scheduler(*, optimizer, **params):\n        key_value_flag = params.pop(""_key_value"", False)\n\n        if key_value_flag:\n            scheduler = {}\n            for key, params_ in params.items():\n                scheduler[key] = ConfigExperiment._get_scheduler(\n                    optimizer=optimizer, **params_\n                )\n        else:\n            scheduler = SCHEDULERS.get_from_params(\n                **params, optimizer=optimizer\n            )\n        return scheduler\n\n    def get_scheduler(self, stage: str, optimizer: Optimizer) -> Scheduler:\n        """"""Returns the scheduler for a given stage.""""""\n        scheduler_params = self.stages_config[stage].get(\n            ""scheduler_params"", {}\n        )\n        scheduler = self._get_scheduler(\n            optimizer=optimizer, **scheduler_params\n        )\n        return scheduler\n\n    @staticmethod\n    def _get_transform(**params) -> Callable:\n        key_value_flag = params.pop(""_key_value"", False)\n\n        if key_value_flag:\n            transforms_composition = {\n                key: ConfigExperiment._get_transform(**params_)\n                for key, params_ in params.items()\n            }\n\n            transform = AugmentorCompose(\n                {\n                    key: Augmentor(\n                        dict_key=key,\n                        augment_fn=transform,\n                        input_key=key,\n                        output_key=key,\n                    )\n                    for key, transform in transforms_composition.items()\n                }\n            )\n        else:\n            if ""transforms"" in params:\n                transforms_composition = [\n                    ConfigExperiment._get_transform(**transform_params)\n                    for transform_params in params[""transforms""]\n                ]\n                params.update(transforms=transforms_composition)\n\n            transform = TRANSFORMS.get_from_params(**params)\n\n        return transform\n\n    def get_transforms(\n        self, stage: str = None, dataset: str = None\n    ) -> Callable:\n        """"""Returns transform for a given stage and mode.\n\n        Args:\n            stage (str): stage name\n            dataset (str): dataset name (e.g. ""train"", ""valid""),\n                will be used only if the value of `_key_value`` is ``True``\n        """"""\n        transform_params = deepcopy(\n            self.stages_config[stage].get(""transform_params"", {})\n        )\n\n        key_value_flag = transform_params.pop(""_key_value"", False)\n        if key_value_flag:\n            transform_params = transform_params.get(dataset, {})\n\n        transform = self._get_transform(**transform_params)\n        if transform is None:\n\n            def transform(dict_):\n                return dict_\n\n        elif not isinstance(transform, AugmentorCompose):\n            transform_ = transform\n\n            def transform(dict_):\n                return transform_(**dict_)\n\n        return transform\n\n    def get_loaders(\n        self, stage: str, epoch: int = None,\n    ) -> ""OrderedDict[str, DataLoader]"":\n        """"""Returns the loaders for a given stage.""""""\n        data_params = dict(self.stages_config[stage][""data_params""])\n        loaders = utils.get_loaders_from_params(\n            get_datasets_fn=self.get_datasets,\n            initial_seed=self.initial_seed,\n            stage=stage,\n            **data_params,\n        )\n        return loaders\n\n    @staticmethod\n    def _get_callback(**params):\n        wrapper_params = params.pop(""_wrapper"", None)\n        callback = CALLBACKS.get_from_params(**params)\n        if wrapper_params is not None:\n            wrapper_params[""base_callback""] = callback\n            return ConfigExperiment._get_callback(**wrapper_params)\n        return callback\n\n    @staticmethod\n    def _process_callbacks(callbacks: OrderedDict) -> None:\n        """"""\n        Iterate over each of the callbacks and update\n        approptiate parameters required for success\n        run of config experiment.\n\n        Arguments:\n            callbacks (OrderedDict): finalized order of callbacks.\n        """"""\n        for callback in callbacks.values():\n            if isinstance(callback, CheckpointCallback):\n                if callback.load_on_stage_start is None:\n                    callback.load_on_stage_start = ""best""\n                if (\n                    isinstance(callback.load_on_stage_start, dict)\n                    and ""model"" not in callback.load_on_stage_start\n                ):\n                    callback.load_on_stage_start[""model""] = ""best""\n\n    def get_callbacks(self, stage: str) -> ""OrderedDict[Callback]"":\n        """"""Returns the callbacks for a given stage.""""""\n        callbacks_params = self.stages_config[stage].get(\n            ""callbacks_params"", {}\n        )\n\n        callbacks = OrderedDict()\n        for key, callback_params in callbacks_params.items():\n            callback = self._get_callback(**callback_params)\n            callbacks[key] = callback\n\n        default_callbacks = []\n        if self._verbose:\n            default_callbacks.append((""_verbose"", VerboseLogger))\n        if self._check_time:\n            default_callbacks.append((""_timer"", TimerCallback))\n        if self._check_run:\n            default_callbacks.append((""_check"", CheckRunCallback))\n\n        if not stage.startswith(""infer""):\n            default_callbacks.append((""_metrics"", MetricManagerCallback))\n            default_callbacks.append(\n                (""_validation"", ValidationManagerCallback)\n            )\n            default_callbacks.append((""_console"", ConsoleLogger))\n\n            if self.logdir is not None:\n                default_callbacks.append((""_saver"", CheckpointCallback))\n                default_callbacks.append((""_tensorboard"", TensorboardLogger))\n\n            if self.stages_config[stage].get(""criterion_params"", {}):\n                default_callbacks.append((""_criterion"", CriterionCallback))\n            if self.stages_config[stage].get(""optimizer_params"", {}):\n                default_callbacks.append((""_optimizer"", OptimizerCallback))\n            if self.stages_config[stage].get(""scheduler_params"", {}):\n                default_callbacks.append((""_scheduler"", SchedulerCallback))\n\n        default_callbacks.append((""_exception"", ExceptionCallback))\n\n        for callback_name, callback_fn in default_callbacks:\n            is_already_present = False\n            for x in callbacks.values():\n                if isinstance(x, callback_fn):\n                    is_already_present = True\n                    break\n            if not is_already_present:\n                callbacks[callback_name] = callback_fn()\n\n        self._process_callbacks(callbacks)\n\n        return callbacks\n\n\n__all__ = [""ConfigExperiment""]\n'"
catalyst/dl/experiment/experiment.py,3,"b'from typing import Any, Dict, Iterable, List, Mapping, Tuple, Union\nfrom collections import OrderedDict\nimport warnings\n\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\n\nfrom catalyst.core import IExperiment\nfrom catalyst.dl import (\n    Callback,\n    CheckpointCallback,\n    CheckRunCallback,\n    ConsoleLogger,\n    ExceptionCallback,\n    MetricManagerCallback,\n    TensorboardLogger,\n    TimerCallback,\n    utils,\n    ValidationManagerCallback,\n    VerboseLogger,\n)\nfrom catalyst.tools import settings\nfrom catalyst.tools.typing import Criterion, Model, Optimizer, Scheduler\n\n\nclass Experiment(IExperiment):\n    """"""\n    Super-simple one-staged experiment,\n    you can use to declare experiment in code.\n    """"""\n\n    def __init__(\n        self,\n        model: Model,\n        datasets: ""OrderedDict[str, Union[Dataset, Dict, Any]]"" = None,\n        loaders: ""OrderedDict[str, DataLoader]"" = None,\n        callbacks: ""Union[OrderedDict[str, Callback], List[Callback]]"" = None,\n        logdir: str = None,\n        stage: str = ""train"",\n        criterion: Criterion = None,\n        optimizer: Optimizer = None,\n        scheduler: Scheduler = None,\n        num_epochs: int = 1,\n        valid_loader: str = ""valid"",\n        main_metric: str = ""loss"",\n        minimize_metric: bool = True,\n        verbose: bool = False,\n        check_time: bool = False,\n        check_run: bool = False,\n        stage_kwargs: Dict = None,\n        checkpoint_data: Dict = None,\n        distributed_params: Dict = None,\n        initial_seed: int = 42,\n    ):\n        """"""\n        Args:\n            model (Model): model\n            datasets (OrderedDict[str, Union[Dataset, Dict, Any]]): dictionary\n                with one or several  ``torch.utils.data.Dataset``\n                for training, validation or inference\n                used for Loaders automatic creation\n                preferred way for distributed training setup\n            loaders (OrderedDict[str, DataLoader]): dictionary\n                with one or several ``torch.utils.data.DataLoader``\n                for training, validation or inference\n            callbacks (Union[List[Callback], OrderedDict[str, Callback]]):\n                list or dictionary with Catalyst callbacks\n            logdir (str): path to output directory\n            stage (str): current stage\n            criterion (Criterion): criterion function\n            optimizer (Optimizer): optimizer\n            scheduler (Scheduler): scheduler\n            num_epochs (int): number of experiment\'s epochs\n            valid_loader (str): loader name used to calculate\n                the metrics and save the checkpoints. For example,\n                you can pass `train` and then\n                the metrics will be taken from `train` loader.\n            main_metric (str): the key to the name of the metric\n                by which the checkpoints will be selected.\n            minimize_metric (bool): flag to indicate whether\n                the ``main_metric`` should be minimized.\n            verbose (bool): if True, it displays the status of the training\n                to the console.\n            check_time (bool): if True, computes the execution time\n                of training process and displays it to the console.\n            check_run (bool): if True, we run only 3 batches per loader\n                and 3 epochs per stage to check pipeline correctness\n            stage_kwargs (dict): additional stage params\n            checkpoint_data (dict): additional data to save in checkpoint,\n                for example: ``class_names``, ``date_of_training``, etc\n            distributed_params (dict): dictionary with the parameters\n                for distributed and FP16 method\n            initial_seed (int): experiment\'s initial seed value\n        """"""\n        assert (\n            datasets is not None or loaders is not None\n        ), ""Please specify the data sources""\n\n        self._model = model\n        self._loaders, self._valid_loader = self.process_loaders(\n            loaders=loaders,\n            datasets=datasets,\n            stage=stage,\n            valid_loader=valid_loader,\n            initial_seed=initial_seed,\n        )\n        self._callbacks = utils.sort_callbacks_by_order(callbacks)\n\n        self._criterion = criterion\n        self._optimizer = optimizer\n        self._scheduler = scheduler\n\n        self._initial_seed = initial_seed\n        self._logdir = logdir\n        self._stage = stage\n        self._num_epochs = num_epochs\n        self._main_metric = main_metric\n        self._minimize_metric = minimize_metric\n        self._verbose = verbose\n        self._check_time = check_time\n        self._check_run = check_run\n        self._stage_kwargs = stage_kwargs or {}\n        self._checkpoint_data = checkpoint_data or {}\n        self._distributed_params = distributed_params or {}\n\n    @property\n    def initial_seed(self) -> int:\n        """"""Experiment\'s initial seed value.""""""\n        return self._initial_seed\n\n    @property\n    def logdir(self):\n        """"""Path to the directory where the experiment logs.""""""\n        return self._logdir\n\n    @property\n    def stages(self) -> Iterable[str]:\n        """"""Experiment\'s stage names (array with one value).""""""\n        return [self._stage]\n\n    @property\n    def distributed_params(self) -> Dict:\n        """"""Dict with the parameters for distributed and FP16 method.""""""\n        return self._distributed_params\n\n    @staticmethod\n    def process_loaders(\n        loaders: ""OrderedDict[str, DataLoader]"",\n        datasets: Dict,\n        stage: str,\n        valid_loader: str,\n        initial_seed: int,\n    ) -> ""Tuple[OrderedDict[str, DataLoader], str]"":\n        """"""Prepares loaders for a given stage.""""""\n        if datasets is not None:\n            loaders = utils.get_loaders_from_params(\n                initial_seed=initial_seed, **datasets,\n            )\n        if not stage.startswith(settings.stage_infer_prefix):  # train stage\n            if len(loaders) == 1:\n                valid_loader = list(loaders.keys())[0]\n                warnings.warn(\n                    ""Attention, there is only one dataloader - ""\n                    + str(valid_loader)\n                )\n            assert valid_loader in loaders, (\n                ""The validation loader must be present ""\n                ""in the loaders used during experiment.""\n            )\n        return loaders, valid_loader\n\n    def get_stage_params(self, stage: str) -> Mapping[str, Any]:\n        """"""Returns the state parameters for a given stage.""""""\n        default_params = {\n            ""logdir"": self.logdir,\n            ""num_epochs"": self._num_epochs,\n            ""valid_loader"": self._valid_loader,\n            ""main_metric"": self._main_metric,\n            ""verbose"": self._verbose,\n            ""minimize_metric"": self._minimize_metric,\n            ""checkpoint_data"": self._checkpoint_data,\n        }\n        stage_params = {**default_params, **self._stage_kwargs}\n        return stage_params\n\n    def get_model(self, stage: str) -> Model:\n        """"""Returns the model for a given stage.""""""\n        return self._model\n\n    def get_criterion(self, stage: str) -> Criterion:\n        """"""Returns the criterion for a given stage.""""""\n        return self._criterion\n\n    def get_optimizer(self, stage: str, model: nn.Module) -> Optimizer:\n        """"""Returns the optimizer for a given stage.""""""\n        return self._optimizer\n\n    def get_scheduler(self, stage: str, optimizer=None) -> Scheduler:\n        """"""Returns the scheduler for a given stage.""""""\n        return self._scheduler\n\n    def get_loaders(\n        self, stage: str, epoch: int = None,\n    ) -> ""OrderedDict[str, DataLoader]"":\n        """"""Returns the loaders for a given stage.""""""\n        return self._loaders\n\n    def get_callbacks(self, stage: str) -> ""OrderedDict[str, Callback]"":\n        """"""\n        Returns the callbacks for a given stage.\n        """"""\n        callbacks = self._callbacks or OrderedDict()\n        default_callbacks = []\n\n        if self._verbose:\n            default_callbacks.append((""_verbose"", VerboseLogger))\n        if self._check_time:\n            default_callbacks.append((""_timer"", TimerCallback))\n        if self._check_run:\n            default_callbacks.append((""_check"", CheckRunCallback))\n\n        if not stage.startswith(""infer""):\n            default_callbacks.append((""_metrics"", MetricManagerCallback))\n            default_callbacks.append(\n                (""_validation"", ValidationManagerCallback)\n            )\n            default_callbacks.append((""_console"", ConsoleLogger))\n            if self.logdir is not None:\n                default_callbacks.append((""_saver"", CheckpointCallback))\n                default_callbacks.append((""_tensorboard"", TensorboardLogger))\n        default_callbacks.append((""_exception"", ExceptionCallback))\n\n        for callback_name, callback_fn in default_callbacks:\n            is_already_present = any(\n                isinstance(x, callback_fn) for x in callbacks.values()\n            )\n            if not is_already_present:\n                callbacks[callback_name] = callback_fn()\n\n        return callbacks\n\n\n__all__ = [""Experiment""]\n'"
catalyst/dl/experiment/supervised.py,1,"b'from collections import OrderedDict\n\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\nfrom catalyst.dl import (\n    Callback,\n    CriterionCallback,\n    OptimizerCallback,\n    SchedulerCallback,\n)\nfrom catalyst.tools.typing import Criterion, Optimizer, Scheduler\n\nfrom .experiment import Experiment\n\n\nclass SupervisedExperiment(Experiment):\n    """"""\n    Supervised experiment.\n\n    The main difference with Experiment that it will\n    add several callbacks by default if you haven\'t.\n\n    Here are list of callbacks by default:\n        CriterionCallback:\n            measures loss with specified ``criterion``.\n        OptimizerCallback:\n            abstraction over ``optimizer`` step.\n        SchedulerCallback:\n            only in case if you provided scheduler to your experiment does\n            `lr_scheduler.step`\n        CheckpointCallback:\n            saves model and optimizer state each epoch callback to save/restore\n            your model/criterion/optimizer/metrics.\n        ConsoleLogger:\n            standard Catalyst logger,\n            translates ``runner.*_metrics`` to console and text file\n        TensorboardLogger:\n            will write ``runner.*_metrics`` to tensorboard\n        RaiseExceptionCallback:\n            will raise exception if needed\n    """"""\n\n    def get_callbacks(self, stage: str) -> ""OrderedDict[str, Callback]"":\n        """"""\n        Override of ``BaseExperiment.get_callbacks`` method.\n        Will add several of callbacks by default in case they missed.\n\n        Args:\n            stage (str): name of stage. It should start with `infer` if you\n                don\'t need default callbacks, as they required only for\n                training stages.\n\n        Returns:\n            (OrderedDict[str, Callback]): Ordered dictionary of callbacks\n                for experiment\n        """"""\n        callbacks = super().get_callbacks(stage=stage) or OrderedDict()\n\n        default_callbacks = []\n\n        if not stage.startswith(""infer""):\n            if self._criterion is not None and isinstance(\n                self._criterion, Criterion\n            ):\n                default_callbacks.append((""_criterion"", CriterionCallback))\n            if self._optimizer is not None and isinstance(\n                self._optimizer, Optimizer\n            ):\n                default_callbacks.append((""_optimizer"", OptimizerCallback))\n            if self._scheduler is not None and isinstance(\n                self._scheduler, (Scheduler, ReduceLROnPlateau)\n            ):\n                default_callbacks.append((""_scheduler"", SchedulerCallback))\n\n        for callback_name, callback_fn in default_callbacks:\n            is_already_present = any(\n                isinstance(x, callback_fn) for x in callbacks.values()\n            )\n            if not is_already_present:\n                callbacks[callback_name] = callback_fn()\n\n        return callbacks\n\n\n__all__ = [""SupervisedExperiment""]\n'"
catalyst/dl/runner/__init__.py,0,b'# flake8: noqa\n\nfrom .runner import Runner\nfrom .supervised import SupervisedRunner\n'
catalyst/dl/runner/runner.py,8,"b'from typing import Any, Callable, Dict, Generator, List, Mapping, Union\nfrom collections import OrderedDict\n\nimport torch\nfrom torch.jit import ScriptModule\nfrom torch.utils.data import DataLoader, Dataset\n\nfrom catalyst.core import Callback, CheckpointCallback, IStageBasedRunner\nfrom catalyst.dl import utils\nfrom catalyst.dl.experiment.experiment import Experiment\nfrom catalyst.tools.typing import (\n    Criterion,\n    Device,\n    Model,\n    Optimizer,\n    Scheduler,\n)\n\n\nclass Runner(IStageBasedRunner):\n    """"""\n    Deep Learning Runner for supervised, unsupervised, gan, etc runs.\n    """"""\n\n    _experiment_fn: Callable = Experiment\n\n    def _init(self, **kwargs):\n        self.experiment: Experiment = None\n\n    def train(\n        self,\n        *,\n        model: Model,\n        criterion: Criterion = None,\n        optimizer: Optimizer = None,\n        scheduler: Scheduler = None,\n        datasets: ""OrderedDict[str, Union[Dataset, Dict, Any]]"" = None,\n        loaders: ""OrderedDict[str, DataLoader]"" = None,\n        callbacks: ""Union[List[Callback], OrderedDict[str, Callback]]"" = None,\n        logdir: str = None,\n        resume: str = None,\n        num_epochs: int = 1,\n        valid_loader: str = ""valid"",\n        main_metric: str = ""loss"",\n        minimize_metric: bool = True,\n        verbose: bool = False,\n        stage_kwargs: Dict = None,\n        checkpoint_data: Dict = None,\n        fp16: Union[Dict, bool] = None,\n        distributed: bool = False,\n        check: bool = False,\n        timeit: bool = False,\n        load_best_on_end: bool = False,\n        initial_seed: int = 42,\n        state_kwargs: Dict = None,\n    ) -> None:\n        """"""\n        Starts the train stage of the model.\n\n        Args:\n            model (Model): model to train\n            criterion (Criterion): criterion function for training\n            optimizer (Optimizer): optimizer for training\n            scheduler (Scheduler): scheduler for training\n            datasets (OrderedDict[str, Union[Dataset, Dict, Any]]): dictionary\n                with one or several  ``torch.utils.data.Dataset``\n                for training, validation or inference\n                used for Loaders automatic creation\n                preferred way for distributed training setup\n            loaders (OrderedDict[str, DataLoader]): dictionary\n                with one or several ``torch.utils.data.DataLoader``\n                for training, validation or inference\n            callbacks (Union[List[Callback], OrderedDict[str, Callback]]):\n                list or dictionary with Catalyst callbacks\n            logdir (str): path to output directory\n            resume (str): path to checkpoint for model\n            num_epochs (int): number of training epochs\n            valid_loader (str): loader name used to calculate\n                the metrics and save the checkpoints. For example,\n                you can pass `train` and then\n                the metrics will be taken from `train` loader.\n            main_metric (str): the key to the name of the metric\n                by which the checkpoints will be selected.\n            minimize_metric (bool): flag to indicate whether\n                the ``main_metric`` should be minimized.\n            verbose (bool): if `True`, it displays the status of the training\n                to the console.\n            stage_kwargs (dict): additional params for stage\n            checkpoint_data (dict): additional data to save in checkpoint,\n                for example: ``class_names``, ``date_of_training``, etc\n            fp16 (Union[Dict, bool]): If not None, then sets training to FP16.\n                See https://nvidia.github.io/apex/amp.html#properties\n                if fp16=True, params by default will be ``{""opt_level"": ""O1""}``\n            distributed (bool): if `True` will start training\n                in distributed mode.\n                Note: Works only with python scripts. No jupyter support.\n            check (bool): if True, then only checks that pipeline is working\n                (3 epochs only)\n            timeit (bool): if True, computes the execution time\n                of training process and displays it to the console.\n            load_best_on_end (bool): if True, Runner will load\n                best checkpoint state (model, optimizer, etc)\n                according to validation metrics. Requires specified ``logdir``.\n            initial_seed (int): experiment\'s initial seed value\n        """"""\n        assert state_kwargs is None or stage_kwargs is None\n\n        if isinstance(fp16, bool) and fp16:\n            fp16 = {""opt_level"": ""O1""}\n\n        if resume is not None or load_best_on_end:\n            load_on_stage_end = None\n            if load_best_on_end:\n                load_on_stage_end = ""best_full""\n                assert logdir is not None, (\n                    ""For ``load_best_on_end`` feature ""\n                    ""you need to specify ``logdir``""\n                )\n            callbacks = utils.sort_callbacks_by_order(callbacks)\n            checkpoint_callback_flag = any(\n                isinstance(x, CheckpointCallback) for x in callbacks.values()\n            )\n            if not checkpoint_callback_flag:\n                callbacks[""loader""] = CheckpointCallback(\n                    resume=resume, load_on_stage_end=load_on_stage_end,\n                )\n            else:\n                raise NotImplementedError(""CheckpointCallback already exist"")\n\n        experiment = self._experiment_fn(\n            stage=""train"",\n            model=model,\n            datasets=datasets,\n            loaders=loaders,\n            callbacks=callbacks,\n            logdir=logdir,\n            criterion=criterion,\n            optimizer=optimizer,\n            scheduler=scheduler,\n            num_epochs=num_epochs,\n            valid_loader=valid_loader,\n            main_metric=main_metric,\n            minimize_metric=minimize_metric,\n            verbose=verbose,\n            check_time=timeit,\n            check_run=check,\n            stage_kwargs=stage_kwargs or state_kwargs,\n            checkpoint_data=checkpoint_data,\n            distributed_params=fp16,\n            initial_seed=initial_seed,\n        )\n        self.experiment = experiment\n        utils.distributed_cmd_run(self.run_experiment, distributed)\n\n    def infer(\n        self,\n        *,\n        model: Model,\n        datasets: ""OrderedDict[str, Union[Dataset, Dict, Any]]"" = None,\n        loaders: ""OrderedDict[str, DataLoader]"" = None,\n        callbacks: ""Union[List[Callback], OrderedDict[str, Callback]]"" = None,\n        logdir: str = None,\n        resume: str = None,\n        verbose: bool = False,\n        stage_kwargs: Dict = None,\n        fp16: Union[Dict, bool] = None,\n        check: bool = False,\n        timeit: bool = False,\n        initial_seed: int = 42,\n        state_kwargs: Dict = None,\n    ) -> None:\n        """"""\n        Starts the inference stage of the model.\n\n        Args:\n            model (Model): model for inference\n            datasets (OrderedDict[str, Union[Dataset, Dict, Any]]): dictionary\n                with one or several  ``torch.utils.data.Dataset``\n                for training, validation or inference\n                used for Loaders automatic creation\n                preferred way for distributed training setup\n            loaders (OrderedDict[str, DataLoader]): dictionary\n                with one or several ``torch.utils.data.DataLoader``\n                for training, validation or inference\n            callbacks (Union[List[Callback], OrderedDict[str, Callback]]):\n                list or dictionary with Catalyst callbacks\n            logdir (str): path to output directory\n            verbose (bool): if `True`, it displays the status of the training\n                to the console.\n            stage_kwargs (dict): additional stage params\n            checkpoint_data (dict): additional data to save in checkpoint,\n                for example: ``class_names``, ``date_of_training``, etc\n            fp16 (Union[Dict, bool]): If not None, then sets training to FP16.\n                See https://nvidia.github.io/apex/amp.html#properties\n                if fp16=True, params by default will be ``{""opt_level"": ""O1""}``\n            check (bool): if True, then only checks that pipeline is working\n                (3 epochs only)\n            timeit (bool): if True, computes the execution time\n                of training process and displays it to the console.\n            initial_seed (int): experiment\'s initial seed value\n        """"""\n        assert state_kwargs is None or stage_kwargs is None\n\n        if isinstance(fp16, bool) and fp16:\n            fp16 = {""opt_level"": ""O1""}\n\n        if resume is not None:\n            callbacks = utils.sort_callbacks_by_order(callbacks)\n            checkpoint_callback_flag = any(\n                isinstance(x, CheckpointCallback) for x in callbacks.values()\n            )\n            if not checkpoint_callback_flag:\n                callbacks[""loader""] = CheckpointCallback(resume=resume)\n            else:\n                raise NotImplementedError(""CheckpointCallback already exist"")\n\n        experiment = self._experiment_fn(\n            stage=""infer"",\n            model=model,\n            datasets=datasets,\n            loaders=loaders,\n            callbacks=callbacks,\n            logdir=logdir,\n            verbose=verbose,\n            check_time=timeit,\n            check_run=check,\n            stage_kwargs=stage_kwargs or state_kwargs,\n            distributed_params=fp16,\n            initial_seed=initial_seed,\n        )\n        self.run_experiment(experiment)\n\n    @torch.no_grad()\n    def predict_batch(\n        self, batch: Mapping[str, Any], **kwargs\n    ) -> Mapping[str, Any]:\n        """"""\n        Run model inference on specified data batch.\n\n        Args:\n            batch (Mapping[str, Any]): dictionary with data batches\n                from DataLoader.\n            **kwargs: additional kwargs to pass to the model\n\n        Returns:\n            Mapping[str, Any]: model output dictionary\n        """"""\n        raise NotImplementedError(\n            ""Please implement `runner.predict_batch` method""\n        )\n\n    @torch.no_grad()\n    def predict_loader(\n        self,\n        *,\n        loader: DataLoader,\n        model: Model = None,\n        resume: str = None,\n        fp16: Union[Dict, bool] = None,\n        initial_seed: int = 42,\n    ) -> Generator:\n        """"""\n        Runs model inference on PyTorch Dataloader and returns\n        python Generator with model predictions from `runner.predict_batch`\n\n        Args:\n            loader (DataLoader):\n            model (Model):\n            resume (str):\n            fp16 (Union[Dict, bool]):\n            initial_seed (int):\n\n        Returns:\n            (Generator) model predictions from `runner.predict_batch` method.\n        """"""\n        if isinstance(fp16, bool) and fp16:\n            fp16 = {""opt_level"": ""O1""}\n\n        if model is not None:\n            self.model = model\n        assert self.model is not None\n\n        if resume is not None:\n            checkpoint = utils.load_checkpoint(resume)\n            utils.unpack_checkpoint(checkpoint, model=self.model)\n\n        self.model, _, _, _, self.device = utils.process_components(\n            model=self.model, distributed_params=fp16, device=self.device,\n        )\n\n        utils.set_global_seed(initial_seed)\n        for batch in loader:\n            yield self.predict_batch(batch)\n\n    def trace(\n        self,\n        *,\n        model: Model = None,\n        batch: Any = None,\n        logdir: str = None,\n        loader: DataLoader = None,\n        method_name: str = ""forward"",\n        mode: str = ""eval"",\n        requires_grad: bool = False,\n        fp16: Union[Dict, bool] = None,\n        device: Device = ""cpu"",\n        predict_params: dict = None,\n    ) -> ScriptModule:\n        """"""\n        Traces model using Torch Jit.\n\n        Args:\n            model (Model): model to trace\n            batch: batch to forward through the model to trace\n            logdir (str, optional): If specified,\n                the result will be written to the directory\n            loader (DataLoader, optional): if batch is not specified, the batch\n                will be ``next(iter(loader))``\n            method_name (str): model\'s method name that will be traced\n            mode (str): ``train`` or ``eval``\n            requires_grad (bool): flag to trace with gradients\n            fp16 (Union[Dict, bool]): If not None, then sets\n                tracing params to FP16\n            device (Device): Torch device or a string\n            predict_params (dict): additional parameters for model forward\n        """"""\n        if batch is None:\n            if loader is None:\n                raise ValueError(\n                    ""If batch is not provided the loader must be specified""\n                )\n            batch = next(iter(loader))\n\n        if model is not None:\n            self.model = model\n        assert self.model is not None\n\n        if isinstance(fp16, bool) and fp16:\n            opt_level = ""O1""\n        elif isinstance(fp16, bool) and not fp16:\n            opt_level = None\n        elif isinstance(fp16, dict):\n            opt_level = fp16[""opt_level""]\n        else:\n            opt_level = fp16\n\n        if opt_level is not None:\n            device = ""cuda""\n        elif device is None:\n            if self.device is None:\n                self.device = utils.get_device()\n            device = self.device\n\n        # Dumping previous state of the model, we will need it to restore\n        _device, _is_training, _requires_grad = (\n            self.device,\n            self.model.training,\n            utils.get_requires_grad(self.model),\n        )\n\n        self.model.to(device)\n\n        # function to run prediction on batch\n        def predict_fn(model, inputs, **kwargs):\n            _model = self.model\n            self.model = model\n            result = self.predict_batch(inputs, **kwargs)\n            self.model = _model\n            return result\n\n        traced_model = utils.trace_model(\n            model=self.model,\n            predict_fn=predict_fn,\n            batch=batch,\n            method_name=method_name,\n            mode=mode,\n            requires_grad=requires_grad,\n            opt_level=opt_level,\n            device=device,\n            predict_params=predict_params,\n        )\n\n        if logdir is not None:\n            utils.save_traced_model(\n                model=traced_model,\n                logdir=logdir,\n                method_name=method_name,\n                mode=mode,\n                requires_grad=requires_grad,\n                opt_level=opt_level,\n            )\n\n        # Restore previous state of the model\n        getattr(self.model, ""train"" if _is_training else ""eval"")()\n        utils.set_requires_grad(self.model, _requires_grad)\n        self.model.to(_device)\n\n        return traced_model\n\n\n__all__ = [""Runner""]\n'"
catalyst/dl/runner/supervised.py,2,"b'from typing import Any, Callable, List, Mapping, Tuple, Union\nimport logging\n\nimport torch\n\nfrom catalyst.dl.experiment.supervised import SupervisedExperiment\nfrom catalyst.dl.runner.runner import Runner\nfrom catalyst.tools.typing import Device, RunnerModel\n\nlogger = logging.getLogger(__name__)\n\n\nclass SupervisedRunner(Runner):\n    """"""Runner for experiments with supervised model.""""""\n\n    _experiment_fn: Callable = SupervisedExperiment\n\n    def __init__(\n        self,\n        model: RunnerModel = None,\n        device: Device = None,\n        input_key: Any = ""features"",\n        output_key: Any = ""logits"",\n        input_target_key: str = ""targets"",\n    ):\n        """"""\n        Args:\n            model (RunnerModel): Torch model object\n            device (Device): Torch device\n            input_key (Any): Key in batch dict mapping for model input\n            output_key (Any): Key in output dict model output\n                will be stored under\n            input_target_key (str): Key in batch dict mapping for target\n        """"""\n        super().__init__(\n            model=model,\n            device=device,\n            input_key=input_key,\n            output_key=output_key,\n            input_target_key=input_target_key,\n        )\n\n    def _init(\n        self,\n        input_key: Any = ""features"",\n        output_key: Any = ""logits"",\n        input_target_key: str = ""targets"",\n    ):\n        """"""\n        Args:\n            input_key (Any): Key in batch dict mapping for model input\n            output_key (Any): Key in output dict model output\n                will be stored under\n            input_target_key (str): Key in batch dict mapping for target\n        """"""\n        self.experiment: SupervisedExperiment = None\n\n        self.input_key = input_key\n        self.output_key = output_key\n        self.target_key = input_target_key\n\n        if isinstance(self.input_key, str):\n            # when model expects value\n            self._process_input = self._process_input_str\n        elif isinstance(self.input_key, (list, tuple)):\n            # when model expects tuple\n            self._process_input = self._process_input_list\n        elif self.input_key is None:\n            # when model expects dict\n            self._process_input = self._process_input_none\n        else:\n            raise NotImplementedError()\n\n        if isinstance(output_key, str):\n            # when model returns value\n            self._process_output = self._process_output_str\n        elif isinstance(output_key, (list, tuple)):\n            # when model returns tuple\n            self._process_output = self._process_output_list\n        elif self.output_key is None:\n            # when model returns dict\n            self._process_output = self._process_output_none\n        else:\n            raise NotImplementedError()\n\n    def _batch2device(self, batch: Mapping[str, Any], device: Device):\n        if isinstance(batch, (tuple, list)):\n            assert len(batch) == 2\n            batch = {self.input_key: batch[0], self.target_key: batch[1]}\n        batch = super()._batch2device(batch, device)\n        return batch\n\n    def _process_input_str(self, batch: Mapping[str, Any], **kwargs):\n        output = self.model(batch[self.input_key], **kwargs)\n        return output\n\n    def _process_input_list(self, batch: Mapping[str, Any], **kwargs):\n        input = {key: batch[key] for key in self.input_key}\n        output = self.model(**input, **kwargs)\n        return output\n\n    def _process_input_none(self, batch: Mapping[str, Any], **kwargs):\n        output = self.model(**batch, **kwargs)\n        return output\n\n    def _process_output_str(self, output: torch.Tensor):\n        output = {self.output_key: output}\n        return output\n\n    def _process_output_list(self, output: Union[Tuple, List]):\n        output = {key: value for key, value in zip(self.output_key, output)}\n        return output\n\n    def _process_output_none(self, output: Mapping[str, Any]):\n        return output\n\n    def forward(self, batch: Mapping[str, Any], **kwargs) -> Mapping[str, Any]:\n        """"""\n        Forward method for your Runner.\n        Should not be called directly outside of runner.\n        If your model has specific interface, override this method to use it\n\n        Args:\n            batch (Mapping[str, Any]): dictionary with data batches\n                from DataLoaders.\n            **kwargs: additional parameters to pass to the model\n        """"""\n        output = self._process_input(batch, **kwargs)\n        output = self._process_output(output)\n        return output\n\n    def _handle_batch(self, batch: Mapping[str, Any]) -> None:\n        """"""\n        Inner method to handle specified data batch.\n        Used to make a train/valid/infer stage during Experiment run.\n\n        Args:\n            batch (Mapping[str, Any]): dictionary with data batches\n                from DataLoader.\n        """"""\n        self.output = self.forward(batch)\n\n    @torch.no_grad()\n    def predict_batch(\n        self, batch: Mapping[str, Any], **kwargs\n    ) -> Mapping[str, Any]:\n        """"""\n        Run model inference on specified data batch.\n\n        .. warning::\n            You should not override this method. If you need specific model\n            call, override forward() method\n\n        Args:\n            batch (Mapping[str, Any]): dictionary with data batches\n                from DataLoader.\n            **kwargs: additional kwargs to pass to the model\n\n        Returns:\n            Mapping[str, Any]: model output dictionary\n        """"""\n        batch = self._batch2device(batch, self.device)\n        output = self.forward(batch, **kwargs)\n        return output\n\n\n__all__ = [""SupervisedRunner""]\n'"
catalyst/dl/scripts/__init__.py,0,b''
catalyst/dl/scripts/init.py,0,"b'import argparse\nfrom pathlib import Path\n\nfrom catalyst.dl import utils\n\n\ndef build_args(parser):\n    """"""Constructs the command-line arguments for ``catalyst-dl init``.""""""\n    parser.add_argument(\n        ""-p"",\n        ""--pipeline"",\n        type=str,\n        default=None,\n        choices=[""empty"", ""classification"", ""segmentation"", ""detection""],\n        help=""select a Catalyst pipeline"",\n    )\n    parser.add_argument(\n        ""-i"",\n        ""--interactive"",\n        action=""store_true"",\n        help=""use interactive wizard to setup Catalyst pipeline"",\n    )\n    parser.add_argument(\n        ""-o"", ""--out-dir"", type=Path, default=""./"", help=""path where to init""\n    )\n\n    return parser\n\n\ndef parse_args():\n    """"""Parses the command line arguments for the main method.""""""\n    parser = argparse.ArgumentParser()\n    build_args(parser)\n    args = parser.parse_args()\n    return args\n\n\ndef main(args, _):\n    """"""Run the ``catalyst-dl init`` script.""""""\n    if args.interactive:\n        utils.run_wizard()\n    else:\n        utils.clone_pipeline(args.pipeline, args.out_dir)\n\n\nif __name__ == ""__main__"":\n    args = parse_args()\n    main(args, None)\n'"
catalyst/dl/scripts/run.py,0,"b'#!/usr/bin/env python\n\nimport argparse\nfrom argparse import ArgumentParser\nimport os\nfrom pathlib import Path\n\nfrom catalyst.dl import utils\nfrom catalyst.dl.registry import EXPERIMENTS\nfrom catalyst.utils import distributed_cmd_run, get_rank\n\n\ndef build_args(parser: ArgumentParser):\n    """"""Constructs the command-line arguments for ``catalyst-dl run``.""""""\n    parser.add_argument(\n        ""--config"",\n        ""--configs"",\n        ""-C"",\n        nargs=""+"",\n        help=""path to config/configs"",\n        metavar=""CONFIG_PATH"",\n        dest=""configs"",\n        required=True,\n    )\n    parser.add_argument(""--expdir"", type=str, default=None)\n    parser.add_argument(""--logdir"", type=str, default=None)\n    parser.add_argument(""--baselogdir"", type=str, default=None)\n    parser.add_argument(\n        ""-j"",\n        ""--num-workers"",\n        default=None,\n        type=int,\n        help=""number of data loading workers"",\n    )\n    parser.add_argument(\n        ""-b"", ""--batch-size"", default=None, type=int, help=""mini-batch size""\n    )\n    parser.add_argument(\n        ""-e"", ""--num-epochs"", default=None, type=int, help=""number of epochs""\n    )\n    parser.add_argument(\n        ""--resume"",\n        default=None,\n        type=str,\n        metavar=""PATH"",\n        help=""path to latest checkpoint"",\n    )\n    parser.add_argument(\n        ""--autoresume"",\n        type=str,\n        help=(\n            ""try automatically resume from logdir//{best,last}_full.pth ""\n            ""if --resume is empty""\n        ),\n        required=False,\n        choices=[""best"", ""last""],\n        default=None,\n    )\n    parser.add_argument(""--seed"", type=int, default=42)\n    utils.boolean_flag(\n        parser,\n        ""apex"",\n        default=os.getenv(""USE_APEX"", ""1"") == ""1"",\n        help=""Enable/disable using of Apex extension"",\n    )\n    utils.boolean_flag(\n        parser,\n        ""distributed"",\n        shorthand=""ddp"",\n        default=os.getenv(""USE_DDP"", ""0"") == ""1"",\n        help=""Run in distributed mode"",\n    )\n    utils.boolean_flag(parser, ""verbose"", default=None)\n    utils.boolean_flag(parser, ""timeit"", default=None)\n    utils.boolean_flag(parser, ""check"", default=None)\n    utils.boolean_flag(\n        parser,\n        ""deterministic"",\n        default=None,\n        help=""Deterministic mode if running in CuDNN backend"",\n    )\n    utils.boolean_flag(\n        parser, ""benchmark"", default=None, help=""Use CuDNN benchmark""\n    )\n\n    return parser\n\n\ndef parse_args():\n    """"""Parses the command line arguments and returns arguments and config.""""""\n    parser = argparse.ArgumentParser()\n    build_args(parser)\n    args, unknown_args = parser.parse_known_args()\n    return args, unknown_args\n\n\ndef main_worker(args, unknown_args):\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    args, config = utils.parse_args_uargs(args, unknown_args)\n    utils.set_global_seed(args.seed)\n    utils.prepare_cudnn(args.deterministic, args.benchmark)\n\n    config.setdefault(""distributed_params"", {})[""apex""] = args.apex\n\n    Experiment, Runner = utils.import_experiment_and_runner(Path(args.expdir))\n    if Experiment is None:\n        experiment_params = config.get(""experiment_params"", {})\n        experiment = experiment_params.get(""experiment"", ""Experiment"")\n        Experiment = EXPERIMENTS.get(experiment)\n\n    runner_params = config.get(""runner_params"", {})\n    experiment = Experiment(config)\n    runner = Runner(**runner_params)\n\n    if experiment.logdir is not None and get_rank() <= 0:\n        utils.dump_environment(config, experiment.logdir, args.configs)\n        utils.dump_code(args.expdir, experiment.logdir)\n\n    runner.run_experiment(experiment)\n\n\ndef main(args, unknown_args):\n    """"""Run the ``catalyst-dl run`` script.""""""\n    distributed_cmd_run(main_worker, args.distributed, args, unknown_args)\n\n\nif __name__ == ""__main__"":\n    args, unknown_args = parse_args()\n    main(args, unknown_args)\n'"
catalyst/dl/scripts/trace.py,0,"b'import argparse\nfrom argparse import ArgumentParser\nfrom pathlib import Path\n\nfrom catalyst.dl.utils import save_traced_model, trace_model_from_checkpoint\n\n\ndef build_args(parser: ArgumentParser):\n    """"""Builds the command line parameters.""""""\n    parser.add_argument(""logdir"", type=Path, help=""Path to model logdir"")\n    parser.add_argument(\n        ""--method"", ""-m"", default=""forward"", help=""Model method to trace""\n    )\n    parser.add_argument(\n        ""--checkpoint"",\n        ""-c"",\n        default=""best"",\n        help=""Checkpoint\'s name to trace"",\n        metavar=""CHECKPOINT_NAME"",\n    )\n    parser.add_argument(\n        ""--out-dir"",\n        type=Path,\n        default=None,\n        help=""Output directory to save traced model"",\n    )\n    parser.add_argument(\n        ""--out-model"",\n        type=Path,\n        default=None,\n        help=""Output path to save traced model (overrides --out-dir)"",\n    )\n    parser.add_argument(\n        ""--mode"",\n        type=str,\n        choices=[""eval"", ""train""],\n        default=""eval"",\n        help=""Model\'s mode \'eval\' or \'train\'"",\n    )\n    parser.add_argument(\n        ""--with-grad"",\n        action=""store_true"",\n        default=False,\n        help=""If true, model will be traced with `requires_grad_(True)`"",\n    )\n    parser.add_argument(\n        ""--opt-level"",\n        type=str,\n        default=None,\n        help=""Opt level for FP16 (optional)"",\n    )\n\n    parser.add_argument(\n        ""--stage"",\n        type=str,\n        default=None,\n        help=""Stage from experiment from which model and loader will be taken"",\n    )\n\n    parser.add_argument(\n        ""--loader"",\n        type=str,\n        default=None,\n        help=""Loader name to get the batch from"",\n    )\n\n    return parser\n\n\ndef parse_args():\n    """"""Parses the command line arguments for the main method.""""""\n    parser = argparse.ArgumentParser()\n    build_args(parser)\n    args = parser.parse_args()\n    return args\n\n\ndef main(args, _):\n    """"""Main method for ``catalyst-dl trace``.""""""\n    logdir: Path = args.logdir\n    method_name: str = args.method\n    checkpoint_name: str = args.checkpoint\n    mode: str = args.mode\n    requires_grad: bool = args.with_grad\n    opt_level: str = args.opt_level\n\n    if opt_level is not None:\n        device = ""cuda""\n    else:\n        device = ""cpu""\n\n    traced_model = trace_model_from_checkpoint(\n        logdir,\n        method_name,\n        checkpoint_name=checkpoint_name,\n        stage=args.stage,\n        loader=args.loader,\n        mode=mode,\n        requires_grad=requires_grad,\n        opt_level=opt_level,\n        device=device,\n    )\n\n    save_traced_model(\n        model=traced_model,\n        logdir=logdir,\n        method_name=method_name,\n        mode=mode,\n        requires_grad=requires_grad,\n        opt_level=opt_level,\n        out_model=args.out_model,\n        out_dir=args.out_dir,\n        checkpoint_name=checkpoint_name,\n    )\n\n\nif __name__ == ""__main__"":\n    main(parse_args(), None)\n'"
catalyst/dl/tests/__init__.py,0,b''
catalyst/dl/tests/test_dataset.py,0,"b'# flake8: noqa\nfrom catalyst.data.dataset import _Path, PathsDataset\n\n\ndef test_PathsDataset() -> None:\n    def get_target(path: _Path) -> int:\n        result = str(path).split(""."")[0].split(""_"")[1]\n        result = int(result)\n\n        return result\n\n    def identity(x):\n        return x\n\n    filenames = [""path1_1.jpg"", ""path2_1.jpg"", ""path_0.jpg""]\n    targets = [1, 1, 0]\n\n    dataset = PathsDataset(filenames, open_fn=identity, label_fn=get_target)\n\n    result = True\n    for data, target in zip(dataset.data, targets):\n        result &= data[""targets""] == target\n        assert result\n'"
catalyst/dl/tests/test_main.py,0,"b'import pytest\n\nfrom .. import __main__ as main\n\n\ndef test_arg_parser_run():\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    parser = main.build_parser()\n\n    args, uargs = parser.parse_known_args(\n        [""run"", ""--config"", ""test.yml"", ""--unknown""]\n    )\n\n    assert args.command == ""run""\n    assert args.configs == [""test.yml""]\n    assert ""--unknown"" in uargs\n\n\ndef test_run_multiple_configs():\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    parser = main.build_parser()\n\n    args, uargs = parser.parse_known_args(\n        [""run"", ""--config"", ""test.yml"", ""test1.yml""]\n    )\n\n    assert args.configs == [""test.yml"", ""test1.yml""]\n\n\ndef test_arg_parser_fail_on_none():\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    parser = main.build_parser()\n\n    with pytest.raises(SystemExit):\n        # Raises SystemExit when args are not ok\n        parser.parse_known_args([""--config"", ""test.yml"", ""--unknown""])\n'"
catalyst/dl/utils/__init__.py,0,"b'# flake8: noqa\n\nfrom catalyst.contrib.utils import *\nfrom catalyst.core.utils import *\nfrom catalyst.utils import *\n\nfrom .torch import get_loader\nfrom .trace import (\n    get_trace_name,\n    load_traced_model,\n    save_traced_model,\n    trace_model,\n    trace_model_from_checkpoint,\n    trace_model_from_runner,\n)\nfrom .wizard import run_wizard, Wizard\n'"
catalyst/dl/utils/torch.py,3,"b'from typing import Callable, Iterable\n\nimport torch\nfrom torch.utils.data.dataloader import default_collate as default_collate_fn\n\nfrom catalyst.data import ListDataset\n\n\ndef get_loader(\n    data_source: Iterable[dict],\n    open_fn: Callable,\n    dict_transform: Callable = None,\n    sampler=None,\n    collate_fn: Callable = default_collate_fn,\n    batch_size: int = 32,\n    num_workers: int = 4,\n    shuffle: bool = False,\n    drop_last: bool = False,\n):\n    """"""Creates a DataLoader from given source and its open/transform params.\n\n    Args:\n        data_source (Iterable[dict]): and iterable containing your\n            data annotations,\n            (for example path to images, labels, bboxes, etc)\n        open_fn (Callable): function, that can open your\n            annotations dict and\n            transfer it to data, needed by your network\n            (for example open image by path, or tokenize read string)\n        dict_transform (callable): transforms to use on dict\n            (for example normalize image, add blur, crop/resize/etc)\n        sampler (Sampler, optional): defines the strategy to draw samples from\n            the dataset\n        collate_fn (callable, optional): merges a list of samples to form a\n            mini-batch of Tensor(s).  Used when using batched loading from a\n            map-style dataset\n        batch_size (int, optional): how many samples per batch to load\n        num_workers (int, optional): how many subprocesses to use for data\n            loading. ``0`` means that the data will be loaded\n            in the main process\n        shuffle (bool, optional): set to ``True`` to have the data reshuffled\n            at every epoch (default: ``False``).\n        drop_last (bool, optional): set to ``True`` to drop\n            the last incomplete batch, if the dataset size is not divisible\n            by the batch size. If ``False`` and the size of dataset\n            is not divisible by the batch size, then the last batch\n            will be smaller. (default: ``False``)\n\n    Returns:\n        DataLoader with ``catalyst.data.ListDataset``\n    """"""\n    dataset = ListDataset(\n        list_data=data_source, open_fn=open_fn, dict_transform=dict_transform,\n    )\n    loader = torch.utils.data.DataLoader(\n        dataset=dataset,\n        sampler=sampler,\n        collate_fn=collate_fn,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        shuffle=shuffle,\n        pin_memory=torch.cuda.is_available(),\n        drop_last=drop_last,\n    )\n    return loader\n\n\n__all__ = [""get_loader""]\n'"
catalyst/dl/utils/trace.py,1,"b'from typing import (  # isort:skip\n    Any,\n    Callable,\n    Dict,\n    List,\n    Union,\n)\nimport inspect\nfrom pathlib import Path\n\nfrom torch import nn\nfrom torch.jit import load, save, ScriptModule, trace\n\nfrom catalyst.core.runner import IRunner\nfrom catalyst.dl.experiment.config import ConfigExperiment\nfrom catalyst.tools.typing import Device, Model\nfrom catalyst.utils import (\n    any2device,\n    assert_fp16_available,\n    get_fn_argsnames,\n    get_native_batch_from_loaders,\n    get_nn_from_ddp_module,\n    get_requires_grad,\n    import_experiment_and_runner,\n    load_checkpoint,\n    load_config,\n    pack_checkpoint,\n    set_requires_grad,\n    unpack_checkpoint,\n)\n\n\ndef _get_input_argnames(\n    fn: Callable[..., Any], exclude: List[str] = None\n) -> List[str]:\n    """"""\n    Function to get input argument names of function.\n\n    Args:\n        fn (Callable[..., Any]): Function to get argument names from\n        exclude (List[str]): List of string of names to exclude\n\n    Returns:\n        (List[str]): List of input argument names\n    """"""\n    argspec = inspect.getfullargspec(fn)\n    assert (\n        argspec.varargs is None and argspec.varkw is None\n    ), ""not supported by PyTorch""\n\n    return get_fn_argsnames(fn, exclude=exclude)\n\n\nclass _ForwardOverrideModel(nn.Module):\n    """"""Model that calls specified method instead of forward.\n\n    (Workaround, single method tracing is not supported)\n    """"""\n\n    def __init__(self, model, method_name):\n        super().__init__()\n        self.model = model\n        self.method_name = method_name\n\n    def forward(self, *args, **kwargs):\n        return getattr(self.model, self.method_name)(*args, **kwargs)\n\n\nclass _TracingModelWrapper(nn.Module):\n    """"""Wrapper that traces model with batch instead of calling it.\n\n    (Workaround, to use native model batch handler)\n    """"""\n\n    def __init__(self, model, method_name):\n        super().__init__()\n        self.model = model\n        self.method_name = method_name\n        self.tracing_result: ScriptModule\n\n    def __call__(self, *args, **kwargs):\n        method_model = _ForwardOverrideModel(self.model, self.method_name)\n\n        try:\n            assert len(args) == 0, ""only KV support implemented""\n\n            fn = getattr(self.model, self.method_name)\n            method_argnames = _get_input_argnames(fn=fn, exclude=[""self""])\n            method_input = tuple(kwargs[name] for name in method_argnames)\n\n            self.tracing_result = trace(method_model, method_input)\n        except Exception:\n            # for backward compatibility\n            self.tracing_result = trace(method_model, *args, **kwargs)\n        output = self.model.forward(*args, **kwargs)\n\n        return output\n\n\ndef trace_model(\n    model: Model,\n    predict_fn: Callable,\n    batch=None,\n    method_name: str = ""forward"",\n    mode: str = ""eval"",\n    requires_grad: bool = False,\n    opt_level: str = None,\n    device: Device = ""cpu"",\n    predict_params: dict = None,\n) -> ScriptModule:\n    """"""Traces model using runner and batch.\n\n    Args:\n        model: Model to trace\n        predict_fn: Function to run prediction with the model provided,\n            takes model, inputs parameters\n        batch: Batch to trace the model\n        method_name (str): Model\'s method name that will be\n            used as entrypoint during tracing\n        mode (str): Mode for model to trace (``train`` or ``eval``)\n        requires_grad (bool): Flag to use grads\n        opt_level (str): Apex FP16 init level, optional\n        device (str): Torch device\n        predict_params (dict): additional parameters for model forward\n\n    Returns:\n        (ScriptModule): Traced model\n    """"""\n    if batch is None or predict_fn is None:\n        raise ValueError(""Both batch and predict_fn must be specified."")\n\n    if mode not in [""train"", ""eval""]:\n        raise ValueError(f""Unknown mode \'{mode}\'. Must be \'eval\' or \'train\'"")\n\n    predict_params = predict_params or {}\n\n    tracer = _TracingModelWrapper(model, method_name)\n    if opt_level is not None:\n        assert_fp16_available()\n        # If traced in AMP we need to initialize the model before calling\n        # the jit\n        # https://github.com/NVIDIA/apex/issues/303#issuecomment-493142950\n        from apex import amp\n\n        model = model.to(device)\n        model = amp.initialize(model, optimizers=None, opt_level=opt_level)\n\n    getattr(model, mode)()\n    set_requires_grad(model, requires_grad=requires_grad)\n\n    predict_fn(tracer, batch, **predict_params)\n\n    return tracer.tracing_result\n\n\ndef trace_model_from_checkpoint(\n    logdir: Path,\n    method_name: str,\n    checkpoint_name: str,\n    stage: str = None,\n    loader: Union[str, int] = None,\n    mode: str = ""eval"",\n    requires_grad: bool = False,\n    opt_level: str = None,\n    device: Device = ""cpu"",\n):\n    """"""\n    Traces model using created experiment and runner.\n\n    Args:\n        logdir (Union[str, Path]): Path to Catalyst logdir with model\n        checkpoint_name (str): Name of model checkpoint to use\n        stage (str): experiment\'s stage name\n        loader (Union[str, int]): experiment\'s loader name or its index\n        method_name (str): Model\'s method name that will be\n            used as entrypoint during tracing\n        mode (str): Mode for model to trace (``train`` or ``eval``)\n        requires_grad (bool): Flag to use grads\n        opt_level (str): AMP FP16 init level\n        device (str): Torch device\n\n    Returns:\n        the traced model\n    """"""\n    config_path = logdir / ""configs"" / ""_config.json""\n    checkpoint_path = logdir / ""checkpoints"" / f""{checkpoint_name}.pth""\n    print(""Load config"")\n    config: Dict[str, dict] = load_config(config_path)\n    runner_params = config.get(""runner_params"", {}) or {}\n\n    # Get expdir name\n    config_expdir = Path(config[""args""][""expdir""])\n    # We will use copy of expdir from logs for reproducibility\n    expdir = Path(logdir) / ""code"" / config_expdir.name\n\n    print(""Import experiment and runner from logdir"")\n    ExperimentType, RunnerType = import_experiment_and_runner(expdir)\n    experiment: ConfigExperiment = ExperimentType(config)\n\n    print(f""Load model state from checkpoints/{checkpoint_name}.pth"")\n    if stage is None:\n        stage = list(experiment.stages)[0]\n\n    model = experiment.get_model(stage)\n    checkpoint = load_checkpoint(checkpoint_path)\n    unpack_checkpoint(checkpoint, model=model)\n\n    runner: RunnerType = RunnerType(**runner_params)\n    runner.model, runner.device = model, device\n\n    if loader is None:\n        loader = 0\n    batch = get_native_batch_from_loaders(\n        loaders=experiment.get_loaders(stage), loader=loader\n    )\n\n    # function to run prediction on batch\n    def predict_fn(model, inputs, **kwargs):\n        _model = runner.model\n        runner.model = model\n        result = runner.predict_batch(inputs, **kwargs)\n        runner.model = _model\n        return result\n\n    print(""Tracing"")\n    traced_model = trace_model(\n        model=model,\n        predict_fn=predict_fn,\n        batch=batch,\n        method_name=method_name,\n        mode=mode,\n        requires_grad=requires_grad,\n        opt_level=opt_level,\n        device=device,\n    )\n\n    print(""Done"")\n    return traced_model\n\n\ndef trace_model_from_runner(\n    runner: IRunner,\n    checkpoint_name: str = None,\n    method_name: str = ""forward"",\n    mode: str = ""eval"",\n    requires_grad: bool = False,\n    opt_level: str = None,\n    device: Device = ""cpu"",\n) -> ScriptModule:\n    """"""\n    Traces model using created experiment and runner.\n\n    Args:\n        runner (Runner): Current runner.\n        checkpoint_name (str): Name of model checkpoint to use, if None\n            traces current model from runner\n        method_name (str): Model\'s method name that will be\n            used as entrypoint during tracing\n        mode (str): Mode for model to trace (``train`` or ``eval``)\n        requires_grad (bool): Flag to use grads\n        opt_level (str): AMP FP16 init level\n        device (str): Torch device\n\n    Returns:\n        (ScriptModule): Traced model\n    """"""\n    logdir = runner.logdir\n    model = get_nn_from_ddp_module(runner.model)\n\n    if checkpoint_name is not None:\n        dumped_checkpoint = pack_checkpoint(model=model)\n        checkpoint_path = logdir / ""checkpoints"" / f""{checkpoint_name}.pth""\n        checkpoint = load_checkpoint(filepath=checkpoint_path)\n        unpack_checkpoint(checkpoint=checkpoint, model=model)\n\n    # getting input names of args for method since we don\'t have Runner\n    # and we don\'t know input_key to preprocess batch for method call\n    fn = getattr(model, method_name)\n    method_argnames = _get_input_argnames(fn=fn, exclude=[""self""])\n\n    batch = {}\n    for name in method_argnames:\n        # TODO: We don\'t know input_keys without runner\n        assert name in runner.input, (\n            ""Input batch should contain the same keys as input argument ""\n            ""names of `forward` function to be traced correctly""\n        )\n        batch[name] = runner.input[name]\n\n    batch = any2device(batch, device)\n\n    # Dumping previous runner of the model, we will need it to restore\n    _device, _is_training, _requires_grad = (\n        runner.device,\n        model.training,\n        get_requires_grad(model),\n    )\n\n    model.to(device)\n\n    # Function to run prediction on batch\n    def predict_fn(model: Model, inputs, **kwargs):\n        return model(**inputs, **kwargs)\n\n    traced_model = trace_model(\n        model=model,\n        predict_fn=predict_fn,\n        batch=batch,\n        method_name=method_name,\n        mode=mode,\n        requires_grad=requires_grad,\n        opt_level=opt_level,\n        device=device,\n    )\n\n    if checkpoint_name is not None:\n        unpack_checkpoint(checkpoint=dumped_checkpoint, model=model)\n\n    # Restore previous runner of the model\n    getattr(model, ""train"" if _is_training else ""eval"")()\n    set_requires_grad(model, _requires_grad)\n    model.to(_device)\n\n    return traced_model\n\n\ndef get_trace_name(\n    method_name: str,\n    mode: str = ""eval"",\n    requires_grad: bool = False,\n    opt_level: str = None,\n    additional_string: str = None,\n) -> str:\n    """"""Creates a file name for the traced model.\n\n    Args:\n        method_name (str): model\'s method name\n        mode (str): ``train`` or ``eval``\n        requires_grad (bool): flag if model was traced with gradients\n        opt_level (str): opt_level if model was traced in FP16\n        additional_string (str): any additional information\n\n    Returns:\n        file_name (str): Filename for traced model to be saved.\n    """"""\n    file_name = f""traced""\n    if additional_string is not None:\n        file_name += f""-{additional_string}""\n\n    file_name += f""-{method_name}""\n    if mode == ""train"":\n        file_name += ""-in_train""\n\n    if requires_grad:\n        file_name += f""-with_grad""\n\n    if opt_level is not None:\n        file_name += f""-opt_{opt_level}""\n\n    file_name += "".pth""\n\n    return file_name\n\n\ndef save_traced_model(\n    model: ScriptModule,\n    logdir: Union[str, Path] = None,\n    method_name: str = ""forward"",\n    mode: str = ""eval"",\n    requires_grad: bool = False,\n    opt_level: str = None,\n    out_dir: Union[str, Path] = None,\n    out_model: Union[str, Path] = None,\n    checkpoint_name: str = None,\n):\n    """"""Saves traced model.\n\n    Args:\n        model (ScriptModule): Traced model\n        logdir (Union[str, Path]): Path to experiment\n        method_name (str): Name of the method was traced\n        mode (str): Model\'s mode - `train` or `eval`\n        requires_grad (bool): Whether model was traced with require_grad or not\n        opt_level (str): Apex FP16 init level used during tracing\n        out_dir (Union[str, Path]): Directory to save model to\n            (overrides logdir)\n        out_model (Union[str, Path]): Path to save model to\n            (overrides logdir & out_dir)\n        checkpoint_name (str): Checkpoint name used to restore the model\n    """"""\n    if out_model is None:\n        file_name = get_trace_name(\n            method_name=method_name,\n            mode=mode,\n            requires_grad=requires_grad,\n            opt_level=opt_level,\n            additional_string=checkpoint_name,\n        )\n\n        output: Path = out_dir\n        if output is None:\n            if logdir is None:\n                raise ValueError(\n                    ""One of `logdir`, `out_dir` or `out_model` ""\n                    ""should be specified""\n                )\n            output: Path = Path(logdir) / ""trace""\n\n        output.mkdir(exist_ok=True, parents=True)\n\n        out_model = str(output / file_name)\n    else:\n        out_model = str(out_model)\n\n    save(model, out_model)\n\n\ndef load_traced_model(\n    model_path: Union[str, Path],\n    device: Device = ""cpu"",\n    opt_level: str = None,\n) -> ScriptModule:\n    """"""Loads a traced model.\n\n    Args:\n        model_path: Path to traced model\n        device (str): Torch device\n        opt_level (str): Apex FP16 init level, optional\n\n    Returns:\n        (ScriptModule): Traced model\n    """"""\n    # jit.load dont work with pathlib.Path\n    model_path = str(model_path)\n\n    if opt_level is not None:\n        device = ""cuda""\n\n    model = load(model_path, map_location=device)\n\n    if opt_level is not None:\n        assert_fp16_available()\n        from apex import amp\n\n        model = amp.initialize(model, optimizers=None, opt_level=opt_level)\n\n    return model\n\n\n__all__ = [\n    ""trace_model"",\n    ""trace_model_from_checkpoint"",\n    ""trace_model_from_runner"",\n    ""get_trace_name"",\n    ""save_traced_model"",\n    ""load_traced_model"",\n]\n'"
catalyst/dl/utils/wizard.py,0,"b'from collections import OrderedDict\nimport pathlib\n\nfrom prompt_toolkit import prompt\nimport yaml\n\nfrom catalyst.dl import registry\nfrom catalyst.utils import clone_pipeline, import_module\nfrom catalyst.utils.pipelines import URLS\n\nyaml.add_representer(\n    OrderedDict,\n    lambda dumper, data: dumper.represent_mapping(\n        ""tag:yaml.org,2002:map"", data.items()\n    ),\n)\n\n\nclass Wizard:\n    """"""\n    Class for Catalyst Config API Wizard.\n\n    The instance of this class will be created and called from cli command:\n    ``catalyst-dl init --interactive``.\n\n    With help of this Wizard user will be able to setup pipeline from available\n    templates and make choices of what predefined classes to use in different\n    parts of pipeline.\n    """"""\n\n    def __init__(self):\n        """"""\n        Initialization of instance of this class will print welcome message and\n        logo of Catalyst in ASCII format. Also here we\'ll save all classes of\n        Catalyst own pipeline parts to be able to put user\'s modules on top of\n        lists to ease the choice.\n        """"""\n        self.__sep(""Welcome to Catalyst Config API wizard!"")\n        print(\n            """"""\n                       ___________\n                      (_         _)\n                        |       |\n                        |       |\n                        |       |\n                       /         \\\\\n                      /    (      \\\\\n                     /    /  (#    \\\\\n                    /    #     #    \\\\\n                   /    #       #    \\\\\n                  /      #######      \\\\\n                 (_____________________)\n        \\n""""""\n        )\n\n        self._cfg = OrderedDict(\n            [\n                (""model_params"", OrderedDict()),\n                (""args"", OrderedDict()),\n                (""stages"", OrderedDict()),\n            ]\n        )\n\n        self.pipeline_path = pathlib.Path(""./"")\n        self.__before_export = {\n            ""MODELS"": registry.__dict__[""MODELS""].all(),\n            ""CRITERIONS"": registry.__dict__[""CRITERIONS""].all(),\n            ""OPTIMIZERS"": registry.__dict__[""OPTIMIZERS""].all(),\n            ""SCHEDULERS"": registry.__dict__[""SCHEDULERS""].all(),\n            ""CALLBACKS"": registry.__dict__[""CALLBACKS""].all(),\n        }\n\n    @staticmethod\n    def __sep(step_name: str = None):\n        """"""Separator between Wizard sections.""""""\n        if step_name is None:\n            print(""\\n"" + ""="" * 100 + ""\\n"")\n        else:\n            msg = ""\\n"" + ""="" * 100 + ""\\n""\n            msg += ""="" * 10 + "" "" + step_name + "" ""\n            msg += ""="" * (100 - len(step_name) - 12)\n            msg += ""\\n"" + ""="" * 100 + ""\\n""\n            print(msg)\n\n    @staticmethod\n    def _export_step():\n        print(\n            ""Config is complete. What is next?\\n\\n""\n            ""1. Preview config in YAML format\\n""\n            ""2. Save config to file\\n""\n            ""3. Discard changes and exit\\n""\n        )\n        return prompt(""Enter the number: "")\n\n    @staticmethod\n    def __res(result, is_yaml=False):\n        if is_yaml:\n            print(f""->\\n{yaml.dump(result, default_flow_style=False)}"")\n        else:\n            print(f""-> {result}"")\n\n    def __sorted_for_user(self, key):\n        """"""\n        Here we put user\'s modules of specific part of pipeline on top of\n        modules predefined in Catalyst.\n        """"""\n        modules = registry.__dict__[key].all()\n        user_modules = list(set(modules) - set(self.__before_export[key]))\n        user_modules = sorted(user_modules)\n        return user_modules + sorted(m for m in modules if m[0].isupper())\n\n    def _preview(self):\n        """"""Showing user final config in YAML format.""""""\n        self.__sep()\n        print(yaml.dump(self._cfg, default_flow_style=False))\n        self.__sep()\n\n    def _dump_step(self):\n        """"""Asking where and saving final config converted into YAML.""""""\n        path = prompt(""Enter config path: "", default=""./configs/config.yml"")\n        self.__res(path)\n        path = pathlib.Path(path)\n        with path.open(mode=""w"") as stream:\n            yaml.dump(self._cfg, stream, default_flow_style=False)\n        print(f""Config was written to {path}"")\n\n    def _skip_override_stages_common(self, param_name):\n        """"""\n        Stages could have common params, in that case we will ask user if it\n        should be overriden for specific step. If not - we\'ll just skip entire\n        params section for stage.\n        """"""\n        common = None\n        if param_name in self._cfg[""stages""]:\n            common = self._cfg[""stages""][param_name]\n            print(\n                ""You have common setting for all stages:\\n""\n                + yaml.dump(common, default_flow_style=False)\n            )\n            res = prompt(""Do you want to override it? (y/N): "", default=""N"")\n            self.__res(res)\n            return res.upper() == ""N""\n        return False\n\n    def _callbacks_step(self, stage):\n        self.__sep(f""Callbacks"")\n        print(\n            ""Let\'s add some callbacks!\\n\\n""\n            ""!!! Remember that Catalyst will add Criterion, Optimizer and ""\n            ""Checkpoint callbacks for you\\n""\n            ""with default settings if name of the step is NOT started ""\n            ""with ``infer``.\\n""\n        )\n        opts = OrderedDict()\n        while True:\n            callback = prompt(\n                ""Enter callback section name, e.g. ""\n                ""\'loss_aggregator\'""\n                ""(or hit Enter to stop adding callbacks): ""\n            )\n            if not callback:\n                if opts:\n                    stage[""callbacks""] = opts\n                return\n            self.__res(callback)\n            callback_params = OrderedDict()\n            self._basic_params_step(""callback"", callback_params)\n            opts[callback] = callback_params[""callback_params""]\n\n    def _basic_params_step(self, param, stage, optional=False):\n        """"""\n        Step #x\n\n        Models, criterions, callbacks, schedulers could be choosen from list of\n        predefined in Catalyst as well as from imported from user expdir. Also\n        it even could not exist yet, so we provide a way to enter class name of\n        the entity. Also we request args or params of those modules, but they\n        are weak-typed now and will be all strings/ints in final config.\n        """"""\n        self.__sep(f""{param}_params"")\n        if self._skip_override_stages_common(f""{param}_params""):\n            return\n        opts = OrderedDict()\n        modules = self.__sorted_for_user(f""{param.upper()}S"")\n        msg = f""What {param} you\'ll be using:\\n\\n""\n        if modules:\n            if optional:\n                msg += ""0: Skip this param\\n""\n            msg += ""\\n"".join([f""{n+1}: {m}"" for n, m in enumerate(modules)])\n            print(msg)\n            module = prompt(\n                ""\\nEnter number from list above or ""\n                f""class name of {param} you\'ll be using: ""\n            )\n            if module.isdigit():\n                module = int(module)\n                if module == 0:\n                    self.__res(""Skipping..."")\n                    return\n                module = modules[module - 1]\n                self.__res(module)\n        else:\n            module = prompt(\n                f""Enter class name of {param} "" ""you\'ll be using: ""\n            )\n            self.__res(module)\n        opts[param] = module\n        res = prompt(\n            ""If there are arguments you want to provide during ""\n            f""{param} initialization, provide them here in ""\n            ""following format:\\n\\nlr=0.001,beta=3.41\\n\\n""\n            ""Or just skip this step (press Enter): ""\n        )\n        if res:\n            res = [t.split(""="") for t in res.split("","")]\n            for k, val in res:\n                # We can add regex to parse params properly into types we need\n                opts[k] = int(val) if val.isdigit() else val\n        self.__res(opts, is_yaml=True)\n        stage[f""{param}_params""] = opts\n\n    def _stage_params_step(self, stage):\n        """"""\n        Step #5.b\n\n        ``stage_params`` of Experiment.\n        """"""\n        self.__sep(f""stage_params"")\n        if self._skip_override_stages_common(""stage_params""):\n            return\n        opts = OrderedDict()\n        opts[""num_epochs""] = int(\n            prompt(\n                ""How much epochs you want to run this "" ""stage: "", default=""1""\n            )\n        )\n        self.__res(opts[""num_epochs""])\n        opts[""main_metric""] = prompt(\n            ""What is the main_metric?: "", default=""loss""\n        )\n        self.__res(opts[""main_metric""])\n        minimize = bool(\n            prompt(""Will it be minimized (True/False): "", default=""True"")\n        )\n        opts[""minimize_metric""] = minimize\n        self.__res(opts[""minimize_metric""])\n        stage[""stage_params""] = opts\n\n    def _data_params_step(self, stage):\n        """"""\n        Step #5.a\n\n        Here we\'ll store required ``data_params``. Right now experiment\n        couldn\'t be run without ``num_worker`` param, but it\'s rarely when user\n        needs batch_size of 1\n        """"""\n        self.__sep(f""data_params"")\n        if self._skip_override_stages_common(""data_params""):\n            return\n        opts = OrderedDict()\n        opts[""batch_size""] = int(\n            prompt(""What is the batch_size?: "", default=""1"")\n        )\n        self.__res(opts[""batch_size""])\n        opts[""num_workers""] = int(\n            prompt(""What is the num_workers?: "", default=""1"")\n        )\n        self.__res(opts[""num_workers""])\n        stage[""data_params""] = opts\n\n    def _stage_step(self, stage):\n        """"""\n        Step #5\n\n        For stages\' common params and for every stage params we\'ll run this\n        method to gather all we need to know about the stage and its settings\n        """"""\n        self._data_params_step(stage)\n        self._stage_params_step(stage)\n        self._basic_params_step(""criterion"", stage)\n        self._basic_params_step(""optimizer"", stage)\n        self._basic_params_step(""scheduler"", stage, optional=True)\n        self._callbacks_step(stage)\n        return\n\n    def _stages_step(self):\n        """"""\n        Step #4\n\n        Stages params. We need to understand how much stages will be there,\n        what are their names and if user wants to predefine something common\n        for all stages\n        """"""\n        self.__sep(""stages"")\n        cnt = prompt(""How much stages your exepriment will contain: "")\n        self.__res(cnt)\n        cnt = int(cnt) or 1\n        if cnt > 1:\n            res = prompt(\n                ""Do you want to assign some common settings ""\n                ""for all stages? (y/N): "",\n                default=""y"",\n            )\n            self.__res(res)\n            if res.lower() == ""y"":\n                self._stage_step(self._cfg[""stages""])\n            print(f""\\nNow we\'ll configure all {cnt} stages one-by-one\\n"")\n        for stage_id in range(cnt):\n            name = prompt(\n                ""What would be the name of this stage: "",\n                default=f""stage{stage_id + 1}"",\n            )\n            self.__res(name)\n            stage = OrderedDict()\n            self._stage_step(stage)\n            self._cfg[""stages""][name] = stage\n\n    def _model_step(self):\n        """"""\n        Step #3\n\n        We need to user choose its model for experiment\n        """"""\n        self._basic_params_step(""model"", self._cfg)\n\n    def __export_user_modules(self):\n        """"""\n        Private method to try to export user\'s modules.\n        We need this to add user\'s modules to list of choices for pipeline\n        parts\n        """"""\n        try:\n            # We need to import module to add possible modules to registry\n            expdir = self._cfg[""args""][""expdir""]\n            if not isinstance(expdir, pathlib.Path):\n                expdir = pathlib.Path(expdir)\n            import_module(expdir)\n            self.__res(f""Modules from {expdir} exported"")\n        except OSError:\n            print(f""There is no modules to import found: {expdir}"")\n        except Exception as err:\n            print(\n                ""Unexpected error when tried to import modules from ""\n                f""{expdir}: {err}""\n            )\n\n    def _args_step(self):\n        """"""\n        Step #2\n\n        ``args`` section where two params required:\n            expdir - where all user modules stored\n            logdir - where Catalyst will write logs\n        """"""\n        self.__sep(""args"")\n        self._cfg[""args""][""expdir""] = prompt(\n            ""Provide expdir for your experiment ""\n            ""(where is the `__init__.py` with your modules stored): "",\n            default=str(self.pipeline_path / ""src""),\n        )\n        self.__res(self._cfg[""args""][""expdir""])\n        self._cfg[""args""][""logdir""] = prompt(\n            ""Provide logdir for your experiment ""\n            ""(where Catalyst supposed to save its logs): "",\n            default=str(self.pipeline_path / ""logs/experiment""),\n        )\n        self.__res(self._cfg[""args""][""logdir""])\n\n        self.__export_user_modules()\n\n    def _pipeline_step(self):\n        """"""\n        Step #1\n\n        User can choose which pipeline to clone and if not skipped - where.\n        Then pipeline will be copied in requested directory\n        """"""\n        self.__sep(""Pipeline templates"")\n        opts = list(URLS.keys()) + [""empty""]\n        opts = [opt.capitalize() for opt in opts]\n        msg = ""0: Skip this step\\n""\n        msg += ""\\n"".join([f""{n + 1}: {v}"" for n, v in enumerate(opts)])\n        print(msg)\n        res = int(\n            prompt(\n                ""\\nChoose pipeline template you want to init ""\n                ""your project from: ""\n            )\n        )\n        if res == 0:\n            self.__res(""Skipped..."")\n            return\n        pipeline = opts[res - 1]\n        self.__res(pipeline)\n        out_dir = prompt(\n            f""Where we need to copy {pipeline} "" ""template files?: "",\n            default=""./"",\n        )\n        self.pipeline_path = pathlib.Path(out_dir)\n        clone_pipeline(pipeline.lower(), self.pipeline_path)\n        self.__res(f""{pipeline} cloned to {self.pipeline_path}"")\n\n    def run(self):\n        """"""Walks user through predefined wizard steps.""""""\n        self._pipeline_step()\n        self._args_step()\n        self._model_step()\n        self._stages_step()\n        while True:\n            res = self._export_step()\n            if res == ""1"":\n                self._preview()\n            elif res == ""2"":\n                self._dump_step()\n                return\n            elif res == ""3"":\n                return\n            else:\n                print(f""Unknown option `{res}`"")\n\n\ndef run_wizard():\n    """"""Method to initialize and run wizard.""""""\n    wiz = Wizard()\n    wiz.run()\n\n\n__all__ = [""run_wizard"", ""Wizard""]\n'"
catalyst/tools/meters/__init__.py,0,b'# flake8: noqa\nfrom .apmeter import APMeter\nfrom .aucmeter import AUCMeter\nfrom .averagevaluemeter import AverageValueMeter\nfrom .classerrormeter import ClassErrorMeter\nfrom .confusionmeter import ConfusionMeter\nfrom .mapmeter import mAPMeter\nfrom .movingaveragevaluemeter import MovingAverageValueMeter\nfrom .msemeter import MSEMeter\nfrom .ppv_tpr_f1_meter import PrecisionRecallF1ScoreMeter\n'
catalyst/tools/meters/apmeter.py,18,"b'""""""\nThe APMeter measures the average precision per class.\n""""""\nimport math\n\nimport torch\n\nfrom . import meter\n\n\nclass APMeter(meter.Meter):\n    """"""\n    The APMeter is designed to operate on `NxK` Tensors `output` and\n    `target`, and optionally a `Nx1` Tensor weight where:\n\n    1. The `output` contains model output scores for `N` examples and\n    `K` classes that ought to be higher when the model is more convinced\n    that the example should be positively labeled, and smaller when the\n    model believes the example should be negatively labeled\n    (for instance, the output of a sigmoid function).\n\n    2. The `target` contains only values 0 (for negative examples)\n    and 1 (for positive examples).\n\n    3. The `weight` ( > 0) represents weight\n    for each sample.\n    """"""\n\n    def __init__(self):\n        """"""Constructor method for the ``APMeter`` class.""""""\n        super(APMeter, self).__init__()\n        self.reset()\n\n    def reset(self):\n        """"""Resets the meter with empty member variables.""""""\n        self.scores = torch.FloatTensor(torch.FloatStorage())\n        self.targets = torch.LongTensor(torch.LongStorage())\n        self.weights = torch.FloatTensor(torch.FloatStorage())\n\n    def add(\n        self,\n        output: torch.Tensor,\n        target: torch.Tensor,\n        weight: torch.Tensor = None,\n    ) -> None:\n        """"""Add a new observation.\n\n        Args:\n            output (Tensor): NxK tensor that for each of the N examples\n                indicates the probability of the example belonging to each of\n                the K classes, according to the model. The probabilities should\n                sum to one over all classes\n            target (Tensor): binary NxK tensort that encodes which of the K\n                classes are associated with the N-th input\n                (eg: a row [0, 1, 0, 1] indicates that the example is\n                associated with classes 2 and 4)\n            weight (optional, Tensor): Nx1 tensor representing the weight for\n                each example (each weight > 0)\n        """"""\n        if not torch.is_tensor(output):\n            output = torch.from_numpy(output)\n        if not torch.is_tensor(target):\n            target = torch.from_numpy(target)\n\n        if weight is not None:\n            if not torch.is_tensor(weight):\n                weight = torch.from_numpy(weight)\n            weight = weight.squeeze()\n        if output.dim() == 1:\n            output = output.view(-1, 1)\n        else:\n            assert (\n                output.dim() == 2\n            ), ""wrong output size (should be 1D or 2D with one column \\\n                per class)""\n\n        if target.dim() == 1:\n            target = target.view(-1, 1)\n        else:\n            assert (\n                target.dim() == 2\n            ), ""wrong target size (should be 1D or 2D with one column \\\n                per class)""\n\n        if weight is not None:\n            assert weight.dim() == 1, ""Weight dimension should be 1""\n            assert weight.numel() == target.size(\n                0\n            ), ""Weight dimension 1 should be the same as that of target""\n            assert torch.min(weight) >= 0, ""Weight should be non-negative only""\n        assert torch.equal(\n            target ** 2, target\n        ), ""targets should be binary (0 or 1)""\n        if self.scores.numel() > 0:\n            assert target.size(1) == self.targets.size(\n                1\n            ), ""dimensions for output should match previously added examples.""\n\n        # make sure storage is of sufficient size\n        if self.scores.storage().size() < self.scores.numel() + output.numel():\n            new_size = math.ceil(self.scores.storage().size() * 1.5)\n            new_weight_size = math.ceil(self.weights.storage().size() * 1.5)\n            self.scores.storage().resize_(int(new_size + output.numel()))\n            self.targets.storage().resize_(int(new_size + output.numel()))\n            if weight is not None:\n                self.weights.storage().resize_(\n                    int(new_weight_size + output.size(0))\n                )\n\n        # store scores and targets\n        offset = self.scores.size(0) if self.scores.dim() > 0 else 0\n        self.scores.resize_(offset + output.size(0), output.size(1))\n        self.targets.resize_(offset + target.size(0), target.size(1))\n        self.scores.narrow(0, offset, output.size(0)).copy_(output)\n        self.targets.narrow(0, offset, target.size(0)).copy_(target)\n\n        if weight is not None:\n            self.weights.resize_(offset + weight.size(0))\n            self.weights.narrow(0, offset, weight.size(0)).copy_(weight)\n\n    def value(self):\n        """"""Returns the model""s average precision for each class.\n\n        Return:\n            FloatTensor: 1xK tensor, with avg precision for each class k\n        """"""\n        if self.scores.numel() == 0:\n            return 0\n        ap = torch.zeros(self.scores.size(1))\n        if hasattr(torch, ""arange""):\n            rg = torch.arange(1, self.scores.size(0) + 1).float()\n        else:\n            rg = torch.range(1, self.scores.size(0)).float()\n        if self.weights.numel() > 0:\n            weight = self.weights.new(self.weights.size())\n            weighted_truth = self.weights.new(self.weights.size())\n\n        # compute average precision for each class\n        for k in range(self.scores.size(1)):\n            # sort scores\n            scores = self.scores[:, k]\n            targets = self.targets[:, k]\n            _, sortind = torch.sort(scores, 0, True)\n            truth = targets[sortind]\n            if self.weights.numel() > 0:\n                weight = self.weights[sortind]\n                weighted_truth = truth.float() * weight\n                rg = weight.cumsum(0)\n\n            # compute true positive sums\n            if self.weights.numel() > 0:\n                tp = weighted_truth.cumsum(0)\n            else:\n                tp = truth.float().cumsum(0)\n\n            # compute precision curve\n            precision = tp.div(rg)\n\n            # compute average precision\n            ap[k] = precision[truth.byte()].sum() / max(float(truth.sum()), 1)\n        return ap\n'"
catalyst/tools/meters/aucmeter.py,7,"b'""""""\nThe AUCMeter measures the area under the receiver-operating characteristic\n(ROC) curve for binary classification problems. The area under the curve\n(AUC) can be interpreted as the probability that, given a randomly selected\npositive     example and a randomly selected negative example, the positive\nexample is assigned a higher score by the classification model than the\nnegative example.\n""""""\nimport numbers\n\nimport numpy as np\n\nimport torch\n\nfrom . import meter\n\n\nclass AUCMeter(meter.Meter):\n    """"""\n    The AUCMeter is designed to operate on one-dimensional Tensors `output`\n    and `target`, where:\n\n    1. The `output` contains model output scores that ought to be higher\n    when the model is more convinced that the example should be positively\n    labeled, and smaller when the model believes the example should be\n    negatively labeled (for instance, the output of a sigmoid function)\n\n    2. The `target` contains only values 0 (for\n    negative examples) and 1 (for positive examples).\n    """"""\n\n    def __init__(self):\n        """"""Constructor method for the ``AUCMeter`` class.""""""\n        super(AUCMeter, self).__init__()\n        self.reset()\n\n    def reset(self) -> None:\n        """"""Reset stored scores and targets.""""""\n        self.scores = torch.DoubleTensor(torch.DoubleStorage()).numpy()\n        self.targets = torch.LongTensor(torch.LongStorage()).numpy()\n\n    def add(self, output: torch.Tensor, target: torch.Tensor) -> None:\n        """"""Update stored scores and targets.\n\n        Args:\n            output (Tensor): one-dimensional tensor `output`\n            target (Tensor): one-dimensional tensor `target`\n        """"""\n        if torch.is_tensor(output):\n            output = output.cpu().squeeze().numpy()\n        if torch.is_tensor(target):\n            target = target.cpu().squeeze().numpy()\n        elif isinstance(target, numbers.Number):\n            target = np.asarray([target])\n        assert np.ndim(output) == 1, ""wrong output size (1D expected)""\n        assert np.ndim(target) == 1, ""wrong target size (1D expected)""\n        assert (\n            output.shape[0] == target.shape[0]\n        ), ""number of outputs and targets does not match""\n        assert np.all(\n            np.add(np.equal(target, 1), np.equal(target, 0))\n        ), ""targets should be binary (0, 1)""\n\n        self.scores = np.append(self.scores, output)\n        self.targets = np.append(self.targets, target)\n\n    def value(self):\n        """"""Return metric values of AUC, TPR and FPR.\n\n        Returns:\n             tuple of floats: (AUC, TPR, FPR)\n        """"""\n        # case when number of elements added are 0\n        if self.scores.shape[0] == 0:\n            return 0.5\n\n        # sorting the arrays\n        scores, sortind = torch.sort(\n            torch.from_numpy(self.scores), dim=0, descending=True\n        )\n        scores = scores.numpy()\n        sortind = sortind.numpy()\n\n        # creating the roc curve\n        tpr = np.zeros(shape=(scores.size + 1), dtype=np.float64)\n        fpr = np.zeros(shape=(scores.size + 1), dtype=np.float64)\n\n        for i in range(1, scores.size + 1):\n            if self.targets[sortind[i - 1]] == 1:\n                tpr[i] = tpr[i - 1] + 1\n                fpr[i] = fpr[i - 1]\n            else:\n                tpr[i] = tpr[i - 1]\n                fpr[i] = fpr[i - 1] + 1\n\n        tpr /= self.targets.sum() * 1.0\n        fpr /= (self.targets - 1.0).sum() * -1.0\n\n        # calculating area under curve using trapezoidal rule\n        n = tpr.shape[0]\n        h = fpr[1:n] - fpr[0 : n - 1]\n        sum_h = np.zeros(fpr.shape)\n        sum_h[0 : n - 1] = h\n        sum_h[1:n] += h\n        area = (sum_h * tpr).sum() / 2.0\n\n        return (area, tpr, fpr)\n'"
catalyst/tools/meters/averagevaluemeter.py,0,"b'""""""\nAverage value meter\n""""""\nimport numpy as np\n\nfrom . import meter\n\n\nclass AverageValueMeter(meter.Meter):\n    """"""\n    Average value meter stores mean and standard deviation\n    for population of input values.\n    Meter updates are applied online, one value for each update.\n    Values are not cached, only the last added.\n    """"""\n\n    def __init__(self):\n        """"""Constructor method for the ``AverageValueMeter`` class.""""""\n        super(AverageValueMeter, self).__init__()\n        self.n = 0\n        self.val = 0.0\n        self.mean = np.nan\n        self.mean_old = 0.0\n        self.m_s = 0.0\n        self.std = np.nan\n        self.n_samples = 0\n\n    def add(self, value, batch_size) -> None:\n        """"""Add a new observation.\n\n        Updates of mean and std are going online, with\n        `Welford\'s online algorithm\n        <https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance>`_.\n\n        Args:\n            value (float): value for update,\n                can be scalar number or PyTorch tensor\n\n        .. note::\n            Because of algorithm design,\n            you can update meter values with only one value a time.\n        """"""\n        self.val = value\n        self.n += 1\n        self.n_samples += batch_size\n\n        if self.n == 1:\n            self.mean = 0.0 + value  # Force a copy in torch/numpy\n            self.std = 0.0\n            self.mean_old = self.mean\n            self.m_s = 0.0\n        else:\n            self.mean = self.mean_old + (\n                value - self.mean_old\n            ) * batch_size / float(self.n_samples)\n            self.m_s += (\n                (value - self.mean_old) * (value - self.mean) * batch_size\n            )\n            self.mean_old = self.mean\n            self.std = np.sqrt(self.m_s / (self.n_samples - 1.0))\n\n    def value(self):\n        """"""Returns meter values.\n\n        Returns:\n            mean (float): Mean that has been updated online.\n            std (float): Standard deviation that has been updated online.\n        """"""\n        return self.mean, self.std\n\n    def reset(self):\n        """"""Resets the meter to default settings.""""""\n        self.n = 0\n        self.val = 0.0\n        self.mean = np.nan\n        self.mean_old = 0.0\n        self.m_s = 0.0\n        self.std = np.nan\n        self.n_samples = 0\n'"
catalyst/tools/meters/classerrormeter.py,3,"b'""""""\n\n""""""\nimport numbers\n\nimport numpy as np\n\nimport torch\n\nfrom . import meter\n\n\nclass ClassErrorMeter(meter.Meter):\n    """"""@TODO: Docs. Contribution is welcome.""""""\n\n    def __init__(self, topk=None, accuracy=False):\n        """"""Constructor method for the ``AverageValueMeter`` class.""""""\n        super(ClassErrorMeter, self).__init__()\n        self.topk = np.sort(topk) if topk is not None else [1]\n        self.accuracy = accuracy\n        self.reset()\n\n    def reset(self) -> None:\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        self.sum = {v: 0 for v in self.topk}\n        self.n = 0\n\n    def add(self, output, target) -> None:\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        if torch.is_tensor(output):\n            output = output.cpu().squeeze().numpy()\n        if torch.is_tensor(target):\n            target = np.atleast_1d(target.cpu().squeeze().numpy())\n        elif isinstance(target, numbers.Number):\n            target = np.asarray([target])\n        if np.ndim(output) == 1:\n            output = output[np.newaxis]\n        else:\n            assert (\n                np.ndim(output) == 2\n            ), ""wrong output size (1D or 2D expected)""\n            assert np.ndim(target) == 1, ""target and output do not match""\n        assert (\n            target.shape[0] == output.shape[0]\n        ), ""target and output do not match""\n        topk = self.topk\n        maxk = int(topk[-1])  # seems like Python3 wants int and not np.int64\n        no = output.shape[0]\n\n        pred = torch.from_numpy(output).topk(maxk, 1, True, True)[1].numpy()\n        correct = pred == target[:, np.newaxis].repeat(pred.shape[1], 1)\n\n        for k in topk:\n            self.sum[k] += no - correct[:, 0:k].sum()\n        self.n += no\n\n    def value(self, k=-1):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        if k != -1:\n            assert (\n                k in self.sum.keys()\n            ), ""invalid k (this k was not provided at construction time)""\n            if self.accuracy:\n                return (1.0 - float(self.sum[k]) / self.n) * 100.0\n            else:\n                return float(self.sum[k]) / self.n * 100.0\n        else:\n            return [self.value(k_) for k_ in self.topk]\n'"
catalyst/tools/meters/confusionmeter.py,1,"b'""""""\nMaintains a confusion matrix for a given classification problem.\n""""""\nimport numpy as np\n\nimport torch\n\nfrom . import meter\n\n\nclass ConfusionMeter(meter.Meter):\n    """"""\n    ConfusionMeter constructs a confusion matrix for a multi-class\n    classification problems. It does not support multi-label, multi-class\n    problems: for such problems, please use MultiLabelConfusionMeter.\n    """"""\n\n    def __init__(self, k: int, normalized: bool = False):\n        """"""\n        Args:\n            k (int): number of classes in the classification problem\n            normalized (boolean): determines whether or not the confusion\n                matrix is normalized or not\n        """"""\n        super(ConfusionMeter, self).__init__()\n        self.conf = np.ndarray((k, k), dtype=np.int32)\n        self.normalized = normalized\n        self.k = k\n        self.reset()\n\n    def reset(self) -> None:\n        """"""Reset confusion matrix, filling it with zeros.""""""\n        self.conf.fill(0)\n\n    def add(self, predicted: torch.Tensor, target: torch.Tensor) -> None:\n        """"""Computes the confusion matrix of ``K x K`` size\n        where ``K`` is no of classes.\n\n        Args:\n            predicted (tensor): Can be an N x K tensor of predicted scores\n                obtained from the model for N examples and K classes\n                or an N-tensor of integer values between 0 and K-1\n            target (tensor): Can be a N-tensor of integer values assumed\n                to be integer values between 0 and K-1 or N x K tensor, where\n                targets are assumed to be provided as one-hot vectors\n        """"""\n        predicted = predicted.cpu().numpy()\n        target = target.cpu().numpy()\n\n        assert (\n            predicted.shape[0] == target.shape[0]\n        ), ""number of targets and predicted outputs do not match""\n\n        if np.ndim(predicted) != 1:\n            assert (\n                predicted.shape[1] == self.k\n            ), ""number of predictions does not match size of confusion matrix""\n            predicted = np.argmax(predicted, 1)\n        else:\n            assert (predicted.max() < self.k) and (\n                predicted.min() >= 0\n            ), ""predicted values are not between 1 and k""\n\n        onehot_target = np.ndim(target) != 1\n        if onehot_target:\n            assert (\n                target.shape[1] == self.k\n            ), ""Onehot target does not match size of confusion matrix""\n            assert (target >= 0).all() and (\n                target <= 1\n            ).all(), ""in one-hot encoding, target values should be 0 or 1""\n            assert (\n                target.sum(1) == 1\n            ).all(), ""multi-label setting is not supported""\n            target = np.argmax(target, 1)\n        else:\n            assert (predicted.max() < self.k) and (\n                predicted.min() >= 0\n            ), ""predicted values are not between 0 and k-1""\n\n        # hack for bincounting 2 arrays together\n        x = predicted + self.k * target\n        bincount_2d = np.bincount(x.astype(np.int32), minlength=self.k ** 2)\n        assert bincount_2d.size == self.k ** 2\n        conf = bincount_2d.reshape((self.k, self.k))\n\n        self.conf += conf\n\n    def value(self):\n        """"""\n        Returns:\n            Confusion matrix of K rows and K columns, where rows corresponds\n            to ground-truth targets and columns corresponds to predicted\n            targets.\n        """"""\n        if self.normalized:\n            conf = self.conf.astype(np.float32)\n            return conf / conf.sum(1).clip(min=1e-12)[:, None]\n        else:\n            return self.conf\n'"
catalyst/tools/meters/mapmeter.py,3,"b'""""""\nThe mAP meter measures the mean average precision over all classes.\n""""""\nfrom typing import Optional\n\nimport torch\n\nfrom . import APMeter, meter\n\n\nclass mAPMeter(meter.Meter):\n    """"""\n    This meter is a wrapper for\n    :py:class:`catalyst.tools.meters.apmeter.APMeter`.\n    The mAPMeter is designed to operate on `NxK` Tensors `output` and\n    `target`, and optionally a `Nx1` Tensor weight where:\n\n    1. The `output` contains model output scores for `N` examples and `K`\n    classes that ought to be higher when the model is more convinced that\n    the example should be positively labeled, and smaller when the model\n    believes the example should be negatively labeled\n    (for instance, the output of a sigmoid function)\n\n    2. The `target` contains only values 0 (for negative examples) and 1\n    (for positive examples)\n\n    3. The `weight` ( > 0) represents weight\n    for each sample.\n    """"""\n\n    def __init__(self):\n        """"""Constructor method for the ``mAPMeter`` class.""""""\n        super(mAPMeter, self).__init__()\n        self.apmeter = APMeter()\n\n    def reset(self) -> None:\n        """"""Reset `self.apmeter`.""""""\n        self.apmeter.reset()\n\n    def add(\n        self,\n        output: torch.Tensor,\n        target: torch.Tensor,\n        weight: Optional[torch.Tensor] = None,\n    ) -> None:\n        """"""Update `self.apmeter`.\n\n        Args:\n            output (Tensor): Model output scores as `NxK` tensor\n            target (Tensor): Target scores as `NxK` tensor\n            weight (Tensor, optional): Weight values for each sample\n                as `Nx1` Tensor\n        """"""\n        self.apmeter.add(output, target, weight)\n\n    def value(self):\n        """"""Returns mean of `self.apmeter` value.\n\n        Return:\n            FloatTensor: mAP scalar tensor\n        """"""\n        return self.apmeter.value().mean()\n'"
catalyst/tools/meters/meter.py,0,"b'""""""\nMeters provide a way to keep track of important statistics in an online manner.\n""""""\n\n\nclass Meter(object):\n    """"""\n    This class is abstract, but provides a standard interface for all meters to\n    follow.\n    """"""\n\n    def reset(self):\n        """"""Resets the meter to default settings.""""""\n        pass\n\n    def add(self, value):\n        """"""Log a new value to the meter.\n\n        Args:\n            value: Next result to include.\n\n        """"""\n        pass\n\n    def value(self):\n        """"""Get the value of the meter in the current state.""""""\n        pass\n'"
catalyst/tools/meters/movingaveragevaluemeter.py,1,"b'""""""\nMoving average meter calculates average for moving window of values.\n""""""\nimport math\n\nimport torch\n\nfrom . import meter\n\n\nclass MovingAverageValueMeter(meter.Meter):\n    """"""\n    MovingAverageValueMeter stores mean and standard deviation\n    for population of array that is handled like a queue during updates.\n    Queue(window) is filled with zeros from the start by default.\n    Meter updates are applied online, one value for each update.\n    Meter values are moving average and moving standard deviation.\n    """"""\n\n    def __init__(self, windowsize):\n        """"""\n        Args:\n            windowsize (int): size of window of values, which is continuous\n                and ends on last updated element\n        """"""\n        super(MovingAverageValueMeter, self).__init__()\n        self.windowsize = windowsize\n        self.valuequeue = torch.Tensor(windowsize)\n        self.reset()\n\n    def reset(self) -> None:\n        """"""\n        Reset sum, number of elements, moving variance and zero out window.\n        """"""\n        self.sum = 0.0\n        self.n = 0\n        self.var = 0.0\n        self.valuequeue.fill_(0)\n\n    def add(self, value: float) -> None:\n        """"""Adds observation sample.\n\n        Args:\n            value (float): scalar\n        """"""\n        queueid = self.n % self.windowsize\n        oldvalue = self.valuequeue[queueid]\n        self.sum += value - oldvalue\n        self.var += value * value - oldvalue * oldvalue\n        self.valuequeue[queueid] = value\n        self.n += 1\n\n    def value(self):\n        """"""Return mean and standard deviation of window.\n\n        Returns:\n            tuple of floats: (window mean, window std)\n        """"""\n        n = min(self.n, self.windowsize)\n        mean = self.sum / max(1, n)\n        std = math.sqrt(max((self.var - n * mean * mean) / max(1, n - 1), 0))\n        return mean, std\n'"
catalyst/tools/meters/msemeter.py,5,"b'""""""\nMSE and RMSE meters.\n""""""\nimport math\n\nimport torch\n\nfrom . import meter\n\n\nclass MSEMeter(meter.Meter):\n    """"""\n    This meter can handle MSE and RMSE.\n    Root calculation can be toggled(not calculated by default).\n    """"""\n\n    def __init__(self, root: bool = False):\n        """"""\n        Args:\n            root (bool): Toggle between calculation of\n                RMSE (True) and MSE (False)\n        """"""\n        super(MSEMeter, self).__init__()\n        self.reset()\n        self.root = root\n\n    def reset(self) -> None:\n        """"""Reset meter number of elements and squared error sum.""""""\n        self.n = 0\n        self.sesum = 0.0\n\n    def add(self, output: torch.Tensor, target: torch.Tensor) -> None:\n        """"""Update squared error stored sum and number of elements.\n\n        Args:\n            output (Tensor): Model output tensor or numpy array\n            target (Tensor): Target tensor or numpy array\n        """"""\n        if not torch.is_tensor(output) and not torch.is_tensor(target):\n            output = torch.from_numpy(output)\n            target = torch.from_numpy(target)\n        self.n += output.numel()\n        self.sesum += torch.sum((output - target) ** 2)\n\n    def value(self) -> float:\n        """"""Calculate MSE and return RMSE or MSE.\n\n        Returns:\n            float: Root of MSE if `self.root` is True else MSE\n        """"""\n        mse = self.sesum / max(1, self.n)\n        return math.sqrt(mse) if self.root else mse\n'"
catalyst/tools/meters/ppv_tpr_f1_meter.py,6,"b'""""""\nIn this module **precision**, **recall** and **F1 score**\ncalculations are defined in separate functions.\n\n:py:class:`PrecisionRecallF1ScoreMeter` can keep track for all three of these.\n""""""\nfrom collections import defaultdict\n\nimport torch\n\nfrom . import meter\n\n\ndef f1score(precision_value, recall_value, eps=1e-5):\n    """"""\n    Calculating F1-score from precision and recall to reduce computation\n    redundancy.\n\n    Args:\n        precision_value: precision (0-1)\n        recall_value: recall (0-1)\n\n    Returns:\n        F1 score (0-1)\n    """"""\n    numerator = 2 * (precision_value * recall_value)\n    denominator = precision_value + recall_value + eps\n    return numerator / denominator\n\n\ndef precision(tp, fp, eps: float = 1e-5) -> float:\n    """"""\n    Calculates precision (a.k.a. positive predictive value) for binary\n    classification and segmentation.\n\n    Args:\n        tp (int): number of true positives\n        fp (int): number of false positives\n\n    Returns:\n        precision value (0-1)\n    """"""\n    # originally precision is: ppv = tp / (tp + fp + eps)\n    # but when both masks are empty this gives: tp=0 and fp=0 => ppv=0\n    # so here precision is defined as ppv := 1 - fdr (false discovery rate)\n    return 1 - fp / (tp + fp + eps)\n\n\ndef recall(tp, fn, eps=1e-5) -> float:\n    """"""\n    Calculates recall (a.k.a. true positive rate) for binary classification and\n    segmentation.\n\n    Args:\n        tp: number of true positives\n        fn: number of false negatives\n\n    Returns:\n        recall value (0-1)\n    """"""\n    # originally reacall is: tpr := tp / (tp + fn + eps)\n    # but when both masks are empty this gives: tp=0 and fn=0 => tpr=0\n    # so here recall is defined as tpr := 1 - fnr (false negative rate)\n    return 1 - fn / (fn + tp + eps)\n\n\nclass PrecisionRecallF1ScoreMeter(meter.Meter):\n    """"""\n    Keeps track of global true positives, false positives, and false negatives\n    for each epoch and calculates precision, recall, and F1-score based on\n    those metrics. Currently, this meter works for binary cases only, please\n    use multiple instances of this class for multi-label cases.\n    """"""\n\n    def __init__(self, threshold=0.5):\n        """"""\n        Constructor method for the `` PrecisionRecallF1ScoreMeter`` class.\n        """"""\n        super(PrecisionRecallF1ScoreMeter, self).__init__()\n        self.threshold = threshold\n        self.reset()\n\n    def reset(self) -> None:\n        """"""\n        Resets true positive, false positive and false negative counts to 0.\n        """"""\n        self.tp_fp_fn_counts = defaultdict(int)\n\n    def add(self, output: torch.Tensor, target: torch.Tensor) -> None:\n        """"""\n        Thresholds predictions and calculates the true positives,\n        false positives, and false negatives in comparison to the target.\n\n        Args:\n            output (torch.Tensor): prediction after activation function\n                shape should be (batch_size, ...), but works with any shape\n            target (torch.Tensor): label (binary),\n                shape should be the same as output\'s shape\n        """"""\n        output = (output > self.threshold).float()\n\n        tp = torch.sum(target * output)\n        fp = torch.sum(output) - tp\n        fn = torch.sum(target) - tp\n\n        self.tp_fp_fn_counts[""tp""] += tp\n        self.tp_fp_fn_counts[""fp""] += fp\n        self.tp_fp_fn_counts[""fn""] += fn\n\n    def value(self):\n        """"""\n        Calculates precision/recall/f1 based on the current stored\n        tp/fp/fn counts.\n\n        Returns:\n            tuple of floats: (precision, recall, f1)\n        """"""\n        precision_value = precision(\n            self.tp_fp_fn_counts[""tp""], self.tp_fp_fn_counts[""fp""]\n        )\n        recall_value = recall(\n            self.tp_fp_fn_counts[""tp""], self.tp_fp_fn_counts[""fn""]\n        )\n        f1_value = f1score(precision_value, recall_value)\n        return (float(precision_value), float(recall_value), float(f1_value))\n'"
catalyst/utils/criterion/__init__.py,0,"b'# flake8: noqa\n# backward compatibility\n""""""\nCriterion utils are the same as :py:mod:`catalyst.utils.metrics` no difference between them for now.\n""""""\nfrom catalyst.utils.metrics import *\n'"
catalyst/utils/metrics/__init__.py,0,"b'# flake8: noqa\nfrom .accuracy import accuracy, average_accuracy, mean_average_accuracy\nfrom .dice import dice\nfrom .f1_score import f1_score\nfrom .focal import reduced_focal_loss, sigmoid_focal_loss\nfrom .iou import iou, jaccard\n'"
catalyst/utils/metrics/accuracy.py,0,"b'""""""\nVarious accuracy metrics:\n    * :func:`accuracy`\n    * :func:`average_accuracy`\n    * :func:`mean_average_accuracy`\n""""""\nimport numpy as np\n\nfrom catalyst.utils.torch import get_activation_fn\n\n\ndef accuracy(\n    outputs,\n    targets,\n    topk=(1,),\n    threshold: float = None,\n    activation: str = None,\n):\n    """"""\n    Computes the accuracy.\n\n    It can be used either for:\n\n    1. Multi-class task, in this case:\n\n      - you can use topk.\n      - threshold and activation are not required.\n      - targets is a tensor: batch_size\n      - outputs is a tensor: batch_size x num_classes\n      - computes the accuracy@k for the specified values of k.\n\n    2. Multi-label task, in this case:\n\n      - you must specify threshold and activation\n      - topk will not be used\n        (because of there is no method to apply top-k in\n        multi-label classification).\n      - outputs, targets are tensors with shape: batch_size x num_classes\n      - targets is a tensor with binary vectors\n    """"""\n    activation_fn = get_activation_fn(activation)\n    outputs = activation_fn(outputs)\n\n    if threshold:\n        outputs = (outputs > threshold).long()\n\n    # multi-label classification\n    if len(targets.shape) > 1 and targets.size(1) > 1:\n        res = (targets.long() == outputs.long()).sum().float() / np.prod(\n            targets.shape\n        )\n        return [res]\n\n    max_k = max(topk)\n    batch_size = targets.size(0)\n\n    if len(outputs.shape) == 1 or outputs.shape[1] == 1:\n        pred = outputs.t()\n    else:\n        _, pred = outputs.topk(max_k, 1, True, True)\n        pred = pred.t()\n    correct = pred.eq(targets.long().view(1, -1).expand_as(pred))\n\n    res = []\n    for k in topk:\n        correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n        res.append(correct_k.mul_(1.0 / batch_size))\n    return res\n\n\ndef average_accuracy(outputs, targets, k=10):\n    """"""Computes the average accuracy at k.\n\n    This function computes the average\n    accuracy at k between two lists of items.\n\n    Args:\n        outputs (list): A list of predicted elements\n        targets (list):  A list of elements that are to be predicted\n        k (int, optional): The maximum number of predicted elements\n\n    Returns:\n        double: The average accuracy at k over the input lists\n    """"""\n    if len(outputs) > k:\n        outputs = outputs[:k]\n\n    score = 0.0\n    num_hits = 0.0\n\n    for i, predict in enumerate(outputs):\n        if predict in targets and predict not in outputs[:i]:\n            num_hits += 1.0\n            score += num_hits / (i + 1.0)\n\n    if not targets:\n        return 0.0\n\n    return score / min(len(targets), k)\n\n\ndef mean_average_accuracy(outputs, targets, topk=(1,)):\n    """"""Computes the mean average accuracy at k.\n\n    This function computes the mean average accuracy at k between two lists\n    of lists of items.\n\n    Args:\n        outputs (list): A list of lists of predicted elements\n        targets (list): A list of lists of elements that are to be predicted\n        topk (int, optional): The maximum number of predicted elements\n\n    Returns:\n        double: The mean average accuracy at k over the input lists\n    """"""\n    max_k = max(topk)\n    _, pred = outputs.topk(max_k, 1, True, True)\n\n    targets = targets.data.cpu().numpy().tolist()\n    actual_list = []\n    for a in targets:\n        actual_list.append([a])\n    targets = actual_list\n    pred = pred.tolist()\n\n    res = []\n    for k in topk:\n        ap = np.mean(\n            [average_accuracy(p, a, k) for a, p in zip(targets, pred)]\n        )\n        res.append(ap)\n    return res\n\n\n__all__ = [""accuracy"", ""average_accuracy"", ""mean_average_accuracy""]\n'"
catalyst/utils/metrics/dice.py,5,"b'""""""\nDice metric.\n""""""\nimport torch\n\nfrom catalyst.utils.torch import get_activation_fn\n\n\ndef dice(\n    outputs: torch.Tensor,\n    targets: torch.Tensor,\n    eps: float = 1e-7,\n    threshold: float = None,\n    activation: str = ""Sigmoid"",\n):\n    """"""Computes the dice metric.\n\n    Args:\n        outputs (list):  a list of predicted elements\n        targets (list): a list of elements that are to be predicted\n        eps (float): epsilon\n        threshold (float): threshold for outputs binarization\n        activation (str): An torch.nn activation applied to the outputs.\n            Must be one of [""none"", ""Sigmoid"", ""Softmax2d""]\n\n    Returns:\n        double:  Dice score\n    """"""\n    activation_fn = get_activation_fn(activation)\n    outputs = activation_fn(outputs)\n\n    if threshold is not None:\n        outputs = (outputs > threshold).float()\n\n    intersection = torch.sum(targets * outputs)\n    union = torch.sum(targets) + torch.sum(outputs)\n    # this looks a bit awkward but `eps * (union == 0)` term\n    # makes sure that if I and U are both 0, than Dice == 1\n    # and if U != 0 and I == 0 the eps term in numerator is zeroed out\n    # i.e. (0 + eps) / (U - 0 + eps) doesn\'t happen\n    dice = (2 * intersection + eps * (union == 0)) / (union + eps)\n\n    return dice\n\n\n__all__ = [""dice""]\n'"
catalyst/utils/metrics/f1_score.py,8,"b'""""""\nF1 score.\n""""""\nimport torch\n\nfrom catalyst.utils.torch import get_activation_fn\n\n\ndef f1_score(\n    outputs: torch.Tensor,\n    targets: torch.Tensor,\n    beta: float = 1.0,\n    eps: float = 1e-7,\n    threshold: float = None,\n    activation: str = ""Sigmoid"",\n):\n    """"""\n    Args:\n        outputs (torch.Tensor): A list of predicted elements\n        targets (torch.Tensor):  A list of elements that are to be predicted\n        eps (float): epsilon to avoid zero division\n        beta (float): beta param for f_score\n        threshold (float): threshold for outputs binarization\n        activation (str): An torch.nn activation applied to the outputs.\n            Must be one of [""none"", ""Sigmoid"", ""Softmax2d""]\n\n    Returns:\n        float: F_1 score\n    """"""\n    activation_fn = get_activation_fn(activation)\n\n    outputs = activation_fn(outputs)\n\n    if threshold is not None:\n        outputs = (outputs > threshold).float()\n\n    true_positive = torch.sum(targets * outputs)\n    false_positive = torch.sum(outputs) - true_positive\n    false_negative = torch.sum(targets) - true_positive\n\n    precision_plus_recall = (\n        (1 + beta ** 2) * true_positive\n        + beta ** 2 * false_negative\n        + false_positive\n        + eps\n    )\n\n    score = ((1 + beta ** 2) * true_positive + eps) / precision_plus_recall\n\n    return score\n\n\n__all__ = [""f1_score""]\n'"
catalyst/utils/metrics/focal.py,7,"b'""""""\nFocal losses:\n    * :func:`sigmoid_focal_loss`\n    * :func:`reduced_focal_loss`\n""""""\n\nimport torch\nimport torch.nn.functional as F\n\n\ndef sigmoid_focal_loss(\n    outputs: torch.Tensor,\n    targets: torch.Tensor,\n    gamma: float = 2.0,\n    alpha: float = 0.25,\n    reduction: str = ""mean"",\n):\n    """"""\n    Compute binary focal loss between target and output logits.\n\n    Args:\n        outputs: tensor of arbitrary shape\n        targets: tensor of the same shape as input\n        reduction (string, optional):\n            specifies the reduction to apply to the output:\n            ``""none""`` | ``""mean""`` | ``""sum""`` | ``""batchwise_mean""``.\n            ``""none""``: no reduction will be applied,\n            ``""mean""``: the sum of the output will be divided by the number of\n            elements in the output,\n            ``""sum""``: the output will be summed.\n\n    Source: https://github.com/BloodAxe/pytorch-toolbelt\n    """"""\n    targets = targets.type(outputs.type())\n\n    logpt = -F.binary_cross_entropy_with_logits(\n        outputs, targets, reduction=""none""\n    )\n    pt = torch.exp(logpt)\n\n    # compute the loss\n    loss = -((1 - pt).pow(gamma)) * logpt\n\n    if alpha is not None:\n        loss = loss * (alpha * targets + (1 - alpha) * (1 - targets))\n\n    if reduction == ""mean"":\n        loss = loss.mean()\n    if reduction == ""sum"":\n        loss = loss.sum()\n    if reduction == ""batchwise_mean"":\n        loss = loss.sum(0)\n\n    return loss\n\n\ndef reduced_focal_loss(\n    outputs: torch.Tensor,\n    targets: torch.Tensor,\n    threshold: float = 0.5,\n    gamma: float = 2.0,\n    reduction=""mean"",\n):\n    """"""Compute reduced focal loss between target and output logits.\n\n    It has been proposed in `Reduced Focal Loss\\: 1st Place Solution to xView\n    object detection in Satellite Imagery`_ paper.\n\n    Args:\n        outputs: tensor of arbitrary shape\n        targets: tensor of the same shape as input\n        reduction (string, optional):\n            specifies the reduction to apply to the output:\n            ``""none""`` | ``""mean""`` | ``""sum""`` | ``""batchwise_mean""``.\n            ``""none""``: no reduction will be applied,\n            ``""mean""``: the sum of the output will be divided by the number of\n            elements in the output,\n            ``""sum""``: the output will be summed.\n            ``""batchwise_mean""`` computes mean loss per sample in batch.\n            Default: ""mean""\n\n    .. note::\n        ``size_average`` and ``reduce`` params are in the process of being\n        deprecated, and in the meantime, specifying either of those two args\n        will override ``reduction``.\n\n    Source: https://github.com/BloodAxe/pytorch-toolbelt\n\n    .. _Reduced Focal Loss\\: 1st Place Solution to xView object detection\n        in Satellite Imagery: https://arxiv.org/abs/1903.01347\n    """"""\n    targets = targets.type(outputs.type())\n\n    logpt = -F.binary_cross_entropy_with_logits(\n        outputs, targets, reduction=""none""\n    )\n    pt = torch.exp(logpt)\n\n    # compute the loss\n    focal_reduction = ((1.0 - pt) / threshold).pow(gamma)\n    focal_reduction[pt < threshold] = 1\n\n    loss = -focal_reduction * logpt\n\n    if reduction == ""mean"":\n        loss = loss.mean()\n    if reduction == ""sum"":\n        loss = loss.sum()\n    if reduction == ""batchwise_mean"":\n        loss = loss.sum(0)\n\n    return loss\n\n\n__all__ = [""sigmoid_focal_loss"", ""reduced_focal_loss""]\n'"
catalyst/utils/metrics/iou.py,7,"b'""""""\nIoU metric. Jaccard metric refers to IoU here, same functionality.\n""""""\n\nfrom typing import List, Union\nfrom functools import partial\n\nimport torch\n\nfrom catalyst.utils.torch import get_activation_fn\n\n\ndef iou(\n    outputs: torch.Tensor,\n    targets: torch.Tensor,\n    # values are discarded, only None check\n    # used for compatibility with MultiMetricCallback\n    classes: List[str] = None,\n    eps: float = 1e-7,\n    threshold: float = None,\n    activation: str = ""Sigmoid"",\n) -> Union[float, List[float]]:\n    """"""\n    Args:\n        outputs (torch.Tensor): A list of predicted elements\n        targets (torch.Tensor):  A list of elements that are to be predicted\n        eps (float): epsilon to avoid zero division\n        threshold (float): threshold for outputs binarization\n        activation (str): An torch.nn activation applied to the outputs.\n            Must be one of [""none"", ""Sigmoid"", ""Softmax2d""]\n\n    Returns:\n        Union[float, List[float]]: IoU (Jaccard) score(s)\n    """"""\n    activation_fn = get_activation_fn(activation)\n    outputs = activation_fn(outputs)\n\n    if threshold is not None:\n        outputs = (outputs > threshold).float()\n\n    # ! fix backward compatibility\n    if classes is not None:\n        # if classes are specified we reduce across all dims except channels\n        _sum = partial(torch.sum, dim=[0, 2, 3])\n    else:\n        _sum = torch.sum\n\n    intersection = _sum(targets * outputs)\n    union = _sum(targets) + _sum(outputs)\n    # this looks a bit awkward but `eps * (union == 0)` term\n    # makes sure that if I and U are both 0, than IoU == 1\n    # and if U != 0 and I == 0 the eps term in numerator is zeroed out\n    # i.e. (0 + eps) / (U - 0 + eps) doesn\'t happen\n    iou = (intersection + eps * (union == 0)) / (union - intersection + eps)\n\n    return iou\n\n\njaccard = iou\n\n__all__ = [""iou"", ""jaccard""]\n'"
catalyst/utils/tests/__init__.py,0,b''
catalyst/utils/tests/registery_foo.py,0,"b'def foo(a, b):\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    return {""a"": a, ""b"": b}\n\n\ndef bar():\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    pass\n\n\n__all__ = [""foo""]\n'"
catalyst/utils/tests/test_config.py,0,"b'# flake8: noqa\nimport argparse\nimport io\nimport json\n\nimport numpy as np\n\nfrom catalyst import utils\nfrom catalyst.utils import config\n\n\ndef test_parse_config_args():\n    configuration = {\n        ""stages"": {""one"": ""uno"", ""two"": ""dos"", ""three"": ""tres""},\n        ""key"": {""value"": ""key2""},\n    }\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--command"")\n\n    args, uargs = parser.parse_known_args(\n        [\n            ""--command"",\n            ""run"",\n            ""--path=test.yml:str"",\n            ""--stages/zero=cero:str"",\n            ""-C=like:str"",\n        ]\n    )\n\n    configuration, args = utils.parse_config_args(\n        config=configuration, args=args, unknown_args=uargs\n    )\n\n    assert args.command == ""run""\n    assert args.path == ""test.yml""\n    assert configuration.get(""stages"") is not None\n    assert ""zero"" in configuration[""stages""]\n    assert configuration[""stages""][""zero""] == ""cero""\n    assert configuration.get(""args"") is not None\n    assert configuration[""args""][""path""] == ""test.yml""\n    assert configuration[""args""][""C""] == ""like""\n    assert configuration[""args""][""command""] == ""run""\n\n    for key, value in args._get_kwargs():\n        v = configuration[""args""].get(key)\n        assert v is not None\n        assert v == value\n\n\ndef test_parse_numbers():\n    configuration = {\n        ""a"": 1,\n        ""b"": 20,\n        ""c"": 303e5,\n        ""d"": -4,\n        ""e"": -50,\n        ""f"": -666e7,\n        ""g"": 0.35,\n        ""h"": 7.35e-5,\n        ""k"": 8e-10,\n    }\n\n    buffer = io.StringIO()\n    json.dump(configuration, buffer)\n    buffer.seek(0)\n    yaml_config = config._load_ordered_yaml(buffer)\n\n    for key, item in configuration.items():\n        assert np.isclose(yaml_config[key], item)\n'"
catalyst/utils/tests/test_hash.py,0,"b'from ..hash import get_hash\n\n\ndef test_hash():\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    a = get_hash({""a"": ""foo""})\n    print(a)\n'"
catalyst/utils/tests/test_registry.py,0,"b'import pytest\n\nfrom catalyst.tools.registry import Registry, RegistryException\n\nfrom . import registery_foo as module\nfrom .registery_foo import foo\n\n\ndef test_add_function():\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    r = Registry("""")\n\n    r.add(foo)\n\n    assert ""foo"" in r._factories\n\n\ndef test_add_function_name_override():\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    r = Registry("""")\n\n    r.add(foo, name=""bar"")\n\n    assert ""bar"" in r._factories\n\n\ndef test_add_lambda_fail():\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    r = Registry("""")\n\n    with pytest.raises(RegistryException):\n        r.add(lambda x: x)\n\n\ndef test_add_lambda_override():\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    r = Registry("""")\n\n    r.add(lambda x: x, name=""bar"")\n\n    assert ""bar"" in r._factories\n\n\ndef test_fail_multiple_with_name():\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    r = Registry("""")\n\n    with pytest.raises(RegistryException):\n        r.add(foo, foo, name=""bar"")\n\n\ndef test_fail_double_add_different():\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    r = Registry("""")\n    r.add(foo)\n\n    with pytest.raises(RegistryException):\n\n        def bar():\n            pass\n\n        r.add(foo=bar)\n\n\ndef test_double_add_same_nofail():\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    r = Registry("""")\n    r.add(foo)\n    # It\'s ok to add same twice, forced by python relative import\n    # implementation\n    # https://github.com/catalyst-team/catalyst/issues/135\n    r.add(foo)\n\n\ndef test_instantiations():\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    r = Registry("""")\n\n    r.add(foo)\n\n    res = r.get_instance(""foo"", 1, 2)\n    assert res == {""a"": 1, ""b"": 2}\n\n    res = r.get_instance(""foo"", 1, b=2)\n    assert res == {""a"": 1, ""b"": 2}\n\n    res = r.get_instance(""foo"", a=1, b=2)\n    assert res == {""a"": 1, ""b"": 2}\n\n\ndef test_from_config():\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    r = Registry(""obj"")\n\n    r.add(foo)\n\n    res = r.get_from_params(**{""obj"": ""foo"", ""a"": 1, ""b"": 2})\n    assert res == {""a"": 1, ""b"": 2}\n\n    res = r.get_from_params(**{})\n    assert res is None\n\n\ndef test_meta_factory():\n    """"""@TODO: Docs. Contribution is welcome.""""""  # noqa: D202\n\n    def meta_1(fn, args, kwargs):\n        return fn\n\n    def meta_2(fn, args, kwargs):\n        return 1\n\n    r = Registry(""obj"", meta_1)\n    r.add(foo)\n\n    res = r.get_from_params(**{""obj"": ""foo""})\n    assert res == foo\n\n    res = r.get_from_params(**{""obj"": ""foo""}, meta_factory=meta_2)\n    assert res == 1\n\n\ndef test_fail_instantiation():\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    r = Registry("""")\n\n    r.add(foo)\n\n    with pytest.raises(RegistryException) as e_ifo:\n        r.get_instance(""foo"", c=1)\n\n    assert hasattr(e_ifo.value, ""__cause__"")\n\n\ndef test_decorator():\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    r = Registry("""")\n\n    @r.add\n    def bar():\n        pass\n\n    r.get(""bar"")\n\n\ndef test_kwargs():\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    r = Registry("""")\n\n    r.add(bar=foo)\n\n    r.get(""bar"")\n\n\ndef test_add_module():\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    r = Registry("""")\n\n    r.add_from_module(module)\n\n    r.get(""foo"")\n\n    with pytest.raises(RegistryException):\n        r.get_instance(""bar"")\n'"
catalyst/utils/tests/test_torch.py,5,"b'from typing import Dict\n\nimport torch\nfrom torch import nn\n\nfrom .. import torch as torch_utils\n\n\ndef test_network_output():\n    """"""Test for ``catalyst.utils.torch.get_network_output``.""""""\n    # case #1 - test net with one input variable\n    net = nn.Identity()\n    assert torch_utils.get_network_output(net, (1, 20)).shape == (1, 1, 20)\n\n    net = nn.Linear(20, 10)\n    assert torch_utils.get_network_output(net, (1, 20)).shape == (1, 1, 10)\n\n    # case #2 - test net with several input variables\n    class Net(nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.net1 = nn.Linear(20, 10)\n            self.net2 = nn.Linear(10, 5)\n\n        def forward(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n            z = torch.cat((self.net1(x), self.net2(y)), dim=-1)\n            return z\n\n    net = Net()\n    assert torch_utils.get_network_output(net, (20,), (10,)).shape == (1, 15)\n\n    # case #3 - test net with key-value input\n    class Net(nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.net = nn.Linear(20, 10)\n\n        def forward(self, *, x: torch.Tensor = None) -> torch.Tensor:\n            y = self.net(x)\n            return y\n\n    net = Net()\n    input_shapes = {""x"": (20,)}\n    assert torch_utils.get_network_output(net, **input_shapes).shape == (1, 10)\n\n    # case #4 - test net with dict of variables input\n    class Net(nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.net = nn.Linear(20, 10)\n\n        def forward(self, x: Dict[str, torch.Tensor]) -> torch.Tensor:\n            y = self.net(x[""x""])\n            return y\n\n    net = Net()\n    input_shapes = {""x"": (20,)}\n    assert torch_utils.get_network_output(net, input_shapes).shape == (1, 10)\n'"
examples/_empty/src/__init__.py,0,b'# flake8: noqa\nfrom catalyst.dl import SupervisedRunner as Runner\n\nfrom .experiment import Experiment\nfrom .model import Model\n'
examples/_empty/src/dataset.py,1,"b'from torch.utils.data import Dataset\n\n\nclass SomeDataset(Dataset):\n    """"""Class representing a `Dataset`.""""""\n\n    def __init__(self):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        pass\n\n    def __getitem__(self, index: int):\n        """"""Fetch a data sample for a given index.\n\n        Args:\n            index (int): index of the element in the dataset\n\n        Returns:\n            Single element by index\n        """"""\n        raise NotImplementedError\n\n    def __len__(self) -> int:\n        """"""\n        Returns:\n            int: length of the dataset\n        """"""\n        pass\n'"
examples/_empty/src/experiment.py,0,"b'# flake8: noqa\nfrom collections import OrderedDict\n\nfrom torchvision import transforms\n\nfrom catalyst.dl import ConfigExperiment\n\nfrom .dataset import SomeDataset\n\n\nclass Experiment(ConfigExperiment):\n    @staticmethod\n    def get_transforms(stage: str = None, mode: str = None):\n\n        # CHANGE ME\n        result = transforms.Compose(\n            [\n                transforms.ToTensor(),\n                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n            ]\n        )\n\n        return result\n\n    def get_datasets(\n        self, stage: str, batch_size: int, num_workers: int, **kwargs\n    ):\n        datasets = OrderedDict()\n\n        # CHANGE TO YOUR DATASET\n        trainset = SomeDataset()\n\n        # CHANGE TO YOUR DATASET\n        validset = SomeDataset()\n\n        datasets[""train""] = trainset\n        datasets[""valid""] = validset\n\n        return datasets\n'"
examples/_empty/src/model.py,1,"b'import torch\n\nfrom catalyst.contrib import registry\n\n\n@registry.Model\nclass Model(torch.nn.Module):\n    """"""\n    @TODO: Docs. Contribution is welcome\n    """"""\n\n    def __init__(self, **kwargs):\n        """"""\n        @TODO: Docs. Contribution is welcome\n        """"""\n        super().__init__()\n\n    def forward(self, x):\n        """"""\n        @TODO: Docs. Contribution is welcome\n        """"""\n        # CHANGE ME\n        return x\n\n    @classmethod\n    def get_from_params(cls, **model_params,) -> ""Model"":\n        """"""\n        @TODO: Docs. Contribution is welcome\n        """"""\n        # CHANGE ME\n        model = cls(**model_params)\n        return model\n'"
examples/cifar_simple/experiments/__init__.py,0,b'# flake8: noqa\n\nfrom .simple_experiment import SimpleExperiment\n'
examples/cifar_simple/experiments/simple_experiment.py,0,"b'from collections import OrderedDict\n\nimport torchvision\nfrom torchvision import transforms\n\nfrom catalyst.dl import ConfigExperiment\n\n\nclass SimpleExperiment(ConfigExperiment):\n    """"""\n    @TODO: Docs. Contribution is welcome\n    """"""\n\n    @staticmethod\n    def get_transforms(stage: str = None, mode: str = None):\n        """"""\n        @TODO: Docs. Contribution is welcome\n        """"""\n        return transforms.Compose(\n            [\n                transforms.ToTensor(),\n                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n            ]\n        )\n\n    def get_datasets(self, stage: str, **kwargs):\n        """"""\n        @TODO: Docs. Contribution is welcome\n        """"""\n        datasets = OrderedDict()\n\n        trainset = torchvision.datasets.CIFAR10(\n            root=""./data"",\n            train=True,\n            download=True,\n            transform=SimpleExperiment.get_transforms(\n                stage=stage, mode=""train""\n            ),\n        )\n        testset = torchvision.datasets.CIFAR10(\n            root=""./data"",\n            train=False,\n            download=True,\n            transform=SimpleExperiment.get_transforms(\n                stage=stage, mode=""valid""\n            ),\n        )\n\n        datasets[""train""] = trainset\n        datasets[""valid""] = testset\n\n        return datasets\n'"
tests/_tests_cv_classification_experiment_registry/test1/__init__.py,0,"b'# flake8: noqa\n\nfrom catalyst.dl import registry, SupervisedRunner as Runner\n\nfrom .experiment import SimpleExperiment\nfrom .model import SimpleNet\n\nregistry.Model(SimpleNet)\nregistry.Experiment(SimpleExperiment)\n'"
tests/_tests_cv_classification_experiment_registry/test1/experiment.py,0,"b'from collections import OrderedDict\n\nfrom catalyst.contrib.data.transforms import Compose, Normalize, ToTensor\nfrom catalyst.contrib.datasets import MNIST\nfrom catalyst.dl import ConfigExperiment\n\n\nclass SimpleExperiment(ConfigExperiment):\n    """"""\n    @TODO: Docs. Contribution is welcome\n    """"""\n\n    @staticmethod\n    def get_transforms(stage: str = None, mode: str = None):\n        """"""\n        @TODO: Docs. Contribution is welcome\n        """"""\n        return Compose([ToTensor(), Normalize((0.1307,), (0.3081,))])\n\n    def get_datasets(self, stage: str, **kwargs):\n        """"""\n        @TODO: Docs. Contribution is welcome\n        """"""\n        datasets = OrderedDict()\n\n        if stage != ""infer"":\n            trainset = MNIST(\n                ""./data"",\n                train=False,\n                download=True,\n                transform=SimpleExperiment.get_transforms(\n                    stage=stage, mode=""train""\n                ),\n            )\n            testset = MNIST(\n                ""./data"",\n                train=False,\n                download=True,\n                transform=SimpleExperiment.get_transforms(\n                    stage=stage, mode=""valid""\n                ),\n            )\n\n            datasets[""train""] = trainset\n            datasets[""valid""] = testset\n        else:\n            testset = MNIST(\n                ""./data"",\n                train=False,\n                download=True,\n                transform=SimpleExperiment.get_transforms(\n                    stage=stage, mode=""valid""\n                ),\n            )\n            datasets[""infer""] = testset\n\n        return datasets\n'"
tests/_tests_cv_classification_experiment_registry/test1/model.py,1,"b'from torch import nn\nfrom torch.nn import functional as F\n\n\nclass SimpleNet(nn.Module):\n    """"""\n    @TODO: Docs. Contribution is welcome\n    """"""\n\n    def __init__(self):\n        """"""\n        @TODO: Docs. Contribution is welcome\n        """"""\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n        self.fc1 = nn.Linear(4 * 4 * 50, 500)\n        self.fc2 = nn.Linear(500, 10)\n\n    def forward(self, x):\n        """"""\n        @TODO: Docs. Contribution is welcome\n        """"""\n        x = F.relu(self.conv1(x))\n        x = F.max_pool2d(x, 2, 2)\n        x = F.relu(self.conv2(x))\n        x = F.max_pool2d(x, 2, 2)\n        x = x.view(-1, 4 * 4 * 50)\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n'"
tests/_tests_cv_classification_experiment_registry/test2/__init__.py,0,"b'# flake8: noqa\nimport experiments as exp\n\nfrom catalyst.dl import registry, SupervisedRunner as Runner\n\nfrom .model import SimpleNet\n\nregistry.Model(SimpleNet)\nregistry.EXPERIMENTS.add_from_module(exp)\n'"
tests/_tests_cv_classification_experiment_registry/test2/model.py,1,"b'from torch import nn\nfrom torch.nn import functional as F\n\n\nclass SimpleNet(nn.Module):\n    """"""\n    @TODO: Docs. Contribution is welcome\n    """"""\n\n    def __init__(self):\n        """"""\n        @TODO: Docs. Contribution is welcome\n        """"""\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n        self.fc1 = nn.Linear(4 * 4 * 50, 500)\n        self.fc2 = nn.Linear(500, 10)\n\n    def forward(self, x):\n        """"""\n        @TODO: Docs. Contribution is welcome\n        """"""\n        x = F.relu(self.conv1(x))\n        x = F.max_pool2d(x, 2, 2)\n        x = F.relu(self.conv2(x))\n        x = F.max_pool2d(x, 2, 2)\n        x = x.view(-1, 4 * 4 * 50)\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n'"
catalyst/contrib/data/cv/__init__.py,0,b'# flake8: noqa\n\nfrom .mixins import *\nfrom .reader import *\nfrom .transforms import *\n'
catalyst/contrib/data/cv/reader.py,0,"b'from typing import Tuple, Union\n\nfrom catalyst import utils\nfrom catalyst.data.reader import ReaderSpec\n\n\nclass ImageReader(ReaderSpec):\n    """"""Image reader abstraction. Reads images from a ``csv`` dataset.""""""\n\n    def __init__(\n        self,\n        input_key: str,\n        output_key: str,\n        rootpath: str = None,\n        grayscale: bool = False,\n    ):\n        """"""\n        Args:\n            input_key (str): key to use from annotation dict\n            output_key (str): key to use to store the result\n            rootpath (str): path to images dataset root directory\n                (so your can use relative paths in annotations)\n            grayscale (bool): flag if you need to work only\n                with grayscale images\n        """"""\n        super().__init__(input_key, output_key)\n        self.rootpath = rootpath\n        self.grayscale = grayscale\n\n    def __call__(self, element):\n        """"""Reads a row from your annotations dict with filename and\n        transfer it to an image\n\n        Args:\n            element: elem in your dataset\n\n        Returns:\n            np.ndarray: Image\n        """"""\n        image_name = str(element[self.input_key])\n        img = utils.imread(\n            image_name, rootpath=self.rootpath, grayscale=self.grayscale\n        )\n\n        output = {self.output_key: img}\n        return output\n\n\nclass MaskReader(ReaderSpec):\n    """"""Mask reader abstraction. Reads masks from a `csv` dataset.""""""\n\n    def __init__(\n        self,\n        input_key: str,\n        output_key: str,\n        rootpath: str = None,\n        clip_range: Tuple[Union[int, float], Union[int, float]] = (0, 1),\n    ):\n        """"""\n        Args:\n            input_key (str): key to use from annotation dict\n            output_key (str): key to use to store the result\n            rootpath (str): path to images dataset root directory\n                (so your can use relative paths in annotations)\n            clip_range (Tuple[int, int]): lower and upper interval edges,\n                image values outside the interval are clipped\n                to the interval edges\n        """"""\n        super().__init__(input_key, output_key)\n        self.rootpath = rootpath\n        self.clip = clip_range\n\n    def __call__(self, element):\n        """"""Reads a row from your annotations dict with filename and\n        transfer it to a mask\n\n        Args:\n            element: elem in your dataset.\n\n        Returns:\n            np.ndarray: Mask\n        """"""\n        mask_name = str(element[self.input_key])\n        mask = utils.mimread(\n            mask_name, rootpath=self.rootpath, clip_range=self.clip\n        )\n\n        output = {self.output_key: mask}\n        return output\n\n\n__all__ = [""ImageReader"", ""MaskReader""]\n'"
catalyst/contrib/data/nlp/__init__.py,0,b'# flake8: noqa\nfrom .dataset import *\n'
catalyst/contrib/dl/callbacks/__init__.py,0,"b'# flake8: noqa\nimport logging\n\nfrom catalyst.tools import settings\n\nfrom .cutmix_callback import CutmixCallback\nfrom .gradnorm_logger import GradNormLogger\nfrom .knn_metric import KNNMetricCallback\nfrom .periodic_loader_callback import PeriodicLoaderCallback\nfrom .perplexity_metric import PerplexityMetricCallback\nfrom .telegram_logger import TelegramLogger\nfrom .tracer_callback import TracerCallback\n\nlogger = logging.getLogger(__name__)\n\ntry:\n    import imageio\n    from .mask_inference import InferMaskCallback\nexcept ImportError as ex:\n    if settings.cv_required:\n        logger.warning(\n            ""some of catalyst-cv dependencies not available,""\n            "" to install dependencies, run `pip install catalyst[cv]`.""\n        )\n        raise ex\n\ntry:\n    import alchemy\n    from .alchemy_logger import AlchemyLogger\nexcept ImportError as ex:\n    if settings.alchemy_logger_required:\n        logger.warning(\n            ""alchemy not available, to install alchemy, ""\n            ""run `pip install alchemy`.""\n        )\n        raise ex\n\ntry:\n    import visdom\n    from .visdom_logger import VisdomLogger\nexcept ImportError as ex:\n    if settings.visdom_logger_required:\n        logger.warning(\n            ""visdom not available, to install visdom, ""\n            ""run `pip install visdom`.""\n        )\n        raise ex\n\ntry:\n    import neptune\n    from .neptune_logger import NeptuneLogger\nexcept ImportError as ex:\n    if settings.neptune_logger_required:\n        logger.warning(\n            ""neptune not available, to install neptune, ""\n            ""run `pip install neptune-client`.""\n        )\n        raise ex\n\ntry:\n    import wandb\n    from .wandb_logger import WandbLogger\nexcept ImportError as ex:\n    if settings.wandb_logger_required:\n        logger.warning(\n            ""wandb not available, to install wandb, ""\n            ""run `pip install wandb`.""\n        )\n        raise ex\n'"
catalyst/contrib/dl/callbacks/alchemy_logger.py,0,"b'from typing import Dict, List\n\nfrom alchemy import Logger\n\nfrom catalyst import utils\nfrom catalyst.core.callback import (\n    Callback,\n    CallbackNode,\n    CallbackOrder,\n    CallbackScope,\n)\nfrom catalyst.core.runner import IRunner\n\n\nclass AlchemyLogger(Callback):\n    """"""Logger callback, translates ``runner.*_metrics`` to Alchemy.\n    Read about Alchemy here https://alchemy.host\n\n    Example:\n        .. code-block:: python\n\n            from catalyst.dl import SupervisedRunner, AlchemyLogger\n\n            runner = SupervisedRunner()\n\n            runner.train(\n                model=model,\n                criterion=criterion,\n                optimizer=optimizer,\n                loaders=loaders,\n                logdir=logdir,\n                num_epochs=num_epochs,\n                verbose=True,\n                callbacks={\n                    ""logger"": AlchemyLogger(\n                        token=""..."", # your Alchemy token\n                        project=""your_project_name"",\n                        experiment=""your_experiment_name"",\n                        group=""your_experiment_group_name"",\n                    )\n                }\n            )\n\n    Powered by Catalyst.Ecosystem.\n    """"""\n\n    def __init__(\n        self,\n        metric_names: List[str] = None,\n        log_on_batch_end: bool = True,\n        log_on_epoch_end: bool = True,\n        **logging_params,\n    ):\n        """"""\n        Args:\n            metric_names (List[str]): list of metric names to log,\n                if none - logs everything\n            log_on_batch_end (bool): logs per-batch metrics if set True\n            log_on_epoch_end (bool): logs per-epoch metrics if set True\n        """"""\n        super().__init__(\n            order=CallbackOrder.Logging,\n            node=CallbackNode.Master,\n            scope=CallbackScope.Experiment,\n        )\n        self.metrics_to_log = metric_names\n        self.log_on_batch_end = log_on_batch_end\n        self.log_on_epoch_end = log_on_epoch_end\n\n        if not (self.log_on_batch_end or self.log_on_epoch_end):\n            raise ValueError(""You have to log something!"")\n\n        if (self.log_on_batch_end and not self.log_on_epoch_end) or (\n            not self.log_on_batch_end and self.log_on_epoch_end\n        ):\n            self.batch_log_suffix = """"\n            self.epoch_log_suffix = """"\n        else:\n            self.batch_log_suffix = ""_batch""\n            self.epoch_log_suffix = ""_epoch""\n\n        self.logger = Logger(**logging_params)\n\n    def __del__(self):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        self.logger.close()\n\n    def _log_metrics(\n        self, metrics: Dict[str, float], step: int, mode: str, suffix=""""\n    ):\n        if self.metrics_to_log is None:\n            metrics_to_log = sorted(metrics.keys())\n        else:\n            metrics_to_log = self.metrics_to_log\n\n        for name in metrics_to_log:\n            if name in metrics:\n                metric_name = f""{name}/{mode}{suffix}""\n                metric_value = metrics[name]\n                self.logger.log_scalar(\n                    name=metric_name, value=metric_value, step=step,\n                )\n\n    def on_batch_end(self, runner: IRunner):\n        """"""Translate batch metrics to Alchemy.""""""\n        if self.log_on_batch_end:\n            mode = runner.loader_name\n            metrics_ = runner.batch_metrics\n            self._log_metrics(\n                metrics=metrics_,\n                step=runner.global_sample_step,\n                mode=mode,\n                suffix=self.batch_log_suffix,\n            )\n\n    def on_loader_end(self, runner: IRunner):\n        """"""Translate loader metrics to Alchemy.""""""\n        if self.log_on_epoch_end:\n            mode = runner.loader_name\n            metrics_ = runner.loader_metrics\n            self._log_metrics(\n                metrics=metrics_,\n                step=runner.global_epoch,\n                mode=mode,\n                suffix=self.epoch_log_suffix,\n            )\n\n    def on_epoch_end(self, runner: IRunner):\n        """"""Translate epoch metrics to Alchemy.""""""\n        extra_mode = ""_base""\n        splitted_epoch_metrics = utils.split_dict_to_subdicts(\n            dct=runner.epoch_metrics,\n            prefixes=list(runner.loaders.keys()),\n            extra_key=extra_mode,\n        )\n\n        if self.log_on_epoch_end:\n            self._log_metrics(\n                metrics=splitted_epoch_metrics[extra_mode],\n                step=runner.global_epoch,\n                mode=extra_mode,\n                suffix=self.epoch_log_suffix,\n            )\n\n\n__all__ = [""AlchemyLogger""]\n'"
catalyst/contrib/dl/callbacks/cutmix_callback.py,1,"b'from typing import List\n\nimport numpy as np\n\nimport torch\n\nfrom catalyst.core.callbacks import CriterionCallback\nfrom catalyst.core.runner import IRunner\n\n\nclass CutmixCallback(CriterionCallback):\n    """"""\n    Callback to do Cutmix augmentation that has been proposed in\n    `CutMix: Regularization Strategy to Train Strong Classifiers\n    with Localizable Features`_.\n\n    .. warning::\n        :class:`catalyst.contrib.dl.callbacks.CutmixCallback` is inherited from\n        :class:`catalyst.dl.CriterionCallback` and does its work.\n        You may not use them together.\n\n    .. _CutMix\\: Regularization Strategy to Train Strong Classifiers\n        with Localizable Features: https://arxiv.org/abs/1905.04899\n    """"""\n\n    def __init__(\n        self,\n        fields: List[str] = (""features"",),\n        alpha=1.0,\n        on_train_only=True,\n        **kwargs\n    ):\n        """"""\n        Args:\n            fields (List[str]): list of features which must be affected.\n            alpha (float): beta distribution parameter.\n            on_train_only (bool): Apply to train only.\n                So, if on_train_only is True, use a standard output/metric\n                for validation.\n        """"""\n        assert (\n            len(fields) > 0\n        ), ""At least one field for CutmixCallback is required""\n        assert alpha >= 0, ""alpha must be >=0""\n\n        super().__init__(**kwargs)\n\n        self.on_train_only = on_train_only\n        self.fields = fields\n        self.alpha = alpha\n        self.lam = 1\n        self.index = None\n        self.is_needed = True\n\n    def _compute_loss(self, runner: IRunner, criterion):\n        """"""Computes loss.\n\n        If self.is_needed is ``False`` then calls ``_compute_loss``\n        from ``CriterionCallback``, otherwise computes loss value.\n\n        Args:\n            runner (IRunner): current runner\n            criterion: that is used to compute loss\n        """"""\n        if not self.is_needed:\n            return super()._compute_loss_value(runner, criterion)\n\n        pred = runner.output[self.output_key]\n        y_a = runner.input[self.input_key]\n        y_b = runner.input[self.input_key][self.index]\n        loss = self.lam * criterion(pred, y_a) + (1 - self.lam) * criterion(\n            pred, y_b\n        )\n        return loss\n\n    def _rand_bbox(self, size, lam):\n        """"""\n        Generates top-left and bottom-right coordinates of the box\n        of the given size.\n\n        Args:\n            size: size of the box\n            lam: lambda parameter\n\n        Returns:\n            top-left and bottom-right coordinates of the box\n        """"""\n        w = size[2]\n        h = size[3]\n        cut_rat = np.sqrt(1.0 - lam)\n        cut_w = np.int(w * cut_rat)\n        cut_h = np.int(h * cut_rat)\n\n        cx = np.random.randint(w)\n        cy = np.random.randint(h)\n\n        bbx1 = np.clip(cx - cut_w // 2, 0, w)\n        bby1 = np.clip(cy - cut_h // 2, 0, h)\n        bbx2 = np.clip(cx + cut_w // 2, 0, w)\n        bby2 = np.clip(cy + cut_h // 2, 0, h)\n\n        return bbx1, bby1, bbx2, bby2\n\n    def on_loader_start(self, runner: IRunner) -> None:\n        """"""Checks if it is needed for the loader.\n\n        Args:\n            runner (IRunner): current runner\n        """"""\n        self.is_needed = not self.on_train_only or runner.is_train_loader\n\n    def on_batch_start(self, runner: IRunner) -> None:\n        """"""Mixes data according to Cutmix algorithm.\n\n        Args:\n            runner (IRunner): current runner\n        """"""\n        if not self.is_needed:\n            return\n\n        if self.alpha > 0:\n            self.lam = np.random.beta(self.alpha, self.alpha)\n        else:\n            self.lam = 1\n\n        self.index = torch.randperm(runner.input[self.fields[0]].shape[0])\n        self.index.to(runner.device)\n\n        bbx1, bby1, bbx2, bby2 = self._rand_bbox(\n            runner.input[self.fields[0]].shape, self.lam\n        )\n\n        for f in self.fields:\n            runner.input[f][:, :, bbx1:bbx2, bby1:bby2] = runner.input[f][\n                self.index, :, bbx1:bbx2, bby1:bby2\n            ]\n\n        self.lam = 1 - (\n            (bbx2 - bbx1)\n            * (bby2 - bby1)\n            / (\n                runner.input[self.fields[0]].shape[-1]\n                * runner.input[self.fields[0]].shape[-2]\n            )\n        )\n\n\n__all__ = [""CutmixCallback""]\n'"
catalyst/contrib/dl/callbacks/gradnorm_logger.py,2,"b'from typing import Dict\n\nfrom torch.nn import DataParallel\nfrom torch.nn.parallel import DistributedDataParallel\n\nfrom catalyst.core.callback import Callback, CallbackNode, CallbackOrder\nfrom catalyst.core.runner import IRunner\nfrom catalyst.tools.typing import Model\n\n\nclass GradNormLogger(Callback):\n    """"""Callback for logging model gradients.""""""\n\n    def __init__(\n        self, norm_type: int = 2, accumulation_steps: int = 1,\n    ):\n        """"""\n        Args:\n            norm_type (int): norm type used to calculate norm of gradients.\n                If `OptimizerCallback` provides non-default argument\n                `grad_clip_params` with custom norm type, then corresponding\n                norm type should be used in this class.\n            accumulation_steps (int): number of steps before\n                ``model.zero_grad()``.\n                Should be the same as in `OptimizerCallback`.\n        """"""\n        super().__init__(\n            order=CallbackOrder.Optimizer + 1, node=CallbackNode.All\n        )\n\n        self.grad_norm_prefix = ""_grad_norm""\n        self.norm_type = norm_type\n\n        self.accumulation_steps: int = accumulation_steps\n        self._accumulation_counter: int = 0\n\n    @staticmethod\n    def grad_norm(*, model: Model, prefix: str, norm_type: int,) -> Dict:\n        """"""Computes gradient norms for a given model.\n\n        Args:\n            model (Model): model which gradients to be saved.\n            prefix (str): prefix for keys in resulting dictionary.\n            norm_type (int): norm type of gradient norm.\n        Returns:\n            Dict: dictionary in which gradient norms are stored.\n        """"""\n        if isinstance(model, (DataParallel, DistributedDataParallel)):\n            model = model.module\n\n        total_norm = 0.0\n        grad_norm = {}\n\n        for tag, value in model.named_parameters():\n            tag = tag.replace(""."", ""/"")\n            metrics_tag = f""{prefix}/{tag}""\n            param_norm = value.grad.data.norm(norm_type).item()\n            total_norm += param_norm ** norm_type\n            grad_norm[metrics_tag] = param_norm\n\n        total_norm = total_norm ** (1.0 / norm_type)\n        tag = ""total""\n        metrics_tag = f""{prefix}/{tag}""\n        grad_norm[metrics_tag] = total_norm\n\n        return grad_norm\n\n    def on_batch_end(self, runner: IRunner) -> None:\n        """"""On batch end event\n\n        Args:\n            runner (IRunner): current runner\n        """"""\n        if not runner.is_train_loader:\n            return\n\n        self._accumulation_counter += 1\n        need_gradient_step = (\n            self._accumulation_counter % self.accumulation_steps == 0\n        )\n\n        if need_gradient_step:\n            grad_norm = self.grad_norm(\n                model=runner.model,\n                prefix=self.grad_norm_prefix,\n                norm_type=self.norm_type,\n            )\n\n            runner.batch_metrics.update(**grad_norm)\n            self._accumulation_counter = 0\n\n\n__all__ = [""GradNormLogger""]\n'"
catalyst/contrib/dl/callbacks/knn_metric.py,2,"b'from typing import Dict, List\nfrom math import ceil\n\nimport numpy as np\nfrom scipy import stats\nfrom sklearn.metrics import (\n    accuracy_score,\n    f1_score,\n    precision_score,\n    recall_score,\n)\nfrom sklearn.neighbors import NearestNeighbors\n\nimport torch\n\nfrom catalyst.core.callback import Callback, CallbackOrder\nfrom catalyst.core.runner import IRunner\n\n\nclass KNNMetricCallback(Callback):\n    """"""A callback that returns single metric on ``runner.on_loader_end``.""""""\n\n    def __init__(\n        self,\n        input_key: str = ""logits"",\n        output_key: str = ""targets"",\n        prefix: str = ""knn"",\n        num_classes: int = 2,\n        class_names: dict = None,\n        cv_loader_names: Dict[str, List[str]] = None,\n        metric_fn: str = ""f1-score"",\n        knn_metric: str = ""euclidean"",\n        num_neighbors: int = 5,\n    ):\n        """"""Returns metric value calculated using kNN algorithm.\n\n        Args:\n            input_key: input key to get features.\n            output_key: output key to get targets.\n            prefix: key to store in logs.\n            num_classes: Number of classes; must be > 1.\n            class_names: of indexes and class names.\n            cv_loader_names: dict with keys and values of loader_names\n                for which cross validation should be calculated.\n                For example {""train"" : [""valid"", ""test""]}.\n            metric_fn: one of `accuracy`, `precision`, `recall`, `f1-score`.\n                       default is `f1-score`.\n            knn_metric: look sklearn.neighbors.NearestNeighbors parameter.\n            num_neighbors: number of neighbors, default is 5.\n        """"""\n        super().__init__(CallbackOrder.Metric)\n\n        assert num_classes > 1, ""`num_classes` should be more than 1""\n\n        metric_fns = {\n            ""accuracy"": accuracy_score,\n            ""recall"": recall_score,\n            ""precision"": precision_score,\n            ""f1-score"": f1_score,\n        }\n\n        assert (\n            metric_fn in metric_fns\n        ), f""Metric function with value `{metric_fn}` not implemented""\n\n        self.prefix = prefix\n        self.features_key = input_key\n        self.targets_key = output_key\n\n        self.num_classes = num_classes\n        self.class_names = (\n            class_names\n            if class_names is not None\n            else [str(i) for i in range(num_classes)]\n        )\n\n        self.cv_loader_names = cv_loader_names\n\n        self.metric_fn = metric_fns[metric_fn]\n        self.knn_metric = knn_metric\n        self.num_neighbors = num_neighbors\n\n        self.num_folds = 1\n\n        self._reset_cache()\n        self._reset_sets()\n\n    def _reset_cache(self):\n        """"""Function to reset cache for features and labels.""""""\n        self.features = []\n        self.targets = []\n\n    def _reset_sets(self):\n        """"""Function to reset cache for all sets.""""""\n        self.sets = {}\n\n    def _knn(self, train_set, test_set=None):\n        """"""Returns accuracy calculated using kNN algorithm.\n\n        Args:\n            train_set: dict of feature ""values"" and ""labels"" for training set.\n            test_set: dict of feature ""values"" and ""labels"" for test set.\n\n        Returns:\n            cm: tuple of lists of true & predicted classes.\n        """"""\n        # if the test_set is None, we will test train_set on itself,\n        # in that case we need to delete the closest neighbor\n        leave_one_out = test_set is None\n\n        if leave_one_out:\n            test_set = train_set\n\n        x_train, y_train = train_set[""values""], train_set[""labels""]\n        x_test, y_test = test_set[""values""], test_set[""labels""]\n\n        size = len(y_train)\n\n        result = None\n        while result is None:\n            try:\n                y_pred = []\n\n                # fit nearest neighbors class on our train data\n                classifier = NearestNeighbors(\n                    num_neighbors=self.num_neighbors + int(leave_one_out),\n                    metric=self.knn_metric,\n                    algorithm=""brute"",\n                )\n                classifier.fit(x_train, y_train)\n\n                # data could be evaluated in num_folds in order to avoid OOM\n                end_idx, batch_size = 0, ceil(size / self.num_folds)\n                for start_idx in range(0, size, batch_size):\n\n                    end_idx = min(start_idx + batch_size, size)\n\n                    x = x_test[start_idx:end_idx]\n\n                    knn_ids = classifier.kneighbors(x, return_distance=False)\n\n                    # if we predict train set on itself we have to delete 0th\n                    # neighbor for all of the distances\n                    if leave_one_out:\n                        knn_ids = knn_ids[:, 1:]\n\n                    # calculate the most frequent class across k neighbors\n                    knn_classes = y_train[knn_ids]\n                    knn_classes, _ = stats.mode(knn_classes, axis=1)\n\n                    y_pred.extend(knn_classes[:, 0].tolist())\n\n                y_pred = np.asarray(y_pred)\n\n                result = (y_test, y_pred)\n\n            # this try catch block made because sometimes sets are quite big\n            # and it is not possible to put everything in memory, so we split\n            except MemoryError:\n                print(\n                    f""Memory error with {self.num_folds} folds, trying more.""\n                )\n                self.num_folds *= 2\n                result = None\n\n        return result\n\n    def on_batch_end(self, runner: IRunner) -> None:\n        """"""Batch end hook.\n\n        Args:\n            runner (IRunner): current runner\n        """"""\n        features: torch.Tensor = runner.output[\n            self.features_key\n        ].cpu().detach().numpy()\n        targets: torch.Tensor = runner.input[\n            self.targets_key\n        ].cpu().detach().numpy()\n\n        self.features.extend(features)\n        self.targets.extend(targets)\n\n    def on_loader_end(self, runner: IRunner) -> None:\n        """"""Loader end hook.\n\n        Args:\n            runner (IRunner): current runner\n        """"""\n        self.features = np.stack(self.features)\n        self.targets = np.stack(self.targets)\n\n        if len(np.unique(self.targets)) > self.num_classes:\n            raise Warning(""Targets has more classes than num_classes"")\n\n        s = {\n            ""values"": self.features,\n            ""labels"": self.targets,\n        }\n\n        self.sets[runner.loader_name] = s\n\n        y_true, y_pred = self._knn(s)\n\n        loader_values = runner.loader_metrics\n        if self.num_classes == 2:\n            loader_values[self.prefix] = self.metric_fn(\n                y_true, y_pred, average=""binary""\n            )\n        else:\n            values = self.metric_fn(y_true, y_pred, average=None)\n\n            loader_values[f""{self.prefix}""] = np.mean(values)\n            for i, value in enumerate(values):\n                loader_values[f""{self.prefix}/{self.class_names[i]}""] = value\n\n        self._reset_cache()\n\n    def on_epoch_end(self, runner: IRunner) -> None:\n        """"""Epoch end hook.\n\n        Args:\n            runner (IRunner): current runner\n        """"""\n        if self.cv_loader_names is not None:\n            for k, vs in self.cv_loader_names.items():\n\n                # checking for presence of subset\n                if k not in self.sets:\n                    print(\n                        f""Set `{k}` not found in the sets. ""\n                        f""Please change `cv_loader_names` parameter.""\n                    )\n                    continue\n\n                for v in vs:\n\n                    # checking for presence of subset\n                    if v not in self.sets:\n                        print(\n                            f""Set `{v}` not found in the sets. ""\n                            f""Please change `cv_loader_names` parameter.""\n                        )\n                        continue\n\n                    y_true, y_pred = self._knn(self.sets[k], self.sets[v])\n\n                    loader_values = runner.epoch_metrics[f""{k}_{v}_cv""]\n\n                    if self.num_classes == 2:\n                        loader_values[f""{self.prefix}""] = self.metric_fn(\n                            y_true, y_pred, average=""binary""\n                        )\n                    else:\n                        values = self.metric_fn(y_true, y_pred, average=None)\n\n                        loader_values[f""{self.prefix}""] = np.mean(values)\n                        for i, value in enumerate(values):\n                            prefix = f""{self.prefix}/{self.class_names[i]}""\n                            loader_values[prefix] = value\n\n        self._reset_cache()\n        self._reset_sets()\n\n\n__all__ = [""KNNMetricCallback""]\n'"
catalyst/contrib/dl/callbacks/mask_inference.py,3,"b'import os\n\nimport imageio\nimport numpy as np\nfrom skimage.color import label2rgb\n\nimport torch\nimport torch.nn.functional as F\n\nfrom catalyst.core.callback import Callback, CallbackOrder\nfrom catalyst.core.runner import IRunner\nfrom catalyst.dl import utils\n\n\nclass InferMaskCallback(Callback):\n    """"""@TODO: Docs. Contribution is welcome.""""""\n\n    def __init__(\n        self,\n        out_dir=None,\n        out_prefix=None,\n        input_key=None,\n        output_key=None,\n        name_key=None,\n        mean=None,\n        std=None,\n        threshold: float = 0.5,\n        mask_strength: float = 0.5,\n        mask_type: str = ""soft"",\n    ):\n        """"""\n        Args:\n            @TODO: Docs. Contribution is welcome\n        """"""\n        super().__init__(CallbackOrder.Internal)\n        self.out_dir = out_dir\n        self.out_prefix = out_prefix\n        self.mean = mean or np.array([0.485, 0.456, 0.406])\n        self.std = std or np.array([0.229, 0.224, 0.225])\n        assert input_key is not None\n        assert output_key is not None\n        self.threshold = threshold\n        self.mask_strength = mask_strength\n        self.mask_type = mask_type\n        self.input_key = input_key\n        self.output_key = output_key\n        self.name_key = name_key\n        self.counter = 0\n        self._keys_from_runner = [""out_dir"", ""out_prefix""]\n\n    def on_stage_start(self, runner: IRunner):\n        """"""Stage start hook.\n\n        Args:\n            runner (IRunner): current runner\n        """"""\n        for key in self._keys_from_runner:\n            value = getattr(runner, key, None)\n            if value is not None:\n                setattr(self, key, value)\n        # assert self.out_prefix is not None\n        self.out_prefix = (\n            self.out_prefix if self.out_prefix is not None else """"\n        )\n        if self.out_dir is not None:\n            self.out_prefix = str(self.out_dir) + ""/"" + str(self.out_prefix)\n        os.makedirs(os.path.dirname(self.out_prefix), exist_ok=True)\n\n    def on_loader_start(self, runner: IRunner):\n        """"""Loader start hook.\n\n        Args:\n            runner (IRunner): current runner\n        """"""\n        lm = runner.loader_name\n        os.makedirs(f""{self.out_prefix}/{lm}/"", exist_ok=True)\n\n    def on_batch_end(self, runner: IRunner):\n        """"""Batch end hook.\n\n        Args:\n            runner (IRunner): current runner\n        """"""\n        lm = runner.loader_name\n        names = runner.input.get(self.name_key, [])\n\n        features = runner.input[self.input_key].detach().cpu()\n        images = utils.tensor_to_ndimage(features)\n\n        logits = runner.output[self.output_key]\n        logits = (\n            torch.unsqueeze_(logits, dim=1)\n            if len(logits.shape) < 4\n            else logits\n        )\n\n        if self.mask_type == ""soft"":\n            probabilities = torch.sigmoid(logits)\n        else:\n            probabilities = F.softmax(logits, dim=1)\n        probabilities = probabilities.detach().cpu().numpy()\n\n        masks = []\n        for probability in probabilities:\n            mask = np.zeros_like(probability[0], dtype=np.int32)\n            for i, ch in enumerate(probability):\n                mask[ch >= self.threshold] = i + 1\n            masks.append(mask)\n\n        for i, (image, mask) in enumerate(zip(images, masks)):\n            try:\n                suffix = names[i]\n            except IndexError:\n                suffix = f""{self.counter:06d}""\n            self.counter += 1\n\n            mask = label2rgb(mask, bg_label=0)\n\n            image = (\n                image * (1 - self.mask_strength) + mask * self.mask_strength\n            )\n            image = (image * 255).clip(0, 255).round().astype(np.uint8)\n\n            filename = f""{self.out_prefix}/{lm}/{suffix}.jpg""\n            imageio.imwrite(filename, image)\n\n\n__all__ = [""InferMaskCallback""]\n'"
catalyst/contrib/dl/callbacks/neptune_logger.py,0,"b'from typing import Dict, List\n\nimport neptune\n\nfrom catalyst.core.callback import (\n    Callback,\n    CallbackNode,\n    CallbackOrder,\n    CallbackScope,\n)\nfrom catalyst.core.runner import IRunner\n\n\nclass NeptuneLogger(Callback):\n    """"""Logger callback, translates ``runner.*_metrics`` to Neptune.\n    Read about Neptune here https://neptune.ai\n\n    Example:\n        .. code-block:: python\n\n            from catalyst.dl import SupervisedRunner\n            from catalyst.contrib.dl.callbacks.neptune import NeptuneLogger\n\n            runner = SupervisedRunner()\n\n            runner.train(\n                model=model,\n                criterion=criterion,\n                optimizer=optimizer,\n                loaders=loaders,\n                logdir=logdir,\n                num_epochs=num_epochs,\n                verbose=True,\n                callbacks=[\n                    NeptuneLogger(\n                        api_token=""..."", # your Neptune token\n                        project_name=""your_project_name"",\n                        offline_mode=False, # turn off neptune for debug\n                        name=""your_experiment_name"",\n                        params={...},  # your hyperparameters\n                        tags=[""resnet"", ""no-augmentations""], # tags\n                        upload_source_files=[""*.py""], # files to save\n                         )\n                    ]\n                 )\n\n        You can see an example experiment here:\n        https://ui.neptune.ai/o/shared/org/catalyst-integration/e/CAT-13/charts\n\n        You can log your experiments without registering.\n        Just use ""ANONYMOUS"" token::\n\n            runner.train(\n                ...\n                callbacks=[\n                    ""NepuneLogger(\n                        api_token=""ANONYMOUS"",\n                        project_name=""shared/catalyst-integration"",\n                        ...\n                         )\n                    ]\n                )\n    """"""\n\n    def __init__(\n        self,\n        metric_names: List[str] = None,\n        log_on_batch_end: bool = True,\n        log_on_epoch_end: bool = True,\n        offline_mode: bool = False,\n        **logging_params,\n    ):\n        """"""\n        Args:\n            metric_names (List[str]): list of metric names to log,\n                if none - logs everything\n            log_on_batch_end (bool): logs per-batch metrics if set True\n            log_on_epoch_end (bool): logs per-epoch metrics if set True\n            offline_mode (bool): whether logging to Neptune server should\n                 be turned off. It is useful for debugging\n        """"""\n        super().__init__(\n            order=CallbackOrder.Logging,\n            node=CallbackNode.Master,\n            scope=CallbackScope.Experiment,\n        )\n        self.metrics_to_log = metric_names\n        self.log_on_batch_end = log_on_batch_end\n        self.log_on_epoch_end = log_on_epoch_end\n\n        if not (self.log_on_batch_end or self.log_on_epoch_end):\n            raise ValueError(""You have to log something!"")\n\n        if (self.log_on_batch_end and not self.log_on_epoch_end) or (\n            not self.log_on_batch_end and self.log_on_epoch_end\n        ):\n            self.batch_log_suffix = """"\n            self.epoch_log_suffix = """"\n        else:\n            self.batch_log_suffix = ""_batch""\n            self.epoch_log_suffix = ""_epoch""\n\n        if offline_mode:\n            neptune.init(\n                project_qualified_name=""dry-run/project"",\n                backend=neptune.OfflineBackend(),\n            )\n        else:\n            neptune.init(\n                api_token=logging_params[""api_token""],\n                project_qualified_name=logging_params[""project_name""],\n            )\n\n        logging_params.pop(""api_token"")\n        logging_params.pop(""project_name"")\n\n        self.experiment = neptune.create_experiment(**logging_params)\n\n    def __del__(self):\n        """"""@TODO: Docs. Contribution is welcome""""""\n        if hasattr(self, ""experiment""):\n            self.experiment.stop()\n\n    def _log_metrics(\n        self, metrics: Dict[str, float], step: int, mode: str, suffix=""""\n    ):\n        if self.metrics_to_log is None:\n            metrics_to_log = sorted(metrics.keys())\n        else:\n            metrics_to_log = self.metrics_to_log\n\n        for name in metrics_to_log:\n            if name in metrics:\n                metric_name = f""{name}/{mode}{suffix}""\n                metric_value = metrics[name]\n                self.experiment.log_metric(metric_name, y=metric_value, x=step)\n\n    def on_batch_end(self, runner: IRunner):\n        """"""Log batch metrics to Neptune.""""""\n        if self.log_on_batch_end:\n            mode = runner.loader_name\n            metrics_ = runner.batch_metrics\n            self._log_metrics(\n                metrics=metrics_,\n                step=runner.global_sample_step,\n                mode=mode,\n                suffix=self.batch_log_suffix,\n            )\n\n    def on_loader_end(self, runner: IRunner):\n        """"""Translate epoch metrics to Neptune.""""""\n        if self.log_on_epoch_end:\n            mode = runner.loader_name\n            metrics_ = runner.loader_metrics\n            self._log_metrics(\n                metrics=metrics_,\n                step=runner.global_epoch,\n                mode=mode,\n                suffix=self.epoch_log_suffix,\n            )\n\n\n__all__ = [""NeptuneLogger""]\n'"
catalyst/contrib/dl/callbacks/periodic_loader_callback.py,1,"b'from typing import Mapping\nfrom collections import OrderedDict\nimport copy\n\nfrom torch.utils.data import DataLoader\n\nfrom catalyst.core.callback import Callback, CallbackOrder\nfrom catalyst.core.runner import IRunner\n\n\nclass PeriodicLoaderCallback(Callback):\n    """"""Callback for runing loaders with specified period.\n    To disable loader use ``0`` as period.\n\n    Example:\n\n        >>> PeriodicLoaderRunnerCallback(\n        >>>     train_additional=2,\n        >>>     valid=3,\n        >>>     valid_additional=5\n        >>> )\n    """"""\n\n    def __init__(self, **kwargs):\n        """"""\n        Args:\n            kwargs: loader names and their run periods.\n        """"""\n        super().__init__(order=CallbackOrder.External)\n\n        self.valid_loader: str = None\n        self.valid_metrics: Mapping[str, float] = None\n        self.loaders: Mapping[str, DataLoader] = OrderedDict()\n\n        self.loader_periods = {}\n        for loader, period in kwargs.items():\n            if not isinstance(period, (int, float)):\n                raise TypeError(\n                    ""Expected loader period type is int/float ""\n                    f""but got {type(period)}""\n                )\n            self.loader_periods[loader] = int(period)\n\n    def on_stage_start(self, runner: IRunner) -> None:\n        """"""Collect information about loaders.\n\n        Arguments:\n            runner (IRunner): current runner\n        """"""\n        # store pointers to data loader objects\n        for name, loader in runner.loaders.items():\n            self.loaders[name] = loader\n        # stage validation loader\n        self.valid_loader = copy.copy(runner.valid_loader)\n        is_loaders_match = all(\n            loader in runner.loaders for loader in self.loader_periods.keys()\n        )\n        is_same_loaders_number = len(self.loader_periods) == len(\n            runner.loaders\n        )\n        if is_same_loaders_number and is_loaders_match:\n            # find potential epoch with zero loaders\n            zero_loaders_epochs = list(\n                filter(\n                    lambda n: all(\n                        (p == 0 or n % p != 0)\n                        for p in self.loader_periods.values()\n                    ),\n                    range(1, runner.num_epochs + 1),\n                )\n            )\n            if len(zero_loaders_epochs) > 0:\n                epoch_with_err = zero_loaders_epochs[0]\n                raise ValueError(\n                    f""There will be no loaders in epoch {epoch_with_err}!""\n                )\n\n    def on_epoch_start(self, runner: IRunner) -> None:\n        """"""Set loaders for current epoch.\n        If validation is not required then the first loader\n        from loaders used in current epoch will be used\n        as validation loader.\n        Metrics from the latest epoch with true\n        validation loader will be used\n        in the epochs where this loader is missing.\n\n        Arguments:\n            runner (IRunner): current runner\n        """"""\n        epoch_num = runner.epoch\n        # loaders to use in current epoch\n        epoch_loaders = OrderedDict()\n        for name, loader in self.loaders.items():\n            period = self.loader_periods.get(name, 1)\n            # ignore loaders where period - 0\n            if period > 0 and epoch_num % period == 0:\n                epoch_loaders[name] = loader\n        if len(epoch_loaders) == 0:\n            raise ValueError(f""There is no loaders in epoch {epoch_num}!"")\n        first_loader = next(iter(epoch_loaders.keys()))\n        runner.valid_loader = (\n            self.valid_loader\n            if self.valid_loader in epoch_loaders\n            else first_loader\n        )\n        runner.loaders = epoch_loaders\n\n    def on_epoch_end(self, runner: IRunner) -> None:\n        """"""Store validation metrics and use latest validation score\n        when validation loader is not required.\n\n        Arguments:\n            runner (IRunner): current runner\n        """"""\n        if self.valid_loader in runner.loaders:\n            self.valid_metrics = {\n                runner.main_metric: runner.valid_metrics[runner.main_metric]\n            }\n        elif self.valid_metrics is not None:\n            # use previous score on validation\n            runner.valid_metrics = self.valid_metrics\n\n\n__all__ = [""PeriodicLoaderCallback""]\n'"
catalyst/contrib/dl/callbacks/perplexity_metric.py,0,"b'from torch import nn\n\nfrom catalyst.core.callbacks import MetricCallback\n\n\nclass PerplexityMetricCallback(MetricCallback):\n    """"""\n    Perplexity is a very popular metric in NLP\n    especially in Language Modeling task.\n    It is 2^cross_entropy.\n    """"""\n\n    def __init__(\n        self,\n        input_key: str = ""targets"",\n        output_key: str = ""logits"",\n        prefix: str = ""perplexity"",\n        ignore_index: int = None,\n    ):\n        """"""\n        Args:\n            input_key (str): input key to use for perplexity calculation,\n                target tokens\n            output_key (str): output key to use for perplexity calculation,\n                logits of the predicted tokens\n            ignore_index (int): index to ignore, usually pad_index\n        """"""\n        self.ignore_index = ignore_index or nn.CrossEntropyLoss().ignore_index\n        self.cross_entropy_loss = nn.CrossEntropyLoss(\n            ignore_index=self.ignore_index\n        )\n        super().__init__(\n            metric_fn=self.metric_fn,\n            input_key=input_key,\n            output_key=output_key,\n            prefix=prefix,\n        )\n\n    def metric_fn(self, outputs, targets):\n        """"""Calculate perplexity""""""\n        cross_entropy = (\n            self.cross_entropy_loss(outputs, targets).detach().cpu()\n        )\n        perplexity = 2 ** cross_entropy\n        return perplexity.item()\n'"
catalyst/contrib/dl/callbacks/telegram_logger.py,0,"b'from typing import List\nimport logging\nfrom urllib.parse import quote_plus\nfrom urllib.request import Request, urlopen\n\nfrom catalyst import utils\nfrom catalyst.core.callback import Callback, CallbackNode, CallbackOrder\nfrom catalyst.core.runner import IRunner\nfrom catalyst.tools import settings\n\n\nclass TelegramLogger(Callback):\n    """"""\n    Logger callback, translates ``runner.metric_manager`` to telegram channel.\n    """"""\n\n    def __init__(\n        self,\n        token: str = None,\n        chat_id: str = None,\n        metric_names: List[str] = None,\n        log_on_stage_start: bool = True,\n        log_on_loader_start: bool = True,\n        log_on_loader_end: bool = True,\n        log_on_stage_end: bool = True,\n        log_on_exception: bool = True,\n    ):\n        """"""\n        Args:\n            token (str): telegram bot\'s token,\n                see https://core.telegram.org/bots\n            chat_id (str): Chat unique identifier\n            metric_names: List of metric names to log.\n                if none - logs everything.\n            log_on_stage_start (bool): send notification on stage start\n            log_on_loader_start (bool): send notification on loader start\n            log_on_loader_end (bool): send notification on loader end\n            log_on_stage_end (bool): send notification on stage end\n            log_on_exception (bool): send notification on exception\n        """"""\n        super().__init__(order=CallbackOrder.Logging, node=CallbackNode.Master)\n        # @TODO: replace this logic with global catalyst config at ~/.catalyst\n        self._token = token or settings.telegram_logger_token\n        self._chat_id = chat_id or settings.telegram_logger_chat_id\n        assert self._token is not None and self._chat_id is not None\n        self._base_url = (\n            f""https://api.telegram.org/bot{self._token}/sendMessage""\n        )\n\n        self.log_on_stage_start = log_on_stage_start\n        self.log_on_loader_start = log_on_loader_start\n        self.log_on_loader_end = log_on_loader_end\n        self.log_on_stage_end = log_on_stage_end\n        self.log_on_exception = log_on_exception\n\n        self.metrics_to_log = metric_names\n\n    def _send_text(self, text: str):\n        try:\n            url = (\n                f""{self._base_url}?""\n                f""chat_id={self._chat_id}&""\n                f""disable_web_page_preview=1&""\n                f""text={quote_plus(text, safe=\'\')}""\n            )\n\n            request = Request(url)\n            urlopen(request)\n        except Exception as e:\n            logging.getLogger(__name__).warning(f""telegram.send.error:{e}"")\n\n    def on_stage_start(self, runner: IRunner):\n        """"""Notify about starting a new stage.""""""\n        if self.log_on_stage_start:\n            text = f""{runner.stage_name} stage was started""\n\n            self._send_text(text)\n\n    def on_loader_start(self, runner: IRunner):\n        """"""Notify about starting running the new loader.""""""\n        if self.log_on_loader_start:\n            text = (\n                f""{runner.loader_name} {runner.global_epoch} epoch has started""\n            )\n\n            self._send_text(text)\n\n    def on_loader_end(self, runner: IRunner):\n        """"""Translate ``runner.metric_manager`` to telegram channel.""""""\n        if self.log_on_loader_end:\n            metrics = runner.loader_metrics\n\n            if self.metrics_to_log is None:\n                metrics_to_log = sorted(metrics.keys())\n            else:\n                metrics_to_log = self.metrics_to_log\n\n            rows: List[str] = [\n                f""{runner.loader_name} {runner.global_epoch}""\n                f"" epoch was finished:""\n            ]\n\n            for name in metrics_to_log:\n                if name in metrics:\n                    rows.append(utils.format_metric(name, metrics[name]))\n\n            text = ""\\n"".join(rows)\n\n            self._send_text(text)\n\n    def on_stage_end(self, runner: IRunner):\n        """"""Notify about finishing a stage.""""""\n        if self.log_on_stage_end:\n            text = f""{runner.stage_name} stage was finished""\n\n            self._send_text(text)\n\n    def on_exception(self, runner: IRunner):\n        """"""Notify about raised ``Exception``.""""""\n        if self.log_on_exception:\n            exception = runner.exception\n            if utils.is_exception(exception) and not isinstance(\n                exception, KeyboardInterrupt\n            ):\n                text = (\n                    f""`{type(exception).__name__}` exception was raised:\\n""\n                    f""{exception}""\n                )\n\n                self._send_text(text)\n\n\n__all__ = [""TelegramLogger""]\n'"
catalyst/contrib/dl/callbacks/tracer_callback.py,0,"b'from typing import Union  # isort:skip\n\nfrom pathlib import Path\nimport warnings\n\nfrom catalyst.core.callback import Callback, CallbackNode, CallbackOrder\nfrom catalyst.core.runner import IRunner\nfrom catalyst.dl.utils import save_traced_model, trace_model_from_runner\n\n\nclass TracerCallback(Callback):\n    """"""\n    Traces model during training if `metric` provided is improved.\n    """"""\n\n    def __init__(\n        self,\n        metric: str = ""loss"",\n        minimize: bool = True,\n        min_delta: float = 1e-6,\n        mode: str = ""best"",\n        do_once: bool = True,\n        method_name: str = ""forward"",\n        requires_grad: bool = False,\n        opt_level: str = None,\n        trace_mode: str = ""eval"",\n        out_dir: Union[str, Path] = None,\n        out_model: Union[str, Path] = None,\n    ):\n        """"""\n        Args:\n            metric (str): Metric key we should trace model based on\n            minimize (bool): Whether do we minimize metric or not\n            min_delta (float): Minimum value of change for metric to be\n                considered as improved\n            mode (str): One of `best` or `last`\n            do_once (str): Whether do we trace once per stage or every epoch\n            method_name (str): Model\'s method name that will be\n                used as entrypoint during tracing\n            requires_grad (bool): Flag to use grads\n            opt_level (str): AMP FP16 init level\n            trace_mode (str): Mode for model to trace\n                (``train`` or ``eval``)\n            out_dir (Union[str, Path]): Directory to save model to\n            out_model (Union[str, Path]): Path to save model to\n                (overrides `out_dir` argument)\n        """"""\n        super().__init__(order=CallbackOrder.External, node=CallbackNode.All)\n\n        if trace_mode not in [""train"", ""eval""]:\n            raise ValueError(\n                f""Unknown `trace_mode` \'{trace_mode}\'. ""\n                f""Must be \'eval\' or \'train\'""\n            )\n\n        if mode not in [""best"", ""last""]:\n            raise ValueError(\n                f""Unknown `mode` \'{mode}\'. "" f""Must be \'best\' or \'last\'""\n            )\n\n        if opt_level is not None:\n            warnings.warn(\n                ""TracerCallback: ""\n                ""`opt_level` is not supported yet, ""\n                ""model will be traced as is"",\n                stacklevel=2,\n            )\n\n        self.metric = metric\n        self.mode = mode\n        self.do_once = do_once\n        self.best_score = None\n        self.is_better = None\n        if minimize:\n            self.is_better = lambda score, best: score <= (best - min_delta)\n        else:\n            self.is_better = lambda score, best: score >= (best + min_delta)\n\n        self.requires_grad = requires_grad\n        self.method_name = method_name\n        self.trace_mode = trace_mode\n        self.opt_level = None\n\n        if out_model is not None:\n            out_model = Path(out_model)\n        self.out_model = out_model\n\n        if out_dir is not None:\n            out_dir = Path(out_dir)\n        self.out_dir = out_dir\n\n    def _trace(self, runner: IRunner):\n        """"""\n        Performing model tracing on epoch end if condition metric is improved.\n\n        Args:\n            runner (IRunner): Current runner\n        """"""\n        if self.opt_level is not None:\n            device = ""cuda""\n        else:\n            device = ""cpu""\n\n        # the only case we need to restore model from previous checkpoint\n        # is when we need to trace best model only once in the end of stage\n        checkpoint_name_to_restore = None\n        if self.do_once and self.mode == ""best"":\n            checkpoint_name_to_restore = ""best""\n\n        traced_model = trace_model_from_runner(\n            runner=runner,\n            checkpoint_name=checkpoint_name_to_restore,\n            method_name=self.method_name,\n            mode=self.trace_mode,\n            requires_grad=self.requires_grad,\n            opt_level=self.opt_level,\n            device=device,\n        )\n\n        save_traced_model(\n            model=traced_model,\n            logdir=runner.logdir,\n            checkpoint_name=self.mode,\n            method_name=self.method_name,\n            mode=self.trace_mode,\n            requires_grad=self.requires_grad,\n            opt_level=self.opt_level,\n            out_model=self.out_model,\n            out_dir=self.out_dir,\n        )\n\n    def on_epoch_end(self, runner: IRunner):\n        """"""\n        Performing model tracing on epoch end if condition metric is improved\n\n        Args:\n            runner (IRunner): Current runner\n        """"""\n        if not self.do_once:\n            if self.mode == ""best"":\n                score = runner.valid_metrics[self.metric]\n\n                if self.best_score is None:\n                    self.best_score = score\n\n                # TODO: EarlyStoppingCallback and TracerCallback\n                #  will never work very first epoch\n                if self.is_better(score, self.best_score):\n                    self.best_score = score\n                    self._trace(runner)\n            else:\n                self._trace(runner)\n\n    def on_stage_end(self, runner: IRunner):\n        """"""\n        Performing model tracing on stage end if `do_once` is True.\n\n        Args:\n            runner (IRunner): Current runner\n        """"""\n        if self.do_once:\n            self._trace(runner)\n\n\n__all__ = [""TracerCallback""]\n'"
catalyst/contrib/dl/callbacks/visdom_logger.py,0,"b'from typing import Dict, List, Union\nfrom collections import Counter\nimport logging\nimport queue\nimport threading\nimport time\n\nfrom alchemy.logger import Logger\nimport visdom\n\nfrom catalyst.core.callback import (\n    Callback,\n    CallbackNode,\n    CallbackOrder,\n    CallbackScope,\n)\nfrom catalyst.core.runner import IRunner\n\n\nclass Visdom(Logger):\n    """"""Logger, translates ``runner.*_metrics`` to Visdom.\n    Read about Visdom here https://github.com/facebookresearch/visdom\n\n    Example:\n        .. code-block:: python\n\n            VisdomLogger(\n                env_name=""..."", # enviroment name\n                server=""localhost"", # visdom server name\n                port=8097, # visdom server port\n            )\n    """"""\n\n    def __init__(\n        self,\n        env_name: str,\n        batch_size: int = None,\n        server: str = ""localhost"",\n        port: int = 8097,\n        log_to_filename: str = None,\n        username: str = None,\n        password: str = None,\n    ):\n        """"""\n        Args:\n            env_name (str): Environment name to plot to when\n                no env is provided (default: main)\n            batch_size (int): batch_size for log_on_batch_end\n            server (str): the hostname of your\n                visdom server (default: \'http://localhost\')\n            port (str): the port for your visdom server (default: 8097)\n            log_to_filename (str): logs per-epoch metrics if set True\n            username (str): username to use for authentication,\n                if server started with -enable_login (default: None)\n            password (str): password to use for authentication,\n                if server started with -enable_login (default: None)\n        """"""\n        self._batch_size = max(int(batch_size or int(1e3)), 1)\n        self._counters = Counter()\n        self._queue = queue.Queue()\n        self._thread = threading.Thread(target=self._run_worker)\n        self._thread.start()\n        try:\n            self.viz = visdom.Visdom(\n                server=server,\n                port=port,\n                env=env_name,\n                log_to_filename=log_to_filename,\n                username=username,\n                password=password,\n            )\n            startup_sec = 1\n            while not self.viz.check_connection() and startup_sec > 0:\n                time.sleep(0.1)\n                startup_sec -= 0.1\n            assert (\n                self.viz.check_connection()\n            ), ""No connection could be formed quickly""\n        except BaseException as e:\n            logging.error(\n                ""The visdom experienced an exception while""\n                + ""running: {}"".format(repr(e))\n            )\n\n    def _run_worker(self):\n        """"""Runs worker to gather batch statistics.""""""\n        running = True\n        while running:\n            batch = []\n            try:\n                while len(batch) < self._batch_size:\n                    if batch:\n                        msg = self._queue.get_nowait()\n                    else:\n                        msg = self._queue.get()\n                    if msg is None:\n                        running = False\n                        break\n                    batch.append(msg)\n            except queue.Empty:\n                pass\n            if batch:\n                self.plot_lines(batch)\n\n    def plot_lines(self, batch: List[Dict]):\n        """"""Plots vales from batch statistics.\n\n        Args:\n            batch (List[Dict]): List with dictionaries from log_scalar\n        """"""\n        for msg in batch:\n            opts = {\n                ""xlabel"": ""epochs"",\n                ""legend"": [""train"", ""valid""],\n                ""ylabel"": msg[""name""],\n                ""title"": msg[""name""],\n            }\n            self.viz.line(\n                X=[self._counters[msg[""full_name""]]],\n                Y=[msg[""value""]],\n                win=msg[""name""],\n                name=msg[""mode""],\n                update=""append"",\n                opts=opts,\n            )\n\n    def log_scalar(\n        self, name: str, mode: str, full_name: str, value: Union[int, float],\n    ):\n        """"""Logs scalar.\n\n        Args:\n            name (str): Environment name to plot to when\n                no env is provided (default: main)\n            mode (str): Metric\'s mode (example: train)\n            full_name (str): Full metric name\n            value (Union[int, float]): Metric\'s value\n        """"""\n        self._queue.put(\n            {\n                ""name"": name,\n                ""full_name"": full_name,\n                ""mode"": mode,\n                ""value"": value,\n                ""step"": self._counters[full_name],\n            }\n        )\n        self._counters[full_name] += 1\n\n\nclass VisdomLogger(Callback):\n    """"""Logger callback, translates ``runner.*_metrics`` to Visdom.\n    Read about Visdom here https://github.com/facebookresearch/visdom\n\n    Example:\n        .. code-block:: python\n\n            from catalyst.dl import SupervisedRunner, VisdomLogger\n\n            runner = SupervisedRunner()\n\n            runner.train(\n                model=model,\n                criterion=criterion,\n                optimizer=optimizer,\n                loaders=loaders,\n                logdir=logdir,\n                num_epochs=num_epochs,\n                verbose=True,\n                callbacks={\n                    ""logger"": VisdomLogger(\n                        env_name=""..."", # enviroment name\n                        server=""localhost"", # visdom server name\n                        port=8097, # visdom server port\n                    )\n                }\n            )\n    """"""\n\n    def __init__(\n        self,\n        metric_names: List[str] = None,\n        log_on_batch_end: bool = False,\n        log_on_epoch_end: bool = True,\n        **logging_params,\n    ):\n        """"""\n        Args:\n            metric_names (List[str]): list of metric names to log,\n                if none - logs everything\n            log_on_batch_end (bool): logs per-batch metrics if set True\n            log_on_epoch_end (bool): logs per-epoch metrics if set True\n        """"""\n        super().__init__(\n            order=CallbackOrder.Logging,\n            node=CallbackNode.Master,\n            scope=CallbackScope.Experiment,\n        )\n        self.metrics_to_log = metric_names\n        self.log_on_batch_end = log_on_batch_end\n        self.log_on_epoch_end = log_on_epoch_end\n\n        if not (self.log_on_batch_end or self.log_on_epoch_end):\n            raise ValueError(""You have to log something!"")\n\n        if (self.log_on_batch_end and not self.log_on_epoch_end) or (\n            not self.log_on_batch_end and self.log_on_epoch_end\n        ):\n            self.batch_log_suffix = """"\n            self.epoch_log_suffix = """"\n        else:\n            self.batch_log_suffix = ""_batch""\n            self.epoch_log_suffix = ""_epoch""\n        self.logger = Visdom(**logging_params)\n\n    def _log_metrics(\n        self, metrics: Dict[str, float], step: int, mode: str, suffix=""""\n    ):\n        """"""Translate batch metrics to Visdom logger.\n\n        Args:\n            metrics (Dict[str, float]): Metrics from Catalyst\n            step (int): Iteration step from Catalyst\n            mode (str): Metric\'s mode (example: train)\n            suffix (str): Additional suffix\n        """"""\n        if self.metrics_to_log is None:\n            metrics_to_log = sorted(metrics.keys())\n        else:\n            metrics_to_log = self.metrics_to_log\n\n        for name in metrics_to_log:\n            if name in metrics:\n                # Renaming catalyst metric names to visdom formatting\n                real_mode = name.split(""_"")[0]\n                splitted_name = name.split(real_mode + ""_"")[-1]\n                metric_name = f""{splitted_name}{suffix}""\n                full_metric_name = f""{real_mode}/{metric_name}""\n                metric_value = metrics[name]\n                # Log values\n                self.logger.log_scalar(\n                    metric_name, real_mode, full_metric_name, metric_value\n                )\n\n    def __del__(self):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        self.logger.close()\n\n    def on_batch_end(self, runner: IRunner):\n        """"""Translate batch metrics to Visdom.""""""\n        if self.log_on_batch_end:\n            mode = runner.loader_name\n            metrics_ = runner.batch_metrics\n            self._log_metrics(\n                metrics=metrics_,\n                step=runner.global_sample_step,\n                mode=mode,\n                suffix=self.batch_log_suffix,\n            )\n\n    def on_epoch_end(self, runner: IRunner):\n        """"""Translate epoch metrics to Visdom.""""""\n        if self.log_on_epoch_end:\n            self._log_metrics(\n                metrics=runner.epoch_metrics,\n                step=runner.global_epoch,\n                mode=runner.loader_name,\n                suffix=self.epoch_log_suffix,\n            )\n\n\n__all__ = [""VisdomLogger""]\n'"
catalyst/contrib/dl/callbacks/wandb_logger.py,5,"b'from typing import Dict, List\n\nimport wandb\n\nfrom catalyst import utils\nfrom catalyst.core.callback import (\n    Callback,\n    CallbackNode,\n    CallbackOrder,\n    CallbackScope,\n)\nfrom catalyst.core.runner import IRunner\n\n\nclass WandbLogger(Callback):\n    """"""Logger callback, translates ``runner.*_metrics`` to Weights & Biases.\n    Read about Weights & Biases here https://docs.wandb.com/\n\n    Example:\n        .. code-block:: python\n\n            from catalyst import dl\n            import torch\n            import torch.nn as nn\n            import torch.optim as optim\n            from torch.utils.data import DataLoader, TensorDataset\n\n            class Projector(nn.Module):\n                def __init__(self, input_size):\n                    super().__init__()\n                    self.linear = nn.Linear(input_size, 1)\n\n                def forward(self, X):\n                    return self.linear(X).squeeze(-1)\n\n            X = torch.rand(16, 10)\n            y = torch.rand(X.shape[0])\n            model = Projector(X.shape[1])\n            dataset = TensorDataset(X, y)\n            loader = DataLoader(dataset, batch_size=8)\n            runner = dl.SupervisedRunner()\n\n            runner.train(\n                model=model,\n                loaders={\n                    ""train"": loader,\n                    ""valid"": loader\n                },\n                criterion=nn.MSELoss(),\n                optimizer=optim.Adam(model.parameters()),\n                logdir=""log_example"",\n                callbacks=[\n                    dl.callbacks.WandbLogger(\n                        project=""wandb_logger_example""\n                    )\n                ],\n                num_epochs=10\n            )\n    """"""\n\n    def __init__(\n        self,\n        metric_names: List[str] = None,\n        log_on_batch_end: bool = False,\n        log_on_epoch_end: bool = True,\n        **logging_params,\n    ):\n        """"""\n        Args:\n            metric_names (List[str]): list of metric names to log,\n                if None - logs everything\n            log_on_batch_end (bool): logs per-batch metrics if set True\n            log_on_epoch_end (bool): logs per-epoch metrics if set True\n            **logging_params: any parameters of function `wandb.init`\n                except `reinit` which is automatically set to `True`\n                and `dir` which is set to `<logdir>`\n        """"""\n        super().__init__(\n            order=CallbackOrder.Logging,\n            node=CallbackNode.Master,\n            scope=CallbackScope.Experiment,\n        )\n        self.metrics_to_log = metric_names\n        self.log_on_batch_end = log_on_batch_end\n        self.log_on_epoch_end = log_on_epoch_end\n\n        if not (self.log_on_batch_end or self.log_on_epoch_end):\n            raise ValueError(""You have to log something!"")\n\n        if (self.log_on_batch_end and not self.log_on_epoch_end) or (\n            not self.log_on_batch_end and self.log_on_epoch_end\n        ):\n            self.batch_log_suffix = """"\n            self.epoch_log_suffix = """"\n        else:\n            self.batch_log_suffix = ""_batch""\n            self.epoch_log_suffix = ""_epoch""\n\n        self.logging_params = logging_params\n\n    def _log_metrics(\n        self,\n        metrics: Dict[str, float],\n        step: int,\n        mode: str,\n        suffix="""",\n        commit=True,\n    ):\n        if self.metrics_to_log is None:\n            metrics_to_log = sorted(metrics.keys())\n        else:\n            metrics_to_log = self.metrics_to_log\n\n        def key_locate(key: str):\n            """"""\n            Wandb uses first symbol _ for it service purposes\n            because of that fact, we can not send original metric names\n\n            Args:\n                key: metric name\n\n            Returns:\n                formatted metric name\n            """"""\n            if key.startswith(""_""):\n                return key[1:]\n            return key\n\n        metrics = {\n            f""{key_locate(key)}/{mode}{suffix}"": value\n            for key, value in metrics.items()\n            if key in metrics_to_log\n        }\n        wandb.log(metrics, step=step, commit=commit)\n\n    def on_stage_start(self, runner: IRunner):\n        """"""Initialize Weights & Biases.""""""\n        wandb.init(**self.logging_params, reinit=True, dir=str(runner.logdir))\n\n    def on_stage_end(self, runner: IRunner):\n        """"""Finish logging to Weights & Biases.""""""\n        wandb.join()\n\n    def on_batch_end(self, runner: IRunner):\n        """"""Translate batch metrics to Weights & Biases.""""""\n        if self.log_on_batch_end:\n            mode = runner.loader_name\n            metrics_ = runner.batch_metrics\n            self._log_metrics(\n                metrics=metrics_,\n                step=runner.global_sample_step,\n                mode=mode,\n                suffix=self.batch_log_suffix,\n                commit=True,\n            )\n\n    def on_loader_end(self, runner: IRunner):\n        """"""Translate loader metrics to Weights & Biases.""""""\n        if self.log_on_epoch_end:\n            mode = runner.loader_name\n            metrics_ = runner.loader_metrics\n            self._log_metrics(\n                metrics=metrics_,\n                step=runner.global_epoch,\n                mode=mode,\n                suffix=self.epoch_log_suffix,\n                commit=False,\n            )\n\n    def on_epoch_end(self, runner: IRunner):\n        """"""Translate epoch metrics to Weights & Biases.""""""\n        extra_mode = ""_base""\n        splitted_epoch_metrics = utils.split_dict_to_subdicts(\n            dct=runner.epoch_metrics,\n            prefixes=list(runner.loaders.keys()),\n            extra_key=extra_mode,\n        )\n\n        if self.log_on_epoch_end:\n            self._log_metrics(\n                metrics=splitted_epoch_metrics[extra_mode],\n                step=runner.global_epoch,\n                mode=extra_mode,\n                suffix=self.epoch_log_suffix,\n                commit=True,\n            )\n\n\n__all__ = [""WandbLogger""]\n'"
catalyst/contrib/dl/experiment/__init__.py,0,b''
catalyst/contrib/dl/runner/__init__.py,0,b''
catalyst/contrib/models/cv/__init__.py,0,b'# flake8: noqa\n# isort:skip_file\n\nfrom .encoders import *\nfrom .classification import *\nfrom .segmentation import *\n'
catalyst/contrib/models/nlp/__init__.py,0,b'# flake8: noqa\n# isort:skip_file\n\nfrom .encoders import *\nfrom .classification import *\n'
catalyst/contrib/models/tests/__init__.py,0,b''
catalyst/contrib/models/tests/test_hydra.py,20,"b'from collections import OrderedDict\nimport copy\nfrom pathlib import Path\n\nimport pytest\n\nimport torch\nfrom torch import nn\n\nfrom catalyst import utils\nfrom catalyst.contrib.models import Hydra, SequentialNet\nfrom catalyst.contrib.nn.modules import Normalize\n\n\ndef _pop_normalization(dct):\n    for values in dct.values():\n        if isinstance(values, dict):\n            values.pop(""normalize_output"", None)\n            _pop_normalization(values)\n\n\ndef _check_lists(left, right):\n    assert sorted(left) == sorted(right)\n\n\ndef _check_named_parameters(left, right):\n    left_keys = dict(left.named_parameters()).keys()\n    right_keys = dict(right.named_parameters()).keys()\n    _check_lists(left_keys, right_keys)\n\n\ndef test_config1():\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    config1 = {\n        ""encoder_params"": {\n            ""hiddens"": [16, 16],\n            ""layer_fn"": {""module"": ""Linear"", ""bias"": False},\n            ""norm_fn"": ""LayerNorm"",\n        },\n        ""heads_params"": {\n            ""head1"": {\n                ""hiddens"": [2],\n                ""layer_fn"": {""module"": ""Linear"", ""bias"": True},\n            },\n            ""_head2"": {\n                ""_hidden"": {\n                    ""hiddens"": [16],\n                    ""layer_fn"": {""module"": ""Linear"", ""bias"": False},\n                },\n                ""head2_1"": {\n                    ""hiddens"": [32],\n                    ""layer_fn"": {""module"": ""Linear"", ""bias"": True},\n                    ""normalize_output"": True,\n                },\n                ""_head2_2"": {\n                    ""_hidden"": {\n                        ""hiddens"": [16, 16, 16],\n                        ""layer_fn"": {""module"": ""Linear"", ""bias"": False},\n                    },\n                    ""head2_2_1"": {\n                        ""hiddens"": [32],\n                        ""layer_fn"": {""module"": ""Linear"", ""bias"": True},\n                        ""normalize_output"": False,\n                    },\n                },\n            },\n        },\n        ""embedders_params"": {\n            ""target1"": {""num_embeddings"": 2, ""normalize_output"": True},\n            ""target2"": {""num_embeddings"": 2, ""normalize_output"": False},\n        },\n    }\n\n    hydra = Hydra.get_from_params(**config1)\n\n    config1_ = copy.deepcopy(config1)\n    _pop_normalization(config1_)\n    encoder_params = config1_[""encoder_params""]\n    heads_params = config1_[""heads_params""]\n    heads_params[""head1""][""hiddens""].insert(0, 16)\n    heads_params[""_head2""][""_hidden""][""hiddens""].insert(0, 16)\n    heads_params[""_head2""][""head2_1""][""hiddens""].insert(0, 16)\n    heads_params[""_head2""][""_head2_2""][""_hidden""][""hiddens""].insert(0, 16)\n    heads_params[""_head2""][""_head2_2""][""head2_2_1""][""hiddens""].insert(0, 16)\n\n    net = nn.ModuleDict(\n        {\n            ""encoder"": SequentialNet(**encoder_params),\n            ""embedders"": nn.ModuleDict(\n                {\n                    ""target1"": nn.Sequential(\n                        OrderedDict(\n                            [\n                                (\n                                    ""embedding"",\n                                    nn.Embedding(\n                                        embedding_dim=16, num_embeddings=2\n                                    ),\n                                ),\n                                (""normalize"", Normalize()),\n                            ]\n                        )\n                    ),\n                    ""target2"": nn.Sequential(\n                        OrderedDict(\n                            [\n                                (\n                                    ""embedding"",\n                                    nn.Embedding(\n                                        embedding_dim=16, num_embeddings=2\n                                    ),\n                                ),\n                            ]\n                        )\n                    ),\n                }\n            ),\n            ""heads"": nn.ModuleDict(\n                {\n                    ""head1"": nn.Sequential(\n                        OrderedDict(\n                            [(""net"", SequentialNet(**heads_params[""head1""]))]\n                        )\n                    ),\n                    ""_head2"": nn.ModuleDict(\n                        {\n                            ""_hidden"": nn.Sequential(\n                                OrderedDict(\n                                    [\n                                        (\n                                            ""net"",\n                                            SequentialNet(\n                                                **heads_params[""_head2""][\n                                                    ""_hidden""\n                                                ]\n                                            ),\n                                        )\n                                    ]\n                                )\n                            ),\n                            ""head2_1"": nn.Sequential(\n                                OrderedDict(\n                                    [\n                                        (\n                                            ""net"",\n                                            SequentialNet(\n                                                **heads_params[""_head2""][\n                                                    ""head2_1""\n                                                ]\n                                            ),\n                                        ),\n                                        (""normalize"", Normalize()),\n                                    ]\n                                )\n                            ),\n                            ""_head2_2"": nn.ModuleDict(\n                                {\n                                    ""_hidden"": nn.Sequential(\n                                        OrderedDict(\n                                            [\n                                                (\n                                                    ""net"",\n                                                    SequentialNet(\n                                                        **heads_params[\n                                                            ""_head2""\n                                                        ][""_head2_2""][\n                                                            ""_hidden""\n                                                        ]\n                                                    ),\n                                                )\n                                            ]\n                                        )\n                                    ),\n                                    ""head2_2_1"": nn.Sequential(\n                                        OrderedDict(\n                                            [\n                                                (\n                                                    ""net"",\n                                                    SequentialNet(\n                                                        **heads_params[\n                                                            ""_head2""\n                                                        ][""_head2_2""][\n                                                            ""head2_2_1""\n                                                        ]\n                                                    ),\n                                                )\n                                            ]\n                                        )\n                                    ),\n                                }\n                            ),\n                        }\n                    ),\n                }\n            ),\n        }\n    )\n\n    _check_named_parameters(hydra.encoder, net[""encoder""])\n    _check_named_parameters(hydra.heads, net[""heads""])\n    _check_named_parameters(hydra.embedders, net[""embedders""])\n\n    input_ = torch.rand(1, 16)\n\n    output_kv = hydra(input_)\n    assert (input_ == output_kv[""features""]).sum().item() == 16\n    kv_keys = [\n        ""features"",\n        ""embeddings"",\n        ""head1"",\n        ""_head2/"",\n        ""_head2/head2_1"",\n        ""_head2/_head2_2/"",\n        ""_head2/_head2_2/head2_2_1"",\n    ]\n    _check_lists(output_kv.keys(), kv_keys)\n\n    output_kv = hydra(input_, target1=torch.ones(1, 2).long())\n    kv_keys = [\n        ""features"",\n        ""embeddings"",\n        ""head1"",\n        ""_head2/"",\n        ""_head2/head2_1"",\n        ""_head2/_head2_2/"",\n        ""_head2/_head2_2/head2_2_1"",\n        ""target1_embeddings"",\n    ]\n    _check_lists(output_kv.keys(), kv_keys)\n\n    output_kv = hydra(input_, target2=torch.ones(1, 2).long())\n    kv_keys = [\n        ""features"",\n        ""embeddings"",\n        ""head1"",\n        ""_head2/"",\n        ""_head2/head2_1"",\n        ""_head2/_head2_2/"",\n        ""_head2/_head2_2/head2_2_1"",\n        ""target2_embeddings"",\n    ]\n    _check_lists(output_kv.keys(), kv_keys)\n\n    output_kv = hydra(\n        input_,\n        target1=torch.ones(1, 2).long(),\n        target2=torch.ones(1, 2).long(),\n    )\n    kv_keys = [\n        ""features"",\n        ""embeddings"",\n        ""head1"",\n        ""_head2/"",\n        ""_head2/head2_1"",\n        ""_head2/_head2_2/"",\n        ""_head2/_head2_2/head2_2_1"",\n        ""target1_embeddings"",\n        ""target2_embeddings"",\n    ]\n    _check_lists(output_kv.keys(), kv_keys)\n\n    output_tuple = hydra.forward_tuple(input_)\n    assert len(output_tuple) == 5\n    assert (output_tuple[0] == output_kv[""features""]).sum().item() == 16\n    assert (output_tuple[1] == output_kv[""embeddings""]).sum().item() == 16\n\n\ndef test_config2():\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    config2 = {\n        ""in_features"": 16,\n        ""heads_params"": {\n            ""head1"": {\n                ""hiddens"": [2],\n                ""layer_fn"": {""module"": ""Linear"", ""bias"": True},\n            },\n            ""_head2"": {\n                ""_hidden"": {\n                    ""hiddens"": [16],\n                    ""layer_fn"": {""module"": ""Linear"", ""bias"": False},\n                },\n                ""head2_1"": {\n                    ""hiddens"": [32],\n                    ""layer_fn"": {""module"": ""Linear"", ""bias"": True},\n                    ""normalize_output"": True,\n                },\n                ""_head2_2"": {\n                    ""_hidden"": {\n                        ""hiddens"": [16, 16, 16],\n                        ""layer_fn"": {""module"": ""Linear"", ""bias"": False},\n                    },\n                    ""head2_2_1"": {\n                        ""hiddens"": [32],\n                        ""layer_fn"": {""module"": ""Linear"", ""bias"": True},\n                        ""normalize_output"": False,\n                    },\n                },\n            },\n        },\n    }\n\n    hydra = Hydra.get_from_params(**config2)\n\n    config2_ = copy.deepcopy(config2)\n    _pop_normalization(config2_)\n    heads_params = config2_[""heads_params""]\n    heads_params[""head1""][""hiddens""].insert(0, 16)\n    heads_params[""_head2""][""_hidden""][""hiddens""].insert(0, 16)\n    heads_params[""_head2""][""head2_1""][""hiddens""].insert(0, 16)\n    heads_params[""_head2""][""_head2_2""][""_hidden""][""hiddens""].insert(0, 16)\n    heads_params[""_head2""][""_head2_2""][""head2_2_1""][""hiddens""].insert(0, 16)\n\n    net = nn.ModuleDict(\n        {\n            ""encoder"": nn.Sequential(),\n            ""heads"": nn.ModuleDict(\n                {\n                    ""head1"": nn.Sequential(\n                        OrderedDict(\n                            [(""net"", SequentialNet(**heads_params[""head1""]))]\n                        )\n                    ),\n                    ""_head2"": nn.ModuleDict(\n                        {\n                            ""_hidden"": nn.Sequential(\n                                OrderedDict(\n                                    [\n                                        (\n                                            ""net"",\n                                            SequentialNet(\n                                                **heads_params[""_head2""][\n                                                    ""_hidden""\n                                                ]\n                                            ),\n                                        )\n                                    ]\n                                )\n                            ),\n                            ""head2_1"": nn.Sequential(\n                                OrderedDict(\n                                    [\n                                        (\n                                            ""net"",\n                                            SequentialNet(\n                                                **heads_params[""_head2""][\n                                                    ""head2_1""\n                                                ]\n                                            ),\n                                        ),\n                                        (""normalize"", Normalize()),\n                                    ]\n                                )\n                            ),\n                            ""_head2_2"": nn.ModuleDict(\n                                {\n                                    ""_hidden"": nn.Sequential(\n                                        OrderedDict(\n                                            [\n                                                (\n                                                    ""net"",\n                                                    SequentialNet(\n                                                        **heads_params[\n                                                            ""_head2""\n                                                        ][""_head2_2""][\n                                                            ""_hidden""\n                                                        ]\n                                                    ),\n                                                )\n                                            ]\n                                        )\n                                    ),\n                                    ""head2_2_1"": nn.Sequential(\n                                        OrderedDict(\n                                            [\n                                                (\n                                                    ""net"",\n                                                    SequentialNet(\n                                                        **heads_params[\n                                                            ""_head2""\n                                                        ][""_head2_2""][\n                                                            ""head2_2_1""\n                                                        ]\n                                                    ),\n                                                )\n                                            ]\n                                        )\n                                    ),\n                                }\n                            ),\n                        }\n                    ),\n                }\n            ),\n        }\n    )\n\n    _check_named_parameters(hydra.encoder, net[""encoder""])\n    _check_named_parameters(hydra.heads, net[""heads""])\n    assert hydra.embedders == {}\n\n    input_ = torch.rand(1, 16)\n\n    output_kv = hydra(input_)\n    assert (input_ == output_kv[""features""]).sum().item() == 16\n    assert (input_ == output_kv[""embeddings""]).sum().item() == 16\n    kv_keys = [\n        ""features"",\n        ""embeddings"",\n        ""head1"",\n        ""_head2/"",\n        ""_head2/head2_1"",\n        ""_head2/_head2_2/"",\n        ""_head2/_head2_2/head2_2_1"",\n    ]\n    _check_lists(output_kv.keys(), kv_keys)\n\n    with pytest.raises(KeyError):\n        output_kv = hydra(input_, target1=torch.ones(1, 2).long())\n    with pytest.raises(KeyError):\n        output_kv = hydra(input_, target2=torch.ones(1, 2).long())\n    with pytest.raises(KeyError):\n        output_kv = hydra(\n            input_,\n            target1=torch.ones(1, 2).long(),\n            target2=torch.ones(1, 2).long(),\n        )\n\n    output_tuple = hydra.forward_tuple(input_)\n    assert len(output_tuple) == 5\n    assert (output_tuple[0] == output_kv[""features""]).sum().item() == 16\n    assert (output_tuple[1] == output_kv[""embeddings""]).sum().item() == 16\n\n\ndef test_config3():\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    config_path = Path(__file__).absolute().parent / ""config3.yml""\n    config3 = utils.load_config(config_path)[""model_params""]\n\n    hydra = Hydra.get_from_params(**config3)\n\n    config3_ = copy.deepcopy(config3)\n    _pop_normalization(config3_)\n    encoder_params = config3_[""encoder_params""]\n    heads_params = config3_[""heads_params""]\n    heads_params[""head1""][""hiddens""].insert(0, 16)\n    heads_params[""_head2""][""_hidden""][""hiddens""].insert(0, 16)\n    heads_params[""_head2""][""head2_1""][""hiddens""].insert(0, 16)\n    heads_params[""_head2""][""_head2_2""][""_hidden""][""hiddens""].insert(0, 16)\n    heads_params[""_head2""][""_head2_2""][""head2_2_1""][""hiddens""].insert(0, 16)\n\n    net = nn.ModuleDict(\n        {\n            ""encoder"": SequentialNet(**encoder_params),\n            ""embedders"": nn.ModuleDict(\n                {\n                    ""target1"": nn.Sequential(\n                        OrderedDict(\n                            [\n                                (\n                                    ""embedding"",\n                                    nn.Embedding(\n                                        embedding_dim=16, num_embeddings=2\n                                    ),\n                                ),\n                                (""normalize"", Normalize()),\n                            ]\n                        )\n                    ),\n                    ""target2"": nn.Sequential(\n                        OrderedDict(\n                            [\n                                (\n                                    ""embedding"",\n                                    nn.Embedding(\n                                        embedding_dim=16, num_embeddings=2\n                                    ),\n                                ),\n                            ]\n                        )\n                    ),\n                }\n            ),\n            ""heads"": nn.ModuleDict(\n                {\n                    ""head1"": nn.Sequential(\n                        OrderedDict(\n                            [(""net"", SequentialNet(**heads_params[""head1""]))]\n                        )\n                    ),\n                    ""_head2"": nn.ModuleDict(\n                        {\n                            ""_hidden"": nn.Sequential(\n                                OrderedDict(\n                                    [\n                                        (\n                                            ""net"",\n                                            SequentialNet(\n                                                **heads_params[""_head2""][\n                                                    ""_hidden""\n                                                ]\n                                            ),\n                                        )\n                                    ]\n                                )\n                            ),\n                            ""head2_1"": nn.Sequential(\n                                OrderedDict(\n                                    [\n                                        (\n                                            ""net"",\n                                            SequentialNet(\n                                                **heads_params[""_head2""][\n                                                    ""head2_1""\n                                                ]\n                                            ),\n                                        ),\n                                        (""normalize"", Normalize()),\n                                    ]\n                                )\n                            ),\n                            ""_head2_2"": nn.ModuleDict(\n                                {\n                                    ""_hidden"": nn.Sequential(\n                                        OrderedDict(\n                                            [\n                                                (\n                                                    ""net"",\n                                                    SequentialNet(\n                                                        **heads_params[\n                                                            ""_head2""\n                                                        ][""_head2_2""][\n                                                            ""_hidden""\n                                                        ]\n                                                    ),\n                                                )\n                                            ]\n                                        )\n                                    ),\n                                    ""head2_2_1"": nn.Sequential(\n                                        OrderedDict(\n                                            [\n                                                (\n                                                    ""net"",\n                                                    SequentialNet(\n                                                        **heads_params[\n                                                            ""_head2""\n                                                        ][""_head2_2""][\n                                                            ""head2_2_1""\n                                                        ]\n                                                    ),\n                                                )\n                                            ]\n                                        )\n                                    ),\n                                }\n                            ),\n                        }\n                    ),\n                }\n            ),\n        }\n    )\n\n    _check_named_parameters(hydra.encoder, net[""encoder""])\n    _check_named_parameters(hydra.heads, net[""heads""])\n    _check_named_parameters(hydra.embedders, net[""embedders""])\n\n    input_ = torch.rand(1, 16)\n\n    output_kv = hydra(input_)\n    assert (input_ == output_kv[""features""]).sum().item() == 16\n    kv_keys = [\n        ""features"",\n        ""embeddings"",\n        ""head1"",\n        ""_head2/"",\n        ""_head2/head2_1"",\n        ""_head2/_head2_2/"",\n        ""_head2/_head2_2/head2_2_1"",\n    ]\n    _check_lists(output_kv.keys(), kv_keys)\n\n    output_kv = hydra(input_, target1=torch.ones(1, 2).long())\n    kv_keys = [\n        ""features"",\n        ""embeddings"",\n        ""head1"",\n        ""_head2/"",\n        ""_head2/head2_1"",\n        ""_head2/_head2_2/"",\n        ""_head2/_head2_2/head2_2_1"",\n        ""target1_embeddings"",\n    ]\n    _check_lists(output_kv.keys(), kv_keys)\n\n    output_kv = hydra(input_, target2=torch.ones(1, 2).long())\n    kv_keys = [\n        ""features"",\n        ""embeddings"",\n        ""head1"",\n        ""_head2/"",\n        ""_head2/head2_1"",\n        ""_head2/_head2_2/"",\n        ""_head2/_head2_2/head2_2_1"",\n        ""target2_embeddings"",\n    ]\n    _check_lists(output_kv.keys(), kv_keys)\n\n    output_kv = hydra(\n        input_,\n        target1=torch.ones(1, 2).long(),\n        target2=torch.ones(1, 2).long(),\n    )\n    kv_keys = [\n        ""features"",\n        ""embeddings"",\n        ""head1"",\n        ""_head2/"",\n        ""_head2/head2_1"",\n        ""_head2/_head2_2/"",\n        ""_head2/_head2_2/head2_2_1"",\n        ""target1_embeddings"",\n        ""target2_embeddings"",\n    ]\n    _check_lists(output_kv.keys(), kv_keys)\n\n    output_tuple = hydra.forward_tuple(input_)\n    assert len(output_tuple) == 5\n    assert (output_tuple[0] == output_kv[""features""]).sum().item() == 16\n    assert (output_tuple[1] == output_kv[""embeddings""]).sum().item() == 16\n\n\ndef test_config4():\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    config_path = Path(__file__).absolute().parent / ""config4.yml""\n    config4 = utils.load_config(config_path)[""model_params""]\n\n    with pytest.raises(AssertionError):\n        hydra = Hydra.get_from_params(**config4)\n    config4[""in_features""] = 16\n    hydra = Hydra.get_from_params(**config4)\n\n    config4_ = copy.deepcopy(config4)\n    _pop_normalization(config4_)\n    heads_params = config4_[""heads_params""]\n    heads_params[""head1""][""hiddens""].insert(0, 16)\n    heads_params[""_head2""][""_hidden""][""hiddens""].insert(0, 16)\n    heads_params[""_head2""][""head2_1""][""hiddens""].insert(0, 16)\n    heads_params[""_head2""][""_head2_2""][""_hidden""][""hiddens""].insert(0, 16)\n    heads_params[""_head2""][""_head2_2""][""head2_2_1""][""hiddens""].insert(0, 16)\n\n    net = nn.ModuleDict(\n        {\n            ""encoder"": nn.Sequential(),\n            ""heads"": nn.ModuleDict(\n                {\n                    ""head1"": nn.Sequential(\n                        OrderedDict(\n                            [(""net"", SequentialNet(**heads_params[""head1""]))]\n                        )\n                    ),\n                    ""_head2"": nn.ModuleDict(\n                        {\n                            ""_hidden"": nn.Sequential(\n                                OrderedDict(\n                                    [\n                                        (\n                                            ""net"",\n                                            SequentialNet(\n                                                **heads_params[""_head2""][\n                                                    ""_hidden""\n                                                ]\n                                            ),\n                                        )\n                                    ]\n                                )\n                            ),\n                            ""head2_1"": nn.Sequential(\n                                OrderedDict(\n                                    [\n                                        (\n                                            ""net"",\n                                            SequentialNet(\n                                                **heads_params[""_head2""][\n                                                    ""head2_1""\n                                                ]\n                                            ),\n                                        ),\n                                        (""normalize"", Normalize()),\n                                    ]\n                                )\n                            ),\n                            ""_head2_2"": nn.ModuleDict(\n                                {\n                                    ""_hidden"": nn.Sequential(\n                                        OrderedDict(\n                                            [\n                                                (\n                                                    ""net"",\n                                                    SequentialNet(\n                                                        **heads_params[\n                                                            ""_head2""\n                                                        ][""_head2_2""][\n                                                            ""_hidden""\n                                                        ]\n                                                    ),\n                                                )\n                                            ]\n                                        )\n                                    ),\n                                    ""head2_2_1"": nn.Sequential(\n                                        OrderedDict(\n                                            [\n                                                (\n                                                    ""net"",\n                                                    SequentialNet(\n                                                        **heads_params[\n                                                            ""_head2""\n                                                        ][""_head2_2""][\n                                                            ""head2_2_1""\n                                                        ]\n                                                    ),\n                                                )\n                                            ]\n                                        )\n                                    ),\n                                }\n                            ),\n                        }\n                    ),\n                }\n            ),\n        }\n    )\n\n    _check_named_parameters(hydra.encoder, net[""encoder""])\n    _check_named_parameters(hydra.heads, net[""heads""])\n    assert hydra.embedders == {}\n\n    input_ = torch.rand(1, 16)\n\n    output_kv = hydra(input_)\n    assert (input_ == output_kv[""features""]).sum().item() == 16\n    assert (input_ == output_kv[""embeddings""]).sum().item() == 16\n    kv_keys = [\n        ""features"",\n        ""embeddings"",\n        ""head1"",\n        ""_head2/"",\n        ""_head2/head2_1"",\n        ""_head2/_head2_2/"",\n        ""_head2/_head2_2/head2_2_1"",\n    ]\n    _check_lists(output_kv.keys(), kv_keys)\n\n    with pytest.raises(KeyError):\n        output_kv = hydra(input_, target1=torch.ones(1, 2).long())\n    with pytest.raises(KeyError):\n        output_kv = hydra(input_, target2=torch.ones(1, 2).long())\n    with pytest.raises(KeyError):\n        output_kv = hydra(\n            input_,\n            target1=torch.ones(1, 2).long(),\n            target2=torch.ones(1, 2).long(),\n        )\n\n    output_tuple = hydra.forward_tuple(input_)\n    assert len(output_tuple) == 5\n    assert (output_tuple[0] == output_kv[""features""]).sum().item() == 16\n    assert (output_tuple[1] == output_kv[""embeddings""]).sum().item() == 16\n'"
catalyst/contrib/nn/criterion/__init__.py,1,"b'# flake8: noqa\nfrom torch.nn.modules.loss import *\n\nfrom .ce import (\n    MaskCrossEntropyLoss,\n    NaiveCrossEntropyLoss,\n    SymmetricCrossEntropyLoss,\n)\nfrom .circle import CircleLoss\nfrom .contrastive import (\n    ContrastiveDistanceLoss,\n    ContrastiveEmbeddingLoss,\n    ContrastivePairwiseEmbeddingLoss,\n)\nfrom .dice import BCEDiceLoss, DiceLoss\nfrom .focal import FocalLossBinary, FocalLossMultiClass\nfrom .gan import GradientPenaltyLoss, MeanOutputLoss\nfrom .huber import HuberLoss\nfrom .iou import BCEIoULoss, IoULoss\nfrom .lovasz import (\n    LovaszLossBinary,\n    LovaszLossMultiClass,\n    LovaszLossMultiLabel,\n)\nfrom .margin import MarginLoss\nfrom .triplet import TripletLoss, TripletLossV2, TripletPairwiseEmbeddingLoss\nfrom .wing import WingLoss\n'"
catalyst/contrib/nn/criterion/ce.py,18,"b'import torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\n\nclass NaiveCrossEntropyLoss(nn.Module):\n    """"""@TODO: Docs. Contribution is welcome.""""""\n\n    def __init__(self, size_average=True):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        super().__init__()\n        self.size_average = size_average\n\n    def forward(\n        self, input: torch.Tensor, target: torch.Tensor\n    ) -> torch.Tensor:\n        """"""Calculates loss between ``input`` and ``target`` tensors.\n\n        Args:\n            input (torch.Tensor): input tensor of shape ...\n            target (torch.Tensor): target tensor of shape ...\n\n        @TODO: Docs (add shapes). Contribution is welcome.\n        """"""\n        assert input.size() == target.size()\n        input = F.log_softmax(input)\n        loss = -torch.sum(input * target)\n        loss = loss / input.size()[0] if self.size_average else loss\n        return loss\n\n\nclass SymmetricCrossEntropyLoss(nn.Module):\n    """"""The Symmetric Cross Entropy loss.\n\n    It has been proposed in `Symmetric Cross Entropy for Robust Learning\n    with Noisy Labels`_.\n\n    .. _Symmetric Cross Entropy for Robust Learning with Noisy Labels:\n        https://arxiv.org/abs/1908.06112\n    """"""\n\n    def __init__(self, alpha: float = 1.0, beta: float = 1.0):\n        """"""\n        Args:\n            alpha(float):\n                corresponds to overfitting issue of CE\n            beta(float):\n                corresponds to flexible exploration on the robustness of RCE\n        """"""\n        super(SymmetricCrossEntropyLoss, self).__init__()\n        self.alpha = alpha\n        self.beta = beta\n\n    def forward(\n        self, input: torch.Tensor, target: torch.Tensor\n    ) -> torch.Tensor:\n        """"""Calculates loss between ``input`` and ``target`` tensors.\n\n        Args:\n            input (torch.Tensor): input tensor of size\n                (batch_size, num_classes)\n            target (torch.Tensor): target tensor of size (batch_size), where\n                values of a vector correspond to class index\n        """"""\n        num_classes = input.shape[1]\n        target_one_hot = F.one_hot(target, num_classes).float()\n        assert target_one_hot.shape == input.shape\n\n        input = torch.clamp(input, min=1e-7, max=1.0)\n        target_one_hot = torch.clamp(target_one_hot, min=1e-4, max=1.0)\n\n        cross_entropy = (\n            -torch.sum(target_one_hot * torch.log(input), dim=1)\n        ).mean()\n        reverse_cross_entropy = (\n            -torch.sum(input * torch.log(target_one_hot), dim=1)\n        ).mean()\n        loss = self.alpha * cross_entropy + self.beta * reverse_cross_entropy\n        return loss\n\n\nclass MaskCrossEntropyLoss(torch.nn.CrossEntropyLoss):\n    """"""@TODO: Docs. Contribution is welcome.""""""\n\n    def __init__(\n        self,\n        *args,\n        target_name: str = ""targets"",\n        mask_name: str = ""mask"",\n        **kwargs\n    ):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        super().__init__(*args, **kwargs)\n        self.target_name = target_name\n        self.mask_name = mask_name\n        self.reduction = ""none""\n\n    def forward(\n        self, input: torch.Tensor, target_mask: torch.Tensor\n    ) -> torch.Tensor:\n        """"""Calculates loss between ``input`` and ``target_mask`` tensors.\n\n        @TODO: Docs. Contribution is welcome.\n        """"""\n        target = target_mask[self.target_name]\n        mask = target_mask[self.mask_name]\n\n        loss = super().forward(input, target)\n        loss = torch.mean(loss[mask == 1])\n        return loss\n\n\n__all__ = [\n    ""MaskCrossEntropyLoss"",\n    ""SymmetricCrossEntropyLoss"",\n    ""NaiveCrossEntropyLoss"",\n]\n'"
catalyst/contrib/nn/criterion/circle.py,6,"b'from typing import Tuple\n\nimport torch\nfrom torch import nn, Tensor\n\n\ndef _convert_label_to_similarity(\n    normed_features: Tensor, labels: Tensor\n) -> Tuple[Tensor, Tensor]:\n    similarity_matrix = normed_features @ normed_features.transpose(1, 0)\n    label_matrix = labels.unsqueeze(1) == labels.unsqueeze(0)\n\n    positive_matrix = label_matrix.triu(diagonal=1)\n    negative_matrix = label_matrix.logical_not().triu(diagonal=1)\n\n    similarity_matrix = similarity_matrix.view(-1)\n    positive_matrix = positive_matrix.view(-1)\n    negative_matrix = negative_matrix.view(-1)\n    sp, sn = (\n        similarity_matrix[positive_matrix],\n        similarity_matrix[negative_matrix],\n    )\n    return sp, sn\n\n\nclass CircleLoss(nn.Module):\n    """"""\n    CircleLoss from\n    ""Circle Loss: A Unified Perspective of Pair Similarity Optimization""\n    https://arxiv.org/abs/2002.10857\n\n    Adapter from:\n    https://github.com/TinyZeaMays/CircleLoss\n\n    Example:\n        >>> import torch\n        >>> from torch.nn import functional as F\n        >>> from catalyst.contrib.nn import CircleLoss\n        >>>\n        >>> features = F.normalize(torch.rand(256, 64, requires_grad=True))\n        >>> labels = torch.randint(high=10, size=(256,))\n        >>> criterion = CircleLoss(margin=0.25, gamma=256)\n        >>> criterion(features, labels)\n    """"""\n\n    def __init__(self, margin: float, gamma: float) -> None:\n        """"""\n\n        Args:\n            margin: margin to use\n            gamma: gamma to use\n        """"""\n        super().__init__()\n        self.margin = margin\n        self.gamma = gamma\n        self.soft_plus = nn.Softplus()\n\n    def forward(self, normed_features: Tensor, labels: Tensor) -> Tensor:\n        """"""\n\n        Args:\n            normed_features: batch with samples features of shape\n                [bs; feature_len]\n            labels: batch with samples correct labels of shape [bs; ]\n\n        Returns:\n            (Tensor): circle loss\n        """"""\n        sp, sn = _convert_label_to_similarity(normed_features, labels)\n\n        ap = torch.clamp_min(-sp.detach() + 1 + self.margin, min=0.0)\n        an = torch.clamp_min(sn.detach() + self.margin, min=0.0)\n\n        delta_p = 1 - self.margin\n        delta_n = self.margin\n\n        logit_p = -ap * (sp - delta_p) * self.gamma\n        logit_n = an * (sn - delta_n) * self.gamma\n\n        loss = self.soft_plus(\n            torch.logsumexp(logit_n, dim=0) + torch.logsumexp(logit_p, dim=0)\n        )\n\n        return loss\n\n\n__all__ = [""CircleLoss""]\n'"
catalyst/contrib/nn/criterion/contrastive.py,24,"b'import torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\n\nclass ContrastiveEmbeddingLoss(nn.Module):\n    """"""The Contrastive embedding loss.\n\n    It has been proposed in `Dimensionality Reduction\n    by Learning an Invariant Mapping`_.\n\n    .. _Dimensionality Reduction by Learning an Invariant Mapping:\n        http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n    """"""\n\n    def __init__(self, margin=1.0, reduction=""mean""):\n        """"""\n        Args:\n            margin: margin parameter\n            reduction: criterion reduction type\n        """"""\n        super().__init__()\n        self.margin = margin\n        self.reduction = reduction or ""none""\n\n    def forward(\n        self,\n        embeddings_left: torch.Tensor,\n        embeddings_right: torch.Tensor,\n        distance_true,\n    ) -> torch.Tensor:\n        """"""Forward propagation method for the contrastive loss.\n\n        Args:\n            embeddings_left (torch.Tensor): left objects embeddings\n            embeddings_right (torch.Tensor): right objects embeddings\n            distance_true: true distances\n\n        Returns:\n            torch.Tensor: loss\n        """"""\n        # euclidian distance\n        diff = embeddings_left - embeddings_right\n        distance_pred = torch.sqrt(torch.sum(torch.pow(diff, 2), 1))\n\n        bs = len(distance_true)\n        margin_distance = self.margin - distance_pred\n        margin_distance_ = torch.clamp(margin_distance, min=0.0)\n        loss = (1 - distance_true) * torch.pow(\n            distance_pred, 2\n        ) + distance_true * torch.pow(margin_distance_, 2)\n\n        if self.reduction == ""mean"":\n            loss = torch.sum(loss) / 2.0 / bs\n        elif self.reduction == ""sum"":\n            loss = torch.sum(loss)\n        return loss\n\n\nclass ContrastiveDistanceLoss(nn.Module):\n    """"""The Contrastive distance loss.\n\n    @TODO: Docs. Contribution is welcome.\n    """"""\n\n    def __init__(self, margin=1.0, reduction=""mean""):\n        """"""\n        Args:\n            margin: margin parameter\n            reduction (str): criterion reduction type\n        """"""\n        super().__init__()\n        self.margin = margin\n        self.reduction = reduction or ""none""\n\n    def forward(self, distance_pred, distance_true) -> torch.Tensor:\n        """"""Forward propagation method for the contrastive loss.\n\n        Args:\n            distance_pred: predicted distances\n            distance_true: true distances\n\n        Returns:\n            torch.Tensor: loss\n        """"""\n        bs = len(distance_true)\n        margin_distance = self.margin - distance_pred\n        margin_distance_ = torch.clamp(margin_distance, min=0.0)\n        loss = (1 - distance_true) * torch.pow(\n            distance_pred, 2\n        ) + distance_true * torch.pow(margin_distance_, 2)\n\n        if self.reduction == ""mean"":\n            loss = torch.sum(loss) / 2.0 / bs\n        elif self.reduction == ""sum"":\n            loss = torch.sum(loss)\n        return loss\n\n\nclass ContrastivePairwiseEmbeddingLoss(nn.Module):\n    """"""ContrastivePairwiseEmbeddingLoss \xe2\x80\x93 proof of concept criterion.\n\n    Still work in progress.\n\n    @TODO: Docs. Contribution is welcome.\n    """"""\n\n    def __init__(self, margin=1.0, reduction=""mean""):\n        """"""\n        Args:\n            margin: margin parameter\n            reduction: criterion reduction type\n        """"""\n        super().__init__()\n        self.margin = margin\n        self.reduction = reduction or ""none""\n\n    def forward(self, embeddings_pred, embeddings_true) -> torch.Tensor:\n        """"""Forward propagation method for the contrastive loss.\n\n        Work in progress.\n\n        Args:\n            embeddings_pred: predicted embeddings\n            embeddings_true: true embeddings\n\n        Returns:\n            torch.Tensor: loss\n        """"""\n        device = embeddings_pred.device\n        # s - state space\n        # d - embeddings space\n        # a - action space\n        pairwise_similarity = torch.einsum(\n            ""se,ae->sa"", embeddings_pred, embeddings_true\n        )\n        bs = embeddings_pred.shape[0]\n        batch_idx = torch.arange(bs, device=device)\n        loss = F.cross_entropy(\n            pairwise_similarity, batch_idx, reduction=self.reduction\n        )\n        return loss\n\n\n__all__ = [\n    ""ContrastiveEmbeddingLoss"",\n    ""ContrastiveDistanceLoss"",\n    ""ContrastivePairwiseEmbeddingLoss"",\n]\n'"
catalyst/contrib/nn/criterion/dice.py,1,"b'from functools import partial\n\nimport torch\nfrom torch import nn\n\nfrom catalyst.utils import metrics\n\n\nclass DiceLoss(nn.Module):\n    """"""@TODO: Docs. Contribution is welcome.""""""\n\n    def __init__(\n        self,\n        eps: float = 1e-7,\n        threshold: float = None,\n        activation: str = ""Sigmoid"",\n    ):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        super().__init__()\n\n        self.loss_fn = partial(\n            metrics.dice, eps=eps, threshold=threshold, activation=activation\n        )\n\n    def forward(self, logits: torch.Tensor, targets: torch.Tensor):\n        """"""Calculates loss between ``logits`` and ``target`` tensors.\n\n        @TODO: Docs. Contribution is welcome\n        """"""\n        dice = self.loss_fn(logits, targets)\n        return 1 - dice\n\n\nclass BCEDiceLoss(nn.Module):\n    """"""@TODO: Docs. Contribution is welcome.""""""\n\n    def __init__(\n        self,\n        eps: float = 1e-7,\n        threshold: float = None,\n        activation: str = ""Sigmoid"",\n        bce_weight: float = 0.5,\n        dice_weight: float = 0.5,\n    ):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        super().__init__()\n\n        if bce_weight == 0 and dice_weight == 0:\n            raise ValueError(\n                ""Both bce_wight and dice_weight cannot be ""\n                ""equal to 0 at the same time.""\n            )\n\n        self.bce_weight = bce_weight\n        self.dice_weight = dice_weight\n\n        if self.bce_weight != 0:\n            self.bce_loss = nn.BCEWithLogitsLoss()\n\n        if self.dice_weight != 0:\n            self.dice_loss = DiceLoss(\n                eps=eps, threshold=threshold, activation=activation\n            )\n\n    def forward(self, outputs, targets):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        if self.bce_weight == 0:\n            return self.dice_weight * self.dice_loss(outputs, targets)\n        if self.dice_weight == 0:\n            return self.bce_weight * self.bce_loss(outputs, targets)\n\n        dice = self.dice_weight * self.dice_loss(outputs, targets)\n        bce = self.bce_weight * self.bce_loss(outputs, targets)\n        return dice + bce\n\n\n__all__ = [""BCEDiceLoss"", ""DiceLoss""]\n'"
catalyst/contrib/nn/criterion/focal.py,1,"b'from functools import partial\n\nfrom torch.nn.modules.loss import _Loss\n\nfrom catalyst.utils import metrics\n\n\nclass FocalLossBinary(_Loss):\n    """"""Compute focal loss for binary classification problem.\n\n    It has been proposed in `Focal Loss for Dense Object Detection`_ paper.\n\n    @TODO: Docs (add `Example`). Contribution is welcome.\n\n    .. _Focal Loss for Dense Object Detection:\n        https://arxiv.org/abs/1708.02002\n    """"""\n\n    def __init__(\n        self,\n        ignore: int = None,\n        reduced: bool = False,\n        gamma: float = 2.0,\n        alpha: float = 0.25,\n        threshold: float = 0.5,\n        reduction: str = ""mean"",\n    ):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        super().__init__()\n        self.ignore = ignore\n\n        if reduced:\n            self.loss_fn = partial(\n                metrics.reduced_focal_loss,\n                gamma=gamma,\n                threshold=threshold,\n                reduction=reduction,\n            )\n        else:\n            self.loss_fn = partial(\n                metrics.sigmoid_focal_loss,\n                gamma=gamma,\n                alpha=alpha,\n                reduction=reduction,\n            )\n\n    def forward(self, logits, targets):\n        """"""\n        Args:\n            logits: [bs; ...]\n            targets: [bs; ...]\n\n        @TODO: Docs. Contribution is welcome.\n        """"""\n        targets = targets.view(-1)\n        logits = logits.view(-1)\n\n        if self.ignore is not None:\n            # Filter predictions with ignore label from loss computation\n            not_ignored = targets != self.ignore\n            logits = logits[not_ignored]\n            targets = targets[not_ignored]\n\n        loss = self.loss_fn(logits, targets)\n\n        return loss\n\n\nclass FocalLossMultiClass(FocalLossBinary):\n    """"""Compute focal loss for multi-class problem.\n    Ignores targets having -1 label.\n\n    It has been proposed in `Focal Loss for Dense Object Detection`_ paper.\n\n    @TODO: Docs (add `Example`). Contribution is welcome.\n\n    .. _Focal Loss for Dense Object Detection:\n        https://arxiv.org/abs/1708.02002\n    """"""\n\n    def forward(self, logits, targets):\n        """"""\n        Args:\n            logits: [bs; num_classes; ...]\n            targets: [bs; ...]\n\n        @TODO: Docs. Contribution is welcome.\n        """"""\n        num_classes = logits.size(1)\n        loss = 0\n        targets = targets.view(-1)\n        logits = logits.view(-1, num_classes)\n\n        # Filter anchors with -1 label from loss computation\n        if self.ignore is not None:\n            not_ignored = targets != self.ignore\n\n        for cls in range(num_classes):\n            cls_label_target = (targets == (cls + 0)).long()\n            cls_label_input = logits[..., cls]\n\n            if self.ignore is not None:\n                cls_label_target = cls_label_target[not_ignored]\n                cls_label_input = cls_label_input[not_ignored]\n\n            loss += self.loss_fn(cls_label_input, cls_label_target)\n\n        return loss\n\n\n# @TODO: check\n# class FocalLossMultiLabel(_Loss):\n#     """"""Compute focal loss for multi-label problem.\n#     Ignores targets having -1 label.\n#\n#     It has been proposed in `Focal Loss for Dense Object Detection`_ paper.\n#\n#     @TODO: Docs (add `Example`). Contribution is welcome.\n#\n#     .. _Focal Loss for Dense Object Detection:\n#         https://arxiv.org/abs/1708.02002\n#     """"""\n#\n#     def forward(self, logits, targets):\n#         """"""\n#         Args:\n#             logits: [bs; num_classes]\n#             targets: [bs; num_classes]\n#         """"""\n#         num_classes = logits.size(1)\n#         loss = 0\n#\n#         for cls in range(num_classes):\n#             # Filter anchors with -1 label from loss computation\n#             if cls == self.ignore:\n#                 continue\n#\n#             cls_label_target = targets[..., cls].long()\n#             cls_label_input = logits[..., cls]\n#\n#             loss += self.loss_fn(cls_label_input, cls_label_target)\n#\n#         return loss\n\n__all__ = [""FocalLossBinary"", ""FocalLossMultiClass""]\n'"
catalyst/contrib/nn/criterion/functional.py,30,"b'from typing import List, Optional, Union\n\nimport torch\nimport torch.nn.functional as F\n\n_EPS = 1e-8\n\n\ndef euclidean_distance(\n    x: torch.Tensor, y: torch.Tensor = None,\n) -> torch.Tensor:\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    x_norm = (x ** 2).sum(1).unsqueeze(1)\n    if y is not None:\n        y_norm = (y ** 2).sum(1).unsqueeze(0)\n    else:\n        y = x\n        y_norm = x_norm.t()\n\n    dist = x_norm + y_norm - 2.0 * torch.mm(x, torch.transpose(y, 0, 1))\n    dist.clamp_min_(0.0)\n    return dist\n\n\ndef cosine_distance(\n    x: torch.Tensor, z: Optional[torch.Tensor] = None,\n) -> torch.Tensor:\n    """"""Calculate cosine distance between x and z.\n\n    Args:\n        @TODO: Docs. Contribution is welcome.\n    """"""\n    x = F.normalize(x)\n\n    if z is not None:\n        z = F.normalize(z)\n    else:\n        z = x.clone()\n\n    return torch.sub(1, torch.mm(x, z.transpose(0, 1)))\n\n\ndef batch_all(\n    labels: torch.Tensor, exclude_negatives: bool = True,\n) -> torch.Tensor:\n    """"""Create a 3D mask of all possible triplets.\n\n    Args:\n        @TODO: Docs. Contribution is welcome.\n    """"""\n    batch_size = labels.size(0)\n    indices_equal = torch.eye(batch_size, device=labels.device).type(\n        torch.bool\n    )\n    indices_not_equal = ~indices_equal\n\n    i_not_equal_j = indices_not_equal.unsqueeze(2)\n    i_not_equal_k = indices_not_equal.unsqueeze(1)\n    j_not_equal_k = indices_not_equal.unsqueeze(0)\n\n    distinct_indices = i_not_equal_j & i_not_equal_k & j_not_equal_k\n\n    label_equal = torch.eq(labels.unsqueeze(0), labels.unsqueeze(1))\n\n    yi_equal_yj = label_equal.unsqueeze(2)\n    yi_equal_yk = label_equal.unsqueeze(1)\n\n    yi_not_equal_yk = ~yi_equal_yk\n    valid_labels = yi_equal_yj & yi_not_equal_yk\n\n    mask = distinct_indices & valid_labels\n\n    if exclude_negatives:\n        mask = mask & create_negative_mask(labels)\n\n    return mask.float()\n\n\ndef create_negative_mask(\n    labels: torch.Tensor, neg_label: int = -1\n) -> torch.Tensor:\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    neg_labels = torch.ge(labels, neg_label)\n    pos_labels = ~neg_labels\n\n    i_less_neg = pos_labels.unsqueeze(1).unsqueeze(2)\n    j_less_neg = pos_labels.unsqueeze(1).unsqueeze(0)\n    k_less_neg = pos_labels.unsqueeze(0).unsqueeze(0)\n\n    anchors = labels.unsqueeze(1).unsqueeze(2)\n    negatives = labels.unsqueeze(0).unsqueeze(0)\n    k_equal = torch.eq(anchors + neg_label, negatives)\n\n    k_less_or_equal = k_equal | k_less_neg\n    mask = i_less_neg & j_less_neg & k_less_or_equal\n\n    return mask\n\n\ndef triplet_loss(\n    embeddings: torch.Tensor, labels: torch.Tensor, margin: float = 0.3\n) -> torch.Tensor:\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    cosine_dists = cosine_distance(embeddings)\n    mask = batch_all(labels)\n\n    anchor_positive_dist = cosine_dists.unsqueeze(2)\n    anchor_negative_dist = cosine_dists.unsqueeze(1)\n    triplet_loss_value = F.relu(\n        anchor_positive_dist - anchor_negative_dist + margin\n    )\n    triplet_loss_value = torch.mul(triplet_loss_value, mask)\n\n    num_positive_triplets = torch.gt(triplet_loss_value, _EPS).sum().float()\n    triplet_loss_value = triplet_loss_value.sum() / (\n        num_positive_triplets + _EPS\n    )\n\n    return triplet_loss_value\n\n\ndef _create_margin_mask(labels: torch.Tensor) -> torch.Tensor:\n    equal_labels_mask = torch.eq(labels.unsqueeze(0), labels.unsqueeze(1))\n    marign_mask = 2 * equal_labels_mask.float() - 1\n    return marign_mask\n\n\ndef _skip_labels_mask(\n    labels: torch.Tensor, skip_labels: Union[int, List[int]]\n) -> torch.Tensor:\n    skip_labels = torch.tensor(\n        skip_labels, dtype=labels.dtype, device=labels.device\n    ).reshape(-1)\n    skip_condition = (labels.unsqueeze(-1) == skip_labels).any(-1)\n    skip_mask = ~(skip_condition.unsqueeze(-1) & skip_condition.unsqueeze(0))\n    return skip_mask\n\n\ndef margin_loss(\n    embeddings: torch.Tensor,\n    labels: torch.Tensor,\n    alpha: float = 0.2,\n    beta: float = 1.0,\n    skip_labels: Union[int, List[int]] = -1,\n) -> torch.Tensor:\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    embeddings = F.normalize(embeddings, p=2.0, dim=1)\n    distances = euclidean_distance(embeddings, embeddings)\n\n    margin_mask = _create_margin_mask(labels)\n    skip_mask = _skip_labels_mask(labels, skip_labels).float()\n    loss = torch.mul(\n        skip_mask,\n        F.relu(alpha + torch.mul(margin_mask, torch.sub(distances, beta))),\n    )\n    return loss.sum() / (skip_mask.sum() + _EPS)\n'"
catalyst/contrib/nn/criterion/gan.py,4,"b'import torch\nfrom torch import nn\n\n\nclass MeanOutputLoss(nn.Module):\n    """"""\n    Criterion to compute simple mean of the output, completely ignoring target\n    (maybe useful e.g. for WGAN real/fake validity averaging.\n    """"""\n\n    def forward(self, output, target):\n        """"""Compute criterion.\n\n        @TODO: Docs (add typing). Contribution is welcome.\n        """"""\n        return output.mean()\n\n\nclass GradientPenaltyLoss(nn.Module):\n    """"""Criterion to compute gradient penalty.\n\n    WARN: SHOULD NOT BE RUN WITH CriterionCallback,\n        use special GradientPenaltyCallback instead\n    """"""\n\n    def forward(self, fake_data, real_data, critic, critic_condition_args):\n        """"""Compute gradient penalty.\n\n        Args:\n            @TODO: Docs. Contribution is welcome.\n        """"""\n        device = real_data.device\n        # Random weight term for interpolation between real and fake samples\n        alpha = torch.rand((real_data.size(0), 1, 1, 1), device=device)\n        # Get random interpolation between real and fake samples\n        interpolates = (alpha * real_data + ((1 - alpha) * fake_data)).detach()\n        interpolates.requires_grad_(True)\n        with torch.set_grad_enabled(True):  # to compute in validation mode\n            d_interpolates = critic(interpolates, *critic_condition_args)\n\n        fake = torch.ones(\n            (real_data.size(0), 1), device=device, requires_grad=False\n        )\n        # Get gradient w.r.t. interpolates\n        gradients = torch.autograd.grad(\n            outputs=d_interpolates,\n            inputs=interpolates,\n            grad_outputs=fake,\n            create_graph=True,\n            retain_graph=True,\n            only_inputs=True,\n        )[0]\n        gradients = gradients.view(gradients.size(0), -1)\n        gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n        return gradient_penalty\n\n\n__all__ = [""MeanOutputLoss"", ""GradientPenaltyLoss""]\n'"
catalyst/contrib/nn/criterion/huber.py,8,"b'import torch\nfrom torch import nn\n\n\nclass HuberLoss(nn.Module):\n    """"""@TODO: Docs. Contribution is welcome.""""""\n\n    def __init__(self, clip_delta=1.0, reduction=""mean""):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        super().__init__()\n        self.clip_delta = clip_delta\n        self.reduction = reduction or ""none""\n\n    def forward(\n        self, y_pred: torch.Tensor, y_true: torch.Tensor, weights=None\n    ) -> torch.Tensor:\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        td_error = y_true - y_pred\n        td_error_abs = torch.abs(td_error)\n        quadratic_part = torch.clamp(td_error_abs, max=self.clip_delta)\n        linear_part = td_error_abs - quadratic_part\n        loss = 0.5 * quadratic_part ** 2 + self.clip_delta * linear_part\n\n        if weights is not None:\n            loss = torch.mean(loss * weights, dim=1)\n        else:\n            loss = torch.mean(loss, dim=1)\n\n        if self.reduction == ""mean"":\n            loss = torch.mean(loss)\n        elif self.reduction == ""sum"":\n            loss = torch.sum(loss)\n\n        return loss\n\n\n__all__ = [""HuberLoss""]\n'"
catalyst/contrib/nn/criterion/iou.py,2,"b'from functools import partial\n\nfrom torch import nn\n\nfrom catalyst.utils import metrics\n\n\nclass IoULoss(nn.Module):\n    """"""The intersection over union (Jaccard) loss.\n\n    @TODO: Docs. Contribution is welcome.\n    """"""\n\n    def __init__(\n        self,\n        eps: float = 1e-7,\n        threshold: float = None,\n        activation: str = ""Sigmoid"",\n    ):\n        """"""\n        Args:\n            eps (float): epsilon to avoid zero division\n            threshold (float): threshold for outputs binarization\n            activation (str): An torch.nn activation applied to the outputs.\n                Must be one of ``\'none\'``, ``\'Sigmoid\'``, ``\'Softmax2d\'``\n        """"""\n        super().__init__()\n        self.metric_fn = partial(\n            metrics.iou, eps=eps, threshold=threshold, activation=activation\n        )\n\n    def forward(self, outputs, targets):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        iou = self.metric_fn(outputs, targets)\n        return 1 - iou\n\n\nclass BCEIoULoss(nn.Module):\n    """"""The Intersection over union (Jaccard) with BCE loss.\n\n    @TODO: Docs. Contribution is welcome.\n    """"""\n\n    def __init__(\n        self,\n        eps: float = 1e-7,\n        threshold: float = None,\n        activation: str = ""Sigmoid"",\n        reduction: str = ""mean"",\n    ):\n        """"""\n        Args:\n            eps (float): epsilon to avoid zero division\n            threshold (float): threshold for outputs binarization\n            activation (str): An torch.nn activation applied to the outputs.\n                Must be one of ``\'none\'``, ``\'Sigmoid\'``, ``\'Softmax2d\'``\n            reduction (str): Specifies the reduction to apply\n                to the output of BCE\n        """"""\n        super().__init__()\n        self.bce_loss = nn.BCEWithLogitsLoss(reduction=reduction)\n        self.iou_loss = IoULoss(eps, threshold, activation)\n\n    def forward(self, outputs, targets):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        iou = self.iou_loss.forward(outputs, targets)\n        bce = self.bce_loss(outputs, targets)\n        loss = iou + bce\n        return loss\n\n\n__all__ = [""IoULoss"", ""BCEIoULoss""]\n'"
catalyst/contrib/nn/criterion/lovasz.py,7,"b'# Lovasz-Softmax and Jaccard hinge loss in PyTorch\n# Maxim Berman 2018 ESAT-PSI KU Leuven (MIT License)\n\nfrom itertools import filterfalse as ifilterfalse\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.modules.loss import _Loss\n\n# --------------------------- HELPER FUNCTIONS ---------------------------\n\n\ndef isnan(x):\n    return x != x\n\n\ndef mean(values, ignore_nan=False, empty=0):\n    """"""\n    Nanmean compatible with generators.\n    """"""\n    values = iter(values)\n    if ignore_nan:\n        values = ifilterfalse(isnan, values)\n    try:\n        n = 1\n        acc = next(values)\n    except StopIteration:\n        if empty == ""raise"":\n            raise ValueError(""Empty mean"")\n        return empty\n    for n, v in enumerate(values, 2):  # noqa: B007\n        acc += v\n    if n == 1:\n        return acc\n    return acc / n\n\n\ndef _lovasz_grad(gt_sorted):\n    """"""\n    Compute gradient of the Lovasz extension w.r.t sorted errors,\n    see Alg. 1 in paper\n    """"""\n    p = len(gt_sorted)\n    gts = gt_sorted.sum()\n    intersection = gts - gt_sorted.float().cumsum(0)\n    union = gts + (1 - gt_sorted).float().cumsum(0)\n    jaccard = 1.0 - intersection / union\n    if p > 1:  # cover 1-pixel case\n        jaccard[1:p] = jaccard[1:p] - jaccard[0:-1]\n    return jaccard\n\n\n# ---------------------------- BINARY LOSSES -----------------------------\n\n\ndef _flatten_binary_scores(logits, targets, ignore=None):\n    """"""\n    Flattens predictions in the batch (binary case).\n    Remove targets equal to ""ignore""\n    """"""\n    logits = logits.reshape(-1)\n    targets = targets.reshape(-1)\n    if ignore is None:\n        return logits, targets\n    valid = targets != ignore\n    logits_ = logits[valid]\n    targets_ = targets[valid]\n    return logits_, targets_\n\n\ndef _lovasz_hinge_flat(logits, targets):\n    """"""The binary Lovasz hinge loss.\n\n    Args:\n        logits: [P] Variable, logits at each prediction\n            (between -iinfinity and +iinfinity)\n        targets: [P] Tensor, binary ground truth targets (0 or 1)\n    """"""\n    if len(targets) == 0:\n        # only void pixels, the gradients should be 0\n        return logits.sum() * 0.0\n    signs = 2.0 * targets.float() - 1.0\n    errors = 1.0 - logits * signs\n    errors_sorted, perm = torch.sort(errors, dim=0, descending=True)\n    perm = perm.data\n    gt_sorted = targets[perm]\n    grad = _lovasz_grad(gt_sorted)\n    loss = torch.dot(F.relu(errors_sorted), grad)\n    return loss\n\n\ndef _lovasz_hinge(logits, targets, per_image=True, ignore=None):\n    """"""The binary Lovasz hinge loss.\n\n    Args:\n        logits: [B, H, W] Variable, logits at each pixel\n            (between -infinity and +infinity)\n        targets: [B, H, W] Tensor, binary ground truth masks (0 or 1)\n        per_image: compute the loss per image instead of per batch\n        ignore: void class id\n    """"""\n    if per_image:\n        loss = mean(\n            _lovasz_hinge_flat(\n                *_flatten_binary_scores(\n                    logit.unsqueeze(0), target.unsqueeze(0), ignore\n                )\n            )\n            for logit, target in zip(logits, targets)\n        )\n    else:\n        loss = _lovasz_hinge_flat(\n            *_flatten_binary_scores(logits, targets, ignore)\n        )\n    return loss\n\n\n# --------------------------- MULTICLASS LOSSES ---------------------------\n\n\ndef _flatten_probabilities(probabilities, targets, ignore=None):\n    """"""\n    Flattens predictions in the batch\n    """"""\n    if probabilities.dim() == 3:\n        # assumes output of a sigmoid layer\n        B, H, W = probabilities.size()\n        probabilities = probabilities.view(B, 1, H, W)\n    B, C, H, W = probabilities.size()\n    # B * H * W, C = P, C\n    probabilities = probabilities.permute(0, 2, 3, 1).contiguous().view(-1, C)\n    targets = targets.view(-1)\n    if ignore is None:\n        return probabilities, targets\n    valid = targets != ignore\n    probabilities_ = probabilities[valid.nonzero().squeeze()]\n    targets_ = targets[valid]\n    return probabilities_, targets_\n\n\ndef _lovasz_softmax_flat(probabilities, targets, classes=""present""):\n    """"""The multi-class Lovasz-Softmax loss.\n\n    Args:\n        probabilities: [P, C]\n            class probabilities at each prediction (between 0 and 1)\n        targets: [P] ground truth targets (between 0 and C - 1)\n        classes: ""all"" for all,\n            ""present"" for classes present in targets,\n             or a list of classes to average.\n    """"""\n    if probabilities.numel() == 0:\n        # only void pixels, the gradients should be 0\n        return probabilities * 0.0\n    C = probabilities.size(1)\n    losses = []\n    class_to_sum = list(range(C)) if classes in [""all"", ""present""] else classes\n    for c in class_to_sum:\n        fg = (targets == c).float()  # foreground for class c\n        if classes == ""present"" and fg.sum() == 0:\n            continue\n        if C == 1:\n            if len(class_to_sum) > 1:\n                raise ValueError(""Sigmoid output possible only with 1 class"")\n            class_pred = probabilities[:, 0]\n        else:\n            class_pred = probabilities[:, c]\n        errors = (fg - class_pred).abs()\n        errors_sorted, perm = torch.sort(errors, 0, descending=True)\n        perm = perm.data\n        fg_sorted = fg[perm]\n        losses.append(torch.dot(errors_sorted, _lovasz_grad(fg_sorted)))\n    return mean(losses)\n\n\ndef _lovasz_softmax(\n    probabilities, targets, classes=""present"", per_image=False, ignore=None\n):\n    """"""The multi-class Lovasz-Softmax loss.\n\n    Args:\n        probabilities: [B, C, H, W]\n            class probabilities at each prediction (between 0 and 1).\n            Interpreted as binary (sigmoid) output\n            with outputs of size [B, H, W].\n        targets: [B, H, W] ground truth targets (between 0 and C - 1)\n        classes: ""all"" for all,\n            ""present"" for classes present in targets,\n            or a list of classes to average.\n        per_image: compute the loss per image instead of per batch\n        ignore: void class targets\n    """"""\n    if per_image:\n        loss = mean(\n            _lovasz_softmax_flat(\n                *_flatten_probabilities(\n                    prob.unsqueeze(0), lab.unsqueeze(0), ignore\n                ),\n                classes=classes\n            )\n            for prob, lab in zip(probabilities, targets)\n        )\n    else:\n        loss = _lovasz_softmax_flat(\n            *_flatten_probabilities(probabilities, targets, ignore),\n            classes=classes\n        )\n    return loss\n\n\n# ------------------------------ CRITERION -------------------------------\n\n\nclass LovaszLossBinary(_Loss):\n    """"""Creates a criterion that optimizes a binary Lovasz loss.\n\n    It has been proposed in `The Lovasz-Softmax loss: A tractable surrogate\n    for the optimization of the intersection-over-union measure\n    in neural networks`_.\n\n    .. _The Lovasz-Softmax loss\\: A tractable surrogate for the optimization\n        of the intersection-over-union measure in neural networks:\n        https://arxiv.org/abs/1705.08790\n    """"""\n\n    def __init__(self, per_image=False, ignore=None):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        super().__init__()\n        self.ignore = ignore\n        self.per_image = per_image\n\n    def forward(self, logits, targets):\n        """"""Forward propagation method for the Lovasz loss.\n\n        Args:\n            logits: [bs; ...]\n            targets: [bs; ...]\n\n        @TODO: Docs. Contribution is welcome.\n        """"""\n        loss = _lovasz_hinge(\n            logits, targets, per_image=self.per_image, ignore=self.ignore\n        )\n        return loss\n\n\nclass LovaszLossMultiClass(_Loss):\n    """"""Creates a criterion that optimizes a multi-class Lovasz loss.\n\n    It has been proposed in `The Lovasz-Softmax loss: A tractable surrogate\n    for the optimization of the intersection-over-union measure\n    in neural networks`_.\n\n    .. _The Lovasz-Softmax loss\\: A tractable surrogate for the optimization\n        of the intersection-over-union measure in neural networks:\n        https://arxiv.org/abs/1705.08790\n    """"""\n\n    def __init__(self, per_image=False, ignore=None):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        super().__init__()\n        self.ignore = ignore\n        self.per_image = per_image\n\n    def forward(self, logits, targets):\n        """"""Forward propagation method for the Lovasz loss.\n\n        Args:\n            logits: [bs; num_classes; ...]\n            targets: [bs; ...]\n\n        @TODO: Docs. Contribution is welcome.\n        """"""\n        loss = _lovasz_softmax(\n            logits, targets, per_image=self.per_image, ignore=self.ignore\n        )\n        return loss\n\n\nclass LovaszLossMultiLabel(_Loss):\n    """"""Creates a criterion that optimizes a multi-label Lovasz loss.\n\n    It has been proposed in `The Lovasz-Softmax loss: A tractable surrogate\n    for the optimization of the intersection-over-union measure\n    in neural networks`_.\n\n    .. _The Lovasz-Softmax loss\\: A tractable surrogate for the optimization\n        of the intersection-over-union measure in neural networks:\n        https://arxiv.org/abs/1705.08790\n    """"""\n\n    def __init__(self, per_image=False, ignore=None):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        super().__init__()\n        self.ignore = ignore\n        self.per_image = per_image\n\n    def forward(self, logits, targets):\n        """"""Forward propagation method for the Lovasz loss.\n\n        Args:\n            logits: [bs; num_classes; ...]\n            targets: [bs; num_classes; ...]\n\n        @TODO: Docs. Contribution is welcome.\n        """"""\n        losses = [\n            _lovasz_hinge(\n                logits[:, i, ...],\n                targets[:, i, ...],\n                per_image=self.per_image,\n                ignore=self.ignore,\n            )\n            for i in range(logits.shape[1])\n        ]\n        loss = torch.mean(torch.stack(losses))\n        return loss\n\n\n__all__ = [""LovaszLossBinary"", ""LovaszLossMultiClass"", ""LovaszLossMultiLabel""]\n'"
catalyst/contrib/nn/criterion/margin.py,2,"b'from typing import List, Union\n\nimport torch\nfrom torch import nn\n\nfrom .functional import margin_loss\n\n\nclass MarginLoss(nn.Module):\n    """"""@TODO: Docs. Contribution is welcome.""""""\n\n    def __init__(\n        self,\n        alpha: float = 0.2,\n        beta: float = 1.0,\n        skip_labels: Union[int, List[int]] = -1,\n    ):\n        """"""\n        Args:\n            alpha (float):\n            beta (float):\n            skip_labels (int or List[int]):\n\n        @TODO: Docs. Contribution is welcome.\n        """"""\n        super().__init__()\n        self.alpha = alpha\n        self.beta = beta\n        self.skip_labels = skip_labels\n\n    def forward(\n        self, embeddings: torch.Tensor, targets: torch.Tensor\n    ) -> torch.Tensor:\n        """"""Forward propagation method for the margin loss.\n\n        @TODO: Docs. Contribution is welcome.\n        """"""\n        return margin_loss(\n            embeddings,\n            targets,\n            alpha=self.alpha,\n            beta=self.beta,\n            skip_labels=self.skip_labels,\n        )\n'"
catalyst/contrib/nn/criterion/triplet.py,13,"b'import torch\nfrom torch import nn\n\nfrom .functional import triplet_loss\n\n\nclass TripletLoss(nn.Module):\n    """"""Triplet loss with hard positive/negative mining.\n\n    Reference:\n        Code imported from https://github.com/NegatioN/OnlineMiningTripletLoss\n    """"""\n\n    def __init__(self, margin: float = 0.3):\n        """"""\n        Args:\n            margin (float): margin for triplet\n        """"""\n        super().__init__()\n        self.margin = margin\n        self.ranking_loss = nn.MarginRankingLoss(margin=margin)\n\n    def _pairwise_distances(self, embeddings, squared=False):\n        """"""Compute the 2D matrix of distances between all the embeddings.\n\n        Args:\n            embeddings: tensor of shape (batch_size, embed_dim)\n            squared (bool): if true, output is the pairwise squared euclidean\n                distance matrix. If false, output is the pairwise euclidean\n                distance matrix\n\n        Returns:\n            torch.Tensor: pairwise matrix of size (batch_size, batch_size)\n        """"""\n        # Get squared L2 norm for each embedding.\n        # We can just take the diagonal of `dot_product`.\n        # This also provides more numerical stability\n        # (the diagonal of the result will be exactly 0).\n        # shape (batch_size,)\n        square = torch.mm(embeddings, embeddings.t())\n        diag = torch.diag(square)\n\n        # Compute the pairwise distance matrix as we have:\n        # ||a - b||^2 = ||a||^2  - 2 <a, b> + ||b||^2\n        # shape (batch_size, batch_size)\n        distances = diag.view(-1, 1) - 2.0 * square + diag.view(1, -1)\n\n        # Because of computation errors, some distances\n        # might be negative so we put everything >= 0.0\n        distances[distances < 0] = 0\n\n        if not squared:\n            # Because the gradient of sqrt is infinite\n            # when distances == 0.0 (ex: on the diagonal)\n            # we need to add a small epsilon where distances == 0.0\n            mask = distances.eq(0).float()\n            distances = distances + mask * 1e-16\n\n            distances = (1.0 - mask) * torch.sqrt(distances)\n\n        return distances\n\n    def _get_anchor_positive_triplet_mask(self, labels):\n        """"""\n        Return a 2D mask where mask[a, p] is True\n        if a and p are distinct and have same label.\n\n        Args:\n            labels: tf.int32 `Tensor` with shape [batch_size]\n\n        Returns:\n            mask: tf.bool `Tensor` with shape [batch_size, batch_size]\n        """"""\n        indices_equal = torch.eye(labels.size(0)).bool()\n\n        # labels and indices should be on\n        # the same device, otherwise - exception\n        indices_equal = indices_equal.to(""cuda"" if labels.is_cuda else ""cpu"")\n\n        # Check that i and j are distinct\n        indices_not_equal = ~indices_equal\n\n        # Check if labels[i] == labels[j]\n        # Uses broadcasting where the 1st argument\n        # has shape (1, batch_size) and the 2nd (batch_size, 1)\n        labels_equal = labels.unsqueeze(0) == labels.unsqueeze(1)\n\n        return labels_equal & indices_not_equal\n\n    def _get_anchor_negative_triplet_mask(self, labels):\n        """"""Return 2D mask where mask[a, n] is True if a and n have same label.\n\n        Args:\n            labels: tf.int32 `Tensor` with shape [batch_size]\n\n        Returns:\n            mask: tf.bool `Tensor` with shape [batch_size, batch_size]\n        """"""\n        # Check if labels[i] != labels[k]\n        # Uses broadcasting where the 1st argument\n        # has shape (1, batch_size) and the 2nd (batch_size, 1)\n        return ~(labels.unsqueeze(0) == labels.unsqueeze(1))\n\n    def _batch_hard_triplet_loss(\n        self, embeddings, labels, margin, squared=True,\n    ):\n        """"""\n        Build the triplet loss over a batch of embeddings.\n        For each anchor, we get the hardest positive and\n        hardest negative to form a triplet.\n\n        Args:\n            labels: labels of the batch, of size (batch_size,)\n            embeddings: tensor of shape (batch_size, embed_dim)\n            margin: margin for triplet loss\n            squared: Boolean. If true, output is the pairwise squared\n                     euclidean distance matrix. If false, output is the\n                     pairwise euclidean distance matrix.\n\n        Returns:\n            triplet_loss: scalar tensor containing the triplet loss\n        """"""\n        # Get the pairwise distance matrix\n        pairwise_dist = self._pairwise_distances(embeddings, squared=squared)\n\n        # For each anchor, get the hardest positive\n        # First, we need to get a mask for every valid\n        # positive (they should have same label)\n        mask_anchor_positive = self._get_anchor_positive_triplet_mask(\n            labels\n        ).float()\n\n        # We put to 0 any element where (a, p) is not valid\n        # (valid if a != p and label(a) == label(p))\n        anchor_positive_dist = mask_anchor_positive * pairwise_dist\n\n        # shape (batch_size, 1)\n        hardest_positive_dist, _ = anchor_positive_dist.max(1, keepdim=True)\n\n        # For each anchor, get the hardest negative\n        # First, we need to get a mask for every valid negative\n        # (they should have different labels)\n        mask_anchor_negative = self._get_anchor_negative_triplet_mask(\n            labels\n        ).float()\n\n        # We add the maximum value in each row\n        # to the invalid negatives (label(a) == label(n))\n        max_anchor_negative_dist, _ = pairwise_dist.max(1, keepdim=True)\n        anchor_negative_dist = pairwise_dist + max_anchor_negative_dist * (\n            1.0 - mask_anchor_negative\n        )\n\n        # shape (batch_size,)\n        hardest_negative_dist, _ = anchor_negative_dist.min(1, keepdim=True)\n\n        # Combine biggest d(a, p) and smallest d(a, n) into final triplet loss\n        tl = hardest_positive_dist - hardest_negative_dist + margin\n        tl[tl < 0] = 0\n        triplet_loss = tl.mean()\n\n        return triplet_loss\n\n    def forward(self, embeddings, targets):\n        """"""Forward propagation method for the triplet loss.\n\n        Args:\n            embeddings: tensor of shape (batch_size, embed_dim)\n            targets: labels of the batch, of size (batch_size,)\n\n        Returns:\n            triplet_loss: scalar tensor containing the triplet loss\n        """"""\n        return self._batch_hard_triplet_loss(embeddings, targets, self.margin)\n\n\nclass TripletLossV2(nn.Module):\n    """"""@TODO: Docs. Contribution is welcome.""""""\n\n    def __init__(self, margin=0.3):\n        """"""\n        Args:\n            margin (float): margin for triplet.\n        """"""\n        super().__init__()\n        self.margin = margin\n\n    def forward(self, embeddings, targets):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        return triplet_loss(embeddings, targets, margin=self.margin,)\n\n\nclass TripletPairwiseEmbeddingLoss(nn.Module):\n    """"""TripletPairwiseEmbeddingLoss \xe2\x80\x93 proof of concept criterion.\n\n    Still work in progress.\n\n    @TODO: Docs. Contribution is welcome.\n    """"""\n\n    def __init__(self, margin: float = 0.3, reduction: str = ""mean""):\n        """"""\n        Args:\n            margin (float): margin parameter\n            reduction (str): criterion reduction type\n        """"""\n        super().__init__()\n        self.margin = margin\n        self.reduction = reduction or ""none""\n\n    def forward(self, embeddings_pred, embeddings_true):\n        """"""\n        Work in progress.\n\n        Args:\n            embeddings_pred: predicted embeddings\n                with shape [batch_size, embedding_size]\n            embeddings_true: true embeddings\n                with shape [batch_size, embedding_size]\n\n        Returns:\n            torch.Tensor: loss\n        """"""\n        device = embeddings_pred.device\n        # s - state space\n        # d - embeddings space\n        # a - action space\n        # [batch_size, embedding_size] x [batch_size, embedding_size]\n        # -> [batch_size, batch_size]\n        pairwise_similarity = torch.einsum(\n            ""se,ae->sa"", embeddings_pred, embeddings_true\n        )\n        bs = embeddings_pred.shape[0]\n        batch_idx = torch.arange(bs, device=device)\n        negative_similarity = pairwise_similarity + torch.diag(\n            torch.full([bs], -(10 ** 9), device=device)\n        )\n        # TODO argsort, take k worst\n        hard_negative_ids = negative_similarity.argmax(dim=-1)\n\n        negative_similarities = pairwise_similarity[\n            batch_idx, hard_negative_ids\n        ]\n        positive_similarities = pairwise_similarity[batch_idx, batch_idx]\n        loss = torch.relu(\n            self.margin - positive_similarities + negative_similarities\n        )\n        if self.reduction == ""mean"":\n            loss = torch.sum(loss) / bs\n        elif self.reduction == ""sum"":\n            loss = torch.sum(loss)\n        return loss\n\n\n__all__ = [""TripletLoss"", ""TripletPairwiseEmbeddingLoss""]\n'"
catalyst/contrib/nn/criterion/wing.py,6,"b'from functools import partial\nimport math\n\nimport torch\nfrom torch import nn\n\n\ndef wing_loss(\n    outputs: torch.Tensor,\n    targets: torch.Tensor,\n    width: int = 5,\n    curvature: float = 0.5,\n    reduction: str = ""mean"",\n) -> torch.Tensor:\n    """"""The Wing loss.\n\n    It has been proposed in `Wing Loss for Robust Facial Landmark Localisation\n    with Convolutional Neural Networks`_.\n\n    Args:\n        @TODO: Docs. Contribution is welcome.\n\n    Adapted from:\n    https://github.com/BloodAxe/pytorch-toolbelt (MIT License)\n\n    .. _Wing Loss for Robust Facial Landmark Localisation with Convolutional\n        Neural Networks: https://arxiv.org/abs/1711.06753\n    """"""\n    diff_abs = (targets - outputs).abs()\n    loss = diff_abs.clone()\n\n    idx_smaller = diff_abs < width\n    idx_bigger = diff_abs >= width\n\n    loss[idx_smaller] = width * torch.log(\n        1 + diff_abs[idx_smaller] / curvature\n    )\n\n    c = width - width * math.log(1 + width / curvature)\n    loss[idx_bigger] = loss[idx_bigger] - c\n\n    if reduction == ""sum"":\n        loss = loss.sum()\n    if reduction == ""mean"":\n        loss = loss.mean()\n\n    return loss\n\n\nclass WingLoss(nn.Module):\n    """"""Creates a criterion that optimizes a Wing loss.\n\n    It has been proposed in `Wing Loss for Robust Facial Landmark Localisation\n    with Convolutional Neural Networks`_.\n\n    Examples:\n        @TODO: Docs. Contribution is welcome.\n\n    Adapted from:\n    https://github.com/BloodAxe/pytorch-toolbelt\n\n    .. _Wing Loss for Robust Facial Landmark Localisation with Convolutional\n        Neural Networks: https://arxiv.org/abs/1711.06753\n    """"""\n\n    def __init__(\n        self, width: int = 5, curvature: float = 0.5, reduction: str = ""mean""\n    ):\n        """"""\n        Args:\n            @TODO: Docs. Contribution is welcome.\n        """"""\n        super().__init__()\n        self.loss_fn = partial(\n            wing_loss, width=width, curvature=curvature, reduction=reduction\n        )\n\n    def forward(\n        self, outputs: torch.Tensor, targets: torch.Tensor\n    ) -> torch.Tensor:\n        """"""\n        Args:\n            @TODO: Docs. Contribution is welcome.\n        """"""\n        loss = self.loss_fn(outputs, targets)\n        return loss\n\n\n__all__ = [""WingLoss""]\n'"
catalyst/contrib/nn/modules/__init__.py,1,"b'# flake8: noqa\nfrom torch.nn.modules import *\n\nfrom .common import Flatten, GaussianNoise, Lambda, Normalize\nfrom .lama import LamaPooling, TemporalAttentionPooling, TemporalConcatPooling\nfrom .pooling import (\n    GlobalAttnPool2d,\n    GlobalAvgAttnPool2d,\n    GlobalAvgPool2d,\n    GlobalConcatAttnPool2d,\n    GlobalConcatPool2d,\n    GlobalMaxAttnPool2d,\n    GlobalMaxPool2d,\n)\nfrom .rms_norm import RMSNorm\nfrom .se import (\n    ChannelSqueezeAndSpatialExcitation,\n    ConcurrentSpatialAndChannelSqueezeAndChannelExcitation,\n    SqueezeAndExcitation,\n)\n'"
catalyst/contrib/nn/modules/common.py,4,"b'import torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\n\nclass Flatten(nn.Module):\n    """"""Flattens the input. Does not affect the batch size.\n\n    @TODO: Docs (add `Example`). Contribution is welcome.\n    """"""\n\n    def __init__(self):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        super().__init__()\n\n    def forward(self, x):\n        """"""Forward call.""""""\n        return x.view(x.shape[0], -1)\n\n\nclass Lambda(nn.Module):\n    """"""@TODO: Docs. Contribution is welcome.""""""\n\n    def __init__(self, lambda_fn):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        super().__init__()\n        self.lambda_fn = lambda_fn\n\n    def forward(self, x):\n        """"""Forward call.""""""\n        return self.lambda_fn(x)\n\n\nclass Normalize(nn.Module):\n    """"""Performs :math:`L_p` normalization of inputs over specified dimension.\n\n    @TODO: Docs (add `Example`). Contribution is welcome.\n    """"""\n\n    def __init__(self, **normalize_kwargs):\n        """"""\n        Args:\n            **normalize_kwargs: see ``torch.nn.functional.normalize`` params\n        """"""\n        super().__init__()\n        self.normalize_kwargs = normalize_kwargs\n\n    def forward(self, x):\n        """"""Forward call.""""""\n        return F.normalize(x, **self.normalize_kwargs)\n\n\nclass GaussianNoise(nn.Module):\n    """"""\n    A gaussian noise module.\n\n    Shape:\n\n    - Input: (batch, \\*)\n    - Output: (batch, \\*) (same shape as input)\n    """"""\n\n    def __init__(self, stddev: float = 0.1):\n        """"""\n        Args:\n            stddev (float): The standard deviation of the normal distribution.\n                Default: 0.1.\n        """"""\n        super().__init__()\n        self.stddev = stddev\n\n    def forward(self, x: torch.Tensor):\n        """"""Forward call.""""""\n        noise = torch.empty_like(x)\n        noise.normal_(0, self.stddev)\n'"
catalyst/contrib/nn/modules/lama.py,20,"b'import torch\nfrom torch import nn\n\nfrom catalyst.utils.initialization import outer_init\n\n\nclass TemporalLastPooling(nn.Module):\n    """"""@TODO: Docs. Contribution is welcome.""""""\n\n    def forward(\n        self, x: torch.Tensor, mask: torch.Tensor = None\n    ) -> torch.Tensor:\n        """"""Forward call.""""""\n        x_out = x[:, -1:, :]\n        return x_out\n\n\nclass TemporalAvgPooling(nn.Module):\n    """"""@TODO: Docs. Contribution is welcome.""""""\n\n    def forward(\n        self, x: torch.Tensor, mask: torch.Tensor = None\n    ) -> torch.Tensor:\n        """"""Forward call.""""""\n        if mask is None:\n            x_out = x.mean(1, keepdim=True)\n        else:\n            x_ = torch.sum(x * mask.float(), dim=1, keepdim=True)\n            mask_ = torch.sum(mask.float(), dim=1, keepdim=True)\n            x_out = x_ / mask_\n        return x_out\n\n\nclass TemporalMaxPooling(nn.Module):\n    """"""@TODO: Docs. Contribution is welcome.""""""\n\n    def forward(\n        self, x: torch.Tensor, mask: torch.Tensor = None\n    ) -> torch.Tensor:\n        """"""Forward call.""""""\n        if mask is not None:\n            mask_ = (~mask.bool()).float() * (-x.max()).float()\n            x = torch.sum(x + mask_, dim=1, keepdim=True)\n        x_out = x.max(1, keepdim=True)[0]\n        return x_out\n\n\nclass TemporalAttentionPooling(nn.Module):\n    """"""@TODO: Docs. Contribution is welcome.""""""\n\n    name2activation = {\n        ""softmax"": nn.Softmax(dim=1),\n        ""tanh"": nn.Tanh(),\n        ""sigmoid"": nn.Sigmoid(),\n    }\n\n    def __init__(self, in_features, activation=None, kernel_size=1, **params):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        super().__init__()\n        self.in_features = in_features\n        activation = activation or ""softmax""\n\n        self.attention_pooling = nn.Sequential(\n            nn.Conv1d(\n                in_channels=in_features,\n                out_channels=1,\n                kernel_size=kernel_size,\n                **params\n            ),\n            TemporalAttentionPooling.name2activation[activation],\n        )\n        self.attention_pooling.apply(outer_init)\n\n    def forward(\n        self, x: torch.Tensor, mask: torch.Tensor = None\n    ) -> torch.Tensor:\n        """"""\n        Args:\n            x (torch.Tensor): tensor of size\n                (batch_size, history_len, feature_size)\n\n        @TODO: Docs. Contribution is welcome.\n        """"""\n        batch_size, history_len, feature_size = x.shape\n\n        x = x.view(batch_size, history_len, -1)\n        x_a = x.transpose(1, 2)\n        x_attn = (self.attention_pooling(x_a) * x_a).transpose(1, 2)\n        x_attn = x_attn.sum(1, keepdim=True)\n\n        return x_attn\n\n\nclass TemporalConcatPooling(nn.Module):\n    """"""@TODO: Docs. Contribution is welcome.""""""\n\n    def __init__(self, in_features, history_len=1):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = in_features * history_len\n\n    def forward(\n        self, x: torch.Tensor, mask: torch.Tensor = None\n    ) -> torch.Tensor:\n        """"""\n        Args:\n            x (torch.Tensor): tensor of size\n                (batch_size, history_len, feature_size)\n\n        @TODO: Docs. Contribution is welcome.\n        """"""\n        x = x.view(x.shape[0], -1)\n        return x\n\n\nclass TemporalDropLastWrapper(nn.Module):\n    """"""@TODO: Docs. Contribution is welcome.""""""\n\n    def __init__(self, net):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        super().__init__()\n        self.net = net\n\n    def forward(self, x: torch.Tensor, mask: torch.Tensor = None):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        x = x[:, :-1, :]\n        x_out = self.net(x)\n        return x_out\n\n\ndef _get_pooling(key, in_features, **params):\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    key_ = key.split(""_"", 1)[0]\n\n    if key_ == ""last"":\n        return TemporalLastPooling()\n    elif key_ == ""avg"":\n        layer = TemporalAvgPooling()\n    elif key_ == ""max"":\n        layer = TemporalMaxPooling()\n    elif key_ in [""softmax"", ""tanh"", ""sigmoid""]:\n        layer = TemporalAttentionPooling(\n            in_features=in_features, activation=key_, **params\n        )\n    else:\n        raise NotImplementedError()\n\n    if ""droplast"" in key:\n        layer = TemporalDropLastWrapper(layer)\n\n    return layer\n\n\nclass LamaPooling(nn.Module):\n    """"""@TODO: Docs. Contribution is welcome.""""""\n\n    available_groups = [\n        ""last"",\n        ""avg"",\n        ""avg_droplast"",\n        ""max"",\n        ""max_droplast"",\n        ""sigmoid"",\n        ""sigmoid_droplast"",\n        ""softmax"",\n        ""softmax_droplast"",\n        ""tanh"",\n        ""tanh_droplast"",\n    ]\n\n    def __init__(self, in_features, groups=None):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        super().__init__()\n        self.in_features = in_features\n        self.groups = groups or [\n            ""last"",\n            ""avg_droplast"",\n            ""max_droplast"",\n            ""softmax_droplast"",\n        ]\n        self.out_features = in_features * len(self.groups)\n\n        groups = {}\n        for key in self.groups:\n            if isinstance(key, str):\n                groups[key] = _get_pooling(key, self.in_features)\n            elif isinstance(key, dict):\n                key_ = key.pop(""key"")\n                groups[key_] = _get_pooling(key_, in_features, **key)\n            else:\n                raise NotImplementedError()\n\n        self.groups = nn.ModuleDict(groups)\n\n    def forward(\n        self, x: torch.Tensor, mask: torch.Tensor = None\n    ) -> torch.Tensor:\n        """"""\n        Args:\n            x (torch.Tensor): tensor of size\n                (batch_size, history_len, feature_size)\n\n        @TODO: Docs. Contribution is welcome.\n        """"""\n        batch_size, history_len, feature_size = x.shape\n\n        x_ = []\n        for pooling_fn in self.groups.values():\n            features_ = pooling_fn(x, mask)\n            x_.append(features_)\n        x = torch.cat(x_, dim=1)\n        x = x.view(batch_size, -1)\n\n        return x\n\n\n__all__ = [\n    ""TemporalLastPooling"",\n    ""TemporalAvgPooling"",\n    ""TemporalMaxPooling"",\n    ""TemporalDropLastWrapper"",\n    ""TemporalAttentionPooling"",\n    ""TemporalConcatPooling"",\n    ""LamaPooling"",\n]\n'"
catalyst/contrib/nn/modules/pooling.py,13,"b'import torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nfrom catalyst.contrib.registry import MODULES\n\n\nclass GlobalAvgPool2d(nn.Module):\n    """"""Applies a 2D global average pooling operation over an input signal\n    composed of several input planes.\n\n    @TODO: Docs (add `Example`). Contribution is welcome.\n    """"""\n\n    def __init__(self):\n        """"""Constructor method for the ``GlobalAvgPool2d`` class.""""""\n        super().__init__()\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        """"""Forward call.""""""\n        h, w = x.shape[2:]\n        return F.avg_pool2d(input=x, kernel_size=(h, w))\n\n    @staticmethod\n    def out_features(in_features):\n        """"""Returns number of channels produced by the pooling.\n\n        Args:\n            in_features: number of channels in the input sample\n        """"""\n        return in_features\n\n\nclass GlobalMaxPool2d(nn.Module):\n    """"""Applies a 2D global max pooling operation over an input signal\n    composed of several input planes.\n\n    @TODO: Docs (add `Example`). Contribution is welcome.\n    """"""\n\n    def __init__(self):\n        """"""Constructor method for the ``GlobalMaxPool2d`` class.""""""\n        super().__init__()\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        """"""Forward call.""""""\n        h, w = x.shape[2:]\n        return F.max_pool2d(input=x, kernel_size=(h, w))\n\n    @staticmethod\n    def out_features(in_features):\n        """"""Returns number of channels produced by the pooling.\n\n        Args:\n            in_features: number of channels in the input sample\n        """"""\n        return in_features\n\n\nclass GlobalConcatPool2d(nn.Module):\n    """"""@TODO: Docs (add `Example`). Contribution is welcome.""""""\n\n    def __init__(self):\n        """"""Constructor method for the ``GlobalConcatPool2d`` class.""""""\n        super().__init__()\n        self.avg = GlobalAvgPool2d()\n        self.max = GlobalMaxPool2d()\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        """"""Forward call.""""""\n        return torch.cat([self.avg(x), self.max(x)], 1)\n\n    @staticmethod\n    def out_features(in_features):\n        """"""Returns number of channels produced by the pooling.\n\n        Args:\n            in_features: number of channels in the input sample\n        """"""\n        return in_features * 2\n\n\nclass GlobalAttnPool2d(nn.Module):\n    """"""@TODO: Docs. Contribution is welcome.""""""\n\n    def __init__(self, in_features, activation_fn=""Sigmoid""):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        super().__init__()\n\n        activation_fn = MODULES.get_if_str(activation_fn)\n        self.attn = nn.Sequential(\n            nn.Conv2d(\n                in_features, 1, kernel_size=1, stride=1, padding=0, bias=False\n            ),\n            activation_fn(),\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        """"""Forward call.""""""\n        x_a = self.attn(x)\n        x = x * x_a\n        x = torch.sum(x, dim=[-2, -1], keepdim=True)\n        return x\n\n    @staticmethod\n    def out_features(in_features):\n        """"""Returns number of channels produced by the pooling.\n\n        Args:\n            in_features: number of channels in the input sample\n        """"""\n        return in_features\n\n\nclass GlobalAvgAttnPool2d(nn.Module):\n    """"""@TODO: Docs (add `Example`). Contribution is welcome.""""""\n\n    def __init__(self, in_features, activation_fn=""Sigmoid""):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        super().__init__()\n        self.avg = GlobalAvgPool2d()\n        self.attn = GlobalAttnPool2d(in_features, activation_fn)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        """"""Forward call.""""""\n        return torch.cat([self.avg(x), self.attn(x)], 1)\n\n    @staticmethod\n    def out_features(in_features):\n        """"""Returns number of channels produced by the pooling.\n\n        Args:\n            in_features: number of channels in the input sample\n        """"""\n        return in_features * 2\n\n\nclass GlobalMaxAttnPool2d(nn.Module):\n    """"""@TODO: Docs (add `Example`). Contribution is welcome.""""""\n\n    def __init__(self, in_features, activation_fn=""Sigmoid""):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        super().__init__()\n        self.max = GlobalMaxPool2d()\n        self.attn = GlobalAttnPool2d(in_features, activation_fn)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        """"""Forward call.""""""\n        return torch.cat([self.max(x), self.attn(x)], 1)\n\n    @staticmethod\n    def out_features(in_features):\n        """"""Returns number of channels produced by the pooling.\n\n        Args:\n            in_features: number of channels in the input sample\n        """"""\n        return in_features * 2\n\n\nclass GlobalConcatAttnPool2d(nn.Module):\n    """"""@TODO: Docs (add `Example`). Contribution is welcome.""""""\n\n    def __init__(self, in_features, activation_fn=""Sigmoid""):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        super().__init__()\n        self.avg = GlobalAvgPool2d()\n        self.max = GlobalMaxPool2d()\n        self.attn = GlobalAttnPool2d(in_features, activation_fn)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        """"""Forward call.""""""\n        return torch.cat([self.avg(x), self.max(x), self.attn(x)], 1)\n\n    @staticmethod\n    def out_features(in_features):\n        """"""Returns number of channels produced by the pooling.\n\n        Args:\n            in_features: number of channels in the input sample\n        """"""\n        return in_features * 3\n'"
catalyst/contrib/nn/modules/rms_norm.py,4,"b'import torch\nfrom torch import nn\n\n\nclass RMSNorm(nn.Module):\n    """"""An implementation of RMS Normalization.\n\n    @TODO: Docs (link to paper). Contribution is welcome.\n    """"""\n\n    def __init__(\n        self, dimension: int, epsilon: float = 1e-8, is_bias: bool = False\n    ):\n        """"""\n        Args:\n            dimension (int): the dimension of the layer output to normalize\n            epsilon (float): an epsilon to prevent dividing by zero\n                in case the layer has zero variance. (default = 1e-8)\n            is_bias (bool): a boolean value whether to include bias term\n                while normalization\n        """"""\n        super().__init__()\n        self.dimension = dimension\n        self.epsilon = epsilon\n        self.is_bias = is_bias\n        self.scale = nn.Parameter(torch.ones(self.dimension))\n        if self.is_bias:\n            self.bias = nn.Parameter(torch.zeros(self.dimension))\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        x_std = torch.sqrt(torch.mean(x ** 2, -1, keepdim=True))\n        x_norm = x / (x_std + self.epsilon)\n        if self.is_bias:\n            return self.scale * x_norm + self.bias\n        return self.scale * x_norm\n'"
catalyst/contrib/nn/modules/se.py,10,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass SqueezeAndExcitation(nn.Module):\n    """"""\n    The channel-wise SE (Squeeze and Excitation) block from the\n    [Squeeze-and-Excitation Networks](https://arxiv.org/abs/1709.01507) paper.\n\n    Adapted from\n    https://www.kaggle.com/c/tgs-salt-identification-challenge/discussion/65939\n    and\n    https://www.kaggle.com/c/tgs-salt-identification-challenge/discussion/66178\n\n    Shape:\n\n    - Input: (batch, channels, height, width)\n    - Output: (batch, channels, height, width) (same shape as input)\n    """"""\n\n    def __init__(self, in_channels: int, r: int = 16):\n        """"""\n        Args:\n            in_channels (int): The number of channels\n                in the feature map of the input.\n            r (int): The reduction ratio of the intermediate channels.\n                Default: 16.\n        """"""\n        super().__init__()\n        self.linear_1 = nn.Linear(in_channels, in_channels // r)\n        self.linear_2 = nn.Linear(in_channels // r, in_channels)\n\n    def forward(self, x: torch.Tensor):\n        """"""Forward call.""""""\n        input_x = x\n\n        x = x.view(*(x.shape[:-2]), -1).mean(-1)\n        x = F.relu(self.linear_1(x), inplace=True)\n        x = self.linear_2(x)\n        x = x.unsqueeze(-1).unsqueeze(-1)\n        x = torch.sigmoid(x)\n\n        x = torch.mul(input_x, x)\n        return x\n\n\nclass ChannelSqueezeAndSpatialExcitation(nn.Module):\n    """"""\n    The sSE (Channel Squeeze and Spatial Excitation) block from the\n    [Concurrent Spatial and Channel \xe2\x80\x98Squeeze & Excitation\xe2\x80\x99\n    in Fully Convolutional Networks](https://arxiv.org/abs/1803.02579) paper.\n\n    Adapted from\n    https://www.kaggle.com/c/tgs-salt-identification-challenge/discussion/66178\n\n    Shape:\n\n    - Input: (batch, channels, height, width)\n    - Output: (batch, channels, height, width) (same shape as input)\n    """"""\n\n    def __init__(self, in_channels: int):\n        """"""\n        Args:\n            in_channels (int): The number of channels\n                in the feature map of the input.\n        """"""\n        super().__init__()\n        self.conv = nn.Conv2d(in_channels, 1, kernel_size=1, stride=1)\n\n    def forward(self, x: torch.Tensor):\n        """"""Forward call.""""""\n        input_x = x\n\n        x = self.conv(x)\n        x = torch.sigmoid(x)\n\n        x = torch.mul(input_x, x)\n        return x\n\n\nclass ConcurrentSpatialAndChannelSqueezeAndChannelExcitation(nn.Module):\n    """"""\n    The scSE (Concurrent Spatial and Channel Squeeze and Channel Excitation)\n    block from the [Concurrent Spatial and Channel \xe2\x80\x98Squeeze & Excitation\xe2\x80\x99\n    in Fully Convolutional Networks](https://arxiv.org/abs/1803.02579) paper.\n\n    Adapted from\n    https://www.kaggle.com/c/tgs-salt-identification-challenge/discussion/66178\n\n    Shape:\n\n    - Input: (batch, channels, height, width)\n    - Output: (batch, channels, height, width) (same shape as input)\n    """"""\n\n    def __init__(self, in_channels: int, r: int = 16):\n        """"""\n        Args:\n            in_channels (int): The number of channels\n                in the feature map of the input.\n            r (int): The reduction ratio of the intermediate channels.\n                Default: 16.\n        """"""\n        super().__init__()\n        self.cse_block = SqueezeAndExcitation(in_channels, r)\n        self.sse_block = ChannelSqueezeAndSpatialExcitation(in_channels)\n\n    def forward(self, x: torch.Tensor):\n        """"""Forward call.""""""\n        cse = self.cse_block(x)\n        sse = self.sse_block(x)\n        x = torch.add(cse, sse)\n        return x\n'"
catalyst/contrib/nn/optimizers/__init__.py,1,b'# flake8: noqa\nfrom torch.optim import *\n\nfrom .lamb import Lamb\nfrom .lookahead import Lookahead\nfrom .qhadamw import QHAdamW\nfrom .radam import RAdam\nfrom .ralamb import Ralamb\n'
catalyst/contrib/nn/optimizers/lamb.py,4,"b'from typing import Callable, Optional, Tuple\nimport collections\n\nimport torch\nfrom torch.optim.optimizer import Optimizer\n\n\ndef _log_lamb_rs(optimizer: Optimizer, event_writer, token_count: int):\n    """"""Log a histogram of trust ratio scalars in across layers.""""""\n    results = collections.defaultdict(list)\n    for group in optimizer.param_groups:\n        for p in group[""params""]:\n            state = optimizer.state[p]\n            for i in (""weight_norm"", ""adam_norm"", ""trust_ratio""):\n                if i in state:\n                    results[i].append(state[i])\n\n    for k, v in results.items():\n        event_writer.add_histogram(f""lamb/{k}"", torch.tensor(v), token_count)\n\n\nclass Lamb(Optimizer):\n    """"""Implements Lamb algorithm.\n\n    It has been proposed in `Training BERT in 76 minutes`_.\n\n    .. _`Training BERT in 76 minutes`:\n        https://arxiv.org/abs/1904.00962\n    """"""\n\n    def __init__(\n        self,\n        params,\n        lr: Optional[float] = 1e-3,\n        betas: Optional[Tuple[float, float]] = (0.9, 0.999),\n        eps: Optional[float] = 1e-6,\n        weight_decay: Optional[float] = 0.0,\n        adam: Optional[bool] = False,\n    ):\n        """"""\n        Args:\n            params (iterable): iterable of parameters to optimize or dicts\n                defining parameter groups\n            lr (float, optional): learning rate (default: 1e-3)\n            betas (Tuple[float, float], optional): coefficients used for\n                computing running averages of gradient\n                and its square (default: (0.9, 0.999))\n            eps (float, optional): term added to the denominator to improve\n                numerical stability (default: 1e-8)\n            weight_decay (float, optional): weight decay (L2 penalty)\n                (default: 0)\n            adam (bool, optional): always use trust ratio = 1, which turns\n                this into Adam. Useful for comparison purposes.\n        """"""\n        if not 0.0 <= lr:\n            raise ValueError(f""Invalid learning rate: {lr}"")\n        if not 0.0 <= eps:\n            raise ValueError(f""Invalid epsilon value: {eps}"")\n        if not 0.0 <= betas[0] < 1.0:\n            raise ValueError(f""Invalid beta parameter at index 0: {betas[0]}"")\n        if not 0.0 <= betas[1] < 1.0:\n            raise ValueError(f""Invalid beta parameter at index 1: {betas[1]}"")\n        defaults = {\n            ""lr"": lr,\n            ""betas"": betas,\n            ""eps"": eps,\n            ""weight_decay"": weight_decay,\n        }\n        self.adam = adam\n        super(Lamb, self).__init__(params, defaults)\n\n    def step(self, closure: Optional[Callable] = None):\n        """"""Makes optimizer step.\n\n        Args:\n            closure (callable, optional): A closure that reevaluates\n                the model and returns the loss.\n        """"""\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            for p in group[""params""]:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data\n                if grad.is_sparse:\n                    raise RuntimeError(\n                        ""Lamb does not support sparse gradients, ""\n                        ""consider SparseAdam instad.""\n                    )\n\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state[""step""] = 0\n                    # Exponential moving average of gradient values\n                    state[""exp_avg""] = torch.zeros_like(p.data)\n                    # Exponential moving average of squared gradient values\n                    state[""exp_avg_sq""] = torch.zeros_like(p.data)\n\n                exp_avg, exp_avg_sq = state[""exp_avg""], state[""exp_avg_sq""]\n                beta1, beta2 = group[""betas""]\n\n                state[""step""] += 1\n\n                # Decay the first and second moment\n                # running average coefficient\n                # m_t\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n                # v_t\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n\n                # Paper v3 does not use debiasing.\n                # bias_correction1 = 1 - beta1 ** state[""step""]\n                # bias_correction2 = 1 - beta2 ** state[""step""]\n                # Apply bias to lr to avoid broadcast.\n                # * math.sqrt(bias_correction2) / bias_correction1\n                step_size = group[""lr""]\n\n                weight_norm = p.data.pow(2).sum().sqrt().clamp(0, 10)\n\n                adam_step = exp_avg / exp_avg_sq.sqrt().add(group[""eps""])\n                if group[""weight_decay""] != 0:\n                    adam_step.add_(group[""weight_decay""], p.data)\n\n                adam_norm = adam_step.pow(2).sum().sqrt()\n                if weight_norm == 0 or adam_norm == 0:\n                    trust_ratio = 1\n                else:\n                    trust_ratio = weight_norm / adam_norm\n                state[""weight_norm""] = weight_norm\n                state[""adam_norm""] = adam_norm\n                state[""trust_ratio""] = trust_ratio\n                if self.adam:\n                    trust_ratio = 1\n\n                p.data.add_(-step_size * trust_ratio, adam_step)\n\n        return loss\n'"
catalyst/contrib/nn/optimizers/lookahead.py,3,"b'from typing import Callable, Dict, Optional\nfrom collections import defaultdict\n\nimport torch\nfrom torch.optim import Optimizer\n\n\nclass Lookahead(Optimizer):\n    """"""Implements Lookahead algorithm.\n\n    It has been proposed in `Lookahead Optimizer: k steps forward,\n    1 step back`_.\n\n    Adapted from:\n    https://github.com/alphadl/lookahead.pytorch (MIT License)\n\n    .. _`Lookahead Optimizer\\: k steps forward, 1 step back`:\n        https://arxiv.org/abs/1907.08610\n    """"""\n\n    def __init__(self, optimizer: Optimizer, k: int = 5, alpha: float = 0.5):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        self.optimizer = optimizer\n        self.k = k\n        self.alpha = alpha\n        self.param_groups = self.optimizer.param_groups\n        self.defaults = self.optimizer.defaults\n        self.state = defaultdict(dict)\n        self.fast_state = self.optimizer.state\n        for group in self.param_groups:\n            group[""counter""] = 0\n\n    def update(self, group):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        for fast in group[""params""]:\n            param_state = self.state[fast]\n            if ""slow_param"" not in param_state:\n                param_state[""slow_param""] = torch.zeros_like(fast.data)\n                param_state[""slow_param""].copy_(fast.data)\n            slow = param_state[""slow_param""]\n            slow += (fast.data - slow) * self.alpha\n            fast.data.copy_(slow)\n\n    def update_lookahead(self):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        for group in self.param_groups:\n            self.update(group)\n\n    def step(self, closure: Optional[Callable] = None):\n        """"""Makes optimizer step.\n\n        Args:\n            closure (callable, optional): A closure that reevaluates\n                the model and returns the loss.\n        """"""\n        loss = self.optimizer.step(closure)\n        for group in self.param_groups:\n            if group[""counter""] == 0:\n                self.update(group)\n            group[""counter""] += 1\n            if group[""counter""] >= self.k:\n                group[""counter""] = 0\n        return loss\n\n    def state_dict(self):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        fast_state_dict = self.optimizer.state_dict()\n        slow_state = {\n            (id(k) if isinstance(k, torch.Tensor) else k): v\n            for k, v in self.state.items()\n        }\n        fast_state = fast_state_dict[""state""]\n        param_groups = fast_state_dict[""param_groups""]\n        return {\n            ""fast_state"": fast_state,\n            ""slow_state"": slow_state,\n            ""param_groups"": param_groups,\n        }\n\n    def load_state_dict(self, state_dict):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        slow_state_dict = {\n            ""state"": state_dict[""slow_state""],\n            ""param_groups"": state_dict[""param_groups""],\n        }\n        fast_state_dict = {\n            ""state"": state_dict[""fast_state""],\n            ""param_groups"": state_dict[""param_groups""],\n        }\n        super(Lookahead, self).load_state_dict(slow_state_dict)\n        self.optimizer.load_state_dict(fast_state_dict)\n        self.fast_state = self.optimizer.state\n\n    def add_param_group(self, param_group):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        param_group[""counter""] = 0\n        self.optimizer.add_param_group(param_group)\n\n    @classmethod\n    def get_from_params(\n        cls, params: Dict, base_optimizer_params: Dict = None, **kwargs,\n    ) -> ""Lookahead"":\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        from catalyst.dl.registry import OPTIMIZERS\n\n        base_optimizer = OPTIMIZERS.get_from_params(\n            params=params, **base_optimizer_params\n        )\n        optimizer = cls(optimizer=base_optimizer, **kwargs)\n        return optimizer\n'"
catalyst/contrib/nn/optimizers/qhadamw.py,3,"b'from typing import Callable, Optional\n\nimport torch\nfrom torch.optim.optimizer import Optimizer\n\n\nclass QHAdamW(Optimizer):\n    """"""Implements QHAdam algorithm.\n\n    Combines QHAdam algorithm that was proposed in  `Quasi-hyperbolic momentum\n    and Adam for deep learning`_ with weight decay decoupling from\n    `Decoupled Weight Decay Regularization`_ paper.\n\n    Example:\n        >>> optimizer = QHAdamW(\n        ...     model.parameters(),\n        ...     lr=3e-4, nus=(0.8, 1.0), betas=(0.99, 0.999))\n        >>> optimizer.zero_grad()\n        >>> loss_fn(model(input), target).backward()\n        >>> optimizer.step()\n\n    Adapted from:\n    https://github.com/iprally/qhadamw-pytorch/blob/master/qhadamw.py\n    (MIT License)\n\n    .. _Decoupled Weight Decay Regularization:\n        https://arxiv.org/abs/1711.05101\n    .. _Quasi-hyperbolic momentum and Adam for deep learning:\n        https://arxiv.org/abs/1810.06801\n    """"""\n\n    def __init__(\n        self,\n        params,\n        lr=1e-3,\n        betas=(0.995, 0.999),\n        nus=(0.7, 1.0),\n        weight_decay=0.0,\n        eps=1e-8,\n    ):\n        r""""""\n        Args:\n            params (iterable):\n                iterable of parameters to optimize or dicts defining parameter\n                groups\n            lr (float, optional): learning rate (:math:`\\alpha` from the paper)\n                (default: 1e-3)\n            betas (Tuple[float, float], optional): coefficients used for\n                computing running averages of the gradient and its square\n                (default: (0.995, 0.999))\n            nus (Tuple[float, float], optional): immediate discount factors\n                used to estimate the gradient and its square\n                (default: (0.7, 1.0))\n            eps (float, optional): term added to the denominator to improve\n                numerical stability\n                (default: 1e-8)\n            weight_decay (float, optional): weight decay\n                (L2 regularization coefficient, times two)\n                (default: 0.0)\n        """"""\n        if not 0.0 <= lr:\n            raise ValueError(f""Invalid learning rate: {lr}"")\n        if not 0.0 <= eps:\n            raise ValueError(f""Invalid epsilon value: {eps}"")\n        if not 0.0 <= betas[0] < 1.0:\n            raise ValueError(f""Invalid beta parameter at index 0: {betas[0]}"")\n        if not 0.0 <= betas[1] < 1.0:\n            raise ValueError(f""Invalid beta parameter at index 1: {betas[1]}"")\n        if weight_decay < 0.0:\n            raise ValueError(f""Invalid weight_decay value: {weight_decay}"")\n\n        defaults = {\n            ""lr"": lr,\n            ""betas"": betas,\n            ""nus"": nus,\n            ""weight_decay"": weight_decay,\n            ""eps"": eps,\n        }\n        super(QHAdamW, self).__init__(params, defaults)\n\n    def step(self, closure: Optional[Callable] = None):\n        """"""Makes optimizer step.\n\n        Args:\n            closure (callable, optional): A closure that reevaluates\n                the model and returns the loss.\n        """"""\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            lr = group[""lr""]\n            beta1, beta2 = group[""betas""]\n            nu1, nu2 = group[""nus""]\n            weight_decay = group[""weight_decay""]\n            eps = group[""eps""]\n\n            for p in group[""params""]:\n                if p.grad is None:\n                    continue\n\n                d_p = p.grad.data\n                if d_p.is_sparse:\n                    raise RuntimeError(\n                        ""QHAdamW does not support sparse gradients""\n                    )\n\n                param_state = self.state[p]\n\n                # Original QHAdam implementation for weight decay:\n                # if weight_decay != 0:\n                #    d_p.add_(weight_decay, p.data)\n\n                d_p_sq = d_p.mul(d_p)\n\n                if len(param_state) == 0:\n                    param_state[""beta1_weight""] = 0.0\n                    param_state[""beta2_weight""] = 0.0\n                    param_state[""exp_avg""] = torch.zeros_like(p.data)\n                    param_state[""exp_avg_sq""] = torch.zeros_like(p.data)\n\n                param_state[""beta1_weight""] = (\n                    1.0 + beta1 * param_state[""beta1_weight""]\n                )\n                param_state[""beta2_weight""] = (\n                    1.0 + beta2 * param_state[""beta2_weight""]\n                )\n\n                beta1_weight = param_state[""beta1_weight""]\n                beta2_weight = param_state[""beta2_weight""]\n                exp_avg = param_state[""exp_avg""]\n                exp_avg_sq = param_state[""exp_avg_sq""]\n\n                beta1_adj = 1.0 - (1.0 / beta1_weight)\n                beta2_adj = 1.0 - (1.0 / beta2_weight)\n                exp_avg.mul_(beta1_adj).add_(1.0 - beta1_adj, d_p)\n                exp_avg_sq.mul_(beta2_adj).add_(1.0 - beta2_adj, d_p_sq)\n\n                avg_grad = exp_avg.mul(nu1)\n                if nu1 != 1.0:\n                    avg_grad.add_(1.0 - nu1, d_p)\n\n                avg_grad_rms = exp_avg_sq.mul(nu2)\n                if nu2 != 1.0:\n                    avg_grad_rms.add_(1.0 - nu2, d_p_sq)\n                avg_grad_rms.sqrt_()\n                if eps != 0.0:\n                    avg_grad_rms.add_(eps)\n\n                # Original QHAdam implementation:\n                # p.data.addcdiv_(-lr, avg_grad, avg_grad_rms)\n\n                # Implementation following AdamW paper:\n                p.data.add_(-weight_decay, p.data).addcdiv_(\n                    -lr, avg_grad, avg_grad_rms\n                )\n\n        return loss\n'"
catalyst/contrib/nn/optimizers/radam.py,3,"b'from typing import Callable, Optional\nimport math\n\nimport torch\nfrom torch.optim.optimizer import Optimizer\n\n\nclass RAdam(Optimizer):\n    """"""Implements RAdam algorithm.\n\n    It has been proposed in `On the Variance of the Adaptive Learning Rate\n    and Beyond`_.\n\n    @TODO: Docs (add `Example`). Contribution is welcome\n\n    Adapted from:\n    https://github.com/LiyuanLucasLiu/RAdam (Apache-2.0 License)\n\n    .. _On the Variance of the Adaptive Learning Rate and Beyond:\n        https://arxiv.org/abs/1908.03265\n    """"""\n\n    def __init__(\n        self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0\n    ):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        defaults = {\n            ""lr"": lr,\n            ""betas"": betas,\n            ""eps"": eps,\n            ""weight_decay"": weight_decay,\n        }\n        self.buffer = [[None, None, None] for _ in range(10)]\n        super(RAdam, self).__init__(params, defaults)\n\n    def __setstate__(self, state):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        super(RAdam, self).__setstate__(state)\n\n    def step(self, closure: Optional[Callable] = None):\n        """"""Makes optimizer step.\n\n        Args:\n            closure (callable, optional): A closure that reevaluates\n                the model and returns the loss.\n        """"""\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n\n            for p in group[""params""]:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data.float()\n                if grad.is_sparse:\n                    raise RuntimeError(\n                        ""RAdam does not support sparse gradients""\n                    )\n\n                p_data_fp32 = p.data.float()\n\n                state = self.state[p]\n\n                if len(state) == 0:\n                    state[""step""] = 0\n                    state[""exp_avg""] = torch.zeros_like(p_data_fp32)\n                    state[""exp_avg_sq""] = torch.zeros_like(p_data_fp32)\n                else:\n                    state[""exp_avg""] = state[""exp_avg""].type_as(p_data_fp32)\n                    state[""exp_avg_sq""] = state[""exp_avg_sq""].type_as(\n                        p_data_fp32\n                    )\n\n                exp_avg, exp_avg_sq = state[""exp_avg""], state[""exp_avg_sq""]\n                beta1, beta2 = group[""betas""]\n\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n\n                state[""step""] += 1\n                buffered = self.buffer[int(state[""step""] % 10)]\n                if state[""step""] == buffered[0]:\n                    N_sma, step_size = buffered[1], buffered[2]\n                else:\n                    buffered[0] = state[""step""]\n                    beta2_t = beta2 ** state[""step""]\n                    N_sma_max = 2 / (1 - beta2) - 1\n                    N_sma = N_sma_max - 2 * state[""step""] * beta2_t / (\n                        1 - beta2_t\n                    )\n                    buffered[1] = N_sma\n\n                    # more conservative since it\'s an approximated value\n                    if N_sma >= 5:\n                        step_size = (\n                            group[""lr""]\n                            * math.sqrt(\n                                (1 - beta2_t)\n                                * (N_sma - 4)\n                                / (N_sma_max - 4)\n                                * (N_sma - 2)\n                                / N_sma\n                                * N_sma_max\n                                / (N_sma_max - 2)\n                            )\n                            / (1 - beta1 ** state[""step""])\n                        )\n                    else:\n                        step_size = group[""lr""] / (1 - beta1 ** state[""step""])\n                    buffered[2] = step_size\n\n                if group[""weight_decay""] != 0:\n                    p_data_fp32.add_(\n                        -group[""weight_decay""] * group[""lr""], p_data_fp32\n                    )\n\n                # more conservative since it\'s an approximated value\n                if N_sma >= 5:\n                    denom = exp_avg_sq.sqrt().add_(group[""eps""])\n                    p_data_fp32.addcdiv_(-step_size, exp_avg, denom)\n                else:\n                    p_data_fp32.add_(-step_size, exp_avg)\n\n                p.data.copy_(p_data_fp32)\n\n        return loss\n'"
catalyst/contrib/nn/optimizers/ralamb.py,3,"b'from typing import Callable, Iterable, Optional, Tuple\nimport math\n\nimport torch\nfrom torch.optim.optimizer import Optimizer\n\n\nclass Ralamb(Optimizer):\n    """"""RAdam optimizer with LARS/LAMB tricks.\n\n    Adapted from:\n    https://github.com/mgrankin/over9000/blob/master/ralamb.py\n    (Apache-2.0 License)\n    """"""\n\n    def __init__(\n        self,\n        params: Iterable,\n        lr: float = 1e-3,\n        betas: Tuple[float, float] = (0.9, 0.999),\n        eps: float = 1e-8,\n        weight_decay: float = 0,\n    ):\n        """"""\n        Args:\n            params (iterable): iterable of parameters to optimize\n                or dicts defining parameter groups\n            lr (float, optional): learning rate (default: 1e-3)\n            betas (Tuple[float, float], optional): coefficients used for\n                computing running averages of gradient\n                and its square (default: (0.9, 0.999))\n            eps (float, optional): term added to the denominator to improve\n                numerical stability (default: 1e-8)\n            weight_decay (float, optional): weight decay\n                (L2 penalty) (default: 0)\n        """"""\n        defaults = {\n            ""lr"": lr,\n            ""betas"": betas,\n            ""eps"": eps,\n            ""weight_decay"": weight_decay,\n        }\n        self.buffer = [[None, None, None] for ind in range(10)]\n        super(Ralamb, self).__init__(params, defaults)\n\n    def __setstate__(self, state):\n        """"""Sets state.""""""\n        super(Ralamb, self).__setstate__(state)\n\n    def step(self, closure: Optional[Callable] = None):\n        """"""Makes optimizer step.\n\n        Args:\n            closure (callable, optional): A closure that reevaluates\n                the model and returns the loss.\n        """"""\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n\n            for p in group[""params""]:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data.float()\n                if grad.is_sparse:\n                    raise RuntimeError(\n                        ""Ralamb does not support sparse gradients""\n                    )\n\n                p_data_fp32 = p.data.float()\n\n                state = self.state[p]\n\n                if len(state) == 0:\n                    state[""step""] = 0\n                    state[""exp_avg""] = torch.zeros_like(p_data_fp32)\n                    state[""exp_avg_sq""] = torch.zeros_like(p_data_fp32)\n                else:\n                    state[""exp_avg""] = state[""exp_avg""].type_as(p_data_fp32)\n                    state[""exp_avg_sq""] = state[""exp_avg_sq""].type_as(\n                        p_data_fp32\n                    )\n\n                exp_avg, exp_avg_sq = state[""exp_avg""], state[""exp_avg_sq""]\n                beta1, beta2 = group[""betas""]\n\n                # Decay the first and second moment running average coefficient\n                # m_t\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n                # v_t\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n\n                state[""step""] += 1\n                buffered = self.buffer[int(state[""step""] % 10)]\n\n                if state[""step""] == buffered[0]:\n                    N_sma, radam_step_size = buffered[1], buffered[2]\n                else:\n                    buffered[0] = state[""step""]\n                    beta2_t = beta2 ** state[""step""]\n                    N_sma_max = 2 / (1 - beta2) - 1\n                    N_sma = N_sma_max - 2 * state[""step""] * beta2_t / (\n                        1 - beta2_t\n                    )\n                    buffered[1] = N_sma\n\n                    # more conservative since it""s an approximated value\n                    if N_sma >= 5:\n                        radam_step_size = math.sqrt(\n                            (1 - beta2_t)\n                            * (N_sma - 4)\n                            / (N_sma_max - 4)\n                            * (N_sma - 2)\n                            / N_sma\n                            * N_sma_max\n                            / (N_sma_max - 2)\n                        ) / (1 - beta1 ** state[""step""])\n                    else:\n                        radam_step_size = 1.0 / (1 - beta1 ** state[""step""])\n                    buffered[2] = radam_step_size\n\n                if group[""weight_decay""] != 0:\n                    p_data_fp32.add_(\n                        -group[""weight_decay""] * group[""lr""], p_data_fp32\n                    )\n\n                # more conservative since it""s an approximated value\n                radam_step = p_data_fp32.clone()\n                if N_sma >= 5:\n                    denom = exp_avg_sq.sqrt().add_(group[""eps""])\n                    radam_step.addcdiv_(\n                        -radam_step_size * group[""lr""], exp_avg, denom\n                    )\n                else:\n                    radam_step.add_(-radam_step_size * group[""lr""], exp_avg)\n\n                radam_norm = radam_step.pow(2).sum().sqrt()\n                weight_norm = p.data.pow(2).sum().sqrt().clamp(0, 10)\n                if weight_norm == 0 or radam_norm == 0:\n                    trust_ratio = 1\n                else:\n                    trust_ratio = weight_norm / radam_norm\n\n                state[""weight_norm""] = weight_norm\n                state[""adam_norm""] = radam_norm\n                state[""trust_ratio""] = trust_ratio\n\n                if N_sma >= 5:\n                    p_data_fp32.addcdiv_(\n                        -radam_step_size * group[""lr""] * trust_ratio,\n                        exp_avg,\n                        denom,\n                    )\n                else:\n                    p_data_fp32.add_(\n                        -radam_step_size * group[""lr""] * trust_ratio, exp_avg\n                    )\n\n                p.data.copy_(p_data_fp32)\n\n        return loss\n'"
catalyst/contrib/nn/schedulers/__init__.py,1,"b'# flake8: noqa\nfrom torch.optim.lr_scheduler import *\n\nfrom .base import BaseScheduler, BatchScheduler\nfrom .onecycle import OneCycleLRWithWarmup\n'"
catalyst/contrib/nn/schedulers/base.py,1,"b'from typing import List, Optional\nfrom abc import ABC\n\nfrom torch.optim.lr_scheduler import _LRScheduler\n\nfrom catalyst.utils.torch import set_optimizer_momentum\n\n\nclass BaseScheduler(_LRScheduler, ABC):\n    """"""Base class for all schedulers with momentum update.""""""\n\n    def get_momentum(self) -> List[float]:\n        """"""Function that returns the new momentum for optimizer.\n\n        Returns:\n            List[float]: calculated momentum for every param groups\n        """"""\n        raise NotImplementedError\n\n    def step(self, epoch: Optional[int] = None) -> None:\n        """"""Make one scheduler step.\n\n        Args:\n            epoch (int, optional): current epoch num\n        """"""\n        super().step(epoch)\n        momentums = self.get_momentum()\n        for i, momentum in enumerate(momentums):\n            set_optimizer_momentum(self.optimizer, momentum, index=i)\n\n\nclass BatchScheduler(BaseScheduler, ABC):\n    """"""@TODO: Docs. Contribution is welcome.""""""\n'"
catalyst/contrib/nn/schedulers/onecycle.py,1,"b'from typing import List\n\nimport numpy as np\n\nfrom torch.optim import Optimizer\n\nfrom catalyst.utils.torch import get_optimizer_momentum\n\nfrom .base import BatchScheduler\n\n\nclass OneCycleLRWithWarmup(BatchScheduler):\n    """"""OneCycle scheduler with warm-up & lr decay stages.\n\n    First stage increases lr from ``init_lr`` to ``max_lr``,\n    and called ``warmup``. Also it decreases momentum\n    from ``init_momentum`` to ``min_momentum``. Takes ``warmup_steps`` steps\n\n    Second is ``annealing`` stage. Decrease lr from ``max_lr`` to ``min_lr``,\n    Increase momentum from ``min_momentum`` to ``max_momentum``.\n\n    Third, optional, lr decay.\n    """"""\n\n    def __init__(\n        self,\n        optimizer: Optimizer,\n        num_steps: int,\n        lr_range=(1.0, 0.005),\n        init_lr: float = None,\n        warmup_steps: int = 0,\n        warmup_fraction: float = None,\n        decay_steps: int = 0,\n        decay_fraction: float = None,\n        momentum_range=(0.8, 0.99, 0.999),\n        init_momentum: float = None,\n    ):\n        """"""\n        Args:\n            optimizer: PyTorch optimizer\n            num_steps (int): total number of steps\n            lr_range: tuple with two or three elements\n                (max_lr, min_lr, [final_lr])\n            init_lr (float, optional): initial lr\n            warmup_steps (int): count of steps for warm-up stage\n            warmup_fraction (float, optional): fraction in [0; 1) to calculate\n                number of warmup steps.\n                Cannot be set together with ``warmup_steps``\n            decay_steps (int): count of steps for lr decay stage\n            decay_fraction (float, optional): fraction in [0; 1) to calculate\n                number of decay steps.\n                Cannot be set together with ``decay_steps``\n            momentum_range: tuple with two or three elements\n                (min_momentum, max_momentum, [final_momentum])\n            init_momentum (float, optional): initial momentum\n        """"""\n        if len(lr_range) == 2:\n            max_lr, min_lr = lr_range\n            final_lr = min_lr\n        elif len(lr_range) == 3:\n            max_lr, min_lr, final_lr = lr_range\n\n        if len(momentum_range) == 2:\n            min_momentum, max_momentum = momentum_range\n            final_momentum = max_momentum\n        elif len(momentum_range) == 3:\n            min_momentum, max_momentum, final_momentum = momentum_range\n\n        if init_lr is None:\n            init_lr = optimizer.defaults[""lr""]\n        if init_momentum is None:\n            init_momentum = get_optimizer_momentum(optimizer)\n\n        warmup_steps = self._calculate_warmup(\n            num_steps, warmup_steps, warmup_fraction\n        )\n\n        decay_steps = self._calculate_decay(\n            num_steps, decay_steps, decay_fraction\n        )\n\n        lr_annealing_steps = num_steps - (warmup_steps + decay_steps)\n\n        self.warmup_steps = warmup_steps\n        self.lr_annealing_steps = lr_annealing_steps\n        self.decay_steps = decay_steps\n        self.num_steps = warmup_steps + lr_annealing_steps + decay_steps\n\n        self.lr_range = init_lr, max_lr, min_lr, final_lr\n        self.momentum_range = (\n            init_momentum,\n            min_momentum,\n            max_momentum,\n            final_momentum,\n        )\n\n        self._calculate_lr_momentum(\n            warmup_steps, lr_annealing_steps, decay_steps\n        )\n\n        self.total_groups = len(optimizer.param_groups)\n        super().__init__(optimizer)\n\n    def _calculate_warmup(\n        self, num_steps: int, warmup_steps: int, warmup_fraction: float\n    ):\n        if warmup_fraction is not None:\n            assert 0.0 <= warmup_fraction < 1.0 and warmup_steps == 0, (\n                ""You should pass either warmup_steps or ""\n                ""warmup_fraction in range [0; 1) ""\n            )\n            warmup_steps = int(num_steps * warmup_fraction)\n\n        self.warmup_steps = warmup_steps\n        self.has_warmup = warmup_steps != 0\n        return self.warmup_steps\n\n    def _calculate_decay(\n        self, num_steps: int, decay_steps: int, decay_fraction: float\n    ):\n        if decay_fraction is not None:\n            assert 0.0 <= decay_fraction < 1.0 and decay_steps == 0, (\n                ""You should pass either decay_steps or ""\n                ""decay_fraction in range [0; 1) ""\n            )\n            decay_steps = int(num_steps * decay_fraction)\n\n        self.decay_steps = decay_steps\n        self.has_decay = decay_steps != 0\n        return self.decay_steps\n\n    def _calculate_lr_momentum(\n        self, warmup_steps: int, lr_annealing_steps: int, decay_steps: int\n    ):\n        init_lr, max_lr, min_lr, final_lr = self.lr_range\n        (\n            init_momentum,\n            min_momentum,\n            max_momentum,\n            final_momentum,\n        ) = self.momentum_range\n\n        lr_warmup = np.linspace(init_lr, max_lr, warmup_steps)\n        lr_annealing = np.linspace(max_lr, min_lr, lr_annealing_steps)\n        lr_decay = np.linspace(min_lr, final_lr, decay_steps)\n\n        self.learning_rates = np.concatenate(\n            (lr_warmup, lr_annealing, lr_decay)\n        )\n\n        momentum_decay = np.linspace(init_momentum, min_momentum, warmup_steps)\n        momentum_annealing = np.linspace(\n            min_momentum, max_momentum, lr_annealing_steps\n        )\n        momentum_warmup = np.linspace(\n            max_momentum, final_momentum, decay_steps\n        )\n\n        self.momentums = np.concatenate(\n            (momentum_decay, momentum_annealing, momentum_warmup)\n        )\n\n    def _get_steps_lr_momentum(self, step_num: int):\n        if step_num < len(self.learning_rates):\n            lr = self.learning_rates[step_num]\n        else:\n            _, _, _, final_lr = self.lr_range\n            lr = final_lr\n\n        if step_num < len(self.momentums):\n            momentum = self.momentums[step_num]\n        else:\n            _, _, _, final_momentum = self.momentum_range\n            momentum = final_momentum\n        return lr, momentum\n\n    def get_lr(self) -> List[float]:\n        """"""Function that returns the new lr for optimizer.\n\n        Returns:\n            List[float]: calculated lr for every param groups\n        """"""\n        lr, _ = self._get_steps_lr_momentum(self.last_epoch)\n        return [lr] * self.total_groups\n\n    def get_momentum(self) -> List[float]:\n        """"""Function that returns the new momentum for optimizer.\n\n        Returns:\n            List[float]: calculated momentum for every param groups\n        """"""\n        _, momentum = self._get_steps_lr_momentum(self.last_epoch)\n        return [momentum] * self.total_groups\n\n    def reset(self):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        self._calculate_lr_momentum(\n            self.warmup_steps, self.lr_annealing_steps, self.decay_steps\n        )\n        self.last_epoch = 0\n\n    def recalculate(self, loader_len: int, current_step: int,) -> None:\n        """"""Recalculates total num_steps for ``batch`` mode.\n\n        Args:\n            loader_len (int): total count of batches in an epoch\n            current_step (int): current step\n        """"""\n        warmup_steps = self.warmup_steps * loader_len\n        lr_annealing_steps = self.lr_annealing_steps * loader_len\n        decay_steps = self.decay_steps * loader_len\n\n        self._calculate_lr_momentum(\n            warmup_steps, lr_annealing_steps, decay_steps\n        )\n        self.last_epoch = current_step * loader_len\n'"
catalyst/contrib/nn/tests/__init__.py,0,b''
catalyst/contrib/nn/tests/test_criterion.py,0,"b'from catalyst.contrib.nn import criterion as module\nfrom catalyst.contrib.nn.criterion import CircleLoss\n\n\ndef test_criterion_init():\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    for cls in module.__dict__.values():\n        if isinstance(cls, type):\n            if cls == CircleLoss:\n                instance = cls(margin=0.25, gamma=256)\n            else:\n                instance = cls()\n            assert instance is not None\n'"
catalyst/contrib/nn/tests/test_optimizer.py,0,"b'from torch import nn, optim\n\nfrom catalyst.contrib.nn import optimizers as module\nfrom catalyst.contrib.nn.optimizers import Lookahead\n\n\ndef test_optimizer_init():\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    model = nn.Linear(10, 10)\n    for name, cls in module.__dict__.items():\n        if isinstance(cls, type):\n            if name == ""Optimizer"":\n                continue\n            elif cls == Lookahead:\n                instance = optim.SGD(model.parameters(), lr=1e-3)\n                instance = cls(instance)\n            else:\n                instance = cls(model.parameters(), lr=1e-3)\n            assert instance is not None\n'"
catalyst/contrib/tools/tests/__init__.py,0,b''
catalyst/contrib/tools/tests/test_tensorboard.py,0,"b'# flake8: noqa\n\nfrom io import BytesIO\nfrom pathlib import Path\nfrom unittest.mock import patch\n\nimport numpy as np\nimport pytest\n\nfrom catalyst.contrib.tools.tensorboard import (\n    EventReadingException,\n    EventsFileReader,\n    SummaryReader,\n)\n\n\ndef _get_test_data():\n    """"""Test events file\n\n    tag  value         step\n    -----------------------\n    x     1.0          1\n    y    -1.0          1\n    x     2.0          2\n\n    The first event is empty with wall_time = 1557489465\n\n    log.add_scalar(""x"", 1.0, global_step=1)\n    log.add_scalar(""y"", -1.0, global_step=1)\n    log.add_scalar(""x"", 2.0, global_step=2)\n    """"""\n\n    data_raw = [\n        None,\n        {""tag"": ""x"", ""value"": 1.0, ""step"": 1, ""type"": ""scalar""},\n        {""tag"": ""y"", ""value"": -1.0, ""step"": 1, ""type"": ""scalar""},\n        {""tag"": ""x"", ""value"": 2.0, ""step"": 2, ""type"": ""scalar""},\n    ]\n    # noqa: Q000\n    data = (\n        b""\\t\\x00\\x00\\x00\\x00\\x00\\x00\\x007\\xf9q9\\t\\xc9\\xebE\\x18`5""\n        b""\\xd7A\\x04A\\xf4n\\x17\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xe7\\xce""\n        b""\\xf8\\x1e\\t=\\x82{\\x19`5\\xd7A\\x10\\x01*\\n\\n\\x08\\n\\x01x\\x15""\n        b""\\x00\\x00\\x80?c\\xf1\\xd84\\x17\\x00\\x00\\x00\\x00\\x00\\x00\\x00""\n        b""\\xe7\\xce\\xf8\\x1e\\tT\\xe4g\\x1a`5\\xd7A\\x10\\x01*\\n\\n\\x08\\n""\n        b""\\x01y\\x15\\x00\\x00\\x80\\xbf{;wp\\x17\\x00\\x00\\x00\\x00\\x00\\x00""\n        b\'\\x00\\xe7\\xce\\xf8\\x1e\\t""S\\xbc\\x1b`5\\xd7A\\x10\\x02*\\n\\n\\x08\'\n        b""\\n\\x01x\\x15\\x00\\x00\\x00@\\x1d\\xb9\\xdc\\x83`\\x00\\x00\\x00\\x00""\n        b\'\\x00\\x00\\x00(!\\xc6\\xda\\t.\\x03H""`5\\xd7A\\x10\\x01*S\\nQ\\n\\x01\'\n        b\'z""L\\x08\\x02\\x10\\x02\\x18\\x03""D\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\'\n        b""\\rIHDR\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\x02\\x08\\x02\\x00\\x00\\x00""\n        b""\\xfd\\xd4\\x9as\\x00\\x00\\x00\\x0bIDATx\\x9cc`@\\x06\\x00\\x00\\x0e""\n        b""\\x00\\x01\\xa9\\x91s\\xb1\\x00\\x00\\x00\\x00IEND\\xaeB`\\x82\\x96x9j""\n    )\n    return data, data_raw\n\n\ndef test_events_reader_successful():\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    data, data_raw = _get_test_data()\n    reader = EventsFileReader(BytesIO(data))\n    for event, event_raw in zip(reader, data_raw):\n        if event_raw is not None:\n            assert event.step == event_raw[""step""]\n            assert event.HasField(""summary"")\n            assert len(event.summary.value) == 1\n            if event_raw[""type""] == ""scalar"":\n                assert event.summary.value[0].HasField(""simple_value"")\n                assert event.summary.value[0].tag == event_raw[""tag""]\n                assert (\n                    event.summary.value[0].simple_value == event_raw[""value""]\n                )\n\n\ndef test_events_reader_empty():\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    data = BytesIO(b"""")\n    reader = EventsFileReader(data)\n    assert len(list(reader)) == 0\n\n\ndef test_events_reader_invalid_data():\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    data, _ = _get_test_data()\n    data1 = bytearray(data)\n    data1[0] = (data1[0] + 1) % 256\n    reader = EventsFileReader(BytesIO(data1))\n    with pytest.raises(EventReadingException):\n        list(reader)\n\n    data2 = bytearray(data)\n    data2[123] = (data2[123] + 1) % 256\n    reader = EventsFileReader(BytesIO(data2))\n    with pytest.raises(EventReadingException):\n        list(reader)\n\n\ndef test_events_reader_unexpected_end():\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    data, _ = _get_test_data()\n    data = data[:-5]\n    reader = EventsFileReader(BytesIO(data))\n    with pytest.raises(EventReadingException):\n        list(reader)\n\n\ndef _open(path, mode):\n    data, _ = _get_test_data()\n    return BytesIO(data)\n\n\n@patch(""pathlib.Path.glob"", lambda s, p: [Path(""1""), Path(""2"")])\n@patch(""pathlib.Path.is_file"", lambda s: True)\n@patch(""builtins.open"", _open)\ndef test_summary_reader_iterate():\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    reader = SummaryReader(""logs"", types=[""scalar""])\n    _, data_raw = _get_test_data()\n    data_raw2 = 2 * [d for d in data_raw if d is not None]\n    items = list(reader)\n\n    assert len(items) == len(data_raw2)\n\n    for item, event_raw in zip(items, data_raw2):\n        assert item.step == event_raw[""step""]\n        assert item.tag == event_raw[""tag""]\n        assert item.type == event_raw[""type""]\n        assert np.all(item.value == event_raw[""value""])\n\n\n@patch(""pathlib.Path.glob"", lambda s, p: [Path(""1""), Path(""2"")])\n@patch(""pathlib.Path.is_file"", lambda s: True)\n@patch(""builtins.open"", _open)\ndef test_summary_reader_filter():\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    tags = [""x"", ""z""]\n    reader = SummaryReader(""logs"", tag_filter=tags, types=[""scalar""])\n    _, data_raw = _get_test_data()\n    data_raw2 = 2 * [d for d in data_raw if d is not None and d[""tag""] in tags]\n    items = list(reader)\n\n    assert len(items) == len(data_raw2)\n\n    for item, event_raw in zip(items, data_raw2):\n        assert item.step == event_raw[""step""]\n        assert item.tag == event_raw[""tag""]\n        assert item.type == event_raw[""type""]\n        assert item.tag in tags\n        assert np.all(item.value == event_raw[""value""])\n\n\n@patch(""pathlib.Path.glob"", lambda s, p: [Path(""1""), Path(""2"")])\n@patch(""pathlib.Path.is_file"", lambda s: True)\n@patch(""builtins.open"", _open)\ndef test_summary_reader_filter_scalars():\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    types = [""scalar""]\n    reader = SummaryReader(""logs"", types=types)\n    _, data_raw = _get_test_data()\n    data_raw2 = 2 * [\n        d for d in data_raw if d is not None and d[""type""] in types\n    ]\n    items = list(reader)\n\n    assert len(items) == len(data_raw2)\n\n    for item, event_raw in zip(items, data_raw2):\n        assert item.step == event_raw[""step""]\n        assert item.tag == event_raw[""tag""]\n        assert item.type == ""scalar""\n        assert np.all(item.value == event_raw[""value""])\n\n\ndef test_summary_reader_invalid_type():\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    with pytest.raises(ValueError):\n        SummaryReader(""."", types=[""unknown-type""])\n'"
catalyst/contrib/utils/cv/__init__.py,0,"b'# flake8: noqa\n# isort:skip_file\n\nimport logging\nimport os\n\nlogger = logging.getLogger(__name__)\n\nfrom catalyst.tools import settings\n\ntry:\n    from catalyst.contrib.utils.cv.image import (\n        has_image_extension,\n        imread,\n        imwrite,\n        imsave,\n        mask_to_overlay_image,\n        mimread,\n        mimwrite_with_meta,\n    )\nexcept ImportError as ex:\n    if settings.cv_required:\n        logger.warning(\n            ""some of catalyst-cv dependencies not available,""\n            "" to install dependencies, run `pip install catalyst[cv]`.""\n        )\n        raise ex\n\nfrom catalyst.contrib.utils.cv.tensor import (\n    tensor_from_rgb_image,\n    tensor_to_ndimage,\n)\n'"
catalyst/contrib/utils/cv/image.py,0,"b'from typing import List, Tuple, Union\nimport logging\nimport os\nimport pathlib\nimport tempfile\n\nimport imageio\nimport numpy as np\nfrom skimage.color import label2rgb, rgb2gray\n\nfrom catalyst.tools import settings\n\nlogger = logging.getLogger(__name__)\n\nif settings.use_libjpeg_turbo:\n    try:\n        import jpeg4py as jpeg\n\n        # check libjpeg-turbo availability through image reading\n        img = np.zeros((1, 1, 3), dtype=np.uint8)\n        with tempfile.NamedTemporaryFile(suffix="".jpg"") as fp:\n            imageio.imwrite(fp.name, img)\n            img = jpeg.JPEG(fp.name).decode()\n\n    except ImportError as ex:\n        logger.warning(\n            ""jpeg4py not available. ""\n            ""To install jpeg4py, run `pip install jpeg4py`.""\n        )\n        raise ex\n    except OSError as ex:\n        logger.warning(\n            ""libjpeg-turbo not available. ""\n            ""To install libjpeg-turbo, run `apt-get install libturbojpeg`.""\n        )\n        raise ex\n\n\ndef imread(\n    uri,\n    grayscale: bool = False,\n    expand_dims: bool = True,\n    rootpath: Union[str, pathlib.Path] = None,\n    **kwargs,\n) -> np.ndarray:\n    """"""\n    Reads an image from the specified file.\n\n    Args:\n        uri (str, pathlib.Path, bytes, file): the resource to load the image\n          from, e.g. a filename, ``pathlib.Path``, http address or file object,\n          see ``imageio.imread`` docs for more info\n        grayscale (bool):\n        expand_dims (bool):\n        rootpath (Union[str, pathlib.Path]): path to the resource with image\n            (allows to use relative path)\n\n    Returns:\n        np.ndarray: image\n    """"""\n    uri = str(uri)\n\n    if rootpath is not None:\n        rootpath = str(rootpath)\n        uri = uri if uri.startswith(rootpath) else os.path.join(rootpath, uri)\n\n    if settings.use_libjpeg_turbo and uri.endswith(\n        (""jpg"", ""JPG"", ""jpeg"", ""JPEG"")\n    ):\n        img = jpeg.JPEG(uri).decode()\n    else:\n        # @TODO: add tiff support, currently \xe2\x80\x93 jpg and png\n        img = imageio.imread(uri, as_gray=grayscale, pilmode=""RGB"", **kwargs)\n    if grayscale:\n        img = rgb2gray(img)\n\n    if expand_dims and len(img.shape) < 3:  # grayscale\n        img = np.expand_dims(img, -1)\n\n    return img\n\n\ndef imwrite(**kwargs):\n    """"""\n    ``imwrite(uri, im, format=None, **kwargs)``\n\n    Write an image to the specified file.\n    Alias for ``imageio.imwrite``.\n\n    Args:\n        **kwargs: parameters for ``imageio.imwrite``\n    """"""\n    return imageio.imwrite(**kwargs)\n\n\ndef imsave(**kwargs):\n    """"""\n    ``imwrite(uri, im, format=None, **kwargs)``\n\n    Write an image to the specified file.\n    Alias for ``imageio.imsave``.\n\n    Args:\n        **kwargs: parameters for ``imageio.imsave``\n    """"""\n    return imageio.imsave(**kwargs)\n\n\ndef mimread(\n    uri,\n    clip_range: Tuple[int, int] = None,\n    expand_dims: bool = True,\n    rootpath: Union[str, pathlib.Path] = None,\n    **kwargs,\n) -> np.ndarray:\n    """"""\n    Reads multiple images from the specified file.\n\n    Args:\n        uri (str, pathlib.Path, bytes, file): the resource to load the image\n          from, e.g. a filename, ``pathlib.Path``, http address or file object,\n          see ``imageio.mimread`` docs for more info\n        clip_range (Tuple[int, int]): lower and upper interval edges,\n          image values outside the interval are clipped to the interval edges\n        expand_dims (bool): if True, append channel axis to grayscale images\n          rootpath (Union[str, pathlib.Path]): path to the resource with image\n          (allows to use relative path),\n\n    Returns:\n        np.ndarray: image\n    """"""\n    if rootpath is not None:\n        uri = uri if uri.startswith(rootpath) else os.path.join(rootpath, uri)\n\n    image = np.dstack(imageio.mimread(uri, **kwargs))\n    if clip_range is not None:\n        image = np.clip(image, *clip_range)\n\n    if expand_dims and len(image.shape) < 3:  # grayscale\n        image = np.expand_dims(image, -1)\n\n    return image\n\n\ndef mimwrite_with_meta(uri, ims, meta, **kwargs):\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    writer = imageio.get_writer(uri, mode=""I"", **kwargs)\n    writer.set_meta_data(meta)\n    with writer:\n        for i in ims:\n            writer.append_data(i)\n\n\ndef mask_to_overlay_image(\n    image: np.ndarray,\n    masks: List[np.ndarray],\n    threshold: float = 0,\n    mask_strength: float = 0.5,\n) -> np.ndarray:\n    """"""Draws every mask for with some color over image.\n\n    Args:\n        image (np.ndarray): RGB image used as underlay for masks\n        masks (List[np.ndarray]): list of masks\n        threshold (float): threshold for masks binarization\n        mask_strength (float): opacity of colorized masks\n\n    Returns:\n        np.ndarray: HxWx3 image with overlay\n    """"""\n    h, w = image.shape[:2]\n    labels = np.zeros((h, w), np.uint8)\n\n    for idx, mask in enumerate(masks, start=1):\n        labels[mask > threshold] = idx\n\n    mask = label2rgb(labels, bg_label=0)\n\n    image = np.array(image) / 255.0\n    image_with_overlay = image * (1 - mask_strength) + mask * mask_strength\n    image_with_overlay = (\n        (image_with_overlay * 255).clip(0, 255).round().astype(np.uint8)\n    )\n\n    return image_with_overlay\n\n\ndef has_image_extension(uri) -> bool:\n    """"""Check that file has image extension.\n\n    Args:\n        uri (Union[str, pathlib.Path]): the resource to load the file from\n\n    Returns:\n        bool: True if file has image extension, False otherwise\n    """"""\n    _, ext = os.path.splitext(uri)\n    return ext.lower() in {"".bmp"", "".png"", "".jpeg"", "".jpg"", "".tif"", "".tiff""}\n\n\n__all__ = [\n    ""has_image_extension"",\n    ""imread"",\n    ""imwrite"",\n    ""imsave"",\n    ""mask_to_overlay_image"",\n    ""mimread"",\n    ""mimwrite_with_meta"",\n]\n'"
catalyst/contrib/utils/cv/tensor.py,4,"b'from typing import Tuple\n\nimport numpy as np\n\nimport torch\n\n_IMAGENET_STD = (0.229, 0.224, 0.225)\n_IMAGENET_MEAN = (0.485, 0.456, 0.406)\n\n\ndef tensor_from_rgb_image(image: np.ndarray) -> torch.Tensor:\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    image = np.moveaxis(image, -1, 0)\n    image = np.ascontiguousarray(image)\n    image = torch.from_numpy(image)\n    return image\n\n\ndef tensor_to_ndimage(\n    images: torch.Tensor,\n    denormalize: bool = True,\n    mean: Tuple[float, float, float] = _IMAGENET_MEAN,\n    std: Tuple[float, float, float] = _IMAGENET_STD,\n    move_channels_dim: bool = True,\n    dtype=np.float32,\n) -> np.ndarray:\n    """"""\n    Convert float image(s) with standard normalization to\n    np.ndarray with [0..1] when dtype is np.float32 and [0..255]\n    when dtype is `np.uint8`.\n\n    Args:\n        images (torch.Tensor): [B]xCxHxW float tensor\n        denormalize (bool): if True, multiply image(s) by std and add mean\n        mean (Tuple[float, float, float]): per channel mean to add\n        std (Tuple[float, float, float]): per channel std to multiply\n        move_channels_dim (bool): if True, convert tensor to [B]xHxWxC format\n        dtype: result ndarray dtype. Only float32 and uint8 are supported\n\n    Returns:\n        [B]xHxWxC np.ndarray of dtype\n    """"""\n    if denormalize:\n        has_batch_dim = len(images.shape) == 4\n\n        mean = images.new_tensor(mean).view(\n            *((1,) if has_batch_dim else ()), len(mean), 1, 1\n        )\n        std = images.new_tensor(std).view(\n            *((1,) if has_batch_dim else ()), len(std), 1, 1\n        )\n\n        images = images * std + mean\n\n    images = images.clamp(0, 1).numpy()\n\n    if move_channels_dim:\n        images = np.moveaxis(images, -3, -1)\n\n    if dtype == np.uint8:\n        images = (images * 255).round().astype(dtype)\n    else:\n        assert dtype == np.float32, ""Only float32 and uint8 are supported""\n\n    return images\n\n\n__all__ = [""tensor_from_rgb_image"", ""tensor_to_ndimage""]\n'"
catalyst/contrib/utils/nlp/__init__.py,0,"b'# flake8: noqa\n# isort:skip_file\n\nimport logging\nimport os\n\nlogger = logging.getLogger(__name__)\n\nfrom catalyst.tools import settings\n\ntry:\n    import transformers  # noqa: F401\n    from catalyst.contrib.utils.nlp.text import (\n        tokenize_text,\n        process_bert_output,\n    )\nexcept ImportError as ex:\n    if settings.transformers_required:\n        logger.warning(\n            ""transformers not available, to install transformers,""\n            "" run `pip install transformers`.""\n        )\n        raise ex\n'"
catalyst/contrib/utils/nlp/text.py,1,"b'from typing import Dict, List, Union\nimport string\n\nimport numpy as np\n\nimport torch\n\nfrom catalyst.contrib.nn.modules import LamaPooling\n\n\ndef tokenize_text(\n    text: str,\n    tokenizer,  # HuggingFace tokenizer, ex: BertTokenizer\n    max_length: int,\n    strip: bool = True,\n    lowercase: bool = True,\n    remove_punctuation: bool = True,\n) -> Dict[str, np.array]:\n    """"""Tokenizes givin text.\n\n    Args:\n        text (str): text to tokenize\n        tokenizer: Tokenizer instance from HuggingFace\n        max_length (int): maximum length of tokens\n        strip (bool): if true strips text before tokenizing\n        lowercase (bool): if true makes text lowercase before tokenizing\n        remove_punctuation (bool): if true\n            removes ``string.punctuation`` from text before tokenizing\n    """"""\n    if strip:\n        text = text.strip()\n    if lowercase:\n        text = text.lower()\n    if remove_punctuation:\n        text.replace(string.punctuation, """")\n\n    inputs = tokenizer.encode_plus(\n        text, """", add_special_tokens=True, max_length=max_length\n    )\n    input_ids, token_type_ids = inputs[""input_ids""], inputs[""token_type_ids""]\n    attention_mask = [1] * len(input_ids)\n\n    padding_length = max_length - len(input_ids)\n    input_ids = input_ids + ([0] * padding_length)\n    attention_mask = attention_mask + ([0] * padding_length)\n    token_type_ids = token_type_ids + ([0] * padding_length)\n\n    return {\n        ""input_ids"": np.array(input_ids, dtype=np.int64),\n        ""token_type_ids"": np.array(token_type_ids, dtype=np.int64),\n        ""attention_mask"": np.array(attention_mask, dtype=np.int64),\n    }\n\n\ndef process_bert_output(\n    bert_output,\n    hidden_size: int,\n    output_hidden_states: bool = False,\n    pooling_groups: List[str] = None,\n    mask: torch.Tensor = None,\n    level: Union[int, str] = None,\n):\n    """"""Processed the output.""""""\n    # @TODO: make this functional\n    pooling = (\n        LamaPooling(groups=pooling_groups, in_features=hidden_size)\n        if pooling_groups is not None\n        else None\n    )\n\n    def _process_features(features):\n        if pooling is not None:\n            features = pooling(features, mask=mask)\n        return features\n\n    if isinstance(level, str):\n        assert level in (""pooling"", ""class"")\n        if level == ""pooling"":\n            return _process_features(bert_output[0])\n        else:\n            return bert_output[1]\n    elif isinstance(level, int):\n        return _process_features(bert_output[2][level])\n\n    output = {\n        ""pooling"": _process_features(bert_output[0]),\n        ""class"": bert_output[1],\n    }\n\n    if output_hidden_states:\n        for i, feature_ in enumerate(bert_output[2]):\n            output[i] = _process_features(feature_)\n\n    return output\n\n\n__all__ = [""tokenize_text"", ""process_bert_output""]\n'"
catalyst/contrib/utils/tests/__init__.py,0,b''
catalyst/contrib/utils/tests/test_argparse.py,0,"b'# flake8: noqa\nfrom catalyst import utils\n\n\ndef test_args_are_not_none():\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    assert utils.args_are_not_none(1, 2, 3, """")\n    assert not utils.args_are_not_none(-8, """", None, True)\n    assert not utils.args_are_not_none(None)\n'"
catalyst/contrib/utils/tests/test_dataset.py,0,"b'import pandas as pd\n\nfrom catalyst.utils import split_dataframe_on_stratified_folds\n\n\ndef _setup_data(num_rows=10):\n    df_data = []\n    for i in range(num_rows):\n        if i < (num_rows / 2):\n            df_data.append([""ants"", ""%s.jpg"" % i, 0])\n        else:\n            df_data.append([""bees"", ""%s.jpg"" % i, 1])\n    return pd.DataFrame(df_data, columns=[""tag"", ""filepath"", ""class""])\n\n\ndef test_stratified_fold_split():\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    df = _setup_data()\n\n    splitted = split_dataframe_on_stratified_folds(\n        dataframe=df, class_column=""class""\n    )\n\n    assert int == splitted[""fold""].dtype\n    assert set(range(5)) == set(splitted[""fold""].unique())\n    ants_folds = set(splitted[splitted[""tag""] == ""ants""][""fold""])\n    bees_folds = set(splitted[splitted[""tag""] == ""bees""][""fold""])\n    assert ants_folds == bees_folds\n\n\ndef test_stratified_fold_split_num_folds():\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    df = _setup_data()\n\n    splitted = split_dataframe_on_stratified_folds(df, ""class"", n_folds=2)\n\n    assert set(range(2)) == set(splitted[""fold""].unique())\n'"
catalyst/contrib/utils/tests/test_misc.py,0,"b'# flake8: noqa\nfrom torch import nn\n\nfrom catalyst import utils\n\n\ndef test_get_fn_argsnames():\n    class Net1(nn.Module):\n        def forward(self, x):\n            return x\n\n    class Net2(nn.Module):\n        def forward(self, x, y):\n            return x\n\n    class Net3(nn.Module):\n        def forward(self, x, y=None):\n            return x\n\n    class Net4(nn.Module):\n        def forward(self, x, *, y=None):\n            return x\n\n    class Net5(nn.Module):\n        def forward(self, *, x):\n            return x\n\n    class Net6(nn.Module):\n        def forward(self, *, x, y):\n            return x\n\n    class Net7(nn.Module):\n        def forward(self, *, x, y=None):\n            return x\n\n    nets = [Net1, Net2, Net3, Net4, Net5, Net6, Net7]\n    params_true = [\n        [""x""],\n        [""x"", ""y""],\n        [""x"", ""y""],\n        [""x"", ""y""],\n        [""x""],\n        [""x"", ""y""],\n        [""x"", ""y""],\n    ]\n\n    params_predicted = list(\n        map(\n            lambda x: utils.get_fn_argsnames(x.forward, exclude=[""self""]), nets\n        )\n    )\n    assert params_predicted == params_true\n\n\ndef test_fn_ends_with_pass():\n    def useless_fn():\n        pass\n\n    def usefull_fn():\n        print(""I am useful!"")\n\n    assert utils.fn_ends_with_pass(useless_fn) is True\n    assert utils.fn_ends_with_pass(usefull_fn) is False\n\n\ndef test_fn_ends_with_pass_on_callbacks():\n    def test_fn_ends_with_pass_on_callback(\n        callback, events,\n    ):\n        for event in events[""covered""]:\n            fn_name = f""on_{event}""\n            assert (\n                utils.fn_ends_with_pass(getattr(callback.__class__, fn_name))\n                is False\n            )\n        for event in events[""non-covered""]:\n            fn_name = f""on_{event}""\n            assert (\n                utils.fn_ends_with_pass(getattr(callback.__class__, fn_name))\n                is True\n            )\n\n    # Callback test\n    from catalyst.dl import Callback\n\n    callback = Callback(order=1)\n    start_events = [\n        ""stage_start"",\n        ""epoch_start"",\n        ""batch_start"",\n        ""loader_start"",\n    ]\n    end_events = [\n        ""stage_end"",\n        ""epoch_end"",\n        ""batch_end"",\n        ""loader_end"",\n        ""exception"",\n    ]\n    events = {""covered"": [], ""non-covered"": [*start_events, *end_events]}\n    test_fn_ends_with_pass_on_callback(callback=callback, events=events)\n\n    # CriterionCallback test\n    from catalyst.dl import CriterionCallback\n\n    callback = CriterionCallback()\n    covered_events = [""stage_start"", ""batch_end""]\n    non_covered_start_events = [""epoch_start"", ""batch_start"", ""loader_start""]\n    non_covered_end_events = [\n        ""stage_end"",\n        ""epoch_end"",\n        ""loader_end"",\n        ""exception"",\n    ]\n    events = {\n        ""covered"": [*covered_events],\n        ""non-covered"": [*non_covered_start_events, *non_covered_end_events],\n    }\n    test_fn_ends_with_pass_on_callback(callback=callback, events=events)\n'"
catalyst/contrib/utils/tests/test_pandas.py,0,"b'import pytest\n\nfrom catalyst.contrib.utils import pandas\n\n\ndef test_folds_to_list():\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    assert pandas.folds_to_list(""1,2,1,3,4,2,4,6"") == [1, 2, 3, 4, 6]\n    assert pandas.folds_to_list([1, 2, 3.0, 5, 2, 1]) == [1, 2, 3, 5]\n    assert pandas.folds_to_list([]) == []\n\n    with pytest.raises(ValueError):\n        pandas.folds_to_list([1, ""True"", 3.0, None, 2, 1])\n'"
catalyst/data/scripts/tests/__init__.py,0,b''
catalyst/data/scripts/tests/test_tag2label.py,0,"b'# flake8: noqa\nfrom catalyst.data.scripts.tag2label import _prepare_df_from_dirs\n\n\ndef _setup_dataset_fs(tmp_path):\n    def create_children(parent, children):\n        for child, sub_children in children.items():\n            sub_parent = parent / child\n            sub_parent.mkdir()\n            if type(sub_children) == list:\n                [\n                    (sub_parent / sub_child).touch()\n                    for sub_child in sub_children\n                ]\n            else:\n                create_children(sub_parent, sub_children)\n\n    fs_structure = {\n        ""datasets"": {\n            ""root1"": {""act1"": [""a.txt"", ""b.txt""], ""act2"": [""c.txt""]},\n            ""root2"": {""act1"": [""d.txt""], ""act2"": [""e.txt"", ""f.txt""]},\n        }\n    }\n\n    create_children(tmp_path, fs_structure)\n\n\ndef test_prepare_df_from_dirs_one(tmp_path):\n    def check_filepath(filepath):\n        return filepath.startswith(""act1"") or filepath.startswith(""act2"")\n\n    _setup_dataset_fs(tmp_path)\n    root_path = tmp_path / ""datasets/root1""\n    df = _prepare_df_from_dirs(str(root_path), ""label"")\n\n    assert df.shape[0] == 3\n    assert df.filepath.apply(check_filepath).sum().all()\n    assert df.label.isin([""act1"", ""act2""]).all()\n\n\ndef test_prepare_df_from_dirs_multi(tmp_path):\n    def check_filepath(filepath):\n        return (\n            filepath.startswith(""root1/act1"")\n            or filepath.startswith(""root1/act2"")\n            or filepath.startswith(""root2/act1"")\n            or filepath.startswith(""root2/act2"")\n        )\n\n    _setup_dataset_fs(tmp_path)\n    ds_path = tmp_path / ""datasets""\n    root_paths = "","".join([str(ds_path / ""root1""), str(ds_path / ""root2"")])\n    df = _prepare_df_from_dirs(root_paths, ""label"")\n\n    assert df.shape[0] == 6\n    assert df.filepath.apply(check_filepath).sum().all()\n    assert df.label.isin([""act1"", ""act2""]).all()\n'"
catalyst/dl/callbacks/metrics/__init__.py,0,"b'# flake8: noqa\n\nfrom .accuracy import AccuracyCallback, MapKCallback\nfrom .auc import AUCCallback\nfrom .dice import DiceCallback, MulticlassDiceMetricCallback\nfrom .f1_score import F1ScoreCallback\nfrom .iou import (\n    ClasswiseIouCallback,\n    ClasswiseJaccardCallback,\n    IouCallback,\n    JaccardCallback,\n)\nfrom .ppv_tpr_f1 import PrecisionRecallF1ScoreCallback\n'"
catalyst/dl/callbacks/metrics/accuracy.py,1,"b'from typing import List\n\nfrom catalyst.core import MultiMetricCallback\nfrom catalyst.utils import metrics\n\n\ndef _get_default_accuracy_args(num_classes: int) -> List[int]:\n    """"""Calculate list params for ``Accuracy@k`` and ``mAP@k``.\n\n    Examples:\n        >>> _get_default_accuracy_args(num_classes=4)\n        >>> [1, 3]\n        >>> _get_default_accuracy_args(num_classes=8)\n        >>> [1, 3, 5]\n\n    Args:\n        num_classes (int): number of classes\n\n    Returns:\n        iterable: array of accuracy arguments\n    """"""\n    result = [1]\n\n    if num_classes is None:\n        return result\n\n    if num_classes > 3:\n        result.append(3)\n    if num_classes > 5:\n        result.append(5)\n\n    return result\n\n\nclass AccuracyCallback(MultiMetricCallback):\n    """"""Accuracy metric callback.\n\n    It can be used either for\n        - multi-class task:\n            -you can use accuracy_args.\n            -threshold and activation are not required.\n            -input_key point on tensor: batch_size.\n            -output_key point on tensor: batch_size x num_classes.\n        - OR multi-label task, in this case:\n            -you must specify threshold and activation.\n            -accuracy_args and num_classes will not be used\n            (because of there is no method to apply top-k in\n            multi-label classification).\n            -input_key, output_key point on tensor: batch_size x num_classes.\n            -output_key point on a tensor with binary vectors.\n\n    .. note::\n        There is no need to choose a type (multi-class/multi label).\n        An appropriate type will be chosen automatically via shape of tensors.\n    """"""\n\n    def __init__(\n        self,\n        input_key: str = ""targets"",\n        output_key: str = ""logits"",\n        prefix: str = ""accuracy"",\n        accuracy_args: List[int] = None,\n        num_classes: int = None,\n        threshold: float = None,\n        activation: str = None,\n    ):\n        """"""\n        Args:\n            input_key (str): input key to use for accuracy calculation;\n                specifies our `y_true`\n            output_key (str): output key to use for accuracy calculation;\n                specifies our `y_pred`\n            prefix (str): key for the metric\'s name\n            accuracy_args (List[int]): specifies which accuracy@K to log:\n                [1] - accuracy\n                [1, 3] - accuracy at 1 and 3\n                [1, 3, 5] - accuracy at 1, 3 and 5\n            num_classes (int): number of classes to calculate ``accuracy_args``\n                if ``accuracy_args`` is None\n            threshold (float): threshold for outputs binarization.\n            activation (str): An torch.nn activation applied to the outputs.\n                Must be one of ``""none""``, ``""Sigmoid""``, or ``""Softmax""``\n        """"""\n        list_args = accuracy_args or _get_default_accuracy_args(num_classes)\n\n        super().__init__(\n            prefix=prefix,\n            metric_fn=metrics.accuracy,\n            list_args=list_args,\n            input_key=input_key,\n            output_key=output_key,\n            topk=list_args,\n            threshold=threshold,\n            activation=activation,\n        )\n\n\nclass MapKCallback(MultiMetricCallback):\n    """"""mAP@k metric callback.""""""\n\n    def __init__(\n        self,\n        input_key: str = ""targets"",\n        output_key: str = ""logits"",\n        prefix: str = ""map"",\n        map_args: List[int] = None,\n        num_classes: int = None,\n    ):\n        """"""\n        Args:\n            input_key (str): input key to use for\n                calculation mean average accuracy at k;\n                specifies our `y_true`\n            output_key (str): output key to use for\n                calculation mean average accuracy at k;\n                specifies our `y_pred`\n            prefix (str): key for the metric\'s name\n            map_args (List[int]): specifies which map@K to log.\n                [1] - map@1\n                [1, 3] - map@1 and map@3\n                [1, 3, 5] - map@1, map@3 and map@5\n            num_classes (int): number of classes to calculate ``map_args``\n                if ``map_args`` is None\n        """"""\n        list_args = map_args or _get_default_accuracy_args(num_classes)\n\n        super().__init__(\n            prefix=prefix,\n            metric_fn=metrics.mean_average_accuracy,\n            list_args=list_args,\n            input_key=input_key,\n            output_key=output_key,\n            topk=list_args,\n        )\n\n\n__all__ = [""AccuracyCallback"", ""MapKCallback""]\n'"
catalyst/dl/callbacks/metrics/auc.py,1,"b'from typing import List\n\nfrom catalyst.dl.callbacks import MeterMetricsCallback\nfrom catalyst.tools import meters\n\n\nclass AUCCallback(MeterMetricsCallback):\n    """"""Calculates the AUC  per class for each loader.\n\n    .. note::\n        Currently, supports binary and multi-label cases.\n    """"""\n\n    def __init__(\n        self,\n        input_key: str = ""targets"",\n        output_key: str = ""logits"",\n        prefix: str = ""auc"",\n        class_names: List[str] = None,\n        num_classes: int = 2,\n        activation: str = ""Sigmoid"",\n    ):\n        """"""\n        Args:\n            input_key (str): input key to use for auc calculation\n                specifies our ``y_true``\n            output_key (str): output key to use for auc calculation;\n                specifies our ``y_pred``\n            prefix (str): name to display for auc when printing\n            class_names (List[str]): class names to display in the logs.\n                If None, defaults to indices for each class, starting from 0\n            num_classes (int): Number of classes; must be > 1\n            activation (str): An torch.nn activation applied to the outputs.\n                Must be one of ``\'none\'``, ``\'Sigmoid\'``, or ``\'Softmax2d\'``\n        """"""\n        num_classes = num_classes if class_names is None else len(class_names)\n\n        meter_list = [meters.AUCMeter() for _ in range(num_classes)]\n\n        super().__init__(\n            metric_names=[prefix],\n            meter_list=meter_list,\n            input_key=input_key,\n            output_key=output_key,\n            class_names=class_names,\n            num_classes=num_classes,\n            activation=activation,\n        )\n\n\n__all__ = [""AUCCallback""]\n'"
catalyst/dl/callbacks/metrics/dice.py,0,"b'import numpy as np\n\nfrom catalyst.core import Callback, CallbackOrder, IRunner, MetricCallback\nfrom catalyst.dl import utils\nfrom catalyst.utils import metrics\n\nfrom .functional import calculate_dice\n\n\nclass DiceCallback(MetricCallback):\n    """"""Dice metric callback.""""""\n\n    def __init__(\n        self,\n        input_key: str = ""targets"",\n        output_key: str = ""logits"",\n        prefix: str = ""dice"",\n        eps: float = 1e-7,\n        threshold: float = None,\n        activation: str = ""Sigmoid"",\n    ):\n        """"""\n        Args:\n            input_key (str): input key to use for dice calculation;\n                specifies our `y_true`\n            output_key (str): output key to use for dice calculation;\n                specifies our `y_pred`\n        """"""\n        super().__init__(\n            prefix=prefix,\n            metric_fn=metrics.dice,\n            input_key=input_key,\n            output_key=output_key,\n            eps=eps,\n            threshold=threshold,\n            activation=activation,\n        )\n\n\nclass MulticlassDiceMetricCallback(Callback):\n    """"""\n    Global Multi-Class Dice Metric Callback: calculates the exact\n    dice score across multiple batches. This callback is good for getting\n    the dice score with small batch sizes where the batchwise dice is noisier.\n    """"""\n\n    def __init__(\n        self,\n        input_key: str = ""targets"",\n        output_key: str = ""logits"",\n        prefix: str = ""dice"",\n        class_names=None,\n    ):\n        """"""\n        Args:\n            input_key (str): input key to use for dice calculation;\n                specifies our `y_true`\n            output_key (str): output key to use for dice calculation;\n                specifies our `y_pred`\n            prefix (str): prefix for printing the metric\n            class_names (dict/List): if dictionary, should be:\n                {class_id: class_name, ...} where class_id is an integer\n                This allows you to ignore class indices.\n                if list, make sure it corresponds to the number of classes\n        """"""\n        super().__init__(CallbackOrder.Metric)\n        self.input_key = input_key\n        self.output_key = output_key\n        self.prefix = prefix\n        self.confusion_matrix = None\n        self.class_names = class_names\n\n    def _reset_stats(self):\n        """"""Resets the confusion matrix holding the epoch-wise stats.""""""\n        self.confusion_matrix = None\n\n    def on_batch_end(self, runner: IRunner):\n        """"""Records the confusion matrix at the end of each batch.\n\n        Args:\n            runner (IRunner): current runner\n        """"""\n        outputs = runner.output[self.output_key]\n        targets = runner.input[self.input_key]\n\n        confusion_matrix = utils.calculate_confusion_matrix_from_tensors(\n            outputs, targets\n        )\n\n        if self.confusion_matrix is None:\n            self.confusion_matrix = confusion_matrix\n        else:\n            self.confusion_matrix += confusion_matrix\n\n    def on_loader_end(self, runner: IRunner):\n        """"""@TODO: Docs. Contribution is welcome.\n\n        Args:\n            runner (IRunner): current runner\n        """"""\n        tp_fp_fn_dict = utils.calculate_tp_fp_fn(self.confusion_matrix)\n\n        dice_scores: np.ndarray = calculate_dice(**tp_fp_fn_dict)\n\n        # logging the dice scores in the state\n        for i, dice in enumerate(dice_scores):\n            if isinstance(self.class_names, dict) and i not in list(\n                self.class_names.keys()\n            ):\n                continue\n            postfix = (\n                self.class_names[i] if self.class_names is not None else str(i)\n            )\n\n            runner.loader_metrics[f""{self.prefix}_{postfix}""] = dice\n\n        # For supporting averaging of only classes specified in `class_names`\n        values_to_avg = [\n            value\n            for key, value in runner.loader_metrics.items()\n            if key.startswith(f""{self.prefix}_"")\n        ]\n        runner.loader_metrics[f""{self.prefix}_mean""] = np.mean(values_to_avg)\n\n        self._reset_stats()\n\n\n__all__ = [""DiceCallback"", ""MulticlassDiceMetricCallback""]\n'"
catalyst/dl/callbacks/metrics/f1_score.py,1,"b'from catalyst.core import MetricCallback\nfrom catalyst.utils import metrics\n\n\nclass F1ScoreCallback(MetricCallback):\n    """"""F1 score metric callback.""""""\n\n    def __init__(\n        self,\n        input_key: str = ""targets"",\n        output_key: str = ""logits"",\n        prefix: str = ""f1_score"",\n        beta: float = 1.0,\n        eps: float = 1e-7,\n        threshold: float = None,\n        activation: str = ""Sigmoid"",\n    ):\n        """"""\n        Args:\n            input_key (str): input key to use for iou calculation\n                specifies our ``y_true``\n            output_key (str): output key to use for iou calculation;\n                specifies our ``y_pred``\n            prefix (str): key to store in logs\n            beta (float): beta param for f_score\n            eps (float): epsilon to avoid zero division\n            threshold (float): threshold for outputs binarization\n            activation (str): An torch.nn activation applied to the outputs.\n                Must be one of ``\'none\'``, ``\'Sigmoid\'``, or ``\'Softmax2d\'``\n        """"""\n        super().__init__(\n            prefix=prefix,\n            metric_fn=metrics.f1_score,\n            input_key=input_key,\n            output_key=output_key,\n            beta=beta,\n            eps=eps,\n            threshold=threshold,\n            activation=activation,\n        )\n\n\n__all__ = [""F1ScoreCallback""]\n'"
catalyst/dl/callbacks/metrics/functional.py,0,"b'import numpy as np\n\n\ndef calculate_dice(\n    true_positives: np.array,\n    false_positives: np.array,\n    false_negatives: np.array,\n) -> np.array:\n    """"""Calculate list of Dice coefficients.\n\n    Args:\n        true_positives:\n        false_positives:\n        false_negatives:\n\n    Returns:\n        np.array: dice score\n    """"""\n    epsilon = 1e-7\n\n    dice = (2 * true_positives + epsilon) / (\n        2 * true_positives + false_positives + false_negatives + epsilon\n    )\n\n    if not np.all(dice <= 1):\n        raise ValueError(""Dice index should be less or equal to 1"")\n\n    if not np.all(dice > 0):\n        raise ValueError(""Dice index should be more than 1"")\n\n    return dice\n'"
catalyst/dl/callbacks/metrics/iou.py,2,"b'from typing import List\n\nfrom catalyst.core import MetricCallback, MultiMetricCallback\nfrom catalyst.utils import metrics\n\n\ndef _get_default_classwise_iou_args(num_classes: int) -> List[int]:\n    assert num_classes > 0, ""num_classes must be greater than 0""\n    return [str(i) for i in range(num_classes)]\n\n\nclass IouCallback(MetricCallback):\n    """"""IoU (Jaccard) metric callback.""""""\n\n    def __init__(\n        self,\n        input_key: str = ""targets"",\n        output_key: str = ""logits"",\n        prefix: str = ""iou"",\n        eps: float = 1e-7,\n        threshold: float = None,\n        activation: str = ""Sigmoid"",\n    ):\n        """"""\n        Args:\n            input_key (str): input key to use for iou calculation\n                specifies our ``y_true``\n            output_key (str): output key to use for iou calculation;\n                specifies our ``y_pred``\n            prefix (str): key to store in logs\n            eps (float): epsilon to avoid zero division\n            threshold (float): threshold for outputs binarization\n            activation (str): An torch.nn activation applied to the outputs.\n                Must be one of ``\'none\'``, ``\'Sigmoid\'``, ``\'Softmax2d\'``\n        """"""\n        super().__init__(\n            prefix=prefix,\n            metric_fn=metrics.iou,\n            input_key=input_key,\n            output_key=output_key,\n            eps=eps,\n            threshold=threshold,\n            activation=activation,\n        )\n\n\nJaccardCallback = IouCallback\n\n\nclass ClasswiseIouCallback(MultiMetricCallback):\n    """"""Classwise IoU (Jaccard) metric callback.""""""\n\n    def __init__(\n        self,\n        input_key: str = ""targets"",\n        output_key: str = ""logits"",\n        prefix: str = ""iou"",\n        classes: List[str] = None,\n        num_classes: int = None,\n        eps: float = 1e-7,\n        threshold: float = None,\n        activation: str = ""Sigmoid"",\n    ):\n        """"""\n        Args:\n            input_key (str): input key to use for iou calculation\n                specifies our ``y_true``\n            output_key (str): output key to use for iou calculation;\n                specifies our ``y_pred``\n            prefix (str): key to store in logs (will be prefix_class_name)\n            classes (List[str]): list of class names\n                You should specify either \'classes\' or \'num_classes\'\n            num_classes (int): number of classes\n                You should specify either \'classes\' or \'num_classes\'\n            eps (float): epsilon to avoid zero division\n            threshold (float): threshold for outputs binarization\n            activation (str): An torch.nn activation applied to the outputs.\n                Must be one of ``\'none\'``, ``\'Sigmoid\'``, ``\'Softmax2d\'``\n        """"""\n        assert (\n            classes is not None or num_classes is not None\n        ), ""You should specify either \'classes\' or \'num_classes\'""\n        list_args = classes or _get_default_classwise_iou_args(num_classes)\n\n        super().__init__(\n            prefix=prefix,\n            metric_fn=metrics.iou,\n            list_args=list_args,\n            input_key=input_key,\n            output_key=output_key,\n            classes=list_args,\n            eps=eps,\n            threshold=threshold,\n            activation=activation,\n        )\n\n\nClasswiseJaccardCallback = ClasswiseIouCallback\n\n__all__ = [\n    ""IouCallback"",\n    ""JaccardCallback"",\n    ""ClasswiseIouCallback"",\n    ""ClasswiseJaccardCallback"",\n]\n'"
catalyst/dl/callbacks/metrics/ppv_tpr_f1.py,1,"b'from typing import List\n\nfrom catalyst.dl.callbacks import MeterMetricsCallback\nfrom catalyst.tools import meters\n\n\nclass PrecisionRecallF1ScoreCallback(MeterMetricsCallback):\n    """"""\n    Calculates the global precision (positive predictive value or ppv),\n    recall (true positive rate or tpr), and F1-score per class for each loader.\n\n    .. note::\n        Currently, supports binary and multi-label cases.\n    """"""\n\n    def __init__(\n        self,\n        input_key: str = ""targets"",\n        output_key: str = ""logits"",\n        class_names: List[str] = None,\n        num_classes: int = 2,\n        threshold: float = 0.5,\n        activation: str = ""Sigmoid"",\n    ):\n        """"""\n        Args:\n            input_key (str): input key to use for metric calculation\n                specifies our ``y_true``\n            output_key (str): output key to use for metric calculation;\n                specifies our ``y_pred``\n            class_names (List[str]): class names to display in the logs.\n                If None, defaults to indices for each class, starting from 0.\n            num_classes (int): Number of classes; must be > 1\n            threshold (float): threshold for outputs binarization\n            activation (str): An torch.nn activation applied to the outputs.\n                Must be one of ``\'none\'``, ``\'Sigmoid\'``, ``\'Softmax2d\'``\n        """"""\n        # adjusting num_classes automatically if class_names is not None\n        num_classes = num_classes if class_names is None else len(class_names)\n\n        meter_list = [\n            meters.PrecisionRecallF1ScoreMeter(threshold)\n            for _ in range(num_classes)\n        ]\n\n        super().__init__(\n            metric_names=[""ppv"", ""tpr"", ""f1""],\n            meter_list=meter_list,\n            input_key=input_key,\n            output_key=output_key,\n            class_names=class_names,\n            num_classes=num_classes,\n            activation=activation,\n        )\n\n\n__all__ = [""PrecisionRecallF1ScoreCallback""]\n'"
catalyst/dl/experiment/tests/test_config.py,3,"b'from collections import OrderedDict\n\nimport pytest\n\nimport torch\n\nfrom catalyst.dl import (\n    CheckpointCallback,\n    ConsoleLogger,\n    CriterionCallback,\n    ExceptionCallback,\n    MetricManagerCallback,\n    OptimizerCallback,\n    registry,\n    SchedulerCallback,\n    TensorboardLogger,\n    ValidationManagerCallback,\n)\nfrom catalyst.dl.experiment.config import ConfigExperiment\n\nDEFAULT_MINIMAL_CONFIG = {\n    ""model_params"": {""model"": ""SomeModel""},\n    ""stages"": {""data_params"": {""num_workers"": 0}, ""train"": {}},\n    ""args"": {""logdir"": ""./logdir""},\n}\n\nDEFAULT_CALLBACKS = OrderedDict(\n    [\n        (""_metrics"", MetricManagerCallback),\n        (""_validation"", ValidationManagerCallback),\n        (""_saver"", CheckpointCallback),\n        (""_console"", ConsoleLogger),\n        (""_tensorboard"", TensorboardLogger),\n        (""_exception"", ExceptionCallback),\n    ]\n)\n\n\nclass SomeModel(torch.nn.Module):\n    """"""Dummy test torch model.""""""\n\n    pass\n\n\nclass SomeOptimizer(torch.nn.Module):\n    """"""Dummy test torch optimizer.""""""\n\n    def __init__(self, **kwargs):\n        """"""Dummy optimizer""""""\n        super().__init__()\n\n\nclass SomeScheduler(torch.nn.Module):\n    """"""Dummy test torch scheduler.""""""\n\n    def __init__(self, **kwargs):\n        """"""Dummy scheduler""""""\n        super().__init__()\n\n\nregistry.MODELS.add(SomeModel)\nregistry.OPTIMIZERS.add(SomeOptimizer)\nregistry.SCHEDULERS.add(SomeScheduler)\n\n\ndef _test_callbacks(test_callbacks, exp, stage=""train""):\n    exp_callbacks = exp.get_callbacks(stage)\n    exp_callbacks = OrderedDict(\n        sorted(exp_callbacks.items(), key=lambda t: t[0])\n    )\n    test_callbacks = OrderedDict(\n        sorted(test_callbacks.items(), key=lambda t: t[0])\n    )\n    print(test_callbacks.keys())\n    print(exp_callbacks.keys())\n\n    assert exp_callbacks.keys() == test_callbacks.keys()\n    cbs = zip(exp_callbacks.values(), test_callbacks.values())\n    for callback, klass in cbs:\n        assert isinstance(callback, klass)\n\n\ndef test_defaults():\n    """"""\n    Test on ConfigExperiment defaults.\n    It\'s pretty similar to BaseExperiment\'s test\n    but the thing is that those two are very different classes and\n    inherit from different parent classes.\n    Also very important to check which callbacks are added by default\n    """"""\n    exp = ConfigExperiment(config=DEFAULT_MINIMAL_CONFIG)\n\n    assert exp.initial_seed == 42\n    assert exp.logdir == ""./logdir""\n    assert exp.stages == [""train""]\n    assert exp.distributed_params == {}\n    assert exp.get_stage_params(""train"") == {\n        ""logdir"": ""./logdir"",\n    }\n    assert isinstance(exp.get_model(""train""), SomeModel)\n    assert exp.get_criterion(""train"") is None\n    assert exp.get_optimizer(""train"", SomeModel()) is None\n    assert exp.get_scheduler(""train"", None) is None\n\n    _test_callbacks(DEFAULT_CALLBACKS, exp)\n\n\ndef test_defaults_criterion_optimizer_scheduler():\n    """"""\n    Test on ConfigExperiment defaults.\n    when {criterion, optimizer, scheduler}_params are specified\n    the respective callback should be generated automatically\n    """"""\n    callbacks = DEFAULT_CALLBACKS.copy()\n    callbacks[""_criterion""] = CriterionCallback\n    callbacks[""_optimizer""] = OptimizerCallback\n    callbacks[""_scheduler""] = SchedulerCallback\n\n    config = DEFAULT_MINIMAL_CONFIG.copy()\n    config[""stages""][""criterion_params""] = {""criterion"": ""BCEWithLogitsLoss""}\n    config[""stages""][""optimizer_params""] = {""optimizer"": ""SomeOptimizer""}\n    config[""stages""][""scheduler_params""] = {""scheduler"": ""SomeScheduler""}\n    exp = ConfigExperiment(config=config)\n\n    assert exp.initial_seed == 42\n    assert exp.logdir == ""./logdir""\n    assert exp.stages == [""train""]\n    assert exp.distributed_params == {}\n    assert exp.get_stage_params(""train"") == {\n        ""logdir"": ""./logdir"",\n    }\n    assert isinstance(exp.get_model(""train""), SomeModel)\n    assert exp.get_criterion(""train"") is not None\n    assert exp.get_optimizer(""train"", SomeModel()) is not None\n    assert exp.get_scheduler(""train"", None) is not None\n\n    _test_callbacks(callbacks, exp)\n\n\ndef test_when_callback_defined():\n    """"""\n    There should be no default callback of same kind if there is user defined\n    already.\n    """"""\n    callbacks = DEFAULT_CALLBACKS.copy()\n    callbacks[""my_criterion""] = CriterionCallback\n    callbacks[""my_optimizer""] = OptimizerCallback\n    callbacks[""my_scheduler""] = SchedulerCallback\n\n    config = DEFAULT_MINIMAL_CONFIG.copy()\n    config[""stages""][""criterion_params""] = {""criterion"": ""BCEWithLogitsLoss""}\n    config[""stages""][""optimizer_params""] = {""optimizer"": ""SomeOptimizer""}\n    config[""stages""][""scheduler_params""] = {""scheduler"": ""SomeScheduler""}\n    config[""stages""][""callbacks_params""] = {\n        ""my_criterion"": {""callback"": ""CriterionCallback""},\n        ""my_optimizer"": {""callback"": ""OptimizerCallback""},\n        ""my_scheduler"": {""callback"": ""SchedulerCallback""},\n    }\n    exp = ConfigExperiment(config=config)\n    _test_callbacks(callbacks, exp)\n\n\ndef test_not_implemented_datasets():\n    """"""\n    Test on ``get_datasets`` method, which should be implememnted by user.\n    Method ``get_loaders`` will call ``get_dataset``.\n    """"""\n    exp = ConfigExperiment(config=DEFAULT_MINIMAL_CONFIG)\n\n    with pytest.raises(NotImplementedError):\n        exp.get_loaders(""train"")\n    with pytest.raises(NotImplementedError):\n        exp.get_datasets(""train"")\n'"
catalyst/dl/experiment/tests/test_core.py,3,"b'from collections import OrderedDict\n\nimport torch\n\nfrom catalyst.dl import (\n    ConsoleLogger,\n    ExceptionCallback,\n    MetricManagerCallback,\n    ValidationManagerCallback,\n)\nfrom catalyst.dl.experiment.experiment import Experiment\n\n\ndef _test_callbacks(test_callbacks, exp, stage=""train""):\n    exp_callbacks = exp.get_callbacks(stage)\n    exp_callbacks = OrderedDict(\n        sorted(exp_callbacks.items(), key=lambda t: t[0])\n    )\n    test_callbacks = OrderedDict(\n        sorted(test_callbacks.items(), key=lambda t: t[0])\n    )\n\n    assert exp_callbacks.keys() == test_callbacks.keys()\n    cbs = zip(exp_callbacks.values(), test_callbacks.values())\n    for callback, klass in cbs:\n        assert isinstance(callback, klass)\n\n\ndef test_defaults():\n    """"""\n    Test on defaults for BaseExperiment. It will be useful if we decide to\n    change anything in those values as it could make breaking change.\n    """"""\n    model = torch.nn.Module()\n    dataset = torch.utils.data.Dataset()\n    dataloader = torch.utils.data.DataLoader(dataset)\n    loaders = OrderedDict()\n    loaders[""train""] = dataloader\n    test_callbacks = OrderedDict(\n        [\n            (""_metrics"", MetricManagerCallback),\n            (""_validation"", ValidationManagerCallback),\n            (""_console"", ConsoleLogger),\n            (""_exception"", ExceptionCallback),\n        ]\n    )\n\n    exp = Experiment(model=model, loaders=loaders, valid_loader=""train"")\n\n    assert exp.initial_seed == 42\n    assert exp.logdir is None\n    assert exp.stages == [""train""]\n    assert exp.distributed_params == {}\n    assert exp.get_stage_params("""") == {\n        ""logdir"": None,\n        ""num_epochs"": 1,\n        ""valid_loader"": ""train"",\n        ""main_metric"": ""loss"",\n        ""verbose"": False,\n        ""minimize_metric"": True,\n        ""checkpoint_data"": {},\n    }\n    assert exp.get_model("""") == model\n    assert exp.get_criterion("""") is None\n    assert exp.get_optimizer("""", model) is None\n    assert exp.get_scheduler("""") is None\n    _test_callbacks(test_callbacks, exp)\n    assert exp.get_loaders("""") == loaders\n'"
catalyst/dl/experiment/tests/test_supervised.py,40,"b'from collections import OrderedDict\n\nimport torch\n\nfrom catalyst.dl import (\n    CheckpointCallback,\n    CheckRunCallback,\n    ConsoleLogger,\n    CriterionCallback,\n    ExceptionCallback,\n    MetricManagerCallback,\n    OptimizerCallback,\n    SchedulerCallback,\n    TensorboardLogger,\n    TimerCallback,\n    ValidationManagerCallback,\n    VerboseLogger,\n)\nfrom catalyst.dl.experiment.supervised import SupervisedExperiment\n\n\ndef _test_callbacks(test_callbacks, exp, stage=""train""):\n    exp_callbacks = exp.get_callbacks(stage)\n    exp_callbacks = OrderedDict(\n        sorted(exp_callbacks.items(), key=lambda t: t[0])\n    )\n    test_callbacks = OrderedDict(\n        sorted(test_callbacks.items(), key=lambda t: t[0])\n    )\n\n    assert exp_callbacks.keys() == test_callbacks.keys()\n    cbs = zip(exp_callbacks.values(), test_callbacks.values())\n    for callback, klass in cbs:\n        assert isinstance(callback, klass)\n\n\ndef test_defaults():\n    """"""\n    Test on defaults for SupervisedExperiment class, which is child class of\n    BaseExperiment.  That\'s why we check only default callbacks functionality\n    here\n    """"""\n    model = torch.nn.Module()\n    dataset = torch.utils.data.Dataset()\n    dataloader = torch.utils.data.DataLoader(dataset)\n    loaders = OrderedDict()\n    loaders[""train""] = dataloader\n\n    test_callbacks = OrderedDict(\n        [\n            (""_metrics"", MetricManagerCallback),\n            (""_validation"", ValidationManagerCallback),\n            (""_console"", ConsoleLogger),\n            (""_exception"", ExceptionCallback),\n        ]\n    )\n\n    exp = SupervisedExperiment(\n        model=model, loaders=loaders, valid_loader=""train"",\n    )\n    _test_callbacks(test_callbacks, exp)\n\n\ndef test_defaults_verbose():\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    test_callbacks = OrderedDict(\n        [\n            (""_verbose"", VerboseLogger),\n            (""_metrics"", MetricManagerCallback),\n            (""_validation"", ValidationManagerCallback),\n            (""_saver"", CheckpointCallback),\n            (""_console"", ConsoleLogger),\n            (""_tensorboard"", TensorboardLogger),\n            (""_exception"", ExceptionCallback),\n        ]\n    )\n\n    model = torch.nn.Module()\n    dataset = torch.utils.data.Dataset()\n    dataloader = torch.utils.data.DataLoader(dataset)\n    loaders = OrderedDict()\n    loaders[""train""] = dataloader\n\n    exp = SupervisedExperiment(\n        model=model,\n        loaders=loaders,\n        verbose=True,\n        valid_loader=""train"",\n        logdir=""./logs"",\n    )\n    _test_callbacks(test_callbacks, exp)\n\n\ndef test_defaults_check():\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    test_callbacks = OrderedDict(\n        [\n            (""_check"", CheckRunCallback),\n            (""_metrics"", MetricManagerCallback),\n            (""_validation"", ValidationManagerCallback),\n            (""_saver"", CheckpointCallback),\n            (""_console"", ConsoleLogger),\n            (""_tensorboard"", TensorboardLogger),\n            (""_exception"", ExceptionCallback),\n        ]\n    )\n\n    model = torch.nn.Module()\n    dataset = torch.utils.data.Dataset()\n    dataloader = torch.utils.data.DataLoader(dataset)\n    loaders = OrderedDict()\n    loaders[""train""] = dataloader\n\n    exp = SupervisedExperiment(\n        model=model,\n        loaders=loaders,\n        check_run=True,\n        valid_loader=""train"",\n        logdir=""./logs"",\n    )\n    _test_callbacks(test_callbacks, exp)\n\n\ndef test_criterion():\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    test_callbacks = OrderedDict(\n        [\n            (""_metrics"", MetricManagerCallback),\n            (""_validation"", ValidationManagerCallback),\n            (""_saver"", CheckpointCallback),\n            (""_console"", ConsoleLogger),\n            (""_tensorboard"", TensorboardLogger),\n            (""_exception"", ExceptionCallback),\n            (""_criterion"", CriterionCallback),\n        ]\n    )\n\n    model = torch.nn.Linear(10, 10)\n    criterion = torch.nn.CrossEntropyLoss()\n    optimizer = None\n    scheduler = None\n    dataset = torch.utils.data.Dataset()\n    dataloader = torch.utils.data.DataLoader(dataset)\n    loaders = OrderedDict()\n    loaders[""train""] = dataloader\n\n    exp = SupervisedExperiment(\n        model=model,\n        loaders=loaders,\n        criterion=criterion,\n        optimizer=optimizer,\n        scheduler=scheduler,\n        valid_loader=""train"",\n        logdir=""./logs"",\n    )\n    _test_callbacks(test_callbacks, exp)\n\n\ndef test_optimizer():\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    test_callbacks = OrderedDict(\n        [\n            (""_metrics"", MetricManagerCallback),\n            (""_validation"", ValidationManagerCallback),\n            (""_saver"", CheckpointCallback),\n            (""_console"", ConsoleLogger),\n            (""_tensorboard"", TensorboardLogger),\n            (""_exception"", ExceptionCallback),\n            (""_optimizer"", OptimizerCallback),\n        ]\n    )\n\n    model = torch.nn.Linear(10, 10)\n    criterion = None\n    optimizer = torch.optim.Adam(model.parameters())\n    scheduler = None\n    dataset = torch.utils.data.Dataset()\n    dataloader = torch.utils.data.DataLoader(dataset)\n    loaders = OrderedDict()\n    loaders[""train""] = dataloader\n\n    exp = SupervisedExperiment(\n        model=model,\n        loaders=loaders,\n        criterion=criterion,\n        optimizer=optimizer,\n        scheduler=scheduler,\n        valid_loader=""train"",\n        logdir=""./logs"",\n    )\n    _test_callbacks(test_callbacks, exp)\n\n\ndef test_scheduler():\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    test_callbacks = OrderedDict(\n        [\n            (""_metrics"", MetricManagerCallback),\n            (""_validation"", ValidationManagerCallback),\n            (""_saver"", CheckpointCallback),\n            (""_timer"", TimerCallback),\n            (""_console"", ConsoleLogger),\n            (""_tensorboard"", TensorboardLogger),\n            (""_exception"", ExceptionCallback),\n            (""_optimizer"", OptimizerCallback),\n            (""_scheduler"", SchedulerCallback),\n        ]\n    )\n\n    model = torch.nn.Linear(10, 10)\n    optimizer = torch.optim.Adam(model.parameters())\n    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 10)\n    dataset = torch.utils.data.Dataset()\n    dataloader = torch.utils.data.DataLoader(dataset)\n    loaders = OrderedDict()\n    loaders[""train""] = dataloader\n\n    exp = SupervisedExperiment(\n        model=model,\n        loaders=loaders,\n        optimizer=optimizer,\n        scheduler=scheduler,\n        valid_loader=""train"",\n        logdir=""./logs"",\n        check_time=True,\n    )\n    _test_callbacks(test_callbacks, exp)\n\n\ndef test_all():\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    test_callbacks = OrderedDict(\n        [\n            (""_verbose"", VerboseLogger),\n            (""_check"", CheckRunCallback),\n            (""_metrics"", MetricManagerCallback),\n            (""_validation"", ValidationManagerCallback),\n            (""_console"", ConsoleLogger),\n            (""_exception"", ExceptionCallback),\n            (""_criterion"", CriterionCallback),\n            (""_optimizer"", OptimizerCallback),\n            (""_scheduler"", SchedulerCallback),\n        ]\n    )\n\n    model = torch.nn.Linear(10, 10)\n    criterion = torch.nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters())\n    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 10)\n    dataset = torch.utils.data.Dataset()\n    dataloader = torch.utils.data.DataLoader(dataset)\n    loaders = OrderedDict()\n    loaders[""train""] = dataloader\n\n    exp = SupervisedExperiment(\n        model=model,\n        loaders=loaders,\n        criterion=criterion,\n        optimizer=optimizer,\n        scheduler=scheduler,\n        verbose=True,\n        check_run=True,\n        valid_loader=""train"",\n    )\n    _test_callbacks(test_callbacks, exp)\n\n\ndef test_infer_defaults():\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    test_callbacks = OrderedDict([(""_exception"", ExceptionCallback)])\n\n    model = torch.nn.Linear(10, 10)\n    criterion = torch.nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters())\n    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 10)\n    dataset = torch.utils.data.Dataset()\n    dataloader = torch.utils.data.DataLoader(dataset)\n    loaders = OrderedDict()\n    loaders[""train""] = dataloader\n\n    exp = SupervisedExperiment(\n        model=model,\n        loaders=loaders,\n        criterion=criterion,\n        optimizer=optimizer,\n        scheduler=scheduler,\n        stage=""infer"",\n    )\n    _test_callbacks(test_callbacks, exp, ""infer"")\n\n\ndef test_infer_all():\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    test_callbacks = OrderedDict(\n        [\n            (""_verbose"", VerboseLogger),\n            (""_check"", CheckRunCallback),\n            (""_exception"", ExceptionCallback),\n        ]\n    )\n\n    model = torch.nn.Linear(10, 10)\n    criterion = torch.nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters())\n    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 10)\n    dataset = torch.utils.data.Dataset()\n    dataloader = torch.utils.data.DataLoader(dataset)\n    loaders = OrderedDict()\n    loaders[""train""] = dataloader\n\n    exp = SupervisedExperiment(\n        model=model,\n        loaders=loaders,\n        criterion=criterion,\n        optimizer=optimizer,\n        scheduler=scheduler,\n        verbose=True,\n        check_run=True,\n        stage=""infer"",\n    )\n    _test_callbacks(test_callbacks, exp, ""infer"")\n'"
catalyst/tools/meters/tests/__init__.py,0,b''
catalyst/tools/meters/tests/test_averagevaluemeter.py,4,"b'import torch\n\nfrom catalyst.tools import meters\n\n\ndef test_averagevaluemeter():\n    """"""Test for ``catalyst.tools.meters.AverageValueMeter``.""""""\n    meter = meters.AverageValueMeter()\n\n    def batch_generator(length, batch_size=10):\n        data = torch.rand(length)\n        for i in range(length // batch_size):\n            yield data[i * batch_size : (i + 1) * batch_size]\n        if length % batch_size:\n            yield data[-(length % batch_size) :]\n\n    def test(meter, length, batch_size):\n        x2 = torch.zeros(length)\n        i = 0\n        for batch in batch_generator(length, batch_size):\n            bs = batch.shape[0]\n            meter.add(batch.mean(), bs)\n            x2[i : i + bs] = batch.mean()\n            i += bs\n        assert torch.allclose(\n            torch.tensor((x2.mean(), x2.std())), torch.tensor(meter.value())\n        )\n        meter.reset()\n\n    confs = ((100, 1), (100, 10), (100, 16), (1024, 53), (10, 16), (100, 100))\n    for conf in confs:\n        test(meter, *conf)\n'"
catalyst/tools/meters/tests/test_ppv_tpr_f1.py,6,"b'import torch\n\nfrom catalyst.tools import meters\nfrom catalyst.tools.meters.ppv_tpr_f1_meter import f1score, precision, recall\n\n\ndef precision_recall_f1(tp, fp, fn):\n    """"""Calculates precision, recall, and f1 score.\n\n    Args:\n        tp: number of true positives\n        fp: number of false positives\n        fn: number of false negatives\n\n    Returns:\n        precision value (0-1), recall_value (0-1), f1score (0-1)\n    """"""\n    precision_value = round(precision(tp, fp), 3)\n    recall_value = round(recall(tp, fn), 3)\n    f1_value = round(f1score(precision_value, recall_value), 3)\n    return (precision_value, recall_value, f1_value)\n\n\ndef test_precision_recall_f1score():\n    """"""Sanity checks for the `precision`, `recall`, `f1score` functions.""""""\n    # case 1\n    tp, fp, fn = (10, 0, 0)\n    ppv, tpr, f1 = precision_recall_f1(tp, fp, fn)\n    assert ppv == tpr == f1 == 1, ""No fp and fn means everything should be =1""\n\n    # case 2\n    tp, fp, fn = (0, 0, 0)\n    ppv, tpr, f1 = precision_recall_f1(tp, fp, fn)\n    assert (\n        ppv == tpr == f1 == 1\n    ), ""No tp, fp and fn means there weren\'t any objects (everything =1)""\n\n    # case 3\n    tp, fp, fn = (10, 10, 10)\n    ppv, tpr, f1 = precision_recall_f1(tp, fp, fn)\n    assert ppv == tpr == 0.5, ""Example where ppv and tpr should be =0.5.""\n\n    # case 4\n    tp, fp, fn = (0, 1, 1)\n    ppv, tpr, f1 = precision_recall_f1(tp, fp, fn)\n    assert ppv == tpr == f1 == 0, ""No tp means everything should be =0""\n\n\ndef create_dummy_tensors_single():\n    """"""Binary: 1 actual, 1 predicted (tp: 1, fp: 0, fn: 0).""""""\n    label = torch.tensor([1])\n    pred = torch.tensor([1])\n    return (label, pred)\n\n\ndef create_dummy_tensors_batched(batch_size=16):\n    """"""Binary: 1 actual, 1 predicted (tp: 1, fp: 0, fn: 0).""""""\n    label = torch.ones((batch_size, 1))\n    pred = torch.ones((batch_size, 1))\n    return (label, pred)\n\n\ndef create_dummy_tensors_seg(batch_size=16, channels=1):\n    """"""Binary: 1 actual, 1 predicted (tp: 1, fp: 0, fn: 0).""""""\n    base_shape = (channels, 15, 15)\n    label = torch.ones((batch_size,) + base_shape)\n    pred = torch.ones((batch_size,) + base_shape)\n    return (label, pred)\n\n\ndef runs_tests_on_meter_counts_and_value(meter, num_tp_check=16):\n    """"""\n    Tests the meter\'s counts and values (ppv, tpr, f1). Assumes there are no\n    fp and fn (everything is tp).\n    """"""\n    counts_dict = meter.tp_fp_fn_counts\n    assert counts_dict[""tp""] == num_tp_check\n    assert (\n        counts_dict[""fp""] == 0 and counts_dict[""fn""] == 0\n    ), ""There should be no fp and fn for this test case.""\n    ppv, tpr, f1 = meter.value()\n    ppv, tpr, f1 = map(lambda x: round(x, 3), [ppv, tpr, f1])\n    assert (\n        ppv == tpr == f1 == 1\n    ), ""No fp and fn means that all metrics should be =1.""\n\n\ndef test_meter():\n    """"""\n    Tests:\n        * .reset()\n        * .add()\n        * .value()\n    """"""\n    meter = meters.PrecisionRecallF1ScoreMeter()\n    # tests the .reset() method, which happens to be called in initialization\n    for key in [""tp"", ""fp"", ""fn""]:\n        assert (\n            meter.tp_fp_fn_counts[key] == 0\n        ), ""Counts should be initialized to 0.""\n\n    # testing .add() and .value() with tensors w/no batch size dim\n    binary_y, binary_pred = create_dummy_tensors_single()\n    meter.add(binary_pred, binary_y)\n    runs_tests_on_meter_counts_and_value(meter, num_tp_check=1)\n\n    # testing .add() and .value() with tensors w/the batch size dim\n    meter.reset()\n    batch_size = 16\n    binary_y, binary_pred = create_dummy_tensors_batched(batch_size)\n    meter.add(binary_pred, binary_y)\n    runs_tests_on_meter_counts_and_value(meter, num_tp_check=batch_size)\n\n    # testing with seg; shape (batch_size, n_channels, h, w)\n    meter.reset()\n    batch_size = 16\n    binary_y, binary_pred = create_dummy_tensors_seg(batch_size)\n    meter.add(binary_pred, binary_y)\n    runs_tests_on_meter_counts_and_value(\n        meter, num_tp_check=batch_size * 15 * 15\n    )\n'"
catalyst/utils/metrics/tests/__init__.py,0,b''
catalyst/utils/metrics/tests/test_dice.py,4,"b'import torch\n\nfrom catalyst.utils import metrics\n\n\ndef test_dice():\n    """"""\n    Tests for catalyst.utils.metrics.dice metric.\n    """"""\n    size = 4\n    half_size = size // 2\n    shape = (1, 1, size, size)\n\n    # check 0: one empty\n    empty = torch.zeros(shape)\n    full = torch.ones(shape)\n    assert metrics.dice(empty, full, activation=""none"").item() == 0\n\n    # check 0: no overlap\n    left = torch.ones(shape)\n    left[:, :, :, half_size:] = 0\n    right = torch.ones(shape)\n    right[:, :, :, :half_size] = 0\n    assert metrics.dice(left, right, activation=""none"").item() == 0\n\n    # check 1: both empty, both full, complete overlap\n    assert metrics.dice(empty, empty, activation=""none"") == 1\n    assert metrics.dice(full, full, activation=""none"") == 1\n    assert metrics.dice(left, left, activation=""none"") == 1\n'"
catalyst/utils/metrics/tests/test_iou.py,9,"b'import torch\n\nfrom catalyst.utils import metrics\n\n\ndef test_iou():\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    size = 4\n    half_size = size // 2\n    shape = (1, 1, size, size)\n\n    # check 0: one empty\n    empty = torch.zeros(shape)\n    full = torch.ones(shape)\n    assert metrics.iou(empty, full, activation=""none"").item() == 0\n\n    # check 0: no overlap\n    left = torch.ones(shape)\n    left[:, :, :, half_size:] = 0\n    right = torch.ones(shape)\n    right[:, :, :, :half_size] = 0\n    assert metrics.iou(left, right, activation=""none"").item() == 0\n\n    # check 1: both empty, both full, complete overlap\n    assert metrics.iou(empty, empty, activation=""none"") == 1\n    assert metrics.iou(full, full, activation=""none"") == 1\n    assert metrics.iou(left, left, activation=""none"") == 1\n\n    # check 0.5: half overlap\n    top_left = torch.zeros(shape)\n    top_left[:, :, :half_size, :half_size] = 1\n    assert metrics.iou(top_left, left, activation=""none"").item() == 0.5\n\n    # check multiclass: 0, 0, 1, 1, 1, 0.5\n    a = torch.cat([empty, left, empty, full, left, top_left], dim=1)\n    b = torch.cat([full, right, empty, full, left, left], dim=1)\n    ans = torch.Tensor([0, 0, 1, 1, 1, 0.5])\n    assert torch.all(\n        metrics.iou(a, b, classes=[""dummy""], activation=""none"") == ans\n    )\n'"
tests/_tests_cv_classification_experiment_registry/test2/experiments/SimpleExperiment1.py,0,"b'from collections import OrderedDict\n\nfrom catalyst.contrib.data.transforms import Compose, Normalize, ToTensor\nfrom catalyst.contrib.datasets import MNIST\nfrom catalyst.dl import ConfigExperiment\n\n\nclass SimpleExperiment1(ConfigExperiment):\n    """"""\n    @TODO: Docs. Contribution is welcome\n    """"""\n\n    @staticmethod\n    def get_transforms(stage: str = None, mode: str = None):\n        """"""\n        @TODO: Docs. Contribution is welcome\n        """"""\n        return Compose([ToTensor(), Normalize((0.1307,), (0.3081,))])\n\n    def get_datasets(self, stage: str, **kwargs):\n        """"""\n        @TODO: Docs. Contribution is welcome\n        """"""\n        datasets = OrderedDict()\n\n        if stage != ""infer"":\n            trainset = MNIST(\n                ""./data"",\n                train=False,\n                download=True,\n                transform=SimpleExperiment1.get_transforms(\n                    stage=stage, mode=""train""\n                ),\n            )\n            testset = MNIST(\n                ""./data"",\n                train=False,\n                download=True,\n                transform=SimpleExperiment1.get_transforms(\n                    stage=stage, mode=""valid""\n                ),\n            )\n\n            datasets[""train""] = trainset\n            datasets[""valid""] = testset\n        else:\n            testset = MNIST(\n                ""./data"",\n                train=False,\n                download=True,\n                transform=SimpleExperiment1.get_transforms(\n                    stage=stage, mode=""valid""\n                ),\n            )\n            datasets[""infer""] = testset\n\n        return datasets\n'"
tests/_tests_cv_classification_experiment_registry/test2/experiments/SimpleExperiment2.py,0,"b'from collections import OrderedDict\n\nfrom catalyst.contrib.data.transforms import Compose, Normalize, ToTensor\nfrom catalyst.contrib.datasets import MNIST\nfrom catalyst.dl import ConfigExperiment\n\n\nclass SimpleExperiment2(ConfigExperiment):\n    """"""\n    @TODO: Docs. Contribution is welcome\n    """"""\n\n    @staticmethod\n    def get_transforms(stage: str = None, mode: str = None):\n        """"""\n        @TODO: Docs. Contribution is welcome\n        """"""\n        return Compose([ToTensor(), Normalize((0.1307,), (0.3081,))])\n\n    def get_datasets(self, stage: str, **kwargs):\n        """"""\n        @TODO: Docs. Contribution is welcome\n        """"""\n        datasets = OrderedDict()\n\n        if stage != ""infer"":\n            trainset = MNIST(\n                ""./data"",\n                train=False,\n                download=True,\n                transform=SimpleExperiment2.get_transforms(\n                    stage=stage, mode=""train""\n                ),\n            )\n            testset = MNIST(\n                ""./data"",\n                train=False,\n                download=True,\n                transform=SimpleExperiment2.get_transforms(\n                    stage=stage, mode=""valid""\n                ),\n            )\n\n            datasets[""train""] = trainset\n            datasets[""valid""] = testset\n        else:\n            testset = MNIST(\n                ""./data"",\n                train=False,\n                download=True,\n                transform=SimpleExperiment2.get_transforms(\n                    stage=stage, mode=""valid""\n                ),\n            )\n            datasets[""infer""] = testset\n\n        return datasets\n'"
tests/_tests_cv_classification_experiment_registry/test2/experiments/__init__.py,0,b'# flake8: noqa\n\nfrom .SimpleExperiment1 import SimpleExperiment1\nfrom .SimpleExperiment2 import SimpleExperiment2\n'
catalyst/contrib/data/cv/mixins/__init__.py,0,b'# flake8: noqa\n\nfrom .blur import BlurMixin\nfrom .flare import FlareMixin\nfrom .rotate import RotateMixin\n'
catalyst/contrib/data/cv/mixins/blur.py,0,"b'from typing import List\nimport random\n\nimport numpy as np\n\nimport albumentations as A\n\n\nclass BlurMixin:\n    """"""Calculates blur factor for augmented image.""""""\n\n    def __init__(\n        self,\n        input_key: str = ""image"",\n        output_key: str = ""blur_factor"",\n        blur_min: int = 3,\n        blur_max: int = 9,\n        blur: List[str] = None,\n    ):\n        """"""\n        Args:\n            input_key (str): input key to use from annotation dict\n            output_key (str): output key to use to store the result\n        """"""\n        self.input_key = input_key\n        self.output_key = output_key\n\n        self.blur_min = blur_min\n        self.blur_max = blur_max\n        blur = blur or [""Blur""]\n        self.blur = [A.__dict__[x]() for x in blur]\n        self.num_blur = len(self.blur)\n        self.num_blur_classes = blur_max - blur_min + 1 + 1\n        self.blur_probability = (\n            self.num_blur_classes - 1\n        ) / self.num_blur_classes\n\n    def __call__(self, dictionary):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        image = dictionary[self.input_key]\n        blur_factor = 0\n\n        if random.random() < self.blur_probability:\n            blur_fn = np.random.choice(self.blur)\n            blur_factor = int(\n                np.random.randint(self.blur_min, self.blur_max)\n                - self.blur_min\n                + 1\n            )\n            image = blur_fn.apply(image=image, ksize=blur_factor)\n\n        dictionary[self.input_key] = image\n        dictionary[self.output_key] = blur_factor\n\n        return dictionary\n\n\n__all__ = [""BlurMixin""]\n'"
catalyst/contrib/data/cv/mixins/flare.py,0,"b'from typing import Dict\nimport random\n\nimport albumentations as A\n\n\nclass FlareMixin:\n    """"""Calculates flare factor for augmented image.""""""\n\n    def __init__(\n        self,\n        input_key: str = ""image"",\n        output_key: str = ""flare_factor"",\n        sunflare_params: Dict = None,\n    ):\n        """"""\n        Args:\n            input_key (str): input key to use from annotation dict\n            output_key (str): output key to use to store the result\n            sunflare_params (dict): params to init\n                ``albumentations.RandomSunFlare``\n        """"""\n        self.input_key = input_key\n        self.output_key = output_key\n\n        self.sunflare_params = sunflare_params or {}\n        self.transform = A.RandomSunFlare(**self.sunflare_params)\n\n    def __call__(self, dictionary):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        image = dictionary[self.input_key]\n        sunflare_factor = 0\n\n        if random.random() < self.transform.p:\n            params = self.transform.get_params()\n            image = self.transform.apply(image=image, **params)\n            sunflare_factor = 1\n\n        dictionary[self.input_key] = image\n        dictionary[self.output_key] = sunflare_factor\n\n        return dictionary\n\n\n__all__ = [""FlareMixin""]\n'"
catalyst/contrib/data/cv/mixins/rotate.py,0,"b'import random\n\nimport albumentations as A\n\nfrom catalyst import utils\n\n\nclass RotateMixin:\n    """"""Calculates rotation factor for augmented image.""""""\n\n    def __init__(\n        self,\n        input_key: str = ""image"",\n        output_key: str = ""rotation_factor"",\n        targets_key: str = None,\n        rotate_probability: float = 1.0,\n        hflip_probability: float = 0.5,\n        one_hot_classes: int = None,\n    ):\n        """"""\n        Args:\n            input_key (str): input key to use from annotation dict\n            output_key (str): output key to use to store the result\n        """"""\n        self.input_key = input_key\n        self.output_key = output_key\n        self.targets_key = targets_key\n        self.rotate_probability = rotate_probability\n        self.hflip_probability = hflip_probability\n        self.rotate = A.RandomRotate90()\n        self.hflip = A.HorizontalFlip()\n        self.one_hot_classes = (\n            one_hot_classes * 8 if one_hot_classes is not None else None\n        )\n\n    def __call__(self, dictionary):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        image = dictionary[self.input_key]\n        rotation_factor = 0\n\n        if random.random() < self.rotate_probability:\n            rotation_factor = self.rotate.get_params()[""factor""]\n            image = self.rotate.apply(img=image, factor=rotation_factor)\n\n        if random.random() < self.hflip_probability:\n            rotation_factor += 4\n            image = self.hflip.apply(img=image)\n\n        dictionary[self.input_key] = image\n        dictionary[self.output_key] = rotation_factor\n\n        if self.targets_key is not None:\n            class_rotation_factor = (\n                dictionary[self.targets_key] * 8 + rotation_factor\n            )\n            key = f""class_rotation_{self.targets_key}""\n            dictionary[key] = class_rotation_factor\n\n            if self.one_hot_classes is not None:\n                one_hot = utils.get_one_hot(\n                    class_rotation_factor, self.one_hot_classes\n                )\n                key = f""class_rotation_{self.targets_key}_one_hot""\n                dictionary[key] = one_hot\n\n        return dictionary\n\n\n__all__ = [""RotateMixin""]\n'"
catalyst/contrib/data/cv/transforms/__init__.py,0,"b'# flake8: noqa\n\nfrom .tensor import TensorToImage, ToTensor\n'"
catalyst/contrib/data/cv/transforms/tensor.py,7,"b'import numpy as np\n\nfrom albumentations import ImageOnlyTransform\nfrom albumentations.pytorch import ToTensorV2\nimport torch\n\nfrom catalyst import utils\n\n\nclass TensorToImage(ImageOnlyTransform):\n    """"""Casts ``torch.tensor`` to ``numpy.array``.""""""\n\n    def __init__(\n        self,\n        denormalize: bool = False,\n        move_channels_dim: bool = True,\n        always_apply: bool = False,\n        p: float = 1.0,\n    ):\n        """"""\n        Args:\n            denormalize (bool): if True, multiply image(s) by ImageNet std and\n                add ImageNet mean\n            move_channels_dim (bool): if True, convert [B]xCxHxW tensor\n                to [B]xHxWxC format\n            always_apply (bool): need to apply this transform anyway\n            p (float): probability for this transform\n        """"""\n        super().__init__(always_apply, p)\n        self.denormalize = denormalize\n        self.move_channels_dim = move_channels_dim\n\n    def apply(self, img: torch.Tensor, **params) -> np.ndarray:\n        """"""Apply the transform to the image.""""""\n        if len(img.shape) == 2:\n            img = img.unsqueeze(0)\n\n        return utils.tensor_to_ndimage(\n            img,\n            denormalize=self.denormalize,\n            move_channels_dim=self.move_channels_dim,\n        )\n\n\nclass ToTensor(ToTensorV2):\n    """"""Casts ``numpy.array`` to ``torch.tensor``.""""""\n\n    def __init__(\n        self,\n        move_channels_dim: bool = True,\n        always_apply: bool = False,\n        p: float = 1.0,\n    ):\n        """"""\n        Args:\n            move_channels_dim (bool): if ``False``, casts numpy array\n                to ``torch.tensor``, but do not move channels dim\n            always_apply (bool): need to apply this transform anyway\n            p (float): probability for this transform\n        """"""\n        super().__init__(always_apply, p)\n        self.move_channels_dim = move_channels_dim\n\n    def apply(self, img: np.ndarray, **params) -> torch.Tensor:\n        """"""Apply the transform to the image.""""""\n        if self.move_channels_dim:\n            return super().apply(img, **params)\n        return torch.from_numpy(img)\n\n    def apply_to_mask(self, mask: np.ndarray, **params) -> torch.Tensor:\n        """"""Apply the transform to the mask.""""""\n        if self.move_channels_dim:\n            mask = mask.transpose(2, 0, 1)\n        return super().apply_to_mask(mask.astype(np.float32), **params)\n\n    def get_transform_init_args_names(self) -> tuple:\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        return (""move_channels_dim"",)\n\n\n__all__ = [""TensorToImage"", ""ToTensor""]\n'"
catalyst/contrib/data/nlp/dataset/__init__.py,0,"b'# flake8: noqa\n\nfrom catalyst.contrib.data.nlp.dataset.language_modeling import (\n    LanguageModelingDataset,\n)\nfrom catalyst.contrib.data.nlp.dataset.text_classification import (\n    TextClassificationDataset,\n)\n'"
catalyst/contrib/data/nlp/dataset/language_modeling.py,5,"b'from typing import Iterable, Union\n\nfrom tqdm.auto import tqdm\n\nimport torch\nfrom torch.utils.data import Dataset\nimport transformers\nfrom transformers import AutoTokenizer\n\n\nclass LanguageModelingDataset(Dataset):\n    """"""\n    Dataset for (masked) language model task.\n    Can sort sequnces for efficient padding.\n    """"""\n\n    def __init__(\n        self,\n        texts: Iterable[str],\n        tokenizer: Union[\n            str, transformers.tokenization_utils.PreTrainedTokenizer\n        ],\n        max_seq_length: int = None,\n        sort: bool = True,\n        lazy: bool = False,\n    ):\n        """"""\n        Args:\n            texts (Iterable): Iterable object with text\n            tokenizer (str or tokenizer): pre trained\n                huggingface tokenizer or model name\n            max_seq_length (int): max sequence length to tokenize\n            sort (bool): If True then sort all sequences by length\n                for efficient padding\n            lazy (bool): If True then tokenize and encode sequence\n                in __getitem__ method\n                else will tokenize in __init__ also\n                if set to true sorting is unavialible\n        """"""\n        if sort and lazy:\n            raise Exception(\n                ""lazy is set to True so we can\'t sort""\n                "" sequences by length.\\n""\n                ""You should set sort=False and lazy=True""\n                "" if you want to encode text in __get_item__ function""\n            )\n        if isinstance(tokenizer, str):\n            self.tokenizer = AutoTokenizer.from_pretrained(tokenizer)\n        elif isinstance(\n            tokenizer, transformers.tokenization_utils.PreTrainedTokenizer\n        ):\n            self.tokenizer = tokenizer\n        else:\n            raise TypeError(\n                ""tokenizer argument should be a model name""\n                + "" or huggingface PreTrainedTokenizer""\n            )\n\n        self.max_seq_length = max_seq_length\n\n        self.lazy = lazy\n\n        if lazy:\n            self.texts = texts\n\n        if not lazy:\n            pbar = tqdm(texts, desc=""tokenizing texts"")\n            self.encoded = [\n                self.tokenizer.encode(text, max_length=max_seq_length)\n                for text in pbar\n            ]\n            if sort:\n                self.encoded.sort(key=len)\n\n        self.length = len(texts)\n\n        self._getitem_fn = (\n            self._getitem_lazy if lazy else self._getitem_encoded\n        )\n\n    def __len__(self):\n        """"""Return length of dataloader""""""\n        return self.length\n\n    def _getitem_encoded(self, idx) -> torch.Tensor:\n        return torch.tensor(self.encoded[idx])\n\n    def _getitem_lazy(self, idx) -> torch.Tensor:\n        encoded = self.tokenizer.encode(\n            self.texts[idx], max_length=self.max_seq_length\n        )\n        return torch.tensor(encoded)\n\n    def __getitem__(self, idx):\n        """"""Return tokenized and encoded sequence""""""\n        return self._getitem_fn(idx)\n'"
catalyst/contrib/data/nlp/dataset/text_classification.py,8,"b'from typing import List, Mapping\nimport logging\n\nimport torch\nfrom torch.utils.data import Dataset\nfrom transformers import AutoTokenizer\n\n\nclass TextClassificationDataset(Dataset):\n    """"""Wrapper around Torch Dataset to perform text classification.""""""\n\n    def __init__(\n        self,\n        texts: List[str],\n        labels: List[str] = None,\n        label_dict: Mapping[str, int] = None,\n        max_seq_length: int = 512,\n        model_name: str = ""distilbert-base-uncased"",\n    ):\n        """"""\n        Args:\n            texts (List[str]): a list with texts to classify or to train the\n                classifier on\n            labels List[str]: a list with classification labels (optional)\n            label_dict (dict): a dictionary mapping class names to class ids,\n                to be passed to the validation data (optional)\n            max_seq_length (int): maximal sequence length in tokens,\n                texts will be stripped to this length\n            model_name (str): transformer model name, needed to perform\n                appropriate tokenization\n        """"""\n        self.texts = texts\n        self.labels = labels\n        self.label_dict = label_dict\n        self.max_seq_length = max_seq_length\n\n        if self.label_dict is None and labels is not None:\n            # {\'class1\': 0, \'class2\': 1, \'class3\': 2, ...}\n            # using this instead of `sklearn.preprocessing.LabelEncoder`\n            # no easily handle unknown target values\n            self.label_dict = dict(\n                zip(sorted(set(labels)), range(len(set(labels))))\n            )\n\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        # suppresses tokenizer warnings\n        logging.getLogger(""transformers.tokenization_utils"").setLevel(\n            logging.FATAL\n        )\n\n        # special tokens for transformers\n        # in the simplest case a [CLS] token is added in the beginning\n        # and [SEP] token is added in the end of a piece of text\n        # [CLS] <indexes text tokens> [SEP] .. <[PAD]>\n        self.sep_vid = self.tokenizer.vocab[""[SEP]""]\n        self.cls_vid = self.tokenizer.vocab[""[CLS]""]\n        self.pad_vid = self.tokenizer.vocab[""[PAD]""]\n\n    def __len__(self) -> int:\n        """"""\n        Returns:\n            int: length of the dataset\n        """"""\n        return len(self.texts)\n\n    def __getitem__(self, index) -> Mapping[str, torch.Tensor]:\n        """"""Gets element of the dataset.\n\n        Args:\n            index (int): index of the element in the dataset\n\n        Returns:\n            Single element by index\n        """"""\n        # encoding the text\n        x = self.texts[index]\n        x_encoded = self.tokenizer.encode(\n            x,\n            add_special_tokens=True,\n            max_length=self.max_seq_length,\n            return_tensors=""pt"",\n        ).squeeze(0)\n\n        # padding short texts\n        true_seq_length = x_encoded.size(0)\n        pad_size = self.max_seq_length - true_seq_length\n        pad_ids = torch.Tensor([self.pad_vid] * pad_size).long()\n        x_tensor = torch.cat((x_encoded, pad_ids))\n\n        # dealing with attention masks - there\'s a 1 for each input token and\n        # if the sequence is shorter that `max_seq_length` then the rest is\n        # padded with zeroes. Attention mask will be passed to the model in\n        # order to compute attention scores only with input data\n        # ignoring padding\n        mask = torch.ones_like(x_encoded, dtype=torch.int8)\n        mask_pad = torch.zeros_like(pad_ids, dtype=torch.int8)\n        mask = torch.cat((mask, mask_pad))\n\n        output_dict = {""features"": x_tensor, ""attention_mask"": mask}\n\n        # encoding target\n        if self.labels is not None:\n            y = self.labels[index]\n            y_encoded = (\n                torch.Tensor([self.label_dict.get(y, -1)]).long().squeeze(0)\n            )\n            output_dict[""targets""] = y_encoded\n\n        return output_dict\n\n\n__all__ = [""TextClassificationDataset""]\n'"
catalyst/contrib/dl/callbacks/tests/__init__.py,0,b''
catalyst/contrib/dl/callbacks/tests/test_gradnorm_logger.py,5,"b'from typing import Tuple\nimport collections\nfrom numbers import Number\nimport shutil\n\nimport torch\nimport torch.nn as nn\nfrom torch.optim import Adam\nfrom torch.utils.data import DataLoader\n\nfrom catalyst.contrib import registry\nfrom catalyst.contrib.data.transforms import ToTensor\nfrom catalyst.contrib.datasets import MNIST\nfrom catalyst.contrib.dl.callbacks.gradnorm_logger import GradNormLogger\nfrom catalyst.core.callback import Callback, CallbackOrder\nfrom catalyst.core.callbacks import CriterionCallback, OptimizerCallback\nfrom catalyst.core.runner import IRunner\nfrom catalyst.dl import SupervisedRunner\n\n\n@registry.Model\nclass _SimpleNet(nn.Module):\n    def __init__(self, input_shape: Tuple[int]):\n        super().__init__()\n        assert len(input_shape) == 3\n        c, h, w = input_shape\n        self.conv1 = nn.Conv2d(in_channels=c, out_channels=64, kernel_size=3)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=2)\n        self.flatten = nn.Flatten()\n\n        for conv in [self.conv1, self.conv2]:\n            h_kernel, w_kernel = conv.kernel_size\n            h_stride, w_stride = conv.stride\n            c = conv.out_channels\n            h, w = self.conv2d_size_out(\n                size=(h, w),\n                kernel_size=(h_kernel, w_kernel),\n                stride=(h_stride, w_stride),\n            )\n\n        self.fc1 = nn.Linear(in_features=c * h * w, out_features=10)\n\n    def forward(self, x: torch.Tensor):\n        for conv in [self.conv1, self.conv2]:\n            x = conv(x)\n            x = self.relu(x)\n\n        x = self.flatten(x)\n        x = self.fc1(x)\n        return x\n\n    @staticmethod\n    def conv2d_size_out(\n        *, size: Tuple[int], kernel_size: Tuple[int], stride: Tuple[int],\n    ):\n        """"""Computes output size for 2D convolution layer.\n        cur_layer_img_w = conv2d_size_out(cur_layer_img_w, kernel_size, stride)\n        cur_layer_img_h = conv2d_size_out(cur_layer_img_h, kernel_size, stride)\n        to understand the shape for dense layer\'s input.\n\n        Args:\n            size (Tuple[int]): size of input.\n            kernel_size (Tuple[int]): size of convolution kernel.\n            stride (Tuple[int]): size of convolution stride.\n\n        Returns:\n            int: output size\n        """"""\n        size, kernel_size, stride = map(\n            lambda x: torch.tensor(x, dtype=torch.int32),\n            (size, kernel_size, stride),\n        )\n        output_size = (size - (kernel_size - 1) - 1) / stride + 1\n        h, w = map(lambda x: x.item(), output_size)\n\n        return h, w\n\n\ndef _get_loaders(*, root: str, batch_size: int = 1, num_workers: int = 1):\n    data_transform = ToTensor()\n\n    trainset = MNIST(\n        root=root, train=True, download=True, transform=data_transform\n    )\n    trainloader = DataLoader(\n        trainset, batch_size=batch_size, num_workers=num_workers\n    )\n    testset = MNIST(\n        root=root, train=False, download=True, transform=data_transform\n    )\n    testloader = DataLoader(\n        testset, batch_size=batch_size, num_workers=num_workers\n    )\n\n    loaders = collections.OrderedDict(train=trainloader, valid=testloader)\n\n    return loaders\n\n\nclass _OnBatchEndCheckGradsCallback(Callback):\n    def __init__(self, prefix: str):\n        super().__init__(CallbackOrder.External)\n        self.prefix = prefix\n\n    def on_batch_end(self, runner: IRunner):\n        if not runner.is_train_loader:\n            return\n\n        for layer in [""conv1"", ""conv2"", ""fc1""]:\n            for weights in [""weight"", ""bias""]:\n                tag = f""{self.prefix}/{layer}/{weights}""\n                assert tag in runner.batch_metrics\n                assert isinstance(runner.batch_metrics[tag], Number)\n\n        tag = f""{self.prefix}/total""\n        assert tag in runner.batch_metrics\n        assert isinstance(runner.batch_metrics[tag], Number)\n\n\ndef test_save_model_grads():\n    """"""\n    Tests a feature of `OptimizerCallback` for saving model gradients\n    """"""\n    logdir = ""./logs""\n    dataset_root = ""./dataset""\n    loaders = _get_loaders(root=dataset_root, batch_size=4, num_workers=1)\n    images, _ = next(iter(loaders[""train""]))\n    _, c, h, w = images.shape\n    input_shape = (c, h, w)\n\n    model = _SimpleNet(input_shape)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = Adam(model.parameters())\n\n    criterion_callback = CriterionCallback()\n    optimizer_callback = OptimizerCallback()\n    save_model_grads_callback = GradNormLogger()\n    prefix = save_model_grads_callback.grad_norm_prefix\n    test_callback = _OnBatchEndCheckGradsCallback(prefix)\n\n    callbacks = collections.OrderedDict(\n        loss=criterion_callback,\n        optimizer=optimizer_callback,\n        grad_norm=save_model_grads_callback,\n        test_callback=test_callback,\n    )\n\n    runner = SupervisedRunner()\n    runner.train(\n        model=model,\n        criterion=criterion,\n        optimizer=optimizer,\n        loaders=loaders,\n        logdir=logdir,\n        callbacks=callbacks,\n        check=True,\n        verbose=True,\n    )\n\n    shutil.rmtree(logdir)\n    shutil.rmtree(dataset_root)\n'"
catalyst/contrib/dl/callbacks/tests/test_perplexity_callback.py,2,"b'import torch\nfrom transformers import AutoModelWithLMHead, AutoTokenizer\nfrom transformers.data.data_collator import DataCollatorForLanguageModeling\n\nfrom catalyst import dl\nfrom catalyst.contrib.data.nlp import LanguageModelingDataset\nfrom catalyst.contrib.dl.callbacks import PerplexityMetricCallback\n\n\nclass HuggingFaceRunner(dl.Runner):\n    """"""Just an example""""""\n\n    def _handle_batch(self, batch):\n        masked_lm_labels = batch.get(""masked_lm_labels"")\n        lm_labels = batch.get(""lm_labels"")\n        if masked_lm_labels is None and lm_labels is None:\n            # expecting huggingface style mapping\n            raise Exception(""batch mast have mlm_labels or lm_labels key"")\n        output = self.model(**batch)\n        vocab_size = output[1].size(2)\n\n        loss = output[0]\n        logits = output[1].view(-1, vocab_size)\n        self.batch_metrics = {""loss"": loss}\n        if masked_lm_labels is not None:\n            self.input[""targets""] = masked_lm_labels.view(-1)\n            self.output = {""loss"": loss, ""logits"": logits}\n        else:\n            self.input[""targets""] = lm_labels.view(-1)\n            self.output = {""loss"": loss, ""logits"": logits}\n\n\ntexts = [\n    """"""Bonaparte Crossing the Alps is an oil-on-canvas painting by French artist"""""",  # noqa: E501\n    """"""Bhaskara\'s Lemma is an identity used as a lemma during the chakravala method. """""",  # noqa: E501\n]\n\n\ndef test_is_running():\n    """"""Test if perplexity is running normal""""""\n    tok = AutoTokenizer.from_pretrained(""distilbert-base-uncased"")\n    model = AutoModelWithLMHead.from_pretrained(""distilbert-base-uncased"")\n    dataset = LanguageModelingDataset(texts, tok)\n    collate_fn = DataCollatorForLanguageModeling(tok).collate_batch\n    dataloader = torch.utils.data.DataLoader(dataset, collate_fn=collate_fn)\n    optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n\n    runner = HuggingFaceRunner()\n    runner.train(\n        model=model,\n        optimizer=optimizer,\n        loaders={""train"": dataloader},\n        callbacks={\n            ""optimizer"": dl.OptimizerCallback(),\n            ""perplexity"": PerplexityMetricCallback(),\n        },\n        check=True,\n    )\n    assert True\n'"
catalyst/contrib/dl/callbacks/tests/test_tracer_callback.py,12,"b'from typing import Dict, Tuple, Union\nimport collections\nfrom pathlib import Path\nimport shutil\n\nimport torch\nimport torch.nn as nn\nfrom torch.optim import Adam\nfrom torch.utils.data import DataLoader\n\nfrom catalyst.contrib import registry\nfrom catalyst.contrib.data.transforms import ToTensor\nfrom catalyst.contrib.datasets import MNIST\nfrom catalyst.contrib.dl.callbacks.tracer_callback import TracerCallback\nfrom catalyst.core.callback import Callback, CallbackOrder\nfrom catalyst.core.callbacks import CriterionCallback, OptimizerCallback\nfrom catalyst.core.runner import IRunner\nfrom catalyst.dl import SupervisedRunner\nfrom catalyst.dl.utils import get_device, get_trace_name\n\n\n@registry.Model\nclass _TracedNet(nn.Module):\n    """"""\n    Simple model for the testing.\n    """"""\n\n    def __init__(self, input_shape: Tuple[int]):\n        """"""\n        Args:\n            input_shape (Tuple[int]): Shape of input tensor.\n        """"""\n        super().__init__()\n        assert len(input_shape) == 3\n        c, h, w = input_shape\n        self.conv1 = nn.Conv2d(in_channels=c, out_channels=64, kernel_size=3)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=2)\n        self.flatten = nn.Flatten()\n\n        for conv in [self.conv1, self.conv2]:\n            h_kernel, w_kernel = conv.kernel_size\n            h_stride, w_stride = conv.stride\n            c = conv.out_channels\n            h, w = self.conv2d_size_out(\n                size=(h, w),\n                kernel_size=(h_kernel, w_kernel),\n                stride=(h_stride, w_stride),\n            )\n\n        self.fc1 = nn.Linear(in_features=c * h * w, out_features=10)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        """"""\n        Args:\n            x (torch.Tensor): Input tensor\n\n        Returns:\n            (torch.Tensor): Output tensor\n        """"""\n        for conv in [self.conv1, self.conv2]:\n            x = conv(x)\n            x = self.relu(x)\n\n        x = self.flatten(x)\n        x = self.fc1(x)\n        return x\n\n    @staticmethod\n    def conv2d_size_out(\n        *, size: Tuple[int], kernel_size: Tuple[int], stride: Tuple[int],\n    ) -> Tuple[int, int]:\n        """"""\n        Computes output size for 2D convolution layer.\n        cur_layer_img_w = conv2d_size_out(cur_layer_img_w, kernel_size, stride)\n        cur_layer_img_h = conv2d_size_out(cur_layer_img_h, kernel_size, stride)\n        to understand the shape for dense layer\'s input.\n\n        Args:\n            size (Tuple[int]): size of input.\n            kernel_size (Tuple[int]): size of convolution kernel.\n            stride (Tuple[int]): size of convolution stride.\n\n        Returns:\n            (Tuple[int, int]): output size\n        """"""\n        size, kernel_size, stride = map(\n            lambda x: torch.tensor(x, dtype=torch.int32),\n            (size, kernel_size, stride),\n        )\n        output_size = (size - (kernel_size - 1) - 1) / stride + 1\n        h, w = map(lambda x: x.item(), output_size)\n\n        return h, w\n\n\ndef _get_loaders(\n    *, root: str, batch_size: int = 1, num_workers: int = 1\n) -> Dict[str, DataLoader]:\n    """"""\n    Function to get loaders just for testing.\n\n    Args:\n        root (str): Path to root of dataset.\n        batch_size (int): Batch size.\n        num_workers (int): Num of workers.\n\n    Returns:\n        (Dict[str, DataLoader]): Dict of loaders.\n    """"""\n    data_transform = ToTensor()\n\n    trainset = MNIST(\n        root=root, train=True, download=True, transform=data_transform\n    )\n    trainloader = DataLoader(\n        trainset, batch_size=batch_size, num_workers=num_workers\n    )\n    testset = MNIST(\n        root=root, train=False, download=True, transform=data_transform\n    )\n    testloader = DataLoader(\n        testset, batch_size=batch_size, num_workers=num_workers\n    )\n\n    loaders = collections.OrderedDict(train=trainloader, valid=testloader)\n\n    return loaders\n\n\nclass _OnStageEndCheckModelTracedCallback(Callback):\n    """"""\n    Callback to test traced model at the end of the stage.\n    """"""\n\n    def __init__(self, path: Union[str, Path], inputs: torch.Tensor):\n        """"""\n        Args:\n            path (Union[str, Path]): Path to traced model.\n            inputs (torch.Tensor): Input samples.\n        """"""\n        super().__init__(CallbackOrder.External)\n        self.path: Path = Path(path)\n        self.inputs: torch.Tensor = inputs\n        self.device = get_device()\n\n    def on_stage_end(self, runner: IRunner):\n        """"""\n        Args:\n            runner (IRunner): current runner\n        """"""\n        assert self.path.exists(), ""Traced model was not found""\n\n        traced_model = torch.jit.load(str(self.path))\n        traced_model = traced_model.to(self.device)\n        self.inputs = self.inputs.to(self.device)\n        result = traced_model(self.inputs)\n\n        assert result is not None and isinstance(\n            result, torch.Tensor\n        ), ""Traced model is not working correctly""\n\n\ndef test_tracer_callback():\n    """"""\n    Tests a feature of `TracerCallback` for model tracing during training\n    """"""\n    logdir = ""./logs""\n    dataset_root = ""./dataset""\n    loaders = _get_loaders(root=dataset_root, batch_size=4, num_workers=1)\n    images, targets = next(iter(loaders[""train""]))\n    _, c, h, w = images.shape\n    input_shape = (c, h, w)\n\n    model = _TracedNet(input_shape)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = Adam(model.parameters())\n\n    method_name = ""forward""\n    mode = ""eval""\n    requires_grad = False\n    checkpoint_name = ""best""\n    opt_level = None\n\n    trace_name = get_trace_name(\n        method_name=method_name,\n        mode=mode,\n        requires_grad=requires_grad,\n        additional_string=checkpoint_name,\n    )\n    tracing_path = Path(logdir) / ""trace"" / trace_name\n    criterion_callback = CriterionCallback()\n    optimizer_callback = OptimizerCallback()\n    tracer_callback = TracerCallback(\n        metric=""loss"",\n        minimize=False,\n        trace_mode=mode,\n        mode=checkpoint_name,\n        do_once=True,\n        method_name=method_name,\n        requires_grad=requires_grad,\n        opt_level=opt_level,\n    )\n    test_callback = _OnStageEndCheckModelTracedCallback(\n        path=tracing_path, inputs=images,\n    )\n\n    callbacks = collections.OrderedDict(\n        loss=criterion_callback,\n        optimizer=optimizer_callback,\n        tracer_callback=tracer_callback,\n        test_callback=test_callback,\n    )\n\n    runner = SupervisedRunner(input_key=""x"")\n    runner.train(\n        model=model,\n        criterion=criterion,\n        optimizer=optimizer,\n        loaders=loaders,\n        logdir=logdir,\n        callbacks=callbacks,\n        check=True,\n        verbose=True,\n    )\n\n    shutil.rmtree(logdir)\n    shutil.rmtree(dataset_root)\n'"
catalyst/contrib/models/cv/classification/__init__.py,0,b''
catalyst/contrib/models/cv/encoders/__init__.py,0,"b'# flake8: noqa\n\nfrom .resnet import ResnetEncoder\n\n__all__ = [""ResnetEncoder""]\n'"
catalyst/contrib/models/cv/encoders/resnet.py,2,"b'from typing import Union\nfrom pathlib import Path\n\nimport torch\nfrom torch import nn\nimport torchvision\n\nfrom catalyst import utils\nfrom catalyst.contrib.nn.modules import Flatten\nfrom catalyst.contrib.registry import MODULES\n\n\nclass ResnetEncoder(nn.Module):\n    """"""Specifies ResNet encoders for classification network.\n\n    Examples:\n        >>> encoders = ResnetEncoder(\n        >>>    arch=""resnet18"",\n        >>>    pretrained=False,\n        >>>    state_dict=""/model/path/resnet18-5c106cde.pth""\n        >>> )\n    """"""\n\n    def __init__(\n        self,\n        arch: str = ""resnet18"",\n        pretrained: bool = True,\n        frozen: bool = True,\n        pooling: str = None,\n        pooling_kwargs: dict = None,\n        cut_layers: int = 2,\n        state_dict: Union[dict, str, Path] = None,\n    ):\n        """"""\n        Args:\n            arch (str): Name for resnet. Have to be one of\n                resnet18, resnet34, resnet50, resnet101, resnet152\n            pretrained (bool): If True, returns a model pre-trained on ImageNet\n            frozen (bool): If frozen, sets requires_grad to False\n            pooling (str): pooling\n            pooling_kwargs (dict): params for pooling\n            state_dict (Union[dict, str, Path]): Path to ``torch.Model``\n                or a dict containing parameters and persistent buffers.\n        """"""\n        super().__init__()\n\n        resnet = torchvision.models.__dict__[arch](pretrained=pretrained)\n        if state_dict is not None:\n            if isinstance(state_dict, (Path, str)):\n                state_dict = torch.load(str(state_dict))\n            resnet.load_state_dict(state_dict)\n\n        modules = list(resnet.children())[:-cut_layers]  # delete last layers\n\n        if frozen:\n            for module in modules:\n                utils.set_requires_grad(module, requires_grad=False)\n\n        if pooling is not None:\n            pooling_kwargs = pooling_kwargs or {}\n            pooling_layer_fn = MODULES.get(pooling)\n            pooling_layer = (\n                pooling_layer_fn(\n                    in_features=resnet.fc.in_features, **pooling_kwargs\n                )\n                if ""attn"" in pooling.lower()\n                else pooling_layer_fn(**pooling_kwargs)\n            )\n            modules += [pooling_layer]\n\n            if hasattr(pooling_layer, ""out_features""):\n                out_features = pooling_layer.out_features(\n                    in_features=resnet.fc.in_features\n                )\n            else:\n                out_features = None\n        else:\n            out_features = resnet.fc.in_features\n\n        modules += [Flatten()]\n        self.out_features = out_features\n\n        self.encoder = nn.Sequential(*modules)\n\n    def forward(self, image):\n        """"""Extract the image feature vectors.""""""\n        features = self.encoder(image)\n        return features\n'"
catalyst/contrib/models/cv/segmentation/__init__.py,0,"b'# flake8: noqa\n\nfrom .abn import *\nfrom .blocks import *\nfrom .bridge import *\nfrom .core import *\nfrom .decoder import *\nfrom .encoder import *\nfrom .fpn import *\nfrom .head import *\nfrom .linknet import *\nfrom .psp import *\nfrom .unet import *\n\n__all__ = [\n    ""UnetMetaSpec"",\n    ""UnetSpec"",\n    ""ResnetUnetSpec"",\n    ""Unet"",\n    ""Linknet"",\n    ""FPNUnet"",\n    ""PSPnet"",\n    ""ResnetUnet"",\n    ""ResnetLinknet"",\n    ""ResnetFPNUnet"",\n    ""ResnetPSPnet"",\n]\n'"
catalyst/contrib/models/cv/segmentation/abn.py,0,"b'from typing import Dict\n\nfrom torch import nn\n\n\nclass ABN(nn.Module):\n    """"""Activated Batch Normalization.\n\n    This gathers a `BatchNorm2d` and an activation function in a single module.\n\n    @TODO: Docs (add `Example`). Contribution is welcome.\n    """"""\n\n    def __init__(\n        self,\n        num_features: int,\n        activation: str = ""leaky_relu"",\n        batchnorm_params: Dict = None,\n        activation_params: Dict = None,\n        use_batchnorm: bool = True,\n    ):\n        """"""\n        Args:\n            num_features (int): number of feature channels\n                in the input and output\n            activation (str): name of the activation functions, one of:\n                ``\'leaky_relu\'``, ``\'elu\'`` or ``\'none\'``.\n            batchnorm_params (dict): additional ``nn.BatchNorm2d`` params\n            activation_params (dict): additional params for activation fucntion\n            use_batchnorm (bool): @TODO: Docs. Contribution is welcome\n        """"""\n        super().__init__()\n        batchnorm_params = batchnorm_params or {}\n        activation_params = activation_params or {}\n\n        layers = []\n        if use_batchnorm:\n            layers.append(\n                nn.BatchNorm2d(num_features=num_features, **batchnorm_params)\n            )\n        if activation is not None and activation.lower() != ""none"":\n            layers.append(\n                nn.__dict__[activation](inplace=True, **activation_params)\n            )\n\n        self.net = nn.Sequential(*layers)\n\n    def forward(self, x):\n        """"""Forward call.""""""\n        x = self.net(x)\n        return x\n'"
catalyst/contrib/models/cv/segmentation/core.py,6,"b'from typing import Dict, List, Union\nfrom pathlib import Path\n\nimport torch\nfrom torch import nn\n\nfrom .bridge import BridgeSpec\nfrom .decoder import DecoderSpec\nfrom .encoder import EncoderSpec, ResnetEncoder, UnetEncoder\nfrom .head import HeadSpec\n\n\nclass UnetMetaSpec(nn.Module):\n    """"""@TODO: Docs. Contribution is welcome.""""""\n\n    def __init__(\n        self,\n        encoder: EncoderSpec,\n        decoder: DecoderSpec,\n        bridge: BridgeSpec = None,\n        head: HeadSpec = None,\n        state_dict: Union[dict, str, Path] = None,\n    ):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        super().__init__()\n        self.encoder = encoder\n        self.bridge = bridge or (lambda x: x)\n        self.decoder = decoder\n        self.head = head or (lambda x: x)\n\n        if state_dict is not None:\n            if isinstance(state_dict, (Path, str)):\n                state_dict = torch.load(str(state_dict))\n            if ""model_state_dict"" in state_dict.keys():\n                state_dict = state_dict[""model_state_dict""]\n            self.load_state_dict(state_dict)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        """"""Forward call.""""""\n        encoder_features: List[torch.Tensor] = self.encoder(x)\n        bridge_features: List[torch.Tensor] = self.bridge(encoder_features)\n        decoder_features: List[torch.Tensor] = self.decoder(bridge_features)\n        output: torch.Tensor = self.head(decoder_features)\n        return output\n\n\nclass UnetSpec(UnetMetaSpec):\n    """"""@TODO: Docs. Contribution is welcome.""""""\n\n    def __init__(\n        self,\n        num_classes: int = 1,\n        in_channels: int = 3,\n        num_channels: int = 32,\n        num_blocks: int = 4,\n        encoder_params: Dict = None,\n        bridge_params: Dict = None,\n        decoder_params: Dict = None,\n        head_params: Dict = None,\n        state_dict: Union[dict, str, Path] = None,\n    ):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        encoder_params = encoder_params or {}\n        bridge_params = bridge_params or {}\n        decoder_params = decoder_params or {}\n        head_params = head_params or {}\n\n        encoder = UnetEncoder(\n            in_channels=in_channels,\n            num_channels=num_channels,\n            num_blocks=num_blocks,\n            **encoder_params\n        )\n\n        encoder, bridge, decoder, head = self._get_components(\n            encoder, num_classes, bridge_params, decoder_params, head_params\n        )\n\n        super().__init__(\n            encoder=encoder,\n            bridge=bridge,\n            decoder=decoder,\n            head=head,\n            state_dict=state_dict,\n        )\n\n    def _get_components(\n        self,\n        encoder: UnetEncoder,\n        num_classes: int,\n        bridge_params: Dict,\n        decoder_params: Dict,\n        head_params: Dict,\n    ):\n        raise NotImplementedError()\n\n\nclass ResnetUnetSpec(UnetMetaSpec):\n    """"""@TODO: Docs. Contribution is welcome.""""""\n\n    def __init__(\n        self,\n        num_classes: int = 1,\n        arch: str = ""resnet18"",\n        pretrained: bool = True,\n        encoder_params: Dict = None,\n        bridge_params: Dict = None,\n        decoder_params: Dict = None,\n        head_params: Dict = None,\n        state_dict: Union[dict, str, Path] = None,\n    ):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        encoder_params = encoder_params or {}\n        bridge_params = bridge_params or {}\n        decoder_params = decoder_params or {}\n        head_params = head_params or {}\n\n        encoder = ResnetEncoder(\n            arch=arch, pretrained=pretrained, **encoder_params\n        )\n\n        encoder, bridge, decoder, head = self._get_components(\n            encoder, num_classes, bridge_params, decoder_params, head_params\n        )\n\n        super().__init__(\n            encoder=encoder,\n            bridge=bridge,\n            decoder=decoder,\n            head=head,\n            state_dict=state_dict,\n        )\n\n    def _get_components(\n        self,\n        encoder: UnetEncoder,\n        num_classes: int,\n        bridge_params: Dict,\n        decoder_params: Dict,\n        head_params: Dict,\n    ):\n        raise NotImplementedError()\n'"
catalyst/contrib/models/cv/segmentation/fpn.py,0,"b'from typing import Dict\n\nfrom .blocks import EncoderDownsampleBlock\nfrom .bridge import UnetBridge\nfrom .core import ResnetUnetSpec, UnetSpec\nfrom .decoder import FPNDecoder\nfrom .encoder import ResnetEncoder, UnetEncoder\nfrom .head import FPNHead\n\n\nclass FPNUnet(UnetSpec):\n    """"""@TODO: Docs. Contribution is welcome.""""""\n\n    def _get_components(\n        self,\n        encoder: UnetEncoder,\n        num_classes: int,\n        bridge_params: Dict,\n        decoder_params: Dict,\n        head_params: Dict,\n    ):\n        bridge = UnetBridge(\n            in_channels=encoder.out_channels,\n            in_strides=encoder.out_strides,\n            out_channels=encoder.out_channels[-1] * 2,\n            block_fn=EncoderDownsampleBlock,\n            **bridge_params\n        )\n        decoder = FPNDecoder(\n            in_channels=bridge.out_channels,\n            in_strides=bridge.out_strides,\n            **decoder_params\n        )\n        head = FPNHead(\n            in_channels=decoder.out_channels,\n            in_strides=decoder.out_strides,\n            out_channels=num_classes,\n            upsample_scale=decoder.out_strides[-1],\n            interpolation_mode=""bilinear"",\n            align_corners=True,\n            **head_params\n        )\n        return encoder, bridge, decoder, head\n\n\nclass ResnetFPNUnet(ResnetUnetSpec):\n    """"""@TODO: Docs. Contribution is welcome.""""""\n\n    def _get_components(\n        self,\n        encoder: ResnetEncoder,\n        num_classes: int,\n        bridge_params: Dict,\n        decoder_params: Dict,\n        head_params: Dict,\n    ):\n        bridge = None\n        decoder = FPNDecoder(\n            in_channels=encoder.out_channels,\n            in_strides=encoder.out_strides,\n            **decoder_params\n        )\n        head = FPNHead(\n            in_channels=decoder.out_channels,\n            in_strides=decoder.out_strides,\n            out_channels=num_classes,\n            upsample_scale=decoder.out_strides[-1],\n            interpolation_mode=""bilinear"",\n            align_corners=True,\n            **head_params\n        )\n        return encoder, bridge, decoder, head\n'"
catalyst/contrib/models/cv/segmentation/linknet.py,0,"b'from typing import Dict\nfrom functools import partial\n\nimport numpy as np\n\nfrom .blocks import DecoderSumBlock, EncoderDownsampleBlock\nfrom .bridge import UnetBridge\nfrom .core import ResnetUnetSpec, UnetSpec\nfrom .decoder import UNetDecoder\nfrom .encoder import ResnetEncoder, UnetEncoder\nfrom .head import UnetHead\n\n\nclass Linknet(UnetSpec):\n    """"""@TODO: Docs. Contribution is welcome.""""""\n\n    def _get_components(\n        self,\n        encoder: UnetEncoder,\n        num_classes: int,\n        bridge_params: Dict,\n        decoder_params: Dict,\n        head_params: Dict,\n    ):\n        bridge = UnetBridge(\n            in_channels=encoder.out_channels,\n            in_strides=encoder.out_strides,\n            out_channels=encoder.out_channels[-1] * 2,\n            block_fn=EncoderDownsampleBlock,\n            **bridge_params\n        )\n        decoder = UNetDecoder(\n            in_channels=bridge.out_channels,\n            in_strides=bridge.out_strides,\n            block_fn=DecoderSumBlock,\n            **decoder_params\n        )\n        head = UnetHead(\n            in_channels=decoder.out_channels,\n            in_strides=decoder.out_strides,\n            out_channels=num_classes,\n            num_upsample_blocks=int(np.log2(decoder.out_strides[-1])),\n            **head_params\n        )\n        return encoder, bridge, decoder, head\n\n\nclass ResnetLinknet(ResnetUnetSpec):\n    """"""@TODO: Docs. Contribution is welcome.""""""\n\n    def _get_components(\n        self,\n        encoder: ResnetEncoder,\n        num_classes: int,\n        bridge_params: Dict,\n        decoder_params: Dict,\n        head_params: Dict,\n    ):\n        bridge = None\n        decoder = UNetDecoder(\n            in_channels=encoder.out_channels,\n            in_strides=encoder.out_strides,\n            block_fn=partial(\n                DecoderSumBlock, aggregate_first=False, upsample_scale=None\n            ),\n            **decoder_params\n        )\n        head = UnetHead(\n            in_channels=decoder.out_channels,\n            in_strides=decoder.out_strides,\n            out_channels=num_classes,\n            num_upsample_blocks=int(np.log2(decoder.out_strides[-1])),\n            **head_params\n        )\n        return encoder, bridge, decoder, head\n'"
catalyst/contrib/models/cv/segmentation/psp.py,0,"b'from typing import Dict\n\nfrom .core import ResnetUnetSpec, UnetSpec\nfrom .decoder import PSPDecoder\nfrom .encoder import ResnetEncoder, UnetEncoder\nfrom .head import UnetHead\n\n\nclass PSPnet(UnetSpec):\n    """"""@TODO: Docs. Contribution is welcome.""""""\n\n    def _get_components(\n        self,\n        encoder: UnetEncoder,\n        num_classes: int,\n        bridge_params: Dict,\n        decoder_params: Dict,\n        head_params: Dict,\n    ):\n        bridge = None\n        decoder = PSPDecoder(\n            in_channels=encoder.out_channels,\n            in_strides=encoder.out_strides,\n            **decoder_params\n        )\n        head = UnetHead(\n            in_channels=decoder.out_channels,\n            in_strides=decoder.out_strides,\n            out_channels=num_classes,\n            upsample_scale=decoder.out_strides[-1],\n            interpolation_mode=""bilinear"",\n            align_corners=True,\n            **head_params\n        )\n        return encoder, bridge, decoder, head\n\n\nclass ResnetPSPnet(ResnetUnetSpec):\n    """"""@TODO: Docs. Contribution is welcome.""""""\n\n    def _get_components(\n        self,\n        encoder: ResnetEncoder,\n        num_classes: int,\n        bridge_params: Dict,\n        decoder_params: Dict,\n        head_params: Dict,\n    ):\n        bridge = None\n        decoder = PSPDecoder(\n            in_channels=encoder.out_channels,\n            in_strides=encoder.out_strides,\n            **decoder_params\n        )\n        head = UnetHead(\n            in_channels=decoder.out_channels,\n            in_strides=decoder.out_strides,\n            out_channels=num_classes,\n            upsample_scale=decoder.out_strides[-1],\n            interpolation_mode=""bilinear"",\n            align_corners=True,\n            **head_params\n        )\n        return encoder, bridge, decoder, head\n'"
catalyst/contrib/models/cv/segmentation/unet.py,0,"b'from typing import Dict\nfrom functools import partial\n\nimport numpy as np\n\nfrom .blocks import (\n    DecoderConcatBlock,\n    EncoderDownsampleBlock,\n    EncoderUpsampleBlock,\n)\nfrom .bridge import UnetBridge\nfrom .core import ResnetUnetSpec, UnetSpec\nfrom .decoder import UNetDecoder\nfrom .encoder import ResnetEncoder, UnetEncoder\nfrom .head import UnetHead\n\n\nclass Unet(UnetSpec):\n    """"""@TODO: Docs. Contribution is welcome.""""""\n\n    def _get_components(\n        self,\n        encoder: UnetEncoder,\n        num_classes: int,\n        bridge_params: Dict,\n        decoder_params: Dict,\n        head_params: Dict,\n    ):\n        bridge = UnetBridge(\n            in_channels=encoder.out_channels,\n            in_strides=encoder.out_strides,\n            out_channels=encoder.out_channels[-1] * 2,\n            block_fn=EncoderDownsampleBlock,\n            **bridge_params,\n        )\n        decoder = UNetDecoder(\n            in_channels=bridge.out_channels,\n            in_strides=bridge.out_strides,\n            block_fn=DecoderConcatBlock,\n            **decoder_params,\n        )\n        head = UnetHead(\n            in_channels=decoder.out_channels,\n            in_strides=decoder.out_strides,\n            out_channels=num_classes,\n            num_upsample_blocks=int(np.log2(decoder.out_strides[-1])),\n            **head_params,\n        )\n\n        return encoder, bridge, decoder, head\n\n\nclass ResnetUnet(ResnetUnetSpec):\n    """"""@TODO: Docs. Contribution is welcome.""""""\n\n    def _get_components(\n        self,\n        encoder: ResnetEncoder,\n        num_classes: int,\n        bridge_params: Dict,\n        decoder_params: Dict,\n        head_params: Dict,\n    ):\n        bridge = UnetBridge(\n            in_channels=encoder.out_channels,\n            in_strides=encoder.out_strides,\n            out_channels=encoder.out_channels[-1],\n            block_fn=partial(EncoderUpsampleBlock, pool_first=True),\n            **bridge_params,\n        )\n        decoder = UNetDecoder(\n            in_channels=bridge.out_channels,\n            in_strides=bridge.out_strides,\n            block_fn=partial(\n                DecoderConcatBlock, aggregate_first=True, upsample_scale=2\n            ),\n            **decoder_params,\n        )\n        head = UnetHead(\n            in_channels=decoder.out_channels,\n            in_strides=decoder.out_strides,\n            out_channels=num_classes,\n            num_upsample_blocks=int(np.log2(decoder.out_strides[-1])),\n            **head_params,\n        )\n        return encoder, bridge, decoder, head\n'"
catalyst/contrib/models/nlp/classification/__init__.py,0,b'# flake8: noqa\n\nfrom .bert import BertClassifier\n'
catalyst/contrib/models/nlp/classification/bert.py,7,"b'from typing import Optional\n\nimport torch\nfrom torch import nn\nfrom transformers import AutoConfig, AutoModel\n\n\nclass BertClassifier(nn.Module):\n    """"""Simplified version of the same class by HuggingFace.\n\n    See ``transformers/modeling_distilbert.py`` in the transformers repository.\n    """"""\n\n    def __init__(\n        self, pretrained_model_name: str, num_classes: Optional[int] = None\n    ):\n        """"""\n        Args:\n            pretrained_model_name (str): HuggingFace model name.\n                See transformers/modeling_auto.py\n            num_classes (int, optional): the number of class labels\n                in the classification task\n        """"""\n        super().__init__()\n\n        config = AutoConfig.from_pretrained(\n            pretrained_model_name, num_labels=num_classes\n        )\n\n        self.distilbert = AutoModel.from_pretrained(\n            pretrained_model_name, config=config\n        )\n        self.pre_classifier = nn.Linear(config.dim, config.dim)\n        self.classifier = nn.Sequential(\n            nn.ReLU(),\n            nn.Dropout(config.seq_classif_dropout),\n            nn.Linear(config.dim, num_classes),\n        )\n\n    def forward(\n        self,\n        features: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        head_mask: Optional[torch.Tensor] = None,\n    ) -> torch.Tensor:\n        """"""Compute class probabilities for the input sequence.\n\n        Args:\n            features (torch.Tensor): ids of each token,\n                size ([bs, seq_length]\n            attention_mask (torch.Tensor, optional): binary tensor,\n                used to select tokens which are used to compute attention\n                scores in the self-attention heads, size [bs, seq_length]\n            head_mask (torch.Tensor, optional): 1.0 in head_mask indicates that\n                we keep the head, size: [num_heads]\n                or [num_hidden_layers x num_heads]\n\n        Returns:\n            PyTorch Tensor with predicted class probabilities\n        """"""\n        assert attention_mask is not None, ""attention mask is none""\n        distilbert_output = self.distilbert(\n            input_ids=features,\n            attention_mask=attention_mask,\n            head_mask=head_mask,\n        )\n        # we only need the hidden state here and don\'t need\n        # transformer output, so index 0\n        hidden_state = distilbert_output[0]  # (bs, seq_len, dim)\n        # we take embeddings from the [CLS] token, so again index 0\n        pooled_output = hidden_state[:, 0]  # (bs, dim)\n        pooled_output = self.pre_classifier(pooled_output)  # (bs, dim)\n        logits = self.classifier(pooled_output)  # (bs, dim)\n\n        return logits\n\n\n__all__ = [""BertClassifier""]\n'"
catalyst/contrib/models/nlp/encoders/__init__.py,0,b''
catalyst/contrib/utils/cv/tests/__init__.py,0,b''
catalyst/contrib/utils/cv/tests/test_image.py,1,"b'import numpy as np\n\nimport torch\n\nfrom catalyst import utils\nfrom catalyst.contrib.data.transforms import normalize, to_tensor\nfrom catalyst.contrib.utils.cv.tensor import _IMAGENET_MEAN, _IMAGENET_STD\n\n\ndef test_imread():\n    """"""Tests ``imread`` functionality.""""""\n    jpg_rgb_uri = (\n        ""https://raw.githubusercontent.com/catalyst-team/catalyst-pics/master""\n        ""/test_images/catalyst_icon.jpg""\n    )\n    jpg_grs_uri = (\n        ""https://raw.githubusercontent.com/catalyst-team/catalyst-pics/master""\n        ""/test_images/catalyst_icon_grayscale.jpg""\n    )\n    png_rgb_uri = (\n        ""https://raw.githubusercontent.com/catalyst-team/catalyst-pics/master""\n        ""/test_images/catalyst_icon.png""\n    )\n    png_grs_uri = (\n        ""https://raw.githubusercontent.com/catalyst-team/catalyst-pics/master""\n        ""/test_images/catalyst_icon_grayscale.png""\n    )\n\n    for uri in [jpg_rgb_uri, jpg_grs_uri, png_rgb_uri, png_grs_uri]:\n        img = utils.imread(uri)\n        assert img.shape == (400, 400, 3)\n        img = utils.imread(uri, grayscale=True)\n        assert img.shape == (400, 400, 1)\n\n\ndef test_tensor_to_ndimage():\n    """"""Tests ``tensor_to_ndimage`` functionality.""""""\n    orig_images = np.random.randint(0, 255, (2, 20, 10, 3), np.uint8)\n\n    torch_images = torch.stack(\n        [\n            normalize(to_tensor(im), _IMAGENET_MEAN, _IMAGENET_STD)\n            for im in orig_images\n        ],\n        dim=0,\n    )\n\n    byte_images = utils.tensor_to_ndimage(torch_images, dtype=np.uint8)\n    float_images = utils.tensor_to_ndimage(torch_images, dtype=np.float32)\n\n    assert np.allclose(byte_images, orig_images)\n    assert np.allclose(float_images, orig_images / 255, atol=1e-3, rtol=1e-3)\n\n    assert np.allclose(\n        utils.tensor_to_ndimage(torch_images[0]),\n        orig_images[0] / 255,\n        atol=1e-3,\n        rtol=1e-3,\n    )\n'"
catalyst/contrib/data/cv/mixins/tests/__init__.py,0,b''
catalyst/contrib/data/cv/mixins/tests/test_mixin.py,0,"b'from catalyst import utils\nfrom catalyst.contrib.data.cv.mixins import BlurMixin, FlareMixin, RotateMixin\n\njpg_rgb_uri = (\n    ""https://raw.githubusercontent.com/catalyst-team/catalyst-pics/master""\n    ""/test_images/catalyst_icon.jpg""\n)\n\nimage = utils.imread(jpg_rgb_uri)\n\n\ndef test_blur_mixin():\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    global image\n    image_ = image.copy()\n\n    mixin = BlurMixin()\n    input = {""image"": image_}\n    output = mixin(input)\n\n    assert mixin.input_key in output\n    assert mixin.output_key in output\n    assert output[mixin.input_key].shape == image_.shape\n\n    assert 0 <= output[mixin.output_key] < mixin.blur_max\n\n\ndef test_flare_mixin():\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    global image\n    image_ = image.copy()\n\n    mixin = FlareMixin()\n    input = {""image"": image_}\n    output = mixin(input)\n\n    assert mixin.input_key in output\n    assert mixin.output_key in output\n    assert output[mixin.input_key].shape == image_.shape\n\n    assert 0 <= output[mixin.output_key]\n\n\ndef test_rotate_mixin():\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    global image\n    image_ = image.copy()\n\n    mixin = RotateMixin()\n    input = {""image"": image_}\n    output = mixin(input)\n\n    assert mixin.input_key in output\n    assert mixin.output_key in output\n    assert output[mixin.input_key].shape == image_.shape\n\n    assert 0 <= output[mixin.output_key] < 8\n'"
catalyst/contrib/data/nlp/dataset/tests/__init__.py,0,b''
catalyst/contrib/data/nlp/dataset/tests/test_language_modeling_dataset.py,0,"b'import pytest\n\nimport torch  # noqa: F401\nfrom transformers import AutoTokenizer\n\nfrom catalyst.contrib.data.nlp.dataset import LanguageModelingDataset\n\ntexts = [\n    """"""Bonaparte Crossing the Alps is an oil-on-canvas painting by French artist"""""",  # noqa: E501\n    """"""Bhaskara\'s Lemma is an identity used as a lemma during the chakravala method. """""",  # noqa: E501\n]\n\n\ndef test_tokenizer_str():\n    """"""Test initialization with string""""""\n    dataset = LanguageModelingDataset(texts, ""bert-base-uncased"")\n    assert dataset[0] is not None\n    assert len(dataset) == 2\n\n\ndef test_tokenizer_tokenizer():\n    """"""Test initialization with tokenizer""""""\n    tok = AutoTokenizer.from_pretrained(""bert-base-uncased"")\n    dataset = LanguageModelingDataset(texts, tok)\n    assert dataset[0] is not None\n    assert len(dataset) == 2\n\n\n@pytest.mark.xfail(raises=Exception)\ndef test_exception_with_sort():\n    """"""Test lazy=True sort=True case""""""\n    tok = AutoTokenizer.from_pretrained(""bert-base-uncased"")\n    dataset = LanguageModelingDataset(  # noqa: F841\n        texts, tok, lazy=True, sort=True\n    )\n\n\n@pytest.mark.xfail(raises=TypeError)\ndef test_tokenizer_type_error():\n    """"""Test if tokenizer neither hf nor string""""""\n    tok = lambda x: x\n    dataset = LanguageModelingDataset(texts, tok)  # noqa: F841\n'"
catalyst/contrib/data/nlp/dataset/tests/test_text_classification_dataset.py,0,"b'from catalyst.contrib.data.nlp.dataset import TextClassificationDataset\n\ntexts = [\n    ""The color of this T-shirt is sooo so horrible"",\n    ""Nice gears, enjoy price-ti-quality ratio"",\n]\n\nlabels = [""negative"", ""positive""]\n\n\ndef test_should_have_cls_id_as_first_token_for_input_ids():\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    dataset = TextClassificationDataset(texts, labels)\n    features = dataset[0][""features""]\n    assert features[0] == dataset.cls_vid\n\n\ndef test_input_ids_should_be_padded():\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    dataset = TextClassificationDataset(texts, labels)\n    features = dataset[0][""features""]\n    assert features.size(0) == 512\n\n\ndef test_mask_sum_should_be_eq_to_seq_len():\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    dataset = TextClassificationDataset(texts, labels)\n    mask = dataset[0][""attention_mask""]\n    assert mask.size(0) == 512\n    assert mask.sum() == 14\n    assert mask[:14].sum() == 14\n\n\ndef test_label_dict():\n    """"""@TODO: Docs. Contribution is welcome.""""""\n    dataset = TextClassificationDataset(texts, labels)\n    label_dict = dataset.label_dict\n    assert label_dict == {""negative"": 0, ""positive"": 1}\n'"
catalyst/contrib/models/cv/segmentation/blocks/__init__.py,0,b'# flake8: noqa\nfrom .core import *\nfrom .fpn import *\nfrom .psp import *\nfrom .unet import *\n'
catalyst/contrib/models/cv/segmentation/blocks/core.py,6,"b'from abc import ABC, abstractmethod\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nfrom ..abn import ABN\n\n\ndef _get_block(\n    in_channels: int,\n    out_channels: int,\n    abn_block: nn.Module = ABN,\n    activation: str = ""ReLU"",\n    kernel_size: int = 3,\n    padding: int = 1,\n    first_stride: int = 1,\n    second_stride: int = 1,\n    complexity: int = 1,\n    **kwargs\n):\n    layers = [\n        nn.Conv2d(\n            in_channels,\n            out_channels,\n            kernel_size=kernel_size,\n            padding=padding,\n            stride=first_stride,\n            bias=False,\n            **kwargs\n        ),\n        abn_block(out_channels, activation=activation),\n    ]\n    if complexity > 0:\n        layers_ = [\n            nn.Conv2d(\n                out_channels,\n                out_channels,\n                kernel_size=kernel_size,\n                padding=padding,\n                stride=second_stride,\n                bias=False,\n                **kwargs\n            ),\n            abn_block(out_channels, activation=activation),\n        ] * complexity\n        layers = layers + layers_\n    block = nn.Sequential(*layers)\n    return block\n\n\ndef _upsample(\n    x: torch.Tensor,\n    scale: int = None,\n    size: int = None,\n    interpolation_mode: str = ""bilinear"",\n    align_corners: bool = True,\n) -> torch.Tensor:\n    if scale is None:\n        x = F.interpolate(\n            x, size=size, mode=interpolation_mode, align_corners=align_corners\n        )\n    else:\n        x = F.interpolate(\n            x,\n            scale_factor=scale,\n            mode=interpolation_mode,\n            align_corners=align_corners,\n        )\n    return x\n\n\nclass EncoderBlock(ABC, nn.Module):\n    """"""@TODO: Docs (add description, `Example`). Contribution is welcome.""""""\n\n    def __init__(\n        self, in_channels: int, out_channels: int, in_strides: int = None\n    ):\n        """"""\n        Args:\n            @TODO: Docs. Contribution is welcome.\n        """"""\n        super().__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.in_strides = in_strides\n\n    @property\n    @abstractmethod\n    def out_strides(self) -> int:\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        pass\n\n    @property\n    @abstractmethod\n    def block(self) -> nn.Module:\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        pass\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        """"""Forward call.""""""\n        return self.block(x)\n\n\nclass DecoderBlock(ABC, nn.Module):\n    """"""@TODO: Docs (add description, `Example`). Contribution is welcome.""""""\n\n    def __init__(\n        self,\n        in_channels: int,\n        enc_channels: int,\n        out_channels: int,\n        in_strides: int = None,\n        *args,\n        **kwargs\n    ):\n        """"""\n        Args:\n            @TODO: Docs. Contribution is welcome.\n        """"""\n        super().__init__()\n        self.in_channels = in_channels\n        self.enc_channels = enc_channels\n        self.out_channels = out_channels\n        self.in_strides = in_strides\n\n        self.block = self._get_block(*args, **kwargs)\n\n    @abstractmethod\n    def _get_block(self, *args, **kwargs) -> nn.Module:\n        pass\n\n    @property\n    def out_strides(self) -> int:\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        return self.in_strides // 2 if self.in_strides is not None else None\n\n    @abstractmethod\n    def forward(\n        self, bottom: torch.Tensor, left: torch.Tensor\n    ) -> torch.Tensor:\n        """"""Forward call.""""""\n        pass\n'"
catalyst/contrib/models/cv/segmentation/blocks/fpn.py,5,"b'import torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nfrom .core import DecoderBlock\n\n\nclass DecoderFPNBlock(DecoderBlock):\n    """"""@TODO: Docs (add description, `Example`). Contribution is welcome.""""""\n\n    def __init__(\n        self,\n        in_channels: int,\n        enc_channels: int,\n        out_channels: int,\n        in_strides: int = None,\n        upsample_scale: int = 2,\n        interpolation_mode: str = ""nearest"",\n        align_corners: bool = None,\n        aggregate_first: bool = False,\n        **kwargs\n    ):\n        """"""\n        Args:\n            @TODO: Docs. Contribution is welcome.\n        """"""\n        self.upsample_scale = upsample_scale\n        self.interpolation_mode = interpolation_mode\n        self.align_corners = align_corners\n        super().__init__(\n            in_channels, enc_channels, out_channels, in_strides, **kwargs\n        )\n\n    def _get_block(self):\n        block = nn.Conv2d(self.enc_channels, self.out_channels, kernel_size=1)\n        return block\n\n    def forward(\n        self, bottom: torch.Tensor, left: torch.Tensor\n    ) -> torch.Tensor:\n        """"""Forward call.""""""\n        x = F.interpolate(\n            bottom,\n            scale_factor=self.upsample_scale,\n            mode=self.interpolation_mode,\n            align_corners=self.align_corners,\n        )\n        left = self.block(left)\n        x = x + left\n        return x\n\n\nclass Conv3x3GNReLU(nn.Module):\n    """"""@TODO: Docs (add description, `Example`). Contribution is welcome.""""""\n\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        upsample=False,\n        upsample_scale: int = 2,\n        interpolation_mode: str = ""bilinear"",\n        align_corners: bool = True,\n    ):\n        """"""\n        Args:\n            @TODO: Docs. Contribution is welcome.\n        """"""\n        super().__init__()\n        self.upsample = upsample\n        self.upsample_scale = upsample_scale\n        self.interpolation_mode = interpolation_mode\n        self.align_corners = align_corners\n\n        self.block = nn.Sequential(\n            nn.Conv2d(\n                in_channels,\n                out_channels,\n                kernel_size=3,\n                stride=1,\n                padding=1,\n                bias=False,\n            ),\n            nn.GroupNorm(32, out_channels),\n            nn.ReLU(inplace=True),\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        """"""Forward call.""""""\n        x = self.block(x)\n        if self.upsample:\n            x = F.interpolate(\n                x,\n                scale_factor=self.upsample_scale,\n                mode=self.interpolation_mode,\n                align_corners=self.align_corners,\n            )\n        return x\n\n\nclass SegmentationBlock(nn.Module):\n    """"""@TODO: Docs (add description, `Example`). Contribution is welcome.""""""\n\n    def __init__(\n        self, in_channels: int, out_channels: int, num_upsamples: int = 0\n    ):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        super().__init__()\n\n        blocks = [\n            Conv3x3GNReLU(\n                in_channels, out_channels, upsample=bool(num_upsamples)\n            )\n        ]\n\n        if num_upsamples > 1:\n            for _ in range(1, num_upsamples):\n                blocks.append(\n                    Conv3x3GNReLU(out_channels, out_channels, upsample=True)\n                )\n\n        self.block = nn.Sequential(*blocks)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        """"""Forward call.""""""\n        return self.block(x)\n'"
catalyst/contrib/models/cv/segmentation/blocks/psp.py,4,"b'from typing import Tuple\nfrom functools import partial\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nfrom ..abn import ABN\nfrom .core import _get_block\n\n\nclass PyramidBlock(nn.Module):\n    """"""@TODO: Docs. Contribution is welcome.""""""\n\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        pool_size: int,\n        use_batchnorm: bool = True,\n        interpolation_mode: str = ""bilinear"",\n        align_corners: bool = True,\n        complexity: int = 0,\n    ):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        super().__init__()\n        self.interpolation_mode = interpolation_mode\n        self.align_corners = align_corners\n\n        if pool_size == 1:\n            use_batchnorm = False\n\n        self._block = nn.Sequential(\n            nn.AdaptiveAvgPool2d(output_size=(pool_size, pool_size)),\n            _get_block(\n                in_channels,\n                out_channels,\n                abn_block=partial(ABN, use_batchnorm=use_batchnorm),\n                complexity=complexity,\n            ),\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        """"""Forward call.""""""\n        h, w = x.shape[-2:]\n        x = self._block(x)\n        x = F.interpolate(\n            x,\n            size=(h, w),\n            mode=self.interpolation_mode,\n            align_corners=self.align_corners,\n        )\n        return x\n\n\nclass PSPBlock(nn.Module):\n    """"""@TODO: Docs. Contribution is welcome.""""""\n\n    def __init__(\n        self,\n        in_channels: int,\n        pool_sizes: Tuple[int] = (1, 2, 3, 6),\n        use_batchnorm: bool = True,\n    ):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        super().__init__()\n\n        self.stages = nn.ModuleList(\n            [\n                PyramidBlock(\n                    in_channels,\n                    in_channels // len(pool_sizes),\n                    pool_size,\n                    use_batchnorm=use_batchnorm,\n                )\n                for pool_size in pool_sizes\n            ]\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        """"""Forward call.""""""\n        xs = [stage(x) for stage in self.stages] + [x]\n        x = torch.cat(xs, dim=1)\n        return x\n'"
catalyst/contrib/models/cv/segmentation/blocks/unet.py,8,"b'import torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nfrom ..abn import ABN\nfrom .core import _get_block, _upsample, DecoderBlock, EncoderBlock\n\n\nclass EncoderDownsampleBlock(EncoderBlock):\n    """"""@TODO: Docs. Contribution is welcome.""""""\n\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        in_strides: int = None,\n        abn_block: nn.Module = ABN,\n        activation: str = ""ReLU"",\n        first_stride: int = 2,\n        second_stride: int = 1,\n        **kwargs\n    ):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        super().__init__(in_channels, out_channels, in_strides)\n        self._out_strides = (\n            in_strides * first_stride * second_stride\n            if in_strides is not None\n            else None\n        )\n        self._block = _get_block(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            abn_block=abn_block,\n            activation=activation,\n            first_stride=first_stride,\n            second_stride=second_stride,\n            **kwargs\n        )\n\n    @property\n    def out_strides(self) -> int:\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        return self._out_strides\n\n    @property\n    def block(self):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        return self._block\n\n\nclass EncoderUpsampleBlock(EncoderBlock):\n    """"""@TODO: Docs. Contribution is welcome.""""""\n\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        in_strides: int = None,\n        abn_block: nn.Module = ABN,\n        activation: str = ""ReLU"",\n        first_stride: int = 1,\n        second_stride: int = 1,\n        pool_first: bool = False,\n        upsample_scale: int = 2,\n        interpolation_mode: str = ""nearest"",\n        align_corners: bool = None,\n        **kwargs\n    ):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        super().__init__(in_channels, out_channels, in_strides)\n        if in_strides is None:\n            self._out_strides = None\n        elif pool_first:\n            self._out_strides = (\n                in_strides * first_stride * second_stride * 2 // upsample_scale\n            )\n        else:\n            self._out_strides = (\n                in_strides * first_stride * second_stride // upsample_scale\n            )\n        self.pool_first = pool_first\n        self.upsample_scale = upsample_scale\n        self.interpolation_mode = interpolation_mode\n        self.align_corners = align_corners\n        self._block = _get_block(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            abn_block=abn_block,\n            activation=activation,\n            first_stride=first_stride,\n            second_stride=second_stride,\n            **kwargs\n        )\n\n    @property\n    def out_strides(self) -> int:\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        return self._out_strides\n\n    @property\n    def block(self):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        return self._block\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        """"""Forward call.""""""\n        if self.pool_first:\n            x = F.max_pool2d(\n                x, kernel_size=self.upsample_scale, stride=self.upsample_scale\n            )\n        x = F.interpolate(\n            x,\n            scale_factor=self.upsample_scale,\n            mode=self.interpolation_mode,\n            align_corners=self.align_corners,\n        )\n        return self.block(x)\n\n\nclass DecoderConcatBlock(DecoderBlock):\n    """"""@TODO: Docs. Contribution is welcome.""""""\n\n    def __init__(\n        self,\n        in_channels: int,\n        enc_channels: int,\n        out_channels: int,\n        in_strides: int = None,\n        abn_block: nn.Module = ABN,\n        activation: str = ""ReLU"",\n        pre_dropout_rate: float = 0.0,\n        post_dropout_rate: float = 0.0,\n        upsample_scale: int = None,\n        interpolation_mode: str = ""bilinear"",\n        align_corners: bool = True,\n        aggregate_first: bool = False,\n        **kwargs\n    ):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        self.upsample_scale = upsample_scale\n        self.interpolation_mode = interpolation_mode\n        self.align_corners = align_corners\n        self.aggregate_first = aggregate_first\n\n        super().__init__(\n            in_channels,\n            enc_channels,\n            out_channels,\n            in_strides,\n            abn_block=abn_block,\n            activation=activation,\n            pre_dropout_rate=pre_dropout_rate,\n            post_dropout_rate=post_dropout_rate,\n            **kwargs\n        )\n\n    def _get_block(\n        self,\n        abn_block: nn.Module = ABN,\n        activation: str = ""ReLU"",\n        pre_dropout_rate: float = 0.0,\n        post_dropout_rate: float = 0.0,\n        **kwargs\n    ):\n        layers = []\n        if pre_dropout_rate > 0:\n            layers.append(nn.Dropout2d(pre_dropout_rate, inplace=True))\n        layers.append(\n            _get_block(\n                in_channels=self.in_channels + self.enc_channels,\n                out_channels=self.out_channels,\n                abn_block=abn_block,\n                activation=activation,\n                first_stride=1,\n                second_stride=1,\n                **kwargs\n            )\n        )\n        if post_dropout_rate > 0:\n            layers.append(nn.Dropout2d(pre_dropout_rate, inplace=True))\n\n        block = nn.Sequential(*layers)\n        return block\n\n    def forward(\n        self, bottom: torch.Tensor, left: torch.Tensor\n    ) -> torch.Tensor:\n        """"""Forward call.""""""\n        if self.aggregate_first:\n            x = torch.cat([bottom, left], 1)\n            x = _upsample(\n                x,\n                scale=self.upsample_scale,\n                interpolation_mode=self.interpolation_mode,\n                align_corners=self.align_corners,\n            )\n        else:\n            x = _upsample(\n                bottom,\n                scale=self.upsample_scale,\n                size=left.shape[2:],\n                interpolation_mode=self.interpolation_mode,\n                align_corners=self.align_corners,\n            )\n            x = torch.cat([x, left], 1)\n\n        return self.block(x)\n\n\nclass DecoderSumBlock(DecoderConcatBlock):\n    """"""@TODO: Docs (add description, `Example`). Contribution is welcome""""""\n\n    def __init__(self, enc_channels: int, **kwargs):\n        """"""\n        Args:\n            @TODO: Docs. Contribution is welcome.\n        """"""\n        super().__init__(enc_channels=0, **kwargs)\n\n    def forward(\n        self, bottom: torch.Tensor, left: torch.Tensor\n    ) -> torch.Tensor:\n        """"""Forward call.""""""\n        if self.aggregate_first:\n            x = bottom + left\n            x = _upsample(\n                x,\n                scale=self.upsample_scale,\n                interpolation_mode=self.interpolation_mode,\n                align_corners=self.align_corners,\n            )\n            x = self.block(x)\n        else:\n            x = _upsample(\n                bottom,\n                scale=self.upsample_scale,\n                size=left.shape[2:],\n                interpolation_mode=self.interpolation_mode,\n                align_corners=self.align_corners,\n            )\n            x = self.block(x)\n            x = x + left\n\n        return x\n'"
catalyst/contrib/models/cv/segmentation/bridge/__init__.py,0,b'# flake8: noqa\nfrom .core import *\nfrom .unet import *\n'
catalyst/contrib/models/cv/segmentation/bridge/core.py,1,"b'from typing import List\nfrom abc import ABC, abstractmethod\n\nimport torch\nfrom torch import nn\n\n\nclass BridgeSpec(ABC, nn.Module):\n    """"""@TODO: Docs. Contribution is welcome.""""""\n\n    def __init__(self, in_channels: List[int], in_strides: List[int]):\n        """"""\n        Args:\n            in_channels (List[int]): number of channels in the input sample\n            in_strides (List[int]): the stride of the block\n        """"""\n        super().__init__()\n        self._in_channels = in_channels\n        self._in_strides = in_strides\n\n    @property\n    def in_channels(self) -> List[int]:\n        """"""Number of channels in the input sample.""""""\n        return self._in_channels\n\n    @property\n    def in_strides(self) -> List[int]:\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        return self._in_strides\n\n    @property\n    @abstractmethod\n    def out_channels(self) -> List[int]:\n        """"""Number of channels produced by the block.""""""\n        pass\n\n    @property\n    @abstractmethod\n    def out_strides(self) -> List[int]:\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        pass\n\n    @abstractmethod\n    def forward(self, x: List[torch.Tensor]) -> List[torch.Tensor]:\n        """"""Forward call.""""""\n        pass\n'"
catalyst/contrib/models/cv/segmentation/bridge/unet.py,3,"b'from typing import List\n\nimport torch\n\nfrom ..blocks import EncoderBlock, EncoderDownsampleBlock\nfrom .core import BridgeSpec\n\n\nclass UnetBridge(BridgeSpec):\n    """"""@TODO: Docs. Contribution is welcome.""""""\n\n    def __init__(\n        self,\n        in_channels: List[int],\n        in_strides: List[int],\n        out_channels: int,\n        block_fn: EncoderBlock = EncoderDownsampleBlock,\n        **kwargs\n    ):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        super().__init__(in_channels, in_strides)\n\n        self.block = block_fn(\n            in_channels=in_channels[-1],\n            in_strides=in_strides[-1],\n            out_channels=out_channels,\n            **kwargs\n        )\n\n        self._out_channels = in_channels + [self.block.out_channels]\n        self._out_strides = in_strides + [self.block.out_strides]\n\n    @property\n    def out_channels(self) -> List[int]:\n        """"""Number of channels produced by the block.""""""\n        return self._out_channels\n\n    @property\n    def out_strides(self) -> List[int]:\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        return self._out_strides\n\n    def forward(self, x: List[torch.Tensor]) -> List[torch.Tensor]:\n        """"""Forward call.""""""\n        x_: torch.Tensor = x[-1]\n        x_: torch.Tensor = self.block(x_)\n        output = x + [x_]\n        return output\n'"
catalyst/contrib/models/cv/segmentation/decoder/__init__.py,0,b'# flake8: noqa\nfrom .core import *\nfrom .fpn import *\nfrom .psp import *\nfrom .unet import *\n'
catalyst/contrib/models/cv/segmentation/decoder/core.py,1,"b'from typing import List\nfrom abc import ABC, abstractmethod\n\nimport torch\nfrom torch import nn\n\n\nclass DecoderSpec(ABC, nn.Module):\n    """"""@TODO: Docs. Contribution is welcome.""""""\n\n    def __init__(self, in_channels: List[int], in_strides: List[int]):\n        """"""\n        Args:\n            in_channels (List[int]): number of channels in the input sample\n            in_strides (List[int]): the stride of the block\n        """"""\n        super().__init__()\n        self.in_channels = in_channels\n        self.in_strides = in_strides\n\n    @property\n    @abstractmethod\n    def out_channels(self) -> List[int]:\n        """"""Number of channels produced by the block.""""""\n        pass\n\n    @property\n    @abstractmethod\n    def out_strides(self) -> List[int]:\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        pass\n\n    @abstractmethod\n    def forward(self, x: List[torch.Tensor]) -> torch.Tensor:\n        """"""Forward call.""""""\n        pass\n'"
catalyst/contrib/models/cv/segmentation/decoder/fpn.py,1,"b'from typing import List\n\nimport torch\nfrom torch import nn\n\nfrom ..blocks.fpn import DecoderFPNBlock\nfrom .core import DecoderSpec\n\n\nclass FPNDecoder(DecoderSpec):\n    """"""@TODO: Docs. Contribution is welcome.""""""\n\n    def __init__(\n        self,\n        in_channels: List[int],\n        in_strides: List[int],\n        pyramid_channels: int = 256,\n        **kwargs\n    ):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        super().__init__(in_channels, in_strides)\n        out_strides_ = [in_strides[-1]]\n\n        self.center_conv = nn.Conv2d(\n            in_channels[-1], pyramid_channels, kernel_size=1\n        )\n\n        # features from encoders blocks\n        reversed_features = list(reversed(in_channels[:-1]))\n\n        blocks = []\n        for encoder_features in reversed_features:\n            blocks.append(\n                DecoderFPNBlock(\n                    in_channels=pyramid_channels,\n                    enc_channels=encoder_features,\n                    out_channels=pyramid_channels,\n                    in_strides=out_strides_[-1],\n                    **kwargs\n                )\n            )\n            out_strides_.append(blocks[-1].out_strides)\n        self.blocks = nn.ModuleList(blocks)\n        self._out_channels = [pyramid_channels] * len(in_channels)\n        self._out_strides = out_strides_\n\n    @property\n    def out_channels(self) -> List[int]:\n        """"""Number of channels produced by the block.""""""\n        return self._out_channels\n\n    @property\n    def out_strides(self) -> List[int]:\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        return self._out_strides\n\n    def forward(self, x: List[torch.Tensor]) -> List[torch.Tensor]:\n        """"""Forward call.""""""\n        # features from center block\n        fpn_features = [self.center_conv(x[-1])]\n        # features from encoders blocks\n        reversed_features = list(reversed(x[:-1]))\n\n        for _i, (fpn_block, encoder_output) in enumerate(\n            zip(self.blocks, reversed_features)\n        ):\n            fpn_features.append(fpn_block(fpn_features[-1], encoder_output))\n\n        return fpn_features\n'"
catalyst/contrib/models/cv/segmentation/decoder/psp.py,1,"b'from typing import List\nfrom functools import partial\n\nimport torch\n\nfrom ..abn import ABN\nfrom ..blocks.core import _get_block\nfrom ..blocks.psp import PSPBlock\nfrom .core import DecoderSpec\n\n\nclass PSPDecoder(DecoderSpec):\n    """"""@TODO: Docs. Contribution is welcome.""""""\n\n    def __init__(\n        self,\n        in_channels: List[int],\n        in_strides: List[int],\n        downsample_factor: int = 8,\n        use_batchnorm: bool = True,\n        out_channels: int = 512,\n    ):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        super().__init__(in_channels, in_strides)\n        self.block_offset = self._get_block_offset(downsample_factor)\n        psp_out_channels: int = self._get(in_channels)\n\n        self.psp = PSPBlock(\n            psp_out_channels,\n            pool_sizes=(1, 2, 3, 6),\n            use_batchnorm=use_batchnorm,\n        )\n\n        self.conv = _get_block(\n            psp_out_channels * 2,\n            out_channels,\n            kernel_size=1,\n            padding=0,\n            abn_block=partial(ABN, use_batchnorm=use_batchnorm),\n            complexity=0,\n        )\n        self._out_channels = out_channels\n        self.downsample_factor = downsample_factor\n\n    @property\n    def out_channels(self) -> List[int]:\n        """"""Number of channels produced by the block.""""""\n        return [self._out_channels]\n\n    @property\n    def out_strides(self) -> List[int]:\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        return [self.downsample_factor]\n\n    def _get_block_offset(self, downsample_factor: int):\n        offset = self.in_strides.index(downsample_factor)\n        return offset\n\n    def _get(self, xs: List):\n        return xs[self.block_offset]\n\n    def forward(self, x: List[torch.Tensor]) -> List[torch.Tensor]:\n        """"""Forward call.""""""\n        features = self._get(x)\n        x = self.psp(features)\n        x = self.conv(x)\n        return [x]\n'"
catalyst/contrib/models/cv/segmentation/decoder/unet.py,1,"b'from typing import List\n\nimport torch\nfrom torch import nn\n\nfrom ..blocks.core import DecoderBlock\nfrom ..blocks.unet import DecoderConcatBlock\nfrom .core import DecoderSpec\n\n\nclass UNetDecoder(DecoderSpec):\n    """"""@TODO: Docs. Contribution is welcome.""""""\n\n    def __init__(\n        self,\n        in_channels: List[int],\n        in_strides: List[int],\n        block_fn: DecoderBlock = DecoderConcatBlock,\n        **kwargs\n    ):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        super().__init__(in_channels, in_strides)\n\n        # features from center block\n        out_channels_ = [in_channels[-1]]\n        out_strides_ = [in_strides[-1]]\n        # features from encoders blocks\n        reversed_channels = list(reversed(in_channels[:-1]))\n\n        blocks: List[DecoderBlock] = []\n        for encoder_channels in reversed_channels:\n            out_channels_.append(encoder_channels)\n            blocks.append(\n                block_fn(\n                    in_channels=out_channels_[-2],\n                    enc_channels=encoder_channels,\n                    out_channels=out_channels_[-1],\n                    in_strides=out_strides_[-1],\n                    **kwargs\n                )\n            )\n            out_strides_.append(blocks[-1].out_strides)\n\n        self.blocks = nn.ModuleList(blocks)\n        self._out_channels = out_channels_\n        self._out_strides = out_strides_\n\n    @property\n    def out_channels(self) -> List[int]:\n        """"""Number of channels produced by the block.""""""\n        return self._out_channels\n\n    @property\n    def out_strides(self) -> List[int]:\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        return self._out_strides\n\n    def forward(self, x: List[torch.Tensor]) -> List[torch.Tensor]:\n        """"""Forward call.""""""\n        # features from center block\n        decoder_outputs = [x[-1]]\n        # features from encoders blocks\n        reversed_features = list(reversed(x[:-1]))\n\n        for _i, (decoder_block, encoder_output) in enumerate(\n            zip(self.blocks, reversed_features)\n        ):\n            decoder_outputs.append(\n                decoder_block(decoder_outputs[-1], encoder_output)\n            )\n\n        return decoder_outputs\n'"
catalyst/contrib/models/cv/segmentation/encoder/__init__.py,0,b'# flake8: noqa\nfrom .core import *\nfrom .resnet import *\nfrom .unet import *\n'
catalyst/contrib/models/cv/segmentation/encoder/core.py,0,"b'from typing import List\nfrom abc import ABC, abstractmethod\n\nfrom torch import nn\n\n\ndef _take(elements, indexes):\n    return [elements[i] for i in indexes]\n\n\nclass EncoderSpec(ABC, nn.Module):\n    """"""@TODO: Docs. Contribution is welcome.""""""\n\n    @property\n    @abstractmethod\n    def out_channels(self) -> List[int]:\n        """"""Number of channels produced by the block.""""""\n        pass\n\n    @property\n    @abstractmethod\n    def out_strides(self) -> List[int]:\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        pass\n'"
catalyst/contrib/models/cv/segmentation/encoder/resnet.py,3,"b'from typing import List, Union\nfrom collections import OrderedDict\nfrom pathlib import Path\n\nimport torch\nfrom torch import nn\nimport torchvision\n\nfrom catalyst import utils\n\nfrom .core import _take, EncoderSpec\n\nRESNET_PARAMS = {\n    ""resnet18"": {\n        ""channels"": [64, 64, 128, 256, 512],\n        ""strides"": [2, 4, 8, 16, 32],\n    },\n    ""resnet34"": {\n        ""channels"": [64, 64, 128, 256, 512],\n        ""strides"": [2, 4, 8, 16, 32],\n    },\n    ""resnet50"": {\n        ""channels"": [64, 256, 512, 1024, 2048],\n        ""strides"": [2, 4, 8, 16, 32],\n    },\n    ""resnet101"": {\n        ""channels"": [64, 256, 512, 1024, 2048],\n        ""strides"": [2, 4, 8, 16, 32],\n    },\n    ""resnet152"": {\n        ""channels"": [64, 256, 512, 1024, 2048],\n        ""strides"": [2, 4, 8, 16, 32],\n    },\n}\n\n\nclass ResnetEncoder(EncoderSpec):\n    """"""Specifies ResNet encoders for segmentation network.\n\n    Examples:\n        >>> encoders = ResnetEncoder(\n        >>>    arch=""resnet18"",\n        >>>    pretrained=False,\n        >>>    state_dict=""/model/path/resnet18-5c106cde.pth""\n        >>> )\n    """"""\n\n    def __init__(\n        self,\n        arch: str = ""resnet18"",\n        pretrained: bool = True,\n        requires_grad: bool = True,\n        layers_indices: List[int] = None,\n        state_dict: Union[dict, str, Path] = None,\n    ):\n        """"""\n        Args:\n            arch (str): Name for resnet. Have to be one of\n                resnet18, resnet34, resnet50, resnet101, resnet152\n            pretrained (bool): If True, returns a model pre-trained on ImageNet\n            requires_grad (bool): Flag for set_requires_grad.\n                If None, calculates as ``not requires_grad``\n            layers_indices (List[int]): layers of encoders\n                used for segmentation\n                If None, calculates as ``[1, 2, 3, 4]``\n            state_dict (Union[dict, str, Path]): Path to ``torch.Model``\n                or a dict containing parameters and persistent buffers.\n        """"""\n        super().__init__()\n\n        resnet = torchvision.models.__dict__[arch](pretrained=pretrained)\n        resnet_params = RESNET_PARAMS[arch]\n        if state_dict is not None:\n            if isinstance(state_dict, (Path, str)):\n                state_dict = torch.load(str(state_dict))\n            resnet.load_state_dict(state_dict)\n        self._layers_indices = layers_indices or [1, 2, 3, 4]\n        self._channels, self._strides = (\n            resnet_params[""channels""],\n            resnet_params[""strides""],\n        )\n        self._channels = _take(self._channels, self._layers_indices)\n        self._strides = _take(self._strides, self._layers_indices)\n\n        layer0 = nn.Sequential(\n            OrderedDict(\n                [\n                    (""conv1"", resnet.conv1),\n                    (""bn1"", resnet.bn1),\n                    (""relu"", resnet.relu),\n                ]\n            )\n        )\n        self._layers = nn.ModuleList(\n            [\n                layer0,\n                resnet.layer1,\n                resnet.layer2,\n                resnet.layer3,\n                resnet.layer4,\n            ]\n        )\n        self.maxpool0 = resnet.maxpool\n\n        if requires_grad is None:\n            requires_grad = not pretrained\n\n        utils.set_requires_grad(self, requires_grad)\n\n    @property\n    def out_channels(self) -> List[int]:\n        """"""Number of channels produced by the block.""""""\n        return self._channels\n\n    @property\n    def out_strides(self) -> List[int]:\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        return self._strides\n\n    def forward(self, x: torch.Tensor) -> List[torch.Tensor]:\n        """"""Forward call.""""""\n        output = []\n        for i, layer in enumerate(self._layers):\n            layer_output = layer(x)\n            output.append(layer_output)\n\n            if i == 0:\n                # Fist maxpool operator is not a part of layer0\n                # because we want that layer0 output to have stride of 2\n                layer_output = self.maxpool0(layer_output)\n            x = layer_output\n\n        output = _take(output, self._layers_indices)\n        return output\n'"
catalyst/contrib/models/cv/segmentation/encoder/unet.py,1,"b'from typing import List\n\nimport torch\nfrom torch import nn\n\nfrom ..blocks.unet import EncoderDownsampleBlock\nfrom .core import _take, EncoderSpec\n\n\nclass UnetEncoder(EncoderSpec):\n    """"""@TODO: Docs. Contribution is welcome.""""""\n\n    def __init__(\n        self,\n        in_channels: int,\n        num_channels: int,\n        num_blocks: int,\n        layers_indices: List[int] = None,\n        **kwargs,\n    ):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        super().__init__()\n\n        self.num_filters = num_channels\n        self.num_blocks = num_blocks\n        self._layers_indices = layers_indices or list(range(num_blocks))\n\n        self._channels = [\n            self.num_filters * 2 ** i for i in range(self.num_blocks)\n        ]\n        self._strides = [2 ** (i) for i in range(self.num_blocks)]\n        self._channels = _take(self._channels, self._layers_indices)\n        self._strides = _take(self._strides, self._layers_indices)\n\n        for i in range(num_blocks):\n            in_channels = in_channels if not i else num_channels * 2 ** (i - 1)\n            out_channels = num_channels * 2 ** i\n            self.add_module(\n                f""block{i + 1}"",\n                EncoderDownsampleBlock(\n                    in_channels, out_channels, first_stride=1, **kwargs\n                ),\n            )\n            if i != self.num_blocks - 1:\n                self.add_module(f""pool{i + 1}"", nn.MaxPool2d(2, 2))\n\n    @property\n    def out_channels(self) -> List[int]:\n        """"""Number of channels produced by the block.""""""\n        return self._channels\n\n    @property\n    def out_strides(self) -> List[int]:\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        return self._strides\n\n    def forward(self, x: torch.Tensor) -> List[torch.Tensor]:\n        """"""Forward call.""""""\n        output = []\n        for i in range(self.num_blocks):\n            x = self.__getattr__(f""block{i + 1}"")(x)\n            output.append(x)\n            if i != self.num_blocks - 1:\n                x = self.__getattr__(f""pool{i + 1}"")(x)\n        output = _take(output, self._layers_indices)\n        return output\n'"
catalyst/contrib/models/cv/segmentation/head/__init__.py,0,b'# flake8: noqa\nfrom .core import *\nfrom .fpn import *\nfrom .unet import *\n'
catalyst/contrib/models/cv/segmentation/head/core.py,1,"b'from typing import List\nfrom abc import ABC, abstractmethod\n\nimport torch\nfrom torch import nn\n\n\nclass HeadSpec(ABC, nn.Module):\n    """"""@TODO: Docs. Contribution is welcome.""""""\n\n    def __init__(\n        self,\n        in_channels: List[int],\n        out_channles: int,\n        in_strides: List[int] = None,\n    ):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        super().__init__()\n        self.in_channels = in_channels\n        self.in_strides = in_strides\n        self.out_channles = out_channles\n\n    @abstractmethod\n    def forward(self, x: List[torch.Tensor]) -> torch.Tensor:\n        """"""Forward call.""""""\n        pass\n'"
catalyst/contrib/models/cv/segmentation/head/fpn.py,2,"b'from typing import List\n\nimport numpy as np\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nfrom ..blocks import EncoderUpsampleBlock, SegmentationBlock\nfrom .core import HeadSpec\n\n\nclass FPNHead(HeadSpec):\n    """"""@TODO: Docs. Contribution is welcome.""""""\n\n    def __init__(\n        self,\n        in_channels: List[int],\n        out_channels: int,\n        hid_channel: int = 256,\n        in_strides: List[int] = None,\n        dropout: float = 0.0,\n        num_upsample_blocks: int = 0,\n        upsample_scale: int = 1,\n        interpolation_mode: str = ""bilinear"",\n        align_corners: bool = True,\n    ):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        super().__init__(in_channels, out_channels, in_strides)\n        self.upsample_scale = upsample_scale\n        self.interpolation_mode = interpolation_mode\n        self.align_corners = align_corners\n\n        segmentation_blocks = []\n        for i, in_channels_ in enumerate(in_channels):\n            if in_strides is not None:\n                i = (\n                    np.log2(in_strides[i])\n                    - num_upsample_blocks\n                    - np.log2(upsample_scale)\n                )\n            segmentation_blocks.append(\n                SegmentationBlock(\n                    in_channels=in_channels_,\n                    out_channels=hid_channel,\n                    num_upsamples=int(i),\n                )\n            )\n        self.segmentation_blocks = nn.ModuleList(segmentation_blocks)\n\n        additional_layers = [\n            EncoderUpsampleBlock(hid_channel, hid_channel)\n        ] * num_upsample_blocks\n        if dropout > 0:\n            additional_layers.append(nn.Dropout2d(p=dropout, inplace=True))\n        self.head = nn.Sequential(\n            *additional_layers, nn.Conv2d(hid_channel, out_channels, 1)\n        )\n\n    def forward(self, x: List[torch.Tensor]) -> torch.Tensor:\n        """"""Forward call.""""""\n        x = list(\n            map(\n                lambda block, features: block(features),\n                self.segmentation_blocks,\n                x,\n            )\n        )\n        x = sum(x)\n        x = self.head(x)\n        if self.upsample_scale > 1:\n            x = F.interpolate(\n                x,\n                scale_factor=self.upsample_scale,\n                mode=self.interpolation_mode,\n                align_corners=self.align_corners,\n            )\n        return x\n'"
catalyst/contrib/models/cv/segmentation/head/unet.py,2,"b'from typing import List\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nfrom ..blocks import EncoderUpsampleBlock\nfrom .core import HeadSpec\n\n\nclass UnetHead(HeadSpec):\n    """"""@TODO: Docs. Contribution is welcome.""""""\n\n    def __init__(\n        self,\n        in_channels: List[int],\n        out_channels: int,\n        in_strides: List[int] = None,\n        dropout: float = 0.0,\n        num_upsample_blocks: int = 0,\n        upsample_scale: int = 1,\n        interpolation_mode: str = ""bilinear"",\n        align_corners: bool = True,\n    ):\n        """"""@TODO: Docs. Contribution is welcome.""""""\n        super().__init__(in_channels, out_channels, in_strides)\n        self.upsample_scale = upsample_scale\n        self.interpolation_mode = interpolation_mode\n        self.align_corners = align_corners\n\n        in_channels_ = in_channels[-1]\n        additional_layers = [\n            EncoderUpsampleBlock(in_channels_, in_channels_)\n        ] * num_upsample_blocks\n        if dropout > 0:\n            additional_layers.append(nn.Dropout2d(p=dropout, inplace=True))\n        self.head = nn.Sequential(\n            *additional_layers, nn.Conv2d(in_channels_, out_channels, 1)\n        )\n\n    def forward(self, x: List[torch.Tensor]) -> torch.Tensor:\n        """"""Forward call.""""""\n        x_ = x[-1]\n        x = self.head(x_)\n        if self.upsample_scale > 1:\n            x = F.interpolate(\n                x,\n                scale_factor=self.upsample_scale,\n                mode=self.interpolation_mode,\n                align_corners=self.align_corners,\n            )\n        return x\n'"
