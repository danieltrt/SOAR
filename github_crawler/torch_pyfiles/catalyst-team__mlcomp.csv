file_path,api_count,code
setup.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n# Note: To use the \'upload\' functionality of this file, you must:\n#   $ pip install twine\n\n# Based on https://github.com/catalyst-team/catalyst/blob/master/setup.py\n\nimport io\nimport os\nimport sys\nfrom glob import glob\nfrom shutil import rmtree\n\nfrom setuptools import find_packages, setup, Command\n\n# Package meta-data.\nNAME = \'mlcomp\'\nDESCRIPTION = \'Machine learning pipelines. \' \\\n              \'Especially, for competitions, like Kaggle\'\nURL = \'https://github.com/catalyst-team/mlcomp\'\nEMAIL = \'lightsanweb@gmail.com\'\nAUTHOR = \'Evgeny Semyonov\'\nREQUIRES_PYTHON = \'>=3.6.0\'\n\nPROJECT_ROOT = os.path.abspath(os.path.dirname(__file__))\n\n\ndef load_requirements():\n    with open(os.path.join(PROJECT_ROOT, \'requirements.txt\'), \'r\') as f:\n        return f.read()\n\n\ndef load_readme():\n    readme_path = os.path.join(PROJECT_ROOT, \'README.md\')\n    with io.open(readme_path, encoding=\'utf-8\') as f:\n        return \'\\n\' + f.read()\n\n\ndef load_version():\n    context = {}\n    with open(os.path.join(PROJECT_ROOT, \'mlcomp\', \'__version__.py\')) as f:\n        exec(f.read(), context)\n    return context[\'__version__\']\n\n\ndef files(directory):\n    objs = glob(os.path.join(directory, \'**\'), recursive=True)\n    folders = [o for o in objs if os.path.isdir(o)]\n    for folder in folders:\n        if \'__pycache__\' in folder:\n            continue\n\n        folder_files = [\n            os.path.join(folder, f) for f in os.listdir(folder)\n            if os.path.isfile(os.path.join(folder, f))\n        ]\n        yield folder, folder_files\n\n\ndef get_data_files():\n    res = []\n\n    front_folder = \'mlcomp/server/front/dist\'\n    front_readme = \'https://github.com/catalyst-team/\' \\\n                   \'mlcomp/blob/master/mlcomp/server/front/README.md\'\n\n    if not os.path.exists(front_folder):\n        raise Exception(f\'There is no {front_folder}. \'\n                        f\'You should compile front-end files first\'\n                        f\' Please check {front_readme}\')\n    if len(os.listdir(front_folder)) == 0:\n        raise Exception(f\'There are no files in {front_folder}. \'\n                        f\'You should compile front-end files first\'\n                        f\' Please check {front_readme}\')\n\n    res.extend(files(\'mlcomp/utils\'))\n    res.extend(files(\'mlcomp/bin\'))\n    res.extend(files(\'mlcomp/docker\'))\n    res.extend(files(front_folder))\n    res.extend(files(\'mlcomp/migration\'))\n    return res\n\n\nclass UploadCommand(Command):\n    """"""Support setup.py upload.""""""\n\n    description = \'Build and publish the package.\'\n    user_options = []\n\n    @staticmethod\n    def status(s):\n        """"""Prints things in bold.""""""\n        print(\'\\033[1m{0}\\033[0m\'.format(s))\n\n    def initialize_options(self):\n        pass\n\n    def finalize_options(self):\n        pass\n\n    def run(self):\n        try:\n            self.status(\'Removing previous builds\xe2\x80\xa6\')\n            rmtree(os.path.join(PROJECT_ROOT, \'dist\'))\n        except OSError:\n            pass\n\n        self.status(\'Building Source and Wheel (universal) distribution\xe2\x80\xa6\')\n        os.system(\n            \'{0} setup.py sdist bdist_wheel --universal\'.format(\n                sys.executable\n            )\n        )\n\n        self.status(\'Uploading the package to PyPI via Twine\xe2\x80\xa6\')\n        os.system(\'python -m twine upload dist/*\')\n\n        self.status(\'Pushing git tags\xe2\x80\xa6\')\n        os.system(\'git tag v{0}\'.format(load_version()))\n        os.system(\'git push --tags\')\n\n        sys.exit()\n\n\nsetup(\n    name=NAME,\n    version=load_version(),\n    description=DESCRIPTION,\n    long_description=load_readme(),\n    long_description_content_type=\'text/markdown\',\n    author=AUTHOR,\n    author_email=EMAIL,\n    python_requires=REQUIRES_PYTHON,\n    url=URL,\n    packages=find_packages(exclude=(\'tests\',)),\n    install_requires=load_requirements(),\n    include_package_data=True,\n    data_files=get_data_files(),\n    zip_safe=False,\n    license=\'MIT\',\n    classifiers=[\n        \'License :: OSI Approved :: MIT License\',\n        \'Intended Audience :: Developers\',\n        \'Intended Audience :: Science/Research\',\n        \'Programming Language :: Python\',\n        \'Programming Language :: Python :: 3.6\',\n        \'Programming Language :: Python :: 3.7\',\n        \'Programming Language :: Python :: Implementation :: CPython\',\n        \'Topic :: Scientific/Engineering :: Artificial Intelligence\'\n    ],\n    # $ setup.py publish support.\n    cmdclass={\n        \'upload\': UploadCommand\n    },\n    entry_points={\n        \'console_scripts\': [\n            \'mlcomp-server = mlcomp.server.__main__:main\',\n            \'mlcomp-worker = mlcomp.worker.__main__:main\',\n            \'mlcomp-contrib = mlcomp.contrib.__main__:main\',\n            \'mlcomp = mlcomp.__main__:main\',\n        ],\n    }\n)\n'"
docs/conf.py,0,"b'# -*- coding: utf-8 -*-\n#\n# Configuration file for the Sphinx documentation builder.\n#\n# This file does only contain a selection of the most common options. For a\n# full list see the documentation:\n# http://www.sphinx-doc.org/en/master/config\n\n# -- Path setup --------------------------------------------------------------\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\n\n\nimport os\nimport datetime\nimport sys\nimport re\n\nroot_path = ""../""\nsys.path.insert(0, os.path.abspath(root_path))\n\n# -- Project information -----------------------------------------------------\n\nproject = ""MLComp""\ncopyright = ""{}, Evgeny Semyonov"".format(datetime.datetime.now().year)\nauthor = ""Evgeny Semyonov""\n\ndocs_repo = ""mlcomp""\ndocs_user = ""catalyst-team""\n\nreleases_github_path = ""catalyst-team/mlcomp""\n\n\ndef get_version(mode: str = ""full"") -> str:\n    current_dir = os.path.abspath(os.path.dirname(__file__))\n    root = os.path.dirname(current_dir)\n    version_file = os.path.join(root, ""mlcomp"", ""__version__.py"")\n    if not os.path.exists(version_file):\n        version_file = os.path.join(root, ""__version__.py"")\n\n    version_ = ""1.0""\n    try:\n        with open(version_file) as f:\n            version_ = re.search(\n                r\'^__version__ = [\\\'""]([^\\\'""]*)[\\\'""]\', f.read(), re.M\n            ).group(1)\n    except Exception:\n        pass\n\n    if mode == ""short"":\n        try:\n            version_ = re.search(r""^(\\d+\\.\\d+.?\\d+?)"", version_, re.M).group(1)\n        except Exception:\n            pass\n\n    return version_\n\n\n# The short X.Y version\nversion = get_version(""short"")\n# The full version, including alpha/beta/rc tags\nrelease = get_version(""full"")\n\n# -- General configuration ---------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = ""1.0""\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named ""sphinx.ext.*"") or your custom\n# ones.\nextensions = [\n    ""sphinx.ext.autodoc"",\n    ""sphinx.ext.todo"",\n    ""sphinx.ext.coverage"",\n    ""sphinx.ext.mathjax"",\n    ""sphinx.ext.viewcode"",\n    ""sphinx.ext.githubpages"",\n    ""sphinx.ext.napoleon""\n]\n\nautodoc_inherit_docstrings = False\n\nnapoleon_google_docstring = True\nnapoleon_include_init_with_doc = True\nnapoleon_numpy_docstring = False\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [""_templates""]\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\n# source_suffix = ["".rst"", "".md""]\nsource_suffix = "".rst""\n\n# The master toctree document.\nmaster_doc = ""index""\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This pattern also affects html_static_path and html_extra_path.\nexclude_patterns = [""_build"", ""Thumbs.db"", "".DS_Store""]\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = None\n\n# -- Options for HTML output -------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = ""alabaster""\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#\n# html_theme_options = {\n#     ""display_version"": True,\n#     ""prev_next_buttons_location"": ""bottom"",\n#     ""collapse_navigation"": True,\n#     ""sticky_navigation"": True\n# }\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\n# html_static_path = [""_static""]\n\nhtml_short_title = ""MLComp""\n\n# Custom sidebar templates, must be a dictionary that maps document names\n# to template names.\n#\n# The default sidebars (for documents that don""t match any pattern) are\n# defined by theme itself.  Builtin themes are using these templates by\n# default: ``[""localtoc.html"", ""relations.html"", ""sourcelink.html"",\n# ""searchbox.html""]``.\n#\n# html_sidebars = {}\n\nhtml_context = {\n    ""display_github"": True,\n    ""source_url_prefix"": (\n        f""https://github.com/{docs_user}/{docs_repo}/tree/master/docs""\n    ),\n    ""github_host"": ""github.com"",\n    ""github_user"": docs_user,\n    ""github_repo"": docs_repo,\n    ""github_version"": ""master"",\n    ""conf_py_path"": ""/docs/"",\n    ""source_suffix"": "".rst""\n}\n\n# -- Options for HTMLHelp output ---------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = ""MLCompdoc""\n\n# -- Options for LaTeX output ------------------------------------------------\n\nlatex_elements = {\n    # The paper size (""letterpaper"" or ""a4paper"").\n    #\n    # ""papersize"": ""letterpaper"",\n\n    # The font size (""10pt"", ""11pt"" or ""12pt"").\n    #\n    # ""pointsize"": ""10pt"",\n\n    # Additional stuff for the LaTeX preamble.\n    #\n    # ""preamble"": """",\n\n    # Latex figure (float) alignment\n    #\n    # ""figure_align"": ""htbp"",\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (\n        master_doc, ""MLComp.tex"", ""MLComp Documentation"", ""lightforever"",\n        ""manual""\n    ),\n]\n\n# -- Options for manual page output ------------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [(master_doc, ""mlcomp"", ""MLComp Documentation"", [author], 1)]\n\n# -- Options for Texinfo output ----------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (\n        master_doc, ""MLComp"", ""MLComp Documentation"", author, ""MLComp"",\n        ""One line description of project."", ""Miscellaneous""\n    ),\n]\n\n# -- Options for Epub output -------------------------------------------------\n\n# Bibliographic Dublin Core info.\nepub_title = project\n\n# The unique identifier of the text. This can be a ISBN number\n# or the project homepage.\n#\n# epub_identifier = """"\n\n# A unique identification for the text.\n#\n# epub_uid = """"\n\n# A list of files that should not be packed into the epub file.\nepub_exclude_files = [""search.html""]\n\n# -- Extension configuration -------------------------------------------------\n\n# -- Options for todo extension ----------------------------------------------\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = True\n'"
examples/__init__.py,0,b''
mlcomp/__init__.py,0,"b'import os\nfrom os.path import join\nimport shutil\n\nfrom .__version__ import __version__  # noqa: F401\n\nROOT_FOLDER = os.path.abspath(\n    os.path.expanduser(os.getenv(\'ROOT_FOLDER\', \'~/mlcomp\')))\n\ntest_worker = os.getenv(\'PYTEST_XDIST_WORKER\')\nif test_worker:\n    ROOT_FOLDER = join(ROOT_FOLDER, \'tests\', test_worker)\n    shutil.rmtree(ROOT_FOLDER, ignore_errors=True)\n\nDATA_FOLDER = join(ROOT_FOLDER, \'data\')\nMODEL_FOLDER = join(ROOT_FOLDER, \'models\')\nTASK_FOLDER = join(ROOT_FOLDER, \'tasks\')\nLOG_FOLDER = join(ROOT_FOLDER, \'logs\')\nCONFIG_FOLDER = join(ROOT_FOLDER, \'configs\')\nDB_FOLDER = join(ROOT_FOLDER, \'db\')\nREPORT_FOLDER = join(ROOT_FOLDER, \'report\')\nTMP_FOLDER = join(ROOT_FOLDER, \'tmp\')\n\nos.makedirs(ROOT_FOLDER, exist_ok=True)\nos.makedirs(DATA_FOLDER, exist_ok=True)\nos.makedirs(MODEL_FOLDER, exist_ok=True)\nos.makedirs(TASK_FOLDER, exist_ok=True)\nos.makedirs(LOG_FOLDER, exist_ok=True)\nos.makedirs(CONFIG_FOLDER, exist_ok=True)\nos.makedirs(DB_FOLDER, exist_ok=True)\nos.makedirs(REPORT_FOLDER, exist_ok=True)\nos.makedirs(TMP_FOLDER, exist_ok=True)\n\n# copy conf files if they do not exist\n\nfolder = os.path.dirname(__file__)\ndocker_folder = join(folder, \'docker\')\n\nfor name in os.listdir(docker_folder):\n    file = join(docker_folder, name)\n    target_file = join(CONFIG_FOLDER, name)\n    if not os.path.exists(target_file):\n        shutil.copy(file, target_file)\n\n# exporting environment variables\nenv_file = join(CONFIG_FOLDER, \'.env\')\n\nwith open(env_file) as f:\n    for l in f.readlines():\n        k, v = l.strip().split(\'=\')\n        os.environ[k] = v\n\n# extra env\nif os.getenv(\'ENV\'):\n    extra_env_file = join(CONFIG_FOLDER, os.getenv(\'ENV\', \'\')+\'.env\')\n    extra_env = open(extra_env_file).readlines()\n    for p in extra_env:\n        if \'=\' not in p:\n            continue\n        k, v = p.strip().split(\'=\')\n        os.environ[k] = v\n\n# contour env file\nCONTOUR_FILE = join(CONFIG_FOLDER, \'contour.yml\')\n\n# for debugging\nos.environ[\'PYTHONPATH\'] = \'.\'\n\nMASTER_PORT_RANGE = list(map(int, os.getenv(\'MASTER_PORT_RANGE\').split(\'-\')))\nFILE_SYNC_INTERVAL = int(os.getenv(\'FILE_SYNC_INTERVAL\', \'0\'))\nWORKER_USAGE_INTERVAL = int(os.getenv(\'WORKER_USAGE_INTERVAL\', \'10\'))\nINSTALL_DEPENDENCIES = os.getenv(\'INSTALL_DEPENDENCIES\') == \'True\'\n\nREDIS_HOST = os.getenv(\'REDIS_HOST\')\nREDIS_PASSWORD = os.getenv(\'REDIS_PASSWORD\')\nREDIS_PORT = os.getenv(\'REDIS_PORT\')\n\nTOKEN = os.getenv(\'TOKEN\')\nDOCKER_IMG = os.getenv(\'DOCKER_IMG\', \'default\')\nWEB_HOST = os.getenv(\'WEB_HOST\')\nWEB_PORT = int(os.getenv(\'WEB_PORT\'))\nWORKER_INDEX = os.getenv(\'WORKER_INDEX\', -1)\n\nCONSOLE_LOG_LEVEL = os.getenv(\'CONSOLE_LOG_LEVEL\', \'DEBUG\')\nDB_LOG_LEVEL = os.getenv(\'DB_LOG_LEVEL\', \'DEBUG\')\nFILE_LOG_LEVEL = os.getenv(\'FILE_LOG_LEVEL\', \'INFO\')\nLOG_NAME = os.getenv(\'LOG_NAME\', \'log\')\nSYNC_WITH_THIS_COMPUTER = os.getenv(\'SYNC_WITH_THIS_COMPUTER\') == \'True\'\nCAN_PROCESS_TASKS = os.getenv(\'CAN_PROCESS_TASKS\') == \'True\'\n\nDB_TYPE = os.getenv(\'DB_TYPE\')\nif DB_TYPE == \'POSTGRESQL\':\n    DATABASE = {\n        \'dbname\': os.getenv(\'POSTGRES_DB\'),\n        \'user\': os.getenv(\'POSTGRES_USER\'),\n        \'password\': os.getenv(\'POSTGRES_PASSWORD\'),\n        \'host\': os.getenv(\'POSTGRES_HOST\'),\n        \'port\': int(os.getenv(\'POSTGRES_PORT\')),\n    }\n\n    SA_CONNECTION_STRING = f""postgresql+psycopg2://{DATABASE[\'user\']}:"" \\\n                           f""{DATABASE[\'password\']}@{DATABASE[\'host\']}:"" \\\n                           f""{DATABASE[\'port\']}/{DATABASE[\'dbname\']}""\nelif DB_TYPE == \'SQLITE\':\n    SA_CONNECTION_STRING = f\'sqlite:///{DB_FOLDER}/sqlite3.sqlite\'\nelse:\n    raise Exception(f\'Unknown DB_TYPE = {DB_TYPE}\')\n\nFLASK_ENV = os.getenv(\'FLASK_ENV\')\nDOCKER_MAIN = os.getenv(\'DOCKER_MAIN\', \'True\') == \'True\'\n\nIP = os.getenv(\'IP\')\nPORT = int(os.getenv(\'PORT\'))\n\n__all__ = [\n    \'ROOT_FOLDER\', \'DATA_FOLDER\', \'MODEL_FOLDER\', \'TASK_FOLDER\', \'LOG_FOLDER\',\n    \'CONFIG_FOLDER\', \'DB_FOLDER\', \'MASTER_PORT_RANGE\', \'REDIS_HOST\',\n    \'REDIS_PASSWORD\', \'REDIS_PORT\', \'TOKEN\', \'DOCKER_IMG\', \'WEB_HOST\',\n    \'WEB_PORT\', \'WORKER_INDEX\', \'CONSOLE_LOG_LEVEL\', \'DB_LOG_LEVEL\',\n    \'FILE_LOG_LEVEL\', \'DB_TYPE\', \'SA_CONNECTION_STRING\', \'FLASK_ENV\',\n    \'DOCKER_MAIN\', \'IP\', \'PORT\', \'LOG_NAME\', \'WORKER_USAGE_INTERVAL\',\n    \'FILE_SYNC_INTERVAL\', \'INSTALL_DEPENDENCIES\', \'SYNC_WITH_THIS_COMPUTER\',\n    \'CAN_PROCESS_TASKS\', \'TMP_FOLDER\', \'CONTOUR_FILE\', \'REPORT_FOLDER\'\n]\n'"
mlcomp/__main__.py,2,"b'from os.path import join, exists\nfrom typing import Tuple\n\nimport click\nimport socket\nfrom multiprocessing import cpu_count\n\nimport torch\n\nfrom mlcomp.utils.io import yaml_load, yaml_dump\nfrom mlcomp.contrib.search.grid import grid_cells\nfrom mlcomp.migration.manage import migrate as _migrate\nfrom mlcomp import ROOT_FOLDER, IP, PORT, \\\n    WORKER_INDEX, SYNC_WITH_THIS_COMPUTER, CAN_PROCESS_TASKS, CONFIG_FOLDER, \\\n    DOCKER_IMG, MASTER_PORT_RANGE\nfrom mlcomp.db.core import Session\nfrom mlcomp.db.enums import DagType, ComponentType, TaskStatus\nfrom mlcomp.db.models import Computer, Docker\nfrom mlcomp.db.providers import \\\n    ComputerProvider, \\\n    TaskProvider, \\\n    StepProvider, \\\n    ProjectProvider, DockerProvider\nfrom mlcomp.report import create_report, check_statuses\nfrom mlcomp.utils.config import merge_dicts_smart, dict_from_list_str\nfrom mlcomp.utils.logging import create_logger\nfrom mlcomp.worker.executors.kaggle import Submit\nfrom mlcomp.worker.sync import sync_directed, correct_folders\nfrom mlcomp.worker.tasks import execute_by_id\nfrom mlcomp.utils.misc import memory, disk, get_username, \\\n    get_default_network_interface, now\nfrom mlcomp.server.back.create_dags import dag_standard, dag_pipe\n\n_session = Session.create_session(key=__name__)\n\n\ndef _dag(config: str, debug: bool = False, control_reqs=True,\n         params: Tuple[str] = ()):\n    logger = create_logger(_session, name=\'_dag\')\n    logger.info(\'started\', ComponentType.Client)\n\n    config_text = open(config, \'r\').read()\n    config_parsed = yaml_load(config_text)\n    params = dict_from_list_str(params)\n    config_parsed = merge_dicts_smart(config_parsed, params)\n    config_text = yaml_dump(config_parsed)\n\n    logger.info(\'config parsed\', ComponentType.Client)\n\n    type_name = config_parsed[\'info\'].get(\'type\', \'standard\')\n    if type_name == DagType.Standard.name.lower():\n        cells = grid_cells(\n            config_parsed[\'grid\']) if \'grid\' in config_parsed else [None]\n        dags = []\n        for cell in cells:\n            dag = dag_standard(\n                session=_session,\n                config=config_parsed,\n                debug=debug,\n                config_text=config_text,\n                config_path=config,\n                control_reqs=control_reqs,\n                logger=logger,\n                component=ComponentType.Client,\n                grid_cell=cell\n            )\n            dags.append(dag)\n\n        return dags\n\n    return [\n        dag_pipe(\n            session=_session, config=config_parsed, config_text=config_text\n        )\n    ]\n\n\ndef _create_computer():\n    tot_m, used_m, free_m = memory()\n    tot_d, used_d, free_d = disk(ROOT_FOLDER)\n    computer = Computer(\n        name=socket.gethostname(),\n        gpu=torch.cuda.device_count(),\n        cpu=cpu_count(),\n        memory=tot_m,\n        ip=IP,\n        port=PORT,\n        user=get_username(),\n        disk=tot_d,\n        root_folder=ROOT_FOLDER,\n        sync_with_this_computer=SYNC_WITH_THIS_COMPUTER,\n        can_process_tasks=CAN_PROCESS_TASKS\n    )\n    ComputerProvider(_session).create_or_update(computer, \'name\')\n\n\ndef _create_docker():\n    docker = Docker(\n        name=DOCKER_IMG,\n        computer=socket.gethostname(),\n        ports=\'-\'.join(list(map(str, MASTER_PORT_RANGE))),\n        last_activity=now()\n    )\n    DockerProvider(_session).create_or_update(docker, \'name\', \'computer\')\n\n\n@click.group()\ndef main():\n    pass\n\n\n@main.command()\ndef migrate():\n    _migrate()\n\n\n@main.command()\n@click.argument(\'config\')\n@click.option(\'--control_reqs\', type=bool, default=True)\n@click.option(\'--params\', multiple=True)\ndef dag(config: str, control_reqs: bool, params):\n    check_statuses()\n    _dag(config, control_reqs=control_reqs, params=params)\n\n\n@main.command()\ndef report():\n    create_report()\n\n\n@main.command()\n@click.argument(\'config\')\n@click.option(\'--debug\', type=bool, default=True)\n@click.option(\'--params\', multiple=True)\ndef execute(config: str, debug: bool, params):\n    check_statuses()\n\n    _create_computer()\n    _create_docker()\n\n    # Fail all InProgress Tasks\n    logger = create_logger(_session, __name__)\n\n    provider = TaskProvider(_session)\n    step_provider = StepProvider(_session)\n\n    for t in provider.by_status(\n            TaskStatus.InProgress, worker_index=WORKER_INDEX\n    ):\n        step = step_provider.last_for_task(t.id)\n        logger.error(\n            f\'Task Id = {t.id} was in InProgress state \'\n            f\'when another tasks arrived to the same worker\',\n            ComponentType.Worker, t.computer_assigned, t.id, step\n        )\n        provider.change_status(t, TaskStatus.Failed)\n\n    # Create dags\n    dags = _dag(config, debug, params=params)\n    for dag in dags:\n        for ids in dag.values():\n            for id in ids:\n                task = provider.by_id(id)\n                task.gpu_assigned = \',\'.join(\n                    [str(i) for i in range(torch.cuda.device_count())])\n\n                provider.commit()\n                execute_by_id(id, exit=False)\n\n\n@main.command()\n@click.argument(\'project\')\n@click.option(\'--computer\', help=\'sync computer with all the others\')\n@click.option(\n    \'--only_from\',\n    is_flag=True,\n    help=\'only copy files from the computer to all the others\'\n)\n@click.option(\n    \'--only_to\',\n    is_flag=True,\n    help=\'only copy files from all the others to the computer\'\n)\n@click.option(\n    \'--online\',\n    is_flag=True,\n    help=\'sync with only online computers\'\n)\ndef sync(project: str, computer: str, only_from: bool, only_to: bool,\n         online: bool):\n    """"""\n    Syncs specified project on this computer with other computers\n    """"""\n    check_statuses()\n\n    _create_computer()\n    _create_docker()\n\n    computer = computer or socket.gethostname()\n    provider = ComputerProvider(_session)\n    project_provider = ProjectProvider(_session)\n    computer = provider.by_name(computer)\n    computers = provider.all_with_last_activtiy()\n    p = project_provider.by_name(project)\n    assert p, f\'Project={project} is not found\'\n\n    sync_folders = yaml_load(p.sync_folders)\n    ignore_folders = yaml_load(p.ignore_folders)\n\n    sync_folders = correct_folders(sync_folders, p.name)\n    ignore_folders = correct_folders(ignore_folders, p.name)\n\n    if not isinstance(sync_folders, list):\n        sync_folders = []\n    if not isinstance(ignore_folders, list):\n        ignore_folders = []\n\n    folders = [[s, ignore_folders] for s in sync_folders]\n\n    for c in computers:\n        if c.name != computer.name:\n            if online and (now() - c.last_activity).total_seconds() > 100:\n                continue\n\n            if not only_from:\n                sync_directed(_session, computer, c, folders)\n            if not only_to:\n                sync_directed(_session, c, computer, folders)\n\n\n@main.command()\ndef init():\n    env_path = join(CONFIG_FOLDER, \'.env\')\n    lines = open(env_path).readlines()\n    for i in range(len(lines)):\n        if \'NCCL_SOCKET_IFNAME\' in lines[i]:\n            interface = get_default_network_interface()\n            if interface:\n                lines[i] = f\'NCCL_SOCKET_IFNAME={interface}\\n\'\n    open(env_path, \'w\').writelines(lines)\n\n\n@main.command()\ndef submit():\n    assert exists(\'submit.yml\'), \'no file submit.yml\'\n\n    data = yaml_load(file=\'submit.yml\')\n    submit = Submit(\n        competition=data[\'competition\'],\n        submit_type=\'kernel\',\n        max_size=data.get(\'max_size\', 1),\n        folders=data.get(\'folders\', []),\n        datasets=data.get(\'datasets\', []),\n        files=data.get(\'files\', [])\n    )\n    submit.work()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
mlcomp/__version__.py,0,"b""__version__ = '20.3.1.1'\n"""
mlcomp/report.py,0,"b'import shutil\nimport traceback\nfrom glob import glob\nfrom os import makedirs\nfrom os.path import dirname, join, basename, exists\n\nimport pandas as pd\nimport migrate.versioning.api as api\n\nfrom mlcomp.worker.tasks import kill\nfrom mlcomp.utils.io import zip_folder\nfrom mlcomp.db.enums import LogStatus, ComponentType\nfrom mlcomp import SA_CONNECTION_STRING, REPORT_FOLDER, LOG_FOLDER, DB_TYPE, \\\n    CONFIG_FOLDER, ROOT_FOLDER, DATA_FOLDER, MODEL_FOLDER, TASK_FOLDER, \\\n    DB_FOLDER, TMP_FOLDER\nfrom mlcomp.utils.misc import now, to_snake\nfrom mlcomp.db.providers import DagProvider, LogProvider\nfrom mlcomp.worker.app import app as celery\n\n\ndef statuses(folder: str = None):\n    rows = []\n\n    folder_status = \'OK\'\n    folder_comment = \'\'\n\n    folders = [\n        ROOT_FOLDER,\n        DATA_FOLDER,\n        MODEL_FOLDER,\n        TASK_FOLDER,\n        LOG_FOLDER,\n        CONFIG_FOLDER,\n        DB_FOLDER,\n        REPORT_FOLDER,\n        TMP_FOLDER\n    ]\n    for f in folders:\n        if not exists(f):\n            folder_status = \'ERROR\'\n            folder_comment = f\'folder {f} does not exist\'\n\n    files = [\n        join(CONFIG_FOLDER, \'.env\')\n    ]\n    for f in files:\n        if not exists(f):\n            folder_status = \'ERROR\'\n            folder_comment = f\'file {f} does not exist\'\n\n    rows.append({\n        \'name\': \'Folders\',\n        \'status\': folder_status,\n        \'comment\': folder_comment\n    })\n\n    database_status = \'OK\'\n    database_comment = f\'DB_TYPE = {DB_TYPE}\'\n    try:\n        provider = DagProvider()\n        provider.count()\n    except Exception:\n        database_status = \'ERROR\'\n        database_comment += \' \' + traceback.format_exc()\n\n    rows.append({\n        \'name\': \'Database\',\n        \'status\': database_status,\n        \'comment\': database_comment\n    })\n\n    redis_status = \'OK\'\n    redis_comment = f\'\'\n    try:\n        celery.backend.client.echo(1)\n    except Exception:\n        redis_status = \'ERROR\'\n        redis_comment += \' \' + traceback.format_exc()\n\n    rows.append({\n        \'name\': \'Redis\',\n        \'status\': redis_status,\n        \'comment\': redis_comment\n    })\n\n    if database_status == \'OK\':\n        migrate_status = \'OK\'\n\n        repository_folder = join(dirname(__file__), \'migration\')\n        repository_version = api.version(repository_folder)\n\n        db_version = api.db_version(SA_CONNECTION_STRING, repository_folder)\n\n        if db_version != repository_version:\n            migrate_status = \'ERROR\'\n            migrate_comment = f\'Repository version = {repository_version} \' \\\n                              f\'Db version = {db_version}\'\n        else:\n            migrate_comment = f\'version: {db_version}\'\n\n        rows.append({\n            \'name\': \'Migrate\',\n            \'status\': migrate_status,\n            \'comment\': migrate_comment\n        })\n\n    df = pd.DataFrame(rows)\n\n    if folder is not None:\n        print(\'Statuses:\')\n        print(df)\n\n        df.to_csv(join(folder, \'statuses.csv\'), index=False)\n\n    return df\n\n\ndef check_statuses():\n    stats = statuses()\n    failed = stats[stats[\'status\'] != \'OK\']\n    if failed.shape[0] > 0:\n        print(\'There are errors in statuses\')\n        for row in stats.itertuples():\n            print(f\'name: {row.name} status\'\n                  f\' {row.status} comment {row.comment}\')\n\n        import time\n        time.sleep(0.01)\n\n        raise Exception(\'There are errors in statuses. \'\n                        \'Please check them above\')\n\n\ndef logs(statuses, folder: str = None):\n    if folder is not None:\n        for file in glob(join(LOG_FOLDER, \'*\')):\n            shutil.copy(file, join(folder, basename(file)))\n        print(\'logs formed\')\n\n    if statuses.query(\'status == ""ERROR""\').shape[0] > 0:\n        return\n\n    log_provider = LogProvider()\n    errors = log_provider.last(count=1000, levels=[LogStatus.Error.value])\n    service_components = [ComponentType.Supervisor.value,\n                          ComponentType.API.value,\n                          ComponentType.WorkerSupervisor.value]\n    services = log_provider.last(count=1000, components=service_components)\n    logs = errors + services\n\n    rows = []\n    for l, _ in logs:\n        rows.append({\n            \'status\': to_snake(LogStatus(l.level).name),\n            \'component\': to_snake(ComponentType(l.component).name),\n            \'time\': l.time,\n            \'message\': l.message,\n        })\n    df = pd.DataFrame(rows)\n    df.to_csv(join(folder, \'logs_db.csv\'), index=False)\n    return df\n\n\ndef create_report():\n    print(\'*** Report Start ***\')\n    print()\n\n    folder = join(REPORT_FOLDER, f\'{now()}\'.split(\'.\')[0])\n    makedirs(folder, exist_ok=True)\n\n    statuses_res = statuses(folder)\n\n    print()\n\n    logs(statuses_res, folder)\n\n    print()\n\n    zip_path = folder + \'.zip\'\n    zip_folder(folder, dst=zip_path)\n\n    print(\'Report path\', zip_path)\n    print()\n\n    print(\'*** Report End ***\')\n\n\n__all__ = [\'check_statuses\', \'create_report\']\n'"
examples/cifar_simple/__init__.py,0,b'# flake8: noqa\nfrom catalyst.dl import SupervisedRunner as Runner\nfrom experiment import Experiment\nfrom model import Net\n'
examples/cifar_simple/experiment.py,0,"b""from collections import OrderedDict\n\nimport numpy as np\n\nimport torchvision\nfrom torchvision import transforms\n\nfrom catalyst.dl import ConfigExperiment\n\n\nclass Experiment(ConfigExperiment):\n    @staticmethod\n    def get_transforms(stage: str = None, mode: str = None):\n        return torchvision.transforms.Compose(\n            [\n                transforms.ToTensor(),\n                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n            ]\n        )\n\n    def denormilize(self, img: np.array):\n        # ``input[channel] = (input[channel] - mean[channel]) / std[channel]``\n\n        res = np.zeros((img.shape[1], img.shape[2], 3), dtype=np.uint8)\n        for i in range(res.shape[2]):\n            res[:, :, i] = (img[i] * 0.5 + 0.5) * 255\n        return res\n\n    def get_datasets(self, stage: str, **kwargs):\n        datasets = OrderedDict()\n\n        trainset = torchvision.datasets.CIFAR10(\n            root='./data',\n            train=True,\n            download=True,\n            transform=Experiment.get_transforms(stage=stage, mode='train')\n        )\n        testset = torchvision.datasets.CIFAR10(\n            root='./data',\n            train=False,\n            download=True,\n            transform=Experiment.get_transforms(stage=stage, mode='valid')\n        )\n\n        datasets['train'] = trainset\n        datasets['valid'] = testset\n\n        return datasets\n"""
examples/cifar_simple/model.py,2,"b'import torch.nn as nn\nimport torch.nn.functional as F\n\nfrom catalyst.contrib import registry\n\n\n@registry.Model\nclass Net(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(-1, 16 * 5 * 5)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n'"
examples/click/main.py,0,"b""import click\nimport time\nfrom tqdm import tqdm\n\n\n@click.group()\ndef base():\n    pass\n\n\n@base.command()\n@click.option('--count', type=int, default=100)\ndef work(count: int):\n    print('start')\n    items = list(range(count))\n    bar = tqdm(items)\n    for item in bar:\n        bar.set_description(f'item={item}')\n        time.sleep(0.01)\n    print('end')\n\n\nif __name__ == '__main__':\n    base()\n"""
examples/digit-recognizer/__init__.py,0,b'# flake8: noqa\nfrom catalyst.dl import SupervisedRunner as Runner\nfrom experiment import Experiment\nfrom model import Net'
examples/digit-recognizer/dataset.py,1,"b""import pandas as pd\nimport numpy as np\n\nfrom torch.utils.data import Dataset\n\n\nclass MnistDataset(Dataset):\n    def __init__(\n        self,\n        file: str,\n        fold_csv: str = None,\n        fold_number: int = 0,\n        is_test: bool = False,\n        max_count: int = None,\n        transforms=None\n    ):\n        df = pd.read_csv(file)\n        if fold_csv is not None:\n            fold = pd.read_csv(fold_csv)\n            if is_test:\n                df = df[fold['fold'] == fold_number]\n            else:\n                df = df[fold['fold'] != fold_number]\n\n        if max_count:\n            df = df[:max_count]\n\n        if 'label' in df.columns:\n            self.y = df['label'].values.astype(np.int)\n            df = df.drop(columns='label', axis=0)\n        else:\n            self.y = None\n\n        self.x = df.values.reshape((-1, 1, 28, 28)).astype(np.float32)\n        self.transforms = transforms\n\n    def __len__(self):\n        return len(self.x)\n\n    def __getitem__(self, index):\n        x = self.x[index]\n        if self.transforms:\n            x = self.transforms(image=x)['image']\n        res = {'features': x}\n\n        if self.y is not None:\n            res['targets'] = self.y[index]\n        return res\n"""
examples/digit-recognizer/experiment.py,0,"b""from collections import OrderedDict\n\nimport albumentations as A\n\nfrom catalyst.dl import ConfigExperiment\n\nfrom dataset import MnistDataset\n\n\nclass Experiment(ConfigExperiment):\n    def get_datasets(self, stage: str, **kwargs):\n        datasets = OrderedDict()\n\n        train = MnistDataset(\n            'data/train.csv',\n            fold_csv='data/fold.csv',\n            transforms=Experiment.get_test_transforms()\n        )\n\n        valid = MnistDataset(\n            'data/train.csv',\n            fold_csv='data/fold.csv',\n            is_test=True,\n            transforms=Experiment.get_test_transforms()\n        )\n\n        datasets['train'] = train\n        datasets['valid'] = valid\n\n        return datasets\n\n    @staticmethod\n    def get_test_transforms():\n        return A.Compose([A.Normalize(mean=(0.485, ), std=(0.229, ))])\n"""
examples/digit-recognizer/model.py,2,"b'import torch.nn as nn\nimport torch.nn.functional as F\n\nfrom catalyst.contrib import registry\n\n\n@registry.Model\nclass Net(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n        self.fc1 = nn.Linear(4 * 4 * 50, 500)\n        self.fc2 = nn.Linear(500, 10)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.max_pool2d(x, 2, 2)\n        x = F.relu(self.conv2(x))\n        x = F.max_pool2d(x, 2, 2)\n        x = x.view(-1, 4 * 4 * 50)\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return F.log_softmax(x, dim=1)\n'"
examples/hierarchical_logging/executors.py,0,"b""from mlcomp.worker.executors import Executor\n\n\n@Executor.register\nclass Step(Executor):\n    def work(self):\n        self.step.start(1, 'step 1')\n\n        self.step.start(1, 'step 2')\n\n        self.step.start(2, 'step 2.1')\n\n        self.step.start(3, 'step 2.1.1')\n\n        self.step.start(3, 'step 2.1.2')\n\n        self.step.start(2, 'step 2.2')\n\n        self.step.end(0)\n"""
examples/progress_bar/executor.py,0,"b'import time\n\nfrom mlcomp.worker.executors import Executor\n\n\n@Executor.register\nclass Progress(Executor):\n    def work(self):\n        items = list(range(1000))\n        for i in self.tqdm(items, interval=1):\n            time.sleep(0.01)\n'"
mlcomp/contrib/__init__.py,0,b''
mlcomp/contrib/__main__.py,0,"b""import re\nimport os\nfrom os.path import join\nfrom uuid import uuid4\n\nimport click\nimport pandas as pd\n\nfrom mlcomp.contrib.scripts.split import file_group_kfold\n\ncurrent_folder = os.getcwd()\n\n\n@click.group()\ndef main():\n    pass\n\n\n@main.command()\n@click.argument('path')\n@click.option('--n_splits', type=int, default=5)\ndef split_pandas(path: str, n_splits: int):\n    output = join(current_folder, 'fold.csv')\n    df = pd.read_csv(path)\n    folds = file_group_kfold(n_splits,\n                             image=df[df.columns[0]]\n                             )\n    df['fold'] = folds['fold']\n    df.to_csv(output)\n\n\n@main.command()\n@click.argument('img_path')\n@click.option('--n_splits', type=int, default=5)\n@click.option('--group-regex')\ndef split_classify(img_path: str,\n                   n_splits: int,\n                   group_regex: str = None):\n    output = join(current_folder, 'fold.csv')\n    get_group = None\n    if group_regex:\n        pattern = re.compile(group_regex)\n\n        def get_group(x):\n            match = pattern.match(x)\n            if not match:\n                return str(uuid4())\n            return match.group(1)\n\n    images_labels = [(img, sub_folder)\n                     for sub_folder in os.listdir(img_path)\n                     for img in os.listdir(join(img_path, sub_folder))]\n\n    file_group_kfold(n_splits, output, get_group=get_group,\n                     image=[join(label, img) for img, label in images_labels],\n                     label=[label for img, label in images_labels]\n                     )\n\n\n@main.command()\n@click.argument('img_path')\n@click.argument('mask_path')\n@click.option('--n_splits', type=int, default=5)\n@click.option('--group-regex')\ndef split_segment(img_path: str,\n                  mask_path: str,\n                  n_splits: int,\n                  group_regex: str = None):\n    output = join(current_folder, 'fold.csv')\n    get_group = None\n    if group_regex:\n        pattern = re.compile(group_regex)\n\n        def get_group(x):\n            match = pattern.match(x)\n            if not match:\n                return str(uuid4())\n            return match.group(1)\n\n    file_group_kfold(n_splits, output, get_group=get_group,\n                     image=os.listdir(img_path),\n                     mask=os.listdir(mask_path),\n                     sort=True,\n                     must_equal=['image', 'mask']\n                     )\n\n\n@main.command()\n@click.argument('img_path')\ndef split_test_img(img_path: str):\n    output = join(current_folder, 'fold_test.csv')\n    df = pd.DataFrame({\n        'image': sorted(list(os.listdir(img_path))),\n        'fold': 0\n    })\n    df.to_csv(output, index=False)\n\n\nif __name__ == '__main__':\n    main()\n"""
mlcomp/db/__init__.py,0,b'# flake8: noqa\nfrom .signals import *'
mlcomp/db/enums.py,0,"b""from enum import Enum\nfrom mlcomp.utils.misc import to_snake\n\n\nclass OrderedEnum(Enum):\n    def __ge__(self, other):\n        if self.__class__ is other.__class__:\n            return self.value >= other.value\n        return NotImplemented\n\n    def __gt__(self, other):\n        if self.__class__ is other.__class__:\n            return self.value > other.value\n        return NotImplemented\n\n    def __le__(self, other):\n        if self.__class__ is other.__class__:\n            return self.value <= other.value\n        return NotImplemented\n\n    def __lt__(self, other):\n        if self.__class__ is other.__class__:\n            return self.value < other.value\n        return NotImplemented\n\n    @classmethod\n    def names(cls):\n        return [e.name for e in cls]\n\n    @classmethod\n    def names_snake(cls):\n        return [to_snake(n) for n in cls.names()]\n\n    @classmethod\n    def from_name(cls, name: str):\n        if '_' in name or not name[0].isupper():\n            return cls.names_snake().index(name)\n        return cls.names().index(name)\n\n\nclass DagType(OrderedEnum):\n    Standard = 0\n    Pipe = 1\n\n\nclass TaskStatus(OrderedEnum):\n    NotRan = 0\n    Queued = 1\n    InProgress = 2\n    Failed = 3\n    Stopped = 4\n    Skipped = 5\n    Success = 6\n\n\nclass TaskType(OrderedEnum):\n    User = 0\n    Train = 1\n    Service = 2\n\n\nclass ComponentType(OrderedEnum):\n    API = 0\n    Supervisor = 1\n    Worker = 2\n    WorkerSupervisor = 3\n    Client = 4\n\n\nclass LogStatus(OrderedEnum):\n    Debug = 10\n    Info = 20\n    Warning = 30\n    Error = 40\n"""
mlcomp/db/signals.py,0,"b""from functools import wraps\n\nfrom sqlalchemy import event\nfrom sqlalchemy.orm.exc import StaleDataError\n\nfrom mlcomp.db.providers import TaskProvider, StepProvider, DagProvider\nfrom mlcomp.db.models import Task, Step, Log, ReportImg, File\nfrom mlcomp.utils.misc import now\nfrom mlcomp.db.core import Session\n\n_session = Session.create_session(key=__name__)\n\n\ndef error_handler(f):\n    @wraps(f)\n    def decorated(*args, **kwargs):\n        global _session\n        try:\n            f(*args, **kwargs)\n        except Exception as e:\n            if Session.sqlalchemy_error(e):\n                Session.cleanup(key=__name__)\n                _session = Session.create_session(key=__name__)\n            raise e\n\n    return decorated\n\n\n@event.listens_for(Task, 'before_update')\n@error_handler\ndef task_before_update(mapper, connection, target):\n    target.last_activity = now()\n    if target.parent:\n        provider = TaskProvider(_session)\n        parent = provider.by_id(target.parent)\n        if parent is None:\n            return\n\n        parent.last_activity = target.last_activity\n\n        try:\n            provider.commit()\n        except StaleDataError:\n            pass\n\n\n@event.listens_for(Step, 'before_insert')\n@event.listens_for(Step, 'before_update')\n@error_handler\ndef step_before_insert_update(mapper, connection, target):\n    TaskProvider(_session).update_last_activity(target.task)\n\n\n@event.listens_for(Log, 'before_insert')\n@error_handler\ndef log_before_insert(mapper, connection, target):\n    if target.step is None:\n        return\n    step = StepProvider(_session).by_id(target.step)\n    TaskProvider(_session).update_last_activity(step.task)\n\n\n@event.listens_for(ReportImg, 'before_insert')\ndef dag_before_create(mapper, connection, target):\n    provider = DagProvider(_session)\n    dag = provider.by_id(target.dag)\n    dag.img_size += target.size\n    provider.commit()\n\n\n__all__ = [\n    'task_before_update', 'step_before_insert_update', 'log_before_insert',\n    'dag_before_create'\n]\n"""
mlcomp/migration/__init__.py,0,b'# template repository default module\n'
mlcomp/migration/manage.py,0,"b""import os\n\nimport migrate.versioning.api as api\nfrom migrate.exceptions import DatabaseAlreadyControlledError\n\nfrom mlcomp import SA_CONNECTION_STRING\n\n\ndef migrate(connection_string: str = None):\n    folder = os.path.dirname(__file__)\n    connection_string = connection_string or SA_CONNECTION_STRING\n    try:\n        api.version_control(url=connection_string, repository=folder)\n    except DatabaseAlreadyControlledError:\n        pass\n\n    api.upgrade(url=connection_string, repository=folder)\n\n\n__all__ = ['migrate']\n"""
mlcomp/server/__init__.py,0,b''
mlcomp/server/__main__.py,0,"b'import os\nfrom multiprocessing import cpu_count\nfrom time import sleep\n\nfrom subprocess import Popen\n\nimport click\n\nfrom mlcomp import CONFIG_FOLDER, REDIS_PORT, REDIS_PASSWORD\nfrom mlcomp.report import check_statuses\nfrom mlcomp.server.back.app import start_server as _start_server\nfrom mlcomp.server.back.app import stop_server as _stop_server\nfrom mlcomp.utils.misc import kill_child_processes\n\n\n@click.group()\ndef main():\n    pass\n\n\n@main.command()\ndef start_site():\n    """"""\n    Start only site\n    """"""\n    check_statuses()\n    _start_server()\n\n\n@main.command()\ndef stop_site():\n    """"""\n    Stop site\n    """"""\n    check_statuses()\n    _stop_server()\n\n\n@main.command()\n@click.option(\'--daemon\', type=bool, default=False,\n              help=\'start supervisord in a daemon mode\')\n@click.option(\'--debug\', type=bool, default=False,\n              help=\'use source files instead the installed library\')\n@click.option(\'--workers\', type=int, default=cpu_count(),\n              help=\'count of workers\')\n@click.option(\'--log_level\', type=str, default=\'DEBUG\',\n              help=\'log level of supervisord\')\ndef start(daemon: bool, debug: bool, workers: int, log_level: str):\n    """"""\n    Start both server and worker on the same machine.\n\n    It starts: redis-server, site, worker_supervisor, workers\n    """"""\n    # creating supervisord config\n    supervisor_command = \'mlcomp-worker worker-supervisor\'\n    worker_command = \'mlcomp-worker worker\'\n    server_command = \'mlcomp-server start-site\'\n\n    if debug:\n        supervisor_command = \'python mlcomp/worker/__main__.py \' \\\n                             \'worker-supervisor\'\n        worker_command = \'python mlcomp/worker/__main__.py worker\'\n        server_command = \'python mlcomp/server/__main__.py start-site\'\n\n    folder = os.path.dirname(os.path.dirname(__file__))\n    redis_path = os.path.join(folder, \'bin/redis-server\')\n\n    daemon_text = \'false\' if daemon else \'true\'\n    text = [\n        \'[supervisord]\', f\'nodaemon={daemon_text}\', \'\',\n        \'[program:redis]\', f\'command={redis_path} --port {REDIS_PORT}\'\n                           f\' --requirepass {REDIS_PASSWORD}\',\n    ]\n    conf = os.path.join(CONFIG_FOLDER, \'supervisord-redis.conf\')\n    with open(conf, \'w\') as f:\n        f.writelines(\'\\n\'.join(text))\n\n    Popen(\n        [\'supervisord\', f\'--configuration={conf}\', f\'--loglevel={log_level}\'])\n\n    sleep(5)\n    check_statuses()\n\n    daemon_text = \'false\' if daemon else \'true\'\n    text = [\n        \'[supervisord]\', f\'nodaemon={daemon_text}\', \'\',\n        \'[program:supervisor]\',\n        f\'command={supervisor_command}\', \'autostart=true\', \'autorestart=true\',\n        \'\',\n        \'[program:server]\',\n        f\'command={server_command}\',\n        \'autostart=true\',\n        \'autorestart=true\', \'\'\n    ]\n\n    for p in range(workers):\n        text.append(f\'[program:worker{p}]\')\n        text.append(f\'command={worker_command} {p}\')\n        text.append(\'autostart=true\')\n        text.append(\'autorestart=true\')\n        text.append(\'\')\n\n    conf = os.path.join(CONFIG_FOLDER, \'supervisord.conf\')\n    with open(conf, \'w\') as f:\n        f.writelines(\'\\n\'.join(text))\n\n    os.system(f\'supervisord \' f\'-c {conf} -e {log_level}\')\n\n\n@main.command()\ndef stop():\n    """"""\n    Stop supervisord started by start command\n    """"""\n    check_statuses()\n\n    lines = os.popen(\'ps -ef | grep supervisord\').readlines()\n    for line in lines:\n        if \'mlcomp/configs/supervisord.conf\' not in line:\n            continue\n        pid = int(line.split()[1])\n        kill_child_processes(pid)\n\n\n@main.command()\ndef status():\n    """"""\n    Shows if mlcomp server is already running\n    """"""\n    lines = os.popen(""ps ef | grep mlcomp"").readlines()\n    pids = {}\n    for line in lines:\n        if ""mlcomp/configs/supervisord.conf"" in line:\n            pids[""server""] = line\n        elif ""mlcomp-server start-site"" in line:\n            pids[""site""] = line\n        elif ""redis-server"" in line:\n            pids[""redis""] = line\n    if not pids:\n        print(""There are no mlcomp services started"")\n        return\n    text = ""Current MLComp services status:\\n""\n    for k, v in pids.items():\n        text += f""  (\xe2\x9c\x94) {k} is started on pid {v.split()[0]}\\n""\n    print(text)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
mlcomp/utils/__init__.py,0,b''
mlcomp/utils/config.py,0,"b""from collections import defaultdict\nfrom typing import List\nimport os\nimport json\n\nimport albumentations as A\n\nfrom mlcomp import DATA_FOLDER\nfrom mlcomp.utils.io import yaml_load\nfrom mlcomp.utils.misc import dict_flatten, dict_unflatten\n\n\nclass Config(dict):\n    @property\n    def data_folder(self):\n        return os.path.join(DATA_FOLDER, self['info']['project'])\n\n    @staticmethod\n    def from_json(config: str):\n        return Config(json.loads(config))\n\n    @staticmethod\n    def from_yaml(config: str):\n        return Config(yaml_load(config))\n\n\ndef merge_dicts_smart(target: dict, source: dict, sep='/'):\n    target_flatten = dict_flatten(target)\n    mapping = defaultdict(list)\n    hooks = dict()\n\n    for k, v in target_flatten.items():\n        parts = k.split(sep)\n        for i in range(len(parts) - 1, -1, -1):\n            key = sep.join(parts[i:])\n            mapping[key].append(k)\n\n            if 0 < i < len(parts) - 1:\n                hooks[sep.join(parts[i:-1])] = sep.join(parts[:i + 1])\n\n    for k, v in list(source.items()):\n        if isinstance(v, dict):\n            source.update(\n                {k + sep + kk: vv for kk, vv in dict_flatten(v).items()})\n\n    for k, v in source.items():\n        if len(mapping[k]) == 0:\n            parts = k.split(sep)\n            hook = None\n            for i in range(len(parts) - 1, -1, -1):\n                h = sep.join(parts[:i])\n                if h in hooks:\n                    hook = hooks[h] + sep + sep.join(parts[i:])\n                    break\n\n            if not hook:\n                hook = k\n\n            mapping[k] = [hook]\n        assert len(mapping[k]) == 1, f'ambiguous mapping for {k}'\n        key = mapping[k][0]\n        target_flatten[key] = v\n\n    return dict_unflatten(target_flatten)\n\n\ndef dict_from_list_str(params):\n    params = dict(p.split(':') for p in params)\n    for k, v in params.items():\n        if v.isnumeric():\n            if '.' in v:\n                params[k] = float(v)\n            else:\n                params[k] = int(v)\n    return params\n\n\ndef parse_albu(configs: List[dict]):\n    res = []\n    for config in configs:\n        assert 'name' in config, f'name is required in {config}'\n        config = config.copy()\n        name = config.pop('name')\n        if name == 'Compose':\n            items = config.pop('items')\n            aug = A.Compose(parse_albu(items), **config)\n        else:\n            aug = getattr(A, name)(**config)\n        res.append(aug)\n    return res\n\n\ndef parse_albu_short(config, always_apply=False):\n    if isinstance(config, str):\n        if config == 'hflip':\n            return A.HorizontalFlip(always_apply=always_apply)\n        if config == 'vflip':\n            return A.VerticalFlip(always_apply=always_apply)\n        if config == 'transpose':\n            return A.Transpose(always_apply=always_apply)\n\n        raise Exception(f'Unknwon augmentation {config}')\n    assert type(config) == dict\n    return parse_albu([config])\n\n\n__all__ = ['Config', 'merge_dicts_smart', 'parse_albu', 'parse_albu_short',\n           'dict_from_list_str']\n"""
mlcomp/utils/describe.py,0,"b""import time\nfrom math import ceil\nfrom socket import gethostname\nimport warnings\nimport datetime\nfrom typing import List\n\nfrom IPython import display\nimport matplotlib.pyplot as plt\nimport networkx as nx\nfrom matplotlib.ticker import MaxNLocator\nimport pandas as pd\n\nfrom mlcomp.db.enums import TaskStatus, ComponentType\nfrom mlcomp.db.providers import TaskProvider, LogProvider, \\\n    DagProvider, ComputerProvider, ReportSeriesProvider\nfrom mlcomp.utils.misc import now, to_snake\n\nwarnings.simplefilter('ignore')\n\n\ndef describe_tasks(dag: int, axis):\n    provider = TaskProvider()\n    columns = ['Id', 'Started', 'Duration', 'Step', 'Status']\n    cells = []\n    cells_colours = []\n\n    tasks = provider.by_dag(dag)\n\n    status_colors = {\n        'not_ran': 'gray',\n        'queued': 'lightblue',\n        'in_progress': 'lime',\n        'failed': '#e83217',\n        'stopped': '#cb88ea',\n        'skipped': 'orange',\n        'success': 'green'\n    }\n\n    finish = True\n\n    for task in tasks:\n        started = ''\n        duration = ''\n\n        if task.status <= TaskStatus.InProgress.value:\n            finish = False\n\n        if task.started:\n            started = task.started.strftime('%m.%d %H:%M:%S')\n            if task.finished:\n                duration = (task.finished - task.started).total_seconds()\n            else:\n                duration = (now() - task.started).total_seconds()\n\n            if duration > 3600:\n                duration = f'{int(duration // 3600)} hours ' \\\n                           f'{int((duration % 3600) // 60)} min' \\\n                           f' {int(duration % 60)} sec'\n            elif duration > 60:\n                duration = f'{int(duration // 60)} min' \\\n                           f' {int(duration % 60)} sec'\n            else:\n                duration = f'{int(duration)} sec'\n\n        status = to_snake(TaskStatus(task.status).name)\n        status_color = status_colors[status]\n\n        task_cells = [\n            str(task.id), started, duration, task.current_step or '1', status\n        ]\n        task_colors = ['white', 'white', 'white', 'white', status_color]\n        cells.append(task_cells)\n        cells_colours.append(task_colors)\n\n    table = axis.table(\n        cellText=cells,\n        colLabels=columns,\n        cellColours=cells_colours,\n        cellLoc='center',\n        colWidths=[0.2, 0.3, 0.4, 0.1, 0.2],\n        bbox=[0, 0, 1.0, 1.0],\n        loc='center'\n    )\n\n    table.auto_set_font_size(False)\n    table.set_fontsize(14)\n\n    axis.set_xticks([])\n    axis.axis('off')\n    axis.set_title('Tasks')\n\n    return finish\n\n\ndef describe_logs(\n        dag: int,\n        axis,\n        max_log_text: int = None,\n        log_count: int = 5,\n        col_withds: List[float] = None\n):\n    columns = ['Component', 'Level', 'Task', 'Time', 'Text']\n    provider = LogProvider()\n    logs = provider.last(log_count, dag=dag)\n\n    res = []\n\n    cells = []\n    cells_colours = []\n\n    for log, task_id in logs:\n        component = to_snake(ComponentType(log.component).name)\n\n        level = log.level\n        level = 'debug' if level == 10 else 'info' \\\n            if level == 20 else 'warning' \\\n            if level == 30 else 'error'\n        message = log.message\n        if max_log_text:\n            message = message[:max_log_text]\n        log_cells = [\n            component, level,\n            str(task_id),\n            log.time.strftime('%m.%d %H:%M:%S'), message\n        ]\n\n        cells.append(log_cells)\n\n        level_color = 'lightblue' if level == 'info' else 'lightyellow' \\\n            if level == 'warning' else 'red' if level == 'error' else 'white'\n\n        log_colours = ['white', level_color, 'white', 'white', 'white']\n        cells_colours.append(log_colours)\n\n        if level == 'error':\n            res.append(log)\n\n    col_withds = col_withds or [0.2, 0.1, 0.25, 0.2, 0.45]\n    if len(cells) > 0:\n        table = axis.table(\n            cellText=cells,\n            colLabels=columns,\n            cellColours=cells_colours,\n            cellLoc='center',\n            colWidths=col_withds,\n            bbox=[0, 0, 1, 1.0],\n            loc='center'\n        )\n\n        table.auto_set_font_size(False)\n        table.set_fontsize(14)\n\n    axis.set_xticks([])\n    axis.axis('off')\n    axis.set_title('Logs')\n\n    return res\n\n\ndef describe_dag(dag, axis):\n    provider = DagProvider()\n    graph = provider.graph(dag)\n\n    status_colors = {\n        'not_ran': '#808080',\n        'queued': '#add8e6',\n        'in_progress': '#bfff00',\n        'failed': '#e83217',\n        'stopped': '#cb88ea',\n        'skipped': '#ffa500',\n        'success': '#006400'\n    }\n    node_color = []\n    edge_color = []\n\n    G = nx.DiGraph()\n    labels = dict()\n    for n in graph['nodes']:\n        G.add_node(n['id'])\n        labels[n['id']] = n['id']\n        node_color.append(status_colors[n['status']])\n\n    edges = []\n    for e in graph['edges']:\n        G.add_edge(e['from'], e['to'])\n        edges.append((e['from'], e['to']))\n        edge_color.append(status_colors[e['status']])\n\n    pos = nx.spring_layout(G, seed=0)\n    nx.draw_networkx_nodes(\n        G, pos, node_color=node_color, ax=axis, node_size=2000\n    )\n    nx.draw_networkx_labels(\n        G,\n        pos,\n        labels,\n        ax=axis,\n        with_labels=True,\n        font_color='orange',\n        font_weight='bold',\n        font_size=18\n    )\n    nx.draw_networkx_edges(\n        G,\n        pos,\n        edgelist=edges,\n        edge_color=edge_color,\n        arrows=True,\n        arrowsize=80,\n        ax=axis\n    )\n\n    axis.set_xticks([])\n    axis.axis('off')\n    axis.set_title('Graph')\n\n\ndef describe_resources(computer: str, axis):\n    provider = ComputerProvider()\n    res = provider.get({})['data']\n    res = [r for r in res if r['name'] == computer][0]\n    usage = res['usage_history']\n    x = [\n        datetime.datetime.strptime(t, provider.datetime_format)\n        for t in usage['time']\n    ]\n\n    for item in usage['mean']:\n        if item['name'] == 'disk':\n            continue\n\n        axis.plot(x, item['value'], label=item['name'])\n\n    axis.set_title('Resources')\n    axis.set_ylabel('%')\n    axis.legend(loc='lower left')\n\n\ndef describe_metrics(series, axis, last_n_epoch=None):\n    for i in range(len(axis)):\n        ax = axis[i]\n        if i >= len(series):\n            ax.axis('off')\n            continue\n\n        ax = axis[i]\n        ax.axis('on')\n        task_name, metric, groups = series[i]\n\n        for group in groups:\n            if last_n_epoch:\n                group['epoch'] = group['epoch'][-last_n_epoch:]\n                group['value'] = group['value'][-last_n_epoch:]\n\n            ax.plot(group['epoch'], group['value'], label=group['name'])\n\n        ax.set_title(f'{task_name}, {metric} score')\n        ax.set_ylabel(metric, labelpad=20)\n        ax.set_xlabel('epoch')\n        ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n        ax.legend()\n\n\ndef describe_task_names(dag: int):\n    pd.set_option('display.max_columns', None)\n    pd.set_option('display.expand_frame_repr', False)\n    pd.set_option('max_colwidth', -1)\n\n    provider = TaskProvider()\n    tasks = provider.by_dag(dag)\n    return pd.DataFrame([{'id': t.id, 'name': t.name} for t in tasks])\n\n\ndef describe(\n        dag: int,\n        metrics=None,\n        last_n_epoch=None,\n        computer: str = None,\n        max_log_text: int = 45,\n        fig_size=(12, 10),\n        grid_spec: dict = None,\n        log_count=5,\n        log_col_widths: List[float] = None,\n        wait=True,\n        wait_interval=5,\n        task_with_metric_count=0\n):\n    grid_spec = grid_spec or {}\n    metrics = metrics or []\n\n    series_count = task_with_metric_count * len(metrics)\n    size = (4 + ceil(series_count / 2), 2)\n    default_grid_spec = {\n        'tasks': {\n            'rowspan': 1,\n            'colspan': 2,\n            'loc': (0, 0)\n        },\n        'dag': {\n            'rowspan': 1,\n            'colspan': 2,\n            'loc': (1, 0)\n        },\n        'logs': {\n            'rowspan': 1,\n            'colspan': 2,\n            'loc': (2, 0)\n        },\n        'resources': {\n            'rowspan': 1,\n            'colspan': 2,\n            'loc': (3, 0)\n        },\n        'size': size\n    }\n\n    loc = (4, 0)\n    for i in range(series_count):\n        default_grid_spec[i] = {'rowspan': 1, 'colspan': 1,\n                                'loc': loc}\n        if loc[1] == 1:\n            loc = (loc[0] + 1, 0)\n        else:\n            loc = (loc[0], 1)\n\n    default_grid_spec.update(grid_spec)\n    grid_spec = default_grid_spec\n\n    fig = plt.figure(figsize=fig_size)\n\n    def grid_cell(spec: dict):\n        return plt.subplot2grid(\n            size,\n            spec['loc'],\n            colspan=spec['colspan'],\n            rowspan=spec['rowspan'],\n            fig=fig\n        )\n\n    while True:\n        computer = computer or gethostname()\n\n        task_axis = grid_cell(grid_spec['tasks'])\n        dag_axis = grid_cell(grid_spec['dag'])\n        resources_axis = grid_cell(grid_spec['resources'])\n        logs_axis = grid_cell(grid_spec['logs'])\n\n        finish = describe_tasks(dag, task_axis)\n        describe_dag(dag, dag_axis)\n        errors = describe_logs(\n            dag,\n            axis=logs_axis,\n            max_log_text=max_log_text,\n            log_count=log_count,\n            col_withds=log_col_widths\n        )\n        describe_resources(computer=computer, axis=resources_axis)\n\n        series_provider = ReportSeriesProvider()\n        series = series_provider.by_dag(dag, metrics)\n\n        metric_axis = [grid_cell(grid_spec[i]) for i, s in enumerate(series)]\n\n        describe_metrics(series, last_n_epoch=last_n_epoch, axis=metric_axis)\n\n        plt.tight_layout()\n\n        display.clear_output(wait=True)\n\n        for error in errors:\n            print(error.time)\n            print(error.message)\n\n        display.display(fig)\n\n        if not wait or finish:\n            break\n\n        time.sleep(wait_interval)\n\n    plt.close(fig)\n\n\n__all__ = ['describe']\n\nif __name__ == '__main__':\n    describe(dag=293, metrics=['loss', 'dice'], wait_interval=2)\n"""
mlcomp/utils/img.py,0,"b""from typing import Tuple\n\nimport numpy as np\nimport cv2\n\n\ndef resize_saving_ratio(img: np.array, size: Tuple[int, int]):\n    if not size:\n        return img\n    if size[0] and img.shape[0] > size[0]:\n        k = size[0] / img.shape[0]\n        img = cv2.resize(img, (int(k * img.shape[1]), size[0]))\n    if size[1] and img.shape[1] > size[1]:\n        k = size[1] / img.shape[1]\n        img = cv2.resize(img, (size[1], int(k * img.shape[0])))\n    return img\n\n\n__all__ = ['resize_saving_ratio']\n"""
mlcomp/utils/io.py,0,"b""import os\nfrom os.path import join\n\nfrom typing import List\n\nfrom io import BytesIO\nfrom zipfile import ZipFile\n\nimport pandas as pd\nimport yaml\n\nyaml.warnings({'YAMLLoadWarning': False})\n\n\ndef read_pandas(file):\n    if file.endswith('.csv'):\n        df = pd.read_csv(file)\n    elif file.endswith('.parquet'):\n        df = pd.read_parquet(file)\n    else:\n        raise Exception('Unknown file type')\n    return df\n\n\ndef read_lines(file: str):\n    return [l.strip() for l in open(file)]\n\n\ndef from_module_path(file: str, path: str):\n    return os.path.join(os.path.dirname(file), path)\n\n\ndef yaml_load(text: str = None, file: str = None):\n    stream = text or ''\n    if file is not None:\n        stream = open(file).read()\n    res = yaml.load(stream, Loader=yaml.FullLoader)\n    if res is None:\n        return {}\n    return res\n\n\ndef yaml_dump(data, file: str = None):\n    res = yaml.dump(data, default_flow_style=False, sort_keys=False)\n    if file:\n        open(file, 'w').write(res)\n    return res\n\n\ndef zip_folder(\n        folder: str = None,\n        dst: str = None,\n        folders: List[str] = (),\n        files: List[str] = (),\n        root: bool = None\n):\n    if root is None and len(folders) > 0:\n        root = True\n    if dst is None:\n        dst = BytesIO()\n    if folder:\n        folders = (folder,)\n\n    with ZipFile(dst, 'w') as zip_obj:\n        # Iterate over all the files in directory\n        for folder in folders:\n            for folderName, subfolders, filenames in os.walk(folder):\n                for filename in filenames:\n                    # create complete filepath of file in directory\n                    filePath = join(folderName, filename)\n                    # Add file to zip\n                    rel_path = os.path.relpath(filePath, folder)\n                    if root:\n                        rel_path = join(os.path.basename(folder), rel_path)\n                    zip_obj.write(filePath, rel_path)\n\n        for file in files:\n            zip_obj.write(file, file)\n    return dst\n\n\n__all__ = ['read_lines', 'from_module_path', 'yaml_load', 'yaml_dump',\n           'zip_folder']\n"""
mlcomp/utils/logging.py,0,"b'import os\nimport logging\nimport sys\nfrom logging.handlers import RotatingFileHandler\n\nfrom mlcomp import LOG_FOLDER, LOG_NAME, FILE_LOG_LEVEL, DB_LOG_LEVEL, \\\n    CONSOLE_LOG_LEVEL\nfrom mlcomp.db.core import Session\nfrom mlcomp.db.providers import LogProvider\nfrom mlcomp.db.models import Log\nfrom mlcomp.utils.misc import now\n\nROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), \'../\'))\n\n\nclass Formatter(logging.Formatter):\n    def format(self, record):\n        if not record.pathname.startswith(ROOT):\n            try:\n                return super().format(record)\n            except Exception:\n                return record.msg\n\n        msg = str(record.msg)\n        if record.args:\n            try:\n                msg = msg % record.args\n            except Exception:\n                pass\n        record.message = msg\n        if self.usesTime():\n            record.asctime = self.formatTime(record, self.datefmt)\n        s = self.formatMessage(record)\n        if record.exc_info:\n            # Cache the traceback text to avoid converting it multiple times\n            # (it\'s constant anyway)\n            if not record.exc_text:\n                record.exc_text = self.formatException(record.exc_info)\n        if record.exc_text:\n            if s[-1:] != \'\\n\':\n                s = s + \'\\n\'\n            s = s + record.exc_text\n        if record.stack_info:\n            if s[-1:] != \'\\n\':\n                s = s + \'\\n\'\n            s = s + self.formatStack(record.stack_info)\n        return s\n\n\nclass DbHandler(logging.Handler):\n    """"""\n    A handler class which writes logging records, appropriately formatted,\n    to the database.\n    """"""\n\n    def __init__(self, session: Session):\n        """"""\n        Initialize the handler.\n        """"""\n        logging.Handler.__init__(self)\n        self.provider = LogProvider(session)\n\n    def emit(self, record):\n        """"""\n        Emit a record.\n        """"""\n        try:\n            if not record.pathname.startswith(ROOT):\n                return\n\n            assert 1 <= len(record.args), \\\n                \'Args weer not been provided for logging\'\n            assert len(record.args) <= 4, \'Too many args for logging\'\n\n            step = None\n            task = None\n            computer = None\n\n            if len(record.args) == 1:\n                component = record.args[0]\n            elif len(record.args) == 2:\n                component, computer = record.args\n            elif len(record.args) == 3:\n                component, computer, task = record.args\n            else:\n                component, computer, task, step = record.args\n\n            if not isinstance(component, int):\n                component = component.value\n\n            module = os.path.relpath(record.pathname, ROOT). \\\n                replace(os.sep, \'.\').replace(\'.py\', \'\')\n            if record.funcName and record.funcName != \'<module>\':\n                module = f\'{module}:{record.funcName}\'\n            log = Log(\n                message=record.msg[-16000:],\n                time=now(),\n                level=record.levelno,\n                step=step,\n                component=component,\n                line=record.lineno,\n                module=module,\n                task=task,\n                computer=computer\n            )\n            self.provider.add(log)\n        except Exception:\n            self.handleError(record)\n\n\ndef create_logger(session: Session, name: str, db=True, file=True,\n                  console=True):\n    logger = logging.Logger(name)\n    logger.handlers = []\n\n    if console:\n        console_handler = logging.StreamHandler()\n        console_handler.setLevel(CONSOLE_LOG_LEVEL)\n        console_handler.stream = sys.stdout\n        logger.handlers.append(console_handler)\n\n    if file:\n        file_path = os.path.join(LOG_FOLDER, f\'{LOG_NAME}.txt\')\n        file_handler = RotatingFileHandler(file_path)\n        file_handler.setLevel(FILE_LOG_LEVEL)\n        file_handler.maxBytes = 10485760\n        file_handler.backupCount = 1\n        logger.handlers.append(file_handler)\n\n    if db:\n        handler = DbHandler(session)\n        handler.setLevel(DB_LOG_LEVEL)\n        logger.handlers.append(handler)\n\n    for h in logger.handlers:\n        fmt = \'%(asctime)s.%(msecs)03d %(levelname)s\' \\\n              \' %(module)s - %(funcName)s: %(message)s\'\n        datefmt = \'%Y-%m-%d %H:%M:%S\'\n        if isinstance(h, DbHandler):\n            fmt, datefmt = None, None\n        h.formatter = Formatter(fmt=fmt, datefmt=datefmt)\n\n    # ignore messages from some libraries\n    class NoRunningFilter(logging.Filter):\n        def filter(self, record):\n            return \'ran tasks\' not in str(record.msg)\n\n    for k in logging.root.manager.loggerDict:\n        if \'apscheduler\' in k:\n            logging.getLogger(k).setLevel(logging.ERROR)\n        if \'mlcomp\' in k:\n            logging.getLogger(k).addFilter(NoRunningFilter())\n        if \'serializer\' in k:\n            logging.getLogger(k).setLevel(logging.ERROR)\n\n    return logger\n\n\n__all__ = [\'create_logger\']\n'"
mlcomp/utils/misc.py,2,"b'import collections\nimport copy\nimport subprocess\n\nfrom datetime import datetime\nimport re\nfrom typing import List\nimport os\nimport pwd\nimport random\n\nimport dateutil\nimport numpy as np\nimport signal\nimport psutil\nfrom subprocess import check_output\n\nfirst_cap_re = re.compile(\'(.)([A-Z][a-z]+)\')\nall_cap_re = re.compile(\'([a-z0-9])([A-Z])\')\n\n\ndef set_global_seed(seed: int) -> None:\n    """"""\n    Sets random seed into PyTorch, TensorFlow, Numpy and Random.\n\n    Args:\n        seed: random seed\n    """"""\n\n    try:\n        import torch\n    except ImportError:\n        pass\n    else:\n        torch.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n\n    random.seed(seed)\n    np.random.seed(seed)\n\n\ndef dict_func(objcts: List, func=np.mean):\n    if len(objcts) == 0:\n        return {}\n    first = objcts[0]\n\n    res = dict()\n    for k in first:\n        k_objcts = [o[k] for o in objcts]\n        if isinstance(first[k], dict):\n            res[k] = dict_func(k_objcts, func)\n        elif isinstance(first[k], list):\n            res[k] = [dict_func([o[k][i] for o in objcts], func) for i in\n                      range(len(first[k]))]\n        else:\n            res[k] = func(k_objcts)\n    return res\n\n\ndef du(path):\n    """"""disk usage in human readable format (e.g. \'2,1GB\')""""""\n    res = subprocess.check_output([\'du\', \'-sh\', \'-L\', path]).split()[0].decode(\n        \'utf-8\')\n    if \'G\' in res:\n        return float(res[:-1].replace(\',\', \'.\'))\n    if \'M\' in res:\n        return float(res[:-1].replace(\',\', \'.\')) / 1000\n    if \'K\' in res:\n        return float(res[:-1].replace(\',\', \'.\')) / 10 ** 6\n    raise Exception(\'unknown files size\' + res)\n\n\ndef get_pid(name):\n    res = []\n    lines = check_output([\'ps\', \'-ef\']).decode().split(\'\\n\')\n    header = lines[0].split()\n    for line in lines[1:]:\n        if line.strip() == \'\':\n            continue\n\n        parts = line.split()\n        parts = parts[:7] + [\' \'.join(parts[7:])]\n\n        item = {}\n        for p, h in zip(parts, header):\n            if h == \'CMD:\':\n                h = \'CMD\'\n\n            if h in [\'PID\', \'PPID\']:\n                p = int(p)\n\n            item[h] = p\n        if \'CMD\' in item and name in item[\'CMD\']:\n            res.append(item)\n    return res\n\n\ndef get_default_network_interface():\n    lines = check_output([\'route\']).decode().split(\'\\n\')\n    header = lines[1].split()\n\n    for line in lines[2:]:\n        parts = line.split()\n        item = dict()\n        for h, p in zip(header, parts):\n            item[h] = p\n        if item[\'Destination\'] == \'default\' and \'Iface\' in item:\n            return item[\'Iface\']\n    return None\n\n\ndef now():\n    return datetime.utcnow()\n\n\ndef merge_dicts(*dicts: dict) -> dict:\n    """"""\n    Recursive dict merge.\n    Instead of updating only top-level keys,\n    ``merge_dicts`` recurses down into dicts nested\n    to an arbitrary depth, updating keys.\n    Args:\n        *dicts: several dictionaries to merge\n    Returns:\n        dict: deep-merged dictionary\n    """"""\n    assert len(dicts) > 1\n\n    dict_ = copy.deepcopy(dicts[0])\n\n    for merge_dict in dicts[1:]:\n        merge_dict = merge_dict or {}\n        for k, v in merge_dict.items():\n            if (\n                    k in dict_ and isinstance(dict_[k], dict)\n                    and isinstance(merge_dict[k], collections.Mapping)\n            ):\n                dict_[k] = merge_dicts(dict_[k], merge_dict[k])\n            else:\n                dict_[k] = merge_dict[k]\n\n    return dict_\n\n\ndef to_snake(name):\n    s1 = first_cap_re.sub(r\'\\1_\\2\', name)\n    return all_cap_re.sub(r\'\\1_\\2\', s1).lower()\n\n\ndef log_name(level: int):\n    if level == 10:\n        return \'DEBUG\'\n    if level == 20:\n        return \'INFO\'\n    if level == 30:\n        return \'WARNING\'\n    if level == 40:\n        return \'ERROR\'\n\n    raise Exception(\'Unknown log level\')\n\n\ndef duration_format(delta: float):\n    """"""\n    Duration format\n    :param delta: seconds\n    :return: string representation: 1 days 1 hour 1 min\n    """"""\n    if delta < 0:\n        delta = f\'{int(delta * 1000)} ms\'\n    elif delta < 60:\n        delta = f\'{int(delta)} sec\'\n    elif delta < 3600:\n        delta = f\'{int(delta / 60)} min {int(delta % 60)} sec\'\n    elif delta < 3600 * 24:\n        hour = int(delta / 3600)\n        delta = f\'{hour} {""hours"" if hour > 1 else ""hour""} \' \\\n                f\'{int((delta % 3600) / 60)} min\'\n    else:\n        day = int(delta / (3600 * 24))\n        hour = int((delta % (3600 * 24)) / 3600)\n        delta = f\'{day} {""days"" if day > 1 else ""day""} {hour}\' \\\n                f\' {""hours"" if hour > 1 else ""hour""} \' \\\n                f\'{int((delta % 3600) / 60)} min\'\n    return delta\n\n\ndef adapt_db_types(d):\n    dic = d.__dict__ if not isinstance(d, dict) else d\n    for k in dic:\n        if type(dic[k]) in [np.int64]:\n            dic[k] = int(dic[k])\n        elif type(dic[k]) in [np.float64]:\n            dic[k] = float(dic[k])\n\n\ndef dict_flatten(d, parent_key=\'\', sep=\'/\'):\n    items = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, collections.MutableMapping):\n            items.extend(dict_flatten(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n\n\ndef dict_unflatten(d: dict, sep=\'/\'):\n    res = dict()\n    for key, value in d.items():\n        parts = key.split(sep)\n        c = res\n        for part in parts[:-1]:\n            if part not in c:\n                c[part] = dict()\n            c = c[part]\n        c[parts[-1]] = value\n\n    return res\n\n\ndef memory():\n    return map(int, os.popen(\'free -t -m\').readlines()[1].split()[1:4])\n\n\ndef disk(folder: str):\n    filesystem, total, used, available, use, mounded_point \\\n        = os.popen(f\'df {folder}\').readlines()[1].split()\n    total = int(int(total) / 10 ** 6)\n    available = int(int(available) / 10 ** 6)\n    use = int(use[:-1])\n    return total, use, available\n\n\ndef get_username():\n    return pwd.getpwuid(os.getuid())[0]\n\n\ndef parse_time(time):\n    if not time:\n        return None\n    if isinstance(time, str):\n        return dateutil.parser.parse(time)\n    return time\n\n\ndef kill_child_processes(parent_pid, sig=signal.SIGTERM):\n    try:\n        parent = psutil.Process(parent_pid)\n    except psutil.NoSuchProcess:\n        return\n    children = parent.children(recursive=True)\n    for process in children:\n        process.send_signal(sig)\n\n    parent.send_signal(sig)\n\n\nif __name__ == \'__main__\':\n    print(dict_unflatten({\'a/b\': 10, \'a/e\': 20, \'a/c/d\': 40}))\n'"
mlcomp/utils/plot.py,0,"b'import io\nfrom PIL import Image\n\nimport numpy as np\n\nfrom matplotlib.figure import Figure\nimport matplotlib.pyplot as plt\n\n\ndef figure_to_binary(figure: Figure, **kwargs):\n    buf = io.BytesIO()\n    figure.savefig(buf, format=\'jpg\', bbox_inches=\'tight\', **kwargs)\n    buf.seek(0)\n    content = buf.read()\n    buf.close()\n    return content\n\n\ndef figure_to_img(figure: Figure, **kwargs):\n    buf = io.BytesIO()\n    figure.savefig(buf, format=\'jpg\', bbox_inches=\'tight\', **kwargs)\n    buf.seek(0)\n    im = Image.open(buf)\n    im.show()\n    buf.close()\n    return im\n\n\ndef binary_to_img(b: bytes):\n    buf = io.BytesIO()\n    buf.write(b)\n    buf.seek(0)\n    im = Image.open(buf)\n    im.show()\n    return im\n\n\ndef show_values(pc, fmt=\'%.2f\', **kw):\n    """"""\n    Heatmap with text in each cell with matplotlib\'s pyplot\n    Source: https://stackoverflow.com/a/25074150/395857\n    By HYRY\n    """"""\n    pc.update_scalarmappable()\n    ax = pc.axes\n    for p, color, value in zip(\n            pc.get_paths(), pc.get_facecolors(), pc.get_array()\n    ):\n        x, y = p.vertices[:-2, :].mean(0)\n        if np.all(color[:3] > 0.5):\n            color = (0.0, 0.0, 0.0)\n        else:\n            color = (1.0, 1.0, 1.0)\n        ax.text(x, y, fmt % value, ha=\'center\', va=\'center\', color=color, **kw)\n\n\ndef cm2inch(*tupl):\n    """"""\n    Specify figure size in centimeter in matplotlib\n    Source: https://stackoverflow.com/a/22787457/395857\n    By gns-ank\n    """"""\n    inch = 2.54\n    if type(tupl[0]) == tuple:\n        return tuple(i / inch for i in tupl[0])\n    else:\n        return tuple(i / inch for i in tupl)\n\n\ndef heatmap(\n        AUC,\n        xlabel,\n        ylabel,\n        xticklabels,\n        yticklabels,\n        figure_width=40,\n        figure_height=20,\n        correct_orientation=False,\n        cmap=\'RdBu\'\n):\n    """"""\n    Inspired by:\n    - https://stackoverflow.com/a/16124677/395857\n    - https://stackoverflow.com/a/25074150/395857\n    """"""\n\n    # Plot it out\n    fig, ax = plt.subplots()\n    c = ax.pcolor(\n        AUC, edgecolors=\'k\', linestyle=\'dashed\', linewidths=0.2, cmap=cmap\n    )\n\n    # put the major ticks at the middle of each cell\n    ax.set_yticks(np.arange(AUC.shape[0]) + 0.5, minor=False)\n    ax.set_xticks(np.arange(AUC.shape[1]) + 0.5, minor=False)\n\n    # set tick labels\n    ax.set_xticklabels(xticklabels, minor=False)\n    ax.set_yticklabels(yticklabels, minor=False)\n\n    # set title and x/y labels\n    ax.set_xlabel(xlabel)\n    ax.set_ylabel(ylabel)\n\n    # Remove last blank column\n    plt.xlim((0, AUC.shape[1]))\n\n    # Turn off all the ticks\n    ax = plt.gca()\n    for t in ax.xaxis.get_major_ticks():\n        t.tick1line.set_visible(False)\n        t.tick1line.set_visible(False)\n    for t in ax.yaxis.get_major_ticks():\n        t.tick1line.set_visible(False)\n        t.tick1line.set_visible(False)\n\n    # Add color bar\n    fig.colorbar(c)\n\n    # Add text in each cell\n    show_values(c)\n\n    # Proper orientation (origin at the top left instead of bottom left)\n    if correct_orientation:\n        ax.invert_yaxis()\n        ax.xaxis.tick_top()\n\n    # resize\n    fig.set_size_inches(cm2inch(figure_width, figure_height))\n    return fig\n\n\ndef plot_classification_report(classification_report, cmap=\'RdBu\'):\n    """"""\n    Plot scikit-learn classification report.\n    Extension based on https://stackoverflow.com/a/31689645/395857\n    """"""\n    lines = classification_report.split(\'\\n\')\n\n    classes = []\n    plotMat = []\n    support = []\n    class_names = []\n    for line in lines[2:len(lines)]:\n        if \'avg\' in line or line.strip() == \'\' or \'accuracy\' in line:\n            continue\n        t = line.strip().split()\n        if len(t) < 2:\n            continue\n        classes.append(t[0])\n        v = [float(x) for x in t[1:len(t) - 1]]\n        support.append(int(t[-1]))\n        class_names.append(t[0])\n        plotMat.append(v)\n\n    xlabel = \'Metrics\'\n    ylabel = \'Classes\'\n    xticklabels = [\'Precision\', \'Recall\', \'F1-score\']\n    yticklabels = [\n        \'{0} ({1})\'.format(class_names[idx], sup)\n        for idx, sup in enumerate(support)\n    ]\n    figure_width = 25\n    figure_height = len(class_names) + 7\n    correct_orientation = False\n    return heatmap(\n        np.array(plotMat),\n        xlabel,\n        ylabel,\n        xticklabels,\n        yticklabels,\n        figure_width,\n        figure_height,\n        correct_orientation,\n        cmap=cmap\n    )\n\n\ndef show(*imgs, figsize=(20, 8)):\n    fig, axes = plt.subplots(1, len(imgs), figsize=figsize)\n    if not isinstance(axes, list) and not isinstance(axes, np.ndarray):\n        axes = [axes]\n    for img, ax in zip(imgs, axes):\n        ax.imshow(img)\n    plt.show()\n'"
mlcomp/utils/req.py,0,"b""from typing import List\nimport os\nimport ast\nfrom glob import glob\nimport pathspec\nimport pkg_resources\n\nfrom mlcomp.db.core import Session\nfrom mlcomp.utils.logging import create_logger\nfrom mlcomp.utils.io import read_lines\n\n_mapping = {\n    'cv2': 'opencv-python',\n    'sklearn': 'scikit-learn',\n    'migrate': 'sqlalchemy-migrate'\n}\n\n\ndef find_imports(\n    path: str,\n    files: List[str] = None,\n    exclude_patterns: List[str] = None,\n    encoding='utf-8'\n):\n    res = []\n    raw_imports = []\n    files = files if files is not None \\\n        else glob(os.path.join(path, '**', '*.py'), recursive=True)\n\n    exclude_patterns = exclude_patterns \\\n        if exclude_patterns is not None else []\n    spec = pathspec.PathSpec.from_lines(\n        pathspec.patterns.GitWildMatchPattern, exclude_patterns\n    )\n\n    for file in files:\n        if not file.endswith('.py'):\n            continue\n        file_rel = os.path.relpath(file, path)\n        if spec.match_file(file_rel):\n            continue\n\n        with open(file, 'r', encoding=encoding) as f:\n            content = f.read()\n            try:\n                tree = ast.parse(content)\n                for node in ast.walk(tree):\n                    if isinstance(node, ast.Import):\n                        for subnode in node.names:\n                            raw_imports.append((subnode.name, file_rel))\n                    elif isinstance(node, ast.ImportFrom):\n                        raw_imports.append((node.module, file_rel))\n            except Exception as exc:\n                logger = create_logger(Session.create_session(), __name__)\n                logger.error('Failed on file: %s' % file_rel)\n                raise exc\n\n    for lib, file in raw_imports:\n        name = lib.split('.')[0]\n        try:\n            if name in _mapping:\n                name = _mapping[name]\n\n            version = pkg_resources.get_distribution(name).version\n            res.append((name, version))\n        except Exception:\n            pass\n\n    return res\n\n\ndef _read_requirements(file: str):\n    res = []\n    for line in read_lines(file):\n        if line == '':\n            continue\n        name, rel, ver = None, None, None\n        if '>=' in line:\n            rel = '>='\n        elif '==' in line:\n            rel = '=='\n\n        name = line.split(rel)[0].strip()\n        if rel:\n            ver = line.split(rel)[1].strip()\n\n        res.append([name, rel, ver])\n\n    return res\n\n\ndef _write_requirements(file: str, reqs: List):\n    with open(file, 'w') as f:\n        text = '\\n'.join(\n            [f'{name}{rel}{ver}' if rel else name for name, rel, ver in reqs]\n        )\n\n        f.write(text)\n\n\ndef control_requirements(\n    path: str, files: List[str] = None, exclude_patterns: List[str] = None\n):\n    req_file = os.path.join(path, 'requirements.txt')\n    if not os.path.exists(req_file):\n        with open(req_file, 'w') as f:\n            f.write('')\n\n    req_ignore_file = os.path.join(path, 'requirements.ignore.txt')\n    if not os.path.exists(req_ignore_file):\n        with open(req_ignore_file, 'w') as f:\n            f.write('')\n\n    libs = find_imports(path, files=files, exclude_patterns=exclude_patterns)\n    module_folder = os.path.dirname(__file__)\n    stdlib_file = os.path.join(module_folder, 'req_stdlib')\n    ignore_libs = set(read_lines(req_ignore_file) + read_lines(stdlib_file))\n\n    reqs = _read_requirements(req_file)\n    for lib, version in libs:\n        if lib in ignore_libs:\n            continue\n        found = False\n        for i in range(len(reqs)):\n            if reqs[i][0] == lib:\n                found = True\n                reqs[i][1] = '=='\n                reqs[i][2] = version\n                break\n        if not found:\n            reqs.append([lib, '==', version])\n\n    _write_requirements(req_file, reqs)\n    return reqs\n\n\nif __name__ == '__main__':\n    folder = '/home/light/projects/mlcomp/'\n    # for l, v in find_imports(folder):\n    #     print(f'{l}={v}')\n    control_requirements(folder, exclude_patterns=['mlcomp/server/front'])\n"""
mlcomp/utils/schedule.py,0,"b""import atexit\n\nfrom apscheduler.schedulers.background import BackgroundScheduler\n\n\ndef start_schedule(jobs):\n    scheduler = BackgroundScheduler()\n    for func, interval in jobs:\n        scheduler.add_job(func=func, trigger='interval', seconds=interval,\n                          max_instances=1)\n    scheduler.start()\n\n    # Shut down the scheduler when exiting the app\n    atexit.register(lambda: scheduler.shutdown())\n"""
mlcomp/utils/tests.py,0,b'import pytest\nfrom importlib import reload\nimport shutil\n\nimport mlcomp\nfrom mlcomp import ROOT_FOLDER\nfrom mlcomp.db.core import Session\nfrom mlcomp.migration.manage import migrate\n\n\n@pytest.fixture()\ndef session():\n    if ROOT_FOLDER:\n        shutil.rmtree(ROOT_FOLDER)\n        reload(mlcomp)\n\n    migrate()\n    res = Session.create_session()\n    yield res\n'
mlcomp/utils/torch.py,4,"b""import numpy as np\nfrom tqdm import tqdm\n\nimport torch\nfrom torch.jit import load\nfrom torch.utils.data import DataLoader, Dataset\n\nfrom mlcomp.contrib.transform.tta import TtaWrap\n\n\ndef apply_activation(x, activation):\n    if not activation:\n        return x\n    if activation == 'sigmoid':\n        return torch.sigmoid(x)\n    if activation == 'softmax':\n        return torch.softmax(x, 1)\n    raise Exception(f'unknown activation = {activation}')\n\n\ndef _infer_batch(model, loader: DataLoader, activation=None):\n    for batch in tqdm(loader, total=len(loader)):\n        features = batch['features'].cuda()\n        logits = model(features)\n        p = apply_activation(logits, activation)\n\n        if isinstance(loader.dataset, TtaWrap):\n            p = loader.dataset.inverse(p)\n        p = p.detach().cpu().numpy()\n\n        yield {'prob': p, 'count': p.shape[0], **batch}\n\n\ndef _infer(model, loader: DataLoader, activation=None):\n    pred = []\n    for batch in tqdm(loader, total=len(loader)):\n        features = batch['features'].cuda()\n        logits = model(features)\n        p = apply_activation(logits, activation)\n\n        if isinstance(loader.dataset, TtaWrap):\n            p = loader.dataset.inverse(p)\n        p = p.detach().cpu().numpy()\n\n        pred.append(p)\n\n    pred = np.vstack(pred)\n    return pred\n\n\ndef infer(\n        x: Dataset,\n        file: str,\n        batch_size: int = 1,\n        batch_mode: bool = False,\n        activation=None,\n        num_workers: int = 1,\n):\n    loader = DataLoader(\n        x,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=num_workers,\n        pin_memory=True\n    )\n    model = load(file).cuda()\n    if batch_mode:\n        return _infer_batch(model, loader, activation=activation)\n\n    return _infer(model, loader, activation=activation)\n\n\n__all__ = ['infer', 'apply_activation']\n"""
mlcomp/worker/__init__.py,0,b''
mlcomp/worker/__main__.py,1,"b'import time\nimport socket\nimport json\nimport os\nimport traceback\nfrom multiprocessing import cpu_count\n\nimport click\nimport GPUtil\nimport psutil\nimport numpy as np\nimport torch\n\nfrom mlcomp.report import check_statuses\nfrom mlcomp.utils.io import yaml_load\n\nfrom mlcomp import ROOT_FOLDER, MASTER_PORT_RANGE, CONFIG_FOLDER, \\\n    DOCKER_IMG, DOCKER_MAIN, IP, PORT, WORKER_USAGE_INTERVAL, \\\n    SYNC_WITH_THIS_COMPUTER, CAN_PROCESS_TASKS\nfrom mlcomp.db.core import Session\nfrom mlcomp.db.enums import ComponentType, TaskStatus\nfrom mlcomp.utils.logging import create_logger\nfrom mlcomp.db.providers import DockerProvider, TaskProvider\nfrom mlcomp.utils.schedule import start_schedule\nfrom mlcomp.utils.misc import dict_func, now, disk, get_username, \\\n    kill_child_processes, get_pid\nfrom mlcomp.worker.app import app\nfrom mlcomp.db.providers import ComputerProvider\nfrom mlcomp.db.models import ComputerUsage, Computer, Docker\nfrom mlcomp.utils.misc import memory\nfrom mlcomp.worker.sync import FileSync\n\n_session = Session.create_session(key=\'worker\')\n\n\n@click.group()\ndef main():\n    pass\n\n\ndef error_handler(f):\n    name = f.__name__\n    wrapper_vars = {\'session\': Session.create_session(key=name)}\n    wrapper_vars[\'logger\'] = create_logger(wrapper_vars[\'session\'], name)\n\n    hostname = socket.gethostname()\n\n    def wrapper():\n        try:\n            f(wrapper_vars[\'session\'], wrapper_vars[\'logger\'])\n        except Exception as e:\n            if Session.sqlalchemy_error(e):\n                Session.cleanup(name)\n\n                wrapper_vars[\'session\'] = Session.create_session(key=name)\n                wrapper_vars[\'logger\'] = create_logger(wrapper_vars[\'session\'],\n                                                       name)\n\n            wrapper_vars[\'logger\'].error(\n                traceback.format_exc(), ComponentType.WorkerSupervisor,\n                hostname\n            )\n\n    return wrapper\n\n\n@error_handler\ndef stop_processes_not_exist(session: Session, logger):\n    provider = TaskProvider(session)\n    hostname = socket.gethostname()\n    tasks = provider.by_status(\n        TaskStatus.InProgress,\n        task_docker_assigned=DOCKER_IMG,\n        computer_assigned=hostname\n    )\n    # Kill processes which does not exist\n    hostname = socket.gethostname()\n    for t in tasks:\n        if not psutil.pid_exists(t.pid):\n            # tasks can retry the execution\n            if (now() - t.last_activity).total_seconds() < 30:\n                continue\n\n            os.system(f\'kill -9 {t.pid}\')\n            t.status = TaskStatus.Failed.value\n            logger.error(\n                f\'process with pid = {t.pid} does not exist. \'\n                f\'Set task to failed state\',\n                ComponentType.WorkerSupervisor, hostname, t.id\n            )\n\n            provider.commit()\n\n            additional_info = yaml_load(t.additional_info)\n            for p in additional_info.get(\'child_processes\', []):\n                logger.info(f\'killing child process = {p}\')\n                os.system(f\'kill -9 {p}\')\n\n    # Kill processes which exist but should not\n    processes = get_pid(\'worker \')\n    ids = [p[\'PID\'] for p in processes]\n    tasks = provider.by_ids(ids)\n    tasks = {t.pid: t for t in tasks}\n\n    for p in processes:\n        pid = p[\'PID\']\n        if pid in tasks:\n            task = tasks[pid]\n            if task.status in [TaskStatus.Stopped.value,\n                               TaskStatus.Failed.value,\n                               TaskStatus.Skipped.value]:\n\n                logger.info(f\'Kill processes which exist but should not. \'\n                            f\'Pid = {pid}\')\n                os.system(f\'kill -9 {pid}\')\n\n\n@error_handler\ndef worker_usage(session: Session, logger):\n    provider = ComputerProvider(session)\n    docker_provider = DockerProvider(session)\n\n    computer = socket.gethostname()\n    docker = docker_provider.get(computer, DOCKER_IMG)\n    usages = []\n\n    count = int(10 / WORKER_USAGE_INTERVAL)\n    count = max(1, count)\n\n    for _ in range(count):\n        # noinspection PyProtectedMember\n        memory = dict(psutil.virtual_memory()._asdict())\n\n        try:\n            gpus = GPUtil.getGPUs()\n        except ValueError as err:\n            logger.info(f""Active GPUs not found: {err}"")\n            gpus = []\n\n        usage = {\n            \'cpu\': psutil.cpu_percent(),\n            \'disk\': disk(ROOT_FOLDER)[1],\n            \'memory\': memory[\'percent\'],\n            \'gpu\': [\n                {\n                    \'memory\': 0 if np.isnan(\n                        g.memoryUtil) else g.memoryUtil * 100,\n                    \'load\': 0 if np.isnan(g.load) else g.load * 100\n                } for g in gpus\n            ]\n        }\n\n        provider.current_usage(computer, usage)\n        usages.append(usage)\n        docker.last_activity = now()\n        docker_provider.update()\n\n        time.sleep(WORKER_USAGE_INTERVAL)\n\n    usage = json.dumps({\'mean\': dict_func(usages, np.mean)})\n    provider.add(ComputerUsage(computer=computer, usage=usage, time=now()))\n\n\n@main.command()\n@click.argument(\'number\', type=int)\ndef worker(number):\n    """"""\n    Start worker\n\n    :param number: worker index\n    """"""\n    name = f\'{socket.gethostname()}_{DOCKER_IMG}\'\n    argv = [\n        \'worker\', \'--loglevel=INFO\', \'-P=solo\', f\'-n={name}_{number}\',\n        \'-O fair\', \'-c=1\', \'--prefetch-multiplier=1\', \'-Q\', f\'{name},\'\n                                                            f\'{name}_{number}\'\n    ]\n    app.worker_main(argv)\n\n\n@main.command()\n@click.option(\'--workers\', type=int, default=cpu_count(),\n              help=\'count of workers\')\ndef worker_supervisor(workers: int):\n    """"""\n    Start worker supervisor.\n    This program controls workers ran on the same machine.\n    Also, it writes metric of resources consumption.\n    """"""\n    check_statuses()\n\n    host = socket.gethostname()\n\n    logger = create_logger(_session, \'worker_supervisor\')\n    logger.info(\'worker_supervisor start\',\n                ComponentType.WorkerSupervisor,\n                host)\n\n    _create_computer(workers)\n    _create_docker()\n\n    start_schedule([(stop_processes_not_exist, 10)])\n\n    if DOCKER_MAIN:\n        syncer = FileSync()\n        start_schedule([(worker_usage, 0)])\n        start_schedule([(syncer.sync, 0)])\n\n    name = f\'{host}_{DOCKER_IMG}_supervisor\'\n    argv = [\n        \'worker\', \'--loglevel=INFO\', \'-P=solo\', f\'-n={name}\', \'-O fair\',\n        \'-c=1\', \'--prefetch-multiplier=1\', \'-Q\', f\'{name}\'\n    ]\n\n    logger.info(\'worker_supervisor run celery\',\n                ComponentType.WorkerSupervisor,\n                host)\n\n    app.worker_main(argv)\n\n\n@main.command()\n@click.option(\'--daemon\', type=bool, default=False,\n              help=\'start supervisord in a daemon mode\')\n@click.option(\'--debug\', type=bool, default=False,\n              help=\'use source files instead the installed library\')\n@click.option(\'--workers\', type=int, default=cpu_count(),\n              help=\'count of workers\')\n@click.option(\'--log_level\', type=str, default=\'DEBUG\',\n              help=\'log level of supervisord\')\ndef start(daemon: bool, debug: bool, workers: int, log_level: str):\n    """"""\n       Start worker_supervisor and workers\n    """"""\n    check_statuses()\n\n    # creating supervisord config\n    supervisor_command = \'mlcomp-worker worker-supervisor\'\n    worker_command = \'mlcomp-worker worker\'\n    if debug:\n        supervisor_command = \'python mlcomp/worker/__main__.py \' \\\n                             \'worker-supervisor\'\n        worker_command = \'python mlcomp/worker/__main__.py worker\'\n\n    daemon_text = \'false\' if daemon else \'true\'\n    text = [\n        \'[supervisord]\',\n        f\'nodaemon={daemon_text}\',\n        \'\',\n        \'[program:supervisor]\',\n        f\'command={supervisor_command} --workers {workers}\',\n        \'autostart=true\',\n        \'autorestart=true\',\n        \'\'\n    ]\n    for p in range(workers):\n        text.append(f\'[program:worker{p}]\')\n        text.append(f\'command={worker_command} {p}\')\n        text.append(\'autostart=true\')\n        text.append(\'autorestart=true\')\n        text.append(\'\')\n\n    conf = os.path.join(CONFIG_FOLDER, \'supervisord.conf\')\n    with open(conf, \'w\') as f:\n        f.writelines(\'\\n\'.join(text))\n\n    os.system(f\'supervisord \' f\'-c {conf} -e {log_level}\')\n\n\n@main.command()\ndef stop():\n    """"""\n    Stop supervisord started by start command\n    """"""\n    check_statuses()\n\n    lines = os.popen(\'ps -ef | grep supervisord\').readlines()\n    for line in lines:\n        if \'mlcomp/configs/supervisord.conf\' not in line:\n            continue\n        pid = int(line.split()[1])\n        kill_child_processes(pid)\n\n\ndef _create_docker():\n    docker = Docker(\n        name=DOCKER_IMG,\n        computer=socket.gethostname(),\n        ports=\'-\'.join(list(map(str, MASTER_PORT_RANGE))),\n        last_activity=now()\n    )\n    DockerProvider(_session).create_or_update(docker, \'name\', \'computer\')\n\n\ndef _create_computer(workers: int):\n    tot_m, used_m, free_m = memory()\n    tot_d, used_d, free_d = disk(ROOT_FOLDER)\n    computer = Computer(\n        name=socket.gethostname(),\n        gpu=torch.cuda.device_count(),\n        cpu=workers,\n        memory=tot_m,\n        ip=IP,\n        port=PORT,\n        user=get_username(),\n        disk=tot_d,\n        root_folder=ROOT_FOLDER,\n        sync_with_this_computer=SYNC_WITH_THIS_COMPUTER,\n        can_process_tasks=CAN_PROCESS_TASKS\n    )\n    ComputerProvider(_session).create_or_update(computer, \'name\')\n\n\nif __name__ == \'__main__\':\n    main()\n'"
mlcomp/worker/app.py,0,"b""from __future__ import absolute_import, unicode_literals\nfrom celery import Celery\nimport os\nimport sys\n\nfrom mlcomp import REDIS_PASSWORD, REDIS_HOST, REDIS_PORT\n\nsys.path.insert(0, os.path.dirname(__file__))\n\nbroker = f'redis://:{REDIS_PASSWORD}@{REDIS_HOST}:{REDIS_PORT}/0'\n\napp = Celery(\n    'mlcomp',\n    broker=broker,\n    backend=broker,\n    include=['mlcomp.worker.tasks']\n)\n__all__ = ['app']\n"""
mlcomp/worker/storage.py,0,"b""from glob import glob\nimport os\nfrom os.path import isdir, join\nimport hashlib\nfrom typing import List, Tuple\nimport pkgutil\nimport sys\nimport pathspec\nimport pkg_resources\nimport pyclbr\nimport importlib\n\nfrom sqlalchemy.orm import joinedload\n\nfrom mlcomp import TASK_FOLDER, DATA_FOLDER, MODEL_FOLDER, INSTALL_DEPENDENCIES\nfrom mlcomp.db.core import Session\nfrom mlcomp.db.enums import ComponentType\nfrom mlcomp.db.models import DagStorage, Dag, DagLibrary, File, Task\nfrom mlcomp.utils.misc import now, to_snake\nfrom mlcomp.db.providers import FileProvider, \\\n    DagStorageProvider, \\\n    TaskProvider, \\\n    DagLibraryProvider, DagProvider\n\nfrom mlcomp.utils.config import Config\nfrom mlcomp.utils.req import control_requirements, read_lines\n\n\ndef get_super_names(cls: pyclbr.Class):\n    res = []\n    if isinstance(cls.super, list):\n        for s in cls.super:\n            if isinstance(s, str):\n                res.append(s)\n            else:\n                res.append(s.name)\n                res.extend(get_super_names(s))\n    elif isinstance(cls.super, pyclbr.Class):\n        res.append(cls.super.name)\n        res.extend(get_super_names(cls.super))\n    elif isinstance(cls, str):\n        res.append(cls)\n    return res\n\n\nclass Storage:\n    def __init__(self, session: Session, logger=None,\n                 component: ComponentType = None,\n                 max_file_size: int = 10 ** 5, max_count=10 ** 3):\n        self.file_provider = FileProvider(session)\n        self.provider = DagStorageProvider(session)\n        self.task_provider = TaskProvider(session)\n        self.library_provider = DagLibraryProvider(session)\n        self.dag_provider = DagProvider(session)\n\n        self.logger = logger\n        self.component = component\n        self.max_file_size = max_file_size\n        self.max_count = max_count\n\n    def log_info(self, message: str):\n        if self.logger:\n            self.logger.info(message, self.component)\n\n    def copy_from(self, src: int, dag: Dag):\n        storages = self.provider.query(DagStorage). \\\n            filter(DagStorage.dag == src). \\\n            all()\n        libraries = self.library_provider.query(DagLibrary). \\\n            filter(DagLibrary.dag == src). \\\n            all()\n\n        s_news = []\n        for s in storages:\n            s_new = DagStorage(\n                dag=dag.id, file=s.file, path=s.path, is_dir=s.is_dir\n            )\n            s_news.append(s_new)\n        l_news = []\n        for l in libraries:\n            l_new = DagLibrary(\n                dag=dag.id, library=l.library, version=l.version\n            )\n            l_news.append(l_new)\n\n        self.provider.add_all(s_news)\n        self.library_provider.add_all(l_news)\n\n    def _build_spec(self, folder: str):\n        ignore_file = os.path.join(folder, '.ignore')\n        if not os.path.exists(ignore_file):\n            ignore_patterns = []\n        else:\n            ignore_patterns = read_lines(ignore_file)\n        ignore_patterns.extend(\n            ['log', 'logs', '/data', '/models', '__pycache__', '*.ipynb'])\n\n        return pathspec.PathSpec.from_lines(\n            pathspec.patterns.GitWildMatchPattern, ignore_patterns\n        )\n\n    def upload(self, folder: str, dag: Dag, control_reqs: bool = True):\n        self.log_info('upload started')\n        hashs = self.file_provider.hashs(dag.project)\n        self.log_info('hashes are retrieved')\n\n        all_files = []\n        spec = self._build_spec(folder)\n\n        files = glob(os.path.join(folder, '**'))\n        for file in files[:]:\n            path = os.path.relpath(file, folder)\n            if spec.match_file(path) or path == '.':\n                continue\n            if os.path.isdir(file):\n                child_files = glob(os.path.join(folder, file, '**'),\n                                   recursive=True)\n                files.extend(child_files)\n\n        if self.max_count and len(files) > self.max_count:\n            raise Exception(f'files count = {len(files)} '\n                            f'But max count = {self.max_count}')\n\n        self.log_info('list of files formed')\n\n        folders_to_add = []\n        files_to_add = []\n        files_storage_to_add = []\n\n        total_size_added = 0\n\n        for o in files:\n            path = os.path.relpath(o, folder)\n            if spec.match_file(path) or path == '.':\n                continue\n\n            if isdir(o):\n                folder_to_add = DagStorage(dag=dag.id, path=path, is_dir=True)\n                folders_to_add.append(folder_to_add)\n                continue\n            content = open(o, 'rb').read()\n            size = sys.getsizeof(content)\n            if self.max_file_size and size > self.max_file_size:\n                raise Exception(\n                    f'file = {o} has size {size}.'\n                    f' But max size is set to {self.max_file_size}')\n            md5 = hashlib.md5(content).hexdigest()\n\n            all_files.append(o)\n\n            if md5 not in hashs:\n                file = File(\n                    md5=md5,\n                    content=content,\n                    project=dag.project,\n                    dag=dag.id,\n                    created=now()\n                )\n                hashs[md5] = file\n                files_to_add.append(file)\n                total_size_added += size\n\n            file_storage = DagStorage(\n                dag=dag.id, path=path, file=hashs[md5],\n                is_dir=False)\n            files_storage_to_add.append(file_storage)\n\n        self.log_info('inserting DagStorage folders')\n\n        if len(folders_to_add) > 0:\n            self.provider.bulk_save_objects(folders_to_add)\n\n        self.log_info('inserting Files')\n\n        if len(files_to_add) > 0:\n            self.file_provider.bulk_save_objects(files_to_add,\n                                                 return_defaults=True)\n\n        self.log_info('inserting DagStorage Files')\n\n        if len(files_storage_to_add) > 0:\n            for file_storage in files_storage_to_add:\n                if isinstance(file_storage.file, File):\n                    # noinspection PyUnresolvedReferences\n                    file_storage.file = file_storage.file.id\n\n            self.provider.bulk_save_objects(files_storage_to_add)\n\n        dag.file_size += total_size_added\n\n        self.dag_provider.update()\n\n        if INSTALL_DEPENDENCIES and control_reqs:\n            reqs = control_requirements(folder, files=all_files)\n            for name, rel, version in reqs:\n                self.library_provider.add(\n                    DagLibrary(dag=dag.id, library=name, version=version)\n                )\n\n    def download_dag(self, dag: int, folder: str):\n        os.makedirs(folder, exist_ok=True)\n\n        items = self.provider.by_dag(dag)\n        items = sorted(items, key=lambda x: x[1] is not None)\n        for item, file in items:\n            path = os.path.join(folder, item.path)\n            if item.is_dir:\n                os.makedirs(path, exist_ok=True)\n            else:\n                with open(path, 'wb') as f:\n                    f.write(file.content)\n\n    def download(self, task: int):\n        task = self.task_provider.by_id(\n            task, joinedload(Task.dag_rel, innerjoin=True)\n        )\n        folder = join(TASK_FOLDER, str(task.id))\n        self.download_dag(task.dag, folder)\n\n        config = Config.from_yaml(task.dag_rel.config)\n        info = config['info']\n        try:\n            data_folder = os.path.join(DATA_FOLDER, info['project'])\n            os.makedirs(data_folder, exist_ok=True)\n\n            os.symlink(\n                data_folder,\n                os.path.join(folder, 'data'),\n                target_is_directory=True\n            )\n        except FileExistsError:\n            pass\n\n        try:\n            model_folder = os.path.join(MODEL_FOLDER, info['project'])\n            os.makedirs(model_folder, exist_ok=True)\n\n            os.symlink(\n                model_folder,\n                os.path.join(folder, 'models'),\n                target_is_directory=True\n            )\n        except FileExistsError:\n            pass\n\n        sys.path.insert(0, folder)\n        return folder\n\n    def import_executor(\n            self,\n            folder: str,\n            base_folder: str,\n            executor: str,\n            libraries: List[Tuple] = None\n    ):\n\n        sys.path.insert(0, base_folder)\n\n        spec = self._build_spec(folder)\n        was_installation = False\n\n        folders = [\n            p for p in glob(f'{folder}/*', recursive=True)\n            if os.path.isdir(p) and not spec.match_file(p)\n        ]\n        folders += [folder]\n        library_names = set(n for n, v in (libraries or []))\n        library_versions = {n: v for n, v in (libraries or [])}\n\n        for n in library_names:\n            try:\n                version = pkg_resources.get_distribution(n).version\n                need_install = library_versions[n] != version\n            except Exception:\n                need_install = True\n\n            if INSTALL_DEPENDENCIES and need_install:\n                os.system(f'pip install {n}=={library_versions[n]}')\n                was_installation = True\n\n        def is_valid_class(cls: pyclbr.Class):\n            return cls.name == executor or \\\n                   cls.name.lower() == executor or \\\n                   to_snake(cls.name) == executor\n\n        def relative_name(path: str):\n            rel = os.path.relpath(path, base_folder)\n            parts = [str(p).split('.')[0] for p in rel.split(os.sep)]\n            return '.'.join(parts)\n\n        for (module_loader, module_name,\n             ispkg) in pkgutil.iter_modules(folders):\n            module = module_loader.find_module(module_name)\n            rel_path = os.path.relpath(\n                os.path.splitext(module.path)[0], base_folder\n            ).replace('/', '.')\n            try:\n                classes = pyclbr.readmodule(rel_path, path=[base_folder])\n            except Exception:\n                continue\n            for k, v in classes.items():\n                if is_valid_class(v):\n                    importlib.import_module(relative_name(module.path))\n                    return True, was_installation\n\n        return False, was_installation\n\n\n__all__ = ['Storage']\n"""
mlcomp/worker/sync.py,0,"b'import os\nimport socket\nimport time\nimport traceback\nimport subprocess\nfrom os.path import join\nfrom typing import List\n\nfrom mlcomp import FILE_SYNC_INTERVAL\nfrom mlcomp.db.core import Session\nfrom mlcomp.db.enums import ComponentType\nfrom mlcomp.db.models import Computer, TaskSynced\nfrom mlcomp.db.providers import ComputerProvider, \\\n    TaskSyncedProvider, DockerProvider, ProjectProvider\nfrom mlcomp.utils.logging import create_logger\nfrom mlcomp.utils.misc import now\nfrom mlcomp.utils.io import yaml_load, yaml_dump\n\n\ndef sync_directed(\n        session: Session,\n        source: Computer,\n        target: Computer,\n        folders: List\n):\n    current_computer = socket.gethostname()\n    logger = create_logger(session, __name__)\n    for folder, excluded in folders:\n        end = \' --perms  --chmod=777 --size-only\'\n        if len(excluded) > 0:\n            parts = []\n            folder_excluded = False\n            for i in range(len(excluded)):\n                if excluded[i] == folder:\n                    folder_excluded = True\n                    break\n                if not excluded[i].startswith(folder):\n                    continue\n\n                part = os.path.relpath(excluded[i], folder)\n                part = f\'--exclude {part}\'\n                parts.append(part)\n\n            if folder_excluded:\n                continue\n\n            if len(parts) > 0:\n                end += \' \' + \' \'.join(parts)\n\n        source_folder = join(source.root_folder, folder)\n        target_folder = join(target.root_folder, folder)\n\n        if current_computer == source.name:\n            command = f\'rsync -vhru -e \' \\\n                      f\'""ssh -p {target.port} -o StrictHostKeyChecking=no"" \' \\\n                      f\'{source_folder}/ \' \\\n                      f\'{target.user}@{target.ip}:{target_folder}/ {end}\'\n        elif current_computer == target.name:\n            command = f\'rsync -vhru -e \' \\\n                      f\'""ssh -p {source.port} -o StrictHostKeyChecking=no"" \' \\\n                      f\'{source.user}@{source.ip}:{source_folder}/ \' \\\n                      f\'{target_folder}/ {end}\'\n        else:\n            command = f\'rsync -vhru -e \' \\\n                      f\'""ssh -p {target.port} -o StrictHostKeyChecking=no"" \' \\\n                      f\' {source_folder}/ \' \\\n                      f\'{target.user}@{target.ip}:{target_folder}/ {end}\'\n\n            command = f\'ssh -p {source.port} \' \\\n                      f\'{source.user}@{source.ip} ""{command}""\'\n\n        logger.info(command, ComponentType.WorkerSupervisor, current_computer)\n        try:\n            subprocess.check_output(command, shell=True,\n                                    stderr=subprocess.STDOUT,\n                                    universal_newlines=True)\n        except subprocess.CalledProcessError as exc:\n            raise Exception(exc.output)\n\n\ndef copy_remote(\n        session: Session, computer_from: str, path_from: str, path_to: str\n):\n    provider = ComputerProvider(session)\n    src = provider.by_name(computer_from)\n    host = socket.gethostname()\n    if host != computer_from:\n        c = f\'scp -P {src.port} {src.user}@{src.ip}:{path_from} {path_to}\'\n    else:\n        f\'cp {path_from} {path_to}\'\n    subprocess.check_output(c, shell=True)\n    return os.path.exists(path_to)\n\n\ndef correct_folders(sync_folders: List[str], project_name: str):\n    for i in range(len(sync_folders)):\n        s = sync_folders[i]\n        parts = s.split(\'/\')\n        if parts[0] in [\'data\', \'models\']:\n            if len(parts) == 1 or parts[1] != project_name:\n                parts[0] = join(parts[0], project_name)\n        sync_folders[i] = \'/\'.join(parts)\n    return sync_folders\n\n\nclass FileSync:\n    session = Session.create_session(key=\'FileSync\')\n    logger = create_logger(session, \'FileSync\')\n\n    def process_error(self, e: Exception):\n        if Session.sqlalchemy_error(e):\n            Session.cleanup(\'FileSync\')\n            self.session = Session.create_session(key=\'FileSync\')\n            self.logger = create_logger(self.session, \'FileSync\')\n\n        hostname = socket.gethostname()\n        self.logger.error(\n            traceback.format_exc(), ComponentType.WorkerSupervisor,\n            hostname\n        )\n\n    def sync_manual(self, computer: Computer, provider: ComputerProvider):\n        """"""\n        button sync was clicked manually\n        """"""\n        if not computer.meta:\n            return\n\n        meta = yaml_load(computer.meta)\n        if \'manual_sync\' not in meta:\n            return\n\n        manual_sync = meta[\'manual_sync\']\n\n        project_provider = ProjectProvider(self.session)\n        docker_provider = DockerProvider(self.session)\n\n        dockers = docker_provider.get_online()\n        project = project_provider.by_id(manual_sync[\'project\'])\n        sync_folders = manual_sync[\'sync_folders\']\n        ignore_folders = manual_sync[\'ignore_folders\']\n\n        sync_folders = correct_folders(sync_folders, project.name)\n        ignore_folders = correct_folders(ignore_folders, project.name)\n\n        if not isinstance(sync_folders, list):\n            sync_folders = []\n        if not isinstance(ignore_folders, list):\n            ignore_folders = []\n\n        for docker in dockers:\n            if docker.computer == computer.name:\n                continue\n\n            source = provider.by_name(docker.computer)\n            folders = [[s, ignore_folders] for s in sync_folders]\n\n            computer.syncing_computer = source.name\n            provider.update()\n\n            try:\n                sync_directed(\n                    self.session,\n                    target=computer,\n                    source=source,\n                    folders=folders\n                )\n            except Exception as e:\n                self.process_error(e)\n        del meta[\'manual_sync\']\n        computer.meta = yaml_dump(meta)\n        provider.update()\n\n    def sync(self):\n        hostname = socket.gethostname()\n        try:\n            provider = ComputerProvider(self.session)\n            task_synced_provider = TaskSyncedProvider(self.session)\n\n            computer = provider.by_name(hostname)\n            sync_start = now()\n\n            if FILE_SYNC_INTERVAL == 0:\n                time.sleep(1)\n            else:\n                self.sync_manual(computer, provider)\n\n                computers = provider.all_with_last_activtiy()\n                computers = [\n                    c for c in computers\n                    if (now() - c.last_activity).total_seconds() < 10\n                ]\n                computers_names = {c.name for c in computers}\n\n                for c, project, tasks in task_synced_provider.for_computer(\n                        computer.name):\n                    if c.sync_with_this_computer:\n                        if c.name not in computers_names:\n                            self.logger.info(f\'Computer = {c.name} \'\n                                             f\'is offline. Can not sync\',\n                                             ComponentType.WorkerSupervisor,\n                                             hostname)\n                            continue\n\n                        if c.syncing_computer:\n                            continue\n\n                        sync_folders = yaml_load(project.sync_folders)\n                        ignore_folders = yaml_load(project.ignore_folders)\n\n                        sync_folders = correct_folders(sync_folders,\n                                                       project.name)\n                        ignore_folders = correct_folders(ignore_folders,\n                                                         project.name)\n\n                        if not isinstance(sync_folders, list):\n                            sync_folders = []\n                        if not isinstance(ignore_folders, list):\n                            ignore_folders = []\n\n                        folders = [[s, ignore_folders] for s in sync_folders]\n\n                        computer.syncing_computer = c.name\n                        provider.update()\n\n                        sync_directed(self.session, c, computer, folders)\n\n                    for t in tasks:\n                        task_synced_provider.add(\n                            TaskSynced(computer=computer.name, task=t.id)\n                        )\n\n                    time.sleep(FILE_SYNC_INTERVAL)\n\n            computer.last_synced = sync_start\n            computer.syncing_computer = None\n            provider.update()\n        except Exception as e:\n            self.process_error(e)\n'"
mlcomp/worker/tasks.py,0,"b'import os\nimport shutil\nimport socket\nimport time\nimport traceback\nimport sys\nfrom os.path import join, dirname, abspath\nfrom typing import List\n\nimport psutil\nfrom sqlalchemy.orm import joinedload\nfrom celery.signals import celeryd_after_setup\nfrom celery import states\n\nfrom mlcomp import MODEL_FOLDER, TASK_FOLDER, DOCKER_IMG\nfrom mlcomp.db.core import Session\nfrom mlcomp.db.enums import ComponentType, TaskStatus\nfrom mlcomp.db.models import Task, Dag\nfrom mlcomp.db.providers import TaskProvider, \\\n    DagLibraryProvider, \\\n    DockerProvider\nfrom mlcomp.utils.logging import create_logger\nfrom mlcomp.utils.io import yaml_load, yaml_dump\nfrom mlcomp.utils.misc import set_global_seed, now\nfrom mlcomp.worker.app import app\nfrom mlcomp.worker.executors import Executor\nfrom mlcomp.worker.storage import Storage\nfrom mlcomp.utils.config import Config\n\n\nclass ExecuteBuilder:\n    def __init__(self, id: int, repeat_count: int = 1, exit=True):\n        self.session = Session.create_session(key=\'ExecuteBuilder\')\n        self.id = id\n        self.repeat_count = repeat_count\n        self.logger = create_logger(self.session, \'ExecuteBuilder\')\n        self.logger_db = create_logger(self.session, \'ExecuteBuilder.db\',\n                                       console=False)\n        self.exit = exit\n\n        self.provider = None\n        self.library_provider = None\n        self.storage = None\n        self.task = None\n        self.dag = None\n        self.executor = None\n        self.hostname = None\n        self.docker_img = None\n        self.worker_index = None\n        self.queue_personal = None\n        self.config = None\n        self.executor_type = None\n\n    def info(self, msg: str, step=None):\n        self.logger.info(msg, ComponentType.Worker, self.hostname, self.id,\n                         step)\n\n    def error(self, msg: str, step=None):\n        self.logger.error(msg, ComponentType.Worker, self.hostname, self.id,\n                          step)\n\n    def warning(self, msg: str, step=None):\n        self.logger.warning(msg, ComponentType.Worker, self.hostname, self.id,\n                            step)\n\n    def debug(self, msg: str, step=None):\n        self.logger.debug(msg, ComponentType.Worker, self.hostname, self.id,\n                          step)\n\n    def create_base(self):\n        self.info(\'create_base\')\n\n        if app.current_task:\n            app.current_task.update_state(state=states.SUCCESS)\n            app.control.revoke(app.current_task.request.id, terminate=True)\n\n        self.provider = TaskProvider(self.session)\n        self.library_provider = DagLibraryProvider(self.session)\n        self.storage = Storage(self.session)\n\n        self.task = self.provider.by_id(\n            self.id, joinedload(Task.dag_rel, innerjoin=True)\n        )\n        if not self.task:\n            raise Exception(f\'task with id = {self.id} is not found\')\n\n        self.dag = self.task.dag_rel\n        self.executor = None\n        self.hostname = socket.gethostname()\n\n        self.docker_img = DOCKER_IMG\n        self.worker_index = os.getenv(\'WORKER_INDEX\', -1)\n\n        self.queue_personal = f\'{self.hostname}_{self.docker_img}_\' \\\n                              f\'{self.worker_index}\'\n\n        self.config = Config.from_yaml(self.dag.config)\n\n        set_global_seed(self.config[\'info\'].get(\'seed\', 0))\n\n        self.executor_type = self.config[\'executors\'][self.task.executor][\n            \'type\']\n\n        executor = self.config[\'executors\'][self.task.executor]\n\n        cuda_visible_devices = os.getenv(\'CUDA_VISIBLE_DEVICES\', \'\')\n        self.info(f\'Env.before execution \'\n                  f\'CUDA_VISIBLE_DEVICES={cuda_visible_devices}\')\n\n        if cuda_visible_devices.strip() != \'\':\n            gpu_assigned = self.task.gpu_assigned or \'\'\n\n            cuda_visible_devices = cuda_visible_devices.split(\',\')\n            cuda_visible_devices = \',\'.join(\n                [cuda_visible_devices[int(g)] for g in\n                 gpu_assigned.split(\',\') if g.strip() != \'\'])\n        else:\n            cuda_visible_devices = self.task.gpu_assigned\n\n        cuda_visible_devices = cuda_visible_devices or \'\'\n\n        env = {\n            \'MKL_NUM_THREADS\': 1,\n            \'OMP_NUM_THREADS\': 1,\n            \'CUDA_VISIBLE_DEVICES\': cuda_visible_devices\n        }\n        env.update(executor.get(\'env\', {}))\n\n        for k, v in env.items():\n            os.environ[k] = str(v)\n            self.info(f\'Set env. {k} = {v}\')\n\n    def check_status(self):\n        self.info(\'check_status\')\n\n        assert self.dag is not None, \'You must fetch task with dag_rel\'\n\n        if self.task.status >= TaskStatus.InProgress.value:\n            msg = f\'Task = {self.task.id}. Status = {self.task.status}, \' \\\n                  f\'before the execute_by_id invocation.\'\n            if app.current_task:\n                msg += f\' Request Id = {app.current_task.request.id}\'\n            self.error(msg)\n            return True\n\n    def change_status(self):\n        self.info(\'change_status\')\n\n        self.task.computer_assigned = self.hostname\n        self.task.pid = os.getpid()\n        self.task.worker_index = self.worker_index\n        self.task.docker_assigned = self.docker_img\n        self.task.status = TaskStatus.InProgress.value\n        self.task.started = now()\n        self.provider.commit()\n\n    def download(self):\n        self.info(\'download\')\n\n        if not self.task.debug:\n            folder = self.storage.download(task=self.id)\n        else:\n            folder = os.getcwd()\n\n        os.chdir(folder)\n\n        libraries = self.library_provider.dag(self.task.dag)\n        executor_type = self.executor_type\n\n        self.info(\'download. folder changed\')\n\n        mlcomp_executors_folder = join(dirname(abspath(__file__)), \'executors\')\n        mlcomp_base_folder = os.path.abspath(join(mlcomp_executors_folder,\n                                                  \'../../../\'))\n\n        imported, was_installation = self.storage.import_executor(\n            mlcomp_executors_folder,\n            mlcomp_base_folder,\n            executor_type)\n\n        if not imported:\n            imported, was_installation = self.storage.import_executor(\n                folder,\n                folder,\n                executor_type,\n                libraries)\n\n            if not imported:\n                raise Exception(f\'Executor = {executor_type} not found\')\n\n        self.info(\'download. executor imported\')\n\n        if was_installation and not self.task.debug:\n            if self.repeat_count > 0:\n                self.info(\'was installation. \'\n                          \'set task status to Queued. \'\n                          \'And resending the task to a queue\')\n                self.task.status = TaskStatus.Queued.value\n                self.provider.commit()\n\n                try:\n                    execute.apply_async(\n                        (self.id, self.repeat_count - 1),\n                        queue=self.queue_personal,\n                        retry=False\n                    )\n                except Exception:\n                    pass\n                finally:\n                    sys.exit()\n\n        assert Executor.is_registered(executor_type), \\\n            f\'Executor {executor_type} was not found\'\n\n    def create_executor(self):\n        self.info(\'create_executor\')\n\n        additional_info = yaml_load(self.task.additional_info) \\\n            if self.task.additional_info else dict()\n        self.executor = Executor.from_config(\n            executor=self.task.executor, config=self.config,\n            additional_info=additional_info,\n            session=self.session, logger=self.logger, logger_db=self.logger_db\n        )\n\n    def execute(self):\n        self.info(\'execute start\')\n\n        res = self.executor(task=self.task, task_provider=self.provider,\n                            dag=self.dag)\n        self.info(\'execute executor finished\')\n\n        res = res or {}\n        self.task.result = yaml_dump(res)\n        self.provider.commit()\n\n        if \'stage\' in res and \'stages\' in res:\n            index = res[\'stages\'].index(res[\'stage\'])\n            if index < len(res[\'stages\']) - 1:\n                self.executor.info(\n                    f\'stage = {res[""stage""]} done. \'\n                    f\'Go to the stage = \'\n                    f\'{res[""stages""][index + 1]}\'\n                )\n\n                time.sleep(3)\n\n                self.executor.info(f\'sending {(self.id, self.repeat_count)} \'\n                                   f\'to {self.queue_personal}\')\n\n                self.task.status = TaskStatus.Queued.value\n                self.provider.commit()\n\n                execute.apply_async(\n                    (self.id, self.repeat_count), queue=self.queue_personal,\n                    retry=False\n                )\n                return\n\n        self.executor.step.finish()\n        self.provider.change_status(self.task, TaskStatus.Success)\n\n        self.info(\'execute end\')\n\n    def build(self):\n        try:\n            self.create_base()\n\n            bad_status = self.check_status()\n            if bad_status:\n                return\n\n            self.change_status()\n\n            self.download()\n\n            self.create_executor()\n\n            self.execute()\n\n        except Exception as e:\n            step = self.executor.step.id if \\\n                (self.executor and self.executor.step) else None\n\n            if Session.sqlalchemy_error(e):\n                Session.cleanup(key=\'ExecuteBuilder\')\n                self.session = Session.create_session(key=\'ExecuteBuilder\')\n                self.logger.session = create_logger(self.session,\n                                                    \'ExecuteBuilder\')\n\n            self.error(traceback.format_exc(), step)\n            if self.task.status <= TaskStatus.InProgress.value:\n                self.provider.change_status(self.task, TaskStatus.Failed)\n            raise e\n        finally:\n            if app.current_task:\n                app.close()\n\n            if self.exit:\n                # noinspection PyProtectedMember\n                os._exit(0)\n\n\ndef execute_by_id(id: int, repeat_count=1, exit=True):\n    ex = ExecuteBuilder(id, repeat_count=repeat_count, exit=exit)\n    ex.build()\n\n\n@celeryd_after_setup.connect\ndef capture_worker_name(sender, instance, **kwargs):\n    os.environ[\'WORKER_INDEX\'] = sender.split(\'_\')[-1]\n\n\n@app.task\ndef execute(id: int, repeat_count: int = 1):\n    execute_by_id(id, repeat_count)\n\n\n@app.task\ndef kill(pid: int):\n    os.system(f\'kill -9 {pid}\')\n    print(\'KILLED\', pid)\n    return not psutil.pid_exists(pid)\n\n\n@app.task\ndef kill_all(pids: List[int]):\n    os.system(f\'kill -9 {"" "".join(map(str, pids))}\')\n    return all(not psutil.pid_exists(pid) for pid in pids)\n\n\n@app.task\ndef remove(path: str):\n    if not os.path.exists(path):\n        return\n    if os.path.isdir(path):\n        shutil.rmtree(path)\n    else:\n        os.remove(path)\n\n\ndef remove_from_all(session: Session, path: str):\n    provider = DockerProvider(session)\n    dockers = provider.get_online()\n    for docker in dockers:\n        queue = f\'{docker.computer}_{docker.name or ""default""}_supervisor\'\n        remove.apply((path,), queue=queue)\n\n\ndef remove_model(session: Session, project_name: str, model_name: str):\n    path = join(MODEL_FOLDER, project_name, model_name + \'.pth\')\n    path_weight = join(MODEL_FOLDER, project_name, model_name + \'_weight.pth\')\n\n    remove_from_all(session, path)\n    remove_from_all(session, path_weight)\n\n\ndef remove_task(session: Session, id: int):\n    path = join(TASK_FOLDER, str(id))\n    remove_from_all(session, path)\n\n\ndef remove_dag(session: Session, id: int):\n    tasks = TaskProvider(session).by_dag(id)\n    for task in tasks:\n        remove_task(session, task.id)\n\n\ndef stop(logger, session: Session, task: Task, dag: Dag):\n    provider = TaskProvider(session)\n    if task.status > TaskStatus.InProgress.value:\n        return task.status\n\n    status = TaskStatus.Stopped\n    try:\n        if task.status != TaskStatus.NotRan.value:\n            app.control.revoke(task.celery_id, terminate=True)\n        else:\n            status = TaskStatus.Skipped\n    except Exception as e:\n        if Session.sqlalchemy_error(e):\n            try:\n                logger.error(traceback.format_exc(), ComponentType.API)\n            except Exception:\n                pass\n            raise\n        logger.error(traceback.format_exc(), ComponentType.API)\n    finally:\n        if task.pid:\n            queue = f\'{task.computer_assigned}_\' \\\n                    f\'{dag.docker_img or ""default""}_supervisor\'\n            kill.apply_async((task.pid,), queue=queue, retry=False)\n\n            additional_info = yaml_load(task.additional_info)\n            for p in additional_info.get(\'child_processes\', []):\n                kill.apply_async((p,), queue=queue, retry=False)\n        provider.change_status(task, status)\n\n    return task.status\n\n\nif __name__ == \'__main__\':\n    execute(81)\n    # from task.tasks import execute\n    # execute.delay(42)\n'"
examples/digit-recognizer/executors/__init__.py,0,b''
examples/digit-recognizer/executors/infer.py,0,"b""import pickle\nfrom copy import deepcopy\n\nimport numpy as np\nimport pandas as pd\n\nfrom mlcomp.worker.executors import Executor\n\nfrom mlcomp.worker.executors.infer import Infer\nfrom mlcomp.worker.reports.classification import ClassificationReportBuilder\n\nfrom dataset import MnistDataset\nfrom experiment import Experiment\n\n\n@Executor.register\nclass InferMnist(Infer):\n    def __init__(self, **kwargs):\n        cache_names = ['y']\n        super().__init__(cache_names=cache_names, layout='img_classify',\n                         **kwargs)\n\n        if self.test:\n            self.x_source = MnistDataset(\n                file='data/test.csv',\n                transforms=Experiment.get_test_transforms(),\n                max_count=self.max_count,\n            )\n        else:\n            self.x_source = MnistDataset(\n                file='data/train.csv',\n                fold_csv='data/fold.csv',\n                is_test=True,\n                transforms=Experiment.get_test_transforms(),\n                max_count=self.max_count\n            )\n\n        self.builder = None\n        self.x = None\n        self.res = []\n        self.submit_res = []\n\n    def create_base(self):\n        self.builder = ClassificationReportBuilder(\n            session=self.session,\n            task=self.task,\n            layout=self.layout,\n            name=self.name,\n            plot_count=self.plot_count\n        )\n        self.builder.create_base()\n\n    def count(self):\n        return len(self.x_source)\n\n    def adjust_part(self, part):\n        self.x = deepcopy(self.x_source)\n        self.x.x = self.x.x[part[0]:part[1]]\n        if not self.test:\n            self.x.y = self.x.y[part[0]:part[1]]\n\n    def save(self, preds, folder: str):\n        self.res.extend(preds)\n\n    def save_final(self, folder):\n        pickle.dump(np.array(self.res),\n                    open(f'{folder}/{self.model_name}_{self.suffix}.p', 'wb'))\n\n    def submit(self, preds):\n        argmax = preds.argmax(axis=1)\n        self.submit_res.extend(\n            [{'ImageId': len(self.submit_res) + i + 1, 'Label': p} for i, p in\n             enumerate(argmax)])\n\n    def submit_final(self, folder):\n        pd.DataFrame(self.submit_res). \\\n            to_csv(f'{folder}/{self.model_name}_{self.suffix}.csv',\n                   index=False)\n\n    def _plot_main(self, preds):\n        imgs = [\n            ((row['features'][0] * 0.229 + 0.485) * 255).astype(np.uint8)\n            for row in self.x\n        ]\n        attrs = [\n            {\n                'attr1': p.argmax()\n            } for p in preds\n        ]\n\n        self.builder.process_pred(\n            imgs=imgs,\n            preds=preds,\n            attrs=attrs\n        )\n\n    def plot(self, preds):\n        self._plot_main(preds)\n"""
examples/digit-recognizer/executors/valid.py,0,"b""from copy import deepcopy\n\nimport numpy as np\nimport cv2\n\nfrom mlcomp.worker.executors import Executor\nfrom mlcomp.worker.executors.valid import Valid\nfrom mlcomp.worker.reports.classification import ClassificationReportBuilder\n\nfrom dataset import MnistDataset\nfrom experiment import Experiment\n\n\n@Executor.register\nclass ValidMnist(Valid):\n    def __init__(self, **kwargs):\n        cache_names = ['y']\n        super().__init__(cache_names=cache_names, layout='img_classify',\n                         **kwargs)\n\n        self.x_source = MnistDataset(\n            file='data/train.csv',\n            fold_csv='data/fold.csv',\n            is_test=True,\n            max_count=self.max_count,\n            transforms=Experiment.get_test_transforms()\n        )\n        self.builder = None\n        self.x = None\n        self.scores = []\n\n    def create_base(self):\n        self.builder = ClassificationReportBuilder(\n            session=self.session,\n            task=self.task,\n            layout=self.layout,\n            name=self.name,\n            plot_count=self.plot_count\n        )\n        self.builder.create_base()\n\n    def count(self):\n        return len(self.x_source)\n\n    def score(self, preds):\n        # noinspection PyUnresolvedReferences\n        res = (preds.argmax(axis=1) == self.x.y).astype(np.float)\n        self.scores.extend(res)\n        return res\n\n    def score_final(self):\n        return np.mean(self.scores)\n\n    def adjust_part(self, part):\n        self.x = deepcopy(self.x_source)\n        self.x.x = self.x.x[part[0]:part[1]]\n        self.x.y = self.x.y[part[0]:part[1]]\n\n    def plot(self, preds, scores):\n        imgs = [\n            cv2.cvtColor(\n                ((row['features'][0] * 0.229 + 0.485) * 255).astype(np.uint8),\n                cv2.COLOR_GRAY2BGR\n            ) for row in self.x\n        ]\n\n        attrs = [\n            {\n                'attr1': p.argmax(),\n                'attr2': t\n            } for p, t in zip(preds, self.x.y)\n        ]\n\n        self.builder.process_pred(\n            imgs=imgs,\n            preds=preds,\n            targets=self.x.y,\n            attrs=attrs,\n            scores={'accuracy': scores}\n        )\n\n    def plot_final(self, score):\n        self.builder.process_scores({'accuracy': score})\n"""
mlcomp/contrib/catalyst/__init__.py,0,"b""from .register import register\n\n__all__ = ['register']\n"""
mlcomp/contrib/catalyst/register.py,0,"b""from catalyst.dl import registry\nfrom catalyst.contrib.models.cv.segmentation import (\n    Unet, ResnetLinknet, MobileUnet, ResnetUnet, ResnetFPNUnet, ResnetPSPnet,\n    FPNUnet, Linknet, PSPnet,\n    ResNetLinknet)\n\nfrom mlcomp.contrib.criterion import RingLoss\nfrom mlcomp.contrib.catalyst.callbacks.inference import InferBestCallback\nfrom mlcomp.contrib.catalyst.optim import OneCycleCosineAnnealLR\nfrom mlcomp.contrib.model.segmentation_model_pytorch import \\\n            SegmentationModelPytorch\nfrom mlcomp.contrib.model import Pretrained\nfrom mlcomp.contrib.segmentation.deeplabv3.deeplab import DeepLab\n\n\ndef register():\n    registry.Criterion(RingLoss)\n\n    registry.Callback(InferBestCallback)\n\n    registry.Scheduler(OneCycleCosineAnnealLR)\n\n    # classification\n    registry.Model(Pretrained)\n\n    # segmentation\n    registry.Model(Unet)\n    registry.Model(ResnetLinknet)\n    registry.Model(MobileUnet)\n    registry.Model(ResnetUnet)\n    registry.Model(ResnetFPNUnet)\n    registry.Model(ResnetPSPnet)\n    registry.Model(FPNUnet)\n    registry.Model(Linknet)\n    registry.Model(PSPnet)\n    registry.Model(ResNetLinknet)\n\n    registry.Model(SegmentationModelPytorch)\n    registry.Model(DeepLab)\n\n\n__all__ = ['register']\n"""
mlcomp/contrib/complex/__init__.py,0,b''
mlcomp/contrib/complex/infer.py,5,"b""from typing import List\n\nimport cv2\nimport torch\nimport albumentations as A\nimport numpy as np\nfrom tqdm import tqdm\n\nfrom mlcomp.contrib.transform.albumentations import ChannelTranspose\nfrom mlcomp.contrib.model import Pretrained\nfrom mlcomp.contrib.model.timm import Timm\n\n\ndef infer(\n        files: List[str],\n        checkpoint: str,\n        model=None,\n        class_='Pretrained',\n        variant: str = 'resnet34',\n        activation: str = 'softmax',\n        num_classes: int = 2,\n        batch_size: int = 16,\n        device: str = 'cuda',\n        transforms=None):\n    assert activation in ['softmax', 'sigmoid', None]\n\n    if model is None:\n        if class_ == 'Pretrained':\n            model = Pretrained(variant=variant, num_classes=num_classes)\n        elif class_ == 'Timm':\n            model = Timm(variant=variant, num_classes=num_classes)\n        else:\n            raise Exception('unknown model class')\n    if transforms is None:\n        transforms = A.Compose([\n            A.Resize(224, 224),\n            A.Normalize(mean=(0.485, 0.456, 0.406),\n                        std=(0.229, 0.224, 0.225)\n                        ),\n            ChannelTranspose()\n        ])\n\n    checkpoint = torch.load(checkpoint,\n                            map_location=torch.device(device))\n    model.load_state_dict(checkpoint['model_state_dict'])\n    model = model.to(device)\n    model.eval()\n\n    imgs = []\n    preds = []\n\n    def predict_batch(imgs):\n        if len(imgs) == 0:\n            return\n\n        imgs_t = np.array(\n            [transforms(image=img)['image'] for img in imgs]\n        )\n        tensor = torch.from_numpy(imgs_t).to(device)\n        pred = model(tensor)\n        if activation == 'softmax':\n            pred = torch.softmax(pred, dim=1)\n        elif activation == 'sigmoid':\n            pred = torch.sigmoid(pred)\n        pred = pred.detach().cpu().numpy()\n        preds.extend(pred.tolist())\n\n        imgs.clear()\n\n    for file in tqdm(files):\n        img = cv2.imread(file)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        imgs.append(img)\n\n        if len(imgs) >= batch_size:\n            predict_batch(imgs)\n\n    predict_batch(imgs)\n    return np.array(preds)\n\n\nif __name__ == '__main__':\n    print(infer([\n        '/home/light/projects/deepfake/data/eyes/dfdc_train_part_49/uowiocuqqt.mp4/0.tar/0_0.jpg'\n    ], checkpoint='/home/light/mlcomp/models/deepfake/best.pth'))\n"""
mlcomp/contrib/criterion/__init__.py,0,"b""from .ring import RingLoss\n\n__all__ = ['RingLoss']\n"""
mlcomp/contrib/criterion/ce.py,2,"b""from torch.nn import CrossEntropyLoss\nfrom torch.nn.functional import nll_loss, log_softmax\n\n\nclass LabelSmoothingCrossEntropy(CrossEntropyLoss):\n    def __init__(self, eps: float = 0.1, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.eps = eps\n\n    def forward(self, output, target):\n        c = output.size()[-1]\n        log_preds = log_softmax(output, dim=-1)\n        if self.reduction == 'sum':\n            loss = -log_preds.sum()\n        else:\n            loss = -log_preds.sum(dim=-1)\n            if self.reduction == 'mean':\n                loss = loss.mean()\n        nl = nll_loss(log_preds, target, reduction=self.reduction)\n        return loss * self.eps / c + (1 - self.eps) * nl\n\n\n__all__ = ['LabelSmoothingCrossEntropy']\n"""
mlcomp/contrib/criterion/ring.py,7,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn.parameter import Parameter\nfrom torch.nn.modules.loss import CrossEntropyLoss\n\n\nclass RingLoss(nn.Module):\n    def __init__(self, type=\'auto\', loss_weight=1.0, softmax_loss_weight=1.0):\n        """"""\n        :param type: type of loss (\'l1\', \'l2\', \'auto\')\n        :param loss_weight: weight of loss, for \'l1\' and \'l2\', try with 0.01.\n            For \'auto\', try with 1.0.\n\n        Source: https://github.com/Paralysis/ringloss\n        """"""\n        super().__init__()\n        self.radius = Parameter(torch.Tensor(1))\n        self.radius.data.fill_(-1)\n        self.loss_weight = loss_weight\n        self.type = type\n        self.softmax = CrossEntropyLoss()\n        self.softmax_loss_weight = softmax_loss_weight\n\n    def forward(self, x, y):\n        softmax = self.softmax(x, y).mul_(self.softmax_loss_weight)\n        x = x.pow(2).sum(dim=1).pow(0.5)\n        if self.radius.data[0] < 0:\n            self.radius.data.fill_(x.mean().data)\n        if self.type == \'l1\':\n            loss1 = F.smooth_l1_loss(x, self.radius.expand_as(x)). \\\n                mul_(self.loss_weight)\n            loss2 = F.smooth_l1_loss(self.radius.expand_as(x), x). \\\n                mul_(self.loss_weight)\n            ringloss = loss1 + loss2\n        elif self.type == \'auto\':\n            diff = x.sub(self.radius.expand_as(x)) / \\\n                   (x.mean().detach().clamp(min=0.5))\n            diff_sq = torch.pow(torch.abs(diff), 2).mean()\n            ringloss = diff_sq.mul_(self.loss_weight)\n        else:  # L2 Loss, if not specified\n            diff = x.sub(self.radius.expand_as(x))\n            diff_sq = torch.pow(torch.abs(diff), 2).mean()\n            ringloss = diff_sq.mul_(self.loss_weight)\n        return softmax + ringloss\n\n\n__all__ = [\'RingLoss\']\n'"
mlcomp/contrib/criterion/triplet.py,6,"b""import torch\nimport torch.nn.functional as F\nfrom catalyst.contrib.nn.criterion.functional import cosine_distance, \\\n    batch_all, _EPS\n\n\ndef triplet_loss(\n        embeddings: torch.Tensor, labels: torch.Tensor, margin: float = 0.3,\n        reduction='mean'\n) -> torch.Tensor:\n    cosine_dists = cosine_distance(embeddings)\n    mask = batch_all(labels)\n\n    anchor_positive_dist = cosine_dists.unsqueeze(2)\n    anchor_negative_dist = cosine_dists.unsqueeze(1)\n    triplet_loss_value = \\\n        F.relu(anchor_positive_dist - anchor_negative_dist + margin)\n    triplet_loss_value = torch.mul(triplet_loss_value, mask)\n\n    if reduction == 'mean':\n        num_positive_triplets = torch.gt(\n            triplet_loss_value,\n            _EPS).sum().float()\n        triplet_loss_value = (\n                triplet_loss_value.sum() / (num_positive_triplets + _EPS)\n        )\n    elif reduction == 'none':\n        triplet_loss_value = torch.sum(triplet_loss_value, dim=[1, 2])\n    else:\n        raise Exception(f'Unknown reduction scheme {reduction}')\n    return triplet_loss_value\n"""
mlcomp/contrib/dataset/__init__.py,0,b''
mlcomp/contrib/dataset/classify.py,1,"b""import ast\nfrom os.path import join\nfrom numbers import Number\nfrom collections import defaultdict\nimport os\nfrom typing import Callable, Dict\n\nimport tifffile\nimport numpy as np\nimport pandas as pd\nimport cv2\n\nfrom torch.utils.data import Dataset\n\n\nclass ImageDataset(Dataset):\n    def __init__(\n            self,\n            *,\n            img_folder: str,\n            fold_csv: str = None,\n            fold: int = None,\n            is_test: bool = False,\n            gray_scale: bool = False,\n            num_classes=2,\n            max_count=None,\n            meta_cols=(),\n            transforms=None,\n            postprocess_func: Callable[[Dict], Dict] = None,\n            include_image_orig=False\n    ):\n        self.img_folder = img_folder\n\n        if fold_csv:\n            df = pd.read_csv(fold_csv)\n            if fold is not None:\n                if is_test:\n                    self.data = df[df['fold'] == fold]\n                else:\n                    self.data = df[df['fold'] != fold]\n            else:\n                self.data = df\n        else:\n            self.data = pd.DataFrame(\n                {'image': os.listdir(img_folder)}).sort_values(by='image')\n\n        self.data = self.data.to_dict(orient='row')\n        if max_count is not None:\n            self.apply_max_count(max_count)\n\n        for row in self.data:\n            self.preprocess_row(row)\n\n        self.transforms = transforms\n        self.gray_scale = gray_scale\n        self.num_classes = num_classes\n        self.meta_cols = meta_cols\n        self.postprocess_func = postprocess_func\n        self.include_image_orig = include_image_orig\n\n    def apply_max_count(self, max_count):\n        if isinstance(max_count, Number):\n            self.data = self.data[:max_count]\n        else:\n            data = defaultdict(list)\n            for row in self.data:\n                data[row['label']].append(row)\n            min_index = np.argmin(max_count)\n            min_count = len(data[min_index])\n            for k, v in data.items():\n                count = int(min_count * (max_count[k] / max_count[min_index]))\n                data[k] = data[k][:count]\n\n            self.data = [v for i in range(len(data)) for v in data[i]]\n\n    def preprocess_row(self, row: dict):\n        row['image'] = join(self.img_folder, row['image'])\n\n    def __len__(self):\n        return len(self.data)\n\n    def _get_item_before_transform(self, row: dict, item: dict):\n        pass\n\n    def _get_item_after_transform(self, row: dict,\n                                  transformed: dict,\n                                  res: dict):\n        if 'label' in row:\n            res['targets'] = ast.literal_eval(str(row['label']))\n            if isinstance(res['targets'], list):\n                res['targets'] = np.array(res['targets'], dtype=np.float32)\n\n    def __getitem__(self, index):\n        row = self.data[index]\n        image = self.read_image_file(row['image'], self.gray_scale)\n        item = {'image': image}\n\n        self._get_item_before_transform(row, item)\n\n        if self.transforms:\n            item = self.transforms(**item)\n        if self.gray_scale:\n            item['image'] = np.expand_dims(item['image'], axis=0)\n        res = {\n            'features': item['image'].astype(np.float32),\n            'image_file': row['image']\n        }\n        if self.include_image_orig:\n            res['image'] = image\n\n        for c in self.meta_cols:\n            res[c] = row[c]\n\n        self._get_item_after_transform(row, item, res)\n        if self.postprocess_func:\n            res = self.postprocess_func(res)\n        return res\n\n    @staticmethod\n    def read_image_file(path: str, gray_scale=False):\n        if not os.path.exists(path):\n            raise Exception(f'Image at path {path} does not exist')\n\n        if path.endswith('.tiff') and not gray_scale:\n            return tifffile.imread(path)\n        elif path.endswith('.npy'):\n            return np.load(path)\n        else:\n            if gray_scale:\n                img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n                assert img is not None, \\\n                    f'Image at path {path} is empty'\n                return img.astype(np.uint8)\n            else:\n                img = cv2.imread(path)\n                assert img is not None, \\\n                    f'Image at path {path} is empty'\n                return cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n\n__all__ = ['ImageDataset']\n"""
mlcomp/contrib/dataset/segment.py,0,"b""from os.path import join\n\nimport cv2\nimport numpy as np\n\nfrom mlcomp.contrib.dataset.classify import ImageDataset\n\n\nclass ImageWithMaskDataset(ImageDataset):\n    def __init__(\n            self, *, mask_folder: str, crop_positive=None, encode=True,\n            include_binary=False, **kwargs\n    ):\n        assert mask_folder, 'Mask folder is required'\n        self.mask_folder = mask_folder\n        self.crop_positive = crop_positive\n        self.encode = encode\n        self.include_binary = include_binary\n        if not encode and kwargs.get('num_classes', 1) > 1:\n            kwargs['num_classes'] += 1\n\n        super().__init__(**kwargs)\n\n    def preprocess_row(self, row: dict):\n        row['image'] = join(self.img_folder, row['image'])\n        row['mask'] = join(self.mask_folder, row['mask'])\n\n    def _get_item_before_transform(self, row: dict, item: dict):\n        if 'mask' in row and self.mask_folder:\n            item['mask'] = self.read_image_file(row['mask'], True)\n            self._process_crop_positive(item)\n\n    def _get_item_after_transform(\n            self, row: dict, transformed: dict, res: dict\n    ):\n        if 'mask' in transformed:\n            mask = transformed['mask'].astype(np.int64)\n            if len(mask.shape) == 2:\n                mask_encoded = np.zeros(\n                    (self.num_classes, *mask.shape), dtype=np.float32\n                )\n                if self.num_classes == 1:\n                    mask = (mask >= 1).astype(np.uint8)\n\n                if self.encode:\n                    for i in range(1, self.num_classes + 1):\n                        mask_encoded[i - 1] = mask == i\n\n                    mask = mask_encoded\n                else:\n                    for i in range(1, self.num_classes + 1):\n                        mask_encoded[i - 1] = mask == i\n\n                    res['targets_encoded'] = mask_encoded\n\n            res['targets'] = mask.astype(np.float32)\n\n            if self.include_binary:\n                for i, c in enumerate(mask):\n                    res[f'empty_{i}'] = int(c.sum() == 0)\n                res['empty_all'] = int(mask.sum() == 0)\n\n    def _process_crop_positive(self, item: dict):\n        if not self.crop_positive:\n            return\n        mask = item['mask']\n        size = mask.shape\n\n        # import matplotlib.pyplot as plt\n        # plt.imshow(item['image'])\n        # plt.show()\n        # plt.imshow(item['mask'] * 53)\n        # plt.show()\n        crop_pos_y = self.crop_positive[0]\n        if type(crop_pos_y) == tuple:\n            crop_pos_y = np.random.randint(crop_pos_y[0], crop_pos_y[1])\n\n        crop_pos_x = self.crop_positive[1]\n        if type(crop_pos_x) == tuple:\n            crop_pos_x = np.random.randint(crop_pos_x[0], crop_pos_x[1])\n\n        p = self.crop_positive[2]\n\n        if mask.sum() == 0 or np.random.uniform(0, 1) < p:\n            min_y = 0\n            max_y = size[0] - crop_pos_y\n            min_x = 0\n            max_x = size[1] - crop_pos_x\n        else:\n            mask_binary = (mask > 0).astype(np.uint8)\n            contours, _ = cv2.findContours(\n                mask_binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n            )\n            contour = contours[np.random.randint(0, len(contours))]\n            rect = cv2.boundingRect(contour)\n            min_y = rect[1] - crop_pos_y + int(rect[3] // 2)\n            max_y = rect[1] + int(rect[3] // 2)\n\n            min_x = rect[0] - crop_pos_x + int(rect[2] // 2)\n            max_x = rect[0] + int(rect[2] // 2)\n\n        min_x = max(0, min_x)\n        min_y = max(0, min_y)\n\n        max_x = min(max_x, size[1] - crop_pos_x)\n        max_y = min(max_y, size[0] - crop_pos_y)\n\n        x = np.random.randint(min_x, max_x + 1)\n        y = np.random.randint(min_y, max_y + 1)\n        item['image'] = item['image'][y:y + crop_pos_y, x:x + crop_pos_x]\n\n        item['mask'] = item['mask'][y:y + crop_pos_y, x:x + crop_pos_x]\n\n        # plt.imshow(item['image'])\n        # plt.show()\n        # plt.imshow(item['mask'] * 53)\n        # plt.show()\n"""
mlcomp/contrib/dataset/video.py,1,"b""import os\nimport pickle\nimport random\nimport warnings\nfrom collections import defaultdict\nfrom numbers import Number\nfrom os.path import join\nfrom typing import Callable, Dict, List\n\nimport cv2\nimport numpy as np\nimport pandas as pd\nfrom torch.utils.data import Dataset\nfrom torchvision.datasets.video_utils import VideoClips\n\nwarnings.filterwarnings('ignore')\n\n\nclass VideoClipsFolder:\n    def __init__(self, video_paths: List[str], clip_length_in_frames: int,\n                 frames_between_clips: int,\n                 _precomputed_metadata: dict = None):\n        self.video_paths = video_paths\n        self.clip_length_in_frames = clip_length_in_frames\n        self.frames_between_clips = frames_between_clips\n        self.cumulative_sizes = []\n        self.clips = []\n        self.metadata = _precomputed_metadata\n        self.compute_clips()\n\n    # noinspection PyTypeChecker\n    def compute_clips(self):\n        if self.metadata is not None:\n            self.clips = self.metadata['clips']\n            self.cumulative_sizes = self.metadata[\n                'cumulative_sizes']\n            self.video_paths = self.metadata['video_paths']\n            return\n\n        clips_size = 0\n        for video_index, folder in enumerate(self.video_paths):\n            files = sorted(os.listdir(folder))\n            assert len(files) >= self.clip_length_in_frames, \\\n                f'folder = {folder} has only {len(files)} files'\n\n            for i in range(0, len(files) - self.clip_length_in_frames,\n                           self.frames_between_clips):\n                clips_size += 1\n                self.clips.append({\n                    'video_index': video_index,\n                    'min_index': i,\n                    'max_index': i + self.clip_length_in_frames\n                })\n\n            self.cumulative_sizes.append(clips_size)\n\n        self.metadata = {\n            'clips': self.clips,\n            'cumulative_sizes': self.cumulative_sizes,\n            'video_paths': self.video_paths\n        }\n\n    def get_clip(self, index: int):\n        clip = self.clips[index]\n        files = sorted(os.listdir(self.video_paths[clip['video_index']]))\n        imgs = []\n        for index in range(clip['min_index'], clip['max_index']):\n            file = join(self.video_paths[clip['video_index']], files[index])\n            img = cv2.imread(file)\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n            imgs.append(img)\n        return imgs, None, None, clip['video_index']\n\n\nclass VideoDataset(Dataset):\n    def __init__(\n            self,\n            *,\n            video_folder: str,\n            fold_csv: str = None,\n            fold: int = None,\n            is_test: bool = False,\n            clip_length_in_frames: int = 1,\n            frames_between_clips: int = 1,\n            num_classes=2,\n            max_count=None,\n            transforms=None,\n            postprocess_func: Callable[[Dict], Dict] = None,\n            clips_per_video: int = 1,\n            metadata_path: str = None\n    ):\n        self.video_folder = video_folder\n\n        if fold_csv:\n            df = pd.read_csv(fold_csv)\n            if fold is not None:\n                if is_test:\n                    self.data = df[df['fold'] == fold]\n                else:\n                    self.data = df[df['fold'] != fold]\n            else:\n                self.data = df\n        else:\n            self.data = pd.DataFrame(\n                {'video': os.listdir(video_folder)}).sort_values(by='video')\n\n        self.data = self.data.sample(frac=1)\n        self.data = self.data.to_dict(orient='row')\n        if max_count is not None:\n            self.apply_max_count(max_count)\n\n        for row in self.data:\n            self.preprocess_row(row)\n\n        self.transforms = transforms\n        self.num_classes = num_classes\n        self.postprocess_func = postprocess_func\n        self.clip_length_in_frames = clip_length_in_frames\n        self.frames_between_clips = frames_between_clips\n        self.clips_per_video = clips_per_video\n        self.metadata_path = metadata_path\n\n        self.video_paths = [row['video'] for row in self.data]\n        self.scan_mode = any(os.path.isdir(v) for v in self.video_paths)\n        self.clips = self.create_clips()\n\n    def create_clips(self):\n        precomputed_metadata = None\n        if self.metadata_path:\n            if os.path.exists(self.metadata_path):\n                precomputed_metadata = pickle.load(\n                    open(self.metadata_path, 'rb'))\n                if precomputed_metadata is not None:\n                    if len(precomputed_metadata['video_paths']) != len(\n                            self.video_paths):\n                        precomputed_metadata = None\n\n                    if precomputed_metadata is not None and any(\n                            [p1 != p2 for p1, p2 in zip(self.video_paths,\n                                                        precomputed_metadata[\n                                                            'video_paths'])]):\n                        precomputed_metadata = None\n\n        if precomputed_metadata is None:\n            print('computing metadata')\n\n        cls = VideoClipsFolder if self.scan_mode else VideoClips\n        clips = cls(\n            video_paths=self.video_paths,\n            clip_length_in_frames=self.clip_length_in_frames,\n            frames_between_clips=self.frames_between_clips,\n            _precomputed_metadata=precomputed_metadata\n        )\n\n        if precomputed_metadata is None and self.metadata_path:\n            folder = os.path.dirname(self.metadata_path)\n            os.makedirs(folder, exist_ok=True)\n            pickle.dump(clips.metadata, open(self.metadata_path, 'wb'))\n\n        return clips\n\n    def apply_max_count(self, max_count):\n        if isinstance(max_count, Number):\n            self.data = self.data[:max_count]\n        else:\n            data = defaultdict(list)\n            for row in self.data:\n                data[row['label']].append(row)\n            min_index = np.argmin(max_count)\n            min_count = len(data[min_index])\n            for k, v in data.items():\n                count = int(min_count * (max_count[k] / max_count[min_index]))\n                data[k] = data[k][:count]\n\n            self.data = [v for i in range(len(data)) for v in data[i]]\n\n    def preprocess_row(self, row: dict):\n        row['video'] = join(self.video_folder, row['video'])\n\n    def __len__(self):\n        return len(self.clips.video_paths) * self.clips_per_video\n\n    def __getitem__(self, index):\n        video_index = index // self.clips_per_video\n        max_clip_index = self.clips.cumulative_sizes[video_index]\n        min_clip_index = self.clips.cumulative_sizes[\n            video_index - 1] if video_index > 0 else 0\n\n        clip_index = np.random.randint(min_clip_index, max_clip_index)\n        video, audio, info, _ = self.clips.get_clip(clip_index)\n        row = self.data[video_index]\n\n        if self.transforms:\n            seed = random.randint(0, 10 ** 6)\n            frames = []\n            if not self.scan_mode:\n                # noinspection PyUnresolvedReferences\n                video = video.numpy()\n\n            for v in video:\n                random.seed(seed)\n                frames.append(self.transforms(image=v)['image'][None])\n\n            video = np.vstack(frames)\n\n        res = {\n            'features': video,\n            'video_file': row['video']\n        }\n        if 'label' in row:\n            res['targets'] = row['label']\n\n        if self.postprocess_func:\n            res = self.postprocess_func(res)\n        return res\n\n\n__all__ = ['VideoDataset']\n"""
mlcomp/contrib/metrics/__init__.py,0,b''
mlcomp/contrib/metrics/dice.py,0,"b'import numpy as np\n\n\ndef dice_numpy(targets, outputs, threshold=None, min_area=None,\n               empty_one: bool = True, eps=1e-6):\n    if threshold is not None:\n        # noinspection PyUnresolvedReferences\n        outputs = (outputs >= threshold).astype(np.uint8)\n\n    targets_sum = targets.sum()\n    outputs_sum = outputs.sum()\n\n    if min_area and outputs_sum < min_area:\n        outputs = np.zeros(outputs.shape, dtype=np.uint8)\n        outputs_sum = 0\n\n    if empty_one and targets_sum == 0 and outputs_sum == 0:\n        return 1\n\n    intersection = (targets * outputs).sum()\n    union = targets_sum + outputs_sum\n    dice = 2 * intersection / (union + eps)\n    return dice\n'"
mlcomp/contrib/model/__init__.py,0,"b""from .pretrained import Pretrained\n\n__all__ = ['Pretrained']\n"""
mlcomp/contrib/model/efficientnet.py,2,"b'from efficientnet_pytorch import EfficientNet as _EfficientNet\n\nimport torch.nn as nn\n\nfrom mlcomp.contrib.torch.layers import LambdaLayer\n\n\nclass EfficientNet(nn.Module):\n    def __init__(self, variant, num_classes, pretrained=True, activation=None):\n        super().__init__()\n        if \'efficientnet\' not in variant:\n            variant = f\'efficientnet-{variant}\'\n\n        if pretrained:\n            model = _EfficientNet.from_pretrained(variant,\n                                                  num_classes=num_classes)\n        else:\n            model = _EfficientNet.from_name(variant, {\n                \'num_classes\': num_classes\n            })\n        self.model = model\n\n        self.model._fc = nn.Sequential(\n            LambdaLayer(lambda x: x.unsqueeze_(0)),\n            nn.AdaptiveAvgPool1d(self.model._fc.in_features),\n            LambdaLayer(lambda x: x.squeeze_(0).view(x.size(0), -1)),\n            self.model._fc\n        )\n\n        if callable(activation) or activation is None:\n            self.activation = activation\n        elif activation == \'softmax\':\n            self.activation = nn.Softmax(dim=1)\n        elif activation == \'sigmoid\':\n            self.activation = nn.Sigmoid()\n        else:\n            raise ValueError(\n                \'Activation should be ""sigmoid""/""softmax""/callable/None\')\n\n    def forward(self, x):\n        res = self.model(x)\n        if isinstance(res, tuple):\n            res = res[0]\n        if self.activation:\n            res = self.activation(res)\n        return res\n\n\n__all__ = [\'EfficientNet\']\n'"
mlcomp/contrib/model/pretrained.py,2,"b'import torch.nn as nn\n\nimport pretrainedmodels\n\nfrom mlcomp.contrib.torch.layers import LambdaLayer\n\n\nclass Pretrained(nn.Module):\n    def __init__(self, variant, num_classes, pretrained=True, activation=None):\n        super().__init__()\n        params = {\'num_classes\': 1000}\n        if not pretrained:\n            params[\'pretrained\'] = None\n\n        model = pretrainedmodels.__dict__[variant](**params)\n        self.model = model\n        linear = self.model.last_linear\n\n        if isinstance(linear, nn.Linear):\n            self.model.last_linear = nn.Linear(\n                model.last_linear.in_features,\n                num_classes\n            )\n            self.model.last_linear.in_channels = linear.in_features\n        elif isinstance(linear, nn.Conv2d):\n            self.model.last_linear = nn.Conv2d(\n                linear.in_channels,\n                num_classes,\n                kernel_size=linear.kernel_size,\n                bias=True\n            )\n            self.model.last_linear.in_features = linear.in_channels\n\n        self.model.last_linear = nn.Sequential(\n            LambdaLayer(lambda x: x.unsqueeze_(0)),\n            nn.AdaptiveAvgPool1d(self.model.last_linear.in_channels),\n            LambdaLayer(lambda x: x.squeeze_(0).view(x.size(0), -1)),\n            self.model.last_linear\n        )\n\n        if callable(activation) or activation is None:\n            self.activation = activation\n        elif activation == \'softmax\':\n            self.activation = nn.Softmax(dim=1)\n        elif activation == \'sigmoid\':\n            self.activation = nn.Sigmoid()\n        else:\n            raise ValueError(\n                \'Activation should be ""sigmoid""/""softmax""/callable/None\')\n\n    def forward(self, x):\n        res = self.model(x)\n        if isinstance(res, tuple):\n            res = res[0]\n        if self.activation:\n            res = self.activation(res)\n        return res\n\n\n__all__ = [\'Pretrained\']\n'"
mlcomp/contrib/model/segmentation_model_pytorch.py,0,"b""from torch import nn\n\nimport mlcomp.contrib.segmentation as smb\n\n\nclass SegmentationModelPytorch(nn.Module):\n    def __init__(\n        self,\n        arch: str,\n        encoder: str,\n        num_classes: int = 1,\n        encoder_weights: str = 'imagenet',\n        activation=None,\n        **kwargs\n    ):\n        super().__init__()\n\n        model = getattr(smb, arch)\n        self.model = model(\n            encoder_name=encoder,\n            classes=num_classes,\n            encoder_weights=encoder_weights,\n            activation=activation,\n            **kwargs\n        )\n\n    def forward(self, x):\n        res = self.model.forward(x)\n        if self.model.activation:\n            res = self.model.activation(res)\n        return res\n\n\n__all__ = ['SegmentationModelPytorch']\n"""
mlcomp/contrib/model/timm.py,1,"b'import timm\nimport torch.nn as nn\n\n\nclass Timm(nn.Module):\n    def __init__(self, variant, num_classes, pretrained=True, activation=None):\n        super().__init__()\n\n        model = timm.create_model(\n            variant, pretrained=pretrained,\n            num_classes=num_classes)\n\n        self.model = model\n        # self.model.fc = nn.Sequential(\n        #     LambdaLayer(lambda x: x.unsqueeze_(0)),\n        #     nn.AdaptiveAvgPool1d(self.model.fc.in_features),\n        #     LambdaLayer(lambda x: x.squeeze_(0).view(x.size(0), -1)),\n        #     self.model.fc\n        # )\n\n        if callable(activation) or activation is None:\n            self.activation = activation\n        elif activation == \'softmax\':\n            self.activation = nn.Softmax(dim=1)\n        elif activation == \'sigmoid\':\n            self.activation = nn.Sigmoid()\n        else:\n            raise ValueError(\n                \'Activation should be ""sigmoid""/""softmax""/callable/None\')\n\n    def forward(self, x):\n        res = self.model(x)\n        if isinstance(res, tuple):\n            res = res[0]\n        if self.activation:\n            res = self.activation(res)\n        return res\n\n\n__all__ = [\'Timm\']\n'"
mlcomp/contrib/sampler/__init__.py,0,b''
mlcomp/contrib/sampler/balanced.py,1,"b'from collections import defaultdict\nfrom typing import List, Iterator\nimport numpy as np\nfrom torch.utils.data import Sampler\n\n\nclass BalanceClassSampler(Sampler):\n    """"""\n    Abstraction over data sampler. Allows you to create stratified sample\n    on unbalanced classes.\n    """"""\n\n    def __init__(\n            self,\n            labels: List,\n            mode: str = \'downsampling\',\n            count_per_class: dict = None,\n            max_count: int = None\n    ):\n        """"""\n        Args:\n            labels (List[int]): list of class label\n                for each elem in the datasety\n            mode (str): Strategy to balance classes.\n                Must be one of [downsampling, upsampling]\n        """"""\n        samples_per_class = defaultdict(int)\n        for l in labels:\n            samples_per_class[l] += 1\n\n        self.lbl2idx = dict()\n        for i, l in enumerate(labels):\n            if l not in self.lbl2idx:\n                self.lbl2idx[l] = []\n            self.lbl2idx[l].append(i)\n\n        if mode == \'upsampling\' or max_count is not None:\n            samples_per_class = max_count \\\n                if max_count is not None \\\n                else max(samples_per_class.values())\n        else:\n            samples_per_class = min(samples_per_class.values())\n\n        if count_per_class is not None:\n            self.count_per_class = count_per_class\n        else:\n            self.count_per_class = {l: samples_per_class for l in self.lbl2idx}\n        self.labels = labels\n        self.length = sum(self.count_per_class.values())\n\n        super().__init__(labels)\n\n    def __iter__(self) -> Iterator[int]:\n        """"""\n        Yields:\n            indices of stratified sample\n        """"""\n        indices = []\n        for key in sorted(self.lbl2idx):\n            samples = self.count_per_class[key]\n            replace_ = samples > len(self.lbl2idx[key])\n            indices += np.random.choice(\n                self.lbl2idx[key], samples, replace=replace_\n            ).tolist()\n        assert (len(indices) == self.length)\n        np.random.shuffle(indices)\n\n        return iter(indices)\n\n    def __len__(self) -> int:\n        """"""\n        Returns:\n             length of result sample\n        """"""\n        return self.length\n\n\n__all__ = [\'BalanceClassSampler\']\n'"
mlcomp/contrib/sampler/distributed.py,1,"b""import numpy as np\n\nfrom torch.utils.data import DistributedSampler\n\n\nclass DistributedSamplerIndices(DistributedSampler):\n    def __init__(self, sampler, *args, **kwargs):\n        super().__init__(sampler, num_replicas=None, rank=None, shuffle=True)\n        self.sampler = sampler\n\n    def get_indices(self):\n        return list(self.sampler.__iter__())\n\n    def __iter__(self):\n        # deterministically shuffle based on epoch\n        np.random.seed(self.epoch)\n\n        indices = self.get_indices()\n\n        if self.shuffle:\n            np.random.shuffle(indices)\n\n        # add extra samples to make it evenly divisible\n        indices += indices[:(self.total_size - len(indices))]\n        assert len(indices) == self.total_size\n\n        # subsample\n        indices = indices[self.rank:self.total_size:self.num_replicas]\n        assert len(indices) == self.num_samples\n\n        return iter(indices)\n\n\n__all__ = ['DistributedSamplerIndices']\n"""
mlcomp/contrib/sampler/hard_negative.py,3,"b""from typing import Tuple\n\nimport numpy as np\nfrom torch.nn import CrossEntropyLoss\n\nfrom torch.nn.functional import cross_entropy\nfrom torch.utils.data import Sampler\n\nfrom catalyst.core import _State\nfrom catalyst.dl import Callback\n\n\nclass HardNegativeSampler(Sampler, Callback):\n    def __init__(\n            self,\n            data_source,\n            name: str,\n            count: int,\n            batch_size: int = None,\n            hard_interval: Tuple[float, float] = (50, 100),\n            index_count: int = 1,\n            criterion_data: dict = None\n    ):\n        super().__init__(data_source)\n        self.criterion_data = criterion_data\n        self.name = name\n        self.order = 0\n        self.hard_interval = hard_interval\n        self.index_count = index_count\n\n        self.sampled = 0\n        self.count = count\n        self.batch_size = batch_size or count\n        self.max_index = len(data_source)\n        self.loss = []\n        self.indices = []\n\n    def get_loss(self, state: _State, criterion=None, meta: dict = None):\n        criterion = criterion or state.criterion\n        if isinstance(criterion, dict):\n            res = np.zeros(len(state.input['targets']))\n            for k, v in criterion.items():\n                res += self.get_loss(state, criterion=v,\n                                     meta=self.criterion_data[k])\n            return res\n\n        output_key = 'logits' if meta is None else meta['output_key']\n        input_key = 'targets' if meta is None else meta['input_key']\n\n        if isinstance(criterion, CrossEntropyLoss):\n            loss = cross_entropy(state.output[output_key],\n                                 state.input[input_key],\n                                 reduction='none'\n                                 )\n            loss = loss.cpu().detach().numpy()\n        else:\n            loss = []\n            for i in range(len(state.input[input_key])):\n                v = criterion(\n                    state.output[output_key][i:i + 1],\n                    state.input[input_key][i:i + 1]\n                )\n                loss.append(float(v))\n            loss = np.array(loss)\n\n        weight = 1 if meta is None else meta['weight']\n        return loss * weight\n\n    def __len__(self):\n        return self.count\n\n    def random(self, count: int = None):\n        count = count or self.batch_size\n        replace = count > self.max_index\n        indices = []\n\n        for i in range(self.index_count):\n            index = np.random.choice(np.arange(0, self.max_index),\n                                     replace=replace, size=count)\n            indices.append(index)\n        return indices\n\n    def sample_batch(self):\n        if len(self.loss) > 0:\n            percentile_high = np.percentile(self.loss, self.hard_interval[1])\n            percentile_low = np.percentile(self.loss, self.hard_interval[0])\n\n            cond = (self.loss >= percentile_low) & (\n                    self.loss <= percentile_high)\n            hard_samples = np.where(cond)[0]\n            indices = [index[hard_samples] for index in self.indices]\n        else:\n            indices = [[] for _ in range(self.index_count)]\n\n        indices_random = self.random(self.batch_size - len(indices[0]))\n        indices_shuffle = np.arange(0, self.batch_size)\n        np.random.shuffle(indices_shuffle)\n\n        for i in range(len(indices)):\n            indices[i] = np.concatenate([indices[i], indices_random[i]])\n            indices[i] = indices[i][indices_shuffle].astype(np.int32)\n\n        if len(indices) == 1:\n            return indices[0]\n        return zip(indices)\n\n    def __iter__(self):\n        while self.sampled < self.count:\n            batch = self.sample_batch()\n            self.sampled += self.batch_size\n            yield from batch\n\n        self.sampled = 0\n\n    def on_batch_end(self, state: _State):\n        if state.loader_name != self.name:\n            return\n\n        self.loss = self.get_loss(state)\n        self.indices = []\n\n        for k in sorted(list(state.input)):\n            if 'index_' in k:\n                self.indices.append(state.input[k].cpu().detach().numpy())\n\n\nclass HardNegativePairSampler(HardNegativeSampler):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, index_count=2, **kwargs)\n\n\nclass HardNegativeTripleSampler(HardNegativeSampler):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, index_count=4, **kwargs)\n\n\nclass HardNegativeFourSampler(HardNegativeSampler):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, index_count=4, **kwargs)\n\n\n__all__ = ['HardNegativeSampler', 'HardNegativePairSampler',\n           'HardNegativeTripleSampler', 'HardNegativeFourSampler']\n"""
mlcomp/contrib/scripts/__init__.py,0,b''
mlcomp/contrib/scripts/split.py,0,"b""from os.path import basename, splitext\n\nimport pandas as pd\n\nfrom sklearn.model_selection import GroupKFold\n\n\ndef file_group_kfold(\n        n_splits: int,\n        output: str = None,\n        get_group=None,\n        sort=False,\n        must_equal=(),\n        **files\n):\n    assert len(files) > 0, 'at lease 1 type of files is required'\n    fold = GroupKFold(n_splits)\n    keys = sorted(list(files))\n\n    def get_name(file):\n        return splitext(basename(file))[0]\n\n    if sort:\n        for k, v in files.items():\n            files[k] = sorted(files[k], key=get_name)\n\n    file_first = sorted(files[keys[0]])\n\n    assert len(file_first) > n_splits, \\\n        f'at least {n_splits} files is required. Provided: {len(file_first)}'\n\n    for k, v in files.items():\n        assert len(files[k]) == len(file_first), \\\n            f'count of files in key = {k} is not the same as in {keys[0]}'\n\n    for k, v in files.items():\n        if k not in must_equal:\n            continue\n        for i in range(len(file_first)):\n            names_equal = get_name(v[i]) == get_name(file_first[i])\n            assert names_equal, \\\n                f'file name in {k} does not equal to {keys[0]}, ' \\\n                f'file name = {basename(v[i])}'\n\n    df = pd.DataFrame(files)[keys]\n    df['fold'] = 0\n\n    groups = [\n        i if not get_group else get_group(file)\n        for i, file in enumerate(file_first)\n    ]\n\n    for i, (train_index,\n            test_index) in enumerate(fold.split(groups, groups=groups)):\n        df.loc[test_index, 'fold'] = i\n\n    df = df.sample(frac=1)\n    if output:\n        df.to_csv(output, index=False)\n    return df\n"""
mlcomp/contrib/search/__init__.py,0,b''
mlcomp/contrib/search/grid.py,0,"b""from itertools import product\nfrom os.path import join\nfrom typing import List\nfrom glob import glob\n\nfrom mlcomp.utils.io import yaml_load\nfrom mlcomp.utils.misc import dict_flatten\n\n\ndef cell_name(cell: dict):\n    c = dict_flatten(cell)\n    parts = []\n    for k, v in c.items():\n        parts.append(f'{k}={v}')\n\n    return ' '.join(parts)[-300:]\n\n\ndef grid_cells(grid: List):\n    for i, row in enumerate(grid):\n        row_type = type(row)\n\n        if row_type == list:\n            if len(row) == 0:\n                raise Exception(f'Empty list at {i} position')\n            if type(row[0]) != dict:\n                raise Exception('List entries can be dicts only')\n        elif row_type == dict:\n            if len(row) != 1:\n                raise Exception('Dict must contain only one element')\n            key = list(row)[0]\n            val_type = type(row[key])\n            if val_type not in [list, str]:\n                raise Exception('Dict value must be list or str')\n            new_row = []\n            if val_type == str:\n                if '-' in row[key]:\n                    start, end = map(int, row[key].split('-'))\n                    for p in range(start, end + 1):\n                        new_row.append({key: p})\n                else:\n                    if key == '_folder':\n                        for file in glob(join(row[key], '*.yml')):\n                            new_row.append(yaml_load(file))\n            else:\n                for v in row[key]:\n                    if key == '_file':\n                        new_row.append(yaml_load(v))\n                    else:\n                        new_row.append({key: v})\n\n            grid[i] = new_row\n        else:\n            raise Exception(f'Unknown type of row = {row_type}')\n\n    res = list(product(*grid))\n    for i, r in enumerate(res):\n        d = {}\n        for dd in r:\n            d.update(dd)\n        res[i] = d\n    return [[r, cell_name(r)] for r in res]\n\n\n__all__ = ['grid_cells']\n"""
mlcomp/contrib/segmentation/__init__.py,0,b'# flake8: noqa\nfrom .unet import Unet\nfrom .linknet import Linknet\nfrom .fpn import FPN\nfrom .pspnet import PSPNet\n\nfrom . import encoders'
mlcomp/contrib/split/__init__.py,0,"b""from .frame import stratified_group_k_fold, stratified_k_fold\n\n__all__ = ['stratified_group_k_fold', 'stratified_k_fold']\n"""
mlcomp/contrib/split/frame.py,0,"b""from collections import defaultdict\n\nimport pandas as pd\nimport numpy as np\nfrom numpy.random.mtrand import RandomState\n\nfrom sklearn.model_selection import StratifiedKFold\n\n\ndef stratified_group_k_fold(\n        label: str,\n        group_column: str,\n        df: pd.DataFrame = None,\n        file: str = None,\n        n_splits=5,\n        seed: int = 0\n):\n    random_state = RandomState(seed)\n\n    if file is not None:\n        df = pd.read_csv(file)\n\n    labels = defaultdict(set)\n    for g, l in zip(df[group_column], df[label]):\n        labels[g].add(l)\n\n    group_labels = dict()\n    groups = []\n    Y = []\n    for k, v in labels.items():\n        group_labels[k] = random_state.choice(list(v))\n        Y.append(group_labels[k])\n        groups.append(k)\n\n    index = np.arange(len(group_labels))\n    folds = StratifiedKFold(n_splits=n_splits, shuffle=True,\n                            random_state=random_state).split(index, Y)\n\n    group_folds = dict()\n    for i, (train, val) in enumerate(folds):\n        for j in val:\n            group_folds[groups[j]] = i\n\n    res = np.zeros(len(df))\n    for i, g in enumerate(df[group_column]):\n        res[i] = group_folds[g]\n\n    return res.astype(np.int)\n\n\ndef stratified_k_fold(\n        label: str, df: pd.DataFrame = None, file: str = None, n_splits=5,\n        seed: int = 0\n):\n    random_state = RandomState(seed)\n\n    if file is not None:\n        df = pd.read_csv(file)\n\n    index = np.arange(df.shape[0])\n    res = np.zeros(index.shape)\n    folds = StratifiedKFold(n_splits=n_splits,\n                            random_state=random_state,\n                            shuffle=True).split(index, df[label])\n\n    for i, (train, val) in enumerate(folds):\n        res[val] = i\n    return res.astype(np.int)\n\n\n__all__ = ['stratified_group_k_fold', 'stratified_k_fold']\n"""
mlcomp/contrib/torch/__init__.py,0,b''
mlcomp/contrib/torch/layers.py,1,"b""import torch.nn as nn\n\n\nclass LambdaLayer(nn.Module):\n    def __init__(self, lambd):\n        super(LambdaLayer, self).__init__()\n        self.lambd = lambd\n\n    def forward(self, x):\n        return self.lambd(x)\n\n\n__all__ = ['LambdaLayer']\n"""
mlcomp/contrib/torch/tensors.py,2,"b""import torch\n\n\ndef flip(x, dim):\n    indices = [slice(None)] * x.dim()\n    indices[dim] = torch.arange(x.size(dim) - 1, -1, -1,\n                                dtype=torch.long, device=x.device)\n    return x[tuple(indices)]\n\n\n__all__ = ['flip']\n"""
mlcomp/contrib/transform/__init__.py,0,b''
mlcomp/contrib/transform/albumentations.py,0,"b""import numpy as np\nfrom albumentations import ImageOnlyTransform\n\n\nclass ChannelTranspose(ImageOnlyTransform):\n    def get_transform_init_args_names(self):\n        return ()\n\n    def get_params_dependent_on_targets(self, params):\n        pass\n\n    def __init__(self, axes=(2, 0, 1)):\n        super().__init__(always_apply=True)\n        self.axes = axes\n\n    def apply(self, img, **params):\n        return np.transpose(img, self.axes)\n\n\nclass Ensure4d(ImageOnlyTransform):\n    def get_transform_init_args_names(self):\n        return ()\n\n    def get_params_dependent_on_targets(self, params):\n        pass\n\n    def __init__(self):\n        super().__init__(always_apply=True)\n\n    def apply(self, img, **params):\n        while len(img.shape) < 4:\n            img = img[None]\n        return img\n\n\n__all__ = ['ChannelTranspose', 'Ensure4d']\n"""
mlcomp/contrib/transform/rle.py,0,"b'import numpy as np\n\n\ndef mask2rle(img):\n    """"""\n    img: numpy array, 1 - mask, 0 - background\n    Returns run length as string formatted\n    """"""\n    pixels = img.T.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return \' \'.join(str(x) for x in runs)\n\n\ndef rle2mask(mask_rle, shape):\n    """"""\n    mask_rle: run-length as string formatted (start length)\n    shape: (width,height) of array to return\n    Returns numpy array, 1 - mask, 0 - background\n\n    """"""\n    s = mask_rle.split()\n    starts, lengths = [np.asarray(x, dtype=int) for x in\n                       (s[0:][::2], s[1:][::2])]\n    starts -= 1\n    ends = starts + lengths\n    img = np.zeros(shape[0] * shape[1], dtype=np.uint8)\n    for lo, hi in zip(starts, ends):\n        img[lo:hi] = 1\n    return img.reshape(shape).T\n\n\n__all__ = [\'mask2rle\', \'rle2mask\']\n'"
mlcomp/contrib/transform/tta.py,2,"b""import albumentations as A\nimport numpy as np\n\nfrom torch.utils.data import Dataset\n\nfrom mlcomp.contrib.torch.tensors import flip\n\n\nclass TtaWrap(Dataset):\n    def __init__(self, dataset: Dataset, tfms=()):\n        self.dataset = dataset\n        self.tfms = tfms\n\n    def __getitem__(self, item):\n        return self.dataset[item]\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def inverse(self, a: np.array):\n        last_dim = len(a.shape) - 1\n        for t in self.tfms:\n            if isinstance(t, A.HorizontalFlip):\n                a = flip(a, last_dim)\n            elif isinstance(t, A.VerticalFlip):\n                a = flip(a, last_dim - 1)\n            elif isinstance(t, A.Transpose):\n                axis = (0, 1, 3, 2) if len(a.shape) == 4 else (0, 2, 1)\n                a = a.permute(*axis)\n\n        return a\n\n\n__all__ = ['TtaWrap']\n"""
mlcomp/db/core/__init__.py,0,"b""from .db import Session\nfrom .options import PaginatorOptions\n\n__all__ = ['Session', 'PaginatorOptions']\n"""
mlcomp/db/core/db.py,0,"b""import sqlalchemy as sa\nimport sqlalchemy.orm.session as session\nfrom sqlalchemy.orm import scoped_session, sessionmaker\nfrom sqlalchemy import event\n\nfrom mlcomp import SA_CONNECTION_STRING, DB_TYPE\nfrom mlcomp.utils.misc import adapt_db_types\n\n\nclass Session(session.Session):\n    __session = dict()\n\n    def __init__(self, *args, **kwargs):\n        key = kwargs.pop('key')\n        if key in self.__session:\n            raise Exception('Use static create_session for session creating')\n        super().__init__(*args, **kwargs)\n\n    @staticmethod\n    def create_session(*, connection_string: str = None, key='default'):\n        if key in Session.__session:\n            return Session.__session[key][0]\n\n        session_factory = scoped_session(sessionmaker(class_=Session, key=key))\n        connect_args = {}\n        if DB_TYPE == 'SQLITE':\n            connect_args = {\n                'check_same_thread': False,\n                'timeout': 30\n            }\n\n        engine = sa.create_engine(\n            connection_string or SA_CONNECTION_STRING,\n            echo=False,\n            connect_args=connect_args\n        )\n        if DB_TYPE == 'SQLITE':\n            def _fk_pragma_on_connect(dbapi_con, con_record):\n                dbapi_con.execute('pragma foreign_keys=ON')\n\n            event.listen(engine, 'connect', _fk_pragma_on_connect)\n\n        session_factory.configure(bind=engine)\n        s = session_factory()\n\n        Session.__session[key] = [s, engine]\n        return s\n\n    @classmethod\n    def cleanup(cls, key: str):\n        if key in cls.__session:\n            s, engine = cls.__session[key]\n            try:\n                s.close()\n            except Exception:\n                pass\n\n            try:\n                engine.dispose()\n            except Exception:\n                pass\n\n            del cls.__session[key]\n\n    def query(self, *entities, **kwargs):\n        try:\n            return super().query(*entities, **kwargs)\n        except Exception as e:\n            self.rollback()\n            raise e\n\n    def add_all(self, objs, commit=True):\n        try:\n            for obj in objs:\n                adapt_db_types(obj)\n            super().add_all(objs)\n        except Exception as e:\n            raise e\n\n        if commit:\n            try:\n                self.commit()\n            except Exception as e:\n                self.rollback()\n                raise e\n\n    def add(self, obj, commit=True, _warn=False):\n        try:\n            adapt_db_types(obj)\n            super().add(obj, _warn=_warn)\n        except Exception as e:\n            raise e\n\n        if commit:\n            try:\n                self.commit()\n            except Exception as e:\n                self.rollback()\n                raise e\n        return obj\n\n    def commit(self):\n        try:\n            super().commit()\n        except Exception as e:\n            self.rollback()\n            raise e\n\n    def update(self):\n        try:\n            self.commit()\n        except Exception as e:\n            self.rollback()\n            raise e\n\n    @staticmethod\n    def sqlalchemy_error(e):\n        s = str(type(e))\n        return 'sqlalchemy.' in s\n\n\n__all__ = ['Session']\n"""
mlcomp/db/core/options.py,0,"b""class PaginatorOptions:\n    def __init__(\n        self,\n        page_number: int,\n        page_size: int,\n        sort_column: str = None,\n        sort_descending: bool = None\n    ):\n        self.sort_column = sort_column\n        self.sort_descending = sort_descending\n        self.page_number = page_number\n        self.page_size = page_size\n\n        assert (page_number is not None and page_size) \\\n            or (page_number is not None and not page_size), \\\n            'Specify both page_number and page_size'\n\n        if not sort_column:\n            self.sort_column = 'id'\n            self.sort_descending = True\n\n\n__all__ = ['PaginatorOptions']\n"""
mlcomp/db/models/__init__.py,0,"b""from .project import Project\nfrom .task import Task, TaskDependence, TaskSynced\nfrom .file import File\nfrom .dag_storage import DagStorage, DagLibrary\nfrom .computer import Computer, ComputerUsage\nfrom .log import Log\nfrom .step import Step\nfrom .dag import Dag, DagTag\nfrom .report import ReportSeries, ReportImg, ReportTasks, Report, ReportLayout\nfrom .docker import Docker\nfrom .model import Model\nfrom .auxilary import Auxiliary\nfrom .memory import Memory\nfrom .space import Space, SpaceTag\n\n__all__ = [\n    'Project', 'Task', 'TaskDependence', 'File', 'DagStorage', 'DagLibrary',\n    'Computer', 'ComputerUsage', 'Log', 'Step', 'Dag', 'ReportSeries',\n    'ReportImg', 'ReportTasks', 'Report', 'ReportLayout', 'Docker', 'Model',\n    'Auxiliary', 'TaskSynced', 'Memory', 'Space', 'DagTag'\n]\n"""
mlcomp/db/models/auxilary.py,0,"b""import sqlalchemy as sa\n\nfrom mlcomp.db.models.base import Base\n\n\nclass Auxiliary(Base):\n    __tablename__ = 'auxiliary'\n\n    name = sa.Column(sa.String, primary_key=True)\n    data = sa.Column(sa.String)\n\n\n__all__ = ['Auxiliary']\n"""
mlcomp/db/models/base.py,0,b'from sqlalchemy.ext.declarative import declarative_base\nfrom sqlalchemy_serializer import SerializerMixin\n\nBase = declarative_base(cls=SerializerMixin)\n'
mlcomp/db/models/computer.py,0,"b""import sqlalchemy as sa\nfrom sqlalchemy import ForeignKey\n\nfrom mlcomp.db.models.base import Base\nfrom mlcomp.utils.misc import now\n\n\nclass Computer(Base):\n    __tablename__ = 'computer'\n\n    name = sa.Column(sa.String, primary_key=True)\n    gpu = sa.Column(sa.Integer, default=0)\n    cpu = sa.Column(sa.Integer, default=1)\n    memory = sa.Column(sa.Float, default=0.1)\n    usage = sa.Column(sa.String)\n    ip = sa.Column(sa.String)\n    port = sa.Column(sa.Integer)\n    user = sa.Column(sa.String)\n    last_synced = sa.Column(sa.DateTime)\n    disk = sa.Column(sa.Integer)\n    syncing_computer = sa.Column(sa.String, ForeignKey('computer.name'))\n    root_folder = sa.Column(sa.String)\n    can_process_tasks = sa.Column(sa.Boolean)\n    sync_with_this_computer = sa.Column(sa.Boolean)\n    meta = sa.Column(sa.String)\n\n\nclass ComputerUsage(Base):\n    __tablename__ = 'computer_usage'\n\n    id = sa.Column(sa.Integer, primary_key=True)\n    computer = sa.Column(sa.String, ForeignKey('computer.name'))\n    usage = sa.Column(sa.String)\n    time = sa.Column(sa.DateTime, default=now())\n\n\n__all__ = ['Computer', 'ComputerUsage']\n"""
mlcomp/db/models/dag.py,0,"b""import sqlalchemy as sa\nfrom sqlalchemy import ForeignKey\nfrom sqlalchemy.orm import relationship\n\nfrom mlcomp.db.models.base import Base\nfrom mlcomp.utils.misc import now\n\n\nclass Dag(Base):\n    __tablename__ = 'dag'\n\n    id = sa.Column(sa.Integer, primary_key=True)\n    project = sa.Column(sa.Integer, ForeignKey('project.id'))\n    created = sa.Column(sa.DateTime, default=now())\n    config = sa.Column(sa.String)\n    name = sa.Column(sa.String)\n    tasks = relationship('Task', lazy='noload')\n    project_rel = relationship('Project', lazy='noload')\n    docker_img = sa.Column(sa.String)\n    img_size = sa.Column(sa.BigInteger, nullable=False, default=0)\n    file_size = sa.Column(sa.BigInteger, nullable=False, default=0)\n    type = sa.Column(sa.Integer, default=0)\n    report = sa.Column(sa.Integer, ForeignKey('report.id'))\n    report_rel = relationship('Report', lazy='noload')\n\n\nclass DagTag(Base):\n    __tablename__ = 'dag_tag'\n\n    dag = sa.Column(sa.Integer, ForeignKey('dag.id'), primary_key=True)\n    tag = sa.Column(sa.String, primary_key=True)\n\n\n__all__ = ['Dag', 'DagTag']\n"""
mlcomp/db/models/dag_storage.py,0,"b""import sqlalchemy as sa\nfrom sqlalchemy import ForeignKey\n\nfrom mlcomp.db.models.base import Base\n\n\nclass DagStorage(Base):\n    __tablename__ = 'dag_storage'\n\n    id = sa.Column(sa.Integer, primary_key=True)\n    dag = sa.Column(sa.Integer, ForeignKey('dag.id'))\n    file = sa.Column(sa.Integer, ForeignKey('file.id'))\n    path = sa.Column(sa.String)\n    is_dir = sa.Column(sa.Boolean)\n\n\nclass DagLibrary(Base):\n    __tablename__ = 'dag_library'\n\n    id = sa.Column(sa.Integer, primary_key=True)\n    dag = sa.Column(sa.Integer, ForeignKey('dag.id'))\n    library = sa.Column(sa.String)\n    version = sa.Column(sa.String)\n\n\n__all__ = ['DagStorage', 'DagLibrary']\n"""
mlcomp/db/models/docker.py,0,"b""import sqlalchemy as sa\nfrom sqlalchemy import ForeignKey\n\nfrom mlcomp.db.models.base import Base\n\n\nclass Docker(Base):\n    __tablename__ = 'docker'\n\n    name = sa.Column(sa.String, primary_key=True)\n    computer = sa.Column(sa.String,\n                         ForeignKey('computer.name'),\n                         primary_key=True)\n    last_activity = sa.Column(sa.DateTime, nullable=False)\n    ports = sa.Column(sa.String, nullable=False)\n\n\n__all__ = ['Docker']\n"""
mlcomp/db/models/file.py,0,"b""import sys\n\nimport sqlalchemy as sa\nfrom sqlalchemy import ForeignKey\n\nfrom mlcomp.db.models.base import Base\n\n\nclass File(Base):\n    __tablename__ = 'file'\n\n    id = sa.Column(sa.Integer, primary_key=True)\n    md5 = sa.Column(sa.String)\n    created = sa.Column(sa.DateTime, default='Now()')\n    content = sa.Column(sa.LargeBinary)\n    project = sa.Column(sa.Integer, ForeignKey('project.id'))\n    dag = sa.Column(sa.Integer, ForeignKey('dag.id'))\n    size = sa.Column(sa.BigInteger, nullable=False, default=0)\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.size = sys.getsizeof(self.content)\n\n\n__all__ = ['File']\n"""
mlcomp/db/models/log.py,0,"b""import sqlalchemy as sa\nfrom sqlalchemy import ForeignKey\n\nfrom mlcomp.db.models.base import Base\n\n\nclass Log(Base):\n    __tablename__ = 'log'\n\n    id = sa.Column(sa.Integer, primary_key=True)\n    step = sa.Column(sa.Integer, ForeignKey('step.id'))\n    message = sa.Column(sa.String)\n    time = sa.Column(sa.DateTime)\n    level = sa.Column(sa.Integer)\n    component = sa.Column(sa.Integer)\n    module = sa.Column(sa.String)\n    line = sa.Column(sa.Integer)\n    task = sa.Column(sa.Integer, ForeignKey('task.id'))\n    computer = sa.Column(sa.String, ForeignKey('computer.name'))\n\n\n__all__ = ['Log']\n"""
mlcomp/db/models/memory.py,0,"b""import sqlalchemy as sa\n\nfrom mlcomp.db.models.base import Base\n\n\nclass Memory(Base):\n    __tablename__ = 'memory'\n\n    id = sa.Column(sa.Integer, primary_key=True)\n    model = sa.Column(sa.String, nullable=False)\n    variant = sa.Column(sa.String)\n    num_classes = sa.Column(sa.Integer)\n    img_size = sa.Column(sa.Integer)\n    batch_size = sa.Column(sa.Integer, nullable=False)\n    memory = sa.Column(sa.Float, nullable=False)\n\n\n__all__ = ['Memory']\n"""
mlcomp/db/models/model.py,0,"b""import sqlalchemy as sa\nfrom sqlalchemy import ForeignKey\nfrom sqlalchemy.orm import relationship\n\nfrom mlcomp.db.models.base import Base\n\n\nclass Model(Base):\n    __tablename__ = 'model'\n\n    id = sa.Column(sa.Integer, primary_key=True)\n    name = sa.Column(sa.String)\n    score_local = sa.Column(sa.Float)\n    score_public = sa.Column(sa.Float)\n    project = sa.Column(sa.Integer, ForeignKey('project.id'))\n    dag = sa.Column(sa.Integer, ForeignKey('dag.id'))\n    created = sa.Column(sa.DateTime)\n    equations = sa.Column(sa.String)\n    fold = sa.Column(sa.Integer)\n\n    dag_rel = relationship('Dag', lazy='noload')\n    project_rel = relationship('Project', lazy='noload')\n\n\n__all__ = ['Model']\n"""
mlcomp/db/models/project.py,0,"b""import sqlalchemy as sa\n\nfrom mlcomp.db.models.base import Base\n\n\nclass Project(Base):\n    __tablename__ = 'project'\n\n    id = sa.Column(sa.Integer, primary_key=True)\n    name = sa.Column(sa.String, nullable=False)\n    class_names = sa.Column(sa.String, nullable=False)\n    sync_folders = sa.Column(sa.String, nullable=False)\n    ignore_folders = sa.Column(sa.String, nullable=False)\n\n\n__all__ = ['Project']\n"""
mlcomp/db/models/report.py,0,"b""import sys\n\nimport sqlalchemy as sa\nfrom sqlalchemy import ForeignKey\nfrom sqlalchemy.orm import relationship\n\nfrom mlcomp.db.models.base import Base\nfrom mlcomp.utils.misc import now\n\n\nclass ReportSeries(Base):\n    __tablename__ = 'report_series'\n\n    id = sa.Column(sa.Integer, primary_key=True)\n    name = sa.Column(sa.String)\n    value = sa.Column(sa.Float)\n    epoch = sa.Column(sa.Integer)\n    time = sa.Column(sa.DateTime)\n    task = sa.Column(sa.Integer, ForeignKey('task.id'))\n    part = sa.Column(sa.String)\n    stage = sa.Column(sa.String)\n\n    task_rel = relationship('Task', lazy='noload')\n\n\nclass ReportImg(Base):\n    __tablename__ = 'report_img'\n\n    id = sa.Column(sa.Integer, primary_key=True)\n    group = sa.Column(sa.String)\n    epoch = sa.Column(sa.Integer)\n    task = sa.Column(sa.Integer, ForeignKey('task.id'))\n    img = sa.Column(sa.LargeBinary)\n    dag = sa.Column(sa.Integer, ForeignKey('dag.id'))\n    part = sa.Column(sa.String)\n    project = sa.Column(sa.Integer, ForeignKey('project.id'))\n    y_pred = sa.Column(sa.Integer)\n    y = sa.Column(sa.Integer)\n    score = sa.Column(sa.Float)\n    size = sa.Column(sa.BigInteger)\n    attr1 = sa.Column(sa.Float)\n    attr2 = sa.Column(sa.Float)\n    attr3 = sa.Column(sa.Float)\n    attr4 = sa.Column(sa.Float)\n    attr5 = sa.Column(sa.Float)\n    attr6 = sa.Column(sa.Float)\n    attr7 = sa.Column(sa.Float)\n    attr8 = sa.Column(sa.Float)\n    attr9 = sa.Column(sa.Float)\n    attr1_str = sa.Column(sa.String)\n    attr2_str = sa.Column(sa.String)\n    attr3_str = sa.Column(sa.String)\n    attr4_str = sa.Column(sa.String)\n    attr5_str = sa.Column(sa.String)\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n\n        self.size = sys.getsizeof(self.img)\n\n\nclass Report(Base):\n    __tablename__ = 'report'\n\n    id = sa.Column(sa.Integer, primary_key=True)\n    config = sa.Column(sa.String)\n    time = sa.Column(sa.DateTime, default=now())\n    name = sa.Column(sa.String)\n    project = sa.Column(sa.Integer, ForeignKey('project.id'))\n    layout = sa.Column(sa.String)\n\n\nclass ReportTasks(Base):\n    __tablename__ = 'report_task'\n\n    id = sa.Column(sa.Integer, primary_key=True)\n    report = sa.Column(sa.Integer, ForeignKey('report.id'))\n    task = sa.Column(sa.Integer, ForeignKey('task.id'))\n\n\nclass ReportLayout(Base):\n    __tablename__ = 'report_layout'\n\n    name = sa.Column(sa.String, primary_key=True)\n    content = sa.Column(sa.String)\n    last_modified = sa.Column(sa.TIMESTAMP)\n\n\n__all__ = [\n    'ReportSeries', 'ReportImg', 'ReportTasks', 'Report', 'ReportLayout'\n]\n"""
mlcomp/db/models/space.py,0,"b""import sqlalchemy as sa\nfrom sqlalchemy import ForeignKey\n\nfrom mlcomp.db.models.base import Base\n\n\nclass Space(Base):\n    __tablename__ = 'space'\n\n    name = sa.Column(sa.String, nullable=False, primary_key=True)\n    created = sa.Column(sa.DateTime, nullable=False)\n    changed = sa.Column(sa.DateTime, nullable=False)\n    content = sa.Column(sa.String, nullable=False)\n\n\nclass SpaceRelation(Base):\n    __tablename__ = 'space_relation'\n\n    parent = sa.Column(sa.String, ForeignKey('space.name'),\n                       primary_key=True)\n    child = sa.Column(sa.String, ForeignKey('space.name'),\n                      primary_key=True)\n\n\nclass SpaceTag(Base):\n    __tablename__ = 'space_tag'\n\n    space = sa.Column(sa.String, ForeignKey('space.name'), primary_key=True)\n    tag = sa.Column(sa.String, primary_key=True)\n\n\n__all__ = ['Space', 'SpaceRelation', 'SpaceTag']\n"""
mlcomp/db/models/step.py,0,"b""import sqlalchemy as sa\nfrom sqlalchemy import ForeignKey\nfrom sqlalchemy.orm import relationship\n\nfrom mlcomp.db.models.base import Base\n\n\nclass Step(Base):\n    __tablename__ = 'step'\n\n    id = sa.Column(sa.Integer, primary_key=True)\n    level = sa.Column(sa.Integer)\n    task = sa.Column(sa.Integer, ForeignKey('task.id'))\n    started = sa.Column(sa.DateTime)\n    finished = sa.Column(sa.DateTime)\n    name = sa.Column(sa.String)\n    task_rel = relationship('Task', lazy='noload')\n    index = sa.Column(sa.Integer)\n\n\n__all__ = ['Step']\n"""
mlcomp/db/models/task.py,0,"b""import sqlalchemy as sa\nfrom sqlalchemy import ForeignKey\nfrom sqlalchemy.orm import relationship, deferred\n\nfrom mlcomp.db.enums import TaskStatus\nfrom mlcomp.db.models.base import Base\n\n\nclass Task(Base):\n    __tablename__ = 'task'\n\n    id = sa.Column(sa.Integer, primary_key=True)\n    name = sa.Column(sa.String)\n    started = sa.Column(sa.DateTime)\n    finished = sa.Column(sa.DateTime)\n    last_activity = sa.Column(sa.DateTime)\n    computer = sa.Column(sa.String)\n    gpu = sa.Column(sa.Integer, default=0)\n    gpu_max = sa.Column(sa.Integer, default=0)\n    cpu = sa.Column(sa.Integer, default=1)\n    executor = sa.Column(sa.String)\n    status = sa.Column(sa.Integer, default=TaskStatus.NotRan.value)\n    computer_assigned = sa.Column(sa.String, ForeignKey('computer.name'))\n    computer_assigned_rel = relationship('Computer', lazy='noload')\n\n    memory = sa.Column(sa.Float, default=0.1)\n    steps = sa.Column(sa.Integer, default=1)\n    current_step = sa.Column(sa.String)\n    dag = sa.Column(sa.Integer, ForeignKey('dag.id'))\n    celery_id = sa.Column(sa.String)\n    dag_rel = relationship('Dag', lazy='noload')\n    debug = sa.Column(sa.Boolean, default=False)\n    pid = sa.Column(sa.Integer)\n    worker_index = sa.Column(sa.Integer)\n    docker_assigned = sa.Column(sa.String)\n    type = sa.Column(sa.Integer)\n    score = sa.Column(sa.Float)\n    report = sa.Column(sa.Integer, ForeignKey('report.id'))\n    report_rel = relationship('Report', lazy='noload')\n    gpu_assigned = sa.Column(sa.String)\n    parent = sa.Column(sa.Integer, ForeignKey('task.id'))\n    parent_rel = relationship('Task', lazy='noload')\n    loss = sa.Column(sa.Float)\n\n    continued = sa.Column(sa.Boolean, default=False)\n\n    batch_index = sa.Column(sa.Integer)\n    batch_total = sa.Column(sa.Integer)\n    loader_name = sa.Column(sa.String)\n    epoch_duration = sa.Column(sa.Integer)\n    epoch_time_remaining = sa.Column(sa.Integer)\n\n    result = deferred(sa.Column(sa.String))\n    additional_info = deferred(sa.Column(sa.String))\n\n\nclass TaskDependence(Base):\n    __tablename__ = 'task_dependency'\n\n    task_id = sa.Column(sa.Integer, primary_key=True)\n    depend_id = sa.Column(sa.Integer, primary_key=True)\n\n\nclass TaskSynced(Base):\n    __tablename__ = 'task_synced'\n\n    computer = sa.Column(sa.String, ForeignKey('computer.name'),\n                         primary_key=True)\n    task = sa.Column(sa.Integer, ForeignKey('task.id'), primary_key=True)\n\n\n__all__ = ['Task', 'TaskDependence', 'TaskSynced']\n"""
mlcomp/db/providers/__init__.py,0,"b""from .project import ProjectProvider\nfrom .task import TaskProvider\nfrom .file import FileProvider\nfrom .dag_storage import DagStorageProvider, DagLibraryProvider\nfrom .log import LogProvider\nfrom .step import StepProvider\nfrom .computer import ComputerProvider\nfrom .dag import DagProvider\nfrom .report import \\\n    ReportImgProvider, \\\n    ReportProvider, \\\n    ReportLayoutProvider, \\\n    ReportSeriesProvider, \\\n    ReportTasksProvider\nfrom .docker import DockerProvider\nfrom .model import ModelProvider\nfrom .auxiliary import AuxiliaryProvider\nfrom .task_synced import TaskSyncedProvider\nfrom .memory import MemoryProvider\nfrom .space import SpaceProvider\n\n__all__ = [\n    'ProjectProvider', 'TaskProvider', 'FileProvider', 'DagStorageProvider',\n    'DagLibraryProvider', 'LogProvider', 'StepProvider', 'ComputerProvider',\n    'DagProvider', 'ReportImgProvider', 'ReportProvider',\n    'ReportLayoutProvider', 'ReportSeriesProvider', 'ReportTasksProvider',\n    'DockerProvider', 'ModelProvider', 'AuxiliaryProvider',\n    'TaskSyncedProvider', 'MemoryProvider', 'SpaceProvider'\n]\n"""
mlcomp/db/providers/auxiliary.py,0,"b""from mlcomp.db.models import Auxiliary\nfrom mlcomp.db.providers.base import BaseDataProvider\nfrom mlcomp.utils.io import yaml_load\n\n\nclass AuxiliaryProvider(BaseDataProvider):\n    model = Auxiliary\n\n    def get(self):\n        query = self.query(self.model)\n        res = dict()\n        for r in query.all():\n            res[r.name] = yaml_load(r.data)\n            res[r.name] = self.serializer(res[r.name])\n        return res\n\n\n__all__ = ['AuxiliaryProvider']\n"""
mlcomp/db/providers/base.py,0,"b""from typing import List\n\nfrom sqlalchemy.orm.query import Query\nfrom sqlalchemy import desc\nfrom sqlalchemy_serializer import Serializer\nfrom sqlalchemy.orm import joinedload\n\nfrom mlcomp.db.core import Session, PaginatorOptions\nfrom mlcomp.db.models.base import Base\nfrom mlcomp.utils.misc import adapt_db_types\n\n\nclass BaseDataProvider:\n    model = None\n\n    date_format = '%Y-%m-%d'\n    datetime_format = '%Y-%m-%d %H:%M:%SZ'\n    time_format = '%H:%M'\n\n    def __init__(self, session: Session = None):\n        if session is None:\n            session = Session.create_session()\n        self._session = session\n        self.serializer = Serializer(\n            date_format=self.date_format,\n            datetime_format=self.datetime_format,\n            time_format=self.time_format\n        )\n\n    def serialize_datetime(self, value):\n        return self.serializer.serialize_datetime(value)\n\n    def remove(self, key_value, key_column: str = 'id'):\n        self.query(self.model). \\\n            filter(getattr(self.model, key_column) == key_value). \\\n            delete(synchronize_session=False)\n        self.session.commit()\n\n    def detach(self, obj):\n        self.session.expunge(obj)\n\n    @property\n    def query(self):\n        return self.session.query\n\n    def add_all(self, obs: List[Base], commit=True):\n        self._session.add_all(obs, commit=commit)\n\n    def add(self, obj: Base, commit=True):\n        self._session.add(obj, commit=commit)\n        return obj\n\n    def bulk_save_objects(\n            self,\n            objs,\n            return_defaults=False,\n            update_changed_only=True,\n            preserve_order=True,\n    ):\n        for obj in objs:\n            adapt_db_types(obj)\n\n        self._session.bulk_save_objects(\n            objs,\n            return_defaults=return_defaults,\n            update_changed_only=update_changed_only,\n            preserve_order=preserve_order\n        )\n        self._session.commit()\n\n    def by_id(self, id: int, joined_load=None, key_column: str = 'id'):\n        res = self.query(self.model).filter(\n            getattr(self.model, key_column) == id)\n        if joined_load is not None:\n            for n in joined_load:\n                res = res.options(joinedload(n, innerjoin=True))\n        return res.first()\n\n    def to_dict(self, item, rules=(), datetime_format=None):\n        datetime_format = datetime_format or self.datetime_format\n        return item.to_dict(\n            date_format=self.date_format,\n            datetime_format=datetime_format,\n            time_format=self.time_format,\n            rules=rules\n        )\n\n    def create_or_update(self, obj: Base, *fields):\n        query = self.session.query(obj.__class__)\n        for k in fields:\n            query = query.filter(getattr(obj.__class__, k) == getattr(obj, k))\n\n        db = query.first()\n        if db is not None:\n            for field, value in obj.__dict__.items():\n                if not field.startswith('_'):\n                    setattr(db, field, value)\n            self.session.update()\n        else:\n            self.add(obj)\n\n    def all(self):\n        return self.query(self.model).all()\n\n    def update(self):\n        self.session.update()\n        self.session.commit()\n\n    def commit(self):\n        self.session.commit()\n\n    def rollback(self):\n        self.session.rollback()\n\n    @property\n    def session(self):\n        return self._session\n\n    def paginator(self, query: Query, options: PaginatorOptions):\n        if options is None:\n            return query\n\n        if options.sort_column:\n            column = getattr(self.model, options.sort_column) if \\\n                options.sort_column in self.model.__dict__ \\\n                else options.sort_column\n            criterion = column if not options.sort_descending else desc(column)\n            query = query.order_by(criterion)\n\n        if options.page_size:\n            query = query. \\\n                offset(options.page_size * options.page_number). \\\n                limit(options.page_size)\n\n        return query\n\n\n__all__ = ['BaseDataProvider']\n"""
mlcomp/db/providers/computer.py,0,"b""import json\nimport datetime\nfrom collections import defaultdict\n\nfrom sqlalchemy import func, case\nimport numpy as np\n\nfrom mlcomp.db.core import PaginatorOptions\nfrom mlcomp.db.enums import TaskStatus\nfrom mlcomp.db.providers.base import BaseDataProvider\nfrom mlcomp.db.models import Computer, ComputerUsage, Task, Docker, Project\nfrom mlcomp.utils.misc import now, parse_time\n\n\nclass ComputerProvider(BaseDataProvider):\n    model = Computer\n\n    def computers(self):\n        return {\n            c.name:\n                {k: v\n                 for k, v in c.__dict__.items() if not k.startswith('_')}\n            for c in self.query(Computer).all()\n        }\n\n    def get(self, filter: dict, options: PaginatorOptions = None):\n        query = self.query(Computer)\n        query = query.filter(\n            Computer.last_synced >= now() - datetime.timedelta(days=1))\n        total = query.count()\n        if options:\n            query = self.paginator(query, options)\n        res = []\n        for c in query.all():\n            item = self.to_dict(c)\n            default_usage = {\n                'cpu': 0,\n                'memory': 0,\n                'gpu': [{\n                    'memory': 0,\n                    'load': 0\n                } for i in range(item['gpu'])]\n            }\n            sync_status = 'Not synced'\n            sync_date = None\n            if c.last_synced:\n                sync_date = self.serialize_datetime(c.last_synced)\n                sync_status = f'Last synced'\n\n            if c.syncing_computer:\n                if c.last_synced is None or \\\n                        (now() - c.last_synced).total_seconds() >= 5:\n                    sync_status = f'Syncing with {c.syncing_computer}'\n                    if c.last_synced:\n                        sync_status += f' from '\n                    sync_date = self.serialize_datetime(c.last_synced)\n\n            item['sync_status'] = sync_status\n            item['sync_date'] = sync_date\n\n            item['usage'] = json.loads(item['usage']) \\\n                if item['usage'] else default_usage\n            item['memory'] = int(item['memory'] / 1000)\n            item['usage']['cpu'] = int(item['usage']['cpu'])\n            item['usage']['memory'] = int(item['usage']['memory'])\n            for gpu in item['usage']['gpu']:\n                gpu['memory'] = 0 if np.isnan(gpu['memory']) else int(\n                    gpu['memory'])\n                gpu['load'] = 0 if np.isnan(gpu['load']) else int(gpu['load'])\n\n            min_time = parse_time(filter.get('usage_min_time'))\n\n            item['usage_history'] = self.usage_history(\n                c.name, min_time\n            )\n            item['dockers'] = self.dockers(c.name, c.cpu)\n            res.append(item)\n\n        return {'data': res, 'total': total}\n\n    def usage_history(self, computer: str, min_time: datetime = None):\n        min_time = min_time or (now() - datetime.timedelta(days=1))\n        query = self.query(ComputerUsage).filter(\n            ComputerUsage.time >= min_time\n        ).filter(ComputerUsage.computer == computer\n                 ).order_by(ComputerUsage.time)\n        res = {'time': [], 'mean': []}\n        mean = defaultdict(list)\n        for c in query.all():\n            item = self.to_dict(c, datetime_format='%Y-%m-%d %H:%M:%SZ')\n            usage = json.loads(item['usage'])\n            res['time'].append(item['time'])\n\n            mean['cpu'].append(usage['mean']['cpu'])\n            mean['memory'].append(usage['mean']['memory'])\n            mean['disk'].append(usage['mean']['disk'])\n            for i, gpu in enumerate(usage['mean']['gpu']):\n                mean[f'gpu_{i}'].append(gpu['load'])\n\n        for item in mean:\n            res['mean'].append({'name': item, 'value': mean[item]})\n\n        return dict(res)\n\n    def current_usage(self, name: str, usage: dict):\n        computer = self.query(Computer).filter(Computer.name == name).first()\n        computer.usage = json.dumps(usage)\n        self.update()\n\n    def by_name(self, name: str):\n        return self.query(Computer).filter(Computer.name == name).one()\n\n    def computers_have_succeeded_tasks(self, min_time: datetime):\n        res = self.session.query(Task.computer_assigned.distinct()). \\\n            filter(Task.finished >= min_time). \\\n            filter(Task.status == TaskStatus.Success.value). \\\n            all()\n        res = [r[0] for r in res]\n        return self.session.query(Computer). \\\n            filter(Computer.name.in_(res)). \\\n            all()\n\n    def dockers(self, computer: str, cpu: int):\n        count_cond = func.sum(\n            case(\n                whens=[(Task.status == TaskStatus.InProgress.value, 1)],\n                else_=0\n            ).label('count')\n        )\n\n        res = self.query(Docker, count_cond). \\\n            join(Task, Task.computer_assigned == computer, isouter=True). \\\n            filter(Docker.computer == computer). \\\n            group_by(Docker.name, Docker.computer). \\\n            all()\n        return [\n            {\n                'name': r[0].name,\n                'last_activity': self.serialize_datetime(\n                    r[0].last_activity\n                ),\n                'in_progress': r[1],\n                'free': cpu - r[1]\n            } for r in res\n        ]\n\n    def all_with_last_activtiy(self):\n        query = self.query(Computer, func.max(Docker.last_activity)). \\\n            join(Docker, Docker.computer == Computer.name). \\\n            group_by(Computer.name)\n        res = []\n        for c, a in query.all():\n            c.last_activity = a\n            res.append(c)\n        return res\n\n    def gpu_available(self):\n        computers = self.all_with_last_activtiy()\n        return sum([c.gpu for c in computers if\n                    (now() - c.last_activity).total_seconds() <= 30])\n\n    def sync_start(self):\n        projects = self.query(Project).order_by(Project.id.desc()).all()\n        res = []\n        for p in projects:\n            res.append({\n                'id': p.id,\n                'name': p.name,\n                'sync_folders': p.sync_folders,\n                'ignore_folders': p.ignore_folders,\n            })\n        return {'projects': res}\n\n\n__all__ = ['ComputerProvider']\n"""
mlcomp/db/providers/dag.py,0,"b'from typing import List\n\nfrom sqlalchemy import func, or_, case\n\nfrom mlcomp.db.core import PaginatorOptions\nfrom mlcomp.db.enums import TaskStatus, TaskType\nfrom mlcomp.db.models import Project, Dag, Task, ReportTasks, TaskDependence, \\\n    DagTag\nfrom mlcomp.db.providers.base import BaseDataProvider\nfrom mlcomp.utils.misc import to_snake, duration_format, now, parse_time\n\n\nclass DagProvider(BaseDataProvider):\n    model = Dag\n\n    # noinspection PyMethodMayBeStatic\n    def _get_filter(self, query, filter: dict, last_activity):\n        if filter.get(\'project\'):\n            query = query.filter(Dag.project == filter[\'project\'])\n        if filter.get(\'name\'):\n            query = query.filter(Dag.name.like(f\'%{filter[""name""]}%\'))\n        if filter.get(\'id\'):\n            query = query.filter(Dag.id == int(filter[\'id\']))\n\n        if filter.get(\'id_min\'):\n            query = query.filter(Dag.id >= filter[\'id_min\'])\n\n        if filter.get(\'id_max\'):\n            query = query.filter(Dag.id <= filter[\'id_max\'])\n\n        if filter.get(\'created_min\'):\n            created_min = parse_time(filter[\'created_min\'])\n            query = query.filter(Dag.created >= created_min)\n        if filter.get(\'created_max\'):\n            created_max = parse_time(filter[\'created_max\'])\n            query = query.filter(Dag.created <= created_max)\n        if filter.get(\'last_activity_min\'):\n            last_activity_min = parse_time(filter[\'last_activity_min\'])\n            query = query.having(last_activity >= last_activity_min)\n        if filter.get(\'last_activity_max\'):\n            last_activity_max = parse_time(filter[\'last_activity_max\'])\n            query = query.having(last_activity <= last_activity_max)\n        if filter.get(\'report\'):\n            query = query.filter(Dag.report is not None)\n\n        tags = filter.get(\'tags\', [])\n        if len(tags) > 0:\n            query = query.join(DagTag).filter(DagTag.tag.in_(tags))\n        return query\n\n    def get(self, filter: dict, options: PaginatorOptions = None):\n        task_status = []\n        for e in TaskStatus:\n            task_status.append(\n                func.sum(\n                    case(\n                        whens=[(Task.status == e.value, 1)],\n                        else_=0\n                    ).label(e.name)\n                )\n            )\n\n        last_activity = func.max(Task.last_activity).label(\'last_activity\')\n        funcs = [\n            func.count(Task.id).label(\'task_count\'), last_activity,\n            func.min(Task.started).label(\'started\'),\n            func.max(Task.finished).label(\'finished\')\n        ]\n\n        query = self.query(Dag, Project.name, *funcs,\n                           *task_status).join(Project)\n        query = self._get_filter(query, filter, last_activity)\n\n        status_clauses = []\n        for agg, e in zip(task_status, TaskStatus):\n            if filter.get(\'status\', {}).get(to_snake(e.name)):\n                status_clauses.append(agg > 0)\n        if len(status_clauses) > 0:\n            query = query.having(or_(*status_clauses))\n\n        query = query.join(Task, isouter=True).group_by(Dag.id, Project.name)\n        # Do not include service tasks\n        query = query.filter(Task.type < TaskType.Service.value)\n\n        total = query.count()\n        paginator = self.paginator(query, options) if options else query\n        res = []\n        rules = (\'-tasks.dag_rel\',)\n\n        for dag, \\\n            project_name, \\\n            task_count, \\\n            last_activity, \\\n            started, \\\n            finished, \\\n            *(task_status) in paginator.all():\n\n            items = self.to_dict(dag, rules=rules).items()\n            # noinspection PyDictCreation\n            r = {\n                \'task_count\': task_count,\n                \'last_activity\': last_activity,\n                \'started\': started,\n                \'finished\': finished,\n                **{k: v\n                   for k, v in items if k not in [\'tasks\', \'config\']}\n            }\n            r[\'project\'] = {\'name\': project_name}\n\n            r[\'task_statuses\'] = [\n                {\n                    \'name\': to_snake(e.name),\n                    \'count\': s\n                } for e, s in zip(TaskStatus, task_status)\n            ]\n            r[\'last_activity\'] = self.serializer.serialize_datetime(\n                r[\'last_activity\']\n            ) if r[\'last_activity\'] else None\n            r[\'started\'] = self.serializer.serialize_datetime(r[\'started\']) \\\n                if r[\'started\'] else None\n            r[\'finished\'] = self.serializer.serialize_datetime(\n                r[\'finished\']\n            ) if r[\'finished\'] else None\n\n            if task_status[TaskStatus.InProgress.value] > 0:\n                delta = (now() - started).total_seconds() if started else 0\n            elif sum(\n                    task_status[TaskStatus.InProgress.value:]\n            ) == 0 or not started or not last_activity:\n                delta = 0\n            else:\n                delta = (last_activity - started).total_seconds()\n\n            r[\'duration\'] = duration_format(delta)\n            res.append(r)\n\n        dag_ids = [r[\'id\'] for r in res]\n\n        if filter.get(\'report\'):\n            tasks_dags = self.query(Task.id, Task.dag). \\\n                filter(Task.type <= TaskType.Train.value). \\\n                filter(Task.dag.in_(dag_ids)). \\\n                all()\n\n            tasks_within_report = self.query(ReportTasks.task). \\\n                filter(ReportTasks.report == int(filter[\'report\']))\n\n            tasks_within_report = {t[0] for t in tasks_within_report}\n            dags_not_full_included = {\n                d\n                for t, d in tasks_dags if t not in tasks_within_report\n            }\n            for r in res:\n                r[\'report_full\'] = r[\'id\'] not in dags_not_full_included\n\n        tags = self.query(DagTag).filter(Dag.id.in_(dag_ids))\n        for r in res:\n            r[\'tags\'] = []\n\n            for t in tags:\n                if r[\'id\'] == t.dag:\n                    r[\'tags\'].append(t.tag)\n\n        projects = self.query(Project.name, Project.id). \\\n            order_by(Project.id.desc()). \\\n            limit(20). \\\n            all()\n\n        projects = [{\'name\': name, \'id\': id} for name, id in projects]\n        return {\'total\': total, \'data\': res, \'projects\': projects}\n\n    def config(self, id: int):\n        return self.by_id(id).config\n\n    def duration(self, t: Task):\n        if not t.started:\n            return duration_format(0)\n        finished = (\n            t.finished if t.finished else now()\n        )\n        delta = (finished - t.started).total_seconds()\n        return duration_format(delta)\n\n    def graph(self, id: int):\n        tasks = self.query(Task). \\\n            filter(Task.dag == id). \\\n            filter(Task.type <= TaskType.Train.value). \\\n            all()\n\n        task_ids = [t.id for t in tasks]\n        dep = self.query(TaskDependence).filter(\n            TaskDependence.task_id.in_(task_ids)\n        ).all()\n        task_by_id = {t.id: t for t in tasks}\n\n        def label(t: Task):\n            res = [t.executor]\n            if t.status >= TaskStatus.InProgress.value:\n                res.append(self.duration(t))\n                res.append(\n                    f\'{t.current_step if t.current_step else """"}/\'\n                    f\'{t.steps if t.steps else """"}\'\n                )\n            return \'\\n\'.join(res)\n\n        nodes = [\n            {\n                \'id\': t.id,\n                \'label\': label(t),\n                \'name\': t.name,\n                \'status\': to_snake(TaskStatus(t.status).name)\n            } for t in tasks\n        ]\n        edges = [\n            {\n                \'from\': d.depend_id,\n                \'to\': d.task_id,\n                \'status\': to_snake(\n                    TaskStatus(task_by_id[d.depend_id].status).name\n                )\n            } for d in dep\n        ]\n        return {\'nodes\': nodes, \'edges\': edges}\n\n    def by_project(self, id: int):\n        return self.query(Dag).filter(Dag.project == id).all()\n\n    def remove_all(self, ids: List[int]):\n        self.query(Dag).filter(Dag.id.in_(ids)).delete(\n            synchronize_session=False)\n        self.commit()\n\n    def count(self):\n        return self.query(Dag).count()\n\n    def remove_tag(self, dag: int, tag: str):\n        self.query(DagTag).filter(DagTag.dag == dag).filter(\n            DagTag.tag == tag).delete(synchronize_session=False)\n        self.commit()\n\n    def tags(self, name: str):\n        tag_count = func.count(DagTag.tag).label(\'count\')\n        query = self.query(DagTag.tag, tag_count)\n        if name:\n            query = query.filter(DagTag.tag.contains(name))\n\n        tags = query.group_by(\n            DagTag.tag).order_by(tag_count.desc()).limit(10)\n        tags = [t for t, c in tags]\n        return {\'tags\': tags}\n\n\n__all__ = [\'DagProvider\']\n'"
mlcomp/db/providers/dag_storage.py,0,"b""from mlcomp.db.models import DagStorage, File, DagLibrary\nfrom mlcomp.db.providers.base import BaseDataProvider\n\n\nclass DagStorageProvider(BaseDataProvider):\n    model = DagStorage\n\n    def by_dag(self, dag: int):\n        query = self.query(DagStorage, File).join(File, isouter=True). \\\n            filter(DagStorage.dag == dag). \\\n            order_by(DagStorage.path)\n        return query.all()\n\n\nclass DagLibraryProvider(BaseDataProvider):\n    model = DagLibrary\n\n    def dag(self, dag: int):\n        return self.query(DagLibrary.library, DagLibrary.version). \\\n            filter(DagLibrary.dag == dag).all()\n\n\n__all__ = ['DagStorageProvider', 'DagLibraryProvider']\n"""
mlcomp/db/providers/docker.py,0,"b""import datetime\n\nfrom mlcomp.db.models import Docker\nfrom mlcomp.db.providers.base import BaseDataProvider\nfrom mlcomp.utils.misc import now\n\n\nclass DockerProvider(BaseDataProvider):\n    model = Docker\n\n    def get(self, computer: str, name: str):\n        return self.query(Docker). \\\n            filter(Docker.computer == computer). \\\n            filter(Docker.name == name). \\\n            one()\n\n    def get_online(self):\n        min_activity = now() - datetime.timedelta(seconds=30)\n        return self.query(Docker). \\\n            filter(Docker.last_activity >= min_activity). \\\n            all()\n\n    def queues_online(self):\n        res = []\n        for docker in self.get_online():\n            res.append((docker.computer,\n                        f'{docker.computer}_{docker.name}_supervisor'))\n        return res\n\n\n__all__ = ['DockerProvider']\n"""
mlcomp/db/providers/file.py,0,"b""from mlcomp.db.models import File, Dag, Project\nfrom mlcomp.db.providers.base import BaseDataProvider\n\n\nclass FileProvider(BaseDataProvider):\n    model = File\n\n    def hashs(self, project: int):\n        return {\n            obj[0]: obj[1]\n            for obj in self.query(File.md5, File.id\n                                  ).filter(File.project == project).all()\n        }\n\n    def remove(self, filter: dict):\n        query = self.query(File)\n        if filter.get('dag'):\n            query = query.filter(File.dag == filter['dag'])\n        if filter.get('project'):\n            query = query.filter(File.project == filter['project'])\n        query.delete(synchronize_session=False)\n        self.session.commit()\n\n        query = self.query(Dag)\n        if filter.get('dag'):\n            query.filter(Dag.id == filter['dag']).update({'file_size': 0})\n\n        if filter.get('project'):\n            query.filter(Dag.project == filter['project']). \\\n                update({'file_size': 0})\n\n        self.session.commit()\n\n    def by_md5(self, md5):\n        return self.query(File).filter(File.md5 == md5).first()\n\n\n__all__ = ['FileProvider']\n"""
mlcomp/db/providers/log.py,0,"b'from mlcomp.db.models import Log, Step, Task, Computer\nfrom mlcomp.db.core import PaginatorOptions\nfrom mlcomp.db.enums import ComponentType\nfrom mlcomp.db.providers.base import BaseDataProvider\nfrom mlcomp.utils.misc import log_name, to_snake\n\n\nclass LogProvider(BaseDataProvider):\n    model = Log\n\n    def get(self, filter: dict, options: PaginatorOptions):\n        query = self.query(Log, Step, Task). \\\n            join(Step, Step.id == Log.step, isouter=True). \\\n            join(Task, Task.id == Log.task, isouter=True)\n\n        if filter.get(\'message\'):\n            query = query.filter(Log.message.contains(filter[\'message\']))\n\n        if filter.get(\'dag\'):\n            query = query.filter(Task.dag == filter[\'dag\'])\n\n        if filter.get(\'task\'):\n            child_tasks = self.query(Task.id\n                                     ).filter(Task.parent == filter[\'task\']\n                                              ).all()\n            child_tasks = [c[0] for c in child_tasks]\n            child_tasks.append(filter[\'task\'])\n\n            query = query.filter(Task.id.in_(child_tasks))\n\n        if len(filter.get(\'components\', [])) > 0:\n            query = query.filter(Log.component.in_(filter[\'components\']))\n\n        if filter.get(\'computer\'):\n            query = query.filter(Computer.name == filter[\'computer\'])\n\n        if len(filter.get(\'levels\', [])) > 0:\n            query = query.filter(Log.level.in_(filter[\'levels\']))\n\n        if filter.get(\'task_name\'):\n            query = query.filter(Task.name.like(f\'%{filter[""task_name""]}%\'))\n\n        if filter.get(\'step_name\'):\n            query = query.filter(Step.name.like(f\'%{filter[""step_name""]}%\'))\n\n        if filter.get(\'step\'):\n            query = query.filter(Step.id == filter[\'step\'])\n\n        total = query.count()\n        data = []\n        for log, step, task in self.paginator(query, options):\n            item = {\n                \'id\': log.id,\n                \'message\': log.message.split(\'\\n\'),\n                \'module\': log.module,\n                \'line\': log.line,\n                \'time\': self.serializer.serialize_datetime(log.time),\n                \'level\': log_name(log.level),\n                \'component\': to_snake(ComponentType(log.component).name),\n                \'computer\': log.computer,\n                \'step\': self.to_dict(step) if step else None,\n                \'task\': self.to_dict(task, rules=(\'-additional_info\',))\n                if task else None\n            }\n            data.append(item)\n\n        return {\'total\': total, \'data\': data}\n\n    def last(self, count: int, dag: int = None, task: int = None,\n             levels=None, components=None):\n        query = self.query(Log, Task.id).outerjoin(Task)\n        if dag is not None:\n            query = query.filter(Task.dag == dag)\n        if task is not None:\n            query = query.filter(Task.id == task)\n        if levels is not None:\n            query = query.filter(Log.level.in_(levels))\n        if components is not None:\n            query = query.filter(Log.component.in_(components))\n        return query.order_by(Log.id.desc()).limit(count).all()\n\n\n__all__ = [\'LogProvider\']\n'"
mlcomp/db/providers/memory.py,0,"b""from mlcomp.db.core import PaginatorOptions\nfrom mlcomp.db.models import Memory\nfrom mlcomp.db.providers.base import BaseDataProvider\n\n\nclass MemoryProvider(BaseDataProvider):\n    model = Memory\n\n    def get(self, filter: dict, options: PaginatorOptions = None):\n        query = self.query(Memory)\n        if filter.get('model'):\n            query = query.filter(Memory.model.contains(filter['model']))\n        if filter.get('variant'):\n            query = query.filter(Memory.variant.contains(filter['variant']))\n\n        total = query.count()\n        paginator = self.paginator(query, options) if options else query\n        data = []\n        for p in paginator.all():\n            item = self.to_dict(p)\n            data.append(item)\n\n        return {\n            'total': total,\n            'data': data\n        }\n\n    def find(self, data: dict):\n        query = self.query(Memory)\n        for k, v in data.items():\n            if k in ['batch_size']:\n                continue\n\n            if hasattr(Memory, k):\n                query = query.filter(getattr(Memory, k) == v)\n        return query.all()\n\n\n__all__ = ['MemoryProvider']\n"""
mlcomp/db/providers/model.py,0,"b'import datetime\n\nfrom sqlalchemy import func\nfrom sqlalchemy.orm import joinedload\n\nfrom mlcomp.db.core import PaginatorOptions\nfrom mlcomp.db.enums import DagType\nfrom mlcomp.db.models import Model, Dag, Project, Task\nfrom mlcomp.db.providers.base import BaseDataProvider\nfrom mlcomp.utils.config import Config\nfrom mlcomp.utils.io import yaml_load\nfrom mlcomp.utils.misc import parse_time\n\n\nclass ModelProvider(BaseDataProvider):\n    model = Model\n\n    def get(self, filter, options: PaginatorOptions):\n        query = self.query(Model). \\\n            options(joinedload(Model.dag_rel, innerjoin=False)). \\\n            options(joinedload(Model.project_rel, innerjoin=True))\n\n        if filter.get(\'project\'):\n            query = query.filter(Model.project == filter[\'project\'])\n        if filter.get(\'name\'):\n            query = query.filter(Model.name.like(f\'%{filter[""name""]}%\'))\n\n        if filter.get(\'created_min\'):\n            created_min = parse_time(filter[\'created_min\'])\n            query = query.filter(Model.created >= created_min)\n        if filter.get(\'created_max\'):\n            created_max = parse_time(filter[\'created_max\'])\n            query = query.filter(Model.created <= created_max)\n\n        total = query.count()\n        paginator = self.paginator(query, options) if options else query\n        res = []\n        models = paginator.all()\n        models_projects = set()\n        for model in models:\n            if not model.project_rel:\n                model.project_rel = self.query(Project).filter(\n                    Project.id == model.project).one()\n\n            row = self.to_dict(model, rules=(\'-project_rel.class_names\',))\n            res.append(row)\n            models_projects.add(model.project)\n\n        last_activity = func.max(Task.last_activity).label(\'last_activity\')\n        projects = self.query(Project.name, Project.id,\n                              last_activity). \\\n            join(Dag, Dag.project == Project.id, isouter=True). \\\n            join(Task, isouter=True). \\\n            group_by(Project.id). \\\n            order_by(last_activity.desc()). \\\n            all()\n        projects = [{\'name\': name, \'id\': id} for name, id, _ in projects]\n        return {\'total\': total, \'data\': res, \'projects\': projects}\n\n    def change_dag(self, project: int, name: str, to: int):\n        ids = self.query(Model.id). \\\n            join(Dag). \\\n            filter(Model.project == project). \\\n            filter(Dag.name == name). \\\n            filter(Dag.type == DagType.Pipe.value). \\\n            all()\n\n        ids = [id[0] for id in ids]\n\n        self.query(Model). \\\n            filter(Model.id.in_(ids)). \\\n            update({\'dag\': to}, synchronize_session=False)\n        self.commit()\n\n    def model_start_begin(self, model_id: int):\n        model = self.by_id(model_id)\n\n        models_dags = self.query(Dag). \\\n            filter(Dag.type == DagType.Pipe.value). \\\n            filter(Dag.project == model.project). \\\n            order_by(Dag.id.desc()). \\\n            all()\n\n        used_dag_names = set()\n        versions = yaml_load(model.equations)\n\n        res_dags = []\n        res_dag = None\n\n        for dag in models_dags:\n            if dag.name in used_dag_names:\n                continue\n            config = Config.from_yaml(dag.config)\n            d = {\n                \'name\': dag.name,\n                \'id\': dag.id,\n                \'pipes\': [{\n                    \'name\': p\n                } for p in config[\'pipes\']]\n            }\n            for pipe in d[\'pipes\']:\n                pipe[\'versions\'] = versions.get(pipe[\'name\'], [])\n                used = [v.get(\'used\', datetime.datetime.min) for v in\n                        pipe[\'versions\']]\n                pipe[\'used\'] = datetime.datetime.min if len(\n                    used) == 0 else max(used)\n\n            d[\'pipes\'] = sorted(d[\'pipes\'], key=lambda x: x[\'used\'],\n                                reverse=True)\n            for p in d[\'pipes\']:\n                del p[\'used\']\n                for v in p[\'versions\']:\n                    if \'used\' in v:\n                        del v[\'used\']\n\n            used_dag_names.add(dag.name)\n            res_dags.append(d)\n\n            if d[\'id\'] == model.dag:\n                res_dag = d\n\n        return {\'dags\': res_dags, \'dag\': res_dag, \'model_id\': model_id}\n\n\n__all__ = [\'ModelProvider\']\n'"
mlcomp/db/providers/project.py,0,"b'import os\nfrom typing import List\n\nfrom sqlalchemy import func\n\nfrom mlcomp import DATA_FOLDER, MODEL_FOLDER\nfrom mlcomp.db.core import PaginatorOptions\nfrom mlcomp.db.models import Project, Dag, Task\nfrom mlcomp.db.providers.base import BaseDataProvider\nfrom mlcomp.utils.io import yaml_dump\n\n\nclass ProjectProvider(BaseDataProvider):\n    model = Project\n\n    def add_project(\n            self,\n            name: str,\n            class_names: dict = None,\n            sync_folders: List[str] = None,\n            ignore_folders: List[str] = None\n    ):\n        class_names = class_names or {}\n        ignore_folders = ignore_folders or []\n\n        assert type(class_names) == dict, \'class_names type must be dict\'\n        assert isinstance(ignore_folders, list), \\\n            \'ignore_folders type must be list\'\n\n        project = Project(\n            name=name,\n            class_names=yaml_dump(class_names),\n            sync_folders=yaml_dump(sync_folders),\n            ignore_folders=yaml_dump(ignore_folders),\n        )\n        project = self.session.add(project)\n\n        os.makedirs(os.path.join(DATA_FOLDER, name), exist_ok=True)\n        os.makedirs(os.path.join(MODEL_FOLDER, name), exist_ok=True)\n\n        return project\n\n    def edit_project(\n            self,\n            name: str,\n            class_names: dict,\n            sync_folders: List[str],\n            ignore_folders: List[str]\n    ):\n        assert type(class_names) == dict, \'class_names type must be dict\'\n        assert isinstance(ignore_folders, list), \\\n            \'ignore_folders type must be list\'\n\n        project = self.by_name(name)\n        project.class_names = yaml_dump(class_names)\n        project.sync_folders = yaml_dump(sync_folders)\n        project.ignore_folders = yaml_dump(ignore_folders)\n        self.commit()\n\n    def get(self, filter: dict = None, options: PaginatorOptions = None):\n        filter = filter or {}\n\n        query = self.query(Project,\n                           func.count(Dag.id),\n                           func.max(Task.last_activity)). \\\n            join(Dag, Dag.project == Project.id, isouter=True). \\\n            join(Task, isouter=True). \\\n            group_by(Project.id)\n\n        if filter.get(\'name\'):\n            query = query.filter(Project.name.like(f\'%{filter[""name""]}%\'))\n\n        total = query.count()\n        paginator = self.paginator(query, options)\n        res = []\n        for p, dag_count, last_activity in paginator.all():\n            last_activity = self.serializer.serialize_datetime(last_activity) \\\n                if last_activity else None\n\n            file_size, img_size = self.query(func.sum(Dag.file_size),\n                                             func.sum(Dag.img_size)).filter(\n                Dag.project == p.id).one()\n\n            res.append(\n                {\n                    \'dag_count\': dag_count,\n                    \'last_activity\': last_activity,\n                    \'img_size\': int(img_size or 0),\n                    \'file_size\': int(file_size or 0),\n                    \'id\': p.id,\n                    \'name\': p.name,\n                    \'sync_folders\': p.sync_folders,\n                    \'ignore_folders\': p.ignore_folders,\n                    \'class_names\': p.class_names\n                })\n        return {\'total\': total, \'data\': res}\n\n    def all_last_activity(self):\n        query = self.query(Project,\n                           func.max(Task.last_activity)). \\\n            join(Dag, Dag.project == Project.id, isouter=True). \\\n            join(Task, isouter=True). \\\n            group_by(Project.id)\n\n        res = query.all()\n        for p, last_activity in res:\n            p.last_activity = last_activity\n        return [r[0] for r in res]\n\n    def by_name(self, name: str):\n        return self.query(Project).filter(Project.name == name).first()\n\n\n__all__ = [\'ProjectProvider\']\n'"
mlcomp/db/providers/space.py,0,"b""from sqlalchemy import literal_column, func\n\nfrom mlcomp.db.core import PaginatorOptions\nfrom mlcomp.db.models import Space\nfrom mlcomp.db.models.space import SpaceRelation, SpaceTag\nfrom mlcomp.db.providers.base import BaseDataProvider\n\n\nclass SpaceProvider(BaseDataProvider):\n    model = Space\n\n    def get(self, filter: dict, options: PaginatorOptions = None):\n        query = self.query(Space, literal_column('0').label('relation'))\n        if 'parent' in filter:\n            query = query.filter(Space.name != filter['parent'])\n\n        if filter.get('name'):\n            query = query.filter(Space.name.contains(filter['name']))\n\n        tags = filter.get('tags', [])\n        if len(tags) > 0:\n            query = query.join(SpaceTag).filter(SpaceTag.tag.in_(tags))\n\n        if filter.get('parent'):\n            relation = literal_column('1').label('relation')\n            query2 = self.query(Space, relation). \\\n                join(SpaceRelation, SpaceRelation.child == Space.name).filter(\n                SpaceRelation.parent == filter['parent']\n            )\n            query2_names = [space.name for space, _ in query2.all()]\n            query = query.filter(Space.name.notin_(query2_names))\n            query = query.union_all(query2)\n            query = query.order_by(relation.desc())\n\n        total = query.count()\n        paginator = self.paginator(query, options) if options else query\n        data = []\n\n        for space, relation in paginator.all():\n            item = self.to_dict(space)\n            item['relation'] = relation\n            data.append(item)\n\n        names = [d['name'] for d in data]\n        tags = self.query(SpaceTag).filter(SpaceTag.space.in_(names))\n        for d in data:\n            d['tags'] = []\n            for t in tags:\n                if d['name'] == t.space:\n                    d['tags'].append(t.tag)\n\n        return {\n            'total': total,\n            'data': data\n        }\n\n    def tags(self, name: str):\n        tag_count = func.count(SpaceTag.tag).label('count')\n        query = self.query(SpaceTag.tag, tag_count)\n        if name:\n            query = query.filter(SpaceTag.tag.contains(name))\n\n        tags = query.group_by(\n            SpaceTag.tag).order_by(tag_count.desc()).limit(10)\n        tags = [t for t, c in tags]\n        return {'tags': tags}\n\n    def add_relation(self, parent: str, child: str):\n        self.add(SpaceRelation(parent=parent, child=child))\n\n    def remove_relation(self, parent: str, child: str):\n        self.query(SpaceRelation).filter(\n            SpaceRelation.parent == parent).filter(\n            SpaceRelation.child == child).delete(synchronize_session=False)\n\n        self.session.commit()\n\n    def related(self, parent: str):\n        res = self.query(Space).join(SpaceRelation,\n                                     SpaceRelation.child == Space.name) \\\n            .filter(SpaceRelation.parent == parent) \\\n            .all()\n        return res\n\n    def remove_tag(self, space: str, tag: str):\n        self.query(SpaceTag).filter(SpaceTag.space == space).filter(\n            SpaceTag.tag == tag).delete(synchronize_session=False)\n\n        self.session.commit()\n\n    def names(self, name: str):\n        query = self.query(Space.name)\n        if name:\n            query = query.filter(Space.name.contains(name))\n\n        res = query.group_by(Space.name).order_by(Space.name).limit(10).all()\n        res = [r[0] for r in res]\n        return {'names': res}\n\n\n__all__ = ['SpaceProvider']\n"""
mlcomp/db/providers/step.py,0,"b""from sqlalchemy import func, case\n\nfrom mlcomp.db.enums import LogStatus\nfrom mlcomp.db.models import Step, Log\nfrom mlcomp.db.providers.base import BaseDataProvider\nfrom mlcomp.utils.misc import to_snake, now\n\n\nclass StepProvider(BaseDataProvider):\n    model = Step\n\n    def _hierarchy(self, parent: dict, steps: list, start: int, end: int):\n        current = start\n        while current <= end:\n            s = steps[current][0]\n            if s.level == parent['level'] + 1:\n                child = {**self.step_info(steps[current]), 'children': []}\n                parent['children'].append(child)\n                for i in range(current + 1, end + 1):\n                    if steps[i][0].level <= s.level:\n                        self._hierarchy(child, steps, current + 1, i - 1)\n                        current = i\n                        break\n                    if i == len(steps) - 1:\n                        self._hierarchy(child, steps, current + 1, i)\n                else:\n                    break\n            else:\n                current += 1\n\n    def step_info(self, step):\n        step, *log_status = step\n        duration = ((step.finished if step.finished else now()) - step.started)\n        res = {\n            'id': step.id,\n            'name': step.name,\n            'level': step.level,\n            'duration': duration.total_seconds(),\n            'log_statuses': [\n                {\n                    'name': to_snake(e.name),\n                    'count': s\n                } for e, s in zip(LogStatus, log_status)\n            ]\n        }\n        return res\n\n    def get(self, task_id: int):\n        log_status = []\n        for s in LogStatus:\n            log_status.append(\n                func.sum(\n                    case(\n                        whens=[(Log.level == s.value, 1)],\n                        else_=0\n                    ).label(s.name)\n                )\n            )\n\n        query = self.query(Step, *log_status).filter(Step.task == task_id\n                                                     ).order_by(Step.id)\n        query = query.join(Log, isouter=True).group_by(Step.id)\n        steps = query.all()\n        if len(steps) == 0:\n            return []\n        d = self.step_info(steps[0]) if len(steps) > 0 else dict()\n\n        hierarchy = {**d, 'children': []}\n        self._hierarchy(hierarchy, steps, 1, len(steps) - 1)\n        return {'data': [hierarchy]}\n\n    def last_for_task(self, id: int):\n        return self.query(Step).filter(Step.task == id\n                                       ).order_by(Step.started.desc()).first()\n\n    def unfinished(self, task_id: int):\n        return self.query(Step).filter(Step.task == task_id\n                                       ).filter(Step.finished.__eq__(None)\n                                                ).order_by(Step.started).all()\n\n\n__all__ = ['StepProvider']\n"""
mlcomp/db/providers/task.py,0,"b'import datetime\nfrom typing import List, Union\n\nfrom sqlalchemy import func, case\nfrom sqlalchemy.orm import joinedload, aliased\n\nfrom mlcomp.db.core import PaginatorOptions\nfrom mlcomp.db.providers.base import BaseDataProvider\nfrom mlcomp.db.enums import TaskType, DagType, TaskStatus\nfrom mlcomp.utils.misc import to_snake, duration_format, now, parse_time\nfrom mlcomp.db.models import Task, Project, Dag, TaskDependence, ReportTasks\n\n\nclass TaskProvider(BaseDataProvider):\n    model = Task\n\n    def _get_filter(self, query, filter: dict):\n        if filter.get(\'dag\'):\n            query = query.filter(Task.dag == filter[\'dag\'])\n\n        if filter.get(\'name\'):\n            query = query.filter(Task.name.like(f\'%{filter[""name""]}%\'))\n\n        if filter.get(\'status\'):\n            status = [\n                TaskStatus.from_name(k) for k, v in filter[\'status\'].items()\n                if v\n            ]\n            if len(status) > 0:\n                query = query.filter(Task.status.in_(status))\n\n        if filter.get(\'id_min\'):\n            query = query.filter(Task.id >= filter[\'id_min\'])\n\n        if filter.get(\'id_max\'):\n            query = query.filter(Task.id <= filter[\'id_max\'])\n\n        if filter.get(\'id\'):\n            query = query.filter(Task.id == filter[\'id\'])\n\n        if filter.get(\'project\'):\n            query = query.filter(Dag.project == filter[\'project\'])\n\n        if filter.get(\'created_min\'):\n            created_min = parse_time(filter[\'created_min\'])\n            query = query.filter(Dag.created >= created_min)\n        if filter.get(\'created_max\'):\n            created_max = parse_time(filter[\'created_max\'])\n            query = query.filter(Dag.created <= created_max)\n        if filter.get(\'last_activity_min\'):\n            last_activity_min = parse_time(filter[\'last_activity_min\'])\n            query = query.filter(Task.last_activity >= last_activity_min)\n        if filter.get(\'last_activity_max\'):\n            last_activity_max = parse_time(filter[\'last_activity_max\'])\n            query = query.filter(Task.last_activity <= last_activity_max)\n        if filter.get(\'report\'):\n            query = query.filter(Task.report is not None)\n        if filter.get(\'parent\'):\n            query = query.filter(Task.parent == filter[\'parent\'])\n\n        types = filter.get(\'type\', [\'User\', \'Train\'])\n        types = [TaskType.from_name(t) for t in types]\n        query = query.filter(Task.type.in_(types))\n\n        return query\n\n    def get(self, filter: dict, options: PaginatorOptions):\n        query = self.query(Task, Dag, Project.name). \\\n            join(Dag, Dag.id == Task.dag). \\\n            join(Project, Project.id == Dag.project)\n\n        query = self._get_filter(query, filter)\n\n        total = query.count()\n        paginator = self.paginator(query, options)\n        res = []\n\n        for p, d, project_name in paginator.all():\n            # noinspection PyDictCreation\n            item = {**self.to_dict(p, rules=(\'-additional_info\',))}\n            item[\'status\'] = to_snake(TaskStatus(item[\'status\']).name)\n            item[\'type\'] = to_snake(TaskType(item[\'type\']).name)\n            item[\'dag_rel\'] = self.to_dict(d)\n            item[\'dag_rel\'][\'project\'] = {\n                \'id\': item[\'dag_rel\'][\'project\'],\n                \'name\': project_name\n            }\n            if p.started is None:\n                delta = 0\n            elif p.status == TaskStatus.InProgress.value:\n                delta = (now() - p.started).total_seconds()\n            else:\n                finish = (p.finished or p.last_activity)\n                delta = (finish - p.started).total_seconds()\n            item[\'duration\'] = duration_format(delta)\n            res.append(item)\n\n        if filter.get(\'report\'):\n            tasks_within_report = self.query(\n                ReportTasks.task\n            ).filter(ReportTasks.report == int(filter[\'report\']))\n            tasks_within_report = {t[0] for t in tasks_within_report}\n            for r in res:\n                r[\'report_full\'] = r[\'id\'] in tasks_within_report\n\n        projects = self.query(Project.name, Project.id). \\\n            order_by(Project.id.desc()). \\\n            limit(20). \\\n            all()\n\n        dags = self.query(Dag.name, Dag.id). \\\n            order_by(Dag.id.desc()). \\\n            limit(20). \\\n            all()\n\n        projects = [{\'name\': name, \'id\': id} for name, id in projects]\n        dags = [{\'name\': name, \'id\': id} for name, id in dags]\n\n        dags_model = self.query(Dag.name, Dag.id, Dag.project). \\\n            filter(Dag.type == DagType.Pipe.value). \\\n            order_by(Dag.id.desc()). \\\n            all()\n\n        dags_model_dict = []\n        used_dag_names = set()\n\n        for name, id, project in dags_model:\n            if name in used_dag_names:\n                continue\n\n            dag = {\'name\': name, \'id\': id, \'project\': project}\n            dags_model_dict.append(dag)\n            used_dag_names.add(name)\n\n        return {\n            \'total\': total,\n            \'data\': res,\n            \'projects\': projects,\n            \'dags\': dags,\n            \'dags_model\': dags_model_dict\n        }\n\n    def last_tasks(self, min_time: datetime, status: int, joined_load=None):\n        res = self.query(Task).filter(\n            Task.finished >= min_time). \\\n            filter(Task.status == status)\n\n        if joined_load is not None:\n            for n in joined_load:\n                res = res.options(joinedload(n, innerjoin=True))\n\n        return res.all()\n\n    def add_dependency(self, task_id: int, depend_id: int) -> None:\n        self.add(TaskDependence(task_id=task_id, depend_id=depend_id))\n\n    def by_id(self, id, options=None) -> Task:\n        query = self.query(Task).filter(Task.id == id)\n        if options:\n            query = query.options(options)\n        return query.one_or_none()\n\n    def by_ids(self, ids, options=None) -> List[Task]:\n        query = self.query(Task).filter(Task.id.in_(ids))\n        if options:\n            query = query.options(options)\n        return query.all()\n\n    def change_status(self, task, status: TaskStatus):\n        if status == TaskStatus.InProgress:\n            task.started = now()\n        elif status in [\n            TaskStatus.Failed, TaskStatus.Stopped, TaskStatus.Success\n        ]:\n            task.finished = now()\n\n        task.status = status.value\n        self.update()\n        self.commit()\n\n    def change_status_all(self, tasks: List[int], status: TaskStatus):\n        updates = {\'status\': status.value}\n        if status == TaskStatus.InProgress:\n            updates[\'started\'] = now()\n        elif status in [\n            TaskStatus.Failed, TaskStatus.Stopped, TaskStatus.Success\n        ]:\n            updates[\'finished\'] = now()\n\n        self.query(Task). \\\n            filter(Task.id.in_(tasks)). \\\n            update(updates,\n                   synchronize_session=False)\n        self.commit()\n\n    def by_status(\n            self,\n            *statuses: TaskStatus,\n            task_docker_assigned: str = None,\n            worker_index: int = None,\n            computer_assigned: str = None,\n            project: int = None\n    ):\n        statuses = [s.value for s in statuses]\n        query = self.query(Task).filter(Task.status.in_(statuses)). \\\n            options(joinedload(Task.dag_rel, innerjoin=True))\n\n        if task_docker_assigned:\n            query = query.filter(Task.docker_assigned == task_docker_assigned)\n        if worker_index is not None:\n            query = query.filter(Task.worker_index == worker_index)\n        if computer_assigned is not None:\n            query = query.filter(Task.computer_assigned == computer_assigned)\n        if project:\n            query = query.filter(Dag.project == project)\n        return query.all()\n\n    def dependency_status(self, tasks: List[Task]):\n        res = {t.id: set() for t in tasks}\n        task_ids = [task.id for task in tasks]\n        items = self.query(TaskDependence, Task). \\\n            filter(TaskDependence.task_id.in_(task_ids)). \\\n            join(Task, Task.id == TaskDependence.depend_id).all()\n        for item, task in items:\n            res[item.task_id].add(task.status)\n\n        return res\n\n    def update_last_activity(self, task: int):\n        self.query(Task).filter(Task.id == task\n                                ).update({\'last_activity\': now()})\n        self.session.commit()\n\n    def stop(self, id: int = None, tasks: List[Task] = None):\n        if id is not None:\n            task = self.by_id(id)\n            self.change_status(task, TaskStatus.Stopped)\n        if tasks is not None:\n            self.change_status_all(tasks=[t.id for t in tasks],\n                                   status=TaskStatus.Stopped)\n\n    def last_succeed_time(self):\n        res = self.query(Task.finished). \\\n            filter(Task.status == TaskStatus.Success.value). \\\n            order_by(Task.finished.desc()). \\\n            first()\n        return res[0] if res else None\n\n    def by_dag(self, id: int):\n        return self.query(Task).filter(Task.dag == id).order_by(Task.id).all()\n\n    def parent_tasks_stats(self):\n        task_parent = aliased(Task)\n        task_child = aliased(Task)\n\n        task_status = []\n        for e in TaskStatus:\n            task_status.append(\n                func.sum(\n                    case(whens=[(task_child.status == e.value, 1)],\n                         else_=0).label(e.name)\n                )\n            )\n\n        times = [func.min(task_child.started), func.max(task_child.finished)]\n\n        parent_statuses = [\n            TaskStatus.Queued.value, TaskStatus.InProgress.value\n        ]\n\n        query = self.query(task_parent, *times, *task_status). \\\n            filter(task_parent.status.in_(parent_statuses)). \\\n            filter(task_child.continued.__eq__(False)). \\\n            join(task_child, task_parent.id == task_child.parent,\n                 isouter=True). \\\n            group_by(task_parent.id)\n\n        res = []\n        for task, started, finished, *(statuses) in query.all():\n            res.append(\n                [\n                    task, started, finished,\n                    {e: s\n                     for e, s in zip(TaskStatus, statuses)}\n                ]\n            )\n\n        return res\n\n    def has_id(self, id: int):\n        return self.query(Task).filter(Task.id == id).count() > 0\n\n    def children(self, ids: Union[int, List[int]], joined_load=None):\n        if isinstance(ids, int):\n            ids = [ids]\n\n        res = self.query(Task).filter(Task.parent.in_(ids)).filter(\n            Task.continued.__eq__(False))\n\n        res = res.order_by(Task.id)\n        if joined_load is not None:\n            for n in joined_load:\n                res = res.options(joinedload(n, innerjoin=True))\n        return res.all()\n\n    def project(self, task_id: int):\n        return self.query(Project).join(Dag).join(Task).filter(\n            Task.id == task_id\n        ).one()\n\n    def find_dependents(self, task_id: int):\n        res = self.query(Task). \\\n            join(TaskDependence, Task.id == TaskDependence.depend_id). \\\n            filter(TaskDependence.task_id == task_id). \\\n            all()\n        return res\n\n    def get_dependencies(self, dag: int):\n        res = self.query(TaskDependence).join(\n            Task, Task.id == TaskDependence.task_id).\\\n            filter(Task.dag == dag).\\\n            all()\n\n        return res\n\n\n__all__ = [\'TaskProvider\']\n'"
mlcomp/db/providers/task_synced.py,0,"b""from collections import defaultdict\n\nfrom sqlalchemy import and_\n\nfrom mlcomp.db.enums import TaskStatus, TaskType\nfrom mlcomp.db.models import TaskSynced, Task, Dag, Project, Computer\nfrom mlcomp.db.providers.base import BaseDataProvider\n\n\nclass TaskSyncedProvider(BaseDataProvider):\n    model = TaskSynced\n\n    def for_computer(self, name: str):\n        query = self.query(Task).filter(\n            Task.status == TaskStatus.Success.value). \\\n            filter(Task.type <= TaskType.Train.value). \\\n            filter(Task.computer_assigned.__ne__(None)). \\\n            join(TaskSynced, and_(TaskSynced.task == Task.id,\n                                  TaskSynced.computer == name), isouter=True).\\\n            filter(TaskSynced.task.__eq__(None))\n\n        res = []\n        by_computer_project = defaultdict(list)\n        for task in query.all():\n            if task.computer_assigned == name:\n                continue\n            dag = self.query(Dag).filter(Dag.id == task.dag).one()\n            by_computer_project[(task.computer_assigned, dag.project)].append(\n                task)\n\n        for (c, p), tasks in by_computer_project.items():\n            c = self.query(Computer).filter(Computer.name == c).one()\n            p = self.query(Project).filter(Project.id == p).one()\n\n            res.append((c, p, tasks))\n        return res\n\n\n__all__ = ['TaskSyncedProvider']\n"""
mlcomp/db/report_info/__init__.py,0,"b""from .f1 import ReportLayoutF1\nfrom .img_classify import ReportLayoutImgClassify\nfrom .info import ReportLayoutInfo\nfrom .metric import ReportLayoutMetric\nfrom .precision_recall import ReportLayoutPrecisionRecall\nfrom .series import ReportLayoutSeries\nfrom .item import ReportLayoutItem\n\n__all__ = [\n    'ReportLayoutF1', 'ReportLayoutImgClassify', 'ReportLayoutInfo',\n    'ReportLayoutMetric', 'ReportLayoutPrecisionRecall', 'ReportLayoutSeries',\n    'ReportLayoutItem'\n]\n"""
mlcomp/db/report_info/f1.py,0,"b""import numpy as np\n\nfrom sklearn.metrics import classification_report\n\nfrom mlcomp.db.report_info.item import ReportLayoutItem\nfrom mlcomp.utils.plot import figure_to_binary, plot_classification_report\n\n\nclass ReportLayoutF1(ReportLayoutItem):\n    def plot(self, y: np.array, pred: np.array):\n        report = classification_report(y, pred)\n        fig = plot_classification_report(report)\n        return figure_to_binary(fig, dpi=70)\n\n\n__all__ = ['ReportLayoutF1']\n"""
mlcomp/db/report_info/img_classify.py,0,"b""from collections import OrderedDict\n\nfrom mlcomp.db.report_info.item import ReportLayoutItem\n\n\nclass ReportLayoutImgClassify(ReportLayoutItem):\n    def __init__(\n        self,\n        name: str,\n        confusion_matrix: bool,\n    ):\n        super().__init__(name)\n\n        self.confusion_matrix = confusion_matrix\n\n    @classmethod\n    def from_dict(cls, name: str, value: OrderedDict):\n        value.pop('type')\n        confusion_matrix = value.pop('confusion_matrix', False)\n\n        assert len(value) == 0, f'Unknown parameter in ' \\\n            f'report.img_classify={value.popitem()}'\n        return cls(\n            name,\n            confusion_matrix=confusion_matrix\n        )\n\n\n__all__ = ['ReportLayoutImgClassify']\n"""
mlcomp/db/report_info/img_segment.py,0,"b""from collections import OrderedDict\n\nfrom mlcomp.db.report_info.item import ReportLayoutItem\n\n\nclass ReportLayoutImgSegment(ReportLayoutItem):\n    def __init__(\n        self,\n        name: str,\n        max_height: int,\n        max_width: int\n    ):\n        super().__init__(name)\n\n        self.max_height = max_height\n        self.max_width = max_width\n\n    @classmethod\n    def from_dict(cls, name: str, value: OrderedDict):\n        value.pop('type')\n        max_height = value.pop('max_height', None)\n        max_width = value.pop('max_width', None)\n\n        assert len(value) == 0, f'Unknown parameter in ' \\\n            f'report.img_segment={value.popitem()}'\n\n        return cls(\n            name,\n            max_height=max_height,\n            max_width=max_width\n        )\n\n\n__all__ = ['ReportLayoutImgSegment']\n"""
mlcomp/db/report_info/info.py,0,"b'from typing import List\nfrom copy import deepcopy\n\nfrom mlcomp.db.report_info.f1 import ReportLayoutF1\nfrom mlcomp.db.report_info.img_classify import ReportLayoutImgClassify\nfrom mlcomp.db.report_info.img_segment import ReportLayoutImgSegment\nfrom mlcomp.db.report_info.metric import ReportLayoutMetric\nfrom mlcomp.db.report_info.precision_recall import ReportLayoutPrecisionRecall\nfrom mlcomp.db.report_info.series import ReportLayoutSeries\n\n\nclass ReportLayoutInfo:\n    def __init__(self, data: dict):\n        data[\'items\'] = data.get(\'items\', {})\n        data[\'metric\'] = data.get(\'metric\', {\n            \'minimize\': True,\n            \'name\': \'loss\'\n        })\n        data[\'layout\'] = data.get(\'layout\', [])\n\n        self.data = data\n        self.series = self._get_series()\n        self.precision_recall = self._get_precision_recall()\n        self.metric = self._get_metric()\n        self.f1 = self._get_f1()\n        self.img_classify = self._get_img_classify()\n        self.img_segment = self._get_img_segment()\n        self.layout = {\'type\': \'root\', \'items\': data[\'layout\']}\n        self._check_layout(self.layout)\n\n    def _check_layout(self, item):\n        types = [\n            \'root\', \'panel\', \'blank\', \'series\', \'table\', \'img_classify\', \'img\',\n            \'img_segment\'\n        ]\n        assert item.get(\'type\') in types, f\'Unknown item type = {item[""type""]}\'\n\n        fields = {\n            \'root\': [(\'items\', False)],\n            \'panel\': [\n                \'title\', (\'parent_cols\', False), (\'cols\', False),\n                (\'row_height\', False), (\'rows\', False), (\'items\', False),\n                (\'expanded\', False), (\'table\', False)\n            ],\n            \'blank\': [(\'cols\', False), (\'rows\', False)],\n            \'series\': [\n                (\'multi\', False), (\'group\', False), \'source\', (\'cols\', False),\n                (\'rows\', False)\n            ],\n            \'table\': [\n                \'source\',\n                (\'cols\', False),\n                (\'rows\', False),\n            ],\n            \'img_classify\': [\n                \'source\', (\'attrs\', False), (\'cols\', False), (\'rows\', False)\n            ],\n            \'img_segment\': [\n                \'source\', (\'attrs\', False), (\'cols\', False), (\'rows\', False),\n                (\'max_width\', False), (\'max_height\', False)\n            ],\n            \'img\': [\'source\', (\'cols\', False), (\'rows\', False)]\n        }\n        keys = set(item.keys()) - {\'type\'}\n        for f in fields[item[\'type\']]:\n            req = True\n            if type(f) == tuple:\n                f, req = f\n            if req and f not in item:\n                raise Exception(f\'Type {item[""type""]} must contain field {f}\')\n            if f in keys:\n                keys.remove(f)\n        assert len(keys) == 0, \\\n            f\'Unknown fields {keys} for type = {item[""type""]}\'\n\n        if \'items\' in item:\n            for item in item[\'items\']:\n                self._check_layout(item)\n\n    def has_classification(self):\n        return len(self.precision_recall) > 0\n\n    def _by_type(self, t: str, c):\n        return [\n            c.from_dict(k, v) for k, v in self.data[\'items\'].items()\n            if v.get(\'type\') == t\n        ]\n\n    def _get_img_classify(self) -> List[ReportLayoutImgClassify]:\n        return self._by_type(\'img_classify\', ReportLayoutImgClassify)\n\n    def _get_img_segment(self) -> List[ReportLayoutImgSegment]:\n        return self._by_type(\'img_segment\', ReportLayoutImgSegment)\n\n    def _get_f1(self) -> List[ReportLayoutF1]:\n        return self._by_type(\'f1\', ReportLayoutF1)\n\n    def _get_series(self) -> List[ReportLayoutSeries]:\n        return self._by_type(\'series\', ReportLayoutSeries)\n\n    def _get_precision_recall(self) -> List[ReportLayoutPrecisionRecall]:\n        return self._by_type(\'precision_recall\', ReportLayoutPrecisionRecall)\n\n    def _get_metric(self) -> ReportLayoutMetric:\n        return ReportLayoutMetric.from_dict(self.data[\'metric\'])\n\n    @classmethod\n    def union_layouts(cls, name: str, layouts: dict, return_dict: bool = True):\n        assert name in layouts, f\'Layout {name} is not in the collection\'\n\n        layout = deepcopy(layouts[name])\n        r = dict()\n        if layout.get(\'extend\'):\n            assert layout[\'extend\'] in layouts, \\\n                f\'Layout for extending = {layout[""extend""]}\' \\\n                f\' is not in the collection\'\n            r = cls.union_layouts(layout[\'extend\'], layouts)\n\n        if \'metric\' in layout:\n            r[\'metric\'] = layout[\'metric\']\n\n        if \'items\' in layout:\n            r[\'items\'] = r.get(\'items\', dict())\n            r[\'items\'].update(layout[\'items\'])\n\n        if \'layout\' in layout:\n            r[\'layout\'] = r.get(\'layout\', []) + layout[\'layout\']\n\n        if return_dict:\n            return r\n\n        return ReportLayoutInfo(r)\n\n\n__all__ = [\'ReportLayoutInfo\']\n'"
mlcomp/db/report_info/item.py,0,"b""from collections import OrderedDict\n\n\nclass ReportLayoutItem:\n    def __init__(self, name: str):\n        self.name = name\n\n    @classmethod\n    def from_dict(cls, name: str, value: OrderedDict):\n        value.pop('type')\n        assert len(value) == 0, f'Unknown parameter in ' \\\n            f'report info item = {name}: {value.popitem()}'\n        return cls(name)\n\n\n__all__ = ['ReportLayoutItem']\n"""
mlcomp/db/report_info/metric.py,0,"b""class ReportLayoutMetric:\n    def __init__(self, name: str, minimize: bool):\n        self.name = name\n        self.minimize = minimize\n\n    @staticmethod\n    def from_dict(data: dict):\n        name = data.pop('name')\n        minimize = data.pop('minimize')\n        assert len(data) == 0, f'Unknown parameter in ' \\\n            f'report.metric={data.popitem()}'\n        return ReportLayoutMetric(name, minimize)\n\n    def serialize(self):\n        return {'minimize': self.minimize, 'name': self.name}\n\n\n__all__ = ['ReportLayoutMetric']\n"""
mlcomp/db/report_info/precision_recall.py,0,"b""import numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.metrics import precision_recall_curve\n\nfrom mlcomp.db.report_info.item import ReportLayoutItem\nfrom mlcomp.utils.plot import figure_to_binary\n\n\nclass ReportLayoutPrecisionRecall(ReportLayoutItem):\n    def plot(self, y: np.array, pred: np.array):\n        p, r, t = precision_recall_curve(y, pred)\n        fig, ax = plt.subplots(figsize=(4.2, 2.7))\n        ax2 = ax.twinx()\n\n        t = np.hstack([t, t[-1]])\n\n        ax.plot(r, p)\n\n        ax.set_xlabel('Recall')\n        ax.set_ylabel('Precision')\n        ax.set_ylim([0.0, 1.05])\n        ax.set_xlim([0.0, 1.0])\n        ax2.set_ylabel('Threashold')\n        ax2.plot(r, t, c='red')\n        return figure_to_binary(fig)\n\n\n__all__ = ['ReportLayoutPrecisionRecall']\n"""
mlcomp/db/report_info/series.py,0,"b""from collections import OrderedDict\n\nfrom mlcomp.db.report_info.item import ReportLayoutItem\n\n\nclass ReportLayoutSeries(ReportLayoutItem):\n    def __init__(self, name: str, key: str):\n        super().__init__(name)\n\n        self.key = key\n\n    @classmethod\n    def from_dict(cls, name: str, value: OrderedDict):\n        assert 'key' in value, f'report.series={name}. key is required'\n        value.pop('type')\n        key = value.pop('key')\n\n        assert len(value) == 0, f'Unknown parameter in ' \\\n            f'report.series={name}: {value.popitem()}'\n        return cls(name, key)\n\n\n__all__ = ['ReportLayoutSeries']\n"""
mlcomp/db/tests/__init__.py,0,b''
mlcomp/db/tests/test_project.py,0,"b""# flake8: noqa\n# noinspection PyUnresolvedReferences\nfrom mlcomp.utils.tests import session\nfrom mlcomp.db.core import Session\nfrom mlcomp.db.providers import ProjectProvider\n\n\nclass TestProject(object):\n\n    def _configure(self, session):\n        provider = ProjectProvider(session)\n        provider.add_project(name='test')\n        return provider\n\n    def test_add(self, session: Session):\n        provider = ProjectProvider(session)\n        project = provider.add_project(name='test')\n        assert provider.by_id(project.id)\n\n    def test_by_name(self, session: Session):\n        provider = self._configure(session)\n        project = provider.by_name('test')\n        assert project is not None\n\n    def test_get(self, session: Session):\n        provider = self._configure(session)\n        res = provider.get()\n        assert len(res['data']) == 1\n        assert res['total'] == 1\n"""
mlcomp/migration/versions/001_init.py,0,"b""from sqlalchemy import Table, Column, Integer, String, MetaData, Float, \\\n    TIMESTAMP, Boolean, LargeBinary, Index, BigInteger\nfrom migrate.changeset.constraint import ForeignKeyConstraint, UniqueConstraint\n\nmeta = MetaData()\n\nauxiliary = Table(\n    'auxiliary', meta,\n    Column('name', String(100), primary_key=True),\n    Column('data', String(16000), nullable=False)\n)\n\ncomputer = Table(\n    'computer', meta,\n    Column('name', String(100), primary_key=True),\n    Column('gpu', Integer, default=0, nullable=False),\n    Column('cpu', Integer, default=1, nullable=False),\n    Column('memory', Float, default=0.1, nullable=False),\n    Column('usage', String(2000)),\n    Column('ip', String(100), nullable=False),\n    Column('port', Integer, nullable=False),\n    Column('user', String, nullable=False),\n    Column('last_synced', TIMESTAMP),\n    Column('disk', Integer, nullable=False),\n    Column('syncing_computer', String(100)),\n    Column('root_folder', String(100), nullable=False),\n    Column('can_process_tasks', Boolean),\n    Column('sync_with_this_computer', Boolean),\n    Column('meta', String(10000)),\n)\n\ncomputer_usage = Table(\n    'computer_usage', meta,\n    Column('id', Integer, primary_key=True, autoincrement=True),\n    Column('computer', String(100), nullable=False),\n    Column('usage', String(4000), nullable=False),\n    Column('time', TIMESTAMP, nullable=False, default='now()')\n)\n\ndag = Table(\n    'dag', meta,\n    Column('id', Integer, primary_key=True, autoincrement=True),\n    Column('name', String(100), nullable=False),\n    Column('created', TIMESTAMP, nullable=False, default='now()'),\n    Column('config', String(8000), nullable=False),\n    Column('project', Integer, nullable=False),\n    Column('docker_img', String(100)),\n    Column('img_size', BigInteger, nullable=False),\n    Column('file_size', BigInteger, nullable=False),\n    Column('type', Integer, nullable=False),\n    Column('report', Integer)\n)\n\ndag_library = Table(\n    'dag_library', meta,\n    Column('id', Integer, primary_key=True, autoincrement=True),\n    Column('library', String(100), nullable=False),\n    Column('version', String(30), nullable=False),\n    Column('dag', Integer, nullable=False)\n)\n\ndag_storage = Table(\n    'dag_storage', meta,\n    Column('id', Integer, primary_key=True, autoincrement=True),\n    Column('dag', Integer, nullable=False),\n    Column('file', Integer),\n    Column('path', String(210), nullable=False),\n    Column('is_dir', Boolean, nullable=False, default=False)\n)\n\nfile = Table(\n    'file', meta,\n    Column('id', Integer, primary_key=True, autoincrement=True),\n    Column('md5', String(32), nullable=False),\n    Column('created', TIMESTAMP, nullable=False, default='now()'),\n    Column('content', LargeBinary, nullable=False),\n    Column('project', Integer, nullable=False),\n    Column('dag', Integer, nullable=False),\n    Column('size', BigInteger, nullable=False),\n)\n\nlog = Table(\n    'log', meta,\n    Column('id', Integer, primary_key=True, autoincrement=True),\n    Column('step', Integer),\n    Column('message', String(8000), nullable=False),\n    Column('time', TIMESTAMP, nullable=False, default='now()'),\n    Column('level', Integer, nullable=False),\n    Column('component', Integer, nullable=False),\n    Column('module', String(200), nullable=False),\n    Column('line', Integer, nullable=False),\n    Column('task', Integer),\n    Column('computer', String(100))\n)\n\nproject = Table(\n    'project', meta,\n    Column('id', Integer, primary_key=True, autoincrement=True),\n    Column('name', String(180), nullable=False),\n    Column('class_names', String(8000), nullable=False),\n    Column('ignore_folders', String(8000), nullable=False)\n)\n\nreport = Table(\n    'report', meta,\n    Column('id', Integer, primary_key=True, autoincrement=True),\n    Column('config', String(4000), nullable=False),\n    Column('time', TIMESTAMP, nullable=False, default='now()'),\n    Column('name', String(300), nullable=False),\n    Column('project', Integer, nullable=False),\n    Column('layout', String(100), nullable=False)\n)\n\nreport_img = Table(\n    'report_img', meta,\n    Column('id', Integer, primary_key=True, autoincrement=True),\n    Column('epoch', Integer, nullable=False),\n    Column('task', Integer, nullable=False),\n    Column('group', String(100), nullable=False),\n    Column('img', LargeBinary, nullable=False),\n    Column('project', Integer, nullable=False),\n    Column('dag', Integer, nullable=False),\n    Column('part', String(30)),\n    Column('y_pred', Integer),\n    Column('y', Integer),\n    Column('score', Float),\n    Column('attr1', Float),\n    Column('attr2', Float),\n    Column('attr3', Float),\n    Column('attr4', Float),\n    Column('attr5', Float),\n    Column('attr6', Float),\n    Column('attr7', Float),\n    Column('attr8', Float),\n    Column('attr9', Float),\n    Column('attr1_str', String(500)),\n    Column('attr2_str', String(500)),\n    Column('attr3_str', String(500)),\n    Column('attr4_str', String(500)),\n    Column('attr5_str', String(500)),\n    Column('size', BigInteger, nullable=False),\n)\n\nreport_layout = Table(\n    'report_layout', meta,\n    Column('name', String(400), primary_key=True),\n    Column('content', String(8000), nullable=False),\n    Column('last_modified', TIMESTAMP, nullable=False, default='now()')\n)\n\nreport_series = Table(\n    'report_series', meta,\n    Column('id', Integer, primary_key=True, autoincrement=True),\n    Column('task', Integer, nullable=False),\n    Column('time', TIMESTAMP, nullable=False, default='now()'),\n    Column('epoch', Integer, nullable=False),\n    Column('value', Float, nullable=False),\n    Column('name', String(100), nullable=False),\n    Column('part', String(50), nullable=False),\n    Column('stage', String(100), nullable=False)\n)\n\nreport_task = Table(\n    'report_task', meta,\n    Column('id', Integer, primary_key=True, autoincrement=True),\n    Column('report', Integer, nullable=False),\n    Column('task', Integer, nullable=False),\n)\n\nstep = Table(\n    'step', meta,\n    Column('id', Integer, primary_key=True, autoincrement=True),\n    Column('task', Integer, nullable=False),\n    Column('level', Integer, nullable=False),\n    Column('started', TIMESTAMP, nullable=False),\n    Column('finished', TIMESTAMP),\n    Column('name', String(100), nullable=False),\n    Column('index', Integer, nullable=False)\n)\n\ntask = Table(\n    'task', meta,\n    Column('id', Integer, primary_key=True, autoincrement=True),\n    Column('name', String(300), nullable=False),\n    Column('status', Integer, nullable=False),\n    Column('started', TIMESTAMP),\n    Column('finished', TIMESTAMP),\n    Column('computer', String(100)),\n    Column('gpu', Integer, nullable=False, default=0),\n    Column('gpu_max', Integer, nullable=False, default=0),\n    Column('cpu', Integer, nullable=False, default=1),\n    Column('executor', String(100), nullable=False),\n    Column('computer_assigned', String(100)),\n    Column('memory', Float, nullable=False, default=0.1),\n    Column('steps', Integer, nullable=False, default=1),\n    Column('current_step', String(100)),\n    Column('dag', Integer, nullable=False),\n    Column('celery_id', String(50)),\n    Column('last_activity', TIMESTAMP),\n    Column('debug', Boolean, nullable=False, default=False),\n    Column('pid', Integer),\n    Column('worker_index', Integer),\n    Column('additional_info', String(16000), nullable=False),\n    Column('docker_assigned', String(100)),\n    Column('type', Integer, nullable=False),\n    Column('score', Float),\n    Column('report', Integer),\n    Column('gpu_assigned', String(200)),\n    Column('parent', Integer),\n    Column('result', String(16000)),\n\n    Column('batch_index', Integer),\n    Column('batch_total', Integer),\n    Column('loader_name', String),\n    Column('epoch_duration', Integer),\n    Column('epoch_time_remaining', Integer),\n)\n\ntask_dependency = Table(\n    'task_dependency', meta,\n    Column('task_id', Integer, primary_key=True),\n    Column('depend_id', Integer, primary_key=True),\n)\n\ntask_synced = Table(\n    'task_synced', meta,\n    Column('computer', String(100), primary_key=True),\n    Column('task', Integer, primary_key=True),\n)\n\ndocker = Table(\n    'docker', meta,\n    Column('name', String(100), primary_key=True),\n    Column('computer', String(100), primary_key=True),\n    Column('last_activity', TIMESTAMP, nullable=False, default='now()'),\n    Column('ports', String(100), nullable=False)\n)\n\nmodel = Table(\n    'model', meta,\n    Column('id', Integer, primary_key=True, autoincrement=True),\n    Column('name', String(500), nullable=False),\n    Column('score_local', Float),\n    Column('score_public', Float),\n    Column('dag', Integer),\n    Column('project', Integer, nullable=False),\n    Column('created', TIMESTAMP, nullable=False),\n    Column('equations', String(2000000), nullable=False),\n    Column('fold', Integer, nullable=False)\n)\n\n\ndef upgrade(migrate_engine):\n    conn = migrate_engine.connect()\n    trans = conn.begin()\n\n    try:\n        meta.bind = conn\n\n        computer.create()\n        computer_usage.create()\n        dag.create()\n        dag_library.create()\n        dag_storage.create()\n        file.create()\n        log.create()\n        project.create()\n        report.create()\n        report_img.create()\n        report_series.create()\n        report_task.create()\n        step.create()\n        task.create()\n        task_dependency.create()\n        report_layout.create()\n        docker.create()\n        model.create()\n        auxiliary.create()\n        task_synced.create()\n\n        ForeignKeyConstraint([computer_usage.c.computer],\n                             [computer.c.name],\n                             ondelete='CASCADE').create()\n        ForeignKeyConstraint([computer.c.syncing_computer],\n                             [computer.c.name],\n                             ondelete='CASCADE').create()\n\n        Index('computer_usage_time_idx', computer_usage.c.time.desc()).create()\n\n        ForeignKeyConstraint([dag.c.project], [project.c.id],\n                             ondelete='CASCADE').create()\n        ForeignKeyConstraint([dag.c.report], [report.c.id],\n                             ondelete='CASCADE').create()\n\n        Index('dag_project_idx', dag.c.project.desc()).create()\n        Index('dag_created_idx', dag.c.created.desc()).create()\n        ForeignKeyConstraint([dag_library.c.dag], [dag.c.id],\n                             ondelete='CASCADE').create()\n        Index('dag_library_dag_idx', dag_library.c.dag.desc()).create()\n\n        ForeignKeyConstraint([dag_storage.c.dag], [dag.c.id],\n                             ondelete='CASCADE').create()\n        ForeignKeyConstraint([dag_storage.c.file], [file.c.id],\n                             ondelete='CASCADE').create()\n        Index('dag_storage_dag_idx', dag_storage.c.dag.desc()).create()\n\n        ForeignKeyConstraint([file.c.project], [project.c.id],\n                             ondelete='CASCADE').create()\n        Index('file_created_idx', file.c.created.desc()).create()\n        Index('file_project_idx', file.c.project.desc()).create()\n        UniqueConstraint(file.c.md5, file.c.project,\n                         name='file_md5_idx').create()\n\n        ForeignKeyConstraint([log.c.task], [task.c.id],\n                             ondelete='CASCADE').create()\n        ForeignKeyConstraint([log.c.step], [step.c.id],\n                             ondelete='CASCADE').create()\n        ForeignKeyConstraint([log.c.computer], [computer.c.name],\n                             ondelete='CASCADE').create()\n\n        Index('log_step_idx', log.c.step.desc()).create()\n        Index('log_time_idx', log.c.time.desc()).create()\n\n        UniqueConstraint(project.c.name, name='project_name').create()\n\n        ForeignKeyConstraint([report.c.project], [project.c.id],\n                             ondelete='CASCADE').create()\n\n        ForeignKeyConstraint([report_img.c.project], [project.c.id],\n                             ondelete='CASCADE').create()\n        Index('report_img_project_idx', report_img.c.project.desc()).create()\n        Index('report_img_task_idx', report_img.c.task.desc()).create()\n        Index('report_img_attr1_idx', report_img.c.attr1).create()\n        Index('report_img_attr2_idx', report_img.c.attr2).create()\n        Index('report_img_attr3_idx', report_img.c.attr3).create()\n        Index('report_img_attr4_idx', report_img.c.attr4).create()\n        Index('report_img_attr5_idx', report_img.c.attr5).create()\n        Index('report_img_attr6_idx', report_img.c.attr6).create()\n        Index('report_img_attr7_idx', report_img.c.attr7).create()\n        Index('report_img_attr8_idx', report_img.c.attr8).create()\n        Index('report_img_attr9_idx', report_img.c.attr9).create()\n        Index('report_img_attr1_str_idx', report_img.c.attr1_str).create()\n        Index('report_img_attr2_str_idx', report_img.c.attr2_str).create()\n        Index('report_img_attr3_str_idx', report_img.c.attr3_str).create()\n        Index('report_img_attr4_str_idx', report_img.c.attr4_str).create()\n        Index('report_img_attr5_str_idx', report_img.c.attr5_str).create()\n\n        ForeignKeyConstraint([report_series.c.task], [task.c.id],\n                             ondelete='CASCADE').create()\n\n        ForeignKeyConstraint([report_task.c.task], [task.c.id],\n                             ondelete='CASCADE').create()\n        ForeignKeyConstraint([report_task.c.report], [report.c.id],\n                             ondelete='CASCADE').create()\n\n        ForeignKeyConstraint([step.c.task], [task.c.id],\n                             ondelete='CASCADE').create()\n        Index('step_name_idx', step.c.name).create()\n\n        ForeignKeyConstraint([task.c.computer], [computer.c.name],\n                             ondelete='CASCADE').create()\n        ForeignKeyConstraint([task.c.report], [report.c.id],\n                             ondelete='CASCADE').create()\n        ForeignKeyConstraint([task.c.computer_assigned], [computer.c.name],\n                             ondelete='CASCADE').create()\n        ForeignKeyConstraint([task.c.dag], [dag.c.id],\n                             ondelete='CASCADE').create()\n        ForeignKeyConstraint([task.c.parent], [task.c.id],\n                             ondelete='CASCADE').create()\n        ForeignKeyConstraint(\n            [task.c.docker_assigned, task.c.computer_assigned],\n            [docker.c.name, docker.c.computer], ondelete='CASCADE').create()\n\n        Index('log_message_idx', log.c.message).create()\n        Index('task_status_idx', task.c.status).create()\n        Index('task_dag_idx', task.c.dag.desc()).create()\n        Index('task_finished_idx', task.c.finished.desc()).create()\n        Index('task_name_idx', task.c.name).create()\n        Index('task_started_idx', task.c.started.desc()).create()\n        Index('task_docker_idx', task.c.docker_assigned,\n              task.c.computer_assigned).create()\n\n        Index('task_dependency_task_idx',\n              task_dependency.c.task_id.desc()).create()\n        Index('task_dependency_depend_idx',\n              task_dependency.c.depend_id.desc()).create()\n\n        ForeignKeyConstraint([task_dependency.c.task_id], [task.c.id],\n                             ondelete='CASCADE').create()\n        ForeignKeyConstraint([task_dependency.c.depend_id], [task.c.id],\n                             ondelete='CASCADE').create()\n\n        ForeignKeyConstraint([docker.c.computer], [computer.c.name],\n                             ondelete='CASCADE').create()\n\n        ForeignKeyConstraint([model.c.dag], [dag.c.id],\n                             ondelete='CASCADE').create()\n        ForeignKeyConstraint([model.c.project], [project.c.id],\n                             ondelete='CASCADE').create()\n        UniqueConstraint(model.c.project, model.c.name,\n                         name='model_project_name_unique').create()\n\n        ForeignKeyConstraint([task_synced.c.computer], [computer.c.name],\n                             ondelete='CASCADE').create()\n        ForeignKeyConstraint([task_synced.c.task], [task.c.id],\n                             ondelete='CASCADE').create()\n    except Exception:\n        trans.rollback()\n        raise\n    else:\n        trans.commit()\n\n\ndef downgrade(migrate_engine):\n    conn = migrate_engine.connect()\n    trans = conn.begin()\n    meta.bind = conn\n    try:\n        auxiliary.drop()\n        model.drop()\n        docker.drop()\n        computer_usage.drop()\n        dag_library.drop()\n        dag_storage.drop()\n        log.drop()\n        file.drop()\n        report_task.drop()\n        report_img.drop()\n        report_series.drop()\n        report_layout.drop()\n        report.drop()\n        task_dependency.drop()\n        step.drop()\n        task.drop()\n        dag.drop()\n        computer.drop()\n        project.drop()\n    except Exception:\n        trans.rollback()\n        raise\n    else:\n        trans.commit()\n"""
mlcomp/migration/versions/002_data.py,0,"b""from glob import glob\nimport os\n\nfrom mlcomp.db.core import Session\nfrom mlcomp.db.models import ReportLayout\nfrom mlcomp.db.providers import ReportLayoutProvider\nfrom mlcomp.utils.misc import now\n\n\ndef upgrade(migrate_engine):\n    folder = os.path.dirname(__file__)\n    session = Session.create_session(connection_string=migrate_engine.url)\n    provider = ReportLayoutProvider(session)\n\n    try:\n        files = os.path.join(folder, '002', 'report_layout', '*.yml')\n        for path in glob(files):\n            name = str(os.path.basename(path).split('.')[0])\n            text = open(path).read()\n            provider.add(\n                ReportLayout(name=name, content=text, last_modified=now()),\n                commit=False\n            )\n\n        provider.commit()\n    except Exception:\n        provider.rollback()\n        raise\n\n\ndef downgrade(migrate_engine):\n    session = Session.create_session(connection_string=migrate_engine.url)\n    provider = ReportLayoutProvider(session)\n    provider.session.query(ReportLayout).delete(synchronize_session=False)\n    provider.session.commit()\n"""
mlcomp/migration/versions/003_task_loss.py,0,"b""from sqlalchemy import Table, Column, MetaData, Float\n\nmeta = MetaData()\n\n\ndef upgrade(migrate_engine):\n    conn = migrate_engine.connect()\n    trans = conn.begin()\n\n    try:\n        meta.bind = conn\n\n        task = Table('task', meta, autoload=True)\n        task_loss = Column('loss', Float)\n        task_loss.create(task)\n    except Exception:\n        trans.rollback()\n        raise\n    else:\n        trans.commit()\n\n\ndef downgrade(migrate_engine):\n    conn = migrate_engine.connect()\n    trans = conn.begin()\n\n    try:\n        meta.bind = conn\n\n        task = Table('task', meta, autoload=True)\n        task.c.loss.drop()\n    except Exception:\n        trans.rollback()\n        raise\n    else:\n        trans.commit()\n"""
mlcomp/migration/versions/004_task_continued.py,0,"b""from sqlalchemy import Table, Column, MetaData, Boolean\n\nmeta = MetaData()\n\n\ndef upgrade(migrate_engine):\n    conn = migrate_engine.connect()\n    trans = conn.begin()\n\n    try:\n        meta.bind = conn\n\n        task = Table('task', meta, autoload=True)\n        col = Column('continued', Boolean)\n        col.create(task)\n    except Exception:\n        trans.rollback()\n        raise\n    else:\n        trans.commit()\n\n\ndef downgrade(migrate_engine):\n    conn = migrate_engine.connect()\n    trans = conn.begin()\n\n    try:\n        meta.bind = conn\n\n        task = Table('task', meta, autoload=True)\n        task.c.continued.drop()\n    except Exception:\n        trans.rollback()\n        raise\n    else:\n        trans.commit()\n"""
mlcomp/migration/versions/005_project_sync_folders.py,0,"b""from sqlalchemy import Table, Column, MetaData, String\n\nmeta = MetaData()\n\n\ndef upgrade(migrate_engine):\n    conn = migrate_engine.connect()\n    trans = conn.begin()\n\n    try:\n        meta.bind = conn\n\n        table = Table('project', meta, autoload=True)\n        col = Column('sync_folders', String(8000))\n        col.create(table)\n    except Exception:\n        trans.rollback()\n        raise\n    else:\n        trans.commit()\n\n\ndef downgrade(migrate_engine):\n    conn = migrate_engine.connect()\n    trans = conn.begin()\n\n    try:\n        meta.bind = conn\n\n        table = Table('project', meta, autoload=True)\n        table.c.continued.drop()\n    except Exception:\n        trans.rollback()\n        raise\n    else:\n        trans.commit()\n"""
mlcomp/migration/versions/006_space.py,0,"b""from sqlalchemy import Table, Column, MetaData, String, TIMESTAMP\n\nmeta = MetaData()\n\n\ntable = Table(\n    'space', meta,\n    Column('name', String(200), primary_key=True),\n    Column('changed', TIMESTAMP, nullable=False),\n    Column('created', TIMESTAMP, nullable=False),\n    Column('content', String(10000), nullable=False),\n)\n\n\ndef upgrade(migrate_engine):\n    conn = migrate_engine.connect()\n    trans = conn.begin()\n\n    try:\n        meta.bind = conn\n        table.create()\n    except Exception:\n        trans.rollback()\n        raise\n    else:\n        trans.commit()\n\n\ndef downgrade(migrate_engine):\n    conn = migrate_engine.connect()\n    trans = conn.begin()\n\n    try:\n        meta.bind = conn\n        table.drop()\n    except Exception:\n        trans.rollback()\n        raise\n    else:\n        trans.commit()\n"""
mlcomp/migration/versions/007_space_relation.py,0,"b""from migrate import ForeignKeyConstraint\nfrom sqlalchemy import Table, Column, MetaData, String\n\nmeta = MetaData()\n\ntable = Table(\n    'space_relation', meta,\n    Column('parent', String(200), primary_key=True),\n    Column('child', String(200), primary_key=True),\n)\n\n\ndef upgrade(migrate_engine):\n    conn = migrate_engine.connect()\n    trans = conn.begin()\n\n    try:\n        meta.bind = conn\n        table.create()\n\n        space = Table('space', meta, autoload=True)\n        ForeignKeyConstraint([table.c.parent], [space.c.name],\n                             ondelete='CASCADE').create()\n        ForeignKeyConstraint([table.c.child], [space.c.name],\n                             ondelete='CASCADE').create()\n    except Exception:\n        trans.rollback()\n        raise\n    else:\n        trans.commit()\n\n\ndef downgrade(migrate_engine):\n    conn = migrate_engine.connect()\n    trans = conn.begin()\n\n    try:\n        meta.bind = conn\n        table.drop()\n    except Exception:\n        trans.rollback()\n        raise\n    else:\n        trans.commit()\n"""
mlcomp/migration/versions/008_space_tag.py,0,"b""from migrate import ForeignKeyConstraint\nfrom sqlalchemy import Table, Column, MetaData, String\n\nmeta = MetaData()\n\ntable = Table(\n    'space_tag', meta,\n    Column('space', String(200), primary_key=True),\n    Column('tag', String(100), primary_key=True),\n)\n\n\ndef upgrade(migrate_engine):\n    conn = migrate_engine.connect()\n    trans = conn.begin()\n\n    try:\n        meta.bind = conn\n        table.create()\n\n        space = Table('space', meta, autoload=True)\n        ForeignKeyConstraint([table.c.space], [space.c.name],\n                             ondelete='CASCADE').create()\n    except Exception:\n        trans.rollback()\n        raise\n    else:\n        trans.commit()\n\n\ndef downgrade(migrate_engine):\n    conn = migrate_engine.connect()\n    trans = conn.begin()\n\n    try:\n        meta.bind = conn\n        table.drop()\n    except Exception:\n        trans.rollback()\n        raise\n    else:\n        trans.commit()\n"""
mlcomp/migration/versions/009_dag_tag.py,0,"b""from migrate import ForeignKeyConstraint\nfrom sqlalchemy import Table, Column, MetaData, String, Integer\n\nmeta = MetaData()\n\ntable = Table(\n    'dag_tag', meta,\n    Column('dag', Integer, primary_key=True),\n    Column('tag', String(100), primary_key=True),\n)\n\n\ndef upgrade(migrate_engine):\n    conn = migrate_engine.connect()\n    trans = conn.begin()\n\n    try:\n        meta.bind = conn\n        table.create()\n\n        dag = Table('dag', meta, autoload=True)\n        ForeignKeyConstraint([table.c.dag], [dag.c.id],\n                             ondelete='CASCADE').create()\n    except Exception:\n        trans.rollback()\n        raise\n    else:\n        trans.commit()\n\n\ndef downgrade(migrate_engine):\n    conn = migrate_engine.connect()\n    trans = conn.begin()\n\n    try:\n        meta.bind = conn\n        table.drop()\n    except Exception:\n        trans.rollback()\n        raise\n    else:\n        trans.commit()\n"""
mlcomp/migration/versions/010_memory.py,0,"b""from sqlalchemy import Table, Column, MetaData, String, Integer, Float\n\nmeta = MetaData()\n\ntable = Table(\n    'memory', meta,\n    Column('id', Integer, primary_key=True),\n    Column('model', String(200), nullable=False),\n    Column('memory', Float, nullable=False),\n    Column('batch_size', Integer, nullable=False),\n    Column('num_classes', Integer),\n    Column('variant', String(200)),\n    Column('img_size', Integer),\n)\n\n\ndef upgrade(migrate_engine):\n    conn = migrate_engine.connect()\n    trans = conn.begin()\n\n    try:\n        meta.bind = conn\n        table.create()\n    except Exception:\n        trans.rollback()\n        raise\n    else:\n        trans.commit()\n\n\ndef downgrade(migrate_engine):\n    conn = migrate_engine.connect()\n    trans = conn.begin()\n\n    try:\n        meta.bind = conn\n        table.drop()\n    except Exception:\n        trans.rollback()\n        raise\n    else:\n        trans.commit()\n"""
mlcomp/server/back/__init__.py,0,b''
mlcomp/server/back/app.py,0,"b""import hashlib\nimport shutil\nimport traceback\nimport requests\nimport os\nimport simplejson as json\nfrom collections import OrderedDict\nfrom functools import wraps\n\nfrom flask import Flask, request, Response, send_from_directory\nfrom flask_cors import CORS\nfrom sqlalchemy.orm import joinedload\n\nimport mlcomp.worker.tasks as celery_tasks\nfrom mlcomp import TOKEN, WEB_PORT, WEB_HOST, FLASK_ENV, TMP_FOLDER\nfrom mlcomp.db.enums import TaskStatus, ComponentType\nfrom mlcomp.db.core import PaginatorOptions, Session\nfrom mlcomp.db.models.dag import DagTag\nfrom mlcomp.db.providers import ComputerProvider, ProjectProvider, \\\n    ReportLayoutProvider, ReportProvider, ModelProvider, ReportImgProvider, \\\n    DagProvider, DagStorageProvider, TaskProvider, LogProvider, StepProvider, \\\n    FileProvider, AuxiliaryProvider, MemoryProvider, SpaceProvider\nfrom mlcomp.db.report_info import ReportLayoutInfo\nfrom mlcomp.server.back.create_dags.copy import dag_copy\nfrom mlcomp.server.back.supervisor import register_supervisor\nfrom mlcomp.utils.logging import create_logger\nfrom mlcomp.utils.io import from_module_path, zip_folder\nfrom mlcomp.server.back.create_dags import dag_model_add, dag_model_start\nfrom mlcomp.utils.misc import now\nfrom mlcomp.db.models import Model, Report, ReportLayout, Task, File, Memory, \\\n    Space, SpaceTag\nfrom mlcomp.utils.io import yaml_load, yaml_dump\nfrom mlcomp.worker.storage import Storage\n\napp = Flask(__name__)\nCORS(app)\n\n_read_session = Session.create_session(key='server.read')\n_write_session = Session.create_session(key='server.write')\n\nlogger = create_logger(_write_session, __name__)\nsupervisor = None\n\n\n@app.route('/', defaults={'path': ''}, methods=['GET'])\n@app.route('/<path:path>', methods=['GET'])\ndef send_static(path):\n    file = 'index.html'\n    if '.' in path:\n        file = path\n\n    module_path = from_module_path(__file__, f'../front/dist/mlcomp/')\n    return send_from_directory(module_path, file)\n\n\ndef request_data():\n    return json.loads(request.data.decode('utf-8'))\n\n\ndef parse_int(args: dict, key: str):\n    return int(args[key]) if args.get(key) and args[key].isnumeric() else None\n\n\ndef construct_paginator_options(args: dict, default_sort_column: str):\n    return PaginatorOptions(\n        sort_column=args.get('sort_column') or default_sort_column,\n        sort_descending=args.get('sort_descending', 'true') == 'true',\n        page_number=parse_int(args, 'page_number'),\n        page_size=parse_int(args, 'page_size'),\n    )\n\n\ndef check_auth(token):\n    return str(token).strip() == TOKEN\n\n\ndef authenticate():\n    return Response(\n        'Could not verify your access level for that URL.\\n'\n        'You have to login with proper credentials', 401,\n        {'WWW-Authenticate': 'xBasic'}\n    )\n\n\ndef requires_auth(f):\n    @wraps(f)\n    def decorated(*args, **kwargs):\n        token = request.headers.get('Authorization')\n        if not token or not check_auth(token):\n            return authenticate()\n        return f(*args, **kwargs)\n\n    return decorated\n\n\ndef error_handler(f):\n    @wraps(f)\n    def decorated(*args, **kwargs):\n        global _read_session, _write_session, logger\n\n        success = True\n        status = 200\n        error = ''\n\n        try:\n            res = f(*args, **kwargs)\n        except Exception as e:\n            if Session.sqlalchemy_error(e):\n                Session.cleanup('server.read')\n                Session.cleanup('server.write')\n\n                _read_session = Session.create_session(key='server.read')\n                _write_session = Session.create_session(key='server.write')\n\n                logger = create_logger(_write_session, __name__)\n\n            logger.error(\n                f'Requested Url: {request.path}\\n\\n{traceback.format_exc()}',\n                ComponentType.API\n            )\n\n            error = traceback.format_exc()\n            success = False\n            status = 500\n            res = None\n\n        res = res or {}\n        if isinstance(res, Response):\n            return res\n\n        res['success'] = success\n        res['error'] = error\n\n        return Response(json.dumps(res, ignore_nan=True), status=status)\n\n    return decorated\n\n\n@app.route('/api/computers', methods=['POST'])\n@requires_auth\n@error_handler\ndef computers():\n    data = request_data()\n    options = PaginatorOptions(**data['paginator'])\n    options.sort_column = 'name'\n\n    provider = ComputerProvider(_read_session)\n    return provider.get(data, options)\n\n\n@app.route('/api/computer_sync_start', methods=['POST'])\n@requires_auth\n@error_handler\ndef computer_sync_start():\n    provider = ComputerProvider(_read_session)\n    return provider.sync_start()\n\n\n@app.route('/api/computer_sync_end', methods=['POST'])\n@requires_auth\n@error_handler\ndef computer_sync_end():\n    data = request_data()\n    provider = ComputerProvider(_write_session)\n    for computer in provider.all():\n        if data.get('computer') and data['computer'] != computer.name:\n            continue\n        meta = yaml_load(computer.meta)\n        meta['manual_sync'] = {\n            'project': data['id'],\n            'sync_folders': yaml_load(data['sync_folders']),\n            'ignore_folders': yaml_load(data['ignore_folders']),\n        }\n        computer.meta = yaml_dump(meta)\n    provider.update()\n\n\n@app.route('/api/projects', methods=['POST'])\n@requires_auth\n@error_handler\ndef projects():\n    data = request_data()\n    options = PaginatorOptions(**data['paginator'])\n\n    provider = ProjectProvider(_read_session)\n    res = provider.get(data, options)\n    return res\n\n\n@app.route('/api/project/add', methods=['POST'])\n@requires_auth\n@error_handler\ndef project_add():\n    data = request_data()\n\n    provider = ProjectProvider(_write_session)\n    provider.add_project(\n        data['name'],\n        yaml_load(data['class_names']),\n        yaml_load(data['sync_folders']),\n        yaml_load(data['ignore_folders']),\n    )\n\n\n@app.route('/api/project/stop_all_dags', methods=['POST'])\n@requires_auth\n@error_handler\ndef stop_all_dags():\n    data = request_data()\n    provider = TaskProvider(_write_session)\n    tasks = provider.by_status(TaskStatus.InProgress,\n                               TaskStatus.Queued,\n                               TaskStatus.NotRan,\n                               project=data['project']\n                               )\n\n    for t in tasks:\n        info = yaml_load(t.additional_info)\n        info['stopped'] = True\n        t.additional_info = yaml_dump(info)\n\n    provider.update()\n    supervisor.stop_tasks(tasks)\n\n\n@app.route('/api/project/remove_all_dags', methods=['POST'])\n@requires_auth\n@error_handler\ndef remove_all_dags():\n    data = request_data()\n    provider = DagProvider(_write_session)\n    dags = provider.by_project(data['project'])\n    provider.remove_all([d.id for d in dags])\n\n\n@app.route('/api/project/edit', methods=['POST'])\n@requires_auth\n@error_handler\ndef project_edit():\n    data = request_data()\n\n    provider = ProjectProvider(_write_session)\n    provider.edit_project(\n        data['name'],\n        yaml_load(data['class_names']),\n        yaml_load(data['sync_folders']),\n        yaml_load(data['ignore_folders']),\n    )\n\n\n@app.route('/api/report/add_start', methods=['POST'])\n@requires_auth\n@error_handler\ndef report_add_start():\n    return {\n        'projects': ProjectProvider(_read_session).get()['data'],\n        'layouts': ReportLayoutProvider(_read_session).get()['data']\n    }\n\n\n@app.route('/api/report/add_end', methods=['POST'])\n@requires_auth\n@error_handler\ndef report_add_end():\n    data = request_data()\n\n    provider = ReportProvider(_write_session)\n    layouts = ReportLayoutProvider(_write_session).all()\n    layout = layouts[data['layout']]\n    report = Report(\n        name=data['name'], project=data['project'], config=yaml_dump(layout)\n    )\n    provider.add(report)\n\n\n@app.route('/api/layouts', methods=['POST'])\n@requires_auth\n@error_handler\ndef report_layouts():\n    data = request_data()\n\n    provider = ReportLayoutProvider(_read_session)\n    options = PaginatorOptions(**data['paginator'])\n    res = provider.get(data, options)\n    return res\n\n\n@app.route('/api/layout/add', methods=['POST'])\n@requires_auth\n@error_handler\ndef report_layout_add():\n    data = request_data()\n\n    provider = ReportLayoutProvider(_write_session)\n    layout = ReportLayout(name=data['name'], content='', last_modified=now())\n    provider.add(layout)\n\n\n@app.route('/api/layout/edit', methods=['POST'])\n@requires_auth\n@error_handler\ndef report_layout_edit():\n    data = request_data()\n\n    provider = ReportLayoutProvider(_write_session)\n    layout = provider.by_name(data['name'])\n    layout.last_modified = now()\n    if 'content' in data and data['content'] is not None:\n        data_loaded = yaml_load(data['content'])\n        ReportLayoutInfo(data_loaded)\n        layout.content = data['content']\n    if 'new_name' in data and data['new_name'] is not None:\n        layout.name = data['new_name']\n\n    provider.commit()\n\n\n@app.route('/api/layout/remove', methods=['POST'])\n@requires_auth\n@error_handler\ndef report_layout_remove():\n    data = request_data()\n\n    provider = ReportLayoutProvider(_write_session)\n    provider.remove(data['name'], key_column='name')\n\n\n@app.route('/api/model/add', methods=['POST'])\n@requires_auth\n@error_handler\ndef model_add():\n    data = request_data()\n    dag_model_add(_write_session, data)\n\n\n@app.route('/api/model/remove', methods=['POST'])\n@requires_auth\n@error_handler\ndef model_remove():\n    data = request_data()\n    provider = ModelProvider(_write_session)\n    model = provider.by_id(data['id'], joined_load=[Model.project_rel])\n    celery_tasks.remove_model(\n        _write_session, model.project_rel.name, model.name\n    )\n    provider.remove(model.id)\n\n\n@app.route('/api/model/start_begin', methods=['POST'])\n@requires_auth\n@error_handler\ndef model_start_begin():\n    data = request_data()\n    return ModelProvider(_read_session).model_start_begin(data['model_id'])\n\n\n@app.route('/api/model/start_end', methods=['POST'])\n@requires_auth\n@error_handler\ndef model_start_end():\n    data = request_data()\n    dag_model_start(_write_session, data)\n\n\n@app.route('/api/img_classify', methods=['POST'])\n@requires_auth\n@error_handler\ndef img_classify():\n    data = request_data()\n    options = PaginatorOptions(**data['paginator'])\n    res = ReportImgProvider(_read_session).detail_img_classify(data, options)\n    return res\n\n\n@app.route('/api/img_segment', methods=['POST'])\n@requires_auth\n@error_handler\ndef img_segment():\n    data = request_data()\n    options = PaginatorOptions(**data['paginator'])\n    res = ReportImgProvider(_read_session).detail_img_segment(data, options)\n    return res\n\n\n@app.route('/api/config', methods=['POST'])\n@requires_auth\n@error_handler\ndef config():\n    id = request_data()\n    res = DagProvider(_read_session).config(id)\n    return {'data': res}\n\n\n@app.route('/api/graph', methods=['POST'])\n@requires_auth\n@error_handler\ndef graph():\n    id = request_data()\n    res = DagProvider(_read_session).graph(id)\n    return res\n\n\n@app.route('/api/dags', methods=['POST'])\n@requires_auth\n@error_handler\ndef dags():\n    data = request_data()\n    options = PaginatorOptions(**data['paginator'])\n    provider = DagProvider(_read_session)\n    res = provider.get(data, options)\n    return res\n\n\n@app.route('/api/space/tag_add', methods=['POST'])\n@requires_auth\n@error_handler\ndef space_tag_add():\n    data = request_data()\n    provider = SpaceProvider(_write_session)\n    tag = SpaceTag(space=data['space'], tag=data['tag'])\n    provider.add(tag)\n\n\n@app.route('/api/space/tag_remove', methods=['POST'])\n@requires_auth\n@error_handler\ndef space_tag_remove():\n    data = request_data()\n    provider = SpaceProvider(_write_session)\n    provider.remove_tag(space=data['space'], tag=data['tag'])\n\n\n@app.route('/api/dag/tag_add', methods=['POST'])\n@requires_auth\n@error_handler\ndef dag_tag_add():\n    data = request_data()\n    provider = DagProvider(_write_session)\n    tag = DagTag(dag=data['dag'], tag=data['tag'])\n    provider.add(tag)\n\n\n@app.route('/api/dag/tag_remove', methods=['POST'])\n@requires_auth\n@error_handler\ndef dag_tag_remove():\n    data = request_data()\n    provider = DagProvider(_write_session)\n    provider.remove_tag(dag=data['dag'], tag=data['tag'])\n\n\n@app.route('/api/dag/restart', methods=['POST'])\n@requires_auth\n@error_handler\ndef dag_restart():\n    data = request_data()\n    dag_copy(_write_session, data['dag'], data['file_changes'])\n\n\n@app.route('/api/spaces', methods=['POST'])\n@requires_auth\n@error_handler\ndef spaces():\n    data = request_data()\n    options = PaginatorOptions(**data['paginator'])\n    if options.sort_column == 'id':\n        options.sort_column = 'name'\n    provider = SpaceProvider(_read_session)\n    res = provider.get(data, options)\n    return res\n\n\ndef set_space_fields(space: Space, data: dict):\n    data['content'] = data.get('content', '')\n    yaml_load(data['content'])\n\n    space.name = data['name']\n    space.content = data['content']\n    if not space.created:\n        space.created = now()\n    space.changed = now()\n    return space\n\n\n@app.route('/api/space/relation_append', methods=['POST'])\n@requires_auth\n@error_handler\ndef space_relation_append():\n    data = request_data()\n    provider = SpaceProvider(_write_session)\n    provider.add_relation(data['parent'], data['child'])\n\n\n@app.route('/api/space/relation_remove', methods=['POST'])\n@requires_auth\n@error_handler\ndef space_relation_remove():\n    data = request_data()\n    provider = SpaceProvider(_write_session)\n    provider.remove_relation(data['parent'], data['child'])\n\n\n@app.route('/api/space/run', methods=['POST'])\n@requires_auth\n@error_handler\ndef space_run():\n    data = request_data()\n    provider = SpaceProvider(_write_session)\n    file_changes = data.get('file_changes', '\\n')\n    file_changes = yaml_load(file_changes)\n\n    def merge(d: dict, d2: dict):\n        res = {}\n        for k in set(d) | set(d2):\n            if k in d and k in d2:\n                v = d[k]\n                v2 = d2[k]\n                if isinstance(v, list) and isinstance(v2, list):\n                    res[k] = v[:]\n                    res[k].extend(v2)\n                elif isinstance(v, dict) and isinstance(v2, dict):\n                    res[k] = v.copy()\n                    res[k].update(v2)\n                else:\n                    raise Exception(f'Types are different: {type(v)}, {type(v2)}')\n            elif k in d:\n                res[k] = d[k]\n            elif k in d2:\n                res[k] = d2[k]\n        return res\n\n    suffix = []\n    for space in data['spaces']:\n        if space['logic'] == 'and':\n            space = provider.by_id(space['value'], key_column='name')\n            if space.content:\n                d = yaml_load(space.content)\n                file_changes = merge(file_changes, d)\n                suffix.append(space.name)\n\n    for space in data['spaces']:\n        if space['logic'] != 'or':\n            continue\n        space = provider.by_id(space['value'], key_column='name')\n        space_related = provider.related(space.name)\n        if space.content:\n            space_related += [space]\n\n        for rel in space_related:\n            content = rel.content\n            d = yaml_load(content)\n            d = merge(file_changes, d)\n\n            dag_suffix = ' '.join(suffix+rel.name)\n            dag_copy(_write_session, data['dag'], file_changes=yaml_dump(d),\n                     dag_suffix=dag_suffix)\n\n    if not any(s['logic'] == 'or' for s in data['spaces']):\n        dag_copy(_write_session, data['dag'], file_changes=yaml_dump(file_changes),\n                 dag_suffix=' '.join(suffix))\n\n@app.route('/api/space/add', methods=['POST'])\n@requires_auth\n@error_handler\ndef space_add():\n    data = request_data()\n    provider = SpaceProvider(_write_session)\n    space = Space()\n    set_space_fields(space, data)\n    provider.add(space)\n\n\n@app.route('/api/space/copy', methods=['POST'])\n@requires_auth\n@error_handler\ndef space_copy():\n    data = request_data()\n    provider = SpaceProvider(_write_session)\n    space = Space()\n    set_space_fields(space, data['space'])\n    provider.add(space)\n\n    old_children = provider.related(data['old_space'])\n    for c in old_children:\n        provider.add_relation(space.name, c.name)\n\n\n@app.route('/api/space/edit', methods=['POST'])\n@requires_auth\n@error_handler\ndef space_edit():\n    data = request_data()\n    provider = SpaceProvider(_write_session)\n    space = provider.by_id(data['name'], key_column='name')\n    set_space_fields(space, data)\n    provider.update()\n\n\n@app.route('/api/space/remove', methods=['POST'])\n@requires_auth\n@error_handler\ndef space_remove():\n    data = request_data()\n    provider = SpaceProvider(_write_session)\n    provider.remove(data['name'], key_column='name')\n\n\n@app.route('/api/space/tags', methods=['POST'])\n@requires_auth\n@error_handler\ndef space_tags():\n    data = request_data()\n    provider = SpaceProvider(_write_session)\n    return provider.tags(data['name'])\n\n\n@app.route('/api/space/names', methods=['POST'])\n@requires_auth\n@error_handler\ndef space_names():\n    data = request_data()\n    provider = SpaceProvider(_write_session)\n    return provider.names(data['name'])\n\n\n@app.route('/api/memories', methods=['POST'])\n@requires_auth\n@error_handler\ndef memories():\n    data = request_data()\n    options = PaginatorOptions(**data['paginator'])\n    provider = MemoryProvider(_read_session)\n    res = provider.get(data, options)\n    return res\n\n\ndef set_memory_fields(memory: Memory, data: dict):\n    memory.model = data['model']\n    memory.memory = float(data['memory'])\n    memory.batch_size = int(data['batch_size'])\n    memory.variant = data.get('variant')\n    if data.get('num_classes'):\n        memory.num_classes = int(data['num_classes'])\n\n\n@app.route('/api/memory/add', methods=['POST'])\n@requires_auth\n@error_handler\ndef memory_add():\n    data = request_data()\n    provider = MemoryProvider(_write_session)\n    memory = Memory()\n    set_memory_fields(memory, data)\n    provider.add(memory)\n\n\n@app.route('/api/memory/edit', methods=['POST'])\n@requires_auth\n@error_handler\ndef memory_edit():\n    data = request_data()\n    provider = MemoryProvider(_write_session)\n    memory = provider.by_id(data['id'])\n    set_memory_fields(memory, data)\n    provider.update()\n\n\n@app.route('/api/memory/remove', methods=['POST'])\n@requires_auth\n@error_handler\ndef memory_remove():\n    data = request_data()\n    provider = MemoryProvider(_write_session)\n    provider.remove(data['id'])\n\n\n@app.route('/api/code', methods=['POST'])\n@requires_auth\n@error_handler\ndef code():\n    id = request_data()\n    res = OrderedDict()\n    parents = dict()\n\n    for s, f in DagStorageProvider(_read_session).by_dag(id):\n        s.path = s.path.strip()\n        parent = os.path.dirname(s.path)\n        name = os.path.basename(s.path)\n        if name == '':\n            continue\n\n        if s.is_dir:\n            node = {'name': name, 'children': [], 'id': s.id, 'dag': id,\n                    'storage': s.id}\n            if not parent:\n                res[name] = node\n                parents[s.path] = node\n            else:\n                # noinspection PyUnresolvedReferences\n                parents[parent]['children'].append(node)\n                parents[os.path.join(parent, name)] = node\n        else:\n            node = {'name': name, 'id': f.id, 'dag': id, 'storage': s.id}\n\n            try:\n                node['content'] = f.content.decode('utf-8')\n            except UnicodeDecodeError:\n                node['content'] = ''\n\n            if not parent:\n                res[name] = node\n            else:\n                # noinspection PyUnresolvedReferences\n                parents[parent]['children'].append(node)\n\n    def sort_key(x):\n        if 'children' in x and len(x['children']) > 0:\n            return '_' * 5 + x['name']\n        return x['name']\n\n    def sort(node: dict):\n        if 'children' in node and len(node['children']) > 0:\n            node['children'] = sorted(node['children'], key=sort_key)\n\n            for c in node['children']:\n                sort(c)\n\n    res = sorted(list(res.values()), key=sort_key)\n    for r in res:\n        sort(r)\n\n    return {'items': res}\n\n\n@app.route('/api/update_code', methods=['POST'])\n@requires_auth\n@error_handler\ndef update_code():\n    data = request_data()\n    provider = FileProvider(_write_session)\n    file = provider.by_id(data['file_id'])\n    content = data['content'].encode('utf-8')\n    md5 = hashlib.md5(content).hexdigest()\n\n    if md5 == file.md5:\n        return\n\n    if file.dag != data['dag']:\n        new_file = File(md5=md5, content=content, project=file.project,\n                        dag=data['dag'])\n        provider.add(new_file)\n\n        storage = DagStorageProvider(_write_session).by_id(data['storage'])\n        storage.file = new_file.id\n        provider.commit()\n        return {'file': new_file.id}\n    else:\n        file.content = content\n        file.md5 = md5\n        provider.commit()\n        return {'file': file.id}\n\n\n@app.route('/api/code_download', methods=['GET'])\n@requires_auth\n@error_handler\ndef code_download():\n    id = int(request.args['id'])\n    storage = Storage(_read_session)\n    dag = DagProvider().by_id(id)\n    folder = os.path.join(TMP_FOLDER, f'{dag.id}({dag.name})')\n\n    try:\n        storage.download_dag(id, folder)\n\n        file_name = f'{dag.id}({dag.name}).zip'\n        dst = os.path.join(TMP_FOLDER, file_name)\n        zip_folder(folder, dst)\n        res = send_from_directory(TMP_FOLDER, file_name)\n        os.remove(dst)\n        return res\n    finally:\n        shutil.rmtree(folder, ignore_errors=True)\n\n\n@app.route('/api/tasks', methods=['POST'])\n@requires_auth\n@error_handler\ndef tasks():\n    data = request_data()\n    options = PaginatorOptions(**data['paginator'])\n    provider = TaskProvider(_read_session)\n    res = provider.get(data, options)\n    return res\n\n\n@app.route('/api/task/stop', methods=['POST'])\n@requires_auth\n@error_handler\ndef task_stop():\n    data = request_data()\n    provider = TaskProvider(_write_session)\n    task = provider.by_id(data['id'], joinedload(Task.dag_rel, innerjoin=True))\n\n    tasks = [task] + provider.children(task.id)\n    supervisor.stop_tasks(tasks)\n\n\n@app.route('/api/task/info', methods=['POST'])\n@requires_auth\n@error_handler\ndef task_info():\n    data = request_data()\n    task = TaskProvider(_read_session).by_id(\n        data['id'], joinedload(Task.dag_rel, innerjoin=True)\n    )\n    return {\n        'pid': task.pid,\n        'worker_index': task.worker_index,\n        'gpu_assigned': task.gpu_assigned,\n        'celery_id': task.celery_id,\n        'additional_info': task.additional_info or '',\n        'result': task.result or '',\n        'id': task.id\n    }\n\n\n@app.route('/api/dag/tags', methods=['POST'])\n@requires_auth\n@error_handler\ndef dag_tags():\n    data = request_data()\n    provider = DagProvider(_write_session)\n    return provider.tags(data['name'])\n\n\n@app.route('/api/dag/stop', methods=['POST'])\n@requires_auth\n@error_handler\ndef dag_stop():\n    data = request_data()\n    provider = TaskProvider(_write_session)\n    id = int(data['id'])\n    tasks = provider.by_dag(id)\n\n    supervisor.stop_tasks(tasks)\n\n    dag_provider = DagProvider(_write_session)\n    return {'dag': dag_provider.get({'id': id})['data'][0]}\n\n\n@app.route('/api/dag/start', methods=['POST'])\n@requires_auth\n@error_handler\ndef dag_start():\n    data = request_data()\n    id = int(data['id'])\n    supervisor.start_dag(id)\n\n\n@app.route('/api/auxiliary', methods=['POST'])\n@requires_auth\n@error_handler\ndef auxiliary():\n    provider = AuxiliaryProvider(_read_session)\n    return provider.get()\n\n\n@app.route('/api/dag/toogle_report', methods=['POST'])\n@requires_auth\n@error_handler\ndef dag_toogle_report():\n    data = request_data()\n    provider = ReportProvider(_write_session)\n    if data.get('remove'):\n        provider.remove_dag(int(data['id']), int(data['report']))\n    else:\n        provider.add_dag(int(data['id']), int(data['report']))\n    return {'report_full': not data.get('remove')}\n\n\n@app.route('/api/task/toogle_report', methods=['POST'])\n@requires_auth\n@error_handler\ndef task_toogle_report():\n    data = request_data()\n    provider = ReportProvider(_write_session)\n    if data.get('remove'):\n        provider.remove_task(int(data['id']), int(data['report']))\n    else:\n        provider.add_task(int(data['id']), int(data['report']))\n    return {'report_full': not data.get('remove')}\n\n\n@app.route('/api/logs', methods=['POST'])\n@requires_auth\n@error_handler\ndef logs():\n    provider = LogProvider(_read_session)\n    data = request_data()\n    options = PaginatorOptions(**data['paginator'])\n    res = provider.get(data, options)\n    return res\n\n\n@app.route('/api/reports', methods=['POST'])\n@requires_auth\n@error_handler\ndef reports():\n    data = request_data()\n    options = PaginatorOptions(**data['paginator'])\n    provider = ReportProvider(_read_session)\n    res = provider.get(data, options)\n    return res\n\n\n@app.route('/api/report', methods=['POST'])\n@requires_auth\n@error_handler\ndef report():\n    id = request_data()\n    provider = ReportProvider(_read_session)\n    res = provider.detail(id)\n    return res\n\n\n@app.route('/api/report/update_layout_start', methods=['POST'])\n@requires_auth\n@error_handler\ndef report_update_layout_start():\n    id = request_data()['id']\n    provider = ReportProvider(_write_session)\n    res = provider.update_layout_start(id)\n    return res\n\n\n@app.route('/api/report/update_layout_end', methods=['POST'])\n@requires_auth\n@error_handler\ndef report_update_layout_end():\n    data = request_data()\n    provider = ReportProvider(_write_session)\n    layout_provider = ReportLayoutProvider(_write_session)\n    provider.update_layout_end(\n        data['id'], data['layout'], layout_provider.all()\n    )\n    return provider.detail(data['id'])\n\n\n@app.route('/api/task/steps', methods=['POST'])\n@requires_auth\n@error_handler\ndef steps():\n    id = request_data()\n    provider = StepProvider(_read_session)\n    res = provider.get(id)\n    return res\n\n\n@app.route('/api/token', methods=['POST'])\ndef token():\n    data = request_data()\n    if str(data['token']).strip() != TOKEN:\n        return Response(\n            json.dumps({\n                'success': False,\n                'reason': 'invalid token'\n            }),\n            status=401\n        )\n    return json.dumps({'success': True})\n\n\n@app.route('/api/project/remove', methods=['POST'])\n@requires_auth\n@error_handler\ndef project_remove():\n    id = request_data()['id']\n    ProjectProvider(_write_session).remove(id)\n\n\n@app.route('/api/remove_imgs', methods=['POST'])\n@requires_auth\n@error_handler\ndef remove_imgs():\n    data = request_data()\n    provider = ReportImgProvider(_write_session)\n    res = provider.remove(data)\n    return res\n\n\n@app.route('/api/remove_files', methods=['POST'])\n@requires_auth\n@error_handler\ndef remove_files():\n    data = request_data()\n    provider = FileProvider(_write_session)\n    res = provider.remove(data)\n    return res\n\n\n@app.route('/api/dag/remove', methods=['POST'])\n@requires_auth\n@error_handler\ndef dag_remove():\n    id = request_data()['id']\n    celery_tasks.remove_dag(_write_session, id)\n\n    dag_provider = DagProvider(_write_session)\n    dag_provider.remove(id)\n\n\n@app.route('/api/models', methods=['POST'])\n@requires_auth\n@error_handler\ndef models():\n    data = request_data()\n    options = PaginatorOptions(**data['paginator'])\n    provider = ModelProvider(_read_session)\n    res = provider.get(data, options)\n    return res\n\n\n@app.route('/api/stop')\n@requires_auth\n@error_handler\ndef stop():\n    pass\n\n\ndef shutdown_server():\n    func = request.environ.get('werkzeug.server.shutdown')\n    if func is None:\n        raise RuntimeError('Not running with the Werkzeug Server')\n    func()\n\n\n@app.route('/api/shutdown', methods=['POST'])\n@requires_auth\n@error_handler\ndef shutdown():\n    shutdown_server()\n    return 'Server shutting down...'\n\n\ndef start_server():\n    global supervisor\n    if os.environ.get('WERKZEUG_RUN_MAIN') != 'true':\n        logger.info(f'Server TOKEN = {TOKEN}', ComponentType.API)\n        supervisor = register_supervisor()\n\n    app.run(debug=FLASK_ENV == 'development', port=WEB_PORT, host=WEB_HOST)\n\n\ndef stop_server():\n    requests.post(\n        f'http://localhost:{WEB_PORT}/api/shutdown',\n        headers={'Authorization': TOKEN}\n    )\n"""
mlcomp/server/back/supervisor.py,0,"b'import datetime\nimport time\nimport traceback\nfrom typing import List\n\nfrom sqlalchemy.orm.exc import ObjectDeletedError\n\nfrom mlcomp.db.core import Session\nfrom mlcomp.db.enums import ComponentType, TaskStatus, TaskType\nfrom mlcomp.db.models import Task, Auxiliary\nfrom mlcomp.db.providers import \\\n    ComputerProvider, \\\n    TaskProvider, \\\n    DockerProvider, \\\n    AuxiliaryProvider, DagProvider, LogProvider\nfrom mlcomp.utils.io import yaml_dump, yaml_load\nfrom mlcomp.utils.logging import create_logger\nfrom mlcomp.utils.misc import now\nfrom mlcomp.worker.tasks import execute\nfrom mlcomp.utils.schedule import start_schedule\nimport mlcomp.worker.tasks as celery_tasks\n\n\nclass SupervisorBuilder:\n    def __init__(self):\n        self.session = Session.create_session(key=\'SupervisorBuilder\')\n        self.logger = create_logger(self.session, \'SupervisorBuilder\')\n        self.provider = None\n        self.computer_provider = None\n        self.docker_provider = None\n        self.auxiliary_provider = None\n        self.dag_provider = None\n        self.log_provider = None\n        self.queues = None\n        self.not_ran_tasks = None\n        self.dep_status = None\n        self.computers = None\n        self.auxiliary = {}\n\n        self.tasks = []\n        self.tasks_stop = []\n        self.dags_start = []\n        self.sent_tasks = 0\n\n    def create_base(self):\n        self.session.commit()\n\n        self.provider = TaskProvider(self.session)\n        self.computer_provider = ComputerProvider(self.session)\n        self.docker_provider = DockerProvider(self.session)\n        self.auxiliary_provider = AuxiliaryProvider(self.session)\n        self.dag_provider = DagProvider(self.session)\n        self.log_provider = LogProvider(self.session)\n\n        self.queues = [\n            f\'{d.computer}_{d.name}\' for d in self.docker_provider.all()\n            if d.last_activity >= now() - datetime.timedelta(seconds=15)\n        ]\n\n        self.auxiliary[\'queues\'] = self.queues\n\n    def load_tasks(self):\n        self.tasks = self.provider.by_status(TaskStatus.NotRan,\n                                             TaskStatus.InProgress,\n                                             TaskStatus.Queued)\n\n        not_ran_tasks = [t for t in self.tasks if\n                         t.status == TaskStatus.NotRan.value]\n\n        self.not_ran_tasks = [task for task in not_ran_tasks if not task.debug]\n        self.not_ran_tasks = sorted(\n            self.not_ran_tasks, key=lambda x: x.gpu or 0,\n            reverse=True)\n\n        self.logger.debug(\n            f\'Found {len(not_ran_tasks)} not ran tasks\',\n            ComponentType.Supervisor\n        )\n\n        self.dep_status = self.provider.dependency_status(self.not_ran_tasks)\n\n        self.auxiliary[\'not_ran_tasks\'] = [\n            {\n                \'id\': t.id,\n                \'name\': t.name,\n                \'dep_status\': [\n                    TaskStatus(s).name\n                    for s in self.dep_status.get(t.id, set())\n                ]\n            } for t in not_ran_tasks[:5]\n        ]\n\n    def load_computers(self):\n        computers = self.computer_provider.computers()\n        for computer in computers.values():\n            computer[\'gpu\'] = [0] * computer[\'gpu\']\n            computer[\'ports\'] = set()\n            computer[\'cpu_total\'] = computer[\'cpu\']\n            computer[\'memory_total\'] = computer[\'memory\']\n            computer[\'gpu_total\'] = len(computer[\'gpu\'])\n            computer[\'can_process_tasks\'] = computer[\'can_process_tasks\']\n\n        tasks = [\n            t for t in self.tasks if\n            t.status in [TaskStatus.InProgress.value,\n                         TaskStatus.Queued.value]\n        ]\n\n        for task in tasks:\n            if task.computer_assigned is None:\n                continue\n            assigned = task.computer_assigned\n            comp_assigned = computers[assigned]\n            comp_assigned[\'cpu\'] -= task.cpu\n\n            if task.gpu_assigned is not None:\n                for g in task.gpu_assigned.split(\',\'):\n                    comp_assigned[\'gpu\'][int(g)] = task.id\n            comp_assigned[\'memory\'] -= task.memory * 1024\n\n            info = yaml_load(task.additional_info)\n            if \'distr_info\' in info:\n                dist_info = info[\'distr_info\']\n                if dist_info[\'rank\'] == 0:\n                    comp_assigned[\'ports\'].add(dist_info[\'master_port\'])\n\n        self.computers = [\n            {\n                **value, \'name\': name\n            } for name, value in computers.items()\n        ]\n\n        self.auxiliary[\'computers\'] = self.computers\n\n    def process_to_celery(self, task: Task, queue: str, computer: dict):\n        r = execute.apply_async((task.id,), queue=queue, retry=False)\n        task.status = TaskStatus.Queued.value\n        task.computer_assigned = computer[\'name\']\n        task.docker_assigned = queue.split(\'_\')[1]\n        task.celery_id = r.id\n\n        if task.computer_assigned is not None:\n            if task.gpu_assigned:\n                for g in map(int, task.gpu_assigned.split(\',\')):\n                    computer[\'gpu\'][g] = task.id\n            computer[\'cpu\'] -= task.cpu\n            computer[\'memory\'] -= task.memory * 1024\n\n        self.logger.info(\n            f\'Sent task={task.id} to celery. Queue = {queue} \'\n            f\'Task status = {task.status} Celery_id = {r.id}\',\n            ComponentType.Supervisor)\n        self.provider.update()\n\n    def create_service_task(\n            self,\n            task: Task,\n            gpu_assigned=None,\n            distr_info: dict = None,\n            resume: dict = None\n    ):\n        new_task = Task(\n            name=task.name,\n            computer=task.computer,\n            executor=task.executor,\n            status=TaskStatus.NotRan.value,\n            type=TaskType.Service.value,\n            gpu_assigned=gpu_assigned,\n            parent=task.id,\n            report=task.report,\n            dag=task.dag\n        )\n        new_task.additional_info = task.additional_info\n\n        if distr_info:\n            additional_info = yaml_load(new_task.additional_info)\n            additional_info[\'distr_info\'] = distr_info\n            new_task.additional_info = yaml_dump(additional_info)\n\n        if resume:\n            additional_info = yaml_load(new_task.additional_info)\n            additional_info[\'resume\'] = resume\n            new_task.additional_info = yaml_dump(additional_info)\n\n        return self.provider.add(new_task)\n\n    def find_port(self, c: dict, docker_name: str):\n        docker = self.docker_provider.get(c[\'name\'], docker_name)\n        ports = list(map(int, docker.ports.split(\'-\')))\n        for p in range(ports[0], ports[1] + 1):\n            if p not in c[\'ports\']:\n                return p\n        raise Exception(f\'All ports in {c[""name""]} are taken\')\n\n    def _process_task_valid_computer(self, task: Task, c: dict,\n                                     single_node: bool):\n        if not c[\'can_process_tasks\']:\n            return \'this computer can not process tasks\'\n\n        if task.computer is not None and task.computer != c[\'name\']:\n            return \'name set in the config!= name of this computer\'\n\n        if task.cpu > c[\'cpu\']:\n            return f\'task cpu = {task.cpu} > computer\' \\\n                   f\' free cpu = {c[""cpu""]}\'\n\n        if task.memory > c[\'memory\']:\n            return f\'task cpu = {task.cpu} > computer \' \\\n                   f\'free memory = {c[""memory""]}\'\n\n        queue = f\'{c[""name""]}_\' \\\n                f\'{task.dag_rel.docker_img or ""default""}\'\n        if queue not in self.queues:\n            return f\'required queue = {queue} not in queues\'\n\n        if task.gpu > 0 and not any(g == 0 for g in c[\'gpu\']):\n            return f\'task requires gpu, but there is not any free\'\n\n        free_gpu = sum(g == 0 for g in c[\'gpu\'])\n        if single_node and task.gpu > free_gpu:\n            return f\'task requires {task.gpu} \' \\\n                   f\'but there are only {free_gpu} free\'\n\n    def _process_task_get_computers(\n            self, executor: dict, task: Task, auxiliary: dict\n    ):\n        single_node = executor.get(\'single_node\', True)\n\n        computers = []\n        for c in self.computers:\n            error = self._process_task_valid_computer(task, c, single_node)\n            auxiliary[\'computers\'].append({\'name\': c[\'name\'], \'error\': error})\n            if not error:\n                computers.append(c)\n\n        if task.gpu > 0 and single_node and len(computers) > 0:\n            computers = sorted(\n                computers,\n                key=lambda x: sum(g == 0 for g in c[\'gpu\']),\n                reverse=True\n            )[:1]\n\n        free_gpu = sum(sum(g == 0 for g in c[\'gpu\']) for c in computers)\n        if task.gpu > free_gpu:\n            auxiliary[\'not_valid\'] = f\'gpu required by the \' \\\n                                     f\'task = {task.gpu},\' \\\n                                     f\' but there are only {free_gpu} \' \\\n                                     f\'free gpus\'\n            return []\n        return computers\n\n    def _process_task_to_send(\n            self, executor: dict, task: Task, computers: List[dict]\n    ):\n        distr = executor.get(\'distr\', True)\n        to_send = []\n        for computer in computers:\n            queue = f\'{computer[""name""]}_\' \\\n                    f\'{task.dag_rel.docker_img or ""default""}\'\n\n            if task.gpu_max > 1 and distr:\n                for index, task_taken_gpu in enumerate(computer[\'gpu\']):\n                    if task_taken_gpu:\n                        continue\n                    to_send.append([computer, queue, index])\n\n                    if len(to_send) >= task.gpu_max:\n                        break\n\n                if len(to_send) >= task.gpu_max:\n                    break\n            elif task.gpu_max > 0:\n                cuda_devices = []\n                for index, task_taken_gpu in enumerate(computer[\'gpu\']):\n                    if task_taken_gpu:\n                        continue\n\n                    cuda_devices.append(index)\n                    if len(cuda_devices) >= task.gpu_max:\n                        break\n\n                task.gpu_assigned = \',\'.join(map(str, cuda_devices))\n                self.provider.commit()\n\n                self.process_to_celery(task, queue, computer)\n            else:\n                self.process_to_celery(task, queue, computer)\n                break\n        return to_send\n\n    def process_task(self, task: Task):\n        auxiliary = self.auxiliary[\'process_tasks\'][-1]\n        auxiliary[\'computers\'] = []\n\n        config = yaml_load(task.dag_rel.config)\n        executor = config[\'executors\'][task.executor]\n\n        computers = self._process_task_get_computers(executor, task, auxiliary)\n        if len(computers) == 0:\n            return\n\n        to_send = self._process_task_to_send(executor, task, computers)\n        auxiliary[\'to_send\'] = to_send[:5]\n        additional_info = yaml_load(task.additional_info)\n\n        rank = 0\n        master_port = None\n        if len(to_send) > 0:\n\n            master_port = self.find_port(\n                to_send[0][0], to_send[0][1].split(\'_\')[1]\n            )\n            computer_names = {c[\'name\'] for c, _, __ in to_send}\n            if len(computer_names) == 1:\n                task.computer_assigned = list(computer_names)[0]\n\n        for computer, queue, gpu_assigned in to_send:\n            main_cmp = to_send[0][0]\n            # noinspection PyTypeChecker\n            ip = \'localhost\' if computer[\'name\'] == main_cmp[\'name\'] \\\n                else main_cmp[\'ip\']\n\n            distr_info = {\n                \'master_addr\': ip,\n                \'rank\': rank,\n                \'local_rank\': gpu_assigned,\n                \'master_port\': master_port,\n                \'world_size\': len(to_send),\n                \'master_computer\': main_cmp[\'name\']\n            }\n            service_task = self.create_service_task(\n                task,\n                distr_info=distr_info,\n                gpu_assigned=gpu_assigned,\n                resume=additional_info.get(\'resume\')\n            )\n            self.process_to_celery(service_task, queue, computer)\n            rank += 1\n            main_cmp[\'ports\'].add(master_port)\n\n        if len(to_send) > 0:\n            task.status = TaskStatus.Queued.value\n            self.sent_tasks += len(to_send)\n\n    def process_tasks(self):\n        self.auxiliary[\'process_tasks\'] = []\n\n        for task in self.not_ran_tasks:\n            auxiliary = {\'id\': task.id, \'name\': task.name}\n            self.auxiliary[\'process_tasks\'].append(auxiliary)\n\n            if task.dag_rel is None:\n                task.dag_rel = self.dag_provider.by_id(task.dag)\n\n            if TaskStatus.Stopped.value in self.dep_status[task.id] \\\n                    or TaskStatus.Failed.value in self.dep_status[task.id] or \\\n                    TaskStatus.Skipped.value in self.dep_status[task.id]:\n                auxiliary[\'not_valid\'] = \'stopped or failed in dep_status\'\n                self.provider.change_status(task, TaskStatus.Skipped)\n                continue\n\n            if len(self.dep_status[task.id]) != 0 \\\n                    and self.dep_status[task.id] != {TaskStatus.Success.value}:\n                auxiliary[\'not_valid\'] = \'not all dep tasks are finished\'\n                continue\n            self.process_task(task)\n\n        self.auxiliary[\'process_tasks\'] = self.auxiliary[\'process_tasks\'][:5]\n\n    def _stop_child_tasks(self, task: Task):\n        self.provider.commit()\n\n        children = self.provider.children(task.id, [Task.dag_rel])\n        dags = [c.dag_rel for c in children]\n        for c, d in zip(children, dags):\n            celery_tasks.stop(self.logger, self.session, c, d)\n\n    def _correct_catalyst_hangs(self, task, statuses):\n        if task.type != TaskType.Train.value:\n            return\n        success = statuses[TaskStatus.Success]\n        in_progress = statuses[TaskStatus.InProgress]\n\n        sum_statuses = sum(statuses.values())\n        if (success + in_progress == sum_statuses) \\\n                and in_progress > 0 and success > 0:\n            child_tasks = self.provider.children(task.id)\n            for t in child_tasks:\n                if t.status == TaskStatus.InProgress.value:\n                    response = celery_tasks.kill.apply_async(\n                        (t.pid,),\n                        queue=f\'{t.computer_assigned}_{t.docker_assigned}\',\n                        retry=False)\n\n                    result = response.get()\n                    if result:\n                        t.status = TaskStatus.Success.value\n            self.provider.commit()\n\n    def _correct_catalyst_fails(self, task):\n        if task.type != TaskType.Train.value:\n            return\n\n        messages = [\n            \'CUDA error: an illegal memory access was encountered\',\n            \'CUDA error: device-side assert triggered\',\n            \'CUDNN_STATUS_INTERNAL_ERROR\'\n        ]\n        children = self.provider.children(task.id, [Task.dag_rel])\n        children = sorted(children, key=lambda x: x.id, reverse=True)\n        for c in children:\n            if c.status == TaskStatus.Failed.value:\n                logs = self.log_provider.last(task=c.id, count=1)\n                if len(logs) > 0:\n                    message = logs[0][0].message\n                    for m in messages:\n                        if m in message:\n                            self.logger.info(\n                                \'Restart DAG: correct_catalyst_fails\',\n                                ComponentType.Supervisor,\n                                None,\n                                task.id\n                            )\n                            self.start_dag(task.dag)\n                            return\n\n    def process_parent_tasks(self):\n        tasks = self.provider.parent_tasks_stats()\n\n        was_change = False\n        for task, started, finished, statuses in tasks:\n            self._correct_catalyst_hangs(task, statuses)\n\n            status = task.status\n            if statuses[TaskStatus.Failed] > 0:\n                status = TaskStatus.Failed.value\n            elif statuses[TaskStatus.Skipped] > 0:\n                status = TaskStatus.Skipped.value\n            elif statuses[TaskStatus.Queued] > 0:\n                status = TaskStatus.Queued.value\n            elif statuses[TaskStatus.InProgress] > 0:\n                status = TaskStatus.InProgress.value\n            elif statuses[TaskStatus.Success] > 0:\n                status = TaskStatus.Success.value\n\n            if status != task.status:\n                if status == TaskStatus.InProgress.value:\n                    task.started = started\n                elif status >= TaskStatus.Failed.value:\n                    task.started = started\n                    task.finished = finished\n\n                    self._stop_child_tasks(task)\n                    self._correct_catalyst_fails(task)\n\n                was_change = True\n                task.status = status\n\n        if was_change:\n            self.provider.commit()\n\n        self.auxiliary[\'parent_tasks_stats\'] = [\n            {\n                \'name\': task.name,\n                \'id\': task.id,\n                \'started\': task.started,\n                \'finished\': finished,\n                \'statuses\': [\n                    {\n                        \'name\': k.name,\n                        \'count\': v\n                    } for k, v in statuses.items()\n                ],\n            } for task, started, finished, statuses in tasks[:5]\n        ]\n\n    def write_auxiliary(self):\n        self.auxiliary[\'duration\'] = (now() - self.auxiliary[\'time\']). \\\n            total_seconds()\n\n        auxiliary = Auxiliary(\n            name=\'supervisor\', data=yaml_dump(self.auxiliary)\n        )\n        if len(auxiliary.data) > 16000:\n            return\n\n        self.auxiliary_provider.create_or_update(auxiliary, \'name\')\n\n    def stop_tasks(self, tasks: List[Task]):\n        self.tasks_stop.extend([t.id for t in tasks])\n\n    def process_stop_tasks(self):\n        # Stop not running tasks\n        if len(self.tasks_stop) == 0:\n            return\n\n        tasks = self.provider.by_ids(self.tasks_stop)\n        tasks_not_ran = [t.id for t in tasks if\n                         t.status in [TaskStatus.NotRan.value,\n                                      TaskStatus.Queued.value]]\n        tasks_started = [t for t in tasks if\n                         t.status in [TaskStatus.InProgress.value]]\n        tasks_started_ids = [t.id for t in tasks_started]\n\n        self.provider.change_status_all(tasks=tasks_not_ran,\n                                        status=TaskStatus.Skipped)\n\n        pids = []\n        for task in tasks_started:\n            if task.pid:\n                pids.append((task.computer_assigned, task.pid))\n\n            additional_info = yaml_load(task.additional_info)\n            for p in additional_info.get(\'child_processes\', []):\n                pids.append((task.computer_assigned, p))\n\n        for computer, queue in self.docker_provider.queues_online():\n            pids_computer = [p for c, p in pids if c == computer]\n            if len(pids_computer) > 0:\n                celery_tasks.kill_all.apply_async((pids_computer,),\n                                                  queue=queue,\n                                                  retry=False)\n\n        self.provider.change_status_all(tasks=tasks_started_ids,\n                                        status=TaskStatus.Stopped)\n\n        self.tasks_stop = []\n\n    def fast_check(self):\n        if self.provider is None or self.computer_provider is None:\n            return False\n\n        if self.not_ran_tasks is None or self.queues is None:\n            return False\n\n        if len(self.tasks_stop) > 0:\n            return False\n\n        if len(self.dags_start) > 0:\n            return False\n\n        if len(self.auxiliary.get(\'to_send\', [])) > 0:\n            return False\n\n        queues = set([\n            f\'{d.computer}_{d.name}\' for d in self.docker_provider.all()\n            if d.last_activity >= now() - datetime.timedelta(seconds=15)\n        ])\n\n        queues_set = set(queues)\n        queues_set2 = set(self.queues)\n\n        if queues_set != queues_set2:\n            return False\n\n        tasks = self.provider.by_status(TaskStatus.NotRan,\n                                        TaskStatus.Queued,\n                                        TaskStatus.InProgress)\n        tasks_set = {t.id for t in tasks if\n                     t.status == TaskStatus.NotRan.value and not t.debug}\n        tasks_set2 = {t.id for t in self.tasks if\n                      t.status == TaskStatus.NotRan.value}\n\n        if tasks_set != tasks_set2:\n            return False\n\n        tasks_set = {t.id for t in tasks if\n                     t.status == TaskStatus.InProgress.value}\n        tasks_set2 = {t.id for t in self.tasks if\n                      t.status == TaskStatus.InProgress.value}\n\n        if tasks_set != tasks_set2:\n            return False\n\n        tasks_set = {t.id for t in tasks if\n                     t.status == TaskStatus.Queued.value}\n        tasks_set2 = {t.id for t in self.tasks if\n                      t.status == TaskStatus.Queued.value}\n\n        if tasks_set != tasks_set2:\n            return False\n\n        return True\n\n    def start_dag(self, id: int):\n        self.dags_start.append(id)\n\n    def process_start_dags(self):\n        if len(self.dags_start) == 0:\n            return\n\n        for id in self.dags_start:\n            can_start_statuses = [\n                TaskStatus.Failed.value, TaskStatus.Skipped.value,\n                TaskStatus.Stopped.value\n            ]\n\n            tasks = self.provider.by_dag(id)\n            children_all = self.provider.children([t.id for t in tasks])\n\n            def find_resume(task):\n                children = [c for c in children_all if c.parent == task.id]\n                children = sorted(children, key=lambda x: x.id, reverse=True)\n\n                if len(children) > 0:\n                    for c in children:\n                        if c.parent != task.id:\n                            continue\n\n                        info = yaml_load(c.additional_info)\n                        if \'distr_info\' not in info:\n                            continue\n\n                        if info[\'distr_info\'][\'rank\'] == 0:\n                            return {\n                                \'master_computer\': c.computer_assigned,\n                                \'master_task_id\': c.id,\n                                \'load_last\': True\n                            }\n                    raise Exception(\'Master task not found\')\n                else:\n                    return {\n                        \'master_computer\': task.computer_assigned,\n                        \'master_task_id\': task.id,\n                        \'load_last\': True\n                    }\n\n            for t in tasks:\n                if t.parent:\n                    t.continued = True\n                    continue\n\n                if t.status not in can_start_statuses:\n                    continue\n\n                if t.type == TaskType.Train.value:\n                    info = yaml_load(t.additional_info)\n                    info[\'resume\'] = find_resume(t)\n                    t.additional_info = yaml_dump(info)\n\n                t.status = TaskStatus.NotRan.value\n                t.pid = None\n                t.started = None\n                t.finished = None\n                t.computer_assigned = None\n                t.celery_id = None\n                t.worker_index = None\n                t.docker_assigned = None\n\n        self.provider.commit()\n        self.dags_start = []\n\n    def build(self):\n        try:\n            # if self.fast_check():\n            #     return\n\n            self.auxiliary = {\'time\': now()}\n\n            self.create_base()\n\n            self.process_stop_tasks()\n\n            self.process_start_dags()\n\n            self.process_parent_tasks()\n\n            self.load_tasks()\n\n            self.load_computers()\n\n            self.process_tasks()\n\n            self.write_auxiliary()\n\n        except ObjectDeletedError:\n            pass\n        except Exception as e:\n            if Session.sqlalchemy_error(e):\n                Session.cleanup(key=\'SupervisorBuilder\')\n                self.session = Session.create_session(key=\'SupervisorBuilder\')\n                self.logger = create_logger(self.session, \'SupervisorBuilder\')\n\n            self.logger.error(traceback.format_exc(), ComponentType.Supervisor)\n\n\ndef register_supervisor():\n    builder = SupervisorBuilder()\n    start_schedule([(builder.build, 1)])\n    return builder\n\n\n__all__ = [\'SupervisorBuilder\', \'register_supervisor\']\n'"
mlcomp/server/tests/__init__.py,0,b''
mlcomp/worker/executors/__init__.py,0,"b'# flake8: noqa\nfrom .base import StepWrap, Executor\n'"
mlcomp/worker/executors/bash.py,0,"b""import subprocess\n\nfrom mlcomp.worker.executors import Executor\n\n\n@Executor.register\nclass Bash(Executor):\n    def __init__(self, command: str, **kwargs):\n        super().__init__(**kwargs)\n\n        for k, v in kwargs.items():\n            command = command.replace(f'${k}', str(v))\n\n        self.command = command\n\n    def work(self):\n        self.info('Opening Process')\n        sub_commands = self.command.split('&&')\n        for sub in sub_commands:\n            self.info('executing '+sub)\n\n            process = subprocess.Popen('exec ' + sub,\n                                       stdout=subprocess.PIPE,\n                                       stderr=subprocess.PIPE,\n                                       shell=True\n                                       )\n            try:\n                self.add_child_process(process.pid)\n                self.info('Opening Process. Finished')\n\n                while True:\n                    line = process.stdout.readline()\n                    if not line:\n                        break\n                    self.info(line.decode().strip())\n\n                error = []\n                while True:\n                    line = process.stderr.readline()\n                    if not line:\n                        break\n                    line = line.decode().strip()\n                    error.append(line)\n\n                process.communicate()\n\n                if process.returncode != 0:\n                    raise Exception('\\n'.join(error))\n            finally:\n                process.kill()\n\n\n__all__ = ['Bash']\n"""
mlcomp/worker/executors/click.py,0,"b""import importlib\nimport sys\nfrom os import getcwd\nfrom os.path import join\n\nfrom mlcomp.worker.executors import Executor\n\n\n@Executor.register\nclass Click(Executor):\n    def __init__(self, module: str, command: str = None, **kwargs):\n        super().__init__(**kwargs)\n\n        self.module = module\n        self.command = command\n\n    def work(self):\n        self.info('click. creating module')\n        spec = importlib.util.spec_from_file_location(\n            self.module,\n            join(getcwd(), self.module+'.py')\n        )\n        m = importlib.util.module_from_spec(spec)\n        spec.loader.exec_module(m)\n        self.info('click. module created')\n\n        command = getattr(m, self.command)\n        self.info('click. command get')\n\n        m.tqdm = self.tqdm\n\n        self.info('click. tqdm set')\n\n        for p in command.params:\n            if p.name in self.kwargs:\n                p.default = self.kwargs[p.name]\n\n        sys.argv = sys.argv[:1]\n\n        self.info('click. setup finished. executing command')\n\n        stdout = sys.stdout\n        sys.stdout = self\n        self.info('click. stdout set')\n\n        command(standalone_mode=False)\n\n        sys.stdout = stdout\n\n        self.info('click. command finished')\n\n\n__all__ = ['Click']\n"""
mlcomp/worker/executors/kaggle.py,0,"b""import json\n\nfrom mlcomp.utils.io import zip_folder\nfrom typing import List\n\nimport shutil\nfrom enum import Enum\nimport os\nimport time\n\nimport socket\n\nfrom mlcomp.db.core import Session\nfrom mlcomp.db.enums import ComponentType\nfrom mlcomp.utils.misc import du\nfrom mlcomp.worker.executors.base.executor import Executor\nfrom mlcomp.utils.logging import create_logger\nfrom mlcomp.utils.config import Config\n\ntry:\n    from kaggle import api\nexcept OSError:\n    try:\n        logger = create_logger(Session.create_session(), __name__)\n        logger.warning(\n            'Could not find kaggle.json. '\n            'Kaggle executors can not be used', ComponentType.Worker,\n            socket.gethostname()\n        )\n    except Exception:\n        pass\n\n\nclass DownloadType(Enum):\n    Kaggle = 0\n    Link = 1\n\n\n@Executor.register\nclass Download(Executor):\n    def __init__(\n            self,\n            output: str,\n            type=DownloadType.Kaggle,\n            competition: str = None,\n            link: str = None,\n            **kwargs\n    ):\n        super().__init__(**kwargs)\n\n        if type == DownloadType.Kaggle and competition is None:\n            raise Exception('Competition is required for Kaggle')\n        self.type = type\n        self.competition = competition\n        self.link = link\n        self.output = output\n\n    def work(self):\n        api.competition_download_files(self.competition, self.output)\n\n    @classmethod\n    def _from_config(\n            cls, executor: dict, config: Config, additional_info: dict\n    ):\n        output = os.path.join(config.data_folder, config.get('output', '.'))\n        return cls(output=output, competition=executor['competition'])\n\n\n@Executor.register\nclass Submit(Executor):\n    def __init__(\n            self,\n            competition: str,\n            submit_type: str = 'kernel',\n            kernel_suffix: str = 'api',\n            message: str = '',\n            wait_seconds: int = 60 * 20,\n            file: str = None,\n            max_size: int = None,\n            datasets: List[str] = (),\n            folders: List[str] = (),\n            files: List[str] = (),\n            **kwargs\n    ):\n        super().__init__(**kwargs)\n\n        if not message and hasattr(self, 'model_id'):\n            message = f'model_id = {self.model_id}'\n\n        self.max_size = max_size\n        self.competition = competition\n        self.wait_seconds = wait_seconds\n        self.submit_type = submit_type\n        self.kernel_suffix = kernel_suffix\n        self.message = message\n        self.folders = folders\n        self.datasets = datasets\n        self.files = files\n\n        if not file and hasattr(self, 'model_name'):\n            file = f'data/submissions/{self.model_name}_{self.suffix}.csv'\n        self.file = file\n\n        assert self.submit_type in ['file', 'kernel']\n\n    def file_submit(self):\n        self.info(f'file_submit. file = {self.file} start')\n        api.competition_submit(\n            self.file, message=self.message, competition=self.competition\n        )\n        self.info(f'file_submit. file = {self.file} end')\n\n    def kernel_submit(self):\n        self.info('kernel_submit creating dataset')\n\n        folder = os.path.expanduser(\n            f'~/.kaggle/competitions/{self.competition}'\n        )\n        shutil.rmtree(folder, ignore_errors=True)\n        os.makedirs(folder, exist_ok=True)\n\n        total_size = sum([du(f) for f in self.folders])\n        total_size += sum([du(f) for f in self.files])\n\n        if self.max_size:\n            assert total_size < self.max_size, \\\n                f'max_size = {self.max_size} Gb. Current size = {total_size}'\n\n        config = api.read_config_file()\n        username = config['username']\n        competition = self.competition\n        dataset_meta = {\n            'competition': f'{competition}',\n            'id': f'{username}/{competition}-api-dataset',\n            'licenses': [{\n                'name': 'CC0-1.0'\n            }],\n            'title': 'API auto'\n        }\n        with open(f'{folder}/dataset-metadata.json', 'w') as f:\n            json.dump(dataset_meta, f)\n\n        self.info('\\tzipping folders')\n\n        dst = os.path.join(folder, 'dataset.zip')\n        zip_folder(folders=self.folders, dst=dst, files=self.files)\n\n        self.info('\\tfolders are zipped. uploading dataset')\n        if not any(d.ref == dataset_meta['id'] for d in\n                   api.dataset_list(user=username)):\n            api.dataset_create_new(folder)\n        else:\n            res = api.dataset_create_version(folder, 'Updated')\n            if res.status == 'error':\n                raise Exception('dataset_create_version Error: ' + res.error)\n\n        self.info('dataset uploaded. starting kernel')\n\n        # dataset update time\n        time.sleep(30)\n\n        slug = 'predict'\n\n        def push_notebook(file: str, slug: str):\n            shutil.copy(file, os.path.join(folder, 'predict.ipynb'))\n\n            datasets = [dataset_meta['id']] + list(self.datasets)\n            kernel_meta = {\n                'id': f'{username}/{slug}',\n                'code_file': 'predict.ipynb',\n                'language': 'python',\n                'kernel_type': 'notebook',\n                'is_private': 'true',\n                'enable_gpu': 'true',\n                'enable_internet': 'false',\n                'dataset_sources': datasets,\n                'competition_sources': [competition],\n                'title': f'{slug}',\n                'kernel_sources': []\n            }\n            with open(f'{folder}/kernel-metadata.json', 'w') as f:\n                json.dump(kernel_meta, f)\n\n            api.kernels_push(folder)\n\n        push_notebook('predict.ipynb', 'predict')\n\n        self.info('kernel is pushed. waiting for the end of the commit')\n\n        self.info(f'kernel address: https://www.kaggle.com/{username}/{slug}')\n\n    def work(self):\n        if self.submit_type == 'file':\n            self.file_submit()\n        else:\n            self.kernel_submit()\n\n\n__all__ = ['Download', 'Submit']\n"""
mlcomp/worker/executors/model.py,4,"b'import os\nfrom os.path import join\nimport shutil\nfrom pathlib import Path\nimport sys\n\nimport safitty\nimport torch\nfrom mlcomp.utils.io import yaml_load\n\nfrom catalyst.dl import Runner\nfrom torch.jit import ScriptModule\nimport torch.nn as nn\n\nfrom catalyst.dl.core import Experiment\nfrom catalyst import utils\nfrom catalyst.utils import import_experiment_and_runner\n\nfrom mlcomp import TASK_FOLDER, MODEL_FOLDER\nfrom mlcomp.db.models import Model\nfrom mlcomp.db.providers import TaskProvider, ModelProvider, \\\n    ProjectProvider, DagProvider\nfrom mlcomp.utils.misc import now\nfrom mlcomp.utils.config import Config\nfrom mlcomp.worker.executors import Executor\n\n\nclass _ForwardOverrideModel(nn.Module):\n    """"""\n    Model that calls specified method instead of forward\n\n    (Workaround, single method tracing is not supported)\n    """"""\n\n    def __init__(self, model, method_name):\n        super().__init__()\n        self.model = model\n        self.method = method_name\n\n    def forward(self, *args, **kwargs):\n        args = args[0][self.method]\n        if isinstance(args, dict):\n            kwargs = args\n            args = ()\n        return getattr(self.model, self.method)(*args, **kwargs)\n\n\nclass _TracingModelWrapper(nn.Module):\n    """"""\n    Wrapper that traces model with batch instead of calling it\n\n    (Workaround, to use native model batch handler)\n    """"""\n\n    def __init__(self, model, method_name):\n        super().__init__()\n        self.method_name = method_name\n        self.model = model\n        self.tracing_result: ScriptModule\n\n    def __call__(self, *args, **kwargs):\n        method_model = _ForwardOverrideModel(self.model, self.method_name)\n        example_inputs = {\n            self.method_name: kwargs if len(kwargs) > 0 else args\n        }\n\n        # noinspection PyTypeChecker\n        self.tracing_result = torch.jit.trace(\n            method_model, example_inputs=example_inputs\n        )\n\n\ndef trace_model(\n    model: Model,\n    runner: Runner,\n    batch=None,\n    method_name: str = ""forward"",\n    mode: str = ""eval"",\n    requires_grad: bool = False,\n    opt_level: str = None,\n    device: str = ""cpu"",\n    predict_params: dict = None,\n) -> ScriptModule:\n    """"""\n    Traces model using runner and batch\n\n    Args:\n        model: Model to trace\n        runner: Model\'s native runner that was used to train model\n        batch: Batch to trace the model\n        method_name (str): Model\'s method name that will be\n            used as entrypoint during tracing\n        mode (str): Mode for model to trace (``train`` or ``eval``)\n        requires_grad (bool): Flag to use grads\n        opt_level (str): Apex FP16 init level, optional\n        device (str): Torch device\n        predict_params (dict): additional parameters for model forward\n\n    Returns:\n        (ScriptModule): Traced model\n    """"""\n    if batch is None or runner is None:\n        raise ValueError(""Both batch and runner must be specified."")\n\n    if mode not in [""train"", ""eval""]:\n        raise ValueError(f""Unknown mode \'{mode}\'. Must be \'eval\' or \'train\'"")\n\n    predict_params = predict_params or {}\n\n    tracer = _TracingModelWrapper(model, method_name)\n    if opt_level is not None:\n        utils.assert_fp16_available()\n        # If traced in AMP we need to initialize the model before calling\n        # the jit\n        # https://github.com/NVIDIA/apex/issues/303#issuecomment-493142950\n        from apex import amp\n        model = model.to(device)\n        model = amp.initialize(model, optimizers=None, opt_level=opt_level)\n        # after fixing this bug https://github.com/pytorch/pytorch/issues/23993\n        params = {**predict_params, ""check_trace"": False}\n    else:\n        params = predict_params\n\n    getattr(model, mode)()\n    utils.set_requires_grad(model, requires_grad=requires_grad)\n\n    _runner_model, _runner_device = runner.model, runner.device\n\n    runner.model, runner.device = tracer, device\n    runner.predict_batch(batch, **params)\n    result: ScriptModule = tracer.tracing_result\n\n    runner.model, runner.device = _runner_model, _runner_device\n    return result\n\n\ndef trace_model_from_checkpoint(logdir, logger, method_name=\'forward\',\n                                file=\'best\'):\n    config_path = f\'{logdir}/configs/_config.json\'\n    checkpoint_path = f\'{logdir}/checkpoints/{file}.pth\'\n    logger.info(\'Load config\')\n    config = safitty.load(config_path)\n    if \'distributed_params\' in config:\n        del config[\'distributed_params\']\n\n    # Get expdir name\n    # noinspection SpellCheckingInspection,PyTypeChecker\n    # We will use copy of expdir from logs for reproducibility\n    expdir_name = config[\'args\'][\'expdir\']\n    logger.info(f\'expdir_name from args: {expdir_name}\')\n\n    sys.path.insert(0, os.path.abspath(join(logdir, \'../\')))\n\n    expdir_from_logs = os.path.abspath(join(logdir, \'../\', expdir_name))\n\n    logger.info(f\'expdir_from_logs: {expdir_from_logs}\')\n    logger.info(\'Import experiment and runner from logdir\')\n\n    ExperimentType, RunnerType = \\\n        import_experiment_and_runner(Path(expdir_from_logs))\n    experiment: Experiment = ExperimentType(config)\n\n    logger.info(f\'Load model state from checkpoints/{file}.pth\')\n    model = experiment.get_model(next(iter(experiment.stages)))\n    checkpoint = utils.load_checkpoint(checkpoint_path)\n    utils.unpack_checkpoint(checkpoint, model=model)\n\n    device = \'cpu\'\n    stage = list(experiment.stages)[0]\n    loader = 0\n    mode = \'eval\'\n    requires_grad = False\n    opt_level = None\n\n    runner: RunnerType = RunnerType()\n    runner.model, runner.device = model, device\n\n    batch = experiment.get_native_batch(stage, loader)\n\n    logger.info(\'Tracing\')\n    traced = trace_model(\n        model,\n        runner,\n        batch,\n        method_name=method_name,\n        mode=mode,\n        requires_grad=requires_grad,\n        opt_level=opt_level,\n        device=device,\n    )\n\n    logger.info(\'Done\')\n    return traced\n\n\n@Executor.register\nclass ModelAdd(Executor):\n    def __init__(\n            self,\n            name: str,\n            project: int,\n            fold: int,\n            train_task: int = None,\n            child_task: int = None,\n            file: str = None,\n            **kwargs\n    ):\n        super().__init__(**kwargs)\n\n        self.train_task = train_task\n        self.name = name\n        self.child_task = child_task\n        self.project = project\n        self.file = file\n        self.fold = fold\n\n    def work(self):\n        project = ProjectProvider(self.session).by_id(self.project)\n\n        self.info(f\'Task = {self.train_task} child_task: {self.child_task}\')\n\n        model = Model(\n            created=now(),\n            name=self.name,\n            project=self.project,\n            equations=\'\',\n            fold=self.fold\n        )\n\n        provider = ModelProvider(self.session)\n        if self.train_task:\n            task_provider = TaskProvider(self.session)\n            dag_provider = DagProvider(self.session)\n            task = task_provider.by_id(self.train_task)\n            dag = dag_provider.by_id(task.dag)\n\n            task_dir = join(TASK_FOLDER, str(self.child_task or task.id))\n\n            # get log directory\n            config = yaml_load(dag.config)\n            executor_config = config[\'executors\'][task.executor]\n            catalyst_config_file = executor_config[\'args\'][\'config\']\n            catalyst_config_file = join(task_dir, catalyst_config_file)\n            catalyst_config = yaml_load(file=catalyst_config_file)\n            catalyst_logdir = catalyst_config[\'args\'][\'logdir\']\n\n            model.score_local = task.score\n\n            src_log = f\'{task_dir}/{catalyst_logdir}\'\n            models_dir = join(MODEL_FOLDER, project.name)\n            os.makedirs(models_dir, exist_ok=True)\n\n            model_path_tmp = f\'{src_log}/traced.pth\'\n            traced = trace_model_from_checkpoint(src_log, self, file=self.file)\n\n            model_path = f\'{models_dir}/{model.name}.pth\'\n            model_weight_path = f\'{models_dir}/{model.name}_weight.pth\'\n            torch.jit.save(traced, model_path_tmp)\n            shutil.copy(model_path_tmp, model_path)\n            file = self.file = \'best_full\'\n            shutil.copy(f\'{src_log}/checkpoints/{file}.pth\',\n                        model_weight_path)\n\n        provider.add(model)\n\n    @classmethod\n    def _from_config(\n            cls, executor: dict, config: Config, additional_info: dict\n    ):\n        return ModelAdd(\n            name=executor[\'name\'],\n            project=executor[\'project\'],\n            train_task=executor[\'task\'],\n            child_task=executor[\'child_task\'],\n            fold=executor[\'fold\'],\n            file=executor[\'file\']\n        )\n\n\n__all__ = [\'ModelAdd\', \'trace_model_from_checkpoint\']\n\nif __name__ == \'__main__\':\n    import logging\n    trace_model_from_checkpoint(\n        \'/home/ingenix/mlcomp/tasks/64913/log\',\n        logging\n    )'"
mlcomp/worker/executors/split.py,0,"b""from os.path import join\n\nimport pandas as pd\n\nfrom mlcomp.utils.config import Config\nfrom mlcomp.contrib.split import stratified_k_fold\nfrom mlcomp.worker.executors import Executor\n\n\n@Executor.register\nclass Split(Executor):\n    def __init__(\n        self,\n        variant: str,\n        out: str,\n        n_splits: int = 5,\n        file: str = None,\n        label: str = None\n    ):\n        self.variant = variant\n        self.file = file\n        self.n_splits = n_splits\n        self.out = out\n        self.label = label\n\n    def work(self):\n        if self.variant == 'frame':\n            fold = stratified_k_fold(\n                file=self.file, n_splits=self.n_splits, label=self.label\n            )\n            df = pd.DataFrame({'fold': fold})\n            df.to_csv(self.out, index=False)\n\n    @classmethod\n    def _from_config(\n        cls, executor: dict, config: Config, additional_info: dict\n    ):\n        file = join(config.data_folder, executor.get('file'))\n        return cls(\n            variant=executor['variant'],\n            out=join(config.data_folder, 'fold.csv'),\n            file=file,\n            label=executor['label']\n        )\n\n\n__all__ = ['Split']\n"""
mlcomp/worker/reports/__init__.py,0,b''
mlcomp/worker/reports/classification.py,0,"b""import pickle\nfrom typing import Tuple\n\nimport cv2\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix\n\nfrom mlcomp.db.core import Session\nfrom mlcomp.db.models import Report, ReportImg, Task, ReportTasks, ReportSeries\nfrom mlcomp.db.providers import ReportProvider, \\\n    ReportLayoutProvider, \\\n    TaskProvider,\\\n    ReportImgProvider, \\\n    ReportTasksProvider, \\\n    ReportSeriesProvider, \\\n    DagProvider\nfrom mlcomp.utils.img import resize_saving_ratio\nfrom mlcomp.utils.io import yaml_load, yaml_dump\nfrom mlcomp.utils.misc import now\n\n\nclass ClassificationReportBuilder:\n    def __init__(\n        self,\n        session: Session,\n        task: Task,\n        layout: str,\n        part: str = 'valid',\n        name: str = 'img_classify',\n        max_img_size: Tuple[int, int] = None,\n        main_metric: str = 'accuracy',\n        plot_count: int = 0\n    ):\n        self.session = session\n        self.task = task\n        self.layout = layout\n        self.part = part\n        self.name = name or 'img_classify'\n        self.max_img_size = max_img_size\n        self.main_metric = main_metric\n        self.plot_count = plot_count\n\n        self.dag_provider = DagProvider(session)\n        self.report_provider = ReportProvider(session)\n        self.layout_provider = ReportLayoutProvider(session)\n        self.task_provider = TaskProvider(session)\n        self.report_img_provider = ReportImgProvider(session)\n        self.report_task_provider = ReportTasksProvider(session)\n        self.report_series_provider = ReportSeriesProvider(session)\n\n        self.project = self.task_provider.project(task.id).id\n        self.layout = self.layout_provider.by_name(layout)\n        self.layout_dict = yaml_load(self.layout.content)\n\n    def create_base(self):\n        report = Report(\n            config=yaml_dump(self.layout_dict),\n            time=now(),\n            layout=self.layout.name,\n            project=self.project,\n            name=self.name\n        )\n        self.report_provider.add(report)\n        self.report_task_provider.add(\n            ReportTasks(report=report.id, task=self.task.id)\n        )\n\n        self.task.report = report.id\n        self.task_provider.update()\n\n    def process_scores(self, scores):\n        for key, item in self.layout_dict['items'].items():\n            item['name'] = key\n            if item['type'] == 'series' and item['key'] in scores:\n                series = ReportSeries(\n                    name=item['name'],\n                    value=float(scores[item['key']]),\n                    epoch=0,\n                    time=now(),\n                    task=self.task.id,\n                    part='valid',\n                    stage='stage1'\n                )\n\n                self.report_series_provider.add(series)\n\n    def process_pred(self, imgs: np.array, preds: np.array,\n                     targets: np.array = None, attrs=None, scores=None):\n        for key, item in self.layout_dict['items'].items():\n            item['name'] = key\n            if item['type'] != 'img_classify':\n                continue\n\n            report_imgs = []\n            dag = self.dag_provider.by_id(self.task.dag)\n\n            for i in range(len(imgs)):\n                if self.plot_count <= 0:\n                    break\n\n                img = resize_saving_ratio(imgs[i], self.max_img_size)\n                pred = preds[i]\n                attr = attrs[i] if attrs else {}\n\n                y = None\n                score = None\n                if targets is not None:\n                    y = targets[i]\n                    score = float(scores[self.main_metric][i])\n\n                y_pred = pred.argmax()\n                retval, buffer = cv2.imencode('.jpg', img)\n                report_img = ReportImg(\n                    group=item['name'],\n                    epoch=0,\n                    task=self.task.id,\n                    img=buffer,\n                    dag=self.task.dag,\n                    part=self.part,\n                    project=self.project,\n                    y_pred=y_pred,\n                    y=y,\n                    score=score,\n                    **attr\n                )\n\n                report_imgs.append(report_img)\n                dag.img_size += report_img.size\n\n            self.dag_provider.commit()\n            self.report_img_provider.bulk_save_objects(report_imgs)\n\n            if targets is not None and item.get('confusion_matrix'):\n                matrix = confusion_matrix(\n                    targets,\n                    preds.argmax(axis=1),\n                    labels=np.arange(preds.shape[1])\n                )\n                matrix = np.array(matrix)\n                c = {'data': matrix}\n                obj = ReportImg(\n                    group=item['name'] + '_confusion',\n                    epoch=0,\n                    task=self.task.id,\n                    img=pickle.dumps(c),\n                    project=self.project,\n                    dag=self.task.dag,\n                    part=self.part\n                )\n                self.report_img_provider.add(obj)\n\n            self.plot_count -= 1\n\n\n__all__ = ['ClassificationReportBuilder']\n"""
mlcomp/worker/reports/segmenation.py,0,"b""from typing import Tuple, List\n\nimport cv2\nimport numpy as np\n\nfrom mlcomp.db.core import Session\nfrom mlcomp.db.models import Report, ReportImg, Task, ReportTasks, ReportSeries\nfrom mlcomp.db.providers import ReportProvider, ReportLayoutProvider, \\\n    TaskProvider, ReportImgProvider, ReportTasksProvider, \\\n    ReportSeriesProvider, DagProvider\nfrom mlcomp.utils.img import resize_saving_ratio\nfrom mlcomp.utils.io import yaml_load, yaml_dump\nfrom mlcomp.utils.misc import now\n\n\nclass SegmentationReportBuilder:\n    def __init__(\n            self,\n            session: Session,\n            task: Task,\n            layout: str,\n            part: str = 'valid',\n            name: str = 'img_segment',\n            max_img_size: Tuple[int, int] = None,\n            stack_type: str = 'vertical',\n            main_metric: str = 'dice',\n            plot_count: int = 0,\n            colors: List[Tuple] = None\n    ):\n        self.session = session\n        self.task = task\n        self.layout = layout\n        self.part = part\n        self.name = name or 'img_segment'\n        self.max_img_size = max_img_size\n        self.stack_type = stack_type\n        self.main_metric = main_metric\n        self.colors = colors\n        self.plot_count = plot_count\n\n        self.dag_provider = DagProvider(session)\n        self.report_provider = ReportProvider(session)\n        self.layout_provider = ReportLayoutProvider(session)\n        self.task_provider = TaskProvider(session)\n        self.report_img_provider = ReportImgProvider(session)\n        self.report_task_provider = ReportTasksProvider(session)\n        self.report_series_provider = ReportSeriesProvider(session)\n\n        self.project = self.task_provider.project(task.id).id\n        self.layout = self.layout_provider.by_name(layout)\n        self.layout_dict = yaml_load(self.layout.content)\n\n        self.create_base()\n\n    def create_base(self):\n        report = Report(\n            config=yaml_dump(self.layout_dict),\n            time=now(),\n            layout=self.layout.name,\n            project=self.project,\n            name=self.name\n        )\n        self.report_provider.add(report)\n        self.report_task_provider.add(\n            ReportTasks(report=report.id, task=self.task.id)\n        )\n\n        self.task.report = report.id\n        self.task_provider.update()\n\n    def encode_pred(self, mask: np.array):\n        res = np.zeros((*mask.shape[1:], 3), dtype=np.uint8)\n        for i, c in enumerate(mask):\n            c = np.repeat(c[:, :, None], 3, axis=2)\n            color = self.colors[i] if self.colors is not None else (\n                255, 255, 255\n            )\n            res += (c * color).astype(np.uint8)\n\n        return res\n\n    def plot_mask(self, img: np.array, mask: np.array):\n        if len(img.shape) == 2:\n            img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n        img = img.astype(np.uint8)\n        mask = mask.astype(np.uint8)\n\n        for i, c in enumerate(mask):\n            contours, _ = cv2.findContours(\n                c, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE\n            )\n            color = self.colors[i] if self.colors else (0, 255, 0)\n            for i in range(0, len(contours)):\n                cv2.polylines(img, contours[i], True, color, 2)\n\n        return img\n\n    def process_scores(self, scores):\n        for key, item in self.layout_dict['items'].items():\n            item['name'] = key\n            if item['type'] == 'series' and item['key'] in scores:\n                series = ReportSeries(\n                    name=item['name'],\n                    value=scores[item['key']],\n                    epoch=0,\n                    time=now(),\n                    task=self.task.id,\n                    part='valid',\n                    stage='stage1'\n                )\n\n                self.report_series_provider.add(series)\n\n    def process_pred(self, imgs: np.array, preds: dict,\n                     targets: np.array = None, attrs=None, scores=None):\n        for key, item in self.layout_dict['items'].items():\n            item['name'] = key\n            if item['type'] != 'img_segment':\n                continue\n\n            report_imgs = []\n            dag = self.dag_provider.by_id(self.task.dag)\n\n            for i in range(len(imgs)):\n                if self.plot_count <= 0:\n                    break\n\n                if targets is not None:\n                    img = self.plot_mask(imgs[i], targets[i])\n                else:\n                    img = imgs[i]\n\n                imgs_add = [img]\n                for key, value in preds.items():\n                    imgs_add.append(self.encode_pred(value[i]))\n\n                for j in range(len(imgs_add)):\n                    imgs_add[j] = resize_saving_ratio(imgs_add[j],\n                                                      self.max_img_size)\n\n                if self.stack_type == 'horizontal':\n                    img = np.hstack(imgs_add)\n                else:\n                    img = np.vstack(imgs_add)\n\n                attr = attrs[i] if attrs else {}\n\n                score = None\n                if targets is not None:\n                    score = scores[self.main_metric][i]\n\n                retval, buffer = cv2.imencode('.jpg', img)\n                report_img = ReportImg(\n                    group=item['name'],\n                    epoch=0,\n                    task=self.task.id,\n                    img=buffer,\n                    dag=self.task.dag,\n                    part=self.part,\n                    project=self.project,\n                    score=score,\n                    **attr\n                )\n\n                self.plot_count -= 1\n                report_imgs.append(report_img)\n                dag.img_size += report_img.size\n\n            self.dag_provider.commit()\n            self.report_img_provider.bulk_save_objects(report_imgs)\n\n\n__all__ = ['SegmentationReportBuilder']\n"""
mlcomp/worker/tests/__init__.py,0,b''
mlcomp/contrib/catalyst/callbacks/__init__.py,0,b''
mlcomp/contrib/catalyst/callbacks/inference.py,0,"b""import os\n\nimport numpy as np\n\nfrom collections import defaultdict\nfrom catalyst.dl import State as RunnerState\nfrom catalyst.dl.core import Callback\n\n\nclass InferBestCallback(Callback):\n    def __init__(self, out_dir=None, out_prefix=None, best_only=False):\n        self.out_dir = out_dir\n        self.out_prefix = out_prefix\n        self.predictions = defaultdict(lambda: [])\n        self.best_only = best_only\n        self._keys_from_state = ['out_dir', 'out_prefix']\n\n    def on_stage_start(self, state: RunnerState):\n        for key in self._keys_from_state:\n            value = getattr(state, key, None)\n            if value is not None:\n                setattr(self, key, value)\n        # assert self.out_prefix is not None\n        if self.out_dir is not None:\n            self.out_prefix = str(self.out_dir) + '/' + str(self.out_prefix)\n        if self.out_prefix is not None:\n            os.makedirs(os.path.dirname(self.out_prefix), exist_ok=True)\n\n    def on_loader_start(self, state: RunnerState):\n        self.predictions = defaultdict(lambda: [])\n\n    def on_batch_end(self, state: RunnerState):\n        dct = state.output\n        dct = {key: value.detach().cpu().numpy() for key, value in dct.items()}\n        for key, value in dct.items():\n            self.predictions[key].append(value)\n\n    def on_loader_end(self, state: RunnerState):\n        if self.best_only and not state.metrics.is_best:\n            return\n\n        self.predictions = {\n            key: np.concatenate(value, axis=0)\n            for key, value in self.predictions.items()\n        }\n        if self.out_prefix is not None:\n            for key, value in self.predictions.items():\n                suffix = '.'.join([state.loader_name, key])\n                np.save(f'{self.out_prefix}/{suffix}.npy', value)\n"""
mlcomp/contrib/catalyst/optim/__init__.py,0,"b""from .cosineanneal import OneCycleCosineAnnealLR\n\n__all__ = ['OneCycleCosineAnnealLR']\n"""
mlcomp/contrib/catalyst/optim/cosineanneal.py,1,"b""from torch.optim.lr_scheduler import CosineAnnealingLR\n\n\nclass OneCycleCosineAnnealLR(CosineAnnealingLR):\n    def __init__(self, *args, **kwargs):\n        self.start_epoch = None\n        self.last_epoch = None\n        super().__init__(*args, **kwargs)\n\n    def step(self, epoch=None):\n        if self.last_epoch is not None:\n            if self.start_epoch is None:\n                self.start_epoch = self.last_epoch\n                self.last_epoch = 0\n                for i in range(len(self.base_lrs)):\n                    self.optimizer.param_groups[i]['lr'] = self.base_lrs[0]\n\n            if self.last_epoch >= self.T_max - 1:\n                self.start_epoch = self.last_epoch\n                self.last_epoch = -1\n                for i in range(len(self.base_lrs)):\n                    self.optimizer.param_groups[i]['lr'] = self.base_lrs[0]\n\n        super().step(epoch)\n\n\n__all__ = ['OneCycleCosineAnnealLR']\n"""
mlcomp/contrib/model/video/__init__.py,0,b'from .resnext3d import ResNeXt3D\n'
mlcomp/contrib/segmentation/base/__init__.py,0,b'# flake8: noqa\nfrom .encoder_decoder import EncoderDecoder'
mlcomp/contrib/segmentation/base/encoder_decoder.py,3,"b'import torch\nimport torch.nn as nn\nfrom .model import Model\n\n\nclass EncoderDecoder(Model):\n\n    def __init__(self, encoder, decoder, activation):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n        if callable(activation) or activation is None:\n            self.activation = activation\n        elif activation == \'softmax\':\n            self.activation = nn.Softmax(dim=1)\n        elif activation == \'sigmoid\':\n            self.activation = nn.Sigmoid()\n        else:\n            raise ValueError(\n                \'Activation should be ""sigmoid""/""softmax""/callable/None\')\n\n    def forward(self, x):\n        """"""Sequentially pass `x` trough model`s\n         `encoder` and `decoder` (return logits!)""""""\n        x = self.encoder(x)\n        x = self.decoder(x)\n        return x\n\n    def predict(self, x):\n        """"""Inference method. Switch model to `eval` mode, call `.forward(x)`\n        and apply activation function\n         (if activation is not `None`) with `torch.no_grad()`\n\n        Args:\n            x: 4D torch tensor with shape (batch_size, channels, height, width)\n\n        Return:\n            prediction: 4D torch tensor with shape\n            (batch_size, classes, height, width)\n\n        """"""\n        if self.training:\n            self.eval()\n\n        with torch.no_grad():\n            x = self.forward(x)\n            if self.activation:\n                x = self.activation(x)\n\n        return x\n'"
mlcomp/contrib/segmentation/base/model.py,1,"b""import torch.nn as nn\n\n\nclass Model(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n\n    def initialize(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out',\n                                        nonlinearity='relu')\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n"""
mlcomp/contrib/segmentation/common/__init__.py,0,b''
mlcomp/contrib/segmentation/common/blocks.py,1,"b'import torch.nn as nn\n\n\nclass Conv2dReLU(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, padding=0,\n                 stride=1, use_batchnorm=True, **batchnorm_params):\n        super().__init__()\n\n        layers = [\n            nn.Conv2d(in_channels, out_channels, kernel_size,\n                      stride=stride, padding=padding,\n                      bias=not (use_batchnorm)),\n            nn.ReLU(inplace=True),\n        ]\n\n        if use_batchnorm:\n            layers.insert(1, nn.BatchNorm2d(out_channels, **batchnorm_params))\n\n        self.block = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.block(x)\n\n\nclass SCSEModule(nn.Module):\n    def __init__(self, ch, re=16):\n        super().__init__()\n        self.cSE = nn.Sequential(nn.AdaptiveAvgPool2d(1),\n                                 nn.Conv2d(ch, ch // re, 1),\n                                 nn.ReLU(inplace=True),\n                                 nn.Conv2d(ch // re, ch, 1),\n                                 nn.Sigmoid()\n                                 )\n        self.sSE = nn.Sequential(nn.Conv2d(ch, ch, 1),\n                                 nn.Sigmoid())\n\n    def forward(self, x):\n        return x * self.cSE(x) + x * self.sSE(x)\n'"
mlcomp/contrib/segmentation/deeplabv3/__init__.py,0,b''
mlcomp/contrib/segmentation/deeplabv3/aspp.py,5,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass _ASPPModule(nn.Module):\n    def __init__(self, inplanes, planes, kernel_size, padding, dilation,\n                 BatchNorm):\n        super(_ASPPModule, self).__init__()\n        self.atrous_conv = nn.Conv2d(inplanes, planes, kernel_size=kernel_size,\n                                     stride=1, padding=padding,\n                                     dilation=dilation, bias=False)\n        self.bn = BatchNorm(planes)\n        self.relu = nn.ReLU()\n\n        self._init_weight()\n\n    def forward(self, x):\n        x = self.atrous_conv(x)\n        x = self.bn(x)\n\n        return self.relu(x)\n\n    def _init_weight(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                torch.nn.init.kaiming_normal_(m.weight)\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n\nclass ASPP(nn.Module):\n    def __init__(self, backbone, output_stride, BatchNorm):\n        super(ASPP, self).__init__()\n        if backbone == 'drn':\n            inplanes = 512\n        elif backbone == 'mobilenet':\n            inplanes = 320\n        else:\n            inplanes = 2048\n        if output_stride == 16:\n            dilations = [1, 6, 12, 18]\n        elif output_stride == 8:\n            dilations = [1, 12, 24, 36]\n        else:\n            raise NotImplementedError\n\n        self.aspp1 = _ASPPModule(inplanes, 256, 1, padding=0,\n                                 dilation=dilations[0], BatchNorm=BatchNorm)\n        self.aspp2 = _ASPPModule(inplanes, 256, 3, padding=dilations[1],\n                                 dilation=dilations[1], BatchNorm=BatchNorm)\n        self.aspp3 = _ASPPModule(inplanes, 256, 3, padding=dilations[2],\n                                 dilation=dilations[2], BatchNorm=BatchNorm)\n        self.aspp4 = _ASPPModule(inplanes, 256, 3, padding=dilations[3],\n                                 dilation=dilations[3], BatchNorm=BatchNorm)\n\n        self.global_avg_pool = nn.Sequential(nn.AdaptiveAvgPool2d((1, 1)),\n                                             nn.Conv2d(inplanes, 256, 1,\n                                                       stride=1, bias=False),\n                                             BatchNorm(256),\n                                             nn.ReLU())\n        self.conv1 = nn.Conv2d(1280, 256, 1, bias=False)\n        self.bn1 = BatchNorm(256)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(0.5)\n        self._init_weight()\n\n    def forward(self, x):\n        x1 = self.aspp1(x)\n        x2 = self.aspp2(x)\n        x3 = self.aspp3(x)\n        x4 = self.aspp4(x)\n        x5 = self.global_avg_pool(x)\n        x5 = F.interpolate(x5, size=x4.size()[2:], mode='bilinear',\n                           align_corners=True)\n        x = torch.cat((x1, x2, x3, x4, x5), dim=1)\n\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n\n        return self.dropout(x)\n\n    def _init_weight(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                # n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                # m.weight.data.normal_(0, math.sqrt(2. / n))\n                torch.nn.init.kaiming_normal_(m.weight)\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n\ndef build_aspp(backbone, output_stride, BatchNorm):\n    return ASPP(backbone, output_stride, BatchNorm)\n"""
mlcomp/contrib/segmentation/deeplabv3/decoder.py,4,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Decoder(nn.Module):\n    def __init__(self, num_classes, backbone, BatchNorm):\n        super(Decoder, self).__init__()\n        if backbone == 'resnet' or backbone == 'drn':\n            low_level_inplanes = 256\n        elif backbone == 'xception':\n            low_level_inplanes = 128\n        elif backbone == 'mobilenet':\n            low_level_inplanes = 24\n        else:\n            raise NotImplementedError\n\n        self.conv1 = nn.Conv2d(low_level_inplanes, 48, 1, bias=False)\n        self.bn1 = BatchNorm(48)\n        self.relu = nn.ReLU()\n        self.last_conv = nn.Sequential(\n            nn.Conv2d(304, 256, kernel_size=3, stride=1, padding=1,\n                      bias=False),\n            BatchNorm(256),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1,\n                      bias=False),\n            BatchNorm(256),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Conv2d(256, num_classes, kernel_size=1, stride=1))\n        self._init_weight()\n\n    def forward(self, x, low_level_feat):\n        low_level_feat = self.conv1(low_level_feat)\n        low_level_feat = self.bn1(low_level_feat)\n        low_level_feat = self.relu(low_level_feat)\n\n        x = F.interpolate(x, size=low_level_feat.size()[2:], mode='bilinear',\n                          align_corners=True)\n        x = torch.cat((x, low_level_feat), dim=1)\n        x = self.last_conv(x)\n\n        return x\n\n    def _init_weight(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                torch.nn.init.kaiming_normal_(m.weight)\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n\ndef build_decoder(num_classes, backbone, BatchNorm):\n    return Decoder(num_classes, backbone, BatchNorm)\n"""
mlcomp/contrib/segmentation/deeplabv3/deeplab.py,3,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom mlcomp.contrib.segmentation.deeplabv3.aspp import build_aspp\nfrom mlcomp.contrib.segmentation.deeplabv3.backbone import build_backbone\nfrom mlcomp.contrib.segmentation.deeplabv3.decoder import build_decoder\n\n\nclass DeepLab(nn.Module):\n    def __init__(self, backbone='resnet', output_stride=16, num_classes=21,\n                 freeze_bn=False):\n        super(DeepLab, self).__init__()\n        if backbone == 'drn':\n            output_stride = 8\n\n        BatchNorm = nn.BatchNorm2d\n        self.backbone = build_backbone(backbone, output_stride, BatchNorm)\n        self.aspp = build_aspp(backbone, output_stride, BatchNorm)\n        self.decoder = build_decoder(num_classes, backbone, BatchNorm)\n\n        if freeze_bn:\n            self.freeze_bn()\n\n    def forward(self, input):\n        x, low_level_feat = self.backbone(input)\n        x = self.aspp(x)\n        x = self.decoder(x, low_level_feat)\n        x = F.interpolate(x, size=input.size()[2:], mode='bilinear',\n                          align_corners=True)\n\n        return x\n\n    def freeze_bn(self):\n        for m in self.modules():\n            if isinstance(m, nn.BatchNorm2d):\n                m.eval()\n\n    def get_1x_lr_params(self):\n        modules = [self.backbone]\n        for i in range(len(modules)):\n            for m in modules[i].named_modules():\n                if isinstance(m[1], nn.Conv2d) or \\\n                        isinstance(m[1], nn.BatchNorm2d):\n                    for p in m[1].parameters():\n                        if p.requires_grad:\n                            yield p\n\n    def get_10x_lr_params(self):\n        modules = [self.aspp, self.decoder]\n        for i in range(len(modules)):\n            for m in modules[i].named_modules():\n                if isinstance(m[1], nn.Conv2d) \\\n                        or isinstance(m[1], nn.BatchNorm2d):\n                    for p in m[1].parameters():\n                        if p.requires_grad:\n                            yield p\n\n\nif __name__ == '__main__':\n    model = DeepLab(backbone='drn', output_stride=16)\n    model.eval()\n    input = torch.rand(1, 3, 513, 513)\n    output = model(input)\n    print(output.size())\n"""
mlcomp/contrib/segmentation/encoders/__init__.py,1,"b""import functools\nimport torch.utils.model_zoo as model_zoo\n\nfrom .resnet import resnet_encoders\nfrom .dpn import dpn_encoders\nfrom .vgg import vgg_encoders\nfrom .senet import senet_encoders\nfrom .densenet import densenet_encoders\nfrom .inceptionresnetv2 import inception_encoders\n\nfrom ._preprocessing import preprocess_input\n\nencoders = {}\nencoders.update(resnet_encoders)\nencoders.update(dpn_encoders)\nencoders.update(vgg_encoders)\nencoders.update(senet_encoders)\nencoders.update(densenet_encoders)\nencoders.update(inception_encoders)\n\n\ndef get_encoder(name, encoder_weights=None):\n    Encoder = encoders[name]['encoder']\n    encoder = Encoder(**encoders[name]['params'])\n    encoder.out_shapes = encoders[name]['out_shapes']\n\n    if encoder_weights is not None:\n        settings = encoders[name]['pretrained_settings'][encoder_weights]\n        encoder.load_state_dict(model_zoo.load_url(settings['url']))\n\n    return encoder\n\n\ndef get_encoder_names():\n    return list(encoders.keys())\n\n\ndef get_preprocessing_params(encoder_name, pretrained='imagenet'):\n    settings = encoders[encoder_name]['pretrained_settings']\n\n    if pretrained not in settings.keys():\n        raise ValueError(\n            'Avaliable pretrained options {}'.format(settings.keys()))\n\n    formatted_settings = {}\n    formatted_settings['input_space'] = settings[pretrained].get('input_space')\n    formatted_settings['input_range'] = settings[pretrained].get('input_range')\n    formatted_settings['mean'] = settings[pretrained].get('mean')\n    formatted_settings['std'] = settings[pretrained].get('std')\n    return formatted_settings\n\n\ndef get_preprocessing_fn(encoder_name, pretrained='imagenet'):\n    params = get_preprocessing_params(encoder_name, pretrained=pretrained)\n    return functools.partial(preprocess_input, **params)\n"""
mlcomp/contrib/segmentation/encoders/_preprocessing.py,0,"b""import numpy as np\n\n\ndef preprocess_input(x, mean=None, std=None, input_space='RGB',\n                     input_range=None, **kwargs):\n    if input_space == 'BGR':\n        x = x[..., ::-1].copy()\n\n    if input_range is not None:\n        if x.max() > 1 and input_range[1] == 1:\n            x = x / 255.\n\n    if mean is not None:\n        mean = np.array(mean)\n        x = x - mean\n\n    if std is not None:\n        std = np.array(std)\n        x = x / std\n\n    return x\n"""
mlcomp/contrib/segmentation/encoders/densenet.py,1,"b""import re\nimport torch.nn as nn\n\nfrom pretrainedmodels.models.torchvision_models import pretrained_settings\nfrom torchvision.models.densenet import DenseNet\n\n\nclass DenseNetEncoder(DenseNet):\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.pretrained = False\n        del self.classifier\n        self.initialize()\n\n    @staticmethod\n    def _transition(x, transition_block):\n        for module in transition_block:\n            x = module(x)\n            if isinstance(module, nn.ReLU):\n                skip = x\n        return x, skip\n\n    def initialize(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out',\n                                        nonlinearity='relu')\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n    def forward(self, x):\n\n        x = self.features.conv0(x)\n        x = self.features.norm0(x)\n        x = self.features.relu0(x)\n        x0 = x\n\n        x = self.features.pool0(x)\n        x = self.features.denseblock1(x)\n        x, x1 = self._transition(x, self.features.transition1)\n\n        x = self.features.denseblock2(x)\n        x, x2 = self._transition(x, self.features.transition2)\n\n        x = self.features.denseblock3(x)\n        x, x3 = self._transition(x, self.features.transition3)\n\n        x = self.features.denseblock4(x)\n        x4 = self.features.norm5(x)\n\n        features = [x4, x3, x2, x1, x0]\n        return features\n\n    def load_state_dict(self, state_dict):\n        pattern = re.compile(\n            r'^(.*denselayer\\d+\\.(?:norm|relu|conv))'\n            r'\\.((?:[12])\\.(?:weight|bias|running_mean|running_var))$')\n        for key in list(state_dict.keys()):\n            res = pattern.match(key)\n            if res:\n                new_key = res.group(1) + res.group(2)\n                state_dict[new_key] = state_dict[key]\n                del state_dict[key]\n\n        # remove linear\n        state_dict.pop('classifier.bias')\n        state_dict.pop('classifier.weight')\n\n        super().load_state_dict(state_dict)\n\n\ndensenet_encoders = {\n    'densenet121': {\n        'encoder': DenseNetEncoder,\n        'pretrained_settings': pretrained_settings['densenet121'],\n        'out_shapes': (1024, 1024, 512, 256, 64),\n        'params': {\n            'num_init_features': 64,\n            'growth_rate': 32,\n            'block_config': (6, 12, 24, 16),\n        }\n    },\n\n    'densenet169': {\n        'encoder': DenseNetEncoder,\n        'pretrained_settings': pretrained_settings['densenet169'],\n        'out_shapes': (1664, 1280, 512, 256, 64),\n        'params': {\n            'num_init_features': 64,\n            'growth_rate': 32,\n            'block_config': (6, 12, 32, 32),\n        }\n    },\n\n    'densenet201': {\n        'encoder': DenseNetEncoder,\n        'pretrained_settings': pretrained_settings['densenet201'],\n        'out_shapes': (1920, 1792, 512, 256, 64),\n        'params': {\n            'num_init_features': 64,\n            'growth_rate': 32,\n            'block_config': (6, 12, 48, 32),\n        }\n    },\n\n    'densenet161': {\n        'encoder': DenseNetEncoder,\n        'pretrained_settings': pretrained_settings['densenet161'],\n        'out_shapes': (2208, 2112, 768, 384, 96),\n        'params': {\n            'num_init_features': 96,\n            'growth_rate': 48,\n            'block_config': (6, 12, 36, 24),\n        }\n    },\n\n}\n"""
mlcomp/contrib/segmentation/encoders/dpn.py,4,"b""import numpy as np\n\nimport torch\nimport torch.nn.functional as F\n\nfrom pretrainedmodels.models.dpn import DPN\nfrom pretrainedmodels.models.dpn import pretrained_settings\n\n\nclass DPNEncorder(DPN):\n\n    def __init__(self, feature_blocks, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.feature_blocks = np.cumsum(feature_blocks)\n        self.pretrained = False\n\n        del self.last_linear\n\n    def forward(self, x):\n\n        features = []\n\n        input_block = self.features[0]\n\n        x = input_block.conv(x)\n        x = input_block.bn(x)\n        x = input_block.act(x)\n        features.append(x)\n\n        x = input_block.pool(x)\n\n        for i, module in enumerate(self.features[1:], 1):\n            x = module(x)\n            if i in self.feature_blocks:\n                features.append(x)\n\n        out_features = [\n            features[4],\n            F.relu(torch.cat(features[3], dim=1), inplace=True),\n            F.relu(torch.cat(features[2], dim=1), inplace=True),\n            F.relu(torch.cat(features[1], dim=1), inplace=True),\n            features[0],\n        ]\n\n        return out_features\n\n    def load_state_dict(self, state_dict, **kwargs):\n        state_dict.pop('last_linear.bias')\n        state_dict.pop('last_linear.weight')\n        super().load_state_dict(state_dict, **kwargs)\n\n\ndpn_encoders = {\n    'dpn68': {\n        'encoder': DPNEncorder,\n        'out_shapes': (832, 704, 320, 144, 10),\n        'pretrained_settings': pretrained_settings['dpn68'],\n        'params': {\n            'feature_blocks': (3, 4, 12, 4),\n            'groups': 32,\n            'inc_sec': (16, 32, 32, 64),\n            'k_r': 128,\n            'k_sec': (3, 4, 12, 3),\n            'num_classes': 1000,\n            'num_init_features': 10,\n            'small': True,\n            'test_time_pool': True\n        },\n    },\n\n    'dpn68b': {\n        'encoder': DPNEncorder,\n        'out_shapes': (832, 704, 320, 144, 10),\n        'pretrained_settings': pretrained_settings['dpn68b'],\n        'params': {\n            'feature_blocks': (3, 4, 12, 4),\n            'b': True,\n            'groups': 32,\n            'inc_sec': (16, 32, 32, 64),\n            'k_r': 128,\n            'k_sec': (3, 4, 12, 3),\n            'num_classes': 1000,\n            'num_init_features': 10,\n            'small': True,\n            'test_time_pool': True,\n        },\n    },\n\n    'dpn92': {\n        'encoder': DPNEncorder,\n        'out_shapes': (2688, 1552, 704, 336, 64),\n        'pretrained_settings': pretrained_settings['dpn92'],\n        'params': {\n            'feature_blocks': (3, 4, 20, 4),\n            'groups': 32,\n            'inc_sec': (16, 32, 24, 128),\n            'k_r': 96,\n            'k_sec': (3, 4, 20, 3),\n            'num_classes': 1000,\n            'num_init_features': 64,\n            'test_time_pool': True\n        },\n    },\n\n    'dpn98': {\n        'encoder': DPNEncorder,\n        'out_shapes': (2688, 1728, 768, 336, 96),\n        'pretrained_settings': pretrained_settings['dpn98'],\n        'params': {\n            'feature_blocks': (3, 6, 20, 4),\n            'groups': 40,\n            'inc_sec': (16, 32, 32, 128),\n            'k_r': 160,\n            'k_sec': (3, 6, 20, 3),\n            'num_classes': 1000,\n            'num_init_features': 96,\n            'test_time_pool': True,\n        },\n    },\n\n    'dpn107': {\n        'encoder': DPNEncorder,\n        'out_shapes': (2688, 2432, 1152, 376, 128),\n        'pretrained_settings': pretrained_settings['dpn107'],\n        'params': {\n            'feature_blocks': (4, 8, 20, 4),\n            'groups': 50,\n            'inc_sec': (20, 64, 64, 128),\n            'k_r': 200,\n            'k_sec': (4, 8, 20, 3),\n            'num_classes': 1000,\n            'num_init_features': 128,\n            'test_time_pool': True\n        },\n    },\n\n    'dpn131': {\n        'encoder': DPNEncorder,\n        'out_shapes': (2688, 1984, 832, 352, 128),\n        'pretrained_settings': pretrained_settings['dpn131'],\n        'params': {\n            'feature_blocks': (4, 8, 28, 4),\n            'groups': 40,\n            'inc_sec': (16, 32, 32, 128),\n            'k_r': 160,\n            'k_sec': (4, 8, 28, 3),\n            'num_classes': 1000,\n            'num_init_features': 128,\n            'test_time_pool': True\n        },\n    },\n\n}\n"""
mlcomp/contrib/segmentation/encoders/inceptionresnetv2.py,1,"b""import torch.nn as nn\nfrom pretrainedmodels.models.inceptionresnetv2 import InceptionResNetV2\nfrom pretrainedmodels.models.inceptionresnetv2 import pretrained_settings\n\n\nclass InceptionResNetV2Encoder(InceptionResNetV2):\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        self.pretrained = False\n\n        # correct paddings\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                if m.kernel_size == (3, 3):\n                    m.padding = (1, 1)\n            if isinstance(m, nn.MaxPool2d):\n                m.padding = (1, 1)\n\n        # remove linear layers\n        del self.avgpool_1a\n        del self.last_linear\n\n    def forward(self, x):\n        x = self.conv2d_1a(x)\n        x = self.conv2d_2a(x)\n        x = self.conv2d_2b(x)\n        x0 = x\n\n        x = self.maxpool_3a(x)\n        x = self.conv2d_3b(x)\n        x = self.conv2d_4a(x)\n        x1 = x\n\n        x = self.maxpool_5a(x)\n        x = self.mixed_5b(x)\n        x = self.repeat(x)\n        x2 = x\n\n        x = self.mixed_6a(x)\n        x = self.repeat_1(x)\n        x3 = x\n\n        x = self.mixed_7a(x)\n        x = self.repeat_2(x)\n        x = self.block8(x)\n        x = self.conv2d_7b(x)\n        x4 = x\n\n        features = [x4, x3, x2, x1, x0]\n        return features\n\n    def load_state_dict(self, state_dict, **kwargs):\n        state_dict.pop('last_linear.bias')\n        state_dict.pop('last_linear.weight')\n        super().load_state_dict(state_dict, **kwargs)\n\n\ninception_encoders = {\n    'inceptionresnetv2': {\n        'encoder': InceptionResNetV2Encoder,\n        'pretrained_settings': pretrained_settings['inceptionresnetv2'],\n        'out_shapes': (1536, 1088, 320, 192, 64),\n        'params': {\n            'num_classes': 1000,\n        }\n\n    }\n}\n"""
mlcomp/contrib/segmentation/encoders/resnet.py,6,"b""from torchvision.models.resnet import ResNet\nfrom torchvision.models.resnet import BasicBlock\nfrom torchvision.models.resnet import Bottleneck\nfrom pretrainedmodels.models.torchvision_models import pretrained_settings\n\n\nclass ResNetEncoder(ResNet):\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.pretrained = False\n        del self.fc\n\n    def forward(self, x):\n        x0 = self.conv1(x)\n        x0 = self.bn1(x0)\n        x0 = self.relu(x0)\n\n        x1 = self.maxpool(x0)\n        x1 = self.layer1(x1)\n\n        x2 = self.layer2(x1)\n        x3 = self.layer3(x2)\n        x4 = self.layer4(x3)\n\n        return [x4, x3, x2, x1, x0]\n\n    def load_state_dict(self, state_dict, **kwargs):\n        state_dict.pop('fc.bias')\n        state_dict.pop('fc.weight')\n        super().load_state_dict(state_dict, **kwargs)\n\n\nresnet_encoders = {\n    'resnet18': {\n        'encoder': ResNetEncoder,\n        'pretrained_settings': pretrained_settings['resnet18'],\n        'out_shapes': (512, 256, 128, 64, 64),\n        'params': {\n            'block': BasicBlock,\n            'layers': [2, 2, 2, 2],\n        },\n    },\n\n    'resnet34': {\n        'encoder': ResNetEncoder,\n        'pretrained_settings': pretrained_settings['resnet34'],\n        'out_shapes': (512, 256, 128, 64, 64),\n        'params': {\n            'block': BasicBlock,\n            'layers': [3, 4, 6, 3],\n        },\n    },\n\n    'resnet50': {\n        'encoder': ResNetEncoder,\n        'pretrained_settings': pretrained_settings['resnet50'],\n        'out_shapes': (2048, 1024, 512, 256, 64),\n        'params': {\n            'block': Bottleneck,\n            'layers': [3, 4, 6, 3],\n        },\n    },\n\n    'resnet101': {\n        'encoder': ResNetEncoder,\n        'pretrained_settings': pretrained_settings['resnet101'],\n        'out_shapes': (2048, 1024, 512, 256, 64),\n        'params': {\n            'block': Bottleneck,\n            'layers': [3, 4, 23, 3],\n        },\n    },\n\n    'resnet152': {\n        'encoder': ResNetEncoder,\n        'pretrained_settings': pretrained_settings['resnet152'],\n        'out_shapes': (2048, 1024, 512, 256, 64),\n        'params': {\n            'block': Bottleneck,\n            'layers': [3, 8, 36, 3],\n        },\n    },\n\n    'resnext50_32x4d': {\n        'encoder': ResNetEncoder,\n        'pretrained_settings': {\n            'imagenet': {\n                'url': 'https://download.pytorch.org/'\n                       'models/resnext50_32x4d-7cdf4587.pth',\n                'input_space': 'RGB',\n                'input_size': [3, 224, 224],\n                'input_range': [0, 1],\n                'mean': [0.485, 0.456, 0.406],\n                'std': [0.229, 0.224, 0.225],\n                'num_classes': 1000\n            }\n        },\n        'out_shapes': (2048, 1024, 512, 256, 64),\n        'params': {\n            'block': Bottleneck,\n            'layers': [3, 4, 6, 3],\n            'groups': 32,\n            'width_per_group': 4\n        },\n    },\n\n    'resnext101_32x8d': {\n        'encoder': ResNetEncoder,\n        'pretrained_settings': {\n            'imagenet': {\n                'url': 'https://download.pytorch.org/'\n                       'models/resnext101_32x8d-8ba56ff5.pth',\n                'input_space': 'RGB',\n                'input_size': [3, 224, 224],\n                'input_range': [0, 1],\n                'mean': [0.485, 0.456, 0.406],\n                'std': [0.229, 0.224, 0.225],\n                'num_classes': 1000\n            },\n            'instagram': {\n                'url': 'https://download.pytorch.org/'\n                       'models/ig_resnext101_32x8-c38310e5.pth',\n                'input_space': 'RGB',\n                'input_size': [3, 224, 224],\n                'input_range': [0, 1],\n                'mean': [0.485, 0.456, 0.406],\n                'std': [0.229, 0.224, 0.225],\n                'num_classes': 1000\n            }\n        },\n        'out_shapes': (2048, 1024, 512, 256, 64),\n        'params': {\n            'block': Bottleneck,\n            'layers': [3, 4, 23, 3],\n            'groups': 32,\n            'width_per_group': 8\n        },\n    },\n\n    'resnext101_32x16d': {\n        'encoder': ResNetEncoder,\n        'pretrained_settings': {\n            'instagram': {\n                'url': 'https://download.pytorch.org/'\n                       'models/ig_resnext101_32x16-c6f796b0.pth',\n                'input_space': 'RGB',\n                'input_size': [3, 224, 224],\n                'input_range': [0, 1],\n                'mean': [0.485, 0.456, 0.406],\n                'std': [0.229, 0.224, 0.225],\n                'num_classes': 1000\n            }\n        },\n        'out_shapes': (2048, 1024, 512, 256, 64),\n        'params': {\n            'block': Bottleneck,\n            'layers': [3, 4, 23, 3],\n            'groups': 32,\n            'width_per_group': 16\n        },\n    },\n\n    'resnext101_32x32d': {\n        'encoder': ResNetEncoder,\n        'pretrained_settings': {\n            'instagram': {\n                'url': 'https://download.pytorch.org/'\n                       'models/ig_resnext101_32x32-e4b90b00.pth',\n                'input_space': 'RGB',\n                'input_size': [3, 224, 224],\n                'input_range': [0, 1],\n                'mean': [0.485, 0.456, 0.406],\n                'std': [0.229, 0.224, 0.225],\n                'num_classes': 1000\n            }\n        },\n        'out_shapes': (2048, 1024, 512, 256, 64),\n        'params': {\n            'block': Bottleneck,\n            'layers': [3, 4, 23, 3],\n            'groups': 32,\n            'width_per_group': 32\n        },\n    },\n\n    'resnext101_32x48d': {\n        'encoder': ResNetEncoder,\n        'pretrained_settings': {\n            'instagram': {\n                'url': 'https://download.pytorch.org/'\n                       'models/ig_resnext101_32x48-3e41cc8a.pth',\n                'input_space': 'RGB',\n                'input_size': [3, 224, 224],\n                'input_range': [0, 1],\n                'mean': [0.485, 0.456, 0.406],\n                'std': [0.229, 0.224, 0.225],\n                'num_classes': 1000\n            }\n        },\n        'out_shapes': (2048, 1024, 512, 256, 64),\n        'params': {\n            'block': Bottleneck,\n            'layers': [3, 4, 23, 3],\n            'groups': 32,\n            'width_per_group': 48\n        },\n    },\n}\n"""
mlcomp/contrib/segmentation/encoders/senet.py,0,"b""from pretrainedmodels.models.senet import SENet\nfrom pretrainedmodels.models.senet import SEBottleneck\nfrom pretrainedmodels.models.senet import SEResNetBottleneck\nfrom pretrainedmodels.models.senet import SEResNeXtBottleneck\nfrom pretrainedmodels.models.senet import pretrained_settings\n\n\nclass SENetEncoder(SENet):\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.pretrained = False\n\n        del self.last_linear\n        del self.avg_pool\n\n    def forward(self, x):\n        for module in self.layer0[:-1]:\n            x = module(x)\n\n        x0 = x\n        x = self.layer0[-1](x)\n        x1 = self.layer1(x)\n        x2 = self.layer2(x1)\n        x3 = self.layer3(x2)\n        x4 = self.layer4(x3)\n\n        features = [x4, x3, x2, x1, x0]\n        return features\n\n    def load_state_dict(self, state_dict, **kwargs):\n        state_dict.pop('last_linear.bias')\n        state_dict.pop('last_linear.weight')\n        super().load_state_dict(state_dict, **kwargs)\n\n\nsenet_encoders = {\n    'senet154': {\n        'encoder': SENetEncoder,\n        'pretrained_settings': pretrained_settings['senet154'],\n        'out_shapes': (2048, 1024, 512, 256, 128),\n        'params': {\n            'block': SEBottleneck,\n            'dropout_p': 0.2,\n            'groups': 64,\n            'layers': [3, 8, 36, 3],\n            'num_classes': 1000,\n            'reduction': 16\n        },\n    },\n\n    'se_resnet50': {\n        'encoder': SENetEncoder,\n        'pretrained_settings': pretrained_settings['se_resnet50'],\n        'out_shapes': (2048, 1024, 512, 256, 64),\n        'params': {\n            'block': SEResNetBottleneck,\n            'layers': [3, 4, 6, 3],\n            'downsample_kernel_size': 1,\n            'downsample_padding': 0,\n            'dropout_p': None,\n            'groups': 1,\n            'inplanes': 64,\n            'input_3x3': False,\n            'num_classes': 1000,\n            'reduction': 16\n        },\n    },\n\n    'se_resnet101': {\n        'encoder': SENetEncoder,\n        'pretrained_settings': pretrained_settings['se_resnet101'],\n        'out_shapes': (2048, 1024, 512, 256, 64),\n        'params': {\n            'block': SEResNetBottleneck,\n            'layers': [3, 4, 23, 3],\n            'downsample_kernel_size': 1,\n            'downsample_padding': 0,\n            'dropout_p': None,\n            'groups': 1,\n            'inplanes': 64,\n            'input_3x3': False,\n            'num_classes': 1000,\n            'reduction': 16\n        },\n    },\n\n    'se_resnet152': {\n        'encoder': SENetEncoder,\n        'pretrained_settings': pretrained_settings['se_resnet152'],\n        'out_shapes': (2048, 1024, 512, 256, 64),\n        'params': {\n            'block': SEResNetBottleneck,\n            'layers': [3, 8, 36, 3],\n            'downsample_kernel_size': 1,\n            'downsample_padding': 0,\n            'dropout_p': None,\n            'groups': 1,\n            'inplanes': 64,\n            'input_3x3': False,\n            'num_classes': 1000,\n            'reduction': 16\n        },\n    },\n\n    'se_resnext50_32x4d': {\n        'encoder': SENetEncoder,\n        'pretrained_settings': pretrained_settings['se_resnext50_32x4d'],\n        'out_shapes': (2048, 1024, 512, 256, 64),\n        'params': {\n            'block': SEResNeXtBottleneck,\n            'layers': [3, 4, 6, 3],\n            'downsample_kernel_size': 1,\n            'downsample_padding': 0,\n            'dropout_p': None,\n            'groups': 32,\n            'inplanes': 64,\n            'input_3x3': False,\n            'num_classes': 1000,\n            'reduction': 16\n        },\n    },\n\n    'se_resnext101_32x4d': {\n        'encoder': SENetEncoder,\n        'pretrained_settings': pretrained_settings['se_resnext101_32x4d'],\n        'out_shapes': (2048, 1024, 512, 256, 64),\n        'params': {\n            'block': SEResNeXtBottleneck,\n            'layers': [3, 4, 23, 3],\n            'downsample_kernel_size': 1,\n            'downsample_padding': 0,\n            'dropout_p': None,\n            'groups': 32,\n            'inplanes': 64,\n            'input_3x3': False,\n            'num_classes': 1000,\n            'reduction': 16\n        },\n    },\n}\n"""
mlcomp/contrib/segmentation/encoders/vgg.py,1,"b""import torch.nn as nn\nfrom torchvision.models.vgg import VGG\nfrom torchvision.models.vgg import make_layers\nfrom pretrainedmodels.models.torchvision_models import pretrained_settings\n\ncfg = {\n    'A': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n    'B': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512,\n          'M'],\n    'D': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M',\n          512, 512, 512, 'M'],\n    'E': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512,\n          512, 'M', 512, 512, 512, 512, 'M'],\n}\n\n\nclass VGGEncoder(VGG):\n\n    def __init__(self, config, batch_norm=False, *args, **kwargs):\n        super().__init__(\n            make_layers(config, batch_norm=batch_norm),\n            *args,\n            **kwargs\n        )\n        self.pretrained = False\n        del self.classifier\n\n    def forward(self, x):\n        features = []\n        for module in self.features:\n            if isinstance(module, nn.MaxPool2d):\n                features.append(x)\n            x = module(x)\n        features.append(x)\n\n        features = features[1:]\n        features = features[::-1]\n        return features\n\n    def load_state_dict(self, state_dict, **kwargs):\n        keys = list(state_dict.keys())\n        for k in keys:\n            if k.startswith('classifier'):\n                state_dict.pop(k)\n        super().load_state_dict(state_dict, **kwargs)\n\n\nvgg_encoders = {\n\n    'vgg11': {\n        'encoder': VGGEncoder,\n        'out_shapes': (512, 512, 512, 256, 128),\n        'pretrained_settings': pretrained_settings['vgg11'],\n        'params': {\n            'config': cfg['A'],\n            'batch_norm': False,\n        },\n    },\n\n    'vgg11_bn': {\n        'encoder': VGGEncoder,\n        'out_shapes': (512, 512, 512, 256, 128),\n        'pretrained_settings': pretrained_settings['vgg11_bn'],\n        'params': {\n            'config': cfg['A'],\n            'batch_norm': True,\n        },\n    },\n\n    'vgg13': {\n        'encoder': VGGEncoder,\n        'out_shapes': (512, 512, 512, 256, 128),\n        'pretrained_settings': pretrained_settings['vgg13'],\n        'params': {\n            'config': cfg['B'],\n            'batch_norm': False,\n        },\n    },\n\n    'vgg13_bn': {\n        'encoder': VGGEncoder,\n        'out_shapes': (512, 512, 512, 256, 128),\n        'pretrained_settings': pretrained_settings['vgg13_bn'],\n        'params': {\n            'config': cfg['B'],\n            'batch_norm': True,\n        },\n    },\n\n    'vgg16': {\n        'encoder': VGGEncoder,\n        'out_shapes': (512, 512, 512, 256, 128),\n        'pretrained_settings': pretrained_settings['vgg16'],\n        'params': {\n            'config': cfg['D'],\n            'batch_norm': False,\n        },\n    },\n\n    'vgg16_bn': {\n        'encoder': VGGEncoder,\n        'out_shapes': (512, 512, 512, 256, 128),\n        'pretrained_settings': pretrained_settings['vgg16_bn'],\n        'params': {\n            'config': cfg['D'],\n            'batch_norm': True,\n        },\n    },\n\n    'vgg19': {\n        'encoder': VGGEncoder,\n        'out_shapes': (512, 512, 512, 256, 128),\n        'pretrained_settings': pretrained_settings['vgg19'],\n        'params': {\n            'config': cfg['E'],\n            'batch_norm': False,\n        },\n    },\n\n    'vgg19_bn': {\n        'encoder': VGGEncoder,\n        'out_shapes': (512, 512, 512, 256, 128),\n        'pretrained_settings': pretrained_settings['vgg19_bn'],\n        'params': {\n            'config': cfg['E'],\n            'batch_norm': True,\n        },\n    },\n}\n"""
mlcomp/contrib/segmentation/fpn/__init__.py,0,b'# flake8: noqa\nfrom .model import FPN'
mlcomp/contrib/segmentation/fpn/decoder.py,2,"b""import torch.nn as nn\nimport torch.nn.functional as F\n\nfrom ..base.model import Model\n\n\nclass Conv3x3GNReLU(nn.Module):\n    def __init__(self, in_channels, out_channels, upsample=False):\n        super().__init__()\n        self.upsample = upsample\n        self.block = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, (3, 3),\n                      stride=1, padding=1, bias=False),\n            nn.GroupNorm(32, out_channels),\n            nn.ReLU(inplace=True),\n        )\n\n    def forward(self, x):\n        x = self.block(x)\n        if self.upsample:\n            x = F.interpolate(x, scale_factor=2, mode='bilinear',\n                              align_corners=True)\n        return x\n\n\nclass FPNBlock(nn.Module):\n    def __init__(self, pyramid_channels, skip_channels):\n        super().__init__()\n        self.skip_conv = nn.Conv2d(skip_channels, pyramid_channels,\n                                   kernel_size=1)\n\n    def forward(self, x):\n        x, skip = x\n\n        x = F.interpolate(x, scale_factor=2, mode='nearest')\n        skip = self.skip_conv(skip)\n\n        x = x + skip\n        return x\n\n\nclass SegmentationBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, n_upsamples=0):\n        super().__init__()\n\n        blocks = [\n            Conv3x3GNReLU(in_channels, out_channels,\n                          upsample=bool(n_upsamples))\n        ]\n\n        if n_upsamples > 1:\n            for _ in range(1, n_upsamples):\n                blocks.append(\n                    Conv3x3GNReLU(out_channels, out_channels, upsample=True))\n\n        self.block = nn.Sequential(*blocks)\n\n    def forward(self, x):\n        return self.block(x)\n\n\nclass FPNDecoder(Model):\n\n    def __init__(\n            self,\n            encoder_channels,\n            pyramid_channels=256,\n            segmentation_channels=128,\n            final_channels=1,\n            dropout=0.2,\n    ):\n        super().__init__()\n\n        self.conv1 = nn.Conv2d(encoder_channels[0], pyramid_channels,\n                               kernel_size=(1, 1))\n\n        self.p4 = FPNBlock(pyramid_channels, encoder_channels[1])\n        self.p3 = FPNBlock(pyramid_channels, encoder_channels[2])\n        self.p2 = FPNBlock(pyramid_channels, encoder_channels[3])\n\n        self.s5 = SegmentationBlock(pyramid_channels, segmentation_channels,\n                                    n_upsamples=3)\n        self.s4 = SegmentationBlock(pyramid_channels, segmentation_channels,\n                                    n_upsamples=2)\n        self.s3 = SegmentationBlock(pyramid_channels, segmentation_channels,\n                                    n_upsamples=1)\n        self.s2 = SegmentationBlock(pyramid_channels, segmentation_channels,\n                                    n_upsamples=0)\n\n        self.dropout = nn.Dropout2d(p=dropout, inplace=True)\n        self.final_conv = nn.Conv2d(segmentation_channels, final_channels,\n                                    kernel_size=1, padding=0)\n\n        self.initialize()\n\n    def forward(self, x):\n        c5, c4, c3, c2, _ = x\n\n        p5 = self.conv1(c5)\n        p4 = self.p4([p5, c4])\n        p3 = self.p3([p4, c3])\n        p2 = self.p2([p3, c2])\n\n        s5 = self.s5(p5)\n        s4 = self.s4(p4)\n        s3 = self.s3(p3)\n        s2 = self.s2(p2)\n\n        x = s5 + s4 + s3 + s2\n\n        x = self.dropout(x)\n        x = self.final_conv(x)\n\n        x = F.interpolate(x, scale_factor=4, mode='bilinear',\n                          align_corners=True)\n        return x\n"""
mlcomp/contrib/segmentation/fpn/model.py,1,"b'from .decoder import FPNDecoder\nfrom ..base import EncoderDecoder\nfrom ..encoders import get_encoder\n\n\nclass FPN(EncoderDecoder):\n    """"""FPN_ is a fully convolution neural\n    network for image semantic segmentation\n    Args:\n        encoder_name: name of classification model\n         (without last dense layers) used as feature\n                extractor to build segmentation model.\n        encoder_weights: one of ``None`` (random initialization),\n        ``imagenet`` (pre-training on ImageNet).\n        decoder_pyramid_channels: a number of convolution\n        filters in Feature Pyramid of FPN_.\n        decoder_segmentation_channels: a number of\n        convolution filters in segmentation head of FPN_.\n        classes: a number of classes for output\n        (output shape - ``(batch, classes, h, w)``).\n        dropout: spatial dropout rate in range (0, 1).\n        activation: activation function used\n        in ``.predict(x)`` method for inference.\n            One of [``sigmoid``, ``softmax``, callable, None]\n\n    Returns:\n        ``torch.nn.Module``: **FPN**\n\n    .. _FPN:\n        http://presentations.cocodataset.org/COCO17-Stuff-FAIR.pdf\n\n    """"""\n\n    def __init__(\n            self,\n            encoder_name=\'resnet34\',\n            encoder_weights=\'imagenet\',\n            decoder_pyramid_channels=256,\n            decoder_segmentation_channels=128,\n            classes=1,\n            dropout=0.2,\n            activation=\'sigmoid\',\n    ):\n        encoder = get_encoder(\n            encoder_name,\n            encoder_weights=encoder_weights\n        )\n\n        decoder = FPNDecoder(\n            encoder_channels=encoder.out_shapes,\n            pyramid_channels=decoder_pyramid_channels,\n            segmentation_channels=decoder_segmentation_channels,\n            final_channels=classes,\n            dropout=dropout,\n        )\n\n        super().__init__(encoder, decoder, activation)\n\n        self.name = \'fpn-{}\'.format(encoder_name)\n'"
mlcomp/contrib/segmentation/linknet/__init__.py,0,b'# flake8: noqa\nfrom .model import Linknet'
mlcomp/contrib/segmentation/linknet/decoder.py,1,"b'import torch.nn as nn\n\nfrom ..common.blocks import Conv2dReLU\nfrom ..base.model import Model\n\n\nclass TransposeX2(nn.Module):\n\n    def __init__(self, in_channels, out_channels, use_batchnorm=True,\n                 **batchnorm_params):\n        super().__init__()\n        layers = []\n        layers.append(\n            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=4,\n                               stride=2, padding=1))\n        if use_batchnorm:\n            layers.append(nn.BatchNorm2d(out_channels, **batchnorm_params))\n        layers.append(nn.ReLU(inplace=True))\n\n        self.block = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.block(x)\n\n\nclass DecoderBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, use_batchnorm=True):\n        super().__init__()\n\n        self.block = nn.Sequential(\n            Conv2dReLU(in_channels, in_channels // 4, kernel_size=1,\n                       use_batchnorm=use_batchnorm),\n            TransposeX2(in_channels // 4, in_channels // 4,\n                        use_batchnorm=use_batchnorm),\n            Conv2dReLU(in_channels // 4, out_channels, kernel_size=1,\n                       use_batchnorm=use_batchnorm),\n        )\n\n    def forward(self, x):\n        x, skip = x\n        x = self.block(x)\n        if skip is not None:\n            x = x + skip\n        return x\n\n\nclass LinknetDecoder(Model):\n\n    def __init__(\n            self,\n            encoder_channels,\n            prefinal_channels=32,\n            final_channels=1,\n            use_batchnorm=True,\n    ):\n        super().__init__()\n\n        in_channels = encoder_channels\n\n        self.layer1 = DecoderBlock(in_channels[0], in_channels[1],\n                                   use_batchnorm=use_batchnorm)\n        self.layer2 = DecoderBlock(in_channels[1], in_channels[2],\n                                   use_batchnorm=use_batchnorm)\n        self.layer3 = DecoderBlock(in_channels[2], in_channels[3],\n                                   use_batchnorm=use_batchnorm)\n        self.layer4 = DecoderBlock(in_channels[3], in_channels[4],\n                                   use_batchnorm=use_batchnorm)\n        self.layer5 = DecoderBlock(in_channels[4], prefinal_channels,\n                                   use_batchnorm=use_batchnorm)\n        self.final_conv = nn.Conv2d(prefinal_channels, final_channels,\n                                    kernel_size=(1, 1))\n\n        self.initialize()\n\n    def forward(self, x):\n        encoder_head = x[0]\n        skips = x[1:]\n\n        x = self.layer1([encoder_head, skips[0]])\n        x = self.layer2([x, skips[1]])\n        x = self.layer3([x, skips[2]])\n        x = self.layer4([x, skips[3]])\n        x = self.layer5([x, None])\n        x = self.final_conv(x)\n\n        return x\n'"
mlcomp/contrib/segmentation/linknet/model.py,1,"b'from .decoder import LinknetDecoder\nfrom ..base import EncoderDecoder\nfrom ..encoders import get_encoder\n\n\nclass Linknet(EncoderDecoder):\n    """"""Linknet_ is a fully convolution neural\n    network for fast image semantic segmentation\n\n    Note:\n        This implementation by default has 4 skip connections (original - 3).\n\n    Args:\n        encoder_name: name of classification model\n        (without last dense layers) used as feature\n            extractor to build segmentation model.\n        encoder_weights: one of ``None`` (random initialization),\n         ``imagenet`` (pre-training on ImageNet).\n        decoder_use_batchnorm: if ``True``, ``BatchNormalisation``\n         layer between ``Conv2D`` and ``Activation`` layers\n            is used.\n        classes: a number of classes for output\n         (output shape - ``(batch, classes, h, w)``).\n        activation: activation function used in\n        ``.predict(x)`` method for inference.\n            One of [``sigmoid``, ``softmax``, callable, None]\n\n    Returns:\n        ``torch.nn.Module``: **Linknet**\n\n    .. _Linknet:\n        https://arxiv.org/pdf/1707.03718.pdf\n    """"""\n\n    def __init__(\n            self,\n            encoder_name=\'resnet34\',\n            encoder_weights=\'imagenet\',\n            decoder_use_batchnorm=True,\n            classes=1,\n            activation=\'sigmoid\',\n    ):\n        encoder = get_encoder(\n            encoder_name,\n            encoder_weights=encoder_weights\n        )\n\n        decoder = LinknetDecoder(\n            encoder_channels=encoder.out_shapes,\n            prefinal_channels=32,\n            final_channels=classes,\n            use_batchnorm=decoder_use_batchnorm,\n        )\n\n        super().__init__(encoder, decoder, activation)\n\n        self.name = \'link-{}\'.format(encoder_name)\n'"
mlcomp/contrib/segmentation/pspnet/__init__.py,0,b'# flake8: noqa\nfrom .model import PSPNet'
mlcomp/contrib/segmentation/pspnet/decoder.py,3,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom ..base.model import Model\nfrom ..common.blocks import Conv2dReLU\n\n\ndef _upsample(x, size):\n    return F.interpolate(x, size=size, mode='bilinear', align_corners=True)\n\n\nclass PyramidStage(nn.Module):\n\n    def __init__(self, in_channels, out_channels, pool_size,\n                 use_bathcnorm=True):\n        super().__init__()\n        if pool_size == 1:\n            use_bathcnorm = False\n        self.pool = nn.Sequential(\n            nn.AdaptiveAvgPool2d(output_size=(pool_size, pool_size)),\n            Conv2dReLU(in_channels, out_channels, (1, 1),\n                       use_batchnorm=use_bathcnorm)\n        )\n\n    def forward(self, x):\n        h, w = x.size(2), x.size(3)\n        x = self.pool(x)\n        x = _upsample(x, size=(h, w))\n        return x\n\n\nclass PSPModule(nn.Module):\n    def __init__(self, in_channels, sizes=(1, 2, 3, 6), use_bathcnorm=True):\n        super().__init__()\n\n        self.stages = nn.ModuleList([\n            PyramidStage(in_channels, in_channels // len(sizes), size,\n                         use_bathcnorm=use_bathcnorm) for size in sizes\n        ])\n\n    def forward(self, x):\n        xs = [stage(x) for stage in self.stages] + [x]\n        x = torch.cat(xs, dim=1)\n        return x\n\n\nclass AUXModule(nn.Module):\n    def __init__(self, in_features, out_features):\n        super().__init__()\n        self.linear = nn.Linear(in_features, out_features)\n\n    def forward(self, x):\n        x = F.adaptive_max_pool2d(x, output_size=(1, 1))\n        x = x.view(-1, x.size(1))\n        x = self.linear(x)\n        return x\n\n\nclass PSPDecoder(Model):\n\n    def __init__(\n            self,\n            encoder_channels,\n            downsample_factor=8,\n            use_batchnorm=True,\n            psp_out_channels=512,\n            final_channels=21,\n            aux_output=False,\n            dropout=0.2,\n    ):\n        super().__init__()\n        self.downsample_factor = downsample_factor\n        self.out_channels = self._get(encoder_channels)\n        self.aux_output = aux_output\n        self.dropout_factor = dropout\n\n        self.psp = PSPModule(\n            self.out_channels,\n            sizes=(1, 2, 3, 6),\n            use_bathcnorm=use_batchnorm,\n        )\n\n        self.conv = Conv2dReLU(\n            self.out_channels * 2,\n            psp_out_channels,\n            kernel_size=1,\n            use_batchnorm=use_batchnorm,\n        )\n\n        if self.dropout_factor:\n            self.dropout = nn.Dropout2d(p=dropout)\n\n        self.final_conv = nn.Conv2d(psp_out_channels, final_channels,\n                                    kernel_size=(3, 3), padding=1)\n\n        if self.aux_output:\n            self.aux = AUXModule(self.out_channels, final_channels)\n\n        self.initialize()\n\n    def _get(self, xs):\n        if self.downsample_factor == 4:\n            return xs[3]\n        elif self.downsample_factor == 8:\n            return xs[2]\n        elif self.downsample_factor == 16:\n            return xs[1]\n        else:\n            raise ValueError(\n                'Downsample factor should bi in [4, 8, 16], got {}'\n                .format(self.downsample_factor))\n\n    def forward(self, x):\n\n        features = self._get(x)\n        x = self.psp(features)\n        x = self.conv(x)\n        if self.dropout_factor:\n            x = self.dropout(x)\n        x = self.final_conv(x)\n        x = F.interpolate(\n            x,\n            scale_factor=self.downsample_factor,\n            mode='bilinear',\n            align_corners=True\n        )\n\n        if self.training and self.aux_output:\n            aux = self.aux(features)\n            x = [x, aux]\n\n        return x\n"""
mlcomp/contrib/segmentation/pspnet/model.py,1,"b'from .decoder import PSPDecoder\nfrom ..base import EncoderDecoder\nfrom ..encoders import get_encoder\n\n\nclass PSPNet(EncoderDecoder):\n    """"""PSPNet_ is a fully convolution\n     neural network for image semantic segmentation\n\n    Args:\n        encoder_name: name of classification model used as feature\n                extractor to build segmentation model.\n        encoder_weights: one of ``None`` (random initialization),\n         ``imagenet`` (pre-training on ImageNet).\n        psp_in_factor: one of 4, 8 and 16.\n        Downsampling rate or in other words backbone depth\n            to construct PSP module on it.\n        psp_out_channels: number of filters in PSP block.\n        psp_use_batchnorm: if ``True``, ``BatchNormalisation``\n        layer between ``Conv2D`` and ``Activation`` layers\n                is used.\n        psp_aux_output: if ``True`` add auxiliary\n        classification output for encoder training\n        psp_dropout: spatial dropout rate between 0 and 1.\n        classes: a number of classes for output\n        (output shape - ``(batch, classes, h, w)``).\n        activation: activation function\n        used in ``.predict(x)`` method for inference.\n            One of [``sigmoid``, ``softmax``, callable, None]\n\n    Returns:\n        ``torch.nn.Module``: **PSPNet**\n\n    .. _PSPNet:\n        https://arxiv.org/pdf/1612.01105.pdf\n    """"""\n\n    def __init__(\n            self,\n            encoder_name=\'resnet34\',\n            encoder_weights=\'imagenet\',\n            psp_in_factor=8,\n            psp_out_channels=512,\n            psp_use_batchnorm=True,\n            psp_aux_output=False,\n            classes=21,\n            dropout=0.2,\n            activation=\'softmax\',\n    ):\n        encoder = get_encoder(encoder_name, encoder_weights=encoder_weights)\n\n        decoder = PSPDecoder(\n            encoder_channels=encoder.out_shapes,\n            downsample_factor=psp_in_factor,\n            psp_out_channels=psp_out_channels,\n            final_channels=classes,\n            dropout=dropout,\n            aux_output=psp_aux_output,\n            use_batchnorm=psp_use_batchnorm,\n        )\n\n        super().__init__(encoder, decoder, activation)\n\n        self.name = \'psp-{}\'.format(encoder_name)\n'"
mlcomp/contrib/segmentation/unet/__init__.py,0,b'# flake8: noqa\nfrom .model import Unet'
mlcomp/contrib/segmentation/unet/decoder.py,3,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom ..common.blocks import Conv2dReLU, SCSEModule\nfrom ..base.model import Model\n\n\nclass DecoderBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, use_batchnorm=True,\n                 attention_type=None):\n        super().__init__()\n        if attention_type is None:\n            self.attention1 = nn.Identity()\n            self.attention2 = nn.Identity()\n        elif attention_type == 'scse':\n            self.attention1 = SCSEModule(in_channels)\n            self.attention2 = SCSEModule(out_channels)\n\n        self.block = nn.Sequential(\n            Conv2dReLU(in_channels, out_channels, kernel_size=3, padding=1,\n                       use_batchnorm=use_batchnorm),\n            Conv2dReLU(out_channels, out_channels, kernel_size=3, padding=1,\n                       use_batchnorm=use_batchnorm),\n        )\n\n    def forward(self, x):\n        x, skip = x\n        x = F.interpolate(x, scale_factor=2, mode='nearest')\n        if skip is not None:\n            x = torch.cat([x, skip], dim=1)\n            x = self.attention1(x)\n\n        x = self.block(x)\n        x = self.attention2(x)\n        return x\n\n\nclass CenterBlock(DecoderBlock):\n\n    def forward(self, x):\n        return self.block(x)\n\n\nclass UnetDecoder(Model):\n\n    def __init__(\n            self,\n            encoder_channels,\n            decoder_channels=(256, 128, 64, 32, 16),\n            final_channels=1,\n            use_batchnorm=True,\n            center=False,\n            attention_type=None\n    ):\n        super().__init__()\n\n        if center:\n            channels = encoder_channels[0]\n            self.center = CenterBlock(channels, channels,\n                                      use_batchnorm=use_batchnorm)\n        else:\n            self.center = None\n\n        in_channels = self.compute_channels(encoder_channels, decoder_channels)\n        out_channels = decoder_channels\n\n        self.layer1 = DecoderBlock(in_channels[0], out_channels[0],\n                                   use_batchnorm=use_batchnorm,\n                                   attention_type=attention_type)\n        self.layer2 = DecoderBlock(in_channels[1], out_channels[1],\n                                   use_batchnorm=use_batchnorm,\n                                   attention_type=attention_type)\n        self.layer3 = DecoderBlock(in_channels[2], out_channels[2],\n                                   use_batchnorm=use_batchnorm,\n                                   attention_type=attention_type)\n        self.layer4 = DecoderBlock(in_channels[3], out_channels[3],\n                                   use_batchnorm=use_batchnorm,\n                                   attention_type=attention_type)\n        self.layer5 = DecoderBlock(in_channels[4], out_channels[4],\n                                   use_batchnorm=use_batchnorm,\n                                   attention_type=attention_type)\n        self.final_conv = nn.Conv2d(out_channels[4], final_channels,\n                                    kernel_size=(1, 1))\n\n        self.initialize()\n\n    def compute_channels(self, encoder_channels, decoder_channels):\n        channels = [\n            encoder_channels[0] + encoder_channels[1],\n            encoder_channels[2] + decoder_channels[0],\n            encoder_channels[3] + decoder_channels[1],\n            encoder_channels[4] + decoder_channels[2],\n            0 + decoder_channels[3],\n        ]\n        return channels\n\n    def forward(self, x):\n        encoder_head = x[0]\n        skips = x[1:]\n\n        if self.center:\n            encoder_head = self.center(encoder_head)\n\n        x = self.layer1([encoder_head, skips[0]])\n        x = self.layer2([x, skips[1]])\n        x = self.layer3([x, skips[2]])\n        x = self.layer4([x, skips[3]])\n        x = self.layer5([x, None])\n        x = self.final_conv(x)\n\n        return x\n"""
mlcomp/contrib/segmentation/unet/model.py,1,"b'from .decoder import UnetDecoder\nfrom ..base import EncoderDecoder\nfrom ..encoders import get_encoder\n\n\nclass Unet(EncoderDecoder):\n    """"""Unet_ is a fully convolution neural\n    network for image semantic segmentation\n    Args:\n        encoder_name: name of classification model\n        (without last dense layers) used as feature\n            extractor to build segmentation model.\n        encoder_weights: one of ``None`` (random initialization),\n        ``imagenet`` (pre-training on ImageNet).\n        decoder_channels: list of numbers of ``Conv2D``\n         layer filters in decoder blocks\n        decoder_use_batchnorm: if ``True``, ``BatchNormalisation``\n         layer between ``Conv2D`` and ``Activation`` layers\n            is used.\n        classes: a number of classes for output\n         (output shape - ``(batch, classes, h, w)``).\n        activation: activation function used\n        in ``.predict(x)`` method for inference.\n            One of [``sigmoid``, ``softmax``, callable, None]\n        center: if ``True`` add ``Conv2dReLU``\n        block on encoder head (useful for VGG models)\n        attention_type: attention module used in decoder of the model\n            One of [``None``, ``scse``]\n    Returns:\n        ``torch.nn.Module``: **Unet**\n    .. _Unet:\n        https://arxiv.org/pdf/1505.04597\n    """"""\n\n    def __init__(\n            self,\n            encoder_name=\'resnet34\',\n            encoder_weights=\'imagenet\',\n            decoder_use_batchnorm=True,\n            decoder_channels=(256, 128, 64, 32, 16),\n            classes=1,\n            activation=\'sigmoid\',\n            center=False,  # usefull for VGG models\n            attention_type=None\n    ):\n        encoder = get_encoder(\n            encoder_name,\n            encoder_weights=encoder_weights\n        )\n\n        decoder = UnetDecoder(\n            encoder_channels=encoder.out_shapes,\n            decoder_channels=decoder_channels,\n            final_channels=classes,\n            use_batchnorm=decoder_use_batchnorm,\n            center=center,\n            attention_type=attention_type\n        )\n\n        super().__init__(encoder, decoder, activation)\n\n        self.name = \'u-{}\'.format(encoder_name)\n'"
mlcomp/db/providers/report/__init__.py,0,"b""from .img import ReportImgProvider\nfrom .report import ReportProvider\nfrom .layout import ReportLayoutProvider\nfrom .series import ReportSeriesProvider\nfrom .task import ReportTasksProvider\n\n__all__ = [\n    'ReportImgProvider', 'ReportProvider', 'ReportLayoutProvider',\n    'ReportSeriesProvider', 'ReportTasksProvider'\n]\n"""
mlcomp/db/providers/report/img.py,0,"b""import base64\nimport pickle\n\nimport cv2\nimport numpy as np\nfrom sqlalchemy import and_, or_\n\nfrom mlcomp.db.core import PaginatorOptions\nfrom mlcomp.db.models import Project, Dag, ReportImg, Task\nfrom mlcomp.db.providers.base import BaseDataProvider\nfrom mlcomp.utils.img import resize_saving_ratio\nfrom mlcomp.utils.io import yaml_load\n\n\nclass ReportImgProvider(BaseDataProvider):\n    model = ReportImg\n\n    def remove(self, filter: dict):\n        query = self.query(ReportImg)\n        if filter.get('dag'):\n            query = query.filter(ReportImg.dag == filter['dag'])\n        if filter.get('project'):\n            query = query.filter(ReportImg.project == filter['project'])\n        query.delete(synchronize_session=False)\n        self.session.commit()\n\n        query = self.query(Dag)\n        if filter.get('dag'):\n            query.filter(Dag.id == filter['dag']).update({'img_size': 0})\n\n        if filter.get('project'):\n            query.filter(Dag.project == filter['project']\n                         ).update({'img_size': 0})\n\n        self.session.commit()\n\n    def remove_lower(self, task_id: int, name: str, epoch: int):\n        self.query(ReportImg).filter(ReportImg.task == task_id). \\\n            filter(ReportImg.group == name). \\\n            filter(ReportImg.epoch < epoch). \\\n            delete(synchronize_session=False)\n        self.session.commit()\n\n    def detail_img_classify(\n        self, filter: dict, options: PaginatorOptions = None\n    ):\n        res = {'data': []}\n        confusion = self.query(ReportImg.img). \\\n            filter(ReportImg.task == filter['task']). \\\n            filter(ReportImg.group == filter['group'] + '_confusion').first()\n\n        if confusion:\n            confusion = pickle.loads(confusion[0])['data']\n            res['confusion'] = {'data': confusion.tolist()}\n\n        res.update(filter)\n\n        query = self.query(ReportImg).filter(\n            ReportImg.task == filter['task']). \\\n            filter(ReportImg.group == filter['group'])\n\n        if filter.get('y') is not None and filter.get('y_pred') is not None:\n            query = query.filter(\n                and_(\n                    ReportImg.y == filter['y'],\n                    ReportImg.y_pred == filter['y_pred']\n                )\n            )\n\n        if filter.get('score_min') is not None:\n            query = query.filter(\n                or_(\n                    ReportImg.score >= filter['score_min'],\n                    ReportImg.score.__eq__(None)\n                )\n            )\n\n        if filter.get('score_max') is not None:\n            query = query.filter(\n                or_(\n                    ReportImg.score <= filter['score_max'],\n                    ReportImg.score.__eq__(None)\n                )\n            )\n\n        layout = filter.get('layout')\n\n        if layout and layout.get('attrs'):\n            for attr in layout['attrs']:\n                field = getattr(ReportImg, attr['source'])\n                if attr.get('equal') is not None:\n                    query = query.filter(field == attr['equal'])\n                if attr.get('greater') is not None:\n                    query = query.filter(field >= attr['greater'])\n                if attr.get('less') is not None:\n                    query = query.filter(field <= attr['less'])\n\n        res['total'] = query.count()\n\n        if confusion is not None:\n            project = self.query(Project).join(Dag).join(Task).filter(\n                Task.id == filter['task']\n            ).first()\n            class_names = yaml_load(project.class_names)\n\n            if 'default' in class_names:\n                res['class_names'] = class_names['default']\n            else:\n                res['class_names'] = [\n                    str(i) for i in range(confusion.shape[1])\n                ]\n\n        query = self.paginator(query, options)\n        img_objs = query.all()\n        for img_obj in img_objs:\n            buffer = img_obj.img\n            if layout:\n                buffer = np.fromstring(buffer, np.uint8)\n                img = cv2.imdecode(buffer, cv2.IMREAD_COLOR)\n                img = resize_saving_ratio(\n                    img, (layout.get('max_height'), layout.get('max_width'))\n                )\n                retval, buffer = cv2.imencode('.jpg', img)\n\n            jpg_as_text = base64.b64encode(buffer).decode('utf-8')\n\n            # noinspection PyTypeChecker\n            res['data'].append(\n                {\n                    'content': jpg_as_text,\n                    'id': img_obj.id,\n                    'y_pred': img_obj.y_pred,\n                    'y': img_obj.y,\n                    'score': round(img_obj.score, 2) if img_obj.score else None\n                }\n            )\n\n        return res\n\n    def detail_img_segment(\n        self, filter: dict, options: PaginatorOptions = None\n    ):\n        res = {'data': []}\n        res.update(filter)\n\n        query = self.query(ReportImg).filter(\n            ReportImg.task == filter['task']). \\\n            filter(ReportImg.group == filter['group'])\n\n        if filter.get('y') is not None and filter.get('y_pred') is not None:\n            query = query.filter(\n                and_(\n                    ReportImg.y == filter['y'],\n                    ReportImg.y_pred == filter['y_pred']\n                )\n            )\n\n        if filter.get('score_min') is not None:\n            query = query.filter(\n                or_(\n                    ReportImg.score >= filter['score_min'],\n                    ReportImg.score.__eq__(None)\n                )\n            )\n\n        if filter.get('score_max') is not None:\n            query = query.filter(\n                or_(\n                    ReportImg.score <= filter['score_max'],\n                    ReportImg.score.__eq__(None)\n                )\n            )\n\n        layout = filter.get('layout')\n\n        if layout and layout.get('attrs'):\n            for attr in layout['attrs']:\n                field = getattr(ReportImg, attr['source'])\n                if attr.get('equal') is not None:\n                    query = query.filter(field == attr['equal'])\n                if attr.get('greater') is not None:\n                    query = query.filter(field >= attr['greater'])\n                if attr.get('less') is not None:\n                    query = query.filter(field <= attr['less'])\n\n        res['total'] = query.count()\n\n        query = self.paginator(query, options)\n        img_objs = query.all()\n        for img_obj in img_objs:\n            buffer = img_obj.img\n            if layout:\n                buffer = np.fromstring(buffer, np.uint8)\n                img = cv2.imdecode(buffer, cv2.IMREAD_COLOR)\n                img = resize_saving_ratio(\n                    img, (layout.get('max_height'), layout.get('max_width'))\n                )\n                retval, buffer = cv2.imencode('.jpg', img)\n\n            jpg_as_text = base64.b64encode(buffer).decode('utf-8')\n\n            # noinspection PyTypeChecker\n            res['data'].append(\n                {\n                    'content': jpg_as_text,\n                    'id': img_obj.id,\n                    'y_pred': img_obj.y_pred,\n                    'y': img_obj.y,\n                    'score': round(img_obj.score, 2)\n                    if img_obj.score is not None else None\n                }\n            )\n\n        return res\n\n\n__all__ = ['ReportImgProvider']\n"""
mlcomp/db/providers/report/layout.py,0,"b""from mlcomp.db.core import PaginatorOptions\nfrom mlcomp.db.models import ReportLayout\nfrom mlcomp.db.providers.base import BaseDataProvider\nfrom mlcomp.db.report_info import ReportLayoutInfo\nfrom mlcomp.utils.misc import now\nfrom mlcomp.utils.io import yaml_load, yaml_dump\n\n\nclass ReportLayoutProvider(BaseDataProvider):\n    model = ReportLayout\n\n    def get(self, filter: dict = None, options: PaginatorOptions = None):\n        query = self.query(ReportLayout)\n        total = query.count()\n        paginator = self.paginator(query, options)\n\n        res = []\n        for item in paginator.all():\n            res.append(self.to_dict(item))\n\n        return {'total': total, 'data': res}\n\n    def by_name(self, name: str):\n        return self.query(ReportLayout).filter(ReportLayout.name == name).one()\n\n    def add_item(self, k: str, v: dict):\n        self.add(\n            ReportLayout(content=yaml_dump(v), name=k, last_modified=now())\n        )\n\n    def all(self):\n        res = {\n            s.name: yaml_load(s.content)\n            for s in self.query(ReportLayout).all()\n        }\n\n        for k, v in res.items():\n            res[k] = ReportLayoutInfo.union_layouts(k, res)\n        return res\n\n    def change(self, k: str, v: dict):\n        self.query(ReportLayout).filter(ReportLayout.name == k).update(\n            {\n                'last_modified': now(),\n                'content': yaml_dump(v)\n            }\n        )\n\n\n__all__ = ['ReportLayoutProvider']\n"""
mlcomp/db/providers/report/report.py,0,"b""import base64\nimport pickle\nfrom collections import defaultdict\n\nfrom itertools import groupby\nfrom typing import List\n\nfrom sqlalchemy import func, case\nfrom sqlalchemy.orm import joinedload\n\nfrom mlcomp.db.core import PaginatorOptions, Session\nfrom mlcomp.db.enums import TaskStatus, TaskType\nfrom mlcomp.db.models import Report, ReportTasks, Task, ReportSeries, \\\n    ReportImg, ReportLayout\nfrom mlcomp.db.providers.base import BaseDataProvider\nfrom mlcomp.db.report_info import ReportLayoutSeries, ReportLayoutInfo\nfrom mlcomp.db.report_info.item import ReportLayoutItem\nfrom mlcomp.utils.io import yaml_load, yaml_dump\n\n\nclass ReportProvider(BaseDataProvider):\n    model = Report\n\n    def __init__(self, session: Session = None):\n        super(ReportProvider, self).__init__(session)\n\n    def get(self, filter: dict, options: PaginatorOptions):\n        task_count_cond = func.sum(\n            case(\n                whens=[(Task.status <= TaskStatus.InProgress.value, 1)],\n                else_=0\n            ).label('tasks_not_finished')\n        )\n\n        query = self.query(Report,\n                           func.count(ReportTasks.task).label('tasks_count'),\n                           task_count_cond, ). \\\n            join(ReportTasks, ReportTasks.report == Report.id, isouter=True). \\\n            join(Task, Task.id == ReportTasks.task, isouter=True)\n\n        if filter.get('task'):\n            query = query.filter(ReportTasks.task == filter['task'])\n\n        query = query.group_by(Report.id)\n\n        total = query.count()\n        data = []\n        for report, task_count, tasks_not_finished in self.paginator(\n                query, options\n        ):\n            item = {\n                'id': report.id,\n                'time': self.serialize_datetime(report.time),\n                'tasks': task_count,\n                'tasks_not_finished': tasks_not_finished,\n                'name': report.name\n            }\n            data.append(item)\n\n        return {'total': total, 'data': data}\n\n    def _detail_series(\n            self, series: List[ReportSeries], name: str, result_key: str\n    ):\n        series = [s for s in series if s.name == name]\n        res = []\n\n        series = sorted(series, key=lambda x: x.part)\n        series_group = groupby(series, key=lambda x: x.part)\n        for key, group in series_group:\n            group = list(group)\n            group = sorted(group, key=lambda x: x.task)\n            for task_key, group_task in groupby(group, key=lambda x: x.task):\n                group_task = list(group_task)\n                res.append(\n                    {\n                        'x': [item.epoch for item in group_task],\n                        'y': [item.value for item in group_task],\n                        'stage': [item.stage for item in group_task],\n                        'color': 'orange' if key == 'valid' else 'blue',\n                        'time': [\n                            self.serialize_datetime(item.time)\n                            for item in group_task\n                        ],\n                        'group': key,\n                        'task_name': group_task[0].task_rel.name,\n                        'task_id': task_key,\n                        'source': name,\n                        'name': result_key\n                    }\n                )\n\n        return res\n\n    def _detail_single_img(self, report: int, item: ReportLayoutItem):\n        res = []\n        img_objs = self.query(ReportImg). \\\n            filter(ReportImg.group == item.name).all()\n\n        for img_obj in img_objs:\n            img_decoded = pickle.loads(img_obj.img)\n            item = {\n                'name': f'{img_obj.group} - {img_obj.part}',\n                'data': base64.b64encode(img_decoded['img']).decode('utf-8')\n            }\n            res.append(item)\n\n        return res\n\n    def detail_img_classify_descr(self, report: int, item: ReportLayoutItem):\n        res = []\n        tasks = self.query(ReportTasks.task.distinct()\n                           ).filter(ReportTasks.report == report).all()\n        tasks = [t[0] for t in tasks]\n        task_names = {\n            id: name\n            for id, name in self.query(Task.id, Task.name\n                                       ).filter(Task.id.in_(tasks)).all()\n        }\n        for task in tasks:\n            obj = {\n                'name': task_names[task],\n                'group': item.name,\n                'task': task,\n            }\n            res.append(obj)\n        return res\n\n    def detail_img_segment_descr(self, report: int, item: ReportLayoutItem):\n        res = []\n        tasks = self.query(ReportTasks.task.distinct()\n                           ).filter(ReportTasks.report == report).all()\n        tasks = [t[0] for t in tasks]\n        task_names = {\n            id: name\n            for id, name in self.query(Task.id, Task.name\n                                       ).filter(Task.id.in_(tasks)).all()\n        }\n        for task in tasks:\n            obj = {\n                'name': task_names[task],\n                'group': item.name,\n                'task': task,\n            }\n            res.append(obj)\n        return res\n\n    def detail(self, id: int):\n        report_obj = self.by_id(id)\n        tasks = self.query(ReportTasks.task).filter(ReportTasks.report == id\n                                                    ).all()\n        tasks = [t[0] for t in tasks]\n        config = yaml_load(report_obj.config)\n        report = ReportLayoutInfo(config)\n\n        series = self.query(ReportSeries). \\\n            filter(ReportSeries.task.in_(tasks)). \\\n            order_by(ReportSeries.epoch). \\\n            options(joinedload(ReportSeries.task_rel, innerjoin=True)).all()\n\n        items = dict()\n        series_names = set([s.name for s in series])\n        series_map = defaultdict(list)\n        for s in report.series:\n            series_map[s.key].append(s)\n\n        for name in series_names:\n            report_series = series_map.get(name, [\n                ReportLayoutSeries(name=name, key=name)])\n\n            for s in report_series:\n                items[s.name] = self._detail_series(series, s.key, s.name)\n\n        for element in report.precision_recall + report.f1:\n            items[element.name] = self._detail_single_img(id, element)\n\n        for element in report.img_classify:\n            items[element.name] = self.detail_img_classify_descr(id, element)\n\n        for element in report.img_segment:\n            items[element.name] = self.detail_img_segment_descr(id, element)\n\n        return {\n            'data': items,\n            'layout': report.layout,\n            'metric': report.metric.serialize()\n        }\n\n    def add_dag(self, dag: int, report: int):\n        tasks = self.query(Task.id). \\\n            filter(Task.dag == dag). \\\n            filter(Task.type <= TaskType.Train.value). \\\n            all()\n\n        report_tasks = self.query(ReportTasks.task\n                                  ).filter(ReportTasks.report == report).all()\n\n        for t in set(t[0] for t in tasks) - set(t[0] for t in report_tasks):\n            self.add(ReportTasks(report=report, task=t))\n\n    def remove_dag(self, dag: int, report: int):\n        tasks = self.query(Task.id).filter(Task.dag == dag).all()\n        tasks = [t[0] for t in tasks]\n        self.query(ReportTasks).filter(ReportTasks.report == report). \\\n            filter(ReportTasks.task.in_(tasks)). \\\n            delete(synchronize_session=False)\n\n        self.session.commit()\n\n    def remove_task(self, task: int, report: int):\n        self.query(ReportTasks).filter(ReportTasks.report == report). \\\n            filter(ReportTasks.task == task). \\\n            delete(synchronize_session=False)\n\n        self.session.commit()\n\n    def add_task(self, task: int, report: int):\n        self.add(ReportTasks(task=task, report=report))\n        self.session.commit()\n\n    def update_layout_start(self, id: int):\n        layouts = self.query(ReportLayout.name).all()\n        report = self.by_id(id)\n        layouts = [l[0] for l in layouts]\n        if report.layout in layouts:\n            layouts.remove(report.layout)\n            layouts.insert(0, report.layout)\n\n        return {'id': id, 'layouts': layouts}\n\n    def update_layout_end(self, id: int, layout: str, layouts: dict):\n        layout_content = yaml_dump(layouts[layout])\n        report = self.by_id(id)\n        report.config = layout_content\n        report.layout = layout\n        self.commit()\n\n\n__all__ = ['ReportProvider']\n"""
mlcomp/db/providers/report/series.py,0,"b""from typing import List\nfrom itertools import groupby\n\nfrom mlcomp.db.models import ReportSeries, Task\nfrom mlcomp.db.providers.base import BaseDataProvider\n\n\nclass ReportSeriesProvider(BaseDataProvider):\n    model = ReportSeries\n\n    def by_dag(self, dag: int, metrics: List[str]):\n        tasks = self.query(Task.id, Task.name).filter(Task.dag == dag).all()\n        ids = [id for id, _ in tasks]\n\n        series = self.query(ReportSeries).filter(\n            ReportSeries.task.in_(ids)).filter(\n            ReportSeries.name.in_(metrics)).order_by(ReportSeries.task).all()\n\n        res = []\n        for task_id, task_series in groupby(series, key=lambda x: x.task):\n            task_series = list(task_series)\n            task_series = sorted(task_series, key=lambda x: x.name)\n\n            for name, name_series in groupby(task_series,\n                                             key=lambda x: x.name):\n                name_series = list(name_series)\n                name_series = sorted(name_series, key=lambda x: x.part)\n                groups = []\n                for group, part_series in groupby(name_series,\n                                                  key=lambda x: x.part):\n                    part_series = list(part_series)\n                    part_series = sorted(part_series, key=lambda x: x.epoch)\n                    group = {'name': group,\n                             'epoch': [s.epoch for s in part_series],\n                             'value': [s.value for s in part_series]\n                             }\n                    groups.append(group)\n                res.append((task_id, name, groups))\n        return res\n\n\n__all__ = ['ReportSeriesProvider']\n"""
mlcomp/db/providers/report/task.py,0,"b""from mlcomp.db.models import ReportTasks\nfrom mlcomp.db.providers.base import BaseDataProvider\n\n\nclass ReportTasksProvider(BaseDataProvider):\n    model = ReportTasks\n\n\n__all__ = ['ReportTasksProvider']\n"""
mlcomp/server/back/create_dags/__init__.py,0,b'# flake8: noqa\nfrom .standard import dag_standard\nfrom .pipe import dag_pipe\nfrom .model_add import dag_model_add\nfrom .model_start import dag_model_start\n'
mlcomp/server/back/create_dags/copy.py,0,"b""import hashlib\n\nimport re\n\nfrom mlcomp.utils.config import merge_dicts_smart\nfrom mlcomp.utils.io import yaml_load, yaml_dump\n\nfrom mlcomp.db.core import Session\nfrom mlcomp.db.enums import ComponentType, TaskStatus\nfrom mlcomp.db.models import Dag, Task, TaskDependence, DagStorage, File\nfrom mlcomp.db.providers import DagProvider, TaskProvider, DagStorageProvider, \\\n    FileProvider\nfrom mlcomp.utils.misc import now\n\n\nclass DagCopyBuilder:\n    def __init__(\n            self,\n            session: Session,\n            dag: int,\n            file_changes: str = '',\n            dag_suffix: str = '',\n            logger=None,\n            component: ComponentType = None\n    ):\n        self.dag = dag\n        self.file_changes = file_changes\n        self.session = session\n        self.logger = logger\n        self.component = component\n        self.dag_suffix = dag_suffix\n\n        self.dag_db = None\n\n        self.dag_provider = None\n        self.task_provider = None\n        self.file_provider = None\n        self.dag_storage_provider = None\n\n    def log_info(self, message: str):\n        if self.logger:\n            self.logger.info(message, self.component)\n\n    def create_providers(self):\n        self.log_info('create_providers')\n\n        self.dag_provider = DagProvider(self.session)\n        self.task_provider = TaskProvider(self.session)\n        self.file_provider = FileProvider(self.session)\n        self.dag_storage_provider = DagStorageProvider(self.session)\n\n    def create_dag(self):\n        dag = self.dag_provider.by_id(self.dag)\n        name = dag.name\n        if self.dag_suffix:\n            name += ' ' + self.dag_suffix\n        dag_new = Dag(name=name, created=now(), config=dag.config,\n                      project=dag.project, docker_img=dag.docker_img,\n                      img_size=0, file_size=0, type=dag.type)\n        self.dag_provider.add(dag_new)\n        self.dag_db = dag_new\n\n    def find_replace(self, changes: dict, path: str):\n        for k, v in changes.items():\n            if not re.match(k, path):\n                continue\n            return v\n\n    def create_tasks(self):\n        tasks = self.task_provider.by_dag(self.dag)\n        tasks_new = []\n        tasks_old = []\n\n        for t in tasks:\n            if t.parent:\n                continue\n\n            task = Task(name=t.name, status=TaskStatus.NotRan.value,\n                        computer=t.computer, gpu=t.gpu, gpu_max=t.gpu_max,\n                        cpu=t.cpu, executor=t.executor, memory=t.memory,\n                        steps=t.steps, dag=self.dag_db.id, debug=t.debug,\n                        type=t.type,\n                        )\n            task.additional_info = t.additional_info\n            tasks_new.append(task)\n            tasks_old.append(t)\n\n        self.task_provider.bulk_save_objects(tasks_new, return_defaults=True)\n        old2new = {t_old.id: t_new.id for t_new, t_old in\n                   zip(tasks_new, tasks_old)}\n        dependencies = self.task_provider.get_dependencies(self.dag)\n        dependencies_new = []\n        for d in dependencies:\n            d_new = TaskDependence(task_id=old2new[d.task_id],\n                                   depend_id=old2new[d.depend_id])\n            dependencies_new.append(d_new)\n\n        self.task_provider.bulk_save_objects(dependencies_new,\n                                             return_defaults=False)\n\n        changes = yaml_load(self.file_changes)\n        storages = self.dag_storage_provider.by_dag(self.dag)\n        storages_new = []\n\n        for s, f in storages:\n            if not isinstance(changes, dict):\n                continue\n\n            replace = self.find_replace(changes, s.path)\n            if replace is not None and f:\n                content = f.content.decode('utf-8')\n                if s.path.endswith('.yml'):\n                    data = yaml_load(content)\n                    data = merge_dicts_smart(data, replace)\n                    content = yaml_dump(data)\n                else:\n                    for k, v in replace:\n                        if k not in content:\n                            raise Exception(f'{k} is not in the content')\n                        content = content.replace(k, v)\n                content = content.encode('utf-8')\n                md5 = hashlib.md5(content).hexdigest()\n                f = self.file_provider.by_md5(md5)\n                if not f:\n                    f = File(\n                        content=content,\n                        created=now(),\n                        project=self.dag_db.project,\n                        md5=md5,\n                        dag=self.dag_db.id\n                    )\n                self.file_provider.add(f)\n\n            s_new = DagStorage(dag=self.dag_db.id, file=f.id, path=s.path,\n                               is_dir=s.is_dir)\n            storages_new.append(s_new)\n\n        self.dag_storage_provider.bulk_save_objects(\n            storages_new,\n            return_defaults=False\n        )\n\n    def build(self):\n        self.create_providers()\n        self.create_dag()\n        self.create_tasks()\n\n\ndef dag_copy(session: Session, dag: int, file_changes: str = '',\n             dag_suffix: str = ''):\n    builder = DagCopyBuilder(session, dag=dag, file_changes=file_changes,\n                             dag_suffix=dag_suffix)\n    builder.build()\n\n\n__all__ = ['dag_copy']\n"""
mlcomp/server/back/create_dags/model_add.py,0,"b""from sqlalchemy.orm import joinedload\n\nfrom mlcomp.db.core import Session\nfrom mlcomp.db.models import Task, Model\nfrom mlcomp.db.providers import TaskProvider, ProjectProvider, ModelProvider\nfrom mlcomp.server.back.create_dags.standard import dag_standard\nfrom mlcomp.utils.misc import now\n\n\ndef dag_model_add(session: Session, data: dict):\n    if not data.get('task'):\n        model = Model(\n            name=data['name'],\n            project=data['project'],\n            equations=data['equations'],\n            created=now()\n        )\n        ModelProvider(session).add(model)\n        return\n\n    task_provider = TaskProvider(session)\n    task = task_provider.by_id(\n        data['task'], options=joinedload(Task.dag_rel, innerjoin=True)\n    )\n    child_tasks = task_provider.children(task.id)\n    computer = task.computer_assigned\n    child_task = None\n    if len(child_tasks) > 0:\n        child_task = child_tasks[0].id\n        computer = child_tasks[0].computer_assigned\n\n    project = ProjectProvider(session).by_id(task.dag_rel.project)\n    config = {\n        'info': {\n            'name': 'model_add',\n            'project': project.name,\n            'computer': computer\n        },\n        'executors': {\n            'model_add': {\n                'type': 'model_add',\n                'project': data['project'],\n                'task': data.get('task'),\n                'name': data['name'],\n                'file': data['file'],\n                'child_task': child_task,\n                'fold': data['fold']\n            }\n        }\n    }\n\n    dag_standard(\n        session=session, config=config, debug=False, upload_files=False\n    )\n\n\n__all__ = ['dag_model_add']\n"""
mlcomp/server/back/create_dags/model_start.py,0,"b""from mlcomp.db.core import Session\nfrom mlcomp.db.models import Dag\nfrom mlcomp.db.providers import ModelProvider, DagProvider\nfrom mlcomp.server.back.create_dags.standard import dag_standard\nfrom mlcomp.utils.config import Config\nfrom mlcomp.utils.io import yaml_load, yaml_dump\nfrom mlcomp.utils.misc import now\n\n\ndef dag_model_start(session: Session, data: dict):\n    provider = ModelProvider(session)\n    model = provider.by_id(data['model_id'])\n    dag_provider = DagProvider(session)\n    dag = dag_provider.by_id(data['dag'], joined_load=[Dag.project_rel])\n\n    project = dag.project_rel\n    src_config = Config.from_yaml(dag.config)\n    pipe = src_config['pipes'][data['pipe']['name']]\n\n    equations = yaml_load(model.equations)\n    versions = data['pipe']['versions']\n\n    if len(versions) > 0:\n        version = data['pipe']['version']\n        pipe_equations = yaml_load(version['equations'])\n        found_version = versions[0]\n        for v in versions:\n            if v['name'] == version['name']:\n                found_version = v\n                break\n\n        found_version['used'] = now()\n\n        for v in pipe.values():\n            v.update(pipe_equations)\n\n    equations[data['pipe']['name']] = versions\n    model.equations = yaml_dump(equations)\n\n    for v in pipe.values():\n        v['model_id'] = model.id\n        v['model_name'] = model.name\n\n    config = {\n        'info': {\n            'name': data['pipe']['name'],\n            'project': project.name\n        },\n        'executors': pipe\n    }\n\n    if model.dag:\n        old_dag = dag_provider.by_id(model.dag)\n        if old_dag.name != dag.name:\n            model.dag = dag.id\n    else:\n        model.dag = dag.id\n\n    provider.commit()\n\n    dag_standard(\n        session=session,\n        config=config,\n        debug=False,\n        upload_files=False,\n        copy_files_from=data['dag']\n    )\n\n\n__all__ = ['dag_model_start']\n"""
mlcomp/server/back/create_dags/pipe.py,0,"b""import os\n\nfrom mlcomp.db.core import Session\nfrom mlcomp.db.enums import DagType\nfrom mlcomp.db.providers import DagProvider, ProjectProvider, ModelProvider\nfrom mlcomp.db.models import Dag\nfrom mlcomp.worker.storage import Storage\n\n\ndef dag_pipe(session: Session, config: dict, config_text: str = None):\n    assert 'pipes' in config, 'pipe missed'\n\n    info = config['info']\n\n    storage = Storage(session)\n    dag_provider = DagProvider(session)\n\n    folder = os.getcwd()\n    project = ProjectProvider(session).by_name(info['project']).id\n    dag = dag_provider.add(\n        Dag(\n            config=config_text,\n            project=project,\n            name=info['name'],\n            docker_img=info.get('docker_img'),\n            type=DagType.Pipe.value\n        )\n    )\n    storage.upload(folder, dag)\n\n    # Change model dags which have the same name\n    ModelProvider(session\n                  ).change_dag(project=project, name=info['name'], to=dag.id)\n\n\n__all__ = ['dag_pipe']\n"""
mlcomp/server/back/create_dags/standard.py,0,"b'from collections import OrderedDict\nimport os\nfrom copy import deepcopy\n\nfrom mlcomp.contrib.search.grid import grid_cells\nfrom mlcomp.db.core import Session\nfrom mlcomp.db.enums import TaskType, DagType, ComponentType\nfrom mlcomp.db.models import Report, Task, Dag, ReportTasks, TaskDependence\nfrom mlcomp.db.providers import TaskProvider, \\\n    ReportProvider, \\\n    ReportTasksProvider, \\\n    ReportLayoutProvider, \\\n    DagProvider, \\\n    ProjectProvider\nfrom mlcomp.utils.misc import now\nfrom mlcomp.worker.executors import Executor\nfrom mlcomp.worker.storage import Storage\nfrom mlcomp.utils.io import yaml_dump\n\n\nclass DagStandardBuilder:\n    def __init__(\n            self,\n            session: Session,\n            config: dict,\n            debug: bool,\n            config_text: str = None,\n            upload_files: bool = True,\n            copy_files_from: int = None,\n            config_path: str = None,\n            control_reqs: bool = True,\n            logger=None,\n            component: ComponentType = None,\n            grid_cell: dict = None\n    ):\n        self.session = session\n        self.config = config\n        self.debug = debug\n        self.config_text = config_text\n        self.upload_files = upload_files\n        self.copy_files_from = copy_files_from\n        self.config_path = config_path\n        self.control_reqs = control_reqs\n\n        self.info = config[\'info\']\n        self.layout_name = self.info.get(\'layout\')\n\n        self.provider = None\n        self.report_provider = None\n        self.report_tasks_provider = None\n        self.report_layout_provider = None\n        self.storage = None\n        self.dag_provider = None\n        self.logger = logger\n        self.component = component\n        self.grid_cell = grid_cell\n\n        self.project = None\n        self.layouts = None\n        self.dag = None\n        self.dag_report_id = None\n        self.created = None\n        self.project_provider = None\n\n    def log_info(self, message: str):\n        if self.logger:\n            self.logger.info(message, self.component)\n\n    def create_providers(self):\n        self.log_info(\'create_providers\')\n\n        self.provider = TaskProvider(self.session)\n        self.report_provider = ReportProvider(self.session)\n        self.report_tasks_provider = ReportTasksProvider(self.session)\n        self.report_layout_provider = ReportLayoutProvider(self.session)\n        self.project_provider = ProjectProvider(self.session)\n\n        self.storage = Storage(self.session, logger=self.logger,\n                               component=self.component)\n        self.dag_provider = DagProvider(self.session)\n\n    def load_base(self):\n        self.log_info(\'load_base\')\n\n        project = self.project_provider.by_name(self.info[\'project\'])\n        if project is None:\n            project = self.project_provider.add_project(self.info[\'project\'])\n\n        self.project = project.id\n        self.layouts = self.report_layout_provider.all()\n\n    def create_report(self):\n        self.log_info(\'create_report\')\n\n        self.dag_report_id = None\n        layout_name = self.layout_name\n        if layout_name:\n            if layout_name not in self.layouts:\n                raise Exception(f\'Unknown layout = {layout_name}\')\n\n            report = Report(\n                config=yaml_dump(self.layouts[layout_name]),\n                name=self.info[\'name\'],\n                project=self.project,\n                layout=layout_name\n            )\n            self.report_provider.add(report)\n            self.dag_report_id = report.id\n\n    def create_dag(self):\n        self.log_info(\'create_dag\')\n\n        name = self.info[\'name\']\n        if self.grid_cell:\n            name = f\'{name} {self.grid_cell[1]}\'\n\n        dag = Dag(\n            config=self.config_text or yaml_dump(self.config),\n            project=self.project,\n            name=name,\n            docker_img=self.info.get(\'docker_img\'),\n            type=DagType.Standard.value,\n            created=now(),\n            report=self.dag_report_id\n        )\n\n        self.dag = self.dag_provider.add(dag)\n\n    def upload(self):\n        self.log_info(\'upload\')\n\n        if self.upload_files:\n            folder = os.path.dirname(os.path.abspath(self.config_path))\n            if \'expdir\' in self.config[\'info\']:\n                path = os.path.dirname(os.path.abspath(self.config_path))\n                folder = os.path.abspath(\n                    os.path.join(path, self.config[\'info\'][\'expdir\'])\n                )\n            self.storage.upload(folder, self.dag,\n                                control_reqs=self.control_reqs)\n        elif self.copy_files_from:\n            self.storage.copy_from(self.copy_files_from, self.dag)\n\n    def create_task(self, k: str, v: dict, name: str, info: dict,\n                    cell: dict = None):\n        task_type = TaskType.User.value\n        v = deepcopy(v)\n        if v.get(\'task_type\') == \'train\' or \\\n                Executor.is_trainable(v[\'type\']):\n            task_type = TaskType.Train.value\n\n        gpu = str(v.get(\'gpu\', \'0\'))\n        if \'-\' not in gpu:\n            gpu = int(gpu)\n            gpu_max = gpu\n        else:\n            gpu, gpu_max = map(int, gpu.split(\'-\'))\n\n        if gpu == 0 and gpu_max > 0:\n            raise Exception(f\'Executor = {k} Gpu_max can""t be>0 when gpu=0\')\n\n        task = Task(\n            name=name,\n            executor=k,\n            computer=self.info.get(\'computer\') or v.get(\'computer\'),\n            gpu=gpu,\n            gpu_max=gpu_max,\n            cpu=v.get(\'cpu\', 1),\n            memory=v.get(\'memory\', 0.1),\n            dag=self.dag.id,\n            debug=self.debug,\n            steps=int(v.get(\'steps\', \'1\')),\n            type=task_type\n        )\n\n        if cell is not None:\n            v.update(cell)\n\n        info[\'executor\'] = v\n        task.additional_info = yaml_dump(info)\n        report = None\n\n        if self.layout_name and task_type == TaskType.Train.value:\n            if self.layout_name not in self.layouts:\n                raise Exception(f\'Unknown report = {v[""report""]}\')\n\n            report_config = self.layouts[self.layout_name]\n            info[\'report_config\'] = report_config\n\n            task.additional_info = yaml_dump(info)\n            report = Report(\n                config=yaml_dump(report_config),\n                name=task.name,\n                project=self.project,\n                layout=self.layout_name\n            )\n\n        return task, report\n\n    def create_tasks(self):\n        self.log_info(\'create_tasks\')\n\n        created = OrderedDict()\n        executors = self.config[\'executors\']\n\n        tasks = []\n        dependencies = []\n        reports = []\n\n        while len(created) < len(executors):\n            for k, v in executors.items():\n                if self.grid_cell:\n                    v.update(**self.grid_cell[0])\n\n                valid = True\n                if \'depends\' in v:\n                    depends = v[\'depends\']\n                    if not isinstance(depends, list):\n                        depends = [depends]\n\n                    for d in depends:\n                        if d == k:\n                            raise Exception(f\'Executor {k} depends on itself\')\n\n                        if d not in executors:\n                            raise Exception(\n                                f\'Executor {k} depend on {d} \'\n                                f\'which does not exist\'\n                            )\n\n                        valid = valid and d in created\n                if valid:\n                    names = []\n                    infos = []\n                    task_cells = []\n                    if \'grid\' in v:\n                        grid = v[\'grid\']\n                        del v[\'grid\']\n\n                        cells = grid_cells(grid)\n                        for i, (cell, cell_name) in enumerate(cells):\n                            names.append(cell_name)\n                            infos.append({\'grid_cell\': i})\n                            task_cells.append(cell)\n                    else:\n                        names.append(v.get(\'name\', k))\n                        infos.append({})\n                        task_cells.append({})\n\n                    k_tasks = []\n                    for name, cell, info in zip(names, task_cells, infos):\n                        task, report = self.create_task(k, v, name=name,\n                                                        info=info, cell=cell)\n                        tasks.append(task)\n                        k_tasks.append(task)\n                        reports.append(report)\n\n                        if \'depends\' in v:\n                            depends = v[\'depends\']\n                            if not isinstance(depends, list):\n                                depends = [depends]\n\n                            for d in depends:\n                                for dd in created[d]:\n                                    dependencies.append((task, dd))\n                    created[k] = k_tasks\n\n        not_empty_reports = [r for r in reports if r is not None]\n        if len(not_empty_reports) > 0:\n            self.provider.bulk_save_objects(not_empty_reports,\n                                            return_defaults=True)\n            for report, task in zip(reports, tasks):\n                if report is not None:\n                    task.report = report.id\n\n        self.provider.bulk_save_objects(tasks,\n                                        return_defaults=True)\n\n        if len(not_empty_reports) > 0:\n            report_tasks = []\n            for report, task in zip(reports, tasks):\n                if report is not None:\n                    report_tasks.append(\n                        ReportTasks(report=report.id, task=task.id))\n            self.report_tasks_provider.bulk_save_objects(report_tasks)\n\n        dependencies = [\n            TaskDependence(task_id=task.id, depend_id=dd.id) for task, dd in\n            dependencies\n        ]\n        self.provider.bulk_save_objects(dependencies)\n\n        for k, v in created.items():\n            created[k] = [vv.id for vv in v]\n        self.created = created\n\n    def build(self):\n        self.create_providers()\n\n        self.load_base()\n\n        self.create_report()\n\n        self.create_dag()\n\n        self.upload()\n\n        self.create_tasks()\n\n        self.log_info(\'Done\')\n\n        return self.created\n\n\ndef dag_standard(\n        session: Session,\n        config: dict,\n        debug: bool,\n        config_text: str = None,\n        upload_files: bool = True,\n        copy_files_from: int = None,\n        config_path: str = None,\n        control_reqs: bool = True,\n        logger=None,\n        component: ComponentType = None,\n        grid_cell: dict = None\n):\n    builder = DagStandardBuilder(\n        session=session,\n        config=config,\n        debug=debug,\n        config_text=config_text,\n        upload_files=upload_files,\n        copy_files_from=copy_files_from,\n        config_path=config_path,\n        control_reqs=control_reqs,\n        logger=logger,\n        component=component,\n        grid_cell=grid_cell\n    )\n    return builder.build()\n\n\n__all__ = [\'dag_standard\']\n'"
mlcomp/worker/executors/base/__init__.py,0,"b""from .step import StepWrap\nfrom .executor import Executor\n\n__all__ = ['StepWrap', 'Executor']\n"""
mlcomp/worker/executors/base/executor.py,0,"b'from abc import ABC, abstractmethod\nimport time\n\nfrom tqdm import tqdm\n\nfrom mlcomp.utils.io import yaml_load, yaml_dump\nfrom mlcomp import FILE_SYNC_INTERVAL\nfrom mlcomp.db.core import Session\nfrom mlcomp.db.models import Task, Dag\nfrom mlcomp.utils.config import Config\nfrom mlcomp.db.providers import TaskProvider, TaskSyncedProvider\nfrom mlcomp.utils.misc import to_snake\nfrom mlcomp.worker.executors.base.step import StepWrap\n\n\nclass TqdmWrapper:\n    def __init__(\n            self, executor: \'Executor\', iterable=None,\n            desc: str = \'progress\', interval: int = 10,\n            **kwargs\n    ):\n        self.desc = desc\n        self.iterable = iterable\n        self.interval = interval\n        self.executor = executor\n        self.tqdm = tqdm(iterable=iterable, **kwargs)\n\n    def refresh(self):\n        executor = self.executor\n        tqdm = self.tqdm\n\n        executor.task.loader_name = self.desc\n        executor.task.batch_index = tqdm.n\n        executor.task.batch_total = tqdm.total\n        executor.task.epoch_duration = time.time() - tqdm.start_t\n        if tqdm.n > 0:\n            frac = (tqdm.total - tqdm.n) / tqdm.n\n            executor.task.epoch_time_remaining = \\\n                executor.task.epoch_duration * frac\n\n        executor.task_provider.update()\n        return time.time()\n\n    def set_description(self, desc=None, refresh=True):\n        """"""\n        Set/modify description of the progress bar.\n\n        Parameters\n        ----------\n        desc  : str, optional\n        refresh  : bool, optional\n            Forces refresh [default: True].\n        """"""\n        self.desc = desc or \'\'\n        if refresh:\n            self.refresh()\n\n    def __iter__(self):\n        last_written = self.refresh()\n\n        for item in self.tqdm:\n            elapsed = time.time() - last_written\n            if elapsed > self.interval:\n                last_written = self.refresh()\n            yield item\n        self.refresh()\n\n\nclass Executor(ABC):\n    _child = dict()\n\n    session = None\n    task_provider = None\n    logger = None\n    logger_db = None\n    step = None\n\n    def __init__(self, **kwargs):\n        self.kwargs = kwargs\n\n    def debug(self, message: str, db: bool = False):\n        if self.step:\n            self.step.debug(message, db=db)\n        else:\n            print(message)\n\n    def info(self, message: str, db: bool = False):\n        if self.step:\n            self.step.info(message, db=db)\n        else:\n            print(message)\n\n    def warning(self, message: str, db: bool = False):\n        if self.step:\n            self.step.warning(message, db=db)\n        else:\n            print(message)\n\n    def error(self, message: str, db: bool = False):\n        if self.step:\n            self.step.error(message, db=db)\n        else:\n            print(message)\n\n    def write(self, message: str):\n        if message.strip() != \'\':\n            self.info(message, db=True)\n\n    def flush(self):\n        pass\n\n    def add_child_process(self, pid: int):\n        additional_info = yaml_load(self.task.additional_info)\n        additional_info[\'child_processes\'] = additional_info.get(\n            \'child_processes\', []) + [pid]\n        self.task.additional_info = yaml_dump(additional_info)\n        self.task_provider.update()\n\n    def __call__(\n            self, *, task: Task, task_provider: TaskProvider, dag: Dag\n    ) -> dict:\n        assert dag is not None, \'You must fetch task with dag_rel\'\n\n        self.task_provider = task_provider\n        self.task = task\n        self.dag = dag\n        self.step = StepWrap(self.session, self.logger, self.logger_db, task,\n                             task_provider)\n        self.step.enter()\n\n        if not task.debug and FILE_SYNC_INTERVAL:\n            self.wait_data_sync()\n        res = self.work()\n        self.step.task_provider.commit()\n        return res\n\n    @abstractmethod\n    def work(self) -> dict:\n        pass\n\n    @classmethod\n    def _from_config(\n            cls, executor: dict, config: Config, additional_info: dict\n    ):\n        return cls(**executor)\n\n    @staticmethod\n    def from_config(\n            *, executor: str, config: Config, additional_info: dict,\n            session: Session, logger, logger_db\n    ) -> \'Executor\':\n        if executor not in config[\'executors\']:\n            raise ModuleNotFoundError(\n                f\'Executor {executor} \'\n                f\'has not been found\'\n            )\n\n        executor = additional_info[\'executor\']\n        child_class = Executor._child[executor[\'type\']]\n\n        # noinspection PyProtectedMember\n        res = child_class._from_config(executor, config, additional_info)\n        res.session = session\n        res.logger = logger\n        res.logger_db = logger_db\n        return res\n\n    @staticmethod\n    def register(cls):\n        Executor._child[cls.__name__] = cls\n        Executor._child[cls.__name__.lower()] = cls\n        Executor._child[to_snake(cls.__name__)] = cls\n        return cls\n\n    @staticmethod\n    def is_registered(cls: str):\n        return cls in Executor._child\n\n    def wait_data_sync(self):\n        self.info(f\'Start data sync\')\n\n        while True:\n            provider = TaskSyncedProvider(self.session)\n            provider.commit()\n\n            wait = False\n            for computer, project, tasks in provider.for_computer(\n                    self.task.computer_assigned\n            ):\n                if project.id == self.dag.project:\n                    wait = True\n\n            if wait:\n                time.sleep(1)\n            else:\n                break\n\n        self.info(f\'Finish data sync\')\n\n    @staticmethod\n    def is_trainable(type: str):\n        variants = [\'Catalyst\']\n        return type in (variants + [v.lower() for v in variants])\n\n    def tqdm(self, iterable=None, desc: str = \'progress\', interval: int = 10,\n             **kwargs):\n        """"""\n        tqdm wrapper. writes progress to Database\n        Args:\n            iterable: iterable for tqdm\n            desc: name of the progress bar\n            interval: interval of writing to Database\n            **kwargs: tqdm additional arguments\n\n        Returns: tqdm wrapper\n        """"""\n        return TqdmWrapper(\n            executor=self, iterable=iterable,\n            desc=desc, interval=interval,\n            **kwargs)\n\n    def dependent_results(self):\n        tasks = self.task_provider.find_dependents(self.task.id)\n        res = dict()\n        for t in tasks:\n            res[t.id] = yaml_load(t.result)\n        return res\n\n\n__all__ = [\'Executor\']\n'"
mlcomp/worker/executors/base/step.py,0,"b""from mlcomp.db.core import Session\nfrom mlcomp.db.enums import ComponentType\nfrom mlcomp.db.models import Task, Step\nfrom mlcomp.db.providers import LogProvider, StepProvider, TaskProvider\nfrom mlcomp.utils.misc import now\n\n\nclass StepWrap:\n    def __init__(\n            self, session: Session, logger, logger_db, task: Task,\n            task_provider: TaskProvider\n    ):\n        self.log_provider = LogProvider(session)\n        self.step_provider = StepProvider(session)\n        self.task_provider = task_provider\n        self.task = task\n        self.children = []\n        self.step = None\n        self.logger = logger\n        self.logger_db = logger_db\n\n    @property\n    def id(self):\n        return self.step.id\n\n    def enter(self):\n        task = self.task if not self.task.parent else self.task_provider.by_id(\n            self.task.parent\n        )\n        self.children = self.step_provider.unfinished(task.id)\n        if len(self.children) == 0:\n            self.step = self.start(0, 'main', 0)\n        else:\n            self.step = self.children[-1]\n\n    def _finish(self):\n        if len(self.children) == 0:\n            return\n        step = self.children.pop()\n        step.finished = now()\n        self.step_provider.update()\n        self.step = self.children[-1] if len(self.children) > 0 else step\n\n        self.debug('End of the step')\n\n    def finish(self):\n        while len(self.children) > 0:\n            self._finish()\n\n    def update_task_step(self, task):\n        task.current_step = '.'.join(\n            [\n                str(c.index + 1)\n                for c in self.children[1:]\n            ]\n        )\n        self.task_provider.commit()\n\n    def start(self, level: int, name: str = None, index: int = None):\n        if any(c.level == level and c.index == index and c.name == name for c\n               in self.children):\n            return\n\n        task = self.task if not self.task.parent else self.task_provider.by_id(\n            self.task.parent\n        )\n\n        if index is None and task.current_step:\n            parts = task.current_step.split('.')\n            if len(parts) >= level:\n                index = int(parts[level - 1])\n\n        if self.step is not None:\n            diff = level - self.step.level\n            assert level > 0, 'level must be positive'\n            assert diff <= 1, \\\n                f'Level {level} can not be started after {self.step.level}'\n\n            if diff <= 0:\n                for _ in range(abs(diff) + 1):\n                    self._finish()\n\n        step = Step(\n            level=level,\n            name=name or '',\n            started=now(),\n            task=task.id,\n            index=index or 0\n        )\n        self.step_provider.add(step)\n        self.children.append(step)\n        self.step = step\n\n        self.update_task_step(task)\n\n        self.debug('Begin of the step')\n\n        return step\n\n    def end(self, level: int):\n        diff = level - self.step.level\n        assert diff <= 0, 'you can end only the same step or lower'\n        for i in range(abs(diff) + 1):\n            self._finish()\n\n    def debug(self, message: str, db: bool = False):\n        logger = self.logger_db if db else self.logger\n        logger.debug(\n            message, ComponentType.Worker, self.task.computer_assigned,\n            self.task.id, self.step.id\n        )\n\n    def info(self, message: str, db: bool = False):\n        logger = self.logger_db if db else self.logger\n        logger.info(\n            message, ComponentType.Worker, self.task.computer_assigned,\n            self.task.id, self.step.id\n        )\n\n    def warning(self, message: str, db: bool = False):\n        logger = self.logger_db if db else self.logger\n        logger.warning(\n            message, ComponentType.Worker, self.task.computer_assigned,\n            self.task.id, self.step.id\n        )\n\n    def error(self, message: str, db: bool = False):\n        logger = self.logger_db if db else self.logger\n        logger.error(\n            message, ComponentType.Worker, self.task.computer_assigned,\n            self.task.id, self.step.id\n        )\n\n\n__all__ = ['StepWrap']\n"""
mlcomp/worker/executors/catalyst_/__init__.py,0,"b""from .catalyst_ import Catalyst\n\n__all__ = ['Catalyst']\n"""
mlcomp/worker/executors/catalyst_/catalyst_.py,5,"b'import os\nimport socket\nfrom collections import OrderedDict\nfrom pathlib import Path\nfrom os.path import join\n\nimport torch\n\nfrom catalyst.utils import set_global_seed, \\\n    import_experiment_and_runner, parse_args_uargs, dump_environment, \\\n    load_checkpoint\nfrom catalyst.dl import State, Callback, Runner, CheckpointCallback, \\\n    CallbackNode\n\nfrom mlcomp import TASK_FOLDER\nfrom mlcomp.db.providers import ReportSeriesProvider, ComputerProvider, \\\n    MemoryProvider\nfrom mlcomp.db.report_info import ReportLayoutInfo\nfrom mlcomp.utils.io import yaml_load, yaml_dump\nfrom mlcomp.utils.misc import now\nfrom mlcomp.db.models import ReportSeries\nfrom mlcomp.utils.config import Config, merge_dicts_smart\nfrom mlcomp.worker.executors.base import Executor\nfrom mlcomp.worker.executors.model import trace_model_from_checkpoint\nfrom mlcomp.worker.sync import copy_remote\n\n\nclass Args:\n    baselogdir = None\n    batch_size = None\n    check = False\n    config = None\n    configs = []\n    expdir = None\n    logdir = None\n    num_epochs = None\n    num_workers = None\n    resume = None\n    seed = 42\n    verbose = True\n\n    def _get_kwargs(self):\n        return [\n            (k, v) for k, v in self.__dict__.items() if not k.startswith(\'_\')\n        ]\n\n\n# noinspection PyTypeChecker\n@Executor.register\nclass Catalyst(Executor, Callback):\n    def __init__(\n            self,\n            args: Args,\n            report: ReportLayoutInfo,\n            distr_info: dict,\n            resume: dict,\n            grid_config: dict,\n            trace: str,\n            params: dict,\n            **kwargs\n    ):\n        super().__init__(**kwargs)\n\n        self.series_provider = ReportSeriesProvider(self.session)\n        self.computer_provider = ComputerProvider(self.session)\n        self.memory_provider = MemoryProvider(self.session)\n\n        self.order = 0\n        self.resume = resume\n        self.distr_info = distr_info\n        self.args = args\n        self.report = report\n        self.experiment = None\n        self.runner = None\n        self.grid_config = grid_config\n        self.master = True\n        self.trace = trace\n        self.params = params\n        self.last_batch_logged = None\n        self.loader_started_time = None\n        self.parent = None\n        self.node = CallbackNode.All\n\n    def get_parent_task(self):\n        if self.parent:\n            return self.parent\n        return self.task\n\n    def callbacks(self):\n        result = OrderedDict()\n        if self.master:\n            result[\'catalyst\'] = self\n\n        return result\n\n    def on_loader_start(self, state: State):\n        self.loader_started_time = now()\n\n    def on_epoch_start(self, state: State):\n        stage_index = self.experiment.stages.index(state.stage_name)\n        self.step.start(1, name=state.stage_name, index=stage_index)\n\n        self.step.start(\n            2, name=f\'epoch {state.epoch}\', index=state.epoch - 1\n        )\n\n    def on_batch_start(self, state: State):\n        if self.last_batch_logged and state.loader_step != state.loader_len:\n            if (now() - self.last_batch_logged).total_seconds() < 10:\n                return\n\n        task = self.get_parent_task()\n        task.batch_index = state.loader_step\n        task.batch_total = state.loader_len\n        task.loader_name = state.loader_name\n\n        duration = int((now() - self.loader_started_time).total_seconds())\n        task.epoch_duration = duration\n        task.epoch_time_remaining = int(duration * (\n                task.batch_total / task.batch_index)) - task.epoch_duration\n        if state.epoch_metrics.get(\'train_loss\') is not None:\n            task.loss = float(state.epoch_metrics[\'train_loss\'])\n        if state.epoch_metrics.get(\'valid_loss\') is not None:\n            task.loss = float(state.epoch_metrics[\'valid_loss\'])\n\n        self.task_provider.update()\n        self.last_batch_logged = now()\n\n    def on_epoch_end(self, state: State):\n        self.step.end(2)\n\n        values = state.epoch_metrics\n\n        for k, v in values.items():\n            part = \'\'\n            name = k\n\n            for loader in state.loaders:\n                if k.startswith(loader):\n                    part = loader\n                    name = k.replace(loader, \'\')\n                    if name.startswith(\'_\'):\n                        name = name[1:]\n\n            task_id = self.task.parent or self.task.id\n            series = ReportSeries(\n                part=part,\n                name=name,\n                epoch=state.epoch - 1,\n                task=task_id,\n                value=v,\n                time=now(),\n                stage=state.stage_name\n            )\n            self.series_provider.add(series)\n\n            if name == self.report.metric.name:\n                best = False\n                task = self.task\n                if task.parent:\n                    task = self.task_provider.by_id(task.parent)\n\n                if self.report.metric.minimize:\n                    if task.score is None or v < task.score:\n                        best = True\n                else:\n                    if task.score is None or v > task.score:\n                        best = True\n                if best:\n                    task.score = v\n                    self.task_provider.update()\n\n    def on_stage_end(self, state: State):\n        self.step.end(1)\n\n    @classmethod\n    def _from_config(\n            cls, executor: dict, config: Config, additional_info: dict\n    ):\n        args = Args()\n        for k, v in executor[\'args\'].items():\n            v = str(v)\n            if v in [\'False\', \'True\']:\n                v = v == \'True\'\n            elif v.isnumeric():\n                v = int(v)\n\n            setattr(args, k, v)\n\n        assert \'report_config\' in additional_info, \'layout was not filled\'\n        report_config = additional_info[\'report_config\']\n        report = ReportLayoutInfo(report_config)\n        if len(args.configs) == 0:\n            args.configs = [args.config]\n\n        distr_info = additional_info.get(\'distr_info\', {})\n        resume = additional_info.get(\'resume\')\n        params = executor.get(\'params\', {})\n        params.update(additional_info.get(\'params\', {}))\n\n        grid_config = executor.copy()\n        grid_config.pop(\'args\', \'\')\n\n        return cls(\n            args=args,\n            report=report,\n            grid_config=grid_config,\n            distr_info=distr_info,\n            resume=resume,\n            trace=executor.get(\'trace\'),\n            params=params\n        )\n\n    def set_dist_env(self, config):\n        info = self.distr_info\n        os.environ[\'MASTER_ADDR\'] = info[\'master_addr\']\n        os.environ[\'MASTER_PORT\'] = str(info[\'master_port\'])\n        os.environ[\'WORLD_SIZE\'] = str(info[\'world_size\'])\n\n        os.environ[\'RANK\'] = str(info[\'rank\'])\n        os.environ[\'LOCAL_RANK\'] = ""0""\n        distributed_params = config.get(\'distributed_params\', {})\n        distributed_params[\'rank\'] = info[\'rank\']\n        config[\'distributed_params\'] = distributed_params\n\n        torch.cuda.set_device(0)\n\n        torch.distributed.init_process_group(\n            backend=""nccl"", init_method=""env://""\n        )\n\n        if info[\'rank\'] > 0:\n            self.master = False\n            self.node = CallbackNode.Worker\n        else:\n            self.node = CallbackNode.Master\n\n    def parse_args_uargs(self):\n        args, config = parse_args_uargs(self.args, [])\n        config = merge_dicts_smart(config, self.grid_config)\n        config = merge_dicts_smart(config, self.params)\n\n        if self.distr_info:\n            self.set_dist_env(config)\n        return args, config\n\n    def _fix_memory(self, experiment):\n        if not torch.cuda.is_available():\n            return\n        max_memory = torch.cuda.get_device_properties(0).total_memory / (\n                    2 ** 30)\n        stages_config = experiment.stages_config\n        for k, v in list(stages_config.items()):\n            query = {}\n            # noinspection PyProtectedMember\n            for kk, vv in experiment._config[\'model_params\'].items():\n                query[kk] = vv\n            for kk, vv in v[\'data_params\'].items():\n                query[kk] = vv\n            variants = self.memory_provider.find(query)\n            variants = [v for v in variants if v.memory < max_memory]\n            if len(variants) == 0:\n                continue\n            variant = max(variants, key=lambda x: x.memory)\n            v[\'data_params\'][\'batch_size\'] = variant.batch_size\n\n    def _checkpoint_fix_config(self, experiment):\n        resume = self.resume\n        if not resume:\n            return\n        if experiment.logdir is None:\n            return\n\n        checkpoint_dir = join(experiment.logdir, \'checkpoints\')\n        os.makedirs(checkpoint_dir, exist_ok=True)\n\n        file = \'last_full.pth\' if resume.get(\'load_last\') else \'best_full.pth\'\n\n        path = join(checkpoint_dir, file)\n        computer = socket.gethostname()\n        if computer != resume[\'master_computer\']:\n            master_computer = self.computer_provider.by_name(\n                resume[\'master_computer\'])\n            path_from = join(\n                master_computer.root_folder, str(resume[\'master_task_id\']),\n                experiment.logdir,\n                \'checkpoints\', file\n            )\n            self.info(\n                f\'copying checkpoint from: computer = \'\n                f\'{resume[""master_computer""]} path_from={path_from} \'\n                f\'path_to={path}\'\n            )\n\n            success = copy_remote(\n                session=self.session,\n                computer_from=resume[\'master_computer\'],\n                path_from=path_from,\n                path_to=path\n            )\n\n            if not success:\n                self.error(\n                    f\'copying from \'\n                    f\'{resume[""master_computer""]}/\'\n                    f\'{path_from} failed\'\n                )\n            else:\n                self.info(\'checkpoint copied successfully\')\n\n        elif self.task.id != resume[\'master_task_id\']:\n            path = join(\n                TASK_FOLDER, str(resume[\'master_task_id\']), experiment.logdir,\n                \'checkpoints\', file\n            )\n            self.info(\n                f\'master_task_id!=task.id, using checkpoint\'\n                f\' from task_id = {resume[""master_task_id""]}\'\n            )\n\n        if not os.path.exists(path):\n            self.info(f\'no checkpoint at {path}\')\n            return\n\n        ckpt = load_checkpoint(path)\n        stages_config = experiment.stages_config\n        for k, v in list(stages_config.items()):\n            if k == ckpt[\'stage\']:\n                stage_epoch = ckpt[\'checkpoint_data\'][\'epoch\'] + 1\n\n                # if it is the last epoch in the stage\n                if stage_epoch >= v[\'state_params\'][\'num_epochs\'] \\\n                        or resume.get(\'load_best\'):\n                    del stages_config[k]\n                    break\n\n                self.checkpoint_stage_epoch = stage_epoch\n                v[\'state_params\'][\'num_epochs\'] -= stage_epoch\n                break\n            del stages_config[k]\n\n        stage = experiment.stages_config[experiment.stages[0]]\n        for k, v in stage[\'callbacks_params\'].items():\n            if v.get(\'callback\') == \'CheckpointCallback\':\n                v[\'resume\'] = path\n\n        self.info(f\'found checkpoint at {path}\')\n\n    def _checkpoint_fix_callback(self, callbacks: dict):\n        def mock(state):\n            pass\n\n        for k, c in callbacks.items():\n            if not isinstance(c, CheckpointCallback):\n                continue\n\n            if c.resume:\n                self.checkpoint_resume = True\n\n            if not self.master:\n                c.on_epoch_end = mock\n                c.on_stage_end = mock\n                c.on_batch_start = mock\n\n    def work(self):\n        args, config = self.parse_args_uargs()\n        set_global_seed(args.seed)\n\n        Experiment, R = import_experiment_and_runner(Path(args.expdir))\n\n        runner_params = config.pop(\'runner_params\', {})\n\n        experiment = Experiment(config)\n        runner: Runner = R(**runner_params)\n\n        self.experiment = experiment\n        self.runner = runner\n\n        stages = experiment.stages[:]\n\n        if self.task.parent:\n            self.parent = self.task_provider.by_id(self.task.parent)\n\n        if self.master:\n            task = self.get_parent_task()\n            task.steps = len(stages)\n            self.task_provider.commit()\n\n        self._checkpoint_fix_config(experiment)\n        self._fix_memory(experiment)\n\n        _get_callbacks = experiment.get_callbacks\n\n        def get_callbacks(stage):\n            res = self.callbacks()\n            for k, v in _get_callbacks(stage).items():\n                res[k] = v\n\n            self._checkpoint_fix_callback(res)\n            return res\n\n        experiment.get_callbacks = get_callbacks\n\n        if experiment.logdir is not None:\n            dump_environment(config, experiment.logdir, args.configs)\n\n        if self.distr_info:\n            info = yaml_load(self.task.additional_info)\n            info[\'resume\'] = {\n                \'master_computer\': self.distr_info[\'master_computer\'],\n                \'master_task_id\': self.task.id - self.distr_info[\'rank\'],\n                \'load_best\': True\n            }\n            self.task.additional_info = yaml_dump(info)\n            self.task_provider.commit()\n\n            experiment.stages_config = {\n                k: v\n                for k, v in experiment.stages_config.items()\n                if k == experiment.stages[0]\n            }\n\n        runner.run_experiment(experiment)\n        if runner.state.exception:\n            raise runner.state.exception\n\n        if self.master and self.trace:\n            traced = trace_model_from_checkpoint(self.experiment.logdir, self)\n            torch.jit.save(traced, self.trace)\n        return {\'stage\': experiment.stages[-1], \'stages\': stages}\n\n\n__all__ = [\'Catalyst\']\n'"
mlcomp/contrib/model/video/resnext3d/__init__.py,0,b'from .resnext3d import ResNeXt3D'
mlcomp/contrib/model/video/resnext3d/r2plus1_util.py,1,"b'#!/usr/bin/env python3\n# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport logging\n\nimport torch.nn as nn\n\n\ndef r2plus1_unit(\n        dim_in,\n        dim_out,\n        temporal_stride,\n        spatial_stride,\n        groups,\n        inplace_relu,\n        bn_eps,\n        bn_mmt,\n        dim_mid=None,\n):\n    """"""\n    Implementation of `R(2+1)D unit <https://arxiv.org/abs/1711.11248>`_.\n    Decompose one 3D conv into one 2D spatial conv and one 1D temporal conv.\n    Choose the middle dimensionality so that the total No. of parameters\n    in 2D spatial conv and 1D temporal conv is unchanged.\n\n    Args:\n        dim_in (int): the channel dimensions of the input.\n        dim_out (int): the channel dimension of the output.\n        temporal_stride (int): the temporal stride of the bottleneck.\n        spatial_stride (int): the spatial_stride of the bottleneck.\n        groups (int): number of groups for the convolution.\n        inplace_relu (bool): calculate the relu on the original input\n            without allocating new memory.\n        bn_eps (float): epsilon for batch norm.\n        bn_mmt (float): momentum for batch norm. Noted that BN momentum in\n            PyTorch = 1 - BN momentum in Caffe2.\n        dim_mid (Optional[int]): If not None, use the provided channel dimension\n            for the output of the 2D spatial conv. If None, compute the output\n            channel dimension of the 2D spatial conv so that the total No. of\n            model parameters remains unchanged.\n    """"""\n    if dim_mid is None:\n        dim_mid = int(\n            dim_out * dim_in * 3 * 3 * 3 / (dim_in * 3 * 3 + dim_out * 3))\n        logging.info(\n            ""dim_in: %d, dim_out: %d. Set dim_mid to %d"" % (\n            dim_in, dim_out, dim_mid)\n        )\n    # 1x3x3 group conv, BN, ReLU\n    conv_middle = nn.Conv3d(\n        dim_in,\n        dim_mid,\n        [1, 3, 3],  # kernel\n        stride=[1, spatial_stride, spatial_stride],\n        padding=[0, 1, 1],\n        groups=groups,\n        bias=False,\n    )\n    conv_middle_bn = nn.BatchNorm3d(dim_mid, eps=bn_eps, momentum=bn_mmt)\n    conv_middle_relu = nn.ReLU(inplace=inplace_relu)\n    # 3x1x1 group conv\n    conv = nn.Conv3d(\n        dim_mid,\n        dim_out,\n        [3, 1, 1],  # kernel\n        stride=[temporal_stride, 1, 1],\n        padding=[1, 0, 0],\n        groups=groups,\n        bias=False,\n    )\n    return nn.Sequential(conv_middle, conv_middle_bn, conv_middle_relu, conv)\n'"
mlcomp/contrib/model/video/resnext3d/resnext3d.py,2,"b'import torch\nimport torch.nn as nn\n\nfrom .resnext3d_stage import ResStage\nfrom .resnext3d_stem import R2Plus1DStem, ResNeXt3DStem\n\nmodel_stems = {\n    ""r2plus1d_stem"": R2Plus1DStem,\n    ""resnext3d_stem"": ResNeXt3DStem,\n    # For more types of model stem, add them below\n}\n\n\nclass FullyConvolutionalLinear(nn.Module):\n    def __init__(self, dim_in, num_classes):\n        super(FullyConvolutionalLinear, self).__init__()\n        # Perform FC in a fully convolutional manner. The FC layer will be\n        # initialized with a different std comparing to convolutional layers.\n        self.projection = nn.Linear(dim_in, num_classes, bias=True)\n\n    def forward(self, x):\n        x = x.view(x.shape[0], -1)\n        x = self.projection(x)\n        return x\n\n\nclass ResNeXt3D(torch.nn.Module):\n    """"""\n    Implementation of:\n        1. Conventional `post-activated 3D ResNe(X)t <https://arxiv.org/\n        abs/1812.03982>`_.\n\n        2. `Pre-activated 3D ResNe(X)t <https://arxiv.org/abs/1811.12814>`_.\n        The model consists of one stem, a number of stages, and one or multiple\n        heads that are attached to different blocks in the stage.\n    """"""\n\n    def __init__(\n            self,\n            input_planes: int = 3,\n            skip_transformation_type: str = \'postactivated_shortcut\',\n            residual_transformation_type: str = \'basic_transformation\',\n            num_blocks: list = (2, 2, 2, 2),\n            stem_name: str = \'resnext3d_stem\',\n            stem_planes: int = 64,\n            stem_temporal_kernel: int = 3,\n            stem_spatial_kernel: int = 7,\n            stem_maxpool: bool = False,\n            stage_planes: int = 64,\n            stage_temporal_kernel_basis: list = ([3], [3], [3], [3]),\n            temporal_conv_1x1: list = (False, False, False, False),\n            stage_temporal_stride: list = (1, 2, 2, 2),\n            stage_spatial_stride: list = (1, 2, 2, 2),\n            num_groups: int = 1,\n            width_per_group: int = 64,\n            zero_init_residual_transform: bool = False,\n            in_plane: int = 512,\n            num_classes: int = 2\n    ):\n        """"""\n        Args:\n            input_planes (int): the channel dimension of the input.\n                Normally 3 is used for rgb input.\n            skip_transformation_type (str): the type of skip transformation.\n                residual_transformation_type (str):\n                the type of residual transformation.\n            num_blocks (list): list of the number of blocks in stages.\n            stem_name (str): name of model stem.\n            stem_planes (int): the output dimension\n                of the convolution in the model stem.\n            stem_temporal_kernel (int): the temporal kernel\n                size of the convolution\n                in the model stem.\n            stem_spatial_kernel (int): the spatial kernel size\n                of the convolution in the model stem.\n            stem_maxpool (bool): If true, perform max pooling.\n            stage_planes (int): the output channel dimension\n                of the 1st residual stage\n            stage_temporal_kernel_basis (list): Basis of temporal kernel\n                sizes for each of the stage.\n            temporal_conv_1x1 (bool): Only useful for BottleneckTransformation.\n                In a pathaway, if True, do temporal convolution\n                in the first 1x1\n                Conv3d. Otherwise, do it in the second 3x3 Conv3d.\n            stage_temporal_stride (int): the temporal stride of the residual\n                transformation.\n            stage_spatial_stride (int): the spatial stride of the the residual\n                transformation.\n            num_groups (int): number of groups for the convolution.\n                num_groups = 1 is for standard ResNet like networks, and\n                num_groups > 1 is for ResNeXt like networks.\n            width_per_group (int): Number of channels per group in 2nd (group)\n                conv in the residual transformation in the first stage\n            zero_init_residual_transform (bool): if true, the weight of last\n                operation, which could be either BatchNorm3D in post-activated\n                transformation or Conv3D in pre-activated transformation,\n                in the residual transformation is initialized to zero\n            pool_size: for fully convolution layer\n            in_plane: for fully convolution layer\n            num_classes: number of classes\n        """"""\n\n        super().__init__()\n\n        num_stages = len(num_blocks)\n        out_planes = [stage_planes * 2 ** i for i in range(num_stages)]\n        in_planes = [stem_planes] + out_planes[:-1]\n        inner_planes = [\n            num_groups * width_per_group * 2 ** i for i in range(num_stages)\n        ]\n\n        self.stem = model_stems[stem_name](\n            stem_temporal_kernel,\n            stem_spatial_kernel,\n            input_planes,\n            stem_planes,\n            stem_maxpool,\n        )\n\n        stages = []\n        for s in range(num_stages):\n            stage = ResStage(\n                s + 1,\n                # stem is viewed as stage 0, and following stages start from 1\n                [in_planes[s]],\n                [out_planes[s]],\n                [inner_planes[s]],\n                [stage_temporal_kernel_basis[s]],\n                [temporal_conv_1x1[s]],\n                [stage_temporal_stride[s]],\n                [stage_spatial_stride[s]],\n                [num_blocks[s]],\n                [num_groups],\n                skip_transformation_type,\n                residual_transformation_type,\n                disable_pre_activation=(s == 0),\n                final_stage=(s == (num_stages - 1)),\n            )\n            stages.append(stage)\n\n        self.stages = nn.Sequential(*stages)\n        self._init_parameter(zero_init_residual_transform)\n\n        self.final_avgpool = nn.AdaptiveAvgPool1d(in_plane)\n        self.head_fcl = FullyConvolutionalLinear(\n            in_plane, num_classes\n        )\n\n    def _init_parameter(self, zero_init_residual_transform):\n        for m in self.modules():\n            if isinstance(m, nn.Conv3d):\n                if (\n                        hasattr(m, ""final_transform_op"")\n                        and m.final_transform_op\n                        and zero_init_residual_transform\n                ):\n                    nn.init.constant_(m.weight, 0)\n                else:\n                    nn.init.kaiming_normal_(\n                        m.weight, mode=""fan_out"", nonlinearity=""relu""\n                    )\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm3d) and m.affine:\n                if (\n                        hasattr(m, ""final_transform_op"")\n                        and m.final_transform_op\n                        and zero_init_residual_transform\n                ):\n                    batchnorm_weight = 0.0\n                else:\n                    batchnorm_weight = 1.0\n                nn.init.constant_(m.weight, batchnorm_weight)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                nn.init.normal_(m.weight, mean=0.0, std=0.01)\n                nn.init.constant_(m.bias, 0)\n\n    def forward(self, x):\n        """"""\n        Args:\n            x: Tensor(B, T, W, H, C)\n        """"""\n        out = self.stem([x])\n        out = self.stages(out)[0]\n        out = out.view((out.shape[0], 1, -1))\n        out = self.final_avgpool(out)\n        out = self.head_fcl(out)\n\n        return out\n'"
mlcomp/contrib/model/video/resnext3d/resnext3d_block.py,1,"b'#!/usr/bin/env python3\n# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport torch.nn as nn\n\nfrom .r2plus1_util import r2plus1_unit\n\n\nclass BasicTransformation(nn.Module):\n    """"""\n    Basic transformation: 3x3x3 group conv, 3x3x3 group conv\n    """"""\n\n    def __init__(\n        self,\n        dim_in,\n        dim_out,\n        temporal_stride,\n        spatial_stride,\n        groups,\n        inplace_relu=True,\n        bn_eps=1e-5,\n        bn_mmt=0.1,\n        **kwargs\n    ):\n        """"""\n        Args:\n            dim_in (int): the channel dimensions of the input.\n            dim_out (int): the channel dimension of the output.\n            temporal_stride (int): the temporal stride of the bottleneck.\n            spatial_stride (int): the spatial_stride of the bottleneck.\n            groups (int): number of groups for the convolution.\n            inplace_relu (bool): calculate the relu on the original input\n                without allocating new memory.\n            bn_eps (float): epsilon for batch norm.\n            bn_mmt (float): momentum for batch norm. Noted that BN momentum in\n                PyTorch = 1 - BN momentum in Caffe2.\n        """"""\n        super(BasicTransformation, self).__init__()\n        self._construct_model(\n            dim_in,\n            dim_out,\n            temporal_stride,\n            spatial_stride,\n            groups,\n            inplace_relu,\n            bn_eps,\n            bn_mmt,\n        )\n\n    def _construct_model(\n        self,\n        dim_in,\n        dim_out,\n        temporal_stride,\n        spatial_stride,\n        groups,\n        inplace_relu,\n        bn_eps,\n        bn_mmt,\n    ):\n        # 3x3x3 group conv, BN, ReLU.\n        branch2a = nn.Conv3d(\n            dim_in,\n            dim_out,\n            [3, 3, 3],  # kernel\n            stride=[temporal_stride, spatial_stride, spatial_stride],\n            padding=[1, 1, 1],\n            groups=groups,\n            bias=False,\n        )\n        branch2a_bn = nn.BatchNorm3d(dim_out, eps=bn_eps, momentum=bn_mmt)\n        branch2a_relu = nn.ReLU(inplace=inplace_relu)\n        # 3x3x3 group conv, BN, ReLU.\n        branch2b = nn.Conv3d(\n            dim_out,\n            dim_out,\n            [3, 3, 3],  # kernel\n            stride=[1, 1, 1],\n            padding=[1, 1, 1],\n            groups=groups,\n            bias=False,\n        )\n        branch2b_bn = nn.BatchNorm3d(dim_out, eps=bn_eps, momentum=bn_mmt)\n        branch2b_bn.final_transform_op = True\n\n        self.transform = nn.Sequential(\n            branch2a, branch2a_bn, branch2a_relu, branch2b, branch2b_bn\n        )\n\n    def forward(self, x):\n        return self.transform(x)\n\n\nclass BasicR2Plus1DTransformation(BasicTransformation):\n    """"""\n    Basic transformation: 3x3x3 group conv, 3x3x3 group conv\n    """"""\n\n    def __init__(\n        self,\n        dim_in,\n        dim_out,\n        temporal_stride,\n        spatial_stride,\n        groups,\n        inplace_relu=True,\n        bn_eps=1e-5,\n        bn_mmt=0.1,\n        **kwargs\n    ):\n        """"""\n        Args:\n            dim_in (int): the channel dimensions of the input.\n            dim_out (int): the channel dimension of the output.\n            temporal_stride (int): the temporal stride of the bottleneck.\n            spatial_stride (int): the spatial_stride of the bottleneck.\n            groups (int): number of groups for the convolution.\n            inplace_relu (bool): calculate the relu on the original input\n                without allocating new memory.\n            bn_eps (float): epsilon for batch norm.\n            bn_mmt (float): momentum for batch norm. Noted that BN momentum in\n                PyTorch = 1 - BN momentum in Caffe2.\n        """"""\n        super(BasicR2Plus1DTransformation, self).__init__(\n            dim_in,\n            dim_out,\n            temporal_stride,\n            spatial_stride,\n            groups,\n            inplace_relu=inplace_relu,\n            bn_eps=bn_eps,\n            bn_mmt=bn_mmt,\n        )\n\n    def _construct_model(\n        self,\n        dim_in,\n        dim_out,\n        temporal_stride,\n        spatial_stride,\n        groups,\n        inplace_relu,\n        bn_eps,\n        bn_mmt,\n    ):\n        # Implementation of R(2+1)D operation <https://arxiv.org/abs/1711.11248>.\n        # decompose the original 3D conv into one 2D spatial conv and one\n        # 1D temporal conv\n        branch2a = r2plus1_unit(\n            dim_in,\n            dim_out,\n            temporal_stride,\n            spatial_stride,\n            groups,\n            inplace_relu,\n            bn_eps,\n            bn_mmt,\n        )\n        branch2a_bn = nn.BatchNorm3d(dim_out, eps=bn_eps, momentum=bn_mmt)\n        branch2a_relu = nn.ReLU(inplace=inplace_relu)\n\n        branch2b = r2plus1_unit(\n            dim_out,\n            dim_out,\n            1,  # temporal_stride\n            1,  # spatial_stride\n            groups,\n            inplace_relu,\n            bn_eps,\n            bn_mmt,\n        )\n        branch2b_bn = nn.BatchNorm3d(dim_out, eps=bn_eps, momentum=bn_mmt)\n        branch2b_bn.final_transform_op = True\n\n        self.transform = nn.Sequential(\n            branch2a, branch2a_bn, branch2a_relu, branch2b, branch2b_bn\n        )\n\n\nclass PostactivatedBottleneckTransformation(nn.Module):\n    """"""\n    Bottleneck transformation: Tx1x1, 1x3x3, 1x1x1, where T is the size of\n        temporal kernel.\n    """"""\n\n    def __init__(\n        self,\n        dim_in,\n        dim_out,\n        temporal_stride,\n        spatial_stride,\n        num_groups,\n        dim_inner,\n        temporal_kernel_size=3,\n        temporal_conv_1x1=True,\n        spatial_stride_1x1=False,\n        inplace_relu=True,\n        bn_eps=1e-5,\n        bn_mmt=0.1,\n        **kwargs\n    ):\n        """"""\n        Args:\n            dim_in (int): the channel dimensions of the input.\n            dim_out (int): the channel dimension of the output.\n            temporal_kernel_size (int): the temporal kernel sizes of the middle\n                convolution in the bottleneck.\n            temporal_conv_1x1 (bool): if True, do temporal convolution in the fist\n                1x1 Conv3d. Otherwise, do it in the second 3x3 Conv3d\n            temporal_stride (int): the temporal stride of the bottleneck.\n            spatial_stride (int): the spatial_stride of the bottleneck.\n            num_groups (int): number of groups for the convolution.\n            dim_inner (int): the inner dimension of the block.\n                is for standard ResNet like networks, and num_groups>1 is for\n                ResNeXt like networks.\n            spatial_stride_1x1 (bool): if True, apply spatial_stride to 1x1 conv.\n            inplace_relu (bool): calculate the relu on the original input\n                without allocating new memory.\n            bn_eps (float): epsilon for batch norm.\n            bn_mmt (float): momentum for batch norm. Noted that BN momentum in\n                PyTorch = 1 - BN momentum in Caffe2.\n        """"""\n        super(PostactivatedBottleneckTransformation, self).__init__()\n        (temporal_kernel_size_1x1, temporal_kernel_size_3x3) = (\n            (temporal_kernel_size, 1)\n            if temporal_conv_1x1\n            else (1, temporal_kernel_size)\n        )\n        # MSRA -> stride=2 is on 1x1; TH/C2 -> stride=2 is on 3x3.\n        (str1x1, str3x3) = (\n            (spatial_stride, 1) if spatial_stride_1x1 else (1, spatial_stride)\n        )\n        # Tx1x1 conv, BN, ReLU.\n        self.branch2a = nn.Conv3d(\n            dim_in,\n            dim_inner,\n            kernel_size=[temporal_kernel_size_1x1, 1, 1],\n            stride=[1, str1x1, str1x1],\n            padding=[temporal_kernel_size_1x1 // 2, 0, 0],\n            bias=False,\n        )\n        self.branch2a_bn = nn.BatchNorm3d(dim_inner, eps=bn_eps, momentum=bn_mmt)\n        self.branch2a_relu = nn.ReLU(inplace=inplace_relu)\n        # Tx3x3 group conv, BN, ReLU.\n        self.branch2b = nn.Conv3d(\n            dim_inner,\n            dim_inner,\n            [temporal_kernel_size_3x3, 3, 3],\n            stride=[temporal_stride, str3x3, str3x3],\n            padding=[temporal_kernel_size_3x3 // 2, 1, 1],\n            groups=num_groups,\n            bias=False,\n        )\n        self.branch2b_bn = nn.BatchNorm3d(dim_inner, eps=bn_eps, momentum=bn_mmt)\n        self.branch2b_relu = nn.ReLU(inplace=inplace_relu)\n        # 1x1x1 conv, BN.\n        self.branch2c = nn.Conv3d(\n            dim_inner,\n            dim_out,\n            kernel_size=[1, 1, 1],\n            stride=[1, 1, 1],\n            padding=[0, 0, 0],\n            bias=False,\n        )\n        self.branch2c_bn = nn.BatchNorm3d(dim_out, eps=bn_eps, momentum=bn_mmt)\n        self.branch2c_bn.final_transform_op = True\n\n    def forward(self, x):\n        # Explicitly forward every layer.\n        # Branch2a.\n        x = self.branch2a(x)\n        x = self.branch2a_bn(x)\n        x = self.branch2a_relu(x)\n\n        # Branch2b.\n        x = self.branch2b(x)\n        x = self.branch2b_bn(x)\n        x = self.branch2b_relu(x)\n\n        # Branch2c\n        x = self.branch2c(x)\n        x = self.branch2c_bn(x)\n        return x\n\n\nclass PreactivatedBottleneckTransformation(nn.Module):\n    """"""\n    Bottleneck transformation with pre-activation, which includes BatchNorm3D\n        and ReLu. Conv3D kernsl are Tx1x1, 1x3x3, 1x1x1, where T is the size of\n        temporal kernel (https://arxiv.org/abs/1603.05027).\n    """"""\n\n    def __init__(\n        self,\n        dim_in,\n        dim_out,\n        temporal_stride,\n        spatial_stride,\n        num_groups,\n        dim_inner,\n        temporal_kernel_size=3,\n        temporal_conv_1x1=True,\n        spatial_stride_1x1=False,\n        inplace_relu=True,\n        bn_eps=1e-5,\n        bn_mmt=0.1,\n        disable_pre_activation=False,\n        **kwargs\n    ):\n        """"""\n        Args:\n            dim_in (int): the channel dimensions of the input.\n            dim_out (int): the channel dimension of the output.\n            temporal_kernel_size (int): the temporal kernel sizes of the middle\n                convolution in the bottleneck.\n            temporal_conv_1x1 (bool): if True, do temporal convolution in the fist\n                1x1 Conv3d. Otherwise, do it in the second 3x3 Conv3d\n            temporal_stride (int): the temporal stride of the bottleneck.\n            spatial_stride (int): the spatial_stride of the bottleneck.\n            num_groups (int): number of groups for the convolution.\n            dim_inner (int): the inner dimension of the block.\n                is for standard ResNet like networks, and num_groups>1 is for\n                ResNeXt like networks.\n            spatial_stride_1x1 (bool): if True, apply spatial_stride to 1x1 conv.\n            inplace_relu (bool): calculate the relu on the original input\n                without allocating new memory.\n            bn_eps (float): epsilon for batch norm.\n            bn_mmt (float): momentum for batch norm. Noted that BN momentum in\n                PyTorch = 1 - BN momentum in Caffe2.\n            disable_pre_activation (bool): If true, disable pre activation,\n                including BatchNorm3D and ReLU.\n        """"""\n        super(PreactivatedBottleneckTransformation, self).__init__()\n        (temporal_kernel_size_1x1, temporal_kernel_size_3x3) = (\n            (temporal_kernel_size, 1)\n            if temporal_conv_1x1\n            else (1, temporal_kernel_size)\n        )\n        (str1x1, str3x3) = (\n            (spatial_stride, 1) if spatial_stride_1x1 else (1, spatial_stride)\n        )\n\n        self.disable_pre_activation = disable_pre_activation\n        if not disable_pre_activation:\n            self.branch2a_bn = nn.BatchNorm3d(dim_in, eps=bn_eps, momentum=bn_mmt)\n            self.branch2a_relu = nn.ReLU(inplace=inplace_relu)\n\n        self.branch2a = nn.Conv3d(\n            dim_in,\n            dim_inner,\n            kernel_size=[temporal_kernel_size_1x1, 1, 1],\n            stride=[1, str1x1, str1x1],\n            padding=[temporal_kernel_size_1x1 // 2, 0, 0],\n            bias=False,\n        )\n        # Tx3x3 group conv, BN, ReLU.\n        self.branch2b_bn = nn.BatchNorm3d(dim_inner, eps=bn_eps, momentum=bn_mmt)\n        self.branch2b_relu = nn.ReLU(inplace=inplace_relu)\n        self.branch2b = nn.Conv3d(\n            dim_inner,\n            dim_inner,\n            [temporal_kernel_size_3x3, 3, 3],\n            stride=[temporal_stride, str3x3, str3x3],\n            padding=[temporal_kernel_size_3x3 // 2, 1, 1],\n            groups=num_groups,\n            bias=False,\n        )\n        # 1x1x1 conv, BN.\n        self.branch2c_bn = nn.BatchNorm3d(dim_inner, eps=bn_eps, momentum=bn_mmt)\n        self.branch2c_relu = nn.ReLU(inplace=inplace_relu)\n        self.branch2c = nn.Conv3d(\n            dim_inner,\n            dim_out,\n            kernel_size=[1, 1, 1],\n            stride=[1, 1, 1],\n            padding=[0, 0, 0],\n            bias=False,\n        )\n        self.branch2c.final_transform_op = True\n\n    def forward(self, x):\n        # Branch2a\n        if not self.disable_pre_activation:\n            x = self.branch2a_bn(x)\n            x = self.branch2a_relu(x)\n        x = self.branch2a(x)\n        # Branch2b\n        x = self.branch2b_bn(x)\n        x = self.branch2b_relu(x)\n        x = self.branch2b(x)\n        # Branch2c\n        x = self.branch2c_bn(x)\n        x = self.branch2c_relu(x)\n        x = self.branch2c(x)\n        return x\n\n\nresidual_transformations = {\n    ""basic_r2plus1d_transformation"": BasicR2Plus1DTransformation,\n    ""basic_transformation"": BasicTransformation,\n    ""postactivated_bottleneck_transformation"": PostactivatedBottleneckTransformation,\n    ""preactivated_bottleneck_transformation"": PreactivatedBottleneckTransformation,\n    # For more types of residual transformations, add them below\n}\n\n\nclass PostactivatedShortcutTransformation(nn.Module):\n    """"""\n    Skip connection used in ResNet3D model.\n    """"""\n\n    def __init__(\n        self,\n        dim_in,\n        dim_out,\n        temporal_stride,\n        spatial_stride,\n        bn_eps=1e-5,\n        bn_mmt=0.1,\n        **kwargs\n    ):\n        super(PostactivatedShortcutTransformation, self).__init__()\n        # Use skip connection with projection if dim or spatial/temporal res change.\n        assert (dim_in != dim_out) or (spatial_stride != 1) or (temporal_stride != 1)\n        self.branch1 = nn.Conv3d(\n            dim_in,\n            dim_out,\n            kernel_size=1,\n            stride=[temporal_stride, spatial_stride, spatial_stride],\n            padding=0,\n            bias=False,\n        )\n        self.branch1_bn = nn.BatchNorm3d(dim_out, eps=bn_eps, momentum=bn_mmt)\n\n    def forward(self, x):\n        return self.branch1_bn(self.branch1(x))\n\n\nclass PreactivatedShortcutTransformation(nn.Module):\n    """"""\n    Skip connection with pre-activation, which includes BatchNorm3D and ReLU,\n        in ResNet3D model (https://arxiv.org/abs/1603.05027).\n    """"""\n\n    def __init__(\n        self,\n        dim_in,\n        dim_out,\n        temporal_stride,\n        spatial_stride,\n        inplace_relu=True,\n        bn_eps=1e-5,\n        bn_mmt=0.1,\n        disable_pre_activation=False,\n        **kwargs\n    ):\n        super(PreactivatedShortcutTransformation, self).__init__()\n        # Use skip connection with projection if dim or spatial/temporal res change.\n        assert (dim_in != dim_out) or (spatial_stride != 1) or (temporal_stride != 1)\n        if not disable_pre_activation:\n            self.branch1_bn = nn.BatchNorm3d(dim_in, eps=bn_eps, momentum=bn_mmt)\n            self.branch1_relu = nn.ReLU(inplace=inplace_relu)\n        self.branch1 = nn.Conv3d(\n            dim_in,\n            dim_out,\n            kernel_size=1,\n            stride=[temporal_stride, spatial_stride, spatial_stride],\n            padding=0,\n            bias=False,\n        )\n\n    def forward(self, x):\n        if hasattr(self, ""branch1_bn"") and hasattr(self, ""branch1_relu""):\n            x = self.branch1_relu(self.branch1_bn(x))\n        x = self.branch1(x)\n        return x\n\n\nskip_transformations = {\n    ""postactivated_shortcut"": PostactivatedShortcutTransformation,\n    ""preactivated_shortcut"": PreactivatedShortcutTransformation,\n    # For more types of skip transformations, add them below\n}\n\n\nclass ResBlock(nn.Module):\n    """"""\n    Residual block with skip connection.\n    """"""\n\n    def __init__(\n        self,\n        dim_in,\n        dim_out,\n        dim_inner,\n        temporal_kernel_size,\n        temporal_conv_1x1,\n        temporal_stride,\n        spatial_stride,\n        skip_transformation_type,\n        residual_transformation_type,\n        num_groups=1,\n        inplace_relu=True,\n        bn_eps=1e-5,\n        bn_mmt=0.1,\n        disable_pre_activation=False,\n    ):\n        """"""\n        ResBlock class constructs redisual blocks. More details can be found in:\n            ""Deep residual learning for image recognition.""\n            https://arxiv.org/abs/1512.03385\n        Args:\n            dim_in (int): the channel dimensions of the input.\n            dim_out (int): the channel dimension of the output.\n            dim_inner (int): the inner dimension of the block.\n            temporal_kernel_size (int): the temporal kernel sizes of the middle\n                convolution in the bottleneck.\n            temporal_conv_1x1 (bool): Only useful for PostactivatedBottleneckTransformation.\n                if True, do temporal convolution in the fist 1x1 Conv3d.\n                Otherwise, do it in the second 3x3 Conv3d\n            temporal_stride (int): the temporal stride of the bottleneck.\n            spatial_stride (int): the spatial_stride of the bottleneck.\n            stride (int): the stride of the bottleneck.\n            skip_transformation_type (str): the type of skip transformation\n            residual_transformation_type (str): the type of residual transformation\n            num_groups (int): number of groups for the convolution. num_groups=1\n                is for standard ResNet like networks, and num_groups>1 is for\n                ResNeXt like networks.\n            disable_pre_activation (bool): If true, disable the preactivation,\n                which includes BatchNorm3D and ReLU.\n        """"""\n        super(ResBlock, self).__init__()\n\n        assert skip_transformation_type in skip_transformations, (\n            ""unknown skip transformation: %s"" % skip_transformation_type\n        )\n\n        if (dim_in != dim_out) or (spatial_stride != 1) or (temporal_stride != 1):\n            self.skip = skip_transformations[skip_transformation_type](\n                dim_in,\n                dim_out,\n                temporal_stride,\n                spatial_stride,\n                bn_eps=bn_eps,\n                bn_mmt=bn_mmt,\n                disable_pre_activation=disable_pre_activation,\n            )\n\n        assert residual_transformation_type in residual_transformations, (\n            ""unknown residual transformation: %s"" % residual_transformation_type\n        )\n        self.residual = residual_transformations[residual_transformation_type](\n            dim_in,\n            dim_out,\n            temporal_stride,\n            spatial_stride,\n            num_groups,\n            dim_inner,\n            temporal_kernel_size=temporal_kernel_size,\n            temporal_conv_1x1=temporal_conv_1x1,\n            disable_pre_activation=disable_pre_activation,\n        )\n        self.relu = nn.ReLU(inplace_relu)\n\n    def forward(self, x):\n        if hasattr(self, ""skip""):\n            x = self.skip(x) + self.residual(x)\n        else:\n            x = x + self.residual(x)\n        x = self.relu(x)\n        return x\n'"
mlcomp/contrib/model/video/resnext3d/resnext3d_stage.py,1,"b'#!/usr/bin/env python3\n# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nfrom collections import OrderedDict\n\nimport torch.nn as nn\n\nfrom .resnext3d_block import ResBlock\n\n\nclass ResStageBase(nn.Module):\n    def __init__(\n        self,\n        stage_idx,\n        dim_in,\n        dim_out,\n        dim_inner,\n        temporal_kernel_basis,\n        temporal_conv_1x1,\n        temporal_stride,\n        spatial_stride,\n        num_blocks,\n        num_groups,\n    ):\n        super(ResStageBase, self).__init__()\n\n        assert (\n            len(\n                {\n                    len(dim_in),\n                    len(dim_out),\n                    len(temporal_kernel_basis),\n                    len(temporal_conv_1x1),\n                    len(temporal_stride),\n                    len(spatial_stride),\n                    len(num_blocks),\n                    len(dim_inner),\n                    len(num_groups),\n                }\n            )\n            == 1\n        )\n\n        self.stage_idx = stage_idx\n        self.num_blocks = num_blocks\n        self.num_pathways = len(self.num_blocks)\n\n        self.temporal_kernel_sizes = [\n            (temporal_kernel_basis[i] * num_blocks[i])[: num_blocks[i]]\n            for i in range(len(temporal_kernel_basis))\n        ]\n\n    def _block_name(self, pathway_idx, stage_idx, block_idx):\n        return ""pathway{}-stage{}-block{}"".format(pathway_idx, stage_idx, block_idx)\n\n    def _pathway_name(self, pathway_idx):\n        return ""pathway{}"".format(pathway_idx)\n\n    def forward(self, inputs):\n        output = []\n        for p in range(self.num_pathways):\n            x = inputs[p]\n            pathway_module = getattr(self, self._pathway_name(p))\n            output.append(pathway_module(x))\n        return output\n\n\nclass ResStage(ResStageBase):\n    """"""\n    Stage of 3D ResNet. It expects to have one or more tensors as input for\n        single pathway (C2D, I3D, SlowOnly), and multi-pathway (SlowFast) cases.\n        More details can be found here:\n        ""Slowfast networks for video recognition.""\n        https://arxiv.org/pdf/1812.03982.pdf\n    """"""\n\n    def __init__(\n        self,\n        stage_idx,\n        dim_in,\n        dim_out,\n        dim_inner,\n        temporal_kernel_basis,\n        temporal_conv_1x1,\n        temporal_stride,\n        spatial_stride,\n        num_blocks,\n        num_groups,\n        skip_transformation_type,\n        residual_transformation_type,\n        block_callback=None,\n        inplace_relu=True,\n        bn_eps=1e-5,\n        bn_mmt=0.1,\n        disable_pre_activation=False,\n        final_stage=False,\n    ):\n        """"""\n        The `__init__` method of any subclass should also contain these arguments.\n        ResStage builds p streams, where p can be greater or equal to one.\n        Args:\n            stage_idx (int): integer index of stage.\n            dim_in (list): list of p the channel dimensions of the input.\n                Different channel dimensions control the input dimension of\n                different pathways.\n            dim_out (list): list of p the channel dimensions of the output.\n                Different channel dimensions control the input dimension of\n                different pathways.\n            dim_inner (list): list of the p inner channel dimensions of the\n                input.\n                Different channel dimensions control the input dimension of\n                different pathways.\n            temporal_kernel_basis (list): Basis of temporal kernel sizes for each of\n                the stage.\n            temporal_conv_1x1 (list): Only useful for BottleneckBlock.\n                In a pathaway, if True, do temporal convolution in the fist 1x1 Conv3d.\n                Otherwise, do it in the second 3x3 Conv3d\n            temporal_stride (list): the temporal stride of the bottleneck.\n            spatial_stride (list): the spatial_stride of the bottleneck.\n            num_blocks (list): list of p numbers of blocks for each of the\n                pathway.\n            num_groups (list): list of number of p groups for the convolution.\n                num_groups=1 is for standard ResNet like networks, and\n                num_groups>1 is for ResNeXt like networks.\n            skip_transformation_type (str): the type of skip transformation\n            residual_transformation_type (str): the type of residual transformation\n            block_callback (function object): a callback function to be called with\n                residual block and its name as input arguments\n            disable_pre_activation (bool): If true, disable the preactivation,\n                which includes BatchNorm3D and ReLU.\n            final_stage (bool): If true, this is the last stage in the model.\n        """"""\n        super(ResStage, self).__init__(\n            stage_idx,\n            dim_in,\n            dim_out,\n            dim_inner,\n            temporal_kernel_basis,\n            temporal_conv_1x1,\n            temporal_stride,\n            spatial_stride,\n            num_blocks,\n            num_groups,\n        )\n\n        for p in range(self.num_pathways):\n            blocks = []\n            for i in range(self.num_blocks[p]):\n                # Retrieve the transformation function.\n                # Construct the block.\n                block_disable_pre_activation = (\n                    True if disable_pre_activation and i == 0 else False\n                )\n                res_block = ResBlock(\n                    dim_in[p] if i == 0 else dim_out[p],\n                    dim_out[p],\n                    dim_inner[p],\n                    self.temporal_kernel_sizes[p][i],\n                    temporal_conv_1x1[p],\n                    temporal_stride[p] if i == 0 else 1,\n                    spatial_stride[p] if i == 0 else 1,\n                    skip_transformation_type,\n                    residual_transformation_type,\n                    num_groups=num_groups[p],\n                    inplace_relu=inplace_relu,\n                    bn_eps=bn_eps,\n                    bn_mmt=bn_mmt,\n                    disable_pre_activation=block_disable_pre_activation,\n                )\n                block_name = self._block_name(p, stage_idx, i)\n                if block_callback:\n                    res_block = block_callback(block_name, res_block)\n                blocks.append((block_name, res_block))\n\n            if final_stage and (\n                residual_transformation_type == ""preactivated_bottleneck_transformation""\n            ):\n                # For pre-activation residual transformation, we conduct\n                # activation in the final stage before continuing forward pass\n                # through the head\n                activate_bn = nn.BatchNorm3d(dim_out[p])\n                activate_relu = nn.ReLU(inplace=True)\n                activate_bn_name = ""-"".join([block_name, ""bn""])\n                activate_relu_name = ""-"".join([block_name, ""relu""])\n                if block_callback:\n                    activate_relu = block_callback(activate_relu_name, activate_relu)\n                blocks.append((activate_bn_name, activate_bn))\n                blocks.append((activate_relu_name, activate_relu))\n\n            self.add_module(self._pathway_name(p), nn.Sequential(OrderedDict(blocks)))\n'"
mlcomp/contrib/model/video/resnext3d/resnext3d_stem.py,1,"b'#!/usr/bin/env python3\n# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport torch.nn as nn\n\nfrom .r2plus1_util import r2plus1_unit\n\n\nclass ResNeXt3DStemSinglePathway(nn.Module):\n    """"""\n    ResNe(X)t 3D basic stem module. Assume a single pathway.\n    Performs spatiotemporal Convolution, BN, and Relu following by a\n        spatiotemporal pooling.\n    """"""\n\n    def __init__(\n            self,\n            dim_in,\n            dim_out,\n            kernel,\n            stride,\n            padding,\n            maxpool=True,\n            inplace_relu=True,\n            bn_eps=1e-5,\n            bn_mmt=0.1,\n    ):\n        """"""\n        The `__init__` method of any subclass should also contain these arguments.\n\n        Args:\n            dim_in (int): the channel dimension of the input. Normally 3 is used\n                for rgb input\n            dim_out (int): the output dimension of the convolution in the stem\n                layer.\n            kernel (list): the kernel size of the convolution in the stem layer.\n                temporal kernel size, height kernel size, width kernel size in\n                order.\n            stride (list): the stride size of the convolution in the stem layer.\n                temporal kernel stride, height kernel size, width kernel size in\n                order.\n            padding (int): the padding size of the convolution in the stem\n                layer, temporal padding size, height padding size, width\n                padding size in order.\n            maxpool (bool): If true, perform max pooling.\n            inplace_relu (bool): calculate the relu on the original input\n                without allocating new memory.\n            bn_eps (float): epsilon for batch norm.\n            bn_mmt (float): momentum for batch norm. Noted that BN momentum in\n                PyTorch = 1 - BN momentum in Caffe2.\n        """"""\n        super(ResNeXt3DStemSinglePathway, self).__init__()\n        self.kernel = kernel\n        self.stride = stride\n        self.padding = padding\n        self.inplace_relu = inplace_relu\n        self.bn_eps = bn_eps\n        self.bn_mmt = bn_mmt\n        self.maxpool = maxpool\n\n        # Construct the stem layer.\n        self._construct_stem(dim_in, dim_out)\n\n    def _construct_stem(self, dim_in, dim_out):\n        self.conv = nn.Conv3d(\n            dim_in,\n            dim_out,\n            self.kernel,\n            stride=self.stride,\n            padding=self.padding,\n            bias=False,\n        )\n        self.bn = nn.BatchNorm3d(dim_out, eps=self.bn_eps,\n                                 momentum=self.bn_mmt)\n        self.relu = nn.ReLU(self.inplace_relu)\n        if self.maxpool:\n            self.pool_layer = nn.MaxPool3d(\n                kernel_size=[1, 3, 3], stride=[1, 2, 2], padding=[0, 1, 1]\n            )\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.relu(x)\n        if self.maxpool:\n            x = self.pool_layer(x)\n        return x\n\n\nclass R2Plus1DStemSinglePathway(ResNeXt3DStemSinglePathway):\n    """"""\n    R(2+1)D basic stem module. Assume a single pathway.\n    Performs spatial convolution, temporal convolution, BN, and Relu following by a\n        spatiotemporal pooling.\n    """"""\n\n    def __init__(\n            self,\n            dim_in,\n            dim_out,\n            kernel,\n            stride,\n            padding,\n            maxpool=True,\n            inplace_relu=True,\n            bn_eps=1e-5,\n            bn_mmt=0.1,\n    ):\n        """"""\n        The `__init__` method of any subclass should also contain these arguments.\n\n        Args:\n            dim_in (int): the channel dimension of the input. Normally 3 is used\n                for rgb input\n            dim_out (int): the output dimension of the convolution in the stem\n                layer.\n            kernel (list): the kernel size of the convolution in the stem layer.\n                temporal kernel size, height kernel size, width kernel size in\n                order.\n            stride (list): the stride size of the convolution in the stem layer.\n                temporal kernel stride, height kernel size, width kernel size in\n                order.\n            padding (int): the padding size of the convolution in the stem\n                layer, temporal padding size, height padding size, width\n                padding size in order.\n            maxpool (bool): If true, perform max pooling.\n            inplace_relu (bool): calculate the relu on the original input\n                without allocating new memory.\n            bn_eps (float): epsilon for batch norm.\n            bn_mmt (float): momentum for batch norm. Noted that BN momentum in\n                PyTorch = 1 - BN momentum in Caffe2.\n        """"""\n        super(R2Plus1DStemSinglePathway, self).__init__(\n            dim_in,\n            dim_out,\n            kernel,\n            stride,\n            padding,\n            maxpool=maxpool,\n            inplace_relu=inplace_relu,\n            bn_eps=bn_eps,\n            bn_mmt=bn_mmt,\n        )\n\n    def _construct_stem(self, dim_in, dim_out):\n        assert (\n                self.stride[1] == self.stride[2]\n        ), ""Only support identical height stride and width stride""\n        self.conv = r2plus1_unit(\n            dim_in,\n            dim_out,\n            self.stride[0],  # temporal_stride\n            self.stride[1],  # spatial_stride\n            1,  # groups\n            self.inplace_relu,\n            self.bn_eps,\n            self.bn_mmt,\n            dim_mid=45,  # hard-coded middle channels\n        )\n        self.bn = nn.BatchNorm3d(dim_out, eps=self.bn_eps,\n                                 momentum=self.bn_mmt)\n        self.relu = nn.ReLU(self.inplace_relu)\n        if self.maxpool:\n            self.pool_layer = nn.MaxPool3d(\n                kernel_size=[1, 3, 3], stride=[1, 2, 2], padding=[0, 1, 1]\n            )\n\n\nclass ResNeXt3DStemMultiPathway(nn.Module):\n    """"""\n    Video 3D stem module. Provides stem operations of Conv, BN, ReLU, MaxPool\n    on input data tensor for one or multiple pathways.\n    """"""\n\n    def __init__(\n            self,\n            dim_in,\n            dim_out,\n            kernel,\n            stride,\n            padding,\n            inplace_relu=True,\n            bn_eps=1e-5,\n            bn_mmt=0.1,\n            maxpool=(True,),\n    ):\n        """"""\n        The `__init__` method of any subclass should also contain these\n        arguments. List size of 1 for single pathway models (C2D, I3D, SlowOnly\n        and etc), list size of 2 for two pathway models (SlowFast).\n\n        Args:\n            dim_in (list): the list of channel dimensions of the inputs.\n            dim_out (list): the output dimension of the convolution in the stem\n                layer.\n            kernel (list): the kernels\' size of the convolutions in the stem\n                layers. Temporal kernel size, height kernel size, width kernel\n                size in order.\n            stride (list): the stride sizes of the convolutions in the stem\n                layer. Temporal kernel stride, height kernel size, width kernel\n                size in order.\n            padding (list): the paddings\' sizes of the convolutions in the stem\n                layer. Temporal padding size, height padding size, width padding\n                size in order.\n            inplace_relu (bool): calculate the relu on the original input\n                without allocating new memory.\n            bn_eps (float): epsilon for batch norm.\n            bn_mmt (float): momentum for batch norm. Noted that BN momentum in\n                PyTorch = 1 - BN momentum in Caffe2.\n            maxpool (iterable): At training time, when crop size is 224 x 224, do max\n                pooling. When crop size is 112 x 112, skip max pooling.\n                Default value is a (True,)\n        """"""\n        super(ResNeXt3DStemMultiPathway, self).__init__()\n\n        assert (\n                len({len(dim_in), len(dim_out), len(kernel), len(stride),\n                     len(padding)})\n                == 1\n        ), ""Input pathway dimensions are not consistent.""\n        self.num_pathways = len(dim_in)\n        self.kernel = kernel\n        self.stride = stride\n        self.padding = padding\n        self.inplace_relu = inplace_relu\n        self.bn_eps = bn_eps\n        self.bn_mmt = bn_mmt\n        self.maxpool = maxpool\n\n        # Construct the stem layer.\n        self._construct_stem(dim_in, dim_out)\n\n    def _construct_stem(self, dim_in, dim_out):\n        assert type(dim_in) == list\n        assert all(dim > 0 for dim in dim_in)\n        assert type(dim_out) == list\n        assert all(dim > 0 for dim in dim_out)\n\n        self.blocks = {}\n        for p in range(len(dim_in)):\n            stem = ResNeXt3DStemSinglePathway(\n                dim_in[p],\n                dim_out[p],\n                self.kernel[p],\n                self.stride[p],\n                self.padding[p],\n                inplace_relu=self.inplace_relu,\n                bn_eps=self.bn_eps,\n                bn_mmt=self.bn_mmt,\n                maxpool=self.maxpool[p],\n            )\n            stem_name = self._stem_name(p)\n            self.add_module(stem_name, stem)\n            self.blocks[stem_name] = stem\n\n    def _stem_name(self, path_idx):\n        return ""stem-path{}"".format(path_idx)\n\n    def forward(self, x):\n        assert (\n                len(x) == self.num_pathways\n        ), ""Input tensor does not contain {} pathway"".format(self.num_pathways)\n        for p in range(len(x)):\n            stem_name = self._stem_name(p)\n            x[p] = self.blocks[stem_name](x[p])\n        return x\n\n\nclass R2Plus1DStemMultiPathway(ResNeXt3DStemMultiPathway):\n    """"""\n    Video R(2+1)D stem module. Provides stem operations of Conv, BN, ReLU, MaxPool\n    on input data tensor for one or multiple pathways.\n    """"""\n\n    def __init__(\n            self,\n            dim_in,\n            dim_out,\n            kernel,\n            stride,\n            padding,\n            inplace_relu=True,\n            bn_eps=1e-5,\n            bn_mmt=0.1,\n            maxpool=(True,),\n    ):\n        """"""\n        The `__init__` method of any subclass should also contain these\n        arguments. List size of 1 for single pathway models (C2D, I3D, SlowOnly\n        and etc), list size of 2 for two pathway models (SlowFast).\n\n        Args:\n            dim_in (list): the list of channel dimensions of the inputs.\n            dim_out (list): the output dimension of the convolution in the stem\n                layer.\n            kernel (list): the kernels\' size of the convolutions in the stem\n                layers. Temporal kernel size, height kernel size, width kernel\n                size in order.\n            stride (list): the stride sizes of the convolutions in the stem\n                layer. Temporal kernel stride, height kernel size, width kernel\n                size in order.\n            padding (list): the paddings\' sizes of the convolutions in the stem\n                layer. Temporal padding size, height padding size, width padding\n                size in order.\n            inplace_relu (bool): calculate the relu on the original input\n                without allocating new memory.\n            bn_eps (float): epsilon for batch norm.\n            bn_mmt (float): momentum for batch norm. Noted that BN momentum in\n                PyTorch = 1 - BN momentum in Caffe2.\n            maxpool (iterable): At training time, when crop size is 224 x 224, do max\n                pooling. When crop size is 112 x 112, skip max pooling.\n                Default value is a (True,)\n        """"""\n        super(R2Plus1DStemMultiPathway, self).__init__(\n            dim_in,\n            dim_out,\n            kernel,\n            stride,\n            padding,\n            inplace_relu=inplace_relu,\n            bn_eps=bn_eps,\n            bn_mmt=bn_mmt,\n            maxpool=maxpool,\n        )\n\n    def _construct_stem(self, dim_in, dim_out):\n        assert type(dim_in) == list\n        assert all(dim > 0 for dim in dim_in)\n        assert type(dim_out) == list\n        assert all(dim > 0 for dim in dim_out)\n\n        self.blocks = {}\n        for p in range(len(dim_in)):\n            stem = R2Plus1DStemSinglePathway(\n                dim_in[p],\n                dim_out[p],\n                self.kernel[p],\n                self.stride[p],\n                self.padding[p],\n                inplace_relu=self.inplace_relu,\n                bn_eps=self.bn_eps,\n                bn_mmt=self.bn_mmt,\n                maxpool=self.maxpool[p],\n            )\n            stem_name = self._stem_name(p)\n            self.add_module(stem_name, stem)\n            self.blocks[stem_name] = stem\n\n\nclass ResNeXt3DStem(nn.Module):\n    def __init__(\n            self, temporal_kernel, spatial_kernel, input_planes, stem_planes,\n            maxpool\n    ):\n        super(ResNeXt3DStem, self).__init__()\n        self._construct_stem(\n            temporal_kernel, spatial_kernel, input_planes, stem_planes, maxpool\n        )\n\n    def _construct_stem(\n            self, temporal_kernel, spatial_kernel, input_planes, stem_planes,\n            maxpool\n    ):\n        self.stem = ResNeXt3DStemMultiPathway(\n            [input_planes],\n            [stem_planes],\n            [[temporal_kernel, spatial_kernel, spatial_kernel]],\n            [[1, 2, 2]],  # stride\n            [\n                [temporal_kernel // 2, spatial_kernel // 2,\n                 spatial_kernel // 2]\n            ],  # padding\n            maxpool=[maxpool],\n        )\n\n    def forward(self, x):\n        return self.stem(x)\n\n\nclass R2Plus1DStem(ResNeXt3DStem):\n    def __init__(\n            self, temporal_kernel, spatial_kernel, input_planes, stem_planes,\n            maxpool\n    ):\n        super(R2Plus1DStem, self).__init__(\n            temporal_kernel, spatial_kernel, input_planes, stem_planes, maxpool\n        )\n\n    def _construct_stem(\n            self, temporal_kernel, spatial_kernel, input_planes, stem_planes,\n            maxpool\n    ):\n        self.stem = R2Plus1DStemMultiPathway(\n            [input_planes],\n            [stem_planes],\n            [[temporal_kernel, spatial_kernel, spatial_kernel]],\n            [[1, 2, 2]],  # stride\n            [\n                [temporal_kernel // 2, spatial_kernel // 2,\n                 spatial_kernel // 2]\n            ],  # padding\n            maxpool=[maxpool],\n        )\n'"
mlcomp/contrib/segmentation/deeplabv3/backbone/__init__.py,0,"b""from mlcomp.contrib.segmentation.deeplabv3.backbone import resnet, \\\n    xception, drn, mobilenet\n\n\ndef build_backbone(backbone, output_stride, BatchNorm):\n    if backbone == 'resnet':\n        return resnet.ResNet101(output_stride, BatchNorm)\n    elif backbone == 'xception':\n        return xception.AlignedXception(output_stride, BatchNorm)\n    elif backbone == 'drn':\n        return drn.drn_d_54(BatchNorm)\n    elif backbone == 'mobilenet':\n        return mobilenet.MobileNetV2(output_stride, BatchNorm)\n    else:\n        raise NotImplementedError\n"""
mlcomp/contrib/segmentation/deeplabv3/backbone/drn.py,4,"b""import torch.nn as nn\nimport math\nimport torch.utils.model_zoo as model_zoo\n\nwebroot = 'http://dl.yf.io/drn/'\n\nmodel_urls = {\n    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n    'drn-c-26': webroot + 'drn_c_26-ddedf421.pth',\n    'drn-c-42': webroot + 'drn_c_42-9d336e8c.pth',\n    'drn-c-58': webroot + 'drn_c_58-0a53a92c.pth',\n    'drn-d-22': webroot + 'drn_d_22-4bd2f8ea.pth',\n    'drn-d-38': webroot + 'drn_d_38-eebb45f0.pth',\n    'drn-d-54': webroot + 'drn_d_54-0e0534ff.pth',\n    'drn-d-105': webroot + 'drn_d_105-12b40979.pth'\n}\n\n\ndef conv3x3(in_planes, out_planes, stride=1, padding=1, dilation=1):\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=padding, bias=False, dilation=dilation)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None,\n                 dilation=(1, 1), residual=True, BatchNorm=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride,\n                             padding=dilation[0], dilation=dilation[0])\n        self.bn1 = BatchNorm(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes,\n                             padding=dilation[1], dilation=dilation[1])\n        self.bn2 = BatchNorm(planes)\n        self.downsample = downsample\n        self.stride = stride\n        self.residual = residual\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n        if self.residual:\n            out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None,\n                 dilation=(1, 1), residual=True, BatchNorm=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = BatchNorm(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                               padding=dilation[1], bias=False,\n                               dilation=dilation[1])\n        self.bn2 = BatchNorm(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = BatchNorm(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass DRN(nn.Module):\n\n    def __init__(self, block, layers, arch='D',\n                 channels=(16, 32, 64, 128, 256, 512, 512, 512),\n                 BatchNorm=None):\n        super(DRN, self).__init__()\n        self.inplanes = channels[0]\n        self.out_dim = channels[-1]\n        self.arch = arch\n\n        if arch == 'C':\n            self.conv1 = nn.Conv2d(3, channels[0], kernel_size=7, stride=1,\n                                   padding=3, bias=False)\n            self.bn1 = BatchNorm(channels[0])\n            self.relu = nn.ReLU(inplace=True)\n\n            self.layer1 = self._make_layer(\n                BasicBlock, channels[0], layers[0], stride=1,\n                BatchNorm=BatchNorm)\n            self.layer2 = self._make_layer(\n                BasicBlock, channels[1], layers[1], stride=2,\n                BatchNorm=BatchNorm)\n\n        elif arch == 'D':\n            self.layer0 = nn.Sequential(\n                nn.Conv2d(3, channels[0], kernel_size=7, stride=1, padding=3,\n                          bias=False),\n                BatchNorm(channels[0]),\n                nn.ReLU(inplace=True)\n            )\n\n            self.layer1 = self._make_conv_layers(\n                channels[0], layers[0], stride=1, BatchNorm=BatchNorm)\n            self.layer2 = self._make_conv_layers(\n                channels[1], layers[1], stride=2, BatchNorm=BatchNorm)\n\n        self.layer3 = self._make_layer(block, channels[2], layers[2], stride=2,\n                                       BatchNorm=BatchNorm)\n        self.layer4 = self._make_layer(block, channels[3], layers[3], stride=2,\n                                       BatchNorm=BatchNorm)\n        self.layer5 = self._make_layer(block, channels[4], layers[4],\n                                       dilation=2, new_level=False,\n                                       BatchNorm=BatchNorm)\n        self.layer6 = None if layers[5] == 0 else \\\n            self._make_layer(block, channels[5], layers[5], dilation=4,\n                             new_level=False, BatchNorm=BatchNorm)\n\n        if arch == 'C':\n            self.layer7 = None if layers[6] == 0 else \\\n                self._make_layer(BasicBlock, channels[6], layers[6],\n                                 dilation=2,\n                                 new_level=False, residual=False,\n                                 BatchNorm=BatchNorm)\n            self.layer8 = None if layers[7] == 0 else \\\n                self._make_layer(BasicBlock, channels[7], layers[7],\n                                 dilation=1,\n                                 new_level=False, residual=False,\n                                 BatchNorm=BatchNorm)\n        elif arch == 'D':\n            self.layer7 = None if layers[6] == 0 else \\\n                self._make_conv_layers(channels[6], layers[6], dilation=2,\n                                       BatchNorm=BatchNorm)\n            self.layer8 = None if layers[7] == 0 else \\\n                self._make_conv_layers(channels[7], layers[7], dilation=1,\n                                       BatchNorm=BatchNorm)\n\n        self._init_weight()\n\n    def _init_weight(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, stride=1, dilation=1,\n                    new_level=True, residual=True, BatchNorm=None):\n        assert dilation == 1 or dilation % 2 == 0\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                BatchNorm(planes * block.expansion),\n            )\n\n        layers = list()\n        layers.append(block(\n            self.inplanes, planes, stride, downsample,\n            dilation=(1, 1) if dilation == 1 else (\n                dilation // 2 if new_level else dilation, dilation),\n            residual=residual, BatchNorm=BatchNorm))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, residual=residual,\n                                dilation=(dilation, dilation),\n                                BatchNorm=BatchNorm))\n\n        return nn.Sequential(*layers)\n\n    def _make_conv_layers(self, channels, convs, stride=1, dilation=1,\n                          BatchNorm=None):\n        modules = []\n        for i in range(convs):\n            modules.extend([\n                nn.Conv2d(self.inplanes, channels, kernel_size=3,\n                          stride=stride if i == 0 else 1,\n                          padding=dilation, bias=False, dilation=dilation),\n                BatchNorm(channels),\n                nn.ReLU(inplace=True)])\n            self.inplanes = channels\n        return nn.Sequential(*modules)\n\n    def forward(self, x):\n        if self.arch == 'C':\n            x = self.conv1(x)\n            x = self.bn1(x)\n            x = self.relu(x)\n        elif self.arch == 'D':\n            x = self.layer0(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n\n        x = self.layer3(x)\n        low_level_feat = x\n\n        x = self.layer4(x)\n        x = self.layer5(x)\n\n        if self.layer6 is not None:\n            x = self.layer6(x)\n\n        if self.layer7 is not None:\n            x = self.layer7(x)\n\n        if self.layer8 is not None:\n            x = self.layer8(x)\n\n        return x, low_level_feat\n\n\nclass DRN_A(nn.Module):\n\n    def __init__(self, block, layers, BatchNorm=None):\n        self.inplanes = 64\n        super(DRN_A, self).__init__()\n        self.out_dim = 512 * block.expansion\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = BatchNorm(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0],\n                                       BatchNorm=BatchNorm)\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\n                                       BatchNorm=BatchNorm)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=1,\n                                       dilation=2, BatchNorm=BatchNorm)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=1,\n                                       dilation=4, BatchNorm=BatchNorm)\n\n        self._init_weight()\n\n    def _init_weight(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, stride=1, dilation=1,\n                    BatchNorm=None):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                BatchNorm(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample,\n                            BatchNorm=BatchNorm))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes,\n                                dilation=(dilation, dilation,),\n                                BatchNorm=BatchNorm))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        return x\n\n\ndef drn_a_50(BatchNorm, pretrained=True):\n    model = DRN_A(Bottleneck, [3, 4, 6, 3], BatchNorm=BatchNorm)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls['resnet50']))\n    return model\n\n\ndef drn_c_26(BatchNorm, pretrained=True):\n    model = DRN(BasicBlock, [1, 1, 2, 2, 2, 2, 1, 1], arch='C',\n                BatchNorm=BatchNorm)\n    if pretrained:\n        pretrained = model_zoo.load_url(model_urls['drn-c-26'])\n        del pretrained['fc.weight']\n        del pretrained['fc.bias']\n        model.load_state_dict(pretrained)\n    return model\n\n\ndef drn_c_42(BatchNorm, pretrained=True):\n    model = DRN(BasicBlock, [1, 1, 3, 4, 6, 3, 1, 1], arch='C',\n                BatchNorm=BatchNorm)\n    if pretrained:\n        pretrained = model_zoo.load_url(model_urls['drn-c-42'])\n        del pretrained['fc.weight']\n        del pretrained['fc.bias']\n        model.load_state_dict(pretrained)\n    return model\n\n\ndef drn_c_58(BatchNorm, pretrained=True):\n    model = DRN(Bottleneck, [1, 1, 3, 4, 6, 3, 1, 1], arch='C',\n                BatchNorm=BatchNorm)\n    if pretrained:\n        pretrained = model_zoo.load_url(model_urls['drn-c-58'])\n        del pretrained['fc.weight']\n        del pretrained['fc.bias']\n        model.load_state_dict(pretrained)\n    return model\n\n\ndef drn_d_22(BatchNorm, pretrained=True):\n    model = DRN(BasicBlock, [1, 1, 2, 2, 2, 2, 1, 1], arch='D',\n                BatchNorm=BatchNorm)\n    if pretrained:\n        pretrained = model_zoo.load_url(model_urls['drn-d-22'])\n        del pretrained['fc.weight']\n        del pretrained['fc.bias']\n        model.load_state_dict(pretrained)\n    return model\n\n\ndef drn_d_24(BatchNorm, pretrained=True):\n    model = DRN(BasicBlock, [1, 1, 2, 2, 2, 2, 2, 2], arch='D',\n                BatchNorm=BatchNorm)\n    if pretrained:\n        pretrained = model_zoo.load_url(model_urls['drn-d-24'])\n        del pretrained['fc.weight']\n        del pretrained['fc.bias']\n        model.load_state_dict(pretrained)\n    return model\n\n\ndef drn_d_38(BatchNorm, pretrained=True):\n    model = DRN(BasicBlock, [1, 1, 3, 4, 6, 3, 1, 1], arch='D',\n                BatchNorm=BatchNorm)\n    if pretrained:\n        pretrained = model_zoo.load_url(model_urls['drn-d-38'])\n        del pretrained['fc.weight']\n        del pretrained['fc.bias']\n        model.load_state_dict(pretrained)\n    return model\n\n\ndef drn_d_40(BatchNorm, pretrained=True):\n    model = DRN(BasicBlock, [1, 1, 3, 4, 6, 3, 2, 2], arch='D',\n                BatchNorm=BatchNorm)\n    if pretrained:\n        pretrained = model_zoo.load_url(model_urls['drn-d-40'])\n        del pretrained['fc.weight']\n        del pretrained['fc.bias']\n        model.load_state_dict(pretrained)\n    return model\n\n\ndef drn_d_54(BatchNorm, pretrained=True):\n    model = DRN(Bottleneck, [1, 1, 3, 4, 6, 3, 1, 1], arch='D',\n                BatchNorm=BatchNorm)\n    if pretrained:\n        pretrained = model_zoo.load_url(model_urls['drn-d-54'])\n        del pretrained['fc.weight']\n        del pretrained['fc.bias']\n        model.load_state_dict(pretrained)\n    return model\n\n\ndef drn_d_105(BatchNorm, pretrained=True):\n    model = DRN(Bottleneck, [1, 1, 3, 4, 23, 3, 1, 1], arch='D',\n                BatchNorm=BatchNorm)\n    if pretrained:\n        pretrained = model_zoo.load_url(model_urls['drn-d-105'])\n        del pretrained['fc.weight']\n        del pretrained['fc.bias']\n        model.load_state_dict(pretrained)\n    return model\n\n\nif __name__ == '__main__':\n    import torch\n\n    model = drn_a_50(BatchNorm=nn.BatchNorm2d, pretrained=True)\n    input = torch.rand(1, 3, 512, 512)\n    output, low_level_feat = model(input)\n    print(output.size())\n    print(low_level_feat.size())\n"""
mlcomp/contrib/segmentation/deeplabv3/backbone/mobilenet.py,5,"b""import torch\nimport torch.nn.functional as F\nimport torch.nn as nn\nimport torch.utils.model_zoo as model_zoo\n\n\ndef conv_bn(inp, oup, stride, BatchNorm):\n    return nn.Sequential(\n        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n        BatchNorm(oup),\n        nn.ReLU6(inplace=True)\n    )\n\n\ndef fixed_padding(inputs, kernel_size, dilation):\n    kernel_size_effective = kernel_size + (kernel_size - 1) * (dilation - 1)\n    pad_total = kernel_size_effective - 1\n    pad_beg = pad_total // 2\n    pad_end = pad_total - pad_beg\n    padded_inputs = F.pad(inputs, (pad_beg, pad_end, pad_beg, pad_end))\n    return padded_inputs\n\n\nclass InvertedResidual(nn.Module):\n    def __init__(self, inp, oup, stride, dilation, expand_ratio, BatchNorm):\n        super(InvertedResidual, self).__init__()\n        self.stride = stride\n        assert stride in [1, 2]\n\n        hidden_dim = round(inp * expand_ratio)\n        self.use_res_connect = self.stride == 1 and inp == oup\n        self.kernel_size = 3\n        self.dilation = dilation\n\n        if expand_ratio == 1:\n            self.conv = nn.Sequential(\n                # dw\n                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 0, dilation,\n                          groups=hidden_dim, bias=False),\n                BatchNorm(hidden_dim),\n                nn.ReLU6(inplace=True),\n                # pw-linear\n                nn.Conv2d(hidden_dim, oup, 1, 1, 0, 1, 1, bias=False),\n                BatchNorm(oup),\n            )\n        else:\n            self.conv = nn.Sequential(\n                # pw\n                nn.Conv2d(inp, hidden_dim, 1, 1, 0, 1, bias=False),\n                BatchNorm(hidden_dim),\n                nn.ReLU6(inplace=True),\n                # dw\n                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 0, dilation,\n                          groups=hidden_dim, bias=False),\n                BatchNorm(hidden_dim),\n                nn.ReLU6(inplace=True),\n                # pw-linear\n                nn.Conv2d(hidden_dim, oup, 1, 1, 0, 1, bias=False),\n                BatchNorm(oup),\n            )\n\n    def forward(self, x):\n        x_pad = fixed_padding(x, self.kernel_size, dilation=self.dilation)\n        if self.use_res_connect:\n            x = x + self.conv(x_pad)\n        else:\n            x = self.conv(x_pad)\n        return x\n\n\nclass MobileNetV2(nn.Module):\n    def __init__(self, output_stride=8, BatchNorm=None, width_mult=1.,\n                 pretrained=True):\n        super(MobileNetV2, self).__init__()\n        block = InvertedResidual\n        input_channel = 32\n        current_stride = 1\n        rate = 1\n        interverted_residual_setting = [\n            # t, c, n, s\n            [1, 16, 1, 1],\n            [6, 24, 2, 2],\n            [6, 32, 3, 2],\n            [6, 64, 4, 2],\n            [6, 96, 3, 1],\n            [6, 160, 3, 2],\n            [6, 320, 1, 1],\n        ]\n\n        # building first layer\n        input_channel = int(input_channel * width_mult)\n        self.features = [conv_bn(3, input_channel, 2, BatchNorm)]\n        current_stride *= 2\n        # building inverted residual blocks\n        for t, c, n, s in interverted_residual_setting:\n            if current_stride == output_stride:\n                stride = 1\n                dilation = rate\n                rate *= s\n            else:\n                stride = s\n                dilation = 1\n                current_stride *= s\n            output_channel = int(c * width_mult)\n            for i in range(n):\n                if i == 0:\n                    self.features.append(\n                        block(input_channel, output_channel, stride, dilation,\n                              t, BatchNorm))\n                else:\n                    self.features.append(\n                        block(input_channel, output_channel, 1, dilation, t,\n                              BatchNorm))\n                input_channel = output_channel\n        self.features = nn.Sequential(*self.features)\n        self._initialize_weights()\n\n        if pretrained:\n            self._load_pretrained_model()\n\n        self.low_level_features = self.features[0:4]\n        self.high_level_features = self.features[4:]\n\n    def forward(self, x):\n        low_level_feat = self.low_level_features(x)\n        x = self.high_level_features(low_level_feat)\n        return x, low_level_feat\n\n    def _load_pretrained_model(self):\n        pretrain_dict = model_zoo.load_url(\n            'http://jeff95.me/models/mobilenet_v2-6a65762b.pth')\n        model_dict = {}\n        state_dict = self.state_dict()\n        for k, v in pretrain_dict.items():\n            if k in state_dict:\n                model_dict[k] = v\n        state_dict.update(model_dict)\n        self.load_state_dict(state_dict)\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                # n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                # m.weight.data.normal_(0, math.sqrt(2. / n))\n                torch.nn.init.kaiming_normal_(m.weight)\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n\nif __name__ == '__main__':\n    input = torch.rand(1, 3, 512, 512)\n    model = MobileNetV2(output_stride=16, BatchNorm=nn.BatchNorm2d)\n    output, low_level_feat = model(input)\n    print(output.size())\n    print(low_level_feat.size())\n"""
mlcomp/contrib/segmentation/deeplabv3/backbone/resnet.py,4,"b'import math\nimport torch.nn as nn\nimport torch.utils.model_zoo as model_zoo\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, dilation=1, downsample=None,\n                 BatchNorm=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = BatchNorm(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                               dilation=dilation, padding=dilation, bias=False)\n        self.bn2 = BatchNorm(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = BatchNorm(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n        self.dilation = dilation\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNet(nn.Module):\n\n    def __init__(self, block, layers, output_stride, BatchNorm,\n                 pretrained=True):\n        self.inplanes = 64\n        super(ResNet, self).__init__()\n        blocks = [1, 2, 4]\n        if output_stride == 16:\n            strides = [1, 2, 2, 1]\n            dilations = [1, 1, 1, 2]\n        elif output_stride == 8:\n            strides = [1, 2, 1, 1]\n            dilations = [1, 1, 2, 4]\n        else:\n            raise NotImplementedError\n\n        # Modules\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = BatchNorm(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n\n        self.layer1 = self._make_layer(block, 64, layers[0], stride=strides[0],\n                                       dilation=dilations[0],\n                                       BatchNorm=BatchNorm)\n        self.layer2 = self._make_layer(block, 128, layers[1],\n                                       stride=strides[1],\n                                       dilation=dilations[1],\n                                       BatchNorm=BatchNorm)\n        self.layer3 = self._make_layer(block, 256, layers[2],\n                                       stride=strides[2],\n                                       dilation=dilations[2],\n                                       BatchNorm=BatchNorm)\n        self.layer4 = self._make_MG_unit(block, 512, blocks=blocks,\n                                         stride=strides[3],\n                                         dilation=dilations[3],\n                                         BatchNorm=BatchNorm)\n        self._init_weight()\n\n        if pretrained:\n            self._load_pretrained_model()\n\n    def _make_layer(self, block, planes, blocks, stride=1, dilation=1,\n                    BatchNorm=None):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                BatchNorm(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(\n            block(self.inplanes, planes, stride, dilation, downsample,\n                  BatchNorm))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, dilation=dilation,\n                                BatchNorm=BatchNorm))\n\n        return nn.Sequential(*layers)\n\n    def _make_MG_unit(self, block, planes, blocks, stride=1, dilation=1,\n                      BatchNorm=None):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                BatchNorm(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(\n            block(self.inplanes, planes, stride, dilation=blocks[0] * dilation,\n                  downsample=downsample, BatchNorm=BatchNorm))\n        self.inplanes = planes * block.expansion\n        for i in range(1, len(blocks)):\n            layers.append(block(self.inplanes, planes, stride=1,\n                                dilation=blocks[i] * dilation,\n                                BatchNorm=BatchNorm))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, input):\n        x = self.conv1(input)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        low_level_feat = x\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        return x, low_level_feat\n\n    def _init_weight(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _load_pretrained_model(self):\n        pretrain_dict = model_zoo.load_url(\n            \'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\')\n        model_dict = {}\n        state_dict = self.state_dict()\n        for k, v in pretrain_dict.items():\n            if k in state_dict:\n                model_dict[k] = v\n        state_dict.update(model_dict)\n        self.load_state_dict(state_dict)\n\n\ndef ResNet101(output_stride, BatchNorm, pretrained=True):\n    """"""Constructs a ResNet-101 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 23, 3], output_stride, BatchNorm,\n                   pretrained=pretrained)\n    return model\n\n\nif __name__ == \'__main__\':\n    import torch\n\n    model = ResNet101(BatchNorm=nn.BatchNorm2d, pretrained=True,\n                      output_stride=8)\n    input = torch.rand(1, 3, 512, 512)\n    output, low_level_feat = model(input)\n    print(output.size())\n    print(low_level_feat.size())\n'"
mlcomp/contrib/segmentation/deeplabv3/backbone/xception.py,4,"b'import math\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.model_zoo as model_zoo\n\n\ndef fixed_padding(inputs, kernel_size, dilation):\n    kernel_size_effective = kernel_size + (kernel_size - 1) * (dilation - 1)\n    pad_total = kernel_size_effective - 1\n    pad_beg = pad_total // 2\n    pad_end = pad_total - pad_beg\n    padded_inputs = F.pad(inputs, (pad_beg, pad_end, pad_beg, pad_end))\n    return padded_inputs\n\n\nclass SeparableConv2d(nn.Module):\n    def __init__(self, inplanes, planes, kernel_size=3, stride=1, dilation=1,\n                 bias=False, BatchNorm=None):\n        super(SeparableConv2d, self).__init__()\n\n        self.conv1 = nn.Conv2d(inplanes, inplanes, kernel_size, stride, 0,\n                               dilation,\n                               groups=inplanes, bias=bias)\n        self.bn = BatchNorm(inplanes)\n        self.pointwise = nn.Conv2d(inplanes, planes, 1, 1, 0, 1, 1, bias=bias)\n\n    def forward(self, x):\n        x = fixed_padding(x, self.conv1.kernel_size[0],\n                          dilation=self.conv1.dilation[0])\n        x = self.conv1(x)\n        x = self.bn(x)\n        x = self.pointwise(x)\n        return x\n\n\nclass Block(nn.Module):\n    def __init__(self, inplanes, planes, reps, stride=1, dilation=1,\n                 BatchNorm=None,\n                 start_with_relu=True, grow_first=True, is_last=False):\n        super(Block, self).__init__()\n\n        if planes != inplanes or stride != 1:\n            self.skip = nn.Conv2d(inplanes, planes, 1, stride=stride,\n                                  bias=False)\n            self.skipbn = BatchNorm(planes)\n        else:\n            self.skip = None\n\n        self.relu = nn.ReLU(inplace=True)\n        rep = []\n\n        filters = inplanes\n        if grow_first:\n            rep.append(self.relu)\n            rep.append(SeparableConv2d(inplanes, planes, 3, 1, dilation,\n                                       BatchNorm=BatchNorm))\n            rep.append(BatchNorm(planes))\n            filters = planes\n\n        for i in range(reps - 1):\n            rep.append(self.relu)\n            rep.append(SeparableConv2d(filters, filters, 3, 1, dilation,\n                                       BatchNorm=BatchNorm))\n            rep.append(BatchNorm(filters))\n\n        if not grow_first:\n            rep.append(self.relu)\n            rep.append(SeparableConv2d(inplanes, planes, 3, 1, dilation,\n                                       BatchNorm=BatchNorm))\n            rep.append(BatchNorm(planes))\n\n        if stride != 1:\n            rep.append(self.relu)\n            rep.append(\n                SeparableConv2d(planes, planes, 3, 2, BatchNorm=BatchNorm))\n            rep.append(BatchNorm(planes))\n\n        if stride == 1 and is_last:\n            rep.append(self.relu)\n            rep.append(\n                SeparableConv2d(planes, planes, 3, 1, BatchNorm=BatchNorm))\n            rep.append(BatchNorm(planes))\n\n        if not start_with_relu:\n            rep = rep[1:]\n\n        self.rep = nn.Sequential(*rep)\n\n    def forward(self, inp):\n        x = self.rep(inp)\n\n        if self.skip is not None:\n            skip = self.skip(inp)\n            skip = self.skipbn(skip)\n        else:\n            skip = inp\n\n        x = x + skip\n\n        return x\n\n\nclass AlignedXception(nn.Module):\n    """"""\n    Modified Alighed Xception\n    """"""\n\n    def __init__(self, output_stride, BatchNorm,\n                 pretrained=True):\n        super(AlignedXception, self).__init__()\n\n        if output_stride == 16:\n            entry_block3_stride = 2\n            middle_block_dilation = 1\n            exit_block_dilations = (1, 2)\n        elif output_stride == 8:\n            entry_block3_stride = 1\n            middle_block_dilation = 2\n            exit_block_dilations = (2, 4)\n        else:\n            raise NotImplementedError\n\n        # Entry flow\n        self.conv1 = nn.Conv2d(3, 32, 3, stride=2, padding=1, bias=False)\n        self.bn1 = BatchNorm(32)\n        self.relu = nn.ReLU(inplace=True)\n\n        self.conv2 = nn.Conv2d(32, 64, 3, stride=1, padding=1, bias=False)\n        self.bn2 = BatchNorm(64)\n\n        self.block1 = Block(64, 128, reps=2, stride=2, BatchNorm=BatchNorm,\n                            start_with_relu=False)\n        self.block2 = Block(128, 256, reps=2, stride=2, BatchNorm=BatchNorm,\n                            start_with_relu=False,\n                            grow_first=True)\n        self.block3 = Block(256, 728, reps=2, stride=entry_block3_stride,\n                            BatchNorm=BatchNorm,\n                            start_with_relu=True, grow_first=True,\n                            is_last=True)\n\n        # Middle flow\n        self.block4 = Block(728, 728, reps=3, stride=1,\n                            dilation=middle_block_dilation,\n                            BatchNorm=BatchNorm, start_with_relu=True,\n                            grow_first=True)\n        self.block5 = Block(728, 728, reps=3, stride=1,\n                            dilation=middle_block_dilation,\n                            BatchNorm=BatchNorm, start_with_relu=True,\n                            grow_first=True)\n        self.block6 = Block(728, 728, reps=3, stride=1,\n                            dilation=middle_block_dilation,\n                            BatchNorm=BatchNorm, start_with_relu=True,\n                            grow_first=True)\n        self.block7 = Block(728, 728, reps=3, stride=1,\n                            dilation=middle_block_dilation,\n                            BatchNorm=BatchNorm, start_with_relu=True,\n                            grow_first=True)\n        self.block8 = Block(728, 728, reps=3, stride=1,\n                            dilation=middle_block_dilation,\n                            BatchNorm=BatchNorm, start_with_relu=True,\n                            grow_first=True)\n        self.block9 = Block(728, 728, reps=3, stride=1,\n                            dilation=middle_block_dilation,\n                            BatchNorm=BatchNorm, start_with_relu=True,\n                            grow_first=True)\n        self.block10 = Block(728, 728, reps=3, stride=1,\n                             dilation=middle_block_dilation,\n                             BatchNorm=BatchNorm, start_with_relu=True,\n                             grow_first=True)\n        self.block11 = Block(728, 728, reps=3, stride=1,\n                             dilation=middle_block_dilation,\n                             BatchNorm=BatchNorm, start_with_relu=True,\n                             grow_first=True)\n        self.block12 = Block(728, 728, reps=3, stride=1,\n                             dilation=middle_block_dilation,\n                             BatchNorm=BatchNorm, start_with_relu=True,\n                             grow_first=True)\n        self.block13 = Block(728, 728, reps=3, stride=1,\n                             dilation=middle_block_dilation,\n                             BatchNorm=BatchNorm, start_with_relu=True,\n                             grow_first=True)\n        self.block14 = Block(728, 728, reps=3, stride=1,\n                             dilation=middle_block_dilation,\n                             BatchNorm=BatchNorm, start_with_relu=True,\n                             grow_first=True)\n        self.block15 = Block(728, 728, reps=3, stride=1,\n                             dilation=middle_block_dilation,\n                             BatchNorm=BatchNorm, start_with_relu=True,\n                             grow_first=True)\n        self.block16 = Block(728, 728, reps=3, stride=1,\n                             dilation=middle_block_dilation,\n                             BatchNorm=BatchNorm, start_with_relu=True,\n                             grow_first=True)\n        self.block17 = Block(728, 728, reps=3, stride=1,\n                             dilation=middle_block_dilation,\n                             BatchNorm=BatchNorm, start_with_relu=True,\n                             grow_first=True)\n        self.block18 = Block(728, 728, reps=3, stride=1,\n                             dilation=middle_block_dilation,\n                             BatchNorm=BatchNorm, start_with_relu=True,\n                             grow_first=True)\n        self.block19 = Block(728, 728, reps=3, stride=1,\n                             dilation=middle_block_dilation,\n                             BatchNorm=BatchNorm, start_with_relu=True,\n                             grow_first=True)\n\n        # Exit flow\n        self.block20 = Block(728, 1024, reps=2, stride=1,\n                             dilation=exit_block_dilations[0],\n                             BatchNorm=BatchNorm, start_with_relu=True,\n                             grow_first=False, is_last=True)\n\n        self.conv3 = SeparableConv2d(1024, 1536, 3, stride=1,\n                                     dilation=exit_block_dilations[1],\n                                     BatchNorm=BatchNorm)\n        self.bn3 = BatchNorm(1536)\n\n        self.conv4 = SeparableConv2d(1536, 1536, 3, stride=1,\n                                     dilation=exit_block_dilations[1],\n                                     BatchNorm=BatchNorm)\n        self.bn4 = BatchNorm(1536)\n\n        self.conv5 = SeparableConv2d(1536, 2048, 3, stride=1,\n                                     dilation=exit_block_dilations[1],\n                                     BatchNorm=BatchNorm)\n        self.bn5 = BatchNorm(2048)\n\n        # Init weights\n        self._init_weight()\n\n        # Load pretrained model\n        if pretrained:\n            self._load_pretrained_model()\n\n    def forward(self, x):\n        # Entry flow\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n\n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = self.relu(x)\n\n        x = self.block1(x)\n        # add relu here\n        x = self.relu(x)\n        low_level_feat = x\n        x = self.block2(x)\n        x = self.block3(x)\n\n        # Middle flow\n        x = self.block4(x)\n        x = self.block5(x)\n        x = self.block6(x)\n        x = self.block7(x)\n        x = self.block8(x)\n        x = self.block9(x)\n        x = self.block10(x)\n        x = self.block11(x)\n        x = self.block12(x)\n        x = self.block13(x)\n        x = self.block14(x)\n        x = self.block15(x)\n        x = self.block16(x)\n        x = self.block17(x)\n        x = self.block18(x)\n        x = self.block19(x)\n\n        # Exit flow\n        x = self.block20(x)\n        x = self.relu(x)\n        x = self.conv3(x)\n        x = self.bn3(x)\n        x = self.relu(x)\n\n        x = self.conv4(x)\n        x = self.bn4(x)\n        x = self.relu(x)\n\n        x = self.conv5(x)\n        x = self.bn5(x)\n        x = self.relu(x)\n\n        return x, low_level_feat\n\n    def _init_weight(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _load_pretrained_model(self):\n        pretrain_dict = model_zoo.load_url(\n            \'http://data.lip6.fr/cadene/\'\n            \'pretrainedmodels/xception-b5690688.pth\')\n        model_dict = {}\n        state_dict = self.state_dict()\n\n        for k, v in pretrain_dict.items():\n            if k in model_dict:\n                if \'pointwise\' in k:\n                    v = v.unsqueeze(-1).unsqueeze(-1)\n                if k.startswith(\'block11\'):\n                    model_dict[k] = v\n                    model_dict[k.replace(\'block11\', \'block12\')] = v\n                    model_dict[k.replace(\'block11\', \'block13\')] = v\n                    model_dict[k.replace(\'block11\', \'block14\')] = v\n                    model_dict[k.replace(\'block11\', \'block15\')] = v\n                    model_dict[k.replace(\'block11\', \'block16\')] = v\n                    model_dict[k.replace(\'block11\', \'block17\')] = v\n                    model_dict[k.replace(\'block11\', \'block18\')] = v\n                    model_dict[k.replace(\'block11\', \'block19\')] = v\n                elif k.startswith(\'block12\'):\n                    model_dict[k.replace(\'block12\', \'block20\')] = v\n                elif k.startswith(\'bn3\'):\n                    model_dict[k] = v\n                    model_dict[k.replace(\'bn3\', \'bn4\')] = v\n                elif k.startswith(\'conv4\'):\n                    model_dict[k.replace(\'conv4\', \'conv5\')] = v\n                elif k.startswith(\'bn4\'):\n                    model_dict[k.replace(\'bn4\', \'bn5\')] = v\n                else:\n                    model_dict[k] = v\n        state_dict.update(model_dict)\n        self.load_state_dict(state_dict)\n\n\nif __name__ == \'__main__\':\n    import torch\n\n    model = AlignedXception(BatchNorm=nn.BatchNorm2d, pretrained=True,\n                            output_stride=16)\n    input = torch.rand(1, 3, 512, 512)\n    output, low_level_feat = model(input)\n    print(output.size())\n    print(low_level_feat.size())\n'"
