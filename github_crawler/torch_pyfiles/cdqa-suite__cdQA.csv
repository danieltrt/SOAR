file_path,api_count,code
api.py,0,"b'from flask import Flask, request, jsonify\nfrom flask_cors import CORS\n\nimport os\nfrom ast import literal_eval\nimport pandas as pd\n\nfrom cdqa.utils.filters import filter_paragraphs\nfrom cdqa.pipeline import QAPipeline\n\napp = Flask(__name__)\nCORS(app)\n\ndataset_path = os.environ[""dataset_path""]\nreader_path = os.environ[""reader_path""]\n\ndf = pd.read_csv(dataset_path, converters={""paragraphs"": literal_eval})\ndf = filter_paragraphs(df)\n\ncdqa_pipeline = QAPipeline(reader=reader_path)\ncdqa_pipeline.fit_retriever(df=df)\n\n\n@app.route(""/api"", methods=[""GET""])\ndef api():\n\n    query = request.args.get(""query"")\n    prediction = cdqa_pipeline.predict(query=query)\n\n    return jsonify(\n        query=query, answer=prediction[0], title=prediction[1], paragraph=prediction[2]\n    )\n'"
setup.py,0,"b'import os\nfrom setuptools import setup, find_packages\n\n\ndef read(file):\n    return open(os.path.join(os.path.dirname(__file__), file)).read()\n\n\nsetup(\n    name=""cdqa"",\n    version=""1.3.9"",\n    author=""F\xc3\xa9lix MIKAELIAN, Andr\xc3\xa9 FARIAS, Matyas AMROUCHE, Olivier SANS, Th\xc3\xa9o NAZON"",\n    description=""An End-To-End Closed Domain Question Answering System"",\n    long_description=read(""README.md""),\n    long_description_content_type=""text/markdown"",\n    keywords=""reading comprehension question answering deep learning natural language processing information retrieval bert"",\n    license=""Apache-2.0"",\n    url=""https://github.com/cdqa-suite/cdQA"",\n    packages=find_packages(),\n    install_requires=read(""requirements.txt"").split(),\n)\n'"
cdqa/__init__.py,0,b''
tests/__init__.py,0,b''
tests/test_converters.py,0,"b'import os\nimport wget\nimport pytest\nfrom cdqa.utils.converters import pdf_converter, md_converter\n\n\n@pytest.fixture(scope=""session"")\ndef download_test_assets(tmpdir_factory):\n    assets_urls = [\n        # PDF\n        ""https://invest.bnpparibas.com/documents/1q19-pr-12648"",\n        ""https://invest.bnpparibas.com/documents/4q18-pr-18000"",\n        ""https://invest.bnpparibas.com/documents/4q17-pr"",\n        # MD\n        ""https://raw.githubusercontent.com/cdqa-suite/cdQA/master/README.md"",\n        ""https://raw.githubusercontent.com/huggingface/pytorch-transformers/master/docs/source/quickstart.md"",\n        ""https://raw.githubusercontent.com/huggingface/pytorch-transformers/master/docs/source/migration.md"",\n    ]\n\n    print(""\\nDownloading assets..."")\n    fn = tmpdir_factory.mktemp(""assets_data"")\n    for url in assets_urls:\n        wget.download(url=url, out=str(fn))\n    return fn\n\n\nclass Test_converter:\n    @pytest.fixture(autouse=True)\n    def get_assets_folder(self, download_test_assets):\n        self.assets_folder = download_test_assets\n\n    def df_converter_check(self, df, include_line_breaks=False):\n        errors = []\n        # replace assertions by conditions\n        if not df.shape == (3, 2):\n            errors.append(""resulting dataframe has unexpected shape."")\n        if not (isinstance(df.paragraphs[0][0], str) and isinstance(df.title[0], str)):\n            errors.append(""paragraph column content has wrong format."")\n        if include_line_breaks:\n            para_len = [len(df.paragraphs[i]) for i in range(df.shape[0])]\n            para_len.sort()\n            if not para_len == [58, 80, 87]:\n                errors.append(f""error in number of paragraphs : {para_len}"")\n\n        # assert no error message has been registered, else print messages\n        assert not errors, ""errors occured:\\n{}"".format(""\\n"".join(errors))\n\n    def test_md_converter(self):\n        df = md_converter(directory_path=self.assets_folder)\n        self.df_converter_check(df)\n\n    def test_pdf_converter(self):\n        df = pdf_converter(directory_path=self.assets_folder)\n        self.df_converter_check(df)\n        df_line_para = pdf_converter(\n            directory_path=self.assets_folder, include_line_breaks=True\n        )\n        self.df_converter_check(df_line_para, True)\n'"
tests/test_evaluation.py,0,"b'import json\nfrom ast import literal_eval\nimport pandas as pd\nimport torch\n\nfrom cdqa.utils.filters import filter_paragraphs\nfrom cdqa.utils.evaluation import evaluate_pipeline, evaluate_reader\nfrom cdqa.utils.download import *\nfrom cdqa.pipeline import QAPipeline\n\n\ndef test_evaluate_pipeline():\n\n    download_bnpp_data(""./data/bnpp_newsroom_v1.1/"")\n    download_model(""bert-squad_1.1"", dir=""./models"")\n    df = pd.read_csv(\n        ""./data/bnpp_newsroom_v1.1/bnpp_newsroom-v1.1.csv"",\n        converters={""paragraphs"": literal_eval},\n    )\n    df = filter_paragraphs(df)\n\n    test_data = {\n        ""data"": [\n            {\n                ""title"": ""BNP Paribas\xe2\x80\x99 commitment to universities and schools"",\n                ""paragraphs"": [\n                    {\n                        ""context"": ""Since January 2016, BNP Paribas has offered an Excellence Program targeting new Master\xe2\x80\x99s level graduates (BAC+5) who show high potential. The aid program lasts 18 months and comprises three assignments of six months each. It serves as a strong career accelerator that enables participants to access high-level management positions at a faster rate. The program allows participants to discover the BNP Paribas Group and its various entities in France and abroad, build an internal and external network by working on different assignments and receive personalized assistance from a mentor and coaching firm at every step along the way."",\n                        ""qas"": [\n                            {\n                                ""answers"": [\n                                    {""answer_start"": 6, ""text"": ""January 2016""},\n                                    {""answer_start"": 6, ""text"": ""January 2016""},\n                                    {""answer_start"": 6, ""text"": ""January 2016""},\n                                ],\n                                ""question"": ""Since when does the Excellence Program of BNP Paribas exist?"",\n                                ""id"": ""56be4db0acb8001400a502ec"",\n                            }\n                        ],\n                    }\n                ],\n            }\n        ],\n        ""version"": ""1.1"",\n    }\n\n    with open(""./test_data.json"", ""w"") as f:\n        json.dump(test_data, f)\n\n    cdqa_pipeline = QAPipeline(reader=""./models/bert_qa.joblib"", n_jobs=-1)\n    cdqa_pipeline.fit_retriever(df)\n\n    eval_dict = evaluate_pipeline(cdqa_pipeline, ""./test_data.json"", output_dir=None)\n\n    assert eval_dict[""exact_match""] > 0.8\n    assert eval_dict[""f1""] > 0.8\n\n\ndef test_evaluate_reader():\n\n    download_model(""bert-squad_1.1"", dir=""./models"")\n    cdqa_pipeline = QAPipeline(reader=""./models/bert_qa.joblib"", n_jobs=-1)\n    eval_dict = evaluate_reader(cdqa_pipeline, ""./test_data.json"")\n\n    assert eval_dict[""exact_match""] > 0.8\n    assert eval_dict[""f1""] > 0.8\n'"
tests/test_pipeline.py,0,"b'import pytest\nfrom ast import literal_eval\nimport pandas as pd\nimport torch\n\nfrom cdqa.utils.filters import filter_paragraphs\nfrom cdqa.utils.download import *\nfrom cdqa.pipeline import QAPipeline\n\ndef execute_pipeline(model, query, n_predictions=None):\n    download_bnpp_data(""./data/bnpp_newsroom_v1.1/"")\n    download_model(model + ""-squad_1.1"", dir=""./models"")\n    df = pd.read_csv(\n        ""./data/bnpp_newsroom_v1.1/bnpp_newsroom-v1.1.csv"",\n        converters={""paragraphs"": literal_eval},\n    )\n    df = filter_paragraphs(df)\n\n    reader_path = ""models/"" + model + ""_qa.joblib""\n\n    cdqa_pipeline = QAPipeline(reader=reader_path)\n    cdqa_pipeline.fit_retriever(df)\n\n    if n_predictions is not None:\n        predictions = cdqa_pipeline.predict(query, n_predictions=n_predictions)\n        result = []\n\n        for answer, title, paragraph, score in predictions:\n            prediction = (answer, title)\n            result.append(prediction)\n        return result\n    else:\n        prediction = cdqa_pipeline.predict(query)\n        result = (prediction[0], prediction[1])\n        return result\n\n@pytest.mark.parametrize(""model"", [""bert"", ""distilbert""])\ndef test_predict(model):\n    assert execute_pipeline(model,\n        ""Since when does the Excellence Program of BNP Paribas exist?""\n    ) == (""January 2016"", ""BNP Paribas\xe2\x80\x99 commitment to universities and schools"")\n\n\ndef test_n_predictions():\n    predictions = execute_pipeline(""distilbert"",\n        ""Since when does the Excellence Program of BNP Paribas exist?"", 5\n    )\n\n    assert len(predictions) == 5\n\n    assert predictions[0] == (\n        ""January 2016"",\n        ""BNP Paribas\xe2\x80\x99 commitment to universities and schools"",\n    )\n'"
tests/test_processor.py,0,"b'from cdqa.utils.download import *\nfrom cdqa.reader.bertqa_sklearn import convert_examples_to_features, read_squad_examples\nfrom transformers import BertTokenizer\n\n\ndef test_processor_functions():\n\n    download_squad(dir=""./data"")\n\n    tokenizer = BertTokenizer.from_pretrained(""bert-base-uncased"", do_lower_case=True)\n    max_seq_length = 256\n    max_query_length = 32\n    doc_stride = 128\n    is_training = False\n    verbose = False\n\n    examples = read_squad_examples(\n        ""./data/SQuAD_1.1/dev-v1.1.json"",\n        is_training=is_training,\n        version_2_with_negative=False,\n    )\n    assert len(examples) == 10570\n\n    features = convert_examples_to_features(\n        examples,\n        tokenizer,\n        max_seq_length,\n        doc_stride,\n        max_query_length,\n        is_training,\n        verbose,\n    )\n    assert len(features) == 12006\n'"
cdqa/pipeline/__init__.py,0,"b'from .cdqa_sklearn import QAPipeline\n\n__all__ = [""QAPipeline""]\n'"
cdqa/pipeline/cdqa_sklearn.py,5,"b'import joblib\nimport warnings\n\nimport pandas as pd\nimport numpy as np\nimport torch\n\nfrom sklearn.base import BaseEstimator\n\nfrom cdqa.retriever import TfidfRetriever, BM25Retriever\nfrom cdqa.utils.converters import generate_squad_examples\nfrom cdqa.reader import BertProcessor, BertQA\n\nRETRIEVERS = {""bm25"": BM25Retriever, ""tfidf"": TfidfRetriever}\n\n\nclass QAPipeline(BaseEstimator):\n    """"""\n    A scikit-learn implementation of the whole cdQA pipeline\n\n    Parameters\n    ----------\n    reader: str (path to .joblib) or .joblib object of an instance of BertQA (BERT model with sklearn wrapper), optional\n\n    retriever: ""bm25"" or ""tfidf""\n        The type of retriever\n\n    retrieve_by_doc: bool (default: True). If Retriever will rank by documents\n        or by paragraphs.\n\n    kwargs: kwargs for BertQA(), BertProcessor(), TfidfRetriever() and BM25Retriever()\n        Please check documentation for these classes\n\n    Examples\n    --------\n    >>> from cdqa.pipeline import QAPipeline\n    >>> qa_pipeline = QAPipeline(reader=\'bert_qa_squad_vCPU-sklearn.joblib\')\n    >>> qa_pipeline.fit_retriever(df=df)\n    >>> prediction = qa_pipeline.predict(query=\'When BNP Paribas was created?\')\n\n    >>> from cdqa.pipeline import QAPipeline\n    >>> qa_pipeline = QAPipeline()\n    >>> qa_pipeline.fit_reader(\'train-v1.1.json\')\n    >>> qa_pipeline.fit_retriever(df=df)\n    >>> prediction = qa_pipeline.predict(X=\'When BNP Paribas was created?\')\n\n    """"""\n\n    def __init__(self, reader=None, retriever=""bm25"", retrieve_by_doc=False, **kwargs):\n\n        if retriever not in RETRIEVERS:\n            raise ValueError(\n                ""You provided a type of retriever that is not supported. ""\n                + ""Please provide a retriver in the following list: ""\n                + str(list(RETRIEVERS.keys()))\n            )\n\n        retriever_class = RETRIEVERS[retriever]\n\n        # Separating kwargs\n        kwargs_bertqa = {\n            key: value\n            for key, value in kwargs.items()\n            if key in BertQA.__init__.__code__.co_varnames\n        }\n\n        kwargs_processor = {\n            key: value\n            for key, value in kwargs.items()\n            if key in BertProcessor.__init__.__code__.co_varnames\n        }\n\n        kwargs_retriever = {\n            key: value\n            for key, value in kwargs.items()\n            if key in retriever_class.__init__.__code__.co_varnames\n        }\n\n        if not reader:\n            self.reader = BertQA(**kwargs_bertqa)\n        elif type(reader) == str:\n            self.reader = joblib.load(reader)\n        else:\n            self.reader = reader\n\n        self.processor_train = BertProcessor(is_training=True, **kwargs_processor)\n\n        self.processor_predict = BertProcessor(is_training=False, **kwargs_processor)\n\n        self.retriever = retriever_class(**kwargs_retriever)\n\n        self.retrieve_by_doc = retrieve_by_doc\n\n        if torch.cuda.is_available():\n            self.cuda()\n\n    def fit_retriever(self, df: pd.DataFrame = None):\n        """""" Fit the QAPipeline retriever to a list of documents in a dataframe.\n        Parameters\n        ----------\n        df: pandas.Dataframe\n            Dataframe with the following columns: ""title"", ""paragraphs""\n        """"""\n\n        if self.retrieve_by_doc:\n            self.metadata = df\n            self.metadata[""content""] = self.metadata[""paragraphs""].apply(\n                lambda x: "" "".join(x)\n            )\n        else:\n            self.metadata = self._expand_paragraphs(df)\n\n        self.retriever.fit(self.metadata)\n\n        return self\n\n    def fit_reader(self, data=None):\n        """""" Fit the QAPipeline retriever to a list of documents in a dataframe.\n\n        Parameters\n        ----------\n        data: dict str-path to json file\n             Annotated dataset in squad-like for Reader training\n\n        """"""\n\n        train_examples, train_features = self.processor_train.fit_transform(data)\n        self.reader.fit(X=(train_examples, train_features))\n\n        return self\n\n    def predict(\n        self,\n        query: str = None,\n        n_predictions: int = None,\n        retriever_score_weight: float = 0.35,\n        return_all_preds: bool = False,\n    ):\n        """""" Compute prediction of an answer to a question\n\n        Parameters\n        ----------\n        query: str\n            Sample (question) to perform a prediction on\n\n        n_predictions: int or None (default: None).\n            Number of returned predictions. If None, only one prediction is return\n\n        retriever_score_weight: float (default: 0.35).\n            The weight of retriever score in the final score used for prediction.\n            Given retriever score and reader average of start and end logits, the final score used for ranking is:\n\n            final_score = retriever_score_weight * retriever_score + (1 - retriever_score_weight) * (reader_avg_logit)\n\n        return_all_preds: boolean (default: False)\n            whether to return a list of all predictions done by the Reader or not\n\n        Returns\n        -------\n        if return_all_preds is False:\n        prediction: tuple (answer, title, paragraph, score/logit)\n\n        if return_all_preds is True:\n        List of dictionnaries with all metadada of all answers outputted by the Reader\n        given the question.\n\n        """"""\n\n        if not isinstance(query, str):\n            raise TypeError(\n                ""The input is not a string. Please provide a string as input.""\n            )\n        if not (\n            isinstance(n_predictions, int) or n_predictions is None or n_predictions < 1\n        ):\n            raise TypeError(""n_predictions should be a positive Integer or None"")\n        best_idx_scores = self.retriever.predict(query)\n        squad_examples = generate_squad_examples(\n            question=query,\n            best_idx_scores=best_idx_scores,\n            metadata=self.metadata,\n            retrieve_by_doc=self.retrieve_by_doc,\n        )\n        examples, features = self.processor_predict.fit_transform(X=squad_examples)\n        prediction = self.reader.predict(\n            X=(examples, features),\n            n_predictions=n_predictions,\n            retriever_score_weight=retriever_score_weight,\n            return_all_preds=return_all_preds,\n        )\n        return prediction\n\n    def to(self, device):\n        """""" Send reader to CPU if device==\'cpu\' or to GPU if device==\'cuda\'\n        """"""\n        if device not in (""cpu"", ""cuda""):\n            raise ValueError(""Attribute device should be \'cpu\' or \'cuda\'."")\n\n        self.reader.model.to(device)\n        self.reader.device = torch.device(device)\n        return self\n\n    def cpu(self):\n        """""" Send reader to CPU\n        """"""\n        self.reader.model.cpu()\n        self.reader.device = torch.device(""cpu"")\n        return self\n\n    def cuda(self):\n        """""" Send reader to GPU\n        """"""\n        self.reader.model.cuda()\n        self.reader.device = torch.device(""cuda"")\n        return self\n\n    def dump_reader(self, filename):\n        """""" Dump reader model to a .joblib object\n        """"""\n        self.cpu()\n        joblib.dump(self.reader, filename)\n        if torch.cuda.is_available():\n            self.cuda()\n\n    @staticmethod\n    def _expand_paragraphs(df):\n        # Snippet taken from: https://stackoverflow.com/a/48532692/11514226\n        lst_col = ""paragraphs""\n        df = pd.DataFrame(\n            {\n                col: np.repeat(df[col].values, df[lst_col].str.len())\n                for col in df.columns.drop(lst_col)\n            }\n        ).assign(**{lst_col: np.concatenate(df[lst_col].values)})[df.columns]\n        df[""content""] = df[""paragraphs""]\n        return df.drop(""paragraphs"", axis=1)\n'"
cdqa/reader/__init__.py,0,"b'from .bertqa_sklearn import BertQA, BertProcessor\n\n__all__ = [""BertQA"", ""BertProcessor""]\n'"
cdqa/reader/bertqa_sklearn.py,32,"b'# coding=utf-8\n# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import, division, print_function\n\nimport argparse\nimport collections\nimport json\nimport logging\nimport math\nimport os\nimport random\nimport sys\nfrom io import open\nimport uuid\n\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\nfrom torch.utils.data.distributed import DistributedSampler\nfrom torch.optim.lr_scheduler import LambdaLR\nfrom tqdm.autonotebook import tqdm, trange\n\nfrom transformers import PYTORCH_PRETRAINED_BERT_CACHE, WEIGHTS_NAME, CONFIG_NAME\n\nfrom transformers import BertForQuestionAnswering, DistilBertForQuestionAnswering\nfrom transformers import BertConfig, DistilBertConfig\nfrom transformers import BertTokenizer, DistilBertTokenizer\nfrom transformers import AdamW\nfrom transformers.tokenization_bert import BasicTokenizer, whitespace_tokenize\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nif sys.version_info[0] == 2:\n    import cPickle as pickle\nelse:\n    import pickle\n\nlogger = logging.getLogger(__name__)\n\n\nclass SquadExample(object):\n    """"""\n    A single training/test example for the Squad dataset.\n    For examples without an answer, the start and end position are -1.\n    """"""\n\n    def __init__(\n        self,\n        qas_id,\n        question_text,\n        doc_tokens,\n        orig_answer_text=None,\n        start_position=None,\n        end_position=None,\n        is_impossible=None,\n        paragraph=None,\n        title=None,\n        retriever_score=None,\n    ):\n        self.qas_id = qas_id\n        self.question_text = question_text\n        self.doc_tokens = doc_tokens\n        self.orig_answer_text = orig_answer_text\n        self.start_position = start_position\n        self.end_position = end_position\n        self.is_impossible = is_impossible\n        self.paragraph = paragraph\n        self.title = title\n        self.retriever_score = retriever_score\n\n    def __str__(self):\n        return self.__repr__()\n\n    def __repr__(self):\n        s = """"\n        s += ""qas_id: %s"" % (self.qas_id)\n        s += "", question_text: %s"" % (self.question_text)\n        s += "", doc_tokens: [%s]"" % ("" "".join(self.doc_tokens))\n        if self.start_position:\n            s += "", start_position: %d"" % (self.start_position)\n        if self.end_position:\n            s += "", end_position: %d"" % (self.end_position)\n        if self.is_impossible:\n            s += "", is_impossible: %r"" % (self.is_impossible)\n        return s\n\n\nclass InputFeatures(object):\n    """"""A single set of features of data.""""""\n\n    def __init__(self,\n                 unique_id,\n                 example_index,\n                 doc_span_index,\n                 tokens,\n                 token_to_orig_map,\n                 token_is_max_context,\n                 input_ids,\n                 input_mask,\n                 segment_ids,\n                 cls_index,\n                 p_mask,\n                 paragraph_len,\n                 start_position=None,\n                 end_position=None,\n                 is_impossible=None):\n        self.unique_id = unique_id\n        self.example_index = example_index\n        self.doc_span_index = doc_span_index\n        self.tokens = tokens\n        self.token_to_orig_map = token_to_orig_map\n        self.token_is_max_context = token_is_max_context\n        self.input_ids = input_ids\n        self.input_mask = input_mask\n        self.segment_ids = segment_ids\n        self.cls_index = cls_index\n        self.p_mask = p_mask\n        self.paragraph_len = paragraph_len\n        self.start_position = start_position\n        self.end_position = end_position\n        self.is_impossible = is_impossible\n\n\ndef read_squad_examples(input_file, is_training, version_2_with_negative):\n    """"""Read a SQuAD json file into a list of SquadExample.""""""\n\n    if isinstance(input_file, str):\n        with open(input_file, ""r"", encoding=""utf-8"") as reader:\n            input_data = json.load(reader)[""data""]\n    else:\n        input_data = input_file\n\n    examples = []\n    for entry in input_data:\n        for paragraph in entry[""paragraphs""]:\n            paragraph_text = paragraph[""context""]\n            doc_tokens = []\n            char_to_word_offset = []\n            prev_is_whitespace = True\n            for c in paragraph_text:\n                if _is_whitespace(c):\n                    prev_is_whitespace = True\n                else:\n                    if prev_is_whitespace:\n                        doc_tokens.append(c)\n                    else:\n                        doc_tokens[-1] += c\n                    prev_is_whitespace = False\n                char_to_word_offset.append(len(doc_tokens) - 1)\n\n            for qa in paragraph[""qas""]:\n                qas_id = qa[""id""]\n                question_text = qa[""question""]\n                try:\n                    retriever_score = qa[""retriever_score""]\n                except KeyError:\n                    retriever_score = 0\n                start_position = None\n                end_position = None\n                orig_answer_text = None\n                is_impossible = False\n                if is_training:\n                    if version_2_with_negative:\n                        is_impossible = qa[""is_impossible""]\n                    if (len(qa[""answers""]) != 1) and (not is_impossible):\n                        raise ValueError(\n                            ""For training, each question should have exactly 1 answer.""\n                        )\n                    if not is_impossible:\n                        answer = qa[""answers""][0]\n                        orig_answer_text = answer[""text""]\n                        answer_offset = answer[""answer_start""]\n                        answer_length = len(orig_answer_text)\n                        start_position = char_to_word_offset[answer_offset]\n                        end_position = char_to_word_offset[\n                            answer_offset + answer_length - 1\n                        ]\n                        # Only add answers where the text can be exactly recovered from the\n                        # document. If this CAN\'T happen it\'s likely due to weird Unicode\n                        # stuff so we will just skip the example.\n                        #\n                        # Note that this means for training mode, every example is NOT\n                        # guaranteed to be preserved.\n                        actual_text = "" "".join(\n                            doc_tokens[start_position : (end_position + 1)]\n                        )\n                        cleaned_answer_text = "" "".join(\n                            whitespace_tokenize(orig_answer_text)\n                        )\n                        if actual_text.find(cleaned_answer_text) == -1:\n                            logger.warning(\n                                ""Could not find answer: \'%s\' vs. \'%s\'"",\n                                actual_text,\n                                cleaned_answer_text,\n                            )\n                            continue\n                    else:\n                        start_position = -1\n                        end_position = -1\n                        orig_answer_text = """"\n\n                examples.append(\n                    SquadExample(\n                        qas_id=qas_id,\n                        question_text=question_text,\n                        doc_tokens=doc_tokens,\n                        orig_answer_text=orig_answer_text,\n                        start_position=start_position,\n                        end_position=end_position,\n                        is_impossible=is_impossible,\n                        paragraph=paragraph_text,\n                        title=entry[""title""],\n                        retriever_score=retriever_score,\n                    )\n                )\n    return examples\n\ndef _is_whitespace(c):\n    if c == "" "" or c == ""\\t"" or c == ""\\r"" or c == ""\\n"" or ord(c) == 0x202F:\n        return True\n    return False\n\ndef convert_examples_to_features(\n    examples,\n    tokenizer,\n    max_seq_length,\n    doc_stride,\n    max_query_length,\n    is_training,\n    verbose,\n):\n    """"""Loads a data file into a list of `InputBatch`s.""""""\n\n    cls_token_at_end = False\n    cls_token = \'[CLS]\'\n    sep_token = \'[SEP]\'\n    pad_token = 0\n    sequence_a_segment_id = 0\n    sequence_b_segment_id = 1\n    cls_token_segment_id = 0\n    pad_token_segment_id = 0\n    mask_padding_with_zero = True\n\n    features = []\n\n    for (example_index, example) in enumerate(examples):\n        query_tokens = tokenizer.tokenize(example.question_text)\n\n        if len(query_tokens) > max_query_length:\n            query_tokens = query_tokens[0:max_query_length]\n\n        tok_to_orig_index = []\n        orig_to_tok_index = []\n        all_doc_tokens = []\n        for (i, token) in enumerate(example.doc_tokens):\n            orig_to_tok_index.append(len(all_doc_tokens))\n            sub_tokens = tokenizer.tokenize(token)\n            for sub_token in sub_tokens:\n                tok_to_orig_index.append(i)\n                all_doc_tokens.append(sub_token)\n\n        tok_start_position = None\n        tok_end_position = None\n        if is_training and example.is_impossible:\n            tok_start_position = -1\n            tok_end_position = -1\n        if is_training and not example.is_impossible:\n            tok_start_position = orig_to_tok_index[example.start_position]\n            if example.end_position < len(example.doc_tokens) - 1:\n                tok_end_position = orig_to_tok_index[example.end_position + 1] - 1\n            else:\n                tok_end_position = len(all_doc_tokens) - 1\n            (tok_start_position, tok_end_position) = _improve_answer_span(\n                all_doc_tokens,\n                tok_start_position,\n                tok_end_position,\n                tokenizer,\n                example.orig_answer_text,\n            )\n\n        # The -3 accounts for [CLS], [SEP] and [SEP]\n        max_tokens_for_doc = max_seq_length - len(query_tokens) - 3\n\n        # We can have documents that are longer than the maximum sequence length.\n        # To deal with this we do a sliding window approach, where we take chunks\n        # of the up to our max length with a stride of `doc_stride`.\n        _DocSpan = collections.namedtuple(  # pylint: disable=invalid-name\n            ""DocSpan"", [""start"", ""length""]\n        )\n        doc_spans = []\n        start_offset = 0\n        while start_offset < len(all_doc_tokens):\n            length = len(all_doc_tokens) - start_offset\n            if length > max_tokens_for_doc:\n                length = max_tokens_for_doc\n            doc_spans.append(_DocSpan(start=start_offset, length=length))\n            if start_offset + length == len(all_doc_tokens):\n                break\n            start_offset += min(length, doc_stride)\n\n        for (doc_span_index, doc_span) in enumerate(doc_spans):\n            unique_id = uuid.uuid4().int\n            tokens = []\n            token_to_orig_map = {}\n            token_is_max_context = {}\n            segment_ids = []\n\n            # p_mask: mask with 1 for token than cannot be in the answer (0 for token which can be in an answer)\n            # Original TF implem also keep the classification token (set to 0) (not sure why...)\n            p_mask = []\n\n            # CLS token at the beginning\n            if not cls_token_at_end:\n                tokens.append(cls_token)\n                segment_ids.append(cls_token_segment_id)\n                p_mask.append(0)\n                cls_index = 0\n\n            # Query\n            for token in query_tokens:\n                tokens.append(token)\n                segment_ids.append(sequence_a_segment_id)\n                p_mask.append(1)\n\n            # SEP token\n            tokens.append(sep_token)\n            segment_ids.append(sequence_a_segment_id)\n            p_mask.append(1)\n\n            # Paragraph\n            for i in range(doc_span.length):\n                split_token_index = doc_span.start + i\n                token_to_orig_map[len(tokens)] = tok_to_orig_index[split_token_index]\n\n                is_max_context = _check_is_max_context(doc_spans, doc_span_index,\n                                                       split_token_index)\n                token_is_max_context[len(tokens)] = is_max_context\n                tokens.append(all_doc_tokens[split_token_index])\n                segment_ids.append(sequence_b_segment_id)\n                p_mask.append(0)\n            paragraph_len = doc_span.length\n\n            # SEP token\n            tokens.append(sep_token)\n            segment_ids.append(sequence_b_segment_id)\n            p_mask.append(1)\n\n            # CLS token at the end\n            if cls_token_at_end:\n                tokens.append(cls_token)\n                segment_ids.append(cls_token_segment_id)\n                p_mask.append(0)\n                cls_index = len(tokens) - 1  # Index of classification token\n\n            input_ids = tokenizer.convert_tokens_to_ids(tokens)\n\n            # The mask has 1 for real tokens and 0 for padding tokens. Only real\n            # tokens are attended to.\n            input_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n\n            # Zero-pad up to the sequence length.\n            while len(input_ids) < max_seq_length:\n                input_ids.append(pad_token)\n                input_mask.append(0 if mask_padding_with_zero else 1)\n                segment_ids.append(pad_token_segment_id)\n                p_mask.append(1)\n\n            assert len(input_ids) == max_seq_length\n            assert len(input_mask) == max_seq_length\n            assert len(segment_ids) == max_seq_length\n\n            span_is_impossible = example.is_impossible\n            start_position = None\n            end_position = None\n            if is_training and not span_is_impossible:\n                # For training, if our document chunk does not contain an annotation\n                # we throw it out, since there is nothing to predict.\n                doc_start = doc_span.start\n                doc_end = doc_span.start + doc_span.length - 1\n                out_of_span = False\n                if not (tok_start_position >= doc_start and\n                        tok_end_position <= doc_end):\n                    out_of_span = True\n                if out_of_span:\n                    start_position = 0\n                    end_position = 0\n                    span_is_impossible = True\n                else:\n                    doc_offset = len(query_tokens) + 2\n                    start_position = tok_start_position - doc_start + doc_offset\n                    end_position = tok_end_position - doc_start + doc_offset\n\n            if is_training and span_is_impossible:\n                start_position = cls_index\n                end_position = cls_index\n\n            if example_index < 20 and verbose:\n                logger.info(""*** Example ***"")\n                logger.info(""unique_id: %s"" % (unique_id))\n                logger.info(""example_index: %s"" % (example_index))\n                logger.info(""doc_span_index: %s"" % (doc_span_index))\n                logger.info(""tokens: %s"" % "" "".join(tokens))\n                logger.info(""token_to_orig_map: %s"" % "" "".join([\n                    ""%d:%d"" % (x, y) for (x, y) in token_to_orig_map.items()]))\n                logger.info(""token_is_max_context: %s"" % "" "".join([\n                    ""%d:%s"" % (x, y) for (x, y) in token_is_max_context.items()\n                ]))\n                logger.info(""input_ids: %s"" % "" "".join([str(x) for x in input_ids]))\n                logger.info(\n                    ""input_mask: %s"" % "" "".join([str(x) for x in input_mask]))\n                logger.info(\n                    ""segment_ids: %s"" % "" "".join([str(x) for x in segment_ids]))\n                if is_training and span_is_impossible:\n                    logger.info(""impossible example"")\n                if is_training and not span_is_impossible:\n                    answer_text = "" "".join(tokens[start_position:(end_position + 1)])\n                    logger.info(""start_position: %d"" % (start_position))\n                    logger.info(""end_position: %d"" % (end_position))\n                    logger.info(\n                        ""answer: %s"" % (answer_text))\n\n            features.append(\n                InputFeatures(\n                    unique_id=unique_id,\n                    example_index=example_index,\n                    doc_span_index=doc_span_index,\n                    tokens=tokens,\n                    token_to_orig_map=token_to_orig_map,\n                    token_is_max_context=token_is_max_context,\n                    input_ids=input_ids,\n                    input_mask=input_mask,\n                    segment_ids=segment_ids,\n                    cls_index=cls_index,\n                    p_mask=p_mask,\n                    paragraph_len=paragraph_len,\n                    start_position=start_position,\n                    end_position=end_position,\n                    is_impossible=span_is_impossible))\n\n    return features\n\ndef _improve_answer_span(\n    doc_tokens, input_start, input_end, tokenizer, orig_answer_text\n):\n    """"""Returns tokenized answer spans that better match the annotated answer.""""""\n\n    # The SQuAD annotations are character based. We first project them to\n    # whitespace-tokenized words. But then after WordPiece tokenization, we can\n    # often find a ""better match"". For example:\n    #\n    #   Question: What year was John Smith born?\n    #   Context: The leader was John Smith (1895-1943).\n    #   Answer: 1895\n    #\n    # The original whitespace-tokenized answer will be ""(1895-1943)."". However\n    # after tokenization, our tokens will be ""( 1895 - 1943 ) ."". So we can match\n    # the exact answer, 1895.\n    #\n    # However, this is not always possible. Consider the following:\n    #\n    #   Question: What country is the top exporter of electornics?\n    #   Context: The Japanese electronics industry is the lagest in the world.\n    #   Answer: Japan\n    #\n    # In this case, the annotator chose ""Japan"" as a character sub-span of\n    # the word ""Japanese"". Since our WordPiece tokenizer does not split\n    # ""Japanese"", we just use ""Japanese"" as the annotation. This is fairly rare\n    # in SQuAD, but does happen.\n    tok_answer_text = "" "".join(tokenizer.tokenize(orig_answer_text))\n\n    for new_start in range(input_start, input_end + 1):\n        for new_end in range(input_end, new_start - 1, -1):\n            text_span = "" "".join(doc_tokens[new_start : (new_end + 1)])\n            if text_span == tok_answer_text:\n                return (new_start, new_end)\n\n    return (input_start, input_end)\n\n\ndef _check_is_max_context(doc_spans, cur_span_index, position):\n    """"""Check if this is the \'max context\' doc span for the token.""""""\n\n    # Because of the sliding window approach taken to scoring documents, a single\n    # token can appear in multiple documents. E.g.\n    #  Doc: the man went to the store and bought a gallon of milk\n    #  Span A: the man went to the\n    #  Span B: to the store and bought\n    #  Span C: and bought a gallon of\n    #  ...\n    #\n    # Now the word \'bought\' will have two scores from spans B and C. We only\n    # want to consider the score with ""maximum context"", which we define as\n    # the *minimum* of its left and right context (the *sum* of left and\n    # right context will always be the same, of course).\n    #\n    # In the example the maximum context for \'bought\' would be span C since\n    # it has 1 left context and 3 right context, while span B has 4 left context\n    # and 0 right context.\n    best_score = None\n    best_span_index = None\n    for (span_index, doc_span) in enumerate(doc_spans):\n        end = doc_span.start + doc_span.length - 1\n        if position < doc_span.start:\n            continue\n        if position > end:\n            continue\n        num_left_context = position - doc_span.start\n        num_right_context = end - position\n        score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n        if best_score is None or score > best_score:\n            best_score = score\n            best_span_index = span_index\n\n    return cur_span_index == best_span_index\n\n\nRawResult = collections.namedtuple(\n    ""RawResult"", [""unique_id"", ""start_logits"", ""end_logits""]\n)\n\n\ndef write_predictions(\n    all_examples,\n    all_features,\n    all_results,\n    n_best_size,\n    max_answer_length,\n    do_lower_case,\n    output_prediction_file,\n    output_nbest_file,\n    output_null_log_odds_file,\n    verbose_logging,\n    version_2_with_negative,\n    null_score_diff_threshold,\n    retriever_score_weight,\n    n_predictions=None,\n):\n    """"""\n    Write final predictions to the json file and log-odds of null if needed.\n    It returns:\n        - if n_predictions == None: a tuple (best_prediction, final_predictions)\n        - if n_predictions != None: a tuple (best_prediction, final_predictions, n_best_predictions_list)\n    """"""\n    if verbose_logging:\n        logger.info(""Writing predictions to: %s"" % (output_prediction_file))\n        logger.info(""Writing nbest to: %s"" % (output_nbest_file))\n\n    example_index_to_features = collections.defaultdict(list)\n    for feature in all_features:\n        example_index_to_features[feature.example_index].append(feature)\n\n    unique_id_to_result = {}\n    for result in all_results:\n        unique_id_to_result[result.unique_id] = result\n\n    _PrelimPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n        ""PrelimPrediction"",\n        [""feature_index"", ""start_index"", ""end_index"", ""start_logit"", ""end_logit""],\n    )\n\n    all_predictions = collections.OrderedDict()\n    all_nbest_json = collections.OrderedDict()\n    scores_diff_json = collections.OrderedDict()\n    final_predictions = []\n\n    for (example_index, example) in enumerate(all_examples):\n        features = example_index_to_features[example_index]\n\n        prelim_predictions = []\n        # keep track of the minimum score of null start+end of position 0\n        score_null = 1000000  # large and positive\n        min_null_feature_index = 0  # the paragraph slice with min null score\n        null_start_logit = 0  # the start logit at the slice with min null score\n        null_end_logit = 0  # the end logit at the slice with min null score\n        for (feature_index, feature) in enumerate(features):\n            result = unique_id_to_result[feature.unique_id]\n            start_indexes = _get_best_indexes(result.start_logits, n_best_size)\n            end_indexes = _get_best_indexes(result.end_logits, n_best_size)\n            # if we could have irrelevant answers, get the min score of irrelevant\n            if version_2_with_negative:\n                feature_null_score = result.start_logits[0] + result.end_logits[0]\n                if feature_null_score < score_null:\n                    score_null = feature_null_score\n                    min_null_feature_index = feature_index\n                    null_start_logit = result.start_logits[0]\n                    null_end_logit = result.end_logits[0]\n            for start_index in start_indexes:\n                for end_index in end_indexes:\n                    # We could hypothetically create invalid predictions, e.g., predict\n                    # that the start of the span is in the question. We throw out all\n                    # invalid predictions.\n                    if start_index >= len(feature.tokens):\n                        continue\n                    if end_index >= len(feature.tokens):\n                        continue\n                    if start_index not in feature.token_to_orig_map:\n                        continue\n                    if end_index not in feature.token_to_orig_map:\n                        continue\n                    if not feature.token_is_max_context.get(start_index, False):\n                        continue\n                    if end_index < start_index:\n                        continue\n                    length = end_index - start_index + 1\n                    if length > max_answer_length:\n                        continue\n                    prelim_predictions.append(\n                        _PrelimPrediction(\n                            feature_index=feature_index,\n                            start_index=start_index,\n                            end_index=end_index,\n                            start_logit=result.start_logits[start_index],\n                            end_logit=result.end_logits[end_index],\n                        )\n                    )\n        if version_2_with_negative:\n            prelim_predictions.append(\n                _PrelimPrediction(\n                    feature_index=min_null_feature_index,\n                    start_index=0,\n                    end_index=0,\n                    start_logit=null_start_logit,\n                    end_logit=null_end_logit,\n                )\n            )\n        prelim_predictions = sorted(\n            prelim_predictions,\n            key=lambda x: (x.start_logit + x.end_logit),\n            reverse=True,\n        )\n\n        _NbestPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n            ""NbestPrediction"", [""text"", ""start_logit"", ""end_logit""]\n        )\n\n        seen_predictions = {}\n        nbest = []\n        for pred in prelim_predictions:\n            if len(nbest) >= n_best_size:\n                break\n            feature = features[pred.feature_index]\n            if pred.start_index > 0:  # this is a non-null prediction\n                tok_tokens = feature.tokens[pred.start_index : (pred.end_index + 1)]\n                orig_doc_start = feature.token_to_orig_map[pred.start_index]\n                orig_doc_end = feature.token_to_orig_map[pred.end_index]\n                orig_tokens = example.doc_tokens[orig_doc_start : (orig_doc_end + 1)]\n                tok_text = "" "".join(tok_tokens)\n\n                # De-tokenize WordPieces that have been split off.\n                tok_text = tok_text.replace("" ##"", """")\n                tok_text = tok_text.replace(""##"", """")\n\n                # Clean whitespace\n                tok_text = tok_text.strip()\n                tok_text = "" "".join(tok_text.split())\n                orig_text = "" "".join(orig_tokens)\n\n                final_text = get_final_text(\n                    tok_text, orig_text, do_lower_case, verbose_logging\n                )\n                if final_text in seen_predictions:\n                    continue\n\n                seen_predictions[final_text] = True\n            else:\n                final_text = """"\n                seen_predictions[final_text] = True\n\n            nbest.append(\n                _NbestPrediction(\n                    text=final_text,\n                    start_logit=pred.start_logit,\n                    end_logit=pred.end_logit,\n                )\n            )\n        # if we didn\'t include the empty option in the n-best, include it\n        if version_2_with_negative:\n            if """" not in seen_predictions:\n                nbest.append(\n                    _NbestPrediction(\n                        text="""", start_logit=null_start_logit, end_logit=null_end_logit\n                    )\n                )\n\n            # In very rare edge cases we could only have single null prediction.\n            # So we just create a nonce prediction in this case to avoid failure.\n            if len(nbest) == 1:\n                nbest.insert(\n                    0, _NbestPrediction(text=""empty"", start_logit=0.0, end_logit=0.0)\n                )\n\n        # In very rare edge cases we could have no valid predictions. So we\n        # just create a nonce prediction in this case to avoid failure.\n        if not nbest:\n            nbest.append(_NbestPrediction(text=""empty"", start_logit=0.0, end_logit=0.0))\n\n        assert len(nbest) >= 1\n\n        total_scores = []\n        best_non_null_entry = None\n        for entry in nbest:\n            total_scores.append(entry.start_logit + entry.end_logit)\n            if not best_non_null_entry:\n                if entry.text:\n                    best_non_null_entry = entry\n\n        probs = _compute_softmax(total_scores)\n\n        nbest_json = []\n        for (i, entry) in enumerate(nbest):\n            output = {}\n            output[""text""] = entry.text\n            output[""probability""] = probs[i]\n            output[""start_logit""] = entry.start_logit\n            output[""end_logit""] = entry.end_logit\n            nbest_json.append(output)\n\n        assert len(nbest_json) >= 1\n\n        if not version_2_with_negative:\n            all_predictions[example.qas_id] = nbest_json[0][""text""]\n        else:\n            # predict """" iff the null score - the score of best non-null > threshold\n            score_diff = (\n                score_null\n                - best_non_null_entry.start_logit\n                - (best_non_null_entry.end_logit)\n            )\n            scores_diff_json[example.qas_id] = score_diff\n            if score_diff > null_score_diff_threshold:\n                all_predictions[example.qas_id] = """"\n            else:\n                all_predictions[example.qas_id] = best_non_null_entry.text\n                all_nbest_json[example.qas_id] = nbest_json\n\n        best_dict = nbest_json[0]\n        best_dict[""qas_id""] = example.qas_id\n        best_dict[""title""] = example.title\n        best_dict[""paragraph""] = example.paragraph\n        best_dict[""retriever_score""] = float(example.retriever_score)\n        best_dict[""final_score""] = (1 - retriever_score_weight) * (\n            best_dict[""start_logit""] + best_dict[""end_logit""]\n        ) + retriever_score_weight * best_dict[""retriever_score""]\n        final_predictions.append(best_dict)\n\n    final_predictions_sorted = sorted(\n        final_predictions, key=lambda d: d[""final_score""], reverse=True\n    )\n\n    best_prediction = (\n        final_predictions_sorted[0][""text""],\n        final_predictions_sorted[0][""title""],\n        final_predictions_sorted[0][""paragraph""],\n        final_predictions_sorted[0][""final_score""],\n    )\n\n    return_list = [best_prediction, final_predictions_sorted]\n\n    if n_predictions:\n        n_best_predictions_list = _n_best_predictions(\n            final_predictions_sorted, n_predictions\n        )\n        return_list.append(n_best_predictions_list)\n\n    if output_prediction_file:\n        with open(output_prediction_file, ""w"") as writer:\n            writer.write(json.dumps(all_predictions, indent=4) + ""\\n"")\n    if output_nbest_file:\n        with open(output_nbest_file, ""w"") as writer:\n            writer.write(json.dumps(all_nbest_json, indent=4) + ""\\n"")\n    if version_2_with_negative and output_null_log_odds_file:\n        with open(output_null_log_odds_file, ""w"") as writer:\n            writer.write(json.dumps(scores_diff_json, indent=4) + ""\\n"")\n\n    return tuple(return_list)\n\n\ndef get_final_text(pred_text, orig_text, do_lower_case, verbose_logging=False):\n    """"""Project the tokenized prediction back to the original text.""""""\n\n    # When we created the data, we kept track of the alignment between original\n    # (whitespace tokenized) tokens and our WordPiece tokenized tokens. So\n    # now `orig_text` contains the span of our original text corresponding to the\n    # span that we predicted.\n    #\n    # However, `orig_text` may contain extra characters that we don\'t want in\n    # our prediction.\n    #\n    # For example, let\'s say:\n    #   pred_text = steve smith\n    #   orig_text = Steve Smith\'s\n    #\n    # We don\'t want to return `orig_text` because it contains the extra ""\'s"".\n    #\n    # We don\'t want to return `pred_text` because it\'s already been normalized\n    # (the SQuAD eval script also does punctuation stripping/lower casing but\n    # our tokenizer does additional normalization like stripping accent\n    # characters).\n    #\n    # What we really want to return is ""Steve Smith"".\n    #\n    # Therefore, we have to apply a semi-complicated alignment heuristic between\n    # `pred_text` and `orig_text` to get a character-to-character alignment. This\n    # can fail in certain cases in which case we just return `orig_text`.\n\n    def _strip_spaces(text):\n        ns_chars = []\n        ns_to_s_map = collections.OrderedDict()\n        for (i, c) in enumerate(text):\n            if c == "" "":\n                continue\n            ns_to_s_map[len(ns_chars)] = i\n            ns_chars.append(c)\n        ns_text = """".join(ns_chars)\n        return (ns_text, ns_to_s_map)\n\n    # We first tokenize `orig_text`, strip whitespace from the result\n    # and `pred_text`, and check if they are the same length. If they are\n    # NOT the same length, the heuristic has failed. If they are the same\n    # length, we assume the characters are one-to-one aligned.\n    tokenizer = BasicTokenizer(do_lower_case=do_lower_case)\n\n    tok_text = "" "".join(tokenizer.tokenize(orig_text))\n\n    start_position = tok_text.find(pred_text)\n    if start_position == -1:\n        if verbose_logging:\n            logger.info(""Unable to find text: \'%s\' in \'%s\'"" % (pred_text, orig_text))\n        return orig_text\n    end_position = start_position + len(pred_text) - 1\n\n    (orig_ns_text, orig_ns_to_s_map) = _strip_spaces(orig_text)\n    (tok_ns_text, tok_ns_to_s_map) = _strip_spaces(tok_text)\n\n    if len(orig_ns_text) != len(tok_ns_text):\n        if verbose_logging:\n            logger.info(\n                ""Length not equal after stripping spaces: \'%s\' vs \'%s\'"",\n                orig_ns_text,\n                tok_ns_text,\n            )\n        return orig_text\n\n    # We then project the characters in `pred_text` back to `orig_text` using\n    # the character-to-character alignment.\n    tok_s_to_ns_map = {}\n    for (i, tok_index) in tok_ns_to_s_map.items():\n        tok_s_to_ns_map[tok_index] = i\n\n    orig_start_position = None\n    if start_position in tok_s_to_ns_map:\n        ns_start_position = tok_s_to_ns_map[start_position]\n        if ns_start_position in orig_ns_to_s_map:\n            orig_start_position = orig_ns_to_s_map[ns_start_position]\n\n    if orig_start_position is None:\n        if verbose_logging:\n            logger.info(""Couldn\'t map start position"")\n        return orig_text\n\n    orig_end_position = None\n    if end_position in tok_s_to_ns_map:\n        ns_end_position = tok_s_to_ns_map[end_position]\n        if ns_end_position in orig_ns_to_s_map:\n            orig_end_position = orig_ns_to_s_map[ns_end_position]\n\n    if orig_end_position is None:\n        if verbose_logging:\n            logger.info(""Couldn\'t map end position"")\n        return orig_text\n\n    output_text = orig_text[orig_start_position : (orig_end_position + 1)]\n    return output_text\n\n\ndef _get_best_indexes(logits, n_best_size):\n    """"""Get the n-best logits from a list.""""""\n    index_and_score = sorted(enumerate(logits), key=lambda x: x[1], reverse=True)\n\n    best_indexes = []\n    for i in range(len(index_and_score)):\n        if i >= n_best_size:\n            break\n        best_indexes.append(index_and_score[i][0])\n    return best_indexes\n\n\ndef _compute_softmax(scores):\n    """"""Compute softmax probability over raw logits.""""""\n    if not scores:\n        return []\n\n    max_score = None\n    for score in scores:\n        if max_score is None or score > max_score:\n            max_score = score\n\n    exp_scores = []\n    total_sum = 0.0\n    for score in scores:\n        x = math.exp(score - max_score)\n        exp_scores.append(x)\n        total_sum += x\n\n    probs = []\n    for score in exp_scores:\n        probs.append(score / total_sum)\n    return probs\n\n\ndef _n_best_predictions(final_predictions_sorted, n):\n    n = min(n, len(final_predictions_sorted))\n    final_prediction_list = []\n    for i in range(n):\n        curr_pred = (\n            final_predictions_sorted[i][""text""],\n            final_predictions_sorted[i][""title""],\n            final_predictions_sorted[i][""paragraph""],\n            final_predictions_sorted[i][""final_score""],\n        )\n        final_prediction_list.append(curr_pred)\n    return final_prediction_list\n\ndef get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps, last_epoch=-1):\n    """""" Create a schedule with a learning rate that decreases linearly after\n    linearly increasing during a warmup period.\n    """"""\n    def lr_lambda(current_step):\n        if current_step < num_warmup_steps:\n            return float(current_step) / float(max(1, num_warmup_steps))\n        return max(0.0, float(num_training_steps - current_step) / float(max(1, num_training_steps - num_warmup_steps)))\n\n    return LambdaLR(optimizer, lr_lambda, last_epoch)\n\nclass BertProcessor(BaseEstimator, TransformerMixin):\n    """"""\n    A scikit-learn transformer to convert SQuAD examples to BertQA input format.\n\n    Parameters\n    ----------\n    bert_version : str\n        Bert pre-trained model selected in the list: bert-base-uncased,\n        bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased,\n        bert-base-multilingual-cased, bert-base-chinese.\n    do_lower_case : bool, optional\n        Whether to lower case the input text. True for uncased models, False for cased models.\n        Default: True\n    is_training : bool, optional\n        Whether you are in training phase.\n    version_2_with_negative : bool, optional\n        If true, the SQuAD examples contain some that do not have an answer.\n    max_seq_length : int, optional\n        The maximum total input sequence length after WordPiece tokenization. Sequences\n        longer than this will be truncated, and sequences shorter than this will be padded.\n    doc_stride : int, optional\n        When splitting up a long document into chunks, how much stride to take between chunks.\n    max_query_length : int, optional\n        The maximum number of tokens for the question. Questions longer than this will\n        be truncated to this length.\n    verbose : bool, optional\n        If true, all of the warnings related to data processing will be printed.\n\n    Returns\n    -------\n    examples : list\n        SquadExample\n    features : list\n        InputFeatures\n\n    Examples\n    --------\n    >>> from cdqa.reader import BertProcessor\n    >>> processor = BertProcessor(bert_model=\'bert-base-uncased\', do_lower_case=True, is_training=False)\n    >>> examples, features = processor.fit_transform(X=squad_examples)\n\n    """"""\n\n    def __init__(\n        self,\n        bert_model=""bert-base-uncased"",\n        do_lower_case=True,\n        is_training=False,\n        version_2_with_negative=False,\n        max_seq_length=384,\n        doc_stride=128,\n        max_query_length=64,\n        verbose=False,\n        tokenizer=None,\n    ):\n\n        self.bert_model = bert_model\n        self.do_lower_case = do_lower_case\n        self.is_training = is_training\n        self.version_2_with_negative = version_2_with_negative\n        self.max_seq_length = max_seq_length\n        self.doc_stride = doc_stride\n        self.max_query_length = max_query_length\n        self.verbose = verbose\n\n        if tokenizer is None:\n            self.tokenizer = BertTokenizer.from_pretrained(\n                self.bert_model, do_lower_case=self.do_lower_case\n            )\n        else:\n            self.tokenizer = tokenizer\n            logger.info(""loading custom tokenizer"")\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        examples = read_squad_examples(\n            input_file=X,\n            is_training=self.is_training,\n            version_2_with_negative=self.version_2_with_negative,\n        )\n\n        features = convert_examples_to_features(\n            examples=examples,\n            tokenizer=self.tokenizer,\n            max_seq_length=self.max_seq_length,\n            doc_stride=self.doc_stride,\n            max_query_length=self.max_query_length,\n            is_training=self.is_training,\n            verbose=self.verbose,\n        )\n\n        return examples, features\n\n\nclass BertQA(BaseEstimator):\n    """"""\n    A scikit-learn estimator for BertForQuestionAnswering.\n\n    Parameters\n    ----------\n    bert_model : str\n        Bert pre-trained model selected in the list: bert-base-uncased,\n        bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased,\n        bert-base-multilingual-cased, bert-base-chinese.\n    train_batch_size : int, optional\n        Total batch size for training. (the default is 32)\n    predict_batch_size : int, optional\n        Total batch size for predictions. (the default is 8)\n    learning_rate : float, optional\n        The initial learning rate for Adam. (the default is 5e-5)\n    num_train_epochs : float, optional\n        Total number of training epochs to perform. (the default is 3.0)\n    warmup_proportion : float, optional\n        Proportion of training to perform linear learning rate warmup for. E.g., 0.1 = 10%%\n        of training. (the default is 0.1)\n    warmup_steps : int, optional\n        Linear warmup over warmup_steps.\n    adam_epsilon : float\n        Epsilon for Adam optimizer. (default: 1e-8)\n    n_best_size : int, optional\n        The total number of n-best predictions to generate in the nbest_predictions.json\n        output file. (the default is 20)\n    max_answer_length : int, optional\n        The maximum length of an answer that can be generated. This is needed because the start\n        and end predictions are not conditioned on one another. (the default is 30)\n    verbose_logging : bool, optional\n        If true, all of the warnings related to data processing will be printed.\n        A number of warnings are expected for a normal SQuAD evaluation. (the default is False)\n    no_cuda : bool, optional\n        Whether not to use CUDA when available (the default is False)\n    seed : int, optional\n        random seed for initialization (the default is 42)\n    gradient_accumulation_steps : int, optional\n        Number of updates steps to accumulate before performing a backward/update pass. (the default is 1)\n    do_lower_case : bool, optional\n        Whether to lower case the input text. True for uncased models, False for cased models. (the default is True)\n    local_rank : int, optional\n        local_rank for distributed training on gpus (the default is -1)\n    fp16 : bool, optional\n        Whether to use 16-bit float precision instead of 32-bit (the default is False)\n    loss_scale : int, optional\n        Loss scaling to improve fp16 numeric stability. Only used when fp16 set to True.\n        0 (default value): dynamic loss scaling.\n        Positive power of 2: static loss scaling value. (the default is 0)\n    version_2_with_negative : bool, optional\n        If true, the SQuAD examples contain some that do not have an answer. (the default is False)\n    null_score_diff_threshold : float, optional\n        If null_score - best_non_null is greater than the threshold predict null. (the default is 0.0)\n    output_dir : str, optional\n        The output directory where the model checkpoints and predictions will be written.\n        If None, nothing is saved. (the default is None)\n    server_ip : str, optional\n        Can be used for distant debugging. (the default is \'\')\n    server_port : str, optional\n        Can be used for distant debugging. (the default is \'\')\n\n\n    Attributes\n    ----------\n    device : torch.device\n        [description]\n    n_gpu : int\n        [description]\n    model : pytorch_pretrained_bert.modeling.BertForQuestionAnswering\n        [description]\n\n    Examples\n    --------\n    >>> from cdqa.reader import BertQA\n    >>> model = BertQA(bert_model=\'bert-base-uncased\',\n                train_batch_size=12,\n                learning_rate=3e-5,\n                num_train_epochs=2,\n                do_lower_case=True,\n                fp16=True,\n                output_dir=\'models/bert_qa_squad_v1.1_sklearn\')\n    >>> model.fit(X=(train_examples, train_features))\n    >>> final_prediction = model.predict(X=(test_examples, test_features))\n\n    """"""\n\n    def __init__(\n        self,\n        bert_model=""bert-base-uncased"",\n        train_batch_size=32,\n        predict_batch_size=8,\n        learning_rate=5e-5,\n        num_train_epochs=3.0,\n        warmup_proportion=0.1,\n        warmup_steps=0,\n        adam_epsilon=1e-8,\n        n_best_size=20,\n        max_answer_length=30,\n        verbose_logging=False,\n        no_cuda=False,\n        seed=42,\n        gradient_accumulation_steps=1,\n        do_lower_case=True,\n        local_rank=-1,\n        fp16=False,\n        loss_scale=0,\n        version_2_with_negative=False,\n        null_score_diff_threshold=0.0,\n        output_dir=None,\n        server_ip="""",\n        server_port="""",\n    ):\n\n        self.bert_model = bert_model\n        self.train_batch_size = train_batch_size\n        self.predict_batch_size = predict_batch_size\n        self.learning_rate = learning_rate\n        self.num_train_epochs = num_train_epochs\n        self.warmup_proportion = warmup_proportion\n        self.warmup_steps = warmup_steps\n        self.adam_epsilon = adam_epsilon\n        self.n_best_size = n_best_size\n        self.max_answer_length = max_answer_length\n        self.verbose_logging = verbose_logging\n        self.no_cuda = no_cuda\n        self.seed = seed\n        self.gradient_accumulation_steps = gradient_accumulation_steps\n        self.do_lower_case = do_lower_case\n        self.local_rank = local_rank\n        self.fp16 = fp16\n        self.loss_scale = loss_scale\n        self.version_2_with_negative = version_2_with_negative\n        self.null_score_diff_threshold = null_score_diff_threshold\n        self.output_dir = output_dir\n        self.server_ip = server_ip\n        self.server_port = server_port\n\n        # Prepare model\n        self.model = BertForQuestionAnswering.from_pretrained(\n            self.bert_model,\n            cache_dir=os.path.join(\n                str(PYTORCH_PRETRAINED_BERT_CACHE),\n                ""distributed_{}"".format(self.local_rank),\n            ),\n        )\n\n        if self.server_ip and self.server_port:\n            # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script\n            import ptvsd\n\n            print(""Waiting for debugger attach"")\n            ptvsd.enable_attach(\n                address=(self.server_ip, self.server_port), redirect_output=True\n            )\n            ptvsd.wait_for_attach()\n\n        if self.local_rank == -1 or self.no_cuda:\n            self.device = torch.device(\n                ""cuda"" if torch.cuda.is_available() and not self.no_cuda else ""cpu""\n            )\n            self.n_gpu = torch.cuda.device_count()\n        else:\n            torch.cuda.set_device(self.local_rank)\n            self.device = torch.device(""cuda"", self.local_rank)\n            self.n_gpu = 1\n            # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n            torch.distributed.init_process_group(backend=""nccl"")\n\n        if self.verbose_logging:\n            logging.basicConfig(\n                format=""%(asctime)s - %(levelname)s - %(name)s -   %(message)s"",\n                datefmt=""%m/%d/%Y %H:%M:%S"",\n                level=logging.INFO if self.local_rank in [-1, 0] else logging.WARN,\n            )\n\n            logger.info(\n                ""device: {} n_gpu: {}, distributed training: {}, 16-bits training: {}"".format(\n                    self.device, self.n_gpu, bool(self.local_rank != -1), self.fp16\n                )\n            )\n\n    def fit(self, X, y=None):\n\n        train_examples, train_features = X\n\n        if self.gradient_accumulation_steps < 1:\n            raise ValueError(\n                ""Invalid gradient_accumulation_steps parameter: {}, should be >= 1"".format(\n                    self.gradient_accumulation_steps\n                )\n            )\n\n        self.train_batch_size = (\n            self.train_batch_size // self.gradient_accumulation_steps\n        )\n\n        random.seed(self.seed)\n        np.random.seed(self.seed)\n        torch.manual_seed(self.seed)\n        if self.n_gpu > 0:\n            torch.cuda.manual_seed_all(self.seed)\n\n        if self.output_dir and not os.path.exists(self.output_dir):\n            os.makedirs(self.output_dir)\n\n        if self.fp16:\n            self.model.half()\n        self.model.to(self.device)\n        if self.local_rank != -1:\n            try:\n                from apex.parallel import DistributedDataParallel as DDP\n            except ImportError:\n                raise ImportError(\n                    ""Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training.""\n                )\n\n            self.model = DDP(self.model)\n        elif self.n_gpu > 1:\n            self.model = torch.nn.DataParallel(self.model)\n\n        global_step = 0\n\n        all_input_ids = torch.tensor(\n            [f.input_ids for f in train_features], dtype=torch.long\n        )\n        all_input_mask = torch.tensor(\n            [f.input_mask for f in train_features], dtype=torch.long\n        )\n        all_segment_ids = torch.tensor(\n            [f.segment_ids for f in train_features], dtype=torch.long\n        )\n        all_start_positions = torch.tensor(\n            [f.start_position for f in train_features], dtype=torch.long\n        )\n        all_end_positions = torch.tensor(\n            [f.end_position for f in train_features], dtype=torch.long\n        )\n        train_data = TensorDataset(\n            all_input_ids,\n            all_input_mask,\n            all_segment_ids,\n            all_start_positions,\n            all_end_positions,\n        )\n        if self.local_rank == -1:\n            train_sampler = RandomSampler(train_data)\n        else:\n            train_sampler = DistributedSampler(train_data)\n        train_dataloader = DataLoader(\n            train_data, sampler=train_sampler, batch_size=self.train_batch_size\n        )\n\n        num_train_optimization_steps = (\n            len(train_dataloader)\n            // self.gradient_accumulation_steps\n            * self.num_train_epochs\n        )\n\n        if self.local_rank != -1:\n            num_train_optimization_steps = (\n                num_train_optimization_steps // torch.distributed.get_world_size()\n            )\n\n        if self.verbose_logging:\n            logger.info(""***** Running training *****"")\n            logger.info(""  Num orig examples = %d"", len(train_examples))\n            logger.info(""  Num split examples = %d"", len(train_features))\n            logger.info(""  Batch size = %d"", self.train_batch_size)\n            logger.info(""  Num steps = %d"", num_train_optimization_steps)\n\n        # Prepare optimizer\n        param_optimizer = list(self.model.named_parameters())\n\n        # hack to remove pooler, which is not used\n        # thus it produce None grad that break apex\n        param_optimizer = [n for n in param_optimizer if ""pooler"" not in n[0]]\n\n        no_decay = [""bias"", ""LayerNorm.bias"", ""LayerNorm.weight""]\n        optimizer_grouped_parameters = [\n            {\n                ""params"": [\n                    p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n                ],\n                ""weight_decay"": 0.01,\n            },\n            {\n                ""params"": [\n                    p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n                ],\n                ""weight_decay"": 0.0,\n            },\n        ]\n\n        if self.fp16:\n            try:\n                from apex.optimizers import FP16_Optimizer\n                from apex.optimizers import FusedAdam\n            except ImportError:\n                raise ImportError(\n                    ""Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training.""\n                )\n\n            optimizer = FusedAdam(\n                optimizer_grouped_parameters,\n                lr=self.learning_rate,\n                bias_correction=False,\n                max_grad_norm=1.0,\n            )\n            if self.loss_scale == 0:\n                optimizer = FP16_Optimizer(\n                    optimizer, dynamic_loss_scale=True, verbose=False\n                )\n            else:\n                optimizer = FP16_Optimizer(\n                    optimizer, static_loss_scale=self.loss_scale, verbose=False\n                )\n            warmup_linear = WarmupLinearSchedule(\n                warmup=self.warmup_proportion, t_total=num_train_optimization_steps\n            )\n        else:\n            optimizer = AdamW(optimizer_grouped_parameters, lr=self.learning_rate, eps=self.adam_epsilon)\n            scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=self.warmup_steps, num_training_steps=num_train_optimization_steps)\n\n        self.model.train()\n        for _ in trange(int(self.num_train_epochs), desc=""Epoch""):\n            for step, batch in enumerate(\n                tqdm(\n                    train_dataloader,\n                    desc=""Iteration"",\n                    disable=self.local_rank not in [-1, 0],\n                )\n            ):\n                if self.n_gpu == 1:\n                    batch = tuple(\n                        t.to(self.device) for t in batch\n                    )  # multi-gpu does scattering it-self\n                inputs = {\'input_ids\':       batch[0],\n                          \'attention_mask\':  batch[1],\n                          \'start_positions\': batch[3],\n                          \'end_positions\':   batch[4]}\n                if \'distilbert\' not in self.bert_model:\n                    inputs[\'token_type_ids\'] = batch[2]\n                outputs = self.model(**inputs)\n                loss = outputs[0]\n                if self.n_gpu > 1:\n                    loss = loss.mean()  # mean() to average on multi-gpu.\n                if self.gradient_accumulation_steps > 1:\n                    loss = loss / self.gradient_accumulation_steps\n\n                if self.fp16:\n                    optimizer.backward(loss)\n                else:\n                    loss.backward()\n                if (step + 1) % self.gradient_accumulation_steps == 0:\n                    if self.fp16:\n                        # modify learning rate with special warm up BERT uses\n                        # if self.fp16 is False, BertAdam is used and handles this automatically\n                        lr_this_step = self.learning_rate * warmup_linear.get_lr(\n                            global_step, self.warmup_proportion\n                        )\n                        for param_group in optimizer.param_groups:\n                            param_group[""lr""] = lr_this_step\n                    optimizer.step()\n                    scheduler.step()  # Update learning rate schedule\n                    optimizer.zero_grad()\n                    global_step += 1\n\n        # Save a trained model and configuration\n        model_to_save = (\n            self.model.module if hasattr(self.model, ""module"") else self.model\n        )  # Only save the model it-self\n\n        # If we save using the predefined names, we can load using `from_pretrained`\n        if self.output_dir:\n            output_model_file = os.path.join(self.output_dir, WEIGHTS_NAME)\n            output_config_file = os.path.join(self.output_dir, CONFIG_NAME)\n\n            torch.save(model_to_save.state_dict(), output_model_file)\n            model_to_save.config.to_json_file(output_config_file)\n\n        self.model.to(self.device)\n\n        return self\n\n    def predict(\n        self, X, n_predictions=None, retriever_score_weight=0.35, return_all_preds=False\n    ):\n\n        eval_examples, eval_features = X\n        if self.verbose_logging:\n            logger.info(""***** Running predictions *****"")\n            logger.info(""  Num orig examples = %d"", len(eval_examples))\n            logger.info(""  Num split examples = %d"", len(eval_features))\n            logger.info(""  Batch size = %d"", self.predict_batch_size)\n\n        all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\n        all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\n        all_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)\n        all_cls_index = torch.tensor([f.cls_index for f in eval_features], dtype=torch.long)\n        all_p_mask = torch.tensor([f.p_mask for f in eval_features], dtype=torch.float)\n        all_example_index = torch.arange(all_input_ids.size(0), dtype=torch.long)\n\n        eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids,\n                                  all_example_index, all_cls_index, all_p_mask)\n        # Run prediction for full data\n        eval_sampler = SequentialSampler(eval_data)\n        eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=self.predict_batch_size)\n\n        self.model.to(self.device)\n        self.model.eval()\n        all_results = []\n        if self.verbose_logging:\n            logger.info(""Start evaluating"")\n        for batch in eval_dataloader:\n            if len(all_results) % 1000 == 0 and self.verbose_logging:\n                logger.info(""Processing example: %d"" % (len(all_results)))\n            batch = tuple(t.to(self.device) for t in batch)\n            with torch.no_grad():\n                inputs = {\'input_ids\':      batch[0],\n                          \'attention_mask\': batch[1]\n                          }\n                if \'distilbert\' not in self.bert_model:\n                    inputs[\'token_type_ids\'] = batch[2]\n                example_indices = batch[3]\n                batch_start_logits, batch_end_logits = self.model(**inputs)\n\n            for i, example_index in enumerate(example_indices):\n                start_logits = batch_start_logits[i].detach().cpu().tolist()\n                end_logits = batch_end_logits[i].detach().cpu().tolist()\n                eval_feature = eval_features[example_index.item()]\n                unique_id = int(eval_feature.unique_id)\n                all_results.append(\n                    RawResult(\n                        unique_id=unique_id,\n                        start_logits=start_logits,\n                        end_logits=end_logits,\n                    )\n                )\n        if self.output_dir:\n            if not os.path.exists(self.output_dir):\n                os.makedirs(self.output_dir)\n            output_prediction_file = os.path.join(self.output_dir, ""predictions.json"")\n            output_nbest_file = os.path.join(self.output_dir, ""nbest_predictions.json"")\n            output_null_log_odds_file = os.path.join(self.output_dir, ""null_odds.json"")\n        else:\n            output_prediction_file = None\n            output_nbest_file = None\n            output_null_log_odds_file = None\n\n        result_tuple = write_predictions(\n            eval_examples,\n            eval_features,\n            all_results,\n            self.n_best_size,\n            self.max_answer_length,\n            self.do_lower_case,\n            output_prediction_file,\n            output_nbest_file,\n            output_null_log_odds_file,\n            self.verbose_logging,\n            self.version_2_with_negative,\n            self.null_score_diff_threshold,\n            retriever_score_weight,\n            n_predictions,\n        )\n\n        if n_predictions is not None:\n            return result_tuple[-1]\n\n        best_prediction, final_predictions = result_tuple\n\n        if return_all_preds:\n            return final_predictions\n\n        return best_prediction\n'"
cdqa/retriever/__init__.py,0,"b'from .retriever_sklearn import TfidfRetriever, BM25Retriever\n\n__all__ = [""TfidfRetriever"", ""BM25Retriever""]\n'"
cdqa/retriever/retriever_sklearn.py,0,"b'import pandas as pd\nimport prettytable\nimport time\nfrom abc import ABC, abstractmethod\nfrom collections import OrderedDict\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.base import BaseEstimator\nfrom .vectorizers import BM25Vectorizer\n\n\nclass BaseRetriever(BaseEstimator, ABC):\n    """"""\n    Abstract base class for all Retriever classes.\n    All retrievers should inherit from this class.\n    Each retriever class should implement a _fit_vectorizer method and a\n    _compute_scores method\n    """"""\n\n    def __init__(self, vectorizer, top_n=10, verbose=False):\n        self.vectorizer = vectorizer\n        self.top_n = top_n\n        self.verbose = verbose\n\n    def fit(self, df: pd.DataFrame, y=None):\n        """"""\n        Fit the retriever to a list of documents or paragraphs\n\n        Parameters\n        ----------\n        df: pandas.DataFrame object with all documents\n        """"""\n        self.metadata = df\n        return self._fit_vectorizer(df)\n\n    @abstractmethod\n    def _fit_vectorizer(self, df):\n        pass\n\n    @abstractmethod\n    def _compute_scores(self, query):\n        pass\n\n    def predict(self, query: str) -> OrderedDict:\n        """"""\n        Compute the top_n closest documents given a query\n\n        Parameters\n        ----------\n        query: str\n\n        Returns\n        -------\n        best_idx_scores: OrderedDict\n            Dictionnaire with top_n best scores and idices of the documents as keys\n\n        """"""\n        t0 = time.time()\n        scores = self._compute_scores(query)\n        idx_scores = [(idx, score) for idx, score in enumerate(scores)]\n        best_idx_scores = OrderedDict(\n            sorted(idx_scores, key=(lambda tup: tup[1]), reverse=True)[: self.top_n]\n        )\n\n        # inspired from https://github.com/facebookresearch/DrQA/blob/50d0e49bb77fe0c6e881efb4b6fe2e61d3f92509/scripts/reader/interactive.py#L63\n        if self.verbose:\n            rank = 1\n            table = prettytable.PrettyTable([""rank"", ""index"", ""title""])\n            for i in range(len(closest_docs_indices)):\n                index = closest_docs_indices[i]\n                if self.paragraphs:\n                    article_index = self.paragraphs[int(index)][""index""]\n                    title = self.metadata.iloc[int(article_index)][""title""]\n                else:\n                    title = self.metadata.iloc[int(index)][""title""]\n                table.add_row([rank, index, title])\n                rank += 1\n            print(table)\n            print(""Time: {} seconds"".format(round(time.time() - t0, 5)))\n\n        return best_idx_scores\n\n\nclass TfidfRetriever(BaseRetriever):\n    """"""\n    A scikit-learn estimator for TfidfRetriever. Trains a tf-idf matrix from a corpus\n    of documents then finds the most N similar documents of a given input document by\n    taking the dot product of the vectorized input document and the trained tf-idf matrix.\n\n    Parameters\n    ----------\n    lowercase : boolean\n        Convert all characters to lowercase before tokenizing. (default is True)\n    preprocessor : callable or None\n        Override the preprocessing (string transformation) stage while preserving\n        the tokenizing and n-grams generation steps. (default is None)\n    tokenizer : callable or None\n        Override the string tokenization step while preserving the preprocessing\n        and n-grams generation steps (default is None)\n    stop_words : string {\xe2\x80\x98english\xe2\x80\x99}, list, or None\n        If a string, it is passed to _check_stop_list and the appropriate stop\n        list is returned. \xe2\x80\x98english\xe2\x80\x99 is currently the only supported string value.\n        If a list, that list is assumed to contain stop words, all of which will\n        be removed from the resulting tokens.\n        If None, no stop words will be used. max_df can be set to a value in the\n        range [0.7, 1.0) to automatically detect and filter stop words based on\n        intra corpus document frequency of terms.\n        (default is None)\n    token_pattern : string\n        Regular expression denoting what constitutes a \xe2\x80\x9ctoken\xe2\x80\x9d. The default regexp\n        selects tokens of 2 or more alphanumeric characters (punctuation is completely\n        ignored and always treated as a token separator).\n    ngram_range : tuple (min_n, max_n)\n        The lower and upper boundary of the range of n-values for different n-grams\n        to be extracted. All values of n such that min_n <= n <= max_n will be used.\n        (default is (1, 1))\n    max_df : float in range [0.0, 1.0] or int\n        When building the vocabulary ignore terms that have a document frequency strictly\n        higher than the given threshold (corpus-specific stop words). If float, the parameter\n        represents a proportion of documents, integer absolute counts. This parameter is\n        ignored if vocabulary is not None. (default is 1.0)\n    min_df : float in range [0.0, 1.0] or int\n        When building the vocabulary ignore terms that have a document frequency\n        strictly lower than the given threshold. This value is also called cut-off\n        in the literature. If float, the parameter represents a proportion of\n        documents, integer absolute counts. This parameter is ignored if vocabulary\n        is not None. (default is 1)\n    vocabulary : Mapping or iterable, optional\n        Either a Mapping (e.g., a dict) where keys are terms and values are indices\n        in the feature matrix, or an iterable over terms. If not given, a vocabulary\n        is determined from the input documents. (default is None)\n    paragraphs : iterable\n        an iterable which yields either str, unicode or file objects\n    top_n : int (default 20)\n        maximum number of top articles (or paragraphs) to retrieve\n    verbose : bool, optional\n        If true, all of the warnings related to data processing will be printed.\n\n    Attributes\n    ----------\n    vectorizer : TfidfVectorizer\n        See https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n    tfidf_matrix : sparse matrix, [n_samples, n_features]\n        Tf-idf-weighted document-term matrix.\n\n    Examples\n    --------\n    >>> from cdqa.retriever import TfidfRetriever\n\n    >>> retriever = TfidfRetriever(ngram_range=(1, 2), max_df=0.85, stop_words=\'english\')\n    >>> retriever.fit(X=df)\n    >>> best_idx_scores = retriever.predict(X=\'Since when does the the Excellence Program of BNP Paribas exist?\')\n    """"""\n\n    def __init__(\n        self,\n        lowercase=True,\n        preprocessor=None,\n        tokenizer=None,\n        stop_words=""english"",\n        token_pattern=r""(?u)\\b\\w\\w+\\b"",\n        ngram_range=(1, 2),\n        max_df=0.85,\n        min_df=2,\n        vocabulary=None,\n        top_n=20,\n        verbose=False,\n    ):\n        self.lowercase = lowercase\n        self.preprocessor = preprocessor\n        self.tokenizer = tokenizer\n        self.stop_words = stop_words\n        self.token_pattern = token_pattern\n        self.ngram_range = ngram_range\n        self.max_df = max_df\n        self.min_df = min_df\n        self.vocabulary = vocabulary\n\n        vectorizer = TfidfVectorizer(\n            lowercase=self.lowercase,\n            preprocessor=self.preprocessor,\n            tokenizer=self.tokenizer,\n            stop_words=self.stop_words,\n            token_pattern=self.token_pattern,\n            ngram_range=self.ngram_range,\n            max_df=self.max_df,\n            min_df=self.min_df,\n            vocabulary=self.vocabulary,\n        )\n        super().__init__(vectorizer, top_n, verbose)\n\n    def _fit_vectorizer(self, df, y=None):\n        self.tfidf_matrix = self.vectorizer.fit_transform(df[""content""])\n        return self\n\n    def _compute_scores(self, query):\n        question_vector = self.vectorizer.transform([query])\n        scores = self.tfidf_matrix.dot(question_vector.T).toarray()\n        return scores\n\n\nclass BM25Retriever(BaseRetriever):\n    """"""\n    A scikit-learn estimator for BM25Retriever. Trains a matrix based on BM25 statistics\n    from a corpus of documents then finds the most N similar documents of a given input\n    query by computing the BM25 score for each document based on the query.\n\n    Parameters\n    ----------\n    lowercase : boolean\n        Convert all characters to lowercase before tokenizing. (default is True)\n    preprocessor : callable or None\n        Override the preprocessing (string transformation) stage while preserving\n        the tokenizing and n-grams generation steps. (default is None)\n    tokenizer : callable or None\n        Override the string tokenization step while preserving the preprocessing\n        and n-grams generation steps (default is None)\n    stop_words : string {\xe2\x80\x98english\xe2\x80\x99}, list, or None\n        If a string, it is passed to _check_stop_list and the appropriate stop\n        list is returned. \xe2\x80\x98english\xe2\x80\x99 is currently the only supported string value.\n        If a list, that list is assumed to contain stop words, all of which will\n        be removed from the resulting tokens.\n        If None, no stop words will be used. max_df can be set to a value in the\n        range [0.7, 1.0) to automatically detect and filter stop words based on\n        intra corpus document frequency of terms.\n        (default is None)\n    token_pattern : string\n        Regular expression denoting what constitutes a \xe2\x80\x9ctoken\xe2\x80\x9d. The default regexp\n        selects tokens of 2 or more alphanumeric characters (punctuation is completely\n        ignored and always treated as a token separator).\n    ngram_range : tuple (min_n, max_n)\n        The lower and upper boundary of the range of n-values for different n-grams\n        to be extracted. All values of n such that min_n <= n <= max_n will be used.\n        (default is (1, 1))\n    max_df : float in range [0.0, 1.0] or int\n        When building the vocabulary ignore terms that have a document frequency strictly\n        higher than the given threshold (corpus-specific stop words). If float, the parameter\n        represents a proportion of documents, integer absolute counts. This parameter is\n        ignored if vocabulary is not None. (default is 1.0)\n    min_df : float in range [0.0, 1.0] or int\n        When building the vocabulary ignore terms that have a document frequency\n        strictly lower than the given threshold. This value is also called cut-off\n        in the literature. If float, the parameter represents a proportion of\n        documents, integer absolute counts. This parameter is ignored if vocabulary\n        is not None. (default is 1)\n    vocabulary : Mapping or iterable, optional\n        Either a Mapping (e.g., a dict) where keys are terms and values are indices\n        in the feature matrix, or an iterable over terms. If not given, a vocabulary\n        is determined from the input documents. (default is None)\n    paragraphs : iterable\n        an iterable which yields either str, unicode or file objects\n    top_n : int (default 20)\n        maximum number of top articles (or paragraphs) to retrieve\n    verbose : bool, optional\n        If true, all of the warnings related to data processing will be printed.\n    k1 : float, optional (default=2.0)\n        term k1 in the BM25 formula\n    b : float, optional (default=0.75)\n        term b in the BM25 formula\n    floor : float or None, optional (default=None)\n        floor value for idf terms\n\n    Attributes\n    ----------\n    vectorizer : BM25Vectorizer\n\n    Examples\n    --------\n    >>> from cdqa.retriever import BM25Retriever\n\n    >>> retriever = BM25Retriever(ngram_range=(1, 2), max_df=0.85, stop_words=\'english\')\n    >>> retriever.fit(df=df)\n    >>> best_idx_scores = retriever.predict(query=\'Since when does the the Excellence Program of BNP Paribas exist?\')\n\n    """"""\n\n    def __init__(\n        self,\n        lowercase=True,\n        preprocessor=None,\n        tokenizer=None,\n        stop_words=""english"",\n        token_pattern=r""(?u)\\b\\w\\w+\\b"",\n        ngram_range=(1, 2),\n        max_df=0.85,\n        min_df=2,\n        vocabulary=None,\n        top_n=20,\n        verbose=False,\n        k1=2.0,\n        b=0.75,\n        floor=None,\n    ):\n\n        self.lowercase = lowercase\n        self.preprocessor = preprocessor\n        self.tokenizer = tokenizer\n        self.stop_words = stop_words\n        self.token_pattern = token_pattern\n        self.ngram_range = ngram_range\n        self.max_df = max_df\n        self.min_df = min_df\n        self.vocabulary = vocabulary\n        self.k1 = k1\n        self.b = b\n        self.floor = floor\n\n        vectorizer = BM25Vectorizer(\n            lowercase=self.lowercase,\n            preprocessor=self.preprocessor,\n            tokenizer=self.tokenizer,\n            stop_words=self.stop_words,\n            token_pattern=self.token_pattern,\n            ngram_range=self.ngram_range,\n            max_df=self.max_df,\n            min_df=self.min_df,\n            vocabulary=self.vocabulary,\n            k1=self.k1,\n            b=self.b,\n            floor=self.floor,\n        )\n        super().__init__(vectorizer, top_n, verbose)\n\n    def _fit_vectorizer(self, df, y=None):\n        self.bm25_matrix = self.vectorizer.fit_transform(df[""content""])\n        return self\n\n    def _compute_scores(self, query):\n        question_vector = self.vectorizer.transform([query], is_query=True)\n        scores = self.bm25_matrix.dot(question_vector.T).toarray()\n        return scores\n'"
cdqa/retriever/text_transformers.py,0,"b'# BSD 3-Clause License\n#\n# Copyright (c) 2018, Sho IIZUKA\n# All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n# * Redistributions of source code must retain the above copyright notice, this\n#   list of conditions and the following disclaimer.\n#\n# * Redistributions in binary form must reproduce the above copyright notice,\n#   this list of conditions and the following disclaimer in the documentation\n#   and/or other materials provided with the distribution.\n#\n# * Neither the name of the copyright holder nor the names of its\n#   contributors may be used to endorse or promote products derived from\n#   this software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n# DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n# FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n# DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n# SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n# OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n\nimport numpy as np\nimport scipy.sparse as sp\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.utils.validation import check_is_fitted, check_array, FLOAT_DTYPES\nfrom sklearn.feature_extraction.text import _document_frequency\nfrom sklearn.preprocessing import normalize\n\n\nclass BM25Transformer(BaseEstimator, TransformerMixin):\n    """"""\n    Parameters\n    ----------\n     norm : \'l1\', \'l2\' or None, optional (default=None)\n        Each output row will have unit norm, either:\n        * \'l2\': Sum of squares of vector elements is 1. The cosine\n        similarity between two vectors is their dot product when l2 norm has\n        been applied.\n        * \'l1\': Sum of absolute values of vector elements is 1.\n    use_idf : boolean, optional (default=True)\n        Enable inverse-document-frequency reweighting\n    k1 : float, optional (default=2.0)\n        term k1 in the BM25 formula\n    b : float, optional (default=0.75)\n        term b in the BM25 formula\n    floor : float or None, optional (default=None)\n        floor value for idf terms\n    References\n    ----------\n    Okapi BM25: a non-binary model - Introduction to Information Retrieval\n    http://nlp.stanford.edu/IR-book/html/htmledition/okapi-bm25-a-non-binary-model-1.html\n    """"""\n\n    def __init__(self, norm=None, use_idf=True, k1=2.0, b=0.75, floor=None):\n        self.norm = norm\n        self.use_idf = use_idf\n        self.k1 = k1\n        self.b = b\n        self.floor = floor\n\n    def fit(self, X):\n        """"""\n        Parameters\n        ----------\n        X : sparse matrix, [n_samples, n_features]\n            document-term matrix\n        """"""\n        X = check_array(X, accept_sparse=(""csr"", ""csc""))\n        if not sp.issparse(X):\n            X = sp.csc_matrix(X)\n        if self.use_idf:\n            n_samples, n_features = X.shape\n            df = _document_frequency(X)\n            idf = np.log((n_samples - df + 0.5) / (df + 0.5))\n            if self.floor is not None:\n                idf = idf * (idf > self.floor) + self.floor * (idf < self.floor)\n            self._idf_diag = sp.spdiags(idf, diags=0, m=n_features, n=n_features)\n\n        # Create BM25 features\n\n        # Document length (number of terms) in each row\n        # Shape is (n_samples, 1)\n        dl = X.sum(axis=1)\n        # Number of non-zero elements in each row\n        # Shape is (n_samples, )\n        sz = X.indptr[1:] - X.indptr[0:-1]\n        # In each row, repeat `dl` for `sz` times\n        # Shape is (sum(sz), )\n        # Example\n        # -------\n        # dl = [4, 5, 6]\n        # sz = [1, 2, 3]\n        # rep = [4, 5, 5, 6, 6, 6]\n        rep = np.repeat(np.asarray(dl), sz)\n        # Average document length\n        # Scalar value\n        avgdl = np.average(dl)\n        # Compute BM25 score only for non-zero elements\n        data = (\n            X.data\n            * (self.k1 + 1)\n            / (X.data + self.k1 * (1 - self.b + self.b * rep / avgdl))\n        )\n        X = sp.csr_matrix((data, X.indices, X.indptr), shape=X.shape)\n\n        if self.norm:\n            X = normalize(X, norm=self.norm, copy=False)\n\n        self._doc_matrix = X\n        return self\n\n    def transform(self, X=None, copy=True, is_query=False):\n        """"""\n        Parameters\n        ----------\n        X : sparse matrix, [n_samples, n_features]\n            document-term query matrix\n        copy : boolean, optional (default=True)\n        query: boolean (default=False)\n            whether to transform a query or the documents database\n\n        Returns\n        -------\n        vectors : sparse matrix, [n_samples, n_features]\n\n        """"""\n        if is_query:\n            X = check_array(X, accept_sparse=""csr"", dtype=FLOAT_DTYPES, copy=copy)\n            if not sp.issparse(X):\n                X = sp.csr_matrix(X, dtype=np.float64)\n\n            n_samples, n_features = X.shape\n\n            expected_n_features = self._doc_matrix.shape[1]\n            if n_features != expected_n_features:\n                raise ValueError(\n                    ""Input has n_features=%d while the model""\n                    "" has been trained with n_features=%d""\n                    % (n_features, expected_n_features)\n                )\n\n            if self.use_idf:\n                check_is_fitted(self, ""_idf_diag"", ""idf vector is not fitted"")\n                X = sp.csr_matrix(X.toarray() * self._idf_diag.diagonal())\n\n            return X\n\n        else:\n            return self._doc_matrix\n\n    @property\n    def idf_(self):\n        # if _idf_diag is not set, this will raise an attribute error,\n        # which means hasattr(self, ""idf_"") is False\n        return np.ravel(self._idf_diag.sum(axis=0))\n\n    @idf_.setter\n    def idf_(self, value):\n        value = np.asarray(value, dtype=np.float64)\n        n_features = value.shape[0]\n        self._idf_diag = sp.spdiags(\n            value, diags=0, m=n_features, n=n_features, format=""csr""\n        )\n'"
cdqa/retriever/vectorizers.py,0,"b'import numpy as np\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom .text_transformers import BM25Transformer\n\n\nclass BM25Vectorizer(CountVectorizer):\n    """"""Convert a collection of raw documents to a matrix of BM25 features and computes\n    scores of the documents based on a query\n\n    Vectorizer inspired on the sklearn.feature_extraction.text.TfidfVectorizer\n    class\n\n    Parameters\n    ----------\n    input : string {\'filename\', \'file\', \'content\'}\n        If \'filename\', the sequence passed as an argument to fit is\n        expected to be a list of filenames that need reading to fetch\n        the raw content to analyze.\n\n        If \'file\', the sequence items must have a \'read\' method (file-like\n        object) that is called to fetch the bytes in memory.\n\n        Otherwise the input is expected to be the sequence strings or\n        bytes items are expected to be analyzed directly.\n\n    encoding : string, \'utf-8\' by default.\n        If bytes or files are given to analyze, this encoding is used to\n        decode.\n\n    decode_error : {\'strict\', \'ignore\', \'replace\'} (default=\'strict\')\n        Instruction on what to do if a byte sequence is given to analyze that\n        contains characters not of the given `encoding`. By default, it is\n        \'strict\', meaning that a UnicodeDecodeError will be raised. Other\n        values are \'ignore\' and \'replace\'.\n\n    strip_accents : {\'ascii\', \'unicode\', None} (default=None)\n        Remove accents and perform other character normalization\n        during the preprocessing step.\n        \'ascii\' is a fast method that only works on characters that have\n        an direct ASCII mapping.\n        \'unicode\' is a slightly slower method that works on any characters.\n        None (default) does nothing.\n\n        Both \'ascii\' and \'unicode\' use NFKD normalization from\n        :func:`unicodedata.normalize`.\n\n    lowercase : boolean (default=True)\n        Convert all characters to lowercase before tokenizing.\n\n    preprocessor : callable or None (default=None)\n        Override the preprocessing (string transformation) stage while\n        preserving the tokenizing and n-grams generation steps.\n\n    tokenizer : callable or None (default=None)\n        Override the string tokenization step while preserving the\n        preprocessing and n-grams generation steps.\n        Only applies if ``analyzer == \'word\'``.\n\n    analyzer : string, {\'word\', \'char\', \'char_wb\'} or callable\n        Whether the feature should be made of word or character n-grams.\n        Option \'char_wb\' creates character n-grams only from text inside\n        word boundaries; n-grams at the edges of words are padded with space.\n\n        If a callable is passed it is used to extract the sequence of features\n        out of the raw, unprocessed input.\n\n        .. versionchanged:: 0.21\n        Since v0.21, if ``input`` is ``filename`` or ``file``, the data is\n        first read from the file and then passed to the given callable\n        analyzer.\n\n    stop_words : string {\'english\'}, list, or None (default=None)\n        If a string, it is passed to _check_stop_list and the appropriate stop\n        list is returned. \'english\' is currently the only supported string\n        value.\n        There are several known issues with \'english\' and you should\n        consider an alternative (see :ref:`stop_words`).\n\n        If a list, that list is assumed to contain stop words, all of which\n        will be removed from the resulting tokens.\n        Only applies if ``analyzer == \'word\'``.\n\n        If None, no stop words will be used. max_df can be set to a value\n        in the range [0.7, 1.0) to automatically detect and filter stop\n        words based on intra corpus document frequency of terms.\n\n    token_pattern : string\n        Regular expression denoting what constitutes a ""token"", only used\n        if ``analyzer == \'word\'``. The default regexp selects tokens of 2\n        or more alphanumeric characters (punctuation is completely ignored\n        and always treated as a token separator).\n\n    ngram_range : tuple (min_n, max_n) (default=(1, 1))\n        The lower and upper boundary of the range of n-values for different\n        n-grams to be extracted. All values of n such that min_n <= n <= max_n\n        will be used.\n\n    max_df : float in range [0.0, 1.0] or int (default=1.0)\n        When building the vocabulary ignore terms that have a document\n        frequency strictly higher than the given threshold (corpus-specific\n        stop words).\n        If float, the parameter represents a proportion of documents, integer\n        absolute counts.\n        This parameter is ignored if vocabulary is not None.\n\n    min_df : float in range [0.0, 1.0] or int (default=1)\n        When building the vocabulary ignore terms that have a document\n        frequency strictly lower than the given threshold. This value is also\n        called cut-off in the literature.\n        If float, the parameter represents a proportion of documents, integer\n        absolute counts.\n        This parameter is ignored if vocabulary is not None.\n\n    max_features : int or None (default=None)\n        If not None, build a vocabulary that only consider the top\n        max_features ordered by term frequency across the corpus.\n\n        This parameter is ignored if vocabulary is not None.\n\n    vocabulary : Mapping or iterable, optional (default=None)\n        Either a Mapping (e.g., a dict) where keys are terms and values are\n        indices in the feature matrix, or an iterable over terms. If not\n        given, a vocabulary is determined from the input documents.\n\n    binary : boolean (default=False)\n        If True, all non-zero term counts are set to 1. This does not mean\n        outputs will have only 0/1 values, only that the tf term in tf-idf\n        is binary. (Set idf and normalization to False to get 0/1 outputs.)\n\n    dtype : type, optional (default=float64)\n        Type of the matrix returned by fit_transform() or transform().\n\n    norm : \'l1\', \'l2\' or None, optional (default=\'l2\')\n        Each output row will have unit norm, either:\n        * \'l2\': Sum of squares of vector elements is 1. The cosine\n        similarity between two vectors is their dot product when l2 norm has\n        been applied.\n        * \'l1\': Sum of absolute values of vector elements is 1.\n        See :func:`preprocessing.normalize`\n\n    use_idf : boolean (default=True)\n        Enable inverse-document-frequency reweighting.\n\n    k1 : float, optional (default=2.0)\n        term k1 in the BM25 formula\n\n    b : float, optional (default=0.75)\n        term b in the BM25 formula\n\n    floor : float or None, optional (default=None)\n        floor value for idf terms\n\n    Attributes\n    ----------\n    vocabulary_ : dict\n        A mapping of terms to feature indices.\n\n    idf_ : array, shape (n_features)\n        The inverse document frequency (IDF) vector; only defined\n        if ``use_idf`` is True.\n\n    stop_words_ : set\n        Terms that were ignored because they either:\n\n          - occurred in too many documents (`max_df`)\n          - occurred in too few documents (`min_df`)\n          - were cut off by feature selection (`max_features`).\n\n        This is only available if no vocabulary was given.\n    """"""\n\n    def __init__(\n        self,\n        input=""content"",\n        encoding=""utf-8"",\n        decode_error=""strict"",\n        strip_accents=None,\n        lowercase=True,\n        preprocessor=None,\n        tokenizer=None,\n        analyzer=""word"",\n        stop_words=None,\n        token_pattern=r""(?u)\\b\\w\\w+\\b"",\n        ngram_range=(1, 2),\n        max_df=1.0,\n        min_df=1,\n        max_features=None,\n        vocabulary=None,\n        binary=False,\n        dtype=np.float64,\n        norm=None,\n        use_idf=True,\n        k1=2.0,\n        b=0.75,\n        floor=None,\n    ):\n\n        super().__init__(\n            input=input,\n            encoding=encoding,\n            decode_error=decode_error,\n            strip_accents=strip_accents,\n            lowercase=lowercase,\n            preprocessor=preprocessor,\n            tokenizer=tokenizer,\n            analyzer=analyzer,\n            stop_words=stop_words,\n            token_pattern=token_pattern,\n            ngram_range=ngram_range,\n            max_df=max_df,\n            min_df=min_df,\n            max_features=max_features,\n            vocabulary=vocabulary,\n            binary=binary,\n            dtype=dtype,\n        )\n\n        self._bm25 = BM25Transformer(norm, use_idf, k1, b)\n\n    # Broadcast the BM25 parameters to the underlying transformer instance\n    # for easy grid search and repr\n    @property\n    def norm(self):\n        return self._bm25.norm\n\n    @norm.setter\n    def norm(self, value):\n        self._bm25.norm = value\n\n    @property\n    def use_idf(self):\n        return self._bm25.use_idf\n\n    @use_idf.setter\n    def use_idf(self, value):\n        self._bm25.use_idf = value\n\n    @property\n    def k1(self):\n        return self._bm25.k1\n\n    @k1.setter\n    def k1(self, value):\n        self._bm25.k1 = value\n\n    @property\n    def b(self):\n        return self._bm25.b\n\n    @b.setter\n    def b(self, value):\n        self._bm25.b = value\n\n    @property\n    def idf_(self):\n        return self._bm25.idf_\n\n    @idf_.setter\n    def idf_(self, value):\n        self._validate_vocabulary()\n        if hasattr(self, ""vocabulary_""):\n            if len(self.vocabulary_) != len(value):\n                raise ValueError(\n                    ""idf length = %d must be equal ""\n                    ""to vocabulary size = %d"" % (len(value), len(self.vocabulary))\n                )\n        self._bm25.idf_ = value\n\n    def fit(self, raw_documents, y=None):\n        """"""\n        Learn vocabulary and BM25 stats from training set.\n\n        Parameters\n        ----------\n        raw_documents : iterable\n            an iterable which yields either str, unicode or file objects\n\n        Returns\n        -------\n        self : BM25Vectorizer\n        """"""\n        X = super().fit_transform(raw_documents)\n        self._bm25.fit(X)\n        return self\n\n    def transform(self, raw_corpus, is_query=False):\n        """"""\n        Vectorizes the input, whether it is a query or the list of documents\n\n        Parameters\n        ----------\n        raw_corpus : iterable\n            an iterable which yields either str, unicode or file objects\n\n        Returns\n        -------\n        vectors : sparse matrix, [n_queries, n_documents]\n            scores from BM25 statics for each document with respect to each query\n        """"""\n        X = super().transform(raw_corpus) if is_query else None\n\n        return self._bm25.transform(X, copy=False, is_query=is_query)\n\n    def fit_transform(self, raw_documents, y=None):\n        """"""\n        Learn vocabulary, idf and BM25 features. Return term-document matrix.\n        This is equivalent to fit followed by transform, but more efficiently\n        implemented.\n\n        Parameters\n        ----------\n        raw_documents : iterable\n            an iterable which yields either str, unicode or file objects\n            \n        Returns\n        -------\n        X : sparse matrix, [n_samples, n_features]\n            BM25 document-term matrix.\n        """"""\n        X = super().fit_transform(raw_documents)\n        self._bm25.fit(X)\n        return self._bm25.transform(X, copy=False)\n'"
cdqa/utils/__init__.py,0,b''
cdqa/utils/converters.py,0,"b'import json\nimport os\nimport re\nimport sys\nfrom tqdm import tqdm\nfrom tika import parser\nimport pandas as pd\nimport uuid\nimport markdown\nfrom pathlib import Path\nfrom html.parser import HTMLParser\n\n\ndef df2squad(df, squad_version=""v1.1"", output_dir=None, filename=None):\n    """"""\n     Converts a pandas dataframe with columns [\'title\', \'paragraphs\'] to a json file with SQuAD format.\n\n     Parameters\n    ----------\n     df : pandas.DataFrame\n         a pandas dataframe with columns [\'title\', \'paragraphs\']\n     squad_version : str, optional\n         the SQuAD dataset version format (the default is \'v2.0\')\n     output_dir : str, optional\n         Enable export of output (the default is None)\n     filename : str, optional\n         [description]\n\n    Returns\n    -------\n    json_data: dict\n        A json object with SQuAD format\n\n     Examples\n     --------\n     >>> from ast import literal_eval\n     >>> import pandas as pd\n     >>> from cdqa.utils.converters import df2squad\n     >>> from cdqa.utils.filters import filter_paragraphs\n\n     >>> df = pd.read_csv(\'../data/bnpp_newsroom_v1.1/bnpp_newsroom-v1.1.csv\', converters={\'paragraphs\': literal_eval})\n     >>> df[\'paragraphs\'] = df[\'paragraphs\'].apply(filter_paragraphs)\n\n     >>> json_data = df2squad(df=df, squad_version=\'v1.1\', output_dir=\'../data\', filename=\'bnpp_newsroom-v1.1\')\n    """"""\n\n    json_data = {}\n    json_data[""version""] = squad_version\n    json_data[""data""] = []\n\n    for idx, row in tqdm(df.iterrows()):\n        temp = {""title"": row[""title""], ""paragraphs"": []}\n        for paragraph in row[""paragraphs""]:\n            temp[""paragraphs""].append({""context"": paragraph, ""qas"": []})\n        json_data[""data""].append(temp)\n\n    if output_dir:\n        with open(os.path.join(output_dir, ""{}.json"".format(filename)), ""w"") as outfile:\n            json.dump(json_data, outfile)\n\n    return json_data\n\n\ndef generate_squad_examples(question, best_idx_scores, metadata, retrieve_by_doc):\n    """"""\n    Creates a SQuAD examples json object for a given question using outputs of retriever and document database.\n\n    Parameters\n    ----------\n    question : [type]\n        [description]\n    best_idx_scores : [type]\n        [description]\n    metadata : [type]\n        [description]\n\n    Returns\n    -------\n    squad_examples: list\n        [description]\n\n    Examples\n    --------\n    >>> from cdqa.utils.converters import generate_squad_examples\n    >>> squad_examples = generate_squad_examples(question=\'Since when does the the Excellence Program of BNP Paribas exist?\',\n                                         best_idx_scores=[(788, 1.2), (408, 0.4), (2419, 0.2)],\n                                         metadata=df)\n\n    """"""\n\n    squad_examples = []\n\n    metadata_sliced = metadata.loc[best_idx_scores.keys()]\n\n    for idx, row in metadata_sliced.iterrows():\n        temp = {""title"": row[""title""], ""paragraphs"": []}\n\n        if retrieve_by_doc:\n            for paragraph in row[""paragraphs""]:\n                temp[""paragraphs""].append(\n                    {\n                        ""context"": paragraph,\n                        ""qas"": [\n                            {\n                                ""answers"": [],\n                                ""question"": question,\n                                ""id"": str(uuid.uuid4()),\n                                ""retriever_score"": best_idx_scores[idx],\n                            }\n                        ],\n                    }\n                )\n        else:\n            temp[""paragraphs""] = [\n                {\n                    ""context"": row[""content""],\n                    ""qas"": [\n                        {\n                            ""answers"": [],\n                            ""question"": question,\n                            ""id"": str(uuid.uuid4()),\n                            ""retriever_score"": best_idx_scores[idx],\n                        }\n                    ],\n                }\n            ]\n\n        squad_examples.append(temp)\n\n    return squad_examples\n\n\ndef pdf_converter(directory_path, min_length=200, include_line_breaks=False):\n    """"""\n    Function to convert PDFs to Dataframe with columns as title & paragraphs.\n\n    Parameters\n    ----------\n\n    min_length : integer\n        Minimum character length to be considered as a single paragraph\n\n    include_line_breaks: bool\n        To concatenate paragraphs less than min_length to a single paragraph\n\n\n\n    Returns\n    -------------\n    df : Dataframe\n\n\n    Description\n    -----------------\n    If include_line_breaks is set to True, paragraphs with character length\n    less than min_length (minimum character length of a paragraph) will be\n    considered as a line. Lines before or after each paragraph(length greater\n    than or equal to min_length) will be concatenated to a single paragraph to\n    form the list of paragraphs in Dataframe.\n\n    Else paragraphs are appended directly to form the list.\n\n    """"""\n    list_file = os.listdir(directory_path)\n    list_pdf = []\n    for file in list_file:\n        if file.endswith(""pdf""):\n            list_pdf.append(file)\n    df = pd.DataFrame(columns=[""title"", ""paragraphs""])\n    for i, pdf in enumerate(list_pdf):\n        try:\n            df.loc[i] = [pdf.replace("".pdf"",\'\'), None]\n            raw = parser.from_file(os.path.join(directory_path, pdf))\n            s = raw[""content""].strip()\n            paragraphs = re.split(""\\n\\n(?=\\u2028|[A-Z-0-9])"", s)\n            list_par = []\n            temp_para = """"  # variable that stores paragraphs with length<min_length\n            # (considered as a line)\n            for p in paragraphs:\n                if not p.isspace():  # checking if paragraph is not only spaces\n                    if include_line_breaks:  # if True, check length of paragraph\n                        if len(p) >= min_length:\n                            if temp_para:\n                                # if True, append temp_para which holds concatenated\n                                # lines to form a paragraph before current paragraph p\n                                list_par.append(temp_para.strip())\n                                temp_para = (\n                                    """"\n                                )  # reset temp_para for new lines to be concatenated\n                                list_par.append(\n                                    p.replace(""\\n"", """")\n                                )  # append current paragraph with length>min_length\n                            else:\n                                list_par.append(p.replace(""\\n"", """"))\n                        else:\n                            # paragraph p (line) is concatenated to temp_para\n                            line = p.replace(""\\n"", "" "").strip()\n                            temp_para = temp_para + f"" {line}""\n                    else:\n                        # appending paragraph p as is to list_par\n                        list_par.append(p.replace(""\\n"", """"))\n                else:\n                    if temp_para:\n                        list_par.append(temp_para.strip())\n\n            df.loc[i, ""paragraphs""] = list_par\n        except:\n            print(""Unexpected error:"", sys.exc_info()[0])\n            print(""Unable to process file {}"".format(pdf))\n    return df\n\n\nclass MLStripper(HTMLParser):\n    def __init__(self):\n        self.reset()\n        self.strict = False\n        self.convert_charrefs = True\n        self.fed = []\n\n    def handle_data(self, d):\n        self.fed.append(d)\n\n    def get_data(self):\n        return """".join(self.fed)\n\n\ndef strip_tags(html):\n    s = MLStripper()\n    s.feed(html)\n    return s.get_data()\n\n\ndef md_converter(directory_path):\n    """"""Get all md, convert them to html and create the pandas dataframe with columns [\'title\', \'paragraphs\']""""""\n    dict_doc = {""title"": [], ""paragraphs"": []}\n    for md_file in Path(directory_path).glob(""**/*.md""):\n        md_file = str(md_file)\n        filename = md_file.split(""/"")[-1]\n        try:\n            with open(md_file, ""r"") as f:\n                dict_doc[""title""].append(filename)\n                md_text = f.read()\n                html_text = markdown.markdown(md_text)\n                html_text_list = list(html_text.split(""<p>""))\n                for i in range(len(html_text_list)):\n                    html_text_list[i] = (\n                        strip_tags(html_text_list[i])\n                        .replace(""\\n"", "" "")\n                        .lstrip()\n                        .rstrip()\n                    )\n                clean_text_list = list(filter(None, html_text_list))\n                dict_doc[""paragraphs""].append(clean_text_list)\n        except:\n            print(""Unexpected error:"", sys.exc_info()[0])\n            print(""Unable to process file {}"".format(filename))\n    df = pd.DataFrame.from_dict(dict_doc)\n    return df\n'"
cdqa/utils/download.py,0,"b'import os\nimport wget\n\n\ndef download_squad(dir="".""):\n    """"""\n    Download SQuAD 1.1 and SQuAD 2.0 datasets\n\n    Parameters\n    ----------\n    dir: str\n        Directory where the dataset will be stored\n\n    """"""\n\n    dir = os.path.expanduser(dir)\n    if not os.path.exists(dir):\n        os.makedirs(dir)\n\n    # Download SQuAD 1.1\n    print(""Downloading SQuAD v1.1 data..."")\n\n    dir_squad11 = os.path.join(dir, ""SQuAD_1.1"")\n    squad11_urls = [\n        ""https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json"",\n        ""https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json"",\n    ]\n\n    if not os.path.exists(dir_squad11):\n        os.makedirs(dir_squad11)\n\n    for squad_url in squad11_urls:\n        file = squad_url.split(""/"")[-1]\n        if os.path.exists(os.path.join(dir_squad11, file)):\n            print(file, ""already downloaded"")\n        else:\n            wget.download(url=squad_url, out=dir_squad11)\n\n    # Download SQuAD 2.0\n    print(""\\nDownloading SQuAD v2.0 data..."")\n\n    dir_squad20 = os.path.join(dir, ""SQuAD_2.0"")\n    squad20_urls = [\n        ""https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json"",\n        ""https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json"",\n    ]\n\n    if not os.path.exists(dir_squad20):\n        os.makedirs(dir_squad20)\n\n    for squad_url in squad20_urls:\n        file = squad_url.split(""/"")[-1]\n        if os.path.exists(os.path.join(dir_squad20, file)):\n            print(file, ""already downloaded"")\n        else:\n            wget.download(url=squad_url, out=dir_squad20)\n\n\ndef download_model(model=""bert-squad_1.1"", dir="".""):\n    """"""\n    Download pretrained models\n\n    Parameters\n    ----------\n    model: str\n        Model to be download. It should be one of the models in the list:\n        `bert-squad_1.1`, `distilbert-squad_1.1`\n\n    dir: str\n        Directory where the dataset will be stored\n\n    """"""\n\n    models_url = {\n        ""bert-squad_1.1"": ""https://github.com/cdqa-suite/cdQA/releases/download/bert_qa/bert_qa.joblib"",\n        ""distilbert-squad_1.1"": ""https://github.com/cdqa-suite/cdQA/releases/download/distilbert_qa/distilbert_qa.joblib""\n    }\n\n    if not model in models_url:\n        print(\n            ""The model you chose does not exist. Please choose one of the following models:""\n        )\n        for model in models_url.keys():\n            print(model)\n    else:\n        print(""\\nDownloading trained model..."")\n\n        dir = os.path.expanduser(dir)\n        if not os.path.exists(dir):\n            os.makedirs(dir)\n\n        url = models_url[model]\n        file = url.split(""/"")[-1]\n        if os.path.exists(os.path.join(dir, file)):\n            print(file, ""already downloaded"")\n        else:\n            wget.download(url=url, out=dir)\n\n\ndef download_bnpp_data(dir="".""):\n    """"""\n    Download BNP Paribas\' dataset\n\n    Parameters\n    ----------\n    dir: str\n        Directory where the dataset will be stored\n\n    """"""\n\n    dir = os.path.expanduser(dir)\n    if not os.path.exists(dir):\n        os.makedirs(dir)\n\n    url = ""https://github.com/cdqa-suite/cdQA/releases/download/bnpp_newsroom_v1.1/bnpp_newsroom-v1.1.csv""\n\n    print(""\\nDownloading BNP data..."")\n\n    file = url.split(""/"")[-1]\n    if os.path.exists(os.path.join(dir, file)):\n        print(file, ""already downloaded"")\n    else:\n        wget.download(url=url, out=dir)\n'"
cdqa/utils/evaluation.py,1,"b'"""""" Official evaluation script for v1.1 of the SQuAD dataset. """"""\nfrom __future__ import print_function\nfrom collections import Counter\nimport torch\nimport string\nimport re\nimport argparse\nimport tqdm\nimport json\nimport sys\nimport os\n\nimport joblib\nfrom tqdm.autonotebook import tqdm\n\n\ndef normalize_answer(s):\n    """"""Lower text and remove punctuation, articles and extra whitespace.""""""\n\n    def remove_articles(text):\n        return re.sub(r""\\b(a|an|the)\\b"", "" "", text)\n\n    def white_space_fix(text):\n        return "" "".join(text.split())\n\n    def remove_punc(text):\n        exclude = set(string.punctuation)\n        return """".join(ch for ch in text if ch not in exclude)\n\n    def lower(text):\n        return text.lower()\n\n    return white_space_fix(remove_articles(remove_punc(lower(s))))\n\n\ndef f1_score(prediction, ground_truth):\n    prediction_tokens = normalize_answer(prediction).split()\n    ground_truth_tokens = normalize_answer(ground_truth).split()\n    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n    num_same = sum(common.values())\n    if num_same == 0:\n        return 0\n    precision = 1.0 * num_same / len(prediction_tokens)\n    recall = 1.0 * num_same / len(ground_truth_tokens)\n    f1 = (2 * precision * recall) / (precision + recall)\n    return f1\n\n\ndef exact_match_score(prediction, ground_truth):\n    return normalize_answer(prediction) == normalize_answer(ground_truth)\n\n\ndef metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n    scores_for_ground_truths = []\n    for ground_truth in ground_truths:\n        score = metric_fn(prediction, ground_truth)\n        scores_for_ground_truths.append(score)\n    return max(scores_for_ground_truths)\n\n\ndef evaluate(dataset, predictions, unique_pred=True):\n    f1 = exact_match = total = 0\n    for article in dataset:\n        for paragraph in article[""paragraphs""]:\n            for qa in paragraph[""qas""]:\n                total += 1\n                if qa[""id""] not in predictions:\n                    message = (\n                        ""Unanswered question "" + qa[""id""] + "" will receive score 0.""\n                    )\n                    print(message, file=sys.stderr)\n                    continue\n                ground_truths = list(map(lambda x: x[""text""], qa[""answers""]))\n                if unique_pred:\n                    prediction = predictions[qa[""id""]]\n                    increm_em = metric_max_over_ground_truths(\n                        exact_match_score, prediction, ground_truths\n                    )\n                    increm_f1 = metric_max_over_ground_truths(\n                        f1_score, prediction, ground_truths\n                    )\n                else:\n                    preds = predictions[qa[""id""]]\n                    increm_em = max(\n                        [\n                            metric_max_over_ground_truths(\n                                exact_match_score, prediction, ground_truths\n                            )\n                            for prediction in preds\n                        ]\n                    )\n                    increm_f1 = max(\n                        [\n                            metric_max_over_ground_truths(\n                                f1_score, prediction, ground_truths\n                            )\n                            for prediction in preds\n                        ]\n                    )\n\n                exact_match += increm_em\n                f1 += increm_f1\n\n    exact_match = 100.0 * exact_match / total\n    f1 = 100.0 * f1 / total\n\n    return {""exact_match"": exact_match, ""f1"": f1}\n\n\ndef evaluate_reader(cdqa_pipeline, dataset_file, expected_version=""1.1""):\n    """"""Evaluation for SQuAD\n\n    Parameters\n    ----------\n    cdqa_pipeline: QAPipeline object\n        Pipeline with reader to be evaluated\n    dataset_file : str\n        path to json file in SQuAD format\n    expected_version : str, optional\n        [description], by default \'1.1\'\n\n    Returns\n    -------\n    A dictionary with exact match and f1 scores\n    """"""\n\n    with open(dataset_file, ""r"") as dataset_file:\n        dataset_json = json.load(dataset_file)\n        if dataset_json[""version""] != expected_version:\n            print(\n                ""Evaluation expects v-""\n                + expected_version\n                + "", but got dataset with v-""\n                + dataset_json[""version""],\n                file=sys.stderr,\n            )\n        dataset = dataset_json[""data""]\n\n    if torch.cuda.is_available():\n        cdqa_pipeline.cuda()\n    reader = cdqa_pipeline.reader\n    processor = cdqa_pipeline.processor_predict\n    examples, features = processor.fit_transform(dataset)\n    preds = reader.predict((examples, features), return_all_preds=True)\n    all_predictions = {d[\'qas_id\']: d[\'text\'] for d in preds}\n\n    return evaluate(dataset, all_predictions)\n\n\ndef evaluate_pipeline(\n    cdqa_pipeline,\n    annotated_json,\n    output_dir=""./results"",\n    n_predictions=None,\n    verbose=True,\n):\n    """"""Evaluation method for a whole pipeline (retriever + reader)\n\n    Parameters\n    ----------\n    cdqa_pipeline: QAPipeline object\n        Pipeline to be evaluated\n    annotated_json: str\n        path to json file in SQuAD format with annotated questions and answers\n    output_dir: str\n        path to directory where results and predictions will be saved. If None,\n        no file will be saved\n    verbose: boolean\n        whether the result should be printed or not\n\n    Returns\n    -------\n    A dictionary with exact match and f1 scores\n\n    """"""\n    if output_dir is not None:\n        dir = os.path.expanduser(output_dir)\n        if not os.path.exists(dir):\n            os.makedirs(dir)\n        dir = os.path.join(dir, annotated_json.split(""/"")[-1][:-5])\n        if not os.path.exists(dir):\n            os.makedirs(dir)\n        preds_path = os.path.join(dir, ""all_predictions.json"")\n        results_path = os.path.join(dir, ""results.json"")\n\n    with open(annotated_json, ""r"") as file:\n        data_dict = json.load(file)\n\n    queries = _get_queries_list(data_dict)\n    all_predictions = _pipeline_predictions(cdqa_pipeline, queries, n_predictions)\n    if output_dir is not None:\n        with open(preds_path, ""w"") as f:\n            json.dump(all_predictions, f)\n\n    unique_pred = n_predictions is None\n    results = evaluate(data_dict[""data""], all_predictions, unique_pred)\n    if output_dir is not None:\n        with open(results_path, ""w"") as f:\n            json.dump(results, f)\n    if verbose:\n        print(""\\nEvaluation results:"", results)\n\n    return results\n\n\ndef _get_queries_list(data_dict):\n\n    queries = []\n    articles = data_dict[""data""]\n    for article in articles:\n        paragraphs = article[""paragraphs""]\n        for paragraph in paragraphs:\n            questions = paragraph[""qas""]\n            for question in questions:\n                query = question[""question""]\n                id = question[""id""]\n                queries.append((id, query))\n\n    return queries\n\n\ndef _pipeline_predictions(cdqa_pipeline, queries, n_predictions=None):\n\n    all_predictions = dict()\n    for id, query in tqdm(queries):\n        if n_predictions is None:\n            all_predictions[id] = cdqa_pipeline.predict(query)[0]\n        else:\n            preds = cdqa_pipeline.predict(query, n_predictions=n_predictions)\n            all_predictions[id] = [pred[0] for pred in preds]\n    return all_predictions\n'"
cdqa/utils/filters.py,0,"b'import os\nimport pandas as pd\nimport numpy as np\n\n\ndef filter_paragraphs(\n    articles,\n    drop_empty=True,\n    read_threshold=1000,\n    public_data=True,\n    min_length=50,\n    max_length=300,\n):\n    """"""\n    Cleans the paragraphs and filters them regarding their length\n\n    Parameters\n    ----------\n    articles : DataFrame of all the articles \n\n\n    Returns\n    -------\n    Cleaned and filtered dataframe\n\n    Examples\n    --------\n    >>> import pandas as pd\n    >>> from cdqa.utils.filters import filter_paragraphs\n\n    >>> df = pd.read_csv(\'data.csv\')\n    >>> df_cleaned = filter_paragraphs(df)\n    """"""\n\n    # Replace and split\n    def replace_and_split(paragraphs):\n        for paragraph in paragraphs:\n            paragraph.replace(""\'s"", "" "" ""s"").replace(""\\\\n"", """").split(""\'"")\n        return paragraphs\n\n    # Select paragraphs with the required size\n    def filter_on_size(paragraphs, min_length=min_length, max_length=max_length):\n        paragraph_filtered = [\n            paragraph.strip()\n            for paragraph in paragraphs\n            if len(paragraph.split()) >= min_length\n            and len(paragraph.split()) <= max_length\n        ]\n        return paragraph_filtered\n\n    # Cleaning and filtering\n    articles[""paragraphs""] = articles[""paragraphs""].apply(replace_and_split)\n    articles[""paragraphs""] = articles[""paragraphs""].apply(filter_on_size)\n    articles[""paragraphs""] = articles[""paragraphs""].apply(\n        lambda x: x if len(x) > 0 else np.nan\n    )\n\n    # Read threshold for private dataset\n    if not public_data:\n        articles = articles.loc[articles[""number_of_read""] >= read_threshold]\n\n    # Drop empty articles\n    if drop_empty:\n        articles = articles.dropna(subset=[""paragraphs""]).reset_index(drop=True)\n\n    return articles\n'"
